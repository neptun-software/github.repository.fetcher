{
  "metadata": {
    "timestamp": 1736568474159,
    "page": 447,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ0OQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "pingcap/docs-cn",
      "stars": 1813,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.212890625,
          "content": "# Created by .ignore support plugin (hsz.mobi)\n### Example user template template\n### Example user template\n\n# IntelliJ project files\n.idea/\n.vscode/\n*.iml\numl/\nout\ngen\n.DS_Store\n\n/node_modules/\npackage.json\nyarn.lock\n"
        },
        {
          "name": ".markdownlint.yaml",
          "type": "blob",
          "size": 1.1708984375,
          "content": "comment: 'Markdownlint Rules for PingCAP docs-cn'\n\n# disable all by default\ndefault: false\n\n# MD001\nheading-increment: true\n\n# MD003\nheading-style:\n  style: atx\n\n# MD009\n# no-trailing-spaces: true\n\n# MD010\nno-hard-tabs: true\n\n# MD007\nul-indent:\n  indent: 4\n\n# MD012\nno-multiple-blanks: true\n\n# commands-show-output: true\n\n# MD018\nno-missing-space-atx: true\n\n# MD019\nno-multiple-space-atx: true\n\n# MD022\nblanks-around-headings: true\n\n# MD023\nheading-start-left: true\n\n# MD024\nno-duplicate-heading:\n  siblings_only: true\n\n# MD025\nsingle-title:\n  front_matter_title: ''\n\n# MD026\nno-trailing-punctuation:\n  punctuation: '.,;:!。，；：！'\n\n# MD027\nno-multiple-space-blockquote: true\n\n# MD029\nol-prefix:\n  style: ordered\n\n# MD030\nlist-marker-space: true\n\n# MD031\nblanks-around-fences: true\n\n# MD032\nblanks-around-lists: true\n\n# MD034\nno-bare-urls: true\n\n# MD037\nno-space-in-emphasis: true\n\n# MD038\nno-space-in-code: true\n\n# MD039\nno-space-in-links: true\n\n# MD042\nno-empty-links: true\n\n# proper-names:\n#   names: ['TiDB', 'TiKV', 'PingCAP']\n#   code_blocks: false\n\n# MD045\nno-alt-text: true\n\n# MD046\ncode-block-style:\n  style: fenced\n\n# single-trailing-newline: true\n\n# MD041\nfirst-line-heading: true\n"
        },
        {
          "name": ".vaunt",
          "type": "tree",
          "content": null
        },
        {
          "name": ".zhlint.yaml",
          "type": "blob",
          "size": 1.33203125,
          "content": "includes:\n  - \"/*.md\"\n  - \"s*/*.md\"\nignore:\n  - path: encryption-at-rest.md\n    scope: all\n    ruleId: ZH426\n  # 括号作为开闭区间标记，忽略左右括号匹配\n  - path: sql-statements/sql-statement-split-region.md\n    scope: all\n    ruleId: ZH423\n  - path: experimental-features.md\n    scope: all\n    ruleId: ZH417\n  # 忽略顿号带来的中英文符号检测\n  - path: mysql-compatibility.md\n    scope: all\n    ruleId: ZH417\n  - path: tidb-storage.md\n    scope: line\n    ruleId: ZH423\n    line: 51\n  - path: statistics.md\n    scope: line\n    ruleId: ZH400\n    line: 32\n  - path: tidb-troubleshooting-map.md\n    scope: all\n    ruleId: ZH417\n  - path: sql-statements/sql-statement-show-table-regions.md\n    scope: line\n    line: 165\n    ruleId: ZH423\n  - path: sql-statements/sql-statement-explain-analyze.md\n    scope: all\n    ruleId: ZH403\n  - path: sql-statements/sql-statement-explain-analyze.md\n    scope: all\n    ruleId: ZH428\n  - path: tidb-configuration-file.md\n    scope: line\n    line: 120\n    ruleId: ZH417\n  - path: storage-engine/titan-overview.md\n    line: 77\n    scope: line\n    ruleId: ZH427\n  - path: storage-engine/rocksdb-overview.md \n    scope: all\n    ruleId: ZH403\n  - path: pd-control.md\n    scope: all\n    ruleId: ZH403\n\nfailed_msg: \"以上报错结果仅供参考，请 PR 作者二次确认错误信息后再修改文档。\"\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 14.8955078125,
          "content": "# TiDB 中文文档贡献指南\n\n无论你是热爱技术的程序员，还是擅长书面表达的语言爱好者，亦或是纯粹想帮 TiDB 改进文档的热心小伙伴，都欢迎来为 TiDB 文档做贡献，一起打造更加易用友好的 TiDB 文档！\n\n## 我能为 TiDB 文档做什么贡献？\n\n你可以在提升 TiDB 文档质量、易用性、维护效率、翻译效率等方面做贡献，比如：\n\n- [改进中文文档](#改进中文文档)\n- [翻译中文文档](#翻译中文文档)\n- 优化文档提交的流程、维护方式\n- 建立文档翻译记忆库、术语库\n\n下面主要介绍了如何改进和翻译中文文档。\n\n### 改进中文文档\n\n你可从以下任一方面入手：\n\n- 提 [Pull Request](https://docs.github.com/en/pull-requests/collaborating-with-pull-requests/proposing-changes-to-your-work-with-pull-requests/about-pull-requests) (PR) 更新过时内容\n- 提 PR 补充缺失内容\n- 提 PR 修正文档格式，如标点、空格、缩进、代码块等\n- 提 PR 改正错别字\n- 回复或解决 [issue](https://github.com/pingcap/docs-cn/issues?q=is%3Aopen+is%3Aissue) 并提 PR 更新相关文档\n- 其它改进\n\n### 翻译中文文档\n\nTiDB 中文文档的日常更新特别活跃，相应地，[TiDB 英文文档](https://docs.pingcap.com/tidb/dev/)也需要频繁更新。这一过程会涉及很多的**中译英**，即将 pingcap/docs-cn 仓库里已 merge 但尚未进行翻译处理的 [PR](https://github.com/pingcap/docs-cn/pulls?q=is%3Apr+label%3Atranslation%2Fwelcome+is%3Aclosed) 翻译为英文，并在 [pingcap/docs 仓库](https://github.com/pingcap/docs)中提交 Pull Request。\n\n> **注意：**\n>\n> - 绝大多数情况下，中英文档需要保持完全一致。但个别文档由于受众不同，可能会有差异。\n> - 通常，TiDB 文档先完成中文版后再完成英文版。但也偶有例外。\n> - [参考资料](#参考资料)一节中汇总了**中英术语表**和**风格指南**等参考文档，建议译前阅读。\n\n关于如何认领翻译任务的详细步骤，请参见[参考资料](#参考资料)。\n\n## 如何提 Pull Request？\n\n最常见的贡献方式就是提 Pull Request 了，那么提交流程是怎样的，又需要遵守哪些规范呢？下面的视频教程可以帮你快速上手 GitHub 的 Pull Request 流程：\n\n- [Git 与 GitHub 基础概念](https://www.bilibili.com/video/BV1h5411E7pM/?p=1)\n- [如何创建一个 Pull Request (PR)](https://www.bilibili.com/video/BV1h5411E7pM?p=2)\n- [跟进 PR 的后续操作](https://www.bilibili.com/video/BV1h5411E7pM?p=3)\n- [批量接受 Review 建议和处理 CI 检查](https://www.bilibili.com/video/BV1h5411E7pM?p=4)\n\n你也可以查阅 [docs-cn 仓库现有的 Pull Requests](https://github.com/pingcap/docs-cn/pulls) 作为参考。关于提 Pull Request 的详细步骤，请查阅[提交 Pull Request 的详细流程](#提交-pull-request-的详细流程)。\n\n## PR Checklist\n\n在合入 (Merge) PR 之前，请务必检查以下内容：\n\n- [ ] 文档内容准确、清晰、简洁，遵循写作规范。参考 [PingCAP 中文技术文档风格 — 极简指南](#pingcap-中文技术文档风格--极简指南)。\n- [ ] PR 的各元素完整、准确，包括：\n    - [ ] 标题清晰、有意义，包括修改的类型+文档所属的模块。参考 [Commit Message and Pull Request Style](https://github.com/pingcap/community/blob/master/contributors/commit-message-pr-style.md)。例如：\n        - Fix typos in tidb-monitoring-api.md\n        - benchmark: add the v5.3.0 benchmark document\n    - [ ] 有简要描述，例如修改背景等，并添加对应的 issue 号（如果有）\n    - [ ] 选择正确的 label\n    - [ ] cherry-pick 到适用且必要的分支版本\n- [ ] 如果新增文档、删除文档，需要同时更新 `TOC.md`，删除文档时需要在文件开头添加 `aliases` 确保旧链接能够正常跳转。\n- [ ] 预览文档，确保文档格式正确、清晰、可读，特别注意表格、图片、列表等特殊样式能够正常显示。\n\n## PingCAP 中文技术文档风格 — 极简指南\n\n![One Page Style Guide](/media/one-page-style-guide.png)\n\n参考文档：\n\n- [PingCAP 中文文档风格指南](/resources/pingcap-style-guide-zh.pdf)\n- [TiDB 中文用户文档模板](/resources/doc-templates)\n\n## 提交 Pull Request 的详细流程\n\nTiDB 文档的修改需要遵循一定的流程，具体如下。考虑到有些小伙伴是纯语言背景，命令行的流程掌握起来可能需要花些时间，之后我们也会提供更适合小白上手的 GitHub Desktop 客户端版提交流程（在添加至这里之前，可暂时参考 [lilin90](https://github.com/lilin90) 撰写的[小白上手流程](https://zhuanlan.zhihu.com/p/64880410)）。\n\n> **注意：**\n>\n> 目前（2023 年 12 月）TiDB 主要维护以下几个版本的文档：dev（最新开发版，对应文档仓库的 master 分支）、v7.5、v7.4、v7.3、v7.1、v6.5、v6.1、v6.0、v5.4、v5.3、v5.2、v5.1、v5.0。提 Pull Request 前请务必考虑修改会影响的文档版本，并据此修改所有相应的版本。选择版本时，请参考[参考资料](#参考资料)中的**如何选择文档适用的版本分支？**。\n\n### 第 0 步：签署 Contributor License Agreement\n\n首次在本仓库提 PR 时，请务必签署 [Contributor License Agreement](https://cla-assistant.io/pingcap/docs-cn) (CLA)，否则我们将无法合并你的 PR。成功签署 CLA 后，可继续进行后续操作。\n\n### 第 1 步：Fork pingcap/docs-cn 仓库\n\n1. 打开 pingcap/docs-cn 项目[仓库](https://help.github.com/articles/github-glossary/#repository)：<https://github.com/pingcap/docs-cn>\n2. 点击右上角的 [**Fork**](https://help.github.com/articles/github-glossary/#fork) 按钮，等待 Fork 完成即可。\n\n### 第 2 步：将 Fork 的仓库克隆至本地\n\n```\ncd $working_dir # 将 $working_dir 替换为你想放置 repo 的目录。例如，`cd ~/Documents/GitHub`\ngit clone git@github.com:$user/docs-cn.git # 将 `$user` 替换为你的 GitHub ID\n\ncd $working_dir/docs-cn\ngit remote add upstream git@github.com:pingcap/docs-cn.git # 添加上游仓库\ngit remote -v\n```\n\n### 第 3 步：新建一个 Branch\n\n1. 确保本地 master branch 与 upstream/master 保持最新。\n\n    ```\n    cd $working_dir/docs-cn\n    git fetch upstream\n    git checkout master\n    git rebase upstream/master\n    ```\n\n2. 基于 master branch 新建一个 branch，名称格式为：aaa-bbb-ccc。\n\n    ```\n    git checkout -b new-branch-name\n    ```\n\n### 第 4 步：编辑文档进行增删或修改\n\n在建好的 `new-branch-name` branch 上进行编辑，可使用 Markdown 编辑器（如 Visual Studio Code）打开 docs-cn repo，对相应文档进行增、删，或修改，并保存你的修改。\n\n### 第 5 步：提交你的修改\n\n```\ngit status\ngit add <file> ... # 如果你想提交所有的文档修改，可直接使用 `git add .`\ngit commit -m \"commit-message: update the xx\"\n```\n\n参考[如何写 commit message](https://github.com/pingcap/community/blob/master/contributors/commit-message-pr-style.md#how-to-write-a-good-commit-message)。\n\n### 第 6 步：保持新建 branch 与 upstream/master 一致\n\n```\n# 在新建 branch 上\ngit fetch upstream\ngit rebase upstream/master\n```\n\n### 第 7 步：将你的修改推至远程\n\n```\ngit push -u origin new-branch-name\n```\n\n### 第 8 步：创建一个 Pull Request\n\n1. 打开你 Fork 的仓库：`https://github.com/$user/docs-cn`（将其中的 `$user` 替换为你的 GitHub ID）\n2. 点击 `Compare & pull request` 按钮即可创建 PR。参考[如何写 PR title 和描述](https://github.com/pingcap/community/blob/master/contributors/commit-message-pr-style.md)。\n\n> **注意：**\n>\n> - 如果你的修改影响多个文档版本 (如 dev、v7.5、v7.4 等)，务必**在 PR 描述框中勾选相应的版本**，后续仓库管理员会为你的 PR 打上相应的 cherry-pick label。\n\n## 预览 EBNF 格式的 SQL 语法图\n\n[TiDB 文档](https://docs.pingcap.com/zh/tidb/stable)提供了大量 SQL 语法图，以帮助用户理解 SQL 语法。例如，[`ALTER INDEX` 文档](https://docs.pingcap.com/zh/tidb/stable/sql-statement-alter-index#语法图)中的语法图。\n\n这些语法图的源代码是使用[扩展巴科斯范式 (EBNF)](https://zh.wikipedia.org/wiki/扩展巴科斯范式) 编写的。在为 SQL 语句添加 EBNF 代码时，可以将代码复制到 <https://kennytm.github.io/website-docs/dist/> 并点击 **Render**，即可轻松预览 EBNF 效果图。\n\n## 参考资料\n\n<details>\n<summary>如何认领中文翻译任务？</summary>\n\n目前，中文文档翻译任务以 [docs-cn 仓库的 Pull Request](https://github.com/pingcap/docs-cn/pulls) (PR) 为形式，通过仓库管理员为 PR 加上的 labels 来认领翻译任务及追踪翻译任务状态。\n\n你可以通过以下简单几步来认领并提交一个 PR 翻译任务：\n\n> **注意：**\n>\n> 关于下面步骤中所提到的 comment 式命令，详细说明请参考[参考资料](#参考资料)中的**常用 bot 命令**。\n\n1. 查看待认领 PR\n\n    打开 [pingcap/docs-cn PR 翻译任务页面](https://github.com/pingcap/docs-cn/pulls?q=is%3Apr+label%3Atranslation%2Fwelcome+-label%3Atranslation%2Fdone+)，即可看到所有打上了 `translation/welcome` label 的 PR。这类 PR 只要没有 `translation/done` 的 label，无论是处于 open 还是 closed 状态，均可认领。\n\n2. 认领 PR\n\n    打开你想认领的 PR，拉到底部留下这条 comment：`/assign`，即可将此 PR 的翻译任务分配给自己。\n\n3. 修改 PR label\n\n    PR 认领成功后，继续在底部 comment 区域依次发送：`/remove-translation welcome` 及 `/translation doing`，即可将右侧 Labels 栏中的 `translation/welcome` 改为 `translation/doing`，之后你便可以开始翻译了。\n\n4. 翻译 PR 并提交\n\n    由于 TiDB 的中英文文档分别存放于 [pingcap/docs-cn](https://github.com/pingcap/docs-cn) 和 [pingcap/docs](https://github.com/pingcap/docs) 中，并且两个仓库的文件结构完全对应。如果你是首次认领翻译任务，需先 fork docs 仓库，并将 fork 的 docs 仓库克隆到本地，然后找到源 PR 中对应的改动文件再开始翻译。翻译完毕后，创建新 PR，将翻译好的文件提交至 docs 仓库。\n\n5. 填写 PR 描述并修改 label\n\n    新建 PR 成功后，先按照模板说明完整填写 PR 描述，接着在底部发送：`/translation from-docs-cn`，为 PR 添加 `translation/from-docs-cn` label，表明此 PR 是从中文翻译过来的。然后回到源 PR 依次发送：`/remove-translation doing` 及 `/translation done`，将源 PR label 修改为 `translation/done`，表明翻译已完成。\n\n6. 分配 Reviewer（推荐，非必需）\n\n    每个 PR 都需要经过 Review 后才能合并，分配 Reviewer 一般由文档仓库管理员负责，但我们也十分欢迎你来主动承担这个任务。\n\n    具体操作为：在新建的 PR 下发送 `/cc @lilin90 @technical-reviewer`（将 technical-reviewer 替换为源 PR 作者的 GitHub ID），即可将 Review 任务分配给 docs 仓库管理员 @lilin90 及源 PR 的作者。\n\n</details>\n\n<details>\n<summary>如何选择文档适用的版本分支？</summary>\n\n创建 Pull Request 时，你需要在 Pull Request 的描述模版中选择文档改动适用的版本分支。\n\n如果你的 PR 改动符合以下任一情况，推荐**只选择 master 分支**。此 PR 的改动在合并后将显示到[官网文档 Dev 页面](https://docs.pingcap.com/zh/tidb/dev/)，在下一次 TiDB 发新版本时将显示到新版本的文档页面。\n\n- 完善文档，例如补充缺失或不完整的信息。\n- 改正错误，例如默认值错误、描述不准确、示例错误、拼写错误等。\n- 重构文档，例如“部署标准集群”、“数据迁移”、“TiDB 数据迁移工具”等。\n\n如果你的 PR 改动符合以下任一情况，请**选择 master 分支以及受影响的 release 分支**：\n\n- 涉及与版本相关的功能行为变化。\n- 涉及与版本相关的兼容性变化，例如更改某个配置项或变量的默认值。\n- 修复文档页面的渲染或显示错误。\n- 修复文档内的死链。\n\n</details>\n\n<details>\n<summary>Markdown 规范</summary>\n\nTiDB 中文文档使用 Markdown 语言进行编写，为了保证文档质量和格式规范，你修改的文档需要遵循一定的 Markdown 规则。我们为 docs-cn 仓库设置了检测 markdown 文件规范的 CI check，即 [markdownlint check](https://github.com/DavidAnson/markdownlint/blob/master/doc/Rules.md)。如果你提交的 PR 不符合规范，很可能**无法通过 markdownlint check**，最终导致无法合并 PR。\n\n我们为 TiDB 中文文档提前设置了 25 条 [markdownlint 规则](/resources/markdownlint-rules.md)，并附上了简单易懂的解释，强烈推荐花 5 分钟通读一遍。\n\n假如你提 PR 之前没有熟悉相关 Markdown 规范，提 PR 时遇到了 markdownlint check 失败，也不必担心，报错信息中有错误详情、出错的文件和位置，帮你快速定位和解决问题。\n\n此外，你还可以选择在本地进行 markdownlint check：\n\n```bash\n./scripts/markdownlint [FILE...]\n```\n\n</details>\n\n<details>\n<summary>常用 bot 命令</summary>\n\n我们为 docs 和 docs-cn 仓库提前设置了一些命令语句，只要按照一定的格式在 PR 中留言，就能触发 bot 完成相应操作。详情见下表。\n\n| 命令 | 含义 | 示例 |\n| ------ | ------ | ------ |\n| `/label` | 给 PR 添加 label，多个 label 间需要用逗号分隔。如果 label 中有斜线 `/`，则命令为 `/[label 的第一个单词] [label 其他部分]` | `/label contribution`，`/translation from-docs` |\n| `/remove-label` | 删除 PR label。如果 label 中有斜线 `/`，则命令为 `/remove-[label 的第一个单词] [label 其他部分]` | `/remove-label contribution`，`/remove-translation welcome` |\n| `/assign` | 将 PR 分配给指定的人，需 @指定用户的 GitHub ID，多个 GitHub ID 间用逗号分隔。如果想要将 PR 分配给自己，`/assign`后可不跟 GitHub ID。 | `/assign @lilin90` |\n| `/unassign` | 移除 PR 之前指定的 assignee。 | `/unassign @lilin90` |\n| `/cc` | 将 PR 分配给指定的 reviewer，需 @指定用户的 GitHub ID，多个 GitHub ID 间用逗号分隔。 | `/cc @lilin90, @hfxsd` |\n| `/uncc` | 移除 PR 之前指定的 reviewer。  | `/uncc @lilin90`|\n\n</details>\n\n其他快速上手资源\n\n- [TiDB 中英术语表](/resources/tidb-terms.md)\n- [GitHub Docs](https://docs.github.com/en)\n- [Pull Request Commit Message 规范](https://github.com/pingcap/community/blob/master/contributors/commit-message-pr-style.md#how-to-write-a-good-commit-message)\n- [Pull Request 标题规范](https://github.com/pingcap/community/blob/master/contributors/commit-message-pr-style.md#pull-request-title-style)\n- [代码注释规范](https://github.com/pingcap/community/blob/master/contributors/code-comment-style.md)\n- [Figma 快速上手教程](/resources/figma-quick-start-guide.md)\n- [EBNF 语法图在线渲染](https://kennytm.github.io/website-docs/dist/)\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 21.6806640625,
          "content": "Attribution-ShareAlike 3.0 Unported\n\n=======================================================================\n\nCREATIVE COMMONS CORPORATION IS NOT A LAW FIRM AND DOES NOT PROVIDE\nLEGAL SERVICES. DISTRIBUTION OF THIS LICENSE DOES NOT CREATE AN\nATTORNEY-CLIENT RELATIONSHIP. CREATIVE COMMONS PROVIDES THIS\nINFORMATION ON AN \"AS-IS\" BASIS. CREATIVE COMMONS MAKES NO WARRANTIES\nREGARDING THE INFORMATION PROVIDED, AND DISCLAIMS LIABILITY FOR DAMAGES\nRESULTING FROM ITS USE.\n\nLicense\n\nTHE WORK (AS DEFINED BELOW) IS PROVIDED UNDER THE TERMS OF THIS\nCREATIVE COMMONS PUBLIC LICENSE (\"CCPL\" OR \"LICENSE\"). THE WORK IS\nPROTECTED BY COPYRIGHT AND/OR OTHER APPLICABLE LAW. ANY USE OF THE WORK\nOTHER THAN AS AUTHORIZED UNDER THIS LICENSE OR COPYRIGHT LAW IS\nPROHIBITED.\n\nBY EXERCISING ANY RIGHTS TO THE WORK PROVIDED HERE, YOU ACCEPT AND\nAGREE TO BE BOUND BY THE TERMS OF THIS LICENSE. TO THE EXTENT THIS\nLICENSE MAY BE CONSIDERED TO BE A CONTRACT, THE LICENSOR GRANTS YOU THE\nRIGHTS CONTAINED HERE IN CONSIDERATION OF YOUR ACCEPTANCE OF SUCH TERMS\nAND CONDITIONS.\n\n1. Definitions\n\n   a. \"Adaptation\" means a work based upon the Work, or upon the Work\n   and other pre-existing works, such as a translation, adaptation,\n   derivative work, arrangement of music or other alterations of a\n   literary or artistic work, or phonogram or performance and includes\n   cinematographic adaptations or any other form in which the Work may\n   be recast, transformed, or adapted including in any form\n   recognizably derived from the original, except that a work that\n   constitutes a Collection will not be considered an Adaptation for\n   the purpose of this License. For the avoidance of doubt, where the\n   Work is a musical work, performance or phonogram, the\n   synchronization of the Work in timed-relation with a moving image\n   (\"synching\") will be considered an Adaptation for the purpose of\n   this License.\n\n   b. \"Collection\" means a collection of literary or artistic works,\n   such as encyclopedias and anthologies, or performances, phonograms\n   or broadcasts, or other works or subject matter other than works\n   listed in Section 1(f) below, which, by reason of the selection and\n   arrangement of their contents, constitute intellectual creations,\n   in which the Work is included in its entirety in unmodified form\n   along with one or more other contributions, each constituting\n   separate and independent works in themselves, which together are\n   assembled into a collective whole. A work that constitutes a\n   Collection will not be considered an Adaptation (as defined below)\n   for the purposes of this License.\n\n   c. \"Creative Commons Compatible License\" means a license that is\n   listed at https://creativecommons.org/compatiblelicenses that has\n   been approved by Creative Commons as being essentially equivalent\n   to this License, including, at a minimum, because that license: (i)\n   contains terms that have the same purpose, meaning and effect as\n   the License Elements of this License; and, (ii) explicitly permits\n   the relicensing of adaptations of works made available under that\n   license under this License or a Creative Commons jurisdiction\n   license with the same License Elements as this License.\n\n   d. \"Distribute\" means to make available to the public the original\n   and copies of the Work or Adaptation, as appropriate, through sale\n   or other transfer of ownership.\n\n   e. \"License Elements\" means the following high-level license\n   attributes as selected by Licensor and indicated in the title of\n   this License: Attribution, ShareAlike.\n\n   f. \"Licensor\" means the individual, individuals, entity or entities\n   that offer(s) the Work under the terms of this License.\n\n   g. \"Original Author\" means, in the case of a literary or artistic\n   work, the individual, individuals, entity or entities who created\n   the Work or if no individual or entity can be identified, the\n   publisher; and in addition (i) in the case of a performance the\n   actors, singers, musicians, dancers, and other persons who act,\n   sing, deliver, declaim, play in, interpret or otherwise perform\n   literary or artistic works or expressions of folklore; (ii) in the\n   case of a phonogram the producer being the person or legal entity\n   who first fixes the sounds of a performance or other sounds; and,\n   (iii) in the case of broadcasts, the organization that transmits\n   the broadcast.\n\n   h. \"Work\" means the literary and/or artistic work offered under the\n   terms of this License including without limitation any production\n   in the literary, scientific and artistic domain, whatever may be\n   the mode or form of its expression including digital form, such as\n   a book, pamphlet and other writing; a lecture, address, sermon or\n   other work of the same nature; a dramatic or dramatico-musical\n   work; a choreographic work or entertainment in dumb show; a musical\n   composition with or without words; a cinematographic work to which\n   are assimilated works expressed by a process analogous to\n   cinematography; a work of drawing, painting, architecture,\n   sculpture, engraving or lithography; a photographic work to which\n   are assimilated works expressed by a process analogous to\n   photography; a work of applied art; an illustration, map, plan,\n   sketch or three-dimensional work relative to geography, topography,\n   architecture or science; a performance; a broadcast; a phonogram; a\n   compilation of data to the extent it is protected as a\n   copyrightable work; or a work performed by a variety or circus\n   performer to the extent it is not otherwise considered a literary\n   or artistic work.\n\n   i. \"You\" means an individual or entity exercising rights under this\n   License who has not previously violated the terms of this License\n   with respect to the Work, or who has received express permission\n   from the Licensor to exercise rights under this License despite a\n   previous violation.\n\n   j. \"Publicly Perform\" means to perform public recitations of the\n   Work and to communicate to the public those public recitations, by\n   any means or process, including by wire or wireless means or public\n   digital performances; to make available to the public Works in such\n   a way that members of the public may access these Works from a\n   place and at a place individually chosen by them; to perform the\n   Work to the public by any means or process and the communication to\n   the public of the performances of the Work, including by public\n   digital performance; to broadcast and rebroadcast the Work by any\n   means including signs, sounds or images.\n   k. \"Reproduce\" means to make copies of the Work by any means\n   including without limitation by sound or visual recordings and the\n   right of fixation and reproducing fixations of the Work, including\n   storage of a protected performance or phonogram in digital form or\n   other electronic medium.\n\n2. Fair Dealing Rights. Nothing in this License is intended to reduce,\nlimit, or restrict any uses free from copyright or rights arising from\nlimitations or exceptions that are provided for in connection with the\ncopyright protection under copyright law or other applicable laws.\n\n3. License Grant. Subject to the terms and conditions of this License,\nLicensor hereby grants You a worldwide, royalty-free, non-exclusive,\nperpetual (for the duration of the applicable copyright) license to\nexercise the rights in the Work as stated below:\n\n   a. to Reproduce the Work, to incorporate the Work into one or more\n   Collections, and to Reproduce the Work as incorporated in the\n   Collections;\n\n   b. to create and Reproduce Adaptations provided that any such\n   Adaptation, including any translation in any medium, takes\n   reasonable steps to clearly label, demarcate or otherwise identify\n   that changes were made to the original Work. For example, a\n   translation could be marked \"The original work was translated from\n   English to Spanish,\" or a modification could indicate \"The original\n   work has been modified.\";\n\n   c. to Distribute and Publicly Perform the Work including as\n   incorporated in Collections; and,\n\n   d. to Distribute and Publicly Perform Adaptations.\n\n   e. For the avoidance of doubt:\n\n      i. Non-waivable Compulsory License Schemes. In those\n      jurisdictions in which the right to collect royalties through\n      any statutory or compulsory licensing scheme cannot be waived,\n      the Licensor reserves the exclusive right to collect such\n      royalties for any exercise by You of the rights granted under\n      this License;\n\n      ii. Waivable Compulsory License Schemes. In those jurisdictions\n      in which the right to collect royalties through any statutory or\n      compulsory licensing scheme can be waived, the Licensor waives\n      the exclusive right to collect such royalties for any exercise\n      by You of the rights granted under this License; and,\n\n      iii. Voluntary License Schemes. The Licensor waives the right to\n      collect royalties, whether individually or, in the event that\n      the Licensor is a member of a collecting society that\n      administers voluntary licensing schemes, via that society, from\n      any exercise by You of the rights granted under this License.\n\n   The above rights may be exercised in all media and formats whether\n   now known or hereafter devised. The above rights include the right\n   to make such modifications as are technically necessary to exercise\n   the rights in other media and formats. Subject to Section 8(f), all\n   rights not expressly granted by Licensor are hereby reserved.\n\n4. Restrictions. The license granted in Section 3 above is expressly\nmade subject to and limited by the following restrictions:\n\n   a. You may Distribute or Publicly Perform the Work only under the\n   terms of this License. You must include a copy of, or the Uniform\n   Resource Identifier (URI) for, this License with every copy of the\n   Work You Distribute or Publicly Perform. You may not offer or\n   impose any terms on the Work that restrict the terms of this\n   License or the ability of the recipient of the Work to exercise the\n   rights granted to that recipient under the terms of the License.\n   You may not sublicense the Work. You must keep intact all notices\n   that refer to this License and to the disclaimer of warranties with\n   every copy of the Work You Distribute or Publicly Perform. When You\n   Distribute or Publicly Perform the Work, You may not impose any\n   effective technological measures on the Work that restrict the\n   ability of a recipient of the Work from You to exercise the rights\n   granted to that recipient under the terms of the License. This\n   Section 4(a) applies to the Work as incorporated in a Collection,\n   but this does not require the Collection apart from the Work itself\n   to be made subject to the terms of this License. If You create a\n   Collection, upon notice from any Licensor You must, to the extent\n   practicable, remove from the Collection any credit as required by\n   Section 4(c), as requested. If You create an Adaptation, upon\n   notice from any Licensor You must, to the extent practicable,\n   remove from the Adaptation any credit as required by Section 4(c),\n   as requested.\n\n   b. You may Distribute or Publicly Perform an Adaptation only under\n   the terms of: (i) this License; (ii) a later version of this\n   License with the same License Elements as this License; (iii) a\n   Creative Commons jurisdiction license (either this or a later\n   license version) that contains the same License Elements as this\n   License (e.g., Attribution-ShareAlike 3.0 US)); (iv) a Creative\n   Commons Compatible License. If you license the Adaptation under one\n   of the licenses mentioned in (iv), you must comply with the terms\n   of that license. If you license the Adaptation under the terms of\n   any of the licenses mentioned in (i), (ii) or (iii) (the\n   \"Applicable License\"), you must comply with the terms of the\n   Applicable License generally and the following provisions: (I) You\n   must include a copy of, or the URI for, the Applicable License with\n   every copy of each Adaptation You Distribute or Publicly Perform;\n   (II) You may not offer or impose any terms on the Adaptation that\n   restrict the terms of the Applicable License or the ability of the\n   recipient of the Adaptation to exercise the rights granted to that\n   recipient under the terms of the Applicable License; (III) You must\n   keep intact all notices that refer to the Applicable License and to\n   the disclaimer of warranties with every copy of the Work as\n   included in the Adaptation You Distribute or Publicly Perform; (IV)\n   when You Distribute or Publicly Perform the Adaptation, You may not\n   impose any effective technological measures on the Adaptation that\n   restrict the ability of a recipient of the Adaptation from You to\n   exercise the rights granted to that recipient under the terms of\n   the Applicable License. This Section 4(b) applies to the Adaptation\n   as incorporated in a Collection, but this does not require the\n   Collection apart from the Adaptation itself to be made subject to\n   the terms of the Applicable License.\n\n   c. If You Distribute, or Publicly Perform the Work or any\n   Adaptations or Collections, You must, unless a request has been\n   made pursuant to Section 4(a), keep intact all copyright notices\n   for the Work and provide, reasonable to the medium or means You are\n   utilizing: (i) the name of the Original Author (or pseudonym, if\n   applicable) if supplied, and/or if the Original Author and/or\n   Licensor designate another party or parties (e.g., a sponsor\n   institute, publishing entity, journal) for attribution\n   (\"Attribution Parties\") in Licensor's copyright notice, terms of\n   service or by other reasonable means, the name of such party or\n   parties; (ii) the title of the Work if supplied; (iii) to the\n   extent reasonably practicable, the URI, if any, that Licensor\n   specifies to be associated with the Work, unless such URI does not\n   refer to the copyright notice or licensing information for the\n   Work; and (iv) , consistent with Ssection 3(b), in the case of an\n   Adaptation, a credit identifying the use of the Work in the\n   Adaptation (e.g., \"French translation of the Work by Original\n   Author,\" or \"Screenplay based on original Work by Original\n   Author\"). The credit required by this Section 4(c) may be\n   implemented in any reasonable manner; provided, however, that in\n   the case of a Adaptation or Collection, at a minimum such credit\n   will appear, if a credit for all contributing authors of the\n   Adaptation or Collection appears, then as part of these credits and\n   in a manner at least as prominent as the credits for the other\n   contributing authors. For the avoidance of doubt, You may only use\n   the credit required by this Section for the purpose of attribution\n   in the manner set out above and, by exercising Your rights under\n   this License, You may not implicitly or explicitly assert or imply\n   any connection with, sponsorship or endorsement by the Original\n   Author, Licensor and/or Attribution Parties, as appropriate, of You\n   or Your use of the Work, without the separate, express prior\n   written permission of the Original Author, Licensor and/or\n   Attribution Parties.\n\n   d. Except as otherwise agreed in writing by the Licensor or as may\n   be otherwise permitted by applicable law, if You Reproduce,\n   Distribute or Publicly Perform the Work either by itself or as part\n   of any Adaptations or Collections, You must not distort, mutilate,\n   modify or take other derogatory action in relation to the Work\n   which would be prejudicial to the Original Author's honor or\n   reputation. Licensor agrees that in those jurisdictions (e.g.\n   Japan), in which any exercise of the right granted in Section 3(b)\n   of this License (the right to make Adaptations) would be deemed to\n   be a distortion, mutilation, modification or other derogatory\n   action prejudicial to the Original Author's honor and reputation,\n   the Licensor will waive or not assert, as appropriate, this\n   Section, to the fullest extent permitted by the applicable national\n   law, to enable You to reasonably exercise Your right under Section\n   3(b) of this License (right to make Adaptations) but not otherwise.\n\n5. Representations, Warranties and Disclaimer\n\n   UNLESS OTHERWISE MUTUALLY AGREED TO BY THE PARTIES IN WRITING,\n   LICENSOR OFFERS THE WORK AS-IS AND MAKES NO REPRESENTATIONS OR\n   WARRANTIES OF ANY KIND CONCERNING THE WORK, EXPRESS, IMPLIED,\n   STATUTORY OR OTHERWISE, INCLUDING, WITHOUT LIMITATION, WARRANTIES\n   OF TITLE, MERCHANTIBILITY, FITNESS FOR A PARTICULAR PURPOSE,\n   NONINFRINGEMENT, OR THE ABSENCE OF LATENT OR OTHER DEFECTS,\n   ACCURACY, OR THE PRESENCE OF ABSENCE OF ERRORS, WHETHER OR NOT\n   DISCOVERABLE. SOME JURISDICTIONS DO NOT ALLOW THE EXCLUSION OF\n   IMPLIED WARRANTIES, SO SUCH EXCLUSION MAY NOT APPLY TO YOU.\n\n6. Limitation on Liability.\n\n   EXCEPT TO THE EXTENT REQUIRED BY APPLICABLE LAW, IN NO EVENT WILL\n   LICENSOR BE LIABLE TO YOU ON ANY LEGAL THEORY FOR ANY SPECIAL,\n   INCIDENTAL, CONSEQUENTIAL, PUNITIVE OR EXEMPLARY DAMAGES ARISING\n   OUT OF THIS LICENSE OR THE USE OF THE WORK, EVEN IF LICENSOR HAS\n   BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\n7. Termination\n\n   a. This License and the rights granted hereunder will terminate\n   automatically upon any breach by You of the terms of this License.\n   Individuals or entities who have received Adaptations or\n   Collections from You under this License, however, will not have\n   their licenses terminated provided such individuals or entities\n   remain in full compliance with those licenses. Sections 1, 2, 5, 6,\n   7, and 8 will survive any termination of this License.\n\n   b. Subject to the above terms and conditions, the license granted\n   here is perpetual (for the duration of the applicable copyright in\n   the Work). Notwithstanding the above, Licensor reserves the right\n   to release the Work under different license terms or to stop\n   distributing the Work at any time; provided, however that any such\n   election will not serve to withdraw this License (or any other\n   license that has been, or is required to be, granted under the\n   terms of this License), and this License will continue in full\n   force and effect unless terminated as stated above.\n\n8. Miscellaneous\n\n   a. Each time You Distribute or Publicly Perform the Work or a\n   Collection, the Licensor offers to the recipient a license to the\n   Work on the same terms and conditions as the license granted to You\n   under this License.\n\n   b. Each time You Distribute or Publicly Perform an Adaptation,\n   Licensor offers to the recipient a license to the original Work on\n   the same terms and conditions as the license granted to You under\n   this License.\n\n   c. If any provision of this License is invalid or unenforceable\n   under applicable law, it shall not affect the validity or\n   enforceability of the remainder of the terms of this License, and\n   without further action by the parties to this agreement, such\n   provision shall be reformed to the minimum extent necessary to make\n   such provision valid and enforceable.\n\n   d. No term or provision of this License shall be deemed waived and\n   no breach consented to unless such waiver or consent shall be in\n   writing and signed by the party to be charged with such waiver or\n   consent.\n\n   e. This License constitutes the entire agreement between the\n   parties with respect to the Work licensed here. There are no\n   understandings, agreements or representations with respect to the\n   Work not specified here. Licensor shall not be bound by any\n   additional provisions that may appear in any communication from\n   You. This License may not be modified without the mutual written\n   agreement of the Licensor and You.\n\n   f. The rights granted under, and the subject matter referenced, in\n   this License were drafted utilizing the terminology of the Berne\n   Convention for the Protection of Literary and Artistic Works (as\n   amended on September 28, 1979), the Rome Convention of 1961, the\n   WIPO Copyright Treaty of 1996, the WIPO Performances and Phonograms\n   Treaty of 1996 and the Universal Copyright Convention (as revised\n   on July 24, 1971). These rights and subject matter take effect in\n   the relevant jurisdiction in which the License terms are sought to\n   be enforced according to the corresponding provisions of the\n   implementation of those treaty provisions in the applicable\n   national law. If the standard suite of rights granted under\n   applicable copyright law includes additional rights not granted\n   under this License, such additional rights are deemed to be\n   included in the License; this License is not intended to restrict\n   the license of any rights under applicable law.\n\n=======================================================================\n\nCreative Commons Notice\n\nCreative Commons is not a party to this License, and makes no warranty\nwhatsoever in connection with the Work. Creative Commons will not be\nliable to You or any party on any legal theory for any damages\nwhatsoever, including without limitation any general, special,\nincidental or consequential damages arising in connection to this\nlicense. Notwithstanding the foregoing two (2) sentences, if Creative\nCommons has expressly identified itself as the Licensor hereunder, it\nshall have all rights and obligations of Licensor.\n\nExcept for the limited purpose of indicating to the public that the\nWork is licensed under the CCPL, Creative Commons does not authorize\nthe use by either party of the trademark \"Creative Commons\" or any\nrelated trademark or logo of Creative Commons without the prior written\nconsent of Creative Commons. Any permitted use will be in compliance\nwith Creative Commons' then-current trademark usage guidelines, as may\nbe published on its website or otherwise made available upon request\nfrom time to time. For the avoidance of doubt, this trademark\nrestriction does not form part of the License.\n\nCreative Commons may be contacted at https://creativecommons.org/.\n"
        },
        {
          "name": "OWNERS",
          "type": "blob",
          "size": 0.4775390625,
          "content": "# See the OWNERS docs at https://go.k8s.io/owners\napprovers:\n  - breezewish\n  - csuzhangxc\n  - hfxsd\n  - Icemap\n  - jackysp\n  - kissmydb\n  - lance6716\n  - lilin90\n  - Oreoxmt\n  - overvenus\n  - qiancai\n  - tangenta\nreviewers:\n  - 3pointer\n  - amyangfei\n  - anotherrachel\n  - aylei\n  - crazycs520\n  - dveeden\n  - ericsyh\n  - glkappe\n  - GMHDBJD\n  - Joyinqin\n  - junlan-zhang\n  - KanShiori\n  - lucklove\n  - lysu\n  - ngaut\n  - superlzs0476\n  - tiancaiamao\n  - weekface\n  - Yisaer\n  - zimulala\n"
        },
        {
          "name": "OWNERS_ALIASES",
          "type": "blob",
          "size": 0.23046875,
          "content": "aliases:\n  sig-develop-docs-approvers:\n    - Oreoxmt\n    - qiancai\n    - ran-huang\n  sig-develop-docs-reviewers:\n    - Icemap\n    - sykp241095\n    - shizn\n    - winkyao\n    - shczhen\n    - hooopo\n    - Mini256\n    - wd0517\n    - it2911\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.2177734375,
          "content": "# TiDB 文档\n\n欢迎来到 [TiDB](https://github.com/pingcap/tidb) 文档仓库！\n\n这里存放的是 [PingCAP 官网 TiDB 中文文档](https://docs.pingcap.com/zh/tidb/stable)的源文件。[官网英文文档](https://docs.pingcap.com/tidb/stable)的源文件则存放于 [pingcap/docs](https://github.com/pingcap/docs)。\n\n如果你发现或遇到了 TiDB 的文档问题，可随时[提 Issue](https://github.com/pingcap/docs-cn/issues/new/choose) 来反馈，或者直接[提交 Pull Request](/CONTRIBUTING.md#如何提-pull-request) 来进行修改。\n\n如果你想在本地定制输出符合特定场景需求的 PDF 格式的 TiDB 文档，例如对 TiDB 文档目录进行自由排序和删减，请参考[自助生成 TiDB 文档 PDF 教程](/resources/tidb-pdf-generation-tutorial.md)。\n\n## TiDB 文档维护方式及版本说明\n\n目前，TiDB 的文档维护在以下 branch，对应着官网文档的不同版本：\n\n| 文档仓库 branch | 对应 TiDB 文档版本 |\n|:---------|:----------|\n| [`master`](https://github.com/pingcap/docs-cn/tree/master) | dev 最新开发版 |\n| [`release-8.5`](https://github.com/pingcap/docs-cn/tree/release-8.5) | 8.5 长期支持版 (LTS) |\n| [`release-8.4`](https://github.com/pingcap/docs-cn/tree/release-8.4) | 8.4 开发里程碑版 (DMR) |\n| [`release-8.3`](https://github.com/pingcap/docs-cn/tree/release-8.3) | 8.3 开发里程碑版 (DMR) |\n| [`release-8.2`](https://github.com/pingcap/docs-cn/tree/release-8.2) | 8.2 开发里程碑版 (DMR) （该版本文档已归档，不再提供任何更新） |\n| [`release-8.1`](https://github.com/pingcap/docs-cn/tree/release-8.1) | 8.1 长期支持版 (LTS) |\n| [`release-8.0`](https://github.com/pingcap/docs-cn/tree/release-8.0) | 8.0 开发里程碑版 (DMR) （该版本文档已归档，不再提供任何更新）|\n| [`release-7.6`](https://github.com/pingcap/docs-cn/tree/release-7.6) | 7.6 开发里程碑版 (DMR) （该版本文档已归档，不再提供任何更新）|\n| [`release-7.5`](https://github.com/pingcap/docs-cn/tree/release-7.5) | 7.5 长期支持版 (LTS) |\n| [`release-7.4`](https://github.com/pingcap/docs-cn/tree/release-7.4) | 7.4 开发里程碑版 (DMR) （该版本文档已归档，不再提供任何更新）|\n| [`release-7.3`](https://github.com/pingcap/docs-cn/tree/release-7.3) | 7.3 开发里程碑版 (DMR) （该版本文档已归档，不再提供任何更新）|\n| [`release-7.2`](https://github.com/pingcap/docs-cn/tree/release-7.2) | 7.2 开发里程碑版 (DMR)（该版本文档已归档，不再提供任何更新） |\n| [`release-7.1`](https://github.com/pingcap/docs-cn/tree/release-7.1) | 7.1 长期支持版 (LTS) |\n| [`release-7.0`](https://github.com/pingcap/docs-cn/tree/release-7.0) | 7.0 开发里程碑版 (DMR)（该版本文档已归档，不再提供任何更新） |\n| [`release-6.6`](https://github.com/pingcap/docs-cn/tree/release-6.6) | 6.6 开发里程碑版 (DMR)（该版本文档已归档，不再提供任何更新） |\n| [`release-6.5`](https://github.com/pingcap/docs-cn/tree/release-6.5) | 6.5 长期支持版 (LTS) |\n| [`release-6.4`](https://github.com/pingcap/docs-cn/tree/release-6.4) | 6.4 开发里程碑版 (DMR)（该版本文档已归档，不再提供任何更新） |\n| [`release-6.3`](https://github.com/pingcap/docs-cn/tree/release-6.3) | 6.3 开发里程碑版 (DMR)（该版本文档已归档，不再提供任何更新） |\n| [`release-6.2`](https://github.com/pingcap/docs-cn/tree/release-6.2) | 6.2 开发里程碑版 (DMR)（该版本文档已归档，不再提供任何更新） |\n| [`release-6.1`](https://github.com/pingcap/docs-cn/tree/release-6.1) | 6.1 长期支持版 (LTS) |\n| [`release-6.0`](https://github.com/pingcap/docs-cn/tree/release-6.0) | 6.0 开发里程碑版 (DMR)（该版本文档已归档，不再提供任何更新） |\n| [`release-5.4`](https://github.com/pingcap/docs-cn/tree/release-5.4) | 5.4 稳定版 |\n| [`release-5.3`](https://github.com/pingcap/docs-cn/tree/release-5.3) | 5.3 稳定版 （该版本文档已归档，不再提供任何更新） |\n| [`release-5.2`](https://github.com/pingcap/docs-cn/tree/release-5.2) | 5.2 稳定版 （该版本文档已归档，不再提供任何更新）|\n| [`release-5.1`](https://github.com/pingcap/docs-cn/tree/release-5.1) | 5.1 稳定版 （该版本文档已归档，不再提供任何更新）|\n| [`release-5.0`](https://github.com/pingcap/docs-cn/tree/release-5.0) | 5.0 稳定版（该版本文档已归档，不再提供任何更新） |\n| [`release-4.0`](https://github.com/pingcap/docs-cn/tree/release-4.0) | 4.0 稳定版（该版本文档已归档，不再提供任何更新） |\n| [`release-3.1`](https://github.com/pingcap/docs-cn/tree/release-3.1) | 3.1 稳定版（该版本文档已归档，不再提供任何更新） |\n| [`release-3.0`](https://github.com/pingcap/docs-cn/tree/release-3.0) | 3.0 稳定版（该版本文档已归档，不再提供任何更新） |\n| [`release-2.1`](https://github.com/pingcap/docs-cn/tree/release-2.1) | 2.1 稳定版（该版本文档已归档，不再提供任何更新） |\n\n## 贡献文档\n\n[<img src=\"media/contribution-map.png\" alt=\"contribution-map\" width=\"180\"></img>](https://github.com/pingcap/docs-cn/blob/master/credits.md)\n\n你提交的第一个 [Pull Request](https://help.github.com/en/github/getting-started-with-github/github-glossary#pull-request) (PR) 合并以后，即可成为 TiDB 文档的 Contributor。查看 [TiDB 中文文档贡献指南](/CONTRIBUTING.md)，开始你的贡献吧！\n\n<a href=\"https://next.ossinsight.io/widgets/official/compose-recent-active-contributors?repo_id=64188788&limit=30\" target=\"_blank\" style=\"display: block;\" align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://next.ossinsight.io/widgets/official/compose-recent-active-contributors/thumbnail.png?repo_id=64188788&limit=30&image_size=auto&color_scheme=dark\" width=\"655\" height=\"auto\" />\n    <img alt=\"Active Contributors of pingcap/docs-cn - Last 28 days\" src=\"https://next.ossinsight.io/widgets/official/compose-recent-active-contributors/thumbnail.png?repo_id=64188788&limit=30&image_size=auto&color_scheme=light\" width=\"655\" height=\"auto\" />\n  </picture>\n</a>\n\n## License\n\n自 TiDB v7.0 起，所有文档的许可证均为 [CC BY-SA 3.0](https://creativecommons.org/licenses/by-sa/3.0/)。\n"
        },
        {
          "name": "TOC.md",
          "type": "blob",
          "size": 73.3037109375,
          "content": "<!-- markdownlint-disable MD007 -->\n<!-- markdownlint-disable MD041 -->\n\n- [文档中心](https://docs.pingcap.com/zh)\n- 关于 TiDB\n  - [TiDB 简介](/overview.md)\n  - [TiDB 8.5 Release Notes](/releases/release-8.5.0.md)\n  - [功能概览](/basic-features.md)\n  - [与 MySQL 的兼容性](/mysql-compatibility.md)\n  - [使用限制](/tidb-limitations.md)\n  - [荣誉列表](/credits.md)\n  - [路线图](/tidb-roadmap.md)\n- 快速上手\n  - [快速上手 TiDB](/quick-start-with-tidb.md)\n  - [快速上手 HTAP](/quick-start-with-htap.md)\n  - [SQL 基本操作](/basic-sql-operations.md)\n  - [深入探索 HTAP](/explore-htap.md)\n- 应用开发\n  - [概览](/develop/dev-guide-overview.md)\n  - 快速开始\n    - [使用 TiDB Cloud Serverless 构建 TiDB 集群](/develop/dev-guide-build-cluster-in-cloud.md)\n    - [使用 TiDB 的增删改查 SQL](/develop/dev-guide-tidb-crud-sql.md)\n  - 示例程序\n    - Java\n      - [JDBC](/develop/dev-guide-sample-application-java-jdbc.md)\n      - [MyBatis](/develop/dev-guide-sample-application-java-mybatis.md)\n      - [Hibernate](/develop/dev-guide-sample-application-java-hibernate.md)\n      - [Spring Boot](/develop/dev-guide-sample-application-java-spring-boot.md)\n    - Go\n      - [Go-MySQL-Driver](/develop/dev-guide-sample-application-golang-sql-driver.md)\n      - [GORM](/develop/dev-guide-sample-application-golang-gorm.md)\n    - Python\n      - [mysqlclient](/develop/dev-guide-sample-application-python-mysqlclient.md)\n      - [MySQL Connector/Python](/develop/dev-guide-sample-application-python-mysql-connector.md)\n      - [PyMySQL](/develop/dev-guide-sample-application-python-pymysql.md)\n      - [SQLAlchemy](/develop/dev-guide-sample-application-python-sqlalchemy.md)\n      - [peewee](/develop/dev-guide-sample-application-python-peewee.md)\n      - [Django](/develop/dev-guide-sample-application-python-django.md)\n    - Node.js\n      - [node-mysql2](/develop/dev-guide-sample-application-nodejs-mysql2.md)\n      - [mysql.js](/develop/dev-guide-sample-application-nodejs-mysqljs.md)\n      - [Prisma](/develop/dev-guide-sample-application-nodejs-prisma.md)\n      - [Sequelize](/develop/dev-guide-sample-application-nodejs-sequelize.md)\n      - [TypeORM](/develop/dev-guide-sample-application-nodejs-typeorm.md)\n      - [Next.js](/develop/dev-guide-sample-application-nextjs.md)\n      - [AWS Lambda](/develop/dev-guide-sample-application-aws-lambda.md)\n    - Ruby\n      - [mysql2](/develop/dev-guide-sample-application-ruby-mysql2.md)\n      - [Rails](/develop/dev-guide-sample-application-ruby-rails.md)\n  - 连接到 TiDB\n    - GUI 数据库工具\n      - [MySQL Workbench](/develop/dev-guide-gui-mysql-workbench.md)\n      - [Navicat](/develop/dev-guide-gui-navicat.md)\n    - [选择驱动或 ORM 框架](/develop/dev-guide-choose-driver-or-orm.md)\n    - [连接到 TiDB](/develop/dev-guide-connect-to-tidb.md)\n    - [连接池与连接参数](/develop/dev-guide-connection-parameters.md)\n  - 数据库模式设计\n    - [概览](/develop/dev-guide-schema-design-overview.md)\n    - [创建数据库](/develop/dev-guide-create-database.md)\n    - [创建表](/develop/dev-guide-create-table.md)\n    - [创建二级索引](/develop/dev-guide-create-secondary-indexes.md)\n  - 数据写入\n    - [插入数据](/develop/dev-guide-insert-data.md)\n    - [更新数据](/develop/dev-guide-update-data.md)\n    - [删除数据](/develop/dev-guide-delete-data.md)\n    - [使用 TTL (Time to Live) 定期删除过期数据](/time-to-live.md)\n    - [预处理语句](/develop/dev-guide-prepared-statement.md)\n  - 数据读取\n    - [单表读取](/develop/dev-guide-get-data-from-single-table.md)\n    - [多表连接查询](/develop/dev-guide-join-tables.md)\n    - [子查询](/develop/dev-guide-use-subqueries.md)\n    - [查询结果分页](/develop/dev-guide-paginate-results.md)\n    - [视图](/develop/dev-guide-use-views.md)\n    - [临时表](/develop/dev-guide-use-temporary-tables.md)\n    - [公共表表达式](/develop/dev-guide-use-common-table-expression.md)\n    - 读取副本数据\n      - [Follower Read](/develop/dev-guide-use-follower-read.md)\n      - [Stale Read](/develop/dev-guide-use-stale-read.md)\n    - [HTAP 查询](/develop/dev-guide-hybrid-oltp-and-olap-queries.md)\n  - 向量搜索\n    - [概述](/vector-search/vector-search-overview.md)\n    - 快速入门\n      - [使用 SQL 开始向量搜索](/vector-search/vector-search-get-started-using-sql.md)\n      - [使用 Python 开始向量搜索](/vector-search/vector-search-get-started-using-python.md)\n    - 集成\n      - [集成概览](/vector-search/vector-search-integration-overview.md)\n      - AI 框架\n        - [LlamaIndex](/vector-search/vector-search-integrate-with-llamaindex.md)\n        - [Langchain](/vector-search/vector-search-integrate-with-langchain.md)\n      - 嵌入模型/服务\n        - [Jina AI](/vector-search/vector-search-integrate-with-jinaai-embedding.md)\n      - ORM 库\n        - [SQLAlchemy](/vector-search/vector-search-integrate-with-sqlalchemy.md)\n        - [peewee](/vector-search/vector-search-integrate-with-peewee.md)\n        - [Django](/vector-search/vector-search-integrate-with-django-orm.md)\n    - [优化搜索性能](/vector-search/vector-search-improve-performance.md)\n    - [使用限制](/vector-search/vector-search-limitations.md)\n  - 事务\n    - [概览](/develop/dev-guide-transaction-overview.md)\n    - [乐观事务和悲观事务](/develop/dev-guide-optimistic-and-pessimistic-transaction.md)\n    - [事务限制](/develop/dev-guide-transaction-restraints.md)\n    - [事务错误处理](/develop/dev-guide-transaction-troubleshoot.md)\n  - 优化 SQL 性能\n    - [概览](/develop/dev-guide-optimize-sql-overview.md)\n    - [SQL 性能调优](/develop/dev-guide-optimize-sql.md)\n    - [性能调优最佳实践](/develop/dev-guide-optimize-sql-best-practices.md)\n    - [索引的最佳实践](/develop/dev-guide-index-best-practice.md)\n    - 其他优化\n      - [避免隐式类型转换](/develop/dev-guide-implicit-type-conversion.md)\n      - [唯一序列号生成方案](/develop/dev-guide-unique-serial-number-generation.md)\n  - 故障诊断\n    - [SQL 或事务问题](/develop/dev-guide-troubleshoot-overview.md)\n    - [结果集不稳定](/develop/dev-guide-unstable-result-set.md)\n    - [超时](/develop/dev-guide-timeouts-in-tidb.md)\n  - 引用文档\n    - [Bookshop 示例应用](/develop/dev-guide-bookshop-schema-design.md)\n    - 规范\n      - [命名规范](/develop/dev-guide-object-naming-guidelines.md)\n      - [SQL 开发规范](/develop/dev-guide-sql-development-specification.md)\n  - 云原生开发环境\n    - [Gitpod](/develop/dev-guide-playground-gitpod.md)\n  - 第三方工具支持\n    - [TiDB 支持的第三方工具](/develop/dev-guide-third-party-support.md)\n    - [已知的第三方工具兼容问题](/develop/dev-guide-third-party-tools-compatibility.md)\n    - [ProxySQL 集成指南](/develop/dev-guide-proxysql-integration.md)\n- 部署标准集群\n  - [软硬件环境需求](/hardware-and-software-requirements.md)\n  - [环境与系统配置检查](/check-before-deployment.md)\n  - 规划集群拓扑\n    - [最小部署拓扑结构](/minimal-deployment-topology.md)\n    - [TiFlash 部署拓扑](/tiflash-deployment-topology.md)\n    - [PD 微服务部署拓扑](/pd-microservices-deployment-topology.md)\n    - [TiProxy 部署拓扑](/tiproxy/tiproxy-deployment-topology.md)\n    - [TiCDC 部署拓扑](/ticdc-deployment-topology.md)\n    - [TiSpark 部署拓扑](/tispark-deployment-topology.md)\n    - [跨机房部署拓扑结构](/geo-distributed-deployment-topology.md)\n    - [混合部署拓扑结构](/hybrid-deployment-topology.md)\n  - [使用 TiUP 部署](/production-deployment-using-tiup.md)\n  - [在 Kubernetes 上部署](/tidb-in-kubernetes.md)\n  - [验证集群状态](/post-installation-check.md)\n  - 测试集群性能\n    - [用 Sysbench 测试 TiDB](/benchmark/benchmark-tidb-using-sysbench.md)\n    - [对 TiDB 进行 TPC-C 测试](/benchmark/benchmark-tidb-using-tpcc.md)\n    - [对 TiDB 进行 CH-benCHmark 测试](/benchmark/benchmark-tidb-using-ch.md)\n- 数据迁移\n  - [数据迁移概述](/migration-overview.md)\n  - [数据迁移工具](/migration-tools.md)\n  - [数据导入最佳实践](/tidb-lightning/data-import-best-practices.md)\n  - 数据迁移场景\n    - [从 Aurora 迁移数据到 TiDB](/migrate-aurora-to-tidb.md)\n    - [从小数据量 MySQL 迁移数据到 TiDB](/migrate-small-mysql-to-tidb.md)\n    - [从大数据量 MySQL 迁移数据到 TiDB](/migrate-large-mysql-to-tidb.md)\n    - [从小数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-small-mysql-shards-to-tidb.md)\n    - [从大数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-large-mysql-shards-to-tidb.md)\n    - [从 Vitess 迁移数据到 TiDB](/migrate-from-vitess.md)\n    - [从 MariaDB 迁移数据到 TiDB](/migrate-from-mariadb.md)\n    - [从 CSV 文件迁移数据到 TiDB](/migrate-from-csv-files-to-tidb.md)\n    - [从 SQL 文件迁移数据到 TiDB](/migrate-from-sql-files-to-tidb.md)\n    - [从 Parquet 文件迁移数据到 TiDB](/migrate-from-parquet-files-to-tidb.md)\n    - [从 TiDB 集群迁移数据至另一 TiDB 集群](/migrate-from-tidb-to-tidb.md)\n    - [从 TiDB 集群迁移数据至兼容 MySQL 的数据库](/migrate-from-tidb-to-mysql.md)\n  - 复杂迁移场景\n    - [上游使用 pt/gh-ost 工具的持续同步场景](/migrate-with-pt-ghost.md)\n    - [下游存在更多列的迁移场景](/migrate-with-more-columns-downstream.md)\n    - [如何根据类型或 DDL 内容过滤 binlog 事件](/filter-binlog-event.md)\n    - [如何通过 SQL 表达式过滤 DML binlog 事件](/filter-dml-event.md)\n- 数据同步\n  - [TiCDC 概述](/ticdc/ticdc-overview.md)\n  - [安装部署与集群运维](/ticdc/deploy-ticdc.md)\n  - Changefeed\n    - [Changefeed 概述](/ticdc/ticdc-changefeed-overview.md)\n    - 创建 Changefeed\n      - [同步数据到 MySQL 兼容的数据库](/ticdc/ticdc-sink-to-mysql.md)\n      - [同步数据到 Kafka](/ticdc/ticdc-sink-to-kafka.md)\n      - [同步数据到 Pulsar](/ticdc/ticdc-sink-to-pulsar.md)\n      - [同步数据到存储服务](/ticdc/ticdc-sink-to-cloud-storage.md)\n    - [管理 Changefeed](/ticdc/ticdc-manage-changefeed.md)\n    - [日志过滤器](/ticdc/ticdc-filter.md)\n    - [DDL 同步](/ticdc/ticdc-ddl.md)\n    - [双向复制](/ticdc/ticdc-bidirectional-replication.md)\n  - 监控告警\n    - [基本监控指标](/ticdc/ticdc-summary-monitor.md)\n    - [详细监控指标](/ticdc/monitor-ticdc.md)\n    - [报警规则](/ticdc/ticdc-alert-rules.md)\n  - 数据集成场景\n    - [数据集成概述](/integration-overview.md)\n    - [与 Confluent Cloud 和 Snowflake 进行数据集成](/ticdc/integrate-confluent-using-ticdc.md)\n    - [与 Apache Kafka 和 Apache Flink 进行数据集成](/replicate-data-to-kafka.md)\n  - 参考指南\n    - [TiCDC 架构设计与原理](/ticdc/ticdc-architecture.md)\n    - [TiCDC Server 配置参数](/ticdc/ticdc-server-config.md)\n    - [TiCDC Changefeed 配置参数](/ticdc/ticdc-changefeed-config.md)\n    - [TiCDC 客户端鉴权](/ticdc/ticdc-client-authentication.md)\n    - [单行数据正确性校验](/ticdc/ticdc-integrity-check.md)\n    - [主从集群数据校验和快照读](/ticdc/ticdc-upstream-downstream-check.md)\n    - [拆分 UPDATE 事件行为说明](/ticdc/ticdc-split-update-behavior.md)\n    - 输出数据协议\n      - [TiCDC Avro Protocol](/ticdc/ticdc-avro-protocol.md)\n      - [TiCDC Canal-JSON Protocol](/ticdc/ticdc-canal-json.md)\n      - [TiCDC CSV Protocol](/ticdc/ticdc-csv.md)\n      - [TiCDC Debezium Protocol](/ticdc/ticdc-debezium.md)\n      - [TiCDC Open Protocol](/ticdc/ticdc-open-protocol.md)\n      - [TiCDC Simple Protocol](/ticdc/ticdc-simple-protocol.md)\n    - [TiCDC Open API v2](/ticdc/ticdc-open-api-v2.md)\n    - [TiCDC Open API v1](/ticdc/ticdc-open-api.md)\n    - TiCDC 数据消费\n      - [基于 Avro 的 TiCDC 行数据 Checksum 校验](/ticdc/ticdc-avro-checksum-verification.md)\n      - [Storage sink 消费程序编写指引](/ticdc/ticdc-storage-consumer-dev-guide.md)\n    - [TiCDC 兼容性](/ticdc/ticdc-compatibility.md)\n  - [故障处理](/ticdc/troubleshoot-ticdc.md)\n  - [常见问题解答](/ticdc/ticdc-faq.md)\n  - [术语表](/ticdc/ticdc-glossary.md)\n- 运维操作\n  - 安全加固\n    - [TiDB 安全配置最佳实践](/best-practices-for-security-configuration.md)\n    - [为 TiDB 客户端服务端间通信开启加密传输](/enable-tls-between-clients-and-servers.md)\n    - [为 TiDB 组件间通信开启加密传输](/enable-tls-between-components.md)\n    - [生成自签名证书](/generate-self-signed-certificates.md)\n    - [静态加密](/encryption-at-rest.md)\n    - [为 TiDB 落盘文件开启加密](/enable-disk-spill-encrypt.md)\n    - [日志脱敏](/log-redaction.md)\n  - 升级 TiDB 版本\n    - [使用 TiUP 升级](/upgrade-tidb-using-tiup.md)\n    - [使用 TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/upgrade-a-tidb-cluster)\n    - [平滑升级 TiDB](/smooth-upgrade-tidb.md)\n    - [TiFlash 升级帮助](/tiflash-upgrade-guide.md)\n  - 扩缩容\n    - [使用 TiUP（推荐）](/scale-tidb-using-tiup.md)\n    - [使用 TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/scale-a-tidb-cluster)\n  - 备份与恢复\n    - [备份与恢复概述](/br/backup-and-restore-overview.md)\n    - 架构设计\n      - [架构概述](/br/backup-and-restore-design.md)\n      - [快照备份与恢复架构](/br/br-snapshot-architecture.md)\n      - [日志备份与 PITR 架构](/br/br-log-architecture.md)\n    - 使用 BR 进行备份与恢复\n      - [使用概述](/br/br-use-overview.md)\n      - [快照备份与恢复](/br/br-snapshot-guide.md)\n      - [日志备份与 PITR](/br/br-pitr-guide.md)\n      - [实践示例](/br/backup-and-restore-use-cases.md)\n      - [备份存储](/br/backup-and-restore-storages.md)\n    - br cli 命令手册\n      - [命令概述](/br/use-br-command-line-tool.md)\n      - [快照备份与恢复命令手册](/br/br-snapshot-manual.md)\n      - [日志备份与 PITR 命令手册](/br/br-pitr-manual.md)\n    - 参考指南\n      - BR 特性\n        - [自动调节](/br/br-auto-tune.md)\n        - [批量建表](/br/br-batch-create-table.md)\n        - [断点备份](/br/br-checkpoint-backup.md)\n        - [断点恢复](/br/br-checkpoint-restore.md)\n      - [使用 Dumpling 和 TiDB Lightning 备份与恢复](/backup-and-restore-using-dumpling-lightning.md)\n      - [备份与恢复 RawKV](/br/rawkv-backup-and-restore.md)\n      - [增量备份与恢复](/br/br-incremental-guide.md)\n  - 集群容灾\n    - [容灾方案介绍](/dr-solution-introduction.md)\n    - [基于主备集群的容灾](/dr-secondary-cluster.md)\n    - [基于多副本的单集群容灾](/dr-multi-replica.md)\n    - [基于备份与恢复的容灾](/dr-backup-restore.md)\n  - [使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)\n  - [修改时区](/configure-time-zone.md)\n  - [日常巡检](/daily-check.md)\n  - [TiFlash 常用运维操作](/tiflash/maintain-tiflash.md)\n  - [使用 TiUP 运维集群](/maintain-tidb-using-tiup.md)\n  - [在线修改集群配置](/dynamic-config.md)\n  - [在线有损恢复](/online-unsafe-recovery.md)\n  - [搭建双集群主从复制](/replicate-between-primary-and-secondary-clusters.md)\n- 监控与告警\n  - [监控框架概述](/tidb-monitoring-framework.md)\n  - [监控 API](/tidb-monitoring-api.md)\n  - [手动部署监控](/deploy-monitoring-services.md)\n  - [升级监控组件](/upgrade-monitoring-services.md)\n  - [将 Grafana 监控数据导出成快照](/exporting-grafana-snapshots.md)\n  - [TiDB 集群报警规则与处理方法](/alert-rules.md)\n  - [TiFlash 报警规则与处理方法](/tiflash/tiflash-alert-rules.md)\n  - [自定义监控组件的配置](/tiup/customized-montior-in-tiup-environment.md)\n  - [BR 监控告警](/br/br-monitoring-and-alert.md)\n- 故障诊断\n  - 故障诊断问题汇总\n    - [TiDB 集群问题导图](/tidb-troubleshooting-map.md)\n    - [TiDB 集群常见问题](/troubleshoot-tidb-cluster.md)\n    - [TiFlash 常见问题](/tiflash/troubleshoot-tiflash.md)\n  - 故障场景\n    - 慢查询\n      - [定位慢查询](/identify-slow-queries.md)\n      - [分析慢查询](/analyze-slow-queries.md)\n    - [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n    - [热点问题处理](/troubleshoot-hot-spot-issues.md)\n    - [读写延迟增加](/troubleshoot-cpu-issues.md)\n    - [写冲突与写性能下降](/troubleshoot-write-conflicts.md)\n    - [磁盘 I/O 过高](/troubleshoot-high-disk-io.md)\n    - [锁冲突与 TTL 超时](/troubleshoot-lock-conflicts.md)\n    - [数据索引不一致报错](/troubleshoot-data-inconsistency-errors.md)\n  - 故障诊断方法\n    - [通过 SQL 诊断获取集群诊断信息](/information-schema/information-schema-sql-diagnostics.md)\n    - [通过 Statement Summary 排查 SQL 性能问题](/statement-summary-tables.md)\n    - [使用 Top SQL 定位系统资源消耗过多的查询](/dashboard/top-sql.md)\n    - [通过日志定位消耗系统资源多的查询](/identify-expensive-queries.md)\n    - [保存和恢复集群现场信息](/sql-plan-replayer.md)\n    - [理解 TiKV 中的 Stale Read 和 safe-ts](/troubleshoot-stale-read.md)\n  - [获取支持](/support.md)\n- 性能调优\n  - 优化手册\n    - [优化概述](/performance-tuning-overview.md)\n    - [优化方法](/performance-tuning-methods.md)\n    - [OLTP 负载性能优化实践](/performance-tuning-practices.md)\n    - [TiFlash 性能分析方法](/tiflash-performance-tuning-methods.md)\n    - [TiCDC 性能分析方法](/ticdc-performance-tuning-methods.md)\n    - [延迟的拆解分析](/latency-breakdown.md)\n    - [在公有云上部署 TiDB 的最佳实践](/best-practices-on-public-cloud.md)\n  - 配置调优\n      - [操作系统性能参数调优](/tune-operating-system.md)\n      - [TiDB 内存调优](/configure-memory-usage.md)\n      - [TiKV 线程调优](/tune-tikv-thread-performance.md)\n      - [TiKV 内存调优](/tune-tikv-memory-performance.md)\n      - [TiKV Follower Read](/follower-read.md)\n      - [TiKV MVCC 内存引擎](/tikv-in-memory-engine.md)\n      - [Region 性能调优](/tune-region-performance.md)\n      - [TiFlash 调优](/tiflash/tune-tiflash-performance.md)\n      - [下推计算结果缓存](/coprocessor-cache.md)\n      - 垃圾回收 (GC)\n        - [GC 机制简介](/garbage-collection-overview.md)\n        - [GC 配置](/garbage-collection-configuration.md)\n  - SQL 性能调优\n    - [SQL 性能调优概览](/sql-tuning-overview.md)\n    - 理解 TiDB 执行计划\n      - [TiDB 执行计划概览](/explain-overview.md)\n      - [使用 `EXPLAIN` 解读执行计划](/explain-walkthrough.md)\n      - [MPP 模式查询的执行计划](/explain-mpp.md)\n      - [索引查询的执行计划](/explain-indexes.md)\n      - [Join 查询的执行计划](/explain-joins.md)\n      - [子查询的执行计划](/explain-subqueries.md)\n      - [聚合查询的执行计划](/explain-aggregation.md)\n      - [视图查询的执行计划](/explain-views.md)\n      - [分区查询的执行计划](/explain-partitions.md)\n      - [开启 IndexMerge 查询的执行计划](/explain-index-merge.md)\n    - SQL 优化流程\n      - [SQL 优化流程概览](/sql-optimization-concepts.md)\n      - 逻辑优化\n        - [逻辑优化概览](/sql-logical-optimization.md)\n        - [子查询相关的优化](/subquery-optimization.md)\n        - [列裁剪](/column-pruning.md)\n        - [关联子查询去关联](/correlated-subquery-optimization.md)\n        - [Max/Min 消除](/max-min-eliminate.md)\n        - [谓词下推](/predicate-push-down.md)\n        - [分区裁剪](/partition-pruning.md)\n        - [TopN 和 Limit 下推](/topn-limit-push-down.md)\n        - [Join Reorder](/join-reorder.md)\n        - [从窗口函数中推导 TopN 或 Limit](/derive-topn-from-window.md)\n      - 物理优化\n        - [物理优化概览](/sql-physical-optimization.md)\n        - [索引的选择](/choose-index.md)\n        - [常规统计信息](/statistics.md)\n        - [扩展统计信息](/extended-statistics.md)\n        - [错误索引的解决方案](/wrong-index-solution.md)\n        - [Distinct 优化](/agg-distinct-optimization.md)\n        - [代价模型](/cost-model.md)\n        - [Runtime Filter](/runtime-filter.md)\n      - [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n      - [非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)\n    - 控制执行计划\n      - [控制执行计划概览](/control-execution-plan.md)\n      - [Optimizer Hints](/optimizer-hints.md)\n      - [执行计划管理](/sql-plan-management.md)\n      - [优化规则及表达式下推的黑名单](/blocklist-control-plan.md)\n      - [Optimizer Fix Controls](/optimizer-fix-controls.md)\n- 教程\n  - [单区域多 AZ 部署](/multi-data-centers-in-one-city-deployment.md)\n  - [双区域多 AZ 部署](/three-data-centers-in-two-cities-deployment.md)\n  - [单区域双 AZ 部署](/two-data-centers-in-one-city-deployment.md)\n  - 读取历史数据\n    - 使用 Stale Read 功能读取历史数据（推荐）\n      - [Stale Read 使用场景介绍](/stale-read.md)\n      - [使用 `AS OF TIMESTAMP` 语法读取历史数据](/as-of-timestamp.md)\n      - [使用系统变量 `tidb_read_staleness` 读取历史数据](/tidb-read-staleness.md)\n      - [使用系统变量 `tidb_external_ts` 读取历史数据](/tidb-external-ts.md)\n    - [使用系统变量 `tidb_snapshot` 读取历史数据](/read-historical-data.md)\n  - 最佳实践\n    - [TiDB 最佳实践](/best-practices/tidb-best-practices.md)\n    - [Java 应用开发最佳实践](/best-practices/java-app-best-practices.md)\n    - [HAProxy 最佳实践](/best-practices/haproxy-best-practices.md)\n    - [高并发写入场景最佳实践](/best-practices/high-concurrency-best-practices.md)\n    - [Grafana 监控最佳实践](/best-practices/grafana-monitor-best-practices.md)\n    - [PD 调度策略最佳实践](/best-practices/pd-scheduling-best-practices.md)\n    - [海量 Region 集群调优](/best-practices/massive-regions-best-practices.md)\n    - [三节点混合部署最佳实践](/best-practices/three-nodes-hybrid-deployment.md)\n    - [在三数据中心下就近读取数据](/best-practices/three-dc-local-read.md)\n    - [使用 UUID](/best-practices/uuid.md)\n    - [只读存储节点最佳实践](/best-practices/readonly-nodes.md)\n  - [Placement Rules 使用文档](/configure-placement-rules.md)\n  - [Load Base Split 使用文档](/configure-load-base-split.md)\n  - [Store Limit 使用文档](/configure-store-limit.md)\n  - [DDL 执行原理及最佳实践](/ddl-introduction.md)\n  - [数据批量处理](/batch-processing.md)\n  - PD 微服务使用文档\n    - [PD 微服务概览](/pd-microservices.md)\n    - [使用 TiUP 扩容缩容 PD 微服务节点](/scale-microservices-using-tiup.md)\n    - [TSO 配置文件描述](/tso-configuration-file.md)\n    - [TSO 配置参数](/command-line-flags-for-tso-configuration.md)\n    - [Scheduling 配置文件描述](/scheduling-configuration-file.md)\n    - [Scheduling 配置参数](/command-line-flags-for-scheduling-configuration.md)\n- TiDB 工具\n  - [功能概览](/ecosystem-tool-user-guide.md)\n  - [使用场景](/ecosystem-tool-user-case.md)\n  - [工具下载](/download-ecosystem-tools.md)\n  - TiUP\n    - [文档地图](/tiup/tiup-documentation-guide.md)\n    - [概览](/tiup/tiup-overview.md)\n    - [术语及核心概念](/tiup/tiup-terminology-and-concepts.md)\n    - [TiUP 组件管理](/tiup/tiup-component-management.md)\n    - [FAQ](/tiup/tiup-faq.md)\n    - [故障排查](/tiup/tiup-troubleshooting-guide.md)\n    - TiUP 命令参考手册\n      - [命令概览](/tiup/tiup-reference.md)\n      - TiUP 命令\n        - [tiup clean](/tiup/tiup-command-clean.md)\n        - [tiup completion](/tiup/tiup-command-completion.md)\n        - [tiup env](/tiup/tiup-command-env.md)\n        - [tiup help](/tiup/tiup-command-help.md)\n        - [tiup install](/tiup/tiup-command-install.md)\n        - [tiup list](/tiup/tiup-command-list.md)\n        - tiup mirror\n          - [tiup mirror 概览](/tiup/tiup-command-mirror.md)\n          - [tiup mirror clone](/tiup/tiup-command-mirror-clone.md)\n          - [tiup mirror genkey](/tiup/tiup-command-mirror-genkey.md)\n          - [tiup mirror grant](/tiup/tiup-command-mirror-grant.md)\n          - [tiup mirror init](/tiup/tiup-command-mirror-init.md)\n          - [tiup mirror merge](/tiup/tiup-command-mirror-merge.md)\n          - [tiup mirror modify](/tiup/tiup-command-mirror-modify.md)\n          - [tiup mirror publish](/tiup/tiup-command-mirror-publish.md)\n          - [tiup mirror rotate](/tiup/tiup-command-mirror-rotate.md)\n          - [tiup mirror set](/tiup/tiup-command-mirror-set.md)\n          - [tiup mirror sign](/tiup/tiup-command-mirror-sign.md)\n        - [tiup status](/tiup/tiup-command-status.md)\n        - [tiup telemetry](/tiup/tiup-command-telemetry.md)\n        - [tiup uninstall](/tiup/tiup-command-uninstall.md)\n        - [tiup update](/tiup/tiup-command-update.md)\n      - TiUP Cluster 命令\n        - [TiUP Cluster 命令概览](/tiup/tiup-component-cluster.md)\n        - [tiup cluster audit](/tiup/tiup-component-cluster-audit.md)\n        - [tiup cluster audit cleanup](/tiup/tiup-component-cluster-audit-cleanup.md)\n        - [tiup cluster check](/tiup/tiup-component-cluster-check.md)\n        - [tiup cluster clean](/tiup/tiup-component-cluster-clean.md)\n        - [tiup cluster deploy](/tiup/tiup-component-cluster-deploy.md)\n        - [tiup cluster destroy](/tiup/tiup-component-cluster-destroy.md)\n        - [tiup cluster disable](/tiup/tiup-component-cluster-disable.md)\n        - [tiup cluster display](/tiup/tiup-component-cluster-display.md)\n        - [tiup cluster edit-config](/tiup/tiup-component-cluster-edit-config.md)\n        - [tiup cluster enable](/tiup/tiup-component-cluster-enable.md)\n        - [tiup cluster help](/tiup/tiup-component-cluster-help.md)\n        - [tiup cluster import](/tiup/tiup-component-cluster-import.md)\n        - [tiup cluster list](/tiup/tiup-component-cluster-list.md)\n        - [tiup cluster meta backup](/tiup/tiup-component-cluster-meta-backup.md)\n        - [tiup cluster meta restore](/tiup/tiup-component-cluster-meta-restore.md)\n        - [tiup cluster patch](/tiup/tiup-component-cluster-patch.md)\n        - [tiup cluster prune](/tiup/tiup-component-cluster-prune.md)\n        - [tiup cluster reload](/tiup/tiup-component-cluster-reload.md)\n        - [tiup cluster rename](/tiup/tiup-component-cluster-rename.md)\n        - [tiup cluster replay](/tiup/tiup-component-cluster-replay.md)\n        - [tiup cluster restart](/tiup/tiup-component-cluster-restart.md)\n        - [tiup cluster scale-in](/tiup/tiup-component-cluster-scale-in.md)\n        - [tiup cluster scale-out](/tiup/tiup-component-cluster-scale-out.md)\n        - [tiup cluster start](/tiup/tiup-component-cluster-start.md)\n        - [tiup cluster stop](/tiup/tiup-component-cluster-stop.md)\n        - [tiup cluster template](/tiup/tiup-component-cluster-template.md)\n        - [tiup cluster upgrade](/tiup/tiup-component-cluster-upgrade.md)\n      - TiUP DM 命令\n        - [TiUP DM 命令概览](/tiup/tiup-component-dm.md)\n        - [tiup dm audit](/tiup/tiup-component-dm-audit.md)\n        - [tiup dm deploy](/tiup/tiup-component-dm-deploy.md)\n        - [tiup dm destroy](/tiup/tiup-component-dm-destroy.md)\n        - [tiup dm disable](/tiup/tiup-component-dm-disable.md)\n        - [tiup dm display](/tiup/tiup-component-dm-display.md)\n        - [tiup dm edit-config](/tiup/tiup-component-dm-edit-config.md)\n        - [tiup dm enable](/tiup/tiup-component-dm-enable.md)\n        - [tiup dm help](/tiup/tiup-component-dm-help.md)\n        - [tiup dm import](/tiup/tiup-component-dm-import.md)\n        - [tiup dm list](/tiup/tiup-component-dm-list.md)\n        - [tiup dm patch](/tiup/tiup-component-dm-patch.md)\n        - [tiup dm prune](/tiup/tiup-component-dm-prune.md)\n        - [tiup dm reload](/tiup/tiup-component-dm-reload.md)\n        - [tiup dm replay](/tiup/tiup-component-dm-replay.md)\n        - [tiup dm restart](/tiup/tiup-component-dm-restart.md)\n        - [tiup dm scale-in](/tiup/tiup-component-dm-scale-in.md)\n        - [tiup dm scale-out](/tiup/tiup-component-dm-scale-out.md)\n        - [tiup dm start](/tiup/tiup-component-dm-start.md)\n        - [tiup dm stop](/tiup/tiup-component-dm-stop.md)\n        - [tiup dm template](/tiup/tiup-component-dm-template.md)\n        - [tiup dm upgrade](/tiup/tiup-component-dm-upgrade.md)\n    - [TiDB 集群拓扑文件配置](/tiup/tiup-cluster-topology-reference.md)\n    - [DM 集群拓扑文件配置](/tiup/tiup-dm-topology-reference.md)\n    - [TiUP 镜像参考指南](/tiup/tiup-mirror-reference.md)\n    - TiUP 组件文档\n      - [tiup-playground 运行本地测试集群](/tiup/tiup-playground.md)\n      - [tiup-cluster 部署运维生产集群](/tiup/tiup-cluster.md)\n      - [tiup-mirror 定制离线镜像](/tiup/tiup-mirror.md)\n      - [tiup-bench 进行 TPCC/TPCH 压力测试](/tiup/tiup-bench.md)\n  - [TiDB Operator](/tidb-operator-overview.md)\n  - TiDB Data Migration\n    - [关于 Data Migration](/dm/dm-overview.md)\n    - [架构简介](/dm/dm-arch.md)\n    - [快速开始](/dm/quick-start-with-dm.md)\n    - [最佳实践](/dm/dm-best-practices.md)\n    - 部署 DM 集群\n      - [软硬件要求](/dm/dm-hardware-and-software-requirements.md)\n      - [使用 TiUP 联网部署（推荐）](/dm/deploy-a-dm-cluster-using-tiup.md)\n      - [使用 TiUP 离线部署](/dm/deploy-a-dm-cluster-using-tiup-offline.md)\n      - [使用 Binary 部署](/dm/deploy-a-dm-cluster-using-binary.md)\n      - [在 Kubernetes 环境中部署](https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/deploy-tidb-dm)\n    - 入门指南\n      - [创建数据源](/dm/quick-start-create-source.md)\n      - [数据源操作](/dm/dm-manage-source.md)\n      - [任务配置向导](/dm/dm-task-configuration-guide.md)\n      - [分库分表合并](/dm/dm-shard-merge.md)\n      - [表路由](/dm/dm-table-routing.md)\n      - [黑白名单](/dm/dm-block-allow-table-lists.md)\n      - [过滤 binlog 事件](/dm/dm-binlog-event-filter.md)\n      - [通过 SQL 表达式过滤 DML](/dm/feature-expression-filter.md)\n      - [Online DDL 工具支持](/dm/dm-online-ddl-tool-support.md)\n      - [自定义加解密 key](/dm/dm-customized-secret-key.md)\n      - 迁移任务操作\n        - [任务前置检查](/dm/dm-precheck.md)\n        - [创建任务](/dm/dm-create-task.md)\n        - [查询状态](/dm/dm-query-status.md)\n        - [暂停任务](/dm/dm-pause-task.md)\n        - [恢复任务](/dm/dm-resume-task.md)\n        - [停止任务](/dm/dm-stop-task.md)\n    - 进阶教程\n      - 分库分表合并迁移\n        - [概述](/dm/feature-shard-merge.md)\n        - [悲观模式](/dm/feature-shard-merge-pessimistic.md)\n        - [乐观模式](/dm/feature-shard-merge-optimistic.md)\n        - [手动处理 Sharding DDL Lock](/dm/manually-handling-sharding-ddl-locks.md)\n      - [迁移使用 GH-ost/PT-osc 的数据源](/dm/feature-online-ddl.md)\n      - [上下游列数量不一致的迁移](/migrate-with-more-columns-downstream.md)\n      - [增量数据校验](/dm/dm-continuous-data-validation.md)\n    - 运维管理\n      - 集群版本升级\n        - [使用 TiUP 运维集群（推荐）](/dm/maintain-dm-using-tiup.md)\n        - [1.0.x 到 2.0+ 手动升级](/dm/manually-upgrade-dm-1.0-to-2.0.md)\n        - [在线应用 Hotfix 到 DM 集群](/tiup/tiup-component-dm-patch.md)\n      - 集群运维工具\n        - [使用 WebUI 管理迁移任务](/dm/dm-webui-guide.md)\n        - [使用 dmctl 管理迁移任务](/dm/dmctl-introduction.md)\n      - 性能调优\n        - [性能数据](/dm/dm-benchmark-v5.4.0.md)\n        - [配置调优](/dm/dm-tune-configuration.md)\n        - [如何进行压力测试](/dm/dm-performance-test.md)\n        - [性能问题及处理方法](/dm/dm-handle-performance-issues.md)\n      - 数据源管理\n        - [变更同步的数据源地址](/dm/usage-scenario-master-slave-switch.md)\n      - 任务管理\n        - [处理出错的 DDL 语句](/dm/handle-failed-ddl-statements.md)\n        - [管理迁移表的表结构](/dm/dm-manage-schema.md)\n      - [导出和导入集群的数据源和任务配置](/dm/dm-export-import-config.md)\n      - [处理告警](/dm/dm-handle-alerts.md)\n      - [日常巡检](/dm/dm-daily-check.md)\n    - 参考手册\n      - 架构组件\n        - [DM-worker 说明](/dm/dm-worker-intro.md)\n        - [安全模式](/dm/dm-safe-mode.md)\n        - [Relay Log](/dm/relay-log.md)\n      - 运行机制\n        - [DML 同步机制](/dm/dm-dml-replication-logic.md)\n        - [高可用机制](/dm/dm-high-availability.md)\n        - [DDL 特殊处理说明](/dm/dm-ddl-compatible.md)\n      - 命令行\n        - [DM-master & DM-worker](/dm/dm-command-line-flags.md)\n      - 配置文件\n        - [概述](/dm/dm-config-overview.md)\n        - [数据源配置](/dm/dm-source-configuration-file.md)\n        - [迁移任务配置](/dm/task-configuration-file-full.md)\n        - [DM-master 配置](/dm/dm-master-configuration-file.md)\n        - [DM-worker 配置](/dm/dm-worker-configuration-file.md)\n        - [Table Selector](/dm/table-selector.md)\n      - [OpenAPI](/dm/dm-open-api.md)\n      - [兼容性目录](/dm/dm-compatibility-catalog.md)\n      - 安全\n        - [为 DM 的连接开启加密传输](/dm/dm-enable-tls.md)\n        - [生成自签名证书](/dm/dm-generate-self-signed-certificates.md)\n      - 监控告警\n        - [监控指标](/dm/monitor-a-dm-cluster.md)\n        - [告警信息](/dm/dm-alert-rules.md)\n      - [错误码](/dm/dm-error-handling.md#常见故障处理方法)\n      - [术语表](/dm/dm-glossary.md)\n      - 使用示例\n        - [使用 DM 迁移数据](/dm/migrate-data-using-dm.md)\n        - [快速创建迁移任务](/dm/quick-start-create-task.md)\n        - [分表合并数据迁移最佳实践](/dm/shard-merge-best-practices.md)\n      - 异常解决\n        - [常见问题](/dm/dm-faq.md)\n        - [错误处理及恢复](/dm/dm-error-handling.md)\n      - [版本发布历史](/dm/dm-release-notes.md)\n  - TiDB Lightning\n    - [概述](/tidb-lightning/tidb-lightning-overview.md)\n    - [`IMPORT INTO` 和 TiDB Lightning 对比](/tidb-lightning/import-into-vs-tidb-lightning.md)\n    - [`IMPORT INTO` 和 TiDB Lightning 与日志备份和 TiCDC 的兼容性](/tidb-lightning/tidb-lightning-compatibility-and-scenarios.md)\n    - [快速上手](/get-started-with-tidb-lightning.md)\n    - [部署 TiDB Lightning](/tidb-lightning/deploy-tidb-lightning.md)\n    - [目标数据库要求](/tidb-lightning/tidb-lightning-requirements.md)\n    - 数据源\n      - [文件匹配规则](/tidb-lightning/tidb-lightning-data-source.md)\n      - [表库重命名](/tidb-lightning/tidb-lightning-data-source.md#表库重命名)\n      - [CSV](/tidb-lightning/tidb-lightning-data-source.md#csv)\n      - [SQL](/tidb-lightning/tidb-lightning-data-source.md#sql)\n      - [Parquet](/tidb-lightning/tidb-lightning-data-source.md#parquet)\n      - [压缩文件](/tidb-lightning/tidb-lightning-data-source.md#压缩文件)\n      - [自定义文件匹配](/tidb-lightning/tidb-lightning-data-source.md#自定义文件匹配)\n      - [从 Amazon S3 导入数据](/tidb-lightning/tidb-lightning-data-source.md#从-amazon-s3-导入数据)\n    - 物理导入模式\n      - [概述](/tidb-lightning/tidb-lightning-physical-import-mode.md)\n      - [必要条件及限制](/tidb-lightning/tidb-lightning-physical-import-mode.md#必要条件及限制)\n      - [配置及使用](/tidb-lightning/tidb-lightning-physical-import-mode-usage.md)\n      - [冲突检测](/tidb-lightning/tidb-lightning-physical-import-mode-usage.md#冲突数据检测)\n      - [性能调优](/tidb-lightning/tidb-lightning-physical-import-mode-usage.md#性能调优)\n    - 逻辑导入模式\n      - [概述](/tidb-lightning/tidb-lightning-logical-import-mode.md)\n      - [必要条件及限制](/tidb-lightning/tidb-lightning-logical-import-mode.md#必要条件)\n      - [配置及使用](/tidb-lightning/tidb-lightning-logical-import-mode-usage.md)\n      - [冲突检测](/tidb-lightning/tidb-lightning-logical-import-mode-usage.md#冲突数据检测)\n      - [性能调优](/tidb-lightning/tidb-lightning-logical-import-mode-usage.md#性能调优)\n    - [前置检查](/tidb-lightning/tidb-lightning-prechecks.md)\n    - [表库过滤](/table-filter.md)\n    - [断点续传](/tidb-lightning/tidb-lightning-checkpoints.md)\n    - [并行导入](/tidb-lightning/tidb-lightning-distributed-import.md)\n    - [可容忍错误](/tidb-lightning/tidb-lightning-error-resolution.md)\n    - [故障处理](/tidb-lightning/troubleshoot-tidb-lightning.md)\n    - 参考手册\n      - [完整配置文件](/tidb-lightning/tidb-lightning-configuration.md)\n      - [命令行参数](/tidb-lightning/tidb-lightning-command-line-full.md)\n      - [监控告警](/tidb-lightning/monitor-tidb-lightning.md)\n      - [Web 界面](/tidb-lightning/tidb-lightning-web-interface.md)\n      - [FAQ](/tidb-lightning/tidb-lightning-faq.md)\n      - [术语表](/tidb-lightning/tidb-lightning-glossary.md)\n  - [Dumpling](/dumpling-overview.md)\n  - PingCAP Clinic 诊断服务\n    - [概述](/clinic/clinic-introduction.md)\n    - [快速上手](/clinic/quick-start-with-clinic.md)\n    - [使用 PingCAP Clinic 诊断集群](/clinic/clinic-user-guide-for-tiup.md)\n    - [使用 PingCAP Clinic 生成诊断报告](/clinic/clinic-report.md)\n    - [采集 SQL 查询计划信息](/clinic/clinic-collect-sql-query-plan.md)\n    - [数据采集说明](/clinic/clinic-data-instruction-for-tiup.md)\n  - TiSpark\n    - [TiSpark 用户指南](/tispark-overview.md)\n  - sync-diff-inspector\n    - [概述](/sync-diff-inspector/sync-diff-inspector-overview.md)\n    - [不同库名或表名的数据校验](/sync-diff-inspector/route-diff.md)\n    - [分库分表场景下的数据校验](/sync-diff-inspector/shard-diff.md)\n    - [基于 DM 同步场景下的数据校验](/sync-diff-inspector/dm-diff.md)\n  - TiProxy\n    - [概述](/tiproxy/tiproxy-overview.md)\n    - [负载均衡策略](/tiproxy/tiproxy-load-balance.md)\n    - [流量回放](/tiproxy/tiproxy-traffic-replay.md)\n    - [配置文件](/tiproxy/tiproxy-configuration.md)\n    - [命令行参数](/tiproxy/tiproxy-command-line-flags.md)\n    - [监控指标](/tiproxy/tiproxy-grafana.md)\n    - [API](/tiproxy/tiproxy-api.md)\n    - [故障诊断](/tiproxy/troubleshoot-tiproxy.md)\n    - [性能测试报告](/tiproxy/tiproxy-performance-test.md)\n- 参考指南\n  - 架构\n    - [概述](/tidb-architecture.md)\n    - [存储](/tidb-storage.md)\n    - [计算](/tidb-computing.md)\n    - [调度](/tidb-scheduling.md)\n    - [TSO](/tso.md)\n  - 存储引擎 TiKV\n    - [TiKV 简介](/tikv-overview.md)\n    - [RocksDB 简介](/storage-engine/rocksdb-overview.md)\n    - [Titan 简介](/storage-engine/titan-overview.md)\n    - [Titan 配置说明](/storage-engine/titan-configuration.md)\n    - [Partitioned Raft KV](/partitioned-raft-kv.md)\n  - 存储引擎 TiFlash\n    - [TiFlash 简介](/tiflash/tiflash-overview.md)\n    - [构建 TiFlash 副本](/tiflash/create-tiflash-replicas.md)\n    - [使用 TiDB 读取 TiFlash](/tiflash/use-tidb-to-read-tiflash.md)\n    - [使用 TiSpark 读取 TiFlash](/tiflash/use-tispark-to-read-tiflash.md)\n    - [使用 MPP 模式](/tiflash/use-tiflash-mpp-mode.md)\n    - [TiFlash 存算分离架构与 S3 支持](/tiflash/tiflash-disaggregated-and-s3.md)\n    - [使用 FastScan 功能](/tiflash/use-fastscan.md)\n    - [TiFlash 支持的计算下推](/tiflash/tiflash-supported-pushdown-calculations.md)\n    - [TiFlash 查询结果物化](/tiflash/tiflash-results-materialization.md)\n    - [TiFlash 延迟物化](/tiflash/tiflash-late-materialization.md)\n    - [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)\n    - [TiFlash 数据校验](/tiflash/tiflash-data-validation.md)\n    - [TiFlash MinTSO 调度器](/tiflash/tiflash-mintso-scheduler.md)\n    - [TiFlash 兼容性说明](/tiflash/tiflash-compatibility.md)\n    - [TiFlash Pipeline Model 执行模型](/tiflash/tiflash-pipeline-model.md)\n  - TiDB 分布式执行框架\n    - [TiDB 分布式执行框架介绍](/tidb-distributed-execution-framework.md)\n    - [TiDB 全局排序](/tidb-global-sort.md)\n  - [系统变量](/system-variables.md)\n  - [系统变量索引](/system-variable-reference.md)\n  - [服务器状态变量](/status-variables.md)\n  - 配置文件参数\n    - [tidb-server](/tidb-configuration-file.md)\n    - [tikv-server](/tikv-configuration-file.md)\n    - [tiflash-server](/tiflash/tiflash-configuration.md)\n    - [pd-server](/pd-configuration-file.md)\n  - CLI\n    - [tikv-ctl](/tikv-control.md)\n    - [pd-ctl](/pd-control.md)\n    - [tidb-ctl](/tidb-control.md)\n    - [pd-recover](/pd-recover.md)\n  - 命令行参数\n    - [tidb-server](/command-line-flags-for-tidb-configuration.md)\n    - [tikv-server](/command-line-flags-for-tikv-configuration.md)\n    - [tiflash-server](/tiflash/tiflash-command-line-flags.md)\n    - [pd-server](/command-line-flags-for-pd-configuration.md)\n  - 监控指标\n    - [Overview 面板](/grafana-overview-dashboard.md)\n    - [Performance Overview 面板](/grafana-performance-overview-dashboard.md)\n    - [TiDB 面板](/grafana-tidb-dashboard.md)\n    - [PD 面板](/grafana-pd-dashboard.md)\n    - [TiKV 面板](/grafana-tikv-dashboard.md)\n    - [TiFlash 监控指标](/tiflash/monitor-tiflash.md)\n    - [TiCDC 监控指标](/ticdc/monitor-ticdc.md)\n    - [Resource Control 监控指标](/grafana-resource-control-dashboard.md)\n  - 权限\n    - [与 MySQL 安全特性差异](/security-compatibility-with-mysql.md)\n    - [权限管理](/privilege-management.md)\n    - [TiDB 用户账户管理](/user-account-management.md)\n    - [TiDB 密码管理](/password-management.md)\n    - [基于角色的访问控制](/role-based-access-control.md)\n    - [TiDB 证书鉴权使用指南](/certificate-authentication.md)\n  - SQL\n    - SQL 语言结构和语法\n      - 属性\n        - [AUTO_INCREMENT](/auto-increment.md)\n        - [AUTO_RANDOM](/auto-random.md)\n        - [SHARD_ROW_ID_BITS](/shard-row-id-bits.md)\n      - [字面值](/literal-values.md)\n      - [Schema 对象名](/schema-object-names.md)\n      - [关键字](/keywords.md)\n      - [用户自定义变量](/user-defined-variables.md)\n      - [表达式语法](/expression-syntax.md)\n      - [注释语法](/comment-syntax.md)\n    - SQL 语句\n      - [概览](/sql-statements/sql-statement-overview.md)\n      - [`ADMIN`](/sql-statements/sql-statement-admin.md)\n      - [`ADMIN ALTER DDL JOBS`](/sql-statements/sql-statement-admin-alter-ddl.md)\n      - [`ADMIN CANCEL DDL`](/sql-statements/sql-statement-admin-cancel-ddl.md)\n      - [`ADMIN CHECKSUM TABLE`](/sql-statements/sql-statement-admin-checksum-table.md)\n      - [`ADMIN CHECK [TABLE|INDEX]`](/sql-statements/sql-statement-admin-check-table-index.md)\n      - [`ADMIN CLEANUP`](/sql-statements/sql-statement-admin-cleanup.md)\n      - [`ADMIN PAUSE DDL`](/sql-statements/sql-statement-admin-pause-ddl.md)\n      - [`ADMIN RECOVER INDEX`](/sql-statements/sql-statement-admin-recover.md)\n      - [`ADMIN RESUME DDL`](/sql-statements/sql-statement-admin-resume-ddl.md)\n      - [`ADMIN [SET|SHOW|UNSET] BDR ROLE`](/sql-statements/sql-statement-admin-bdr-role.md)\n      - [`ADMIN SHOW DDL [JOBS|JOB QUERIES]`](/sql-statements/sql-statement-admin-show-ddl.md)\n      - [`ALTER DATABASE`](/sql-statements/sql-statement-alter-database.md)\n      - [`ALTER INSTANCE`](/sql-statements/sql-statement-alter-instance.md)\n      - [`ALTER PLACEMENT POLICY`](/sql-statements/sql-statement-alter-placement-policy.md)\n      - [`ALTER RANGE`](/sql-statements/sql-statement-alter-range.md)\n      - [`ALTER RESOURCE GROUP`](/sql-statements/sql-statement-alter-resource-group.md)\n      - [`ALTER SEQUENCE`](/sql-statements/sql-statement-alter-sequence.md)\n      - `ALTER TABLE`\n        - [概述](/sql-statements/sql-statement-alter-table.md)\n        - [`ADD COLUMN`](/sql-statements/sql-statement-add-column.md)\n        - [`ADD INDEX`](/sql-statements/sql-statement-add-index.md)\n        - [`ALTER INDEX`](/sql-statements/sql-statement-alter-index.md)\n        - [`CHANGE COLUMN`](/sql-statements/sql-statement-change-column.md)\n        - [`COMPACT`](/sql-statements/sql-statement-alter-table-compact.md)\n        - [`DROP COLUMN`](/sql-statements/sql-statement-drop-column.md)\n        - [`DROP INDEX`](/sql-statements/sql-statement-drop-index.md)\n        - [`MODIFY COLUMN`](/sql-statements/sql-statement-modify-column.md)\n        - [`RENAME INDEX`](/sql-statements/sql-statement-rename-index.md)\n      - [`ALTER USER`](/sql-statements/sql-statement-alter-user.md)\n      - [`ANALYZE TABLE`](/sql-statements/sql-statement-analyze-table.md)\n      - [`BACKUP`](/sql-statements/sql-statement-backup.md)\n      - [`BATCH`](/sql-statements/sql-statement-batch.md)\n      - [`BEGIN`](/sql-statements/sql-statement-begin.md)\n      - [`CALIBRATE RESOURCE`](/sql-statements/sql-statement-calibrate-resource.md)\n      - [`CANCEL IMPORT JOB`](/sql-statements/sql-statement-cancel-import-job.md)\n      - [`COMMIT`](/sql-statements/sql-statement-commit.md)\n      - [`CREATE BINDING`](/sql-statements/sql-statement-create-binding.md)\n      - [`CREATE DATABASE`](/sql-statements/sql-statement-create-database.md)\n      - [`CREATE INDEX`](/sql-statements/sql-statement-create-index.md)\n      - [`CREATE PLACEMENT POLICY`](/sql-statements/sql-statement-create-placement-policy.md)\n      - [`CREATE RESOURCE GROUP`](/sql-statements/sql-statement-create-resource-group.md)\n      - [`CREATE ROLE`](/sql-statements/sql-statement-create-role.md)\n      - [`CREATE SEQUENCE`](/sql-statements/sql-statement-create-sequence.md)\n      - [`CREATE TABLE LIKE`](/sql-statements/sql-statement-create-table-like.md)\n      - [`CREATE TABLE`](/sql-statements/sql-statement-create-table.md)\n      - [`CREATE USER`](/sql-statements/sql-statement-create-user.md)\n      - [`CREATE VIEW`](/sql-statements/sql-statement-create-view.md)\n      - [`DEALLOCATE`](/sql-statements/sql-statement-deallocate.md)\n      - [`DELETE`](/sql-statements/sql-statement-delete.md)\n      - [`DESC`](/sql-statements/sql-statement-desc.md)\n      - [`DESCRIBE`](/sql-statements/sql-statement-describe.md)\n      - [`DO`](/sql-statements/sql-statement-do.md)\n      - [`DROP BINDING`](/sql-statements/sql-statement-drop-binding.md)\n      - [`DROP DATABASE`](/sql-statements/sql-statement-drop-database.md)\n      - [`DROP PLACEMENT POLICY`](/sql-statements/sql-statement-drop-placement-policy.md)\n      - [`DROP RESOURCE GROUP`](/sql-statements/sql-statement-drop-resource-group.md)\n      - [`DROP ROLE`](/sql-statements/sql-statement-drop-role.md)\n      - [`DROP SEQUENCE`](/sql-statements/sql-statement-drop-sequence.md)\n      - [`DROP STATS`](/sql-statements/sql-statement-drop-stats.md)\n      - [`DROP TABLE`](/sql-statements/sql-statement-drop-table.md)\n      - [`DROP USER`](/sql-statements/sql-statement-drop-user.md)\n      - [`DROP VIEW`](/sql-statements/sql-statement-drop-view.md)\n      - [`EXECUTE`](/sql-statements/sql-statement-execute.md)\n      - [`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md)\n      - [`EXPLAIN`](/sql-statements/sql-statement-explain.md)\n      - [`FLASHBACK CLUSTER`](/sql-statements/sql-statement-flashback-cluster.md)\n      - [`FLASHBACK DATABASE`](/sql-statements/sql-statement-flashback-database.md)\n      - [`FLASHBACK TABLE`](/sql-statements/sql-statement-flashback-table.md)\n      - [`FLUSH PRIVILEGES`](/sql-statements/sql-statement-flush-privileges.md)\n      - [`FLUSH STATUS`](/sql-statements/sql-statement-flush-status.md)\n      - [`FLUSH TABLES`](/sql-statements/sql-statement-flush-tables.md)\n      - [`GRANT <privileges>`](/sql-statements/sql-statement-grant-privileges.md)\n      - [`GRANT <role>`](/sql-statements/sql-statement-grant-role.md)\n      - [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md)\n      - [`INSERT`](/sql-statements/sql-statement-insert.md)\n      - [`KILL`](/sql-statements/sql-statement-kill.md)\n      - [`LOAD DATA`](/sql-statements/sql-statement-load-data.md)\n      - [`LOAD STATS`](/sql-statements/sql-statement-load-stats.md)\n      - [`LOCK STATS`](/sql-statements/sql-statement-lock-stats.md)\n      - [`[LOCK|UNLOCK] TABLES`](/sql-statements/sql-statement-lock-tables-and-unlock-tables.md)\n      - [`PREPARE`](/sql-statements/sql-statement-prepare.md)\n      - [`QUERY WATCH`](/sql-statements/sql-statement-query-watch.md)\n      - [`RECOVER TABLE`](/sql-statements/sql-statement-recover-table.md)\n      - [`RENAME USER`](/sql-statements/sql-statement-rename-user.md)\n      - [`RENAME TABLE`](/sql-statements/sql-statement-rename-table.md)\n      - [`REPLACE`](/sql-statements/sql-statement-replace.md)\n      - [`RESTORE`](/sql-statements/sql-statement-restore.md)\n      - [`REVOKE <privileges>`](/sql-statements/sql-statement-revoke-privileges.md)\n      - [`REVOKE <role>`](/sql-statements/sql-statement-revoke-role.md)\n      - [`ROLLBACK`](/sql-statements/sql-statement-rollback.md)\n      - [`SAVEPOINT`](/sql-statements/sql-statement-savepoint.md)\n      - [`SELECT`](/sql-statements/sql-statement-select.md)\n      - [`SET DEFAULT ROLE`](/sql-statements/sql-statement-set-default-role.md)\n      - [`SET [NAMES|CHARACTER SET]`](/sql-statements/sql-statement-set-names.md)\n      - [`SET PASSWORD`](/sql-statements/sql-statement-set-password.md)\n      - [`SET RESOURCE GROUP`](/sql-statements/sql-statement-set-resource-group.md)\n      - [`SET ROLE`](/sql-statements/sql-statement-set-role.md)\n      - [`SET TRANSACTION`](/sql-statements/sql-statement-set-transaction.md)\n      - [`SET <variable>`](/sql-statements/sql-statement-set-variable.md)\n      - [`SHOW [BACKUPS|RESTORES]`](/sql-statements/sql-statement-show-backups.md)\n      - [`SHOW ANALYZE STATUS`](/sql-statements/sql-statement-show-analyze-status.md)\n      - [`SHOW BINDINGS`](/sql-statements/sql-statement-show-bindings.md)\n      - [`SHOW BUILTINS`](/sql-statements/sql-statement-show-builtins.md)\n      - [`SHOW CHARACTER SET`](/sql-statements/sql-statement-show-character-set.md)\n      - [`SHOW COLLATION`](/sql-statements/sql-statement-show-collation.md)\n      - [`SHOW COLUMN_STATS_USAGE`](/sql-statements/sql-statement-show-column-stats-usage.md)\n      - [`SHOW COLUMNS FROM`](/sql-statements/sql-statement-show-columns-from.md)\n      - [`SHOW CONFIG`](/sql-statements/sql-statement-show-config.md)\n      - [`SHOW CREATE PLACEMENT POLICY`](/sql-statements/sql-statement-show-create-placement-policy.md)\n      - [`SHOW CREATE RESOURCE GROUP`](/sql-statements/sql-statement-show-create-resource-group.md)\n      - [`SHOW CREATE SEQUENCE`](/sql-statements/sql-statement-show-create-sequence.md)\n      - [`SHOW CREATE TABLE`](/sql-statements/sql-statement-show-create-table.md)\n      - [`SHOW CREATE DATABASE`](/sql-statements/sql-statement-show-create-database.md)\n      - [`SHOW CREATE USER`](/sql-statements/sql-statement-show-create-user.md)\n      - [`SHOW DATABASES`](/sql-statements/sql-statement-show-databases.md)\n      - [`SHOW ENGINES`](/sql-statements/sql-statement-show-engines.md)\n      - [`SHOW ERRORS`](/sql-statements/sql-statement-show-errors.md)\n      - [`SHOW FIELDS FROM`](/sql-statements/sql-statement-show-fields-from.md)\n      - [`SHOW GRANTS`](/sql-statements/sql-statement-show-grants.md)\n      - [`SHOW IMPORT JOB`](/sql-statements/sql-statement-show-import-job.md)\n      - [`SHOW INDEXES`](/sql-statements/sql-statement-show-indexes.md)\n      - [`SHOW MASTER STATUS`](/sql-statements/sql-statement-show-master-status.md)\n      - [`SHOW PLACEMENT`](/sql-statements/sql-statement-show-placement.md)\n      - [`SHOW PLACEMENT FOR`](/sql-statements/sql-statement-show-placement-for.md)\n      - [`SHOW PLACEMENT LABELS`](/sql-statements/sql-statement-show-placement-labels.md)\n      - [`SHOW PLUGINS`](/sql-statements/sql-statement-show-plugins.md)\n      - [`SHOW PRIVILEGES`](/sql-statements/sql-statement-show-privileges.md)\n      - [`SHOW PROCESSLIST`](/sql-statements/sql-statement-show-processlist.md)\n      - [`SHOW PROFILES`](/sql-statements/sql-statement-show-profiles.md)\n      - [`SHOW SCHEMAS`](/sql-statements/sql-statement-show-schemas.md)\n      - [`SHOW STATS_BUCKETS`](/sql-statements/sql-statement-show-stats-buckets.md)\n      - [`SHOW STATS_HEALTHY`](/sql-statements/sql-statement-show-stats-healthy.md)\n      - [`SHOW STATS_HISTOGRAMS`](/sql-statements/sql-statement-show-stats-histograms.md)\n      - [`SHOW STATS_LOCKED`](/sql-statements/sql-statement-show-stats-locked.md)\n      - [`SHOW STATS_META`](/sql-statements/sql-statement-show-stats-meta.md)\n      - [`SHOW STATS_TOPN`](/sql-statements/sql-statement-show-stats-topn.md)\n      - [`SHOW STATUS`](/sql-statements/sql-statement-show-status.md)\n      - [`SHOW TABLE NEXT_ROW_ID`](/sql-statements/sql-statement-show-table-next-rowid.md)\n      - [`SHOW TABLE REGIONS`](/sql-statements/sql-statement-show-table-regions.md)\n      - [`SHOW TABLE STATUS`](/sql-statements/sql-statement-show-table-status.md)\n      - [`SHOW TABLES`](/sql-statements/sql-statement-show-tables.md)\n      - [`SHOW VARIABLES`](/sql-statements/sql-statement-show-variables.md)\n      - [`SHOW WARNINGS`](/sql-statements/sql-statement-show-warnings.md)\n      - [`SHUTDOWN`](/sql-statements/sql-statement-shutdown.md)\n      - [`SPLIT REGION`](/sql-statements/sql-statement-split-region.md)\n      - [`START TRANSACTION`](/sql-statements/sql-statement-start-transaction.md)\n      - [`TABLE`](/sql-statements/sql-statement-table.md)\n      - [`TRACE`](/sql-statements/sql-statement-trace.md)\n      - [`TRUNCATE`](/sql-statements/sql-statement-truncate.md)\n      - [`UNLOCK STATS`](/sql-statements/sql-statement-unlock-stats.md)\n      - [`UPDATE`](/sql-statements/sql-statement-update.md)\n      - [`USE`](/sql-statements/sql-statement-use.md)\n      - [`WITH`](/sql-statements/sql-statement-with.md)\n    - 数据类型\n      - [数据类型概述](/data-type-overview.md)\n      - [数据类型默认值](/data-type-default-values.md)\n      - [数值类型](/data-type-numeric.md)\n      - [日期和时间类型](/data-type-date-and-time.md)\n      - [字符串类型](/data-type-string.md)\n      - [JSON 类型](/data-type-json.md)\n      - [向量数据类型](/vector-search/vector-search-data-types.md)\n    - 函数与操作符\n      - [函数与操作符概述](/functions-and-operators/functions-and-operators-overview.md)\n      - [表达式求值的类型转换](/functions-and-operators/type-conversion-in-expression-evaluation.md)\n      - [操作符](/functions-and-operators/operators.md)\n      - [控制流程函数](/functions-and-operators/control-flow-functions.md)\n      - [字符串函数](/functions-and-operators/string-functions.md)\n      - [数值函数与操作符](/functions-and-operators/numeric-functions-and-operators.md)\n      - [日期和时间函数](/functions-and-operators/date-and-time-functions.md)\n      - [位函数和操作符](/functions-and-operators/bit-functions-and-operators.md)\n      - [Cast 函数和操作符](/functions-and-operators/cast-functions-and-operators.md)\n      - [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n      - [锁函数](/functions-and-operators/locking-functions.md)\n      - [信息函数](/functions-and-operators/information-functions.md)\n      - [向量函数和操作符](/vector-search/vector-search-functions-and-operators.md)\n      - JSON 函数\n        - [概览](/functions-and-operators/json-functions.md)\n        - [创建 JSON 的函数](/functions-and-operators/json-functions/json-functions-create.md)\n        - [搜索 JSON 的函数](/functions-and-operators/json-functions/json-functions-search.md)\n        - [修改 JSON 的函数](/functions-and-operators/json-functions/json-functions-modify.md)\n        - [返回 JSON 的函数](/functions-and-operators/json-functions/json-functions-return.md)\n        - [JSON 效用函数](/functions-and-operators/json-functions/json-functions-utility.md)\n        - [聚合 JSON 的函数](/functions-and-operators/json-functions/json-functions-aggregate.md)\n        - [验证 JSON 的函数](/functions-and-operators/json-functions/json-functions-validate.md)\n      - [GROUP BY 聚合函数](/functions-and-operators/aggregate-group-by-functions.md)\n      - [GROUP BY 修饰符](/functions-and-operators/group-by-modifier.md)\n      - [窗口函数](/functions-and-operators/window-functions.md)\n      - [其它函数](/functions-and-operators/miscellaneous-functions.md)\n      - [精度数学](/functions-and-operators/precision-math.md)\n      - [集合运算](/functions-and-operators/set-operators.md)\n      - [序列函数](/functions-and-operators/sequence-functions.md)\n      - [下推到 TiKV 的表达式列表](/functions-and-operators/expressions-pushed-down.md)\n      - [TiDB 特有的函数](/functions-and-operators/tidb-functions.md)\n      - [Oracle 与 TiDB 函数和语法差异对照](/oracle-functions-to-tidb.md)\n    - [聚簇索引](/clustered-indexes.md)\n    - [向量索引](/vector-search/vector-search-index.md)\n    - [约束](/constraints.md)\n    - [生成列](/generated-columns.md)\n    - [SQL 模式](/sql-mode.md)\n    - [表属性](/table-attributes.md)\n    - 事务\n      - [事务概览](/transaction-overview.md)\n      - [隔离级别](/transaction-isolation-levels.md)\n      - [乐观事务](/optimistic-transaction.md)\n      - [悲观事务](/pessimistic-transaction.md)\n      - [非事务 DML 语句](/non-transactional-dml.md)\n      - [Pipelined DML](/pipelined-dml.md)\n    - [视图](/views.md)\n    - [分区表](/partitioned-table.md)\n    - [临时表](/temporary-tables.md)\n    - [缓存表](/cached-tables.md)\n    - [外键约束](/foreign-key.md)\n    - 字符集和排序规则\n      - [概述](/character-set-and-collation.md)\n      - [GBK](/character-set-gbk.md)\n    - [Placement Rules in SQL](/placement-rules-in-sql.md)\n    - 系统表\n      - `mysql` Schema\n        - [概述](/mysql-schema/mysql-schema.md)\n        - [`user`](/mysql-schema/mysql-schema-user.md)\n      - INFORMATION_SCHEMA\n        - [概述](/information-schema/information-schema.md)\n        - [`ANALYZE_STATUS`](/information-schema/information-schema-analyze-status.md)\n        - [`CHECK_CONSTRAINTS`](/information-schema/information-schema-check-constraints.md)\n        - [`CLIENT_ERRORS_SUMMARY_BY_HOST`](/information-schema/client-errors-summary-by-host.md)\n        - [`CLIENT_ERRORS_SUMMARY_BY_USER`](/information-schema/client-errors-summary-by-user.md)\n        - [`CLIENT_ERRORS_SUMMARY_GLOBAL`](/information-schema/client-errors-summary-global.md)\n        - [`CHARACTER_SETS`](/information-schema/information-schema-character-sets.md)\n        - [`CLUSTER_CONFIG`](/information-schema/information-schema-cluster-config.md)\n        - [`CLUSTER_HARDWARE`](/information-schema/information-schema-cluster-hardware.md)\n        - [`CLUSTER_INFO`](/information-schema/information-schema-cluster-info.md)\n        - [`CLUSTER_LOAD`](/information-schema/information-schema-cluster-load.md)\n        - [`CLUSTER_LOG`](/information-schema/information-schema-cluster-log.md)\n        - [`CLUSTER_SYSTEMINFO`](/information-schema/information-schema-cluster-systeminfo.md)\n        - [`COLLATIONS`](/information-schema/information-schema-collations.md)\n        - [`COLLATION_CHARACTER_SET_APPLICABILITY`](/information-schema/information-schema-collation-character-set-applicability.md)\n        - [`COLUMNS`](/information-schema/information-schema-columns.md)\n        - [`DATA_LOCK_WAITS`](/information-schema/information-schema-data-lock-waits.md)\n        - [`DDL_JOBS`](/information-schema/information-schema-ddl-jobs.md)\n        - [`DEADLOCKS`](/information-schema/information-schema-deadlocks.md)\n        - [`ENGINES`](/information-schema/information-schema-engines.md)\n        - [`INSPECTION_RESULT`](/information-schema/information-schema-inspection-result.md)\n        - [`INSPECTION_RULES`](/information-schema/information-schema-inspection-rules.md)\n        - [`INSPECTION_SUMMARY`](/information-schema/information-schema-inspection-summary.md)\n        - [`KEYWORDS`](/information-schema/information-schema-keywords.md)\n        - [`KEY_COLUMN_USAGE`](/information-schema/information-schema-key-column-usage.md)\n        - [`MEMORY_USAGE`](/information-schema/information-schema-memory-usage.md)\n        - [`MEMORY_USAGE_OPS_HISTORY`](/information-schema/information-schema-memory-usage-ops-history.md)\n        - [`METRICS_SUMMARY`](/information-schema/information-schema-metrics-summary.md)\n        - [`METRICS_TABLES`](/information-schema/information-schema-metrics-tables.md)\n        - [`PARTITIONS`](/information-schema/information-schema-partitions.md)\n        - [`PLACEMENT_POLICIES`](/information-schema/information-schema-placement-policies.md)\n        - [`PROCESSLIST`](/information-schema/information-schema-processlist.md)\n        - [`REFERENTIAL_CONSTRAINTS`](/information-schema/information-schema-referential-constraints.md)\n        - [`RESOURCE_GROUPS`](/information-schema/information-schema-resource-groups.md)\n        - [`RUNAWAY_WATCHES`](/information-schema/information-schema-runaway-watches.md)\n        - [`SCHEMATA`](/information-schema/information-schema-schemata.md)\n        - [`SEQUENCES`](/information-schema/information-schema-sequences.md)\n        - [`SESSION_VARIABLES`](/information-schema/information-schema-session-variables.md)\n        - [`SLOW_QUERY`](/information-schema/information-schema-slow-query.md)\n        - [`STATISTICS`](/information-schema/information-schema-statistics.md)\n        - [`TABLES`](/information-schema/information-schema-tables.md)\n        - [`TABLE_CONSTRAINTS`](/information-schema/information-schema-table-constraints.md)\n        - [`TABLE_STORAGE_STATS`](/information-schema/information-schema-table-storage-stats.md)\n        - [`TIDB_CHECK_CONSTRAINTS`](/information-schema/information-schema-tidb-check-constraints.md)\n        - [`TIDB_HOT_REGIONS`](/information-schema/information-schema-tidb-hot-regions.md)\n        - [`TIDB_HOT_REGIONS_HISTORY`](/information-schema/information-schema-tidb-hot-regions-history.md)\n        - [`TIDB_INDEXES`](/information-schema/information-schema-tidb-indexes.md)\n        - [`TIDB_INDEX_USAGE`](/information-schema/information-schema-tidb-index-usage.md)\n        - [`TIDB_SERVERS_INFO`](/information-schema/information-schema-tidb-servers-info.md)\n        - [`TIDB_TRX`](/information-schema/information-schema-tidb-trx.md)\n        - [`TIFLASH_REPLICA`](/information-schema/information-schema-tiflash-replica.md)\n        - [`TIFLASH_SEGMENTS`](/information-schema/information-schema-tiflash-segments.md)\n        - [`TIFLASH_TABLES`](/information-schema/information-schema-tiflash-tables.md)\n        - [`TIKV_REGION_PEERS`](/information-schema/information-schema-tikv-region-peers.md)\n        - [`TIKV_REGION_STATUS`](/information-schema/information-schema-tikv-region-status.md)\n        - [`TIKV_STORE_STATUS`](/information-schema/information-schema-tikv-store-status.md)\n        - [`USER_ATTRIBUTES`](/information-schema/information-schema-user-attributes.md)\n        - [`USER_PRIVILEGES`](/information-schema/information-schema-user-privileges.md)\n        - [`VARIABLES_INFO`](/information-schema/information-schema-variables-info.md)\n        - [`VIEWS`](/information-schema/information-schema-views.md)\n      - [`METRICS_SCHEMA`](/metrics-schema.md)\n      - PERFORMANCE_SCHEMA\n        - [概述](/performance-schema/performance-schema.md)\n        - [`SESSION_CONNECT_ATTRS`](/performance-schema/performance-schema-session-connect-attrs.md)\n      - SYS\n        - [概述](/sys-schema/sys-schema.md)\n        - [`schema_unused_indexes`](/sys-schema/sys-schema-unused-indexes.md)\n    - [元数据锁](/metadata-lock.md)\n    - [TiDB 加速建表](/accelerated-table-creation.md)\n    - [Schema 缓存](/schema-cache.md)\n  - UI\n    - TiDB Dashboard\n      - [简介](/dashboard/dashboard-intro.md)\n      - 运维\n        - [部署](/dashboard/dashboard-ops-deploy.md)\n        - [反向代理](/dashboard/dashboard-ops-reverse-proxy.md)\n        - [用户管理](/dashboard/dashboard-user.md)\n        - [安全](/dashboard/dashboard-ops-security.md)\n      - [访问](/dashboard/dashboard-access.md)\n      - [概况页面](/dashboard/dashboard-overview.md)\n      - [集群信息页面](/dashboard/dashboard-cluster-info.md)\n      - [Top SQL 页面](/dashboard/top-sql.md)\n      - [流量可视化页面](/dashboard/dashboard-key-visualizer.md)\n      - [监控关系图](/dashboard/dashboard-metrics-relation.md)\n      - SQL 语句分析\n        - [列表页面](/dashboard/dashboard-statement-list.md)\n        - [执行详情页面](/dashboard/dashboard-statement-details.md)\n      - [慢查询页面](/dashboard/dashboard-slow-query.md)\n      - 集群诊断页面\n        - [访问](/dashboard/dashboard-diagnostics-access.md)\n        - [查看报告](/dashboard/dashboard-diagnostics-report.md)\n        - [使用示例](/dashboard/dashboard-diagnostics-usage.md)\n      - [监控指标页面](/dashboard/dashboard-monitoring.md)\n      - [日志搜索页面](/dashboard/dashboard-log-search.md)\n      - [资源管控页面](/dashboard/dashboard-resource-manager.md)\n      - 实例性能分析\n        - [手动分析页面](/dashboard/dashboard-profiling.md)\n        - [持续分析页面](/dashboard/continuous-profiling.md)\n      - 会话管理与配置\n        - [分享会话](/dashboard/dashboard-session-share.md)\n        - [配置 SSO 登录](/dashboard/dashboard-session-sso.md)\n      - [常见问题](/dashboard/dashboard-faq.md)\n  - [遥测](/telemetry.md)\n  - [错误码](/error-codes.md)\n  - [通过拓扑 label 进行副本调度](/schedule-replicas-by-topology-labels.md)\n  - [外部存储服务的 URI 格式](/external-storage-uri.md)\n- 常见问题解答 (FAQ)\n  - [FAQ 汇总](/faq/faq-overview.md)\n  - [产品 FAQ](/faq/tidb-faq.md)\n  - [SQL FAQ](/faq/sql-faq.md)\n  - [安装部署 FAQ](/faq/deploy-and-maintain-faq.md)\n  - [迁移 FAQ](/faq/migration-tidb-faq.md)\n  - [升级 FAQ](/faq/upgrade-faq.md)\n  - [监控 FAQ](/faq/monitor-faq.md)\n  - [集群管理 FAQ](/faq/manage-cluster-faq.md)\n  - [高可用 FAQ](/faq/high-availability-faq.md)\n  - [高可靠 FAQ](/faq/high-reliability-faq.md)\n  - [备份恢复 FAQ](/faq/backup-and-restore-faq.md)\n- 版本发布历史\n  - [发布版本汇总](/releases/release-notes.md)\n  - [版本发布时间线](/releases/release-timeline.md)\n  - [TiDB 版本规则](/releases/versioning.md)\n  - [TiDB 离线包](/binary-package.md)\n  - v8.5\n    - [8.5.0](/releases/release-8.5.0.md)\n  - v8.4\n    - [8.4.0-DMR](/releases/release-8.4.0.md)\n  - v8.3\n    - [8.3.0-DMR](/releases/release-8.3.0.md)\n  - v8.2\n    - [8.2.0-DMR](/releases/release-8.2.0.md)\n  - v8.1\n    - [8.1.2](/releases/release-8.1.2.md)\n    - [8.1.1](/releases/release-8.1.1.md)\n    - [8.1.0](/releases/release-8.1.0.md)\n  - v8.0\n    - [8.0.0-DMR](/releases/release-8.0.0.md)\n  - v7.6\n    - [7.6.0-DMR](/releases/release-7.6.0.md)\n  - v7.5\n    - [7.5.5](/releases/release-7.5.5.md)\n    - [7.5.4](/releases/release-7.5.4.md)\n    - [7.5.3](/releases/release-7.5.3.md)\n    - [7.5.2](/releases/release-7.5.2.md)\n    - [7.5.1](/releases/release-7.5.1.md)\n    - [7.5.0](/releases/release-7.5.0.md)\n  - v7.4\n    - [7.4.0-DMR](/releases/release-7.4.0.md)\n  - v7.3\n    - [7.3.0-DMR](/releases/release-7.3.0.md)\n  - v7.2\n    - [7.2.0-DMR](/releases/release-7.2.0.md)\n  - v7.1\n    - [7.1.6](/releases/release-7.1.6.md)\n    - [7.1.5](/releases/release-7.1.5.md)\n    - [7.1.4](/releases/release-7.1.4.md)\n    - [7.1.3](/releases/release-7.1.3.md)\n    - [7.1.2](/releases/release-7.1.2.md)\n    - [7.1.1](/releases/release-7.1.1.md)\n    - [7.1.0](/releases/release-7.1.0.md)\n  - v7.0\n    - [7.0.0-DMR](/releases/release-7.0.0.md)\n  - v6.6\n    - [6.6.0-DMR](/releases/release-6.6.0.md)\n  - v6.5\n    - [6.5.11](/releases/release-6.5.11.md)\n    - [6.5.10](/releases/release-6.5.10.md)\n    - [6.5.9](/releases/release-6.5.9.md)\n    - [6.5.8](/releases/release-6.5.8.md)\n    - [6.5.7](/releases/release-6.5.7.md)\n    - [6.5.6](/releases/release-6.5.6.md)\n    - [6.5.5](/releases/release-6.5.5.md)\n    - [6.5.4](/releases/release-6.5.4.md)\n    - [6.5.3](/releases/release-6.5.3.md)\n    - [6.5.2](/releases/release-6.5.2.md)\n    - [6.5.1](/releases/release-6.5.1.md)\n    - [6.5.0](/releases/release-6.5.0.md)\n  - v6.4\n    - [6.4.0-DMR](/releases/release-6.4.0.md)\n  - v6.3\n    - [6.3.0-DMR](/releases/release-6.3.0.md)\n  - v6.2\n    - [6.2.0-DMR](/releases/release-6.2.0.md)\n  - v6.1\n    - [6.1.7](/releases/release-6.1.7.md)\n    - [6.1.6](/releases/release-6.1.6.md)\n    - [6.1.5](/releases/release-6.1.5.md)\n    - [6.1.4](/releases/release-6.1.4.md)\n    - [6.1.3](/releases/release-6.1.3.md)\n    - [6.1.2](/releases/release-6.1.2.md)\n    - [6.1.1](/releases/release-6.1.1.md)\n    - [6.1.0](/releases/release-6.1.0.md)\n  - v6.0\n    - [6.0.0-DMR](/releases/release-6.0.0-dmr.md)\n  - v5.4\n    - [5.4.3](/releases/release-5.4.3.md)\n    - [5.4.2](/releases/release-5.4.2.md)\n    - [5.4.1](/releases/release-5.4.1.md)\n    - [5.4.0](/releases/release-5.4.0.md)\n  - v5.3\n    - [5.3.4](/releases/release-5.3.4.md)\n    - [5.3.3](/releases/release-5.3.3.md)\n    - [5.3.2](/releases/release-5.3.2.md)\n    - [5.3.1](/releases/release-5.3.1.md)\n    - [5.3.0](/releases/release-5.3.0.md)\n  - v5.2\n    - [5.2.4](/releases/release-5.2.4.md)\n    - [5.2.3](/releases/release-5.2.3.md)\n    - [5.2.2](/releases/release-5.2.2.md)\n    - [5.2.1](/releases/release-5.2.1.md)\n    - [5.2.0](/releases/release-5.2.0.md)\n  - v5.1\n    - [5.1.5](/releases/release-5.1.5.md)\n    - [5.1.4](/releases/release-5.1.4.md)\n    - [5.1.3](/releases/release-5.1.3.md)\n    - [5.1.2](/releases/release-5.1.2.md)\n    - [5.1.1](/releases/release-5.1.1.md)\n    - [5.1.0](/releases/release-5.1.0.md)\n  - v5.0\n    - [5.0.6](/releases/release-5.0.6.md)\n    - [5.0.5](/releases/release-5.0.5.md)\n    - [5.0.4](/releases/release-5.0.4.md)\n    - [5.0.3](/releases/release-5.0.3.md)\n    - [5.0.2](/releases/release-5.0.2.md)\n    - [5.0.1](/releases/release-5.0.1.md)\n    - [5.0 GA](/releases/release-5.0.0.md)\n    - [5.0.0-rc](/releases/release-5.0.0-rc.md)\n  - v4.0\n    - [4.0.16](/releases/release-4.0.16.md)\n    - [4.0.15](/releases/release-4.0.15.md)\n    - [4.0.14](/releases/release-4.0.14.md)\n    - [4.0.13](/releases/release-4.0.13.md)\n    - [4.0.12](/releases/release-4.0.12.md)\n    - [4.0.11](/releases/release-4.0.11.md)\n    - [4.0.10](/releases/release-4.0.10.md)\n    - [4.0.9](/releases/release-4.0.9.md)\n    - [4.0.8](/releases/release-4.0.8.md)\n    - [4.0.7](/releases/release-4.0.7.md)\n    - [4.0.6](/releases/release-4.0.6.md)\n    - [4.0.5](/releases/release-4.0.5.md)\n    - [4.0.4](/releases/release-4.0.4.md)\n    - [4.0.3](/releases/release-4.0.3.md)\n    - [4.0.2](/releases/release-4.0.2.md)\n    - [4.0.1](/releases/release-4.0.1.md)\n    - [4.0 GA](/releases/release-4.0-ga.md)\n    - [4.0.0-rc.2](/releases/release-4.0.0-rc.2.md)\n    - [4.0.0-rc.1](/releases/release-4.0.0-rc.1.md)\n    - [4.0.0-rc](/releases/release-4.0.0-rc.md)\n    - [4.0.0-beta.2](/releases/release-4.0.0-beta.2.md)\n    - [4.0.0-beta.1](/releases/release-4.0.0-beta.1.md)\n    - [4.0.0-beta](/releases/release-4.0.0-beta.md)\n  - v3.1\n    - [3.1.2](/releases/release-3.1.2.md)\n    - [3.1.1](/releases/release-3.1.1.md)\n    - [3.1.0 GA](/releases/release-3.1.0-ga.md)\n    - [3.1.0-rc](/releases/release-3.1.0-rc.md)\n    - [3.1.0-beta.2](/releases/release-3.1.0-beta.2.md)\n    - [3.1.0-beta.1](/releases/release-3.1.0-beta.1.md)\n    - [3.1.0-beta](/releases/release-3.1.0-beta.md)\n  - v3.0\n    - [3.0.20](/releases/release-3.0.20.md)\n    - [3.0.19](/releases/release-3.0.19.md)\n    - [3.0.18](/releases/release-3.0.18.md)\n    - [3.0.17](/releases/release-3.0.17.md)\n    - [3.0.16](/releases/release-3.0.16.md)\n    - [3.0.15](/releases/release-3.0.15.md)\n    - [3.0.14](/releases/release-3.0.14.md)\n    - [3.0.13](/releases/release-3.0.13.md)\n    - [3.0.12](/releases/release-3.0.12.md)\n    - [3.0.11](/releases/release-3.0.11.md)\n    - [3.0.10](/releases/release-3.0.10.md)\n    - [3.0.9](/releases/release-3.0.9.md)\n    - [3.0.8](/releases/release-3.0.8.md)\n    - [3.0.7](/releases/release-3.0.7.md)\n    - [3.0.6](/releases/release-3.0.6.md)\n    - [3.0.5](/releases/release-3.0.5.md)\n    - [3.0.4](/releases/release-3.0.4.md)\n    - [3.0.3](/releases/release-3.0.3.md)\n    - [3.0.2](/releases/release-3.0.2.md)\n    - [3.0.1](/releases/release-3.0.1.md)\n    - [3.0 GA](/releases/release-3.0-ga.md)\n    - [3.0.0-rc.3](/releases/release-3.0.0-rc.3.md)\n    - [3.0.0-rc.2](/releases/release-3.0.0-rc.2.md)\n    - [3.0.0-rc.1](/releases/release-3.0.0-rc.1.md)\n    - [3.0.0-beta.1](/releases/release-3.0.0-beta.1.md)\n    - [3.0.0-beta](/releases/release-3.0-beta.md)\n  - v2.1\n    - [2.1.19](/releases/release-2.1.19.md)\n    - [2.1.18](/releases/release-2.1.18.md)\n    - [2.1.17](/releases/release-2.1.17.md)\n    - [2.1.16](/releases/release-2.1.16.md)\n    - [2.1.15](/releases/release-2.1.15.md)\n    - [2.1.14](/releases/release-2.1.14.md)\n    - [2.1.13](/releases/release-2.1.13.md)\n    - [2.1.12](/releases/release-2.1.12.md)\n    - [2.1.11](/releases/release-2.1.11.md)\n    - [2.1.10](/releases/release-2.1.10.md)\n    - [2.1.9](/releases/release-2.1.9.md)\n    - [2.1.8](/releases/release-2.1.8.md)\n    - [2.1.7](/releases/release-2.1.7.md)\n    - [2.1.6](/releases/release-2.1.6.md)\n    - [2.1.5](/releases/release-2.1.5.md)\n    - [2.1.4](/releases/release-2.1.4.md)\n    - [2.1.3](/releases/release-2.1.3.md)\n    - [2.1.2](/releases/release-2.1.2.md)\n    - [2.1.1](/releases/release-2.1.1.md)\n    - [2.1 GA](/releases/release-2.1-ga.md)\n    - [2.1 RC5](/releases/release-2.1-rc.5.md)\n    - [2.1 RC4](/releases/release-2.1-rc.4.md)\n    - [2.1 RC3](/releases/release-2.1-rc.3.md)\n    - [2.1 RC2](/releases/release-2.1-rc.2.md)\n    - [2.1 RC1](/releases/release-2.1-rc.1.md)\n    - [2.1 Beta](/releases/release-2.1-beta.md)\n  - v2.0\n    - [2.0.11](/releases/release-2.0.11.md)\n    - [2.0.10](/releases/release-2.0.10.md)\n    - [2.0.9](/releases/release-2.0.9.md)\n    - [2.0.8](/releases/release-2.0.8.md)\n    - [2.0.7](/releases/release-2.0.7.md)\n    - [2.0.6](/releases/release-2.0.6.md)\n    - [2.0.5](/releases/release-2.0.5.md)\n    - [2.0.4](/releases/release-2.0.4.md)\n    - [2.0.3](/releases/release-2.0.3.md)\n    - [2.0.2](/releases/release-2.0.2.md)\n    - [2.0.1](/releases/release-2.0.1.md)\n    - [2.0](/releases/release-2.0-ga.md)\n    - [2.0 RC5](/releases/release-2.0-rc.5.md)\n    - [2.0 RC4](/releases/release-2.0-rc.4.md)\n    - [2.0 RC3](/releases/release-2.0-rc.3.md)\n    - [2.0 RC1](/releases/release-2.0-rc.1.md)\n    - [1.1 Beta](/releases/release-1.1-beta.md)\n    - [1.1 Alpha](/releases/release-1.1-alpha.md)\n  - v1.0\n    - [1.0](/releases/release-1.0-ga.md)\n    - [Pre-GA](/releases/release-pre-ga.md)\n    - [RC4](/releases/release-rc.4.md)\n    - [RC3](/releases/release-rc.3.md)\n    - [RC2](/releases/release-rc.2.md)\n    - [RC1](/releases/release-rc.1.md)\n- [术语表](/glossary.md)\n"
        },
        {
          "name": "_docHome.md",
          "type": "blob",
          "size": 5.1064453125,
          "content": "---\ntitle: 文档中心\nhide_sidebar: true\nhide_commit: true\nhide_leftNav: true\nsummary: TiDB 文档中心为您提供丰富的操作指南和参考资料，助您轻松上手 TiDB 产品，完成数据迁移和应用开发等操作。TiDB 是一款开源分布式关系型数据库，支持在线事务处理与在线分析处理，具备水平扩容、高可用、云原生、兼容 MySQL 协议等特性。TiDB Cloud 是全托管的数据库即服务产品，让数据库部署、运维和性能调优变得轻松简单，适用于中国出海企业和开发者。此外，还提供开发者手册、免费课程、TiDB 社区、TiDB 博客等资源，欢迎贡献内容。\n---\n\n<DocHomeContainer title=\"TiDB 文档中心\" subTitle=\"欢迎来到 TiDB 文档中心！我们为您提供了丰富的操作指南和详实的参考资料，助您轻松上手 TiDB 产品，顺利完成数据迁移和基于数据库的应用开发等操作。\" ctaLabel=\"快速上手 TiDB\" ctaLink=\"/zh/tidb/stable/quick-start-with-tidb\">\n\n<DocHomeSection label=\"TiDB\" anchor=\"tidb\" id=\"tidb\">\n\nTiDB 是 PingCAP 公司自主设计、研发的开源分布式关系型数据库，是一款同时支持在线事务处理与在线分析处理 (Hybrid Transactional and Analytical Processing, HTAP) 的融合型分布式数据库产品，具备水平扩容或者缩容、金融级高可用、实时 HTAP、云原生的分布式数据库、兼容 MySQL 协议和 MySQL 生态等重要特性，支持在本地和云上部署。\n\n<DocHomeCardContainer>\n\n<DocHomeCard href=\"/zh/tidb/stable/overview\" label=\"TiDB 简介\" icon=\"oss-product-blue\">\n\nTiDB 简介，核心特性与应用场景\n\n</DocHomeCard>\n\n<DocHomeCard href=\"/zh/tidb/stable/quick-start-with-tidb\" label=\"快速上手 TiDB\" icon=\"oss-getstarted-blue\">\n\n快速了解和使用 TiDB\n\n</DocHomeCard>\n\n<DocHomeCard href=\"/zh/tidb/stable/production-deployment-using-tiup\" label=\"部署本地 TiDB 集群\" icon=\"oss-deploy-blue\">\n\n在生产环境中部署本地 TiDB 集群\n\n</DocHomeCard>\n\n<DocHomeCard href=\"/zh/tidb/stable/dev-guide-overview\" label=\"开发者指南\" icon=\"oss-developer-blue\">\n\n为应用程序开发者编写的开发者手册\n\n</DocHomeCard>\n\n<DocHomeCard href=\"/zh/tidb/stable/mysql-compatibility\" label=\"与 MySQL 兼容性对比\" icon=\"oss-mysql-blue\">\n\nTiDB 高度兼容 MySQL 协议，以及 MySQL 5.7 和 MySQL 8.0 常用的功能及语法\n\n</DocHomeCard>\n\n<DocHomeCard href=\"/zh/tidb/dev/tidb-roadmap\" label=\"TiDB 路线图\" icon=\"oss-roadmap-blue\">\n\n提前了解 TiDB 的未来规划，关注开发进展，了解关键里程碑，并对开发工作提出反馈\n\n</DocHomeCard>\n\n</DocHomeCardContainer>\n\n</DocHomeSection>\n\n<DocHomeSection label=\"TiDB Cloud\" anchor=\"tidb-cloud\" id=\"tidb-cloud\">\n\nTiDB Cloud 是全托管的数据库即服务 (Database-as-a-Service, DBaaS) 产品，依托于公有云提供开箱即用的 TiDB 服务。TiDB Cloud 让数据库部署、运维和性能调优变得轻松简单，通过界面上的几次点击即可快速创建和管理 TiDB 集群，让您可以专注于自身业务。适用于中国出海企业和开发者。\n\n<DocHomeCardContainer>\n\n<DocHomeCard href=\"/tidbcloud/tidb-cloud-intro\" label=\"TiDB Cloud 简介\" icon=\"cloud-product-mauve\">\n\nTiDB Cloud 核心特性与应用场景简介\n\n</DocHomeCard>\n\n<DocHomeCard href=\"/tidbcloud/tidb-cloud-quickstart\" label=\"快速上手 TiDB Cloud Serverless\" icon=\"cloud-getstarted-mauve\">\n\n快速了解和使用 TiDB Cloud\n\n</DocHomeCard>\n\n<DocHomeCard href=\"/tidbcloud/dev-guide-overview\" label=\"开发者指南\" icon=\"cloud-developer-mauve\">\n\n使用你熟悉的语言或框架连接到 TiDB Cloud Serverless\n\n</DocHomeCard>\n\n</DocHomeCardContainer>\n\n体验全托管的云原生、分布式、实时 HTAP 数据库 TiDB Cloud。\n\n<a href=\"https://tidbcloud.com/free-trial\" class=\"button button-primary\" target=\"_blank\" referrerpolicy=\"no-referrer-when-downgrade\">免费试用</a>\n\n</DocHomeSection>\n\n<DocHomeSection label=\"更多资源\" anchor=\"resources\" id=\"resources\">\n\n<DocHomeCardContainer>\n\n<DocHomeCard href=\"https://pingcap.com/zh/education\" label=\"学习中心\" icon=\"global-tidb-education\">\n\n提供众多免费课程，助您深入学习 TiDB，成为 TiDB 技术专家\n\n</DocHomeCard>\n\n<DocHomeCard href=\"https://tidb.net\" label=\"TiDB 社区\" icon=\"global-tidb-community\">\n\n开发者、用户、Contributor、合作伙伴一起建立的学习和分享平台\n\n</DocHomeCard>\n\n<DocHomeCard href=\"https://pingcap.com/zh/blog\" label=\"TiDB 博客\" icon=\"global-tidb-blog\">\n\n满满的技术干货、深度解读、技术分享\n\n</DocHomeCard>\n\n<DocHomeCard href=\"https://asktug.com\" label=\"Ask TiDB User Group\" icon=\"global-tidb-asktug\">\n\n互助交流，有问有答\n\n</DocHomeCard>\n\n<DocHomeCard href=\"https://ossinsight.io/\" label=\"OSS Insight\" icon=\"global-tidb-ossinsight\">\n\n一款由 TiDB Cloud 驱动的强大有趣的洞察工具，帮您深入分析 GitHub 仓库\n\n</DocHomeCard>\n\n<DocHomeCard href=\"https://github.com/pingcap/docs-cn/blob/master/CONTRIBUTING.md\" label=\"贡献内容\" icon=\"global-tidb-contribute\">\n\n欢迎为 TiDB 文档做贡献，一起打造更好的 TiDB 文档！\n\n</DocHomeCard>\n\n</DocHomeCardContainer>\n\n</DocHomeSection>\n\n</DocHomeContainer>\n"
        },
        {
          "name": "_index.md",
          "type": "blob",
          "size": 4.7978515625,
          "content": "---\ntitle: TiDB 产品文档\naliases: ['/docs-cn/dev/','/docs-cn/','/docs-cn/stable/']\nhide_sidebar: true\nhide_commit: true\nsummary: TiDB 是 PingCAP 公司自主设计、研发的开源分布式关系型数据库。产品文档包括了 TiDB 简介、功能概览、TiFlash、快速上手 TiDB、HTAP、开发者手册概览、软硬件环境需求、使用 TiUP 部署 TiDB、数据迁移概览、运维、监控、调优、工具、TiDB 路线图、配置文件参数、命令行参数、TiDB Control、系统变量、发布历史、常见问题。\n---\n\n<LearningPathContainer platform=\"tidb\" title=\"TiDB\" subTitle=\"TiDB 是 PingCAP 公司自主设计、研发的开源分布式关系型数据库。您可以在这里查看概念介绍、操作指南、应用开发、参考等产品文档。\">\n\n<LearningPath label=\"了解\" icon=\"cloud1\">\n\n[TiDB 简介](https://docs.pingcap.com/zh/tidb/dev/overview)\n\n[功能概览](https://docs.pingcap.com/zh/tidb/dev/basic-features)\n\n[TiFlash](https://docs.pingcap.com/zh/tidb/dev/tiflash-overview)\n\n</LearningPath>\n\n<LearningPath label=\"试用\" icon=\"cloud5\">\n\n[快速上手 TiDB](https://docs.pingcap.com/zh/tidb/dev/quick-start-with-tidb)\n\n[快速上手 HTAP](https://docs.pingcap.com/zh/tidb/dev/quick-start-with-htap)\n\n[深入探索 HTAP](https://docs.pingcap.com/zh/tidb/dev/explore-htap)\n\n</LearningPath>\n\n<LearningPath label=\"开发\" icon=\"doc8\">\n\n[开发者手册概览](https://docs.pingcap.com/zh/tidb/dev/dev-guide-overview)\n\n[快速开始](https://docs.pingcap.com/zh/tidb/dev/dev-guide-build-cluster-in-cloud)\n\n[示例程序](https://docs.pingcap.com/zh/tidb/dev/dev-guide-sample-application-spring-boot)\n\n</LearningPath>\n\n<LearningPath label=\"部署\" icon=\"deploy\">\n\n[软硬件环境需求](https://docs.pingcap.com/zh/tidb/dev/hardware-and-software-requirements)\n\n[使用 TiUP 部署 TiDB（推荐）](https://docs.pingcap.com/zh/tidb/dev/production-deployment-using-tiup)\n\n[在 Kubernetes 上部署 TiDB](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable)\n\n</LearningPath>\n\n<LearningPath label=\"迁移\" icon=\"cloud3\">\n\n[数据迁移概览](https://docs.pingcap.com/zh/tidb/dev/migration-overview)\n\n[迁移工具](https://docs.pingcap.com/zh/tidb/dev/migration-tools)\n\n[应用场景](https://docs.pingcap.com/zh/tidb/dev/migrate-aurora-to-tidb)\n\n</LearningPath>\n\n<LearningPath label=\"运维\" icon=\"maintain\">\n\n[升级集群](https://docs.pingcap.com/zh/tidb/dev/upgrade-tidb-using-tiup)\n\n[扩容集群](https://docs.pingcap.com/zh/tidb/dev/scale-tidb-using-tiup)\n\n[备份与恢复](https://docs.pingcap.com/zh/tidb/dev/backup-and-restore-overview)\n\n[日常巡检](https://docs.pingcap.com/zh/tidb/dev/daily-check)\n\n[使用 TiUP 运维](https://docs.pingcap.com/zh/tidb/dev/maintain-tidb-using-tiup)\n\n</LearningPath>\n\n<LearningPath label=\"监控\" icon=\"cloud6\">\n\n[使用 Prometheus 和 Grafana](https://docs.pingcap.com/zh/tidb/dev/tidb-monitoring-framework)\n\n[监控 API](https://docs.pingcap.com/zh/tidb/dev/tidb-monitoring-api)\n\n[报警规则](https://docs.pingcap.com/zh/tidb/dev/alert-rules)\n\n</LearningPath>\n\n<LearningPath label=\"调优\" icon=\"tidb-cloud-tune\">\n\n[调优概述](https://docs.pingcap.com/zh/tidb/dev/performance-tuning-overview)\n\n[调优方法](https://docs.pingcap.com/zh/tidb/dev/performance-tuning-methods)\n\n[调优实践](https://docs.pingcap.com/zh/tidb/dev/performance-tuning-practices)\n\n[操作系统性能参数调优](https://docs.pingcap.com/zh/tidb/dev/tune-operating-system)\n\n[内存调优](https://docs.pingcap.com/zh/tidb/dev/configure-memory-usage)\n\n[SQL 调优](https://docs.pingcap.com/zh/tidb/dev/sql-tuning-overview)\n\n</LearningPath>\n\n<LearningPath label=\"工具\" icon=\"doc7\">\n\n[TiUP](https://docs.pingcap.com/zh/tidb/dev/tiup-overview)\n\n[TiDB Operator](https://docs.pingcap.com/zh/tidb/dev/tidb-operator-overview)\n\n[TiDB Data Migration (DM)](https://docs.pingcap.com/zh/tidb/dev/dm-overview)\n\n[TiDB Lightning](https://docs.pingcap.com/zh/tidb/dev/tidb-lightning-overview)\n\n[Dumpling](https://docs.pingcap.com/zh/tidb/dev/dumpling-overview)\n\n[TiCDC](https://docs.pingcap.com/zh/tidb/dev/ticdc-overview)\n\n[Backup & Restore (BR)](https://docs.pingcap.com/zh/tidb/dev/backup-and-restore-overview)\n\n[PingCAP Clinic](https://docs.pingcap.com/zh/tidb/dev/clinic-introduction)\n\n</LearningPath>\n\n<LearningPath label=\"参考\" icon=\"cloud-dev\">\n\n[TiDB 路线图](https://docs.pingcap.com/zh/tidb/dev/tidb-roadmap)\n\n[TiDB 配置文件参数](https://docs.pingcap.com/zh/tidb/dev/tidb-configuration-file)\n\n[TiDB 命令行参数](https://docs.pingcap.com/zh/tidb/dev/command-line-flags-for-tidb-configuration)\n\n[TiDB Control](https://docs.pingcap.com/zh/tidb/dev/tidb-control)\n\n[系统变量](https://docs.pingcap.com/zh/tidb/dev/system-variables)\n\n[发布历史](https://docs.pingcap.com/zh/tidb/dev/release-notes)\n\n[常见问题](https://docs.pingcap.com/zh/tidb/dev/faq-overview)\n\n</LearningPath>\n\n</LearningPathContainer>\n"
        },
        {
          "name": "accelerated-table-creation.md",
          "type": "blob",
          "size": 2.1669921875,
          "content": "---\ntitle: 提升 TiDB 建表性能\nsummary: 介绍 TiDB 加速建表中的概念、原理、实现和影响。\naliases: ['/zh/tidb/dev/ddl-v2/']\n---\n\n# 提升 TiDB 建表性能\n\nTiDB v7.6.0 引入了系统变量 [`tidb_ddl_version`](https://docs.pingcap.com/zh/tidb/v7.6/system-variables#tidb_ddl_version-从-v760-版本开始引入) 实现支持加速建表，可提升大批量建表的速度。从 v8.0.0 开始，该系统变量更名为 [`tidb_enable_fast_create_table`](/system-variables.md#tidb_enable_fast_create_table-从-v800-版本开始引入)。\n\n通过 [`tidb_enable_fast_create_table`](/system-variables.md#tidb_enable_fast_create_table-从-v800-版本开始引入) 系统变量开启加速建表后，同时提交到同一个 TiDB 节点的相同 schema 的建表语句会被合并为批量建表语句，以提高建表性能。因此为了提高建表性能，需要尽量连接相同的 TiDB 节点并发创建同一个 schema 下的表，并适当提高并发度。\n\n合并后的批量建表语句在同一个事务内执行，如果其中一个语句失败，所有语句都会失败。\n\n## 与 TiDB 工具的兼容性\n\n- 在 TiDB v8.3.0 之前的版本中，[TiCDC](/ticdc/ticdc-overview.md) 不支持同步通过 TiDB 加速创建的表。从 v8.3.0 开始，TiCDC 可以正常同步这类表。\n\n## 限制\n\nTiDB 加速建表目前仅适用于 [`CREATE TABLE`](/sql-statements/sql-statement-create-table.md) 语句，且该建表语句不带任何外键约束。\n\n## 使用方法\n\n你可以通过设置系统变量 [`tidb_enable_fast_create_table`](/system-variables.md#tidb_enable_fast_create_table-从-v800-版本开始引入) 的值来开启或关闭加速建表的功能。\n\n从 TiDB v8.5.0 开始，新创建的集群默认开启 TiDB 加速建表功能，即 `tidb_enable_fast_create_table` 默认值为 `ON`。如果从 v8.4.0 及之前版本的集群升级至 v8.5.0 及之后的版本，`tidb_enable_fast_create_table` 的默认值不发生变化。\n\n要开启该功能，将该变量的值设置为 `ON`：\n\n```sql\nSET GLOBAL tidb_enable_fast_create_table = ON;\n```\n\n要关闭该功能，将该变量的值设置为 `OFF`：\n\n```sql\nSET GLOBAL tidb_enable_fast_create_table = OFF;\n```\n"
        },
        {
          "name": "agg-distinct-optimization.md",
          "type": "blob",
          "size": 4.2783203125,
          "content": "---\ntitle: Distinct 优化\naliases: ['/docs-cn/dev/agg-distinct-optimization/']\nsummary: 本文介绍了对于 DISTINCT 的优化，包括简单 DISTINCT 和聚合函数 DISTINCT 的优化。简单的 DISTINCT 通常会被优化成 GROUP BY 来执行。而带有 DISTINCT 的聚合函数会在 TiDB 侧单线程执行，可以通过系统变量或 TiDB 配置项控制优化器是否执行。在优化后，DISTINCT 被下推到了 Coprocessor，在 HashAgg 里新增了一个 group by 列。\n---\n\n# Distinct 优化\n\n本文档介绍可用于 `DISTINCT` 的优化，包括简单 `DISTINCT` 和聚合函数 `DISTINCT` 的优化。\n\n## 简单 DISTINCT\n\n通常简单的 `DISTINCT` 会被优化成 GROUP BY 来执行。例如：\n\n```sql\nmysql> explain select DISTINCT a from t;\n+--------------------------+---------+-----------+---------------+-------------------------------------------------------+\n| id                       | estRows | task      | access object | operator info                                         |\n+--------------------------+---------+-----------+---------------+-------------------------------------------------------+\n| HashAgg_6                | 2.40    | root      |               | group by:test.t.a, funcs:firstrow(test.t.a)->test.t.a |\n| └─TableReader_11         | 3.00    | root      |               | data:TableFullScan_10                                 |\n|   └─TableFullScan_10     | 3.00    | cop[tikv] | table:t       | keep order:false, stats:pseudo                        |\n+--------------------------+---------+-----------+---------------+-------------------------------------------------------+\n3 rows in set (0.00 sec)\n```\n\n## 聚合函数 DISTINCT\n\n通常来说，带有 `DISTINCT` 的聚合函数会单线程的在 TiDB 侧执行。使用系统变量 [`tidb_opt_distinct_agg_push_down`](/system-variables.md#tidb_opt_distinct_agg_push_down) 或者 TiDB 的配置项 [distinct-agg-push-down](/tidb-configuration-file.md#distinct-agg-push-down) 控制优化器是否执行带有 `DISTINCT` 的聚合函数（比如 `select count(distinct a) from t`）下推到 Coprocessor 的优化操作。\n\n在以下示例中，`tidb_opt_distinct_agg_push_down` 开启前，TiDB 需要从 TiKV 读取所有数据，并在 TiDB 侧执行 `disctinct`。`tidb_opt_distinct_agg_push_down` 开启后，`distinct a` 被下推到了 Coprocessor，在 `HashAgg_5` 里新增了一个 `group by` 列 `test.t.a`。\n\n```sql\nmysql> desc select count(distinct a) from test.t;\n+-------------------------+----------+-----------+---------------+------------------------------------------+\n| id                      | estRows  | task      | access object | operator info                            |\n+-------------------------+----------+-----------+---------------+------------------------------------------+\n| StreamAgg_6             | 1.00     | root      |               | funcs:count(distinct test.t.a)->Column#4 |\n| └─TableReader_10        | 10000.00 | root      |               | data:TableFullScan_9                     |\n|   └─TableFullScan_9     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo           |\n+-------------------------+----------+-----------+---------------+------------------------------------------+\n3 rows in set (0.01 sec)\n\nmysql> set session tidb_opt_distinct_agg_push_down = 1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> desc select count(distinct a) from test.t;\n+---------------------------+----------+-----------+---------------+------------------------------------------+\n| id                        | estRows  | task      | access object | operator info                            |\n+---------------------------+----------+-----------+---------------+------------------------------------------+\n| HashAgg_8                 | 1.00     | root      |               | funcs:count(distinct test.t.a)->Column#3 |\n| └─TableReader_9           | 1.00     | root      |               | data:HashAgg_5                           |\n|   └─HashAgg_5             | 1.00     | cop[tikv] |               | group by:test.t.a,                       |\n|     └─TableFullScan_7     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo           |\n+---------------------------+----------+-----------+---------------+------------------------------------------+\n4 rows in set (0.00 sec)\n```\n"
        },
        {
          "name": "alert-rules.md",
          "type": "blob",
          "size": 34.220703125,
          "content": "---\ntitle: TiDB 集群报警规则\nsummary: TiDB 集群中各组件的报警规则详解。\naliases: ['/docs-cn/dev/alert-rules/','/docs-cn/dev/reference/alert-rules/']\n---\n\n# TiDB 集群报警规则\n\n本文介绍了 TiDB 集群中各组件的报警规则，包括 TiDB、TiKV、PD、TiFlash、TiCDC、Node_exporter 和 Blackbox_exporter 的各报警项的规则描述及处理方法。\n\n按照严重程度由高到低，报警项可分为紧急级别 \\> 严重级别 \\> 警告级别三类。该分级适用于以下各组件的报警项。\n\n|  严重程度 |  说明   |\n| :-------- | :----- |\n|  紧急级别  | 最高严重程度，服务不可用，通常由于服务停止或节点故障导致，此时需要**马上进行人工干预** |\n|  严重级别  |  服务可用性下降，需要用户密切关注异常指标 |\n|  警告级别  |  对某一问题或错误的提醒   |\n\n## TiDB 报警规则\n\n本节介绍了 TiDB 组件的报警项。\n\n### 紧急级别报警项\n\n#### `TiDB_schema_error`\n\n* 报警规则：\n\n    `increase(tidb_session_schema_lease_error_total{type=\"outdated\"}[15m]) > 0`\n\n* 规则描述：\n\n    TiDB 在一个 Lease 时间内没有重载到最新的 Schema 信息。如果 TiDB 无法继续对外提供服务，则报警。\n\n* 处理方法：\n\n    该问题通常由于 TiKV Region 不可用或超时导致，需要看 TiKV 的监控指标定位问题。\n\n#### `TiDB_tikvclient_region_err_total`\n\n* 报警规则：\n\n    `increase(tidb_tikvclient_region_err_total[10m]) > 6000`\n\n* 规则描述：\n\n    TiDB server 根据自己的缓存信息访问 TiKV 的 Region leader。如果 Region leader 有变化或者当前 TiKV 的 Region 信息与 TiDB 的缓存不一致，就会产生 Region 缓存错误。如果在 10 分钟之内该错误多于 6000 次，则报警。\n\n* 处理方法：\n\n    查看 [**TiKV-Details** > **Cluster** 面板](/grafana-tikv-dashboard.md#cluster)，检查 leader 的分布是否均衡。\n\n#### `TiDB_domain_load_schema_total`\n\n* 报警规则：\n\n    `increase(tidb_domain_load_schema_total{type=\"failed\"}[10m]) > 10`\n\n* 规则描述：\n\n    TiDB 重载最新的 Schema 信息失败的总次数。如果在 10 分钟之内重载失败次数超过 10 次，则报警。\n\n* 处理方法：\n\n    参考 [`TiDB_schema_error`](#tidb_schema_error) 的处理方法。\n\n### 严重级别报警项\n\n#### `TiDB_server_panic_total`\n\n* 报警规则：\n\n    `increase(tidb_server_panic_total[10m]) > 0`\n\n* 规则描述：\n\n    发生崩溃的 TiDB 线程的数量。当出现崩溃的时候会报警。该线程通常会被恢复，否则 TiDB 会频繁重启。\n\n* 处理方法：\n\n    收集 panic 日志，定位原因。\n\n### 警告级别报警项\n\n#### `TiDB_memory_abnormal`\n\n* 报警规则：\n\n    `go_memstats_heap_inuse_bytes{job=\"tidb\"} > 1e+10`\n\n* 规则描述：\n\n    对 TiDB 内存使用量的监控。如果内存使用大于 10 G，则报警。\n\n* 处理方法：\n\n    通过 HTTP API 来排查 goroutine 泄露的问题。\n\n#### `TiDB_query_duration`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket[1m])) BY (le, instance)) > 1`\n\n* 规则描述：\n\n    TiDB 处理请求的延时。99% 的请求的响应时间都应在 1 秒之内，否则报警。\n\n* 处理方法：\n\n    查看 TiDB 的日志，搜索 SLOW_QUERY 和 TIME_COP_PROCESS 关键字，查找慢 SQL。\n\n#### `TiDB_server_event_error`\n\n* 报警规则：\n\n    `increase(tidb_server_event_total{type=~\"server_start|server_hang\"}[15m]) > 0`\n\n* 规则描述：\n\n    TiDB 服务中发生的事件数量。当出现以下事件的时候会报警：\n\n    1. start：TiDB 服务启动。\n    2. hang：当发生了 Critical 级别的事件时，TiDB 进入 `hang` 模式，并等待人工 Kill。\n\n* 处理方法：\n\n    重启 TiDB 以恢复服务。\n\n#### `TiDB_tikvclient_backoff_seconds_count`\n\n* 报警规则：\n\n    `increase(tidb_tikvclient_backoff_seconds_count[10m]) > 10`\n\n* 规则描述：\n\n    TiDB 访问 TiKV 发生错误时发起重试的次数。如果在 10 分钟之内重试次数多于 10 次，则报警。\n\n* 处理方法：\n\n    查看 TiKV 的监控状态。\n\n#### `TiDB_monitor_time_jump_back_error`\n\n* 报警规则：\n\n    `increase(tidb_monitor_time_jump_back_total[10m]) > 0`\n\n* 规则描述：\n\n    如果 TiDB 所在机器的时间发生了回退，则报警。\n\n* 处理方法：\n\n    排查 NTP 配置。\n\n#### `TiDB_ddl_waiting_jobs`\n\n* 报警规则：\n\n    `sum(tidb_ddl_waiting_jobs) > 5`\n\n* 规则描述：\n\n    如果 TiDB 中等待执行的 DDL 任务的数量大于 5，则报警。\n\n* 处理方法：\n\n    通过 `admin show ddl` 语句检查是否有耗时的 add index 操作正在执行。\n\n## PD 报警规则\n\n本节介绍了 PD 组件的报警项。\n\n### 紧急级别报警项\n\n#### `PD_cluster_down_store_nums`\n\n* 报警规则：\n\n    `(sum(pd_cluster_status{type=\"store_down_count\"}) by (instance) > 0) and (sum(etcd_server_is_leader) by (instance) > 0)`\n\n* 规则描述：\n\n    PD 长时间（默认配置是 30 分钟）没有收到 TiKV/TiFlash 心跳。\n\n* 处理方法：\n\n    * 检查 TiKV/TiFlash 进程是否正常、网络是否隔离以及负载是否过高，并尽可能地恢复服务。\n    * 如果确定 TiKV/TiFlash 无法恢复，可做下线处理。\n\n### 严重级别报警项\n\n#### `PD_etcd_write_disk_latency`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(etcd_disk_wal_fsync_duration_seconds_bucket[1m])) by (instance, job, le)) > 1`\n\n* 规则描述：\n\n    fsync 操作延迟大于 1s，代表 etcd 写盘慢，这很容易引起 PD leader 超时或者 TSO 无法及时存盘等问题，从而导致整个集群停止服务。\n\n* 处理方法：\n\n    * 排查写入慢的原因。可能是由于其他服务导致系统负载过高。可以检查 PD 本身是否占用了大量 CPU 或 IO 资源。\n    * 可尝试重启 PD 或手动 transfer leader 至其他的 PD 来恢复服务。\n    * 如果由于环境原因无法恢复，可将有问题的 PD 下线替换。\n\n#### `PD_miss_peer_region_count`\n\n* 报警规则：\n\n    `(sum(pd_regions_status{type=\"miss-peer-region-count\"}) by (instance)  > 100) and (sum(etcd_server_is_leader) by (instance) > 0)`\n\n* 规则描述：\n\n    Region 的副本数小于 `max-replicas` 配置的值。\n\n* 处理方法：\n\n    * 查看是否有 TiKV 宕机或在做下线操作，尝试定位问题产生的原因。\n    * 观察 region health 面板，查看 `miss-peer-region-count` 是否在不断减少。\n\n### 警告级别报警项\n\n#### `PD_cluster_lost_connect_store_nums`\n\n* 报警规则：\n\n    `(sum(pd_cluster_status{type=\"store_disconnected_count\"}) by (instance) > 0) and (sum(etcd_server_is_leader) by (instance) > 0)`\n\n* 规则描述：\n\n    PD 在 20 秒之内未收到 TiKV/TiFlash 上报心跳。正常情况下是每 10 秒收到 1 次心跳。\n\n* 处理方法：\n\n    * 排查是否在重启 TiKV/TiFlash。\n    * 检查 TiKV/TiFlash 进程是否正常、网络是否隔离以及负载是否过高，并尽可能地恢复服务。\n    * 如果确定 TiKV/TiFlash 无法恢复，可做下线处理。\n    * 如果确定 TiKV/TiFlash 可以恢复，但在短时间内还无法恢复，可以考虑延长 `max-down-time` 配置，防止超时后 TiKV/TiFlash 被判定为无法恢复并开始搬移数据。\n\n#### `PD_cluster_unhealthy_tikv_nums`\n\n* 报警规则：\n\n    `(sum(pd_cluster_status{type=\"store_unhealth_count\"}) by (instance) > 0) and (sum(etcd_server_is_leader) by (instance) > 0)`\n\n* 规则描述：\n\n    表示存在异常状态的 Store。如果这种状态持续一段时间（取决于配置的 [`max-store-down-time`](/pd-configuration-file.md#max-store-down-time)，默认值为 `30m`），Store 可能会变为 `Offline` 状态并触发 [`PD_cluster_down_store_nums`](#pd_cluster_down_store_nums) 报警。\n\n* 处理方法：\n\n    检查 TiKV Store 的状态。\n\n#### `PD_cluster_low_space`\n\n* 报警规则：\n\n    `(sum(pd_cluster_status{type=\"store_low_space_count\"}) by (instance) > 0) and (sum(etcd_server_is_leader) by (instance) > 0)`\n\n* 规则描述：\n\n    表示 TiKV/TiFlash 节点空间不足。\n\n* 处理方法：\n\n    * 检查集群中的空间是否普遍不足。如果是，则需要扩容。\n    * 检查 Region balance 调度是否有问题。如果有问题，会导致数据分布不均衡。\n    * 检查是否有文件占用了大量磁盘空间，比如日志、快照、core dump 等文件。\n    * 降低该节点的 Region weight 来减少数据量。\n    * 无法释放空间时，可以考虑主动下线该节点，防止由于磁盘空间不足而宕机。\n\n#### `PD_etcd_network_peer_latency`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(etcd_network_peer_round_trip_time_seconds_bucket[1m])) by (To, instance, job, le)) > 1`\n\n* 规则描述：\n\n    PD 节点之间网络延迟高，严重情况下会导致 leader 超时和 TSO 存盘超时，从而影响集群服务。\n\n* 处理方法：\n\n    * 检查网络状况和系统负载情况。\n    * 如果由于环境原因无法恢复，可将有问题的 PD 下线替换。\n\n#### `PD_tidb_handle_requests_duration`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(pd_client_request_handle_requests_duration_seconds_bucket{type=\"tso\"}[1m])) by (instance, job, le)) > 0.1`\n\n* 规则描述：\n\n    PD 处理 TSO 请求耗时过长，一般是由于负载过高。\n\n* 处理方法：\n\n    * 检查服务器负载状况。\n    * 使用 pprof 抓取 PD 的 CPU profile 进行分析。\n    * 手动切换 PD leader。\n    * 如果是环境问题，则将有问题的 PD 下线替换。\n\n#### `PD_down_peer_region_nums`\n\n* 报警规则：\n\n    `(sum(pd_regions_status{type=\"down-peer-region-count\"}) by (instance)  > 0) and (sum(etcd_server_is_leader) by (instance) > 0)`\n\n* 规则描述：\n\n    Raft leader 上报有不响应 peer 的 Region 数量。\n\n* 处理方法：\n\n    * 检查是否有 TiKV 宕机，或刚发生重启，或者繁忙。\n    * 观察 region health 面板，检查 `down_peer_region_count` 是否在不断减少。\n    * 检查是否有 TiKV 之间网络不通。\n\n#### `PD_pending_peer_region_count`\n\n* 报警规则：\n\n    `(sum(pd_regions_status{type=\"pending-peer-region-count\"}) by (instance) > 100) and (sum(etcd_server_is_leader) by (instance) > 0)`\n\n* 规则描述：\n\n    Raft log 落后的 Region 过多。由于调度产生少量的 pending peer 是正常的，但是如果持续很高，就可能有问题。\n\n* 处理方法：\n\n    * 观察 region health 面板，检查 `pending_peer_region_count` 是否在不断减少。\n    * 检查 TiKV 之间的网络状况，特别是带宽是否足够。\n\n#### `PD_leader_change`\n\n* 报警规则：\n\n    `count(changes(pd_tso_events{type=\"save\"}[10m]) > 0) >= 2`\n\n* 规则描述：\n\n    近期发生了 PD leader 切换。\n\n* 处理方法：\n\n    * 排除人为因素，比如重启 PD、手动 transfer leader 或调整 leader 优先级等。\n    * 检查网络状况和系统负载情况。\n    * 如果由于环境原因无法恢复，可将有问题的 PD 下线替换。\n\n#### `TiKV_space_used_more_than_80%`\n\n* 报警规则：\n\n    `sum(pd_cluster_status{type=\"storage_size\"}) / sum(pd_cluster_status{type=\"storage_capacity\"}) * 100 > 80`\n\n* 规则描述：\n\n    集群空间占用超过 80%。\n\n* 处理方法：\n\n    * 确认是否需要扩容。\n    * 排查是否有文件占用了大量磁盘空间，比如日志、快照或 core dump 等文件。\n\n#### `PD_system_time_slow`\n\n* 报警规则：\n\n    `changes(pd_tso_events{type=\"system_time_slow\"}[10m]) >= 1`\n\n* 规则描述：\n\n    系统时间可能发生回退。\n\n* 处理方法：\n\n    检查系统时间设置是否正确。\n\n#### `PD_no_store_for_making_replica`\n\n* 报警规则：\n\n    `increase(pd_checker_event_count{type=\"replica_checker\", name=\"no_target_store\"}[1m]) > 0`\n\n* 规则描述：\n\n    没有合适的 store 用来补副本。\n\n* 处理方法：\n\n    * 检查 store 是否空间不足。\n    * 根据 label 配置（如果有这个配置的话）来检查是否有可以补副本的 store。\n\n#### `PD_cluster_slow_tikv_nums`\n\n* 报警规则：\n\n    `sum(pd_cluster_status{type=\"store_slow_count\"}) by (instance) > 0) and (sum(etcd_server_is_leader) by (instance) > 0`\n\n* 规则描述：\n\n    某一个 TiKV 被检测为慢节点。慢节点的检测由 TiKV `raftstore.inspect-interval` 参数控制，参见 [TiKV 配置文件描述](/tikv-configuration-file.md#inspect-interval)。\n\n* 处理方法：\n\n    * 观察 [**TiKV-Details** > **PD** 面板](/grafana-tikv-dashboard.md#pd)，查看 Store Slow Score 监控指标，找出指标数值超过 80 的节点，该节点即为被检测到的慢节点。\n    * 观察 [**TiKV-Details** > **Raft IO** 面板](/grafana-tikv-dashboard.md#raft-io)，查看延迟是否升高。如果延迟很高，表明磁盘可能存在瓶颈。\n    * 调大 TiKV [`raftstore.inspect-interval`](/tikv-configuration-file.md#inspect-interval) 参数，提高延迟检测的超时上限。\n    * 如果需要进一步分析报警的 TiKV 节点的性能问题，找到优化方法，可以参考[性能分析和优化方法](/performance-tuning-methods.md#storage-async-write-durationstore-duration-和-apply-duration)。\n\n## TiKV 报警规则\n\n本节介绍了 TiKV 组件的报警项。\n\n### 紧急级别报警项\n\n#### `TiKV_memory_used_too_fast`\n\n* 报警规则：\n\n    `process_resident_memory_bytes{job=~\"tikv\",instance=~\".*\"} - (process_resident_memory_bytes{job=~\"tikv\",instance=~\".*\"} offset 5m) > 5*1024*1024*1024`\n\n* 规则描述：\n\n    目前没有和内存相关的 TiKV 的监控，你可以通过 Node_exporter 监控集群内机器的内存使用情况。如上规则表示，如果在 5 分钟之内内存使用超过 5GB（TiKV 内存占用的太快），则报警。\n\n* 处理方法：\n\n    调整 `rocksdb.defaultcf` 和 `rocksdb.writecf` 的 `block-cache-size` 的大小。\n\n#### `TiKV_GC_can_not_work`\n\n* 报警规则：\n\n    `sum(increase(tikv_gcworker_gc_tasks_vec{task=\"gc\"}[1d])) < 1 and (sum(increase(tikv_gc_compaction_filter_perform[1d])) < 1 and sum(increase(tikv_engine_event_total{db=\"kv\", cf=\"write\", type=\"compaction\"}[1d])) >= 1)`\n\n* 规则描述：\n\n    在 24 小时内一个 TiKV 实例上没有成功执行 GC，说明 GC 不能正常工作了。短期内 GC 不运行不会造成太大的影响，但如果 GC 一直不运行，版本会越来越多，从而导致查询变慢。\n\n* 处理方法：\n\n    1. 执行 `SELECT VARIABLE_VALUE FROM mysql.tidb WHERE VARIABLE_NAME=\"tikv_gc_leader_desc\"` 来找到 gc leader 对应的 `tidb-server`；\n    2. 查看该 `tidb-server` 的日志，grep gc_worker tidb.log；\n    3. 如果发现这段时间一直在 resolve locks（最后一条日志是 `start resolve locks`）或者 delete ranges（最后一条日志是 `start delete {number} ranges`），说明 GC 进程是正常的。否则请从 PingCAP 官方或 TiDB 社区[获取支持](/support.md)。\n\n### 严重级别报警项\n\n#### `TiKV_server_report_failure_msg_total`\n\n* 报警规则：\n\n    `sum(rate(tikv_server_report_failure_msg_total{type=\"unreachable\"}[10m])) BY (store_id) > 10`\n\n* 规则描述：\n\n    表明无法连接远端的 TiKV。\n\n* 处理方法：\n\n    1. 检查网络是否通畅。\n    2. 检查远端 TiKV 是否挂掉。\n    3. 如果远端 TiKV 没有挂掉，检查压力是否太大，参考 [`TiKV_channel_full_total`](#tikv_channel_full_total) 处理方法。\n\n#### `TiKV_channel_full_total`\n\n* 报警规则：\n\n    `sum(rate(tikv_channel_full_total[10m])) BY (type, instance) > 0`\n\n* 规则描述：\n\n    该错误通常是因为 Raftstore 线程卡死，TiKV 的压力已经非常大了。\n\n* 处理方法：\n\n    1. 观察 [**TiKV-Details** > **Raft Propose** 面板](/grafana-tikv-dashboard.md#raft-propose)，查看这个报警的 TiKV 节点是否明显比其他 TiKV 高很多。如果是，表明这个 TiKV 上有热点，需要检查热点调度是否能正常工作。\n    2. 观察 [**TiKV-Details** > **Raft IO** 面板](/grafana-tikv-dashboard.md#raft-io)，查看延迟是否升高。如果延迟很高，表明磁盘可能存在瓶颈。\n    3. 观察 [**TiKV-Details** > **Raft process** 面板](/grafana-tikv-dashboard.md#raft-process)，关注 `tick duration` 是否很高。如果是，需要将 TiKV 配置项 [`raftstore.raft-base-tick-interval`](/tikv-configuration-file.md#raft-base-tick-interval) 设置为 `\"2s\"`。\n\n#### `TiKV_write_stall`\n\n* 报警规则：\n\n    `delta(tikv_engine_write_stall[10m]) > 0`\n\n* 规则描述：\n\n    RocksDB 写入压力太大，出现了 stall。\n\n* 处理方法：\n\n    1. 观察磁盘监控，排除磁盘问题。\n    2. 看 TiKV 是否有写入热点。\n    3. 在 `[rocksdb]` 和 `[raftdb]` 配置下调大 `max-sub-compactions` 的值。\n\n#### `TiKV_raft_log_lag`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tikv_raftstore_log_lag_bucket[1m])) by (le, instance)) > 5000`\n\n* 规则描述：\n\n    这个值偏大，表明 Follower 已经远远落后于 Leader，Raft 没法正常同步了。可能的原因是 Follower 所在的 TiKV 卡住或者挂掉了。\n\n#### `TiKV_async_request_snapshot_duration_seconds`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tikv_storage_engine_async_request_duration_seconds_bucket{type=\"snapshot\"}[1m])) by (le, instance, type)) > 1`\n\n* 规则描述：\n\n    这个值偏大，表明 Raftstore 负载压力很大，可能已经卡住。\n\n* 处理方法：\n\n    参考 [`TiKV_channel_full_total`](#tikv_channel_full_total) 的处理方法。\n\n#### `TiKV_async_request_write_duration_seconds`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tikv_storage_engine_async_request_duration_seconds_bucket{type=\"write\"}[1m])) by (le, instance, type)) > 1`\n\n* 规则描述：\n\n    这个值偏大，表明 Raft write 耗时很长。\n\n* 处理方法：\n\n    1. 观察 [**TiKV-Details** > **Raft propose** 面板](/grafana-tikv-dashboard.md#raft-propose)，查看这个报警的 TiKV 节点的 **99% Propose wait duration per server** 是否明显比其他 TiKV 高很多。如果是，表明这个 TiKV 上有热点，需要检查热点调度是否能正常工作。\n    2. 观察 [**TiKV-Details** > **Raft IO** 面板](/grafana-tikv-dashboard.md#raft-io)，查看延迟是否升高。如果延迟很高，表明磁盘可能存在瓶颈。\n    3. 如果需要进一步分析报警的 TiKV 节点的性能问题，找到优化方法，可以参考[性能分析和优化方法](/performance-tuning-methods.md#storage-async-write-durationstore-duration-和-apply-duration)。\n\n#### `TiKV_coprocessor_request_wait_seconds`\n\n* 报警规则：\n\n    `histogram_quantile(0.9999, sum(rate(tikv_coprocessor_request_wait_seconds_bucket[1m])) by (le, instance, req)) > 10`\n\n* 规则描述：\n\n    这个值偏大，表明 Coprocessor worker 压力很大。可能有比较慢的任务卡住了 Coprocessor 线程。\n\n* 处理方法：\n\n    1. 从 TiDB 日志中查看慢查询日志，看查询是否用到了索引或全表扫，或者看是否需要做 analyze。\n    2. 排查是否有热点。\n    3. 查看 Coprocessor 监控，看 `coprocessor table/index scan` 里 `total` 和 `process` 是否匹配。如果相差太大，表明做了太多的无效查询。看是否有 `over seek bound`，如果有，表明版本太多，GC 工作不及时，需要增大并行 GC 的线程数。\n\n#### `TiKV_raftstore_thread_cpu_seconds_total`\n\n* 报警规则：\n\n    `sum(rate(tikv_thread_cpu_seconds_total{name=~\"raftstore_.*\"}[1m])) by (instance) > 1.6`\n\n* 规则描述：\n\n    监测 raftstore 的 CPU 消耗。如果该值偏大，表明 Raftstore 线程压力很大。\n\n    该报警项的阈值为 [`raftstore.store-pool-size`](/tikv-configuration-file.md#store-pool-size) 的 80%。`raftstore.store-pool-size` 默认为 2，所以该阈值为 1.6。 \n\n* 处理方法：\n\n    参考 [`TiKV_channel_full_total`](#tikv_channel_full_total) 的处理方法。\n\n#### `TiKV_raft_append_log_duration_secs`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tikv_raftstore_append_log_duration_seconds_bucket[1m])) by (le, instance)) > 1`\n\n* 规则描述：\n\n    表示 append Raft log 的耗时，如果高，通常是因为 IO 太忙了。\n\n#### `TiKV_raft_apply_log_duration_secs`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tikv_raftstore_apply_log_duration_seconds_bucket[1m])) by (le, instance)) > 1`\n\n* 规则描述：\n\n    表示 apply Raft log 耗时，如果高，通常是因为 IO 太忙了。\n\n#### `TiKV_scheduler_latch_wait_duration_seconds`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tikv_scheduler_latch_wait_duration_seconds_bucket[1m])) by (le, instance, type)) > 1`\n\n* 规则描述：\n\n    Scheduler 中写操作获取内存锁时的等待时间。如果这个值高，表明写操作冲突较多，也可能是某些引起冲突的操作耗时较长，阻塞了其它等待相同锁的操作。\n\n* 处理方法：\n\n    1. 查看 Scheduler-All 监控中的 scheduler command duration，看哪一个命令耗时最大。\n    2. 查看 Scheduler-All 监控中的 scheduler scan details，看 `total` 和 `process` 是否匹配。如果相差太大，表明有很多无效的扫描，另外观察是否有 `over seek bound`，如果太多，表明 GC 不及时。\n    3. 查看 Storage 监控中的 storage async snapshot/write duration，看是否 Raft 操作不及时。\n\n#### `TiKV_thread_apply_worker_cpu_seconds`\n\n* 报警规则：\n\n    `max(rate(tikv_thread_cpu_seconds_total{name=~\"apply_.*\"}[1m])) by (instance) > 0.9`\n\n* 规则描述：\n\n    Apply Raft log 线程压力太大，已经接近或超过 apply 线程的处理上限。通常是因为短期内写入的数据量太多造成的。\n\n### 警告级别报警项\n\n#### `TiKV_leader_drops`\n\n* 报警规则：\n\n    `delta(tikv_pd_heartbeat_tick_total{type=\"leader\"}[30s]) < -10`\n\n* 规则描述：\n\n    该问题通常是因为 Raftstore 线程卡住了。\n\n* 处理方法：\n\n    1. 参考 [`TiKV_channel_full_total`](#tikv_channel_full_total) 的处理方法。\n    2. 如果 TiKV 压力很小，考虑 PD 的调度是否太频繁。可以查看 PD 页面的 Operator Create 面板，排查 PD 产生调度的类型和数量。\n\n#### `TiKV_raft_process_ready_duration_secs`\n\n* 报警规则：\n\n    `histogram_quantile(0.999, sum(rate(tikv_raftstore_raft_process_duration_secs_bucket{type='ready'}[1m])) by (le, instance, type)) > 2`\n\n* 规则描述：\n\n    表示处理 Raft ready 的耗时。这个值大，通常是因为 append log 任务卡住了。\n\n#### `TiKV_raft_process_tick_duration_secs`\n\n* 报警规则：\n\n    `histogram_quantile(0.999, sum(rate(tikv_raftstore_raft_process_duration_secs_bucket{type=’tick’}[1m])) by (le, instance, type)) > 2`\n\n* 规则描述：\n\n    表示处理 Raft tick 的耗时，这个值大，通常是因为 Region 太多导致的。\n\n* 处理方法：\n\n    1. 考虑使用更高等级的日志，比如 `warn` 或者 `error`。\n    2. 在 `[raftstore]` 配置下添加 `raft-base-tick-interval = “2s”`。\n\n#### `TiKV_scheduler_context_total`\n\n* 报警规则：\n\n    `abs(delta( tikv_scheduler_context_total[5m])) > 1000`\n\n* 规则描述：\n\n    Scheduler 正在执行的写命令数量。这个值高，表示任务完成得不及时。\n\n* 处理方法：\n\n    参考 [`TiKV_scheduler_latch_wait_duration_seconds`](#tikv_scheduler_latch_wait_duration_seconds) 的处理方法。\n\n#### `TiKV_scheduler_command_duration_seconds`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tikv_scheduler_command_duration_seconds_bucket[1m])) by (le, instance, type)) > 1`\n\n* 规则描述：\n\n    表明 Scheduler 执行命令的耗时。\n\n* 处理方法：\n\n    参考 [`TiKV_scheduler_latch_wait_duration_seconds`](#tikv_scheduler_latch_wait_duration_seconds) 的处理方法。\n\n#### `TiKV_coprocessor_outdated_request_wait_seconds`\n\n* 报警规则：\n\n    `delta(tikv_coprocessor_outdated_request_wait_seconds_count[10m]) > 0`\n\n* 规则描述：\n\n    Coprocessor 已经过期的请求等待的时间。这个值高，表示 Coprocessor 压力已经非常大了。\n\n* 处理方法：\n\n    参考 [`TiKV_coprocessor_request_wait_seconds`](#tikv_coprocessor_request_wait_seconds) 的处理方法。\n\n#### `TiKV_coprocessor_pending_request`\n\n* 报警规则：\n\n    `delta(tikv_coprocessor_pending_request[10m]) > 5000`\n\n* 规则描述：\n\n    Coprocessor 排队的请求。\n\n* 处理方法：\n\n    参考 [`TiKV_coprocessor_request_wait_seconds`](#tikv_coprocessor_request_wait_seconds) 的处理方法。\n\n#### `TiKV_batch_request_snapshot_nums`\n\n* 报警规则：\n\n    `sum(rate(tikv_thread_cpu_seconds_total{name=~\"cop_.*\"}[1m])) by (instance) / (count(tikv_thread_cpu_seconds_total{name=~\"cop_.*\"}) * 0.9) / count(count(tikv_thread_cpu_seconds_total) by (instance)) > 0`\n\n* 规则描述：\n\n    某个 TiKV 的 Coprocessor CPU 使用率超过了 90%。\n\n#### `TiKV_pending_task`\n\n* 报警规则：\n\n    `sum(tikv_worker_pending_task_total) BY (instance,name)  > 1000`\n\n* 规则描述：\n\n    TiKV 等待的任务数量。\n\n* 处理方法：\n\n    观察 [**TiKV-Details** > **Task** 面板](/grafana-tikv-dashboard.md#task)，查看是哪一类任务的 `Worker pending tasks` 值偏高。\n\n#### `TiKV_low_space`\n\n* 报警规则：\n\n    `sum(tikv_store_size_bytes{type=\"available\"}) by (instance) / sum(tikv_store_size_bytes{type=\"capacity\"}) by (instance) < 0.2`\n\n* 规则描述：\n\n    TiKV 数据量超过节点配置容量或物理磁盘容量的 80%。\n\n* 处理方法：\n\n    确认节点空间均衡情况，做好扩容计划。\n\n#### `TiKV_approximate_region_size`\n\n* 报警规则：\n\n    `histogram_quantile(0.99, sum(rate(tikv_raftstore_region_size_bucket[1m])) by (le)) > 1073741824`\n\n* 规则描述：\n\n    TiKV split checker 扫描到的最大的 Region approximate size 在 1 分钟内持续大于 1 GB。\n\n* 处理方法：\n\n    Region 分裂的速度不及写入的速度。为缓解这种情况，建议更新到支持 batch-split 的版本 (>= 2.1.0-rc1)。如暂时无法更新，可以使用 `pd-ctl operator add split-region <region_id> --policy=approximate` 手动分裂 Region。\n\n## TiFlash 报警规则\n\n关于 TiFlash 报警规则的详细描述，参见 [TiFlash 报警规则](/tiflash/tiflash-alert-rules.md)。\n\n## TiCDC 报警规则\n\n关于 TiCDC 报警规则的详细描述，参见 [TiCDC 集群监控报警](/ticdc/ticdc-alert-rules.md)。\n\n## Node_exporter 主机报警规则\n\n本节介绍了 Node_exporter 主机的报警项。\n\n### 紧急级别报警项\n\n#### `NODE_disk_used_more_than_80%`\n\n* 报警规则：\n\n    `node_filesystem_avail_bytes{fstype=~\"(ext.|xfs)\", mountpoint!~\"/boot\"} / node_filesystem_size_bytes{fstype=~\"(ext.|xfs)\", mountpoint!~\"/boot\"} * 100 <= 20`\n\n* 规则描述：\n\n    机器磁盘空间使用率超过 80%。\n\n* 处理方法：\n\n    登录机器，执行 `df -h` 命令，查看磁盘空间使用率，做好扩容计划。\n\n#### `NODE_disk_inode_more_than_80%`\n\n* 报警规则：\n\n    `node_filesystem_files_free{fstype=~\"(ext.|xfs)\"} / node_filesystem_files{fstype=~\"(ext.|xfs)\"} * 100 < 20`\n\n* 规则描述：\n\n    机器磁盘挂载目录文件系统 inode 使用率超过 80%。\n\n* 处理方法：\n\n    登录机器，执行 `df -i` 命令，查看磁盘挂载目录文件系统 inode 使用率，做好扩容计划。\n\n#### `NODE_disk_readonly`\n\n* 报警规则：\n\n    `node_filesystem_readonly{fstype=~\"(ext.|xfs)\"} == 1`\n\n* 规则描述：\n\n    磁盘挂载目录文件系统只读，无法写入数据，一般是因为磁盘故障或文件系统损坏。\n\n* 处理方法：\n\n    * 登录机器创建文件测试是否正常。\n    * 检查该服务器硬盘指示灯是否正常，如异常，需更换磁盘并修复该机器文件系统。\n\n### 严重级别报警项\n\n#### `NODE_memory_used_more_than_80%`\n\n* 报警规则：\n\n    `(((node_memory_MemTotal_bytes-node_memory_MemFree_bytes-node_memory_Cached_bytes)/(node_memory_MemTotal_bytes)*100)) >= 80`\n\n* 规则描述：\n\n    机器内存使用率超过 80%。\n\n* 处理方法：\n\n    * 在 Grafana Node Exporter 页面查看该主机的 Memory 面板，检查 `Used` 是否过高，`Available` 内存是否过低。\n    * 登录机器，执行 `free -m` 命令查看内存使用情况，执行 `top` 看是否有异常进程的内存使用率过高。\n\n### 警告级别报警项\n\n#### `NODE_node_overload`\n\n* 报警规则：\n\n    `(node_load5 / count without (cpu, mode) (node_cpu_seconds_total{mode=\"system\"})) > 1`\n\n* 规则描述：\n\n    机器 CPU 负载较高。\n\n* 处理方法：\n\n    * 在 Grafana Node exporter 页面上查看该主机的 CPU Usage 及 Load Average，检查是否过高。\n    * 登录机器，执行 `top` 查看 load average 及 CPU 使用率，看是否是异常进程的 CPU 使用率过高。\n\n#### `NODE_cpu_used_more_than_80%`\n\n* 报警规则：\n\n    `avg(irate(node_cpu_seconds_total{mode=\"idle\"}[5m])) by(instance) * 100 <= 20`\n\n* 规则描述：\n\n    机器 CPU 使用率超过 80%。\n\n* 处理方法：\n\n    * 在 Grafana Node exporter 页面上查看该主机的 CPU Usage 及 Load Average，检查是否过高。\n    * 登录机器，执行 `top` 查看 load average 及 CPU 使用率，看是否是异常进程的 CPU 使用率过高。\n\n#### `NODE_tcp_estab_num_more_than_50000`\n\n* 报警规则：\n\n    `node_netstat_Tcp_CurrEstab > 50000`\n\n* 规则描述：\n\n    机器 `establish` 状态的 TCP 链接超过 50,000。\n\n* 处理方法：\n\n    登录机器执行 `ss -s` 可查看当前系统 `estab` 状态的 TCP 链接数，执行 `netstat` 查看是否有异常链接。\n\n#### `NODE_disk_read_latency_more_than_32ms`\n\n* 报警规则：\n\n    `((rate(node_disk_read_time_seconds_total{device=~\".+\"}[5m]) / rate(node_disk_reads_completed_total{device=~\".+\"}[5m])) or (irate(node_disk_read_time_seconds_total{device=~\".+\"}[5m]) / irate(node_disk_reads_completed_total{device=~\".+\"}[5m])) ) * 1000 > 32`\n\n* 规则描述：\n\n    磁盘读延迟超过 32 毫秒。\n\n* 处理方法：\n\n    * 查看 Grafana Disk Performance Dashboard 观察磁盘使用情况。\n    * 查看 Disk Latency 面板观察磁盘的读延迟。\n    * 查看 Disk IO Utilization 面板观察 IO 使用率。\n\n#### `NODE_disk_write_latency_more_than_16ms`\n\n* 报警规则：\n\n    `((rate(node_disk_write_time_seconds_total{device=~\".+\"}[5m]) / rate(node_disk_writes_completed_total{device=~\".+\"}[5m])) or (irate(node_disk_write_time_seconds_total{device=~\".+\"}[5m]) / irate(node_disk_writes_completed_total{device=~\".+\"}[5m])))> 16`\n\n* 规则描述：\n\n    机器磁盘写延迟超过 16 毫秒。\n\n* 处理方法：\n\n    * 查看 Grafana Disk Performance Dashboard 观察磁盘使用情况。\n    * 查看 Disk Latency 面板可查看磁盘的写延迟。\n    * 查看 Disk IO Utilization 面板可查看 IO 使用率。\n\n## Blackbox_exporter TCP、ICMP 和 HTTP 报警规则\n\n本节介绍了 Blackbox_exporter TCP、ICMP 和 HTTP 的报警项。\n\n### 紧急级别报警项\n\n#### `TiDB_server_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"tidb\"} == 0`\n\n* 规则描述：\n\n    TiDB 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 TiDB 服务所在机器是否宕机。\n    * 检查 TiDB 进程是否存在。\n    * 检查监控机与 TiDB 服务所在机器之间网络是否正常。\n\n#### `TiFlash_server_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"tiflash\"} == 0`\n\n* 规则描述：\n\n    TiFlash 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 TiFlash 服务所在机器是否宕机。\n    * 检查 TiFlash 进程是否存在。\n    * 检查监控机与 TiFlash 服务所在机器之间网络是否正常。\n\n#### `TiKV_server_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"tikv\"} == 0`\n\n* 规则描述：\n\n    TiKV 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 TiKV 服务所在机器是否宕机。\n    * 检查 TiKV 进程是否存在。\n    * 检查监控机与 TiKV 服务所在机器之间网络是否正常。\n\n#### `PD_server_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"pd\"} == 0`\n\n* 规则描述：\n\n    PD 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 PD 服务所在机器是否宕机。\n    * 检查 PD 进程是否存在。\n    * 检查监控机与 PD 服务所在机器之间网络是否正常。\n\n#### `Node_exporter_server_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"node_exporter\"} == 0`\n\n* 规则描述：\n\n    Node_exporter 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 Node_exporter 服务所在机器是否宕机。\n    * 检查 Node_exporter 进程是否存在。\n    * 检查监控机与 Node_exporter 服务所在机器之间网络是否正常。\n\n#### `Blackbox_exporter_server_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"blackbox_exporter\"} == 0`\n\n* 规则描述：\n\n    Blackbox_exporter 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 Blackbox_exporter 服务所在机器是否宕机。\n    * 检查 Blackbox_exporter 进程是否存在。\n    * 检查监控机与 Blackbox_exporter 服务所在机器之间网络是否正常。\n\n#### `Grafana_server_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"grafana\"} == 0`\n\n* 规则描述：\n\n    Grafana 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 Grafana 服务所在机器是否宕机。\n    * 检查 Grafana 进程是否存在。\n    * 检查监控机与 Grafana 服务所在机器之间网络是否正常。\n\n#### `Pushgateway_server_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"pushgateway\"} == 0`\n\n* 规则描述：\n\n    Pushgateway 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 Pushgateway 服务所在机器是否宕机。\n    * 检查 Pushgateway 进程是否存在。\n    * 检查监控机与 Pushgateway 服务所在机器之间网络是否正常。\n\n#### `Kafka_exporter_is_down`\n\n* 报警规则：\n\n    `probe_success{group=\"kafka_exporter\"} == 0`\n\n* 规则描述：\n\n    Kafka_exporter 服务端口探测失败。\n\n* 处理方法：\n\n    * 检查 Kafka_exporter 服务所在机器是否宕机。\n    * 检查 Kafka_exporter 进程是否存在。\n    * 检查监控机与 Kafka_exporter 服务所在机器之间网络是否正常。\n\n#### `Pushgateway_metrics_interface`\n\n* 报警规则：\n\n    `probe_success{job=\"blackbox_exporter_http\"} == 0`\n\n* 规则描述：\n\n    Pushgateway 服务 http 接口探测失败。\n\n* 处理方法：\n\n    * 检查 Pushgateway 服务所在机器是否宕机。\n    * 检查 Pushgateway 进程是否存在。\n    * 检查监控机与 Pushgateway 服务所在机器之间网络是否正常。\n\n### 警告级别报警项\n\n#### `BLACKER_ping_latency_more_than_1s`\n\n* 报警规则：\n\n    `max_over_time(probe_duration_seconds{job=~\"blackbox_exporter.*_icmp\"}[1m]) > 1`\n\n* 规则描述：\n\n    Ping 延迟超过 1 秒。\n\n* 处理方法：\n\n    * 在 Grafana Blackbox Exporter 页面上检查两个节点间的 ping 延迟是否太高。\n    * 在 Grafana Node Exporter 页面的 TCP 面板上检查是否有丢包。\n"
        },
        {
          "name": "analyze-slow-queries.md",
          "type": "blob",
          "size": 18.4423828125,
          "content": "---\ntitle: 分析慢查询\nsummary: 学习如何定位和分析慢查询。\n---\n\n# 分析慢查询\n\n处理慢查询分为两步：\n\n1. 从大量查询中定位出哪一类查询比较慢\n2. 分析这类慢查询的原因\n\n第一步可以通过[慢日志](/identify-slow-queries.md)、[statement-summary](/statement-summary-tables.md) 方便地定位，推荐直接使用 [TiDB Dashboard](/dashboard/dashboard-overview.md)，它整合了这两个功能，且能方便直观地在浏览器中展示出来。本文聚焦第二步。\n\n首先将慢查询归因成两大类：\n\n- 优化器问题：如选错索引，选错 Join 类型或顺序。\n- 系统性问题：将非优化器问题都归结于此类。如：某个 TiKV 实例忙导致处理请求慢，Region 信息过期导致查询变慢。\n\n实际中，优化器问题可能造成系统性问题。例如对于某类查询，优化器应使用索引，但却使用了全表扫。这可能导致这类 SQL 消耗大量资源，造成某些 KV 实例 CPU 飚高等问题。表现上看就是一个系统性问题，但本质是优化器问题。\n\n分析优化器问题需要有判断执行计划是否合理的能力，而系统性问题的定位相对简单，因此面对慢查询推荐的分析过程如下：\n\n1. 定位查询瓶颈：即查询过程中耗时多的部分\n2. 分析系统性问题：根据瓶颈点，结合当时的监控/日志等信息，分析可能的原因\n3. 分析优化器问题：分析是否有更好的执行计划\n\n接下来会分别介绍上面几点。\n\n## 定位查询瓶颈\n\n定位查询瓶颈需要对查询过程有一个大致理解，TiDB 处理查询过程的关键阶段都在 [performance-map](/media/performance-map.png) 图中了。\n\n查询的耗时信息可以从下面几种方式获得：\n\n- [慢日志](/identify-slow-queries.md)（推荐直接在 [TiDB Dashboard](/dashboard/dashboard-overview.md) 中查看）\n- [`explain analyze` 语句](/sql-statements/sql-statement-explain-analyze.md)\n\n他们的侧重点不同：\n\n- 慢日志记录了 SQL 从解析到返回，几乎所有阶段的耗时，较为全面（在 TiDB Dashboard 中可以直观地查询和分析慢日志）；\n- `explain analyze` 可以拿到 SQL 实际执行中每个执行算子的耗时，对执行耗时有更细分的统计；\n\n总的来说，利用慢日志和 `explain analyze` 可以比较准确地定位查询的瓶颈点，帮助你判断这条 SQL 慢在哪个模块 (TiDB/TiKV)，慢在哪个阶段，下面会有一些例子。\n\n另外在 4.0.3 之后，慢日志中的 `Plan` 字段也会包含 SQL 的执行信息，也就是 `explain analyze` 的结果，这样一来 SQL 的所有耗时信息都可以在慢日志中找到。\n\n## 分析系统性问题\n\n对于系统性问题，我们根据执行阶段，分成三个大类：\n\n1. TiKV 处理慢：如 coprocessor 处理数据慢\n2. TiDB 执行慢：主要指执行阶段，如某个 Join 算子处理数据慢\n3. 其他关键阶段慢：如取时间戳慢\n\n拿到一个慢查询，我们应该先根据已有信息判断大致是哪个大类，再具体分析。\n\n### TiKV 处理慢\n\n如果是 TiKV 处理慢，可以很明显的通过 `explain analyze` 中看出来。例如下面这个例子，可以看到 `StreamAgg_8` 和 `TableFullScan_15` 这两个 `tikv-task` （在 `task` 列可以看出这两个任务类型是 `cop[tikv]`）花费了 `170ms`，而 TiDB 部分的算子耗时，减去这 `170ms` 后，耗时占比非常小，说明瓶颈在 TiKV。\n\n```sql\n+----------------------------+---------+---------+-----------+---------------+------------------------------------------------------------------------------+---------------------------------+-----------+------+\n| id                         | estRows | actRows | task      | access object | execution info                                                               | operator info                   | memory    | disk |\n+----------------------------+---------+---------+-----------+---------------+------------------------------------------------------------------------------+---------------------------------+-----------+------+\n| StreamAgg_16               | 1.00    | 1       | root      |               | time:170.08572ms, loops:2                                                     | funcs:count(Column#5)->Column#3 | 372 Bytes | N/A  |\n| └─TableReader_17           | 1.00    | 1       | root      |               | time:170.080369ms, loops:2, rpc num: 1, rpc time:17.023347ms, proc keys:28672 | data:StreamAgg_8                | 202 Bytes | N/A  |\n|   └─StreamAgg_8            | 1.00    | 1       | cop[tikv] |               | time:170ms, loops:29                                                          | funcs:count(1)->Column#5        | N/A       | N/A  |\n|     └─TableFullScan_15     | 7.00    | 28672   | cop[tikv] | table:t       | time:170ms, loops:29                                                          | keep order:false, stats:pseudo  | N/A       | N/A  |\n+----------------------------+---------+---------+-----------+---------------+------------------------------------------------------------------------------+---------------------------------+-----------+------\n```\n\n另外在慢日志中，`Cop_process` 和 `Cop_wait` 字段也可以帮助判断，如下面这个例子，查询整个耗时是 180.85ms 左右，而最大的那个 `coptask` 就消耗了 `171ms`，可以说明对这个查询而言，瓶颈在 TiKV 侧。\n\n慢日志中的各个字段的说明可以参考慢查询日志中的[字段含义说明](/identify-slow-queries.md#字段含义说明)\n\n```log\n# Query_time: 0.18085\n...\n# Num_cop_tasks: 1\n# Cop_process: Avg_time: 170ms P90_time: 170ms Max_time: 170ms Max_addr: 10.6.131.78\n# Cop_wait: Avg_time: 1ms P90_time: 1ms Max_time: 1ms Max_Addr: 10.6.131.78\n```\n\n根据上述方式判断是 TiKV 慢后，可以依次排查 TiKV 慢的原因。\n\n#### TiKV 实例忙\n\n一条 SQL 可能会去从多个 TiKV 上拿数据，如果某个 TiKV 响应慢，可能拖慢整个 SQL 的处理速度。\n\n慢日志中的 `Cop_wait` 可以帮忙判断这个问题：\n\n```log\n# Cop_wait: Avg_time: 1ms P90_time: 2ms Max_time: 110ms Max_Addr: 10.6.131.78\n```\n\n如上图，发给 `10.6.131.78` 的一个 `cop-task` 等待了 110ms 才被执行，可以判断是当时该实例忙，此时可以打开当时的 CPU 监控辅助判断。\n\n#### 过期 MVCC 版本和 key 过多\n\n如果 TiKV 上过期 MVCC 版本过多，或 GC 历史版本数据的保留时间长，导致累积了过多 MVCC。处理这些不必要的 MVCC 版本会影响扫描速度。\n\n这可以通过 `Total_keys` 和 `Processed_keys` 判断，如果两者相差较大，则说明旧版本的 key 太多：\n\n```\n...\n# Total_keys: 2215187529 Processed_keys: 1108056368\n...\n```\n\nTiDB v8.5.0 引入了内存引擎功能，可以加速这类慢查询。详见 [TiKV MVCC 内存引擎](/tikv-in-memory-engine.md)。\n\n### 其他关键阶段慢\n\n#### 取 TS 慢\n\n可以对比慢日志中的 `Wait_TS` 和 `Query_time`，因为 TS 有预取操作，通常来说 `Wait_TS` 应该很低。\n\n```\n# Query_time: 0.0300000\n...\n# Wait_TS: 0.02500000\n```\n\n#### Region 信息过期\n\nTiDB 侧 Region 信息可能过期，此时 TiKV 可能返回 `regionMiss` 的错误，然后 TiDB 会从 PD 去重新获取 Region 信息，这些信息会被反应在 `Cop_backoff` 信息内，失败的次数和总耗时都会被记录下来。\n\n```\n# Cop_backoff_regionMiss_total_times: 200 Cop_backoff_regionMiss_total_time: 0.2 Cop_backoff_regionMiss_max_time: 0.2 Cop_backoff_regionMiss_max_addr: 127.0.0.1 Cop_backoff_regionMiss_avg_time: 0.2 Cop_backoff_regionMiss_p90_time: 0.2\n# Cop_backoff_rpcPD_total_times: 200 Cop_backoff_rpcPD_total_time: 0.2 Cop_backoff_rpcPD_max_time: 0.2 Cop_backoff_rpcPD_max_addr: 127.0.0.1 Cop_backoff_rpcPD_avg_time: 0.2 Cop_backoff_rpcPD_p90_time: 0.2\n```\n\n#### 子查询被提前执行\n\n对于带有非关联子查询的语句，子查询部分可能被提前执行，如：`select * from t1 where a = (select max(a) from t2)`，`select max(a) from t2` 部分可能在优化阶段被提前执行。这种查询用 `explain analyze` 看不到对应的耗时，如下：\n\n```sql\nmysql> explain analyze select count(*) from t where a=(select max(t1.a) from t t1, t t2 where t1.a=t2.a);\n+------------------------------+----------+---------+-----------+---------------+--------------------------+----------------------------------+-----------+------+\n| id                           | estRows  | actRows | task      | access object | execution info           | operator info                    | memory    | disk |\n+------------------------------+----------+---------+-----------+---------------+--------------------------+----------------------------------+-----------+------+\n| StreamAgg_59                 | 1.00     | 1       | root      |               | time:4.69267ms, loops:2  | funcs:count(Column#10)->Column#8 | 372 Bytes | N/A  |\n| └─TableReader_60             | 1.00     | 1       | root      |               | time:4.690428ms, loops:2 | data:StreamAgg_48                | 141 Bytes | N/A  |\n|   └─StreamAgg_48             | 1.00     |         | cop[tikv] |               | time:0ns, loops:0        | funcs:count(1)->Column#10        | N/A       | N/A  |\n|     └─Selection_58           | 16384.00 |         | cop[tikv] |               | time:0ns, loops:0        | eq(test.t.a, 1)                  | N/A       | N/A  |\n|       └─TableFullScan_57     | 16384.00 | -1      | cop[tikv] | table:t       | time:0s, loops:0         | keep order:false                 | N/A       | N/A  |\n+------------------------------+----------+---------+-----------+---------------+--------------------------+----------------------------------+-----------+------+\n5 rows in set (7.77 sec)\n```\n\n不过可以从慢日志中排查这种情况：\n\n```\n# Query_time: 7.770634843\n...\n# Rewrite_time: 7.765673663 Preproc_subqueries: 1 Preproc_subqueries_time: 7.765231874\n```\n\n可以看到有 1 个子查询被提前执行，花费了 `7.76s`。\n\n### TiDB 执行慢\n\n这里我们假设 TiDB 的执行计划正确（不正确的情况在[分析优化器问题](#分析优化器问题)这一节中说明），但是执行上很慢；\n\n解决这类问题主要靠调整参数或利用 hint，并结合 `explain analyze` 对 SQL 进行调整。\n\n#### 并发太低\n\n如果发现瓶颈在有并发的算子上，可以通过调整并发度来尝试提速，如下面的执行计划中：\n\n```sql\nmysql> explain analyze select sum(t1.a) from t t1, t t2 where t1.a=t2.a;\n+----------------------------------+--------------+-----------+-----------+---------------+-------------------------------------------------------------------------------------+------------------------------------------------+------------------+---------+\n| id                               | estRows      | actRows   | task      | access object | execution info                                                                      | operator info                                  | memory           | disk    |\n+----------------------------------+--------------+-----------+-----------+---------------+-------------------------------------------------------------------------------------+------------------------------------------------+------------------+---------+\n| HashAgg_11                       | 1.00         | 1         | root      |               | time:9.666832189s, loops:2, PartialConcurrency:4, FinalConcurrency:4                | funcs:sum(Column#6)->Column#5                  | 322.125 KB       | N/A     |\n| └─Projection_24                  | 268435456.00 | 268435456 | root      |               | time:9.098644711s, loops:262145, Concurrency:4                                      | cast(test.t.a, decimal(65,0) BINARY)->Column#6 | 199 KB           | N/A     |\n|   └─HashJoin_14                  | 268435456.00 | 268435456 | root      |               | time:6.616773501s, loops:262145, Concurrency:5, probe collision:0, build:881.404µs  | inner join, equal:[eq(test.t.a, test.t.a)]     | 131.75 KB        | 0 Bytes |\n|     ├─TableReader_21(Build)      | 16384.00     | 16384     | root      |               | time:6.553717ms, loops:17                                                           | data:Selection_20                              | 33.6318359375 KB | N/A     |\n|     │ └─Selection_20             | 16384.00     |           | cop[tikv] |               | time:0ns, loops:0                                                                   | not(isnull(test.t.a))                          | N/A              | N/A     |\n|     │   └─TableFullScan_19       | 16384.00     | -1        | cop[tikv] | table:t2      | time:0s, loops:0                                                                    | keep order:false                               | N/A              | N/A     |\n|     └─TableReader_18(Probe)      | 16384.00     | 16384     | root      |               | time:6.880923ms, loops:17                                                           | data:Selection_17                              | 33.6318359375 KB | N/A     |\n|       └─Selection_17             | 16384.00     |           | cop[tikv] |               | time:0ns, loops:0                                                                   | not(isnull(test.t.a))                          | N/A              | N/A     |\n|         └─TableFullScan_16       | 16384.00     | -1        | cop[tikv] | table:t1      | time:0s, loops:0                                                                    | keep order:false                               | N/A              | N/A     |\n+----------------------------------+--------------+-----------+-----------+---------------+-------------------------------------------------------------------------------------+------------------------------------------------+------------------+---------+\n9 rows in set (9.67 sec)\n```\n\n发现耗时主要在 `HashJoin_14` 和 `Projection_24`，可以酌情通过 SQL 变量来提高他们的并发度进行提速。\n\n[system-variables](/system-variables.md) 中有所有的系统变量，如想提高 `HashJoin_14` 的并发度，则可以修改变量 `tidb_hash_join_concurrency`。\n\n#### 产生了落盘\n\n执行慢的另一个原因是执行过程中，因为到达内存限制，产生了落盘，这点在执行计划和慢日志中都能看到：\n\n```sql\n+-------------------------+-----------+---------+-----------+---------------+------------------------------+----------------------+-----------------------+----------------+\n| id                      | estRows   | actRows | task      | access object | execution info               | operator info        | memory                | disk           |\n+-------------------------+-----------+---------+-----------+---------------+------------------------------+----------------------+-----------------------+----------------+\n| Sort_4                  | 462144.00 | 462144  | root      |               | time:2.02848898s, loops:453  | test.t.a             | 149.68795776367188 MB | 219.3203125 MB |\n| └─TableReader_8         | 462144.00 | 462144  | root      |               | time:616.211272ms, loops:453 | data:TableFullScan_7 | 197.49601364135742 MB | N/A            |\n|   └─TableFullScan_7     | 462144.00 | -1      | cop[tikv] | table:t       | time:0s, loops:0             | keep order:false     | N/A                   | N/A            |\n+-------------------------+-----------+---------+-----------+---------------+------------------------------+----------------------+-----------------------+----------------+\n```\n\n```\n...\n# Disk_max: 229974016\n...\n```\n\n#### 做了笛卡尔积的 Join\n\n做笛卡尔积的 Join 会产生 `左边孩子行数 * 右边孩子行数` 这么多数据，效率较低，应该尽量避免；\n\n目前对于产生笛卡尔积的 Join 会在执行计划中显示的标明 `CARTESIAN`，如下：\n\n```sql\nmysql> explain select * from t t1, t t2 where t1.a>t2.a;\n+------------------------------+-------------+-----------+---------------+---------------------------------------------------------+\n| id                           | estRows     | task      | access object | operator info                                           |\n+------------------------------+-------------+-----------+---------------+---------------------------------------------------------+\n| HashJoin_8                   | 99800100.00 | root      |               | CARTESIAN inner join, other cond:gt(test.t.a, test.t.a) |\n| ├─TableReader_15(Build)      | 9990.00     | root      |               | data:Selection_14                                       |\n| │ └─Selection_14             | 9990.00     | cop[tikv] |               | not(isnull(test.t.a))                                   |\n| │   └─TableFullScan_13       | 10000.00    | cop[tikv] | table:t2      | keep order:false, stats:pseudo                          |\n| └─TableReader_12(Probe)      | 9990.00     | root      |               | data:Selection_11                                       |\n|   └─Selection_11             | 9990.00     | cop[tikv] |               | not(isnull(test.t.a))                                   |\n|     └─TableFullScan_10       | 10000.00    | cop[tikv] | table:t1      | keep order:false, stats:pseudo                          |\n+------------------------------+-------------+-----------+---------------+---------------------------------------------------------+\n```\n\n## 分析优化器问题\n\n分析优化问题需要有判断执行计划是否合理的能力，这需要对优化过程和各算子有一定了解。\n\n下面是一组例子，假设表结构为 `create table t (id int, a int, b int, c int, primary key(id), key(a), key(b, c))`：\n\n1. `select * from t`：没有过滤条件，会扫全表，所以会用 `TableFullScan` 算子读取数据；\n2. `select a from t where a=2`：有过滤条件且只读索引列，所以会用 `IndexReader` 算子读取数据；\n3. `select * from t where a=2`：在 `a` 有过滤条件，但索引 `a` 不能完全覆盖需要读取的内容，因此会采用 `IndexLookup`；\n4. `select b from t where c=3`：多列索引没有前缀条件就用不上，所以会用 `IndexFullScan`；\n5. ...\n\n上面举例了数据读入相关的算子，在[理解 TiDB 执行计划](/explain-overview.md)中描述了更多算子的情况。\n\n另外阅读 [SQL 性能调优](/sql-tuning-overview.md)整个小节能增加你对 TiDB 优化器的了解，帮助判断执行计划是否合理。\n\n由于大多数优化器问题在 [SQL 性能调优](/sql-tuning-overview.md)已经有解释，这里就直接列举出来跳转过去：\n\n1. [索引选择错误](/wrong-index-solution.md)\n2. [Join 顺序错误](/join-reorder.md)\n3. [表达式未下推](/blocklist-control-plan.md)\n"
        },
        {
          "name": "as-of-timestamp.md",
          "type": "blob",
          "size": 7.2783203125,
          "content": "---\ntitle: 使用 AS OF TIMESTAMP 语法读取历史数据\nsummary: 了解如何使用 AS OF TIMESTAMP 语法读取历史数据。\n---\n\n# 使用 AS OF TIMESTAMP 语法读取历史数据\n\n本文档介绍如何通过 `AS OF TIMESTAMP` 语句使用 [Stale Read](/stale-read.md) 功能来读取 TiDB 历史版本数据，包括具体的操作示例以及历史数据的保存策略。\n\nTiDB 支持通过标准 SQL 接口，即通过 `AS OF TIMESTAMP` SQL 语法的形式读取历史数据，无需特殊的服务器或者驱动器。当数据被更新或删除后，你可以通过 SQL 接口将更新或删除前的数据读取出来。\n\n> **注意：**\n>\n> 读取历史数据时，即使当前数据的表结构相较于历史数据的表结构已经发生改变，历史数据也会以当时的历史表结构来返回。\n\n## 语法方式\n\n你可以通过以下三种方式使用 `AS OF TIMESTAMP` 语法：\n\n- [`SELECT ... FROM ... AS OF TIMESTAMP`](/sql-statements/sql-statement-select.md)\n- [`START TRANSACTION READ ONLY AS OF TIMESTAMP`](/sql-statements/sql-statement-start-transaction.md)\n- [`SET TRANSACTION READ ONLY AS OF TIMESTAMP`](/sql-statements/sql-statement-set-transaction.md)\n\n如果你想要指定一个精确的时间点，可在 `AS OF TIMESTAMP` 中使用日期时间和时间函数，日期时间的格式为：\"2016-10-08 16:45:26.999\"，最小时间精度范围为毫秒，通常可只写到秒，例如 \"2016-10-08 16:45:26\"。你也可以通过 `NOW(3)` 函数获得精确到毫秒的当前时间。如果想读取几秒前的数据，推荐使用例如 `NOW() - INTERVAL 10 SECOND` 的表达式。（推荐）\n\n如果你想要指定一个时间范围，需要使用 [`TIDB_BOUNDED_STALENESS()`](/functions-and-operators/tidb-functions.md#tidb_bounded_staleness) 函数。使用该函数，TiDB 会在指定的时间范围内选择一个合适的时间戳，该时间戳能保证所访问的副本上不存在开始于这个时间戳之前且还没有提交的相关事务，即能保证所访问的可用副本上执行读取操作而且不会被阻塞。用法为 `TIDB_BOUNDED_STALENESS(t1, t2)`，其中 `t1` 和 `t2` 为时间范围的两端，支持使用日期时间和时间函数。\n\n示例如下：\n\n- `AS OF TIMESTAMP '2016-10-08 16:45:26'` 表示读取在 2016 年 10 月 8 日 16 点 45 分 26 秒时最新的数据。\n- `AS OF TIMESTAMP NOW() - INTERVAL 10 SECOND` 表示读取 10 秒前最新的数据。\n- `AS OF TIMESTAMP TIDB_BOUNDED_STALENESS('2016-10-08 16:45:26', '2016-10-08 16:45:29')` 表示读取在 2016 年 10 月 8 日 16 点 45 分 26 秒到 29 秒的时间范围内尽可能新的数据。\n- `AS OF TIMESTAMP TIDB_BOUNDED_STALENESS(NOW() - INTERVAL 20 SECOND, NOW())` 表示读取 20 秒前到现在的时间范围内尽可能新的数据。\n\n> **注意：**\n>\n> 除了指定时间戳，`AS OF TIMESTAMP` 语法最常用使用的方式是读几秒前的数据。如果采用这种方式，推荐读 5 秒以上的历史数据。\n>\n> 使用 Stale Read 时需要为 TiDB 和 PD 节点部署 NTP 服务，防止 TiDB 指定的时间戳超过当前最新的 TSO 分配进度（如几秒后的时间戳），或者落后于 GC safe point 的时间戳。当指定的时间戳超过服务范围，TiDB 会返回错误。\n>\n> 你可以通过调整 TiKV 的 `advance-ts-interval` 配置项提高 Stale Read 数据的时效性（即减少延时）。详情参见[减少 Stale Read 延时](/stale-read.md#减少-stale-read-延时)。\n\n## 示例\n\n本节通过多个示例介绍 `AS OF TIMESTAMP` 语法的不同使用方法。在本节中，先介绍如何准备用于恢复的数据，再分别展示如何通过 `SELECT`、`START TRANSACTION READ ONLY AS OF TIMESTAMP`、`SET TRANSACTION READ ONLY AS OF TIMESTAMP` 使用 `AS OF TIMESTAMP`。\n\n### 准备数据\n\n在准备数据阶段，创建一张表，并插入若干行数据：\n\n```sql\ncreate table t (c int);\n```\n\n```\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n```sql\ninsert into t values (1), (2), (3);\n```\n\n```\nQuery OK, 3 rows affected (0.00 sec)\n```\n\n查看表中的数据：\n\n```sql\nselect * from t;\n```\n\n```\n+------+\n| c    |\n+------+\n|    1 |\n|    2 |\n|    3 |\n+------+\n3 rows in set (0.00 sec)\n```\n\n查看当前时间：\n\n```sql\nselect now();\n```\n\n```\n+---------------------+\n| now()               |\n+---------------------+\n| 2021-05-26 16:45:26 |\n+---------------------+\n1 row in set (0.00 sec)\n```\n\n更新某一行数据：\n\n```sql\nupdate t set c=22 where c=2;\n```\n\n```\nQuery OK, 1 row affected (0.00 sec)\n```\n\n确认数据已经被更新：\n\n```sql\nselect * from t;\n```\n\n```\n+------+\n| c    |\n+------+\n|    1 |\n|   22 |\n|    3 |\n+------+\n3 rows in set (0.00 sec)\n```\n\n### 通过 `SELECT` 读取历史数据\n\n通过 [`SELECT ... FROM ... AS OF TIMESTAMP`](/sql-statements/sql-statement-select.md) 语句读取一个基于历史时间的数据。\n\n```sql\nselect * from t as of timestamp '2021-05-26 16:45:26';\n```\n\n```\n+------+\n| c    |\n+------+\n|    1 |\n|    2 |\n|    3 |\n+------+\n3 rows in set (0.00 sec)\n```\n\n> **注意：**\n>\n> 通过 `SELECT` 语句读取多个表时要保证 TIMESTAMP EXPRESSION 是一致的。比如：`select * from t as of timestamp NOW() - INTERVAL 2 SECOND, c as of timestamp NOW() - INTERVAL 2 SECOND;`。此外，在 `SELECT` 语句中，你必须要指定相关数据表的 as of 信息，若不指定，`SELECT` 语句会默认读最新的数据。\n\n### 通过 `START TRANSACTION READ ONLY AS OF TIMESTAMP` 读取历史数据\n\n通过 [`START TRANSACTION READ ONLY AS OF TIMESTAMP`](/sql-statements/sql-statement-start-transaction.md) 语句，你可以开启一个基于历史时间的只读事务，该事务基于所提供的历史时间来读取历史数据。\n\n```sql\nstart transaction read only as of timestamp '2021-05-26 16:45:26';\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nselect * from t;\n```\n\n```\n+------+\n| c    |\n+------+\n|    1 |\n|    2 |\n|    3 |\n+------+\n3 rows in set (0.00 sec)\n```\n\n```sql\ncommit;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n当事务结束后，即可读取最新数据。\n\n```sql\nselect * from t;\n```\n\n```\n+------+\n| c    |\n+------+\n|    1 |\n|   22 |\n|    3 |\n+------+\n3 rows in set (0.00 sec)\n```\n\n> **注意：**\n>\n> 通过 `START TRANSACTION READ ONLY AS OF TIMESTAMP` 开启的事务为只读事务。假如在该事务中执行写入操作，操作将会被该事务拒绝。\n\n### 通过 `SET TRANSACTION READ ONLY AS OF TIMESTAMP` 读取历史数据\n\n通过 [`SET TRANSACTION READ ONLY AS OF TIMESTAMP`](/sql-statements/sql-statement-set-transaction.md) 语句，你可以将下一个事务设置为基于指定历史时间的只读事务。该事务将会基于所提供的历史时间来读取历史数据。\n\n```sql\nset transaction read only as of timestamp '2021-05-26 16:45:26';\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nbegin;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nselect * from t;\n```\n\n```\n+------+\n| c    |\n+------+\n|    1 |\n|    2 |\n|    3 |\n+------+\n3 rows in set (0.00 sec)\n```\n\n```sql\ncommit;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n当事务结束后，即可读取最新数据。\n\n```sql\nselect * from t;\n```\n\n```\n+------+\n| c    |\n+------+\n|    1 |\n|   22 |\n|    3 |\n+------+\n3 rows in set (0.00 sec)\n```\n\n> **注意：**\n>\n> 通过 `SET TRANSACTION READ ONLY AS OF TIMESTAMP` 开启的事务为只读事务。假如在该事务中执行写入操作，操作将会被该事务拒绝。\n"
        },
        {
          "name": "auto-increment.md",
          "type": "blob",
          "size": 16.900390625,
          "content": "---\ntitle: AUTO_INCREMENT\nsummary: 介绍 TiDB 的 `AUTO_INCREMENT` 列属性。\naliases: ['/docs-cn/dev/auto-increment/']\n---\n\n# AUTO_INCREMENT\n\n本文介绍列属性 `AUTO_INCREMENT` 的基本概念、实现原理、自增相关的特性，以及使用限制。\n\n> **注意：**\n>\n> 使用 `AUTO_INCREMENT` 可能会给生产环境带热点问题，因此推荐使用 [`AUTO_RANDOM`](/auto-random.md) 代替。详情请参考 [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md#tidb-热点问题处理)。\n\n在 [`CREATE TABLE`](/sql-statements/sql-statement-create-table.md) 语句中也可以使用 `AUTO_INCREMENT` 参数来指定自增字段的初始值。\n\n## 基本概念\n\n`AUTO_INCREMENT` 是用于自动填充缺省列值的列属性。当 `INSERT` 语句没有指定 `AUTO_INCREMENT` 列的具体值时，系统会自动地为该列分配一个值。\n\n出于性能原因，自增编号是系统批量分配给每台 TiDB 服务器的值（默认 3 万个值），因此自增编号能保证唯一性，但分配给 `INSERT` 语句的值仅在单台 TiDB 服务器上具有单调性。\n\n> **注意：**\n>\n> 如果要求自增编号在所有 TiDB 实例上具有单调性，并且你的 TiDB 版本在 v6.5.0 及以上，推荐使用 [MySQL 兼容模式](#mysql-兼容模式)。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t(id int PRIMARY KEY AUTO_INCREMENT, c int);\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO t(c) VALUES (1);\nINSERT INTO t(c) VALUES (2);\nINSERT INTO t(c) VALUES (3), (4), (5);\n```\n\n```sql\nSELECT * FROM t;\n+----+---+\n| id | c |\n+----+---+\n| 1  | 1 |\n| 2  | 2 |\n| 3  | 3 |\n| 4  | 4 |\n| 5  | 5 |\n+----+---+\n5 rows in set (0.01 sec)\n```\n\n此外，`AUTO_INCREMENT` 还支持显式指定列值的插入语句，此时 TiDB 会保存显式指定的值：\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO t(id, c) VALUES (6, 6);\n```\n\n```sql\nSELECT * FROM t;\n+----+---+\n| id | c |\n+----+---+\n| 1  | 1 |\n| 2  | 2 |\n| 3  | 3 |\n| 4  | 4 |\n| 5  | 5 |\n| 6  | 6 |\n+----+---+\n6 rows in set (0.01 sec)\n```\n\n以上用法和 MySQL 的 `AUTO_INCREMENT` 用法一致。但在隐式分配的具体值方面，TiDB 和 MySQL 之间具有较为显著的差异。\n\n## 实现原理\n\nTiDB 实现 `AUTO_INCREMENT` 隐式分配的原理是，对于每一个自增列，都使用一个全局可见的键值对用于记录当前已分配的最大 ID。由于分布式环境下的节点通信存在一定开销，为了避免写请求放大的问题，每个 TiDB 节点在分配 ID 时，都申请一段 ID 作为缓存，用完之后再去取下一段，而不是每次分配都向存储节点申请。例如，对于以下新建的表：\n\n```sql\nCREATE TABLE t(id int UNIQUE KEY AUTO_INCREMENT, c int);\n```\n\n假设集群中有两个 TiDB 实例 A 和 B，如果向 A 和 B 分别对 `t` 执行一条插入语句：\n\n```sql\nINSERT INTO t (c) VALUES (1)\n```\n\n实例 A 可能会缓存 `[1,30000]` 的自增 ID，而实例 B 则可能缓存 `[30001,60000]` 的自增 ID。各自实例缓存的 ID 将随着执行将来的插入语句被作为缺省值，顺序地填充到 `AUTO_INCREMENT` 列中。\n\n## 基本特性\n\n### 唯一性保证\n\n> **警告：**\n>\n> 在集群中有多个 TiDB 实例时，如果表结构中有自增 ID，建议不要混用显式插入和隐式分配（即自增列的缺省值和自定义值），否则可能会破坏隐式分配值的唯一性。\n\n例如在上述示例中，依次执行如下操作：\n\n1. 客户端向实例 B 插入一条将 `id` 设置为 `2` 的语句 `INSERT INTO t VALUES (2, 1)`，并执行成功。\n2. 客户端向实例 A 发送 `INSERT` 语句 `INSERT INTO t (c) (1)`，这条语句中没有指定 `id` 的值，所以会由 A 分配。当前 A 缓存了 `[1, 30000]` 这段 ID，可能会分配 `2` 为自增 ID 的值，并把本地计数器加 `1`。而此时数据库中已经存在 `id` 为 `2` 的数据，最终返回 `Duplicated Error` 错误。\n\n### 单调性保证\n\nTiDB 保证 `AUTO_INCREMENT` 自增值在单台服务器上单调递增。以下示例在一台服务器上生成连续的 `AUTO_INCREMENT` 自增值 `1`-`3`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t (a int PRIMARY KEY AUTO_INCREMENT, b timestamp NOT NULL DEFAULT NOW());\nINSERT INTO t (a) VALUES (NULL), (NULL), (NULL);\nSELECT * FROM t;\n```\n\n```sql\nQuery OK, 0 rows affected (0.11 sec)\nQuery OK, 3 rows affected (0.02 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n+---+---------------------+\n| a | b                   |\n+---+---------------------+\n| 1 | 2020-09-09 20:38:22 |\n| 2 | 2020-09-09 20:38:22 |\n| 3 | 2020-09-09 20:38:22 |\n+---+---------------------+\n3 rows in set (0.00 sec)\n```\n\nTiDB 能保证自增值的单调性，但并不能保证其连续性。参考以下示例：\n\n```sql\nCREATE TABLE t (id INT NOT NULL PRIMARY KEY AUTO_INCREMENT, a VARCHAR(10), cnt INT NOT NULL DEFAULT 1, UNIQUE KEY (a));\nINSERT INTO t (a) VALUES ('A'), ('B');\nSELECT * FROM t;\nINSERT INTO t (a) VALUES ('A'), ('C') ON DUPLICATE KEY UPDATE cnt = cnt + 1;\nSELECT * FROM t;\n```\n\n```sql\nQuery OK, 0 rows affected (0.00 sec)\n\nQuery OK, 2 rows affected (0.00 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n\n+----+------+-----+\n| id | a    | cnt |\n+----+------+-----+\n|  1 | A    |   1 |\n|  2 | B    |   1 |\n+----+------+-----+\n2 rows in set (0.00 sec)\n\nQuery OK, 3 rows affected (0.00 sec)\nRecords: 2  Duplicates: 1  Warnings: 0\n\n+----+------+-----+\n| id | a    | cnt |\n+----+------+-----+\n|  1 | A    |   2 |\n|  2 | B    |   1 |\n|  4 | C    |   1 |\n+----+------+-----+\n3 rows in set (0.00 sec)\n```\n\n在以上示例 `INSERT INTO t (a) VALUES ('A'), ('C') ON DUPLICATE KEY UPDATE cnt = cnt + 1;` 语句中，自增值 `3` 被分配为 `A` 键对应的 `id` 值，但实际上 `3` 并未作为 `id` 值插入进表中。这是因为该 `INSERT` 语句包含一个重复键 `A`，使得自增序列不连续，出现了间隙。该行为尽管与 MySQL 不同，但仍是合法的。MySQL 在其他情况下也会出现自增序列不连续的情况，例如事务被中止和回滚时。\n\n## AUTO_ID_CACHE\n\n如果在另一台服务器上执行插入操作，那么 `AUTO_INCREMENT` 值的顺序可能会剧烈跳跃，这是由于每台服务器都有各自缓存的 `AUTO_INCREMENT` 自增值。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t (a INT PRIMARY KEY AUTO_INCREMENT, b TIMESTAMP NOT NULL DEFAULT NOW());\nINSERT INTO t (a) VALUES (NULL), (NULL), (NULL);\nINSERT INTO t (a) VALUES (NULL);\nSELECT * FROM t;\n```\n\n```sql\nQuery OK, 1 row affected (0.03 sec)\n\n+---------+---------------------+\n| a       | b                   |\n+---------+---------------------+\n|       1 | 2020-09-09 20:38:22 |\n|       2 | 2020-09-09 20:38:22 |\n|       3 | 2020-09-09 20:38:22 |\n| 2000001 | 2020-09-09 20:43:43 |\n+---------+---------------------+\n4 rows in set (0.00 sec)\n```\n\n以下示例在最先的一台服务器上执行一个插入 `INSERT` 操作，生成 `AUTO_INCREMENT` 值 `4`。因为这台服务器上仍有剩余的 `AUTO_INCREMENT` 缓存值可用于分配。在该示例中，值的顺序不具有全局单调性：\n\n```sql\nINSERT INTO t (a) VALUES (NULL);\nQuery OK, 1 row affected (0.01 sec)\n\nSELECT * FROM t ORDER BY b;\n+---------+---------------------+\n| a       | b                   |\n+---------+---------------------+\n|       1 | 2020-09-09 20:38:22 |\n|       2 | 2020-09-09 20:38:22 |\n|       3 | 2020-09-09 20:38:22 |\n| 2000001 | 2020-09-09 20:43:43 |\n|       4 | 2020-09-09 20:44:43 |\n+---------+---------------------+\n5 rows in set (0.00 sec)\n```\n\n`AUTO_INCREMENT` 缓存不会持久化，重启会导致缓存值失效。以下示例中，最先的一台服务器重启后，向该服务器执行一条插入操作：\n\n```sql\nINSERT INTO t (a) VALUES (NULL);\nQuery OK, 1 row affected (0.01 sec)\n\nSELECT * FROM t ORDER BY b;\n+---------+---------------------+\n| a       | b                   |\n+---------+---------------------+\n|       1 | 2020-09-09 20:38:22 |\n|       2 | 2020-09-09 20:38:22 |\n|       3 | 2020-09-09 20:38:22 |\n| 2000001 | 2020-09-09 20:43:43 |\n|       4 | 2020-09-09 20:44:43 |\n| 2030001 | 2020-09-09 20:54:11 |\n+---------+---------------------+\n6 rows in set (0.00 sec)\n```\n\nTiDB 服务器频繁重启可能导致 `AUTO_INCREMENT` 缓存值被快速消耗。在以上示例中，最先的一台服务器本来有可用的缓存值 `[5-3000]`。但重启后，这些值便丢失了，无法进行重新分配。\n\n用户不应指望 `AUTO_INCREMENT` 值保持连续。在以下示例中，一台 TiDB 服务器的缓存值为 `[2000001-2030000]`。当手动插入值 `2029998` 时，TiDB 取用了一个新缓存区间的值：\n\n```sql\nINSERT INTO t (a) VALUES (2029998);\nQuery OK, 1 row affected (0.01 sec)\n\nINSERT INTO t (a) VALUES (NULL);\nQuery OK, 1 row affected (0.01 sec)\n\nINSERT INTO t (a) VALUES (NULL);\nQuery OK, 1 row affected (0.00 sec)\n\nINSERT INTO t (a) VALUES (NULL);\nQuery OK, 1 row affected (0.02 sec)\n\nINSERT INTO t (a) VALUES (NULL);\nQuery OK, 1 row affected (0.01 sec)\n\nSELECT * FROM t ORDER BY b;\n+---------+---------------------+\n| a       | b                   |\n+---------+---------------------+\n|       1 | 2020-09-09 20:38:22 |\n|       2 | 2020-09-09 20:38:22 |\n|       3 | 2020-09-09 20:38:22 |\n| 2000001 | 2020-09-09 20:43:43 |\n|       4 | 2020-09-09 20:44:43 |\n| 2030001 | 2020-09-09 20:54:11 |\n| 2029998 | 2020-09-09 21:08:11 |\n| 2029999 | 2020-09-09 21:08:11 |\n| 2030000 | 2020-09-09 21:08:11 |\n| 2060001 | 2020-09-09 21:08:11 |\n| 2060002 | 2020-09-09 21:08:11 |\n+---------+---------------------+\n11 rows in set (0.00 sec)\n```\n\n以上示例插入 `2030000` 后，下一个值为 `2060001`，即顺序出现跳跃。这是因为另一台 TiDB 服务器获取了中间缓存区间 `[2030001-2060000]`。当部署有多台 TiDB 服务器时，`AUTO_INCREMENT` 值的顺序会出现跳跃，因为对缓存值的请求是交叉出现的。\n\n### 缓存大小控制\n\nTiDB 自增 ID 的缓存大小在早期版本中是对用户透明的。从 v3.1.2、v3.0.14 和 v4.0.rc-2 版本开始，TiDB 引入了 `AUTO_ID_CACHE` 表选项来允许用户自主设置自增 ID 分配缓存的大小。例如：\n\n```sql\nCREATE TABLE t(a int AUTO_INCREMENT key) AUTO_ID_CACHE 100;\nQuery OK, 0 rows affected (0.02 sec)\n\nINSERT INTO t VALUES();\nQuery OK, 1 row affected (0.00 sec)\n\nSELECT * FROM t;\n+---+\n| a |\n+---+\n| 1 |\n+---+\n1 row in set (0.01 sec)\n\nSHOW CREATE TABLE t;\n+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table | Create Table                                                                                                                                                                                                                             |\n+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| t     | CREATE TABLE `t` (\n  `a` int NOT NULL AUTO_INCREMENT,\n  PRIMARY KEY (`a`) /*T![clustered_index] CLUSTERED */\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin AUTO_INCREMENT=101 /*T![auto_id_cache] AUTO_ID_CACHE=100 */ |\n+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n此时如果重启 TiDB，自增 ID 缓存将会丢失，新的插入操作将从一个之前缓存范围外的更高的 ID 值开始分配。\n\n```sql\nINSERT INTO t VALUES();\nQuery OK, 1 row affected (0.00 sec)\n\nSELECT * FROM t;\n+-----+\n| a   |\n+-----+\n|   1 |\n| 101 |\n+-----+\n2 rows in set (0.01 sec)\n```\n\n可以看到再一次分配的值为 `101`，说明该表的自增 ID 分配缓存的大小为 `100`。\n\n此外如果在批量插入的 `INSERT` 语句中所需连续 ID 长度超过 `AUTO_ID_CACHE` 的长度时，TiDB 会适当调大缓存以便能够保证该语句的正常插入。\n\n### 清除自增 ID 缓存\n\n在一些场景中，你可能需要清除自增 ID 缓存，以保证数据一致性。例如：\n\n- 使用 [Data Migration (DM)](/dm/dm-overview.md) 进行增量同步，当同步结束后，下游 TiDB 的数据写入方式将从 DM 切换回正常的业务数据写入，此时自增列 ID 的写入模式通常由显式写入转换成隐式分配。\n- 当业务同时使用了显式写入和隐式分配时，需要清除自增 ID 缓存，以防止后续隐式分配的自增 ID 与已显式写入的 ID 发生冲突，导致主键冲突错误。具体场景参考[唯一性保证](/auto-increment.md#唯一性保证)。\n\n你可以执行 `ALTER TABLE` 语句设置 `AUTO_INCREMENT = 0` 来清除集群中所有 TiDB 节点的自增 ID 缓存。例如：\n\n```sql\nCREATE TABLE t(a int AUTO_INCREMENT key) AUTO_ID_CACHE 100;\nQuery OK, 0 rows affected (0.02 sec)\n\nINSERT INTO t VALUES();\nQuery OK, 1 row affected (0.02 sec)\n\nINSERT INTO t VALUES(50);\nQuery OK, 1 row affected (0.00 sec)\n\nSELECT * FROM t;\n+----+\n| a  |\n+----+\n|  1 |\n| 50 |\n+----+\n2 rows in set (0.01 sec)\n```\n\n```sql\nALTER TABLE t AUTO_INCREMENT = 0;\nQuery OK, 0 rows affected, 1 warning (0.07 sec)\n\nSHOW WARNINGS;\n+---------+------+-------------------------------------------------------------------------+\n| Level   | Code | Message                                                                 |\n+---------+------+-------------------------------------------------------------------------+\n| Warning | 1105 | Can't reset AUTO_INCREMENT to 0 without FORCE option, using 101 instead |\n+---------+------+-------------------------------------------------------------------------+\n1 row in set (0.01 sec)\n\nINSERT INTO t VALUES();\nQuery OK, 1 row affected (0.02 sec)\n\nSELECT * FROM t;\n+-----+\n| a   |\n+-----+\n|   1 |\n|  50 |\n| 101 |\n+-----+\n3 rows in set (0.01 sec)\n```\n\n### 自增步长和偏移量设置\n\n从 v3.0.9 和 v4.0.rc-1 开始，和 MySQL 的行为类似，自增列隐式分配的值遵循 session 变量 `@@auto_increment_increment` 和 `@@auto_increment_offset` 的控制，其中自增列隐式分配的值 (ID) 将满足式子 `(ID - auto_increment_offset) % auto_increment_increment == 0`。\n\n## MySQL 兼容模式\n\n从 v6.4.0 开始，TiDB 实现了中心化分配自增 ID 的服务，可以支持 TiDB 实例不缓存数据，而是每次请求都访问中心化服务获取 ID。\n\n当前中心化分配服务内置在 TiDB 进程，类似于 DDL Owner 的工作模式。有一个 TiDB 实例将充当“主”的角色提供 ID 分配服务，而其它的 TiDB 实例将充当“备”角色。当“主”节点发生故障时，会自动进行“主备切换”，从而保证中心化服务的高可用。\n\nMySQL 兼容模式的使用方式是，建表时将 `AUTO_ID_CACHE` 设置为 `1`：\n\n```sql\nCREATE TABLE t(a int AUTO_INCREMENT key) AUTO_ID_CACHE 1;\n```\n\n> **注意：**\n>\n> 在 TiDB 各个版本中，`AUTO_ID_CACHE` 设置为 `1` 都表明 TiDB 不再缓存 ID，但是不同版本的实现方式不一样：\n>\n> - 对于 TiDB v6.4.0 之前的版本，由于每次分配 ID 都需要通过一个 TiKV 事务完成 `AUTO_INCREMENT` 值的持久化修改，因此设置 `AUTO_ID_CACHE` 为 `1` 会出现性能下降。\n> - 对于 v6.4.0 及以上版本，由于引入了中心化的分配服务，`AUTO_INCREMENT` 值的修改只是在 TiDB 服务进程中的一个内存操作，相较于之前版本更快。\n> - 将 `AUTO_ID_CACHE` 设置为 `0` 表示 TiDB 使用默认的缓存大小 `30000`。\n\n使用 MySQL 兼容模式后，能保证 ID **唯一**、**单调递增**，行为几乎跟 MySQL 完全一致。即使跨 TiDB 实例访问，ID 也不会出现回退。只有在中心化分配自增 ID 服务的“主” TiDB 实例进程退出（如该 TiDB 节点重启）或者异常崩溃时，才有可能造成部分 ID 不连续。这是因为主备切换时，“备” 节点需要丢弃一部分之前的“主” 节点已经分配的 ID，以保证 ID 不出现重复。\n\n## 使用限制\n\n目前在 TiDB 中使用 `AUTO_INCREMENT` 有以下限制：\n\n- 对于 v6.6.0 及更早的 TiDB 版本，定义的列必须为主键或者索引前缀。\n- 只能定义在类型为整数、`FLOAT` 或 `DOUBLE` 的列上。\n- 不支持与列的默认值 `DEFAULT` 同时指定在同一列上。\n- 不支持使用 `ALTER TABLE` 来添加 `AUTO_INCREMENT` 属性，包括使用 `ALTER TABLE ... MODIFY/CHANGE COLUMN` 语法为已存在的列添加 `AUTO_INCREMENT` 属性，以及使用 `ALTER TABLE ... ADD COLUMN` 添加带有 `AUTO_INCREMENT` 属性的列。\n- 支持使用 `ALTER TABLE` 来移除 `AUTO_INCREMENT` 属性。但从 TiDB 2.1.18 和 3.0.4 版本开始，TiDB 通过 session 变量 `@@tidb_allow_remove_auto_inc` 控制是否允许通过 `ALTER TABLE MODIFY` 或 `ALTER TABLE CHANGE` 来移除列的 `AUTO_INCREMENT` 属性，默认是不允许移除。\n- `ALTER TABLE` 需要 `FORCE` 选项来将 `AUTO_INCREMENT` 设置为较小的值。\n- 将 `AUTO_INCREMENT` 设置为小于 `MAX(<auto_increment_column>)` 的值会导致重复键，因为预先存在的值不会被跳过。\n"
        },
        {
          "name": "auto-random.md",
          "type": "blob",
          "size": 12.3349609375,
          "content": "---\ntitle: AUTO_RANDOM\nsummary: 本文介绍了 TiDB 的 `AUTO_RANDOM` 列属性。\naliases: ['/docs-cn/dev/auto-random/','/docs-cn/dev/reference/sql/attributes/auto-random/']\n---\n\n# AUTO_RANDOM <span class=\"version-mark\">从 v3.1.0 版本开始引入</span>\n\n## 使用场景\n\n由于 `AUTO_RANDOM` 的值具有随机性和唯一性，因此 `AUTO_RANDOM` 通常用于代替 [`AUTO_INCREMENT`](/auto-increment.md)，以避免 TiDB 分配连续的 ID 值造成单个存储节点的写热点问题。如果当前表的 `AUTO_INCREMENT` 列是主键列，且列类型为 `BIGINT`，可以通过 `ALTER TABLE t MODIFY COLUMN id BIGINT AUTO_RANDOM(5);` 从 `AUTO_INCREMENT` 切换成 `AUTO_RANDOM`。\n\n关于如何在高并发写入场景下调优 TiDB，请参阅 [TiDB 高并发写入场景最佳实践](/best-practices/high-concurrency-best-practices.md)。\n\n在 [`CREATE TABLE`](/sql-statements/sql-statement-create-table.md) 语句中的 `AUTO_RANDOM_BASE` 参数，也可以用来指定 `AUTO_RANDOM` 自增部分的初始值，该参数可以被认为属于内部接口的一部分，对于用户而言请忽略。\n\n## 基本概念\n\n`AUTO_RANDOM` 是应用在 `BIGINT` 类型列的属性，用于列值的自动分配。其自动分配的值满足**随机性**和**唯一性**。\n\n以下语句均可创建包含 `AUTO_RANDOM` 列的表，其中 `AUTO_RANDOM` 列必须被包含在主键中，并且是主键的第一列。\n\n```sql\nCREATE TABLE t (a BIGINT AUTO_RANDOM, b VARCHAR(255), PRIMARY KEY (a));\nCREATE TABLE t (a BIGINT PRIMARY KEY AUTO_RANDOM, b VARCHAR(255));\nCREATE TABLE t (a BIGINT AUTO_RANDOM(6), b VARCHAR(255), PRIMARY KEY (a));\nCREATE TABLE t (a BIGINT AUTO_RANDOM(5, 54), b VARCHAR(255), PRIMARY KEY (a));\nCREATE TABLE t (a BIGINT AUTO_RANDOM(5, 54), b VARCHAR(255), PRIMARY KEY (a, b));\n```\n\n`AUTO_RANDOM` 关键字可以被包裹在 TiDB 可执行注释中，注释语法请参考 [TiDB 可执行注释](/comment-syntax.md#tidb-可执行的注释语法)。\n\n```sql\nCREATE TABLE t (a bigint /*T![auto_rand] AUTO_RANDOM */, b VARCHAR(255), PRIMARY KEY (a));\nCREATE TABLE t (a bigint PRIMARY KEY /*T![auto_rand] AUTO_RANDOM */, b VARCHAR(255));\nCREATE TABLE t (a BIGINT /*T![auto_rand] AUTO_RANDOM(6) */, b VARCHAR(255), PRIMARY KEY (a));\nCREATE TABLE t (a BIGINT  /*T![auto_rand] AUTO_RANDOM(5, 54) */, b VARCHAR(255), PRIMARY KEY (a));\n```\n\n在用户执行 `INSERT` 语句时：\n\n- 如果语句中显式指定了 `AUTO_RANDOM` 列的值，则该值会被正常插入到表中。\n- 如果语句中没有显式指定 `AUTO_RANDOM` 列的值，TiDB 会自动生成一个随机的值插入到表中。\n\n```sql\ntidb> CREATE TABLE t (a BIGINT PRIMARY KEY AUTO_RANDOM, b VARCHAR(255)) /*T! PRE_SPLIT_REGIONS=2 */ ;\nQuery OK, 0 rows affected, 1 warning (0.01 sec)\n\ntidb> INSERT INTO t(a, b) VALUES (1, 'string');\nQuery OK, 1 row affected (0.00 sec)\n\ntidb> SELECT * FROM t;\n+---+--------+\n| a | b      |\n+---+--------+\n| 1 | string |\n+---+--------+\n1 row in set (0.01 sec)\n\ntidb> INSERT INTO t(b) VALUES ('string2');\nQuery OK, 1 row affected (0.00 sec)\n\ntidb> INSERT INTO t(b) VALUES ('string3');\nQuery OK, 1 row affected (0.00 sec)\n\ntidb> SELECT * FROM t;\n+---------------------+---------+\n| a                   | b       |\n+---------------------+---------+\n|                   1 | string  |\n| 1152921504606846978 | string2 |\n| 4899916394579099651 | string3 |\n+---------------------+---------+\n3 rows in set (0.00 sec)\n\ntidb> SHOW CREATE TABLE t;\n+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table | Create Table                                                                                                                                                                                                                                                    |\n+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| t     | CREATE TABLE `t` (\n  `a` bigint NOT NULL /*T![auto_rand] AUTO_RANDOM(5) */,\n  `b` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`a`) /*T![clustered_index] CLUSTERED */\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin /*T! PRE_SPLIT_REGIONS=2 */ |\n+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\ntidb> SHOW TABLE t REGIONS;\n+-----------+-----------------------------+-----------------------------+-----------+-----------------+---------------------+------------+---------------+------------+----------------------+------------------+------------------------+------------------+\n| REGION_ID | START_KEY                   | END_KEY                     | LEADER_ID | LEADER_STORE_ID | PEERS               | SCATTERING | WRITTEN_BYTES | READ_BYTES | APPROXIMATE_SIZE(MB) | APPROXIMATE_KEYS | SCHEDULING_CONSTRAINTS | SCHEDULING_STATE |\n+-----------+-----------------------------+-----------------------------+-----------+-----------------+---------------------+------------+---------------+------------+----------------------+------------------+------------------------+------------------+\n|     62798 | t_158_                      | t_158_r_2305843009213693952 |     62810 |              28 | 62811, 62812, 62810 |          0 |           151 |          0 |                    1 |                0 |                        |                  |\n|     62802 | t_158_r_2305843009213693952 | t_158_r_4611686018427387904 |     62803 |               1 | 62803, 62804, 62805 |          0 |            39 |          0 |                    1 |                0 |                        |                  |\n|     62806 | t_158_r_4611686018427387904 | t_158_r_6917529027641081856 |     62813 |               4 | 62813, 62814, 62815 |          0 |           160 |          0 |                    1 |                0 |                        |                  |\n|      9289 | t_158_r_6917529027641081856 | 78000000                    |     48268 |               1 | 48268, 58951, 62791 |          0 |         10628 |      43639 |                    2 |             7999 |                        |                  |\n+-----------+-----------------------------+-----------------------------+-----------+-----------------+---------------------+------------+---------------+------------+----------------------+------------------+------------------------+------------------+\n4 rows in set (0.00 sec)\n```\n\nTiDB 自动分配的 `AUTO_RANDOM(S, R)` 列值共有 64 位：\n\n- `S` 表示分片位的数量，取值范围是 `1` 到 `15`。默认为 `5`。\n- `R` 表示自动分配值域的总长度，取值范围是 `32` 到 `64`。默认为 `64`。\n\n有符号位的 `AUTO_RANDOM` 列值的具体结构如下：\n\n| 符号位 | 保留位      | 分片位 | 自增位       |\n|--------|-------------|--------|--------------|\n| 1 bit | `64-R` bits | `S` bits | `R-1-S` bits |\n\n无符号位的 `AUTO_RANDOM` 列值的具体结构如下：\n\n| 保留位      | 分片位 | 自增位     |\n|-------------|--------|------------|\n| `64-R` bits | `S` bits | `R-S` bits |\n\n- 是否有符号位取决于该列是否存在 `UNSIGNED` 属性。\n- 保留位的长度为 `64-R`，保留位的内容始终为 `0`。\n- 分片位的内容通过计算当前事务的开始时间的哈希值而得。要使用不同的分片位数量（例如 10），可以在建表时指定 `AUTO_RANDOM(10)`。\n- 自增位的值保存在存储引擎中，按顺序分配，每次分配完值后会自增 1。自增位保证了 `AUTO_RANDOM` 列值全局唯一。当自增位耗尽后，再次自动分配时会报 `Failed to read auto-increment value from storage engine` 的错误。\n- 关于取值范围：最终生成值包含的最大位数 = 分片位 + 自增位。有符号位的列的取值范围是 `[-(2^(R-1))+1, (2^(R-1))-1]`。无符号位的列的取值范围是 `[0, (2^R)-1]`。\n- `AUTO_RANDOM` 可以与 [`PRE_SPLIT_REGIONS`](/sql-statements/sql-statement-split-region.md#pre_split_regions) 结合使用，用来在建表成功后就开始将表中的数据预均匀切分 `2^(PRE_SPLIT_REGIONS)` 个 Region。\n\n> **注意：**\n>\n> 分片位长度 (`S`) 的选取：\n>\n> - 由于总位数固定为 64 位，分片位的数量会影响到自增位的数量：当分片位数增加时，自增位数会减少，反之亦然。因此，你需要权衡“自动分配值的随机性”以及“可用空间”。\n> - 最佳实践是将分片位设置为 `log(2, x)`，其中 `x` 为当前集群存储引擎的数量。例如，一个 TiDB 集群中存在 16 个 TiKV，分片位可以设置为 `log(2, 16)`，即 `4`。在所有 Region 被均匀调度到各个 TiKV 上以后，此时大批量写入的负载可被均匀分布到不同 TiKV 节点，以实现资源最大化利用。\n>\n> 值域长度 (`R`) 的选取：\n>\n> - 通常，在应用程序的数值类型无法表示完整的 64 位整数时需要设置 `R` 参数。\n> - 例如：JSON number 类型的取值范围为 `[-(2^53)+1, (2^53)-1]`，而 TiDB 很容易会为 `AUTO_RANDOM(5)` 的列分配超出该范围的整数，导致应用程序读取该列的值时出现异常。此时，对于有符号的列你可以用 `AUTO_RANDOM(5, 54)` 代替 `AUTO_RANDOM(5)`，无符号列可以用 `AUTO_RANDOM(5, 53)` 代替 `AUTO_RANDOM(5)`，这样使得 TiDB 不会分配出大于 `9007199254740991` (2^53-1) 的整数。\n\n`AUTO RANDOM` 列隐式分配的值会影响 `last_insert_id()`。可以使用 `SELECT last_insert_id()` 获取上一次 TiDB 隐式分配的 ID。\n\n要查看某张含有 `AUTO_RANDOM` 属性的表的分片位数量，除了 `SHOW CREATE TABLE` 以外，还可以在系统表 `INFORMATION_SCHEMA.TABLES` 中 `TIDB_ROW_ID_SHARDING_INFO` 一列中查到模式为 `PK_AUTO_RANDOM_BITS=x` 的值，其中 `x` 为分片位的数量。\n\n创建完一张含有 `AUTO_RANDOM` 属性的表后，可以使用 `SHOW WARNINGS` 查看当前表可支持的最大隐式分配的次数：\n\n```sql\nCREATE TABLE t (a BIGINT AUTO_RANDOM, b VARCHAR(255), PRIMARY KEY (a));\nSHOW WARNINGS;\n```\n\n输出结果如下：\n\n```sql\n+-------+------+---------------------------------------------------------+\n| Level | Code | Message                                                 |\n+-------+------+---------------------------------------------------------+\n| Note  | 1105 | Available implicit allocation times: 288230376151711743 |\n+-------+------+---------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n## ID 隐式分配规则\n\n`AUTO_RANDOM` 列隐式分配的值和自增列类似，也遵循 session 变量 [`auto_increment_increment`](/system-variables.md#auto_increment_increment) 和 [`auto_increment_offset`](/system-variables.md#auto_increment_offset) 的控制，其中隐式分配值的自增位 (ID) 满足等式 `(ID - auto_increment_offset) % auto_increment_increment == 0`。\n\n## 使用限制\n\n目前在 TiDB 中使用 `AUTO_RANDOM` 有以下限制：\n\n- 要使用显式插入的功能，需要将系统变量 `@@allow_auto_random_explicit_insert` 的值设置为 `1`（默认值为 `0`）。不建议自行显式指定含有 `AUTO_RANDOM` 列的值。不恰当地显式赋值，可能会导致该表提前耗尽用于自动分配的数值。\n- 该属性必须指定在 `BIGINT` 类型的主键列上，否则会报错。此外，当主键属性为 `NONCLUSTERED` 时，即使是整型主键列，也不支持使用 `AUTO_RANDOM`。要了解关于 `CLUSTERED` 主键的详细信息，请参考[聚簇索引](/clustered-indexes.md)。\n- 不支持使用 `ALTER TABLE` 来修改 `AUTO_RANDOM` 属性，包括添加或移除该属性。\n- 支持将 `AUTO_INCREMENT` 属性改为 `AUTO_RANDOM` 属性。但在 `AUTO_INCREMENT` 的列数据最大值已接近 `BIGINT` 类型最大值的情况下，修改可能会失败。\n- 不支持修改含有 `AUTO_RANDOM` 属性的主键列的列类型。\n- 不支持与 `AUTO_INCREMENT` 同时指定在同一列上。\n- 不支持与列的默认值 `DEFAULT` 同时指定在同一列上。\n- `AUTO_RANDOM` 列的数据很难迁移到 `AUTO_INCREMENT` 列上，因为 `AUTO_RANDOM` 列自动分配的值通常都很大。\n"
        },
        {
          "name": "backup-and-restore-using-dumpling-lightning.md",
          "type": "blob",
          "size": 7.7119140625,
          "content": "---\ntitle: 使用 Dumpling 和 TiDB Lightning 备份与恢复\nsummary: 了解如何使用 Dumpling 和 TiDB Lightning 备份与恢复集群数据。\n---\n\n# 使用 Dumpling 和 TiDB Lightning 备份与恢复\n\n本文档介绍如何使用 Dumpling 和 TiDB Lightning 进行全量备份与恢复。\n\n在备份与恢复场景中，如果需要全量备份少量数据（例如小于 50 GiB），且不要求备份速度，你可以使用 [Dumpling](/dumpling-overview.md) 从 TiDB 数据库导出数据进行备份，再使用 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 将数据导入至 TiDB 数据库实现恢复。\n\n如果需要备份大量数据，建议使用 [BR](/br/backup-and-restore-overview.md)。注意，Dumpling 也可以用于导出大量数据，但 BR 是更好的工具。\n\n## 前提条件\n\n- 安装 Dumpling：\n\n    ```shell\n    tiup install dumpling\n    ```\n\n- 安装 TiDB Lightning：\n\n    ```shell\n    tiup install tidb-lightning\n    ```\n\n- [获取 Dumpling 所需上游数据库权限](/dumpling-overview.md#从-tidbmysql-导出数据)\n- [获取 TiDB Lightning 所需下游数据库权限](/tidb-lightning/tidb-lightning-requirements.md#目标数据库权限要求)\n\n## 资源要求\n\n**操作系统**：本文档示例使用的是若干新的、纯净版 CentOS 7 实例，你可以在本地虚拟化一台主机，或在供应商提供的平台上部署一台小型的云虚拟主机。TiDB Lightning 运行过程中，默认会占满 CPU，建议单独部署在一台主机上。如果条件不允许，你可以将 TiDB Lightning 和其他组件（比如 `tikv-server`）部署在同一台机器上，然后设置 `region-concurrency` 配置项的值为逻辑 CPU 数的 75%，以限制 TiDB Lightning 对 CPU 资源的使用。\n\n**内存和 CPU**：因为 TiDB Lightning 对计算机资源消耗较高，建议分配 64 GB 以上的内存以及 32 核以上的 CPU，而且确保 CPU 核数和内存 (GB) 比为 1:2 以上，以获取最佳性能。\n\n**磁盘空间**：\n\n推荐使用 Amazon S3、Google Cloud Storage (GCS) 和 Azure Blob Storage 等外部存储，以便能够快速存储备份文件，且不受磁盘空间限制。\n\n如果需要保存单次备份数据到本地磁盘，需要注意以下磁盘空间限制：\n\n- Dumpling 需要能够储存整个数据源的存储空间，即可以容纳要导出的所有上游表的空间。计算方式参考[目标数据库所需空间](/tidb-lightning/tidb-lightning-requirements.md#目标数据库所需空间)。\n- TiDB Lightning 导入期间，需要临时空间来存储排序键值对，磁盘空间需要至少能存储数据源的最大单表。\n\n**说明**：目前无法精确计算 Dumpling 从 TiDB 导出的数据大小，但你可以用下面 SQL 语句统计信息表的 `DATA_LENGTH` 字段估算数据量：\n\n```sql\n-- 统计所有 schema 大小\nSELECT\n  TABLE_SCHEMA,\n  FORMAT_BYTES(SUM(DATA_LENGTH)) AS 'Data Size',\n  FORMAT_BYTES(SUM(INDEX_LENGTH)) 'Index Size'\nFROM\n  information_schema.tables\nGROUP BY\n  TABLE_SCHEMA;\n\n-- 统计最大的 5 个单表\nSELECT\n  TABLE_NAME,\n  TABLE_SCHEMA,\n  FORMAT_BYTES(SUM(data_length)) AS 'Data Size',\n  FORMAT_BYTES(SUM(index_length)) AS 'Index Size',\n  FORMAT_BYTES(SUM(data_length+index_length)) AS 'Total Size'\nFROM\n  information_schema.tables\nGROUP BY\n  TABLE_NAME,\n  TABLE_SCHEMA\nORDER BY\n  SUM(DATA_LENGTH+INDEX_LENGTH) DESC\nLIMIT\n  5;\n```\n\n### 目标 TiKV 集群的磁盘空间要求\n\n目标 TiKV 集群必须有足够空间接收新导入的数据。除了[标准硬件配置](/hardware-and-software-requirements.md)以外，目标 TiKV 集群的总存储空间必须大于**数据源大小 × [副本数量](/faq/manage-cluster-faq.md#每个-region-的-replica-数量可配置吗调整的方法是) × 2**。例如，集群默认使用 3 副本，那么总存储空间需为数据源大小的 6 倍以上。公式中的 2 倍可能难以理解，其依据是以下因素的估算空间占用：\n\n* 索引会占据额外的空间。\n* RocksDB 的空间放大效应。\n\n## 使用 Dumpling 备份全量数据\n\n1. 运行以下命令，从 TiDB 导出全量数据至 Amazon S3 存储路径 `s3://my-bucket/sql-backup`：\n\n    ```shell\n    tiup dumpling -h ${ip} -P 3306 -u root -t 16 -r 200000 -F 256MiB -B my_db1 -f 'my_db1.table[12]' -o 's3://my-bucket/sql-backup'\n    ```\n\n    Dumpling 默认导出数据格式为 SQL 文件，你也可以通过设置 `--filetype` 指定导出文件的类型。\n\n    关于更多 Dumpling 的配置，请参考 [Dumpling 主要选项表](/dumpling-overview.md#dumpling-主要选项表)。\n\n2. 导出完成后，可以在数据存储目录 `s3://my-bucket/sql-backup` 查看导出的备份文件。\n\n## 使用 TiDB Lightning 恢复全量数据\n\n1. 编写配置文件 `tidb-lightning.toml`，将 Dumpling 备份的全量数据从 `s3://my-bucket/sql-backup` 恢复到目标 TiDB 集群：\n\n    ```toml\n    [lightning]\n    # 日志\n    level = \"info\"\n    file = \"tidb-lightning.log\"\n\n    [tikv-importer]\n    # \"local\"：默认使用该模式，适用于 TB 级以上大数据量，但导入期间下游 TiDB 无法对外提供服务。\n    # \"tidb\"：TB 级以下数据量也可以采用`tidb`后端模式，下游 TiDB 可正常提供服务。关于后端模式更多信息请参阅：https://docs.pingcap.com/tidb/stable/tidb-lightning-backends\n    backend = \"local\"\n    # 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小。建议设为与 `data-source-dir` 不同的磁盘目录并使用闪存介质，独占 IO 会获得更好的导入性能\n    sorted-kv-dir = \"${sorted-kv-dir}\"\n\n    [mydumper]\n    # 源数据目录，即上一章节中 Dumpling 保存数据的路径。\n    data-source-dir = \"${data-path}\" # 本地或 S3 路径，例如：'s3://my-bucket/sql-backup'\n\n    [tidb]\n    # 目标集群的信息\n    host = ${host}                # 例如：172.16.32.1\n    port = ${port}                # 例如：4000\n    user = \"${user_name}\"         # 例如：\"root\"\n    password = \"${password}\"      # 例如：\"rootroot\"\n    status-port = ${status-port}  # 导入过程 Lightning 需要在从 TiDB 的“状态端口”获取表结构信息，例如：10080\n    pd-addr = \"${ip}:${port}\"     # 集群 PD 的地址，Lightning 通过 PD 获取部分信息，例如 172.16.31.3:2379。当 backend = \"local\" 时 status-port 和 pd-addr 必须正确填写，否则导入将出现异常。\n    ```\n\n    关于更多 TiDB Lightning 的配置，请参考 [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)。\n\n2. 运行 `tidb-lightning`。如果直接在命令行中启动程序，可能会因为 `SIGHUP` 信号而退出，建议配合 `nohup` 或 `screen` 等工具，如：\n\n    若从 Amazon S3 导入，则需将有权限访问该 S3 后端存储的账号的 SecretKey 和 AccessKey 作为环境变量传入 Lightning 节点。同时还支持从 `~/.aws/credentials` 读取凭证文件。\n\n    ```shell\n    export AWS_ACCESS_KEY_ID=${access_key}\n    export AWS_SECRET_ACCESS_KEY=${secret_key}\n    nohup tiup tidb-lightning -config tidb-lightning.toml > nohup.out 2>&1 &\n    ```\n\n3. 导入开始后，可以通过 `grep` 日志关键字 `progress` 查看进度，默认 5 分钟更新一次。\n\n4. 导入完毕后，TiDB Lightning 会自动退出。查看 `tidb-lightning.log` 日志末尾是否有 `the whole procedure completed`，如果有，数据导入成功，恢复完成。如果没有，则表示导入遇到了问题，可根据日志中的 error 提示解决遇到的问题。\n\n> **注意：**\n>\n> 无论导入成功与否，最后一行都会显示 `tidb lightning exit`。它只是表示 TiDB Lightning 正常退出，不代表恢复任务完成。\n\n如果恢复过程中遇到问题，请参见 [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)。\n"
        },
        {
          "name": "basic-features.md",
          "type": "blob",
          "size": 26.8310546875,
          "content": "---\ntitle: TiDB 功能概览\nsummary: 了解 TiDB 的功能概览。\naliases: ['/docs-cn/dev/basic-features/','/docs-cn/dev/experimental-features-4.0/','/zh/tidb/dev/experimental-features-4.0/','/zh/tidb/dev/experimental-features']\n---\n\n# TiDB 功能概览\n\n本文列出了 TiDB 功能在不同版本中的支持变化情况，包括[长期支持版本 (LTS)](/releases/versioning.md#长期支持版本) 和最新的 LTS 版本之后的[开发里程碑版本 (DMR)](/releases/versioning.md#开发里程碑版本)。\n\n> **注意：**\n>\n> PingCAP 不提供基于 DMR 版本的 bug 修复版本，如有 bug，会在后续版本中修复。如无特殊需求，建议使用[最新 LTS 版本](https://docs.pingcap.com/zh/tidb/stable)。\n>\n> 下表中出现的缩写字母含义如下：\n>\n> - Y：已 GA 的功能，可以在生产环境中使用。注意即使某个功能在 DMR 版本中 GA，也建议在后续 LTS 版本中将该功能用于生产环境。\n> - N：不支持该功能。\n> - E：未 GA 的功能，即实验特性 (experimental)，请注意使用场景限制。实验特性会在未事先通知的情况下发生变化或删除。语法和实现可能会在 GA 前发生变化。如果遇到问题，请在 GitHub 上提交 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n## 数据类型，函数和操作符\n\n| 数据类型，函数，操作符 | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [数值类型](/data-type-numeric.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [日期和时间类型](/data-type-date-and-time.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [字符串类型](/data-type-string.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [JSON 类型](/data-type-json.md) |  Y |  Y |  Y  | Y | Y | E | E | E | E | E |\n| [向量数据类型](/vector-search/vector-search-data-types.md) |  E |  N |  N  | N | N | N | N | N | N | N |\n| [控制流程函数](/functions-and-operators/control-flow-functions.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [字符串函数](/functions-and-operators/string-functions.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [数值函数与操作符](/functions-and-operators/numeric-functions-and-operators.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [日期和时间函数](/functions-and-operators/date-and-time-functions.md)|  Y  |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [位函数和操作符](/functions-and-operators/bit-functions-and-operators.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [Cast 函数和操作符](/functions-and-operators/cast-functions-and-operators.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [向量函数和操作符](/vector-search/vector-search-functions-and-operators.md) |  E |  N |  N  | N | N | N | N | N | N | N |\n| [信息函数](/functions-and-operators/information-functions.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [JSON 函数](/functions-and-operators/json-functions.md) |  Y |  Y |  Y  | Y | Y | E | E | E | E | E |\n| [聚合函数](/functions-and-operators/aggregate-group-by-functions.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [窗口函数](/functions-and-operators/window-functions.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [其他函数](/functions-and-operators/miscellaneous-functions.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [操作符](/functions-and-operators/operators.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [字符集和排序规则](/character-set-and-collation.md) [^1] |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [用户级别锁](/functions-and-operators/locking-functions.md) |  Y |  Y |  Y  | Y | Y | Y | N | N | N | N |\n\n## 索引和约束\n\n| 索引和约束 | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [表达式索引](/sql-statements/sql-statement-create-index.md#表达式索引) [^2] | Y |  Y |  Y  | Y | Y | E | E | E | E | E |\n| [列式存储 (TiFlash)](/tiflash/tiflash-overview.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [使用 FastScan 加速 OLAP 场景下的查询](/tiflash/use-fastscan.md) | Y |  Y |  Y  | Y | E | N | N | N | N | N |\n| [RocksDB 引擎](/storage-engine/rocksdb-overview.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [Titan 插件](/storage-engine/titan-overview.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [Titan Level Merge](/storage-engine/titan-configuration.md#level-merge实验功能) | E |  E |  E  | E | E | E | E | E | E | E |\n| [使用 bucket 提高数据扫描并发度](/tune-region-performance.md#使用-bucket-增加并发) | E |  E |  E  | E | E | E | N | N | N | N |\n| [不可见索引](/sql-statements/sql-statement-create-index.md#不可见索引) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [复合主键](/constraints.md#主键约束) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [`CHECK` 约束](/constraints.md#check-约束) | Y |  Y |  Y  | N | N | N | N | N | N | N |\n| [唯一约束](/constraints.md#唯一约束) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [整型主键上的聚簇索引](/clustered-indexes.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [复合或非整型主键上的聚簇索引](/clustered-indexes.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [多值索引](/sql-statements/sql-statement-create-index.md#多值索引) | Y |  Y |  Y  | Y | N | N | N | N | N | N |\n| [外键约束](/foreign-key.md) | Y |  E |  E  | E | N | N | N | N | N | N |\n| [TiFlash 延迟物化](/tiflash/tiflash-late-materialization.md) | Y |  Y |  Y  | Y | N | N | N | N | N | N |\n| [全局索引 (Global Index)](/partitioned-table.md#全局索引) | Y |  N | N  | N | N | N | N | N | N | N |\n| [向量索引](/vector-search/vector-search-index.md) |  E |  N |  N  | N | N | N | N | N | N | N |\n\n## SQL 语句\n\n| SQL 语句 [^3] | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| `SELECT`，`INSERT`，`UPDATE`，`DELETE`，`REPLACE` | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| `INSERT ON DUPLICATE KEY UPDATE` | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| `LOAD DATA INFILE` | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| `SELECT INTO OUTFILE` | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| `INNER JOIN`, <code>LEFT\\|RIGHT [OUTER] JOIN</code> |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| `UNION`，`UNION ALL` | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [`EXCEPT` 和 `INTERSECT` 运算符](/functions-and-operators/set-operators.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| `GROUP BY`，`ORDER BY` | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [`GROUP BY` 修饰符](/functions-and-operators/group-by-modifier.md) | Y |  Y |  Y  | N | N | N | N | N | N | N |\n| [窗口函数](/functions-and-operators/window-functions.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [公共表表达式 (CTE)](/sql-statements/sql-statement-with.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| `START TRANSACTION`，`COMMIT`，`ROLLBACK` | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [`EXPLAIN`](/sql-statements/sql-statement-explain.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md) | Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [用户自定义变量](/user-defined-variables.md) | E |  E |  E  | E | E | E | E | E | E | E |\n| [`BATCH [ON COLUMN] LIMIT INTEGER DELETE`](/sql-statements/sql-statement-batch.md) | Y |  Y |  Y  | Y | Y | Y | N | N | N | N |\n| [`BATCH [ON COLUMN] LIMIT INTEGER INSERT/UPDATE/REPLACE`](/sql-statements/sql-statement-batch.md) | Y |  Y |  Y  | Y | Y | N | N | N | N | N |\n| [`ALTER TABLE ... COMPACT`](/sql-statements/sql-statement-alter-table-compact.md) | Y |  Y |  Y  | Y | Y | E | N | N | N | N |\n| [表级锁 (Table Lock)](/sql-statements/sql-statement-lock-tables-and-unlock-tables.md) | E |  E  |  E  | E | E | E | E | E | E | E |\n| [物化列式存储的查询结果](/tiflash/tiflash-results-materialization.md) | Y |  Y |  Y  | Y | E | N | N | N | N | N |\n\n## 高级 SQL 功能\n\n| 高级 SQL 功能 | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [向量搜索](/vector-search/vector-search-overview.md) |  E |  N |  N  | N | N | N | N | N | N | N |\n| [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md) | Y |  Y  |  Y  | Y | Y | Y | Y | Y | E | E |\n| [非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md) | Y |  Y  |  Y  | E | N | N | N | N | N | N |\n| [实例级执行计划缓存](/system-variables.md#tidb_enable_instance_plan_cache-从-v840-版本开始引入) | E |  N  |  N  | N | N | N | N | N | N | N |\n| [执行计划绑定 (SQL Binding)](/sql-plan-management.md#执行计划绑定-sql-binding) | Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [跨数据库执行计划绑定 (Cross-DB Binding)](/sql-plan-management.md#跨数据库绑定执行计划-cross-db-binding) | Y |  Y  |  N  | N | N | N | N | N | N | N |\n| [根据历史执行计划创建绑定](/sql-plan-management.md#根据历史执行计划创建绑定) | Y |  Y  |  Y  | Y | E | N | N | N | N | N |\n| [下推计算结果缓存 (Coprocessor Cache)](/coprocessor-cache.md) | Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [Stale Read](/stale-read.md) | Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [Follower Read](/follower-read.md) | Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [通过系统变量 `tidb_snapshot` 读取历史数据](/read-historical-data.md) | Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [Optimizer hints](/optimizer-hints.md) | Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [MPP 执行引擎](/explain-mpp.md) | Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [MPP 执行引擎 - compression exchange](/explain-mpp.md#mpp-version-和-exchange-数据压缩) | Y |  Y  |  Y  | Y | N | N | N | N | N | N |\n| [TiFlash Pipeline 执行模型](/tiflash/tiflash-pipeline-model.md) | Y |  Y  |  Y  | N | N | N | N | N | N | N |\n| [TiFlash 副本选择策略](/system-variables.md#tiflash_replica_read-从-v730-版本开始引入) | Y |  Y  |  Y  | N | N | N | N | N | N | N |\n| [索引合并](/explain-index-merge.md) | Y |  Y  |  Y  | Y | Y | Y | Y | E | E | E |\n| [基于 SQL 的数据放置规则](/placement-rules-in-sql.md) | Y |  Y  |  Y  | Y | Y | Y | E | E | N | N |\n| [Cascades Planner](/system-variables.md#tidb_enable_cascades_planner) |  E  |  E  |  E  | E | E | E | E | E | E | E |\n| [Runtime Filter](/runtime-filter.md) | Y |  Y  |  Y  | N | N | N | N | N | N | N |\n\n## 数据定义语言 (DDL)\n\n| 数据定义语言 (DDL) | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| `CREATE`，`DROP`，`ALTER`，`RENAME`，`TRUNCATE` |  Y  |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [生成列](/generated-columns.md) |  Y  |  Y  |  Y  | Y | E | E | E | E | E | E |\n| [视图](/views.md) |  Y  |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [序列](/sql-statements/sql-statement-create-sequence.md) |  Y  |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [`AUTO_INCREMENT` 列](/auto-increment.md) |  Y  |  Y  |  Y  | Y | Y[^4] | Y | Y | Y | Y | Y |\n| [`AUTO_RANDOM` 列](/auto-random.md) |  Y  |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [TTL (Time to Live)](/time-to-live.md) |  Y  |  Y  |  Y  | Y | E | N | N | N | N | N |\n| [DDL 算法断言](/sql-statements/sql-statement-alter-table.md) |  Y  |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| 在单条语句中添加多列 |  Y  |  Y  |  Y  | Y | Y | E | E | E | E | E |\n| [更改列类型](/sql-statements/sql-statement-modify-column.md) |  Y  |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [临时表](/temporary-tables.md) |  Y  |  Y  |  Y  | Y | Y | Y | Y | Y | N | N |\n| 并行 DDL |  Y  |  Y  |  Y  | Y | Y | N | N | N | N | N |\n| [添加索引加速](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入) |  Y  |  Y  |  Y  | Y | Y | N | N | N | N | N |\n| [元数据锁](/metadata-lock.md) |  Y  |  Y  | Y | Y | N | N | N | N | N |\n| [`FLASHBACK CLUSTER`](/sql-statements/sql-statement-flashback-cluster.md) |  Y  |  Y  |  Y  | Y | Y | N | N | N | N | N |\n| [暂停](/sql-statements/sql-statement-admin-pause-ddl.md)/[恢复](/sql-statements/sql-statement-admin-resume-ddl.md) DDL |  Y  |  Y  | Y | N | N | N | N | N | N | N |\n| [TiDB 加速建表](/accelerated-table-creation.md) | Y | E | N | N | N | N | N | N | N | N |\n| [设置 BDR Role 用于 TiCDC 双向同步时同步 DDL](/sql-statements/sql-statement-admin-bdr-role.md#admin-setshowunset-bdr-role) | Y | E | N | N | N | N | N | N | N | N |\n\n## 事务\n\n| 事务 | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [Async commit](/system-variables.md#tidb_enable_async_commit-从-v50-版本开始引入) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [1PC](/system-variables.md#tidb_enable_1pc-从-v50-版本开始引入) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [大事务 (1 TiB)](/transaction-overview.md#事务限制) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [悲观事务](/pessimistic-transaction.md) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [乐观事务](/optimistic-transaction.md) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [可重复读隔离（快照隔离）](/transaction-isolation-levels.md) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [读已提交隔离](/transaction-isolation-levels.md) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [自动终止长时间未提交的空闲事务](/system-variables.md#tidb_idle_transaction_timeout-从-v760-版本开始引入) |  Y |  Y  | N | N | N | N | N | N | N | N |\n| [批量 DML 语句的执行方式 (`tidb_dml_type = \"bulk\"`)](/system-variables.md#tidb_dml_type-从-v800-版本开始引入) | E |  N | N | N | N | N | N | N | N |\n\n## 分区\n\n| 分区 | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [Range 分区](/partitioned-table.md#range-分区) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [Hash 分区](/partitioned-table.md#hash-分区) |  Y |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [Key 分区](/partitioned-table.md#key-分区) |  Y |  Y  |  Y  | Y | N | N | N | N | N | N |\n| [List 分区](/partitioned-table.md#list-分区) |  Y |  Y  |  Y  | Y | Y | Y | E | E | E | E |\n| [List COLUMNS 分区](/partitioned-table.md#list-columns-分区) |  Y |  Y  |  Y  | Y | Y | Y | E | E | E | E |\n| [List 和 List COLUMNS 分区表的默认分区](/partitioned-table.md#默认的-list-分区) |  Y |  Y  |  Y  | N | N | N | N | N | N | N |\n| [`EXCHANGE PARTITION`](/partitioned-table.md) |  Y |  Y  |  Y  | Y | Y | E | E | E | E | E |\n| [`REORGANIZE PARTITION`](/partitioned-table.md#重组分区) |  Y |  Y  |  Y  | Y | N | N | N | N | N | N |\n| [`COALESCE PARTITION`](/partitioned-table.md#减少分区数量) |  Y |  Y  |  Y  | Y | N | N | N | N | N | N |\n| [动态裁剪](/partitioned-table.md#动态裁剪模式) |  Y |  Y  |  Y  | Y | Y | Y | E | E | E | E |\n| [Range COLUMNS 分区](/partitioned-table.md#range-columns-分区) |  Y |  Y  |  Y  | Y | Y | N | N | N | N | N |\n| [Range INTERVAL 分区](/partitioned-table.md#range-interval-分区) |  Y |  Y  |  Y  | Y | E | N | N | N | N | N |\n| [分区表转换为非分区表](/partitioned-table.md#将分区表转换为非分区表) |  Y |  Y  |  Y  | N | N | N | N | N | N | N |\n| [对现有表进行分区](/partitioned-table.md#对现有表进行分区) |  Y |  Y  |  Y  | N | N | N | N | N | N | N |\n\n## 统计信息\n\n| 统计信息 | 8.5 | 8.1 | 7.5  | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:----:|:----:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [CM-Sketch](/statistics.md) | 默认关闭| 默认关闭| 默认关闭 | 默认关闭 | 默认关闭 | 默认关闭 | 默认关闭 | 默认关闭 | Y | Y |\n| [直方图](/statistics.md) |  Y  |  Y  |  Y   | Y | Y | Y | Y | Y | Y | Y |\n| [扩展统计信息](/extended-statistics.md) |  E  |  E  |  E   | E | E | E | E | E | E | E |\n| 统计反馈 |  N  |  N  |  N   | N | N | 已废弃 | 已废弃 | E | E | E |\n| [统计信息自动更新](/statistics.md#自动更新) |  Y  |  Y  | Y | Y | Y | Y | Y | Y | Y | Y |\n| [动态裁剪](/partitioned-table.md#动态裁剪模式) |  Y  |  Y   |  Y   | Y | Y | Y | E | E | E | E |\n| [收集部分列的统计信息](/statistics.md#收集部分列的统计信息) |  Y   |  E   |  E   | E | E | E | E | N | N | N |\n| [限制统计信息的内存使用量](/statistics.md#统计信息收集的内存限制) |  E   |  E  |  E   | E | E | E | N | N | N | N |\n| [随机采样约 10000 行数据来快速构建统计信息](/system-variables.md#tidb_enable_fast_analyze) | 已废弃 | 已废弃 | 已废弃 | E | E | E | E | E | E | E |\n| [锁定统计信息](/statistics.md#锁定统计信息) |  Y  |  Y  |  Y   | E | E | N | N | N | N | N |\n| [轻量级统计信息初始化](/statistics.md#加载统计信息) |  Y  |  Y  |  Y   | E | N | N | N | N | N | N |\n| [显示统计信息收集的进度](/sql-statements/sql-statement-show-analyze-status.md) |  Y  |  Y  |  Y   | N | N | N | N | N | N | N |\n\n## 安全\n\n| 安全 | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [传输层加密 (TLS)](/enable-tls-between-clients-and-servers.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [静态加密 (TDE)](/encryption-at-rest.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [基于角色的访问控制 (RBAC)](/role-based-access-control.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [证书鉴权](/certificate-authentication.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [`caching_sha2_password` 认证](/system-variables.md#default_authentication_plugin) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | N |\n| [`tidb_sm3_password` 认证](/system-variables.md#default_authentication_plugin) |  Y |  Y |  Y  | Y | Y | N | N | N | N | N |\n| [`tidb_auth_token` 认证](/security-compatibility-with-mysql.md#tidb_auth_token) |  Y |  Y |  Y  | Y | Y | N | N | N | N | N |\n| [`authentication_ldap_sasl` 认证](/system-variables.md#default_authentication_plugin) |  Y |  Y |  Y  | Y | N | N | N | N | N | N |\n| [`authentication_ldap_simple` 认证](/system-variables.md#default_authentication_plugin) |  Y |  Y |  Y  | Y | N | N | N | N | N | N |\n| [密码管理](/password-management.md) |  Y |  Y |  Y  | Y | Y | N | N | N | N | N |\n| [与 MySQL 兼容的 `GRANT` 权限管理](/privilege-management.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [动态权限](/privilege-management.md#动态权限) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [安全增强模式](/system-variables.md#tidb_enable_enhanced_security) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n| [日志脱敏](/log-redaction.md) |  Y |  Y |  Y  | Y | Y | Y | Y | Y | Y | Y |\n\n## 数据导入和导出\n\n| 数据导入和导出 | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [快速导入 (TiDB Lightning)](/tidb-lightning/tidb-lightning-overview.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [快速导入 (`IMPORT INTO`)](/sql-statements/sql-statement-import-into.md) | Y | Y | Y | N | N | N | N | N | N | N |\n| mydumper 逻辑导出 | 已废弃 | 已废弃 | 已废弃 | 已废弃 | 已废弃 | 已废弃 | 已废弃 | 已废弃 | 已废弃 | 已废弃 |\n| [Dumpling 逻辑导出](/dumpling-overview.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [事务 `LOAD DATA`](/sql-statements/sql-statement-load-data.md) [^5] | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [数据迁移工具](/migration-overview.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [TiDB Binlog](https://docs.pingcap.com/zh/tidb/v8.3/tidb-binlog-overview) [^6] | 已移除 | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [Change data capture (CDC)](/ticdc/ticdc-overview.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [TiCDC 支持保存数据到存储服务 (Amazon S3/GCS/Azure Blob Storage/NFS)](/ticdc/ticdc-sink-to-cloud-storage.md) | Y | Y | Y | Y | E | N | N | N | N | N |\n| [TiCDC 支持在两个 TiDB 集群之间进行双向复制](/ticdc/ticdc-bidirectional-replication.md) | Y | Y | Y | Y | Y | N | N | N | N | N |\n| [TiCDC OpenAPI v2](/ticdc/ticdc-open-api-v2.md) | Y | Y | Y | Y | N | N | N | N | N | N |\n| [DM](/dm/dm-overview.md) 支持迁移 MySQL 8.0  | Y | Y | E | E | E | E | N | N | N | N |\n\n## 管理，可视化和工具\n\n| 管理，可视化和工具 | 8.5 | 8.1 | 7.5 | 7.1 | 6.5 | 6.1 | 5.4 | 5.3 | 5.2 | 5.1 |\n|---|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|:---:|\n| [TiDB Dashboard 图形化展示](/dashboard/dashboard-intro.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [TiDB Dashboard 持续性能分析功能](/dashboard/continuous-profiling.md) | Y | Y | Y | Y | Y | Y | E | E | N | N |\n| [TiDB Dashboard Top SQL 功能](/dashboard/top-sql.md) | Y | Y | Y | Y | Y | Y | E | N | N | N |\n| [TiDB Dashboard SQL 诊断功能](/information-schema/information-schema-sql-diagnostics.md) | Y | Y | Y | Y | Y | E | E | E | E | E |\n| [TiDB Dashboard 集群诊断功能](/dashboard/dashboard-diagnostics-access.md) | Y | Y | Y | Y | Y | E | E | E | E | E |\n| [Grafana 中的 TiKV-FastTune 面板](/grafana-tikv-dashboard.md#tikv-fasttune-面板) | E | E | E | E | E | E | E | E | E | E |\n| [Information schema](/information-schema/information-schema.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [Metrics schema](/metrics-schema.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [Statements summary tables](/statement-summary-tables.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [Statements summary tables - 持久化 statements summary](/statement-summary-tables.md#持久化-statements-summary) | E | E | E | E | N | N | N | N | N | N |\n| [慢查询日志](/identify-slow-queries.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [TiUP 部署](/tiup/tiup-overview.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [Kubernetes operator](https://docs.pingcap.com/tidb-in-kubernetes/stable) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [内置物理备份](/br/backup-and-restore-use-cases.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [Global Kill](/sql-statements/sql-statement-kill.md) | Y | Y | Y | Y | Y | Y | E | E | E | E |\n| [Lock View](/information-schema/information-schema-data-lock-waits.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | E |\n| [`SHOW CONFIG`](/sql-statements/sql-statement-show-config.md) | Y | Y | Y | Y | Y | Y | Y | Y | Y | Y |\n| [`SET CONFIG`](/dynamic-config.md) | Y | Y | Y | Y | Y | Y | E | E | E | E |\n| [DM WebUI](/dm/dm-webui-guide.md) | E | E | E | E | E | E | N | N | N | N |\n| [前台限流](/tikv-configuration-file.md#前台限流) | Y | Y | Y | Y | Y | E | N | N | N | N |\n| [后台限流](/tikv-configuration-file.md#后台限流) | E | E | E | E | E | N | N | N | N | N |\n| [基于 EBS 的备份和恢复](https://docs.pingcap.com/zh/tidb-in-kubernetes/v1.4/volume-snapshot-backup-restore) | Y | Y | Y | Y | Y | N | N | N | N | N |\n| [PITR](/br/br-pitr-guide.md) | Y | Y | Y | Y | Y | N | N | N | N | N |\n| [全局内存控制](/configure-memory-usage.md#如何配置-tidb-server-实例使用内存的阈值) | Y | Y | Y | Y | Y | N | N | N | N | N |\n| [RawKV 跨集群复制](/tikv-configuration-file.md#api-version-从-v610-版本开始引入) | E | E | E| E | E | N | N | N | N | N |\n| [Green GC](/system-variables.md#tidb_gc_scan_lock_mode-从-v50-版本开始引入) | E | E | E | E | E | E | E | E | E | E |\n| [资源管控 (Resource Control)](/tidb-resource-control.md) | Y | Y | Y | Y | N | N | N | N | N | N |\n| [Runaway Queries 自动管理](/tidb-resource-control.md#管理资源消耗超出预期的查询-runaway-queries) | Y | Y | E | N | N | N | N | N | N | N |\n| [后台任务资源管控](/tidb-resource-control.md#管理后台任务) | E | E | E | N | N | N | N | N | N | N |\n| [TiFlash 存算分离架构与 S3 支持](/tiflash/tiflash-disaggregated-and-s3.md) | Y | Y | Y | E | N | N | N | N | N | N |\n| [选择执行分布式执行框架任务的 TiDB 节点](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) | Y | Y | Y | N | N | N | N | N | N | N |\n| 通过系统变量 [`tidb_enable_tso_follower_proxy`](/system-variables.md#tidb_enable_tso_follower_proxy-从-v530-版本开始引入) 控制 PD Follower Proxy 功能 | Y | Y | Y | Y | Y | Y | Y | Y | N | N |\n| 通过系统变量 [`pd_enable_follower_handle_region`](/system-variables.md#pd_enable_follower_handle_region-从-v760-版本开始引入) 控制 [Active PD Follower](/tune-region-performance.md#通过-active-pd-follower-提升-pd-region-信息查询服务的扩展能力) 功能 | Y | E | N | N | N | N | N | N | N | N |\n| [PD 微服务](/pd-microservices.md) | E | E | N | N | N | N | N | N | N | N |\n| [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md) | Y | Y | Y | E | N | N | N | N | N | N |\n| [全局排序](/tidb-global-sort.md) | Y | Y | E | N | N | N | N | N | N | N |\n| [TiProxy](/tiproxy/tiproxy-overview.md) | Y | Y | N | N | N | N | N | N | N | N |\n| [Schema 缓存](/schema-cache.md) | Y | N | N | N | N | N | N | N | N | N |\n\n[^1]: TiDB 误将 latin1 处理为 utf8 的子集。见 [TiDB #18955](https://github.com/pingcap/tidb/issues/18955)。\n\n[^2]: 从 v6.5.0 起，系统变量 [`tidb_allow_function_for_expression_index`](/system-variables.md#tidb_allow_function_for_expression_index-从-v520-版本开始引入) 所列出的函数已通过表达式索引的测试，可以在生产环境中创建并使用，未来版本会持续增加。对于没有列出的函数，则不建议在生产环境中使用相应的表达式索引。详情请参考[表达式索引](/sql-statements/sql-statement-create-index.md#表达式索引)。\n\n[^3]: TiDB 支持的完整 SQL 列表，见[语句参考](/sql-statements/sql-statement-select.md)。\n\n[^4]: 从 [TiDB v6.4.0](/releases/release-6.4.0.md) 开始，支持[高性能、全局单调递增的 `AUTO_INCREMENT` 列](/auto-increment.md#mysql-兼容模式)。\n\n[^5]: 从 [TiDB v7.0.0](/releases/release-7.0.0.md) 开始新增的参数 `FIELDS DEFINED NULL BY` 以及新增支持从 S3 和 GCS 导入数据，均为实验特性。从 [TiDB v7.6.0](/releases/release-7.6.0.md) 开始 `LOAD DATA` 的事务行为与 MySQL 的事务行为一致，包括事务内的 `LOAD DATA` 语句本身不再自动提交当前事务，也不会开启新事务，并且事务内的 `LOAD DATA` 语句可以被显式提交或者回滚。此外，`LOAD DATA` 语句会受 TiDB 事务模式设置（乐观/悲观）影响。\n\n[^6]: 从 v7.5.0 开始，[TiDB Binlog](https://docs.pingcap.com/zh/tidb/v8.3/tidb-binlog-overview) 的数据同步功能被废弃。从 v8.3.0 开始，TiDB Binlog 被完全废弃。从 v8.4.0 开始，TiDB Binlog 被移除。如需进行增量数据同步，请使用 [TiCDC](/ticdc/ticdc-overview.md)。如需按时间点恢复 (point-in-time recovery, PITR)，请使用 [PITR](/br/br-pitr-guide.md)。在将 TiDB 集群升级到 v8.4.0 或之后版本前，务必先切换至 TiCDC 和 PITR。\n"
        },
        {
          "name": "basic-sql-operations.md",
          "type": "blob",
          "size": 6.6806640625,
          "content": "---\ntitle: SQL 基本操作\naliases: ['/docs-cn/dev/basic-sql-operations/','/docs-cn/dev/how-to/get-started/explore-sql/']\nsummary: TiDB 是一个兼容 MySQL 的数据库，可以执行 DDL、DML、DQL 和 DCL 操作。可以使用 SHOW DATABASES 查看数据库列表，使用 CREATE DATABASE 创建数据库，使用 DROP DATABASE 删除数据库。使用 CREATE TABLE 创建表，使用 SHOW CREATE TABLE 查看建表语句，使用 DROP TABLE 删除表。使用 CREATE INDEX 创建索引，使用 SHOW INDEX 查看表内所有索引，使用 DROP INDEX 删除索引。使用 INSERT 向表内插入记录，使用 UPDATE 修改记录，使用 DELETE 删除记录。使用 SELECT 检索表内数据，使用 WHERE 子句进行筛选。使用 CREATE USER 创建用户，使用 GRANT 授权用户，使用 DROP USER 删除用户。\n---\n\n# SQL 基本操作\n\n成功部署 TiDB 集群之后，便可以在 TiDB 中执行 SQL 语句了。因为 TiDB 兼容 MySQL，你可以使用 MySQL 客户端连接 TiDB，并且[大多数情况下](/mysql-compatibility.md)可以直接执行 MySQL 语句。\n\nSQL 是一门声明性语言，它是数据库用户与数据库交互的方式。它更像是一种自然语言，好像在用英语与数据库进行对话。本文档介绍基本的 SQL 操作。完整的 TiDB SQL 语句列表，参见 [SQL 语句概览](/sql-statements/sql-statement-overview.md)。\n\n## 分类\n\nSQL 语言通常按照功能划分成以下的 4 个部分：\n\n- DDL (Data Definition Language)：数据定义语言，用来定义数据库对象，包括库、表、视图和索引等。\n\n- DML (Data Manipulation Language)：数据操作语言，用来操作和业务相关的记录。\n\n- DQL (Data Query Language)：数据查询语言，用来查询经过条件筛选的记录。\n\n- DCL (Data Control Language)：数据控制语言，用来定义访问权限和安全级别。\n\n常用的 DDL 功能是对象（如表、索引等）的创建、属性修改和删除，对应的命令分别是 CREATE、ALTER 和 DROP。\n\n## 查看、创建和删除数据库\n\nTiDB 语境中的 Database 或者说数据库，可以认为是表和索引等对象的集合。\n\n使用 `SHOW DATABASES` 语句查看系统中数据库列表：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW DATABASES;\n```\n\n使用名为 `mysql` 的数据库：\n\n{{< copyable \"sql\" >}}\n\n```sql\nUSE mysql;\n```\n\n使用 `SHOW TABLES` 语句查看数据库中的所有表。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW TABLES FROM mysql;\n```\n\n使用 `CREATE DATABASE` 语句创建数据库。语法如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE DATABASE db_name [options];\n```\n\n例如，要创建一个名为 `samp_db` 的数据库，可使用以下语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE DATABASE IF NOT EXISTS samp_db;\n```\n\n添加 `IF NOT EXISTS` 可防止发生错误。\n\n使用 `DROP DATABASE` 语句删除数据库。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nDROP DATABASE samp_db;\n```\n\n## 创建、查看和删除表\n\n使用 `CREATE TABLE` 语句创建表。语法如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE table_name column_name data_type constraint;\n```\n\n例如，要创建一个名为 `person` 的表，包括编号、名字、生日等字段，可使用以下语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE person (\n    id INT,\n    name VARCHAR(255),\n    birthday DATE\n    );\n```\n\n使用 `SHOW CREATE` 语句查看建表语句，即 DDL。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW CREATE TABLE person;\n```\n\n使用 `DROP TABLE` 语句删除表。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nDROP TABLE person;\n```\n\n## 创建、查看和删除索引\n\n索引通常用于加速索引列上的查询。对于值不唯一的列，可使用 `CREATE INDEX` 或 `ALTER TABLE` 语句创建普通索引。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE INDEX person_id ON person (id);\n```\n\n或者：\n\n{{< copyable \"sql\" >}}\n\n```sql\nALTER TABLE person ADD INDEX person_id (id);\n```\n\n对于值唯一的列，可以创建唯一索引。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE UNIQUE INDEX person_unique_id ON person (id);\n```\n\n或者：\n\n{{< copyable \"sql\" >}}\n\n```sql\nALTER TABLE person ADD UNIQUE person_unique_id (id);\n```\n\n使用 `SHOW INDEX` 语句查看表内所有索引：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW INDEX FROM person;\n```\n\n使用 `ALTER TABLE` 或 `DROP INDEX` 语句来删除索引。与 `CREATE INDEX` 语句类似，`DROP INDEX` 也可以嵌入 `ALTER TABLE` 语句。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nDROP INDEX person_id ON person;\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nALTER TABLE person DROP INDEX person_unique_id;\n```\n\n注意：DDL 操作不是事务，在执行 DDL 时，不需要对应 COMMIT 语句。\n\n常用的 DML 功能是对表记录的新增、修改和删除，对应的命令分别是 INSERT、UPDATE 和 DELETE。\n\n## 记录的增删改\n\n使用 `INSERT` 语句向表内插入表记录。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO person VALUES(1,'tom','20170912');\n```\n\n使用 `INSERT` 语句向表内插入包含部分字段数据的表记录。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO person(id,name) VALUES('2','bob');\n```\n\n使用 `UPDATE` 语句向表内修改表记录的部分字段数据。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nUPDATE person SET birthday='20180808' WHERE id=2;\n```\n\n使用 `DELETE` 语句向表内删除部分表记录。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nDELETE FROM person WHERE id=2;\n```\n\n注意：UPDATE 和 DELETE 操作如果不带 WHERE 过滤条件是对全表进行操作。\n\nDQL 数据查询语言是从一个表或多个表中检索出想要的数据行，通常是业务开发的核心内容。\n\n## 查询数据\n\n使用 `SELECT` 语句检索表内数据。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM person;\n```\n\n在 SELECT 后面加上要查询的列名。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT name FROM person;\n```\n\n```sql\n+------+\n| name |\n+------+\n| tom  |\n+------+\n1 rows in set (0.00 sec)\n```\n\n使用 WHERE 子句，对所有记录进行是否符合条件的筛选后再返回。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM person WHERE id<5;\n```\n\n常用的 DCL 功能是创建或删除用户，和对用户权限的管理。\n\n## 创建、授权和删除用户\n\n使用 `CREATE USER` 语句创建一个用户 `tiuser`，密码为 `123456`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE USER 'tiuser'@'localhost' IDENTIFIED BY '123456';\n```\n\n授权用户 `tiuser` 可检索数据库 `samp_db` 内的表：\n\n{{< copyable \"sql\" >}}\n\n```sql\nGRANT SELECT ON samp_db.* TO 'tiuser'@'localhost';\n```\n\n查询用户 `tiuser` 的权限：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW GRANTS for tiuser@localhost;\n```\n\n删除用户 `tiuser`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nDROP USER 'tiuser'@'localhost';\n```\n"
        },
        {
          "name": "batch-processing.md",
          "type": "blob",
          "size": 3.8896484375,
          "content": "---\ntitle: 数据批量处理\nsummary: 介绍了 TiDB 为数据批量处理场景提供的功能，包括 Pipelined DML、非事务性 DML、IMPORT INTO 语句以及已被废弃的 batch-dml。\n---\n\n# 数据批量处理\n\n批量数据处理是实际业务中常见且重要的操作，它涉及到对大量数据进行高效操作，如数据迁移、批量导入、归档操作或大规模更新等。\n\n为了提升批量处理性能，TiDB 随着版本的演进提供了多种数据批量处理功能：\n\n- 数据导入\n    - `IMPORT INTO` 语句（从 TiDB v7.2.0 开始引入，在 v7.5.0 成为正式功能）\n- 数据增删改\n    - Pipelined DML（从 TiDB v8.0.0 开始引入，实验特性）\n    - 非事务性 DML（从 TiDB v6.1.0 开始引入）\n    - 已废弃的 batch-dml 功能\n\n本文分别介绍这些功能的主要优势、限制和使用场景，帮助你根据实际需求选择合适的方案，从而更高效地完成批量数据处理任务。\n\n## 数据导入\n\n`IMPORT INTO` 语句专为数据导入设计，使你无需单独部署 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md)，即可将 CSV、SQL 或 PARQUET 等格式的数据快速导入到 TiDB 的一张空表中。\n\n主要优势：\n\n- 导入速度非常快。\n- 比 TiDB Lightning 更易用。\n\n主要限制：\n\n- 不满足事务 [ACID](/glossary.md#acid) 性质。\n- 使用限制较多。\n\n适用场景：\n\n- 数据导入场景，例如数据迁移、数据恢复等。建议在合适的场景下，使用 IMPORT INTO 代替 TiDB Lightning。\n\n更多信息，请参考 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md)。\n\n## 数据增删改\n\n### Pipelined DML\n\nPipelined DML 是从 TiDB v8.0.0 开始引入的实验特性。在 v8.5.0 中，TiDB 对该功能进行了完善，其性能得到大幅提升。\n\n主要优势：\n\n- 在事务执行过程中，通过将数据持续写入存储层，而不是全部缓存在内存中，使得事务大小不再受到 TiDB 内存限制，支持处理超大规模数据。\n- 性能比标准 DML 更好。\n- 通过系统变量启用，无需修改 SQL 语句。\n\n主要限制：\n\n- 只适用于[自动提交](/transaction-overview.md#自动提交)的 `INSERT`、`REPLACE`、`UPDATE`、`DELETE` 语句。\n\n适用场景：\n\n- 通用的批量数据处理场景，例如大量数据的插入、更新、删除等。\n\n更多信息，请参考 [Pipelined DML](/pipelined-dml.md)。\n\n### 非事务 DML 语句\n\n非事务 DML 语句是从 TiDB v6.1.0 开始引入的功能。在 v6.1.0 中，该功能仅支持 `DELETE` 语句。从 v6.5.0 起，该功能新增支持 `INSERT`、`REPLACE`、`UPDATE` 语句。\n\n主要优势：\n\n- 通过将一条 SQL 语句拆为多条语句执行，使得每个语句的事务更小，绕开内存限制。\n- 处理速度比标准 DML 稍快或相当。\n\n主要限制：\n\n- 只适用于[自动提交](/transaction-overview.md#自动提交)的语句。\n- 需要修改 SQL 语句。\n- 对 SQL 语句本身限制较多，不符合条件的语句可能需要改写。\n- 因为 SQL 语句被拆分执行，不具有完整的事务 ACID 性质，在失败时语句可能部分完成。\n\n适用场景：\n\n- 大量数据的插入、更新、删除等场景。由于限制较多，建议在 Pipelined DML 不适用的场景下考虑使用。\n\n更多信息，请参考[非事务 DML 语句](/non-transactional-dml.md)。\n\n### 已被废弃的 batch-dml\n\nTiDB 在 v4.0 之前提供了 batch-dml 功能，用于批量数据处理。该功能已被废弃，不再推荐使用。batch-dml 功能由以下这些系统变量控制：\n\n- `tidb_batch_insert`\n- `tidb_batch_delete`\n- `tidb_batch_commit`\n- `tidb_enable_batch_dml`\n- `tidb_dml_batch_size`\n\n因为该功能可能引起数据索引不一致，导致数据损坏或丢失，以上变量已被废弃，并计划将在未来的版本中逐渐移除。\n\n不建议在任何场景下使用已被废弃的 batch-dml 功能。建议选择上面描述的其它方案。"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "best-practices-for-security-configuration.md",
          "type": "blob",
          "size": 7.599609375,
          "content": "---\ntitle: TiDB 安全配置最佳实践\nsummary: 介绍 TiDB 安全配置的最佳实践，帮助你降低潜在的安全风险。\n---\n\n# TiDB 安全配置最佳实践\n\nTiDB 的安全性对于保护数据完整性和机密性至关重要。本文提供了 TiDB 集群部署时的安全配置指南。遵循这些最佳实践可以有效降低潜在安全风险、防范数据泄露，并确保 TiDB 数据库系统能够持续稳定、可靠地运行。\n\n> **注意：**\n>\n> 本文提供关于 TiDB 安全配置的一般建议。PingCAP 不保证信息的完整性或准确性，对使用本指南所产生的任何问题不承担责任。用户应根据自身需求评估这些建议，并咨询专业人士以获得具体的建议。\n\n## 设置 root 用户初始密码\n\n默认情况下，新创建的 TiDB 集群中 root 用户的密码为空，这可能导致潜在的安全风险。任何人都可以尝试使用 root 用户登录 TiDB 数据库，从而可能访问和修改数据。\n\n为避免此风险，建议在部署过程中设置 root 密码：\n\n- 使用 TiUP 部署时，参考[使用 TiUP 部署 TiDB 集群](/production-deployment-using-tiup.md#第-7-步启动集群)为 root 用户生成随机密码。\n- 使用 TiDB Operator 部署时，参考[初始化账号和密码设置](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/initialize-a-cluster#初始化账号和密码设置)为 root 用户设置密码。\n\n## 启用密码复杂性检查\n\n默认情况下，TiDB 未启用密码复杂性策略，这可能导致使用弱密码或空密码，增加安全风险。\n\n为确保数据库用户创建强密码，建议配置合理的[密码复杂度策略](/password-management.md#密码复杂度策略)。例如，要求密码包含大写字母、小写字母、数字和特殊字符的组合。启用密码复杂性检查可以提高数据库的安全性、防止暴力破解攻击、减少内部威胁、遵守法规和合规性要求、降低数据泄露风险，并提高整体安全水平。\n\n## 修改 Grafana 默认密码\n\nTiDB 安装时默认包含 Grafana 组件，其默认的用户名密码通常为 `admin/admin`。如不及时修改，可能被攻击者利用获取系统控制权。\n\n建议在部署 TiDB 时立即将 Grafana 的密码修改为强密码，并定期更新密码以确保系统安全。修改 Grafana 密码的方式如下：\n\n- 首次登录 Grafana 时，根据提示完成新密码的修改。\n\n    ![Grafana Password Reset Guide](/media/grafana-password-reset1.png)\n\n- 进入 Grafana 个人配置中心完成新密码的修改。\n\n    ![Grafana Password Reset Guide](/media/grafana-password-reset2.png)\n\n## 提高 TiDB Dashboard 安全性\n\n### 使用最小权限用户\n\nTiDB Dashboard 的账号体系与 TiDB SQL 用户一致，并基于 TiDB SQL 用户的权限进行 TiDB Dashboard 授权验证。TiDB Dashboard 所需的权限较少，甚至可以只有只读权限。\n\n为提高系统安全性，建议为访问 TiDB Dashboard 创建一个[最小权限的 SQL 用户](/dashboard/dashboard-user.md)，并用该用户登录 TiDB Dashboard，避免使用高权限用户，提升安全性。\n\n### 限制访问控制\n\n默认情况下，TiDB Dashboard 设计为供受信任的用户访问。默认端口将包含除 TiDB Dashboard 外的其他 API 接口。如果你希望让外部网络用户或不受信任的用户访问 TiDB Dashboard，需要采取以下的措施以避免安全漏洞的出现：\n\n- 使用防火墙等手段将默认的 `2379` 端口限制在可信域内，禁止外部用户进行访问。\n\n    > **注意：**\n    >\n    > TiDB、TiKV 等组件需要通过 PD Client 端口与 PD 组件进行通信。请勿对组件内部网络阻止访问，这将导致集群不可用。\n\n- [配置反向代理](/dashboard/dashboard-ops-reverse-proxy.md#通过反向代理使用-tidb-dashboard)，将 TiDB Dashboard 服务在另一个端口上安全地提供给外部。\n\n## 保护内部端口\n\nTiDB 的默认安装中存在许多用于组件间通信的特权接口。这些端口通常不需要向用户端开放，因为它们主要用于内部通信。当这些端口直接暴露在公共网络上时，会增加潜在的攻击面，违反了安全最小化原则，增加了安全风险的产生。下表列出了 TiDB 集群默认监听端口的详细情况：\n\n| 组件                | 默认监听端口  | 协议       |\n|-------------------|--------------|------------|\n| TiDB              | 4000         | MySQL      |\n| TiDB              | 10080        | HTTP       |\n| TiKV              | 20160        | Protocol   |\n| TiKV              | 20180        | HTTP       |\n| PD                | 2379         | HTTP/Protocol|\n| PD                | 2380         | Protocol   |\n| TiFlash           | 3930         | Protocol   |\n| TiFlash           | 20170        | Protocol   |\n| TiFlash           | 20292        | HTTP       |\n| TiFlash           | 8234         | HTTP       |\n| TiFlow            |  8261/8291 | HTTP  |\n| TiFlow            |  8262      | HTTP  |\n| TiFlow            |  8300     | HTTP       |\n| TiDB Lightning    | 8289         | HTTP       |\n| TiDB Operator     | 6060         | HTTP       |\n| TiDB Dashboard    | 2379         | HTTP       |\n| TiDB Binlog       |  8250   | HTTP       |\n| TiDB Binlog       |  8249 | HTTP      |\n| TMS               | 8082         | HTTP       |\n| TEM               | 8080         | HTTP       |\n| TEM               | 8000         | HTTP       |\n| TEM               | 4110         | HTTP       |\n| TEM               | 4111         | HTTP       |\n| TEM               | 4112         | HTTP       |\n| TEM               | 4113         | HTTP       |\n| TEM               | 4124         | HTTP       |\n| Prometheus        | 9090         | HTTP       |\n| Grafana           | 3000         | HTTP       |\n| AlertManager      | 9093         | HTTP       |\n| AlertManager      | 9094         | Protocol   |\n| Node Exporter     | 9100         | HTTP       |\n| Blackbox Exporter | 9115        | HTTP       |\n| NG Monitoring     | 12020        | HTTP       |\n\n建议向普通用户只公开数据库的 `4000` 端口和 Grafana 面板的 `9000` 端口，并通过网络安全策略组或防火墙限制其他端口。以下是使用 `iptables` 限制端口访问的示例：\n\n```shell\n# 允许来自各组件白名单 IP 地址范围的内部端口通讯\nsudo iptables -A INPUT -s 内网 IP 地址范围 -j ACCEPT\n\n# 仅对外部用户开放 4000 和 9000 端口\nsudo iptables -A INPUT -p tcp --dport 4000 -j ACCEPT\nsudo iptables -A INPUT -p tcp --dport 9000 -j ACCEPT\n\n# 默认拒绝所有其他流量\nsudo iptables -P INPUT DROP\n```\n\n如果需要访问 TiDB Dashboard，建议通过[配置反向代理](/dashboard/dashboard-ops-reverse-proxy.md#通过反向代理使用-tidb-dashboard)的方式将 TiDB Dashboard 服务安全地提供给外部网络，并将其部署在另外的端口上。\n\n## 解决第三方扫描器 MySQL 漏洞误报\n\n大多数漏洞扫描器在检测 MySQL 漏洞时，会根据版本信息来匹配 CVE 漏洞。由于 TiDB 仅兼容 MySQL 协议而非 MySQL 本身，基于版本信息的漏洞扫描可能导致误报。建议漏洞扫描应以原理扫描为主。当合规漏洞扫描工具要求 MySQL 版本时，你可以[修改服务器版本号](/faq/high-reliability-faq.md#我们的安全漏洞扫描工具对-mysql-version-有要求tidb-是否支持修改-server-版本号呢)，以满足其要求。\n\n通过修改服务器版本号，可避免漏洞扫描器产生误报。[`server-version`](/tidb-configuration-file.md#server-version) 的值会被 TiDB 节点用于验证当前 TiDB 的版本。在进行 TiDB 集群升级前，请将 `server-version` 的值设置为空或者当前 TiDB 真实的版本值，避免出现非预期行为。\n"
        },
        {
          "name": "best-practices-on-public-cloud.md",
          "type": "blob",
          "size": 13.7333984375,
          "content": "---\ntitle: 在公有云上部署 TiDB 的最佳实践\nsummary: 了解在公有云上部署 TiDB 的最佳实践。\n---\n\n# 在公有云上部署 TiDB 的最佳实践\n\n随着公有云基础设施的普及，越来越多的用户选择在公有云上部署和管理 TiDB。然而，要想在公有云上充分发挥 TiDB 的性能，需要关注多个方面，包括性能优化、成本控制、系统可靠性和可扩展性。\n\n本文介绍在公有云上部署 TiDB 的一系列最佳实践，例如减少 KV RocksDB 中的 compaction I/O 流量、为 Raft Engine 配置专用磁盘、优化跨可用区的流量成本、缓解 Google Cloud 上实时迁移维护事件带来的性能影响，以及对大规模集群中的 PD Server 进行性能调优。遵循这些最佳实践，可以显著提升 TiDB 在公有云上的性能、成本效率、可靠性和可扩展性。\n\n## 减少 KV RocksDB 中的 compaction I/O 流量\n\n[RocksDB](https://rocksdb.org/) 是 TiKV 的存储引擎，负责存储用户数据。出于成本考虑，云上提供的 EBS IO 吞吐量通常比较有限，因此 RocksDB 可能会表现出较高的写放大，导致磁盘吞吐量成为负载的瓶颈。随着时间的推移，待 compaction 的字节总量会不断增加从而触发流量控制，这意味着此时 TiKV 缺乏足够的磁盘带宽来处理前台写入流量。\n\n要缓解由磁盘吞吐量受限引起的瓶颈，你可以通过[启用 Titan](#启用-titan) 来改善性能。如果数据的平均行大小低于 512 字节，Titan 并不适用，此时你可以通过[提高所有压缩级别](#提高所有压缩级别) 来改善性能。\n\n### 启用 Titan\n\n[Titan](/storage-engine/titan-overview.md) 是基于 [RocksDB](https://github.com/facebook/rocksdb) 的高性能单机 key-value 存储引擎插件。当 value 较大时，Titan 可以减少 RocksDB 中的写放大。\n\n如果数据的平均行大小超过 512 字节，可以通过启用 Titan，设置 `min-blob-size` 为 `\"512B\"` 或 `\"1KB\"`，以及设置 `blob-file-compression` 为 `\"zstd\"` 来减少压缩 I/O 流量：\n\n```toml\n[rocksdb.titan]\nenabled = true\n[rocksdb.defaultcf.titan]\nmin-blob-size = \"1KB\"\nblob-file-compression = \"zstd\"\n```\n\n> **注意：**\n>\n> 启用 Titan 后，主键范围扫描的性能可能会略有下降。更多信息，请参考 [`min-blob-size` 对性能的影响](/storage-engine/titan-overview.md#min-blob-size-对性能的影响)。\n\n### 提高所有压缩级别\n\n如果数据的平均行大小低于 512 字节，你可以将默认 Column Family 的所有压缩级别设置为 `\"zstd\"` 来提高性能，如下所示：\n\n```toml\n[rocksdb.defaultcf]\ncompression-per-level = [\"zstd\", \"zstd\", \"zstd\", \"zstd\", \"zstd\", \"zstd\", \"zstd\"]\n```\n\n## 为 Raft Engine 分配专用磁盘\n\n在 TiKV 中，[Raft Engine](/glossary.md#raft-engine) 扮演着类似传统数据库中预写日志（WAL）的关键角色。为了达到最佳性能和稳定性，在公有云上部署 TiDB 时，为 Raft Engine 分配专用磁盘至关重要。以下 `iostat` 显示了写密集型工作负载下 TiKV 节点的 I/O 特征。\n\n```\nDevice            r/s     rkB/s       w/s     wkB/s      f/s  aqu-sz  %util\nsdb           1649.00 209030.67   1293.33 304644.00    13.33    5.09  48.37\nsdd           1033.00   4132.00   1141.33  31685.33   571.00    0.94 100.00\n```\n\n设备 `sdb` 用于 KV RocksDB，而 `sdd` 用于存储 Raft Engine 日志。注意，`sdd` 的 `f/s` 值明显更高，这表示该设备每秒完成的刷新请求数量更高。在 Raft Engine 中，当 batch 中的写入操作被标记为同步时，batch leader 将在写入后调用 `fdatasync()`，以确保缓存的数据被写入存储。通过为 Raft Engine 分配专用磁盘，TiKV 可以减少请求的平均队列长度，从而确保写入延迟的最优化和稳定性。\n\n不同的云服务提供商提供了多种性能各异的磁盘类型（例如，IOPS 和 MBPS）。根据你的负载情况，选择合适的云服务提供商、磁盘类型和磁盘大小非常重要。\n\n### 在公有云上为 Raft Engine 选择合适的磁盘\n\n本节介绍在不同公有云上为 Raft Engine 选择合适磁盘的最佳实践。根据性能需求，推荐使用中端或高端这两种磁盘类型。\n\n#### 中端磁盘\n\n对于不同的公有云，推荐的中端磁盘如下：\n\n- AWS：推荐使用 [gp3](https://aws.amazon.com/cn/ebs/general-purpose/) 卷。gp3 卷提供 3000 IOPS 和 125 MB/s 的免费吞吐量，无论 gp3 卷的大小如何选择，通常都足以满足 Raft Engine 的需求。\n\n- Google Cloud：推荐使用 [pd-ssd](https://cloud.google.com/compute/docs/disks?hl=zh-cn#disk-types/)。IOPS 和 MBPS 随分配的磁盘大小而不同。为了满足性能需求，建议为 Raft Engine 分配 200 GB 的空间。尽管 Raft Engine 并不需要这么大的空间，但这样可以确保最佳的性能。\n\n- Azure：推荐使用 [Premium SSD v2](https://learn.microsoft.com/zh-cn/azure/virtual-machines/disks-types#premium-ssd-v2)。类似于 AWS gp3，Premium SSD v2 提供 3000 IOPS 和 125 MB/s 的免费吞吐量，无论 Premium SSD v2 的大小如何选择，通常都足以满足 Raft Engine 的需求。\n\n#### 高端磁盘\n\n如果你希望 Raft Engine 有更低的延迟，可以考虑使用高端磁盘。对于不同的公有云，推荐的高端磁盘如下：\n\n- AWS：推荐使用 [io2](https://docs.aws.amazon.com/zh_cn/ebs/latest/userguide/ebs-volume-types.html)。磁盘大小和 IOPS 可以根据你的具体需求进行配置。\n\n- Google Cloud：推荐使用 [pd-extreme](https://cloud.google.com/compute/docs/disks?hl=zh-cn#disk-types/)。磁盘大小、IOPS 和 MBPS 可以进行配置，但仅在 CPU 核数超过 64 的实例上可用。\n\n- Azure：推荐使用 [Ultra Disk](https://learn.microsoft.com/zh-cn/azure/virtual-machines/disks-types#ultra-disks)。磁盘大小、IOPS 和 MBPS 可以根据你的具体需求进行配置。\n\n### 示例 1：在 AWS 上运行社交网络工作负载\n\n在 AWS 上，一个 20 GB 的 [gp3](https://aws.amazon.com/cn/ebs/general-purpose/) 卷提供 3000 IOPS 和 125 MBPS/s 的吞吐量。\n\n为一个写密集型的社交网络应用分配一个专用的 20 GB [gp3](https://aws.amazon.com/cn/ebs/general-purpose/) Raft Engine 磁盘后，性能提升如下，而成本仅增加 0.4%：\n\n- QPS（每秒查询数）提高 17.5%\n- 插入语句的平均延迟降低 18.7%\n- 插入语句的 p99 延迟降低 45.6%\n\n| 指标 | 共享 Raft Engine 磁盘 | 专用 Raft Engine 磁盘 | 差异 (%) |\n| ------------- | ------------- | ------------- | ------------- |\n| QPS (K/s) | 8.0 | 9.4 | 17.5 |\n| 平均插入延迟 (ms) | 11.3 | 9.2 | -18.7 |\n| p99 插入延迟 (ms) | 29.4 | 16.0 | -45.6 |\n\n### 示例 2：在 Azure 上运行 TPC-C/Sysbench 工作负载\n\n在 Azure 上，为 Raft Engine 分配一个专用的 32 GB [Ultra Disk](https://learn.microsoft.com/zh-cn/azure/virtual-machines/disks-types#ultra-disks) 后，性能提升如下：\n\n- Sysbench `oltp_read_write` 工作负载：QPS 提高 17.8%，平均延迟降低 15.6%。\n- TPC-C 工作负载：QPS 提高 27.6%，平均延迟降低 23.1%。\n\n| 指标 | 工作负载 | 共享 Raft Engine 磁盘 | 专用 Raft Engine 磁盘 | 差异 (%) |\n| ------------- | ------------- | ------------- | ------------- | ------------- |\n| QPS (K/s) | Sysbench `oltp_read_write` | 60.7 | 71.5 | 17.8 |\n| QPS (K/s) | TPC-C | 23.9 | 30.5 | 27.6 |\n| 平均延迟 (ms) | Sysbench `oltp_read_write` | 4.5 | 3.8 | -15.6 |\n| 平均延迟 (ms) | TPC-C | 3.9 | 3.0 | -23.1 |\n\n### 示例 3：在 Google Cloud 上为 TiKV 配置专用的 pd-ssd 磁盘\n\n以下 TiKV 配置示例展示了如何为由 [TiDB Operator](https://docs.pingcap.com/tidb-in-kubernetes/stable) 部署到 Google Cloud 上的集群额外配置一个 512 GB 的 [pd-ssd](https://cloud.google.com/compute/docs/disks?hl=zh-cn#disk-types/) 磁盘，其中 `raft-engine.dir` 的配置用于将 Raft Engine 日志存储在该磁盘上。\n\n```\ntikv:\n    config: |\n      [raft-engine]\n        dir = \"/var/lib/raft-pv-ssd/raft-engine\"\n        enable = true\n        enable-log-recycle = true\n    requests:\n      storage: 4Ti\n    storageClassName: pd-ssd\n    storageVolumes:\n    - mountPath: /var/lib/raft-pv-ssd\n      name: raft-pv-ssd\n      storageSize: 512Gi\n```\n\n## 降低跨可用区网络流量的成本\n\n在跨多个可用区部署 TiDB 时，跨可用区的数据传输可能会增加部署成本。为了降低这些成本，减少跨可用区的网络流量非常重要。\n\n要减少跨可用区的读取流量，你可以启用 [Follower Read 功能](/follower-read.md)，该功能允许 TiDB 优先选择在同一可用区内的副本进行读取。要启用该功能，请将 [`tidb_replica_read`](/system-variables.md#tidb_replica_read-从-v40-版本开始引入) 变量设置为 `closest-replicas` 或 `closest-adaptive`。\n\n要减少 TiKV 实例中跨可用区的写入流量，你可以启用 gRPC 压缩功能，该功能在网络传输数据之前会对其进行压缩。以下配置示例展示了如何为 TiKV 启用 gzip gRPC 压缩：\n\n```\nserver_configs:\n  tikv:\n    server.grpc-compression-type: gzip\n```\n\n要减少 TiFlash MPP 任务中数据交换（data shuffle 过程）所带来的网络流量，建议在同一可用区内部署多个 TiFlash 实例。从 v6.6.0 开始，[Exchange 数据压缩](/explain-mpp.md#mpp-version-和-exchange-数据压缩)功能默认启用，以减少 MPP 数据交换导致的网络流量。\n\n## 缓解 Google Cloud 上的实时迁移维护事件带来的性能影响\n\nGoogle Cloud 的[实时迁移功能](https://cloud.google.com/compute/docs/instances/live-migration-process?hl=zh-cn)允许虚拟机在主机之间无缝迁移而不会导致停机。然而，这些迁移事件虽不频繁，但可能会显著影响虚拟机的性能，其中包括那些在 TiDB 集群中运行的虚拟机。在这些事件发生期间，受影响的虚拟机可能会出现性能下降，导致 TiDB 集群中的查询处理时间增加。\n\n为了检测 Google Cloud 发起的实时迁移事件并减轻这些事件对性能的影响，TiDB 提供了一个[监控脚本](https://github.com/PingCAP-QE/tidb-google-maintenance)（该脚本基于 Google 元数据[示例](https://github.com/GoogleCloudPlatform/python-docs-samples/blob/master/compute/metadata/main.py)）。你可以将此脚本部署在 TiDB、TiKV 和 PD 节点上，以检测维护事件。当检测到维护事件时，该脚本将自动采取以下适当措施，以尽量减少中断并优化集群行为：\n\n- TiDB：通过下线 TiDB 节点并删除 TiDB pod 来处理维护事件（假设 TiDB 实例的节点池设置为自动扩展且专用于 TiDB）。节点上运行的其他 pod 可能会遇到中断，而被隔离的节点将会被自动扩展程序回收。\n- TiKV：在维护期间驱逐受影响的 TiKV 存储上的 Leader。\n- PD：如果当前 PD 实例是 PD Leader，则会重新分配 Leader。\n\n需要注意的是，此监控脚本是专门为 [TiDB Operator](https://docs.pingcap.com/tidb-in-kubernetes/dev/tidb-operator-overview) 部署的 TiDB 集群设计的，TiDB Operator 为 Kubernetes 环境中的 TiDB 提供了增强的管理功能。\n\n通过使用该监控脚本，并在维护事件期间采取必要的措施，TiDB 集群可以更好地应对 Google Cloud 上的实时迁移事件，确保对查询处理和响应时间的影响最小以及系统的平稳运行。\n\n## 为具有高 QPS 的大规模 TiDB 集群优化 PD 性能\n\n在 TiDB 集群中，一个活跃的 Placement Driver (PD) Server 承担着许多关键任务，例如处理提供 TSO (Timestamp Oracle) 和处理请求。然而，依赖单个活跃 PD Server 可能会限制 TiDB 集群的扩展性。\n\n### PD 限制的特征\n\n以下图表展示了一个由三个 PD Server 组成的大规模 TiDB 集群的特征，其中每个 PD Server 均配置了 56 核的 CPU。可以看出，当每秒查询数（QPS）超过 100 万次且每秒 TSO 请求数超过 162,000 次时，CPU 利用率达到约 4600%。这一高 CPU 利用率表明 PD Leader 的负载已经相当高且可用的 CPU 资源即将耗尽。\n\n![pd-server-cpu](/media/performance/public-cloud-best-practice/baseline_cpu.png)\n![pd-server-metrics](/media/performance/public-cloud-best-practice/baseline_metrics.png)\n\n### 优化 PD 性能\n\n为了解决 PD Server 的高 CPU 利用率问题，可以进行以下调整：\n\n#### 调整 PD 配置\n\n[`tso-update-physical-interval`](/pd-configuration-file.md#tso-update-physical-interval)：此参数控制 PD Server 更新物理 TSO batch 的间隔。通过缩短此间隔，PD Server 可以更频繁地分配 TSO batch，从而减少下一次分配的等待时间。\n\n```\ntso-update-physical-interval = \"10ms\" # 默认值为 50ms\n```\n\n#### 调整 TiDB 全局变量\n\n除了 PD 配置外，启用 TSO 客户端攒批操作的等待功能可以进一步优化 TSO 客户端的行为。要启用此功能，可以将全局变量 [`tidb_tso_client_batch_max_wait_time`](/system-variables.md#tidb_tso_client_batch_max_wait_time-从-v530-版本开始引入) 设置为非零值：\n\n```\nset global tidb_tso_client_batch_max_wait_time = 2; # 默认值为 0\n```\n\n#### 调整 TiKV 配置\n\n为了减少 Region 数量并降低系统的心跳开销，可以参考[调整 Region 大小](/best-practices/massive-regions-best-practices.md#方法六调整-region-大小)将 TiKV 配置中的 Region 大小适度调大\n\n```\n[coprocessor]\n  region-split-size = \"288MiB\"\n```\n\n### 调整后的效果\n\n调整后的效果如下：\n\n- 每秒 TSO 请求数减少到 64,800 次。\n- CPU 利用率显著降低，从约 4600% 降低到 1400%。\n- `PD server TSO handle time` 的 P999 值从 2 ms 降低到 0.5 ms。\n\n以上性能提升表明，这些调整措施成功地降低了 PD Server 的 CPU 利用率，同时保持了稳定的 TSO 处理性能。\n\n![pd-server-cpu](/media/performance/public-cloud-best-practice/after_tuning_cpu.png)\n![pd-server-metrics](/media/performance/public-cloud-best-practice/after_tuning_metrics.png)"
        },
        {
          "name": "best-practices",
          "type": "tree",
          "content": null
        },
        {
          "name": "binary-package.md",
          "type": "blob",
          "size": 3.1796875,
          "content": "---\ntitle: TiDB 离线包\nsummary: 了解 TiDB 离线包及其包含的内容。\n---\n\n# TiDB 离线包\n\n在[使用 TiUP 离线部署 TiDB](/production-deployment-using-tiup.md#离线部署) 前，你需要在[官方下载页面](https://cn.pingcap.com/product-community/)选择对应版本的 TiDB server 离线镜像包（包含 TiUP 离线组件包）。\n\nTiDB 提供了 amd64 和 arm64 两种架构的离线包。对于每种架构，TiDB 提供了两个二进制离线包：`TiDB-community-server` 软件包和 `TiDB-community-toolkit` 软件包。\n\n`TiDB-community-server` 软件包中包含以下内容：\n\n| 内容 | 变更说明 |\n|---|---|\n| tidb-{version}-linux-{arch}.tar.gz |  |\n| tikv-{version}-linux-{arch}.tar.gz |  |\n| tiflash-{version}-linux-{arch}.tar.gz |  |\n| pd-{version}-linux-{arch}.tar.gz |  |\n| ctl-{version}-linux-{arch}.tar.gz |  |\n| grafana-{version}-linux-{arch}.tar.gz |  |\n| alertmanager-{version}-linux-{arch}.tar.gz |  |\n| blackbox_exporter-{version}-linux-{arch}.tar.gz |  |\n| prometheus-{version}-linux-{arch}.tar.gz |  |\n| node_exporter-{version}-linux-{arch}.tar.gz |  |\n| tiup-linux-{arch}.tar.gz |  |\n| tiup-{version}-linux-{arch}.tar.gz |  |\n| local_install.sh |  |\n| cluster-{version}-linux-{arch}.tar.gz |  |\n| insight-{version}-linux-{arch}.tar.gz |  |\n| diag-{version}-linux-{arch}.tar.gz | 从 v6.0.0 起新增 |\n| influxdb-{version}-linux-{arch}.tar.gz |  |\n| playground-{version}-linux-{arch}.tar.gz |  |\n| tiproxy-{version}-linux-{arch}.tar.gz | 从 v7.6.0 起新增 |\n\n> **注意：**\n>\n> 以上离线包名称中，`{version}` 取决于离线包中内容的版本号，`{arch}` 取决于离线包对应的架构（amd64 或 arm64）。\n\n`TiDB-community-toolkit` 软件包中包含以下内容：\n\n| 内容 | 变更说明 |\n|---|---|\n| pd-recover-{version}-linux-{arch}.tar.gz |  |\n| etcdctl | 从 v6.0.0 起新增 |\n| tiup-linux-{arch}.tar.gz |  |\n| tiup-{version}-linux-{arch}.tar.gz |  |\n| tidb-lightning-{version}-linux-{arch}.tar.gz |  |\n| tidb-lightning-ctl |  |\n| dumpling-{version}-linux-{arch}.tar.gz |  |\n| cdc-{version}-linux-{arch}.tar.gz |  |\n| dm-{version}-linux-{arch}.tar.gz |  |\n| dm-worker-{version}-linux-{arch}.tar.gz |  |\n| dm-master-{version}-linux-{arch}.tar.gz |  |\n| dmctl-{version}-linux-{arch}.tar.gz |  |\n| br-{version}-linux-{arch}.tar.gz |  |\n| package-{version}-linux-{arch}.tar.gz |  |\n| bench-{version}-linux-{arch}.tar.gz |  |\n| errdoc-{version}-linux-{arch}.tar.gz |  |\n| dba-{version}-linux-{arch}.tar.gz |  |\n| PCC-{version}-linux-{arch}.tar.gz |  |\n| sync_diff_inspector |  |\n| reparo |  |\n| server-{version}-linux-{arch}.tar.gz | 从 v6.2.0 起新增 |\n| grafana-{version}-linux-{arch}.tar.gz | 从 v6.2.0 起新增 |\n| alertmanager-{version}-linux-{arch}.tar.gz | 从 v6.2.0 起新增 |\n| prometheus-{version}-linux-{arch}.tar.gz | 从 v6.2.0 起新增  |\n| blackbox_exporter-{version}-linux-{arch}.tar.gz | 从 v6.2.0 起新增 |\n| node_exporter-{version}-linux-{arch}.tar.gz | 从 v6.2.0 起新增 |\n\n> **注意：**\n>\n> 以上离线包名称中，`{version}` 取决于离线包中工具的版本号，`{arch}` 取决于离线包对应的架构（amd64 或 arm64）。\n\n## 延伸阅读\n\n[离线部署 TiDB 集群](/production-deployment-using-tiup.md#离线部署)\n"
        },
        {
          "name": "blocklist-control-plan.md",
          "type": "blob",
          "size": 10.392578125,
          "content": "---\ntitle: 优化规则与表达式下推的黑名单\nsummary: 了解优化规则与表达式下推的黑名单。\naliases: ['/docs-cn/dev/blacklist-control-plan/','/zh/tidb/dev/blacklist-control-plan']\n---\n\n# 优化规则与表达式下推的黑名单\n\n本文主要介绍优化规则的黑名单与表达式下推的黑名单。\n\n## 优化规则黑名单\n\n**优化规则黑名单**是针对优化规则的调优手段之一，主要用于手动禁用一些优化规则。\n\n### 重要的优化规则\n\n|**优化规则**|**规则名称**|**简介**|\n| :--- | :--- | :--- |\n| 列裁剪 | column_prune | 对于上层算子不需要的列，不在下层算子输出该列，减少计算 |\n| 子查询去关联 | decorrelate | 尝试对相关子查询进行改写，将其转换为普通 join 或 aggregation 计算 |\n| 聚合消除 | aggregation_eliminate | 尝试消除执行计划中的某些不必要的聚合算子 |\n| 投影消除 | projection_eliminate |  消除执行计划中不必要的投影算子 |\n| 最大最小消除 | max_min_eliminate | 改写聚合中的 max/min 计算，转化为 `order by` + `limit 1` |\n| 谓词下推 | predicate_push_down | 尝试将执行计划中过滤条件下推到离数据源更近的算子上 |\n| 外连接消除 | outer_join_eliminate | 尝试消除执行计划中不必要的 left join 或者 right join |\n| 分区裁剪 | partition_processor | 将分区表查询改成为用 union all，并裁剪掉不满足过滤条件的分区 |\n| 聚合下推 | aggregation_push_down | 尝试将执行计划中的聚合算子下推到更底层的计算节点 |\n| TopN 下推 | topn_push_down | 尝试将执行计划中的 TopN 算子下推到离数据源更近的算子上 |\n| Join 重排序 | join_reorder | 对多表 join 确定连接顺序 |\n| 从窗口函数中推导 TopN 或 Limit | derive_topn_from_window | 从窗口函数中推导出 TopN 或者 Limit |\n\n### 禁用优化规则\n\n当某些优化规则在一些特殊查询中的优化结果不理想时，可以使用**优化规则黑名单**禁用一些优化规则。\n\n#### 使用方法\n\n> **注意：**\n>\n> 以下操作都需要数据库的 super privilege 权限。每个优化规则都有各自的名字，比如列裁剪的名字是 \"column_prune\"。所有优化规则的名字都可以在[重要的优化规则](#重要的优化规则)表格中第二列查到。\n\n- 如果你想禁用某些规则，可以在 `mysql.opt_rule_blacklist` 表中写入规则的名字，例如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    INSERT INTO mysql.opt_rule_blacklist VALUES(\"join_reorder\"), (\"topn_push_down\");\n    ```\n\n    执行以下 SQL 语句可让禁用规则立即生效，包括相应 TiDB Server 的所有旧链接：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    ADMIN reload opt_rule_blacklist;\n    ```\n\n    > **注意：**\n    >\n    > `admin reload opt_rule_blacklist` 只对执行该 SQL 语句的 TiDB server 生效。若需要集群中所有 TiDB server 生效，需要在每台 TiDB server 上执行该 SQL 语句。\n\n- 需要解除一条规则的禁用时，需要删除表中禁用该条规则的相应数据，再执行 `admin reload`：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    DELETE FROM mysql.opt_rule_blacklist WHERE name IN (\"join_reorder\", \"topn_push_down\");\n    admin reload opt_rule_blacklist;\n    ```\n\n## 表达式下推黑名单\n\n**表达式下推黑名单**是针对表达式下推的调优手段之一，主要用于对于某些存储类型手动禁用一些表达式。\n\n### 已支持下推的表达式\n\n目前已经支持下推的表达式信息，请参考[表达式列表](/functions-and-operators/expressions-pushed-down.md#已支持下推的表达式列表)。\n\n### 禁止特定表达式下推\n\n当函数的计算过程由于下推而出现异常时，可通过黑名单功能禁止其下推来快速恢复业务。具体而言，你可以将上述支持的函数或运算符名加入黑名单 `mysql.expr_pushdown_blacklist` 中，以禁止特定表达式下推。\n\n`mysql.expr_pushdown_blacklist` 的 schema 如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nDESC mysql.expr_pushdown_blacklist;\n```\n\n```sql\n+------------+--------------+------+------+-------------------+-------+\n| Field      | Type         | Null | Key  | Default           | Extra |\n+------------+--------------+------+------+-------------------+-------+\n| name       | char(100)    | NO   |      | NULL              |       |\n| store_type | char(100)    | NO   |      | tikv,tiflash,tidb |       |\n| reason     | varchar(200) | YES  |      | NULL              |       |\n+------------+--------------+------+------+-------------------+-------+\n3 rows in set (0.00 sec)\n```\n\n以上结果字段解释如下：\n\n+ `name`：禁止下推的函数名。\n+ `store_type`：用于指明希望禁止该函数下推到哪些组件进行计算。组件可选 `tidb`、`tikv` 和 `tiflash`。`store_type` 不区分大小写，如果需要禁止向多个存储引擎下推，各个存储之间需用逗号隔开。\n    - `store_type` 为 `tidb` 时表示在读取 TiDB 内存表时，是否允许该函数在其他 TiDB Server 上执行。\n    - `store_type` 为 `tikv` 时表示是否允许该函数在 TiKV Server 的 Coprocessor 模块中执行。\n    - `store_type` 为 `tiflash` 时表示是否允许该函数在 TiFlash Server 的 Coprocessor 模块中执行。\n+ `reason`：用于记录该函数被加入黑名单的原因。\n\n### 使用方法\n\n#### 加入黑名单\n\n如果要将一个或多个函数或运算符加入黑名单，执行以下步骤：\n\n1. 向 `mysql.expr_pushdown_blacklist` 插入对应的函数名或运算符名以及希望禁止下推的存储引擎集合。\n\n2. 执行 `admin reload expr_pushdown_blacklist;`。\n\n#### 移出黑名单\n\n如果要将一个或多个函数及运算符移出黑名单，执行以下步骤：\n\n1. 从 `mysql.expr_pushdown_blacklist` 表中删除对应的函数名或运算符名。\n\n2. 执行 `admin reload expr_pushdown_blacklist;`。\n\n> **注意：**\n>\n> `admin reload expr_pushdown_blacklist` 只对执行该 SQL 语句的 TiDB server 生效。若需要集群中所有 TiDB server 生效，需要在每台 TiDB server 上执行该 SQL 语句。\n\n### 表达式黑名单用法示例\n\n以下示例首先将运算符 `<` 及 `>` 加入黑名单，然后将运算符 `>` 从黑名单中移出。\n\n黑名单是否生效可以从 `explain` 结果中进行观察（参见 [TiDB 执行计划概览](/explain-overview.md)）。\n\n1. 对于以下 SQL 语句，`where` 条件中的 `a < 2` 和 `a > 2` 可以下推到 TiKV 进行计算。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    EXPLAIN SELECT * FROM t WHERE a < 2 AND a > 2;\n    ```\n\n    ```sql\n    +-------------------------+----------+-----------+---------------+------------------------------------+\n    | id                      | estRows  | task      | access object | operator info                      |\n    +-------------------------+----------+-----------+---------------+------------------------------------+\n    | TableReader_7           | 0.00     | root      |               | data:Selection_6                   |\n    | └─Selection_6           | 0.00     | cop[tikv] |               | gt(ssb_1.t.a, 2), lt(ssb_1.t.a, 2) |\n    |   └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo     |\n    +-------------------------+----------+-----------+---------------+------------------------------------+\n    3 rows in set (0.00 sec)\n    ```\n\n2. 往 `mysql.expr_pushdown_blacklist` 表中插入禁用表达式，并且执行 `admin reload expr_pushdown_blacklist`。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    INSERT INTO mysql.expr_pushdown_blacklist VALUES('<','tikv',''), ('>','tikv','');\n    ```\n\n    ```sql\n    Query OK, 2 rows affected (0.01 sec)\n    Records: 2  Duplicates: 0  Warnings: 0\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    ADMIN reload expr_pushdown_blacklist;\n    ```\n\n    ```sql\n    Query OK, 0 rows affected (0.00 sec)\n    ```\n\n3. 重新观察执行计划，发现表达式下推黑名单生效，`where` 条件中的 `<` 和 `>` 没有被下推到 TiKV Coprocessor 上。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    EXPLAIN SELECT * FROM t WHERE a < 2 and a > 2;\n    ```\n\n    ```sql\n    +-------------------------+----------+-----------+---------------+------------------------------------+\n    | id                      | estRows  | task      | access object | operator info                      |\n    +-------------------------+----------+-----------+---------------+------------------------------------+\n    | Selection_7             | 10000.00 | root      |               | gt(ssb_1.t.a, 2), lt(ssb_1.t.a, 2) |\n    | └─TableReader_6         | 10000.00 | root      |               | data:TableFullScan_5               |\n    |   └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo     |\n    +-------------------------+----------+-----------+---------------+------------------------------------+\n    3 rows in set (0.00 sec)\n    ```\n\n4. 将某一表达式（`>` 大于）禁用规则从黑名单表中删除，并且执行 `admin reload expr_pushdown_blacklist`。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    DELETE FROM mysql.expr_pushdown_blacklist WHERE name = '>';\n    ```\n\n    ```sql\n    Query OK, 1 row affected (0.01 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    ADMIN reload expr_pushdown_blacklist;\n    ```\n\n    ```sql\n    Query OK, 0 rows affected (0.00 sec)\n    ```\n\n5. 重新观察执行计划，可以看到只有 `>` 表达式被重新下推到 TiKV Coprocessor，`<` 表达式仍然被禁用下推。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    EXPLAIN SELECT * FROM t WHERE a < 2 AND a > 2;\n    ```\n\n    ```sql\n    +---------------------------+----------+-----------+---------------+--------------------------------+\n    | id                        | estRows  | task      | access object | operator info                  |\n    +---------------------------+----------+-----------+---------------+--------------------------------+\n    | Selection_8               | 0.00     | root      |               | lt(ssb_1.t.a, 2)               |\n    | └─TableReader_7           | 0.00     | root      |               | data:Selection_6               |\n    |   └─Selection_6           | 0.00     | cop[tikv] |               | gt(ssb_1.t.a, 2)               |\n    |     └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo |\n    +---------------------------+----------+-----------+---------------+--------------------------------+\n    4 rows in set (0.00 sec)\n    ```\n"
        },
        {
          "name": "br",
          "type": "tree",
          "content": null
        },
        {
          "name": "cached-tables.md",
          "type": "blob",
          "size": 11.326171875,
          "content": "---\ntitle: 缓存表\nsummary: 了解 TiDB 中的缓存表功能，用于很少被修改的热点小表，提升读性能。\n---\n\n# 缓存表\n\nTiDB 在 v6.0.0 版本中引入了缓存表功能。该功能适用于频繁被访问且很少被修改的热点小表，即把整张表的数据加载到 TiDB 服务器的内存中，直接从内存中获取表数据，避免从 TiKV 获取表数据，从而提升读性能。\n\n本文介绍了 TiDB 缓存表的使用场景、使用示例、与其他 TiDB 功能的兼容性限制。\n\n## 使用场景\n\nTiDB 缓存表功能适用于以下特点的表：\n\n- 表的数据量不大，例如 4 MiB 以下\n- 只读表，或者几乎很少修改，例如写入 QPS 低于每分钟 10 次\n- 表的访问很频繁，期望有更好的读性能，例如在直接读取 TiKV 时遇到小表热点瓶颈\n\n当表的数据量不大，访问又特别频繁的情况下，数据会集中在 TiKV 一个 Region 上，形成热点，从而影响性能。因此，TiDB 缓存表的典型使用场景如下：\n\n- 配置表，业务通过该表读取配置信息\n- 金融场景中的存储汇率的表，该表不会实时更新，每天只更新一次\n- 银行分行或者网点信息表，该表很少新增记录项\n\n以配置表为例，当业务重启的瞬间，全部连接一起加载配置，会造成较高的数据库读延迟。如果使用了缓存表，则可以解决这样的问题。\n\n## 使用示例\n\n本节通过示例介绍缓存表的使用方法。\n\n### 将普通表设为缓存表\n\n假设已存在普通表 `users`:\n\n```sql\nCREATE TABLE users (\n    id BIGINT,\n    name VARCHAR(100),\n    PRIMARY KEY(id)\n);\n```\n\n通过 `ALTER TABLE` 语句，可以将这张表设置成缓存表：\n\n```sql\nALTER TABLE users CACHE;\n```\n\n```\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n### 验证是否为缓存表\n\n要验证一张表是否为缓存表，使用 `SHOW CREATE TABLE` 语句。如果为缓存表，返回结果中会带有 `CACHED ON` 属性：\n\n```sql\nSHOW CREATE TABLE users;\n```\n\n```\n+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table | Create Table                                                                                                                                                                                                               |\n+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| users | CREATE TABLE `users` (\n  `id` bigint NOT NULL,\n  `name` varchar(100) DEFAULT NULL,\n  PRIMARY KEY (`id`) /*T![clustered_index] CLUSTERED */\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin /* CACHED ON */ |\n+-------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n从缓存表读取数据后，TiDB 会将数据加载到内存中。你可使用 [`TRACE`](/sql-statements/sql-statement-trace.md) 语句查看 TiDB 是否已将数据加载到内存中。当缓存还未加载时，语句的返回结果会出现 `regionRequest.SendReqCtx`，表示 TiDB 从 TiKV 读取了数据。\n\n```sql\nTRACE SELECT * FROM users;\n```\n\n```\n+------------------------------------------------+-----------------+------------+\n| operation                                      | startTS         | duration   |\n+------------------------------------------------+-----------------+------------+\n| trace                                          | 17:47:39.969980 | 827.73µs   |\n|   ├─session.ExecuteStmt                        | 17:47:39.969986 | 413.31µs   |\n|   │ ├─executor.Compile                         | 17:47:39.969993 | 198.29µs   |\n|   │ └─session.runStmt                          | 17:47:39.970221 | 157.252µs  |\n|   │   └─TableReaderExecutor.Open               | 17:47:39.970294 | 47.068µs   |\n|   │     └─distsql.Select                       | 17:47:39.970312 | 24.729µs   |\n|   │       └─regionRequest.SendReqCtx           | 17:47:39.970454 | 189.601µs  |\n|   ├─*executor.UnionScanExec.Next               | 17:47:39.970407 | 353.073µs  |\n|   │ ├─*executor.TableReaderExecutor.Next       | 17:47:39.970411 | 301.106µs  |\n|   │ └─*executor.TableReaderExecutor.Next       | 17:47:39.970746 | 6.57µs     |\n|   └─*executor.UnionScanExec.Next               | 17:47:39.970772 | 17.589µs   |\n|     └─*executor.TableReaderExecutor.Next       | 17:47:39.970776 | 6.59µs     |\n+------------------------------------------------+-----------------+------------+\n12 rows in set (0.01 sec)\n```\n\n而再次执行 [`TRACE`](/sql-statements/sql-statement-trace.md)，返回结果中不再有 `regionRequest.SendReqCtx`，表示 TiDB 已经不再从 TiKV 读取数据，而是直接从内存中读取：\n\n```\n+----------------------------------------+-----------------+------------+\n| operation                              | startTS         | duration   |\n+----------------------------------------+-----------------+------------+\n| trace                                  | 17:47:40.533888 | 453.547µs  |\n|   ├─session.ExecuteStmt                | 17:47:40.533894 | 402.341µs  |\n|   │ ├─executor.Compile                 | 17:47:40.533903 | 205.54µs   |\n|   │ └─session.runStmt                  | 17:47:40.534141 | 132.084µs  |\n|   │   └─TableReaderExecutor.Open       | 17:47:40.534202 | 14.749µs   |\n|   ├─*executor.UnionScanExec.Next       | 17:47:40.534306 | 3.21µs     |\n|   └─*executor.UnionScanExec.Next       | 17:47:40.534316 | 1.219µs    |\n+----------------------------------------+-----------------+------------+\n7 rows in set (0.00 sec)\n```\n\n注意，读取缓存表会使用 `UnionScan` 算子，所以通过 `explain` 查看缓存表的执行计划时，可能会在结果中看到 `UnionScan`：\n\n```sql\n+-------------------------+---------+-----------+---------------+--------------------------------+\n| id                      | estRows | task      | access object | operator info                  |\n+-------------------------+---------+-----------+---------------+--------------------------------+\n| UnionScan_5             | 1.00    | root      |               |                                |\n| └─TableReader_7         | 1.00    | root      |               | data:TableFullScan_6           |\n|   └─TableFullScan_6     | 1.00    | cop[tikv] | table:users   | keep order:false, stats:pseudo |\n+-------------------------+---------+-----------+---------------+--------------------------------+\n3 rows in set (0.00 sec)\n```\n\n### 往缓存表写入数据\n\n缓存表支持写入数据。例如，往 `users` 表中插入一条记录：\n\n```sql\nINSERT INTO users(id, name) VALUES(1001, 'Davis');\n```\n\n```\nQuery OK, 1 row affected (0.00 sec)\n```\n\n```sql\nSELECT * FROM users;\n```\n\n```\n+------+-------+\n| id   | name  |\n+------+-------+\n| 1001 | Davis |\n+------+-------+\n1 row in set (0.00 sec)\n```\n\n> **注意：**\n>\n> 往缓存表写入数据时，有可能出现秒级别的写入延迟。延迟的时长由全局环境变量 [`tidb_table_cache_lease`](/system-variables.md#tidb_table_cache_lease-从-v600-版本开始引入) 控制。你可根据实际业务能否承受此限制带来的延迟，决定是否适合使用缓存表功能。例如，对于完全只读的场景，可以将 `tidb_table_cache_lease` 调大：\n>\n> ```sql\n> set @@global.tidb_table_cache_lease = 10;\n> ```\n>\n> 缓存表的写入延时高是受到实现的限制。存在多个 TiDB 实例时，一个 TiDB 实例并不知道其它的 TiDB 实例是否缓存了数据，如果该实例直接修改了表数据，而其它 TiDB 实例依然读取旧的缓存数据，就会读到错误的结果。为了保证数据正确性，缓存表的实现使用了一套基于 lease 的复杂机制：读操作在缓存数据同时，还会对于缓存设置一个有效期，也就是 lease。在 lease 过期之前，无法对数据执行修改操作。因为修改操作必须等待 lease 过期，所以会出现写入延迟。\n\n缓存表相关的元信息存储在 `mysql.table_cache_meta` 表中。这张表记录了所有缓存表的 ID、当前的锁状态 `lock_type`，以及锁租约 `lease` 相关的信息。这张表仅供 TiDB 内部使用，不建议用户修改该表，否则可能导致不可预期的错误。\n\n```sql\nSHOW CREATE TABLE mysql.table_cache_meta\\G\n*************************** 1. row ***************************\n       Table: table_cache_meta\nCreate Table: CREATE TABLE `table_cache_meta` (\n  `tid` bigint NOT NULL DEFAULT '0',\n  `lock_type` enum('NONE','READ','INTEND','WRITE') NOT NULL DEFAULT 'NONE',\n  `lease` bigint NOT NULL DEFAULT '0',\n  `oldReadLease` bigint NOT NULL DEFAULT '0',\n  PRIMARY KEY (`tid`) /*T![clustered_index] CLUSTERED */\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin\n1 row in set (0.00 sec)\n```\n\n### 将缓存表恢复为普通表\n\n> **注意：**\n>\n> 对缓存表执行 DDL 语句会失败。若要对缓存表执行 DDL 语句，需要先去掉缓存属性，将缓存表设回普通表后，才能对其执行 DDL 语句。\n\n```sql\nTRUNCATE TABLE users;\n```\n\n```\nERROR 8242 (HY000): 'Truncate Table' is unsupported on cache tables.\n```\n\n```sql\nmysql> ALTER TABLE users ADD INDEX k_id(id);\n```\n\n```\nERROR 8242 (HY000): 'Alter Table' is unsupported on cache tables.\n```\n\n使用 `ALTER TABLE t NOCACHE` 语句可以将缓存表恢复成普通表：\n\n```sql\nALTER TABLE users NOCACHE\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n## 缓存表大小限制\n\n由于 TiDB 将整张缓存表的数据加载到 TiDB 进程的内存中，并且执行修改操作后缓存会失效，需要重新加载，所以 TiDB 缓存表只适用于表比较小的场景。\n\n目前 TiDB 对于每张缓存表的大小限制为 64 MiB。如果表的数据超过了 64 MiB，执行 `ALTER TABLE t CACHE` 会失败。\n\n## 与其他 TiDB 功能的兼容性限制\n\n以下是缓存表不支持的功能：\n\n- 不支持对分区表执行 `ALTER TABLE t CACHE` 操作\n- 不支持对临时表执行 `ALTER TABLE t CACHE` 操作\n- 不支持对视图执行 `ALTER TABLE t CACHE` 操作\n- 不支持 Stale Read 功能\n- 不支持对缓存表直接做 DDL 操作，需要先通过 `ALTER TABLE t NOCACHE` 将缓存表改回普通表后再进行 DDL 操作。\n\n以下是缓存表无法使用缓存的场景：\n\n- 设置系统变量 `tidb_snapshot` 读取历史数据\n- 执行修改操作期间，已有缓存会失效，直到数据被再次加载\n\n## TiDB 数据迁移工具兼容性\n\n缓存表并不是标准的 MySQL 功能，而是 TiDB 扩展。只有 TiDB 能识别 `ALTER TABLE ... CACHE` 语句。所有的 TiDB 数据迁移工具均不支持缓存表功能，包括 Backup & Restore (BR)、TiCDC、Dumpling 等组件，它们会将缓存表当作普通表处理。\n\n这意味着，备份恢复一张缓存表时，它会变成一张普通表。如果下游集群是另一套 TiDB 集群并且你希望继续使用缓存表功能，可以对下游集群中的表执行 `ALTER TABLE ... CACHE` 手动开启缓存表功能。\n\n## 另请参阅\n\n* [ALTER TABLE](/sql-statements/sql-statement-alter-table.md)\n* [System Variables](/system-variables.md)\n"
        },
        {
          "name": "certificate-authentication.md",
          "type": "blob",
          "size": 16.7685546875,
          "content": "---\ntitle: TiDB 证书鉴权使用指南\nsummary: 了解使用 TiDB 的证书鉴权功能。\naliases: ['/docs-cn/dev/certificate-authentication/','/docs-cn/dev/reference/security/cert-based-authentication/']\n---\n\n# TiDB 证书鉴权使用指南\n\nTiDB 支持基于证书鉴权的登录方式。采用这种方式，TiDB 对不同用户签发证书，使用加密连接来传输数据，并在用户登录时验证证书。相比 MySQL 用户常用的用户名密码验证方式，与 MySQL 相兼容的证书鉴权方式更安全，因此越来越多的用户使用证书鉴权来代替用户名密码验证。\n\n在 TiDB 上使用证书鉴权的登录方法，可能需要进行以下操作：\n\n+ 创建安全密钥和证书\n+ 配置 TiDB 和客户端使用的证书\n+ 配置登录时需要校验的用户证书信息\n+ 更新和替换证书\n\n本文介绍了如何进行证书鉴权的上述几个操作。\n\n## 创建安全密钥和证书\n\n目前推荐使用 [OpenSSL](https://www.openssl.org/) 来生成密钥和证书，生成证书的过程和[为 TiDB 客户端服务端间通信开启加密传输](/enable-tls-between-clients-and-servers.md)过程类似，下面更多演示如何在证书中配置更多需校验的属性字段。\n\n### 生成 CA 密钥和证书\n\n1. 执行以下命令生成 CA 密钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl genrsa 2048 > ca-key.pem\n    ```\n\n    命令执行后输出以下结果：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    Generating RSA private key, 2048 bit long modulus (2 primes)\n    ....................+++++\n    ...............................................+++++\n    e is 65537 (0x010001)\n    ```\n\n2. 执行以下命令生成该密钥对应的证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl req -new -x509 -nodes -days 365000 -key ca-key.pem -out ca-cert.pem\n    ```\n\n3. 输入证书细节信息，示例如下：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    Country Name (2 letter code) [AU]:US\n    State or Province Name (full name) [Some-State]:California\n    Locality Name (eg, city) []:San Francisco\n    Organization Name (eg, company) [Internet Widgits Pty Ltd]:PingCAP Inc.\n    Organizational Unit Name (eg, section) []:TiDB\n    Common Name (e.g. server FQDN or YOUR name) []:TiDB admin\n    Email Address []:s@pingcap.com\n    ```\n\n    > **注意：**\n    >\n    > 以上信息中，`:` 后的文字为用户输入的信息。\n\n### 生成服务端密钥和证书\n\n1. 执行以下命令生成服务端的密钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl req -newkey rsa:2048 -days 365000 -nodes -keyout server-key.pem -out server-req.pem\n    ```\n\n2. 输入证书细节信息，示例如下：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    Country Name (2 letter code) [AU]:US\n    State or Province Name (full name) [Some-State]:California\n    Locality Name (eg, city) []:San Francisco\n    Organization Name (eg, company) [Internet Widgits Pty Ltd]:PingCAP Inc.\n    Organizational Unit Name (eg, section) []:TiKV\n    Common Name (e.g. server FQDN or YOUR name) []:TiKV Test Server\n    Email Address []:k@pingcap.com\n\n    Please enter the following 'extra' attributes\n    to be sent with your certificate request\n    A challenge password []:\n    An optional company name []:\n    ```\n\n3. 执行以下命令生成服务端的 RSA 密钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl rsa -in server-key.pem -out server-key.pem\n    ```\n\n    输出结果如下：\n\n    ```bash\n    writing RSA key\n    ```\n\n4. 使用 CA 证书签名来生成服务端的证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl x509 -req -in server-req.pem -days 365000 -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.pem\n    ```\n\n    输出结果示例如下：\n\n    ```bash\n    Signature ok\n    subject=C = US, ST = California, L = San Francisco, O = PingCAP Inc., OU = TiKV, CN = TiKV Test Server, emailAddress = k@pingcap.com\n    Getting CA Private Key\n    ```\n\n    > **注意：**\n    >\n    > 以上结果中，用户登录时 TiDB 将强制检查 `subject` 部分的信息是否一致。\n\n### 生成客户端密钥和证书\n\n生成服务端密钥和证书后，需要生成客户端使用的密钥和证书。通常需要为不同的用户生成不同的密钥和证书。\n\n1. 执行以下命令生成客户端的密钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl req -newkey rsa:2048 -days 365000 -nodes -keyout client-key.pem -out client-req.pem\n    ```\n\n2. 输入证书细节信息，示例如下：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    Country Name (2 letter code) [AU]:US\n    State or Province Name (full name) [Some-State]:California\n    Locality Name (eg, city) []:San Francisco\n    Organization Name (eg, company) [Internet Widgits Pty Ltd]:PingCAP Inc.\n    Organizational Unit Name (eg, section) []:TiDB\n    Common Name (e.g. server FQDN or YOUR name) []:tpch-user1\n    Email Address []:zz@pingcap.com\n\n    Please enter the following 'extra' attributes\n    to be sent with your certificate request\n    A challenge password []:\n    An optional company name []:\n    ```\n\n3. 执行以下命令生成客户端 RSA 证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl rsa -in client-key.pem -out client-key.pem\n    ```\n\n    以上命令的输出结果如下：\n\n    ```bash\n    writing RSA key\n    ```\n\n4. 执行以下命令，使用 CA 证书签名来生成客户端证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl x509 -req -in client-req.pem -days 365000 -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out client-cert.pem\n    ```\n\n    输出结果示例如下：\n\n    ```bash\n    Signature ok\n    subject=C = US, ST = California, L = San Francisco, O = PingCAP Inc., OU = TiDB, CN = tpch-user1, emailAddress = zz@pingcap.com\n    Getting CA Private Key\n    ```\n\n    > **注意：**\n    >\n    > 以上结果中，`subject` 部分后的信息会被用来在 `require` 中配置和要求验证。\n\n### 验证证书\n\n执行以下命令验证证书：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nopenssl verify -CAfile ca-cert.pem server-cert.pem client-cert.pem\n```\n\n如果验证通过，会显示以下信息：\n\n```\nserver-cert.pem: OK\nclient-cert.pem: OK\n```\n\n## 配置 TiDB 和客户端使用证书\n\n在生成证书后，需要在 TiDB 中配置服务端所使用的证书，同时让客户端程序使用客户端证书。\n\n### 配置 TiDB 服务端\n\n修改 TiDB 配置文件中的 `[security]` 段。这一步指定 CA 证书、服务端密钥和服务端证书存放的路径。可将 `path/to/server-cert.pem`、`path/to/server-key.pem` 和 `path/to/ca-cert.pem` 替换成实际的路径。\n\n{{< copyable \"\" >}}\n\n```\n[security]\nssl-cert =\"path/to/server-cert.pem\"\nssl-key =\"path/to/server-key.pem\"\nssl-ca=\"path/to/ca-cert.pem\"\n```\n\n启动 TiDB 日志。如果日志中有以下内容，即代表配置生效：\n\n```\n[INFO] [server.go:286] [\"mysql protocol server secure connection is enabled\"] [\"client verification enabled\"=true]\n```\n\n### 配置客户端程序\n\n配置客户端程序，让客户端使用客户端密钥和证书来登录 TiDB。\n\n以 MySQL 客户端为例，可以通过指定 `ssl-cert`、`ssl-key`、`ssl-ca` 来使用新的 CA 证书、客户端密钥和证书：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nmysql -utest -h0.0.0.0 -P4000 --ssl-cert /path/to/client-cert.new.pem --ssl-key /path/to/client-key.new.pem --ssl-ca /path/to/ca-cert.pem\n```\n\n> **注意：**\n>\n> `/path/to/client-cert.new.pem`、`/path/to/client-key.new.pem` 和 `/path/to/ca-cert.pem` 是 CA 证书、客户端密钥和客户端存放的路径。可将以上命令中的这些部分替换为实际的路径。\n\n## 配置登录时需要校验的用户证书信息\n\n使用客户端连接 TiDB 进行授权配置。先获取需要验证的用户证书信息，再对这些信息进行配置。\n\n### 获取用户证书信息\n\n用户证书信息可由 `REQUIRE SUBJECT`、`REQUIRE ISSUER`、`REQUIRE SAN` 和 `REQUIRE CIPHER` 来指定，用于检查 X.509 certificate attributes。\n\n+ `REQUIRE SUBJECT`：指定用户在连接时需要提供客户端证书的 `subject` 内容。指定该选项后，不需要再配置 `require ssl` 或 x509。配置内容对应[生成客户端密钥和证书](#生成客户端密钥和证书)中的录入信息。\n\n    可以执行以下命令来获取该项的信息：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```\n    openssl x509 -noout -subject -in client-cert.pem | sed 's/.\\{8\\}//'  | sed 's/, /\\//g' | sed 's/ = /=/g' | sed 's/^/\\//'\n    ```\n\n+ `REQUIRE ISSUER`：指定签发用户证书的 CA 证书的 `subject` 内容。配置内容对应[生成 CA 密钥和证书](#生成-ca-密钥和证书)中的录入信息。\n\n    可以执行以下命令来获取该项的信息：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```\n    openssl x509 -noout -subject -in ca-cert.pem | sed 's/.\\{8\\}//'  | sed 's/, /\\//g' | sed 's/ = /=/g' | sed 's/^/\\//'\n    ```\n\n+ `REQUIRE SAN`：指定签发用户证书的 CA 证书的 `Subject Alternative Name` 内容。配置内容对应生成客户端证书使用的 [openssl.cnf 配置文件的 `alt_names` 信息](/generate-self-signed-certificates.md)。\n\n    + 可以执行以下命令来获取已生成证书中的 `REQUIRE SAN` 项的信息：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        openssl x509 -noout -extensions subjectAltName -in client.crt\n        ```\n\n    + `REQUIRE SAN` 目前支持以下 `Subject Alternative Name` 检查项：\n\n        - URI\n        - IP\n        - DNS\n\n    + 多个检查项可通过逗号连接后进行配置。例如，对用户 `u1` 进行以下配置：\n\n        {{< copyable \"sql\" >}}\n\n        ```sql\n        CREATE USER 'u1'@'%' REQUIRE SAN 'DNS:d1,URI:spiffe://example.org/myservice1,URI:spiffe://example.org/myservice2';\n        ```\n\n        以上配置只允许用户 `u1` 使用 URI 项为 `spiffe://example.org/myservice1` 或 `spiffe://example.org/myservice2`、DNS 项为 `d1` 的证书登录 TiDB。\n\n+ `REQUIRE CIPHER`：配置该项检查客户端支持的 `cipher method`。可以使用以下语句来查看支持的列表：\n\n    ```sql\n    SHOW SESSION STATUS LIKE 'Ssl_cipher_list';\n    ```\n\n### 配置用户证书信息\n\n获取用户证书信息（`REQUIRE SUBJECT`、`REQUIRE ISSUER`、`REQUIRE SAN` 和 `REQUIRE CIPHER`）后，可在创建用户、赋予权限或更改用户时配置用户证书信息。将以下命令中的 `<replaceable>` 替换为对应的信息。可以选择配置其中一项或多项，使用空格或 `and` 分隔。\n\n+ 可以在创建用户 (`CREATE USER`) 时配置登录时需要校验的证书信息：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    CREATE USER 'u1'@'%' REQUIRE ISSUER '<replaceable>' SUBJECT '<replaceable>' SAN '<replaceable>' CIPHER '<replaceable>';\n    ```\n\n+ 可以在修改已有用户 (`ALTER USER`) 时配置登录时需要校验的证书信息：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    ALTER USER 'u1'@'%' REQUIRE ISSUER '<replaceable>' SUBJECT '<replaceable>' SAN '<replaceable>' CIPHER '<replaceable>';\n    ```\n\n配置完成后，用户在登录时 TiDB 会验证以下内容：\n\n+ 使用 SSL 登录，且证书为服务器配置的 CA 证书所签发\n+ 证书的 `issuer` 信息和权限配置里的 `REQUIRE ISSUER` 信息相匹配\n+ 证书的 `subject` 信息和权限配置里的 `REQUIRE CIPHER` 信息相匹配\n+ 证书的 `Subject Alternative Name` 信息和权限配置里的 `REQUIRE SAN` 信息相匹配\n\n全部验证通过后用户才能登录，否则会报 `ERROR 1045 (28000): Access denied` 错误。登录后，可以通过以下命令来查看当前链接是否使用证书登录、TLS 版本和 Cipher 算法。\n\n连接 MySQL 客户端并执行：\n\n```sql\n\\s\n```\n\n返回结果如下：\n\n```\n--------------\nmysql  Ver 8.5.0 for Linux on x86_64 (MySQL Community Server - GPL)\n\nConnection id:       1\nCurrent database:    test\nCurrent user:        root@127.0.0.1\nSSL:                 Cipher in use is TLS_AES_128_GCM_SHA256\n```\n\n然后执行：\n\n```sql\nSHOW VARIABLES LIKE '%ssl%';\n```\n\n返回结果如下：\n\n```\n+---------------+----------------------------------+\n| Variable_name | Value                            |\n+---------------+----------------------------------+\n| have_openssl  | YES                              |\n| have_ssl      | YES                              |\n| ssl_ca        | /path/to/ca-cert.pem             |\n| ssl_cert      | /path/to/server-cert.pem         |\n| ssl_cipher    |                                  |\n| ssl_key       | /path/to/server-key.pem          |\n+---------------+----------------------------------+\n6 rows in set (0.06 sec)\n```\n\n## 更新和替换证书\n\n证书和密钥通常会周期性更新。下文介绍更新密钥和证书的流程。\n\nCA 证书是客户端和服务端相互校验的依据，所以如果需要替换 CA 证书，则需要生成一个组合证书来在替换期间同时支持客户端和服务器上新旧证书的验证，并优先替换客户端和服务端的 CA 证书，再替换客户端和服务端的密钥和证书。\n\n### 更新 CA 密钥和证书\n\n1. 以替换 CA 密钥为例（假设 `ca-key.pem` 被盗），将旧的 CA 密钥和证书进行备份：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    mv ca-key.pem ca-key.old.pem && \\\n    mv ca-cert.pem ca-cert.old.pem\n    ```\n\n2. 生成新的 CA 密钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl genrsa 2048 > ca-key.pem\n    ```\n\n3. 用新的密钥生成新的 CA 证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl req -new -x509 -nodes -days 365000 -key ca-key.pem -out ca-cert.new.pem\n    ```\n\n    > **注意：**\n    >\n    > 生成新的 CA 证书是为了替换密钥和证书，保证在线用户不受影响。所以以上命令中填写的附加信息必须与已配置的 `require issuer` 信息一致。\n\n4. 生成组合 CA 证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    cat ca-cert.new.pem ca-cert.old.pem > ca-cert.pem\n    ```\n\n之后使用新生成的组合 CA 证书并重启 TiDB Server，此时服务端可以同时接受和使用新旧 CA 证书。\n\n之后先将所有客户端用的 CA 证书也替换为新生成的组合 CA 证书，使客户端能同时和使用新旧 CA 证书。\n\n### 更新客户端密钥和证书\n\n> **注意：**\n>\n> 需要将集群中所有服务端和客户端使用的 CA 证书都替换为新生成的组合 CA 证书后才能开始进行以下步骤。\n\n1. 生成新的客户端 RSA 密钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl req -newkey rsa:2048 -days 365000 -nodes -keyout client-key.new.pem -out client-req.new.pem && \\\n    sudo openssl rsa -in client-key.new.pem -out client-key.new.pem\n    ```\n\n    > **注意：**\n    >\n    > 以上命令是为了替换密钥和证书，保证在线用户不受影响，所以以上命令中填写的附加信息必须与已配置的 `require subject` 信息一致。\n\n2. 使用新的组合 CA 证书和新 CA 密钥生成新客户端证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl x509 -req -in client-req.new.pem -days 365000 -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out client-cert.new.pem\n    ```\n\n3. 让客户端使用新的客户端密钥和证书来连接 TiDB （以 MySQL 客户端为例）：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    mysql -utest -h0.0.0.0 -P4000 --ssl-cert /path/to/client-cert.new.pem --ssl-key /path/to/client-key.new.pem --ssl-ca /path/to/ca-cert.pem\n    ```\n\n    > **注意：**\n    >\n    > `/path/to/client-cert.new.pem`、`/path/to/client-key.new.pem` 和 `/path/to/ca-cert.pem` 是 CA 证书、客户端密钥和客户端存放的路径。可将以上命令中的这些部分替换为实际的路径。\n\n### 更新服务端密钥和证书\n\n1. 生成新的服务端 RSA 密钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl req -newkey rsa:2048 -days 365000 -nodes -keyout server-key.new.pem -out server-req.new.pem && \\\n    sudo openssl rsa -in server-key.new.pem -out server-key.new.pem\n    ```\n\n2. 使用新的组合 CA 证书和新 CA 密钥生成新服务端证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo openssl x509 -req -in server-req.new.pem -days 365000 -CA ca-cert.pem -CAkey ca-key.pem -set_serial 01 -out server-cert.new.pem\n    ```\n\n3. 配置 TiDB 使用上面新生成的服务端密钥和证书并重启。参见[配置 TiDB 服务端](#配置-tidb-服务端)。\n\n## 基于策略的证书访问控制\n\nTiDB 支持基于策略的证书访问控制 (PBAC)，利用底层密钥管理服务器定义的策略。这使得用户能够根据各种条件进行细粒度的访问控制，例如基于时间的策略（如证书仅在特定时间段内有效）、基于位置的策略（如限制对特定地理位置的访问）以及其他可自定义的条件，从而确保在证书管理中提供更高的安全性和灵活性。"
        },
        {
          "name": "character-set-and-collation.md",
          "type": "blob",
          "size": 22.11328125,
          "content": "---\ntitle: 字符集和排序规则\naliases: ['/docs-cn/dev/character-set-and-collation/','/docs-cn/dev/reference/sql/characterset-and-collation/','/docs-cn/dev/reference/sql/character-set/']\nsummary: TiDB 支持的字符集包括 ascii、binary、gbk、latin1、utf8 和 utf8mb4。排序规则包括 ascii_bin、binary、gbk_bin、gbk_chinese_ci、latin1_bin、utf8_bin、utf8_general_ci、utf8_unicode_ci、utf8mb4_0900_ai_ci、utf8mb4_0900_bin、utf8mb4_bin、utf8mb4_general_ci 和 utf8mb4_unicode_ci。TiDB 强烈建议使用 utf8mb4 字符集，因为它支持更多字符。在 TiDB 中，默认的排序规则受到客户端的连接排序规则设置的影响。如果客户端使用 utf8mb4_0900_ai_ci 作为连接排序规则，TiDB 将遵循客户端的配置。TiDB 还支持新的排序规则框架，用于在语义上支持不同的排序规则。\n---\n\n# 字符集和排序规则\n\n本文介绍了 TiDB 中支持的字符集和排序规则。\n\n## 字符集和排序规则的概念\n\n字符集 (character set) 是符号与编码的集合。TiDB 中的默认字符集是 `utf8mb4`，与 MySQL 8.0 及更高版本中的默认字符集匹配。\n\n排序规则 (collation) 是在字符集中比较字符以及字符排序顺序的规则。例如，在二进制排序规则中，比较 `A` 和 `a` 的结果是不一样的：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET NAMES utf8mb4 COLLATE utf8mb4_bin;\nSELECT 'A' = 'a';\nSET NAMES utf8mb4 COLLATE utf8mb4_general_ci;\nSELECT 'A' = 'a';\n```\n\n```sql\nSELECT 'A' = 'a';\n```\n\n```sql\n+-----------+\n| 'A' = 'a' |\n+-----------+\n|         0 |\n+-----------+\n1 row in set (0.00 sec)\n```\n\n```sql\nSET NAMES utf8mb4 COLLATE utf8mb4_general_ci;\n```\n\n```sql\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nSELECT 'A' = 'a';\n```\n\n```sql\n+-----------+\n| 'A' = 'a' |\n+-----------+\n|         1 |\n+-----------+\n1 row in set (0.00 sec)\n```\n\n## 支持的字符集和排序规则\n\n目前 TiDB 支持以下字符集：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW CHARACTER SET;\n```\n\n```sql\n+---------+-------------------------------------+-------------------+--------+\n| Charset | Description                         | Default collation | Maxlen |\n+---------+-------------------------------------+-------------------+--------+\n| ascii   | US ASCII                            | ascii_bin         |      1 |\n| binary  | binary                              | binary            |      1 |\n| gbk     | Chinese Internal Code Specification | gbk_bin           |      2 |\n| latin1  | Latin1                              | latin1_bin        |      1 |\n| utf8    | UTF-8 Unicode                       | utf8_bin          |      3 |\n| utf8mb4 | UTF-8 Unicode                       | utf8mb4_bin       |      4 |\n+---------+-------------------------------------+-------------------+--------+\n6 rows in set (0.00 sec)\n```\n\nTiDB 支持以下排序规则：\n\n```sql\nSHOW COLLATION;\n```\n\n```sql\n+--------------------+---------+------+---------+----------+---------+\n| Collation          | Charset | Id   | Default | Compiled | Sortlen |\n+--------------------+---------+------+---------+----------+---------+\n| ascii_bin          | ascii   |   65 | Yes     | Yes      |       1 |\n| binary             | binary  |   63 | Yes     | Yes      |       1 |\n| gbk_bin            | gbk     |   87 |         | Yes      |       1 |\n| gbk_chinese_ci     | gbk     |   28 | Yes     | Yes      |       1 |\n| latin1_bin         | latin1  |   47 | Yes     | Yes      |       1 |\n| utf8_bin           | utf8    |   83 | Yes     | Yes      |       1 |\n| utf8_general_ci    | utf8    |   33 |         | Yes      |       1 |\n| utf8_unicode_ci    | utf8    |  192 |         | Yes      |       1 |\n| utf8mb4_0900_ai_ci | utf8mb4 |  255 |         | Yes      |       1 |\n| utf8mb4_0900_bin   | utf8mb4 |  309 |         | Yes      |       1 |\n| utf8mb4_bin        | utf8mb4 |   46 | Yes     | Yes      |       1 |\n| utf8mb4_general_ci | utf8mb4 |   45 |         | Yes      |       1 |\n| utf8mb4_unicode_ci | utf8mb4 |  224 |         | Yes      |       1 |\n+--------------------+---------+------+---------+----------+---------+\n13 rows in set (0.00 sec)\n```\n\n> **警告：**\n>\n> TiDB 会错误地将 `latin1` 视为 `utf8` 的子集。当用户存储不同于 `latin1` 和 `utf8` 编码的字符时，可能会导致意外情况出现。因此强烈建议使用 `utf8mb4` 字符集。详情参阅 [TiDB #18955](https://github.com/pingcap/tidb/issues/18955)。\n\n> **注意：**\n>\n> TiDB 中的默认排序规则（后缀为 `_bin` 的二进制排序规则）与 [MySQL 中的默认排序规则](https://dev.mysql.com/doc/refman/8.0/en/charset-charsets.html)不同，后者通常是一般排序规则，后缀为 `_general_ci` 或 `_ai_ci`。当用户指定了显式字符集，但依赖于待选的隐式默认排序规则时，这个差异可能导致兼容性问题。\n> 在 TiDB 中，默认的排序规则也受到客户端的[连接排序规则](https://dev.mysql.com/doc/refman/8.0/en/charset-connection.html#charset-connection-system-variables)设置的影响。例如，MySQL 8.x 客户端默认使用 `utf8mb4_0900_ai_ci` 作为 `utf8mb4` 字符集的连接排序规则。\n>\n> - 在 TiDB v7.4.0 之前，如果客户端使用 `utf8mb4_0900_ai_ci` 作为[连接排序规则](https://dev.mysql.com/doc/refman/8.0/en/charset-connection.html#charset-connection-system-variables)，因为 TiDB 不支持 `utf8mb4_0900_ai_ci` 排序规则，TiDB 将回退到使用 TiDB 服务器默认的排序规则 `utf8mb4_bin`。\n> - 从 v7.4.0 开始，如果客户端使用 `utf8mb4_0900_ai_ci` 作为[连接排序规则](https://dev.mysql.com/doc/refman/8.0/en/charset-connection.html#charset-connection-system-variables)，TiDB 将遵循客户端的配置，使用 `utf8mb4_0900_ai_ci` 作为默认排序规则。\n\n利用以下的语句可以查看字符集对应的排序规则（以下是[新的排序规则框架](#新框架下的排序规则支持)）下的结果：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW COLLATION WHERE Charset = 'utf8mb4';\n```\n\n```sql\n+--------------------+---------+------+---------+----------+---------+\n| Collation          | Charset | Id   | Default | Compiled | Sortlen |\n+--------------------+---------+------+---------+----------+---------+\n| utf8mb4_0900_ai_ci | utf8mb4 |  255 |         | Yes      |       1 |\n| utf8mb4_0900_bin   | utf8mb4 |  309 |         | Yes      |       1 |\n| utf8mb4_bin        | utf8mb4 |   46 | Yes     | Yes      |       1 |\n| utf8mb4_general_ci | utf8mb4 |   45 |         | Yes      |       1 |\n| utf8mb4_unicode_ci | utf8mb4 |  224 |         | Yes      |       1 |\n+--------------------+---------+------+---------+----------+---------+\n5 rows in set (0.00 sec)\n```\n\nTiDB 对 GBK 字符集的支持详情见 [GBK](/character-set-gbk.md)。\n\n## TiDB 中的 `utf8` 和 `utf8mb4`\n\nMySQL 限制字符集 `utf8` 为最多 3 个字节。这足以存储在基本多语言平面 (BMP) 中的字符，但不足以存储表情符号 (emoji) 等字符。因此，建议改用字符集`utf8mb4`。\n\n默认情况下，TiDB 同样限制字符集 `utf8` 为最多 3 个字节，以确保 TiDB 中创建的数据可以在 MySQL 中顺利恢复。你可以禁用此功能，方法是将系统变量 [`tidb_check_mb4_value_in_utf8`](/system-variables.md#tidb_check_mb4_value_in_utf8) 的值更改为 `OFF`。\n\n以下示例演示了在表中插入 4 字节的表情符号字符（emoji 字符）时的默认行为。`utf8` 字符集下 `INSERT` 语句不能执行，`utf8mb4` 字符集下可以执行 `INSERT` 语句：\n\n```sql\nCREATE TABLE utf8_test (\n     c char(1) NOT NULL\n    ) CHARACTER SET utf8;\n```\n\n```sql\nQuery OK, 0 rows affected (0.09 sec)\n```\n\n```sql\nCREATE TABLE utf8m4_test (\n     c char(1) NOT NULL\n    ) CHARACTER SET utf8mb4;\n```\n\n```sql\nQuery OK, 0 rows affected (0.09 sec)\n```\n\n```sql\nINSERT INTO utf8_test VALUES ('😉');\n```\n\n```sql\nERROR 1366 (HY000): incorrect utf8 value f09f9889(😉) for column c\n```\n\n```sql\nINSERT INTO utf8m4_test VALUES ('😉');\n```\n\n```sql\nQuery OK, 1 row affected (0.02 sec)\n```\n\n```sql\nSELECT char_length(c), length(c), c FROM utf8_test;\n```\n\n```sql\nEmpty set (0.01 sec)\n```\n\n```sql\nSELECT char_length(c), length(c), c FROM utf8m4_test;\n```\n\n```sql\n+----------------+-----------+------+\n| char_length(c) | length(c) | c    |\n+----------------+-----------+------+\n|              1 |         4 | 😉     |\n+----------------+-----------+------+\n1 row in set (0.00 sec)\n```\n\n## 不同范围的字符集和排序规则\n\n字符集和排序规则可以在设置在不同的层次。\n\n### 数据库的字符集和排序规则\n\n每个数据库都有相应的字符集和排序规则。数据库的字符集和排序规则可以通过以下语句来设置：\n\n```sql\nCREATE DATABASE db_name\n    [[DEFAULT] CHARACTER SET charset_name]\n    [[DEFAULT] COLLATE collation_name]\n\nALTER DATABASE db_name\n    [[DEFAULT] CHARACTER SET charset_name]\n    [[DEFAULT] COLLATE collation_name]\n```\n\n在这里 `DATABASE` 可以跟 `SCHEMA` 互换使用。\n\n不同的数据库之间可以使用不一样的字符集和排序规则。\n\n通过系统变量 `character_set_database` 和 `collation_database` 可以查看到当前数据库的字符集以及排序规则：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE SCHEMA test1 CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;\n```\n\n```sql\nQuery OK, 0 rows affected (0.09 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nUSE test1;\n```\n\n```sql\nDatabase changed\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT @@character_set_database, @@collation_database;\n```\n\n```sql\n+--------------------------|----------------------+\n| @@character_set_database | @@collation_database |\n+--------------------------|----------------------+\n| utf8mb4                  | utf8mb4_general_ci   |\n+--------------------------|----------------------+\n1 row in set (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE SCHEMA test2 CHARACTER SET latin1 COLLATE latin1_bin;\n```\n\n```sql\nQuery OK, 0 rows affected (0.09 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nUSE test2;\n```\n\n```sql\nDatabase changed\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT @@character_set_database, @@collation_database;\n```\n\n```sql\n+--------------------------|----------------------+\n| @@character_set_database | @@collation_database |\n+--------------------------|----------------------+\n| latin1                   | latin1_bin           |\n+--------------------------|----------------------+\n1 row in set (0.00 sec)\n```\n\n在 INFORMATION_SCHEMA 中也可以查看到这两个值：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT DEFAULT_CHARACTER_SET_NAME, DEFAULT_COLLATION_NAME\nFROM INFORMATION_SCHEMA.SCHEMATA WHERE SCHEMA_NAME = 'db_name';\n```\n\n### 表的字符集和排序规则\n\n表的字符集和排序规则可以通过以下语句来设置：\n\n```sql\nCREATE TABLE tbl_name (column_list)\n    [[DEFAULT] CHARACTER SET charset_name]\n    [COLLATE collation_name]]\n\nALTER TABLE tbl_name\n    [[DEFAULT] CHARACTER SET charset_name]\n    [COLLATE collation_name]\n```\n\n例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1(a int) CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;\n```\n\n```sql\nQuery OK, 0 rows affected (0.08 sec)\n```\n\n如果表的字符集和排序规则没有设置，那么数据库的字符集和排序规则就作为其默认值。在仅指定字符集为 `utf8mb4`，但未设置排序规则时，排序规则为变量 [`default_collation_for_utf8mb4`](/system-variables.md#default_collation_for_utf8mb4-从-v740-版本开始引入) 指定的值。\n\n### 列的字符集和排序规则\n\n列的字符集和排序规则的语法如下：\n\n```sql\ncol_name {CHAR | VARCHAR | TEXT} (col_length)\n    [CHARACTER SET charset_name]\n    [COLLATE collation_name]\n\ncol_name {ENUM | SET} (val_list)\n    [CHARACTER SET charset_name]\n    [COLLATE collation_name]\n```\n\n如果列的字符集和排序规则没有设置，那么表的字符集和排序规则就作为其默认值。在仅指定字符集为 `utf8mb4`，但未设置排序规则时，排序规则为变量 [`default_collation_for_utf8mb4`](/system-variables.md#default_collation_for_utf8mb4-从-v740-版本开始引入) 指定的值。\n\n### 字符串的字符集和排序规则\n\n每一个字符串都对应一个字符集和一个排序规则，在使用字符串时指此选项可选，如下：\n\n```sql\n[_charset_name]'string' [COLLATE collation_name]\n```\n\n示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT 'string';\nSELECT _utf8mb4'string';\nSELECT _utf8mb4'string' COLLATE utf8mb4_general_ci;\n```\n\n规则如下：\n\n* 规则 1：如果指定了 `CHARACTER SET charset_name` 和 `COLLATE collation_name`，则直接使用 `charset_name` 字符集和 `collation_name` 排序规则。\n* 规则 2：如果指定了 `CHARACTER SET charset_name` 且未指定 `COLLATE collation_name`，则使用 `charset_name` 字符集和 `charset_name` 对应的默认排序规则。\n* 规则 3：如果 `CHARACTER SET charset_name` 和 `COLLATE collation_name` 都未指定，则使用 `character_set_connection` 和 `collation_connection` 系统变量给出的字符集和排序规则。\n\n### 客户端连接的字符集和排序规则\n\n* 服务器的字符集和排序规则可以通过系统变量 `character_set_server` 和 `collation_server` 获取。\n* 数据库的字符集和排序规则可以通过环境变量 `character_set_database` 和 `collation_database` 获取。\n\n对于每一个客户端的连接，也有相应的变量表示字符集和排序规则：`character_set_connection` 和 `collation_connection`。\n\n`character_set_client` 代表客户端的字符集。在返回结果前，服务端会把结果根据 `character_set_results` 转换成对应的字符集，包括结果的元信息等。\n\n可以用以下的语句来影响这些跟客户端相关的字符集变量：\n\n* `SET NAMES 'charset_name' [COLLATE 'collation_name']`\n\n    `SET NAMES` 用来设定客户端会在之后的请求中使用的字符集。`SET NAMES utf8mb4` 表示客户端会在接下来的请求中，都使用 utf8mb4 字符集。服务端也会在之后返回结果的时候使用 utf8mb4 字符集。`SET NAMES 'charset_name'` 语句其实等于下面语句的组合：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SET character_set_client = charset_name;\n    SET character_set_results = charset_name;\n    SET character_set_connection = charset_name;\n    ```\n\n    `COLLATE` 是可选的，如果没有提供，将会用 `charset_name` 对应的默认排序规则设置 `collation_connection`。\n\n* `SET CHARACTER SET 'charset_name'`\n\n    跟 `SET NAMES` 类似，等价于下面语句的组合：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SET character_set_client = charset_name;\n    SET character_set_results = charset_name;\n    SET character_set_connection=@@character_set_database;\n    SET collation_connection = @@collation_database;\n    ```\n\n## 服务器、数据库、表、列、字符串的字符集和排序规则的优先级\n\n优先级从高到低排列顺序为：\n\n字符串 > 列 > 表 > 数据库 > 服务器\n\n## 字符集和排序规则的通用选择规则\n\n* 规则 1：如果指定了 `CHARACTER SET charset_name` 和 `COLLATE collation_name`，则直接使用 `charset_name` 字符集和 `collation_name` 排序规则。\n* 规则 2：如果指定了 `CHARACTER SET charset_name` 且未指定 `COLLATE collation_name`，则使用 `charset_name` 字符集和 `charset_name` 对应的默认排序规则。\n* 规则 3：如果 `CHARACTER SET charset_name` 和 `COLLATE collation_name` 都未指定，则使用更高优先级的字符集和排序规则。\n\n## 字符合法性检查\n\n当指定的字符集为 utf8 或 utf8mb4 时，TiDB 仅支持合法的 utf8 字符。对于不合法的字符，会报错：`incorrect utf8 value`。该字符合法性检查与 MySQL 8.0 兼容，与 MySQL 5.7 及以下版本不兼容。\n\n如果不希望报错，可以通过 `set @@tidb_skip_utf8_check=1;` 跳过字符检查。\n\n> **注意：**\n>\n> 跳过字符检查可能会使 TiDB 检测不到应用写入的非法 UTF-8 字符，进一步导致执行 `ANALYZE` 时解码错误，以及引入其他未知的编码问题。如果应用不能保证写入字符串的合法性，不建议跳过该检查。\n\n## 排序规则支持\n\n排序规则的语法支持和语义支持受到配置项 [`new_collations_enabled_on_first_bootstrap`](/tidb-configuration-file.md#new_collations_enabled_on_first_bootstrap) 的影响。这里语法支持和语义支持有所区别。语法支持是指 TiDB 能够解析和设置排序规则；而语义支持是指 TiDB 能够在比较字符串时正确地使用排序规则。\n\n在 4.0 版本之前，TiDB 只提供了旧的排序规则框架，能够在语法上支持的绝大部分 MySQL 排序规则，但语义上所有的排序规则都当成二进制排序规则。\n\n4.0 版本中，TiDB 增加了新的排序规则框架用于在语义上支持不同的排序规则，保证字符串比较时严格遵循对应的排序规则，详情请见下文。\n\n### 旧框架下的排序规则支持\n\n在 4.0 版本之前，TiDB 中可以指定大部分 MySQL 中的排序规则，并把这些排序规则按照默认排序规则处理，即以编码字节序为字符定序。和 MySQL 不同的是，TiDB 不会处理字符末尾的空格，因此会造成以下的行为区别：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t(a varchar(20) charset utf8mb4 collate utf8mb4_general_ci PRIMARY KEY);\n```\n\n```sql\nQuery OK, 0 rows affected\n```\n\n```sql\nINSERT INTO t VALUES ('A');\n```\n\n```sql\nQuery OK, 1 row affected\n```\n\n```sql\nINSERT INTO t VALUES ('a');\n```\n\n```sql\nQuery OK, 1 row affected\n```\n\n以上语句，在 TiDB 会执行成功，而在 MySQL 中，由于 `utf8mb4_general_ci` 大小写不敏感，报错 `Duplicate entry 'a'`。\n\n```sql\nINSERT INTO t VALUES ('a ');\n```\n\n```sql\nQuery OK, 1 row affected\n```\n\n以上语句，在 TiDB 会执行成功，而在 MySQL 中，由于补齐空格比较，报错 `Duplicate entry 'a '`。\n\n### 新框架下的排序规则支持\n\nTiDB 4.0 新增了完整的排序规则支持框架，从语义上支持了排序规则，并新增了配置开关 `new_collations_enabled_on_first_bootstrap`，在集群初次初始化时决定是否启用新排序规则框架。如需启用新排序规则框架，可将 `new_collations_enabled_on_first_bootstrap` 的值设为 `true`，详情参见 [`new_collations_enabled_on_first_bootstrap`](/tidb-configuration-file.md#new_collations_enabled_on_first_bootstrap)。\n\n对于一个已经初始化完成的 TiDB 集群，可以通过 `mysql.tidb` 表中的 `new_collation_enabled` 变量确认是否启用了新排序规则框架。\n\n> **注意：**\n>\n> 当 `mysql.tidb` 表查询结果和 `new_collations_enabled_on_first_bootstrap` 的值不同时，以 `mysql.tidb` 表的结果为准。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT VARIABLE_VALUE FROM mysql.tidb WHERE VARIABLE_NAME='new_collation_enabled';\n```\n\n```sql\n+----------------+\n| VARIABLE_VALUE |\n+----------------+\n| True           |\n+----------------+\n1 row in set (0.00 sec)\n```\n\n在新的排序规则框架下，TiDB 能够支持 `utf8_general_ci`、`utf8mb4_general_ci`、`utf8_unicode_ci`、`utf8mb4_unicode_ci`、`utf8mb4_0900_bin`、`utf8mb4_0900_ai_ci`、`gbk_chinese_ci` 和 `gbk_bin` 这几种排序规则，与 MySQL 兼容。\n\n使用 `utf8_general_ci`、`utf8mb4_general_ci`、`utf8_unicode_ci`、`utf8mb4_unicode_ci`、`utf8mb4_0900_ai_ci` 和 `gbk_chinese_ci` 中任一种时，字符串之间的比较是大小写不敏感 (case-insensitive) 和口音不敏感 (accent-insensitive) 的。同时，TiDB 还修正了排序规则的 `PADDING` 行为：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t(a varchar(20) charset utf8mb4 collate utf8mb4_general_ci PRIMARY KEY);\n```\n\n```sql\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nINSERT INTO t VALUES ('A');\n```\n\n```sql\nQuery OK, 1 row affected (0.00 sec)\n```\n\n```sql\nINSERT INTO t VALUES ('a');\n```\n\n```sql\nERROR 1062 (23000): Duplicate entry 'a' for key 't.PRIMARY'\n```\n\nTiDB 兼容了 MySQL 的 case insensitive collation。\n\n```sql\nINSERT INTO t VALUES ('a ');\n```\n\n```sql\nERROR 1062 (23000): Duplicate entry 'a ' for key 't.PRIMARY'\n```\n\nTiDB 修正了 `PADDING` 行为，与 MySQL 兼容。\n\n> **注意：**\n>\n> TiDB 中 padding 的实现方式与 MySQL 的不同。在 MySQL 中，padding 是通过补齐空格实现的。而在 TiDB 中 padding 是通过裁剪掉末尾的空格来实现的。两种做法在绝大多数情况下是一致的，唯一的例外是字符串尾部包含小于空格 (0x20) 的字符时，例如 `'a' < 'a\\t'` 在 TiDB 中的结果为 `1`，而在 MySQL 中，其等价于 `'a ' < 'a\\t'`，结果为 `0`。\n\n## 表达式中排序规则的 Coercibility 值\n\n如果一个表达式涉及多个不同排序规则的子表达式时，需要对计算时用的排序规则进行推断，规则如下：\n\n+ 显式 `COLLATE` 子句的 coercibility 值为 `0`。\n+ 如果两个字符串的排序规则不兼容，这两个字符串 `concat` 结果的 coercibility 值为 `1`。\n+ 列或者 `CAST()`、`CONVERT()` 和 `BINARY()` 的排序规则的 coercibility 值为 `2`。\n+ 系统常量（`USER()` 或者 `VERSION()` 返回的字符串）的 coercibility 值为 `3`。\n+ 常量的 coercibility 值为 `4`。\n+ 数字或者中间变量的 coercibility 值为 `5`。\n+ `NULL` 或者由 `NULL` 派生出的表达式的 coercibility 值为 `6`。\n\n在推断排序规则时，TiDB 优先使用 coercibility 值较低的表达式的排序规则。如果 coercibility 值相同，则按以下优先级确定排序规则：\n\nbinary > utf8mb4_bin > (utf8mb4_general_ci = utf8mb4_unicode_ci) > utf8_bin > (utf8_general_ci = utf8_unicode_ci) > latin1_bin > ascii_bin\n\n以下情况 TiDB 无法推断排序规则并报错：\n\n- 如果两个子表达式的排序规则不相同，而且表达式的 coercibility 值都为 `0`。\n- 如果两个子表达式的排序规则不兼容，而且表达式的返回类型为 `String` 类。\n\n## `COLLATE` 子句\n\nTiDB 支持使用 `COLLATE` 子句来指定一个表达式的排序规则，该表达式的 coercibility 值为 `0`，具有最高的优先级。示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT 'a' = _utf8mb4 'A' collate utf8mb4_general_ci;\n```\n\n```sql\n+-----------------------------------------------+\n| 'a' = _utf8mb4 'A' collate utf8mb4_general_ci |\n+-----------------------------------------------+\n|                                             1 |\n+-----------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n更多细节，参考 [Connection Character Sets and Collations](https://dev.mysql.com/doc/refman/8.0/en/charset-connection.html)。\n"
        },
        {
          "name": "character-set-gbk.md",
          "type": "blob",
          "size": 6.0771484375,
          "content": "---\ntitle: GBK\nsummary: 本文介绍 TiDB 对 GBK 字符集的支持情况。\n---\n\n# GBK\n\nTiDB 从 v5.4.0 开始支持 GBK 字符集。本文档介绍 TiDB 对 GBK 字符集的支持和兼容情况。\n\n```sql\nSHOW CHARACTER SET WHERE CHARSET = 'gbk';\n+---------+-------------------------------------+-------------------+--------+\n| Charset | Description                         | Default collation | Maxlen |\n+---------+-------------------------------------+-------------------+--------+\n| gbk     | Chinese Internal Code Specification | gbk_bin           |      2 |\n+---------+-------------------------------------+-------------------+--------+\n1 row in set (0.00 sec)\n\nSHOW COLLATION WHERE CHARSET = 'gbk';\n+----------------+---------+------+---------+----------+---------+\n| Collation      | Charset | Id   | Default | Compiled | Sortlen |\n+----------------+---------+------+---------+----------+---------+\n| gbk_bin        | gbk     |   87 |         | Yes      |       1 |\n+----------------+---------+------+---------+----------+---------+\n1 rows in set (0.00 sec)\n```\n\n## 与 MySQL 的兼容性\n\n本节介绍 TiDB 中 GBK 字符集与 MySQL 的兼容情况。\n\n### 排序规则兼容性\n\nMySQL 的字符集默认排序规则是 `gbk_chinese_ci`。与 MySQL 不同，TiDB GBK 字符集的默认排序规则为 `gbk_bin`。另外，TiDB 支持的 `gbk_bin` 与 MySQL 支持的 `gbk_bin` 排序规则也不一致，TiDB 是将 GBK 转换成 UTF8MB4 然后做二进制排序。\n\n如果要使 TiDB 兼容 MySQL 的 GBK 字符集排序规则，你需要在初次初始化 TiDB 集群时设置 TiDB 配置项[`new_collations_enabled_on_first_bootstrap`](/tidb-configuration-file.md#new_collations_enabled_on_first_bootstrap) 为 `true` 来开启[新的排序规则框架](/character-set-and-collation.md#新框架下的排序规则支持)。\n\n开启新的排序规则框架后，如果查看 GBK 字符集对应的排序规则，你可以看到 TiDB GBK 默认排序规则已经切换为 `gbk_chinese_ci`。\n\n```sql\nSHOW CHARACTER SET WHERE CHARSET = 'gbk';\n+---------+-------------------------------------+-------------------+--------+\n| Charset | Description                         | Default collation | Maxlen |\n+---------+-------------------------------------+-------------------+--------+\n| gbk     | Chinese Internal Code Specification | gbk_chinese_ci    |      2 |\n+---------+-------------------------------------+-------------------+--------+\n1 row in set (0.00 sec)\n\nSHOW COLLATION WHERE CHARSET = 'gbk';\n+----------------+---------+------+---------+----------+---------+\n| Collation      | Charset | Id   | Default | Compiled | Sortlen |\n+----------------+---------+------+---------+----------+---------+\n| gbk_bin        | gbk     |   87 |         | Yes      |       1 |\n| gbk_chinese_ci | gbk     |   28 | Yes     | Yes      |       1 |\n+----------------+---------+------+---------+----------+---------+\n2 rows in set (0.00 sec)\n```\n\n### 非法字符兼容性\n\n* 在系统变量 [`character_set_client`](/system-variables.md#character_set_client) 和 [`character_set_connection`](/system-variables.md#character_set_connection) 不同时设置为 `gbk` 的情况下，TiDB 处理非法字符的方式与 MySQL 一致。\n* 在 `character_set_client` 和 `character_set_connection` 同时为 `gbk` 的情况下，TiDB 处理非法字符的方式与 MySQL 有所区别。\n\n    - MySQL 处理非法 GBK 字符集时，对读和写操作的处理方式不同。\n    - TiDB 处理非法 GBK 字符集时，对读和写操作的处理方式相同。TiDB 在严格模式下读写非法 GBK 字符都会报错，在非严格模式下，读写非法 GBK 字符都会用 `?` 替换。\n\n例如，当 `SET NAMES gbk` 时，如果分别在 MySQL 和 TiDB 上通过 `CREATE TABLE gbk_table(a VARCHAR(32) CHARACTER SET gbk)` 语句建表，然后按照下表中的 SQL 语句进行操作，就能看到具体的区别。\n\n| 数据库    |    如果设置的 SQL 模式包含 `STRICT_ALL_TABLES` 或 `STRICT_TRANS_TABLES`                                               | 如果设置的 SQL 模式不包含 `STRICT_ALL_TABLES` 和  `STRICT_TRANS_TABLES`                                                                     |\n|-------|-------------------------------------------------------------------------------------------------------------------|------------------------------------------------------------------------------------------------------------------------------------|\n| MySQL | `SELECT HEX('一a');` <br /> `e4b88061`<br /><br />`INSERT INTO gbk_table values('一a');`<br /> `Incorrect Error`       | `SELECT HEX('一a');` <br /> `e4b88061`<br /><br />`INSERT INTO gbk_table VALUES('一a');`<br />`SELECT HEX(a) FROM gbk_table;`<br /> `e4b8` |\n| TiDB  | `SELECT HEX('一a');` <br /> `Incorrect Error`<br /><br />`INSERT INTO gbk_table VALUES('一a');`<br /> `Incorrect Error` | `SELECT HEX('一a');` <br /> `e4b83f`<br /><br />`INSERT INTO gbk_table VALUES('一a');`<br />`SELECT HEX(a) FROM gbk_table;`<br /> `e4b83f`  |\n\n说明：该表中 `SELECT HEX('一a');` 在 `utf8mb4` 字节集下的结果为 `e4b88061`。\n\n### 其它\n\n* 目前 TiDB 不支持通过 `ALTER TABLE` 语句将其它字符集类型改成 `gbk` 或者从 `gbk` 转成其它字符集类型。\n\n* TiDB 不支持使用 `_gbk`，比如：\n\n  ```sql\n  CREATE TABLE t(a CHAR(10) CHARSET BINARY);\n  Query OK, 0 rows affected (0.00 sec)\n  INSERT INTO t VALUES (_gbk'啊');\n  ERROR 1115 (42000): Unsupported character introducer: 'gbk'\n  ```\n\n* 对于 `ENUM` 和 `SET` 类型中的二进制字符，TiDB 目前都会将其作为 `utf8mb4` 字符集处理。\n\n## 组件兼容性\n\n* TiFlash 目前不支持 GBK 字符集。\n\n* TiDB Data Migration (DM) 在 v5.4.0 之前不支持将 `charset=GBK` 的表迁移到 TiDB。\n\n* TiDB Lightning 在 v5.4.0 之前不支持导入 `charset=GBK` 的表。\n\n* TiCDC 在 v6.1.0 之前不支持同步 `charset=GBK` 的表。另外，任何版本的 TiCDC 都不支持同步 `charset=GBK` 的表到 6.1.0 之前的 TiDB 集群。\n\n* TiDB Backup & Restore（BR）在 v5.4.0 之前不支持恢复 `charset=GBK` 的表。另外，任何版本的 BR 都不支持恢复 `charset=GBK` 的表到 5.4.0 之前的 TiDB 集群。"
        },
        {
          "name": "check-before-deployment.md",
          "type": "blob",
          "size": 27.1650390625,
          "content": "---\ntitle: TiDB 环境与系统配置检查\nsummary: 了解部署 TiDB 前的环境检查操作。\naliases: ['/docs-cn/dev/check-before-deployment/']\n---\n\n# TiDB 环境与系统配置检查\n\n本文介绍部署 TiDB 前的环境检查操作，以下各项操作按优先级排序。\n\n## 在 TiKV 部署目标机器上添加数据盘 EXT4 文件系统挂载参数\n\n生产环境部署，建议使用 EXT4 类型文件系统的 NVMe 类型的 SSD 磁盘存储 TiKV 数据文件。这个配置方案为最佳实施方案，其可靠性、安全性、稳定性已经在大量线上场景中得到证实。\n\n使用 `root` 用户登录目标机器，将部署目标机器数据盘格式化成 ext4 文件系统，挂载时添加 `nodelalloc` 和 `noatime` 挂载参数。`nodelalloc` 是必选参数，否则 TiUP 安装时检测无法通过；`noatime` 是可选建议参数。\n\n> **注意：**\n>\n> 如果你的数据盘已经格式化成 ext4 并挂载了磁盘，可先执行 `umount /dev/nvme0n1p1` 命令卸载，从编辑 `/etc/fstab` 文件步骤开始执行，添加挂载参数重新挂载即可。\n\n以 `/dev/nvme0n1` 数据盘为例，具体操作步骤如下：\n\n1. 查看数据盘。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    fdisk -l\n    ```\n\n    ```\n    Disk /dev/nvme0n1: 1000 GB\n    ```\n\n2. 创建分区。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    parted -s -a optimal /dev/nvme0n1 mklabel gpt -- mkpart primary ext4 1 -1\n    ```\n\n    如果 NVMe 设备容量较大，可以创建多个分区。\n\n    ```bash\n    parted -s -a optimal /dev/nvme0n1 mklabel gpt -- mkpart primary ext4 1 2000GB\n    parted -s -a optimal /dev/nvme0n1 -- mkpart primary ext4 2000GB -1\n    ```\n\n    > **注意：**\n    >\n    > 使用 `lsblk` 命令查看分区的设备号：对于 NVMe 磁盘，生成的分区设备号一般为 `nvme0n1p1`；对于普通磁盘（例如 `/dev/sdb`），生成的分区设备号一般为 `sdb1`。\n\n3. 格式化文件系统。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    mkfs.ext4 /dev/nvme0n1p1\n    ```\n\n4. 查看数据盘分区 UUID。\n\n    本例中 `nvme0n1p1` 的 UUID 为 `c51eb23b-195c-4061-92a9-3fad812cc12f`。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    lsblk -f\n    ```\n\n    ```\n    NAME    FSTYPE LABEL UUID                                 MOUNTPOINT\n    sda\n    ├─sda1  ext4         237b634b-a565-477b-8371-6dff0c41f5ab /boot\n    ├─sda2  swap         f414c5c0-f823-4bb1-8fdf-e531173a72ed\n    └─sda3  ext4         547909c1-398d-4696-94c6-03e43e317b60 /\n    sr0\n    nvme0n1\n    └─nvme0n1p1 ext4         c51eb23b-195c-4061-92a9-3fad812cc12f\n    ```\n\n5. 编辑 `/etc/fstab` 文件，添加 `nodelalloc` 挂载参数。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    vi /etc/fstab\n    ```\n\n    ```\n    UUID=c51eb23b-195c-4061-92a9-3fad812cc12f /data1 ext4 defaults,nodelalloc,noatime 0 2\n    ```\n\n6. 挂载数据盘。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    mkdir /data1 && \\\n    systemctl daemon-reload && \\\n    mount -a\n    ```\n\n7. 执行以下命令，如果文件系统为 ext4，并且挂载参数中包含 `nodelalloc`，则表示已生效。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    mount -t ext4\n    ```\n\n    ```\n    /dev/nvme0n1p1 on /data1 type ext4 (rw,noatime,nodelalloc,data=ordered)\n    ```\n\n## 检测及关闭系统 swap\n\nTiDB 需要充足的内存来运行。如果 TiDB 使用的内存被换出 (swapped out) 然后再换入 (swapped back in)，这可能会导致延迟激增。如果您想保持稳定的性能，建议永久禁用系统 swap，但可能在内存偏小时触发 OOM 问题。如果想避免此类 OOM 问题，则可只将 swap 优先级调低，但不做永久关闭。\n\n- 开启并使用 swap 可能会引入性能抖动问题，对于低延迟、稳定性要求高的数据库服务，建议永久关闭操作系统层 swap。要永久关闭 swap，可使用以下方法：\n\n    - 在操作系统初始化阶段，不单独划分 swap 分区盘。\n    - 如果在操作系统初始化阶段，已经单独划分了 swap 分区盘，并且启用了 swap，则使用以下命令进行关闭：\n\n        ```bash\n        echo \"vm.swappiness = 0\">> /etc/sysctl.conf\n        sysctl -p\n        swapoff -a && swapon -a\n        ```\n\n- 如果主机内存偏小，关闭系统 swap 可能会更容易触发 OOM 问题，可参考以如下方法将 swap 优先级调低，但不做永久关闭：\n\n    ```bash\n    echo \"vm.swappiness = 0\">> /etc/sysctl.conf\n    sysctl -p\n    ```\n\n## 设置 TiDB 节点的临时空间（推荐）\n\nTiDB 的部分操作需要向服务器写入临时文件，因此需要确保运行 TiDB 的操作系统用户具有足够的权限对目标目录进行读写。如果 TiDB 实例不是以 `root` 权限启动，则需要检查目录权限并进行正确设置。\n\n- TiDB 临时工作区\n\n    哈希表构建、排序等内存消耗较大的操作可能会向磁盘写入临时数据，用来减少内存消耗，提升稳定性。写入的磁盘位置由配置项 [`tmp-storage-path`](/tidb-configuration-file.md#tmp-storage-path) 定义。在默认设置下，确保运行 TiDB 的用户对操作系统临时文件夹（通常为 `/tmp`）有读写权限。\n\n- Fast Online DDL 工作区\n\n    当变量 [`tidb_ddl_enable_fast_reorg`](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入) 被设置为 `ON`（v6.5.0 及以上版本中默认值为 `ON`）时，会激活 Fast Online DDL，这时部分 DDL 要对临时文件进行读写。临时文件位置由配置 [`temp-dir`](/tidb-configuration-file.md#temp-dir-从-v630-版本开始引入) 定义，需要确保运行 TiDB 的用户对操作系统中该目录有读写权限。默认目录 `/tmp/tidb` 使用 tmpfs (temporary file system)，建议显式指定为磁盘上的目录，以 `/data/tidb-deploy/tempdir` 为例：\n\n    > **注意：**\n    >\n    > 如果业务中可能存在针对大对象的 DDL 操作，推荐为 [`temp-dir`](/tidb-configuration-file.md#temp-dir-从-v630-版本开始引入) 配置独立文件系统及更大的临时空间。\n\n    ```shell\n    sudo mkdir -p /data/tidb-deploy/tempdir\n    ```\n\n    如果目录 `/data/tidb-deploy/tempdir` 已经存在，需确保有写入权限。\n\n    ```shell\n    sudo chmod -R 777 /data/tidb-deploy/tempdir\n    ```\n\n    > **注意：**\n    >\n    > 如果目录不存在，TiDB 在启动时会自动创建该目录。如果目录创建失败，或者 TiDB 对该目录没有读写权限，[Fast Online DDL](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入) 在运行时会被禁用。\n\n## 检测及关闭目标部署机器的防火墙\n\n本段介绍如何关闭目标主机防火墙配置，因为在 TiDB 集群中，需要将节点间的访问端口打通才可以保证读写请求、数据心跳等信息的正常的传输。在普遍线上场景中，数据库到业务服务和数据库节点的网络联通都是在安全域内完成数据交互。如果没有特殊安全的要求，建议将目标节点的防火墙进行关闭。否则建议[按照端口使用规则](/hardware-and-software-requirements.md#网络要求)，将端口信息配置到防火墙服务的白名单中。\n\n1. 检查防火墙状态（以 CentOS Linux release 7.7.1908 (Core) 为例）\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    sudo firewall-cmd --state\n    sudo systemctl status firewalld.service\n    ```\n\n2. 关闭防火墙服务\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo systemctl stop firewalld.service\n    ```\n\n3. 关闭防火墙自动启动服务\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo systemctl disable firewalld.service\n    ```\n\n4. 检查防火墙状态\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo systemctl status firewalld.service\n    ```\n\n## 检测及安装 NTP 服务\n\nTiDB 是一套分布式数据库系统，需要节点间保证时间的同步，从而确保 ACID 模型的事务线性一致性。目前解决授时的普遍方案是采用 NTP 服务，可以通过互联网中的 `pool.ntp.org` 授时服务来保证节点的时间同步，也可以使用离线环境自己搭建的 NTP 服务来解决授时。\n\n采用如下步骤检查是否安装 NTP 服务以及与 NTP 服务器正常同步：\n\n1. 执行以下命令，如果输出 `running` 表示 NTP 服务正在运行：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo systemctl status ntpd.service\n    ```\n\n    ```\n    ntpd.service - Network Time Service\n    Loaded: loaded (/usr/lib/systemd/system/ntpd.service; disabled; vendor preset: disabled)\n    Active: active (running) since 一 2017-12-18 13:13:19 CST; 3s ago\n    ```\n\n    - 若返回报错信息 `Unit ntpd.service could not be found.`，请尝试执行以下命令，以查看与 NTP 进行时钟同步所使用的系统配置是 `chronyd` 还是 `ntpd`：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```bash\n        sudo systemctl status chronyd.service\n        ```\n\n        ```\n        chronyd.service - NTP client/server\n        Loaded: loaded (/usr/lib/systemd/system/chronyd.service; enabled; vendor preset: enabled)\n        Active: active (running) since Mon 2021-04-05 09:55:29 EDT; 3 days ago\n        ```\n\n      若发现系统既没有配置 `chronyd` 也没有配置 `ntpd`，则表示系统尚未安装任一服务。此时，应先安装其中一个服务，并保证它可以自动启动，默认使用 `ntpd`。\n\n        如果你使用的系统配置是 `chronyd`，请直接执行步骤 3。\n\n2. 执行 `ntpstat` 命令检测是否与 NTP 服务器同步：\n\n    > **注意：**\n    >\n    > Ubuntu 系统需安装 `ntpstat` 软件包。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    ntpstat\n    ```\n\n    - 如果输出 `synchronised to NTP server`，表示正在与 NTP 服务器正常同步：\n\n        ```\n        synchronised to NTP server (85.199.214.101) at stratum 2\n        time correct to within 91 ms\n        polling server every 1024 s\n        ```\n\n    - 以下情况表示 NTP 服务未正常同步：\n\n        ```\n        unsynchronised\n        ```\n\n    - 以下情况表示 NTP 服务未正常运行：\n\n        ```\n        Unable to talk to NTP daemon. Is it running?\n        ```\n\n3. 执行 `chronyc tracking` 命令查看 Chrony 服务是否与 NTP 服务器同步。\n\n    > **注意：**\n    >\n    > 该操作仅适用于使用 Chrony 的系统，不适用于使用 NTPd 的系统。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    chronyc tracking\n    ```\n\n    - 如果该命令返回结果为 `Leap status : Normal`，则代表同步过程正常。\n\n        ```\n        Reference ID    : 5EC69F0A (ntp1.time.nl)\n        Stratum         : 2\n        Ref time (UTC)  : Thu May 20 15:19:08 2021\n        System time     : 0.000022151 seconds slow of NTP time\n        Last offset     : -0.000041040 seconds\n        RMS offset      : 0.000053422 seconds\n        Frequency       : 2.286 ppm slow\n        Residual freq   : -0.000 ppm\n        Skew            : 0.012 ppm\n        Root delay      : 0.012706812 seconds\n        Root dispersion : 0.000430042 seconds\n        Update interval : 1029.8 seconds\n        Leap status     : Normal\n        ```\n\n    - 如果该命令返回结果如下，则表示同步过程出错：\n\n        ```\n        Leap status    : Not synchronised\n        ```\n\n    - 如果该命令返回结果如下，则表示 Chrony 服务未正常运行：\n\n        ```\n        506 Cannot talk to daemon\n        ```\n\n如果要使 NTP 服务尽快开始同步，执行以下命令。可以将 `pool.ntp.org` 替换为你的 NTP 服务器：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nsudo systemctl stop ntpd.service && \\\nsudo ntpdate pool.ntp.org && \\\nsudo systemctl start ntpd.service\n```\n\n如果要在 CentOS 7 系统上手动安装 NTP 服务，可执行以下命令：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nsudo yum install ntp ntpdate && \\\nsudo systemctl start ntpd.service && \\\nsudo systemctl enable ntpd.service\n```\n\n## 检查和配置操作系统优化参数\n\n在生产系统的 TiDB 中，建议对操作系统进行如下的配置优化：\n\n1. 关闭透明大页（即 Transparent Huge Pages，缩写为 THP）。数据库的内存访问模式往往是稀疏的而非连续的。当高阶内存碎片化比较严重时，分配 THP 页面会出现较高的延迟。\n2. 设置存储介质的 I/O 调度器。\n\n    - 对于高速 SSD 存储介质，内核默认的 I/O 调度器可能会导致性能损失。建议将闪存存储的 I/O 调度器设置为先入先出 (First-in-first-out, FIFO) 的调度器，如 `noop` 或 `none`，这样内核将不做调度操作，直接将 I/O 请求传递给硬件，从而提升性能。\n    - 对于 NVMe 存储介质，默认的 I/O 调度器为 `none`，无需进行调整。\n\n3. 为调整 CPU 频率的 cpufreq 模块选用 performance 模式。将 CPU 频率固定在其支持的最高运行频率上，不进行动态调节，可获取最佳的性能。\n\n采用如下步骤检查操作系统的当前配置，并配置系统优化参数：\n\n1. 执行以下命令查看透明大页的开启状态。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    cat /sys/kernel/mm/transparent_hugepage/enabled\n    ```\n\n    ```\n    [always] madvise never\n    ```\n\n    > **注意：**\n    >\n    > `[always] madvise never` 表示透明大页处于启用状态，需要关闭。\n\n2. 执行以下命令查看数据目录所在磁盘的 I/O 调度器。\n\n    如果数据目录所在磁盘使用的是 SD 或 VD 设备，可以执行以下命令查看当前 I/O 调度器的配置：\n\n    ```bash\n    cat /sys/block/sd[bc]/queue/scheduler\n    ```\n\n    ```\n    noop [deadline] cfq\n    noop [deadline] cfq\n    ```\n\n    > **注意：**\n    >\n    > `noop [deadline] cfq` 表示磁盘的 I/O 调度器使用 `deadline`，需要进行修改。\n\n    如果数据目录使用 NVMe 设备，可以执行以下命令查看 I/O 调度器：\n\n    ```bash\n    cat /sys/block/nvme[01]*/queue/scheduler\n    ```\n\n    ```\n    [none] mq-deadline kyber bfq\n    [none] mq-deadline kyber bfq\n    ```\n\n    > **注意：**\n    >\n    > `[none] mq-deadline kyber bfq` 表示 NVMe 设备的 I/O 调度器使用 `none`，不需要进行修改。\n\n3. 执行以下命令查看磁盘的唯一标识 `ID_SERIAL`。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    udevadm info --name=/dev/sdb | grep ID_SERIAL\n    ```\n\n    ```\n    E: ID_SERIAL=36d0946606d79f90025f3e09a0c1f9e81\n    E: ID_SERIAL_SHORT=6d0946606d79f90025f3e09a0c1f9e81\n    ```\n\n    > **注意：**\n    >\n    > - 如果多个磁盘都分配了数据目录，需要为每个磁盘都执行以上命令，记录所有磁盘各自的唯一标识。\n    > - 已经使用 `noop` 或者 `none` 调度器的设备不需要记录标识，无需配置 udev 规则和 tuned 策略中的相关内容。\n\n4. 执行以下命令查看 cpufreq 模块选用的节能策略。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    cpupower frequency-info --policy\n    ```\n\n    ```\n    analyzing CPU 0:\n    current policy: frequency should be within 1.20 GHz and 3.10 GHz.\n                  The governor \"powersave\" may decide which speed to use within this range.\n    ```\n\n    > **注意：**\n    >\n    > `The governor \"powersave\"` 表示 cpufreq 的节能策略使用 powersave，需要调整为 performance 策略。如果是虚拟机或者云主机，则不需要调整，命令输出通常为 `Unable to determine current policy`。\n\n5. 配置系统优化参数\n\n    + 方法一：使用 tuned（推荐）\n\n        1. 执行 `tuned-adm list` 命令查看当前操作系统的 tuned 策略。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            tuned-adm list\n            ```\n\n            ```\n            Available profiles:\n            - balanced                    - General non-specialized tuned profile\n            - desktop                     - Optimize for the desktop use-case\n            - hpc-compute                 - Optimize for HPC compute workloads\n            - latency-performance         - Optimize for deterministic performance at the cost of increased power consumption\n            - network-latency             - Optimize for deterministic performance at the cost of increased power consumption, focused on low latency network performance\n            - network-throughput          - Optimize for streaming network throughput, generally only necessary on older CPUs or 40G+ networks\n            - powersave                   - Optimize for low power consumption\n            - throughput-performance      - Broadly applicable tuning that provides excellent performance across a variety of common server workloads\n            - virtual-guest               - Optimize for running inside a virtual guest\n            - virtual-host                - Optimize for running KVM guests\n            Current active profile: balanced\n            ```\n\n            `Current active profile: balanced` 表示当前操作系统的 tuned 策略使用 balanced，建议在当前策略的基础上添加操作系统优化配置。\n\n        2. 创建新的 tuned 策略。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            mkdir /etc/tuned/balanced-tidb-optimal/\n            vi /etc/tuned/balanced-tidb-optimal/tuned.conf\n            ```\n\n            ```\n            [main]\n            include=balanced\n\n            [cpu]\n            governor=performance\n\n            [vm]\n            transparent_hugepages=never\n\n            [disk]\n            devices_udev_regex=(ID_SERIAL=36d0946606d79f90025f3e09a0c1fc035)|(ID_SERIAL=36d0946606d79f90025f3e09a0c1f9e81)\n            elevator=noop\n            ```\n\n            `include=balanced` 表示在现有的 balanced 策略基础上添加操作系统优化配置。\n\n        3. 应用新的 tuned 策略。\n\n            > **注意：**\n            >\n            > 如果已经使用 `noop` 或 `none` I/O 调度器，则无需在 tuned 策略中配置调度器相关的内容，可以跳过此步骤。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            tuned-adm profile balanced-tidb-optimal\n            ```\n\n    + 方法二：使用脚本方式。如果已经使用 tuned 方法，请跳过本方法。\n\n        1. 执行 `grubby` 命令查看默认内核版本。\n\n            > **注意：**\n            >\n            > 需安装 `grubby` 软件包。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            grubby --default-kernel\n            ```\n\n            ```bash\n            /boot/vmlinuz-3.10.0-957.el7.x86_64\n            ```\n\n        2. 执行 `grubby --update-kernel` 命令修改内核配置。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            grubby --args=\"transparent_hugepage=never\" --update-kernel `grubby --default-kernel`\n            ```\n\n            > **注意：**\n            >\n            > 你也可以在 `--update-kernel` 后指定实际的版本号，例如：`--update-kernel /boot/vmlinuz-3.10.0-957.el7.x86_64`。\n\n        3. 执行 `grubby --info` 命令查看修改后的默认内核配置。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            grubby --info /boot/vmlinuz-3.10.0-957.el7.x86_64\n            ```\n\n            > **注意：**\n            >\n            > `--info` 后需要使用实际的默认内核版本。\n\n            ```\n            index=0\n            kernel=/boot/vmlinuz-3.10.0-957.el7.x86_64\n            args=\"ro crashkernel=auto rd.lvm.lv=centos/root rd.lvm.lv=centos/swap rhgb quiet LANG=en_US.UTF-8 transparent_hugepage=never\"\n            root=/dev/mapper/centos-root\n            initrd=/boot/initramfs-3.10.0-957.el7.x86_64.img\n            title=CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core)\n            ```\n\n        4. 修改当前的内核配置立即关闭透明大页。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            echo never > /sys/kernel/mm/transparent_hugepage/enabled\n            echo never > /sys/kernel/mm/transparent_hugepage/defrag\n            ```\n\n        5. 配置 udev 脚本应用 IO 调度器策略。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            vi /etc/udev/rules.d/60-tidb-schedulers.rules\n            ```\n\n            ```\n            ACTION==\"add|change\", SUBSYSTEM==\"block\", ENV{ID_SERIAL}==\"36d0946606d79f90025f3e09a0c1fc035\", ATTR{queue/scheduler}=\"noop\"\n            ACTION==\"add|change\", SUBSYSTEM==\"block\", ENV{ID_SERIAL}==\"36d0946606d79f90025f3e09a0c1f9e81\", ATTR{queue/scheduler}=\"noop\"\n\n            ```\n\n        6. 应用 udev 脚本。\n\n            > **注意：**\n            >\n            > 对于已经使用 `noop` 或 `none` I/O 调度器的设备，无需配置 udev 规则，可以跳过此步骤。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            udevadm control --reload-rules\n            udevadm trigger --type=devices --action=change\n            ```\n\n        7. 创建 CPU 节能策略配置服务。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            cat  >> /etc/systemd/system/cpupower.service << EOF\n            [Unit]\n            Description=CPU performance\n            [Service]\n            Type=oneshot\n            ExecStart=/usr/bin/cpupower frequency-set --governor performance\n            [Install]\n            WantedBy=multi-user.target\n            EOF\n            ```\n\n        8. 应用 CPU 节能策略配置服务。\n\n            {{< copyable \"shell-regular\" >}}\n\n            ```bash\n            systemctl daemon-reload\n            systemctl enable cpupower.service\n            systemctl start cpupower.service\n            ```\n\n6. 执行以下命令验证透明大页的状态。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    cat /sys/kernel/mm/transparent_hugepage/enabled\n    ```\n\n    ```\n    always madvise [never]\n    ```\n\n7. 执行以下命令验证数据目录所在磁盘的 I/O 调度器。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    cat /sys/block/sd[bc]/queue/scheduler\n    ```\n\n    ```\n    [noop] deadline cfq\n    [noop] deadline cfq\n    ```\n\n8. 执行以下命令查看 cpufreq 模块选用的节能策略。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    cpupower frequency-info --policy\n    ```\n\n    ```\n    analyzing CPU 0:\n    current policy: frequency should be within 1.20 GHz and 3.10 GHz.\n                  The governor \"performance\" may decide which speed to use within this range.\n    ```\n\n9. 执行以下命令修改 sysctl 参数。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    echo \"fs.file-max = 1000000\">> /etc/sysctl.conf\n    echo \"net.core.somaxconn = 32768\">> /etc/sysctl.conf\n    echo \"net.ipv4.tcp_tw_recycle = 0\">> /etc/sysctl.conf\n    echo \"net.ipv4.tcp_syncookies = 0\">> /etc/sysctl.conf\n    echo \"vm.overcommit_memory = 1\">> /etc/sysctl.conf\n    echo \"vm.min_free_kbytes = 1048576\">> /etc/sysctl.conf\n    sysctl -p\n    ```\n\n    > **注意：**\n    >\n    > - `vm.min_free_kbytes` 是 Linux 内核的一个参数，用于控制系统预留的最小空闲内存量，单位为 KiB。\n    > - `vm.min_free_kbytes` 的设置会影响内存回收机制。设置得过大，会导致可用内存变少，设置得过小，可能会导致内存的申请速度超过后台的回收速度，进而导致内存回收并引起内存分配延迟。\n    > - 建议将 `vm.min_free_kbytes` 最小设置为 `1048576` KiB（即 1 GiB）。如果[安装了 NUMA](/check-before-deployment.md#安装-numactl-工具)，建议设置为 `NUMA 节点个数 * 1048576` KiB。\n    > - 对于内存小于 16 GiB 的小规格服务器，保持 `vm.min_free_kbytes` 的默认值即可。\n    > - `tcp_tw_recycle` 从 Linux 4.12 内核版本开始移除，在使用高版本内核时无需配置该项。\n\n10. 执行以下命令配置用户的 limits.conf 文件。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    cat << EOF >>/etc/security/limits.conf\n    tidb           soft    nofile          1000000\n    tidb           hard    nofile          1000000\n    tidb           soft    stack          32768\n    tidb           hard    stack          32768\n    EOF\n    ```\n\n## 手动配置 SSH 互信及 sudo 免密码\n\n对于有需求，通过手动配置中控机至目标节点互信的场景，可参考本段。通常推荐使用 TiUP 部署工具会自动配置 SSH 互信及免密登录，可忽略本段内容。\n\n1. 以 `root` 用户依次登录到部署目标机器创建 `tidb` 用户并设置登录密码。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    useradd tidb && \\\n    passwd tidb\n    ```\n\n2. 执行以下命令，将 `tidb ALL=(ALL) NOPASSWD: ALL` 添加到文件末尾，即配置好 sudo 免密码。\n\n    {{< copyable \"shell-root\" >}}\n\n    ```bash\n    visudo\n    ```\n\n    ```\n    tidb ALL=(ALL) NOPASSWD: ALL\n    ```\n\n3. 以 `tidb` 用户登录到中控机，执行以下命令。将 `10.0.1.1` 替换成你的部署目标机器 IP，按提示输入部署目标机器 `tidb` 用户密码，执行成功后即创建好 SSH 互信，其他机器同理。新建的 `tidb` 用户下没有 `.ssh` 目录，需要执行生成 rsa 密钥的命令来生成 `.ssh` 目录。如果要在中控机上部署 TiDB 组件，需要为中控机和中控机自身配置互信。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    ssh-keygen -t rsa\n    ssh-copy-id -i ~/.ssh/id_rsa.pub 10.0.1.1\n    ```\n\n4. 以 `tidb` 用户登录中控机，通过 `ssh` 的方式登录目标机器 IP。如果不需要输入密码并登录成功，即表示 SSH 互信配置成功。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    ssh 10.0.1.1\n    ```\n\n    ```\n    [tidb@10.0.1.1 ~]$\n    ```\n\n5. 以 `tidb` 用户登录到部署目标机器后，执行以下命令，不需要输入密码并切换到 `root` 用户，表示 `tidb` 用户 sudo 免密码配置成功。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    sudo -su root\n    ```\n\n    ```\n    [root@10.0.1.1 tidb]#\n    ```\n\n## 安装 numactl 工具\n\n本段主要介绍如何安装 NUMA 工具。在生产环境中，因为硬件机器配置往往高于需求，为了更合理规划资源，会考虑单机多实例部署 TiDB 或者 TiKV。NUMA 绑核工具的使用，主要为了防止 CPU 资源的争抢，引发性能衰退。\n\n> **注意：**\n>\n> - NUMA 绑核是用来隔离 CPU 资源的一种方法，适合高配置物理机环境部署多实例使用。\n> - 通过 `tiup cluster deploy` 完成部署操作，就可以通过 `exec` 命令来进行集群级别管理工作。\n\n安装 NUMA 工具有两种方法：\n\n方法 1：登录到目标节点进行安装（以 CentOS Linux release 7.7.1908 (Core) 为例）。\n\n```bash\nsudo yum -y install numactl\n```\n\n方法 2：通过 `tiup cluster exec` 在集群上批量安装 NUMA。\n\n1. 使用 TiUP 安装 TiDB 集群，参考[使用 TiUP 部署 TiDB 集群](/production-deployment-using-tiup.md)完成 `tidb-test` 集群的部署。如果本地已有集群，可跳过这一步。\n\n    ```bash\n    tiup cluster deploy tidb-test v6.1.0 ./topology.yaml --user root [-p] [-i /home/root/.ssh/gcp_rsa]\n    ```\n\n2. 执行 `tiup cluster exec` 命令，以 `sudo` 权限在 `tidb-test` 集群所有目标主机上安装 NUMA。\n\n    ```bash\n    tiup cluster exec tidb-test --sudo --command \"yum -y install numactl\"\n    ```\n\n    你可以执行 `tiup cluster exec --help` 查看的 `tiup cluster exec` 命令的说明信息。\n\n## 关闭 SELinux\n\n使用 [getenforce(8)](https://linux.die.net/man/8/getenforce) 工具检查是否已禁用 SELinux 或设置为宽容模式 (Permissive)。处于强制模式 (Enforcing) 的 SELinux 可能会导致部署失败。有关禁用 SELinux 的说明，请参阅操作系统的文档。\n"
        },
        {
          "name": "choose-index.md",
          "type": "blob",
          "size": 85.4248046875,
          "content": "---\ntitle: 索引的选择\nsummary: 介绍 TiDB 如何选择索引去读入数据，以及相关的一些控制索引选择的方式。\naliases: ['/docs-cn/dev/choose-index/']\n---\n\n# 索引的选择\n\n从存储层读取数据是 SQL 计算过程中最为耗时的部分之一，TiDB 目前支持从不同的存储和不同的索引中读取数据，索引选择得是否合理将很大程度上决定一个查询的运行速度。\n\n本章节将介绍 TiDB 如何选择索引去读入数据，以及相关的一些控制索引选择的方式。\n\n## 读表\n\n在介绍索引的选择之前，首先要了解 TiDB 有哪些读表的方式，这些方式的触发条件是什么，不同方式有什么区别，各有什么优劣。\n\n### 读表算子\n\n| 读表算子 | 触发条件 | 适用场景 | 说明 |\n| :------- | :------- | :------- | :---- |\n| PointGet/BatchPointGet | 读表的范围是一个或多个单点范围 | 任何场景 | 如果能被触发，通常被认为是最快的算子，因为其直接调用 kvget 的接口进行计算，不走 coprocessor  |\n| TableReader | 无 | 任何场景 | 该 TableReader 算子用于 TiKV。从 TiKV 端直接扫描表数据，一般被认为是效率最低的算子，除非在 `_tidb_rowid` 这一列上存在范围查询，或者无其他可以选择的读表算子时，才会选择这个算子  |\n| TableReader | 表在 TiFlash 节点上存在副本 | 需要读取的列比较少，但是需要计算的行很多 | 该 TableReader 算子用于 TiFlash。TiFlash 是列式存储，如果需要对少量的列和大量的行进行计算，一般会选择这个算子 |\n| IndexReader | 表有一个或多个索引，且计算所需的列被包含在索引里 | 存在较小的索引上的范围查询，或者对索引列有顺序需求的时候 | 当存在多个索引的时候，会根据估算代价选择合理的索引 |\n| IndexLookupReader | 表有一个或多个索引，且计算所需的列**不完全**被包含在索引里 | 同 IndexReader | 因为计算列不完全被包含在索引里，所以读完索引后需要回表，这里会比 IndexReader 多一些开销 |\n| IndexMerge | 表有多个索引或多值索引 | 使用多值索引或同时使用多个索引的时候 | 可以通过 [optimizer hints](/optimizer-hints.md) 指定使用该算子，或让优化器根据代价估算自动选择该算子，参见[用 EXPLAIN 查看索引合并的 SQL 执行计划](/explain-index-merge.md) |\n\n> **注意：**\n> \n> TableReader 是基于 `_tidb_rowid` 的索引，TiFlash 是列存索引，所以索引的选择即是读表算子的选择。\n\n## 索引的选择\n\nTiDB 基于规则或基于代价来选择索引。基于的规则包括前置规则和 Skyline-Pruning。在选择索引时，TiDB 会先尝试前置规则。如果存在索引满足某一条前置规则，则直接选择该索引。否则，TiDB 会采用 Skyline-Pruning 来排除不合适的索引，然后基于每个读表算子的代价估算，选择代价最小的索引。\n\n### 基于规则选择\n\n#### 前置规则\n\nTiDB 采用如下的启发式前置规则来选择索引：\n\n+ 规则 1：如果存在索引满足“唯一性索引全匹配 + 不需要回表（即该索引生成的计划是 IndexReader）”时，直接选择该索引。\n\n+ 规则 2：如果存在索引满足“唯一性索引全匹配 + 需要回表（即该索引生成的计划是 IndexLookupReader）”时，选择满足该条件且回表行数最小的索引作为候选索引。\n\n+ 规则 3：如果存在索引满足“普通索引不需要回表 + 读取行数小于一定阈值”时，选择满足该条件且读取行数最小的索引作为候选索引。\n\n+ 规则 4：如果规则 2 和 3 之中仅选出一条候选索引，则选择该候选索引。如果规则 2 和 3 均选出候选索引，则选择读取行数（读索引行数 + 回表行数）较小的索引。\n\n上述规则中的“索引全匹配”指每个索引列上均存在等值条件。在执行 `EXPLAIN FORMAT = 'verbose' ...` 语句时，如果前置规则匹配了某一索引，TiDB 会输出一条 NOTE 级别的 warning 提示该索引匹配了前置规则。\n\n在以下示例中，因为索引 `idx_b` 满足规则 2 中“唯一性索引全匹配 + 需要回表”的条件，TiDB 选择索引 `idx_b` 作为访问路径，`SHOW WARNING` 返回了索引 `idx_b` 命中前置规则的提示。\n\n```sql\nmysql> CREATE TABLE t(a INT PRIMARY KEY, b INT, c INT, UNIQUE INDEX idx_b(b));\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> EXPLAIN FORMAT = 'verbose' SELECT b, c FROM t WHERE b = 3 OR b = 6;\n+-------------------+---------+---------+------+-------------------------+------------------------------+\n| id                | estRows | estCost | task | access object           | operator info                |\n+-------------------+---------+---------+------+-------------------------+------------------------------+\n| Batch_Point_Get_5 | 2.00    | 8.80    | root | table:t, index:idx_b(b) | keep order:false, desc:false |\n+-------------------+---------+---------+------+-------------------------+------------------------------+\n1 row in set, 1 warning (0.00 sec)\n\nmysql> SHOW WARNINGS;\n+-------+------+-------------------------------------------------------------------------------------------+\n| Level | Code | Message                                                                                   |\n+-------+------+-------------------------------------------------------------------------------------------+\n| Note  | 1105 | unique index idx_b of t is selected since the path only has point ranges with double scan |\n+-------+------+-------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n#### Skyline-Pruning\n\nSkyline-Pruning 是一个针对索引的启发式过滤规则，能降低错误估算导致选错索引的概率。Skyline-Pruning 从以下维度衡量一个索引的优劣：\n\n- 索引的列涵盖了多少访问条件。“访问条件”指的是可以转化为某列范围的 `where` 条件，如果某个索引的列集合涵盖的访问条件越多，那么它在这个维度上更优。\n\n- 选择该索引读表时，是否需要回表（即该索引生成的计划是 IndexReader 还是 IndexLookupReader）。不用回表的索引在这个维度上优于需要回表的索引。如果均需要回表，则比较索引的列涵盖了多少过滤条件。过滤条件指的是可以根据索引判断的 `where` 条件。如果某个索引的列集合涵盖的访问条件越多，则回表数量越少，那么它在这个维度上越优。\n\n- 选择该索引是否能满足一定的顺序。因为索引的读取可以保证某些列集合的顺序，所以满足查询要求顺序的索引在这个维度上优于不满足的索引。\n\n- 该索引是否为[全局索引](/partitioned-table.md#全局索引)。在分区表中，全局索引相比普通索引能有效的降低一个 SQL 的 cop task 数量，进而提升整体性能。\n\n对于上述维度，如果索引 `idx_a` 在这四个维度上都不比 `idx_b` 差，且有一个维度比 `idx_b` 好，那么 TiDB 会优先选择 `idx_a`。在执行 `EXPLAIN FORMAT = 'verbose' ...` 语句时，如果 Skyline-Pruning 排除了某些索引，TiDB 会输出一条 NOTE 级别的 warning 提示哪些索引在 Skyline-Pruning 排除之后保留下来。\n\n在以下示例中，索引 `idx_b` 和 `idx_e` 均劣于 `idx_b_c`，因而被 Skyline-Pruning 排除，`SHOW WARNING` 的返回结果显示了经过 Skyline-Pruning 后剩余的索引。\n\n```sql\nmysql> CREATE TABLE t(a INT PRIMARY KEY, b INT, c INT, d INT, e INT, INDEX idx_b(b), INDEX idx_b_c(b, c), INDEX idx_e(e));\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> EXPLAIN FORMAT = 'verbose' SELECT * FROM t WHERE b = 2 AND c > 4;\n+-------------------------------+---------+---------+-----------+------------------------------+----------------------------------------------------+\n| id                            | estRows | estCost | task      | access object                | operator info                                      |\n+-------------------------------+---------+---------+-----------+------------------------------+----------------------------------------------------+\n| IndexLookUp_10                | 33.33   | 738.29  | root      |                              |                                                    |\n| ├─IndexRangeScan_8(Build)     | 33.33   | 2370.00 | cop[tikv] | table:t, index:idx_b_c(b, c) | range:(2 4,2 +inf], keep order:false, stats:pseudo |\n| └─TableRowIDScan_9(Probe)     | 33.33   | 2370.00 | cop[tikv] | table:t                      | keep order:false, stats:pseudo                     |\n+-------------------------------+---------+---------+-----------+------------------------------+----------------------------------------------------+\n3 rows in set, 1 warning (0.00 sec)\n\nmysql> SHOW WARNINGS;\n+-------+------+------------------------------------------------------------------------------------------+\n| Level | Code | Message                                                                                  |\n+-------+------+------------------------------------------------------------------------------------------+\n| Note  | 1105 | [t,idx_b_c] remain after pruning paths for t given Prop{SortItems: [], TaskTp: rootTask} |\n+-------+------+------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n### 基于代价选择\n\n在使用 Skyline-Pruning 规则排除了不合适的索引之后，索引的选择完全基于代价估算，读表的代价估算需要考虑以下几个方面：\n\n- 索引的每行数据在存储层的平均长度。\n- 索引生成的查询范围的行数量。\n- 索引的回表代价。\n- 索引查询时的范围数量。\n\n根据这些因子和代价模型，优化器会选择一个代价最低的索引进行读表。\n\n#### 代价选择调优的常见问题\n\n1. 估算的行数量不准确？\n\n    一般是统计信息过期或者准确度不够造成的，可以重新执行 `ANALYZE TABLE` 或者修改 `ANALYZE TABLE` 的参数。\n\n2. 统计信息准确，为什么读 TiFlash 更快，而优化器选择了 TiKV？\n\n    目前区别 TiFlash 和 TiKV 的代价模型还比较粗糙，可以调小 [`tidb_opt_seek_factor`](/system-variables.md#tidb_opt_seek_factor) 的值，让优化器倾向于选择 TiFlash。\n\n3. 统计信息准确，某个索引要回表，但是它比另一个不用回表的索引实际执行更快，为什么选择了不用回表的索引？\n\n    碰到这种情况，可能是代价估算时对于回表的代价计算得过大，可以调小 [`tidb_opt_network_factor`](/system-variables.md#tidb_opt_network_factor)，降低回表的代价。\n\n## 控制索引的选择\n\n通过 [Optimizer Hints](/optimizer-hints.md) 可以实现单条查询对索引选择的控制。\n\n- `USE_INDEX`/`IGNORE_INDEX` 可以强制优化器使用/不使用某些索引。`FORCE_INDEX` 和 `USE_INDEX` 的作用相同。\n\n- `READ_FROM_STORAGE` 可以强制优化器对于某些表选择 TiKV/TiFlash 的存储引擎进行查询。\n\n## 使用多值索引\n\n[多值索引](/sql-statements/sql-statement-create-index.md#多值索引)和普通索引有所不同，TiDB 目前只会使用 [IndexMerge](/explain-index-merge.md) 来访问多值索引。因此要想使用多值索引进行数据访问，请确保 [`tidb_enable_index_merge`](/system-variables.md#tidb_enable_index_merge-从-v40-版本开始引入) 被设置为 `ON`。\n\n多值索引的使用限制请参考 [`CREATE INDEX`](/sql-statements/sql-statement-create-index.md#特性与限制)。\n\n### 支持多值索引的场景\n\n目前 TiDB 支持将 `json_member_of`、`json_contains` 和 `json_overlaps` 条件自动转换成 IndexMerge 来访问多值索引。既可以依赖优化器根据代价自动选择，也可通过 [`use_index_merge`](/optimizer-hints.md#use_index_merget1_name-idx1_name--idx2_name-) optimizer hint 或 [`use_index`](/optimizer-hints.md#use_indext1_name-idx1_name--idx2_name-) 指定选择多值索引，见下面例子：\n\n```sql\nmysql> CREATE TABLE t1 (j JSON, INDEX idx((CAST(j->'$.path' AS SIGNED ARRAY)))); -- 使用 '$.path' 作为路径创建多值索引\nQuery OK, 0 rows affected (0.04 sec)\n\nmysql> EXPLAIN SELECT /*+ use_index_merge(t1, idx) */ * FROM t1 WHERE (1 MEMBER OF (j->'$.path'));\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+\n| id                              | estRows | task      | access object                                                               | operator info                                                          |\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+\n| Selection_5                     | 8000.00 | root      |                                                                             | json_memberof(cast(1, json BINARY), json_extract(test.t1.j, \"$.path\")) |\n| └─IndexMerge_8                  | 10.00   | root      |                                                                             | type: union                                                            |\n|   ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t1, index:idx(cast(json_extract(`j`, _utf8'$.path') as signed array)) | range:[1,1], keep order:false, stats:pseudo                            |\n|   └─TableRowIDScan_7(Probe)     | 10.00   | cop[tikv] | table:t1                                                                    | keep order:false, stats:pseudo                                         |\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+\n4 rows in set, 1 warning (0.00 sec)\n\nmysql> EXPLAIN SELECT /*+ use_index_merge(t1, idx) */ * FROM t1 WHERE JSON_CONTAINS((j->'$.path'), '[1, 2, 3]');\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n| id                            | estRows | task      | access object                                                               | operator info                               |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n| IndexMerge_9                  | 10.00   | root      |                                                                             | type: intersection                          |\n| ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t1, index:idx(cast(json_extract(`j`, _utf8'$.path') as signed array)) | range:[1,1], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t1, index:idx(cast(json_extract(`j`, _utf8'$.path') as signed array)) | range:[2,2], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t1, index:idx(cast(json_extract(`j`, _utf8'$.path') as signed array)) | range:[3,3], keep order:false, stats:pseudo |\n| └─TableRowIDScan_8(Probe)     | 10.00   | cop[tikv] | table:t1                                                                    | keep order:false, stats:pseudo              |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n5 rows in set (0.00 sec)\n\nmysql> EXPLAIN SELECT /*+ use_index_merge(t1, idx) */ * FROM t1 WHERE JSON_OVERLAPS((j->'$.path'), '[1, 2, 3]');\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n| id                              | estRows | task      | access object                                                               | operator info                                                                    |\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n| Selection_5                     | 8000.00 | root      |                                                                             | json_overlaps(json_extract(test.t1.j, \"$.path\"), cast(\"[1, 2, 3]\", json BINARY)) |\n| └─IndexMerge_10                 | 10.00   | root      |                                                                             | type: union                                                                      |\n|   ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t1, index:idx(cast(json_extract(`j`, _utf8'$.path') as signed array)) | range:[1,1], keep order:false, stats:pseudo                                      |\n|   ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t1, index:idx(cast(json_extract(`j`, _utf8'$.path') as signed array)) | range:[2,2], keep order:false, stats:pseudo                                      |\n|   ├─IndexRangeScan_8(Build)     | 10.00   | cop[tikv] | table:t1, index:idx(cast(json_extract(`j`, _utf8'$.path') as signed array)) | range:[3,3], keep order:false, stats:pseudo                                      |\n|   └─TableRowIDScan_9(Probe)     | 10.00   | cop[tikv] | table:t1                                                                    | keep order:false, stats:pseudo                                                   |\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n6 rows in set, 1 warning (0.00 sec)\n```\n\n复合的多值索引，也一样可以使用 IndexMerge 进行访问：\n\n```sql\nCREATE TABLE t2 (a INT, j JSON, b INT, k JSON, INDEX idx(a, (CAST(j->'$.path' AS SIGNED ARRAY)), b), INDEX idx2(b, (CAST(k->'$.path' AS SIGNED ARRAY))));\nEXPLAIN SELECT /*+ use_index_merge(t2, idx) */ * FROM t2 WHERE a=1 AND (1 MEMBER OF (j->'$.path')) AND b=2;\nEXPLAIN SELECT /*+ use_index_merge(t2, idx) */ * FROM t2 WHERE a=1 AND JSON_CONTAINS((j->'$.path'), '[1, 2, 3]');\nEXPLAIN SELECT /*+ use_index_merge(t2, idx) */ * FROM t2 WHERE a=1 AND JSON_OVERLAPS((j->'$.path'), '[1, 2, 3]');\nEXPLAIN SELECT /*+ use_index_merge(t2, idx, idx2) */ * FROM t2 WHERE (a=1 AND 1 member of (j->'$.path')) AND (b=1 AND 2 member of (k->'$.path'));\n```\n\n```sql\n> EXPLAIN SELECT /*+ use_index_merge(t2, idx) */ * FROM t2 WHERE a=1 AND (1 MEMBER OF (j->'$.path')) AND b=2;\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-----------------------------------------------------+\n| id                            | estRows | task      | access object                                                                     | operator info                                       |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-----------------------------------------------------+\n| IndexMerge_7                  | 0.00    | root      |                                                                                   | type: union                                         |\n| ├─IndexRangeScan_5(Build)     | 0.00    | cop[tikv] | table:t2, index:idx(a, cast(json_extract(`j`, _utf8'$.path') as signed array), b) | range:[1 1 2,1 1 2], keep order:false, stats:pseudo |\n| └─TableRowIDScan_6(Probe)     | 0.00    | cop[tikv] | table:t2                                                                          | keep order:false, stats:pseudo                      |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-----------------------------------------------------+\n\n> EXPLAIN SELECT /*+ use_index_merge(t2, idx) */ * FROM t2 WHERE a=1 AND JSON_CONTAINS((j->'$.path'), '[1, 2, 3]');\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-------------------------------------------------+\n| id                            | estRows | task      | access object                                                                     | operator info                                   |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-------------------------------------------------+\n| IndexMerge_9                  | 0.00    | root      |                                                                                   | type: intersection                              |\n| ├─IndexRangeScan_5(Build)     | 0.10    | cop[tikv] | table:t2, index:idx(a, cast(json_extract(`j`, _utf8'$.path') as signed array), b) | range:[1 1,1 1], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_6(Build)     | 0.10    | cop[tikv] | table:t2, index:idx(a, cast(json_extract(`j`, _utf8'$.path') as signed array), b) | range:[1 2,1 2], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_7(Build)     | 0.10    | cop[tikv] | table:t2, index:idx(a, cast(json_extract(`j`, _utf8'$.path') as signed array), b) | range:[1 3,1 3], keep order:false, stats:pseudo |\n| └─TableRowIDScan_8(Probe)     | 0.00    | cop[tikv] | table:t2                                                                          | keep order:false, stats:pseudo                  |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-------------------------------------------------+\n\n> EXPLAIN SELECT /*+ use_index_merge(t2, idx) */ * FROM t2 WHERE a=1 AND JSON_OVERLAPS((j->'$.path'), '[1, 2, 3]');\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n| id                              | estRows | task      | access object                                                                     | operator info                                                                    |\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n| Selection_5                     | 0.24    | root      |                                                                                   | json_overlaps(json_extract(test.t2.j, \"$.path\"), cast(\"[1, 2, 3]\", json BINARY)) |\n| └─IndexMerge_10                 | 0.30    | root      |                                                                                   | type: union                                                                      |\n|   ├─IndexRangeScan_6(Build)     | 0.10    | cop[tikv] | table:t2, index:idx(a, cast(json_extract(`j`, _utf8'$.path') as signed array), b) | range:[1 1,1 1], keep order:false, stats:pseudo                                  |\n|   ├─IndexRangeScan_7(Build)     | 0.10    | cop[tikv] | table:t2, index:idx(a, cast(json_extract(`j`, _utf8'$.path') as signed array), b) | range:[1 2,1 2], keep order:false, stats:pseudo                                  |\n|   ├─IndexRangeScan_8(Build)     | 0.10    | cop[tikv] | table:t2, index:idx(a, cast(json_extract(`j`, _utf8'$.path') as signed array), b) | range:[1 3,1 3], keep order:false, stats:pseudo                                  |\n|   └─TableRowIDScan_9(Probe)     | 0.30    | cop[tikv] | table:t2                                                                          | keep order:false, stats:pseudo                                                   |\n+---------------------------------+---------+-----------+-----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n\n> EXPLAIN SELECT /*+ use_index_merge(t2, idx, idx2) */ * FROM t2 WHERE (a=1 AND 1 member of (j->'$.path')) AND (b=1 AND 2 member of (k->'$.path'));\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-----------------------------------------------------+\n| id                            | estRows | task      | access object                                                                     | operator info                                       |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-----------------------------------------------------+\n| IndexMerge_8                  | 0.00    | root      |                                                                                   | type: intersection                                  |\n| ├─IndexRangeScan_5(Build)     | 0.00    | cop[tikv] | table:t2, index:idx(a, cast(json_extract(`j`, _utf8'$.path') as signed array), b) | range:[1 1 1,1 1 1], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_6(Build)     | 0.10    | cop[tikv] | table:t2, index:idx2(b, cast(json_extract(`k`, _utf8'$.path') as signed array))   | range:[1 2,1 2], keep order:false, stats:pseudo     |\n| └─TableRowIDScan_7(Probe)     | 0.00    | cop[tikv] | table:t2                                                                          | keep order:false, stats:pseudo                      |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------------+-----------------------------------------------------+\n```\n\n同时，多值索引可以和普通索引共同组成 IndexMerge 使用。例如：\n\n```sql\nCREATE TABLE t3(j1 JSON, j2 JSON, a INT, INDEX k1((CAST(j1->'$.path' AS SIGNED ARRAY))), INDEX k2((CAST(j2->'$.path' AS SIGNED ARRAY))), INDEX ka(a));\nEXPLAIN SELECT /*+ use_index_merge(t3, k1, k2, ka) */ * FROM t3 WHERE 1 member of (j1->'$.path') OR a = 3;\nEXPLAIN SELECT /*+ use_index_merge(t3, k1, k2, ka) */ * FROM t3 WHERE 1 member of (j1->'$.path') AND 2 member of (j2->'$.path') AND (a = 3);\n```\n\n```sql\n> EXPLAIN SELECT /*+ use_index_merge(t3, k1, k2, ka) */ * FROM t3 WHERE 1 member of (j1->'$.path') OR a = 3;\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n| id                            | estRows | task      | access object                                                               | operator info                               |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n| IndexMerge_8                  | 19.99   | root      |                                                                             | type: union                                 |\n| ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t3, index:k1(cast(json_extract(`j1`, _utf8'$.path') as signed array)) | range:[1,1], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t3, index:ka(a)                                                       | range:[3,3], keep order:false, stats:pseudo |\n| └─TableRowIDScan_7(Probe)     | 19.99   | cop[tikv] | table:t3                                                                    | keep order:false, stats:pseudo              |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n\n> EXPLAIN SELECT /*+ use_index_merge(t3, k1, k2, ka) */ * FROM t3 WHERE 1 member of (j1->'$.path') AND 2 member of (j2->'$.path') AND (a = 3);\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n| id                            | estRows | task      | access object                                                               | operator info                               |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n| IndexMerge_9                  | 0.00    | root      |                                                                             | type: intersection                          |\n| ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t3, index:ka(a)                                                       | range:[3,3], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t3, index:k1(cast(json_extract(`j1`, _utf8'$.path') as signed array)) | range:[1,1], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t3, index:k2(cast(json_extract(`j2`, _utf8'$.path') as signed array)) | range:[2,2], keep order:false, stats:pseudo |\n| └─TableRowIDScan_8(Probe)     | 0.00    | cop[tikv] | table:t3                                                                    | keep order:false, stats:pseudo              |\n+-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n```\n\n对于由 `OR` 或 `AND` 连接而成的多个 `json_member_of`、`json_contains` 或 `json_overlaps` 条件，需要满足以下条件才能使用 IndexMerge 访问：\n\n```sql\nCREATE TABLE t4(a INT, j JSON, INDEX mvi1((CAST(j->'$.a' AS UNSIGNED ARRAY))), INDEX mvi2((CAST(j->'$.b' AS UNSIGNED ARRAY))));\n```\n\n- 对于由 `OR` 连接而成的条件，其中每个子条件都需要分别能够使用 IndexMerge 访问。例如：\n\n    ```sql\n    EXPLAIN SELECT /*+ use_index_merge(t4, mvi1) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1, 2]') OR json_overlaps(j->'$.a', '[3, 4]');\n    EXPLAIN SELECT /*+ use_index_merge(t4, mvi1) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1, 2]') OR json_length(j->'$.a') = 3;\n    SHOW WARNINGS;\n    ```\n\n    ```sql\n    > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1, 2]') OR json_overlaps(j->'$.a', '[3, 4]');\n    +----------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    | id                               | estRows | task      | access object                                                               | operator info                                                                                                                                              |\n    +----------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    | Selection_5                      | 31.95   | root      |                                                                             | or(json_overlaps(json_extract(test.t4.j, \"$.a\"), cast(\"[1, 2]\", json BINARY)), json_overlaps(json_extract(test.t4.j, \"$.a\"), cast(\"[3, 4]\", json BINARY))) |\n    | └─IndexMerge_11                  | 39.94   | root      |                                                                             | type: union                                                                                                                                                |\n    |   ├─IndexRangeScan_6(Build)      | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo                                                                                                                |\n    |   ├─IndexRangeScan_7(Build)      | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo                                                                                                                |\n    |   ├─IndexRangeScan_8(Build)      | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo                                                                                                                |\n    |   ├─IndexRangeScan_9(Build)      | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[4,4], keep order:false, stats:pseudo                                                                                                                |\n    |   └─TableRowIDScan_10(Probe)     | 39.94   | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo                                                                                                                             |\n    +----------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------+\n    \n    -- json_length(j->'$.a') = 3 无法使用 IndexMerge 访问，因此该 SQL 整体无法使用 IndexMerge 访问\n    > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1, 2]') OR json_length(j->'$.a') = 3;\n    +-------------------------+----------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------------------+\n    | id                      | estRows  | task      | access object | operator info                                                                                                                      |\n    +-------------------------+----------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------------------+\n    | Selection_5             | 8000.00  | root      |               | or(json_overlaps(json_extract(test.t4.j, \"$.a\"), cast(\"[1, 2]\", json BINARY)), eq(json_length(json_extract(test.t4.j, \"$.a\")), 3)) |\n    | └─TableReader_7         | 10000.00 | root      |               | data:TableFullScan_6                                                                                                               |\n    |   └─TableFullScan_6     | 10000.00 | cop[tikv] | table:t4      | keep order:false, stats:pseudo                                                                                                     |\n    +-------------------------+----------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------------------+\n    \n    > SHOW WARNINGS;\n    +---------+------+----------------------------+\n    | Level   | Code | Message                    |\n    +---------+------+----------------------------+\n    | Warning | 1105 | IndexMerge is inapplicable |\n    +---------+------+----------------------------+\n    ```\n\n- 对于由 `AND` 连接而成的条件，其中一部分子条件需要分别能够使用 IndexMerge 访问。最终使用 IndexMerge 访问时，也只能利用这一部分条件。例如：\n\n    ```sql\n    EXPLAIN SELECT /*+ use_index_merge(t4, mvi1) */ * FROM t4 WHERE json_contains(j->'$.a', '[1, 2]') AND json_contains(j->'$.a', '[3, 4]');\n    EXPLAIN SELECT /*+ use_index_merge(t4, mvi1) */ * FROM t4 WHERE json_contains(j->'$.a', '[1, 2]') AND json_contains(j->'$.a', '[3, 4]') AND json_length(j->'$.a') = 2;\n    ```\n\n    ```sql\n    > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1) */ * FROM t4 WHERE json_contains(j->'$.a', '[1, 2]') AND json_contains(j->'$.a', '[3, 4]');\n    +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n    | id                            | estRows | task      | access object                                                               | operator info                               |\n    +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n    | IndexMerge_10                 | 0.00    | root      |                                                                             | type: intersection                          |\n    | ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo |\n    | ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo |\n    | ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo |\n    | ├─IndexRangeScan_8(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[4,4], keep order:false, stats:pseudo |\n    | └─TableRowIDScan_9(Probe)     | 0.00    | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo              |\n    +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n    \n    -- json_length(j->'$.a') = 3 无法使用 IndexMerge 访问，因此只有另外两个 json_contains 表达式可以使用 IndexMerge 访问，而 json_length(j->'$.a') = 3 成为一个单独的 Selection 算子\n    > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1) */ * FROM t4 WHERE json_contains(j->'$.a', '[1, 2]') AND json_contains(j->'$.a', '[3, 4]') AND json_length(j->'$.a') = 2;\n    +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+----------------------------------------------------+\n    | id                            | estRows | task      | access object                                                               | operator info                                      |\n    +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+----------------------------------------------------+\n    | IndexMerge_11                 | 0.00    | root      |                                                                             | type: intersection                                 |\n    | ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo        |\n    | ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo        |\n    | ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo        |\n    | ├─IndexRangeScan_8(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[4,4], keep order:false, stats:pseudo        |\n    | └─Selection_10(Probe)         | 0.00    | cop[tikv] |                                                                             | eq(json_length(json_extract(test.t4.j, \"$.a\")), 2) |\n    |   └─TableRowIDScan_9          | 0.00    | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo                     |\n    +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+----------------------------------------------------+\n    ```\n\n- 用于构成整体 IndexMerge 计划的每个子条件，需要分别符合用于连接的 `OR` 或 `AND` 的语义。具体如下：\n\n    - `json_contains` 由 `AND` 连接，符合语义。例如：\n\n        ```sql\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_contains(j->'$.a', '[1]') AND json_contains(j->'$.b', '[2, 3]');\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_contains(j->'$.a', '[1]') OR json_contains(j->'$.b', '[2, 3]');\n        ```\n\n        ```sql\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_contains(j->'$.a', '[1]') AND json_contains(j->'$.b', '[2, 3]');\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        | id                            | estRows | task      | access object                                                               | operator info                               |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        | IndexMerge_9                  | 0.00    | root      |                                                                             | type: intersection                          |\n        | ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo |\n        | ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo |\n        | ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo |\n        | └─TableRowIDScan_8(Probe)     | 0.00    | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo              |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n\n        -- 不符合语义，如前文所述，该 SQL 整体无法使用 IndexMerge 访问\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_contains(j->'$.a', '[1]') OR json_contains(j->'$.b', '[2, 3]');\n        +-------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n        | id                      | estRows  | task      | access object | operator info                                                                                                                                           |\n        +-------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n        | TableReader_7           | 10.01    | root      |               | data:Selection_6                                                                                                                                        |\n        | └─Selection_6           | 10.01    | cop[tikv] |               | or(json_contains(json_extract(test.t4.j, \"$.a\"), cast(\"[1]\", json BINARY)), json_contains(json_extract(test.t4.j, \"$.b\"), cast(\"[2, 3]\", json BINARY))) |\n        |   └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t4      | keep order:false, stats:pseudo                                                                                                                          |\n        +-------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n        ```\n\n    - `json_overlaps` 由 `OR` 连接，符合语义。例如：\n\n        ```sql\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1]') OR json_overlaps(j->'$.b', '[2, 3]');\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1]') AND json_overlaps(j->'$.b', '[2, 3]');\n        ```\n\n        ```sql\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1]') OR json_overlaps(j->'$.b', '[2, 3]');\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n        | id                              | estRows | task      | access object                                                               | operator info                                                                                                                                           |\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n        | Selection_5                     | 23.98   | root      |                                                                             | or(json_overlaps(json_extract(test.t4.j, \"$.a\"), cast(\"[1]\", json BINARY)), json_overlaps(json_extract(test.t4.j, \"$.b\"), cast(\"[2, 3]\", json BINARY))) |\n        | └─IndexMerge_10                 | 29.97   | root      |                                                                             | type: union                                                                                                                                             |\n        |   ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo                                                                                                             |\n        |   ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo                                                                                                             |\n        |   ├─IndexRangeScan_8(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo                                                                                                             |\n        |   └─TableRowIDScan_9(Probe)     | 29.97   | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo                                                                                                                          |\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------+\n        \n        -- 不符合语义，如前文所述，只能利用一部分条件使用 IndexMerge 访问\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1]') AND json_overlaps(j->'$.b', '[2, 3]');\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n        | id                              | estRows | task      | access object                                                               | operator info                                                                                                                                       |\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n        | Selection_5                     | 15.99   | root      |                                                                             | json_overlaps(json_extract(test.t4.j, \"$.a\"), cast(\"[1]\", json BINARY)), json_overlaps(json_extract(test.t4.j, \"$.b\"), cast(\"[2, 3]\", json BINARY)) |\n        | └─IndexMerge_8                  | 10.00   | root      |                                                                             | type: union                                                                                                                                         |\n        |   ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo                                                                                                         |\n        |   └─TableRowIDScan_7(Probe)     | 10.00   | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo                                                                                                                      |\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------+\n        ```\n\n    - `json_member_of` 由 `OR` 或 `AND` 连接，均符合语义。例如：\n        \n        ```sql\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE 1 member of (j->'$.a') AND 2 member of (j->'$.b') AND 3 member of (j->'$.a');\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE 1 member of (j->'$.a') OR 2 member of (j->'$.b') OR 3 member of (j->'$.a');\n        ```\n\n        ```sql\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE 1 member of (j->'$.a') AND 2 member of (j->'$.b') AND 3 member of (j->'$.a');\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        | id                            | estRows | task      | access object                                                               | operator info                               |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        | IndexMerge_9                  | 0.00    | root      |                                                                             | type: intersection                          |\n        | ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo |\n        | ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo |\n        | ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo |\n        | └─TableRowIDScan_8(Probe)     | 0.00    | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo              |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE 1 member of (j->'$.a') OR 2 member of (j->'$.b') OR 3 member of (j->'$.a');\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        | id                            | estRows | task      | access object                                                               | operator info                               |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        | IndexMerge_9                  | 29.97   | root      |                                                                             | type: union                                 |\n        | ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo |\n        | ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo |\n        | ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo |\n        | └─TableRowIDScan_8(Probe)     | 29.97   | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo              |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        ```\n\n    - 包含多个值的 `json_contains` 由 `OR` 连接，或包含多个值的 `json_overlaps` 由 `AND` 连接，不符合语义，而如果它们只包含一个值则符合语义。例如：\n        \n        ```sql\n        -- 不符合语义的例子可参考前文，此处只举符合语义的例子\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1]') AND json_overlaps(j->'$.b', '[2]');\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_contains(j->'$.a', '[1]') OR json_contains(j->'$.b', '[2]');\n        ```\n\n        ```sql\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_overlaps(j->'$.a', '[1]') AND json_overlaps(j->'$.b', '[2]');\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n        | id                              | estRows | task      | access object                                                               | operator info                                                                                                                                    |\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n        | Selection_5                     | 8.00    | root      |                                                                             | json_overlaps(json_extract(test.t4.j, \"$.a\"), cast(\"[1]\", json BINARY)), json_overlaps(json_extract(test.t4.j, \"$.b\"), cast(\"[2]\", json BINARY)) |\n        | └─IndexMerge_9                  | 0.01    | root      |                                                                             | type: intersection                                                                                                                               |\n        |   ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo                                                                                                      |\n        |   ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo                                                                                                      |\n        |   └─TableRowIDScan_8(Probe)     | 0.01    | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo                                                                                                                   |\n        +---------------------------------+---------+-----------+-----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------+\n        \n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE json_contains(j->'$.a', '[1]') OR json_contains(j->'$.b', '[2]');\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        | id                            | estRows | task      | access object                                                               | operator info                               |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        | IndexMerge_8                  | 19.99   | root      |                                                                             | type: union                                 |\n        | ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo |\n        | ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo |\n        | └─TableRowIDScan_7(Probe)     | 19.99   | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo              |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+---------------------------------------------+\n        ```\n\n    - 当 `OR` 和 `AND` 同时出现（本质上是 `OR` 和 `AND` 嵌套）时，构成 IndexMerge 的条件只能全部符合 `OR` 的语义或者全部符合 `AND` 的语义，不能部分符合 `OR` 的语义而另一部分符合 `AND` 的语义。例如：\n        \n        ```sql\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE 1 member of (j->'$.a') AND (2 member of (j->'$.b') OR 3 member of (j->'$.a'));\n        EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE 1 member of (j->'$.a') OR (2 member of (j->'$.b') AND 3 member of (j->'$.a'));\n        ```\n\n        ```sql\n        -- 只有符合 OR 语义的 2 member of (j->'$.b') 和 3 member of (j->'$.a') 构成了 IndexMerge，而符合 AND 语义的 1 member of (j->'$.a') 没有构成 IndexMerge\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE 1 member of (j->'$.a') AND (2 member of (j->'$.b') OR 3 member of (j->'$.a'));\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n        | id                            | estRows | task      | access object                                                               | operator info                                                                                                                                                                                                     |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n        | IndexMerge_9                  | 0.00    | root      |                                                                             | type: union                                                                                                                                                                                                       |\n        | ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi2(cast(json_extract(`j`, _utf8'$.b') as unsigned array)) | range:[2,2], keep order:false, stats:pseudo                                                                                                                                                                       |\n        | ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo                                                                                                                                                                       |\n        | └─Selection_8(Probe)          | 0.00    | cop[tikv] |                                                                             | json_memberof(cast(1, json BINARY), json_extract(test.t4.j, \"$.a\")), or(json_memberof(cast(2, json BINARY), json_extract(test.t4.j, \"$.b\")), json_memberof(cast(3, json BINARY), json_extract(test.t4.j, \"$.a\"))) |\n        |   └─TableRowIDScan_7          | 19.99   | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo                                                                                                                                                                                    |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n        -- 只有符合 OR 语义的 1 member of (j->'$.a') 和 3 member of (j->'$.a') 构成了 IndexMerge，而嵌套的符合 AND 语义的 2 member of (j->'$.b') 没有构成 IndexMerge\n        > EXPLAIN SELECT /*+ use_index_merge(t4, mvi1, mvi2) */ * FROM t4 WHERE 1 member of (j->'$.a') OR (2 member of (j->'$.b') AND 3 member of (j->'$.a'));\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n        | id                            | estRows | task      | access object                                                               | operator info                                                                                                                                                                                                          |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n        | IndexMerge_9                  | 0.02    | root      |                                                                             | type: union                                                                                                                                                                                                            |\n        | ├─IndexRangeScan_5(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[1,1], keep order:false, stats:pseudo                                                                                                                                                                            |\n        | ├─IndexRangeScan_6(Build)     | 10.00   | cop[tikv] | table:t4, index:mvi1(cast(json_extract(`j`, _utf8'$.a') as unsigned array)) | range:[3,3], keep order:false, stats:pseudo                                                                                                                                                                            |\n        | └─Selection_8(Probe)          | 0.02    | cop[tikv] |                                                                             | or(json_memberof(cast(1, json BINARY), json_extract(test.t4.j, \"$.a\")), and(json_memberof(cast(2, json BINARY), json_extract(test.t4.j, \"$.b\")), json_memberof(cast(3, json BINARY), json_extract(test.t4.j, \"$.a\")))) |\n        |   └─TableRowIDScan_7          | 19.99   | cop[tikv] | table:t4                                                                    | keep order:false, stats:pseudo                                                                                                                                                                                         |\n        +-------------------------------+---------+-----------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n        ```\n\n如果表达式包含嵌套的 `OR` 或 `AND`，或者表达式需要进行展开等变形之后才能直接一一对应各索引列，TiDB 可能无法使用 IndexMerge 或不能充分利用所有过滤条件。建议针对具体场景预先进行测试验证。以下是部分场景示例：\n\n```sql\nCREATE TABLE t5 (a INT, j JSON, b INT, k JSON, INDEX idx(a, (CAST(j AS SIGNED ARRAY))), INDEX idx2(b, (CAST(k as SIGNED ARRAY))));\nCREATE TABLE t6 (a INT, j JSON, b INT, k JSON, INDEX idx(a, (CAST(j AS SIGNED ARRAY)), b), INDEX idx2(a, (CAST(k as SIGNED ARRAY)), b));\n```\n\n当 `AND` 嵌套在由 `OR` 连接的条件中，且嵌套的 `AND` 所连接的子条件与多列索引的各个列能够一一对应时，TiDB 通常能够充分利用过滤条件。例如：\n\n```sql\nEXPLAIN SELECT /*+ use_index_merge(t5, idx, idx2) */ * FROM t5 WHERE (a=1 AND 1 member of (j)) OR (b=2 AND 2 member of (k));\n```\n\n```sql\n> EXPLAIN SELECT /*+ use_index_merge(t5, idx, idx2) */ * FROM t5 WHERE (a=1 AND 1 member of (j)) OR (b=2 AND 2 member of (k));\n+-------------------------------+---------+-----------+----------------------------------------------------+-------------------------------------------------+\n| id                            | estRows | task      | access object                                      | operator info                                   |\n+-------------------------------+---------+-----------+----------------------------------------------------+-------------------------------------------------+\n| IndexMerge_8                  | 0.20    | root      |                                                    | type: union                                     |\n| ├─IndexRangeScan_5(Build)     | 0.10    | cop[tikv] | table:t5, index:idx(a, cast(`j` as signed array))  | range:[1 1,1 1], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_6(Build)     | 0.10    | cop[tikv] | table:t5, index:idx2(b, cast(`k` as signed array)) | range:[2 2,2 2], keep order:false, stats:pseudo |\n| └─TableRowIDScan_7(Probe)     | 0.20    | cop[tikv] | table:t5                                           | keep order:false, stats:pseudo                  |\n+-------------------------------+---------+-----------+----------------------------------------------------+-------------------------------------------------+\n```\n\n当单个 `OR` 嵌套在由 `AND` 连接的条件中，且 `OR` 所连接的子条件通过展开表达式可以直接对应索引列时，TiDB 通常能够充分利用过滤条件。例如：\n\n```sql\nEXPLAIN SELECT /*+ use_index_merge(t6, idx, idx2) */ * FROM t6 WHERE a=1 AND (1 member of (j) OR 2 member of (k));\n```\n\n```sql\n+-------------------------------+---------+-----------+-------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| id                            | estRows | task      | access object                                         | operator info                                                                                                           |\n+-------------------------------+---------+-----------+-------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| IndexMerge_9                  | 0.20    | root      |                                                       | type: union                                                                                                             |\n| ├─IndexRangeScan_5(Build)     | 0.10    | cop[tikv] | table:t6, index:idx(a, cast(`j` as signed array), b)  | range:[1 1,1 1], keep order:false, stats:pseudo                                                                         |\n| ├─IndexRangeScan_6(Build)     | 0.10    | cop[tikv] | table:t6, index:idx2(a, cast(`k` as signed array), b) | range:[1 2,1 2], keep order:false, stats:pseudo                                                                         |\n| └─Selection_8(Probe)          | 0.20    | cop[tikv] |                                                       | eq(test2.t6.a, 1), or(json_memberof(cast(1, json BINARY), test2.t6.j), json_memberof(cast(2, json BINARY), test2.t6.k)) |\n|   └─TableRowIDScan_7          | 0.20    | cop[tikv] | table:t6                                              | keep order:false, stats:pseudo                                                                                          |\n+-------------------------------+---------+-----------+-------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n```\n\n当多个 `OR` 嵌套在由 `AND` 连接的条件中，且 `OR` 所连接的子条件需要展开表达式才能直接对应索引列的时候，TiDB 可能无法充分利用过滤条件。例如：\n\n```sql\nEXPLAIN SELECT /*+ use_index_merge(t6, idx, idx2) */ * FROM t6 WHERE a=1 AND (1 member of (j) OR 2 member of (k)) and (b = 1 OR b = 2);\nEXPLAIN SELECT /*+ use_index_merge(t6, idx, idx2) */ * FROM t6 WHERE a=1 AND ((1 member of (j) AND b = 1) OR (1 member of (j) AND b = 2) OR (2 member of (k) AND b = 1) OR (2 member of (k) AND b = 2));\n```\n\n```sql\n-- 由于目前实现上的限制，(b = 1 or b = 2) 没有参与构成 IndexMerge，而是成为了一个单独的 Selection 算子\n> EXPLAIN SELECT /*+ use_index_merge(t6, idx, idx2) */ * FROM t6 WHERE a=1 AND (1 member of (j) OR 2 member of (k)) AND (b = 1 OR b = 2);\n+-------------------------------+---------+-----------+-------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id                            | estRows | task      | access object                                         | operator info                                                                                                                                                |\n+-------------------------------+---------+-----------+-------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| IndexMerge_9                  | 0.20    | root      |                                                       | type: union                                                                                                                                                  |\n| ├─IndexRangeScan_5(Build)     | 0.10    | cop[tikv] | table:t6, index:idx(a, cast(`j` as signed array), b)  | range:[1 1,1 1], keep order:false, stats:pseudo                                                                                                              |\n| ├─IndexRangeScan_6(Build)     | 0.10    | cop[tikv] | table:t6, index:idx2(a, cast(`k` as signed array), b) | range:[1 2,1 2], keep order:false, stats:pseudo                                                                                                              |\n| └─Selection_8(Probe)          | 0.20    | cop[tikv] |                                                       | eq(test.t6.a, 1), or(eq(test.t6.b, 1), eq(test.t6.b, 2)), or(json_memberof(cast(1, json BINARY), test.t6.j), json_memberof(cast(2, json BINARY), test.t6.k)) |\n|   └─TableRowIDScan_7          | 0.20    | cop[tikv] | table:t6                                              | keep order:false, stats:pseudo                                                                                                                               |\n+-------------------------------+---------+-----------+-------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n-- 将 SQL 中由 AND 连接的两个 OR 表达式手动展开之后，TiDB 能够充分利用这些条件\n> EXPLAIN SELECT /*+ use_index_merge(t6, idx, idx2) */ * FROM t6 WHERE a=1 AND ((1 member of (j) AND b = 1) OR (1 member of (j) AND b = 2) OR (2 member of (k) AND b = 1) OR (2 member of (k) AND b = 2));\n+-------------------------------+---------+-----------+-------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id                            | estRows | task      | access object                                         | operator info                                                                                                                                                                                                                                                                                                            |\n+-------------------------------+---------+-----------+-------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| IndexMerge_11                 | 0.00    | root      |                                                       | type: union                                                                                                                                                                                                                                                                                                              |\n| ├─IndexRangeScan_5(Build)     | 0.00    | cop[tikv] | table:t6, index:idx(a, cast(`j` as signed array), b)  | range:[1 1 1,1 1 1], keep order:false, stats:pseudo                                                                                                                                                                                                                                                                      |\n| ├─IndexRangeScan_6(Build)     | 0.00    | cop[tikv] | table:t6, index:idx(a, cast(`j` as signed array), b)  | range:[1 1 2,1 1 2], keep order:false, stats:pseudo                                                                                                                                                                                                                                                                      |\n| ├─IndexRangeScan_7(Build)     | 0.00    | cop[tikv] | table:t6, index:idx2(a, cast(`k` as signed array), b) | range:[1 2 1,1 2 1], keep order:false, stats:pseudo                                                                                                                                                                                                                                                                      |\n| ├─IndexRangeScan_8(Build)     | 0.00    | cop[tikv] | table:t6, index:idx2(a, cast(`k` as signed array), b) | range:[1 2 2,1 2 2], keep order:false, stats:pseudo                                                                                                                                                                                                                                                                      |\n| └─Selection_10(Probe)         | 0.00    | cop[tikv] |                                                       | eq(test.t6.a, 1), or(or(and(json_memberof(cast(1, json BINARY), test.t6.j), eq(test.t6.b, 1)), and(json_memberof(cast(1, json BINARY), test.t6.j), eq(test.t6.b, 2))), or(and(json_memberof(cast(2, json BINARY), test.t6.k), eq(test.t6.b, 1)), and(json_memberof(cast(2, json BINARY), test.t6.k), eq(test.t6.b, 2)))) |\n|   └─TableRowIDScan_9          | 0.00    | cop[tikv] | table:t6                                              | keep order:false, stats:pseudo                                                                                                                                                                                                                                                                                           |\n+-------------------------------+---------+-----------+-------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n\n受限于多值索引的特性，当多值索引无法生效时，使用 [`use_index`](/optimizer-hints.md#use_indext1_name-idx1_name--idx2_name-) 可能会返回 `Can't find a proper physical plan for this query` 的错误，而使用 [`use_index_merge`](/optimizer-hints.md#use_index_merget1_name-idx1_name--idx2_name-) 不会，因此建议使用 `use_index_merge`：\n\n```sql\nmysql> EXPLAIN SELECT /*+ use_index(t3, idx) */ * FROM t3 WHERE ((1 member of (j)) AND (2 member of (j))) OR ((3 member of (j)) AND (4 member of (j)));\nERROR 1815 (HY000): Internal : Cant find a proper physical plan for this query\n\nmysql> EXPLAIN SELECT /*+ use_index_merge(t3, idx) */ * FROM t3 WHERE ((1 member of (j)) AND (2 member of (j))) OR ((3 member of (j)) AND (4 member of (j)));\n+-------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id                      | estRows  | task      | access object | operator info                                                                                                                                                                                                |\n+-------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Selection_5             | 8000.00  | root      |               | or(and(json_memberof(cast(1, json BINARY), test.t3.j), json_memberof(cast(2, json BINARY), test.t3.j)), and(json_memberof(cast(3, json BINARY), test.t3.j), json_memberof(cast(4, json BINARY), test.t3.j))) |\n| └─TableReader_7         | 10000.00 | root      |               | data:TableFullScan_6                                                                                                                                                                                         |\n|   └─TableFullScan_6     | 10000.00 | cop[tikv] | table:t3      | keep order:false, stats:pseudo                                                                                                                                                                               |\n+-------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n3 rows in set, 2 warnings (0.00 sec)\n```\n\n### 多值索引与执行计划缓存\n\n通过 `member of` 条件选择的多值索引的执行计划可以被缓存。如果执行计划是通过 `JSON_CONTAINS()` 或 `JSON_OVERLAPS()` 函数选择的多值索引，则该计划暂时无法被缓存。\n\n下面是可以缓存的例子：\n\n```sql\nmysql> CREATE TABLE t5 (j1 JSON, j2 JSON, INDEX idx1((CAST(j1 AS SIGNED ARRAY))));\nQuery OK, 0 rows affected (0.04 sec)\n\nmysql> PREPARE st FROM 'SELECT /*+ use_index(t5, idx1) */ * FROM t5 WHERE (? member of (j1))';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SET @a=1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> EXECUTE st USING @a;\nEmpty set (0.01 sec)\n\nmysql> EXECUTE st USING @a;\nEmpty set (0.00 sec)\n\nmysql> SELECT @@last_plan_from_cache;\n+------------------------+\n| @@last_plan_from_cache |\n+------------------------+\n|                      1 |\n+------------------------+\n1 row in set (0.00 sec)\n\nmysql> PREPARE st FROM 'SELECT /*+ use_index(t5, idx1) */ * FROM t5 WHERE (? member of (j1)) AND JSON_CONTAINS(j2, ?)';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SET @a=1, @b='[1,2]';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> EXECUTE st USING @a, @b;\nEmpty set (0.00 sec)\n\nmysql> EXECUTE st USING @a, @b;\nEmpty set (0.00 sec)\n\nmysql> SELECT @@LAST_PLAN_FROM_CACHE; -- can hit plan cache if the JSON_CONTAINS doesn't impact index selection\n+------------------------+\n| @@LAST_PLAN_FROM_CACHE |\n+------------------------+\n|                      1 |\n+------------------------+\n1 row in set (0.00 sec)\n```\n\n下面是无法缓存的例子：\n\n```sql\nmysql> PREPARE st2 FROM 'SELECT /*+ use_index(t5, idx1) */ * FROM t5 WHERE JSON_CONTAINS(j1, ?)';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SET @a='[1,2]';\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> EXECUTE st2 USING @a;\nEmpty set, 1 warning (0.00 sec)\n\nmysql> SHOW WARNINGS;  -- cannot hit plan cache since the JSON_CONTAINS predicate might affect index selection\n+---------+------+-------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                               |\n+---------+------+-------------------------------------------------------------------------------------------------------+\n| Warning | 1105 | skip prepared plan-cache: json_contains function with immutable parameters can affect index selection |\n+---------+------+-------------------------------------------------------------------------------------------------------+\n1 row in set (0.01 sec)\n```\n"
        },
        {
          "name": "clinic",
          "type": "tree",
          "content": null
        },
        {
          "name": "clustered-indexes.md",
          "type": "blob",
          "size": 11.4072265625,
          "content": "---\ntitle: 聚簇索引\nsummary: 本文档介绍了聚簇索引的概念、使用场景、使用方法、限制和兼容性。\n---\n\n# 聚簇索引\n\n聚簇索引 (clustered index) 是 TiDB 从 v5.0 开始支持的特性，用于控制含有主键的表数据的存储方式。通过使用聚簇索引，TiDB 可以更好地组织数据表，从而提高某些查询的性能。有些数据库管理系统也将聚簇索引称为“索引组织表” (index-organized tables)。\n\n目前 TiDB 中含有主键的表分为以下两类：\n\n- `NONCLUSTERED`，表示该表的主键为非聚簇索引。在非聚簇索引表中，行数据的键由 TiDB 内部隐式分配的 `_tidb_rowid` 构成，而主键本质上是唯一索引，因此非聚簇索引表存储一行至少需要两个键值对，分别为\n    - `_tidb_rowid`（键）- 行数据（值）\n    - 主键列数据（键） - `_tidb_rowid`（值）\n- `CLUSTERED`，表示该表的主键为聚簇索引。在聚簇索引表中，行数据的键由用户给定的主键列数据构成，因此聚簇索引表存储一行至少只要一个键值对，即\n    - 主键列数据（键） - 行数据（值）\n\n> **注意：**\n>\n> TiDB 仅支持根据表的主键来进行聚簇操作。聚簇索引启用时，“主键”和“聚簇索引”两个术语在一些情况下可互换使用。主键指的是约束（一种逻辑属性），而聚簇索引描述的是数据存储的物理实现。\n\n## 使用场景\n\n相较于非聚簇索引表，聚簇索引表在以下几个场景中，性能和吞吐量都有较大优势：\n\n- 插入数据时会减少一次从网络写入索引数据。\n- 等值条件查询仅涉及主键时会减少一次从网络读取数据。\n- 范围条件查询仅涉及主键时会减少多次从网络读取数据。\n- 等值或范围条件查询仅涉及主键的前缀时会减少多次从网络读取数据。\n\n另一方面，聚簇索引表也存在一定的劣势：\n\n- 批量插入大量取值相邻的主键时，可能会产生较大的写热点问题。\n- 当使用大于 64 位的数据类型作为主键时，可能导致表数据需要占用更多的存储空间。该现象在存在多个二级索引时尤为明显。\n\n## 使用方法\n\n### 创建聚簇索引表\n\n从 TiDB 版本 5.0 开始，要指定一个表的主键是否使用聚簇索引，可以在 `CREATE TABLE` 语句中将 `CLUSTERED` 或者 `NONCLUSTERED` 非保留关键字标注在 `PRIMARY KEY` 后面，例如：\n\n```sql\nCREATE TABLE t (a BIGINT PRIMARY KEY CLUSTERED, b VARCHAR(255));\nCREATE TABLE t (a BIGINT PRIMARY KEY NONCLUSTERED, b VARCHAR(255));\nCREATE TABLE t (a BIGINT KEY CLUSTERED, b VARCHAR(255));\nCREATE TABLE t (a BIGINT KEY NONCLUSTERED, b VARCHAR(255));\nCREATE TABLE t (a BIGINT, b VARCHAR(255), PRIMARY KEY(a, b) CLUSTERED);\nCREATE TABLE t (a BIGINT, b VARCHAR(255), PRIMARY KEY(a, b) NONCLUSTERED);\n```\n\n注意，列定义中的 `KEY` 和 `PRIMARY KEY` 含义相同。\n\n此外，TiDB 支持使用[可执行的注释语法](/comment-syntax.md)指定聚簇索引属性：\n\n```sql\nCREATE TABLE t (a BIGINT PRIMARY KEY /*T![clustered_index] CLUSTERED */, b VARCHAR(255));\nCREATE TABLE t (a BIGINT PRIMARY KEY /*T![clustered_index] NONCLUSTERED */, b VARCHAR(255));\nCREATE TABLE t (a BIGINT, b VARCHAR(255), PRIMARY KEY(a, b) /*T![clustered_index] CLUSTERED */);\nCREATE TABLE t (a BIGINT, b VARCHAR(255), PRIMARY KEY(a, b) /*T![clustered_index] NONCLUSTERED */);\n```\n\n对于未显式指定该关键字的语句，默认行为受系统变量 [`@@global.tidb_enable_clustered_index`](/system-variables.md#tidb_enable_clustered_index-从-v50-版本开始引入) 影响。该变量有三个取值：\n\n- `OFF` 表示所有主键默认使用非聚簇索引。\n- `ON` 表示所有主键默认使用聚簇索引。\n- `INT_ONLY` 此时的行为受配置项 `alter-primary-key` 控制。如果该配置项取值为 `true`，则所有主键默认使用非聚簇索引；如果该配置项取值为 `false`，则由单个整数类型的列构成的主键默认使用聚簇索引，其他类型的主键默认使用非聚簇索引。\n\n系统变量 `@@global.tidb_enable_clustered_index` 本身的默认值为 `ON`。\n\n### 添加、删除聚簇索引\n\n目前 TiDB 不支持在建表之后添加或删除聚簇索引，也不支持聚簇索引和非聚簇索引的互相转换。例如：\n\n```sql\nALTER TABLE t ADD PRIMARY KEY(b, a) CLUSTERED; -- 暂不支持\nALTER TABLE t DROP PRIMARY KEY;     -- 如果主键为聚簇索引，则不支持\nALTER TABLE t DROP INDEX `PRIMARY`; -- 如果主键为聚簇索引，则不支持\n```\n\n### 添加、删除非聚簇索引\n\nTiDB 支持在建表之后添加或删除非聚簇索引。此时可以选择显式指定 `NONCLUSTERED` 关键字或省略关键字：\n\n```sql\nALTER TABLE t ADD PRIMARY KEY(b, a) NONCLUSTERED;\nALTER TABLE t ADD PRIMARY KEY(b, a); -- 不指定关键字，则为非聚簇索引\nALTER TABLE t DROP PRIMARY KEY;\nALTER TABLE t DROP INDEX `PRIMARY`;\n```\n\n### 查询主键是否为聚簇索引\n\n可通过以下方式来确定一张表的主键是否使用了聚簇索引：\n\n- 执行语句 `SHOW CREATE TABLE`。\n- 执行语句 `SHOW INDEX FROM`。\n- 查询系统表 `information_schema.tables` 中的 `TIDB_PK_TYPE` 列。\n\n通过 `SHOW CREATE TABLE` 查看，`PRIMARY KEY` 的属性可能为 `CLUSTERED` 或 `NONCLUSTERED`：\n\n```sql\nmysql> SHOW CREATE TABLE t;\n+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table | Create Table                                                                                                                                                                                      |\n+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| t     | CREATE TABLE `t` (\n  `a` bigint NOT NULL,\n  `b` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`a`) /*T![clustered_index] CLUSTERED */\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin |\n+-------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.01 sec)\n```\n\n通过 `SHOW INDEX FROM` 查看，`Clustered` 一列可能的结果为 `Yes` 或 `No`：\n\n```sql\nmysql> SHOW INDEX FROM t;\n+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+-----------+\n| Table | Non_unique | Key_name | Seq_in_index | Column_name | Collation | Cardinality | Sub_part | Packed | Null | Index_type | Comment | Index_comment | Visible | Expression | Clustered |\n+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+-----------+\n| t     |          0 | PRIMARY  |            1 | a           | A         |           0 |     NULL | NULL   |      | BTREE      |         |               | YES     | NULL       | YES       |\n+-------+------------+----------+--------------+-------------+-----------+-------------+----------+--------+------+------------+---------+---------------+---------+------------+-----------+\n1 row in set (0.01 sec)\n```\n\n查询 `information_schema.tables` 系统表中的 `TIDB_PK_TYPE` 列，可能的结果为 `CLUSTERED` 或 `NONCLUSTERED`：\n\n```sql\nmysql> SELECT TIDB_PK_TYPE FROM information_schema.tables WHERE table_schema = 'test' AND table_name = 't';\n+--------------+\n| TIDB_PK_TYPE |\n+--------------+\n| CLUSTERED    |\n+--------------+\n1 row in set (0.03 sec)\n```\n\n## 限制\n\n目前 TiDB 的聚簇索引具有以下几类限制：\n\n- 明确不支持且没有支持计划的使用限制：\n    - 不支持与 [`SHARD_ROW_ID_BITS`](/shard-row-id-bits.md) 一起使用；[`PRE_SPLIT_REGIONS`](/sql-statements/sql-statement-split-region.md#pre_split_regions) 在聚簇索引表上不生效。\n    - 不支持对聚簇索引表进行降级。如需降级，请使用逻辑备份工具迁移数据。\n- 尚未支持，但未来有计划支持的使用限制：\n    - 尚未支持通过 `ALTER TABLE` 语句增加、删除、修改聚簇索引。\n\n与 `SHARD_ROW_ID_BITS` 一起使用时会报以下错误：\n\n```sql\nmysql> CREATE TABLE t (a VARCHAR(255) PRIMARY KEY CLUSTERED) SHARD_ROW_ID_BITS = 3;\nERROR 8200 (HY000): Unsupported shard_row_id_bits for table with primary key as row id\n```\n\n## 兼容性\n\n### 升降级兼容性\n\nTiDB 支持对聚簇索引表的升级兼容，但不支持降级兼容，即高版本 TiDB 聚簇索引表的数据在低版本 TiDB 上不可用。\n\n聚簇索引在 TiDB v3.0 和 v4.0 中已完成部分支持，当表中存在单个整数列作为主键时默认启用，即：\n\n- 表设置了主键\n- 主键只有一列\n- 主键的数据类型为整数类型\n\nTiDB v5.0 完成了所有类型主键的支持，但默认行为与 TiDB v3.0 和 v4.0 保持一致。要修改默认行为，请设置系统变量 `@@tidb_enable_clustered_index` 为 `ON` 或 `OFF`。\n\n### MySQL 兼容性\n\nTiDB 支持使用可执行注释的语法来包裹 `CLUSTERED` 或 `NONCLUSTERED` 关键字，且 `SHOW CREATE TABLE` 的结果均包含 TiDB 特有的可执行注释，这些注释在 MySQL 或低版本的 TiDB 中会被忽略。\n\n### TiDB 数据迁移工具兼容性\n\n聚簇索引仅与 v5.0 及以后版本的以下数据迁移工具兼容：\n\n- 备份与恢复工具 BR、Dumpling、TiDB Lightning。\n- 数据迁移和同步工具 DM、TiCDC。\n\nv5.0 的 BR 不能通过备份恢复将非聚簇索引表转换成聚簇索引表，反之亦然。\n\n### 与 TiDB 其他特性的兼容性\n\n在非单整数列作为主键的表中，从非聚簇索引变为聚簇索引之后，在 v5.0 之前版本的 TiDB 能够执行的 `SPLIT TABLE BY/BETWEEN` 语句在 v5.0 及以后版本的 TiDB 上不再可用，原因是行数据键的构成发生了变化。在聚簇索引表上执行 `SPLIT TABLE BY/BETWEEN` 时需要依据主键列指定值，而不是指定一个整数值。例如：\n\n```sql\nmysql> create table t (a int, b varchar(255), primary key(a, b) clustered);\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> split table t between (0) and (1000000) regions 5;\nERROR 1105 (HY000): Split table region lower value count should be 2\n\nmysql> split table t by (0), (50000), (100000);\nERROR 1136 (21S01): Column count doesn't match value count at row 0\n\nmysql> split table t between (0, 'aaa') and (1000000, 'zzz') regions 5;\n+--------------------+----------------------+\n| TOTAL_SPLIT_REGION | SCATTER_FINISH_RATIO |\n+--------------------+----------------------+\n|                  4 |                    1 |\n+--------------------+----------------------+\n1 row in set (0.00 sec)\n\nmysql> split table t by (0, ''), (50000, ''), (100000, '');\n+--------------------+----------------------+\n| TOTAL_SPLIT_REGION | SCATTER_FINISH_RATIO |\n+--------------------+----------------------+\n|                  3 |                    1 |\n+--------------------+----------------------+\n1 row in set (0.01 sec)\n```\n\n[`AUTO_RANDOM`](/auto-random.md) 属性只能在聚簇索引表上使用。在非聚簇索引上使用 `AUTO_RANDOM` 会报以下错误：\n\n```sql\nmysql> create table t (a bigint primary key nonclustered auto_random);\nERROR 8216 (HY000): Invalid auto random: column a is not the integer primary key, or the primary key is nonclustered\n```\n"
        },
        {
          "name": "column-pruning.md",
          "type": "blob",
          "size": 1.5380859375,
          "content": "---\ntitle: 列裁剪\naliases: ['/docs-cn/dev/column-pruning/']\nsummary: 列裁剪是优化器在优化过程中删除不需要的列的基本思想。这样可以减少 I/O 资源占用并为后续优化带来便利。TiDB 会在逻辑优化阶段进行列裁剪，减少资源浪费。该扫描过程称作“列裁剪”，对应逻辑优化规则中的 columnPruner。如果要关闭这个规则，可以参照优化规则及表达式下推的黑名单中的关闭方法。\n---\n\n# 列裁剪\n\n列裁剪的基本思想在于：对于算子中实际用不上的列，优化器在优化的过程中没有必要保留它们。对这些列的删除会减少 I/O 资源占用，并为后续的优化带来便利。下面给出一个列重复的例子：\n\n假设表 t 里面有 a b c d 四列，执行如下语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect a from t where b > 5\n```\n\n在该查询的过程中，t 表实际上只有 a, b 两列会被用到，而 c, d 的数据则显得多余。对应到该语句的查询计划，Selection 算子会用到 b 列，下面接着的 DataSource 算子会用到 a, b 两列，而剩下 c, d 两列则都可以裁剪掉，DataSource 算子在读数据时不需要将它们读进来。\n\n出于上述考量，TiDB 会在逻辑优化阶段进行自上而下的扫描，裁剪不需要的列，减少资源浪费。该扫描过程称作 “列裁剪”，对应逻辑优化规则中的 `columnPruner`。如果要关闭这个规则，可以在参照[优化规则及表达式下推的黑名单](/blocklist-control-plan.md)中的关闭方法。\n"
        },
        {
          "name": "command-line-flags-for-pd-configuration.md",
          "type": "blob",
          "size": 4.853515625,
          "content": "---\ntitle: PD 配置参数\naliases: ['/docs-cn/dev/command-line-flags-for-pd-configuration/','/docs-cn/dev/reference/configuration/pd-server/configuration/']\nsummary: PD 配置参数可以通过命令行参数或环境变量配置。包括外部访问 PD 的 URL 列表，其他 PD 节点访问某个 PD 节点的 URL 列表，PD 监听的客户端 URL 列表，PD 节点监听其他 PD 节点的 URL 列表，配置文件，PD 存储数据路径，初始化 PD 集群配置，动态加入 PD 集群，Log 级别，Log 文件，是否开启日志切割，当前 PD 的名字，CA 文件路径，包含 X509 证书的 PEM 文件路径，包含 X509 key 的 PEM 文件路径，指定 Prometheus Pushgateway 的地址，强制使用当前节点创建新的集群，输出版本信息并退出。\n---\n\n# PD 配置参数\n\nPD 可以通过命令行参数或环境变量配置。\n\n## `--advertise-client-urls`\n\n+ 用于外部访问 PD 的 URL 列表。\n+ 默认：`${client-urls}`\n+ 在某些情况下，例如 Docker 或者 NAT 网络环境，客户端并不能通过 PD 自己监听的 client URLs 来访问到 PD，这时候，你就可以设置 advertise URLs 来让客户端访问。\n+ 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 2379:2379`，那么可以设置为 `--advertise-client-urls=\"http://192.168.100.113:2379\"`，客户端可以通过 `http://192.168.100.113:2379` 来找到这个服务。\n\n## `--advertise-peer-urls`\n\n+ 用于其他 PD 节点访问某个 PD 节点的 URL 列表。\n+ 默认：`${peer-urls}`\n+ 在某些情况下，例如 Docker 或者 NAT 网络环境，其他节点并不能通过 PD 自己监听的 peer URLs 来访问到 PD，这时候，你就可以设置 advertise URLs 来让其他节点访问。\n+ 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 2380:2380`，那么可以设置为 `advertise-peer-urls=\"http://192.168.100.113:2380\"`，其他 PD 节点可以通过 `http://192.168.100.113:2380` 来找到这个服务。\n\n## `--client-urls`\n\n+ PD 监听的客户端 URL 列表。\n+ 默认：`http://127.0.0.1:2379`\n+ 如果部署一个集群，`--client-urls` 必须指定当前主机的 IP 地址，例如 `http://192.168.100.113:2379\"`，如果是运行在 Docker 则需要指定为 `http://0.0.0.0:2379`。\n\n## `--peer-urls`\n\n+ PD 节点监听其他 PD 节点的 URL 列表。\n+ 默认：`http://127.0.0.1:2380`\n+ 如果部署一个集群，`--peer-urls` 必须指定当前主机的 IP 地址，例如 `http://192.168.100.113:2380`，如果是运行在 Docker 则需要指定为 `http://0.0.0.0:2380`。\n\n## `--config`\n\n+ 配置文件。\n+ 默认：\"\"\n+ 如果你指定了配置文件，PD 会首先读取配置文件的配置。然后如果对应的配置在命令行参数里面也存在，PD 就会使用命令行参数的配置来覆盖配置文件里面的。\n\n## `--data-dir`\n\n+ PD 存储数据路径。\n+ 默认：`default.${name}`\n\n## `--initial-cluster`\n\n+ 初始化 PD 集群配置。\n+ 默认：`\"{name}=http://{advertise-peer-url}\"`\n+ 例如，如果 name 是 \"pd\"，并且 `advertise-peer-urls` 是 `http://192.168.100.113:2380`，那么 `initial-cluster` 就是 `pd=http://192.168.100.113:2380`。\n+ 如果你需要启动三台 PD，那么 `initial-cluster` 可能就是 `pd1=http://192.168.100.113:2380, pd2=http://192.168.100.114:2380, pd3=192.168.100.115:2380`。\n\n## `--join`\n\n+ 动态加入 PD 集群。\n+ 默认：\"\"\n+ 如果你想动态将一台 PD 加入集群，你可以使用 `--join=\"${advertise-client-urls}\"`，`advertise-client-url` 是当前集群里面任意 PD 的 `advertise-client-url`，你也可以使用多个 PD 的，需要用逗号分隔。\n\n## `-L`\n\n+ Log 级别。\n+ 默认：\"info\"\n+ 可选：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n\n## `--log-file`\n\n+ Log 文件。\n+ 默认：\"\"\n+ 如果没设置这个参数，log 会默认输出到 \"stderr\"，如果设置了，log 就会输出到对应的文件里面。\n\n## `--log-rotate`\n\n+ 是否开启日志切割。\n+ 默认：true\n+ 当值为 true 时，按照 PD 配置文件中 `[log.file]` 信息执行。\n\n## `--name`\n\n+ 当前 PD 的名字。\n+ 默认：\"pd-${hostname}\"\n+ 如果你需要启动多个 PD，一定要给 PD 使用不同的名字\n\n## `--cacert`\n\n+ CA 文件路径，用于开启 TLS。\n+ 默认：\"\"\n\n## `--cert`\n\n+ 包含 X509 证书的 PEM 文件路径，用户开启 TLS。\n+ 默认：\"\"\n\n## `--key`\n\n+ 包含 X509 key 的 PEM 文件路径，用于开启 TLS。\n+ 默认：\"\"\n\n## `--metrics-addr`\n\n+ 指定 Prometheus Pushgateway 的地址。\n+ 默认：\"\"\n+ 如果留空，则不开启 Prometheus Push。\n\n## `--force-new-cluster`\n\n+ 强制使用当前节点创建新的集群。\n+ 默认：false\n+ 仅用于在 PD 丢失多数副本的情况下恢复服务，可能会产生部分数据丢失。\n\n## `-V`, `--version`\n\n+ 输出版本信息并退出。\n"
        },
        {
          "name": "command-line-flags-for-scheduling-configuration.md",
          "type": "blob",
          "size": 2.5419921875,
          "content": "---\ntitle: Scheduling 配置参数\nsummary: Scheduling 配置参数可以通过命令行参数或环境变量配置。\n---\n\n# Scheduling 配置参数\n\nScheduling 节点用于提供 PD 的 `scheduling` 微服务。你可以通过命令行参数或环境变量配置 Scheduling 节点。\n\n## `--advertise-listen-addr`\n\n- 用于外部访问 Scheduling 节点的 URL。\n- 默认：`${listen-addr}`\n- 在某些情况下，例如 Docker 或者 NAT 网络环境，客户端并不能通过 Scheduling 节点自己监听的地址来访问 Scheduling 节点。此时，你可以设置 `--advertise-listen-addr` 来让客户端访问。\n- 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 3379:3379`，那么可以设置 `--advertise-listen-addr=\"http://192.168.100.113:3379\"`，然后客户端就可以通过 `http://192.168.100.113:3379` 来找到这个服务。\n\n## `--backend-endpoints`\n\n- Scheduling 节点监听其他 Scheduling 节点的 URL 列表。\n- 默认：`http://127.0.0.1:2379`\n\n## `--cacert`\n\n- CA 文件路径，用于开启 TLS。\n- 默认：\"\"\n\n## `--cert`\n\n- 包含 X.509 证书的 PEM 文件路径，用于开启 TLS。\n- 默认：\"\"\n\n## `--config`\n\n- 配置文件。\n- 默认：\"\"\n- 如果你指定了配置文件，Scheduling 节点会首先读取配置文件的配置。然后如果对应的配置在命令行参数里面也存在，Scheduling 节点就会使用命令行参数的配置来覆盖配置文件里面的配置。\n\n## `--data-dir`\n\n- Scheduling 节点上的数据存储路径。\n- 默认：`default.${name}`\n\n## `--key`\n\n- 包含 X.509 key 的 PEM 文件路径，用于开启 TLS。\n- 默认：\"\"\n\n## `--listen-addr`\n\n- Scheduling 节点监听的客户端 URL。\n- 默认：`http://127.0.0.1:3379`\n- 部署集群时，`--listen-addr` 必须指定当前主机的 IP 地址，例如 `http://192.168.100.113:3379`。如果运行在 Docker 中，则需要指定为 `http://0.0.0.0:3379`。\n\n## `--log-file`\n\n- Log 文件。\n- 默认：\"\"\n- 如果未设置该参数，log 会默认输出到 \"stderr\"。如果设置了该参数，log 将输出到指定的文件。\n\n## `--name` <span class=\"version-mark\">从 v8.3.0 版本开始引入</span>\n\n+ 当前 Scheduling 节点的名字。\n+ 默认：`\"scheduling-${hostname}\"`\n+ 如果你需要启动多个 Scheduling 节点，建议为不同 Scheduling 节点设置不同的名字，以方便区分。\n\n## `-L`\n\n- Log 级别。\n- 默认：\"info\"\n- 可选：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n\n## `-V`, `--version`\n\n- 输出版本信息并退出。\n"
        },
        {
          "name": "command-line-flags-for-tidb-configuration.md",
          "type": "blob",
          "size": 9.744140625,
          "content": "---\ntitle: TiDB 配置参数\naliases: ['/docs-cn/dev/command-line-flags-for-tidb-configuration/','/docs-cn/dev/reference/configuration/tidb-server/configuration/']\nsummary: TiDB 配置参数包括启动参数和环境变量。启动参数包括 advertise-address、config、config-check、config-strict、cors 等。其中默认端口为 4000 和 10080。其他参数包括 log-file、metrics-addr、metrics-interval 等。注意配置文件的有效性和安全模式下的启动。\n---\n\n# TiDB 配置参数\n\n在启动 TiDB 时，你可以使用命令行参数或环境变量来配置 TiDB。\n\n要快速了解 TiDB 的参数体系与参数作用域，建议先观看下面的培训视频（时长 17 分钟）。\n\n<video src=\"https://download.pingcap.com/docs-cn%2FLesson10_config.mp4\" width=\"600px\" height=\"450px\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_lesson10.png\"></video>\n\n本文将详细介绍 TiDB 的命令行启动参数。TiDB 的默认端口为 4000（客户端请求）与 10080（状态报告）。\n\n## `--advertise-address`\n\n+ 登录 TiDB 的 IP 地址\n+ 默认：\"\"\n+ 必须确保用户和集群中的其他机器都能够访问到该 IP 地址\n\n## `--config`\n\n+ 配置文件\n+ 默认：\"\"\n+ 如果你指定了配置文件，TiDB 会首先读取配置文件的配置。如果对应的配置在命令行参数里面也存在，TiDB 就会使用命令行参数的配置来覆盖配置文件中的配置。详细的配置项请参阅 [TiDB 配置文件描述](/tidb-configuration-file.md)。\n\n## `--config-check`\n\n- 检查配置文件的有效性并退出\n- 默认：false\n\n## `--config-strict`\n\n- 增强配置文件的有效性\n- 默认：false\n\n## `--cors`\n\n+ 用于设置 TiDB HTTP 状态服务的 Access-Control-Allow-Origin\n+ 默认：\"\"\n\n## `--host`\n\n+ TiDB 服务监听的 host\n+ 默认：\"0.0.0.0\"\n+ 0.0.0.0 默认会监听所有的网卡地址。如果有多块网卡，可以指定对外提供服务的网卡，如 192.168.100.113\n\n## `--initialize-insecure`\n\n- 在不安全模式下启动 tidb-server\n- 默认：true\n\n## `--initialize-secure`\n\n- 在安全模式下启动 tidb-server\n- 默认：false\n\n## `--initialize-sql-file`\n\n- 用于指定 TiDB 集群初次启动时执行的 SQL 脚本。参考[配置项 `initialize-sql-file`](/tidb-configuration-file.md#initialize-sql-file-从-v660-版本开始引入)\n- 默认：\"\"\n\n## `-L`\n\n+ Log 级别\n+ 默认：\"info\"\n+ 可选：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n\n## `--lease`\n\n- Schema lease 的持续时间。除非你知道更改该值带来的后果，否则你的更改操作是**危险的**。\n- 默认：45s\n\n## `--log-file`\n\n+ Log 文件\n+ 默认：\"\"\n+ 如果未设置该参数，log 会默认输出到 \"stderr\"；如果设置了该参数，log 会输出到对应的文件中。\n\n## `--log-general`\n\n+ [General Log](/system-variables.md#tidb_general_log) 文件名\n+ 默认：\"\"\n+ 如果未设置该参数，general log 会默认输出到 [`--log-file`](#--log-file) 指定的文件中。\n\n## `--log-slow-query`\n\n+ 慢查询日志文件路径\n+ 默认：\"\"\n+ 如果未设置该参数，log 会默认输出到 `--log-file` 指定的文件中\n\n## `--metrics-addr`\n\n+ Prometheus Pushgateway 地址\n+ 默认：\"\"\n+ 如果该参数为空，TiDB 不会将统计信息推送给 Pushgateway。参数格式示例：`--metrics-addr=192.168.100.115:9091`\n\n## `--metrics-interval`\n\n+ 推送统计信息到 Prometheus Pushgateway 的时间间隔\n+ 默认：15s\n+ 设置为 0 表示不推送统计信息给 Pushgateway。示例：`--metrics-interval=2` 指每两秒推送到 Pushgateway\n\n## `-P`\n\n+ TiDB 服务监听端口\n+ 默认：\"4000\"\n+ TiDB 服务会使用该端口接受 MySQL 客户端发来的请求\n\n## `--path`\n\n+ 对于本地存储引擎 \"unistore\" 来说，path 指定的是实际的数据存放路径\n+ 当 `--store = tikv` 时，必须指定 path；当 `--store = unistore` 时，如果不指定 path，会使用默认值。\n+ 对于 \"TiKV\" 存储引擎来说，path 指定的是实际的 PD 地址。假如在 192.168.100.113:2379、192.168.100.114:2379 和 192.168.100.115:2379 上面部署了 PD，那么 path 为 \"192.168.100.113:2379, 192.168.100.114:2379, 192.168.100.115:2379\"\n+ 默认：\"/tmp/tidb\"\n+ 可以通过 `tidb-server --store=unistore --path=\"\"` 来启动一个纯内存引擎的 TiDB\n\n## `--proxy-protocol-networks`\n\n+ 允许使用 [PROXY 协议](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)连接 TiDB 的代理服务器地址列表。\n+ 默认：\"\"\n+ 通常情况下，通过反向代理使用 TiDB 时，TiDB 会将反向代理服务器的 IP 地址视为客户端 IP 地址。对于支持 [PROXY 协议](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)的反向代理（如 HAProxy），开启 PROXY 协议后能让反向代理透传客户端真实的 IP 地址给 TiDB。\n+ 配置该参数后，TiDB 将允许配置的源 IP 地址使用 PROXY 协议连接到 TiDB，且拒绝这些源 IP 地址使用非 PROXY 协议连接。其他地址可以使用非 PROXY 协议连接到 TiDB。若该参数为空，则任何源 IP 地址都不能使用 PROXY 协议连接到 TiDB。地址可以使用 IP 地址格式 (192.168.1.50) 或者 CIDR 格式 (192.168.1.0/24)，并可用 `,` 分隔多个地址，或用 `*` 代表所有 IP 地址。\n\n> **警告：**\n>\n> 需谨慎使用 `*` 符号，因为它可能引入安全风险，允许来自任何 IP 的客户端自行汇报其 IP 地址。另外，当 [`--proxy-protocol-fallbackable`](#--proxy-protocol-fallbackable) 设置为 `true` 以外的值时，使用 `*` 可能会导致部分直接连接 TiDB 的内部组件无法使用，例如 TiDB Dashboard。\n\n> **注意：**\n>\n> 如果使用 AWS 的 Network Load Balancer (NLB) 并开启 PROXY 协议，需要设置 NLB 的 `target group` 属性：将 `proxy_protocol_v2.client_to_server.header_place` 设为 `on_first_ack`。同时向 AWS 的 Support 提工单开通此功能的支持。注意，AWS NLB 在开启 PROXY 协议后，客户端将无法获取服务器端的握手报文，因此报文会一直阻塞到客户端超时。这是因为，NLB 默认只在客户端发送数据之后才会发送 PROXY 的报文，而在客户端发送数据包之前，服务器端发送的任何数据包都会在内网被丢弃。\n\n## `--proxy-protocol-fallbackable`\n\n+ 用于控制是否启用 PROXY 协议回退模式。如果设置为 `true`，TiDB 可以接受属于 `--proxy-protocol-networks` 的客户端使用非 PROXY 协议规范或者没有发送 PROXY 协议头的客户端连接。默认情况下，TiDB 仅接受属于 `--proxy-protocol-networks` 的客户端发送 PROXY 协议头的客户端连接。\n+ 默认：`false`\n\n## `--proxy-protocol-header-timeout`\n\n+ PROXY 协议请求头读取超时时间\n+ 默认：5\n+ 单位：秒\n\n> **警告：**\n>\n> 自 v6.3.0 起，该参数被废弃。因为自 v6.3.0 起，读取 PROXY 协议报头的操作会在第一次读取网络数据时进行，废弃该参数可避免影响首次读取网络数据时设置的超时时间。\n\n> **注意：**\n>\n> 不要将该参数配置为 `0`。除非特殊情况，一般使用默认值即可。\n\n## `--report-status`\n\n+ 用于打开或者关闭服务状态监听端口\n+ 默认：true\n+ 将参数值设置为 `true` 表明开启状态监听端口；设置为 `false` 表明关闭状态监听端口\n\n## `--run-ddl`\n\n+ tidb-server 是否运行 DDL 语句，集群内至少需要有一台 tidb-server 设置该参数\n+ 默认：true\n+ 值可以为 `true` 或者 `false`。设置为 `true` 表明自身会运行 DDL；设置为 `false` 表明自身不会运行 DDL\n\n## `--socket string`\n\n+ TiDB 服务使用 unix socket file 方式接受外部连接\n+ 默认：\"\"\n+ 例如可以使用 \"/tmp/tidb.sock\" 来打开 unix socket file\n\n## `--status`\n\n+ TiDB 服务状态监听端口\n+ 默认：\"10080\"\n+ 该端口用于展示 TiDB 内部数据，包括 [prometheus 统计](https://prometheus.io/)和 [pprof](https://golang.org/pkg/net/http/pprof/)\n+ Prometheus 统计可以通过 `http://host:status_port/metrics` 访问\n+ pprof 数据可以通过 `http://host:status_port/debug/pprof` 访问\n\n## `--status-host`\n\n+ TiDB 服务状态监听 host\n+ 默认：\"0.0.0.0\"\n\n## `--store`\n\n+ 用来指定 TiDB 底层使用的存储引擎\n+ 默认：\"unistore\"\n+ 可以选择 \"unistore\"（本地存储引擎）或者 \"tikv\"（分布式存储引擎）\n\n## `--temp-dir`\n\n- TiDB 用于存放临时文件的目录\n- 默认：\"/tmp/tidb\"\n\n## `--tidb-service-scope`\n\n+ 用于设置当前 TiDB 实例 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) 的初始值。\n+ 默认：`\"\"`\n\n## `--token-limit`\n\n+ TiDB 中同时允许运行的 Session 数量，用于流量控制\n+ 默认：1000\n+ 如果当前运行的连接多于该 token-limit，那么请求会阻塞，等待已经完成的操作释放 Token\n\n## `-V`\n\n+ 输出 TiDB 的版本\n+ 默认：\"\"\n\n## `--plugin-dir`\n\n+ plugin 存放目录\n+ 默认：\"/data/deploy/plugin\"\n\n## `--plugin-load`\n\n+ 需要加载的 plugin 名称，多个 plugin 以 \",\" 逗号分隔\n+ 默认：\"\"\n\n## `--affinity-cpus`\n\n+ 设置 TiDB server CPU 亲和性，以 \",\" 逗号分隔，例如 \"1,2,3\"\n+ 默认：\"\"\n\n## `--redact`\n\n+ 设置 TiDB server 是否在使用子命令 `collect-log` 时脱敏日志文件。\n+ 默认：false\n+ 取值为 `true` 时为脱敏操作，所有被标记符号 `‹ ›` 包裹的字段会被替换为 `?`。取值为 `false` 时为还原操作，所有标记符号会被去除。具体使用方法为：执行 `./tidb-server --redact=xxx collect-log <input> <output>` 将 `<input>` 指向的 TiDB server 日志文件进行脱敏或者还原，并输出到 `<output>`。更多详情，请参考系统变量 [`tidb_redact_log`](/system-variables.md#tidb_redact_log)。\n\n## `--repair-mode`\n\n+ 是否开启修复模式，仅用于数据修复场景\n+ 默认：false\n\n## `--repair-list`\n\n+ 修复模式下需要修复的表名\n+ 默认：\"\""
        },
        {
          "name": "command-line-flags-for-tikv-configuration.md",
          "type": "blob",
          "size": 4.2158203125,
          "content": "---\ntitle: TiKV 配置参数\naliases: ['/docs-cn/dev/command-line-flags-for-tikv-configuration/','/docs-cn/dev/reference/configuration/tikv-server/configuration/']\nsummary: TiKV 配置参数支持文件大小和时间的可读性好的单位转换。命令行参数包括监听地址、对外访问地址、服务状态监听端口、对外访问服务状态地址、配置文件、存储数据的容量、配置信息输出格式、数据存储路径、日志级别、日志文件、PD 地址列表。需要注意的是，PD 地址列表需要使用逗号分隔多个地址。\n---\n\n# TiKV 配置参数\n\nTiKV 的命令行参数支持一些可读性好的单位转换。\n\n+ 文件大小（以 bytes 为单位）：KB, MB, GB, TB, PB（也可以全小写）\n+ 时间（以毫秒为单位）：ms, s, m, h\n\n## `-A, --addr`\n\n+ TiKV 监听地址。\n+ 默认：\"127.0.0.1:20160\"\n+ 如果部署一个集群，\\-\\-addr 必须指定当前主机的 IP 地址，例如 \"192.168.100.113:20160\"，如果是运行在 docker 则需要指定为 \"0.0.0.0:20160\"。\n\n## `--advertise-addr`\n\n+ TiKV 对外访问地址。\n+ 默认：${addr}\n+ 在某些情况下，比如 Docker 或者 NAT 网络环境，客户端并不能通过 `--addr` 的地址来访问到 TiKV。这时候，你可以设置 `--advertise-addr` 来让客户端访问 TiKV。\n+ 例如，docker 内部 IP 地址为 172.17.0.1，而宿主机的 IP 地址为 192.168.100.113 并且设置了端口映射 -p 20160:20160，那么可以设置为 \\-\\-advertise-addr=\"192.168.100.113:20160\"，客户端可以通过 192.168.100.113:20160 来找到这个服务。\n\n## `--status-addr`\n\n+ TiKV 服务状态监听端口。\n+ 默认：\"20180\"\n+ Prometheus 统计可以通过 `http://host:status_port/metrics` 访问。\n+ Profile 数据可以通过 `http://host:status_port/debug/pprof/profile` 访问。\n\n## `--advertise-status-addr`\n\n+ TiKV 对外访问服务状态地址。\n+ 默认：使用 `--status-addr`\n+ 在某些情况下，例如 docker 或者 NAT 网络环境，客户端并不能通过 `--status-addr` 的地址来访问到 TiKV。此时，你可以设置 `--advertise-status-addr` 来让客户端访问 TiKV。\n+ 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 20180:20180`，那么可以设置 `--advertise-status-addr=\"192.168.100.113:20180\"`，客户端可以通过 `192.168.100.113:20180` 来找到这个服务。\n\n## `-C, --config`\n\n+ 配置文件。\n+ 默认：\"\"\n+ 如果你指定了配置文件，TiKV 会首先读取配置文件的配置。然后如果对应的配置在命令行参数里面也存在，TiKV 就会使用命令行参数的配置来覆盖配置文件里面的。\n\n## `--capacity`\n\n+ TiKV 存储数据的容量。\n+ 默认：0（无限）\n+ PD 需要使用这个值来对整个集群做 balance 操作。（提示：你可以使用 10GB 来替代 10737418240，从而简化参数的传递）。\n\n## `--config-info <FORMAT>`\n\n+ 按照指定的 `FORMAT` 输出各个配置项的取值信息并退出。\n+ `FORMAT` 可选值：`json`。\n+ 目前仅支持以 JSON 格式输出每个配置项的名字 (Name)、默认值 (DefaultValue) 和当前配置值 (ValueInFile)。当执行此命令时，若同时指定了 `-C` 或 `--config` 参数，则对应的配置文件包含的配置项会同时输出当前配置值和默认值，其他未指定的配置项仅输出默认值，示例如下：\n\n  ```json\n  {\n    \"Component\": \"TiKV Server\",\n    \"Version\": \"6.2.0\",\n    \"Parameters\": [\n      {\n        \"Name\": \"log-level\",\n        \"DefaultValue\": \"info\",\n        \"ValueInFile\": \"warn\"\n      },\n      {\n        \"Name\": \"log-file\",\n        \"DefaultValue\": \"\"\n      },\n      ...\n    ]\n  }\n  ```\n\n## `--data-dir`\n\n+ TiKV 数据存储路径。\n+ 默认：\"/tmp/tikv/store\"\n\n## `-L`\n\n+ Log 级别。\n+ 默认：\"info\"\n+ 可选值：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n\n## `--log-file`\n\n+ Log 文件。\n+ 默认：\"\"\n+ 如果没设置这个参数，log 会默认输出到 \"stderr\"，如果设置了，log 就会输出到对应的文件里面。\n\n## `--pd`\n\n+ PD 地址列表。\n+ 默认：\"\"\n+ TiKV 必须使用这个值连接 PD，才能正常工作。使用逗号来分隔多个 PD 地址，例如：192.168.100.113:2379, 192.168.100.114:2379, 192.168.100.115:2379。\n"
        },
        {
          "name": "command-line-flags-for-tso-configuration.md",
          "type": "blob",
          "size": 2.412109375,
          "content": "---\ntitle: TSO 配置参数\nsummary: TSO 配置参数可以通过命令行参数或环境变量配置。\n---\n\n# TSO 配置参数\n\nTSO 节点用于提供 PD 的 `tso` 微服务。你可以通过命令行参数或环境变量配置 TSO 节点。\n\n## `--advertise-listen-addr`\n\n- 用于外部访问 TSO 节点的 URL。\n- 默认：`${listen-addr}`\n- 在某些情况下，例如 Docker 或者 NAT 网络环境，客户端并不能通过 TSO 节点自己监听的地址来访问 TSO 节点。此时，你可以设置 `--advertise-listen-addr` 来让客户端访问。\n- 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 3379:3379`，那么可以设置 `--advertise-listen-addr=\"http://192.168.100.113:3379\"`，然后客户端就可以通过 `http://192.168.100.113:3379` 来找到这个服务。\n\n## `--backend-endpoints`\n\n- TSO 节点监听其他 TSO 节点的 URL 列表。\n- 默认：`http://127.0.0.1:2379`\n\n## `--cacert`\n\n- CA 文件路径，用于开启 TLS。\n- 默认：\"\"\n\n## `--cert`\n\n- 包含 X.509 证书的 PEM 文件路径，用于开启 TLS。\n- 默认：\"\"\n\n## `--config`\n\n- 配置文件。\n- 默认：\"\"\n- 如果你指定了配置文件，TSO 节点会首先读取配置文件的配置。然后如果对应的配置在命令行参数里面也存在，TSO 节点就会使用命令行参数的配置来覆盖配置文件里面的配置。\n\n## `--data-dir`\n\n- TSO 节点上的数据存储路径。\n- 默认：`default.${name}`\n\n## `--key`\n\n- 包含 X.509 key 的 PEM 文件路径，用于开启 TLS。\n- 默认：\"\"\n\n## `--listen-addr`\n\n- TSO 节点监听的客户端 URL。\n- 默认：`http://127.0.0.1:3379`\n- 部署集群时，`--listen-addr` 必须指定当前主机的 IP 地址，例如 `http://192.168.100.113:3379`。如果运行在 Docker 中，则需要指定为 `http://0.0.0.0:3379`。\n\n## `--log-file`\n\n- Log 文件。\n- 默认：\"\"\n- 如果未设置该参数，log 会默认输出到 \"stderr\"。如果设置了该参数，log 将输出到指定的文件。\n\n## `--name` <span class=\"version-mark\">从 v8.3.0 版本开始引入</span>\n\n+ 当前 TSO 节点的名字。\n+ 默认：`\"tso-${hostname}\"`\n+ 如果你需要启动多个 TSO 节点，建议为不同 TSO 节点设置不同的名字，以方便区分。\n\n## `-L`\n\n- Log 级别。\n- 默认：\"info\"\n- 可选：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n\n## `-V`, `--version`\n\n- 输出版本信息并退出。\n"
        },
        {
          "name": "comment-syntax.md",
          "type": "blob",
          "size": 3.6123046875,
          "content": "---\ntitle: 注释语法\nsummary: 本文介绍 TiDB 支持的注释语法。\naliases: ['/docs-cn/dev/comment-syntax/','/docs-cn/dev/reference/sql/language-structure/comment-syntax/']\n---\n\n# 注释语法\n\n本文档介绍 TiDB 支持的注释语法。\n\nTiDB 支持三种注释风格：\n\n* 用 `#` 注释一行：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT 1+1;     # 注释文字\n    ```\n\n    ```\n    +------+\n    | 1+1  |\n    +------+\n    |    2 |\n    +------+\n    1 row in set (0.00 sec)\n    ```\n\n* 用 `--` 注释一行：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT 1+1;     -- 注释文字\n    ```\n\n    ```\n    +------+\n    | 1+1  |\n    +------+\n    |    2 |\n    +------+\n    1 row in set (0.00 sec)\n    ```\n\n    用 `--` 注释时，必须要在其之后留出至少一个空格，否则注释不生效：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT 1+1--1;\n    ```\n\n    ```\n    +--------+\n    | 1+1--1 |\n    +--------+\n    |      3 |\n    +--------+\n    1 row in set (0.01 sec)\n    ```\n\n* 用 `/* */` 注释一块，可以注释多行：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT 1 /* 这是行内注释文字 */ + 1;\n    ```\n\n    ```\n    +--------+\n    | 1  + 1 |\n    +--------+\n    |      2 |\n    +--------+\n    1 row in set (0.01 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT 1+\n    /*\n    /*> 这是一条\n    /*> 多行注释\n    /*> */\n        1;\n    ```\n\n    ```\n    +-------------------+\n    | 1+\n\n            1 |\n    +-------------------+\n    |                 2 |\n    +-------------------+\n    1 row in set (0.001 sec)\n    ```\n\n## MySQL 兼容的注释语法\n\nTiDB 也跟 MySQL 保持一致，支持一种 C 风格注释的变体：\n\n```\n/*! Specific code */\n```\n\n或者\n\n```\n/*!50110 Specific code */\n```\n\n和 MySQL 一样，TiDB 会执行注释中的语句。\n\n例如：`SELECT /*! STRAIGHT_JOIN */ col1 FROM table1,table2 WHERE ...`\n\n在 TiDB 中，这种写法等价于 `SELECT STRAIGHT_JOIN col1 FROM table1,table2 WHERE ...`\n\n如果注释中指定了 Server 版本号，例如 `/*!50110 KEY_BLOCK_SIZE=1024 */`，在 MySQL 中表示只有 MySQL 的版本大于等于 5.1.10 才会处理这个 comment 中的内容。但是在 TiDB 中，这个 MySQL 版本号不会起作用，所有的 comment 都被会处理。\n\n## TiDB 可执行的注释语法\n\nTiDB 也有独立的注释语法，称为 TiDB 可执行注释语法。主要分为两种：\n\n* `/*T! Specific code */`：该语法只能被 TiDB 解析执行，而在其他数据库中会被忽略。\n\n* `/*T![feature_id] Specific code */`：该语法用于保证 TiDB 不同版本之间的兼容性。只有在当前版本中实现了 `feature_id` 对应的功能特性的 TiDB，才会试图解析该注释里的 SQL 片段。例如 v3.1.1 中引入了 `AUTO_RANDOM` 特性，该版本能够将 `/*T![auto_rand] auto_random */` 解析为 `auto_random`；而 v3.0.0 中没有实现 `AUTO_RANDOM` 特性，则上述 SQL 语句片段会被忽略。**注意前几个字符 `/*T![` 中，各字符之间没有任何空格**。\n\n## 优化器注释语法\n\n还有一种注释会被当做是优化器 Hint 特殊对待：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ hint */ FROM ...;\n```\n\nTiDB 支持的相关优化器 hint 详见 [Optimizer Hints](/optimizer-hints.md)。\n\n> **注意：**\n>\n> 在 MySQL 客户端中，TiDB 可执行注释语法会被默认当成注释被清除掉。在 MySQL 客户端 5.7.7 之前的版本中，Hint 也会被默认当成注释被清除掉。推荐在启动客户端时加上 `--comments` 选项，例如 `mysql -h 127.0.0.1 -P 4000 -uroot --comments`。\n\n更多细节，请参考 [MySQL 文档](https://dev.mysql.com/doc/refman/8.0/en/comments.html)。\n"
        },
        {
          "name": "config-templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "configure-load-base-split.md",
          "type": "blob",
          "size": 4.802734375,
          "content": "---\ntitle: Load Base Split\nsummary: 介绍 Load Base Split 功能。\naliases: ['/docs-cn/dev/configure-load-base-split/']\n---\n\n# Load Base Split \n\nLoad Base Split 是 TiKV 在 4.0 版本引入的特性，旨在解决 Region 访问分布不均匀造成的热点问题，比如小表的全表扫描。\n\n## 场景描述\n\n在 TiDB 中，当流量集中在某些节点时很容易形成热点。PD 会尝试通过调度 Hot Region，尽可能让这些 Hot Region 均匀分布在各个节点上，以求获得更好的性能。\n\n但是 PD 的调度的最小粒度是 Region。如果集群的热点数目少于节点数目，或者说存在某几个热点流量远高于其他 Region，对 PD 的热点调度来说，能做到的也只是让热点从一个节点转移到另一个节点，而无法让整个集群承担负载。\n\n这种场景在读请求居多的 workload 中尤为常见。例如对小表的全表扫描和索引查找，或者是对一些字段的频繁访问。\n\n在此之前解决此类问题的办法是手动输入命令去拆分一个或几个热点 Region，但是这样的操作存在以下两个问题：\n\n- 均匀拆分 Region 并不一定是最好的选择，请求可能集中在某几个 Key 上，即使均匀拆分后热点可能仍然集中在其中一个 Region 上，可能需要经过多次均匀拆分才能达到目标。\n- 人工介入不够及时和易用。\n\n## 实现原理\n\nLoad Base Split 会基于统计信息自动拆分 Region。通过统计信息识别出读流量或 CPU 使用率在 10s 内持续超过阈值的 Region，并在合适的位置将这些 Region 拆分。在选择拆分的位置时，会尽可能平衡拆分后两个 Region 的访问量，并尽量避免跨 Region 的访问。\n\nLoad Base Split 后的 Region 不会被迅速 Merge。一方面，PD 的 `MergeChecker` 会跳过 hot Region，另一方面 PD 也会针对心跳信息中的 `QPS`去进行判断，避免 Merge 两个 `QPS` 很高的 Region。\n\n## 使用方法\n\n目前的 Load Base Split 的控制参数如下：\n\n- [`split.qps-threshold`](/tikv-configuration-file.md#qps-threshold)：表明一个 Region 被识别为热点的 QPS 阈值。当 [`region-split-size`](/tikv-configuration-file.md#region-split-size) 小于 4 GB 时，默认为每秒 `3000` QPS。当 `region-split-size` 大于或等于 4 GB 时，默认值为每秒 `7000` QPS。\n- [`split.byte-threshold`](/tikv-configuration-file.md#byte-threshold-从-v50-版本开始引入)：自 v5.0 引入，表明一个 Region 被识别为热点的流量阈值，单位为 Byte。当 `region-split-size` 小于 4 GB 时，默认值为每秒 `30 MiB` 流量。当 `region-split-size` 大于或等于 4 GB 时，默认值为每秒 `100 MiB` 流量。\n- [`split.region-cpu-overload-threshold-ratio`](/tikv-configuration-file.md#region-cpu-overload-threshold-ratio-从-v620-版本开始引入)：自 v6.2.0 引入，表明一个 Region 被识别为热点的 CPU 使用率（占读线程池 CPU 时间的百分比）阈值。当 `region-split-size` 小于 4 GB 时，默认值为 `0.25`。当 `region-split-size` 大于或等于 4 GB 时，默认值为 `0.75`。\n\n如果连续 10s 内，某个 Region 每秒的各类读请求之和超过了 `split.qps-threshold`、流量超过了 `split.byte-threshold`，或 CPU 使用率在 Unified Read Pool 内的占比超过了 `split.region-cpu-overload-threshold-ratio`，那么就会尝试对此 Region 进行拆分。\n\n目前默认开启 Load Base Split，但配置相对保守。如果想要关闭这个功能，将 QPS 和 Byte 阈值全部调到足够高并将 CPU 占比阈值调为 0 即可。\n\n目前有两种办法修改配置：\n\n- 通过 SQL 语句修改，例如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    # 设置 QPS 阈值为 1500\n    SET config tikv split.qps-threshold=1500;\n    # 设置 Byte 阈值为 15 MiB (15 * 1024 * 1024)\n    SET config tikv split.byte-threshold=15728640;\n    # 设置 CPU 使用率阈值为 50%\n    SET config tikv split.region-cpu-overload-threshold-ratio=0.5;\n    ```\n\n- 通过 TiKV 修改，例如：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    curl -X POST \"http://ip:status_port/config\" -H \"accept: application/json\" -d '{\"split.qps-threshold\":\"1500\"}'\n    curl -X POST \"http://ip:status_port/config\" -H \"accept: application/json\" -d '{\"split.byte-threshold\":\"15728640\"}'\n    curl -X POST \"http://ip:status_port/config\" -H \"accept: application/json\" -d '{\"split.region-cpu-overload-threshold-ratio\":\"0.5\"}'\n    ```\n\n同理，目前也有两种办法查看配置：\n\n- 通过 SQL 查看，例如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    show config where type='tikv' and name like '%split.qps-threshold%'\n    ```\n\n- 通过 TiKV 查看，例如：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    curl \"http://ip:status_port/config\"\n    ```\n\n> **注意：**\n>\n> 从 v4.0.0-rc.2 起可以使用 SQL 语句来修改和查看配置。\n"
        },
        {
          "name": "configure-memory-usage.md",
          "type": "blob",
          "size": 18.51953125,
          "content": "---\ntitle: TiDB 内存控制文档\naliases: ['/docs-cn/dev/configure-memory-usage/','/docs-cn/dev/how-to/configure/memory-control/']\nsummary: TiDB 内存控制文档介绍了如何追踪和控制 SQL 查询过程中的内存使用情况，以及配置内存使用阈值和 tidb-server 实例的内存使用阈值。还介绍了使用 INFORMATION_SCHEMA 系统表查看内存使用情况，以及降低写入事务内存使用的方法。另外还介绍了流量控制和数据落盘的内存控制策略，以及通过设置环境变量 GOMEMLIMIT 缓解 OOM 问题。\n---\n\n# TiDB 内存控制文档\n\n目前 TiDB 已经能够做到追踪单条 SQL 查询过程中的内存使用情况，当内存使用超过一定阈值后也能采取一些操作来预防 OOM 或者排查 OOM 原因。你可以使用系统变量 [`tidb_mem_oom_action`](/system-variables.md#tidb_mem_oom_action-从-v610-版本开始引入) 来控制查询超过内存限制后所采取的操作：\n\n- 如果变量值为 `LOG`，那么当一条 SQL 的内存使用超过一定阈值（由 session 变量 `tidb_mem_quota_query` 控制）后，这条 SQL 会继续执行，但 TiDB 会在 log 文件中打印一条 LOG。\n- 如果变量值为 `CANCEL`，那么当一条 SQL 的内存使用超过一定阈值后，TiDB 会立即中断这条 SQL 的执行，并给客户端返回一个错误，错误信息中会详细写明在这条 SQL 执行过程中占用内存的各个物理执行算子的内存使用情况。\n\n## 如何配置一条 SQL 执行过程中的内存使用阈值\n\n使用系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 来配置一条 SQL 执行过程中的内存使用阈值，单位为字节。例如：\n\n配置整条 SQL 的内存使用阈值为 8GB：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET tidb_mem_quota_query = 8 << 30;\n```\n\n配置整条 SQL 的内存使用阈值为 8MB：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET tidb_mem_quota_query = 8 << 20;\n```\n\n配置整条 SQL 的内存使用阈值为 8KB：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET tidb_mem_quota_query = 8 << 10;\n```\n\n## 如何配置 tidb-server 实例使用内存的阈值\n\n自 v6.5.0 版本起，可以通过系统变量 [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-从-v640-版本开始引入) 设置 tidb-server 实例的内存使用阈值。\n\n例如，配置 tidb-server 实例的内存使用总量，将其设置成为 32 GB：\n\n{{< copyable \"\" >}}\n\n```sql\nSET GLOBAL tidb_server_memory_limit = \"32GB\";\n```\n\n设置该变量后，当 tidb-server 实例的内存用量达到 32 GB 时，TiDB 会依次终止正在执行的 SQL 操作中内存用量最大的 SQL 操作，直至 tidb-server 实例内存使用下降到 32 GB 以下。被强制终止的 SQL 操作会向客户端返回报错信息 `Out Of Memory Quota!`。\n\n当前 `tidb_server_memory_limit` 所设的内存限制**不终止**以下 SQL 操作：\n\n- DDL 操作\n- 包含窗口函数和公共表表达式的 SQL 操作\n\n> **警告：**\n>\n> + TiDB 在启动过程中不保证 [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-从-v640-版本开始引入) 限制生效。如果操作系统的空闲内存不足，TiDB 仍有可能出现 OOM。你需要确保 TiDB 实例有足够的可用内存。\n> + 在内存控制过程中，TiDB 的整体内存使用量可能会略微超过 `tidb_server_memory_limit` 的限制。\n> + `server-memory-quota` 配置项自 v6.5.0 起被废弃。为了保证兼容性，在升级到 v6.5.0 或更高版本的集群后，`tidb_server_memory_limit` 会继承配置项 `server-memory-quota` 的值。如果集群在升级至 v6.5.0 或更高版本前没有配置 `server-memory-quota`，`tidb_server_memory_limit` 会使用默认值，即 `80%`。\n\n在 tidb-server 实例内存用量到达总内存的一定比例时（比例由系统变量 [`tidb_server_memory_limit_gc_trigger`](/system-variables.md#tidb_server_memory_limit_gc_trigger-从-v640-版本开始引入) 控制）, tidb-server 会尝试主动触发一次 Golang GC 以缓解内存压力。为了避免实例内存在阈值上下范围不断波动导致频繁 GC 进而带来的性能问题，该 GC 方式 1 分钟最多只会触发 1 次。\n\n> **注意：**\n>\n> 在混合部署的情况下，`tidb_server_memory_limit` 为单个 tidb-server 实例的内存使用阈值，而不是整个物理机的总内存阈值。\n\n## 使用 INFORMATION_SCHEMA 系统表查看当前 tidb-server 的内存用量\n\n要查看当前实例或集群的内存使用情况，你可以查询系统表 [`INFORMATION_SCHEMA.(CLUSTER_)MEMORY_USAGE`](/information-schema/information-schema-memory-usage.md)。\n\n要查看本实例或集群中内存相关的操作和执行依据，可以查询系统表 [`INFORMATION_SCHEMA.(CLUSTER_)MEMORY_USAGE_OPS_HISTORY`](/information-schema/information-schema-memory-usage-ops-history.md)。对于每个实例，该表保留最近 50 条记录。\n\n## tidb-server 内存占用过高时的报警\n\n当 tidb-server 实例的内存使用量超过内存阈值（默认为总内存量的 70%）且满足以下任一条件时，TiDB 将记录相关状态文件，并打印报警日志。\n\n- 第一次内存使用量超过内存阈值。\n- 内存使用量超过内存阈值，且距离上一次报警超过 60 秒。\n- 内存使用量超过内存阈值，且 `(本次内存使用量 - 上次报警时内存使用量) / 总内存量 > 10%`。\n\n你可以通过系统变量 [`tidb_memory_usage_alarm_ratio`](/system-variables.md#tidb_memory_usage_alarm_ratio) 修改触发该报警的内存使用比率，从而控制内存报警的阈值。\n\n当触发 tidb-server 内存占用过高的报警时，TiDB 的报警行为如下：\n\n- TiDB 将以下信息记录到 TiDB 日志文件 [`filename`](/tidb-configuration-file.md#filename) 所在目录中。\n\n    - 当前正在执行的所有 SQL 语句中内存使用最高的 10 条语句和运行时间最长的 10 条语句的相关信息\n    - goroutine 栈信息\n    - 堆内存使用状态\n\n- TiDB 将输出一条包含关键字 `tidb-server has the risk of OOM` 以及以下内存相关系统变量的日志。\n\n    - [`tidb_mem_oom_action`](/system-variables.md#tidb_mem_oom_action-从-v610-版本开始引入)\n    - [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query)\n    - [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-从-v640-版本开始引入)\n    - [`tidb_analyze_version`](/system-variables.md#tidb_analyze_version-从-v510-版本开始引入)\n    - [`tidb_enable_rate_limit_action`](/system-variables.md#tidb_enable_rate_limit_action)\n\n为避免报警时产生的状态文件累积过多，目前 TiDB 默认只保留最近 5 次报警时所生成的状态文件。你可以通过配置系统变量 [`tidb_memory_usage_alarm_keep_record_num`](/system-variables.md#tidb_memory_usage_alarm_keep_record_num-从-v640-版本开始引入) 调整该次数。\n\n下例通过构造一个占用大量内存的 SQL 语句触发报警，对该报警功能进行演示：\n\n1. 配置报警比例为 `0.85`：\n\n    {{< copyable \"\" >}}\n\n    ```sql\n    SET GLOBAL tidb_memory_usage_alarm_ratio = 0.85;\n    ```\n\n2. 创建单表 `CREATE TABLE t(a int);` 并插入 1000 行数据。\n\n3. 执行 `select * from t t1 join t t2 join t t3 order by t1.a`。该 SQL 语句会输出 1000000000 条记录，占用巨大的内存，进而触发报警。\n\n4. 检查 `tidb.log` 文件，其中会记录系统总内存、系统当前内存使用量、tidb-server 实例的内存使用量以及状态文件所在目录。\n\n    ```\n    [2022/10/11 16:39:02.281 +08:00] [WARN] [memoryusagealarm.go:212] [\"tidb-server has the risk of OOM because of memory usage exceeds alarm ratio. Running SQLs and heap profile will be recorded in record path\"] [\"is tidb_server_memory_limit set\"=false] [\"system memory total\"=33682427904] [\"system memory usage\"=22120655360] [\"tidb-server memory usage\"=21468556992] [memory-usage-alarm-ratio=0.85] [\"record path\"=/tiup/deploy/tidb-4000/log/oom_record]\n    ```\n\n    以上 Log 字段的含义如下：\n\n    * `is tidb_server_memory_limit set`：表示系统变量 [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-从-v640-版本开始引入) 是否被设置\n    * `system memory total`：表示当前系统的总内存\n    * `system memory usage`：表示当前系统的内存使用量\n    * `tidb-server memory usage`：表示 tidb-server 实例的内存使用量\n    * `memory-usage-alarm-ratio`：表示系统变量 [`tidb_memory_usage_alarm_ratio`](/system-variables.md#tidb_memory_usage_alarm_ratio) 的值\n    * `record path`：表示状态文件存放的目录\n\n5. 通过访问状态文件所在目录（该示例中的目录为 `/tiup/deploy/tidb-4000/log/oom_record`），可以看到标记了记录时间的 record 目录（例：`record2022-10-09T17:18:38+08:00`），其中包括 `goroutinue`、`heap`、`running_sql` 3 个文件，文件以记录状态文件的时间为后缀。这 3 个文件分别用来记录报警时的 goroutine 栈信息，堆内存使用状态，及正在运行的 SQL 信息。其中 `running_sql` 文件内容请参考 [`expensive-queries`](/identify-expensive-queries.md)。\n\n## 如何降低 tidb-server 写入事务的内存使用\n\nTiDB 采用的事务模型要求，所有待提交的事务写入操作需先在内存中进行缓存。在写入大的事务时，内存使用可能会增加并成为瓶颈。为了减少或避免大事务使用大量内存，你可以在满足各项限制条件的前提下通过调整 [`tidb_dml_type`](/system-variables.md#tidb_dml_type-从-v800-版本开始引入) 为 `\"bulk\"` 或使用[非事务 DML 语句](/non-transactional-dml.md)的方式来实现。\n\n## tidb-server 其它内存控制策略\n\n### 流量控制\n\n- TiDB 支持对读数据算子的动态内存控制功能。读数据的算子默认启用 [`tidb_distsql_scan_concurrency`](/system-variables.md#tidb_distsql_scan_concurrency) 所允许的最大线程数来读取数据。当单条 SQL 语句的内存使用每超过 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 一次，读数据的算子就会停止一个线程。\n- 流控行为由参数 [`tidb_enable_rate_limit_action`](/system-variables.md#tidb_enable_rate_limit_action) 控制。\n- 当流控被触发时，会在日志中打印一条包含关键字 `memory exceeds quota, destroy one token now` 的日志。\n\n### 数据落盘\n\nTiDB 支持对执行算子的数据落盘功能。当 SQL 的内存使用超过 Memory Quota 时，tidb-server 可以通过落盘执行算子的中间数据，缓解内存压力。支持落盘的算子有：Sort、MergeJoin、HashJoin、HashAgg。\n\n- 落盘行为由参数 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query)、[`tidb_enable_tmp_storage_on_oom`](/system-variables.md#tidb_enable_tmp_storage_on_oom)、[`tmp-storage-path`](/tidb-configuration-file.md#tmp-storage-path)、[`tmp-storage-quota`](/tidb-configuration-file.md#tmp-storage-quota) 共同控制。\n- 当落盘被触发时，TiDB 会在日志中打印一条包含关键字 `memory exceeds quota, spill to disk now` 或 `memory exceeds quota, set aggregate mode to spill-mode` 的日志。\n- Sort、MergeJoin、HashJoin 落盘是从 v4.0.0 版本开始引入的，非并行 HashAgg 的落盘是从 v5.2.0 版本开始引入的，并行 HashAgg 的落盘在 v8.0.0 版本以实验特性引入，在 v8.2.0 版本成为正式功能 (GA)。TopN 的落盘从 v8.3.0 版本开始引入。\n- 你可以通过系统变量 [`tidb_enable_parallel_hashagg_spill`](/system-variables.md#tidb_enable_parallel_hashagg_spill-从-v800-版本开始引入) 控制是否启用支持落盘的并行 HashAgg 算法。该变量将在未来版本中废弃。\n- 当包含 Sort、MergeJoin、HashJoin、HashAgg 或 TopN 的 SQL 语句引起内存 OOM 时，TiDB 默认会触发落盘。\n\n> **注意：**\n>\n> + HashAgg 落盘功能目前不支持 distinct 聚合函数。使用 distinct 函数且内存占用过大时，无法进行落盘。\n\n本示例通过构造一个占用大量内存的 SQL 语句，对 HashAgg 落盘功能进行演示：\n\n1. 将 SQL 语句的 Memory Quota 配置为 1GB（默认 1GB）：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SET tidb_mem_quota_query = 1 << 30;\n    ```\n\n2. 创建单表 `CREATE TABLE t(a int);` 并插入 256 行不同的数据。\n\n3. 尝试执行以下 SQL 语句：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    [tidb]> explain analyze select /*+ HASH_AGG() */ count(*) from t t1 join t t2 join t t3 group by t1.a, t2.a, t3.a;\n    ```\n\n    该 SQL 语句占用大量内存，返回 Out of Memory Quota 错误。\n\n    ```sql\n    ERROR 1105 (HY000): Out Of Memory Quota![conn_id=3]\n    ```\n\n4. 执行相同的 SQL 语句，不再返回错误，可以执行成功。从详细的执行计划可以看出，HashAgg 使用了 600MB 的硬盘空间。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    [tidb]> explain analyze select /*+ HASH_AGG() */ count(*) from t t1 join t t2 join t t3 group by t1.a, t2.a, t3.a;\n    ```\n\n    ```sql\n    +---------------------------------+-------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------+----------+\n    | id                              | estRows     | actRows  | task      | access object | execution info                                                                                                                                                      | operator info                                                   | memory    | disk     |\n    +---------------------------------+-------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------+----------+\n    | HashAgg_11                      | 204.80      | 16777216 | root      |               | time:1m37.4s, loops:16385                                                                                                                                           | group by:test.t.a, test.t.a, test.t.a, funcs:count(1)->Column#7 | 1.13 GB   | 600.0 MB |\n    | └─HashJoin_12                   | 16777216.00 | 16777216 | root      |               | time:21.5s, loops:16385, build_hash_table:{total:267.2µs, fetch:228.9µs, build:38.2µs}, probe:{concurrency:1, total:35s, max:35s, probe:35s, fetch:962.2µs}         | CARTESIAN inner join                                            | 8.23 KB   | 4 KB     |\n    |   ├─TableReader_21(Build)       | 256.00      | 256      | root      |               | time:87.2µs, loops:2, cop_task: {num: 1, max: 150µs, proc_keys: 0, rpc_num: 1, rpc_time: 145.1µs, copr_cache_hit_ratio: 0.00}                                       | data:TableFullScan_20                                           | 885 Bytes | N/A      |\n    |   │ └─TableFullScan_20          | 256.00      | 256      | cop[tikv] | table:t3      | tikv_task:{time:23.2µs, loops:256}                                                                                                                                  | keep order:false, stats:pseudo                                  | N/A       | N/A      |\n    |   └─HashJoin_14(Probe)          | 65536.00    | 65536    | root      |               | time:728.1µs, loops:65, build_hash_table:{total:307.5µs, fetch:277.6µs, build:29.9µs}, probe:{concurrency:1, total:34.3s, max:34.3s, probe:34.3s, fetch:278µs}      | CARTESIAN inner join                                            | 8.23 KB   | 4 KB     |\n    |     ├─TableReader_19(Build)     | 256.00      | 256      | root      |               | time:126.2µs, loops:2, cop_task: {num: 1, max: 308.4µs, proc_keys: 0, rpc_num: 1, rpc_time: 295.3µs, copr_cache_hit_ratio: 0.00}                                    | data:TableFullScan_18                                           | 885 Bytes | N/A      |\n    |     │ └─TableFullScan_18        | 256.00      | 256      | cop[tikv] | table:t2      | tikv_task:{time:79.2µs, loops:256}                                                                                                                                  | keep order:false, stats:pseudo                                  | N/A       | N/A      |\n    |     └─TableReader_17(Probe)     | 256.00      | 256      | root      |               | time:211.1µs, loops:2, cop_task: {num: 1, max: 295.5µs, proc_keys: 0, rpc_num: 1, rpc_time: 279.7µs, copr_cache_hit_ratio: 0.00}                                    | data:TableFullScan_16                                           | 885 Bytes | N/A      |\n    |       └─TableFullScan_16        | 256.00      | 256      | cop[tikv] | table:t1      | tikv_task:{time:71.4µs, loops:256}                                                                                                                                  | keep order:false, stats:pseudo                                  | N/A       | N/A      |\n    +---------------------------------+-------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------+----------+\n    9 rows in set (1 min 37.428 sec)\n    ```\n\n## 其它\n\n### 设置环境变量 `GOMEMLIMIT` 缓解 OOM 问题\n\nGolang 自 Go 1.19 版本开始引入 [`GOMEMLIMIT`](https://pkg.go.dev/runtime@go1.19#hdr-Environment_Variables) 环境变量，该变量用来设置触发 Go GC 的内存上限。\n\n对于 v6.1.3 <= TiDB < v6.5.0 的版本，你可以通过手动设置 Go `GOMEMLIMIT` 环境变量的方式来缓解一类 OOM 问题。该类 OOM 问题具有一个典型特征：观察 Grafana 监控，OOM 前的时刻，TiDB-Runtime > Memory Usage 面板中 **estimate-inuse** 立柱部分在整个立柱中仅仅占一半。如下图所示：\n\n![normal OOM case example](/media/configure-memory-usage-oom-example.png)\n\n为了验证 `GOMEMLIMIT` 在该类场景下的效果，以下通过一个对比实验进行说明：\n\n- 在 TiDB v6.1.2 下，模拟负载在持续运行几分钟后，TiDB server 会发生 OOM（系统内存约 48 GiB）：\n\n    ![v6.1.2 workload oom](/media/configure-memory-usage-612-oom.png)\n\n- 在 TiDB v6.1.3 下，设置 `GOMEMLIMIT` 为 40000 MiB，模拟负载长期稳定运行、TiDB server 未发生 OOM 且进程最高内存用量稳定在 40.8 GiB 左右：\n\n    ![v6.1.3 workload no oom with GOMEMLIMIT](/media/configure-memory-usage-613-no-oom.png)\n"
        },
        {
          "name": "configure-placement-rules.md",
          "type": "blob",
          "size": 17.3125,
          "content": "---\ntitle: Placement Rules 使用文档\nsummary: 如何配置 Placement Rules\naliases: ['/docs-cn/dev/configure-placement-rules/','/docs-cn/dev/how-to/configure/placement-rules/']\n---\n\n# Placement Rules 使用文档\n\n> **注意：**\n>\n> 本文介绍如何手动在 Placement Driver (PD) 中设置 Placement Rules。推荐使用 [Placement Rules in SQL](/placement-rules-in-sql.md)，让你更方便地设置表和分区的放置。\n\nPlacement Rules 是 PD 在 4.0 版本引入的一套副本规则系统，用于指导 PD 针对不同类型的数据生成对应的调度。通过组合不同的调度规则，用户可以精细地控制任何一段连续数据的副本数量、存放位置、主机类型、是否参与 Raft 投票、是否可以担任 Raft leader 等属性。\n\nPlacement Rules 特性在 TiDB v5.0 及以上的版本中默认开启。如需关闭 Placement Rules 特性，请参考[关闭 Placement Rules](#关闭-placement-rules-特性)。\n\n## 规则系统介绍\n\n整个规则系统的配置由多条规则即 Rule 组成。每条 Rule 可以指定不同的副本数量、Raft 角色、放置位置等属性，以及这条规则生效的 key range。PD 在进行调度时，会先根据 Region 的 key range 在规则系统中查到该 Region 对应的规则，然后再生成对应的调度，来使得 Region 副本的分布情况符合 Rule。\n\n多条规则的 key range 可以有重叠部分的，即一个 Region 能匹配到多条规则。这种情况下 PD 根据 Rule 的属性来决定规则是相互覆盖还是同时生效。如果有多条规则同时生效，PD 会按照规则的堆叠次序依次去生成调度进行规则匹配。\n\n此外，为了满足不同来源的规则相互隔离的需求，支持更灵活的方式来组织规则，还引入了分组 (Group) 的概念。通常情况下，用户可根据规则的不同来源把规则放置在不同的 Group。\n\nPlacement Rules 示意图如下所示：\n\n![Placement rules overview](/media/placement-rules-1.png)\n\n### 规则字段\n\n以下是每条规则中各个字段的具体含义：\n\n| 字段名           | 类型及约束                      | 说明                                |\n| :---            | :---                           | :---                                |\n| `GroupID`         | `string`                         | 分组 ID，标识规则的来源               |\n| `ID`              | `string`                         | 分组内唯一 ID                        |\n| `Index`           | `int`                            | 分组内堆叠次序                       |\n| `Override`        | `true`/`false`                     | 是否覆盖 index 的更小 Rule（限分组内） |\n| `StartKey`        | `string`，十六进制编码                | 适用 Range 起始 key                 |\n| `EndKey`          | `string`，十六进制编码                | 适用 Range 终止 key                 |\n| `Role`            | `string` | 副本角色，包括 voter/leader/follower/learner                           |\n| `Count`           | `int`，正整数                     | 副本数量                            |\n| `LabelConstraint` | `[]Constraint`                    | 用于按 label 筛选节点               |\n| `LocationLabels`  | `[]string`                        | 用于物理隔离                        |\n| `IsolationLevel`  | `string`                          | 用于设置最小强制物理隔离级别             |\n\n`LabelConstraint` 与 Kubernetes 中的功能类似，支持通过 `in`、`notIn`、`exists` 和 `notExists` 四种原语来筛选 label。这四种原语的意义如下：\n\n+ `in`：给定 key 的 label value 包含在给定列表中。\n+ `notIn`：给定 key 的 label value 不包含在给定列表中。\n+ `exists`：包含给定的 label key。\n+ `notExists`：不包含给定的 label key。\n\n`LocationLabels` 的意义和作用与 PD v4.0 之前的版本相同。比如配置 `[zone,rack,host]` 定义了三层的拓扑结构：集群分为多个 zone（可用区），每个 zone 下有多个 rack（机架），每个 rack 下有多个 host（主机）。PD 在调度时首先会尝试将 Region 的 Peer 放置在不同的 zone，假如无法满足（比如配置 3 副本但总共只有 2 个 zone）则保证放置在不同的 rack；假如 rack 的数量也不足以保证隔离，那么再尝试 host 级别的隔离，以此类推。\n\n`IsolationLevel` 的意义和作用详细请参考[配置集群拓扑](/schedule-replicas-by-topology-labels.md)。例如已配置 `LocationLabels` 为 `[zone,rack,host]` 的前提下，设置 `IsolationLevel` 为 `zone`，则 PD 在调度时会保证每个 Region 的所有 Peer 均被放置在不同的 zone。假如无法满足 `IsolationLevel` 的最小强制隔离级别限制（比如配置 3 副本但总共只有 2 个 zone），PD 也不会尝试补足，以满足该限制。`IsolationLevel` 默认值为空字符串，即禁用状态。\n\n### 规则分组字段\n\n以下是规则分组字段的含义：\n\n| 字段名 | 类型及约束  | 说明 |\n| :--- | :--- | :--- |\n| `ID` | `string` | 分组 ID，用于标识规则来源 |\n| `Index` | `int` | 不同分组的堆叠次序 |\n| `Override` | `true`/`false` | 是否覆盖 index 更小的分组 |\n\n如果不单独设置规则分组，默认 `Override=false`，对应的行为是不同分组之间相互不影响，不同分组内的规则是同时生效的。\n\n## 配置规则操作步骤\n\n本节的操作步骤以使用 [pd-ctl](/pd-control.md) 工具为例，涉及到的命令也支持通过 HTTP API 进行调用。\n\n### 开启 Placement Rules 特性\n\nPlacement Rules 特性在 TiDB v5.0 及以上的版本中默认开启。如需关闭 Placement Rules 特性，请参考[关闭 Placement Rules](#关闭-placement-rules-特性)。如需在关闭后重新开启该特性，可以集群初始化以前设置 PD 配置文件：\n\n{{< copyable \"\" >}}\n\n```toml\n[replication]\nenable-placement-rules = true\n```\n\n这样，PD 在初始化成功后会开启这个特性，并根据 [`max-replicas`](/pd-configuration-file.md#max-replicas)、[`location-labels`](/pd-configuration-file.md#location-labels) 及 [`isolation-level`](/pd-configuration-file.md#isolation-level) 配置生成对应的规则：\n\n{{< copyable \"\" >}}\n\n```json\n{\n  \"group_id\": \"pd\",\n  \"id\": \"default\",\n  \"start_key\": \"\",\n  \"end_key\": \"\",\n  \"role\": \"voter\",\n  \"count\": 3,\n  \"location_labels\": [\"zone\", \"rack\", \"host\"],\n  \"isolation_level\": \"\"\n}\n```\n\n如果是已经初始化过的集群，也可以通过 pd-ctl 进行在线开启：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config placement-rules enable\n```\n\nPD 同样将根据系统的 `max-replicas`、`location-labels` 及 `isolation-level` 生成默认的规则。\n\n> **注意：**\n>\n> - 开启 Placement Rules 且存在多条 rule 的情况下，原先的 `max-replicas`、`location-labels` 及 `isolation-level` 配置项将不再生效。如果需要调整副本策略，应当使用 Placement Rules 相关接口。\n> - 开启 Placement Rules 且只存在一条默认的 rule 的情况下，当改变 `max-replicas`、`location-labels` 或 `isolation-level` 配置项时，系统会自动更新这条默认的 rule。\n\n### 关闭 Placement Rules 特性\n\n使用 pd-ctl 可以关闭 Placement Rules 特性，切换为之前的调度策略。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config placement-rules disable\n```\n\n> **注意：**\n>\n> 关闭 Placement Rules 后，PD 将使用原先的 `max-replicas`、`location-labels` 及 `isolation-level` 配置。在 Placement Rules 开启期间对 Rule 的修改不会导致这三项配置的同步更新。此外，设置好的所有 Rule 都会保留在系统中，会在下次开启 Placement Rules 时被使用。\n\n### 使用 pd-ctl 设置规则\n\n> **注意：**\n>\n> 规则的变更将实时地影响 PD 调度，不恰当的规则设置可能导致副本数较少，影响系统的高可用。\n\npd-ctl 支持使用多种方式查看系统中的 Rule，输出是 json 格式的 Rule 或 Rule 列表：\n\n+ 查看所有规则列表\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config placement-rules show\n    ```\n\n+ 查看 PD Group 的所有规则列表\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config placement-rules show --group=pd\n    ```\n\n+ 查看对应 Group 和 ID 的某条规则\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config placement-rules show --group=pd --id=default\n    ```\n\n+ 查看 Region 所匹配的规则列表\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config placement-rules show --region=2\n    ```\n\n    上面的例子中 `2` 为 Region ID。\n\n新增和编辑规则是类似的，需要把对应的规则写进文件，然后使用 `save` 命令保存至 PD：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ncat > rules.json <<EOF\n[\n    {\n        \"group_id\": \"pd\",\n        \"id\": \"rule1\",\n        \"role\": \"voter\",\n        \"count\": 3,\n        \"location_labels\": [\"zone\", \"rack\", \"host\"]\n    },\n    {\n        \"group_id\": \"pd\",\n        \"id\": \"rule2\",\n        \"role\": \"voter\",\n        \"count\": 2,\n        \"location_labels\": [\"zone\", \"rack\", \"host\"]\n    }\n]\nEOF\n\n» ./pd-ctl -u 127.0.0.1:2379 config placement-rules save --in=rules.json\nSuccess!\n```\n\n以上操作会将 rule1、rule2 两条规则写入 PD，如果系统中已经存在 GroupID+ID 相同的规则，则会覆盖该规则。\n\n如果需要删除某条规则，只需要将规则的 `count` 置为 `0` 即可，对应 GroupID+ID 相同的规则会被删除。以下命令将删除 `pd/rule2` 这条规则：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ncat > rules.json <<EOF\n[\n    {\n        \"group_id\": \"pd\",\n        \"id\": \"rule2\"\n    }\n]\nEOF\n\n» ./pd-ctl -u 127.0.0.1:2379 config placement-rules save --in=rules.json\nSuccess!\n```\n\n### 使用 pd-ctl 设置规则分组\n\n+ 查看所有的规则分组列表\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config placement-rules rule-group show\n    ```\n\n+ 查看指定 ID 的规则分组\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config placement-rules rule-group show pd\n    ```\n\n+ 设置规则分组的 index 和 override 属性\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config placement-rules rule-group set pd 100 true\n    ```\n\n+ 删除规则分组配置（如组内还有规则，则使用默认分组配置）\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config placement-rules rule-group delete pd\n    ```\n\n### 使用 pd-ctl 批量更新分组及组内规则\n\n使用 `rule-bundle` 子命令，可以方便地同时查看和修改规则分组及组内的所有规则。\n\n该子命令中 `get {group_id}` 用来查询一个分组，输出结果为嵌套形式的规则分组和组内规则：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config placement-rules rule-bundle get pd\n```\n\n输出示例：\n\n```json\n{\n  \"group_id\": \"pd\",\n  \"group_index\": 0,\n  \"group_override\": false,\n  \"rules\": [\n    {\n      \"group_id\": \"pd\",\n      \"id\": \"default\",\n      \"start_key\": \"\",\n      \"end_key\": \"\",\n      \"role\": \"voter\",\n      \"count\": 3\n    }\n  ]\n}\n```\n\n`rule-bundle get` 子命令中可以添加 `--out` 参数来将输出写入文件，方便后续修改保存。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config placement-rules rule-bundle get pd --out=\"group.json\"\n```\n\n修改完成后，使用 `rule-bundle set` 子命令将文件中的配置保存至 PD 服务器。与前面介绍的 `save` 不同，此命令会替换服务器端该分组内的所有规则。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config placement-rules rule-bundle set pd --in=\"group.json\"\n```\n\n### 使用 pd-ctl 查看和修改所有配置\n\n用户还可以使用 pd-ctl 查看和修改所有配置，即把全部配置保存至文件，修改后再覆盖保存。该操作同样使用 `rule-bundle` 子命令。\n\n下面的命令将所有配置保存至 `rules.json` 文件：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config placement-rules rule-bundle load --out=\"rules.json\"\n```\n\n编辑完文件后，使用下面的命令将配置保存至 PD 服务器：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config placement-rules rule-bundle save --in=\"rules.json\"\n```\n\n### 使用 tidb-ctl 查询表相关的 key range\n\n若需要针对元数据或某个特定的表进行特殊配置，可以通过 [tidb-ctl](https://github.com/pingcap/tidb-ctl) 的 [`keyrange` 命令](https://github.com/pingcap/tidb-ctl/blob/master/doc/tidb-ctl_keyrange.md)来查询相关的 key。注意要添加 `--encode` 返回 PD 中的表示形式。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntidb-ctl keyrange --database test --table ttt --encode\n```\n\n```text\nglobal ranges:\n  meta: (6d00000000000000f8, 6e00000000000000f8)\n  table: (7400000000000000f8, 7500000000000000f8)\ntable ttt ranges: (NOTE: key range might be changed after DDL)\n  table: (7480000000000000ff2d00000000000000f8, 7480000000000000ff2e00000000000000f8)\n  table indexes: (7480000000000000ff2d5f690000000000fa, 7480000000000000ff2d5f720000000000fa)\n    index c2: (7480000000000000ff2d5f698000000000ff0000010000000000fa, 7480000000000000ff2d5f698000000000ff0000020000000000fa)\n    index c3: (7480000000000000ff2d5f698000000000ff0000020000000000fa, 7480000000000000ff2d5f698000000000ff0000030000000000fa)\n    index c4: (7480000000000000ff2d5f698000000000ff0000030000000000fa, 7480000000000000ff2d5f698000000000ff0000040000000000fa)\n  table rows: (7480000000000000ff2d5f720000000000fa, 7480000000000000ff2e00000000000000f8)\n```\n\n> **注意：**\n>\n> DDL 等操作会导致 table ID 发生变化，需要同步更新对应的规则。\n\n## 典型场景示例\n\n本部分介绍 Placement Rules 的使用场景示例。\n\n### 场景一：普通的表使用 3 副本，元数据使用 5 副本提升集群容灾能力\n\n只需要增加一条规则，将 key range 限定在 meta 数据的范围，并把 `count` 值设为 `5`。添加规则示例如下：\n\n{{< copyable \"\" >}}\n\n```json\n{\n  \"group_id\": \"pd\",\n  \"id\": \"meta\",\n  \"index\": 1,\n  \"override\": true,\n  \"start_key\": \"6d00000000000000f8\",\n  \"end_key\": \"6e00000000000000f8\",\n  \"role\": \"voter\",\n  \"count\": 5,\n  \"location_labels\": [\"zone\", \"rack\", \"host\"]\n}\n```\n\n### 场景二：5 副本按 2-2-1 的比例放置在 3 个数据中心，且第 3 个中心不产生 Leader\n\n创建三条规则，分别设置副本数为 2、2、1，并且在每个规则内通过 `label_constraints` 将副本限定在对应的数据中心内。另外，不需要 leader 的数据中心将 `role` 改为 `follower`。\n\n{{< copyable \"\" >}}\n\n```json\n[\n    {\n        \"group_id\": \"pd\",\n        \"id\": \"zone1\",\n        \"start_key\": \"\",\n        \"end_key\": \"\",\n        \"role\": \"voter\",\n        \"count\": 2,\n        \"label_constraints\": [\n            {\"key\": \"zone\", \"op\": \"in\", \"values\": [\"zone1\"]}\n        ],\n        \"location_labels\": [\"rack\", \"host\"]\n    },\n    {\n        \"group_id\": \"pd\",\n        \"id\": \"zone2\",\n        \"start_key\": \"\",\n        \"end_key\": \"\",\n        \"role\": \"voter\",\n        \"count\": 2,\n        \"label_constraints\": [\n            {\"key\": \"zone\", \"op\": \"in\", \"values\": [\"zone2\"]}\n        ],\n        \"location_labels\": [\"rack\", \"host\"]\n    },\n    {\n        \"group_id\": \"pd\",\n        \"id\": \"zone3\",\n        \"start_key\": \"\",\n        \"end_key\": \"\",\n        \"role\": \"follower\",\n        \"count\": 1,\n        \"label_constraints\": [\n            {\"key\": \"zone\", \"op\": \"in\", \"values\": [\"zone3\"]}\n        ],\n        \"location_labels\": [\"rack\", \"host\"]\n    }\n]\n```\n\n### 场景三：为某张表添加 2 个 TiFlash Learner 副本\n\n为表的 row key 单独添加一条规则，限定数量为 2，并且通过 `label_constraints` 保证副本产生在 `engine=tiflash` 的节点。注意这里使用了单独的 `group_id`，保证这条规则不会与系统中其他来源的规则互相覆盖或产生冲突。\n\n{{< copyable \"\" >}}\n\n```json\n{\n  \"group_id\": \"tiflash\",\n  \"id\": \"learner-replica-table-ttt\",\n  \"start_key\": \"7480000000000000ff2d5f720000000000fa\",\n  \"end_key\": \"7480000000000000ff2e00000000000000f8\",\n  \"role\": \"learner\",\n  \"count\": 2,\n  \"label_constraints\": [\n    {\"key\": \"engine\", \"op\": \"in\", \"values\": [\"tiflash\"]}\n  ],\n  \"location_labels\": [\"host\"]\n}\n```\n\n### 场景四：为某张表在有高性能磁盘的北京节点添加 2 个 Follower 副本\n\n这个例子展示了比较复杂的 `label_constraints` 配置，下面的例子限定了副本放置在 bj1 或 bj2 机房，且磁盘类型为 `nvme`。\n\n{{< copyable \"\" >}}\n\n```json\n{\n  \"group_id\": \"follower-read\",\n  \"id\": \"follower-read-table-ttt\",\n  \"start_key\": \"7480000000000000ff2d00000000000000f8\",\n  \"end_key\": \"7480000000000000ff2e00000000000000f8\",\n  \"role\": \"follower\",\n  \"count\": 2,\n  \"label_constraints\": [\n    {\"key\": \"zone\", \"op\": \"in\", \"values\": [\"bj1\", \"bj2\"]},\n    {\"key\": \"disk\", \"op\": \"in\", \"values\": [\"nvme\"]}\n  ],\n  \"location_labels\": [\"host\"]\n}\n```\n\n### 场景五：将某张表迁移至 SSD 节点\n\n与场景三不同，这个场景不是要在原有配置的基础上增加新副本，而是要强制覆盖一段数据的其它配置，因此需要通过配置规则分组来指定一个足够大的 index 以及设置 override 来覆盖原有规则。\n\n规则：\n\n{{< copyable \"\" >}}\n\n```json\n{\n  \"group_id\": \"ssd-override\",\n  \"id\": \"ssd-table-45\",\n  \"start_key\": \"7480000000000000ff2d5f720000000000fa\",\n  \"end_key\": \"7480000000000000ff2e00000000000000f8\",\n  \"role\": \"voter\",\n  \"count\": 3,\n  \"label_constraints\": [\n    {\"key\": \"disk\", \"op\": \"in\", \"values\": [\"ssd\"]}\n  ],\n  \"location_labels\": [\"rack\", \"host\"]\n}\n```\n\n规则分组：\n\n{{< copyable \"\" >}}\n\n```json\n{\n  \"id\": \"ssd-override\",\n  \"index\": 1024,\n  \"override\": true,\n}\n```\n"
        },
        {
          "name": "configure-store-limit.md",
          "type": "blob",
          "size": 4.326171875,
          "content": "---\ntitle: Store Limit\nsummary: 介绍 Store Limit 功能。\naliases: ['/docs-cn/dev/configure-store-limit/']\n---\n\n# Store Limit\n\nStore Limit 是 PD 在 3.0 版本引入的特性，旨在能够更加细粒度地控制调度的速度，针对不同调度场景进行调优。\n\n## 实现原理\n\nPD 的调度是以 operator 为单位执行的。一个 operator 可能包含多个调度操作。示例如下；\n\n```\n\"replace-down-replica {mv peer: store [2] to [3]} (kind:region,replica, region:10(4,5), createAt:2020-05-18 06:40:25.775636418 +0000 UTC m=+2168762.679540369, startAt:2020-05-18 06:40:25.775684648 +0000 UTC m=+2168762.679588599, currentStep:0, steps:[add learner peer 20 on store 3, promote learner peer 20 on store 3 to voter, remove peer on store 2])\"\n```\n\n以上示例中，`replace-down-replica` 这个 operator 具体包含以下操作：\n\n1. 在 `store 3` 上添加一个 learner peer，ID 为 `20`。\n2. 将 `store 3` 上 ID 为 `20` 的 learner peer 提升为 voter。\n3. 删除 `store 2` 上的 peer。\n\nStore Limit 是通过在内存中维护了一个 store ID 到令牌桶的映射，来实现 store 级别的限速。这里不同的操作对应不同的令牌桶，目前仅支持限制添加 learner/peer 和删除 peer 两种操作的速度，即对应于每个 store 存在两种类型的令牌桶。\n\n每次 operator 产生后会检查所包含的操作对应的令牌桶中是否有足够的 token。如果 token 充足才会将该 operator 加入到调度的队列中，同时从令牌桶中拿走对应的 token，否则该 operator 被丢弃。令牌桶会按照固定的速率补充 token，从而实现限速的目的。\n\nStore Limit 与 PD 其他 limit 相关的参数（如 `region-schedule-limit`，`leader-schedule-limit` 等）不同的是，Store Limit 限制的主要是 operator 的消费速度，而其他的 limit 主要是限制 operator 的产生速度。引入 Store Limit 特性之前，调度的限速主要是全局的，所以即使限制了全局的速度，但还是有可能存在调度都集中在部分 store 上面，因而影响集群的性能。而 Store Limit 通过将限速的粒度进一步细化，可以更好的控制调度的行为。\n\n## 使用方法\n\nStore Limit 相关的参数可以通过 `pd-ctl` 进行设置。\n\n### 查看当前 store 的 limit 设置\n\n查看当前 store 的 limit 示例如下：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nstore limit                         // 显示所有 store 添加和删除 peer 的速度上限。\nstore limit add-peer                // 显示所有 store 添加 peer 的速度上限。\nstore limit remove-peer             // 显示所有 store 删除 peer 的速度上限。\n```\n\n### 设置全部 store 的 limit\n\n设置全部 store 的 limit 示例如下：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nstore limit all 5                   // 设置所有 store 添加和删除 peer 的速度上限为每分钟 5 个。\nstore limit all 5 add-peer          // 设置所有 store 添加 peer 的速度上限为每分钟 5 个。\nstore limit all 5 remove-peer       // 设置所有 store 删除 peer 的速度上限为每分钟 5 个。\n```\n\n### 设置单个 store 的 limit\n\n设置单个 store 的 limit 示例如下：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nstore limit 1 5                     // 设置 store 1 添加和删除 peer 的速度上限为每分钟 5 个。\nstore limit 1 5 add-peer            // 设置 store 1 添加 peer 的速度上限为每分钟 5 个。\nstore limit 1 5 remove-peer         // 设置 store 1 删除 peer 的速度上限为每分钟 5 个。\n```\n\n## Store Limit v2 原理\n\n当 [`store-limit-version`](/pd-configuration-file.md#store-limit-version-从-v710-版本开始引入) 设置为 `v2` 时，Store Limit v2 生效。在此模式下，Operator 调度限制将根据 TiKV Snapshot 执行情况进行动态调整。当 TiKV 积压的任务较少时，PD 会增加其调度任务。相反，PD 会减少对该节点的调度任务。此时，你无需关注如何设置 `store limit` 以加快调度进度。\n\n在该模式下，TiKV 执行速度成为迁移进度的主要瓶颈。你可以通过 **TiKV Details** > **Snapshot** > **Snapshot Speed** 面板判断当前调度速度是否达到 TiKV 限流设置。通过调整 TiKV Snapshot Limit ([`snap-io-max-bytes-per-sec`](/tikv-configuration-file.md#snap-io-max-bytes-per-sec)) 来增加或减少该节点的调度速度。"
        },
        {
          "name": "configure-time-zone.md",
          "type": "blob",
          "size": 4.8076171875,
          "content": "---\ntitle: 时区支持\naliases: ['/docs-cn/dev/configure-time-zone/','/docs-cn/dev/how-to/configure/time-zone/']\nsummary: TiDB 的时区设置由 `time_zone` 系统变量控制，可以在会话级别或全局级别进行设置。`TIMESTAMP` 数据类型的的显示值受时区设置影响，但 `DATETIME`、`DATE` 或 `TIME` 数据类型不受影响。在数据迁移时，需要特别注意主库和从库的时区设置是否一致。\n---\n\n# 时区支持\n\nTiDB 使用的时区由 [`time_zone`](/system-variables.md#time_zone) 系统变量决定，该变量可以在会话级别或全局级别设置。`time_zone` 的默认值为 `SYSTEM`。`SYSTEM` 对应的实际时区是在 TiDB 集群 bootstrap 初始化时配置的，具体逻辑如下：\n\n1. TiDB 优先使用 `TZ` 环境变量。\n2. 如果 `TZ` 环境变量不可用，TiDB 会从 `/etc/localtime` 的软链接中读取时区信息。\n3. 如果上述方法均失败，TiDB 将使用 `UTC` 作为系统时区。\n\n## 查看时区\n\n要查看当前全局时区、客户端时区或系统时区的值，可以执行以下语句：\n\n```sql\nSELECT @@global.time_zone, @@session.time_zone, @@global.system_time_zone;\n```\n\n## 设置时区\n\n在 TiDB 中，`time_zone` 系统变量的值可以设置为以下格式之一：\n\n- `SYSTEM`（默认值），表示使用系统时间。\n- 相对于 UTC 时间的偏移，比如 `'+10:00'` 或 `'-6:00'`。\n- 某个时区的名字，比如 `'Europe/Helsinki'`、`'US/Eastern'` 或 `'MET'`。\n\n根据需要，你可以在全局级别或会话级别设置 TiDB 的时区：\n\n- 设置全局时区：\n\n    ```sql\n    SET GLOBAL time_zone = ${time-zone-value};\n    ```\n\n    例如，执行以下语句可以将全局时区设置为 UTC：\n\n    ```sql\n    SET GLOBAL time_zone = 'UTC';\n    ```\n\n- 设置会话的时区：\n\n    ```sql\n    SET time_zone = ${time-zone-value};\n    ```\n\n    例如，执行以下语句可以将当前会话的时区设置为 US/Pacific：\n\n    ```sql\n    SET time_zone = 'US/Pacific';\n    ```\n\n## 受时区设置影响的函数和数据类型\n\n对于时区敏感的时间值，例如由 [`NOW()`](/functions-and-operators/date-and-time-functions.md) 和 `CURTIME()` 函数返回的值，它们的显示和处理会受到当前会话时区设置的影响。如需进行时区转换，可以使用 `CONVERT_TZ()` 函数。若要获取基于 UTC 的时间戳以避免时区相关问题，可以使用 `UTC_TIMESTAMP()` 函数。\n\n在 TiDB 中，`TIMESTAMP` 数据类型会记录时间戳的具体数值和时区信息，因此它的显示值会受到时区设置的影响。其他数据类型（如 `DATETIME`、`DATE` 和 `TIME`）不记录时区信息，因此它们的值不会受到时区变化的影响。\n\n例如：\n\n```sql\ncreate table t (ts timestamp, dt datetime);\n```\n\n```\nQuery OK, 0 rows affected (0.02 sec)\n```\n\n```sql\nset @@time_zone = 'UTC';\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\ninsert into t values ('2017-09-30 11:11:11', '2017-09-30 11:11:11');\n```\n\n```\nQuery OK, 1 row affected (0.00 sec)\n```\n\n```sql\nset @@time_zone = '+8:00';\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nselect * from t;\n```\n\n```\n+---------------------|---------------------+\n| ts                  | dt                  |\n+---------------------|---------------------+\n| 2017-09-30 19:11:11 | 2017-09-30 11:11:11 |\n+---------------------|---------------------+\n1 row in set (0.00 sec)\n```\n\n在以上示例中，无论如何调整时区值，`DATETIME` 数据类型的值不会受到影响。然而，`TIMESTAMP` 数据类型的显示值会根据时区的变化而变化。事实上，存储在数据库中的 `TIMESTAMP` 值始终没有变化过，只是根据时区的不同显示的值不同。\n\n## 时区设置的注意事项\n\n- 在 `TIMESTAMP` 和 `DATETIME` 值的转换过程中，会涉及到时区。这种情况一律基于当前会话的 `time_zone` 时区处理。\n- 数据迁移时，需要特别注意主库和从库的时区设置是否一致。\n- 为了获取准确的时间戳，强烈建议使用网络时间协议 (NTP) 或精确时间协议 (PTP) 服务配置可靠的时钟。有关如何检查 NTP 服务的信息，请参考[检测及安装 NTP 服务](/check-before-deployment.md#检测及安装-ntp-服务)。\n- 当使用遵循夏令时 (Daylight Saving Time, DST) 的时区时，请注意可能出现时间戳不明确或不存在的情况，特别是在对这些时间戳进行计算时。\n- MySQL 需要使用 [`mysql_tzinfo_to_sql`](https://dev.mysql.com/doc/refman/8.4/en/mysql-tzinfo-to-sql.html) 将操作系统的时区数据库转换为 `mysql` 数据库中的表。TiDB 则可以利用 Go 编程语言的内置时区处理能力，直接从操作系统的时区数据库中读取时区数据文件。\n\n## 另请参阅\n\n- [日期和时间类型](/data-type-date-and-time.md)\n- [日期和时间函数](/functions-and-operators/date-and-time-functions.md)"
        },
        {
          "name": "constraints.md",
          "type": "blob",
          "size": 16.654296875,
          "content": "---\ntitle: 约束\naliases: ['/docs-cn/dev/constraints/','/docs-cn/dev/reference/sql/constraints/']\nsummary: TiDB 支持的约束与 MySQL 基本相同，包括非空约束和 CHECK 约束。非空约束规则与 MySQL 相同，而 CHECK 约束需要在 tidb_enable_check_constraint 设置为 ON 后才能开启。可以通过 CREATE TABLE 或 ALTER TABLE 语句添加 CHECK 约束。唯一约束和主键约束也与 MySQL 相似，但 TiDB 目前仅支持对 NONCLUSTERED 的主键进行添加和删除操作。外键约束从 v6.6.0 开始支持，可以使用 CREATE TABLE 和 ALTER TABLE 命令来添加和删除外键。\n---\n\n# 约束\n\nTiDB 支持的约束与 MySQL 的基本相同。\n\n## 非空约束\n\nTiDB 支持的非空约束规则与 MySQL 支持的一致。例如：\n\n```sql\nCREATE TABLE users (\n id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n age INT NOT NULL,\n last_login TIMESTAMP\n);\n```\n\n```sql\nINSERT INTO users (id,age,last_login) VALUES (NULL,123,NOW());\n```\n\n```\nQuery OK, 1 row affected (0.02 sec)\n```\n\n```sql\nINSERT INTO users (id,age,last_login) VALUES (NULL,NULL,NOW());\n```\n\n```\nERROR 1048 (23000): Column 'age' cannot be null\n```\n\n```sql\nINSERT INTO users (id,age,last_login) VALUES (NULL,123,NULL);\n```\n\n```\nQuery OK, 1 row affected (0.03 sec)\n```\n\n* 第一条 `INSERT` 语句成功，因为对于定义为 `AUTO_INCREMENT` 的列，允许 `NULL` 作为其特殊值。TiDB 将为其分配下一个自动值。\n\n* 第二条 `INSERT` 语句失败，因为 `age` 列被定义为 `NOT NULL`。\n\n* 第三条 `INSERT` 语句成功，因为 `last_login` 列没有被明确地指定为 `NOT NULL`。默认允许 `NULL` 值。\n\n## `CHECK` 约束\n\n> **注意：**\n>\n> `CHECK` 约束功能默认关闭，需要将变量 [`tidb_enable_check_constraint`](/system-variables.md#tidb_enable_check_constraint-从-v720-版本开始引入) 设置为 `ON` 后才能开启。\n\n`CHECK` 约束用于限制表中某个字段的值必须满足指定条件。当为表添加 `CHECK` 约束后，在插入或者更新表的数据时，TiDB 会检查约束条件是否满足，如果不满足，则会报错。\n\nTiDB 中 `CHECK` 约束的语法如下，与 MySQL 中一致：\n\n```sql\n[CONSTRAINT [symbol]] CHECK (expr) [[NOT] ENFORCED]\n```\n\n语法说明：\n\n- `[]` 中的内容表示可选项。\n- `CONSTRAINT [symbol]` 表示 `CHECK` 约束的名称。\n- `CHECK (expr)` 表示约束条件，其中 `expr` 需要为一个布尔表达式。对于表中的每一行，该表达式的计算结果必须为 `TRUE`、`FALSE` 或 `UNKNOWN` (对于 `NULL` 值) 中的一个。对于某行数据，如果该表达式计算结果为 `FALSE`，则表示违反约束条件。\n- `[NOT] ENFORCED` 表示是否执行约束，可以用于启用或者禁用 `CHECK` 约束。\n\n### 添加 `CHECK` 约束\n\n在 TiDB 中，你可以在 [`CREATE TABLE`](/sql-statements/sql-statement-create-table.md) 或者 [`ALTER TABLE`](/sql-statements/sql-statement-modify-column.md) 语句中为表添加 `CHECK` 约束。\n\n- 在 `CREATE TABLE` 语句中添加 `CHECK` 约束的示例：\n\n    ```sql\n    CREATE TABLE t(a INT CHECK(a > 10) NOT ENFORCED, b INT, c INT, CONSTRAINT c1 CHECK (b > c));\n    ```\n\n- 在 `ALTER TABLE` 语句中添加 `CHECK` 约束的示例：\n\n    ```sql\n    ALTER TABLE t ADD CONSTRAINT CHECK (1 < c);\n    ```\n\n在添加或者启用 `CHECK` 约束时，TiDB 会对表中的存量数据进行校验。如果存在违反约束的数据，添加 `CHECK` 约束操作将失败并且报错。\n\n在添加 `CHECK` 约束时，可以指定约束名，也可以不指定约束名。如果不指定约束名，那么 TiDB 会自动生成一个格式为 `<tableName>_chk_<1, 2, 3...>` 的约束名。\n\n### 查看 `CHECK` 约束\n\n你可以通过 [`SHOW CREATE TABLE`](/sql-statements/sql-statement-show-create-table.md) 查看表中的约束信息。例如：\n\n```sql\nSHOW CREATE TABLE t;\n+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table | Create Table                                                                                                                                                                                                                                                                                                     |\n+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| t     | CREATE TABLE `t` (\n  `a` int DEFAULT NULL,\n  `b` int DEFAULT NULL,\n  `c` int DEFAULT NULL,\nCONSTRAINT `c1` CHECK ((`b` > `c`)),\nCONSTRAINT `t_chk_1` CHECK ((`a` > 10)) /*!80016 NOT ENFORCED */,\nCONSTRAINT `t_chk_2` CHECK ((1 < `c`))\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin |\n+-------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n### 删除 `CHECK` 约束\n\n删除 `CHECK` 约束时，你需要指定需要删除的约束名。例如：\n\n```sql\nALTER TABLE t DROP CONSTRAINT t_chk_1;\n```\n\n### 启用或禁用 `CHECK` 约束\n\n在为表[添加 `CHECK` 约束](#添加-check-约束)的时候，可以指定当插入或者更新数据时 TiDB 是否执行约束检查。\n\n- 如果指定了 `NOT ENFORCED`，当插入或者更新数据时，TiDB 不会检查约束条件。\n- 如果未指定 `NOT ENFORCED` 或者指定了 `ENFORCED`，当插入或者更新数据时，TiDB 会检查约束条件。\n\n除了在添加约束时候指定 `[NOT] ENFORCED`，你还可以在 `ALTER TABLE` 语句中启用或者禁用 `CHECK` 约束。例如：\n\n```sql\nALTER TABLE t ALTER CONSTRAINT c1 NOT ENFORCED;\n```\n\n### 与 MySQL 的兼容性\n\n- 不支持在添加列的同时添加 `CHECK` 约束（例如，`ALTER TABLE t ADD COLUMN a CHECK(a > 0)`)），否则只有列会被添加成功，TiDB 会忽略 `CHECK` 约束但不会报错。\n- 不支持使用 `ALTER TABLE t CHANGE a b int CHECK(b > 0)` 添加 `CHECK` 约束，使用该语句时 TiDB 会报错。\n\n## 唯一约束\n\n唯一约束是指唯一索引和主键列中所有的非空值都是唯一的。\n\n### 乐观事务\n\n在 TiDB 的乐观事务中，默认会对唯一约束进行[惰性检查](/transaction-overview.md#惰性检查)。通过在事务提交时再进行批量检查，TiDB 能够减少网络开销、提升性能。例如：\n\n```sql\nDROP TABLE IF EXISTS users;\nCREATE TABLE users (\n id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n username VARCHAR(60) NOT NULL,\n UNIQUE KEY (username)\n);\nINSERT INTO users (username) VALUES ('dave'), ('sarah'), ('bill');\n```\n\n乐观事务模式下且 `tidb_constraint_check_in_place=OFF`：\n\n```sql\nBEGIN OPTIMISTIC;\nINSERT INTO users (username) VALUES ('jane'), ('chris'), ('bill');\n```\n\n```\nQuery OK, 3 rows affected (0.00 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n```\n\n```sql\nINSERT INTO users (username) VALUES ('steve'),('elizabeth');\n```\n\n```\nQuery OK, 2 rows affected (0.00 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n```\n\n```sql\nCOMMIT;\n```\n\n```\nERROR 1062 (23000): Duplicate entry 'bill' for key 'users.username'\n```\n\n在以上乐观事务的示例中，唯一约束的检查推迟到事务提交时才进行。由于 `bill` 值已经存在，这一行为导致了重复键错误。\n\n你可通过设置 [`tidb_constraint_check_in_place`](/system-variables.md#tidb_constraint_check_in_place) 为 `ON` 停用此行为（该变量仅适用于乐观事务，悲观事务需通过 `tidb_constraint_check_in_place_pessimistic` 设置）。当 `tidb_constraint_check_in_place` 设置为 `ON` 时，TiDB 会在执行语句时就对唯一约束进行检查。例如：\n\n```sql\nDROP TABLE IF EXISTS users;\nCREATE TABLE users (\n id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n username VARCHAR(60) NOT NULL,\n UNIQUE KEY (username)\n);\nINSERT INTO users (username) VALUES ('dave'), ('sarah'), ('bill');\n```\n\n```sql\nSET tidb_constraint_check_in_place = ON;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nBEGIN OPTIMISTIC;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nINSERT INTO users (username) VALUES ('jane'), ('chris'), ('bill');\n```\n\n```\nERROR 1062 (23000): Duplicate entry 'bill' for key 'users.username'\n```\n\n第一条 `INSERT` 语句导致了重复键错误。这会造成额外的网络通信开销，并可能降低插入操作的吞吐量。\n\n### 悲观事务\n\n在 TiDB 的悲观事务中，默认在执行任何一条需要插入或更新唯一索引的 SQL 语句时都会进行唯一约束检查：\n\n```sql\nDROP TABLE IF EXISTS users;\nCREATE TABLE users (\n id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n username VARCHAR(60) NOT NULL,\n UNIQUE KEY (username)\n);\nINSERT INTO users (username) VALUES ('dave'), ('sarah'), ('bill');\n\nBEGIN PESSIMISTIC;\nINSERT INTO users (username) VALUES ('jane'), ('chris'), ('bill');\n```\n\n```\nERROR 1062 (23000): Duplicate entry 'bill' for key 'users.username'\n```\n\n对于悲观事务，你可以设置变量 [`tidb_constraint_check_in_place_pessimistic`](/system-variables.md#tidb_constraint_check_in_place_pessimistic-从-v630-版本开始引入) 为 `OFF` 来推迟唯一约束检查，到下一次对该唯一索引项加锁时或事务提交时再进行检查，同时也跳过对该悲观锁加锁，以获得更好的性能。此时需要注意：\n\n- 由于推迟了唯一约束检查，TiDB 可能会读取到不满足唯一约束的结果，执行 `COMMIT` 语句时可能返回 `Duplicate entry` 错误。返回该错误时，TiDB 会回滚当前事务。\n\n    下面这个例子跳过了对 `bill` 的加锁，因此 TiDB 可能读到不满足唯一性约束的结果：\n\n    ```sql\n    SET tidb_constraint_check_in_place_pessimistic = OFF;\n    BEGIN PESSIMISTIC;\n    INSERT INTO users (username) VALUES ('jane'), ('chris'), ('bill'); -- Query OK, 3 rows affected\n    SELECT * FROM users FOR UPDATE;\n    ```\n\n    TiDB 读到了不满足唯一性约束的结果：有两个 `bill`。\n\n    ```sql\n    +----+----------+\n    | id | username |\n    +----+----------+\n    | 1  | dave     |\n    | 2  | sarah    |\n    | 3  | bill     |\n    | 7  | jane     |\n    | 8  | chris    |\n    | 9  | bill     |\n    +----+----------+\n    ```\n\n    此时，如果提交事务，TiDB 将进行唯一约束检查，报出 `Duplicate entry` 错误并回滚事务。\n\n    ```sql\n    COMMIT;\n    ```\n\n    ```\n    ERROR 1062 (23000): Duplicate entry 'bill' for key 'users.username'\n    ```\n\n- 关闭该变量时，如果在事务中写入数据，执行 `COMMIT` 语句可能会返回 `Write conflict` 错误。返回该错误时，TiDB 会回滚当前事务。\n\n    在下面这个例子中，当有并发事务写入时，跳过悲观锁导致事务提交时报出 `Write conflict` 错误并回滚。\n\n    ```sql\n    DROP TABLE IF EXISTS users;\n    CREATE TABLE users (\n    id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n    username VARCHAR(60) NOT NULL,\n    UNIQUE KEY (username)\n    );\n\n    SET tidb_constraint_check_in_place_pessimistic = OFF;\n    BEGIN PESSIMISTIC;\n    INSERT INTO users (username) VALUES ('jane'), ('chris'), ('bill'); -- Query OK, 3 rows affected\n    ```\n\n    然后另一个会话中写入了 `bill`：\n\n    ```sql\n    INSERT INTO users (username) VALUES ('bill'); -- Query OK, 1 row affected\n    ```\n\n    在第一个会话中提交时，TiDB 会报出 `Write conflict` 错误。\n\n    ```sql\n    COMMIT;\n    ```\n\n    ```\n    ERROR 9007 (HY000): Write conflict, txnStartTS=435688780611190794, conflictStartTS=435688783311536129, conflictCommitTS=435688783311536130, key={tableID=74, indexID=1, indexValues={bill, }} primary={tableID=74, indexID=1, indexValues={bill, }}, reason=LazyUniquenessCheck [try again later]\n    ```\n\n- 关闭该变量时，如果多个悲观事务之间存在写冲突，悲观锁可能会在其它悲观事务提交时被强制回滚，因此产生 `PessimisticLockNotFound` 错误。发生该错误时，说明该业务不适合推迟悲观事务的唯一约束检查，应考虑调整业务避免冲突，或在发生错误后重试事务。\n\n- 关闭该变量会导致悲观事务中可能报出错误 `8147: LazyUniquenessCheckFailure`。\n\n    > **注意：**\n    >\n    > 返回 8147 错误时当前事务回滚。\n\n    下面的例子在 INSERT 语句执行时跳过了一次加锁后，在 DELETE 语句执行时对该唯一索引加锁并检查，即会在该语句报错：\n\n    ```sql\n    SET tidb_constraint_check_in_place_pessimistic = OFF;\n    BEGIN PESSIMISTIC;\n    INSERT INTO users (username) VALUES ('jane'), ('chris'), ('bill'); -- Query OK, 3 rows affected\n    DELETE FROM users where username = 'bill';\n    ```\n\n    ```\n    ERROR 8147 (23000): transaction aborted because lazy uniqueness check is enabled and an error occurred: [kv:1062]Duplicate entry 'bill' for key 'users.username'\n    ```\n\n- 关闭该变量时，`1062 Duplicate entry` 报错不一定是当前执行的 SQL 语句所发生的错误。因此，在一个事务操作多个表，且这些表有同名索引时，请注意 `1062` 报错信息中提示的是哪个表的哪个索引发生了错误。\n\n## 主键约束\n\n与 MySQL 行为一样，主键约束包含了唯一约束，即创建了主键约束相当于拥有了唯一约束。此外，TiDB 其他的主键约束规则也与 MySQL 相似。例如：\n\n```sql\nCREATE TABLE t1 (a INT NOT NULL PRIMARY KEY);\n```\n\n```\nQuery OK, 0 rows affected (0.12 sec)\n```\n\n```sql\nCREATE TABLE t2 (a INT NULL PRIMARY KEY);\n```\n\n```\nERROR 1171 (42000): All parts of a PRIMARY KEY must be NOT NULL; if you need NULL in a key, use UNIQUE instead\n```\n\n```sql\nCREATE TABLE t3 (a INT NOT NULL PRIMARY KEY, b INT NOT NULL PRIMARY KEY);\n```\n\n```\nERROR 1068 (42000): Multiple primary key defined\n```\n\n```sql\nCREATE TABLE t4 (a INT NOT NULL, b INT NOT NULL, PRIMARY KEY (a,b));\n```\n\n```\nQuery OK, 0 rows affected (0.10 sec)\n```\n\n分析：\n\n* 表 `t2` 创建失败，因为定义为主键的列 `a` 不能允许 `NULL` 值。\n* 表 `t3` 创建失败，因为一张表只能有一个主键。\n* 表 `t4` 创建成功，因为虽然只能有一个主键，但 TiDB 支持定义一个多列组合作为复合主键。\n\n除上述规则外，TiDB 目前仅支持对 `NONCLUSTERED` 的主键进行添加和删除操作。例如：\n\n```sql\nCREATE TABLE t5 (a INT NOT NULL, b INT NOT NULL, PRIMARY KEY (a,b) CLUSTERED);\nALTER TABLE t5 DROP PRIMARY KEY;\n```\n\n```\nERROR 8200 (HY000): Unsupported drop primary key when the table is using clustered index\n```\n\n```sql\nCREATE TABLE t5 (a INT NOT NULL, b INT NOT NULL, PRIMARY KEY (a,b) NONCLUSTERED);\nALTER TABLE t5 DROP PRIMARY KEY;\n```\n\n```\nQuery OK, 0 rows affected (0.10 sec)\n```\n\n要了解关于 `CLUSTERED` 主键的详细信息，请参考[聚簇索引](/clustered-indexes.md)。\n\n## 外键约束\n\n> **注意：**\n>\n> TiDB 从 v6.6.0 开始支持[外键约束](/foreign-key.md)。在 v6.6.0 之前，TiDB 支持创建和删除外键约束，但外键约束并不生效。升级到 v6.6.0 或更高版本后，可以先删除不生效的外键后再创建外键使外键约束生效。外键约束在 v8.5.0 成为正式功能。\n\nTiDB 支持创建外键约束。例如：\n\n```sql\nCREATE TABLE users (\n id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n doc JSON\n);\nCREATE TABLE orders (\n id INT NOT NULL PRIMARY KEY AUTO_INCREMENT,\n user_id INT NOT NULL,\n doc JSON,\n FOREIGN KEY fk_user_id (user_id) REFERENCES users(id)\n);\n```\n\n```sql\nSELECT table_name, column_name, constraint_name, referenced_table_name, referenced_column_name\nFROM information_schema.key_column_usage WHERE table_name IN ('users', 'orders');\n```\n\n```\n+------------+-------------+-----------------+-----------------------+------------------------+\n| table_name | column_name | constraint_name | referenced_table_name | referenced_column_name |\n+------------+-------------+-----------------+-----------------------+------------------------+\n| users      | id          | PRIMARY         | NULL                  | NULL                   |\n| orders     | id          | PRIMARY         | NULL                  | NULL                   |\n| orders     | user_id     | fk_user_id      | users                 | id                     |\n+------------+-------------+-----------------+-----------------------+------------------------+\n3 rows in set (0.00 sec)\n```\n\nTiDB 也支持使用 `ALTER TABLE` 命令来删除外键 (`DROP FOREIGN KEY`) 和添加外键 (`ADD FOREIGN KEY`)：\n\n```sql\nALTER TABLE orders DROP FOREIGN KEY fk_user_id;\nALTER TABLE orders ADD FOREIGN KEY fk_user_id (user_id) REFERENCES users(id);\n```\n"
        },
        {
          "name": "control-execution-plan.md",
          "type": "blob",
          "size": 1.935546875,
          "content": "---\ntitle: 控制执行计划\naliases: ['/docs-cn/dev/control-execution-plan/']\nsummary: 本章节介绍了控制执行计划的方法，包括使用提示指导执行计划、执行计划管理、优化规则及表达式下推的黑名单。此外，还介绍了一些系统变量对执行计划的影响，以及引入的特殊变量 `tidb_opt_fix_control`，用于更细粒度地控制优化器的行为。\n---\n\n# 控制执行计划\n\nSQL 性能调优的前两个章节介绍了如何理解 TiDB 的执行计划以及 TiDB 如何生成一个执行计划。本章节将介绍当你确定了执行计划所存在的问题时，可以使用哪些手段来控制执行计划的生成。本章节主要包括以下三方面内容：\n\n- [Optimizer Hints](/optimizer-hints.md)中，我们会介绍如何使用 Hint 来指导 TiDB 生成执行计划。\n- 但是使用 Hint 会侵入性地更改 SQL，在一些场景下并不能简单的插入 Hint。在[执行计划管理](/sql-plan-management.md)中，我们会介绍 TiDB 如何使用另一种语法来非侵入地控制执行计划的生成，同时还会介绍后台自动对执行计划进行演进的手段。该手段可用来减轻诸如版本升级等原因造成的执行计划不稳定和集群性能下降的问题。\n- 最后在[优化规则及表达式下推的黑名单](/blocklist-control-plan.md)中，我们会介绍黑名单的使用。\n\n除以上手段之外，执行计划还会被一部分系统变量所影响。通过在系统级或会话级对变量进行修改，能够控制执行计划的生成。自 v6.5.3 和 v7.1.0 起，TiDB 引入了一个相对特殊的变量 [`tidb_opt_fix_control`](/system-variables.md#tidb_opt_fix_control-从-v653-和-v710-版本开始引入)。这个变量能够接受多个控制项，用来更细粒度地控制优化器的行为，避免集群升级后优化器行为变化导致的性能回退。详细介绍请参考 [Optimizer Fix Controls](/optimizer-fix-controls.md)。\n"
        },
        {
          "name": "coprocessor-cache.md",
          "type": "blob",
          "size": 5.7998046875,
          "content": "---\ntitle: 下推计算结果缓存\naliases: ['/docs-cn/dev/coprocessor-cache/']\nsummary: TiDB 4.0 支持下推计算结果缓存，配置位于 `tikv-client.copr-cache`，缓存仅存储在 TiDB 内存中，不共享缓存，对 Region 写入会导致缓存失效。缓存命中率可通过 `EXPLAIN ANALYZE` 或 Grafana 监控面板查看。\n---\n\n# 下推计算结果缓存\n\nTiDB 从 4.0 起支持下推计算结果缓存（即 Coprocessor Cache 功能）。开启该功能后，将在 TiDB 实例侧缓存下推给 TiKV 计算的结果，在部分场景下起到加速效果。\n\n## 配置\n\nCoprocessor Cache 的配置均位于 TiDB 的 `tikv-client.copr-cache` 配置项中。Coprocessor 的具体开启和配置方法，见 [TiDB 配置文件描述](/tidb-configuration-file.md#tikv-clientcopr-cache-从-v400-版本开始引入)。\n\n## 特性说明\n\n+ 所有 SQL 在单个 TiDB 实例上的首次执行都不会被缓存。\n+ 缓存仅存储在 TiDB 内存中，TiDB 重启后缓存会失效。\n+ 不同 TiDB 实例之间不共享缓存。\n+ 缓存的是下推计算结果，即使缓存命中，后续仍有 TiDB 计算。\n+ 缓存以 Region 为单位。对 Region 的写入会导致涉及该 Region 的缓存失效。基于此原因，该功能主要会对很少变更的数据有效果。\n+ 下推计算请求相同时，缓存会被命中。通常在以下场景下，下推计算的请求是相同或部分相同的：\n    - SQL 语句完全一致，例如重复执行相同的 SQL 语句。\n\n        该场景下所有下推计算的请求都是一致的，所有请求都能利用上下推计算缓存。\n\n    - SQL 语句包含一个变化的条件，其他部分一致，变化的条件是表主键或分区主键。\n\n        该场景下一部分下推计算的请求会与之前出现过的一致，部分请求能利用上下推计算结果缓存。\n\n    - SQL 语句包含多个变化的条件，其他部分一致，变化的条件完全匹配一个复合索引列。\n\n        该场景下一部分下推计算的请求会与之前出现过的一致，部分请求能利用上下推计算结果缓存。\n\n+ 该功能对用户透明，开启和关闭都不影响计算结果，仅影响 SQL 执行时间。\n\n## 检验缓存效果\n\n可以通过执行 `EXPLAIN ANALYZE` 或查看 Grafana 监控面板来检查 Coprocessor 的缓存效果。\n\n### 使用 `EXPLAIN ANALYZE`\n\n用户可以通过 [`EXPLAIN ANALYZE` 语句](/sql-statements/sql-statement-explain-analyze.md)查看[读表算子](/choose-index.md#读表算子)中的缓存命中率，示例如下：\n\n```sql\nEXPLAIN ANALYZE SELECT * FROM t USE INDEX(a);\n+-------------------------------+-----------+---------+-----------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----------------------+------+\n| id                            | estRows   | actRows | task      | access object          | execution info                                                                                                                                                                                                                                           | operator info                  | memory                | disk |\n+-------------------------------+-----------+---------+-----------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----------------------+------+\n| IndexLookUp_6                 | 262400.00 | 262400  | root      |                        | time:620.513742ms, loops:258, cop_task: {num: 4, max: 5.530817ms, min: 1.51829ms, avg: 2.70883ms, p95: 5.530817ms, max_proc_keys: 2480, p95_proc_keys: 2480, tot_proc: 1ms, tot_wait: 1ms, rpc_num: 4, rpc_time: 10.816328ms, copr_cache_hit_rate: 0.75} |                                | 6.685169219970703 MB  | N/A  |\n| ├─IndexFullScan_4(Build)      | 262400.00 | 262400  | cop[tikv] | table:t, index:a(a, c) | proc max:93ms, min:1ms, p80:93ms, p95:93ms, iters:275, tasks:4                                                                                                                                                                                           | keep order:false, stats:pseudo | 1.7549400329589844 MB | N/A  |\n| └─TableRowIDScan_5(Probe)     | 262400.00 | 0       | cop[tikv] | table:t                | time:0ns, loops:0                                                                                                                                                                                                                                        | keep order:false, stats:pseudo | N/A                   | N/A  |\n+-------------------------------+-----------+---------+-----------+------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------+-----------------------+------+\n3 rows in set (0.62 sec)\n```\n\n执行结果的 `execution info` 列有 `copr_cache_hit_ratio` 信息，表示下推计算结果缓存的命中率。以上示例的 `0.75` 表示命中率大约是 75%。\n\n### 查看 Grafana 监控面板\n\n在 Grafana 监控中，`tidb` 命名空间下 `distsql` 子系统中可见 **copr-cache** 面板，该面板监控整个集群中下推计算结果缓存的命中次数、未命中次数和缓存丢弃次数。\n"
        },
        {
          "name": "correlated-subquery-optimization.md",
          "type": "blob",
          "size": 10.5400390625,
          "content": "---\ntitle: 关联子查询去关联\nsummary: 了解如何给关联子查询解除关联。\naliases: ['/docs-cn/dev/correlated-subquery-optimization/']\n---\n\n# 关联子查询去关联\n\n[子查询相关的优化](/subquery-optimization.md)中介绍了当没有关联列时，TiDB 是如何处理子查询的。由于为关联子查询解除关联依赖比较复杂，本文档中会介绍一些简单的场景以及这个优化规则的适用范围。\n\n## 简介\n\n以 `select * from t1 where t1.a < (select sum(t2.a) from t2 where t2.b = t1.b)` 为例，这里子查询 `t1.a < (select sum(t2.a) from t2 where t2.b = t1.b)` 中涉及了关联列上的条件 `t2.b=t1.b`，不过恰好由于这是一个等值条件，因此可以将其等价的改写为 `select t1.* from t1, (select b, sum(a) sum_a from t2 group by b) t2 where t1.b = t2.b and t1.a < t2.sum_a;`。这样，一个关联子查询就被重新改写为 `JOIN` 的形式。\n\nTiDB 之所以要进行这样的改写，是因为关联子查询每次子查询执行时都是要和它的外部查询结果绑定的。在上面的例子中，如果 `t1.a` 有一千万个值，那这个子查询就要被重复执行一千万次，因为 `t2.b=t1.b` 这个条件会随着 `t1.a` 值的不同而发生变化。当通过一些手段将关联依赖解除后，这个子查询就只需要被执行一次了。\n\n## 限制\n\n这种改写的弊端在于，在关联没有被解除时，优化器是可以使用关联列上的索引的。也就是说，虽然这个子查询可能被重复执行多次，但是每次都可以使用索引过滤数据。而解除关联的变换上，通常是会导致关联列的位置发生改变而导致虽然子查询只被执行了一次，但是单次执行的时间会比没有解除关联时的单次执行时间长。\n\n因此，在外部的值比较少的情况下，不解除关联依赖反而可能对执行性能更有帮助。这时可以通过使用 Optimizer Hint [`NO_DECORRELATE`](/optimizer-hints.md#no_decorrelate) 或[优化规则及表达式下推的黑名单](/blocklist-control-plan.md)中关闭“子查询去关联”优化规则的方式来关闭这个优化。在一般情况下，推荐使用 Optimizer Hint 并在需要时配合[执行计划管理](/sql-plan-management.md)功能来禁止解除关联。\n\n## 样例\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t1(a int, b int);\ncreate table t2(a int, b int, index idx(b));\nexplain select * from t1 where t1.a < (select sum(t2.a) from t2 where t2.b = t1.b);\n```\n\n```sql\n+----------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------------------------+\n| id                               | estRows  | task      | access object | operator info                                                                           |\n+----------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------------------------+\n| HashJoin_11                      | 9990.00  | root      |               | inner join, equal:[eq(test.t1.b, test.t2.b)], other cond:lt(cast(test.t1.a), Column#7)  |\n| ├─HashAgg_23(Build)              | 7992.00  | root      |               | group by:test.t2.b, funcs:sum(Column#8)->Column#7, funcs:firstrow(test.t2.b)->test.t2.b |\n| │ └─TableReader_24               | 7992.00  | root      |               | data:HashAgg_16                                                                         |\n| │   └─HashAgg_16                 | 7992.00  | cop[tikv] |               | group by:test.t2.b, funcs:sum(test.t2.a)->Column#8                                      |\n| │     └─Selection_22             | 9990.00  | cop[tikv] |               | not(isnull(test.t2.b))                                                                  |\n| │       └─TableFullScan_21       | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo                                                          |\n| └─TableReader_15(Probe)          | 9990.00  | root      |               | data:Selection_14                                                                       |\n|   └─Selection_14                 | 9990.00  | cop[tikv] |               | not(isnull(test.t1.b))                                                                  |\n|     └─TableFullScan_13           | 10000.00 | cop[tikv] | table:t1      | keep order:false, stats:pseudo                                                          |\n+----------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------------------------+\n\n```\n\n上面是优化生效的情况，可以看到 `HashJoin_11` 是一个普通的 `inner join`。\n\n接下来，通过 Optimizer Hint `NO_DECORRELATE` 提示优化器不对该子查询解除关联：\n\n{{< copyable \"sql\" >}}\n\n```sql\nexplain select * from t1 where t1.a < (select /*+ NO_DECORRELATE() */ sum(t2.a) from t2 where t2.b = t1.b);\n```\n\n```sql\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n| id                                       | estRows   | task      | access object          | operator info                                                                        |\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n| Projection_10                            | 10000.00  | root      |                        | test.t1.a, test.t1.b                                                                 |\n| └─Apply_12                               | 10000.00  | root      |                        | CARTESIAN inner join, other cond:lt(cast(test.t1.a, decimal(10,0) BINARY), Column#7) |\n|   ├─TableReader_14(Build)                | 10000.00  | root      |                        | data:TableFullScan_13                                                                |\n|   │ └─TableFullScan_13                   | 10000.00  | cop[tikv] | table:t1               | keep order:false, stats:pseudo                                                       |\n|   └─MaxOneRow_15(Probe)                  | 10000.00  | root      |                        |                                                                                      |\n|     └─StreamAgg_20                       | 10000.00  | root      |                        | funcs:sum(Column#14)->Column#7                                                       |\n|       └─Projection_45                    | 100000.00 | root      |                        | cast(test.t2.a, decimal(10,0) BINARY)->Column#14                                     |\n|         └─IndexLookUp_44                 | 100000.00 | root      |                        |                                                                                      |\n|           ├─IndexRangeScan_42(Build)     | 100000.00 | cop[tikv] | table:t2, index:idx(b) | range: decided by [eq(test.t2.b, test.t1.b)], keep order:false, stats:pseudo         |\n|           └─TableRowIDScan_43(Probe)     | 100000.00 | cop[tikv] | table:t2               | keep order:false, stats:pseudo                                                       |\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n```\n\n也可以通过全局关闭关联规则达到同样的效果：\n\n{{< copyable \"sql\" >}}\n\n```sql\ninsert into mysql.opt_rule_blacklist values(\"decorrelate\");\nadmin reload opt_rule_blacklist;\nexplain select * from t1 where t1.a < (select sum(t2.a) from t2 where t2.b = t1.b);\n```\n\n```\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n| id                                       | estRows   | task      | access object          | operator info                                                                        |\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n| Projection_10                            | 10000.00  | root      |                        | test.t1.a, test.t1.b                                                                 |\n| └─Apply_12                               | 10000.00  | root      |                        | CARTESIAN inner join, other cond:lt(cast(test.t1.a, decimal(10,0) BINARY), Column#7) |\n|   ├─TableReader_14(Build)                | 10000.00  | root      |                        | data:TableFullScan_13                                                                |\n|   │ └─TableFullScan_13                   | 10000.00  | cop[tikv] | table:t1               | keep order:false, stats:pseudo                                                       |\n|   └─MaxOneRow_15(Probe)                  | 10000.00  | root      |                        |                                                                                      |\n|     └─StreamAgg_20                       | 10000.00  | root      |                        | funcs:sum(Column#14)->Column#7                                                       |\n|       └─Projection_45                    | 100000.00 | root      |                        | cast(test.t2.a, decimal(10,0) BINARY)->Column#14                                     |\n|         └─IndexLookUp_44                 | 100000.00 | root      |                        |                                                                                      |\n|           ├─IndexRangeScan_42(Build)     | 100000.00 | cop[tikv] | table:t2, index:idx(b) | range: decided by [eq(test.t2.b, test.t1.b)], keep order:false, stats:pseudo         |\n|           └─TableRowIDScan_43(Probe)     | 100000.00 | cop[tikv] | table:t2               | keep order:false, stats:pseudo                                                       |\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n```\n\n在执行了关闭关联规则的语句后，可以在 `IndexRangeScan_42(Build)` 的 `operator info` 中看到 `range: decided by [eq(test.t2.b, test.t1.b)]`。这部分信息就是关联依赖未被解除时，TiDB 使用关联条件进行索引范围查询的显示结果。\n"
        },
        {
          "name": "cost-model.md",
          "type": "blob",
          "size": 2.662109375,
          "content": "---\ntitle: 代价模型\nsummary: 介绍 TiDB 进行物理优化时所使用的代价模型的原理。\n---\n\n# 代价模型\n\nTiDB 在进行[物理优化](/sql-physical-optimization.md)时会使用代价模型来进行索引选择和算子选择，如下图所示：\n\n![CostModel](/media/cost-model.png)\n\nTiDB 会计算每个索引的访问代价和计划中每个物理算子的执行代价（如 HashJoin、IndexJoin 等），选择代价最低的计划。\n\n下面是一个简化的例子，用来解释代价模型的原理，比如有这样一张表：\n\n```sql\nmysql> SHOW CREATE TABLE t;\n+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table | Create Table                                                                                                                                                                                        |\n+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| t     | CREATE TABLE `t` (\n  `a` int DEFAULT NULL,\n  `b` int DEFAULT NULL,\n  `c` int DEFAULT NULL,\n  KEY `b` (`b`),\n  KEY `c` (`c`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin |\n+-------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n在处理查询 `SELECT * FROM t WHERE b < 100 and c < 100` 时，假设 TiDB 对 `b < 100` 和 `c < 100` 的行数估计分别为 20 和 500，`INT` 类型索引行宽为 8，则 TiDB 会分别计算两个索引的代价：\n\n+ 索引 `b` 的扫描代价 = `b < 100` 的行数 \\* 索引 `b` 的行宽 = 20 * 8 = 160\n+ 索引 `c` 的扫描代价 = `c < 100` 的行数 \\* 索引 `c` 的行宽 = 500 * 8 = 4000\n\n由于扫描 `b` 的代价更低，因此 TiDB 会选择 `b` 索引。\n\n上述是一个简化后的例子，只是用于做原理解释，实际 TiDB 的代价模型会更加复杂。\n\n## Cost Model Version 2\n\nTiDB v6.2.0 引入了新的代价模型 Cost Model Version 2。\n\nCost Model Version 2 对代价公式进行了更精确的回归校准，调整了部分代价公式，比此前版本的代价公式更加准确。\n\n你可以通过设置变量 [`tidb_cost_model_version`](/system-variables.md#tidb_cost_model_version-从-v620-版本开始引入) 来控制代价模型的版本。\n\n> **注意：**\n>\n> 切换代价模型版本可能引起执行计划的变动。"
        },
        {
          "name": "credits.md",
          "type": "blob",
          "size": 1.92578125,
          "content": "---\ntitle: TiDB 社区荣誉列表\nsummary: 了解 TiDB 社区贡献者列表及角色。\naliases: ['/docs-cn/dev/credits/']\n---\n\n# TiDB 社区荣誉列表\n\n每一位贡献者都是推动 TiDB 健壮发展的重要成员，我们感谢所有为 TiDB 提交代码、撰写或翻译文档的贡献者。\n\n## TiDB 开发者\n\nTiDB 开发者为 TiDB 的新功能开发、性能优化、稳定性保障做出了贡献。以下链接包含了 TiDB 相关 repo 的贡献者名单：\n\n- [pingcap/tidb](https://github.com/pingcap/tidb/graphs/contributors)\n- [tikv/tikv](https://github.com/tikv/tikv/graphs/contributors)\n- [tikv/pd](https://github.com/tikv/pd/graphs/contributors)\n- [pingcap/tiflash](https://github.com/pingcap/tiflash/graphs/contributors)\n- [pingcap/tidb-operator](https://github.com/pingcap/tidb-operator/graphs/contributors)\n- [pingcap/tiup](https://github.com/pingcap/tiup/graphs/contributors)\n- [pingcap/tidb-dashboard](https://github.com/pingcap/tidb-dashboard/graphs/contributors)\n- [pingcap/tiflow](https://github.com/pingcap/tiflow/graphs/contributors)\n- [pingcap/tidb-tools](https://github.com/pingcap/tidb-tools/graphs/contributors)\n- [pingcap/tispark](https://github.com/pingcap/tispark/graphs/contributors)\n- [tikv/client-java](https://github.com/tikv/client-java/graphs/contributors)\n- [tidb-incubator/TiBigData](https://github.com/tidb-incubator/TiBigData/graphs/contributors)\n- [ti-community-infra](https://github.com/orgs/ti-community-infra/people)\n\n## TiDB 文档写作者和译员\n\nTiDB 文档写作者和译员为 TiDB 及相关项目撰写文档、提供翻译。以下链接包含了 TiDB 文档相关 repo 的贡献者名单：\n\n- [pingcap/docs-cn](https://github.com/pingcap/docs-cn/graphs/contributors)\n- [pingcap/docs](https://github.com/pingcap/docs/graphs/contributors)\n- [pingcap/docs-tidb-operator](https://github.com/pingcap/docs-tidb-operator/graphs/contributors)\n- [tikv/website](https://github.com/tikv/website/graphs/contributors)\n"
        },
        {
          "name": "daily-check.md",
          "type": "blob",
          "size": 4.2734375,
          "content": "---\ntitle: 日常巡检\nsummary: 介绍 TiDB 集群需要常关注的性能指标。\naliases: ['/docs-cn/dev/daily-check/','/docs-cn/dev/daily-inspection/','/zh/tidb/dev/daily-inspection']\n---\n\n# 日常巡检\n\nTiDB 作为分布式数据库，对比单机数据库机制更加复杂，其自带的监控项也很丰富。为了更便捷地运维 TiDB，本文介绍了运维 TiDB 集群需要常关注的关键性能指标。\n\n## TiDB Dashboard 关键指标\n\n从 4.0 版本开始，TiDB 提供了一个新的 [TiDB Dashboard](/dashboard/dashboard-intro.md) 运维管理工具，集成在 PD 组件上，默认地址为 `http://${pd-ip}:${pd_port}/dashboard`。\n\n使用 TiDB Dashboard，简化了对 TiDB 数据库的运维，可在一个界面查看整个分布式数据库集群的运行状况。下面举例说明。\n\n### 实例面板\n\n![实例面板](/media/instance-status-panel.png)\n\n以上实例面板的各指标说明如下：\n\n+ **状态**：用于查看实例状态是否正常，如果在线可忽略此项。\n+ **启动时间**：是关键指标。如果有发现启动时间有变动，那么需要排查组件重启的原因。\n+ **版本**、**部署路径**、**Git 哈希值**：通过查看这三个指标可以避免有部署路径和版本不一致或者错误的情况。\n\n### 主机面板\n\n![主机面板](/media/host-panel.png)\n\n通过主机面板可以查看 CPU、内存、磁盘使用率。当任何资源的使用率超过 80% 时，推荐扩容对应组件。\n\n### SQL 分析面板\n\n![SQL 分析面板](/media/sql-analysis-panel.png)\n\n通过 SQL 分析面板可以分析对集群影响较大的慢 SQL，然后进行对应的 SQL 优化。\n\n### Region 信息面板\n\n![Region 信息面板](/media/region-panel.png)\n\n以上 Region 信息面板说明如下：\n\n+ `down-peer-region-count`：Raft leader 上报有不响应 peer 的 Region 数量。\n+ `empty-region-count`：空 Region 的数量，大小小于 1 MiB。一般是 `TRUNCATE TABLE`/`DROP TABLE` 语句导致。如果数量较多，可以考虑开启跨表 Region merge。\n+ `extra-peer-region-count`：多副本的 Region 数量，调度过程中会产生。\n+ `learner-peer-region-count`：含有 learner peer 的 Region 数量。learner peer 的来源可能是多个，例如 TiFlash 上的 learner peer，以及配置的 Placement Rules 包含 learner peer。\n+ `miss-peer-region-count`：缺副本的 Region 数量，不会一直大于 0。\n+ `offline-peer-region-count`：peer 下线过程中的 Region 数量。\n+ `oversized-region-count`：Region 大小大于 `region-max-size` 或 `region-max-keys` 的 Region 数量。\n+ `pending-peer-region-count`：Raft log 落后的 Region 数量。由于调度产生少量的 pending peer 是正常的，但是如果 pending peer 的数量持续（超过 30 分钟）很高，可能存在问题。\n+ `undersized-region-count`：Region 大小小于 `max-merge-region-size` 或 `max-merge-region-keys` 的 Region 数量。\n\n原则上来说，该监控面板偶尔有数据是符合预期的。但长期有数据，需要排查是否存在问题。\n\n### KV Request Duration\n\n![TiKV 相应时间](/media/kv-duration-panel.png)\n\nTiKV 当前 .99（百分位）的响应时间。如果发现有明显高的节点，可以排查是否有热点，或者相关节点性能较差。\n\n### PD TSO Wait Duration\n\n![TiDB 从 PD 获取 TSO 的时间](/media/pd-duration-panel.png)\n\nTiDB 从 PD 获取 TSO 的时间。如果相关响应时间较高，一般常见原因如下：\n\n+ TiDB 到 PD 的网络延迟较高，可以手动 Ping 一下网络延迟。\n+ TiDB 压力较高，导致获取较慢。\n+ PD 服务器压力较高，导致获取较慢。\n\n### Overview 面板\n\n![Overview 面板](/media/overview-panel.png)\n\n以上面板展示常见的负载、内存、网络、IO 监控。发现有瓶颈时，推荐扩容或者优化集群拓扑，优化 SQL、集群参数等。\n\n### 异常监控面板\n\n![异常监控面板](/media/failed-query-panel.png)\n\n以上面板展示每个 TiDB 实例上，执行 SQL 语句发生的错误，并按照错误类型进行统计，例如语法错误、主键冲突等。\n\n### GC 状态面板\n\n![GC 状态面板](/media/garbage-collation-panel.png)\n\n以上面板展示最后 GC（垃圾清理）的时间，观察 GC 是否正常。如果 GC 发生异常，可能会造成历史数据存留过多，影响访问效率。\n"
        },
        {
          "name": "dashboard",
          "type": "tree",
          "content": null
        },
        {
          "name": "data-type-date-and-time.md",
          "type": "blob",
          "size": 13.9296875,
          "content": "---\ntitle: 日期和时间类型\naliases: ['/docs-cn/dev/data-type-date-and-time/','/docs-cn/dev/reference/sql/data-types/date-and-time/']\nsummary: TiDB 支持 MySQL 的所有日期和时间类型，包括 DATE、TIME、DATETIME、TIMESTAMP 和 YEAR。每种类型都有有效值范围，值为 0 表示无效值。TIMESTAMP 和 DATETIME 类型能自动生成新的时间值。关于日期和时间值类型，需要注意日期部分必须是“年 - 月 - 日”的格式，如果日期的年份部分是 2 位数，TiDB 会根据具体规则进行转换。不同类型的零值如下表所示：DATE:0000-00-00, TIME:00:00:00, DATETIME:0000-00-00 00:00:00, TIMESTAMP:0000-00-00 00:00:00, YEAR:0000。如果 SQL 模式允许使用无效的 DATE、DATETIME、TIMESTAMP 值，无效值会自动转换为相应的零值。\n---\n\n# 日期和时间类型\n\nTiDB 支持 MySQL 所有的日期和时间类型，包括 [`DATE`](#date-类型)、[`TIME`](#time-类型)、[`DATETIME`](#datetime-类型)、[`TIMESTAMP`](#timestamp-类型) 以及 [`YEAR`](#year-类型)。完整信息可以参考 [MySQL 中的时间和日期类型](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-types.html)。\n\n每种类型都有有效值范围，值为 0 表示无效值。此外，`TIMESTAMP` 和 `DATETIME` 类型能自动生成新的时间值。\n\n关于日期和时间值类型，需要注意：\n\n- 日期部分必须是“年-月-日”的格式（例如 `1998-09-04`），而不是“月-日-年”或“日-月-年”的格式。\n- 如果日期的年份部分是 2 位数，TiDB 会根据[年份为两位数的具体规则](#年份为两位数)进行转换。\n- 如果格式必须是数值类型，TiDB 会自动将日期或时间值转换为数值类型。例如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT NOW(), NOW()+0, NOW(3)+0;\n    ```\n\n    ```sql\n    +---------------------+----------------+--------------------+\n    | NOW()               | NOW()+0        | NOW(3)+0           |\n    +---------------------+----------------+--------------------+\n    | 2012-08-15 09:28:00 | 20120815092800 | 20120815092800.889 |\n    +---------------------+----------------+--------------------+\n    ```\n\n- TiDB 可以自动将无效值转换同一类型的零值。是否进行转换取决于 SQL 模式的设置。比如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    show create table t1;\n    ```\n\n    ```sql\n    +-------+---------------------------------------------------------------------------------------------------------+\n    | Table | Create Table                                                                                            |\n    +-------+---------------------------------------------------------------------------------------------------------+\n    | t1    | CREATE TABLE `t1` (\n      `a` time DEFAULT NULL\n    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin |\n    +-------+---------------------------------------------------------------------------------------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select @@sql_mode;\n    ```\n\n    ```sql\n    +-------------------------------------------------------------------------------------------------------------------------------------------+\n    | @@sql_mode                                                                                                                                |\n    +-------------------------------------------------------------------------------------------------------------------------------------------+\n    | ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |\n    +-------------------------------------------------------------------------------------------------------------------------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    insert into t1 values (`2090-11-32:22:33:44`);\n    ```\n\n    ```sql\n    ERROR 1292 (22007): Truncated incorrect time value: `2090-11-32:22:33:44`\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    set @@sql_mode=``;\n    ```\n\n    ```sql\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    insert into t1 values (`2090-11-32:22:33:44`);\n    ```\n\n    ```sql\n    Query OK, 1 row affected, 1 warning (0.01 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t1;\n    ```\n\n    ```sql\n    +----------+\n    | a        |\n    +----------+\n    | 00:00:00 |\n    +----------+\n    1 row in set (0.01 sec)\n    ```\n\n- SQL 模式的不同设置，会改变 TiDB 对格式的要求。\n- 如果 SQL 模式的 `NO_ZERO_DATE` 被禁用，TiDB 允许 `DATE` 和 `DATETIME` 列中的月份或日期为零。例如，`2009-00-00` 或 `2009-01-00`。如果使用函数计算这种日期类型，例如使用 `DATE_SUB()` 或 `DATE_ADD()` 函数，计算结果可能不正确。\n- 默认情况下，TiDB 启用 `NO_ZERO_DATE` SQL 模式。该模式可以避免存储像 `0000-00-00` 这样的零值。\n\n不同类型的零值如下表所示：\n\n| 数据类型 | 零值 |\n| :------   |  :----       |\n| DATE      | `0000-00-00` |\n| TIME      | `00:00:00`   |\n| DATETIME  | `0000-00-00 00:00:00` |\n| TIMESTAMP | `0000-00-00 00:00:00` |\n| YEAR      | 0000         |\n\n如果 SQL 模式允许使用无效的 `DATE`、`DATETIME`、`TIMESTAMP` 值，无效值会自动转换为相应的零值（`0000-00-00` 或 `0000-00-00 00:00:00`）。\n\n## 类型定义\n\n### `DATE` 类型\n\n`DATE` 类型只包含日期部分，不包含时间部分。`DATE` 类型的格式为 `YYYY-MM-DD`，支持的范围是 `0000-01-01` 到 `9999-12-31`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nDATE\n```\n\n### `TIME` 类型\n\n`TIME` 类型的格式为 `HH:MM:SS[.fraction]`，支持的范围是 `-838:59:59.000000` 到 `838:59:59.000000`。`TIME` 不仅可用于指示一天内的时间，还可用于指两个事件之间的时间间隔。`fsp` 参数表示秒精度，取值范围为：0 ~ 6，默认值为 0。\n\n{{< copyable \"sql\" >}}\n\n```sql\nTIME[(fsp)]\n```\n\n> **注意：**\n>\n> 注意 `TIME` 的缩写形式。例如，`11:12` 表示 `11:12:00` 而不是 `00:11:12`。但是，`1112` 表示 `00:11:12`。这些差异取决于 `:` 字符的存在与否。\n\n### `DATETIME` 类型\n\n`DATETIME` 类型是日期和时间的组合，格式为 `YYYY-MM-DD HH:MM:SS[.fraction]`。支持的范围是 `0000-01-01 00:00:00.000000` 到 `9999-12-31 23:59:59.999999`。`fsp` 参数表示秒精度，取值范围为 0~6，默认值为 0。TiDB 支持字符串或数字转换为 `DATETIME` 类型。\n\n{{< copyable \"sql\" >}}\n\n```sql\nDATETIME[(fsp)]\n```\n\n### `TIMESTAMP` 类型\n\n`TIMESTAMP` 类型是日期和时间的组合，支持的范围是 UTC 时间从 `1970-01-01 00:00:01.000000` 到 `2038-01-19 03:14:07.999999`。`fsp` 参数表示秒精度，取值范围为 0~6，默认值为 0。在 `TIMESTAMP` 中，不允许零出现在月份部分或日期部分，唯一的例外是零值本身 `0000-00-00 00:00:00`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nTIMESTAMP[(fsp)]\n```\n\n#### 时区处理\n\n当存储 `TIMESTAMP` 时，TiDB 会将当前时区的 `TIMESTAMP` 值转换为 UTC 时区。当读取 `TIMESTAMP` 时，TiDB 将存储的 `TIMESTAMP` 值从 UTC 时区转换为当前时区（注意：`DATETIME` 不会这样处理）。每次连接的默认时区是服务器的本地时区，可以通过环境变量 `time_zone` 进行修改。\n\n> **警告：**\n>\n> 和 MySQL 一样，`TIMESTAMP` 数据类型受 [2038 年问题](https://zh.wikipedia.org/wiki/2038%E5%B9%B4%E9%97%AE%E9%A2%98)的影响。如果存储的值大于 2038，建议使用 `DATETIME` 类型。\n\n### `YEAR` 类型\n\n`YEAR` 类型的格式为 `YYYY`，支持的值范围是 `1901` 到 `2155`，也支持零值 `0000`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nYEAR[(4)]\n```\n\n`YEAR` 类型遵循以下格式规则：\n\n+ 如果是四位数的数值，支持的范围是 `1901` 至 `2155`。\n+ 如果是四位数的字符串，支持的范围是 `'1901'` 到 `'2155'`。\n+ 如果是 1~99 之间的一位数或两位数的数字，1~69 换算成 2001~2069，70~99 换算成 1970~1999。\n+ 支持 `'0'` 到 `'99'` 之间的一位数或两位数字符串的范围\n+ 将数值 `0` 转换为 `0000`，将字符串 `'0'` 或 `'00'` 转换为 `'2000'`。\n\n无效的 `YEAR` 值会自动转换为 `0000`（如果用户没有使用 `NO_ZERO_DATE` SQL 模式）。\n\n## 自动初始化和更新 `TIMESTAMP` 或 `DATETIME`\n\n带有 `TIMESTAMP` 或 `DATETIME` 数据类型的列可以自动初始化为或更新为当前时间。\n\n对于表中任何带有 `TIMESTAMP` 或 `DATETIME` 数据类型的列，你可以设置默认值，或自动更新为当前时间戳。\n\n在定义列的时候，`TIMESTAMP` 和 `DATETIME` 可以通过 `DEFAULT CURRENT_TIMESTAMP` 和 `ON UPDATE CURRENT_TIMESTAMP` 来设置。`DEFAULT` 也可以设置为一个特定的值，例如 `DEFAULT 0` 或 `DEFAULT '2000-01-01 00:00:00'`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n    ts TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    dt DATETIME DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n);\n```\n\n除非指定 `DATETIME` 的值为 `NOT NULL`，否则默认 `DATETIME` 的值为 `NULL`。指定 `DATETIME` 的值为 `NOT NULL` 时，如果没有设置默认值，则默认值为 `0`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n    dt1 DATETIME ON UPDATE CURRENT_TIMESTAMP,         -- default NULL\n    dt2 DATETIME NOT NULL ON UPDATE CURRENT_TIMESTAMP -- default 0\n);\n```\n\n## 时间值的小数部分\n\n`DATETIME` 和 `TIMESTAMP` 值最多可以有 6 位小数，精确到微秒。如果包含小数部分，值的格式为 `YYYY-MM-DD HH:MM:SS[.fraction]`，小数部分的范围为 `000000` 到`999999`。必须使用小数点分隔小数部分与其他部分。\n\n+ 使用 `type_name(fsp)` 可以定义精确到小数的列，其中 `type_name` 可以是`TIME`、`DATETIME` 或 `TIMESTAMP`。例如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    CREATE TABLE t1 (t TIME(3), dt DATETIME(6));\n    ```\n\n    `fsp` 范围是 `0` 到 `6`。\n\n    `0` 表示没有小数部分。如果省略了 `fsp`，默认为 `0`。\n\n+ 当插入包含小数部分的 `TIME`、`DATETIME` 或 `TIMESTAMP` 时，如果小数部分的位数过少或过多，可能需要进行四舍五入。例如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    CREATE TABLE fractest( c1 TIME(2), c2 DATETIME(2), c3 TIMESTAMP(2) );\n    ```\n\n    ```sql\n    Query OK, 0 rows affected (0.33 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    INSERT INTO fractest VALUES\n         > ('17:51:04.777', '2014-09-08 17:51:04.777',   '2014-09-08 17:51:04.777');\n    ```\n\n    ```sql\n    Query OK, 1 row affected (0.03 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT * FROM fractest;\n    ```\n\n    ```sql\n    +-------------|------------------------|------------------------+\n    | c1          | c2                     | c3                     |\n    +-------------|------------------------|------------------------+\n    | 17:51:04.78 | 2014-09-08 17:51:04.78 | 2014-09-08 17:51:04.78 |\n    +-------------|------------------------|------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n## 日期和时间类型的转换\n\n在日期和时间类型之间进行转换时，有些转换可能会导致信息丢失。例如，`DATE`、`DATETIME` 和 `TIMESTAMP` 都有各自的有效值范围。`TIMESTAMP` 不能早于 UTC 时间的 1970 年，也不能晚于 UTC 时间的 `2038-01-19 03:14:07`。根据这个规则，`1968-01-01` 对于 `DATE` 或 `DATETIME` 是有效的，但当 `1968-01-01` 转换为 `TIMESTAMP` 时，就会变成 0。\n\n`DATE` 的转换：\n\n+ 当 `DATE` 转换为 `DATETIME` 或 `TIMESTAMP` 时，会添加时间部分 `00:00:00`，因为 `DATE` 不包含任何时间信息。\n+ 当 `DATE` 转换为 `TIME` 时，结果是 `00:00:00`。\n\n`DATETIME` 或 `TIMESTAMP` 的转换：\n\n+ 当 `DATETIME` 或 `TIMESTAMP` 转换为 `DATE` 时，时间和小数部分将被舍弃。例如，`1999-12-31 23:59:59.499` 被转换为 `1999-12-31`。\n+ 当 `DATETIME` 或 `TIMESTAMP` 转换为 `TIME` 时，日期部分被舍弃，因为 `TIME` 不包含任何日期信息。\n\n如果要将 `TIME` 转换为其他时间和日期格式，日期部分会自动指定为 `CURRENT_DATE()`。最终的转换结果是由 `TIME` 和 `CURRENT_DATE()` 组成的日期。也就是说，如果 `TIME` 的值超出了 `00:00:00` 到 `23:59:59` 的范围，那么转换后的日期部分并不表示当前的日期。\n\n当 `TIME` 转换为 `DATE` 时，转换过程类似，时间部分被舍弃。\n\n使用 `CAST()` 函数可以显式地将值转换为 `DATE` 类型。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\ndate_col = CAST(datetime_col AS DATE)\n```\n\n将 `TIME` 和 `DATETIME` 转换为数字格式。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT CURTIME(), CURTIME()+0, CURTIME(3)+0;\n```\n\n```sql\n+-----------|-------------|--------------+\n| CURTIME() | CURTIME()+0 | CURTIME(3)+0 |\n+-----------|-------------|--------------+\n| 09:28:00  |       92800 |    92800.887 |\n+-----------|-------------|--------------+\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT NOW(), NOW()+0, NOW(3)+0;\n```\n\n```sql\n+---------------------|----------------|--------------------+\n| NOW()               | NOW()+0        | NOW(3)+0           |\n+---------------------|----------------|--------------------+\n| 2012-08-15 09:28:00 | 20120815092800 | 20120815092800.889 |\n+---------------------|----------------|--------------------+\n```\n\n## 年份为两位数\n\n如果日期中包含年份为两位数，这个年份是有歧义的，并不显式地表示实际年份。\n\n对于 `DATETIME`、`DATE` 和 `TIMESTAMP` 类型，TiDB 使用如下规则来消除歧义。\n\n- 将 01 至 69 之间的值转换为 2001 至 2069 之间的值。\n- 将 70 至 99 之间的值转化为 1970 至 1999 之间的值。\n\n上述规则也适用于 `YEAR` 类型，但有一个例外。将数字 `00` 插入到 `YEAR(4)` 中时，结果是 0000 而不是 2000。\n\n如果想让结果是 2000，需要指定值为 `2000`。\n\n对于 `MIN()` 和 `MAX()` 等函数，年份为两位数时可能会得到错误的计算结果。建议年份为四位数时使用这类函数。\n"
        },
        {
          "name": "data-type-default-values.md",
          "type": "blob",
          "size": 5.6689453125,
          "content": "---\ntitle: 数据类型的默认值\naliases: ['/docs-cn/dev/data-type-default-values/','/docs-cn/dev/reference/sql/data-types/default-values/']\nsummary: 数据类型的默认值描述了列的默认值设置规则。默认值必须是常量，对于时间类型可以使用特定函数。从 v8.0.0 开始，BLOB、TEXT 和 JSON 可以设置表达式默认值。如果列没有设置 DEFAULT，TiDB 会根据规则添加隐式默认值。对于 NOT NULL 列，根据 SQL_MODE 进行不同行为。表达式默认值是实验特性，不建议在生产环境中使用。MySQL 8.0.13 开始支持在 DEFAULT 子句中指定表达式为默认值。TiDB 支持为 BLOB、TEXT 和 JSON 数据类型分配默认值，但仅支持通过表达式来设置。\n---\n\n# 数据类型的默认值\n\n在一个数据类型描述中的 `DEFAULT value` 段描述了一个列的默认值。\n\n所有数据类型都可以设置默认值。这个默认值通常情况下必须是常量，不可以是一个函数或者是表达式，但也存在以下例外情况：\n\n- 对于时间类型，可以使用 `NOW`、`CURRENT_TIMESTAMP`、`LOCALTIME`、`LOCALTIMESTAMP` 等函数作为 `DATETIME` 或者 `TIMESTAMP` 列的默认值。\n- 对于整数类型，可以使用 `NEXT VALUE FOR` 函数将序列的下一个值作为列的默认值，使用 [`RAND()`](/functions-and-operators/numeric-functions-and-operators.md) 函数生成随机浮点值作为列的默认值。\n- 对于字符串类型，可以使用 [`UUID()`](/functions-and-operators/miscellaneous-functions.md) 函数生成[通用唯一标识符 (UUID)](/best-practices/uuid.md) 作为列的默认值。\n- 对于二进制类型，可以使用 [`UUID_TO_BIN()`](/functions-and-operators/miscellaneous-functions.md) 函数将 UUID 转换为二进制格式后作为列的默认值。\n- 从 v8.0.0 开始，新增支持 [`BLOB`](/data-type-string.md#blob-类型)、[`TEXT`](/data-type-string.md#text-类型) 以及 [`JSON`](/data-type-json.md#json-数据类型) 这三种数据类型设置默认值，但仅支持使用表达式设置[默认值](#表达式默认值)。\n\n如果一个列的定义中没有 `DEFAULT` 的设置。TiDB 按照如下的规则决定：\n\n* 如果该类型可以使用 `NULL` 作为值，那么这个列会在定义时添加隐式的默认值设置 `DEFAULT NULL`。\n* 如果该类型无法使用 `NULL` 作为值，那么这个列在定义时不会添加隐式的默认值设置。\n\n对于一个设置了 `NOT NULL` 但是没有显式设置 `DEFAULT` 的列，当 `INSERT`、`REPLACE` 没有涉及到该列的值时，TiDB 根据当时的 `SQL_MODE` 进行不同的行为：\n\n* 如果此时是 `strict sql mode`，在事务中的语句会导致事务失败并回滚，非事务中的语句会直接报错。\n* 如果此时不是 `strict sql mode`，TiDB 会为这列赋值为列数据类型的隐式默认值。\n\n此时隐式默认值的设置按照如下规则：\n\n* 对于数值类型，它们的默认值是 0。当有 `AUTO_INCREMENT` 参数时，默认值会按照增量情况赋予正确的值。\n* 对于除了时间戳外的日期时间类型，默认值会是该类型的“零值”。时间戳类型的默认值会是当前的时间。\n* 对于除枚举以外的字符串类型，默认值会是空字符串。对于枚举类型，默认值是枚举中的第一个值。\n\n## 表达式默认值\n\nMySQL 从 8.0.13 开始支持在 `DEFAULT` 子句中指定表达式为默认值。具体可参考 [Explicit Default Handling as of MySQL 8.0.13](https://dev.mysql.com/doc/refman/8.0/en/data-type-defaults.html#data-type-defaults-explicit)。\n\nTiDB 支持在 `DEFAULT` 子句中指定以下表达式作为字段的默认值：\n\n* `UPPER(SUBSTRING_INDEX(USER(), '@', 1))`\n* `REPLACE(UPPER(UUID()), '-', '')`\n* `DATE_FORMAT` 相关表达式，具体格式如下：\n    * `DATE_FORMAT(NOW(), '%Y-%m')`\n    * `DATE_FORMAT(NOW(), '%Y-%m-%d')`\n    * `DATE_FORMAT(NOW(), '%Y-%m-%d %H.%i.%s')`\n    * `DATE_FORMAT(NOW(), '%Y-%m-%d %H:%i:%s')`\n* `STR_TO_DATE('1980-01-01', '%Y-%m-%d')`\n* [`CURRENT_TIMESTAMP()`](/functions-and-operators/date-and-time-functions.md) 和 [`CURRENT_DATE()`](/functions-and-operators/date-and-time-functions.md)：均使用默认的时间精度(fractional seconds precision, fsp)\n* [`JSON_OBJECT()`](/functions-and-operators/json-functions.md)，[`JSON_ARRAY()`](/functions-and-operators/json-functions.md)，[`JSON_QUOTE()`](/functions-and-operators/json-functions.md)\n* [`NEXTVAL()`](/functions-and-operators/sequence-functions.md#nextval)\n* [`RAND()`](/functions-and-operators/numeric-functions-and-operators.md)\n* [`UUID()`](/functions-and-operators/miscellaneous-functions.md#uuid)，[`UUID_TO_BIN()`](/functions-and-operators/miscellaneous-functions.md#uuid_to_bin)\n* [`VEC_FROM_TEXT()`](/vector-search/vector-search-functions-and-operators.md#vec_from_text)\n\nTiDB 支持为 `BLOB`、`TEXT` 以及 `JSON` 数据类型分配默认值，但是，你只能使用表达式来设置这些数据类型的默认值，而不能使用字面量。\n\n以下是 `BLOB` 的示例：\n\n```sql\nCREATE TABLE t2 (\n  b BLOB DEFAULT (RAND())\n);\n```\n\n以下是使用 UUID 的示例：\n\n```sql\nCREATE TABLE t3 (\n  uuid BINARY(16) DEFAULT (UUID_TO_BIN(UUID())),\n  name VARCHAR(255)\n);\n```\n\n更多关于如何使用 UUID 的内容，请参考 [UUID 最佳实践](/best-practices/uuid.md)。\n\n以下是使用 `JSON` 的示例：\n\n```sql\nCREATE TABLE t4 (\n  id bigint AUTO_RANDOM PRIMARY KEY,\n  j json DEFAULT (JSON_OBJECT(\"a\", 1, \"b\", 2))\n);\n```\n\n以下是使用 `JSON` 时不被允许的示例：\n\n```sql\nCREATE TABLE t5 (\n  id bigint AUTO_RANDOM PRIMARY KEY,\n  j json DEFAULT ('{\"a\": 1, \"b\": 2}')\n);\n```\n\n最后两个示例都描述了相似的默认值，但只有第一个是允许的，因为它使用的是表达式而不是字面量。\n"
        },
        {
          "name": "data-type-json.md",
          "type": "blob",
          "size": 5.8974609375,
          "content": "---\ntitle: JSON 数据类型\naliases: ['/docs-cn/dev/data-type-json/','/docs-cn/dev/reference/sql/data-types/json/']\nsummary: JSON 类型存储半结构化数据，使用 Binary 格式序列化，加快查询和解析速度。JSON 字段不能创建索引，但可以对 JSON 文档中的子字段创建索引。TiDB 仅支持下推部分 JSON 函数到 TiFlash，不建议使用 BR 恢复包含 JSON 列的数据到 v6.3.0 之前的 TiDB 集群。请勿同步非标准 JSON 类型的数据。MySQL 误标记二进制类型数据为 STRING 类型，TiDB 保持正确的二进制类型。ENUM 或 SET 数据类型转换为 JSON 时，TiDB 会检查格式正确性。TiDB 支持使用 ORDER BY 对 JSON Array 或 JSON Object 进行排序。在 INSERT JSON 列时，TiDB 会将值隐式转换为 JSON。\n---\n\n# JSON 数据类型\n\nJSON 类型可以存储 JSON 这种半结构化的数据，相比于直接将 JSON 存储为字符串，它的好处在于：\n\n1. 使用 Binary 格式进行序列化，对 JSON 的内部字段的查询、解析加快；\n2. 多了 JSON 合法性验证的步骤，只有合法的 JSON 文档才可以放入这个字段中；\n\nJSON 字段本身上，并不能创建索引，但是可以对 JSON 文档中的某个子字段创建索引。例如：\n\n```sql\nCREATE TABLE city (\n    id INT PRIMARY KEY,\n    detail JSON,\n    population INT AS (JSON_EXTRACT(detail, '$.population')),\n    index index_name (population)\n);\nINSERT INTO city (id,detail) VALUES (1, '{\"name\": \"Beijing\", \"population\": 100}');\nSELECT id FROM city WHERE population >= 100;\n```\n\n更多信息，请参考 [JSON 函数](/functions-and-operators/json-functions.md)和[生成列](/generated-columns.md)。\n\n## JSON 值的类型\n\nJSON 文档中的每个值都属于一种特定的数据类型。可以通过 [`JSON_TYPE`()](/functions-and-operators/json-functions/json-functions-return.md#json_type) 的输出结果查看。\n\n| 类型             | 示例                        |\n|------------------|--------------------------------|\n| ARRAY            | `[]`                           |\n| BIT              |                                |\n| BLOB             | `0x616263`                          |\n| BOOLEAN          | `true`                         |\n| DATE             | `\"2025-06-14\"`                 |\n| DATETIME         | `\"2025-06-14 09:05:10.000000\"` |\n| DOUBLE           | `1.14`                         |\n| INTEGER          | `5`                            |\n| NULL             | `null`                         |\n| OBJECT           | `{}`                           |\n| OPAQUE           |                                |\n| STRING           | `\"foobar\"`                     |\n| TIME             | `\"09:10:00.000000\"`            |\n| UNSIGNED INTEGER | `9223372036854776000`          |\n\n## 使用限制\n\n- 目前 TiDB 仅支持下推部分 JSON 函数到 TiFlash。详情请参考 [TiFlash 支持下推的表达式](/tiflash/tiflash-supported-pushdown-calculations.md#支持下推的表达式)。\n- TiDB Backup & Restore（BR）在 v6.3.0 版本对 JSON 列的数据的编码进行了修改。因此不建议使用 BR 恢复包含 JSON 列的数据到 v6.3.0 之前的 TiDB 集群。\n- 请勿使用任何同步工具同步非标准 JSON 类型（例如 DATE、DATETIME、TIME 等）的数据。\n\n## MySQL 兼容性\n\n- 当使用二进制类型数据创建 JSON 时，目前 MySQL 会将其误标记为 STRING 类型，而 TiDB 会保持正确的二进制类型。\n\n    ```sql\n    CREATE TABLE test(a json);\n    INSERT INTO test SELECT json_objectagg('a', b'01010101');\n\n    -- 在 TiDB 中，执行以下 SQL 语句返回结果如下所示。在 MySQL 中，执行以下 SQL 语句的结果为 `0, 1`。\n    mysql> SELECT JSON_EXTRACT(JSON_OBJECT('a', b'01010101'), '$.a') = \"base64:type15:VQ==\" AS r1, JSON_EXTRACT(a, '$.a') = \"base64:type15:VQ==\" AS r2 FROM test;\n    +------+------+\n    | r1   | r2   |\n    +------+------+\n    |    0 |    0 |\n    +------+------+\n    1 row in set (0.01 sec)\n    ```\n\n    详情可见此 [issue](https://github.com/pingcap/tidb/issues/37443)。\n\n- 当将 ENUM 或 SET 数据类型转换为 JSON 时，TiDB 会检查其格式正确性。例如，当执行下面的 SQL 语句时，TiDB 中会报错：\n\n    ```sql\n    CREATE TABLE t(e ENUM('a'));\n    INSERT INTO t VALUES ('a');\n    mysql> SELECT CAST(e AS JSON) FROM t;\n    ERROR 3140 (22032): Invalid JSON text: The document root must not be followed by other values.\n    ```\n\n    详情可见此 [issue](https://github.com/pingcap/tidb/issues/9999)。\n\n- TiDB 支持使用 `ORDER BY` 对 JSON Array 或 JSON Object 进行排序。\n\n    当使用 `ORDER BY` 对 JSON Array 或 JSON Object 进行排序时，MySQL 会返回一个警告，且排序结果与比较运算结果不一致：\n\n    ```sql\n    CREATE TABLE t(j JSON);\n    INSERT INTO t VALUES ('[1,2,3,4]');\n    INSERT INTO t VALUES ('[5]');\n\n    mysql> SELECT j FROM t WHERE j < JSON_ARRAY(5);\n    +--------------+\n    | j            |\n    +--------------+\n    | [1, 2, 3, 4] |\n    +--------------+\n    1 row in set (0.00 sec)\n\n    -- 在 TiDB 中，执行以下 SQL 语句返回结果如下所示。在 MySQL 中，执行以下 SQL 语句会返回警告 “This version of MySQL doesn't yet support 'sorting of non-scalar JSON values'. ”，且排序结果与 `<` 比较结果不一致。\n    mysql> SELECT j FROM t ORDER BY j;\n    +--------------+\n    | j            |\n    +--------------+\n    | [1, 2, 3, 4] |\n    | [5]          |\n    +--------------+\n    2 rows in set (0.00 sec)\n    ```\n\n    详情可见此 [issue](https://github.com/pingcap/tidb/issues/37506)。\n\n- 在 INSERT JSON 列时，TiDB 会将值隐式转换为 JSON：\n\n    ```sql\n    CREATE TABLE t(col JSON);\n\n    -- 在 TiDB 中，执行以下 INSERT 语句成功。在 MySQL 中，执行以下 INSERT 语句将返回 Invalid JSON text 错误。\n    INSERT INTO t VALUES (3);\n    ```\n\n有关 JSON 的更多信息，可以参考 [JSON 函数](/functions-and-operators/json-functions.md)和[生成列](/generated-columns.md)。\n"
        },
        {
          "name": "data-type-numeric.md",
          "type": "blob",
          "size": 7.8212890625,
          "content": "---\ntitle: 数值类型\naliases: ['/docs-cn/dev/data-type-numeric/','/docs-cn/dev/reference/sql/data-types/numeric/']\nsummary: TiDB 支持 MySQL 的所有数值类型，包括整数类型、浮点类型和定点类型。整数类型包括 BIT、BOOLEAN、TINYINT、SMALLINT、MEDIUMINT、INTEGER 和 BIGINT，存储空间和取值范围各不相同。浮点类型包括 FLOAT 和 DOUBLE，存储空间分别为 4 和 8 字节。定点类型包括 DECIMAL 和 NUMERIC，可设置小数位数和小数点后位数。建议使用 DECIMAL 类型存储精确值。\n---\n\n# 数值类型\n\nTiDB 支持 MySQL 所有的数值类型，按照精度可以分为：\n\n+ [整数类型（精确值）](#整数类型)\n+ [浮点类型（近似值）](#浮点类型)\n+ [定点类型（精确值）](#定点类型)\n\n## 整数类型\n\nTiDB 支持 MySQL 所有的整数类型，包括 `INTEGER`/`INT`、`TINYINT`、`SMALLINT`、`MEDIUMINT` 以及 `BIGINT`，完整信息参考[这篇](https://dev.mysql.com/doc/refman/8.0/en/integer-types.html)文档。\n\n字段说明：\n\n> **警告：**\n>\n> 从 v8.5.0 开始，整数显示宽度功能已废弃（[`deprecate-integer-display-length`](/tidb-configuration-file.md#deprecate-integer-display-length) 默认为 `true`）。不建议为整数类型指定显示宽度。\n\n| 语法元素 | 说明 |\n| ---- | --------|\n| M | 类型显示宽度，可选 |\n| UNSIGNED | 无符号数，如果不加这个标识，则为有符号数 |\n| ZEROFILL | 补零标识，如果有这个标识，TiDB 会自动给类型增加 UNSIGNED 标识，但是没有做补零的操作 |\n\n### 类型定义\n\n#### `BIT` 类型\n\n比特值类型。M 表示比特位的长度，取值范围从 1 到 64，其默认值是 1。\n\n{{< copyable \"sql\" >}}\n\n```sql\nBIT[(M)]\n```\n\n#### `BOOLEAN` 类型\n\n布尔类型，别名为 `BOOL`，和 `TINYINT(1)` 等价。零值被认为是 `False`，非零值认为是 `True`。在 TiDB 内部，`True` 存储为 `1`，`False` 存储为 `0`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nBOOLEAN\n```\n\n#### `TINYINT` 类型\n\n`TINYINT` 类型。有符号数的范围是 `[-128, 127]`。无符号数的范围是 `[0, 255]`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nTINYINT[(M)] [UNSIGNED] [ZEROFILL]\n```\n\n#### `SMALLINT` 类型\n\n`SMALLINT` 类型。有符号数的范围是 `[-32768, 32767]`。无符号数的范围是 `[0, 65535]`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSMALLINT[(M)] [UNSIGNED] [ZEROFILL]\n```\n\n#### `MEDIUMINT` 类型\n\n`MEDIUMINT` 类型。有符号数的范围是 `[-8388608, 8388607]`。无符号数的范围是 `[0, 16777215]`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nMEDIUMINT[(M)] [UNSIGNED] [ZEROFILL]\n```\n\n### `INTEGER` 类型\n\n`INTEGER` 类型，别名 `INT`。有符号数的范围是 `[-2147483648, 2147483647]`。无符号数的范围是 `[0, 4294967295]`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nINT[(M)] [UNSIGNED] [ZEROFILL]\n```\n\n或者：\n\n{{< copyable \"sql\" >}}\n\n```sql\nINTEGER[(M)] [UNSIGNED] [ZEROFILL]\n```\n\n### `BIGINT` 类型\n\n`BIGINT` 类型。有符号数的范围是 `[-9223372036854775808, 9223372036854775807]`。无符号数的范围是 `[0, 18446744073709551615]`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nBIGINT[(M)] [UNSIGNED] [ZEROFILL]\n```\n\n### 存储空间以及取值范围\n\n每种类型对存储空间的需求以及最大/最小值如下表所示：\n\n| 类型        | 存储空间 | 最小值(有符号/无符号) | 最大值(有符号/无符号) |\n| ----------- |----------|-----------------------| --------------------- |\n| `TINYINT`   | 1        | -128 / 0              | 127 / 255             |\n| `SMALLINT`  | 2        | -32768 / 0            | 32767 / 65535         |\n| `MEDIUMINT` | 3        | -8388608 / 0          | 8388607 / 16777215    |\n| `INT`       | 4        | -2147483648 / 0       | 2147483647 / 4294967295 |\n| `BIGINT`    | 8        | -9223372036854775808 / 0 | 9223372036854775807 / 18446744073709551615 |\n\n## 浮点类型\n\nTiDB 支持 MySQL 所有的浮点类型，包括 `FLOAT`、`DOUBLE`，完整信息参考[这篇](https://dev.mysql.com/doc/refman/8.0/en/floating-point-types.html)文档。\n\n字段说明：\n\n| 语法元素 | 说明                            |\n| -------- | ------------------------------- |\n| M        | 小数总位数 |\n| D        | 小数点后位数 |\n| UNSIGNED | 无符号数，如果不加这个标识，则为有符号数 |\n| ZEROFILL | 补零标识，如果有这个标识，TiDB 会自动给类型增加 UNSIGNED 标识 |\n\n### 类型定义\n\n#### `FLOAT` 类型\n\n单精度浮点数。允许的值范围为 -2^128 ~ +2^128，也即 -3.402823466E+38 到 -1.175494351E-38、0 和 1.175494351E-38 到 3.402823466E+38。这些是基于 IEEE 标准的理论限制。实际的范围根据硬件或操作系统的不同可能稍微小些。\n\n`FLOAT(p)` 类型中，`p` 表示精度（以位数表示）。只使用该值来确定是否结果列的数据类型为 `FLOAT` 或 `DOUBLE`。如果 `p` 为从 0 到 24，数据类型变为没有 M 或 D 值的 `FLOAT`。如果 `p` 为从 25 到 53，数据类型变为没有 M 或 D 值的 `DOUBLE`。结果列范围与本节前面描述的单精度 `FLOAT` 或双精度 `DOUBLE` 数据类型相同。\n\n{{< copyable \"sql\" >}}\n\n```sql\nFLOAT[(M,D)] [UNSIGNED] [ZEROFILL]\nFLOAT(p) [UNSIGNED] [ZEROFILL]\n```\n\n> **注意：**\n>\n> + 与在 MySQL 中一样，`FLOAT` 数据类型用于存储近似值。对于类似货币的精确值，建议使用 `DECIMAL` 类型。\n> + 在 TiDB 中，`FLOAT` 数据类型默认的精度是 8 位，这与 MySQL 不同。在 MySQL 中，`FLOAT` 数据类型默认的精度是 6 位。例如，同时往 MySQL 和 TiDB 中类型为 `FLOAT` 的列中插入数据 `123456789` 和 `1.23456789`。在 MySQL 中查询对应的值，将会得到结果 `123457000` 和 `1.23457`。而在 TiDB 中查询对应的值，将会得到 `123456790` 和 `1.2345679`。\n\n#### `DOUBLE` 类型\n\n双精度浮点数，别名为 `DOUBLE PRECISION`。允许的值范围为：-2^1024 ~ +2^1024，也即是 -1.7976931348623157E+308 到 -2.2250738585072014E-308、0 和 2.2250738585072014E-308 到 1.7976931348623157E+308。这些是基于 IEEE 标准的理论限制。实际的范围根据硬件或操作系统的不同可能稍微小些。\n\n{{< copyable \"sql\" >}}\n\n```sql\nDOUBLE[(M,D)] [UNSIGNED] [ZEROFILL]\nDOUBLE PRECISION [(M,D)] [UNSIGNED] [ZEROFILL], REAL[(M,D)] [UNSIGNED] [ZEROFILL]\n```\n\n> **警告：**\n>\n> 与在 MySQL 中一样，`DOUBLE` 数据类型存储近似值。对于货币之类的精确值，建议使用 `DECIMAL` 类型。\n\n> **注意：**\n>\n> 当 TiDB 将用科学计数法表示的双精度浮点数转换到 `CHAR` 类型时，其结果在显示上与 MySQL 不一致，详情参见 [Cast 函数和操作符](/functions-and-operators/cast-functions-and-operators.md)。\n\n### 存储空间\n\n每种类型对存储空间的需求如下表所示：\n\n| 类型        | 存储空间 |\n| ----------- |----------|\n| `FLOAT`     | 4        |\n| `FLOAT(p)`  | 如果 0 <= p <= 24 为 4 个字节，如果 25 <= p <= 53 为 8 个字节|\n| `DOUBLE`    | 8        |\n\n## 定点类型\n\nTiDB 支持 MySQL 所有的定点类型，包括 `DECIMAL`、`NUMERIC`，完整信息参考[这篇](https://dev.mysql.com/doc/refman/8.0/en/fixed-point-types.html)文档。\n\n字段说明：\n\n| 语法元素 | 说明                            |\n| -------- | ------------------------------- |\n| M        | 小数总位数 |\n| D        | 小数点后位数 |\n| UNSIGNED | 无符号数，如果不加这个标识，则为有符号数 |\n| ZEROFILL | 补零标识，如果有这个标识，TiDB 会自动给类型增加 UNSIGNED 标识 |\n\n### 类型定义\n\n#### `DECIMAL` 类型\n\n定点数，别名为 `NUMERIC`。M 是小数位数（精度）的总数，D 是小数点（标度）后面的位数。小数点和 `-`（负数）符号不包括在 M 中。如果 D 是 0，则值没有小数点或分数部分。如果 D 被省略，默认是 0。如果 M 被省略，默认是 10。\n\n{{< copyable \"sql\" >}}\n\n```sql\nDECIMAL[(M[,D])] [UNSIGNED] [ZEROFILL]\nNUMERIC[(M[,D])] [UNSIGNED] [ZEROFILL]\n```\n"
        },
        {
          "name": "data-type-overview.md",
          "type": "blob",
          "size": 1.76953125,
          "content": "---\ntitle: 数据类型概述\naliases: ['/docs-cn/dev/data-type-overview/','/docs-cn/dev/reference/sql/data-types/overview/']\nsummary: TiDB 支持除了空间类型（SPATIAL）之外的所有 MySQL 数据类型，包括数值型类型、字符串类型、时间和日期类型、JSON 类型。数据类型定义一般为 T(M[, D])，其中 T 表示具体的类型，M 在整数类型中表示最大显示长度，在浮点数或者定点数中表示精度，在字符类型中表示最大长度，D 表示浮点数、定点数的小数位长度，fsp 在时间和日期类型里的 TIME、DATETIME 以及 TIMESTAMP 中表示秒的精度，其取值范围是 0 到 6，值为 0 表示没有小数部分，如果省略，则默认精度为 0。\n---\n\n# 数据类型概述\n\nTiDB 支持除空间类型 (`SPATIAL`) 之外的所有 MySQL 数据类型，包括[数值型类型](/data-type-numeric.md)、[字符串类型](/data-type-string.md)、[时间和日期类型](/data-type-date-and-time.md)、[JSON 类型](/data-type-json.md)。\n\n数据类型定义一般为 `T(M[, D])`，其中：\n\n* `T` 表示具体的类型。\n* `M` 在整数类型中表示最大显示长度；在浮点数或者定点数中表示精度；在字符类型中表示最大长度。`M` 的最大值取决于具体的类型。\n\n    > **警告：**\n    >\n    > 从 v8.5.0 开始，整数显示宽度功能已废弃（[`deprecate-integer-display-length`](/tidb-configuration-file.md#deprecate-integer-display-length) 默认为 `true`）。不建议为整数类型指定显示宽度。\n\n* `D` 表示浮点数、定点数的小数位长度。\n* `fsp` 在时间和日期类型里的 `TIME`、`DATETIME` 以及 `TIMESTAMP` 中表示秒的精度，其取值范围是 0 到 6。值为 0 表示没有小数部分。如果省略，则默认精度为 0。\n"
        },
        {
          "name": "data-type-string.md",
          "type": "blob",
          "size": 7.056640625,
          "content": "---\ntitle: 字符串类型\naliases: ['/docs-cn/dev/data-type-string/','/docs-cn/dev/reference/sql/data-types/string/']\nsummary: TiDB 支持 MySQL 所有字符串类型，包括 CHAR、VARCHAR、BINARY、VARBINARY、BLOB、TEXT、ENUM 和 SET。CHAR 是定长字符串，长度固定为创建表时声明的长度。VARCHAR 是变长字符串，空间占用大小不得超过 65535 字节。TEXT 是文本串，最大列长为 65535 字节。TINYTEXT 最大列长度为 255。MEDIUMTEXT 最大列长度为 16777215。LONGTEXT 最大列长度为 4294967295。BINARY 存储二进制字符串。VARBINARY 存储二进制字符串。BLOB 是二进制大文件，最大列长度为 65535 字节。TINYBLOB 最大列长度为 255。MEDIUMBLOB 最大列长度为 16777215。LONGBLOB 最大列长度为 4294967295。ENUM 是枚举类型，值必须从固定集合中选取。SET 是集合类型，包含零个或多个值的字符串。\n---\n\n# 字符串类型\n\nTiDB 支持 MySQL 所有的字符串类型，包括 `CHAR`、`VARCHAR`、`BINARY`、`VARBINARY`、`BLOB`、`TEXT`、`ENUM` 以及 `SET`，完整信息参考[这篇](https://dev.mysql.com/doc/refman/8.0/en/string-types.html)文档。\n\n## 类型定义\n\n### `CHAR` 类型\n\n定长字符串。`CHAR` 列的长度固定为创建表时声明的长度。M 表示列长度（字符的个数，不是字节的个数）。长度可以为从 0 到 255 的任何值。和 `VARCHAR` 类型不同，`CHAR` 列在写入时会对数据末尾的空格进行截断。\n\n{{< copyable \"sql\" >}}\n\n```sql\n[NATIONAL] CHAR[(M)] [CHARACTER SET charset_name] [COLLATE collation_name]\n```\n\n### `VARCHAR` 类型\n\n变长字符串。M 表示最大列长度（字符的最大个数）。`VARCHAR` 的空间占用大小不得超过 65535 字节。在选择 `VARCHAR` 长度时，应当根据最长的行的大小和使用的字符集确定。\n\n对于不同的字符集，单个字符所占用的空间可能有所不同。以下表格是各个字符集下单个字符占用的字节数，以及 `VARCHAR` 列长度的取值范围：\n\n| 字符集 | 单个字符字节数 | VARCHAR 最大列长度的取值范围 |\n| ----- | ---- | ---- |\n| ascii | 1 | (0, 65535] |\n| latin1 | 1 | (0, 65535] |\n| binary | 1 | (0, 65535] |\n| utf8 | 3 | (0, 21845] |\n| utf8mb4 | 4 | (0, 16383] |\n\n{{< copyable \"sql\" >}}\n\n```sql\n[NATIONAL] VARCHAR(M) [CHARACTER SET charset_name] [COLLATE collation_name]\n```\n\n### `TEXT` 类型\n\n文本串。最大列长为 65,535 字节。可选的 M 参数以字符为单位，用于自动选择 `TEXT` 列的最合适类型。例如 `TEXT(60)` 会产生一个 `TINYTEXT` 数据类型，最多可容纳 255 字节，即容纳一个 60 字符的 UTF-8 字符串，每个字符最多包含 4 字节（即 4×60=240）。不推荐使用 M 参数。\n\n{{< copyable \"sql\" >}}\n\n```sql\nTEXT[(M)] [CHARACTER SET charset_name] [COLLATE collation_name]\n```\n\n### `TINYTEXT` 类型\n\n类似于 [`TEXT`](#text-类型)，区别在于最大列长度为 255。\n\n{{< copyable \"sql\" >}}\n\n```sql\nTINYTEXT [CHARACTER SET charset_name] [COLLATE collation_name]\n```\n\n### `MEDIUMTEXT` 类型\n\n类似于 [`TEXT`](#text-类型)，区别在于最大列长度为 16,777,215。但由于 [`txn-entry-size-limit`](/tidb-configuration-file.md#txn-entry-size-limit-从-v4010-和-v500-版本开始引入) 的限制，TiDB 中默认单行存储最大不超过 6 MiB，可通过配置项将该限制调整至 120 MiB。\n\n{{< copyable \"sql\" >}}\n\n```sql\nMEDIUMTEXT [CHARACTER SET charset_name] [COLLATE collation_name]\n```\n\n### `LONGTEXT` 类型\n\n类似于 [`TEXT`](#text-类型)，区别在于最大列长度为 4,294,967,295。但由于 [`txn-entry-size-limit`](/tidb-configuration-file.md#txn-entry-size-limit-从-v4010-和-v500-版本开始引入) 的限制，TiDB 中默认单行存储最大不超过 6 MiB，可通过配置项将该限制调整至 120 MiB。\n\n{{< copyable \"sql\" >}}\n\n```sql\nLONGTEXT [CHARACTER SET charset_name] [COLLATE collation_name]\n```\n\n### `BINARY` 类型\n\n类似于 [`CHAR`](#char-类型)，区别在于 `BINARY` 存储的是二进制字符串。\n\n{{< copyable \"sql\" >}}\n\n```sql\nBINARY(M)\n```\n\n### `VARBINARY` 类型\n\n类似于 [`VARCHAR`](#varchar-类型)，区别在于 `VARBINARY` 存储的是二进制字符串。\n\n{{< copyable \"sql\" >}}\n\n```sql\nVARBINARY(M)\n```\n\n### `BLOB` 类型\n\n二进制大文件。M 表示最大列长度，单位是字节，范围是 0 到 65535。\n\n{{< copyable \"sql\" >}}\n\n```sql\nBLOB[(M)]\n```\n\n### `TINYBLOB` 类型\n\n类似于 [`BLOB`](#blob-类型)，区别在于最大列长度为 255。\n\n{{< copyable \"sql\" >}}\n\n```sql\nTINYBLOB\n```\n\n### `MEDIUMBLOB` 类型\n\n类似于 [`BLOB`](#blob-类型)，区别在于最大列长度为 16,777,215。但由于 [`txn-entry-size-limit`](/tidb-configuration-file.md#txn-entry-size-limit-从-v4010-和-v500-版本开始引入) 的限制，TiDB 中默认单行存储最大不超过 6 MiB，可通过配置项将该限制调整至 120 MiB。\n\n{{< copyable \"sql\" >}}\n\n```sql\nMEDIUMBLOB\n```\n\n### `LONGBLOB` 类型\n\n类似于 [`BLOB`](#blob-类型)，区别在于最大列长度为 4,294,967,295。但由于 [`txn-entry-size-limit`](/tidb-configuration-file.md#txn-entry-size-limit-从-v4010-和-v500-版本开始引入) 的限制，TiDB 中默认单行存储最大不超过 6 MiB，可通过配置项将该限制调整至 120 MiB。\n\n{{< copyable \"sql\" >}}\n\n```sql\nLONGBLOB\n```\n\n### `ENUM` 类型\n\n枚举类型是一个字符串，它只能有一个值的字符串对象。其值必须是从一个固定集合中选取，这个固定集合在创建表的时候定义，语法是：\n\n{{< copyable \"sql\" >}}\n\n```sql\nENUM('value1','value2',...) [CHARACTER SET charset_name] [COLLATE collation_name]\n```\n\n例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nENUM('apple', 'orange', 'pear')\n```\n\n枚举类型的值在 TiDB 内部使用数值来存储，每个值会按照定义的顺序转换为一个数字，比如上面的例子中，每个字符串值都会映射为一个数字：\n\n| 值 | 数字 |\n| ---- | ---- |\n| NULL  | NULL |\n| '' | 0 |\n| 'apple' | 1 |\n| 'orange' | 2 |\n| 'pear' | 3 |\n\n更多信息参考 [MySQL 枚举文档](https://dev.mysql.com/doc/refman/8.0/en/enum.html)。\n\n### `SET` 类型\n\n集合类型是一个包含零个或多个值的字符串，其中每个值必须是从一个固定集合中选取，这个固定集合在创建表的时候定义，语法是：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET('value1','value2',...) [CHARACTER SET charset_name] [COLLATE collation_name]\n```\n\n例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET('1', '2') NOT NULL\n```\n\n上面的例子中，这列的有效值可以是：\n\n```\n''\n'1'\n'2'\n'1,2'\n```\n\n集合类型的值在 TiDB 内部会转换为一个 Int64 数值，每个元素是否存在用一个二进制位的 0/1 值来表示，比如这个例子 `SET('a','b','c','d')`，每一个元素都被映射为一个数字，且每个数字的二进制表示只会有一位是 1：\n\n| 成员 | 十进制表示 | 二进制表示 |\n| ---- | ---- | ------ |\n| 'a'  | 1 | 0001 |\n| 'b' | 2 | 0010 |\n| 'c' | 4 | 0100 |\n| 'd' | 8 | 1000 |\n\n这样对于值为 `('a', 'c')` 的元素，其二进制表示即为 0101。\n\n更多信息参考 [MySQL 集合文档](https://dev.mysql.com/doc/refman/8.0/en/set.html)。\n"
        },
        {
          "name": "ddl-introduction.md",
          "type": "blob",
          "size": 13.734375,
          "content": "---\ntitle: DDL 语句的执行原理及最佳实践\nsummary: 介绍 TiDB 中 DDL 语句的实现原理、在线变更过程、最佳实践等内容。\n---\n\n# DDL 语句的执行原理及最佳实践\n\n本文介绍了 TiDB 中 DDL 语句的执行原理（包括 DDL Owner 模块和在线变更 DDL 的流程）和最佳实践。\n\n## DDL 执行原理\n\nTiDB 采用在线异步变更的方式执行 DDL 语句，从而实现 DDL 语句的执行不会阻塞其他会话中的 DML 语句。因此，在业务执行过程中，你可以通过在线异步变更 DDL 对数据库对象定义进行变更。\n\n### DDL 语句类型简介\n\n按照执行期间是否阻塞用户业务，DDL 语句可以划分为：\n\n- **离线 DDL 语句**：即数据库接收到用户的 DDL 语句后，会先对要修改的数据库对象进行加锁，再执行元数据变更，在 DDL 执行过程中将阻塞用户业务对数据的修改。\n\n- **在线 DDL 语句**：即数据库在执行 DDL 语句时，通过一定的方法，使得 DDL 执行不阻塞用户业务，且能够保证用户业务可在 DDL 执行期间提交修改，在执行过程中保证对应对象的数据正确性与一致性。\n\n按照是否需要操作 DDL 目标对象所包括的数据来划分，DDL 语句可以划分为：\n\n- **逻辑 DDL 语句**：通常只修改数据库对象的元数据，不对变更对象存储的数据进行处理，例如变更表名或变更列名。\n\n    在 TiDB 中，逻辑 DDL 语句又被称为 General DDL。General DDL 的执行时间通常较短，只需要几十毫秒或者几秒。执行这类 DDL 语句几乎不消耗系统资源，因此不会影响业务负载。\n\n- **物理 DDL 语句**：不但会修改变更对象的元数据，同时也修改变更对象所存储的用户数据。例如，为表创建索引，不仅需要变更表的定义，同时也需要做一次全表扫描以构建新增加的索引。\n\n    在 TiDB 中，物理 DDL 被称为 Reorg DDL（Reorg 即 Reorganization）。目前物理 DDL 只包含 `ADD INDEX` 以及有损列类型变更（例如从 `INT` 转成 `CHAR` 类型）这两种类型。物理 DDL 的特点是执行时间较长，且执行时间与表的数据量、机器配置以及业务负载有关。\n\n    执行物理 DDL 会影响业务负载，具体有两个方面。一方面需要从 TiKV 中读取数据并写入新数据，因此会消耗 TiKV 的 CPU 及 I/O 资源。另一方面，**DDL Owner 所在的 TiDB 节点**或者**被 TiDB 分布式执行框架调度而执行 `ADD INDEX` 任务的 TiDB 节点**需要进行相应的计算，因此会消耗 TiDB 的 CPU 资源。\n\n    > **注意：**\n    >\n    > DDL 语句对用户业务的影响通常都是由于执行物理 DDL 任务造成的。因此要优化 DDL 语句对于用户业务的影响，重点在于对物理 DDL 任务执行期间的设计，降低对于用户业务的影响。\n\n### TiDB DDL 模块\n\nTiDB DDL 模块引入 DDL Owner（简称 Owner）角色来代理执行所有进入到 TiDB 集群中的 DDL 语句。对于当前 TiDB DDL 模块的实现，在同一时间，整个 TiDB 集群中只有一个 TiDB 节点能当选为 Owner。当选 Owner 后，TiDB 节点中启动的 worker 才能处理集群中的 DDL 任务。\n\nTiDB 通过 etcd 的选举功能从多个 TiDB 节点中选举出一个节点来担任 Owner 的宿主节点。默认情况下，每个 TiDB 节点都可能当选 Owner（你可以通过配置 `run-ddl` 控制某个 TiDB 节点是否竞选 Owner）。Owner 节点是有任期的，并会主动维护自己的任期，即续约。当 Owner 节点宕机后，其他节点可以通过 etcd 感知到并重新选举出新的 Owner，在集群中继续担任 DDL 任务执行者的角色。\n\nDDL Owner 的简单示意图如下：\n\n![DDL Owner](/media/ddl-owner.png)\n\n你可以通过 `ADMIN SHOW DDL` 语句查看当前 DDL owner：\n\n```sql\nADMIN SHOW DDL;\n```\n\n```sql\n+------------+--------------------------------------+---------------+--------------+--------------------------------------+-------+\n| SCHEMA_VER | OWNER_ID                             | OWNER_ADDRESS | RUNNING_JOBS | SELF_ID                              | QUERY |\n+------------+--------------------------------------+---------------+--------------+--------------------------------------+-------+\n|         26 | 2d1982af-fa63-43ad-a3d5-73710683cc63 | 0.0.0.0:4000  |              | 2d1982af-fa63-43ad-a3d5-73710683cc63 |       |\n+------------+--------------------------------------+---------------+--------------+--------------------------------------+-------+\n1 row in set (0.00 sec)\n```\n\n### TiDB 在线 DDL 异步变更的原理\n\nTiDB DDL 模块从设计之初就选择了在线异步变更的模式，为 TiDB 的用户提供了不停机变更业务的服务能力。\n\nDDL 变更即两个状态之间的切换（变更前 -> 变更后）。在线变更 DDL，是在两个状态之间引入多个相互兼容的小版本状态。同时，在同一个 DDL 语句在执行期间，对于同一集群中不同 TiDB 节点，允许不同节点的变更对象小版本存在不同（集群中各 TiDB 节点变更对象的小版本差距不超过两个版本），因为相邻两个小版本之间可以相互兼容。\n\n通过多个小版本演进的方式，确保多个 TiDB 节点能够正确同步元数据，并保证期间执行用户事务更改数据的正确性与一致性。\n\n以 `ADD INDEX` 为例，整个变更状态流程如下：\n\n```\nabsent -> delete only -> write only -> write reorg -> public\n```\n\n对于用户来说，新建的索引在 `public` 状态前都不可用。\n\n<SimpleTab>\n<div label=\"并发 DDL 框架（TiDB v6.2 及以上）\">\n\n在 TiDB v6.2 之前，由于 Owner 每次只能执行一个同种类型（逻辑或物理）的 DDL 任务，这个约束较为严格，同时影响用户体验。\n\n当 DDL 任务之间不存在相关依赖时，并行执行并不会影响数据正确性和一致性。例如：用户 A 在 `T1` 表上增加一个索引，同时用户 B 从 `T2` 表删除一列。这两条 DDL 语句可以并行执行。\n\n为了提升 DDL 执行的用户体验，从 v6.2.0 起，TiDB 对原有的 DDL Owner 角色进行了升级，使得 Owner 能对 DDL 任务做相关性判断，判断逻辑如下：\n\n+ 涉及同一张表的 DDL 相互阻塞。\n+ `DROP DATABASE` 和数据库内所有对象的 DDL 互相阻塞。\n+ 涉及不同表的加索引和列类型变更可以并发执行。\n+ 逻辑 DDL 需要等待之前正在执行的逻辑 DDL 执行完才能执行。\n+ 其他情况下 DDL 可以根据 Concurrent DDL 并行度可用情况确定是否可以执行。\n\n具体来说，TiDB 在 v6.2.0 中对 DDL 执行框架进行了如下升级：\n\n+ DDL Owner 能够根据以上判断逻辑并行执行 DDL 任务。\n+ 改善了 DDL Job 队列先入先出的问题。DDL Owner 不再选择当前队列最前面的 DDL Job，而是选择当前可以执行的 DDL Job。\n+ 扩充了处理物理 DDL 的 worker 数量，使得能够并行地添加多个物理 DDL。\n\n    因为 TiDB 中所有支持的 DDL 任务都是以在线变更的方式来实现的，TiDB 通过 Owner 即可对新的 DDL Job 进行相关性判断，并根据相关性结果进行 DDL 任务的调度，从而使分布式数据库实现了和传统数据库中 DDL 并发相同的效果。\n\n并发 DDL 框架的实现进一步加强了 TiDB 中 DDL 语句的执行能力，并更符合商用数据库的使用习惯。\n\n</div>\n<div label=\"Online DDL 异步变更流程（TiDB v6.2.0 前）\">\n\n在 v6.2.0 之前，TiDB SQL 层中处理异步 Schema 变更的基本流程如下：\n\n1. MySQL Client 发送给 TiDB server 一个 DDL 操作请求。\n\n2. 某个 TiDB server 收到请求（即 TiDB server 的 MySQL Protocol 层对请求进行解析优化），然后发送到 TiDB SQL 层进行执行。\n\n    TiDB SQL 层接到 DDL 请求后，会启动 `start job` 模块根据请求将请求封装成特定的 DDL Job（即 DDL 任务），然后将此 Job 按语句类型分类，分别存储到 KV 层的对应 DDL Job 队列，并通知自身对应的 worker 有 Job 需要处理。\n\n3. 接收到处理 Job 通知的 worker，会判断自身是否处于 DDL Owner 的角色。如果是 Owner 角色则直接处理此 Job。如果没有处于 Owner 角色则退出不做任何处理。\n\n    假设某台 TiDB server 不是 Owner 角色，那么其他某个节点一定有一个是 Owner。处于 Owner 角色的节点的 worker 通过定期检测机制来检查是否有 Job 可以被执行。如果发现有 Job ，那么 worker 就会处理该 Job。\n\n4. Worker 处理完 Job 后，会将此 Job 从 KV 层对应的 Job queue 中移除，并放入 `job history queue`。之前封装 Job 的 `start job` 模块会定期在 `job history queue` 中查看是否有已经处理完成的 Job 的 ID。如果有，则这个 Job 对应的整个 DDL 操作结束执行。\n\n5. TiDB server 将 DDL 处理结果返回至 MySQL Client。\n\n在 TiDB v6.2.0 前，该 DDL 执行框架存在以下限制：\n\n- TiKV 集群中只有 `general job queue` 和 `add index job queue` 两个队列，分别处理逻辑 DDL 和物理 DDL。\n- DDL Owner 总是以先入先出的方式处理 DDL Job。\n- DDL Owner 每次只能执行一个同种类型（逻辑或物理）的 DDL 任务，这个约束较为严格。\n\n这些限制可能会导致一些“非预期”的 DDL 阻塞行为。具体可以参考 [SQL FAQ - DDL 执行](/faq/sql-faq.md#ddl-执行)。\n\n</div>\n</SimpleTab>\n\n## 最佳实践\n\n### 通过系统变量来平衡物理 DDL 的执行速度与对业务负载的影响\n\n执行物理 DDL（包括添加索引或列类型变更）时，适当调整以下系统变量可以平衡 DDL 执行速度与对业务负载的影响：\n\n- [`tidb_ddl_reorg_worker_cnt`](/system-variables.md#tidb_ddl_reorg_worker_cnt)：这个变量用来设置 DDL 操作 reorg worker 的数量，控制回填的并发度。\n\n- [`tidb_ddl_reorg_batch_size`](/system-variables.md#tidb_ddl_reorg_batch_size)：这个变量用来设置 DDL 操作 `re-organize` 阶段的 batch size，以控制回填的数据量。\n\n    推荐值：\n\n    - 在无其他负载情况下，如需让 `ADD INDEX` 尽快完成，可以将 `tidb_ddl_reorg_worker_cnt` 和 `tidb_ddl_reorg_batch_size` 的值适当调大，例如将两个变量值分别调为 `20` 和 `2048`。\n    - 在有其他负载情况下，如需让 `ADD INDEX` 尽量不影响其他业务，可以将 `tidb_ddl_reorg_worker_cnt` 和 `tidb_ddl_reorg_batch_size` 适当调小，例如将两个变量值分别调为 `4` 和 `256`。\n\n> **建议：**\n>\n> - 以上两个变量均可以在 DDL 任务执行过程中动态调整，并且在下一个事务批次中生效。\n> - 根据 DDL 操作的类型，并结合业务负载压力，选择合适的时间点执行，例如建议在业务负载比较低的情况运行 `ADD INDEX` 操作。\n> - 由于添加索引的时间跨度较长，发送相关的指令后，TiDB 会在后台执行任务，TiDB server 宕机不会影响继续执行。\n\n### 并发发送 DDL 请求实现快速建大量表\n\n一个建表的操作耗时大约 50 毫秒。受框架的限制，建表耗时可能更长。\n\n为了更快地建表，推荐通过并发发送多个 DDL 请求以达到最快建表速度。如果串行地发送 DDL 请求，并且没有发给 Owner 节点，则建表速度会很慢。\n\n### 在一条 `ALTER` 语句中进行多次变更\n\n自 v6.2.0 起，TiDB 支持在一条 `ALTER` 语句中修改一张表的多个模式对象（如列、索引），同时保证整个语句的原子性。因此推荐在一条 `ALTER` 语句中进行多次变更。\n\n### 检查读写性能\n\n在添加索引时，回填数据阶段会对集群造成一定的读写压力。在 `ADD INDEX` 的命令发送成功后，并且在 `write reorg` 阶段，建议检查 Grafana 面板上 TiDB 和 TiKV 读写相关的性能指标，以及业务响应时间，来确定 `ADD INDEX` 操作对集群是否造成影响。\n\n## DDL 相关的命令介绍\n\n- `ADMIN SHOW DDL`：用于查看 TiDB DDL 的状态，包括当前 schema 版本号、DDL Owner 的 DDL ID 和地址、正在执行的 DDL 任务和 SQL、当前 TiDB 实例的 DDL ID。详情参阅 [`ADMIN SHOW DDL`](/sql-statements/sql-statement-admin-show-ddl.md#admin-show-ddl)。\n\n- `ADMIN SHOW DDL JOBS`：查看集群环境中的 DDL 任务运行中详细的状态。详情参阅 [`ADMIN SHOW DDL JOBS`](/sql-statements/sql-statement-admin-show-ddl.md#admin-show-ddl-jobs)。\n\n- `ADMIN SHOW DDL JOB QUERIES job_id [, job_id]`：用于查看 job_id 对应的 DDL 任务的原始 SQL 语句。详情参阅 [`ADMIN SHOW DDL JOB QUERIES`](/sql-statements/sql-statement-admin-show-ddl.md#admin-show-ddl-job-queries)。\n\n- `ADMIN CANCEL DDL JOBS job_id [, job_id]`：用于取消已经提交但未执行完成的 DDL 任务。取消完成后，执行 DDL 任务的 SQL 语句会返回 `ERROR 8214 (HY000): Cancelled DDL job` 错误。\n\n    取消一个已经执行完成的 DDL 任务会在 `RESULT` 列看到 `DDL Job:90 not found` 的错误，表示该任务已从 DDL 等待队列中被移除。\n\n- `ADMIN PAUSE DDL JOBS job_id [, job_id]`：用于暂停正在执行的 DDL 任务。执行该命令后，执行 DDL 任务的 SQL 语句体现为正在执行，后台任务暂停执行。详情参阅 [`ADMIN PAUSE DDL JOBS`](/sql-statements/sql-statement-admin-pause-ddl.md)。\n\n    只有处于执行中或仍在等待中的 DDL 任务可以暂停，否则会在 `RESULT` 列看到 `Job 3 can't be paused now`。\n\n- `ADMIN RESUME DDL JOBS job_id [, job_id]`：用于恢复已被暂停的 DDL 任务。执行该命令后，执行 DDL 任务的 SQL 语句体现为正在执行，后台任务正常执行。详情参阅 [`ADMIN RESUME DDL JOBS`](/sql-statements/sql-statement-admin-resume-ddl.md)。\n\n    你只能对暂停状态的 DDL 任务进行恢复操作，否则会在 `RESULT` 列看到 `Job 3 can't be resumed`。\n\n## 常见问题\n\nDDL 语句执行相关的常见问题，参考 [SQL FAQ - DDL 执行](/faq/sql-faq.md#ddl-执行)。\n"
        },
        {
          "name": "deploy-monitoring-services.md",
          "type": "blob",
          "size": 8.330078125,
          "content": "---\ntitle: 集群监控部署\naliases: ['/docs-cn/dev/deploy-monitoring-services/','/docs-cn/dev/monitor-a-tidb-cluster/','/docs-cn/dev/how-to/monitor/monitor-a-cluster/']\nsummary: 本文适用于手动部署 TiDB 监控报警系统的用户。假设 TiDB 的拓扑结构如下：Node1 主机 IP 为 192.168.199.113，服务包括 PD1、TiDB、node_export、Prometheus、Grafana；Node2 主机 IP 为 192.168.199.114，服务包括 PD2、node_export；Node3 主机 IP 为 192.168.199.115，服务包括 PD3、node_export；Node4 主机 IP 为 192.168.199.116，服务包括 TiKV1、node_export；Node5 主机 IP 为 192.168.199.117，服务包括 TiKV2、node_export；Node6 主机 IP 为 192.168.199.118，服务包括 TiKV3、node_export。具体部署步骤包括下载二进制包、启动 node_exporter 服务、启动 Prometheus 服务、启动 Grafana 服务、配置 Grafana 数据源和导入 Grafana 面板。可查看 TiDB Server、PD Server 和 TiKV Server 的监控信息。\n---\n\n# TiDB 集群监控部署\n\n本文档适用于希望手动部署 TiDB 监控报警系统的用户。TiUP 部署方式，会同时自动部署监控报警系统，无需手动部署。\n\n## 部署 Prometheus 和 Grafana\n\n假设 TiDB 的拓扑结构如下：\n\n| 节点  | 主机 IP | 服务 |\n| :-- | :-- | :-------------- |\n| Node1 | 192.168.199.113| PD1, TiDB, node_export, Prometheus, Grafana |\n| Node2 | 192.168.199.114| PD2, node_export |\n| Node3 | 192.168.199.115| PD3, node_export |\n| Node4 | 192.168.199.116| TiKV1, node_export |\n| Node5 | 192.168.199.117| TiKV2, node_export |\n| Node6 | 192.168.199.118| TiKV3, node_export |\n\n### 第 1 步：下载二进制包\n\n下载二进制包：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nwget https://github.com/prometheus/prometheus/releases/download/v2.49.1/prometheus-2.49.1.linux-amd64.tar.gz\nwget https://download.pingcap.org/node_exporter-v1.3.1-linux-amd64.tar.gz\nwget https://download.pingcap.org/grafana-7.5.17.linux-amd64.tar.gz\n```\n\n解压二进制包：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntar -xzf prometheus-2.49.1.linux-amd64.tar.gz\ntar -xzf node_exporter-v1.3.1-linux-amd64.tar.gz\ntar -xzf grafana-7.5.17.linux-amd64.tar.gz\n```\n\n### 第 2 步：在 Node1，Node2，Node3，Node4 上启动 `node_exporter`\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ncd node_exporter-v1.3.1-linux-amd64\n```\n\n启动 node_exporter 服务：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\n./node_exporter --web.listen-address=\":9100\" \\\n    --log.level=\"info\" &\n```\n\n### 第 3 步：在 Node1 上启动 Prometheus\n\n编辑 Prometheus 的配置文件：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ncd prometheus-2.49.1.linux-amd64 &&\nvi prometheus.yml\n```\n\n```ini\n...\n\nglobal:\n  scrape_interval:     15s\n  evaluation_interval: 15s\n  # scrape_timeout 设置为全局默认值 (10s)\n  external_labels:\n    cluster: 'test-cluster'\n    monitor: \"prometheus\"\n\nscrape_configs:\n  - job_name: 'overwritten-nodes'\n    honor_labels: true  # 不要覆盖 job 和实例的 label\n    static_configs:\n    - targets:\n      - '192.168.199.113:9100'\n      - '192.168.199.114:9100'\n      - '192.168.199.115:9100'\n      - '192.168.199.116:9100'\n      - '192.168.199.117:9100'\n      - '192.168.199.118:9100'\n\n  - job_name: 'tidb'\n    honor_labels: true  # 不要覆盖 job 和实例的 label\n    static_configs:\n    - targets:\n      - '192.168.199.113:10080'\n\n  - job_name: 'pd'\n    honor_labels: true  # 不要覆盖 job 和实例的 label\n    static_configs:\n    - targets:\n      - '192.168.199.113:2379'\n      - '192.168.199.114:2379'\n      - '192.168.199.115:2379'\n\n  - job_name: 'tikv'\n    honor_labels: true  # 不要覆盖 job 和实例的 label\n    static_configs:\n    - targets:\n      - '192.168.199.116:20180'\n      - '192.168.199.117:20180'\n      - '192.168.199.118:20180'\n\n...\n```\n\n如需开启 TiDB、PD 和 TiKV 等组件的报警规则，请单独下载组件对应的报警规则文件，并在 Prometheus 的配置文件中添加报警规则文件的配置。\n\n- TiDB：[`tidb.rules.yml`](https://github.com/pingcap/tidb/blob/master/pkg/metrics/alertmanager/tidb.rules.yml)\n- PD：[`pd.rules.yml`](https://github.com/tikv/pd/blob/master/metrics/alertmanager/pd.rules.yml)\n- TiKV：[`tikv.rules.yml`](https://github.com/tikv/tikv/blob/master/metrics/alertmanager/tikv.rules.yml)\n- TiFlash：[`tiflash.rules.yml`](https://github.com/pingcap/tiflash/blob/master/metrics/alertmanager/tiflash.rules.yml)\n- TiCDC：[`ticdc.rules.yml`](https://github.com/pingcap/tiflow/blob/master/metrics/alertmanager/ticdc.rules.yml)\n- TiDB Lightning：[`lightning.rules.yml`](https://github.com/pingcap/tidb/blob/master/br/metrics/alertmanager/lightning.rules.yml)\n\n```ini\nrule_files:\n  - 'tidb.rules.yml'\n  - 'pd.rules.yml'\n  - 'tikv.rules.yml'\n  - 'tiflash.rules.yml'\n  - 'ticdc.rules.yml'\n  - 'lightning.rules.yml'\n```\n\n启动 Prometheus 服务：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\n./prometheus \\\n    --config.file=\"./prometheus.yml\" \\\n    --web.listen-address=\":9090\" \\\n    --web.external-url=\"http://192.168.199.113:9090/\" \\\n    --web.enable-admin-api \\\n    --log.level=\"info\" \\\n    --storage.tsdb.path=\"./data.metrics\" \\\n    --storage.tsdb.retention=\"15d\" &\n```\n\n### 第 4 步：在 Node1 上启动 Grafana\n\n编辑 Grafana 的配置文件：\n\n```bash\ncd grafana-7.5.17 &&\nvi conf/grafana.ini\n```\n\n```ini\n...\n\n[paths]\ndata = ./data\nlogs = ./data/log\nplugins = ./data/plugins\n[server]\nhttp_port = 3000\ndomain = 192.168.199.113\n[database]\n[session]\n[analytics]\ncheck_for_updates = true\n[security]\nadmin_user = admin\nadmin_password = admin\n[snapshots]\n[users]\n[auth.anonymous]\n[auth.basic]\n[auth.ldap]\n[smtp]\n[emails]\n[log]\nmode = file\n[log.console]\n[log.file]\nlevel = info\nformat = text\n[log.syslog]\n[event_publisher]\n[dashboards.json]\nenabled = false\npath = ./data/dashboards\n[metrics]\n[grafana_net]\nurl = https://grafana.net\n\n...\n\n```\n\n启动 Grafana 服务：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\n./bin/grafana-server \\\n    --config=\"./conf/grafana.ini\" &\n```\n\n## 配置 Grafana\n\n本小节介绍如何配置 Grafana。\n\n### 第 1 步：添加 Prometheus 数据源\n\n1. 登录 Grafana 界面。\n\n    - 默认地址：`http://localhost:3000`\n    - 默认账户：admin\n    - 默认密码：admin\n\n    > **注意：**\n    >\n    > **Change Password** 步骤可以选择 **Skip**。\n\n2. 点击 Grafana 侧边栏菜单 **Configuration** 中的 **Data Source**。\n\n3. 点击 **Add data source**。\n\n4. 指定数据源的相关信息：\n\n    - 在 **Name** 处，为数据源指定一个名称。\n    - 在 **Type** 处，选择 **Prometheus**。\n    - 在 **URL** 处，指定 Prometheus 的 IP 地址。\n    - 根据需求指定其它字段。\n\n5. 点击 **Add** 保存新的数据源。\n\n### 第 2 步：导入 Grafana 面板\n\n执行以下步骤，为 PD Server、TiKV Server 和 TiDB Server 分别导入 Grafana 面板：\n\n1. 点击侧边栏的 Grafana 图标。\n\n2. 在侧边栏菜单中，依次点击 **Dashboards** > **Import** 打开 **Import Dashboard** 窗口。\n\n3. 点击 **Upload .json File** 上传对应的 JSON 文件（从 [pingcap/tidb](https://github.com/pingcap/tidb/tree/master/pkg/metrics/grafana)、[tikv/tikv](https://github.com/tikv/tikv/tree/master/metrics/grafana) 和 [tikv/pd](https://github.com/tikv/pd/tree/master/metrics/grafana) 下载 TiDB Grafana 配置文件）。\n\n    > **注意：**\n    >\n    > TiKV、PD 和 TiDB 面板对应的 JSON 文件分别为 `tikv_summary.json`，`tikv_details.json`，`tikv_trouble_shooting.json`，`pd.json`，`tidb.json`，`tidb_summary.json`。\n\n4. 点击 **Load**。\n\n5. 选择一个 Prometheus 数据源。\n\n6. 点击 **Import**，Prometheus 面板即导入成功。\n\n## 查看组件 metrics\n\n在顶部菜单中，点击 **New dashboard**，选择要查看的面板。\n\n![view dashboard](/media/view-dashboard.png)\n\n可查看以下集群组件信息：\n\n+ **TiDB Server:**\n    + query 处理时间，可以看到延迟和吞吐\n    + ddl 过程监控\n    + TiKV client 相关的监控\n    + PD client 相关的监控\n\n+ **PD Server:**\n    + 命令执行的总次数\n    + 某个命令执行失败的总次数\n    + 某个命令执行成功的耗时统计\n    + 某个命令执行失败的耗时统计\n    + 某个命令执行完成并返回结果的耗时统计\n\n+ **TiKV Server:**\n    + GC 监控\n    + 执行 KV 命令的总次数\n    + Scheduler 执行命令的耗时统计\n    + Raft propose 命令的总次数\n    + Raft 执行命令的耗时统计\n    + Raft 执行命令失败的总次数\n    + Raft 处理 ready 状态的总次数\n"
        },
        {
          "name": "derive-topn-from-window.md",
          "type": "blob",
          "size": 16.6826171875,
          "content": "---\ntitle: 从窗口函数中推导 TopN 或 Limit\nsummary: 介绍从窗口函数中推导 TopN 或 Limit 的优化规则，以及如何开启该规则。\n---\n\n# 从窗口函数中推导 TopN 或 Limit\n\n[窗口函数](/functions-and-operators/window-functions.md)是一种常见的 SQL 函数。对于 `ROW_NUMBER()` 或者 `RANK()` 等编号相关的窗口函数，一种常见的用法是在进行窗口函数求值之后，对求值的结果进行过滤，例如：\n\n```sql\nSELECT * FROM (SELECT ROW_NUMBER() OVER (ORDER BY a) AS rownumber FROM t) dt WHERE rownumber <= 3\n```\n\n按照正常的 SQL 执行流程，TiDB 需要先对 `t` 表的所有数据进行排序，然后为每一行都求得相应的 `ROW_NUMBER()` 结果，最后再进行 `rownumber <= 3` 的过滤。 \n\n从 v7.0.0 开始，TiDB 支持从窗口函数中推导 TopN 或 Limit 算子。通过该优化规则，TiDB 可以将原始 SQL 等价改写成以下形式：\n\n```sql\nWITH t_topN AS (SELECT a FROM t1 ORDER BY a LIMIT 3) SELECT * FROM (SELECT ROW_NUMBER() OVER (ORDER BY a) AS rownumber FROM t_topN) dt WHERE rownumber <= 3\n```\n\n可以看出，改写后，TiDB 可以从窗口函数与后续的过滤条件中推导出一个 TopN 算子，相比于原始 SQL 中的 Sort 算子（对应 `ORDER BY`），TopN 算子的运行效率远高于 Sort 算子，而且 TiKV 和 TiFlash 均支持 TopN 算子的下推，因此能进一步提升改写后的 SQL 的性能。\n\n从窗口函数中推导 TopN 或 Limit 默认关闭。你可以通过将 session 变量 [tidb_opt_derive_topn](/system-variables.md#tidb_opt_derive_topn-从-v700-版本开始引入) 设置为 `ON` 开启该功能。\n\n开启后，如需关闭，可以进行以下操作之一：\n\n* 设置 session 变量 [tidb_opt_derive_topn](/system-variables.md#tidb_opt_derive_topn-从-v700-版本开始引入) 为 `false`。\n* 可参照[优化规则及表达式下推的黑名单](/blocklist-control-plan.md)中的关闭方法。\n\n## 限制\n\n* 目前仅 `ROW_NUMBER()` 窗口函数支持 SQL 语句改写。\n* 只有当 SQL 语句的过滤条件是针对 `ROW_NUMBER()` 结果而且过滤条件为 `<` 或者 `<=` 时，TiDB 才支持改写 SQL 语句。\n\n## 示例\n\n以下通过一些例子对该优化规则进行说明。\n\n### 不包含 PARTITION BY 的窗口函数\n\n#### 示例 1：不包含 ORDER BY 的窗口函数\n\n```sql\nCREATE TABLE t(id int, value int);\nSET tidb_opt_derive_topn=on;\nEXPLAIN SELECT * FROM (SELECT ROW_NUMBER() OVER () AS rownumber FROM t) dt WHERE rownumber <= 3;\n```\n\n```\n+----------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------+\n| id                               | estRows | task      | access object | operator info                                                         |\n+----------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------+\n| Projection_9                     | 2.40    | root      |               | Column#5                                                              |\n| └─Selection_10                   | 2.40    | root      |               | le(Column#5, 3)                                                       |\n|   └─Window_11                    | 3.00    | root      |               | row_number()->Column#5 over(rows between current row and current row) |\n|     └─Limit_15                   | 3.00    | root      |               | offset:0, count:3                                                     |\n|       └─TableReader_26           | 3.00    | root      |               | data:Limit_25                                                         |\n|         └─Limit_25               | 3.00    | cop[tikv] |               | offset:0, count:3                                                     |\n|           └─TableFullScan_24     | 3.00    | cop[tikv] | table:t       | keep order:false, stats:pseudo                                        |\n+----------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------+\n```\n\n在该查询中，优化器从窗口函数中推导出来了 Limit 并将它下推给了 TiKV。\n\n#### 示例 2：包含 ORDER BY 的窗口函数\n\n```sql\nCREATE TABLE t(id int, value int);\nSET tidb_opt_derive_topn=on;\nEXPLAIN SELECT * FROM (SELECT ROW_NUMBER() OVER (ORDER BY value) AS rownumber FROM t) dt WHERE rownumber <= 3;\n```\n\n```\n+----------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n| id                               | estRows  | task      | access object | operator info                                                                               |\n+----------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n| Projection_10                    | 2.40     | root      |               | Column#5                                                                                    |\n| └─Selection_11                   | 2.40     | root      |               | le(Column#5, 3)                                                                             |\n|   └─Window_12                    | 3.00     | root      |               | row_number()->Column#5 over(order by test.t.value rows between current row and current row) |\n|     └─TopN_13                    | 3.00     | root      |               | test.t.value, offset:0, count:3                                                             |\n|       └─TableReader_25           | 3.00     | root      |               | data:TopN_24                                                                                |\n|         └─TopN_24                | 3.00     | cop[tikv] |               | test.t.value, offset:0, count:3                                                             |\n|           └─TableFullScan_23     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                              |\n+----------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n```\n\n在该查询中，优化器从窗口函数中推导出来了 TopN 并将它下推给了 TiKV。\n\n### 包含 PARTITION BY 的窗口函数\n\n> **注意：**\n>\n> 当窗口函数包含 PARTITION BY 时，该优化规则仅在 partition 列是主键的前缀且主键是聚簇索引的时候才能生效。\n\n#### 示例 3：不包含 ORDER BY 的窗口函数\n\n```sql\nCREATE TABLE t(id1 int, id2 int, value1 int, value2 int, primary key(id1,id2) clustered);\nSET tidb_opt_derive_topn=on;\nEXPLAIN SELECT * FROM (SELECT ROW_NUMBER() OVER (PARTITION BY id1) AS rownumber FROM t) dt WHERE rownumber <= 3;\n```\n\n```\n+------------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------+\n| id                                 | estRows | task      | access object | operator info                                                                                 |\n+------------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------+\n| Projection_10                      | 2.40    | root      |               | Column#6                                                                                      |\n| └─Selection_11                     | 2.40    | root      |               | le(Column#6, 3)                                                                               |\n|   └─Shuffle_26                     | 3.00    | root      |               | execution info: concurrency:2, data sources:[TableReader_24]                                  |\n|     └─Window_12                    | 3.00    | root      |               | row_number()->Column#6 over(partition by test.t.id1 rows between current row and current row) |\n|       └─Sort_25                    | 3.00    | root      |               | test.t.id1                                                                                    |\n|         └─TableReader_24           | 3.00    | root      |               | data:Limit_23                                                                                 |\n|           └─Limit_23               | 3.00    | cop[tikv] |               | partition by test.t.id1, offset:0, count:3                                                    |\n|             └─TableFullScan_22     | 3.00    | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                                |\n+------------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------+\n```\n\n在该查询中，优化器从窗口函数中推导出来了 Limit 并将它下推给了 TiKV。值得一提的是这个 Limit 其实是 partition Limit，也就是说对于每个相同 `id1` 值组成的一组数据上都会进行一次 Limit。\n\n#### 示例 4：包含 ORDER BY 的窗口函数\n\n```sql\nCREATE TABLE t(id1 int, id2 int, value1 int, value2 int, primary key(id1,id2) clustered);\nSET tidb_opt_derive_topn=on;\nEXPLAIN SELECT * FROM (SELECT ROW_NUMBER() OVER (PARTITION BY id1 ORDER BY value1) AS rownumber FROM t) dt WHERE rownumber <= 3;\n```\n\n```\n+------------------------------------+----------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------+\n| id                                 | estRows  | task      | access object | operator info                                                                                                        |\n+------------------------------------+----------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------+\n| Projection_10                      | 2.40     | root      |               | Column#6                                                                                                             |\n| └─Selection_11                     | 2.40     | root      |               | le(Column#6, 3)                                                                                                      |\n|   └─Shuffle_23                     | 3.00     | root      |               | execution info: concurrency:3, data sources:[TableReader_21]                                                         |\n|     └─Window_12                    | 3.00     | root      |               | row_number()->Column#6 over(partition by test.t.id1 order by test.t.value1 rows between current row and current row) |\n|       └─Sort_22                    | 3.00     | root      |               | test.t.id1, test.t.value1                                                                                            |\n|         └─TableReader_21           | 3.00     | root      |               | data:TopN_19                                                                                                         |\n|           └─TopN_19                | 3.00     | cop[tikv] |               | partition by test.t.id1 order by test.t.value1, offset:0, count:3                                                    |\n|             └─TableFullScan_18     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                                                       |\n+------------------------------------+----------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------+\n```\n\n在该查询中，优化器从窗口函数中推导出来了 TopN 并将它下推给了 TiKV。需要注意的是，这个 TopN 其实是 partition 的 TopN, 也就是说对于每个相同 `id1` 值组成的一组数据上都会进行一次 TopN。\n\n#### 示例 5：PARTITION BY 列不是主键的前缀\n\n```sql\nCREATE TABLE t(id1 int, id2 int, value1 int, value2 int, primary key(id1,id2) clustered);\nSET tidb_opt_derive_topn=on;\nEXPLAIN SELECT * FROM (SELECT ROW_NUMBER() OVER (PARTITION BY value1) AS rownumber FROM t) dt WHERE rownumber <= 3;\n```\n\n```\n+----------------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------+\n| id                               | estRows  | task      | access object | operator info                                                                                    |\n+----------------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------+\n| Projection_9                     | 8000.00  | root      |               | Column#6                                                                                         |\n| └─Selection_10                   | 8000.00  | root      |               | le(Column#6, 3)                                                                                  |\n|   └─Shuffle_15                   | 10000.00 | root      |               | execution info: concurrency:5, data sources:[TableReader_13]                                     |\n|     └─Window_11                  | 10000.00 | root      |               | row_number()->Column#6 over(partition by test.t.value1 rows between current row and current row) |\n|       └─Sort_14                  | 10000.00 | root      |               | test.t.value1                                                                                    |\n|         └─TableReader_13         | 10000.00 | root      |               | data:TableFullScan_12                                                                            |\n|           └─TableFullScan_12     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                                   |\n+----------------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------+\n```\n\n在该查询中，因为 partition 的列不是主键的前缀，所以 SQL 没有被改写。\n\n#### 示例 6：PARTITION BY 列是主键的前缀，但主键不是聚簇索引\n\n```sql\nCREATE TABLE t(id1 int, id2 int, value1 int, value2 int, primary key(id1,id2) nonclustered);\nSET tidb_opt_derive_topn=on;\nEXPLAIN SELECT * FROM (SELECT ROW_NUMBER() OVER (PARTITION BY id1) AS rownumber FROM t use index()) dt WHERE rownumber <= 3;\n```\n\n```\n+----------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------------------------------+\n| id                               | estRows  | task      | access object | operator info                                                                                 |\n+----------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------------------------------+\n| Projection_9                     | 8000.00  | root      |               | Column#7                                                                                      |\n| └─Selection_10                   | 8000.00  | root      |               | le(Column#7, 3)                                                                               |\n|   └─Shuffle_15                   | 10000.00 | root      |               | execution info: concurrency:5, data sources:[TableReader_13]                                  |\n|     └─Window_11                  | 10000.00 | root      |               | row_number()->Column#7 over(partition by test.t.id1 rows between current row and current row) |\n|       └─Sort_14                  | 10000.00 | root      |               | test.t.id1                                                                                    |\n|         └─TableReader_13         | 10000.00 | root      |               | data:TableFullScan_12                                                                         |\n|           └─TableFullScan_12     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                                |\n+----------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------------------------------+\n```\n\n在该查询中，即使 PARTITION 的列是主键的前缀，但是因为主键不是聚簇索引，所以 SQL 没被改写。\n"
        },
        {
          "name": "develop",
          "type": "tree",
          "content": null
        },
        {
          "name": "dm",
          "type": "tree",
          "content": null
        },
        {
          "name": "download-ecosystem-tools.md",
          "type": "blob",
          "size": 3.271484375,
          "content": "---\ntitle: TiDB 工具下载\naliases: ['/docs-cn/dev/download-ecosystem-tools/','/docs-cn/dev/reference/tools/download/']\nsummary: 本文介绍如何下载 TiDB 工具包。TiDB 工具包包含常用工具如 Dumpling、TiDB Lightning、BR 等。如果部署环境能访问互联网，可直接通过 TiUP 命令一键部署所需的 TiDB 工具。操作系统需为 Linux，架构为 amd64 或 arm64。下载步骤包括访问 TiDB 社区版页面，找到 TiDB-community-toolkit 软件包并点击立即下载。注意，点击立即下载后默认下载当前 TiDB 的最新发布版本。根据要使用的工具选择安装对应的离线包。\n---\n\n# TiDB 工具下载\n\n本文介绍如何下载 TiDB 工具包。关于 TiDB 工具包的内容，请查看 [TiDB 离线包](/binary-package.md)。\n\n## TiDB 工具包下载\n\nTiDB 工具包中包含了一些常用的 TiDB 工具，例如数据导出工具 Dumpling、数据导入工具 TiDB Lightning、备份恢复工具 BR。\n\n> **建议：**\n>\n> 如果你的部署环境能访问互联网，无需单独下载 TiDB 工具包，可以直接通过使用 [TiUP 命令一键部署](/tiup/tiup-component-management.md)所需的 TiDB 工具。\n\n### 环境要求\n\n- 操作系统：Linux\n- 架构：amd64 或 arm64\n\n### 下载步骤\n\n1. 访问 [TiDB 社区版](https://pingcap.com/zh/product-community/)页面。\n2. 找到 **TiDB-community-toolkit 软件包**，点击**立即下载**。\n\n> **注意：**\n>\n> - 点击**立即下载**后，默认下载当前 TiDB 的最新发布版本。如需下载其它版本，请在 [TiDB 社区版](https://pingcap.com/zh/product-community/)页面底部查看其它版本下载信息。\n> - 如需在 Kubernetes 上部署运维 TiDB，无需下载 TiDB-community-toolkit 软件包，请参考[离线安装 TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/deploy-tidb-operator#离线安装-tidb-operator)。\n> - 如需使用 [PD Control](/pd-control.md) 工具 `pd-ctl`，请下载 **TiDB-community-server 软件包**。\n\n### TiDB 工具包说明\n\n在 TiDB 工具包中，你可以依据要使用的工具，选择安装对应的离线包。\n\n| 工具  | 离线包名称  |\n|:------|:----------|\n| [TiUP](/tiup/tiup-overview.md)  | `tiup-linux-{arch}.tar.gz` <br/>`tiup-{tiup-version}-linux-{arch}.tar.gz` <br/>`dm-{tiup-version}-linux-{arch}.tar.gz` <br/> `server-{version}-linux-{arch}.tar.gz` |\n| [Dumpling](/dumpling-overview.md)  | `dumpling-{version}-linux-{arch}.tar.gz`  |\n| [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md)  | `tidb-lightning-ctl` <br/>`tidb-lightning-{version}-linux-{arch}.tar.gz`  |\n| [TiDB DM (Data Migration)](/dm/dm-overview.md)  | `dm-worker-{version}-linux-{arch}.tar.gz` <br/>`dm-master-{version}-linux-{arch}.tar.gz` <br/>`dmctl-{version}-linux-{arch}.tar.gz`  |\n| [TiCDC](/ticdc/ticdc-overview.md)  | `cdc-{version}-linux-{arch}.tar.gz`  |\n| [Backup & Restore (BR)](/br/backup-and-restore-overview.md)  | `br-{version}-linux-{arch}.tar.gz`  |\n| [sync-diff-inspector](/sync-diff-inspector/sync-diff-inspector-overview.md)  | `sync_diff_inspector`  |\n| [PD Recover](/pd-recover.md)  | `pd-recover-{version}-linux-{arch}.tar.gz` |\n\n> **注意：**\n>\n> 以上离线包名称中，`{version}` 取决于离线包中工具的版本号，`{arch}` 取决于离线包对应的架构（amd64 或 arm64）。\n"
        },
        {
          "name": "dr-backup-restore.md",
          "type": "blob",
          "size": 2.931640625,
          "content": "---\ntitle: 基于备份与恢复的容灾方案\nsummary: 了解如何基于 TiDB 的备份与恢复功能实现容灾。\n---\n\n# 基于备份与恢复的容灾方案\n\nTiDB 集群自身的多副本特性可以让其容忍单个机房或地理区域的故障，并继续向外提供服务。在更大范围的自然灾害、软件漏洞、硬件故障、病毒攻击或常见的人为误操作的情况下，TiDB 的备份与恢复功能可以将数据备份到独立的灾备存储设备中，以保护用户数据不受损坏。与其他方案相比，备份与恢复功能具有灵活性、可靠性、可恢复性和成本效益等优势：\n\n- 灵活性：可以在不同的时间点进行备份，并且可以根据需要调整备份频率。这使得备份与恢复功能更加灵活，能够更好地适应不同的业务场景。\n- 可靠性：备份与恢复功能通常会将备份数据保存在独立的存储设备上，使得数据安全得到进一步升级。\n- 可恢复性：任何意外情况导致的原始数据丢失或损坏，都可以通过恢复备份数据的方式来恢复。这使得备份恢复功能具有很高的可恢复性，能够保证数据库的正常使用。\n- 成本效益：备份恢复功能通常具有较低的成本，可以在不增加太多费用的情况下实现对数据库的安全保护。\n\n总的来说，备份恢复功能是数据安全的最后一道防线，可以在不增加太多成本的情况下，提高数据库的安全性和可靠性。它能够在各种意外情况发生时保护数据，使你能够安心地使用数据库，而不必担心数据丢失或损坏的风险。\n\n## 进行备份恢复\n\n![BR log backup and PITR architecture](/media/dr/dr-backup-and-restore.png)\n\n按照上述架构，你可以将数据备份到其他区域的灾备存储设备中，并在需要时将数据恢复回来。这样，系统就能够容忍单个区域的故障，并且 Recovery Point Objective (RPO) 可以达到 5 分钟，Recovery Time Objective (RTO) 通常在几十分钟到数小时之间。但是，如果数据库尺寸较大，RTO 时间可能会更长。\n\n此外，TiDB 还提供了基于块存储快照技术的备份和恢复特性，可以将集群的恢复时间缩短到小时级别甚至 1 小时以内。TiDB 也在不断完善和优化基于存储快照技术的备份和恢复能力，以提供更好的服务。\n\nTiDB 提供了丰富的文档，帮助你了解如何在容灾场景下使用备份和恢复功能。其中，\n\n- [TiDB 备份和恢复功能使用概述](/br/br-use-overview.md)提供了备份与恢复的使用指引，包含备份策略及备份数据存放组织等。\n- [TiDB 备份恢复常见问题](/faq/backup-and-restore-faq.md)列出了使用备份与恢复功能时可能遇到的问题，以及解决方案。\n- [TiDB 备份与恢复功能架构](/br/backup-and-restore-design.md)描述了 TiDB 备份与恢复功能的设计架构，包括备份和恢复的流程，以及备份文件设计。\n"
        },
        {
          "name": "dr-multi-replica.md",
          "type": "blob",
          "size": 8.796875,
          "content": "---\ntitle: 基于多副本的单集群容灾方案\nsummary: 了解 TiDB 提供的基于多副本的单集群容灾方案。\n---\n\n# 基于多副本的单集群容灾方案\n\n本文介绍了基于多副本的单集群容灾方案，文档内容组织如下：\n\n- 方案简介\n- 搭建集群\n- 配置副本\n- 监控集群\n- 容灾切换\n\n## 简介\n\n对于重要的生产系统，很多用户需要能够实现区域级别的容灾，并且做到 RPO = 0 和分钟级别的 RTO。TiDB 作为基于 Raft 协议的分布式数据库，其自带的多副本特性可以用于支持区域级别的容灾目标，并同时确保数据的一致性和高可用性。而同区域可用区 (Available Zone, AZ) 之间的网络延迟相对较小，可以把业务流量同时派发到同区域两个 AZ，并通过控制 Region Leader 和 PD Leader 分布实现同区域 AZ 共同负载业务流量。\n\n## 搭建集群和配置副本\n\n在这一部分当中，会以一个 5 副本的集群为例，演示如何使用 TiUP 创建一个跨 3 个区域的集群，以及如何控制数据和 PD 的分布位置，从而达到容灾的目的。\n\n在下面的示例中，TiDB 集群的区域 1 作为 primary region，区域 2 作为 secondary region，而区域 3 则作为投票使用的第三个区域，一共包含 5 个副本。同理，PD 集群也包含了 5 个副本，其功能和 TiDB 集群的功能基本一致。\n\n1. 创建类似于以下的集群拓扑文件：\n\n    ```toml\n    global:\n      user: \"root\"\n      ssh_port: 22\n      deploy_dir: \"/data/tidb_cluster/tidb-deploy\"\n      data_dir: \"/data/tidb_cluster/tidb-data\"\n\n    server_configs:\n      tikv:\n        server.grpc-compression-type: gzip\n      pd:\n        replication.location-labels:  [\"Region\",\"AZ\"] # PD 会根据 TiKV 节点的 Region 和 AZ 配置来进行副本的调度。\n\n    pd_servers:\n      - host: tidb-dr-test1\n        name: \"pd-1\"\n      - host: tidb-dr-test2\n        name: \"pd-2\"\n      - host: tidb-dr-test3\n        name: \"pd-3\"\n      - host: tidb-dr-test4\n        name: \"pd-4\"\n      - host: tidb-dr-test5\n        name: \"pd-5\"\n\n    tidb_servers:\n      - host: tidb-dr-test1\n      - host: tidb-dr-test3\n\n    tikv_servers:  # 在 TiKV 节点中通过 labels 选项来对每个 TiKV 节点所在的 Region 和 AZ 进行标记\n      - host: tidb-dr-test1\n        config:\n          server.labels: { Region: \"Region1\", AZ: \"AZ1\" }\n      - host: tidb-dr-test2\n        config:\n          server.labels: { Region: \"Region1\", AZ: \"AZ2\" }\n      - host: tidb-dr-test3\n        config:\n          server.labels: { Region: \"Region2\", AZ: \"AZ3\" }\n      - host: tidb-dr-test4\n        config:\n          server.labels: { Region: \"Region2\", AZ: \"AZ4\" }\n      - host: tidb-dr-test5\n        config:\n          server.labels: { Region: \"Region3\", AZ: \"AZ5\" }\n\n          raftstore.raft-min-election-timeout-ticks: 50\n          raftstore.raft-max-election-timeout-ticks: 60\n\n    monitoring_servers:\n      - host: tidb-dr-test2\n\n    grafana_servers:\n      - host: tidb-dr-test2\n\n    alertmanager_servers:\n      - host: tidb-dr-test2\n    ```\n\n    在上面的配置中，使用了以下一系列配置来针对跨区域容灾场景进行优化：\n\n    - 使用 `server.grpc-compression-type`：gzip 启用 TiKV 之间的消息压缩，从而降低网络流量。\n    - 使用 `raftstore.raft-min-election-timeout-ticks` 和 `raftstore.raft-max-election-timeout-ticks` 延长区域 3 参加选举的时间，从而避免该区域中的副本被选举为主节点。\n\n2. 使用上面的配置文件创建集群：\n\n    ```shell\n    tiup cluster deploy drtest v6.4.0 ./topo.yaml\n    tiup cluster start drtest --init\n    tiup cluster display drtest\n    ```\n\n    对集群的副本数和 Leader 限制进行配置：\n\n    ```shell\n    tiup ctl:v6.4.0 pd config set max-replicas 5\n    tiup ctl:v6.4.0 pd config set label-property reject-leader Region Region3\n\n    # 下面的步骤用于向集群中添加一些测试数据，可选\n    tiup bench tpcc  prepare -H 127.0.0.1 -P 4000 -D tpcc --warehouses 1\n    ```\n\n    指定 PD leader 的优先级：\n\n    ```shell\n    tiup ctl:v6.4.0 pd member leader_priority  pd-1 4\n    tiup ctl:v6.4.0 pd member leader_priority  pd-2 3\n    tiup ctl:v6.4.0 pd member leader_priority  pd-3 2\n    tiup ctl:v6.4.0 pd member leader_priority  pd-4 1\n    tiup ctl:v6.4.0 pd member leader_priority  pd-5 0\n    ```\n\n    > **注意：**\n    >\n    > 在可用的 PD 节点中，优先级数值最大的节点会直接当选 leader。\n\n3. 创建 placement rule，并将测试表的主副本固定在区域 1：\n\n    ```sql\n    -- 创建两个 placement rules，第一个是区域 1 作为主区域，在系统正常时使用，第二个是区域 2 作为备区域。\n    -- 作为主区域，当区域 1 出现问题时，区域 2 会作为主区域。\n    MySQL [(none)]> CREATE PLACEMENT POLICY primary_rule_for_region1 PRIMARY_REGION=\"Region1\" REGIONS=\"Region1, Region2,Region3\";\n    MySQL [(none)]> CREATE PLACEMENT POLICY secondary_rule_for_region2 PRIMARY_REGION=\"Region2\" REGIONS=\"Region1,Region2,Region3\";\n\n    -- 将刚刚创建的规则 primary_rule_for_region1 应用到对应的用户表上。\n    ALTER TABLE tpcc.warehouse PLACEMENT POLICY=primary_rule_for_region1;\n    ALTER TABLE tpcc.district PLACEMENT POLICY=primary_rule_for_region1;\n\n    -- 说明：请根据需要修改上面的数据库名称、表名和 placement rule 的名称。\n\n    -- 使用类似下面的查询，用户可以查看每个区域包含的 leader 数量，以确认 leader 迁移是否完成。\n    SELECT STORE_ID, address, leader_count, label FROM TIKV_STORE_STATUS ORDER BY store_id;\n    ```\n\n    下面的语句可以产生一个 SQL 脚本，把所有非系统 schema 中的表的 leader 都设置到特定的区域上：\n\n    ```sql\n    SET @region_name=primary_rule_for_region1;\n    SELECT concat('ALTER TABLE ', table_schema, '.', table_name, ' PLACEMENT POLICY=', @region_name, ';') FROM information_schema.tables WHERE table_schema NOT IN ('METRICS_SCHEMA', 'PERFORMANCE_SCHEMA', 'INFORMATION_SCHEMA','mysql');\n    ```\n\n## 监控集群\n\n对于部署的集群，你可以通过访问集群中的 Grafana 地址或者 TiDB Dashboard 组件来对集群中的各个 TiKV、TiDB 和 PD 组件的各种性能指标进行监控。根据组件的状态，确定是否进行容灾切换。详细信息，请参考如下文档：\n\n- [TiDB 重要监控指标详解](/grafana-tidb-dashboard.md)\n- [TiKV 监控指标详解](/grafana-tikv-dashboard.md)\n- [PD 重要监控指标详解](/grafana-pd-dashboard.md)\n- [TiDB Dashboard 监控页面](/dashboard/dashboard-monitoring.md)\n\n## 容灾切换\n\n本部分介绍容灾切换，包括计划内切换和计划外切换。\n\n### 计划内切换\n\n指根据维护需要进行的主备区域切换，可用于验证容灾系统是否可以正常工作。本部分介绍如何在计划内切换主备区域。\n\n1. 执行如下命令，将所有用户表和 PD Leader 都切换到区域 2：\n\n    ```sql\n    -- 将之前创建的规则 secondary_rule_for_region2 应用到对应的用户表上。\n    ALTER TABLE tpcc.warehouse PLACEMENT POLICY=secondary_rule_for_region2;\n    ALTER TABLE tpcc.district PLACEMENT POLICY=secondary_rule_for_region2;\n    ```\n\n    说明：请根据需要修改上面的数据库名称、表名和 placement rule 的名称。\n\n    执行如下命令，调低区域 1 的 PD 节点的优先级，并调高区域 2 的 PD 节点的优先级。\n\n    ``` shell\n    tiup ctl:v6.4.0 pd member leader_priority pd-1 2\n    tiup ctl:v6.4.0 pd member leader_priority pd-2 1\n    tiup ctl:v6.4.0 pd member leader_priority pd-3 4\n    tiup ctl:v6.4.0 pd member leader_priority pd-4 3\n    ```\n\n2. 观察 Grafana 中 PD 和 TiKV 部分中的内容，确保 PD 的 Leader 和用户表的 Leader 已经迁移到对应的区域。另外，切换回原有区域的步骤与上面的步骤基本相同，本文不做过多的描述。\n\n### 计划外切换\n\n计划外切换，指灾难发生时的主备区域切换，或者为了验证容灾系统的有效性，而模拟灾难发生时的主备区域切换。\n\n1. 执行类似下面的命令终止区域 1 上所有的 TiKV、TiDB 和 PD 节点:\n\n    ``` shell\n    tiup cluster stop drtest -N tidb-dr-test1:20160,tidb-dr-test2:20160,tidb-dr-test1:2379,tidb-dr-test2:2379\n    ```\n\n2. 运行类似于下面的命令切换用户表的 leader 到区域 2:\n\n    ```sql\n    -- 将之前创建的规则 secondary_rule_for_region2 应用到对应的用户表上。\n    ALTER TABLE tpcc.warehouse PLACEMENT POLICY=secondary_rule_for_region2;\n    ALTER TABLE tpcc.district PLACEMENT POLICY=secondary_rule_for_region2;\n\n    ---可以使用类似下面的查询查看每个区域包含的 leader 数量，以确认 leader 迁移是否完成。\n    SELECT STORE_ID, address, leader_count, label FROM TIKV_STORE_STATUS ORDER BY store_id;\n    ```\n\n    当区域 1 恢复正常之后，可以使用类似于上面的命令将用户表的 leader 重新切换到区域 1。\n"
        },
        {
          "name": "dr-secondary-cluster.md",
          "type": "blob",
          "size": 19.162109375,
          "content": "---\ntitle: 基于主备集群的容灾方案\nsummary: 了解如何使用 TiCDC 构建主备集群进行容灾。\n---\n\n# 基于主备集群的容灾方案\n\n使用主、备数据库进行容灾是一种常用的容灾方式。在这种方案下，系统有一个主用集群和一个备用集群。主集群用于处理用户的请求，备集群负责备份主集群的数据。当主集群发生故障时，系统可以切换到备集群，使用备份的数据继续提供服务。这样，系统就可以在发生故障的情况下继续正常运行，避免因为故障而导致的服务中断。\n\n主备集群容灾方案具有如下优势：\n\n- 高可用性：主、备集群的架构可以有效提高系统的可用性，使得系统在遇到故障时能够快速恢复。\n- 快速切换：在主集群发生故障的情况下，系统可以快速切换到备用集群，继续提供服务。\n- 数据一致性：备用集群会近实时备份主集群的数据，因此，在故障发生后切换到备集群时，数据基本是最新的。\n\n本文包含以下主要内容：\n\n- 构建主备集群\n- 从主集群复制数据至备集群\n- 监控集群\n- 容灾切换\n\n同时，本文还介绍了如何在备用集群上进行业务查询，以及如何在主备集群间进行双向同步。\n\n## 基于 TiCDC 构建 TiDB 主备集群\n\n### 架构概览\n\n![TiCDC secondary cluster architecture](/media/dr/dr-ticdc-secondary-cluster.png)\n\n上述架构包含两个 TiDB 集群：Primary Cluster 和 Secondary Cluster。\n\n- Primary Cluster：主用集群，运行在区域 1 (Region 1)，三副本，用于处理读写业务。\n- Secondary Cluster：备用集群，运行在区域 2 (Region 2)，通过 TiCDC 从 Primary Cluster 同步数据。\n\n这种容灾架构简洁易用，可以容忍区域级别的故障，既可以保证主用集群的写入性能不会下降，还可以在备用集群处理一些延迟不敏感的只读业务。该方案的 Recovery Point Objective (RPO) 在秒级别，Recovery Time Objective (RTO) 可以达到分钟级别甚至更低。这个方案适用于重要的生产系统。\n\n> **注意：**\n>\n> 不要使用多个 TiCDC Changefeed 同步数据至备用集群，也不要在备用集群基础上运行另一个备用集群，否则，备用集群的数据事务完整性无法保证。\n\n### 搭建主备集群\n\n本文将 TiDB 主集群和备用集群分别部署在两个不同的区域（区域 1 和区域 2）。由于主备集群之间存在一定的网络延迟，TiCDC 与 TiDB 备用集群应部署在一起，以实现最好的数据同步性能。在本教程示例中，每台服务器部署一个组件节点，具体的部署拓扑如下：\n\n|区域 | 主机 | 集群 | 组件 |\n| --- | --- | --- | --- |\n| 区域 1 | 10.0.1.1/10.0.1.2/10.0.1.3 | Primary | PD |\n| 区域 1 | 10.0.1.4/10.0.1.5 | Primary| TiDB |\n| 区域 1 | 10.0.1.6/10.0.1.7/10.0.1.8 | Primary | TiKV |\n| 区域 1 | 10.0.1.9 | Primary | Monitor、Grafana 或 AlterManager |\n| 区域 2 | 10.1.1.9/10.1.1.10 | Primary | TiCDC |\n| 区域 2 | 10.1.1.1/10.1.1.2/10.1.1.3 | Secondary | PD |\n| 区域 2 | 10.1.1.4/10.1.1.5 | Secondary | TiDB |\n| 区域 2 | 10.1.1.6/10.1.1.7/10.1.1.8 | Secondary | TiKV |\n| 区域 2 | 10.0.1.11 | Secondary | Monitor、Grafana 或 AlterManager |\n\n关于服务器配置信息，可以参考如下文档：\n\n- [TiDB 软件和硬件环境需求](/hardware-and-software-requirements.md)\n- [TiCDC 软件和硬件环境推荐配置](/ticdc/deploy-ticdc.md#软件和硬件环境推荐配置)\n\n部署 TiDB 主集群和备用集群的详细过程，可以参考[部署 TiDB 集群](/production-deployment-using-tiup.md)。\n\n部署 TiCDC 组件需要注意的是，Secondary Cluster 和 TiCDC 需要在一起部署和管理，并且它们之间的网络需要能够连通。\n\n- 如果需要在已有的 Primary Cluster 上部署 TiCDC，请参考[部署 TiCDC 组件](/ticdc/deploy-ticdc.md#使用-tiup-在原有-tidb-集群上新增或扩容-ticdc-组件)。\n- 如果部署全新的 Primary Cluster 和 TiCDC 组件，则可以使用以下 TiUP 部署模版，并按照需要修改配置参数：\n\n    ```yaml\n    global:\n    user: \"tidb\"\n    ssh_port: 22\n    deploy_dir: \"/tidb-deploy\"\n    data_dir: \"/tidb-data\"\n    server_configs: {}\n    pd_servers:\n    - host: 10.0.1.1\n    - host: 10.0.1.2\n    - host: 10.0.1.3\n    tidb_servers:\n    - host: 10.0.1.4\n    - host: 10.0.1.5\n    tikv_servers:\n    - host: 10.0.1.6\n    - host: 10.0.1.7\n    - host: 10.0.1.8\n    monitoring_servers:\n    - host: 10.0.1.9\n    grafana_servers:\n    - host: 10.0.1.9\n    alertmanager_servers:\n    - host: 10.0.1.9\n    cdc_servers:\n    - host: 10.1.1.9\n        gc-ttl: 86400\n        data_dir: \"/cdc-data\"\n        ticdc_cluster_id: \"DR_TiCDC\"\n    - host: 10.1.1.10\n        gc-ttl: 86400\n        data_dir: \"/cdc-data\"\n        ticdc_cluster_id: \"DR_TiCDC\"\n    ```\n\n### 从主集群复制数据到备用集群\n\n搭建好 TiDB 主集群和备用集群之后，需要先将主集群的数据迁移到备用集群，然后创建同步任务从主集群复制实时变更数据到备用集群。\n\n#### 选择外部存储\n\n数据迁移和实时变更数据复制都需要使用外部存储。推荐使用 Amazon S3 作为存储系统。如果 TiDB 集群部署在自建机房中，则推荐以下方式：\n\n* 搭建 [MinIO](https://docs.min.io/docs/minio-quickstart-guide.html) 作为备份存储系统，使用 S3 协议将数据备份到 MinIO 中。\n* 挂载 NFS 盘（如 NAS）到 br、TiKV 和 TiCDC 实例节点，使用 POSIX 文件系统接口将备份数据写入对应的 NFS 目录中。\n\n下面以 MinIO 为示例，仅供参考。注意需要在区域 1 或者区域 2 中准备独立的服务器部署 MinIO。\n\n```shell\nwget https://dl.min.io/server/minio/release/linux-amd64/minio\nchmod +x minio\n# 配置访问 MinIO 的 access-key 和 access-secret-id\nexport HOST_IP='10.0.1.10' # 替换为实际部署 MinIO 的机器 IP 地址\nexport MINIO_ROOT_USER='minio'\nexport MINIO_ROOT_PASSWORD='miniostorage'\n# 创建 TiCDC redo log 和 backup 数据保存的目录，其中 redo、backup 为 bucket 名字\nmkdir -p data/redo\nmkdir -p data/backup\n# 启动 MinIO，暴露端口在 6060\nnohup ./minio server ./data --address :6060 &\n```\n\n上述命令启动了一个单节点的 MinIO server 模拟 S3 服务，相关参数为：\n\n* `endpoint`：`http://10.0.1.10:6060/`\n* `access-key`：`minio`\n* `secret-access-key`：`miniostorage`\n* `bucket`：`redo`/`backup`\n\n其访问链接为：\n\n```\ns3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://10.0.1.10:6060&force-path-style=true\n```\n\n#### 数据迁移\n\n在 TiDB 主备集群之间使用 [TiDB 备份恢复功能 BR](/br/backup-and-restore-overview.md) 进行数据迁移。\n\n1. 关闭垃圾回收机制 (GC)。为了保证增量迁移过程中新写入的数据不丢失，在开始备份之前，需要关闭上游集群的 GC 机制，以确保系统不再清理历史数据。\n\n    执行如下命令关闭 GC：\n\n    ```sql\n    SET GLOBAL tidb_gc_enable=FALSE;\n    ```\n\n    查询 `tidb_gc_enable` 的取值，以确认 GC 是否已关闭：\n\n    ```sql\n    SELECT @@global.tidb_gc_enable;\n    ```\n\n    输出结果为 `0` 表明 GC 已关闭：\n\n    ```\n    +-------------------------+\n    | @@global.tidb_gc_enable |\n    +-------------------------+\n    |                       0 |\n    +-------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n    > **注意：**\n    >\n    > 在生产集群中，关闭 GC 机制和备份操作会一定程度上降低集群的读性能。建议在业务低峰期进行备份，并设置合适的 `RATE_LIMIT` 限制备份操作对线上业务的影响。\n\n2. 备份数据。在 TiDB 主集群中执行 `BACKUP` 语句备份数据：\n\n    ```sql\n    BACKUP DATABASE * TO '`s3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://10.0.1.10:6060&force-path-style=true`';\n    ```\n\n    ```\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    | Destination          | Size     | BackupTS           | Queue Time          | Execution Time      |\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    | s3://backup          | 10315858 | 431434047157698561 | 2022-02-25 19:57:59 | 2022-02-25 19:57:59 |\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    1 row in set (2.11 sec)\n    ```\n\n    备份语句提交成功后，TiDB 会返回关于备份数据的元信息，这里需要重点关注 `BackupTS`，它意味着该时间点之前的数据会被备份。本文后续步骤中，使用 `BackupTS` 作为**实时变更数据复制的起始时间点**。\n\n3. 恢复数据。在 TiDB 备用集群中执行 `RESTORE` 语句恢复数据：\n\n    ```sql\n    RESTORE DATABASE * FROM '`s3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://10.0.1.10:6060&force-path-style=true`';\n    ```\n\n    ```\n    +----------------------+----------+----------+---------------------+---------------------+\n    | Destination          | Size     | BackupTS | Queue Time          | Execution Time      |\n    +----------------------+----------+----------+---------------------+---------------------+\n    | s3://backup          | 10315858 | 0        | 2022-02-25 20:03:59 | 2022-02-25 20:03:59 |\n    +----------------------+----------+----------+---------------------+---------------------+\n    1 row in set (41.85 sec)\n    ```\n\n#### 复制实时变更数据\n\n完成以上数据迁移的操作步骤后，从备份的 **BackupTS** 时间点开始同步主集群增量变更数据到备用集群。\n\n1. 创建 TiCDC 同步任务 Changefeed。\n\n    创建 Changefeed 配置文件并保存为 `changefeed.toml`。\n\n    ```toml\n    [consistent]\n    # eventual consistency：使用 redo log，提供上游灾难情况下的最终一致性。\n    level = \"eventual\"\n    # 单个 redo log 文件大小，单位 MiB，默认值 64，建议该值不超过 128。\n    max-log-size = 64\n    # 刷新或上传 redo log 至 S3 的间隔，单位毫秒，默认 1000，建议范围 500-2000。\n    flush-interval = 2000\n    # 存储 redo log 的地址\n    storage = \"s3://redo?access-key=minio&secret-access-key=miniostorage&endpoint=http://10.0.1.10:6060&force-path-style=true\"\n    ```\n\n    在主用集群中，执行以下命令创建从主用集群到备用集群的同步链路：\n\n    ```shell\n    tiup cdc cli changefeed create --server=http://10.1.1.9:8300 --sink-uri=\"mysql://{username}:{password}@10.1.1.4:4000\" --changefeed-id=\"dr-primary-to-secondary\" --start-ts=\"431434047157698561\"\n    ```\n\n    更多关于 Changefeed 的配置，请参考 [TiCDC Changefeed 配置参数](/ticdc/ticdc-changefeed-config.md)。\n\n2. 查询 Changefeed 是否正常运行。使用 `changefeed query` 命令可以查询特定同步任务（对应某个同步任务的信息和状态），指定 `--simple` 或 `-s` 参数会简化输出，提供基本的同步状态和 checkpoint 信息。不指定该参数会输出详细的任务配置、同步状态和同步表信息。\n\n    ```shell\n    tiup cdc cli changefeed query -s --server=http://10.1.1.9:8300 --changefeed-id=\"dr-primary-to-secondary\"\n    ```\n\n    ```shell\n    {\n    \"state\": \"normal\",\n    \"tso\": 431434047157998561,  # changefeed 已经同步到的时间点\n    \"checkpoint\": \"2020-08-27 10:12:19.579\", # TSO 对应的物理时间点\n    \"error\": null\n    }\n    ```\n\n3. 重新开启 GC。\n\n    TiCDC 可以保证未同步的历史数据不会被回收。因此，创建完从主集群到备用集群的 Changefeed 之后，就可以执行如下命令恢复集群的垃圾回收功能。\n\n   执行如下命令打开 GC：\n\n    ```sql\n    SET GLOBAL tidb_gc_enable=TRUE;\n    ```\n\n    查询 `tidb_gc_enable` 的取值，判断 GC 是否已开启：\n\n    ```sql\n    SELECT @@global.tidb_gc_enable;\n    ```\n\n    结果输出 `1` 表明 GC 已开启：\n\n    ```\n    +-------------------------+\n    | @@global.tidb_gc_enable |\n    +-------------------------+\n    |                       1 |\n    +-------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n### 主备集群状态监控\n\nTiDB 目前还没有提供 DR Dashboard，你可以通过以下 Dashboard 了解 TiDB 主备集群的状态，从而决定是否需要进行容灾切换：\n\n- [TiDB 集群运行状态 Dashboard](/grafana-overview-dashboard.md)\n- [Changefeed 运行状态](/ticdc/monitor-ticdc.md#changefeed-面板)\n\n### 容灾切换\n\n本部分介绍容灾演练，遇到真正灾难时的主备切换，以及重建灾备集群的步骤。\n\n#### 计划中的主备切换\n\n定期对非常重要的业务系统进行容灾演练，检验系统的可靠性是非常有必要的。下面是容灾演练推荐的操作步骤，因为没有考虑演练中业务写入是否为模拟、业务访问数据库是否使用 proxy 服务等，与实际的演练场景会有出入，请根据你的实际情况进行修改。\n\n1. 停止主集群上的业务写入。\n2. 业务写入完全停止后，查询 TiDB 集群当前最新的 TSO (`Position`)：\n\n    ```sql\n    BEGIN; SELECT TIDB_CURRENT_TSO(); ROLLBACK;\n    ```\n\n    ```sql\n    Query OK, 0 rows affected (0.00 sec)\n\n    +--------------------+\n    | TIDB_CURRENT_TSO() |\n    +--------------------+\n    | 452654700157468673 |\n    +--------------------+\n    1 row in set (0.00 sec)\n\n    Query OK, 0 rows affected (0.00 sec)\n    ```\n\n3. 轮询 Changefeed `dr-primary-to-secondary` 的同步位置时间点 TSO 直到满足 `TSO >= Position`。\n\n    ```shell\n    tiup cdc cli changefeed query -s --server=http://10.1.1.9:8300 --changefeed-id=\"dr-primary-to-secondary\"\n\n    {\n        \"state\": \"normal\",\n        \"tso\": 438224029039198209,  # Changefeed 已经同步到的时间点\n        \"checkpoint\": \"2022-12-22 14:53:25.307\", # TSO 对应的物理时间点\n        \"error\": null\n    }\n    ```\n\n4. 停止 Changefeed `dr-primary-to-secondary`。通过删除 Changefeed 的方式，暂停 Changefeed `dr-primary-to-secondary`：\n\n    ```shell\n    tiup cdc cli changefeed remove --server=http://10.1.1.9:8300 --changefeed-id=\"dr-primary-to-secondary\"\n    ```\n\n5. 创建 Changefeed `dr-secondary-to-primary`。不需要指定 Changefeed `start-ts` 参数，Changefeed 从当前时间开始同步即可。\n6. 修改业务应用的数据库访问配置，并重启业务应用，使得业务访问备用集群。\n7. 检查业务状态是否正常。\n\n容灾演练后，再重复一遍以上步骤，即可恢复原有的系统主备配置。\n\n#### 真正灾难中主备切换\n\n当发生真正的灾难，比如主集群所在区域停电，主备集群的同步链路可能会突然中断，从而导致备用集群数据处于事务不一致的状态。\n\n1. 恢复备用集群到事务一致的状态。在区域 2 的任意 TiCDC 节点执行以下命令，以向备用集群重放 redo log，使备用集群达到最终一致性状态：\n\n    ```shell\n    tiup cdc redo apply --storage \"s3://redo?access-key=minio&secret-access-key=miniostorage&endpoint=http://10.0.1.10:6060&force-path-style=true\" --tmp-dir /tmp/redo --sink-uri \"mysql://{username}:{password}@10.1.1.4:4000\"\n    ```\n\n    命令中参数描述如下：\n\n    - `--storage`：指定 redo log 所在的 S3 位置\n    - `--tmp-dir`：为从 S3 下载 redo log 的缓存目录\n    - `--sink-uri`：指定备份集群的地址\n\n2. 修改业务应用的数据库访问配置，并重启业务应用，使得业务访问备用集群。\n3. 检查业务状态是否正常。\n\n#### 灾难后重建主备集群\n\n当 TiDB 主集群所遭遇的灾难解决后，或者主集群暂时不能恢复，此时 TiDB 集群是脆弱的，只有一个备用集群临时作为新的主集群提供服务。为了维持系统的可靠性，需要重建灾备集群保护系统的可靠性。\n\n目前，重建 TiDB 主备集群，通用的方案是重新部署一个新的集群，组成新的容灾主备集群。操作请参考：\n\n1. [搭建主备集群](#搭建主备集群)。\n2. [从主集群复制数据到备用集群](#从主集群复制数据到备用集群)。\n3. 完成以上操作步骤后，如果你希望新集群成为主集群，那么请参考[主从切换](#计划中的主备切换)。\n\n> **注意：**\n>\n> 如果在业务上能够修正灾难发生后主集群和备用集群的数据不一致的问题，那么也可以使用修正后的集群重建主备集群，而不需要重建新集群。\n\n### 在备用集群上进行业务查询\n\n在主备集群容灾场景中，将备用集群作为只读集群来运行一些延迟不敏感的查询是常见的需求，TiDB 主备集群容灾方案也提供了这种功能。\n\n创建 Changefeed 时，你只需要在配置文件中开启 Syncpoint 功能，Changefeed 就会定期 (`sync-point-interval`) 在备用集群中通过执行 `SET GLOBAL tidb_external_ts = @@tidb_current_ts` 设置已复制完成的一致性快照点。\n\n当业务需要从备用集群查询数据的时候，在业务应用中设置 `SET GLOBAL|SESSION tidb_enable_external_ts_read = ON;` 就可以在备用集群上获得事务状态完成的数据。\n\n```toml\n# 从 v6.4.0 开始支持，使用 Syncpoint 功能需要同步任务拥有下游集群的 SYSTEM_VARIABLES_ADMIN 或者 SUPER 权限\nenable-sync-point = true\n\n# 记录主集群和备用集群一致快照点的时间间隔，它也代表能读到完整事务的最大延迟时间，比如在备用集群读取到主集群两分钟之前的事务数据\n# 配置格式为 h m s，例如 \"1h30m30s\"。默认值为 10m，最小值为 30s\nsync-point-interval = \"10m\"\n\n# Syncpoint 功能在下游表中保存的数据的时长，超过这个时间的数据会被清理\n# 配置格式为 h m s，例如 \"24h30m30s\"。默认值为 24h\nsync-point-retention = \"1h\"\n\n[consistent]\n# eventual consistency： 使用 redo log，提供上游灾难情况下的最终一致性。\nlevel = \"eventual\"\n# 单个 redo log 文件大小，单位 MiB，默认值 64，建议该值不超过 128。\nmax-log-size = 64\n# 刷新或上传 redo log 至 S3 的间隔，单位毫秒，默认 1000，建议范围 500-2000。\nflush-interval = 2000\n# 存储 redo log\nstorage = \"s3://redo?access-key=minio&secret-access-key=miniostorage&endpoint=http://10.0.1.10:6060&force-path-style=true\"\n```\n\n> **注意：**\n>\n> 在主备集群容灾架构中，每个备用集群只能被一个 Changefeed 同步数据，否则就无法保证备用集群的事务完整性。\n\n### 在主备集群之间进行双向复制\n\n在主备集群容灾场景中，部分用户希望让两个区域的 TiDB 集群互为灾备集群：用户的业务流量按其区域属性写入对应的 TiDB 集群，同时两套 TiDB 集群备份对方集群的数据。\n\n![TiCDC bidirectional replication](/media/dr/bdr-ticdc.png)\n\n在双向复制容灾集群方案中，两个区域的 TiDB 集群互相备份对方的数据，使得它们可以在故障发生时互为灾备集群。这种方案既能满足安全性和可靠性的需求，同时也能保证数据库的写入性能。在计划中的主备切换场景中，不需要停止正在运行的 Changefeed 和启动新的 Changefeed 等操作，在运维上也更加简单。\n\n搭建双向容灾复制集群的步骤，请参考教程 [TiCDC 双向复制](/ticdc/ticdc-bidirectional-replication.md)。\n\n## 常见问题处理\n\n以上任何步骤遇到问题，可以先通过 [TiDB FAQ](/faq/faq-overview.md) 查找问题的处理方法。如果问题仍不能解决，请尝试 [TiDB 支持资源](/support.md)。\n"
        },
        {
          "name": "dr-solution-introduction.md",
          "type": "blob",
          "size": 9.7412109375,
          "content": "---\ntitle: TiDB 容灾方案概述\nsummary: 了解 TiDB 提供的几种容灾方案，包括基于主备集群的容灾、基于多副本的单集群容灾和基于备份与恢复的容灾。\n---\n\n# TiDB 容灾方案概述\n\n本文将以如下结构系统介绍 TiDB 容灾解决方案：\n\n- 介绍容灾解决方案涉及的基本概念。\n- 介绍核心组件 TiDB、TiCDC 及 BR 的架构。\n- 介绍各种容灾方案。\n- 对比不同的容灾解决方案。\n\n## 基本概念\n\n- RTO (Recovery Time Objective)：是指灾难发生后，系统恢复服务所需的时间。\n- RPO (Recovery Point Objective)：是指灾难发生后，确保对业务不产生损失的前提下，可以丢失的最大数据量。\n\n下面的图形描述了这两个概念：\n\n![RTO and RPO](/media/dr/rto-rpo.png)\n\n- 错误容忍目标：由于灾难可能影响的地域范围是不同的，在本文中，使用“错误容忍目标”来描述系统能够容忍的灾难的最大范围。\n- 区域：本文主要讨论区域 (region) 级别的容灾方案，这里的区域通常是指一个物理世界中的地区或者城市。\n\n## 组件架构\n\n在介绍具体的容灾方案之前，本部分将从容灾角度介绍容灾系统中的组件架构，包括 TiDB、TiCDC 和 BR。\n\n### TiDB 架构\n\n![TiDB architecture](/media/dr/tidb-architecture.png)\n\nTiDB 的设计采用了计算、存储分离的架构：\n\n- TiDB 为系统的计算层。\n- TiKV 是系统的存储层，采用行存的方式保存数据库的数据记录，其中 [Region](/glossary.md#regionpeerraft-group) 是经过排序的若干行数据的集合，也是系统调度数据的单位。同一个 Region 的数据保存至少 3 份，通过 Raft 协议在日志层复制数据改变。\n- TiFlash 副本是可选的，它是一款列式存储，用于加速分析类查询的速度。数据通过 Raft group 中的 learner 角色与 TiKV 中的数据进行复制。\n\n由于 TiDB 保存了三份完整的数据副本，所以天然就具备了基于多副本数据复制的容灾能力。同时，由于 TiDB 采用了 Raft log 来进行事务日志同步，也在一定程度上具备了基于事务日志同步的容灾能力。\n\n### TiCDC 架构\n\n![TiCDC architecture](/media/ticdc/cdc-architecture.png)\n\nTiCDC 作为 TiDB 的增量数据同步工具，通过 PD 内部的 etcd 实现高可用，通过多个 Capture 进程获取 TiKV 节点上的数据改变，在内部进行排序、合并等处理之后，通过多个同步任务，同时向多个下游系统进行数据同步。在上面的架构中：\n\n- TiKV server：负责将对应节点上数据的改变推送到TiCDC 节点。当然，如果 TiCDC 发现收到的数据改变不完整，也会主动联系 TiKV server 获取需要的数据改变。\n- TiCDC：负责启动多个 Capture 进程，每个 Capture 负责拉取一部分的 KV change logs，并对获取到的数据改变进行排序，最后同步到不同的下游当中。\n\n从上面的架构中可以看到，TiCDC 的架构和事务日志复制系统比较类似，但是扩展性更好，同时又兼顾了逻辑数据复制的很多特点。所以，TiCDC 也可以为 TiDB 在容灾场景提供很好的帮助和补充。\n\n### BR 架构\n\n![BR architecture](/media/br/br-snapshot-arch.png)\n\nBR 作为 TiDB 的备份恢复工具，可以对 TiDB 集群进行基于时间点的全量快照备份和持续的日志备份，从而保护 TiDB 集群的数据。当 TiDB 集群完全不可用时，可以通过备份文件，在全新的集群中进行恢复。备份恢复通常是数据安全的最后一道防线。\n\n## 方案介绍\n\n### 基于 TiCDC 的主备集群容灾方案\n\n![Primary-secondary cluster DR](/media/dr/ticdc-dr.png)\n\n在上面的架构中包含了两个 TiDB 集群，Cluster1 为主用集群，运行在区域 1 (Region 1)，包含 3 个副本，承担读写业务。Cluster2 作为灾备集群，运行在区域 2 (Region 2)。当 Cluster1 出现灾难时，Cluster2 继续对外提供服务。两个集群之间通过 TiCDC 进行数据改变的同步。这种架构，简称为“1:1”解决方案。\n\n这种架构看起来非常简洁，可用性比较高，最大的错误容忍目标可以做到区域级别，写能力也能够得到扩展，RPO 在秒级别，RTO 在分钟级别，甚至更低。如果 RPO 为 0 并不是必须满足的要求，推荐在重要生产系统使用该容灾方案。对于该方案的详细信息，请参考[基于主备集群的容灾方案](/dr-secondary-cluster.md)。\n\n### 基于多副本的单集群容灾方案\n\n![Multi-replica cluster DR](/media/dr/multi-replica-dr.png)\n\n在上面的架构中，每个区域都包含两份完整的数据副本，它们位于不同的可用区 (Available Zone, AZ) 当中（通常情况下，两个可用区之间的网络速度和带宽条件较好，在同一个区域中的不同 AZ 中读写请求的延迟很低），整个集群横跨了三个区域。区域 1 通常是用来处理读写业务请求的主区域，当区域 1 出现灾难后完全不可用时，区域 2 可以作为灾难恢复的区域。而区域 3 (Region 3) 更多的是为了满足多数派协议而存在的一个副本。这种架构，简称为“2-2-1”解决方案。\n\n该方案最大的错误容忍目标可以达到区域级别，写能力也能够得到扩展，并且 RPO 为 0，RTO 也可以达到分钟级别，甚至更低。如果 RPO 为 0 是必须满足的要求，推荐在重要生产系统使用该容灾方案。对于该方案的详细信息，请参考[基于多副本的单集群容灾方案](/dr-multi-replica.md)。\n\n### 多副本与 TiCDC 相结合的容灾解决方案\n\n以上两种容灾解决方案都可以实现区域级别的容灾，但是都无法解决多个区域同时不可用的问题。如果你的系统非常重要，需要“错误容忍目标”达到多个区域，就需要将以上两种容灾解决方案进行结合。\n\n![TiCDC-based multi-replica cluster DR](/media/dr/ticdc-multi-replica-dr.png)\n\n在上面的部署中存在两个 TiDB 集群。Cluster1 有 5 个副本，跨 3 个区域。区域 1 (Region 1) 包含两个副本作为主区域，用于服务写入。区域 2 (Region 2) 有两个副本作为区域 1 的容灾区域，可以提供一些延迟不敏感的读取服务。最后一个副本用于投票，位于区域 3 (Region 3) 中。\n\n作为区域 1 和区域 2 的容灾集群，Cluster2 在区域 3 中运行，并包含 3 个副本。TiCDC 在两个集群之间同步数据更改。这种部署可能看起来比较复杂，但它可以将容错目标提高到多区域。如果多区域故障不要求 RPO 必须为 0，这种架构是一个很好的选择。这种架构，简称为 “2-2-1:1” 解决方案。\n\n当然，如果“错误容忍目标”为多个区域，并且 RPO 为 0 是一个必须满足的要求，你也可以考虑创建一个包含至少 9 个副本，横跨 5 个区域的集群来实现该能力。这种架构，简称为“2-2-2-2-1”解决方案。\n\n### 基于备份与恢复的容灾解决方案\n\n![BR-based cluster DR](/media/dr/br-dr.png)\n\n按照上面的部署，TiDB Cluster1 部署在区域 1 (Region 1)，BR 工具定期将集群的数据备份到区域 2 (Region 2)，并且持续将数据改变日志也备份到区域 2。当区域 1 出现灾难导致 Cluster1 无法恢复时，你可以使用备份的数据和数据改变在区域 2 恢复新的集群 Cluster2 对外提供服务。\n\n基于备份恢复的容灾方案，目前，RPO 低于 5 分钟，而 RTO 则取决于需要恢复的集群数据大小，对于 v6.5.0 版本的 BR，其恢复速度可以参考[快照恢复的性能与影响](/br/br-snapshot-guide.md#快照恢复的性能与影响)和 [PITR 的性能指标](/br//br-pitr-guide.md#pitr-的性能指标)。通常来说，大部分客户会把跨区域 的备份作为数据安全的最后一道防线，是大多数系统都需要的。对于该方案的详细信息，请参考[基于备份与恢复的容灾方案](/dr-backup-restore.md)。\n\n另外，从 v6.5.0 开始，BR 支持[基于 AWS 上的 EBS 快照的快速恢复](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/restore-from-aws-s3-by-snapshot)。如果你在 AWS 上运行 TiDB 集群，要求备份过程对集群没有任何影响，并且要求恢复的时间尽量短，可以考虑使用该特性来降低系统的 RTO。\n\n### 其他容灾解决方案\n\n除了以上容灾方案，针对同城双中心这种特定的场景，如果 RPO=0 是一个必须的条件，你也可以采用 DR-AUTO sync 解决方案。详细的信息请参考[单区域双 AZ 部署 TiDB](/two-data-centers-in-one-city-deployment.md)。\n\n## 方案对比\n\n最后，对本文提到的各种容灾解决方案进行对比，以方便你根据自己的业务需要选择合适的容灾方案。\n\n| 容灾方案 | TCO | 错误容忍目标 | RPO | RTO | 网络延迟要求 | 使用的系统 |\n| --- | --- | --- | --- | --- | --- | --- |\n| 基于多副本的单集群容灾方案 (2-2-1) | 高 | 单个区域 | 0 | 分钟级 | 区域之间的网络延迟要求小于 30 ms。 | 对灾备和响应时间有明确要求 (RPO = 0) 的重要生产系统。 |\n| 基于 TiCDC 的主备集群容灾方案 (1:1)  | 中等 | 单个区域 | 小于 10 秒 | 小于 5 分钟 | 区域之间的网络延迟要求小于 100 ms。 | 对灾备和响应时间有明确要求 (RPO > 0) 的重要生产系统。 |\n| 多副本与 TiCDC 相结合的容灾解决方案 (2-2-1:1) | 高 | 多个区域 | 小于 10 秒 | 小于 5 分钟 | 对于通过多副本进行容灾的区域，网络延迟建议小于 30 ms。对于第三区域与其他区域之间，建议延迟小于 100 ms。 | 对灾难恢复和响应时间有严格要求的关键生产系统。 |\n| 基于备份恢复的容灾方案 | 低 | 单个区域 | 小于 5 分钟 |  小时级 | 无特殊要求 | 能够接受 RPO < 5 分钟，RTO 达到小时级别的系统。 |\n"
        },
        {
          "name": "dumpling-overview.md",
          "type": "blob",
          "size": 22.37890625,
          "content": "---\ntitle: 使用 Dumpling 导出数据\nsummary: 使用 Dumpling 从 TiDB 导出数据。\naliases: ['/docs-cn/dev/dumpling-overview/','/docs-cn/dev/mydumper-overview/','/docs-cn/dev/reference/tools/mydumper/','/zh/tidb/dev/mydumper-overview/']\n---\n\n# 使用 Dumpling 导出数据\n\n使用数据导出工具 [Dumpling](https://github.com/pingcap/tidb/tree/master/dumpling)，你可以把存储在 TiDB 或 MySQL 中的数据导出为 SQL 或 CSV 格式，用于逻辑全量备份。Dumpling 也支持将数据导出到 Amazon S3 中。\n\n要快速了解 Dumpling 的基本功能，建议先观看下面的培训视频（时长 28 分钟）。注意本视频只作为功能介绍、学习参考，具体操作步骤和最新功能，请以文档内容为准。\n\n<video src=\"https://download.pingcap.com/docs-cn%2FLesson18_dumpling.mp4\" width=\"100%\" height=\"100%\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_lesson18.png\"></video>\n\n你可以通过下列任意方式获取 Dumpling：\n\n- TiUP 执行 `tiup install dumpling` 命令。获取后，使用 `tiup dumpling ...` 命令运行 Dumpling。\n- 下载包含 Dumpling 的 [tidb-toolkit 安装包](/download-ecosystem-tools.md)。\n\n更多详情，可以使用 --help 选项查看，或参考 [Dumpling 主要选项表](#dumpling-主要选项表)。\n\n使用 Dumpling 时，需要在已经启动的集群上执行导出命令。\n\nTiDB 还提供了其他工具，你可以根据需要选择使用：\n\n- 如果需要直接备份 SST 文件（键值对），或者对延迟不敏感的增量备份，请使用备份工具 [BR](/br/backup-and-restore-overview.md)。\n- 如果需要实时的增量备份，请使用 [TiCDC](/ticdc/ticdc-overview.md)。\n- 所有的导出数据都可以用 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 导回到 TiDB。\n\n> **注意：**\n>\n> PingCAP 之前维护的 Mydumper 工具 fork 自 [mydumper project](https://github.com/maxbube/mydumper)，针对 TiDB 的特性进行了优化。从 v7.5.0 开始，[Mydumper](https://docs.pingcap.com/tidb/v4.0/mydumper-overview) 废弃，其绝大部分功能已经被 [Dumpling](/dumpling-overview.md) 取代，强烈建议切换到 Dumpling。\n\nDumpling 具有以下优势：\n\n- 支持导出多种数据形式，包括 SQL/CSV。\n- 支持全新的 [table-filter](https://github.com/pingcap/tidb-tools/blob/master/pkg/table-filter/README.md)，筛选数据更加方便。\n- 支持导出到 Amazon S3 云盘。\n- 针对 TiDB 进行了更多优化：\n    - 支持配置 TiDB 单条 SQL 内存限制。\n    - 针对 TiDB v4.0.0 及更新版本，如果 Dumpling 能够访问 TiDB 集群的 PD 地址以及 [`INFORMATION_SCHEMA.CLUSTER_INFO`](/information-schema/information-schema-cluster-info.md) 表，则支持自动调整 [GC](/garbage-collection-overview.md) 的 safe point 从而阻塞 GC。\n    - 使用 TiDB 的隐藏列 `_tidb_rowid` 优化了单表内数据的并发导出性能。\n    - 对于 TiDB 可以设置 [tidb_snapshot](/read-historical-data.md#操作流程) 的值指定备份数据的时间点，从而保证备份的一致性，而不是通过 `FLUSH TABLES WITH READ LOCK` 来保证备份一致性。\n\n> **注意：**\n>\n> 在以下情况下，Dumpling 无法连接到 PD：\n>\n> - TiDB 集群正在 Kubernetes 上运行（Dumpling 本身在 Kubernetes 环境中运行时除外）。\n> - TiDB 集群正在 TiDB Cloud 上运行。\n>\n> 在这种情况下，你需要手动[调整 TiDB GC 时间](#手动设置-tidb-gc-时间)，以避免导出失败。\n\n## 从 TiDB/MySQL 导出数据\n\n### 需要的权限\n\n- PROCESS：需要该权限用于查询集群信息以获取 PD 地址，从而通过 PD 控制 GC。\n- SELECT：导出目标表时需要。\n- RELOAD：使用 consistency flush 时需要。注意，只有 TiDB 支持该权限，当上游为 RDS 或采用托管服务时，可忽略该权限。\n- LOCK TABLES：使用 consistency lock 时需要，需要导出的库表都有该权限。\n- REPLICATION CLIENT：导出 metadata 记录数据快照点时需要，可选，如果不需要导出 metadata，可忽略该权限。\n\n### 导出为 SQL 文件\n\n本文假设在 `127.0.0.1:4000` 有一个 TiDB 实例，并且这个 TiDB 实例中有无密码的 root 用户。\n\nDumpling 默认导出数据格式为 SQL 文件。也可以通过设置 `--filetype sql` 导出数据到 SQL 文件：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dumpling -u root -P 4000 -h 127.0.0.1 --filetype sql -t 8 -o /tmp/test -r 200000 -F 256MiB\n```\n\n以上命令中：\n\n- `-h`、`-P`、`-u` 分别代表地址、端口、用户。如果需要密码验证，可以使用 `-p $YOUR_SECRET_PASSWORD` 将密码传给 Dumpling。\n- `-o`（或 `--output`）用于选择存储导出文件的目录，支持本地文件的绝对路径或[外部存储服务的 URI 格式](#存储服务的-uri-格式说明)。\n- `-t` 用于指定导出的线程数。增加线程数会增加 Dumpling 并发度提高导出速度，但也会加大数据库内存消耗，因此不宜设置过大。一般不超过 64。\n- `-r` 用于开启表内并发加速导出。默认值是 `0`，表示不开启。取值大于 0 表示开启，取值是 INT 类型。当数据源为 TiDB 时，设置 `-r` 参数大于 0 表示使用 TiDB region 信息划分区间，同时减少内存使用。具体取值不影响划分算法。对数据源为 MySQL 且表的主键是 INT 的场景，该参数也有表内并发效果。\n- `-F` 选项用于指定单个文件的最大大小，单位为 `MiB`，可接受类似 `5GiB` 或 `8KB` 的输入。如果你想使用 TiDB Lightning 将该文件加载到 TiDB 实例中，建议将 `-F` 选项的值保持在 256 MiB 或以下。\n\n> **注意：**\n>\n> 如果导出的单表大小超过 10 GB，**强烈建议**使用 `-r` 和 `-F` 参数。\n\n#### 存储服务的 URI 格式说明\n\n本部分介绍 Amazon S3、GCS、和 Azure Blob Storage 存储服务的 URI 格式。基本格式如下：\n\n```shell\n[scheme]://[host]/[path]?[parameters]\n```\n\n关于 URI 格式的详细信息，请参考[外部存储服务的 URI 格式](/external-storage-uri.md)。\n\n### 导出为 CSV 文件\n\n你可以通过使用 `--filetype csv` 导出数据到 CSV 文件。\n\n当你导出 CSV 文件时，你可以使用 `--sql <SQL>` 导出指定 SQL 选择出来的记录。例如，导出 `test.sbtest1` 中所有 `id < 100` 的记录：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dumpling -u root -P 4000 -h 127.0.0.1 -o /tmp/test --filetype csv --sql 'select * from `test`.`sbtest1` where id < 100' -F 100MiB --output-filename-template 'test.sbtest1.{{.Index}}'\n```\n\n以上命令中：\n\n- `--sql` 选项仅仅可用于导出 CSV 文件的场景。上述命令将在要导出的所有表上执行 `SELECT * FROM <table-name> WHERE id < 100` 语句。如果部分表没有指定的字段，那么导出会失败。\n- 使用 `--sql` 配置导出时，Dumpling 无法获知导出的表库信息，此时可以使用 `--output-filename-template` 选项来指定 CSV 文件的文件名格式，以方便后续使用 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 导入数据文件。例如 `--output-filename-template='test.sbtest1.{{.Index}}'` 指定导出的 CSV 文件为 `test.sbtest1.000000000`、`test.sbtest1.000000001` 等。\n- 你可以使用 `--csv-separator`、`--csv-delimiter` 等选项，配置 CSV 文件的格式。具体信息可查阅 [Dumpling 主要选项表](#dumpling-主要选项表)。\n\n> **注意：**\n>\n> Dumpling 导出不区分*字符串*与*关键字*。如果导入的数据是 Boolean 类型的 `true` 和 `false`，导出时会被转换为 `1` 和 `0`。\n\n### 压缩导出的数据文件\n\n你可以使用 `--compress <format>` 压缩导出的 CSV、SQL 数据与表结构文件。该参数支持 `gzip`、`snappy`、`zstd` 压缩算法。默认不压缩。\n\n- 该选项只能压缩单个数据与表结构文件，无法直接压缩整个文件夹生成单个压缩集合包。\n- 该选项可以节省磁盘空间，但也会导致导出速度变慢，并增加 CPU 消耗。对导出速度要求较高的场景需慎用。\n- TiDB Lightning v6.5.0 及以上版本支持直接使用 Dumpling 压缩文件作为数据源导入，无需额外配置。\n\n> **注意：**\n>\n> Snappy 压缩文件必须遵循[官方 Snappy 格式](https://github.com/google/snappy)。不支持其他非官方压缩格式。\n\n### 输出文件格式\n\n+ `metadata`：此文件包含导出的起始时间，以及 master binary log 的位置。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    cat metadata\n    ```\n\n    ```shell\n    Started dump at: 2020-11-10 10:40:19\n    SHOW MASTER STATUS:\n            Log: tidb-binlog\n            Pos: 420747102018863124\n\n    Finished dump at: 2020-11-10 10:40:20\n    ```\n\n+ `{schema}-schema-create.sql`：创建 schema 的 SQL 文件。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    cat test-schema-create.sql\n    ```\n\n    ```shell\n    CREATE DATABASE `test` /*!40100 DEFAULT CHARACTER SET utf8mb4 */;\n    ```\n\n+ `{schema}.{table}-schema.sql`：创建 table 的 SQL 文件\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    cat test.t1-schema.sql\n    ```\n\n    ```shell\n    CREATE TABLE `t1` (\n      `id` int DEFAULT NULL\n    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin;\n    ```\n\n+ `{schema}.{table}.{0001}.{sql|csv}`：数据源文件\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    cat test.t1.0.sql\n    ```\n\n    ```shell\n    /*!40101 SET NAMES binary*/;\n    INSERT INTO `t1` VALUES\n    (1);\n    ```\n\n+ `*-schema-view.sql`、`*-schema-trigger.sql`、`*-schema-post.sql`：其他导出文件\n\n### 导出到 Amazon S3 云盘\n\nDumpling 在 v4.0.8 及更新版本支持导出到 Amazon S3 云盘。如果需要将数据备份到 Amazon S3 后端存储，那么需要在 `-o` 参数中指定 Amazon S3 的存储路径。\n\n可以参照 [AWS 官方文档 - 如何创建 S3 存储桶](https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/user-guide/create-bucket.html)在指定的 `Region` 区域中创建一个 S3 桶 `Bucket`。如有需要，还可以参照 [AWS 官方文档 - 创建文件夹](https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/user-guide/create-folder.html)在 Bucket 中创建一个文件夹 `Folder`。\n\n将有权限访问该 Amazon S3 后端存储的账号的 `SecretKey` 和 `AccessKey` 作为环境变量传入 Dumpling 节点。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\nexport AWS_ACCESS_KEY_ID=${AccessKey}\nexport AWS_SECRET_ACCESS_KEY=${SecretKey}\n```\n\nDumpling 同时还支持从 `~/.aws/credentials` 读取凭证文件。更多参数描述，请参考[外部存储服务的 URI 格式](/external-storage-uri.md)。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dumpling -u root -P 4000 -h 127.0.0.1 -r 200000 -o \"s3://${Bucket}/${Folder}\"\n```\n\n### 筛选导出的数据\n\n#### 使用 `--where` 选项筛选数据\n\n默认情况下，Dumpling 会导出排除系统数据库（包括 `mysql` 、`sys` 、`INFORMATION_SCHEMA` 、`PERFORMANCE_SCHEMA`、`METRICS_SCHEMA` 和 `INSPECTION_SCHEMA`）外所有其他数据库。你可以使用 `--where <SQL where expression>` 来指定要导出的记录。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dumpling -u root -P 4000 -h 127.0.0.1 -o /tmp/test --where \"id < 100\"\n```\n\n上述命令将会导出各个表的 id < 100 的数据。注意 `--where` 参数无法与 `--sql` 一起使用。\n\n#### 使用 `--filter` 选项筛选数据\n\nDumpling 可以通过 `--filter` 指定 table-filter 来筛选特定的库表。table-filter 的语法与 `.gitignore` 相似，详细语法参考[表库过滤](/table-filter.md)。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dumpling -u root -P 4000 -h 127.0.0.1 -o /tmp/test -r 200000 --filter \"employees.*\" --filter \"*.WorkOrder\"\n```\n\n上述命令将会导出 `employees` 数据库的所有表，以及所有数据库中的 `WorkOrder` 表。\n\n#### 使用 `-B` 或 `-T` 选项筛选数据\n\nDumpling 也可以通过 `-B` 或 `-T` 选项导出特定的数据库/数据表。\n\n> **注意：**\n>\n> - `--filter` 选项与 `-T` 选项不可同时使用。\n> - `-T` 选项只能接受完整的 `库名.表名` 形式，不支持只指定表名。例：Dumpling 无法识别 `-T WorkOrder`。\n\n例如通过指定：\n\n- `-B employees` 导出 `employees` 数据库\n- `-T employees.WorkOrder` 导出 `employees.WorkOrder` 数据表\n\n### 通过并发提高 Dumpling 的导出效率\n\n默认情况下，导出的文件会存储到 `./export-<current local time>` 目录下。常用选项如下：\n\n- `-t` 用于指定导出的线程数。增加线程数会增加 Dumpling 并发度提高导出速度，但也会加大数据库内存消耗，因此不宜设置过大。\n- `-r` 选项用于指定单个文件的最大记录数，或者说，数据库中的行数。开启后 Dumpling 会开启表内并发，提高导出大表的速度。当上游为 TiDB 且版本为 v3.0 或更新版本时，设置 `-r` 参数大于 0 表示使用 TiDB region 信息划分表内并发，具体取值不影响划分算法。对上游为 MySQL 且表的主键是 int 的场景，该参数也有表内并发效果。\n- `--compress <format>` 选项可以用于压缩导出的数据，支持 `gzip`、`snappy`、`zstd` 压缩算法。压缩可以显著降低导出数据的大小，同时如果存储的写入 I/O 带宽不足，可以使用该选项来加速导出。但该选项也有副作用，由于该选项会对每个文件单独压缩，因此会增加 CPU 消耗。\n\n利用以上选项可以提高 Dumpling 的导出速度。\n\n### 调整 Dumpling 的数据一致性选项\n\n> **注意：**\n>\n> 数据一致性选项的默认值为 `auto`。在大多数场景下，你不需要调整该选项。\n\nDumpling 通过 `--consistency <consistency level>` 标志控制导出数据“一致性保证”的方式。在使用 snapshot 来保证一致性的时候，可以使用 `--snapshot` 选项指定要备份的时间戳。还可以使用以下的一致性级别：\n\n- `flush`：使用 [`FLUSH TABLES WITH READ LOCK`](https://dev.mysql.com/doc/refman/8.0/en/flush.html#flush-tables-with-read-lock) 短暂地中断备份库的 DML 和 DDL 操作、保证备份连接的全局一致性和记录 POS 信息。所有的备份连接启动事务后释放该锁。推荐在业务低峰或者 MySQL 备份库上进行全量备份。\n- `snapshot`：获取指定时间戳的一致性快照并导出。\n- `lock`：为待导出的所有表上读锁。\n- `none`：不做任何一致性保证。\n- `auto`：对 MySQL 使用 `flush`，对 TiDB 使用 `snapshot`。\n\n操作完成之后，你可以在 `/tmp/test` 查看导出的文件：\n\n```shell\n$ ls -lh /tmp/test | awk '{print $5 \"\\t\" $9}'\n\n140B  metadata\n66B   test-schema-create.sql\n300B  test.sbtest1-schema.sql\n190K  test.sbtest1.0.sql\n300B  test.sbtest2-schema.sql\n190K  test.sbtest2.0.sql\n300B  test.sbtest3-schema.sql\n190K  test.sbtest3.0.sql\n```\n\n### 导出 TiDB 的历史数据快照\n\nDumpling 可以通过 `--snapshot` 指定导出某个 [tidb_snapshot](/read-historical-data.md#操作流程) 的数据。\n\n`--snapshot` 选项可设为 TSO（`SHOW MASTER STATUS` 输出的 `Position` 字段）或有效的 `datetime` 时间（`YYYY-MM-DD hh:mm:ss` 形式），例如：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dumpling --snapshot 417773951312461825\ntiup dumpling --snapshot \"2020-07-02 17:12:45\"\n```\n\n即可导出 TSO 为 `417773951312461825` 或 `2020-07-02 17:12:45` 时的 TiDB 历史数据快照。\n\n### 控制导出 TiDB 大表（超过 1 TB）时的内存使用\n\nDumpling 导出 TiDB 较大单表（超过 1 TB）时，可能会因为导出数据过大导致 TiDB 内存溢出 (OOM)，从而使连接中断导出失败。可以通过以下参数减少 TiDB 的内存使用。\n\n+ 设置 `-r` 参数，可以划分导出数据区块减少 TiDB 扫描数据的内存开销，同时也可开启表内并发提高导出效率。当上游为 TiDB 且版本为 v3.0 或更新版本时，设置 `-r` 参数大于 0 表示使用 TiDB region 信息划分表内并发，具体取值不影响划分算法。\n+ 调小 `--tidb-mem-quota-query` 参数到 `8589934592` (8GB) 或更小。可控制 TiDB 单条查询语句的内存使用。\n+ 调整 `--params \"tidb_distsql_scan_concurrency=5\"` 参数，即设置导出时的 session 变量 [`tidb_distsql_scan_concurrency`](/system-variables.md#tidb_distsql_scan_concurrency) 从而减少 TiDB scan 操作的并发度。\n\n### 手动设置 TiDB GC 时间\n\n当导出的数据量少于 1 TB，导出的 TiDB 版本为 v4.0.0 或更新版本，且 Dumpling 可以访问 TiDB 集群的 PD 地址以及 [`INFORMATION_SCHEMA.CLUSTER_INFO`](/information-schema/information-schema-cluster-info.md) 表时，Dumpling 会自动调整 GC 的 safe point 从而阻塞 GC 且不会对原集群造成影响。\n\n但是，在以下场景中，Dumpling 无法自动调整 GC 时间：\n\n- 数据量非常大（超过 1 TB）。\n- Dumpling 无法直接连接到 PD，例如 TiDB 集群运行在 TiDB Cloud 上，或者 TiDB 集群运行在 Kubernetes 上且与 Dumpling 分离。\n\n在这些场景中，你必须提前手动调长 GC 时间，以避免因为导出过程中发生 GC 导致导出失败。\n\n使用以下 SQL 语句手动调整 GC 时间：\n\n```sql\nSET GLOBAL tidb_gc_life_time = '720h';\n```\n\n在 Dumpling 退出后，无论导出是否成功，都必须将 GC 时间恢复为其原始值（默认值为 `10m`）。\n\n```sql\nSET GLOBAL tidb_gc_life_time = '10m';\n```\n\n## Dumpling 主要选项表\n\n| 主要选项 | 用途 | 默认值 |\n| --------| --- | --- |\n| -V 或 --version | 输出 Dumpling 版本并直接退出 |\n| -B 或 --database | 导出指定数据库 |\n| -T 或 --tables-list | 导出指定数据表 |\n| -f 或 --filter | 导出能匹配模式的表，语法可参考 [table-filter](/table-filter.md) | `[\\*.\\*,!/^(mysql&#124;sys&#124;INFORMATION_SCHEMA&#124;PERFORMANCE_SCHEMA&#124;METRICS_SCHEMA&#124;INSPECTION_SCHEMA)$/.\\*]`（导出除系统库外的所有库表） |\n| --case-sensitive | table-filter 是否大小写敏感 | false，大小写不敏感 |\n| -h 或 --host| 连接的数据库主机的地址 | \"127.0.0.1\" |\n| -t 或 --threads | 备份并发线程数| 4 |\n| -r 或 --rows | 用于开启表内并发加速导出。默认值是 `0`，表示不开启。取值大于 0 表示开启，取值是 INT 类型。当数据源为 TiDB 时，设置 `-r` 参数大于 0 表示使用 TiDB region 信息划分区间，同时减少内存使用。具体取值不影响划分算法。对数据源为 MySQL 且表的主键是 INT 的场景，该参数也有表内并发效果。 |\n| -L 或 --logfile | 日志输出地址，为空时会输出到控制台 | \"\" |\n| --loglevel | 日志级别 {debug,info,warn,error,dpanic,panic,fatal} | \"info\" |\n| --logfmt | 日志输出格式 {text,json} | \"text\" |\n| -d 或 --no-data | 不导出数据，适用于只导出 schema 场景 |\n| --no-header | 导出 csv 格式的 table 数据，不生成 header |\n| -W 或 --no-views| 不导出 view | true |\n| -m 或 --no-schemas | 不导出 schema，只导出数据 |\n| -s 或--statement-size | 控制 `INSERT` SQL 语句的大小，单位 bytes |\n| -F 或 --filesize | 将 table 数据划分出来的文件大小，需指明单位（如 `128B`, `64KiB`, `32MiB`, `1.5GiB`） |\n| --filetype| 导出文件类型（csv/sql） | \"sql\" |\n| -o 或 --output | 导出本地文件的绝对路径或[外部存储服务的 URI 格式](/external-storage-uri.md) | \"./export-${time}\" |\n| -S 或 --sql | 根据指定的 sql 导出数据，该选项不支持并发导出 |\n| --consistency | flush: dump 前用 FTWRL <br/> snapshot: 通过 TSO 来指定 dump 某个快照时间点的 TiDB 数据 <br/> lock: 对需要 dump 的所有表执行 `lock tables read` 命令 <br/> none: 不加锁 dump，无法保证一致性 <br/> auto: 对 MySQL 使用 --consistency flush；对 TiDB 使用 --consistency snapshot | \"auto\" |\n| --snapshot | snapshot tso，只在 consistency=snapshot 下生效 |\n| --where | 对备份的数据表通过 where 条件指定范围 |\n| -p 或 --password | 连接的数据库主机的密码 |\n| -P 或 --port | 连接的数据库主机的端口 | 4000 |\n| -u 或 --user | 连接的数据库主机的用户名 | \"root\" |\n| --dump-empty-database | 导出空数据库的建库语句 | true |\n| --ca | 用于 TLS 连接的 certificate authority 文件的地址 |\n| --cert | 用于 TLS 连接的 client certificate 文件的地址 |\n| --key | 用于 TLS 连接的 client private key 文件的地址 |\n| --csv-delimiter | CSV 文件中字符类型变量的定界符 | '\"' |\n| --csv-separator | CSV 文件中各值的分隔符，如果数据中可能有逗号，建议源文件导出时分隔符使用非常见组合字符| ','|\n| --csv-null-value | CSV 文件空值的表示 | \"\\\\N\" |\n| --csv-line-terminator | CSV 文件中表示行尾的换行符。将数据导出为 CSV 文件时，可以通过该选项传入所需的换行符。该选项支持 \"\\\\r\\\\n\" 和 \"\\\\n\"，默认值为 \"\\\\r\\\\n\"，和历史版本保持一致。由于 bash 中不同的引号会应用不同的转义规则，如需指定 LF 为换行符，可使用类似 `--csv-line-terminator $'\\n'` 的语法。| \"\\\\r\\\\n\" |\n| --csv-output-dialect | 表示可以将源数据导出成数据库所需的格式存储到 CSV。该选项取值可为 `\"\"`，`\"snowflake\"`、`\"redshift\"`、`\"bigquery\"`。默认值为 `\"\"`，表示会按 UTF-8 进行编码并导出数据。如果设置为 `\"snowflake\"` 或 `\"redshift\"`，会把 Binary 数据类型转换成十六进制，但会丢失十六进制数的前缀 `0x`，例如 `0x61` 将被表示成 `61`。如果设置为 `\"bigquery\"`，会使用 base64 对 Binary 数据类型进行编码。在某些情况下，Binary 字符串会出现乱码。| `\"\"` |\n| --escape-backslash | 使用反斜杠 (`\\`) 来转义导出文件中的特殊字符 | true |\n| --output-filename-template | 以 [golang template](https://golang.org/pkg/text/template/#hdr-Arguments) 格式表示的数据文件名格式 <br/> 支持 `{{.DB}}`、`{{.Table}}`、`{{.Index}}` 三个参数 <br/> 分别表示数据文件的库名、表名、分块 ID | '{{.DB}}.{{.Table}}.{{.Index}}' |\n| --status-addr | Dumpling 的服务地址，包含了 Prometheus 拉取 metrics 信息及 pprof 调试的地址 | \":8281\" |\n| --tidb-mem-quota-query | 单条 dumpling 命令导出 SQL 语句的内存限制，单位为 byte。对于 v4.0.10 或以上版本，若不设置该参数，默认使用 TiDB 中的 `mem-quota-query` 配置项值作为内存限制值。对于 v4.0.10 以下版本，该参数值默认为 32 GB | 34359738368 |\n| --params | 为需导出的数据库连接指定 session 变量，可接受的格式: \"character_set_client=latin1,character_set_connection=latin1\" |\n| -c 或 --compress | 压缩 Dumpling 导出的 CSV、SQL 数据与表结构文件为指定格式，支持 \"gzip\"、\"snappy\" 和 \"zstd\" 压缩算法 | \"\" |\n"
        },
        {
          "name": "dynamic-config.md",
          "type": "blob",
          "size": 25.7890625,
          "content": "---\ntitle: 在线修改集群配置\nsummary: 介绍在线修改集群配置的功能。\naliases: ['/docs-cn/dev/dynamic-config/']\n---\n\n# 在线修改集群配置\n\n在线配置变更主要是通过利用 SQL 对包括 TiDB、TiKV 以及 PD 在内的各组件的配置进行在线更新。用户可以通过在线配置变更对各组件进行性能调优而无需重启集群组件。但目前在线修改 TiDB 实例配置的方式和修改其他组件 (TiKV, PD) 的有所不同。\n\n## 常用操作\n\n### 查看实例配置\n\n可以通过 SQL 语句 `show config` 来直接查看集群所有实例的配置信息，结果如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nshow config;\n```\n\n```sql\n+------+-----------------+-----------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Type | Instance        | Name                                                      | Value                                                                                                                                                                                                                                                                            |\n+------+-----------------+-----------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| tidb | 127.0.0.1:4001  | advertise-address                                         | 127.0.0.1                                                                                                                                                                                                                                                                        |\n| tidb | 127.0.0.1:4001  | binlog.binlog-socket                                      |                                                                                                                                                                                                                                                                                  |\n| tidb | 127.0.0.1:4001  | binlog.enable                                             | false                                                                                                                                                                                                                                                                            |\n| tidb | 127.0.0.1:4001  | binlog.ignore-error                                       | false                                                                                                                                                                                                                                                                            |\n| tidb | 127.0.0.1:4001  | binlog.strategy                                           | range                                                                                                                                                                                                                                                                            |\n| tidb | 127.0.0.1:4001  | binlog.write-timeout                                      | 15s                                                                                                                                                                                                                                                                              |\n| tidb | 127.0.0.1:4001  | check-mb4-value-in-utf8                                   | true                                                                                                                                                                                                                                                                             |\n\n...\n```\n\n还可以根据对应的字段进行过滤，如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nshow config where type='tidb'\nshow config where instance in (...)\nshow config where name like '%log%'\nshow config where type='tikv' and name='log.level'\n```\n\n### 在线修改 TiKV 配置\n\n> **注意：**\n>\n> 在线修改 TiKV 配置项后，同时会自动修改 TiKV 的配置文件。但还需要使用 `tiup edit-config` 命令来修改对应的配置项，否则 `upgrade` 和 `reload` 等运维操作会将在线修改配置后的结果覆盖。修改配置的操作请参考：[使用 TiUP 修改配置](/maintain-tidb-using-tiup.md#修改配置参数)。执行 `tiup edit-config` 后不需要执行 `tiup reload` 操作。\n\n执行 SQL 语句 `set config`，可以结合实例地址或组件类型来修改单个实例配置或全部实例配置，如：\n\n修改全部 TiKV 实例配置：\n\n> **注意：**\n>\n> 建议使用反引号包裹变量名称。\n\n{{< copyable \"sql\" >}}\n\n```sql\nset config tikv `split.qps-threshold`=1000\n```\n\n修改单个 TiKV 实例配置：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset config \"127.0.0.1:20180\" `split.qps-threshold`=1000\n```\n\n设置成功会返回 `Query OK`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n在批量修改时如果有错误发生，会以 warning 的形式返回：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset config tikv `log-level`='warn';\n```\n\n```sql\nQuery OK, 0 rows affected, 1 warning (0.04 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nshow warnings;\n```\n\n```sql\n+---------+------+---------------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                                       |\n+---------+------+---------------------------------------------------------------------------------------------------------------+\n| Warning | 1105 | bad request to http://127.0.0.1:20180/config: fail to update, error: \"config log-level can not be changed\" |\n+---------+------+---------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n批量修改配置不保证原子性，可能出现某些实例成功，而某些失败的情况。如使用 `set tikv key=val` 命令修改整个 TiKV 集群配置时，可能有部分实例失败，请执行 `show warnings` 进行查看。\n\n如遇到部分修改失败的情况，需要重新执行对应的修改语句，或通过修改单个实例的方式完成修改。如果因网络或者机器故障等原因无法访问到的 TiKV，需要等到恢复后再次进行修改。\n\n针对 TiKV 可在线修改的配置项，如果成功修改后，修改的结果会被持久化到配置文件中，后续以配置文件中的配置为准。某些配置项名称可能和 TiDB 预留关键字冲突，如 `limit`、`key` 等，对于此类配置项，需要用反引号 ``` ` ``` 包裹起来，如 ``` `raftstore.raft-log-gc-size-limit` ```。\n\n支持的配置项列表如下：\n\n| 配置项 | 简介 |\n| --- | --- |\n| log.level | 日志等级 |\n| raftstore.raft-max-inflight-msgs | 待确认的日志个数，如果超过这个数量，Raft 状态机会减缓发送日志的速度 |\n| raftstore.raft-log-gc-tick-interval | 删除 Raft 日志的轮询任务调度间隔时间 |\n| raftstore.raft-log-gc-threshold | 允许残余的 Raft 日志个数，软限制 |\n| raftstore.raft-log-gc-count-limit | 允许残余的 Raft 日志个数，硬限制 |\n| raftstore.raft-log-gc-size-limit | 允许残余的 Raft 日志大小，硬限制 |\n| raftstore.raft-max-size-per-msg | 允许生成的单个消息包的大小，软限制 |\n| raftstore.raft-entry-max-size | 单个 Raft 日志最大大小，硬限制 |\n| raftstore.raft-entry-cache-life-time | 内存中日志 cache 允许的最长残留时间 |\n| raftstore.max-apply-unpersisted-log-limit | 允许 apply 已 commit 但尚未持久化的 Raft 日志的最大数量 |\n| raftstore.split-region-check-tick-interval | 检查 Region 是否需要分裂的时间间隔 |\n| raftstore.region-split-check-diff | 允许 Region 数据超过指定大小的最大值 |\n| raftstore.region-compact-check-interval | 检查是否需要人工触发 RocksDB compaction 的时间间隔 |\n| raftstore.region-compact-check-step | 每轮校验人工 compaction 时，一次性检查的 Region 个数 |\n| raftstore.region-compact-min-tombstones | 触发 RocksDB compaction 需要的 tombstone 个数 |\n| raftstore.region-compact-tombstones-percent | 触发 RocksDB compaction 需要的 tombstone 所占比例 |\n| raftstore.pd-heartbeat-tick-interval | 触发 Region 对 PD 心跳的时间间隔 |\n| raftstore.pd-store-heartbeat-tick-interval | 触发 store 对 PD 心跳的时间间隔 |\n| raftstore.snap-mgr-gc-tick-interval | 触发回收过期 snapshot 文件的时间间隔 |\n| raftstore.snap-gc-timeout | snapshot 文件的最长保存时间 |\n| raftstore.lock-cf-compact-interval | 触发对 lock CF compact 检查的时间间隔 |\n| raftstore.lock-cf-compact-bytes-threshold | 触发对 lock CF 进行 compact 的大小 |\n| raftstore.messages-per-tick | 每轮处理的消息最大个数 |\n| raftstore.max-peer-down-duration | 副本允许的最长未响应时间 |\n| raftstore.max-leader-missing-duration | 允许副本处于无主状态的最长时间，超过将会向 PD 校验自己是否已经被删除 |\n| raftstore.abnormal-leader-missing-duration | 允许副本处于无主状态的时间，超过将视为异常，标记在 metrics 和日志中 |\n| raftstore.peer-stale-state-check-interval | 触发检验副本是否处于无主状态的时间间隔 |\n| raftstore.consistency-check-interval | 触发一致性检查的时间间隔（不建议使用该配置项，因为与 TiDB GC 操作不兼容）|\n| raftstore.raft-store-max-leader-lease | Region 主可信任期的最长时间 |\n| raftstore.merge-check-tick-interval | 触发 Merge 完成检查的时间间隔 |\n| raftstore.cleanup-import-sst-interval | 触发检查过期 SST 文件的时间间隔 |\n| raftstore.local-read-batch-size | 一轮处理读请求的最大个数 |\n| raftstore.apply-yield-write-size | Apply 线程每一轮处理单个状态机写入的最大数据量 |\n| raftstore.hibernate-timeout | 启动后进入静默状态前需要等待的最短时间，在该时间段内不会进入静默状态（未 release）|\n| raftstore.apply-pool-size | 处理把数据落盘至磁盘的线程池中线程的数量，即 Apply 线程池大小 |\n| raftstore.store-pool-size | 处理 Raft 的线程池中线程的数量，即 Raftstore 线程池的大小 |\n| raftstore.apply-max-batch-size | Raft 状态机由 BatchSystem 批量执行数据写入请求，该配置项指定每批可执行请求的最多 Raft 状态机个数。 |\n| raftstore.store-max-batch-size |  Raft 状态机由 BatchSystem 批量执行把日志落盘至磁盘的请求，该配置项指定每批可执行请求的最多 Raft 状态机个数。 |\n| raftstore.store-io-pool-size | 处理 Raft I/O 任务的线程池中线程的数量，即 StoreWriter 线程池的大小（不支持将该配置项由非零值调整为 0，或者从 0 调整为非零值）|\n| raftstore.periodic-full-compact-start-max-cpu | 控制 TiKV 执行周期性全量数据整理时的 CPU 使用率阈值 |\n| readpool.unified.max-thread-count | 统一处理读请求的线程池最多的线程数量，即 UnifyReadPool 线程池大小 |\n| readpool.unified.max-tasks-per-worker | 统一处理读请求的线程池中单个线程允许积压的最大任务数量，超出后会返回 Server Is Busy。 |\n| readpool.unified.auto-adjust-pool-size | 是否开启自适应调整 UnifyReadPool 的大小 |\n| resource-control.priority-ctl-strategy | 配置低优先级任务的流量管控策略。 |\n| coprocessor.split-region-on-table | 开启按 table 分裂 Region 的开关 |\n| coprocessor.batch-split-limit | 批量分裂 Region 的阈值 |\n| coprocessor.region-max-size | Region 容量空间的最大值 |\n| coprocessor.region-split-size | 分裂后新 Region 的大小 |\n| coprocessor.region-max-keys | Region 最多允许的 key 的个数 |\n| coprocessor.region-split-keys | 分裂后新 Region 的 key 的个数 |\n| pessimistic-txn.wait-for-lock-timeout | 悲观事务遇到锁后的最长等待时间 |\n| pessimistic-txn.wake-up-delay-duration | 悲观事务被重新唤醒的时间 |\n| pessimistic-txn.pipelined | 是否开启流水线式加悲观锁流程 |\n| pessimistic-txn.in-memory | 是否开启内存悲观锁功能 |\n| pessimistic-txn.in-memory-peer-size-limit | 控制单个 Region 内存悲观锁的内存使用上限 |\n| pessimistic-txn.in-memory-instance-size-limit | 控制单个 TiKV 实例内存悲观锁的内存使用上限 |\n| quota.foreground-cpu-time | 限制处理 TiKV 前台读写请求所使用的 CPU 资源使用量，软限制 |\n| quota.foreground-write-bandwidth | 限制前台事务写入的带宽，软限制 |\n| quota.foreground-read-bandwidth | 限制前台事务读取数据和 Coprocessor 读取数据的带宽，软限制 |\n| quota.background-cpu-time | 限制处理 TiKV 后台读写请求所使用的 CPU 资源使用量，软限制 |\n| quota.background-write-bandwidth | 限制后台事务写入的带宽，软限制，暂未生效 |\n| quota.background-read-bandwidth | 限制后台事务读取数据和 Coprocessor 读取数据的带宽，软限制，暂未生效 |\n| quota.enable-auto-tune | 是否支持 quota 动态调整。如果打开该配置项，TiKV 会根据 TiKV 实例的负载情况动态调整对后台请求的限制 quota |\n| quota.max-delay-duration | 单次读写请求被强制等待的最大时间 |\n| gc.ratio-threshold | 跳过 Region GC 的阈值（GC 版本个数/key 个数）|\n| gc.batch-keys | 一轮处理 key 的个数 |\n| gc.max-write-bytes-per-sec | 一秒可写入 RocksDB 的最大字节数 |\n| gc.enable-compaction-filter | 是否使用 compaction filter |\n| gc.compaction-filter-skip-version-check | 是否跳过 compaction filter 的集群版本检查（未 release）|\n| {db-name}.max-total-wal-size | WAL 总大小限制 |\n| {db-name}.max-background-jobs | RocksDB 后台线程个数 |\n| {db-name}.max-background-flushes | RocksDB flush 线程个数 |\n| {db-name}.max-open-files | RocksDB 可以打开的文件总数 |\n| {db-name}.compaction-readahead-size | Compaction 时候 readahead 的大小 |\n| {db-name}.bytes-per-sync | 异步同步的限速速率 |\n| {db-name}.wal-bytes-per-sync | WAL 同步的限速速率 |\n| {db-name}.writable-file-max-buffer-size | WritableFileWrite 所使用的最大的 buffer 大小 |\n| {db-name}.{cf-name}.block-cache-size | block cache size 大小 |\n| {db-name}.{cf-name}.write-buffer-size | memtable 大小 |\n| {db-name}.{cf-name}.max-write-buffer-number | 最大 memtable 个数 |\n| {db-name}.{cf-name}.max-bytes-for-level-base | base level (L1) 最大字节数 |\n| {db-name}.{cf-name}.target-file-size-base | base level 的目标文件大小 |\n| {db-name}.{cf-name}.level0-file-num-compaction-trigger | 触发 compaction 的 L0 文件最大个数 |\n| {db-name}.{cf-name}.level0-slowdown-writes-trigger | 触发 write stall 的 L0 文件最大个数 |\n| {db-name}.{cf-name}.level0-stop-writes-trigger | 完全阻停写入的 L0 文件最大个数 |\n| {db-name}.{cf-name}.max-compaction-bytes | 一次 compaction 最大写入字节数 |\n| {db-name}.{cf-name}.max-bytes-for-level-multiplier | 每一层的默认放大倍数 |\n| {db-name}.{cf-name}.disable-auto-compactions | 自动 compaction 的开关 |\n| {db-name}.{cf-name}.soft-pending-compaction-bytes-limit | pending compaction bytes 的软限制 |\n| {db-name}.{cf-name}.hard-pending-compaction-bytes-limit | pending compaction bytes 的硬限制 |\n| {db-name}.{cf-name}.titan.blob-run-mode | 处理 blob 文件的模式 |\n| {db-name}.{cf-name}.titan.min-blob-size | 数据存储在 Titan 的阈值，当数据的 value 达到该阈值时将存储在 Titan 的 Blob 文件中 |\n| {db-name}.{cf-name}.titan.blob-file-compression | Titan 的 Blob 文件所使用的压缩算法 |\n| {db-name}.{cf-name}.titan.discardable-ratio | Titan 数据文件 GC 的垃圾数据比例阈值，当一个 Blob 文件中无用数据的比例超过该阈值时将会触发 Titan GC |\n| server.grpc-memory-pool-quota | gRPC 可使用的内存大小限制 |\n| server.max-grpc-send-msg-len | gRPC 可发送的最大消息长度 |\n| server.raft-msg-max-batch-size | 单个 gRPC 消息可包含的最大 Raft 消息个数 |\n| server.simplify-metrics | 精简监控采样数据的开关 |\n| server.snap-io-max-bytes-per-sec | 处理 snapshot 时最大允许使用的磁盘带宽 |\n| server.concurrent-send-snap-limit | 同时发送 snapshot 的最大个数 |\n| server.concurrent-recv-snap-limit | 同时接受 snapshot 的最大个数 |\n| storage.block-cache.capacity | 共享 block cache 的大小（自 v4.0.3 起支持） |\n| storage.flow-control.enable | 是否开启流量控制机制 |\n| storage.flow-control.memtables-threshold | 触发流量控制的 KvDB memtable 数量阈值 |\n| storage.flow-control.l0-files-threshold | 触发流量控制的 KvDB L0 文件数量阈值 |\n| storage.flow-control.soft-pending-compaction-bytes-limit | 触发流控机制开始拒绝部分写入请求的 KvDB pending compaction bytes 阈值 |\n| storage.flow-control.hard-pending-compaction-bytes-limit | 触发流控机制拒绝所有新写入请求的 KvDB pending compaction bytes 阈值 |\n| storage.scheduler-worker-pool-size | Scheduler 线程池中线程的数量 |\n| import.num-threads | 处理恢复或导入 RPC 请求的线程数量（自 v8.1.2 起支持在线修改） |\n| backup.num-threads | backup 线程的数量（自 v4.0.3 起支持） |\n| split.qps-threshold | 对 Region 执行 load-base-split 的阈值。如果连续 10s 内，某个 Region 的读请求的 QPS 超过 qps-threshold，则尝试切分该 Region |\n| split.byte-threshold | 对 Region 执行 load-base-split 的阈值。如果连续 10s 内，某个 Region 的读请求的流量超过 byte-threshold，则尝试切分该 Region |\n| split.region-cpu-overload-threshold-ratio | 对 Region 执行 load-base-split 的阈值。如果连续 10s 内，某个 Region 的 Unified Read Pool CPU 使用时间占比超过了 region-cpu-overload-threshold-ratio，则尝试切分该 Region（自 v6.2.0 起支持）|\n| split.split-balance-score | load-base-split 的控制参数，确保 Region 切分后左右访问尽量均匀，数值越小越均匀，但也可能导致无法切分 |\n| split.split-contained-score | load-base-split 的控制参数，数值越小，Region 切分后跨 Region 的访问越少 |\n| cdc.min-ts-interval | 定期推进 Resolved TS 的时间间隔 |\n| cdc.old-value-cache-memory-quota | 缓存在内存中的 TiCDC Old Value 的条目占用内存的上限 |\n| cdc.sink-memory-quota| 缓存在内存中的 TiCDC 数据变更事件占用内存的上限 |\n| cdc.incremental-scan-speed-limit| 增量扫描历史数据的速度上限 |\n| cdc.incremental-scan-concurrency | 增量扫描历史数据任务的最大并发执行个数 |\n上述前缀为 `{db-name}` 或 `{db-name}.{cf-name}` 的是 RocksDB 相关的配置项。`db-name` 的取值可为 `rocksdb` 或 `raftdb`。\n\n- 当 `db-name` 为 `rocksdb` 时，`cf-name` 的可取值有：`defaultcf`、`writecf`、`lockcf`、`raftcf`；\n- 当 `db-name` 为 `raftdb` 时，`cf-name` 的可取值有：`defaultcf`。\n\n具体配置项的意义可参考 [TiKV 配置文件描述](/tikv-configuration-file.md)\n\n### 在线修改 PD 配置\n\nPD 暂不支持单个实例拥有独立配置。所有实例共享一份配置，可以通过下列方式修改 PD 的配置项：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset config pd `log.level`='info'\n```\n\n设置成功会返回 `Query OK`：\n\n```sql\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n针对 PD 可在线修改的配置项，成功修改后则会持久化到 etcd 中，不会对配置文件进行持久化，后续以 etcd 中的配置为准。同上，若和 TiDB 预留关键字冲突，需要用反引号 ``` ` ``` 包裹此类配置项，例如 ``` `schedule.leader-schedule-limit` ```。\n\n支持配置项列表如下：\n\n| 配置项 | 简介 |\n| --- | --- |\n| log.level| 日志级别 |\n| cluster-version | 集群的版本 |\n| schedule.max-merge-region-size |  控制 Region Merge 的 size 上限（单位是 MiB） |\n| schedule.max-merge-region-keys | 控制 Region Merge 的 key 数量上限 |\n| schedule.patrol-region-interval | 控制 checker 检查 Region 健康状态的运行频率 |\n| scheduler.patrol-region-worker-count| 控制 checker 检查 Region 健康状态时，创建 operator 的并发数 |\n| schedule.split-merge-interval | 控制对同一个 Region 做 split 和 merge 操作的间隔 |\n| schedule.max-snapshot-count | 控制单个 store 最多同时接收或发送的 snapshot 数量 |\n| schedule.max-pending-peer-count | 控制单个 store 的 pending peer 上限 |\n| schedule.max-store-down-time | PD 认为失联 store 无法恢复的时间 |\n| schedule.leader-schedule-policy | 用于控制 leader 调度的策略 |\n| schedule.leader-schedule-limit | 可以控制同时进行 leader 调度的任务个数 |\n| schedule.region-schedule-limit | 可以控制同时进行 Region 调度的任务个数 |\n| schedule.replica-schedule-limit | 可以控制同时进行 replica 调度的任务个数 |\n| schedule.merge-schedule-limit | 控制同时进行的 Region Merge 调度的任务 |\n| schedule.hot-region-schedule-limit | 可以控制同时进行的热点调度的任务个数 |\n| schedule.hot-region-cache-hits-threshold | 用于设置 Region 被视为热点的阈值 |\n| schedule.high-space-ratio | 用于设置 store 空间充裕的阈值 |\n| schedule.low-space-ratio | 用于设置 store 空间不足的阈值 |\n| schedule.tolerant-size-ratio | 控制 balance 缓冲区大小 |\n| schedule.enable-remove-down-replica | 用于开启自动删除 DownReplica 的特性 |\n| schedule.enable-replace-offline-replica | 用于开启迁移 OfflineReplica 的特性 |\n| schedule.enable-make-up-replica | 用于开启补充副本的特性 |\n| schedule.enable-remove-extra-replica | 用于开启删除多余副本的特性 |\n| schedule.enable-location-replacement | 用于开启隔离级别检查 |\n| schedule.enable-cross-table-merge | 用于开启跨表 Merge |\n| schedule.enable-one-way-merge | 用于开启单向 Merge（只允许和下一个相邻的 Region Merge） |\n| replication.max-replicas | 用于设置副本的数量 |\n| replication.location-labels | 用于设置 TiKV 集群的拓扑信息 |\n| replication.enable-placement-rules | 开启 Placement Rules |\n| replication.strictly-match-label | 开启 label 检查 |\n| pd-server.use-region-storage | 开启独立的 Region 存储 |\n| pd-server.max-gap-reset-ts | 用于设置最大的重置 timestamp 的间隔（BR）|\n| pd-server.key-type| 用于设置集群 key 的类型 |\n| pd-server.metric-storage | 用于设置集群 metrics 的存储地址 |\n| pd-server.dashboard-address | 用于设置 dashboard 的地址 |\n| replication-mode.replication-mode | 备份的模式 |\n\n具体配置项意义可参考 [PD 配置文件描述](/pd-configuration-file.md)。\n\n### 在线修改 TiDB 配置\n\n在线修改 TiDB 配置的方式和 TiKV/PD 有所不同，你可以通过修改[系统变量](/system-variables.md)来实现。\n\n下面例子展示了如何通过变量 `tidb_slow_log_threshold` 在线修改配置项 `slow-threshold`。\n\n`slow-threshold` 默认值是 300 毫秒，可以通过设置系统变量 `tidb_slow_log_threshold` 将其修改为 200 毫秒：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset tidb_slow_log_threshold = 200;\n```\n\n```sql\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect @@tidb_slow_log_threshold;\n```\n\n```sql\n+---------------------------+\n| @@tidb_slow_log_threshold |\n+---------------------------+\n| 200                       |\n+---------------------------+\n1 row in set (0.00 sec)\n```\n\n支持在线修改的配置项和相应的 TiDB 系统变量如下：\n\n| 配置项 | 对应变量 | 简介 |\n| --- | --- | --- |\n| instance.tidb_enable_slow_log | tidb_enable_slow_log | 慢日志的开关 |\n| instance.tidb_slow_log_threshold | tidb_slow_log_threshold | 慢日志阈值 |\n| instance.tidb_expensive_query_time_threshold  | tidb_expensive_query_time_threshold | expensive 查询阈值 |\n| instance.tidb_enable_collect_execution_info | tidb_enable_collect_execution_info | 控制是否记录各个算子的执行信息 |\n| instance.tidb_record_plan_in_slow_log | tidb_record_plan_in_slow_log | 控制是否在慢日志中记录执行计划 |\n| instance.tidb_force_priority | tidb_force_priority | 该 TiDB 实例的语句优先级 |\n| instance.max_connections | max_connections | 该 TiDB 实例同时允许的最大客户端连接数 |\n| instance.tidb_enable_ddl | tidb_enable_ddl | 控制该 TiDB 实例是否可以成为 DDL owner |\n| pessimistic-txn.constraint-check-in-place-pessimistic | tidb_constraint_check_in_place_pessimistic | 控制悲观事务中唯一约束检查是否会被推迟到下一次对该唯一索引加锁时或事务提交时才进行 |\n\n### 在线修改 TiFlash 配置\n\n目前，你可以通过修改系统变量 [`tidb_max_tiflash_threads`](/system-variables.md#tidb_max_tiflash_threads-从-v610-版本开始引入) 来在线修改 TiFlash 配置项 `max_threads`。`tidb_max_tiflash_threads` 表示 TiFlash 中 request 执行的最大并发度。\n\n`tidb_max_tiflash_threads` 默认值是 `-1`，表示此系统变量无效，由 TiFlash 的配置文件决定 max_threads。你可以通过设置系统变量 `tidb_max_tiflash_threads` 将其修改为 10：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset tidb_max_tiflash_threads = 10;\n```\n\n```sql\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect @@tidb_max_tiflash_threads;\n```\n\n```sql\n+----------------------------+\n| @@tidb_max_tiflash_threads |\n+----------------------------+\n| 10                         |\n+----------------------------+\n1 row in set (0.00 sec)\n```\n"
        },
        {
          "name": "ecosystem-tool-user-case.md",
          "type": "blob",
          "size": 2.3916015625,
          "content": "---\ntitle: TiDB 工具的使用场景\nsummary: 本文档介绍 TiDB 工具的常见使用场景与工具选择。\naliases: ['/docs-cn/dev/ecosystem-tool-user-case/']\n---\n\n# TiDB 工具的使用场景\n\n本文档从数据迁移工具的使用场景出发，介绍部分常见场景下的迁移工具的选择。\n\n## 在物理机或虚拟机上部署运维 TiDB\n\n当需要在物理机或虚拟机上部署运维 TiDB 时，你可以先安装 [TiUP](/tiup/tiup-overview.md)，再通过 TiUP 管理 TiDB 的众多组件，如 TiDB、PD、TiKV 等。\n\n## 在 Kubernetes 上部署运维 TiDB\n\n当需要在 Kubernetes 上部署运维 TiDB 时，你可以先创建 Kubernetes 集群，部署[TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable)，然后使用 TiDB Operator 部署运维 TiDB 集群。\n\n## 从 CSV 导入数据到 TiDB\n\n当需要将其他工具导出的格式兼容的 CSV files 导入到 TiDB 时，可使用 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md)。\n\n## 从 MySQL/Aurora 导入全量数据\n\n当需要从 MySQL/Aurora 导入全量数据时，可先使用 [Dumpling](/dumpling-overview.md) 将数据导出为 SQL dump files，然后再使用 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 将数据导入到 TiDB 集群。\n\n## 从 MySQL/Aurora 迁移数据\n\n当既需要从 MySQL/Aurora 导入全量数据，又需要迁移增量数据时，可使用 [TiDB Data Migration (DM)](/dm/dm-overview.md) 完成[从 Amazon Aurora 迁移数据到 TiDB](/migrate-aurora-to-tidb.md)。\n\n如果全量数据量较大（TB 级别），则可先使用 [Dumpling](/dumpling-overview.md) 与 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 完成全量数据的迁移，再使用 DM 完成增量数据的迁移。\n\n## TiDB 集群备份与恢复\n\n当需要对 TiDB 集群进行备份或在之后对 TiDB 集群进行恢复时，可使用 [BR](/br/backup-and-restore-overview.md)。\n\n## 迁出数据到 TiDB\n\n当需要将 TiDB 集群的数据迁出到其他 TiDB 集群时，可使用 [Dumpling](/dumpling-overview.md) 从 TiDB 将全量数据导出为 SQL dump files，然后再使用 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 将数据导入到 TiDB。\n\n如果还需要执行增量数据的迁移，则可使用 [TiCDC](/ticdc/ticdc-overview.md)。\n\n## TiDB 增量数据订阅\n\n当需要订阅 TiDB 增量数据的变更时，可使用 [TiCDC](/ticdc/ticdc-overview.md)。\n"
        },
        {
          "name": "ecosystem-tool-user-guide.md",
          "type": "blob",
          "size": 7.572265625,
          "content": "---\ntitle: TiDB 工具功能概览\naliases: ['/docs-cn/dev/ecosystem-tool-user-guide/','/docs-cn/dev/reference/tools/user-guide/','/docs-cn/dev/how-to/migrate/from-mysql/', '/docs-cn/dev/how-to/migrate/incrementally-from-mysql/', '/docs-cn/dev/how-to/migrate/overview/', '/docs-cn/dev/reference/tools/use-guide/']\nsummary: TiDB 提供了丰富的工具，包括部署运维工具 TiUP 和 TiDB Operator，数据管理工具如 TiDB Data Migration（DM）、Dumpling、TiDB Lightning、Backup & Restore（BR）、TiCDC、sync-diff-inspector，以及 OLAP 分析工具 TiSpark。这些工具可用于部署、数据迁移、备份恢复、数据校验等多种操作，满足不同需求。\n---\n\n# TiDB 工具功能概览\n\nTiDB 提供了丰富的工具，可以帮助你进行部署运维、数据管理（例如，数据迁移、备份恢复、数据校验）、在 TiKV 上运行 Spark SQL。请根据需要选择适用的工具。\n\n## 部署运维工具\n\nTiDB 提供了 TiUP 和 TiDB Operator 部署运维工具，满足你在不同系统环境下的部署运维需求。\n\n### 在物理机或虚拟机上部署运维 TiDB\n\n#### TiUP\n\n[TiUP](/tiup/tiup-overview.md) 是在物理机或虚拟机上的 TiDB 包管理器，管理着 TiDB 的众多的组件，如 TiDB、PD、TiKV 等。当你想要运行 TiDB 生态中任何组件时，只需要执行一行 TiUP 命令即可。\n\n[TiUP cluster](https://github.com/pingcap/tiup/tree/master/components/cluster) 是 TiUP 提供的使用 Golang 编写的集群管理组件，通过 TiUP cluster 组件就可以进行日常的运维工作，包括部署、启动、关闭、销毁、弹性扩缩容、升级 TiDB 集群，以及管理 TiDB 集群参数。\n\n基本信息：\n\n- [术语及核心概念](/tiup/tiup-terminology-and-concepts.md)\n- [使用 TiUP 部署 TiDB 集群](/production-deployment-using-tiup.md)\n- [TiUP 组件管理](/tiup/tiup-component-management.md)\n- 适用 TiDB 版本：v4.0 及以上\n\n### 在 Kubernetes 上部署运维 TiDB - TiDB Operator\n\n[TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable) 是 Kubernetes 上的 TiDB 集群自动运维系统，提供包括部署、升级、扩缩容、备份恢复、配置变更的 TiDB 全生命周期管理。借助 TiDB Operator，TiDB 可以无缝运行在公有云或自托管的 Kubernetes 集群上。\n\n基本信息：\n\n- [TiDB Operator 架构](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/architecture)\n- [在 Kubernetes 上部署运维 TiDB 快速上手](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/get-started/)\n- 适用 TiDB 版本：v2.1 及以上\n\n## 数据管理工具\n\n TiDB 提供了丰富的数据管理工具，例如数据迁移、导入导出、备份恢复、增量同步、数据校验等。\n\n### 数据迁入 - TiDB Data Migration (DM)\n\n[TiDB Data Migration (DM)](/dm/dm-overview.md) 是将 MySQL/MariaDB 数据迁移到 TiDB 的工具，支持全量数据的迁移和增量数据的复制。\n\n基本信息：\n\n- TiDB DM 的输入：MySQL/MariaDB\n- TiDB DM 的输出：TiDB 集群\n- 适用 TiDB 版本：所有版本\n- Kubernetes 支持：使用 [TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/dev/deploy-tidb-dm) 在 Kubernetes 上部署 TiDB DM。\n\n如果数据量在 TB 级别以下，推荐直接使用 TiDB DM 迁移 MySQL/MariaDB 数据到 TiDB（迁移的过程包括全量数据的导出导入和增量数据的复制）。\n\n如果数据量在 TB 级别，推荐的迁移步骤如下：\n\n1. 使用 [Dumpling](/dumpling-overview.md) 导出 MySQL/MariaDB 全量数据。\n2. 使用 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 将全量导出数据导入 TiDB 集群。\n3. 使用 TiDB DM 复制 MySQL/MariaDB 增量数据到 TiDB。\n\n> **注意：**\n>\n> - 原 Syncer 工具已停止维护，不再推荐使用，相关场景请使用 TiDB DM 的增量复制模式进行替代。\n\n### 全量导出 - Dumpling\n\n[Dumpling](/dumpling-overview.md) 是一个用于从 MySQL/TiDB 进行全量逻辑导出的工具。\n\n基本信息：\n\n- Dumpling 的输入：MySQL/TiDB 集群\n- Dumpling 的输出：SQL/CSV 文件\n- 适用 TiDB 版本：所有版本\n- Kubernetes 支持：尚未支持\n\n> **注意：**\n>\n> PingCAP 之前维护的 Mydumper 工具 fork 自 [mydumper project](https://github.com/maxbube/mydumper)，针对 TiDB 的特性进行了优化。从 v7.5.0 开始，[Mydumper](https://docs.pingcap.com/tidb/v4.0/mydumper-overview) 废弃，其绝大部分功能已经被 [Dumpling](/dumpling-overview.md) 取代，强烈建议切换到 Dumpling。\n\n### 全量导入 - TiDB Lightning\n\n[TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 是一个用于将全量数据导入到 TiDB 集群的工具。\n\n使用 TiDB Lightning 导入数据到 TiDB 时，有以下模式：\n\n- [物理导入模式](/tidb-lightning/tidb-lightning-physical-import-mode.md)：TiDB Lightning 将数据解析为有序的键值对，并直接将其导入 TiKV。这种模式一般用于导入大量的数据（TB 级别）到新集群，但在数据导入过程中集群无法提供正常的服务。\n- [逻辑导入模式](/tidb-lightning/tidb-lightning-logical-import-mode.md)：以 TiDB/MySQL 作为后端，这种模式相比物理导入模式，导入速度较慢，但是可以在线导入，同时也支持将数据导入到 MySQL。\n\n基本信息：\n\n- TiDB Lightning 的输入：\n    - Dumpling 输出文件\n    - 其他格式兼容的 CSV 文件\n    - 从 Aurora、Hive 或 Snowflake 导出的 Parquet 文件\n- 适用 TiDB 版本：v2.1 及以上\n- Kubernetes 支持：[使用 TiDB Lightning 快速恢复 Kubernetes 上的 TiDB 集群数据](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/restore-data-using-tidb-lightning)\n\n> **注意：**\n>\n> 原 Loader 工具已停止维护，不再推荐使用。相关场景请使用 TiDB Lightning 的 `tidb` 模式进行替代。\n\n### 备份和恢复 - Backup & Restore\n\n[Backup & Restore (BR)](/br/backup-and-restore-overview.md) 是一个对 TiDB 进行分布式备份和恢复的工具，可以高效地对大数据量的 TiDB 集群进行数据备份和恢复。\n\n基本信息：\n\n- [备份输出和恢复输入的文件类型](/br/backup-and-restore-design.md)\n- 适用 TiDB 版本：v4.0 及以上\n- Kubernetes 支持：[使用 BR 工具备份 TiDB 集群数据到兼容 S3 的存储](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/backup-to-aws-s3-using-br)，[使用 BR 工具恢复 S3 兼容存储上的备份数据](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/restore-from-aws-s3-using-br)\n\n### TiDB 增量数据同步 - TiCDC\n\n[TiCDC](/ticdc/ticdc-overview.md) 是一款通过拉取 TiKV 变更日志实现的 TiDB 增量数据同步工具，具有将数据还原到与上游任意 TSO 一致状态的能力，同时提供开放数据协议 (TiCDC Open Protocol)，支持其他系统订阅数据变更。\n\n基本信息：\n\n- TiCDC 的输入：TiDB 集群\n- TiCDC 的输出：TiDB 集群、MySQL、Kafka、Confluent\n- 适用 TiDB 版本：v4.0.6 及以上\n\n### 数据校验 - sync-diff-inspector\n\n[sync-diff-inspector](/sync-diff-inspector/sync-diff-inspector-overview.md) 是一个用于校验 MySQL/TiDB 中两份数据是否一致的工具。该工具还提供了修复数据的功能，可用于修复少量不一致的数据。\n\n基本信息：\n\n- sync-diff-inspector 的输入：TiDB、MySQL\n- sync-diff-inspector 的输出：TiDB、MySQL\n- 适用 TiDB 版本：所有版本\n\n## OLAP 分析工具 - TiSpark\n\n[TiSpark](/tispark-overview.md) 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品。它借助 Spark 平台，同时融合 TiKV 分布式集群的优势，和 TiDB 一起为用户一站式解决 HTAP (Hybrid Transactional/Analytical Processing) 的需求。\n"
        },
        {
          "name": "enable-disk-spill-encrypt.md",
          "type": "blob",
          "size": 1.0322265625,
          "content": "---\ntitle: 为 TiDB 落盘文件开启加密\nsummary: 了解如何为 TiDB 落盘文件开启加密。\n---\n\n# 为 TiDB 落盘文件开启加密\n\n当系统变量 [`tidb_enable_tmp_storage_on_oom`](/system-variables.md#tidb_enable_tmp_storage_on_oom) 为 `ON` 时，如果单条 SQL 语句的内存使用超出系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 的限制，某些算子可以将执行时的中间结果作为临时文件落盘保存，直到查询执行完成之后将它们删除。\n\n用户可以开启落盘文件加密功能，防止攻击者通过读取临时文件来访问数据。\n\n## 配置方法\n\n要启用落盘文件加密功能，可以在 TiDB 配置文件中的 `[security]` 部分，配置 [`spilled-file-encryption-method`](/tidb-configuration-file.md#spilled-file-encryption-method) 选项：\n\n```toml\n[security]\nspilled-file-encryption-method = \"aes128-ctr\"\n```\n\n`spilled-file-encryption-method` 的可选值为 `aes128-ctr` 和 `plaintext`。默认值为 `plaintext`，表示不启用加密。\n"
        },
        {
          "name": "enable-tls-between-clients-and-servers.md",
          "type": "blob",
          "size": 11.8486328125,
          "content": "---\ntitle: 为 TiDB 客户端服务端间通信开启加密传输\naliases: ['/docs-cn/dev/enable-tls-between-clients-and-servers/','/docs-cn/dev/how-to/secure/enable-tls-clients/','/docs-cn/dev/encrypted-connections-with-tls-protocols/','/docs-cn/dev/enable-tls-between-clients/']\nsummary: TiDB 服务端与客户端间默认采用非加密连接，容易造成信息泄露。建议使用加密连接确保安全性。要开启 TLS 加密传输，需要在服务端配置开启 TLS 支持，并在客户端应用程序中配置使用 TLS 加密连接。可以通过配置系统变量或在创建 / 修改用户时指定要求加密连接。可通过命令检查当前连接是否是加密连接。TLS 版本为 TLSv1.2 和 TLSv1.3，支持的加密算法包括 AES 和 CHACHA20_POLY1305。\n---\n\n# 为 TiDB 客户端服务端间通信开启加密传输\n\n默认情况下，TiDB 允许服务端与客户端之间的不安全连接，因而具备监视信道流量能力的第三方可以知悉 TiDB 服务端与客户端之间发送和接受的数据，包括但不限于查询语句内容、查询结果等。若信道是不可信的，例如客户端是通过公网连接到 TiDB 服务端的，则不安全连接容易造成信息泄露，建议使用 TLS 连接确保安全性。\n\nTiDB 服务端支持启用基于 TLS（传输层安全）协议的安全连接，协议与 MySQL 安全连接一致，现有 MySQL Client 如 MySQL Shell 和 MySQL 驱动等能直接支持。TLS 的前身是 SSL，因而 TLS 有时也被称为 SSL，但由于 SSL 协议有已知安全漏洞，TiDB 实际上并未支持。TiDB 支持的 TLS/SSL 协议版本为 TLSv1.2 和 TLSv1.3。\n\n使用 TLS 安全连接后，连接将具有以下安全性质：\n\n- 保密性：流量明文被加密，无法被窃听；\n- 完整性：流量明文无法被篡改；\n- 身份验证（可选）：客户端和服务端能验证双方身份，避免中间人攻击。\n\n要为 TiDB 客户端与服务端间的通信开启 TLS 安全传输，首先需要在 TiDB 服务端通过配置开启 TLS 加密连接的支持，然后通过配置客户端应用程序使用 TLS 加密连接。一般情况下，如果服务端正确配置了 TLS 加密连接支持，客户端库都会自动启用 TLS 安全传输。\n\n另外，与 MySQL 相同，TiDB 也支持在同一 TCP 端口上开启 TLS 连接或非 TLS 连接。对于开启了 TLS 连接支持的 TiDB 服务端，客户端既可以选择通过加密连接安全地连接到该 TiDB 服务端，也可以选择使用普通的非加密连接。如需使用加密连接，你可以通过以下方式进行配置：\n\n+ 通过配置系统变量 [`require_secure_transport`](/system-variables.md#require_secure_transport-从-v610-版本开始引入) 要求所有用户必须使用加密连接来连接到 TiDB。\n+ 通过在创建用户 (`create user`)，或修改已有用户 (`alter user`) 时指定 `REQUIRE SSL` 要求指定用户必须使用 TLS 连接来连接 TiDB。以创建用户为例：\n\n    ```sql\n    CREATE USER 'u1'@'%' IDENTIFIED BY 'my_random_password' REQUIRE SSL;\n    ```\n\n> **注意：**\n>\n> 如果登录用户已配置使用 [TiDB 证书鉴权功能](/certificate-authentication.md#配置登录时需要校验的用户证书信息)校验用户证书，也会隐式要求对应用户必须使用加密连接连接 TiDB。\n\n## 配置 TiDB 服务端启用安全连接\n\n要启用安全连接，请参考以下相关参数说明：\n\n- [`auto-tls`](/tidb-configuration-file.md#auto-tls)：启用证书自动生成功能（从 v5.2.0 开始）\n- [`ssl-cert`](/tidb-configuration-file.md#ssl-cert)：指定 SSL 证书文件路径\n- [`ssl-key`](/tidb-configuration-file.md#ssl-key)：指定证书文件对应的私钥\n- [`ssl-ca`](/tidb-configuration-file.md#ssl-ca)：可选，指定受信任的 CA 证书文件路径\n- [`tls-version`](/tidb-configuration-file.md#tls-version)：可选，指定最低 TLS 版本，例如 `TLSv1.2`\n\n`auto-tls` 支持安全连接，但不提供客户端证书验证。有关证书验证和控制证书生成方式的说明，请参考下面配置 `ssl-cert`、`ssl-key` 和 `ssl-ca` 变量的建议：\n\n- 在启动 TiDB 时，至少需要在配置文件中同时指定 `ssl-cert` 和 `ssl-key` 参数，才能在 TiDB 服务端开启安全连接。还可以指定 `ssl-ca` 参数进行客户端身份验证（请参见[配置启用身份验证](#配置启用身份验证)章节）。\n- 参数指定的文件都为 PEM 格式。另外目前 TiDB 尚不支持加载有密码保护的私钥，因此必须提供一个没有密码的私钥文件。若提供的证书或私钥无效，则 TiDB 服务端将照常启动，但并不支持客户端 TLS 连接到 TiDB 服务端。\n- 若证书参数无误，则 TiDB 在启动时将会输出 `mysql protocol server secure connection is enabled` 到 `\"INFO\"` 级别日志中。\n\n## 配置 MySQL Client 使用 TLS 连接\n\nMySQL 5.7 及以上版本自带的客户端默认尝试使用 TLS 连接，若服务端不支持安全连接则自动退回到使用非安全连接；MySQL 5.7 以下版本自带的客户端默认采用非 TLS 连接。\n\n可以通过命令行参数修改客户端的连接行为，包括：\n\n- 强制使用安全连接，若服务端不支持安全连接则连接失败 (`--ssl-mode=REQUIRED`)\n- 尝试使用安全连接，若服务端不支持安全连接则退回使用不安全连接\n- 使用不安全连接 (`--ssl-mode=DISABLED`)\n\n除此参数外，MySQL 8.x 客户端有两种 SSL 模式：\n\n- `--ssl-mode=VERIFY_CA`：根据 `--ssl-ca` 签发的 CA 验证来自服务器的证书。\n- `--ssl-mode=VERIFY_IDENTITY`：与 `VERIFY_CA` 相同，但也验证所连接的主机名是否与证书匹配。\n\n对于 MySQL 5.7 和 MariaDB 客户端及之前版本，可以使用 `--ssl-verify-server-cert` 参数启用对服务端证书的验证。\n\n详细信息请参阅 MySQL 文档中关于[客户端配置安全连接](https://dev.mysql.com/doc/refman/8.0/en/using-encrypted-connections.html#using-encrypted-connections-client-side-configuration)的部分。\n\n## 配置启用身份验证\n\n若在 TiDB 服务端或 MySQL Client 中未指定 `ssl-ca` 参数，则默认不会进行客户端或服务端身份验证，无法抵御中间人攻击，例如客户端可能会“安全地”连接到了一个伪装的服务端。可以在服务端和客户端中配置 `ssl-ca` 参数进行身份验证。一般情况下只需验证服务端身份，但也可以验证客户端身份进一步增强安全性。\n\n- 若要使 MySQL Client 验证 TiDB 服务端身份，TiDB 服务端需至少配置 `ssl-cert` 和 `ssl-key` 参数，客户端需至少指定 `--ssl-ca` 参数，且 `--ssl-mode` 至少为 `VERIFY_CA`。必须确保 TiDB 服务端配置的证书 (`ssl-cert`) 是由客户端 `--ssl-ca` 参数所指定的 CA 所签发的，否则身份验证失败。\n- 若要使 TiDB 服务端验证 MySQL Client 身份，TiDB 服务端需配置 `ssl-cert`、`ssl-key`、`ssl-ca` 参数，客户端需至少指定 `--ssl-cert`、`--ssl-key` 参数。必须确保服务端配置的证书和客户端配置的证书都是由服务端配置指定的 `ssl-ca` 签发的。\n- 若要进行双向身份验证，请同时满足上述要求。\n\n默认情况，服务端对客户端的身份验证是可选的。若客户端在 TLS 握手时未出示自己的身份证书，也能正常建立 TLS 连接。但也可以通过在创建用户 (`CREATE USER`) 或修改已有用户 (`ALTER USER`) 时指定 `REQUIRE x509` 要求客户端需进行身份验证，以创建用户为例：\n\n```sql\nCREATE USER 'u1'@'%'  REQUIRE X509;\n```\n\n> **注意：**\n>\n> 如果登录用户已配置使用 [TiDB 证书鉴权功能](/certificate-authentication.md#配置登录时需要校验的用户证书信息)校验用户证书，也会隐式要求对应用户需进行身份验证。\n\n## 检查当前连接是否是加密连接\n\n可以通过 `SHOW STATUS LIKE \"%Ssl%\";` 了解当前连接的详细情况，包括是否使用了安全连接、安全连接采用的加密协议、TLS 版本号等。\n\n以下是一个安全连接中执行该语句的结果。由于客户端支持的 TLS 版本号和加密协议会有所不同，执行结果相应地也会有所变化。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW STATUS LIKE \"Ssl%\";\n```\n\n```\n+-----------------------+------------------------------------------------------->\n| Variable_name         | Value                                                 >\n+-----------------------+------------------------------------------------------->\n| Ssl_cipher            | TLS_AES_128_GCM_SHA256                                >\n| Ssl_cipher_list       | RC4-SHA:DES-CBC3-SHA:AES128-SHA:AES256-SHA:AES128-SHA2>\n| Ssl_server_not_after  | Apr 23 07:59:47 2024 UTC                              >\n| Ssl_server_not_before | Jan 24 07:59:47 2024 UTC                              >\n| Ssl_verify_mode       | 5                                                     >\n| Ssl_version           | TLSv1.3                                               >\n+-----------------------+------------------------------------------------------->\n6 rows in set (0.0062 sec)\n```\n\n除此以外，对于 MySQL 自带客户端，还可以使用 `STATUS` 或 `\\s` 语句查看连接情况：\n\n{{< copyable \"sql\" >}}\n\n```sql\n\\s\n```\n\n```\n...\nSSL: Cipher in use is TLS_AES_128_GCM_SHA256\n...\n```\n\n## 支持的 TLS 版本及密钥交换协议和加密算法\n\nTiDB 支持的 TLS 版本及密钥交换协议和加密算法由 Go 官方库决定。\n\n你使用的客户端库和操作系统加密策略也可能会影响到支持的协议版本和密码套件。\n\n### 支持的 TLS 版本\n\n- TLSv1.2\n- TLSv1.3\n\n可以使用配置项 [`tls-version`](/tidb-configuration-file.md#tls-version) 来限制 TLS 版本。\n\n实际可用的 TLS 版本取决于操作系统的加密策略、MySQL 客户端版本和客户端的 SSL/TLS 库。\n\n### 支持的密钥交换协议及加密算法\n\n- TLS\\_RSA\\_WITH\\_AES\\_128\\_CBC\\_SHA\n- TLS\\_RSA\\_WITH\\_AES\\_256\\_CBC\\_SHA\n- TLS\\_RSA\\_WITH\\_AES\\_128\\_CBC\\_SHA256\n- TLS\\_RSA\\_WITH\\_AES\\_128\\_GCM\\_SHA256\n- TLS\\_RSA\\_WITH\\_AES\\_256\\_GCM\\_SHA384\n- TLS\\_ECDHE\\_ECDSA\\_WITH\\_AES\\_128\\_CBC\\_SHA\n- TLS\\_ECDHE\\_ECDSA\\_WITH\\_AES\\_256\\_CBC\\_SHA\n- TLS\\_ECDHE\\_RSA\\_WITH\\_AES\\_128\\_CBC\\_SHA\n- TLS\\_ECDHE\\_RSA\\_WITH\\_AES\\_256\\_CBC\\_SHA\n- TLS\\_ECDHE\\_ECDSA\\_WITH\\_AES\\_128\\_CBC\\_SHA256\n- TLS\\_ECDHE\\_RSA\\_WITH\\_AES\\_128\\_CBC\\_SHA256\n- TLS\\_ECDHE\\_RSA\\_WITH\\_AES\\_128\\_GCM\\_SHA256\n- TLS\\_ECDHE\\_ECDSA\\_WITH\\_AES\\_128\\_GCM\\_SHA256\n- TLS\\_ECDHE\\_RSA\\_WITH\\_AES\\_256\\_GCM\\_SHA384\n- TLS\\_ECDHE\\_ECDSA\\_WITH\\_AES\\_256\\_GCM\\_SHA384\n- TLS\\_ECDHE\\_RSA\\_WITH\\_CHACHA20\\_POLY1305\\_SHA256\n- TLS\\_ECDHE\\_ECDSA\\_WITH\\_CHACHA20\\_POLY1305\\_SHA256\n- TLS\\_AES\\_128\\_GCM\\_SHA256\n- TLS\\_AES\\_256\\_GCM\\_SHA384\n- TLS\\_CHACHA20\\_POLY1305\\_SHA256\n\n## 重加载证书、密钥和 CA\n\n在需要替换证书、密钥或 CA 时，可以在完成对应文件替换后，对运行中的 TiDB 实例执行 [`ALTER INSTANCE RELOAD TLS`](/sql-statements/sql-statement-alter-instance.md) 语句从原配置的证书 ([`ssl-cert`](/tidb-configuration-file.md#ssl-cert))、密钥 ([`ssl-key`](/tidb-configuration-file.md#ssl-key)) 和 CA ([`ssl-ca`](/tidb-configuration-file.md#ssl-ca)) 的路径重新加证书、密钥和 CA，而无需重启 TiDB 实例。\n\n新加载的证书密钥和 CA 将在语句执行成功后对新建立的连接生效，不会影响语句执行前已建立的连接。\n\n## 监控\n\n自 TiDB v5.2.0 版本起，你可以使用 `Ssl_server_not_after` 和 `Ssl_server_not_before` 状态变量监控证书有效期的起止时间。\n\n```sql\nSHOW GLOBAL STATUS LIKE 'Ssl\\_server\\_not\\_%';\n```\n\n```\n+-----------------------+--------------------------+\n| Variable_name         | Value                    |\n+-----------------------+--------------------------+\n| Ssl_server_not_after  | Nov 28 06:42:32 2021 UTC |\n| Ssl_server_not_before | Aug 30 06:42:32 2021 UTC |\n+-----------------------+--------------------------+\n2 rows in set (0.0076 sec)\n```\n\n## 另请参阅\n\n- [为 TiDB 组件间通信开启加密传输](/enable-tls-between-components.md)\n"
        },
        {
          "name": "enable-tls-between-components.md",
          "type": "blob",
          "size": 7.7939453125,
          "content": "---\ntitle: 为 TiDB 组件间通信开启加密传输\nsummary: 了解如何为 TiDB 集群内各组件间开启加密传输。\naliases: ['/docs-cn/dev/enable-tls-between-components/','/docs-cn/dev/how-to/secure/enable-tls-between-components/']\n---\n\n# 为 TiDB 组件间通信开启加密传输\n\n本部分介绍如何为 TiDB 集群内各组件间开启加密传输。一旦开启，以下组件间均将使用加密传输：\n\n- TiDB、TiKV、PD、TiFlash 之间的通讯\n- TiDB Control 与 TiDB，TiKV Control 与 TiKV，PD Control 与 PD\n- TiKV、PD、TiDB、TiFlash 各自集群内内部通讯\n\n目前暂不支持只开启其中部分组件的加密传输。\n\n## 配置开启加密传输\n\n1. 准备证书。\n\n    推荐为 TiDB、TiKV、PD 分别准备一个 Server 证书，并保证可以相互验证，而它们的 Control 工具则可选择共用 Client 证书。\n\n    有多种工具可以生成自签名证书，如 `openssl`，`easy-rsa`，`cfssl`。\n\n    这里提供一个使用 `openssl` 生成证书的示例：[生成自签名证书](/generate-self-signed-certificates.md)。\n\n2. 配置证书。\n\n    - TiDB\n\n        在 `config` 文件或命令行参数中设置：\n\n        ```toml\n        [security]\n        # Path of file that contains list of trusted SSL CAs for connection with cluster components.\n        cluster-ssl-ca = \"/path/to/ca.pem\"\n        # Path of file that contains X509 certificate in PEM format for connection with cluster components.\n        cluster-ssl-cert = \"/path/to/tidb-server.pem\"\n        # Path of file that contains X509 key in PEM format for connection with cluster components.\n        cluster-ssl-key = \"/path/to/tidb-server-key.pem\"\n        ```\n\n    - TiKV\n\n        在 `config` 文件或命令行参数中设置，并设置相应的 URL 为 https：\n\n        ```toml\n        [security]\n        # set the path for certificates. Empty string means disabling secure connectoins.\n        ca-path = \"/path/to/ca.pem\"\n        cert-path = \"/path/to/tikv-server.pem\"\n        key-path = \"/path/to/tikv-server-key.pem\"\n        ```\n\n    - PD\n\n        在 `config` 文件或命令行参数中设置，并设置相应的 URL 为 https：\n\n        ```toml\n        [security]\n        # Path of file that contains list of trusted SSL CAs. if set, following four settings shouldn't be empty\n        cacert-path = \"/path/to/ca.pem\"\n        # Path of file that contains X509 certificate in PEM format.\n        cert-path = \"/path/to/pd-server.pem\"\n        # Path of file that contains X509 key in PEM format.\n        key-path = \"/path/to/pd-server-key.pem\"\n        ```\n\n    - TiFlash（从 v4.0.5 版本开始引入）\n\n        在 `tiflash.toml` 文件中设置，将 `http_port` 项改为 `https_port`:\n\n        ```toml\n        [security]\n        # Path of file that contains list of trusted SSL CAs. if set, following four settings shouldn't be empty\n        ca_path = \"/path/to/ca.pem\"\n        # Path of file that contains X509 certificate in PEM format.\n        cert_path = \"/path/to/tiflash-server.pem\"\n        # Path of file that contains X509 key in PEM format.\n        key_path = \"/path/to/tiflash-server-key.pem\"\n        ```\n\n        在 `tiflash-learner.toml` 文件中设置，\n\n        ```toml\n        [security]\n        # Sets the path for certificates. The empty string means that secure connections are disabled.\n        ca-path = \"/path/to/ca.pem\"\n        cert-path = \"/path/to/tiflash-server.pem\"\n        key-path = \"/path/to/tiflash-server-key.pem\"\n        ```\n\n    - TiCDC\n\n        在 `config` 文件中设置\n\n        ```toml\n        [security]\n        ca-path = \"/path/to/ca.pem\"\n        cert-path = \"/path/to/cdc-server.pem\"\n        key-path = \"/path/to/cdc-server-key.pem\"\n        ```\n\n        或者在启动命令行中设置，并设置相应的 URL 为 `https`：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```bash\n        cdc server --pd=https://127.0.0.1:2379 --log-file=ticdc.log --addr=0.0.0.0:8301 --advertise-addr=127.0.0.1:8301 --ca=/path/to/ca.pem --cert=/path/to/ticdc-cert.pem --key=/path/to/ticdc-key.pem\n        ```\n\n    此时 TiDB 集群各个组件间已开启加密传输。\n\n    > **注意：**\n    >\n    > 若 TiDB 集群各个组件间开启加密传输后，在使用 tidb-ctl、tikv-ctl 或 pd-ctl 工具连接集群时，需要指定 client 证书，示例：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    ./tidb-ctl -u https://127.0.0.1:10080 --ca /path/to/ca.pem --ssl-cert /path/to/client.pem --ssl-key /path/to/client-key.pem\n    ```\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    tiup ctl:v<CLUSTER_VERSION> pd -u https://127.0.0.1:2379 --cacert /path/to/ca.pem --cert /path/to/client.pem --key /path/to/client-key.pem\n    ```\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    ./tikv-ctl --host=\"127.0.0.1:20160\" --ca-path=\"/path/to/ca.pem\" --cert-path=\"/path/to/client.pem\" --key-path=\"/path/to/clinet-key.pem\"\n    ```\n\n## 认证组件调用者身份\n\n通常被调用者除了校验调用者提供的密钥、证书和 CA 有效性外，还需要通过 `Common Name` 校验调用方身份以防止拥有有效证书的非法访问者进行访问（例如：TiKV 只能被 TiDB 访问，需阻止拥有合法证书但非 TiDB 的其他访问者访问 TiKV）。\n\n如希望对组件调用方进行身份认证，需要在生成证书时通过 `Common Name` 标识证书调用方身份，并在被调用者的配置文件中配置 `cluster-verify-cn` (TiDB 组件）或 `cert-allowed-cn`（其它组件）来检查调用方身份。\n\n> **注意：**\n>\n> - 从 v8.4.0 起，PD 的 `cert-allowed-cn` 配置项支持设置多个值。你可以根据需要在 TiDB 的 `cluster-verify-cn` 配置项以及其它组件的 `cert-allowed-cn` 配置项中设置多个 `Common Name`。需要额外注意的是，TiUP 在查询组件状态的时候会使用独立的标识，比如集群名是 `test`，它会使用 `test-client` 作为 `Common Name`。\n> - 对于 v8.3.0 及之前版本，PD 的 `cert-allowed-cn` 配置项只能设置一个值。因此，所有认证对象的 `Common Name` 必须设置成同一个值。相关配置示例可参见 [v8.3.0 文档](https://docs.pingcap.com/zh/tidb/v8.3/enable-tls-between-components)。\n\n- TiDB\n\n    在 `config` 文件或命令行参数中设置：\n\n    ```toml\n    [security]\n    cluster-verify-cn = [\"tidb\", \"test-client\", \"prometheus\"]\n    ```\n\n- TiKV\n\n    在 `config` 文件或命令行参数中设置：\n\n    ```toml\n    [security]\n    cert-allowed-cn = [\"tidb\", \"pd\", \"tikv\", \"tiflash\", \"prometheus\"]\n    ```\n\n- PD\n\n    在 `config` 文件或命令行参数中设置：\n\n    ```toml\n    [security]\n    cert-allowed-cn = [\"tidb\", \"pd\", \"tikv\", \"tiflash\", \"test-client\", \"prometheus\"]\n    ```\n\n- TiFlash（从 v4.0.5 版本开始引入）\n\n    在 `tiflash.toml` 文件中设置：\n\n    ```toml\n    [security]\n    cert_allowed_cn = [\"tidb\", \"tikv\", \"prometheus\"]\n    ```\n\n    在 `tiflash-learner.toml` 文件中设置：\n\n    ```toml\n    [security]\n    cert-allowed-cn = [\"tidb\", \"tikv\", \"tiflash\", \"prometheus\"]\n    ```\n\n## 证书重新加载\n\n- 如果 TiDB 集群部署在本地的数据中心，TiDB、PD、TiKV、TiFlash、TiCDC 和各种 client 在每次新建相互通讯的连接时都会重新读取当前的证书和密钥文件内容，实现证书和密钥的重新加载，无需重启 TiDB 集群。\n- 如果 TiDB 集群部署在自己管理的 Cloud，TLS 证书的签发需要与云服务商的证书管理服务集成，TiDB、PD、TiKV、TiFlash、TiCDC 组件的 TLS 证书支持自动轮换，无需重启 TiDB 集群。\n\n## 证书有效期\n\n你可以自定义 TiDB 集群中各组件 TLS 证书的有效期。例如，使用 OpenSSL 签发生成 TLS 证书时，可以通过 **days** 参数设置有效期，详见[生成自签名证书](/generate-self-signed-certificates.md)。\n\n## 另请参阅\n\n- [为 TiDB 客户端服务端间通信开启加密传输](/enable-tls-between-clients-and-servers.md)\n"
        },
        {
          "name": "encryption-at-rest.md",
          "type": "blob",
          "size": 26.576171875,
          "content": "---\ntitle: 静态加密\nsummary: 了解如何启用静态加密功能保护敏感数据。\naliases: ['/docs-cn/dev/encryption-at-rest/']\n---\n\n# 静态加密\n\n> **注意：**\n>\n> 如果集群部署在 AWS 上并在使用 EBS (Elastic Block Store) 存储数据，建议使用 EBS 加密，详细信息请参考 [AWS 文档 - EBS 加密](https://docs.aws.amazon.com/zh_cn/AWSEC2/latest/UserGuide/EBSEncryption.html)。如果集群部署在 AWS 上，但未使用 EBS 存储（例如使用本地 NVMe 存储），则建议使用本文中介绍的静态加密方式。\n\n静态加密 (encryption at rest) 即在存储数据时进行数据加密。对于数据库，静态加密功能也叫透明数据加密 (TDE)，区别于传输数据加密 (TLS) 或使用数据加密（很少使用）。SSD 驱动器、文件系统、云供应商等都可进行静态加密。但区别于这些加密方式，若 TiKV 在存储数据前就进行数据加密，攻击者则必须通过数据库的身份验证才能访问数据。例如，即使攻击者获得物理机的访问权限时，也无法通过复制磁盘上的文件来访问数据。\n\n## 各 TiDB 组件支持的加密方式\n\n在一个 TiDB 集群中，不同的组件使用不同的加密方式。本节介绍 TiKV、 TiFlash、PD 和 Backup & Restore (BR) 等不同 TiDB 组件支持的加密方式。\n\n部署好一个 TiDB 集群后，大部分用户数据会存储在 TiKV 和 TiFlash 节点上。此外，一些元数据会存储在 PD 节点上，例如用作为 TiKV Region 边界的二级索引密钥。为了能够充分发挥静态加密的优势，所有组件都需要启用加密功能。此外，进行加密时，也需要对备份、日志文件和通过网络传输的数据进行加密。\n\n### TiKV\n\nTiKV 支持静态加密，即在 [CTR](https://zh.wikipedia.org/wiki/分组密码工作模式) 模式下使用 [AES](https://zh.wikipedia.org/wiki/高级加密标准) 或 [SM4](https://zh.wikipedia.org/wiki/SM4) 对数据文件进行透明加密。要启用静态加密，用户需提供一个加密密钥，即主密钥。TiKV 自动轮换 (rotate) 用于加密实际数据文件的密钥，主密钥则可以由用户手动轮换。请注意，静态加密仅加密静态数据（即磁盘上的数据），而不加密网络传输中的数据。建议 TLS 与静态加密一起使用。\n\n可以选择将 KMS (Key Management Service) 用于云上部署或本地部署，也可以指定将密钥以明文形式存储在文件中。\n\nTiKV 当前不从核心转储 (core dumps) 中排除加密密钥和用户数据。建议在使用静态加密时禁用 TiKV 进程的核心转储，该功能目前无法由 TiKV 独立处理。\n\nTiKV 使用文件的绝对路径来跟踪已加密的数据文件。一旦 TiKV 节点开启了加密功能，用户就不应更改数据文件的路径配置，例如 `storage.data-dir`，`raftstore.raftdb-path`，`rocksdb.wal-dir` 和 `raftdb.wal-dir`。\n\nSM4 加密只在 v6.3.0 及之后版本的 TiKV 上支持。v6.3.0 之前的 TiKV 仅支持 AES 加密。SM4 加密会对吞吐造成 50% 到 80% 的回退。\n\n### TiFlash\n\nTiFlash 支持静态加密。数据密钥由 TiFlash 生成。TiFlash（包括 TiFlash Proxy）写入的所有文件，包括数据文件、Schema 文件、临时文件等，均由当前数据密钥加密。TiFlash 支持的加密算法、加密配置方法（配置项在 [`tiflash-learner.toml`](/tiflash/tiflash-configuration.md#配置文件-tiflash-learnertoml) 中）和监控项含义等均与 TiKV 一致。\n\n如果 TiFlash 中部署了 Grafana 组件，可以查看 **TiFlash-Proxy-Details** -> **Encryption**。\n\nSM4 加密只在 v6.4.0 及之后版本的 TiFlash 上支持。v6.4.0 之前的 TiFlash 仅支持 AES 加密。\n\n### PD\n\nPD 的静态加密为实验特性，其配置方式与 TiKV 相同。\n\n### BR 备份\n\nBR 支持对备份到 S3 的数据进行 S3 服务端加密 (SSE)。BR S3 服务端加密也支持使用用户自行创建的 AWS KMS 密钥进行加密，详细信息请参考 [BR S3 服务端加密](/encryption-at-rest.md#br-s3-服务端加密)。\n\n### 日志\n\nTiKV，TiDB 和 PD 信息日志中可能包含用于调试的用户数据。信息日志不会被加密，建议开启[日志脱敏](/log-redaction.md)功能。\n\n## TiKV 静态加密\n\nTiKV 当前支持的加密算法包括 AES128-CTR、AES192-CTR、AES256-CTR 和 SM4-CTR（仅 v6.3.0 及之后版本）。TiKV 使用信封加密 (envelop encryption)，所以启用加密后，TiKV 使用以下两种类型的密钥：\n\n* 主密钥 (master key)：主密钥由用户提供，用于加密 TiKV 生成的数据密钥。用户在 TiKV 外部进行主密钥的管理。\n* 数据密钥 (data key)：数据密钥由 TiKV 生成，是实际用于加密的密钥。\n\n多个 TiKV 实例可共用一个主密钥。在生产环境中，推荐通过 KMS 提供主密钥。目前 TiKV 支持 [AWS](https://docs.aws.amazon.com/zh_cn/kms/index.html)、[Google Cloud](https://cloud.google.com/security/products/security-key-management?hl=zh-CN) 和 [Azure](https://learn.microsoft.com/zh-cn/azure/key-vault/) 平台的 KMS 加密。要开启 KMS 加密，首先通过 KMS 创建用户主密钥 (CMK)，然后在配置文件中将 CMK 密钥的 ID 提供给 TiKV。如果 TiKV 无法访问 KMS CMK，TiKV 就无法启动或重新启动。\n\n用户也可以通过文件形式提供主密钥。该文件须包含一个用十六进制字符串编码的 256 位（32 字节）密钥，并以换行符结尾（即 `\\n`），且不包含其他任何内容。将密钥存储在磁盘上会泄漏密钥，因此密钥文件仅适合存储在 RAM 内存的 `tempfs` 中。\n\n数据密钥由 TiKV 传递给底层存储引擎（即 RocksDB）。RocksDB 写入的所有文件，包括 SST 文件，WAL 文件和 MANIFEST 文件，均由当前数据密钥加密。TiKV 使用的其他临时文件（可能包括用户数据）也由相同的数据密钥加密。默认情况下，TiKV 每周自动轮换数据密钥，但是该时间段是可配置的。密钥轮换时，TiKV 不会重写全部现有文件来替换密钥，但如果集群的写入量恒定，则 RocksDB compaction 会将使用最新的数据密钥对数据重新加密。TiKV 跟踪密钥和加密方法，并使用密钥信息对读取的内容进行解密。\n\n无论用户配置了哪种数据加密方法，数据密钥都使用 AES256-GCM 算法进行加密，以方便对主密钥进行验证。所以当使用文件而不是 KMS 方式指定主密钥时，主密钥必须为 256 位（32 字节）。\n\n### 配置加密\n\n要启用加密，你可以在 TiKV 和 PD 的配置文件中添加加密部分：\n\n```\n[security.encryption]\ndata-encryption-method = \"aes128-ctr\"\ndata-key-rotation-period = \"168h\" # 7 days\n```\n\n- `data-encryption-method` 用于指定加密算法，可选值为 `\"aes128-ctr\"`、`\"aes192-ctr\"`、`\"aes256-ctr\"`、`\"sm4-ctr\"`（仅 v6.3.0 及之后版本）、`\"plaintext\"`。默认值为 `\"plaintext\"`，即默认不开启加密功能。\n\n    - 对于新 TiKV 集群或现有 TiKV 集群，只有启用加密功能后写入的数据才保证被加密。\n    - 开启加密功能后，如需禁用加密，请在配置文件中删除 `data-encryption-method`，或将该参数值设置为 `\"plaintext\"`，然后重启 TiKV。\n    - 若要替换加密算法，请将 `data-encryption-method` 替换成已支持的加密算法，然后重启 TiKV。替换加密算法后，旧加密算法生成的加密文件会随着新数据的写入逐渐被重写成新加密算法所生成的加密文件。\n\n- `data-key-rotation-period` 用于指定 TiKV 轮换密钥的频率。\n\n如果启用了加密（即 `data-encryption-method` 的值不是 `\"plaintext\"`），则必须指定主密钥。你可以通过以下方式之一来指定主密钥：\n\n- [通过 KMS 指定主密钥](#通过-kms-指定主密钥)\n- [通过文件指定主密钥](#通过文件指定主密钥)\n\n#### 通过 KMS 指定主密钥\n\nTiKV 支持 AWS、Google Cloud 和 Azure 这三个平台的 KMS 加密。你可以根据服务部署的平台，选择其中之一配置 KMS 加密。\n\n<SimpleTab>\n\n<div label=\"AWS KMS\">\n\n**第 1 步：创建主密钥**\n\n在 AWS 上创建一个密钥，请进行以下操作：\n\n1. 进入 AWS 控制台的 [AWS KMS](https://console.aws.amazon.com/kms)。\n2. 确保在控制台的右上角选择正确的区域。\n3. 点击**创建密钥**，选择**对称** (Symmetric) 作为密钥类型。\n4. 为钥匙设置一个别名。\n\n你也可以使用 AWS CLI 执行该操作：\n\n```shell\naws --region us-west-2 kms create-key\naws --region us-west-2 kms create-alias --alias-name \"alias/tidb-tde\" --target-key-id 0987dcba-09fe-87dc-65ba-ab0987654321\n```\n\n需要在第二条命令中输入的 `--target-key-id` 是第一条命令的结果。\n\n**第 2 步：配置主密钥**\n\n要使用 AWS KMS 方式指定主密钥，请在 TiKV 的配置文件中 `[security.encryption]` 部分之后添加 `[security.encryption.master-key]` 配置：\n\n```\n[security.encryption.master-key]\ntype = \"kms\"\nkey-id = \"0987dcba-09fe-87dc-65ba-ab0987654321\"\nregion = \"us-west-2\"\nendpoint = \"https://kms.us-west-2.amazonaws.com\"\n```\n\n`key-id` 指定 KMS CMK 的密钥 ID。`region` 为 KMS CMK 的 AWS 区域名。`endpoint` 通常无需指定，除非你在使用非 AWS 提供的 AWS KMS 兼容服务或需要使用 [KMS VPC endpoint](https://docs.aws.amazon.com/kms/latest/developerguide/kms-vpc-endpoint.html)。\n\n你也可以使用 AWS [多区域键](https://docs.aws.amazon.com/zh_cn/kms/latest/developerguide/multi-region-keys-overview.html)。为此，你需要在一个特定的区域设置一个主键，并在需要的区域中添加副本密钥。\n\n</div>\n<div label=\"Google Cloud KMS\">\n\n**第 1 步：创建主密钥**\n\n要在 Google Cloud 平台上创建一个密钥，请进行以下操作：\n\n1. 进入 Google Cloud 控制台的[密钥管理](https://console.cloud.google.com/security/kms/keyrings)。\n2. 点击**创建密钥环**。输入密钥环的名称，选择密钥环的位置，然后点击**创建**。注意密钥环的位置需要覆盖 TiDB 集群部署的区域。\n3. 选择上一步创建的密钥环，在密钥环详情页面点击**创建密钥**。\n4. 输入密钥的名称，设置密钥的信息如下，然后点击**创建**。\n\n    - **保护级别**：**软件**或 **HSM**\n    - **密钥材料**：**生成的密钥**\n    - **用途**：**Symmetric encrypt/decrypt**\n\n你也可以使用 gcloud CLI 执行该操作：\n\n```shell\ngcloud kms keyrings create \"key-ring-name\" --location \"global\"\ngcloud kms keys create \"key-name\" --keyring \"key-ring-name\" --location \"global\" --purpose \"encryption\" --rotation-period \"30d\"\n```\n\n请将上述命令中的 `\"key-ring-name\"`、`\"key-name\"`、`\"global\"`、`\"30d\"` 字段的值替换为实际密钥对应的名称和配置。\n\n**第 2 步：配置主密钥**\n\n要使用 Google Cloud KMS 方式指定主密钥，请在 `[security.encryption]` 部分之后添加 `[security.encryption.master-key]` 配置：\n\n```\n[security.encryption.master-key]\ntype = \"kms\"\nkey-id = \"projects/project-name/locations/global/keyRings/key-ring-name/cryptoKeys/key-name\"\nvendor = \"gcp\"\n\n[security.encryption.master-key.gcp]\ncredential-file-path = \"/path/to/credential.json\"\n```\n\n- `key-id` 指定 KMS CMK 的密钥 ID。\n- `credential-file-path` 指向验证凭据配置文件的路径，目前支持 Service Account 和 Authentication User 这两种凭据。如果 TiKV 的运行环境已配置[应用默认凭据](https://cloud.google.com/docs/authentication/application-default-credentials?hl=zh-cn)，则无需配置 `credential-file-path`。\n\n</div>\n<div label=\"Azure KMS\">\n\n**第 1 步：创建主密钥**\n\n在 Azure 平台创建密钥，请参考文档[使用 Azure 门户在 Azure Key Vault 中设置和检索密钥](https://learn.microsoft.com/zh-cn/azure/key-vault/keys/quick-create-portal)。\n\n**第 2 步：配置主密钥**\n\n要使用 Azure KMS 方式指定主密钥，请在 TiKV 的配置文件中 `[security.encryption]` 部分之后添加 `[security.encryption.master-key]` 配置：\n\n```\n[security.encryption.master-key]\ntype = 'kms'\nkey-id = 'your-kms-key-id'\nregion = 'region-name'\nendpoint = 'endpoint'\nvendor = 'azure'\n\n[security.encryption.master-key.azure]\ntenant-id = 'tenant_id'\nclient-id = 'client_id'\nkeyvault-url = 'keyvault_url'\nhsm-name = 'hsm_name'\nhsm-url = 'hsm_url'\n# 如下 4 个参数为可选字段，用于设置 client 认证的凭证，请根据实际的使用场景进行设置\nclient_certificate = \"\"\nclient_certificate_path = \"\"\nclient_certificate_password = \"\"\nclient_secret = \"\"\n```\n\n请将上述配置中除 `vendor` 之外的其他字段值修改为密钥实际的对应配置。\n\n</div>\n</SimpleTab>\n\n#### 通过文件指定主密钥\n\n若要使用文件方式指定主密钥，主密钥配置应如下所示：\n\n```\n[security.encryption.master-key]\ntype = \"file\"\npath = \"/path/to/key/file\"\n```\n\n以上示例中，`path` 为密钥文件的路径。该文件须包含一个 256 位（32 字节）的十六进制字符串，并以换行符结尾（即 `\\n`），且不包含其他任何内容。密钥文件示例如下：\n\n```\n3b5896b5be691006e0f71c3040a29495ddcad20b14aff61806940ebd780d3c62\n```\n\n### 轮换主密钥\n\n要轮换主密钥，你必须在配置中同时指定新的主密钥和旧的主密钥，然后重启 TiKV。用 `security.encryption.master-key` 指定新的主密钥，用 `security.encryption.previous-master-key` 指定旧的主密钥。`security.encryption.previous-master-key` 配置的格式与 `encryption.master-key` 相同。重启时，TiKV 必须能同时访问旧主密钥和新主密钥，但一旦 TiKV 启动并运行，就只需访问新密钥。此后，可以将 `encryption.previous-master-key` 项保留在配置文件中。即使重启时，TiKV 也只会在无法使用新的主密钥解密现有数据时尝试使用旧密钥。\n\nTiKV 当前不支持在线轮换主密钥，因此你需要重启 TiKV 进行主密钥轮换。建议对运行中的、提供在线查询的 TiKV 集群进行滚动重启。\n\n轮换 AWS KMS CMK 的配置示例如下：\n\n```\n[security.encryption.master-key]\ntype = \"kms\"\nkey-id = \"50a0c603-1c6f-11e6-bb9e-3fadde80ce75\"\nregion = \"us-west-2\"\n\n[security.encryption.previous-master-key]\ntype = \"kms\"\nkey-id = \"0987dcba-09fe-87dc-65ba-ab0987654321\"\nregion = \"us-west-2\"\n```\n\n### 监控和调试\n\n要监控静态加密（如果 TiKV 中部署了 Grafana 组件），可以查看 **TiKV-Details** -> **Encryption** 面板中的监控项：\n\n* Encryption initialized：如果在 TiKV 启动期间初始化了加密，则为 `1`，否则为 `0`。进行主密钥轮换时可通过该监控项确认主密钥轮换是否已完成。\n* Encryption data keys：现有数据密钥的数量。每次轮换数据密钥后，该数字都会增加 `1`。通过此监控指标可以监测数据密钥是否按预期轮换。\n* Encrypted files：当前的加密数据文件数量。为先前未加密的集群启用加密时，将此数量与数据目录中的当前数据文件进行比较，可通过此监控指标估计已经被加密的数据量。\n* Encryption meta file size：加密元数据文件的大小。\n* Read/Write encryption meta duration：对用于加密的元数据进行操作带来的额外开销。\n\n在调试方面，可使用 `tikv-ctl` 命令查看加密元数据（例如使用的加密方法和数据密钥列表）。该操作可能会暴露密钥，因此不推荐在生产环境中使用。详情参阅 [TiKV Control](/tikv-control.md#打印加密元数据)。\n\n### TiKV 版本间兼容性\n\n为了减少 TiKV 管理加密元数据造成的 I/O 操作和互斥锁竞争引发的开销，TiKV v4.0.9 对此进行了优化。此优化可以通过 TiKV 配置文件中的 `security.encryption.enable-file-dictionary-log` 参数启用或关闭。此配置参数仅在 TiKV v4.0.9 或更高版本中生效。\n\n该配置项默认开启，此时 TiKV v4.0.8 或更早期的版本无法识别加密元数据的数据格式。例如，假设你正在使用 TiKV v4.0.9 或更高版本，开启了静态加密和默认开启了 `enable-file-dictionary-log` 配置。如果将集群降级到 TiKV v4.0.8 或更早版本，TiKV 将无法启动，并且信息日志中会有类似如下的报错：\n\n```\n[2020/12/07 07:26:31.106 +08:00] [ERROR] [mod.rs:110] [\"encryption: failed to load file dictionary.\"]\n[2020/12/07 07:26:33.598 +08:00] [FATAL] [lib.rs:483] [\"called `Result::unwrap()` on an `Err` value: Other(\\\"[components/encryption/src/encrypted_file/header.rs:18]: unknown version 2\\\")\"]\n```\n\n为了避免上面所示错误，你可以首先将 `security.encryption.enable-file-dictionary-log` 设置为 `false`，然后启动 TiKV v4.0.9 或更高版本。TiKV 成功启动后，加密元数据的数据格式将降级为 TiKV 早期版本可以识别的格式。此时，你可再将 TiKV 集群降级到较早的版本。\n\n## TiFlash 静态加密\n\nTiFlash 当前支持的加密算法与 TiKV 一致，包括 AES128-CTR、AES192-CTR、AES256-CTR 和 SM4-CTR（仅 v6.4.0 及之后版本）。TiFlash 同样使用信封加密 (envelop encryption)，所以启用加密后，TiFlash 使用以下两种类型的密钥：\n\n* 主密钥 (master key)：主密钥由用户提供，用于加密 TiFlash 生成的数据密钥。用户在 TiFlash 外部进行主密钥的管理。\n* 数据密钥 (data key)：数据密钥由 TiFlash 生成，是实际用于加密的密钥。\n\n多个 TiFlash 实例可共用一个主密钥，并且也可以和 TiKV 共用一个主密钥。在生产环境中，推荐通过 AWS KMS 提供主密钥。另外，你也可以通过文件形式提供主密钥。具体的主密钥生成方式和格式均与 TiKV 相同。\n\nTiFlash 使用数据密钥加密所有落盘的数据文件，包括数据文件、Schema 文件和计算过程中产生的临时数据文件等。默认情况下，TiFlash 每周自动轮换数据密钥，该轮换周期也可根据需要自定义配置。密钥轮换时，TiFlash 不会重写全部现有文件来替换密钥，但如果集群的写入量恒定，则后台 compaction 任务将会用最新的数据密钥对数据重新加密。TiFlash 跟踪密钥和加密方法，并使用密钥信息对读取的内容进行解密。\n\n### 创建密钥\n\n如需在 AWS 上创建一个密钥，请参考 TiKV 密钥的创建方式。\n\n### 配置加密\n\n启用加密的配置，可以在 `tiflash-learner.toml` 文件中添加以下内容：\n\n```\n[security.encryption]\ndata-encryption-method = \"aes128-ctr\"\ndata-key-rotation-period = \"168h\" # 7 days\n```\n\n或者，在 TiUP 集群模板文件中添加以下内容：\n\n```\nserver_configs:\n  tiflash-learner:\n    security.encryption.data-encryption-method: \"aes128-ctr\"\n    security.encryption.data-key-rotation-period: \"168h\" # 7 days\n```\n\n`data-encryption-method` 的可选值为 `\"aes128-ctr\"`、`\"aes192-ctr\"`、`\"aes256-ctr\"`、`\"sm4-ctr\"` (仅 v6.4.0 及之后版本) 和 `\"plaintext\"`。默认值为 `\"plaintext\"`，即默认不开启加密功能。`data-key-rotation-period` 指定 TiFlash 轮换密钥的频率。可以为新 TiFlash 集群或现有 TiFlash 集群开启加密，但只有启用后写入的数据才保证被加密。要禁用加密，请在配置文件中删除 `data-encryption-method`，或将该参数值为 `\"plaintext\"`，然后重启 TiFlash。若要替换加密算法，则将 `data-encryption-method` 替换成已支持的加密算法，然后重启 TiFlash。替换加密算法后，旧加密算法生成的加密文件会随着新数据的写入逐渐被重写成新加密算法所生成的加密文件。\n\n如果启用了加密（即 `data-encryption-method` 的值不是 `\"plaintext\"`），则必须指定主密钥。要使用 AWS KMS 方式指定为主密钥，配置方式如下：\n\n在 `tiflash-learner.toml` 配置文件的 `[security.encryption]` 部分之后添加 `[security.encryption.master-key]`：\n\n```\n[security.encryption.master-key]\ntype = \"kms\"\nkey-id = \"0987dcba-09fe-87dc-65ba-ab0987654321\"\nregion = \"us-west-2\"\nendpoint = \"https://kms.us-west-2.amazonaws.com\"\n```\n\n或者，在 TiUP 集群模板文件中添加以下内容：\n\n```\nserver_configs:\n  tiflash-learner:\n    security.encryption.master-key.type: \"kms\"\n    security.encryption.master-key.key-id: \"0987dcba-09fe-87dc-65ba-ab0987654321\"\n    security.encryption.master-key.region: \"us-west-2\"\n    security.encryption.master-key.endpoint: \"https://kms.us-west-2.amazonaws.com\"\n```\n\n上述配置项的含义与 TiKV 均相同。\n\n若要使用文件方式指定主密钥，可以在 `tiflash-learner.toml` 配置文件中添加以下内容：\n\n```\n[security.encryption.master-key]\ntype = \"file\"\npath = \"/path/to/key/file\"\n```\n\n或者，在 TiUP 集群模板文件中添加以下内容：\n\n```\nserver_configs:\n  tiflash-learner:\n    security.encryption.master-key.type: \"file\"\n    security.encryption.master-key.path: \"/path/to/key/file\"\n```\n\n上述配置项的含义以及密钥文件的内容格式与 TiKV 均相同。\n\n### 轮换主密钥\n\nTiFlash 轮换主秘钥的方法与 TiKV 相同。TiFlash 当前也不支持在线轮换主密钥，因此你需要重启 TiFlash 进行主密钥轮换。建议对运行中的、提供在线查询的 TiFlash 集群进行滚动重启。\n\n要轮换 KMS CMK，可以在 `tiflash-learner.toml` 配置文件中添加以下内容：\n\n```\n[security.encryption.master-key]\ntype = \"kms\"\nkey-id = \"50a0c603-1c6f-11e6-bb9e-3fadde80ce75\"\nregion = \"us-west-2\"\n\n[security.encryption.previous-master-key]\ntype = \"kms\"\nkey-id = \"0987dcba-09fe-87dc-65ba-ab0987654321\"\nregion = \"us-west-2\"\n```\n\n或者，在 TiUP 集群模板文件中添加以下内容：\n\n```\nserver_configs:\n  tiflash-learner:\n    security.encryption.master-key.type: \"kms\"\n    security.encryption.master-key.key-id: \"50a0c603-1c6f-11e6-bb9e-3fadde80ce75\"\n    security.encryption.master-key.region: \"us-west-2\"\n    security.encryption.previous-master-key.type: \"kms\"\n    security.encryption.previous-master-key.key-id: \"0987dcba-09fe-87dc-65ba-ab0987654321\"\n    security.encryption.previous-master-key.region: \"us-west-2\"\n```\n\n### 监控和调试\n\n要监控静态加密（如果 TiFlash 中部署了 Grafana 组件），可以查看 **TiFlash-Proxy-Details** -> **Encryption** 面板中的监控项，其中监控项的含义与 TiKV 相同。\n\n在调试方面，由于 TiFlash 复用了 TiKV 管理加密元数据的逻辑，因此也可使用 `tikv-ctl` 命令查看加密元数据（例如使用的加密方法和数据密钥列表）。该操作可能会暴露密钥，因此不推荐在生产环境中使用。详情参阅 [TiKV Control](/tikv-control.md#打印加密元数据)。\n\n### TiFlash 版本间兼容性\n\nTiFlash 在 v4.0.9 同样对加密元数据操作进行了优化，其兼容性要求与 TiKV 相同，详情参考 [TiKV 版本间兼容性](#tikv-版本间兼容性)。\n\n## BR S3 服务端加密\n\n使用 BR 备份数据到 S3 时，若要启用 S3 服务端加密，需要传递 `--s3.sse` 参数并将参数值设置为 `aws:kms`。S3 将使用自己的 KMS 密钥进行加密。示例如下：\n\n```\ntiup br backup full --pd <pd-address> --storage \"s3://<bucket>/<prefix>\" --s3.sse aws:kms\n```\n\n若要使用用户创建和拥有的自定义 AWS KMS CMK，需另外传递 `--s3.sse-kms-key-id` 参数。此时，BR 进程和集群中的所有 TiKV 节点都需访问该 KMS CMK（例如，通过 AWS IAM），并且该 KMS CMK 必须与存储备份的 S3 bucket 位于同一 AWS 区域。建议通过 AWS IAM 向 BR 进程和 TiKV 节点授予对 KMS CMK 的访问权限。参见 AWS 文档中的 [IAM](https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/introduction.html)。示例如下：\n\n```\ntiup br backup full --pd <pd-address> --storage \"s3://<bucket>/<prefix>\" --s3.sse aws:kms --s3.sse-kms-key-id 0987dcba-09fe-87dc-65ba-ab0987654321\n```\n\n恢复备份时，不需要也不可指定 `--s3.sse` 和 `--s3.sse-kms-key-id` 参数。S3 将自动相应进行解密。用于恢复备份数据的 BR 进程和集群中的 TiKV 节点也需要访问 KMS CMK，否则恢复将失败。示例如下：\n\n```\ntiup br restore full --pd <pd-address> --storage \"s3://<bucket>/<prefix>\"\n```\n\n## BR Azure Blob Storage 服务端加密\n\n使用 BR 备份数据到 Azure Blob Storage 时，可以选择使用加密范围 (Encryption Scope) 或加密密钥 (Encryption Key) 进行数据的服务端加密。\n\n### 方法一：使用加密范围\n\n你可以通过以下两种方式为备份数据指定加密范围：\n\n- 在 `backup` 命令中添加 `--azblob.encryption-scope` 参数，并设置为加密范围名：\n\n    ```shell\n    tiup br backup full --pd <pd-address> --storage \"azure://<bucket>/<prefix>\" --azblob.encryption-scope scope1\n    ```\n\n- 在 URI 中添加 `encryption-scope`，并设置为加密范围名：\n\n    ```shell\n    tiup br backup full --pd <pd-address> --storage \"azure://<bucket>/<prefix>?encryption-scope=scope1\"\n    ```\n\n更多信息请参考 Azure 文档中的[上传具有加密范围的 blob](https://learn.microsoft.com/zh-cn/azure/storage/blobs/encryption-scope-manage?tabs=powershell#upload-a-blob-with-an-encryption-scope)。\n\n在恢复备份时，不需要指定加密范围，Azure Blob Storage 将自动进行解密。示例如下：\n\n```shell\ntiup br restore full --pd <pd-address> --storage \"azure://<bucket>/<prefix>\"\n```\n\n### 方法二：使用加密密钥\n\n你可以通过以下三种方式为备份数据指定加密密钥：\n\n- 在 `backup` 命令中添加 `--azblob.encryption-key` 参数，并设置为 AES256 加密密钥：\n\n    ```shell\n    tiup br backup full --pd <pd-address> --storage \"azure://<bucket>/<prefix>\" --azblob.encryption-key <aes256-key>\n    ```\n\n- 在 URI 中添加 `encryption-key`，并设置为 AES256 加密密钥。如果密钥包含 URI 保留字符，例如 `&`、`%` 等，需要先进行百分号编码：\n\n    ```shell\n    tiup br backup full --pd <pd-address> --storage \"azure://<bucket>/<prefix>?encryption-key=<aes256-key>\"\n    ```\n\n- 在 BR 的环境变量中添加 `AZURE_ENCRYPTION_KEY`，并设置为 AES256 加密密钥。在运行前，请确保环境变量中的加密密钥是已知的，避免忘记密钥。\n\n    ```shell\n    export AZURE_ENCRYPTION_KEY=<aes256-key>\n    tiup br backup full --pd <pd-address> --storage \"azure://<bucket>/<prefix>\"\n    ```\n\n更多信息请参考 Azure 文档中的[在对 Blob 存储的请求中提供加密密钥](https://learn.microsoft.com/zh-cn/azure/storage/blobs/encryption-customer-provided-keys)。\n\n在恢复备份时，需要指定加密密钥。示例如下：\n\n- 在 `restore` 命令中传递 `--azblob.encryption-key` 参数：\n\n    ```shell\n    tiup br restore full --pd <pd-address> --storage \"azure://<bucket>/<prefix>\" --azblob.encryption-key <aes256-key>\n    ```\n\n- 在 URI 中添加 `encryption-key`：\n\n    ```shell\n    tiup br restore full --pd <pd-address> --storage \"azure://<bucket>/<prefix>?encryption-key=<aes256-key>\"\n    ```\n\n- 在 BR 的环境变量中添加 `AZURE_ENCRYPTION_KEY`：\n\n    ```shell\n    export AZURE_ENCRYPTION_KEY=<aes256-key>\n    tiup br restore full --pd <pd-address> --storage \"azure://<bucket>/<prefix>\"\n    ```\n"
        },
        {
          "name": "error-codes.md",
          "type": "blob",
          "size": 23.2607421875,
          "content": "---\ntitle: 错误码与故障诊断\naliases: ['/docs-cn/dev/error-codes/','/docs-cn/dev/reference/error-codes/']\nsummary: TiDB 错误码包括 MySQL 兼容的错误码和 TiDB 特有的错误码。如果遇到错误码，请参考官方文档或社区获取支持。常见错误码包括内存使用超限、写入冲突、表数据损坏、事务过大、写入冲突等。另外，TiDB 还提供了故障诊断文档供参考。\n---\n\n# 错误码与故障诊断\n\n本篇文档描述在使用 TiDB 过程中会遇到的问题以及解决方法。\n\n## 错误码\n\nTiDB 兼容 MySQL 的错误码，在大多数情况下，返回和 MySQL 一样的错误码。关于 MySQL 的错误码列表，详见 [MySQL 8.0 Error Message Reference](https://dev.mysql.com/doc/mysql-errors/8.0/en/)。另外还有一些 TiDB 特有的错误码：\n\n> **注意：**\n>\n> 有一部分错误码属于内部错误，正常情况下 TiDB 会自行处理不会直接返回给用户，故没有在此列出。\n>\n> 如果你遇到了这里没有列出的错误码，请从 PingCAP 官方或 TiDB 社区[获取支持](/support.md)。\n\n* Error Number: 8001\n\n    请求使用的内存超过 TiDB 内存使用的阈值限制。出现这种错误，可以通过调整系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 来增大单个 SQL 使用的内存上限。\n\n* Error Number: 8002\n\n    带有 `SELECT FOR UPDATE` 语句的事务，在遇到写入冲突时，为保证一致性无法进行重试，事务将进行回滚并返回该错误。出现这种错误，应用程序可以安全地重新执行整个事务。\n\n* Error Number: 8003\n\n    [`ADMIN CHECK TABLE`](/sql-statements/sql-statement-admin-check-table-index.md) 命令在遇到行数据跟索引不一致的时候返回该错误，在检查表中数据是否有损坏时常出现。出现该错误时，请向 PingCAP 工程师或通过官方论坛寻求帮助。\n\n* Error Number: 8004\n\n    单个事务过大，原因及解决方法请参考[这里](/faq/migration-tidb-faq.md#transaction-too-large-是什么原因怎么解决)\n\n* Error Number: 8005\n\n    完整的报错信息为 `ERROR 8005 (HY000) : Write Conflict, txnStartTS is stale`。\n\n    事务在 TiDB 中遇到了写入冲突。请检查业务逻辑，重试写入操作。\n\n* Error Number: 8018\n\n    当执行重新载入插件时，如果之前插件没有载入过，则会出现该错误。出现该错误，进行插件首次载入即可。\n\n* Error Number: 8019\n\n    重新载入的插件版本与之前的插件版本不一致，无法重新载入插件并报告该错误。可重新载入插件，确保插件的版本与之前载入的插件版本一致。\n\n* Error Number: 8020\n\n    当表被加锁时，如果对该表执行写入操作，将出现该错误。请将表解锁后，再进行尝试写入。\n\n* Error Number: 8021\n\n    当向 TiKV 读取的 key 不存在时将出现该错误，该错误用于内部使用，对外表现为读到的结果为空。\n\n* Error Number: 8022\n\n    事务提交失败，已经回滚，应用程序可以安全的重新执行整个事务。\n\n* Error Number: 8023\n\n    在事务内，写入事务缓存时，设置了空值，将返回该错误。这是一个内部使用的错误，将由内部进行处理，不会返回给应用程序。\n\n* Error Number: 8024\n\n    非法的事务。当事务执行时，发现没有获取事务的 ID (Start Timestamp)，代表正在执行的事务是一个非法的事务，将返回该错误。通常情况下不会出现该问题，当发生时，请向 PingCAP 工程师或通过官方论坛寻求帮助。\n\n* Error Number: 8025\n\n    写入的单条键值对过大。TiDB 默认支持最大 6MB 的单个键值对，超过该限制可适当调整 [`txn-entry-size-limit`](/tidb-configuration-file.md#txn-entry-size-limit-从-v4010-和-v500-版本开始引入) 配置项以放宽限制。\n\n* Error Number: 8026\n\n    使用了没有实现的接口函数。该错误仅用于数据库内部，应用程序不会收到这个错误。\n\n* Error Number: 8027\n\n    表结构版本过期。TiDB 采用在线变更表结构的方法。当 TiDB server 表结构版本落后于整个系统的时，执行 SQL 将遇到该错误。遇到该错误，请检查该 TiDB server 与 PD leader 之间的网络。\n\n* Error Number: 8028\n\n    TiDB v6.3.0 引入了[元数据锁](/metadata-lock.md)特性。在关闭元数据锁的情况下，当事务执行时，事务无法感知到 TiDB 的表结构发生了变化。因此，TiDB 在事务提交时，会对事务涉及表的结构进行检查。如果事务执行中表结构发生了变化，则事务将提交失败，并返回该错误。遇到该错误，应用程序可以安全地重新执行整个事务。\n\n    在打开元数据锁的情况下，非 RC 隔离级别中，如果从事务开始到初次访问一个表之间，该表进行了有损的列类型变更操作（例如 `INT` 类型变成 `CHAR` 类型是有损的，`TINYINT` 类型变成 `INT` 类型这种不需要重写数据的则是无损的），则访问该表的语句报错，事务不会自动回滚。用户可以继续执行其他语句，并决定是否回滚或者提交事务。\n\n* Error Number: 8029\n\n    当数据库内部进行数值转换发生错误时，将会出现该错误，该错误仅在内部使用，对外部应用将转换为具体类型的错误。\n\n* Error Number: 8030\n\n    将值转变为带符号正整数时发生了越界，将结果显示为负数。多在告警信息里出现。\n\n* Error Number: 8031\n\n    将负数转变为无符号数时，将负数转变为了正数。多在告警信息里出现。\n\n* Error Number: 8032\n\n    使用了非法的 year 格式。year 只允许 1 位、2 位和 4 位数。\n\n* Error Number: 8033\n\n    使用了非法的 year 值。year 的合法范围是 (1901, 2155)。\n\n* Error Number: 8037\n\n    week 函数中使用了非法的 mode 格式。mode 必须是一位数字，范围 [0, 7]。\n\n* Error Number: 8038\n\n    字段无法获取到默认值。一般作为内部错误使用，转换成其他具体错误类型后，返回给应用程序。\n\n* Error Number: 8040\n\n    尝试进行不支持的操作，比如在 View 和 Sequence 上进行 lock table。\n\n* Error Number: 8047\n\n    设置了不支持的系统变量值，通常在用户设置了数据库不支持的变量值后的告警信息里出现。\n\n* Error Number: 8048\n\n    设置了不支持的隔离级别，如果是使用第三方工具或框架等无法修改代码进行适配的情况，可以考虑通过 [`tidb_skip_isolation_level_check`](/system-variables.md#tidb_skip_isolation_level_check) 来绕过这一检查。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    set @@tidb_skip_isolation_level_check = 1;\n    ```\n\n* Error Number: 8050\n\n    设置了不支持的权限类型，遇到该错误请参考 [TiDB 权限说明](/privilege-management.md#tidb-各操作需要的权限)进行调整。\n\n* Error Number: 8051\n\n    TiDB 在解析客户端发送的 Exec 参数列表时遇到了未知的数据类型。如果遇到这个错误，请检查客户端是否正常，如果客户端正常请向 PingCAP 工程师或通过官方论坛寻求帮助。\n\n* Error Number: 8052\n\n    来自客户端的数据包的序列号错误。如果遇到这个错误，请检查客户端是否正常，如果客户端正常请向 PingCAP 工程师或通过官方论坛寻求帮助。\n\n* Error Number: 8055\n\n    当前快照过旧，数据可能已经被 GC。可以调大 [`tidb_gc_life_time`](/system-variables.md#tidb_gc_life_time-从-v50-版本开始引入) 的值来避免该问题。从 TiDB v4.0.8 版本起，TiDB 会自动为长时间运行的事务保留数据，一般不会遇到该错误。\n\n    有关 GC 的介绍和配置可以参考 [GC 机制简介](/garbage-collection-overview.md)和 [GC 配置](/garbage-collection-configuration.md)文档。\n\n* Error Number: 8059\n\n    自动随机量可用次数用尽无法进行分配。当前没有恢复这类错误的方法。建议在使用 auto random 功能时使用 bigint 以获取最大的可分配次数，并尽量避免手动给 auto random 列赋值。相关的介绍和使用建议可以参考 [auto random 功能文档](/auto-random.md)。\n\n* Error Number: 8060\n\n    非法的自增列偏移量。请检查 `auto_increment_increment` 和 `auto_increment_offset` 的取值是否符合要求。\n\n* Error Number: 8061\n\n    不支持的 SQL Hint。请参考 [Optimizer Hints](/optimizer-hints.md) 检查和修正 SQL Hint。\n\n* Error Number: 8062\n\n    SQL Hint 中使用了非法的 token，与 Hint 的保留字冲突。请参考 [Optimizer Hints](/optimizer-hints.md) 检查和修正 SQL Hint。\n\n* Error Number: 8063\n\n    SQL Hint 中限制内存使用量超过系统设置的上限，设置被忽略。请参考 [Optimizer Hints](/optimizer-hints.md) 检查和修正 SQL Hint。\n\n* Error Number: 8064\n\n    解析 SQL Hint 失败。请参考 [Optimizer Hints](/optimizer-hints.md) 检查和修正 SQL Hint。\n\n* Error Number: 8065\n\n    SQL Hint 中使用了非法的整数。请参考 [Optimizer Hints](/optimizer-hints.md) 检查和修正 SQL Hint。\n\n* Error Number: 8066\n\n    JSON_OBJECTAGG 函数的第二个参数是非法参数。\n\n* Error Number: 8101\n\n    插件 ID 格式错误，正确的格式是 `[name]-[version]` 并且 name 和 version 中不能带有 '-'。\n\n* Error Number: 8102\n\n    无法读取插件定义信息。请检查插件相关的配置。\n\n* Error Number: 8103\n\n    插件名称错误，请检查插件的配置。\n\n* Error Number: 8104\n\n    插件版本不匹配，请检查插件的配置。\n\n* Error Number: 8105\n\n    插件被重复载入。\n\n* Error Number: 8106\n\n    插件定义的系统变量名称没有以插件名作为开头，请联系插件的开发者进行修复。\n\n* Error Number: 8107\n\n    载入的插件未指定版本或指定的版本过低，请检查插件的配置。\n\n* Error Number: 8108\n\n    不支持的执行计划类型。该错误为内部处理的错误，如果遇到该报错请向 PingCAP 工程师或通过官方论坛寻求帮助。\n\n* Error Number: 8109\n\n    analyze 索引时找不到指定的索引。\n\n* Error Number: 8110\n\n    不能进行笛卡尔积运算，需要将配置文件里的 `cross-join` 设置为 `true`。\n\n* Error Number: 8111\n\n    execute 语句执行时找不到对应的 prepare 语句。\n\n* Error Number: 8112\n\n    execute 语句的参数个数与 prepare 语句不符合。\n\n* Error Number: 8113\n\n    execute 语句涉及的表结构在 prepare 语句执行后发生了变化。\n\n* Error Number: 8115\n\n    不支持 prepare 多行语句。\n\n* Error Number: 8116\n\n    不支持 prepare DDL 语句。\n\n* Error Number: 8120\n\n    获取不到事务的 start tso，请检查 PD Server 状态/监控/日志以及 TiDB Server 与 PD Server 之间的网络。\n\n* Error Number: 8121\n\n    权限检查失败，请检查数据库的权限配置。\n\n* Error Number: 8122\n\n    指定了通配符，但是找不到对应的表名。\n\n* Error Number: 8123\n\n    带聚合函数的 SQL 中返回非聚合的列，违反了 `only_full_group_by` 模式。请修改 SQL 或者考虑关闭 `only_full_group_by` 模式。\n\n* Error Number: 8129\n\n    TiDB 尚不支持键长度 >= 65536 的 JSON 对象。\n\n* Error Number: 8130\n\n    完整的报错信息为 `ERROR 8130 (HY000): client has multi-statement capability disabled`。\n\n    从早期版本的 TiDB 升级后，可能会出现该问题。为了减少 SQL 注入攻击的影响，TiDB 目前默认不允许在同一 `COM_QUERY` 调用中执行多个查询。\n\n    可通过系统变量 [`tidb_multi_statement_mode`](/system-variables.md#tidb_multi_statement_mode-从-v4011-版本开始引入) 控制是否在同一 `COM_QUERY` 调用中执行多个查询。\n\n* Error Number: 8138\n\n    事务试图写入的行值有误，请参考[数据索引不一致报错](/troubleshoot-data-inconsistency-errors.md#error-8138)。\n\n* Error Number: 8139\n\n    事务试图写入的行和索引的 handle 值不一致，请参考[数据索引不一致报错](/troubleshoot-data-inconsistency-errors.md#error-8139)。\n\n* Error Number: 8140\n\n    事务试图写入的行和索引的值不一致，请参考[数据索引不一致报错](/troubleshoot-data-inconsistency-errors.md#error-8140)。\n\n* Error Number: 8141\n\n    事务写入时，对 key 的存在性断言报错，请参考[数据索引不一致报错](/troubleshoot-data-inconsistency-errors.md#error-8141)。\n\n* Error Number: 8143\n\n    非事务 DML 语句的一个 batch 报错，语句中止，请参考[非事务 DML 语句](/non-transactional-dml.md)\n\n* Error Number: 8147\n\n   当 [`tidb_constraint_check_in_place_pessimistic`](/system-variables.md#tidb_constraint_check_in_place_pessimistic-从-v630-版本开始引入) 设置为 `OFF` 时，为保证事务的正确性，SQL 语句执行时产生的任何错误都可能导致 TiDB 返回 `8147` 报错并中止当前事务。具体的错误原因，请参考对应的报错信息。详见[约束](/constraints.md#悲观事务)。\n\n* Error Number: 8154\n\n    目前 `LOAD DATA` 不支持从 TiDB 服务器本地导入数据，可以指定 `LOCAL` 从客户端导入，或者将数据上传到 S3/GCS 再进行导入。请参考 [`LOAD DATA`](/sql-statements/sql-statement-load-data.md)。\n\n* Error Number: 8156\n\n    传入的文件路径不能为空。需要设置正确的路径再进行导入。\n\n* Error Number: 8157\n\n    不支持的文件格式。请参考 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md#format) 查看支持的格式。\n\n* Error Number: 8158\n\n    传入的文件路径不合法。请根据具体的错误提示进行处理。S3 和 GCS 路径设置可参考[外部存储服务的 URI 格式](/external-storage-uri.md)。\n\n* Error Number: 8159\n\n    TiDB 无法访问传入的 S3/GCS 路径。请确保填写的 S3/GCS bucket 存在，且输入了正确的 Access Key 和 Secret Access Key 以让 TiDB 服务器有权限访问 S3/GCS 对应的 bucket。\n\n* Error Number: 8160\n\n    读取数据文件失败。请根据具体的错误提示进行处理。\n\n* Error Number: 8162\n\n    语句存在错误。请根据具体的错误提示进行处理。\n\n* Error Number: 8163\n\n    未知的选项。请参考 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md#参数说明) 查看支持的选项。\n\n* Error Number: 8164\n\n    选项取值无效。请参考 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md#参数说明) 查看有效的取值。\n\n* Error Number: 8165\n\n    重复指定了选项，每个选项只能指定一次。\n\n* Error Number: 8166\n\n    某些选项只能在特定的条件下才可以使用。请根据具体的错误提示进行处理。请参考 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md#参数说明) 查看支持的选项。\n\n* Error Number: 8170\n\n    指定的 job 不存在。\n\n* Error Number: 8171\n\n    该 job 的状态不能进行当前操作。请根据具体的错误提示进行处理。\n\n* Error Number: 8173\n\n    执行 `IMPORT INTO` 时，TiDB 会对当前环境进行检查，比如检查下游表是否为空等。请根据具体的错误提示进行处理。\n\n* Error Number: 8200\n\n    尚不支持的 DDL 语法。请参考[与 MySQL DDL 的兼容性](/mysql-compatibility.md#ddl-的限制)。\n\n* Error Number: 8214\n\n    DDL 操作被 admin cancel 操作终止。\n\n* Error Number: 8215\n\n    Admin Repair 表失败，如果遇到该报错请向 PingCAP 工程师或通过官方论坛寻求帮助。\n\n* Error Number: 8216\n\n    `AUTO_RANDOM` 的使用方法不正确。请参考 [`AUTO_RANDOM`](/auto-random.md) 进行修改。\n\n* Error Number: 8223\n\n    检测出数据与索引不一致的错误，如果遇到该报错请向 PingCAP 工程师或通过官方论坛寻求帮助。\n\n* Error Number: 8224\n\n    找不到 DDL job，请检查 restore 操作指定的 job id 是否存在。\n\n* Error Number: 8225\n\n    DDL 已经完成，无法被取消。\n\n* Error Number: 8226\n\n    DDL 几乎要完成了，无法被取消。\n\n* Error Number: 8227\n\n    创建 Sequence 时使用了不支持的选项，支持的选项的列表可以参考 [Sequence 使用文档](/sql-statements/sql-statement-create-sequence.md#参数说明)。\n\n* Error Number: 8228\n\n    在 Sequence 上使用 `SETVAL` 时指定了不支持的类型，该函数的示例可以在 [Sequence 使用文档](/sql-statements/sql-statement-create-sequence.md#示例)中找到。\n\n* Error Number: 8229\n\n    事务超过存活时间，遇到该问题可以提交或者回滚当前事务，开启一个新事务。\n\n* Error Number: 8230\n\n    TiDB 目前不支持在新添加的列上使用 Sequence 作为默认值，如果尝试进行这类操作会返回该错误。\n\n* Error Number: 8248\n\n    资源组已存在。在重复创建资源组时返回该错误。\n\n* Error Number: 8249\n\n    资源组不存在。在修改或绑定不存在的资源组时返回该错误。请参考[创建资源组](/tidb-resource-control.md#创建资源组)。\n\n* Error Number: 8250\n\n    完整的报错信息如下：\n\n    `ERROR 8250 (HY000) : Resource control feature is disabled. Run \"SET GLOBAL tidb_enable_resource_control='on'\" to enable the feature`\n\n    资源控制的功能没有打开时，使用资源管控 (Resource Control) 相关功能会返回该错误。你可以开启全局变量 [`tidb_enable_resource_control`](/system-variables.md#tidb_enable_resource_control-从-v660-版本开始引入) 启用资源管控。\n\n* Error Number: 8251\n\n    `Resource Control` 组件在 TiDB 启动时进行初始化，相关配置会从 `Resource Control` 的服务端 `Resource Manager` 上获取，如果此过程中出错，则会返回此错误。\n\n* Error Number: 8252\n\n    完整的报错信息如下：\n\n    `ERROR 8252 (HY000) : Exceeded resource group quota limitation`\n\n    在尝试消耗超过资源组的限制时返回该错误。一般出现该错误，是由于单次事务太大或者并发太多导致，需调整事务大小或减少客户端并发数。\n\n* Error Number: 8253\n\n    查询终止，因为满足 Runaway Queries 的条件。请参考 [Runaway Queries](/tidb-resource-control.md#管理资源消耗超出预期的查询-runaway-queries)。\n\n* Error Number: 8254\n\n    查询终止，因为被 Runaway Queries 免疫命中。请参考 [Runaway Queries](/tidb-resource-control.md#管理资源消耗超出预期的查询-runaway-queries)。\n\n* Error Number: 8260\n\n    DDL 操作无法被 `ADMIN PAUSE` 暂停运行。\n\n* Error Number: 8261\n\n    DDL 操作无法被 `ADMIN RESUME` 恢复运行。\n\n* Error Number: 8262\n\n    DDL 已经被 `ADMIN PAUSE` 暂停，无法再次执行。\n\n* Error Number: 8263\n\n    该 DDL 无法在特定的 BDR role 下执行。请确定该集群是否处于[双向复制](/ticdc/ticdc-bidirectional-replication.md) 中。如果集群没有在双向复制中，可以通过 `ADMIN UNSET BDR ROLE;` 使 DDL 恢复正常使用。\n\n* Error Number: 9001\n\n    完整的报错信息为 `ERROR 9001 (HY000) : PD server timeout`。\n\n    请求 PD 超时，请检查 PD Server 状态/监控/日志以及 TiDB Server 与 PD Server 之间的网络。\n\n* Error Number: 9002\n\n    完整的报错信息为 `ERROR 9002 (HY000) : TiKV server timeout`。\n\n    请求 TiKV 超时，请检查 TiKV Server 状态/监控/日志以及 TiDB Server 与 TiKV Server 之间的网络。\n\n* Error Number: 9003\n\n    完整的报错信息为 `ERROR 9003 (HY000) : TiKV Server is Busy`。\n\n    TiKV 操作繁忙，一般出现在数据库负载比较高时，请检查 TiKV Server 状态/监控/日志。\n\n* Error Number: 9004\n\n    完整的报错信息为 `ERROR 9004 (HY000) : Resolve Lock Timeout`。\n\n    清理锁超时，当数据库上承载的业务存在大量的事务冲突时，会遇到这种错误，请检查业务代码是否有锁争用。\n\n* Error Number: 9005\n\n    完整的报错信息为 `ERROR 9005 (HY000) : Region is unavailable`。\n\n    访问的 Region 不可用，某个 Raft Group 不可用，如副本数目不足，出现在 TiKV 比较繁忙或者是 TiKV 节点停机的时候，请检查 TiKV Server 状态/监控/日志。\n\n* Error Number: 9006\n\n    完整的报错信息为 `ERROR 9006 (HY000) : GC life time is shorter than transaction duration`。\n\n    GC Life Time 间隔时间过短，长事务本应读到的数据可能被清理了。你可以使用如下命令修改 [`tidb_gc_life_time`](/system-variables.md#tidb_gc_life_time-从-v50-版本开始引入) 的值：\n\n    ```sql\n    SET GLOBAL tidb_gc_life_time = '30m';\n    ```\n\n    其中 30m 代表仅清理 30 分钟前的数据，这可能会额外占用一定的存储空间。\n\n* Error Number: 9007\n\n    报错信息以 `ERROR 9007 (HY000) : Write conflict` 开头。\n\n    如果报错信息中含有 \"reason=LazyUniquenessCheck\"，说明是悲观事务并且设置了 `@@tidb_constraint_check_in_place_pessimistic=OFF`，业务中存在唯一索引上的写冲突，此时悲观事务不能保证执行成功。可以在应用测重试事务，或将该变量设置成 `ON` 绕过。详见[约束](/constraints.md#悲观事务)。\n\n* Error Number: 9008\n\n    同时向 TiKV 发送的请求过多，超过了限制。请调大 `tidb_store_limit` 或将其设置为 `0` 来取消对请求流量的限制。\n\n* Error Number: 9010\n\n    TiKV 无法处理这条 raft log，请检查 TiKV Server 状态/监控/日志。\n\n* Error Number: 9012\n\n    请求 TiFlash 超时。请检查 TiFlash Server 状态/监控/日志以及 TiDB Server 与 TiFlash Server 之间的网络。\n\n* Error Number: 9013\n\n    TiFlash 操作繁忙。该错误一般出现在数据库负载比较高时。请检查 TiFlash Server 的状态/监控/日志。\n\n### MySQL 原生报错汇总\n\n* Error Number: 2013 (HY000)\n\n    完整的报错信息为 `ERROR 2013 (HY000): Lost connection to MySQL server during query`。\n\n    排查方法如下：\n\n    - log 中是否有 panic\n    - dmesg 中是否有 oom，命令：`dmesg -T | grep -i oom`\n    - 长时间没有访问，也会收到这个报错，一般是 tcp 超时导致的，tcp 长时间不用，会被操作系统 kill。\n\n* Error Number: 1105 (HY000)\n\n    完整的报错信息为 `ERROR 1105 (HY000): other error: unknown error Wire Error(InvalidEnumValue(4004))`\n\n    这类问题一般是 TiDB 和 TiKV 版本不匹配，在升级过程尽量一起升级，避免版本 mismatch。\n\n* Error Number: 1148 (42000)\n\n    完整的报错信息为 `ERROR 1148 (42000): the used command is not allowed with this TiDB version`。\n\n    这个问题是因为在执行 `LOAD DATA LOCAL` 语句的时候，MySQL 客户端不允许执行此语句（即 `local_infile` 选项为 0）。解决方法是在启动 MySQL 客户端时，用 `--local-infile=1` 选项。具体启动指令类似：`mysql --local-infile=1 -u root -h 127.0.0.1 -P 4000`。有些 MySQL 客户端需要设置而有些不需要设置，原因是不同版本的 MySQL 客户端对 `local-infile` 的默认值不同。\n\n* Error Number: 9001 (HY000)\n\n    完整的报错信息为 `ERROR 9001 (HY000): PD server timeout start timestamp may fall behind safe point`\n\n    这个报错一般是 TiDB 访问 PD 出了问题，TiDB 后台有个 worker 会不断地从 PD 查询 safepoint，如果超过 100s 查不成功就会报这个错。一般是因为 PD 磁盘操作过忙、反应过慢，或者 TiDB 和 PD 之间的网络有问题。TiDB 常见错误码请参考[错误码与故障诊断](/error-codes.md)。\n\n* TiDB 日志中的报错信息：EOF\n\n    当客户端或者 proxy 断开连接时，TiDB 不会立刻察觉连接已断开，而是等到开始往连接返回数据时，才发现连接已断开，此时日志会打印 EOF 错误。\n\n## 故障诊断\n\n参见[故障诊断文档](/troubleshoot-tidb-cluster.md)。\n"
        },
        {
          "name": "explain-aggregation.md",
          "type": "blob",
          "size": 19.84375,
          "content": "---\ntitle: 用 EXPLAIN 查看聚合查询的执行计划\nsummary: 了解 TiDB 中 EXPLAIN 语句返回的执行计划信息。\n---\n\n# 用 EXPLAIN 查看聚合查询执行计划\n\nSQL 查询中可能会使用聚合计算，可以通过 `EXPLAIN` 语句来查看聚合查询的执行计划。本文提供多个示例，以帮助用户理解聚合查询是如何执行的。\n\nSQL 优化器会选择以下任一算子实现数据聚合：\n\n- Hash Aggregation\n- Stream Aggregation\n\n为了提高查询效率，数据聚合在 Coprocessor 层和 TiDB 层均会执行。现有示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (id INT NOT NULL PRIMARY KEY auto_increment, pad1 BLOB, pad2 BLOB, pad3 BLOB);\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM dual;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nSELECT SLEEP(1);\nANALYZE TABLE t1;\n```\n\n以上示例创建表格 `t1` 并插入数据后，再执行 [`SHOW TABLE REGIONS`](/sql-statements/sql-statement-show-table-regions.md) 语句。从以下 `SHOW TABLE REGIONS` 的执行结果可知，表 `t1` 被切分为多个 Region：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW TABLE t1 REGIONS;\n```\n\n```sql\n+-----------+--------------+--------------+-----------+-----------------+-------+------------+---------------+------------+----------------------+------------------+\n| REGION_ID | START_KEY    | END_KEY      | LEADER_ID | LEADER_STORE_ID | PEERS | SCATTERING | WRITTEN_BYTES | READ_BYTES | APPROXIMATE_SIZE(MB) | APPROXIMATE_KEYS |\n+-----------+--------------+--------------+-----------+-----------------+-------+------------+---------------+------------+----------------------+------------------+\n|        64 | t_64_        | t_64_r_31766 |        65 |               1 | 65    |          0 |          1325 |  102033520 |                   98 |            52797 |\n|        66 | t_64_r_31766 | t_64_r_63531 |        67 |               1 | 67    |          0 |          1325 |   72522521 |                  104 |            78495 |\n|        68 | t_64_r_63531 | t_64_r_95296 |        69 |               1 | 69    |          0 |          1325 |          0 |                  104 |            95433 |\n|         2 | t_64_r_95296 |              |         3 |               1 | 3     |          0 |          1501 |          0 |                   81 |            63211 |\n+-----------+--------------+--------------+-----------+-----------------+-------+------------+---------------+------------+----------------------+------------------+\n4 rows in set (0.00 sec)\n```\n\n使用 `EXPLAIN` 查看以下聚合语句的执行计划。可以看到 `└─StreamAgg_8` 算子先执行在 TiKV 内每个 Region 上，然后 TiKV 的每个 Region 会返回一行数据给 TiDB，TiDB 在 `StreamAgg_16` 算子上对每个 Region 返回的数据进行聚合：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT COUNT(*) FROM t1;\n```\n\n```sql\n+----------------------------+-----------+-----------+---------------+---------------------------------+\n| id                         | estRows   | task      | access object | operator info                   |\n+----------------------------+-----------+-----------+---------------+---------------------------------+\n| StreamAgg_16               | 1.00      | root      |               | funcs:count(Column#7)->Column#5 |\n| └─TableReader_17           | 1.00      | root      |               | data:StreamAgg_8                |\n|   └─StreamAgg_8            | 1.00      | cop[tikv] |               | funcs:count(1)->Column#7        |\n|     └─TableFullScan_15     | 242020.00 | cop[tikv] | table:t1      | keep order:false                |\n+----------------------------+-----------+-----------+---------------+---------------------------------+\n4 rows in set (0.00 sec)\n```\n\n同样，通过执行 `EXPLAIN ANALYZE` 语句可知，`actRows` 与 `SHOW TABLE REGIONS` 返回结果中的 Region 数匹配，这是因为执行使用了 `TableFullScan` 全表扫并且没有二级索引：\n\n```sql\nEXPLAIN ANALYZE SELECT COUNT(*) FROM t1;\n```\n\n```sql\n+----------------------------+-----------+---------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-----------+------+\n| id                         | estRows   | actRows | task      | access object | execution info                                                                                                                                                                                                                                  | operator info                   | memory    | disk |\n+----------------------------+-----------+---------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-----------+------+\n| StreamAgg_16               | 1.00      | 1       | root      |               | time:12.609575ms, loops:2                                                                                                                                                                                                                       | funcs:count(Column#7)->Column#5 | 372 Bytes | N/A  |\n| └─TableReader_17           | 1.00      | 4       | root      |               | time:12.605155ms, loops:2, cop_task: {num: 4, max: 12.538245ms, min: 9.256838ms, avg: 10.895114ms, p95: 12.538245ms, max_proc_keys: 31765, p95_proc_keys: 31765, tot_proc: 48ms, rpc_num: 4, rpc_time: 43.530707ms, copr_cache_hit_ratio: 0.00} | data:StreamAgg_8                | 293 Bytes | N/A  |\n|   └─StreamAgg_8            | 1.00      | 4       | cop[tikv] |               | proc max:12ms, min:12ms, p80:12ms, p95:12ms, iters:122, tasks:4                                                                                                                                                                                 | funcs:count(1)->Column#7        | N/A       | N/A  |\n|     └─TableFullScan_15     | 242020.00 | 121010  | cop[tikv] | table:t1      | proc max:12ms, min:12ms, p80:12ms, p95:12ms, iters:122, tasks:4                                                                                                                                                                                 | keep order:false                | N/A       | N/A  |\n+----------------------------+-----------+---------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------+-----------+------+\n4 rows in set (0.01 sec)\n```\n\n## Hash Aggregation\n\nHash Aggregation 算法在执行聚合时使用 Hash 表存储中间结果。此算法采用多线程并发优化，执行速度快，但与 Stream Aggregation 算法相比会消耗较多内存。\n\n下面是一个使用 Hash Aggregation（即 `HashAgg` 算子）的例子：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT /*+ HASH_AGG() */ count(*) FROM t1;\n```\n\n```sql\n+---------------------------+-----------+-----------+---------------+---------------------------------+\n| id                        | estRows   | task      | access object | operator info                   |\n+---------------------------+-----------+-----------+---------------+---------------------------------+\n| HashAgg_9                 | 1.00      | root      |               | funcs:count(Column#6)->Column#5 |\n| └─TableReader_10          | 1.00      | root      |               | data:HashAgg_5                  |\n|   └─HashAgg_5             | 1.00      | cop[tikv] |               | funcs:count(1)->Column#6        |\n|     └─TableFullScan_8     | 242020.00 | cop[tikv] | table:t1      | keep order:false                |\n+---------------------------+-----------+-----------+---------------+---------------------------------+\n4 rows in set (0.00 sec)\n```\n\n`operator info` 列显示，用于聚合数据的 Hash 函数为 `funcs:count(1)->Column#6`。\n\n## Stream Aggregation\n\nStream Aggregation 算法通常会比 Hash Aggregation 算法占用更少的内存。但是此算法要求数据按顺序发送，以便对依次到达的值实现流式数据聚合。\n\n下面是一个使用 Stream Aggregation 的例子：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t2 (id INT NOT NULL PRIMARY KEY, col1 INT NOT NULL);\nINSERT INTO t2 VALUES (1, 9),(2, 3),(3,1),(4,8),(6,3);\nEXPLAIN SELECT /*+ STREAM_AGG() */ col1, count(*) FROM t2 GROUP BY col1;\n```\n\n```sql\nQuery OK, 0 rows affected (0.11 sec)\n\nQuery OK, 5 rows affected (0.01 sec)\nRecords: 5  Duplicates: 0  Warnings: 0\n\n+------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n| id                           | estRows  | task      | access object | operator info                                                                               |\n+------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n| Projection_4                 | 8000.00  | root      |               | test.t2.col1, Column#3                                                                      |\n| └─StreamAgg_8                | 8000.00  | root      |               | group by:test.t2.col1, funcs:count(1)->Column#3, funcs:firstrow(test.t2.col1)->test.t2.col1 |\n|   └─Sort_13                  | 10000.00 | root      |               | test.t2.col1                                                                                |\n|     └─TableReader_12         | 10000.00 | root      |               | data:TableFullScan_11                                                                       |\n|       └─TableFullScan_11     | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo                                                              |\n+------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n5 rows in set (0.00 sec)\n```\n\n以上示例中，可以在 `col1` 上添加索引来消除 `└─Sort_13` 算子。添加索引后，TiDB 就可以按顺序读取数据并消除 `└─Sort_13` 算子。\n\n{{< copyable \"sql\" >}}\n\n```sql\nALTER TABLE t2 ADD INDEX (col1);\nEXPLAIN SELECT /*+ STREAM_AGG() */ col1, count(*) FROM t2 GROUP BY col1;\n```\n\n```sql\nQuery OK, 0 rows affected (0.28 sec)\n\n+------------------------------+---------+-----------+----------------------------+----------------------------------------------------------------------------------------------------+\n| id                           | estRows | task      | access object              | operator info                                                                                      |\n+------------------------------+---------+-----------+----------------------------+----------------------------------------------------------------------------------------------------+\n| Projection_4                 | 4.00    | root      |                            | test.t2.col1, Column#3                                                                             |\n| └─StreamAgg_14               | 4.00    | root      |                            | group by:test.t2.col1, funcs:count(Column#4)->Column#3, funcs:firstrow(test.t2.col1)->test.t2.col1 |\n|   └─IndexReader_15           | 4.00    | root      |                            | index:StreamAgg_8                                                                                  |\n|     └─StreamAgg_8            | 4.00    | cop[tikv] |                            | group by:test.t2.col1, funcs:count(1)->Column#4                                                    |\n|       └─IndexFullScan_13     | 5.00    | cop[tikv] | table:t2, index:col1(col1) | keep order:true, stats:pseudo                                                                      |\n+------------------------------+---------+-----------+----------------------------+----------------------------------------------------------------------------------------------------+\n5 rows in set (0.00 sec)\n```\n\n## 多维度数据聚合 ROLLUP\n\n自 v7.4.0 起，TiDB 的 `GROUP BY` 子句支持 `WITH ROLLUP` 修饰符。\n\n你可以在 `GROUP BY` 子句中指定一个或多个列，形成一个分组列表，然后添加 `WITH ROLLUP` 修饰符。TiDB 将会按照分组列表中的列进行多维度的递减分组，并在输出中为你提供各个分组数据的汇总结果。\n\n> **注意**\n>\n> TiDB 暂不支持 Cube 语法。\n\n```sql\nexplain SELECT year, month, grouping(year), grouping(month), SUM(profit) AS profit FROM bank GROUP BY year, month WITH ROLLUP;\n+----------------------------------------+---------+--------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id                                     | estRows | task         | access object | operator info                                                                                                                                                                                                                        |\n+----------------------------------------+---------+--------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| TableReader_44                         | 2.40    | root         |               | MppVersion: 2, data:ExchangeSender_43                                                                                                                                                                                                |\n| └─ExchangeSender_43                    | 2.40    | mpp[tiflash] |               | ExchangeType: PassThrough                                                                                                                                                                                                            |\n|   └─Projection_8                       | 2.40    | mpp[tiflash] |               | Column#6->Column#12, Column#7->Column#13, grouping(gid)->Column#14, grouping(gid)->Column#15, Column#9->Column#16                                                                                                                    |\n|     └─Projection_38                    | 2.40    | mpp[tiflash] |               | Column#9, Column#6, Column#7, gid                                                                                                                                                                                                    |\n|       └─HashAgg_36                     | 2.40    | mpp[tiflash] |               | group by:Column#6, Column#7, gid, funcs:sum(test.bank.profit)->Column#9, funcs:firstrow(Column#6)->Column#6, funcs:firstrow(Column#7)->Column#7, funcs:firstrow(gid)->gid, stream_count: 8                                           |\n|         └─ExchangeReceiver_22          | 3.00    | mpp[tiflash] |               | stream_count: 8                                                                                                                                                                                                                      |\n|           └─ExchangeSender_21          | 3.00    | mpp[tiflash] |               | ExchangeType: HashPartition, Compression: FAST, Hash Cols: [name: Column#6, collate: binary], [name: Column#7, collate: utf8mb4_bin], [name: gid, collate: binary], stream_count: 8                                                  |\n|             └─Expand_20                | 3.00    | mpp[tiflash] |               | level-projection:[test.bank.profit, <nil>->Column#6, <nil>->Column#7, 0->gid],[test.bank.profit, Column#6, <nil>->Column#7, 1->gid],[test.bank.profit, Column#6, Column#7, 3->gid]; schema: [test.bank.profit,Column#6,Column#7,gid] |\n|               └─Projection_16          | 3.00    | mpp[tiflash] |               | test.bank.profit, test.bank.year->Column#6, test.bank.month->Column#7                                                                                                                                                                |\n|                 └─TableFullScan_17     | 3.00    | mpp[tiflash] | table:bank    | keep order:false, stats:pseudo                                                                                                                                                                                                       |\n+----------------------------------------+---------+--------------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n10 rows in set (0.05 sec)\n```\n\n该语句的 SQL 聚合可以按照 `GROUP BY year, month WITH ROLLUP` 语法在 {year, month}、{year}、{} 这 3 个分组中分别计算并连接结果。\n\n更多信息，请参考 [GROUP BY 修饰符](/functions-and-operators/group-by-modifier.md)。\n\n## 其他类型查询的执行计划\n\n+ [MPP 模式查询的执行计划](/explain-mpp.md)\n+ [索引查询的执行计划](/explain-indexes.md)\n+ [Join 查询的执行计划](/explain-joins.md)\n+ [子查询的执行计划](/explain-subqueries.md)\n+ [视图查询的执行计划](/explain-views.md)\n+ [分区查询的执行计划](/explain-partitions.md)\n+ [索引合并查询的执行计划](/explain-index-merge.md)\n"
        },
        {
          "name": "explain-index-merge.md",
          "type": "blob",
          "size": 9.77734375,
          "content": "---\ntitle: 用 EXPLAIN 查看索引合并的 SQL 执行计划\nsummary: 了解 TiDB 中 EXPLAIN 语句返回的执行计划信息。\n---\n\n# 用 EXPLAIN 查看索引合并的 SQL 执行计划\n\n索引合并是从 TiDB v4.0 起引入的一种新的表访问方式。在这种访问方式下，TiDB 优化器可以选择对一张表使用多个索引，并将每个索引的返回结果进行合并。在某些场景下，这种访问方式能够减少大量不必要的数据扫描，提升查询的执行效率。\n\nTiDB 中的索引合并分为交集型和并集型两种类型，分别适用于由 `AND` 连接的表达式和由 `OR` 连接的表达式。其中，并集型索引合并在 TiDB v4.0 作为实验功能引入，在 v5.4.0 成为正式功能 (GA)。交集型索引合并从 TiDB v6.5.0 起引入，且必须使用 [`USE_INDEX_MERGE`](/optimizer-hints.md#use_index_merget1_name-idx1_name--idx2_name-) Hint 指定才能使用。\n\n## 开启索引合并\n\n在 v5.4.0 及以上版本的新建集群中，索引合并默认开启。在其他情况下如果未开启，可将 `tidb_enable_index_merge` 的值设为 `ON` 来开启索引合并功能。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET session tidb_enable_index_merge = ON;\n```\n\n## 示例\n\n```sql\nCREATE TABLE t(a int, b int, c int, d int, INDEX idx_a(a), INDEX idx_b(b), INDEX idx_c(c), INDEX idx_d(d));\n```\n\n```sql\nEXPLAIN SELECT /*+ NO_INDEX_MERGE() */ * FROM t WHERE a = 1 OR b = 1;\n+-------------------------+----------+-----------+---------------+--------------------------------------+\n| id                      | estRows  | task      | access object | operator info                        |\n+-------------------------+----------+-----------+---------------+--------------------------------------+\n| TableReader_7           | 19.99    | root      |               | data:Selection_6                     |\n| └─Selection_6           | 19.99    | cop[tikv] |               | or(eq(test.t.a, 1), eq(test.t.b, 1)) |\n|   └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo       |\n+-------------------------+----------+-----------+---------------+--------------------------------------+\nEXPLAIN SELECT /*+ USE_INDEX_MERGE(t) */ * FROM t WHERE a > 1 OR b > 1;\n+-------------------------------+---------+-----------+-------------------------+------------------------------------------------+\n| id                            | estRows | task      | access object           | operator info                                  |\n+-------------------------------+---------+-----------+-------------------------+------------------------------------------------+\n| IndexMerge_8                  | 5555.56 | root      |                         | type: union                                    |\n| ├─IndexRangeScan_5(Build)     | 3333.33 | cop[tikv] | table:t, index:idx_a(a) | range:(1,+inf], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_6(Build)     | 3333.33 | cop[tikv] | table:t, index:idx_b(b) | range:(1,+inf], keep order:false, stats:pseudo |\n| └─TableRowIDScan_7(Probe)     | 5555.56 | cop[tikv] | table:t                 | keep order:false, stats:pseudo                 |\n+-------------------------------+---------+-----------+-------------------------+------------------------------------------------+\n```\n\n例如，在上述示例中，过滤条件是使用 `OR` 连接的 `WHERE` 子句。在启用索引合并前，每个表只能使用一个索引，不能将 `a = 1` 下推到索引 `a`，也不能将 `b = 1` 下推到索引 `b`。当 `t` 中存在大量数据时，全表扫描的效率会很低。\n\n对于以上查询语句，优化器选择了并集型索引合并的方式访问表。在这种访问方式下，优化器可以选择对一张表使用多个索引，并将每个索引的返回结果进行合并，生成以上两个示例中的后一个执行计划。此时的 `IndexMerge_8` 算子的 `operator info` 中的 `type: union` 表示该算子是一个并集型索引合并算子。它有三个子节点，其中 `IndexRangeScan_5` 和 `IndexRangeScan_6` 根据范围扫描得到符合条件的所有 `RowID`，再由 `TableRowIDScan_7` 算子根据这些 `RowID` 精确地读取所有满足条件的数据。\n\n其中对于 `IndexRangeScan`/`TableRangeScan` 一类按范围进行的扫表操作，`EXPLAIN` 表中 `operator info` 列相比于其他扫表操作，多了被扫描数据的范围这一信息。比如上面的例子中，`IndexRangeScan_5` 算子中的 `range:(1,+inf]` 这一信息表示该算子扫描了从 1 到正无穷这个范围的数据。\n\n```sql\nEXPLAIN SELECT /*+ NO_INDEX_MERGE() */ * FROM t WHERE a > 1 AND b > 1 AND c = 1;  -- 不使用索引合并\n\n+--------------------------------+---------+-----------+-------------------------+---------------------------------------------+\n| id                             | estRows | task      | access object           | operator info                               |\n+--------------------------------+---------+-----------+-------------------------+---------------------------------------------+\n| IndexLookUp_19                 | 1.11    | root      |                         |                                             |\n| ├─IndexRangeScan_16(Build)     | 10.00   | cop[tikv] | table:t, index:idx_c(c) | range:[1,1], keep order:false, stats:pseudo |\n| └─Selection_18(Probe)          | 1.11    | cop[tikv] |                         | gt(test.t.a, 1), gt(test.t.b, 1)            |\n|   └─TableRowIDScan_17          | 10.00   | cop[tikv] | table:t                 | keep order:false, stats:pseudo              |\n+--------------------------------+---------+-----------+-------------------------+---------------------------------------------+\n\nEXPLAIN SELECT /*+ USE_INDEX_MERGE(t, idx_a, idx_b, idx_c) */ * FROM t WHERE a > 1 AND b > 1 AND c = 1;  -- 使用索引合并\n\n+-------------------------------+---------+-----------+-------------------------+------------------------------------------------+\n| id                            | estRows | task      | access object           | operator info                                  |\n+-------------------------------+---------+-----------+-------------------------+------------------------------------------------+\n| IndexMerge_9                  | 1.11    | root      |                         | type: intersection                             |\n| ├─IndexRangeScan_5(Build)     | 3333.33 | cop[tikv] | table:t, index:idx_a(a) | range:(1,+inf], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_6(Build)     | 3333.33 | cop[tikv] | table:t, index:idx_b(b) | range:(1,+inf], keep order:false, stats:pseudo |\n| ├─IndexRangeScan_7(Build)     | 10.00   | cop[tikv] | table:t, index:idx_c(c) | range:[1,1], keep order:false, stats:pseudo    |\n| └─TableRowIDScan_8(Probe)     | 1.11    | cop[tikv] | table:t                 | keep order:false, stats:pseudo                 |\n+-------------------------------+---------+-----------+-------------------------+------------------------------------------------+\n```\n\n在如上示例中，过滤条件是使用 `AND` 连接的 `WHERE` 子句。在启用索引合并前，只能选择使用 `idx_a`、`idx_b` 或 `idx_c` 三个索引中的一个。\n\n如果三个过滤条件中的其中一个的过滤性非常好，直接选择对应的索引即可达到理想的执行效率。但如果数据分布同时满足以下三种情形，可以考虑使用交集型索引合并：\n\n- 全表的数据量相当大，导致直接读全表的执行效率非常低下\n- 每个过滤条件单独的过滤性都不够好，导致 `IndexLookUp` 使用单个索引的执行效率也不够理想\n- 三个过滤条件整体的过滤性非常好\n\n在交集型索引合并访问方式下，优化器可以选择对一张表使用多个索引，并将每个索引的返回结果取交集，生成以上两个示例中的后一个执行计划。此时的 `IndexMerge_9` 算子的 `operator info` 中的 `type: intersection` 表示该算子是一个交集型索引合并算子。该执行计划的其它部分和上述并集型索引合并示例类似。\n\n> **注意：**\n>\n> - TiDB 的索引合并特性在 v5.4.0 及之后的版本默认开启，即 [`tidb_enable_index_merge`](/system-variables.md#tidb_enable_index_merge-从-v40-版本开始引入) 为 `ON`。\n> - 如果查询中使用了 SQL 优化器 Hint [`USE_INDEX_MERGE`](/optimizer-hints.md#use_index_merget1_name-idx1_name--idx2_name-)，无论 `tidb_enable_index_merge` 开关是否开启，都会强制使用索引合并特性。当过滤条件中有无法下推的表达式时，必须使用 Hint [`USE_INDEX_MERGE`](/optimizer-hints.md#use_index_merget1_name-idx1_name--idx2_name-) 才能开启索引合并。\n> - 如果查询有除了全表扫描以外的单索引扫描方式可以选择，优化器不会自动选择索引合并，只能通过 Hint 指定使用索引合并。从 v8.1.0 开始，这个限制可以通过 [Optimizer Fix Control 52869](/optimizer-fix-controls.md#52869-从-v810-版本开始引入) 解除。解除此限制能让优化器在更多查询中自动选择索引合并，但也有可能忽略其他更好的执行计划，因此建议在解除此限制前针对实际场景进行充分测试，确保不会带来性能回退。\n> - 索引合并目前无法在临时表上使用。\n> - 交集型索引合并目前不会被优化器自动选择，必须使用 [`USE_INDEX_MERGE`](/optimizer-hints.md#use_index_merget1_name-idx1_name--idx2_name-) Hint 指定**表名和索引名**时才会被选择。\n\n## 其他类型查询的执行计划\n\n+ [MPP 模式查询的执行计划](/explain-mpp.md)\n+ [索引查询的执行计划](/explain-indexes.md)\n+ [Join 查询的执行计划](/explain-joins.md)\n+ [子查询的执行计划](/explain-subqueries.md)\n+ [聚合查询的执行计划](/explain-aggregation.md)\n+ [视图查询的执行计划](/explain-views.md)\n+ [分区查询的执行计划](/explain-partitions.md)\n"
        },
        {
          "name": "explain-indexes.md",
          "type": "blob",
          "size": 23.3076171875,
          "content": "---\ntitle: 用 EXPLAIN 查看索引查询的执行计划\nsummary: 了解 TiDB 中 EXPLAIN 语句返回的执行计划信息。\n---\n\n# 用 EXPLAIN 查看索引查询的执行计划\n\nSQL 查询可能会使用索引，可以通过 `EXPLAIN` 语句来查看索引查询的执行计划。本文提供多个示例，以帮助用户理解索引查询是如何执行的。\n\nTiDB 支持以下使用索引的算子来提升查询速度：\n\n+ [`IndexLookup`](#indexlookup)\n+ [`IndexReader`](#indexreader)\n+ [`Point_Get` 和 `Batch_Point_Get`](#point_get-和-batch_point_get)\n+ [`IndexFullScan`](#indexfullscan)\n\n本文档中的示例都基于以下数据：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n id INT NOT NULL PRIMARY KEY auto_increment,\n intkey INT NOT NULL,\n pad1 VARBINARY(1024),\n INDEX (intkey)\n);\n\nINSERT INTO t1 SELECT NULL, FLOOR(RAND()*1024), RANDOM_BYTES(1024) FROM dual;\nINSERT INTO t1 SELECT NULL, FLOOR(RAND()*1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, FLOOR(RAND()*1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, FLOOR(RAND()*1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\n```\n\n## IndexLookup\n\nTiDB 从二级索引检索数据时会使用 `IndexLookup` 算子。例如，以下所有查询均会在 `intkey` 列的索引上使用 `IndexLookup` 算子：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT * FROM t1 WHERE intkey = 123;\nEXPLAIN SELECT * FROM t1 WHERE intkey < 10;\nEXPLAIN SELECT * FROM t1 WHERE intkey BETWEEN 300 AND 310;\nEXPLAIN SELECT * FROM t1 WHERE intkey IN (123,29,98);\nEXPLAIN SELECT * FROM t1 WHERE intkey >= 99 AND intkey <= 103;\n```\n\n```sql\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| id                            | estRows | task      | access object                  | operator info                     |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| IndexLookUp_10                | 1.00    | root      |                                |                                   |\n| ├─IndexRangeScan_8(Build)     | 1.00    | cop[tikv] | table:t1, index:intkey(intkey) | range:[123,123], keep order:false |\n| └─TableRowIDScan_9(Probe)     | 1.00    | cop[tikv] | table:t1                       | keep order:false                  |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n3 rows in set (0.00 sec)\n\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| id                            | estRows | task      | access object                  | operator info                     |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| IndexLookUp_10                | 3.60    | root      |                                |                                   |\n| ├─IndexRangeScan_8(Build)     | 3.60    | cop[tikv] | table:t1, index:intkey(intkey) | range:[-inf,10), keep order:false |\n| └─TableRowIDScan_9(Probe)     | 3.60    | cop[tikv] | table:t1                       | keep order:false                  |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n3 rows in set (0.00 sec)\n\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| id                            | estRows | task      | access object                  | operator info                     |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| IndexLookUp_10                | 5.67    | root      |                                |                                   |\n| ├─IndexRangeScan_8(Build)     | 5.67    | cop[tikv] | table:t1, index:intkey(intkey) | range:[300,310], keep order:false |\n| └─TableRowIDScan_9(Probe)     | 5.67    | cop[tikv] | table:t1                       | keep order:false                  |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n3 rows in set (0.00 sec)\n\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------------------------+\n| id                            | estRows | task      | access object                  | operator info                                       |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------------------------+\n| IndexLookUp_10                | 4.00    | root      |                                |                                                     |\n| ├─IndexRangeScan_8(Build)     | 4.00    | cop[tikv] | table:t1, index:intkey(intkey) | range:[29,29], [98,98], [123,123], keep order:false |\n| └─TableRowIDScan_9(Probe)     | 4.00    | cop[tikv] | table:t1                       | keep order:false                                    |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------------------------+\n3 rows in set (0.00 sec)\n\n+-------------------------------+---------+-----------+--------------------------------+----------------------------------+\n| id                            | estRows | task      | access object                  | operator info                    |\n+-------------------------------+---------+-----------+--------------------------------+----------------------------------+\n| IndexLookUp_10                | 6.00    | root      |                                |                                  |\n| ├─IndexRangeScan_8(Build)     | 6.00    | cop[tikv] | table:t1, index:intkey(intkey) | range:[99,103], keep order:false |\n| └─TableRowIDScan_9(Probe)     | 6.00    | cop[tikv] | table:t1                       | keep order:false                 |\n+-------------------------------+---------+-----------+--------------------------------+----------------------------------+\n3 rows in set (0.00 sec)\n```\n\n`IndexLookup` 算子有以下两个子节点：\n\n* `├─IndexRangeScan_8(Build)` 算子节点对 `intkey` 列的索引执行范围扫描，并检索内部的 `RowID` 值（对此表而言，即为主键）。\n* `└─TableRowIDScan_9(Probe)` 算子节点随后从表数据中检索整行。\n\n`IndexLookup` 任务分以上两步执行。如果满足条件的行较多，SQL 优化器可能会根据[常规统计信息](/statistics.md)选择使用 `TableFullScan` 算子。在以下示例中，很多行都满足 `intkey > 100` 这一条件，因此优化器选择了 `TableFullScan`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT * FROM t1 WHERE intkey > 100;\n```\n\n```sql\n+-------------------------+---------+-----------+---------------+-------------------------+\n| id                      | estRows | task      | access object | operator info           |\n+-------------------------+---------+-----------+---------------+-------------------------+\n| TableReader_7           | 898.50  | root      |               | data:Selection_6        |\n| └─Selection_6           | 898.50  | cop[tikv] |               | gt(test.t1.intkey, 100) |\n|   └─TableFullScan_5     | 1010.00 | cop[tikv] | table:t1      | keep order:false        |\n+-------------------------+---------+-----------+---------------+-------------------------+\n3 rows in set (0.00 sec)\n```\n\n`IndexLookup` 算子能在带索引的列上有效优化 `LIMIT`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT * FROM t1 ORDER BY intkey DESC LIMIT 10;\n```\n\n```sql\n+--------------------------------+---------+-----------+--------------------------------+------------------------------------+\n| id                             | estRows | task      | access object                  | operator info                      |\n+--------------------------------+---------+-----------+--------------------------------+------------------------------------+\n| IndexLookUp_21                 | 10.00   | root      |                                | limit embedded(offset:0, count:10) |\n| ├─Limit_20(Build)              | 10.00   | cop[tikv] |                                | offset:0, count:10                 |\n| │ └─IndexFullScan_18           | 10.00   | cop[tikv] | table:t1, index:intkey(intkey) | keep order:true, desc              |\n| └─TableRowIDScan_19(Probe)     | 10.00   | cop[tikv] | table:t1                       | keep order:false, stats:pseudo     |\n+--------------------------------+---------+-----------+--------------------------------+------------------------------------+\n4 rows in set (0.00 sec)\n```\n\n以上示例中，TiDB 从 `intkey` 索引读取最后 10 行，然后从表数据中检索这些行的 `RowID` 值。\n\n## IndexReader\n\nTiDB 支持覆盖索引优化 (covering index optimization)。如果 TiDB 能从索引中检索出所有行，就会跳过 `IndexLookup` 任务中通常所需的第二步（即从表数据中检索整行）。示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT * FROM t1 WHERE intkey = 123;\nEXPLAIN SELECT id FROM t1 WHERE intkey = 123;\n```\n\n```sql\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| id                            | estRows | task      | access object                  | operator info                     |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| IndexLookUp_10                | 1.00    | root      |                                |                                   |\n| ├─IndexRangeScan_8(Build)     | 1.00    | cop[tikv] | table:t1, index:intkey(intkey) | range:[123,123], keep order:false |\n| └─TableRowIDScan_9(Probe)     | 1.00    | cop[tikv] | table:t1                       | keep order:false                  |\n+-------------------------------+---------+-----------+--------------------------------+-----------------------------------+\n3 rows in set (0.00 sec)\n\n+--------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| id                       | estRows | task      | access object                  | operator info                     |\n+--------------------------+---------+-----------+--------------------------------+-----------------------------------+\n| Projection_4             | 1.00    | root      |                                | test.t1.id                        |\n| └─IndexReader_6          | 1.00    | root      |                                | index:IndexRangeScan_5            |\n|   └─IndexRangeScan_5     | 1.00    | cop[tikv] | table:t1, index:intkey(intkey) | range:[123,123], keep order:false |\n+--------------------------+---------+-----------+--------------------------------+-----------------------------------+\n3 rows in set (0.00 sec)\n```\n\n以上结果中，`id` 也是内部的 `RowID` 值，因此 `id` 也存储在 `intkey` 索引中。部分 `└─IndexRangeScan_5` 任务使用 `intkey` 索引后，可直接返回 `RowID` 值。\n\n## Point_Get 和 Batch_Point_Get\n\nTiDB 直接从主键或唯一键检索数据时会使用 `Point_Get` 或 `Batch_Point_Get` 算子。这两个算子比 `IndexLookup` 更有效率。示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT * FROM t1 WHERE id = 1234;\nEXPLAIN SELECT * FROM t1 WHERE id IN (1234,123);\n\nALTER TABLE t1 ADD unique_key INT;\nUPDATE t1 SET unique_key = id;\nALTER TABLE t1 ADD UNIQUE KEY (unique_key);\n\nEXPLAIN SELECT * FROM t1 WHERE unique_key = 1234;\nEXPLAIN SELECT * FROM t1 WHERE unique_key IN (1234, 123);\n```\n\n```sql\n+-------------+---------+------+---------------+---------------+\n| id          | estRows | task | access object | operator info |\n+-------------+---------+------+---------------+---------------+\n| Point_Get_1 | 1.00    | root | table:t1      | handle:1234   |\n+-------------+---------+------+---------------+---------------+\n1 row in set (0.00 sec)\n\n+-------------------+---------+------+---------------+-------------------------------------------------+\n| id                | estRows | task | access object | operator info                                   |\n+-------------------+---------+------+---------------+-------------------------------------------------+\n| Batch_Point_Get_1 | 2.00    | root | table:t1      | handle:[1234 123], keep order:false, desc:false |\n+-------------------+---------+------+---------------+-------------------------------------------------+\n1 row in set (0.00 sec)\n\nQuery OK, 0 rows affected (0.27 sec)\n\nQuery OK, 1010 rows affected (0.06 sec)\nRows matched: 1010  Changed: 1010  Warnings: 0\n\nQuery OK, 0 rows affected (0.37 sec)\n\n+-------------+---------+------+----------------------------------------+---------------+\n| id          | estRows | task | access object                          | operator info |\n+-------------+---------+------+----------------------------------------+---------------+\n| Point_Get_1 | 1.00    | root | table:t1, index:unique_key(unique_key) |               |\n+-------------+---------+------+----------------------------------------+---------------+\n1 row in set (0.00 sec)\n\n+-------------------+---------+------+----------------------------------------+------------------------------+\n| id                | estRows | task | access object                          | operator info                |\n+-------------------+---------+------+----------------------------------------+------------------------------+\n| Batch_Point_Get_1 | 2.00    | root | table:t1, index:unique_key(unique_key) | keep order:false, desc:false |\n+-------------------+---------+------+----------------------------------------+------------------------------+\n1 row in set (0.00 sec)\n```\n\n## IndexFullScan\n\n索引是有序的，所以优化器可以使用 `IndexFullScan` 算子来优化常见的查询，例如在索引值上使用 `MIN` 或 `MAX` 函数：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT MIN(intkey) FROM t1;\nEXPLAIN SELECT MAX(intkey) FROM t1;\n```\n\n```sql\n+------------------------------+---------+-----------+--------------------------------+-------------------------------------+\n| id                           | estRows | task      | access object                  | operator info                       |\n+------------------------------+---------+-----------+--------------------------------+-------------------------------------+\n| StreamAgg_12                 | 1.00    | root      |                                | funcs:min(test.t1.intkey)->Column#4 |\n| └─Limit_16                   | 1.00    | root      |                                | offset:0, count:1                   |\n|   └─IndexReader_29           | 1.00    | root      |                                | index:Limit_28                      |\n|     └─Limit_28               | 1.00    | cop[tikv] |                                | offset:0, count:1                   |\n|       └─IndexFullScan_27     | 1.00    | cop[tikv] | table:t1, index:intkey(intkey) | keep order:true                     |\n+------------------------------+---------+-----------+--------------------------------+-------------------------------------+\n5 rows in set (0.00 sec)\n\n+------------------------------+---------+-----------+--------------------------------+-------------------------------------+\n| id                           | estRows | task      | access object                  | operator info                       |\n+------------------------------+---------+-----------+--------------------------------+-------------------------------------+\n| StreamAgg_12                 | 1.00    | root      |                                | funcs:max(test.t1.intkey)->Column#4 |\n| └─Limit_16                   | 1.00    | root      |                                | offset:0, count:1                   |\n|   └─IndexReader_29           | 1.00    | root      |                                | index:Limit_28                      |\n|     └─Limit_28               | 1.00    | cop[tikv] |                                | offset:0, count:1                   |\n|       └─IndexFullScan_27     | 1.00    | cop[tikv] | table:t1, index:intkey(intkey) | keep order:true, desc               |\n+------------------------------+---------+-----------+--------------------------------+-------------------------------------+\n5 rows in set (0.00 sec)\n```\n\n以上语句的执行过程中，TiDB 在每一个 TiKV Region 上执行 `IndexFullScan` 操作。虽然算子名为 `FullScan` 即全扫描，TiDB 只读取第一行 (`└─Limit_28`)。每个 TiKV Region 返回各自的 `MIN` 或 `MAX` 值给 TiDB，TiDB 再执行流聚合运算来过滤出一行数据。即使表为空，带 `MAX` 或 `MIN` 函数的流聚合运算也能保证返回 `NULL` 值。\n\n相反，在没有索引的值上执行 `MIN` 函数会在每一个 TiKV Region 上执行 `TableFullScan` 操作。该查询会要求在 TiKV 中扫描所有行，但 `TopN` 计算可保证每个 TiKV Region 只返回一行数据给 TiDB。尽管 `TopN` 能减少 TiDB 和 TiKV 之间的多余数据传输，但该查询的效率仍远不及以上示例（`MIN` 能够使用索引）。\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT MIN(pad1) FROM t1;\n```\n\n```sql\n+--------------------------------+---------+-----------+---------------+-----------------------------------+\n| id                             | estRows | task      | access object | operator info                     |\n+--------------------------------+---------+-----------+---------------+-----------------------------------+\n| StreamAgg_13                   | 1.00    | root      |               | funcs:min(test.t1.pad1)->Column#4 |\n| └─TopN_14                      | 1.00    | root      |               | test.t1.pad1, offset:0, count:1   |\n|   └─TableReader_23             | 1.00    | root      |               | data:TopN_22                      |\n|     └─TopN_22                  | 1.00    | cop[tikv] |               | test.t1.pad1, offset:0, count:1   |\n|       └─Selection_21           | 1008.99 | cop[tikv] |               | not(isnull(test.t1.pad1))         |\n|         └─TableFullScan_20     | 1010.00 | cop[tikv] | table:t1      | keep order:false                  |\n+--------------------------------+---------+-----------+---------------+-----------------------------------+\n6 rows in set (0.00 sec)\n```\n\n执行以下语句时，TiDB 将使用 `IndexFullScan` 算子扫描索引中的每一行：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT SUM(intkey) FROM t1;\nEXPLAIN SELECT AVG(intkey) FROM t1;\n```\n\n```sql\n+----------------------------+---------+-----------+--------------------------------+-------------------------------------+\n| id                         | estRows | task      | access object                  | operator info                       |\n+----------------------------+---------+-----------+--------------------------------+-------------------------------------+\n| StreamAgg_20               | 1.00    | root      |                                | funcs:sum(Column#6)->Column#4       |\n| └─IndexReader_21           | 1.00    | root      |                                | index:StreamAgg_8                   |\n|   └─StreamAgg_8            | 1.00    | cop[tikv] |                                | funcs:sum(test.t1.intkey)->Column#6 |\n|     └─IndexFullScan_19     | 1010.00 | cop[tikv] | table:t1, index:intkey(intkey) | keep order:false                    |\n+----------------------------+---------+-----------+--------------------------------+-------------------------------------+\n4 rows in set (0.00 sec)\n\n+----------------------------+---------+-----------+--------------------------------+----------------------------------------------------------------------------+\n| id                         | estRows | task      | access object                  | operator info                                                              |\n+----------------------------+---------+-----------+--------------------------------+----------------------------------------------------------------------------+\n| StreamAgg_20               | 1.00    | root      |                                | funcs:avg(Column#7, Column#8)->Column#4                                    |\n| └─IndexReader_21           | 1.00    | root      |                                | index:StreamAgg_8                                                          |\n|   └─StreamAgg_8            | 1.00    | cop[tikv] |                                | funcs:count(test.t1.intkey)->Column#7, funcs:sum(test.t1.intkey)->Column#8 |\n|     └─IndexFullScan_19     | 1010.00 | cop[tikv] | table:t1, index:intkey(intkey) | keep order:false                                                           |\n+----------------------------+---------+-----------+--------------------------------+----------------------------------------------------------------------------+\n4 rows in set (0.00 sec)\n```\n\n以上示例中，`IndexFullScan` 比 `TableFullScan` 更有效率，因为 `(intkey + RowID)` 索引中值的长度小于整行的长度。\n\n以下语句不支持使用 `IndexFullScan` 算子，因为涉及该表中的其他列：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT AVG(intkey), ANY_VALUE(pad1) FROM t1;\n```\n\n```sql\n+------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------------------------------+\n| id                           | estRows | task      | access object | operator info                                                                                                         |\n+------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------------------------------+\n| Projection_4                 | 1.00    | root      |               | Column#4, any_value(test.t1.pad1)->Column#5                                                                           |\n| └─StreamAgg_16               | 1.00    | root      |               | funcs:avg(Column#10, Column#11)->Column#4, funcs:firstrow(Column#12)->test.t1.pad1                                    |\n|   └─TableReader_17           | 1.00    | root      |               | data:StreamAgg_8                                                                                                      |\n|     └─StreamAgg_8            | 1.00    | cop[tikv] |               | funcs:count(test.t1.intkey)->Column#10, funcs:sum(test.t1.intkey)->Column#11, funcs:firstrow(test.t1.pad1)->Column#12 |\n|       └─TableFullScan_15     | 1010.00 | cop[tikv] | table:t1      | keep order:false                                                                                                      |\n+------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------------------------------+\n5 rows in set (0.00 sec)\n```\n\n## 其他类型查询的执行计划\n\n+ [MPP 模式查询的执行计划](/explain-mpp.md)\n+ [Join 查询的执行计划](/explain-joins.md)\n+ [子查询的执行计划](/explain-subqueries.md)\n+ [聚合查询的执行计划](/explain-aggregation.md)\n+ [视图查询的执行计划](/explain-views.md)\n+ [分区查询的执行计划](/explain-partitions.md)\n+ [索引合并查询的执行计划](/explain-index-merge.md)\n"
        },
        {
          "name": "explain-joins.md",
          "type": "blob",
          "size": 54.2060546875,
          "content": "---\ntitle: 用 EXPLAIN 查看 JOIN 查询的执行计划\nsummary: 了解 TiDB 中 EXPLAIN 语句返回的执行计划信息。\n---\n\n# 用 EXPLAIN 查看 JOIN 查询的执行计划\n\nSQL 查询中可能会使用 JOIN 进行表连接，可以通过 `EXPLAIN` 语句来查看 JOIN 查询的执行计划。本文提供多个示例，以帮助用户理解表连接查询是如何执行的。\n\n在 TiDB 中，SQL 优化器需要确定数据表的连接顺序，且要判断对于某条特定的 SQL 语句，哪一种 Join 算法最为高效。\n\n本文档使用的示例数据如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (id BIGINT NOT NULL PRIMARY KEY auto_increment, pad1 BLOB, pad2 BLOB, pad3 BLOB, int_col INT NOT NULL DEFAULT 0);\nCREATE TABLE t2 (id BIGINT NOT NULL PRIMARY KEY auto_increment, t1_id BIGINT NOT NULL, pad1 BLOB, pad2 BLOB, pad3 BLOB, INDEX(t1_id));\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM dual;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nUPDATE t1 SET int_col = 1 WHERE pad1 = (SELECT pad1 FROM t1 ORDER BY RAND() LIMIT 1);\nSELECT SLEEP(1);\nANALYZE TABLE t1, t2;\n```\n\n## Index Join\n\n如果预计需要连接的行数较少（一般小于 1 万行），推荐使用 Index Join 算法。这个算法与 MySQL 主要使用的 Join 算法类似。在下表的示例中，`TableReader_29(Build)` 算子首先读取表 `t1`，然后根据在 `t1` 中匹配到的每行数据，依次探查表 `t2` 中的数据：\n\n> **注意：**\n>\n> 在执行计划返回结果中，自 v6.4.0 版本起，特定算子（即 `IndexJoin` 和 `Apply` 算子的 Probe 端所有子节点）的 `estRows` 字段意义与 v6.4.0 版本之前的有所不同。细节请参考 [TiDB 执行计划概览](/explain-overview.md#解读-explain-的返回结果)。\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT /*+ INL_JOIN(t1, t2) */ * FROM t1 INNER JOIN t2 ON t1.id = t2.t1_id;\n```\n\n```sql\n+---------------------------------+----------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n| id                              | estRows  | task      | access object                | operator info                                                                                                             |\n+---------------------------------+----------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n| IndexJoin_11                    | 90000.00 | root      |                              | inner join, inner:IndexLookUp_10, outer key:test.t1.id, inner key:test.t2.t1_id, equal cond:eq(test.t1.id, test.t2.t1_id) |\n| ├─TableReader_29(Build)         | 71010.00 | root      |                              | data:TableFullScan_28                                                                                                     |\n| │ └─TableFullScan_28            | 71010.00 | cop[tikv] | table:t1                     | keep order:false                                                                                                          |\n| └─IndexLookUp_10(Probe)         | 90000.00 | root      |                              |                                                                                                                           |\n|   ├─IndexRangeScan_8(Build)     | 90000.00 | cop[tikv] | table:t2, index:t1_id(t1_id) | range: decided by [eq(test.t2.t1_id, test.t1.id)], keep order:false                                                       |\n|   └─TableRowIDScan_9(Probe)     | 90000.00 | cop[tikv] | table:t2                     | keep order:false                                                                                                          |\n+---------------------------------+----------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n```\n\nIndex Join 算法对内存消耗较小，但如果需要执行大量探查操作，运行速度可能会慢于其他 Join 算法。以下面这条查询语句为例：\n\n```sql\nSELECT * FROM t1 INNER JOIN t2 ON t1.id=t2.t1_id WHERE t1.pad1 = 'value' and t2.pad1='value';\n```\n\n在 Inner Join 操作中，TiDB 会先执行 Join Reorder 算法，所以不能确定会先读取 `t1` 还是 `t2`。假设 TiDB 先读取了 `t1` 来构建 Build 端，那么 TiDB 会在探查 `t2` 前先根据谓词 `t1.pad1 = 'value'` 筛选数据，但接下来每次探查 `t2` 时都要应用谓词 `t2.pad1='value'`。所以对于这条语句，Index Join 算法可能不如其他 Join 算法高效。\n\n但如果 Build 端的数据量比 Probe 端小，且 Probe 端的数据已预先建立了索引，那么这种情况下 Index Join 算法效率更高。在下面这段查询语句中，因为 Index Join 比 Hash Join 效率低，所以 SQL 优化器选择了 Hash Join 算法：\n\n{{< copyable \"sql\" >}}\n\n```sql\n-- 删除已有索引\nALTER TABLE t2 DROP INDEX t1_id;\n\nEXPLAIN ANALYZE SELECT /*+ INL_JOIN(t1, t2) */  * FROM t1 INNER JOIN t2 ON t1.id = t2.t1_id WHERE t1.int_col = 1;\nEXPLAIN ANALYZE SELECT /*+ HASH_JOIN(t1, t2) */  * FROM t1 INNER JOIN t2 ON t1.id = t2.t1_id WHERE t1.int_col = 1;\nEXPLAIN ANALYZE SELECT * FROM t1 INNER JOIN t2 ON t1.id = t2.t1_id WHERE t1.int_col = 1;\n```\n\n```sql\n+-----------------------------+----------+---------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------+------+\n| id                          | estRows  | actRows | task      | access object | execution info                                                                                                                                                                                                                                                                                                           | operator info                                                                                                             | memory  | disk |\n+-----------------------------+----------+---------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------+------+\n| IndexJoin_14                | 90000.00 | 0       | root      |               | time:330.2ms, loops:1, inner:{total:72.2ms, concurrency:5, task:12, construct:58.6ms, fetch:13.5ms, build:2.12µs}, probe:26.1ms                                                                                                                                                                                          | inner join, inner:TableReader_10, outer key:test.t2.t1_id, inner key:test.t1.id, equal cond:eq(test.t2.t1_id, test.t1.id) | 88.5 MB | N/A  |\n| ├─TableReader_20(Build)     | 90000.00 | 90000   | root      |               | time:307.2ms, loops:96, cop_task: {num: 24, max: 130.6ms, min: 170.9µs, avg: 33.5ms, p95: 105ms, max_proc_keys: 10687, p95_proc_keys: 9184, tot_proc: 472ms, rpc_num: 24, rpc_time: 802.4ms, copr_cache_hit_ratio: 0.62, distsql_concurrency: 15}                                                                        | data:TableFullScan_19                                                                                                     | 58.6 MB | N/A  |\n| │ └─TableFullScan_19        | 90000.00 | 90000   | cop[tikv] | table:t2      | tikv_task:{proc max:34ms, min:0s, avg: 15.3ms, p80:24ms, p95:30ms, iters:181, tasks:24}, scan_detail: {total_process_keys: 69744, total_process_keys_size: 217533936, total_keys: 69753, get_snapshot_time: 701.6µs, rocksdb: {delete_skipped_count: 97368, key_skipped_count: 236847, block: {cache_hit_count: 3509}}}  | keep order:false                                                                                                          | N/A     | N/A  |\n| └─TableReader_10(Probe)     | 12617.92 | 0       | root      |               | time:11.9ms, loops:12, cop_task: {num: 42, max: 848.8µs, min: 199µs, avg: 451.8µs, p95: 846.2µs, max_proc_keys: 7, p95_proc_keys: 5, rpc_num: 42, rpc_time: 18.3ms, copr_cache_hit_ratio: 0.00, distsql_concurrency: 15}                                                                                                 | data:Selection_9                                                                                                          | N/A     | N/A  |\n|   └─Selection_9             | 12617.92 | 0       | cop[tikv] |               | tikv_task:{proc max:0s, min:0s, avg: 0s, p80:0s, p95:0s, iters:42, tasks:42}, scan_detail: {total_process_keys: 56, total_process_keys_size: 174608, total_keys: 77, get_snapshot_time: 727.7µs, rocksdb: {block: {cache_hit_count: 154}}}                                                                               | eq(test.t1.int_col, 1)                                                                                                    | N/A     | N/A  |\n|     └─TableRangeScan_8      | 90000.00 | 56      | cop[tikv] | table:t1      | tikv_task:{proc max:0s, min:0s, avg: 0s, p80:0s, p95:0s, iters:42, tasks:42}                                                                                                                                                                                                                                             | range: decided by [test.t2.t1_id], keep order:false                                                                       | N/A     | N/A  |\n+-----------------------------+----------+---------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+---------+------+\n\n+------------------------------+----------+---------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n| id                           | estRows  | actRows | task      | access object | execution info                                                                                                                                                                                                                                                                                                         | operator info                                     | memory  | disk    |\n+------------------------------+----------+---------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n| HashJoin_20                  | 90000.00 | 0       | root      |               | time:313.6ms, loops:1, build_hash_table:{total:24.6ms, fetch:21.2ms, build:3.32ms}, probe:{concurrency:5, total:1.57s, max:313.5ms, probe:18.9ms, fetch:1.55s}                                                                                                                                                         | inner join, equal:[eq(test.t1.id, test.t2.t1_id)] | 32.0 MB | 0 Bytes |\n| ├─TableReader_23(Build)      | 9955.54  | 10000   | root      |               | time:23.6ms, loops:12, cop_task: {num: 11, max: 504.6µs, min: 203.7µs, avg: 377.4µs, p95: 504.6µs, rpc_num: 11, rpc_time: 3.92ms, copr_cache_hit_ratio: 1.00, distsql_concurrency: 15}                                                                                                                                 | data:Selection_22                                 | 14.9 MB | N/A     |\n| │ └─Selection_22             | 9955.54  | 10000   | cop[tikv] |               | tikv_task:{proc max:104ms, min:3ms, avg: 24.4ms, p80:33ms, p95:104ms, iters:113, tasks:11}, scan_detail: {get_snapshot_time: 241.4µs, rocksdb: {block: {}}}                                                                                                                                                            | eq(test.t1.int_col, 1)                            | N/A     | N/A     |\n| │   └─TableFullScan_21       | 71010.00 | 71010   | cop[tikv] | table:t1      | tikv_task:{proc max:101ms, min:3ms, avg: 23.8ms, p80:33ms, p95:101ms, iters:113, tasks:11}                                                                                                                                                                                                                             | keep order:false                                  | N/A     | N/A     |\n| └─TableReader_25(Probe)      | 90000.00 | 90000   | root      |               | time:293.7ms, loops:91, cop_task: {num: 24, max: 105.7ms, min: 210.9µs, avg: 31.4ms, p95: 103.8ms, max_proc_keys: 10687, p95_proc_keys: 9184, tot_proc: 407ms, rpc_num: 24, rpc_time: 752.2ms, copr_cache_hit_ratio: 0.62, distsql_concurrency: 15}                                                                    | data:TableFullScan_24                             | 58.6 MB | N/A     |\n|   └─TableFullScan_24         | 90000.00 | 90000   | cop[tikv] | table:t2      | tikv_task:{proc max:31ms, min:0s, avg: 13ms, p80:19ms, p95:26ms, iters:181, tasks:24}, scan_detail: {total_process_keys: 69744, total_process_keys_size: 217533936, total_keys: 69753, get_snapshot_time: 637.2µs, rocksdb: {delete_skipped_count: 97368, key_skipped_count: 236847, block: {cache_hit_count: 3509}}}  | keep order:false                                  | N/A     | N/A     |\n+------------------------------+----------+---------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n\n+------------------------------+----------+---------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n| id                           | estRows  | actRows | task      | access object | execution info                                                                                                                                                                                                                                                                                                           | operator info                                     | memory  | disk    |\n+------------------------------+----------+---------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n| HashJoin_21                  | 90000.00 | 0       | root      |               | time:331.7ms, loops:1, build_hash_table:{total:32.7ms, fetch:26ms, build:6.73ms}, probe:{concurrency:5, total:1.66s, max:331.3ms, probe:16ms, fetch:1.64s}                                                                                                                                                               | inner join, equal:[eq(test.t1.id, test.t2.t1_id)] | 32.3 MB | 0 Bytes |\n| ├─TableReader_26(Build)      | 9955.54  | 10000   | root      |               | time:30.4ms, loops:13, cop_task: {num: 11, max: 1.87ms, min: 844.7µs, avg: 1.29ms, p95: 1.87ms, rpc_num: 11, rpc_time: 13.5ms, copr_cache_hit_ratio: 1.00, distsql_concurrency: 15}                                                                                                                                      | data:Selection_25                                 | 12.2 MB | N/A     |\n| │ └─Selection_25             | 9955.54  | 10000   | cop[tikv] |               | tikv_task:{proc max:104ms, min:3ms, avg: 24.4ms, p80:33ms, p95:104ms, iters:113, tasks:11}, scan_detail: {get_snapshot_time: 521µs, rocksdb: {block: {}}}                                                                                                                                                                | eq(test.t1.int_col, 1)                            | N/A     | N/A     |\n| │   └─TableFullScan_24       | 71010.00 | 71010   | cop[tikv] | table:t1      | tikv_task:{proc max:101ms, min:3ms, avg: 23.8ms, p80:33ms, p95:101ms, iters:113, tasks:11}                                                                                                                                                                                                                               | keep order:false                                  | N/A     | N/A     |\n| └─TableReader_23(Probe)      | 90000.00 | 90000   | root      |               | time:308.6ms, loops:91, cop_task: {num: 24, max: 123.3ms, min: 518.9µs, avg: 32.4ms, p95: 113.4ms, max_proc_keys: 10687, p95_proc_keys: 9184, tot_proc: 499ms, rpc_num: 24, rpc_time: 776ms, copr_cache_hit_ratio: 0.62, distsql_concurrency: 15}                                                                        | data:TableFullScan_22                             | 58.6 MB | N/A     |\n|   └─TableFullScan_22         | 90000.00 | 90000   | cop[tikv] | table:t2      | tikv_task:{proc max:44ms, min:0s, avg: 16.8ms, p80:27ms, p95:40ms, iters:181, tasks:24}, scan_detail: {total_process_keys: 69744, total_process_keys_size: 217533936, total_keys: 69753, get_snapshot_time: 955.4µs, rocksdb: {delete_skipped_count: 97368, key_skipped_count: 236847, block: {cache_hit_count: 3509}}}  | keep order:false                                  | N/A     | N/A     |\n+------------------------------+----------+---------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n```\n\n在上面所示的 Index Join 操作中，`t1.int_col` 一列的索引被删除了。如果加上这个索引，操作执行速度可以从 `0.3 秒` 提高到 `0.06 秒`，如下表所示：\n\n```sql\n-- 重新添加索引\nALTER TABLE t2 ADD INDEX (t1_id);\n\nEXPLAIN ANALYZE SELECT /*+ INL_JOIN(t1, t2) */  * FROM t1 INNER JOIN t2 ON t1.id = t2.t1_id WHERE t1.int_col = 1;\nEXPLAIN ANALYZE SELECT /*+ HASH_JOIN(t1, t2) */  * FROM t1 INNER JOIN t2 ON t1.id = t2.t1_id WHERE t1.int_col = 1;\nEXPLAIN ANALYZE SELECT * FROM t1 INNER JOIN t2 ON t1.id = t2.t1_id WHERE t1.int_col = 1;\n```\n\n```sql\n+----------------------------------+----------+---------+-----------+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+-----------+------+\n| id                               | estRows  | actRows | task      | access object                | execution info                                                                                                                                                                                                                                                                                                                                                                               | operator info                                                                                                             | memory    | disk |\n+----------------------------------+----------+---------+-----------+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+-----------+------+\n| IndexJoin_12                     | 90000.00 | 0       | root      |                              | time:65.6ms, loops:1, inner:{total:129.7ms, concurrency:5, task:7, construct:7.13ms, fetch:122.5ms, build:16.4µs}, probe:2.54ms                                                                                                                                                                                                                                                              | inner join, inner:IndexLookUp_11, outer key:test.t1.id, inner key:test.t2.t1_id, equal cond:eq(test.t1.id, test.t2.t1_id) | 28.7 MB   | N/A  |\n| ├─TableReader_33(Build)          | 9955.54  | 10000   | root      |                              | time:15.4ms, loops:16, cop_task: {num: 11, max: 1.52ms, min: 211.5µs, avg: 416.8µs, p95: 1.52ms, rpc_num: 11, rpc_time: 4.36ms, copr_cache_hit_ratio: 1.00, distsql_concurrency: 15}                                                                                                                                                                                                         | data:Selection_32                                                                                                         | 13.9 MB   | N/A  |\n| │ └─Selection_32                 | 9955.54  | 10000   | cop[tikv] |                              | tikv_task:{proc max:104ms, min:3ms, avg: 24.4ms, p80:33ms, p95:104ms, iters:113, tasks:11}, scan_detail: {get_snapshot_time: 185µs, rocksdb: {block: {}}}                                                                                                                                                                                                                                    | eq(test.t1.int_col, 1)                                                                                                    | N/A       | N/A  |\n| │   └─TableFullScan_31           | 71010.00 | 71010   | cop[tikv] | table:t1                     | tikv_task:{proc max:101ms, min:3ms, avg: 23.8ms, p80:33ms, p95:101ms, iters:113, tasks:11}                                                                                                                                                                                                                                                                                                   | keep order:false                                                                                                          | N/A       | N/A  |\n| └─IndexLookUp_11(Probe)          | 90000.00 | 0       | root      |                              | time:115.6ms, loops:7                                                                                                                                                                                                                                                                                                                                                                        |                                                                                                                           | 555 Bytes | N/A  |\n|   ├─IndexRangeScan_9(Build)      | 90000.00 | 0       | cop[tikv] | table:t2, index:t1_id(t1_id) | time:114.3ms, loops:7, cop_task: {num: 7, max: 42ms, min: 1.3ms, avg: 16.2ms, p95: 42ms, tot_proc: 71ms, rpc_num: 7, rpc_time: 113.2ms, copr_cache_hit_ratio: 0.29, distsql_concurrency: 15}, tikv_task:{proc max:37ms, min:0s, avg: 11.3ms, p80:20ms, p95:37ms, iters:7, tasks:7}, scan_detail: {total_keys: 9296, get_snapshot_time: 141.9µs, rocksdb: {block: {cache_hit_count: 18592}}}  | range: decided by [eq(test.t2.t1_id, test.t1.id)], keep order:false                                                       | N/A       | N/A  |\n|   └─TableRowIDScan_10(Probe)     | 90000.00 | 0       | cop[tikv] | table:t2                     |                                                                                                                                                                                                                                                                                                                                                                                              | keep order:false                                                                                                          | N/A       | N/A  |\n+----------------------------------+----------+---------+-----------+------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------+-----------+------+\n\n+------------------------------+----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n| id                           | estRows  | actRows | task      | access object | execution info                                                                                                                                                                                                                                                                                                             | operator info                                     | memory  | disk    |\n+------------------------------+----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n| HashJoin_32                  | 90000.00 | 0       | root      |               | time:320.2ms, loops:1, build_hash_table:{total:19.3ms, fetch:16.8ms, build:2.52ms}, probe:{concurrency:5, total:1.6s, max:320.1ms, probe:16.1ms, fetch:1.58s}                                                                                                                                                              | inner join, equal:[eq(test.t1.id, test.t2.t1_id)] | 32.0 MB | 0 Bytes |\n| ├─TableReader_35(Build)      | 9955.54  | 10000   | root      |               | time:18.6ms, loops:12, cop_task: {num: 11, max: 713.8µs, min: 197.3µs, avg: 368.5µs, p95: 713.8µs, rpc_num: 11, rpc_time: 3.83ms, copr_cache_hit_ratio: 1.00, distsql_concurrency: 15}                                                                                                                                     | data:Selection_34                                 | 14.9 MB | N/A     |\n| │ └─Selection_34             | 9955.54  | 10000   | cop[tikv] |               | tikv_task:{proc max:104ms, min:3ms, avg: 24.4ms, p80:33ms, p95:104ms, iters:113, tasks:11}, scan_detail: {get_snapshot_time: 178.9µs, rocksdb: {block: {}}}                                                                                                                                                                | eq(test.t1.int_col, 1)                            | N/A     | N/A     |\n| │   └─TableFullScan_33       | 71010.00 | 71010   | cop[tikv] | table:t1      | tikv_task:{proc max:101ms, min:3ms, avg: 23.8ms, p80:33ms, p95:101ms, iters:113, tasks:11}                                                                                                                                                                                                                                 | keep order:false                                  | N/A     | N/A     |\n| └─TableReader_37(Probe)      | 90000.00 | 90000   | root      |               | time:304.4ms, loops:91, cop_task: {num: 24, max: 114ms, min: 251.1µs, avg: 33.1ms, p95: 110.4ms, max_proc_keys: 10687, p95_proc_keys: 9184, tot_proc: 492ms, rpc_num: 24, rpc_time: 793ms, copr_cache_hit_ratio: 0.62, distsql_concurrency: 15}                                                                            | data:TableFullScan_36                             | 58.6 MB | N/A     |\n|   └─TableFullScan_36         | 90000.00 | 90000   | cop[tikv] | table:t2      | tikv_task:{proc max:38ms, min:3ms, avg: 14.1ms, p80:23ms, p95:35ms, iters:181, tasks:24}, scan_detail: {total_process_keys: 69744, total_process_keys_size: 217533936, total_keys: 139497, get_snapshot_time: 577.2µs, rocksdb: {delete_skipped_count: 44208, key_skipped_count: 253431, block: {cache_hit_count: 3527}}}  | keep order:false                                  | N/A     | N/A     |\n+------------------------------+----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n\n+------------------------------+----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n| id                           | estRows  | actRows | task      | access object | execution info                                                                                                                                                                                                                                                                                                             | operator info                                     | memory  | disk    |\n+------------------------------+----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n| HashJoin_33                  | 90000.00 | 0       | root      |               | time:306.3ms, loops:1, build_hash_table:{total:20.5ms, fetch:17.1ms, build:3.45ms}, probe:{concurrency:5, total:1.53s, max:305.9ms, probe:17.1ms, fetch:1.51s}                                                                                                                                                             | inner join, equal:[eq(test.t1.id, test.t2.t1_id)] | 32.0 MB | 0 Bytes |\n| ├─TableReader_42(Build)      | 9955.54  | 10000   | root      |               | time:19.6ms, loops:12, cop_task: {num: 11, max: 1.07ms, min: 246.1µs, avg: 600µs, p95: 1.07ms, rpc_num: 11, rpc_time: 6.17ms, copr_cache_hit_ratio: 1.00, distsql_concurrency: 15}                                                                                                                                         | data:Selection_41                                 | 19.7 MB | N/A     |\n| │ └─Selection_41             | 9955.54  | 10000   | cop[tikv] |               | tikv_task:{proc max:104ms, min:3ms, avg: 24.4ms, p80:33ms, p95:104ms, iters:113, tasks:11}, scan_detail: {get_snapshot_time: 282.9µs, rocksdb: {block: {}}}                                                                                                                                                                | eq(test.t1.int_col, 1)                            | N/A     | N/A     |\n| │   └─TableFullScan_40       | 71010.00 | 71010   | cop[tikv] | table:t1      | tikv_task:{proc max:101ms, min:3ms, avg: 23.8ms, p80:33ms, p95:101ms, iters:113, tasks:11}                                                                                                                                                                                                                                 | keep order:false                                  | N/A     | N/A     |\n| └─TableReader_44(Probe)      | 90000.00 | 90000   | root      |               | time:289.2ms, loops:91, cop_task: {num: 24, max: 108.2ms, min: 252.8µs, avg: 31.3ms, p95: 106.1ms, max_proc_keys: 10687, p95_proc_keys: 9184, tot_proc: 445ms, rpc_num: 24, rpc_time: 750.4ms, copr_cache_hit_ratio: 0.62, distsql_concurrency: 15}                                                                        | data:TableFullScan_43                             | 58.6 MB | N/A     |\n|   └─TableFullScan_43         | 90000.00 | 90000   | cop[tikv] | table:t2      | tikv_task:{proc max:31ms, min:3ms, avg: 13.3ms, p80:24ms, p95:30ms, iters:181, tasks:24}, scan_detail: {total_process_keys: 69744, total_process_keys_size: 217533936, total_keys: 139497, get_snapshot_time: 730.2µs, rocksdb: {delete_skipped_count: 44208, key_skipped_count: 253431, block: {cache_hit_count: 3527}}}  | keep order:false                                  | N/A     | N/A     |\n+------------------------------+----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------+---------+---------+\n```\n\n> **注意：**\n>\n> 在上方示例中，SQL 优化器之所以选择了性能较差的 Hash Join 算法，而不是 Index Join 算法，原因在于查询优化是一个 [NP 完全问题](https://zh.wikipedia.org/wiki/NP%E5%AE%8C%E5%85%A8)，可能会选择不太理想的计划。如果需要频繁调用这个查询，建议通过[执行计划管理](/sql-plan-management.md)的方式将 Hint 与 SQL 语句绑定，这样要比在发送给 TiDB 的 SQL 语句中插入 Hint 更容易管理。\n\n### Index Join 相关算法\n\n如果使用 Hint [`INL_JOIN`](/optimizer-hints.md#inl_joint1_name--tl_name-) 进行 Index Join 操作，TiDB 会在连接外表之前创建一个中间结果的 Hash Table。TiDB 同样也支持使用 Hint [`INL_HASH_JOIN`](/optimizer-hints.md#inl_hash_join) 在外表上建 Hash Table。以上所述的 Index Join 相关算法都由 SQL 优化器自动选择。\n\n### 配置\n\nIndex Join 算法的性能受以下系统变量影响：\n\n* [`tidb_index_join_batch_size`](/system-variables.md#tidb_index_join_batch_size)（默认值：`25000`）- `index lookup join` 操作的 batch 大小。\n* [`tidb_index_lookup_join_concurrency`](/system-variables.md#tidb_index_lookup_join_concurrency)（默认值：`4`）- 可以并发执行的 index lookup 任务数。\n\n## Hash Join\n\n在 Hash Join 操作中，TiDB 首先读取 Build 端的数据并将其缓存在 Hash Table 中，然后再读取 Probe 端的数据，使用 Probe 端的数据来探查 Hash Table 以获得所需行。与 Index Join 算法相比，Hash Join 要消耗更多内存，但如果需要连接的行数很多，运行速度会比 Index Join 快。TiDB 中的 Hash Join 算子是多线程的，并且可以并发执行。\n\n下面是一个 Hash Join 示例：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT /*+ HASH_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n```sql\n+-----------------------------+-----------+-----------+---------------+------------------------------------------------+\n| id                          | estRows   | task      | access object | operator info                                  |\n+-----------------------------+-----------+-----------+---------------+------------------------------------------------+\n| HashJoin_27                 | 142020.00 | root      |               | inner join, equal:[eq(test.t1.id, test.t2.id)] |\n| ├─TableReader_29(Build)     | 142020.00 | root      |               | data:TableFullScan_28                          |\n| │ └─TableFullScan_28        | 142020.00 | cop[tikv] | table:t1      | keep order:false                               |\n| └─TableReader_31(Probe)     | 180000.00 | root      |               | data:TableFullScan_30                          |\n|   └─TableFullScan_30        | 180000.00 | cop[tikv] | table:t2      | keep order:false                               |\n+-----------------------------+-----------+-----------+---------------+------------------------------------------------+\n5 rows in set (0.00 sec)\n```\n\nTiDB 会按照以下顺序执行 `HashJoin_27` 算子：\n\n1. 将 Build 端数据缓存在内存中。\n2. 根据缓存数据在 Build 端构造一个 Hash Table。\n3. 读取 Probe 端的数据。\n4. 使用 Probe 端的数据来探查 Hash Table。\n5. 将符合条件的结果返回给用户。\n\n`EXPLAIN` 返回结果中的 `operator info` 一列记录了 `HashJoin_27` 的其他信息，包括该查询是 Inner Join 还是 Outer Join 以及 Join 的条件是什么等。在上面给出的示例中，该查询为 Inner Join，Join 条件是 `equal:[eq(test.t1.id, test.t2.id)]`，与查询语句中的 `WHERE t1.id = t2.id` 部分对应。下面例子中其他几个 Join 算子的 operator info 和此处类似。\n\n### 运行数据\n\n如果在执行操作时，内存使用超过了 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 规定的值（默认为 1 GB），且 [`tidb_enable_tmp_storage_on_oom`](/system-variables.md#tidb_enable_tmp_storage_on_oom) 的值为 `ON` （默认为 `ON`），那么 TiDB 会尝试使用临时存储，在磁盘上创建 Hash Join 的 Build 端。`EXPLAIN ANALYZE` 返回结果中的 `execution info` 一栏记录了有关内存使用情况等运行数据。下面的例子展示了 `tidb_mem_quota_query` 的值分别设为 1 GB（默认）及 500 MB 时，`EXPLAIN ANALYZE` 的返回结果（当内存配额设为 500 MB 时，磁盘用作临时存储区）：\n\n```sql\nEXPLAIN ANALYZE SELECT /*+ HASH_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\nSET tidb_mem_quota_query=500 * 1024 * 1024;\nEXPLAIN ANALYZE SELECT /*+ HASH_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n```sql\n+-----------------------------+-----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------+---------+\n| id                          | estRows   | actRows | task      | access object | execution info                                                                                                                                                                                                                                           | operator info                                  | memory                | disk    |\n+-----------------------------+-----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------+---------+\n| HashJoin_27                 | 142020.00 | 71010   | root      |               | time:647.508572ms, loops:72, build_hash_table:{total:579.254415ms, fetch:566.91012ms, build:12.344295ms}, probe:{concurrency:5, total:3.23315006s, max:647.520113ms, probe:330.884716ms, fetch:2.902265344s}                                             | inner join, equal:[eq(test.t1.id, test.t2.id)] | 209.61642456054688 MB | 0 Bytes |\n| ├─TableReader_29(Build)     | 142020.00 | 71010   | root      |               | time:567.088247ms, loops:72, cop_task: {num: 2, max: 569.809411ms, min: 369.67451ms, avg: 469.74196ms, p95: 569.809411ms, max_proc_keys: 39245, p95_proc_keys: 39245, tot_proc: 400ms, rpc_num: 2, rpc_time: 939.447231ms, copr_cache_hit_ratio: 0.00}   | data:TableFullScan_28                          | 210.2100534439087 MB  | N/A     |\n| │ └─TableFullScan_28        | 142020.00 | 71010   | cop[tikv] | table:t1      | proc max:64ms, min:48ms, p80:64ms, p95:64ms, iters:79, tasks:2                                                                                                                                                                                           | keep order:false                               | N/A                   | N/A     |\n| └─TableReader_31(Probe)     | 180000.00 | 90000   | root      |               | time:337.233636ms, loops:91, cop_task: {num: 3, max: 569.790741ms, min: 332.758911ms, avg: 421.543165ms, p95: 569.790741ms, max_proc_keys: 31719, p95_proc_keys: 31719, tot_proc: 500ms, rpc_num: 3, rpc_time: 1.264570696s, copr_cache_hit_ratio: 0.00} | data:TableFullScan_30                          | 267.1126985549927 MB  | N/A     |\n|   └─TableFullScan_30        | 180000.00 | 90000   | cop[tikv] | table:t2      | proc max:84ms, min:72ms, p80:84ms, p95:84ms, iters:102, tasks:3                                                                                                                                                                                          | keep order:false                               | N/A                   | N/A     |\n+-----------------------------+-----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------+---------+\n5 rows in set (0.65 sec)\n\nQuery OK, 0 rows affected (0.00 sec)\n\n+-----------------------------+-----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------+----------------------+\n| id                          | estRows   | actRows | task      | access object | execution info                                                                                                                                                                                                                                           | operator info                                  | memory                | disk                 |\n+-----------------------------+-----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------+----------------------+\n| HashJoin_27                 | 142020.00 | 71010   | root      |               | time:963.983353ms, loops:72, build_hash_table:{total:775.961447ms, fetch:503.789677ms, build:272.17177ms}, probe:{concurrency:5, total:4.805454793s, max:963.973133ms, probe:922.156835ms, fetch:3.883297958s}                                           | inner join, equal:[eq(test.t1.id, test.t2.id)] | 93.53974533081055 MB  | 210.7459259033203 MB |\n| ├─TableReader_29(Build)     | 142020.00 | 71010   | root      |               | time:504.062018ms, loops:72, cop_task: {num: 2, max: 509.276857ms, min: 402.66386ms, avg: 455.970358ms, p95: 509.276857ms, max_proc_keys: 39245, p95_proc_keys: 39245, tot_proc: 384ms, rpc_num: 2, rpc_time: 911.893237ms, copr_cache_hit_ratio: 0.00}  | data:TableFullScan_28                          | 210.20934200286865 MB | N/A                  |\n| │ └─TableFullScan_28        | 142020.00 | 71010   | cop[tikv] | table:t1      | proc max:88ms, min:72ms, p80:88ms, p95:88ms, iters:79, tasks:2                                                                                                                                                                                           | keep order:false                               | N/A                   | N/A                  |\n| └─TableReader_31(Probe)     | 180000.00 | 90000   | root      |               | time:363.058382ms, loops:91, cop_task: {num: 3, max: 412.659191ms, min: 358.489688ms, avg: 391.463008ms, p95: 412.659191ms, max_proc_keys: 31719, p95_proc_keys: 31719, tot_proc: 484ms, rpc_num: 3, rpc_time: 1.174326746s, copr_cache_hit_ratio: 0.00} | data:TableFullScan_30                          | 267.11340618133545 MB | N/A                  |\n|   └─TableFullScan_30        | 180000.00 | 90000   | cop[tikv] | table:t2      | proc max:92ms, min:64ms, p80:92ms, p95:92ms, iters:102, tasks:3                                                                                                                                                                                          | keep order:false                               | N/A                   | N/A                  |\n+-----------------------------+-----------+---------+-----------+---------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------+-----------------------+----------------------+\n5 rows in set (0.98 sec)\n```\n\n### 配置\n\nHash Join 算法的性能受以下系统变量影响：\n\n* [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query)（默认值：1GB）- 如果某条查询的内存消耗超出了配额，TiDB 会尝试将 Hash Join 的 Build 端移到磁盘上以节省内存。\n* [`tidb_hash_join_concurrency`](/system-variables.md#tidb_hash_join_concurrency)（默认值：`5`）- 可以并发执行的 Hash Join 任务数量。\n\n### 相关优化\n\nTiDB 提供了 Runtime Filter 功能，针对 Hash Join 进行性能优化，大幅提升 Hash Join 的执行速度。具体优化使用方式见 [Runtime Filter](/runtime-filter.md)。\n\n## Merge Join\n\nMerge Join 是一种特殊的 Join 算法。当两个关联表要 Join 的字段需要按排好的顺序读取时，适用 Merge Join 算法。由于 Build 端和 Probe 端的数据都会读取，这种算法的 Join 操作是流式的，类似“拉链式合并”的高效版。Merge Join 占用的内存要远低于 Hash Join，但 Merge Join 不能并发执行。\n\n下面是一个使用 Merge Join 的例子：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT /*+ MERGE_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n```sql\n+-----------------------------+-----------+-----------+---------------+-------------------------------------------------------+\n| id                          | estRows   | task      | access object | operator info                                         |\n+-----------------------------+-----------+-----------+---------------+-------------------------------------------------------+\n| MergeJoin_7                 | 142020.00 | root      |               | inner join, left key:test.t1.id, right key:test.t2.id |\n| ├─TableReader_12(Build)     | 180000.00 | root      |               | data:TableFullScan_11                                 |\n| │ └─TableFullScan_11        | 180000.00 | cop[tikv] | table:t2      | keep order:true                                       |\n| └─TableReader_10(Probe)     | 142020.00 | root      |               | data:TableFullScan_9                                  |\n|   └─TableFullScan_9         | 142020.00 | cop[tikv] | table:t1      | keep order:true                                       |\n+-----------------------------+-----------+-----------+---------------+-------------------------------------------------------+\n5 rows in set (0.00 sec)\n```\n\nTiDB 会按照以下顺序执行 Merge Join 算子：\n\n1. 从 Build 端把一个 Join Group 的数据全部读取到内存中。\n2. 读取 Probe 端的数据。\n3. 将 Probe 端的每行数据与 Build 端的一个完整 Join Group 比较，依次查看是否匹配（除了满足等值条件以外，还有其他非等值条件，这里的“匹配”主要是指查看是否满足非等值条件）。Join Group 指的是所有 Join Key 上值相同的数据。\n\n## 其他类型查询的执行计划\n\n+ [MPP 模式查询的执行计划](/explain-mpp.md)\n+ [索引查询的执行计划](/explain-indexes.md)\n+ [子查询的执行计划](/explain-subqueries.md)\n+ [聚合查询的执行计划](/explain-aggregation.md)\n+ [视图查询的执行计划](/explain-views.md)\n+ [分区查询的执行计划](/explain-partitions.md)\n+ [索引合并查询的执行计划](/explain-index-merge.md)\n"
        },
        {
          "name": "explain-mpp.md",
          "type": "blob",
          "size": 20.2939453125,
          "content": "---\ntitle: 用 EXPLAIN 查看 MPP 模式查询的执行计划\nsummary: 了解 TiDB 中 EXPLAIN 语句返回的执行计划信息。\n---\n\n# 用 EXPLAIN 查看 MPP 模式查询的执行计划\n\nTiDB 支持使用 [MPP 模式](/tiflash/use-tiflash-mpp-mode.md)来执行查询。在 MPP 执行模式下，SQL 优化器会生成 MPP 的执行计划。注意 MPP 模式仅对有 [TiFlash](/tiflash/tiflash-overview.md) 副本的表生效。\n\n本文档使用的示例数据如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (id int, value int);\nINSERT INTO t1 values(1,2),(2,3),(1,3);\nALTER TABLE t1 set tiflash replica 1;\nANALYZE TABLE t1;\nSET tidb_allow_mpp = 1;\n```\n\n## MPP 查询片段和 MPP 任务\n\n在 MPP 模式下，一个查询在逻辑上会被切分为多个 MPP 查询片段 (query fragment)。示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT COUNT(*) FROM t1 GROUP BY id;\n```\n\n这个查询在 MPP 模式下会包含两个查询片段，一个为一阶段聚合，一个为二阶段聚合（最终聚合）。在查询执行的时候每个查询片段都会被实例化为一个或者多个 MPP 任务。\n\n## Exchange 算子\n\nMPP 查询的执行计划中有两个 MPP 特有的 Exchange 算子，分别为 ExchangeReceiver 和 ExchangeSender。ExchangeReceiver 表示从下游查询片段读取数据，ExchangeSender 表示下游查询片段向上游查询片段发送数据。在 MPP 执行模式下，每个 MPP 查询片段的根算子均为 ExchangeSender 算子，即每个查询片段以 ExchangeSender 为界进行划分。一个简单的 MPP 计划如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT COUNT(*) FROM t1 GROUP BY id;\n```\n\n```sql\n+------------------------------------+---------+-------------------+---------------+----------------------------------------------------+\n| id                                 | estRows | task              | access object | operator info                                      |\n+------------------------------------+---------+-------------------+---------------+----------------------------------------------------+\n| TableReader_31                     | 2.00    | root              |               | data:ExchangeSender_30                             |\n| └─ExchangeSender_30                | 2.00    | batchCop[tiflash] |               | ExchangeType: PassThrough                          |\n|   └─Projection_26                  | 2.00    | batchCop[tiflash] |               | Column#4                                           |\n|     └─HashAgg_27                   | 2.00    | batchCop[tiflash] |               | group by:test.t1.id, funcs:sum(Column#7)->Column#4 |\n|       └─ExchangeReceiver_29        | 2.00    | batchCop[tiflash] |               |                                                    |\n|         └─ExchangeSender_28        | 2.00    | batchCop[tiflash] |               | ExchangeType: HashPartition, Hash Cols: test.t1.id |\n|           └─HashAgg_9              | 2.00    | batchCop[tiflash] |               | group by:test.t1.id, funcs:count(1)->Column#7      |\n|             └─TableFullScan_25     | 3.00    | batchCop[tiflash] | table:t1      | keep order:false                                   |\n+------------------------------------+---------+-------------------+---------------+----------------------------------------------------+\n```\n\n以上执行计划中有两个查询片段：\n\n* `[TableFullScan_25, HashAgg_9, ExchangeSender_28]` 为第一个查询片段，其主要完成一阶段聚合的计算。\n* `[ExchangeReceiver_29, HashAgg_27, Projection_26, ExchangeSender_30]` 为第二个查询片段，其主要完成二阶段聚合的计算。\n\nExchangeSender 算子的 `operator info` 列输出了 ExchangeType 信息。目前有以下三种 ExchangeType：\n\n* HashPartition：ExchangeSender 把数据按 Hash 值进行分区之后分发给上游的 MPP 任务的 ExchangeReceiver 算子，通常在 Hash Aggregation 以及 Shuffle Hash Join 算法中使用。\n* Broadcast：ExchangeSender 通过广播的方式把数据分发给上游的 MPP 任务，通常在 Broadcast Join 中使用。\n* PassThrough：ExchangeSender 把数据分发给上游的 MPP Task，与 Broadcast 的区别是此时上游有且仅有一个 MPP 任务，通常用于向 TiDB 返回数据。\n\n上述例子中 ExchangeSender 的 ExchangeType 为 HashPartition 以及 PassThrough，分别对应于 Hash Aggregation 运算以及向 TiDB 返回数据。\n\n另外一个典型的 MPP 应用为 join 运算。TiDB MPP 支持两种类型的 join，分别为：\n\n* Shuffle Hash Join：join 的 input 通过 HashPartition 的方式 shuffle 数据，上游的 MPP 任务进行分区内的 join。 \n* Broadcast Join：join 中的小表以 Broadcast 的方式把数据广播到各个节点，各个节点各自进行 join。\n\n典型的 Shuffle Hash Join 执行计划如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET tidb_broadcast_join_threshold_count=0; SET tidb_broadcast_join_threshold_size=0; EXPLAIN SELECT COUNT(*) FROM t1 a JOIN t1 b ON a.id = b.id;\n```\n\n```sql\n+----------------------------------------+---------+--------------+---------------+----------------------------------------------------+\n| id                                     | estRows | task         | access object | operator info                                      |\n+----------------------------------------+---------+--------------+---------------+----------------------------------------------------+\n| StreamAgg_14                           | 1.00    | root         |               | funcs:count(1)->Column#7                           |\n| └─TableReader_48                       | 9.00    | root         |               | data:ExchangeSender_47                             |\n|   └─ExchangeSender_47                  | 9.00    | cop[tiflash] |               | ExchangeType: PassThrough                          |\n|     └─HashJoin_44                      | 9.00    | cop[tiflash] |               | inner join, equal:[eq(test.t1.id, test.t1.id)]     |\n|       ├─ExchangeReceiver_19(Build)     | 6.00    | cop[tiflash] |               |                                                    |\n|       │ └─ExchangeSender_18            | 6.00    | cop[tiflash] |               | ExchangeType: HashPartition, Hash Cols: test.t1.id |\n|       │   └─Selection_17               | 6.00    | cop[tiflash] |               | not(isnull(test.t1.id))                            |\n|       │     └─TableFullScan_16         | 6.00    | cop[tiflash] | table:a       | keep order:false                                   |\n|       └─ExchangeReceiver_23(Probe)     | 6.00    | cop[tiflash] |               |                                                    |\n|         └─ExchangeSender_22            | 6.00    | cop[tiflash] |               | ExchangeType: HashPartition, Hash Cols: test.t1.id |\n|           └─Selection_21               | 6.00    | cop[tiflash] |               | not(isnull(test.t1.id))                            |\n|             └─TableFullScan_20         | 6.00    | cop[tiflash] | table:b       | keep order:false                                   |\n+----------------------------------------+---------+--------------+---------------+----------------------------------------------------+\n12 rows in set (0.00 sec)\n```\n\n以上执行计划中，\n\n* `[TableFullScan_20, Selection_21, ExchangeSender_22]` 完成表 b 的数据读取并通过 HashPartition 的方式把数据 shuffle 给上游 MPP 任务。\n* `[TableFullScan_16, Selection_17, ExchangeSender_18]` 完成表 a 的数据读取并通过 HashPartition 的方式把数据 shuffle 给上游 MPP 任务。\n* `[ExchangeReceiver_19, ExchangeReceiver_23, HashJoin_44, ExchangeSender_47]` 完成 join 并把数据返回给 TiDB。\n\n典型的 Broadcast Join 执行计划如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT COUNT(*) FROM t1 a JOIN t1 b ON a.id = b.id;\n```\n\n```sql\n+----------------------------------------+---------+--------------+---------------+------------------------------------------------+\n| id                                     | estRows | task         | access object | operator info                                  |\n+----------------------------------------+---------+--------------+---------------+------------------------------------------------+\n| StreamAgg_15                           | 1.00    | root         |               | funcs:count(1)->Column#7                       |\n| └─TableReader_47                       | 9.00    | root         |               | data:ExchangeSender_46                         |\n|   └─ExchangeSender_46                  | 9.00    | cop[tiflash] |               | ExchangeType: PassThrough                      |\n|     └─HashJoin_43                      | 9.00    | cop[tiflash] |               | inner join, equal:[eq(test.t1.id, test.t1.id)] |\n|       ├─ExchangeReceiver_20(Build)     | 6.00    | cop[tiflash] |               |                                                |\n|       │ └─ExchangeSender_19            | 6.00    | cop[tiflash] |               | ExchangeType: Broadcast                        |\n|       │   └─Selection_18               | 6.00    | cop[tiflash] |               | not(isnull(test.t1.id))                        |\n|       │     └─TableFullScan_17         | 6.00    | cop[tiflash] | table:a       | keep order:false                               |\n|       └─Selection_22(Probe)            | 6.00    | cop[tiflash] |               | not(isnull(test.t1.id))                        |\n|         └─TableFullScan_21             | 6.00    | cop[tiflash] | table:b       | keep order:false                               |\n+----------------------------------------+---------+--------------+---------------+------------------------------------------------+\n```\n\n以上执行计划中，\n\n* `[TableFullScan_17, Selection_18, ExchangeSender_19]` 从小表（表 a）读数据并广播给大表（表 b）数据所在的各个节点。\n* `[TableFullScan_21, Selection_22, ExchangeReceiver_20, HashJoin_43, ExchangeSender_46]` 完成 join 并将数据返回给 TiDB。\n\n## 对 MPP 模式的查询使用 `EXPLAIN ANALYZE`\n\n`EXPLAIN ANALYZE` 语句与 `EXPLAIN` 类似，但还会输出一些运行时的信息。一个简单的 `EXPLAIN ANALYZE` 输出信息如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN ANALYZE SELECT COUNT(*) FROM t1 GROUP BY id;\n```\n\n```sql\n+------------------------------------+---------+---------+-------------------+---------------+---------------------------------------------------------------------------------------------------+----------------------------------------------------------------+--------+------+\n| id                                 | estRows | actRows | task              | access object | execution info                                                                                    | operator info                                                  | memory | disk |\n+------------------------------------+---------+---------+-------------------+---------------+---------------------------------------------------------------------------------------------------+----------------------------------------------------------------+--------+------+\n| TableReader_31                     | 4.00    | 2       | root              |               | time:44.5ms, loops:2, cop_task: {num: 1, max: 0s, proc_keys: 0, copr_cache_hit_ratio: 0.00}       | data:ExchangeSender_30                                         | N/A    | N/A  |\n| └─ExchangeSender_30                | 4.00    | 2       | batchCop[tiflash] |               | tiflash_task:{time:16.5ms, loops:1, threads:1}                                                    | ExchangeType: PassThrough, tasks: [2, 3, 4]                    | N/A    | N/A  |\n|   └─Projection_26                  | 4.00    | 2       | batchCop[tiflash] |               | tiflash_task:{time:16.5ms, loops:1, threads:1}                                                    | Column#4                                                       | N/A    | N/A  |\n|     └─HashAgg_27                   | 4.00    | 2       | batchCop[tiflash] |               | tiflash_task:{time:16.5ms, loops:1, threads:1}                                                    | group by:test.t1.id, funcs:sum(Column#7)->Column#4             | N/A    | N/A  |\n|       └─ExchangeReceiver_29        | 4.00    | 2       | batchCop[tiflash] |               | tiflash_task:{time:14.5ms, loops:1, threads:20}                                                   |                                                                | N/A    | N/A  |\n|         └─ExchangeSender_28        | 4.00    | 0       | batchCop[tiflash] |               | tiflash_task:{time:9.49ms, loops:0, threads:0}                                                    | ExchangeType: HashPartition, Hash Cols: test.t1.id, tasks: [1] | N/A    | N/A  |\n|           └─HashAgg_9              | 4.00    | 0       | batchCop[tiflash] |               | tiflash_task:{time:9.49ms, loops:0, threads:0}                                                    | group by:test.t1.id, funcs:count(1)->Column#7                  | N/A    | N/A  |\n|             └─TableFullScan_25     | 6.00    | 0       | batchCop[tiflash] | table:t1      | tiflash_task:{time:9.49ms, loops:0, threads:0}, tiflash_scan:{dtfile:{total_scanned_packs:1,...}} | keep order:false                                               | N/A    | N/A  |\n+------------------------------------+---------+---------+-------------------+---------------+---------------------------------------------------------------------------------------------------+----------------------------------------------------------------+--------+------+\n```\n\n与 `EXPLAIN` 相比，ExchangeSender 的 `operator info` 中多了 `task id` 的输出，其记录了该查询片段实例化成的 MPP 任务的任务 ID。此外 MPP 算子中都会有 `threads` 这一列，这列记录了 MPP 在执行该算子时使用的并发数（如果集群由多个节点组成，该并发数是所有节点并发数相加的结果）。\n\n## MPP Version 和 Exchange 数据压缩\n\n从 v6.6.0 开始，MPP 执行计划新增字段 `MppVersion` 和 `Compression`。\n\n- `MppVersion`：MPP 执行计划的版本号，可通过系统变量 [`mpp_version`](/system-variables.md#mpp_version-从-v660-版本开始引入) 设置。\n- `Compression`：`Exchange` 算子的数据压缩模式，可通过系统变量 [`mpp_exchange_compression_mode`](/system-variables.md#mpp_exchange_compression_mode-从-v660-版本开始引入) 设置。如果未启用数据压缩，则执行计划中不显示该字段。\n\n```sql\nmysql > EXPLAIN SELECT COUNT(*) AS count_order FROM lineitem GROUP BY l_returnflag, l_linestatus ORDER BY l_returnflag, l_linestatus;\n\n+----------------------------------------+--------------+--------------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id                                     | estRows      | task         | access object  | operator info                                                                                                                                                                                                                                                                        |\n+----------------------------------------+--------------+--------------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Projection_6                           | 3.00         | root         |                | Column#18                                                                                                                                                                                                                                                                            |\n| └─Sort_8                               | 3.00         | root         |                | tpch100.lineitem.l_returnflag, tpch100.lineitem.l_linestatus                                                                                                                                                                                                                         |\n|   └─TableReader_36                     | 3.00         | root         |                | MppVersion: 1, data:ExchangeSender_35                                                                                                                                                                                                                                                |\n|     └─ExchangeSender_35                | 3.00         | mpp[tiflash] |                | ExchangeType: PassThrough                                                                                                                                                                                                                                                            |\n|       └─Projection_31                  | 3.00         | mpp[tiflash] |                | Column#18, tpch100.lineitem.l_returnflag, tpch100.lineitem.l_linestatus                                                                                                                                                                                                              |\n|         └─HashAgg_32                   | 3.00         | mpp[tiflash] |                | group by:tpch100.lineitem.l_linestatus, tpch100.lineitem.l_returnflag, funcs:sum(Column#23)->Column#18, funcs:firstrow(tpch100.lineitem.l_returnflag)->tpch100.lineitem.l_returnflag, funcs:firstrow(tpch100.lineitem.l_linestatus)->tpch100.lineitem.l_linestatus, stream_count: 20 |\n|           └─ExchangeReceiver_34        | 3.00         | mpp[tiflash] |                | stream_count: 20                                                                                                                                                                                                                                                                     |\n|             └─ExchangeSender_33        | 3.00         | mpp[tiflash] |                | ExchangeType: HashPartition, Compression: FAST, Hash Cols: [name: tpch100.lineitem.l_returnflag, collate: utf8mb4_bin], [name: tpch100.lineitem.l_linestatus, collate: utf8mb4_bin], stream_count: 20                                                                                |\n|               └─HashAgg_14             | 3.00         | mpp[tiflash] |                | group by:tpch100.lineitem.l_linestatus, tpch100.lineitem.l_returnflag, funcs:count(1)->Column#23                                                                                                                                                                                     |\n|                 └─TableFullScan_30     | 600037902.00 | mpp[tiflash] | table:lineitem | keep order:false                                                                                                                                                                                                                                                                     |\n+----------------------------------------+--------------+--------------+----------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n\n例如上面的执行计划结果中，TiDB 使用了版本为 `1` 的 MPP 执行计划来构建 `TableReader`。其中类型为 `HashPartition` 的 ExchangeSender 算子使用 `FAST` 数据压缩模式，类型为 `PassThrough` 的 ExchangeSender 算子未启用数据压缩。\n\n## 其他类型查询的执行计划\n\n+ [索引查询的执行计划](/explain-indexes.md)\n+ [Join 查询的执行计划](/explain-joins.md)\n+ [子查询的执行计划](/explain-subqueries.md)\n+ [聚合查询的执行计划](/explain-aggregation.md)\n+ [视图查询的执行计划](/explain-views.md)\n+ [分区查询的执行计划](/explain-partitions.md)\n+ [索引合并查询的执行计划](/explain-index-merge.md)\n"
        },
        {
          "name": "explain-overview.md",
          "type": "blob",
          "size": 19.48046875,
          "content": "---\ntitle: TiDB 执行计划概览\nsummary: 了解 TiDB 中 EXPLAIN 语句返回的执行计划。\naliases: ['/docs-cn/dev/query-execution-plan/','/docs-cn/dev/reference/performance/understanding-the-query-execution-plan/','/docs-cn/dev/index-merge/','/docs-cn/dev/reference/performance/index-merge/','/zh/tidb/dev/query-execution-plan/']\n---\n\n# TiDB 执行计划概览\n\n> **注意：**\n>\n> 使用 MySQL 客户端连接到 TiDB 时，为避免输出结果在终端中换行，可先执行 `pager less -S` 命令。执行命令后，新的 `EXPLAIN` 的输出结果不再换行，可按右箭头 <kbd>→</kbd> 键水平滚动阅读输出结果。\n\n使用 `EXPLAIN` 可查看 TiDB 执行某条语句时选用的执行计划。也就是说，TiDB 在考虑上数百或数千种可能的执行计划后，最终认定该执行计划消耗的资源最少、执行的速度最快。\n\n`EXPLAIN` 示例如下：\n\n```sql\nCREATE TABLE t (id INT NOT NULL PRIMARY KEY auto_increment, a INT NOT NULL, pad1 VARCHAR(255), INDEX(a));\nINSERT INTO t VALUES (1, 1, 'aaa'),(2,2, 'bbb');\nEXPLAIN SELECT * FROM t WHERE a = 1;\n```\n\n返回的结果如下：\n\n```sql\nQuery OK, 0 rows affected (0.96 sec)\n\nQuery OK, 2 rows affected (0.02 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n\n+-------------------------------+---------+-----------+---------------------+---------------------------------------------+\n| id                            | estRows | task      | access object       | operator info                               |\n+-------------------------------+---------+-----------+---------------------+---------------------------------------------+\n| IndexLookUp_10                | 10.00   | root      |                     |                                             |\n| ├─IndexRangeScan_8(Build)     | 10.00   | cop[tikv] | table:t, index:a(a) | range:[1,1], keep order:false, stats:pseudo |\n| └─TableRowIDScan_9(Probe)     | 10.00   | cop[tikv] | table:t             | keep order:false, stats:pseudo              |\n+-------------------------------+---------+-----------+---------------------+---------------------------------------------+\n3 rows in set (0.00 sec)\n```\n\n`EXPLAIN` 实际不会执行查询。[`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md) 可用于实际执行查询并显示执行计划。如果 TiDB 所选的执行计划非最优，可用 `EXPLAIN` 或 `EXPLAIN ANALYZE` 来进行诊断。有关 `EXPLAIN` 用法的详细内容，参阅以下文档：\n\n+ [MPP 模式查询的执行计划](/explain-mpp.md)\n+ [索引查询的执行计划](/explain-indexes.md)\n+ [Join 查询的执行计划](/explain-joins.md)\n+ [子查询的执行计划](/explain-subqueries.md)\n+ [聚合查询的执行计划](/explain-aggregation.md)\n+ [视图查询的执行计划](/explain-views.md)\n+ [分区查询的执行计划](/explain-partitions.md)\n+ [索引合并查询的执行计划](/explain-index-merge.md)\n\n## 解读 EXPLAIN 的返回结果\n\n`EXPLAIN` 的返回结果包含以下字段：\n\n+ `id` 为算子名，或执行 SQL 语句需要执行的子任务。详见[算子简介](#算子简介)。\n+ `estRows` 为显示 TiDB 预计会处理的行数。该预估数可能基于字典信息（例如访问方法基于主键或唯一键），或基于 `CMSketch` 或直方图等统计信息估算而来。\n+ `task` 显示算子在执行语句时的所在位置。详见 [Task 简介](#task-简介)。\n+ `access-object` 显示被访问的表、分区和索引。显示的索引为部分索引。以上示例中 TiDB 使用了 `a` 列的索引。尤其是在有组合索引的情况下，该字段显示的信息很有参考意义。\n+ `operator info` 显示访问表、分区和索引的其他信息。详见 [`operator info` 结果](#operator-info-结果)。\n\n> **注意：**\n>\n> 在执行计划返回结果中，自 v6.4.0 版本起，特定算子（即 `IndexJoin` 和 `Apply` 算子的 Probe 端所有子节点）的 `estRows` 字段意义与 v6.4.0 版本之前的有所不同。\n>\n> 在 v6.4.0 之前，`estRows` 表示对于 Build 端子节点的每一行，Probe 端预计会处理的行数。自 v6.4.0 起，`estRows` 表示 Probe 端预计会处理的**总行数**。由于 `EXPLAIN ANALYZE` 中展示的实际行数（`actRows` 列）表示的是总行数，v6.4.0 起这些算子 `estRows` 的含义与 `actRows` 列的含义保持一致。\n>\n>\n> 例如：\n>\n> ```sql\n> CREATE TABLE t1(a INT, b INT);\n> CREATE TABLE t2(a INT, b INT, INDEX ia(a));\n> EXPLAIN SELECT /*+ INL_JOIN(t2) */ * FROM t1 JOIN t2 ON t1.a = t2.a;\n> EXPLAIN SELECT (SELECT a FROM t2 WHERE t2.a = t1.b LIMIT 1) FROM t1;\n> ```\n>\n> ```sql\n> -- v6.4.0 之前：\n> +---------------------------------+----------+-----------+-----------------------+-----------------------------------------------------------------------------------------------------------------+\n> | id                              | estRows  | task      | access object         | operator info                                                                                                   |\n> +---------------------------------+----------+-----------+-----------------------+-----------------------------------------------------------------------------------------------------------------+\n> | IndexJoin_12                    | 12487.50 | root      |                       | inner join, inner:IndexLookUp_11, outer key:test.t1.a, inner key:test.t2.a, equal cond:eq(test.t1.a, test.t2.a) |\n> | ├─TableReader_24(Build)         | 9990.00  | root      |                       | data:Selection_23                                                                                               |\n> | │ └─Selection_23                | 9990.00  | cop[tikv] |                       | not(isnull(test.t1.a))                                                                                          |\n> | │   └─TableFullScan_22          | 10000.00 | cop[tikv] | table:t1              | keep order:false, stats:pseudo                                                                                  |\n> | └─IndexLookUp_11(Probe)         | 1.25     | root      |                       |                                                                                                                 |\n> |   ├─Selection_10(Build)         | 1.25     | cop[tikv] |                       | not(isnull(test.t2.a))                                                                                          |\n> |   │ └─IndexRangeScan_8          | 1.25     | cop[tikv] | table:t2, index:ia(a) | range: decided by [eq(test.t2.a, test.t1.a)], keep order:false, stats:pseudo                                    |\n> |   └─TableRowIDScan_9(Probe)     | 1.25     | cop[tikv] | table:t2              | keep order:false, stats:pseudo                                                                                  |\n> +---------------------------------+----------+-----------+-----------------------+-----------------------------------------------------------------------------------------------------------------+\n> +---------------------------------+----------+-----------+-----------------------+------------------------------------------------------------------------------+\n> | id                              | estRows  | task      | access object         | operator info                                                                |\n> +---------------------------------+----------+-----------+-----------------------+------------------------------------------------------------------------------+\n> | Projection_12                   | 10000.00 | root      |                       | test.t2.a                                                                    |\n> | └─Apply_14                      | 10000.00 | root      |                       | CARTESIAN left outer join                                                    |\n> |   ├─TableReader_16(Build)       | 10000.00 | root      |                       | data:TableFullScan_15                                                        |\n> |   │ └─TableFullScan_15          | 10000.00 | cop[tikv] | table:t1              | keep order:false, stats:pseudo                                               |\n> |   └─Limit_17(Probe)             | 1.00     | root      |                       | offset:0, count:1                                                            |\n> |     └─IndexReader_21            | 1.00     | root      |                       | index:Limit_20                                                               |\n> |       └─Limit_20                | 1.00     | cop[tikv] |                       | offset:0, count:1                                                            |\n> |         └─IndexRangeScan_19     | 1.00     | cop[tikv] | table:t2, index:ia(a) | range: decided by [eq(test.t2.a, test.t1.b)], keep order:false, stats:pseudo |\n> +---------------------------------+----------+-----------+-----------------------+------------------------------------------------------------------------------+\n> \n> -- 自 v6.4.0 起：\n>\n> -- 可以发现 `IndexLookUp_11`、`Selection_10`、`IndexRangeScan_8` 和 `TableRowIDScan_9` 在 `estRows` 列显示的行数与 v6.4.0 以前的不同\n> +---------------------------------+----------+-----------+-----------------------+-----------------------------------------------------------------------------------------------------------------+\n> | id                              | estRows  | task      | access object         | operator info                                                                                                   |\n> +---------------------------------+----------+-----------+-----------------------+-----------------------------------------------------------------------------------------------------------------+\n> | IndexJoin_12                    | 12487.50 | root      |                       | inner join, inner:IndexLookUp_11, outer key:test.t1.a, inner key:test.t2.a, equal cond:eq(test.t1.a, test.t2.a) |\n> | ├─TableReader_24(Build)         | 9990.00  | root      |                       | data:Selection_23                                                                                               |\n> | │ └─Selection_23                | 9990.00  | cop[tikv] |                       | not(isnull(test.t1.a))                                                                                          |\n> | │   └─TableFullScan_22          | 10000.00 | cop[tikv] | table:t1              | keep order:false, stats:pseudo                                                                                  |\n> | └─IndexLookUp_11(Probe)         | 12487.50 | root      |                       |                                                                                                                 |\n> |   ├─Selection_10(Build)         | 12487.50 | cop[tikv] |                       | not(isnull(test.t2.a))                                                                                          |\n> |   │ └─IndexRangeScan_8          | 12500.00 | cop[tikv] | table:t2, index:ia(a) | range: decided by [eq(test.t2.a, test.t1.a)], keep order:false, stats:pseudo                                    |\n> |   └─TableRowIDScan_9(Probe)     | 12487.50 | cop[tikv] | table:t2              | keep order:false, stats:pseudo                                                                                  |\n> +---------------------------------+----------+-----------+-----------------------+-----------------------------------------------------------------------------------------------------------------+\n>\n> -- 可以发现 `Limit_17`、`IndexReader_21`、`Limit_20` 和 `IndexRangeScan_19` 在 `estRows` 列显示的行数与 v6.4.0 以前的不同\n> +---------------------------------+----------+-----------+-----------------------+------------------------------------------------------------------------------+\n> | id                              | estRows  | task      | access object         | operator info                                                                |\n> +---------------------------------+----------+-----------+-----------------------+------------------------------------------------------------------------------+\n> | Projection_12                   | 10000.00 | root      |                       | test.t2.a                                                                    |\n> | └─Apply_14                      | 10000.00 | root      |                       | CARTESIAN left outer join                                                    |\n> |   ├─TableReader_16(Build)       | 10000.00 | root      |                       | data:TableFullScan_15                                                        |\n> |   │ └─TableFullScan_15          | 10000.00 | cop[tikv] | table:t1              | keep order:false, stats:pseudo                                               |\n> |   └─Limit_17(Probe)             | 10000.00 | root      |                       | offset:0, count:1                                                            |\n> |     └─IndexReader_21            | 10000.00 | root      |                       | index:Limit_20                                                               |\n> |       └─Limit_20                | 10000.00 | cop[tikv] |                       | offset:0, count:1                                                            |\n> |         └─IndexRangeScan_19     | 10000.00 | cop[tikv] | table:t2, index:ia(a) | range: decided by [eq(test.t2.a, test.t1.b)], keep order:false, stats:pseudo |\n> +---------------------------------+----------+-----------+-----------------------+------------------------------------------------------------------------------+\n> ```\n\n### 算子简介\n\n算子是为返回查询结果而执行的特定步骤。真正执行扫表（读盘或者读 TiKV Block Cache）操作的算子有如下几类：\n\n- **TableFullScan**：全表扫描。\n- **TableRangeScan**：带有范围的表数据扫描。\n- **TableRowIDScan**：根据上层传递下来的 RowID 扫描表数据。时常在索引读操作后检索符合条件的行。\n- **IndexFullScan**：另一种“全表扫描”，扫的是索引数据，不是表数据。\n- **IndexRangeScan**：带有范围的索引数据扫描操作。\n\nTiDB 会汇聚 TiKV/TiFlash 上扫描的数据或者计算结果，这种“数据汇聚”算子目前有如下几类：\n\n- **TableReader**：将 TiKV 或 TiFlash 上底层算子得到的数据进行汇总。\n- **IndexReader**：将 TiKV 上底层扫表算子 IndexFullScan 或 IndexRangeScan 得到的数据进行汇总。\n- **IndexLookUp**：先汇总 Build 端 TiKV 扫描上来的 RowID，再去 Probe 端上根据这些 `RowID` 精确地读取 TiKV 上的数据。Build 端是 `IndexFullScan` 或 `IndexRangeScan` 类型的算子，Probe 端是 `TableRowIDScan` 类型的算子。\n- **IndexMerge**：和 `IndexLookupReader` 类似，可以看做是它的扩展，可以同时读取多个索引的数据，有多个 Build 端，一个 Probe 端。执行过程也很类似，先汇总所有 Build 端 TiKV 扫描上来的 RowID，再去 Probe 端上根据这些 RowID 精确地读取 TiKV 上的数据。Build 端是 `IndexFullScan` 或 `IndexRangeScan` 类型的算子，Probe 端是 `TableRowIDScan` 类型的算子。\n\n#### 算子的执行顺序\n\n算子的结构是树状的，但在查询执行过程中，并不严格要求子节点任务在父节点之前完成。TiDB 支持同一查询内的并行处理，即子节点“流入”父节点。父节点、子节点和同级节点可能并行执行查询的一部分。\n\n在以上示例中，`├─IndexRangeScan_8(Build)` 算子为 `a(a)` 索引所匹配的行查找内部 RowID。`└─TableRowIDScan_9(Probe)` 算子随后从表中检索这些行。\n\nBuild 总是先于 Probe 执行，并且 Build 总是出现在 Probe 前面。即如果一个算子有多个子节点，子节点 ID 后面有 Build 关键字的算子总是先于有 Probe 关键字的算子执行。TiDB 在展现执行计划的时候，Build 端总是第一个出现，接着才是 Probe 端。\n\n#### 范围查询\n\n在 `WHERE`/`HAVING`/`ON` 条件中，TiDB 优化器会分析主键或索引键的查询返回。如数字、日期类型的比较符，如大于、小于、等于以及大于等于、小于等于，字符类型的 `LIKE` 符号等。\n\n若要使用索引，条件必须是 \"Sargable\" (Search ARGument ABLE) 的。例如条件 `YEAR(date_column) < 1992` 不能使用索引，但 `date_column < '1992-01-01` 就可以使用索引。\n\n推荐使用同一类型的数据以及同一类型的[字符串和排序规则](/character-set-and-collation.md)进行比较，以避免引入额外的 `cast` 操作而导致不能利用索引。\n\n可以在范围查询条件中使用 `AND`（求交集）和 `OR`（求并集）进行组合。对于多维组合索引，可以对多个列使用条件。例如对组合索引 `(a, b, c)`：\n\n+ 当 `a` 为等值查询时，可以继续求 `b` 的查询范围。\n+ 当 `b` 也为等值查询时，可以继续求 `c` 的查询范围。\n+ 反之，如果 `a` 为非等值查询，则只能求 `a` 的范围。\n\n### Task 简介\n\n目前 TiDB 的计算任务分为四种不同的 task：root task, cop task, batchCop task 和 MPP task。\n\n- root task 是指在 TiDB 中执行的计算任务。\n- cop task 是指使用 TiKV 或 TiFlash 中的 Coprocessor 执行的计算任务。\n- batchCop task 是对 TiFlash cop task 的一种优化，可以在一个任务中执行对多个 Region 的查询。\n- MPP task 是指利用 TiFlash 的 [MPP 模式](/explain-mpp.md)执行查询。\n\nSQL 优化的目标之一是将计算尽可能地下推到 TiKV 或 TiFlash 中执行，以提高查询效率。TiKV 中的 Coprocessor 支持大部分 SQL 内建函数（包括聚合函数和标量函数）、`LIMIT` 操作、索引扫描和表扫描。TiFlash 中的 Coprocessor 与 TiKV 功能类似，但不支持索引扫描。\n\n### `operator info` 结果\n\n`EXPLAIN` 返回结果中 `operator info` 列可显示诸如条件下推等信息。本文以上示例中，`operator info` 结果各字段解释如下：\n\n+ `range: [1,1]` 表示查询的 `WHERE` 字句 (`a = 1`) 被下推到了 TiKV，对应的 task 为 `cop[tikv]`。\n+ `keep order:false` 表示该查询的语义不需要 TiKV 按顺序返回结果。如果查询指定了排序（例如 `SELECT * FROM t WHERE a = 1 ORDER BY id`），该字段的返回结果为 `keep order:true`。\n+ `stats:pseudo` 表示 `estRows` 显示的预估数可能不准确。TiDB 定期在后台更新统计信息。也可以通过执行 `ANALYZE TABLE t` 来手动更新统计信息。\n\n`EXPLAIN` 执行后，不同算子返回不同的信息。你可以使用 Optimizer Hints 来控制优化器的行为，以此控制物理算子的选择。例如 `/*+ HASH_JOIN(t1, t2) */` 表示优化器将使用 Hash Join 算法。详细内容见 [Optimizer Hints](/optimizer-hints.md)。\n\n## 算子相关的系统变量\n\nTiDB 在 MySQL 的基础上，定义了一些专用的系统变量和语法用来优化性能。其中一些系统变量和具体的算子相关，比如算子的并发度，算子的内存使用上限，是否允许使用分区表等。这些都可以通过系统变量进行控制，从而影响各个算子执行的效率。\n\n如果读者想要详细了解所有的系统变量及其使用规则，可以参见[系统变量和语法](/system-variables.md)。\n\n## 另请参阅\n\n* [`EXPLAIN`](/sql-statements/sql-statement-explain.md)\n* [`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md)\n* [`ANALYZE TABLE`](/sql-statements/sql-statement-analyze-table.md)\n* [`TRACE`](/sql-statements/sql-statement-trace.md)\n* [`TiDB in Action`](https://book.tidb.io/session3/chapter1/sql-execution-plan.html)\n* [系统变量](/system-variables.md)\n"
        },
        {
          "name": "explain-partitions.md",
          "type": "blob",
          "size": 9.7353515625,
          "content": "---\ntitle: 用 EXPLAIN 查看分区查询的执行计划\nsummary: 了解 TiDB 中 EXPLAIN 语句返回的执行计划信息。\n---\n\n# 用 EXPLAIN 查看分区查询的执行计划\n\n使用 `EXPLAIN` 语句可以查看 TiDB 在执行查询时需要访问的分区。由于存在[分区裁剪](/partition-pruning.md)，所显示的分区通常只是所有分区的一个子集。本文档介绍了常见分区表的一些优化方式，以及如何解读 `EXPLAIN` 语句返回的执行计划信息。\n\n本文档所使用的示例数据如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n id BIGINT NOT NULL auto_increment,\n d date NOT NULL,\n pad1 BLOB,\n pad2 BLOB,\n pad3 BLOB,\n PRIMARY KEY (id,d)\n) PARTITION BY RANGE (YEAR(d)) (\n PARTITION p2016 VALUES LESS THAN (2017),\n PARTITION p2017 VALUES LESS THAN (2018),\n PARTITION p2018 VALUES LESS THAN (2019),\n PARTITION p2019 VALUES LESS THAN (2020),\n PARTITION pmax VALUES LESS THAN MAXVALUE\n);\n\nINSERT INTO t1 (d, pad1, pad2, pad3) VALUES\n ('2016-01-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2016-06-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2016-09-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2017-01-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2017-06-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2017-09-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2018-01-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2018-06-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2018-09-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2019-01-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2019-06-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2019-09-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2020-01-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2020-06-01', RANDOM_BYTES(102), RANDOM_BYTES(1024), RANDOM_BYTES(1024)),\n ('2020-09-01', RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024));\n\nINSERT INTO t1 SELECT NULL, a.d, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, a.d, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, a.d, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, a.d, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\n\nSELECT SLEEP(1);\nANALYZE TABLE t1;\n```\n\n以下示例解释了基于新建分区表 `t1` 的一条语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT COUNT(*) FROM t1 WHERE d = '2017-06-01';\n```\n\n```sql\n+------------------------------+---------+-----------+---------------------------+-------------------------------------------+\n| id                           | estRows | task      | access object             | operator info                             |\n+------------------------------+---------+-----------+---------------------------+-------------------------------------------+\n| StreamAgg_21                 | 1.00    | root      |                           | funcs:count(Column#8)->Column#6           |\n| └─TableReader_22             | 1.00    | root      |                           | data:StreamAgg_10                         |\n|   └─StreamAgg_10             | 1.00    | cop[tikv] |                           | funcs:count(1)->Column#8                  |\n|     └─Selection_20           | 8.87    | cop[tikv] |                           | eq(test.t1.d, 2017-06-01 00:00:00.000000) |\n|       └─TableFullScan_19     | 8870.00 | cop[tikv] | table:t1, partition:p2017 | keep order:false                          |\n+------------------------------+---------+-----------+---------------------------+-------------------------------------------+\n5 rows in set (0.01 sec)\n```\n\n由上述 `EXPLAIN` 结果可知，从最末尾的 `—TableFullScan_19` 算子开始，再返回到根部的 `StreamAgg_21` 算子的执行过程如下：\n\n* TiDB 成功地识别出只需要访问一个分区 (`p2017`)，并将该信息在 `access object` 列中注明。\n* `└─TableFullScan_19` 算子先对整个分区进行扫描，然后执行 `└─Selection_20` 算子筛选起始日期为 `2017-06-01 00:00:00.000000` 的行。\n* 之后，`└─Selection_20` 算子匹配的行在 Coprocessor 中进行流式聚合，Coprocessor 本身就可以理解聚合函数 `count`。\n* 每个 Coprocessor 请求会发送一行数据给 TiDB 的 `└─TableReader_22` 算子，然后将数据在 `StreamAgg_21` 算子下进行流式聚合，再将一行数据返回给客户端。\n\n以下示例中，分区裁剪不会消除任何分区：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT COUNT(*) FROM t1 WHERE YEAR(d) = 2017;\n```\n\n```sql\n+------------------------------------+----------+-----------+---------------------------+----------------------------------+\n| id                                 | estRows  | task      | access object             | operator info                    |\n+------------------------------------+----------+-----------+---------------------------+----------------------------------+\n| HashAgg_20                         | 1.00     | root      |                           | funcs:count(Column#7)->Column#6  |\n| └─PartitionUnion_21                | 5.00     | root      |                           |                                  |\n|   ├─StreamAgg_36                   | 1.00     | root      |                           | funcs:count(Column#9)->Column#7  |\n|   │ └─TableReader_37               | 1.00     | root      |                           | data:StreamAgg_25                |\n|   │   └─StreamAgg_25               | 1.00     | cop[tikv] |                           | funcs:count(1)->Column#9         |\n|   │     └─Selection_35             | 6000.00  | cop[tikv] |                           | eq(year(test.t1.d), 2017)        |\n|   │       └─TableFullScan_34       | 7500.00  | cop[tikv] | table:t1, partition:p2016 | keep order:false                 |\n|   ├─StreamAgg_55                   | 1.00     | root      |                           | funcs:count(Column#11)->Column#7 |\n|   │ └─TableReader_56               | 1.00     | root      |                           | data:StreamAgg_44                |\n|   │   └─StreamAgg_44               | 1.00     | cop[tikv] |                           | funcs:count(1)->Column#11        |\n|   │     └─Selection_54             | 14192.00 | cop[tikv] |                           | eq(year(test.t1.d), 2017)        |\n|   │       └─TableFullScan_53       | 17740.00 | cop[tikv] | table:t1, partition:p2017 | keep order:false                 |\n|   ├─StreamAgg_74                   | 1.00     | root      |                           | funcs:count(Column#13)->Column#7 |\n|   │ └─TableReader_75               | 1.00     | root      |                           | data:StreamAgg_63                |\n|   │   └─StreamAgg_63               | 1.00     | cop[tikv] |                           | funcs:count(1)->Column#13        |\n|   │     └─Selection_73             | 3977.60  | cop[tikv] |                           | eq(year(test.t1.d), 2017)        |\n|   │       └─TableFullScan_72       | 4972.00  | cop[tikv] | table:t1, partition:p2018 | keep order:false                 |\n|   ├─StreamAgg_93                   | 1.00     | root      |                           | funcs:count(Column#15)->Column#7 |\n|   │ └─TableReader_94               | 1.00     | root      |                           | data:StreamAgg_82                |\n|   │   └─StreamAgg_82               | 1.00     | cop[tikv] |                           | funcs:count(1)->Column#15        |\n|   │     └─Selection_92             | 20361.60 | cop[tikv] |                           | eq(year(test.t1.d), 2017)        |\n|   │       └─TableFullScan_91       | 25452.00 | cop[tikv] | table:t1, partition:p2019 | keep order:false                 |\n|   └─StreamAgg_112                  | 1.00     | root      |                           | funcs:count(Column#17)->Column#7 |\n|     └─TableReader_113              | 1.00     | root      |                           | data:StreamAgg_101               |\n|       └─StreamAgg_101              | 1.00     | cop[tikv] |                           | funcs:count(1)->Column#17        |\n|         └─Selection_111            | 8892.80  | cop[tikv] |                           | eq(year(test.t1.d), 2017)        |\n|           └─TableFullScan_110      | 11116.00 | cop[tikv] | table:t1, partition:pmax  | keep order:false                 |\n+------------------------------------+----------+-----------+---------------------------+----------------------------------+\n27 rows in set (0.00 sec)\n```\n\n由上述 `EXPLAIN` 结果可知：\n\n* TiDB 认为需要访问所有分区 `(p2016..pMax)`。这是因为 TiDB 将谓词 `YEAR（d）= 2017` 视为 [non-sargable](https://en.wikipedia.org/wiki/Sargable)。这个问题并非是 TiDB 特有的。\n* 在扫描每个分区时，`Selection` 算子将筛选出年份不为 2017 的行。\n* 在每个分区上会执行流式聚合，以计算匹配的行数。\n* `└─PartitionUnion_21` 算子会合并访问每个分区后的结果。\n\n## 其他类型查询的执行计划\n\n+ [MPP 模式查询的执行计划](/explain-mpp.md)\n+ [索引查询的执行计划](/explain-indexes.md)\n+ [Join 查询的执行计划](/explain-joins.md)\n+ [子查询的执行计划](/explain-subqueries.md)\n+ [聚合查询的执行计划](/explain-aggregation.md)\n+ [视图查询的执行计划](/explain-views.md)\n+ [索引合并查询的执行计划](/explain-index-merge.md)\n"
        },
        {
          "name": "explain-subqueries.md",
          "type": "blob",
          "size": 26.9375,
          "content": "---\ntitle: 用 EXPLAIN 查看子查询的执行计划\nsummary: 了解 TiDB 中 EXPLAIN 语句返回的执行计划信息。\n---\n\n# 用 EXPLAIN 查看子查询的执行计划\n\nTiDB 会执行多种[子查询相关的优化](/subquery-optimization.md)，以提升子查询的执行性能。本文档介绍一些常见子查询的优化方式，以及如何解读 `EXPLAIN` 语句返回的执行计划信息。\n\n本文档所使用的示例表数据如下：\n\n```sql\nCREATE TABLE t1 (id BIGINT NOT NULL PRIMARY KEY auto_increment, pad1 BLOB, pad2 BLOB, pad3 BLOB, int_col INT NOT NULL DEFAULT 0);\nCREATE TABLE t2 (id BIGINT NOT NULL PRIMARY KEY auto_increment, t1_id BIGINT NOT NULL, pad1 BLOB, pad2 BLOB, pad3 BLOB, INDEX(t1_id));\nCREATE TABLE t3 (\n id INT NOT NULL PRIMARY KEY auto_increment,\n t1_id INT NOT NULL,\n UNIQUE (t1_id)\n);\n\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM dual;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t1 SELECT NULL, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024), 0 FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nINSERT INTO t2 SELECT NULL, a.id, RANDOM_BYTES(1024), RANDOM_BYTES(1024), RANDOM_BYTES(1024) FROM t1 a JOIN t1 b JOIN t1 c LIMIT 10000;\nUPDATE t1 SET int_col = 1 WHERE pad1 = (SELECT pad1 FROM t1 ORDER BY RAND() LIMIT 1);\nINSERT INTO t3 SELECT NULL, id FROM t1 WHERE id < 1000;\n\nSELECT SLEEP(1);\nANALYZE TABLE t1, t2, t3;\n```\n\n## Inner join（无 `UNIQUE` 约束的子查询）\n\n以下示例中，`IN` 子查询会从表 `t2` 中搜索一列 ID。为保证语义正确性，TiDB 需要保证 `t1_id` 列的值具有唯一性。使用 `EXPLAIN` 可查看到该查询的执行计划去掉重复项并执行 `Inner Join` 内连接操作：\n\n```sql\nEXPLAIN SELECT * FROM t1 WHERE id IN (SELECT t1_id FROM t2);\n```\n\n```sql\n+--------------------------------+----------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n| id                             | estRows  | task      | access object                | operator info                                                                                                             |\n+--------------------------------+----------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n| IndexJoin_15                   | 21.11    | root      |                              | inner join, inner:TableReader_12, outer key:test.t2.t1_id, inner key:test.t1.id, equal cond:eq(test.t2.t1_id, test.t1.id) |\n| ├─StreamAgg_44(Build)          | 21.11    | root      |                              | group by:test.t2.t1_id, funcs:firstrow(test.t2.t1_id)->test.t2.t1_id                                                      |\n| │ └─IndexReader_45             | 21.11    | root      |                              | index:StreamAgg_34                                                                                                        |\n| │   └─StreamAgg_34             | 21.11    | cop[tikv] |                              | group by:test.t2.t1_id,                                                                                                   |\n| │     └─IndexFullScan_26       | 90000.00 | cop[tikv] | table:t2, index:t1_id(t1_id) | keep order:true                                                                                                           |\n| └─TableReader_12(Probe)        | 21.11    | root      |                              | data:TableRangeScan_11                                                                                                    |\n|   └─TableRangeScan_11          | 21.11    | cop[tikv] | table:t1                     | range: decided by [test.t2.t1_id], keep order:false                                                                       |\n+--------------------------------+----------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n\n```\n\n由上述查询结果可知，TiDB 通过索引连接操作 `IndexJoin_15` 将子查询做了连接转化。该执行计划首先在 TiKV 侧通过索引扫描算子 `└─IndexFullScan_26` 读取 `t2.t1_id` 列的值，然后由 `└─StreamAgg_34` 算子的部分任务在 TiKV 中对 `t1_id` 值进行去重，然后采用 `├─StreamAgg_44(Build)` 算子的部分任务在 TiDB 中对 `t1_id` 值再次进行去重，去重操作由聚合函数 `firstrow(test.t2.t1_id)` 执行；之后将操作结果与 `t1` 表的主键相连接，连接条件是 `eq(test.t1.id, test.t2.t1_id)`。\n\n## Inner join（有 `UNIQUE` 约束的子查询）\n\n在上述示例中，为了确保 `t1_id` 值在与表 `t1` 连接前具有唯一性，需要执行聚合运算。在以下示例中，由于 `UNIQUE` 约束已能确保 `t3.t1_id` 列值的唯一：\n\n```sql\nEXPLAIN SELECT * FROM t1 WHERE id IN (SELECT t1_id FROM t3);\n```\n\n```sql\n+-----------------------------+---------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n| id                          | estRows | task      | access object                | operator info                                                                                                             |\n+-----------------------------+---------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n| IndexJoin_18                | 999.00  | root      |                              | inner join, inner:TableReader_15, outer key:test.t3.t1_id, inner key:test.t1.id, equal cond:eq(test.t3.t1_id, test.t1.id) |\n| ├─IndexReader_41(Build)     | 999.00  | root      |                              | index:IndexFullScan_40                                                                                                    |\n| │ └─IndexFullScan_40        | 999.00  | cop[tikv] | table:t3, index:t1_id(t1_id) | keep order:false                                                                                                          |\n| └─TableReader_15(Probe)     | 999.00  | root      |                              | data:TableRangeScan_14                                                                                                    |\n|   └─TableRangeScan_14       | 999.00  | cop[tikv] | table:t1                     | range: decided by [test.t3.t1_id], keep order:false                                                                       |\n+-----------------------------+---------+-----------+------------------------------+---------------------------------------------------------------------------------------------------------------------------+\n\n```\n\n从语义上看，因为约束保证了 `t3.t1_id` 列值的唯一性，TiDB 可以直接执行 `INNER JOIN` 查询。\n\n## Semi Join（关联查询）\n\n在前两个示例中，通过 `StreamAgg` 聚合操作或通过 `UNIQUE` 约束保证子查询数据的唯一性之后，TiDB 才能够执行 `Inner Join` 操作。这两种连接均使用了 `Index Join`。\n\n下面的例子中，TiDB 优化器则选择了一种不同的执行计划：\n\n```sql\nEXPLAIN SELECT * FROM t1 WHERE id IN (SELECT t1_id FROM t2 WHERE t1_id != t1.int_col);\n```\n\n```sql\n+-----------------------------+----------+-----------+------------------------------+--------------------------------------------------------------------------------------------------------+\n| id                          | estRows  | task      | access object                | operator info                                                                                          |\n+-----------------------------+----------+-----------+------------------------------+--------------------------------------------------------------------------------------------------------+\n| MergeJoin_9                 | 45446.40 | root      |                              | semi join, left key:test.t1.id, right key:test.t2.t1_id, other cond:ne(test.t2.t1_id, test.t1.int_col) |\n| ├─IndexReader_24(Build)     | 90000.00 | root      |                              | index:IndexFullScan_23                                                                                 |\n| │ └─IndexFullScan_23        | 90000.00 | cop[tikv] | table:t2, index:t1_id(t1_id) | keep order:true                                                                                        |\n| └─TableReader_22(Probe)     | 56808.00 | root      |                              | data:Selection_21                                                                                      |\n|   └─Selection_21            | 56808.00 | cop[tikv] |                              | ne(test.t1.id, test.t1.int_col)                                                                        |\n|     └─TableFullScan_20      | 71010.00 | cop[tikv] | table:t1                     | keep order:true                                                                                        |\n+-----------------------------+----------+-----------+------------------------------+--------------------------------------------------------------------------------------------------------+\n```\n\n由上述查询结果可知，TiDB 执行了 `Semi Join`。不同于 `Inner Join`，`Semi Join` 仅允许右键 (`t2.t1_id`) 上的第一个值，也就是该操作将去除 `Join` 算子任务中的重复数据。`Join` 算法也包含 `Merge Join`，会按照排序顺序同时从左侧和右侧读取数据，这是一种高效的 `Zipper Merge`。\n\n可以将原语句视为*关联子查询*，因为它引入了子查询外的 `t1.int_col` 列。然而，`EXPLAIN` 语句的返回结果显示的是[关联子查询去关联](/correlated-subquery-optimization.md)后的执行计划。条件 `t1_id != t1.int_col` 会被重写为 `t1.id != t1.int_col`。TiDB 可以从表 `t1` 中读取数据并且在 `└─Selection_21` 中执行此操作，因此这种去关联和重写操作会极大提高执行效率。\n\n## Anti Semi Join（`NOT IN` 子查询）\n\n在以下示例中，*除非*子查询中存在 `t3.t1_id`，否则该查询将（从语义上）返回表 `t3` 中的所有行：\n\n```sql\nEXPLAIN SELECT * FROM t3 WHERE t1_id NOT IN (SELECT id FROM t1 WHERE int_col < 100);\n```\n\n```sql\n+-----------------------------+---------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------+\n| id                          | estRows | task      | access object | operator info                                                                                                                 |\n+-----------------------------+---------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------+\n| IndexJoin_16                | 799.20  | root      |               | anti semi join, inner:TableReader_12, outer key:test.t3.t1_id, inner key:test.t1.id, equal cond:eq(test.t3.t1_id, test.t1.id) |\n| ├─TableReader_28(Build)     | 999.00  | root      |               | data:TableFullScan_27                                                                                                         |\n| │ └─TableFullScan_27        | 999.00  | cop[tikv] | table:t3      | keep order:false                                                                                                              |\n| └─TableReader_12(Probe)     | 999.00  | root      |               | data:Selection_11                                                                                                             |\n|   └─Selection_11            | 999.00  | cop[tikv] |               | lt(test.t1.int_col, 100)                                                                                                      |\n|     └─TableRangeScan_10     | 999.00  | cop[tikv] | table:t1      | range: decided by [test.t3.t1_id], keep order:false                                                                           |\n+-----------------------------+---------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------+\n```\n\n上述查询首先读取了表 `t3`，然后根据主键开始探测 (probe) 表 `t1`。连接类型是 _anti semi join_，即反半连接：之所以使用 _anti_，是因为上述示例有不存在匹配值（即 `NOT IN`）的情况；使用 `Semi Join` 则是因为仅需要匹配第一行后就可以停止查询。\n\n## Null-Aware Semi Join（`IN` 和 `= ANY` 子查询）\n\n`IN` 和 `= ANY` 的集合运算符号具有特殊的三值属性（`true`、`false` 和 `NULL`）。这意味着在该运算符所转化得到的 Join 类型中需要对 Join key 两侧的 `NULL` 进行特殊的感知和处理。\n\n`IN` 和 `= ANY` 算子引导的子查询会分别转为 Semi Join 和 Left Outer Semi Join。在上述 [Semi Join](#semi-join关联查询) 小节中，示例中 Join key 两侧的列 `test.t1.id` 和 `test.t2.t1_id` 都为 `not NULL` 属性，所以 Semi Join 本身不需要 Null-Aware 的性质来辅助运算，即不需要特殊处理 `NULL`。当前 TiDB 对于 Null-Aware Semi Join 没有特定的优化，其实现本质都是基于笛卡尔积加过滤 (filter) 的模式。以下为 Null-Aware Semi Join 的例子：\n\n```sql\nCREATE TABLE t(a INT, b INT);\nCREATE TABLE s(a INT, b INT);\nEXPLAIN SELECT (a,b) IN (SELECT * FROM s) FROM t;\nEXPLAIN SELECT * FROM t WHERE (a,b) IN (SELECT * FROM s);\n```\n\n```sql\ntidb> EXPLAIN SELECT (a,b) IN (SELECT * FROM s) FROM t;\n+-----------------------------+---------+-----------+---------------+-------------------------------------------------------------------------------------------+\n| id                          | estRows | task      | access object | operator info                                                                             |\n+-----------------------------+---------+-----------+---------------+-------------------------------------------------------------------------------------------+\n| HashJoin_8                  | 1.00    | root      |               | CARTESIAN left outer semi join, other cond:eq(test.t.a, test.s.a), eq(test.t.b, test.s.b) |\n| ├─TableReader_12(Build)     | 1.00    | root      |               | data:TableFullScan_11                                                                     |\n| │ └─TableFullScan_11        | 1.00    | cop[tikv] | table:s       | keep order:false, stats:pseudo                                                            |\n| └─TableReader_10(Probe)     | 1.00    | root      |               | data:TableFullScan_9                                                                      |\n|   └─TableFullScan_9         | 1.00    | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                            |\n+-----------------------------+---------+-----------+---------------+-------------------------------------------------------------------------------------------+\n5 rows in set (0.00 sec)\n\ntidb> EXPLAIN SELECT * FROM t WHERE (a,b) IN (SELECT * FROM s);\n+------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------------+\n| id                           | estRows | task      | access object | operator info                                                                                       |\n+------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------------+\n| HashJoin_11                  | 1.00    | root      |               | inner join, equal:[eq(test.t.a, test.s.a) eq(test.t.b, test.s.b)]                                   |\n| ├─TableReader_14(Build)      | 1.00    | root      |               | data:Selection_13                                                                                   |\n| │ └─Selection_13             | 1.00    | cop[tikv] |               | not(isnull(test.t.a)), not(isnull(test.t.b))                                                        |\n| │   └─TableFullScan_12       | 1.00    | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                                      |\n| └─HashAgg_17(Probe)          | 1.00    | root      |               | group by:test.s.a, test.s.b, funcs:firstrow(test.s.a)->test.s.a, funcs:firstrow(test.s.b)->test.s.b |\n|   └─TableReader_24           | 1.00    | root      |               | data:Selection_23                                                                                   |\n|     └─Selection_23           | 1.00    | cop[tikv] |               | not(isnull(test.s.a)), not(isnull(test.s.b))                                                        |\n|       └─TableFullScan_22     | 1.00    | cop[tikv] | table:s       | keep order:false, stats:pseudo                                                                      |\n+------------------------------+---------+-----------+---------------+-----------------------------------------------------------------------------------------------------+\n8 rows in set (0.01 sec)\n```\n\n第一个查询 `EXPLAIN SELECT (a,b) IN (SELECT * FROM s) FROM t;` 中，由于 `t` 表和 `s` 表的 `a`、`b` 列都是 NULLABLE 的，所以 `IN` 子查询所转化的 Left Outer Semi Join 是具有 Null-Aware 性质的。具体实现是先进行笛卡尔积，然后将 `IN` 或 `= ANY` 所连接的列作为普通等值条件放到 other condition 进行过滤（filter）。\n\n第二个查询 `EXPLAIN SELECT * FROM t WHERE (a,b) IN (SELECT * FROM s);` 中，由于 `t` 表和 `s` 表的 `a`、`b` 列都是 NULLABLE 的，`IN` 子查询本应该转为具有 Null-Aware 性质的 Semi Join，但当前 TiDB 进行了优化，直接将 Semi Join 转为了 Inner Join + Aggregate 的方式来实现。这是因为在非 scalar 输出的 `IN` 子查询中，`NULL` 和 `false` 是等效的。下推过滤的 `NULL` 行导致了 `WHERE` 子句的否定语义，因此可以事先忽略这些行。\n\n> **注意：**\n>\n> `Exists` 操作符也会被转成 Semi Join，但是 `Exists` 操作符号本身不具有集合运算 Null-Aware 的性质。\n\n## Null-Aware Anti Semi Join（`NOT IN` 和 `!= ALL` 子查询）\n\n`NOT IN` 和 `!= ALL` 的集合运算运算具有特殊的三值属性（`true`、`false` 和 `NULL`）。这意味着在其所转化得到的 Join 类型中需要对 Join key 两侧的 `NULL` 进行特殊的感知和处理。\n\n`NOT IN` 和 `!= ALL` 算子引导的子查询会对应地转为 Anti Semi Join 和 Anti Left Outer Semi Join。在上述的 [Anti Semi Join](#anti-semi-joinnot-in-子查询) 小节中，由于示例中 Join key 两侧的列 `test.t3.t1_id` 和 `test.t1.id` 都是 `not NULL` 属性的，所以 Anti Semi Join 本身不需要 Null-Aware 的性质来辅助计算，即不需要特殊处理 `NULL`。\n\n在 TiDB v6.3.0 版本，TiDB 引入了针对 Null-Aware Anti Join (NAAJ) 的如下特殊优化：\n\n- 利用 Null-Aware 的等值条件 (NA-EQ) 构建哈希连接\n\n    由于集合操作符引入的等值需要对等值两侧操作符数的 `NULL` 值做特殊处理，这里称需要 Null-Aware 的等值条件为 NA-EQ 条件。与 v6.3.0 之前版本不同的是，TiDB 不会再将 NA-EQ 条件处理成普通 EQ 条件，而是专门放置于 Join 后置的 other condition 中，匹配笛卡尔积后再判断结果集的合法性。\n\n    在 TiDB v6.3.0 版本中，NA-EQ 这种弱化的等值条件依然会被用来构建哈希值 (Hash Join)，大大减少了匹配时所需遍历的数据量，加速匹配过程。在 build 表 `DISTINCT` 值比例趋近 1 的时候，加速效果更为显著。\n\n- 利用两侧数据源 `NULL` 值的特殊性质加速匹配过程的返回\n\n    由于 Anti Semi Join 自身具有 CNF (Conjunctive normal form) 表达式的属性，其任何一侧出现的 `NULL` 值都会导致确定的结果。利用这个性质可以来加速整个匹配过程。\n\n以下为 Null-Aware Anti Semi Join 的例子：\n\n```sql\nCREATE TABLE t(a INT, b INT);\nCREATE TABLE s(a INT, b INT);\nEXPLAIN SELECT (a, b) NOT IN (SELECT * FROM s) FROM t;\nEXPLAIN SELECT * FROM t WHERE (a, b) NOT IN (SELECT * FROM s);\n```\n\n```sql\ntidb> EXPLAIN SELECT (a, b) NOT IN (SELECT * FROM s) FROM t;\n+-----------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n| id                          | estRows  | task      | access object | operator info                                                                               |\n+-----------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n| HashJoin_8                  | 10000.00 | root      |               | Null-aware anti left outer semi join, equal:[eq(test.t.b, test.s.b) eq(test.t.a, test.s.a)] |\n| ├─TableReader_12(Build)     | 10000.00 | root      |               | data:TableFullScan_11                                                                       |\n| │ └─TableFullScan_11        | 10000.00 | cop[tikv] | table:s       | keep order:false, stats:pseudo                                                              |\n| └─TableReader_10(Probe)     | 10000.00 | root      |               | data:TableFullScan_9                                                                        |\n|   └─TableFullScan_9         | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                              |\n+-----------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------+\n5 rows in set (0.00 sec)\n\ntidb> EXPLAIN SELECT * FROM t WHERE (a, b) NOT IN (SELECT * FROM s);\n+-----------------------------+----------+-----------+---------------+----------------------------------------------------------------------------------+\n| id                          | estRows  | task      | access object | operator info                                                                    |\n+-----------------------------+----------+-----------+---------------+----------------------------------------------------------------------------------+\n| HashJoin_8                  | 8000.00  | root      |               | Null-aware anti semi join, equal:[eq(test.t.b, test.s.b) eq(test.t.a, test.s.a)] |\n| ├─TableReader_12(Build)     | 10000.00 | root      |               | data:TableFullScan_11                                                            |\n| │ └─TableFullScan_11        | 10000.00 | cop[tikv] | table:s       | keep order:false, stats:pseudo                                                   |\n| └─TableReader_10(Probe)     | 10000.00 | root      |               | data:TableFullScan_9                                                             |\n|   └─TableFullScan_9         | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                                                   |\n+-----------------------------+----------+-----------+---------------+----------------------------------------------------------------------------------+\n5 rows in set (0.00 sec)\n```\n\n第一个查询 `EXPLAIN SELECT (a, b) NOT IN (SELECT * FROM s) FROM t;` 中，由于 `t` 表和 `s` 表的 `a`、`b` 列都是 NULLABLE 的，所以 `NOT IN` 子查询所转化的 Left Outer Semi Join 是具有 Null-Aware 性质的。不同的是，NAAJ 优化将 NA-EQ 条件也作为了 Hash Join 的连接条件，大大加速了 Join 的计算。\n\n第二个查询 `EXPLAIN SELECT * FROM t WHERE (a, b) NOT IN (SELECT * FROM s);` 中，由于 `t` 表和 `s` 表的 `a`、`b` 列都是 NULLABLE 的，所以 `NOT IN` 子查询所转化的 Anti Semi Join 是具有 Null-Aware 性质的。不同的是，NAAJ 优化将 NA-EQ 条件也作为了 Hash Join 的连接条件，大大加速了 Join 的计算。\n\n当前 TiDB 仅针对 Anti Semi Join 和 Anti Left Outer Semi Join 实现了 `NULL` 感知。目前仅支持 Hash Join 类型且其 build 表只能固定为右侧表。\n\n> **注意：**\n>\n> `Not Exists` 操作符也会被转成 Anti Semi Join，但是 `Not Exists` 符号本身不具有集合运算 Null-Aware 的性质。\n\n## 其他类型查询的执行计划\n\n+ [MPP 模式查询的执行计划](/explain-mpp.md)\n+ [索引查询的执行计划](/explain-indexes.md)\n+ [Join 查询的执行计划](/explain-joins.md)\n+ [聚合查询的执行计划](/explain-aggregation.md)\n+ [视图查询的执行计划](/explain-views.md)\n+ [分区查询的执行计划](/explain-partitions.md)\n+ [索引合并查询的执行计划](/explain-index-merge.md)\n"
        },
        {
          "name": "explain-views.md",
          "type": "blob",
          "size": 8.61328125,
          "content": "---\ntitle: 用 EXPLAIN 查看带视图的 SQL 执行计划\nsummary: 了解 TiDB 中视图相关语句的执行计划。\n---\n\n# 用 EXPLAIN 查看带视图的 SQL 执行计划\n\n`EXPLAIN` 语句返回的结果会显示[视图](/views.md)引用的表和索引，而不是视图本身的名称。这是因为视图是一张虚拟表，本身并不存储任何数据。视图的定义会和查询语句的其余部分在 SQL 优化过程中进行合并。\n\n参考 [bikeshare 数据库示例（英文）](https://docs.pingcap.com/tidb/stable/import-example-data)，以下两个示例查询的执行方式类似：\n\n{{< copyable \"sql\" >}}\n\n```sql\nALTER TABLE trips ADD INDEX (duration);\nCREATE OR REPLACE VIEW long_trips AS SELECT * FROM trips WHERE duration > 3600;\nEXPLAIN SELECT * FROM long_trips;\nEXPLAIN SELECT * FROM trips WHERE duration > 3600;\n```\n\n```sql\nQuery OK, 0 rows affected (2 min 10.11 sec)\n\nQuery OK, 0 rows affected (0.13 sec)\n\n+--------------------------------+------------+-----------+---------------------------------------+-------------------------------------+\n| id                             | estRows    | task      | access object                         | operator info                       |\n+--------------------------------+------------+-----------+---------------------------------------+-------------------------------------+\n| IndexLookUp_12                 | 6372547.67 | root      |                                       |                                     |\n| ├─IndexRangeScan_10(Build)     | 6372547.67 | cop[tikv] | table:trips, index:duration(duration) | range:(3600,+inf], keep order:false |\n| └─TableRowIDScan_11(Probe)     | 6372547.67 | cop[tikv] | table:trips                           | keep order:false                    |\n+--------------------------------+------------+-----------+---------------------------------------+-------------------------------------+\n3 rows in set (0.00 sec)\n\n+-------------------------------+-----------+-----------+---------------------------------------+-------------------------------------+\n| id                            | estRows   | task      | access object                         | operator info                       |\n+-------------------------------+-----------+-----------+---------------------------------------+-------------------------------------+\n| IndexLookUp_10                | 833219.37 | root      |                                       |                                     |\n| ├─IndexRangeScan_8(Build)     | 833219.37 | cop[tikv] | table:trips, index:duration(duration) | range:(3600,+inf], keep order:false |\n| └─TableRowIDScan_9(Probe)     | 833219.37 | cop[tikv] | table:trips                           | keep order:false                    |\n+-------------------------------+-----------+-----------+---------------------------------------+-------------------------------------+\n3 rows in set (0.00 sec)\n```\n\n同样，该视图中的谓词被下推至基表：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT * FROM long_trips WHERE bike_number = 'W00950';\nEXPLAIN SELECT * FROM trips WHERE bike_number = 'W00950';\n```\n\n```sql\n+--------------------------------+---------+-----------+---------------------------------------+---------------------------------------------------+\n| id                             | estRows | task      | access object                         | operator info                                     |\n+--------------------------------+---------+-----------+---------------------------------------+---------------------------------------------------+\n| IndexLookUp_14                 | 3.33    | root      |                                       |                                                   |\n| ├─IndexRangeScan_11(Build)     | 3333.33 | cop[tikv] | table:trips, index:duration(duration) | range:(3600,+inf], keep order:false, stats:pseudo |\n| └─Selection_13(Probe)          | 3.33    | cop[tikv] |                                       | eq(bikeshare.trips.bike_number, \"W00950\")         |\n|   └─TableRowIDScan_12          | 3333.33 | cop[tikv] | table:trips                           | keep order:false, stats:pseudo                    |\n+--------------------------------+---------+-----------+---------------------------------------+---------------------------------------------------+\n4 rows in set (0.00 sec)\n\n+-------------------------+-------------+-----------+---------------+-------------------------------------------+\n| id                      | estRows     | task      | access object | operator info                             |\n+-------------------------+-------------+-----------+---------------+-------------------------------------------+\n| TableReader_7           | 43.00       | root      |               | data:Selection_6                          |\n| └─Selection_6           | 43.00       | cop[tikv] |               | eq(bikeshare.trips.bike_number, \"W00950\") |\n|   └─TableFullScan_5     | 19117643.00 | cop[tikv] | table:trips   | keep order:false                          |\n+-------------------------+-------------+-----------+---------------+-------------------------------------------+\n3 rows in set (0.00 sec)\n```\n\n执行以上第一条语句时使用了索引，满足视图定义，接着在 TiDB 读取行时应用了 `bike_number = 'W00950'` 条件。执行以上第二条语句时，不存在满足该语句的索引，因此使用了 `TableFullScan`。\n\nTiDB 使用的索引可以同时满足视图定义和语句本身，如以下组合索引所示：\n\n{{< copyable \"sql\" >}}\n\n```sql\nALTER TABLE trips ADD INDEX (bike_number, duration);\nEXPLAIN SELECT * FROM long_trips WHERE bike_number = 'W00950';\nEXPLAIN SELECT * FROM trips WHERE bike_number = 'W00950';\n```\n\n```sql\nQuery OK, 0 rows affected (2 min 31.20 sec)\n\n+--------------------------------+----------+-----------+-------------------------------------------------------+-------------------------------------------------------+\n| id                             | estRows  | task      | access object                                         | operator info                                         |\n+--------------------------------+----------+-----------+-------------------------------------------------------+-------------------------------------------------------+\n| IndexLookUp_13                 | 63725.48 | root      |                                                       |                                                       |\n| ├─IndexRangeScan_11(Build)     | 63725.48 | cop[tikv] | table:trips, index:bike_number(bike_number, duration) | range:(\"W00950\" 3600,\"W00950\" +inf], keep order:false |\n| └─TableRowIDScan_12(Probe)     | 63725.48 | cop[tikv] | table:trips                                           | keep order:false                                      |\n+--------------------------------+----------+-----------+-------------------------------------------------------+-------------------------------------------------------+\n3 rows in set (0.00 sec)\n\n+-------------------------------+----------+-----------+-------------------------------------------------------+---------------------------------------------+\n| id                            | estRows  | task      | access object                                         | operator info                               |\n+-------------------------------+----------+-----------+-------------------------------------------------------+---------------------------------------------+\n| IndexLookUp_10                | 19117.64 | root      |                                                       |                                             |\n| ├─IndexRangeScan_8(Build)     | 19117.64 | cop[tikv] | table:trips, index:bike_number(bike_number, duration) | range:[\"W00950\",\"W00950\"], keep order:false |\n| └─TableRowIDScan_9(Probe)     | 19117.64 | cop[tikv] | table:trips                                           | keep order:false                            |\n+-------------------------------+----------+-----------+-------------------------------------------------------+---------------------------------------------+\n3 rows in set (0.00 sec)\n```\n\n在第一条语句中，TiDB 能够使用组合索引的两个部分 `(bike_number, duration)`。在第二条语句，TiDB 仅使用了索引 `(bike_number, duration)` 的第一部分 `bike_number`。\n\n## 其他类型查询的执行计划\n\n+ [MPP 模式查询的执行计划](/explain-mpp.md)\n+ [索引查询的执行计划](/explain-indexes.md)\n+ [Join 查询的执行计划](/explain-joins.md)\n+ [子查询的执行计划](/explain-subqueries.md)\n+ [聚合查询的执行计划](/explain-aggregation.md)\n+ [分区查询的执行计划](/explain-partitions.md)\n+ [索引合并查询的执行计划](/explain-index-merge.md)\n"
        },
        {
          "name": "explain-walkthrough.md",
          "type": "blob",
          "size": 25.037109375,
          "content": "---\ntitle: 使用 EXPLAIN 解读执行计划\nsummary: 通过示例了解如何使用 EXPLAIN 分析执行计划。\n---\n\n# 使用 `EXPLAIN` 解读执行计划\n\nSQL 是一种声明性语言，因此用户无法根据 SQL 语句直接判断一条查询的执行是否有效率。用户首先要使用 [`EXPLAIN`](/sql-statements/sql-statement-explain.md) 语句查看当前的执行计划。\n\n以 [bikeshare 数据库示例（英文）](https://docs.pingcap.com/tidb/stable/import-example-data) 中的一个 SQL 语句为例，该语句统计了 2017 年 7 月 1 日的行程次数：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';\n```\n\n```sql\n+------------------------------+----------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------+\n| id                           | estRows  | task      | access object | operator info                                                                                                          |\n+------------------------------+----------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------+\n| StreamAgg_20                 | 1.00     | root      |               | funcs:count(Column#13)->Column#11                                                                                      |\n| └─TableReader_21             | 1.00     | root      |               | data:StreamAgg_9                                                                                                       |\n|   └─StreamAgg_9              | 1.00     | cop[tikv] |               | funcs:count(1)->Column#13                                                                                              |\n|     └─Selection_19           | 250.00   | cop[tikv] |               | ge(bikeshare.trips.start_date, 2017-07-01 00:00:00.000000), le(bikeshare.trips.start_date, 2017-07-01 23:59:59.000000) |\n|       └─TableFullScan_18     | 10000.00 | cop[tikv] | table:trips   | keep order:false, stats:pseudo                                                                                         |\n+------------------------------+----------+-----------+---------------+------------------------------------------------------------------------------------------------------------------------+\n5 rows in set (0.00 sec)\n```\n\n以上是该查询的执行计划结果。从 `└─TableFullScan_18` 算子开始向上看，查询的执行过程如下（非最佳执行计划）：\n\n1. Coprocessor (TiKV) 读取整张 `trips` 表的数据，作为一次 `TableFullScan` 操作，再将读取到的数据传递给 `Selection_19` 算子。`Selection_19` 算子仍在 TiKV 内。\n\n2. `Selection_19` 算子根据谓词 `WHERE start_date BETWEEN ..` 进行数据过滤。预计大约有 250 行数据满足该过滤条件（基于统计信息以及算子的执行逻辑估算而来）。`└─TableFullScan_18` 算子显示 `stats:pseudo`，表示该表没有实际统计信息，执行 `ANALYZE TABLE trips` 收集统计信息后，预计的估算的数字会更加准确。\n\n3. `COUNT` 函数随后应用于满足过滤条件的行，这一过程也是在 TiKV (`cop[tikv]`) 中的 `StreamAgg_9` 算子内完成的。TiKV coprocessor 能执行一些 MySQL 内置函数，`COUNT` 是其中之一。\n\n4. `StreamAgg_9` 算子执行的结果会被传递给 `TableReader_21` 算子（位于 TiDB 进程中，即 `root` 任务）。执行计划中，`TableReader_21` 算子的 `estRows` 为 `1`，表示该算子将从每个访问的 TiKV Region 接收一行数据。这一请求过程的详情，可参阅 [`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md)。\n\n5. `StreamAgg_20` 算子随后对 `└─TableReader_21` 算子传来的每行数据计算 `COUNT` 函数的结果。`StreamAgg_20` 是根算子，会将结果返回给客户端。\n\n> **注意：**\n>\n> 要查看 TiDB 中某张表的 Region 信息，可执行 [`SHOW TABLE REGIONS`](/sql-statements/sql-statement-show-table-regions.md) 语句。\n\n## 评估当前的性能\n\n`EXPLAIN` 语句只返回查询的执行计划，并不执行该查询。若要获取实际的执行时间，可执行该查询，或使用 `EXPLAIN ANALYZE` 语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN ANALYZE SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';\n```\n\n```sql\n+------------------------------+----------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------+------+\n| id                           | estRows  | actRows  | task      | access object | execution info                                                                                                                                                                                                                                    | operator info                                                                                                          | memory    | disk |\n+------------------------------+----------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------+------+\n| StreamAgg_20                 | 1.00     | 1        | root      |               | time:1.031417203s, loops:2                                                                                                                                                                                                                        | funcs:count(Column#13)->Column#11                                                                                      | 632 Bytes | N/A  |\n| └─TableReader_21             | 1.00     | 56       | root      |               | time:1.031408123s, loops:2, cop_task: {num: 56, max: 782.147269ms, min: 5.759953ms, avg: 252.005927ms, p95: 609.294603ms, max_proc_keys: 910371, p95_proc_keys: 704775, tot_proc: 11.524s, tot_wait: 580ms, rpc_num: 56, rpc_time: 14.111932641s} | data:StreamAgg_9                                                                                                       | 328 Bytes | N/A  |\n|   └─StreamAgg_9              | 1.00     | 56       | cop[tikv] |               | proc max:640ms, min:8ms, p80:276ms, p95:480ms, iters:18695, tasks:56                                                                                                                                                                              | funcs:count(1)->Column#13                                                                                              | N/A       | N/A  |\n|     └─Selection_19           | 250.00   | 11409    | cop[tikv] |               | proc max:640ms, min:8ms, p80:276ms, p95:476ms, iters:18695, tasks:56                                                                                                                                                                              | ge(bikeshare.trips.start_date, 2017-07-01 00:00:00.000000), le(bikeshare.trips.start_date, 2017-07-01 23:59:59.000000) | N/A       | N/A  |\n|       └─TableFullScan_18     | 10000.00 | 19117643 | cop[tikv] | table:trips   | proc max:612ms, min:8ms, p80:248ms, p95:460ms, iters:18695, tasks:56                                                                                                                                                                              | keep order:false, stats:pseudo                                                                                         | N/A       | N/A  |\n+------------------------------+----------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------+------+\n5 rows in set (1.03 sec)\n```\n\n执行以上示例查询耗时 `1.03` 秒，说明执行性能较为理想。\n\n以上 `EXPLAIN ANALYZE` 的结果中，`actRows` 表明一些 `estRows` 预估数不准确（预估返回 10000 行数据但实际返回 19117643 行）。`└─TableFullScan_18` 算子的 `operator info` 列 (`stats:pseudo`) 信息也表明该算子的预估数不准确。\n\n如果先执行 [`ANALYZE TABLE`](/sql-statements/sql-statement-analyze-table.md) 再执行 `EXPLAIN ANALYZE`，预估数与实际数会更接近：\n\n{{< copyable \"sql\" >}}\n\n```sql\nANALYZE TABLE trips;\nEXPLAIN ANALYZE SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';\n```\n\n```sql\nQuery OK, 0 rows affected (10.22 sec)\n\n+------------------------------+-------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------+------+\n| id                           | estRows     | actRows  | task      | access object | execution info                                                                                                                                                                                                                                   | operator info                                                                                                          | memory    | disk |\n+------------------------------+-------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------+------+\n| StreamAgg_20                 | 1.00        | 1        | root      |               | time:926.393612ms, loops:2                                                                                                                                                                                                                       | funcs:count(Column#13)->Column#11                                                                                      | 632 Bytes | N/A  |\n| └─TableReader_21             | 1.00        | 56       | root      |               | time:926.384792ms, loops:2, cop_task: {num: 56, max: 850.94424ms, min: 6.042079ms, avg: 234.987725ms, p95: 495.474806ms, max_proc_keys: 910371, p95_proc_keys: 704775, tot_proc: 10.656s, tot_wait: 904ms, rpc_num: 56, rpc_time: 13.158911952s} | data:StreamAgg_9                                                                                                       | 328 Bytes | N/A  |\n|   └─StreamAgg_9              | 1.00        | 56       | cop[tikv] |               | proc max:592ms, min:4ms, p80:244ms, p95:480ms, iters:18695, tasks:56                                                                                                                                                                             | funcs:count(1)->Column#13                                                                                              | N/A       | N/A  |\n|     └─Selection_19           | 432.89      | 11409    | cop[tikv] |               | proc max:592ms, min:4ms, p80:244ms, p95:480ms, iters:18695, tasks:56                                                                                                                                                                             | ge(bikeshare.trips.start_date, 2017-07-01 00:00:00.000000), le(bikeshare.trips.start_date, 2017-07-01 23:59:59.000000) | N/A       | N/A  |\n|       └─TableFullScan_18     | 19117643.00 | 19117643 | cop[tikv] | table:trips   | proc max:564ms, min:4ms, p80:228ms, p95:456ms, iters:18695, tasks:56                                                                                                                                                                             | keep order:false                                                                                                       | N/A       | N/A  |\n+------------------------------+-------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------+------+\n5 rows in set (0.93 sec)\n```\n\n执行 `ANALYZE TABLE` 后，可以看到 `└─TableFullScan_18` 算子的预估行数是准确的，`└─Selection_19` 算子的预估行数也更接近实际行数。以上两个示例中的执行计划（即 TiDB 执行查询所使用的一组算子）未改变，但过时的统计信息常常导致 TiDB 选择到非最优的执行计划。\n\n除 `ANALYZE TABLE` 外，达到 [`tidb_auto_analyze_ratio`](/system-variables.md#tidb_auto_analyze_ratio) 阈值后，TiDB 会自动在后台重新生成统计数据。若要查看 TiDB 有多接近该阈值（即 TiDB 判断统计数据有多健康），可执行 [`SHOW STATS_HEALTHY`](/sql-statements/sql-statement-show-stats-healthy.md) 语句。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW STATS_HEALTHY;\n```\n\n```sql\n+-----------+------------+----------------+---------+\n| Db_name   | Table_name | Partition_name | Healthy |\n+-----------+------------+----------------+---------+\n| bikeshare | trips      |                |     100 |\n+-----------+------------+----------------+---------+\n1 row in set (0.00 sec)\n```\n\n## 确定优化方案\n\n当前执行计划是有效率的：\n\n* 大部分任务是在 TiKV 内处理的，需要通过网络传输给 TiDB 处理的仅有 56 行数据，每行都满足过滤条件，而且都很短。\n\n* 在 TiDB (`StreamAgg_20`) 中和在 TiKV (`└─StreamAgg_9`) 中汇总行数都使用了 Stream Aggregate，该算法在内存使用方面很有效率。\n\n当前执行计划存在的最大问题在于谓词 `start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59'` 并未立即生效，先是 `TableFullScan` 算子读取所有行数据，然后才进行过滤选择。可以在 `SHOW CREATE TABLE trips` 的返回结果中找出问题原因：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW CREATE TABLE trips\\G\n```\n\n```sql\n*************************** 1. row ***************************\n       Table: trips\nCreate Table: CREATE TABLE `trips` (\n  `trip_id` bigint NOT NULL AUTO_INCREMENT,\n  `duration` int NOT NULL,\n  `start_date` datetime DEFAULT NULL,\n  `end_date` datetime DEFAULT NULL,\n  `start_station_number` int DEFAULT NULL,\n  `start_station` varchar(255) DEFAULT NULL,\n  `end_station_number` int DEFAULT NULL,\n  `end_station` varchar(255) DEFAULT NULL,\n  `bike_number` varchar(255) DEFAULT NULL,\n  `member_type` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`trip_id`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin AUTO_INCREMENT=20477318\n1 row in set (0.00 sec)\n```\n\n以上返回结果显示，`start_date` 列**没有**索引。要将该谓词下推到 index reader 算子，还需要一个索引。添加索引如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nALTER TABLE trips ADD INDEX (start_date);\n```\n\n```sql\nQuery OK, 0 rows affected (2 min 10.23 sec)\n```\n\n> **注意：**\n>\n> 你可通过执行 [`ADMIN SHOW DDL JOBS`](/sql-statements/sql-statement-admin-show-ddl.md) 语句来查看 DDL 任务的进度。TiDB 中的默认值的设置较为保守，因此添加索引不会对生产环境下的负载造成太大影响。测试环境下，可以考虑调大 [`tidb_ddl_reorg_batch_size`](/system-variables.md#tidb_ddl_reorg_batch_size) 和 [`tidb_ddl_reorg_worker_cnt`](/system-variables.md#tidb_ddl_reorg_worker_cnt) 的值。在参照系统上，将批处理大小设为 `10240`，将 worker count 并发度设置为 `32`，该系统可获得 10 倍的性能提升（较之使用默认值）。\n\n添加索引后，可以使用 `EXPLAIN` 重复该查询。在以下返回结果中，可见 TiDB 选择了新的执行计划，而且不再使用 `TableFullScan` 和 `Selection` 算子。\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';\n```\n\n```sql\n+-----------------------------+---------+-----------+-------------------------------------------+-------------------------------------------------------------------+\n| id                          | estRows | task      | access object                             | operator info                                                     |\n+-----------------------------+---------+-----------+-------------------------------------------+-------------------------------------------------------------------+\n| StreamAgg_17                | 1.00    | root      |                                           | funcs:count(Column#13)->Column#11                                 |\n| └─IndexReader_18            | 1.00    | root      |                                           | index:StreamAgg_9                                                 |\n|   └─StreamAgg_9             | 1.00    | cop[tikv] |                                           | funcs:count(1)->Column#13                                         |\n|     └─IndexRangeScan_16     | 8471.88 | cop[tikv] | table:trips, index:start_date(start_date) | range:[2017-07-01 00:00:00,2017-07-01 23:59:59], keep order:false |\n+-----------------------------+---------+-----------+-------------------------------------------+-------------------------------------------------------------------+\n4 rows in set (0.00 sec)\n```\n\n若要比较实际的执行时间，可再次使用 `EXPLAIN ANALYZE` 语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN ANALYZE SELECT count(*) FROM trips WHERE start_date BETWEEN '2017-07-01 00:00:00' AND '2017-07-01 23:59:59';\n```\n\n```sql\n+-----------------------------+---------+---------+-----------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+-----------+------+\n| id                          | estRows | actRows | task      | access object                             | execution info                                                                                                   | operator info                                                     | memory    | disk |\n+-----------------------------+---------+---------+-----------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+-----------+------+\n| StreamAgg_17                | 1.00    | 1       | root      |                                           | time:4.516728ms, loops:2                                                                                         | funcs:count(Column#13)->Column#11                                 | 372 Bytes | N/A  |\n| └─IndexReader_18            | 1.00    | 1       | root      |                                           | time:4.514278ms, loops:2, cop_task: {num: 1, max:4.462288ms, proc_keys: 11409, rpc_num: 1, rpc_time: 4.457148ms} | index:StreamAgg_9                                                 | 238 Bytes | N/A  |\n|   └─StreamAgg_9             | 1.00    | 1       | cop[tikv] |                                           | time:4ms, loops:12                                                                                               | funcs:count(1)->Column#13                                         | N/A       | N/A  |\n|     └─IndexRangeScan_16     | 8471.88 | 11409   | cop[tikv] | table:trips, index:start_date(start_date) | time:4ms, loops:12                                                                                               | range:[2017-07-01 00:00:00,2017-07-01 23:59:59], keep order:false | N/A       | N/A  |\n+-----------------------------+---------+---------+-----------+-------------------------------------------+------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------+-----------+------+\n4 rows in set (0.00 sec)\n```\n\n从以上结果可看出，查询时间已从 1.03 秒减少到 0.0 秒。\n\n> **注意：**\n>\n> 以上示例另一个可用的优化方案是 [coprocessor cache](/coprocessor-cache.md)。如果你无法添加索引，可考虑开启 coprocessor cache 功能。开启后，只要算子上次执行以来 Region 未作更改，TiKV 将从缓存中返回值。这也有助于减少 `TableFullScan` 和 `Selection` 算子的大部分运算成本。\n\n## 禁止子查询提前执行\n\n在查询优化过程中，TiDB 会提前执行可以在优化阶段直接计算的子查询。例如：\n\n```sql\nCREATE TABLE t1(a int);\nINSERT INTO t1 VALUES(1);\nCREATE TABLE t2(a int);\nEXPLAIN SELECT * FROM t2 WHERE a = (SELECT a FROM t1);\n```\n\n```sql\n+--------------------------+----------+-----------+---------------+--------------------------------+\n| id                       | estRows  | task      | access object | operator info                  |\n+--------------------------+----------+-----------+---------------+--------------------------------+\n| TableReader_14           | 10.00    | root      |               | data:Selection_13              |\n| └─Selection_13           | 10.00    | cop[tikv] |               | eq(test.t2.a, 1)               |\n|   └─TableFullScan_12     | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo |\n+--------------------------+----------+-----------+---------------+--------------------------------+\n3 rows in set (0.00 sec)\n```\n\n在上述例子中 `a = (SELECT a FROM t1)` 子查询在优化阶段就进行了计算，表达式被改写为 `t2.a=1`。这种执行方式可以在优化阶段进行更多的常量传播和常量折叠优化，但是会影响 `EXPLAIN` 语句的执行时间。当子查询本身耗时较长时，`EXPLAIN` 语句无法执行完成，可能会影响线上问题的排查。\n\n从 v7.3.0 开始，TiDB 引入 [`tidb_opt_enable_non_eval_scalar_subquery`](/system-variables.md#tidb_opt_enable_non_eval_scalar_subquery-从-v730-版本开始引入) 系统变量，可以控制这类子查询在 `EXPLAIN` 语句中是否禁止提前执行计算展开。该变量默认值为 `OFF`，即提前计算子查询。你可以将该变量设置为 `ON` 来禁止子查询提前执行：\n\n```sql\nSET @@tidb_opt_enable_non_eval_scalar_subquery = ON;\nEXPLAIN SELECT * FROM t2 WHERE a = (SELECT a FROM t1);\n```\n\n```sql\n+---------------------------+----------+-----------+---------------+---------------------------------+\n| id                        | estRows  | task      | access object | operator info                   |\n+---------------------------+----------+-----------+---------------+---------------------------------+\n| Selection_13              | 8000.00  | root      |               | eq(test.t2.a, ScalarQueryCol#5) |\n| └─TableReader_15          | 10000.00 | root      |               | data:TableFullScan_14           |\n|   └─TableFullScan_14      | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo  |\n| ScalarSubQuery_10         | N/A      | root      |               | Output: ScalarQueryCol#5        |\n| └─MaxOneRow_6             | 1.00     | root      |               |                                 |\n|   └─TableReader_9         | 1.00     | root      |               | data:TableFullScan_8            |\n|     └─TableFullScan_8     | 1.00     | cop[tikv] | table:t1      | keep order:false, stats:pseudo  |\n+---------------------------+----------+-----------+---------------+---------------------------------+\n7 rows in set (0.00 sec)\n```\n\n可以看到，标量子查询在执行阶段并没有被展开，这样更便于理解该类 SQL 具体的执行过程。\n\n> **注意：**\n>\n> [`tidb_opt_enable_non_eval_scalar_subquery`](/system-variables.md#tidb_opt_enable_non_eval_scalar_subquery-从-v730-版本开始引入) 目前仅控制 `EXPLAIN` 语句的行为，`EXPLAIN ANALYZE` 语句仍然会将子查询提前展开。"
        },
        {
          "name": "explore-htap.md",
          "type": "blob",
          "size": 8.330078125,
          "content": "---\ntitle: HTAP 深入探索指南\nsummary: 本文介绍如何深入探索并使用 TiDB 的 HTAP 功能。\n---\n\n# HTAP 深入探索指南\n\n本指南介绍如何进一步探索并使用 TiDB 在线事务与在线分析处理 (Hybrid Transactional and Analytical Processing, HTAP) 功能。\n\n> **注意：**\n>\n> 如果你对 TiDB HTAP 功能还不太了解，希望快速试用体验，请参阅[快速上手 HTAP](/quick-start-with-htap.md)。\n\n要快速了解 TiDB 在 HTAP 场景下的体系架构与 HTAP 的适用场景，建议先观看下面的培训视频（时长 15 分钟）。注意本视频只作为学习参考，如需了解详细的 HTAP 相关内容，请参阅下方的文档内容。\n\n<video src=\"https://download.pingcap.com/docs-cn%2FLesson04_htap.mp4\" width=\"100%\" height=\"100%\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_lesson4.png\"></video>\n\n## HTAP 适用场景\n\nTiDB HTAP 可以满足企业海量数据的增产需求、降低运维的风险成本、与现有的大数据栈无缝缝合，从而实现数据资产价值的实时变现。\n\n以下是三种 HTAP 典型适用场景：\n\n- 混合负载场景\n\n    当将 TiDB 应用于在线实时分析处理的混合负载场景时，开发人员只需要提供一个入口，TiDB 将自动根据业务类型选择不同的处理引擎。\n\n- 实时流处理场景\n\n    当将 TiDB 应用于实时流处理场景时，TiDB 能保证源源不断流入系统的数据实时可查，同时可兼顾高并发数据服务与 BI 查询。\n\n- 数据中枢场景\n\n    当将 TiDB 应用于数据中枢场景时，TiDB 作为数据中枢可以无缝连接数据业务层和数据仓库层，满足不同业务的需求。\n\n如果想了解更多关于 TiDB HTAP 场景信息，请参阅 [PingCAP 官网中关于 HTAP 的博客](https://pingcap.com/zh/blog/?tag=HTAP)。\n\n当遇到以下技术场景时，建议使用 TiDB HTAP 提升 TiDB 数据库整体表现：\n\n- 提升分析性能\n\n    你的业务中存在某些复杂的分析查询，如聚合、关联等操作。当这些分析查询涉及大量数据（超过 1000 万行）时，如果查询涉及的表无法有效利用索引或者索引的选择性较差，而行存储引擎 [TiKV](/tikv-overview.md) 难以满足查询的性能需求。\n\n- 混合负载隔离\n\n    在高并发的 OLTP 业务同时，你可能需要处理一些 OLAP 业务，同时还需要避免 OLAP 查询影响 OLTP 业务性能，确保系统的整体稳定性。\n\n- 简化 ETL 技术栈\n\n    当需要加工的数据量为中等规模（100 TB 以内）、数据加工调度流程相对简单、并发度不高（10 以内）时，你可能希望简化技术栈，替换原本需要使用多个不同技术栈的 OLTP、ETL 和 OLAP 系统，使用一个数据库同时满足交易系统以及分析系统的需求，降低技术门槛和运维人员需求。\n\n- 强一致性分析\n\n    如果需要对业务数据进行实时、强一致的分析计算，并且要求数据分析结果和业务数据完全一致，避免数据延迟和不一致的问题。\n\n## HTAP 架构\n\n在 TiDB 中，面向在线事务处理的行存储引擎 [TiKV](/tikv-overview.md) 与面向实时分析场景的列存储引擎 [TiFlash](/tiflash/tiflash-overview.md) 同时存在，自动同步，保持强一致性。\n\n更多架构信息，请参考 [TiDB HTAP 形态架构](/tiflash/tiflash-overview.md#整体架构)。\n\n## HTAP 环境准备\n\n在深入探索 TiDB HTAP 功能前，请依据你的数据场景部署 TiDB 以及对应的数据分析引擎。大数据场景 (100 T) 下，推荐使用 TiFlash MPP 作为 HTAP 的主要方案，TiSpark 作为补充方案。\n\n- TiFlash\n\n    - 如果已经部署 TiDB 集群但尚未部署 TiFlash 节点，请参阅[扩容 TiFlash 节点](/scale-tidb-using-tiup.md#扩容-tiflash-节点)中的步骤在现有 TiDB 集群中添加 TiFlash 节点。\n    - 如果尚未部署 TiDB 集群，请使用 [TiUP 部署 TiDB 集群](/production-deployment-using-tiup.md)，并在包含最小拓扑的基础上，同时[增加 TiFlash 拓扑架构](/tiflash-deployment-topology.md)。\n    - 在决定如何选择 TiFlash 节点数量时，请考虑以下几种业务场景：\n\n        - 如果业务场景以 OLTP 为主，做轻量级的 Ad hoc OLAP 计算，通常部署 1 个或几个 TiFlash 节点就会产生明显的加速效果。\n        - 当 OLTP 数据吞吐量对节点 I/O 无明显压力时，每个 TiFlash 节点将会使用较多资源用于计算，这样 TiFlash 集群可实现近似线性的扩展能力。TiFlash 节点数量应根据期待的性能和响应时间调整。\n        - 当 OLTP 数据吞吐量较高时（例如写入或更新超过千万行/小时），由于网络和物理磁盘的写入能力有限，内部 TiKV 与 TiFlash 之间的 I/O 会成为主要瓶颈，也容易产生读写热点。此时 TiFlash 节点数与 OLAP 计算量有较复杂非线性关系，需要根据具体系统状态调整节点数量。\n\n- TiSpark\n\n    - 如果你的业务需要基于 Spark 进行分析，请部署 TiSpark。具体步骤，请参阅 [TiSpark 用户指南](/tispark-overview.md)。\n\n<!--    - 实时流处理\n  - 如果你想将 TiDB 与 Flink 结合构建高效易用的实时数仓，请参与 Apache Flink x TiDB Meetup 系列讲座。-->\n\n## HTAP 数据准备\n\nTiFlash 部署完成后并不会自动同步数据，你需要指定需要同步到 TiFlash 的数据表。指定后，TiDB 将创建对应的 TiFlash 副本。\n\n- 如果 TiDB 集群中还没有数据，请先迁移数据到 TiDB。详情请参阅[数据迁移](/migration-overview.md)。\n- 如果 TiDB 集群中已经有从上游同步过来的数据，TiFlash 部署完成后并不会自动同步数据，而需要手动指定需要同步的表，详情请参阅[使用 TiFlash](/tiflash/tiflash-overview.md#使用-tiflash)。\n\n## HTAP 数据处理\n\n使用 TiDB 时，你只需输入 SQL 语句进行查询或者写入需求。对于创建了 TiFlash 副本的表，TiDB 会依靠前端优化器自由选择最优的执行方式。\n\n> **注意：**\n> \n> TiFlash 的 MPP 模式默认开启。当执行 SQL 语句时，TiDB 会通过优化器自动判断并选择是否以 MPP 模式执行。\n>\n> - 如需关闭 MPP 模式，请将系统变量 [tidb_allow_mpp](/system-variables.md#tidb_allow_mpp-从-v50-版本开始引入) 的值设置为 OFF。\n> - 如需强制使用 TiFlash 的 MPP 模式执行查询，请将系统变量 [tidb_allow_mpp](/system-variables.md#tidb_allow_mpp-从-v50-版本开始引入) 和 [tidb_enforce_mpp](/system-variables.md#tidb_enforce_mpp-从-v51-版本开始引入) 的值设置为 ON。\n> - 如需查看 TiDB 是否选择以 MPP 模式执行，你可以[通过 EXPLAIN 语句查看具体的查询执行计划](/explain-mpp.md#用-explain-查看-mpp-模式查询的执行计划)。如果 EXPLAIN 语句的结果中出现 ExchangeSender 和 ExchangeReceiver 算子，表明 MPP 已生效。\n\n## HTAP 性能监控\n\n在 TiDB 的使用过程中，可以选择以下方式监控 TiDB 集群运行情况并查看性能数据。\n\n- [TiDB Dashboard](/dashboard/dashboard-intro.md)：查看集群整体运行概况，分析集群读写流量分布及趋势变化，详细了解耗时较长的 SQL 语句的执行信息。\n- [监控系统 (Prometheus & Grafana)](/grafana-overview-dashboard.md)：查看 TiDB 集群各组件（包括 PD、TiDB、TiKV、TiFlash、TiCDC、Node_exporter）的相关监控参数。\n\n如需查看 TiDB 和 TiFlash 集群报警规则和处理方法，请查阅 [TiDB 集群报警规则](/alert-rules.md)和 [TiFlash 报警规则](/tiflash/tiflash-alert-rules.md)。\n\n## HTAP 故障诊断\n\n在使用 TiDB 的过程中如果遇到问题，请参阅以下文档：\n\n- [分析慢查询](/analyze-slow-queries.md)\n- [定位消耗系统资源多的查询](/identify-expensive-queries.md)\n- [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md)\n- [TiDB 集群故障诊断](/troubleshoot-tidb-cluster.md)\n- [TiFlash 常见问题](/tiflash/troubleshoot-tiflash.md)\n\n除此之外，你可以在 [Github Issues](https://github.com/pingcap/tiflash/issues) 新建一个 Issue 反馈问题，或者在 [AskTUG](https://asktug.com/) 提交你的问题。\n\n## 探索更多\n\n- 如果要查看 TiFlash 版本、TiFlash 重要日志及系统表，请参阅 [TiFlash 集群运维](/tiflash/maintain-tiflash.md)。\n- 如果需要移除某个 TiFlash 节点，请参阅[缩容 TiFlash 节点](/scale-tidb-using-tiup.md#缩容-tiflash-节点)。\n"
        },
        {
          "name": "exporting-grafana-snapshots.md",
          "type": "blob",
          "size": 3.3896484375,
          "content": "---\ntitle: 将 Grafana 监控数据导出成快照\nsummary: 了解如何将 Grafana 监控数据导出为快照以及如何将快照文件可视化。\n---\n\n> **警告：**\n>\n> - 从 TiDB v6.0.0 起，PingCAP 不再维护 MetricsTool 工具。从 v6.1.0 起，不再维护 MetricsTool 的文档。\n> - 如需导出监控数据，建议使用 [PingCAP Clinic 诊断服务](/clinic/clinic-introduction.md)一键导出诊断 TiDB 集群时所需要的信息，包括监控数据、日志、集群拓扑、配置、参数等。\n\n# 将 Grafana 监控数据导出成快照\n\n> **注意：**\n>\n> 目前该工具仅支持在 Grafana v6.x.x 上使用。\n\n在故障诊断中，监控数据十分重要。当你请求远程协助时，技术支持人员有时需要查看 Grafana Dashboard 以确认问题所在。[MetricsTool](https://metricstool.pingcap.net/) 用于将 Grafana Dashboard 的快照导出为本地文件，并将快照可视化。因此，你可以在不泄露 Grafana 服务器上其他敏感信息的前提下，将监控数据以快照形式分享给外部人员，同时也方便外部人员准确识读数据图表。\n\n## 使用方法\n\n可以通过访问 `<https://metricstool.pingcap.net>` 来使用 MetricsTool。它主要提供以下三种功能：\n\n* **导出快照**：提供一段在浏览器开发者工具上运行的用户脚本。你可以使用这个脚本在任意 Grafana v6.x.x 服务器上下载当前 Dashboard 中所有可见面板的快照。\n\n    ![运行用户脚本后的 MetricsTool Exporter 截图](/media/metricstool-export.png)\n\n* **快照可视化**：通过网页端可视化工具 Visualizer 将快照导出文件可视化。快照经过可视化后，操作起来与实际的 Grafana Dashboard 无异。\n\n    ![MetricsTool Visualizer 截图](/media/metricstool-visualize.png)\n\n* **导入快照**：介绍如何将导出的快照重新导入到已有的 Grafana 实例中。\n\n## FAQs\n\n### 与直接截图及导出 PDF 相比，MetricTool 有什么优势？\n\nMetricsTool 导出的快照文件包含快照生成时的监控指标实际数值。你可以通过 Visualizer 与渲染的图表进行交互，比如切换序列、选择一个较小的时间范围以及检查特定时间点的监控数据值等，就像在操作一个实际的 Grafana Dashboard 一样，因此它比 PDF 文件和截图更强大。\n\n### 快照文件里都包含什么？\n\n快照文件包含所选时间范围内所有图表和面板数据的值，但不保存数据源的原始监控指标，所以无法在 Visualizer 中编辑查询表达式。\n\n### Visualizer 会将上传的快照文件保存到 PingCAP 的服务器上吗？\n\n不会。快照文件解析全部在浏览器中完成，Visualizer 不会将任何信息发送给 PingCAP。你可以放心地使用 Visualizer 查看带有敏感信息的快照文件，不用担心信息会泄露给第三方。\n\n### 可以在所有监控指标数据都加载完毕前就运行脚本吗？\n\n可以。虽然脚本会弹出提示，让你等所有监控数据加载完毕后再运行，但可以手动跳过等待并导出快照，以免有些监控数据加载的时间过长。\n\n### 快照文件可视化后，可以通过网页链接分享吗？\n\n不能。但你可以分享快照文件，并说明如何使用 Visualizer 查看。如果确实需要通过网页链接分享，可以尝试使用 Grafana 内置的 `snapshot.raintank.io` 服务。但在这样做之前，要确保不会泄漏隐私信息。\n"
        },
        {
          "name": "expression-syntax.md",
          "type": "blob",
          "size": 2.640625,
          "content": "---\ntitle: 表达式语法\nsummary: 本文列出 TiDB 的表达式语法。\naliases: ['/docs-cn/dev/expression-syntax/','/docs-cn/dev/reference/sql/language-structure/expression-syntax/']\n---\n\n# 表达式语法 (Expression Syntax)\n\n表达式是一个或多个值、操作符或函数的组合。在 TiDB 中，表达式主要使用在 `SELECT` 语句的各个子句中，包括 Group by 子句、Where 子句、Having 子句、Join 条件以及窗口函数等。此外，部分 DDL 语句也会使用到表达式，例如建表时默认值的设置、生成列的设置，分区规则等。\n\n表达式包含几种类型：\n\n+ 标识符，可参考[模式对象名](/schema-object-names.md)。\n+ 谓词、数值、字符串、日期表达式等，这些类型的[字面值](/literal-values.md)也是表达式。\n+ 函数调用，窗口函数等。可参考[函数和操作符概述](/functions-and-operators/functions-and-operators-overview.md)和[窗口函数](/functions-and-operators/window-functions.md)。\n+ 其他，包括 paramMarker（即 `?`）、系统变量和用户变量、CASE 表达式等。\n\n以下规则是表达式的语法，该语法基于 TiDB parser 的 [`parser.y`](https://github.com/pingcap/tidb/blob/master/pkg/parser/parser.y) 文件中所定义的规则。\n\n```ebnf+diagram\nExpression ::=\n    ( singleAtIdentifier assignmentEq | 'NOT' | Expression ( logOr | 'XOR' | logAnd ) ) Expression\n|   'MATCH' '(' ColumnNameList ')' 'AGAINST' '(' BitExpr FulltextSearchModifierOpt ')'\n|   PredicateExpr ( IsOrNotOp 'NULL' | CompareOp ( ( singleAtIdentifier assignmentEq )? PredicateExpr | AnyOrAll SubSelect ) )* ( IsOrNotOp ( trueKwd | falseKwd | 'UNKNOWN' ) )?\n\nPredicateExpr ::=\n    BitExpr ( BetweenOrNotOp BitExpr 'AND' BitExpr )* ( InOrNotOp ( '(' ExpressionList ')' | SubSelect ) | LikeOrNotOp SimpleExpr LikeEscapeOpt | RegexpOrNotOp SimpleExpr )?\n\nBitExpr ::=\n    BitExpr ( ( '|' | '&' | '<<' | '>>' | '*' | '/' | '%' | 'DIV' | 'MOD' | '^' ) BitExpr | ( '+' | '-' ) ( BitExpr | \"INTERVAL\" Expression TimeUnit ) )\n|   SimpleExpr\n\nSimpleExpr ::=\n    SimpleIdent ( ( '->' | '->>' ) stringLit )?\n|   FunctionCallKeyword\n|   FunctionCallNonKeyword\n|   FunctionCallGeneric\n|   SimpleExpr ( 'COLLATE' CollationName | pipes SimpleExpr )\n|   WindowFuncCall\n|   Literal\n|   paramMarker\n|   Variable\n|   SumExpr\n|   ( '!' | '~' | '-' | '+' | 'NOT' | 'BINARY' ) SimpleExpr\n|   'EXISTS'? SubSelect\n|   ( ( '(' ( ExpressionList ',' )? | 'ROW' '(' ExpressionList ',' ) Expression | builtinCast '(' Expression 'AS' CastType | ( 'DEFAULT' | 'VALUES' ) '(' SimpleIdent | 'CONVERT' '(' Expression ( ',' CastType | 'USING' CharsetName ) ) ')'\n|   'CASE' ExpressionOpt WhenClause+ ElseOpt 'END'\n```\n"
        },
        {
          "name": "extended-statistics.md",
          "type": "blob",
          "size": 7.6865234375,
          "content": "---\ntitle: 扩展统计信息\nsummary: 了解如何使用扩展统计信息指导优化器。\n---\n\n# 扩展统计信息\n\nTiDB 可以收集以下两种类型的统计信息，本文主要介绍如何使用扩展统计信息来指导优化器。阅读本文前，建议先阅读[常规统计信息](/statistics.md)。\n\n- 常规统计信息：主要关注单个列的统计信息，例如直方图和 Count-Min Sketch。这些统计信息对优化器估算查询成本至关重要。详情参见[常规统计信息](/statistics.md)。\n- 扩展统计信息：主要关注指定列之间的数据相关性，指导优化器在查询有相关性的列时更精确地估算查询成本。\n\n当手动或自动执行 `ANALYZE` 语句时，TiDB 默认只收集常规统计信息，不收集扩展统计信息。这是因为扩展统计信息仅在特定场景下用于优化器估算，而且收集扩展统计信息会增加额外开销。\n\n扩展统计信息默认关闭。如果要收集扩展统计信息，请先启用扩展统计信息，然后逐个创建所需的扩展统计信息对象。创建完之后，下次执行 `ANALYZE` 语句时，TiDB 会同时收集常规统计信息和已创建对象的扩展统计信息。\n\n> **警告：**\n>\n> 该功能目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n## 使用限制\n\n在以下场景下，TiDB 不会收集扩展统计信息：\n\n- 仅在索引上收集统计信息\n- 使用 `ANALYZE INCREMENTAL` 命令收集统计信息\n- 系统变量 `tidb_enable_fast_analyze` 设置为 `true` 时收集统计信息\n\n## 常用操作\n\n### 启用扩展统计信息\n\n要启用扩展统计信息，需要将系统变量 `tidb_enable_extended_stats` 设置为 `ON`：\n\n```sql\nSET GLOBAL tidb_enable_extended_stats = ON;\n```\n\n该变量的默认值为 `OFF`。该系统变量的设置对所有扩展统计信息对象生效。\n\n### 创建扩展统计信息对象\n\n创建扩展统计信息对象不是一次性任务，你需要为每个要收集的扩展统计信息分别创建对象。\n\n要创建扩展统计信息对象，使用 SQL 语句 `ALTER TABLE ADD STATS_EXTENDED`。语法如下：\n\n```sql\nALTER TABLE table_name ADD STATS_EXTENDED IF NOT EXISTS stats_name stats_type(column_name, column_name...);\n```\n\n你可以在语法中指定要收集扩展统计信息的表名、统计信息名称、统计信息类型和列名。\n\n- `table_name`：指定要收集扩展统计信息的表名。\n- `stats_name`：指定统计信息对象的名称，每个表的统计信息对象名称必须唯一。\n- `stats_type`：指定统计信息的类型。目前仅支持相关性 (correlation) 类型。\n- `column_name`：指定列组，可以有多个列。目前只支持指定两个列名。\n\n<details>\n<summary>实现原理</summary>\n\n为了提高访问性能，每个 TiDB 节点在系统表 `mysql.stats_extended` 中维护一份缓存，用于记录扩展统计信息。在创建扩展统计信息对象后，下次执行 `ANALYZE` 语句时，如果系统表 `mysql.stats_extended` 中有相应的对象，TiDB 将收集扩展统计信息。\n\n`mysql.stats_extended` 系统表中的每一行都有一个 `version` 列。只要一行数据有更新，`version` 的值就会增加。这样，TiDB 会将表增量加载到内存中，而不是全量加载。\n\nTiDB 定期加载 `mysql.stats_extended` 系统表，以确保缓存与表中的数据保持一致。\n\n> **警告：**\n>\n> 不建议直接操作 `mysql.stats_extended` 系统表，否则不同 TiDB 节点上的缓存会不一致。如果误操作了该系统表，可以在每个 TiDB 节点上执行以下语句，以清除当前缓存，并重新加载 `mysql.stats_extended` 系统表：\n>\n> ```sql\n> ADMIN RELOAD STATS_EXTENDED;\n> ```\n\n</details>\n\n### 删除扩展统计信息对象\n\n要删除扩展统计信息对象，使用以下语句：\n\n```sql\nALTER TABLE table_name DROP STATS_EXTENDED stats_name;\n```\n\n<details>\n<summary>实现原理</summary>\n\n在执行删除扩展统计信息对象的语句后，TiDB 不会直接删除 `mysql.stats_extended` 系统表中的对象，而是将相应对象的 `status` 列的值标记为 `2`。其它 TiDB 节点会读取这个变化，并删除内存缓存中的对象。后台的垃圾回收机制会最终删除该对象。\n\n> **警告：**\n>\n> 不建议直接操作 `mysql.stats_extended` 系统表，否则不同 TiDB 节点上的缓存会不一致。如果误操作了该系统表，可以在每个 TiDB 节点上执行以下语句，以清除当前缓存，并重新加载 `mysql.stats_extended` 系统表：\n>\n> ```sql\n> ADMIN RELOAD STATS_EXTENDED;\n> ```\n\n</details>\n\n### 导出和导入扩展统计信息\n\n导出和导入扩展统计信息的方式与导出和导入常规统计信息的方式相同。详情参见[导出和导入常规统计信息](/statistics.md#导出和导入统计信息)。\n\n## 相关性类型的扩展统计信息使用示例\n\n目前，TiDB 仅支持相关性类型的扩展统计信息。该类型用于估算范围查询中的行数，并改善索引选择。以下示例展示了如何使用相关性类型的扩展统计信息来估算范围查询中的行数。\n\n### 第 1 步：定义表\n\n定义表 `t` 如下：\n\n```sql\nCREATE TABLE t(col1 INT, col2 INT, KEY(col1), KEY(col2));\n```\n\n假设表 `t` 的 `col1` 和 `col2` 在行顺序上都遵循单调递增的约束，这意味着 `col1` 和 `col2` 的值在顺序上严格相关，并且相关性系数为 `1`。\n\n### 第 2 步：执行不使用扩展统计信息的示例查询\n\n执行以下查询，不使用扩展统计信息：\n\n```sql\nSELECT * FROM t WHERE col1 > 1 ORDER BY col2 LIMIT 1;\n```\n\n对于上述查询的执行，TiDB 优化器会通过以下方式之一来访问表 `t`：\n\n- 使用 `col1` 上的索引访问表 `t`，然后按 `col2` 对结果进行排序以计算 `Top-1`。\n- 使用 `col2` 上的索引按顺序扫描表 `t`，直到遇到一条满足 `col1 > 1` 条件的记录后结束扫描。这种访问方式的代价主要取决于 TiDB 按 `col2` 的顺序扫描表时过滤掉了多少行。\n\n在没有扩展统计信息的情况下，TiDB 优化器只会假设 `col1` 和 `col2` 是独立的，这会**导致显著的估算误差**。\n\n### 第 3 步：启用扩展统计信息\n\n将系统变量 `tidb_enable_extended_stats` 设置为 `ON`，并将 `col1` 和 `col2` 创建为扩展统计信息对象：\n\n```sql\nALTER TABLE t ADD STATS_EXTENDED s1 correlation(col1, col2);\n```\n\n创建完统计信息对象后，当执行 `ANALYZE` 语句时，TiDB 会计算表 `t` 的 `col1` 和 `col2` 的[皮尔逊相关系数](https://zh.wikipedia.org/zh-cn/皮尔逊积矩相关系数)，并将对象写入 `mysql.stats_extended` 系统表。\n\n### 第 4 步：查看扩展统计信息的效果\n\n当 TiDB 有了相关性扩展统计信息后，优化器可以更准确地估算需要扫描的行数。\n\n此时，对于上述[第 2 步](#第-2-步执行不使用扩展统计信息的示例查询)中的查询，`col1` 和 `col2` 在顺序上严格相关。如果 TiDB 通过 `col2` 上的索引按顺序扫描表 `t`，直到遇到一条满足 `col1 > 1` 条件的记录后结束扫描，优化器会将行数估算等价转换为以下查询：\n\n```sql\nSELECT * FROM t WHERE col1 <= 1 OR col1 IS NULL;\n```\n\n上述查询结果加 1 即为对行数的最终估算值。这样不再使用独立不相关假设，**避免了显著的估算误差**。\n\n如果相关性系数（本例中为 `1`）小于系统变量 `tidb_opt_correlation_threshold` 的值，优化器会使用独立假设，但会试探性地调大估算值。`tidb_opt_correlation_exp_factor` 的值越大，估算结果越大。相关性系数的绝对值越大，估算结果越大。\n"
        },
        {
          "name": "external-storage-uri.md",
          "type": "blob",
          "size": 5.0712890625,
          "content": "---\ntitle: 外部存储服务的 URI 格式\nsummary: 介绍了外部存储服务 Amazon S3、GCS、和 Azure Blob Storage 的 URI 格式。\n---\n\n# 外部存储服务的 URI 格式\n\n本文介绍 Amazon S3、GCS、和 Azure Blob Storage 存储服务的 URI 格式。基本格式如下：\n\n```shell\n[scheme]://[host]/[path]?[parameters]\n```\n\n## Amazon S3 URI 格式\n\n- `scheme`：`s3`\n- `host`：`bucket name`\n- `parameters`：\n\n    - `access-key`：访问密钥\n    - `secret-access-key`：秘密访问密钥\n    - `session-token`：临时会话令牌（BR v7.6.0 及之后版本支持）\n    - `use-accelerate-endpoint`：是否在 Amazon S3 上使用加速端点，默认为 `false`\n    - `endpoint`：Amazon S3 兼容服务自定义端点的 URL，例如 `<https://s3.example.com/>`\n    - `force-path-style`：使用路径类型 (path-style)，而不是虚拟托管类型 (virtual-hosted-style)，默认为 `true`\n    - `storage-class`：上传对象的存储类别，例如 `STANDARD`、`STANDARD_IA`\n    - `sse`：加密上传的服务端加密算法，可以设置为空、`AES256` 或 `aws:kms`\n    - `sse-kms-key-id`：如果 `sse` 设置为 `aws:kms`，则使用该参数指定 KMS ID\n    - `acl`：上传对象的标准 ACL (Canned ACL)，例如 `private`、`authenticated-read`\n    - `role-arn`：当需要使用特定的 [IAM 角色](https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/id_roles.html)来访问第三方 Amazon S3 的数据时，使用这个参数来指定 IAM 角色的对应 [Amazon Resource Name (ARN)](https://docs.aws.amazon.com/zh_cn/general/latest/gr/aws-arns-and-namespaces.html)（例如 `arn:aws:iam::888888888888:role/my-role`）。关于使用 IAM 角色访问第三方 Amazon S3 数据的场景，请参考 [AWS 相关文档介绍](https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/id_roles_common-scenarios_third-party.html)。（BR v7.6.0 及之后版本支持）\n    - `external-id`：当需要使用特定的 [IAM 角色](https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/id_roles.html)来访问第三方 Amazon S3 的数据时，可能需要同时提供正确的[外部 ID](https://docs.aws.amazon.com/zh_cn/IAM/latest/UserGuide/id_roles_create_for-user_externalid.html) 来确保用户有权限代入该 IAM 角色。这个参数用来指定对应的外部 ID，确保成功代入 IAM 角色。外部 ID 可以是任意字符串，并且不是必须的，一般由控制 Amazon S3 数据访问的第三方来指定。如果第三方对于 IAM 角色没有要求指定外部 ID，则可以不需要提供该参数也能顺利代入对应的 IAM 角色，从而访问对应的 Amazon S3 数据。\n\n以下是用于 TiDB Lightning 和 BR 的 Amazon S3 URI 示例，需要指定文件夹路径 `testfolder`：\n\n```shell\ns3://external/testfolder?access-key=${access-key}&secret-access-key=${secret-access-key}\n```\n\n以下是用于 TiCDC `sink-uri` 的 Amazon S3 URI 示例：\n\n```shell\ntiup cdc:v7.5.0 cli changefeed create \\\n    --server=http://172.16.201.18:8300 \\\n    --sink-uri=\"s3://cdc?endpoint=http://10.240.0.38:9000&access-key=${access-key}&secret-access-key=${secret-access-key}\" \\\n    --changefeed-id=\"cdcTest\" \\\n    --config=cdc_csv.toml\n```\n\n以下是用于 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md) 的 Amazon S3 URI 示例，需要指定具体的文件名 `test.csv`：\n\n```shell\ns3://external/test.csv?access-key=${access-key}&secret-access-key=${secret-access-key}\n```\n\n## GCS URI 格式\n\n- `scheme`：`gcs` 或 `gs`\n- `host`：`bucket name`\n- `parameters`：\n\n    - `credentials-file`：迁移工具节点上凭证 JSON 文件的路径\n    - `storage-class`：上传对象的存储类别，例如 `STANDARD` 或 `COLDLINE`\n    - `predefined-acl`：上传对象的预定义 ACL，例如 `private` 或 `project-private`\n\n以下是用于 TiDB Lightning 和 BR 的 GCS URI 示例，需要指定文件夹路径 `testfolder`：\n\n```shell\ngcs://external/testfolder?credentials-file=${credentials-file-path}\n```\n\n以下是用于 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md) 的 GCS URI 示例，需要指定具体的文件名 `test.csv`：\n\n```shell\ngcs://external/test.csv?credentials-file=${credentials-file-path}\n```\n\n## Azure Blob Storage URI 格式\n\n- `scheme`：`azure` 或 `azblob`\n- `host`：`container name`\n- `parameters`：\n\n    - `account-name`：存储账户名\n    - `account-key`：访问密钥\n    - `sas-token`：共享访问签名令牌\n    - `access-tier`：上传对象的存储类别，例如 `Hot`、`Cool`、`Archive`，默认值为该存储账户的默认访问层。\n    - `encryption-scope`：服务端的[加密范围 (Encryption Scope)](https://learn.microsoft.com/zh-cn/azure/storage/blobs/encryption-scope-manage?tabs=powershell#upload-a-blob-with-an-encryption-scope)\n    - `encryption-key`：服务端使用的[加密密钥 (Encryption Key)](https://learn.microsoft.com/zh-cn/azure/storage/blobs/encryption-customer-provided-keys)，采用的加密算法为 AES256\n\n以下是用于 BR 的 Azure Blob Storage URI 示例，需要指定文件夹路径 `testfolder`：\n\n```shell\nazure://external/testfolder?account-name=${account-name}&account-key=${account-key}\n```\n"
        },
        {
          "name": "faq",
          "type": "tree",
          "content": null
        },
        {
          "name": "filter-binlog-event.md",
          "type": "blob",
          "size": 4.1240234375,
          "content": "---\ntitle: 如何过滤 binlog 事件\nsummary: 介绍如何过滤 binlog 事件。\n---\n\n# 如何过滤 binlog 事件\n\n本文档介绍使用 DM 持续增量数据同步时，如何过滤 binlog 事件。具体迁移操作可参考已有数据迁移场景：\n\n- [从小数据量 MySQL 迁移数据到 TiDB](/migrate-small-mysql-to-tidb.md)\n- [从大数据量 MySQL 迁移数据到 TiDB](/migrate-large-mysql-to-tidb.md)\n- [从小数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-small-mysql-shards-to-tidb.md)\n- [从大数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-large-mysql-shards-to-tidb.md)\n\n## 配置方式\n\n配置 DM 的任务配置文件时，增加如下`filter`，具体配置示例如下图：\n\n```yaml\nfilters:\n  rule-1:\n    schema-pattern: \"test_*\"\n    table-pattern: \"t_*\"\n    events: [\"truncate table\", \"drop table\"]\n    sql-pattern: [\"^DROP\\\\s+PROCEDURE\", \"^CREATE\\\\s+PROCEDURE\"]\n    action: Ignore\n```\n\n- `schema-pattern`/`table-pattern`：对匹配上的 schema 或 table 进行过滤\n- `events`：binlog events，支持的 Event 如下表所示:\n\n| Event           | 分类 | 说明                       |\n| --------------- | ---- | --------------------------|\n| all             |      | 匹配所有 events            |\n| all dml         |      | 匹配所有 DML events        |\n| all ddl         |      | 匹配所有 DDL events        |\n| none            |      | 不匹配任何 events          |\n| none ddl        |      | 不包含任何 DDL events      |\n| none dml        |      | 不包含任何 DML events      |\n| insert          | DML  | 匹配 insert DML event      |\n| update          | DML  | 匹配 update DML event      |\n| delete          | DML  | 匹配 delete DML event      |\n| create database | DDL  | 匹配 create database event |\n| drop database   | DDL  | 匹配 drop database event   |\n| create table    | DDL  | 匹配 create table event    |\n| create index    | DDL  | 匹配 create index event    |\n| drop table      | DDL  | 匹配 drop table event      |\n| truncate table  | DDL  | 匹配 truncate table event  |\n| rename table    | DDL  | 匹配 rename table event    |\n| drop index      | DDL  | 匹配 drop index event      |\n| alter table     | DDL  | 匹配 alter table event     |\n\n- `sql-pattern`：匹配指定的 DDL SQL 语句，支持正则表达式匹配。\n- `action`：可取值 Do 或 Ignore。\n    - `Do`：白名单。binlog event 如果满足下面两个条件之一将会被同步：\n        - 符合 events 条件；\n        - sql-pattern 不为空，且对应的 SQL 可以匹配上 sql-pattern 中任意一项。\n    - `Ignore`：黑名单。如果满足下面两个条件之一就会被过滤掉：\n        - 符合 events 条件；\n        - sql-pattern 不为空，且对应的 SQL 可以匹配上 sql-pattern 中任意一项\n\n注意：如果同时配置 `Do/Ignore`，则 `Ignore` 优先级更高。`binlog event` 不匹配白名单或者匹配黑名单都将被直接过滤。\n\n## 使用场景举例\n\n### 过滤分库分表的所有删除操作\n\n设置 `filter-table-rule` 和 `filter-schema-rule` 两个过滤规则，具体如下：\n\n```\nfilters:\n  filter-table-rule:\n    schema-pattern: \"test_*\"\n    table-pattern: \"t_*\"\n    events: [\"truncate table\", \"drop table\", \"delete\"]\n    action: Ignore\n  filter-schema-rule:\n    schema-pattern: \"test_*\"\n    events: [\"drop database\"]\n    action: Ignore\n```\n\n### 只迁移分库分表的 DML 操作\n\n设置两个 `Binlog event filter rule`：\n\n```\nfilters:\n  do-table-rule:\n    schema-pattern: \"test_*\"\n    table-pattern: \"t_*\"\n    events: [\"create table\", \"all dml\"]\n    action: Do\n  do-schema-rule:\n    schema-pattern: \"test_*\"\n    events: [\"create database\"]\n    action: Do\n```\n\n### 过滤 TiDB 不支持的 SQL 语句\n\n```\nfilters:\n  filter-procedure-rule:\n    schema-pattern: \"*\"\n    sql-pattern: [\".*\\\\s+DROP\\\\s+PROCEDURE\", \".*\\\\s+CREATE\\\\s+PROCEDURE\", \"ALTER\\\\s+TABLE[\\\\s\\\\S]*ADD\\\\s+PARTITION\", \"ALTER\\\\s+TABLE[\\\\s\\\\S]*DROP\\\\s+PARTITION\"]\n    action: Ignore\n```\n\n> **注意：**\n>\n> 全局过滤规则的设置必须尽可能严格，以避免过滤掉需要迁移的数据。\n\n## 探索更多\n\n- [如何通过 SQL 表达式过滤 binlog](/filter-dml-event.md)"
        },
        {
          "name": "filter-dml-event.md",
          "type": "blob",
          "size": 4.7578125,
          "content": "---\ntitle: 如何通过 SQL 表达式过滤 DML\nsummary: 介绍如何通过 SQL 表达式过滤 DML 事件\n---\n\n# 如何通过 SQL 表达式过滤 DML\n\n本文档介绍使用 DM 持续增量数据同步时，如何更加精细的过滤 binlog 事件。具体迁移操作可参考已有数据迁移场景：\n\n- [从小数据量 MySQL 迁移数据到 TiDB](/migrate-small-mysql-to-tidb.md)\n- [从大数据量 MySQL 迁移数据到 TiDB](/migrate-large-mysql-to-tidb.md)\n- [从小数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-small-mysql-shards-to-tidb.md)\n- [从大数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-large-mysql-shards-to-tidb.md)\n\n在进行增量数据迁移时，可以通过[如何过滤 binlog 事件](/filter-binlog-event.md)功能过滤某些类型的 binlog event，例如不向下游迁移 `DELETE` 事件以达到归档、审计等目的。但是 binlog event filter 无法以更细粒度判断某一行的 `DELETE` 事件是否要被过滤。\n\n为了解决上述问题，从 v2.0.5 起，DM 支持在增量数据同步阶段使用`binlog value filter`过滤迁移数据。DM 支持的 `ROW` 格式的 binlog 中，binlog event 带有所有列的值。你可以基于这些值配置 SQL 表达式。如果该表达式对于某条行变更的计算结果是 `TRUE`，DM 就不会向下游迁移该条行变更。\n\n与[如何过滤 binlog 事件](/filter-binlog-event.md)类似，表达式过滤需要在数据迁移任务配置文件里配置，详见下面配置样例。完整的配置及意义，可以参考 [DM 完整配置文件示例](/dm/task-configuration-file-full.md#完整配置文件示例)：\n\n```yaml\nname: test\ntask-mode: all\n\nmysql-instances:\n  - source-id: \"mysql-replica-01\"\n    expression-filters: [\"even_c\"]\n\nexpression-filter:\n  even_c:\n    schema: \"expr_filter\"\n    table: \"tbl\"\n    insert-value-expr: \"c % 2 = 0\"\n```\n\n上面的示例配置了 `even_c` 规则，并让 source ID 为 `mysql-replica-01` 的数据源引用了该规则。`even_c` 规则的含义是：对于 `expr_filter` 库下的 `tbl` 表，当插入的 `c` 的值为偶数 (`c % 2 = 0`) 时，不将这条插入语句迁移到下游。下面展示该规则的使用效果。\n\n在上游数据源增量插入以下数据：\n\n```sql\nINSERT INTO tbl(id, c) VALUES (1, 1), (2, 2), (3, 3), (4, 4);\n```\n\n随后在下游查询 `tbl` 表，可见只有 `c` 的值为单数的行迁移到了下游：\n\n```\nMySQL [test]> select * from tbl;\n\n+------+------+\n| id   | c    |\n+------+------+\n|    1 |    1 |\n|    3 |    3 |\n+------+------+\n\n2 rows in set (0.001 sec)\n```\n\n## 配置参数及规则说明\n\n- `schema`：要匹配的上游数据库库名，不支持通配符匹配或正则匹配。\n- `table`: 要匹配的上游表名，不支持通配符匹配或正则匹配。\n- `insert-value-expr`：配置一个表达式，对 INSERT 类型的 binlog event (WRITE_ROWS_EVENT) 带有的值生效。不能与 `update-old-value-expr`、`update-new-value-expr`、`delete-value-expr` 出现在一个配置项中。\n- `update-old-value-expr`：配置一个表达式，对 UPDATE 类型的 binlog event (UPDATE_ROWS_EVENT) 更新对应的旧值生效。不能与 `insert-value-expr`、`delete-value-expr` 出现在一个配置项中。\n- `update-new-value-expr`：配置一个表达式，对 UPDATE 类型的 binlog event (UPDATE_ROWS_EVENT) 更新对应的新值生效。不能与 `insert-value-expr`、`delete-value-expr` 出现在一个配置项中。\n- `delete-value-expr`：配置一个表达式，对 DELETE 类型的 binlog event (DELETE_ROWS_EVENT) 带有的值生效。不能与 `insert-value-expr`、`update-old-value-expr`、`update-new-value-expr` 出现在一个配置项中。\n\n> **注意：**\n>\n> - `update-old-value-expr` 可以与 `update-new-value-expr` 同时配置。\n> - 当二者同时配置时，会将“更新+旧值“满足`update-old-value-expr` **且**”更新+新值“满足 `update-new-value-expr` 的行过滤掉。\n> - 当只配置一者时，配置的这条表达式会决定是否过滤**整个行变更**，即旧值的删除和新值的插入会作为一个整体被过滤掉。\n\nSQL 表达式可以涉及一列或多列，也可使用 TiDB 支持的 SQL 函数，例如 `c % 2 = 0`、`a*a + b*b = c*c`、`ts > NOW()`。\n\nTIMESTAMP 类型的默认时区是任务配置文件中指定的时区，默认值是下游时区。可以使用 `c_timestamp = '2021-01-01 12:34:56.5678+08:00'` 的方式显式指定时区。\n\n配置项 `expression-filter` 下可以定义多条过滤规则，上游数据源在其 `expression-filters` 配置项中引用需要的规则使其生效。当有多条规则生效时，匹配到**任意一条**规则即会导致某个行变更被过滤。\n\n> **注意：**\n>\n> 为某张表设置过多的表达式过滤会增加 DM 的计算开销，可能导致数据迁移速度变慢。"
        },
        {
          "name": "follower-read.md",
          "type": "blob",
          "size": 6.8115234375,
          "content": "---\ntitle: Follower Read\nsummary: 了解 Follower Read 的使用与实现。\naliases: ['/docs-cn/dev/follower-read/','/docs-cn/dev/reference/performance/follower-read/']\n---\n\n# Follower Read\n\n当系统中存在读取热点 Region 导致 leader 资源紧张成为整个系统读取瓶颈时，启用 Follower Read 功能可明显降低 leader 的负担，并且通过在多个 follower 之间均衡负载，显著地提升整体系统的吞吐能力。本文主要介绍 Follower Read 的使用方法与实现机制。\n\n## 概述\n\nFollower Read 功能是指在强一致性读的前提下使用 Region 的 follower 副本来承载数据读取的任务，从而提升 TiDB 集群的吞吐能力并降低 leader 负载。Follower Read 包含一系列将 TiKV 读取负载从 Region 的 leader 副本上 offload 到 follower 副本的负载均衡机制。TiKV 的 Follower Read 可以保证数据读取的一致性，可以为用户提供强一致的数据读取能力。\n\n> **注意：**\n>\n> 为了获得强一致读取的能力，在当前的实现中，follower 节点需要向 leader 节点询问当前的执行进度（即 `ReadIndex`），这会产生一次额外的网络请求开销，因此目前 Follower Read 的主要优势是将集群中的读请求与写请求隔离开，并提升整体的读吞吐量。\n\n## 使用方式\n\n要开启 TiDB 的 Follower Read 功能，将变量 `tidb_replica_read` 的值设置为对应的目标值即可：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset [session | global] tidb_replica_read = '<目标值>';\n```\n\n作用域：SESSION | GLOBAL\n\n默认值：leader\n\n该变量用于设置期待的数据读取方式。\n\n- 当设置为默认值 `leader` 或者空字符串时，TiDB 会维持原有行为方式，将所有的读取操作都发送给 leader 副本处理。\n- 当设置为 `follower` 时，TiDB 会选择 Region 的 follower 副本完成所有的数据读取操作。\n- 当设置为 `leader-and-follower` 时，TiDB 可以选择任意副本来执行读取操作，此时读请求会在 leader 和 follower 之间负载均衡。\n- 当设置为 `prefer-leader` 时，TiDB 会优先选择 leader 副本执行读取操作。当 leader 副本的处理速度明显变慢时，例如由于磁盘或网络性能抖动，TiDB 将选择其他可用的 follower 副本来执行读取操作。\n- 当设置为 `closest-replicas` 时，TiDB 会优先选择分布在同一可用区的副本执行读取操作，对应的副本可以是 leader 或 follower。如果同一可用区内没有副本分布，则会从 leader 执行读取。\n- 当设置为 `closest-adaptive` 时：\n\n    - 当一个读请求的预估返回结果大于或等于变量 [`tidb_adaptive_closest_read_threshold`](/system-variables.md#tidb_adaptive_closest_read_threshold-从-v630-版本开始引入) 的值时，TiDB 会优先选择分布在同一可用区的副本执行读取操作。此时，为了避免读流量在各个可用区分布不均衡，TiDB 会动态检测当前在线的所有 TiDB 和 TiKV 的可用区数量分布，在每个可用区中 `closest-adaptive` 配置实际生效的 TiDB 节点数总是与包含 TiDB 节点最少的可用区中的 TiDB 节点数相同，并将其他多出的 TiDB 节点自动切换为读取 leader 副本。例如，如果 TiDB 分布在 3 个可用区，其中 A 和 B 两个可用区各包含 3 个 TiDB 节点，C 可用区只包含 2 个 TiDB 节点，那么每个可用区中 `closest-adaptive` 实际生效的 TiDB 节点数为 2，A 和 B 可用区中各有 1 个节点自动被切换为读取 leader 副本。\n    - 当一个读请求的预估返回结果小于变量 [`tidb_adaptive_closest_read_threshold`](/system-variables.md#tidb_adaptive_closest_read_threshold-从-v630-版本开始引入) 的值时，TiDB 会选择 leader 副本执行读取操作。\n\n- 当设置为 `learner` 时，TiDB 会选择 learner 副本执行读取操作。在读取时，如果当前 Region 没有 learner 副本，TiDB 会报错。\n\n> **注意：**\n>\n> 当设置为 `closest-replicas` 或 `closest-adaptive` 时，你需要配置集群以确保副本按照指定的设置分布在各个可用区。请参考[通过拓扑 label 进行副本调度](/schedule-replicas-by-topology-labels.md)为 PD 配置 `location-labels` 并为 TiDB 和 TiKV 设置正确的 `labels`。TiDB 依赖 `zone` 标签匹配位于同一可用区的 TiKV，因此请**务必**在 PD 的 `location-labels` 配置中包含 `zone` 并确保每个 TiDB 和 TiKV 节点的 `labels` 配置中包含 `zone`。如果是使用 TiDB Operator 部署的集群，请参考[数据的高可用](https://docs.pingcap.com/zh/tidb-in-kubernetes/v1.4/configure-a-tidb-cluster#%E6%95%B0%E6%8D%AE%E7%9A%84%E9%AB%98%E5%8F%AF%E7%94%A8)进行配置。\n\n## 实现机制\n\n在 Follower Read 功能出现之前，TiDB 采用 strong leader 策略将所有的读写操作全部提交到 Region 的 leader 节点上完成。虽然 TiKV 能够很均匀地将 Region 分散到多个物理节点上，但是对于每一个 Region 来说，只有 leader 副本能够对外提供服务，另外的 follower 除了时刻同步数据准备着 failover 时投票切换成为 leader 外，没有办法对 TiDB 的请求提供任何帮助。\n\n为了允许在 TiKV 的 follower 节点进行数据读取，同时又不破坏线性一致性和 Snapshot Isolation 的事务隔离，Region 的 follower 节点需要使用 Raft `ReadIndex` 协议确保当前读请求可以读到当前 leader 上已经 commit 的最新数据。在 TiDB 层面，Follower Read 只需根据负载均衡策略将某个 Region 的读取请求发送到 follower 节点。\n\n### Follower 强一致读\n\nTiKV follower 节点处理读取请求时，首先使用 Raft `ReadIndex` 协议与 Region 当前的 leader 进行一次交互，来获取当前 Raft group 最新的 commit index。本地 apply 到所获取的 leader 最新 commit index 后，便可以开始正常的读取请求处理流程。\n\n### Follower 副本选择策略\n\n由于 TiKV 的 Follower Read 不会破坏 TiDB 的 Snapshot Isolation 事务隔离级别，因此 TiDB 选择 follower 的策略可以采用 round robin 的方式。目前，对于 Coprocessor 请求，Follower Read 负载均衡策略粒度是连接级别的，对于一个 TiDB 的客户端连接在某个具体的 Region 上会固定使用同一个 follower，只有在选中的 follower 发生故障或者因调度策略发生调整的情况下才会进行切换。而对于非 Coprocessor 请求（点查等），Follower Read 负载均衡策略粒度是事务级别的，对于一个 TiDB 的事务在某个具体的 Region 上会固定使用同一个 follower，同样在 follower 发生故障或者因调度策略发生调整的情况下才会进行切换。如果同一事务内既有点查请求又有 Coprocessor 请求，两种请求都将按照上述调度策略分别进行读取，即使 Coprocessor 和点查出现在同一个 Region 上，TiDB 也会当作独立事件来处理。\n"
        },
        {
          "name": "foreign-key.md",
          "type": "blob",
          "size": 19.513671875,
          "content": "---\ntitle: 外键约束\nsummary: TiDB 数据库中外键约束的使用概况。\n---\n\n# 外键约束\n\n外键允许跨表交叉引用相关数据，外键约束则可以保证相关数据的一致性。从 v6.6.0 开始，TiDB 支持外键以及外键约束功能。从 v8.5.0 开始，该功能成为正式功能。\n\n> **警告：**\n>\n> 外键功能通常用于强制执行[参照完整性](https://zh.wikipedia.org/wiki/%E5%8F%82%E7%85%A7%E5%AE%8C%E6%95%B4%E6%80%A7)约束检查。使用该功能可能会导致性能下降，在将其应用于性能敏感的场景前，建议先进行全面测试。\n\n外键是在子表中定义的，语法如下：\n\n```ebnf+diagram\nForeignKeyDef\n         ::= ( 'CONSTRAINT' Identifier )? 'FOREIGN' 'KEY'\n             Identifier? '(' ColumnName ( ',' ColumnName )* ')'\n             'REFERENCES' TableName '(' ColumnName ( ',' ColumnName )* ')'\n             ( 'ON' 'DELETE' ReferenceOption )?\n             ( 'ON' 'UPDATE' ReferenceOption )?\n\nReferenceOption\n         ::= 'RESTRICT'\n           | 'CASCADE'\n           | 'SET' 'NULL'\n           | 'SET' 'DEFAULT'\n           | 'NO' 'ACTION'\n```\n\n## 命名\n\n外键的命名遵循以下规则：\n\n- 如果在 `CONSTRAINT identifier` 语句中指定了名称，则使用该名称。\n- 如果 `CONSTRAINT identifier` 语句未指定名称，但在 `FOREIGN KEY identifier` 语句中指定了名称，则使用 `FOREIGN KEY identifier` 定义的名称。\n- 如果 `CONSTRAINT identifier` 和 `FOREIGN KEY identifier` 语句都没有指定名称，则会自动生成一个名称，例如 `fk_1`、`fk_2`、`fk_3` 等。\n- 外键名称必须在当前表中唯一，否则创建时会报错 `ERROR 1826: Duplicate foreign key constraint name 'fk'`。\n\n## 限制\n\n创建外键时需要满足以下条件：\n\n- 父表和子表都不能是临时表。\n- 用户需要对父表有 `REFERENCES` 权限。\n- 外键中的列和引用的父表中的列必须是相同的数据类型，并具有相同的大小、精度、长度、字符集 (charset) 和排序规则 (collation)。\n- 外键中的列不能引用自身。\n- 外键中的列和引用的父表中的列必须有相同的索引，并且索引中的列顺序必须与外键的列顺序一样，这样才能在执行外键约束检查时使用索引来避免全表扫描。\n\n    - 如果父表中没有对应的外键索引，则会报错 `ERROR 1822: Failed to add the foreign key constraint. Missing index for constraint 'fk' in the referenced table 't'`。\n    - 如果子表中没有对应的外键索引，则会自动创建一个索引，索引名和外键名一样。\n\n- 不支持在 `BLOB` 和 `TEXT` 类型的列上创建外键。\n- 不支持在分区表上创建外键。\n- 不支持在虚拟生成列 (`VIRTUAL GENERATED COLUMNS`) 上创建外键。\n\n## 引用操作\n\n当 `UPDATE` 或 `DELETE` 操作影响父表中的外键值时，其在子表中相匹配的外键值取决于外键定义中 `ON UPDATE` 和 `ON DELETE` 定义的引用操作，引用操作包括：\n\n- `CASCADE`：当 `UPDATE` 或 `DELETE` 父表中的行数据时，自动级联更新或删除子表中的匹配行数据。级联操作会用深度优先方式执行。\n- `SET NULL`：当 `UPDATE` 或 `DELETE` 父表中的行数据时，自动将子表中匹配的外键列数据设置为 `NULL`。\n- `RESTRICT`：如果子表中存在外键匹配的行数据，则拒绝 `UPDATE` 或 `DELETE` 父表的操作。\n- `NO ACTION`：行为和 `RESTRICT` 一样。\n- `SET DEFAULT`：行为和 `RESTRICT` 一样。\n\n如果父表中没有匹配的外键值，则拒绝 `INSERT` 或 `UPDATE` 子表的操作。\n\n如果外键定义中没有指定 `ON DELETE` 或者 `ON UPDATE`，则默认的行为是 `NO ACTION`。\n\n如果外键是定义在 `STORED GENERATED COLUMN` 上的，则不支持使用 `CASCADE`、`SET NULL` 和 `SET DEFAULT` 引用操作。\n\n## 外键使用示例\n\n下面的示例通过单列外键关联父表和子表：\n\n```sql\nCREATE TABLE parent (\n    id INT KEY\n);\n\nCREATE TABLE child (\n    id INT,\n    pid INT,\n    INDEX idx_pid (pid),\n    FOREIGN KEY (pid) REFERENCES parent(id) ON DELETE CASCADE\n);\n```\n\n下面是一个更复杂的示例，其中 `product_order` 表有两个外键分别引用其他两个表。一个外键引用 `product` 表中的两列索引。另一个引用 `customer` 表中的单列索引：\n\n```sql\nCREATE TABLE product (\n    category INT NOT NULL,\n    id INT NOT NULL,\n    price DECIMAL(20,10),\n    PRIMARY KEY(category, id)\n);\n\nCREATE TABLE customer (\n    id INT KEY\n);\n\nCREATE TABLE product_order (\n    id INT NOT NULL AUTO_INCREMENT,\n    product_category INT NOT NULL,\n    product_id INT NOT NULL,\n    customer_id INT NOT NULL,\n\n    PRIMARY KEY(id),\n    INDEX (product_category, product_id),\n    INDEX (customer_id),\n\n    FOREIGN KEY (product_category, product_id)\n      REFERENCES product(category, id)\n      ON UPDATE CASCADE ON DELETE RESTRICT,\n\n    FOREIGN KEY (customer_id)\n      REFERENCES customer(id)\n);\n```\n\n## 新增外键约束\n\n可以使用下面 `ALTER TABLE` 语句来新增一个外键约束：\n\n```sql\nALTER TABLE table_name\n    ADD [CONSTRAINT [identifier]] FOREIGN KEY\n    [identifier] (col_name, ...)\n    REFERENCES tbl_name (col_name,...)\n    [ON DELETE reference_option]\n    [ON UPDATE reference_option]\n```\n\n外键可以是自引用的，即引用同一个表。使用 `ALTER TABLE` 向表添加外键约束时，请先在外键引用父表的列上创建索引。\n\n## 删除外键约束\n\n可以使用下面 `ALTER TABLE` 语句来删除一个外键约束：\n\n```sql\nALTER TABLE table_name DROP FOREIGN KEY fk_identifier;\n```\n\n如果外键约束在创建时定义了名称，则可以引用该名称来删除外键约束。否则，只能引用自动生成的约束名称进行删除。你可以使用 `SHOW CREATE TABLE` 查看外键名称：\n\n```sql\nmysql> SHOW CREATE TABLE child\\G\n*************************** 1. row ***************************\n       Table: child\nCreate Table: CREATE TABLE `child` (\n  `id` int DEFAULT NULL,\n  `pid` int DEFAULT NULL,\n  KEY `idx_pid` (`pid`),\n  CONSTRAINT `fk_1` FOREIGN KEY (`pid`) REFERENCES `test`.`parent` (`id`) ON DELETE CASCADE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin\n```\n\n## 外键约束检查\n\nTiDB 支持是否开启外键约束检查，由系统变量 [`foreign_key_checks`](/system-variables.md#foreign_key_checks) 控制，其默认值是 `ON`，即开启外键约束检查，它有 `GLOBAL` 和 `SESSION` 两种作用域。在一般的操作中保持该变量开启可以保证外键引用关系的完整性。\n\n关闭外键约束检查的作用如下：\n\n- 当删除一个被外键引用的父表时，只有关闭外键约束检查时才能删除成功。\n- 当给数据库导入数据时，创建表的顺序可能和外键依赖顺序不一样而导致创建表报错，只有关闭外键约束检查时才能创建表成功，另外，导入数据时关闭外键约束检查也能加快导数据的速度。\n- 当给数据库导入数据时，先导入子表的数据会报错，只有关闭外键约束检查，才能确保顺利导入子表数据。\n- 执行有关外键的 `ALTER TABLE` 操作时，关闭外键约束检查才能执行成功。\n\n当关闭关键约束检查时，不会执行外键约束检查以及引用操作，但以下场景除外：\n\n- 如果执行 `ALTER TABLE` 会导致外键定义不正确，则依然会执行报错。\n- 删除外键所需的索引时，需要先删除外键，否则删除外键会执行报错。\n- 创建外键时，如果不符合外键的条件或限制，则依然会执行报错。\n\n## 锁\n\n在 `INSERT` 或者 `UPDATE` 子表时，外键约束会检查父表中是否存在对应的外键值，并对父表中的该行数据上锁，避免该外键值被其他操作删除，导致破坏外键约束。这里的上锁行为等同于对父表中外键值所在行做 `SELECT FOR UPDATE` 操作。\n\n因为 TiDB 目前暂不支持 `LOCK IN SHARE MODE`，所以，在并发写入子表场景，如果引用的外键值大部分都一样，可能会有比较严重的锁冲突。建议在大批量写入子表数据时，关闭 [`foreign_key_checks`](/system-variables.md#foreign_key_checks)。\n\n## 外键的定义和元信息\n\n你可以使用 [`SHOW CREATE TABLE`](/sql-statements/sql-statement-show-create-table.md) 语句查看外键的定义：\n\n```sql\nmysql> SHOW CREATE TABLE child\\G\n*************************** 1. row ***************************\n       Table: child\nCreate Table: CREATE TABLE `child` (\n  `id` int DEFAULT NULL,\n  `pid` int DEFAULT NULL,\n  KEY `idx_pid` (`pid`),\n  CONSTRAINT `fk_1` FOREIGN KEY (`pid`) REFERENCES `test`.`parent` (`id`) ON DELETE CASCADE\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin\n```\n\n你可以使用以下任一系统表获取有关外键的信息：\n\n- [`INFORMATION_SCHEMA.KEY_COLUMN_USAGE`](/information-schema/information-schema-key-column-usage.md)\n- [`INFORMATION_SCHEMA.TABLE_CONSTRAINTS`](/information-schema/information-schema-table-constraints.md)\n- [`INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS`](/information-schema/information-schema-referential-constraints.md)\n\n下面提供了查询示例：\n\n从 `INFORMATION_SCHEMA.KEY_COLUMN_USAGE` 系统表中获取有关的外键信息：\n\n```sql\nmysql> SELECT TABLE_SCHEMA, TABLE_NAME, COLUMN_NAME, CONSTRAINT_NAME FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE WHERE REFERENCED_TABLE_SCHEMA IS NOT NULL;\n+--------------+---------------+------------------+-----------------+\n| TABLE_SCHEMA | TABLE_NAME    | COLUMN_NAME      | CONSTRAINT_NAME |\n+--------------+---------------+------------------+-----------------+\n| test         | child         | pid              | fk_1            |\n| test         | product_order | product_category | fk_1            |\n| test         | product_order | product_id       | fk_1            |\n| test         | product_order | customer_id      | fk_2            |\n+--------------+---------------+------------------+-----------------+\n```\n\n从 `INFORMATION_SCHEMA.TABLE_CONSTRAINTS` 系统表中获取有关的外键信息：\n\n```sql\nmysql> SELECT * FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS WHERE CONSTRAINT_TYPE='FOREIGN KEY'\\G\n***************************[ 1. row ]***************************\nCONSTRAINT_CATALOG | def\nCONSTRAINT_SCHEMA  | test\nCONSTRAINT_NAME    | fk_1\nTABLE_SCHEMA       | test\nTABLE_NAME         | child\nCONSTRAINT_TYPE    | FOREIGN KEY\n```\n\n从 `INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS` 系统表中获取有关的外键信息：\n\n```sql\nmysql> SELECT * FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS\\G\n***************************[ 1. row ]***************************\nCONSTRAINT_CATALOG        | def\nCONSTRAINT_SCHEMA         | test\nCONSTRAINT_NAME           | fk_1\nUNIQUE_CONSTRAINT_CATALOG | def\nUNIQUE_CONSTRAINT_SCHEMA  | test\nUNIQUE_CONSTRAINT_NAME    | PRIMARY\nMATCH_OPTION              | NONE\nUPDATE_RULE               | NO ACTION\nDELETE_RULE               | CASCADE\nTABLE_NAME                | child\nREFERENCED_TABLE_NAME     | parent\n```\n\n## 查看带有外键的执行计划\n\n你可以使用 `EXPLAIN` 语句查看执行计划。`Foreign_Key_Check` 算子是执行 DML 语句时，执行外键约束检查的算子。\n\n```sql\nmysql> explain insert into child values (1,1);\n+-----------------------+---------+------+---------------+-------------------------------+\n| id                    | estRows | task | access object | operator info                 |\n+-----------------------+---------+------+---------------+-------------------------------+\n| Insert_1              | N/A     | root |               | N/A                           |\n| └─Foreign_Key_Check_3 | 0.00    | root | table:parent  | foreign_key:fk_1, check_exist |\n+-----------------------+---------+------+---------------+-------------------------------+\n```\n\n你可以使用 `EXPLAIN ANALYZE` 语句查看外键引用行为的执行。`Foreign_Key_Cascade` 算子是执行 DML 语句时，执行外键引用行为的算子。\n\n```sql\nmysql> explain analyze delete from parent where id = 1;\n+----------------------------------+---------+---------+-----------+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+-----------+------+\n| id                               | estRows | actRows | task      | access object                   | execution info                                                                                                                                                                               | operator info                               | memory    | disk |\n+----------------------------------+---------+---------+-----------+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+-----------+------+\n| Delete_2                         | N/A     | 0       | root      |                                 | time:117.3µs, loops:1                                                                                                                                                                        | N/A                                         | 380 Bytes | N/A  |\n| ├─Point_Get_1                    | 1.00    | 1       | root      | table:parent                    | time:63.6µs, loops:2, Get:{num_rpc:1, total_time:29.9µs}                                                                                                                                     | handle:1                                    | N/A       | N/A  |\n| └─Foreign_Key_Cascade_3          | 0.00    | 0       | root      | table:child, index:idx_pid      | total:1.28ms, foreign_keys:1                                                                                                                                                                 | foreign_key:fk_1, on_delete:CASCADE         | N/A       | N/A  |\n|   └─Delete_7                     | N/A     | 0       | root      |                                 | time:904.8µs, loops:1                                                                                                                                                                        | N/A                                         | 1.11 KB   | N/A  |\n|     └─IndexLookUp_11             | 10.00   | 1       | root      |                                 | time:869.5µs, loops:2, index_task: {total_time: 371.1µs, fetch_handle: 357.3µs, build: 1.25µs, wait: 12.5µs}, table_task: {total_time: 382.6µs, num: 1, concurrency: 5}                      |                                             | 9.13 KB   | N/A  |\n|       ├─IndexRangeScan_9(Build)  | 10.00   | 1       | cop[tikv] | table:child, index:idx_pid(pid) | time:351.2µs, loops:3, cop_task: {num: 1, max: 282.3µs, proc_keys: 0, rpc_num: 1, rpc_time: 263µs, copr_cache_hit_ratio: 0.00, distsql_concurrency: 15}, tikv_task:{time:220.2µs, loops:0}   | range:[1,1], keep order:false, stats:pseudo | N/A       | N/A  |\n|       └─TableRowIDScan_10(Probe) | 10.00   | 1       | cop[tikv] | table:child                     | time:223.9µs, loops:2, cop_task: {num: 1, max: 168.8µs, proc_keys: 0, rpc_num: 1, rpc_time: 154.5µs, copr_cache_hit_ratio: 0.00, distsql_concurrency: 15}, tikv_task:{time:145.6µs, loops:0} | keep order:false, stats:pseudo              | N/A       | N/A  |\n+----------------------------------+---------+---------+-----------+---------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------------------------------------------+-----------+------+\n```\n\n## 兼容性\n\n### TiDB 版本间兼容性\n\nTiDB 在 v6.6.0 之前已经支持创建外键的语法，但创建的外键并不生效。如果将之前创建的 TiDB 集群升级到 v6.6.0 及之后的版本，之前创建的外键依然是不生效的，可以先删除不生效的外键后再创建外键使外键约束生效。只有在 v6.6.0 及之后版本中新创建的外键才生效。你可以使用 `SHOW CREATE TABLE` 语句查看外键是否生效，不生效的外键会有一条 `/* FOREIGN KEY INVALID */` 注释。\n\n```sql\nmysql> SHOW CREATE TABLE child\\G\n***************************[ 1. row ]***************************\nTable        | child\nCreate Table | CREATE TABLE `child` (\n  `id` int DEFAULT NULL,\n  `pid` int DEFAULT NULL,\n  KEY `idx_pid` (`pid`),\n  CONSTRAINT `fk_1` FOREIGN KEY (`pid`) REFERENCES `test`.`parent` (`id`) ON DELETE CASCADE /* FOREIGN KEY INVALID */\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin\n```\n\n### 与 TiDB 工具的兼容性\n\n- [DM](/dm/dm-overview.md) 不兼容外键功能。DM 在同步数据到下游 TiDB 时，会显式关闭下游 TiDB 的 [`foreign_key_checks`](/system-variables.md#foreign_key_checks)，所以由外键产生的级联操作不会从上游同步到下游，这会导致上下游数据不一致。\n- [TiCDC](/ticdc/ticdc-overview.md) v6.6.0 兼容外键功能。旧版本的 TiCDC 在同步带外键的表时，可能会报错，建议使用 v6.6.0 之前版本 TiCDC 时先关闭下游 TiDB 集群的 `foreign_key_checks`。\n- [BR](/br/backup-and-restore-overview.md) v6.6.0 兼容外键功能。之前版本的 BR 在恢复带外键的表到 v6.6.0 及之后版本的集群时，可能会报错，建议先关闭下游 TiDB 集群的 `foreign_key_checks` 后再恢复集群。\n- [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 导入数据到 TiDB 前，如果目标表使用了外键，建议先关闭 TiDB 集群的 `foreign_key_checks`。对于 v6.6.0 之前的版本，关闭该系统变量也不会生效，你需要为下游数据库用户添加 `REFERENCES` 权限，或者提前手动在下游数据库中创建好目标表，以确保顺利导入数据。\n- [Dumpling](/dumpling-overview.md) 兼容外键功能。\n- [sync-diff-inspector](/sync-diff-inspector/sync-diff-inspector-overview.md) 在对比上下游数据时，如果上下游数据库的版本不一样，且下游 TiDB 中存在[不生效的外键](#tidb-版本间兼容性)，则 sync-diff-inspector 可能会报上下游表结构不一致的错误。因为 TiDB v6.6.0 会对表结构中不生效的外键添加一条 `/* FOREIGN KEY INVALID */` 注释。\n\n### 与 MySQL 的兼容性\n\n创建外键未指定名称时，TiDB 自动生成的外键名称和 MySQL 不一样。例如 TiDB 生成的外键名称为 `fk_1`、`fk_2`、`fk_3` 等，MySQL 生成的外键名称为 `table_name_ibfk_1`、 `table_name_ibfk_2`、`table_name_ibfk_3` 等。\n\nMySQL 和 TiDB 均能解析但会忽略以内联 `REFERENCES` 的方式定义的外键。只有当 `REFERENCES` 作为 `FOREIGN KEY` 定义的一部分时，才会进行检查和执行。下面的示例在定义外键约束时只使用了 `REFERENCES`：\n\n```sql\nCREATE TABLE parent (\n    id INT KEY\n);\n\nCREATE TABLE child (\n    id INT,\n    pid INT REFERENCES parent(id)\n);\n\nSHOW CREATE TABLE child;\n```\n\n输出结果显示 `child` 表不包含任何外键：\n\n```sql\n+-------+-------------------------------------------------------------+\n| Table | Create Table                                                |\n+-------+-------------------------------------------------------------+\n| child | CREATE TABLE `child` (                                      |\n|       |   `id` int DEFAULT NULL,                                |\n|       |   `pid` int DEFAULT NULL                                |\n|       | ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin |\n+-------+-------------------------------------------------------------+\n```\n"
        },
        {
          "name": "functions-and-operators",
          "type": "tree",
          "content": null
        },
        {
          "name": "garbage-collection-configuration.md",
          "type": "blob",
          "size": 6.4609375,
          "content": "---\ntitle: GC 配置\naliases: ['/docs-cn/dev/garbage-collection-configuration/','/docs-cn/dev/reference/garbage-collection/configuration/']\nsummary: TiDB 的 GC 配置可以通过系统变量进行设置，包括启用 GC、运行间隔、数据保留时限、并发线程数量等。此外，TiDB 还支持 GC 流控，可以限制每秒数据写入量。从 TiDB 5.0 版本开始，建议使用系统变量进行配置，避免异常行为。在 TiDB 6.1.0 版本引入了新的系统变量 `tidb_gc_max_wait_time`，用于控制活跃事务阻塞 GC safe point 推进的最长时间。另外，GC in Compaction Filter 机制可以通过配置文件或在线配置开启，但可能会影响 TiKV 扫描性能。\n---\n\n# GC 配置\n\n你可以通过以下系统变量进行 GC 配置：\n\n* [`tidb_gc_enable`](/system-variables.md#tidb_gc_enable-从-v50-版本开始引入)：控制是否启用 TiKV 的垃圾回收 (GC) 机制。\n* [`tidb_gc_run_interval`](/system-variables.md#tidb_gc_run_interval-从-v50-版本开始引入)：指定垃圾回收 (GC) 运行的时间间隔。\n* [`tidb_gc_life_time`](/system-variables.md#tidb_gc_life_time-从-v50-版本开始引入)：指定每次进行垃圾回收 (GC) 时保留数据的时限。\n* [`tidb_gc_concurrency`](/system-variables.md#tidb_gc_concurrency-从-v50-版本开始引入)：指定 GC 在 [Resolve Locks（清理锁）](/garbage-collection-overview.md#resolve-locks清理锁)步骤中线程的数量。\n* [`tidb_gc_scan_lock_mode`](/system-variables.md#tidb_gc_scan_lock_mode-从-v50-版本开始引入)：指定垃圾回收 (GC) 的 Resolve Locks（清理锁）步骤中扫描锁的方式。\n* [`tidb_gc_max_wait_time`](/system-variables.md#tidb_gc_max_wait_time-从-v610-版本开始引入)：指定活跃事务阻碍 GC safe point 推进的最大时间。\n\n关于如何修改系统变量的值，请参考[系统变量](/system-variables.md)。\n\n## 流控\n\nTiDB 支持 GC 流控，可通过配置 `gc.max-write-bytes-per-sec` 限制 GC worker 每秒数据写入量，降低对正常请求的影响，`0` 为关闭该功能。该配置可通过 tikv-ctl 动态修改：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntikv-ctl --host=ip:port modify-tikv-config -n gc.max-write-bytes-per-sec -v 10MB\n```\n\n## TiDB 5.0 引入的变化\n\n在 TiDB 5.0 之前的版本中，GC 是通过系统表 `mysql.tidb` 进行配置的。从 TiDB 5.0 版本起，GC 仍然可以通过系统表 `mysql.tidb` 进行配置，但建议你使用系统变量进行配置，这样可以确保对配置的任何更改都能得到验证，防止造成异常行为 ([#20655](https://github.com/pingcap/tidb/issues/20655))。\n\nTiDB 5.0 及之后的版本不再需要向各个 TiKV Region 都发送触发 GC 的请求，因此不再提供 `CENTRAL` GC 模式的支持，取而代之的是效率更高的 `DISTRIBUTED` GC 模式 （自 TiDB 3.0 起的默认 GC 模式）。\n\n如果要了解 TiDB 历史版本中 GC 配置的变化信息，请使用左侧导航栏中的 _\"TIDB 版本选择器\"_ 切换到本文档的历史版本。\n\n## TiDB 6.1.0 引入的变化\n\n在 TiDB 6.1.0 之前的版本中，TiDB 内部事务不会影响 GC safe point 推进。从 TiDB 6.1.0 版本起，计算 safe point 时会考虑内部事务的 startTS，从而解决内部事务因访问的数据被清理掉而导致失败的问题。带来的负面影响是如果内部事务运行时间过长，会导致 safe point 长时间不推进，进而会影响业务性能。\n\nTiDB v6.1.0 引入了系统变量 [`tidb_gc_max_wait_time`](/system-variables.md#tidb_gc_max_wait_time-从-v610-版本开始引入) 控制活跃事务阻塞 GC safe point 推进的最长时间，超过该值后 GC safe point 会强制向后推进。\n\n## GC in Compaction Filter 机制\n\nGC in Compaction Filter 机制是在分布式 GC 模式 (`DISTRIBUTED` GC mode) 的基础上，由 RocksDB 的 Compaction 过程来进行 GC，而不再使用一个单独的 GC worker 线程。这样做的好处是避免了 GC 引起的额外磁盘读取，以及避免清理掉的旧版本残留大量删除标记影响顺序扫描性能。可以由 TiKV 配置文件中的以下开关控制：\n\n{{< copyable \"\" >}}\n\n```toml\n[gc]\nenable-compaction-filter = true\n```\n\n该 GC 机制可通过在线配置变更开启：\n\n{{< copyable \"sql\" >}}\n\n```sql\nshow config where type = 'tikv' and name like '%enable-compaction-filter%';\n```\n\n```sql\n+------+-------------------+-----------------------------+-------+\n| Type | Instance          | Name                        | Value |\n+------+-------------------+-----------------------------+-------+\n| tikv | 172.16.5.37:20163 | gc.enable-compaction-filter | false |\n| tikv | 172.16.5.36:20163 | gc.enable-compaction-filter | false |\n| tikv | 172.16.5.35:20163 | gc.enable-compaction-filter | false |\n+------+-------------------+-----------------------------+-------+\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nset config tikv gc.enable-compaction-filter = true;\nshow config where type = 'tikv' and name like '%enable-compaction-filter%';\n```\n\n```sql\n+------+-------------------+-----------------------------+-------+\n| Type | Instance          | Name                        | Value |\n+------+-------------------+-----------------------------+-------+\n| tikv | 172.16.5.37:20163 | gc.enable-compaction-filter | true  |\n| tikv | 172.16.5.36:20163 | gc.enable-compaction-filter | true  |\n| tikv | 172.16.5.35:20163 | gc.enable-compaction-filter | true  |\n+------+-------------------+-----------------------------+-------+\n```\n\n> **注意：**\n>\n> 在使用 Compaction Filter 机制时，可能会出现 GC 进度延迟的情况，从而影响 TiKV 扫描性能。当你的负载中含有大量 coprocessor 请求，并且在 [**TiKV-Details > Coprocessor Detail**](/grafana-tikv-dashboard.md#coprocessor-detail) 面板中发现 Total Ops Details 的 `next()` 或 `prev()` 调用次数远远超过 `processed_keys` 调用的三倍时，可以采取以下措施：\n> \n> - 对于 TiDB v7.1.3 之前版本，建议尝试关闭 Compaction Filter，以加快 GC 速度。\n> - 从 v7.1.3 开始，TiDB 会根据每个 Region 的冗余版本数量 [`region-compact-min-redundant-rows`](/tikv-configuration-file.md#region-compact-min-redundant-rows-从-v710-版本开始引入) 和比例 [`region-compact-redundant-rows-percent`](/tikv-configuration-file.md#region-compact-redundant-rows-percent-从-v710-版本开始引入) 自动触发 compaction，从而提高 Compaction Filter 的 GC 速度。因此，在 v7.1.3 及之后的版本中，如果遇到上述情况，建议调整这两个参数，无需关闭 Compaction Filter。\n"
        },
        {
          "name": "garbage-collection-overview.md",
          "type": "blob",
          "size": 5.5947265625,
          "content": "---\ntitle: GC 机制简介\naliases: ['/docs-cn/dev/garbage-collection-overview/','/docs-cn/dev/reference/garbage-collection/overview/']\nsummary: TiDB 的事务实现采用了 MVCC 机制，GC 的任务是清理不再需要的旧数据。整体流程包括 GC leader 控制 GC 的运行，定期触发 GC，以及三个步骤：Resolve Locks 清理锁，Delete Ranges 删除区间，Do GC 进行 GC 清理。Resolve Locks 清理锁有两种执行模式：LEGACY 和 PHYSICAL。Delete Ranges 删除区间会快速物理删除待删除的区间及删除操作的时间戳。Do GC 进行 GC 清理会删除所有 key 的过期版本。GC 每 10 分钟触发一次，默认保留最近 10 分钟内的数据。\n---\n\n# GC 机制简介\n\nTiDB 的事务的实现采用了 MVCC（多版本并发控制）机制，当新写入的数据覆盖旧的数据时，旧的数据不会被替换掉，而是与新写入的数据同时保留，并以时间戳来区分版本。Garbage Collection (GC) 的任务便是清理不再需要的旧数据。\n\n## 整体流程\n\n一个 TiDB 集群中会有一个 TiDB 实例被选举为 GC leader，GC 的运行由 GC leader 来控制。\n\nGC 会被定期触发。每次 GC 时，首先，TiDB 会计算一个称为 safe point 的时间戳，接下来 TiDB 会在保证 safe point 之后的快照全部拥有正确数据的前提下，删除更早的过期数据。每一轮 GC 分为以下三个步骤：\n\n1. \"Resolve Locks\" 阶段会对所有 Region 扫描 safe point 之前的锁，并清理这些锁。\n2. \"Delete Ranges\" 阶段快速地删除由于 `DROP TABLE`/`DROP INDEX` 等操作产生的整区间的废弃数据。\n3. \"Do GC\" 阶段每个 TiKV 节点将会各自扫描该节点上的数据，并对每一个 key 删除其不再需要的旧版本。\n\n默认配置下，GC 每 10 分钟触发一次，每次 GC 会保留最近 10 分钟内的数据（即默认 GC life time 为 10 分钟，safe point 的计算方式为当前时间减去 GC life time）。如果一轮 GC 运行时间太久，那么在一轮 GC 完成之前，即使到了下一次触发 GC 的时间也不会开始下一轮 GC。另外，为了使持续时间较长的事务能在超过 GC life time 之后仍然可以正常运行，safe point 不会超过正在执行中的事务的开始时间 (start_ts)。\n\n## 实现细节\n\n### Resolve Locks（清理锁）\n\nTiDB 的事务是基于 [Google Percolator](https://ai.google/research/pubs/pub36726) 模型实现的，事务的提交是一个两阶段提交的过程。第一阶段完成时，所有涉及的 key 都会上锁，其中一个锁会被选为 Primary，其余的锁 (Secondary) 则会存储一个指向 Primary 的指针；第二阶段会将 Primary 锁所在的 key 加上一个 Write 记录，并去除锁。这里的 Write 记录就是历史上对该 key 进行写入或删除，或者该 key 上发生事务回滚的记录。Primary 锁被替换为何种 Write 记录标志着该事务提交成功与否。接下来，所有 Secondary 锁也会被依次替换。如果因为某些原因（如发生故障等），这些 Secondary 锁没有完成替换、残留了下来，那么也可以根据锁中的信息找到 Primary，并根据 Primary 是否提交来判断整个事务是否提交。但是，如果 Primary 的信息在 GC 中被删除了，而该事务又存在未成功提交的 Secondary 锁，那么就永远无法得知该锁是否可以提交。这样，数据的正确性就无法保证。\n\nResolve Locks 这一步的任务即对 safe point 之前的锁进行清理。即如果一个锁对应的 Primary 已经提交，那么该锁也应该被提交；反之，则应该回滚。而如果 Primary 仍然是上锁的状态（没有提交也没有回滚），则应当将该事务视为超时失败而回滚。\n\nResolve Locks 有两种执行模式：\n\n- `LEGACY` （默认模式）：由 GC leader 对所有的 Region 发送请求扫描过期的锁，并对扫到的锁查询 Primary 的状态，再发送请求对其进行提交或回滚。\n- `PHYSICAL`：TiDB 绕过 Raft 层直接扫描每个 TiKV 节点上的数据。\n\n> **警告：**\n>\n> `PHYSICAL`模式（即启用 Green GC）目前是实验性功能，不建议在生产环境中使用。\n\n你可以通过修改系统变量 [`tidb_gc_scan_lock_mode`](/system-variables.md#tidb_gc_scan_lock_mode-从-v50-版本开始引入) 的值切换 Resolve Locks 的执行模式。\n\n### Delete Ranges（删除区间）\n\n在执行 `DROP TABLE/INDEX` 等操作时，会有大量连续的数据被删除。如果对每个 key 都进行删除操作、再对每个 key 进行 GC 的话，那么执行效率和空间回收速度都可能非常的低下。事实上，这种时候 TiDB 并不会对每个 key 进行删除操作，而是将这些待删除的区间及删除操作的时间戳记录下来。Delete Ranges 会将这些时间戳在 safe point 之前的区间进行快速的物理删除。\n\n### Do GC（进行 GC 清理）\n\n这一步即删除所有 key 的过期版本。为了保证 safe point 之后的任何时间戳都具有一致的快照，这一步删除 safe point 之前提交的数据，但是会对每个 key 保留 safe point 前的最后一次写入（除非最后一次写入是删除）。\n\n在进行这一步时，TiDB 只需要将 safe point 发送给 PD，即可结束整轮 GC。TiKV 会自行检测到 safe point 发生了更新，会对当前节点上所有作为 Region leader 进行 GC。与此同时，GC leader 可以继续触发下一轮 GC。\n\n> **注意：**\n>\n> 从 TiDB 5.0 版本起，`CENTRAL` GC 模式（需要 TiDB 服务器发送 GC 请求到各个 Region）已经废弃，Do GC 这一步将只以 `DISTRIBUTED` GC 模式（从 TiDB 3.0 版起的默认模式）运行。\n"
        },
        {
          "name": "generate-self-signed-certificates.md",
          "type": "blob",
          "size": 4.97265625,
          "content": "---\ntitle: 生成自签名证书\naliases: ['/docs-cn/dev/generate-self-signed-certificates/','/docs-cn/dev/how-to/secure/generate-self-signed-certificates/']\nsummary: 本文介绍了使用 openssl 生成自签名证书的示例。用户可以根据需要生成符合要求的证书和密钥。首先安装 OpenSSL，然后生成 CA 证书和各个组件的证书，最后为客户端签发证书。证书的作用是为各个组件和客户端验证身份。\n---\n\n# 生成自签名证书\n\n> **注意：**\n>\n> 要在 TiDB 客户端与服务端间通信开启加密传输，你只需配置 `auto-tls` 参数。\n\n本文档提供使用 `openssl` 生成自签名证书的一个示例，用户也可以根据自己的需求生成符合要求的证书和密钥。\n\n假设实例集群拓扑如下：\n\n| Name  | Host IP      | Services   |\n| ----- | -----------  | ---------- |\n| node1 | 172.16.10.11 | PD1, TiDB1 |\n| node2 | 172.16.10.12 | PD2        |\n| node3 | 172.16.10.13 | PD3        |\n| node4 | 172.16.10.14 | TiKV1      |\n| node5 | 172.16.10.15 | TiKV2      |\n| node6 | 172.16.10.16 | TiKV3      |\n\n## 安装 OpenSSL\n\n对于 Debian 或 Ubuntu 操作系统：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\napt install openssl\n```\n\n对于 RedHat 或 CentOS 操作系统：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nyum install openssl\n```\n\n也可以参考 OpenSSL 官方的[下载文档](https://www.openssl.org/source/)进行安装。\n\n## 生成 CA 证书\n\nCA 的作用是签发证书。实际情况中，请联系你的管理员签发证书或者使用信任的 CA 机构。CA 会管理多个证书对，这里只需生成原始的一对证书，步骤如下：\n\n1. 生成 root 密钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl genrsa -out root.key 4096\n    ```\n\n2. 生成 root 证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl req -new -x509 -days 1000 -key root.key -out root.crt\n    ```\n\n3. 验证 root 证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl x509 -text -in root.crt -noout\n    ```\n\n## 签发各个组件的证书\n\n### 集群中可能使用到的证书\n\n- tidb certificate 由 TiDB 使用，为其他组件和客户端验证 TiDB 身份。\n- tikv certificate 由 TiKV 使用，为其他组件和客户端验证 TiKV 身份。\n- pd certificate 由 PD 使用，为其他组件和客户端验证 PD 身份。\n- client certificate 用于 PD、TiKV、TiDB 验证客户端。例如 `pd-ctl`，`tikv-ctl` 等。\n\n### 给 TiKV 实例签发证书\n\n给 TiKV 实例签发证书的步骤如下：\n\n1. 生成该证书对应的私钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl genrsa -out tikv.key 2048\n    ```\n\n2. 拷贝一份 OpenSSL 的配置模板文件。\n\n    模板文件可能存在多个位置，请以实际位置为准：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    cp /usr/lib/ssl/openssl.cnf .\n    ```\n\n    如果不知道实际位置，请在根目录下查找：\n\n    ```bash\n    find / -name openssl.cnf\n    ```\n\n3. 编辑 `openssl.cnf`，在 `[ req ]` 字段下加入 `req_extensions = v3_req`，然后在 `[ v3_req ]` 字段下加入 `subjectAltName = @alt_names`。最后新建一个字段，并编辑 SAN 的信息：\n\n    ```\n    [ alt_names ]\n    IP.1 = 127.0.0.1\n    IP.2 = 172.16.10.14\n    IP.3 = 172.16.10.15\n    IP.4 = 172.16.10.16\n    ```\n\n4. 保存 `openssl.cnf` 文件后，生成证书请求文件（在这一步也可以为该证书指定 Common Name，其作用是让服务端验证接入的客户端的身份，各个组件默认不会开启验证，需要在配置文件中启用该功能才生效）：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl req -new -key tikv.key -out tikv.csr -config openssl.cnf\n    ```\n\n5. 签发生成证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl x509 -req -days 365 -CA root.crt -CAkey root.key -CAcreateserial -in tikv.csr -out tikv.crt -extensions v3_req -extfile openssl.cnf\n    ```\n\n6. 验证证书携带 SAN 字段信息（可选）：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl x509 -text -in tikv.crt -noout\n    ```\n\n7. 确认在当前目录下得到如下文件：\n\n    ```\n    root.crt\n    tikv.crt\n    tikv.key\n    ```\n\n为其它 TiDB 组件签发证书的过程类似，此文档不再赘述。\n\n### 为客户端签发证书\n\n为客户端签发证书的步骤如下。\n\n1. 生成该证书对应的私钥：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl genrsa -out client.key 2048\n    ```\n\n2. 生成证书请求文件（在这一步也可以为该证书指定 Common Name，其作用是让服务端验证接入的客户端的身份，默认不会开启对各个组件的验证，需要在配置文件中启用该功能才生效）\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl req -new -key client.key -out client.csr\n    ```\n\n3. 签发生成证书：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    openssl x509 -req -days 365 -CA root.crt -CAkey root.key -CAcreateserial -in client.csr -out client.crt\n    ```\n"
        },
        {
          "name": "generated-columns.md",
          "type": "blob",
          "size": 8.751953125,
          "content": "---\ntitle: 生成列\naliases: ['/docs-cn/dev/generated-columns/','/docs-cn/dev/reference/sql/generated-columns/']\nsummary: 生成列是由列定义中的表达式计算得到的值。它包括存储生成列和虚拟生成列，存储生成列会将计算得到的值存储起来，而虚拟生成列不会存储其值。生成列可以用于从 JSON 数据类型中解出数据，并为该数据建立索引。在 INSERT 和 UPDATE 语句中，会检查生成列计算得到的值是否满足生成列的定义。生成列的局限性包括不能增加存储生成列，不能转换存储生成列为普通列，不能修改存储生成列的生成列表达式，以及不支持所有的 JSON 函数。\n---\n\n# 生成列\n\n本文介绍生成列的概念以及用法。\n\n## 生成列的基本概念\n\n与一般的列不同，生成列的值由列定义中表达式计算得到。对生成列进行插入或更新操作时，并不能对之赋值，只能使用 `DEFAULT`。\n\n生成列包括存储生成列和虚拟生成列。存储生成列会将计算得到的值存储起来，在读取时不需要重新计算。虚拟生成列不会存储其值，在读取时会重新计算。存储生成列和虚拟生成列相比，前者在读取时性能更好，但是要占用更多的磁盘空间。\n\n无论是存储生成列还是虚拟列，都可以在其上面建立索引。\n\n## 生成列的应用\n\n生成列的主要的作用之一：从 JSON 数据类型中解出数据，并为该数据建立索引。\n\nMySQL 8.0 及 TiDB 都不能直接为 JSON 类型的列添加索引，即不支持在如下表结构中的 `address_info` 上建立索引：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE person (\n    id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    address_info JSON,\n    KEY (address_info)\n);\n```\n\n如果要为 JSON 列某个字段添加索引，可以抽取该字段为生成列。\n\n以 `city` 这一 `address_info` 中的字段为例，可以为其建立一个虚拟生成列并添加索引：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE person (\n    id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    address_info JSON,\n    city VARCHAR(64) AS (JSON_UNQUOTE(JSON_EXTRACT(address_info, '$.city'))), -- 虚拟生成列\n    -- city VARCHAR(64) AS (JSON_UNQUOTE(JSON_EXTRACT(address_info, '$.city'))) VIRTUAL, -- 虚拟生成列\n    -- city VARCHAR(64) AS (JSON_UNQUOTE(JSON_EXTRACT(address_info, '$.city'))) STORED, -- 存储生成列\n    KEY (city)\n);\n```\n\n该表中，`city` 列是一个虚拟生成列。并且在该列上建立了索引。以下语句能够利用索引加速语句的执行速度：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT name, id FROM person WHERE city = 'Beijing';\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nEXPLAIN SELECT name, id FROM person WHERE city = 'Beijing';\n```\n\n```\n+---------------------------------+---------+-----------+--------------------------------+-------------------------------------------------------------+\n| id                              | estRows | task      | access object                  | operator info                                               |\n+---------------------------------+---------+-----------+--------------------------------+-------------------------------------------------------------+\n| Projection_4                    | 10.00   | root      |                                | test.person.name, test.person.id                            |\n| └─IndexLookUp_10                | 10.00   | root      |                                |                                                             |\n|   ├─IndexRangeScan_8(Build)     | 10.00   | cop[tikv] | table:person, index:city(city) | range:[\"Beijing\",\"Beijing\"], keep order:false, stats:pseudo |\n|   └─TableRowIDScan_9(Probe)     | 10.00   | cop[tikv] | table:person                   | keep order:false, stats:pseudo                              |\n+---------------------------------+---------+-----------+--------------------------------+-------------------------------------------------------------+\n```\n\n从执行计划中，可以看出使用了 `city` 这个索引来读取满足 `city = 'Beijing'` 这个条件的行的 `HANDLE`，再用这个 `HANDLE` 来读取该行的数据。\n\n如果 `$.city` 路径中无数据，则 `JSON_EXTRACT` 返回 `NULL`。如果想增加约束，`city` 列必须是 `NOT NULL`，则可按照以下方式定义虚拟生成列：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE person (\n    id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    name VARCHAR(255) NOT NULL,\n    address_info JSON,\n    city VARCHAR(64) AS (JSON_UNQUOTE(JSON_EXTRACT(address_info, '$.city'))) NOT NULL,\n    KEY (city)\n);\n```\n\n## 生成列在 INSERT 和 UPDATE 语句中的行为\n\n`INSERT` 和 `UPDATE` 语句都会检查生成列计算得到的值是否满足生成列的定义。未通过有效性检测的行会返回错误：\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO person (name, address_info) VALUES ('Morgan', JSON_OBJECT('Country', 'Canada'));\n```\n\n```\nERROR 1048 (23000): Column 'city' cannot be null\n```\n\n## 索引生成列替换\n\n当查询中出现的某个表达式与一个含索引的生成列严格同等时，TiDB 会将这个表达式替换为对应的生成列，这样就可以在生成查询计划时考虑使用这个索引。\n\n下面的例子为 `a+1` 这个表达式创建生成列并添加索引，从而加速查询。其中，`a` 的列类型是 int，而 `a+1` 的列类型是 bigint。如果将生成列的类型改为 int，就不会发生替换。关于类型转换的规则，可以参见[表达式求值的类型转换](/functions-and-operators/type-conversion-in-expression-evaluation.md)。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t(a int);\ndesc select a+1 from t where a+1=3;\n```\n\n```sql\n+---------------------------+----------+-----------+---------------+--------------------------------+\n| id                        | estRows  | task      | access object | operator info                  |\n+---------------------------+----------+-----------+---------------+--------------------------------+\n| Projection_4              | 8000.00  | root      |               | plus(test.t.a, 1)->Column#3    |\n| └─TableReader_7           | 8000.00  | root      |               | data:Selection_6               |\n|   └─Selection_6           | 8000.00  | cop[tikv] |               | eq(plus(test.t.a, 1), 3)       |\n|     └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo |\n+---------------------------+----------+-----------+---------------+--------------------------------+\n4 rows in set (0.00 sec)\n```\n\n```sql\nalter table t add column b bigint as (a+1) virtual;\nalter table t add index idx_b(b);\ndesc select a+1 from t where a+1=3;\n```\n\n```sql\n+------------------------+---------+-----------+-------------------------+---------------------------------------------+\n| id                     | estRows | task      | access object           | operator info                               |\n+------------------------+---------+-----------+-------------------------+---------------------------------------------+\n| IndexReader_6          | 10.00   | root      |                         | index:IndexRangeScan_5                      |\n| └─IndexRangeScan_5     | 10.00   | cop[tikv] | table:t, index:idx_b(b) | range:[3,3], keep order:false, stats:pseudo |\n+------------------------+---------+-----------+-------------------------+---------------------------------------------+\n2 rows in set (0.01 sec)\n```\n\n> **注意：**\n>\n> 若待替换的表达式类型和生成列类型都是字符类型，但两种类型长度不同时，仍可通过将系统变量 [`tidb_enable_unsafe_substitute`](/system-variables.md#tidb_enable_unsafe_substitute-从-v630-版本开始引入) 设置为 `ON` 来允许其替换。配置该系统变量时，需要保证生成列计算得到的值严格满足生成列的定义，否则，可能因为长度不同，导致数据截断得到错误的结果。详情见 GitHub issue [#35490](https://github.com/pingcap/tidb/issues/35490#issuecomment-1211658886)。\n\n## 生成列的局限性\n\n目前生成列有以下局限性：\n\n- 不能通过 `ALTER TABLE` 增加存储生成列；\n- 不能通过 `ALTER TABLE` 将存储生成列转换为普通列，也不能将普通列转换成存储生成列；\n- 不能通过 `ALTER TABLE` 修改存储生成列的生成列表达式；\n- 并未支持所有的 [JSON 函数](/functions-and-operators/json-functions.md)；\n- 不支持使用 [`NULLIF()` 函数](/functions-and-operators/control-flow-functions.md#nullif)，可以使用 [`CASE` 函数](/functions-and-operators/control-flow-functions.md#case)代替；\n- 目前仅当生成列是虚拟生成列时索引生成列替换规则有效，暂不支持将表达式替换为存储生成列，但仍然可以通过直接使用该生成列本身来使用索引。\n"
        },
        {
          "name": "geo-distributed-deployment-topology.md",
          "type": "blob",
          "size": 7.048828125,
          "content": "---\ntitle: 跨数据中心部署拓扑\nsummary: 介绍跨数据中心部署 TiDB 集群的拓扑结构。\naliases: ['/docs-cn/dev/geo-distributed-deployment-topology/']\n---\n\n# 跨数据中心部署拓扑\n\n本文以典型的两地三中心为例，介绍跨数据中心部署的拓扑以及关键参数。本文示例所涉及的城市是上海（即 `sha`）和北京（即 `bja` 和 `bjb`）。\n\n## 拓扑信息\n\n|实例 | 个数 | 物理机配置 | BJ IP | SH IP |配置 |\n| :-- | :-- | :-- | :-- | :-- | :-- |\n| TiDB |5 | 16 VCore 32GB * 1 | 10.0.1.1 <br/> 10.0.1.2 <br/> 10.0.1.3 <br/> 10.0.1.4 | 10.0.1.5 | 默认端口 <br/>  全局目录配置 |\n| PD | 5 | 4 VCore 8GB * 1 |10.0.1.6 <br/> 10.0.1.7 <br/> 10.0.1.8 <br/> 10.0.1.9 | 10.0.1.10 | 默认端口 <br/> 全局目录配置 |\n| TiKV | 5 | 16 VCore 32GB 4TB (nvme ssd) * 1 | 10.0.1.11 <br/> 10.0.1.12 <br/> 10.0.1.13 <br/> 10.0.1.14 | 10.0.1.15 | 默认端口 <br/> 全局目录配置 |\n| Monitoring & Grafana | 1 | 4 VCore 8GB * 1 500GB (ssd) | 10.0.1.16 || 默认端口 <br/> 全局目录配置 |\n\n### 拓扑模版\n\n<details>\n<summary>跨机房配置模板</summary>\n\n```yaml\n# Tip: PD priority needs to be manually set using the PD-ctl client tool. such as, member Leader_priority PD-name numbers.\n# Global variables are applied to all deployments and used as the default value of\n# the deployments if a specific deployment value is missing.\n#\n# Abbreviations used in this example:\n# sh: Shanghai Zone\n# bj: Beijing Zone\n# sha: Shanghai Datacenter A\n# bja: Beijing Datacenter A\n# bjb: Beijing Datacenter B\n\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\nmonitored:\n  node_exporter_port: 9100\n  blackbox_exporter_port: 9115\n  deploy_dir: \"/tidb-deploy/monitored-9100\"\nserver_configs:\n  tidb:\n    log.level: debug\n    log.slow-query-file: tidb-slow.log\n  tikv:\n    server.grpc-compression-type: gzip\n    readpool.storage.use-unified-pool: true\n    readpool.storage.low-concurrency: 8\n  pd:\n    replication.location-labels: [\"zone\",\"dc\",\"rack\",\"host\"]\n    replication.max-replicas: 5\n    label-property:  # TiDB 5.2 及以上版本默认不支持 label-property 配置。若要设置副本策略，请使用 Placement Rules。\n      reject-leader:\n        - key: \"dc\"\n          value: \"sha\"\npd_servers:\n - host: 10.0.1.6\n - host: 10.0.1.7\n - host: 10.0.1.8\n - host: 10.0.1.9\n - host: 10.0.1.10\ntidb_servers:\n - host: 10.0.1.1\n - host: 10.0.1.2\n - host: 10.0.1.3\n - host: 10.0.1.4\n - host: 10.0.1.5\ntikv_servers:\n - host: 10.0.1.11\n   ssh_port: 22\n   port: 20160\n   status_port: 20180\n   deploy_dir: \"/tidb-deploy/tikv-20160\"\n   data_dir: \"/tidb-data/tikv-20160\"\n   config:\n     server.labels:\n       zone: bj\n       dc: bja\n       rack: rack1\n       host: host1\n - host: 10.0.1.12\n   ssh_port: 22\n   port: 20161\n   status_port: 20181\n   deploy_dir: \"/tidb-deploy/tikv-20161\"\n   data_dir: \"/tidb-data/tikv-20161\"\n   config:\n     server.labels:\n       zone: bj\n       dc: bja\n       rack: rack1\n       host: host2\n - host: 10.0.1.13\n   ssh_port: 22\n   port: 20160\n   status_port: 20180\n   deploy_dir: \"/tidb-deploy/tikv-20160\"\n   data_dir: \"/tidb-data/tikv-20160\"\n   config:\n     server.labels:\n       zone: bj\n       dc: bjb\n       rack: rack1\n       host: host1\n - host: 10.0.1.14\n   ssh_port: 22\n   port: 20161\n   status_port: 20181\n   deploy_dir: \"/tidb-deploy/tikv-20161\"\n   data_dir: \"/tidb-data/tikv-20161\"\n   config:\n     server.labels:\n       zone: bj\n       dc: bjb\n       rack: rack1\n       host: host2\n - host: 10.0.1.15\n   ssh_port: 22\n   port: 20160\n   deploy_dir: \"/tidb-deploy/tikv-20160\"\n   data_dir: \"/tidb-data/tikv-20160\"\n   config:\n     server.labels:\n       zone: sh\n       dc: sha\n       rack: rack1\n       host: host1\n     readpool.storage.use-unified-pool: true\n     readpool.storage.low-concurrency: 10\n     raftstore.raft-min-election-timeout-ticks: 50\n     raftstore.raft-max-election-timeout-ticks: 60\nmonitoring_servers:\n - host: 10.0.1.16\ngrafana_servers:\n - host: 10.0.1.16\n```\n\n</details>\n\n以上 TiDB 集群拓扑文件中，详细的配置项说明见[通过 TiUP 部署 TiDB 集群的拓扑文件配置](/tiup/tiup-cluster-topology-reference.md)。\n\n### 关键参数配置\n\n本节介绍跨数据中心部署 TiDB 集群的关键参数配置。\n\n#### TiKV 参数\n\n- 设置 gRPC 的压缩格式，默认为 `none`。为提高跨机房部署场景的目标节点间 gRPC 包的传输速度，建议设置为 gzip 格式。\n\n    ```yaml\n    server.grpc-compression-type: gzip\n    ```\n\n- label 配置\n\n    由于采用跨机房部署 TiKV，为了避免物理机宕机导致 Raft Group 默认的 5 副本中丢失 3 副本，使集群不可用的问题，可以通过 label 来实现 PD 智能调度，保证同中心、同机柜、同机器 TiKV 实例不会出现 Raft Group 有 3 副本的情况。\n\n- TiKV 配置\n\n    相同物理机配置相同的 host 级别 label 信息：\n\n    ```yaml\n    config:\n      server.labels:\n        zone: bj\n        dc: bja\n        rack: rack1\n        host: host2\n    ```\n\n- 防止异地 TiKV 节点发起不必要的 Raft 选举，需要将异地 TiKV 节点发起选举时经过最少的 tick 个数和最多经过的 tick 个数都调大，这两个参数默认设置均为 `0`。\n\n    ```yaml\n    raftstore.raft-min-election-timeout-ticks: 50\n    raftstore.raft-max-election-timeout-ticks: 60\n    ```\n\n> **注意:**\n>\n> 通过 `raftstore.raft-min-election-timeout-ticks` 和 `raftstore.raft-max-election-timeout-ticks` 为 TiKV 节点配置较大的 election timeout tick 可以大幅降低该节点上的 Region 成为 Leader 的概率。但在发生灾难的场景中，如果部分 TiKV 节点宕机，而其它存活的 TiKV 节点 Raft 日志落后，此时只有这个配置了较大的 election timeout tick 的 TiKV 节点上的 Region 能成为 Leader。由于此 TiKV 节点上的 Region 需要至少等待 `raftstore.raft-min-election-timeout-ticks` 设置的时间后才能发起选举，因此尽量避免将此配置值设置得过大，以免在这种场景下影响集群的可用性。\n\n#### PD 参数\n\n- PD 元数据信息记录 TiKV 集群的拓扑信息，根据四个维度调度 Raft Group 副本。\n\n    ```yaml\n    replication.location-labels: [\"zone\",\"dc\",\"rack\",\"host\"]\n    ```\n\n- 调整 Raft Group 的副本数据量为 5，保证集群的高可用性。\n\n    ```yaml\n    replication.max-replicas: 5\n    ```\n\n- 拒绝异地机房 TiKV 的 Raft 副本选举为 Leader。\n\n    ```yaml\n    label-property:\n          reject-leader:\n            - key: \"dc\"\n              value: \"sha\"\n    ```\n\n    > **注意：**\n    >\n    > TiDB 5.2 及以上版本默认不支持 `label-property` 配置。若要设置副本策略，请使用 [Placement Rules](/configure-placement-rules.md)。\n\n有关 Label 的使用和 Raft Group 副本数量，详见[通过拓扑 label 进行副本调度](/schedule-replicas-by-topology-labels.md)。\n\n> **注意：**\n>\n> - 无需手动创建配置文件中的 `tidb` 用户，TiUP cluster 组件会在目标主机上自动创建该用户。可以自定义用户，也可以和中控机的用户保持一致。\n> - 如果部署目录配置为相对路径，会部署在用户的 Home 目录下。\n"
        },
        {
          "name": "get-started-with-tidb-lightning.md",
          "type": "blob",
          "size": 4.4189453125,
          "content": "---\ntitle: TiDB Lightning 快速上手\naliases: ['/docs-cn/dev/get-started-with-tidb-lightning/','/docs-cn/dev/how-to/get-started/tidb-lightning/']\nsummary: TiDB Lightning 可快速将 MySQL 数据导入到 TiDB 集群中。首先使用 Dumpling 导出数据，然后部署 TiDB 集群。安装最新版本的 TiDB Lightning 并启动，最后检查数据导入情况。详细功能和使用请参考 TiDB Lightning 简介。\n---\n\n# TiDB Lightning 快速上手\n\n本文档介绍如何快速上手 TiDB Lightning，将 MySQL 数据导入到 TiDB 集群中。\n\n> **警告：**\n>\n> 本教程中的部署方法只适用于测试及功能体验，并不适用于生产或开发环境。\n\n## 第 1 步：准备全量备份数据\n\n你可以使用 [Dumpling](/dumpling-overview.md) 从 MySQL 导出数据。\n\n1. 运行 `tiup --version` 检查是否已安装 TiUP。如果已经安装 TiUP，跳过这一步。如果没有安装 TiUP，运行以下命令：\n\n    ```\n    curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh\n    ```\n\n2. 使用 TiUP 安装 Dumpling：\n\n    ```shell\n    tiup install dumpling\n    ```\n\n3. 从 MySQL 导出数据，详细步骤可参考[使用 Dumpling 导出数据](/dumpling-overview.md#导出为-sql-文件)：\n\n    ```sh\n    tiup dumpling -h 127.0.0.1 -P 3306 -u root -t 16 -F 256MB -B test -f 'test.t[12]' -o /data/my_database/\n    ```\n\n    其中：\n\n    - `-t 16`：使用 16 个线程导出数据。\n    - `-F 256MB`：将每张表切分成多个文件，每个文件大小约为 256 MB。\n    - `-B test`：从 `test` 数据库导出。\n    - `-f 'test.t[12]'`：只导出 `test.t1` 和 `test.t2` 这两个表。\n\n    导出的全量备份数据将保存在 `/data/my_database` 目录中。\n\n## 第 2 步：部署 TiDB 集群\n\n在开始导入数据之前，你需要先部署一个要进行导入的 TiDB 集群。如果你已经有 TiDB 集群，可以跳过这一步。\n\n关于部署 TiDB 集群的步骤，请参考 [TiDB 数据库快速上手指南](/quick-start-with-tidb.md)。\n\n## 第 3 步：安装 TiDB Lightning\n\n运行如下命令，安装 TiDB Lightning 的最新版本：\n\n```shell\ntiup install tidb-lightning\n```\n\n## 第 4 步：启动 TiDB Lightning\n\n> **注意：**\n>\n> 本节的导入方法只适用于测试及功能体验，生产环境请参考[从大数据量 MySQL 迁移数据到 TiDB](/migrate-large-mysql-to-tidb.md#第-2-步导入全量数据到-tidb)。\n\n1. 创建配置文件 `tidb-lightning.toml`，并根据你的集群信息填写如下配置：\n\n    ```toml\n    [lightning]\n    # 日志\n    level = \"info\"\n    file = \"tidb-lightning.log\"\n\n    [tikv-importer]\n    # 选择使用的导入模式\n    backend = \"local\"\n    # 设置排序的键值对的临时存放地址，目标路径需要是一个空目录\n    sorted-kv-dir = \"/mnt/ssd/sorted-kv-dir\"\n\n    [mydumper]\n    # 源数据目录。\n    data-source-dir = \"/data/my_datasource/\"\n\n    # 配置通配符规则，默认规则会过滤 mysql、sys、INFORMATION_SCHEMA、PERFORMANCE_SCHEMA、METRICS_SCHEMA、INSPECTION_SCHEMA 系统数据库下的所有表\n    # 若不配置该项，导入系统表时会出现“找不到 schema”的异常\n    filter = ['*.*', '!mysql.*', '!sys.*', '!INFORMATION_SCHEMA.*', '!PERFORMANCE_SCHEMA.*', '!METRICS_SCHEMA.*', '!INSPECTION_SCHEMA.*']\n    [tidb]\n    # 目标集群的信息\n    host = \"172.16.31.2\"\n    port = 4000\n    user = \"root\"\n    password = \"rootroot\"\n    # 表架构信息在从 TiDB 的“状态端口”获取。\n    status-port = 10080\n    # 集群 pd 的地址。从 v7.6.0 开始支持设置多个地址。\n    pd-addr = \"172.16.31.3:2379,56.78.90.12:3456\"\n    ```\n\n2. 运行 `tidb-lightning`。为避免直接在命令行使用 `nohup` 启动程序时因 `SIGHUP` 信号导致的程序退出，建议将 `nohup` 命令放入脚本中。示例如下：\n\n    ```shell\n    #!/bin/bash\n    nohup tiup tidb-lightning -config tidb-lightning.toml > nohup.out &\n    ```\n\n## 第 5 步：检查数据\n\n导入完毕后，TiDB Lightning 会自动退出。若导入成功，日志的最后一行会显示 `tidb lightning exit`。\n\n如果出错，请参见 [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)。\n\n## 总结\n\n本教程对 TiDB Lightning 进行了简单的介绍，并快速部署了一套简单的 TiDB Lightning 集群，将全量备份数据导入到 TiDB 集群中。\n\n关于 TiDB Lightning 的详细功能和使用，参见 [TiDB Lightning 简介](/tidb-lightning/tidb-lightning-overview.md)。\n"
        },
        {
          "name": "glossary.md",
          "type": "blob",
          "size": 18.4619140625,
          "content": "---\ntitle: 术语表\nsummary: 了解 TiDB 相关术语。\naliases: ['/docs-cn/dev/glossary/']\n---\n\n# 术语表\n\n本术语表提供了 TiDB 中的关键术语定义。\n\n此外，你还可以参考以下术语表：\n\n- [TiDB Data Migration 术语表](/dm/dm-glossary.md)\n- [TiCDC 术语表](/ticdc/ticdc-glossary.md)\n- [TiDB Lightning 术语表](/tidb-lightning/tidb-lightning-glossary.md)\n\n## A\n\n### ACID\n\nACID 是指数据库管理系统在写入或更新资料的过程中，为保证事务是正确可靠的，所必须具备的四个特性：原子性 (atomicity)、一致性 (consistency)、隔离性 (isolation) 以及持久性 (durability)。\n\n* 原子性 (atomicity) 指一个事务中的所有操作，或者全部完成，或者全部不完成，不会结束在中间某个环节。TiDB 通过 Primary Key 所在 [Region](#regionpeerraft-group) 的原子性来保证分布式事务的原子性。\n* 一致性 (consistency) 指在事务开始之前和结束以后，数据库的完整性没有被破坏。TiDB 在写入数据之前，会校验数据的一致性，校验通过才会写入内存并返回成功。\n* 隔离性 (isolation) 指数据库允许多个并发事务同时对其数据进行读写和修改的能力。隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，主要用于处理并发场景。关于 TiDB 支持的隔离级别，请参考 [TiDB 事务隔离级别](/transaction-isolation-levels.md#tidb-事务隔离级别)。\n* 持久性 (durability) 指事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。在 TiDB 中，事务一旦提交成功，数据全部持久化存储到 TiKV，此时即使 TiDB 服务器宕机也不会出现数据丢失。\n\n## B\n\n### Backup & Restore (BR)\n\n**Backup & Restore** 或 **BR** 指代 [TiDB 备份恢复功能](/br/backup-and-restore-overview.md)。\n\n`br` 指代进行 TiDB 备份或恢复时使用的 [br 命令行工具](/br/use-br-command-line-tool.md)。\n\n### Batch Create Table\n\n批量建表 (Batch Create Table) 是在 TiDB v6.0.0 中引入的新功能，此功能默认开启。当需要恢复的数据中带有大量的表（约 50000 张）时，批量建表功能显著提升数据恢复的速度。详情参见[批量建表](/br/br-batch-create-table.md)。\n\n### Baseline Capturing\n\n自动捕获绑定 (Baseline Capturing) 会对符合捕获条件的查询进行捕获，为符合条件的查询生成相应的绑定。通常用于升级时的[计划回退防护](/sql-plan-management.md#升级时的计划回退防护)。\n\n### Bucket\n\n一个 [Region](#regionpeerraft-group) 在逻辑上划分为多个小范围，称为 bucket。TiKV 按 bucket 收集查询统计数据，并将 bucket 的情况报告给 PD。详情参见 [Bucket 设计文档](https://github.com/tikv/rfcs/blob/master/text/0082-dynamic-size-region.md#bucket)。\n\n## C\n\n### Cached Table\n\n缓存表 (Cached Table) 是指 TiDB 把整张表的数据加载到服务器的内存中，直接从内存中获取表数据，避免从 TiKV 获取表数据，从而提升读性能。详情参见[缓存表](/cached-tables.md)。\n\n### Coalesce Partition\n\nCoalesce Partition 是一种减少 Hash 分区表或 Key 分区表中分区数量的方法。详情参见[管理 Hash 分区和 Key 分区](/partitioned-table.md#管理-hash-分区和-key-分区)。\n\n### Column Family (CF)\n\n在 RocksDB 和 TiKV 中，Column Family (CF，列族) 表示数据库中键值对的逻辑分组。\n\n### 公共表表达式 (CTE)\n\n公共表表达式 (Common Table Expression, CTE) 用于定义一个临时结果集，能够在 SQL 语句中通过 [`WITH`](/sql-statements/sql-statement-with.md) 子句多次引用。更多信息，请参见[公共表表达式](/develop/dev-guide-use-common-table-expression.md)。\n\n### Continuous Profiling\n\n持续性能分析 (Continuous Profiling) 是从 TiDB v5.3 起引入的一种从系统调用层面解读资源开销的方法。引入该方法后，TiDB 可提供数据库源码级性能观测，通过火焰图的形式帮助研发、运维人员定位性能问题的根因。详情参见 [TiDB Dashboard 实例性能分析 - 持续分析页面](/dashboard/continuous-profiling.md)。\n\n## D\n\n### Data Definition Language (DDL)\n\n数据定义语言 (Data Definition Language, DDL) 是 SQL 标准的一部分，用于创建、修改和删除表及其他对象。更多信息，请参见 [DDL 语句的执行原理及最佳实践](/ddl-introduction.md)。\n\n### Data Migration (DM)\n\nData Migration (DM) 是 TiDB 提供的一款数据迁移工具，用于将数据从 MySQL 兼容的数据库迁移到 TiDB。DM 会从 MySQL 兼容的数据库实例读取数据，然后将其应用到 TiDB 目标实例中。更多信息，请参见 [TiDB Data Migration 简介](/dm/dm-overview.md)。\n\n### Data Modification Language (DML)\n\n数据操作语言 (Data Modification Language, DML) 是 SQL 标准的一部分，用于插入、更新和删除表中的行数据。\n\n### Development Milestone Release (DMR)\n\nTiDB 会在开发里程碑版本 (Development Milestone Release, DMR) 中引入新的功能，但 DMR 不提供长期支持。更多信息，请参见 [TiDB 版本规则](/releases/versioning.md)。\n\n### 容灾 (DR)\n\n容灾 (Disaster Recovery, DR) 是在未来灾难发生时恢复数据和服务的解决方案。TiDB 提供了多种容灾方案，例如备份和复制数据到备用集群。更多信息，请参见 [TiDB 容灾方案概述](/dr-solution-introduction.md)。\n\n### 分布式执行框架 (DXF)\n\n分布式执行框架 (Distributed eXecution Framework, DXF) 允许 TiDB 在处理特定任务（例如创建索引或导入数据）时对这些任务进行统一调度和分布式执行。该框架旨在高效利用集群资源执行任务，控制资源使用，以减少对核心业务事务的影响。更多信息，请参见 [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)。\n\n### Dynamic Pruning\n\n动态裁剪 (Dynamic Pruning) 是 TiDB 访问分区表的两种模式之一。在动态裁剪模式下，TiDB 的每个算子都支持直接访问多个分区，省略 Union 操作，提高执行效率，还避免了 Union 并发管理的问题。\n\n## G\n\n### Garbage Collection (GC)\n\n垃圾回收 (Garbage Collection, GC) 指清理不再需要的旧数据以释放资源的过程。关于 TiKV 垃圾回收过程的详情，请参见[垃圾回收概述](/garbage-collection-overview.md)。\n\n### General Availability (GA)\n\n一个功能 GA (General Availability) 意味着该功能已进行充分测试并可在生产环境中使用。根据每个功能的开发情况不同，TiDB 中的新功能可能会在[开发里程碑版本 (DMR)](#development-milestone-release-dmr) 中 GA，也可能会在[长期支持版本 (LTS)](#long-term-support-lts) 中 GA 。由于 TiDB 不提供基于 DMR 的补丁版本，在生产环境中建议使用 LTS 版本。\n\n### Global Transaction Identifiers (GTIDs)\n\n全局事务标识符 (Global Transaction Identifiers, GTIDs) 是在 MySQL 二进制日志中跟踪已复制事务的唯一标识符。[Data Migration (DM)](/dm/dm-overview.md) 在迁移数据时会使用这些标识符确保复制的一致性。\n\n## H\n\n### Hybrid Transactional and Analytical Processing (HTAP)\n\n混合型在线事务与在线分析处理 (Hybrid Transactional and Analytical Processing, HTAP) 功能支持在同一数据库中同时处理 OLTP（联机事务处理）和 OLAP（联机分析处理）工作负载。在 TiDB 中，HTAP 是通过使用 TiKV 进行行存以及使用进行 TiFlash 进行列存来实现的。更多信息，请参见 [Gartner 网站上的 HTAP 定义](https://www.gartner.com/en/information-technology/glossary/htap-enabling-memory-computing-technologies)。\n\n## I\n\n### In-Memory Pessimistic Lock\n\n内存悲观锁 (In-Memory Pessimistic Lock) 是在 TiDB v6.0.0 中引入的新功能。开启内存悲观锁功能后，悲观锁通常只会被存储在 Region leader 的内存中，而不会将锁持久化到磁盘，也不会通过 Raft 协议将锁同步到其他副本，因此可以大大降低悲观事务加锁的开销，提升悲观事务的吞吐并降低延迟。\n\n### Index Merge\n\n索引合并 (Index Merge) 是在 TiDB v4.0 版本中作为实验特性引入的一种查询执行方式的优化，可以大幅提高查询在扫描多列数据时条件过滤的效率。自 v5.4 版本起，Index Merge 成为正式功能，详情参见[用 EXPLAIN 查看索引合并的 SQL 执行计划](/explain-index-merge.md)。\n\n## K\n\n### Key Management Service (KMS)\n\n密钥管理服务 (Key Management Service, KMS) 提供了一种存储和检索密钥的安全方式。常见的 KMS 包括 AWS KMS、Google Cloud KMS 和 HashiCorp Vault。TiDB 中的多个组件都支持通过 KMS 管理用于存储加密和相关服务的密钥。\n\n### Key-Value (KV)\n\n键值 (Key-Value, KV) 是一种通过唯一键来关联值并存储信息的数据结构，它能够实现快速的数据检索。TiDB 通过 TiKV 将表和索引映射为键值对，从而实现数据库中的高效数据存储和访问。\n\n## L\n\n### Leader/Follower/Learner\n\n它们分别对应 [Peer](#regionpeerraft-group) 的三种角色。其中 Leader 负责响应客户端的读写请求；Follower 被动地从 Leader 同步数据，当 Leader 失效时会进行选举产生新的 Leader；Learner 是一种特殊的角色，它只参与同步 raft log 而不参与投票，在目前的实现中只短暂存在于添加副本的中间步骤。\n\n### Lightweight Directory Access Protocol (LDAP)\n\n轻量级目录访问协议 (Lightweight Directory Access Protocol, LDAP) 是一种标准化的目录信息访问方式，通常用于账户和用户数据的管理。TiDB 对 LDAP 的支持是通过 [LDAP 身份验证插件](/security-compatibility-with-mysql.md#可用的身份验证插件)实现的。\n\n### Long Term Support (LTS)\n\n长期支持 (Long Term Support, LTS) 版本指经过充分测试并在较长时间内维护的软件版本。更多信息，请参见 [TiDB 版本规则](/releases/versioning.md)。\n\n## M\n\n### Massively Parallel Processing (MPP)\n\n从 v5.0 起，TiDB 通过 TiFlash 节点引入了 Massively Parallel Processing (MPP) 架构。这使得大型表连接类查询可以由不同 TiFlash 节点共同分担完成。当 MPP 模式开启后，TiDB 将会根据代价决定是否应该交由 MPP 框架进行计算。MPP 模式下，表连接将通过对 JOIN Key 进行数据计算时重分布（Exchange 操作）的方式把计算压力分摊到各个 TiFlash 执行节点，从而达到加速计算的目的。更多信息请参见[使用 MPP 模式](/tiflash/use-tiflash-mpp-mode.md)。\n\n### Multi-version concurrency control (MVCC)\n\n[MVCC](https://zh.wikipedia.org/wiki/多版本并发控制)（多版本并发控制）是 TiDB 和其他数据库中的一种并发控制机制。它处理事务的内存读取，以实现对 TiDB 的并发访问，从而避免由并发读写冲突引起的阻塞。\n\n## O\n\n### Old value\n\nOld value 特指在 TiCDC 输出的增量变更日志中的“原始值”。可以通过配置来指定 TiCDC 输出的增量变更日志是否包含“原始值”。\n\n### Online Analytical Processing (OLAP)\n\n在线分析处理 (Online Analytical Processing, OLAP) 指的是以分析任务为主的数据库工作负载，例如数据报告和复杂查询。OLAP 的特点是涉及大量行数据的读密集型查询。\n\n### Online Transaction Processing (OLTP)\n\n在线事务处理 (Online Transaction Processing, OLTP) 指的是以事务性任务为主的数据库工作负载，例如读取、插入、更新和删除少量记录。\n\n### Out of Memory (OOM)\n\n内存不足 (Out of Memory, OOM) 指的是系统由于内存不足而引起失败的情况。更多信息，请参见 [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)。\n\n### Operator\n\nOperator 是应用于一个 Region 的，服务于某个调度目的的一系列操作的集合。例如“将 Region 2 的 Leader 迁移至 Store 5”，“将 Region 2 的副本迁移到 Store 1, 4, 5”等。\n\nOperator 可以是由 Scheduler 通过计算生成的，也可以是由外部 API 创建的。\n\n### Operator Step\n\nOperator Step 是 Operator 执行过程的一个步骤，一个 Operator 常常会包含多个 Operator Step。\n\n目前 PD 可生成的 Step 包括：\n\n- `TransferLeader`：将 Region Leader 迁移至指定 Peer\n- `AddPeer`：在指定 Store 添加 Follower\n- `RemovePeer`：删除一个 Region Peer\n- `AddLearner`：在指定 Store 添加 Region Learner\n- `PromoteLearner`：将指定 Learner 提升为 Follower\n- `SplitRegion`：将指定 Region 一分为二\n\n## P\n\n### Partitioning\n\n[Partitioning](/partitioned-table.md)（分区）指通过 `RANGE`、`LIST`、`HASH` 和 `KEY` 等分区方法在物理上将一张表划分为较小的分区。\n\n### Pending/Down\n\nPending 和 Down 是 Peer 可能出现的两种特殊状态。其中 Pending 表示 Follower 或 Learner 的 raft log 与 Leader 有较大差距，Pending 状态的 Follower 无法被选举成 Leader。Down 是指 Leader 长时间没有收到对应 Peer 的消息，通常意味着对应节点发生了宕机或者网络隔离。\n\n### Placement Driver (PD)\n\nPD 是 [TiDB 架构](/tidb-architecture.md) 中的核心组件之一，负责存储元数据，为事务时间戳分配[时间戳服务 (TSO)](/tso.md)，协调 TiKV 上的数据分布，并运行 [TiDB Dashboard](/dashboard/dashboard-overview.md)。更多信息，请参见 [TiDB 调度](/tidb-scheduling.md)。\n\n### Point get\n\n点查 (point get) 是指通过主键或唯一索引直接读取一行的查询方式。点查的返回结果最多是一行数据。\n\n### Point in Time Recovery (PITR)\n\nPITR 用于将数据恢复到特定时间点（例如，在意外执行了 `DELETE` 语句之前的时间点）。更多信息，请参见 [TiDB 日志备份与 PITR 功能架构](/br/br-log-architecture.md)。\n\n### Predicate columns\n\n执行 SQL 语句时，优化器在大多数情况下只会用到部分列（例如，`WHERE`、`JOIN`、`ORDER BY`、`GROUP BY` 子句中出现的列）的统计信息，这些用到的列称为 `PREDICATE COLUMNS`。详情参见[收集部分列的统计信息](/statistics.md#收集部分列的统计信息)。\n\n## Q\n\n### Queries Per Second (QPS)\n\n每秒查询数 (Queries Per Second, QPS) 指的是数据库服务每秒处理的查询数量。它是衡量数据库吞吐量的重要性能指标。\n\n### Quota Limiter\n\n前台限流 (Quota Limiter) 是在 TiDB v6.0.0 版本中作为实验特性引入的功能。当 TiKV 部署的机型资源有限（如 4v CPU，16 G 内存）时，如果 TiKV 前台处理的读写请求量过大，会占用 TiKV 后台处理请求所需的 CPU 资源，最终影响 TiKV 性能的稳定性。此时，开启前台限流相关的 [quota 相关配置项](/tikv-configuration-file.md#quota)可以限制前台各类请求占用的 CPU 资源。\n\n## R\n\n### Raft Engine\n\n一种内置的持久化存储引擎，有着日志结构的设计，为 TiKV 提供 multi-Raft 日志存储。从 v5.4 起，TiDB 支持使用 Raft Engine 作为 TiKV 的日志存储引擎。详情参见 [Raft Engine](/tikv-configuration-file.md#raft-engine)。\n\n### Region Split\n\nTiKV 集群中的 Region 不是一开始就划分好的，而是随着数据写入逐渐分裂生成的，分裂的过程被称为 Region Split。\n\n其机制是集群初始化时构建一个初始 Region 覆盖整个 key space，随后在运行过程中每当 Region 大小或 Key 数量达到阈值之后就通过 Split 产生新的 Region。\n\n### Region/Peer/Raft Group\n\n每个 Region 负责维护集群的一段连续数据（默认配置下平均约 256 MiB），每份数据会在不同的 Store 存储多个副本（默认配置是 3 副本），每个副本称为 Peer。同一个 Region 的多个 Peer 通过 raft 协议进行数据同步，所以 Peer 也用来指代 raft 实例中的成员。TiKV 使用 multi-raft 模式来管理数据，即每个 Region 都对应一个独立运行的 raft 实例，我们也把这样的一个 raft 实例叫做一个 Raft Group。\n\n### Remote Procedure Call (RPC)\n\nRPC（远程过程调用）是软件组件之间的一种通信方式。在 TiDB 集群中，不同组件（例如 TiDB、TiKV 和 TiFlash）之间使用 gRPC 标准进行通信。\n\n### Request Unit (RU)\n\nRU 是 TiDB 中资源使用的统一抽象单位，用于在[资源管控](/tidb-resource-control.md)功能中衡量资源的使用情况。\n\n### Restore\n\n备份操作的逆过程，即利用保存的备份数据还原出原始数据的过程。\n\n## S\n\n### Scheduler\n\nScheduler（调度器）是 PD 中生成调度的组件。PD 中每个调度器是独立运行的，分别服务于不同的调度目的。常用的调度器及其调用目标有：\n\n- `balance-leader-scheduler`：保持不同节点的 Leader 均衡。\n- `balance-region-scheduler`：保持不同节点的 Peer 均衡。\n- `hot-region-scheduler`：保持不同节点的读写热点 Region 均衡。\n- `evict-leader-{store-id}`：驱逐某个节点的所有 Leader。（常用于滚动升级）\n\n### Static Sorted Table / Sorted String Table (SST)\n\nSST 是 RocksDB 使用的文件存储格式。RocksDB 是 [TiKV](/storage-engine/rocksdb-overview.md) 的一种存储引擎。\n\n### Store\n\nPD 中的 Store 指的是集群中的存储节点，也就是 tikv-server 实例。Store 与 TiKV 实例是严格一一对应的，即使在同一主机甚至同一块磁盘部署多个 TiKV 实例，这些实例也对会对应不同的 Store。\n\n## T\n\n### Timestamp Oracle (TSO)\n\n因为 TiKV 是一个分布式的储存系统，它需要一个全球性的授时服务 TSO (Timestamp Oracle)，来分配一个单调递增的时间戳。这样的功能在 TiKV 中是由 PD 提供的，在 Google 的 [Spanner](http://static.googleusercontent.com/media/research.google.com/en//archive/spanner-osdi2012.pdf) 中是由多个原子钟和 GPS 来提供的。详见 [TSO 文档](/tso.md)。\n\n### Top SQL\n\nTop SQL 用于找到一段时间内对某个 TiDB 或 TiKV 节点消耗负载较大的 SQL 查询。详见 [Top SQL 文档](/dashboard/top-sql.md)。\n\n### Transactions Per Second (TPS)\n\n每秒事务数 (Transactions Per Second, TPS) 指的是数据库每秒处理的事务数量。它是衡量数据库性能和吞吐量的关键指标。\n\n## U\n\n### Uniform Resource Identifier (URI)\n\n统一资源标识符 (Uniform Resource Identifier, URI) 是用于标识资源的一种标准化格式。更多信息，请参见维基百科的[统一资源标识符](https://zh.wikipedia.org/wiki/统一资源标识符)页面。\n\n### Universally Unique Identifier (UUID)\n\n通用唯一标识符 (Universally Unique Identifier, UUID) 是一种 128 位（16 字节）的生成 ID，用于在数据库中唯一地标识记录。更多信息，请参见 [UUID](/best-practices/uuid.md)。"
        },
        {
          "name": "grafana-overview-dashboard.md",
          "type": "blob",
          "size": 5.4833984375,
          "content": "---\ntitle: Overview 面板重要监控指标详解\naliases: ['/docs-cn/dev/grafana-overview-dashboard/','/docs-cn/dev/reference/key-monitoring-metrics/overview-dashboard/']\nsummary: TiUP 部署 TiDB 集群时，一键部署监控系统 (Prometheus & Grafana)。Grafana Dashboard 分为 PD、TiDB、TiKV、Node_exporter、Overview、Performance_overview。重要监控指标包括服务在线节点数量、PD 角色、存储容量、Region 数量、TiDB 执行数量、CPU 使用率、内存大小、网络流量等。详细监控说明可参见文章。\n---\n\n# Overview 面板重要监控指标详解\n\n使用 TiUP 部署 TiDB 集群时，一键部署监控系统 (Prometheus & Grafana)，监控架构参见 [TiDB 监控框架概述](/tidb-monitoring-framework.md)。\n\n目前 Grafana Dashboard 整体分为 PD、TiDB、TiKV、Node\\_exporter、Overview、Performance\\_overview 等。\n\n对于日常运维，我们单独挑选出重要的 Metrics 放在 Overview 页面，方便日常运维人员观察集群组件 (PD, TiDB, TiKV) 使用状态以及集群使用状态。\n\n以下为 Overview Dashboard 监控说明：\n\n## Services Port Status\n\n- Services Up：各服务在线节点数量\n\n## PD\n\n- PD role：当前 PD 的角色\n- Storage capacity：TiDB 集群总可用数据库空间大小\n- Current storage size：TiDB 集群目前已用数据库空间大小，TiKV 多副本的空间占用也会包含在内\n- Normal stores：处于正常状态的节点数目\n- Abnormal stores：处于异常状态的节点数目，正常情况应当为 0\n- Number of Regions：当前集群的 Region 总量，请注意 Region 数量与副本数无关\n- 99% completed\\_cmds\\_duration\\_seconds：单位时间内，99% 的 pd-server 请求执行时间小于监控曲线的值，一般 <= 5ms\n- Handle\\_requests\\_duration\\_seconds：PD 发送请求的网络耗时\n- Region health：每个 Region 的状态，通常情况下，pending 的 peer 应该少于 100，miss 的 peer 不能一直大于 0\n- Hot write Region's leader distribution：每个 TiKV 实例上是写入热点的 leader 的数量\n- Hot read Region's leader distribution：每个 TiKV 实例上是读取热点的 leader 的数量\n- Region heartbeat report：TiKV 向 PD 发送的心跳个数\n- 99% Region heartbeat latency：99% 的情况下，心跳的延迟\n\n## TiDB\n\n- Statement OPS：不同类型 SQL 语句每秒执行的数量。按 `SELECT`、`INSERT`、`UPDATE` 等来统计\n- Duration：执行的时间\n    - 客户端网络请求发送到 TiDB，到 TiDB 执行结束后返回给客户端的时间。一般情况下，客户端请求都是以 SQL 语句的形式发送，但也可以包含 `COM_PING`、`COM_SLEEP`、`COM_STMT_FETCH`、`COM_SEND_LONG_DATA` 之类的命令执行的时间\n    - 由于 TiDB 支持 Multi-Query，因此，可以接受客户端一次性发送的多条 SQL 语句，如：`select 1; select 1; select 1;`。此时，统计的执行时间是所有 SQL 执行完之后的总时间\n- CPS By Instance：每个 TiDB 实例上的命令统计。按照命令和执行结果成功或失败来统计\n- Failed Query OPM：每个 TiDB 实例上，每秒钟执行 SQL 语句发生错误按照错误类型的统计（例如语法错误、主键冲突等）。包含了错误所属的模块和错误码\n- Connection count：每个 TiDB 的连接数\n- Memory Usage：每个 TiDB 实例的内存使用统计，分为进程占用内存和 Golang 在堆上申请的内存\n- Transaction OPS：每秒事务执行数量统计\n- Transaction Duration：事务执行的时间\n- KV Cmd OPS：KV 命令执行数量统计\n- KV Cmd Duration 99：KV 命令执行的时间\n- PD TSO OPS：TiDB 每秒向 PD 发送获取 TSO 的 gRPC 请求的数量 (cmd) 和实际的 TSO 请求数量 (request)；每个 gRPC 请求包含一批 TSO 请求\n- PD TSO Wait Duration：TiDB 等待从 PD 获取 TS 的时间\n- TiClient Region Error OPS：TiKV 返回 Region 相关错误信息的数量\n- Lock Resolve OPS：TiDB 清理锁操作的数量。当 TiDB 的读写请求遇到锁时，会尝试进行锁清理\n- Load Schema Duration：TiDB 从 TiKV 获取 Schema 的时间\n- KV Backoff OPS：TiKV 返回错误信息的数量\n\n## TiKV\n\n- leader：各个 TiKV 节点上 Leader 的数量分布\n- region：各个 TiKV 节点上 Region 的数量分布\n- CPU：各个 TiKV 节点的 CPU 使用率\n- Memory：各个 TiKV 节点的内存使用量\n- store size：每个 TiKV 实例的使用的存储空间的大小\n- cf size：每个列族的大小\n- channel full：每个 TiKV 实例上 channel full 错误的数量，正常情况下应当为 0\n- server report failures：每个 TiKV 实例上报错的消息个数，正常情况下应当为 0\n- scheduler pending commands：每个 TiKV 实例上 pending 命令的个数\n- coprocessor executor count：TiKV 每秒收到的 coprocessor 操作数量，按照 coprocessor 类型统计\n- coprocessor request duration：处理 coprocessor 读请求所花费的时间\n- raft store CPU：raftstore 线程的 CPU 使用率，线程数量默认为 2（通过 `raftstore.store-pool-size` 配置）。如果单个线程使用率超过 80%，说明使用率很高\n- Coprocessor CPU：coprocessor 线程的 CPU 使用率\n\n## System Info\n\n- Vcores：CPU 核心数量\n- Memory：内存总大小\n- CPU Usage：CPU 使用率，最大为 100%\n- Load [1m]：1 分钟的负载情况\n- Memory Available：剩余内存大小\n- Network Traffic：网卡流量统计\n- TCP Retrans：TCP 重传数量统计\n- IO Util：磁盘使用率，最高为 100%，一般到 80% - 90% 就需要考虑加节点\n\n## 图例\n\n![overview](/media/grafana_monitor_overview.png)\n"
        },
        {
          "name": "grafana-pd-dashboard.md",
          "type": "blob",
          "size": 8.3759765625,
          "content": "---\ntitle: PD 重要监控指标详解\naliases: ['/docs-cn/dev/grafana-pd-dashboard/','/docs-cn/dev/reference/key-monitoring-metrics/pd-dashboard/']\nsummary: PD 重要监控指标详解：使用 TiUP 部署 TiDB 集群时，一键部署监控系统 (Prometheus & Grafana)，监控架构参见 [TiDB 监控框架概述]。Grafana Dashboard 分为 PD、TiDB、TiKV、Node_exporter、Overview、Performance_overview 等。通过观察 PD 面板上的 Metrics，可以了解 PD 当前的状态。监控包括 PD role、Storage capacity、Current storage size、Current storage usage、Normal stores、Number of Regions、Abnormal stores、Region health、Current peer count 等。Cluster、Operator、Statistics - Balance、Statistics - hot write、Statistics - hot read、Scheduler、gRPC、etcd、TiDB、Heartbeat、Region storage 等指标也很重要。\n---\n\n# PD 重要监控指标详解\n\n使用 TiUP 部署 TiDB 集群时，一键部署监控系统 (Prometheus & Grafana)，监控架构参见 [TiDB 监控框架概述](/tidb-monitoring-framework.md)。\n\n目前 Grafana Dashboard 整体分为 PD、TiDB、TiKV、Node\\_exporter、Overview、Performance\\_overview 等。\n\n对于日常运维，我们通过观察 PD 面板上的 Metrics，可以了解 PD 当前的状态。\n\n以下为 PD Dashboard 监控说明：\n\n- PD role：当前 PD 的角色\n- Storage capacity：TiDB 集群总可用数据库空间大小\n- Current storage size：TiDB 集群目前已用数据库空间大小\n- Current storage usage：TiDB 集群存储空间的使用率\n- Normal stores：处于正常状态的节点数目\n- Number of Regions：当前集群的 Region 总量\n- Abnormal stores：处于异常状态的节点数目，正常情况应当为 0\n- Region health：集群所有 Region 的状态。通常情况下，pending 或 down 的 peer 应该少于 100，miss 的 peer 不能一直大于 0，empty Region 过多需及时打开 Region Merge\n- Current peer count：当前集群 peer 的总量\n![PD Dashboard - Header](/media/pd-dashboard-header-v4.png)\n\n## Cluster\n\n- PD scheduler config：PD 调度配置列表\n- Cluster ID：集群的 cluster id，唯一标识\n- Current TSO：当前分配 TSO 的物理时间戳部分\n- Current ID allocation：当前可分配 ID 的最大值\n- Region label isolation level：不同 label 所在的 level 的 Region 数量\n- Label distribution：集群中 TiKV 节点的 label 分布情况\n- Store Limit：Store 的调度限流状态\n\n![PD Dashboard - Cluster metrics](/media/pd-dashboard-cluster-v4.png)\n\n## Operator\n\n- Schedule operator create：新创建的不同 operator 的数量，单位 opm 代表一分钟内创建的个数 \n- Schedule operator check：已检查的 operator 的次数，主要检查是否当前步骤已经执行完成，如果是，则执行下一个步骤\n- Schedule operator finish：已完成调度的 operator 的数量\n- Schedule operator timeout：已超时的 operator 的数量\n- Schedule operator replaced or canceled：已取消或者被替换的 operator 的数量\n- Schedule operators count by state：不同状态的 operator 的数量\n- Operator finish duration：已完成的 operator 所花费的最长时间\n- Operator step duration：已完成的 operator 的步骤所花费的最长时间\n\n![PD Dashboard - Operator metrics](/media/pd-dashboard-operator-v4.png)\n\n## Statistics - Balance\n\n- Store capacity：每个 TiKV 实例的总的空间大小\n- Store available：每个 TiKV 实例的可用空间大小\n- Store used：每个 TiKV 实例的已使用空间大小\n- Size amplification：每个 TiKV 实例的空间放大比率\n- Size available ratio：每个 TiKV 实例的可用空间比率\n- Store leader score：每个 TiKV 实例的 leader 分数\n- Store Region score：每个 TiKV 实例的 Region 分数\n- Store leader size：每个 TiKV 实例上所有 leader 的大小\n- Store Region size：每个 TiKV 实例上所有 Region 的大小\n- Store leader count：每个 TiKV 实例上所有 leader 的数量\n- Store Region count：每个 TiKV 实例上所有 Region 的数量\n\n![PD Dashboard - Balance metrics](/media/pd-dashboard-balance-v4.png)\n\n## Statistics - hot write\n\n- Hot Region's leader distribution：每个 TiKV 实例上成为写入热点的 leader 的数量\n- Total written bytes on hot leader Regions：每个 TiKV 实例上所有成为写入热点的 leader 的总的写入流量大小\n- Hot write Region's peer distribution：每个 TiKV 实例上成为写入热点的 peer 的数量\n- Total written bytes on hot peer Regions：每个 TiKV 实例上所有成为写入热点的 peer 的写入流量大小\n- Store Write rate bytes：每个 TiKV 实例总的写入的流量\n- Store Write rate keys：每个 TiKV 实例总的写入 keys\n- Hot cache write entry number：每个 TiKV 实例进入热点统计模块的 peer 的数量\n- Selector events：热点调度中选择器的事件发生次数\n- Direction of hotspot move leader：热点调度中 leader 的调度方向，正数代表调入，负数代表调出\n- Direction of hotspot move peer：热点调度中 peer 的调度方向，正数代表调入，负数代表调出\n\n![PD Dashboard - Hot write metrics](/media/pd-dashboard-hotwrite-v4.png)\n\n## Statistics - hot read\n\n- Hot Region's peer distribution：每个 TiKV 实例上成为读取热点的 peer 的数量\n- Total read bytes on hot peer Regions：每个 TiKV 实例上所有成为读取热点的 peer 的总的读取流量大小\n- Store read rate bytes：每个 TiKV 实例总的读取的流量\n- Store read rate keys：每个 TiKV 实例总的读取 keys\n- Hot cache read entry number：每个 TiKV 实例进入热点统计模块的 peer 的数量\n\n![PD Dashboard - Hot read metrics](/media/pd-dashboard-hotread-v4.png)\n\n## Scheduler\n\n- Scheduler is running：所有正在运行的 scheduler\n- Balance leader movement：leader 移动的详细情况\n- Balance Region movement：Region 移动的详细情况\n- Balance leader event：balance leader 的事件数量\n- Balance Region event：balance Region 的事件数量\n- Balance leader scheduler：balance-leader scheduler 的状态\n- Balance Region scheduler：balance-region scheduler 的状态\n- Replica checker：replica checker 的状态\n- Rule checker：rule checker 的状态\n- Region merge checker：merge checker 的状态\n- Filter target：尝试选择 Store 作为调度 target 时没有通过 Filter 的计数\n- Filter source：尝试选择 Store 作为调度 source 时没有通过 Filter 的计数\n- Balance Direction：Store 被选作调度 target 或 source 的次数\n\n![PD Dashboard - Scheduler metrics](/media/pd-dashboard-scheduler-v4.png)\n\n## gRPC\n\n- Completed commands rate：gRPC 命令的完成速率\n- 99% Completed commands duration：99% 命令的最长消耗时间\n\n![PD Dashboard - gRPC metrics](/media/pd-dashboard-grpc-v2.png)\n\n## etcd\n\n- Handle transactions count：etcd 的事务个数\n- 99% Handle transactions duration：99% 的情况下，处理 etcd 事务所需花费的时间\n- 99% WAL fsync duration：99% 的情况下，持久化 WAL 所需花费的时间，这个值通常应该小于 1s\n- 99% Peer round trip time seconds：99% 的情况下，etcd 的网络延时，这个值通常应该小于 1s\n- etcd disk WAL fsync rate：etcd 持久化 WAL 的速率\n- Raft term：当前 Raft 的 term\n- Raft committed index：最后一次 commit 的 Raft index\n- Raft applied index：最后一次 apply 的 Raft index\n\n![PD Dashboard - etcd metrics](/media/pd-dashboard-etcd-v2.png)\n\n## TiDB\n\n- PD Server TSO handle time and Client recv time：从 PD 开始处理 TSO 请求到 client 端接收到 TSO 的总耗时\n- Handle requests count：TiDB 的请求数量\n- Handle requests duration：每个请求所花费的时间，99% 的情况下，应该小于 100ms\n\n![PD Dashboard - TiDB metrics](/media/pd-dashboard-tidb-v4.png)\n\n## Heartbeat\n\n- Heartbeat region event QPS：心跳处理 region 的 QPS，包括更新缓存和持久化\n- Region heartbeat report：TiKV 向 PD 发送的心跳个数\n- Region heartbeat report error：TiKV 向 PD 发送的异常的心跳个数\n- Region heartbeat report active：TiKV 向 PD 发送的正常的心跳个数\n- Region schedule push：PD 向 TiKV 发送的调度命令的个数\n- 99% Region heartbeat latency：99% 的情况下，心跳的延迟\n\n![PD Dashboard - Heartbeat metrics](/media/pd-dashboard-heartbeat-v4.png)\n\n## Region storage\n\n- Syncer Index：Leader 记录 Region 变更历史的最大 index\n- history last index：Follower 成功同步的 Region 变更历史的 index\n\n![PD Dashboard - Region storage](/media/pd-dashboard-region-storage.png)\n"
        },
        {
          "name": "grafana-performance-overview-dashboard.md",
          "type": "blob",
          "size": 11.1875,
          "content": "---\ntitle: Performance Overview 面板重要监控指标详解\nsummary: 本文介绍 Performance Overview 面板上监控指标的含义。\naliases: ['/zh/tidb/v6.0/grafana-performance-overview-dashboard']\n---\n\n# Performance Overview 面板重要监控指标详解\n\n使用 TiUP 部署 TiDB 集群时，你可以一键部署监控系统 (Prometheus & Grafana)。监控架构参见 [TiDB 监控框架概述](/tidb-monitoring-framework.md)。\n\n目前 Grafana Dashboard 整体分为 PD、TiDB、TiKV、Node\\_exporter、Overview、Performance\\_overview 等。\n\nPerformance Overview Dashboard 按总分结构对 TiDB、TiKV、PD 的性能指标进行编排组织，包含了以下三部分内容：\n\n- 总的概览：数据库时间和 SQL 执行时间概览。通过颜色优化法，你可以快速识别数据库负载特征和性能瓶颈。\n- 资源负载：关键指标和资源利用率，包含数据库 QPS、应用和数据库的连接信息和请求命令类型、数据库内部 TSO 和 KV 请求 OPS、TiDB 和 TiKV 的资源使用概况。\n- 自上而下的延迟分解：Query 延迟和连接空闲时间对比、Query 延迟分解、execute 阶段 TSO 请求和 KV 请求的延迟、TiKV 内部写延迟的分解等。\n\n借助 Performance Overview Dashboard，你可以高效地进行性能分析，确认用户响应时间的瓶颈是否在数据库中。如果数据库是整个系统的瓶颈，通过数据库时间概览和 SQL 延迟的分解，定位数据库内部的瓶颈点，并进行针对性的优化。详情请参考 [TiDB 性能分析和优化方法](/performance-tuning-methods.md)。\n\n以下为 Performance Overview Dashboard 监控说明：\n\n## Performance Overview\n\n### Database Time by SQL Type\n\n- database time: 每秒的总数据库时间\n- sql_type: 每种 SQL 语句每秒消耗的数据库时间\n\n### Database Time by SQL Phase\n\n- database time: 每秒的总数据库时间\n- get token/parse/compile/execute: 4 个 SQL 处理阶段每秒消耗的数据库时间\n\nexecute 执行阶段为绿色，其他三个阶段偏红色系，如果非绿色的颜色占比明显，意味着在执行阶段之外数据库消耗了过多时间，需要进一步分析根源。\n\n### SQL Execute Time Overview\n\n- execute time: execute 阶段每秒消耗的数据库时间\n- tso_wait: execute 阶段每秒同步等待 TSO 的时间\n- kv request type: execute 阶段每秒等待每种 KV 请求类型的时间，总的 KV request 等待时间可能超过 execute time，因为 KV request 是并发的。\n- tiflash_mpp: execute 阶段每秒 TiFlash 请求处理时间。\n\n绿色系标识代表常规的写 KV 请求（例如 Prewrite 和 Commit），蓝色系标识代表常规的读 KV 请求，紫色系标识代表 TiFlash MPP 请求，其他色系标识需要注意的问题。例如，悲观锁加锁请求为红色，TSO 等待为深褐色。如果非蓝色系或者非绿色系占比明显，意味着执行阶段存在异常的瓶颈。例如，当发生严重锁冲突时，红色的悲观锁时间会占比明显；当负载中 TSO 等待的消耗时间过长时，深褐色会占比明显。\n\n### QPS\n\nQPS：按 `SELECT`、`INSERT`、`UPDATE` 等类型统计所有 TiDB 实例上每秒执行的 SQL 语句数量\n\n### CPS By Type\n\nCPS By Type：按照类型统计所有 TiDB 实例每秒处理的命令数（Command Per Second）\n\n### Queries Using Plan Cache OPS\n\n- avg-hit：所有 TiDB 实例每秒执行计划缓存的命中次数\n- avg-miss：所有 TiDB 实例每秒执行计划缓存的未命中次数\n\n`avg-hit + avg-miss` 等于 StmtExecute 每秒执行次数。\n\n### KV/TSO Request OPS\n\n- kv request total: 所有 TiDB 实例每秒总的 KV 请求数量\n- kv request by type: 按 `Get`、`Prewrite`、 `Commit` 等类型统计在所有 TiDB 实例每秒的请求数据\n- tso - cmd：所有 TiDB 实例每秒发送的 gRPC 请求的数量，每个 gRPC 请求包含一批 (batch) TSO 请求\n- tso - request：所有 TiDB 实例每秒的 TSO 请求数量\n\n通常 tso - request 除以 tso - cmd 等于 TSO 请求 batch 的平均大小。\n\n### KV Request Time By Source\n\n- kv request total time: 所有 TiDB 实例每秒总的 KV 和 TiFlash 请求处理时间。\n\n- 每种 KV 请求和请求来源组成柱状堆叠图，`external` 标识正常业务的请求，`internal` 标识内部活动的请求（比如 DDL、auto analyze 等请求）。\n\n### TiDB CPU\n\n- avg：所有 TiDB 实例平均 CPU 利用率\n- delta：所有 TiDB 实例中最大 CPU 利用率减去所有 TiDB 实例中最小 CPU 利用率\n- max：所有 TiDB 实例中最大 CPU 利用率\n\n### TiKV CPU/IO MBps\n\n- CPU-Avg：所有 TiKV 实例平均 CPU 利用率\n- CPU-Delta：所有 TiKV 实例中最大 CPU 利用率减去所有 TiKV 实例中最小 CPU 利用率\n- CPU-MAX：所有 TiKV 实例中最大 CPU 利用率\n- IO-Avg：所有 TiKV 实例平均 MBps\n- IO-Delta：所有 TiKV 实例中最大 MBps 减去所有 TiKV 实例中最小 MBps\n- IO-MAX：所有 TiKV 实例中最大 MBps\n\n### Duration\n\n- Duration：执行时间解释\n\n    - 从客户端网络请求发送到 TiDB，到 TiDB 执行结束后返回给客户端的时间。一般情况下，客户端请求都是以 SQL 语句的形式发送，但也可以包含 `COM_PING`、`COM_SLEEP`、`COM_STMT_FETCH`、`COM_SEND_LONG_DATA` 之类的命令执行时间。\n    - 由于 TiDB 支持 Multi-Query，因此，客户端可以一次性发送多条 SQL 语句，如 `select 1; select 1; select 1;`。此时的执行时间是所有 SQL 语句执行完成的总时间。\n\n- avg：所有请求命令的平均执行时间\n- 99： 所有请求命令的 P99 执行时间\n- avg by type：按 `SELECT`、`INSERT`、`UPDATE` 类型统计所有 TiDB 实例上所有请求命令的平均执行时间\n\n### Connection Idle Duration\n\nConnection Idle Duration 指空闲连接的持续时间。\n\n- avg-in-txn：处于事务中，空闲连接的平均持续时间\n- avg-not-in-txn：没有处于事务中，空闲连接的平均持续时间\n- 99-in-txn：处于事务中，空闲连接的 P99 持续时间\n\n### Connection Count\n\n- total：所有 TiDB 节点的总连接数\n- active connections：所有 TiDB 节点的总活跃连接数\n- tidb-{node-number}-peer：各个 TiDB 节点的连接数\n- disconnection/s：集群每秒断开连接的数量\n- 99-not-in-txn：没有处于事务中，空闲连接的 P99 持续时间\n\n### Parse Duration、Compile Duration 和 Execute Duration\n\n- Parse Duration：SQL 语句解析耗时统计\n- Compile Duration：将解析后的 SQL AST 编译成执行计划的耗时\n- Execution Duration：执行 SQL 语句执行计划耗时\n\n这三个时间指标均包含均所有 TiDB 实例的平均值和 P99 值。\n\n### Avg TiDB KV Request Duration\n\n按 `Get`、`Prewrite`、 `Commit` 等类型统计在所有 TiDB 实例 KV 请求的平均执行时间。\n\n### Avg TiKV GRPC Duration\n\n按 `get`、`kv_prewrite`、 `kv_commit` 等类型统计所有 TiKV 实例对 gRPC 请求的平均执行时间。\n\n### PD TSO Wait/RPC Duration\n\n- wait - avg：所有 TiDB 实例等待从 PD 返回 TSO 的平均时间\n- rpc - avg：所有 TiDB 实例从向 PD 发送获取 TSO 的 gRPC 请求到接收到 TSO 的平均耗时\n- wait - 99：所有 TiDB 实例等待从 PD 返回 TSO 的 P99 时间\n- rpc - 99：所有 TiDB 实例从向 PD 发送获取 TSO 的 gRPC 请求到接收到 TSO 的 P99 耗时\n\n### Storage Async Write Duration、Store Duration 和 Apply Duration\n\n- Storage Async Write Duration：异步写所花费的时间\n- Store Duration：异步写 Store 步骤所花费的时间\n- Apply Duration：异步写 Apply 步骤所花费的时间\n\n这三个时间指标都包含所有 TiKV 实例的平均值和 P99 值\n\n平均 Storage async write duration = 平均 Store Duration + 平均 Apply Duration\n\n### Append Log Duration、Commit Log Duration 和 Apply Log Duration\n\n- Append Log Duration：Raft append 日志所花费的时间\n- Commit Log Duration：Raft commit 日志所花费的时间\n- Apply Log Duration：Raft apply 日志所花费的时间\n\n这三个时间指标均包含所有 TiKV 实例的平均值和 P99 值。\n\n### 图例\n\n![performance overview](/media/performance/grafana_performance_overview.png)\n\n## TiFlash\n\n- CPU：每个 TiFlash 实例 CPU 的使用率\n- Memory：每个 TiFlash 实例内存的使用情况\n- IO utilization：每个 TiFlash 实例的 IO 使用率\n- MPP Query count：每个 TiFlash 实例每秒 MPP 查询数量\n- Request QPS：所有 TiFlash 实例收到的 coprocessor 请求数量。\n\n    - `batch`：batch 请求数量\n    - `batch_cop`：batch 请求中的 coprocessor 请求数量\n    - `cop`：直接通过 coprocessor 接口发送的 coprocessor 请求数量\n    - `cop_dag`：所有 coprocessor 请求中 dag 请求数量\n    - `super_batch`：开启 super batch 特性的请求数量\n- Executor QPS：所有 TiFlash 实例收到的请求中，每种 dag 算子的数量，其中 `table_scan` 是扫表算子，`selection` 是过滤算子，`aggregation` 是聚合算子，`top_n` 是 TopN 算子，`limit` 是 limit 算子\n- Request Duration Overview：每秒所有 TiFlash 实例所有请求类型总处理时间的堆叠图\n- Request Duration：所有 TiFlash 实例每种 MPP 和 coprocessor 请求类型的总处理时间，此时间为接收到该 coprocessor 请求至请求应答完毕的时间，包含平均和 P99 处理延迟\n- Request Handle Duration：所有 TiFlash 实例每种 MPP 和 coprocessor 请求的处理时间，此时间为该 coprocessor 请求从开始执行到结束的时间，包含平均和 P99 延迟\n- Raft Wait Index Duration：所有 TiFlash 实例在进行 wait_index 消耗的时间，即拿到 read_index 请求后，等待本地的 Region index >= read_index 所花费的时间\n- Raft Batch Read Index Duration：所有 TiFlash 实例在进行 read_index 消耗的时间，主要消耗在于和 Region leader 的交互和重试时间\n- Write Throughput By Instance：每个实例写入数据的吞吐量，包括 apply Raft 数据日志以及 Raft 快照的写入吞吐量\n- Write flow：所有 TiFlash 实例磁盘写操作的流量\n- Read flow：所有 TiFlash 实例磁盘读操作的流量\n\n## CDC\n\n- CPU usage：TiCDC 节点的 CPU 使用情况\n- Memory usage：TiCDC 节点的内存使用情况\n- Goroutine count：TiCDC 节点 Goroutine 的个数\n- Changefeed checkpoint lag：同步任务上下游数据的进度差（以时间单位秒计算）\n- Changefeed resolved ts lag：TiCDC 节点内部同步状态与上游的进度差（以时间单位秒计算）\n- The status of changefeeds：changefeed 的状态\n\n    - 0：Normal\n    - 1：Error\n    - 2：Failed\n    - 3：Stopped\n    - 4：Finished\n    - -1：Unknown\n- Puller output events/s：TiCDC 节点中 Puller 模块每秒输出到 Sorter 模块的数据变更行数\n- Sorter output events/s：TiCDC 节点中 Sorter 模块每秒输出到 Mounter 模块的行数\n- Mounter output events/s：TiCDC 节点中 Mounter 模块每秒输出到 Sink 模块的行数\n- Table sink output events/s：TiCDC 节点中 Table Sorter 模块每秒输出到 Sink 模块的行数\n- SinkV2 - Sink flush rows/s：TiCDC 节点中 Sink 模块每秒输出到下游的行数\n- Transaction Sink Full Flush Duration：TiCDC 节点中 MySQL Sink 写下游事务的平均延迟和 p999 延迟\n- MQ Worker Send Message Duration Percentile：下游为 Kafka 时 MQ worker 发送消息的延迟\n- Kafka Outgoing Bytes：MQ Workload 写下游事务的流量\n"
        },
        {
          "name": "grafana-resource-control-dashboard.md",
          "type": "blob",
          "size": 5.634765625,
          "content": "---\ntitle: 资源管控 (Resource Control) 监控指标详解\nsummary: 了解资源管控 (Resource Control) 的 Grafana Dashboard 中所展示的关键指标。\n---\n\n# 资源管控 (Resource Control) 监控指标详解\n\n使用 TiUP 部署 TiDB 集群时，可以一键部署监控系统 (Prometheus & Grafana)。监控架构请参见 [TiDB 监控框架概述](/tidb-monitoring-framework.md)。\n\n目前 Grafana Dashboard 整体分为 PD、TiDB、TiKV、Node_exporter、Overview、Performance_overview 等。\n\n如果你的集群配置了 [Resource Control](/tidb-resource-control.md) ，通过观察 Resource Control 面板上的 Metrics，你可以了解当前集群整体的资源消耗状态。\n\nTiDB 使用[令牌桶算法](https://en.wikipedia.org/wiki/Token_bucket) 做流控，正如资源管控实现机制 ([RFC: Global Resource Control in TiDB](https://github.com/pingcap/tidb/blob/master/docs/design/2022-11-25-global-resource-control.md#distributed-token-buckets)) 中所描述：一个 TiDB 节点可能存在多个 Resource Group（资源组），将在 PD 端的 GAC（Global Admission Control）进行流控。每个 TiDB 节点中的本地令牌桶（Local Token Buckets）将定期（默认 5 秒）与 PD 端的 GAC 进行通信，以重新配置本地令牌。其中的本地令牌桶（Local Token Buckets）具体实现为 Resource Controller Client。\n\n以下为 **Resource Control** 关键监控指标的说明。\n\n## Request Unit 相关指标\n\n- RU：以 Resource Group（资源组）为单位进行实时统计的 [Request Unit (RU)](/tidb-resource-control.md#什么是-request-unit-ru) 消耗信息。`total` 为当前所有 Resource Group 消耗的 Request Unit 之和。每个 Resource Group 的 Request Unit 消耗等于其读消耗 (Read Request Unit) 和写消耗 (Write Request Unit) 之和。\n- RU Per Query：平均每个 SQL 语句消耗的 Request Unit 数量。计算方法是将前述 Request Unit 监控指标除以当前每秒执行的 SQL 语句数量。\n- RRU：以 Resource Group 为单位进行实时统计的读请求 Read Request Unit 消耗信息。`total` 为当前所有 Resource Group 消耗的 Read Request Unit 之和。\n- RRU Per Query：平均每个 SQL 语句消耗的 Read Request Unit 数量。计算方法是将前述 Read Request Unit 监控指标除以当前每秒执行的 SQL 语句数量。\n- WRU：以 Resource Group 为单位进行实时统计的写请求 Write Request Unit 消耗信息。`total` 为当前所有 Resource Group 消耗的 Write Request Unit 之和。\n- WRU Per Query：平均每个 SQL 语句消耗的 Write Request Unit 数量。计算方法是将前述 Write Request Unit 监控指标除以当前每秒执行的 SQL 语句数量。\n- Available RU：以 Resource Group 为单位显示 RU 令牌桶内可用的 token。当指标为 0 时，该 Resource Group 将以 `RU_PER_SEC` 指定的速度消耗 token，可以认为处于限速状态。\n- Query Max Duration：以 Resource Group 为单位统计的最大 Query Duration。\n\n## Resource 相关指标\n\n- KV Request Count：以 Resource Group（资源组）为单位进行实时统计的 KV 请求数量，区分了读和写两种类型。`total` 为当前所有 Resource Group 涉及的 KV 请求数量之和。\n- KV Request Count Per Query：平均每个 SQL 语句涉及的读写 KV 请求数量。计算方法是将前述 KV Request Count 监控指标除以当前每秒执行的 SQL 语句数量。\n- Bytes Read：以 Resource Group 为单位进行实时统计的读取数据量。`total` 为当前所有 Resource Group 读取数据量之和。\n- Bytes Read Per Query：平均每个 SQL 语句的读取数据量。将前述 Bytes Read 监控指标除以当前每秒执行的 SQL 语句数量。\n- Bytes Written：以 Resource Group 为单位进行实时统计的写入数据量。`total` 为当前所有 Resource Group 写入数据量之和。\n- Bytes Written Per Query：平均每个 SQL 语句的写入数据量。计算方法是将前述 Bytes Written 监控指标除以当前每秒执行的 SQL 语句数量。\n- KV CPU Time：以 Resource Group 为单位进行实时统计的 KV 层 CPU 时间消耗。`total` 为当前所有 Resource Group 消耗 KV 层 CPU 时间之和。\n- SQL CPU Time：以 Resource Group 为单位进行实时统计的 SQL 层 CPU 时间消耗。`total` 为当前所有 Resource Group 消耗 SQL 层 CPU 时间之和。\n\n## Resource Controller Client 相关指标\n\n- Active Resource Groups：实时统计各个 Resource Controller Client 的 Resource Groups 数量。\n- Total KV Request Count：以 Resource Group 为单位，实时统计各个 Resource Controller Client 的 KV 请求数量。`total` 为 Resource Controller Client 下 KV 请求数量之和。\n- Failed KV Request Count：以 Resource Group 为单位，实时统计各个 Resource Controller Client 的 KV 失败请求数量。`total` 为 Resource Controller Client 下 KV 失败请求数量之和。\n- Successful KV Request Count：以 Resource Group 为单位，实时统计各个 Resource Controller Client 的 KV 成功请求数量。`total` 为 Resource Controller Client 下 KV 成功请求数量之和。\n- Successful KV Request Wait Duration (99/90)：以 Resource Group 为单位，实时统计各个 Resource Controller Client 成功 KV 请求的等待时间（不同百分位）。\n- Token Request Handle Duration (999/99)：以 Resource Group 为单位，实时统计各个 Resource Controller Client 向 Server 端申请 Token 等待的响应时间（不同百分位）。\n- Token Request Count：以 Resource Group 为单位，实时统计各个 Resource Controller Client 向 Server 端申请 Token 的次数。`successful` 和 `failed` 分别为 Resource Controller Client 下申请 Token 成功和失败数量之和。\n"
        },
        {
          "name": "grafana-tidb-dashboard.md",
          "type": "blob",
          "size": 13.6943359375,
          "content": "---\ntitle: TiDB 监控指标\nsummary: 了解 Grafana Dashboard 中展示的关键指标。\naliases: ['/docs-cn/dev/grafana-tidb-dashboard/','/docs-cn/dev/reference/key-monitoring-metrics/tidb-dashboard/']\n---\n\n# TiDB 重要监控指标详解\n\n使用 TiUP 部署 TiDB 集群时，你可以一键部署监控系统 (Prometheus & Grafana)，参考监控架构 [TiDB 监控框架概述](/tidb-monitoring-framework.md)。\n\n目前 Grafana Dashboard 整体分为 PD、TiDB、TiKV、Node\\_exporter、Overview、Performance\\_overview 等。TiDB 分为 TiDB 和 TiDB Summary 面板，两个面板的区别如下：\n\n- TiDB 面板：提供尽可能全面的信息，供排查集群异常。\n- TiDB Summary 面板：将 TiDB 面板中用户最为关心的部分抽取出来，并做了些许修改。主要用于提供数据库日常运行中用户关心的数据，如 QPS、TPS、响应延迟等，以便作为外部展示、汇报用的监控信息。\n\n以下为 **TiDB Dashboard** 关键监控指标的说明：\n\n## 关键指标说明\n\n### Query Summary\n\n- Duration：执行时间\n    - 客户端网络请求发送到 TiDB，到 TiDB 执行结束后返回给客户端的时间。一般情况下，客户端请求都是以 SQL 语句的形式发送，但也可以包含 `COM_PING`、`COM_SLEEP`、`COM_STMT_FETCH`、`COM_SEND_LONG_DATA` 之类的命令执行时间。\n    - 由于 TiDB 支持 Multi-Query，因此，客户端可以一次性发送多条 SQL 语句，如 `select 1; select 1; select 1;`。此时的执行时间是所有 SQL 语句执行完之后的总时间。\n- Command Per Second：TiDB 按照执行结果成功或失败来统计每秒处理的命令数。\n- QPS：按 `SELECT`、`INSERT`、`UPDATE` 类型统计所有 TiDB 实例上每秒执行的 SQL 语句数量。\n- CPS By Instance：按照命令和执行结果成功或失败来统计每个 TiDB 实例上的命令。\n- Failed Query OPM：每个 TiDB 实例上，对每分钟执行 SQL 语句发生的错误按照错误类型进行统计（例如语法错误、主键冲突等）。包含了错误所属的模块和错误码。\n- Slow query：慢查询的处理时间（整个慢查询耗时、Coprocessor 耗时、Coprocessor 调度等待时间），慢查询分为 internal 和 general SQL 语句。\n- Connection Idle Duration：空闲连接的持续时间。\n- 999/99/95/80 Duration：不同类型的 SQL 语句执行耗时（不同百分位）。\n\n### Query Detail\n\n- Duration 80/95/99/999 By Instance：每个 TiDB 实例执行 SQL 语句的耗时（不同百分位）。\n- Failed Query OPM Detail：每个 TiDB 实例上，对每分钟执行 SQL 语句发生的错误按照错误类型进行统计（例如语法错误、主键冲突等）。\n- Internal SQL OPS：整个 TiDB 集群内部 SQL 语句执行的 QPS。内部 SQL 语句是指 TiDB 内部自动执行的 SQL 语句，一般由用户 SQL 语句来触发或者内部定时任务触发。\n\n### Server\n\n- Uptime：每个 TiDB 实例的运行时间。\n- Memory Usage：每个 TiDB 实例的内存使用，分为进程占用内存和 Golang 在堆上申请的内存。\n- CPU Usage：每个 TiDB 实例的 CPU 使用。\n- Connection Count：每个 TiDB 的连接数。\n- Open FD Count：每个 TiDB 实例的打开的文件描述符数量。\n- Disconnection Count：每个 TiDB 实例断开连接的数量。\n- Event OPM：每个 TiDB 实例关键事件，例如 start，close，graceful-shutdown，kill，hang 等。\n- Goroutine Count：每个 TiDB 实例的 Goroutine 数量。\n- Prepare Statement Count：每个 TiDB 实例现存的 `Prepare` 语句数以及总数。\n- Keep Alive OPM：每个 TiDB 实例每分钟刷新监控的次数，通常不需要关注。\n- Panic And Critical Error：TiDB 中出现的 Panic、Critical Error 数量。\n- Time Jump Back OPS：每个 TiDB 实例上每秒操作系统时间回跳的次数。\n- Get Token Duration：每个连接获取 Token 的耗时。\n- Skip Binlog Count：TiDB 写入 Binlog 失败的数量。从 v8.4.0 开始，TiDB Binlog 已移除，该指标不再有计数。\n- Client Data Traffic：TiDB 和客户端的数据流量。\n\n### Transaction\n\n- Transaction OPS：每秒事务的执行数量\n- Duration：事务执行时间\n- Transaction Statement Num：事务中的 SQL 语句数量\n- Transaction Retry Num：事务重试次数\n- Session Retry Error OPS：事务重试时每秒遇到的错误数量，分为重试失败和超过最大重试次数两种类型\n- Commit Token Wait Duration：事务提交时的流控队列等待时间。当出现较长等待时，代表提交事务过大，正在限流。如果系统还有资源可以使用，可以通过增大系统变量 `tidb_committer_concurrency` 的值来加速提交\n- KV Transaction OPS：每个 TiDB 内部每秒执行的事务数量\n    - 一个用户的事务，在 TiDB 内部可能会触发多次事务执行，其中包含，内部元数据的读取，用户事务原子性地多次重试执行等\n    - TiDB 内部的定时任务也会通过事务来操作数据库，这部分也包含在这个面板里\n- KV Transaction Duration：每个 TiDB 内部执行事务的耗时\n- Transaction Regions Num：事务操作的 Region 数量\n- Transaction Write KV Num Rate and Sum：事务写入 KV 的速率总和\n- Transaction Write KV Num：事务操作的 KV 数量\n- Statement Lock Keys：单个语句的加锁个数\n- Send HeartBeat Duration：事务发送心跳的时间间隔\n- Transaction Write Size Bytes Rate and sum：事务写入字节数的速率总和\n- Transaction Write Size Bytes：事务写入的数据大小\n- Acquire Pessimistic Locks Duration：加锁所消耗的时间\n- TTL Lifetime Reach Counter：事务的 TTL 寿命上限。TTL 上限默认值 1 小时，它的含义是从悲观事务第一次加锁，或者乐观事务的第一个 prewrite 开始，超过了 1 小时。可以通过修改 TiDB 配置文件中 `max-txn-ttl` 来改变 TTL 寿命上限\n- Load Safepoint OPS：加载 Safepoint 的次数。Safepoint 作用是在事务读数据时，保证不读到 Safepoint 之前的数据，保证数据安全。因为，Safepoint 之前的数据有可能被 GC 清理掉\n- Pessimistic Statement Retry OPS：悲观语句重试次数。当语句尝试加锁时，可能遇到写入冲突，此时，语句会重新获取新的 snapshot 并再次加锁\n- Transaction Types Per Seconds：每秒采用两阶段提交 (2PC)、异步提交 （Async Commit) 和一阶段提交 (1PC) 机制的事务数量，提供成功和失败两种数量\n\n### Executor\n\n- Parse Duration：SQL 语句解析耗时统计。\n- Compile Duration：将解析后的 SQL AST 编译成执行计划的耗时。\n- Execution Duration：执行 SQL 语句执行计划耗时。\n- Expensive Executor OPS：每秒消耗系统资源比较多的算子。包括 Merge Join、Hash Join、Index Look Up Join、Hash Agg、Stream Agg、Sort、TopN 等。\n- Queries Using Plan Cache OPS：每秒使用 Plan Cache 的查询数量。\n- Plan Cache Miss OPS：每秒出现 Plan Cache Miss 的数量。\n- Plan Cache Memory Usage：每个 TiDB 实例上所有 Plan Cache 缓存的执行计划占用的总内存。\n- Plan Cache Plan Num：每个 TiDB 实例上所有 Plan Cache 缓存的执行计划总数。\n\n### Distsql\n\n- Distsql Duration：Distsql 处理的时长\n- Distsql QPS：每秒 Distsql 的数量\n- Distsql Partial QPS：每秒 Partial Results 的数量\n- Scan Keys Num：每个 Query 扫描的 Key 的数量\n- Scan Keys Partial Num：每一个 Partial Result 扫描的 Key 的数量\n- Partial Num：每个 SQL 语句 Partial Results 的数量\n\n### KV Errors\n\n- KV Backoff Duration：KV 每个请求重试的总时间。TiDB 向 TiKV 的请求都有重试机制，这里统计的是向 TiKV 发送请求时遇到错误重试的总时间\n- TiClient Region Error OPS：TiKV 返回 Region 相关错误信息的数量\n- KV Backoff OPS：TiKV 返回错误信息的数量\n- Lock Resolve OPS：TiDB 清理锁操作的数量。当 TiDB 的读写请求遇到锁时，会尝试进行锁清理\n- Other Errors OPS：其他类型的错误数量，包括清锁和更新 SafePoint\n\n### KV Request\n\n下面的监控指标与发送给 TiKV 的请求相关。重试请求会被多次计数。\n\n- KV Request OPS：KV Request 根据 TiKV 显示执行次数\n- KV Request Duration 99 by store：根据 TiKV 显示 KV Request 执行时间\n- KV Request Duration 99 by type：根据类型显示 KV Request 的执行时间\n- Stale Read Hit/Miss Ops\n    - **hit**：每秒成功执行 Stale Read 的请求数量\n    - **miss**：每秒尝试执行 Stale Read 但失败的请求数量\n- Stale Read Req Ops\n    - **cross-zone**：每秒尝试在远程可用区执行 Stale Read 的请求数量\n    - **local**：每秒尝试在本地可用区执行 Stale Read 的请求数量\n- Stale Read Req Traffic\n    - **cross-zone-in**：尝试在远程可用区执行 Stale Read 的请求的响应的传入流量\n    - **cross-zone-out**：尝试在远程可用区执行 Stale Read 的请求的响应的传出流量\n    - **local-in**：尝试在本地可用区执行 Stale Read 的请求的响应的传入流量\n    - **local-out**：尝试在本地可用区执行 Stale Read 的请求的响应的传出流量\n\n### PD Client\n\n- PD Client CMD OPS：PD Client 每秒执行命令的数量\n- PD Client CMD Duration：PD Client 执行命令耗时\n- PD Client CMD Fail OPS：PD Client 每秒执行命令失败的数量\n- PD TSO OPS：TiDB 每秒向 PD 发送获取 TSO 的 gRPC 请求的数量 (cmd) 和实际的 TSO 请求数量 (request)；每个 gRPC 请求包含一批 TSO 请求\n- PD TSO Wait Duration：TiDB 等待从 PD 返回 TSO 的时间\n- PD TSO RPC Duration：TiDB 从向 PD 发送获取 TSO 的 gRPC 请求到接收到 TSO gRPC 请求响应的耗时\n- Async TSO Duration：TiDB 从准备获取 TSO 到实际开始等待 TSO 返回的时间\n\n### Schema Load\n\n- Load Schema Duration：TiDB 从 TiKV 获取 Schema 的时间\n- Load Schema OPS：TiDB 从 TiKV 每秒获取 Schema 的数量\n- Schema Lease Error OPM：Schema Lease 出错统计，包括 change 和 outdate 两种，change 代表 schema 发生了变化，outdate 代表无法更新 schema，属于较严重错误，出现 outdate 错误时会报警\n- Load Privilege OPS：TiDB 从 TiKV 每秒获取权限信息的数量\n\n### DDL\n\n- DDL Duration 95：DDL 语句处理时间的 95% 分位\n- Batch Add Index Duration 100：创建索引时每个 Batch 所花费的最大时间\n- DDL Waiting Jobs Count：等待的 DDL 任务数量\n- DDL META OPM：DDL 每分钟获取 META 的次数\n- DDL Worker Duration 99：每个 DDL worker 执行时间的 99% 分位\n- Deploy Syncer Duration：Schema Version Syncer 初始化，重启，清空等操作耗时\n- Owner Handle Syncer Duration：DDL Owner 在执行更新，获取以及检查 Schema Version 的耗时\n- Update Self Version Duration：Schema Version Syncer 更新版本信息耗时\n- DDL OPM：DDL 语句的每秒执行次数\n- DDL backfill progress in percentage：backfill DDL 任务的进度展示\n\n### Statistics\n\n- Auto Analyze Duration 95：自动 ANALYZE 耗时\n- Auto Analyze QPS：自动 ANALYZE 数量\n- Stats Inaccuracy Rate：统计信息不准确度\n- Pseudo Estimation OPS：使用假的统计信息优化 SQL 的数量\n- Dump Feedback OPS：存储统计信息 Feedback 的数量\n- Store Query Feedback QPS：存储合并查询的 Feedback 信息的每秒操作数量，该操作在 TiDB 内存中进行\n- Significant Feedback：重要的 Feedback 更新统计信息的数量\n- Update Stats OPS：利用 Feedback 更新统计信息的数量\n\n### Owner\n\n- New ETCD Session Duration 95：创建一个新的 etcd 会话花费的时间。TiDB 通过 etcd client 连接 PD 中的 etcd 保存/读取部分元数据信息。这里记录了创建会话花费的时间\n- Owner Watcher OPS：DDL owner watch PD 的 etcd 的元数据的 goroutine 的每秒操作次数\n\n### Meta\n\n- AutoID QPS：AutoID 相关操作的数量统计，包括全局 ID 分配、单个 Table AutoID 分配、单个 Table AutoID Rebase 三种操作\n- AutoID Duration：AutoID 相关操作的耗时\n- Region Cache Error OPS：TiDB 缓存的 region 信息每秒遇到的错误次数\n- Meta Operations Duration 99：元数据操作延迟\n\n### GC\n\n- Worker Action OPM：GC 相关操作的数量，包括 run\\_job，resolve\\_lock，delete\\_range 等操作\n- Duration 99：GC 相关操作的耗时\n- Config：GC 的数据保存时长 (life time) 和 GC 运行间隔 (run interval) 配置\n- GC Failure OPM：GC 相关操作失败的数量\n- Delete Range Failure OPM：Delete range 失败的次数\n- Too Many Locks Error OPM：GC 清锁过多错误的数量\n- Action Result OPM：GC 相关操作结果数量\n- Delete Range Task Status：Delete range 的任务状态，包含完成和失败状态\n- Push Task Duration 95：将 GC 子任务推送给 GC worker 的耗时\n\n### Batch Client\n\n- Pending Request Count by TiKV：TiKV 批量消息处理的等待数量\n- Wait Duration 95: 批量消息处理的等待时间。\n- Batch Client Unavailable Duration 95：批处理客户端的不可用时长。\n- No Available Connection Counter：批处理客户端不可用的连接数。\n\n### TTL\n\n- TiDB CPU Usage: 每个 TiDB 实例的 CPU 使用。\n- TiKV IO MBps: 每个 TiKV 实例的 I/O 吞吐量。\n- TiKV CPU: 每个 TiKV 实例的 CPU 使用。\n- TTL QPS By Type：TTL 任务产生的不同类型语句的 QPS 信息。\n- TTL Insert Rows Per Second: 每秒钟向 TTL 表插入的数据行数。\n- TTL Processed Rows Per Second：TTL 任务每秒处理的过期数据的行数。\n- TTL Insert Rows Per Hour: 每小时总共向 TTL 表插入的行数。\n- TTL Delete Rows Per Hour: 每小时总共删除的过期行数。\n- TTL Scan/Delete Query Duration：TTL 的扫描/删除语句的执行时间。\n- TTL Scan/Delete Worker Time By Phase：TTL 内部工作线程的不同阶段所占用的时间。\n- TTL Job Count By Status：当前正在执行的 TTL 任务的数量。\n- TTL Task Count By Status：当前正在执行的 TTL 子任务的数量。\n"
        },
        {
          "name": "grafana-tikv-dashboard.md",
          "type": "blob",
          "size": 30.939453125,
          "content": "---\ntitle: TiKV 监控指标详解\naliases: ['/docs-cn/dev/grafana-tikv-dashboard/','/docs-cn/dev/reference/key-monitoring-metrics/tikv-dashboard/']\nsummary: TiKV 监控指标详解：TiUP 部署 TiDB 集群时，一键部署监控系统 (Prometheus & Grafana)，监控架构详见 TiDB 监控框架概述。Grafana Dashboard 分为 PD、TiDB、TiKV、Node_exporter、Overview、Performance_overview 等。对于日常运维，通过观察 TiKV-Details 面板上的指标，可以了解 TiKV 当前的状态。根据性能地图，可以检查集群的状态是否符合预期。TiKV-Details 默认的监控信息包括 Cluster、Errors、Server、gRPC、Thread CPU、PD、Raft IO、Raft process、Raft message、Raft propose、Raft admin、Local reader、Unified Read Pool、Storage、Flow Control、Scheduler 等。\n---\n\n# TiKV 监控指标详解\n\n使用 TiUP 部署 TiDB 集群时，一键部署监控系统 (Prometheus & Grafana)，监控架构参见 [TiDB 监控框架概述](/tidb-monitoring-framework.md)。\n\n目前 Grafana Dashboard 整体分为 PD、TiDB、TiKV、Node\\_exporter、Overview、Performance\\_overview 等。\n\n## TiKV-Details 面板\n\n对于日常运维，通过观察 **TiKV-Details** 面板上的指标，可以了解 TiKV 当前的状态。根据[性能地图](https://asktug.com/_/tidb-performance-map/#/)可以检查集群的状态是否符合预期。\n\n以下为 **TiKV-Details** 默认的监控信息：\n\n### Cluster\n\n- Store size：每个 TiKV 实例的使用的存储空间的大小\n- Available size：每个 TiKV 实例的可用的存储空间的大小\n- Capacity size：每个 TiKV 实例的存储容量的大小\n- CPU：每个 TiKV 实例 CPU 的使用率\n- Memory：每个 TiKV 实例内存的使用情况\n- IO utilization：每个 TiKV 实例 IO 的使用率\n- MBps：每个 TiKV 实例写入和读取的数据量大小\n- QPS：每个 TiKV 实例上各种命令的 QPS\n- Errps：每个 TiKV 实例上 gRPC 消息失败的速率\n- leader：每个 TiKV 实例 leader 的个数\n- Region：每个 TiKV 实例 Region 的个数\n- Uptime：自上次重启以来 TiKV 正常运行的时间\n\n![TiKV Dashboard - Cluster metrics](/media/tikv-dashboard-cluster.png)\n\n### Errors\n\n- Critical error：严重错误的数量\n- Server is busy：各种会导致 TiKV 实例暂时不可用的事件个数，如 write stall，channel full 等，正常情况下应当为 0\n- Server report failures：server 报错的消息个数，正常情况下应当为 0\n- Raftstore error：每个 TiKV 实例上 raftstore 发生错误的个数\n- Scheduler error：每个 TiKV 实例上 scheduler 发生错误的个数\n- Coprocessor error：每个 TiKV 实例上 coprocessor 发生错误的个数\n- gRPC message error：每个 TiKV 实例上 gRPC 消息发生错误的个数\n- Leader drop：每个 TiKV 实例上 drop leader 的个数\n- Leader missing：每个 TiKV 实例上 missing leader 的个数\n- Log Replication Rejected：每个 TiKV 实例上由于内存不足而拒绝 logappend 消息的个数\n\n![TiKV Dashboard - Errors metrics](/media/tikv-dashboard-errors-v610.png)\n\n### Server\n\n- CF size：每个列族的大小\n- Store size：每个 TiKV 实例的使用的存储空间的大小\n- Channel full：每个 TiKV 实例上 channel full 错误的数量，正常情况下应当为 0\n- Active written leaders：各个 TiKV 实例中正在被写入的 Leader 的数量\n- Approximate Region size：每个 Region 近似的大小\n- Approximate Region size Histogram：每个 Region 近似大小的直方图\n- Region average written keys：每个 TiKV 实例上所有 Region 的平均 key 写入个数\n- Region average written bytes：每个 TiKV 实例上所有 Region 的平均写入大小\n\n![TiKV Dashboard - Server metrics](/media/tikv-dashboard-server.png)\n\n### gRPC\n\n- gRPC message count：每种 gRPC 请求的速度\n- gRPC message failed：失败的 gRPC 请求的速度\n- 99% gRPC message duration：99% gRPC 请求的执行时间小于该值\n- Average gRPC message duration：gRPC 请求平均的执行时间\n- gRPC batch size：TiDB 与 TiKV 之间 grpc 请求的 batch 大小\n- raft message batch size：TiKV 与 TiKV 之间 raft 消息的 batch 大小\n- gRPC request sources QPS：不同 gRPC 请求来源的速度\n- gRPC request sources duration：不同 gRPC 请求来源的执行总时间\n- gRPC resource group QPS：不同 resource group 的 gRPC 请求速度\n\n### Thread CPU\n\n- Raft store CPU：raftstore 线程的 CPU 使用率，通常应低于 80% * `raftstore.store-pool-size`\n- Async apply CPU：async apply 线程的 CPU 使用率，通常应低于 90% * `raftstore.apply-pool-size`\n- Store writer CPU：async io 线程的 CPU 使用率，通常应低于 90% * `raftstore.store-io-pool-size`\n- gRPC poll CPU：gRPC 线程的 CPU 使用率，通常应低于 80% * `server.grpc-concurrency`\n- Scheduler worker CPU：scheduler worker 线程的 CPU 使用率，通常应低于 90% * `storage.scheduler-worker-pool-size`\n- Storage ReadPool CPU：storage read pool 线程的 CPU 使用率\n- Unified read pool CPU：unified read pool 线程的 CPU 使用率\n- RocksDB CPU：RocksDB 线程的 CPU 使用率\n- Coprocessor CPU：coprocessor 线程的 CPU 使用率\n- GC worker CPU：GC worker 线程的 CPU 使用率\n- BackGround worker CPU：background worker 线程的 CPU 使用率\n- Import CPU：Import 线程的 CPU 使用率\n- Backup Worker CPU：Backup 线程的 CPU 使用率\n- CDC Worker CPU：CDC Worker 线程的 CPU 使用率\n- CDC endpoint CPU：CDC endpoint 的 CPU 使用率\n- Raftlog fetch worker CPU：Async raft log fetcher worker 的 CPU 使用率\n- TSO Worker CPU: TSO Worker 线程的 CPU 使用率\n\n### PD\n\n- PD requests：TiKV 发送给 PD 的请求速度\n- PD request duration (average)：TiKV 发送给 PD 的请求处理的平均时间\n- PD heartbeats：发送给 PD 的心跳的速度\n- PD validate peers：TiKV 发送给 PD 用于验证 TiKV 的 peer 有效的消息的速度\n\n### Raft IO\n\n- Apply log duration：Raft apply 日志所花费的时间\n- Apply log duration per server：每个 TiKV 实例上 Raft apply 日志所花费的时间\n- Append log duration：Raft append 日志所花费的时间\n- Append log duration per server：每个 TiKV 实例上 Raft append 日志所花费的时间\n- Commit log duration：Raft commit 日志所花费的时间\n- Commit log duration per server：每个 TiKV 实例上 Raft commit 日志所花费的时间\n\n![TiKV Dashboard - Raft IO metrics](/media/tikv-dashboard-raftio.png)\n\n### Raft process\n\n- Ready handled：Raft 中不同 ready 类型的 ops\n    - count：批量处理 ready 的 ops\n    - has_ready_region：获得 ready 的 Region 的 ops\n    - pending_region：被检查是否获得 ready 的 Region 的 ops，v3.0.0 后废弃\n    - message：ready 内待发送 message 的 ops\n    - append：ready 内 Raft log entry 的 ops\n    - commit：ready 内 committed Raft log entry 的 ops\n    - snapshot：携带 snapshot 的 ready 的 ops\n- Max Duration of Raft store events：raftstore 处理事件最慢一次所花费的时间\n- Replica read lock checking duration：处理 Replica Read 时检查 lock 所花费的时间\n- Peer msg length distribution：每个 TiKV 中每个 region 一次性处理 Peer 消息的个数，消息越多说明 peer 越繁忙。\n\n![TiKV Dashboard - Raft process metrics](/media/tikv-dashboard-raft-process.png)\n\n### Raft message\n\n- Sent messages per server：每个 TiKV 实例发送 Raft 消息的 ops\n- Flush messages per server：每个 TiKV 实例中 raft client 往外 flush Raft 消息的 ops\n- Receive messages per server：每个 TiKV 实例接受 Raft 消息的 ops\n- Messages：发送不同类型的 Raft 消息的 ops\n- Vote：Raft 投票消息发送的 ops\n- Raft dropped messages：每秒钟丢弃不同类型的 Raft 消息的个数\n\n![TiKV Dashboard - Raft message metrics](/media/tikv-dashboard-raft-message.png)\n\n### Raft propose\n\n- Raft apply proposals per ready：在一个 batch 内，apply proposal 时每个 ready 中包含 proposal 的个数的直方图\n- Raft read/write proposals：不同类型的 proposal 的 ops\n- Raft read proposals per server：每个 TiKV 实例发起读 proposal 的 ops\n- Raft write proposals per server：每个 TiKV 实例发起写 proposal 的 ops\n- Propose wait duration：proposal 的等待时间的直方图\n- Propose wait duration per server：每个 TiKV 实例上每个 proposal 的等待时间的直方图\n- Apply wait duration：apply 的等待时间的直方图\n- Apply wait duration per server：每个 TiKV 实例上每个 apply 的等待时间的直方图\n- Raft log speed：peer propose 日志的平均速度\n\n![TiKV Dashboard - Raft propose metrics](/media/tikv-dashboard-raft-propose.png)\n\n### Raft admin\n\n- Admin proposals：admin proposal 的 ops\n- Admin apply：apply 命令的 ops\n- Check split：split check 命令的 ops\n- 99.99% Check split duration：99.99% 的情况下，split check 所需花费的时间\n\n![TiKV Dashboard - Raft admin metrics](/media/tikv-dashboard-raft-admin.png)\n\n### Local reader\n\n- Local reader requests：所有请求的总数以及 local read 线程拒绝的请求数量\n\n![TiKV Dashboard - Local reader metrics](/media/tikv-dashboard-local-reader.png)\n\n### Unified Read Pool\n\n- Time used by level：在 unified read pool 中每个级别使用的时间，级别 0 指小查询\n- Level 0 chance：在 unified read pool 中调度的 level 0 任务的比例\n- Running tasks：在 unified read pool 中并发运行的任务数量\n\n### Storage\n\n- Storage command total：收到不同命令的 ops\n- Storage async request error：异步请求出错的 ops\n- Storage async snapshot duration：异步处理 snapshot 所花费的时间，99% 的情况下，应该小于 1s\n- Storage async write duration：异步写所花费的时间，99% 的情况下，应该小于 1s\n\n![TiKV Dashboard - Storage metrics](/media/tikv-dashboard-storage.png)\n\n### Flow Control\n\n- Scheduler flow：每个 TiKV 实例的 scheduler 的实时流量\n- Scheduler discard ratio：每个 TiKV 实例的 scheduler 的请求拒绝比率。如果该比例大于 0，则表明存在流控。当 Compaction pending bytes 超过阈值时，TiKV 会根据超过阈值部分的值，按比例线性增加 Scheduler discard ratio。被拒绝的请求将自动由客户端重试\n- Throttle duration：L0 文件过多并触发流控后，scheduler 执行请求的阻塞时间。如果存在统计数据，则表明存在流控\n- Scheduler throttled CF：由于达到流控阈值，触发 RocksDB 限流的 CF\n- Flow controller actions：由于达到流控阈值，触发 RocksDB 限流的原因\n- Flush/L0 flow：每个 TiKV 实例上 RocksDB 的不同 CF 的 Flush 流量和 L0 compaction 的流量\n- Flow control factors：触发 RocksDB 限流相关的因素\n- Compaction pending bytes：每个 TiKV 实例上 RocksDB 实时等待 compaction 的数据的大小\n- Txn command throttled duration：由于限流，与事务相关的命令的阻塞时间。正常情况下，该指标为 0\n- Non-txn command throttled duration：由于限流，非事务相关的命令的阻塞时间。正常情况下，该指标为 0\n\n![TiKV Dashboard - Flow Control metrics](/media/tikv-dashboard-flow-control.png)\n\n### Scheduler\n\n- Scheduler stage total：每种命令不同阶段的 ops，正常情况下，不会在短时间内出现大量的错误\n- Scheduler writing bytes：每个 TiKV 实例正在处理的命令的写入字节数量\n- Scheduler priority commands：不同优先级命令的 ops\n- Scheduler pending commands：每个 TiKV 实例上 pending 命令的 ops\n\n![TiKV Dashboard - Scheduler metrics](/media/tikv-dashboard-scheduler.png)\n\n### Scheduler - commit\n\n- Scheduler stage total：commit 中每个命令所处不同阶段的 ops，正常情况下，不会在短时间内出现大量的错误\n- Scheduler command duration：执行 commit 命令所需花费的时间，正常情况下，应该小于 1s\n- Scheduler latch wait duration：由于 latch wait 造成的时间开销，正常情况下，应该小于 1s\n- Scheduler keys read：commit 命令读取 key 的个数\n- Scheduler keys written：commit 命令写入 key 的个数\n- Scheduler scan details：执行 commit 命令时，扫描每个 CF 中 key 的详细情况\n- Scheduler scan details [lock]：执行 commit 命令时，扫描每个 lock CF 中 key 的详细情况\n- Scheduler scan details [write]：执行 commit 命令时，扫描每个 write CF 中 key 的详细情况\n- Scheduler scan details [default]：执行 commit 命令时，扫描每个 default CF 中 key 的详细情况\n\n![TiKV Dashboard - Scheduler commit metrics](/media/tikv-dashboard-scheduler-commit.png)\n\n### Scheduler - pessimistic_rollback\n\n- Scheduler stage total：pessimistic_rollback 中每个命令所处不同阶段的 ops，正常情况下，不会在短时间内出现大量的错误\n- Scheduler command duration：执行 pessimistic_rollback 命令所需花费的时间，正常情况下，应该小于 1s\n- Scheduler latch wait duration：由于 latch wait 造成的时间开销，正常情况下，应该小于 1s\n- Scheduler keys read：pessimistic_rollback 命令读取 key 的个数\n- Scheduler keys written：pessimistic_rollback 命令写入 key 的个数\n- Scheduler scan details：执行 pessimistic_rollback 命令时，扫描每个 CF 中 key 的详细情况\n- Scheduler scan details [lock]：执行 pessimistic_rollback 命令时，扫描每个 lock CF 中 key 的详细情况\n- Scheduler scan details [write]：执行 pessimistic_rollback 命令时，扫描每个 write CF 中 key 的详细情况\n- Scheduler scan details [default]：执行 pessimistic_rollback 命令时，扫描每个 default CF 中 key 的详细情况\n\n### Scheduler - prewrite\n\n- Scheduler stage total：prewrite 中每个命令所处不同阶段的 ops，正常情况下，不会在短时间内出现大量的错误\n- Scheduler command duration：执行 prewrite 命令所需花费的时间，正常情况下，应该小于 1s\n- Scheduler latch wait duration：由于 latch wait 造成的时间开销，正常情况下，应该小于 1s\n- Scheduler keys read：prewrite 命令读取 key 的个数\n- Scheduler keys written：prewrite 命令写入 key 的个数\n- Scheduler scan details：执行 prewrite 命令时，扫描每个 CF 中 key 的详细情况\n- Scheduler scan details [lock]：执行 prewrite 命令时，扫描每个 lock CF 中 key 的详细情况\n- Scheduler scan details [write]：执行 prewrite 命令时，扫描每个 write CF 中 key 的详细情况\n- Scheduler scan details [default]：执行 prewrite 命令时，扫描每个 default CF 中 key 的详细情况\n\n### Scheduler - rollback\n\n- Scheduler stage total：rollback 中每个命令所处不同阶段的 ops，正常情况下，不会在短时间内出现大量的错误\n- Scheduler command duration：执行 rollback 命令所需花费的时间，正常情况下，应该小于 1s\n- Scheduler latch wait duration：由于 latch wait 造成的时间开销，正常情况下，应该小于 1s\n- Scheduler keys read：rollback 命令读取 key 的个数\n- Scheduler keys written：rollback 命令写入 key 的个数\n- Scheduler scan details：执行 rollback 命令时，扫描每个 CF 中 key 的详细情况\n- Scheduler scan details [lock]：执行 rollback 命令时，扫描每个 lock CF 中 key 的详细情况\n- Scheduler scan details [write]：执行 rollback 命令时，扫描每个 write CF 中 key 的详细情况\n- Scheduler scan details [default]：执行 rollback 命令时，扫描每个 default CF 中 key 的详细情况\n\n### GC\n\n- GC tasks：由 gc_worker 处理的 GC 任务的个数\n- GC tasks Duration：执行 GC 任务时所花费的时间\n- TiDB GC seconds：TiDB 执行 GC 花费的时间\n- TiDB GC worker actions：TiDB GC worker 的不同 action 的个数\n- TiKV AutoGC Working：Auto GC 管理器的工作状态\n- ResolveLocks Progress：GC 第一阶段 (ResolveLocks) 的进度\n- TiKV Auto GC Progress：GC 第二阶段的进度\n- GC speed：GC 每秒删除的 key 的数量\n- TiKV Auto GC SafePoint：TiKV GC 的 safe point 的数值，safe point 为当前 GC 的时间戳\n- GC lifetime：TiDB 设置的 GC lifetime\n- GC interval：TiDB 设置的 GC 间隔\n- GC in Compaction Filter：write CF 的 Compaction Filter 中已过滤版本的数量\n\n### Snapshot\n\n- Rate snapshot message：发送 Raft snapshot 消息的速率\n- 99% Handle snapshot duration：99% 的情况下，处理 snapshot 所需花费的时间\n- Snapshot state count：不同状态的 snapshot 的个数\n- 99.99% Snapshot size：99.99% 的 snapshot 的大小\n- 99.99% Snapshot KV count：99.99% 的 snapshot 包含的 key 的个数\n\n### Task\n\n- Worker handled tasks：worker 每秒钟处理的任务的数量\n- Worker pending tasks：当前 worker 中，每秒钟 pending 和 running 的任务的数量，正常情况下，应该小于 1000\n- FuturePool handled tasks：future pool 每秒钟处理的任务的数量\n- FuturePool pending tasks：当前 future pool 中，每秒钟 pending 和 running 的任务的数量\n\n### Coprocessor Overview\n\n- Request duration：从收到 coprocessor 请求到处理结束所消耗的总时间\n- Total Requests：每种类型的总请求的 ops\n- Handle duration：每分钟实际处理 coprocessor 请求所消耗的时间的直方图\n- Total Request Errors：Coprocessor 每秒请求错误的数量，正常情况下，短时间内不应该有大量的错误\n- Total KV Cursor Operations：各种类型的 KV cursor 操作的总数量的 ops，例如 select、index、analyze_table、analyze_index、checksum_table、checksum_index 等\n- KV Cursor Operations：每秒各种类型的 KV cursor 操作的数量，以直方图形式显示\n- Total RocksDB Perf Statistics：RocksDB 性能统计数据\n- Total Response Size：coprocessor 回应的数据大小\n\n### Coprocessor Detail\n\n- Handle duration：每秒钟实际处理 coprocessor 请求所消耗的时间的直方图\n- 95% Handle duration by store：每秒钟中 95% 的情况下，每个 TiKV 实例处理 coprocessor 请求所花费的时间\n- Wait duration：coprocessor 每秒钟内请求的等待时间，99.99% 的情况下，应该小于 10s\n- 95% Wait duration by store：每秒钟 95% 的情况下，每个 TiKV 实例上 coprocessor 请求的等待时间\n- Total DAG Requests：DAG 请求的总数量的 ops\n- Total DAG Executors：DAG executor 的总数量的 ops\n- Total Ops Details (Table Scan)：coprocessor 中请求为 select 的 scan 过程中每秒钟各种事件发生的次数\n- Total Ops Details (Index Scan)：coprocessor 中请求为 index 的 scan 过程中每秒钟各种事件发生的次数\n- Total Ops Details by CF (Table Scan)：coprocessor 中对于每个 CF 请求为 select 的 scan 过程中每秒钟各种事件发生的次数\n- Total Ops Details by CF (Index Scan)：coprocessor 中对于每个 CF 请求为 index 的 scan 过程中每秒钟各种事件发生的次数\n\n### Threads\n\n- Threads state：TiKV 线程的状态\n- Threads IO：TiKV 各个线程的 I/O 流量\n- Thread Voluntary Context Switches：TiKV 线程自主切换的次数\n- Thread Nonvoluntary Context Switches：TiKV 线程被动切换的次数\n\n### RocksDB - kv/raft\n\n- Get operations：get 操作的 ops\n- Get duration：get 操作的耗时\n- Seek operations：seek 操作的 ops\n- Seek duration：seek 操作的耗时\n- Write operations：write 操作的 ops\n- Write duration：write 操作的耗时\n- WAL sync operations：sync WAL 操作的 ops\n- Write WAL duration：write 操作中写 WAL 的耗时\n- WAL sync duration：sync WAL 操作的耗时\n- Compaction operations：compaction 和 flush 操作的 ops\n- Compaction duration：compaction 和 flush 操作的耗时\n- SST read duration：读取 SST 所需的时间\n- Write stall duration：由于 write stall 造成的时间开销，正常情况下应为 0\n- Memtable size：每个 CF 的 memtable 的大小\n- Memtable hit：memtable 的命中率\n- Block cache size：block cache 的大小。如果将 `shared block cache` 禁用，即为每个 CF 的 block cache 的大小\n- Block cache hit：block cache 的命中率\n- Block cache flow：不同 block cache 操作的流量\n- Block cache operations 不同 block cache 操作的个数\n- Keys flow：不同操作造成的 key 的流量\n- Total keys：每个 CF 中 key 的个数\n- Read flow：不同读操作的流量\n- Bytes/Read：每次读的大小\n- Write flow：不同写操作的流量\n- Bytes/Write：每次写的大小\n- Compaction flow：compaction 相关的流量\n- Compaction pending bytes：等待 compaction 的大小\n- Compaction Job Size(files)：单个 compaction 任务涉及的 SST 文件数量\n- Read amplification：每个 TiKV 实例的读放大\n- Compression ratio：每一层的压缩比\n- Number of snapshots：每个 TiKV 的 snapshot 的数量\n- Oldest snapshots duration：最旧的 snapshot 保留的时间\n- Number files at each level：每一层的文件个数\n- Ingest SST duration seconds：ingest SST 所花费的时间\n- Stall conditions changed of each CF：每个 CF stall 的原因\n\n### Raft Engine\n\n- Operations\n    - write：Raft Engine 每秒写操作的次数\n    - read_entry：Raft Engine 每秒读 raft 日志的次数\n    - read_message：Raft Engine 每秒读 raft 元数据的次数\n- Write duration：Raft Engine 写操作的耗时，该耗时基本接近写入这些数据所包含的磁盘 IO 的 latency 之和\n- Flow\n    - write：Raft Engine 写流量\n    - rewrite append：重写 append 日志的流量\n    - rewrite rewrite：重写 rewrite 日志的流量\n- Write Duration Breakdown (99%)\n    - wal：写 Raft Engine WAL 的延迟\n    - wait：写入前等待时间\n    - apply：apply 到内存的时间\n- Bytes/Written 每次写入对应的 bytes\n- WAL Duration Breakdown (P99%)：写 WAL 内部各个阶段所花的时间\n- File Count\n    - append：Raft Engine 用于 append 数据的文件个数\n    - rewrite：Raft Engine 用于 rewrite 的文件个数（rewrite 类似于 RocksDB 的 compaction）\n- Entry Count\n    - rewrite：Raft Engine 中已经 rewrite 的记录条数\n    - append：Raft Engine 中已经 append 的记录条数\n\n### Titan - All\n\n- Blob file count：Titan blob 文件的数量\n- Blob file size：Titan blob 文件总大小\n- Live blob size：有效 blob record 的总大小\n- Blob cache hit：Titan 的 blob cache 命中率\n- Iter touched blob file count：单个 Iterator 所涉及到 blob 文件的数量\n- Blob file discardable ratio distribution：blob 文件的失效 blob record 比例的分布情况\n- Blob key size：Titan 中 blob key 的大小\n- Blob value size：Titan 中 blob value 的大小\n- Blob get operations：blob 的 get 操作的数量\n- Blob get duration：blob 的 get 操作的耗时\n- Blob iter operations：blob 的 iter 操作的耗时\n- Blob seek duration：blob 的 seek 操作的耗时\n- Blob next duration：blob 的 next 操作的耗时\n- Blob prev duration：blob 的 prev 操作的耗时\n- Blob keys flow：Titan blob 读写的 key 数量\n- Blob bytes flow：Titan blob 读写的 bytes 数量\n- Blob file read duration：blob 文件的读取耗时\n- Blob file write duration：blob 文件的写入耗时\n- Blob file sync operations：blob 文件 sync 次数\n- Blob file sync duration：blob 文件 sync 耗时\n- Blob GC action：Titan GC 细分动作的次数\n- Blob GC duration：Titan GC 的耗时\n- Blob GC keys flow：Titan GC 读写的 key 数量\n- Blob GC bytes flow：Titan GC 读写的 bytes 数量\n- Blob GC input file size：Titan GC 输入文件的大小\n- Blob GC output file size：Titan GC 输出文件的大小\n- Blob GC file count：Titan GC 涉及的 blob 文件数量\n\n### Pessimistic Locking\n\n- Lock Manager Thread CPU：lock manager 的线程 CPU 使用率\n- Lock Manager Handled tasks：lock manager 处理的任务数量\n- Waiter lifetime duration：事务等待锁释放的时间\n- Wait table：wait table 的状态信息，包括锁的数量和等锁事务的数量\n- Deadlock detect duration：处理死锁检测请求的耗时\n- Detect error：死锁检测遇到的错误数量，包含死锁的数量\n- Deadlock detector leader：死锁检测器 leader 所在节点的信息\n- Total pessimistic locks memory size：内存悲观锁占用内存的总大小\n- In-memory pessimistic locking result：将悲观锁仅保存到内存的结果，其中 full 表示因为超过内存限制而无法将悲观锁保存至内存的次数\n\n### Resolved-TS\n\n- Resolved-TS worker CPU：resolved-ts worker 线程的 CPU 使用率\n- Advance-TS worker CPU：advance-ts worker 线程的 CPU 使用率\n- Scan lock worker CPU：scan lock worker 线程的 CPU 使用率\n- Max gap of resolved-ts：在当前 TiKV 中，所有活跃 Region 的 resolved-ts 与当前时间的最大差值\n- Max gap of safe-ts：在当前 TiKV 中，所有活跃 Region 的 safe-ts 与当前时间的最大差值\n- Min Resolved TS Region：resolved-ts 最小的 Region 的 ID\n- Min Safe TS Region：safe-ts 最小的 Region 的 ID\n- Check Leader Duration：处理 leader 请求所花费的时间的直方图，从发送请求到接收到 leader 的响应\n- Max gap of resolved-ts in Region leaders：在当前 TiKV 中，所有活跃 Region 的 resolved-ts 与当前时间的最大差值，只包含 Region leader\n- Min Leader Resolved TS Region：resolved-ts 最小的 Region 的 ID，只包含 Region leader\n- Lock heap size：resolved-ts 模块中用于跟踪锁的堆的大小\n\n### Memory\n\n- Allocator Stats：内存分配器的统计信息\n\n### Backup\n\n- Backup CPU：backup 的线程 CPU 使用率\n- Range Size：backup range 的大小直方图\n- Backup Duration：backup 的耗时\n- Backup Flow：backup 总的字节大小\n- Disk Throughput：实例磁盘的吞吐量\n- Backup Range Duration：backup range 的耗时\n- Backup Errors：backup 中发生的错误数量\n\n### Encryption\n\n- Encryption data keys：正在使用的加密 data key 的总数量\n- Encrypted files：被加密的文件数量\n- Encryption initialized：显示加密是否被启用，`1` 代表已经启用\n- Encryption meta files size：加密相关的元数据文件的大小\n- Encrypt/decrypt data nanos：每次加密/解密数据的耗时的直方图\n- Read/write encryption meta duration：每秒钟读写加密文件所耗费的时间\n\n### Log Backup\n\n- Handle Event Rate：处理写入事件的速度。\n- Initial Scan Generate Event Throughput：创建新的监听流时，增量扫描的速度。\n- Abnormal Checkpoint TS Lag：各个任务当前 Checkpoint TS 到现在时间的 Lag。\n- Memory Of Events：增量扫描产生的临时数据占用内存的估计值。\n- Observed Region Count：目前监听的 Region 数量。\n- Errors：可重试、非致命错误的数量及类型。\n- Fatal Errors：致命错误的数量及类型。通常致命错误会导致任务暂停。\n- Checkpoint TS of Tasks：各个任务的 Checkpoint TS。\n- Flush Duration：将缓存数据移动到外部存储的耗时的热力图。\n- Initial Scanning Duration：创建新的监听流时，增量扫描的耗时的热力图。\n- Convert Raft Event Duration：创建监听流后，转化 Raft 日志项为备份数据的耗时的热力图。\n- Command Batch Size：监听到的 Raft Command 的 Batch 大小（单个 Raft Group 内）。\n- Save to Temp File Duration：将一批备份数据（跨越数个 Task）暂存到临时文件区的耗时的热力图。\n- Write to Temp File Duration：将一批备份数据（来自某个 Task）暂存到临时文件区的耗时的热力图。\n- System Write Call Duration：将一批备份数据（来自某个 Region）写入到临时文件耗时的热力图。\n- Internal Message Type：TiKV 内部负责日志备份的 Actor 收到的消息的类型。\n- Internal Message Handling Duration (P90|P99)：消费、处理各个类型消息的速度。\n- Initial Scan RocksDB Throughput：增量扫描过程中，RocksDB 内部记录产生的读流量。\n- Initial Scan RocksDB Operation：增量扫描过程中，RocksDB 内部记录的各个操作的数量。\n- Initial Scanning Trigger Reason：触发增量扫描的原因。\n- Region Checkpoint Key Putting：向 PD 记录 Checkpoint 的操作的数量。\n\n> **注意：**\n>\n> 以下这些监控指标的数据源都是 TiDB 节点，但是对日志备份流程有一些影响。因此，为了方便查阅，将其放在了 **TiKV Details** 面板中。大部分时候 TiKV 会主动“推送”进度，但以下部分监控偶尔没有数据采样也属于正常现象。\n\n- Request Checkpoint Batch Size：日志备份协调器请求各个 TiKV 的 Checkpoint 信息时的请求攒批大小。\n- Tick Duration \\[P99|P90\\]：协调器内部 Tick 的耗时。\n- Region Checkpoint Failure Reason：协调器内部无法推进某个 Region Checkpoint 的原因。\n- Request Result：协调器推进 Region Checkpoint 的成功或失败的记录。\n- Get Region Operation Count：协调器向 PD 请求 Region 信息的次数。\n- Try Advance Trigger Time：协调器尝试推进 Checkpoint 的耗时。\n\n### 面板常见参数的解释\n\n#### gRPC 消息类型\n\n1. 使用事务型接口的命令：\n\n    - kv_get：事务型的 get 命令，获取指定 ts 能读到的最新版本数据\n    - kv_scan：扫描连续的一段数据\n    - kv_prewrite：2PC 的第一阶段，预写入事务要提交的数据\n    - kv_pessimistic_lock：对 key 加悲观锁，防止其他事务修改\n    - kv_pessimistic_rollback：删除 key 上的悲观锁\n    - kv_txn_heart_beat：更新悲观事务或大事务的 `lock_ttl` 以防止其被回滚\n    - kv_check_txn_status：检查事务的状态\n    - kv_commit：2PC 的第二阶段，提交 prewrite 阶段写入的数据\n    - kv_cleanup：回滚一个事务（此命令将会在 4.0 中废除）\n    - kv_batch_get：与 `kv_get` 类似，一次性获取批量 key 的 value\n    - kv_batch_rollback：批量回滚多个预写的事务\n    - kv_scan_lock：扫描所有版本号在 `max_version` 之前的锁，用于清理过期的事务\n    - kv_resolve_lock：根据事务状态，提交或回滚事务的锁\n    - kv_gc：触发垃圾回收\n    - kv_delete_range：从 TiKV 中删除连续的一段数据\n\n2. 非事务型的裸命令：\n\n    - raw_get：获取 key 所对应的 value\n    - raw_batch_get：获取一批 key 所对应的 value\n    - raw_scan：扫描一段连续的数据\n    - raw_batch_scan：扫描多段连续的数据\n    - raw_put：写入一个 key/value 对\n    - raw_batch_put：直接写入一批 key/value 对\n    - raw_delete：删除一个 key/value 对\n    - raw_batch_delete：删除一批 key/value 对\n    - raw_delete_range：删除连续的一段区间\n\n## TiKV-FastTune 面板\n\n当 TiKV 出现 QPS 抖动、延迟抖动、延迟增加趋势等性能问题时，你可以查看 **TiKV-FastTune** 面板。**TiKV-FastTune** 包括多组子面板，可帮助你诊断性能问题，尤其适用于集群中写入负载较大的场景。\n\n当出现写入相关的性能问题时，可以先在 Grafana 中查看 TiDB 相关的面板。如果问题出在存储端，打开 **TiKV-FastTune** 面板，浏览并检查上面的每个指标。\n\n在 **TiKV-FastTune** 的面板中，指标标题描述了性能问题的可能成因。要验证成因是否正确，你需要检查具体的图表曲线。\n\n左边 Y 轴表示存储端的 write-RPC QPS，右边 Y 轴上的一组图是倒置绘制的。如果左边 Y 轴的曲线形状与右边的形状匹配，则指标标题描述的问题成因是正确的。\n\n有关该面板的具体监控项以及解释，参考 [TiKV-FastTune 用户手册（英文）](https://docs.google.com/presentation/d/1aeBF2VCKf7eo4-3TMyP7oPzFWIih6UBA53UI8YQASCQ/edit#slide=id.gab6b984c2a_1_352)。\n"
        },
        {
          "name": "hardware-and-software-requirements.md",
          "type": "blob",
          "size": 14.7177734375,
          "content": "---\ntitle: TiDB 软件和硬件环境需求\naliases: ['/docs-cn/dev/hardware-and-software-requirements/','/docs-cn/dev/how-to/deploy/hardware-recommendations/']\nsummary: TiDB 是一款开源的一站式实时 HTAP 数据库，支持部署在多种硬件环境和操作系统上。软件和硬件环境需求包括操作系统要求、编译和运行依赖库、Docker 镜像依赖、软件配置要求、服务器配置要求、网络要求、磁盘空间要求、客户端 Web 浏览器要求以及 TiFlash 存算分离架构的软硬件要求。\n---\n\n# TiDB 软件和硬件环境需求\n\n<!-- Localization note for TiDB:\n\n- 英文：用 distributed SQL，同时开始强调 HTAP\n- 中文：可以保留 NewSQL 字眼，同时强调一栈式实时 HTAP\n- 日文：NewSQL 认可度高，用 NewSQL\n\n-->\n\n本文介绍 TiDB 数据库对软件和硬件环境的需求。TiDB 作为一款开源一栈式实时 HTAP 数据库，可以很好地部署和运行在 Intel 架构服务器环境、ARM 架构的服务器环境及主流虚拟化环境，并支持绝大多数的主流硬件网络。作为一款高性能数据库系统，TiDB 支持主流的 Linux 操作系统环境。\n\n## 操作系统及平台要求\n\n|  操作系统   |   支持的 CPU 架构   |\n|   :---   |   :---   |\n| Red Hat Enterprise Linux 8.4 及以上的 8.x 版本  |  <ul><li>x86_64</li><li>ARM 64</li></ul>  |\n|  Amazon Linux 2         |  <ul><li>x86_64</li><li>ARM 64</li></ul>   |\n|  Amazon Linux 2023      |  <ul><li>x86_64</li><li>ARM 64</li></ul>   |\n|  Rocky Linux 9.1 及以上的版本 |  <ul><li>x86_64</li><li>ARM 64</li></ul> |\n| 麒麟欧拉版 V10 SP1/SP2/SP3（从 v7.5.5 开始支持 SP3）   |   <ul><li>x86_64</li><li>ARM 64</li></ul>   |\n| 统信操作系统 (UOS) V20                 |   <ul><li>x86_64</li><li>ARM 64</li></ul>   |\n| openEuler 22.03 LTS SP1/SP3 |   <ul><li>x86_64</li><li>ARM 64</li></ul>   |\n| macOS 12 (Monterey) 及以上的版本 |  <ul><li>x86_64</li><li>ARM 64</li></ul>  |\n|  Oracle Enterprise Linux 8 及以上的版本  |  x86_64           |\n|   Ubuntu LTS 20.04 及以上的版本  |  x86_64           |\n| CentOS 8 Stream | <ul><li>x86_64</li><li>ARM 64</li></ul> |\n|  Debian 10 (Buster) 及以上的版本  |  x86_64           |\n|  Fedora 38 及以上的版本   |  x86_64           |\n|  openSUSE Leap 15.5 以上的版本（不包含 Tumbleweed） |  x86_64           |\n|  SUSE Linux Enterprise Server 15  |  x86_64                        |\n\n> **警告：**\n>\n> - 根据 [CentOS Linux EOL](https://www.centos.org/centos-linux-eol/)，CentOS Linux 7 的上游支持已于 2024 年 6 月 30 日终止。从 v8.4.0 版本开始，TiDB 已结束对 CentOS 7 的支持，建议使用 Rocky Linux 9.1 及以上的版本。如果将运行在 CentOS 7 上的 TiDB 集群升级到 v8.4.0 或之后版本，将导致集群不可用。升级 TiDB 前，请务必检查你的操作系统版本。\n> - 根据 [Red Hat Enterprise Linux Life Cycle](https://access.redhat.com/support/policy/updates/errata/#Life_Cycle_Dates)，Red Hat Enterprise Linux 7 的 Maintenance Support 已于 2024 年 6 月 30 日终止。从 v8.4.0 版本开始，TiDB 已结束对 Red Hat Enterprise Linux 7 的支持，建议使用 Rocky Linux 9.1 及以上的版本。如果将运行在 Red Hat Enterprise Linux 7 上的 TiDB 集群升级到 v8.4.0 或之后版本，将导致集群不可用。升级 TiDB 前，请务必检查你的操作系统版本。\n> - 对于以上表格中所列操作系统的 32 位版本，TiDB 在这些 32 位操作系统以及对应的 CPU 架构上**不保障**可编译、可构建以及可部署，或 TiDB 不主动适配这些 32 位的操作系统。\n> - 以上未提及的操作系统版本**也许可以**运行 TiDB，但尚未得到 TiDB 官方支持。\n\n> **注意：**\n>\n> - TiDB 只支持 Red Hat 兼容内核 (RHCK) 的 Oracle Enterprise Linux，不支持 Oracle Enterprise Linux 提供的 Unbreakable Enterprise Kernel。\n> - CentOS Linux 8 的上游支持已于 2021 年 12 月 31 日终止，但 CentOS 将继续提供对 CentOS Stream 8 的支持。\n> - TiDB 将不再支持 Ubuntu 16.04。强烈建议升级到 Ubuntu 18.04 或更高版本。\n> - 从 v8.4.0 开始，TiDB 依赖 glibc 2.28。如果 glibc 版本不满足要求，建议使用上述表格中支持的操作系统，或将操作系统升级到支持 glibc 2.28 的版本。\n\n### 编译和运行 TiDB 所依赖的库\n\n|  编译和构建 TiDB 所需的依赖库   |  版本   |\n|   :---   |   :---   |\n|   Golang  |  1.23 及以上版本  |\n|   Rust    |   nightly-2023-12-28 及以上版本  |\n|  GCC      |   7.x      |\n|  LLVM     |  17.0 及以上版本  |\n\n运行时所需的依赖库：glibc（2.28-151.el8 版本）\n\n### Docker 镜像依赖\n\n支持的 CPU 架构如下：\n\n- x86_64，从 TiDB v6.6.0 开始，需要 [x86-64-v2 指令集](https://developers.redhat.com/blog/2021/01/05/building-red-hat-enterprise-linux-9-for-the-x86-64-v2-microarchitecture-level)\n- ARM 64\n\n## 软件配置要求\n\n### 中控机软件配置\n\n| 软件 | 版本 |\n| :----------------------- | :----------: |\n| sshpass | 1.06 及以上 |\n| TiUP | 1.5.0 及以上 |\n\n> **注意：**\n>\n> 中控机需要部署 [TiUP 软件](/tiup/tiup-documentation-guide.md)来完成 TiDB 集群运维管理。\n\n### 目标主机建议配置软件\n\n| 软件 | 版本 |\n| :----- | :----------: |\n| sshpass | 1.06 及以上 |\n| numa | 2.0.12 及以上 |\n| tar  | 任意      |\n\n## 服务器配置要求\n\nTiDB 支持部署和运行在 Intel x86-64 架构的 64 位通用硬件服务器平台或者 ARM 架构的硬件服务器平台。对于开发、测试及生产环境的服务器硬件配置（不包含操作系统 OS 本身的占用）有以下要求和建议：\n\n### 开发及测试环境\n\n| **组件** | **CPU** | **内存** | **本地存储** | **网络** | **实例数量(最低要求)** |\n| --- | --- | --- | --- | --- | --- |\n| TiDB | 8 核+ | 16 GB+ | [磁盘空间要求](#磁盘空间要求) | 千兆网卡 | 1（可与 PD 同机器） |\n| PD | 4 核+ | 8 GB+ | SAS, 200 GB+ | 千兆网卡 | 1（可与 TiDB 同机器） |\n| TiKV | 8 核+ | 32 GB+ | SSD, 200 GB+ | 千兆网卡 | 3 |\n| TiFlash | 32 核+ | 64 GB+ | SSD, 200 GB+ | 千兆网卡 | 1 |\n| TiCDC | 8 核+ | 16 GB+ | SAS, 200 GB+ | 千兆网卡 | 1 |\n\n> **注意：**\n>\n> - 验证测试环境中的 TiDB 和 PD 可以部署在同一台服务器上。\n> - 如进行性能相关的测试，避免采用低性能存储和网络硬件配置，防止对测试结果的正确性产生干扰。\n> - TiKV 的 SSD 盘推荐使用 NVME 接口以保证读写更快。\n> - 如果仅验证功能，建议使用 [TiDB 数据库快速上手指南](/quick-start-with-tidb.md)进行单机功能测试。\n> - 从 v6.3.0 开始，在 Linux AMD64 架构的硬件平台部署 TiFlash 时，CPU 必须支持 AVX2 指令集。确保命令 `grep avx2 /proc/cpuinfo` 有输出。而在 Linux ARM64 架构的硬件平台部署 TiFlash 时，CPU 必须支持 ARMv8 架构。确保命令 `grep 'crc32' /proc/cpuinfo | grep 'asimd'` 有输出。通过使用向量扩展指令集，TiFlash 的向量化引擎能提供更好的性能。\n\n### 生产环境\n\n| **组件** | **CPU** | **内存** | **硬盘类型** | **网络** | **实例数量(最低要求)** |\n| --- | --- | --- | --- | --- | --- |\n| TiDB | 16 核+ | 48 GB+ | SSD | 万兆网卡（2 块最佳） | 2 |\n| PD | 8 核+ | 16 GB+ | SSD | 万兆网卡（2 块最佳） | 3 |\n| TiKV | 16 核+ | 64 GB+ | SSD | 万兆网卡（2 块最佳） | 3 |\n| TiFlash | 48 核+ | 128 GB+ | 1 or more SSDs | 万兆网卡（2 块最佳） | 2 |\n| TiCDC | 16 核+ | 64 GB+ | SSD | 万兆网卡（2 块最佳） | 2 |\n| 监控 | 8 核+ | 16 GB+ | SAS | 千兆网卡 | 1 |\n\n> **注意：**\n>\n> - 生产环境中的 TiDB 和 PD 可以部署和运行在同一台服务器上，如对性能和可靠性有更高的要求，应尽可能分开部署。\n> - 强烈建议分别为生产环境中的 TiDB、TiKV 和 TiFlash 配置至少 8 核的 CPU。强烈推荐使用更高的配置，以获得更好的性能。\n> - TiKV 硬盘大小配置建议 PCIe SSD 不超过 4 TB，普通 SSD 不超过 1.5 TB。\n> - 如果你在云服务商（如 AWS、Google Cloud 或 Azure）上部署 TiDB 集群，建议 TiKV 节点使用云盘。在云环境中，TiKV 实例崩溃时，本地磁盘上的数据可能会丢失。\n\n在部署 TiFlash 之前，请注意以下事项：\n\n- TiFlash 支持[多盘部署](/tiflash/tiflash-configuration.md#多盘部署)。\n- TiFlash 数据目录的第一块磁盘推荐用高性能 SSD 来缓冲 TiKV 同步数据的实时写入，该盘性能应不低于 TiKV 所使用的磁盘，比如 PCIe SSD。并且该磁盘容量建议不小于总容量的 10%，否则它可能成为这个节点的能承载的数据量的瓶颈。而其他磁盘可以根据需求部署多块普通 SSD，当然更好的 PCIe SSD 硬盘会带来更好的性能。\n- TiFlash 推荐与 TiKV 部署在不同节点，如果条件所限必须将 TiFlash 与 TiKV 部署在相同节点，则需要适当增加 CPU 核数和内存，且尽量将 TiFlash 与 TiKV 部署在不同的磁盘，以免互相干扰。\n- TiFlash 硬盘总容量大致为：`整个 TiKV 集群的需同步数据容量 / TiKV 副本数 * TiFlash 副本数`。例如整体 TiKV 的规划容量为 1 TB、TiKV 副本数为 3、TiFlash 副本数为 2，则 TiFlash 的推荐总容量为 `1024 GB / 3 * 2`。用户可以选择同步部分表数据而非全部，具体容量可以根据需要同步的表的数据量具体分析。\n\n在部署 TiCDC 时，建议在大于 500 GB 的 PCIe SSD 磁盘上部署。\n\n## 网络要求\n\n<!-- Localization note for TiDB:\n\n- 英文：用 distributed SQL，同时开始强调 HTAP\n- 中文：可以保留 NewSQL 字眼，同时强调一栈式实时 HTAP\n- 日文：NewSQL 认可度高，用 NewSQL\n\n-->\n\nTiDB 作为开源一栈式实时 HTAP 数据库，其正常运行需要网络环境提供如下的网络端口配置要求，管理员可根据实际环境中 TiDB 组件部署的方案，在网络侧和主机侧开放相关端口：\n\n| 组件 | 默认端口 | 说明 |\n| :-- | :-- | :-- |\n| TiDB |  4000  | 应用及 DBA 工具访问通信端口 |\n| TiDB | 10080  | TiDB 状态信息上报通信端口 |\n| TiKV |  20160 | TiKV 通信端口 |\n| TiKV |  20180 | TiKV 状态信息上报通信端口 |\n| PD | 2379 | 提供 TiDB 和 PD 通信端口 |\n| PD | 2380 | PD 集群节点间通信端口 |\n|TiFlash|9000|TiFlash TCP 服务端口|\n|TiFlash|3930|TiFlash RAFT 服务和 Coprocessor 服务端口|\n|TiFlash|20170|TiFlash Proxy 服务端口|\n|TiFlash|20292|Prometheus 拉取 TiFlash Proxy metrics 端口|\n|TiFlash|8234|Prometheus 拉取 TiFlash metrics 端口|\n| CDC | 8300 | CDC 通信接口 |\n| Monitoring | 9090 | Prometheus 服务通信端口 |\n| Monitoring | 12020 | NgMonitoring 服务通信端口 |\n| Node_exporter | 9100 | TiDB 集群每个节点的系统信息上报通信端口 |\n| Blackbox_exporter | 9115 | Blackbox_exporter 通信端口，用于 TiDB 集群端口监控 |\n| Grafana | 3000 | Web 监控服务对外服务和客户端(浏览器)访问端口 |\n| Alertmanager | 9093 | 告警 web 服务端口 |\n| Alertmanager | 9094 | 告警通信端口 |\n\n## 磁盘空间要求\n\n| 组件 | 磁盘空间要求 | 健康水位使用率 |\n| :-- | :-- | :-- |\n| TiDB | <ul><li>日志盘建议最少预留 30 GB。</li> <li>v6.5.0 及以上版本默认启用了 Fast Online DDL 对添加索引等 DDL 操作进行加速（通过变量 [`tidb_ddl_enable_fast_reorg`](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入) 控制）。如果业务中可能存在针对大对象的 DDL 操作，或需要使用 [<code>IMPORT INTO</code>](/sql-statements/sql-statement-import-into.md) SQL 语句导入数据，推荐为 TiDB 准备额外的 SSD 磁盘空间（建议 100 GB+）。配置方式详见[设置 TiDB 节点的临时空间](/check-before-deployment.md#设置-tidb-节点的临时空间推荐)。</li></ul>| 低于 90% |\n| PD | 数据盘和日志盘建议最少各预留 20 GB | 低于 90% |\n| TiKV | 数据盘和日志盘建议最少各预留 100 GB | 低于 80% |\n| TiFlash | 数据盘建议最少预留 100 GB，日志盘建议最少预留 30 GB | 低于 80% |\n| TiUP | <ul><li>中控机：部署一个版本的 TiDB 集群占用不超过 1 GB 空间，部署多个版本集群所占用的空间会相应增加 </li> <li>部署服务器（实际运行 TiDB 各组件的机器）：TiFlash 占用约 700 MB 空间，其他组件（PD、TiDB、TiKV 等）各占用约 200 MB 空间。同时，部署过程会占用小于 1 MB 临时空间（/tmp）存放临时文件 </li></ul> | 不涉及|\n| Ngmonitoring | <ul><li>Conprof：3 x 1 GB x 组件数量（表示每个组件每天占用约 1 GB，总共 3 天） + 20 GB 预留空间 </li><li> Top SQL：30 x 50 MB x 组件数量（每个组件每天占用约 50 MB，总共 30 天） </li><li> Top SQL 和 Conprof 共享预留空间</li></ul> | 不涉及 |\n\n## 客户端 Web 浏览器要求\n\nTiDB 提供了基于 [Grafana](https://grafana.com/) 的技术平台，对数据库集群的各项指标进行可视化展现。采用支持 Javascript 的微软 Edge、Apple Safari、Google Chrome、Mozilla Firefox 的较新版本即可访问监控入口。\n\n## TiFlash 存算分离架构的软硬件要求\n\n上面的 TiFlash 软硬件要求是针对存算一体架构的。从 v7.0.0 开始，TiFlash 支持[存算分离架构](/tiflash/tiflash-disaggregated-and-s3.md)，该架构下 TiFlash 分为 Write Node 和 Compute Node 两个角色，对应的软硬件要求如下：\n\n- 软件：与存算一体架构一致，详见[操作系统及平台要求](#操作系统及平台要求)。\n- 网络端口：与存算一体架构一致，详见[网络要求](#网络要求)。\n- 磁盘空间：\n    - TiFlash Write Node：推荐 200 GB+，用作增加 TiFlash 副本、Region 副本迁移时向 Amazon S3 上传数据前的本地缓冲区。此外，还需要一个与 Amazon S3 兼容的对象存储。\n    - TiFlash Compute Node：推荐 100 GB+，主要用于缓存从 Write Node 读取的数据以提升性能。Compute Node 的缓存可能会被完全使用，这是正常现象。\n- CPU 以及内存等要求参考下文。\n\n### 开发及测试环境\n\n| 组件 | CPU | 内存 | 本地存储 | 网络 | 实例数量（最低要求） |\n| --- | --- | --- | --- | --- | --- |\n| TiFlash Write Node | 16 核+ | 32 GB+ | SSD, 200 GB+ | 千兆网卡 | 1 |\n| TiFlash Compute Node | 16 核+ | 32 GB+ | SSD, 100 GB+ | 千兆网卡 | 0（参见下文“注意”说明） |\n\n### 生产环境\n\n| 组件 | CPU | 内存 | 硬盘类型 | 网络 | 实例数量（最低要求） |\n| --- | --- | --- | --- | --- | --- |\n| TiFlash Write Node | 32 核+ | 64 GB+ | SSD, 200 GB+ | 万兆网卡（2 块最佳） | 2 |\n| TiFlash Compute Node | 32 核+ | 64 GB+ | SSD, 100 GB+  | 万兆网卡（2 块最佳） | 0（参见下文“注意”说明） |\n\n> **注意：**\n>\n> TiFlash Compute Node 可以使用 TiUP 等部署工具快速扩缩容，扩缩容范围是 `[0, +inf]`。\n"
        },
        {
          "name": "hybrid-deployment-topology.md",
          "type": "blob",
          "size": 10.7939453125,
          "content": "---\ntitle: 混合部署拓扑\nsummary: 介绍混合部署 TiDB 集群的拓扑结构。\naliases: ['/docs-cn/dev/hybrid-deployment-topology/']\n---\n\n# 混合部署拓扑\n\n本文介绍 TiDB 集群的 TiKV 和 TiDB 混合部署拓扑以及主要参数。常见的场景为，部署机为多路 CPU 处理器，内存也充足，为提高物理机资源利用率，可单机多实例部署，即 TiDB、TiKV 通过 numa 绑核，隔离 CPU 资源。PD 和 Prometheus 混合部署，但两者的数据目录需要使用独立的文件系统。\n\n## 拓扑信息\n\n| 实例 | 个数 | 物理机配置 | IP | 配置 |\n| :-- | :-- | :-- | :-- | :-- |\n| TiDB | 6 | 32 VCore 64GB | 10.0.1.1<br/> 10.0.1.2<br/> 10.0.1.3 | 配置 numa 绑核操作 |\n| PD | 3 | 16 VCore 32 GB | 10.0.1.4<br/> 10.0.1.5<br/> 10.0.1.6 | 配置 location_labels 参数 |\n| TiKV | 6 | 32 VCore 64GB | 10.0.1.7<br/> 10.0.1.8<br/> 10.0.1.9 | 1. 区分实例级别的 port、status_port；<br/> 2. 配置全局参数 readpool、storage 以及 raftstore；<br/> 3. 配置实例级别 host 维度的 labels；<br/> 4. 配置 numa 绑核操作|\n| Monitoring & Grafana | 1 | 4 VCore 8GB * 1 500GB (ssd)  | 10.0.1.10 | 默认配置 |\n\n### 拓扑模版\n\n<details>\n<summary>简单混部配置模板</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\nserver_configs:\n  tikv:\n    readpool.unified.max-thread-count: <The value refers to the calculation formula result of the multi-instance topology document.>\n    readpool.storage.use-unified-pool: false\n    readpool.coprocessor.use-unified-pool: true\n    storage.block-cache.capacity: \"<The value refers to the calculation formula result of the multi-instance topology document.>\"\n    raftstore.capacity: \"<The value refers to the calculation formula result of the multi-instance topology document.>\"\n  pd:\n    replication.location-labels: [\"host\"]\n\npd_servers:\n  - host: 10.0.1.4\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.1\n    port: 4000\n    status_port: 10080\n    numa_node: \"0\"\n  - host: 10.0.1.1\n    port: 4001\n    status_port: 10081\n    numa_node: \"1\"\n  - host: 10.0.1.2\n    port: 4000\n    status_port: 10080\n    numa_node: \"0\"\n  - host: 10.0.1.2\n    port: 4001\n    status_port: 10081\n    numa_node: \"1\"\n  - host: 10.0.1.3\n    port: 4000\n    status_port: 10080\n    numa_node: \"0\"\n  - host: 10.0.1.3\n    port: 4001\n    status_port: 10081\n    numa_node: \"1\"\n\ntikv_servers:\n  - host: 10.0.1.7\n    port: 20160\n    status_port: 20180\n    numa_node: \"0\"\n    config:\n      server.labels: { host: \"tikv1\" }\n  - host: 10.0.1.7\n    port: 20161\n    status_port: 20181\n    numa_node: \"1\"\n    config:\n      server.labels: { host: \"tikv1\" }\n  - host: 10.0.1.8\n    port: 20160\n    status_port: 20180\n    numa_node: \"0\"\n    config:\n      server.labels: { host: \"tikv2\" }\n  - host: 10.0.1.8\n    port: 20161\n    status_port: 20181\n    numa_node: \"1\"\n    config:\n      server.labels: { host: \"tikv2\" }\n  - host: 10.0.1.9\n    port: 20160\n    status_port: 20180\n    numa_node: \"0\"\n    config:\n      server.labels: { host: \"tikv3\" }\n  - host: 10.0.1.9\n    port: 20161\n    status_port: 20181\n    numa_node: \"1\"\n    config:\n      server.labels: { host: \"tikv3\" }\n\nmonitoring_servers:\n  - host: 10.0.1.10\n\ngrafana_servers:\n  - host: 10.0.1.10\n\nalertmanager_servers:\n  - host: 10.0.1.10\n```\n\n</details>\n\n<details>\n<summary>详细混部配置模板</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\nmonitored:\n  node_exporter_port: 9100\n  blackbox_exporter_port: 9115\n  deploy_dir: \"/tidb-deploy/monitored-9100\"\n  data_dir: \"/tidb-data/monitored-9100\"\n  log_dir: \"/tidb-deploy/monitored-9100/log\"\n\nserver_configs:\n  tidb:\n    log.slow-threshold: 300\n  tikv:\n    readpool.unified.max-thread-count: <The value refers to the calculation formula result of the multi-instance topology document.>\n    readpool.storage.use-unified-pool: false\n    readpool.coprocessor.use-unified-pool: true\n    storage.block-cache.capacity: \"<The value refers to the calculation formula result of the multi-instance topology document.>\"\n    raftstore.capacity: \"<The value refers to the calculation formula result of the multi-instance topology document.>\"\n  pd:\n    replication.location-labels: [\"host\"]\n    schedule.leader-schedule-limit: 4\n    schedule.region-schedule-limit: 2048\n    schedule.replica-schedule-limit: 64\n\npd_servers:\n  - host: 10.0.1.4\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.1\n    port: 4000\n    status_port: 10080\n    deploy_dir: \"/tidb-deploy/tidb-4000\"\n    log_dir: \"/tidb-deploy/tidb-4000/log\"\n    numa_node: \"0\"\n  - host: 10.0.1.1\n    port: 4001\n    status_port: 10081\n    deploy_dir: \"/tidb-deploy/tidb-4001\"\n    log_dir: \"/tidb-deploy/tidb-4001/log\"\n    numa_node: \"1\"\n  - host: 10.0.1.2\n    port: 4000\n    status_port: 10080\n    deploy_dir: \"/tidb-deploy/tidb-4000\"\n    log_dir: \"/tidb-deploy/tidb-4000/log\"\n    numa_node: \"0\"\n  - host: 10.0.1.2\n    port: 4001\n    status_port: 10081\n    deploy_dir: \"/tidb-deploy/tidb-4001\"\n    log_dir: \"/tidb-deploy/tidb-4001/log\"\n    numa_node: \"1\"\n  - host: 10.0.1.3\n    port: 4000\n    status_port: 10080\n    deploy_dir: \"/tidb-deploy/tidb-4000\"\n    log_dir: \"/tidb-deploy/tidb-4000/log\"\n    numa_node: \"0\"\n  - host: 10.0.1.3\n    port: 4001\n    status_port: 10081\n    deploy_dir: \"/tidb-deploy/tidb-4001\"\n    log_dir: \"/tidb-deploy/tidb-4001/log\"\n    numa_node: \"1\"\n\ntikv_servers:\n  - host: 10.0.1.7\n    port: 20160\n    status_port: 20180\n    deploy_dir: \"/tidb-deploy/tikv-20160\"\n    data_dir: \"/tidb-data/tikv-20160\"\n    log_dir: \"/tidb-deploy/tikv-20160/log\"\n    numa_node: \"0\"\n    config:\n      server.labels: { host: \"tikv1\" }\n  - host: 10.0.1.7\n    port: 20161\n    status_port: 20181\n    deploy_dir: \"/tidb-deploy/tikv-20161\"\n    data_dir: \"/tidb-data/tikv-20161\"\n    log_dir: \"/tidb-deploy/tikv-20161/log\"\n    numa_node: \"1\"\n    config:\n      server.labels: { host: \"tikv1\" }\n  - host: 10.0.1.8\n    port: 20160\n    status_port: 20180\n    deploy_dir: \"/tidb-deploy/tikv-20160\"\n    data_dir: \"/tidb-data/tikv-20160\"\n    log_dir: \"/tidb-deploy/tikv-20160/log\"\n    numa_node: \"0\"\n    config:\n      server.labels: { host: \"tikv2\" }\n  - host: 10.0.1.8\n    port: 20161\n    status_port: 20181\n    deploy_dir: \"/tidb-deploy/tikv-20161\"\n    data_dir: \"/tidb-data/tikv-20161\"\n    log_dir: \"/tidb-deploy/tikv-20161/log\"\n    numa_node: \"1\"\n    config:\n      server.labels: { host: \"tikv2\" }\n  - host: 10.0.1.9\n    port: 20160\n    status_port: 20180\n    deploy_dir: \"/tidb-deploy/tikv-20160\"\n    data_dir: \"/tidb-data/tikv-20160\"\n    log_dir: \"/tidb-deploy/tikv-20160/log\"\n    numa_node: \"0\"\n    config:\n      server.labels: { host: \"tikv3\" }\n  - host: 10.0.1.9\n    port: 20161\n    status_port: 20181\n    deploy_dir: \"/tidb-deploy/tikv-20161\"\n    data_dir: \"/tidb-data/tikv-20161\"\n    log_dir: \"/tidb-deploy/tikv-20161/log\"\n    numa_node: \"1\"\n    config:\n      server.labels: { host: \"tikv3\" }\n\nmonitoring_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # port: 9090\n    # deploy_dir: \"/tidb-deploy/prometheus-8249\"\n    # data_dir: \"/tidb-data/prometheus-8249\"\n    # log_dir: \"/tidb-deploy/prometheus-8249/log\"\n\ngrafana_servers:\n  - host: 10.0.1.10\n    # port: 3000\n    # deploy_dir: /tidb-deploy/grafana-3000\n\nalertmanager_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # web_port: 9093\n    # cluster_port: 9094\n    # deploy_dir: \"/tidb-deploy/alertmanager-9093\"\n    # data_dir: \"/tidb-data/alertmanager-9093\"\n    # log_dir: \"/tidb-deploy/alertmanager-9093/log\"\n```\n\n</details>\n\n以上 TiDB 集群拓扑文件中，详细的配置项说明见[通过 TiUP 部署 TiDB 集群的拓扑文件配置](/tiup/tiup-cluster-topology-reference.md)。\n\n### 混合部署的关键参数介绍\n\n本节介绍单机多实例的关键参数，主要用于 TiDB、TiKV 的单机多实例部署场景。你需要按照提供的计算公式，将结果填写至上一步的配置模板中。\n\n- TiKV 进行配置优化\n\n    - readpool 线程池自适应，配置 `readpool.unified.max-thread-count` 参数可以使 `readpool.storage` 和 `readpool.coprocessor` 共用统一线程池，同时要分别设置自适应开关。\n\n        - 开启 `readpool.storage` 和 `readpool.coprocessor`：\n\n            ```yaml\n            readpool.storage.use-unified-pool: true\n            readpool.coprocessor.use-unified-pool: true\n            ```\n\n        - 计算公式如下：\n\n            ```\n            readpool.unified.max-thread-count = cores * 0.8 / TiKV 数量\n            ```\n\n    - storage CF (all RocksDB column families) 内存自适应，配置 `storage.block-cache.capacity` 参数即可实现 CF 之间自动平衡内存使用。\n\n        - 计算公式如下：\n\n            ```\n            storage.block-cache.capacity = (MEM_TOTAL * 0.5 / TiKV 实例数量)\n            ```\n\n    - 如果多个 TiKV 实例部署在同一块物理磁盘上，需要在 tikv 配置中添加 capacity 参数：\n\n        ```\n        raftstore.capacity = 磁盘总容量 / TiKV 实例数量\n        ```\n\n- label 调度配置\n\n    由于采用单机多实例部署 TiKV，为了避免物理机宕机导致 Region Group 默认 3 副本的 2 副本丢失，导致集群不可用的问题，可以通过 label 来实现 PD 智能调度，保证同台机器的多 TiKV 实例不会出现 Region Group 只有 2 副本的情况。\n\n    - TiKV 配置\n\n        相同物理机配置相同的 host 级别 label 信息：\n\n        ```yml\n        config:\n          server.labels:\n            host: tikv1\n        ```\n\n    - PD 配置\n\n        PD 需要配置 labels 类型来识别并调度 Region：\n\n        ```yml\n        pd:\n          replication.location-labels: [\"host\"]\n        ```\n\n- `numa_node` 绑核\n\n    - 在实例参数模块配置对应的 `numa_node` 参数，并添加对应的物理 CPU 的核数；\n\n    - numa 绑核使用前，确认已经安装 numactl 工具，以及物理机对应的物理机 CPU 的信息后，再进行参数配置；\n\n    - `numa_node` 这个配置参数与 `numactl --membind` 配置对应。\n\n> **注意：**\n>\n> - 编辑配置文件模版时，注意修改必要参数、IP、端口及目录。\n> - 各个组件的 deploy_dir，默认会使用 global 中的 `<deploy_dir>/<components_name>-<port>`。例如 tidb 端口指定 4001，则 deploy_dir 默认为 '/tidb-deploy/tidb-4001'。因此，在多实例场景下指定非默认端口时，无需再次指定目录。\n> - 无需手动创建配置文件中的 `tidb` 用户，TiUP cluster 组件会在部署主机上自动创建该用户。可以自定义用户，也可以和中控机的用户保持一致。\n> - 如果部署目录配置为相对路径，会部署在用户的 Home 目录下。\n"
        },
        {
          "name": "identify-expensive-queries.md",
          "type": "blob",
          "size": 5,
          "content": "---\ntitle: 定位消耗系统资源多的查询\naliases: ['/docs-cn/dev/identify-expensive-queries/','/docs-cn/dev/how-to/maintain/identify-abnormal-queries/identify-expensive-queries/','/docs-cn/how-to/maintain/identify-abnormal-queries/identify-aborted-queries/','/docs-cn/dev/how-to/maintain/identify-abnormal-queries/identify-aborted-queries/']\nsummary: TiDB 会将执行时间超过 tidb_expensive_query_time_threshold 限制（默认值为 60s），或使用内存超过 tidb_mem_quota_query 限制（默认值为 1 GB）的语句输出到 tidb-server 日志文件中，用于定位消耗系统资源多的查询语句。expensive query 日志和慢查询日志的区别在于，expensive query 日志可以将正在执行的语句的相关信息打印出来。当一条语句在执行过程中达到资源使用阈值时，TiDB 会即时将这条语句的相关信息写入日志。\n---\n\n# 定位消耗系统资源多的查询\n\nTiDB 会将执行时间超过 [`tidb_expensive_query_time_threshold`](/system-variables.md#tidb_expensive_query_time_threshold) 限制（默认值为 60s），或使用内存超过 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 限制（默认值为 1 GB）的语句输出到 [tidb-server 日志文件](/tidb-configuration-file.md#logfile)（默认文件为 \"tidb.log\"）中，用于在语句执行结束前定位消耗系统资源多的查询语句（以下简称为 expensive query），帮助用户分析和解决语句执行的性能问题。\n\n注意，expensive query 日志和[慢查询日志](/identify-slow-queries.md)的区别是，慢查询日志是在语句执行完后才打印，expensive query 日志可以将正在执行的语句的相关信息打印出来。当一条语句在执行过程中达到资源使用阈值时（执行时间/使用内存量），TiDB 会即时将这条语句的相关信息写入日志。\n\n## Expensive query 日志示例\n\n```sql\n[expensivequery.go:145] [expensive_query] [cost_time=60.021998785s] [cop_time=0.022540151s] [process_time=28.448316643s] [wait_time=0.045507163s] [request_count=430] [total_keys=3538276] [process_keys=3537846] [num_cop_tasks=430] [process_avg_time=0.066158875s] [process_p90_time=0.140427865s] [process_max_time=0.27903656s] [process_max_addr=tikv-1-peer:20160] [wait_avg_time=0.00010583s] [wait_p90_time=0.000358794s] [wait_max_time=0.001218721s] [wait_max_addr=tikv-1-peer:20160] [stats=usertable:451469035823955972] [conn=1621098504] [user=root] [database=test] [table_ids=\"[104]\"] [txn_start_ts=451469037501677571] [mem_max=\"621043469 Bytes (592.3 MB)\"] [sql=\"insert /*+ SET_VAR(tidb_dml_type=bulk) */ into usertable_2 select * from usertable limit 5000000\"] [session_alias=] [\"affected rows\"=3505282]]\n```\n\n## 字段含义说明\n\n基本字段：\n\n* `cost_time`：日志打印时语句已经花费的执行时间。\n* `stats`：语句涉及到的表或索引使用的统计信息版本。值为 `pseudo` 时表示无可用统计信息，需要对表或索引进行 analyze。\n* `table_ids`：语句涉及到的表的 ID。\n* `txn_start_ts`：事务的开始时间戳，也是事务的唯一 ID，可以用这个值在 TiDB 日志中查找事务相关的其他日志。\n* `sql`：SQL 语句。\n* `session_alias`：当前连接的别名。\n* `affected rows`：语句当前影响的行数。\n\n和内存使用相关的字段：\n\n* `mem_max`：日志打印时语句已经使用的内存空间。该项使用两种单位标识内存使用量，分别为 Bytes 以及易于阅读的自适应单位（比如 MB、GB 等）。\n\n和 SQL 执行的用户相关的字段：\n\n* `user`：执行语句的用户名。\n* `conn_id`：用户的连接 ID，可以用类似 `con:60026` 的关键字在 TiDB 日志中查找该连接相关的其他日志。\n* `database`：执行语句时使用的 database。\n\n和 TiKV Coprocessor Task 相关的字段：\n\n* `wait_time`：该语句在 TiKV 的等待时间之和，因为 TiKV 的 Coprocessor 线程数是有限的，当所有的 Coprocessor 线程都在工作的时候，请求会排队；当队列中有某些请求耗时很长的时候，后面的请求的等待时间都会增加。\n* `request_count`：该语句发送的 Coprocessor 请求的数量。\n* `total_keys`：Coprocessor 扫过的 key 的数量。\n* `processed_keys`：Coprocessor 处理的 key 的数量。与 total_keys 相比，processed_keys 不包含 MVCC 的旧版本。如果 processed_keys 和 total_keys 相差很大，说明旧版本比较多。\n* `num_cop_tasks`：该语句发送的 Coprocessor 请求的数量。\n* `process_avg_time`：Coprocessor 执行 task 的平均执行时间。\n* `process_p90_time`：Coprocessor 执行 task 的 P90 分位执行时间。\n* `process_max_time`：Coprocessor 执行 task 的最长执行时间。\n* `process_max_addr`：task 执行时间最长的 Coprocessor 所在地址。\n* `wait_avg_time`：Coprocessor 上 task 的等待时间。\n* `wait_p90_time`：Coprocessor 上 task 的 P90 分位等待时间。\n* `wait_max_time`：Coprocessor 上 task 的最长等待时间。\n* `wait_max_addr`：task 等待时间最长的 Coprocessor 所在地址。\n"
        },
        {
          "name": "identify-slow-queries.md",
          "type": "blob",
          "size": 32.6953125,
          "content": "---\ntitle: 慢查询日志\naliases: ['/docs-cn/dev/identify-slow-queries/','/docs-cn/dev/how-to/maintain/identify-abnormal-queries/identify-slow-queries/','/docs-cn/sql/slow-query/','/docs-cn/dev/how-to/maintain/identify-slow-queries/']\nsummary: TiDB 会将执行时间超过 300 毫秒的语句输出到慢查询日志中，用于帮助用户定位慢查询语句。可以通过修改系统变量来启用或禁用慢查询日志。日志示例包括执行时间、用户信息、执行计划等字段。用户可通过查询 SLOW_QUERY 表来查询慢查询日志中的内容。还可以使用 pt-query-digest 工具分析 TiDB 慢日志。ADMIN SHOW SLOW 命令可以显示最近的慢查询记录或最慢的查询记录。\n---\n\n# 慢查询日志\n\nTiDB 会将执行时间超过 [`tidb_slow_log_threshold`](/system-variables.md#tidb_slow_log_threshold)（默认值为 300 毫秒）的语句输出到 [slow-query-file](/tidb-configuration-file.md#slow-query-file)（默认值为 \"tidb-slow.log\"）日志文件中，用于帮助用户定位慢查询语句，分析和解决 SQL 执行的性能问题。\n\nTiDB 默认启用慢查询日志，可以修改系统变量 [`tidb_enable_slow_log`](/system-variables.md#tidb_enable_slow_log) 来启用或禁用它。\n\n## 日志示例\n\n```sql\n# Time: 2019-08-14T09:26:59.487776265+08:00\n# Txn_start_ts: 410450924122144769\n# User@Host: root[root] @ localhost [127.0.0.1]\n# Conn_ID: 3086\n# Exec_retry_time: 5.1 Exec_retry_count: 3\n# Query_time: 1.527627037\n# Parse_time: 0.000054933\n# Compile_time: 0.000129729\n# Rewrite_time: 0.000000003 Preproc_subqueries: 2 Preproc_subqueries_time: 0.000000002\n# Optimize_time: 0.00000001\n# Wait_TS: 0.00001078\n# Process_time: 0.07 Request_count: 1 Total_keys: 131073 Process_keys: 131072 Prewrite_time: 0.335415029 Commit_time: 0.032175429 Get_commit_ts_time: 0.000177098 Local_latch_wait_time: 0.106869448 Write_keys: 131072 Write_size: 3538944 Prewrite_region: 1\n# DB: test\n# Is_internal: false\n# Digest: 50a2e32d2abbd6c1764b1b7f2058d428ef2712b029282b776beb9506a365c0f1\n# Stats: t:pseudo\n# Num_cop_tasks: 1\n# Cop_proc_avg: 0.07 Cop_proc_p90: 0.07 Cop_proc_max: 0.07 Cop_proc_addr: 172.16.5.87:20171\n# Cop_wait_avg: 0 Cop_wait_p90: 0 Cop_wait_max: 0 Cop_wait_addr: 172.16.5.87:20171\n# Cop_backoff_regionMiss_total_times: 200 Cop_backoff_regionMiss_total_time: 0.2 Cop_backoff_regionMiss_max_time: 0.2 Cop_backoff_regionMiss_max_addr: 127.0.0.1 Cop_backoff_regionMiss_avg_time: 0.2 Cop_backoff_regionMiss_p90_time: 0.2\n# Cop_backoff_rpcPD_total_times: 200 Cop_backoff_rpcPD_total_time: 0.2 Cop_backoff_rpcPD_max_time: 0.2 Cop_backoff_rpcPD_max_addr: 127.0.0.1 Cop_backoff_rpcPD_avg_time: 0.2 Cop_backoff_rpcPD_p90_time: 0.2\n# Cop_backoff_rpcTiKV_total_times: 200 Cop_backoff_rpcTiKV_total_time: 0.2 Cop_backoff_rpcTiKV_max_time: 0.2 Cop_backoff_rpcTiKV_max_addr: 127.0.0.1 Cop_backoff_rpcTiKV_avg_time: 0.2 Cop_backoff_rpcTiKV_p90_time: 0.2\n# Mem_max: 525211\n# Disk_max: 65536\n# Prepared: false\n# Plan_from_cache: false\n# Succ: true\n# Plan: tidb_decode_plan('ZJAwCTMyXzcJMAkyMAlkYXRhOlRhYmxlU2Nhbl82CjEJMTBfNgkxAR0AdAEY1Dp0LCByYW5nZTpbLWluZiwraW5mXSwga2VlcCBvcmRlcjpmYWxzZSwgc3RhdHM6cHNldWRvCg==')\nuse test;\ninsert into t select * from t;\n```\n\n## 字段含义说明\n\n> **注意：**\n>\n> 慢查询日志中所有时间相关字段的单位都是 **“秒”**\n\nSlow Query 基础信息：\n\n* `Time`：表示日志打印时间。\n* `Query_time`：表示执行这个语句花费的时间。\n* `Parse_time`：表示这个语句在语法解析阶段花费的时间。\n* `Compile_time`：表示这个语句在查询优化阶段花费的时间。\n* `Optimize_time`：表示这个语句在优化查询计划阶段花费的时间。\n* `Wait_TS`：表示这个语句在等待获取事务 TS 阶段花费的时间。\n* `Query`：表示 SQL 语句。慢日志里面不会打印 `Query`，但映射到内存表后，对应的字段叫 `Query`。\n* `Digest`：表示 SQL 语句的指纹。\n* `Txn_start_ts`：表示事务的开始时间戳，也是事务的唯一 ID，可以用这个值在 TiDB 日志中查找事务相关的其他日志。\n* `Is_internal`：表示是否为 TiDB 内部的 SQL 语句。`true` 表示 TiDB 系统内部执行的 SQL 语句，`false` 表示用户执行的 SQL 语句。\n* `Index_names`：表示这个语句执行用到的索引。\n* `Stats`：表示这个语句使用到的统计信息的健康状态、内部版本号、总行数、修改行数以及加载状态。`pseudo` 状态表示统计信息不健康。如果有尝试使用但没有完全加载的统计信息，会在之后输出其内部状态。例如，`t1:439478225786634241[105000;5000][col1:allEvicted][idx1:allEvicted]` 的含义如下：\n    - `t1`：本次查询优化过程中使用了 `t1` 表上的统计信息\n    - `439478225786634241`：其内部版本号\n    - `105000`：统计信息中维护的总行数\n    - `5000`：自上次收集统计信息以来记录的修改的行数\n    - `col1:allEvicted`：`col1` 列对应的统计信息没有完全加载\n    - `idx1:allEvicted`：`idx1` 索引对应的统计信息没有完全加载\n* `Succ`：表示语句是否执行成功。\n* `Backoff_time`：表示语句遇到需要重试的错误时在重试前等待的时间。常见的需要重试的错误有以下几种：遇到了 lock、Region 分裂、`tikv server is busy`。\n* `Plan`：表示语句的执行计划，用 `select tidb_decode_plan('xxx...')` SQL 语句可以解析出具体的执行计划。\n* `Binary_plan`：表示以二进制格式编码后的语句的执行计划，用 [`SELECT tidb_decode_binary_plan('xxx...')`](/functions-and-operators/tidb-functions.md#tidb_decode_binary_plan) SQL 语句可以解析出具体的执行计划。传递的信息和 `Plan` 字段基本相同，但是解析出的执行计划的格式会和 `Plan` 字段不同。\n* `Prepared`：表示这个语句是否是 `Prepare` 或 `Execute` 的请求。\n* `Plan_from_cache`：表示这个语句是否命中了执行计划缓存。\n* `Plan_from_binding`：表示这个语句是否用的绑定的执行计划。\n* `Has_more_results`：表示这个语句的查询结果是否还有更多的数据待用户发起 `fetch` 命令获取。\n* `Rewrite_time`：表示这个语句在查询改写阶段花费的时间。\n* `Preproc_subqueries`：表示这个语句中被提前执行的子查询个数，如 `where id in (select if from t)` 这个子查询就可能被提前执行。\n* `Preproc_subqueries_time`：表示这个语句中被提前执行的子查询耗时。\n* `Exec_retry_count`：表示这个语句执行的重试次数。一般出现在悲观事务中，上锁失败时重试执行该语句。\n* `Exec_retry_time`：表示这个语句的重试执行时间。例如某个查询一共执行了三次（前两次失败），则 `Exec_retry_time` 表示前两次的执行时间之和，`Query_time` 减去 `Exec_retry_time` 则为最后一次执行时间。\n* `KV_total`：表示这个语句在 TiKV/TiFlash 上所有 RPC 请求花费的时间。\n* `PD_total`：表示这个语句在 PD 上所有 RPC 请求花费的时间。\n* `Backoff_total`：表示这个语句在执行过程中所有 backoff 花费的时间。\n* `Write_sql_response_total`：表示这个语句把结果发送回客户端花费的时间。\n* `Result_rows`：表示这个语句查询结果的行数。\n* `Warnings`：表示这个语句执行过程中产生的警告，采用 JSON 格式。通常和 [`SHOW WARNINGS`](/sql-statements/sql-statement-show-warnings.md) 语句的输出结果一致，但是可能会包含 [`SHOW WARNINGS`](/sql-statements/sql-statement-show-warnings.md) 中没有的警告，因而可以提供更多诊断信息。这类警告将被标记为 `IsExtra: true`。\n* `IsExplicitTxn`：表示这个语句是否在一个明确声明的事务中。如果是 `false`，表示这个语句的事务是 `autocommit=1`，即语句执行完成后就自动提交的事务。\n\n和事务执行相关的字段：\n\n* `Prewrite_time`：表示事务两阶段提交中第一阶段（prewrite 阶段）的耗时。\n* `Commit_time`：表示事务两阶段提交中第二阶段（commit 阶段）的耗时。\n* `Get_commit_ts_time`：表示事务两阶段提交中第二阶段（commit 阶段）获取 commit 时间戳的耗时。\n* `Local_latch_wait_time`：表示事务两阶段提交中第二阶段（commit 阶段）发起前在 TiDB 侧等锁的耗时。\n* `Write_keys`：表示该事务向 TiKV 的 Write CF 写入 Key 的数量。\n* `Write_size`：表示事务提交时写 key 或 value 的总大小。\n* `Prewrite_region`：表示事务两阶段提交中第一阶段（prewrite 阶段）涉及的 TiKV Region 数量。每个 Region 会触发一次远程过程调用。\n* `Wait_prewrite_binlog_time`：表示事务提交时用于写 binlog 的时间。从 v8.4.0 开始，TiDB Binlog 已移除，不再有相关时间。\n* `Resolve_lock_time`：表示事务提交时遇到锁后，清理锁或者等待锁过期的时间。\n\n和内存使用相关的字段：\n\n* `Mem_max`：表示执行期间 TiDB 使用的最大内存空间，单位为 byte。\n\n和硬盘使用相关的字段：\n\n* `Disk_max`：表示执行期间 TiDB 使用的最大硬盘空间，单位为 byte。\n\n和 SQL 执行的用户相关的字段：\n\n* `User`：表示执行语句的用户名。\n* `Host`：表示执行语句的用户地址。\n* `Conn_ID`：表示用户的链接 ID，可以用类似 `con:3` 的关键字在 TiDB 日志中查找该链接相关的其他日志。\n* `DB`：表示执行语句时使用的 database。\n\n和 TiKV Coprocessor Task 相关的字段：\n\n* `Request_count`：表示这个语句发送的 Coprocessor 请求的数量。\n* `Total_keys`：表示 Coprocessor 扫过的 key 的数量。\n* `Process_time`：执行 SQL 在 TiKV 的处理时间之和，因为数据会并行的发到 TiKV 执行，这个值可能会超过 `Query_time`。\n* `Wait_time`：表示这个语句在 TiKV 的等待时间之和，因为 TiKV 的 Coprocessor 线程数是有限的，当所有的 Coprocessor 线程都在工作的时候，请求会排队；当队列中有某些请求耗时很长的时候，后面的请求的等待时间都会增加。\n* `Process_keys`：表示 Coprocessor 处理的 key 的数量。相比 total_keys，processed_keys 不包含 MVCC 的旧版本。如果 processed_keys 和 total_keys 相差很大，说明旧版本比较多。\n* `Num_cop_tasks`：表示这个语句发送的 Coprocessor 请求的数量。\n* `Cop_proc_avg`：cop-task 的平均执行时间，包括一些无法统计的等待时间，如 RocksDB 内的 mutex。\n* `Cop_proc_p90`：cop-task 的 P90 分位执行时间。\n* `Cop_proc_max`：cop-task 的最大执行时间。\n* `Cop_proc_addr`：执行时间最长的 cop-task 所在地址。\n* `Cop_wait_avg`：cop-task 的平均等待时间，包括请求排队和获取 snapshot 时间。\n* `Cop_wait_p90`：cop-task 的 P90 分位等待时间。\n* `Cop_wait_max`：cop-task 的最大等待时间。\n* `Cop_wait_addr`：等待时间最长的 cop-task 所在地址。\n* `Rocksdb_delete_skipped_count`：RocksDB 读数据过程中已删除 Key 的扫描数。\n* `Rocksdb_key_skipped_count`：RocksDB 扫数据时遇到的已删除 (tombstone) Key 数量。\n* `Rocksdb_block_cache_hit_count`：RocksDB 从 Block Cache 缓存中读数据的次数。\n* `Rocksdb_block_read_count`：RocksDB 从文件系统中读数据的次数。\n* `Rocksdb_block_read_byte`：RocksDB 从文件系统中读数据的数据量。\n* `Rocksdb_block_read_time`：RocksDB 从文件系统中读数据的时间。\n* `Cop_backoff_{backoff-type}_total_times`：因某种错误造成的 backoff 总次数。\n* `Cop_backoff_{backoff-type}_total_time`：因某种错误造成的 backoff 总时间。\n* `Cop_backoff_{backoff-type}_max_time`：因某种错误造成的最大 backoff 时间。\n* `Cop_backoff_{backoff-type}_max_addr`：因某种错误造成的最大 backoff 时间的 cop-task 地址。\n* `Cop_backoff_{backoff-type}_avg_time`：因某种错误造成的平均 backoff 时间。\n* `Cop_backoff_{backoff-type}_p90_time`：因某种错误造成的 P90 分位 backoff 时间。\n\n`backoff-type` 一般有以下几种：\n\n* `tikvRPC`：给 TiKV 发送 RPC 请求失败而产生的 backoff。\n* `tiflashRPC`：给 TiFlash 发送 RPC 请求失败而产生的 backoff。\n* `pdRPC`：给 PD 发送 RPC 请求失败而产生的 backoff。\n* `txnLock`：遇到锁冲突后产生的 backoff。\n* `regionMiss`：Region 发生分裂或者合并后，TiDB 的 Region 缓存信息过期导致请求失败而产生的 backoff。\n* `regionScheduling`：Region 还在调度中，尚未选出 Leader 导致无法处理请求而产生的 backoff。\n* `tikvServerBusy`：因为 TiKV 负载太高无法处理新请求而产生的 backoff。\n* `tiflashServerBusy`：因为 TiFlash 负载太高无法处理新请求而产生的 backoff。\n* `tikvDiskFull`：因为 TiKV 的磁盘满了而产生的 backoff。\n* `txnLockFast`：因为读数据时遇到了锁而产生的 backoff。\n\n和资源管控相关的字段：\n\n* `Resource_group`：语句执行所绑定的资源组。\n* `Request_unit_read`：执行语句消耗的总读 RU。\n* `Request_unit_write`：执行语句消耗的总写 RU。\n* `Time_queued_by_rc`：执行语句过程中等待可用资源的总耗时。\n\n## 相关系统变量\n\n* [tidb_slow_log_threshold](/system-variables.md#tidb_slow_log_threshold)：设置慢日志的阈值，执行时间超过阈值的 SQL 语句将被记录到慢日志中。默认值是 300 ms。\n* [tidb_query_log_max_len](/system-variables.md#tidb_query_log_max_len)：设置慢日志记录 SQL 语句的最大长度。默认值是 4096 byte。\n* [tidb_redact_log](/system-variables.md#tidb_redact_log)：设置慢日志记录 SQL 时是否将用户数据脱敏用 `?` 代替。默认值是 `0`，即关闭该功能。\n* [tidb_enable_collect_execution_info](/system-variables.md#tidb_enable_collect_execution_info)：设置是否记录执行计划中各个算子的物理执行信息，默认值是 `1`。该功能对性能的影响约为 3%。开启该项后查看 `Plan` 的示例如下：\n\n```sql\n> select tidb_decode_plan('jAOIMAk1XzE3CTAJMQlmdW5jczpjb3VudChDb2x1bW4jNyktPkMJC/BMNQkxCXRpbWU6MTAuOTMxNTA1bXMsIGxvb3BzOjIJMzcyIEJ5dGVzCU4vQQoxCTMyXzE4CTAJMQlpbmRleDpTdHJlYW1BZ2dfOQkxCXQRSAwyNzY4LkgALCwgcnBjIG51bTogMQkMEXMQODg0MzUFK0hwcm9jIGtleXM6MjUwMDcJMjA2HXsIMgk1BWM2zwAAMRnIADcVyAAxHcEQNQlOL0EBBPBbCjMJMTNfMTYJMQkzMTI4MS44NTc4MTk5MDUyMTcJdGFibGU6dCwgaW5kZXg6aWR4KGEpLCByYW5nZTpbLWluZiw1MDAwMCksIGtlZXAgb3JkZXI6ZmFsc2UJMjUBrgnQVnsA');\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| tidb_decode_plan('jAOIMAk1XzE3CTAJMQlmdW5jczpjb3VudChDb2x1bW4jNyktPkMJC/BMNQkxCXRpbWU6MTAuOTMxNTA1bXMsIGxvb3BzOjIJMzcyIEJ5dGVzCU4vQQoxCTMyXzE4CTAJMQlpbmRleDpTdHJlYW1BZ2dfOQkxCXQRSAwyNzY4LkgALCwgcnBjIG51bTogMQkMEXMQODg0MzUFK0hwcm9jIGtleXM6MjUwMDcJMjA2HXsIMg |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|     id                    task    estRows               operator info                                                  actRows    execution info                                                                  memory       disk                              |\n|     StreamAgg_17          root    1                     funcs:count(Column#7)->Column#5                                1          time:10.931505ms, loops:2                                                       372 Bytes    N/A                               |\n|     └─IndexReader_18      root    1                     index:StreamAgg_9                                              1          time:10.927685ms, loops:2, rpc num: 1, rpc time:10.884355ms, proc keys:25007    206 Bytes    N/A                               |\n|       └─StreamAgg_9       cop     1                     funcs:count(1)->Column#7                                       1          time:11ms, loops:25                                                             N/A          N/A                               |\n|         └─IndexScan_16    cop     31281.857819905217    table:t, index:idx(a), range:[-inf,50000), keep order:false    25007      time:11ms, loops:25                                                             N/A          N/A                               |\n+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n\n在性能测试中可以关闭自动收集算子的执行信息：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset @@tidb_enable_collect_execution_info=0;\n```\n\n`Plan` 字段显示的格式和 [`EXPLAIN`](/sql-statements/sql-statement-explain.md) 或者 [`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md) 大致一致。可以查看 [`EXPLAIN`](/sql-statements/sql-statement-explain.md) 或者 [`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md) 文档了解更多关于执行计划的信息。\n\n更多详细信息，可以参见 [TiDB 专用系统变量和语法](/system-variables.md)。\n\n## 慢日志内存映射表\n\n用户可通过查询 `INFORMATION_SCHEMA.SLOW_QUERY` 表来查询慢查询日志中的内容，表中列名和慢日志中字段名一一对应，表结构可查看 [`SLOW_QUERY` 表](/information-schema/information-schema-slow-query.md)中的介绍。\n\n> **注意：**\n>\n> 每次查询 `SLOW_QUERY` 表时，TiDB 都会去读取和解析一次当前的慢查询日志。\n\nTiDB 4.0 中，`SLOW_QUERY` 已经支持查询任意时间段的慢日志，即支持查询已经被 rotate 的慢日志文件的数据。用户查询时只需要指定 `TIME` 时间范围即可定位需要解析的慢日志文件。如果查询不指定时间范围，则仍然只解析当前的慢日志文件，示例如下：\n\n不指定时间范围时，只会解析当前 TiDB 正在写入的慢日志文件的慢查询数据：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect count(*),\n       min(time),\n       max(time)\nfrom slow_query;\n```\n\n```\n+----------+----------------------------+----------------------------+\n| count(*) | min(time)                  | max(time)                  |\n+----------+----------------------------+----------------------------+\n| 122492   | 2020-03-11 23:35:20.908574 | 2020-03-25 19:16:38.229035 |\n+----------+----------------------------+----------------------------+\n```\n\n指定查询 `2020-03-10 00:00:00` 到 `2020-03-11 00:00:00` 时间范围后，会定位指定时间范围内的慢日志文件后解析慢查询数据：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect count(*),\n       min(time),\n       max(time)\nfrom slow_query\nwhere time > '2020-03-10 00:00:00'\n  and time < '2020-03-11 00:00:00';\n```\n\n```\n+----------+----------------------------+----------------------------+\n| count(*) | min(time)                  | max(time)                  |\n+----------+----------------------------+----------------------------+\n| 2618049  | 2020-03-10 00:00:00.427138 | 2020-03-10 23:00:22.716728 |\n+----------+----------------------------+----------------------------+\n```\n\n> **注意：**\n>\n> 如果指定时间范围内的慢日志文件被删除，或者并没有慢查询，则查询结果会返回空。\n\nTiDB 4.0 中新增了 [`CLUSTER_SLOW_QUERY`](/information-schema/information-schema-slow-query.md#cluster_slow_query-table) 系统表，用来查询所有 TiDB 节点的慢查询信息，表结构在 `SLOW_QUERY` 的基础上多增加了 `INSTANCE` 列，表示该行慢查询信息来自的 TiDB 节点地址。使用方式和 [`SLOW_QUERY`](/information-schema/information-schema-slow-query.md) 系统表一样。\n\n关于查询 `CLUSTER_SLOW_QUERY` 表，TiDB 会把相关的计算和判断下推到其他节点执行，而不是把其他节点的慢查询数据都取回来在一台 TiDB 上执行。\n\n## 查询 `SLOW_QUERY`/`CLUSTER_SLOW_QUERY` 示例\n\n### 搜索 Top N 的慢查询\n\n查询 Top 2 的用户慢查询。`is_internal=false` 表示排除 TiDB 内部的慢查询，只看用户的慢查询：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect query_time, query\nfrom information_schema.slow_query\nwhere is_internal = false  -- 排除 TiDB 内部的慢查询 SQL\norder by query_time desc\nlimit 2;\n```\n\n输出样例：\n\n```\n+--------------+------------------------------------------------------------------+\n| query_time   | query                                                            |\n+--------------+------------------------------------------------------------------+\n| 12.77583857  | select * from t_slim, t_wide where t_slim.c0=t_wide.c0;          |\n|  0.734982725 | select t0.c0, t1.c1 from t_slim t0, t_wide t1 where t0.c0=t1.c0; |\n+--------------+------------------------------------------------------------------+\n```\n\n### 搜索某个用户的 Top N 慢查询\n\n下面例子中搜索 test 用户执行的慢查询 SQL，且按执行消耗时间逆序排序显式前 2 条：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect query_time, query, user\nfrom information_schema.slow_query\nwhere is_internal = false  -- 排除 TiDB 内部的慢查询 SQL\n  and user = \"test\"        -- 查找的用户名\norder by query_time desc\nlimit 2;\n```\n\n输出样例：\n\n```\n+-------------+------------------------------------------------------------------+----------------+\n| Query_time  | query                                                            | user           |\n+-------------+------------------------------------------------------------------+----------------+\n| 0.676408014 | select t0.c0, t1.c1 from t_slim t0, t_wide t1 where t0.c0=t1.c1; | test           |\n+-------------+------------------------------------------------------------------+----------------+\n```\n\n### 根据 SQL 指纹搜索同类慢查询\n\n在得到 Top N 的慢查询 SQL 后，可通过 SQL 指纹继续搜索同类慢查询 SQL。\n\n先获取 Top N 的慢查询和对应的 SQL 指纹：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect query_time, query, digest\nfrom information_schema.slow_query\nwhere is_internal = false\norder by query_time desc\nlimit 1;\n```\n\n输出样例：\n\n```\n+-------------+-----------------------------+------------------------------------------------------------------+\n| query_time  | query                       | digest                                                           |\n+-------------+-----------------------------+------------------------------------------------------------------+\n| 0.302558006 | select * from t1 where a=1; | 4751cb6008fda383e22dacb601fde85425dc8f8cf669338d55d944bafb46a6fa |\n+-------------+-----------------------------+------------------------------------------------------------------+\n```\n\n再根据 SQL 指纹搜索同类慢查询：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect query, query_time\nfrom information_schema.slow_query\nwhere digest = \"4751cb6008fda383e22dacb601fde85425dc8f8cf669338d55d944bafb46a6fa\";\n```\n\n输出样例：\n\n```\n+-----------------------------+-------------+\n| query                       | query_time  |\n+-----------------------------+-------------+\n| select * from t1 where a=1; | 0.302558006 |\n| select * from t1 where a=2; | 0.401313532 |\n+-----------------------------+-------------+\n```\n\n### 搜索统计信息为 pseudo 的慢查询 SQL 语句\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect query, query_time, stats\nfrom information_schema.slow_query\nwhere is_internal = false\n  and stats like '%pseudo%';\n```\n\n输出样例：\n\n```\n+-----------------------------+-------------+---------------------------------+\n| query                       | query_time  | stats                           |\n+-----------------------------+-------------+---------------------------------+\n| select * from t1 where a=1; | 0.302558006 | t1:pseudo                       |\n| select * from t1 where a=2; | 0.401313532 | t1:pseudo                       |\n| select * from t1 where a>2; | 0.602011247 | t1:pseudo                       |\n| select * from t1 where a>3; | 0.50077719  | t1:pseudo                       |\n| select * from t1 join t2;   | 0.931260518 | t1:407872303825682445,t2:pseudo |\n+-----------------------------+-------------+---------------------------------+\n```\n\n### 查询执行计划发生变化的慢查询\n\n由于统计信息过时，或者统计信息因为误差无法精确反映数据的真实分布情况时，可能导致同类型 SQL 的执行计划发生改变导致执行变慢，可以用以下 SQL 查询哪些 SQL 具有不同的执行计划：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect count(distinct plan_digest) as count,\n       digest,\n       min(query)\nfrom cluster_slow_query\ngroup by digest\nhaving count > 1\nlimit 3\\G\n```\n\n输出样例：\n\n```\n***************************[ 1. row ]***************************\ncount      | 2\ndigest     | 17b4518fde82e32021877878bec2bb309619d384fca944106fcaf9c93b536e94\nmin(query) | SELECT DISTINCT c FROM sbtest25 WHERE id BETWEEN ? AND ? ORDER BY c [arguments: (291638, 291737)];\n***************************[ 2. row ]***************************\ncount      | 2\ndigest     | 9337865f3e2ee71c1c2e740e773b6dd85f23ad00f8fa1f11a795e62e15fc9b23\nmin(query) | SELECT DISTINCT c FROM sbtest22 WHERE id BETWEEN ? AND ? ORDER BY c [arguments: (215420, 215519)];\n***************************[ 3. row ]***************************\ncount      | 2\ndigest     | db705c89ca2dfc1d39d10e0f30f285cbbadec7e24da4f15af461b148d8ffb020\nmin(query) | SELECT DISTINCT c FROM sbtest11 WHERE id BETWEEN ? AND ? ORDER BY c [arguments: (303359, 303458)];\n```\n\n然后可以用查询结果中的 SQL 指纹进一步查询不同的 plan\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect min(plan),\n       plan_digest\nfrom cluster_slow_query\nwhere digest='17b4518fde82e32021877878bec2bb309619d384fca944106fcaf9c93b536e94'\ngroup by plan_digest\\G\n```\n\n输出样例：\n\n```\n*************************** 1. row ***************************\n  min(plan):    Sort_6                  root    100.00131380758702      sbtest.sbtest25.c:asc\n        └─HashAgg_10            root    100.00131380758702      group by:sbtest.sbtest25.c, funcs:firstrow(sbtest.sbtest25.c)->sbtest.sbtest25.c\n          └─TableReader_15      root    100.00131380758702      data:TableRangeScan_14\n            └─TableScan_14      cop     100.00131380758702      table:sbtest25, range:[502791,502890], keep order:false\nplan_digest: 6afbbd21f60ca6c6fdf3d3cd94f7c7a49dd93c00fcf8774646da492e50e204ee\n*************************** 2. row ***************************\n  min(plan):    Sort_6                  root    1                       sbtest.sbtest25.c:asc\n        └─HashAgg_12            root    1                       group by:sbtest.sbtest25.c, funcs:firstrow(sbtest.sbtest25.c)->sbtest.sbtest25.c\n          └─TableReader_13      root    1                       data:HashAgg_8\n            └─HashAgg_8         cop     1                       group by:sbtest.sbtest25.c,\n              └─TableScan_11    cop     1.2440069558121831      table:sbtest25, range:[472745,472844], keep order:false\n```\n\n### 查询集群各个 TIDB 节点的慢查询数量\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect instance, count(*) from information_schema.cluster_slow_query where time >= \"2020-03-06 00:00:00\" and time < now() group by instance;\n```\n\n输出样例：\n\n```\n+---------------+----------+\n| instance      | count(*) |\n+---------------+----------+\n| 0.0.0.0:10081 | 124      |\n| 0.0.0.0:10080 | 119771   |\n+---------------+----------+\n```\n\n### 查询仅出现在异常时间段的慢日志\n\n假如发现 `2020-03-10 13:24:00` ~ `2020-03-10 13:27:00` 的 QPS 降低或者延迟上升等问题，可能是由于突然出现大查询导致的，可以用下面 SQL 查询仅出现在异常时间段的慢日志，其中 `2020-03-10 13:20:00` ~ `2020-03-10 13:23:00` 为正常时间段。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM\n    (SELECT /*+ AGG_TO_COP(), HASH_AGG() */ count(*),\n         min(time),\n         sum(query_time) AS sum_query_time,\n         sum(Process_time) AS sum_process_time,\n         sum(Wait_time) AS sum_wait_time,\n         sum(Commit_time),\n         sum(Request_count),\n         sum(process_keys),\n         sum(Write_keys),\n         max(Cop_proc_max),\n         min(query),min(prev_stmt),\n         digest\n    FROM information_schema.CLUSTER_SLOW_QUERY\n    WHERE time >= '2020-03-10 13:24:00'\n            AND time < '2020-03-10 13:27:00'\n            AND Is_internal = false\n    GROUP BY  digest) AS t1\nWHERE t1.digest NOT IN\n    (SELECT /*+ AGG_TO_COP(), HASH_AGG() */ digest\n    FROM information_schema.CLUSTER_SLOW_QUERY\n    WHERE time >= '2020-03-10 13:20:00'\n            AND time < '2020-03-10 13:23:00'\n    GROUP BY  digest)\nORDER BY  t1.sum_query_time DESC limit 10\\G\n```\n\n输出样例：\n\n```\n***************************[ 1. row ]***************************\ncount(*)           | 200\nmin(time)          | 2020-03-10 13:24:27.216186\nsum_query_time     | 50.114126194\nsum_process_time   | 268.351\nsum_wait_time      | 8.476\nsum(Commit_time)   | 1.044304306\nsum(Request_count) | 6077\nsum(process_keys)  | 202871950\nsum(Write_keys)    | 319500\nmax(Cop_proc_max)  | 0.263\nmin(query)         | delete from test.tcs2 limit 5000;\nmin(prev_stmt)     |\ndigest             | 24bd6d8a9b238086c9b8c3d240ad4ef32f79ce94cf5a468c0b8fe1eb5f8d03df\n```\n\n## 解析其他的 TiDB 慢日志文件\n\nTiDB 通过 session 变量 `tidb_slow_query_file` 控制查询 `INFORMATION_SCHEMA.SLOW_QUERY` 时要读取和解析的文件，可通过修改改 session 变量的值来查询其他慢查询日志文件的内容：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset tidb_slow_query_file = \"/path-to-log/tidb-slow.log\"\n```\n\n## 用 `pt-query-digest` 工具分析 TiDB 慢日志\n\n可以用 `pt-query-digest` 工具分析 TiDB 慢日志。\n\n> **注意：**\n>\n> 建议使用 pt-query-digest 3.0.13 及以上版本。\n\n示例如下：\n\n{{< copyable \"shell\" >}}\n\n```shell\npt-query-digest --report tidb-slow.log\n```\n\n输出样例：\n\n```\n# 320ms user time, 20ms system time, 27.00M rss, 221.32M vsz\n# Current date: Mon Mar 18 13:18:51 2019\n# Hostname: localhost.localdomain\n# Files: tidb-slow.log\n# Overall: 1.02k total, 21 unique, 0 QPS, 0x concurrency _________________\n# Time range: 2019-03-18-12:22:16 to 2019-03-18-13:08:52\n# Attribute          total     min     max     avg     95%  stddev  median\n# ============     ======= ======= ======= ======= ======= ======= =======\n# Exec time           218s    10ms     13s   213ms    30ms      1s    19ms\n# Query size       175.37k       9   2.01k  175.89  158.58  122.36  158.58\n# Commit time         46ms     2ms     7ms     3ms     7ms     1ms     3ms\n# Conn ID               71       1      16    8.88   15.25    4.06    9.83\n# Process keys     581.87k       2 103.15k  596.43  400.73   3.91k  400.73\n# Process time         31s     1ms     10s    32ms    19ms   334ms    16ms\n# Request coun       1.97k       1      10    2.02    1.96    0.33    1.96\n# Total keys       636.43k       2 103.16k  652.35  793.42   3.97k  400.73\n# Txn start ts     374.38E       0  16.00E 375.48P   1.25P  89.05T   1.25P\n# Wait time          943ms     1ms    19ms     1ms     2ms     1ms   972us\n.\n.\n.\n```\n\n### 定位问题语句的方法\n\n并不是所有 SLOW_QUERY 的语句都是有问题的。会造成集群整体压力增大的，是那些 process_time 很大的语句。wait_time 很大，但 process_time 很小的语句通常不是问题语句，是因为被问题语句阻塞，在执行队列等待造成的响应时间过长。\n\n## `ADMIN SHOW SLOW` 命令\n\n除了获取 TiDB 日志，还有一种定位慢查询的方式是通过 `ADMIN SHOW SLOW` SQL 命令：\n\n{{< copyable \"sql\" >}}\n\n```sql\nADMIN SHOW SLOW recent N;\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nADMIN SHOW SLOW TOP [internal | all] N;\n```\n\n`recent N` 会显示最近的 N 条慢查询记录，例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nADMIN SHOW SLOW recent 10;\n```\n\n`top N` 则显示最近一段时间（大约几天）内，最慢的查询记录。如果指定 `internal` 选项，则返回查询系统内部 SQL 的慢查询记录；如果指定 `all` 选项，返回系统内部和用户 SQL 汇总以后的慢查询记录；默认只返回用户 SQL 中的慢查询记录。\n\n{{< copyable \"sql\" >}}\n\n```sql\nADMIN SHOW SLOW TOP 3;\nADMIN SHOW SLOW TOP internal 3;\nADMIN SHOW SLOW TOP all 5;\n```\n\n由于内存限制，保留的慢查询记录的条数是有限的。当命令查询的 `N` 大于记录条数时，返回的结果记录条数会小于 `N`。\n\n输出内容详细说明，如下：\n\n| 列名 | 描述 |\n|:------|:---- |\n| start | SQL 语句执行开始时间 |\n| duration | SQL 语句执行持续时间 |\n| details | 执行语句的详细信息 |\n| succ | SQL 语句执行是否成功，1：成功，0：失败 |\n| conn_id | session 连接 ID |\n| transaction_ts | 事务的 start ts |\n| user | 执行该语句的用户名 |\n| db | 执行该 SQL 涉及到 database |\n| table_ids | 执行该 SQL 涉及到表的 ID |\n| index_ids | 执行该 SQL 涉及到索引 ID |\n| internal | 表示为 TiDB 内部的 SQL 语句 |\n| digest | 表示 SQL 语句的指纹 |\n| sql | 执行的 SQL 语句 |\n"
        },
        {
          "name": "information-schema",
          "type": "tree",
          "content": null
        },
        {
          "name": "integration-overview.md",
          "type": "blob",
          "size": 1.2041015625,
          "content": "---\ntitle: 数据集成概述\nsummary: 了解使用 TiCDC 进行数据集成的具体场景。\n---\n\n# 数据集成概述\n\n数据集成一般是指数据在各个独立的数据源之间流动、转换和汇集。随着数据量的爆炸式增长和数据价值被深度挖掘，对数据集成的需求越来越普遍和迫切。为了避免 TiDB 成为数据孤岛，顺利与各个数据系统进行集成，TiCDC 提供将 TiDB 增量数据变更日志实时同步到其他数据系统的能力。本文介绍一些常用的数据集成场景，你可以依据这些场景选择最适合自己的数据集成方案。\n\n## 与 Confluent Cloud 和 Snowflake 进行数据集成\n\n你可以使用 TiCDC 将 TiDB 的增量数据同步到 Confluent Cloud，并借助 Confluent Cloud 的能力最终将数据分别同步到 Snowflake、ksqlDB、SQL Server。参见[与 Confluent Cloud 和 Snowflake 进行数据集成](/ticdc/integrate-confluent-using-ticdc.md)。\n\n## 与 Apache Kafka 和 Apache Flink 进行数据集成\n\n你可以使用 TiCDC 将 TiDB 的增量数据同步到 Apache Kafka，并使用 Apache Flink 消费 Kafka 中的数据。参见[与 Apache Kafka 和 Apache Flink 进行数据集成](/replicate-data-to-kafka.md)。\n"
        },
        {
          "name": "join-reorder.md",
          "type": "blob",
          "size": 3.6220703125,
          "content": "---\ntitle: Join Reorder 算法简介\naliases: ['/docs-cn/dev/join-reorder/','/docs-cn/dev/reference/performance/join-reorder/']\nsummary: Join Reorder 算法决定了多表 Join 的顺序，影响执行效率。TiDB 中有贪心算法和动态规划算法两种实现。贪心算法选择行数最小的表与其他表做 Join，直到所有节点完成 Join。动态规划算法枚举所有可能的 Join 顺序，选择最优的。算法受系统变量控制，且存在一些限制，如无法保证一定选到合适的 Join 顺序。\n---\n\n# Join Reorder 算法简介\n\n在实际的业务场景中，多个表的 Join 语句是很常见的，而 Join 的执行效率和各个表参与 Join 的顺序有关系。如 `select * from t1, t2, t3 where t1.a=t2.a and t3.a=t2.a`，这个 SQL 中可能的执行顺序有“t1 和 t2 先做 Join，然后再和 t3 做 Join”以及“t2 和 t3 先做 Join，然后再和 t1 做 Join”两种情况。根据 `t1` 和 `t3` 的数据量及数据分布，这两种执行顺序会有不同的性能表现。\n\n因此优化器需要实现一种决定 Join 顺序的算法。目前 TiDB 中存在两种 Join Reorder 算法，贪心算法和动态规划算法。\n\n- Join Reorder 贪心算法：在所有参与 Join 的节点中，选择行数最小的表与其他各表分别做一次 Join 的结果估算，然后选择其中结果最小的一对进行 Join，再继续这个过程进入下一轮的选择和 Join，直到所有的节点都完成 Join。\n- Join Reorder 动态规划算法：在所有参与 Join 的节点中，枚举所有可能的 Join 顺序，然后选择最优的 Join 顺序。\n\n## Join Reorder 贪心算法实例\n\n以三个表 t1、t2、t3 的 Join 为例。首先获取所有参与 Join 的节点，将所有节点按照行数多少，从少到多进行排序。\n\n![join-reorder-1](/media/join-reorder-1.png)\n\n之后选定其中最小的表，将其与其他两个表分别做一次 Join，观察输出的结果集大小，选择其中结果更小的一对。\n\n![join-reorder-2](/media/join-reorder-2.png)\n\n然后进入下一轮的选择，如果这时是四个表，那么就继续比较输出结果集的大小，进行选择。这里只有三个表，因此就直接得到了最终的 Join 结果。\n\n![join-reorder-3](/media/join-reorder-3.png)\n\n## Join Reorder 动态规划算法实例\n\n仍然以上述例子为例。动态规划算法会枚举所有的可能性，因此相对贪心算法必须从 `t1` 表开始枚举，动态规划算法可以枚举如下的 Join 顺序。\n\n![join-reorder-4](/media/join-reorder-4.png)\n\n当该选择比贪心算法更优时，动态规划算法便可以选择到更优的 Join 顺序。\n\n相应地，因为会枚举所有的可能性，动态规划算法会消耗更多的时间，也会更容易受统计信息影响。\n\n## Join Reorder 算法的控制\n\n目前 Join Reorder 算法由变量 [`tidb_opt_join_reorder_threshold`](/system-variables.md#tidb_opt_join_reorder_threshold) 控制，当参与 Join Reorder 的节点个数大于该阈值时选择贪心算法，反之选择动态规划算法。\n\n## Join Reorder 算法限制\n\n当前的 Join Reorder 算法存在如下限制：\n\n- 受结果集的计算算法所限并不会保证一定会选到合适的 Join order\n- 是否启用 Outer Join 的 Join Reorder 功能由系统变量 [`tidb_enable_outer_join_reorder`](/system-variables.md#tidb_enable_outer_join_reorder-从-v610-版本开始引入) 控制。\n- 目前动态规划算法无法进行 Outer Join 的 Join Reorder。\n\n目前 TiDB 中支持使用 `STRAIGHT_JOIN` 语法来强制指定一种 Join 顺序，参见[语法元素说明](/sql-statements/sql-statement-select.md#语法元素说明)。\n"
        },
        {
          "name": "keywords.md",
          "type": "blob",
          "size": 11.1552734375,
          "content": "---\ntitle: 关键字\nsummary: 本文介绍 TiDB 的关键字。\naliases: ['/docs-cn/dev/keywords/','/docs-cn/dev/keywords-and-reserved-words/','/docs-cn/dev/reference/sql/language-structure/keywords-and-reserved-words/']\n---\n\n# 关键字\n\n本文介绍 TiDB 的关键字，对保留字和非保留字作出区分，并汇总所有的关键字以供查询使用。\n\n关键字是 SQL 语句中具有特殊含义的单词，例如  [`SELECT`](/sql-statements/sql-statement-select.md)、[`UPDATE`](/sql-statements/sql-statement-update.md) 和 [`DELETE`](/sql-statements/sql-statement-delete.md) 等等。它们之中有的能够直接作为标识符，被称为**非保留关键字**（简称**非保留字**），但有需要经过特殊处理才能作为标识符的字，被称为**保留关键字**（简称**保留字**）。但是，也存在一些特殊的非保留关键字，有时候可能也需要进行特殊处理，推荐你将它们当作保留关键字处理。\n\n对于保留字，必须使用反引号包裹，才能作为标识符被使用。例如：\n\n```sql\nCREATE TABLE select (a INT);\n```\n\n```\nERROR 1105 (HY000): line 0 column 19 near \" (a INT)\" (total length 27)\n```\n\n```sql\nCREATE TABLE `select` (a INT);\n```\n\n```\nQuery OK, 0 rows affected (0.09 sec)\n```\n\n而非保留字则不需要反引号也能直接作为标识符。例如 `BEGIN` 和 `END` 是非保留字，以下语句能够正常执行：\n\n```sql\nCREATE TABLE `select` (BEGIN int, END int);\n```\n\n```\nQuery OK, 0 rows affected (0.09 sec)\n```\n\n有一种特殊情况，如果使用了限定符 `.`，那么也不需要用反引号：\n\n```sql\nCREATE TABLE test.select (BEGIN int, END int);\n```\n\n```\nQuery OK, 0 rows affected (0.08 sec)\n```\n\nTiDB 从 v7.5.3 和 v7.6.0 开始提供 [`INFORMATION_SCHEMA.KEYWORDS`](/information-schema/information-schema-keywords.md) 表，可以用于查询 TiDB 中所有的关键字。\n\n## 关键字列表\n\n下表列出了 TiDB 中所有的关键字。其中保留字用 `(R)` 来标识。[窗口函数](/functions-and-operators/window-functions.md)的保留字用 `(R-Window)` 来标识。需要用反引号 `` ` `` 包裹的特殊非保留字用 `(S)` 来标识。\n\n<TabsPanel letters=\"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" />\n\n<a id=\"A\" class=\"letter\" href=\"#A\">A</a>\n\n- ACCOUNT\n- ACTION\n- ADD (R)\n- ADMIN\n- ADVISE\n- AFTER\n- AGAINST\n- AGO\n- ALGORITHM\n- ALL (R)\n- ALTER (R)\n- ALWAYS\n- ANALYZE (R)\n- AND (R)\n- ANY\n- APPLY\n- ARRAY (R)\n- AS (R)\n- ASC (R)\n- ASCII\n- ATTRIBUTE\n- ATTRIBUTES\n- AUTO_ID_CACHE\n- AUTO_INCREMENT\n- AUTO_RANDOM\n- AUTO_RANDOM_BASE\n- AVG\n- AVG_ROW_LENGTH\n\n<a id=\"B\" class=\"letter\" href=\"#B\">B</a>\n\n- BACKEND\n- BACKUP\n- BACKUPS\n- BATCH\n- BDR\n- BEGIN\n- BERNOULLI\n- BETWEEN (R)\n- BIGINT (R)\n- BINARY (R)\n- BINDING\n- BINDINGS\n- BINDING_CACHE\n- BINLOG\n- BIT\n- BLOB (R)\n- BLOCK\n- BOOL\n- BOOLEAN\n- BOTH (R)\n- BTREE\n- BUCKETS\n- BUILTINS\n- BY (R)\n- BYTE\n\n<a id=\"C\" class=\"letter\" href=\"#C\">C</a>\n\n- CACHE\n- CALIBRATE\n- CALL (R)\n- CANCEL\n- CAPTURE\n- CARDINALITY\n- CASCADE (R)\n- CASCADED\n- CASE (R)\n- CAUSAL\n- CHAIN\n- CHANGE (R)\n- CHAR (R)\n- CHARACTER (R)\n- CHARSET\n- CHECK (R)\n- CHECKPOINT\n- CHECKSUM\n- CHECKSUM_CONCURRENCY\n- CIPHER\n- CLEANUP\n- CLIENT\n- CLIENT_ERRORS_SUMMARY\n- CLOSE\n- CLUSTER\n- CLUSTERED\n- CMSKETCH\n- COALESCE\n- COLLATE (R)\n- COLLATION\n- COLUMN (R)\n- COLUMN_FORMAT\n- COLUMN_STATS_USAGE\n- COLUMNS\n- COMMENT\n- COMMIT\n- COMMITTED\n- COMPACT\n- COMPRESSED\n- COMPRESSION\n- COMPRESSION_LEVEL\n- COMPRESSION_TYPE\n- CONCURRENCY\n- CONFIG\n- CONNECTION\n- CONSISTENCY\n- CONSISTENT\n- CONSTRAINT (R)\n- CONTEXT\n- CONTINUE (R)\n- CONVERT (R)\n- CORRELATION\n- CPU\n- CREATE (R)\n- CROSS (R)\n- CSV_BACKSLASH_ESCAPE\n- CSV_DELIMITER\n- CSV_HEADER\n- CSV_NOT_NULL\n- CSV_NULL\n- CSV_SEPARATOR\n- CSV_TRIM_LAST_SEPARATORS\n- CUME_DIST (R-Window)\n- CURRENT\n- CURRENT_DATE (R)\n- CURRENT_ROLE (R)\n- CURRENT_TIME (R)\n- CURRENT_TIMESTAMP (R)\n- CURRENT_USER (R)\n- CURSOR (R)\n- CYCLE\n\n<a id=\"D\" class=\"letter\" href=\"#D\">D</a>\n\n- DATA\n- DATABASE (R)\n- DATABASES (R)\n- DATE\n- DATETIME\n- DAY\n- DAY_HOUR (R)\n- DAY_MICROSECOND (R)\n- DAY_MINUTE (R)\n- DAY_SECOND (R)\n- DDL\n- DEALLOCATE\n- DECIMAL (R)\n- DECLARE\n- DEFAULT (R)\n- DEFINER\n- DELAY_KEY_WRITE\n- DELAYED (R)\n- DELETE (R)\n- DENSE_RANK (R-Window)\n- DEPENDENCY\n- DEPTH\n- DESC (R)\n- DESCRIBE (R)\n- DIGEST\n- DIRECTORY\n- DISABLE\n- DISABLED\n- DISCARD\n- DISK\n- DISTINCT (R)\n- DISTINCTROW (R)\n- DIV (R)\n- DO\n- DOUBLE (R)\n- DRAINER\n- DROP (R)\n- DRY\n- DUAL (R)\n- DUPLICATE\n- DYNAMIC\n\n<a id=\"E\" class=\"letter\" href=\"#E\">E</a>\n\n- ELSE (R)\n- ELSEIF (R)\n- ENABLE\n- ENABLED\n- ENCLOSED (R)\n- ENCRYPTION\n- ENCRYPTION_KEYFILE\n- ENCRYPTION_METHOD\n- END\n- ENFORCED\n- ENGINE\n- ENGINES\n- ENUM\n- ERROR\n- ERRORS\n- ESCAPE\n- ESCAPED (R)\n- EVENT\n- EVENTS\n- EVOLVE\n- EXCEPT (R)\n- EXCHANGE\n- EXCLUSIVE\n- EXECUTE\n- EXISTS (R)\n- EXIT (R)\n- EXPANSION\n- EXPIRE\n- EXPLAIN (R)\n- EXTENDED\n\n<a id=\"F\" class=\"letter\" href=\"#F\">F</a>\n\n- FAILED_LOGIN_ATTEMPTS\n- FALSE (R)\n- FAULTS\n- FETCH (R)\n- FIELDS\n- FILE\n- FIRST\n- FIRST_VALUE (R-Window)\n- FIXED\n- FLOAT (R)\n- FLOAT4 (R)\n- FLOAT8 (R)\n- FLUSH\n- FOLLOWING\n- FOR (R)\n- FORCE (R)\n- FOREIGN (R)\n- FORMAT\n- FOUND\n- FROM (R)\n- FULL\n- FULLTEXT (R)\n- FUNCTION\n\n<a id=\"G\" class=\"letter\" href=\"#G\">G</a>\n\n- GENERAL\n- GENERATED (R)\n- GLOBAL\n- GRANT (R)\n- GRANTS\n- GROUP (R)\n- GROUPS (R-Window)\n\n<a id=\"H\" class=\"letter\" href=\"#H\">H</a>\n\n- HANDLER\n- HASH\n- HAVING (R)\n- HELP\n- HIGH_PRIORITY (R)\n- HISTOGRAM\n- HISTOGRAMS_IN_FLIGHT\n- HISTORY\n- HOSTS\n- HOUR\n- HOUR_MICROSECOND (R)\n- HOUR_MINUTE (R)\n- HOUR_SECOND (R)\n- HYPO\n\n<a id=\"I\" class=\"letter\" href=\"#I\">I</a>\n\n- IDENTIFIED\n- IF (R)\n- IGNORE (R)\n- IGNORE_STATS\n- ILIKE (R)\n- IMPORT\n- IMPORTS\n- IN (R)\n- INCREMENT\n- INCREMENTAL\n- INDEX (R)\n- INDEXES\n- INFILE (R)\n- INNER (R)\n- INOUT (R)\n- INSERT (R)\n- INSERT_METHOD\n- INSTANCE\n- INT (R)\n- INT1 (R)\n- INT2 (R)\n- INT3 (R)\n- INT4 (R)\n- INT8 (R)\n- INTEGER (R)\n- INTERSECT (R)\n- INTERVAL (R)\n- INTO (R)\n- INVISIBLE\n- INVOKER\n- IO\n- IPC\n- IS (R)\n- ISOLATION\n- ISSUER\n- ITERATE (R)\n\n<a id=\"J\" class=\"letter\" href=\"#J\">J</a>\n\n- JOB\n- JOBS\n- JOIN (R)\n- JSON\n\n<a id=\"K\" class=\"letter\" href=\"#K\">K</a>\n\n- KEY (R)\n- KEYS (R)\n- KEY_BLOCK_SIZE\n- KILL (R)\n\n<a id=\"L\" class=\"letter\" href=\"#L\">L</a>\n\n- LABELS\n- LAG (R-Window)\n- LANGUAGE\n- LAST\n- LAST_BACKUP\n- LAST_VALUE (R-Window)\n- LASTVAL\n- LEAD (R-Window)\n- LEADING (R)\n- LEAVE (R)\n- LEFT (R)\n- LESS\n- LEVEL\n- LIKE (R)\n- LIMIT (R)\n- LINEAR (R)\n- LINES (R)\n- LIST\n- LOAD (R)\n- LOAD_STATS\n- LOCAL\n- LOCALTIME (R)\n- LOCALTIMESTAMP (R)\n- LOCATION\n- LOCK (R)\n- LOCKED\n- LOGS\n- LONG (R)\n- LONGBLOB (R)\n- LONGTEXT (R)\n- LOW_PRIORITY (R)\n\n<a id=\"M\" class=\"letter\" href=\"#M\">M</a>\n\n- MASTER\n- MATCH (R)\n- MAXVALUE (R)\n- MAX_CONNECTIONS_PER_HOUR\n- MAX_IDXNUM\n- MAX_MINUTES\n- MAX_QUERIES_PER_HOUR\n- MAX_ROWS\n- MAX_UPDATES_PER_HOUR\n- MAX_USER_CONNECTIONS\n- MB\n- MEDIUMBLOB (R)\n- MEDIUMINT (R)\n- MEDIUMTEXT (R)\n- MEMBER\n- MEMORY\n- MERGE\n- MICROSECOND\n- MIDDLEINT (R)\n- MINUTE\n- MINUTE_MICROSECOND (R)\n- MINUTE_SECOND (R)\n- MINVALUE\n- MIN_ROWS\n- MOD (R)\n- MODE\n- MODIFY\n- MONTH\n\n<a id=\"N\" class=\"letter\" href=\"#N\">N</a>\n\n- NAMES\n- NATIONAL\n- NATURAL (R)\n- NCHAR\n- NEVER\n- NEXT\n- NEXTVAL\n- NO\n- NOCACHE\n- NOCYCLE\n- NODEGROUP\n- NODE_ID\n- NODE_STATE\n- NOMAXVALUE\n- NOMINVALUE\n- NONCLUSTERED\n- NONE\n- NOT (R)\n- NOWAIT\n- NO_WRITE_TO_BINLOG (R)\n- NTH_VALUE (R-Window)\n- NTILE (R-Window)\n- NULL (R)\n- NULLS\n- NUMERIC (R)\n- NVARCHAR\n\n<a id=\"O\" class=\"letter\" href=\"#O\">O</a>\n\n- OF (R)\n- OFF\n- OFFSET\n- OLTP_READ_ONLY\n- OLTP_READ_WRITE\n- OLTP_WRITE_ONLY\n- ON (R)\n- ON_DUPLICATE\n- ONLINE\n- ONLY\n- OPEN\n- OPTIMISTIC\n- OPTIMIZE (R)\n- OPTION (R)\n- OPTIONAL\n- OPTIONALLY (R)\n- OR (R)\n- ORDER (R)\n- OUT (R)\n- OUTER (R)\n- OUTFILE (R)\n- OVER (R-Window)\n\n<a id=\"P\" class=\"letter\" href=\"#P\">P</a>\n\n- PACK_KEYS\n- PAGE\n- PARSER\n- PARTIAL\n- PARTITION (R)\n- PARTITIONING\n- PARTITIONS\n- PASSWORD\n- PASSWORD_LOCK_TIME\n- PAUSE\n- PERCENT\n- PERCENT_RANK (R-Window)\n- PER_DB\n- PER_TABLE\n- PESSIMISTIC\n- PLUGINS\n- POINT\n- POLICY\n- PRECEDING\n- PRECISION (R)\n- PREPARE\n- PRESERVE\n- PRE_SPLIT_REGIONS\n- PRIMARY (R)\n- PRIVILEGES\n- PROCEDURE (R)\n- PROCESS\n- PROCESSLIST\n- PROFILE\n- PROFILES\n- PROXY\n- PUMP\n- PURGE\n\n<a id=\"Q\" class=\"letter\" href=\"#Q\">Q</a>\n\n- QUARTER\n- QUERIES\n- QUERY\n- QUICK\n\n<a id=\"R\" class=\"letter\" href=\"#R\">R</a>\n\n- RANGE (R)\n- RANK (R-Window)\n- RATE_LIMIT\n- READ (R)\n- REAL (R)\n- REBUILD\n- RECOMMEND\n- RECOVER\n- RECURSIVE (R)\n- REDUNDANT\n- REFERENCES (R)\n- REGEXP (R)\n- REGION\n- REGIONS\n- RELEASE (R)\n- RELOAD\n- REMOVE\n- RENAME (R)\n- REORGANIZE\n- REPAIR\n- REPEAT (R)\n- REPEATABLE\n- REPLACE (R)\n- REPLICA\n- REPLICAS\n- REPLICATION\n- REQUIRE (R)\n- REQUIRED\n- RESET\n- RESOURCE\n- RESPECT\n- RESTART\n- RESTORE\n- RESTORES\n- RESTRICT (R)\n- RESUME\n- REUSE\n- REVERSE\n- REVOKE (R)\n- RIGHT (R)\n- RLIKE (R)\n- ROLE\n- ROLLBACK\n- ROLLUP\n- ROUTINE\n- ROW (R)\n- ROW_COUNT\n- ROW_FORMAT\n- ROW_NUMBER (R-Window)\n- ROWS (R-Window)\n- RTREE\n- RUN\n\n<a id=\"S\" class=\"letter\" href=\"#S\">S</a>\n\n- SAMPLERATE\n- SAMPLES\n- SAN\n- SAVEPOINT\n- SECOND\n- SECOND_MICROSECOND (R)\n- SECONDARY\n- SECONDARY_ENGINE\n- SECONDARY_LOAD\n- SECONDARY_UNLOAD\n- SECURITY\n- SELECT (R)\n- SEND_CREDENTIALS_TO_TIKV\n- SEPARATOR\n- SEQUENCE\n- SERIAL\n- SERIALIZABLE\n- SESSION\n- SESSION_STATES\n- SET (R)\n- SETVAL\n- SHARD_ROW_ID_BITS\n- SHARE\n- SHARED\n- SHOW (R)\n- SHUTDOWN\n- SIGNED\n- SIMPLE\n- SKIP\n- SKIP_SCHEMA_FILES\n- SLAVE\n- SLOW\n- SMALLINT (R)\n- SNAPSHOT\n- SOME\n- SOURCE\n- SPATIAL (R)\n- SPLIT\n- SQL (R)\n- SQL_BIG_RESULT (R)\n- SQL_BUFFER_RESULT\n- SQL_CACHE\n- SQL_CALC_FOUND_ROWS (R)\n- SQL_NO_CACHE\n- SQL_SMALL_RESULT (R)\n- SQL_TSI_DAY\n- SQL_TSI_HOUR\n- SQL_TSI_MINUTE\n- SQL_TSI_MONTH\n- SQL_TSI_QUARTER\n- SQL_TSI_SECOND\n- SQL_TSI_WEEK\n- SQL_TSI_YEAR\n- SQLEXCEPTION (R)\n- SQLSTATE (R)\n- SQLWARNING (R)\n- SSL (R)\n- START\n- STARTING (R)\n- STATISTICS\n- STATS\n- STATS_AUTO_RECALC\n- STATS_BUCKETS\n- STATS_COL_CHOICE\n- STATS_COL_LIST\n- STATS_EXTENDED\n- STATS_HEALTHY\n- STATS_HISTOGRAMS\n- STATS_LOCKED\n- STATS_META\n- STATS_OPTIONS\n- STATS_PERSISTENT\n- STATS_SAMPLE_PAGES\n- STATS_SAMPLE_RATE\n- STATS_TOPN\n- STATUS\n- STORAGE\n- STORED (R)\n- STRAIGHT_JOIN (R)\n- STRICT_FORMAT\n- SUBJECT\n- SUBPARTITION\n- SUBPARTITIONS\n- SUPER\n- SWAPS\n- SWITCHES\n- SYSTEM\n- SYSTEM_TIME\n\n<a id=\"T\" class=\"letter\" href=\"#T\">T</a>\n\n- TABLE (R)\n- TABLES\n- TABLESAMPLE (R)\n- TABLESPACE\n- TABLE_CHECKSUM\n- TEMPORARY\n- TEMPTABLE\n- TERMINATED (R)\n- TEXT\n- THAN\n- THEN (R)\n- TIDB\n- TIDB_CURRENT_TSO (R)\n- TIFLASH\n- TIKV_IMPORTER\n- TIME\n- TIMESTAMP\n- TINYBLOB (R)\n- TINYINT (R)\n- TINYTEXT (R)\n- TO (R)\n- TOKEN_ISSUER\n- TOPN\n- TPCC\n- TPCH_10\n- TRACE\n- TRADITIONAL\n- TRAILING (R)\n- TRANSACTION\n- TRIGGER (R)\n- TRIGGERS\n- TRUE (R)\n- TRUNCATE\n- TSO\n- TTL\n- TTL_ENABLE\n- TTL_JOB_INTERVAL\n- TYPE\n\n<a id=\"U\" class=\"letter\" href=\"#U\">U</a>\n\n- UNBOUNDED\n- UNCOMMITTED\n- UNDEFINED\n- UNICODE\n- UNION (R)\n- UNIQUE (R)\n- UNKNOWN\n- UNLOCK (R)\n- UNSET\n- UNSIGNED (R)\n- UNTIL (R)\n- UPDATE (R)\n- USAGE (R)\n- USE (R)\n- USER\n- USING (R)\n- UTC_DATE (R)\n- UTC_TIME (R)\n- UTC_TIMESTAMP (R)\n\n<a id=\"V\" class=\"letter\" href=\"#V\">V</a>\n\n- VALIDATION\n- VALUE\n- VALUES (R)\n- VARBINARY (R)\n- VARCHAR (R)\n- VARCHARACTER (R)\n- VARIABLES\n- VARYING (R)\n- VECTOR \n- VIEW\n- VIRTUAL (R)\n- VISIBLE\n\n<a id=\"W\" class=\"letter\" href=\"#W\">W</a>\n\n- WAIT\n- WAIT_TIFLASH_READY\n- WARNINGS\n- WEEK\n- WEIGHT_STRING\n- WHEN (R)\n- WHERE (R)\n- WHILE (R)\n- WIDTH\n- WINDOW (R-Window)\n- WITH (R)\n- WITH_SYS_TABLE\n- WITHOUT\n- WORKLOAD\n- WRITE (R)\n\n<a id=\"X\" class=\"letter\" href=\"#X\">X</a>\n\n- X509\n- XOR (R)\n\n<a id=\"Y\" class=\"letter\" href=\"#Y\">Y</a>\n\n- YEAR\n- YEAR_MONTH (R)\n\n<a id=\"Z\" class=\"letter\" href=\"#Z\">Z</a>\n\n- ZEROFILL (R)\n"
        },
        {
          "name": "latency-breakdown.md",
          "type": "blob",
          "size": 33.6494140625,
          "content": "---\ntitle: 延迟的拆解分析\nsummary: 详细介绍 TiDB 运行各阶段中时间消耗带来的延迟，以及如何在真实场景中分析延迟。\n---\n\n# 延迟的拆解分析\n\n本文将 TiDB 中 SQL 语句的延迟拆解成各项监控指标，并从用户角度对指标进行分析，包括：\n\n- [通用 SQL 层](#通用-sql-层)\n- [读请求](#读请求)\n- [写请求](#写请求)\n- [批量请求](#批量请求)\n- [TiKV 快照](#tikv-快照)\n- [异步写入](#异步写入)\n\n这些分析可以让你深入了解 TiDB 在执行 SQL 查询时的耗时情况，有助于诊断 TiDB 关键运行路径的问题。除了延迟的指标拆解之外，[诊断场景](#诊断场景)小节介绍了如何在真实场景中分析延迟。\n\n建议在阅读本文前，先阅读 [TiDB 性能分析和优化方法](/performance-tuning-methods.md)。需要注意的是，在将延迟拆解成监控指标时，延迟的耗时数值采用的是均值，而非某几个特定慢查询对应的数值。许多指标都会以直方图的形式展示，以更好地展现耗时或者延迟的分布情况。你需要按如下公式使用总和 (sum) 及数量 (count) 来计算均值 (avg)。\n\n```\navg = ${metric_name}_sum / ${metric_name}_count\n```\n\n本文介绍的监控指标可以从 TiDB 的 Prometheus 管理界面中查询到。\n\n## 通用 SQL 层\n\n通用 SQL 层带来的延迟存在于 TiDB 的最顶部，所有 SQL 查询都具有这一部分的延迟。下面是通用 SQL 层操作的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    NonTerminal(\"Token wait duration\"),\n    Choice(\n        0,\n        Comment(\"Prepared statement\"),\n        NonTerminal(\"Parse duration\"),\n    ),\n    OneOrMore(\n        Sequence(\n        Choice(\n            0,\n            NonTerminal(\"Optimize prepared plan duration\"),\n            Sequence(\n            Comment(\"Plan cache miss\"),\n            NonTerminal(\"Compile duration\"),\n            ),\n        ),\n        NonTerminal(\"TSO wait duration\"),\n        NonTerminal(\"Execution duration\"),\n        ),\n        Comment(\"Retry\"),\n    ),\n)\n```\n\n通用 SQL 层的延迟可以使用 `e2e duration` 指标观察。它的计算方式是：\n\n```text\ne2e duration =\n    tidb_server_get_token_duration_seconds +\n    tidb_session_parse_duration_seconds +\n    tidb_session_compile_duration_seconds +\n    tidb_session_execute_duration_seconds{type=\"general\"}\n```\n\n- `tidb_server_get_token_duration_seconds` 代表令牌 (Token) 等待耗时，它通常小于 1 微秒，因而足以被忽略。\n- `tidb_session_parse_duration_seconds` 代表把 SQL 查询解析成抽象语法树 (AST, Abstract Syntax Tree) 的耗时。要跳过这部分的耗时，可以使用 [`PREPARE/EXECUTE` 语句](/develop/dev-guide-optimize-sql-best-practices.md#使用-prepare)。\n- `tidb_session_compile_duration_seconds` 代表把抽象语法树编译成执行计划的耗时。要跳过这部分的耗时，可以使用[执行计划缓存](/sql-prepared-plan-cache.md)。\n- `tidb_session_execute_duration_seconds{type=\"general\"}` 代表执行各种不同用户查询的耗时。这部分的耗时需要进行细粒度的拆解，以用来分析性能问题或瓶颈。\n\n通常来说，OLTP (Online Transactional Processing) 工作负载可以分为读请求和写请求两类。下面的小节会分别对[读请求](#读请求)和[写请求](#写请求)进行介绍。这两类请求共享了一些关键代码，但执行方式是不一样的。\n\n## 读请求\n\n读请求只有一种处理形式。\n\n### 点查 (Point Get)\n\n下面是点查操作的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    Choice(\n        0,\n        NonTerminal(\"Resolve TSO\"),\n        Comment(\"Read by clustered PK in auto-commit-txn mode or snapshot read\"),\n    ),\n    Choice(\n        0,\n        NonTerminal(\"Read handle by index key\"),\n        Comment(\"Read by clustered PK, encode handle by key\"),\n    ),\n    NonTerminal(\"Read value by handle\"),\n)\n```\n\n在点查过程中，`tidb_session_execute_duration_seconds{type=\"general\"}` 使用如下方式计算：\n\n```text\ntidb_session_execute_duration_seconds{type=\"general\"} =\n    pd_client_cmd_handle_cmds_duration_seconds{type=\"wait\"} +\n    read handle duration +\n    read value duration\n```\n\n`pd_client_cmd_handle_cmds_duration_seconds{type=\"wait\"}` 代表从 PD 中读取 [TSO](/tso.md) 的耗时。在 auto-commit 事务模式下从聚簇索引主键或者快照中读取时，该数值为 0。\n\n`read handle duration` 和 `read value duration` 使用如下方式计算：\n\n```text\nread handle duration = read value duration =\n    tidb_tikvclient_txn_cmd_duration_seconds{type=\"get\"} =\n    send request duration =\n    tidb_tikvclient_request_seconds{type=\"Get\"} =\n    tidb_tikvclient_batch_wait_duration +\n    tidb_tikvclient_batch_send_latency +\n    tikv_grpc_msg_duration_seconds{type=\"kv_get\"} +\n    tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}\n```\n\n`tidb_tikvclient_request_seconds{type=\"Get\"}` 代表通过 gRPC 发往 TiKV 的批量 get 请求耗时。关于 `tidb_tikvclient_batch_wait_duration`、`tidb_tikvclient_batch_send_latency` 和 `tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}` 等批量请求客户端的耗时计算方式，请参考[批量请求](#批量请求)小节。\n\n`tikv_grpc_msg_duration_seconds{type=\"kv_get\"}` 使用如下方式计算：\n\n```text\ntikv_grpc_msg_duration_seconds{type=\"kv_get\"} =\n    tikv_storage_engine_async_request_duration_seconds{type=\"snapshot\"} +\n    tikv_engine_seek_micro_seconds{type=\"seek_average\"} +\n    read value duration +\n    read value duration(non-short value)\n```\n\n此时，请求已经到达 TiKV。TiKV 在处理 get 请求时，会进行一次 seek 和一到两次 read 操作。其中，短数据的键和值被编码在一个 write CF 中，因而只需要进行一次 read 操作。TiKV 在处理 get 请求前会先获取一个快照。关于 TiKV 快照耗时的计算方式，请参考 [TiKV 快照](#tikv-快照)小节。\n\n`read value duration(from disk)` 使用如下方式计算：\n\n```text\nread value duration(from disk) =\n    sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_time\",req=\"get/batch_get_command\"})) / sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_count\",req=\"get/batch_get_command\"}))\n```\n\nTiKV 采用 RocksDB 作为存储引擎。如果 block cache 中找不到要求的值，TiKV 需要从磁盘中读取。对于 `tikv_storage_rocksdb_perf`，get 请求可以是 `get` 或者 `batch_get_command`。\n\n### 批量点查 (Batch Point Get)\n\n下面是批量点查操作的时间消耗图：\n\n```railroad+diagram\nDiagram(\n  NonTerminal(\"Resolve TSO\"),\n  Choice(\n    0,\n    NonTerminal(\"Read all handles by index keys\"),\n    Comment(\"Read by clustered PK, encode handle by keys\"),\n  ),\n  NonTerminal(\"Read values by handles\"),\n)\n```\n\n在进行批量点查时，`tidb_session_execute_duration_seconds{type=\"general\"}` 使用如下方式计算：\n\n```text\ntidb_session_execute_duration_seconds{type=\"general\"} =\n    pd_client_cmd_handle_cmds_duration_seconds{type=\"wait\"} +\n    read handles duration +\n    read values duration\n```\n\n批量点查的过程几乎与[点查](#点查-point-get)一致。不同的是，批量点查会同时得到多个值。\n\n`read handles duration` 和 `read values duration` 使用如下方式计算：\n\n```text\nread handles duration = read values duration =\n    tidb_tikvclient_txn_cmd_duration_seconds{type=\"batch_get\"} =\n    send request duration =\n    tidb_tikvclient_request_seconds{type=\"BatchGet\"} =\n    tidb_tikvclient_batch_wait_duration(transaction) +\n    tidb_tikvclient_batch_send_latency(transaction) +\n    tikv_grpc_msg_duration_seconds{type=\"kv_batch_get\"} +\n    tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}(transaction)\n```\n\n关于 `tidb_tikvclient_batch_wait_duration`、`tidb_tikvclient_batch_send_latency` 和 `tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}` 等批量请求客户端耗时的计算方式，请参考[批量请求](#批量请求)小节。\n\n耗时 `tikv_grpc_msg_duration_seconds{type=\"kv_batch_get\"}` 使用如下方式计算：\n\n```text\ntikv_grpc_msg_duration_seconds{type=\"kv_batch_get\"} =\n    tikv_storage_engine_async_request_duration_seconds{type=\"snapshot\"} +\n    n * (\n        tikv_engine_seek_micro_seconds{type=\"seek_max\"} +\n        read value duration +\n        read value duration(non-short value)\n    )\n\nread value duration(from disk) =\n    sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_time\",req=\"batch_get\"})) / sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_count\",req=\"batch_get\"}))\n```\n\nTiKV 会首先得到一个快照，然后从同一个快照中读取多个值。read 操作的耗时和[点查](#点查-point-get)中的一致。当从磁盘中读取数据时，其平均耗时可以通过带有 `req=\"batch_get\"` 属性的 `tikv_storage_rocksdb_perf` 来计算。\n\n### 表扫描和索引扫描 (Table Scan 和 Index Scan)\n\n下面是表扫描和索引扫描的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    Stack(\n        NonTerminal(\"Resolve TSO\"),\n        NonTerminal(\"Load region cache for related table/index ranges\"),\n        OneOrMore(\n            NonTerminal(\"Wait for result\"),\n            Comment(\"Next loop: drain the result\"),\n        ),\n    ),\n)\n```\n\n在进行表扫描和索引扫描时，耗时 `tidb_session_execute_duration_seconds{type=\"general\"}` 使用如下方式计算：\n\n```text\ntidb_session_execute_duration_seconds{type=\"general\"} =\n    pd_client_cmd_handle_cmds_duration_seconds{type=\"wait\"} +\n    req_per_copr * (\n        tidb_distsql_handle_query_duration_seconds{sql_type=\"general\"}\n    )\n    tidb_distsql_handle_query_duration_seconds{sql_type=\"general\"} <= send request duration\n```\n\n表扫描和索引扫描采用相同的方式处理。`req_per_copr` 是被分配的任务数量。由于执行协处理器和返回数据在不同的线程中运行，`tidb_distsql_handle_query_duration_seconds{sql_type=\"general\"}` 是等待时间，并且小于 `send request duration`。\n\n`send request duration` 和 `req_per_copr` 使用如下方式计算：\n\n```text\nsend request duration =\n    tidb_tikvclient_batch_wait_duration +\n    tidb_tikvclient_batch_send_latency +\n    tikv_grpc_msg_duration_seconds{type=\"coprocessor\"} +\n    tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}\n\ntikv_grpc_msg_duration_seconds{type=\"coprocessor\"} =\n    tikv_coprocessor_request_wait_seconds{type=\"snapshot\"} +\n    tikv_coprocessor_request_wait_seconds{type=\"schedule\"} +\n    tikv_coprocessor_request_handler_build_seconds{type=\"index/select\"} +\n    tikv_coprocessor_request_handle_seconds{type=\"index/select\"}\n\nreq_per_copr = rate(tidb_distsql_handle_query_duration_seconds_count) / rate(tidb_distsql_scan_keys_partial_num_count)\n```\n\n在 TiKV 中，表扫描的类型是 `select`，而索引扫描的类型是 `index`。`select` 和 `index` 类型的内部耗时是一致的。\n\n### 索引回表 (Index Look Up)\n\n下面是通过索引回表操作的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    Stack(\n        NonTerminal(\"Resolve TSO\"),\n        NonTerminal(\"Load region cache for related index ranges\"),\n        OneOrMore(\n            Sequence(\n                NonTerminal(\"Wait for index scan result\"),\n                NonTerminal(\"Wait for table scan result\"),\n            ),\n        Comment(\"Next loop: drain the result\"),\n        ),\n    ),\n)\n```\n\n在通过索引回表的过程中，耗时 `tidb_session_execute_duration_seconds{type=\"general\"}` 使用如下方式计算:\n\n```text\ntidb_session_execute_duration_seconds{type=\"general\"} =\n    pd_client_cmd_handle_cmds_duration_seconds{type=\"wait\"} +\n    req_per_copr * (\n        tidb_distsql_handle_query_duration_seconds{sql_type=\"general\"}\n    ) +\n    req_per_copr * (\n        tidb_distsql_handle_query_duration_seconds{sql_type=\"general\"}\n    )\n\nreq_per_copr = rate(tidb_distsql_handle_query_duration_seconds_count) / rate(tidb_distsql_scan_keys_partial_num_count)\n```\n\n一次通过索引回表的过程结合了索引查找和表查找，其中索引查找和表查找按流水线方式处理。\n\n## 写请求\n\n写请求有多个变种，因而比读请求复杂得多。下面是写请求的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    NonTerminal(\"Execute write query\"),\n    Choice(\n        0,\n        NonTerminal(\"Pessimistic lock keys\"),\n        Comment(\"bypass in optimistic transaction\"),\n    ),\n    Choice(\n        0,\n        NonTerminal(\"Auto Commit Transaction\"),\n        Comment(\"bypass in non-auto-commit or explicit transaction\"),\n    ),\n)\n```\n\n|                | 悲观事务          | 乐观事务    |\n|----------------|------------------|------------|\n| Auto-commit    | 执行 + 加锁 + 提交 | 执行 + 提交 |\n| 非 auto-commit | 执行 + 加锁        | 执行       |\n\n一次写请求可以被分解成以下三个阶段：\n\n- 执行阶段：执行并把更改写入 TiDB 的内存中\n- 加锁阶段：获取执行结果的悲观锁\n- 提交阶段：通过两阶段提交协议 (2PC) 来提交事务\n\n在执行阶段，TiDB 在内存中修改数据，其延迟主要源自读取所需的数据。对于更新和删除查询，TiDB 先从 TiKV 读取数据，再更新或删除内存中的数据。\n\n带有点查和批量点查的加锁时读取操作 (`SELECT FOR UPDATE`) 是一个例外。该操作会在单个 RPC (Remote Procedure Call) 请求中完成读取和加锁操作。\n\n### 加锁时点查\n\n下面是加锁时点查的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    Choice(\n        0,\n        Sequence(\n            NonTerminal(\"Read handle key by index key\"),\n            NonTerminal(\"Lock index key\"),\n        ),\n        Comment(\"Clustered index\"),\n    ),\n    NonTerminal(\"Lock handle key\"),\n    NonTerminal(\"Read value from pessimistic lock cache\"),\n)\n```\n\n在加锁时点查过程中，耗时 `execution(clustered PK)` 和 `execution(non-clustered PK or UK)` 使用如下方式计算：\n\n```text\nexecution(clustered PK) =\n    tidb_tikvclient_txn_cmd_duration_seconds{type=\"lock_keys\"}\nexecution(non-clustered PK or UK) =\n    2 * tidb_tikvclient_txn_cmd_duration_seconds{type=\"lock_keys\"}\n```\n\n加锁时点查会锁定键并获取其对应的值。相比先执行再获取锁的方式，该操作可以节省一次来回通信。加锁时点查的耗时可以看作与[加锁耗时](#加锁阶段)是一样的。\n\n### 加锁时批量点查\n\n下面是加锁时批量点查的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    Choice(\n        0,\n        NonTerminal(\"Read handle keys by index keys\"),\n        Comment(\"Clustered index\"),\n    ),\n    NonTerminal(\"Lock index and handle keys\"),\n    NonTerminal(\"Read values from pessimistic lock cache\"),\n)\n```\n\n在加锁时批量点查过程中，耗时 `execution(clustered PK)` 和 `execution(non-clustered PK or UK)` 使用如下方式计算：\n\n```text\nexecution(clustered PK) =\n    tidb_tikvclient_txn_cmd_duration_seconds{type=\"lock_keys\"}\nexecution(non-clustered PK or UK) =\n    tidb_tikvclient_txn_cmd_duration_seconds{type=\"batch_get\"} +\n    tidb_tikvclient_txn_cmd_duration_seconds{type=\"lock_keys\"}\n```\n\n加锁时批量点查的执行过程与[加锁时点查](#加锁时点查)相似。不同的点在于，加锁时批量点查会在单个 RPC 请求中读取多个值。关于 `tidb_tikvclient_txn_cmd_duration_seconds{type=\"batch_get\"}` 耗时的计算方式，请参考[批量点查](#批量点查-batch-point-get)小节。\n\n### 加锁阶段\n\n本小节介绍加锁阶段的耗时。\n\n```text\nround = ceil(\n    sum(rate(tidb_tikvclient_txn_regions_num_sum{type=\"2pc_pessimistic_lock\"})) /\n    sum(rate(tidb_tikvclient_txn_regions_num_count{type=\"2pc_pessimistic_lock\"})) /\n    committer-concurrency\n)\n\nlock = tidb_tikvclient_txn_cmd_duration_seconds{type=\"lock_keys\"} =\n    round * tidb_tikvclient_request_seconds{type=\"PessimisticLock\"}\n```\n\n锁是按照两阶段锁的结构来获取的，带有流量控制机制。流量控制会按照 `committer-concurrency`（默认值为 `128`）来限制并发在线请求的数量。为了简单说明，流量控制可以看作是请求延迟 (`round`) 的倍增。\n\n`tidb_tikvclient_request_seconds{type=\"PessimisticLock\"}` 使用如下方式计算：\n\n```text\ntidb_tikvclient_request_seconds{type=\"PessimisticLock\"} =\n    tidb_tikvclient_batch_wait_duration +\n    tidb_tikvclient_batch_send_latency +\n    tikv_grpc_msg_duration_seconds{type=\"kv_pessimistic_lock\"} +\n    tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}\n```\n\n关于 `tidb_tikvclient_batch_wait_duration`、`tidb_tikvclient_batch_send_latency` 和 `tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}` 等批量请求客户端耗时的计算方式，请参考[批量请求](#批量请求)小节。\n\n耗时 `tikv_grpc_msg_duration_seconds{type=\"kv_pessimistic_lock\"}` 使用如下方式计算：\n\n```text\ntikv_grpc_msg_duration_seconds{type=\"kv_pessimistic_lock\"} =\n    tikv_scheduler_latch_wait_duration_seconds{type=\"acquire_pessimistic_lock\"} +\n    tikv_storage_engine_async_request_duration_seconds{type=\"snapshot\"} +\n    (lock in-mem key count + lock on-disk key count) * lock read duration +\n    lock on-disk key count / (lock in-mem key count + lock on-disk key count) *\n    lock write duration\n```\n\n- 自 TiDB v6.0 起，TiKV 默认使用[内存悲观锁](/pessimistic-transaction.md#内存悲观锁)。内存悲观锁会跳过异步写入的过程。\n- `tikv_storage_engine_async_request_duration_seconds{type=\"snapshot\"}`是快照类型耗时，详情请参考 [TiKV 快照](#tikv-快照)小节.\n- `lock in-mem key count` 和 `lock on-disk key count` 使用如下方式计算：\n\n    ```text\n    lock in-mem key count =\n        sum(rate(tikv_in_memory_pessimistic_locking{result=\"success\"})) /\n        sum(rate(tikv_grpc_msg_duration_seconds_count{type=\"kv_pessimistic_lock\"}}))\n\n    lock on-disk key count =\n        sum(rate(tikv_in_memory_pessimistic_locking{result=\"full\"})) /\n        sum(rate(tikv_grpc_msg_duration_seconds_count{type=\"kv_pessimistic_lock\"}}))\n    ```\n\n    内存和磁盘中被加锁的键数量可以从内存锁计数中计算得出。TiKV 在得到锁之前会读取键对应的值，其读取耗时可以从 RocksDB performance context 中计算得出：\n\n    ```text\n    lock read duration(from disk) =\n        sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_time\",req=\"acquire_pessimistic_lock\"})) / sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_count\",req=\"acquire_pessimistic_lock\"}))\n    ```\n\n- `lock write duration` 是写入磁盘锁的耗时，具体计算方式请参考[异步写入](#异步写入)小节。\n\n### 提交阶段\n\n本小节介绍提交阶段的耗时。下面是提交操作的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    Stack(\n        Sequence(\n            Choice(\n                0,\n                Comment(\"use 2pc or causal consistency\"),\n                NonTerminal(\"Get min-commit-ts\"),\n            ),\n            Optional(\"Async prewrite binlog\"),\n            NonTerminal(\"Prewrite mutations\"),\n            Optional(\"Wait prewrite binlog result\"),\n        ),\n        Sequence(\n            Choice(\n                1,\n                Comment(\"1pc\"),\n                Sequence(\n                    Comment(\"2pc\"),\n                    NonTerminal(\"Get commit-ts\"),\n                    NonTerminal(\"Check schema\"),\n                    NonTerminal(\"Commit PK mutation\"),\n                ),\n                Sequence(\n                    Comment(\"async-commit\"),\n                    NonTerminal(\"Commit mutations asynchronously\"),\n                ),\n            ),\n            Choice(\n                0,\n                Comment(\"committed\"),\n                NonTerminal(\"Async cleanup\"),\n            ),\n            Optional(\"Commit binlog\"),\n        ),\n    ),\n)\n```\n\n提交的耗时使用如下方式计算：\n\n```text\ncommit =\n    Get_latest_ts_time +\n    Prewrite_time +\n    Get_commit_ts_time +\n    Commit_time\n\nGet_latest_ts_time = Get_commit_ts_time =\n    pd_client_cmd_handle_cmds_duration_seconds{type=\"wait\"}\n\nprewrite_round = ceil(\n    sum(rate(tidb_tikvclient_txn_regions_num_sum{type=\"2pc_prewrite\"})) /\n    sum(rate(tidb_tikvclient_txn_regions_num_count{type=\"2pc_prewrite\"})) /\n    committer-concurrency\n)\n\ncommit_round = ceil(\n    sum(rate(tidb_tikvclient_txn_regions_num_sum{type=\"2pc_commit\"})) /\n    sum(rate(tidb_tikvclient_txn_regions_num_count{type=\"2pc_commit\"})) /\n    committer-concurrency\n)\n\nPrewrite_time =\n    prewrite_round * tidb_tikvclient_request_seconds{type=\"Prewrite\"}\n\nCommit_time =\n    commit_round * tidb_tikvclient_request_seconds{type=\"Commit\"}\n```\n\n提交的耗时可以拆解为以下四个指标：\n\n- `Get_latest_ts_time` 代表异步提交或单阶段 (1PC) 提交事务中获取最新 TSO 的耗时。\n- `Prewrite_time` 代表预先写阶段的耗时。\n- `Get_commit_ts_time` 代表普通两阶段 (2PC) 事务的耗时。\n- `Commit_time` 代表提交阶段的耗时。需要注意的是，异步提交和单阶段 (1PC) 事务没有此阶段。\n\n与悲观锁一样，流量控制充当延迟的放大，即上述公式中的 `prewrite_round` 和 `commit_round`。\n\n`tidb_tikvclient_request_seconds{type=\"Prewrite\"}` 和 `tidb_tikvclient_request_seconds{type=\"Commit\"}` 的耗时使用如下方式计算：\n\n```text\ntidb_tikvclient_request_seconds{type=\"Prewrite\"} =\n    tidb_tikvclient_batch_wait_duration +\n    tidb_tikvclient_batch_send_latency +\n    tikv_grpc_msg_duration_seconds{type=\"kv_prewrite\"} +\n    tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}\n\ntidb_tikvclient_request_seconds{type=\"Commit\"} =\n    tidb_tikvclient_batch_wait_duration +\n    tidb_tikvclient_batch_send_latency +\n    tikv_grpc_msg_duration_seconds{type=\"kv_commit\"} +\n    tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}\n```\n\n关于 `tidb_tikvclient_batch_wait_duration`、`tidb_tikvclient_batch_send_latency` 和 `tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}` 等批量请求客户端耗时的计算方式，请参考[批量请求](#批量请求)小节。\n\n`tikv_grpc_msg_duration_seconds{type=\"kv_prewrite\"}` 使用如下方式计算：\n\n```text\ntikv_grpc_msg_duration_seconds{type=\"kv_prewrite\"} =\n    prewrite key count * prewrite read duration +\n    prewrite write duration\n\nprewrite key count =\n    sum(rate(tikv_scheduler_kv_command_key_write_sum{type=\"prewrite\"})) /\n    sum(rate(tikv_scheduler_kv_command_key_write_count{type=\"prewrite\"}))\n\nprewrite read duration(from disk) =\n    sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_time\",req=\"prewrite\"})) / sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_count\",req=\"prewrite\"}))\n```\n\n与 TiKV 中的锁一样，预先写在读取和写入阶段均进行了处理。读取阶段的耗时可以从 RocksDB performance context 计算。有关写入阶段耗时的计算方式，请参考[异步写入](#异步写入)部分。\n\n`tikv_grpc_msg_duration_seconds{type=\"kv_commit\"}`使用如下方式计算：\n\n```text\ntikv_grpc_msg_duration_seconds{type=\"kv_commit\"} =\n    commit key count * commit read duration +\n    commit write duration\n\ncommit key count =\n    sum(rate(tikv_scheduler_kv_command_key_write_sum{type=\"commit\"})) /\n    sum(rate(tikv_scheduler_kv_command_key_write_count{type=\"commit\"}))\n\ncommit read duration(from disk) =\n    sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_time\",req=\"commit\"})) / sum(rate(tikv_storage_rocksdb_perf{metric=\"block_read_count\",req=\"commit\"})) (storage)\n```\n\n`kv_commit` 的耗时与 `kv_prewrite` 几乎一致。关于写入阶段耗时的计算方式，请参考[异步写入](#异步写入)小节。\n\n## 批量请求\n\n下面是批量请求客户端的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    NonTerminal(\"Get conn pool to the target store\"),\n    Choice(\n        0,\n        Sequence(\n            Comment(\"Batch enabled\"),\n                NonTerminal(\"Push request to channel\"),\n                NonTerminal(\"Wait response\"),\n            ),\n            Sequence(\n            NonTerminal(\"Get conn from pool\"),\n            NonTerminal(\"Call RPC\"),\n            Choice(\n                0,\n                Comment(\"Unary call\"),\n                NonTerminal(\"Recv first\"),\n            ),\n        ),\n    ),\n)\n```\n\n- 总体的发送请求耗时看作 `tidb_tikvclient_request_seconds`。\n- RPC 客户端为每个存储维护各自的连接池（称为 ConnArray），每个连接池都包含带有一个发送批量请求 channel 的 BatchConn\n- 绝大多数情况下，当存储是 TiKV 并且 batch 大小为正数时，批量请求开启。\n- 批量请求 channel 的大小是 [`tikv-client.max-batch-size`](/tidb-configuration-file.md#max-batch-size) 的值（默认值为 `128`）。请求入队的耗时看作 `tidb_tikvclient_batch_wait_duration`。\n- 一共有 `CmdBatchCop`、`CmdCopStream` 和 `CmdMPPConn` 三种流式请求。流式请求会引入一个额外的 `recv()` 调用来获取流中的第一个响应。\n\n`tidb_tikvclient_request_seconds` 大致使用如下方式计算（部分延迟不包含在内）：\n\n```text\ntidb_tikvclient_request_seconds{type=\"?\"} =\n    tidb_tikvclient_batch_wait_duration +\n    tidb_tikvclient_batch_send_latency +\n    tikv_grpc_msg_duration_seconds{type=\"kv_?\"} +\n    tidb_tikvclient_rpc_net_latency_seconds{store=\"?\"}\n```\n\n- `tidb_tikvclient_batch_wait_duration` 记录批量请求系统的等待耗时。\n- `tidb_tikvclient_batch_send_latency` 记录批量请求系统的编码耗时。\n- `tikv_grpc_msg_duration_seconds{type=\"kv_?\"}` 是 TiKV 的处理耗时。\n- `tidb_tikvclient_rpc_net_latency_seconds` 记录网络延迟。\n\n## TiKV 快照\n\n下面是 TiKV 快照操作的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    Choice(\n        0,\n        Comment(\"Local Read\"),\n        Sequence(\n            NonTerminal(\"Propose Wait\"),\n            NonTerminal(\"Read index Read Wait\"),\n        ),\n    ),\n    NonTerminal(\"Fetch A Snapshot From KV Engine\"),\n)\n```\n\n一个 TiKV 快照的总体耗时可以从 `tikv_storage_engine_async_request_duration_seconds{type=\"snapshot\"}` 指标查看，它的计算方式如下：\n\n```text\ntikv_storage_engine_async_request_duration_seconds{type=\"snapshot\"} =\n    tikv_coprocessor_request_wait_seconds{type=\"snapshot\"} =\n    tikv_raftstore_request_wait_time_duration_secs +\n    tikv_raftstore_commit_log_duration_seconds +\n    get snapshot from rocksdb duration\n```\n\n当 leader lease 过期时，TiKV 会在从 RocksDB 获取快照之前提出读索引命令。`tikv_raftstore_request_wait_time_duration_secs` 和 `tikv_raftstore_commit_log_duration_seconds` 是提交读索引命令的耗时。\n\n从 RocksDB 获取快照通常是一个快速操作，因此 `get snapshot from rocksdb duration` 的耗时可以被忽略。\n\n## 异步写入\n\n异步写入是 TiKV 通过回调将数据异步写入基于 Raft 的复制状态机 (Replicated State Machine) 的过程。\n\n- 下面是异步 IO 未开启时，异步写入过程的时间消耗图：\n\n    ```railroad+diagram\n    Diagram(\n        NonTerminal(\"Propose Wait\"),\n        NonTerminal(\"Process Command\"),\n        Choice(\n            0,\n            Sequence(\n                NonTerminal(\"Wait Current Batch\"),\n                NonTerminal(\"Write to Log Engine\"),\n            ),\n            Sequence(\n                NonTerminal(\"RaftMsg Send Wait\"),\n                NonTerminal(\"Commit Log Wait\"),\n            ),\n        ),\n        NonTerminal(\"Apply Wait\"),\n        NonTerminal(\"Apply Log\"),\n    )\n    ```\n\n- 下面是异步 IO 开启时，异步写入过程的时间消耗图：\n\n    ```railroad+diagram\n    Diagram(\n        NonTerminal(\"Propose Wait\"),\n        NonTerminal(\"Process Command\"),\n        Choice(\n            0,\n            NonTerminal(\"Wait Until Persisted by Write Worker\"),\n            Sequence(\n                NonTerminal(\"RaftMsg Send Wait\"),\n                NonTerminal(\"Commit Log Wait\"),\n            ),\n        ),\n        NonTerminal(\"Apply Wait\"),\n        NonTerminal(\"Apply Log\"),\n    )\n    ```\n\n异步写入耗时的计算方式如下：\n\n```text\nasync write duration(async io disabled) =\n    propose +\n    async io disabled commit +\n    tikv_raftstore_apply_wait_time_duration_secs +\n    tikv_raftstore_apply_log_duration_seconds\n\nasync write duration(async io enabled) =\n    propose +\n    async io enabled commit +\n    tikv_raftstore_apply_wait_time_duration_secs +\n    tikv_raftstore_apply_log_duration_seconds\n```\n\n异步写入可以拆解为以下三个阶段：\n\n- 提案阶段 (Propose)\n- 提交阶段 (Commit)\n- 应用阶段 (Apply)：对应上面公式中的 `tikv_raftstore_apply_wait_time_duration_secs + tikv_raftstore_apply_log_duration_seconds`\n\n提案阶段耗时的计算方式如下：\n\n```text\npropose =\n    propose wait duration +\n    propose duration\n\npropose wait duration =\n    tikv_raftstore_store_wf_batch_wait_duration_seconds\n\npropose duration =\n    tikv_raftstore_store_wf_send_to_queue_duration_seconds -\n    tikv_raftstore_store_wf_batch_wait_duration_seconds\n```\n\nRaft 过程以瀑布方式记录，因此，提案阶段的耗时是根据 `tikv_raftstore_store_wf_send_to_queue_duration_seconds` 和 `tikv_raftstore_store_wf_batch_wait_duration_seconds` 两个指标之间的差值计算的。\n\n提交阶段耗时的计算方式如下：\n\n```text\nasync io disabled commit = max(\n    persist log locally duration,\n    replicate log duration\n)\n\nasync io enabled commit = max(\n    wait by write worker duration,\n    replicate log duration\n)\n```\n\n从 TiDB v5.3.0 开始，TiKV 支持通过 StoreWriter 线程池写入 Raft 日志，即异步 IO。异步 IO 会改变提交过程，仅在 [`store-io-pool-size`](/tikv-configuration-file.md#store-io-pool-size-从-v530-版本开始引入) 数值大于 0 时启用。耗时 `persist log locally duration` 和 `wait by write worker duration` 的计算方式如下：\n\n```text\npersist log locally duration =\n    batch wait duration +\n    write to raft db duration\n\nbatch wait duration =\n    tikv_raftstore_store_wf_before_write_duration_seconds -\n    tikv_raftstore_store_wf_send_to_queue_duration_seconds\n\nwrite to raft db duration =\n    tikv_raftstore_store_wf_write_end_duration_seconds -\n    tikv_raftstore_store_wf_before_write_duration_seconds\n\nwait by write worker duration =\n    tikv_raftstore_store_wf_persist_duration_seconds -\n    tikv_raftstore_store_wf_send_to_queue_duration_seconds\n```\n\n是否开启异步 IO 的区别在于本地持久化日志的耗时。使用异步 IO 可以直接从瀑布指标中计算本地持久化日志的耗时，忽略批处理等待耗时。\n\n`replicate log duration` 代表 quorum 副本中日志持久化的耗时，其中包含 RPC 耗时和大多数日志持久化的耗时。`replicate log duration` 耗时的计算方式如下：\n\n```text\nreplicate log duration =\n    raftmsg send wait duration +\n    commit log wait duration\n\nraftmsg send wait duration =\n    tikv_raftstore_store_wf_send_proposal_duration_seconds -\n    tikv_raftstore_store_wf_send_to_queue_duration_seconds\n\ncommit log wait duration =\n    tikv_raftstore_store_wf_commit_log_duration -\n    tikv_raftstore_store_wf_send_proposal_duration_seconds\n```\n\n### Raft DB\n\n下面是 Raft DB 的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    NonTerminal(\"Wait for Writer Leader\"),\n    NonTerminal(\"Write and Sync Log\"),\n    NonTerminal(\"Apply Log to Memtable\"),\n)\n```\n\n```text\nwrite to raft db duration = raft db write duration\ncommit log wait duration >= raft db write duration\n\nraft db write duration(raft engine enabled) =\n    raft_engine_write_preprocess_duration_seconds +\n    raft_engine_write_leader_duration_seconds +\n    raft_engine_write_apply_duration_seconds\n\nraft db write duration(raft engine disabled) =\n    tikv_raftstore_store_perf_context_time_duration_secs{type=\"write_thread_wait\"} +\n    tikv_raftstore_store_perf_context_time_duration_secs{type=\"write_scheduling_flushes_compactions_time\"} +\n    tikv_raftstore_store_perf_context_time_duration_secs{type=\"write_wal_time\"} +\n    tikv_raftstore_store_perf_context_time_duration_secs{type=\"write_memtable_time\"}\n```\n\n`commit log wait duration` 是 quorum 副本中最长的耗时，可能大于 `raft db write duration`。\n\n从 TiDB v6.1.0 开始，TiKV 默认使用 [Raft Engine](/glossary.md#raft-engine) 作为日志存储引擎，这将改变写入日志的过程。\n\n### KV DB\n\n下面是 KV DB 的时间消耗图：\n\n```railroad+diagram\nDiagram(\n    NonTerminal(\"Wait for Writer Leader\"),\n    NonTerminal(\"Preprocess\"),\n    Choice(\n        0,\n        Comment(\"No Need to Switch\"),\n        NonTerminal(\"Switch WAL or Memtable\"),\n    ),\n    NonTerminal(\"Write and Sync WAL\"),\n    NonTerminal(\"Apply to Memtable\"),\n)\n```\n\n```text\ntikv_raftstore_apply_log_duration_seconds =\n    tikv_raftstore_apply_perf_context_time_duration_secs{type=\"write_thread_wait\"} +\n    tikv_raftstore_apply_perf_context_time_duration_secs{type=\"write_scheduling_flushes_compactions_time\"} +\n    tikv_raftstore_apply_perf_context_time_duration_secs{type=\"write_wal_time\"} +\n    tikv_raftstore_apply_perf_context_time_duration_secs{type=\"write_memtable_time\"}\n```\n\n在异步写入过程中，提交的日志需要应用到 KV DB 中，应用耗时可以根据 RocksDB performance context 进行计算。\n\n## 诊断场景\n\n前面的部分详细介绍了 SQL 查询过程中执行时间的细粒度指标。本小节主要介绍遇到慢读取或慢写入查询时常见的指标分析过程。所有指标均可在 [Performance Overview 面板](/grafana-performance-overview-dashboard.md)的 Database Time 中查看。\n\n### 慢读取查询\n\n如果 `SELECT` 语句占 Database Time 的很大一部分，你可以认为 TiDB 在读查询时速度很慢。\n\n慢查询的执行计划可以在 TiDB Dashboard 中的 [Top SQL 语句](/dashboard/dashboard-overview.md#top-sql-语句) 区域查看。要分析慢读取查询的耗时，你可以根据前面的描述分析[点查](#点查-point-get)、[批量点查](#批量点查-batch-point-get)和[表扫描和索引扫描](#表扫描和索引扫描-table-scan-和-index-scan)的耗时情况。\n\n### 慢写入查询\n\n在分析慢写入查询之前，你需要查看 `tikv_scheduler_latch_wait_duration_seconds_sum{type=\"acquire_pessimistic_lock\"} by (instance)` 指标来确认冲突的原因：\n\n- 如果这个指标在某些特定的 TiKV 实例中很高，则在热点区域可能会存在冲突。\n- 如果这个指标在所有实例中都很高，则业务中可能存在冲突。\n\n如果是业务中存在冲突，那么你可以分析[加锁](#加锁阶段)和[提交](#提交阶段)阶段的耗时。\n"
        },
        {
          "name": "literal-values.md",
          "type": "blob",
          "size": 8.6640625,
          "content": "---\ntitle: 字面值\nsummary: 本文介绍了 TiDB SQL 语句的字面值。\naliases: ['/docs-cn/dev/literal-values/','/docs-cn/dev/reference/sql/language-structure/literal-values/']\n---\n\n# 字面值\n\nTiDB 字面值包括字符字面值、数值字面值、时间日期字面值、十六进制、二进制字面值和 NULL 字面值。以下分别对这些字面值进行一一介绍。\n\n## String Literals\n\nString Literals 是一个 bytes 或者 characters 的序列，两端被单引号 `'` 或者双引号 `\"` 包围，例如：\n\n```\n'example string'\n\"example string\"\n```\n\n如果字符串是连续的，会被合并为一个独立的 string。以下表示是一样的：\n\n```\n'a string'\n'a' ' ' 'string'\n\"a\" ' ' \"string\"\n```\n\n如果开启了 `ANSI_QUOTES` SQL MODE，那么只有单引号内的会被认为是 String Literals，对于双引号内的字符串，会被认为是一个 identifier。\n\n字符串分为以下两种：\n\n+ 二进制字符串 (binary string)：由字节序列构成，它的 charset 和 collation 都是 `binary`，在互相比较时利用**字节**作为单位。\n+ 非二进制字符串：由字符序列构成，有除 `binary` 以外的多种 charset 和 collation，在相互比较时用**字符**（一个字符可能包含多个字节，取决于 charset 的选择）作为单位。\n\n一个 String Literal 可以拥有一个可选的 `character set introducer` 和 `COLLATE clause`，可以用来指定特定的 charset 和 collation。\n\n```\n[_charset_name]'string' [COLLATE collation_name]\n```\n\n例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT _latin1'string';\nSELECT _binary'string';\nSELECT _utf8'string' COLLATE utf8_bin;\n```\n\n你可以使用 `N'literal'` 或者 `n'literal'` 来创建使用 national character set 的字符串，下列语句是一样的：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT N'some text';\nSELECT n'some text';\nSELECT _utf8'some text';\n```\n\n要在字符串中表示某些特殊字符，可以利用转义字符进行转义：\n\n| 转义字符 | 含义 |\n| :------ | :---- |\n| \\\\0 | ASCII NUL (X'00') 字符 |\n| \\\\' | 单引号 |\n| \\\\\" | 双引号 |\n| \\\\b | 退格符号 |\n| \\\\n | 换行符 |\n| \\\\r | 回车符 |\n| \\\\t | tab 符（制表符） |\n| \\\\z | ASCII 26 (Ctrl + Z) |\n| \\\\\\\\ | 反斜杠 \\\\ |\n| \\\\% | \\% |\n| \\\\_ | \\_ |\n\n如果要在 `'` 包围的字符串中表示 `\"`，或者在 `\"` 包围的字符串中表示 `'`，可以不使用转义字符。\n\n更多细节见 [MySQL 官方文档](https://dev.mysql.com/doc/refman/8.0/en/string-literals.html)。\n\n## Numeric Literals\n\n数值字面值包括 integer 跟 Decimal 类型跟浮点数字面值。\n\ninteger 可以包括 `.` 作为小数点分隔，数字前可以有 `-` 或者 `+` 来表示正数或者负数。\n\n精确数值字面值可以表示为如下格式：`1, .2, 3.4, -5, -6.78, +9.10`.\n\n科学记数法也是被允许的，表示为如下格式：`1.2E3, 1.2E-3, -1.2E3, -1.2E-3`。\n\n更多细节见 [MySQL 官方文档](https://dev.mysql.com/doc/refman/8.0/en/number-literals.html)。\n\n## Date and Time Literals\n\nDate 跟 Time 字面值有几种格式，例如用字符串表示，或者直接用数字表示。在 TiDB 里面，当 TiDB 期望一个 Date 的时候，它会把 `'2017-08-24'`，`'20170824'`，`20170824` 当做是 Date。\n\nTiDB 的 Date 值有以下几种格式：\n\n* `'YYYY-MM-DD'` 或者 `'YY-MM-DD'`，这里的 `-` 分隔符并不是严格的，可以是任意的标点符号。比如 `'2017-08-24'`，`'2017&08&24'`，`'2012@12^31'` 都是一样的。唯一需要特别对待的是 '.' 号，它被当做是小数点，用于分隔整数和小数部分。\n\n    Date 和 Time 部分可以被 'T' 分隔，它的作用跟空格符是一样的，例如 `2017-8-24 10:42:00` 跟 `2017-8-24T10:42:00` 是一样的。\n\n* `'YYYYMMDDHHMMSS'` 或者 `'YYMMDDHHMMSS'`，例如 `'20170824104520'` 和 `'170824104520'` 被当做是 `'2017-08-24 10:45:20'`，但是如果你提供了一个超过范围的值，例如`'170824304520'`，那这就不是一个有效的 Date 字面值。需要注意 `YYYYMMDD HHMMSS`、`YYYYMMDD HH:MM:DD` 和 `YYYY-MM-DD HHMMSS` 等不正确的格式会插入失败。\n* `YYYYMMDDHHMMSS` 或者 `YYMMDDHHMMSS`，注意这里没有单引号或者双引号，是一个数字。例如 `20170824104520` 表示 `'2017-08-24 10:45:20'`。\n\nDATETIME 或者 TIMESTAMP 值可以接一个小数部分，用来表示微秒（精度最多到小数点后 6 位），用小数点 `.` 分隔。\n\n如果 Date 的 year 部分只有两个数字，这是有歧义的（推荐使用四个数字的格式），TiDB 会尝试用以下的规则来解释：\n\n* year 值如果在 `70-99` 范围，那么被转换成 `1970-1999`。\n* year 值如果在 `00-69` 范围，那么被转换成 `2000-2069`。\n\n对于小于 10 的 month 或者 day 值，`'2017-8-4'` 跟 `'2017-08-04'` 是一样的。对于 Time 也是一样，比如 `'2017-08-24 1:2:3'` 跟 `'2017-08-24 01:02:03'`是一样的。\n\n在需要 Date 或者 Time 的语境下，对于数值，TiDB 会根据数值的长度来选定指定的格式：\n\n* 6 个数字，会被解释为 `YYMMDD`。\n* 12 个数字，会被解释为 `YYMMDDHHMMSS`。\n* 8 个数字，会解释为 `YYYYMMDD`。\n* 14 个数字，会被解释为 `YYYYMMDDHHMMSS`。\n\n对于 Time 类型，TiDB 用以下格式来表示：\n\n* `'D HH:MM:SS'`，或者 `'HH:MM:SS'`，`'HH:MM'`，`'D HH:MM'`，`'D HH'`，`'SS'`。这里的 D 表示 days，合法的范围是 `0-34`。\n* 数值 `HHMMSS`，例如 `231010` 被解释为`'23:10:10'`。\n* 数值 `SS`，`MMSS`，`HHMMSS` 都是可以被当做 Time。\n\nTime 类型的小数点也是 `.`，精度最多小数点后 6 位。\n\n更多细节见 [MySQL 官方文档](https://dev.mysql.com/doc/refman/8.0/en/date-and-time-literals.html)。\n\n## Boolean Literals\n\n常量 `TRUE` 和 `FALSE` 等于 1 和 0，它是大小写不敏感的。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT TRUE, true, tRuE, FALSE, FaLsE, false;\n```\n\n```\n+------+------+------+-------+-------+-------+\n| TRUE | true | tRuE | FALSE | FaLsE | false |\n+------+------+------+-------+-------+-------+\n|    1 |    1 |    1 |     0 |     0 |     0 |\n+------+------+------+-------+-------+-------+\n1 row in set (0.00 sec)\n```\n\n## Hexadecimal Literals\n\n十六进制字面值是有 `X` 和 `0x` 前缀的字符串，后接表示十六进制的数字。注意 `0x` 是大小写敏感的，不能表示为 `0X`。\n\n例如：\n\n```\nX'ac12'\nX'12AC'\nx'ac12'\nx'12AC'\n0xac12\n0x12AC\n```\n\n以下是不合法的十六进制字面值：\n\n```\nX'1z' (z 不是合法的十六进制值)\n0X12AC (0X 必须用小写的 0x)\n```\n\n对于使用 `X'val'` 格式的十六进制字面值，`val` 必须包含偶数个字符，如果 `val` 的长度是奇数（比如 X'A'、X'11A'），可以在前面补一个 0 来避免语法错误。\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect X'aff';\n```\n\n```\nERROR 1105 (HY000): line 0 column 13 near \"\"hex literal: invalid hexadecimal format, must even numbers, but 3 (total length 13)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect X'0aff';\n```\n\n```\n+---------+\n| X'0aff' |\n+---------+\n| 0x0aff  |\n+---------+\n1 row in set (0.00 sec)\n```\n\n默认情况，十六进制字面值是一个二进制字符串。\n\n如果需要将一个字符串或者数字转换为十六进制字面值，可以使用内建函数 `HEX()`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT HEX('TiDB');\n```\n\n```\n+-------------+\n| HEX('TiDB') |\n+-------------+\n| 54694442    |\n+-------------+\n1 row in set (0.01 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT X'54694442';\n```\n\n```\n+-------------+\n| X'54694442' |\n+-------------+\n| TiDB        |\n+-------------+\n1 row in set (0.00 sec)\n```\n\n## Bit-Value Literals\n\n位值字面值用 `b` 或者 `0b` 做前缀，后接以 0 和 1 组成的二进制数字。其中 `0b` 是区分大小写的，`0B` 则会报错。\n\n合法的 Bit-value：\n\n* b'01'\n* B'01'\n* 0b01\n\n非法的 Bit-value：\n\n* `b'2'`（2 不是二进制数值，必须为 0 或 1）\n* `0B01`（0B 必须是小写 0b）\n\n默认情况，位值字面值是一个二进制字符串。\n\nBit-value 是作为二进制返回的，所以输出到 MySQL Client 可能会无法显示，如果要转换为可打印的字符，可以使用内建函数 `BIN()` 或者 `HEX()`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t (b BIT(8));\nINSERT INTO t SET b = b'00010011';\nINSERT INTO t SET b = b'1110';\nINSERT INTO t SET b = b'100101';\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT b+0, BIN(b), HEX(b) FROM t;\n```\n\n```\n+------+--------+--------+\n| b+0  | BIN(b) | HEX(b) |\n+------+--------+--------+\n|   19 | 10011  | 13     |\n|   14 | 1110   | E      |\n|   37 | 100101 | 25     |\n+------+--------+--------+\n3 rows in set (0.00 sec)\n```\n\n## NULL Values\n\n`NULL` 代表数据为空，它是大小写不敏感的，与 `\\N`（大小写敏感）同义。\n\n> **注意：**\n>\n> `NULL` 跟 `0` 并不一样，跟空字符串 `''` 也不一样。\n"
        },
        {
          "name": "log-redaction.md",
          "type": "blob",
          "size": 3.7333984375,
          "content": "---\ntitle: 日志脱敏\nsummary: 了解 TiDB 各组件中的日志脱敏。\n---\n\n# 日志脱敏\n\nTiDB 在提供详细的日志信息时，可能会把数据库敏感的数据（例如用户数据）打印出来，造成数据安全方面的风险。因此 TiDB、TiKV、PD 等组件各提供了一个配置项开关，开关打开后，会隐藏日志中包含的用户数据值。\n\n## TiDB 组件日志脱敏\n\nTiDB 侧的日志脱敏需要将 [`global.tidb_redact_log`](/system-variables.md#tidb_redact_log) 的值设为 `ON` 或 `MARKER`。该变量值默认为 `OFF`，即关闭脱敏。\n\n可以通过 `set` 语法，设置全局系统变量 `tidb_redact_log`，示例如下：\n\n```sql\nset @@global.tidb_redact_log = ON;\n```\n\n设置后，所有新 session 产生的日志都会脱敏：\n\n```sql\ncreate table t (a int, unique key (a));\nQuery OK, 0 rows affected (0.00 sec)\n\ninsert into t values (1),(1);\nERROR 1062 (23000): Duplicate entry '1' for key 't.a'\n```\n\n打印出的错误日志如下：\n\n```\n[2024/07/02 11:35:32.686 +08:00] [INFO] [conn.go:1146] [\"command dispatched failed\"] [conn=1482686470] [session_alias=] [connInfo=\"id:1482686470, addr:127.0.0.1:52258 status:10, collation:utf8mb4_0900_ai_ci, user:root\"] [command=Query] [status=\"inTxn:0, autocommit:1\"] [sql=\"insert into `t` values ( ... )\"] [txn_mode=PESSIMISTIC] [timestamp=450859193514065921] [err=\"[kv:1062]Duplicate entry '?' for key 't.a'\"]\n```\n\n从以上报错日志可以看到，当把 `tidb_redact_log` 的值设为 `ON` 后，TiDB 日志中会把敏感信息隐藏掉（以问号 `?` 替换），以此规避数据安全风险。\n\n此外，TiDB 还提供了 `MARKER` 选项，当设置 `tidb_redact_log` 的值为 `MARKER` 时，TiDB 会在日志中用 `‹ ›` 符号标记出敏感信息，而不是直接隐藏，以便用户能够自定义脱敏规则。\n\n```sql\nset @@global.tidb_redact_log = MARKER;\n```\n\n设置后，所有新 session 产生的日志都会对敏感信息进行标记，而不进行替换：\n\n```sql\ncreate table t (a int, unique key (a));\nQuery OK, 0 rows affected (0.07 sec)\n\ninsert into t values (1),(1);\nERROR 1062 (23000): Duplicate entry '‹1›' for key 't.a'\n```\n\n打印出的错误日志如下：\n\n```\n[2024/07/02 11:35:01.426 +08:00] [INFO] [conn.go:1146] [\"command dispatched failed\"] [conn=1482686470] [session_alias=] [connInfo=\"id:1482686470, addr:127.0.0.1:52258 status:10, collation:utf8mb4_0900_ai_ci, user:root\"] [command=Query] [status=\"inTxn:0, autocommit:1\"] [sql=\"insert into `t` values ( ‹1› ) , ( ‹1› )\"] [txn_mode=PESSIMISTIC] [timestamp=450859185309483010] [err=\"[kv:1062]Duplicate entry '‹1›' for key 't.a'\"]\n```\n\n从以上报错日志可以看到，当把 `tidb_redact_log` 的值设为 `MARKER` 后，TiDB 日志中会用 `‹ ›` 符号标记出敏感信息，你可以根据自己的需求自定义脱敏规则来处理日志中的敏感信息。\n\n## TiKV 组件日志脱敏\n\nTiKV 侧的日志脱敏需要将 [`security.redact-info-log`](/tikv-configuration-file.md#redact-info-log-从-v408-版本开始引入) 的值设为 `true` 或 `\"marker\"`。该配置项值默认为 `false`，即关闭脱敏。\n\n## PD 组件日志脱敏\n\nPD 侧的日志脱敏需要将 [`security.redact-info-log`](/pd-configuration-file.md#redact-info-log-从-v50-版本开始引入) 的值设为 `true` 或 `\"marker\"`。该配置项值默认为 `false`，即关闭脱敏。\n\n## TiFlash 组件日志脱敏\n\nTiFlash 侧的日志脱敏需要将 tiflash-server 中 [`security.redact_info_log`](/tiflash/tiflash-configuration.md#配置文件-tiflashtoml) 配置项的值以及 tiflash-learner 中 [`security.redact-info-log`](/tiflash/tiflash-configuration.md#配置文件-tiflash-learnertoml) 配置项的值均设为 `true` 或者 `\"marker\"`。两配置项默认值均为 `false`，即关闭脱敏。\n"
        },
        {
          "name": "maintain-tidb-using-tiup.md",
          "type": "blob",
          "size": 7.8935546875,
          "content": "---\ntitle: TiUP 常见运维操作\naliases: ['/docs-cn/dev/maintain-tidb-using-tiup/','/docs-cn/dev/how-to/maintain/tiup-operations/']\nsummary: TiUP 是用于管理 TiDB 集群的工具，可以进行查看集群列表、启动、关闭、修改配置参数、查看状态等常见运维操作。操作简单方便，适合用于 TiDB 集群的管理。\n---\n\n# TiUP 常见运维操作\n\n本文介绍了使用 TiUP 运维 TiDB 集群的常见操作，包括查看集群列表、启动集群、查看集群状态、修改配置参数、关闭集群、销毁集群等。\n\n## 查看集群列表\n\nTiUP cluster 组件可以用来管理多个 TiDB 集群，在每个 TiDB 集群部署完毕后，该集群会出现在 TiUP 的集群列表里，可以使用 list 命令来查看。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster list\n```\n\n## 启动集群\n\n启动集群操作会按 PD -> TiKV -> TiDB -> TiFlash -> TiCDC -> Prometheus -> Grafana -> Alertmanager 的顺序启动整个 TiDB 集群所有组件：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster start ${cluster-name}\n```\n\n> **注意：**\n>\n> 你需要将 `${cluster-name}` 替换成实际的集群名字，若忘记集群名字，可通过 `tiup cluster list` 查看。\n\n该命令支持通过 `-R` 和 `-N` 参数来只启动部分组件。\n\n例如，下列命令只启动 PD 组件：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster start ${cluster-name} -R pd\n```\n\n下列命令只启动 `1.2.3.4` 和 `1.2.3.5` 这两台机器上的 PD 组件：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster start ${cluster-name} -N 1.2.3.4:2379,1.2.3.5:2379\n```\n\n> **注意：**\n>\n> 若通过 `-R` 和 `-N` 启动指定组件，需要保证启动顺序正确（例如需要先启动 PD 才能启动 TiKV），否则可能导致启动失败。\n\n## 查看集群状态\n\n集群启动之后需要检查每个组件的运行状态，以确保每个组件工作正常。TiUP 提供了 display 命令，节省了登录到每台机器上去查看进程的时间。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster display ${cluster-name}\n```\n\n## 修改配置参数\n\n集群运行过程中，如果需要调整某个组件的参数，可以使用 `edit-config` 命令来编辑参数。具体的操作步骤如下：\n\n1. 以编辑模式打开该集群的配置文件：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    tiup cluster edit-config ${cluster-name}\n    ```\n\n2. 设置参数：\n\n    首先确定配置的生效范围，有以下两种生效范围：\n\n    - 如果配置的生效范围为该组件全局，则配置到 `server_configs`。例如：\n\n        ```\n        server_configs:\n          tidb:\n            log.slow-threshold: 300\n        ```\n\n    - 如果配置的生效范围为某个节点，则配置到具体节点的 `config` 中。例如：\n\n        ```\n        tidb_servers:\n        - host: 10.0.1.11\n          port: 4000\n          config:\n              log.slow-threshold: 300\n        ```\n\n    参数的格式参考 [TiUP 配置参数模版](https://github.com/pingcap/tiup/blob/master/embed/examples/cluster/topology.example.yaml)。\n\n    **配置项层次结构使用 `.` 表示**。\n\n    关于组件的更多配置参数说明，可参考 [tidb `config.toml.example`](https://github.com/pingcap/tidb/blob/master/pkg/config/config.toml.example)、[tikv `config.toml.example`](https://github.com/tikv/tikv/blob/master/etc/config-template.toml) 和 [pd `config.toml.example`](https://github.com/tikv/pd/blob/master/conf/config.toml)。\n\n3. 执行 `reload` 命令滚动分发配置、重启相应组件：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    tiup cluster reload ${cluster-name} [-N <nodes>] [-R <roles>]\n    ```\n\n### 示例\n\n如果要调整 tidb-server 中事务大小限制参数 `txn-total-size-limit` 为 `1G`，该参数位于 [performance](https://github.com/pingcap/tidb/blob/master/pkg/config/config.toml.example) 模块下，调整后的配置如下：\n\n```\nserver_configs:\n  tidb:\n    performance.txn-total-size-limit: 1073741824\n```\n\n然后执行 `tiup cluster reload ${cluster-name} -R tidb` 命令滚动重启。\n\n## Hotfix 版本替换\n\n常规的升级集群请参考[升级文档](/upgrade-tidb-using-tiup.md)，但是在某些场景下（例如 Debug），可能需要用一个临时的包替换正在运行的组件，此时可以用 `patch` 命令：\n\n{{< copyable \"shell-root\" >}}\n\n```bash\ntiup cluster patch --help\n```\n\n```\nReplace the remote package with a specified package and restart the service\n\nUsage:\n  tiup cluster patch <cluster-name> <package-path> [flags]\n\nFlags:\n  -h, --help                   帮助信息\n  -N, --node strings           指定被替换的节点\n      --overwrite              在未来的 scale-out 操作中使用当前指定的临时包\n  -R, --role strings           指定被替换的服务类型\n      --transfer-timeout int   transfer leader 的超时时间\n\nGlobal Flags:\n      --native-ssh        使用系统默认的 SSH 客户端\n      --wait-timeout int  等待操作超时的时间\n      --ssh-timeout int   SSH 连接的超时时间\n  -y, --yes               跳过所有的确认步骤\n```\n\n例如，有一个 TiDB 实例的 hotfix 包放在 `/tmp/tidb-hotfix.tar.gz` 目录下。如果此时想要替换集群上的所有 TiDB 实例，则可以执行以下命令：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster patch test-cluster /tmp/tidb-hotfix.tar.gz -R tidb\n```\n\n或者只替换其中一个 TiDB 实例：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster patch test-cluster /tmp/tidb-hotfix.tar.gz -N 172.16.4.5:4000\n```\n\n## 重命名集群\n\n部署并启动集群后，可以通过 `tiup cluster rename` 命令来对集群重命名：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster rename ${cluster-name} ${new-name}\n```\n\n> **注意：**\n>\n> + 重命名集群会重启监控（Prometheus 和 Grafana）。\n> + 重命名集群之后 Grafana 可能会残留一些旧集群名的面板，需要手动删除这些面板。\n\n## 关闭集群\n\n关闭集群操作会按 Alertmanager -> Grafana -> Prometheus -> TiCDC -> TiFlash -> TiDB -> TiKV -> PD 的顺序关闭整个 TiDB 集群所有组件（同时也会关闭监控组件）：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster stop ${cluster-name}\n```\n\n和 `start` 命令类似，`stop` 命令也支持通过 `-R` 和 `-N` 参数来只停止部分组件。\n\n例如，下列命令只停止 TiDB 组件：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster stop ${cluster-name} -R tidb\n```\n\n下列命令只停止 `1.2.3.4` 和 `1.2.3.5` 这两台机器上的 TiDB 组件：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster stop ${cluster-name} -N 1.2.3.4:4000,1.2.3.5:4000\n```\n\n## 清除集群数据\n\n此操作会关闭所有服务，并清空其数据目录或/和日志目录，并且无法恢复，需要**谨慎操作**。\n\n清空集群所有服务的数据，但保留日志：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster clean ${cluster-name} --data\n```\n\n清空集群所有服务的日志，但保留数据：\n\n```bash\ntiup cluster clean ${cluster-name} --log\n```\n\n清空集群所有服务的数据和日志：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster clean ${cluster-name} --all\n```\n\n清空 Prometheus 以外的所有服务的日志和数据：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster clean ${cluster-name} --all --ignore-role prometheus\n```\n\n清空节点 `172.16.13.11:9000` 以外的所有服务的日志和数据：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster clean ${cluster-name} --all --ignore-node 172.16.13.11:9000\n```\n\n清空部署在 `172.16.13.12` 以外的所有服务的日志和数据：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster clean ${cluster-name} --all --ignore-node 172.16.13.12\n```\n\n## 销毁集群\n\n销毁集群操作会关闭服务，清空数据目录和部署目录，并且无法恢复，需要**谨慎操作**。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup cluster destroy ${cluster-name}\n```\n"
        },
        {
          "name": "max-min-eliminate.md",
          "type": "blob",
          "size": 6.4619140625,
          "content": "---\ntitle: Max/Min 函数消除规则\naliases: ['/docs-cn/dev/max-min-eliminate/','/docs-cn/dev/reference/performance/max-min-eliminate/']\nsummary: SQL 中的 max/min 函数消除规则能够将 max/min 聚合函数转换为 TopN 算子，利用索引进行查询。当只有一个 max/min 函数时，会重写为 select max(a) from (select a from t where a is not null order by a desc limit 1) t，利用索引只扫描一行数据。存在多个 max/min 函数时，会先检查列是否有索引能够保序，然后重写为两个子查询的笛卡尔积，最终避免对整个表的扫描。\n---\n\n# Max/Min 函数消除规则\n\n在 SQL 中包含了 `max`/`min` 函数时，查询优化器会尝试使用 `max`/`min` 消除优化规则来将 `max`/`min` 聚合函数转换为 TopN 算子，从而能够有效地利用索引进行查询。\n\n根据 `select` 语句中 `max`/`min` 函数的个数，这一优化规则有以下两种表现形式：\n\n+ [只有一个 max/min 函数时的优化规则](#只有一个-maxmin-函数时的优化规则)\n+ [存在多个 max/min 函数时的优化规则](#存在多个-maxmin-函数时的优化规则)\n\n## 只有一个 max/min 函数时的优化规则\n\n当一个 SQL 满足以下条件时，就会应用这个规则：\n\n+ 只有一个聚合函数，且为 `max` 或者 `min` 函数。\n+ 聚合函数没有相应的 `group by` 语句。\n\n例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect max(a) from t\n```\n\n这时 `max`/`min` 消除优化规则会将其重写为：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect max(a) from (select a from t where a is not null order by a desc limit 1) t\n```\n\n这个新的 SQL 语句在 `a` 列存在索引（或 `a` 列是某个联合索引的前缀）时，能够利用索引只扫描一行数据来得到最大或者最小值，从而避免对整个表的扫描。\n\n上述例子最终得到的执行计划如下：\n\n```\nmysql> explain select max(a) from t;\n+------------------------------+---------+-----------+-------------------------+-------------------------------------+\n| id                           | estRows | task      | access object           | operator info                       |\n+------------------------------+---------+-----------+-------------------------+-------------------------------------+\n| StreamAgg_13                 | 1.00    | root      |                         | funcs:max(test.t.a)->Column#4       |\n| └─Limit_17                   | 1.00    | root      |                         | offset:0, count:1                   |\n|   └─IndexReader_27           | 1.00    | root      |                         | index:Limit_26                      |\n|     └─Limit_26               | 1.00    | cop[tikv] |                         | offset:0, count:1                   |\n|       └─IndexFullScan_25     | 1.00    | cop[tikv] | table:t, index:idx_a(a) | keep order:true, desc, stats:pseudo |\n+------------------------------+---------+-----------+-------------------------+-------------------------------------+\n5 rows in set (0.00 sec)\n```\n\n## 存在多个 max/min 函数时的优化规则\n\n当一个 SQL 满足以下条件时，就会应用这个规则：\n\n+ 有多个聚合函数，且所有的聚合函数都是 max/min\n+ 聚合函数没有相应的 `group by` 语句。\n+ 每个 `max`/`min` 聚合函数参数中的列都有索引能够保序。\n\n下面是一个简单的例子：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect max(a) - min(a) from t\n```\n\n优化规则会先检查 `a` 列是否存在索引能够为其保序，如果存在，这个 SQL 会先被重写为两个子查询的笛卡尔积：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect max_a - min_a\nfrom\n    (select max(a) as max_a from t) t1,\n    (select min(a) as min_a from t) t2\n```\n\n这样，两个子句中的 `max`/`min` 函数就可以使用上述“只有一个 `max`/`min` 函数时的优化规则”分别进行优化，最终重写为：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect max_a - min_a\nfrom\n    (select max(a) as max_a from (select a from t where a is not null order by a desc limit 1) t) t1,\n    (select min(a) as min_a from (select a from t where a is not null order by a asc limit 1) t) t2\n```\n\n同样的，如果 `a` 列能够使用索引保序，那这个优化只会扫描两行数据，避免了对整个表的扫描。但如果 `a` 列没有可以保序的索引，这个变换会使原本只需一次的全表扫描变成两次，因此这个规则就不会被应用。\n\n最后得到的执行计划：\n\n```\nmysql> explain select max(a)-min(a) from t;\n+------------------------------------+---------+-----------+-------------------------+-------------------------------------+\n| id                                 | estRows | task      | access object           | operator info                       |\n+------------------------------------+---------+-----------+-------------------------+-------------------------------------+\n| Projection_17                      | 1.00    | root      |                         | minus(Column#4, Column#5)->Column#6 |\n| └─HashJoin_18                      | 1.00    | root      |                         | CARTESIAN inner join                |\n|   ├─StreamAgg_45(Build)            | 1.00    | root      |                         | funcs:min(test.t.a)->Column#5       |\n|   │ └─Limit_49                     | 1.00    | root      |                         | offset:0, count:1                   |\n|   │   └─IndexReader_59             | 1.00    | root      |                         | index:Limit_58                      |\n|   │     └─Limit_58                 | 1.00    | cop[tikv] |                         | offset:0, count:1                   |\n|   │       └─IndexFullScan_57       | 1.00    | cop[tikv] | table:t, index:idx_a(a) | keep order:true, stats:pseudo       |\n|   └─StreamAgg_24(Probe)            | 1.00    | root      |                         | funcs:max(test.t.a)->Column#4       |\n|     └─Limit_28                     | 1.00    | root      |                         | offset:0, count:1                   |\n|       └─IndexReader_38             | 1.00    | root      |                         | index:Limit_37                      |\n|         └─Limit_37                 | 1.00    | cop[tikv] |                         | offset:0, count:1                   |\n|           └─IndexFullScan_36       | 1.00    | cop[tikv] | table:t, index:idx_a(a) | keep order:true, desc, stats:pseudo |\n+------------------------------------+---------+-----------+-------------------------+-------------------------------------+\n12 rows in set (0.01 sec)\n```\n"
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "metadata-lock.md",
          "type": "blob",
          "size": 7.3896484375,
          "content": "---\ntitle: 元数据锁\nsummary: 介绍 TiDB 中元数据锁的概念、原理、实现和影响。\n---\n\n# 元数据锁\n\n本文介绍了 TiDB 中的元数据锁。\n\n## 元数据锁的概念\n\n在 TiDB 中，对元数据对象的更改采用的是在线异步变更算法。事务在执行时会获取开始时对应的元数据快照。如果事务执行过程中相关表上发生了元数据的更改，为了保证数据的一致性，TiDB 会返回 `Information schema is changed` 的错误，导致用户事务提交失败。\n\n为了解决这个问题，在 TiDB v6.3.0 中，online DDL 算法中引入了元数据锁特性。通过协调表元数据变更过程中 DML 语句和 DDL 语句的优先级，让执行中的 DDL 语句等待持有旧版本元数据的 DML 语句提交，尽可能避免 DML 语句报错。\n\n## 适用场景\n\n元数据锁适用于所有的 DDL 语句，包括但不限于：\n\n- [`ADD INDEX`](/sql-statements/sql-statement-add-index.md)\n- [`ADD COLUMN`](/sql-statements/sql-statement-add-column.md)\n- [`DROP COLUMN`](/sql-statements/sql-statement-drop-column.md)\n- [`DROP INDEX`](/sql-statements/sql-statement-drop-index.md)\n- [`DROP PARTITION`](/partitioned-table.md#分区管理)\n- [`TRUNCATE TABLE`](/sql-statements/sql-statement-truncate.md)\n- [`EXCHANGE PARTITION`](/partitioned-table.md#分区管理)\n- [`CHANGE COLUMN`](/sql-statements/sql-statement-change-column.md)\n- [`MODIFY COLUMN`](/sql-statements/sql-statement-modify-column.md)\n\n使用元数据锁机制会给 TiDB DDL 任务的执行带来一定的性能影响。为了降低元数据锁对 DDL 任务的影响，下列场景不需要加元数据锁：\n\n- 开启了 auto-commit 的查询语句\n- 开启了 Stale Read 功能\n- 访问临时表\n\n## 使用元数据锁\n\n在 v6.5.0 及之后的版本中，TiDB 默认开启元数据锁特性。当集群从 v6.5.0 之前的版本升级到 v6.5.0 及之后的版本时，TiDB 会自动开启元数据锁功能。如果需要关闭元数据锁，你可以将系统变量 [`tidb_enable_metadata_lock`](/system-variables.md#tidb_enable_metadata_lock-从-v630-版本开始引入) 设置为 `OFF`。\n\n## 元数据锁的影响\n\n- 对于 DML 语句来说，元数据锁不会导致 DML 语句被阻塞，因此也不会存在死锁的问题。\n- 开启元数据锁后，事务中某个元数据对象的元数据信息在第一次访问时确定，之后不再变化。\n- 对于 DDL 语句来说，在进行元数据状态变更时，会被涉及相关元数据的旧事务所阻塞。例如以下的执行流程：\n\n    | Session 1 | Session 2 |\n    |:---------------------------|:----------|\n    | `CREATE TABLE t (a INT);`  |           |\n    | `INSERT INTO t VALUES(1);` |           |\n    | `BEGIN;`                   |           |\n    |                            | `ALTER TABLE t ADD COLUMN b INT;` |\n    | `SELECT * FROM t;`<br/>（采用 `t` 表当前的元数据版本，返回 `(a=1，b=NULL)`，同时给表 `t` 加锁）|           |\n    |                            | `ALTER TABLE t ADD COLUMN c INT;`（被 Session 1 阻塞）|\n\n    在可重复读隔离级别下，如果从事务开始到确定一个表的元数据过程中，执行了加索引或者变更列类型等需要更改数据的 DDL，则有以下表现：\n\n    | Session 1                  | Session 2                                 |\n    |:---------------------------|:------------------------------------------|\n    | `CREATE TABLE t (a INT);`  |                                           |\n    | `INSERT INTO t VALUES(1);` |                                           |\n    | `BEGIN;`                   |                                           |\n    |                            | `ALTER TABLE t ADD INDEX idx(a);`         |\n    | `SELECT * FROM t;`（索引 `idx` 不可用）|                                 |\n    | `COMMIT;`                  |                                           |\n    | `BEGIN;`                   |                                           |\n    |                            | `ALTER TABLE t MODIFY COLUMN a CHAR(10);` |\n    | `SELECT * FROM t;`（报错 `ERROR 8028 (HY000): public column a has changed`） |             |\n\n## 元数据锁的可观测性\n\nTiDB v6.3.0 引入了 `mysql.tidb_mdl_view` 视图，可以用于查看当前阻塞的 DDL 的相关信息。\n\n> **注意：**\n>\n> 查询 `mysql.tidb_mdl_view` 视图需要有 [`PROCESS` 权限](https://dev.mysql.com/doc/refman/8.0/en/privileges-provided.html#priv_process)。\n\n下面以给表 `t` 添加索引为例，假设有 DDL 语句 `ALTER TABLE t ADD INDEX idx(a)`：\n\n```sql\nSELECT * FROM mysql.tidb_mdl_view\\G\n*************************** 1. row ***************************\n    JOB_ID: 141\n   DB_NAME: test\nTABLE_NAME: t\n     QUERY: ALTER TABLE t ADD INDEX idx(a)\nSESSION ID: 2199023255957\n  TxnStart: 08-30 16:35:41.313(435643624013955072)\nSQL_DIGESTS: [\"begin\",\"select * from `t`\"]\n1 row in set (0.02 sec)\n```\n\n可以从上面的输出结果中了解到，有一个 `SESSION ID` 为 `2199023255957` 的事务阻塞了该添加索引 DDL 的执行。该事务执行的 SQL 语句如 `SQL_DIGESTS` 中所示，即 ``[\"begin\",\"select * from `t`\"]``。如果想要使被阻塞的 DDL 能够继续执行，可以通过如下 Global `KILL` 命令中止 `SESSION ID` 为 `2199023255957` 的事务：\n\n```sql\nmysql> KILL 2199023255957;\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n中止该事务后，再次查询 `mysql.tidb_mdl_view` 视图。此时，查询结果不再显示上面的事务信息，说明 DDL 不再被阻塞：\n\n```sql\nSELECT * FROM mysql.tidb_mdl_view\\G\nEmpty set (0.01 sec)\n```\n\n## 元数据锁的原理\n\n### 问题描述\n\nTiDB 中 DDL 操作使用的是 online DDL 模式。一个 DDL 语句在执行过程中，需要修改定义的对象元数据版本可能会进行多次小版本变更，而元数据在线异步变更的算法只论证了相邻的两个小版本之间是兼容的，即在相邻的两个元数据版本间操作，不会破坏 DDL 变更对象所存储的数据一致性。\n\n以添加索引为例，DDL 语句的状态会经历 None -> Delete Only，Delete Only -> Write Only，Write Only -> Write Reorg，Write Reorg -> Public 这四个变化。\n\n以下的提交流程将违反“相邻的两个小版本之间是兼容的”约束：\n\n| 事务  | 所用的版本  | 集群最新版本 | 版本差 |\n|:-----|:-----------|:-----------|:----|\n| txn1 | None       | None       | 0   |\n| txn2 | DeleteOnly | DeleteOnly | 0   |\n| txn3 | WriteOnly  | WriteOnly  | 0   |\n| txn4 | None       | WriteOnly  | 2   |\n| txn5 | WriteReorg | WriteReorg | 0   |\n| txn6 | WriteOnly  | WriteReorg | 1   |\n| txn7 | Public     | Public     | 0   |\n\n其中 `txn4` 提交时采用的元数据版本与集群最新的元数据版本相差了两个版本，会影响数据正确性。\n\n### 实现\n\n引入元数据锁会保证整个 TiDB 集群中的所有事务所用的元数据版本最多相差一个版本。为此：\n\n- 执行 DML 语句时，TiDB 会在事务上下文中记录该 DML 语句访问的元数据对象，例如表、视图，以及对应的元数据版本。事务提交时会清空这些记录。\n- DDL 语句进行状态变更时，会向所有的 TiDB 节点推送最新版本的元数据。如果一个 TiDB 节点上所有与这次状态变更相关的事务使用的元数据版本与当前元数据版本之差小于 2，则称这个 TiDB 节点获得了该元数据对象的元数据锁。当集群中的所有 TiDB 节点都获得了该元数据对象的元数据锁后，才能进行下一次状态变更。"
        },
        {
          "name": "metrics-schema.md",
          "type": "blob",
          "size": 17.8447265625,
          "content": "---\ntitle: Metrics Schema\nsummary: 了解 TiDB `METRICS SCHEMA` 系统数据库。\naliases: ['/docs-cn/dev/system-tables/system-table-metrics-schema/','/docs-cn/dev/reference/system-databases/metrics-schema/','/zh/tidb/dev/system-table-metrics-schema/']\n---\n\n# Metrics Schema\n\n`METRICS_SCHEMA` 是基于 Prometheus 中 TiDB 监控指标的一组视图。每个表的 PromQL（Prometheus 查询语言）的源均可在 [`INFORMATION_SCHEMA.METRICS_TABLES`](/information-schema/information-schema-metrics-tables.md) 表中找到。\n\n{{< copyable \"sql\" >}}\n\n```sql\nuse metrics_schema;\nSELECT * FROM uptime;\nSELECT * FROM information_schema.metrics_tables WHERE table_name='uptime'\\G\n```\n\n```sql\n+----------------------------+-----------------+------------+--------------------+\n| time                       | instance        | job        | value              |\n+----------------------------+-----------------+------------+--------------------+\n| 2020-07-06 15:26:26.203000 | 127.0.0.1:10080 | tidb       | 123.60300016403198 |\n| 2020-07-06 15:27:26.203000 | 127.0.0.1:10080 | tidb       | 183.60300016403198 |\n| 2020-07-06 15:26:26.203000 | 127.0.0.1:20180 | tikv       | 123.60300016403198 |\n| 2020-07-06 15:27:26.203000 | 127.0.0.1:20180 | tikv       | 183.60300016403198 |\n| 2020-07-06 15:26:26.203000 | 127.0.0.1:2379  | pd         | 123.60300016403198 |\n| 2020-07-06 15:27:26.203000 | 127.0.0.1:2379  | pd         | 183.60300016403198 |\n| 2020-07-06 15:26:26.203000 | 127.0.0.1:9090  | prometheus | 123.72300004959106 |\n| 2020-07-06 15:27:26.203000 | 127.0.0.1:9090  | prometheus | 183.72300004959106 |\n+----------------------------+-----------------+------------+--------------------+\n8 rows in set (0.00 sec)\n*************************** 1. row ***************************\nTABLE_NAME: uptime\n    PROMQL: (time() - process_start_time_seconds{$LABEL_CONDITIONS})\n    LABELS: instance,job\n  QUANTILE: 0\n   COMMENT: TiDB uptime since last restart(second)\n1 row in set (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nshow tables;\n```\n\n```sql\n+---------------------------------------------------+\n| Tables_in_metrics_schema                          |\n+---------------------------------------------------+\n| abnormal_stores                                   |\n| etcd_disk_wal_fsync_rate                          |\n| etcd_wal_fsync_duration                           |\n| etcd_wal_fsync_total_count                        |\n| etcd_wal_fsync_total_time                         |\n| go_gc_count                                       |\n| go_gc_cpu_usage                                   |\n| go_gc_duration                                    |\n| go_heap_mem_usage                                 |\n| go_threads                                        |\n| goroutines_count                                  |\n| node_cpu_usage                                    |\n| node_disk_available_size                          |\n| node_disk_io_util                                 |\n| node_disk_iops                                    |\n| node_disk_read_latency                            |\n| node_disk_size                                    |\n..\n| tikv_storage_async_request_total_time             |\n| tikv_storage_async_requests                       |\n| tikv_storage_async_requests_total_count           |\n| tikv_storage_command_ops                          |\n| tikv_store_size                                   |\n| tikv_thread_cpu                                   |\n| tikv_thread_nonvoluntary_context_switches         |\n| tikv_thread_voluntary_context_switches            |\n| tikv_threads_io                                   |\n| tikv_threads_state                                |\n| tikv_total_keys                                   |\n| tikv_wal_sync_duration                            |\n| tikv_wal_sync_max_duration                        |\n| tikv_worker_handled_tasks                         |\n| tikv_worker_handled_tasks_total_num               |\n| tikv_worker_pending_tasks                         |\n| tikv_worker_pending_tasks_total_num               |\n| tikv_write_stall_avg_duration                     |\n| tikv_write_stall_max_duration                     |\n| tikv_write_stall_reason                           |\n| up                                                |\n| uptime                                            |\n+---------------------------------------------------+\n626 rows in set (0.00 sec)\n```\n\n`METRICS_SCHEMA` 是监控相关的 summary 表的数据源，例如 [`metrics_summary`](/information-schema/information-schema-metrics-summary.md)、[`metrics_summary_by_label`](/information-schema/information-schema-metrics-summary.md) 和 [`inspection_summary`](/information-schema/information-schema-inspection-summary.md)。\n\n## 更多例子\n\n下面以 `metrics_schema` 中的 `tidb_query_duration` 监控表为例，介绍监控表相关的使用和原理，其他的监控表原理均类似。\n\n查询 `information_schema.metrics_tables` 中关于 `tidb_query_duration` 表相关的信息如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from information_schema.metrics_tables where table_name='tidb_query_duration';\n```\n\n```sql\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+----------+----------------------------------------------+\n| TABLE_NAME          | PROMQL                                                                                                                                                   | LABELS            | QUANTILE | COMMENT                                      |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+----------+----------------------------------------------+\n| tidb_query_duration | histogram_quantile($QUANTILE, sum(rate(tidb_server_handle_query_duration_seconds_bucket{$LABEL_CONDITIONS}[$RANGE_DURATION])) by (le,sql_type,instance)) | instance,sql_type | 0.9      | The quantile of TiDB query durations(second) |\n+---------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+----------+----------------------------------------------+\n```\n\n* `TABLE_NAME`：对应于 `metrics_schema` 中的表名，这里表名是 `tidb_query_duration`。\n* `PROMQL`：监控表的原理是将 SQL 映射成 `PromQL` 后向 Prometheus 请求数据，并将 Prometheus 返回的结果转换成 SQL 查询结果。该字段是 `PromQL` 的表达式模板，查询监控表数据时使用查询条件改写模板中的变量，生成最终的查询表达式。\n* `LABELS`：监控项定义的 label，`tidb_query_duration` 有两个 label，分别是 `instance` 和 `sql_type`。\n* `QUANTILE`：百分位。直方图类型的监控数据会指定一个默认百分位。如果值为 `0`，表示该监控表对应的监控不是直方图。`tidb_query_duration` 默认查询 0.9，也就是 P90 的监控值。\n* `COMMENT`：对这个监控表的解释。可以看出 `tidb_query_duration` 表是用来查询 TiDB query 执行的百分位时间，如 P999/P99/P90 的查询耗时，单位是秒。\n\n再来看 `tidb_query_duration` 的表结构：\n\n{{< copyable \"sql\" >}}\n\n```sql\nshow create table metrics_schema.tidb_query_duration;\n```\n\n```sql\n+---------------------+--------------------------------------------------------------------------------------------------------------------+\n| Table               | Create Table                                                                                                       |\n+---------------------+--------------------------------------------------------------------------------------------------------------------+\n| tidb_query_duration | CREATE TABLE `tidb_query_duration` (                                                                               |\n|                     |   `time` datetime unsigned DEFAULT CURRENT_TIMESTAMP,                                                              |\n|                     |   `instance` varchar(512) DEFAULT NULL,                                                                            |\n|                     |   `sql_type` varchar(512) DEFAULT NULL,                                                                            |\n|                     |   `quantile` double unsigned DEFAULT '0.9',                                                                        |\n|                     |   `value` double unsigned DEFAULT NULL                                                                             |\n|                     | ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin COMMENT='The quantile of TiDB query durations(second)' |\n+---------------------+--------------------------------------------------------------------------------------------------------------------+\n```\n\n* `time`：监控项的时间。\n* `instance` 和 `sql_type`：`tidb_query_duration` 监控项的 label。`instance` 表示监控的地址，`sql_type` 表示执行 SQL 的类似。\n* `quantile`，百分位，直方图类型的监控都会有该列，表示查询的百分位时间，如 `quantile=0.9` 就是查询 P90 的时间。\n* `value`：监控项的值。\n\n下面是查询时间 [`2020-03-25 23:40:00`, `2020-03-25 23:42:00`] 范围内的 P99 的 TiDB Query 耗时：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from metrics_schema.tidb_query_duration where value is not null and time>='2020-03-25 23:40:00' and time <= '2020-03-25 23:42:00' and quantile=0.99;\n```\n\n```sql\n+---------------------+-------------------+----------+----------+----------------+\n| time                | instance          | sql_type | quantile | value          |\n+---------------------+-------------------+----------+----------+----------------+\n| 2020-03-25 23:40:00 | 172.16.5.40:10089 | Insert   | 0.99     | 0.509929485256 |\n| 2020-03-25 23:41:00 | 172.16.5.40:10089 | Insert   | 0.99     | 0.494690793986 |\n| 2020-03-25 23:42:00 | 172.16.5.40:10089 | Insert   | 0.99     | 0.493460506934 |\n| 2020-03-25 23:40:00 | 172.16.5.40:10089 | Select   | 0.99     | 0.152058493415 |\n| 2020-03-25 23:41:00 | 172.16.5.40:10089 | Select   | 0.99     | 0.152193879678 |\n| 2020-03-25 23:42:00 | 172.16.5.40:10089 | Select   | 0.99     | 0.140498483232 |\n| 2020-03-25 23:40:00 | 172.16.5.40:10089 | internal | 0.99     | 0.47104        |\n| 2020-03-25 23:41:00 | 172.16.5.40:10089 | internal | 0.99     | 0.11776        |\n| 2020-03-25 23:42:00 | 172.16.5.40:10089 | internal | 0.99     | 0.11776        |\n+---------------------+-------------------+----------+----------+----------------+\n```\n\n以上查询结果的第一行意思是，在 `2020-03-25 23:40:00` 时，在 TiDB 实例 `172.16.5.40:10089` 上，`Insert` 类型的语句的 P99 执行时间是 0.509929485256 秒。其他各行的含义类似，`sql_type` 列的其他值含义如下：\n\n* `Select`：表示执行的 `select` 类型的语句。\n* `internal`：表示 TiDB 的内部 SQL 语句，一般是统计信息更新，获取全局变量相关的内部语句。\n\n进一步再查看上面语句的执行计划如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\ndesc select * from metrics_schema.tidb_query_duration where value is not null and time>='2020-03-25 23:40:00' and time <= '2020-03-25 23:42:00' and quantile=0.99;\n```\n\n```sql\n+------------------+----------+------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id               | estRows  | task | access object             | operator info                                                                                                                                                                                          |\n+------------------+----------+------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Selection_5      | 8000.00  | root |                           | not(isnull(Column#5))                                                                                                                                                                                  |\n| └─MemTableScan_6 | 10000.00 | root | table:tidb_query_duration | PromQL:histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket{}[60s])) by (le,sql_type,instance)), start_time:2020-03-25 23:40:00, end_time:2020-03-25 23:42:00, step:1m0s |\n+------------------+----------+------+---------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n\n可以发现执行计划中有一个 `PromQL`，以及查询监控的 `start_time` 和 `end_time`，还有 `step` 值，在实际执行时，TiDB 会调用 Prometheus 的 `query_range` HTTP API 接口来查询监控数据。\n\n从以上结果可知，在 `[2020-03-25 23:40:00, 2020-03-25 23:42:00]` 时间范围内，每个 label 只有三个时间的值，间隔时间是 1 分钟，即执行计划中的 `step` 值。该间隔时间由以下两个 session 变量决定：\n\n* `tidb_metric_query_step`：查询的分辨率步长。从 Prometheus 的 `query_range` 接口查询数据时需要指定 `start_time`，`end_time` 和 `step`，其中 `step` 会使用该变量的值。\n* `tidb_metric_query_range_duration`：查询监控时，会将 `PROMQL` 中的 `$RANGE_DURATION` 替换成该变量的值，默认值是 60 秒。\n\n如果想要查看不同时间粒度的监控项的值，用户可以修改上面两个 session 变量后查询监控表，示例如下：\n\n首先修改两个 session 变量的值，将时间粒度设置为 30 秒。\n\n> **注意：**\n>\n> Prometheus 支持查询的最小粒度为 30 秒。\n\n{{< copyable \"sql\" >}}\n\n```sql\nset @@tidb_metric_query_step=30;\nset @@tidb_metric_query_range_duration=30;\n```\n\n再查询 `tidb_query_duration` 监控如下，可以发现在三分钟时间范围内，每个 label 有六个时间的值，每个值时间间隔是 30 秒。\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from metrics_schema.tidb_query_duration where value is not null and time>='2020-03-25 23:40:00' and time <= '2020-03-25 23:42:00' and quantile=0.99;\n```\n\n```sql\n+---------------------+-------------------+----------+----------+-----------------+\n| time                | instance          | sql_type | quantile | value           |\n+---------------------+-------------------+----------+----------+-----------------+\n| 2020-03-25 23:40:00 | 172.16.5.40:10089 | Insert   | 0.99     | 0.483285651924  |\n| 2020-03-25 23:40:30 | 172.16.5.40:10089 | Insert   | 0.99     | 0.484151462113  |\n| 2020-03-25 23:41:00 | 172.16.5.40:10089 | Insert   | 0.99     | 0.504576        |\n| 2020-03-25 23:41:30 | 172.16.5.40:10089 | Insert   | 0.99     | 0.493577384561  |\n| 2020-03-25 23:42:00 | 172.16.5.40:10089 | Insert   | 0.99     | 0.49482474311   |\n| 2020-03-25 23:40:00 | 172.16.5.40:10089 | Select   | 0.99     | 0.189253402185  |\n| 2020-03-25 23:40:30 | 172.16.5.40:10089 | Select   | 0.99     | 0.184224951851  |\n| 2020-03-25 23:41:00 | 172.16.5.40:10089 | Select   | 0.99     | 0.151673410553  |\n| 2020-03-25 23:41:30 | 172.16.5.40:10089 | Select   | 0.99     | 0.127953838989  |\n| 2020-03-25 23:42:00 | 172.16.5.40:10089 | Select   | 0.99     | 0.127455434547  |\n| 2020-03-25 23:40:00 | 172.16.5.40:10089 | internal | 0.99     | 0.0624          |\n| 2020-03-25 23:40:30 | 172.16.5.40:10089 | internal | 0.99     | 0.12416         |\n| 2020-03-25 23:41:00 | 172.16.5.40:10089 | internal | 0.99     | 0.0304          |\n| 2020-03-25 23:41:30 | 172.16.5.40:10089 | internal | 0.99     | 0.06272         |\n| 2020-03-25 23:42:00 | 172.16.5.40:10089 | internal | 0.99     | 0.0629333333333 |\n+---------------------+-------------------+----------+----------+-----------------+\n```\n\n最后查看执行计划，也会发现执行计划中的 `PromQL` 以及 `step` 的值都已经变成了 30 秒。\n\n{{< copyable \"sql\" >}}\n\n```sql\ndesc select * from metrics_schema.tidb_query_duration where value is not null and time>='2020-03-25 23:40:00' and time <= '2020-03-25 23:42:00' and quantile=0.99;\n```\n\n```sql\n+------------------+----------+------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id               | estRows  | task | access object             | operator info                                                                                                                                                                                         |\n+------------------+----------+------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Selection_5      | 8000.00  | root |                           | not(isnull(Column#5))                                                                                                                                                                                 |\n| └─MemTableScan_6 | 10000.00 | root | table:tidb_query_duration | PromQL:histogram_quantile(0.99, sum(rate(tidb_server_handle_query_duration_seconds_bucket{}[30s])) by (le,sql_type,instance)), start_time:2020-03-25 23:40:00, end_time:2020-03-25 23:42:00, step:30s |\n+------------------+----------+------+---------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n"
        },
        {
          "name": "migrate-aurora-to-tidb.md",
          "type": "blob",
          "size": 14.7216796875,
          "content": "---\ntitle: 从 Amazon Aurora 迁移数据到 TiDB\nsummary: 介绍如何使用快照从 Amazon Aurora 迁移数据到 TiDB。\naliases: ['/zh/tidb/dev/migrate-from-aurora-using-lightning/','/docs-cn/dev/migrate-from-aurora-mysql-database/','/docs-cn/dev/how-to/migrate/from-mysql-aurora/','/docs-cn/dev/how-to/migrate/from-aurora/','/zh/tidb/dev/migrate-from-aurora-mysql-database/','/zh/tidb/dev/migrate-from-mysql-aurora','/zh/tidb/stable/migrate-from-aurora-using-lightning/']\n---\n\n# 从 Amazon Aurora 迁移数据到 TiDB\n\n本文档介绍如何从 Amazon Aurora 迁移数据到 TiDB，迁移过程采用 [DB snapshot](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora.Managing.Backups.html)，可以节约大量的空间和时间成本。整个迁移包含两个过程：\n\n- 使用 TiDB Lightning 导入全量数据到 TiDB\n- 使用 DM 持续增量同步到 TiDB（可选）\n\n## 前提条件\n\n- [安装 Dumpling 和 TiDB Lightning](/migration-tools.md)。如果你要在目标端手动创建相应的表，则无需安装 Dumpling。\n- [获取 Dumpling 所需上游数据库权限](/dumpling-overview.md#需要的权限)。\n- [获取 TiDB Lightning 所需下游数据库权限](/tidb-lightning/tidb-lightning-faq.md#tidb-lightning-对下游数据库的账号权限要求是怎样的)。\n\n## 导入全量数据到 TiDB\n\n### 第 1 步：导出和导入 schema 文件\n\n如果你已经提前手动在目标库创建好了相应的表，则可以跳过本节内容。\n\n#### 1.1 导出 schema 文件\n\n因为 Amazon Aurora 生成的快照文件并不包含建表语句文件，所以你需要使用 Dumpling 自行导出 schema 并使用 TiDB Lightning 在下游创建 schema。\n\n运行以下命令时，建议使用 `--filter` 参数仅导出所需表的 schema。命令中所用参数描述，请参考 [Dumpling 主要选项表](/dumpling-overview.md#dumpling-主要选项表)。\n\n```shell\nexport AWS_ACCESS_KEY_ID=${access_key}\nexport AWS_SECRET_ACCESS_KEY=${secret_key}\ntiup dumpling --host ${host} --port 3306 --user root --password ${password} --filter 'my_db1.table[12],mydb.*' --consistency none --no-data --output 's3://my-bucket/schema-backup'\n```\n\n记录上面命令中导出的 schema 的 URI，例如 's3://my-bucket/schema-backup'，后续导入 schema 时要用到。\n\n为了获取 Amazon S3 的访问权限，可以将该 Amazon S3 的 Secret Access Key 和 Access Key 作为环境变量传入 Dumpling 或 TiDB Lightning。另外，Dumpling 或 TiDB Lightning 也可以通过 `~/.aws/credentials` 读取凭证文件。使用凭证文件可以让这台机器上所有的 Dumpling 或 TiDB Lightning 任务无需再次传入相关 Secret Access Key 和 Access Key。\n\n#### 1.2 编写用于导入 schema 文件的 TiDB Lightning 配置文件\n\n新建 `tidb-lightning-schema.toml` 文件，将以下内容复制到文件中并替换对应的内容。\n\n```toml\n[tidb]\n\n# 目标 TiDB 集群信息。\nhost = ${host}\nport = ${port}\nuser = \"${user_name}\"\npassword = \"${password}\"\nstatus-port = ${status-port}  # TiDB 的“状态端口”，通常为 10080\npd-addr = \"${ip}:${port}\"     # 集群 PD 的地址，port 通常为 2379\n\n[tikv-importer]\n# 采用默认的物理导入模式 (\"local\")。注意该模式在导入期间下游 TiDB 无法对外提供服务。\n# 关于后端模式更多信息，请参阅：https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-overview\nbackend = \"local\"\n\n# 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小。\n# 建议设为与 `data-source-dir` 不同的磁盘目录并使用闪存介质，独占 IO 会获得更好的导入性能。\nsorted-kv-dir = \"${path}\"\n\n[mydumper]\n# 设置从 Amazon Aurora 导出的 schema 文件的地址\ndata-source-dir = \"s3://my-bucket/schema-backup\"\n```\n\n如果需要在 TiDB 开启 TLS，请参考 [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)。\n\n#### 1.3 导入 schema 文件\n\n使用 TiDB Lightning 导入 schema 到下游的 TiDB。\n\n```shell\nexport AWS_ACCESS_KEY_ID=${access_key}\nexport AWS_SECRET_ACCESS_KEY=${secret_access_key}\nnohup tiup tidb-lightning -config tidb-lightning-schema.toml > nohup.out 2>&1 &\n```\n\n### 第 2 步：导出和导入 Amazon Aurora 快照文件\n\n本节介绍如何导出和导入 Amazon Aurora 快照文件。\n\n#### 2.1 导出 Amazon Aurora 快照文件到 Amazon S3\n\n1. 获取 Amazon Aurora binlog 的名称及位置以便于后续的增量迁移。在 Amazon Aurora 上，执行 `SHOW MASTER STATUS` 并记录当前 binlog 位置：\n\n    ```sql\n    SHOW MASTER STATUS;\n    ```\n\n    你将得到类似以下的输出，请记录 binlog 名称和位置，供后续步骤使用：\n\n    ```\n    +----------------------------+----------+--------------+------------------+-------------------+\n    | File                       | Position | Binlog_Do_DB | Binlog_Ignore_DB | Executed_Gtid_Set |\n    +----------------------------+----------+--------------+------------------+-------------------+\n    | mysql-bin-changelog.018128 |    52806 |              |                  |                   |\n    +----------------------------+----------+--------------+------------------+-------------------+\n    1 row in set (0.012 sec)\n    ```\n\n2. 导出 Amazon Aurora 快照文件。具体方式请参考 Amazon Aurora 的官方文档：[Exporting DB snapshot data to Amazon S3](https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/USER_ExportSnapshot.html)。请注意，执行 `SHOW MASTER STATUS` 命令和导出 Amazon Aurora 快照文件的时间间隔建议不要超过 5 分钟，否则记录的 binlog 位置过旧可能导致增量同步时产生数据冲突。\n\n#### 2.2 编写用于导入快照文件的 TiDB Lightning 配置文件\n\n新建 `tidb-lightning-data.toml` 文件，将以下内容复制到文件中并替换对应的内容。\n\n```toml\n[tidb]\n\n# 目标 TiDB 集群信息。\nhost = ${host}\nport = ${port}\nuser = \"${user_name}\"\npassword = \"${password}\"\nstatus-port = ${status-port}  # TiDB 的“状态端口”，通常为 10080\npd-addr = \"${ip}:${port}\"     # 集群 PD 的地址，port 通常为 2379\n\n[tikv-importer]\n# 采用默认的物理导入模式 (\"local\")。注意该模式在导入期间下游 TiDB 无法对外提供服务。\n# 关于后端模式更多信息请参阅：https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-overview\nbackend = \"local\"\n\n# 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小。\n# 建议设为与 `data-source-dir` 不同的磁盘目录并使用闪存介质，独占 IO 会获得更好的导入性能。\nsorted-kv-dir = \"${path}\"\n\n[mydumper]\n# 设置从 Amazon Aurora 导出的快照文件的地址\ndata-source-dir = \"s3://my-bucket/sql-backup\"\n\n[[mydumper.files]]\n# 解析 parquet 文件所需的表达式\npattern = '(?i)^(?:[^/]*/)*([a-z0-9_]+)\\.([a-z0-9_]+)/(?:[^/]*/)*(?:[a-z0-9\\-_.]+\\.(parquet))$'\nschema = '$1'\ntable = '$2'\ntype = '$3'\n```\n\n如果需要在 TiDB 开启 TLS，请参考 [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)。\n\n#### 2.3 导入全量数据到 TiDB\n\n1. 使用 TiDB Lightning 导入 Aurora Snapshot 的数据到 TiDB。 \n\n    ```shell\n    export AWS_ACCESS_KEY_ID=${access_key}\n    export AWS_SECRET_ACCESS_KEY=${secret_access_key}\n    nohup tiup tidb-lightning -config tidb-lightning-data.toml > nohup.out 2>&1 &\n    ```\n\n2. 导入开始后，可以采用以下任意方式查看进度：\n\n    - 通过 `grep` 日志关键字 `progress` 查看进度，默认 5 分钟更新一次。\n    - 通过监控面板查看进度，请参考 [TiDB Lightning 监控](/tidb-lightning/monitor-tidb-lightning.md)。\n    - 通过 Web 页面查看进度，请参考 [Web 界面](/tidb-lightning/tidb-lightning-web-interface.md)。\n\n3. 导入完毕后，TiDB Lightning 会自动退出。查看 `tidb-lightning.log` 日志末尾是否有 `the whole procedure completed` 信息，如果有，表示导入成功。如果没有，则表示导入遇到了问题，可根据日志中的 error 提示解决遇到的问题。\n\n> **注意：**\n>\n> 无论导入成功与否，最后一行都会显示 `tidb lightning exit`。它只是表示 TiDB Lightning 正常退出，不代表任务完成。\n\n如果导入过程中遇到问题，请参见 [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)。\n\n## 持续增量同步数据到 TiDB（可选）\n\n### 前提条件\n\n- [安装 DM 集群](/dm/deploy-a-dm-cluster-using-tiup.md)\n- [获取 DM 所需上下游数据库权限](/dm/dm-worker-intro.md)\n\n### 第 1 步：创建数据源\n\n1. 新建 `source1.yaml` 文件，写入以下内容：\n\n    ```yaml\n    # 唯一命名，不可重复。\n    source-id: \"mysql-01\"\n\n    # DM-worker 是否使用全局事务标识符 (GTID) 拉取 binlog。使用前提是上游 MySQL 已开启 GTID 模式。若上游存在主从自动切换，则必须使用 GTID 模式。\n    enable-gtid: false\n\n    from:\n      host: \"${host}\"         # 例如：172.16.10.81\n      user: \"root\"\n      password: \"${password}\" # 支持但不推荐使用明文密码，建议使用 dmctl encrypt 对明文密码进行加密后使用\n      port: 3306\n    ```\n\n2. 在终端中执行下面的命令，使用 `tiup dmctl` 将数据源配置加载到 DM 集群中:\n\n    ```shell\n    tiup dmctl --master-addr ${advertise-addr} operate-source create source1.yaml\n    ```\n\n    该命令中的参数描述如下：\n\n    | 参数           | 描述 |\n    | -              | - |\n    | `--master-addr` | dmctl 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如：172.16.10.71:8261 |\n    | `operate-source create` |向 DM 集群加载数据源 |\n\n### 第 2 步：创建迁移任务\n\n新建 `task1.yaml` 文件，写入以下内容：\n\n{{< copyable \"\" >}}\n\n```yaml\n# 任务名，多个同时运行的任务不能重名。\nname: \"test\"\n# 任务模式，可设为\n# full：只进行全量数据迁移\n# incremental： binlog 实时同步\n# all： 全量 + binlog 迁移\ntask-mode: \"incremental\"\n# 下游 TiDB 配置信息。\ntarget-database:\n  host: \"${host}\"                   # 例如：172.16.10.83\n  port: 4000\n  user: \"root\"\n  password: \"${password}\"           # 支持但不推荐使用明文密码，建议使用 dmctl encrypt 对明文密码进行加密后使用\n\n# 黑白名单全局配置，各实例通过配置项名引用。\nblock-allow-list:                     # 如果 DM 版本早于 v2.0.0-beta.2 则使用 black-white-list。\n  listA:                              # 名称\n    do-tables:                        # 需要迁移的上游表的白名单。\n    - db-name: \"test_db\"              # 需要迁移的表的库名。\n      tbl-name: \"test_table\"          # 需要迁移的表的名称。\n\n# 配置数据源\nmysql-instances:\n  - source-id: \"mysql-01\"               # 数据源 ID，即 source1.yaml 中的 source-id\n    block-allow-list: \"listA\"           # 引入上面黑白名单配置。\n#    syncer-config-name: \"global\"        # syncer 配置的名称\n    meta:                               # `task-mode` 为 `incremental` 且下游数据库的 `checkpoint` 不存在时 binlog 迁移开始的位置; 如果 checkpoint 存在，则以 `checkpoint` 为准。如果 `meta` 项和下游数据库的 `checkpoint` 都不存在，则从上游当前最新的 binlog 位置开始迁移。\n      binlog-name: \"mysql-bin.000004\"   # “Step 1. 导出 Amazon Aurora 快照文件到 Amazon S3” 中记录的日志位置，当上游存在主从切换时，必须使用 gtid。\n      binlog-pos: 109227\n      # binlog-gtid: \"09bec856-ba95-11ea-850a-58f2b4af5188:1-9\"\n\n# 【可选配置】 如果增量数据迁移需要重复迁移已经在全量数据迁移中完成迁移的数据，则需要开启 safe mode 避免增量数据迁移报错。\n   ##  该场景多见于以下情况：全量迁移的数据不属于数据源的一个一致性快照，随后从一个早于全量迁移数据之前的位置开始同步增量数据。\n   # syncers:            # sync 处理单元的运行配置参数。\n   #  global:           # 配置名称。\n   #    safe-mode: true # 设置为 true，会将来自数据源的 INSERT 改写为 REPLACE，将 UPDATE 改写为 DELETE 与 REPLACE，从而保证在表结构中存在主键或唯一索引的条件下迁移数据时可以重复导入 DML。在启动或恢复增量复制任务的前 1 分钟内 TiDB DM 会自动启动 safe mode。\n```\n\n以上内容为执行迁移的最小任务配置。关于任务的更多配置项，可以参考 [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)\n\n### 第 3 步：启动任务\n\n在你启动数据迁移任务之前，建议使用 `check-task` 命令检查配置是否符合 DM 的配置要求，以降低后期报错的概率：\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} check-task task.yaml\n```\n\n使用 `tiup dmctl` 执行以下命令启动数据迁移任务。\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} start-task task.yaml\n```\n\n该命令中的参数描述如下：\n\n|参数|描述|\n|-|-|\n|`--master-addr`|dmctl 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如：172.16.10.71:8261|\n|`start-task`|命令用于创建数据迁移任务|\n\n如果任务启动失败，可根据返回结果的提示进行配置变更后，再次执行上述命令，重新启动任务。遇到问题请参考[故障及处理方法](/dm/dm-error-handling.md)以及[常见问题](/dm/dm-faq.md)。\n\n### 第 4 步：查看任务状态\n\n如需了解 DM 集群中是否存在正在运行的迁移任务及任务状态等信息，可使用 `tiup dmctl` 执行 `query-status` 命令进行查询：\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} query-status ${task-name}\n```\n\n关于查询结果的详细解读，请参考[查询状态](/dm/dm-query-status.md)。\n\n### 第 5 步：监控任务与查看日志\n\n要查看迁移任务的历史状态以及更多的内部运行指标，可参考以下步骤。\n\n如果使用 TiUP 部署 DM 集群时，正确部署了 Prometheus、Alertmanager 与 Grafana，则使用部署时填写的 IP 及端口进入 Grafana，选择 DM 的 dashboard 查看 DM 相关监控项。\n\nDM 在运行过程中，DM-worker、DM-master 及 dmctl 都会通过日志输出相关信息。各组件的日志目录如下：\n\n- DM-master 日志目录：通过 DM-master 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-master-8261/log/`。\n- DM-worker 日志目录：通过 DM-worker 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-worker-8262/log/`。\n\n## 探索更多\n\n- [暂停数据迁移任务](/dm/dm-pause-task.md)\n- [恢复数据迁移任务](/dm/dm-resume-task.md)\n- [停止数据迁移任务](/dm/dm-stop-task.md)\n- [导出和导入集群的数据源和任务配置](/dm/dm-export-import-config.md)\n- [处理出错的 DDL 语句](/dm/handle-failed-ddl-statements.md)\n"
        },
        {
          "name": "migrate-from-csv-files-to-tidb.md",
          "type": "blob",
          "size": 7.87890625,
          "content": "---\ntitle: 从 CSV 文件迁移数据到 TiDB\nsummary: 介绍如何从 CSV 等文件迁移数据到 TiDB。\n---\n\n# 从 CSV 文件迁移数据到 TiDB\n\n本文档介绍如何从 CSV 文件迁移数据到 TiDB。\n\nTiDB Lightning 支持读取 CSV 格式的文件，以及其他定界符格式，如 TSV（制表符分隔值）。对于其他“平面文件”类型的数据导入，也可以参考本文档进行。\n\n## 前提条件\n\n- [安装 TiDB Lightning](/migration-tools.md)。\n- [获取 Lightning 所需下游数据库权限](/tidb-lightning/tidb-lightning-requirements.md#目标数据库权限要求)。\n\n## 第 1 步：准备 CSV 文件\n\n将所有要导入的 CSV 文件放在同一目录下，若要 TiDB Lightning 识别所有 CSV 文件，文件名必须满足以下格式：\n\n- 包含整张表数据的 CSV 文件，需命名为 `${db_name}.${table_name}.csv`。\n- 如果一张表分布于多个 CSV 文件，这些 CSV 文件命名需加上文件编号的后缀，如 `${db_name}.${table_name}.003.csv`。数字部分不需要连续，但必须递增，并且需要用零填充数字部分，保证后缀为同样长度。\n\nTiDB Lightning 将递归地寻找该目录下及其子目录内的所有 `.csv` 文件。\n\n## 第 2 步：创建目标表结构\n\nCSV 文件自身未包含表结构信息。要将 CSV 数据导入 TiDB，就必须为数据提供表结构。可以通过以下任一方法创建表结构：\n\n* **方法一**：使用 TiDB Lightning 创建表结构。\n\n    编写包含 DDL 语句的 SQL 文件如下：\n\n    - 文件名格式为 `${db_name}-schema-create.sql`，其内容需包含 `CREATE DATABASE` 语句。\n    - 文件名格式为 `${db_name}.${table_name}-schema.sql`，其内容需包含 `CREATE TABLE` 语句。\n\n* **方法二**：手动在下游 TiDB 建库和表。\n\n## 第 3 步：编写配置文件\n\n新建文件 `tidb-lightning.toml`，包含以下内容：\n\n```toml\n[lightning]\n# 日志\nlevel = \"info\"\nfile = \"tidb-lightning.log\"\n\n[tikv-importer]\n# \"local\"：默认使用该模式，适用于 TB 级以上大数据量，但导入期间下游 TiDB 无法对外提供服务。\n# \"tidb\"：TB 级以下数据量也可以采用 `tidb` 后端模式，下游 TiDB 可正常提供服务。关于导入模式更多信息请参阅：https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-overview#tidb-lightning-整体架构\nbackend = \"local\"\n# 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小，建议设为与 `data-source-dir` 不同的磁盘目录并使用闪存介质，独占 IO 会获得更好的导入性能\nsorted-kv-dir = \"/mnt/ssd/sorted-kv-dir\"\n\n[mydumper]\n# 源数据目录。\ndata-source-dir = \"${data-path}\" # 本地或 S3 路径，例如：'s3://my-bucket/sql-backup'\n\n# 定义 CSV 格式\n[mydumper.csv]\n# 字段分隔符，必须不为空。如果源文件中包含非字符串或数值类型的字段（如 binary, blob, bit 等），则不建议源文件使用默认的“,”简单分隔符，推荐“|+|”等非常见字符组合\nseparator = ','\n# 引用定界符，可以为零或多个字符。\ndelimiter = '\"'\n# 行结束符。默认将 \\r、\\n、\\r\\n 都作为行结束符处理。\n# terminator = \"\\r\\n\"\n# CSV 文件是否包含表头。\n# 如果为 true，则 lightning 会使用首行内容解析字段的对应关系。\nheader = true\n# CSV 是否包含 NULL。\n# 如果为 true，CSV 文件的任何列都不能解析为 NULL。\nnot-null = false\n# 如果 `not-null` 为 false（即 CSV 可以包含 NULL），\n# 为以下值的字段将会被解析为 NULL。\nnull = '\\N'\n# 是否将字符串中包含的反斜杠（'\\'）字符作为转义字符处理\nbackslash-escape = true\n# 是否移除行尾的最后一个分隔符。\ntrim-last-separator = false\n\n[tidb]\n# 目标集群的信息\nhost = ${host}                # 例如：172.16.32.1\nport = ${port}                # 例如：4000\nuser = \"${user_name}\"         # 例如：\"root\"\npassword = \"${password}\"      # 例如：\"rootroot\"\nstatus-port = ${status-port}  # 导入过程 Lightning 需要在从 TiDB 的“状态端口”获取表结构信息，例如：10080\npd-addr = \"${ip}:${port}\"     # 集群 PD 的地址，Lightning 通过 PD 获取部分信息，例如 172.16.31.3:2379。当 backend = \"local\" 时 status-port 和 pd-addr 必须正确填写，否则导入将出现异常。\n```\n\n关于配置文件更多信息，可参阅 [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)。\n\n## 第 4 步：导入性能优化（可选）\n\n导入文件的大小统一约为 256 MiB 时，TiDB Lightning 可达到最佳工作状态。如果导入单个 CSV 大文件，TiDB Lightning 在默认配置下只能使用一个线程来处理，这会降低导入速度。\n\n要解决此问题，可先将 CSV 文件分割为多个文件。对于通用格式的 CSV 文件，在没有读取整个文件的情况下，无法快速确定行的开始和结束位置。因此，默认情况下 TiDB Lightning 不会自动分割 CSV 文件。但如果你确定待导入的 CSV 文件符合特定的限制要求，则可以启用 `strict-format` 模式。启用后，TiDB Lightning 会将单个 CSV 大文件分割为单个大小为 256 MiB 的多个文件块进行并行处理。\n\n> **注意：**\n>\n> 如果 CSV 文件不是严格格式，但 `strict-format` 被误设为 `true`，跨多行的单个完整字段会被分割成两部分，导致解析失败，甚至不报错地导入已损坏的数据。\n\n严格格式的 CSV 文件中，每个字段仅占一行，即必须满足以下条件之一：\n\n- delimiter 为空；\n- 每个字段不包含 CR (\\\\r）或 LF（\\\\n）。\n\n严格格式 `strict-format` 的 CSV 文件需要显式指定行结束符 `terminator`。\n\n如果你确认满足条件，可按如下配置开启 `strict-format` 模式以加快导入速度。\n\n```toml\n[mydumper]\nstrict-format = true\n```\n\n## 第 5 步：执行导入\n\n运行 `tidb-lightning`。如果直接在命令行中启动程序，可能会因为 `SIGHUP` 信号而退出，建议配合 `nohup` 或 `screen` 等工具，如：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\nnohup tiup tidb-lightning -config tidb-lightning.toml > nohup.out 2>&1 &\n```\n\n导入开始后，可以采用以下任意方式查看进度：\n\n- 通过 `grep` 日志关键字 `progress` 查看进度，默认 5 分钟更新一次。\n- 通过监控面板查看进度，请参考 [TiDB Lightning 监控](/tidb-lightning/monitor-tidb-lightning.md)。\n- 通过 Web 页面查看进度，请参考 [Web 界面](/tidb-lightning/tidb-lightning-web-interface.md)。\n\n导入完毕后，TiDB Lightning 会自动退出。查看 `tidb-lightning.log` 日志末尾是否有 `the whole procedure completed` 信息，如果有，表示导入成功。如果没有，则表示导入遇到了问题，可根据日志中的 error 提示解决遇到的问题。\n\n> **注意：**\n>\n> 无论导入成功与否，最后一行都会显示 `tidb lightning exit`。它只是表示 TiDB Lightning 正常退出，不代表任务完成。\n\n如果导入过程中遇到问题，请参见 [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)。\n\n## 其他格式的文件\n\n若数据源为其他格式，除文件名仍必须以 `.csv` 结尾外，配置文件 `tidb-lightning.toml` 的 `[mydumper.csv]` 格式定义同样需要做相应修改。常见的格式修改如下：\n\n**TSV：**\n\n```\n# 格式示例\n# ID    Region    Count\n# 1     East      32\n# 2     South     NULL\n# 3     West      10\n# 4     North     39\n\n# 格式配置\n[mydumper.csv]\nseparator = \"\\t\"\ndelimiter = ''\nheader = true\nnot-null = false\nnull = 'NULL'\nbackslash-escape = false\ntrim-last-separator = false\n```\n\n**TPC-H DBGEN：**\n\n```\n# 格式示例\n# 1|East|32|\n# 2|South|0|\n# 3|West|10|\n# 4|North|39|\n\n# 格式配置\n[mydumper.csv]\nseparator = '|'\ndelimiter = ''\nheader = false\nnot-null = true\nbackslash-escape = false\ntrim-last-separator = true\n```\n\n## 探索更多\n\n- [CSV 支持与限制](/tidb-lightning/tidb-lightning-data-source.md#csv)\n"
        },
        {
          "name": "migrate-from-mariadb.md",
          "type": "blob",
          "size": 13.3251953125,
          "content": "---\ntitle: 从 MariaDB 文件迁移数据到 TiDB\nsummary: 介绍如何将数据从 MariaDB 文件迁移数据到 TiDB。\n---\n\n# 从 MariaDB 文件迁移数据到 TiDB\n\n本文档介绍了如何将数据从 MariaDB 服务器迁移到 TiDB 集群。\n\n## 前提条件\n\n选择合适的迁移策略：\n\n- 第一种策略是[使用 Dumpling 导出数据然后使用 TiDB Lightning 恢复](#使用-dumpling-导出数据后使用-tidb-lightning-导入)。该策略适用于所有版本的 MariaDB，但缺点是需要更长的停机时间。\n- 第二种策略是[使用 DM 迁移数据](#使用-dm-迁移数据)从 MariaDB 到 TiDB。注意 DM 不支持所有版本的 MariaDB。支持的版本请参考 [DM 兼容性目录](/dm/dm-compatibility-catalog.md#tidb-data-migration-兼容性目录)。\n\n除了以上两种策略，还有其他策略适用于特定的场景，例如：\n\n- 使用 Object Relational Mapping (ORM) 工具重新部署和迁移数据。\n- 修改应用程序，使其在迁移过程中同时写入 MariaDB 和 TiDB。\n\n本文档仅介绍前两种策略。\n\n根据你选择的策略，准备以下内容：\n\n- 对于第一种策略：\n    - 安装 [Dumpling](/dumpling-overview.md) 和 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md)。\n    - 确保你在 MariaDB 服务器上拥有[所需的权限](/dumpling-overview.md#需要的权限)，以便 Dumpling 导出数据。\n- 对于第二种策略，设置 [DM](/dm/dm-overview.md)。\n\n## 检查兼容性\n\nTiDB 和 [MySQL 兼容](/mysql-compatibility.md)，而 MySQL 和 MariaDB 也有很多通用的特性。在迁移数据前需要注意，可能某些 MariaDB 特有的特性和 TiDB 并不兼容。\n\n除了检查本小节介绍的事项之外，建议你参考 [MariaDB Compatibility & Differences](https://mariadb.com/kb/en/compatibility-differences/) 检查相关配置。\n\n### 认证\n\n[与 MySQL 安全特性差异](/security-compatibility-with-mysql.md)文档列举了 TiDB 支持的认证方式。TiDB 不支持 MariaDB 中的某些认证方式，你可能需要为账号创建新的密码哈希，或采取其他相应措施。\n\n你可以执行以下语句检查使用的认证方式：\n\n```sql\nSELECT\n  plugin,\n  COUNT(*)\nFROM\n  mysql.user\nGROUP BY\n  plugin;\n```\n\n```sql\n+-----------------------+----------+\n| plugin                | COUNT(*) |\n+-----------------------+----------+\n| mysql_native_password |       11 |\n+-----------------------+----------+\n1 row in set (0.002 sec)\n```\n\n### 系统版本表\n\nTiDB 不支持[系统版本表 (System-Versioned Table)](https://mariadb.com/kb/en/system-versioned-tables/)。但是 TiDB 支持 [`AS OF TIMESTAMP`](/as-of-timestamp.md)，可以在某些场景下取代系统版本表。\n\n你可以执行下列语句检查受影响的表：\n\n```sql\nSELECT\n  TABLE_SCHEMA,\n  TABLE_NAME\nFROM\n  information_schema.tables\nWHERE\n  TABLE_TYPE='SYSTEM VERSIONED';\n```\n\n```sql\n+--------------+------------+\n| TABLE_SCHEMA | TABLE_NAME |\n+--------------+------------+\n| test         | t          |\n+--------------+------------+\n1 row in set (0.005 sec)\n```\n\n要删除 `SYSTEM VERSIONING`，执行 `ALTER TABLE` 语句：\n\n```sql\nMariaDB [test]> ALTER TABLE t DROP SYSTEM VERSIONING;\nQuery OK, 0 rows affected (0.071 sec)\nRecords: 0  Duplicates: 0  Warnings: 0\n```\n\n### 序列\n\nMariaDB 和 TiDB 均支持 [`CREATE SEQUENCE`](/sql-statements/sql-statement-create-sequence.md)，但是 DM 暂不支持。建议在迁移期间不要创建、修改或删除序列，尤其在迁移后要进行相关测试。\n\n执行下列语句检查你是否在使用序列：\n\n```sql\nSELECT\n  TABLE_SCHEMA,\n  TABLE_NAME\nFROM\n  information_schema.tables\nWHERE\n  TABLE_TYPE='SEQUENCE';\n```\n\n```sql\n+--------------+------------+\n| TABLE_SCHEMA | TABLE_NAME |\n+--------------+------------+\n| test         | s1         |\n+--------------+------------+\n1 row in set (0.016 sec)\n```\n\n### 存储引擎\n\nMariaDB 为本地数据提供了存储引擎，例如 `InnoDB`、`MyISAM` 和 `Aria`。虽然 TiDB 不直接支持这些数据格式，但是你仍可以迁移这些数据。但是，也有一些存储引擎将数据放在服务器之外，例如 `CONNECT` 存储引擎和 `Spider`。虽然你可以将这些表迁移到 TiDB，但是 TiDB 无法将数据存储在 TiDB 集群外部。\n\n执行下列语句检查你正在使用的存储引擎：\n\n```sql\nSELECT\n  ENGINE,\n  COUNT(*)\nFROM\n  information_schema.tables\nGROUP BY\n  ENGINE;\n```\n\n```sql\n+--------------------+----------+\n| ENGINE             | COUNT(*) |\n+--------------------+----------+\n| NULL               |      101 |\n| Aria               |       38 |\n| CSV                |        2 |\n| InnoDB             |        6 |\n| MEMORY             |       67 |\n| MyISAM             |        1 |\n| PERFORMANCE_SCHEMA |       81 |\n+--------------------+----------+\n7 rows in set (0.009 sec)\n```\n\n### 语法\n\nMariaDB 支持 `DELETE`、`INSERT` 和 `REPLACE` 语句的 `RETURNING` 关键字。TiDB 不支持这些语句的关键字。你可能需要查看应用程序和查询日志，以检查它是否会影响数据迁移。\n\n### 数据类型\n\nMariaDB 支持的一些数据类型，例如 `UUID`、`INET4` 和 `INET6`，TiDB 并不支持。\n\n执行下列语句检查你正在使用的数据类型：\n\n```sql\nSELECT\n  TABLE_SCHEMA,\n  TABLE_NAME,\n  COLUMN_NAME,\n  DATA_TYPE\nFROM\n  information_schema.columns\nWHERE\n  DATA_TYPE IN('INET4','INET6','UUID');\n```\n\n```sql\n+--------------+------------+-------------+-----------+\n| TABLE_SCHEMA | TABLE_NAME | COLUMN_NAME | DATA_TYPE |\n+--------------+------------+-------------+-----------+\n| test         | u1         | u           | uuid      |\n| test         | u1         | i4          | inet4     |\n| test         | u1         | i6          | inet6     |\n+--------------+------------+-------------+-----------+\n3 rows in set (0.026 sec)\n\n```\n\n### 字符集和排序规则\n\nTiDB 不支持 MariaDB 中常用的 `latin1_swedish_ci` 排序规则。\n\n执行下列语句检查你正在使用的排序规则：\n\n```sql\nSHOW COLLATION;\n```\n\n```sql\n+--------------------+---------+-----+---------+----------+---------+\n| Collation          | Charset | Id  | Default | Compiled | Sortlen |\n+--------------------+---------+-----+---------+----------+---------+\n| ascii_bin          | ascii   |  65 | Yes     | Yes      |       1 |\n| binary             | binary  |  63 | Yes     | Yes      |       1 |\n| gbk_bin            | gbk     |  87 |         | Yes      |       1 |\n| gbk_chinese_ci     | gbk     |  28 | Yes     | Yes      |       1 |\n| latin1_bin         | latin1  |  47 | Yes     | Yes      |       1 |\n| utf8_bin           | utf8    |  83 | Yes     | Yes      |       1 |\n| utf8_general_ci    | utf8    |  33 |         | Yes      |       1 |\n| utf8_unicode_ci    | utf8    | 192 |         | Yes      |       1 |\n| utf8mb4_0900_ai_ci | utf8mb4 | 255 |         | Yes      |       1 |\n| utf8mb4_0900_bin   | utf8mb4 | 309 |         | Yes      |       1 |\n| utf8mb4_bin        | utf8mb4 |  46 | Yes     | Yes      |       1 |\n| utf8mb4_general_ci | utf8mb4 |  45 |         | Yes      |       1 |\n| utf8mb4_unicode_ci | utf8mb4 | 224 |         | Yes      |       1 |\n+--------------------+---------+-----+---------+----------+---------+\n13 rows in set (0.0012 sec)\n```\n\n执行下列语句检查当前表的列使用的排序规则：\n\n```sql\nSELECT\n  TABLE_SCHEMA,\n  COLLATION_NAME,\n  COUNT(*)\nFROM\n  information_schema.columns\nGROUP BY\n  TABLE_SCHEMA, COLLATION_NAME\nORDER BY\n  COLLATION_NAME;\n```\n\n```sql\n+--------------------+--------------------+----------+\n| TABLE_SCHEMA       | COLLATION_NAME     | COUNT(*) |\n+--------------------+--------------------+----------+\n| sys                | NULL               |      562 |\n| test               | NULL               |       14 |\n| mysql              | NULL               |       84 |\n| performance_schema | NULL               |      892 |\n| information_schema | NULL               |      421 |\n| mysql              | latin1_swedish_ci  |       34 |\n| performance_schema | utf8mb3_bin        |       38 |\n| mysql              | utf8mb3_bin        |       61 |\n| sys                | utf8mb3_bin        |       40 |\n| information_schema | utf8mb3_general_ci |      375 |\n| performance_schema | utf8mb3_general_ci |      244 |\n| sys                | utf8mb3_general_ci |      386 |\n| mysql              | utf8mb3_general_ci |       67 |\n| mysql              | utf8mb4_bin        |        8 |\n+--------------------+--------------------+----------+\n14 rows in set (0.045 sec)\n```\n\n更多信息，请参考[字符集和排序规则](/character-set-and-collation.md)。\n\n## 使用 Dumpling 导出数据后使用 TiDB Lightning 导入\n\n该迁移策略假定你将应用程序下线，迁移数据，然后重新配置应用程序以使用迁移后的数据。\n\n> **注意：**\n>\n> 强烈建议你在生产环境操作之前，先在测试或开发环境中进行测试。这样既可以检查可能的兼容性问题，也可以了解迁移所需时长。\n\n将数据从 MariaDB 迁移到 TiDB 的操作步骤如下：\n\n1. 停止应用程序。将应用程序下线。这样可以确保在迁移过程中或迁移之后，MariaDB 中的数据不会被修改。\n\n2. 导出数据。首先使用 [`tiup dumpling`](/dumpling-overview.md#使用-dumpling-导出数据) 命令从 MariaDB 导出数据。\n\n    ```shell\n    tiup dumpling --port 3306 --host 127.0.0.1 --user root --password secret -F 256MB  -o /data/backup\n    ```\n\n3. 使用 `tiup tidb-lightning` 命令恢复数据。请参考[TiDB Lightning 快速上手](/get-started-with-tidb-lightning.md)了解如何配置及运行 TiDB Lightning。\n\n4. 迁移用户账号和权限。请参考[导出用户和授权](#导出用户和授权)了解如何迁移用户账号和权限。\n\n5. 重新配置应用程序。你需要修改应用程序的配置，使其可以连接到 TiDB 服务器。\n\n6. 清理环境。一旦确认迁移成功，你可以在 MariaDB 中做最后一次备份，然后停止 MariaDB 服务器。你可以删除 TiUP、Dumpling 和 TiDB Lightning 等工具。\n\n## 使用 DM 迁移数据\n\n该策略假定你将应用程序下线，等待复制数据，然后重新配置应用程序以使用 TiDB。\n\n要使用 DM，你需要使用 [TiUP 集群](/dm/deploy-a-dm-cluster-using-tiup.md)或 [TiDB Operator](/tidb-operator-overview.md) 部署一组 DM 服务。之后，使用 `dmctl` 配置 DM 服务。\n\n> **注意：**\n>\n> 强烈建议你在生产环境操作之前，先在测试或开发环境中进行测试。这样既可以检查可能的兼容性问题，也可以了解迁移所需时长。\n\n### 第 1 步：准备工作\n\n确保 MariaDB 上启用了 binlog，并且 `binlog_format` 设置为 `ROW`。建议设置 `binlog_annotate_row_events=OFF` 和 `log_bin_compress=OFF`。\n\n你还需要一个拥有 `SUPER` 权限或 `BINLOG MONITOR` 和 `REPLICATION MASTER ADMIN` 权限的账号。该账号还需要对你要迁移的数据库有读权限。\n\n如果你不使用拥有 `SUPER` 权限的账号，那么你可能需要在 DM 配置中添加以下内容，因为 TiDB 不知道如何检查 MariaDB 特有的权限。\n\n```yaml\nignore-checking-items: [\"replication_privilege\"]\n```\n\n使用 DM 迁移数据前，可以使用预检查上游数据库配置是否正确，以确保迁移顺利进行。更多信息，请参考 [TiDB Data Migration 任务前置检查](/dm/dm-precheck.md)。\n\n### 第 2 步：迁移数据\n\n参考 [TiDB Data Migration 快速上手指南](/dm/quick-start-with-dm.md)了解如何从 MariaDB 迁移数据到 TiDB。\n\n注意，与从 MariaDB 到 MariaDB 迁移数据时不同，你不需要先复制初始数据，因为 DM 会完成相关操作。\n\n### 第 3 步：迁移用户账号和权限\n\n参考[导出用户和授权](#导出用户和授权)了解如何迁移用户账号和权限。\n\n### 第 4 步：测试数据\n\n一旦数据开始迁移，你可以在其上运行只读查询来验证数据。更多信息，请参考[测试应用程序](#测试应用程序)。\n\n### 第 5 步：切换系统\n\n要将系统切换到 TiDB，你需要执行以下操作：\n\n1. 停止应用程序。\n2. 监控复制延迟，它应该变为 0 秒。\n3. 修改应用程序的配置，使其连接到 TiDB，然后重新启动应用程序。\n\n要检查复制延迟，使用 `dmctl` 运行 [`query-status <taskname>`](/dm/dm-query-status.md#详情查询结果)，并检查 `subTaskStatus` 中的 `\"synced: true\"`。\n\n### 第 6 步：清理环境\n\n一旦确认迁移成功，你可以在 MariaDB 中做最后一次备份，然后停止 MariaDB 服务器。这也意味着你可以停止并删除 DM 集群。\n\n## 导出用户和授权\n\n你可以使用 [`pt-show-grants`](https://docs.percona.com/percona-toolkit/pt-show-grants.html) 导出用户和授权。它是 Percona Toolkit 的一部分，用于从 MariaDB 导出用户和授权，并将其加载到 TiDB 中。\n\n## 测试应用程序\n\n虽然可以使用 `sysbench` 等通用工具进行测试，但是强烈建议你测试应用程序的某些特定功能。例如，使用临时数据副本，运行应用程序的副本，连接到 TiDB 集群。\n\n这些测试可以确保应用程序与 TiDB 的兼容性和性能。你需要监控应用程序和 TiDB 的日志，以查看是否有任何需要解决的警告。确保测试应用程序使用的数据库驱动程序，例如 Java 应用程序的 MySQL Connector/J。如有必要，你可能需要使用 JMeter 等应用程序对应用程序进行负载测试。\n\n## 验证数据\n\n你可以使用 [sync-diff-inspector](/sync-diff-inspector/sync-diff-inspector-overview.md) 验证 MariaDB 和 TiDB 中的数据是否相同。"
        },
        {
          "name": "migrate-from-parquet-files-to-tidb.md",
          "type": "blob",
          "size": 6.1845703125,
          "content": "---\ntitle: 从 Parquet 文件迁移数据到 TiDB\nsummary: 介绍如何使用 TiDB Lightning 从 Parquet 文件迁移数据到 TiDB。\n---\n\n# 从 Parquet 文件迁移数据到 TiDB\n\n本文介绍如何从 Apache Hive 中生成 Parquet 文件以及如何使用 TiDB Lightning 从 Parquet 文件迁移数据到 TiDB。\n\n如果你从 Amazon Aurora 中导出 Parquet 文件，请参照[从 Amazon Aurora 迁移数据到 TiDB](/migrate-aurora-to-tidb.md)。\n\n## 前提条件\n\n- [使用 TiUP 安装 TiDB Lightning](/migration-tools.md)\n- [获取 TiDB Lightning 所需下游数据库权限](/tidb-lightning/tidb-lightning-requirements.md#目标数据库权限要求)\n\n## 第 1 步：准备 Parquet 文件\n\n本节描述如何从 Hive 中导出能被 TiDB Lightning 读取的 Parquet 文件。\n\nHive 中每个表都能通过标注 `STORED AS PARQUET LOCATION '/path/in/hdfs'` 的形式将表数据导出到 Parquet 文件中。因此，如果你需要导出一张名叫 `test` 的表，请执行以下步骤：\n\n1. 在 Hive 中执行如下 SQL 语句：\n\n    ```sql\n    CREATE TABLE temp STORED AS PARQUET LOCATION '/path/in/hdfs'\n    AS SELECT * FROM test;\n    ```\n\n    执行上述语句后，表数据就成功导出到 HDFS 系统里。\n\n2. 使用 `hdfs dfs -get` 命令将 Parquet 文件导出到本地：\n\n    ```shell\n    hdfs dfs -get /path/in/hdfs /path/in/local\n    ```\n\n    完成导出后，如果你需要将 HDFS 里导出的 Parquet 文件删除，可以直接将这个临时表 (`temp`) 删掉：\n\n    ```sql\n    DROP TABLE temp;\n    ```\n\n3. 从 Hive 导出的 Parquet 文件可能不带有 `.parquet` 的后缀，无法被 TiDB Lightning 正确识别。因此，在进行导入之前，需要对导出的文件进行重命名，添加 `.parquet` 后缀，将完整的文件名修改为 TiDB Lightning 能识别的格式 `${db_name}.${table_name}.parquet`。更多文件类型和命名规则，请参考 [TiDB Lightning 数据源](/tidb-lightning/tidb-lightning-data-source.md)。你也可以通过设置正确的[自定义表达式](/tidb-lightning/tidb-lightning-data-source.md#自定义文件匹配)匹配数据文件。\n\n4. 将所有 Parquet 文件放到统一目录下，例如 `/data/my_datasource/` 或 `s3://my-bucket/sql-backup`。TiDB Lightning 将递归地寻找该目录及其子目录内的所有 `.parquet` 文件。\n\n## 第 2 步：创建目标表结构\n\n在将 Parquet 文件导入 TiDB 前，你必须为 Parquet 文件提供表结构。你可以通过以下任一方法创建表结构：\n\n* **方法一**：使用 TiDB Lightning 创建表结构。\n\n    编写包含 DDL 语句的 SQL 文件：\n\n    - 文件名格式为 `${db_name}-schema-create.sql`，其内容需包含 `CREATE DATABASE` 语句。\n    - 文件名格式为 `${db_name}.${table_name}-schema.sql`，其内容需包含 `CREATE TABLE` 语句。\n\n* **方法二**：手动在下游 TiDB 建库和表。\n\n## 第 3 步：编写配置文件\n\n新建文件 `tidb-lightning.toml`，包含以下内容：\n\n```toml\n[lightning]\n# 日志\nlevel = \"info\"\nfile = \"tidb-lightning.log\"\n\n[tikv-importer]\n# \"local\"：默认使用该模式，适用于 TiB 级以上大数据量，但导入期间下游 TiDB 无法对外提供服务。\nbackend = \"local\"\n# # \"tidb\"：TiB 级以下数据量也可以采用 `tidb` 后端模式，下游 TiDB 可正常提供服务。关于导入模式更多信息请参阅：https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-overview#tidb-lightning-整体架构\n# 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小。建议设为与 `data-source-dir` 不同的磁盘目录并使用闪存介质，独占 I/O 会获得更好的导入性能。\nsorted-kv-dir = \"${sorted-kv-dir}\"\n\n[mydumper]\n# 源数据目录\ndata-source-dir = \"${data-path}\" # 本地或 S3 路径，例如：'s3://my-bucket/sql-backup'\n\n[tidb]\n# 目标集群的信息\nhost = ${host}                # 例如：172.16.32.1\nport = ${port}                # 例如：4000\nuser = \"${user_name}\"         # 例如：\"root\"\npassword = \"${password}\"      # 例如：\"rootroot\"\nstatus-port = ${status-port}  # 导入过程 Lightning 需要在从 TiDB 的“状态端口”获取表结构信息，例如：10080\npd-addr = \"${ip}:${port}\"     # 集群 PD 的地址，Lightning 通过 PD 获取部分信息，例如 172.16.31.3:2379。当 backend = \"local\" 时 status-port 和 pd-addr 必须正确填写，否则导入将出现异常。\n```\n\n关于配置文件更多信息，可参阅 [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)。\n\n## 第 4 步：执行导入\n\n1. 运行 `tidb-lightning`。\n\n    - 如果从 Amazon S3 导入，需先将有权限访问该 S3 后端存储的账号的 SecretKey 和 AccessKey 作为环境变量传入 Lightning 节点。\n\n        ```shell\n        export AWS_ACCESS_KEY_ID=${access_key}\n        export AWS_SECRET_ACCESS_KEY=${secret_key}\n        ```\n\n        此外，TiDB Lightning 还支持从 `~/.aws/credentials` 读取凭证文件。\n\n    - 如果直接在命令行中启动程序，可能会因为 `SIGHUP` 信号而退出，建议配合 `nohup` 或 `screen` 等工具运行 `tidb-lightning`：\n\n        ```shell\n        nohup tiup tidb-lightning -config tidb-lightning.toml > nohup.out 2>&1 &\n        ```\n\n2. 导入开始后，可以采用以下任意方式查看进度：\n\n    - 通过 `grep` 日志关键字 `progress` 查看进度，默认 5 分钟更新一次。\n    - 通过监控面板查看进度，请参考 [TiDB Lightning 监控](/tidb-lightning/monitor-tidb-lightning.md)。\n    - 通过 Web 页面查看进度，请参考 [Web 界面](/tidb-lightning/tidb-lightning-web-interface.md)。\n\n    导入完毕后，TiDB Lightning 会自动退出。\n\n3. 检查导入是否成功。\n\n    查看 `tidb-lightning.log` 日志末尾是否有 `the whole procedure completed` 信息，如果有，表示导入成功。如果没有，则表示导入遇到了问题，可根据日志中的 error 提示解决遇到的问题。\n\n    > **注意：**\n    >\n    > 无论导入成功与否，最后一行都会显示 `tidb lightning exit`。它只是表示 TiDB Lightning 正常退出，不代表任务完成。\n\n如果导入过程中遇到问题，请参见 [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)。\n"
        },
        {
          "name": "migrate-from-sql-files-to-tidb.md",
          "type": "blob",
          "size": 4.8203125,
          "content": "---\ntitle: 从 SQL 文件迁移数据到 TiDB\nsummary: 介绍如何使用 TiDB Lightning 从 MySQL SQL 文件迁移数据到 TiDB。\naliases: ['/docs-cn/dev/migrate-from-mysql-mydumper-files/','/zh/tidb/dev/migrate-from-mysql-mydumper-files/','/zh/tidb/dev/migrate-from-mysql-dumpling-files/']\n---\n\n# 从 SQL 文件迁移数据到 TiDB\n\n本文介绍如何使用 TiDB Lightning 从 MySQL SQL 文件迁移数据到 TiDB。关于如何生成 MySQL SQL 文件，请参考 Dumpling 文档中的[导出为 SQL 文件](/dumpling-overview.md#导出为-sql-文件)。\n\n## 前提条件\n\n- [使用 TiUP 安装 TiDB Lightning](/migration-tools.md)\n- [Lightning 所需下游数据库权限](/tidb-lightning/tidb-lightning-faq.md#tidb-lightning-对下游数据库的账号权限要求是怎样的)\n\n## 第 1 步：准备 SQL 文件\n\n将所有 SQL 文件放到统一目录下，例如 `/data/my_datasource/` 或 `s3://my-bucket/sql-backup`。Lightning 将递归地寻找该目录下及其子目录内的所有 `.sql` 文件。\n\n## 第 2 步：定义目标表结构\n\n要导入 TiDB，就必须为 SQL 文件提供表结构。\n\n如果使用 Dumpling 工具导出数据，则会自动导出表结构文件。此外，其他方式导出的数据可以通过以下任一方法创建表结构：\n\n* **方法一**：使用 TiDB Lightning 创建表结构。\n\n    编写包含 DDL 语句的 SQL 文件：\n\n    - 文件名格式为 `${db_name}-schema-create.sql`，其内容需包含 `CREATE DATABASE` 语句。\n    - 文件名格式为 `${db_name}.${table_name}-schema.sql`，其内容需包含 `CREATE TABLE` 语句。\n\n* **方法二**：手动在下游 TiDB 建库和表。\n\n## 第 3 步：编写配置文件\n\n新建文件 `tidb-lightning.toml`，包含以下内容：\n\n{{< copyable \"\" >}}\n\n```toml\n[lightning]\n# 日志\nlevel = \"info\"\nfile = \"tidb-lightning.log\"\n\n[tikv-importer]\n# \"local\"：默认使用该模式，适用于 TiB 级以上大数据量，但导入期间下游 TiDB 无法对外提供服务。\nbackend = \"local\"\n# # \"tidb\"：TiB 级以下数据量也可以采用 `tidb` 后端模式，下游 TiDB 可正常提供服务。关于后端模式更多信息请参考 https://docs.pingcap.com/zh/tidb/stable/tidb-lightning-backends 。\n# 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小。建议设为与 `data-source-dir` 不同的磁盘目录并使用闪存介质，独占 I/O 会获得更好的导入性能。\nsorted-kv-dir = \"${sorted-kv-dir}\"\n\n[mydumper]\n# 源数据目录\ndata-source-dir = \"${data-path}\" # 本地或 S3 路径，例如：'s3://my-bucket/sql-backup'\n\n[tidb]\n# 目标集群的信息\nhost = ${host}                # 例如：172.16.32.1\nport = ${port}                # 例如：4000\nuser = \"${user_name}\"         # 例如：\"root\"\npassword = \"${password}\"      # 例如：\"rootroot\"\nstatus-port = ${status-port}  # 导入过程 Lightning 需要在从 TiDB 的“状态端口”获取表结构信息，例如：10080\npd-addr = \"${ip}:${port}\"     # 集群 PD 的地址，Lightning 通过 PD 获取部分信息，例如 172.16.31.3:2379。当 backend = \"local\" 时 status-port 和 pd-addr 必须正确填写，否则导入将出现异常。\n```\n\n关于配置文件更多信息，可参阅 [TiDB Lightning Configuration](/tidb-lightning/tidb-lightning-configuration.md)。\n\n## 第 4 步：执行导入\n\n运行 `tidb-lightning`。如果直接在命令行中启动程序，可能会因为 `SIGHUP` 信号而退出，建议配合 `nohup` 或 `screen` 等工具。\n\n若从 Amazon S3 导入，则需将有权限访问该 S3 后端存储的账号的 SecretKey 和 AccessKey 作为环境变量传入 Lightning 节点。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\nexport AWS_ACCESS_KEY_ID=${access_key}\nexport AWS_SECRET_ACCESS_KEY=${secret_key}\nnohup tiup tidb-lightning -config tidb-lightning.toml > nohup.out 2>&1 &\n```\n\n同时，TiDB Lightning 还支持从 `~/.aws/credentials` 读取凭证文件。\n\n导入开始后，可以采用以下任意方式查看进度：\n\n- 通过 `grep` 日志关键字 `progress` 查看进度，默认 5 分钟更新一次。\n- 通过监控面板查看进度，请参考 [TiDB Lightning 监控](/tidb-lightning/monitor-tidb-lightning.md)。\n- 通过 Web 页面查看进度，请参考 [Web 界面](/tidb-lightning/tidb-lightning-web-interface.md)。\n\n导入完毕后，TiDB Lightning 会自动退出。查看 `tidb-lightning.log` 日志末尾是否有 `the whole procedure completed` 信息，如果有，表示导入成功。如果没有，则表示导入遇到了问题，可根据日志中的 error 提示解决遇到的问题。\n\n> **注意：**\n>\n> 无论导入成功与否，最后一行都会显示 `tidb lightning exit`。它只是表示 TiDB Lightning 正常退出，不代表任务完成。\n\n如果导入过程中遇到问题，请参见 [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)。\n"
        },
        {
          "name": "migrate-from-tidb-to-mysql.md",
          "type": "blob",
          "size": 8.7998046875,
          "content": "---\ntitle: 从 TiDB 集群迁移数据至兼容 MySQL 的数据库\nsummary: 了解如何将数据从 TiDB 集群迁移至与 MySQL 兼容的数据库。\n---\n\n# 从 TiDB 集群迁移数据至兼容 MySQL 的数据库\n\n本文档介绍如何将数据从 TiDB 集群迁移至兼容 MySQL 的数据库，如 Aurora、MySQL、MariaDB 等。本文将模拟整个迁移过程，具体包括以下四个步骤：\n\n1. 搭建环境\n2. 迁移全量数据\n3. 迁移增量数据\n4. 平滑切换业务\n\n## 第 1 步：搭建环境\n\n1. 部署上游 TiDB 集群。\n\n    使用 TiUP Playground 快速部署上下游测试集群。更多部署信息，请参考 [TiUP 官方文档](/tiup/tiup-cluster.md)。\n\n    ```shell\n    # 创建上游集群\n    tiup playground --db 1 --pd 1 --kv 1 --tiflash 0 --ticdc 1\n    # 查看集群状态\n    tiup status\n    ```\n\n2. 部署下游 MySQL 实例。\n\n    - 在实验环境中，可以使用 Docker 快速部署 MySQL 实例，执行如下命令：\n\n        ```shell\n        docker run --name some-mysql -e MYSQL_ROOT_PASSWORD=my-secret-pw -p 3306:3306 -d mysql\n        ```\n\n    - 在生产环境中，可以参考 [Installing MySQL](https://dev.mysql.com/doc/refman/8.0/en/installing.html) 来部署 MySQL 实例。\n\n3. 模拟业务负载。\n\n    在测试实验环境下，可以使用 go-tpc 向上游 TiDB 集群写入数据，以让 TiDB 产生事件变更数据。执行如下命令，将首先在上游 TiDB 创建名为 tpcc 的数据库，然后使用 TiUP bench 写入数据到刚创建的 tpcc 数据库中。\n\n    ```shell\n    tiup bench tpcc -H 127.0.0.1 -P 4000 -D tpcc --warehouses 4 prepare\n    tiup bench tpcc -H 127.0.0.1 -P 4000 -D tpcc --warehouses 4 run --time 300s\n    ```\n\n    关于 go-tpc 的更多详细内容，可以参考[如何对 TiDB 进行 TPC-C 测试](/benchmark/benchmark-tidb-using-tpcc.md)。\n\n## 第 2 步：迁移全量数据\n\n搭建好测试环境后，可以使用 [Dumpling](/dumpling-overview.md) 工具导出上游集群的全量数据。\n\n> **注意：**\n>\n> 在生产集群中，关闭 GC 机制和备份操作会一定程度上降低集群的读性能，建议在业务低峰期进行备份，并设置合适的 `RATE_LIMIT` 限制备份操作对线上业务的影响。\n\n1. 关闭 GC (Garbage Collection)。\n\n    为了保证增量迁移过程中新写入的数据不丢失，在开始全量导出之前，需要关闭上游集群的垃圾回收 (GC) 机制，以确保系统不再清理历史数据。对于 TiDB v4.0.0 及之后的版本，Dumpling 可能会[自动调整 GC 的 safe point 从而阻塞 GC](/dumpling-overview.md#手动设置-tidb-gc-时间)。然而，手动关闭 GC 仍然是必要的，因为在 Dumpling 退出后，GC 可能会被触发，从而导致增量变更迁移失败。\n\n    执行如下命令关闭 GC：\n\n    ```sql\n    MySQL [test]> SET GLOBAL tidb_gc_enable=FALSE;\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    查询 `tidb_gc_enable` 的取值，判断 GC 是否已关闭：\n\n    ```sql\n    MySQL [test]> SELECT @@global.tidb_gc_enable;\n    ```\n\n    ```\n    +-------------------------+：\n    | @@global.tidb_gc_enable |\n    +-------------------------+\n    |                       0 |\n    +-------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n2. 备份数据。\n\n    1. 使用 Dumpling 导出 SQL 格式的数据：\n\n        ```shell\n        tiup dumpling -u root -P 4000 -h 127.0.0.1 --filetype sql -t 8 -o ./dumpling_output -r 200000 -F256MiB\n        ```\n\n    2. 导出完毕后，执行如下命令查看导出数据的元信息，metadata 文件中的 `Pos` 就是导出快照的 TSO，将其记录为 BackupTS：\n\n        ```shell\n        cat dumpling_output/metadata\n        ```\n\n        ```\n        Started dump at: 2022-06-28 17:49:54\n        SHOW MASTER STATUS:\n                Log: tidb-binlog\n                Pos: 434217889191428107\n                GTID:\n\n        Finished dump at: 2022-06-28 17:49:57\n        ```\n\n3. 恢复数据。\n\n    使用开源工具 MyLoader 导入数据到下游 MySQL。MyLoader 的安装和详细用例参见 [MyDumpler/MyLoader](https://github.com/mydumper/mydumper)。注意需要使用 MyLoader v0.10 或更早版本，否则会导致 MyLoader 无法处理 Dumpling 导出的 metadata 文件。\n\n    执行以下指令，将 Dumpling 导出的上游全量数据导入到下游 MySQL 实例：\n\n    ```shell\n    myloader -h 127.0.0.1 -P 3306 -d ./dumpling_output/\n    ```\n\n4. （可选）校验数据。\n\n    通过 [sync-diff-inspector](/sync-diff-inspector/sync-diff-inspector-overview.md) 工具，可以验证上下游数据在某个时间点的一致性。\n\n    ```shell\n    sync_diff_inspector -C ./config.yaml\n    ```\n\n    关于 sync-diff-inspector 的配置方法，请参考[配置文件说明](/sync-diff-inspector/sync-diff-inspector-overview.md#配置文件说明)。在本文中，相应的配置如下：\n\n    ```toml\n    # Diff Configuration.\n    ######################### Datasource config #########################\n    [data-sources]\n    [data-sources.upstream]\n            host = \"127.0.0.1\" # 需要替换为实际上游集群 ip\n            port = 4000\n            user = \"root\"\n            password = \"\"\n            snapshot = \"434217889191428107\" # 配置为实际的备份时间点（参见「备份」小节的 BackupTS）\n    [data-sources.downstream]\n            host = \"127.0.0.1\" # 需要替换为实际下游集群 ip\n            port = 3306\n            user = \"root\"\n            password = \"\"\n\n    ######################### Task config #########################\n    [task]\n            output-dir = \"./output\"\n            source-instances = [\"upstream\"]\n            target-instance = \"downstream\"\n            target-check-tables = [\"*.*\"]\n    ```\n\n## 第 3 步：迁移增量数据\n\n1. 部署 TiCDC。\n\n    完成全量数据迁移后，就可以部署并配置 TiCDC 集群同步增量数据，实际生产集群中请参考 [TiCDC 部署](/ticdc/deploy-ticdc.md)。本文在创建测试集群时，已经启动了一个 TiCDC 节点，因此可以直接进行 changefeed 的配置。\n\n2. 创建同步任务。\n\n    在上游集群中，执行以下命令创建从上游到下游集群的同步链路：\n\n    ```shell\n    tiup cdc:v<CLUSTER_VERSION> cli changefeed create --server=http://127.0.0.1:8300 --sink-uri=\"mysql://root:@127.0.0.1:3306\" --changefeed-id=\"upstream-to-downstream\" --start-ts=\"434217889191428107\"\n    ```\n\n    以上命令中：\n\n    - `--server`：TiCDC 集群任意一节点地址\n    - `--sink-uri`：同步任务下游的地址\n    - `--changefeed-id`：同步任务的 ID，格式需要符合正则表达式 `^[a-zA-Z0-9]+(\\-[a-zA-Z0-9]+)*$`\n    - `--start-ts`：TiCDC 同步的起点，需要设置为实际的备份时间点，也就是[第 2 步：迁移全量数据](/migrate-from-tidb-to-mysql.md#第-2-步迁移全量数据)中 “备份数据” 提到的 BackupTS\n\n    更多关于 changefeed 的配置，请参考 [TiCDC Changefeed 配置参数](/ticdc/ticdc-changefeed-config.md)。\n\n3. 重新开启 GC。\n\n    TiCDC 可以保证 GC 只回收已经同步的历史数据。因此，创建完从上游到下游集群的 changefeed 之后，就可以执行如下命令恢复集群的垃圾回收功能。详情请参考 [TiCDC GC safepoint 的完整行为](/ticdc/ticdc-faq.md#ticdc-gc-safepoint-的完整行为是什么)。\n\n    执行如下命令打开 GC：\n\n    ```sql\n    MySQL [test]> SET GLOBAL tidb_gc_enable=TRUE;\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    查询 `tidb_gc_enable` 的取值，判断 GC 是否已开启：\n\n    ```sql\n    MySQL [test]> SELECT @@global.tidb_gc_enable;\n    ```\n\n    ```\n    +-------------------------+\n    | @@global.tidb_gc_enable |\n    +-------------------------+\n    |                       1 |\n    +-------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n## 第 4 步：平滑切换业务\n\n通过 TiCDC 创建上下游的同步链路后，原集群的写入数据会以非常低的延迟同步到新集群，此时可以逐步将读流量迁移到新集群了。观察一段时间，如果新集群表现稳定，就可以将写流量接入新集群，步骤如下：\n\n1. 停止上游集群的写业务。确认上游数据已全部同步到下游后，停止上游到下游集群的 changefeed。\n\n    ```shell\n    # 停止旧集群到新集群的 changefeed\n    tiup cdc cli changefeed pause -c \"upstream-to-downstream\" --pd=http://172.16.6.122:2379\n\n    # 查看 changefeed 状态\n    tiup cdc cli changefeed list\n    ```\n\n    ```\n    [\n      {\n        \"id\": \"upstream-to-downstream\",\n        \"summary\": {\n        \"state\": \"stopped\",  # 需要确认这里的状态为 stopped\n        \"tso\": 434218657561968641,\n        \"checkpoint\": \"2022-06-28 18:38:45.685\", # 确认这里的时间晚于停写的时间\n        \"error\": null\n        }\n      }\n    ]\n    ```\n\n2. 将写业务迁移到下游集群，观察一段时间后，等新集群表现稳定，便可以弃用原集群。\n"
        },
        {
          "name": "migrate-from-tidb-to-tidb.md",
          "type": "blob",
          "size": 13.2392578125,
          "content": "---\ntitle: 从 TiDB 集群迁移数据至另一 TiDB 集群\nsummary: 了解如何将数据从一个 TiDB 集群迁移至另一 TiDB 集群。\naliases: ['/zh/tidb/dev/incremental-replication-between-clusters/']\n---\n\n# 从 TiDB 集群迁移数据至另一 TiDB 集群\n\n本文档介绍如何将数据从一个 TiDB 集群迁移至另一 TiDB。在如下场景中，你可以将数据从一个 TiDB 集群迁移至另一个 TiDB 集群：\n\n- 拆库：原 TiDB 集群体量过大，或者为了避免原有的 TiDB 集群所承载的数个业务之间互相影响，将原 TiDB 集群中的部分表迁到另一个 TiDB 集群。\n- 迁库：是对数据库的物理位置进行迁移，比如更换数据中心。\n- 升级：在对数据正确性要求严苛的场景下，可以将数据迁移到一个更高版本的 TiDB 集群，确保数据安全。\n\n本文将模拟整个迁移过程，具体包括以下四个步骤：\n\n1. 搭建环境\n2. 迁移全量数据\n3. 迁移增量数据\n4. 平滑切换业务\n\n## 第 1 步：搭建环境\n\n1. 部署集群。\n\n    使用 TiUP Playground 快速部署上下游测试集群。更多部署信息，请参考 [TiUP 官方文档](/tiup/tiup-cluster.md)。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    # 创建上游集群\n    tiup --tag upstream playground --host 0.0.0.0 --db 1 --pd 1 --kv 1 --tiflash 0 --ticdc 1\n    # 创建下游集群\n    tiup --tag downstream playground --host 0.0.0.0 --db 1 --pd 1 --kv 1 --tiflash 0 --ticdc 1\n    # 查看集群状态\n    tiup status\n    ```\n\n2. 初始化数据。\n\n    测试集群中默认创建了 test 数据库，因此可以使用 [sysbench](https://github.com/akopytov/sysbench#linux) 工具生成测试数据，用以模拟真实集群中的历史数据。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    sysbench oltp_write_only --config-file=./tidb-config --tables=10 --table-size=10000 prepare\n    ```\n\n    这里通过 sysbench 运行 oltp_write_only 脚本，其将在测试数据库中生成 10 张表，每张表包含 10000 行初始数据。tidb-config 的配置如下：\n\n    ```yaml\n    mysql-host=172.16.6.122 # 这里需要替换为实际上游集群 ip\n    mysql-port=4000\n    mysql-user=root\n    mysql-password=\n    db-driver=mysql         # 设置数据库驱动为 mysql\n    mysql-db=test           # 设置测试数据库为 test\n    report-interval=10      # 设置定期统计的时间间隔为 10 秒\n    threads=10              # 设置 worker 线程数量为 10\n    time=0                  # 设置脚本总执行时间，0 表示不限制\n    rate=100                # 设置平均事务速率 tps = 100\n    ```\n\n3. 模拟业务负载。\n\n    实际生产集群的数据迁移过程中，通常原集群还会写入新的业务数据，本文中可以通过 sysbench 工具模拟持续的写入负载，下面的命令会使用 10 个 worker 在数据库中的 sbtest1、sbtest2 和 sbtest3 三张表中持续写入数据，其总 tps 限制为 100。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    sysbench oltp_write_only --config-file=./tidb-config --tables=3 run\n    ```\n\n4. 准备外部存储。\n\n    在全量数据备份中，上下游集群均需访问备份文件，因此推荐使用[备份存储](/br/backup-and-restore-storages.md)存储备份文件，本文中通过 Minio 模拟兼容 S3 的存储服务：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    wget https://dl.min.io/server/minio/release/linux-amd64/minio\n    chmod +x minio\n    # 配置访问 minio 的 access-key access-screct-id\n    export HOST_IP='172.16.6.122' # 替换为实际上游集群 ip\n    export MINIO_ROOT_USER='minio'\n    export MINIO_ROOT_PASSWORD='miniostorage'\n    # 创建数据目录,  其中 backup 为 bucket 的名称\n    mkdir -p data/backup\n    # 启动 minio, 暴露端口在 6060\n    ./minio server ./data --address :6060 &\n    ```\n\n    上述命令行启动了一个单节点的 minio server 模拟 S3 服务，其相关参数为：\n\n    - Endpoint: <http://${HOST_IP}:6060/>\n    - Access-key: minio\n    - Secret-access-key: miniostorage\n    - Bucket: backup\n\n    相应的访问链接为：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    s3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://${HOST_IP}:6060&force-path-style=true\n    ```\n\n## 第 2 步：迁移全量数据\n\n搭建好测试环境后，可以使用 [BR](https://github.com/pingcap/tidb/tree/master/br) 工具的备份和恢复功能迁移全量数据。BR 工具有多种[使用方式](/br/br-use-overview.md#部署和使用-br)，本文中使用 SQL 语句 [`BACKUP`](/sql-statements/sql-statement-backup.md) 和 [`RESTORE`](/sql-statements/sql-statement-restore.md) 进行备份恢复。\n\n> **注意：**\n>\n> - `BACKUP` 和 `RESTORE` 语句目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n> - 在生产集群中，关闭 GC 机制和备份操作会一定程度上降低集群的读性能，建议在业务低峰期进行备份，并设置合适的 `RATE_LIMIT` 限制备份操作对线上业务的影响。\n> - 上下游集群版本不一致时，应检查 BR 工具的[兼容性](/br/backup-and-restore-overview.md#使用须知)。本文假设上下游集群版本相同。\n\n1. 关闭 GC。\n\n    为了保证增量迁移过程中新写入的数据不丢失，在开始备份之前，需要关闭上游集群的垃圾回收 (GC) 机制，以确保系统不再清理历史数据。\n\n    执行如下命令关闭 GC：\n\n    ```sql\n    MySQL [test]> SET GLOBAL tidb_gc_enable=FALSE;\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    查询 `tidb_gc_enable` 的取值，判断 GC 是否已关闭：\n\n    ```sql\n    MySQL [test]> SELECT @@global.tidb_gc_enable;\n    ```\n\n    ```\n    +-------------------------+：\n    | @@global.tidb_gc_enable |\n    +-------------------------+\n    |                       0 |\n    +-------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n    > **注意：**\n    >\n    > TiCDC 的 `gc-ttl` 默认为 24 小时。如果备份恢复耗时过长，默认的 `gc-ttl` 可能无法满足需求，从而导致后续的[增量同步任务](#第-3-步迁移增量数据)运行失败。为了避免这种情况，请在启动 TiCDC server 时根据实际需求调整 `gc-ttl` 的值。更多信息，请参考 [TiCDC 的 `gc-ttl` 是什么](/ticdc/ticdc-faq.md#ticdc-的-gc-ttl-是什么)。\n\n2. 备份数据。\n\n    在上游集群中执行 BACKUP 语句备份数据：\n\n    ```sql\n    MySQL [(none)]> BACKUP DATABASE * TO 's3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://${HOST_IP}:6060&force-path-style=true' RATE_LIMIT = 120 MB/SECOND;\n    ```\n\n    ```\n    +---------------+----------+--------------------+---------------------+---------------------+\n    | Destination   | Size     | BackupTS           | Queue Time          | Execution Time      |\n    +---------------+----------+--------------------+---------------------+---------------------+\n    | s3://backup   | 10315858 | 431434047157698561 | 2022-02-25 19:57:59 | 2022-02-25 19:57:59 |\n    +---------------+----------+--------------------+---------------------+---------------------+\n    1 row in set (2.11 sec)\n    ```\n\n    备份语句提交成功后，TiDB 会返回关于备份数据的元信息，这里需要重点关注 BackupTS，它意味着该时间点之前数据会被备份，后边的教程中，本文将使用 BackupTS 作为**数据校验截止时间**和 **TiCDC 增量扫描的开始时间**。\n\n3. 恢复数据。\n\n    在下游集群中执行 RESTORE 语句恢复数据：\n\n    ```sql\n    mysql> RESTORE DATABASE * FROM 's3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://${HOST_IP}:6060&force-path-style=true';\n    ```\n\n    ```\n    +--------------+-----------+--------------------+---------------------+---------------------+\n    | Destination  | Size      | BackupTS           | Queue Time          | Execution Time      |\n    +--------------+-----------+--------------------+---------------------+---------------------+\n    | s3://backup  | 10315858  | 431434141450371074 | 2022-02-25 20:03:59 | 2022-02-25 20:03:59 |\n    +--------------+-----------+--------------------+---------------------+---------------------+\n    1 row in set (41.85 sec)\n    ```\n\n4. （可选）校验数据。\n\n    通过 [sync-diff-inspector](/sync-diff-inspector/sync-diff-inspector-overview.md) 工具，可以验证上下游数据在某个时间点的一致性。从上述备份和恢复命令的输出可以看到，上游集群备份的时间点为 431434047157698561，下游集群完成数据恢复的时间点为 431434141450371074。\n\n    ```shell\n    sync_diff_inspector -C ./config.yaml\n    ```\n\n    关于 sync-diff-inspector 的配置方法，请参考[配置文件说明](/sync-diff-inspector/sync-diff-inspector-overview.md#配置文件说明)，在本文中，相应的配置如下：\n\n    ```toml\n    # Diff Configuration.\n    ######################### Datasource config #########################\n    [data-sources]\n    [data-sources.upstream]\n            host = \"172.16.6.122\" # 需要替换为实际上游集群 ip\n            port = 4000\n            user = \"root\"\n            password = \"\"\n            snapshot = \"431434047157698561\" # 配置为实际的备份时间点（参见「备份」小节的 BackupTS）\n    [data-sources.downstream]\n            host = \"172.16.6.125\" # 需要替换为实际下游集群 ip\n            port = 4000\n            user = \"root\"\n            password = \"\"\n\n    ######################### Task config #########################\n    [task]\n            output-dir = \"./output\"\n            source-instances = [\"upstream\"]\n            target-instance = \"downstream\"\n            target-check-tables = [\"*.*\"]\n    ```\n\n## 第 3 步：迁移增量数据\n\n1. 部署 TiCDC。\n\n    完成全量数据迁移后，就可以部署并配置 TiCDC 集群同步增量数据，实际生产集群中请参考 [TiCDC 部署](/ticdc/deploy-ticdc.md)。本文在创建测试集群时，已经启动了一个 TiCDC 节点，因此可以直接进行 changefeed 的配置。\n\n2. 创建同步任务。\n\n    在上游集群中，执行以下命令创建从上游到下游集群的同步链路：\n\n    ```shell\n    tiup cdc cli changefeed create --server=http://172.16.6.122:8300 --sink-uri=\"mysql://root:@172.16.6.125:4000\" --changefeed-id=\"upstream-to-downstream\" --start-ts=\"431434047157698561\"\n    ```\n\n    以上命令中：\n\n    - `--server`：TiCDC 集群中任意一个节点的地址\n    - `--sink-uri`：同步任务下游的地址\n    - `--changefeed-id`：同步任务的 ID，格式需要符合正则表达式 ^[a-zA-Z0-9]+(\\-[a-zA-Z0-9]+)*$\n    - `--start-ts`：TiCDC 同步的起点，需要设置为实际的备份时间点，也就是[第 2 步：迁移全量数据](/migrate-from-tidb-to-mysql.md#第-2-步迁移全量数据)中 “备份数据” 提到的 BackupTS\n\n    更多关于 changefeed 的配置，请参考 [TiCDC Changefeed 配置参数](/ticdc/ticdc-changefeed-config.md)。\n\n3. 重新开启 GC。\n\n    TiCDC 可以保证 GC 只回收已经同步的历史数据。因此，创建完从上游到下游集群的 changefeed 之后，就可以执行如下命令恢复集群的垃圾回收功能。详情请参考 [TiCDC GC safepoint 的完整行为](/ticdc/ticdc-faq.md#ticdc-gc-safepoint-的完整行为是什么)。\n\n   执行如下命令打开 GC：\n\n    ```sql\n    MySQL [test]> SET GLOBAL tidb_gc_enable=TRUE;\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    查询 `tidb_gc_enable` 的取值，判断 GC 是否已开启：\n\n    ```sql\n    MySQL [test]> SELECT @@global.tidb_gc_enable;\n    ```\n\n    ```\n    +-------------------------+\n    | @@global.tidb_gc_enable |\n    +-------------------------+\n    |                       1 |\n    +-------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n## 第 4 步：平滑切换业务\n\n通过 TiCDC 创建上下游的同步链路后，原集群的写入数据会以非常低的延迟同步到新集群，此时可以逐步将读流量迁移到新集群了。观察一段时间，如果新集群表现稳定，就可以将写流量接入新集群，步骤如下：\n\n1. 停止上游集群的写业务。确认上游数据已全部同步到下游后，停止上游到下游集群的 changefeed。\n\n    ```shell\n    # 停止旧集群到新集群的 changefeed\n    tiup cdc cli changefeed pause -c \"upstream-to-downstream\" --server=http://172.16.6.122:8300\n\n    # 查看 changefeed 状态\n    tiup cdc cli changefeed list\n    ```\n\n    ```\n    [\n      {\n        \"id\": \"upstream-to-downstream\",\n        \"summary\": {\n        \"state\": \"stopped\",  # 需要确认这里的状态为 stopped\n        \"tso\": 431747241184329729,\n        \"checkpoint\": \"2022-03-11 15:50:20.387\", # 确认这里的时间晚于停写的时间\n        \"error\": null\n        }\n      }\n    ]\n    ```\n\n2. 创建下游到上游集群的 changefeed。由于此时上下游数据是一致的，且没有新数据写入，因此可以不指定 start-ts，默认为当前时间：\n\n    ```shell\n    tiup cdc cli changefeed create --server=http://172.16.6.125:8300 --sink-uri=\"mysql://root:@172.16.6.122:4000\" --changefeed-id=\"downstream -to-upstream\"\n    ```\n\n3. 将写业务迁移到下游集群，观察一段时间后，等新集群表现稳定，便可以弃用原集群。\n"
        },
        {
          "name": "migrate-from-vitess.md",
          "type": "blob",
          "size": 2.63671875,
          "content": "---\ntitle: 从 Vitess 迁移数据到 TiDB\nsummary: 介绍从 Vitess 迁移数据到 TiDB 所使用的工具。\n---\n\n# 从 Vitess 迁移数据到 TiDB\n\n本文档介绍了将数据从 [Vitess](https://vitess.io/) 迁移到 TiDB 时可以采用的工具。\n\n由于 Vitess 的后端基于 MySQL，当从 Vitess 迁移数据到 TiDB 时，你可以直接使用 MySQL 适用的迁移数据工具，如 [Dumpling](/dumpling-overview.md)、[TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 和 [TiDB Data Migration (DM)](/dm/dm-overview.md)。需要注意的是，针对 Vitess 中的每个分片，你都需要进行相应的迁移工具配置以完成数据迁移。\n\n通常情况下，推荐使用 DM 进行数据迁移。迁移前，需要将 DM 任务的 `task-mode` 设为 `all`，`import-mode` 设为 `physical`。更多信息，请参考[完整配置文件示例](/dm/task-configuration-file-full.md#完整配置文件示例)。\n\n如果数据量超过 10 TiB，建议分两步导入：\n\n1. 使用 Dumpling 和 TiDB Lightning 导入已有数据。\n2. 使用 DM 导入增量数据。\n\n除了以上工具，你还可以使用 [Debezium 的 Vitess 连接器](https://debezium.io/documentation/reference/connectors/vitess.html)。该连接器可以通过 [Kafka Connect](https://kafka.apache.org/documentation/#connect) 或 [Apache Flink](https://nightlies.apache.org/flink/flink-docs-stable/) 将 Vitess 的数据变更同步到 TiDB 中。\n\n由于 Vitess 和 TiDB 都支持 MySQL 协议和 SQL 方言，应用层预计只涉及较少的更改。但对于一些直接管理分片或实现特定业务逻辑的任务，可能涉及较大的更改。为了方便从 Vitess 向 TiDB 迁移数据，TiDB 引入了 [`VITESS_HASH()`](/functions-and-operators/tidb-functions.md) 函数，该函数返回的字符串哈希值与 Vitess 的 HASH 函数兼容。\n\n## 示例\n\n### Dumpling 和 TiDB Lightning\n\n以下两个示例展示了 Dumpling 和 TiDB Lightning 如何协同工作，将数据从 Vitess 迁移到 TiDB。\n\n- 在此示例中，TiDB Lightning 使用[逻辑导入模式](/tidb-lightning/tidb-lightning-logical-import-mode.md)，先将数据编码为 SQL 语句，然后运行这些 SQL 语句来导入数据。\n\n    ![Vitess to TiDB Migration with TiDB backend](/media/vitess_to_tidb.png)\n\n- 在此示例中，TiDB Lightning 使用[物理导入模式](/tidb-lightning/tidb-lightning-physical-import-mode.md)直接将数据导入 TiKV。\n\n    ![Vitess to TiDB Migration with local backend](/media/vitess_to_tidb_dumpling_local.png)\n\n### DM\n\n以下示例展示了 [DM](/dm/dm-overview.md) 如何将数据从 Vitess 迁移到 TiDB。\n\n![Vitess to TiDB with DM](/media/vitess_to_tidb_dm.png)"
        },
        {
          "name": "migrate-large-mysql-shards-to-tidb.md",
          "type": "blob",
          "size": 19.9326171875,
          "content": "---\ntitle: 从大数据量分库分表 MySQL 合并迁移数据到 TiDB\nsummary: 使用 Dumpling 和 TiDB Lightning 合并导入分表数据到 TiDB，以及如何使用 DM 持续增量复制数据。本文介绍的方法适用于导入数据总量大于 1 TiB 的场景。\n---\n\n# 从大数据量分库分表 MySQL 合并迁移数据到 TiDB\n\n如果分表数据总规模特别大（例如大于 1 TiB），并且允许 TiDB 集群在迁移期间无其他业务写入，那么你可以使用 TiDB Lightning 对分表数据进行快速合并导入，然后根据业务需要选择是否使用 TiDB DM 进行增量数据的分表同步。本文所称“大数据量”通常指 TiB 级别以上。本文档举例介绍了导入数据的操作步骤。\n\n如果分库分表合并迁移在 1 TiB 以内，请参考[从小数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-small-mysql-shards-to-tidb.md)，支持全量和增量且更为简单。\n\n在本文的示例中，假设有两个数据库 my_db1 和 my_db2，使用 Dumpling 分别从 my_db1 中导出 table1 和 table2 两个表，从 my_db2 中导出 table3 和 table4 两个表，然后再用 TiDB Lightning 把导出的 4 个表合并导入到下游 TiDB 中的同一个库 my_db 的同一个表格 table5 中。\n\n本文将以三个步骤演示导入流程：\n\n1. 使用 Dumpling 导出全量数据备份。在本文档示例中，分别从两个源数据库中各导出两个表：\n    - 从实例 1 MySQL 的 my_db1 导出 table1、table2\n    - 从实例 2 MySQL 的 my_db2 导出 table3、table4\n2. 启动 TiDB Lightning 执行导入 TiDB 中的 mydb.table5\n3. 使用 DM 进行增量数据迁移（可选）\n\n## 前提条件\n\n- [使用 TiUP 安装 DM 集群](/dm/deploy-a-dm-cluster-using-tiup.md)\n- [使用 TiUP 安装 Dumpling 和 Lightning](/migration-tools.md)\n- [Dumpling 所需上游数据库权限](/dumpling-overview.md#从-tidbmysql-导出数据)\n- [TiDB Lightning 所需下游数据库权限](/tidb-lightning/tidb-lightning-requirements.md)\n- [TiDB Lightning 下游数据库所需空间](/tidb-lightning/tidb-lightning-requirements.md)\n- [DM 所需上下游数据库权限](/dm/dm-worker-intro.md)\n\n### 分表数据冲突检查\n\n迁移中如果涉及合库合表，来自多张分表的数据可能引发主键或唯一索引的数据冲突。因此在迁移之前，需要检查各分表数据的业务特点。详情请参考[跨分表数据在主键或唯一索引冲突处理](/dm/shard-merge-best-practices.md#跨分表数据在主键或唯一索引冲突处理)，这里做简要描述：\n\n假设 table1~4 具有相同的表结构如下：\n\n```sql\nCREATE TABLE `table1` (\n  `id` bigint NOT NULL AUTO_INCREMENT,\n  `sid` bigint NOT NULL,\n  `pid` bigint NOT NULL,\n  `comment` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `sid` (`sid`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n```\n\n其中 `id` 列为主键，具有自增属性，多个分表范围重复会引发数据冲突。`sid` 列为分片键，可以保证全局满足唯一索引。因此可以移除下游 `table5` 表 `id` 列的唯一键属性：\n\n```sql\nCREATE TABLE `table5` (\n  `id` bigint NOT NULL,\n  `sid` bigint NOT NULL,\n  `pid` bigint NOT NULL,\n  `comment` varchar(255) DEFAULT NULL,\n  INDEX (`id`),\n  UNIQUE KEY `sid` (`sid`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n```\n\n## 第 1 步：用 Dumpling 导出全量数据备份\n\n如果需要导出的多个分表属于同一个上游 MySQL 实例，建议直接使用 Dumpling 的 `-f` 参数一次导出多个分表的结果。如果多个分表分布在不同的 MySQL 实例，可以使用 Dumpling 分两次导出，并将两次导出的结果放置在相同的父目录下即可。下面的例子中同时用到了上述两种方式，然后将导出的数据存放在同一父目录下。\n\n首先使用 Dumpling 从 my_db1 中导出表 table1 和 table2，如下：\n\n```\ntiup dumpling -h ${ip} -P 3306 -u root -t 16 -r 200000 -F 256MB -B my_db1 -f 'my_db1.table[12]' -o ${data-path}/my_db1\n```\n\n以上命令行中用到的参数描述如下。要了解更多 Dumpling 参数，请参考 [Dumpling 使用文档](/dumpling-overview.md)。\n\n| 参数                   | 描述 |\n| -                      | - |\n| `-u` 或 `--user`       | MySQL 数据库的用户 |\n| `-p` 或 `--password`   | MySQL 数据库的用户密码 |\n| `-P` 或 `--port`       | MySQL 数据库的端口 |\n| `-h` 或 `--host`       | MySQL 数据库的 IP 地址 |\n| `-t` 或 `--thread`     | 导出的线程数。增加线程数会增加 Dumpling 并发度提高导出速度，但也会加大数据库内存消耗，因此不宜设置过大，一般不超过 64|\n| `-o` 或 `--output`     | 存储导出文件的目录，支持本地文件路径或[外部存储服务的 URI 格式](/external-storage-uri.md) |\n| `-r` 或 `--row`        | 用于指定单个文件的最大行数，指定该参数后 Dumpling 会开启表内并发加速导出，同时减少内存使用 |\n| `-F`                   | 指定单个文件的最大大小，单位为 MiB。强烈建议使用`-F`参数以避免单表过大导致备份过程中断 |\n| `-B` 或 `--database`   | 导出指定数据库 |\n| `-f` 或 `--filter`     | 导出能匹配模式的表，语法可参考 [table-filter](/table-filter.md)|\n\n然后使用同样的方式从 my_db2 中导出表 table3 和 table4。注意路径在相同 `${data-path}` 下的不同子目录 `my_db2`。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dumpling -h ${ip} -P 3306 -u root -t 16 -r 200000 -F 256MB -B my_db2 -f 'my_db2.table[34]' -o ${data-path}/my_db2\n```\n\n这样所需的全量备份数据就全部导出到了 `${data-path}` 目录中。将所有源数据表格存储在一个目录中，是为了后续方便用 TiDB Lightning 导入。\n\n第 3 步增量同步的时候所需的起始位点信息，在 `${data-path}` 目录下，`my_db1` 和 `my_db2` 的 `metadata` 文件中，这是 Dumpling 自动生成的元信息文件，请记录其中的 binlog 位置信息。\n\n## 第 2 步：启动 TiDB Lightning 进行导入\n\n在启动 TiDB Lightning 进行迁移之前，建议先了解如何处理检查点，然后根据需要选择合适的方式进行迁移。\n\n### 断点续传\n\n大量数据导入一般耗时数小时甚至数天，长时间运行的进程会有一定机率发生非正常中断。如果每次重启都从头开始，之前已成功导入的数据就会前功尽弃。为此，TiDB Lightning 提供了断点续传的功能，即使 TiDB Lightning 崩溃，在重启时仍然从断点开始继续工作。\n\n若 TiDB Lightning 因不可恢复的错误而退出，例如数据出错，在重启时不会使用断点，而是直接报错离开。为保证已导入的数据安全，必须先解决掉这些错误才能继续。你可以使用 `tidb-lightning-ctl` 命令控制导入出错后的行为。该命令的选项有：\n\n* --checkpoint-error-destroy：出现错误后，让失败的表从头开始整个导入过程。\n* --checkpoint-error-ignore：如果导入表曾经出错，该命令会清除出错状态，如同错误没有发生过一样。\n* --checkpoint-remove：无论是否有出错，把表的断点清除。\n\n关于断点续传的更多信息，请参考 [TiDB Lightning 断点续传](/tidb-lightning/tidb-lightning-checkpoints.md)。\n\n### 在下游创建 schema\n\n在下游创建 `mydb.table5`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE `table5` (\n  `id` bigint NOT NULL,\n  `sid` bigint NOT NULL,\n  `pid` bigint NOT NULL,\n  `comment` varchar(255) DEFAULT NULL,\n  INDEX (`id`),\n  UNIQUE KEY `sid` (`sid`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n```\n\n### 执行导入操作\n\n启动 tidb-lightning 的步骤如下：\n\n1. 编写配置文件 `tidb-lightning.toml`。\n\n    {{< copyable \"\" >}}\n\n    ```\n\n    [lightning]\n    # 日志\n    level = \"info\"\n    file = \"tidb-lightning.log\"\n\n    [mydumper]\n    data-source-dir = ${data-path}\n\n    [tikv-importer]\n    # \"local\"：默认使用该模式，适用于 TB 级以上大数据量，但导入期间下游 TiDB 无法对外提供服务。\n    # \"tidb\"：TB 级以下数据量也可以采用`tidb`后端模式，下游 TiDB 可正常提供服务。关于后端模式更多信息请参阅：https://docs.pingcap.com/tidb/stable/tidb-lightning-backends\n    backend = \"local\"\n    # 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小。建议设为与 `data-source-dir` 不同的磁盘目录，独占 IO 会获得更好的导入性能\n    sorted-kv-dir = \"${sorted-kv-dir}\"\n\n    # 设置分库分表合并规则，将 my_db1 中的 table1、table2 两个表,以及 my_db2 中的 table3、table4 两个表，共计 2 个数据库中的 4 个表都导入到目的数据库 my_db 中的 table5 表中。\n    [[mydumper.files]]\n    pattern = '(^|/)my_db1\\.table[1-2]\\..*\\.sql$'\n    schema = \"my_db\"\n    table = \"table5\"\n    type = \"sql\"\n\n    [[mydumper.files]]\n    pattern = '(^|/)my_db2\\.table[3-4]\\..*\\.sql$'\n    schema = \"my_db\"\n    table = \"table5\"\n    type = \"sql\"\n\n    # 目标集群的信息，示例仅供参考。请把 IP 地址等信息替换成真实的信息。\n    [tidb]\n    # 目标集群的信息\n    host = ${host}              # 例如：172.16.32.1\n    port = ${port}              # 例如：4000\n    user = \"${user_name}\"       # 例如：\"root\"\n    password = \"${password}\"    # 例如：\"rootroot\"\n    status-port = ${status-port} # 导入过程 Lightning 需要在从 TiDB 的“状态端口”获取表结构信息，例如：10080\n    # PD 集群的地址，Lightning 通过 PD 获取部分信息。\n    pd-addr = \"${ip}:${port}\"   # 例如 172.16.31.3:2379。当 backend = \"local\" 时 status-port 和 pd-addr 必须正确填写，否则导入将出现异常。\n\n    ```\n\n2. 运行 `tidb-lightning`。如果直接在命令行中启动程序，可能会因为 `SIGHUP` 信号而退出，建议配合 `nohup` 或 `screen` 等工具，如：\n\n    若从 Amazon S3 导入，则需将有权限访问该 S3 后端存储的账号的 SecretKey 和 AccessKey 作为环境变量传入 TiDB Lightning 节点。同时还支持从 `~/.aws/credentials` 读取凭证文件。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    export AWS_ACCESS_KEY_ID=${access_key}\n    export AWS_SECRET_ACCESS_KEY=${secret_key}\n    nohup tiup tidb-lightning -config tidb-lightning.toml > nohup.out 2>&1 &\n    ```\n\n3. 导入开始后，可以采用以下任意方式查看进度：\n\n    - 通过 `grep` 日志关键字 `progress` 查看进度，默认 5 分钟更新一次。\n    - 通过监控面板查看进度，请参考 [TiDB Lightning 监控](/tidb-lightning/monitor-tidb-lightning.md)。\n    - 通过 Web 页面查看进度，请参考 [Web 界面](/tidb-lightning/tidb-lightning-web-interface.md)。\n\n4. 导入完毕后，TiDB Lightning 会自动退出。查看 `tidb-lightning.log` 日志末尾是否有 `the whole procedure completed` 信息，如果有，表示导入成功。如果没有，则表示导入遇到了问题，可根据日志中的 error 提示解决遇到的问题。\n\n> **注意：**\n>\n> 无论导入成功与否，最后一行都会显示 `tidb lightning exit`。它只是表示 TiDB Lightning 正常退出，不代表任务完成。\n\n如果导入过程中遇到问题，请参见 [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)。\n\n## 第 3 步：使用 DM 持续复制增量数据到 TiDB (可选)\n\n基于 binlog 从指定位置同步数据库到 TiDB，可以使用 DM 来执行增量复制\n\n### 添加数据源\n\n新建 `source1.yaml` 文件，写入以下内容：\n\n{{< copyable \"\" >}}\n\n```yaml\n\n# 唯一命名，不可重复。\nsource-id: \"mysql-01\"\n\n# DM-worker 是否使用全局事务标识符 (GTID) 拉取 binlog。使用前提是上游 MySQL 已开启 GTID 模式。若上游存在主从自动切换，则必须使用 GTID 模式。\nenable-gtid: true\n\nfrom:\n  host: \"${host}\"           # 例如：172.16.10.81\n  user: \"root\"\n  password: \"${password}\"   # 支持但不推荐使用明文密码，建议使用 dmctl encrypt 对明文密码进行加密后使用\n  port: 3306\n\n```\n\n在终端中执行下面的命令，使用 `tiup dmctl` 将数据源配置加载到 DM 集群中:\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} operate-source create source1.yaml\n```\n\n该命令中的参数描述如下：\n\n| 参数                     | 描述 |\n| -                        | - |\n| `--master-addr`          | dmctl 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`|\n| `operate-source create`  |  向 DM 集群加载数据源  |\n\n重复以上步骤直至所有 MySQL 实例被加入 DM。\n\n### 添加同步任务\n\n编辑 `task.yaml`，配置增量同步模式，以及每个数据源的同步起点：\n\n{{< copyable \"\" >}}\n\n```yaml\n\nname: task-test               # 任务名称，需要全局唯一。\ntask-mode: incremental        # 任务模式，设为 \"incremental\" 即只进行增量数据迁移。\n# 分库分表合并任务则需要配置 shard-mode。默认使用悲观协调模式 \"pessimistic\"，在深入了解乐观协调模式的原理和使用限制后，也可以设置为乐观协调模式 \"optimistic\"\n# 详细信息可参考：https://docs.pingcap.com/zh/tidb/dev/feature-shard-merge/\nshard-mode: \"pessimistic\"\n\n## 配置下游 TiDB 数据库实例访问信息\ntarget-database:              # 下游数据库实例配置。\n  host: \"${host}\"             # 例如：127.0.0.1\n  port: 4000\n  user: \"root\"\n  password: \"${password}\"     # 推荐使用经过 dmctl 加密的密文。\n\n##  使用黑白名单配置需要同步的表\nblock-allow-list:             # 数据源数据库实例匹配的表的 block-allow-list 过滤规则集，如果 DM 版本早于 v2.0.0-beta.2 则使用 black-white-list。\n  bw-rule-1:                  # 黑白名单配置项 ID。\n    do-dbs: [\"my_db1\"]        # 迁移哪些库。这里将实例 1 的 my_db1 和实例 2 的 my_db2 分别配置为两条 rule，以示例如何避免实例 1 的 my_db2 被同步。\n  bw-rule-2:\n    do-dbs: [\"my_db2\"]\n\nroutes:                               # 上游和下游表之间的路由 table routing 规则集\n  route-rule-1:                       # 配置名称。将 my_db1 中的 table1 和 table2 合并导入下游 my_db.table5\n    schema-pattern: \"my_db1\"          # 库名匹配规则，支持通配符 \"*\" 和 \"?\"\n    table-pattern: \"table[1-2]\"       # 表名匹配规则，支持通配符 \"*\" 和 \"?\"\n    target-schema: \"my_db\"            # 目标库名称\n    target-table: \"table5\"            # 目标表名称\n  route-rule-2:                       # 配置名称。将 my_db2 中的 table3 和 table4 合并导入下游 my_db.table5\n    schema-pattern: \"my_db2\"\n    table-pattern: \"table[3-4]\"\n    target-schema: \"my_db\"\n    target-table: \"table5\"\n\n## 配置数据源，以两个数据源为例\nmysql-instances:\n  - source-id: \"mysql-01\"             # 数据源 ID，即 source1.yaml 中的 source-id\n    block-allow-list: \"bw-rule-1\"     # 引入上面黑白名单配置。同步实例1的 my_db1\n    route-rules: [\"route-rule-1\"]     # 引入上面表合并配置。\n#       syncer-config-name: \"global\"  # 引用后面的 syncers 增量数据配置。\n    meta:                             # `task-mode` 为 `incremental` 且下游数据库的 `checkpoint` 不存在时 binlog 迁移开始的位置; 如果 checkpoint 存在，则以 `checkpoint` 为准。如果 `meta` 项和下游数据库的 `checkpoint` 都不存在，则从上游当前最新的 binlog 位置开始迁移。\n      binlog-name: \"${binlog-name}\"   # 第 1 步中 ${data-path}/my_db1/metadata 记录的日志位置，当上游存在主从切换时，必须使用 gtid。\n      binlog-pos: ${binlog-position}\n      # binlog-gtid:                  \" 例如：09bec856-ba95-11ea-850a-58f2b4af5188:1-9\"\n  - source-id: \"mysql-02\"             # 数据源 ID，即 source1.yaml 中的 source-id\n    block-allow-list: \"bw-rule-2\"     # 引入上面黑白名单配置。实例2的 my_db2\n    route-rules: [\"route-rule-2\"]     # 引入上面表合并配置。\n\n#       syncer-config-name: \"global\"  # 引用后面的 syncers 增量数据配置。\n    meta:                             # task-mode 为 incremental 且下游数据库的 checkpoint 不存在时 binlog 迁移开始的位置; 如果 checkpoint 存在，则以 checkpoint 为准。\n      # binlog-name: \"${binlog-name}\"   # 第 1 步中 ${data-path}/my_db2/metadata 记录的日志位置，当上游存在主从切换时，必须使用 gtid。\n      # binlog-pos: ${binlog-position}\n      binlog-gtid: \"09bec856-ba95-11ea-850a-58f2b4af5188:1-9\"\n\n## 【可选配置】 如果增量数据迁移需要重复迁移已经在全量数据迁移中完成迁移的数据，则需要开启 safe mode 避免增量数据迁移报错。\n##  该场景多见于以下情况：全量迁移的数据不属于数据源的一个一致性快照，随后从一个早于全量迁移数据之前的位置开始同步增量数据。\n# syncers:            # sync 处理单元的运行配置参数。\n#  global:           # 配置名称。\n#    safe-mode: true # 设置为 true，会将来自数据源的 INSERT 改写为 REPLACE，将 UPDATE 改写为 DELETE 与 REPLACE，从而保证在表结构中存在主键或唯一索引的条件下迁移数据时可以重复导入 DML。在启动或恢复增量复制任务的前 1 分钟内 TiDB DM 会自动启动 safe mode。\n\n```\n\n关于任务的更多配置项，可以参考 [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)。\n\n在你启动数据迁移任务之前，建议使用 `check-task` 命令检查配置是否符合 DM 的配置要求，以降低后期报错的概率。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} check-task task.yaml\n```\n\n使用 tiup dmctl 执行以下命令启动数据迁移任务。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} start-task task.yaml\n```\n\n该命令中的参数描述如下：\n\n| 参数 | 描述 |\n| - | - |\n| `--master-addr` | dmctl 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如：172.16.10.71:8261 |\n| `start-task`| 命令用于创建数据迁移任务 |\n\n如果任务启动失败，可根据返回结果的提示进行配置变更后执行 start-task task.yaml 命令重新启动任务。遇到问题请参考[故障及处理方法](/dm/dm-error-handling.md)以及[常见问题](/dm/dm-faq.md)。\n\n### 查看任务状态\n\n如需了解 DM 集群中是否存在正在运行的迁移任务及任务状态等信息，可使用 `tiup dmctl` 执行 `query-status` 命令进行查询：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} query-status ${task-name}\n```\n\n关于查询结果的详细解读，请参考[查询状态](/dm/dm-query-status.md)。\n\n### 监控任务与查看日志\n\n你可以通过 Grafana 或者日志查看迁移任务的历史状态以及各种内部运行指标。\n\n- 通过 Grafana 查看\n\n    如果使用 TiUP 部署 DM 集群时，正确部署了 Prometheus、Alertmanager 与 Grafana，则使用部署时填写的 IP 及端口进入 Grafana，选择 DM 的 dashboard 查看 DM 相关监控项。\n\n- 通过日志查看\n\n    DM 在运行过程中，DM-worker、DM-master 及 dmctl 都会通过日志输出相关信息，其中包含迁移任务的相关信息。各组件的日志目录如下：\n\n    - DM-master 日志目录：通过 DM-master 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-master-8261/log/`。\n    - DM-worker 日志目录：通过 DM-worker 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-worker-8262/log/`。\n\n## 探索更多\n\n- [关于 Dumpling](/dumpling-overview.md)\n- [关于 Lightning](/tidb-lightning/tidb-lightning-overview.md)\n- [分库分表合并中的悲观/乐观模式](/dm/feature-shard-merge.md)\n- [暂停数据迁移任务](/dm/dm-pause-task.md)\n- [恢复数据迁移任务](/dm/dm-resume-task.md)\n- [停止数据迁移任务](/dm/dm-stop-task.md)\n- [导出和导入集群的数据源和任务配置](/dm/dm-export-import-config.md)\n- [处理出错的 DDL 语句](/dm/handle-failed-ddl-statements.md)\n- [故障及处理方法](/dm/dm-error-handling.md)\n"
        },
        {
          "name": "migrate-large-mysql-to-tidb.md",
          "type": "blob",
          "size": 15.740234375,
          "content": "---\ntitle: 从大数据量 MySQL 迁移数据到 TiDB\nsummary: 介绍如何从大数据量 MySQL 迁移数据到 TiDB。\n---\n\n# 从大数据量 MySQL 迁移数据到 TiDB\n\n通常数据量较低时，使用 DM 进行迁移较为简单，可直接完成全量+持续增量迁移工作。但当数据量较大时，DM 较低的数据导入速度 (30~50 GiB/h) 可能令整个迁移周期过长。本文所称“大数据量”通常指 TiB 级别以上。\n\n因此，本文档介绍如何使用 Dumpling 和 TiDB Lightning 进行全量数据迁移。TiDB Lightning [物理导入模式](/tidb-lightning/tidb-lightning-physical-import-mode.md)的导入速度最高可达每小时 500 GiB，注意实际导入速度受硬件配置、表结构、索引数量等多方面因素的影响。完成全量数据迁移后，再使用 DM 完成增量数据迁移。\n\n## 前提条件\n\n- [部署 DM 集群](/dm/deploy-a-dm-cluster-using-tiup.md)。\n- [安装 Dumpling 和 TiDB Lightning](/migration-tools.md)。\n- [配置 DM 所需上下游数据库权限](/dm/dm-worker-intro.md#dm-worker-所需权限)。\n- [获取 TiDB Lightning 所需下游数据库权限](/tidb-lightning/tidb-lightning-faq.md#tidb-lightning-对下游数据库的账号权限要求是怎样的)。\n- [获取 Dumpling 所需上游数据库权限](/dumpling-overview.md#从-tidbmysql-导出数据)。\n\n## 资源要求\n\n**操作系统**：本文档示例使用的是若干新的、纯净版 CentOS 7 实例，你可以在本地虚拟化一台主机，或在供应商提供的平台上部署一台小型的云虚拟主机。TiDB Lightning 运行过程中，默认会占满 CPU，建议单独部署在一台主机上。如果条件不允许，你可以将 TiDB Lightning 和其他组件（比如 `tikv-server`）部署在同一台机器上，然后设置 `region-concurrency` 配置项的值为逻辑 CPU 数的 75%，以限制 TiDB Lightning 对 CPU 资源的使用。\n\n**内存和 CPU**：因为 TiDB Lightning 对计算机资源消耗较高，建议分配 64 GB 以上的内存以及 32 核以上的 CPU，而且确保 CPU 核数和内存（GB）比为 1:2 以上，以获取最佳性能。\n\n**磁盘空间**：\n\n- Dumpling 需要能够储存整个数据源的存储空间，即可以容纳要导出的所有上游表的空间。计算方式参考[下游数据库所需空间](/tidb-lightning/tidb-lightning-requirements.md#目标数据库所需空间)。\n- TiDB Lightning 导入期间，需要临时空间来存储排序键值对，磁盘空间需要至少能存储数据源的最大单表。\n- 若全量数据量较大，可适当加长上游 binlog 保存时间，以避免增量同步时缺必要 binlog 导致重做。\n\n**说明**：目前无法精确计算 Dumpling 从 MySQL 导出的数据大小，但你可以用下面 SQL 语句统计信息表的 `DATA_LENGTH` 字段估算数据量：\n\n```sql\n-- 统计所有 schema 大小\nSELECT\n  TABLE_SCHEMA,\n  FORMAT_BYTES(SUM(DATA_LENGTH)) AS 'Data Size',\n  FORMAT_BYTES(SUM(INDEX_LENGTH)) 'Index Size'\nFROM\n  information_schema.tables\nGROUP BY\n  TABLE_SCHEMA;\n\n-- 统计最大的 5 个单表\nSELECT\n  TABLE_NAME,\n  TABLE_SCHEMA,\n  FORMAT_BYTES(SUM(data_length)) AS 'Data Size',\n  FORMAT_BYTES(SUM(index_length)) AS 'Index Size',\n  FORMAT_BYTES(SUM(data_length+index_length)) AS 'Total Size'\nFROM\n  information_schema.tables\nGROUP BY\n  TABLE_NAME,\n  TABLE_SCHEMA\nORDER BY\n  SUM(DATA_LENGTH+INDEX_LENGTH) DESC\nLIMIT\n  5;\n```\n\n### 目标 TiKV 集群的磁盘空间要求\n\n目标 TiKV 集群必须有足够空间接收新导入的数据。除了[标准硬件配置](/hardware-and-software-requirements.md)以外，目标 TiKV 集群的总存储空间必须大于**数据源大小 × [副本数量](/faq/manage-cluster-faq.md#每个-region-的-replica-数量可配置吗调整的方法是) × 2**。例如，集群默认使用 3 副本，那么总存储空间需为数据源大小的 6 倍以上。公式中的 2 倍可能难以理解，其依据是以下因素的估算空间占用：\n\n* 索引会占据额外的空间。\n* RocksDB 的空间放大效应。\n\n## 第 1 步：从 MySQL 导出全量数据\n\n1. 运行以下命令，从 MySQL 导出全量数据：\n\n    ```shell\n    tiup dumpling -h ${ip} -P 3306 -u root -t 16 -r 200000 -F 256MiB -B my_db1 -f 'my_db1.table[12]' -o 's3://my-bucket/sql-backup'\n    ```\n\n    Dumpling 默认导出数据格式为 SQL 文件，你也可以通过设置 `--filetype` 指定导出文件的类型。\n\n    以上命令行中用到的参数描述如下。要了解更多 Dumpling 参数，请参考 [Dumpling 使用文档](/dumpling-overview.md)。\n\n    | 参数              | 说明 |\n    | -                 | - |\n    | `-u` 或 `--user`       | MySQL 数据库的用户 |\n    | `-p` 或 `--password`   | MySQL 数据库的用户密码 |\n    | `-P` 或 `--port`       | MySQL 数据库的端口 |\n    | `-h` 或 `--host`       | MySQL 数据库的 IP 地址 |\n    | `-t` 或 `--thread`     | 导出的线程数。增加线程数会增加 Dumpling 并发度提高导出速度，但也会加大数据库内存消耗，因此不宜设置过大，一般不超过 64 |\n    | `-o` 或 `--output`     | 存储导出文件的目录，支持本地文件路径或[外部存储服务的 URI 格式](/external-storage-uri.md) |\n    | `-r` 或 `--row`        | 用于指定单个文件的最大行数，指定该参数后 Dumpling 会开启表内并发加速导出，同时减少内存使用 |\n    | `-F`                   | 指定单个文件的最大大小，单位为 MiB。强烈建议使用 `-F` 参数以避免单表过大导致备份过程中断 |\n    | `-B` 或 `--database`   | 导出指定数据库 |\n    | `-f` 或 `--filter`     | 导出能匹配模式的表，语法可参考 [table-filter](/table-filter.md)|\n\n    请确保 `${data-path}` 的空间可以容纳要导出的所有上游表，计算方式参考[下游数据库所需空间](/tidb-lightning/tidb-lightning-requirements.md#目标数据库所需空间)。强烈建议使用 `-F` 参数以避免单表过大导致备份过程中断。\n\n2. 查看在 `${data-path}` 目录下的 `metadata` 文件，这是 Dumpling 自动生成的元信息文件，请记录其中的 binlog 位置信息，这将在第 3 步增量同步的时候使用。\n\n    ```\n    SHOW MASTER STATUS:\n    Log: mysql-bin.000004\n    Pos: 109227\n    GTID:\n    ```\n\n## 第 2 步：导入全量数据到 TiDB\n\n1. 编写配置文件 `tidb-lightning.toml`：\n\n    ```toml\n    [lightning]\n    # 日志\n    level = \"info\"\n    file = \"tidb-lightning.log\"\n\n    [tikv-importer]\n    # \"local\"：默认使用该模式，适用于 TB 级以上大数据量，但导入期间下游 TiDB 无法对外提供服务。\n    # \"tidb\"：TB 级以下数据量也可以采用 `tidb` 后端模式，下游 TiDB 可正常提供服务。关于后端模式更多信息请参阅：https://docs.pingcap.com/tidb/stable/tidb-lightning-backends\n    backend = \"local\"\n    # 设置排序的键值对的临时存放地址，目标路径必须是一个空目录，目录空间须大于待导入数据集的大小。建议设为与 `data-source-dir` 不同的磁盘目录并使用闪存介质，独占 IO 会获得更好的导入性能\n    sorted-kv-dir = \"${sorted-kv-dir}\"\n\n    [mydumper]\n    # 源数据目录，即第 1 步中 Dumpling 保存数据的路径。\n    data-source-dir = \"${data-path}\" # 本地或 S3 路径，例如：'s3://my-bucket/sql-backup'\n\n    [tidb]\n    # 目标集群的信息\n    host = ${host}                # 例如：172.16.32.1\n    port = ${port}                # 例如：4000\n    user = \"${user_name}\"         # 例如：\"root\"\n    password = \"${password}\"      # 例如：\"rootroot\"\n    status-port = ${status-port}  # 导入过程 Lightning 需要在从 TiDB 的“状态端口”获取表结构信息，例如：10080\n    pd-addr = \"${ip}:${port}\"     # 集群 PD 的地址，Lightning 通过 PD 获取部分信息，例如 172.16.31.3:2379。当 backend = \"local\" 时 status-port 和 pd-addr 必须正确填写，否则导入将出现异常。\n    ```\n\n    关于更多 TiDB Lightning 的配置，请参考 [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)。\n\n2. 运行 `tidb-lightning`。如果直接在命令行中启动程序，可能会因为 `SIGHUP` 信号而退出，建议配合 `nohup` 或 `screen` 等工具，如：\n\n    若从 Amazon S3 导入，则需将有权限访问该 S3 后端存储的账号的 SecretKey 和 AccessKey 作为环境变量传入 Lightning 节点。同时还支持从 `~/.aws/credentials` 读取凭证文件。\n\n    ```shell\n    export AWS_ACCESS_KEY_ID=${access_key}\n    export AWS_SECRET_ACCESS_KEY=${secret_key}\n    nohup tiup tidb-lightning -config tidb-lightning.toml > nohup.out 2>&1 &\n    ```\n\n3. 导入开始后，可以采用以下任意方式查看进度：\n\n    - 通过 `grep` 日志关键字 `progress` 查看进度，默认 5 分钟更新一次。\n    - 通过监控面板查看进度，请参考 [TiDB Lightning 监控](/tidb-lightning/monitor-tidb-lightning.md)。\n    - 通过 Web 页面查看进度，请参考 [Web 界面](/tidb-lightning/tidb-lightning-web-interface.md)。\n\n4. 导入完毕后，TiDB Lightning 会自动退出。查看 `tidb-lightning.log` 日志末尾是否有 `the whole procedure completed` 信息，如果有，表示导入成功。如果没有，则表示导入遇到了问题，可根据日志中的 error 提示解决遇到的问题。\n\n> **注意：**\n>\n> 无论导入成功与否，最后一行都会显示 `tidb lightning exit`。它只是表示 TiDB Lightning 正常退出，不代表任务完成。\n\n如果导入过程中遇到问题，请参见 [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)。\n\n## 第 3 步：使用 DM 持续复制增量数据到 TiDB\n\n### 添加数据源\n\n1. 新建 `source1.yaml` 文件，写入以下内容：\n\n    ```yaml\n    # 唯一命名，不可重复。\n    source-id: \"mysql-01\"\n\n    # DM-worker 是否使用全局事务标识符 (GTID) 拉取 binlog。使用前提是上游 MySQL 已开启 GTID 模式。若上游存在主从自动切换，则必须使用 GTID 模式。\n    enable-gtid: true\n\n    from:\n      host: \"${host}\"           # 例如：172.16.10.81\n      user: \"root\"\n      password: \"${password}\"   # 支持但不推荐使用明文密码，建议使用 dmctl encrypt 对明文密码进行加密后使用\n      port: 3306\n    ```\n\n2. 在终端中执行下面的命令，使用 `tiup dmctl` 将数据源配置加载到 DM 集群中:\n\n    ```shell\n    tiup dmctl --master-addr ${advertise-addr} operate-source create source1.yaml\n    ```\n\n    该命令中的参数描述如下：\n\n    |参数           |描述|\n    |-              |-|\n    |`--master-addr`|dmctl 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如：172.16.10.71:8261|\n    |`operate-source create`|向 DM 集群加载数据源|\n\n### 添加同步任务\n\n1. 编辑 `task.yaml`，配置增量同步模式，以及每个数据源的同步起点：\n\n    ```yaml\n    name: task-test                      # 任务名称，需要全局唯一。\n    task-mode: incremental               # 任务模式，设为 \"incremental\" 即只进行增量数据迁移。\n\n    # 配置下游 TiDB 数据库实例访问信息\n    target-database:                     # 下游数据库实例配置。\n      host: \"${host}\"                    # 例如：127.0.0.1\n      port: 4000\n      user: \"root\"\n      password: \"${password}\"            # 推荐使用经过 dmctl 加密的密文。\n\n    #  使用黑白名单配置需要同步的表\n    block-allow-list:                    # 数据源数据库实例匹配的表的 block-allow-list 过滤规则集，如果 DM 版本早于 v2.0.0-beta.2 则使用 black-white-list。\n      bw-rule-1:                         # 黑白名单配置项 ID。\n        do-dbs: [\"${db-name}\"]           # 迁移哪些库。\n\n    # 配置数据源\n    mysql-instances:\n      - source-id: \"mysql-01\"            # 数据源 ID，即 source1.yaml 中的 source-id\n        block-allow-list: \"bw-rule-1\"    # 引入上面黑白名单配置。\n        # syncer-config-name: \"global\"    # 引用下面的 syncers 增量数据配置。\n        meta:                            # `task-mode` 为 `incremental` 且下游数据库的 `checkpoint` 不存在时 binlog 迁移开始的位置; 如果 checkpoint 存在，则以 `checkpoint` 为准。如果 `meta` 项和下游数据库的 `checkpoint` 都不存在，则从上游当前最新的 binlog 位置开始迁移。\n          # binlog-name: \"mysql-bin.000004\"  # 第 1 步中记录的日志位置，当上游存在主从切换时，必须使用 gtid。\n          # binlog-pos: 109227\n          binlog-gtid: \"09bec856-ba95-11ea-850a-58f2b4af5188:1-9\"\n\n    # 【可选配置】 如果增量数据迁移需要重复迁移已经在全量数据迁移中完成迁移的数据，则需要开启 safe mode 避免增量数据迁移报错。\n    #  该场景多见于以下情况：全量迁移的数据不属于数据源的一个一致性快照，随后从一个早于全量迁移数据之前的位置开始同步增量数据。\n    # syncers:            # sync 处理单元的运行配置参数。\n    #  global:           # 配置名称。\n    #    safe-mode: true # 设置为 true，会将来自数据源的 INSERT 改写为 REPLACE，将 UPDATE 改写为 DELETE 与 REPLACE，从而保证在表结构中存在主键或唯一索引的条件下迁移数据时可以重复导入 DML。在启动或恢复增量复制任务的前 1 分钟内 TiDB DM 会自动启动 safe mode。\n    ```\n\n    以上内容为执行迁移的最小任务配置。关于任务的更多配置项，可以参考[DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)。\n\n    在你启动数据迁移任务之前，建议使用`check-task`命令检查配置是否符合 DM 的配置要求，以降低后期报错的概率。\n\n    ```shell\n    tiup dmctl --master-addr ${advertise-addr} check-task task.yaml\n    ```\n\n2. 使用 `tiup dmctl` 执行以下命令启动数据迁移任务：\n\n    ```shell\n    tiup dmctl --master-addr ${advertise-addr} start-task task.yaml\n    ```\n\n    该命令中的参数描述如下：\n\n    |参数|描述|\n    |-|-|\n    |`--master-addr`|dmctl 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如：172.16.10.71:8261|\n    |`start-task`|命令用于创建数据迁移任务|\n\n    如果任务启动失败，可根据返回结果的提示进行配置变更，再执行上述命令重新启动任务。遇到问题请参考[故障及处理方法](/dm/dm-error-handling.md)以及[常见问题](/dm/dm-faq.md)。\n\n### 查看任务状态\n\n如需了解 DM 集群中是否存在正在运行的迁移任务及任务状态等信息，可使用 `tiup dmctl` 执行 `query-status` 命令进行查询：\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} query-status ${task-name}\n```\n\n关于查询结果的详细解读，请参考[查询状态](/dm/dm-query-status.md)。\n\n### 监控任务与查看日志\n\n要查看迁移任务的历史状态以及更多的内部运行指标，可参考以下步骤。\n\n如果使用 TiUP 部署 DM 集群时，正确部署了 Prometheus、Alertmanager 与 Grafana，则使用部署时填写的 IP 及端口进入 Grafana，选择 DM 的 dashboard 查看 DM 相关监控项。\n\nDM 在运行过程中，DM-worker、DM-master 及 dmctl 都会通过日志输出相关信息。各组件的日志目录如下：\n\n- DM-master 日志目录：通过 DM-master 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-master-8261/log/`。\n- DM-worker 日志目录：通过 DM-worker 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-worker-8262/log/`。\n\n## 探索更多\n\n- [暂停数据迁移任务](/dm/dm-pause-task.md)\n- [恢复数据迁移任务](/dm/dm-resume-task.md)\n- [停止数据迁移任务](/dm/dm-stop-task.md)\n- [导出和导入集群的数据源和任务配置](/dm/dm-export-import-config.md)\n- [处理出错的 DDL 语句](/dm/handle-failed-ddl-statements.md)\n"
        },
        {
          "name": "migrate-small-mysql-shards-to-tidb.md",
          "type": "blob",
          "size": 10.390625,
          "content": "---\ntitle: 从小数据量分库分表 MySQL 合并迁移数据到 TiDB\nsummary: 介绍如何从 TB 级以下分库分表 MySQL 迁移数据到 TiDB。\naliases: ['/zh/tidb/dev/usage-scenario-shard-merge/','/zh/tidb/dev/usage-scenario-simple-migration/']\n---\n\n# 从小数据量分库分表 MySQL 合并迁移数据到 TiDB\n\n如果你想把上游多个 MySQL 数据库实例合并迁移到下游的同一个 TiDB 数据库中，且数据量较小，你可以使用 DM 工具进行分库分表的合并迁移。本文所称“小数据量”通常指 TiB 级别以下。本文举例介绍了合并迁移的操作步骤、注意事项、故障排查等。本文档适用于:\n\n- TiB 级以内的分库分表数据合并迁移\n- 基于 MySQL binlog 的增量、持续分库分表合并迁移\n\n若要迁移分表总和 1 TiB 以上的数据，则 DM 工具耗时较长，可参考[从大数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-large-mysql-shards-to-tidb.md)。\n\n在本文档的示例中，数据源 MySQL 实例 1 和实例 2 均使用以下表结构，计划将 store_01 和 store_02 中 sale 开头的表合并导入下游 store.sale 表。\n\n|Schema|Tables|\n|-|-|\n|store_01   |sale_01, sale_02|\n|store_02   |sale_01, sale_02|\n\n迁移目标库的结构如下：\n\n|Schema|Tables|\n|-|-|\n|store      |sale|\n\n## 前提条件\n\n- [使用 TiUP 安装 DM 集群](/dm/deploy-a-dm-cluster-using-tiup.md)\n- [DM 所需上下游数据库权限](/dm/dm-worker-intro.md)\n\n## 分表数据冲突检查\n\n迁移中如果涉及合库合表，来自多张分表的数据可能引发主键或唯一索引的数据冲突。因此在迁移之前，需要检查各分表数据的业务特点。详情请参考[跨分表数据在主键或唯一索引冲突处理](/dm/shard-merge-best-practices.md#跨分表数据在主键或唯一索引冲突处理)。\n\n在本示例中：`sale_01` 和 `sale_02` 具有相同的表结构如下：\n\n```sql\nCREATE TABLE `sale_01` (\n  `id` bigint NOT NULL AUTO_INCREMENT,\n  `sid` bigint NOT NULL,\n  `pid` bigint NOT NULL,\n  `comment` varchar(255) DEFAULT NULL,\n  PRIMARY KEY (`id`),\n  UNIQUE KEY `sid` (`sid`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n```\n\n其中 `id` 列为主键，`sid` 列为分片键，具有全局唯一性。`id` 列具有自增属性，多个分表范围重复会引发数据冲突。`sid` 可以保证全局满足唯一索引，因此可以按照参考[去掉自增主键的主键属性](/dm/shard-merge-best-practices.md#去掉自增主键的主键属性)中介绍的操作绕过 `id` 列。在下游创建 `sale` 表时移除 `id` 列的唯一键属性：\n\n```sql\nCREATE TABLE `sale` (\n  `id` bigint NOT NULL,\n  `sid` bigint NOT NULL,\n  `pid` bigint NOT NULL,\n  `comment` varchar(255) DEFAULT NULL,\n  INDEX (`id`),\n  UNIQUE KEY `sid` (`sid`)\n) ENGINE=InnoDB DEFAULT CHARSET=latin1\n```\n\n## 第 1 步：创建数据源\n\n新建 `source1.yaml` 文件，写入以下内容：\n\n{{< copyable \"shell-regular\" >}}\n\n```yaml\n# 唯一命名，不可重复。\nsource-id: \"mysql-01\"\n\n# DM-worker 是否使用全局事务标识符 (GTID) 拉取 binlog。使用前提是上游 MySQL 已开启 GTID 模式。若上游存在主从自动切换，则必须使用 GTID 模式。\nenable-gtid: true\n\nfrom:\n  host: \"${host}\" # 例如：172.16.10.81\n  user: \"root\"\n  password: \"${password}\" # 支持但不推荐使用明文密码，建议使用 dmctl encrypt 对明文密码进行加密后使用\n  port: 3306\n```\n\n在终端中执行下面的命令，使用 `tiup dmctl` 将数据源配置加载到 DM 集群中:\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} operate-source create source1.yaml\n```\n\n该命令中的参数描述如下：\n\n| 参数           | 描述 |\n| -              | - |\n| `--master-addr`| dmctl 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如：172.16.10.71:8261 |\n| `operate-source create` | 向 DM 集群加载数据源 |\n\n重复以上操作直至所有数据源均添加完成。\n\n## 第 2 步：创建迁移任务\n\n新建`task1.yaml`文件，写入以下内容：\n\n{{< copyable \"\" >}}\n\n```yaml\nname: \"shard_merge\"\n# 任务模式，可设为\n# full：只进行全量数据迁移\n# incremental： binlog 实时同步\n# all： 全量 + binlog 迁移\ntask-mode: all\n# 分库分表合并任务则需要配置 shard-mode。默认使用悲观协调模式 \"pessimistic\"，在深入了解乐观协调模式的原理和使用限制后，也可以设置为乐观协调模式 \"optimistic\"\n# 详细信息可参考：https://docs.pingcap.com/zh/tidb/dev/feature-shard-merge/\nshard-mode: \"pessimistic\"\nmeta-schema: \"dm_meta\"                          # 将在下游数据库创建 schema 用于存放元数据\nignore-checking-items: [\"auto_increment_ID\"]    # 本示例中上游存在自增主键，因此需要忽略掉该检查项\n\ntarget-database:\n  host: \"${host}\"                               # 例如：192.168.0.1\n  port: 4000\n  user: \"root\"\n  password: \"${password}\"                       # 支持但不推荐使用明文密码，建议使用 dmctl encrypt 对明文密码进行加密后使用\n\nmysql-instances:\n  -\n    source-id: \"mysql-01\"                                   # 数据源 ID，即 source1.yaml 中的 source-id\n    route-rules: [\"sale-route-rule\"]                        # 应用于该数据源的 table route 规则\n    filter-rules: [\"store-filter-rule\", \"sale-filter-rule\"] # 应用于该数据源的 binlog event filter 规则\n    block-allow-list:  \"log-bak-ignored\"                    # 应用于该数据源的 Block & Allow Lists 规则\n  -\n    source-id: \"mysql-02\"\n    route-rules: [\"sale-route-rule\"]\n    filter-rules: [\"store-filter-rule\", \"sale-filter-rule\"]\n    block-allow-list:  \"log-bak-ignored\"\n\n# 分表合并配置\nroutes:\n  sale-route-rule:\n    schema-pattern: \"store_*\"                               # 合并 store_01 和 store_02 库到下游 store 库\n    table-pattern: \"sale_*\"                                 # 合并上述库中的 sale_01 和 sale_02 表到下游 sale 表\n    target-schema: \"store\"\n    target-table:  \"sale\"\n    # 可选配置：提取各分库分表的源信息，并写入下游用户自建的列，用于标识合表中各行数据的来源。如果配置该项，需要提前在下游手动创建合表，具体可参考下面 Table routing 的用法\n    # extract-table:                                        # 提取分表去除 sale_ 的后缀信息，并写入下游合表 c_table 列，例如，sale_01 分表的数据会提取 01 写入下游 c_table 列\n    #   table-regexp: \"sale_(.*)\"\n    #   target-column: \"c_table\"\n    # extract-schema:                                       # 提取分库去除 store_ 的后缀信息，并写入下游合表 c_schema 列，例如，store_02 分库的数据会提取 02 写入下游 c_schema 列\n    #   schema-regexp: \"store_(.*)\"\n    #   target-column: \"c_schema\"\n    # extract-source:                                       # 提取数据库源实例信息写入 c_source 列，例如，mysql-01 数据源实例的数据会提取 mysql-01 写入下游 c_source 列\n    #   source-regexp: \"(.*)\"\n    #   target-column: \"c_source\"\n\n# 过滤部分 DDL 事件\nfilters:\n  sale-filter-rule:\n    schema-pattern: \"store_*\"\n    table-pattern: \"sale_*\"\n    events: [\"truncate table\", \"drop table\", \"delete\"]\n    action: Ignore\n  store-filter-rule:\n    schema-pattern: \"store_*\"\n    events: [\"drop database\"]\n    action: Ignore\n\n# 黑白名单\nblock-allow-list:\n  log-bak-ignored:\n    do-dbs: [\"store_*\"]\n\n```\n\n以上内容为执行迁移的最小任务配置。关于任务的更多配置项，可以参考 [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)。\n\n若想了解配置文件中 `routes`、`filters` 等更多用法，请参考：\n\n- [Table routing](/dm/dm-table-routing.md)\n- [Block & Allow Table Lists](/dm/dm-block-allow-table-lists.md)\n- [如何过滤 binlog 事件](/filter-binlog-event.md)\n- [如何通过 SQL 表达式过滤 DML](/filter-dml-event.md)\n\n## 第 3 步：启动任务\n\n在你启动数据迁移任务之前，建议使用 `check-task` 命令检查配置是否符合 DM 的配置要求，以降低后期报错的概率。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} check-task task.yaml\n```\n\n使用 `tiup dmctl` 执行以下命令启动数据迁移任务。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} start-task task.yaml\n```\n\n该命令中的参数描述如下：\n\n| 参数 | 描述 |\n| - | - |\n| `--master-addr` | dmctl 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如：172.16.10.71:8261 |\n| `start-task` | 命令用于创建数据迁移任务 |\n\n如果任务启动失败，可根据返回结果的提示进行配置变更后执行 start-task task.yaml 命令重新启动任务。遇到问题请参考[故障及处理方法](/dm/dm-error-handling.md)以及[常见问题](/dm/dm-faq.md)。\n\n## 第 4 步：查看任务状态\n\n如需了解 DM 集群中是否存在正在运行的迁移任务及任务状态等信息，可使用 `tiup dmctl` 执行 `query-status` 命令进行查询：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} query-status ${task-name}\n```\n\n关于查询结果的详细解读，请参考[查询状态](/dm/dm-query-status.md)。\n\n## 第 5 步： 监控任务与查看日志(可选)\n\n你可以通过 Grafana 或者日志查看迁移任务的历史状态以及各种内部运行指标。\n\n- 通过 Grafana 查看\n\n    如果使用 TiUP 部署 DM 集群时，正确部署了 Prometheus、Alertmanager 与 Grafana，则使用部署时填写的 IP 及端口进入 Grafana，选择 DM 的 dashboard 查看 DM 相关监控项。\n\n- 通过日志查看\n\n    DM 在运行过程中，DM-worker、DM-master 及 dmctl 都会通过日志输出相关信息，其中包含迁移任务的相关信息。各组件的日志目录如下：\n\n    - DM-master 日志目录：通过 DM-master 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-master-8261/log/`。\n    - DM-worker 日志目录：通过 DM-worker 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-worker-8262/log/`。\n\n## 探索更多\n\n- [分库分表合并中的悲观/乐观模式](/dm/feature-shard-merge.md)\n- [分表合并数据迁移最佳实践](/dm/shard-merge-best-practices.md)\n- [故障及处理方法](/dm/dm-error-handling.md)\n- [性能问题及处理方法](/dm/dm-handle-performance-issues.md)\n- [常见问题](/dm/dm-faq.md)\n"
        },
        {
          "name": "migrate-small-mysql-to-tidb.md",
          "type": "blob",
          "size": 5.7294921875,
          "content": "---\ntitle: 从小数据量 MySQL 迁移数据到 TiDB\nsummary: 介绍如何从小数据量 MySQL 迁移数据到 TiDB。\naliases: ['/zh/tidb/dev/usage-scenario-incremental-migration/']\n---\n\n# 从小数据量 MySQL 迁移数据到 TiDB\n\n本文档介绍如何使用 TiDB DM （以下简称 DM）以全量+增量的模式数据到 TiDB。本文所称“小数据量”通常指 TiB 级别以下。\n\n一般而言，受到表结构索引数目等信息、硬件以及网络环境影响，迁移速率在 30～50GB/h 不等。使用 TiDB DM 迁移的流程如下图所示。\n\n![dm](/media/dm/migrate-with-dm.png)\n\n## 前提条件\n\n- [使用 TiUP 安装 DM 集群](/dm/deploy-a-dm-cluster-using-tiup.md)\n- [DM 所需上下游数据库权限](/dm/dm-worker-intro.md)\n\n## 第 1 步：创建数据源\n\n首先，新建 `source1.yaml` 文件，写入以下内容：\n\n{{< copyable \"\" >}}\n\n```yaml\n# 唯一命名，不可重复。\nsource-id: \"mysql-01\"\n\n# DM-worker 是否使用全局事务标识符 (GTID) 拉取 binlog。使用前提是上游 MySQL 已开启 GTID 模式。若上游存在主从自动切换，则必须使用 GTID 模式。\nenable-gtid: true\n\nfrom:\n  host: \"${host}\"         # 例如：172.16.10.81\n  user: \"root\"\n  password: \"${password}\" # 支持但不推荐使用明文密码，建议使用 dmctl encrypt 对明文密码进行加密后使用\n  port: 3306\n```\n\n其次，在终端中执行下面的命令后，使用 `tiup dmctl` 将数据源配置加载到 DM 集群中:\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} operate-source create source1.yaml\n```\n\n该命令中的参数描述如下：\n\n|参数           |描述|\n|-              |-|\n|`--master-addr`  |`dmctl` 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如：172.16.10.71:8261|\n|`operate-source create` |向 DM 集群加载数据源|\n\n## 第 2 步：创建迁移任务\n\n新建 `task1.yaml` 文件，写入以下内容：\n\n{{< copyable \"\" >}}\n\n```yaml\n# 任务名，多个同时运行的任务不能重名。\nname: \"test\"\n# 任务模式，可设为\n# full：只进行全量数据迁移\n# incremental： binlog 实时同步\n# all： 全量 + binlog 迁移\ntask-mode: \"all\"\n# 下游 TiDB 配置信息。\ntarget-database:\n  host: \"${host}\"                   # 例如：172.16.10.83\n  port: 4000\n  user: \"root\"\n  password: \"${password}\"           # 支持但不推荐使用明文密码，建议使用 dmctl encrypt 对明文密码进行加密后使用\n\n# 当前数据迁移任务需要的全部上游 MySQL 实例配置。\nmysql-instances:\n-\n  # 上游实例或者复制组 ID。\n  source-id: \"mysql-01\"\n  # 需要迁移的库名或表名的黑白名单的配置项名称，用于引用全局的黑白名单配置，全局配置见下面的 `block-allow-list` 的配置。\n  block-allow-list: \"listA\"\n\n\n# 黑白名单全局配置，各实例通过配置项名引用。\nblock-allow-list:\n  listA:                              # 名称\n    do-tables:                        # 需要迁移的上游表的白名单。\n    - db-name: \"test_db\"              # 需要迁移的表的库名。\n      tbl-name: \"test_table\"          # 需要迁移的表的名称。\n\n```\n\n以上内容为执行迁移的最小任务配置。关于任务的更多配置项，可以参考 [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)。\n\n## 第 3 步：启动任务\n\n在你启动数据迁移任务之前，建议使用 `check-task` 命令检查配置是否符合 DM 的配置要求，以避免后期报错。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} check-task task.yaml\n```\n\n使用 `tiup dmctl` 执行以下命令启动数据迁移任务。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} start-task task.yaml\n```\n\n该命令中的参数描述如下：\n\n|参数|描述|\n|-|-|\n|`--master-addr`|`dmctl` 要连接的集群的任意 DM-master 节点的 `{advertise-addr}`，例如： 172.16.10.71:8261|\n|`start-task`|参数用于启动数据迁移任务|\n\n如果任务启动失败，可根据返回结果的提示进行配置变更后执行 start-task task.yaml 命令重新启动任务。遇到问题请参考[故障及处理方法](/dm/dm-error-handling.md)以及[常见问题](/dm/dm-faq.md)。\n\n## 第 4 步：查看任务状态\n\n如需了解 DM 集群中是否存在正在运行的迁移任务及任务状态等信息，可使用 `tiup dmctl` 执行 `query-status` 命令进行查询：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup dmctl --master-addr ${advertise-addr} query-status ${task-name}\n```\n\n关于查询结果的详细解读，请参考[查询状态](/dm/dm-query-status.md)。\n\n## 第 5 步：监控任务与查看日志（可选）\n\n要查看迁移任务的历史状态以及更多的内部运行指标，可参考以下步骤。\n\n如果使用 TiUP 部署 DM 集群时，正确部署了 Prometheus、Alertmanager 与 Grafana，则使用部署时填写的 IP 及端口进入 Grafana，选择 DM 的 Dashboard 查看 DM 相关监控项。\n\nDM 在运行过程中，DM-worker、DM-master 及 dmctl 都会通过日志输出相关信息。各组件的日志目录如下：\n\n- DM-master 日志目录：通过 DM-master 进程参数 `--log-file`设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-master-8261/log/`。\n- DM-worker 日志目录：通过 DM-worker 进程参数 `--log-file` 设置。如果使用 TiUP 部署 DM，则日志目录默认位于 `/dm-deploy/dm-worker-8262/log/`。\n\n## 探索更多\n\n- [暂停数据迁移任务](/dm/dm-pause-task.md)\n- [恢复数据迁移任务](/dm/dm-resume-task.md)\n- [停止数据迁移任务](/dm/dm-stop-task.md)\n- [导出和导入集群的数据源和任务配置](/dm/dm-export-import-config.md)\n- [处理出错的 DDL 语句](/dm/handle-failed-ddl-statements.md)\n"
        },
        {
          "name": "migrate-with-more-columns-downstream.md",
          "type": "blob",
          "size": 4.5380859375,
          "content": "---\ntitle: 下游存在更多列的迁移场景\nsummary: 介绍下游存在更多列的迁移场景。\naliases: ['zh/tidb/dev/usage-scenario-downstream-more-columns/']\n---\n\n# 下游存在更多列的迁移场景\n\n本文档介绍数据同步时，下游存在更多列的迁移场景需要的注意事项。具体迁移操作可参考已有数据迁移场景：\n\n- [从小数据量 MySQL 迁移数据到 TiDB](/migrate-small-mysql-to-tidb.md)\n- [从大数据量 MySQL 迁移数据到 TiDB](/migrate-large-mysql-to-tidb.md)\n- [从小数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-small-mysql-shards-to-tidb.md)\n- [从大数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-large-mysql-shards-to-tidb.md)\n\n## 使用 DM 迁移至存在更多列的下游\n\nDM 同步上游的 binlog 时，会尝试使用下游当前的表结构来解析 binlog 并生成相应的 DML 语句。如果上游的 binlog 里数据表的列数与下游表结构的列数不一致，则会产生如下错误：\n\n```json\n\"errors\": [\n    {\n        \"ErrCode\": 36027,\n        \"ErrClass\": \"sync-unit\",\n        \"ErrScope\": \"internal\",\n        \"ErrLevel\": \"high\",\n        \"Message\": \"startLocation: [position: (mysql-bin.000001, 2022), gtid-set:09bec856-ba95-11ea-850a-58f2b4af5188:1-9 ], endLocation: [position: (mysql-bin.000001, 2022), gtid-set: 09bec856-ba95-11ea-850a-58f2b4af5188:1-9]: gen insert sqls failed, schema: log, table: messages: Column count doesn't match value count: 3 (columns) vs 2 (values)\",\n        \"RawCause\": \"\",\n        \"Workaround\": \"\"\n    }\n]\n```\n\n例如上游表结构为：\n\n```sql\n# 上游表结构\nCREATE TABLE `messages` (\n  `id` int NOT NULL,\n  PRIMARY KEY (`id`)\n)\n```\n\n例如下游表结构为：\n\n```sql\n# 下游表结构\nCREATE TABLE `messages` (\n  `id` int NOT NULL,\n  `message` varchar(255) DEFAULT NULL, # 下游比上游多出的列。\n  PRIMARY KEY (`id`)\n)\n```\n\n当 DM 尝试使用下游表结构解析上游产生的 binlog event 时，DM 会报出上述 `Column count doesn't match` 错误。\n\n此时，你可以使用 `binlog-schema` 命令来为数据源中需要迁移的表指定表结构，表结构需要对应 DM 将要开始同步的 binlog event 的数据。如果你在进行分表合并的数据迁移，那么需要为每个分表按照如下步骤在 DM 中设置用于解析 binlog event 的表结构。具体操作为：\n\n1. 在 DM 中，新建一个 `.sql` 文件，并将上游表结构对应的 `CREATE TABLE` 语句添加到该文件。例如，将以下表结构保存到 `log.messages.sql` 中。如果是 6.0 及以上版本，可以直接通过 `--from-source/--from-target` 更新，无需创建 SQL 文件。可参考[管理迁移表的表结构](/dm/dm-manage-schema.md)。\n\n    ```sql\n    # 上游表结构\n    CREATE TABLE `messages` (\n    `id` int NOT NULL,\n    PRIMARY KEY (`id`)\n    )\n    ```\n\n2. 使用 `binlog-schema` 命令为数据源中需要迁移的表设置表结构（此时数据迁移任务应该由于上述 `Column count doesn't match` 错误而处于 Paused 状态）。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```\n    tiup dmctl --master-addr ${advertise-addr} binlog-schema update -s ${source-id} ${task-name} ${database-name} ${table-name} ${schema-file}\n    ```\n\n    该命令中的参数描述如下：\n\n    | 参数            |  描述 |\n    | :---           | :--- |\n    | `--master-addr`  | 指定 dmctl 要连接的集群的任意 DM-master 节点的 `${advertise-addr}`。`${advertise-addr}` 表示 DM-master 向外界宣告的地址。 |\n    | `binlog-schema update` | 手动更新 schema 信息 |\n    | `-s`             | 指定 source。`${source-id}` 表示 MySQL 数据源 ID。 |\n    | `${task-name}` | 指定 task。表示数据同步任务配置文件 `task.yaml` 中定义的同步任务名称。|\n    | `${database-name}` | 指定 database。表示上游数据库名。 |\n    | `${table-name}` | 指定 table。表示上游数据表名。|\n    | `${schema-file}` | 指定表的 schema 文件。表示将被设置的表结构文件。|\n\n    例如：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```\n    tiup dmctl --master-addr 172.16.10.71:8261 binlog-schema update -s mysql-01 task-test -d log -t message log.message.sql\n    ```\n\n3. 使用 `resume-task` 命令恢复处于 Paused 状态的同步任务。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```\n    tiup dmctl --master-addr ${advertise-addr} resume-task ${task-name}\n    ```\n\n4. 使用 `query-status` 命令确认数据迁移任务是否运行正常。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```\n    tiup dmctl --master-addr ${advertise-addr} query-status ${task-name}\n    ```\n"
        },
        {
          "name": "migrate-with-pt-ghost.md",
          "type": "blob",
          "size": 3.4453125,
          "content": "---\ntitle: 上游使用 pt-osc/gh-ost 工具的持续同步场景\nsummary: 介绍在使用 DM 持续增量数据同步，上游使用 pt-osc/gh-ost 工具进行在线 DDL 变更时 DM 的处理方式和注意事项。\n---\n\n# 上游使用 pt-osc/gh-ost 工具的持续同步场景\n\n在生产业务中执行 DDL 时，产生的锁表操作会一定程度阻塞数据库的读取或者写入。为了把对读写的影响降到最低，用户往往会选择 online DDL 工具执行 DDL。常见的 Online DDL 工具有 [gh-ost](https://github.com/github/gh-ost) 和 [pt-osc](https://www.percona.com/doc/percona-toolkit/3.0/pt-online-schema-change.html)。\n\n在使用 DM 完成 MySQL 到 TiDB 的数据迁移时，可以开启`online-ddl`配置，实现 DM 工具与 gh-ost 或 pt-osc 的协同。\n\n具体迁移操作可参考已有数据迁移场景：\n\n- [从小数据量 MySQL 迁移数据到 TiDB](/migrate-small-mysql-to-tidb.md)\n- [从大数据量 MySQL 迁移数据到 TiDB](/migrate-large-mysql-to-tidb.md)\n- [从小数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-small-mysql-shards-to-tidb.md)\n- [从大数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-large-mysql-shards-to-tidb.md)\n\n## 开启 DM 的 online-ddl 特性\n\n配置 DM 的任务配置文件时，将全局参数的`online-ddl`设置为 true，具体配置示例如下图：\n\n```yaml\n# ----------- 全局配置 -----------\n## ********* 基本信息配置 *********\nname: test                      # 任务名称，需要全局唯一\ntask-mode: all                  # 任务模式，可设为 \"full\"、\"incremental\"、\"all\"\nshard-mode: \"pessimistic\"       # 如果为分库分表合并任务则需要配置该项。默认使用悲观协调模式 \"pessimistic\"，在深入了解乐观协调模式的原理和使用限制后，也可以设置为乐观协调模式 \"optimistic\"\nmeta-schema: \"dm_meta\"          # 下游储存 `meta` 信息的数据库\nonline-ddl: true                # 开启 DM 的 online DDL 支持特性。兼容上游使用 gh-ost 、pt-osc 两种工具的自动处理\n```\n\n## 开启 online-ddl 的影响\n\n当开启`online-ddl`特性后，DM 同步 gh-ost 或 pt-osc 工具所产生的 DDL 语句将会发生一些变化：\n\n上游 gh-ost 或 pt-osc 工具工作流如下：\n\n- 根据 DDL 目标表 (real table) 的表结构新建一张镜像表 (ghost table)；\n- 在镜像表上应用变更 DDL；\n- 将 DDL 目标表的数据同步到镜像表；\n- 在目标表与镜像表数据一致后，通过 RENAME 语句使镜像表替换掉目标表。\n\n`online-ddl=true` 时 DM 的同步方式：\n\n- 不在下游新建镜像表 (ghost table)；\n- 记录变更 DDL；\n- 仅从镜像表同步数据；\n- 在下游执行 DDL 变更。\n\n![dm-online-ddl](/media/dm/dm-online-ddl.png)\n\n这些变化将带来一些好处：\n\n- 下游 TiDB 无需创建和同步镜像表，节约相应存储空间和网络传输等开销；\n- 在分库分表合并场景下，自动忽略各分表镜像表的 RENAME 操作，保证同步正确性。\n\n如果您想深入了解其实现原理，请阅读以下两篇技术博客：\n\n- [DM 源码阅读系列文章（八）Online Schema Change 迁移支持](https://pingcap.com/blog-cn/dm-source-code-reading-8/#dm-源码阅读系列文章八online-schema-change-迁移支持)\n- [TiDB Online Schema Change 原理](https://pingcap.com/zh/blog/tidb-source-code-reading-17)\n\n## 探索更多\n\n- [DM 与 online DDL 工具协作细节](/dm/feature-online-ddl.md#dm-与-online-ddl-工具协作细节)\n"
        },
        {
          "name": "migration-overview.md",
          "type": "blob",
          "size": 4.388671875,
          "content": "---\ntitle: 数据迁移概述\nsummary: 了解各种数据迁移场景和对应的数据迁移方案。\n---\n\n# 数据迁移概述\n\n本文档总体介绍可用于 TiDB 的数据迁移方案。数据迁移方案如下：\n\n- 全量数据迁移。\n    - 数据导入：使用 TiDB Lightning 将 Aurora Snapshot，CSV 文件或 SQL dump 文件的数据全量导入到 TiDB 集群。\n    - 数据导出：使用 Dumpling 将 TiDB 集群的数据全量导出为 CSV 文件或 SQL dump 文件，从而更好地配合从 MySQL 数据库或 MariaDB 数据库进行数据迁移。\n    - TiDB DM (Data migration) 也提供了适合小规模数据量数据库（例如小于 1 TiB）的全量数据迁移功能。\n\n- 快速初始化 TiDB 集群：TiDB Lightning 提供的快速导入功能可以实现快速初始化 TiDB 集群的指定表的效果。请注意，使用快速初始化 TiDB 集群的功能对 TiDB 集群的影响极大，在进行初始化的过程中，TiDB 集群不支持对外访问。\n\n- 增量数据迁移：使用 TiDB DM 从 MySQL，MariaDB 或 Aurora 同步 Binlog 到 TiDB，该功能可以极大降低业务迁移过程中停机窗口时间。\n\n- TiDB 集群复制：TiDB 支持备份恢复功能，该功能可以实现将 TiDB 的某个快照初始化到另一个全新的 TiDB 集群。\n\n- TiDB 集群增量数据同步：TiCDC 支持同构数据库之间的灾备场景，能够在灾难发生时保证主备集群数据的最终一致性。目前该场景仅支持 TiDB 作为主备集群。\n\n根据迁移数据所在数据库类型、部署位置、业务数据规模大小、业务需求等因素，会有不同数据迁移选择。下面展示一些常用的数据迁移场景，方便用户依据这些线索选择到最适合自己的数据迁移方案。\n\n## 迁移 Aurora MySQL 到 TiDB\n\n从 Aurora 迁移数据到部署在 AWS 的 TiDB 集群，数据迁移可以分为全量迁移和增量迁移两个步骤进行。请根据你的业务需求选择相应的步骤。\n\n- [从 Aurora 迁移数据到 TiDB](/migrate-aurora-to-tidb.md)\n\n## 迁移 MySQL 到 TiDB\n\n如果你没有使用 Cloud storage (S3) 服务，而且网络联通和延迟情况良好，那么从 MySQL 迁移数据到 TiDB 时可以参照下面的方案。\n\n- [从小数据量 MySQL 迁移数据到 TiDB](/migrate-small-mysql-to-tidb.md)\n\n如果你对数据迁移速度有要求，或者数据规模特别大（例如大于 1 TiB），并且禁止 TiDB 集群在迁移期间有其他业务写入，那么你可以先使用 Lightning 进行快速导入，然后根据业务需要选择是否使用 DM 进行增量数据 (Binlog) 同步。\n\n- [从大数据量 MySQL 迁移数据到 TiDB](/migrate-large-mysql-to-tidb.md)\n\n## 分库分表 MySQL 合并迁移到 TiDB\n\n如果你的业务使用了基于 MySQL 分库的方案来存储数据，业务数据从 MySQL 迁移到 TiDB 后，合并这些分表数据到一张合并，那么你可以使用 DM 进行分表合并迁移。\n\n- [从小数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-small-mysql-shards-to-tidb.md)\n\n如果分表数据总规模特别大（例如大于 1 TiB），并且禁止 TiDB 集群在迁移期间有其他业务写入，那么你可以使用 Lightning 对分表数据进行快速合并导入，然后根据业务需要选择是否使用 DM 进行增量数据 (Binlog) 的分表同步。\n\n- [从大数据量分库分表 MySQL 合并迁移数据到 TiDB](/migrate-large-mysql-shards-to-tidb.md)\n\n### 将数据从 Vitess 迁移到 TiDB\n\n将数据从 Vitess 迁移到 TiDB，可参考以下文档：\n\n- [从 Vitess 迁移数据到 TiDB](/migrate-from-vitess.md)\n\n## 从文件迁移数据到 TiDB\n\n- [从 CSV 文件迁移数据到 TiDB](/migrate-from-csv-files-to-tidb.md)\n- [从 SQL 文件迁移数据到 TiDB](/migrate-from-sql-files-to-tidb.md)\n- [从 Parquet 文件迁移数据到 TiDB](/migrate-from-parquet-files-to-tidb.md)\n\n## TiDB 集群增量数据同步\n\n可以使用 TiCDC 进行 TiDB 集群间的增量数据同步。详情请参考 [TiCDC 简介](/ticdc/ticdc-overview.md)。\n\n## 复杂迁移场景\n\nDM 在实时同步过程中，多个已有特性可以使得同步过程更加灵活，适应各类业务需求：\n\n- [上游使用 pt/gh-ost 工具的持续同步场景](/migrate-with-pt-ghost.md)\n- [下游存在更多列的迁移场景](/migrate-with-more-columns-downstream.md)\n- [如何过滤 binlog 事件](/filter-binlog-event.md)\n- [如何通过 SQL 表达式过滤 binlog](/filter-dml-event.md)"
        },
        {
          "name": "migration-tools.md",
          "type": "blob",
          "size": 8.6796875,
          "content": "---\ntitle: 数据迁移工具概览\nsummary: 介绍 TiDB 的数据迁移工具。\n---\n\n# 数据迁移工具概览\n\nTiDB 提供了丰富的数据迁移相关的工具，用于全量迁移、增量迁移、备份恢复、数据同步等多种场景。\n\n本文介绍了使用这些工具的场景、支持的上下游、优势和相关限制等信息。请根据你的需求选择合适的工具。\n\n## [TiDB Data Migration (DM)](/dm/dm-overview.md)\n\n| 使用场景 | <span style=\"font-weight:normal\">用于将数据从与 MySQL 协议兼容的数据库迁移到 TiDB</span> |\n|---|---|\n| **上游** | MySQL，MariaDB，Aurora |\n| **下游** | TiDB |\n| **主要优势** | <ul><li>一体化的数据迁移任务管理工具，支持全量迁移和增量同步</li><li>支持对表与操作进行过滤</li><li>支持分库分表的合并迁移</li></ul>|\n| **使用限制** | 数据导入速度与 TiDB Lightning 的[逻辑导入模式](/tidb-lightning/tidb-lightning-logical-import-mode.md)大致相同，而比 TiDB Lightning 的[物理导入模式](/tidb-lightning/tidb-lightning-physical-import-mode.md)低很多。建议用于 1 TB 以内的存量数据迁移。 |\n\n## [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md)\n\n| 使用场景          | <span style=\"font-weight:normal\">用于将数据全量导入到 TiDB</span>                                                                                                                                                                                                                   |\n|---------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| **上游（输入源文件）** | <ul><li>Dumpling 输出的文件</li><li>从 Amazon Aurora、Apache Hive 或 Snowflake 导出的 Parquet 文件</li><li>CSV 文件</li><li>从本地盘或 Amazon S3 云盘读取数据</li></ul>                                                                                                                             |\n| **下游**        | TiDB                                                                                                                                                                                                                                                                      |\n| **主要优势**      | <ul><li>支持快速导入大量数据，实现快速初始化 TiDB 集群的指定表</li><li>支持断点续传</li><li>支持数据过滤</li></ul>                                                                                                                                                                                            |\n| **使用限制**      | <ul><li>如果使用[物理导入模式](/tidb-lightning/tidb-lightning-physical-import-mode.md)进行数据导入，TiDB Lightning 运行后，TiDB 集群将无法正常对外提供服务。</li><li>如果你不希望 TiDB 集群的对外服务受到影响，可以参考 TiDB Lightning [逻辑导入模式](/tidb-lightning/tidb-lightning-logical-import-mode.md)中的硬件需求与部署方式进行数据导入。</li></ul> |\n\n## [Dumpling](/dumpling-overview.md)\n\n| 使用场景 | <span style=\"font-weight:normal\">用于将数据从 MySQL/TiDB 进行全量导出</span> |\n|---|---|\n| **上游** | MySQL，TiDB |\n| **下游（输出文件）** | SQL，CSV |\n| **主要优势** | <ul><li>支持全新的 table-filter，筛选数据更加方便</li><li>支持导出到 Amazon S3 云盘</li></ul> |\n| **使用限制** | <ul><li>如果导出后计划往非 TiDB 的数据库恢复，建议使用 Dumpling。</li><li>如果导出后计划往另一个 TiDB 恢复，建议使用 [BR](/br/backup-and-restore-overview.md)。 </li></ul> |\n\n## [TiCDC](/ticdc/ticdc-overview.md)\n\n| 使用场景 | <span style=\"font-weight:normal\">通过拉取 TiKV 变更日志实现的 TiDB 增量数据同步工具，具有将数据还原到与上游任意 TSO 一致状态的能力，支持其他系统订阅数据变更</span> |\n|---|---|\n| **上游** | TiDB |\n| **下游** | TiDB，MySQL，Kafka，MQ，Confluent，存储服务（如 Amazon S3、GCS、Azure Blob Storage 和 NFS） |\n| **主要优势** | 提供开放数据协议 (TiCDC Open Protocol)。|\n| **使用限制** | TiCDC 只能同步至少存在一个有效索引的表。暂不支持以下场景：<ul><li>单独使用 RawKV 的 TiKV 集群。</li><li>在 TiDB 中创建 SEQUENCE 的 DDL 操作和 SEQUENCE 函数。</li></ul> |\n\n## [Backup & Restore (BR)](/br/backup-and-restore-overview.md)\n\n| 使用场景 | <span style=\"font-weight:normal\">通过对大数据量的 TiDB 集群进行数据备份和恢复，实现数据迁移</span> |\n|---|---|\n| **上游** | TiDB |\n| **下游（输出文件）** | SST，backup.meta 文件，backup.lock 文件 |\n| **主要优势** | <ul><li>适用于向另一个 TiDB 迁移数据。</li><li>支持数据冷备份到外部存储，可以用于灾备恢复。</li></ul> |\n| **使用限制** | <ul><li>BR 恢复到 TiCDC 的上游集群时，恢复数据无法由 TiCDC 同步到下游。</li><li>BR 只支持在 `mysql.tidb` 表中 `new_collation_enabled` 开关值相同的集群之间进行操作。</li></ul> |\n\n## [sync-diff-inspector](/sync-diff-inspector/sync-diff-inspector-overview.md)\n\n| 使用场景 | <span style=\"font-weight:normal\">用于校验 MySQL/TiDB 中两份数据的一致性</span> |\n|---|---|\n| **上游** | TiDB，MySQL |\n| **下游** | TiDB，MySQL |\n| **主要优势** | 提供了修复数据的功能，适用于修复少量不一致的数据。|\n| **使用限制** | <ul><li>对于 MySQL 和 TiDB 之间的数据同步不支持在线校验。</li><li>不支持 JSON、BIT、BINARY、BLOB 等类型的数据。</li></ul> |\n\n## 使用 TiUP 快速安装\n\n从 TiDB 4.0 开始，TiUP 作为软件包管理器，帮助你轻松管理 TiDB 生态系统中的不同集群组件。现在你可以只用一个 TiUP 命令行来管理任何组件。\n\n### 第 1 步：安装 TiUP\n\n```shell\ncurl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh\n```\n\n重新声明全局环境变量：\n\n```shell\nsource ~/.bash_profile\n```\n\n### 第 2 步：安装组件\n\n你可以通过以下命令查看可用组件：\n\n```shell\ntiup list\n```\n\n输出如下：\n\n```bash\nAvailable components:\nName            Owner      Description\n----            -----      -----------\nPCC             community  A tool used to capture plan changes among different versions of TiDB\nbench           pingcap    Benchmark database with different workloads\nbr              pingcap    TiDB/TiKV cluster backup restore tool.\ncdc             pingcap    CDC is a change data capture tool for TiDB\nchaosd          community  An easy-to-use Chaos Engineering tool used to inject failures to a physical node\nclient          pingcap    Client to connect playground\ncloud           pingcap    CLI tool to manage TiDB Cloud\ncluster         pingcap    Deploy a TiDB cluster for production\nctl             pingcap    TiDB controller suite\ndm              pingcap    Data Migration Platform manager\ndmctl           pingcap    dmctl component of Data Migration Platform.\nerrdoc          pingcap    Document about TiDB errors\npd-recover      pingcap    PD Recover is a disaster recovery tool of PD, used to recover the PD cluster which cannot start or provide services normally.\nplayground      pingcap    Bootstrap a local TiDB cluster for fun\ntidb            pingcap    TiDB is an open source distributed HTAP database compatible with the MySQL protocol.\ntidb-dashboard  pingcap    TiDB Dashboard is a Web UI for monitoring, diagnosing, and managing the TiDB cluster\ntidb-lightning  pingcap    TiDB Lightning is a tool used for fast full import of large amounts of data into a TiDB cluster\ntikv-br         pingcap    TiKV cluster backup restore tool\ntikv-cdc        pingcap    TiKV-CDC is a change data capture tool for TiKV\ntiproxy         pingcap    TiProxy is a database proxy that is based on TiDB.\ntiup            pingcap    TiUP is a command-line component management tool that can help to download and install TiDB platform components to the local system\n```\n\n选择所需要的一个或多个组件进行安装。示例如下：\n\n```shell\ntiup install dm\n```\n\n```shell\ntiup install dm tidb-lightning\n```\n\n> **注意：**\n>\n> 如果需要安装特定版本，可以使用 `tiup install <component>[:version]` 命令.\n\n### 第 3 步： 更新 TiUP 及组件 (可选)\n\n建议先查看新版本的更新日志及兼容性说明：\n\n```shell\ntiup update --self && tiup update dm\n```\n\n## 探索更多\n\n- [离线方式安装 TiUP](/production-deployment-using-tiup.md)\n- [以二进制包形式安装各工具](/download-ecosystem-tools.md)\n"
        },
        {
          "name": "minimal-deployment-topology.md",
          "type": "blob",
          "size": 5.466796875,
          "content": "---\ntitle: 最小拓扑架构\nsummary: 介绍 TiDB 集群的最小拓扑。\naliases: ['/docs-cn/dev/minimal-deployment-topology/']\n---\n\n# 最小拓扑架构\n\n本文档介绍 TiDB 集群最小部署的拓扑架构。\n\n## 拓扑信息\n\n|实例 | 个数 | 物理机配置 | IP |配置 |\n| :-- | :-- | :-- | :-- | :-- |\n| TiDB |2 | 16 VCore 32 GiB <br/> 100 GiB 用于存储| 10.0.1.1 <br/> 10.0.1.2 | 默认端口 <br/>  全局目录配置 |\n| PD | 3 | 4 VCore 8 GiB <br/> 100 GiB 用于存储|10.0.1.4 <br/> 10.0.1.5 <br/> 10.0.1.6 | 默认端口 <br/> 全局目录配置 |\n| TiKV | 3 | 16 VCore 32 GiB <br/> 2 TiB (NVMe SSD) 用于存储 | 10.0.1.7 <br/> 10.0.1.8 <br/> 10.0.1.9 | 默认端口 <br/> 全局目录配置 |\n| Monitoring & Grafana | 1 | 4 VCore 8 GiB <br/> 500 GiB (SSD) 用于存储 | 10.0.1.10 | 默认端口 <br/> 全局目录配置 |\n\n### 拓扑模版\n\n<details>\n<summary>简单最小配置模板</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\npd_servers:\n  - host: 10.0.1.4\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.1\n  - host: 10.0.1.2\n\ntikv_servers:\n  - host: 10.0.1.7\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\nmonitoring_servers:\n  - host: 10.0.1.10\n\ngrafana_servers:\n  - host: 10.0.1.10\n\nalertmanager_servers:\n  - host: 10.0.1.10\n```\n\n</details>\n\n<details>\n<summary>详细最小配置模板</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\n# # Monitored variables are applied to all the machines.\nmonitored:\n  node_exporter_port: 9100\n  blackbox_exporter_port: 9115\n  # deploy_dir: \"/tidb-deploy/monitored-9100\"\n  # data_dir: \"/tidb-data/monitored-9100\"\n  # log_dir: \"/tidb-deploy/monitored-9100/log\"\n\n# # Server configs are used to specify the runtime configuration of TiDB components.\n# # All configuration items can be found in TiDB docs:\n# # - TiDB: https://docs.pingcap.com/zh/tidb/stable/tidb-configuration-file\n# # - TiKV: https://docs.pingcap.com/zh/tidb/stable/tikv-configuration-file\n# # - PD: https://docs.pingcap.com/zh/tidb/stable/pd-configuration-file\n# # All configuration items use points to represent the hierarchy, e.g:\n# #   readpool.storage.use-unified-pool\n# #\n# # You can overwrite this configuration via the instance-level `config` field.\n\nserver_configs:\n  tidb:\n    log.slow-threshold: 300\n    binlog.enable: false\n    binlog.ignore-error: false\n  tikv:\n    # server.grpc-concurrency: 4\n    # raftstore.apply-pool-size: 2\n    # raftstore.store-pool-size: 2\n    # rocksdb.max-sub-compactions: 1\n    # storage.block-cache.capacity: \"16GB\"\n    # readpool.unified.max-thread-count: 12\n    readpool.storage.use-unified-pool: false\n    readpool.coprocessor.use-unified-pool: true\n  pd:\n    schedule.leader-schedule-limit: 4\n    schedule.region-schedule-limit: 2048\n    schedule.replica-schedule-limit: 64\n\npd_servers:\n  - host: 10.0.1.4\n    # ssh_port: 22\n    # name: \"pd-1\"\n    # client_port: 2379\n    # peer_port: 2380\n    # deploy_dir: \"/tidb-deploy/pd-2379\"\n    # data_dir: \"/tidb-data/pd-2379\"\n    # log_dir: \"/tidb-deploy/pd-2379/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.pd` values.\n    # config:\n    #   schedule.max-merge-region-size: 20\n    #   schedule.max-merge-region-keys: 200000\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.1\n    # ssh_port: 22\n    # port: 4000\n    # status_port: 10080\n    # deploy_dir: \"/tidb-deploy/tidb-4000\"\n    # log_dir: \"/tidb-deploy/tidb-4000/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tidb` values.\n    # config:\n    #   log.slow-query-file: tidb-slow-overwrited.log\n  - host: 10.0.1.2\n\ntikv_servers:\n  - host: 10.0.1.7\n    # ssh_port: 22\n    # port: 20160\n    # status_port: 20180\n    # deploy_dir: \"/tidb-deploy/tikv-20160\"\n    # data_dir: \"/tidb-data/tikv-20160\"\n    # log_dir: \"/tidb-deploy/tikv-20160/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tikv` values.\n    # config:\n    #   server.grpc-concurrency: 4\n    #   server.labels: { zone: \"zone1\", dc: \"dc1\", host: \"host1\" }\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\nmonitoring_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # port: 9090\n    # deploy_dir: \"/tidb-deploy/prometheus-8249\"\n    # data_dir: \"/tidb-data/prometheus-8249\"\n    # log_dir: \"/tidb-deploy/prometheus-8249/log\"\n\ngrafana_servers:\n  - host: 10.0.1.10\n    # port: 3000\n    # deploy_dir: /tidb-deploy/grafana-3000\n\nalertmanager_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # web_port: 9093\n    # cluster_port: 9094\n    # deploy_dir: \"/tidb-deploy/alertmanager-9093\"\n    # data_dir: \"/tidb-data/alertmanager-9093\"\n    # log_dir: \"/tidb-deploy/alertmanager-9093/log\"\n```\n\n</details>\n\n以上 TiDB 集群拓扑文件中，详细的配置项说明见[通过 TiUP 部署 TiDB 集群的拓扑文件配置](/tiup/tiup-cluster-topology-reference.md)。\n\n> **注意：**\n>\n> - 无需手动创建配置文件中的 `tidb` 用户，TiUP cluster 组件会在目标主机上自动创建该用户。可以自定义用户，也可以和中控机的用户保持一致。\n> - 如果部署目录配置为相对路径，会部署在用户的 Home 目录下。\n"
        },
        {
          "name": "multi-data-centers-in-one-city-deployment.md",
          "type": "blob",
          "size": 9.2998046875,
          "content": "---\ntitle: 单区域多 AZ 部署 TiDB\nsummary: 本文档介绍单个区域多个可用区部署 TiDB 的方案。\naliases: ['/docs-cn/dev/how-to/deploy/geographic-redundancy/overview/','/docs-cn/dev/geo-redundancy-deployment/']\n---\n\n# 单区域多 AZ 部署 TiDB\n\n<!-- Localization note for TiDB:\n\n- 英文：用 distributed SQL，同时开始强调 HTAP\n- 中文：可以保留 NewSQL 字眼，同时强调一栈式实时 HTAP\n- 日文：NewSQL 认可度高，用 NewSQL\n\n-->\n\n作为一栈式实时 HTAP 数据库，TiDB 兼顾了传统关系型数据库的优秀特性、NoSQL 数据库可扩展性以及跨可用区 (Availability Zone, AZ) 场景下的高可用。本文档旨在介绍同区域多 AZ 部署 TiDB 的方案。\n\n本文中的区域指的是地理隔离的不同位置，AZ 指的是区域内部划分的相互独立的资源集合。本文描述的方案同样适用于一个城市内多个数据中心（同城多中心）的场景。\n\n## 了解 Raft 协议\n\nRaft 是一种分布式一致性算法，在 TiDB 集群的多种组件中，PD 和 TiKV 都通过 Raft 实现了数据的容灾。Raft 的灾难恢复能力通过如下机制实现：\n\n- Raft 成员的本质是日志复制和状态机。Raft 成员之间通过复制日志来实现数据同步；Raft 成员在不同条件下切换自己的成员状态，其目标是选出 leader 以提供对外服务。\n- Raft 是一个表决系统，它遵循多数派协议，在一个 Raft Group 中，某成员获得大多数投票，它的成员状态就会转变为 leader。也就是说，当一个 Raft Group 还保有大多数节点 (majority) 时，它就能够选出 leader 以提供对外服务。\n\n遵循 Raft 可靠性的特点，放到现实场景中：\n\n- 想克服任意 1 台服务器 (host) 的故障，应至少提供 3 台服务器。\n- 想克服任意 1 个机柜 (rack) 的故障，应至少提供 3 个机柜。\n- 想克服任意 1 个可用区（AZ，也可以是同城的多个机房）的故障，应至少提供 3 个 AZ。\n\n- 想应对任意 1 个区域的灾难场景，应至少规划 3 个区域用于部署集群。\n\n可见，原生 Raft 协议对于偶数副本的支持并不是很友好，考虑跨区域网络延迟影响，同区域三 AZ 可能是最适合部署 Raft 的高可用及容灾方案。\n\n## 同区域三 AZ 方案\n\n同区域三 AZ 方案，即同区域有三个机房部署 TiDB 集群，AZ 间的数据在集群内部（通过 Raft 协议）进行同步。同区域三 AZ 可同时对外进行读写服务，任意中心发生故障不影响数据一致性。\n\n### 简易架构图\n\n集群 TiDB、TiKV 和 PD 组件分别部署在 3 个不同的 AZ，这是最常规且高可用性最高的方案。\n\n![三 AZ 部署](/media/deploy-3dc.png)\n\n**优点：**\n\n- 所有数据的副本分布在三个 AZ，具备高可用和容灾能力\n- 任何一个 AZ 失效后，不会产生任何数据丢失 (RPO = 0)\n- 任何一个 AZ 失效后，其他两个 AZ 会自动发起 leader election，并在一定时间内（通常 20s 以内）自动恢复服务\n\n![三 AZ 部署容灾](/media/deploy-3dc-dr.png)\n\n**缺点：**\n\n性能受网络延迟影响。具体影响如下：\n\n- 对于写入的场景，所有写入的数据需要同步复制到至少两个 AZ，由于 TiDB 写入过程使用两阶段提交，故写入延迟至少需要两倍 AZ 间的延迟。\n\n- 对于读请求来说，如果数据 leader 与发起读取的 TiDB 节点不在同一个 AZ，也会受网络延迟影响。\n- TiDB 中的每个事务都需要向 PD leader 获取 TSO，当 TiDB 与 PD leader 不在同一个 AZ 时，TiDB 上运行的事务也会因此受网络延迟影响，每个有写入的事务会获取两次 TSO。\n\n### 架构优化图\n\n如果不需要每个 AZ 同时对外提供服务，可以将业务流量全部派发到一个 AZ，并通过调度策略把 Region leader 和 PD leader 都迁移到同一个 AZ。这样，不管是从 PD 获取 TSO，还是读取 Region，都不会受 AZ 间网络的影响。当该 AZ 失效时，PD leader 和 Region leader 会自动在其它 AZ 选出，只需要把业务流量转移至其他存活的 AZ 即可。\n\n![三 AZ 部署读性能优化](/media/deploy-3dc-optimize.png)\n\n**优点：**\n\n集群 TSO 获取能力以及读取性能有所提升。具体调度策略设置模板参照如下：\n\n```shell\n-- 其他 AZ 将 leader 驱逐至承载业务流量的 AZ\n\nconfig set label-property reject-leader LabelName labelValue\n\n-- 迁移 PD leader 并设置优先级\nmember leader transfer pdName1\nmember leader_priority pdName1 5\nmember leader_priority pdName2 4\nmember leader_priority pdName3 3\n```\n\n> **注意：**\n>\n> TiDB 5.2 及以上版本默认不支持 `label-property` 配置。若要设置副本策略，请使用 [Placement Rules](/configure-placement-rules.md)。\n\n**缺点：**\n\n- 写入场景仍受 AZ 网络延迟影响，这是因为遵循 Raft 多数派协议，所有写入的数据需要同步复制到至少两个 AZ\n\n- TiDB Server 是 AZ 级别单点\n- 业务流量纯走单 AZ，性能受限于单 AZ 网络带宽压力\n- TSO 获取能力以及读取性能受限于业务流量 AZ 集群 PD、TiKV 组件是否正常，否则仍受跨 AZ 网络交互影响\n\n### 样例部署图\n\n#### 样例拓扑架构\n\n假设某区域有三个 AZ，AZ1、AZ2 和 AZ3。每个 AZ 中有两套机架，每个机架有三台服务器，不考虑混合布署以及单台机器多实例部署，同区域三 AZ 架构集群（3 副本）部署参考如下：\n\n![同区域三 AZ 集群部署](/media/multi-data-centers-in-one-city-deployment-sample.png)\n\n#### TiKV Labels 简介\n\nTiKV 是一个 Multi-Raft 系统，其数据按 Region（默认 96M）切分，每个 Region 的 3 个副本构成了一个 Raft Group。假设一个 3 副本 TiDB 集群，由于 Region 的副本数与 TiKV 实例数量无关，则一个 Region 的 3 个副本只会被调度到其中 3 个 TiKV 实例上，也就是说即使集群扩容 N 个 TiKV 实例，其本质仍是一个 3 副本集群。\n\n由于 3 副本的 Raft Group 只能容忍 1 副本故障，当集群被扩容到 N 个 TiKV 实例时，这个集群依然只能容忍一个 TiKV 实例的故障。2 个 TiKV 实例的故障可能会导致某些 Region 丢失多个副本，整个集群的数据也不再完整，访问到这些 Region 上的数据的 SQL 请求将会失败。而 N 个 TiKV 实例中同时有两个发生故障的概率是远远高于 3 个 TiKV 中同时有两个发生故障的概率的，也就是说 Multi-Raft 系统集群扩容 TiKV 实例越多，其可用性是逐渐降低的。\n\n正因为 Multi-Raft TiKV 系统局限性，Labels 标签应运而出，其主要用于描述 TiKV 的位置信息。Label 信息随着部署或滚动更新操作刷新到 TiKV 的启动配置文件中，启动后的 TiKV 会将自己最新的 Label 信息上报给 PD，PD 根据用户登记的 Label 名称（也就是 Label 元信息），结合 TiKV 的拓扑进行 Region 副本的最优调度，从而提高系统可用性。\n\n#### TiKV Labels 样例规划\n\n针对 TiKV Labels 标签，你需要根据已有的物理资源、容灾能力容忍度等方面进行设计与规划，进而提升系统的可用性和容灾能力。并根据已规划的拓扑架构，在集群初始化配置文件中进行配置（此处省略其他非重点项）：\n\n```ini\nserver_configs:\n  pd:\n    replication.location-labels: [\"zone\",\"az\",\"rack\",\"host\"]\n\ntikv_servers:\n  - host: 10.63.10.30\n    config:\n      server.labels: { zone: \"z1\", az: \"az1\", rack: \"r1\", host: \"30\" }\n  - host: 10.63.10.31\n    config:\n      server.labels: { zone: \"z1\", az: \"az1\", rack: \"r1\", host: \"31\" }\n  - host: 10.63.10.32\n    config:\n      server.labels: { zone: \"z1\", az: \"az1\", rack: \"r2\", host: \"32\" }\n  - host: 10.63.10.33\n    config:\n      server.labels: { zone: \"z1\", az: \"az1\", rack: \"r2\", host: \"33\" }\n\n  - host: 10.63.10.34\n    config:\n      server.labels: { zone: \"z2\", az: \"az2\", rack: \"r1\", host: \"34\" }\n  - host: 10.63.10.35\n    config:\n      server.labels: { zone: \"z2\", az: \"az2\", rack: \"r1\", host: \"35\" }\n  - host: 10.63.10.36\n    config:\n      server.labels: { zone: \"z2\", az: \"az2\", rack: \"r2\", host: \"36\" }\n  - host: 10.63.10.37\n    config:\n      server.labels: { zone: \"z2\", az: \"az2\", rack: \"r2\", host: \"37\" }\n\n  - host: 10.63.10.38\n    config:\n      server.labels: { zone: \"z3\", az: \"az3\", rack: \"r1\", host: \"38\" }\n  - host: 10.63.10.39\n    config:\n      server.labels: { zone: \"z3\", az: \"az3\", rack: \"r1\", host: \"39\" }\n  - host: 10.63.10.40\n    config:\n      server.labels: { zone: \"z3\", az: \"az3\", rack: \"r2\", host: \"40\" }\n  - host: 10.63.10.41\n    config:\n      server.labels: { zone: \"z3\", az: \"az3\", rack: \"r2\", host: \"41\" }\n```\n\n本例中，zone 表示逻辑可用区层级，用于控制副本的隔离（当前集群 3 副本）。\n\n不直接采用 az、rack 和 host 三层 Label 结构，是因为考虑到将来可能会扩容 AZ，假设新扩容的 AZ 编号是 AZ2、AZ3 和 AZ4，则只需在对应可用区下扩容 AZ，rack 也只需在对应 AZ 下扩容。\n\n如果直接采用 AZ、rack 和 host 三层 Label 结构，那么扩容 AZ 操作可能需重新添加 Label，TiKV 数据整体需要 Rebalance。\n\n### 高可用和容灾分析\n\n采用区域多 AZ 方案，当任意一个 AZ 故障时，集群能自动恢复服务，不需要人工介入，并能保证数据一致性。注意，各种调度策略主要用于优化性能，当发生故障时，调度机制总是优先考虑可用性而不是性能。\n"
        },
        {
          "name": "mysql-compatibility.md",
          "type": "blob",
          "size": 13.94921875,
          "content": "---\ntitle: 与 MySQL 兼容性对比\nsummary: 本文对 TiDB 和 MySQL 二者之间从语法和功能特性上做出详细的对比。\naliases: ['/docs-cn/dev/mysql-compatibility/','/docs-cn/dev/reference/mysql-compatibility/']\n---\n\n# 与 MySQL 兼容性对比\n\nTiDB 高度兼容 MySQL 协议，以及 MySQL 5.7 和 MySQL 8.0 常用的功能及语法。MySQL 生态中的系统工具（PHPMyAdmin、Navicat、MySQL Workbench、DBeaver 和[其他工具](/develop/dev-guide-third-party-support.md#gui)）、客户端等均适用于 TiDB。\n\n但 TiDB 尚未支持一些 MySQL 功能，可能的原因如下：\n\n- 有更好的解决方案，例如 JSON 取代 XML 函数。\n- 目前对这些功能的需求度不高，例如存储过程和函数。\n- 一些功能在分布式系统上的实现难度较大。\n\n除此以外，TiDB 不支持 MySQL 复制协议，但提供了专用工具用于与 MySQL 复制数据：\n\n- 从 MySQL 复制：[TiDB Data Migration (DM)](/dm/dm-overview.md) 是将 MySQL/MariaDB 数据迁移到 TiDB 的工具，可用于增量数据的复制。\n- 向 MySQL 复制：[TiCDC](/ticdc/ticdc-overview.md) 是一款通过拉取 TiKV 变更日志实现的 TiDB 增量数据同步工具，可通过 [MySQL sink](/ticdc/ticdc-sink-to-mysql.md) 将 TiDB 增量数据复制到 MySQL。\n\n> **注意：**\n>\n> 本页内容仅涉及 MySQL 与 TiDB 的总体差异。关于[安全特性](/security-compatibility-with-mysql.md)、[悲观事务模式](/pessimistic-transaction.md#和-mysql-innodb-的差异)相关的兼容信息，请查看各自具体页面。\n\n## 不支持的功能特性\n\n* 存储过程与函数\n* 触发器\n* 事件\n* 自定义函数\n* 全文语法与索引 [#1793](https://github.com/pingcap/tidb/issues/1793)\n* 空间类型的函数（即 `GIS`/`GEOMETRY`）、数据类型和索引 [#6347](https://github.com/pingcap/tidb/issues/6347)\n* 非 `ascii`、`latin1`、`binary`、`utf8`、`utf8mb4`、`gbk` 的字符集\n* MySQL 追踪优化器\n* XML 函数\n* X-Protocol [#1109](https://github.com/pingcap/tidb/issues/1109)\n* 列级权限 [#9766](https://github.com/pingcap/tidb/issues/9766)\n* `XA` 语法（TiDB 内部使用两阶段提交，但并没有通过 SQL 接口公开）\n* `CREATE TABLE tblName AS SELECT stmt` 语法 [#4754](https://github.com/pingcap/tidb/issues/4754)\n* `CHECK TABLE` 语法 [#4673](https://github.com/pingcap/tidb/issues/4673)\n* `CHECKSUM TABLE` 语法 [#1895](https://github.com/pingcap/tidb/issues/1895)\n* `REPAIR TABLE` 语法\n* `OPTIMIZE TABLE` 语法\n* `HANDLER` 语句\n* `CREATE TABLESPACE` 语句\n* \"Session Tracker: 将 GTID 上下文信息添加到 OK 包中\"\n* 降序索引 [#2519](https://github.com/pingcap/tidb/issues/2519)\n* `SKIP LOCKED` 语法 [#18207](https://github.com/pingcap/tidb/issues/18207)\n* 横向派生表 [#40328](https://github.com/pingcap/tidb/issues/40328)\n\n## 与 MySQL 有差异的特性详细说明\n\n### 自增 ID\n\n- TiDB 的自增列既能保证唯一，也能保证在单个 TiDB server 中自增，使用 [`AUTO_INCREMENT` MySQL 兼容模式](/auto-increment.md#mysql-兼容模式)能保证多个 TiDB server 中自增 ID，但不保证自动分配的值的连续性。建议避免将缺省值和自定义值混用，以免出现 `Duplicated Error` 的错误。\n\n- TiDB 可通过 `tidb_allow_remove_auto_inc` 系统变量开启或者关闭允许移除列的 `AUTO_INCREMENT` 属性。删除列属性的语法是：`ALTER TABLE MODIFY` 或 `ALTER TABLE CHANGE`。\n\n- TiDB 不支持添加列的 `AUTO_INCREMENT` 属性，移除该属性后不可恢复。\n\n- 对于 v6.6.0 及更早的 TiDB 版本，TiDB 的自增列行为与 MySQL InnoDB 保持一致，要求自增列必须为主键或者索引前缀。从 v7.0.0 开始，TiDB 移除了该限制，允许用户更灵活地定义表的主键。关于此更改的详细信息，请参阅 [#40580](https://github.com/pingcap/tidb/issues/40580)\n\n自增 ID 详情可参阅 [AUTO_INCREMENT](/auto-increment.md)。\n\n> **注意：**\n>\n> 若创建表时没有指定主键时，TiDB 会使用 `_tidb_rowid` 来标识行，该数值的分配会和自增列（如果存在的话）共用一个分配器。如果指定了自增列为主键，则 TiDB 会用该列来标识行。因此会有以下的示例情况：\n\n```sql\nmysql> CREATE TABLE t(id INT UNIQUE KEY AUTO_INCREMENT);\nQuery OK, 0 rows affected (0.05 sec)\n\nmysql> INSERT INTO t VALUES();\nQuery OK, 1 rows affected (0.00 sec)\n\nmysql> INSERT INTO t VALUES();\nQuery OK, 1 rows affected (0.00 sec)\n\nmysql> INSERT INTO t VALUES();\nQuery OK, 1 rows affected (0.00 sec)\n\nmysql> SELECT _tidb_rowid, id FROM t;\n+-------------+------+\n| _tidb_rowid | id   |\n+-------------+------+\n|           2 |    1 |\n|           4 |    3 |\n|           6 |    5 |\n+-------------+------+\n3 rows in set (0.01 sec)\n```\n\n可以看到，由于共用分配器，id 每次自增步长是 2。在 [MySQL 兼容模式](/auto-increment.md#mysql-兼容模式)中改掉了该行为，没有共用分配器，因此不会跳号。\n\n> **注意：**\n>\n> 使用 `AUTO_INCREMENT` 可能会给生产环境带热点问题，因此推荐使用 [`AUTO_RANDOM`](/auto-random.md) 代替。详情请参考 [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md#tidb-热点问题处理)。\n\n### Performance schema\n\nTiDB 主要使用 Prometheus 和 Grafana 来存储及查询相关的性能监控指标。因此，TiDB 的 Performance schema 表返回空结果。\n\n### 查询计划\n\nTiDB 中，执行计划（`EXPLAIN` 和 `EXPLAIN FOR`）在输出格式、内容、权限设置方面与 MySQL 有较大差别。\n\nMySQL 系统变量 `optimizer_switch` 在 TiDB 中是只读的，对查询计划没有影响。你还可以在 [optimizer hints](/optimizer-hints.md) 中使用与 MySQL 类似的语法，但可用的 hint 和实现原理可能会有所不同。\n\n详情参见[理解 TiDB 执行计划](/explain-overview.md)。\n\n### 内建函数\n\n支持常用的 MySQL 内建函数，有部分函数并未支持。可通过执行 [`SHOW BUILTINS`](/sql-statements/sql-statement-show-builtins.md) 语句查看可用的内建函数。\n\n### DDL 的限制\n\nTiDB 中，所有支持的 DDL 变更操作都是在线执行的。与 MySQL 相比，TiDB 中的 DDL 存在以下限制：\n\n* 使用 `ALTER TABLE` 语句修改一个表的多个模式对象（如列、索引）时，不允许在多个更改中指定同一个模式对象。例如，`ALTER TABLE t1 MODIFY COLUMN c1 INT, DROP COLUMN c1` 在两个更改中都指定了 `c1` 列，执行该语句会输出 `Unsupported operate same column/index` 的错误。\n* 不支持使用单个 `ALTER TABLE` 语句同时修改多个 TiDB 特有的模式对象，包括 `TIFLASH REPLICA`，`SHARD_ROW_ID_BITS`，`AUTO_ID_CACHE` 等。\n* `ALTER TABLE` 不支持少部分类型的变更。比如，TiDB 不支持从 `DECIMAL` 到 `DATE` 的变更。当遇到不支持的类型变更时，TiDB 将会报 `Unsupported modify column: type %d not match origin %d` 的错误。更多细节，请参考 [`ALTER TABLE`](/sql-statements/sql-statement-modify-column.md)。\n* TiDB 中，`ALGORITHM={INSTANT,INPLACE,COPY}` 语法只作为一种指定，并不更改 `ALTER` 算法，详情参阅 [`ALTER TABLE`](/sql-statements/sql-statement-alter-table.md)。\n* 不支持添加或删除 `CLUSTERED` 类型的主键。要了解关于 `CLUSTERED` 主键的详细信息，请参考[聚簇索引](/clustered-indexes.md)。\n* 不支持指定不同类型的索引 (`HASH|BTREE|RTREE|FULLTEXT`)。若指定了不同类型的索引，TiDB 会解析并忽略这些索引。\n* 分区表支持 `HASH`、`RANGE`、`LIST` 和 `KEY` 分区类型。对于不支持的分区类型，TiDB 会报 `Warning: Unsupported partition type %s, treat as normal table` 错误，其中 `%s` 为不支持的具体分区类型。\n* Range、Range COLUMNS、List、List COLUMNS 分区表支持 `ADD`、`DROP`、`TRUNCATE`、`REORGANIZE` 操作，其他分区操作会被忽略。\n* Hash 和 Key 分区表支持 `ADD`、`COALESCE`、`TRUNCATE` 操作，其他分区操作会被忽略。\n* TiDB 不支持以下分区表语法：\n\n    + `SUBPARTITION`\n    + `{CHECK|OPTIMIZE|REPAIR|IMPORT|DISCARD|REBUILD} PARTITION`\n\n  更多详情，请参考[分区表文档](/partitioned-table.md)。\n\n### `ANALYZE TABLE`\n\nTiDB 中的[信息统计](/statistics.md#手动收集)与 MySQL 中的有所不同：TiDB 中的信息统计会完全重构表的统计数据，语句消耗较多资源，执行过程较长，但在 MySQL/InnoDB 中，它是一个轻量级语句，执行过程较短。\n\n更多信息统计的差异请参阅 [`ANALYZE TABLE`](/sql-statements/sql-statement-analyze-table.md)。\n\n### `SELECT` 的限制\n\nTiDB 的 `SELECT` 语法有以下限制：\n\n- 不支持 `SELECT ... INTO @变量` 语法。\n- TiDB 中的 `SELECT .. GROUP BY expr` 的返回结果与 MySQL 5.7 并不一致。MySQL 5.7 的结果等价于 `GROUP BY expr ORDER BY expr`。\n\n详情参见 [`SELECT`](/sql-statements/sql-statement-select.md)。\n\n### `UPDATE` 语句\n\n详情参见 [`UPDATE`](/sql-statements/sql-statement-update.md)。\n\n### 视图\n\nTiDB 中的视图不可更新，不支持 `UPDATE`、`INSERT`、`DELETE` 等写入操作。\n\n### 临时表\n\n详见 [TiDB 本地临时表与 MySQL 临时表的兼容性](/temporary-tables.md#与-mysql-临时表的兼容性)。\n\n### 字符集和排序规则\n\n* 关于 TiDB 对字符集和排序规则的支持情况，详见[字符集和排序规则](/character-set-and-collation.md)。\n\n* 关于 GBK 字符集与 MySQL 的兼容情况，详见 [GBK 兼容情况](/character-set-gbk.md#与-mysql-的兼容性)。\n\n* TiDB 继承表中使用的字符集作为国家字符集。\n\n### 存储引擎\n\n- 仅在语法上兼容创建表时指定存储引擎，实际上 TiDB 会将元信息统一描述为 InnoDB 存储引擎。TiDB 支持类似 MySQL 的存储引擎抽象，但需要在系统启动时通过 [`--store`](/command-line-flags-for-tidb-configuration.md#--store) 配置项来指定存储引擎。\n\n### SQL 模式\n\nTiDB 支持大部分 [SQL 模式](/sql-mode.md)。不支持的 SQL 模式如下：\n\n- 不支持兼容模式，例如：`Oracle` 和 `PostgreSQL`（TiDB 解析但会忽略这两个兼容模式），MySQL 5.7 已弃用兼容模式，MySQL 8.0 已移除兼容模式。\n- TiDB 的 `ONLY_FULL_GROUP_BY` 模式与 MySQL 5.7 相比有细微的[语义差别](/functions-and-operators/aggregate-group-by-functions.md#与-mysql-的区别)。\n- `NO_DIR_IN_CREATE` 和 `NO_ENGINE_SUBSTITUTION` 仅用于解决与 MySQL 的兼容性问题，并不适用于 TiDB。\n\n### 默认设置\n\nTiDB 的默认设置与 MySQL 5.7 和 MySQL 8.0 有以下区别：\n\n- 字符集：\n    + TiDB 默认：`utf8mb4`。\n    + MySQL 5.7 默认：`latin1`。\n    + MySQL 8.0 默认：`utf8mb4`。\n\n- 排序规则：\n    + TiDB 中 `utf8mb4` 字符集默认：`utf8mb4_bin`。\n    + MySQL 5.7 中 `utf8mb4` 字符集默认：`utf8mb4_general_ci`。\n    + MySQL 8.0 中 `utf8mb4` 字符集默认：`utf8mb4_0900_ai_ci`。\n\n- SQL mode：\n    + TiDB 默认：`ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION`。\n    + MySQL 5.7 默认与 TiDB 相同。\n    + MySQL 8.0 默认 `ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION`。\n\n- `lower_case_table_names`：\n    + TiDB 默认：`2`，且仅支持设置该值为 `2`。\n    + MySQL 默认如下：\n        - Linux 系统中该值为 `0`，表示表名和数据库名按照在 `CREATE TABLE` 或 `CREATE DATABASE` 语句中指定的字母大小写存储在磁盘上，且名称比较时区分大小写。\n        - Windows 系统中该值为 `1`，表示表名按照小写字母存储在磁盘上，名称比较时不区分大小写。MySQL 在存储和查询时将所有表名转换为小写。该行为也适用于数据库名称和表的别名。\n        - macOS 系统中该值为 `2`，表示表名和数据库名按照在 `CREATE TABLE` 或 `CREATE DATABASE` 语句中指定的字母大小写存储在磁盘上，但 MySQL 在查询时将它们转换为小写。名称比较时不区分大小写。\n\n- `explicit_defaults_for_timestamp`：\n    + TiDB 默认：`ON`，且仅支持设置该值为 `ON`。\n    + MySQL 5.7 默认：`OFF`。\n    + MySQL 8.0 默认：`ON`。\n\n### 日期时间处理的区别\n\nTiDB 与 MySQL 在日期时间处理上有如下差异：\n\n- TiDB 采用系统当前安装的所有时区规则进行计算（一般为 `tzdata` 包），不需要导入时区表数据就能使用所有时区名称，导入时区表数据不会修改计算规则。\n\n- MySQL 默认使用本地时区，依赖于系统内置的当前的时区规则（例如什么时候开始夏令时等）进行计算；且在未[导入时区表数据](https://dev.mysql.com/doc/refman/8.0/en/time-zone-support.html#time-zone-installation)的情况下不能通过时区名称来指定时区。\n\n### 类型系统\n\nMySQL 支持 `SQL_TSI_*`（包括 `SQL_TSI_MONTH`、`SQL_TSI_WEEK`、`SQL_TSI_DAY`、`SQL_TSI_HOUR`、`SQL_TSI_MINUTE` 和 `SQL_TSI_SECOND`，但不包括 `SQL_TSI_YEAR`），但 TiDB 不支持。\n\n### 正则函数\n\n关于 TiDB 中正则函数 `REGEXP_INSTR()`、`REGEXP_LIKE()`、`REGEXP_REPLACE()`、`REGEXP_SUBSTR()` 与 MySQL 的兼容情况，请参考[正则函数与 MySQL 的兼容性](/functions-and-operators/string-functions.md#正则函数与-mysql-的兼容性)。\n\n### MySQL 弃用功能导致的不兼容问题\n\nTiDB 不支持 MySQL 中标记为弃用的功能，包括：\n\n* 指定浮点类型的精度。MySQL 8.0 [弃用](https://dev.mysql.com/doc/refman/8.0/en/floating-point-types.html)了此功能，建议改用 `DECIMAL` 类型。\n* `ZEROFILL` 属性。MySQL 8.0 [弃用](https://dev.mysql.com/doc/refman/8.0/en/numeric-type-attributes.html)了此功能，建议在业务应用中填充数字值。\n\n### `CREATE RESOURCE GROUP`，`DROP RESOURCE GROUP` 和 `ALTER RESOURCE GROUP`\n\nTiDB 资源组创建与修改语句的语法与 MySQL 官方不同，详情参见：\n\n- [`CREATE RESOURCE GROUP`](/sql-statements/sql-statement-create-resource-group.md)\n- [`DROP RESOURCE GROUP`](/sql-statements/sql-statement-drop-resource-group.md)\n- [`ALTER RESOURCE GROUP`](/sql-statements/sql-statement-alter-resource-group.md)\n"
        },
        {
          "name": "mysql-schema",
          "type": "tree",
          "content": null
        },
        {
          "name": "non-transactional-dml.md",
          "type": "blob",
          "size": 20.5703125,
          "content": "---\ntitle: 非事务 DML 语句\nsummary: 以事务的原子性和隔离性为代价，将 DML 语句拆成多个语句依次执行，用以提升批量数据处理场景的稳定性和易用性。\n---\n\n# 非事务 DML 语句\n\n本文档介绍非事务 DML 语句的使用场景、使用方法、使用限制和使用该功能的常见问题。\n\n非事务 DML 语句是将一个普通 DML 语句拆成多个 SQL 语句（即多个 batch）执行，以牺牲事务的原子性和隔离性为代价，增强批量数据处理场景下的性能和易用性。\n\n通常，对于消耗内存过多的大事务，你需要在应用中拆分 SQL 语句以绕过事务大小限制。非事务 DML 语句将这一过程集成到 TiDB 内核中，实现等价的效果。非事务 DML 语句的执行效果可以通过拆分 SQL 语句的结果来理解，`DRY RUN` 语法提供了预览拆分后语句的功能。\n\n非事务 DML 语句包括：\n\n- `INSERT INTO ... SELECT`\n- `REPLACE INTO .. SELECT`\n- `UPDATE`\n- `DELETE`\n\n详细的语法介绍见 [`BATCH`](/sql-statements/sql-statement-batch.md)。\n\n> **注意：**\n>\n> - 非事务 DML 语句不保证该语句的原子性和隔离性，不能认为它和原始 DML 语句等价。\n> - 在任意 DML 语句改写为非事务 DML 语句后，不应假设其行为与原来一致。\n> - 使用非事务 DML 前需要分析其拆分后的语句是否会互相影响。\n\n## 使用场景\n\n在大批量的数据处理场景，用户经常需要对一大批数据执行相同的操作。如果直接使用一个 SQL 语句执行操作，很可能导致事务大小超过限制，而大事务会明显影响执行性能。\n\n批量数据处理操作往往和在线业务操作不具有时间或数据上的交集。没有并发操作时，隔离性是不必要的。如果将批量数据操作设计成幂等的，或者易于重试的，那么原子性也是不必要的。如果你的业务满足这两个条件，那么可以考虑使用非事务 DML 语句。\n\n非事务 DML 语句用于在特定场景下绕过大事务的事务大小限制，用一条语句完成原本需要拆分成多个事务完成的任务，且执行效率更高，占用资源更少。\n\n例如，对于一个删除过期数据的需求，在确保没有任何业务会访问过期数据时，适合使用非事务 DML 语句来提升删除性能。\n\n## 前提条件\n\n要使用非事务 DML 语句，必须满足以下条件：\n\n- 确保该语句不需要原子性，即允许执行结果中，一部分行被修改，而一部分行没有被修改。\n- 确保该语句具有幂等性，或是做好准备根据错误信息对部分数据重试。如果系统变量 `tidb_redact_log = 1` 且 `tidb_nontransactional_ignore_error = 1`，则该语句必须是幂等的。否则语句部分失败时，无法准确定位失败的部分。\n- 确保该语句将要操作的数据没有其它并发的写入，即不被其它语句同时更新。否则可能出现漏写、多写、重复修改同一行等非预期的现象。\n- 确保该语句不会修改语句自身会读取的内容，否则后续的 batch 读到之前 batch 写入的内容，容易引起非预期的情况。\n    - 在使用非事务 `INSERT INTO ... SELECT` 处理同一张表时，尽量不要在插入时修改拆分列，否则可能因为多个 batch 读取到同一行，导致重复插入：\n        - 不推荐使用 `BATCH ON test.t.id LIMIT 10000 INSERT INTO t SELECT id+1, value FROM t;`\n        - 推荐使用 `BATCH ON test.t.id LIMIT 10000 INSERT INTO t SELECT id, value FROM t;`\n        - 当 `id` 列具有 `AUTO_INCREMENT` 属性时，推荐使用 `BATCH ON test.t.id LIMIT 10000 INSERT INTO t(value) SELECT value FROM t;`\n    - 在使用非事务 `UPDATE`、`INSERT ... ON DUPLICATE KEY UPDATE`、`REPLACE INTO` 时，拆分列不应该在语句中更新：\n        - 例如，对于一条非事务 `UPDATE` 语句，拆分后的 SQL 依次执行，前一 batch 的修改提交后被后一 batch 读到，导致同一行数据被多次修改。\n        - 这类语句不支持 `BATCH ON test.t.id LIMIT 10000 UPDATE t SET test.t.id = test.t.id-1;`\n        - 不推荐使用 `BATCH ON test.t.id LIMIT 1 INSERT INTO t SELECT id+1, value FROM t ON DUPLICATE KEY UPDATE id = id + 1;`\n    - 拆分列也不应该用于 Join key。例如，下面示例将拆分列 `test.t.id` 作为 Join key，导致一个非事务 `UPDATE` 语句多次更新同一行：\n\n        ```sql\n        CREATE TABLE t(id int, v int, key(id));\n        CREATE TABLE t2(id int, v int, key(id));\n        INSERT INTO t VALUES (1, 1), (2, 2), (3, 3);\n        INSERT INTO t2 VALUES (1, 1), (2, 2), (4, 4);\n        BATCH ON test.t.id LIMIT 1 UPDATE t JOIN t2 ON t.id = t2.id SET t2.id = t2.id+1;\n        SELECT * FROM t2; -- (4, 1) (4, 2) (4, 4)\n        ```\n\n- 确认该语句满足[使用限制](#使用限制)。\n- 不建议在该 DML 语句将要读写的表上同时进行并发的 DDL 操作。\n\n> **警告：**\n>\n> 如果同时开启了 `tidb_redact_log` 和 `tidb_nontransactional_ignore_error`，你可能无法完整得知每个 batch 的错误信息，无法只对失败的 batch 进行重试。因此，如果同时开启了这两个系统变量，该非事务 DML 语句必须是幂等的。\n\n## 使用示例\n\n### 使用非事务 DML 语句\n\n以下部分通过示例说明非事务 DML 语句的使用方法：\n\n创建一张表 `t`，表结构如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t(id int, v int, key(id));\n```\n\n```sql\nQuery OK, 0 rows affected\n```\n\n向表 `t` 中插入一些数据。\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO t VALUES (1,2),(2,3),(3,4),(4,5),(5,6);\n```\n\n```sql\nQuery OK, 5 rows affected\n```\n\n以下操作使用非事务 DML 语句，删除表 `t` 的 `v` 列上小于整数 6 的行。该语句将以 2 为 batch size，以 `id` 列为划分列，拆分为两个 SQL 语句执行。\n\n{{< copyable \"sql\" >}}\n\n```sql\nBATCH ON id LIMIT 2 DELETE FROM t where v < 6;\n```\n\n```sql\n+----------------+---------------+\n| number of jobs | job status    |\n+----------------+---------------+\n| 2              | all succeeded |\n+----------------+---------------+\n1 row in set\n```\n\n查看非事务 DML 语句的删除结果。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM t;\n```\n\n```sql\n+----+---+\n| id | v |\n+----+---+\n| 5  | 6 |\n+----+---+\n1 row in set\n```\n\n以下示例说明多表 join 的使用方法。首先创建表 `t2` 并插入数据。\n\n```sql\nCREATE TABLE t2(id int, v int, key(id));\nINSERT INTO t2 VALUES (1,1), (3,3), (5,5);\n```\n\n然后进行涉及多表 join 的更新（表 `t` 和 `t2`）。需要注意的是，指定拆分列时需要完整的数据库名、表名和列名（`test.t._tidb_rowid`)。\n\n```sql\nBATCH ON test.t._tidb_rowid LIMIT 1 UPDATE t JOIN t2 ON t.id = t2.id SET t2.id = t2.id+1;\n```\n\n查看更新后表的数据：\n\n```sql\nSELECT * FROM t2;\n```\n\n```sql\n+----+---+\n| id | v |\n+----+---+\n| 1  | 1 |\n| 3  | 3 |\n| 6  | 5 |\n+----+---+\n```\n\n### 查看非事务 DML 语句的执行进度\n\n非事务 DML 语句执行过程中，可以通过 `SHOW PROCESSLIST` 查看执行进度，返回结果中的 `Time` 表示当前 batch 执行的耗时。日志、慢日志等也会记录每个拆分后的语句在整个非事务 DML 语句中的进度。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nshow processlist;\n```\n\n```sql\n+------+------+--------------------+--------+---------+------+------------+----------------------------------------------------------------------------------------------------+\n| Id   | User | Host               | db     | Command | Time | State      | Info                                                                                               |\n+------+------+--------------------+--------+---------+------+------------+----------------------------------------------------------------------------------------------------+\n| 1203 | root | 100.64.10.62:52711 | test   | Query   | 0    | autocommit | /* job 506/500000 */ DELETE FROM `test`.`t1` WHERE `test`.`t1`.`_tidb_rowid` BETWEEN 2271 AND 2273 |\n| 1209 | root | 100.64.10.62:52735 | <null> | Query   | 0    | autocommit | show full processlist                                                                              |\n+------+------+--------------------+--------+---------+------+------------+----------------------------------------------------------------------------------------------------+\n```\n\n### 终止一个非事务 DML 语句\n\n通过 `KILL TIDB <processlist_id>` 终止一个非事务语句时，TiDB 会取消当前正在执行的 batch 之后的所有 batch。执行结果信息需要从日志里获得。\n\n关于 `KILL TiDB` 的更多信息，参见 [`KILL`](/sql-statements/sql-statement-kill.md)。\n\n### 查询非事务 DML 语句中划分 batch 的语句\n\n要查询非事务 DML 语句中用于划分 batch 的语句，你可在非事务 DML 语句中添加 `DRY RUN QUERY`。添加后，TiDB 不实际执行这个查询和后续的 DML 操作。\n\n下面这条语句查询 `BATCH ON id LIMIT 2 DELETE FROM t WHERE v < 6` 这条非事务 DML 语句内用于划分 batch 的查询语句。\n\n{{< copyable \"sql\" >}}\n\n```sql\nBATCH ON id LIMIT 2 DRY RUN QUERY DELETE FROM t WHERE v < 6;\n```\n\n```sql\n+--------------------------------------------------------------------------------+\n| query statement                                                                |\n+--------------------------------------------------------------------------------+\n| SELECT `id` FROM `test`.`t` WHERE (`v` < 6) ORDER BY IF(ISNULL(`id`),0,1),`id` |\n+--------------------------------------------------------------------------------+\n1 row in set\n```\n\n### 查询非事务 DML 语句中首末 batch 对应的语句\n\n要查询非事务 DML 语句中第一个和最后一个 batch 对应的实际 DML 语句，你可在语句中添加 `DRY RUN`。添加后，TiDB 只划分 batch，不执行这些 SQL 语句。因为 batch 数量可能很多，不显示全部 batch，只显示第一个和最后一个 batch。\n\n{{< copyable \"sql\" >}}\n\n```sql\nBATCH ON id LIMIT 2 DRY RUN DELETE FROM t where v < 6;\n```\n\n```sql\n+-------------------------------------------------------------------+\n| split statement examples                                          |\n+-------------------------------------------------------------------+\n| DELETE FROM `test`.`t` WHERE (`id` BETWEEN 1 AND 2 AND (`v` < 6)) |\n| DELETE FROM `test`.`t` WHERE (`id` BETWEEN 3 AND 4 AND (`v` < 6)) |\n+-------------------------------------------------------------------+\n2 rows in set\n```\n\n### 在非事务 DML 语句中使用 Optimizer Hint\n\n对于 `DELETE` 语句原本支持的 Optimizer Hint，非事务 `DELETE` 语句也同样支持，hint 位置与普通 `DELETE` 语句中的位置相同：\n\n{{< copyable \"sql\" >}}\n\n```sql\nBATCH ON id LIMIT 2 DELETE /*+ USE_INDEX(t)*/ FROM t where v < 6;\n```\n\n## 最佳实践\n\n建议按照以下步骤执行非事务 DML 语句：\n\n1. 选择合适的[划分列](#参数说明)。建议使用整数或字符串类型。\n2. 在非事务 DML 语句中添加 `DRY RUN QUERY`，手动执行查询，确认 DML 语句影响的数据范围是否大体正确。\n3. 在非事务 DML 语句中添加 `DRY RUN`，手动执行查询，检查拆分后的语句和执行计划。需要关注：\n\n    - 一条拆分后的语句是否有可能读到之前的语句执行写入的结果，否则容易造成异常现象。\n    - 索引选择效率。\n    - 由 TiDB 自动选择的拆分列是否可能会被修改。\n\n4. 执行非事务 DML 语句。\n5. 如果报错，从报错信息或日志中获取具体失败的数据范围，进行重试或手动处理。\n\n## 参数说明\n\n| 参数 | 说明 | 默认值 | 是否必填 | 建议值 |\n| :-- | :-- | :-- | :-- | :-- |\n| 划分列 | 用于划分 batch 的列，例如以上非事务 DML 语句 `BATCH ON id LIMIT 2 DELETE FROM t WHERE v < 6` 中的 `id` 列  | TiDB 尝试自动选择（不建议） | 否 | 选择可以最高效地满足 `WHERE` 条件的列 |\n| Batch size | 用于控制每个 batch 的大小，batch 即 DML 操作拆分成的 SQL 语句个数，例如以上非事务 DML 语句 `BATCH ON id LIMIT 2 DELETE FROM t WHERE v < 6` 中的 `LIMIT 2`。batch 数量越多，batch size 越小 | N/A | 是 | 1000～1000000，过小和过大都会导致性能下降 |\n\n### 划分列的选择\n\n非事务 DML 语句需要用一个列作为数据分批的标准，该列即为划分列。为获得更高的执行效率，划分列必须能够利用索引。不同的索引和划分列所导致的执行效率可能有数十倍的差别。选择划分列时，可以考虑以下建议：\n\n- 当你对业务数据分布有一定了解时，根据 `WHERE` 条件，选择划分 batch 后，划分范围较小的列。\n    - 在理想情况下，`WHERE` 条件可以利用划分列的索引，降低每个 batch 需要扫描的数据量。例如有一个交易表，记录了每一笔交易的开始和结束时间，你希望删除结束时间在一个月之前的所有交易记录。如果在交易的开始时间有索引，且已知交易的开始和结束时间通常相差不大，那么可以选择开始时间作为划分列。\n    - 在不太理想的情况下：划分列的数据分布与 `WHERE` 条件完全无关，无法利用划分列的索引来减少数据扫描的范围。\n- 有聚簇索引时，建议用主键作为划分列，这样语句执行效率更高（包括 `INT` 主键和 `_tidb_rowid`）。\n- 选择重复值较少的列。\n\n你可以不指定划分列，TiDB 默认会使用 handle 的第一列作为划分列。但如果聚簇索引主键的第一列是非事务 DML 语句不支持的数据类型（即 `ENUM`，`BIT`，`SET`，`JSON`），TiDB 会报错。你可根据业务需要选择合适的划分列。\n\n### Batch size 的选择\n\n非事务 DML 语句中，batch size 越大，拆分出来的 SQL 语句越少，每个 SQL 语句执行起来越慢。最优的 batch size 依据 workload 而定。根据经验值，推荐从 50000 开始尝试。过小和过大的 batch size 都会导致执行效率下降。\n\n每个 batch 的信息存储在内存里，因此 batch 数量过多会显著增加内存消耗。这也是 batch size 不能过小的原因之一。非事务语句用于存储 batch 信息消耗的内存上限与 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 相同，超出这个限制时触发的操作由系统变量 [`tidb_mem_oom_action`](/system-variables.md#tidb_mem_oom_action-从-v610-版本开始引入) 控制。\n\n## 使用限制\n\n非事务 DML 语句的硬性限制，不满足这些条件时 TiDB 会报错。\n\n- DML 语句不能包含 `ORDER BY` 或 `LIMIT` 字句。\n- 不支持子查询或集合操作。\n- 用于拆分的列必须被索引。该索引可以是单列的索引，或是一个联合索引的第一列。\n- 必须在 [`autocommit`](/system-variables.md#autocommit) 模式中使用。\n- 不能在开启了 batch-dml 时使用。\n- 不能在设置了 [`tidb_snapshot`](/read-historical-data.md#操作流程) 时使用。\n- 不能与 `prepare` 语句一起使用。\n- 划分列不支持 `ENUM`，`BIT`，`SET`，`JSON` 类型。\n- 不支持用在[临时表](/temporary-tables.md)上。\n- 不支持[公共表表达式](/develop/dev-guide-use-common-table-expression.md)。\n\n## 控制 batch 执行失败\n\n非事务 DML 语句不满足原子性，可能存在一些 batch 成功，一些 batch 失败的情况。系统变量 [`tidb_nontransactional_ignore_error`](/system-variables.md#tidb_nontransactional_ignore_error-从-v610-版本开始引入) 控制非事务 DML 语句处理错误的行为。\n\n一个例外是，如果第一个 batch 就执行失败，有很大概率是语句本身有错，此时整个非事务语句会直接返回一个错误。\n\n## 实现原理\n\n非事务 DML 语句的实现原理，是将原本需要在用户侧手动执行的 SQL 语句拆分工作内置为 TiDB 的一个功能，简化用户操作。要理解非事务 DML 语句的行为，可以将其想象成一个用户脚本进行了如下操作：\n\n对于非事务 DML `BATCH ON $C$ LIMIT $N$ DELETE FROM ... WHERE $P$`，其中 $C$ 为用于拆分的列，$N$ 为 batch size，$P$ 为筛选条件。\n\n1. TiDB 根据原始语句的筛选条件 $P$，和指定的用于拆分的列 $C$，查询出所有满足 $P$ 的 $C$。对这些 $C$ 排序后按 $N$ 分成多个分组 $B_1 \\dots B_k$。对所有 $B_i$，保留它的第一个和最后一个 $C$，记为 $S_i$ 和 $E_i$。这一步所执行的查询语句，可以通过 [`DRY RUN QUERY`](/non-transactional-dml.md#查询非事务-dml-语句中划分-batch-的语句) 查看。\n2. $B_i$ 所涉及的数据就是满足 $P_i$: $C$ BETWEEN $S_i$ AND $E_i$ 的一个子集。可以通过 $P_i$ 来缩小每个 batch 需要处理的数据范围。\n3. 对 $B_i$，将上面这个条件嵌入原始语句的 WHERE 条件，使其变为 WHERE ($P_i$) AND ($P$)。这一步的执行结果，可以通过 [`DRY RUN`](/non-transactional-dml.md#查询非事务-dml-语句中首末-batch-对应的语句) 查看。\n4. 对所有 batch，依次执行新的语句。收集每个分组的错误并合并，在所有分组结束后作为整个非事务 DML 语句的结果返回。\n\n## 与 batch-dml 的异同\n\nbatch-dml 是一种在 DML 语句执行期间将一个事务拆成多个事务提交的机制。\n\n> **注意：**\n>\n> batch-dml 功能使用不当时，存在数据索引不一致风险。该功能已被废弃，因此不建议使用。\n\n非事务 DML 语句尚不能替代所有的 batch-dml 使用场景。它们的主要区别有：\n\n- 性能：在[划分效率](#划分列的选择)较高的情况下，非事务 DML 语句和 batch-dml 性能接近。在划分效率较低的情况下，非事务 DML 语句可能会明显慢于 batch-dml。\n\n- 稳定性：batch-dml 极易因为使用不当导致数据索引不一致问题。而非事务 DML 语句不会导致数据索引不一致问题。但使用不当时，非事务 DML 语句与原始语句不等价，应用可能观察到和预期不符的现象。详见[常见问题](#常见问题)。\n\n## 常见问题\n\n### 执行多表 join 语句后，遇到 `Unknown column xxx in 'where clause'` 错误\n\n当拼接查询语句时，如果 `WHERE` 子句中的条件涉及到了[划分列](#参数说明)所在表以外的其它表，就会出现该错误。例如，以下 SQL 语句中，划分列为 `t2.id`，划分列所在的表为 `t2`，但 `WHERE` 子句中的条件涉及到了 `t2` 和 `t3`。\n\n```sql\nBATCH ON test.t2.id LIMIT 1 \nINSERT INTO t \nSELECT t2.id, t2.v, t3.id FROM t2, t3 WHERE t2.id = t3.id\n```\n\n```sql\n(1054, \"Unknown column 't3.id' in 'where clause'\")\n```\n\n当遇到此错误时，你可以通过 `DRY RUN QUERY` 打印出查询语句来确认。例如：\n\n```sql\nBATCH ON test.t2.id LIMIT 1 \nDRY RUN QUERY INSERT INTO t \nSELECT t2.id, t2.v, t3.id FROM t2, t3 WHERE t2.id = t3.id\n```\n\n要避免该错误，可以尝试将 `WHERE` 子句中涉及其它表的条件移动到 `JOIN` 的 `ON` 条件中。例如：\n\n```sql\nBATCH ON test.t2.id LIMIT 1 \nINSERT INTO t \nSELECT t2.id, t2.v, t3.id FROM t2 JOIN t3 ON t2.id = t3.id\n```\n\n```\n+----------------+---------------+\n| number of jobs | job status    |\n+----------------+---------------+\n| 0              | all succeeded |\n+----------------+---------------+\n```\n\n### 实际的 batch 大小和指定的 batch size 不一样\n\n在非事务 DML 语句的执行过程中，最后一个 batch 处理的数据量可能会小于 batch size。\n\n在**划分列有重复值**时，每个 batch 会将当前 batch 中划分列的最后一个元素的所有重复值全部加入当前 batch 中，因此这个 batch 的行数可能会多于 batch size。\n\n另外，在有其它并发的数据写入时，也可能导致每个 batch 实际处理的行数和 batch size 不一致。\n\n### 执行时出现报错 `Failed to restore the delete statement, probably because of unsupported type of the shard column`\n\n划分列的类型暂时不支持 `ENUM`、`BIT`、`SET`、`JSON` 类型，请尝试重新指定一个划分列。推荐使用整数或字符串类型的列。如果划分列不是这些类型，请从 PingCAP 官方或 TiDB 社区[获取支持](/support.md)。\n\n### 非事务 `DELETE` 出现和普通的 `DELETE` 不等价的“异常”行为\n\n非事务 DML 语句和这个 DML 语句的原始形式并不等价，这可能是由以下原因导致的：\n\n- 有并发的其它写入。\n- 非事务 DML 语句修改了语句自身会读取的值。\n- 在每个 batch 上实际执行的 SQL 语句由于改变了 `WHERE` 条件，可能会导致执行计划以及表达式计算顺序不同，由此导致了执行结果不一样。\n- DML 语句中含有非确定性的操作。\n\n## 兼容信息\n\n非事务语句是 TiDB 独有的功能，与 MySQL 不兼容。\n\n## 探索更多\n\n* [BATCH](/sql-statements/sql-statement-batch.md) 语法\n* [`tidb_nontransactional_ignore_error`](/system-variables.md#tidb_nontransactional_ignore_error-从-v610-版本开始引入)\n"
        },
        {
          "name": "online-unsafe-recovery.md",
          "type": "blob",
          "size": 10.5615234375,
          "content": "---\ntitle: Online Unsafe Recovery 使用文档\nsummary: 如何使用 Online Unsafe Recovery。\n---\n\n# Online Unsafe Recovery 使用文档\n\n> **警告：**\n>\n> - 此功能为有损恢复，无法保证数据索引一致性和事务完整性，若有问题需要额外的工具或者步骤进行相应修复。\n> - 该功能自 v6.1.0 版本开始引入。在 TiDB v6.1 以下版本为实验特性，行为与本文描述有区别，**不推荐**使用。在其他版本使用该功能时，请参考相应版本文档。\n\n当多数副本的永久性损坏造成部分数据不可读写时，可以使用 Online Unsafe Recovery 功能进行数据有损恢复，使 TiKV 正常提供服务。\n\n## 功能说明\n\n在 TiDB 中，根据用户定义的多种副本规则，一份数据可能会同时存储在多个节点中，从而保证在单个或少数节点暂时离线或损坏时，读写数据不受任何影响。但是，当一个 Region 的多数或全部副本在短时间内全部下线时，该 Region 会处于暂不可用的状态，无法进行读写操作。\n\n如果一段数据的多数副本发生了永久性损坏（如磁盘损坏）等问题，从而导致节点无法上线时，此段数据会一直保持暂不可用的状态。这时，如果用户希望集群恢复正常使用，在用户能够容忍数据回退或数据丢失的前提下，用户理论上可以通过手动移除不可用副本的方式，使 Region 重新形成多数派，进而让上层业务可以写入和读取（可能是 stale 的，或者为空）这一段数据分片。\n\n在这个情况下，当存有可容忍丢失的数据的部分节点受到永久性损坏时，用户可以通过使用 Online Unsafe Recovery，快速简单地进行有损恢复。使用 Online Unsafe Recovery 时，PD 会自动暂停调度（包括 split 和 merge），然后收集全部节点内的数据分片元信息，用 PD 的全局视角生成一份更实时、更完整的恢复计划后，将其计划下发给各个存活的节点，使各节点执行数据恢复任务。另外，下发恢复计划后，PD 还会定期查看恢复进度，并在必要时重新向各节点分发恢复计划。\n\n## 适用场景\n\nOnline Unsafe Recovery 功能适用于以下场景：\n\n* 部分节点受到永久性损坏，导致节点无法重启，造成业务端的部分数据不可读、不可写。\n* 可以容忍数据丢失，希望受影响的数据恢复读写。\n\n## 使用步骤\n\n### 前提条件\n\n在使用 Online Unsafe Recovery 功能进行数据有损恢复前，请确认以下事项：\n\n* 离线节点导致部分数据确实不可用。\n* 离线节点确实无法自动恢复或重启。\n\n### 第 1 步：指定无法恢复的节点\n\n使用 PD Control 执行 [`unsafe remove-failed-stores <store_id>[,<store_id>,...]`](/pd-control.md#unsafe-remove-failed-stores-store-ids--show) 命令，指定**所有**已确定无法恢复的 TiKV 和 TiFlash 节点，并用逗号隔开，以触发自动恢复。\n\n```bash\npd-ctl -u <pd_addr> unsafe remove-failed-stores <store_id1,store_id2,...>\n```\n\n> **注意：**\n>\n> - 请确保在该命令中一次性指定**所有**已确定无法恢复的 TiKV 节点和 TiFlash 节点，如果有部分无法恢复的节点被遗漏，恢复可能会被阻塞。\n> - 如果在短时间内 (如一天时间内)，已经运行过一次 Online Unsafe Recovery，请仍确保该命令后续的执行仍然带有之前已经处理过的 TiKV 和 TiFlash 节点。\n\n可通过 `--timeout <seconds>` 指定可允许执行恢复的最长时间。若未指定，默认为 5 分钟。当超时后，恢复中断报错。\n\n该命令输出 `Success` 表示向 PD 注册任务成功。但仅表示请求已被接受，并不代表恢复成功。恢复任务在后台进行，具体进度使用 [`show`](#第-2-步查看进度等待结束) 查看。\n\n该命令输出 `Failed` 表示注册任务失败，可能的错误有：\n\n- `unsafe recovery is running`：已经有正在进行的恢复任务\n- `invalid input store x doesn't exist`：指定的 store ID 不存在\n- `invalid input store x is up and connected`：指定的 store ID 仍然是健康的状态，不应该进行恢复\n\n若 PD 进行过灾难性恢复 [`pd-recover`](/pd-recover.md) 等类似操作，丢失了无法恢复的 TiKV 节点的 store 信息，因此无法确定要传入的 store ID 时，可使用 `--auto-detect` 模式。在该模式下，PD 会自动清理那些没有注册过的 TiKV 节点（或曾经注册过但已经被强制删除的 TiKV 节点）上的副本。\n\n```bash\npd-ctl -u <pd_addr> unsafe remove-failed-stores --auto-detect\n```\n\n> **注意：**\n>\n> - Unsafe Recovery 需要收集来自所有 Peer 的信息，可能会造成 PD 短时间内有明显的内存使用量上涨（10 万个 Peer 预计使用约 500 MiB 内存）。\n> - 若执行过程中 PD 发生重启，则恢复中断，需重新触发命令。\n> - 一旦执行，所指定的节点将被设为 Tombstone 状态，不再允许启动。\n> - 执行过程中，所有调度以及 split/merge 都会被暂停，待恢复成功或失败后自动恢复。\n\n### 第 2 步：查看进度等待结束\n\n节点移除命令运行成功后，使用 PD Control 执行 [`unsafe remove-failed-stores show`](/pd-control.md#config-show--set-option-value--placement-rules) 命令，查看移除进度。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl -u <pd_addr> unsafe remove-failed-stores show\n```\n\n恢复过程有多个可能的阶段：\n\n- `collect report`：初始阶段，第一次接收 TiKV 的报告获得的全局信息。\n- `tombstone tiflash learner`：在不健康的 Region 中，删除比其他健康 Peer 要新的 TiFlash learner，防止极端情况造成 panic。\n- `force leader for commit merge`：特殊阶段。在有未完成的 commit merge 时出现，优先对有 commit merge 的 Region 进行 `force leader`，防止极端情况。\n- `force leader`：强制不健康的 Region 在剩余的健康 Peer 中指定一个成为 Raft leader。\n- `demote failed voter`：将 Region 不健康的 Voter 降级为 Learner，之后 Region 就可以正常地选出 Raft leader。\n- `create empty region`：创建一个空 Region 来补足 key range 的空洞，主要针对的是某些 Region 的所有副本所在的 Store 都损坏了。\n\n每一阶段按照 JSON 格式输出，包括信息，时间，以及具体的恢复计划。例如：\n\n```json\n[\n    {\n        \"info\": \"Unsafe recovery enters collect report stage\",\n        \"time\": \"......\",\n        \"details\" : [\n            \"failed stores 4, 5, 6\",\n        ]\n    },\n    {\n        \"info\": \"Unsafe recovery enters force leader stage\",\n        \"time\": \"......\",\n        \"actions\": {\n            \"store 1\": [\n                \"force leader on regions: 1001, 1002\"\n            ],\n            \"store 2\": [\n                \"force leader on regions: 1003\"\n            ]\n        }\n    },\n    {\n        \"info\": \"Unsafe recovery enters demote failed voter stage\",\n        \"time\": \"......\",\n        \"actions\": {\n            \"store 1\": [\n                \"region 1001 demotes peers { id:101 store_id:4 }, { id:102 store_id:5 }\",\n                \"region 1002 demotes peers { id:103 store_id:5 }, { id:104 store_id:6 }\",\n            ],\n            \"store 2\": [\n                \"region 1003 demotes peers { id:105 store_id:4 }, { id:106 store_id:6 }\",\n            ]\n        }\n    },\n    {\n        \"info\": \"Collecting reports from alive stores(1/3)\",\n        \"time\": \"......\",\n        \"details\": [\n            \"Stores that have not dispatched plan: \",\n            \"Stores that have reported to PD: 4\",\n            \"Stores that have not reported to PD: 5, 6\",\n        ]\n    }\n]\n```\n\nPD 下发恢复计划后，会等待 TiKV 上报执行的结果。如上述输出中最后一阶段的 `Collecting reports from alive stores` 显示 PD 下发恢复计划和接受 TiKV 报告的具体状态。\n\n整个恢复过程包括多个阶段，可能存在某一阶段的多次重试。一般情况下，预计时间为 3~10 个 store heartbeat 周期（一个 store heartbeat 默认为 10s)。当恢复完成后，命令执行结果最后一阶段显示 `\"Unsafe recovery finished\"`，以及受影响的 Region 所属的 table id（若无或使用 RawKV 则不显示）和受影响的 SQL 元数据 Region。如：\n\n```json\n{\n    \"info\": \"Unsafe recovery finished\",\n    \"time\": \"......\",\n    \"details\": [\n        \"Affected table ids: 64, 27\",\n        \"Affected meta regions: 1001\",\n    ]\n}\n```\n\n得到受影响的 table id 后，可以使用 `INFORMATION_SCHEMA.TABLES` 来查看受影响的表名。\n\n```sql\nSELECT TABLE_SCHEMA, TABLE_NAME, TIDB_TABLE_ID FROM INFORMATION_SCHEMA.TABLES WHERE TIDB_TABLE_ID IN (64, 27);\n```\n\n> **注意：**\n>\n> - 恢复操作把一些 failed Voter 变成了 failed Learner，之后还需要 PD 调度经过一些时间将这些 failed Learner 移除。\n> - 建议及时添加新的节点。\n\n若执行过程中发生错误，最后一阶段会显示 `\"Unsafe recovery failed\"` 以及具体错误。如：\n\n```json\n{\n    \"info\": \"Unsafe recovery failed: <error>\",\n    \"time\": \"......\"\n}\n```\n\n### 第 3 步：检查数据索引一致性（RawKV 不需要）\n\n> **注意：**\n>\n> 数据可以读写并不代表没有数据丢失。\n\n执行完成后，数据和索引可能会不一致。请使用 [`ADMIN CHECK`](/sql-statements/sql-statement-admin-check-table-index.md) 对受影响的表进行数据索引的一致性检查。\n\n```sql\nADMIN CHECK TABLE table_name;\n```\n\n若结果有不一致的索引，可以通过重命名旧索引、创建新索引，然后再删除旧索引的步骤来修复数据索引不一致的问题。\n\n1. 重命名旧索引：\n\n    ```sql\n    ALTER TABLE table_name RENAME INDEX index_name TO index_name_lame_duck;\n    ```\n\n2. 创建新索引：\n\n    ```sql\n    ALTER TABLE table_name ADD INDEX index_name (column_name);\n    ```\n\n3. 删除旧索引：\n\n    ```sql\n    ALTER TABLE table_name DROP INDEX index_name_lame_duck;\n    ```\n\n### 第 4 步：移除无法恢复的节点（可选）\n\n<SimpleTab>\n<div label=\"通过 TiUP 部署的节点\">\n\n1. 缩容无法恢复的节点：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    tiup cluster scale-in <cluster-name> -N <host> --force\n    ```\n\n2. 清理 Tombstone 节点：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    tiup cluster prune <cluster-name>\n    ```\n\n</div>\n<div label=\"通过 TiDB Operator 部署的节点\">\n\n1. 删除该 `PersistentVolumeClaim`。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    kubectl delete -n ${namespace} pvc ${pvc_name} --wait=false\n    ```\n\n2. 删除 TiKV Pod，并等待新创建的 TiKV Pod 加入集群。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    kubectl delete -n ${namespace} pod ${pod_name}\n    ```\n\n</div>\n</SimpleTab>\n"
        },
        {
          "name": "optimistic-transaction.md",
          "type": "blob",
          "size": 7.5703125,
          "content": "---\ntitle: TiDB 乐观事务模型\nsummary: 了解 TiDB 的乐观事务模型。\naliases: ['/docs-cn/dev/optimistic-transaction/','/docs-cn/dev/reference/transactions/transaction-optimistic/','/docs-cn/dev/reference/transactions/transaction-model/']\n---\n\n# TiDB 乐观事务模型\n\n乐观事务模型下，将修改冲突视为事务提交的一部分。因此并发事务不常修改同一行时，可以跳过获取行锁的过程进而提升性能。但是并发事务频繁修改同一行（冲突）时，乐观事务的性能可能低于[悲观事务](/pessimistic-transaction.md)。\n\n启用乐观事务前，请确保应用程序可正确处理 `COMMIT` 语句可能返回的错误。如果不确定应用程序将会如何处理，建议改为使用悲观事务。\n\n> **注意：**\n>\n> 自 v3.0.8 开始，TiDB 集群默认使用[悲观事务模式](/pessimistic-transaction.md)。但如果从 3.0.7 及之前版本创建的集群升级到 3.0.8 及之后的版本，不会改变默认事务模式，即**只有新创建的集群才会默认使用悲观事务模式**。\n\n## 乐观事务原理\n\n为支持分布式事务，TiDB 中乐观事务使用两阶段提交协议，流程如下：\n\n![TiDB 中的两阶段提交](/media/2pc-in-tidb.png)\n\n1. 客户端开始一个事务。\n\n    TiDB 从 PD 获取一个全局唯一递增的时间戳作为当前事务的唯一事务 ID，这里称为该事务的 `start_ts`。TiDB 实现了多版本并发控制 (MVCC)，因此 `start_ts` 同时也作为该事务获取的数据库快照版本。该事务只能读到此 `start_ts` 版本可以读到的数据。\n\n2. 客户端发起读请求。\n\n    1. TiDB 从 PD 获取数据路由信息，即数据具体存在哪个 TiKV 节点上。\n    2. TiDB 从 TiKV 获取 `start_ts` 版本下对应的数据。\n\n3. 客户端发起写请求。\n\n    TiDB 校验写入数据是否符合约束（如数据类型是否正确、是否符合非空约束等）。**校验通过的数据将存放在 TiDB 中该事务的私有内存里。**\n\n4. 客户端发起 commit。\n\n5. TiDB 开始两阶段提交，在保证事务原子性的前提下，进行数据持久化。\n\n    1. TiDB 从当前要写入的数据中选择一个 Key 作为当前事务的 Primary Key。\n    2. TiDB 从 PD 获取所有数据的写入路由信息，并将所有的 Key 按照所有的路由进行分类。\n    3. TiDB 并发地向所有涉及的 TiKV 发起 prewrite 请求。TiKV 收到 prewrite 数据后，检查数据版本信息是否存在冲突或已过期。符合条件的数据会被加锁。\n    4. TiDB 收到所有 prewrite 响应且所有 prewrite 都成功。\n    5. TiDB 向 PD 获取第二个全局唯一递增版本号，定义为本次事务的 `commit_ts`。\n    6. TiDB 向 Primary Key 所在 TiKV 发起第二阶段提交。TiKV 收到 commit 操作后，检查数据合法性，清理 prewrite 阶段留下的锁。\n    7. TiDB 收到两阶段提交成功的信息。\n\n6. TiDB 向客户端返回事务提交成功的信息。\n\n7. TiDB 异步清理本次事务遗留的锁信息。\n\n## 优缺点分析\n\n通过分析 TiDB 中事务的处理流程，可以发现 TiDB 事务有如下优点：\n\n* 实现原理简单，易于理解。\n* 基于单实例事务实现了跨节点事务。\n* 锁管理实现了去中心化。\n\n但 TiDB 事务也存在以下缺点：\n\n* 两阶段提交使网络交互增多。\n* 需要一个中心化的分配时间戳服务。\n* 事务数据量过大时易导致内存暴涨。\n\n## 事务的重试\n\n> **注意：**\n>\n> 从 v8.0.0 开始，[`tidb_disable_txn_auto_retry`](/system-variables.md#tidb_disable_txn_auto_retry) 被废弃，不再支持乐观事务的自动重试。推荐使用[悲观事务模式](/pessimistic-transaction.md)。如果使用乐观事务模式发生冲突，请在应用里捕获错误并重试。\n\n使用乐观事务模型时，在高冲突率的场景中，事务容易发生写写冲突而导致提交失败。MySQL 使用悲观事务模型，在执行写入类型的 SQL 语句的过程中进行加锁并且在 Repeatable Read 隔离级别下使用了当前读的机制，能够读取到最新的数据，所以提交时一般不会出现异常。为了降低应用改造难度，TiDB 提供了数据库内部自动重试机制。\n\n### 重试机制\n\n当事务提交时，如果发现写写冲突，TiDB 内部重新执行包含写操作的 SQL 语句。你可以通过设置 `tidb_disable_txn_auto_retry = OFF` 开启自动重试，并通过 `tidb_retry_limit` 设置重试次数：\n\n```toml\n# 设置是否禁用自动重试，默认为 “on”，即不重试。\ntidb_disable_txn_auto_retry = OFF\n# 控制重试次数，默认为 “10”。只有自动重试启用时该参数才会生效。\n# 当 “tidb_retry_limit = 0” 时，也会禁用自动重试。\ntidb_retry_limit = 10\n```\n\n你也可以修改当前 Session 或 Global 的值：\n\n- Session 级别设置：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SET tidb_disable_txn_auto_retry = OFF;\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SET tidb_retry_limit = 10;\n    ```\n\n- Global 级别设置：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SET GLOBAL tidb_disable_txn_auto_retry = OFF;\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SET GLOBAL tidb_retry_limit = 10;\n    ```\n\n> **注意：**\n>\n> `tidb_retry_limit` 变量决定了事务重试的最大次数。当它被设置为 0 时，所有事务都不会自动重试，包括自动提交的单语句隐式事务。这是彻底禁用 TiDB 中自动重试机制的方法。禁用自动重试后，所有冲突的事务都会以最快的方式上报失败信息（包含 `try again later`）给应用层。\n\n### 重试的局限性\n\nTiDB 默认不进行事务重试，因为重试事务可能会导致更新丢失，从而破坏[可重复读的隔离级别](/transaction-isolation-levels.md)。\n\n事务重试的局限性与其原理有关。事务重试可概括为以下三个步骤：\n\n1. 重新获取 `start_ts`。\n2. 重新执行包含写操作的 SQL 语句。\n3. 再次进行两阶段提交。\n\n第二步中，重试时仅重新执行包含写操作的 SQL 语句，并不涉及读操作的 SQL 语句。但是当前事务中读到数据的时间与事务真正开始的时间发生了变化，写入的版本变成了重试时获取的 `start_ts` 而非事务一开始时获取的 `start_ts`。因此，当事务中存在依赖查询结果来更新的语句时，重试将无法保证事务原本可重复读的隔离级别，最终可能导致结果与预期出现不一致。\n\n如果业务可以容忍事务重试导致的异常，或并不关注事务是否以可重复读的隔离级别来执行，则可以开启自动重试。\n\n## 冲突检测\n\n作为一个分布式系统，TiDB 在内存中的冲突检测是在 TiKV 中进行，主要发生在 prewrite 阶段。因为 TiDB 集群是一个分布式系统，TiDB 实例本身无状态，实例之间无法感知到彼此的存在，也就无法确认自己的写入与别的 TiDB 实例是否存在冲突，所以会在 TiKV 这一层检测具体的数据是否有冲突。\n\n具体配置如下：\n\n```toml\n# scheduler 内置一个内存锁机制，防止同时对一个 Key 进行操作。\n# 每个 Key hash 到不同的 slot。（默认为 2048000）\nscheduler-concurrency = 2048000\n```\n\n此外，TiKV 支持监控等待 latch 的时间：\n\n![Scheduler latch wait duration](/media/optimistic-transaction-metric.png)\n\n当 `Scheduler latch wait duration` 的值特别高时，说明大量时间消耗在等待锁的请求上。如果不存在底层写入慢的问题，基本上可以判断该段时间内冲突比较多。\n\n## 更多阅读\n\n- [Percolator 和 TiDB 事务算法](https://pingcap.com/blog-cn/percolator-and-txn/)\n"
        },
        {
          "name": "optimizer-fix-controls.md",
          "type": "blob",
          "size": 7.935546875,
          "content": "---\ntitle: Optimizer Fix Controls\nsummary: 了解 Optimizer Fix Controls 以及如何使用 `tidb_opt_fix_control` 细粒度地控制 TiDB 优化器的行为。\n---\n\n# Optimizer Fix Controls\n\n随着产品迭代演进，TiDB 优化器的行为会发生变化，进而生成更加合理的执行计划。但在某些特定场景下，新的行为可能会导致非预期结果。例如：\n\n- 部分行为的效果和场景相关。有的行为改变，能在大多数场景下带来改进，但可能在极少数场景下导致回退。\n- 有时，行为细节的变化和其导致的结果之间的关系十分复杂。即使是对某处行为细节的改进，也可能在整体上导致执行计划回退。\n\n因此，TiDB 提供了 Optimizer Fix Controls 功能，允许用户通过设置一系列 Fix 控制 TiDB 优化器的行为细节。本文档介绍了 Optimizer Fix Controls 及其使用方法，并列举了当前 TiDB 支持调整的所有 Fix。\n\n## `tidb_opt_fix_control` 介绍\n\n从 TiDB v6.5.3 和 v7.1.0 开始，提供了 [`tidb_opt_fix_control`](/system-variables.md#tidb_opt_fix_control-从-v653-和-v710-版本开始引入) 系统变量来更细粒度地控制优化器的行为。\n\n一个 Fix 是用于调整 TiDB 优化器中一处行为的控制项。它以一个数字编号表示，该数字编号对应一个 GitHub Issue，在 Issue 中会有对技术细节的描述。例如 Fix `44262` 对应 [Issue 44262](https://github.com/pingcap/tidb/issues/44262)。\n\n`tidb_opt_fix_control` 支持设置多个 Fix，不同 Fix 之间使用逗号 (`,`) 分隔。格式形如 `\"<#issue1>:<value1>,<#issue2>:<value2>,...,<#issueN>:<valueN>\"`，其中 `<#issueN>` 代表 Fix 编号。例如：\n\n```sql\nSET SESSION tidb_opt_fix_control = '44262:ON,44389:ON';\n```\n\n## Optimizer Fix Controls 参考\n\n### [`33031`](https://github.com/pingcap/tidb/issues/33031) <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`\n- 是否允许对分区表进行计划缓存。如果设置为 `ON`，则 [Prepared 语句计划缓存](/sql-prepared-plan-cache.md)和[非 Prepared 语句计划缓存](/sql-non-prepared-plan-cache.md)都不会对[分区表](/partitioned-table.md)启用。\n\n### [`44262`](https://github.com/pingcap/tidb/issues/44262) <span class=\"version-mark\">从 v6.5.3 和 v7.2.0 版本开始引入</span>\n\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`\n- 在分区表缺少[全局统计信息](/statistics.md#收集动态裁剪模式下的分区表统计信息)的情况下，是否允许使用[动态裁剪模式](/partitioned-table.md#动态裁剪模式)访问该表。\n\n### [`44389`](https://github.com/pingcap/tidb/issues/44389) <span class=\"version-mark\">从 v6.5.3 和 v7.2.0 版本开始引入</span>\n\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`\n- 对形如 `c = 10 and (a = 'xx' or (a = 'kk' and b = 1))` 的过滤条件，是否尝试为 `IndexRangeScan` 更加完整地构造扫描范围，即 `range`。\n\n### [`44823`](https://github.com/pingcap/tidb/issues/44823) <span class=\"version-mark\">从 v7.3.0 版本开始引入</span>\n\n- 默认值：`200`\n- 可选值：`[0, 2147483647]`\n- 为了节省内存，对于参数个数超过此开关指定个数的查询，Plan Cache 将不会缓存。`0` 表示无限制。\n\n### [`44830`](https://github.com/pingcap/tidb/issues/44830) <span class=\"version-mark\">从 v6.5.7 和 v7.3.0 版本开始引入</span>\n\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`\n- 此开关控制是否让 Plan Cache 对在物理优化阶段形成的 `PointGet` 计划进行缓存。\n\n### [`44855`](https://github.com/pingcap/tidb/issues/44855) <span class=\"version-mark\">从 v6.5.4 和 v7.3.0 版本开始引入</span>\n\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`\n- 在某些场景下，当 `IndexJoin` 算子的 `Probe` 端包含 `Selection` 算子时，TiDB 会严重高估 `IndexScan` 的行数，导致在 `IndexJoin` 更好的时候选择了其它的执行计划。\n- TiDB 已经引入了缓解这类问题的改进逻辑。但是由于潜在的计划回退风险，该改进并没有被默认启用。\n- 此开关控制是否启用这个改进。\n\n### [`45132`](https://github.com/pingcap/tidb/issues/45132) <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 默认值：`1000`\n- 可选值：`[0, 2147483647]`\n- 此开关控制优化器进行启发式访问路径选择的阈值。当某个访问路径（如 `Index_A`）的估算行数远小于其他访问路径时（默认为 `1000` 倍），优化器会跳过代价比较直接选择 `Index_A`。\n- `0` 表示关闭此启发式访问路径选择策略。\n\n### [`45798`](https://github.com/pingcap/tidb/issues/45798) <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n- 默认值：`ON`\n- 可选值：`ON`、`OFF`\n- 此开关控制是否允许 Plan Cache 缓存访问[生成列](/generated-columns.md)的执行计划。\n\n### [`46177`](https://github.com/pingcap/tidb/issues/46177) <span class=\"version-mark\">从 v6.5.6、v7.1.3 和 v7.5.0 版本开始引入</span>\n\n- 默认值：`ON`。在 v8.5.0 之前，默认值为 `OFF`。\n- 可选值：`ON`、`OFF`\n- 此开关控制优化器在查询优化的过程中，找到非强制执行计划后，是否继续查找强制执行计划进行查询优化。\n\n### [`47400`](https://github.com/pingcap/tidb/issues/47400) <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n- 默认值：`ON`\n- 可选值：`ON`、`OFF`\n- 由于查询计划中每个步骤符合条件的行数难以精确估算，优化器有可能会为 `estRows` 估算出一个较小的值。此开关控制是否限制 `estRows` 的最小值。\n- `ON`：将 `estRows` 的最小值限制为 1。这是 v8.4.0 中引入的新行为，与 Oracle 和 DB2 等数据库一致。\n- `OFF`：不限制 `estRows` 的最小值，与 v8.4.0 之前版本的行为保持一致。此时，`estRows` 可能为 0。\n\n### [`52592`](https://github.com/pingcap/tidb/issues/52592) <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`\n- 此开关控制是否禁用 `Point Get` 和 `Batch Point Get` 算子执行查询。默认值 `OFF` 代表允许通过 `Point Get` 和 `Batch Point Get` 执行查询。如果设置为 `ON`，优化器会禁用 `Point Get` 和 `Batch Point Get`，强制选择 Coprocessor 执行查询。\n- `Point Get` 和 `Batch Point Get` 不支持列投影（即无法只返回部分列的数据），这意味着在某些场景中其执行效率可能低于 Coprocessor，此时设置为 `ON` 可以提高查询性能。以下是推荐设置为 `ON` 的场景：\n\n    - 查询具有多列的宽表，且仅涉及表中的少量列。\n    - 查询包含大型 JSON 值的表，且不需要检索整个 JSON 列，或仅需提取 JSON 列中的小部分数据。\n\n### [`52869`](https://github.com/pingcap/tidb/issues/52869) <span class=\"version-mark\">从 v8.1.0 版本开始引入</span>\n\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`\n- 如果查询有除了全表扫描以外的单索引扫描方式可以选择，优化器不会自动选择索引合并。详情请参考[用 EXPLAIN 查看索引合并的 SQL 执行计划](/explain-index-merge.md#示例)中的**注意**部分。\n- 打开此开关后，这个限制会被解除。解除此限制能让优化器在更多查询中自动选择索引合并，但也有可能忽略其他更好的执行计划，因此建议在解除此限制前针对实际场景进行充分测试，确保不会带来性能回退。\n\n### [`54337`](https://github.com/pingcap/tidb/issues/54337) <span class=\"version-mark\">从 v8.2.0 版本开始引入</span>\n\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`\n- 目前，TiDB 优化器在处理每个子句包含范围列表的复杂连接条件时，推导索引范围存在一定限制。此问题可以通过应用通用范围交集来解决。\n- 打开此开关后，这个限制会被解除。解除此限制能让优化器处理复杂范围交集。然而，对于子句数量较多（超过 10 个）的条件，可能会有略微增加优化时间的风险。"
        },
        {
          "name": "optimizer-hints.md",
          "type": "blob",
          "size": 67.69140625,
          "content": "---\ntitle: Optimizer Hints\nsummary: 介绍 TiDB 中 Optimizer Hints 的语法和不同生效范围的 Hint 的使用方法。\naliases: ['/docs-cn/dev/optimizer-hints/','/docs-cn/dev/reference/performance/optimizer-hints/']\n---\n\n# Optimizer Hints\n\nTiDB 支持 Optimizer Hints 语法，它基于 MySQL 5.7 中介绍的类似 comment 的语法，例如 `/*+ HINT_NAME(t1, t2) */`。当 TiDB 优化器选择的不是最优查询计划时，建议使用 Optimizer Hints。\n\n如果遇到 Hint 无法生效的情况，请参考[常见 Hint 不生效问题排查](#常见-hint-不生效问题排查)。\n\n## 语法\n\nOptimizer Hints 不区分大小写，通过 `/*+ ... */` 注释的形式跟在 `SELECT`、`INSERT`、`UPDATE` 或 `DELETE` 关键字的后面。\n\n多个不同的 Hint 之间需用逗号隔开，例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ USE_INDEX(t1, idx1), HASH_AGG(), HASH_JOIN(t1) */ count(*) FROM t t1, t t2 WHERE t1.a = t2.b;\n```\n\n可以通过 [`Explain`](/sql-statements/sql-statement-explain.md)/[`Explain Analyze`](/sql-statements/sql-statement-explain-analyze.md) 语句的输出，来查看 Optimizer Hints 对查询执行计划的影响。\n\n如果 Optimizer Hints 包含语法错误或不完整，查询语句不会报错，而是按照没有 Optimizer Hints 的情况执行。如果 Hint 不适用于当前语句，TiDB 会返回 Warning，用户可以在查询结束后通过 `Show Warnings` 命令查看具体信息。\n\n> **注意：**\n>\n> 如果注释不是跟在指定的关键字后，会被当作是普通的 MySQL comment，注释不会生效，且不会上报 warning。\n\nTiDB 目前支持的 Optimizer Hints 根据生效范围的不同可以划分为两类：第一类是在查询块范围生效的 Hint，例如 [`/*+ HASH_AGG() */`](#hash_agg)；第二类是在整个查询范围生效的 Hint，例如 [`/*+ MEMORY_QUOTA(1024 MB)*/`](#memory_quotan)。\n\n每条语句中每一个查询和子查询都对应着一个不同的查询块，每个查询块有自己对应的名字。以下面这条语句为例：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM (SELECT * FROM t) t1, (SELECT * FROM t) t2;\n```\n\n该查询语句有 3 个查询块，最外面一层 `SELECT` 所在的查询块的名字为 `sel_1`，两个 `SELECT` 子查询的名字依次为 `sel_2` 和 `sel_3`。其中数字序号根据 `SELECT` 出现的位置从左到右计数。如果分别用 `DELETE` 和 `UPDATE` 查询替代第一个 `SELECT` 查询，则对应的查询块名字分别为 `del_1` 和 `upd_1`。\n\n## 查询块范围生效的 Hint\n\n这类 Hint 可以跟在查询语句中**任意** `SELECT`、`UPDATE` 或 `DELETE` 关键字的后面。通过在 Hint 中使用查询块名字可以控制 Hint 的生效范围，以及准确标识查询中的每一个表（有可能表的名字或者别名相同），方便明确 Hint 的参数指向。若不显式地在 Hint 中指定查询块，Hint 默认作用于当前查询块。以如下查询为例：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ HASH_JOIN(@sel_1 t1@sel_1, t3) */ * FROM (SELECT t1.a, t1.b FROM t t1, t t2 WHERE t1.a = t2.a) t1, t t3 WHERE t1.b = t3.b;\n```\n\n该 Hint 在 `sel_1` 这个查询块中生效，参数分别为 `sel_1` 中的 `t1` 表（`sel_2` 中也有一个 `t1` 表）和 `t3` 表。\n\n如上例所述，在 Hint 中使用查询块名字的方式有两种：第一种是作为 Hint 的第一个参数，与其他参数用空格隔开。除 `QB_NAME` 外，本节所列的所有 Hint 除自身明确列出的参数外都有一个隐藏的可选参数 `@QB_NAME`，通过使用这个参数可以指定该 Hint 的生效范围；第二种在 Hint 中使用查询块名字的方式是在参数中的某一个表名后面加 `@QB_NAME`，用以明确指出该参数是哪个查询块中的表。\n\n> **注意：**\n>\n> Hint 声明的位置必须在指定生效的查询块之中或之前，不能是在之后的查询块中，否则无法生效。\n\n### QB_NAME\n\n当查询语句是包含多层嵌套子查询的复杂语句时，识别某个查询块的序号和名字很可能会出错，Hint `QB_NAME` 可以方便我们使用查询块。`QB_NAME` 是 Query Block Name 的缩写，用于为某个查询块指定新的名字，同时查询块原本默认的名字依然有效。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ QB_NAME(QB1) */ * FROM (SELECT * FROM t) t1, (SELECT * FROM t) t2;\n```\n\n这条 Hint 将最外层 `SELECT` 查询块的命名为 `QB1`，此时 `QB1` 和默认名称 `sel_1` 对于这个查询块来说都是有效的。\n\n> **注意：**\n>\n> 上述例子中，如果指定的 `QB_NAME` 为 `sel_2`，并且不给原本 `sel_2` 对应的第二个查询块指定新的 `QB_NAME`，则第二个查询块的默认名字 `sel_2` 会失效。\n\n### MERGE_JOIN(t1_name [, tl_name ...])\n\n`MERGE_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表使用 Sort Merge Join 算法。这个算法通常会占用更少的内存，但执行时间会更久。当数据量太大，或系统内存不足时，建议尝试使用。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ MERGE_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n> **注意：**\n>\n> `MERGE_JOIN` 的别名是 `TIDB_SMJ`，在 3.0.x 及之前版本仅支持使用该别名；之后的版本同时支持使用这两种名称，但推荐使用 `MERGE_JOIN`。\n\n### NO_MERGE_JOIN(t1_name [, tl_name ...])\n\n`NO_MERGE_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表不要使用 Sort Merge Join 算法。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ NO_MERGE_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n### INL_JOIN(t1_name [, tl_name ...])\n\n> **注意：**\n>\n> 部分情况下 `INL_JOIN` Hint 可能无法生效，详情请参阅 [`INL_JOIN` Hint 不生效](#inl_join-hint-不生效)。\n\n`INL_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表使用 Index Nested Loop Join 算法。这个算法可能会在某些场景更快，消耗更少系统资源，有的场景会更慢，消耗更多系统资源。对于外表经过 WHERE 条件过滤后结果集较小（小于 1 万行）的场景，可以尝试使用。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ INL_JOIN(t1, t2) */ * FROM t1, t2, t3 WHERE t1.id = t2.id AND t2.id = t3.id;\n```\n\n在上面的 SQL 中，`INL_JOIN(t1, t2)` 会提示优化器对 `t1` 和 `t2` 使用 Index Nested Loop Join 算法。注意它并不是指 `t1` 和 `t2` 之间使用 Index Nested Loop Join 算法，而是 `t1` 和 `t2` 分别与其他表 (`t3`) 之间使用 Index Nested Loop Join 算法。\n\n`INL_JOIN()` 中的参数是建立查询计划时内表的候选表，比如 `INL_JOIN(t1)` 只会考虑使用 `t1` 作为内表构建查询计划。表如果指定了别名，就只能使用表的别名作为 `INL_JOIN()` 的参数；如果没有指定别名，则用表的本名作为其参数。比如在 `SELECT /*+ INL_JOIN(t1) */ * FROM t t1, t t2 WHERE t1.a = t2.b;` 中，`INL_JOIN()` 的参数只能使用 `t` 的别名 `t1` 或 `t2`，不能用 `t`。\n\n> **注意：**\n>\n> `INL_JOIN` 的别名是 `TIDB_INLJ`，在 3.0.x 及之前版本仅支持使用该别名；之后的版本同时支持使用这两种名称，但推荐使用 `INL_JOIN`。\n\n### NO_INDEX_JOIN(t1_name [, tl_name ...])\n\n`NO_INDEX_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表不要使用 Index Nested Loop Join 算法。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ NO_INDEX_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n### INL_HASH_JOIN\n\n`INL_HASH_JOIN(t1_name [, tl_name])` 提示优化器使用 Index Nested Loop Hash Join 算法。该算法与 Index Nested Loop Join 使用条件完全一样，两者的区别是 `INL_JOIN` 会在连接的内表上建哈希表，而 `INL_HASH_JOIN` 会在连接的外表上建哈希表，后者对于内存的使用是有固定上限的，而前者使用的内存使用取决于内表匹配到的行数。\n\n### NO_INDEX_HASH_JOIN(t1_name [, tl_name ...])\n\n`NO_INDEX_HASH_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表不要使用 Index Nested Loop Hash Join 算法。\n\n### INL_MERGE_JOIN\n\n`INL_MERGE_JOIN(t1_name [, tl_name])` 提示优化器使用 Index Nested Loop Merge Join 算法，该算法与 Index Nested Loop Join 使用条件完全一样。\n\n### NO_INDEX_MERGE_JOIN(t1_name [, tl_name ...])\n\n`NO_INDEX_MERGE_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表不要使用 Index Nested Loop Merge Join 算法。\n\n### HASH_JOIN(t1_name [, tl_name ...])\n\n`HASH_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表使用 Hash Join 算法。这个算法多线程并发执行，执行速度较快，但会消耗较多内存。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ HASH_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n> **注意：**\n>\n> `HASH_JOIN` 的别名是 `TIDB_HJ`，在 3.0.x 及之前版本仅支持使用该别名；之后的版本同时支持使用这两种名称，推荐使用 `HASH_JOIN`。\n\n### NO_HASH_JOIN(t1_name [, tl_name ...])\n\n`NO_HASH_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表不要使用 Hash Join 算法。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ NO_HASH_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n### HASH_JOIN_BUILD(t1_name [, tl_name ...])\n\n`HASH_JOIN_BUILD(t1_name [, tl_name ...])` 提示优化器对指定表使用 Hash Join 算法，同时将指定表作为 Hash Join 算法的 Build 端，即用指定表来构建哈希表。例如：\n\n```sql\nSELECT /*+ HASH_JOIN_BUILD(t1) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n### HASH_JOIN_PROBE(t1_name [, tl_name ...])\n\n`HASH_JOIN_PROBE(t1_name [, tl_name ...])` 提示优化器对指定表使用 Hash Join 算法，同时将指定表作为 Hash Join 算法的探测（Probe）端，即用指定表作为探测端来执行 Hash Join 算法。例如：\n\n```sql\nSELECT /*+ HASH_JOIN_PROBE(t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n### SEMI_JOIN_REWRITE()\n\n`SEMI_JOIN_REWRITE()` 提示优化器将查询语句中的半连接 (Semi Join) 改写为普通的内连接。目前该 Hint 只作用于 `EXISTS` 子查询。\n\n如果不使用该 Hint 进行改写，Semi Join 在选择 Hash Join 的执行方式时，只能够使用子查询构建哈希表，因此在子查询比外查询结果集大时，执行速度可能会不及预期。Semi Join 在选择 Index Join 的执行方式时，只能够使用外查询作为驱动表，因此在子查询比外查询结果集小时，执行速度可能会不及预期。\n\n在使用了 `SEMI_JOIN_REWRITE()` 进行改写后，优化器便可以扩大选择范围，选择更好的执行方式。\n\n{{< copyable \"sql\" >}}\n\n```sql\n-- 不使用 SEMI_JOIN_REWRITE() 进行改写\nEXPLAIN SELECT * FROM t WHERE EXISTS (SELECT 1 FROM t1 WHERE t1.a = t.a);\n```\n\n```sql\n+-----------------------------+---------+-----------+------------------------+---------------------------------------------------+\n| id                          | estRows | task      | access object          | operator info                                     |\n+-----------------------------+---------+-----------+------------------------+---------------------------------------------------+\n| MergeJoin_9                 | 7992.00 | root      |                        | semi join, left key:test.t.a, right key:test.t1.a |\n| ├─IndexReader_25(Build)     | 9990.00 | root      |                        | index:IndexFullScan_24                            |\n| │ └─IndexFullScan_24        | 9990.00 | cop[tikv] | table:t1, index:idx(a) | keep order:true, stats:pseudo                     |\n| └─IndexReader_23(Probe)     | 9990.00 | root      |                        | index:IndexFullScan_22                            |\n|   └─IndexFullScan_22        | 9990.00 | cop[tikv] | table:t, index:idx(a)  | keep order:true, stats:pseudo                     |\n+-----------------------------+---------+-----------+------------------------+---------------------------------------------------+\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\n-- 使用 SEMI_JOIN_REWRITE() 进行改写\nEXPLAIN SELECT * FROM t WHERE EXISTS (SELECT /*+ SEMI_JOIN_REWRITE() */ 1 FROM t1 WHERE t1.a = t.a);\n```\n\n```sql\n+------------------------------+---------+-----------+------------------------+---------------------------------------------------------------------------------------------------------------+\n| id                           | estRows | task      | access object          | operator info                                                                                                 |\n+------------------------------+---------+-----------+------------------------+---------------------------------------------------------------------------------------------------------------+\n| IndexJoin_16                 | 1.25    | root      |                        | inner join, inner:IndexReader_15, outer key:test.t1.a, inner key:test.t.a, equal cond:eq(test.t1.a, test.t.a) |\n| ├─StreamAgg_39(Build)        | 1.00    | root      |                        | group by:test.t1.a, funcs:firstrow(test.t1.a)->test.t1.a                                                      |\n| │ └─IndexReader_34           | 1.00    | root      |                        | index:IndexFullScan_33                                                                                        |\n| │   └─IndexFullScan_33       | 1.00    | cop[tikv] | table:t1, index:idx(a) | keep order:true                                                                                               |\n| └─IndexReader_15(Probe)      | 1.25    | root      |                        | index:Selection_14                                                                                            |\n|   └─Selection_14             | 1.25    | cop[tikv] |                        | not(isnull(test.t.a))                                                                                         |\n|     └─IndexRangeScan_13      | 1.25    | cop[tikv] | table:t, index:idx(a)  | range: decided by [eq(test.t.a, test.t1.a)], keep order:false, stats:pseudo                                   |\n+------------------------------+---------+-----------+------------------------+---------------------------------------------------------------------------------------------------------------+\n```\n\n在上述例子中可以看到，在使用了 Hint 之后，TiDB 可以选择由表 `t1` 作为驱动表的 IndexJoin 的执行方式。\n\n### SHUFFLE_JOIN(t1_name [, tl_name ...])\n\n`SHUFFLE_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表使用 Shuffle Join 算法，该 Hint 只在 MPP 模式下生效。例如：\n\n```sql\nSELECT /*+ SHUFFLE_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n> **注意：**\n>\n> - 使用该 Hint 前，需要保证当前 TiDB 集群能够支持在查询中使用 TiFlash MPP 模式，具体细节见文档[使用 TiFlash MPP 模式](/tiflash/use-tiflash-mpp-mode.md)。\n> - 该 Hint 能与 [`HASH_JOIN_BUILD` Hint](#hash_join_buildt1_name--tl_name-) 和 [`HASH_JOIN_PROBE` Hint](#hash_join_probet1_name--tl_name-) 组合使用，达到控制 Shuffle Join 算法的 Build 端和 Probe 端的作用。\n\n### BROADCAST_JOIN(t1_name [, tl_name ...])\n\n`BROADCAST_JOIN(t1_name [, tl_name ...])` 提示优化器对指定表使用 Broadcast Join 算法，该 Hint 只在 MPP 模式下生效。例如：\n\n```sql\nSELECT /*+ BROADCAST_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n> **注意：**\n>\n> - 使用该 Hint 前，需要保证当前 TiDB 集群能够支持在查询中使用 TiFlash MPP 模式，具体细节见文档[使用 TiFlash MPP 模式](/tiflash/use-tiflash-mpp-mode.md)。\n> - 该 Hint 能与 [`HASH_JOIN_BUILD` Hint](#hash_join_buildt1_name--tl_name-) 和 [`HASH_JOIN_PROBE` Hint](#hash_join_probet1_name--tl_name-) 组合使用，达到控制 Broadcast Join 算法的 Build 端和 Probe 端的作用。\n\n### NO_DECORRELATE()\n\n`NO_DECORRELATE()` 提示优化器不要尝试解除指定查询块中对应子查询的关联。该 Hint 适用于包含关联列的 `EXISTS`、`IN`、`ANY`、`ALL`、`SOME` 和标量子查询，即关联子查询。\n\n将该 Hint 写在一个查询块中后，对于该子查询和其外部查询块之间的关联列，优化器将不再尝试解除关联，而是始终使用 Apply 算子来执行查询。\n\n默认情况下，TiDB 会尝试对关联子查询[解除关联](/correlated-subquery-optimization.md)，以达到更高的执行效率。但是在[一部分场景](/correlated-subquery-optimization.md#限制)下，解除关联反而会降低执行效率。这种情况下，可以使用该 Hint 来人工提示优化器不要进行解除关联操作。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t1(a int, b int);\ncreate table t2(a int, b int, index idx(b));\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\n-- 不使用 NO_DECORRELATE()\nexplain select * from t1 where t1.a < (select sum(t2.a) from t2 where t2.b = t1.b);\n```\n\n```sql\n+----------------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+\n| id                               | estRows  | task      | access object | operator info                                                                                                |\n+----------------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+\n| HashJoin_11                      | 9990.00  | root      |               | inner join, equal:[eq(test.t1.b, test.t2.b)], other cond:lt(cast(test.t1.a, decimal(10,0) BINARY), Column#7) |\n| ├─HashAgg_23(Build)              | 7992.00  | root      |               | group by:test.t2.b, funcs:sum(Column#8)->Column#7, funcs:firstrow(test.t2.b)->test.t2.b                      |\n| │ └─TableReader_24               | 7992.00  | root      |               | data:HashAgg_16                                                                                              |\n| │   └─HashAgg_16                 | 7992.00  | cop[tikv] |               | group by:test.t2.b, funcs:sum(test.t2.a)->Column#8                                                           |\n| │     └─Selection_22             | 9990.00  | cop[tikv] |               | not(isnull(test.t2.b))                                                                                       |\n| │       └─TableFullScan_21       | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo                                                                               |\n| └─TableReader_15(Probe)          | 9990.00  | root      |               | data:Selection_14                                                                                            |\n|   └─Selection_14                 | 9990.00  | cop[tikv] |               | not(isnull(test.t1.b))                                                                                       |\n|     └─TableFullScan_13           | 10000.00 | cop[tikv] | table:t1      | keep order:false, stats:pseudo                                                                               |\n+----------------------------------+----------+-----------+---------------+--------------------------------------------------------------------------------------------------------------+\n```\n\n从以上执行计划中可以发现，优化器自动解除了关联。解除关联之后的执行计划不包含 Apply 算子，取而代之的是子查询和外部查询块之间的 Join 运算，而原本的带有关联列的过滤条件 `t2.b = t1.b` 也变成了一个普通的 join 条件。\n\n{{< copyable \"sql\" >}}\n\n```sql\n-- 使用 NO_DECORRELATE()\nexplain select * from t1 where t1.a < (select /*+ NO_DECORRELATE() */ sum(t2.a) from t2 where t2.b = t1.b);\n```\n\n```sql\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n| id                                       | estRows   | task      | access object          | operator info                                                                        |\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n| Projection_10                            | 10000.00  | root      |                        | test.t1.a, test.t1.b                                                                 |\n| └─Apply_12                               | 10000.00  | root      |                        | CARTESIAN inner join, other cond:lt(cast(test.t1.a, decimal(10,0) BINARY), Column#7) |\n|   ├─TableReader_14(Build)                | 10000.00  | root      |                        | data:TableFullScan_13                                                                |\n|   │ └─TableFullScan_13                   | 10000.00  | cop[tikv] | table:t1               | keep order:false, stats:pseudo                                                       |\n|   └─MaxOneRow_15(Probe)                  | 10000.00  | root      |                        |                                                                                      |\n|     └─StreamAgg_20                       | 10000.00  | root      |                        | funcs:sum(Column#14)->Column#7                                                       |\n|       └─Projection_45                    | 100000.00 | root      |                        | cast(test.t2.a, decimal(10,0) BINARY)->Column#14                                     |\n|         └─IndexLookUp_44                 | 100000.00 | root      |                        |                                                                                      |\n|           ├─IndexRangeScan_42(Build)     | 100000.00 | cop[tikv] | table:t2, index:idx(b) | range: decided by [eq(test.t2.b, test.t1.b)], keep order:false, stats:pseudo         |\n|           └─TableRowIDScan_43(Probe)     | 100000.00 | cop[tikv] | table:t2               | keep order:false, stats:pseudo                                                       |\n+------------------------------------------+-----------+-----------+------------------------+--------------------------------------------------------------------------------------+\n```\n\n从以上执行计划中可以发现，优化器没有解除关联。执行计划中包含 Apply 算子，而带有关联列的条件 `t2.b = t1.b` 仍然是访问 `t2` 表时的过滤条件。\n\n### HASH_AGG()\n\n`HASH_AGG()` 提示优化器对指定查询块中所有聚合函数使用 Hash Aggregation 算法。这个算法多线程并发执行，执行速度较快，但会消耗较多内存。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ HASH_AGG() */ count(*) FROM t1, t2 WHERE t1.a > 10 GROUP BY t1.id;\n```\n\n### STREAM_AGG()\n\n`STREAM_AGG()` 提示优化器对指定查询块中所有聚合函数使用 Stream Aggregation 算法。这个算法通常会占用更少的内存，但执行时间会更久。数据量太大，或系统内存不足时，建议尝试使用。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ STREAM_AGG() */ count(*) FROM t1, t2 WHERE t1.a > 10 GROUP BY t1.id;\n```\n\n### MPP_1PHASE_AGG()\n\n`MPP_1PHASE_AGG()` 提示优化器对指定查询块中所有聚合函数使用一阶段聚合算法，该 Hint 只在 MPP 模式下生效。例如：\n\n```sql\nSELECT /*+ MPP_1PHASE_AGG() */ COUNT(*) FROM t1, t2 WHERE t1.a > 10 GROUP BY t1.id;\n```\n\n> **注意：**\n>\n> 使用该 Hint 前，需要保证当前 TiDB 集群能够支持在查询中使用 TiFlash MPP 模式，具体细节见文档[使用 TiFlash MPP 模式](/tiflash/use-tiflash-mpp-mode.md)。\n\n### MPP_2PHASE_AGG()\n\n`MPP_2PHASE_AGG()` 提示优化器对指定查询块中所有聚合函数使用二阶段聚合算法，该 Hint 只在 MPP 模式下生效。例如：\n\n```sql\nSELECT /*+ MPP_2PHASE_AGG() */ COUNT(*) FROM t1, t2 WHERE t1.a > 10 GROUP BY t1.id;\n```\n\n> **注意：**\n>\n> 使用该 Hint 前，需要保证当前 TiDB 集群能够支持在查询中使用 TiFlash MPP 模式，具体细节见文档[使用 TiFlash MPP 模式](/tiflash/use-tiflash-mpp-mode.md)。\n\n### USE_INDEX(t1_name, idx1_name [, idx2_name ...])\n\n`USE_INDEX(t1_name, idx1_name [, idx2_name ...])` 提示优化器对指定表仅使用给出的索引。\n\n下面例子的效果等价于 `SELECT * FROM t t1 use index(idx1, idx2);`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ USE_INDEX(t1, idx1, idx2) */ * FROM t1;\n```\n\n> **注意：**\n>\n> 当该 Hint 中只指定表名，不指定索引名时，表示不考虑使用任何索引，而是选择全表扫。\n\n### FORCE_INDEX(t1_name, idx1_name [, idx2_name ...])\n\n`FORCE_INDEX(t1_name, idx1_name [, idx2_name ...])` 提示优化器对指定表仅使用给出的索引。\n\n`FORCE_INDEX(t1_name, idx1_name [, idx2_name ...])` 的使用方法、作用和 `USE_INDEX(t1_name, idx1_name [, idx2_name ...])` 相同。\n\n以下四个查询语句的效果相同：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ USE_INDEX(t, idx1) */ * FROM t;\nSELECT /*+ FORCE_INDEX(t, idx1) */ * FROM t;\nSELECT * FROM t use index(idx1);\nSELECT * FROM t force index(idx1);\n```\n\n### IGNORE_INDEX(t1_name, idx1_name [, idx2_name ...])\n\n`IGNORE_INDEX(t1_name, idx1_name [, idx2_name ...])` 提示优化器对指定表忽略给出的索引。\n\n下面例子的效果等价于 `SELECT * FROM t t1 ignore index(idx1, idx2);`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ IGNORE_INDEX(t1, idx1, idx2) */ * FROM t t1;\n```\n\n### ORDER_INDEX(t1_name, idx1_name [, idx2_name ...])\n\n`ORDER_INDEX(t1_name, idx1_name [, idx2_name ...])` 提示优化器对指定表仅使用给出的索引，并且按顺序读取指定的索引。\n\n> **警告：**\n>\n> 这个 hint 有可能会导致 SQL 语句报错，建议先进行测试。如果测试时发生报错，请移除该 Hint。如果测试时运行正常，则可以继续使用。\n\n此 hint 通常应用在下面这种场景中：\n\n```sql\nCREATE TABLE t(a INT, b INT, key(a), key(b));\nEXPLAIN SELECT /*+ ORDER_INDEX(t, a) */ a FROM t ORDER BY a LIMIT 10;\n```\n\n```sql\n+----------------------------+---------+-----------+---------------------+-------------------------------+\n| id                         | estRows | task      | access object       | operator info                 |\n+----------------------------+---------+-----------+---------------------+-------------------------------+\n| Limit_10                   | 10.00   | root      |                     | offset:0, count:10            |\n| └─IndexReader_14           | 10.00   | root      |                     | index:Limit_13                |\n|   └─Limit_13               | 10.00   | cop[tikv] |                     | offset:0, count:10            |\n|     └─IndexFullScan_12     | 10.00   | cop[tikv] | table:t, index:a(a) | keep order:true, stats:pseudo |\n+----------------------------+---------+-----------+---------------------+-------------------------------+\n```\n\n优化器对该查询会生成两类计划：`Limit + IndexScan(keep order: true)` 和 `TopN + IndexScan(keep order: false)`，当使用了 `ORDER_INDEX` Hint，优化器会选择前一种按照顺序读取索引的计划。\n\n> **注意：**\n>\n> - 如果查询本身并不需要按顺序读取索引，即在不使用 Hint 的前提下，优化器在任何情况下都不会生成按顺序读取索引的计划。此时，如果指定了 `ORDER_INDEX` Hint，会出现报错 `Can't find a proper physical plan for this query`，此时应考虑移除对应的 `ORDER_INDEX` Hint。\n>\n> - 分区表上的索引无法支持按顺序读取，所以不应该对分区表及其相关的索引使用 `ORDER_INDEX` Hint。\n\n### NO_ORDER_INDEX(t1_name, idx1_name [, idx2_name ...])\n\n`NO_ORDER_INDEX(t1_name, idx1_name [, idx2_name ...])` 提示优化器对指定表仅使用给出的索引，并且不按顺序读取指定的索引。通常应用在下面这种场景中:\n\n以下示例中查询语句的效果等价于 `SELECT * FROM t t1 use index(idx1, idx2);`：\n\n```sql\nCREATE TABLE t(a INT, b INT, key(a), key(b));\nEXPLAIN SELECT /*+ NO_ORDER_INDEX(t, a) */ a FROM t ORDER BY a LIMIT 10;\n```\n\n```sql\n+----------------------------+----------+-----------+---------------------+--------------------------------+\n| id                         | estRows  | task      | access object       | operator info                  |\n+----------------------------+----------+-----------+---------------------+--------------------------------+\n| TopN_7                     | 10.00    | root      |                     | test.t.a, offset:0, count:10   |\n| └─IndexReader_14           | 10.00    | root      |                     | index:TopN_13                  |\n|   └─TopN_13                | 10.00    | cop[tikv] |                     | test.t.a, offset:0, count:10   |\n|     └─IndexFullScan_12     | 10000.00 | cop[tikv] | table:t, index:a(a) | keep order:false, stats:pseudo |\n+----------------------------+----------+-----------+---------------------+--------------------------------+\n```\n\n和 `ORDER_INDEX` Hint 的示例相同，优化器对该查询会生成两类计划：`Limit + IndexScan(keep order: true)` 和 `TopN + IndexScan(keep order: false)`，当使用了 `NO_ORDER_INDEX` Hint，优化器会选择后一种不按照顺序读取索引的计划。\n\n### AGG_TO_COP()\n\n`AGG_TO_COP()` 提示优化器将指定查询块中的聚合函数下推到 coprocessor。如果优化器没有下推某些适合下推的聚合函数，建议尝试使用。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ AGG_TO_COP() */ sum(t1.a) FROM t t1;\n```\n\n### LIMIT_TO_COP()\n\n`LIMIT_TO_COP()` 提示优化器将指定查询块中的 `Limit` 和 `TopN` 算子下推到 coprocessor。优化器没有下推 `Limit` 或者 `TopN` 算子时建议尝试使用该提示。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ LIMIT_TO_COP() */ * FROM t WHERE a = 1 AND b > 10 ORDER BY c LIMIT 1;\n```\n\n### READ_FROM_STORAGE(TIFLASH[t1_name [, tl_name ...]], TIKV[t2_name [, tl_name ...]])\n\n`READ_FROM_STORAGE(TIFLASH[t1_name [, tl_name ...]], TIKV[t2_name [, tl_name ...]])` 提示优化器从指定的存储引擎来读取指定的表，目前支持的存储引擎参数有 `TIKV` 和 `TIFLASH`。如果为表指定了别名，就只能使用表的别名作为 `READ_FROM_STORAGE()` 的参数；如果没有指定别名，则用表的本名作为其参数。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ READ_FROM_STORAGE(TIFLASH[t1], TIKV[t2]) */ t1.a FROM t t1, t t2 WHERE t1.a = t2.a;\n```\n\n### USE_INDEX_MERGE(t1_name, idx1_name [, idx2_name ...])\n\n`USE_INDEX_MERGE(t1_name, idx1_name [, idx2_name ...])` 提示优化器通过索引合并的方式来访问指定的表。索引合并分为并集型和交集型两种类型，详情参见[用 EXPLAIN 查看索引合并的 SQL 执行计划](/explain-index-merge.md)。\n\n若显式地指定索引列表，优化器会尝试在索引列表中选取索引来构建索引合并。若不指定索引列表，优化器会尝试在所有可用的索引中选取索引来构建索引合并。\n\n对于交集型索引合并，索引列表是必选参数。对于并集型索引合并，Hint 中的索引列表为可选参数。示例如下。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ USE_INDEX_MERGE(t1, idx_a, idx_b, idx_c) */ * FROM t1 WHERE t1.a > 10 OR t1.b > 10;\n```\n\n当对同一张表有多个 `USE_INDEX_MERGE` Hint 时，优化器会从这些 Hint 指定的索引列表的并集中尝试选取索引。\n\n> **注意：**\n>\n> `USE_INDEX_MERGE` 的参数是索引名，而不是列名。对于主键索引，索引名为 `primary`。\n\n### LEADING(t1_name [, tl_name ...])\n\n`LEADING(t1_name [, tl_name ...])` 提示优化器在生成多表连接的执行计划时，按照 hint 中表名出现的顺序来确定多表连接的顺序。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ LEADING(t1, t2) */ * FROM t1, t2, t3 WHERE t1.id = t2.id and t2.id = t3.id;\n```\n\n在以上多表连接查询语句中，`LEADING()` 中表出现的顺序决定了优化器将会先对表 `t1` 和 `t2` 进行连接，再将结果和表 `t3` 进行连接。该 hint 比 [`STRAIGHT_JOIN`](#straight_join) 更为通用。\n\n`LEADING` hint 在以下情况下会失效：\n\n+ 指定了多个 `LEADING` hint\n+ `LEADING` hint 中指定的表名不存在\n+ `LEADING` hint 中指定了重复的表名\n+ 优化器无法按照 `LEADING` hint 指定的顺序进行表连接\n+ 已经存在 `straight_join()` hint\n+ 查询语句中包含 outer join 且同时指定了包含笛卡尔积的情况\n\n当出现了上述失效的情况，会输出 warning 警告。\n\n```sql\n-- 指定了多个 LEADING hint\n\nSELECT /*+ LEADING(t1, t2) LEADING(t3) */ * FROM t1, t2, t3 WHERE t1.id = t2.id and t2.id = t3.id;\n\n-- 通过执行 `show warnings` 了解具体产生冲突的原因\n\nSHOW WARNINGS;\n```\n\n```sql\n+---------+------+-------------------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                                           |\n+---------+------+-------------------------------------------------------------------------------------------------------------------+\n| Warning | 1815 | We can only use one leading hint at most, when multiple leading hints are used, all leading hints will be invalid |\n+---------+------+-------------------------------------------------------------------------------------------------------------------+\n```\n\n> **注意：**\n>\n> 如果查询语句中包含了 outer join，你只能在 hint 中指定可以用于交换连接顺序的表。如果 hint 中存在不能用于交换的表，则该 hint 会失效。例如在 `SELECT * FROM t1 LEFT JOIN (t2 JOIN t3 JOIN t4) ON t1.a = t2.a;` 中，如果想要控制 `t2`、`t3`、`t4` 表的连接顺序，那么在使用 `LEADING` hint 时，hint 中不能出现 `t1` 表。\n\n### MERGE()\n\n在含有[公共表表达式](/develop/dev-guide-use-common-table-expression.md)的查询中使用 `MERGE()` hint，可关闭对当前子查询的物化过程，并将内部查询的内联展开到外部查询。该 hint 适用于非递归的公共表表达式查询，在某些场景下，使用该 hint 会比默认分配一块临时空间的语句执行效率更高。例如将外部查询的条件下推或在嵌套的 CTE 查询中：\n\n{{< copyable \"sql\" >}}\n\n```sql\n-- 使用 hint 将外部查询条件的谓词下推\nWITH CTE AS (SELECT /*+ MERGE() */ * FROM tc WHERE tc.a < 60) SELECT * FROM CTE WHERE CTE.a <18;\n\n-- 在嵌套 CTE 查询中使用该 hint 来指定将某个 CTE 内联展开到外部查询\nWITH CTE1 AS (SELECT * FROM t1), CTE2 AS (WITH CTE3 AS (SELECT /*+ MERGE() */ * FROM t2), CTE4 AS (SELECT * FROM t3) SELECT * FROM CTE3, CTE4) SELECT * FROM CTE1, CTE2;\n```\n\n> **注意：**\n>\n> `MERGE()` 只适用于简单的 CTE 查询，在以下情况下无法使用该 hint：\n>\n> - [递归的 CTE 查询](/develop/dev-guide-use-common-table-expression.md#递归的-cte)\n> - 子查询中有无法进行内联展开的部分，例如聚合算子、窗口函数以及 `DINSTINCT` 等\n>\n> 当 CTE 引用次数过多时，查询性能可能低于默认的物化方式。\n\n## 全局生效的 Hint\n\n全局生效的 Hint 和[视图](/views.md)有关，可以使查询中定义的 Hint 能够在视图内部生效。添加这类 Hint 需要两步：先用 `QB_NAME` Hint 为视图内的查询块命名，再以“视图名@查询块名”的方式加入实际需要的 Hint。\n\n### 第 1 步：使用 `QB_NAME` Hint 重命名视图内的查询块\n\n首先使用 [`QB_NAME` Hint](#qb_name) 重命名视图内部的查询块。其中针对视图的 `QB_NAME` Hint 的概念与[查询块范围生效的 `QB_NAME` Hint](#qb_name)相同，只是在语法上进行了相应的拓展。从 `QB_NAME(QB)` 拓展为 `QB_NAME(QB, 视图名@查询块名 [.视图名@查询块名 .视图名@查询块名 ...])`。\n\n> **注意：**\n>\n> `@查询块名` 与后面紧跟的 `.视图名@查询块名` 部分之间需要有一个空格，否则 `.视图名@查询块名` 会被视作前面 `@查询块名` 的一部分。例如，`QB_NAME(v2_1, v2@SEL_1 .@SEL_1)` 不能写为 `QB_NAME(v2_1, v2@SEL_1.@SEL_1)`。\n\n- 对于单个视图、不包含子查询的简单语句，下面以重命名视图 `v` 的第一个查询块为例：\n\n    ```sql\n    SELECT /* 注释：当前查询块的名字为默认的 @SEL_1 */ * FROM v;\n    ```\n\n    对于视图 `v` 来说，从查询语句开始的首个视图是 `v@SEL_1`。视图 `v` 的第一个查询块可以声明为 `QB_NAME(v_1, v@SEL_1 .@SEL_1)`，也可以省略 `@SEL_1` 简写成 `QB_NAME(v_1, v)`：\n\n    ```sql\n    CREATE VIEW v AS SELECT /* 注释：当前查询块的名字为默认的 @SEL_1 */ * FROM t;\n\n    -- 使用全局生效的 Hint\n    SELECT /*+ QB_NAME(v_1, v) USE_INDEX(t@v_1, idx) */ * FROM v;\n    ```\n\n- 对于嵌套视图和包含子查询的复杂语句，下面以重命名视图 `v1`、`v2` 的两个查询块为例：\n\n    ```sql\n    SELECT /* 注释：当前查询块的名字为默认的 @SEL_1 */ * FROM v2 JOIN (\n        SELECT /* 注释：当前查询块的名字为默认的 @SEL_2 */ * FROM v2) vv;\n    ```\n\n    对于第一个视图 `v2` 来说，从上面的语句开始的首个视图是 `v2@SEL_1`。对于第二个视图 `v2` 来说，首个视图表为 `v2@SEL_2`。下面的查询部分仅考虑第一个视图 `v2`。\n\n    视图 `v2` 的第一个查询块可以声明为 `QB_NAME(v2_1, v2@SEL_1 .@SEL_1)`，视图 `v2` 的第二个查询块可以声明为 `QB_NAME(v2_2, v2@SEL_1 .@SEL_2)`：\n\n    ```sql\n    CREATE VIEW v2 AS\n        SELECT * FROM t JOIN /* 注释：对于视图 v2 来说，当前查询块的名字为默认的 @SEL_1，因此当前查询块的视图列表是 v2@SEL_1 .@SEL_1 */\n        (\n            SELECT COUNT(*) FROM t1 JOIN v1 /* 注释：对于视图 v2 来说，当前查询块的名字为默认的 @SEL_2，因此当前查询块的视图列表是 v2@SEL_1 .@SEL_2 */\n        ) tt;\n    ```\n\n    对于视图 `v1` 来说，从上面的语句开始的首个视图是 `v2@SEL_1 .v1@SEL_2`。视图 `v1` 的第一个查询块可以声明为 `QB_NAME(v1_1, v2@SEL_1 .v1@SEL_2 .@SEL_1)`，视图 `v1` 的第二个查询块可以声明为 `QB_NAME(v1_2, v2@SEL_1 .v1@SEL_2 .@SEL_2)`：\n\n    ```sql\n    CREATE VIEW v1 AS SELECT * FROM t JOIN /* 注释：对于视图 v1 来说，当前查询块的名字为默认的 @SEL_1，因此当前查询块的视图列表是 v2@SEL_1 .v1@SEL_2 .@SEL_1 */\n        (\n            SELECT COUNT(*) FROM t1 JOIN t2 /* 注释：对于视图 v1 来说，当前查询块的名字为默认的 @SEL_2，因此当前查询块的视图列表是 v2@SEL_1 .v1@SEL_2 .@SEL_2 */\n        ) tt;\n    ```\n\n> **注意：**\n>\n> - 与视图相关的全局生效的 Hint 必须先定义了对应的 `QB_NAME` Hint 才能使用。\n>\n> - 使用一个 Hint 来指定视图内的多个表名时，需要保证在同一个 Hint 中出现的表名处于同一个视图的同一个查询块中。\n>\n> - 对于最外层的查询来说，在定义和视图相关的 `QB_NAME` Hint 时：\n>\n>     - 对于 `QB_NAME` Hint 中视图列表序列的第一项，在不显式声明 `@SEL_` 时，默认和定义 `QB_NAME` Hint 的查询块位置保持一致，即省略 `@SEL_` 的查询 `SELECT /*+ QB_NAME(qb1, v2) */ * FROM v2 JOIN (SELECT /*+ QB_NAME(qb2, v2) */ * FROM v2) vv;` 相当于 `SELECT /*+ QB_NAME(qb1, v2@SEL_1) */ * FROM v2 JOIN (SELECT /*+ QB_NAME(qb2, v2@SEL_2) */ * FROM v2) vv;`。\n>     - 对于 `QB_NAME` Hint 中视图列表序列第一项之外的其他部分，只有 `@SEL_1` 可省略。即，如果声明处于当前部分的第一个查询块中，则 `@SEL_1` 可以省略，否则，不能省略 `@SEL_`。对于上面的例子：\n>         - 视图 `v2` 的第一个查询块可以声明为 `QB_NAME(v2_1, v2)`\n>         - 视图 `v2` 的第二个查询块可以声明为 `QB_NAME(v2_2, v2.@SEL_2)`\n>         - 视图 `v1` 的第一个查询块可以声明为 `QB_NAME(v1_1, v2.v1@SEL_2)`\n>         - 视图 `v1` 的第二个查询块可以声明为 `QB_NAME(v1_2, v2.v1@SEL_2 .@SEL_2)`\n\n### 第 2 步：添加实际需要的 Hint\n\n在定义好视图查询块部分的 `QB_NAME` Hint 后，你可以通过查询块的名字使用[查询块范围生效的 Hint](#查询块范围生效的-hint)，以“视图名@查询块名”的方式加入实际需要的 Hint，使其在视图内部生效。例如：\n\n- 指定视图 `v2` 中第一个查询块的 `MERGE_JOIN()` Hint：\n\n    ```sql\n    SELECT /*+ QB_NAME(v2_1, v2) merge_join(t@v2_1) */ * FROM v2;\n    ```\n\n- 指定视图 `v2` 中第二个查询块的 `MERGE_JOIN()` 和 `STREAM_AGG()` Hint：\n\n    ```sql\n    SELECT /*+ QB_NAME(v2_2, v2.@SEL_2) merge_join(t1@v2_2) stream_agg(@v2_2) */ * FROM v2;\n    ```\n\n- 指定视图 `v1` 中第一个查询块的 `HASH_JOIN()` Hint：\n\n    ```sql\n    SELECT /*+ QB_NAME(v1_1, v2.v1@SEL_2) hash_join(t@v1_1) */ * FROM v2;\n    ```\n\n- 指定视图 `v1` 中第二个查询块的 `HASH_JOIN()` 和 `HASH_AGG()` Hint：\n\n    ```sql\n    SELECT /*+ QB_NAME(v1_2, v2.v1@SEL_2 .@SEL_2) hash_join(t1@v1_2) hash_agg(@v1_2) */ * FROM v2;\n    ```\n\n## 查询范围生效的 Hint\n\n这类 Hint 只能跟在语句中**第一个** `SELECT`、`UPDATE` 或 `DELETE` 关键字的后面，等同于在当前这条查询运行时对指定的系统变量进行修改，其优先级高于现有系统变量的值。\n\n> **注意：**\n>\n> 这类 Hint 虽然也有隐藏的可选变量 `@QB_NAME`，但就算指定了该值，Hint 还是会在整个查询范围生效。\n\n### NO_INDEX_MERGE()\n\n`NO_INDEX_MERGE()` 会关闭优化器的 index merge 功能。\n\n下面的例子不会使用 index merge：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ NO_INDEX_MERGE() */ * FROM t WHERE t.a > 0 or t.b > 0;\n```\n\n除了 Hint 外，系统变量 `tidb_enable_index_merge` 也能决定是否开启该功能。\n\n> **注意：**\n>\n> - `NO_INDEX_MERGE` 优先级高于 `USE_INDEX_MERGE`，当这两类 Hint 同时存在时，`USE_INDEX_MERGE` 不会生效。\n> - 当存在子查询时，`NO_INDEX_MERGE` 放在最外层才能生效。\n\n### USE_TOJA(boolean_value)\n\n参数 `boolean_value` 可以是 `TRUE` 或者 `FALSE`。`USE_TOJA(TRUE)` 会开启优化器尝试将 in (subquery) 条件转换为 join 和 aggregation 的功能。相对地，`USE_TOJA(FALSE)` 会关闭该功能。\n\n下面的例子会将 `in (SELECT t2.a FROM t2) subq` 转换为等价的 join 和 aggregation：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ USE_TOJA(TRUE) */ t1.a, t1.b FROM t1 WHERE t1.a in (SELECT t2.a FROM t2) subq;\n```\n\n除了 Hint 外，系统变量 `tidb_opt_insubq_to_join_and_agg` 也能决定是否开启该功能。\n\n### MAX_EXECUTION_TIME(N)\n\n`MAX_EXECUTION_TIME(N)` 把语句的执行时间限制在 `N` 毫秒以内，超时后服务器会终止这条语句的执行。\n\n下面的 Hint 设置了 1000 毫秒（即 1 秒）超时：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ MAX_EXECUTION_TIME(1000) */ * FROM t1 inner join t2 WHERE t1.id = t2.id;\n```\n\n除了 Hint 之外，系统变量 `global.max_execution_time` 也能对语句执行时间进行限制。\n\n### MEMORY_QUOTA(N)\n\n`MEMORY_QUOTA(N)` 用于限制语句执行时的内存使用。该 Hint 支持 MB 和 GB 两种单位。内存使用超过该限制时会根据当前设置的内存超限行为来打出一条 log 或者终止语句的执行。\n\n下面的 Hint 设置了 1024 MB 的内存限制：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ MEMORY_QUOTA(1024 MB) */ * FROM t;\n```\n\n除了 Hint 外，系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 也能限制语句执行的内存使用。\n\n### READ_CONSISTENT_REPLICA()\n\n`READ_CONSISTENT_REPLICA()` 会开启从数据一致的 TiKV follower 节点读取数据的特性。\n\n下面的例子会从 follower 节点读取数据：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ READ_CONSISTENT_REPLICA() */ * FROM t;\n```\n\n除了 Hint 外，环境变量 `tidb_replica_read` 设为 `'follower'` 或者 `'leader'` 也能决定是否开启该特性。\n\n### IGNORE_PLAN_CACHE()\n\n`IGNORE_PLAN_CACHE()` 提示优化器在处理当前 `prepare` 语句时不使用 plan cache。\n\n该 Hint 用于在 [Prepared Plan Cache](/sql-prepared-plan-cache.md) 开启的场景下临时对某类查询禁用 plan cache。\n\n以下示例强制该 `prepare` 语句不使用 plan cache：\n\n{{< copyable \"sql\" >}}\n\n```sql\nprepare stmt FROM 'SELECT  /*+ IGNORE_PLAN_CACHE() */ * FROM t WHERE t.id = ?';\n```\n\n### SET_VAR(VAR_NAME=VAR_VALUE)\n\n`SET_VAR(VAR_NAME=VAR_VALUE)` 允许在语句执行期间以 Hint 形式临时修改会话级系统变量的值。当语句执行完成后，系统变量将在当前会话中自动恢复为原始值。通过这个 Hint 可以修改一部分与优化器、执行器相关的系统变量行为。支持通过 `SET_VAR(VAR_NAME=VAR_VALUE)` Hint 修改的系统变量请查看[系统变量](/system-variables.md)。\n\n> **警告：**\n>\n> - 强烈建议不要利用此 Hint 修改没有明确支持的变量，这可能会引发不可预知的行为。\n> - 注意不要把 `SET_VAR` 写在子查询中，否则可能会不生效。详情请参考 [`SET_VAR` 写在子查询中不生效](#set_var-写在子查询中不生效)。\n\n下面是一个使用示例：\n\n```sql\nSELECT /*+ SET_VAR(MAX_EXECUTION_TIME=1234) */ @@MAX_EXECUTION_TIME;\nSELECT @@MAX_EXECUTION_TIME;\n```\n\n执行上述 SQL，第一个查询返回的结果是 Hint 中设置的 `1234`，而不是变量 `MAX_EXECUTION_TIME` 的默认值。第二个查询会返回变量的默认值。\n\n```sql\n+----------------------+\n| @@MAX_EXECUTION_TIME |\n+----------------------+\n|                 1234 |\n+----------------------+\n1 row in set (0.00 sec)\n+----------------------+\n| @@MAX_EXECUTION_TIME |\n+----------------------+\n|                    0 |\n+----------------------+\n1 row in set (0.00 sec)\n```\n\n### STRAIGHT_JOIN()\n\n`STRAIGHT_JOIN()` 提示优化器在生成表连接顺序时按照表名在 `FROM` 子句中出现的顺序进行连接。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ STRAIGHT_JOIN() */ * FROM t t1, t t2 WHERE t1.a = t2.a;\n```\n\n> **注意：**\n>\n> - `STRAIGHT_JOIN` 优先级高于 `LEADING`，当这两类 Hint 同时存在时，`LEADING` 不会生效。\n> - 建议使用 `LEADING` Hint，它比 `STRAIGHT_JOIN` Hint 更通用。\n\n### NTH_PLAN(N)\n\n`NTH_PLAN(N)` 提示优化器选用在物理优化阶段搜索到的第 `N` 个物理计划。`N` 必须是正整数。\n\n如果指定的 `N` 超出了物理优化阶段的搜索范围，TiDB 会返回 warning，并根据不存在该 Hint 时一样的策略选择最优物理计划。\n\n该 Hint 在启用 cascades planner 的情况下不会生效。\n\n以下示例会强制优化器在物理阶段选择搜索到的第 3 个物理计划：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT /*+ NTH_PLAN(3) */ count(*) from t where a > 5;\n```\n\n> **注意：**\n>\n> `NTH_PLAN(N)` 主要用于测试用途，并且在未来不保证其兼容性，请谨慎使用。\n\n### RESOURCE_GROUP(resource_group_name)\n\n`RESOURCE_GROUP(resource_group_name)` 用于[使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)。此 Hint 将临时使用指定的资源组执行当前的语句。如果指定的资源组不存在，则该 Hint 将被忽略。\n\n示例：\n\n```sql\nSELECT /*+ RESOURCE_GROUP(rg1) */ * FROM t limit 10;\n```\n\n> **注意：**\n>\n> 自 v8.2.0 版本开始，TiDB 为此 Hint 引入权限控制。当系统变量 [`tidb_resource_control_strict_mode`](/system-variables.md#tidb_resource_control_strict_mode-从-v820-版本开始引入) 设置为 `ON` 时，你需要有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 或者 `RESOURCE_GROUP_USER` 权限才能使用此 Hint。如果没有所需权限，则此 Hint 会被忽略，同时 TiDB 会返回 warning，你可以在查询结束后通过 `SHOW WARNINGS;` 命令查看具体信息。\n\n## 常见 Hint 不生效问题排查\n\n### MySQL 命令行客户端清除 Hint 导致不生效\n\nMySQL 命令行客户端在 5.7.7 版本之前默认清除了 Optimizer Hints。如果需要在这些早期版本的客户端中使用 Hint 语法，需要在启动客户端时加上 `--comments` 选项。例如 `mysql -h 127.0.0.1 -P 4000 -uroot --comments`。\n\n### 创建连接时不指定库名导致 Hint 不生效\n\n如果创建连接时未指定数据库名，则可能出现 Hint 失效的情况。例如：\n\n使用 `mysql -h127.0.0.1 -P4000 -uroot` 命令连接数据库时，未使用 `-D` 参数指定数据库名。然后执行下面的 SQL 语句：\n\n```sql\nSELECT /*+ use_index(t, a) */ a FROM test.t;\nSHOW WARNINGS;\n```\n\n由于无法识别表 `t` 对应的数据库名，因此 `use_index(t, a)` Hint 无法生效。\n\n```sql\n+---------+------+----------------------------------------------------------------------+\n| Level   | Code | Message                                                              |\n+---------+------+----------------------------------------------------------------------+\n| Warning | 1815 | use_index(.t, a) is inapplicable, check whether the table(.t) exists |\n+---------+------+----------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n### 跨库查询不指定库名导致 Hint 不生效\n\n对于跨库查询中需要访问的表，需要显式地指定数据库名，否则可能出现 Hint 失效的情况。例如执行下面跨库查询的 SQL 语句：\n\n```sql\nUSE test1;\nCREATE TABLE t1(a INT, KEY(a));\nUSE test2;\nCREATE TABLE t2(a INT, KEY(a));\nSELECT /*+ use_index(t1, a) */ * FROM test1.t1, t2;\nSHOW WARNINGS;\n```\n\n由于 `t1` 不在当前数据库 `test2` 下，因此 `use_index(t1, a)` Hint 无法被正确地识别。\n\n```sql\n+---------+------+----------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                          |\n+---------+------+----------------------------------------------------------------------------------+\n| Warning | 1815 | use_index(test2.t1, a) is inapplicable, check whether the table(test2.t1) exists |\n+---------+------+----------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n此时，需要显式地指定库名，即将 `use_index(t1, a)` 修改为 `use_index(test1.t1, a)`。\n\n### Hint 位置不正确导致不生效\n\n如果没有按照 Optimizer Hints 语法将 Hint 正确地放在指定关键字的后面，它将无法生效。例如：\n\n```sql\nSELECT * /*+ use_index(t, a) */ FROM t;\nSHOW WARNINGS;\n```\n\nWarning 信息如下：\n\n```sql\n+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                                                                                                                                                 |\n+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Warning | 1064 | You have an error in your SQL syntax; check the manual that corresponds to your TiDB version for the right syntax to use [parser:8066]Optimizer hint can only be followed by certain keywords like SELECT, INSERT, etc. |\n+---------+------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.01 sec)\n```\n\n在上面的示例中，你需要将 Hint 直接放在 `SELECT` 关键字之后。具体的语法规则参见 [Hint 语法](#语法)部分。\n\n### `INL_JOIN` Hint 不生效\n\n#### 关联表的列上使用内置函数导致 `INL_JOIN` Hint 不生效\n\n在某些情况下，如果在关联表的列上使用了内置函数，优化器可能无法选择 `IndexJoin` 计划，导致 `INL_JOIN` Hint 也无法生效。\n\n例如，以下查询在关联表的列 `tname` 上使用了内置函数 `substr`：\n\n```sql\nCREATE TABLE t1 (id varchar(10) primary key, tname varchar(10));\nCREATE TABLE t2 (id varchar(10) primary key, tname varchar(10));\nEXPLAIN SELECT /*+ INL_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id=t2.id and SUBSTR(t1.tname,1,2)=SUBSTR(t2.tname,1,2);\n```\n\n查询计划输出结果如下：\n\n```sql\n+------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------+\n| id                           | estRows  | task      | access object | operator info                                                         |\n+------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------+\n| HashJoin_12                  | 12500.00 | root      |               | inner join, equal:[eq(test.t1.id, test.t2.id) eq(Column#5, Column#6)] |\n| ├─Projection_17(Build)       | 10000.00 | root      |               | test.t2.id, test.t2.tname, substr(test.t2.tname, 1, 2)->Column#6      |\n| │ └─TableReader_19           | 10000.00 | root      |               | data:TableFullScan_18                                                 |\n| │   └─TableFullScan_18       | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo                                        |\n| └─Projection_14(Probe)       | 10000.00 | root      |               | test.t1.id, test.t1.tname, substr(test.t1.tname, 1, 2)->Column#5      |\n|   └─TableReader_16           | 10000.00 | root      |               | data:TableFullScan_15                                                 |\n|     └─TableFullScan_15       | 10000.00 | cop[tikv] | table:t1      | keep order:false, stats:pseudo                                        |\n+------------------------------+----------+-----------+---------------+-----------------------------------------------------------------------+\n7 rows in set, 1 warning (0.01 sec)\n```\n\n```sql\nSHOW WARNINGS;\n```\n\n```\n+---------+------+------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                            |\n+---------+------+------------------------------------------------------------------------------------+\n| Warning | 1815 | Optimizer Hint /*+ INL_JOIN(t1, t2) */ or /*+ TIDB_INLJ(t1, t2) */ is inapplicable |\n+---------+------+------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n从该示例中可以看到，`INL_JOIN` Hint 没有生效。该问题的根本原因是优化器限制导致无法使用 `Projection` 或者 `Selection` 算子作为 `IndexJoin` 的探测 (Probe) 端。\n\n从 TiDB v8.0.0 起，你通过设置 [`tidb_enable_inl_join_inner_multi_pattern`](/system-variables.md#tidb_enable_inl_join_inner_multi_pattern-从-v700-版本开始引入) 为 `ON` 来避免该问题。\n\n```sql\nSET @@tidb_enable_inl_join_inner_multi_pattern=ON;\nQuery OK, 0 rows affected (0.00 sec)\n\nEXPLAIN SELECT /*+ INL_JOIN(t1, t2) */ * FROM t1, t2 WHERE t1.id=t2.id AND SUBSTR(t1.tname,1,2)=SUBSTR(t2.tname,1,2);\n+------------------------------+--------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n| id                           | estRows      | task      | access object | operator info                                                                                                                              |\n+------------------------------+--------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n| IndexJoin_18                 | 12500.00     | root      |               | inner join, inner:Projection_14, outer key:test.t1.id, inner key:test.t2.id, equal cond:eq(Column#5, Column#6), eq(test.t1.id, test.t2.id) |\n| ├─Projection_32(Build)       | 10000.00     | root      |               | test.t1.id, test.t1.tname, substr(test.t1.tname, 1, 2)->Column#5                                                                           |\n| │ └─TableReader_34           | 10000.00     | root      |               | data:TableFullScan_33                                                                                                                      |\n| │   └─TableFullScan_33       | 10000.00     | cop[tikv] | table:t1      | keep order:false, stats:pseudo                                                                                                             |\n| └─Projection_14(Probe)       | 100000000.00 | root      |               | test.t2.id, test.t2.tname, substr(test.t2.tname, 1, 2)->Column#6                                                                           |\n|   └─TableReader_13           | 10000.00     | root      |               | data:TableRangeScan_12                                                                                                                     |\n|     └─TableRangeScan_12      | 10000.00     | cop[tikv] | table:t2      | range: decided by [eq(test.t2.id, test.t1.id)], keep order:false, stats:pseudo                                                             |\n+------------------------------+--------------+-----------+---------------+--------------------------------------------------------------------------------------------------------------------------------------------+\n7 rows in set (0.00 sec)\n```\n\n#### 排序规则不兼容导致 `INL_JOIN` Hint、`INL_HASH_JOIN` Hint、`INL_MERGE_JOIN` Hint 不生效\n\n如果两个表的 Join key 的排序规则不能兼容，将无法使用 IndexJoin 来执行查询。此时 [`INL_JOIN` Hint](#inl_joint1_name--tl_name-)、[`INL_HASH_JOIN` Hint](#inl_hash_join)、[`INL_MERGE_JOIN` Hint](#inl_merge_join) 将无法生效。例如：\n\n```sql\nCREATE TABLE t1 (k varchar(8), key(k)) COLLATE=utf8mb4_general_ci;\nCREATE TABLE t2 (k varchar(8), key(k)) COLLATE=utf8mb4_bin;\nEXPLAIN SELECT /*+ tidb_inlj(t1) */ * FROM t1, t2 WHERE t1.k=t2.k;\n```\n\n查询计划输出结果如下：\n\n```sql\n+-----------------------------+----------+-----------+----------------------+----------------------------------------------+\n| id                          | estRows  | task      | access object        | operator info                                |\n+-----------------------------+----------+-----------+----------------------+----------------------------------------------+\n| HashJoin_19                 | 12487.50 | root      |                      | inner join, equal:[eq(test.t1.k, test.t2.k)] |\n| ├─IndexReader_24(Build)     | 9990.00  | root      |                      | index:IndexFullScan_23                       |\n| │ └─IndexFullScan_23        | 9990.00  | cop[tikv] | table:t2, index:k(k) | keep order:false, stats:pseudo               |\n| └─IndexReader_22(Probe)     | 9990.00  | root      |                      | index:IndexFullScan_21                       |\n|   └─IndexFullScan_21        | 9990.00  | cop[tikv] | table:t1, index:k(k) | keep order:false, stats:pseudo               |\n+-----------------------------+----------+-----------+----------------------+----------------------------------------------+\n5 rows in set, 1 warning (0.00 sec)\n```\n\n上面的 SQL 语句中 `t1.k` 和 `t2.k` 的排序规则不能相互兼容（分别为 `utf8mb4_general_ci` 和 `utf8mb4_bin`），导致 IndexJoin 无法适用。因此 `INL_JOIN` 或 `TIDB_INLJ` Hint 也无法生效。\n\n```sql\nSHOW WARNINGS;\n+---------+------+----------------------------------------------------------------------------+\n| Level   | Code | Message                                                                    |\n+---------+------+----------------------------------------------------------------------------+\n| Warning | 1815 | Optimizer Hint /*+ INL_JOIN(t1) */ or /*+ TIDB_INLJ(t1) */ is inapplicable |\n+---------+------+----------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n#### 连接顺序导致 `INL_JOIN` Hint 不生效\n\n[`INL_JOIN(t1, t2)`](#inl_joint1_name--tl_name-) 或 `TIDB_INLJ(t1, t2)` 的语义是让 `t1` 和 `t2` 作为 `IndexJoin` 的内表与其他表进行连接，而不是直接将 `t1` 和 `t2` 进行 `IndexJoin` 连接。例如：\n\n```sql\nEXPLAIN SELECT /*+ inl_join(t1, t3) */ * FROM t1, t2, t3 WHERE t1.id = t2.id AND t2.id = t3.id AND t1.id = t3.id;\n+---------------------------------+----------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id                              | estRows  | task      | access object | operator info                                                                                                                                                           |\n+---------------------------------+----------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| IndexJoin_16                    | 15625.00 | root      |               | inner join, inner:TableReader_13, outer key:test.t2.id, test.t1.id, inner key:test.t3.id, test.t3.id, equal cond:eq(test.t1.id, test.t3.id), eq(test.t2.id, test.t3.id) |\n| ├─IndexJoin_34(Build)           | 12500.00 | root      |               | inner join, inner:TableReader_31, outer key:test.t2.id, inner key:test.t1.id, equal cond:eq(test.t2.id, test.t1.id)                                                     |\n| │ ├─TableReader_40(Build)       | 10000.00 | root      |               | data:TableFullScan_39                                                                                                                                                   |\n| │ │ └─TableFullScan_39          | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo                                                                                                                                          |\n| │ └─TableReader_31(Probe)       | 10000.00 | root      |               | data:TableRangeScan_30                                                                                                                                                  |\n| │   └─TableRangeScan_30         | 10000.00 | cop[tikv] | table:t1      | range: decided by [test.t2.id], keep order:false, stats:pseudo                                                                                                          |\n| └─TableReader_13(Probe)         | 12500.00 | root      |               | data:TableRangeScan_12                                                                                                                                                  |\n|   └─TableRangeScan_12           | 12500.00 | cop[tikv] | table:t3      | range: decided by [test.t2.id test.t1.id], keep order:false, stats:pseudo                                                                                               |\n+---------------------------------+----------+-----------+---------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n```\n\n在上面例子中，`t1` 和 `t3` 并没有直接被一个 `IndexJoin` 连接起来。\n\n如果想要直接使用 `IndexJoin` 来连接 `t1` 和 `t3`，需要先使用 [`LEADING` Hint](#leadingt1_name--tl_name-) 指定 `t1` 和 `t3` 的连接顺序，然后再配合使用 `INL_JOIN`。例如：\n\n```sql\nEXPLAIN SELECT /*+ leading(t1, t3), inl_join(t3) */ * FROM t1, t2, t3 WHERE t1.id = t2.id AND t2.id = t3.id AND t1.id = t3.id;\n+---------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------+\n| id                              | estRows  | task      | access object | operator info                                                                                                       |\n+---------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------+\n| Projection_12                   | 15625.00 | root      |               | test.t1.id, test.t1.name, test.t2.id, test.t2.name, test.t3.id, test.t3.name                                        |\n| └─HashJoin_21                   | 15625.00 | root      |               | inner join, equal:[eq(test.t1.id, test.t2.id) eq(test.t3.id, test.t2.id)]                                           |\n|   ├─TableReader_36(Build)       | 10000.00 | root      |               | data:TableFullScan_35                                                                                               |\n|   │ └─TableFullScan_35          | 10000.00 | cop[tikv] | table:t2      | keep order:false, stats:pseudo                                                                                      |\n|   └─IndexJoin_28(Probe)         | 12500.00 | root      |               | inner join, inner:TableReader_25, outer key:test.t1.id, inner key:test.t3.id, equal cond:eq(test.t1.id, test.t3.id) |\n|     ├─TableReader_34(Build)     | 10000.00 | root      |               | data:TableFullScan_33                                                                                               |\n|     │ └─TableFullScan_33        | 10000.00 | cop[tikv] | table:t1      | keep order:false, stats:pseudo                                                                                      |\n|     └─TableReader_25(Probe)     | 10000.00 | root      |               | data:TableRangeScan_24                                                                                              |\n|       └─TableRangeScan_24       | 10000.00 | cop[tikv] | table:t3      | range: decided by [test.t1.id], keep order:false, stats:pseudo                                                      |\n+---------------------------------+----------+-----------+---------------+---------------------------------------------------------------------------------------------------------------------+\n9 rows in set (0.01 sec)\n```\n\n### 使用 Hint 导致错误 `Can't find a proper physical plan for this query`\n\n在下面几种情况下，可能会出现 `Can't find a proper physical plan for this query` 错误：\n\n- 查询本身并不需要按顺序读取索引，即在不使用 Hint 的前提下，优化器在任何情况下都不会生成按顺序读取索引的计划。此时，如果指定了 `ORDER_INDEX` Hint，会出现此报错，此时应考虑移除对应的 `ORDER_INDEX` Hint。\n- 查询使用了 `NO_JOIN` 相关的 Hint 排除了所有可能的 Join 方式。\n\n```sql\nCREATE TABLE t1 (a INT);\nCREATE TABLE t2 (a INT);\nEXPLAIN SELECT /*+ NO_HASH_JOIN(t1), NO_MERGE_JOIN(t1) */ * FROM t1, t2 WHERE t1.a=t2.a;\nERROR 1815 (HY000): Internal : Can't find a proper physical plan for this query\n```\n\n- 系统变量 [`tidb_opt_enable_hash_join`](/system-variables.md#tidb_opt_enable_hash_join-从-v656v712-和-v740-版本开始引入) 设置为 `OFF`，而且其他 Join 方式也都被排除了。\n\n```sql\nCREATE TABLE t1 (a INT);\nCREATE TABLE t2 (a INT);\nset tidb_opt_enable_hash_join=off;\nEXPLAIN SELECT /*+ NO_MERGE_JOIN(t1) */ * FROM t1, t2 WHERE t1.a=t2.a;\nERROR 1815 (HY000): Internal : Can't find a proper physical plan for this query\n```\n\n### `SET_VAR` 写在子查询中不生效\n\n`SET_VAR` 用来设置当前语句的系统变量，不要写在子查询中。如果写在子查询中，由于子查询会被特殊处理，可能导致 `SET_VAR` 无法生效。\n\n下面示例把 `SET_VAR` 写在了子查询中，所以没有生效。\n\n```sql\nmysql> SELECT @@MAX_EXECUTION_TIME, a FROM (SELECT /*+ SET_VAR(MAX_EXECUTION_TIME=123) */ 1 as a) t;\n+----------------------+---+\n| @@MAX_EXECUTION_TIME | a |\n+----------------------+---+\n|                    0 | 1 |\n+----------------------+---+\n1 row in set (0.00 sec)\n```\n\n下面示例没有把 SET_VAR 写在子查询中，所以可以生效。\n\n```sql\nmysql> SELECT /*+ SET_VAR(MAX_EXECUTION_TIME=123) */ @@MAX_EXECUTION_TIME, a FROM (SELECT 1 as a) t;\n+----------------------+---+\n| @@MAX_EXECUTION_TIME | a |\n+----------------------+---+\n|                  123 | 1 |\n+----------------------+---+\n1 row in set (0.00 sec)\n```\n"
        },
        {
          "name": "oracle-functions-to-tidb.md",
          "type": "blob",
          "size": 9.28125,
          "content": "---\ntitle: Oracle 与 TiDB 函数和语法差异对照\nsummary: 了解 Oracle 与 TiDB 函数和语法差异对照。\n---\n\n# Oracle 与 TiDB 函数和语法差异对照\n\n本文档提供了 Oracle 与 TiDB 的函数和语法差异对照，方便你根据 Oracle 函数查找对应的 TiDB 函数，了解 Oracle 与 TiDB 语法差异。\n\n> **注意：**\n>\n> 本文的内容是基于 Oracle 12.2.0.1.0 和 TiDB v5.4.0，其他版本可能存在差异。\n\n## 函数对照表\n\n下表列出了 Oracle 与 TiDB 部分函数的对照表。\n\n| 函数 | Oracle 语法 | TiDB 语法 | 说明 |\n|---|---|---|---|\n| 转换数据类型 | <li>`TO_NUMBER(key)`</li><li>`TO_CHAR(key)`</li> | `CONVERT(key,dataType)` | TiDB 支持转换为下面类型：`BINARY`、`CHAR`、`DATE`、`DATETIME`、`TIME`、`SIGNED INTEGER`、`UNSIGNED INTEGER` 和 `DECIMAL`。 |\n| 日期类型转换为字符串类型 | <li>`TO_CHAR(SYSDATE,'yyyy-MM-dd hh24:mi:ss')`</li> <li>`TO_CHAR(SYSDATE,'yyyy-MM-dd')`</li> | <li>`DATE_FORMAT(NOW(),'%Y-%m-%d %H:%i:%s')`</li><li>`DATE_FORMAT(NOW(),'%Y-%m-%d')`</li> | TiDB 的格式化字符串大小写敏感。 |\n| 字符串类型转换为日期类型 | <li>`TO_DATE('2021-05-28 17:31:37','yyyy-MM-dd hh24:mi:ss')`</li><li>`TO_DATE('2021-05-28','yyyy-MM-dd hh24:mi:ss')`</li> | <li>`STR_TO_DATE('2021-05-28 17:31:37','%Y-%m-%d %H:%i:%s')`</li><li>`STR_TO_DATE('2021-05-28','%Y-%m-%d%T')` </li> | TiDB 的格式化字符串大小写敏感。 |\n| 获取系统当前时间（精确到秒）| `SYSDATE` | `NOW()` | |\n| 获取当前时间（精确到微秒）| `SYSTIMESTAMP` | `CURRENT_TIMESTAMP(6)` | |\n| 获取两个日期相差的天数 | `date1 - date2` | `DATEDIFF(date1, date2)` | |\n| 获取两个日期间隔月份 | `MONTHS_BETWEEN(ENDDATE,SYSDATE)` | `TIMESTAMPDIFF(MONTH,SYSDATE,ENDDATE)` | Oracle 中 `MONTHS_BETWEEN()` 函数与 TiDB 中 `TIMESTAMPDIFF()` 函数的结果会有误差。`TIMESTAMPDIFF()` 只保留整数月。使用时需要注意，两个函数的参数位置相反。 |\n| 日期增加/减少 n 天 | `DATEVAL + n` | `DATE_ADD(dateVal,INTERVAL n DAY)` | `n` 可为负数。|\n| 日期增加/减少 n 月 | `ADD_MONTHS(dateVal,n)`| `DATE_ADD(dateVal,INTERVAL n MONTH)` | `n` 可为负数。|\n| 获取日期到日 | `TRUNC(SYSDATE)` | <li>`CAST(NOW() AS DATE)`</li><li>`DATE_FORMAT(NOW(),'%Y-%m-%d')`</li> | TiDB 中 `CAST` 与 `DATE_FORMAT` 结果一致。|\n| 获取日期当月第一天 | `TRUNC(SYSDATE,'mm')` | `DATE_ADD(CURDATE(),interval - day(CURDATE()) + 1 day)`  | |\n| 截取数据 | `TRUNC(2.136) = 2`<br/> `TRUNC(2.136,2) = 2.13` | `TRUNCATE(2.136,0) = 2`<br/> `TRUNCATE(2.136,2) = 2.13` | 数据精度保留，直接截取相应小数位，不涉及四舍五入。 |\n| 获取序列下一个值 | `sequence_name.NEXTVAL` | `NEXTVAL(sequence_name)` | |\n| 获取随机序列值 | `SYS_GUID()` | `UUID()` | TiDB 返回一个通用唯一识别码 (UUID)。|\n| 左/右外连接 | `SELECT * FROM a, b WHERE a.id = b.id(+);`<br/>`SELECT * FROM a, b WHERE a.id(+) = b.id;` | `SELECT * FROM a LEFT JOIN b ON a.id = b.id;`<br/>`SELECT * FROM a RIGHT JOIN b ON a.id = b.id;` | 关联查询时，TiDB 不支持使用 (+) 实现左/右关联，只能通过 `LEFT JOIN` 或 `RIGHT JOIN` 实现。|\n| `NVL()` | `NVL(key,val)` | `IFNULL(key,val)` | 如果该字段值为 `NULL`，则返回 val 值，否则返回该字段的值。 |\n| `NVL2()` | `NVL2(key, val1, val2)`  | `IF(key is NOT NULL, val1, val2)` | 如果该字段值非 `NULL`，则返回 val1 值，否则返回 val2 值。|\n| `DECODE()` | <li>`DECODE(key,val1,val2,val3)`</li><li>`DECODE(value,if1,val1,if2,val2,...,ifn,valn,val)`</li> | <li>`IF(key=val1,val2,val3)`</li><li>`CASE WHEN value=if1 THEN val1 WHEN value=if2 THEN val2,...,WHEN value=ifn THEN valn ELSE val END`</li> | <li>如果该字段值等于 val1，则返回 val2，否则返回 val3。</li><li>当该字段值满足条件 1 (if1) 时，返回 val1，满足条件 2 (if2) 时，返回 val2，满足条件 3 (if3) 时，返回 val3。</li> |\n| 拼接字符串 `a` 和 `b` | <code>'a' \\|\\| 'b'</code>  | `CONCAT('a','b')` | |\n| 获取字符串长度 | `LENGTH(str)` | `CHAR_LENGTH(str)` | |\n| 获取子串 | `SUBSTR('abcdefg',0,2) = 'ab'`<br/> `SUBSTR('abcdefg',1,2) = 'ab'` | `SUBSTRING('abcdefg',0,2) = ''`<br/>`SUBSTRING('abcdefg',1,2) = 'ab'` | <li>Oracle 中起始位置 0 与 1 作用一样。</li><li>TiDB 中 0 开始获取的子串为空，若需从字符串的起始位置开始，则应从 1 开始。</li> |\n| 字符串在源字符串中的位置 | `INSTR('abcdefg','b',1,1)` | `INSTR('abcdefg','b')` | 从字符串 `'abcdefg'` 第一个字符开始查询，返回 `'b'` 字符串第一次出现的位置。 |\n| 字符串在源字符串中的位置 | `INSTR('stst','s',1,2)` | `LENGTH(SUBSTRING_INDEX('stst','s',2)) + 1` | 从字符串 `'stst'` 第一个字符开始查找，返回 `'s'` 字符第二次出现的位置。 |\n| 字符串在源字符串中的位置 | `INSTR('abcabc','b',2,1)` | `LOCATE('b','abcabc',2)` | 从字符串 `'abcabc'` 第二个字符开始查询，返回 `'b'` 字符第一次出现的位置。 |\n| 列合并为行 | `LISTAGG(CONCAT(E.dimensionid,'---',E.DIMENSIONNAME),'***') within GROUP(ORDER BY  DIMENSIONNAME)` | `GROUP_CONCAT(CONCAT(E.dimensionid,'---',E.DIMENSIONNAME) ORDER BY DIMENSIONNAME SEPARATOR '***')` | 将一列字段合并为一行并根据 `***` 符号进行分割。 |\n| ASCII 值转化为对应字符 | `CHR(n)` | `CHAR(n)` | Oracle 中制表符 (`CHR(9)`)、换行符 (`CHR(10)`)、回车符 (`CHR(13)`) 对应 TiDB 中的 `CHAR(9)`、`CHAR(10)`、`CHAR(13)`。 |\n\n## 语法差异\n\n本节介绍 Oracle 部分语法与 TiDB 的差异。\n\n### 字符串语法\n\nOracle 中字符串只能使用单引号 ('')。例如 `'a'`。\n\nTiDB 中字符串可以使用单引号 ('') 或双引号 (\"\")。例如 `'a'` 或 `\"a\"`。\n\n### `NULL` 与空字符串的区分\n\nOracle 中不区分 `NULL` 和空字符串 `''`，即 `NULL` 与 `''` 是等价的。\n\nTiDB 中区分 `NULL` 和空字符串 `''`。\n\n### `INSERT` 语句中读写同一张表\n\nOracle 支持 `INSERT` 语句中读写同一张表。例如：\n\n```sql\nINSERT INTO table1 VALUES (field1,(SELECT field2 FROM table1 WHERE...))\n```\n\nTiDB 不支持 `INSERT` 语句中读写同一张表。例如：\n\n```sql\nINSERT INTO table1 VALUES (field1,(SELECT T.fields2 FROM table1 T WHERE...))\n```\n\n### 获取前 n 行数据\n\nOracle 通过 `ROWNUM <= n` 获取前 n 行数据。例如，`ROWNUM <= 10`。\n\nTiDB 通过 `LIMIT n` 获取前 n 行数据。例如，`LIMIT 10`。Hibernate Query Language (HQL) 方式运行带 `LIMIT` 的 SQL 语句会出现错误，需要将 Hibernate 的运行方式改为 SQL 方式运行。\n\n### `UPDATE` 语句多表更新\n\nOracle 多表更新时不需要列出具体的字段更新关系。例如：\n\n```sql\nUPDATE test1 SET(test1.name,test1.age) = (SELECT test2.name,test2.age FROM test2 WHERE test2.id=test1.id)\n```\n\nTiDB 多表更新时需要在 `SET` 时把具体的字段更新关系都列出来。例如：\n\n```sql\nUPDATE test1,test2 SET test1.name=test2.name,test1.age=test2.age WHERE test1.id=test2.id\n```\n\n### 派生表别名\n\nOracle 多表查询时，派生表可以不起别名。例如：\n\n```sql\nSELECT * FROM (SELECT * FROM test)\n```\n\nTiDB 多表查询时，每一个派生出来的表都必须有一个自己的别名。例如：\n\n```sql\nSELECT * FROM (SELECT * FROM test) t\n```\n\n### 差集运算\n\nOracle 使用 `MINUS` 进行差集运算。例如：\n\n```sql\nSELECT * FROM t1 MINUS SELECT * FROM t2\n```\n\nTiDB 不支持 `MINUS`，需要改写为 `EXCEPT` 进行差集运算。例如：\n\n```sql\nSELECT * FROM t1 EXCEPT SELECT * FROM t2\n```\n\n### 注释语法\n\nOracle 中注释语法为 `--注释`，其中 `--` 后面不需要空格。\n\nTiDB 中注释语法为 `-- 注释`，其中 `--` 后面需要有一个空格。\n\n### 分页查询\n\nOracle 分页查询时 `OFFSET m` 表示跳过 `m` 行数据，`FETCH NEXT n ROWS ONLY` 表示取 `n` 条数据。例如：\n\n```sql\nSELECT * FROM tables OFFSET 0 ROWS FETCH NEXT 2000 ROWS ONLY\n```\n\nTiDB 使用 `LIMIT n OFFSET m` 等价改写 `OFFSET m ROWS FETCH NEXT n ROWS ONLY`。例如：\n\n```sql\nSELECT * FROM tables LIMIT 2000 OFFSET 0\n```\n\n### `ORDER BY` 语句对 `NULL` 的排序规则\n\nOracle 中 `ORDER BY` 语句对 `NULL` 的排序规则：\n\n- `ORDER BY COLUMN ASC` 时，`NULL` 默认被放在最后。\n\n- `ORDER BY COLUMN DESC` 时，`NULL` 默认被放在最前。\n\n- `ORDER BY COLUMN [ASC|DESC] NULLS FIRST` 时，强制 `NULL` 放在最前，非 `NULL` 的值仍然按声明顺序 `ASC|DESC` 进行排序。\n\n- `ORDER BY COLUMN [ASC|DESC] NULLS LAST` 时，强制 `NULL` 放在最后，非 `NULL` 的值仍然按声明顺序 `ASC|DESC` 进行排序。\n\nTiDB 中 `ORDER BY` 语句对 `NULL` 的排序规则：\n\n- `ORDER BY COLUMN ASC` 时，`NULL` 默认被放在最前。\n\n- `ORDER BY COLUMN DESC` 时，`NULL` 默认被放在最后。\n\n下表是 Oracle 与 TiDB 中等价 `ORDER BY` 语句示例：\n\n| Oracle 中的 `ORDER BY` | TiDB 中的 `ORDER BY`|\n| :------------------- | :----------------- |\n| `SELECT * FROM t1 ORDER BY name NULLS FIRST;`      | `SELECT * FROM t1 ORDER BY name;`                         |\n| `SELECT * FROM t1 ORDER BY name DESC NULLS LAST;`  | `SELECT * FROM t1 ORDER BY name DESC;`                    |\n| `SELECT * FROM t1 ORDER BY name DESC NULLS FIRST;` | `SELECT * FROM t1 ORDER BY ISNULL(name) DESC, name DESC;` |\n| `SELECT * FROM t1 ORDER BY name ASC NULLS LAST;`   | `SELECT * FROM t1 ORDER BY ISNULL(name), name;`           |\n"
        },
        {
          "name": "overview.md",
          "type": "blob",
          "size": 5.30859375,
          "content": "---\ntitle: TiDB 简介\naliases: ['/docs-cn/dev/overview/','/docs-cn/dev/key-features/','/docs-cn/overview','/docs-cn/stable/overview/','/docs-cn/stable/key-features/']\nsummary: TiDB 是 PingCAP 公司自主设计、研发的开源分布式关系型数据库，支持在线事务处理与在线分析处理 (HTAP)。具有水平扩容、金融级高可用、实时 HTAP、云原生的分布式数据库、兼容 MySQL 协议和 MySQL 生态等特性。适用于高可用、强一致性要求高、数据规模大等各种应用场景。具有一键水平扩缩容、金融级高可用、实时 HTAP、云原生的分布式数据库、兼容 MySQL 协议和 MySQL 生态等五大核心特性，以及金融行业、海量数据及高并发的 OLTP、实时 HTAP、数据汇聚、二次加工处理等四大核心应用场景。\n---\n\n# TiDB 简介\n\n<!-- Localization note for TiDB:\n\n- 英文：用 distributed SQL，同时开始强调 HTAP\n- 中文：可以保留 NewSQL 字眼，同时强调一栈式实时 HTAP\n- 日文：NewSQL 认可度高，用 NewSQL\n\n-->\n\n[TiDB](https://github.com/pingcap/tidb) 是 [PingCAP](https://pingcap.com/about-cn/) 公司自主设计、研发的开源分布式关系型数据库，是一款同时支持在线事务处理与在线分析处理 (Hybrid Transactional and Analytical Processing, HTAP) 的融合型分布式数据库产品，具备水平扩容或者缩容、金融级高可用、实时 HTAP、云原生的分布式数据库、兼容 MySQL 协议和 MySQL 生态等重要特性。目标是为用户提供一站式 OLTP (Online Transactional Processing)、OLAP (Online Analytical Processing)、HTAP 解决方案。TiDB 适合高可用、强一致要求较高、数据规模较大等各种应用场景。\n\n关于 TiDB 的关键技术创新，请观看以下视频。\n\n<video src=\"https://download.pingcap.com/docs-cn%2FLesson01_intro.mp4\" width=\"100%\" height=\"100%\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_lesson1.png\"></video>\n\n## 五大核心特性\n\n- 一键水平扩缩容\n\n    得益于 TiDB 存储计算分离的架构的设计，可按需对计算、存储分别进行在线扩容或者缩容，扩容或者缩容过程中对应用运维人员透明。\n\n- 金融级高可用\n\n    数据采用多副本存储，数据副本通过 Multi-Raft 协议同步事务日志，多数派写入成功事务才能提交，确保数据强一致性且少数副本发生故障时不影响数据的可用性。可按需配置副本地理位置、副本数量等策略，满足不同容灾级别的要求。\n\n- 实时 HTAP\n\n    提供行存储引擎 [TiKV](/tikv-overview.md)、列存储引擎 [TiFlash](/tiflash/tiflash-overview.md) 两款存储引擎，TiFlash 通过 Multi-Raft Learner 协议实时从 TiKV 复制数据，确保行存储引擎 TiKV 和列存储引擎 TiFlash 之间的数据强一致。TiKV、TiFlash 可按需部署在不同的机器，解决 HTAP 资源隔离的问题。\n\n- 云原生的分布式数据库\n\n    专为云而设计的分布式数据库，通过 [TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/tidb-operator-overview) 可在公有云、私有云、混合云中实现部署工具化、自动化。\n\n- 兼容 MySQL 协议和 MySQL 生态\n\n    兼容 MySQL 协议、MySQL 常用的功能、MySQL 生态，应用无需或者修改少量代码即可从 MySQL 迁移到 TiDB。提供丰富的[数据迁移工具](/ecosystem-tool-user-guide.md)帮助应用便捷完成数据迁移。\n\n## 四大核心应用场景\n\n- 金融行业场景\n\n    金融行业对数据一致性及高可靠、系统高可用、可扩展性、容灾要求较高。传统的解决方案的资源利用率低，维护成本高。TiDB 采用多副本 + Multi-Raft 协议的方式将数据调度到不同的机房、机架、机器，确保系统的 RTO <= 30s 及 RPO = 0。\n\n- 海量数据及高并发的 OLTP 场景\n\n    传统的单机数据库无法满足因数据爆炸性的增长对数据库的容量要求。TiDB 是一种性价比高的解决方案，采用计算、存储分离的架构，可对计算、存储分别进行扩缩容，计算最大支持 512 节点，每个节点最大支持 1000 并发，集群容量最大支持 PB 级别。\n\n- 实时 HTAP 场景\n\n    TiDB 适用于需要实时处理的大规模数据和高并发场景。TiDB 在 4.0 版本中引入列存储引擎 TiFlash，结合行存储引擎 TiKV 构建真正的 HTAP 数据库，在增加少量存储成本的情况下，可以在同一个系统中做联机交易处理、实时数据分析，极大地节省企业的成本。\n\n- 数据汇聚、二次加工处理的场景\n\n    TiDB 适用于将企业分散在各个系统的数据汇聚在同一个系统，并进行二次加工处理生成 T+0 或 T+1 的报表。与 Hadoop 相比，TiDB 要简单得多，业务通过 ETL 工具或者 TiDB 的同步工具将数据同步到 TiDB，在 TiDB 中可通过 SQL 直接生成报表。\n\n关于 TiDB 典型应用场景和用户案例的介绍，请观看以下视频。\n\n<video src=\"https://download.pingcap.com/docs-cn%2FLesson06_scenarios.mp4\" width=\"100%\" height=\"100%\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_lesson6.png\"></video>\n\n## 另请参阅\n\n- [TiDB 整体架构](/tidb-architecture.md)\n- [TiDB 数据库的存储](/tidb-storage.md)\n- [TiDB 数据库的计算](/tidb-computing.md)\n- [TiDB 数据库的调度](/tidb-scheduling.md)\n"
        },
        {
          "name": "partition-pruning.md",
          "type": "blob",
          "size": 19.44140625,
          "content": "---\ntitle: 分区裁剪\nsummary: 了解 TiDB 分区裁剪的使用场景。\naliases: ['/docs-cn/dev/partition-pruning/']\n---\n\n# 分区裁剪\n\n分区裁剪是只有当目标表为分区表时，才可以进行的一种优化方式。分区裁剪通过分析查询语句中的过滤条件，只选择可能满足条件的分区，不扫描匹配不上的分区，进而显著地减少计算的数据量。\n\n例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n id INT NOT NULL PRIMARY KEY,\n pad VARCHAR(100)\n)\nPARTITION BY RANGE COLUMNS(id) (\n PARTITION p0 VALUES LESS THAN (100),\n PARTITION p1 VALUES LESS THAN (200),\n PARTITION p2 VALUES LESS THAN (MAXVALUE)\n);\nINSERT INTO t1 VALUES (1, 'test1'),(101, 'test2'), (201, 'test3');\nEXPLAIN SELECT * FROM t1 WHERE id BETWEEN 80 AND 120;\n```\n\n```sql\n+----------------------------+---------+-----------+------------------------+------------------------------------------------+\n| id                         | estRows | task      | access object          | operator info                                  |\n+----------------------------+---------+-----------+------------------------+------------------------------------------------+\n| PartitionUnion_8           | 80.00   | root      |                        |                                                |\n| ├─TableReader_10           | 40.00   | root      |                        | data:TableRangeScan_9                          |\n| │ └─TableRangeScan_9       | 40.00   | cop[tikv] | table:t1, partition:p0 | range:[80,120], keep order:false, stats:pseudo |\n| └─TableReader_12           | 40.00   | root      |                        | data:TableRangeScan_11                         |\n|   └─TableRangeScan_11      | 40.00   | cop[tikv] | table:t1, partition:p1 | range:[80,120], keep order:false, stats:pseudo |\n+----------------------------+---------+-----------+------------------------+------------------------------------------------+\n5 rows in set (0.00 sec)\n```\n\n## 分区裁剪的使用场景\n\n分区表有 Range 分区和 hash 分区两种形式，分区裁剪对两种分区表也有不同的使用场景。\n\n### 分区裁剪在 Hash 分区表上的应用\n\n#### Hash 分区表上可以使用分区裁剪的场景\n\n只有等值比较的查询条件能够支持 Hash 分区表的裁剪。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (x int) partition by hash(x) partitions 4;\nexplain select * from t where x = 1;\n```\n\n```sql\n+-------------------------+----------+-----------+-----------------------+--------------------------------+\n| id                      | estRows  | task      | access object         | operator info                  |\n+-------------------------+----------+-----------+-----------------------+--------------------------------+\n| TableReader_8           | 10.00    | root      |                       | data:Selection_7               |\n| └─Selection_7           | 10.00    | cop[tikv] |                       | eq(test.t.x, 1)                |\n|   └─TableFullScan_6     | 10000.00 | cop[tikv] | table:t, partition:p1 | keep order:false, stats:pseudo |\n+-------------------------+----------+-----------+-----------------------+--------------------------------+\n```\n\n在这条 SQL 中，由条件 `x = 1` 可以知道所有结果均在一个分区上。数值 `1` 在经过 Hash 后，可以确定其在分区 `p1` 中。因此只需要扫描分区 `p1`，而无需访问一定不会出现相关结果的 `p2` 、`p3` 、`p4` 分区。从执行计划来看，其中只出现了一个 `TableFullScan` 算子，且在 `access object` 中指定了 `p1` 分区，确认 `partition pruning` 生效了。\n\n#### Hash 分区表上不能使用分区裁剪的场景\n\n##### 场景一\n\n不能确定查询结果只在一个分区上的条件：如 `in`, `between`, `> < >= <=` 等查询条件，不能使用分区裁剪的优化。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (x int) partition by hash(x) partitions 4;\nexplain select * from t where x > 2;\n```\n\n```sql\n+------------------------------+----------+-----------+-----------------------+--------------------------------+\n| id                           | estRows  | task      | access object         | operator info                  |\n+------------------------------+----------+-----------+-----------------------+--------------------------------+\n| Union_10                     | 13333.33 | root      |                       |                                |\n| ├─TableReader_13             | 3333.33  | root      |                       | data:Selection_12              |\n| │ └─Selection_12             | 3333.33  | cop[tikv] |                       | gt(test.t.x, 2)                |\n| │   └─TableFullScan_11       | 10000.00 | cop[tikv] | table:t, partition:p0 | keep order:false, stats:pseudo |\n| ├─TableReader_16             | 3333.33  | root      |                       | data:Selection_15              |\n| │ └─Selection_15             | 3333.33  | cop[tikv] |                       | gt(test.t.x, 2)                |\n| │   └─TableFullScan_14       | 10000.00 | cop[tikv] | table:t, partition:p1 | keep order:false, stats:pseudo |\n| ├─TableReader_19             | 3333.33  | root      |                       | data:Selection_18              |\n| │ └─Selection_18             | 3333.33  | cop[tikv] |                       | gt(test.t.x, 2)                |\n| │   └─TableFullScan_17       | 10000.00 | cop[tikv] | table:t, partition:p2 | keep order:false, stats:pseudo |\n| └─TableReader_22             | 3333.33  | root      |                       | data:Selection_21              |\n|   └─Selection_21             | 3333.33  | cop[tikv] |                       | gt(test.t.x, 2)                |\n|     └─TableFullScan_20       | 10000.00 | cop[tikv] | table:t, partition:p3 | keep order:false, stats:pseudo |\n+------------------------------+----------+-----------+-----------------------+--------------------------------+\n```\n\n在这条 SQL 中，`x > 2` 条件无法确定对应的 Hash Partition，所以不能使用分区裁剪。\n\n##### 场景二\n\n由于分区裁剪的规则优化是在查询计划的生成阶段，对于执行阶段才能获取到过滤条件的场景，无法利用分区裁剪的优化。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (x int) partition by hash(x) partitions 4;\nexplain select * from t2 where x = (select * from t1 where t2.x = t1.x and t2.x < 2);\n```\n\n```sql\n+--------------------------------------+----------+-----------+------------------------+----------------------------------------------+\n| id                                   | estRows  | task      | access object          | operator info                                |\n+--------------------------------------+----------+-----------+------------------------+----------------------------------------------+\n| Projection_13                        | 9990.00  | root      |                        | test.t2.x                                    |\n| └─Apply_15                           | 9990.00  | root      |                        | inner join, equal:[eq(test.t2.x, test.t1.x)] |\n|   ├─TableReader_18(Build)            | 9990.00  | root      |                        | data:Selection_17                            |\n|   │ └─Selection_17                   | 9990.00  | cop[tikv] |                        | not(isnull(test.t2.x))                       |\n|   │   └─TableFullScan_16             | 10000.00 | cop[tikv] | table:t2               | keep order:false, stats:pseudo               |\n|   └─Selection_19(Probe)              | 0.80     | root      |                        | not(isnull(test.t1.x))                       |\n|     └─MaxOneRow_20                   | 1.00     | root      |                        |                                              |\n|       └─Union_21                     | 2.00     | root      |                        |                                              |\n|         ├─TableReader_24             | 2.00     | root      |                        | data:Selection_23                            |\n|         │ └─Selection_23             | 2.00     | cop[tikv] |                        | eq(test.t2.x, test.t1.x), lt(test.t2.x, 2)   |\n|         │   └─TableFullScan_22       | 2500.00  | cop[tikv] | table:t1, partition:p0 | keep order:false, stats:pseudo               |\n|         └─TableReader_27             | 2.00     | root      |                        | data:Selection_26                            |\n|           └─Selection_26             | 2.00     | cop[tikv] |                        | eq(test.t2.x, test.t1.x), lt(test.t2.x, 2)   |\n|             └─TableFullScan_25       | 2500.00  | cop[tikv] | table:t1, partition:p1 | keep order:false, stats:pseudo               |\n+--------------------------------------+----------+-----------+------------------------+----------------------------------------------+\n```\n\n这个查询每从 `t2` 读取一行，都会去分区表 `t1` 上进行查询，理论上这时会满足 `t1.x = val` 的过滤条件，但实际上由于分区裁剪只作用于查询计划生成阶段，而不是执行阶段，因而不会做裁剪。\n\n### 分区裁剪在 Range 分区表上的应用\n\n#### Range 分区表上可以使用分区裁剪的场景\n\n##### 场景一\n\n等值比较的查询条件可以使用分区裁剪。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (x int) partition by range (x) (\n    partition p0 values less than (5),\n    partition p1 values less than (10),\n    partition p2 values less than (15)\n    );\nexplain select * from t where x = 3;\n```\n\n```sql\n+-------------------------+----------+-----------+-----------------------+--------------------------------+\n| id                      | estRows  | task      | access object         | operator info                  |\n+-------------------------+----------+-----------+-----------------------+--------------------------------+\n| TableReader_8           | 10.00    | root      |                       | data:Selection_7               |\n| └─Selection_7           | 10.00    | cop[tikv] |                       | eq(test.t.x, 3)                |\n|   └─TableFullScan_6     | 10000.00 | cop[tikv] | table:t, partition:p0 | keep order:false, stats:pseudo |\n+-------------------------+----------+-----------+-----------------------+--------------------------------+\n```\n\n使用 `in` 条件的等值比较查询条件也可以使用分区裁剪。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (x int) partition by range (x) (\n    partition p0 values less than (5),\n    partition p1 values less than (10),\n    partition p2 values less than (15)\n    );\nexplain select * from t where x in(1,13);\n```\n\n```sql\n+-----------------------------+----------+-----------+-----------------------+--------------------------------+\n| id                          | estRows  | task      | access object         | operator info                  |\n+-----------------------------+----------+-----------+-----------------------+--------------------------------+\n| Union_8                     | 40.00    | root      |                       |                                |\n| ├─TableReader_11            | 20.00    | root      |                       | data:Selection_10              |\n| │ └─Selection_10            | 20.00    | cop[tikv] |                       | in(test.t.x, 1, 13)            |\n| │   └─TableFullScan_9       | 10000.00 | cop[tikv] | table:t, partition:p0 | keep order:false, stats:pseudo |\n| └─TableReader_14            | 20.00    | root      |                       | data:Selection_13              |\n|   └─Selection_13            | 20.00    | cop[tikv] |                       | in(test.t.x, 1, 13)            |\n|     └─TableFullScan_12      | 10000.00 | cop[tikv] | table:t, partition:p2 | keep order:false, stats:pseudo |\n+-----------------------------+----------+-----------+-----------------------+--------------------------------+\n```\n\n在这条 SQL 中，由条件 `x in(1,13)` 可以知道所有结果只会分布在几个分区上。经过分析，发现所有 `x = 1` 的记录都在分区 `p0` 上，所有 `x = 13` 的记录都在分区 `p2` 上，因此只需要访问 `p0`、`p2` 这两个分区，\n\n##### 场景二\n\n区间比较的查询条件如 `between`, `> < = >= <=` 可以使用分区裁剪。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (x int) partition by range (x) (\n    partition p0 values less than (5),\n    partition p1 values less than (10),\n    partition p2 values less than (15)\n    );\nexplain select * from t where x between 7 and 14;\n```\n\n```sql\n+-----------------------------+----------+-----------+-----------------------+-----------------------------------+\n| id                          | estRows  | task      | access object         | operator info                     |\n+-----------------------------+----------+-----------+-----------------------+-----------------------------------+\n| Union_8                     | 500.00   | root      |                       |                                   |\n| ├─TableReader_11            | 250.00   | root      |                       | data:Selection_10                 |\n| │ └─Selection_10            | 250.00   | cop[tikv] |                       | ge(test.t.x, 7), le(test.t.x, 14) |\n| │   └─TableFullScan_9       | 10000.00 | cop[tikv] | table:t, partition:p1 | keep order:false, stats:pseudo    |\n| └─TableReader_14            | 250.00   | root      |                       | data:Selection_13                 |\n|   └─Selection_13            | 250.00   | cop[tikv] |                       | ge(test.t.x, 7), le(test.t.x, 14) |\n|     └─TableFullScan_12      | 10000.00 | cop[tikv] | table:t, partition:p2 | keep order:false, stats:pseudo    |\n+-----------------------------+----------+-----------+-----------------------+-----------------------------------+\n```\n\n##### 场景三\n\n分区表达式为 `fn(col)` 的简单形式，查询条件是 `>` `<` `=` `>=` `<=` 之一，且 `fn` 是单调函数，可以使用分区裁剪。\n\n关于 `fn` 函数，对于任意 `x` `y`，如果 `x > y`，则 `fn(x) > fn(y)`，那么这种是严格递增的单调函数。非严格递增的单调函数也可以符合分区裁剪要求，只要函数 `fn` 满足：对于任意 `x` `y`，如果 `x > y`，则 `fn(x) >= fn(y)`。理论上，所有满足单调条件（严格或者非严格）的函数都支持分区裁剪。目前，TiDB 支持的单调函数如下：\n\n* [`UNIX_TIMESTAMP()`](/functions-and-operators/date-and-time-functions.md)\n* [`TO_DAYS()`](/functions-and-operators/date-and-time-functions.md)\n* [`EXTRACT(<time unit> FROM <DATETIME/DATE/TIME column>)`](/functions-and-operators/date-and-time-functions.md)。对于 `DATE` 和 `DATETIME` 列，`YEAR` 和 `YEAR_MONTH` 时间单位被视为单调函数。对于 `TIME` 列，`HOUR`、`HOUR_MINUTE`、`HOUR_SECOND` 和 `HOUR_MICROSECOND` 被视为单调函数。请注意，`EXTRACT` 中不支持将 `WEEK` 作为分区裁剪的时间单位。\n\n例如，分区表达式是 `fn(col)` 形式，`fn` 为我们支持的单调函数 `to_days`，就可以使用分区裁剪：\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (id datetime) partition by range (to_days(id)) (\n    partition p0 values less than (to_days('2020-04-01')),\n    partition p1 values less than (to_days('2020-05-01')));\nexplain select * from t where id > '2020-04-18';\n```\n\n```sql\n+-------------------------+----------+-----------+-----------------------+-------------------------------------------+\n| id                      | estRows  | task      | access object         | operator info                             |\n+-------------------------+----------+-----------+-----------------------+-------------------------------------------+\n| TableReader_8           | 3333.33  | root      |                       | data:Selection_7                          |\n| └─Selection_7           | 3333.33  | cop[tikv] |                       | gt(test.t.id, 2020-04-18 00:00:00.000000) |\n|   └─TableFullScan_6     | 10000.00 | cop[tikv] | table:t, partition:p1 | keep order:false, stats:pseudo            |\n+-------------------------+----------+-----------+-----------------------+-------------------------------------------+\n```\n\n#### Range 分区表上不能使用分区裁剪的场景\n\n由于分区裁剪的规则优化是在查询计划的生成阶段，对于执行阶段才能获取到过滤条件的场景，无法利用分区裁剪的优化。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t1 (x int) partition by range (x) (\n    partition p0 values less than (5),\n    partition p1 values less than (10));\ncreate table t2 (x int);\nexplain select * from t2 where x < (select * from t1 where t2.x < t1.x and t2.x < 2);\n```\n\n```sql\n+--------------------------------------+----------+-----------+------------------------+-----------------------------------------------------------+\n| id                                   | estRows  | task      | access object          | operator info                                             |\n+--------------------------------------+----------+-----------+------------------------+-----------------------------------------------------------+\n| Projection_13                        | 9990.00  | root      |                        | test.t2.x                                                 |\n| └─Apply_15                           | 9990.00  | root      |                        | CARTESIAN inner join, other cond:lt(test.t2.x, test.t1.x) |\n|   ├─TableReader_18(Build)            | 9990.00  | root      |                        | data:Selection_17                                         |\n|   │ └─Selection_17                   | 9990.00  | cop[tikv] |                        | not(isnull(test.t2.x))                                    |\n|   │   └─TableFullScan_16             | 10000.00 | cop[tikv] | table:t2               | keep order:false, stats:pseudo                            |\n|   └─Selection_19(Probe)              | 0.80     | root      |                        | not(isnull(test.t1.x))                                    |\n|     └─MaxOneRow_20                   | 1.00     | root      |                        |                                                           |\n|       └─Union_21                     | 2.00     | root      |                        |                                                           |\n|         ├─TableReader_24             | 2.00     | root      |                        | data:Selection_23                                         |\n|         │ └─Selection_23             | 2.00     | cop[tikv] |                        | lt(test.t2.x, 2), lt(test.t2.x, test.t1.x)                |\n|         │   └─TableFullScan_22       | 2.50     | cop[tikv] | table:t1, partition:p0 | keep order:false, stats:pseudo                            |\n|         └─TableReader_27             | 2.00     | root      |                        | data:Selection_26                                         |\n|           └─Selection_26             | 2.00     | cop[tikv] |                        | lt(test.t2.x, 2), lt(test.t2.x, test.t1.x)                |\n|             └─TableFullScan_25       | 2.50     | cop[tikv] | table:t1, partition:p1 | keep order:false, stats:pseudo                            |\n+--------------------------------------+----------+-----------+------------------------+-----------------------------------------------------------+\n14 rows in set (0.00 sec)\n```\n\n这个查询每从 `t2` 读取一行，都会去分区表 `t1` 上进行查询，理论上这时会满足 `t1.x > val` 的过滤条件，但实际上由于分区裁剪只作用于查询计划生成阶段，而不是执行阶段，因而不会做裁剪。\n"
        },
        {
          "name": "partitioned-raft-kv.md",
          "type": "blob",
          "size": 2.4326171875,
          "content": "---\ntitle: Partitioned Raft KV\nsummary: 了解 TiKV 的 Partitioned Raft KV 特性。\n---\n\n# Partitioned Raft KV\n\n> **警告：**\n>\n> Partitioned Raft KV 目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\nv6.6.0 之前，基于 Raft 的存储引擎，TiKV 使用单个 RocksDB 实例存储该 TiKV 实例所有 Region 的数据。\n\n为了更平稳地支持更大的集群，从 v6.6.0 开始，TiDB 引入了一个全新的 TiKV 存储引擎，该引擎使用多个 RocksDB 实例来存储 TiKV 的 Region 数据，每个 Region 的数据都独立存储在单个 RocksDB 实例中。\n\n新的 TiKV 引擎能够更好地控制 RocksDB 实例的文件数和层级，并实现了 Region 间数据操作的物理隔离，避免相互影响。同时，该引擎支持平稳管理更多的数据。你可以理解为，TiKV 通过分区管理多个 RocksDB 实例，这也是该特性 Partitioned Raft KV 名字的由来。\n\n## 使用场景\n\n如果你的 TiKV 集群有以下特点，可以考虑使用该功能：\n\n* 需要在单个 TiKV 实例支持更多的数据。\n* 具有大量写入吞吐。\n* 需要频繁地扩缩容。\n* 负载有较为严重的读写放大。\n* TiKV 内存尚有富余。\n\n该功能的主要优势在于，提高写入性能，加快扩缩容速度，以及在相同硬件下支持更多数据和更大的集群。\n\n## 使用方法\n\n要启用 Partitioned Raft KV，需要在创建集群时将配置项 [`storage.engine`](/tikv-configuration-file.md#engine-从-v660-版本开始引入) 设为 `\"partitioned-raft-kv\"`。同时，在使用 Partitioned Raft KV 特性时，可以通过配置项 [`rocksdb.write-buffer-flush-oldest-first`](/tikv-configuration-file.md#write-buffer-flush-oldest-first-从-v660-版本开始引入) 和 [`rocksdb.write-buffer-limit`](/tikv-configuration-file.md#write-buffer-limit-从-v660-版本开始引入) 来控制 RocksDB 的内存使用。\n\n## 使用限制\n\n由于该功能为实验特性，目前有以下限制：\n\n* 暂不支持基于 EBS 的快照备份\n* 暂不支持 Online Unsafe Recovery 和 Titan\n* 不支持 tikv-ctl 命令行管理工具中的以下子命令：\n    * `unsafe-recover`\n    * `raw-scan`\n    * `remove-fail-stores`\n    * `recreate-region`\n    * `reset-to-version`\n* 暂不兼容 TiFlash\n* 初始化以后不支持开启或者关闭\n"
        },
        {
          "name": "partitioned-table.md",
          "type": "blob",
          "size": 84.47265625,
          "content": "---\ntitle: 分区表\nsummary: 了解如何使用 TiDB 的分区表。\naliases: ['/docs-cn/dev/partitioned-table/','/docs-cn/dev/reference/sql/partitioning/']\n---\n\n# 分区表\n\n本文介绍 TiDB 的分区表。\n\n## 分区类型\n\n本节介绍 TiDB 中的分区类型。当前支持的类型包括 [Range 分区](#range-分区)、[Range COLUMNS 分区](#range-columns-分区)、[Range INTERVAL 分区](#range-interval-分区)、[List 分区](#list-分区)、[List COLUMNS 分区](#list-columns-分区)、[Hash 分区](#hash-分区)和 [Key 分区](#key-分区)。\n\n- Range 分区、Range COLUMNS 分区、List 分区和 List COLUMNS 分区可以用于解决业务中大量删除带来的性能问题，支持快速删除分区。\n- Hash 分区和 Key 分区可以用于大量写入场景下的数据打散。与 Hash 分区相比，Key 分区支持多列打散和非整数类型字段的打散。\n\n### Range 分区\n\n一个表按 Range 分区是指，对于表的每个分区中包含的所有行，按分区表达式计算的值都落在给定的范围内。Range 必须是连续的，并且不能有重叠，通过使用 `VALUES LESS THAN` 进行定义。\n\n下列场景中，假设你要创建一个人事记录的表：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT NOT NULL\n);\n```\n\n你可以根据需求按各种方式进行 Range 分区。其中一种方式是按 `store_id` 列进行分区：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT NOT NULL\n)\n\nPARTITION BY RANGE (store_id) (\n    PARTITION p0 VALUES LESS THAN (6),\n    PARTITION p1 VALUES LESS THAN (11),\n    PARTITION p2 VALUES LESS THAN (16),\n    PARTITION p3 VALUES LESS THAN (21)\n);\n```\n\n在这个分区模式中，所有 `store_id` 为 1 到 5 的员工，都存储在分区 `p0` 里面，`store_id` 为 6 到 10 的员工则存储在分区 `p1` 里面。Range 分区要求，分区的定义必须是有序的，按从小到大递增。\n\n新插入一行数据 `(72, 'Tom', 'John', '2015-06-25', NULL, NULL, 15)` 将会落到分区 `p2` 里面。但如果你插入一条 `store_id` 大于 20 的记录，则会报错，因为 TiDB 无法知晓应该将它插入到哪个分区。这种情况下，可以在建表时使用最大值：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT NOT NULL\n)\n\nPARTITION BY RANGE (store_id) (\n    PARTITION p0 VALUES LESS THAN (6),\n    PARTITION p1 VALUES LESS THAN (11),\n    PARTITION p2 VALUES LESS THAN (16),\n    PARTITION p3 VALUES LESS THAN MAXVALUE\n);\n```\n\n`MAXVALUE` 表示一个比所有整数都大的整数。现在，所有 `store_id` 列大于等于 16 的记录都会存储在 `p3` 分区中。\n\n你也可以按员工的职位编号进行分区，也就是使用 `job_code` 列的值进行分区。假设两位数字编号是用于普通员工，三位数字编号是用于办公室以及客户支持，四位数字编号是管理层职位，那么你可以这样建表：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT NOT NULL\n)\n\nPARTITION BY RANGE (job_code) (\n    PARTITION p0 VALUES LESS THAN (100),\n    PARTITION p1 VALUES LESS THAN (1000),\n    PARTITION p2 VALUES LESS THAN (10000)\n);\n```\n\n在这个例子中，所有普通员工存储在 `p0` 分区，办公室以及支持人员在 `p1` 分区，管理者在 `p2` 分区。\n\n除了可以按 `store_id` 切分，你还可以按日期切分。例如，假设按员工离职的年份进行分区：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT\n)\n\nPARTITION BY RANGE ( YEAR(separated) ) (\n    PARTITION p0 VALUES LESS THAN (1991),\n    PARTITION p1 VALUES LESS THAN (1996),\n    PARTITION p2 VALUES LESS THAN (2001),\n    PARTITION p3 VALUES LESS THAN MAXVALUE\n);\n```\n\n在 Range 分区中，可以基于 `timestamp` 列的值分区，并使用 `unix_timestamp()` 函数，例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE quarterly_report_status (\n    report_id INT NOT NULL,\n    report_status VARCHAR(20) NOT NULL,\n    report_updated TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n)\n\nPARTITION BY RANGE ( UNIX_TIMESTAMP(report_updated) ) (\n    PARTITION p0 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-01-01 00:00:00') ),\n    PARTITION p1 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-04-01 00:00:00') ),\n    PARTITION p2 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-07-01 00:00:00') ),\n    PARTITION p3 VALUES LESS THAN ( UNIX_TIMESTAMP('2008-10-01 00:00:00') ),\n    PARTITION p4 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-01-01 00:00:00') ),\n    PARTITION p5 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-04-01 00:00:00') ),\n    PARTITION p6 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-07-01 00:00:00') ),\n    PARTITION p7 VALUES LESS THAN ( UNIX_TIMESTAMP('2009-10-01 00:00:00') ),\n    PARTITION p8 VALUES LESS THAN ( UNIX_TIMESTAMP('2010-01-01 00:00:00') ),\n    PARTITION p9 VALUES LESS THAN (MAXVALUE)\n);\n```\n\n对于 timestamp 列，使用其它的分区表达式是不允许的。\n\nRange 分区在下列条件之一或者多个都满足时，尤其有效：\n\n* 删除旧数据。如果你使用之前的 `employees` 表的例子，你可以简单使用 `ALTER TABLE employees DROP PARTITION p0;` 删除所有在 1991 年以前停止继续在这家公司工作的员工记录。这会比使用 `DELETE FROM employees WHERE YEAR(separated) <= 1990;` 执行快得多。\n* 使用包含时间或者日期的列，或者是其它按序生成的数据。\n* 频繁查询分区使用的列。例如执行这样的查询 `EXPLAIN SELECT COUNT(*) FROM employees WHERE separated BETWEEN '2000-01-01' AND '2000-12-31' GROUP BY store_id;` 时，TiDB 可以迅速确定，只需要扫描 `p2` 分区的数据，因为其它的分区不满足 `where` 条件。\n\n### Range COLUMNS 分区\n\nRange COLUMNS 分区是 Range 分区的一种变体。你可以使用一个或者多个列作为分区键，分区列的数据类型可以是整数 (integer)、字符串（`CHAR`/`VARCHAR`），`DATE` 和 `DATETIME`。不支持使用任何表达式。\n\n和 Range 分区一样，Range COLUMNS 分区同样需要分区的范围是严格递增的。不支持下面示例中的分区定义：\n\n```sql\nCREATE TABLE t(\n    a int,\n    b datetime,\n    c varchar(8)\n) PARTITION BY RANGE COLUMNS(`c`,`b`)\n(PARTITION `p20240520A` VALUES LESS THAN ('A','2024-05-20 00:00:00'),\n PARTITION `p20240520Z` VALUES LESS THAN ('Z','2024-05-20 00:00:00'),\n PARTITION `p20240521A` VALUES LESS THAN ('A','2024-05-21 00:00:00'));\n```\n\n```\nError 1493 (HY000): VALUES LESS THAN value must be strictly increasing for each partition\n```\n\n假设你想要按名字进行分区，并且能够轻松地删除旧的无效数据，那么你可以创建一个表格，如下所示：\n\n```sql\nCREATE TABLE t (\n  valid_until datetime,\n  name varchar(255) CHARACTER SET ascii,\n  notes text\n)\nPARTITION BY RANGE COLUMNS(name, valid_until)\n(PARTITION `p2022-g` VALUES LESS THAN ('G','2023-01-01 00:00:00'),\n PARTITION `p2023-g` VALUES LESS THAN ('G','2024-01-01 00:00:00'),\n PARTITION `p2022-m` VALUES LESS THAN ('M','2023-01-01 00:00:00'),\n PARTITION `p2023-m` VALUES LESS THAN ('M','2024-01-01 00:00:00'),\n PARTITION `p2022-s` VALUES LESS THAN ('S','2023-01-01 00:00:00'),\n PARTITION `p2023-s` VALUES LESS THAN ('S','2024-01-01 00:00:00'))\n```\n\n该语句将按名字和年份的范围 `[ ('', ''), ('G', '2023-01-01 00:00:00') )`，`[ ('G', '2023-01-01 00:00:00'), ('G', '2024-01-01 00:00:00') )`，`[ ('G', '2024-01-01 00:00:00'), ('M', '2023-01-01 00:00:00') )`，`[ ('M', '2023-01-01 00:00:00'), ('M', '2024-01-01 00:00:00') )`，`[ ('M', '2024-01-01 00:00:00'), ('S', '2023-01-01 00:00:00') )`，`[ ('S', '2023-01-01 00:00:00'), ('S', '2024-01-01 00:00:00') )` 进行分区，删除无效数据，同时仍然可以在 name 和 valid_until 列上进行分区裁剪。其中，`[,)` 是一个左闭右开区间，比如 `[ ('G', '2023-01-01 00:00:00'), ('G', '2024-01-01 00:00:00') )`，表示 name 为 `'G'` ，年份包含 2023-01-01 00:00:00 并大于 2023-01-01 00:00:00 但小于 2024-01-01 00:00:00 的数据，其中不包含 `(G, 2024-01-01 00:00:00)`。\n\n### Range INTERVAL 分区\n\nTiDB v6.3.0 新增了 Range INTERVAL 分区特性，作为语法糖（syntactic sugar）引入。Range INTERVAL 分区是对 Range 分区的扩展。你可以使用特定的间隔（interval）轻松创建分区。\n\n其语法如下：\n\n```sql\nPARTITION BY RANGE [COLUMNS] (<partitioning expression>)\nINTERVAL (<interval expression>)\nFIRST PARTITION LESS THAN (<expression>)\nLAST PARTITION LESS THAN (<expression>)\n[NULL PARTITION]\n[MAXVALUE PARTITION]\n```\n\n示例：\n\n```sql\nCREATE TABLE employees (\n    id int unsigned NOT NULL,\n    fname varchar(30),\n    lname varchar(30),\n    hired date NOT NULL DEFAULT '1970-01-01',\n    separated date DEFAULT '9999-12-31',\n    job_code int,\n    store_id int NOT NULL\n) PARTITION BY RANGE (id)\nINTERVAL (100) FIRST PARTITION LESS THAN (100) LAST PARTITION LESS THAN (10000) MAXVALUE PARTITION\n```\n\n该示例创建的表与如下 SQL 语句相同：\n\n```sql\nCREATE TABLE `employees` (\n  `id` int unsigned NOT NULL,\n  `fname` varchar(30) DEFAULT NULL,\n  `lname` varchar(30) DEFAULT NULL,\n  `hired` date NOT NULL DEFAULT '1970-01-01',\n  `separated` date DEFAULT '9999-12-31',\n  `job_code` int DEFAULT NULL,\n  `store_id` int NOT NULL\n)\nPARTITION BY RANGE (`id`)\n(PARTITION `P_LT_100` VALUES LESS THAN (100),\n PARTITION `P_LT_200` VALUES LESS THAN (200),\n...\n PARTITION `P_LT_9900` VALUES LESS THAN (9900),\n PARTITION `P_LT_10000` VALUES LESS THAN (10000),\n PARTITION `P_MAXVALUE` VALUES LESS THAN (MAXVALUE))\n```\n\nRange INTERVAL 还可以配合 [Range COLUMNS](#range-columns-分区) 分区一起使用。如下面的示例：\n\n```sql\nCREATE TABLE monthly_report_status (\n    report_id int NOT NULL,\n    report_status varchar(20) NOT NULL,\n    report_date date NOT NULL\n)\nPARTITION BY RANGE COLUMNS (report_date)\nINTERVAL (1 MONTH) FIRST PARTITION LESS THAN ('2000-01-01') LAST PARTITION LESS THAN ('2025-01-01')\n```\n\n该示例创建的表与如下 SQL 语句相同：\n\n```sql\nCREATE TABLE `monthly_report_status` (\n  `report_id` int NOT NULL,\n  `report_status` varchar(20) NOT NULL,\n  `report_date` date NOT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin\nPARTITION BY RANGE COLUMNS(`report_date`)\n(PARTITION `P_LT_2000-01-01` VALUES LESS THAN ('2000-01-01'),\n PARTITION `P_LT_2000-02-01` VALUES LESS THAN ('2000-02-01'),\n...\n PARTITION `P_LT_2024-11-01` VALUES LESS THAN ('2024-11-01'),\n PARTITION `P_LT_2024-12-01` VALUES LESS THAN ('2024-12-01'),\n PARTITION `P_LT_2025-01-01` VALUES LESS THAN ('2025-01-01'))\n```\n\n可选参数 `NULL PARTITION` 会创建一个分区，其中分区表达式推导出的值为 `NULL` 的数据会放到该分区。在分区表达式中，`NULL` 会被认为是小于任何其他值。参见[分区对 NULL 值的处理](#range-分区对-null-的处理)。\n\n可选参数 `MAXVALUE PARTITION` 会创建一个最后的分区，其值为 `PARTITION P_MAXVALUE VALUES LESS THAN (MAXVALUE)`。\n\n#### ALTER INTERVAL 分区\n\nINTERVAL 分区还增加了添加和删除分区的更加简单易用的语法。\n\n下面的语句会变更第一个分区，该语句会删除所有小于给定表达式的分区，使匹配的分区成为新的第一个分区。它不会影响 `NULL PARTITION`。\n\n```sql\nALTER TABLE table_name FIRST PARTITION LESS THAN (<expression>)\n```\n\n下面的语句会变更最后一个分区，该语句会添加新的分区，分区范围扩大到给定的表达式的值。如果存在 `MAXVALUE PARTITION`，则该语句不会生效，因为它需要数据重组。\n\n```sql\nALTER TABLE table_name LAST PARTITION LESS THAN (<expression>)\n```\n\n#### INTERVAL 分区相关细节和限制\n\n- INTERVAL 分区特性仅涉及 `CREATE/ALTER TABLE` 语法。元数据保持不变，因此使用该新语法创建或变更的表仍然兼容 MySQL。\n- 为保持兼容 MySQL，`SHOW CREATE TABLE` 的输出格式保持不变。\n- 遵循 INTERVAL 的存量表可以使用新的 `ALTER` 语法。不需要使用 `INTERVAL` 语法重新创建这些表。\n- 如需使用 `INTERVAL` 语法进行 `RANGE COLUMNS` 分区，只能指定一个列为分区键，且该列的类型为整数 (`INTEGER`) 、日期 (`DATE`) 或日期时间 (`DATETIME`) 。\n\n### List 分区\n\nList 分区和 Range 分区有很多相似的地方。不同之处主要在于 List 分区中，对于表的每个分区中包含的所有行，按分区表达式计算的值属于给定的数据集合。每个分区定义的数据集合有任意个值，但不能有重复的值，可通过 `PARTITION ... VALUES IN (...)` 子句对值进行定义。\n\n假设你要创建一张人事记录表，示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    store_id INT\n);\n```\n\n假如一共有 20 个商店分布在 4 个地区，如下表所示：\n\n```\n| Region  | Store ID Numbers     |\n| ------- | -------------------- |\n| North   | 1, 2, 3, 4, 5        |\n| East    | 6, 7, 8, 9, 10       |\n| West    | 11, 12, 13, 14, 15   |\n| Central | 16, 17, 18, 19, 20   |\n```\n\n如果想把同一个地区商店员工的人事数据都存储在同一个分区中，你可以根据 `store_id` 来创建 List 分区：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    store_id INT\n)\nPARTITION BY LIST (store_id) (\n    PARTITION pNorth VALUES IN (1, 2, 3, 4, 5),\n    PARTITION pEast VALUES IN (6, 7, 8, 9, 10),\n    PARTITION pWest VALUES IN (11, 12, 13, 14, 15),\n    PARTITION pCentral VALUES IN (16, 17, 18, 19, 20)\n);\n```\n\n这样就能方便地在表中添加或删除与特定区域相关的记录。例如，假设东部地区 (East) 所有的商店都卖给了另一家公司，所有该地区商店员工相关的行数据都可以通过 `ALTER TABLE employees TRUNCATE PARTITION pEast` 删除，这比等效的 `DELETE` 语句 `DELETE FROM employees WHERE store_id IN (6, 7, 8, 9, 10)` 执行起来更加高效。\n\n使用 `ALTER TABLE employees DROP PARTITION pEast` 也能删除所有这些行，但同时也会从表的定义中删除分区 `pEast`。那样你还需要使用 `ALTER TABLE ... ADD PARTITION` 语句来还原表的原始分区方案。\n\n#### 默认的 List 分区\n\n从 v7.3.0 版本开始，你可以为 List 或者 List COLUMNS 分区表添加默认的 List 分区。默认的 List 分区作为一个后备分区，可以存储那些不匹配任何分区数据集合的行。\n\n> **注意：**\n>\n> 该功能是 TiDB 对 MySQL 语法的扩展。为 List 或 List COLUMNS 分区表添加默认分区后，该分区表的数据无法直接同步到 MySQL 中。\n\n以下面的 List 分区表为例：\n\n```sql\nCREATE TABLE t (\n  a INT,\n  b INT\n)\nPARTITION BY LIST (a) (\n  PARTITION p0 VALUES IN (1, 2, 3),\n  PARTITION p1 VALUES IN (4, 5, 6)\n);\nQuery OK, 0 rows affected (0.11 sec)\n```\n\n通过以下语句，你可以在该表中添加一个名为 `pDef` 的默认 List 分区：\n\n```sql\nALTER TABLE t ADD PARTITION (PARTITION pDef DEFAULT);\n```\n\n或者\n\n```sql\nALTER TABLE t ADD PARTITION (PARTITION pDef VALUES IN (DEFAULT));\n```\n\n此时，如果新插入该表中的值不匹配任何分区的数据集合，对应的数据会自动写入默认分区。\n\n```sql\nINSERT INTO t VALUES (7, 7);\nQuery OK, 1 row affected (0.01 sec)\n```\n\n你也可以在创建 List 或 List COLUMNS 分区表时添加默认分区。例如：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    store_id INT\n)\nPARTITION BY LIST (store_id) (\n    PARTITION pNorth VALUES IN (1, 2, 3, 4, 5),\n    PARTITION pEast VALUES IN (6, 7, 8, 9, 10),\n    PARTITION pWest VALUES IN (11, 12, 13, 14, 15),\n    PARTITION pCentral VALUES IN (16, 17, 18, 19, 20),\n    PARTITION pDefault DEFAULT\n);\n```\n\n对于不包含默认分区的 List 或 List COLUMNS 分区表，`INSERT` 语句要插入的值需要匹配该表 `PARTITION ... VALUES IN (...)` 子句中定义的数据集合。如果要插入的值不匹配任何分区的数据集合，该语句将执行失败并报错，如下例所示：\n\n```sql\nCREATE TABLE t (\n  a INT,\n  b INT\n)\nPARTITION BY LIST (a) (\n  PARTITION p0 VALUES IN (1, 2, 3),\n  PARTITION p1 VALUES IN (4, 5, 6)\n);\nQuery OK, 0 rows affected (0.11 sec)\n\nINSERT INTO t VALUES (7, 7);\nERROR 1525 (HY000): Table has no partition for value 7\n```\n\n要忽略以上错误，可以在 `INSERT` 语句中添加 `IGNORE` 关键字。添加该关键字后，`INSERT` 语句只会插入那些匹配分区数据集合的行，不会插入不匹配的行，并且不会报错：\n\n```sql\ntest> TRUNCATE t;\nQuery OK, 1 row affected (0.00 sec)\n\ntest> INSERT IGNORE INTO t VALUES (1, 1), (7, 7), (8, 8), (3, 3), (5, 5);\nQuery OK, 3 rows affected, 2 warnings (0.01 sec)\nRecords: 5  Duplicates: 2  Warnings: 2\n\ntest> select * from t;\n+------+------+\n| a    | b    |\n+------+------+\n|    5 |    5 |\n|    1 |    1 |\n|    3 |    3 |\n+------+------+\n3 rows in set (0.01 sec)\n```\n\n### List COLUMNS 分区\n\nList COLUMNS 分区是 List 分区的一种变体，可以将多个列用作分区键，并且可以将整数类型以外的数据类型的列用作分区列。你还可以使用字符串类型、`DATE` 和 `DATETIME` 类型的列。\n\n假设商店员工分别来自以下 12 个城市，想要根据相关规定分成 4 个区域，如下表所示：\n\n```\n| Region | Cities                         |\n| :----- | ------------------------------ |\n| 1      | LosAngeles,Seattle, Houston    |\n| 2      | Chicago, Columbus, Boston      |\n| 3      | NewYork, LongIsland, Baltimore |\n| 4      | Atlanta, Raleigh, Cincinnati   |\n```\n\n使用列表列分区，你可以为员工数据创建一张表，将每行数据存储在员工所在城市对应的分区中，如下所示：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE employees_1 (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT,\n    city VARCHAR(15)\n)\nPARTITION BY LIST COLUMNS(city) (\n    PARTITION pRegion_1 VALUES IN('LosAngeles', 'Seattle', 'Houston'),\n    PARTITION pRegion_2 VALUES IN('Chicago', 'Columbus', 'Boston'),\n    PARTITION pRegion_3 VALUES IN('NewYork', 'LongIsland', 'Baltimore'),\n    PARTITION pRegion_4 VALUES IN('Atlanta', 'Raleigh', 'Cincinnati')\n);\n```\n\n与 List 分区不同的是，你不需要在 `COLUMNS()` 子句中使用表达式来将列值转换为整数。\n\nList COLUMNS 分区也可以使用 `DATE` 和 `DATETIME` 类型的列进行分区，如以下示例中所示，该示例使用与先前的 `employees_1` 表相同的名称和列，但根据 `hired` 列采用 List COLUMNS 分区：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE employees_2 (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT,\n    city VARCHAR(15)\n)\nPARTITION BY LIST COLUMNS(hired) (\n    PARTITION pWeek_1 VALUES IN('2020-02-01', '2020-02-02', '2020-02-03',\n        '2020-02-04', '2020-02-05', '2020-02-06', '2020-02-07'),\n    PARTITION pWeek_2 VALUES IN('2020-02-08', '2020-02-09', '2020-02-10',\n        '2020-02-11', '2020-02-12', '2020-02-13', '2020-02-14'),\n    PARTITION pWeek_3 VALUES IN('2020-02-15', '2020-02-16', '2020-02-17',\n        '2020-02-18', '2020-02-19', '2020-02-20', '2020-02-21'),\n    PARTITION pWeek_4 VALUES IN('2020-02-22', '2020-02-23', '2020-02-24',\n        '2020-02-25', '2020-02-26', '2020-02-27', '2020-02-28')\n);\n```\n\n另外，你也可以在 `COLUMNS()` 子句中添加多个列，例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t (\n    id int,\n    name varchar(10)\n)\nPARTITION BY LIST COLUMNS(id,name) (\n     partition p0 values IN ((1,'a'),(2,'b')),\n     partition p1 values IN ((3,'c'),(4,'d')),\n     partition p3 values IN ((5,'e'),(null,null))\n);\n```\n\n### Hash 分区\n\nHash 分区主要用于保证数据均匀地分散到一定数量的分区里面。在 Range 分区中你必须为每个分区指定值的范围；在 Hash 分区中，你只需要指定分区的数量。\n\n创建 Hash 分区表时，需要在 `CREATE TABLE` 后面添加 `PARTITION BY HASH (expr)`，其中 `expr` 是一个返回整数的表达式。当这一列的类型是整数类型时，它可以是一个列名。此外，你很可能还需要加上 `PARTITIONS num`，其中 `num` 是一个正整数，表示将表划分多少分区。\n\n下面的语句将创建一个 Hash 分区表，按 `store_id` 分成 4 个分区：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT\n)\n\nPARTITION BY HASH(store_id)\nPARTITIONS 4;\n```\n\n如果不指定 `PARTITIONS num`，默认的分区数量为 1。\n\n你也可以使用一个返回整数的 SQL 表达式。例如，你可以按入职年份分区：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT\n)\n\nPARTITION BY HASH( YEAR(hired) )\nPARTITIONS 4;\n```\n\n最高效的 Hash 函数是作用在单列上，并且函数的单调性是跟列的值是一样递增或者递减的。\n\n例如，`date_col` 是类型为 `DATE` 的列，表达式 `TO_DAYS(date_col)` 的值是直接随 `date_col` 的值变化的。`YEAR(date_col)` 跟 `TO_DAYS(date_col)` 就不太一样，因为不是每次 `date_col` 变化时 `YEAR(date_col)` 都会得到不同的值。\n\n作为对比，假设我们有一个类型是 INT 的 `int_col` 的列。考虑一下表达式 `POW(5-int_col,3) + 6`，这并不是一个比较好的 Hash 函数，因为随着 `int_col` 的值的变化，表达式的结果不会成比例地变化。改变 `int_col` 的值会使表达式的结果的值变化巨大。例如，`int_col` 从 5 变到 6 表达式的结果变化是 -1，但是从 6 变到 7 的时候表达式的值的变化是 -7。\n\n总而言之，表达式越接近 `y = cx` 的形式，它越是适合作为 Hash 函数。因为表达式越是非线性的，在各个分区上面的数据的分布越是倾向于不均匀。\n\n理论上，Hash 分区也是可以做分区裁剪的。而实际上对于多列的情况，实现很难并且计算很耗时。因此，不推荐 Hash 分区在表达式中涉及多列。\n\n使用 `PARTITIION BY HASH` 的时候，TiDB 通过表达式的结果做“取余”运算，决定数据落在哪个分区。换句话说，如果分区表达式是 `expr`，分区数是 `num`，则由 `MOD(expr, num)` 决定存储的分区。假设 `t1` 定义如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (col1 INT, col2 CHAR(5), col3 DATE)\n    PARTITION BY HASH( YEAR(col3) )\n    PARTITIONS 4;\n```\n\n向 `t1` 插入一行数据，其中 `col3` 列的值是 '2005-09-15'，这条数据会被插入到分区 1 中：\n\n```\nMOD(YEAR('2005-09-01'),4)\n=  MOD(2005,4)\n=  1\n```\n\n### Key 分区\n\nTiDB 从 v7.0.0 开始支持 Key 分区。在 v7.0.0 之前的版本中，创建 Key 分区表时，TiDB 会将其创建为非分区表并给出告警。\n\nKey 分区与 Hash 分区都可以保证将数据均匀地分散到一定数量的分区里面，区别是 Hash 分区只能根据一个指定的整数表达式或字段进行分区，而 Key 分区可以根据字段列表进行分区，且 Key 分区的分区字段不局限于整数类型。TiDB Key 分区表的 Hash 算法与 MySQL 不一样，因此表的数据分布也不一样。\n\n创建 Key 分区表时，你需要在 `CREATE TABLE` 后面添加 `PARTITION BY KEY (columnList)`，其中 `columnList` 是字段列表，可以包含一个或多个字段。每个字段的类型可以是除 `BLOB`、`JSON`、`GEOMETRY` 之外的任意类型（请注意 TiDB 不支持 `GEOMETRY` 类型）。此外，你很可能还需要加上 `PARTITIONS num`，其中 `num` 是一个正整数，表示将表划分多少个分区；或者加上分区名的定义，例如，加上 `(PARTITION p0, PARTITION p1)` 代表将表划分为两个分区，分区名为 `p0` 和 `p1`。\n\n下面的语句将创建一个 Key 分区表，按 `store_id` 分成 4 个分区：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT\n)\n\nPARTITION BY KEY(store_id)\nPARTITIONS 4;\n```\n\n如果不指定 `PARTITIONS num`，默认的分区数量为 1。\n\n你也可以根据 VARCHAR 等非整数字段创建 Key 分区表。下面的语句按 `fname` 将表分成 4 个分区：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT\n)\n\nPARTITION BY KEY(fname)\nPARTITIONS 4;\n```\n\n你还可以根据多列字段创建 Key 分区表。下面的语句按 `fname`、`store_id` 将表分成 4 个分区：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT\n)\n\nPARTITION BY KEY(fname, store_id)\nPARTITIONS 4;\n```\n\n和 MySQL 一样，TiDB 支持分区字段列表 `PARTITION BY KEY` 为空的 Key 分区表。下面的语句将创建一个以主键 `id` 为分区键的分区表：\n\n```sql\nCREATE TABLE employees (\n    id INT NOT NULL PRIMARY KEY,\n    fname VARCHAR(30),\n    lname VARCHAR(30),\n    hired DATE NOT NULL DEFAULT '1970-01-01',\n    separated DATE DEFAULT '9999-12-31',\n    job_code INT,\n    store_id INT\n)\n\nPARTITION BY KEY()\nPARTITIONS 4;\n```\n\n如果表中不存在主键但有唯一键时，使用唯一键作为分区键：\n\n```sql\nCREATE TABLE k1 (\n    id INT NOT NULL,\n    name VARCHAR(20),\n    UNIQUE KEY (id)\n)\nPARTITION BY KEY()\nPARTITIONS 2;\n```\n\n但是，如果唯一键列未被定义为 `NOT NULL`，上述语句将失败。\n\n### TiDB 对 Linear Hash 分区的处理\n\n在 v6.4.0 之前，如果在 TiDB 上执行 [MySQL Linear Hash 分区](https://dev.mysql.com/doc/refman/8.0/en/partitioning-linear-hash.html) 的 DDL 语句，TiDB 只能创建非分区表。在这种情况下，如果你仍然想要在 TiDB 中创建分区表，你需要修改这些 DDL 语句。\n\n从 v6.4.0 起，TiDB 支持解析 MySQL 的 `PARTITION BY LINEAR HASH` 语法，但会忽略其中的 `LINEAR` 关键字。你可以直接在 TiDB 中执行现有的 MySQL Linear Hash 分区的 SQL 语句，而无需修改。\n\n- 对于 MySQL Linear Hash 分区的 `CREATE` 语句，TiDB 将创建一个常规的非线性 Hash 分区表（注意 TiDB 内部实际不存在 Linear Hash 分区表）。如果分区数是 2 的幂，该分区表中行的分布情况与 MySQL 相同。如果分区数不是 2 的幂，该分区表中行的分布情况与 MySQL 会有所差异。这是因为 TiDB 中非线性分区表使用简单的“分区模数”，而线性分区表使用“模数的下一个 2 次方并会折叠分区数和下一个 2 次方之间的值”。详情请见 [#38450](https://github.com/pingcap/tidb/issues/38450)。\n\n- 对于 MySQL Linear Hash 分区的其他 SQL 语句，TiDB 将正常返回对应的 Hash 分区的查询结果。但当分区数不是 2 的幂（意味着分区表中行的分布情况与 MySQL 不同）时，[分区选择](#分区选择)、`TRUNCATE PARTITION`、`EXCHANGE PARTITION` 返回的结果将和 MySQL 有所差异。\n\n### TiDB 对 Linear Key 分区的处理\n\nTiDB 从 v7.0.0 开始支持 Key 分区，并支持解析 MySQL 的 `PARTITION BY LINEAR  KEY` 语法，但会忽略其中的 `LINEAR` 关键字，只采用非线性 Hash 算法。\n\n在 v7.0.0 之前的版本中，创建 Key 分区表时，TiDB 会将其创建为非分区表并给出告警。\n\n### 分区对 NULL 值的处理\n\nTiDB 允许计算结果为 NULL 的分区表达式。注意，NULL 不是一个整数类型，NULL 小于所有的整数类型值，正如 `ORDER BY` 的规则一样。\n\n#### Range 分区对 NULL 的处理\n\n如果插入一行到 Range 分区表，它的分区列的计算结果是 NULL，那么这一行会被插入到最小的那个分区。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n    c1 INT,\n    c2 VARCHAR(20)\n)\n\nPARTITION BY RANGE(c1) (\n    PARTITION p0 VALUES LESS THAN (0),\n    PARTITION p1 VALUES LESS THAN (10),\n    PARTITION p2 VALUES LESS THAN MAXVALUE\n);\n```\n\n```\nQuery OK, 0 rows affected (0.09 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from t1 partition(p0);\n```\n\n```\n+------|--------+\n| c1   | c2     |\n+------|--------+\n| NULL | mothra |\n+------|--------+\n1 row in set (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from t1 partition(p1);\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from t1 partition(p2);\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n删除 `p0` 后验证：\n\n{{< copyable \"sql\" >}}\n\n```sql\nalter table t1 drop partition p0;\n```\n\n```\nQuery OK, 0 rows affected (0.08 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from t1;\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n#### Hash 分区对 NULL 的处理\n\n在 Hash 分区中 NULL 值的处理有所不同，如果分区表达式的计算结果为 NULL，它会被当作 0 值处理。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE th (\n    c1 INT,\n    c2 VARCHAR(20)\n)\n\nPARTITION BY HASH(c1)\nPARTITIONS 2;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO th VALUES (NULL, 'mothra'), (0, 'gigan');\n```\n\n```\nQuery OK, 2 rows affected (0.04 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from th partition (p0);\n```\n\n```\n+------|--------+\n| c1   | c2     |\n+------|--------+\n| NULL | mothra |\n|    0 | gigan  |\n+------|--------+\n2 rows in set (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from th partition (p1);\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n可以看到，插入的记录 `(NULL, 'mothra')` 跟 `(0, 'gigan')` 落在了同一个分区。\n\n> **注意：**\n>\n> 这里 Hash 分区对 NULL 的处理跟 [MySQL 的文档描述](https://dev.mysql.com/doc/refman/8.0/en/partitioning-handling-nulls.html)一致，但是跟 MySQL 的实际行为并不一致。也就是说，MySQL 的文档跟它的实现并不一致。\n>\n> TiDB 的最终行为以本文档描述为准。\n\n#### Key 分区对 NULL 的处理\n\n在 Key 分区中 NULL 值的处理与 Hash 分区一致：如果分区字段的值为 NULL，它会被当作 0 值处理。\n\n## 分区管理\n\n对于 `RANGE`、`RANGE COLUMNS`、`LIST`、`LIST COLUMNS` 分区表，你可以进行以下分区管理操作：\n\n- 使用 `ALTER TABLE <表名> ADD PARTITION (<分区说明>)` 语句添加分区。\n- 使用 `ALTER TABLE <表名> DROP PARTITION <分区列表>` 删除分区。\n- 使用 `ALTER TABLE <表名> TRUNCATE PARTITION <分区列表>` 语句清空分区里的数据。`TRUNCATE PARTITION` 的逻辑与 [`TRUNCATE TABLE`](/sql-statements/sql-statement-truncate.md) 相似，但它的操作对象为分区。\n- 使用 `ALTER TABLE <表名> REORGANIZE PARTITION <分区列表> INTO (<新的分区说明>)`语句对分区进行合并、拆分、或者其他修改。\n\n对于 `HASH` 和 `KEY` 分区表，你可以进行以下分区管理操作：\n\n- 使用 `ALTER TABLE <table name> COALESCE PARTITION <要减少的分区数量>` 语句减少分区数量。此操作会重组分区，将所有数据按照新的分区个数复制到对应的分区。\n- 使用 `ALTER TABLE <table name> ADD PARTITION <要增加的分区数量 | (新的分区说明)>` 语句增加分区的数量。此操作会重组分区，将所有数据按照新的分区个数复制到对应的分区。\n- 使用 `ALTER TABLE <table name> TRUNCATE PARTITION <分区列表>` 语句清空分区里的数据。`TRUNCATE PARTITION` 的逻辑与 [`TRUNCATE TABLE`](/sql-statements/sql-statement-truncate.md) 相似，但它的操作对象为分区。\n\n`EXCHANGE PARTITION` 语句用来交换分区和非分区表，类似于重命名表如 `RENAME TABLE t1 TO t1_tmp, t2 TO t1, t1_tmp TO t2` 的操作。\n\n例如，`ALTER TABLE partitioned_table EXCHANGE PARTITION p1 WITH TABLE non_partitioned_table` 交换的是 `p1` 分区的 `partitioned_table` 表和 `non_partitioned_table` 表。\n\n确保要交换入分区中的所有行与分区定义匹配；否则，交换将失败。\n\n请注意对于以下 TiDB 专有的特性，当表结构中包含这些特性时，在 TiDB 中使用 `EXCHANGE PARTITION` 功能不仅需要满足 [MySQL 的 EXCHANGE PARTITION 条件](https://dev.mysql.com/doc/refman/8.0/en/partitioning-management-exchange.html)，还要保证这些专有特性对于分区表和非分区表的定义相同。\n\n* [Placement Rules in SQL](/placement-rules-in-sql.md)：Placement Policy 定义相同。\n* [TiFlash](/tiflash/tiflash-overview.md)：TiFlash Replica 数量相同。\n* [聚簇索引](/clustered-indexes.md)：分区表和非分区表要么都是聚簇索引 (CLUSTERED)，要么都不是聚簇索引 (NONCLUSTERED)。\n\n此外，`EXCHANGE PARTITION` 和其他组件兼容性上存在一些限制，需要保证分区表和非分区表的一致性：\n\n- TiFlash：TiFlash Replica 定义不同时，无法执行 `EXCHANGE PARTITION` 操作。\n- TiCDC：分区表和非分区表都有主键或者唯一键时，TiCDC 同步 `EXCHANGE PARTITION` 操作；反之 TiCDC 将不会同步。\n- TiDB Lightning 和 BR：使用 TiDB Lightning 导入或使用 BR 恢复的过程中，不要执行 `EXCHANGE PARTITION` 操作。\n\n### 管理 List 分区、List COLUMNS 分区、Range 分区、Range COLUMNS 分区\n\n本小节将以如下 SQL 语句创建的分区表为例，介绍如何管理 Range 分区和 List 分区。\n\n```sql\nCREATE TABLE members (\n    id int,\n    fname varchar(255),\n    lname varchar(255),\n    dob date,\n    data json\n)\nPARTITION BY RANGE (YEAR(dob)) (\n PARTITION pBefore1950 VALUES LESS THAN (1950),\n PARTITION p1950 VALUES LESS THAN (1960),\n PARTITION p1960 VALUES LESS THAN (1970),\n PARTITION p1970 VALUES LESS THAN (1980),\n PARTITION p1980 VALUES LESS THAN (1990),\n PARTITION p1990 VALUES LESS THAN (2000));\n\nCREATE TABLE member_level (\n id int,\n level int,\n achievements json\n)\nPARTITION BY LIST (level) (\n PARTITION l1 VALUES IN (1),\n PARTITION l2 VALUES IN (2),\n PARTITION l3 VALUES IN (3),\n PARTITION l4 VALUES IN (4),\n PARTITION l5 VALUES IN (5));\n```\n\n#### 删除分区\n\n```sql\nALTER TABLE members DROP PARTITION p1990;\n\nALTER TABLE member_level DROP PARTITION l5;\n```\n\n#### 清空分区\n\n```sql\nALTER TABLE members TRUNCATE PARTITION p1980;\n\nALTER TABLE member_level TRUNCATE PARTITION l4;\n```\n\n#### 添加分区\n\n```sql\nALTER TABLE members ADD PARTITION (PARTITION `p1990to2010` VALUES LESS THAN (2010));\n\nALTER TABLE member_level ADD PARTITION (PARTITION l5_6 VALUES IN (5,6));\n```\n\n对于 Range 分区表，`ADD PARTITION` 只能在分区列表的最后添加新的分区。与分区列表中已有的分区相比，你需要将新分区的 `VALUES LESS THAN` 定义为更大的值。否则，执行该语句时将会报错。\n\n```sql\nALTER TABLE members ADD PARTITION (PARTITION p1990 VALUES LESS THAN (2000));\n```\n\n```\nERROR 1493 (HY000): VALUES LESS THAN value must be strictly increasing for each partition\n```\n\n#### 重组分区\n\n拆分分区：\n\n```sql\nALTER TABLE members REORGANIZE PARTITION `p1990to2010` INTO\n(PARTITION p1990 VALUES LESS THAN (2000),\n PARTITION p2000 VALUES LESS THAN (2010),\n PARTITION p2010 VALUES LESS THAN (2020),\n PARTITION p2020 VALUES LESS THAN (2030),\n PARTITION pMax VALUES LESS THAN (MAXVALUE));\n\nALTER TABLE member_level REORGANIZE PARTITION l5_6 INTO\n(PARTITION l5 VALUES IN (5),\n PARTITION l6 VALUES IN (6));\n```\n\n合并分区：\n\n```sql\nALTER TABLE members REORGANIZE PARTITION pBefore1950,p1950 INTO (PARTITION pBefore1960 VALUES LESS THAN (1960));\n\nALTER TABLE member_level REORGANIZE PARTITION l1,l2 INTO (PARTITION l1_2 VALUES IN (1,2));\n```\n\n修改分区表定义：\n\n```sql\nALTER TABLE members REORGANIZE PARTITION pBefore1960,p1960,p1970,p1980,p1990,p2000,p2010,p2020,pMax INTO\n(PARTITION p1800 VALUES LESS THAN (1900),\n PARTITION p1900 VALUES LESS THAN (2000),\n PARTITION p2000 VALUES LESS THAN (2100));\n\nALTER TABLE member_level REORGANIZE PARTITION l1_2,l3,l4,l5,l6 INTO\n(PARTITION lOdd VALUES IN (1,3,5),\n PARTITION lEven VALUES IN (2,4,6));\n```\n\n在重组分区时，需要注意以下关键点：\n\n- 重组分区（包括合并或拆分分区）只能修改分区定义，无法修改分区表类型。例如，无法将 List 类型修改为 Range 类型，或将 Range COLUMNS 类型修改为 Range 类型。\n\n- 对于 Range 分区表，你只能对表中相邻的分区进行重组：\n\n    ```sql\n    ALTER TABLE members REORGANIZE PARTITION p1800,p2000 INTO (PARTITION p2000 VALUES LESS THAN (2100));\n    ```\n\n    ```\n    ERROR 8200 (HY000): Unsupported REORGANIZE PARTITION of RANGE; not adjacent partitions\n    ```\n\n- 对于 Range 分区表，如需修改 Range 定义中的最大值，必须保证 `VALUES LESS THAN` 中新定义的值大于现有分区中的所有值。否则，TiDB 将报错，提示现有的行值对应不到分区。\n\n    ```sql\n    INSERT INTO members VALUES (313, \"John\", \"Doe\", \"2022-11-22\", NULL);\n    ALTER TABLE members REORGANIZE PARTITION p2000 INTO (PARTITION p2000 VALUES LESS THAN (2050)); -- 执行成功，因为 2050 包含了现有的所有行\n    ALTER TABLE members REORGANIZE PARTITION p2000 INTO (PARTITION p2000 VALUES LESS THAN (2020)); -- 执行失败，因为 2022 将对应不到分区\n    ```\n\n    ```\n    ERROR 1526 (HY000): Table has no partition for value 2022\n    ```\n\n- 对于 List 分区表，如需修改分区定义中的数据集合，必须保证新的数据集合能覆盖到该分区中现有的所有值，否则 TiDB 将报错。\n\n    ```sql\n    INSERT INTO member_level (id, level) values (313, 6);\n    ALTER TABLE member_level REORGANIZE PARTITION lEven INTO (PARTITION lEven VALUES IN (2,4));\n    ```\n\n    ```\n    ERROR 1526 (HY000): Table has no partition for value 6\n    ```\n\n- 分区重组后，相应分区的统计信息将会过期，并返回以下警告。此时，你可以通过 [`ANALYZE TABLE`]（/sql-statements/sql-statement-analyze-table.md）语句更新统计信息。\n\n    ```sql\n    +---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n    | Level   | Code | Message                                                                                                                                                |\n    +---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n    | Warning | 1105 | The statistics of related partitions will be outdated after reorganizing partitions. Please use 'ANALYZE TABLE' statement if you want to update it now |\n    +---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n### 管理 Hash 分区和 Key 分区\n\n本小节将以如下 SQL 语句创建的分区表为例，介绍如何管理 Hash 分区。对于 Key 分区，你也可以使用与 Hash 分区相同的分区管理语句。\n\n```sql\nCREATE TABLE example (\n  id INT PRIMARY KEY,\n  data VARCHAR(1024)\n)\nPARTITION BY HASH(id)\nPARTITIONS 2;\n```\n\n#### 增加分区数量\n\n将 `example` 表的分区个数增加 1 个（从 2 增加到 3）：\n\n```sql\nALTER TABLE example ADD PARTITION PARTITIONS 1;\n```\n\n你也可以通过添加分区定义来指定分区选项。例如，你可以通过以下语句将分区数量从 3 增加到 5，并指定新增的分区名为 `pExample4` 和 `pExample5`：\n\n```sql\nALTER TABLE example ADD PARTITION\n(PARTITION pExample4 COMMENT = 'not p3, but pExample4 instead',\n PARTITION pExample5 COMMENT = 'not p4, but pExample5 instead');\n```\n\n#### 减少分区数量\n\n与 Range 和 List 分区不同，Hash 和 Key 分区不支持 `DROP PARTITION`，但可以使用 `COALESCE PARTITION` 来减少分区数量，或使用 `TRUNCATE PARTITION` 清空指定分区的所有数据。\n\n将 `example` 表的分区个数减少 1 个（从 5 减少到 4）：\n\n```sql\nALTER TABLE example COALESCE PARTITION 1;\n```\n\n> **注意：**\n>\n> 更改 Hash 和 Key 分区表的分区个数的过程会重组分区，将所有数据按照新的分区个数复制到对应的分区。因此，更改 Hash 和 Key 分区表的分区个数后，会遇到以下关于过时统计信息的警告。此时，你可以通过 [`ANALYZE TABLE`](/sql-statements/sql-statement-analyze-table.md) 语句更新统计信息。\n>\n> ```sql\n> +---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n> | Level   | Code | Message                                                                                                                                                |\n> +---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n> | Warning | 1105 | The statistics of related partitions will be outdated after reorganizing partitions. Please use 'ANALYZE TABLE' statement if you want to update it now |\n> +---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------+\n> 1 row in set (0.00 sec)\n> ```\n\n为了更好地理解 `example` 表重组后的结构，你可以查看重新创建 `example` 表所使用的 SQL 语句，如下所示：\n\n```sql\nSHOW CREATE TABLE\\G\n```\n\n```\n*************************** 1. row ***************************\n       Table: example\nCreate Table: CREATE TABLE `example` (\n  `id` int NOT NULL,\n  `data` varchar(1024) DEFAULT NULL,\n  PRIMARY KEY (`id`) /*T![clustered_index] CLUSTERED */\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin\nPARTITION BY HASH (`id`)\n(PARTITION `p0`,\n PARTITION `p1`,\n PARTITION `p2`,\n PARTITION `pExample4` COMMENT 'not p3, but pExample4 instead')\n1 row in set (0.01 sec)\n```\n\n#### 清空分区\n\n清空指定分区的所有数据：\n\n```sql\nALTER TABLE example TRUNCATE PARTITION p0;\n```\n\n```\nQuery OK, 0 rows affected (0.03 sec)\n```\n\n### 将分区表转换为非分区表\n\n要将分区表转换为非分区表，你可以使用以下语句。该语句在执行时将会删除分区，复制表中的所有行，并为表在线重新创建索引。\n\n```sql\nALTER TABLE <table_name> REMOVE PARTITIONING\n```\n\n例如，要将分区表 `members` 转换为非分区表，可以执行以下语句：\n\n```sql\nALTER TABLE members REMOVE PARTITIONING\n```\n\n### 对现有表进行分区\n\n要对现有的非分区表进行分区或修改现有分区表的分区类型，你可以使用以下语句。该语句在执行时，将根据新的分区定义复制表中的所有行，并在线重新创建索引：\n\n```sql\nALTER TABLE <table_name> PARTITION BY <new partition type and definitions> [UPDATE INDEXES (<index name> {GLOBAL|LOCAL}[ , <index name> {GLOBAL|LOCAL}...])]\n```\n\n示例：\n\n要将现有的 `members` 表转换为一个包含 10 个分区的 HASH 分区表，可以执行以下语句：\n\n```sql\nALTER TABLE members PARTITION BY HASH(id) PARTITIONS 10;\n```\n\n要将现有的 `member_level` 表转换为 RANGE 分区表，可以执行以下语句：\n\n```sql\nALTER TABLE member_level PARTITION BY RANGE(level)\n(PARTITION pLow VALUES LESS THAN (1),\n PARTITION pMid VALUES LESS THAN (3),\n PARTITION pHigh VALUES LESS THAN (7)\n PARTITION pMax VALUES LESS THAN (MAXVALUE));\n```\n\n对普通表进行分区或者对分区表进行重新分区时，可以根据需要将索引更新为全局索引或普通索引：\n\n```sql\nCREATE TABLE t1 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY uidx12(col1, col2),\n    UNIQUE KEY uidx3(col3)\n);\n\nALTER TABLE t1 PARTITION BY HASH (col1) PARTITIONS 3 UPDATE INDEXES (uidx12 LOCAL, uidx3 GLOBAL);\n```\n\n## 分区裁剪\n\n有一个优化叫做[“分区裁剪”](/partition-pruning.md)，它基于一个非常简单的概念：不需要扫描那些匹配不上的分区。\n\n假设创建一个分区表 `t1`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n    fname VARCHAR(50) NOT NULL,\n    lname VARCHAR(50) NOT NULL,\n    region_code TINYINT UNSIGNED NOT NULL,\n    dob DATE NOT NULL\n)\n\nPARTITION BY RANGE( region_code ) (\n    PARTITION p0 VALUES LESS THAN (64),\n    PARTITION p1 VALUES LESS THAN (128),\n    PARTITION p2 VALUES LESS THAN (192),\n    PARTITION p3 VALUES LESS THAN MAXVALUE\n);\n```\n\n如果你想获得这个 select 语句的结果：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT fname, lname, region_code, dob\n    FROM t1\n    WHERE region_code > 125 AND region_code < 130;\n```\n\n很显然，结果必然是在分区 `p1` 或者 `p2` 里面，也就是说，我们只需要在 `p1` 和 `p2` 里面去搜索匹配的行。去掉不必要的分区就是所谓的裁剪。优化器如果能裁剪掉一部分的分区，则执行会快于处理整个不做分区的表的相同查询。\n\n优化器可以通过 where 条件裁剪的两个场景：\n\n* partition_column = constant\n* partition_column IN (constant1, constant2, ..., constantN)\n\n分区裁剪暂不支持 `LIKE` 语句。\n\n### 分区裁剪生效的场景\n\n1. 分区裁剪需要使用分区表上面的查询条件，所以根据优化器的优化规则，如果查询条件不能下推到分区表，则相应的查询语句无法执行分区裁剪。\n\n    例如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    create table t1 (x int) partition by range (x) (\n        partition p0 values less than (5),\n        partition p1 values less than (10));\n    create table t2 (x int);\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    explain select * from t1 left join t2 on t1.x = t2.x where t2.x > 5;\n    ```\n\n    在这个查询中，外连接可以简化成内连接，然后由 `t1.x = t2.x` 和 `t2.x > 5` 可以推出条件 `t1.x > 5`，于是可以分区裁剪并且只使用 `p1` 分区。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    explain select * from t1 left join t2 on t1.x = t2.x and t2.x > 5;\n    ```\n\n    这个查询中的 `t2.x > 5` 条件不能下推到 `t1` 分区表上面，因此 `t1` 无法分区裁剪。\n\n2. 由于分区裁剪的规则优化是在查询计划的生成阶段，对于执行阶段才能获取到过滤条件的场景，无法利用分区裁剪的优化。\n\n    例如：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    create table t1 (x int) partition by range (x) (\n        partition p0 values less than (5),\n        partition p1 values less than (10));\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    explain select * from t2 where x < (select * from t1 where t2.x < t1.x and t2.x < 2);\n    ```\n\n    这个查询每从 `t2` 读取一行，都会去分区表 `t1` 上进行查询，理论上这时会满足 `t1.x > val` 的过滤条件，但实际上由于分区裁剪只作用于查询计划生成阶段，而不是执行阶段，因而不会做裁剪。\n\n3. 由于当前实现中的一处限制，对于查询条件无法下推到 TiKV 的表达式，不支持分区裁剪。\n\n    对于一个函数表达式 `fn(col)`，如果 TiKV 支持这个函数 `fn`，则在查询优化做谓词下推的时候，`fn(col)` 会被推到叶子节点（也就是分区），因而能够执行分区裁剪。\n\n    如果 TiKV 不支持 `fn`，则优化阶段不会把 `fn(col)` 推到叶子节点，而是在叶子上面连接一个 Selection 节点，分区裁剪的实现没有处理这种父节点的 Selection 中的条件，因此对不能下推到 TiKV 的表达式不支持分区裁剪。\n\n4. 对于 Hash 和 Key 分区类型，只有等值比较的查询条件能够支持分区裁剪。\n\n5. 对于 Range 分区类型，分区表达式必须是 `col` 或者 `fn(col)` 的简单形式，查询条件是 `>`、`<`、`=`、`>=`、`<=` 时才能支持分区裁剪。如果分区表达式是 `fn(col)` 形式，还要求 `fn` 必须是单调函数，才有可能分区裁剪。\n\n    这里单调函数是指某个函数 `fn` 满足条件：对于任意 `x` `y`，如果 `x > y`，则 `fn(x) > fn(y)`。\n\n    这种是严格递增的单调函数，非严格递增的单调函数也可以符合分区裁剪要求，只要函数 `fn` 满足：对于任意 `x` `y`，如果 `x > y`，则 `fn(x) >= fn(y)`。\n\n    理论上所有满足单调条件（严格或者非严格）的函数都是可以支持分区裁剪。实际上，目前 TiDB 已经支持的单调函数只有：\n\n    * [`UNIX_TIMESTAMP()`](/functions-and-operators/date-and-time-functions.md)\n    * [`TO_DAYS()`](/functions-and-operators/date-and-time-functions.md)\n    * [`EXTRACT(<time unit> FROM <DATETIME/DATE/TIME column>)`](/functions-and-operators/date-and-time-functions.md)。对于 `DATE` 和 `DATETIME` 列，`YEAR` 和 `YEAR_MONTH` 时间单位被视为单调函数。对于 `TIME` 列，`HOUR`、`HOUR_MINUTE`、`HOUR_SECOND` 和 `HOUR_MICROSECOND` 被视为单调函数。请注意，`EXTRACT` 中不支持将 `WEEK` 作为分区裁剪的时间单位。\n\n    例如，分区表达式是简单列的情况：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    create table t (id int) partition by range (id) (\n        partition p0 values less than (5),\n        partition p1 values less than (10));\n    select * from t where id > 6;\n    ```\n\n    分区表达式是 `fn(col)` 的形式，`fn` 是我们支持的单调函数 `to_days`：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    create table t (dt datetime) partition by range (to_days(id)) (\n        partition p0 values less than (to_days('2020-04-01')),\n        partition p1 values less than (to_days('2020-05-01')));\n    select * from t where dt > '2020-04-18';\n    ```\n\n    有一处例外是 `floor(unix_timestamp(ts))` 作为分区表达式，TiDB 针对这个场景做了特殊处理，可以支持分区裁剪。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    create table t (ts timestamp(3) not null default current_timestamp(3))\n    partition by range (floor(unix_timestamp(ts))) (\n        partition p0 values less than (unix_timestamp('2020-04-01 00:00:00')),\n        partition p1 values less than (unix_timestamp('2020-05-01 00:00:00')));\n    select * from t where ts > '2020-04-18 02:00:42.123';\n    ```\n\n## 分区选择\n\nSELECT 语句中支持分区选择。实现通过使用一个 `PARTITION` 选项实现。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET @@sql_mode = '';\n\nCREATE TABLE employees  (\n    id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    fname VARCHAR(25) NOT NULL,\n    lname VARCHAR(25) NOT NULL,\n    store_id INT NOT NULL,\n    department_id INT NOT NULL\n)\n\nPARTITION BY RANGE(id)  (\n    PARTITION p0 VALUES LESS THAN (5),\n    PARTITION p1 VALUES LESS THAN (10),\n    PARTITION p2 VALUES LESS THAN (15),\n    PARTITION p3 VALUES LESS THAN MAXVALUE\n);\n\nINSERT INTO employees VALUES\n    ('', 'Bob', 'Taylor', 3, 2), ('', 'Frank', 'Williams', 1, 2),\n    ('', 'Ellen', 'Johnson', 3, 4), ('', 'Jim', 'Smith', 2, 4),\n    ('', 'Mary', 'Jones', 1, 1), ('', 'Linda', 'Black', 2, 3),\n    ('', 'Ed', 'Jones', 2, 1), ('', 'June', 'Wilson', 3, 1),\n    ('', 'Andy', 'Smith', 1, 3), ('', 'Lou', 'Waters', 2, 4),\n    ('', 'Jill', 'Stone', 1, 4), ('', 'Roger', 'White', 3, 2),\n    ('', 'Howard', 'Andrews', 1, 2), ('', 'Fred', 'Goldberg', 3, 3),\n    ('', 'Barbara', 'Brown', 2, 3), ('', 'Alice', 'Rogers', 2, 2),\n    ('', 'Mark', 'Morgan', 3, 3), ('', 'Karen', 'Cole', 3, 2);\n```\n\n你可以查看存储在分区 `p1` 中的行：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM employees PARTITION (p1);\n```\n\n```\n+----|-------|--------|----------|---------------+\n| id | fname | lname  | store_id | department_id |\n+----|-------|--------|----------|---------------+\n|  5 | Mary  | Jones  |        1 |             1 |\n|  6 | Linda | Black  |        2 |             3 |\n|  7 | Ed    | Jones  |        2 |             1 |\n|  8 | June  | Wilson |        3 |             1 |\n|  9 | Andy  | Smith  |        1 |             3 |\n+----|-------|--------|----------|---------------+\n5 rows in set (0.00 sec)\n```\n\n如果希望获得多个分区中的行，可以提供分区名的列表，用逗号隔开。例如，`SELECT * FROM employees PARTITION (p1, p2)` 返回分区 `p1` 和 `p2` 的所有行。\n\n使用分区选择时，仍然可以使用 where 条件，以及 ORDER BY 和 LIMIT 等选项。使用 HAVING 和 GROUP BY 等聚合选项也是支持的。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM employees PARTITION (p0, p2)\n    WHERE lname LIKE 'S%';\n```\n\n```\n+----|-------|-------|----------|---------------+\n| id | fname | lname | store_id | department_id |\n+----|-------|-------|----------|---------------+\n|  4 | Jim   | Smith |        2 |             4 |\n| 11 | Jill  | Stone |        1 |             4 |\n+----|-------|-------|----------|---------------+\n2 rows in set (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT id, CONCAT(fname, ' ', lname) AS name\n    FROM employees PARTITION (p0) ORDER BY lname;\n```\n\n```\n+----|----------------+\n| id | name           |\n+----|----------------+\n|  3 | Ellen Johnson  |\n|  4 | Jim Smith      |\n|  1 | Bob Taylor     |\n|  2 | Frank Williams |\n+----|----------------+\n4 rows in set (0.06 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT store_id, COUNT(department_id) AS c\n    FROM employees PARTITION (p1,p2,p3)\n    GROUP BY store_id HAVING c > 4;\n```\n\n```\n+---|----------+\n| c | store_id |\n+---|----------+\n| 5 |        2 |\n| 5 |        3 |\n+---|----------+\n2 rows in set (0.00 sec)\n```\n\n分支选择支持所有类型的分区表，无论是 Range 分区或是 Hash 分区等。对于 Hash 分区，如果没有指定分区名，会自动使用 `p0`、`p1`、`p2`、……、或 `pN-1` 作为分区名。\n\n在 `INSERT ... SELECT` 的 `SELECT` 中也是可以使用分区选择的。\n\n## 分区的约束和限制\n\n本节介绍当前 TiDB 分区表的一些约束和限制。\n\n- 不支持使用 [`ALTER TABLE ... CHANGE COLUMN`](/sql-statements/sql-statement-change-column.md) 语句更改分区表的列类型。\n- 不支持使用 [`ALTER TABLE ... CACHE`](/cached-tables.md) 语句将分区表设为缓存表。\n- 与 TiDB 的[临时表](/temporary-tables.md)功能不兼容。\n- 不支持在分区表上创建[外键](/foreign-key.md)。\n- [`ORDER_INDEX(t1_name, idx1_name [, idx2_name ...])`](/optimizer-hints.md#order_indext1_name-idx1_name--idx2_name-) Hint 对分区表及其相关索引不生效，因为分区表上的索引不支持按顺序读取。\n\n### 分区键，主键和唯一键\n\n本节讨论分区键，主键和唯一键之间的关系。一句话总结它们之间的关系要满足的规则：**分区表的每个唯一键，必须包含分区表达式中用到的所有列**。\n\n> **注意：**\n>\n> 使用[全局索引](#全局索引)时，可以忽略该规则。\n\n这里所指的唯一也包含了主键，因为根据主键的定义，主键必须是唯一的。例如，下面这些建表语句就是无效的：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY (col1, col2)\n)\n\nPARTITION BY HASH(col3)\nPARTITIONS 4;\n\nCREATE TABLE t2 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY (col1),\n    UNIQUE KEY (col3)\n)\n\nPARTITION BY HASH(col1 + col3)\nPARTITIONS 4;\n```\n\n它们都是有唯一键但没有包含所有分区键的。\n\n下面是一些合法的语句的例子：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY (col1, col2, col3)\n)\n\nPARTITION BY HASH(col3)\nPARTITIONS 4;\n\nCREATE TABLE t2 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY (col1, col3)\n)\n\nPARTITION BY HASH(col1 + col3)\nPARTITIONS 4;\n```\n\n下例中会产生一个报错：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t3 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY (col1, col2),\n    UNIQUE KEY (col3)\n)\n\nPARTITION BY HASH(col1 + col3)\n    PARTITIONS 4;\n```\n\n```\nERROR 8264 (HY000): Global Index is needed for index 'col1', since the unique index is not including all partitioning columns, and GLOBAL is not given as IndexOption\n```\n\n原因是 `col1` 和 `col3` 出现在分区键中，但是几个唯一键定义并没有完全包含它们，做如下修改后语句即为合法：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t3 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY (col1, col2, col3),\n    UNIQUE KEY (col1, col3)\n)\n\nPARTITION BY HASH(col1 + col3)\n    PARTITIONS 4;\n```\n\n下面这个表就没法做分区了，因为无论如何都不可能找到满足条件的分区键：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t4 (\n    col1 INT NOT NULL,\n    col2 INT NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY (col1, col3),\n    UNIQUE KEY (col2, col4)\n);\n```\n\n根据定义，主键也是唯一键，下面两个建表语句是无效的：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t5 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    PRIMARY KEY(col1, col2)\n)\n\nPARTITION BY HASH(col3)\nPARTITIONS 4;\n\nCREATE TABLE t6 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    PRIMARY KEY(col1, col3),\n    UNIQUE KEY(col2)\n)\n\nPARTITION BY HASH( YEAR(col2) )\nPARTITIONS 4;\n```\n\n以上两个例子中，主键都没有包含分区表达式中的全部的列，在主键中补充缺失列后语句即为合法：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t5 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    PRIMARY KEY(col1, col2, col3)\n)\n\nPARTITION BY HASH(col3)\nPARTITIONS 4;\n\nCREATE TABLE t6 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    PRIMARY KEY(col1, col2, col3),\n    UNIQUE KEY(col2)\n)\n\nPARTITION BY HASH( YEAR(col2) )\nPARTITIONS 4;\n```\n\n如果既没有主键，也没有唯一键，则不存在这个限制。\n\nDDL 变更时，添加唯一索引也需要考虑到这个限制。比如创建了这样一个表：\n\n```sql\nCREATE TABLE t_no_pk (c1 INT, c2 INT)\n    PARTITION BY RANGE(c1) (\n        PARTITION p0 VALUES LESS THAN (10),\n        PARTITION p1 VALUES LESS THAN (20),\n        PARTITION p2 VALUES LESS THAN (30),\n        PARTITION p3 VALUES LESS THAN (40)\n    );\n```\n\n```\nQuery OK, 0 rows affected (0.12 sec)\n```\n\n通过 `ALTER TABLE` 添加非唯一索引是可以的。但是添加唯一索引时，唯一索引里面必须包含 `c1` 列。\n\n使用分区表时，前缀索引是不能指定为唯一属性的：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t (a varchar(20), b blob,\n    UNIQUE INDEX (a(5)))\n    PARTITION by range columns (a) (\n    PARTITION p0 values less than ('aaaaa'),\n    PARTITION p1 values less than ('bbbbb'),\n    PARTITION p2 values less than ('ccccc'));\n```\n\n```sql\nERROR 8264 (HY000): Global Index is needed for index 'a', since the unique index is not including all partitioning columns, and GLOBAL is not given as IndexOption\n```\n\n#### 全局索引\n\n在引入全局索引 (Global Index) 之前，TiDB 会为每个分区创建一个局部索引 (Local Index)，即一个分区对应一个局部索引。这种索引方式存在一个[使用限制](#分区键主键和唯一键)：主键和唯一键必须包含所有的分区键，以确保数据的全局唯一性。此外，当查询的数据跨越多个分区时，TiDB 需要扫描各个分区的数据才能返回结果。\n\n为解决这些问题，TiDB 从 v8.3.0 开始引入全局索引。全局索引能覆盖整个表的数据，使得主键和唯一键在不包含分区键的情况下仍能保持全局唯一性。此外，全局索引可以在一次操作中访问多个分区的索引数据，而无需对每个分区的本地索引逐一查找，显著提升了针对非分区键的查询性能。\n\n如果你需要为主键或唯一键创建全局索引，可以通过在索引定义中添加 `GLOBAL` 关键字来实现。\n\n> **注意：**\n>\n> 全局索引对分区管理有影响，执行 `DROP`、`TRUNCATE` 和 `REORGANIZE PARTITION` 操作也会触发表级别全局索引的更新，这意味着这些 DDL 操作只有在对应表的全局索引完全更新后才会返回结果。\n\n```sql\nCREATE TABLE t1 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    col3 INT NOT NULL,\n    col4 INT NOT NULL,\n    UNIQUE KEY uidx12(col1, col2) GLOBAL,\n    UNIQUE KEY uidx3(col3)\n)\nPARTITION BY HASH(col3)\nPARTITIONS 4;\n```\n\n在上面示例中，唯一索引 `uidx12` 将成为全局索引，但 `uidx3` 仍是常规的唯一索引。\n\n请注意，**聚簇索引**不能成为全局索引，如下例所示：\n\n```sql\nCREATE TABLE t2 (\n    col1 INT NOT NULL,\n    col2 DATE NOT NULL,\n    PRIMARY KEY (col2) CLUSTERED GLOBAL\n) PARTITION BY HASH(col1) PARTITIONS 5;\n```\n\n```\nERROR 1503 (HY000): A CLUSTERED INDEX must include all columns in the table's partitioning function\n```\n\n聚簇索引不能成为全局索引，是因为如果聚簇索引是全局索引，则表将不再分区。这是因为聚簇索引的键是分区级别的行数据的键，但全局索引是表级别的，这就造成了冲突。如果需要将主键设置为全局索引，则需要显式设置该主键为非聚簇索引，如 `PRIMARY KEY(col1, col2) NONCLUSTERED GLOBAL`。\n\n你可以通过 [`SHOW CREATE TABLE`](/sql-statements/sql-statement-show-create-table.md) 输出中的 `GLOBAL` 索引选项来识别全局索引。\n\n```sql\nSHOW CREATE TABLE t1\\G\n```\n\n```\n       Table: t1\nCreate Table: CREATE TABLE `t1` (\n  `col1` int NOT NULL,\n  `col2` date NOT NULL,\n  `col3` int NOT NULL,\n  `col4` int NOT NULL,\n  UNIQUE KEY `uidx12` (`col1`,`col2`) /*T![global_index] GLOBAL */,\n  UNIQUE KEY `uidx3` (`col3`)\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin\nPARTITION BY HASH (`col3`) PARTITIONS 4\n1 row in set (0.00 sec)\n```\n\n或查询 [`INFORMATION_SCHEMA.TIDB_INDEXES`](/information-schema/information-schema-tidb-indexes.md) 表并查看输出中的 `IS_GLOBAL` 列来识别全局索引。\n\n```sql\nSELECT * FROM information_schema.tidb_indexes WHERE table_name='t1';\n```\n\n```\n+--------------+------------+------------+----------+--------------+-------------+----------+---------------+------------+----------+------------+-----------+-----------+\n| TABLE_SCHEMA | TABLE_NAME | NON_UNIQUE | KEY_NAME | SEQ_IN_INDEX | COLUMN_NAME | SUB_PART | INDEX_COMMENT | Expression | INDEX_ID | IS_VISIBLE | CLUSTERED | IS_GLOBAL |\n+--------------+------------+------------+----------+--------------+-------------+----------+---------------+------------+----------+------------+-----------+-----------+\n| test         | t1         |          0 | uidx12   |            1 | col1        |     NULL |               | NULL       |        1 | YES        | NO        |         1 |\n| test         | t1         |          0 | uidx12   |            2 | col2        |     NULL |               | NULL       |        1 | YES        | NO        |         1 |\n| test         | t1         |          0 | uidx3    |            1 | col3        |     NULL |               | NULL       |        2 | YES        | NO        |         0 |\n+--------------+------------+------------+----------+--------------+-------------+----------+---------------+------------+----------+------------+-----------+-----------+\n3 rows in set (0.00 sec)\n```\n\n在对未分区的表进行分区，或对已分区的表进行重新分区时，可以根据需要将索引更新为全局索引或将其还原为本地索引：\n\n```sql\nALTER TABLE t1 PARTITION BY HASH (col1) PARTITIONS 3 UPDATE INDEXES (uidx12 LOCAL, uidx3 GLOBAL);\n```\n\n##### 全局索引的限制\n\n- 如果索引定义中未显式指定 `GLOBAL` 关键字，TiDB 将默认创建局部索引 (Local Index)。\n- `GLOBAL` 和 `LOCAL` 关键字仅适用于分区表，对非分区表没有影响。即在非分区表中，全局索引和局部索引之间没有区别。\n- 当前仅支持为唯一列创建全局索引 (Unique Global Index)。如果需要对非唯一列创建全局索引，可以通过包含主键形成复合索引。例如，如果非唯一列是 `col3` 而主键是 `col1`，可以通过执行以下 SQL 语句为 `col3` 创建全局索引：\n  \n    ```sql\n    ALTER TABLE ... ADD UNIQUE INDEX(col3, col1) GLOBAL;\n    ```\n\n- 以下 DDL 操作会触发全局索引的更新：`DROP PARTITION`、`TRUNCATE PARTITION` 和 `REORGANIZE PARTITION`。这些 DDL 需等待全局索引更新完成后才会返回结果，耗时会相应增加。尤其是在数据归档场景下，如 `DROP PARTITION` 和 `TRUNCATE PARTITION`，若没有全局索引，通常可以立即完成；但使用全局索引后，耗时会随着所需更新的索引数量的增加而增加。\n- 包含全局索引的表不支持 `EXCHANGE PARTITION`。\n- 默认情况下，分区表的主键为聚簇索引，且必须包含分区键。如果要求主键不包含分区建，可以在建表时显式指定主键为非聚簇的全局索引，例如：`PRIMARY KEY(col1, col2) NONCLUSTERED GLOBAL`。\n- 如果在表达式列上添加了全局索引，或者一个全局索引同时也是前缀索引（如 `UNIQUE KEY idx_id_prefix (id(10)) GLOBAL`），你需要为该全局索引手动收集统计信息。\n\n### 关于函数的分区限制\n\n只有以下函数可以用于分区表达式：\n\n```\nABS()\nCEILING()\nDATEDIFF()\nDAY()\nDAYOFMONTH()\nDAYOFWEEK()\nDAYOFYEAR()\nEXTRACT() (see EXTRACT() function with WEEK specifier)\nFLOOR()\nHOUR()\nMICROSECOND()\nMINUTE()\nMOD()\nMONTH()\nQUARTER()\nSECOND()\nTIME_TO_SEC()\nTO_DAYS()\nTO_SECONDS()\nUNIX_TIMESTAMP() (with TIMESTAMP columns)\nWEEKDAY()\nYEAR()\nYEARWEEK()\n```\n\n### 兼容性\n\n目前 TiDB 支持 Range 分区、Range Columns 分区、List 分区、List COLUMNS 分区、Hash 分区和 Key 分区，其它的 MySQL 分区类型尚不支持。\n\n分区管理方面，只要底层实现可能会涉及数据挪动的操作，目前都暂不支持。包括且不限于：调整 Hash 分区表的分区数量，修改 Range 分区表的范围，合并分区等。\n\n对于暂不支持的分区类型，在 TiDB 中建表时会忽略分区信息，以普通表的形式创建，并且会报 Warning。\n\nLoad Data 暂时不支持分区选择。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (id int, val int) partition by hash(id) partitions 4;\n```\n\n普通的 Load Data 操作在 TiDB 中是支持的，如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nload local data infile \"xxx\" into t ...\n```\n\n但 Load Data 不支持分区选择操作：\n\n{{< copyable \"sql\" >}}\n\n```sql\nload local data infile \"xxx\" into t partition (p1)...\n```\n\n对于分区表，`select * from t` 的返回结果是分区之间无序的。这跟 MySQL 不同，MySQL 的返回结果是分区之间有序，分区内部无序。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t (id int, val int) partition by range (id) (\n    partition p0 values less than (3),\n    partition p1 values less than (7),\n    partition p2 values less than (11));\n```\n\n```\nQuery OK, 0 rows affected (0.10 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\ninsert into t values (1, 2), (3, 4),(5, 6),(7,8),(9,10);\n```\n\n```\nQuery OK, 5 rows affected (0.01 sec)\nRecords: 5  Duplicates: 0  Warnings: 0\n```\n\nTiDB 每次返回结果会不同，例如：\n\n{{< copyable \"sql\" >}}\n\n```\nselect * from t;\n```\n\n```\n+------|------+\n| id   | val  |\n+------|------+\n|    7 |    8 |\n|    9 |   10 |\n|    1 |    2 |\n|    3 |    4 |\n|    5 |    6 |\n+------|------+\n5 rows in set (0.00 sec)\n```\n\nMySQL 的返回结果：\n\n{{< copyable \"sql\" >}}\n\n```\nselect * from t;\n```\n\n```\n+------|------+\n| id   | val  |\n+------|------+\n|    1 |    2 |\n|    3 |    4 |\n|    5 |    6 |\n|    7 |    8 |\n|    9 |   10 |\n+------|------+\n5 rows in set (0.00 sec)\n```\n\n### 动态裁剪模式\n\nTiDB 访问分区表有两种模式，`dynamic` 和 `static`。从 v6.3.0 开始，默认使用 `dynamic` 模式。但是注意，`dynamic` 模式仅在表级别汇总统计信息（即分区表的全局统计信息）收集完成的情况下生效。如果在全局统计信息未收集完成的情况下启用 `dynamic` 动态裁剪模式，TiDB 仍然会维持 `static` 静态裁剪的状态，直到全局统计信息收集完成。关于全局统计信息的更多信息，请参考[动态裁剪模式下的分区表统计信息](/statistics.md#收集动态裁剪模式下的分区表统计信息)。\n\n{{< copyable \"sql\" >}}\n\n```sql\nset @@session.tidb_partition_prune_mode = 'dynamic'\n```\n\n普通查询和手动 analyze 使用的是 session 级别的 `tidb_partition_prune_mode` 设置，后台的 auto-analyze 使用的是 global 级别的 `tidb_partition_prune_mode` 设置。\n\n静态裁剪模式下，分区表使用的是分区级别的统计信息，而动态裁剪模式下，分区表用的是表级别的汇总统计信息。\n\n从 `static` 静态裁剪模式切到 `dynamic` 动态裁剪模式时，需要手动检查和收集统计信息。在刚切换到 `dynamic` 时，分区表上仍然只有分区的统计信息，需要等到全局 `dynamic` 动态裁剪模式开启后的下一次 `auto-analyze` 周期，才会更新生成汇总统计信息。\n\n{{< copyable \"sql\" >}}\n\n```sql\nset session tidb_partition_prune_mode = 'dynamic';\nshow stats_meta where table_name like \"t\";\n```\n\n```\n+---------+------------+----------------+---------------------+--------------+-----------+\n| Db_name | Table_name | Partition_name | Update_time         | Modify_count | Row_count |\n+---------+------------+----------------+---------------------+--------------+-----------+\n| test    | t          | p0             | 2022-05-27 20:23:34 |            1 |         2 |\n| test    | t          | p1             | 2022-05-27 20:23:34 |            2 |         4 |\n| test    | t          | p2             | 2022-05-27 20:23:34 |            2 |         4 |\n+---------+------------+----------------+---------------------+--------------+-----------+\n3 rows in set (0.01 sec)\n```\n\n为保证开启全局 `dynamic` 动态裁剪模式时，SQL 可以用上正确的统计信息，此时需要手动触发一次 `analyze` 来更新汇总统计信息，可以通过 `analyze` 表或者单个分区来更新。\n\n{{< copyable \"sql\" >}}\n\n```sql\nanalyze table t partition p1;\nshow stats_meta where table_name like \"t\";\n```\n\n```\n+---------+------------+----------------+---------------------+--------------+-----------+\n| Db_name | Table_name | Partition_name | Update_time         | Modify_count | Row_count |\n+---------+------------+----------------+---------------------+--------------+-----------+\n| test    | t          | global         | 2022-05-27 20:50:53 |            0 |         5 |\n| test    | t          | p0             | 2022-05-27 20:23:34 |            1 |         2 |\n| test    | t          | p1             | 2022-05-27 20:50:52 |            0 |         2 |\n| test    | t          | p2             | 2022-05-27 20:50:08 |            0 |         2 |\n+---------+------------+----------------+---------------------+--------------+-----------+\n4 rows in set (0.00 sec)\n```\n\n若 analyze 过程中提示如下 warning，说明分区的统计信息之间存在不一致，需要重新收集分区或整个表统计信息。\n\n```\n| Warning | 8244 | Build table: `t` column: `a` global-level stats failed due to missing partition-level column stats, please run analyze table to refresh columns of all partitions\n```\n\n也可以使用脚本来统一更新所有的分区表统计信息，详见[为动态裁剪模式更新所有分区表的统计信息](/partitioned-table.md#为动态裁剪模式更新所有分区表的统计信息)。\n\n表级别统计信息准备好后，即可开启全局的动态裁剪模式。全局动态裁剪模式，对全局所有的 SQL 和对后台的统计信息自动收集（即 auto analyze）起作用。\n\n{{< copyable \"sql\" >}}\n\n```sql\nset global tidb_partition_prune_mode = dynamic\n```\n\n在 `static` 模式下，TiDB 用多个算子单独访问每个分区，然后通过 Union 将结果合并起来。下面例子进行了一个简单的读取操作，可以发现 TiDB 用 Union 合并了对应两个分区的结果：\n\n{{< copyable \"sql\" >}}\n\n```sql\nmysql> create table t1(id int, age int, key(id)) partition by range(id) (\n          partition p0 values less than (100),\n          partition p1 values less than (200),\n          partition p2 values less than (300),\n          partition p3 values less than (400));\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> explain select * from t1 where id < 150;\n```\n\n```\n+------------------------------+----------+-----------+------------------------+--------------------------------+\n| id                           | estRows  | task      | access object          | operator info                  |\n+------------------------------+----------+-----------+------------------------+--------------------------------+\n| PartitionUnion_9             | 6646.67  | root      |                        |                                |\n| ├─TableReader_12             | 3323.33  | root      |                        | data:Selection_11              |\n| │ └─Selection_11             | 3323.33  | cop[tikv] |                        | lt(test.t1.id, 150)            |\n| │   └─TableFullScan_10       | 10000.00 | cop[tikv] | table:t1, partition:p0 | keep order:false, stats:pseudo |\n| └─TableReader_18             | 3323.33  | root      |                        | data:Selection_17              |\n|   └─Selection_17             | 3323.33  | cop[tikv] |                        | lt(test.t1.id, 150)            |\n|     └─TableFullScan_16       | 10000.00 | cop[tikv] | table:t1, partition:p1 | keep order:false, stats:pseudo |\n+------------------------------+----------+-----------+------------------------+--------------------------------+\n7 rows in set (0.00 sec)\n```\n\n在 `dynamic` 模式下，每个算子都支持直接访问多个分区，所以 TiDB 不再使用 Union。\n\n{{< copyable \"sql\" >}}\n\n```sql\nmysql> set @@session.tidb_partition_prune_mode = 'dynamic';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> explain select * from t1 where id < 150;\n+-------------------------+----------+-----------+-----------------+--------------------------------+\n| id                      | estRows  | task      | access object   | operator info                  |\n+-------------------------+----------+-----------+-----------------+--------------------------------+\n| TableReader_7           | 3323.33  | root      | partition:p0,p1 | data:Selection_6               |\n| └─Selection_6           | 3323.33  | cop[tikv] |                 | lt(test.t1.id, 150)            |\n|   └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t1        | keep order:false, stats:pseudo |\n+-------------------------+----------+-----------+-----------------+--------------------------------+\n3 rows in set (0.00 sec)\n```\n\n从以上查询结果可知，执行计划中的 Union 消失了，分区裁剪依然生效，且执行计划只访问了 `p0` 和 `p1` 两个分区。\n\n`dynamic` 模式让执行计划更简单清晰，省略 Union 操作可提高执行效率，还可避免 Union 并发管理的问题。此外 `dynamic` 模式下，执行计划可以使用 IndexJoin 的方式，这在 `static` 模式下是无法实现的。请看下面的例子：\n\n**示例一**：以下示例在 `static` 模式下执行计划带 IndexJoin 的查询。\n\n{{< copyable \"sql\" >}}\n\n```sql\nmysql> create table t1 (id int, age int, key(id)) partition by range(id)\n          (partition p0 values less than (100),\n           partition p1 values less than (200),\n           partition p2 values less than (300),\n           partition p3 values less than (400));\nQuery OK, 0 rows affected (0,08 sec)\nmysql> create table t2 (id int, code int);\n\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> set @@tidb_partition_prune_mode = 'static';\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n\nmysql> explain select /*+ TIDB_INLJ(t1, t2) */ t1.* from t1, t2 where t2.code = 0 and t2.id = t1.id;\n+--------------------------------+----------+-----------+------------------------+------------------------------------------------+\n| id                             | estRows  | task      | access object          | operator info                                  |\n+--------------------------------+----------+-----------+------------------------+------------------------------------------------+\n| HashJoin_13                    | 12.49    | root      |                        | inner join, equal:[eq(test.t1.id, test.t2.id)] |\n| ├─TableReader_42(Build)        | 9.99     | root      |                        | data:Selection_41                              |\n| │ └─Selection_41               | 9.99     | cop[tikv] |                        | eq(test.t2.code, 0), not(isnull(test.t2.id))   |\n| │   └─TableFullScan_40         | 10000.00 | cop[tikv] | table:t2               | keep order:false, stats:pseudo                 |\n| └─PartitionUnion_15(Probe)     | 39960.00 | root      |                        |                                                |\n|   ├─TableReader_18             | 9990.00  | root      |                        | data:Selection_17                              |\n|   │ └─Selection_17             | 9990.00  | cop[tikv] |                        | not(isnull(test.t1.id))                        |\n|   │   └─TableFullScan_16       | 10000.00 | cop[tikv] | table:t1, partition:p0 | keep order:false, stats:pseudo                 |\n|   ├─TableReader_24             | 9990.00  | root      |                        | data:Selection_23                              |\n|   │ └─Selection_23             | 9990.00  | cop[tikv] |                        | not(isnull(test.t1.id))                        |\n|   │   └─TableFullScan_22       | 10000.00 | cop[tikv] | table:t1, partition:p1 | keep order:false, stats:pseudo                 |\n|   ├─TableReader_30             | 9990.00  | root      |                        | data:Selection_29                              |\n|   │ └─Selection_29             | 9990.00  | cop[tikv] |                        | not(isnull(test.t1.id))                        |\n|   │   └─TableFullScan_28       | 10000.00 | cop[tikv] | table:t1, partition:p2 | keep order:false, stats:pseudo                 |\n|   └─TableReader_36             | 9990.00  | root      |                        | data:Selection_35                              |\n|     └─Selection_35             | 9990.00  | cop[tikv] |                        | not(isnull(test.t1.id))                        |\n|       └─TableFullScan_34       | 10000.00 | cop[tikv] | table:t1, partition:p3 | keep order:false, stats:pseudo                 |\n+--------------------------------+----------+-----------+------------------------+------------------------------------------------+\n17 rows in set, 1 warning (0.00 sec)\n\nmysql> show warnings;\n+---------+------+------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                            |\n+---------+------+------------------------------------------------------------------------------------+\n| Warning | 1815 | Optimizer Hint /*+ INL_JOIN(t1, t2) */ or /*+ TIDB_INLJ(t1, t2) */ is inapplicable |\n+---------+------+------------------------------------------------------------------------------------+\n1 row in set (0,00 sec)\n```\n\n从以上示例一结果可知，即使使用了 `TIDB_INLJ` 的 hint，也无法使得带分区表的查询选上带 IndexJoin 的执行计划。\n\n**示例二**：以下示例在 `dynamic` 模式下尝试执行计划带 IndexJoin 的查询。\n\n{{< copyable \"sql\" >}}\n\n```sql\nmysql> set @@tidb_partition_prune_mode = 'dynamic';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> explain select /*+ TIDB_INLJ(t1, t2) */ t1.* from t1, t2 where t2.code = 0 and t2.id = t1.id;\n+---------------------------------+----------+-----------+------------------------+---------------------------------------------------------------------------------------------------------------------+\n| id                              | estRows  | task      | access object          | operator info                                                                                                       |\n+---------------------------------+----------+-----------+------------------------+---------------------------------------------------------------------------------------------------------------------+\n| IndexJoin_11                    | 12.49    | root      |                        | inner join, inner:IndexLookUp_10, outer key:test.t2.id, inner key:test.t1.id, equal cond:eq(test.t2.id, test.t1.id) |\n| ├─TableReader_16(Build)         | 9.99     | root      |                        | data:Selection_15                                                                                                   |\n| │ └─Selection_15                | 9.99     | cop[tikv] |                        | eq(test.t2.code, 0), not(isnull(test.t2.id))                                                                        |\n| │   └─TableFullScan_14          | 10000.00 | cop[tikv] | table:t2               | keep order:false, stats:pseudo                                                                                      |\n| └─IndexLookUp_10(Probe)         | 12.49    | root      | partition:all          |                                                                                                                     |\n|   ├─Selection_9(Build)          | 12.49    | cop[tikv] |                        | not(isnull(test.t1.id))                                                                                             |\n|   │ └─IndexRangeScan_7          | 12.50    | cop[tikv] | table:t1, index:id(id) | range: decided by [eq(test.t1.id, test.t2.id)], keep order:false, stats:pseudo                                      |\n|   └─TableRowIDScan_8(Probe)     | 12.49    | cop[tikv] | table:t1               | keep order:false, stats:pseudo                                                                                      |\n+---------------------------------+----------+-----------+------------------------+---------------------------------------------------------------------------------------------------------------------+\n8 rows in set (0.00 sec)\n```\n\n从示例二结果可知，开启 `dynamic` 模式后，带 IndexJoin 的计划在执行查询时被选上。\n\n目前，静态裁剪模式不支持执行计划缓存，包括 Prepare 语句和非 Prepare 语句。\n\n#### 为动态裁剪模式更新所有分区表的统计信息\n\n1. 找到所有的分区表：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT DISTINCT CONCAT(TABLE_SCHEMA,'.', TABLE_NAME)\n        FROM information_schema.PARTITIONS\n        WHERE TIDB_PARTITION_ID IS NOT NULL\n        AND TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA', 'mysql', 'sys', 'PERFORMANCE_SCHEMA', 'METRICS_SCHEMA');\n    ```\n\n    ```\n    +-------------------------------------+\n    | concat(TABLE_SCHEMA,'.',TABLE_NAME) |\n    +-------------------------------------+\n    | test.t                              |\n    +-------------------------------------+\n    1 row in set (0.02 sec)\n    ```\n\n2. 生成所有分区表的更新统计信息的语句：\n\n    ```sql\n    SELECT DISTINCT CONCAT('ANALYZE TABLE ',TABLE_SCHEMA,'.',TABLE_NAME,' ALL COLUMNS;')\n        FROM information_schema.PARTITIONS\n        WHERE TIDB_PARTITION_ID IS NOT NULL\n        AND TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA','mysql','sys','PERFORMANCE_SCHEMA','METRICS_SCHEMA');\n    ```\n\n    ```\n    +----------------------------------------------------------------------+\n    | concat('ANALYZE TABLE ',TABLE_SCHEMA,'.',TABLE_NAME,' ALL COLUMNS;') |\n    +----------------------------------------------------------------------+\n    | ANALYZE TABLE test.t ALL COLUMNS;                                    |\n    +----------------------------------------------------------------------+\n    1 row in set (0.01 sec)\n    ```\n\n    可以按需将 `ALL COLUMNS` 改为实际需要的列。\n\n3. 将批量更新语句导出到文件：\n\n    ```shell\n    mysql --host xxxx --port xxxx -u root -p -e \"SELECT DISTINCT CONCAT('ANALYZE TABLE ',TABLE_SCHEMA,'.',TABLE_NAME,' ALL COLUMNS;') \\\n        FROM information_schema.PARTITIONS \\\n        WHERE TIDB_PARTITION_ID IS NOT NULL \\\n        AND TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA','mysql','sys','PERFORMANCE_SCHEMA','METRICS_SCHEMA');\" | tee gatherGlobalStats.sql\n    ```\n\n4. 执行批量更新：\n\n    在运行 source 命令之前处理 SQL 文件：\n\n    ```\n    sed -i \"\" '1d' gatherGlobalStats.sql --- mac\n    sed -i '1d' gatherGlobalStats.sql --- linux\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SET session tidb_partition_prune_mode = dynamic;\n    source gatherGlobalStats.sql\n    ```\n"
        },
        {
          "name": "password-management.md",
          "type": "blob",
          "size": 19.1513671875,
          "content": "---\ntitle: TiDB 密码管理\nsummary: 了解 TiDB 的用户密码管理机制。\n---\n\n# TiDB 密码管理\n\n为了保护用户密码的安全，从 TiDB v6.5.0 开始支持密码管理能力：\n\n- 密码复杂度策略：要求用户设置强密码，以防止出现空密码、弱密码。\n- 密码过期策略：要求用户定期修改密码。\n- 密码重用策略：限制用户重复使用旧密码。\n- 密码连续错误限制登录策略：连续多次密码错误导致登录失败后，临时锁定用户，限制该用户继续尝试登录。\n\n## TiDB 身份验证凭据存储\n\n密码作为一种用户身份凭据，在用户登录到服务端时用于身份验证，确保用户身份的合法性。本文中描述的密码是指由 TiDB 生成、存储、验证的内部凭据，TiDB 将用户密码被存储到 `mysql.user` 系统表中，以下身份验证插件涉及本文的密码管理功能：\n\n- `mysql_native_password`\n- `caching_sha2_password`\n- `tidb_sm3_password`\n\n有关 TiDB 支持身份验证插件的更多信息，请查看[`可用的身份验证插件`](/security-compatibility-with-mysql.md#可用的身份验证插件)。\n\n## 密码复杂度策略\n\n在 TiDB 中，密码复杂度检查默认未开启。通过配置密码复杂度相关的系统变量，你可以开启密码复杂度检查，并确保为账户设置的密码符合密码复杂度策略。\n\n密码复杂度策略支持以下功能：\n\n- 对采用明文方式设置用户密码的 SQL 语句（包括 `CREATE USER`、`ALTER USER`、`SET PASSWORD` ），系统会根据密码复杂度策略检查该密码，如果该密码不符合要求，则拒绝该密码。\n- 可以使用 SQL 函数 [`VALIDATE_PASSWORD_STRENGTH()`](/functions-and-operators/encryption-and-compression-functions.md#validate_password_strength) 评估给定密码的强度。\n\n> **注意：**\n>\n> - 对于 `CREATE USER` 语句，即使该账户最初被锁定，也必须提供满足密码复杂度策略的密码，否则将账户解锁后，该账户可以使用不符合密码复杂度策略的密码访问 TiDB。\n> - 对密码复杂度策略的变更不影响已存在的密码，只会对新设置的密码产生影响。\n\n通过以下 SQL 语句，你可以查看所有密码复杂度策略相关的系统变量：\n\n```sql\nmysql> SHOW VARIABLES LIKE 'validate_password.%';\n+--------------------------------------+--------+\n| Variable_name                        | Value  |\n+--------------------------------------+--------+\n| validate_password.check_user_name    | ON     |\n| validate_password.dictionary         |        |\n| validate_password.enable             | OFF    |\n| validate_password.length             | 8      |\n| validate_password.mixed_case_count   | 1      |\n| validate_password.number_count       | 1      |\n| validate_password.policy             | MEDIUM |\n| validate_password.special_char_count | 1      |\n+--------------------------------------+--------+\n8 rows in set (0.00 sec)\n```\n\n关于这些变量的详细解释，请查阅[系统变量文档](/system-variables.md#validate_passwordcheck_user_name-从-v650-版本开始引入)。\n\n### 配置密码复杂度策略\n\n密码复杂度策略相关的系统变量的配置方式如下：\n\n开启密码复杂度策略检查：\n\n```sql\nSET GLOBAL validate_password.enable = ON;\n```\n\n设置不允许密码与当前用户名相同：\n\n```sql\nSET GLOBAL validate_password.check_user_name = ON;\n```\n\n设置密码复杂度的检查等级为 `LOW`：\n\n```sql\nSET GLOBAL validate_password.policy = LOW;\n```\n\n设置密码最小长度为 10：\n\n```sql\nSET GLOBAL validate_password.length = 10;\n```\n\n设置密码中至少含有 2 个数字，至少含有 1 个大写和小写字符，至少含有 1 个特殊字符：\n\n```sql\nSET GLOBAL validate_password.number_count = 2;\nSET GLOBAL validate_password.mixed_case_count = 1;\nSET GLOBAL validate_password.special_char_count = 1;\n```\n\n设置密码字典功能，要求密码中不允许包含 `mysql` 或 `abcd`：\n\n```sql\nSET GLOBAL validate_password.dictionary = 'mysql;abcd';\n```\n\n> **注意：**\n>\n> - `validate_password.dictionary` 是一个长字符串，长度不超过 1024，字符串内容可包含一个或多个在密码中不允许出现的单词，每个单词之间采用英文分号（`;`）分隔。\n> - 密码字典功能进行单词比较时，不区字符分大小写。\n\n### 密码复杂度检查示例\n\n配置系统变量 `validate_password.enable = ON` 后，TiDB 将开启密码复杂度检查。以下为一些典型的检查示例：\n\n按照默认密码复杂度策略，检测用户明文密码，若设置的密码不符合复杂度策略要求，则设置失败。\n\n```sql\nALTER USER 'test'@'localhost' IDENTIFIED BY 'abc';\nERROR 1819 (HY000): Require Password Length: 8\n```\n\nTiDB 进行密码复杂度检查时，不检查散列后的密码。\n\n```sql\nALTER USER 'test'@'localhost' IDENTIFIED WITH mysql_native_password AS '*0D3CED9BEC10A777AEC23CCC353A8C08A633045E';\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n创建一个最初被锁定的账户时，也必须设置符合密码复杂度策略的密码，否则创建失败。\n\n```sql\nCREATE USER 'user02'@'localhost' ACCOUNT LOCK;\nERROR 1819 (HY000): Require Password Length: 8\n```\n\n### 密码强度评估函数\n\n使用 [`VALIDATE_PASSWORD_STRENGTH()`](/functions-and-operators/encryption-and-compression-functions.md#validate_password_strength) 函数评估给定密码的强度，该函数接受一个密码参数，并返回一个从 0（弱）到 100（强）的整数。\n\n> **注意：**\n>\n> 密码强度是基于当前已配置的密码复杂度策略进行评估的，密码复杂度配置改变后，同一个密码的评估结果可能与之前不同。\n\n[`VALIDATE_PASSWORD_STRENGTH()`](/functions-and-operators/encryption-and-compression-functions.md#validate_password_strength) 函数使用示例如下：\n\n```sql\nSELECT VALIDATE_PASSWORD_STRENGTH('weak');\n+------------------------------------+\n| VALIDATE_PASSWORD_STRENGTH('weak') |\n+------------------------------------+\n|                                 25 |\n+------------------------------------+\n1 row in set (0.01 sec)\n\nSELECT VALIDATE_PASSWORD_STRENGTH('lessweak$_@123');\n+----------------------------------------------+\n| VALIDATE_PASSWORD_STRENGTH('lessweak$_@123') |\n+----------------------------------------------+\n|                                           50 |\n+----------------------------------------------+\n1 row in set (0.01 sec)\n\nSELECT VALIDATE_PASSWORD_STRENGTH('N0Tweak$_@123!');\n+----------------------------------------------+\n| VALIDATE_PASSWORD_STRENGTH('N0Tweak$_@123!') |\n+----------------------------------------------+\n|                                          100 |\n+----------------------------------------------+\n1 row in set (0.01 sec)\n```\n\n## 密码过期策略\n\nTiDB 支持通过设置密码过期策略，要求用户定期修改密码，从而提高密码的安全性。可以手动将指定账户的密码设置为过期，也可以建立密码自动过期策略。自动过期策略分为全局级别和账户级别，管理员可以在全局级别建立密码过期策略，也可以使用账户级别密码过期策略覆盖全局级别策略。设置密码过期策略的权限要求如下：\n\n- 具有 `SUPER` 或者 `CREATE USER` 权限的数据库管理员可以手动设置密码过期。\n- 具有 `SUPER` 或者 `CREATE USER` 权限的数据库管理员可以设置账户级别自动密码过期策略。\n- 具有 `SUPER` 或者 `SYSTEM_VARIABLES_ADMIN` 权限的数据库管理员可以设置全局级别自动密码过期策略。\n\n### 手动密码过期\n\n要手动设置账户密码过期，请使用 `CREATE USER` 或 `ALTER USER` 语句。\n\n```sql\nALTER USER 'test'@'localhost' PASSWORD EXPIRE;\n```\n\n当账户密码被管理员手动设置过期后，必须修改该账户密码才能解除密码过期，不支持取消手动过期。\n\n对于通过 `CREATE ROLE` 语句创建的角色，由于角色不需要设置密码，所以该角色对应的密码字段为空，此时对应的 `password_expired` 属性为 `'Y'`，即该角色的密码处于手动过期状态。如此设计的目的是防止出现角色锁定状态被解除后，该角色以空密码登录到 TiDB，当该角色被 `ALTER USER ... ACCOUNT UNLOCK` 命令解锁后，此时该角色处于可登录的状态，但是密码为空；因此 TiDB 通过 `password_expired` 属性，使得角色的密码处于手动过期状态，从而强制要求为该角色设置一个有效的密码。\n\n```sql\nCREATE ROLE testrole;\nQuery OK, 0 rows affected (0.01 sec)\n\nSELECT user,password_expired,Account_locked FROM mysql.user WHERE user = 'testrole';\n+----------+------------------+----------------+\n| user     | password_expired | Account_locked |\n+----------+------------------+----------------+\n| testrole | Y                | Y              |\n+----------+------------------+----------------+\n1 row in set (0.02 sec)\n```\n\n### 自动密码过期\n\n自动密码过期是基于**密码使用期限**和**密码被允许的生存期**来判断的。\n\n- 密码使用期限：从最近一次密码更改日期到当前日期的时间间隔。系统表 `mysql.user` 中会记录最近一次修改密码的时间。\n- 密码被允许的生存期：一个密码被设置后，其可以正常用于登录 TiDB 的天数。\n\n如果密码使用期限大于其被允许的生存期，服务器会自动将密码视为已过期。\n\nTiDB 支持在全局级别和账户级别设置自动密码过期：\n\n- 全局级别自动密码过期\n\n    你可以设置系统变量 [`default_password_lifetime`](/system-variables.md#default_password_lifetime-从-v650-版本开始引入) 来控制密码生存期。该变量默认值为 0，表示禁用自动密码过期。如果设置该变量的值为正整数 N，则表示允许的密码生存期为 N 天，即必须在 N 天之内更改密码。\n\n    全局自动密码过期策略适用于所有未设置账户级别覆盖的账户。\n\n    以下示例建立全局自动密码过期策略，密码有效期为 180 天：\n\n    ```sql\n    SET GLOBAL default_password_lifetime = 180;\n    ```\n\n- 账户级别自动密码过期\n\n    要为个人账户建立自动密码过期策略，请使用 `CREATE USER` 或 `ALTER USER` 语句的 `PASSWORD EXPIRE` 选项。\n\n    以下示例要求用户密码每 90 天更改一次：\n\n    ```sql\n    CREATE USER 'test'@'localhost' PASSWORD EXPIRE INTERVAL 90 DAY;\n    ALTER USER 'test'@'localhost' PASSWORD EXPIRE INTERVAL 90 DAY;\n    ```\n\n    在账户级别禁用自动密码过期策略：\n\n    ```sql\n    CREATE USER 'test'@'localhost' PASSWORD EXPIRE NEVER;\n    ALTER USER 'test'@'localhost' PASSWORD EXPIRE NEVER;\n    ```\n\n    移除指定账户的账户级别自动密码过期策略，使其遵循于全局自动密码过期策略：\n\n    ```sql\n    CREATE USER 'test'@'localhost' PASSWORD EXPIRE DEFAULT;\n    ALTER USER 'test'@'localhost' PASSWORD EXPIRE DEFAULT;\n    ```\n\n### 密码过期策略检查机制\n\n当客户端连接成功后，服务端将按顺序进行以下检查，判断账号密码是否过期：\n\n1. 服务器检查密码是否已手动过期。\n2. 若密码没有手动过期，服务器根据自动密码过期策略检查密码使用期限是否大于其允许的生存期。如果是，服务器认为密码已过期。\n\n### 密码过期处理机制\n\nTiDB 支持密码过期策略控制。当密码过期后，服务器要么断开客户端的连接，要么将客户端限制为“沙盒模式”。“沙盒模式”下，TiDB 服务端接受密码过期账户的连接，但是连接成功后只允许该用户执行重置密码的操作。\n\nTiDB 服务端可以控制是否将密码已过期用户的连接限制为“沙盒模式”。你可以在 TiDB 配置文件中的 `[security]` 部分，配置 [`disconnect-on-expired-password`](/tidb-configuration-file.md#disconnect-on-expired-password-从-v650-版本开始引入) 选项：\n\n```toml\n[security]\ndisconnect-on-expired-password = true\n```\n\n- 默认情况下，`disconnect-on-expired-password` 为 `true`，表示当密码过期后，服务器将直接断开客户端的连接。\n- 如果配置 `disconnect-on-expired-password` 为 `false`，则服务端处于沙盒模式，服务端允许用户建立连接，但只能执行密码重置操作，密码重置后将允许用户正常执行各类 SQL 语句。\n\n当 `disconnect-on-expired-password` 为 `true` 时，TiDB 将拒绝密码已过期用户的连接，此时可以通过如下方法修改密码：\n\n- 普通用户密码过期，可以由管理员用户通过 SQL 语句修改该用户的密码。\n- 管理员密码过期，可以由其他管理员用户通过 SQL 语句修改该用户的密码。\n- 如果管理员密码过期，且无法寻求其他管理员帮助修改该用户的密码，此时可以采用 `skip-grant-table` 机制修改该用户密码，具体可参看[忘记密码流程](/user-account-management.md#忘记-root-密码)。\n\n## 密码重用策略\n\nTiDB 支持限制重复使用以前的密码。密码重用策略可以基于密码更改的次数或经过的时间，也可以同时基于两者。密码重用策略分为全局级别和账户级别。你可以在全局级别建立密码重用策略，也可以使用账户级别密码重用策略覆盖全局策略。\n\nTiDB 会记录账户的历史密码，并限制从该历史记录中选择新密码：\n\n- 如果密码重用策略基于密码更改次数，则新密码不得与指定数量的历史密码相同。例如，如果密码的最小更改次数设置为 3，则新密码不能与最近 3 个密码中的任何一个相同。\n- 如果密码重用策略基于经过的时间，则新密码不得与历史记录中指定天数内使用过的密码相同。例如，如果密码重用间隔设置为 60，则新密码不能与最近 60 天内使用过的密码相同。\n\n> **注意：**\n>\n> 空密码不计入密码历史记录，可以随时重复使用。\n\n### 全局级别密码重用策略\n\n要在全局范围内建立密码重用策略，请使用 [`password_history`](/system-variables.md#password_history-从-v650-版本开始引入) 和 [`password_reuse_interval`](/system-variables.md#password_reuse_interval-从-v650-版本开始引入) 系统变量。\n\n例如，建立全局密码重用策略，禁止重复使用最近 6 个密码或最近 365 天的密码：\n\n```sql\nSET GLOBAL password_history = 6;\nSET GLOBAL password_reuse_interval = 365;\n```\n\n全局密码重用策略适用于所有未在账户级别设置过密码重用策略的账户。\n\n### 账户级别密码重用策略\n\n要建立账户级别密码重用策略，请使用 `CREATE USER` 或 `ALTER USER` 语句的 `PASSWORD HISTORY` 和 `PASSWORD REUSE INTERVAL` 选项。\n\n示例：\n\n禁止重复使用最近 5 次使用过的密码：\n\n```sql\nCREATE USER 'test'@'localhost' PASSWORD HISTORY 5;\nALTER USER 'test'@'localhost' PASSWORD HISTORY 5;\n```\n\n禁止重复使用最近 365 天内使用过的密码：\n\n```sql\nCREATE USER 'test'@'localhost' PASSWORD REUSE INTERVAL 365 DAY;\nALTER USER 'test'@'localhost' PASSWORD REUSE INTERVAL 365 DAY;\n```\n\n如需组合两种类型的重用策略，请一起使用 `PASSWORD HISTORY` 和 `PASSWORD REUSE INTERVAL`：\n\n```sql\nCREATE USER 'test'@'localhost'\n  PASSWORD HISTORY 5\n  PASSWORD REUSE INTERVAL 365 DAY;\nALTER USER 'test'@'localhost'\n  PASSWORD HISTORY 5\n  PASSWORD REUSE INTERVAL 365 DAY;\n```\n\n移除指定账户的账户级别密码重用策略，使其遵循于全局密码重用策略：\n\n```sql\nCREATE USER 'test'@'localhost'\n  PASSWORD HISTORY DEFAULT\n  PASSWORD REUSE INTERVAL DEFAULT;\nALTER USER 'test'@'localhost'\n  PASSWORD HISTORY DEFAULT\n  PASSWORD REUSE INTERVAL DEFAULT;\n```\n\n> **注意：**\n>\n> - 如果多次设置密码重用策略，则最后一次设置的值生效。\n> - `PASSWORD HISTORY` 和 `PASSWORD REUSE INTERVAL` 选项的默认值为 0，表示禁用该项重用策略。\n> - 在修改用户名时，TiDB 会将 `mysql.password_history` 系统表中原用户名的历史密码记录迁移到新用户名的记录中。\n\n## 密码连续错误限制登录策略\n\nTiDB 支持限制账户持续尝试登录，防止用户密码被暴力破解。当账户连续登录失败次数过多时，账户将被临时锁定。\n\n> **注意：**\n>\n> - 只支持账户级别的密码连续错误限制登录策略，不支持全局级别的策略。\n> - 登录失败是指客户端在连接尝试期间未能提供正确的密码，不包括由于未知用户或网络问题等原因而导致的连接失败。\n> - 对用户启用密码连续错误限制登录策略后，将增加该用户登录时的检查步骤，此时会影响该用户登录相关操作的性能，尤其是高并发登录场景。\n\n### 配置密码连续错误限制登录策略\n\n每个账户的登录失败次数和锁定时间是可配置的，你可以使用 `CREATE USER`、`ALTER USER` 语句的 `FAILED_LOGIN_ATTEMPTS` 和 `PASSWORD_LOCK_TIME` 选项。`FAILED_LOGIN_ATTEMPTS` 和 `PASSWORD_LOCK_TIME` 选项的可设置值如下：\n\n- `FAILED_LOGIN_ATTEMPTS`：N。表示连续登录失败 N 次后，账户将被临时锁定。N 取值范围为 0 到 32767。\n- `PASSWORD_LOCK_TIME`：N | UNBOUNDED。N 表示登录失败后，账户将被临时锁定 N 天。UNBOUNDED 表明锁定时间无限期，账户必须被手动解锁。N 取值范围为 0 到 32767。\n\n> **注意：**\n>\n> - 允许单条 SQL 语句只设置 `FAILED_LOGIN_ATTEMPTS` 或 `PASSWORD_LOCK_TIME` 中的一个选项，这时密码连续错误限制登录策略不会实质生效。\n> - 只有当账户的 `FAILED_LOGIN_ATTEMPTS` 和 `PASSWORD_LOCK_TIME` 都不为 0 时，系统才会跟踪该账户的失败登录次数并执行临时锁定。\n\n配置密码连续错误限制登录策略的示例如下：\n\n新建一个用户，并配置密码连续错误限制登录策略，当该用户密码连续错误 3 次时，临时锁定 3 天：\n\n```sql\nCREATE USER 'test1'@'localhost' IDENTIFIED BY 'password' FAILED_LOGIN_ATTEMPTS 3 PASSWORD_LOCK_TIME 3;\n```\n\n修改用户的密码连续错误限制登录策略，当该用户密码连续错误 4 次时，无限期锁定，直到账户被手动解锁：\n\n```sql\nALTER USER 'test2'@'localhost' FAILED_LOGIN_ATTEMPTS 4 PASSWORD_LOCK_TIME UNBOUNDED;\n```\n\n关闭用户的密码连续错误限制登录策略：\n\n```sql\nALTER USER 'test3'@'localhost' FAILED_LOGIN_ATTEMPTS 0 PASSWORD_LOCK_TIME 0;\n```\n\n### 锁定账户解锁\n\n在以下场景中，TiDB 将重置用户密码连续错误次数的计数：\n\n- 对该用户执行 `ALTER USER ... ACCOUNT UNLOCK` 解锁命令时。\n- 该用户登录成功时。\n\n当用户因密码连续多次错误触发账户锁定后，以下情况下可以解锁账户：\n\n- 该用户锁定时间结束，这种情况下，用户的自动锁定标识将在下次登录尝试时重置。\n- 对该用户执行 `ALTER USER ... ACCOUNT UNLOCK` 解锁命令时。\n\n> **注意：**\n>\n> 当用户因密码连续失败次数达到设定值而被自动锁定时，如果修改该账户的密码连续错误限制登录策略，需要注意：\n>\n> - 修改该用户的登录失败次数 `FAILED_LOGIN_ATTEMPTS`，用户的自动锁定状态不会改变。修改后的连续登录失败次数，将在用户解锁后再次尝试登录时生效。\n> - 修改该用户的锁定时间 `PASSWORD_LOCK_TIME`，用户的自动锁定状态不会改变。修改后的用户锁定时间，将在账户再次登录尝试时检查是否满足此时的锁定时间要求，如果锁定时间结束就会解锁该用户。\n"
        },
        {
          "name": "pd-configuration-file.md",
          "type": "blob",
          "size": 19.8359375,
          "content": "---\ntitle: PD 配置文件描述\naliases: ['/docs-cn/dev/pd-configuration-file/','/docs-cn/dev/reference/configuration/pd-server/configuration-file/']\nsummary: PD 配置文件包含了许多参数，如节点名称、数据路径、客户端 URL、广告客户端 URL、节点 URL 等。还包括了一些实验性特性的配置项，如内存限制、GC 触发阈值、GOGC Tuner 等。此外，还有监控、调度、副本、标签、Dashboard、同步模式和资源控制等相关配置项。\n---\n\n# PD 配置文件描述\n\n<!-- markdownlint-disable MD001 -->\n\nPD 配置文件比命令行参数支持更多的选项。你可以在 [conf/config.toml](https://github.com/pingcap/pd/blob/master/conf/config.toml) 找到默认的配置文件。\n\n本文档只阐述未包含在命令行参数中的参数，命令行参数参见 [PD 配置参数](/command-line-flags-for-pd-configuration.md)。\n\n> **Tip:**\n>\n> 如果你需要调整配置项的值，请参考[修改配置参数](/maintain-tidb-using-tiup.md#修改配置参数)进行操作。\n\n### `name`\n\n+ PD 节点名称。\n+ 默认值：`\"pd\"`\n+ 如果你需要启动多个 PD，一定要给 PD 使用不同的名字。\n\n### `data-dir`\n\n+ PD 存储数据路径。\n+ 默认值：`\"default.${name}\"`\n\n### `client-urls`\n\n+ PD 监听的客户端 URL 列表。\n+ 默认值：`\"http://127.0.0.1:2379\"`\n+ 如果部署一个集群，client URLs 必须指定当前主机的 IP 地址，例如 `\"http://192.168.100.113:2379\"`，如果是运行在 Docker 则需要指定为 `\"http://0.0.0.0:2379\"`。\n\n### `advertise-client-urls`\n\n+ 用于外部访问 PD 的 URL 列表。\n+ 默认值：`\"${client-urls}\"`\n+ 在某些情况下，例如 Docker 或者 NAT 网络环境，客户端并不能通过 PD 自己监听的 client URLs 来访问到 PD，这时候，你就可以设置 advertise URLs 来让客户端访问。\n+ 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 2379:2379`，那么可以设置为 `advertise-client-urls=\"http://192.168.100.113:2379\"`，客户端可以通过 `http://192.168.100.113:2379` 来找到这个服务。\n\n### `peer-urls`\n\n+ PD 节点监听其他 PD 节点的 URL 列表。\n+ 默认：`\"http://127.0.0.1:2380\"`\n+ 如果部署一个集群，peer URLs 必须指定当前主机的 IP 地址，例如 `\"http://192.168.100.113:2380\"`，如果是运行在 Docker 则需要指定为 `\"http://0.0.0.0:2380\"`。\n\n### `advertise-peer-urls`\n\n+ 用于其他 PD 节点访问某个 PD 节点的 URL 列表。\n+ 默认值：`\"${peer-urls}\"`\n+ 在某些情况下，例如 Docker 或者 NAT 网络环境，其他节点并不能通过 PD 自己监听的 peer URLs 来访问到 PD，这时候，你就可以设置 advertise URLs 来让其他节点访问\n+ 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 2380:2380`，那么可以设置为 `advertise-peer-urls=\"http://192.168.100.113:2380\"`，其他 PD 节点可以通过 `http://192.168.100.113:2380` 来找到这个服务。\n\n### `initial-cluster`\n\n+ 初始化 PD 集群配置。\n+ 默认值：`\"{name}=http://{advertise-peer-url}\"`\n+ 例如，如果 name 是 \"pd\"，并且 `advertise-peer-urls` 是 `\"http://192.168.100.113:2380\"`，那么 `initial-cluster` 就是 `\"pd=http://192.168.100.113:2380\"`。\n+ 如果启动三台 PD，那么 `initial-cluster` 可能就是 `pd1=http://192.168.100.113:2380, pd2=http://192.168.100.114:2380, pd3=192.168.100.115:2380`。\n\n### `initial-cluster-state`\n\n+ 集群初始状态\n+ 默认值：\"new\"\n\n### `initial-cluster-token`\n\n+ 用于在集群初始化阶段标识不同的集群。\n+ 默认值：\"pd-cluster\"\n+ 如果先后部署多个集群，且多个集群有相同配置的节点，应指定不同的 token 来隔离不同的集群。\n\n### `lease`\n\n+ PD Leader Key 租约超时时间，超时系统重新选举 Leader。\n+ 默认值：3\n+ 单位：秒\n\n### `quota-backend-bytes`\n\n+ 元信息数据库存储空间的大小，默认 8GiB。\n+ 默认值：8589934592\n\n### `auto-compaction-mod`\n\n+ 元信息数据库自动压缩的模式，可选项为 periodic（按周期），revision（按版本数）。\n+ 默认值：periodic\n\n### `auto-compaction-retention`\n\n+ compaction-mode 为 periodic 时为元信息数据库自动压缩的间隔时间；compaction-mode 设置为 revision 时为自动压缩的版本数。\n+ 默认值：1h\n\n### `force-new-cluster`\n\n+ 强制让该 PD 以一个新集群启动，且修改 raft 成员数为 1。\n+ 默认值：false\n\n### `tso-update-physical-interval`\n\n+ TSO 物理时钟更新周期。\n+ 在默认的一个 TSO 物理时钟更新周期内 (50ms)，PD 最多提供 262144 个 TSO。如果需要更多的 TSO，可以将这个参数调小。最小值为 `1ms`。\n+ 缩短这个参数会增加 PD 的 CPU 消耗。根据测试，相比 `50ms` 更新周期，更新周期为 `1ms` 时，PD 的 CPU 占用率 ([CPU usage](https://man7.org/linux/man-pages/man1/top.1.html)) 将增加约 10%。\n+ 默认值：50ms\n+ 最小值：1ms\n\n## pd-server\n\npd-server 相关配置项。\n\n### `server-memory-limit` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> 在当前版本中，该配置项为实验特性，不建议在生产环境中使用。\n\n+ PD 实例的内存限制比例。`0` 值表示不设内存限制。\n+ 默认值：`0`\n+ 最小值：`0`\n+ 最大值：`0.99`\n\n### `server-memory-limit-gc-trigger` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> 在当前版本中，该配置项为实验特性，不建议在生产环境中使用。\n\n+ PD 尝试触发 GC 的阈值比例。当 PD 的内存使用达到 `server-memory-limit` 值 * `server-memory-limit-gc-trigger` 值时，则会主动触发一次 Golang GC。在一分钟之内只会主动触发一次 GC。\n+ 默认值：`0.7`\n+ 最小值：`0.5`\n+ 最大值：`0.99`\n\n### `enable-gogc-tuner` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> 在当前版本中，该配置项为实验特性，不建议在生产环境中使用。\n\n+ 是否开启 GOGC Tuner。\n+ 默认值：`false`\n\n### `gc-tuner-threshold` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> 在当前版本中，该配置项为实验特性，不建议在生产环境中使用。\n\n+ GOGC Tuner 自动调节的最大内存阈值比例，即 `server-memory-limit` 值 * `server-memory-limit-gc-trigger` 值，超过阈值后 GOGC Tuner 会停止工作。\n+ 默认值：`0.6`\n+ 最小值：`0`\n+ 最大值：`0.9`\n\n### `flow-round-by-digit` <span class=\"version-mark\">从 v5.1 版本开始引入</span>\n\n+ 默认值：3\n+ PD 会对流量信息的末尾数字进行四舍五入处理，减少 Region 流量信息变化引起的统计信息更新。该配置项用于指定对 Region 流量信息的末尾进行四舍五入的位数。例如流量 `100512` 会归约到 `101000`。默认值为 `3`。该配置替换了 `trace-region-flow`。\n\n> **注意：**\n>\n> 如果是从 v4.0 升级至当前版本，升级后的 `flow-round-by-digit` 行为和升级前的 `trace-region-flow` 行为默认保持一致：如果升级前 `trace-region-flow` 为 false，则升级后 `flow-round-by-digit` 为 127；如果升级前 `trace-region-flow` 为 true，则升级后 `flow-round-by-digit` 为 3。\n\n### `min-resolved-ts-persistence-interval` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n+ 设置 PD leader 对集群中 Resolved TS 最小值进行持久化的间隔时间。如果该值设置为 `0`，表示禁用该功能。\n+ 默认值：在 v6.3.0 之前版本中为 `\"0s\"`，在 v6.3.0 及之后的版本中为 `\"1s\"`，即最小正值。\n+ 最小值：`\"0s\"`\n+ 单位：秒\n\n> **注意：**\n>\n> 对于从 v6.0.0~v6.2.0 升级上来的集群，`min-resolved-ts-persistence-interval` 的默认值在升级后将不会发生变化，即仍然为 `\"0s\"`。若要开启该功能，需要手动修改该配置项的值。\n\n## security\n\n安全相关配置项。\n\n### `cacert-path`\n\n+ CA 文件路径\n+ 默认值：\"\"\n\n### `cert-path`\n\n+ 包含 X509 证书的 PEM 文件路径\n+ 默认值：\"\"\n\n### `key-path`\n\n+ 包含 X509 key 的 PEM 文件路径\n+ 默认值：\"\"\n\n### `redact-info-log` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 控制 PD 日志脱敏的开关\n+ 可选值：`false`、`true`、`\"marker\"`\n+ 默认值：`false`\n+ 具体使用方法参见[日志脱敏](/log-redaction.md#pd-组件日志脱敏)。\n\n## log\n\n日志相关的配置项。\n\n### `level`\n\n+ 指定日志的输出级别。\n+ 可选值：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n+ 默认值：\"info\"\n\n### `format`\n\n+ 日志格式。\n+ 可选值：\"text\"，\"json\"\n+ 默认值：\"text\"\n\n### `disable-timestamp`\n\n+ 是否禁用日志中自动生成的时间戳。\n+ 默认值：false\n\n## log.file\n\n日志文件相关的配置项。\n\n### `max-size`\n\n+ 单个日志文件最大大小，超过该值系统自动切分成多个文件。\n+ 默认值：300\n+ 单位：MiB\n+ 最小值为 1\n\n### `max-days`\n\n+ 日志保留的最长天数。\n+ 如果未设置本参数或把本参数设置为默认值 `0`，PD 不清理日志文件。\n+ 默认：0\n\n### `max-backups`\n\n+ 日志文件保留的最大个数。\n+ 如果未设置本参数或把本参数设置为默认值 `0`，PD 会保留所有的日志文件。\n+ 默认：0\n\n## metric\n\n监控相关的配置项。\n\n### `interval`\n\n+ 向 Prometheus 推送监控指标数据的间隔时间。\n+ 默认：15s\n\n## schedule\n\n调度相关的配置项。\n\n> **注意：**\n> \n> 要修改与调度相关的 PD 配置项，请根据集群的情况选择以下方法之一：\n>\n> - 对于新部署集群，你可以直接在 PD 配置文件中进行修改。\n> - 对于已有集群，请使用命令行工具 [PD Control](/pd-control.md) 进行修改。直接修改 PD 配置文件中与调度相关的配置项不会对已有集群生效。\n\n### `max-merge-region-size`\n\n+ 控制 Region Merge 的 size 上限，当 Region Size 大于指定值时 PD 不会将其与相邻的 Region 合并。\n+ 默认：54。在 v8.4.0 之前，默认值为 20；从 v8.4.0 开始，默认值为 54。\n+ 单位：MiB\n\n### `max-merge-region-keys`\n\n+ 控制 Region Merge 的 key 上限，当 Region key 大于指定值时 PD 不会将其与相邻的 Region 合并。\n+ 默认：540000。在 v8.4.0 之前，默认值为 200000；从 v8.4.0 开始，默认值为 540000。\n\n### `patrol-region-interval`\n\n+ 控制 checker 检查 Region 健康状态的运行频率，越短则运行越快，通常状况不需要调整\n+ 默认：10ms\n\n### `patrol-region-worker-count` <span class=\"version-mark\">从 v8.5.0 版本开始引入</span>\n\n> **警告：**\n>\n> 将该配置项设置为大于 1 将启用并发检查。目前该功能为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/tikv/pd/issues)反馈。\n\n+ 控制 checker 检查 Region 健康状态时，创建 [operator](/glossary.md#operator) 的并发数。通常情况下，无需调整此配置项。\n+ 默认：1\n\n### `split-merge-interval`\n\n+ 控制对同一个 Region 做 split 和 merge 操作的间隔，即对于新 split 的 Region 一段时间内不会被 merge。\n+ 默认：1h\n\n### `max-snapshot-count`\n\n+ 控制单个 store 最多同时接收或发送的 snapshot 数量，调度受制于这个配置来防止抢占正常业务的资源。\n+ 默认：64\n\n### `max-pending-peer-count`\n\n+ 控制单个 store 的 pending peer 上限，调度受制于这个配置来防止在部分节点产生大量日志落后的 Region。\n+ 默认值：64\n\n### `max-store-down-time`\n\n+ PD 认为失联 store 无法恢复的时间，当超过指定的时间没有收到 store 的心跳后，PD 会在其他节点补充副本。\n+ 默认值：30m\n\n### `max-store-preparing-time` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n+ 控制 store 上线阶段的最长等待时间。在 store 的上线阶段，PD 可以查询该 store 的上线进度。当超过该配置项指定的时间后，PD 会认为该 store 已完成上线，无法再次查询这个 store 的上线进度，但是不影响 Region 向这个新上线 store 的迁移。通常用户无需修改该配置项。\n+ 默认值：48h\n\n### `leader-schedule-limit`\n\n+ 同时进行 leader 调度的任务个数。\n+ 默认值：4\n\n### `region-schedule-limit`\n\n+ 同时进行 Region 调度的任务个数\n+ 默认值：2048\n\n### `hot-region-schedule-limit`\n\n+ 控制同时进行的 hot Region 任务。该配置项独立于 Region 调度。\n+ 默认值：4\n\n### `hot-region-cache-hits-threshold`\n\n+ 设置识别热点 Region 所需的分钟数。只有当 Region 处于热点状态持续时间超过此分钟数时，PD 才会参与热点调度。\n+ 默认值：3\n\n### `replica-schedule-limit`\n\n+ 同时进行 replica 调度的任务个数。\n+ 默认值：64\n\n### `merge-schedule-limit`\n\n+ 同时进行的 Region Merge 调度的任务，设置为 0 则关闭 Region Merge。\n+ 默认值：8\n\n### `high-space-ratio`\n\n+ 设置 store 空间充裕的阈值。当节点的空间占用比例小于该阈值时，PD 调度时会忽略节点的剩余空间，主要根据实际数据量进行均衡。此配置仅在 `region-score-formula-version = v1` 时生效。\n+ 默认值：0.7\n+ 最小值：大于 0\n+ 最大值：小于 1\n\n### `low-space-ratio`\n\n+ 设置 store 空间不足的阈值。当某个节点的空间占用比例超过该阈值时，PD 会尽可能避免往该节点迁移数据，同时主要根据节点剩余空间大小进行调度，避免对应节点的磁盘空间被耗尽。\n+ 默认值：0.8\n+ 最小值：大于 0\n+ 最大值：小于 1\n\n### `tolerant-size-ratio`\n\n+ 控制 balance 缓冲区大小。\n+ 默认值：0（为 0 为自动调整缓冲区大小）\n+ 最小值：0\n\n### `enable-cross-table-merge`\n\n+ 设置是否开启跨表 merge。\n+ 默认值：true\n\n### `region-score-formula-version` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 设置 Region 算分公式版本。\n+ 默认值：v2\n+ 可选值：v1，v2。v2 相比于 v1，变化会更平滑，空间回收引起的调度抖动情况会得到改善。\n\n> **注意：**\n>\n> 如果是从 v4.0 升级至当前版本，默认不自动开启该算分公式新版本，以保证升级前后 PD 行为一致。若想切换算分公式的版本，使用需要手动通过 `pd-ctl` 设置切换，详见 [PD Control](/pd-control.md#config-show--set-option-value--placement-rules) 文档。\n\n### `enable-joint-consensus` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 是否使用 Joint Consensus 进行副本调度。关闭该特性时，PD 将采用一次调度一个副本的方式进行调度。\n+ 默认值：true\n\n### `enable-diagnostic` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n+ 是否开启诊断功能。开启特性时，PD 将会记录调度中的一些状态来帮助诊断。开启时会略微影响调度速度，在 Store 数量较多时会消耗较大内存。\n+ 默认值：从 v7.1.0 起，默认值从 `false` 变更为 `true`。如果从 v7.1.0 之前版本的集群升级至 v7.1.0 及之后的版本，该默认值不发生变化。\n\n### `hot-regions-write-interval` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n* 设置 PD 存储 Hot Region 信息时间间隔。\n* 默认值：10m\n\n> **注意：**\n>\n> Hot Region 的信息一般 3 分钟更新一次。如果设置时间间隔小于 3 分钟，中间部分的更新可能没有意义。\n\n### `hot-regions-reserved-days` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n* 设置 PD 保留的 Hot Region 信息的最长时间。单位为天。\n* 默认值: 7\n\n## replication\n\n副本相关的配置项。\n\n### `max-replicas`\n\n+ 所有副本数量，即 leader 与 follower 数量之和。默认为 `3`，即 1 个 leader 和 2 个 follower。当此配置被在线修改后，PD 会在后台通过调度使得 Region 的副本数量符合配置。\n+ 默认值：3\n\n### `location-labels`\n\n+ TiKV 集群的拓扑信息。\n+ 默认值：[]\n+ [配置集群拓扑](/schedule-replicas-by-topology-labels.md)\n\n### `isolation-level`\n\n+ TiKV 集群的最小强制拓扑隔离级别。\n+ 默认值：\"\"\n+ [配置集群拓扑](/schedule-replicas-by-topology-labels.md)\n\n### `strictly-match-label`\n\n+ 打开强制 TiKV Label 和 PD 的 location-labels 是否匹配的检查\n+ 默认值：false\n\n### `enable-placement-rules`\n\n+ 打开 `placement-rules`\n+ 默认值：true\n+ 参考 [Placement Rules 使用文档](/configure-placement-rules.md)\n\n### `store-limit-version` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n+ 设置 `store limit` 工作模式\n+ 默认值：v1\n+ 可选值：\n    + v1：在 v1 模式下，你可以手动修改 `store limit` 以限制单个 TiKV 调度速度。\n    + v2：在 v2 模式下，你无需关注 `store limit` 值，PD 将根据 TiKV Snapshot 执行情况动态调整 TiKV 调度速度。详情请参考 [Store Limit v2 原理](/configure-store-limit.md#store-limit-v2-原理)。\n\n## label-property（已废弃）\n\n标签相关的配置项，只支持 `reject-leader` 类型。\n\n> **注意：**\n>\n> 标签相关的配置项已从 v5.2 开始废弃，建议使用 [Placement Rules](/configure-placement-rules.md#场景二5-副本按-2-2-1-的比例放置在-3-个数据中心且第-3-个中心不产生-leader) 设置副本策略。\n\n### `key`（已废弃）\n\n+ 拒绝 leader 的 store 带有的 label key。\n+ 默认值：\"\"\n\n### `value`（已废弃）\n\n+ 拒绝 leader 的 store 带有的 label value。\n+ 默认值：\"\"\n\n## dashboard\n\nPD 中内置的 [TiDB Dashboard](/dashboard/dashboard-intro.md) 相关配置项。\n\n### `tidb-cacert-path`\n\n+ CA 根证书文件路径。可配置该路径来使用 TLS 连接 TiDB 的 SQL 服务。\n+ 默认值：\"\"\n\n### `tidb-cert-path`\n\n+ SSL 证书文件路径。可配置该路径来使用 TLS 连接 TiDB 的 SQL 服务。\n+ 默认值：\"\"\n\n### `tidb-key-path`\n\n+ SSL 私钥文件路径。可配置该路径来使用 TLS 连接 TiDB 的 SQL 服务。\n+ 默认值：\"\"\n\n### `public-path-prefix`\n\n+ 通过反向代理访问 TiDB Dashboard 时，配置反向代理提供服务的路径前缀。\n+ 默认值：\"/dashboard\"\n+ 若不通过反向代理访问 TiDB Dashboard，**请勿配置该项**，否则可能导致 TiDB Dashboard 无法正常访问。关于该配置的详细使用场景，参见[通过反向代理使用 TiDB Dashboard](/dashboard/dashboard-ops-reverse-proxy.md)。\n\n### `enable-telemetry`\n\n> **警告：**\n>\n> 从 TiDB v8.1.0 开始，TiDB Dashboard 已移除遥测功能，该配置项已不再生效。保留该配置项仅用于与之前版本兼容。\n\n+ 在 v8.1.0 之前，用于控制是否启用 TiDB Dashboard 遥测功能。\n+ 默认值：false\n\n## `replication-mode`\n\nRegion 同步模式相关的配置项。更多详情，请参阅[启用自适应同步模式](/two-data-centers-in-one-city-deployment.md#启用自适应同步模式)。\n\n## controller\n\nPD 中内置的 [Resource Control](/tidb-resource-control.md) 相关的配置项。\n\n### `degraded-mode-wait-duration`\n\n+ 触发降级模式需要等待的时间。降级模式是指在 Local Token Bucket (LTB) 和 Global Token Bucket (GTB) 失联的情况下，LTB 将回退到默认的资源组配置，不再有 GTB 授权 token，从而保证在网络隔离或者异常情况下，服务不受影响。\n+ 默认值: 0s\n+ 默认为不开启降级模式\n\n### `request-unit`\n\n下面是 [Request Unit (RU)](/tidb-resource-control.md#什么是-request-unit-ru) 相关的配置项。\n\n#### `read-base-cost`\n\n+ 每次读请求转换成 RU 的基准系数\n+ 默认值: 0.25\n\n#### `write-base-cost`\n\n+ 每次写请求转换成 RU 的基准系数\n+ 默认值: 1\n\n#### `read-cost-per-byte`\n\n+ 读流量转换成 RU 的基准系数\n+ 默认值: 1/(64 * 1024)\n+ 1 RU = 64 KiB 读取字节\n\n#### `write-cost-per-byte`\n\n+ 写流量转换成 RU 的基准系数\n+ 默认值: 1/1024\n+ 1 RU = 1 KiB 写入字节\n\n#### `read-cpu-ms-cost`\n\n+ CPU 转换成 RU 的基准系数\n+ 默认值: 1/3\n+ 1 RU = 3 毫秒 CPU 时间\n"
        },
        {
          "name": "pd-control.md",
          "type": "blob",
          "size": 50.1572265625,
          "content": "---\ntitle: PD Control 使用说明\naliases: ['/docs-cn/dev/pd-control/','/docs-cn/dev/reference/tools/pd-control/']\nsummary: PD Control 是 PD 的命令行工具，用于获取集群状态信息和调整集群。\n---\n\n# PD Control 使用说明\n\nPD Control 是 PD 的命令行工具，用于获取集群状态信息和调整集群。\n\n## 安装方式\n\n> **注意：**\n>\n> 建议使用的 Control 工具版本与集群版本保持一致。\n\n### 使用 TiUP\n\n可直接通过 `tiup ctl:v<CLUSTER_VERSION> pd -u http://<pd_ip>:<pd_port> [-i]` 使用。\n\n### 下载安装包\n\n如需下载最新版本的 `pd-ctl`，直接下载 TiDB 安装包即可。`pd-ctl` 位于 TiDB 安装包的 `ctl-{version}-linux-{arch}.tar.gz` 包中。\n\n| 安装包                                                                    | 操作系统 | 架构  | SHA256 校验和                                                    |\n| :------------------------------------------------------------------------ | :------- | :---- | :--------------------------------------------------------------- |\n| `https://download.pingcap.org/tidb-community-server-{version}-linux-amd64.tar.gz` (pd-ctl) | Linux    | amd64 | `https://download.pingcap.org/tidb-community-server-{version}-linux-amd64.tar.gz.sha256` |\n| `https://download.pingcap.org/tidb-community-server-{version}-linux-arm64.tar.gz` (pd-ctl) | Linux | arm64 | `https://download.pingcap.org/tidb-community-server-{version}-linux-arm64.tar.gz.sha256` |\n\n> **注意：**\n>\n> 下载链接中的 `{version}` 为 TiDB 的版本号。例如，amd64 架构的 `v8.5.0` 版本的下载链接为 `https://download.pingcap.org/tidb-community-server-v8.5.0-linux-amd64.tar.gz`。\n\n### 源码编译\n\n1. [Go](https://golang.org/) 1.23 或以上版本\n2. 在 PD 项目根目录使用 `make` 或者 `make pd-ctl` 命令进行编译，生成 bin/pd-ctl\n\n## 简单例子\n\n单命令模式：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup ctl:v<CLUSTER_VERSION> pd store -u http://127.0.0.1:2379\n```\n\n交互模式：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup ctl:v<CLUSTER_VERSION> pd -i -u http://127.0.0.1:2379\n```\n\n使用环境变量：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\nexport PD_ADDR=http://127.0.0.1:2379 &&\ntiup ctl:v<CLUSTER_VERSION> pd\n```\n\n使用 TLS 加密：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntiup ctl:v<CLUSTER_VERSION> pd -u https://127.0.0.1:2379 --cacert=\"path/to/ca\" --cert=\"path/to/cert\" --key=\"path/to/key\"\n```\n\n## 命令行参数 (flags)\n\n### `--cacert`\n\n- 指定 PEM 格式的受信任 CA 证书的文件路径\n- 默认值：\"\"\n\n### `--cert`\n\n- 指定 PEM 格式的 SSL 证书的文件路径\n- 默认值：\"\"\n\n### `--detach` / `-d`\n\n+ 使用单命令行模式（不进入 readline）\n+ 默认值：true\n\n### `--help` / `-h`\n\n+ 输出帮助信息\n+ 默认值：false\n\n### `--interact`/`-i`\n\n+ 使用交互模式（进入 readline）\n+ 默认值：false\n\n### `--key`\n\n- 指定 PEM 格式的 SSL 证书密钥文件路径，即 `--cert` 所指定的证书的私钥\n- 默认值：\"\"\n\n### `--pd`/`-u`\n\n+ 指定 PD 的地址\n+ 默认地址：`http://127.0.0.1:2379`\n+ 环境变量：`PD_ADDR`\n\n### `--version`/`-V`\n\n- 打印版本信息并退出\n- 默认值：false\n\n## 命令 (command)\n\n### cluster\n\n用于显示集群基本信息。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\ncluster\n```\n\n```\n{\n  \"id\": 6493707687106161130,\n  \"max_peer_count\": 3\n}\n```\n\n### `config [show | set <option> <value> | placement-rules]`\n\n用于显示或调整配置信息。示例如下。\n\n显示 scheduling 的相关 config 信息：\n\n{{< copyable \"\" >}}\n\n```bash\nconfig show\n```\n\n```\n{\n  \"replication\": {\n    \"enable-placement-rules\": \"true\",\n    \"isolation-level\": \"\",\n    \"location-labels\": \"\",\n    \"max-replicas\": 3,\n    \"strictly-match-label\": \"false\"\n  },\n  \"schedule\": {\n    \"enable-cross-table-merge\": \"true\",\n    \"high-space-ratio\": 0.7,\n    \"hot-region-cache-hits-threshold\": 3,\n    \"hot-region-schedule-limit\": 4,\n    \"leader-schedule-limit\": 4,\n    \"leader-schedule-policy\": \"count\",\n    \"low-space-ratio\": 0.8,\n    \"max-merge-region-keys\": 540000,\n    \"max-merge-region-size\": 54,\n    \"max-pending-peer-count\": 64,\n    \"max-snapshot-count\": 64,\n    \"max-store-down-time\": \"30m0s\",\n    \"merge-schedule-limit\": 8,\n    \"patrol-region-interval\": \"10ms\",\n    \"region-schedule-limit\": 2048,\n    \"region-score-formula-version\": \"v2\",\n    \"replica-schedule-limit\": 64,\n    \"scheduler-max-waiting-operator\": 5,\n    \"split-merge-interval\": \"1h0m0s\",\n    \"tolerant-size-ratio\": 0\n  }\n}\n```\n\n显示所有的 config 信息：\n\n{{< copyable \"\" >}}\n\n```bash\nconfig show all\n```\n\n显示 replication 的相关 config 信息：\n\n{{< copyable \"\" >}}\n\n```bash\nconfig show replication\n```\n\n```\n{\n  \"max-replicas\": 3,\n  \"isolation-level\": \"\",\n  \"location-labels\": \"\",\n  \"strictly-match-label\": \"false\",\n  \"enable-placement-rules\": \"true\"\n}\n```\n\n显示目前集群版本，是目前集群 TiKV 节点的最低版本，并不对应 binary 的版本：\n\n{{< copyable \"\" >}}\n\n```bash\nconfig show cluster-version\n```\n\n```\n\"5.2.2\"\n```\n\n- `max-snapshot-count` 控制单个 store 最多同时接收或发送的 snapshot 数量，调度受制于这个配置来防止抢占正常业务的资源。当需要加快补副本或 balance 速度时可以调大这个值。\n\n    设置最大 snapshot 为 64：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set max-snapshot-count 64\n    ```\n\n- `max-pending-peer-count` 控制单个 store 的 pending peer 上限，调度受制于这个配置来防止在部分节点产生大量日志落后的 Region。需要加快补副本或 balance 速度可以适当调大这个值，设置为 0 则表示不限制。\n\n    设置最大 pending peer 数量为 64：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set max-pending-peer-count 64\n    ```\n\n- `max-merge-region-size` 控制 Region Merge 的 size 上限（单位是 MiB）。当 Region Size 大于指定值时 PD 不会将其与相邻的 Region 合并。设置为 0 表示不开启 Region Merge 功能。\n\n    设置 Region Merge 的 size 上限为 16 MiB：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set max-merge-region-size 16\n    ```\n\n- `max-merge-region-keys` 控制 Region Merge 的 keyCount 上限。当 Region KeyCount 大于指定值时 PD 不会将其与相邻的 Region 合并。\n\n    设置 Region Merge 的 keyCount 上限为 50000：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set max-merge-region-keys 50000\n    ```\n\n- `split-merge-interval` 控制对同一个 Region 做 `split` 和 `merge` 操作的间隔，即对于新 `split` 的 Region 一段时间内不会被 `merge`。\n\n    设置 `split` 和 `merge` 的间隔为 1 天：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set split-merge-interval 24h\n    ```\n\n- `enable-one-way-merge` 用于控制是否只允许和相邻的后一个 Region 进行合并。当设置为 `false` 时，PD 允许与相邻的前后 Region 进行合并。\n\n    设置只允许和相邻的后一个 Region 合并：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set enable-one-way-merge true\n    ```\n\n- `enable-cross-table-merge` 用于开启跨表 Region 的合并。当设置为 `false` 时，PD 不会合并不同表的 Region。该选项只在键类型为 \"table\" 时生效。\n\n    设置允许跨表合并：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set enable-cross-table-merge true\n    ```\n\n- `key-type` 用于指定集群的键编码类型。支持的类型有 `[\"table\", \"raw\", \"txn\"]`，默认值为 \"table\"。\n\n    - 如果集群中不存在 TiDB 实例，`key-type` 的值为 \"raw\" 或 \"txn\"。此时，无论 `enable-cross-table-merge` 设置为何，PD 均可以跨表合并 Region。\n    - 如果集群中存在 TiDB 实例，`key-type` 的值应当为 \"table\"。此时，`enable-cross-table-merge` 的设置决定了 PD 是否能跨表合并 Region。如果 `key-type` 的值为 \"raw\"，placement rules 不生效。\n\n    启用跨表合并：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set key-type raw\n    ```\n\n- `region-score-formula-version` 用于设置 Region 算分公式的版本，支持的值有 `[\"v1\", \"v2\"]`。v2 版本公式有助于减少上下线等场景下冗余的 balance Region 调度。\n\n    开启 v2 版本 Region 算分公式：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set region-score-formula-version v2\n    ```\n\n- `patrol-region-interval` 控制 checker 检查 Region 健康状态的运行频率，越短则运行越快，通常状况不需要调整。\n\n    设置 checker 的运行频率为 10 毫秒：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set patrol-region-interval 10ms\n    ```\n\n- `patrol-region-worker-count` 控制 checker 检查 Region 健康状态时，创建 [operator](/glossary.md#operator) 的并发数。通常情况下，无需调整此配置项。将该配置项设置为大于 1 将启用并发检查。目前该功能为实验特性，不建议在生产环境中使用。\n\n    设置 checker 的并发数为 2：\n\n    ```bash\n    config set patrol-region-worker-count 2\n    ```\n\n- `max-store-down-time` 为 PD 认为失联 store 无法恢复的时间，当超过指定的时间没有收到 store 的心跳后，PD 会在其他节点补充副本。\n\n    设置 store 心跳丢失 30 分钟开始补副本：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set max-store-down-time 30m\n    ```\n\n- `max-store-preparing-time` 控制 store 上线阶段的最长等待时间。在 store 的上线阶段，PD 可以查询该 store 的上线进度。当超过该配置项指定的时间后，PD 会认为该 store 已完成上线，无法再次查询这个 store 的上线进度，但是不影响 Region 向这个新上线 store 的迁移。通常用户无需修改该配置项。\n\n    设置 store 上线阶段最多等待 4 小时：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set max-store-preparing-time 4h\n    ```\n\n- 通过调整 `leader-schedule-limit` 可以控制同时进行 leader 调度的任务个数。这个值主要影响 *leader balance* 的速度，值越大调度得越快，设置为 0 则关闭调度。Leader 调度的开销较小，需要的时候可以适当调大。\n\n    最多同时进行 4 个 leader 调度：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set leader-schedule-limit 4\n    ```\n\n- 通过调整 `region-schedule-limit` 可以控制同时进行 Region 调度的任务个数。这个值可以避免创建过多的 Region balance operator。默认值为 `2048`，对所有大小的集群都足够。设置为 `0` 则关闭调度。Region 调度的速度通常受到 `store-limit` 的限制，但除非你熟悉该设置，否则不推荐自定义该参数。\n\n    最多同时进行 2 个 Region 调度：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set region-schedule-limit 2\n    ```\n\n- 通过调整 `replica-schedule-limit` 可以控制同时进行 replica 调度的任务个数。这个值主要影响节点挂掉或者下线的时候进行调度的速度，值越大调度得越快，设置为 0 则关闭调度。Replica 调度的开销较大，所以这个值不宜调得太大。注意：该参数通常保持为默认值。如需调整，需要根据实际情况反复尝试设置该值大小。\n\n    最多同时进行 4 个 replica 调度：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set replica-schedule-limit 4\n    ```\n\n- `merge-schedule-limit` 控制同时进行的 Region Merge 调度的任务，设置为 0 则关闭 Region Merge。Merge 调度的开销较大，所以这个值不宜调得过大。注意：该参数通常保持为默认值。如需调整，需要根据实际情况反复尝试设置该值大小。\n\n    最多同时进行 16 个 merge 调度：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set merge-schedule-limit 16\n    ```\n\n- `hot-region-schedule-limit` 控制同时进行的 Hot Region 调度的任务，设置为 0 则关闭调度。这个值不宜调得过大，否则可能对系统性能造成影响。注意：该参数通常保持为默认值。如需调整，需要根据实际情况反复尝试设置该值大小。\n\n    最多同时进行 4 个 Hot Region 调度：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set hot-region-schedule-limit 4\n    ```\n\n- `hot-region-cache-hits-threshold` 用于设置识别热点 Region 所需的分钟数，只有 Region 处于热点状态持续时间超过该分钟数后，才能参与热点调度。\n\n- `tolerant-size-ratio` 控制 balance 缓冲区大小。当两个 store 的 leader 或 Region 的得分差距小于指定倍数的 Region size 时，PD 会认为此时 balance 达到均衡状态。\n\n    设置缓冲区为约 20 倍平均 RegionSize：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set tolerant-size-ratio 20\n    ```\n\n- `low-space-ratio` 用于设置 store 空间不足的阈值。当节点的空间占用比例超过指定值时，PD 会尽可能避免往对应节点迁移数据，同时主要针对剩余空间大小进行调度，避免对应节点磁盘空间被耗尽。\n\n    设置空间不足阈值为 0.9：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set low-space-ratio 0.9\n    ```\n\n- `high-space-ratio` 用于设置 store 空间充裕的阈值，此配置仅的在 `region-score-formula-version = v1` 时生效。当节点的空间占用比例小于指定值时，PD 调度时会忽略剩余空间这个指标，主要针对实际数据量进行均衡。\n\n    设置空间充裕阈值为 0.5：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set high-space-ratio 0.5\n    ```\n\n- `cluster-version` 集群的版本，用于控制某些 Feature 是否开启，处理兼容性问题。通常是集群正常运行的所有 TiKV 节点中的最低版本，需要回滚到更低的版本时才进行手动设置。\n\n    设置 cluster version 为 1.0.8：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set cluster-version 1.0.8\n    ```\n\n- `leader-schedule-policy` 用于选择 Leader 的调度策略，可以选择按照 `size` 或者 `count` 来进行调度。\n\n- `scheduler-max-waiting-operator` 用于控制每个调度器同时存在的 operator 的个数。\n\n- `enable-remove-down-replica` 用于开启自动删除 DownReplica 的特性。当设置为 false 时，PD 不会自动清理宕机状态的副本。\n\n- `enable-replace-offline-replica` 用于开启迁移 OfflineReplica 的特性。当设置为 false 时，PD 不会迁移下线状态的副本。\n\n- `enable-make-up-replica` 用于开启补充副本的特性。当设置为 false 时，PD 不会为副本数不足的 Region 补充副本。\n\n- `enable-remove-extra-replica` 用于开启删除多余副本的特性。当设置为 false 时，PD 不会为副本数过多的 Region 删除多余副本。\n\n- `enable-location-replacement` 用于开启隔离级别检查。当设置为 false 时，PD 不会通过调度来提升 Region 副本的隔离级别。\n\n- `enable-debug-metrics` 用于开启 debug 的 metrics。当设置为 true 时，PD 会开启一些 metrics，比如 `balance-tolerant-size` 等。\n\n- `enable-placement-rules` 用于开启 placement rules，在 v5.0 及以上的版本默认开启。\n\n- `store-limit-mode` 用于控制 store 限速机制的模式。主要有两种模式：`auto` 和 `manual`。`auto` 模式下会根据 load 自动进行平衡调整（弃用）。\n\n- `store-limit-version` 用于设置 `store limit` 限制模式，目前提供两种方式：`v1` 和 `v2`。默认值为 `v1`。在 `v1` 模式下，你可以手动修改 `store limit` 以限制单个 TiKV 调度速度。在 `v2` 模式下，你无需关注 `store limit` 值，PD 将根据 TiKV Snapshot 执行情况动态调整 TiKV 调度速度。详情请参考 [Store Limit v2 原理](/configure-store-limit.md#store-limit-v2-原理)。\n\n    ```bash\n    config set store-limit-version v2       // 使用 Store Limit v2\n    ```\n\n- PD 会对流量信息的末尾数字进行四舍五入处理，减少 Region 流量信息变化引起的统计信息更新。该配置项用于指定对 Region 流量信息的末尾进行四舍五入的位数。例如流量 `100512` 会归约到 `101000`。默认值为 `3`。该配置替换了 `trace-region-flow`。\n\n    示例：将 `flow-round-by-digit` 的值设为 `4`：\n\n    {{< copyable \"\" >}}\n\n    ```bash\n    config set flow-round-by-digit 4\n    ```\n\n### `config [show | set service-middleware <option> [<key> <value> | <label> <qps|concurrency> <value>]]`\n\n`service-middleware` 是 PD 中的一个配置模块，主要用于管理和控制 PD 服务的中间件功能，如审计日志、请求速率限制和并发限制等。从 v8.5.0 起，PD 支持通过 `pd-ctl` 修改 `service-middleware` 的以下配置：\n\n- `audit`：控制是否开启 PD 处理 HTTP 请求的审计日志（默认开启）。开启时，`service-middleware` 会在 PD 日志中记录 HTTP 请求的相关信息。\n- `rate-limit`：用于限制 PD 处理 HTTP API 请求的最大速率和最大并发。\n- `grpc-rate-limit`：用于限制 PD 处理 gRPC API 请求的最大速率和最大并发。\n\n> **注意：**\n>\n> 为了避免请求速率限制和并发限制对 PD 性能的影响，不建议修改 `service-middleware` 中的配置。\n\n显示 `service-middleware` 的相关 config 信息：\n\n```bash\nconfig show service-middleware\n```\n\n```bash\n{\n  \"audit\": {\n    \"enable-audit\": \"true\"\n  },\n  \"rate-limit\": {\n    \"enable-rate-limit\": \"true\",\n    \"limiter-config\": {}\n  },\n  \"grpc-rate-limit\": {\n    \"enable-grpc-rate-limit\": \"true\",\n    \"grpc-limiter-config\": {}\n  }\n}\n```\n\n`service-middleware audit` 用于开启或关闭 HTTP 请求的日志审计功能。以关闭该功能为例：\n\n```bash\nconfig set service-middleware audit enable-audit false\n```\n\n`service-middleware grpc-rate-limit` 用于控制以下 gRPC API 请求的最大速率和并发度：\n\n- `GetRegion`：获取指定 Region 的信息\n- `GetStore`：获取指定 Store 的信息\n- `GetMembers`：获取 PD 集群成员的信息\n\n控制某个 gRPC API 请求的最大速率，以 `GetRegion` API 请求为例：\n\n```bash\nconfig set service-middleware grpc-rate-limit GetRegion qps 100\n```\n\n控制某个 gRPC API 请求的最大并发度，以 `GetRegion` API 请求为例：\n\n```bash\nconfig set service-middleware grpc-rate-limit GetRegion concurrency 10\n```\n\n查看修改后的配置：\n\n```bash\nconfig show service-middleware\n```\n\n```bash\n{\n  \"audit\": {\n    \"enable-audit\": \"true\"\n  },\n  \"rate-limit\": {\n    \"enable-rate-limit\": \"true\",\n    \"limiter-config\": {}\n  },\n  \"grpc-rate-limit\": {\n    \"enable-grpc-rate-limit\": \"true\",\n    \"grpc-limiter-config\": {\n      \"GetRegion\": {\n        \"QPS\": 100,\n        \"QPSBurst\": 100, // 根据 QPS 设置自动调整，仅作展示\n        \"ConcurrencyLimit\": 10\n      }\n    }\n  }\n}\n```\n\n重置上述设置：\n\n```bash\nconfig set service-middleware grpc-rate-limit GetRegion qps 0\nconfig set service-middleware grpc-rate-limit GetRegion concurrency 0\n```\n\n`service-middleware rate-limit` 用于控制以下 HTTP API 请求的最大速率和并发度：\n\n- `GetRegion`：获取指定 Region 的信息\n- `GetStore`：获取指定 Store 的信息\n\n控制某个 HTTP API 请求的最大速率，以 `GetRegion` API 请求为例：\n\n```bash\nconfig set service-middleware rate-limit GetRegion qps 100\n```\n\n控制某个 HTTP API 请求的最大并发度，以 `GetRegion` API 请求为例：\n\n```bash\nconfig set service-middleware rate-limit GetRegion concurrency 10\n```\n\n重置上述设置：\n\n```bash\nconfig set service-middleware rate-limit GetRegion qps 0\nconfig set service-middleware rate-limit GetRegion concurrency 0\n```\n\n### `config placement-rules [disable | enable | load | save | show | rule-group]`\n\n关于 `config placement-rules` 的具体用法，参考 [Placement Rules 使用文档](/configure-placement-rules.md#配置规则操作步骤)。\n\n### health\n\n用于显示集群健康信息。示例如下。\n\n显示健康信息：\n\n{{< copyable \"\" >}}\n\n```bash\nhealth\n```\n\n```\n[\n  {\n    \"name\": \"pd\",\n    \"member_id\": 13195394291058371180,\n    \"client_urls\": [\n      \"http://127.0.0.1:2379\"\n      ......\n    ],\n    \"health\": true\n  }\n  ......\n]\n```\n\n### `hot [read | write | store| history <start_time> <end_time> [<key> <value>]]`\n\n用于显示集群热点信息。示例如下。\n\n显示读热点信息：\n\n{{< copyable \"\" >}}\n\n```bash\nhot read\n```\n\n显示写热点信息：\n\n{{< copyable \"\" >}}\n\n```bash\nhot write\n```\n\n显示所有 store 的读写信息：\n\n{{< copyable \"\" >}}\n\n```bash\nhot store\n```\n\n显示历史读写热点信息:\n\n{{< copyable \"\" >}}\n\n```\nhot history startTime endTime [ <name> <value> ]\n```\n\n例如查询时间 `1629294000000` 到 `1631980800000` （毫秒）之间的历史热点 Region 信息:\n\n{{< copyable \"\" >}}\n\n```\nhot history 1629294000000 1631980800000\n```\n\n```\n{\n  \"history_hot_region\": [\n    {\n      \"update_time\": 1630864801948,\n      \"region_id\": 103,\n      \"peer_id\": 1369002,\n      \"store_id\": 3,\n      \"is_leader\": true,\n      \"is_learner\": false,\n      \"hot_region_type\": \"read\",\n      \"hot_degree\": 152,\n      \"flow_bytes\": 0,\n      \"key_rate\": 0,\n      \"query_rate\": 305,\n      \"start_key\": \"7480000000000000FF5300000000000000F8\",\n      \"end_key\": \"7480000000000000FF5600000000000000F8\"\n    },\n    ...\n  ]\n}\n```\n\n对于参数的值为数组的请用 `x, y, ...` 的形式进行参数值的设置，所有支持的参数如下所示:\n\n{{< copyable \"\" >}}\n\n```\nhot history 1629294000000 1631980800000 hot_region_type read region_id 1,2,3 store_id 1,2,3 peer_id 1,2,3 is_leader true is_learner true\n```\n\n```\n{\n  \"history_hot_region\": [\n    {\n      \"update_time\": 1630864801948,\n      \"region_id\": 103,\n      \"peer_id\": 1369002,\n      \"store_id\": 3,\n      \"is_leader\": true,\n      \"is_learner\": false,\n      \"hot_region_type\": \"read\",\n      \"hot_degree\": 152,\n      \"flow_bytes\": 0,\n      \"key_rate\": 0,\n      \"query_rate\": 305,\n      \"start_key\": \"7480000000000000FF5300000000000000F8\",\n      \"end_key\": \"7480000000000000FF5600000000000000F8\"\n    },\n    ...\n  ]\n}\n```\n\n### `label [store <name> <value>]`\n\n用于显示集群标签信息。示例如下。\n\n显示所有 label：\n\n{{< copyable \"\" >}}\n\n```bash\nlabel\n```\n\n显示所有包含 label 为 \"zone\":\"cn\" 的 store：\n\n{{< copyable \"\" >}}\n\n```bash\nlabel store zone cn\n```\n\n### `member [delete | leader_priority | leader [show | resign | transfer <member_name>]]`\n\n用于显示 PD 成员信息，删除指定成员，设置成员的 leader 优先级。示例如下。\n\n显示所有成员的信息：\n\n{{< copyable \"\" >}}\n\n```bash\nmember\n```\n\n```\n{\n  \"header\": {......},\n  \"members\": [......],\n  \"leader\": {......},\n  \"etcd_leader\": {......},\n}\n```\n\n下线 \"pd2\"：\n\n{{< copyable \"\" >}}\n\n```bash\nmember delete name pd2\n```\n\n```\nSuccess!\n```\n\n使用 id 下线节点：\n\n{{< copyable \"\" >}}\n\n```bash\nmember delete id 1319539429105371180\n```\n\n```\nSuccess!\n```\n\n显示 leader 的信息：\n\n{{< copyable \"\" >}}\n\n```bash\nmember leader show\n```\n\n```\n{\n  \"name\": \"pd\",\n  \"member_id\": 13155432540099656863,\n  \"peer_urls\": [......],\n  \"client_urls\": [......]\n}\n```\n\n将 leader 从当前成员移走：\n\n{{< copyable \"\" >}}\n\n```bash\nmember leader resign\n```\n\n```\n......\n```\n\n将 leader 迁移至指定成员：\n\n{{< copyable \"\" >}}\n\n```bash\nmember leader transfer pd3\n```\n\n```\n......\n```\n\n指定 PD leader 的优先级：\n\n```bash\nmember leader_priority  pd-1 4\nmember leader_priority  pd-2 3\nmember leader_priority  pd-3 2\nmember leader_priority  pd-4 1\nmember leader_priority  pd-5 0\n```\n\n> **注意：**\n>\n> 在可用的 PD 节点中，优先级数值最大的节点会直接当选 leader。\n\n### `operator [check | show | add | remove]`\n\n用于显示和控制调度操作。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\n>> operator show                                        // 显示所有的 operators\n>> operator show admin                                  // 显示所有的 admin operators\n>> operator show leader                                 // 显示所有的 leader operators\n>> operator show region                                 // 显示所有的 Region operators\n>> operator add add-peer 1 2                            // 在 store 2 上新增 Region 1 的一个副本\n>> operator add add-learner 1 2                         // 在 store 2 上新增 Region 1 的一个 learner 副本\n>> operator add remove-peer 1 2                         // 移除 store 2 上的 Region 1 的一个副本\n>> operator add transfer-leader 1 2                     // 把 Region 1 的 leader 调度到 store 2\n>> operator add transfer-region 1 2 3 4                 // 把 Region 1 调度到 store 2,3,4\n>> operator add transfer-peer 1 2 3                     // 把 Region 1 在 store 2 上的副本调度到 store 3\n>> operator add merge-region 1 2                        // 将 Region 1 与 Region 2 合并\n>> operator add split-region 1 --policy=approximate     // 将 Region 1 对半拆分成两个 Region，基于粗略估计值\n>> operator add split-region 1 --policy=scan            // 将 Region 1 对半拆分成两个 Region，基于精确扫描值\n>> operator remove 1                                    // 把 Region 1 的调度操作删掉\n>> operator check 1                                     // 查看 Region 1 相关 operator 的状态\n```\n\n其中，Region 的分裂都是尽可能地从靠近中间的位置开始。对这个位置的选择支持两种策略，即 scan 和 approximate。它们之间的区别是，前者通过扫描这个 Region 的方式来确定中间的 key，而后者是通过查看 SST 文件中记录的统计信息，来得到近似的位置。一般来说，前者更加精确，而后者消耗更少的 I/O，可以更快地完成。\n\n### `ping`\n\n用于显示`ping` PD 所需要花费的时间\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nping\n```\n\n```\ntime: 43.12698ms\n```\n\n### `region <region_id> [--jq=\"<query string>\"]`\n\n用于显示 Region 信息。使用 jq 格式化输出请参考 [jq 格式化 json 输出示例](#jq-格式化-json-输出示例)。示例如下。\n\n显示所有 Region 信息：\n\n{{< copyable \"\" >}}\n\n```bash\nregion\n```\n\n```\n{\n  \"count\": 1,\n  \"regions\": [......]\n}\n```\n\n显示 Region id 为 2 的信息：\n\n{{< copyable \"\" >}}\n\n```bash\nregion 2\n```\n\n```\n{\n  \"id\": 2,\n  \"start_key\": \"7480000000000000FF1D00000000000000F8\",\n  \"end_key\": \"7480000000000000FF1F00000000000000F8\",\n  \"epoch\": {\n    \"conf_ver\": 1,\n    \"version\": 15\n  },\n  \"peers\": [\n    {\n      \"id\": 40,\n      \"store_id\": 3\n    }\n  ],\n  \"leader\": {\n    \"id\": 40,\n    \"store_id\": 3\n  },\n  \"written_bytes\": 0,\n  \"read_bytes\": 0,\n  \"written_keys\": 0,\n  \"read_keys\": 0,\n  \"approximate_size\": 1,\n  \"approximate_keys\": 0\n}\n```\n\n### `region key [--format=raw|encode|hex] <key>`\n\n用于查询某个 key 位于哪一个 Region 上，支持 raw、encoding 和 hex 格式。使用 encoding 格式时，key 需要使用单引号。\n\nHex 格式（默认）示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion key 7480000000000000FF1300000000000000F8\n{\n  \"region\": {\n    \"id\": 2,\n    ......\n  }\n}\n```\n\nRaw 格式示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion key --format=raw abc\n```\n\n```\n{\n  \"region\": {\n    \"id\": 2,\n    ......\n  }\n}\n```\n\nEncoding 格式示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion key --format=encode 't\\200\\000\\000\\000\\000\\000\\000\\377\\035_r\\200\\000\\000\\000\\000\\377\\017U\\320\\000\\000\\000\\000\\000\\372'\n```\n\n```\n{\n  \"region\": {\n    \"id\": 2,\n    ......\n  }\n}\n```\n\n### `region scan`\n\n用于获取所有 Region。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion scan\n```\n\n```\n{\n  \"count\": 20,\n  \"regions\": [......],\n}\n```\n\n### `region sibling <region_id>`\n\n用于查询某个 Region 相邻的 Region。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion sibling 2\n```\n\n```\n{\n  \"count\": 2,\n  \"regions\": [......],\n}\n```\n\n### `region keys [--format=raw|encode|hex] <start_key> <end_key> <limit>`\n\n用于查询某个 key 范围内的所有 Region。支持不带 `endKey` 的范围。`limit` 的默认值是 `16`，设为 `-1` 则表示无数量限制。示例如下：\n\n显示从 a 开始的所有 Region 信息，数量上限为 16：\n\n{{< copyable \"\" >}}\n\n```bash\nregion keys --format=raw a\n```\n\n```\n{\n  \"count\": 16,\n  \"regions\": [......],\n}\n```\n\n显示 [a, z) 范围内的所有 Region 信息，数量上限为 16：\n\n{{< copyable \"\" >}}\n\n```bash\nregion keys --format=raw a z\n```\n\n```\n{\n  \"count\": 16,\n  \"regions\": [......],\n}\n```\n\n显示 [a, z) 范围内的所有 Region 信息，无数量上限：\n\n{{< copyable \"\" >}}\n\n```bash\nregion keys --format=raw a z -1\n```\n\n```\n{\n  \"count\": ...,\n  \"regions\": [......],\n}\n```\n\n显示从 a 开始的所有 Region 信息，数量上限为 20：\n\n{{< copyable \"\" >}}\n\n```bash\nregion keys --format=raw a \"\" 20\n```\n\n```\n{\n  \"count\": 20,\n  \"regions\": [......],\n}\n```\n\n### `region store <store_id>`\n\n用于查询某个 store 上面所有的 Region。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion store 2\n```\n\n```\n{\n  \"count\": 10,\n  \"regions\": [......],\n}\n```\n\n### `region topread [limit]`\n\n用于查询读流量最大的 Region。limit 的默认值是 16。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion topread\n```\n\n```\n{\n  \"count\": 16,\n  \"regions\": [......],\n}\n```\n\n### `region topwrite [limit]`\n\n用于查询写流量最大的 Region。limit 的默认值是 16。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion topwrite\n```\n\n```\n{\n  \"count\": 16,\n  \"regions\": [......],\n}\n```\n\n### `region topconfver [limit]`\n\n用于查询 conf version 最大的 Region。limit 的默认值是 16。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion topconfver\n```\n\n```\n{\n  \"count\": 16,\n  \"regions\": [......],\n}\n```\n\n### `region topversion [limit]`\n\n用于查询 version 最大的 Region。limit 的默认值是 16。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion topversion\n```\n\n```\n{\n  \"count\": 16,\n  \"regions\": [......],\n}\n```\n\n### `region topsize [limit]`\n\n用于查询 approximate size 最大的 Region。limit 的默认值是 16。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion topsize\n```\n\n```\n{\n  \"count\": 16,\n  \"regions\": [......],\n}\n```\n\n### `region check [miss-peer | extra-peer | down-peer | pending-peer | offline-peer | empty-region | hist-size | hist-keys] [--jq=\"<query string>\"]`\n\n用于查询处于异常状态的 Region，使用 jq 格式化输出请参考 [jq 格式化 JSON 输出示例](#jq-格式化-json-输出示例)。\n\n各类型的意义如下：\n\n- miss-peer：缺副本的 Region\n- extra-peer：多副本的 Region\n- down-peer：有副本状态为 Down 的 Region\n- pending-peer：有副本状态为 Pending 的 Region\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\nregion check miss-peer\n```\n\n```\n{\n  \"count\": 2,\n  \"regions\": [......],\n}\n```\n\n### `resource-manager [command]`\n\n#### 查看资源管控 (Resource Control) 的 controller 配置\n\n```bash\nresource-manager config controller show\n```\n\n```bash\n{\n    \"degraded-mode-wait-duration\": \"0s\",\n    \"ltb-max-wait-duration\": \"30s\", \n    \"request-unit\": {          # RU 的配置，请勿修改\n        \"read-base-cost\": 0.125,\n        \"read-per-batch-base-cost\": 0.5,\n        \"read-cost-per-byte\": 0.0000152587890625,\n        \"write-base-cost\": 1,\n        \"write-per-batch-base-cost\": 1,\n        \"write-cost-per-byte\": 0.0009765625,\n        \"read-cpu-ms-cost\": 0.3333333333333333\n    },\n    \"enable-controller-trace-log\": \"false\"\n}\n```\n\n- `ltb-max-wait-duration`：本地令牌桶 (Local Token Bucket, LTB) 的最大等待时间。默认值为 `30s`，取值范围为 `[0, 24h]`。如果 SQL 请求预估消耗的 [Request Unit (RU)](/tidb-resource-control.md#什么是-request-unit-ru) 超过了当前 LTB 积累的 RU，则需要等待一定时间。如果预估等待时间超过了此最大等待时间，则会提前向应用返回错误 [`ERROR 8252 (HY000) : Exceeded resource group quota limitation`](/error-codes.md)。增大该值可以减少某些突发并发增加、大事务和大查询的情况下容易报错 `ERROR 8252` 的问题。\n- `enable-controller-trace-log`：controller 诊断日志开关。\n\n#### 修改 Resource Control 的 controller 配置\n\n修改 `ltb-max-wait-duration` 的方法如下：\n\n```bash\npd-ctl resource-manager config controller set ltb-max-wait-duration 30m\n```\n\n### `scheduler [show | add | remove | pause | resume | config | describe]`\n\n用于显示和控制调度策略。\n\n示例：\n\n{{< copyable \"\" >}}\n\n```bash\n>> scheduler show                                         // 显示所有已经创建的 schedulers\n>> scheduler add grant-leader-scheduler 1                 // 把 store 1 上的所有 Region 的 leader 调度到 store 1\n>> scheduler add evict-leader-scheduler 1                 // 把 store 1 上的所有 Region 的 leader 从 store 1 调度出去\n>> scheduler config evict-leader-scheduler                // v4.0.0 起，展示该调度器具体在哪些 store 上\n>> scheduler config evict-leader-scheduler add-store 2    // 为 store 2 添加 leader 驱逐调度\n>> scheduler config evict-leader-scheduler delete-store 2 // 为 store 2 移除 leader 驱逐调度\n>> scheduler add evict-slow-store-scheduler               // 当有且仅有一个 slow store 时将该 store 上的所有 Region 的 leader 驱逐出去\n>> scheduler remove grant-leader-scheduler-1              // 把对应的调度器删掉，`-1` 对应 store ID\n>> scheduler pause balance-region-scheduler 10            // 暂停运行 balance-region 调度器 10 秒\n>> scheduler pause all 10                                 // 暂停运行所有的调度器 10 秒\n>> scheduler resume balance-region-scheduler              // 继续运行 balance-region 调度器\n>> scheduler resume all                                   // 继续运行所有的调度器\n>> scheduler config balance-hot-region-scheduler          // 显示 balance-hot-region 调度器的配置\n>> scheduler describe balance-region-scheduler            // 显示 balance-region 的运行状态和相应的诊断信息\n```\n\n### `scheduler describe balance-region-scheduler`\n\n用于查看 `balance-region-scheduler` 的运行状态和相应的诊断信息。\n\n从 TiDB v6.3.0 起，PD 为 `balance-region-scheduler` 和 `balance-leader-scheduler` 提供了运行状态和简要诊断信息的功能，其余 scheduler 和 checker 暂未支持。你可以通过 `pd-ctl` 修改 [`enable-diagnostic`](/pd-configuration-file.md#enable-diagnostic-从-v630-版本开始引入) 配置项开启该功能。\n\n调度器运行状态有以下几种类型：\n\n- `disabled`：表示当前调度器不可用或被移除。\n- `paused`：表示当前调度器暂停工作。\n- `scheduling`：表示当前调度器正在生成调度。\n- `pending`：表示当前调度器无法产生调度。`pending` 状态的调度器，会返回一个概览信息，来帮助用户诊断。概览信息包含了 store 的一些状态信息，解释了它们为什么不能被选中进行调度。\n- `normal`：表示当前调度器无需进行调度。\n\n### `scheduler config balance-leader-scheduler`\n\n用于查看和控制 `balance-leader-scheduler` 策略。\n\n从 TiDB v6.0.0 起，PD 为 `balance-leader-scheduler` 引入了 `Batch` 参数，用于控制 balance-leader 执行任务的速度。你可以通过 pd-ctl 修改 `balance-leader batch` 配置项设置该功能。\n\n在 v6.0.0 前，PD 不带有该配置（即 `balance-leader batch=1`）。在 v6.0.0 或更高版本中，`balance-leader batch` 的默认值为 `4`。如果你想为该配置项设置大于 `4` 的值，你需要同时调大 [`scheduler-max-waiting-operator`](#config-show--set-option-value--placement-rules)（默认值 `5`）。同时调大两个配置项后，你才能体验预期的加速效果。\n\n```bash\nscheduler config balance-leader-scheduler set batch 3  // 将 balance-leader 调度器可以批量执行的算子大小设置为 3\n```\n\n### `scheduler config balance-hot-region-scheduler`\n\n用于查看和控制 `balance-hot-region-scheduler` 策略。\n\n示例：\n\n```bash\nscheduler config balance-hot-region-scheduler  // 显示 balance-hot-region 调度器的所有配置\n{\n  \"min-hot-byte-rate\": 100,\n  \"min-hot-key-rate\": 10,\n  \"min-hot-query-rate\": 10,\n  \"max-zombie-rounds\": 3,\n  \"max-peer-number\": 1000,\n  \"byte-rate-rank-step-ratio\": 0.05,\n  \"key-rate-rank-step-ratio\": 0.05,\n  \"query-rate-rank-step-ratio\": 0.05,\n  \"count-rank-step-ratio\": 0.01,\n  \"great-dec-ratio\": 0.95,\n  \"minor-dec-ratio\": 0.99,\n  \"src-tolerance-ratio\": 1.05,\n  \"dst-tolerance-ratio\": 1.05,\n  \"read-priorities\": [\n    \"query\",\n    \"byte\"\n  ],\n  \"write-leader-priorities\": [\n    \"key\",\n    \"byte\"\n  ],\n  \"write-peer-priorities\": [\n    \"byte\",\n    \"key\"\n  ],\n  \"strict-picking-store\": \"true\",\n  \"enable-for-tiflash\": \"true\",\n  \"rank-formula-version\": \"v2\"\n}\n```\n\n- `min-hot-byte-rate` 指计数的最小字节数，通常为 100。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set min-hot-byte-rate 100\n    ```\n\n- `min-hot-key-rate` 指计数的最小 key 数，通常为 10。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set min-hot-key-rate 10\n    ```\n\n- `min-hot-query-rate` 指计数的最小 query 数，通常为 10。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set min-hot-query-rate 10\n    ```\n\n- `max-zombie-rounds` 指一个 operator 可被纳入 pending influence 所允许的最大心跳次数。如果将它设置为更大的值，更多的 operator 可能会被纳入 pending influence。通常用户不需要修改这个值。pending influence 指的是在调度中产生的、但仍生效的影响。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set max-zombie-rounds 3\n    ```\n\n- `max-peer-number` 指最多要被解决的 peer 数量。这个配置可避免调度器处理速度过慢。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set max-peer-number 1000\n    ```\n\n- `byte-rate-rank-step-ratio`、`key-rate-rank-step-ratio`、`query-rate-rank-step-ratio` 和 `count-rank-step-ratio` 分别控制 byte、key、query 和 count 的 step ranks。rank-step-ratio 决定了计算 rank 时的 step 值。`great-dec-ratio` 和 `minor-dec-ratio` 控制 `dec` 的 rank。通常用户不需要修改这些配置项。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set byte-rate-rank-step-ratio 0.05\n    ```\n\n- `src-tolerance-ratio` 和 `dst-tolerance-ratio` 是期望调度器的配置项。`tolerance-ratio` 的值越小，调度就越容易。当出现冗余调度时，你可以适当调大这个值。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set src-tolerance-ratio 1.1\n    ```\n\n- `read-priorities`、`write-leader-priorities` 和 `write-peer-priorities` 用于控制调度器优先从哪些维度进行热点均衡，支持配置两个维度。\n\n    - `read-priorities` 和 `write-leader-priorities` 用于控制调度器在处理 read 和 write-leader 类型的热点时优先均衡的维度，可选的维度有 `query`、`byte` 和 `key`。\n    - `write-peer-priorities` 用于控制调度器在处理 write-peer 类型的热点时优先均衡的维度，支持配置 `byte` 和 `key` 维度。\n\n    > **注意：**\n    >\n    > 若集群的所有组件未全部升级到 v5.2 及以上版本，`query` 维度的配置不生效，部分组件升级完成后调度器仍默认优先从 `byte` 和 `key` 维度进行热点均衡，集群的所有组件全部升级完成后，也会继续保持这样的兼容配置，可通过 `pd-ctl` 查看实时配置。通常用户不需要修改这些配置项。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set read-priorities query,byte\n    ```\n\n- `strict-picking-store` 是控制热点调度搜索空间的开关，通常为打开。该配置项仅影响 `rank-formula-version` 为 `v1` 时的行为。当打开时，热点调度的目标是保证所配置的两个维度的热点均衡。当关闭后，热点调度只保证处于第一优先级的维度的热点均衡表现更好，但可能会导致其他维度的热点不再那么均衡。通常用户不需要修改这个配置项。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set strict-picking-store true\n    ```\n\n- `rank-formula-version` 适用于热点调度，其用来确定调度策略的算法版本，支持的值有 `[\"v1\", \"v2\"]`。目前该配置的默认值为 `v2`。\n\n    - `v1` 版本为 v6.3.0 之前的策略，主要关注调度是否降低了不同 Store 之间的负载差值，以及是否在另一维度引入副作用。\n    - `v2` 版本是 v6.3.0 引入的实验特性算法，在 v6.4.0 正式发布，主要关注 Store 之间均衡度的提升率，同时降低了对副作用的关注度。对比 `strict-picking-store` 为 `true` 的 `v1` 算法，`v2` 版本更注重优先均衡第一维度。对比 `strict-picking-store` 为 `false` 的 `v1` 算法，`v2` 版本兼顾了第二维度的均衡。\n    - `strict-picking-store` 为 `true` 的 `v1` 版本算法较为保守，只有当存在两个维度的负载都偏高的 Store 时才能产生调度。在特定场景下有可能因为维度冲突导致无法继续均衡，需要将 `strict-picking-store` 改为 `false` 才能在第一维度取得更好的均衡效果。`v2` 版本算法则可以在两个维度都取得更好的均衡效果，并减少无效调度。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set rank-formula-version v2\n    ```\n\n- `enable-for-tiflash` 是控制热点调度是否对 TiFlash 生效的开关。通常为打开，关闭后将不会产生 TiFlash 实例之间的热点调度。\n\n    ```bash\n    scheduler config balance-hot-region-scheduler set enable-for-tiflash true\n    ```\n\n### `scheduler config evict-leader-scheduler`\n\n用于查看和管理 `evict-leader-scheduler` 的配置。\n\n- 在已有 `evict-leader-scheduler` 时，使用 `add-store` 子命令，为指定的 store 添加 leader 驱逐调度：\n\n    ```bash\n    scheduler config evict-leader-scheduler add-store 2       // 为 store 2 添加 leader 驱逐调度\n    ```\n\n- 在已有 `evict-leader-scheduler` 时，使用 `delete-store` 子命令，移除指定 store 的 leader 驱逐调度：\n\n    ```bash\n    scheduler config evict-leader-scheduler delete-store 2    // 为 store 2 移除 leader 驱逐调度\n    ```\n\n    当一个 `evict-leader-scheduler` 的所有 store 配置都被移除后，该调度器也会自动被移除。\n\n- 在已有 `evict-leader-scheduler` 时，使用 `set batch` 子命令修改 `batch` 值。其中，`batch` 用于调整单次调度过程中生成的 Operator 数量，默认值为 `3`，取值范围为 `[1, 10]`。`batch` 值越大，调度速度越快。\n\n    ```bash\n    scheduler config evict-leader-scheduler set batch 10 // 设置 batch 值为 10\n    ```\n\n### `service-gc-safepoint`\n\n用于查询当前的 GC safepoint 与 service GC safepoint，输出结果示例如下：\n\n```bash\n{\n  \"service_gc_safe_points\": [\n    {\n      \"service_id\": \"gc_worker\",\n      \"expired_at\": 9223372036854775807,\n      \"safe_point\": 439923410637160448\n    }\n  ],\n  \"gc_safe_point\": 0\n}\n```\n\n### `store [delete | cancel-delete | label | weight | remove-tombstone | limit ] <store_id> [--jq=\"<query string>\"]`\n\n使用 jq 格式化输出请参考 [jq 格式化 json 输出示例](#jq-格式化-json-输出示例)。\n\n#### 查询 store\n\n显示所有 store 信息：\n\n```bash\nstore\n```\n\n```\n{\n  \"count\": 3,\n  \"stores\": [...]\n}\n```\n\n获取 id 为 1 的 store：\n\n```bash\nstore 1\n```\n\n```\n......\n```\n\n#### 下线 store\n\n下线 id 为 1 的 store：\n\n```bash\nstore delete 1\n```\n\n执行 `store cancel-delete` 命令，你可以撤销已使用 `store delete` 下线并处于 `Offline` 状态的 store。撤销后，该 store 会从 `Offline` 状态变为 `Up` 状态。注意，`store cancel-delete` 命令无法使 `Tombstone` 状态的 store 变回 `Up` 状态。\n\n撤销通过 `store delete` 下线 id 为 1 的 store：\n\n```bash\nstore cancel-delete 1\n```\n\n删除所有 Tombstone 状态的 store：\n\n```bash\nstore remove-tombstone\n```\n\n> **注意：**\n>\n> 若下线过程中切换了 PD leader，需要使用 `store limit` 命令修改 [store 调度限速](#设置-store-调度限速)。\n\n#### 管理 store label\n\n`store label` 命令用于管理 store label。\n\n- 为 id 为 1 的 store 设置键为 `\"zone\"`、值为 `\"cn\"` 的 label：\n\n    ```bash\n    store label 1 zone=cn\n    ```\n\n- 更新 id 为 1 的 store 的 label：\n\n    ```bash\n    store label 1 zone=us\n    ```\n\n- 通过 `--rewrite` 选项重写 id 为 1 的 store 的所有 label，之前的 label 会被覆盖：\n\n    ```bash\n    store label 1 region=us-est-1 disk=ssd --rewrite\n    ```\n\n- 删除 id 为 1 的 store 的键为 `\"disk\"` 的 label ：\n\n    ```bash\n    store label 1 disk --delete\n    ```\n\n> **注意：**\n>\n> - store 的 label 更新方法使用的是合并策略。如果修改了 TiKV 配置文件中的 store label，进程重启之后，PD 会将自身存储的 store label 与其进行合并更新，并持久化合并后的结果。\n> - 如果希望使用 TiUP 统一管理 store label，你可以在集群重启前，使用 PD Control 的 `store label <id> --force` 命令将 PD 存储的 store label 清空。\n\n#### 设置 store weight\n\n将 id 为 1 的 store 的 leader weight 设为 5，Region weight 设为 10：\n\n```bash\nstore weight 1 5 10\n```\n\n#### 设置 store 调度限速\n\n通过 `store-limit`，你可以设置 store 的调度速度。关于 `store limit` 的原理和使用方法，请参考 [`store limit`](/configure-store-limit.md)。\n\n```bash\n>> store limit                         // 显示所有 store 添加和删除 peer 的速度上限\n>> store limit add-peer                // 显示所有 store 添加 peer 的速度上限\n>> store limit remove-peer             // 显示所有 store 删除 peer 的速度上限\n>> store limit all 5                   // 设置所有 store 添加和删除 peer 的速度上限为每分钟 5 个\n>> store limit 1 5                     // 设置 store 1 添加和删除 peer 的速度上限为每分钟 5 个\n>> store limit all 5 add-peer          // 设置所有 store 添加 peer 的速度上限为每分钟 5 个\n>> store limit 1 5 add-peer            // 设置 store 1 添加 peer 的速度上限为每分钟 5 个\n>> store limit 1 5 remove-peer         // 设置 store 1 删除 peer 的速度上限为每分钟 5 个\n>> store limit all 5 remove-peer       // 设置所有 store 删除 peer 的速度上限为每分钟 5 个\n```\n\n> **注意：**\n>\n> 使用 `pd-ctl` 可以查看 TiKV 节点的状态信息，即 `Up`，`Disconnect`，`Offline`，`Down`，或 `Tombstone`。如需查看各个状态之间的关系，请参考 [TiKV Store 状态之间的关系](/tidb-scheduling.md#信息收集)。\n\n### `log [fatal | error | warn | info | debug]`\n\n用于设置 PD leader 的日志级别。\n\n{{< copyable \"\" >}}\n\n```bash\nlog warn\n```\n\n### `tso`\n\n用于解析 TSO 到物理时间和逻辑时间。示例如下。\n\n解析 TSO：\n\n{{< copyable \"\" >}}\n\n```bash\ntso 395181938313123110\n```\n\n```\nsystem:  2017-10-09 05:50:59 +0800 CST\nlogic:  120102\n```\n\n### `unsafe remove-failed-stores [store-ids | show]`\n\n> **警告：**\n>\n> - 此功能为有损恢复，无法保证数据和数据索引完整性。\n> - 建议在 TiDB 团队支持下进行相关操作，操作不当可能导致集群难以恢复。\n\n用于在多数副本永久损坏造成数据不可用时进行有损恢复。示例如下。详见 [Online Unsafe Recovery](/online-unsafe-recovery.md)。\n\n执行 Online Unsafe Recovery，移除永久损坏的节点 (Store):\n\n```bash\nunsafe remove-failed-stores 101,102,103\n```\n\n```bash\nSuccess!\n```\n\n显示正在运行的 Online Unsafe Recovery 的当前状态或历史状态。\n\n```bash\nunsafe remove-failed-stores show\n```\n\n```bash\n[\n  \"Collecting cluster info from all alive stores, 10/12.\",\n  \"Stores that have reports to PD: 1, 2, 3, ...\",\n  \"Stores that have not reported to PD: 11, 12\",\n]\n```\n\n## jq 格式化 JSON 输出示例\n\n### 简化 `store` 的输出\n\n{{< copyable \"\" >}}\n\n```bash\nstore --jq=\".stores[].store | { id, address, state_name}\"\n```\n\n```\n{\"id\":1,\"address\":\"127.0.0.1:20161\",\"state_name\":\"Up\"}\n{\"id\":30,\"address\":\"127.0.0.1:20162\",\"state_name\":\"Up\"}\n...\n```\n\n### 查询节点剩余空间\n\n{{< copyable \"\" >}}\n\n```bash\nstore --jq=\".stores[] | {id: .store.id, available: .status.available}\"\n```\n\n```\n{\"id\":1,\"available\":\"10 GiB\"}\n{\"id\":30,\"available\":\"10 GiB\"}\n...\n```\n\n### 查询状态不为 Up 的所有节点\n\n{{< copyable \"\" >}}\n\n```bash\nstore --jq='.stores[].store | select(.state_name!=\"Up\") | { id, address, state_name}'\n```\n\n```\n{\"id\":1,\"address\":\"127.0.0.1:20161\"\"state_name\":\"Offline\"}\n{\"id\":5,\"address\":\"127.0.0.1:20162\"\"state_name\":\"Offline\"}\n...\n```\n\n### 查询所有的 TiFlash 节点\n\n{{< copyable \"\" >}}\n\n```bash\nstore --jq='.stores[].store | select(.labels | length>0 and contains([{\"key\":\"engine\",\"value\":\"tiflash\"}])) | { id, address, state_name}'\n```\n\n```\n{\"id\":1,\"address\":\"127.0.0.1:20161\"\"state_name\":\"Up\"}\n{\"id\":5,\"address\":\"127.0.0.1:20162\"\"state_name\":\"Up\"}\n...\n```\n\n### 查询 Region 副本的分布情况\n\n{{< copyable \"\" >}}\n\n```bash\nregion --jq=\".regions[] | {id: .id, peer_stores: [.peers[].store_id]}\"\n```\n\n```\n{\"id\":2,\"peer_stores\":[1,30,31]}\n{\"id\":4,\"peer_stores\":[1,31,34]}\n...\n```\n\n### 根据副本数过滤 Region\n\n例如副本数不为 3 的所有 Region：\n\n{{< copyable \"\" >}}\n\n```bash\nregion --jq=\".regions[] | {id: .id, peer_stores: [.peers[].store_id] | select(length != 3)}\"\n```\n\n```\n{\"id\":12,\"peer_stores\":[30,32]}\n{\"id\":2,\"peer_stores\":[1,30,31,32]}\n```\n\n### 根据副本 store ID 过滤 Region\n\n例如在 store30 上有副本的所有 Region：\n\n{{< copyable \"\" >}}\n\n```bash\nregion --jq=\".regions[] | {id: .id, peer_stores: [.peers[].store_id] | select(any(.==30))}\"\n```\n\n```\n{\"id\":6,\"peer_stores\":[1,30,31]}\n{\"id\":22,\"peer_stores\":[1,30,32]}\n...\n```\n\n还可以像这样找出在 store30 或 store31 上有副本的所有 Region：\n\n{{< copyable \"\" >}}\n\n```bash\nregion --jq=\".regions[] | {id: .id, peer_stores: [.peers[].store_id] | select(any(.==(30,31)))}\"\n```\n\n```\n{\"id\":16,\"peer_stores\":[1,30,34]}\n{\"id\":28,\"peer_stores\":[1,30,32]}\n{\"id\":12,\"peer_stores\":[30,32]}\n...\n```\n\n### 恢复数据时寻找相关 Region\n\n例如当 [store1, store30, store31] 宕机时不可用时，我们可以通过查找所有 Down 副本数量大于正常副本数量的所有 Region：\n\n{{< copyable \"\" >}}\n\n```bash\nregion --jq=\".regions[] | {id: .id, peer_stores: [.peers[].store_id] | select(length as $total | map(if .==(1,30,31) then . else empty end) | length>=$total-length) }\"\n```\n\n```\n{\"id\":2,\"peer_stores\":[1,30,31,32]}\n{\"id\":12,\"peer_stores\":[30,32]}\n{\"id\":14,\"peer_stores\":[1,30,32]}\n...\n```\n\n或者在 [store1, store30, store31] 无法启动时，找出 store1 上可以安全手动移除数据的 Region。我们可以这样过滤出所有在 store1 上有副本并且没有其他 DownPeer 的 Region：\n\n{{< copyable \"\" >}}\n\n```bash\nregion --jq=\".regions[] | {id: .id, peer_stores: [.peers[].store_id] | select(length>1 and any(.==1) and all(.!=(30,31)))}\"\n```\n\n```\n{\"id\":24,\"peer_stores\":[1,32,33]}\n```\n\n在 [store30, store31] 宕机时，找出能安全地通过创建 `remove-peer` Operator 进行处理的所有 Region，即有且仅有一个 DownPeer 的 Region：\n\n{{< copyable \"\" >}}\n\n```bash\nregion --jq=\".regions[] | {id: .id, remove_peer: [.peers[].store_id] | select(length>1) | map(if .==(30,31) then . else empty end) | select(length==1)}\"\n```\n\n```\n{\"id\":12,\"remove_peer\":[30]}\n{\"id\":4,\"remove_peer\":[31]}\n{\"id\":22,\"remove_peer\":[30]}\n...\n```\n"
        },
        {
          "name": "pd-microservices-deployment-topology.md",
          "type": "blob",
          "size": 3.5751953125,
          "content": "---\ntitle: PD 微服务部署拓扑\nsummary: 了解在部署最小拓扑集群的基础上，部署 PD 微服务的拓扑结构。\n---\n\n# PD 微服务部署拓扑\n\n本文介绍在部署最小拓扑集群的基础上，部署 [PD 微服务](/pd-microservices.md)的拓扑结构。\n\n## 拓扑信息\n\n| 实例                 | 个数 | 物理机配置                        | IP                                        | 配置                        |\n| :------------------- | :--- | :-------------------------------- | :---------------------------------------- | :-------------------------- |\n| TiDB                 | 2    | 16 VCore 32GB \\* 1                | 10.0.1.1 <br/> 10.0.1.2                   | 默认端口 <br/> 全局目录配置 |\n| PD                   | 3    | 4 VCore 8GB \\* 1                  | 10.0.1.3 <br/> 10.0.1.4 <br/> 10.0.1.5    | 默认端口 <br/> 全局目录配置 |\n| TSO                  | 2    | 4 VCore 8GB \\* 1                  | 10.0.1.6 <br/> 10.0.1.7                   | 默认端口 <br/> 全局目录配置 |\n| Scheduling           | 2    | 4 VCore 8GB \\* 1                  | 10.0.1.8 <br/> 10.0.1.9                   | 默认端口 <br/> 全局目录配置 |\n| TiKV                 | 3    | 16 VCore 32GB 2TB (nvme ssd) \\* 1 | 10.0.1.10 <br/> 10.0.1.11 <br/> 10.0.1.12 | 默认端口 <br/> 全局目录配置 |\n| Monitoring & Grafana | 1    | 4 VCore 8GB \\* 1 500GB (ssd)      | 10.0.1.13                                 | 默认端口 <br/> 全局目录配置 |\n\n### 拓扑模版\n\n<details>\n<summary>简单 PD 微服务配置模版</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n  listen_host: 0.0.0.0\n  arch: \"amd64\"\n  pd_mode: \"ms\" # To enable PD microservices, you must specify this field as \"ms\".\n\nmonitored:\n  node_exporter_port: 9200\n  blackbox_exporter_port: 9215\n\n# # Specifies the configuration of PD servers.\npd_servers:\n  - host: 10.0.1.3\n  - host: 10.0.1.4\n  - host: 10.0.1.5\n\n# # Specifies the configuration of TiDB servers.\ntidb_servers:\n  - host: 10.0.1.1\n  - host: 10.0.1.2\n\n# # Specifies the configuration of TiKV servers.\ntikv_servers:\n  - host: 10.0.1.10\n  - host: 10.0.1.11\n  - host: 10.0.1.12\n\n# # Specifies the configuration of TSO servers.\ntso_servers:\n  - host: 10.0.1.6\n  - host: 10.0.1.7\n\n# # Specifies the configuration of Scheduling servers.\nscheduling_servers:\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\n# # Specifies the configuration of Prometheus servers.\nmonitoring_servers:\n  - host: 10.0.1.13\n\n# # Specifies the configuration of Grafana servers.\ngrafana_servers:\n  - host: 10.0.1.13\n```\n\n</details>\n\n以上 TiDB 集群拓扑文件中，详细的配置项说明见[通过 TiUP 部署 TiDB 集群的拓扑文件配置](/tiup/tiup-cluster-topology-reference.md)。\n\n### 关键参数介绍\n\n- `tso_servers` 实例级别配置 `host` 目前只支持 IP 地址，不支持域名。\n- TSO 具体的参数配置介绍可参考 [TSO 参数配置](/tso-configuration-file.md)。\n- `scheduling_servers` 实例级别配置 `host` 目前只支持 IP 地址，不支持域名。\n- Scheduling 具体的参数配置介绍可参考 [Scheduling 参数配置](/scheduling-configuration-file.md)。\n\n> **注意：**\n>\n> - 无需手动创建配置文件中的 `tidb` 用户，TiUP cluster 组件会在目标主机上自动创建该用户。可以自定义用户，也可以和中控机的用户保持一致。\n> - 如果部署目录配置为相对路径，会部署在用户的 Home 目录下。\n"
        },
        {
          "name": "pd-microservices.md",
          "type": "blob",
          "size": 5.6728515625,
          "content": "---\ntitle: PD 微服务\nsummary: 介绍如何开启 PD 微服务模式，以提高服务质量。\n---\n\n# PD 微服务\n\n从 v8.0.0 开始，PD 支持微服务模式。该模式可将 PD 的时间戳分配和集群调度功能拆分为以下微服务单独部署，从而与 PD 的路由功能解耦，让 PD 专注于元数据的路由服务。\n\n- `tso` 微服务：为整个集群提供单调递增的时间戳分配。\n- `scheduling` 微服务：为整个集群提供调度功能，包括但不限于负载均衡、热点处理、副本修复、副本放置等。\n\n每个微服务都以独立进程的方式部署。当设置某个微服务的副本数大于 1 时，该微服务会自动实现主备的容灾模式，以确保服务的高可用性和可靠性。\n\n> **警告：**\n>\n> PD 微服务目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/tikv/pd/issues) 反馈。\n\n## 使用场景\n\nPD 微服务通常用于解决 PD 出现性能瓶颈的问题，提高 PD 服务质量。利用该特性，你可以避免以下问题：\n\n- PD 集群压力过大而导致 TSO 分配的长尾延迟或者抖动现象\n- 调度模块故障导致整个集群服务不可用的问题\n- PD 自身单点瓶颈的问题\n\n此外，当调度模块发生变更时，你可以单独更新 `scheduling` 微服务，无需再对 PD 进行重启，进而不会影响集群的整体服务。\n\n> **注意：**\n>\n> 如果集群的性能瓶颈不是 PD 引起的，则无需开启微服务。微服务本身会增加组件数量，提高运维成本。\n\n## 使用限制\n\n- `tso` 微服务目前不支持动态启停，开启或关闭 `tso` 微服务后你需要重启 PD 集群才会生效。\n- 只有 TiDB 组件支持通过服务发现直接连接 `tso` 微服务，其他的组件是通过请求转发的方式，将请求通过 PD 转发到 `tso` 微服务以获取时间戳。\n- 与[同步部署模式 (DR Auto-Sync)](/two-data-centers-in-one-city-deployment.md) 特性不兼容。\n- 与 TiDB 系统变量 [`tidb_enable_tso_follower_proxy`](/system-variables.md#tidb_enable_tso_follower_proxy-从-v530-版本开始引入) 不兼容。\n- 由于集群中可能存在[静默 Region](/tikv-configuration-file.md#hibernate-regions)，`scheduling` 微服务在进行主备切换时，为避免冗余调度，集群可能存在最长 [`peer-stale-state-check-interval`](/tikv-configuration-file.md#peer-stale-state-check-interval) 时间内（默认为五分钟）没有调度的现象。\n\n## 使用方法\n\nPD 微服务支持通过 [TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable) 或 [TiUP](/tiup/tiup-overview.md) 进行部署。\n\n<SimpleTab>\n<div label=\"TiDB Operator\">\n\n对于通过 TiDB Operator 部署的 TiDB 集群，PD 微服务详细使用方法请参考以下文档：\n\n- [部署 PD 微服务](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/configure-a-tidb-cluster#部署-pd-微服务)\n- [配置 PD 微服务](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/configure-a-tidb-cluster#配置-pd-微服务)\n- [修改 PD 微服务](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/modify-tidb-configuration#修改-pd-微服务配置)\n- [扩容缩容 PD 微服务组件](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/scale-a-tidb-cluster#扩缩容-pd-微服务组件)\n\n</div>\n<div label=\"TiUP\">\n\n对于通过 TiUP 部署的 TiDB 集群，PD 微服务详细使用方法请参考以下文档：\n\n- [部署 PD 微服务](/pd-microservices-deployment-topology.md)\n- [扩容缩容 PD 微服务节点](/scale-microservices-using-tiup.md)\n- 配置 `tso` 微服务 \n    - [通过配置文件配置](/tso-configuration-file.md)\n    - [通过命令行参数配置](/command-line-flags-for-tso-configuration.md)\n- 配置 `scheduling` 微服务 \n    - [通过配置文件配置](/scheduling-configuration-file.md)\n    - [通过命令行参数配置](/command-line-flags-for-scheduling-configuration.md)\n\n</div>\n<div label=\"TiUP Playground\">\n\n对于通过 TiUP 的 Playground 组件部署的 TiDB 本地集群，PD 微服务详细使用方法请参考以下文档：\n\n- [部署 PD 微服务](/tiup/tiup-playground.md#部署-pd-微服务)\n\n</div>\n</SimpleTab>\n\n## 注意事项\n\n当部署和使用 PD 微服务时，请注意以下事项：\n\n- 开启微服务并重启 PD 后，PD 不再提供 TSO 分配功能。因此，开启微服务时，你需要在集群中部署 `tso` 微服务。\n- 如果集群中部署了 `scheduling` 微服务，调度功能将由 `scheduling` 微服务提供。如果没有部署 `scheduling` 微服务，调度功能仍然由 PD 提供。\n- `scheduling` 微服务支持动态切换。该功能默认开启（`enable-scheduling-fallback` 默认为 `true`）。如果 `scheduling` 微服务进程关闭，PD 默认会继续为集群提供调度服务。\n\n    如果 `scheduling` 微服务和 PD 使用的 binary 版本不同，为防止调度逻辑出现变化，可以通过执行 `pd-ctl config set enable-scheduling-fallback false` 关闭 `scheduling` 微服务动态切换功能。关闭后，如果 `scheduling` 微服务的进程关闭，PD 将不会接管调度服务。这意味着，在 `scheduling` 微服务重新启动前，集群将无法提供调度服务。\n\n## 工具兼容性\n\n微服务不影响数据导入导出以及其他同步工具的正常使用。\n\n## 常见问题\n\n- 如何判断 PD 是否达到了性能瓶颈?\n\n  在集群自身状态正常的前提下，可以查看 Grafana PD 面板中的监控指标。如果 `TiDB - PD server TSO handle time` 指标出现明显延迟上涨或 `Heartbeat - TiKV side heartbeat statistics` 指标出现大量 pending，说明 PD 达到了性能瓶颈。\n"
        },
        {
          "name": "pd-recover.md",
          "type": "blob",
          "size": 7.0009765625,
          "content": "---\ntitle: PD Recover 使用文档\naliases: ['/docs-cn/dev/pd-recover/','/docs-cn/dev/reference/tools/pd-recover/']\nsummary: PD Recover 是用于恢复无法正常启动或服务的 PD 集群的工具。安装方式包括从源代码编译和下载 TiDB 工具包。恢复集群的方式有两种：从存活的 PD 节点重建和完全重建。从存活的 PD 节点重建集群需要停止所有节点，启动存活的 PD 节点，并使用 pd-recover 修复元数据。完全重建 PD 集群需要获取 Cluster ID 和已分配 ID，部署新的 PD 集群，使用 pd-recover 修复，然后重启整个集群。\n---\n\n# PD Recover 使用文档\n\nPD Recover 是对 PD 进行灾难性恢复的工具，用于恢复无法正常启动或服务的 PD 集群。\n\n## 安装 PD Recover\n\n要使用 PD Recover，你可以[从源代码编译](#从源代码编译)，也可以直接[下载 TiDB 工具包](#下载-tidb-工具包)。\n\n### 从源代码编译\n\n* [Go](https://golang.org/)：PD Recover 使用了 Go 模块，请安装 Go 1.23 或以上版本。\n* 在 [PD](https://github.com/pingcap/pd) 根目录下，运行 `make pd-recover` 命令来编译源代码并生成 `bin/pd-recover`。\n\n> **注意：**\n>\n> 一般来说，用户不需要编译源代码，因为发布的二进制文件或 Docker 中已包含 PD Recover 工具。开发者可以参考以上步骤来编译源代码。\n\n### 下载 TiDB 工具包\n\nPD Recover 的安装包位于 TiDB 离线工具包中。下载方式，请参考 [TiDB 工具下载](/download-ecosystem-tools.md)。\n\n下面介绍两种重建集群的方式：从存活的 PD 节点重建和完全重建。\n\n## 方式一：从存活的 PD 节点重建集群\n\n当 PD 集群的大多数节点发生灾难性故障时，集群将无法提供服务。当还有 PD 节点存活时，可以选择一个存活的 PD 节点，通过强制修改 Raft Group 的成员，使该节点重新恢复服务。具体操作步骤如下：\n\n### 第 1 步：停止所有节点\n\n停止集群中的 TiDB、TiKV 和 TiFlash 服务进程，以防止在恢复过程中与 PD 参数交互，造成数据错乱或其他无法挽救的异常状况。\n\n### 第 2 步：启动存活的 PD 节点\n\n使用启动参数 `--force-new-cluster` 拉起该存活的 PD 节点，如：\n\n```shell\n./bin/pd-server --force-new-cluster --name=pd-127.0.0.10-2379 --client-urls=http://0.0.0.0:2379 --advertise-client-urls=http://127.0.0.1:2379 --peer-urls=http://0.0.0.0:2380 --advertise-peer-urls=http://127.0.0.1:2380 --config=conf/pd.toml\n```\n\n### 第 3 步：使用 `pd-recover` 修复元数据\n\n该方法是利用少数派 PD 节点恢复服务，但由于该节点可能存在数据落后的情况，因此对于 `alloc_id` 和 `tso` 等数据，一旦发生回退，可能导致集群数据错乱或不可用。为确保该节点能提供正确的分配 ID 和 TSO 等服务，需要使用 `pd-recover` 修改元数据。具体命令参考：\n\n```shell\n./bin/pd-recover --from-old-member --endpoints=http://127.0.0.1:2379 # 指定对应的 PD 地址\n```\n\n> **注意：**\n>\n> 该步骤会自动将存储中的 `alloc_id` 增加一个安全值 `100000000`。这将导致后续集群中分配的 ID 偏大。\n>\n> 此外，`pd-recover` 不会修改 TSO。因此，在执行此步骤之前，请确保本地时间晚于故障发生时间，并且确认故障前 PD 组件之间已开启 NTP 时钟同步服务。如果未开启，则需要将本地时钟调整到一个未来的时间，以确保 TSO 不会回退。\n\n### 第 4 步：重启这个 PD\n\n当上一步出现 `recovery is successful` 的提示信息后，重启 PD。\n\n### 第 5 步：扩容 PD 并启动集群\n\n通过部署工具扩容 PD，并启动集群中的其他组件。至此服务恢复。\n\n## 方式二：完全重建 PD 集群\n\n该方式适用于所有 PD 的数据都丢失，但 TiDB、TiKV 和 TiFlash 等其他组件数据都还存在的情况。\n\n### 第 1 步：获取 Cluster ID\n\n一般在 PD、TiKV 或 TiDB 的日志中都可以获取 Cluster ID。你可以直接在服务器上查看日志以获取 Cluster ID。\n\n#### 从 PD 日志获取 Cluster ID（推荐）\n\n使用以下命令，从 PD 日志中获取 Cluster ID：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ngrep \"init cluster id\" {{/path/to}}/pd.log\n```\n\n```bash\n[2019/10/14 10:35:38.880 +00:00] [INFO] [server.go:212] [\"init cluster id\"] [cluster-id=6747551640615446306]\n...\n```\n\n或者也可以从 TiDB 或 TiKV 的日志中获取。\n\n#### 从 TiDB 日志获取 Cluster ID\n\n使用以下命令，从 TiDB 日志中获取 Cluster ID：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ngrep \"init cluster id\" {{/path/to}}/tidb.log\n```\n\n```bash\n2019/10/14 19:23:04.688 client.go:161: [info] [pd] init cluster id 6747551640615446306\n...\n```\n\n#### 从 TiKV 日志获取 Cluster ID\n\n使用以下命令，从 TiKV 日志中获取 Cluster ID：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ngrep \"connect to PD cluster\" {{/path/to}}/tikv.log\n```\n\n```bash\n[2019/10/14 07:06:35.278 +00:00] [INFO] [tikv-server.rs:464] [\"connect to PD cluster 6747551640615446306\"]\n...\n```\n\n### 第 2 步：获取已分配 ID\n\n在指定已分配 ID 时，需指定一个比当前最大的已分配 ID 更大的值。可以从监控中获取已分配 ID，也可以直接在服务器上查看日志。\n\n#### 从监控中获取已分配 ID（推荐）\n\n要从监控中获取已分配的 ID，需要确保你所查看的监控指标是**上一任 PD Leader** 的指标。可从 PD Dashboard 中 **Current ID allocation** 面板获取最大的已分配 ID。\n\n#### 从 PD 日志获取已分配 ID\n\n要从 PD 日志中获取分配的 ID，需要确保你所查看的日志是**上一任 PD Leader** 的日志。运行以下命令获取最大的已分配 ID：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ngrep \"idAllocator allocates a new id\" {{/path/to}}/pd*.log |  awk -F'=' '{print $2}' | awk -F']' '{print $1}' | sort -r -n | head -n 1\n```\n\n```bash\n4000\n...\n```\n\n你也可以在所有 PD server 中运行上述命令，找到最大的值。\n\n### 第 3 步：部署一套新的 PD 集群\n\n部署新的 PD 集群之前，需要停止当前的 PD 集群，然后删除旧的数据目录（或者用 `--data-dir` 指定新的数据目录）。\n\n### 第 4 步：使用 pd-recover\n\n只需在一个 PD 节点上执行 `pd-recover` 即可。需要注意的是，为了避免重新分配，建议将参数 `-alloc-id` 设置为大于已分配 ID 的值。例如，从监控或者日志获得的最大已分配 ID 是 `9000`，则建议给参数 `-alloc-id` 传入 `10000` 或更大值。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\n./pd-recover -endpoints http://10.0.1.13:2379 -cluster-id 6747551640615446306 -alloc-id 10000\n```\n\n### 第 5 步：重启整个集群\n\n当出现 `recovery is successful` 的提示信息时，重启整个集群。\n\n## 常见问题\n\n### 获取 Cluster ID 时发现有多个 Cluster ID\n\n新建 PD 集群时，会生成新的 Cluster ID。可以通过日志判断旧集群的 Cluster ID。\n\n### 执行 pd-recover 时返回错误 `dial tcp 10.0.1.13:2379: connect: connection refused`\n\n执行 pd-recover 时需要 PD 提供服务，请先部署并启动 PD 集群。\n"
        },
        {
          "name": "performance-schema",
          "type": "tree",
          "content": null
        },
        {
          "name": "performance-tuning-methods.md",
          "type": "blob",
          "size": 33.25390625,
          "content": "---\ntitle: TiDB 性能分析和优化方法\nsummary: 本文介绍了基于数据库时间的系统优化方法，以及如何利用 TiDB Performance Overview 面板进行性能分析和优化。\naliases: ['/zh/tidb/v6.0/performance-tuning-methods']\n---\n\n# TiDB 性能分析和优化\n\n本文介绍了基于数据库时间的系统优化方法，以及如何利用 TiDB [Performance Overview 面板](/grafana-performance-overview-dashboard.md)进行性能分析和优化。\n\n通过本文中介绍的方法，你可以从全局、自顶向下的角度分析用户响应时间和数据库时间，确认用户响应时间的瓶颈是否在数据库中。如果瓶颈在数据库中，你可以通过数据库时间概览和 SQL 延迟的分解，定位数据库内部的瓶颈点，并进行针对性的优化。\n\n## 基于数据库时间的性能优化方法\n\nTiDB 对 SQL 的处理路径和数据库时间进行了完善的测量和记录，方便定位数据库的性能瓶颈。即使在用户响应时间的性能数据缺失的情况下，基于 TiDB 数据库时间的相关性能指标，你也可以达到以下两个性能分析目标：\n\n1. 通过对比 SQL 处理平均延迟和事务中 TiDB 连接的空闲时间，确定整个系统的瓶颈是否在 TiDB 中。\n2. 如果瓶颈在 TiDB 内部，根据数据库时间概览、颜色优化法、关键指标和资源利用率、自上而下的延迟分解，定位到性能瓶颈具体在整个分布式系统的哪个模块。\n\n### 确定整个系统的瓶颈是否在 TiDB 中\n\n- 如果事务中 TiDB 连接的平均空闲时间比 SQL 平均处理延迟高，说明应用的事务处理中，主要的延迟不在数据库中，数据库时间占用户响应时间比例小，可以确认瓶颈不在数据库中。\n\n    在这种情况下，需要关注数据库外部的组件，比如应用服务器硬件资源是否存在瓶颈，应用到数据库的网络延迟是否过高等。\n\n- 如果 SQL 平均处理延迟比事务中 TiDB 连接的平均空闲时间高，说明事务中主要的瓶颈在 TiDB 内部，数据库时间占用户响应时间比例大。\n\n### 如果瓶颈在 TiDB 内部，如何定位\n\n一个典型的 SQL 的处理流程如下所示，TiDB 的性能指标覆盖了绝大部分的处理路径，对数据库时间进行不同维度的分解和上色，用户可以快速的了解负载特性和数据库内部的瓶颈。\n\n![数据库时间分解图](/media/performance/dashboard-diagnostics-time-relation.png)\n\n数据库时间是所有 SQL 处理时间的总和。通过以下三个维度对数据库时间进行分解，可以帮助你快速定位 TiDB 内部瓶颈：\n\n- 按 SQL 处理类型分解，判断哪种类型的 SQL 语句消耗数据库时间最多。对应的分解公式为：\n\n    `DB Time = Select Time + Insert Time + Update Time + Delete Time + Commit Time + ...`\n\n- 按 SQL 处理的 4 个步骤（即 get_token/parse/compile/execute）分解，判断哪个步骤消耗的时间最多。对应的分解公式为：\n\n    `DB Time = Get Token Time + Parse Time + Compile Time + Execute Time`\n\n- 对于 execute 耗时，按照 TiDB 执行器本身的时间、TSO 等待时间、KV 请求时间和重试的执行时间，判断执行阶段的瓶颈。对应的分解公式为：\n\n    `Execute Time ~= TiDB Executor Time + KV Request Time + PD TSO Wait Time + Retried execution time`\n\n## 利用 Performance Overview 面板进行性能分析和优化\n\n本章介绍如何利用 Grafana 中的 Performance Overview 面板进行基于数据库时间的性能分析和优化。\n\nPerformance Overview 面板按总分结构对 TiDB、TiKV、PD 的性能指标进行编排组织，包括以下三个部分：\n\n- 数据库时间和 SQL 执行时间概览：通过颜色标记不同 SQL 类型，SQL 不同执行阶段、不同请求的数据库时间，帮助你快速识别数据库负载特征和性能瓶颈。\n- 关键指标和资源利用率：包含数据库 QPS、应用和数据库的连接信息和请求命令类型、数据库内部 TSO 和 KV 请求 OPS、TiDB 和 TiKV 的资源使用概况。\n- 自上而下的延迟分解：包括 Query 延迟和连接空闲时间对比、Query 延迟分解、execute 阶段 TSO 请求和 KV 请求的延迟、TiKV 内部写延迟的分解等。\n\n### 数据库时间和 SQL 执行时间概览\n\nDatabase Time 指标为 TiDB 每秒处理 SQL 的延迟总和，即 TiDB 集群每秒并发处理应用 SQL 请求的总时间(等于活跃连接数)。\n\nPerformance Overview 面板提供了以下三个面积堆叠图，帮助你了解数据库负载的类型，快速定位数据库时间的瓶颈主要是处理什么语句，集中在哪个执行阶段，SQL 执行阶段主要等待 TiKV 或者 PD 哪种请求类型。\n\n- Database Time By SQL Type\n- Database Time By SQL Phase\n- SQL Execute Time Overview\n\n#### 颜色优化法\n\n通过观察数据库时间分解图和执行时间概览图，你可以直观地区分正常或者异常的时间消耗，快速定位集群的异常瓶颈点，高效了解集群的负载特征。对于正常的时间消耗和请求类型，图中显示颜色为绿色系或蓝色系。如果非绿色或蓝色系的颜色在这两张图中占据了明显的比例，意味着数据库时间的分布不合理。\n\n- Database Time By SQL Type：蓝色标识代表 Select 语句，绿色标识代表 Update、Insert、Commit 等 DML 语句。红色标识代表 General 类型，包含 StmtPrepare、StmtReset、StmtFetch、StmtClose 等命令。\n- Database Time By SQL Phase：execute 执行阶段为绿色，其他三个阶段偏红色系，如果非绿色的颜色占比明显，意味着在执行阶段之外数据库消耗了过多时间，需要进一步分析根源。一个常见的场景是因为无法使用执行计划缓存，导致 compile 阶段的橙色占比明显。\n- SQL Execute Time Overview：绿色系标识代表常规的写 KV 请求（例如 Prewrite 和 Commit），蓝色系标识代表常规的读 KV 请求（例如 Cop 和 Get），紫色系标识代表 TiFlash MPP 请求，其他色系标识需要注意的问题。例如，悲观锁加锁请求为红色，TSO 等待为深褐色。如果非蓝色系或者非绿色系占比明显，意味着执行阶段存在异常的瓶颈。例如，当发生严重锁冲突时，红色的悲观锁时间会占比明显；当负载中 TSO 等待的消耗时间过长时，深褐色会占比明显。\n\n**示例 1：TPC-C 负载**\n\n![TPC-C](/media/performance/tpcc_db_time.png)\n\n- Database Time by SQL Type：主要消耗时间的语句为 commit、update、select 和 insert 语句。\n- Database Time by SQL Phase：主要消耗时间的阶段为绿色的 execute 阶段。\n- SQL Execute Time Overview：执行阶段主要消耗时间的 KV 请求为绿色的 Prewrite 和 Commit。\n\n> **注意：**\n>\n> - KV 请求的总时间大于 execute time 为正常现象，因为 TiDB 执行器可能并发向多个 TiKV 发送 KV 请求，导致总的 KV 请求等待时间大于 execute time。TPC-C 负载中，事务提交时，TiDB 会向多个 TiKV 并行发送 Prewrite 和 Commit 请求，所以这个例子中 Prewrite、Commit 和 PessimisticsLock 请求的总时间明显大于 execute time。\n>\n> - execute time 也可能明显大于 KV 请求的总时间加上 tso_wait 的时间，这意味着 SQL 执行阶段主要时间花在 TiDB 执行器内部。两种常见的例子：\n>\n>     - 例 1：TiDB 执行器从 TiKV 读取大量数据之后，需要在 TiDB 内部进行复杂的关联和聚合，消耗大量时间。\n>     - 例 2：应用的写语句锁冲突严重，频繁锁重试导致 `Retried execution time` 过长。\n\n**示例 2：OLTP 读密集负载**\n\n![OLTP](/media/performance/oltp_normal_db_time.png)\n\n- Database Time by SQL Type：主要消耗时间的语句为 select、commit、update 和 insert 语句。其中，select 占据绝大部分的数据库时间。\n- Database Time by SQL Phase：主要消耗时间的阶段为绿色的 execute 阶段。\n- SQL Execute Time Overview：执行阶段主要消耗时间为深褐色的 pd tso_wait、蓝色的 KV Get 和绿色的 Prewrite 和 Commit。\n\n**示例 3：只读 OLTP 负载**\n\n![OLTP](/media/performance/oltp_long_compile_db_time.png)\n\n- Database Time by SQL Type：几乎所有语句为 select。\n- Database Time by SQL Phase：主要消耗时间的阶段为橙色的 compile 和绿色的 execute 阶段。compile 阶段延迟最高，代表着 TiDB 生成执行计划的过程耗时过长，需要根据后续的性能数据进一步确定根源。\n- SQL Execute Time Overview：执行阶段主要消耗时间的 KV 请求为蓝色 BatchGet。\n\n> **注意：**\n>\n> 示例 3 select 语句需要从多个 TiKV 并行读取几千行数据，BatchGet 请求的总时间远大于执行时间。\n\n**示例 4： 锁争用负载**\n\n![OLTP](/media/performance/oltp_lock_contention_db_time.png)\n\n- Database Time by SQL Type：主要为 Update 语句。\n- Database Time by SQL Phase：主要消耗时间的阶段为绿色的 execute 阶段。\n- SQL Execute Time Overview：执行阶段主要消耗时间的 KV 请求为红色的悲观锁 PessimisticLock，execute time 明显大于 KV 请求的总时间，这是因为应用的写语句锁冲突严重，频繁锁重试导致 `Retried execution time` 过长。目前 `Retried execution time` 消耗的时间，TiDB 尚未进行测量。\n\n**示例 5： HTAP CH-Benchmark 负载**\n\n![HTAP](/media/performance/htap_tiflash_mpp.png)\n\n- Database Time by SQL Type：主要为 Select 语句。\n- Database Time by SQL Phase：主要消耗时间的阶段为绿色的 execute 阶段。\n- SQL Execute Time Overview：执行阶段主要消耗时间为紫色的 `tiflash_mpp` 请求，其次是 KV 请求，包括蓝色的  `Cop`，以及绿色的 `Prewrite` 和 `Commit`。\n\n### TiDB 关键指标和集群资源利用率\n\n#### Query Per Second、Command Per Second 和 Prepared-Plan-Cache\n\n通过观察 Performance Overview 里的以下三个面板，可以了解应用的负载类型，与 TiDB 的交互方式，以及是否能有效地利用 TiDB 的[执行计划缓存](/sql-prepared-plan-cache.md)。\n\n- QPS：表示 Query Per Second，包含应用的 SQL 语句类型执行次数分布。\n- CPS By Type：CPS 表示 Command Per Second，Command 代表 MySQL 协议的命令类型。同样一个查询语句可以通过 query 或者 prepared statement 的命令类型发送到 TiDB。\n- Queries Using Plan Cache OPS：TiDB 集群每秒执行计划缓存的命中次数（即 `avg-hit`） 和未命中次数（即 `avg-miss`）。\n\n    StmtExecute 每秒执行次数等于 `avg-hit + avg-miss`。执行计划缓存只支持 prepared statement 命令。当 TiDB 开启执行计划缓存时，存在三种使用情况：\n\n    - 完全无法命中执行计划缓存：每秒命中次数 `avg-hit` 为 0，`avg-miss` 等于 StmtExecute 命令每秒执行次数。可能的原因包括：\n        - 应用使用了 query 命令。\n        - 每次 StmtExecute 执行之后，应用调用了 StmtClose 命令，导致缓存的执行计划被清理。\n        - StmtExecute 执行的所有语句都不符合[缓存的条件](/sql-prepared-plan-cache.md)，导致无法命中执行计划缓存。\n    - 完全命中执行计划缓存：每秒命中次数 `avg-hit` 等于 StmtExecute 命令每秒执行次数，每秒未命中次数 `avg-miss` 等于 0。\n    - 部分命中执行计划缓存：每秒命中次数 `avg-hit` 小于 StmtExecute 命令每秒执行次数。执行计划缓存目前存在一些限制，比如不支持子查询，该类型的 SQL 执行计划无法被缓存。\n\n**示例 1：TPC-C 负载**\n\nTPC-C 负载类型主要以 Update、Select 和 Insert 语句为主。总的 QPS 等于每秒 StmtExecute 的次数，并且 StmtExecute 每秒的数据基本等于 Queries Using Plan Cache OPS 面板的 `avg-hits`。这是 OLTP 负载理想的情况，客户端执行使用 prepared statement，并且在客户端缓存了 prepared statement 对象，执行每条 SQL 语句时直接调用 statement 执行。执行时都命中执行计划缓存，不需要重新 compile 生成执行计划。\n\n![TPC-C](/media/performance/tpcc_qps.png)\n\n**示例 2：只读 OLTP 负载，使用 query 命令无法使用执行计划缓存**\n\n这个负载中，Commit QPS = Rollback QPS = Select QPS。应用开启了 auto-commit 并发，每次从连接池获取连接都会执行 rollback，因此这三种语句的执行次数是相同的。\n\n![OLTP-Query](/media/performance/oltp_long_compile_qps.png)\n\n- QPS 面板中出现的红色加粗线为 Failed Query，坐标的值为右边的 Y 轴。非 0 代表此负载中存在错误语句。\n- 总的 QPS 等于 CPS By Type 面板中的 Query，说明应用中使用了 query 命令。\n- Queries Using Plan Cache OPS 面板没有数据，因为不使用 prepared statement 接口，无法使用 TiDB 的执行计划缓存，意味着应用的每一条 query，TiDB 都需要重新解析，重新生成执行计划。通常会导致 compile 时间变长以及 TiDB CPU 消耗的增加。\n\n**示例 3：OLTP 负载，使用 prepared statement 接口无法使用执行计划缓存**\n\nStmtPrepare 次数 = StmtExecute 次数 = StmtClose 次数 ~= StmtFetch 次数，应用使用了 prepare > execute > fetch > close 的 loop，很多框架都会在 execute 之后调用 close，确保资源不会泄露。这会带来两个问题：\n\n- 执行每条 SQL 语句需要 4 个命令，以及 4 次网络往返。\n- Queries Using Plan Cache OPS 为 0，无法命中执行计划缓存。StmtClose 命令默认会清理缓存的执行计划，导致下一次 StmtPrepare 命令需要重新生成执行计划。\n\n> **注意：**\n>\n> 从 TiDB v6.0.0 起，你可以通过全局变量 (`set global tidb_ignore_prepared_cache_close_stmt=on;`) 控制 StmtClose 命令不清理已被缓存的执行计划，使得下一次的 SQL 的执行不需要重新生成执行计划。\n\n![OLTP-Prepared](/media/performance/oltp_prepared_statement_no_plan_cache.png)\n\n**示例 4：Prepared Statement 存在资源泄漏**\n\nStmtPrepare 每秒执行次数远大于 StmtClose，说明应用程序存在 prepared statement 对象泄漏。\n\n![OLTP-Query](/media/performance/prepared_statement_leaking.png)\n\n- QPS 面板中出现的红色加粗线为 Failed Query，坐标的值为右边的 Y 轴。每秒错误语句为 74.6 条。\n- CPS By Type 面板中的 StmtPrepare 每秒执行次数远大于 StmtClose，说明应用程序存在 prepared statement 对象泄漏。\n- Queries Using Plan Cache OPS 面板中的 `avg-miss` 几乎等于 CPS By Type 面板中的 StmtExecute，说明几乎所有的 SQL 执行都未命中执行计划缓存。\n\n#### KV/TSO Request OPS 和 KV Request Time By Source\n\n- 在 KV/TSO Request OPS 面板中，你可以查看 KV 和 TSO 每秒请求的数据统计。其中，`kv request total` 代表 TiDB 到 TiKV 所有请求的总和。通过观察 TiDB 到 PD 和 TiKV 的请求类型，可以了解集群内部的负载特征。\n- 在 KV Request Time By Source 面板中，你可以查看每种 KV 请求和请求来源的时间占比。\n    - kv request total time 是每秒总的 KV 和 TiFlash 请求处理时间\n    - 每种 KV 请求和请求来源组成柱状堆叠图，`external` 标识正常业务的请求，`internal` 标识内部活动的请求（比如 DDL、auto analyze 等请求）。\n\n**示例 1：繁忙的负载**\n\n![TPC-C](/media/performance/tpcc_source_sql.png)\n\n在此 TPC-C 负载中：\n\n- 每秒总的 KV 请求的数量为 79.7 K。按请求数量排序，最高的请求类型为 `Prewrite`、`Commit`、`PessimisticsLock` 和 `BatchGet` 等。\n- KV 处理时间来源主要为 `Commit-external_Commit`、`Prewrite-external_Commit`，说明消耗时间最高的 KV 请求为 `Commit` 和 `Prewrite`，并且来源于外部的 Commit 语句。\n\n**示例 2：Analyze 负载**\n\n![OLTP](/media/performance/internal_stats.png)\n\n集群中只有 analyze 语句运行：\n\n- 每秒总的 KV 请求数据是 35.5，Cop 请求次数是每秒 9.3。\n- KV 处理时间主要来源为 `Cop-internal_stats`，说明 Cop 请求来源于内部的 analyze 操作。\n\n#### TiDB CPU，以及 TiKV CPU 和 IO 使用情况\n\n在 TiDB CPU 和 TiKV CPU/IO MBps 这两个面板中，你可以观察到 TiDB 和 TiKV 的逻辑 CPU 使用率和 IO 吞吐，包含平均、最大和 delta（最大 CPU 使用率减去最小 CPU 使用率），从而用来判定 TiDB 和 TiKV 总体的 CPU 使用率。\n\n- 通过 `delta` 值，你可以判断 TiDB 是否存在 CPU 使用负载不均衡（通常伴随着应用连接不均衡），TiKV 是否存在热点。\n- 通过 TiDB 和 TiKV 的资源使用概览，你可以快速判断集群是否存在资源瓶颈，最需要扩容的组件是 TiDB 还是 TiKV。\n\n**示例 1：TiDB 资源使用率高**\n\n下图负载中，每个 TiDB 和 TiKV 配置 8 CPU。\n\n![TPC-C](/media/performance/tidb_high_cpu.png)\n\n- TiDB 平均 CPU 为 575%。最大 CPU 为 643%，delta CPU 为 136%。\n- TiKV 平均 CPU 为 146%，最大 CPU 215%。delta CPU 为 118%。TiKV 的平均 IO 吞吐为 9.06 MB/s，最大 IO 吞吐为 19.7 MB/s，delta IO 吞吐为 17.1 MB/s。\n\n由此可以判断，TiDB 的 CPU 消耗明显更高，并接近于 8 CPU 的瓶颈，可以考虑扩容 TiDB。\n\n**示例 2：TiKV 资源使用率高**\n\n下图 TPC-C 负载中，每个 TiDB 和 TiKV 配置 16 CPU。\n\n![TPC-C](/media/performance/tpcc_cpu_io.png)\n\n- TiDB 平均 CPU 为 883%。最大 CPU 为 962%，delta CPU 为 153%。\n- TiKV 平均 CPU 为 1288%，最大 CPU 1360%。delta CPU 为 126%。TiKV 的平均 IO 吞吐为 130 MB/s，最大 IO 吞吐为 153 MB/s，delta IO 吞吐为 53.7 MB/s。\n\n由此可以判断，TiKV 的 CPU 消耗更高，因为 TPC-C 是一个写密集场景，这是正常现象，可以考虑扩容 TiKV 节点提升性能。\n\n### Query 延迟分解和关键的延迟指标\n\n延迟面板通常包含平均值和 99 分位数，平均值用来定位总体的瓶颈，99 分位数用来判断是否存在延迟严重抖动的情况。判断性能抖动范围时，可能还需要需要借助 999 分位数。\n\n#### Duration、Connection Idle Duration 和 Connection Count\n\nDuration 面板包含了所有语句的 99 延迟和每种 SQL 类型的平均延迟。Connection Idle Duration 面板包含连接空闲的平均和 99 延迟，连接空闲时包含两种状态：\n\n- in-txn：代表事务中连接的空闲时间，即当连接处于事务中时，处理完上一条 SQL 之后，收到下一条 SQL 语句的间隔时间。\n- not-in-txn：当连接没有处于事务中，处理完上一条 SQL 之后，收到下一条 SQL 语句的间隔时间。\n\n应用进行数据库事务时，通常使用同一个数据库连接。对比 query 的平均延迟和 connection idle duration 的延迟，可以判断整个系统性能瓶颈或者用户响应时间的抖动是否是由 TiDB 导致的。\n\n- 如果应用负载不是只读的，包含事务，对比 query 平均延迟和 `avg-in-txn` 可以判断应用处理事务时，主要的时间是花在数据库内部还是在数据库外面，借此定位用户响应时间的瓶颈。\n- 如果是只读负载，不存在 `avg-in-txn` 指标，可以对比 query 平均延迟和 `avg-not-in-txn` 指标。\n\n现实的客户负载中，瓶颈在数据库外面的情况并不少见，例如：\n\n- 客户端服务器配置过低，CPU 资源不够。\n- 使用 HAProxy 作为 TiDB 集群代理，但是 HAProxy CPU 资源不够。\n- 使用 HAProxy 作为 TiDB 集群代理，但是高负载下 HAProxy 服务器的网络带宽被打满。\n- 应用服务器到数据库延迟过高，比如公有云环境应用和 TiDB 集群不在同一个地区，比如数据库的 DNS 均衡器和 TiDB 集群不在同一个地区。\n- 客户端程序存在瓶颈，无法充分利用服务器的多 CPU 核或者多 Numa 资源，比如应用只使用一个 JVM 向 TiDB 建立上千个 JDBC 连接。\n\n在 Connection Count （连接信息）面板中，你可以查看总的连接数和每个 TiDB 节点的连接数，并由此判断连接总数是否正常，各 TiDB 节点的连接数是否不均衡。`active connections` 记录着活跃连接数，等于每秒的数据库时间，右侧 Y 轴为 `disconnection/s`，代表集群每秒断开连接的数量，用来判断应用是否使用了短连接。\n\n**示例 1：disconnection/s 过高**\n\n![high disconnection/s](/media/performance/high_disconnections.png)\n\n在此负载中：\n\n- 所有 SQL 语句的平均延迟 10.8 ms，P99 延迟 84.1 ms。\n- 事务中连接空闲时间 `avg-in-txn` 为 9.4 ms。\n- 集群总的连接数为 3.7K，每个 TiDB 节点的连接数为 1.8 K。平均活跃连接数为 40.3，大部分连接处于空闲状态。`disconnection/s` 平均为 55.8，说明应用在频繁的新建和断开连接。短连接的行为会对 TiDB 的资源和响应时间造成一定的影响。\n\n**示例 2：用户响应时间的瓶颈在 TiDB 中**\n\n![TiDB is the Bottleneck](/media/performance/tpcc_duration_idle.png)\n\n在此 TPC-C 负载中：\n\n- 所有 SQL 语句的平均延迟 477 us，99 延迟 3.13 ms。平均 commit 语句 2.02 ms，平均 insert 语句 609 us，平均查询语句 468 us。\n- 事务中连接空闲时间 `avg-in-txn` 171 us。\n\n由此可以判断，平均的 query 延迟明显大于 `avg-in-txn`，说明事务处理中，主要的瓶颈在数据库内部。\n\n**示例 3：用户响应时间的瓶颈不在 TiDB 中**\n\n![TiDB is not the Bottleneck](/media/performance/cloud_query_long_idle.png)\n\n在此负载中，平均 query 延迟为 1.69 ms，事务中连接空闲时间 `avg-in-txn` 为 18 ms。说明事务中，TiDB 平均花了 1.69 ms 处理完一个 SQL 语句之后，需要等待 18 ms 才能收到下一条语句。\n\n由此可以判断，用户响应时间的瓶颈不在 TiDB 中。这个例子是在一个公有云环境下，因为应用和数据库不在同一个地区，应用和数据库之间的网络延迟高导致了超高的连接空闲时间。\n\n#### Parse、Compile 和 Execute Duration\n\n在 TiDB 中，从输入查询文本到返回结果的[典型处理流程](/sql-optimization-concepts.md)。\n\nSQL 在 TiDB 内部的处理分为四个阶段，get token、parse、compile 和 execute：\n\n- get token 阶段：通常只有几微秒的时间，可以忽略。除非 TiDB 单个实例的连接数达到的 [token-limit](/tidb-configuration-file.md) 的限制，创建连接的时候被限流。\n- parse 阶段：query 语句解析为抽象语法树 abstract syntax tree (AST)。\n- compile 阶段：根据 parse 阶段输出的 AST 和统计信息，编译出执行计划。整个过程主要步骤为逻辑优化与物理优化，前者通过一些规则对查询计划进行优化，例如基于关系代数的列裁剪等，后者通过统计信息和基于成本的优化器，对执行计划的成本进行估算，并选择整体成本最小的物理执行计划。\n- execute 阶段：时间消耗视情况，先等待全局唯一的时间戳 TSO，之后执行器根据执行计划中算子涉及的 Key 范围，构建出 TiKV 的 API 请求，分发到 TiKV。execute 时间包含 TSO 等待时间、KV 请求的时间和 TiDB 执行器本身处理数据的时间。\n\n如果应用统一使用 query 或者 StmtExecute MySQL 命令接口，可以使用以下公式来定位平均延迟的瓶颈。\n\n```\navg Query Duration = avg Get Token + avg Parse Duration + avg Compile Duration + avg Execute Duration\n```\n\n通常 execute 阶段会占 query 延迟的主要部分，在以下情况下，parse 和 compile 阶段也会占比明显。\n\n- parse 阶段延迟占比明显：比如 query 语句很长，文本解析消耗大量的 CPU。\n- compile 阶段延迟占比明显：如果应用没有使用执行计划缓存，每个语句都需要生成执行计划。compile 阶段的延迟可能达到几毫秒或者几十毫秒。如果无法命中执行计划缓存，compile 阶段需要进行逻辑优化和物理优化，这将消耗大量的 CPU 和内存，并给 Go Runtime 带来压力（因为 TiDB 是 [`Go`](https://go.dev/) 编写的），进一步影响 TiDB 其他组件的性能。这说明，OLTP 负载在 TiDB 中是否能高效运行，执行计划缓存扮演了重要的角色。\n\n**示例 1：数据库瓶颈在 compile 阶段**\n\n![Compile](/media/performance/long_compile.png)\n\n此图中 parse、compile 和 execute 阶段的平均时间分别为 17.1 us、729 us 和 681 us。因为应用使用 query 命令接口，无法使用执行计划缓存，所以 compile 阶段延迟高。\n\n**示例 2：数据库瓶颈在 execute 阶段**\n\n![Execute](/media/performance/long_execute.png)\n\n在此 TPC-C 负载中，parse、compile 和 execute 阶段的平均时间分别为 7.39us、38.1us 和 12.8ms。query 延迟的瓶颈在于 execute 阶段。\n\n#### KV 和 TSO Request Duration\n\n在 execute 阶段，TiDB 会跟 PD 和 TiKV 进行交互。如下图所示，当 TiDB 处理 SQL 语句请求时，在进行 parse 和 compile 之前，如果需要获取 TSO，会先请求生成 TSO。PD Client 不会阻塞调用者，而是直接返回一个 `TSFuture`，并在后台异步处理 TSO 请求的收发，一旦完成立即返回给 TSFuture，TSFuture 的持有者则需要调用 Wait 方法来获得最终的 TSO 结果。当 TiDB 完成 parse 和 compile 之后，进入 execute 阶段，此时存在两个情况：\n\n- 如果 TSO 请求已经完成，Wait 方法会立刻返回一个可用的 TSO 或 error\n- 如果 TSO 请求还未完成，Wait 方法会 block 住等待一个可用的 TSO 或 error（说明 gRPC 请求已发送但尚未收到返回结果，网络延迟较高）\n\nTSO 等待的时间记录为 TSO WAIT，TSO 请求的网络时间记录为 TSO RPC。TiDB TSO 等待完成之后，执行过程中通常需要和 TiKV 进行读写交互：\n\n- 读的 KV 请求常见类型：Get、BatchGet 和 Cop\n- 写的 KV 请求常见类型：PessimisticLock，二阶段提交的 Prewrite 和 Commit\n\n![Execute](/media/performance/execute_phase.png)\n\n这一部分的指标对应以下三个面板：\n\n- Avg TiDB KV Request Duration：TiDB 测量的 KV 请求的平均延迟\n- Avg TiKV GRPC Duration：TiKV 内部 GRPC 消息处理的平均延迟\n- PD TSO Wait/RPC Duration：TiDB 执行器等待 TSO 延迟 (wait) 和 TSO 请求的网络延迟(rpc)。\n\n其中，Avg TiDB KV Request Duration 和 Avg TiKV GRPC Duration 的关系如下\n\n```\nAvg TiDB KV Request Duration = Avg TiKV GRPC Duration + TiDB 与 TiKV 之间的网络延迟 + TiKV GRPC 处理时间 + TiDB GRPC 处理时间和调度延迟。\n```\n\nAvg TiDB KV Request Duration 和 Avg TiKV GRPC Duration 的差值跟网络流量和延迟，TiDB 和 TiKV 的资源使用情况密切相关。\n\n- 同一个机房内，Avg TiDB KV Request Duration 和 Avg TiKV GRPC Duration 的差值通常应该小于 2 毫秒。\n- 同一地区的不同可用区，Avg TiDB KV Request Duration 和 Avg TiKV GRPC Duration 的差值通常应该小于 5 毫秒。\n\n**示例 1：同机器低负载的集群**\n\n![Same Data Center](/media/performance/oltp_kv_tso.png)\n\n在此负载中，TiDB 侧平均 Prewrite 请求延迟为 925 us，TiKV 内部 kv_prewrite 平均处理延迟为 720 us，相差 200 us 左右，是同机房内正常的延迟。TSO wait 平均延迟 206 us，rpc 时间为 144 us。\n\n**示例 2：公有云集群，负载正常**\n\n![Cloud Env ](/media/performance/cloud_kv_tso.png)\n\n在此示例中，TiDB 集群部署在同一个地区的不同机房。TiDB 侧平均 Commit 请求延迟为 12.7 ms，TiKV 内部 kv_commit 平均处理延迟为 10.2 ms，相差 2.5 ms 左右。TSO wait 平均延迟为 3.12 ms，rpc 时间为 693 us。\n\n**示例 3：公有云集群，资源严重过载**\n\n![Cloud Env, TiDB Overloaded](/media/performance/cloud_kv_tso_overloaded.png)\n\n在此示例中，TiDB 集群部署在同一个地区的不同机房，TiDB 网络和 CPU 资源严重过载。TiDB 侧平均 BatchGet 请求延迟为 38.6 ms，TiKV 内部 kv_batch_get 平均处理延迟为 6.15 ms，相差超过 32 ms，远高于正常值。TSO wait 平均延迟为 9.45 ms，rpc 时间为 14.3 ms。\n\n#### Storage Async Write Duration、Store Duration 和 Apply Duration\n\nTiKV 对于写请求的处理流程如下：\n\n- `scheduler worker` 会先处理写请求，进行事务一致性检查，并把写请求转化成键值对，发送到 `raftstore` 模块。\n- `raftstore` 为 TiKV 的共识模块，使用 Raft 共识算法，使多个 TiKV 组成的存储层可以容错。\n\n    Raftstore 分为 Store 线程和 Apply 线程：\n\n    - Store 线程负责处理 Raft 消息和新的 `proposals`。当收到新的 `proposals` 时，leader 节点的 store 线程会写入本地 Raft DB，并将消息复制到多个 follower 节点。当这个 `proposals` 在多数实例持久化成功之后，`proposals` 成功被提交。\n    - Apply 线程负责将提交的数据写入到 KV DB 中。当写操作的数据被成功地写入 KV 数据库中时，Apply 线程会通知外层请求写请求已经完成。\n\n![TiKV Write](/media/performance/store_apply.png)\n\nStorage Async Write Duration 指标记录写请求进入 raftstore 之后的延迟，采集的粒度具体到每个请求的级别。\n\nStorage Async Write Duration 分为 Store Duration 和 Apply Duration。你可以通过以下公式定位写请求的瓶颈主要是在 Store 还是 Apply 步骤。\n\n```\navg Storage Async Write Duration  = avg Store Duration + avg Apply Duration\n```\n\n> **注意：**\n>\n> Store Duration 和 Apply Duration 从 v5.3.0 版本开始支持。\n\n**示例 1：同一个 OLTP 负载在 v5.3.0 和 v5.4.0 版本的对比**\n\n应用以上公式：v5.4.0 版本中，一个写密集的 OLTP 负载 QPS 比 v5.3.0 提升了 14%。\n\n- v5.3.0：24.4 ms ~= 17.7 ms + 6.59 ms\n- v5.4.0：21.4 ms ~= 14.0 ms + 7.33 ms\n\n因为 v5.4.0 版本中，TiKV 对 gRPC 模块进行了优化，优化了 Raft 日志复制速度，相比 v5.3.0 降低了 Store Duration。\n\nv5.3.0：\n\n![v5.3.0](/media/performance/v5.3.0_store_apply.png)\n\nv5.4.0：\n\n![v5.4.0](/media/performance/v5.4.0_store_apply.png)\n\n**示例 2：Store Duration 瓶颈明显**\n\n应用以上公式：10.1 ms ~= 9.81 ms + 0.304 ms，说明写请求的延迟瓶颈在 Store Duration。\n\n![Store](/media/performance/cloud_store_apply.png)\n\n#### Commit Log Duration、Append Log Duration 和 Apply Log Duration\n\nCommit Log Duration、Append Log Duration 和 Apply Log Duration 这三个延迟是 raftstore 内部关键操作的延迟记录。这些记录采集的粒度是 batch 操作级别的，每个操作会把多个写请求合并在一起，因此不能直接对应上文的 Store Duration 和 Apply Duration。\n\n- Commit Log Duration 和 Append Log Duration 均为 store 线程的操作。Commit Log Duration 包含复制 Raft 日志到其他 TiKV 节点，保证 raft-log 的持久化。一般包含两次 Append Log Duration，一次 leader，一次 follower 的。Commit Log Duration 延迟通常会明显高于 Append Log Duration，因为包含了通过网络复制 Raft 日志到其他 TiKV 的时间。\n- Apply Log Duration 记录了 apply 线程 apply Raft 日志的延迟。\n\nCommit Log Duration 慢的常见场景：\n\n- TiKV CPU 资源存在瓶颈，调度延迟高\n- `raftstore.store-pool-size` 设置过小或者过大（过大也可能导致性能下降）\n- IO 延迟高，导致 Append Log Duration 延迟高\n- TiKV 之间的网络延迟比较高\n- TiKV 的 gRPC 线程数设置过小或者多个 gRPC CPU 资源使用不均衡\n\nApply Log Duration 慢的常见场景：\n\n- TiKV CPU 资源存在瓶颈，调度延迟高\n- `raftstore.apply-pool-size` 设置过小或者过大（过大也可能导致性能下降）\n- IO 延迟比较高\n\n**示例 1：同一个 OLTP 负载在 v5.3.0 和 v5.4.0 版本的对比**\n\nv5.4.0 版本，一个写密集的 OLTP 负载 QPS 比 v5.3.0 提升了 14%。对比这三个关键延迟：\n\n| Avg Duration   | v5.3.0(ms)   |    v5.4.0(ms)  |\n|:----------|:----------|:----------|\n| Append Log Duration  | 0.27 | 0.303|\n| Commit Log Duration  | 13   | 8.68 |\n| Apply Log Duration   | 0.457|0.514  |\n\n因为 v5.4.0 版本中，TiKV 对 gRPC 模块进行了优化，优化了 Raft 日志复制速度，相比 v5.3.0 降低了 Commit Log Duration 和 Store Duration。\n\nv5.3.0：\n\n![v5.3.0](/media/performance/v5.3.0_commit_append_apply.png)\n\nv5.4.0：\n\n![v5.4.0](/media/performance/v5.4.0_commit_append_apply.png)\n\n**示例 2：Commit Log Duration 瓶颈明显的例子**\n\n![Store](/media/performance/cloud_append_commit_apply.png)\n\n- 平均 Append Log Duration = 4.38 ms\n- 平均 Commit Log Duration = 7.92 ms\n- 平均 Apply Log Duration = 172 us\n\nStore 线程的 Commit Log Duration 明显比 Apply Log Duration 高，并且 Append Log Duration 比 Apply Log Duration 明显的高，说明 Store 线程在 CPU 和 IO 都可能都存在瓶颈。可能降低 Commit Log Duration 和 Append Log Duration 的方式如下：\n\n- 如果 TiKV CPU 资源充足，考虑增加 Store 线程，即 `raftstore.store-pool-size`。\n- 如果 TiDB 为 v5.4.0 及之后的版本，考虑启用 [`Raft Engine`](/tikv-configuration-file.md#raft-engine)，Raft Engine 具有更轻量的执行路径，在一些场景下显著减少 IO 写入量和写入请求的长尾延迟，启用方式为设置 `raft-engine.enable: true`。\n- 如果 TiKV CPU 资源充足，且 TiDB 为 v5.3.0 及之后的版本，考虑启用 [`StoreWriter`](/tune-tikv-thread-performance.md#tikv-线程池调优)。启用方式：`raftstore.store-io-pool-size: 1`。\n\n## 低于 v6.1.0 的 TiDB 版本如何使用 Performance overview 面板\n\n从 v6.1.0 起，TiDB Grafana 组件默认内置了 Performance Overview 面板。Performance overview 面板兼容 TiDB v4.x 和 v5.x 版本。如果你的 TiDB 版本低于 v6.1.0，需要手动导入 [`performance_overview.json`](https://github.com/pingcap/tidb/blob/master/pkg/metrics/grafana/performance_overview.json)。\n\n导入方法如图所示：\n\n![Store](/media/performance/import_dashboard.png)\n"
        },
        {
          "name": "performance-tuning-overview.md",
          "type": "blob",
          "size": 8.3232421875,
          "content": "---\ntitle:  性能优化概述\nsummary: 本文介绍性能优化的基本概念，比如用户响应时间、吞吐和数据库时间，以及性能优化的通用流程。\naliases: ['/zh/tidb/v6.0/performance-tuning-overview']\n---\n\n# TiDB 性能优化概述\n\n本文介绍性能优化的基本概念，比如用户响应时间、吞吐和数据库时间，以及性能优化的通用流程。\n\n## 用户响应时间和数据库时间\n\n### 用户响应时间\n\n用户响应时间是指应用系统为用户返回请求结果所消耗的时间。一个典型的用户请求的处理时序图如下，包含了用户和应用系统的网络延迟、应用的处理时间、应用和数据库的交互时的网络延迟和数据库的服务时间等。用户响应时间受到请求链路上各个子系统的影响，比如网络延迟和带宽、系统并发用户数和请求类型、服务器 CPU 和 IO 资源使用率等。要对整个系统进行有效的优化，你需要先定位用户响应时间的瓶颈。\n\n你可以通过以下公式计算指定时间范围 (`ΔT`) 内总的用户响应时间：\n\n`ΔT` 时间内总的用户响应时间 = 平均 TPS (Transactions Per Second) x 用户平均响应时间 x `ΔT`。\n\n![用户响应时间](/media/performance/user_response_time_cn.png)\n\n### 数据库时间\n\n数据库时间是指数据库系统提供服务的时间，`ΔT` 时间内的数据库时间为数据库并发处理所有应用请求的时间总和。\n\n你可以通过以下任一方式计算数据库时间：\n\n- 方式一： 通过 QPS 乘以平均 query 延迟乘以 ΔT，即 `DB Time in ΔT = QPS × avg latency × ΔT`\n- 方式二： 通过平均活跃会话数乘以 ΔT，即 `DB Time in ΔT  = avg active connections × ΔT`\n- 方式三： 通过 TiDB 内部的 Prometheus 指标 tidb_server_tokens 计算，即 `ΔT DB Time = rate(tidb_server_tokens) × ΔT`\n\n## 用户响应时间和系统吞吐的关系\n\n用户响应时间包含完成用户请求的服务时间、排队时间和并发等待时间，即：\n\n```\nUser Response time = Service time + Queuing delay + Coherency delay\n```\n\n- Service Time（完成用户请求的服务时间）：系统处理请求时需要消耗某种资源的时间，比如数据库完成一次 SQL 请求需要消耗的 CPU 时间。\n- Queuing delay（排队延迟时间）：系统处理请求时为了等待某种资源的服务，在队列中等待调度的时间。\n- Coherency delay（并发等待延迟）：系统处理请求时为了访问共享资源，需要和其他并发的任务进行通信和协作的时间。\n\n系统吞吐指系统每秒完成的请求数量。用户响应时间和吞吐通常是反比倒数的关系。随着吞吐的上升，系统资源利用率上升，请求服务的排队延迟会随之上升，当资源利用率超过某个拐点，排队延迟会急剧上升。\n\n例如，对于运行 OLTP 负载的数据库系统，当 CPU 利用率超过 65% 之后，CPU 的排队调度延迟会明显上升。因为系统的并发请求不是完全独立的，请求之间存在共享资源的协同和争用，比如不同的数据库请求可能对同样的数据有互斥的加锁操作。当资源利用率上升时，排队和调度延迟上升，这将导致持有的共享资源无法及时释放，反过来延长了其他任务对共享资源的等待时间。\n\n## 性能优化流程\n\n性能优化流程包含以下 6 个步骤：\n\n1. 定义优化目标\n2. 建立性能基线\n3. 定位用户响应时间的瓶颈\n4. 提出优化方案，预估每种方案的收益、风险和成本\n5. 实施优化\n6. 评估优化结果\n\n一个性能优化项目，经常需要对步骤 2 到 6 进行多次循环，才能达到优化的目标。\n\n### 第 1 步：定义优化目标\n\n不同类型系统优化目标不同。例如，对于一个金融核心的 OLTP 系统，优化目标可能是降低交易的长尾延迟；对于一个财务结算系统，优化目标可能是更充分利用硬件资源，缩短批量结算任务时间。\n\n一个好的优化目标应该是容易量化的，比如：\n\n- 好的优化目标：”业务高峰期上午 9 点到 10 点，转账交易的 p99 延迟需要小于 200 毫秒“\n- 差的优化目标：”系统太慢了没有响应，需要优化“\n\n定义一个清晰的优化目标有助于指导后续的性能优化工作。\n\n### 第 2 步：建立性能基线\n\n为了高效地进行性能优化，你需要采集当前的性能数据以建立性能基线。需要采集的性能数据通常包含以下内容：\n\n- 用户响应时间的平均值和长尾值、应用系统的吞吐\n- 数据库时间、Query 延迟和 QPS 等数据库性能数据。\n\n    TiDB 针对不同维度的性能数据进行了完善的测量和存储，例如[慢日志](/identify-slow-queries.md)、[Top SQL](/dashboard/top-sql.md)、[持续性能分析功能](/dashboard/continuous-profiling.md)和[流量可视化](/dashboard/dashboard-key-visualizer.md)等。此外，你还可以对存储在 Prometheus 中的时序指标数据进行历史回溯和对比。\n\n- 资源使用率，包含 CPU、IO 和网络等资源\n- 配置信息，比如应用系统、数据库和操作系统的配置\n\n### 第 3 步：定位用户响应时间的瓶颈\n\n基于性能基线的数据，定位或者推测用户响应时间的瓶颈。\n\n现实中的应用程序往往没有对用户请求的链路进行完整的测量和记录，因此你无法通过应用程序对用户响应时间进行自上而下有效的分解。\n\n与之相反的是，数据库内部对于 query 延迟和吞吐等性能指标记录非常完善。基于数据库时间，你可以判断用户响应时间的瓶颈是否在数据库中。\n\n- 如果瓶颈不在数据库中，需要借助数据库外部搜集的资源利用率，或者对应用程序进行 Profile，以确定数据库外部的瓶颈。常见场景包括应用程序或者代理服务器资源不足，应用程序存在串行点无法充分利用硬件资源等。\n- 如果瓶颈存在数据库中，你可以通过数据库完善的调优工具进行数据库内部性能分析和诊断。常见场景包括存在慢 SQL、应用程序使用数据库的方式不合理、数据库存在读写热点等。\n\n具体的分析诊断方法和工具，请参考[性能优化方法](/performance-tuning-methods.md)。\n\n### 第 4 步：提出优化方案，评估每种方案的收益、风险和成本\n\n通过性能分析确定系统瓶颈点之后，根据实际情况提出低成本、低风险、并能获得最大的收益的优化方案。\n\n根据[阿姆达尔定律](https://zh.wikipedia.org/wiki/%E9%98%BF%E5%A7%86%E8%BE%BE%E5%B0%94%E5%AE%9A%E5%BE%8B)，性能优化的最大收益，取决于优化的部分在整个系统的占比。因此，你需要根据性能数据，确认系统瓶颈和相应的占比，预估瓶颈解决或者优化之后的收益。\n\n需要注意的是，即使某个方案针对最大瓶颈点的优化潜在收益最大，也需要同时评估该方案的风险和成本。例如：\n\n- 对于资源过载的系统，最直接的优化方案是扩容，但是实际中可能因为扩容方案成本太高而无法被采纳。\n- 当某个业务模块里的一个慢 SQL 导致整个模块的响应时间很慢时，升级到数据库新版本的方案可以解决这个慢 SQL 问题，但是同时可能影响原来没有问题的模块，因此该方案可能存在潜在的高风险。一个低风险的方案是不升级数据库版本，直接改写现有慢 SQL，在当前数据库版本中解决该问题。\n\n### 第 5 步：实施优化\n\n综合考量收益、风险和成本，选定一种或者多种优化方案进行实施，并对生产系统的变更进行周全的准备和详细的记录。\n\n为了降低风险和验证优化方案的收益，建议在测试环境和准生产环境对变更的内容进行验证和完整的回归。例如，针对一个查询业务的慢 SQL，如果选定的优化方案是新建索引优化查询的访问路径，你需要确保新的索引不会在现有的数据插入业务中引入明显的写入热点，导致其他业务变慢。\n\n### 第 6 步：评估优化结果\n\n实施优化之后，需要评估优化结果。\n\n- 如果达到优化目标，整个优化项目顺利完成。\n- 如果未达到优化目标，你需要重复步骤 2 到 6，直到达到优化目标。\n\n达到优化目标之后，为了应对业务的增长，你可能还需要进一步做好系统的容量规划。"
        },
        {
          "name": "performance-tuning-practices.md",
          "type": "blob",
          "size": 23.23046875,
          "content": "---\ntitle: OLTP 负载性能优化实践\nsummary: 本文档介绍了如何对 OLTP 负载进行性能分析和优化。\naliases: ['/zh/tidb/v6.0/performance-tuning-practices']\n---\n\n# OLTP 负载性能优化实践\n\nTiDB 提供了完善的性能诊断和分析功能，例如 TiDB Dashboard 的 [Top SQL](/dashboard/top-sql.md) 和 [Continuous Profiling](/dashboard/continuous-profiling.md) 功能，以及 TiDB [Performance Overview 面板](/grafana-performance-overview-dashboard.md)。\n\n本文介绍如何综合利用这些功能，对同一个 OLTP 负载在七种不同运行场景下的性能表现进行分析和对比，并演示了具体的 OLTP 负载的优化过程，帮助你更快地对 TiDB 的性能进行分析和优化。\n\n> **注意：**\n>\n> [Top SQL](/dashboard/top-sql.md) 和 [Continuous Profiling](/dashboard/continuous-profiling.md) 功能默认关闭，需要提前开启。\n\n在这些场景中，通过使用不同的 JDBC 配置运行同一个应用程序，你可以观察应用和数据库之间不同的交互方式将如何影响系统整体的性能，从而更好地掌握[开发 Java 应用使用 TiDB 的最佳实践](/best-practices/java-app-best-practices.md)。\n\n## 负载环境\n\n本文使用一个银行交易系统 OLTP 仿真模拟负载进行演示。以下为该负载的仿真环境配置：\n\n- 负载应用程序的开发语言：JAVA\n- 涉及业务的 SQL 语句：共 200 条，其中 90% 都是 SELECT 语句，属于典型的读密集 OLTP 场景。\n- 涉及交易的表：共 60 张，存在修改操作类的表为 12 张，其余 48 张表只读。\n- 应用程序使用的隔离级别：`read committed`。\n- TiDB 集群配置：3 个 TiDB 节点和 3 个 TiKV 节点，各节点分配 16 CPU。\n- 客户端服务器配置：36 CPU。\n\n## 场景 1：使用 Query 接口\n\n### 应用配置\n\n应用程序使用以下 JDBC 配置，通过 Query 接口连接数据库。\n\n```\nuseServerPrepStmts=false\n```\n\n### 性能分析\n\n#### TiDB Dashboard\n\n从以下 Dashboard 的 Top SQL 页面可以观察到，非业务 SQL 类型 `SELECT @@session.tx_isolation` 消耗的资源最多。虽然 TiDB 处理这类 SQL 语句的速度快，但由于执行次数最多导致总体 CPU 耗时最多。\n\n![dashboard-for-query-interface](/media/performance/case1.png)\n\n观察以下 TiDB 的火焰图，可以发现，在 SQL 的执行过程中，Compile 和 Optimize 等函数的 CPU 消耗占比明显。因为应用使用了 Query 接口，TiDB 无法使用执行计划缓存，导致每个 SQL 都需要编译生成执行计划。\n\n![flame-graph-for-query-interface](/media/performance/7.1.png)\n\n- ExecuteStmt cpu = 38% cpu time = 23.84s\n- Compile cpu = 27%  cpu time = 17.17s\n- Optimize cpu = 26% cpu time = 16.41s\n\n#### Performance Overview 面板\n\n观察以下 Performance Overview 面板中数据库时间概览和 QPS 的数据：\n\n![performance-overview-1-for-query-interface](/media/performance/j-1.png)\n\n- Database Time by SQL Type 中 Select 语句耗时最多\n- Database Time by SQL Phase 中 execute 和 compile 占比最多\n- SQL Execute Time Overview 中占比最多的分别是 Get、Cop 和 tso wait\n- CPS By Type 只有 query 这一种 command\n- Queries Using Plan Cache OPS 没有数据，说明无法命中执行计划缓存\n- execute 和 compile 的延迟在 query duration 中占比最高\n- avg QPS = 56.8k\n\n观察集群的资源消耗，TiDB CPU 的平均利用率为 925%，TiKV CPU 的平均利用率为 201%，TiKV IO 平均吞吐为 18.7 MB/s。TiDB 的资源消耗明显更高。\n\n![performance-overview-2-for-query-interface](/media/performance/5.png)\n\n### 分析结论\n\n需要屏蔽这些大量无用的非业务 SQL 语句。\n\n## 场景 2：使用 maxPerformance 配置\n\n### 应用配置\n\n在场景 1 中的 JDBC 连接串的基础上，新增一个参数 `useConfigs=maxPerformance`。这个参数可以用来屏蔽 JDBC 向数据库发送的一些查询设置类的 SQL 语句（例如 `select @@session.transaction_read_only`），完整配置如下：\n\n```\nuseServerPrepStmts=false&useConfigs=maxPerformance\n```\n\n### 性能分析\n\n#### TiDB Dashboard\n\n在 Dashboard 的 Top SQL 页面，可以看到原本占比最多的 `SELECT @@session.tx_isolation` 已消失。\n\n![dashboard-for-maxPerformance](/media/performance/case2.png)\n\n观察以下 TiDB 的火焰图，可以发现 SQL 语句执行中 Compile 和 Optimize 等函数 CPU 消耗占比高：\n\n![flame-graph-for-maxPerformance](/media/performance/20220507-145257.jpg)\n\n- ExecuteStmt cpu = 43% cpu time =35.84s\n- Compile cpu = 31% cpu time =25.61s\n- Optimize cpu = 30% cpu time = 24.74s\n\n#### Performance Overview 面板\n\n数据库时间概览和 QPS 的数据如下：\n\n![performance-overview-1-for-maxPerformance](/media/performance/j-2.png)\n\n- Database Time by SQL Type 中 select 语句耗时最多\n- Database Time by SQL Phase 中 execute 和 compile 占比最多\n- SQL Execute Time Overview 中占比最多的分别是 Get、Cop、Prewrite 和 tso_wait\n- execute 和 compile 的延迟在 db time 中占比最高\n- CPS By Type 只有 query 这一种 command\n- avg QPS = 24.2k (56.3k->24.2k)\n- 无法命中 plan cache\n\n从场景 1 到场景 2，TiDB CPU 平均利用率从 925% 下降到 874%，TiKV CPU 平均利用率从 201% 上升到 250% 左右。\n\n![performance-overview-2-for-maxPerformance](/media/performance/9.1.1.png)\n\n关键延迟指标变化如下：\n\n![performance-overview-3-for-maxPerformance](/media/performance/9.2.2.png)\n\n- avg query duration = 1.12ms (479μs->1.12ms)\n- avg parse duration = 84.7μs (37.2μs->84.7μs)\n- avg compile duration = 370μs (166μs->370μs)\n- avg execution duration = 626μs (251μs->626μs)\n\n### 分析结论\n\n和场景 1 相比，场景 2 的 QPS 有明显的下降，平均 query duration 和 parse、compile 以及 execute duration 有明显的上升。这是因为场景 1 中类似 `select @@session.transaction_read_only` 这样执行次数多且处理时间快的 SQL 语句拉低了性能平均值，场景 2 屏蔽这类语句后只剩下纯业务 SQL，从而带来了 duration 平均值的上升。\n\n当应用使用 query 接口时，TiDB 无法使用执行计划缓存，编译执行计划消耗高。此时，建议使用 Prepared Statement 预编译接口，利用 TiDB 的执行计划缓存来降低 compile 带来的 TiDB CPU 消耗，降低延迟。\n\n## 场景 3：使用 Prepared Statement 接口，未开启执行计划缓存\n\n### 应用配置\n\n应用程序使用以下连接配置，和场景 2 对比，JDBC 的 `useServerPrepStmts` 参数值修改为 `true`，表示启用了预编译语句的接口。\n\n```\nuseServerPrepStmts=true&useConfigs=maxPerformance\"\n```\n\n### 性能分析\n\n#### TiDB Dashboard\n\n观察以下 TiDB 的火焰图，可以发现启用 Prepared Statement 接口之后，CompileExecutePreparedStmt 和 Optimize 的 CPU 占比依然明显。\n\n![flame-graph-for-PrepStmts](/media/performance/3.1.1.png)\n\n- ExecutePreparedStmt cpu = 31%  cpu time = 23.10s\n- preparedStmtExec cpu = 30% cpu time = 22.92s\n- CompileExecutePreparedStmt cpu = 24% cpu time = 17.83s\n- Optimize cpu = 23%  cpu time = 17.29s\n\n#### Performance Overview 面板\n\n使用 Prepared Statement 接口之后，数据库时间概览和 QPS 的数据如下：\n\n![performance-overview-1-for-PrepStmts](/media/performance/j-3.png)\n\nQPS 从 24.4k 下降到 19.7k，从 CPS By Type 面板可以看到应用程序使用了三种 Prepared 命令。Database Time Overview 出现了 general 的语句类型（包含了 StmtPrepare 和 StmtClose 等命令的执行耗时），占比排名第二。这说明，即使使用了 Prepared Statement 接口，执行计划缓存也没有命中，原因在于 TiDB 内部处理 StmtClose 命令时，会清理修改语句的执行计划缓存。\n\n- Database Time by SQL Type 中 select 语句耗时最多，其次是 general 语句\n- Database Time by SQL Phase 中 execute 和 compile 占比最多\n- SQL Execute Time Overview 中占比最多的分别是 Get、Cop、Prewrite 和 tso_wait\n- CPS By Type 变成 3 种 command：StmtPrepare、StmtExecute、StmtClose\n- avg QPS = 19.7k (24.4k->19.7k)\n- 无法命中 plan cache\n\nTiDB CPU 平均利用率从 874% 上升到 936%\n\n![performance-overview-1-for-PrepStmts](/media/performance/3-2.png)\n\n主要延迟数据如下：\n\n![performance-overview-2-for-PrepStmts](/media/performance/3.4.png)\n\n- avg query duration = 528μs (1.12ms->528μs)\n- avg parse duration = 14.9μs (84.7μs->14.9μs)\n- avg compile duration = 374μs (370μs->374μs)\n- avg execution duration = 649μs (626μs->649μs)\n\n### 分析结论\n\n和场景 2 不同的是，场景 3 启用了 prepare 预编译接口但是仍然无法命中缓存。此外，场景 2 的 CPS By Type 只有 query 这一种 command 类型，场景 3 多了 3 种 command 类型（StmtPrepare、StmtExecute、StmtClose）。与场景 2 相比，相当于多了两次网络往返的延迟。\n\n- QPS 的降低原因分析：从 CPS By Type 面板可以看到，场景 2 只有 query 这一种 command 类型，但场景 3 新增了 3 种 command 类型，即 StmtPrepare、StmtExecute 和 StmtClose。其中，StmtExecute 和 query 为常规类型 command，会被 QPS 统计，而 StmtPrepare 和 StmtClose 为非常规类型 command，不会被 QPS 统计，所以 QPS 降低了。非常规类型 command 的 StmtPrepare 和 StmtClose 被统计在 general SQL 类型中，因此可以看到 database overview 中多了 general 的时间，且占比在 database time 的四分之一以上。\n- 平均 query duration 明显降低原因分析：场景 3 新增了 StmtPrepare 和 StmtClose 这两种 command 类型，TiDB 内部处理时，query duration 也会单独计算，这两类命令处理速度很快，所以平均 query duration 明显被拉低。\n\n虽然场景 3 使用了 Prepare 预编译接口但是因为出现了 StmtClose 导致缓存失效，很多应用框架也会在 execute 后调用 close 方法来防止内存泄漏。从 v6.0.0 版本开始，你可以设置全局变量 `tidb_ignore_prepared_cache_close_stmt=on;`。设置后，即使应用调用了 StmtClose 方法，TiDB 也不会清除缓存的执行计划，使得下一次的 SQL 执行能重用现有的执行计划，避免重复编译执行计划。\n\n## 场景 4：使用 Prepared Statement 接口，开启执行计划缓存\n\n### 应用配置\n\n应用配置保持不变。设置以下参数，解决即使应用触发 StmtClose 导致无法命中缓存的问题。\n\n- 设置 TiDB 全局变量 `set global tidb_ignore_prepared_cache_close_stmt=on;`（TiDB v6.0.0 起正式使用，默认关闭)\n- 设置 TiDB 配置项 `prepared-plan-cache: {enabled: true}` 开启 plan cache 功能\n\n### 性能分析\n\n#### TiDB Dashboard\n\n观察 TiDB 的 CPU 火焰图，可以看到 CompileExecutePreparedStmt 和 Optimize 没有明显的 CPU 消耗。Prepare 命令的 CPU 占比 25%，包含了 PlanBuilder 和 parseSQL 等 Prepare 解析相关的函数。\n\nPreparseStmt cpu = 25% cpu time = 12.75s\n\n![flame-graph-for-3-commands](/media/performance/4.2.png)\n\n#### Performance Overview 面板\n\n在 Performance Overview 面板种，最显著的变化来自于 Compile 阶段的占比，从场景 3 每秒消耗 8.95 秒降低为 1.18 秒。执行计划缓存的命中次数大致等于 StmtExecute 次数。在 QPS 上升的前提下，每秒 Select 语句消耗的数据库时间降低了，general 类型的语句消耗时间变长。\n\n![performance-overview-1-for-3-commands](/media/performance/j-4.png)\n\n- Database Time by SQL Type 中 select 语句耗时最多\n- Database Time by SQL Phase 中 execute 占比最多\n- SQL Execute Time Overview 中占比最多的分别是 tso wait、Get 和 Cop\n- 命中 plan cache，Queries Using Plan Cache OPS 大致等于 StmtExecute 每秒的次数\n- CPS By Type 仍然是 3 种 command\n- general time 相比场景 3 变长，因为 QPS 上升了\n- avg QPS = 22.1k (19.7k->22.1k)\n\nTiDB CPU 平均利用率从 936% 下降到 827%。\n\n![performance-overview-2-for-3-commands](/media/performance/4.4.png)\n\nCompile 平均时间显著下降，从 374 us 下降到 53.3 us，因为 QPS 的上升，平均 execute 时间有所上升。\n\n![performance-overview-3-for-3-commands](/media/performance/4.5.png)\n\n- avg query duration = 426μs (528μs->426μs)\n- avg parse duration = 12.3μs (14.8μs->12.3μs)\n- avg compile duration = 53.3μs (374μs->53.3μs)\n- avg execution duration = 699μs (649μs->699us)\n\n### 分析结论\n\n和场景 3 相比，场景 4 同样存在 3 种 command 类型，不同的是场景 4 可以命中执行计划缓存，所以大大降低了 compile duration，同时降低了 query duration，并且提升了 QPS。\n\n因为 StmtPrepare 和 StmtClose 两种命令消耗的数据库时间明显，并且增加了应用程序每执行一条 SQL 语句需要跟 TiDB 交互的次数。下一个场景将通过 JDBC 配置优化掉这两种命令。\n\n## 场景 5：客户端缓存 prepared 对象\n\n### 应用配置\n\n和场景 4 相比，新增 3 个 JDBC 参数配置 `cachePrepStmts=true&prepStmtCacheSize=1000&prepStmtCacheSqlLimit=20480`，解释如下：\n\n- `cachePrepStmts = true`：在客户端缓存 prepared statement 对象，消除 StmtPrepare 和 StmtClose 调用。\n- `prepStmtCacheSize`：需要配置为大于 0 的值\n- `prepStmtCacheSqlLimit`：需要设置为大于 SQL 文本的长度\n\n完整的 JDBC 参数配置如下：\n\n```\nuseServerPrepStmts=true&cachePrepStmts=true&prepStmtCacheSize=1000&prepStmtCacheSqlLimit=20480&useConfigs=maxPerformance\n```\n\n### 性能分析\n\n#### TiDB Dashboard\n\n观察以下 TiDB 的火焰图，可以发现 Prepare 命令的高 CPU 消耗未再出现。\n\n- ExecutePreparedStmt cpu = 22% cpu time = 8.4s\n\n![flame-graph-for-1-command](/media/performance/5.1.1.png)\n\n#### Performance Overview 面板\n\n在 Performance Overview 面板中，最显著的变化是，CPS By Type 面板中三种 Stmt command 类型变成了一种，Database Time by SQL Type 面板中的 general 语句类型消失了，QPS 面板中 QPS 上升到了 30.9k。\n\n![performance-overview-for-1-command](/media/performance/j-5.png)\n\n- Database Time by SQL Type 中 select 语句耗时最多，general 语句类型消失了\n- Database Time by SQL Phase 中主要为 execute\n- SQL Execute Time Overview 中占比最多的分别是 tso wait、Get 和 Cop\n- 命中 plan cache，Queries Using Plan Cache OPS 大致等于 StmtExecute 每秒的次数\n- CPS By Type 只有一种 command，即 StmtExecute\n- avg QPS = 30.9k (22.1k->30.9k)\n\nTiDB CPU 平均利用率从 827% 下降到 577%，随着 QPS 的上升，TiKV CPU 平均利用率上升为 313%。\n\n![performance-overview-for-2-command](/media/performance/j-5-cpu.png)\n\n关键的延迟指标如下：\n\n![performance-overview-for-3-command](/media/performance/j-5-duration.png)\n\n- avg query duration = 690μs (426->690μs)\n- avg parse duration = 13.5μs (12.3μs->13.5μs )\n- avg compile duration = 49.7μs (53.3μs->49.7μs)\n- avg execution duration = 623μs (699us->623μs)\n- avg pd tso wait duration = 196μs (224μs->196μs)\n- connection idle duration avg-in-txn = 608μs (250μs->608μs)\n\n### 分析结论\n\n- 和场景 4 相比，场景 5 的 CPS By Type 只有 StmtExecute 这一种 command，减少了两次的网络往返，系统总体 QPS 上升。\n- 在 QPS 上升的情况下，从 parse duration、compile duration、execution duration 来看延迟降低了，但 query duration 反而上升了。这是因为，StmtPrepare 和 StmtClose 处理的速度非常快，消除这两种 command 类型之后，平均的 query duration 就会上升。\n- Database Time by SQL Phase 中 execute 占比非常高接近于 database time，同时 SQL Execute Time Overview 中占比最多的是 tso wait，超过四分之一的 execute 时间是在等待 tso。\n- 每秒的 tso wait 总时间为 5.46s。平均 tso wait 时间为 196 us，每秒的 tso cmd 次数为 28k，非常接近于 QPS 的 30.9k。因为在 TiDB 对于 `read committed` 隔离级别的实现中，事务中的每个 SQL 语句都需要都到 PD 请求 tso。\n\nTiDB v6.0 提供了 `rc read`，针对 `read committed` 隔离级别进行了减少 tso cmd 的优化。该功能由全局变量 `set global tidb_rc_read_check_ts=on;`控制。启用此变量后，TiDB 默认行为和 `repeatable-read` 隔离级别一致，只需要从 PD 获取 start-ts 和 commit-ts。事务中的语句先使用 start-ts 从 TiKV 读取数据。如果读到的数据小于 start-ts，则直接返回数据；如果读到大于 start-ts 的数据，则需要丢弃数据，并向 PD 请求 TSO 再进行重试。后续语句的 for update ts 使用最新的 PD TSO。\n\n## 场景 6：开启 tidb_rc_read_check_ts 变量降低 TSO 请求\n\n### 应用配置\n\n应用配置不变，和场景 5 不同的是，设置 `set global tidb_rc_read_check_ts=on;`，减少 TSO 请求。\n\n### 性能分析\n\n#### TiDB Dashboard\n\nTiDB 的 CPU 火焰图没有明显变化。\n\n- ExecutePreparedStmt cpu = 22% cpu time = 8.4s\n\n![flame-graph-for-rc-read](/media/performance/6.2.2.png)\n\n#### Performance Overview 面板\n\n使用 RC read 之后，QPS 从 30.9k 上升到 34.9k，每秒消耗的 tso wait 时间从 5.46 s 下降到 456 ms。\n\n![performance-overview-1-for-rc-read](/media/performance/j-6.png)\n\n- Database Time by SQL Type 中 select 语句耗时最多\n- Database Time by SQL Phase 中 execute 占比最高\n- SQL Execute Time Overview 中占比最多的分别是 Get、Cop 和 Prewrite\n- 命中 plan cache，Queries Using Plan Cache OPS 大致等于 StmtExecute 每秒的次数\n- CPS By Type 只有一种 command\n- avg QPS = 34.9k (30.9k->34.9k)\n\n每秒 tso cmd 从 28.3k 下降到 2.7k。\n\n![performance-overview-2-for-rc-read](/media/performance/j-6-cmd.png)\n\n平均 TiDB CPU 上升为 603% (577%->603%)。\n\n![performance-overview-3-for-rc-read](/media/performance/j-6-cpu.png)\n\n关键延迟指标如下：\n\n![performance-overview-4-for-rc-read](/media/performance/j-6-duration.png)\n\n- avg query duration = 533μs (690μs->533μs)\n- avg parse duration = 13.4μs (13.5μs->13.4μs )\n- avg compile duration = 50.3μs (49.7μs->50.3μs)\n- avg execution duration = 466μs (623μs->466μs)\n- avg pd tso wait duration = 171μs (196μs->171μs)\n\n### 分析结论\n\n通过 `set global tidb_rc_read_check_ts=on;` 启用 RC Read 之后，RC Read 明显降低了 tso cmd 次数从而降低了 tso wait 以及平均 query duration，并且提升了 QPS。\n\n当前数据库时间和延迟瓶颈都在 execute 阶段，而 execute 阶段占比最高的为 Get 和 Cop 读请求。这个负载中，大部分表是只读或者很少修改，可以使用 v6.0.0 的小表缓存功能，使用 TiDB 缓存这些小表的数据，降低 KV 读请求的等待时间和资源消耗。\n\n## 场景 7：使用小表缓存\n\n### 应用配置\n\n应用配置不变，在场景 6 的基础上设置了对业务的只读表进行缓存 `alter table t1 cache;`。\n\n### 性能分析\n\n#### TiDB Dashboard\n\nTiDB CPU 火焰图没有明显变化。\n\n![flame-graph-for-table-cache](/media/performance/7.2.png)\n\n#### Performance Overview 面板\n\nQPS 从 34.9k 上升到 40.9k，execute 时间中占比最高的 KV 请求类型变成了 Prewrite 和 Commit。Get 每秒的时间从 5.33 秒下降到 1.75 秒，Cop 每秒的时间从 3.87 下降到 1.09 秒。\n\n![performance-overview-1-for-table-cache](/media/performance/j-7.png)\n\n- Database Time by SQL Type 中 select 语句耗时最多\n- Database Time by SQL Phase 中 execute 和 compile 占比最多\n- SQL Execute Time Overview 中占比最多的分别是 Prewrite、Commit 和 Get\n- 命中 plan cache，Queries Using Plan Cache OPS 大致等于 StmtExecute 每秒的次数\n- CPS By Type 只有 1 种 command\n- avg QPS = 40.9k (34.9k->40.9k)\n\nTiDB CPU 平均利用率从 603% 下降到 478%，TiKV CPU 平均利用率从 346% 下降到 256%。\n\n![performance-overview-2-for-table-cache](/media/performance/j-7-cpu.png)\n\nQuery 平均延迟从 533 us 下降到 313 us。execute 平均延迟从 466 us 下降到 250 us。\n\n![performance-overview-3-for-table-cache](/media/performance/j-7-duration.png)\n\n- avg query duration = 313μs (533μs->313μs)\n- avg parse duration = 11.9μs (13.4μs->11.9μs)\n- avg compile duration = 47.7μs (50.3μs->47.7μs)\n- avg execution duration = 251μs (466μs->251μs)\n\n### 分析结论\n\n将所有只读表进行缓存后，可以看到，execute duration 下降非常明显，原因是所有只读表都缓存在 TiDB 中，不再需要到 TiKV 中查询数据，所以 query duration 下降，QPS 上升。\n\n这个是比较乐观的结果，实际的业务中可能只读表的数据量较大无法全部在 TiDB 中进行缓存。另一个限制是，当前小表缓存的功能虽然支持写操作，但是写操作默认需要等 3 秒，确保所有 TiDB 节点的缓存失效，对于对延迟要求比较高的应用，可能暂时不太友好。\n\n## 总结\n\n以下表格展示了七个不同场景的性能表现：\n\n| 指标  |   场景 1 | 场景 2   | 场景 3 | 场景 4 |场景 5 | 场景 6 | 场景 7 | 对比场景 5 和场景 2 (%) | 对比场景 7 和场景 3(%) |\n| --- | --- | --- | --- | --- | --- | --- | --- | --- |  --- |\n| query duration  | 479μs | 1120μs | 528μs | 426μs |690μs  | 533μs | 313μs | -38% | -41% |\n| QPS            | 56.3k |  24.2k | 19.7k | 22.1k | 30.9k | 34.9k | 40.9k | +28% | +108% |\n\n其中，场景 2 是应用程序使用 Query 接口的常见场景，场景 5 是应用程序使用 Prepared Statement 接口的理想场景。\n\n- 对比场景 5 和场景 2，可以发现通过使用 Java 应用开发的最佳实践以及客户端缓存 Prepared Statement 对象，每条 SQL 只需要一次命令和数据库交互，就能命中执行计划缓存，从而使 Query 延迟下降了 38%，QPS 上升 28%，同时，TiDB CPU 平均利用率从 936% 下降到 577%。\n- 对比场景 7 和场景 3，可以看到在场景 5 的基础上使用了 RC Read、小表缓存等 TiDB 最新的优化功能，延迟降低了 41%，QPS 提升了 108%，同时，TiDB CPU 平均利用率从 936% 下降到 478%。\n\n通过对比各场景的性能表现，可以得出以下结论：\n\n- TiDB 的执行计划缓存对于 OLTP 发挥着至关重要的作用。而从 v6.0.0 开始引入的 RC Read 和小表缓存功能，在这个负载的深度优化中，也发挥了重要的作用。\n- TiDB 兼容 MySQL 协议的不同命令，最佳的性能表现来自于应用程序使用 Prepared Statement 接口，并设置以下 JDBC 连接参数：\n\n    ```\n    useServerPrepStmts=true&cachePrepStmts=true&prepStmtCacheSize=1000&prepStmtCacheSqlLimit=20480&useConfigs=maxPerformance\n    ```\n\n- 在性能分析和优化过程中，推荐使用 TiDB Dashboard （例如 Top SQL 功能和持续性能分析功能）和 Performance Overview 面板。\n\n    - [Top SQL](/dashboard/top-sql.md) 功能允许你可视化地监控和探索数据库中各个 SQL 语句在执行过程中的 CPU 开销情况，从而对数据库性能问题进行优化和处理。\n    - [持续性能分析功能](/dashboard/continuous-profiling.md) 可以持续地收集 TiDB、TiKV、PD 各个实例的性能数据。应用程序使用不同接口跟 TiDB 交互时，TiDB 的 CPU 消耗有着巨大的差距。\n    - [Performance Overview 面板](/grafana-performance-overview-dashboard.md) 提供了数据库时间的概览和 SQL 执行时间分解信息。借助这个面板，你可以进行基于数据库时间的性能分析和诊断，确定整个系统的性能瓶颈是否处于 TiDB 中。如果瓶颈在 TiDB 中，你可以通过数据库时间和延迟的分解，以及集群关键指标和资源使用情况，确认 TiDB 内部的性能瓶颈，并进行针对性的优化。\n\n综合使用以上几个功能，你可以针对现实中的应用进行高效的性能分析和优化。"
        },
        {
          "name": "pessimistic-transaction.md",
          "type": "blob",
          "size": 12.4521484375,
          "content": "---\ntitle: TiDB 悲观事务模式\nsummary: 了解 TiDB 的悲观事务模式。\naliases: ['/docs-cn/dev/pessimistic-transaction/','/docs-cn/dev/reference/transactions/transaction-pessimistic/']\n---\n\n# TiDB 悲观事务模式\n\n为了使 TiDB 的使用方式更加贴近传统数据库，降低用户迁移的成本，TiDB 自 v3.0 版本开始在乐观事务模型的基础上支持了悲观事务模式。本文将介绍 TiDB 悲观事务的相关特性。\n\n> **注意：**\n>\n> 自 v3.0.8 开始，新创建的 TiDB 集群默认使用悲观事务模式。但如果从 v3.0.7 版本及之前创建的集群升级到 >= v3.0.8 的版本，则不会改变默认的事务模式，即**只有新创建的集群才会默认使用悲观事务模式**。\n\n## 事务模式的修改方法\n\n你可以使用 [`tidb_txn_mode`](/system-variables.md#tidb_txn_mode) 系统变量设置事务模式。执行以下命令，即可使整个集群中所有新创建 session 执行的所有显示事务（即非 autocommit 的事务）进入悲观事务模式：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET GLOBAL tidb_txn_mode = 'pessimistic';\n```\n\n除此之外，还可以执行以下 SQL 语句显式地开启悲观事务：\n\n{{< copyable \"sql\" >}}\n\n```sql\nBEGIN PESSIMISTIC;\n```\n\n{{< copyable \"sql\" >}}\n\n```\nBEGIN /*T! PESSIMISTIC */;\n```\n\n`BEGIN PESSIMISTIC;` 和 `BEGIN OPTIMISTIC;` 等语句的优先级高于 `tidb_txn_mode` 系统变量。使用这两个语句开启的事务，会忽略系统变量，从而支持悲观、乐观事务混合使用。\n\n## 悲观事务模式的行为\n\n悲观事务的行为和 MySQL 基本一致（不一致之处详见[和 MySQL InnoDB 的差异](#和-mysql-innodb-的差异)）：\n\n- 悲观事务中引入快照读和当前读的概念：\n\n    - 快照读是一种不加锁读，读的是该事务开始时刻前已提交的版本。`SELECT` 语句中的读是快照读。\n    - 当前读是一种加锁读，读取的是最新已提交的版本，`UPDATE`、`DELETE` 、`INSERT`、`SELECT FOR UPDATE` 语句中的读是当前读。\n\n    以下示例是对快照读和当前读的详细说明：\n\n    | session 1 | session 2 | session 3 |\n    | :----| :---- | :---- |\n    | CREATE TABLE t (a INT); |  |  |\n    | INSERT INTO T VALUES(1); |  |  |\n    | BEGIN PESSIMISTIC; |  |\n    | UPDATE t SET a = a + 1; |  |  |\n    |  | BEGIN PESSIMISTIC; |  |\n    |  | SELECT * FROM t;  -- 使用快照读，读取本事务开始前已提交的版本，返回(a=1) |  |\n    |  |  | BEGIN PESSIMISTIC;\n    |  |  | SELECT * FROM t FOR UPDATE; -- 使用当前读，等锁 |\n    | COMMIT; -- 释放锁，session 3 的 SELECT FOR UPDATE 操作获得锁，使用当前读，读到最新已提交的版本 (a=2) |  |  |\n    |  | SELECT * FROM t; -- 使用快照读，读取本事务开始前已提交的版本，返回(a=1) |  |\n\n- 悲观锁会在事务提交或回滚时释放。其他尝试修改这一行的写事务会被阻塞，等待悲观锁的释放。其他尝试*读取*这一行的事务不会被阻塞，因为 TiDB 采用多版本并发控制机制 (MVCC)。\n\n- 需要检查唯一性约束的悲观锁可以通过设置系统变量 [`tidb_constraint_check_in_place_pessimistic`](/system-variables.md#tidb_constraint_check_in_place_pessimistic-从-v630-版本开始引入) 控制是否跳过，详见[约束](/constraints.md#悲观事务)。\n\n- 如果多个事务尝试获取各自的锁，会出现死锁，并被检测器自动检测到。其中一个事务会被随机终止掉并返回兼容 MySQL 的错误码 `1213`。\n\n- 通过 `innodb_lock_wait_timeout` 变量，设置事务等锁的超时时间（默认值为 `50`，单位为秒）。等锁超时后返回兼容 MySQL 的错误码 `1205`。如果多个事务同时等待同一个锁释放，会大致按照事务 `start ts` 顺序获取锁。\n\n- 乐观事务和悲观事务可以共存，事务可以任意指定使用乐观模式或悲观模式来执行。\n\n- 支持 `FOR UPDATE NOWAIT` 语法，遇到锁时不会阻塞等锁，而是返回兼容 MySQL 的错误码 `3572`。\n\n- 如果 `Point Get` 和 `Batch Point Get` 算子没有读到数据，依然会对给定的主键或者唯一键加锁，阻塞其他事务对相同主键唯一键加锁或者进行写入操作。\n\n- 支持 `FOR UPDATE OF TABLES` 语法，对于存在多表 join 的语句，只对 `OF TABLES` 中包含的表关联的行进行悲观锁加锁操作。 \n\n## 和 MySQL InnoDB 的差异\n\n1. 有些 `WHERE` 子句中使用了 range，TiDB 在执行这类 DML 语句和 `SELECT FOR UPDATE` 语句时，不会阻塞 range 内并发的 DML 语句的执行。\n\n    举例：\n\n    ```sql\n    CREATE TABLE t1 (\n     id INT NOT NULL PRIMARY KEY,\n     pad1 VARCHAR(100)\n    );\n    INSERT INTO t1 (id) VALUES (1),(5),(10);\n    ```\n\n    ```sql\n    BEGIN /*T! PESSIMISTIC */;\n    SELECT * FROM t1 WHERE id BETWEEN 1 AND 10 FOR UPDATE;\n    ```\n\n    ```sql\n    BEGIN /*T! PESSIMISTIC */;\n    INSERT INTO t1 (id) VALUES (6); -- 仅 MySQL 中出现阻塞。\n    UPDATE t1 SET pad1='new value' WHERE id = 5; -- MySQL 和 TiDB 处于等待阻塞状态。\n    ```\n\n    产生这一行为是因为 TiDB 当前不支持 _gap locking_（间隙锁）。\n\n2. TiDB 不支持 `SELECT LOCK IN SHARE MODE`。\n\n    TiDB 默认不支持 `SELECT LOCK IN SHARE MODE` 语法。可以通过启用 [`tidb_enable_noop_functions`](/system-variables.md#tidb_enable_noop_functions-从-v40-版本开始引入) 来兼容 `SELECT LOCK IN SHARE MODE` 语法。此时，该语法的效果和没有加锁一样，不会阻塞其他事务的读写。\n\n    从 v8.3.0 版本开始，TiDB 支持通过启用 [`tidb_enable_shared_lock_promotion`](/system-variables.md#tidb_enable_shared_lock_promotion-从-v830-版本开始引入) 系统变量使 `SELECT LOCK IN SHARE MODE` 语句产生加锁行为。但需要注意，此时加的锁并不是真正的共享锁，而是与 `SELECT FOR UPDATE` 一致，实际加的是排他锁。如果你需要兼容 `SELECT LOCK IN SHARE MODE` 语法的同时，希望与写入相互阻塞、避免读期间数据被并行的写入事务修改，可考虑启用该变量。该变量无论 [`tidb_enable_noop_functions`](/system-variables.md#tidb_enable_noop_functions-从-v40-版本开始引入) 配置如何都会生效。\n\n3. DDL 可能会导致悲观事务提交失败。\n\n    MySQL 在执行 DDL 语句时，会被正在执行的事务阻塞住，而在 TiDB 中 DDL 操作会成功，造成悲观事务提交失败：`ERROR 1105 (HY000): Information schema is changed. [try again later]`。TiDB 事务执行过程中并发执行 `TRUNCATE TABLE` 语句，可能会导致事务报错 `table doesn't exist`。\n\n4. `START TRANSACTION WITH CONSISTENT SNAPSHOT` 之后，MySQL 仍然可以读取到之后在其他事务创建的表，而 TiDB 不能。\n\n5. autocommit 事务优先采用乐观事务提交。\n    \n    使用悲观事务模式时，autocommit 事务首先尝试使用开销更小的乐观事务模式提交。如果发生了写冲突，重试时才会使用悲观事务提交。所以 `tidb_retry_limit = 0` 时，autocommit 事务遇到写冲突仍会报 `Write Conflict` 错误。\n\n    自动提交的 `SELECT FOR UPDATE` 语句不会等锁。\n\n6. 对语句中 `EMBEDDED SELECT` 读到的相关数据不会加锁。\n\n7. 垃圾回收 (GC) 不会影响到正在执行的事务，但悲观事务的执行时间仍有上限，默认为 1 小时，可通过 TiDB 配置文件 `[performance]` 类别下的 `max-txn-ttl` 修改。\n\n## 隔离级别\n\nTiDB 在悲观事务模式下支持了 2 种隔离级别：\n\n1. 默认使用与 MySQL 行为相同的[可重复读隔离级别 (Repeatable Read)](/transaction-isolation-levels.md#可重复读隔离级别-repeatable-read)。\n\n    > **注意：**\n    >\n    > 在这种隔离级别下，DML 操作会基于已提交的最新数据来执行，行为与 MySQL 相同，但与 TiDB 乐观事务不同，请参考[与 MySQL 可重复读隔离级别的区别](/transaction-isolation-levels.md#与-mysql-可重复读隔离级别的区别)。\n\n2. 使用 [`SET TRANSACTION`](/sql-statements/sql-statement-set-transaction.md) 语句可将隔离级别设置为[读已提交隔离级别 (Read Committed)](/transaction-isolation-levels.md#读已提交隔离级别-read-committed)。\n\n## 悲观事务提交流程\n\nTiDB 悲观锁复用了乐观锁的两阶段提交逻辑，重点在 DML 执行时做了改造。\n\n![TiDB 悲观事务的提交流程](/media/pessimistic-transaction-commit.png)\n\n在两阶段提交之前增加了 Acquire Pessimistic Lock 阶段，简要步骤如下。\n\n1. （同乐观锁）TiDB 收到来自客户端的 begin 请求，获取当前时间戳作为本事务的 StartTS。\n2. TiDB 收到来自客户端的更新数据的请求：TiDB 向 TiKV 发起加悲观锁请求，该锁持久化到 TiKV。\n3. （同乐观锁）客户端发起 commit，TiDB 开始执行与乐观锁一样的两阶段提交。\n\n![TiDB 中的悲观事务](/media/pessimistic-transaction-in-tidb.png)\n\n相关细节本节不再赘述，详情可阅读 [TiDB 悲观锁实现原理](https://tidb.net/blog/7730ed79)。\n\n## Pipelined 加锁流程\n\n加悲观锁需要向 TiKV 写入数据，要经过 Raft 提交并 apply 后才能返回，相比于乐观事务，不可避免的会增加部分延迟。为了降低加锁的开销，TiKV 实现了 pipelined 加锁流程：当数据满足加锁要求时，TiKV 立刻通知 TiDB 执行后面的请求，并异步写入悲观锁，从而降低大部分延迟，显著提升悲观事务的性能。但当 TiKV 出现网络隔离或者节点宕机时，悲观锁异步写入有可能失败，从而产生以下影响：\n\n* 无法阻塞修改相同数据的其他事务。如果业务逻辑依赖加锁或等锁机制，业务逻辑的正确性将受到影响。\n\n* 有较低概率导致事务提交失败，但不会影响事务正确性。\n\n如果业务逻辑依赖加锁或等锁机制，或者即使在集群异常情况下也要尽可能保证事务提交的成功率，应关闭 pipelined 加锁功能。\n\n![Pipelined pessimistic lock](/media/pessimistic-transaction-pipelining.png)\n\n该功能默认开启，可修改 TiKV 配置关闭：\n\n```toml\n[pessimistic-txn]\npipelined = false\n```\n\n若集群是 v4.0.9 及以上版本，也可通过[在线修改 TiKV 配置](/dynamic-config.md#在线修改-tikv-配置)功能动态关闭该功能：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset config tikv pessimistic-txn.pipelined='false';\n```\n\n## 内存悲观锁\n\nTiKV 在 v6.0.0 中引入了内存悲观锁功能。开启内存悲观锁功能后，悲观锁通常只会被存储在 Region leader 的内存中，而不会将锁持久化到磁盘，也不会通过 Raft 协议将锁同步到其他副本，因此可以大大降低悲观事务加锁的开销，提升悲观事务的吞吐并降低延迟。\n\n当内存悲观锁占用的内存达到 [Region](/tikv-configuration-file.md#in-memory-peer-size-limit-从-v840-版本开始引入) 或 [TiKV 节点](/tikv-configuration-file.md#in-memory-instance-size-limit-从-v840-版本开始引入)的阈值时，加悲观锁会回退为使用 [pipelined 加锁流程](#pipelined-加锁流程)。当 Region 发生合并或 leader 迁移时，为避免悲观锁丢失，TiKV 会将内存悲观锁写入磁盘并同步到其他副本。\n\n内存悲观锁实现了和 [pipelined 加锁](#pipelined-加锁流程)类似的表现，即集群无异常时不影响加锁表现，但当 TiKV 出现网络隔离或者节点宕机时，事务加的悲观锁可能丢失。\n\n如果业务逻辑依赖加锁或等锁机制，或者即使在集群异常情况下也要尽可能保证事务提交的成功率，应**关闭**内存悲观锁功能。\n\n该功能默认开启。如要关闭，可修改 TiKV 配置：\n\n```toml\n[pessimistic-txn]\nin-memory = false\n```\n\n也可通过[在线修改 TiKV 配置](/dynamic-config.md#在线修改-tikv-配置)功能动态关闭该功能：\n\n{{< copyable \"sql\" >}}\n\n```sql\nset config tikv pessimistic-txn.in-memory='false';\n```\n\n从 v8.4.0 开始，你可以通过 [`pessimistic-txn.in-memory-peer-size-limit`](/tikv-configuration-file.md#in-memory-peer-size-limit-从-v840-版本开始引入) 或 [`pessimistic-txn.in-memory-instance-size-limit`](/tikv-configuration-file.md#in-memory-instance-size-limit-从-v840-版本开始引入) 配置项修改 Region 或 TiKV 节点内存悲观锁的内存使用上限：\n\n```toml\n[pessimistic-txn]\nin-memory-peer-size-limit = \"512KiB\"\nin-memory-instance-size-limit = \"100MiB\"\n```\n\n也可通过[在线修改 TiKV 配置](/dynamic-config.md#在线修改-tikv-配置)功能动态调整：\n\n```sql\nSET CONFIG tikv `pessimistic-txn.in-memory-peer-size-limit`=\"512KiB\";\nSET CONFIG tikv `pessimistic-txn.in-memory-instance-size-limit`=\"100MiB\";\n```\n"
        },
        {
          "name": "pipelined-dml.md",
          "type": "blob",
          "size": 8.5390625,
          "content": "---\ntitle: Pipelined DML\nsummary: 介绍 Pipelined DML 的使用场景、使用方法、使用限制和使用该功能的常见问题。Pipelined DML 增强了 TiDB 批量处理的能力，使得事务大小不再受到 TiDB 内存限制。\n---\n\n# Pipelined DML\n\n> **警告：**\n>\n> 该功能目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。语法和实现可能会在 GA 前发生变化。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n本文介绍 Pipelined DML 的使用场景、使用方法、使用限制和使用该功能的常见问题。\n\n## 功能概述\n\nPipelined DML 是 TiDB 从 v8.0.0 开始引入的实验特性，用于优化大规模数据写入场景的性能。启用 Pipelined DML 后，当执行 DML 操作时，TiDB 会将相应的数据持续写入存储层，而不是全部缓存在内存中。这种方式就像流水线 (Pipeline) 一样，数据一边被读取（输入），一边被写入到存储层（输出），从而解决了大规模 DML 操作的以下常见问题：\n\n- 内存限制：传统 DML 在处理大量数据时容易导致 OOM\n- 性能瓶颈：大事务执行效率低，容易引起系统负载波动\n\n启用此功能后，你可以：\n\n- 执行不受 TiDB 内存限制的大规模数据操作\n- 获得更平稳的系统负载和更低的操作延迟\n- 将事务内存使用控制在可预测范围内（在 1 GiB 以内）\n\n在以下场景中，建议考虑启用 Pipelined DML：\n\n- 需要处理数百万行或更多数据的写入\n- 执行 DML 操作时遇到内存不足错误\n- 大规模数据操作导致系统负载波动明显\n\n需要注意的是，虽然 Pipelined DML 能显著降低事务处理的内存需求，但执行大规模数据操作时，仍需要[设置合理的内存阈值](/system-variables.md#tidb_mem_quota_query)（建议至少 2 GiB），以确保除事务以外其他模块（如执行器）的正常运行。\n\n## 使用限制\n\n目前，Pipelined DML 存在以下使用限制：\n\n- 与 TiCDC、TiFlash 或 BR 尚不兼容，请勿在与这些组件有关的表上使用 Pipelined DML。强行使用可能会引发阻塞以及这些组件的 OOM 等问题。\n- 不适用于存在写入冲突的场景。在这种场景下，Pipelined DML 性能可能大幅下降，或失败回滚。\n- 在使用 Pipelined DML 执行 DML 语句的过程中，需要确保[元数据锁](/metadata-lock.md)保持开启。\n- 启用 Pipelined DML 后，TiDB 在执行 DML 语句时会自动检测以下条件是否全部符合。如果其中任一条件不符合，TiDB 会拒绝使用 Pipelined DML 执行该语句，自动回退到普通 DML 执行，并生成对应的 warning 信息：\n    - 仅支持[自动提交](/transaction-overview.md#自动提交)的语句。\n    - 仅支持 `INSERT`、`UPDATE`、`REPLACE` 和 `DELETE` 语句。\n    - 操作的表不包含[临时表](/temporary-tables.md)或[缓存表](/cached-tables.md)。\n    - 当[外键约束](/foreign-key.md)检查开启 (`foreign_key_checks = ON`) 时，操作的表不包含外键关系。\n- 当使用 Pipelined DML 执行 `INSERT IGNORE ... ON DUPLICATE KEY UPDATE` 语句时，如果更新操作发生冲突，可能会返回 `Duplicate entry` 错误。\n\n## 使用方法\n\n本小节介绍如何启用 Pipelined DML 并验证其是否生效。\n\n### 启用 Pipelined DML\n\n根据需要，你可以选择以下方式之一启用 Pipelined DML：\n\n- 如需在会话级别启用 Pipelined DML，请将 [`tidb_dml_type`](/system-variables.md#tidb_dml_type-从-v800-版本开始引入) 变量设置为 `\"bulk\"`：\n\n    ```sql\n    SET tidb_dml_type = \"bulk\";\n    ```\n\n- 如需为某一条 DML 语句启用 Pipelined DML，请在该语句中添加 [`SET_VAR`](/optimizer-hints.md#set_varvar_namevar_value) hint。\n\n    - 数据归档示例：\n\n        ```sql\n        INSERT /*+ SET_VAR(tidb_dml_type='bulk') */ INTO target_table SELECT * FROM source_table;\n        ```\n\n    - 批量数据更新示例：\n\n        ```sql\n        UPDATE /*+ SET_VAR(tidb_dml_type='bulk') */ products\n        SET price = price * 1.1\n        WHERE category = 'electronics';\n        ```\n\n    - 批量删除示例：\n\n        ```sql\n        DELETE /*+ SET_VAR(tidb_dml_type='bulk') */ FROM logs WHERE log_time < '2023-01-01';\n        ```\n\n### 验证是否生效\n\n执行 DML 语句后，可以查看 [`tidb_last_txn_info`](/system-variables.md#tidb_last_txn_info-从-v409-版本开始引入) 变量来确认该语句的执行是否使用了 Pipelined DML：\n\n```sql\nSELECT @@tidb_last_txn_info;\n```\n\n如果返回结果中 `pipelined` 字段为 `true`，则表示成功使用了 Pipelined DML。\n\n## 最佳实践\n\n- 将 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 略微调大，以确保执行器等部分的内存使用不会超过限制。建议至少设置为 2 GiB。对于 TiDB 内存充足的情况，可以适当调大。\n- 在向新表插入数据的场景，Pipelined DML 性能易受到热点影响。为实现最佳性能，建议尽可能先打散热点。可以参考 [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md)。\n\n## 相关配置\n\n- 系统变量 [`tidb_dml_type`](/system-variables.md#tidb_dml_type-从-v800-版本开始引入) 用于控制是否在会话级别启用 Pipelined DML。\n- 当 [`tidb_dml_type`](/system-variables.md#tidb_dml_type-从-v800-版本开始引入) 设置为 `\"bulk\"` 时，配置项 [`pessimistic-auto-commit`](/tidb-configuration-file.md#pessimistic-auto-commit) 的效果等同于设置为 `false`。\n- 以 Pipelined DML 方式执行事务时，事务的大小不受 TiDB 配置项 [`txn-total-size-limit`](/tidb-configuration-file.md#txn-total-size-limit) 的限制。\n- 以 Pipelined DML 方式执行超大事务时，事务耗时可能较长。对于这种模式的事务，其事务锁的最大 TTL 为 [`max-txn-ttl`](/tidb-configuration-file.md#max-txn-ttl) 与 24 小时中的较大值。\n- 当事务执行时间超过 [`tidb_gc_max_wait_time`](/system-variables.md#tidb_gc_max_wait_time-从-v610-版本开始引入) 设定值后，GC 可能会强制回滚事务，导致事务失败。\n\n## 观测 Pipelined DML\n\n你可以通过以下方式观测 Pipelined DML 的执行过程：\n\n- 查看系统变量 [`tidb_last_txn_info`](/system-variables.md#tidb_last_txn_info-从-v409-版本开始引入)，获取当前会话上一个事务的执行信息，包括是否使用了 Pipelined DML。\n- 查看 TiDB 日志中包含 `\"[pipelined dml]\"` 字样的行，了解 Pipelined DML 的执行过程和进度，包括当前阶段、已经写入的数据量等。\n- 查看 TiDB 日志中的 [`expensive query` 日志](/identify-expensive-queries.md#expensive-query-日志示例)的 `affected rows` 字段，获取耗时较长语句的当前进度。\n- 查看 [`INFORMATION_SCHEMA.PROCESSLIST`](/information-schema/information-schema-processlist.md) 表，了解事务的执行进度。Pipelined DML 通常用于大事务，执行耗时较长，可以通过该表查看事务的执行进度。\n\n## 常见问题\n\n### 为什么我的查询没有使用 Pipelined DML？\n\n当 TiDB 拒绝以 Pipelined DML 模式执行语句时，会生成对应的警告信息，可以通过检查警告信息 (`SHOW WARNINGS;`) 确定原因。\n\n常见的原因：\n\n- DML 语句不是自动提交的\n- 使用了不支持的表类型，例如[临时表](/temporary-tables.md)或[缓存表](/cached-tables.md)\n- 涉及外键且外键检查开启\n\n### Pipelined DML 会影响事务的隔离级别吗？\n\n不会。Pipelined DML 仅改变了事务写入的实现机制，不影响 TiDB 的事务隔离保证。\n\n### 为什么使用了 Pipelined DML 还是会出现内存不足？\n\n即使开启了 Pipelined DML，仍然有可能碰到内存 quota 不足导致语句被 kill 的情况：\n\n```\nThe query has been canceled due to exceeding the memory limit allowed for a single SQL query. Please try to narrow the query scope or increase the tidb_mem_quota_query limit, and then try again.\n```\n\n这是因为 Pipelined DML 功能仅能控制事务执行过程中数据使用的内存，但语句执行时使用的总内存还包括执行器等部分的内存。如果语句执行时所需的总内存超过了 TiDB 的内存限制，仍然可能会出现内存不足的错误。\n\n通常情况下，将系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 设置为更大的值可以解决该问题。推荐设置为 2 GiB。对于算子复杂或涉及数据量大的 SQL 语句，可能需要将该变量设置为更大的值。\n\n## 探索更多\n\n- [批量处理概览](/batch-processing.md)\n- [TiDB 内存控制](/configure-memory-usage.md)"
        },
        {
          "name": "placement-rules-in-sql.md",
          "type": "blob",
          "size": 25.6796875,
          "content": "---\ntitle: Placement Rules in SQL\nsummary: 了解如何通过 SQL 接口调度表和分区的放置位置。\n---\n\n# Placement Rules in SQL\n\nPlacement Rules in SQL 特性用于通过 SQL 语句配置数据在 TiKV 集群中的放置位置。通过该功能，你可以将集群、数据库、表、或分区的数据部署到不同的地域、机房、机柜、主机。\n\n该功能可以实现以下业务场景：\n\n- 多数据中心部署，配置规则优化数据高可用策略\n- 合并多个不同业务的数据库，物理隔离不同用户的数据，满足实例内部不同用户的隔离需求\n- 增加重要数据的副本数，提高业务可用性和数据可靠性\n\n## 功能概述\n\n通过 Placement Rules in SQL 功能, 你可以[创建放置策略 (placement policy)](#创建并绑定放置策略)，并为不同的数据级别配置所需的放置策略，粒度从粗到细为：\n\n| 级别                | 描述                                                                                    |\n|----------------------------|------------------------------------------------------------------------------------------------|\n| 集群          | TiDB 默认为集群配置 3 副本的策略。你可以为集群配置全局放置策略，参考[集群配置](#为集群指定全局的副本数)。  |\n| 数据库        | 你可以为指定的 Database 配置放置策略，参考[为数据库指定默认的放置策略](#为数据库指定默认的放置策略)。 |\n| 表            | 你可以为指定的 Table 配置放置策略，参考[为表指定放置策略](#为表指定放置策略)。  |\n| 分区          | 你可以为表中不同的 Row 创建分区，并单独对分区配置放置策略，参考[为分区表指定放置策略](#为分区表指定放置策略)。 |\n\n> **建议：**\n>\n> Placement Rules in SQL 底层的实现依赖 PD 提供的放置规则 (placement rules) 功能，参考 [Placement Rules 使用文档](/configure-placement-rules.md)。在 Placement Rules in SQL 语境下，放置规则既可以代指绑定对象的放置策略 (placement policy)，也可以代指 TiDB 发给 PD 的放置规则。\n\n## 使用限制\n\n- 为了降低运维难度，建议将一个集群的 placement policy 数量限制在 10 个以内。\n- 建议将绑定了 placement policy 的表和分区数的总数限制在 10000 以内。为过多的表和分区绑定 policy，会增加 PD 上规则计算的负担，从而影响服务性能。\n- 建议按照本文中提到的示例场景使用 Placement Rules in SQL 功能，不建议使用复杂的放置策略。\n\n## 前提条件\n\n放置策略依赖于 TiKV 节点标签 (label) 的配置。例如，放置选项 `PRIMARY_REGION` 依赖 TiKV 中的 `region` 标签。\n\n创建放置策略时，TiDB 不会检查标签是否存在，而是在绑定放置策略的时候进行检查。因此，在绑定放置策略前，请确保各个 TiKV 节点已配置正确的标签。配置方法为：\n\n```\ntikv-server --labels region=<region>,zone=<zone>,host=<host>\n```\n\n详细配置方法可参考以下示例:\n\n| 方式 | 示例 |\n| --- | --- |\n| 手动部署 | [通过拓扑 label 进行副本调度](/schedule-replicas-by-topology-labels.md) |\n| TiUP 部署 | [跨机房部署拓扑结构](/geo-distributed-deployment-topology.md) |\n| Operator 部署| [在 Kubernetes 中配置 TiDB 集群](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/configure-a-tidb-cluster#数据的高可用) |\n\n如需查看当前 TiKV 集群中所有可用的标签，可以使用 [`SHOW PLACEMENT LABELS`](/sql-statements/sql-statement-show-placement-labels.md) 语句：\n\n```sql\nSHOW PLACEMENT LABELS;\n+--------+----------------+\n| Key    | Values         |\n+--------+----------------+\n| disk   | [\"ssd\"]        |\n| region | [\"us-east-1\"]  |\n| zone   | [\"us-east-1a\"] |\n+--------+----------------+\n3 rows in set (0.00 sec)\n```\n\n## 使用方法\n\n本节介绍如何通过 SQL 语句创建、绑定、查看、修改、删除放置策略。\n\n### 创建并绑定放置策略\n\n1. 使用 [`CREATE PLACEMENT POLICY`](/sql-statements/sql-statement-create-placement-policy.md) 语句创建放置策略：\n\n    ```sql\n    CREATE PLACEMENT POLICY myplacementpolicy PRIMARY_REGION=\"us-east-1\" REGIONS=\"us-east-1,us-west-1\";\n    ```\n\n    在该语句中：\n\n    - `PRIMARY_REGION=\"us-east-1\"` 选项代表 Raft leader 被放置在 `region` 标签为 `us-east-1` 的节点上。\n    - `REGIONS=\"us-east-1,us-west-1\"` 选项代表 Raft followers 被放置在 `region` 标签为 `us-east-1` 和 `region` 标签为 `us-west-1` 的节点上。\n\n    更多可配置的放置选项和对应的含义，请参考[放置选项](#放置选项参考)。\n\n2. 使用 `CREATE TABLE` 或者 `ALTER TABLE` 将放置策略绑定至表或分区表，这样就在表或分区上指定了放置策略：\n\n    ```sql\n    CREATE TABLE t1 (a INT) PLACEMENT POLICY=myplacementpolicy;\n    CREATE TABLE t2 (a INT);\n    ALTER TABLE t2 PLACEMENT POLICY=myplacementpolicy;\n    ```\n\n    `PLACEMENT POLICY` 为全局作用域，不与任何数据库表结构相关联。因此，通过 `CREATE TABLE` 指定放置策略时，无需任何额外的权限。\n\n### 查看放置策略\n\n- 要查看某条已创建的放置策略，可以使用 [`SHOW CREATE PLACEMENT POLICY`](/sql-statements/sql-statement-show-create-placement-policy.md) 语句：\n\n    ```sql\n    SHOW CREATE PLACEMENT POLICY myplacementpolicy\\G\n    *************************** 1. row ***************************\n           Policy: myplacementpolicy\n    Create Policy: CREATE PLACEMENT POLICY myplacementpolicy PRIMARY_REGION=\"us-east-1\" REGIONS=\"us-east-1,us-west-1\"\n    1 row in set (0.00 sec)\n    ```\n\n- 要查看某张表绑定的放置策略，可以使用 [`SHOW CREATE TABLE`](/sql-statements/sql-statement-show-create-table.md) 语句：\n\n    ```sql\n    SHOW CREATE TABLE t1\\G\n    *************************** 1. row ***************************\n           Table: t1\n    Create Table: CREATE TABLE `t1` (\n      `a` int DEFAULT NULL\n    ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin /*T![placement] PLACEMENT POLICY=`myplacementpolicy` */\n    1 row in set (0.00 sec)\n    ```\n\n- 要查看集群中所有放置策略的定义，可以查询 [`INFORMATION_SCHEMA.PLACEMENT_POLICIES`](/information-schema/information-schema-placement-policies.md) 系统表：\n\n    ```sql\n    SELECT * FROM information_schema.placement_policies\\G\n    ***************************[ 1. row ]***************************\n    POLICY_ID            | 1\n    CATALOG_NAME         | def\n    POLICY_NAME          | p1\n    PRIMARY_REGION       | us-east-1\n    REGIONS              | us-east-1,us-west-1\n    CONSTRAINTS          |\n    LEADER_CONSTRAINTS   |\n    FOLLOWER_CONSTRAINTS |\n    LEARNER_CONSTRAINTS  |\n    SCHEDULE             |\n    FOLLOWERS            | 4\n    LEARNERS             | 0\n    1 row in set\n    ```\n\n- 要查看集群中所有绑定了放置策略的表，可以查询 `information_schema.tables` 系统表的 `tidb_placement_policy_name` 列：\n\n    ```sql\n    SELECT * FROM information_schema.tables WHERE tidb_placement_policy_name IS NOT NULL;\n    ```\n\n- 要查看集群中所有绑定了放置策略的分区，可以查询 `information_schema.partitions` 系统表的 `tidb_placement_policy_name` 列：\n\n    ```sql\n    SELECT * FROM information_schema.partitions WHERE tidb_placement_policy_name IS NOT NULL;\n    ```\n\n- 所有绑定放置策略的对象都是异步调度的。要查看放置策略的调度进度，可以使用 [`SHOW PLACEMENT`](/sql-statements/sql-statement-show-placement.md) 语句。\n\n    ```sql\n    SHOW PLACEMENT;\n    ```\n\n### 修改放置策略\n\n要修改放置策略，可以使用 [`ALTER PLACEMENT POLICY`](/sql-statements/sql-statement-alter-placement-policy.md) 语句。该修改将应用于所有绑定了此放置策略的对象。\n\n```sql\nALTER PLACEMENT POLICY myplacementpolicy FOLLOWERS=4;\n```\n\n在该语句中，`FOLLOWERS=4` 选项代表数据有 5 个副本，包括 4 个 follower 和 1 个 leader。更多可配置的放置选项和对应的含义，请参考[放置选项](#放置选项参考)。\n\n### 删除放置策略\n\n要删除没有绑定任何表或分区的放置策略，可以使用 [`DROP PLACEMENT POLICY`](/sql-statements/sql-statement-drop-placement-policy.md) 语句：\n\n```sql\nDROP PLACEMENT POLICY myplacementpolicy;\n```\n\n## 放置选项参考\n\n在创建和修改放置策略时，你可以按需配置放置选项。\n\n> **注意：**\n>\n> `PRIMARY_REGION`、`REGIONS` 和 `SCHEDULE` 选项不可与 `CONSTRAINTS` 选项同时指定，否则会报错。\n\n### 常规放置选项\n\n常规放置选项可以满足数据放置的基本需求。\n\n| 选项名                | 描述                                                                                    |\n|----------------------------|------------------------------------------------------------------------------------------------|\n| `PRIMARY_REGION`           | Raft leader 被放置在有 `region` 标签的节点上，且这些 `region` 标签匹配本选项的值。     |\n| `REGIONS`                  | Raft followers 被放置在有 `region` 标签的节点上，且这些 `region` 标签匹配本选项的值。 |\n| `SCHEDULE`                 |  用于调度 follower 放置位置的策略。可选值为 `EVEN`（默认值）或 `MAJORITY_IN_PRIMARY`。 |\n| `FOLLOWERS`                |  Follower 的数量。例如 `FOLLOWERS=2` 表示数据有 3 个副本（2 个 follower 和 1 个 leader）。 |\n\n### 高级放置选项\n\n高级配置选项可以更灵活地放置数据，满足复杂的场景需求，但其配置方法相对常规配置选项更复杂一些，需要你对集群拓扑和 TiDB 数据分片有深入的了解。\n\n| 选项名                | 描述                                                                                    |\n|----------------------------|------------------------------------------------------------------------------------------------|\n| `CONSTRAINTS`              | 适用于所有角色 (role) 的约束列表。例如，`CONSTRAINTS=\"[+disk=ssd]\"`。|\n| `LEADER_CONSTRAINTS`       | 仅适用于 leader 的约束列表。                                         |\n| `FOLLOWER_CONSTRAINTS`     | 仅适用于 follower 的约束列表。                                           |\n| `LEARNER_CONSTRAINTS`      | 仅适用于 learner 的约束列表。                                           |\n| `LEARNERS`                 | 指定 learner 的数量。     |\n| `SURVIVAL_PREFERENCE`      | 指定按 label 容灾等级的优先级放置副本。例如 `SURVIVAL_PREFERENCE=\"[region, zone, host]\"`。    |\n\n### CONSTRAINTS 格式\n\n`CONSTRAINTS`、`FOLLOWER_CONSTRAINTS`、`LEARNER_CONSTRAINTS` 放置选项支持以下两种配置格式:\n\n| CONSTRAINTS 格式 | 描述 |\n|----------------------------|-----------------------------------------------------------------------------------------------------------|\n| 列表格式  | 如果指定的约束适用于所有副本，可以使用键值对列表格式。键以 `+` 或 `-` 开头。例如：<br/> <ul><li>`[+region=us-east-1]` 表示放置数据在 `region` 标签为 `us-east-1` 的节点上。</li><li>`[+region=us-east-1,-type=fault]` 表示放置数据在 `region` 标签为 `us-east-1` 且 `type` 标签不为 `fault` 的节点上。</li></ul><br/>  |\n| 字典格式 | 如果需要为不同的约束指定不同数量的副本，可以使用字典格式。例如：<br/> <ul><li>`FOLLOWER_CONSTRAINTS=\"{+region=us-east-1: 1,+region=us-east-2: 1,+region=us-west-1: 1}\";` 表示 1 个 follower 位于 `us-east-1`，1 个 follower 位于 `us-east-2`，1 个 follower 位于 `us-west-1`。</li><li>`FOLLOWER_CONSTRAINTS='{\"+region=us-east-1,+type=scale-node\": 1,\"+region=us-west-1\": 1}';` 表示 1 个 follower 位于 `us-east-1` 区域中有标签 `type` 为 `scale-node` 的节点上，1 个 follower 位于 `us-west-1`。</li></ul>字典格式支持以 `+` 或 `-` 开头的键，并支持配置特殊的 `#evict-leader` 属性。例如，`FOLLOWER_CONSTRAINTS='{\"+region=us-east-1\":1, \"+region=us-east-2\": 2, \"+region=us-west-1,#evict-leader\": 1}'` 表示当进行容灾时，`us-west-1` 上尽可能驱逐当选的 leader。|\n\n> **注意：**\n>\n> - `LEADER_CONSTRAINTS` 放置选项只支持列表格式。\n> - 字典和列表格式都基于 YAML 解析，但 YAML 语法有时不能被正常解析。例如 YAML 会把 `\"{+region=east:1,+region=west:2}\"`（`:` 后无空格）错误地解析成 `'{\"+region=east:1\": null, \"+region=west:2\": null}'`，不符合预期。但 `\"{+region=east: 1,+region=west: 2}\"`（`:` 后有空格）能被正确解析成 `'{\"+region=east\": 1, \"+region=west\": 2}'`。因此建议 `:` 后加上空格。\n>\n\n## 基础示例\n\n### 为集群指定全局的副本数\n\n集群初始化后，副本数默认值为 `3`。集群如需更多的副本数，可使用配置策略调大该值，应用到集群级别，可以使用 [`ALTER RANGE`](/sql-statements/sql-statement-alter-range.md)。示例如下：\n\n```sql\nCREATE PLACEMENT POLICY five_replicas FOLLOWERS=4;\nALTER RANGE global PLACEMENT POLICY five_replicas;\n```\n\n注意，TiDB 默认 leader 个数是 1。因此，5 个副本为 4 个 follower 和 1 个 leader。\n\n### 为数据库指定默认的放置策略\n\n你可以为某个数据库指定默认的放置策略，类似于为数据库设置默认字符集或排序规则。如果数据库中的表或分区没有单独指定其他放置策略，就会使用数据库上指定的放置策略。示例如下：\n\n```sql\nCREATE PLACEMENT POLICY p1 PRIMARY_REGION=\"us-east-1\" REGIONS=\"us-east-1,us-east-2\";  -- 创建放置策略\n\nCREATE PLACEMENT POLICY p2 FOLLOWERS=4;\n\nCREATE PLACEMENT POLICY p3 FOLLOWERS=2;\n\nCREATE TABLE t1 (a INT);  -- 创建表 t1，且未指定放置策略。\n\nALTER DATABASE test PLACEMENT POLICY=p2;  -- 更改数据库默认的放置策略为 p2，但更改不影响已有的表 t1。\n\nCREATE TABLE t2 (a INT);  -- 创建表 t2，默认的放置策略 p2 在 t2 上生效。\n\nCREATE TABLE t3 (a INT) PLACEMENT POLICY=p1;  -- 创建表 t3。因为语句中已经指定了其他放置策略，默认的 p2 策略在 t3 上不生效。\n\nALTER DATABASE test PLACEMENT POLICY=p3;  -- 再次更改数据库默认的放置策略，此更改不影响已有的表。\n\nCREATE TABLE t4 (a INT);  -- 创建表 t4，默认的放置策略 p3 在 t4 生效。\n\nALTER PLACEMENT POLICY p3 FOLLOWERS=3; -- 绑定策略 p3 的表，也就是 t4，会采用 FOLLOWERS=3。\n```\n\n注意分区与表之间的继承和这里的继承不同。改变表的默认放置策略，也会让分区应用新的策略。但是只有建表时没有指定放置策略，表才会从数据库继承放置策略，且之后再改变数据库的默认放置策略也不影响已经继承的表。\n\n### 为表指定放置策略\n\n你可以为某个表指定默认的放置策略。示例如下：\n\n```sql\nCREATE PLACEMENT POLICY five_replicas FOLLOWERS=4;\n\nCREATE TABLE t (a INT) PLACEMENT POLICY=five_replicas;  -- 创建表 t。绑定放置策略为 five_replicas。\n\nALTER TABLE t PLACEMENT POLICY=default; -- 删除表 t 已绑定的放置策略 five_replicas，重置为默认的放置策略。\n```\n\n### 为分区表指定放置策略\n\n你还可以给表分区指定放置策略。示例如下：\n\n```sql\nCREATE PLACEMENT POLICY storageforhistorydata CONSTRAINTS=\"[+node=history]\";\nCREATE PLACEMENT POLICY storagefornewdata CONSTRAINTS=\"[+node=new]\";\nCREATE PLACEMENT POLICY companystandardpolicy CONSTRAINTS=\"\";\n\nCREATE TABLE t1 (id INT, name VARCHAR(50), purchased DATE, UNIQUE INDEX idx(id) GLOBAL)\nPLACEMENT POLICY=companystandardpolicy\nPARTITION BY RANGE( YEAR(purchased) ) (\n  PARTITION p0 VALUES LESS THAN (2000) PLACEMENT POLICY=storageforhistorydata,\n  PARTITION p1 VALUES LESS THAN (2005),\n  PARTITION p2 VALUES LESS THAN (2010),\n  PARTITION p3 VALUES LESS THAN (2015),\n  PARTITION p4 VALUES LESS THAN MAXVALUE PLACEMENT POLICY=storagefornewdata\n);\n```\n\n如果没有为表中的某个分区指定任何放置策略，该分区将尝试继承表上可能存在的策略。如果该表有[全局索引](/partitioned-table.md#全局索引)，索引将应用与该表相同的放置策略。在上面示例中：\n\n- `p0` 分区将会应用 `storageforhistorydata` 策略\n- `p4` 分区将会应用 `storagefornewdata` 策略\n- `p1`、`p2`、`p3` 分区将会应用表 `t1` 的放置策略 `companystandardpolicy`\n- 全局索引 `idx` 将应用与表 `t1` 相同的 `companystandardpolicy` 放置策略\n- 如果没有为表 `t1` 指定放置策略，`p1`、`p2` 和 `p3` 分区以及全局索引 `idx` 将继承数据库默认策略或全局默认策略\n\n给分区绑定放置策略后，你可以更改指定分区的放置策略。示例如下：\n\n```sql\nALTER TABLE t1 PARTITION p1 PLACEMENT POLICY=storageforhistorydata;\n```\n\n## 高可用场景示例\n\n假设集群的拓扑结构如下，集群的 TiKV 节点分布在 3 个 `region`（区域），每个 `region` 有 3 个可用的 `zone` （可用区）：\n\n```sql\nSELECT store_id,address,label from INFORMATION_SCHEMA.TIKV_STORE_STATUS;\n+----------+-----------------+--------------------------------------------------------------------------------------------------------------------------+\n| store_id | address         | label                                                                                                                    |\n+----------+-----------------+--------------------------------------------------------------------------------------------------------------------------+\n|        1 | 127.0.0.1:20163 | [{\"key\": \"region\", \"value\": \"us-east-1\"}, {\"key\": \"zone\", \"value\": \"us-east-1a\"}, {\"key\": \"host\", \"value\": \"host1\"}]     |\n|        2 | 127.0.0.1:20162 | [{\"key\": \"region\", \"value\": \"us-east-1\"}, {\"key\": \"zone\", \"value\": \"us-east-1b\"}, {\"key\": \"host\", \"value\": \"host2\"}]     |\n|        3 | 127.0.0.1:20164 | [{\"key\": \"region\", \"value\": \"us-east-1\"}, {\"key\": \"zone\", \"value\": \"us-east-1c\"}, {\"key\": \"host\", \"value\": \"host3\"}]     |\n|        4 | 127.0.0.1:20160 | [{\"key\": \"region\", \"value\": \"us-east-2\"}, {\"key\": \"zone\", \"value\": \"us-east-2a\"}, {\"key\": \"host\", \"value\": \"host4\"}]     |\n|        5 | 127.0.0.1:20161 | [{\"key\": \"region\", \"value\": \"us-east-2\"}, {\"key\": \"zone\", \"value\": \"us-east-2b\"}, {\"key\": \"host\", \"value\": \"host5\"}]     |\n|        6 | 127.0.0.1:20165 | [{\"key\": \"region\", \"value\": \"us-east-2\"}, {\"key\": \"zone\", \"value\": \"us-east-2c\"}, {\"key\": \"host\", \"value\": \"host6\"}]     |\n|        7 | 127.0.0.1:20166 | [{\"key\": \"region\", \"value\": \"us-west-1\"}, {\"key\": \"zone\", \"value\": \"us-west-1a\"}, {\"key\": \"host\", \"value\": \"host7\"}]     |\n|        8 | 127.0.0.1:20167 | [{\"key\": \"region\", \"value\": \"us-west-1\"}, {\"key\": \"zone\", \"value\": \"us-west-1b\"}, {\"key\": \"host\", \"value\": \"host8\"}]     |\n|        9 | 127.0.0.1:20168 | [{\"key\": \"region\", \"value\": \"us-west-1\"}, {\"key\": \"zone\", \"value\": \"us-west-1c\"}, {\"key\": \"host\", \"value\": \"host9\"}]     |\n+----------+-----------------+--------------------------------------------------------------------------------------------------------------------------+\n\n```\n\n### 指定生存偏好\n\n如果你不特别在意数据的具体分布，只希望能满足容灾生存要求，可以使用 `SURVIVAL_PREFERENCES` 选项设置数据的生存能力偏好。\n\n在上面的例子中，TiDB 集群分布在 3 个 `region`，且每个区域有 3 个 `zone`。在为该集群创建放置策略时，假设 `SURVIVAL_PREFERENCES` 的设置如下：\n\n``` sql\nCREATE PLACEMENT POLICY multiaz SURVIVAL_PREFERENCES=\"[region, zone, host]\";\nCREATE PLACEMENT POLICY singleaz CONSTRAINTS=\"[+region=us-east-1]\" SURVIVAL_PREFERENCES=\"[zone]\";\n```\n\n创建好放置策略后，你可以按需将放置策略绑定到对应的表上：\n\n- 对于绑定了 `multiaz` 放置策略的表，数据将以 3 副本的形式放置在不同的 `region` 里，优先满足跨 `region` 级别的生存目标，再满足跨 `zone` 级别的生存目标，最后再满足跨 `host` 级别的生存目标。\n- 对于绑定了 `singleaz` 放置策略的表，数据会优先以 3 副本的形式全部放置在 `us-east-1` 这个 `region` 里，再满足跨 `zone` 级别的数据隔离的生存目标。\n\n> **注意：**\n>\n> `SURVIVAL_PREFERENCES` 和 PD 中的 `location-labels` 是等价的，更多信息可以参考[通过拓扑 label 进行副本调度](/schedule-replicas-by-topology-labels.md)。\n\n### 指定集群多数据中心 5 副本 2:2:1 分布\n\n如需特定的数据分布（如 5 副本 2:2:1 分布），可以配置[字典格式](#constraints-格式)的 `CONSTRAINTS` 为不同的约束指定不同数量的副本：\n\n```sql\nCREATE PLACEMENT POLICY `deploy221` CONSTRAINTS='{\"+region=us-east-1\":2, \"+region=us-east-2\": 2, \"+region=us-west-1\": 1}';\n\nALTER RANGE global PLACEMENT POLICY = \"deploy221\";\n\nSHOW PLACEMENT;\n+-------------------+---------------------------------------------------------------------------------------------+------------------+\n| Target            | Placement                                                                                   | Scheduling_State |\n+-------------------+---------------------------------------------------------------------------------------------+------------------+\n| POLICY deploy221  | CONSTRAINTS=\"{\\\"+region=us-east-1\\\":2, \\\"+region=us-east-2\\\": 2, \\\"+region=us-west-1\\\": 1}\" | NULL             |\n| RANGE TiDB_GLOBAL | CONSTRAINTS=\"{\\\"+region=us-east-1\\\":2, \\\"+region=us-east-2\\\": 2, \\\"+region=us-west-1\\\": 1}\" | SCHEDULED        |\n+-------------------+---------------------------------------------------------------------------------------------+------------------+\n```\n\n通过为集群全局设置 `deploy221` 放置策略后，TiDB 会根据该策略来分布数据：`us-east-1` 区域放置两个副本，`us-east-2` 区域放置两个副本，`us-west-1` 区域放置一个副本。\n\n### 指定 Leader/Follower 分布\n\n你可以通过 Constraints 或 PRIMARY_REGION 指定特殊的 Leader/Follower 的分布。\n\n#### 使用 Constraints 指定\n\n如果你对 Raft Leader 的分布节点有要求，可以使用如下语句指定：\n\n```sql\nCREATE PLACEMENT POLICY deploy221_primary_east1 LEADER_CONSTRAINTS=\"[+region=us-east-1]\" FOLLOWER_CONSTRAINTS='{\"+region=us-east-1\": 1, \"+region=us-east-2\": 2, \"+region=us-west-1\": 1}';\n```\n\n该放置策略创建好并绑定到所需的数据后，这些数据的 Raft Leader 副本将会放置在 `LEADER_CONSTRAINTS` 选项指定的 `us-east-1` 区域中，其他副本将会放置在`FOLLOWER_CONSTRAINTS` 选项指定的区域。需要注意的是，如果集群发生故障，比如 Leader 所在区域 `us-east-1` 的节点宕机，这时候即使其他区域设置的都是 `FOLLOWER_CONSTRAINTS`, 也会从中选举出一个新的 Leader，也就是说保证服务可用的优先级是最高的。\n\n在 `us-east-1` 区域故障发生时，如果希望新的 Leader 不要放置在 `us-west-1`，可以配置特殊的 `evict-leader` 属性，驱逐上面新的 Leader:\n\n```sql\nCREATE PLACEMENT POLICY deploy221_primary_east1 LEADER_CONSTRAINTS=\"[+region=us-east-1]\" FOLLOWER_CONSTRAINTS='{\"+region=us-east-1\": 1, \"+region=us-east-2\": 2, \"+region=us-west-1,#evict-leader\": 1}';\n```\n\n#### 使用 PRIMARY_REGION 指定\n\n如果你的集群拓扑配置了 `region` label，你还可以使用 `PRIMARY_REGION` 和 `REGIONS` 选项来指定 follower 的放置策略：\n\n```sql\nCREATE PLACEMENT POLICY eastandwest PRIMARY_REGION=\"us-east-1\" REGIONS=\"us-east-1,us-east-2,us-west-1\" SCHEDULE=\"MAJORITY_IN_PRIMARY\" FOLLOWERS=4;\nCREATE TABLE t1 (a INT) PLACEMENT POLICY=eastandwest;\n```\n\n- `PRIMARY_REGION` 为 Leader 分布的区域，只能指定一个。\n- `SCHEDULE` 选项指定 TiDB 如何平衡 follower 的分布。\n    - 该选项默认的 `EVEN` 调度规则确保 follower 在所有区域内分布平衡。\n    - 如需保证在 `PRIMARY_REGION`（即 `us-east-1`）内放置足够多的 follower 副本，你可以使用 `MAJORITY_IN_PRIMARY` 调度规则来使该区域的 follower 达到指定数量。该调度牺牲一些可用性来换取更低的事务延迟。如果主区域宕机，`MAJORITY_IN_PRIMARY` 无法提供自动故障转移。\n\n## 数据隔离场景示例\n\n以下示例在创建放置策略时，设置了一个约束，要求数据必须放置在配置了指定的 `app` 标签的 TiKV 节点：\n\n```sql\nCREATE PLACEMENT POLICY app_order CONSTRAINTS=\"[+app=order]\";\nCREATE PLACEMENT POLICY app_list CONSTRAINTS=\"[+app=list_collection]\";\nCREATE TABLE order (id INT, name VARCHAR(50), purchased DATE)\nPLACEMENT POLICY=app_order\nCREATE TABLE list (id INT, name VARCHAR(50), purchased DATE)\nPLACEMENT POLICY=app_list\n```\n\n在该示例中，约束是通过列表格式 (`[+app=order]`) 指定的。你也可以使用字典格式指定，例如 (`{+app=order: 3}`)。\n\n执行示例中的语句后，TiDB 会将 `app_order` 的数据放置在配置了 `app` 标签为 `order` 的 TiKV 节点上，将 `app_list` 的数据放置在配置了 `app` 标签为 `list_collection` 的 TiKV 节点上，从而在存储上达到了物理隔离的效果。\n\n## 兼容性说明\n\n### 功能兼容性\n\n- 临时表不支持放置策略。\n- 放置策略仅保证静态数据被放置在正确的 TiKV 节点上。该策略不保证传输中的数据（通过用户查询或内部操作）只出现在特定区域内。\n- 设置数据的 TiFlash 副本需要通过[构建 TiFlash 副本](/tiflash/create-tiflash-replicas.md)的方式创建，不能使用该特性。\n- 设置 `PRIMARY_REGION` 和 `REGIONS` 时允许存在语法糖。但在未来版本中，我们计划为 `PRIMARY_RACK`、`PRIMARY_ZONE` 和 `PRIMARY_HOST` 添加变体支持，见 [issue #18030](https://github.com/pingcap/tidb/issues/18030)。\n\n### 工具兼容性\n\n| 工具名称 | 最低兼容版本 | 说明 |\n| --- | --- | --- |\n| Backup & Restore (BR) | 6.0 | BR 在 v6.0 之前不支持放置策略的备份与恢复，请参见[恢复 Placement Rule 到集群时为什么会报错？](/faq/backup-and-restore-faq.md#恢复-placement-rule-到集群时为什么会报错) |\n| TiDB Lightning | 暂时不兼容 | 导入包含放置策略的数据时会报错 |\n| TiCDC | 6.0 | 忽略放置策略，不同步策略到下游集群 |\n"
        },
        {
          "name": "post-installation-check.md",
          "type": "blob",
          "size": 5.630859375,
          "content": "---\ntitle: 验证集群运行状态\nsummary: 介绍如何验证集群运行状态。\naliases: ['/docs-cn/dev/post-installation-check/']\n---\n\n# 验证集群运行状态\n\n在部署完一套 TiDB 集群后，需要检查集群是否正常运行。本文介绍如何通过 TiUP 命令、[TiDB Dashboard](/dashboard/dashboard-intro.md) 和 Grafana 检查集群状态，以及如何登录 TiDB 数据库执行简单的 SQL 操作。\n\n## 通过 TiUP 检查集群状态\n\n检查集群状态的命令是 `tiup cluster display <cluster-name>`，例如：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display tidb-test\n```\n\n预期结果输出：各节点 Status 状态信息为 `Up` 说明集群状态正常。\n\n## 通过 TiDB Dashboard 和 Grafana 检查集群状态\n\n本节介绍如何通过 [TiDB Dashboard](/dashboard/dashboard-intro.md) 和 Grafana 检查集群状态。\n\n### 查看 TiDB Dashboard 检查 TiDB 集群状态\n\n1. 通过 `{pd-ip}:{pd-port}/dashboard` 登录 TiDB Dashboard，登录用户和口令为 TiDB 数据库 `root` 用户和口令。如果你修改过数据库的 `root` 密码，则以修改后的密码为准，默认密码为空。\n\n    ![TiDB-Dashboard](/media/tiup/tidb-dashboard.png)\n\n2. 主页面显示 TiDB 集群中节点信息\n\n    ![TiDB-Dashboard-status](/media/tiup/tidb-dashboard-status.png)\n\n### 查看 Grafana 监控 Overview 页面检查 TiDB 集群状态\n\n- 通过 `{Grafana-ip}:3000` 登录 Grafana 监控，默认用户名及密码为 `admin`/`admin`。\n\n- 点击 **Overview** 监控页面检查 TiDB 端口和负载监控信息。\n\n    ![Grafana-overview](/media/tiup/grafana-overview.png)\n\n## 登录数据库执行简单 DML/DDL 操作和查询 SQL 语句\n\n> **注意：**\n>\n> 登录数据库前，你需要安装 MySQL 客户端。\n\n执行以下命令登录数据库：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\nmysql -u root -h ${tidb_server_host_IP_address} -P 4000\n```\n\n其中，`${tidb_server_host_IP_address}` 是在[初始化集群拓扑文件](/production-deployment-using-tiup.md#第-3-步初始化集群拓扑文件)时为 `tidb_servers` 配置的 IP 地址之一，例如 `10.0.1.7`。\n\n输出下列信息表示登录成功：\n\n```sql\nWelcome to the MySQL monitor.  Commands end with ; or \\g.\nYour MySQL connection id is 3\nServer version: 8.0.11-TiDB-v8.5.0 TiDB Server (Apache License 2.0) Community Edition, MySQL 8.0 compatible\n\nCopyright (c) 2000, 2023, Oracle and/or its affiliates. All rights reserved.\n\n\nOracle is a registered trademark of Oracle Corporation and/or its\naffiliates. Other names may be trademarks of their respective\nowners.\n\nType 'help;' or '\\h' for help. Type '\\c' to clear the current input statement.\n```\n\n### 数据库操作\n\n+ 检查 TiDB 版本\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select tidb_version()\\G\n    ```\n\n    预期结果输出：\n\n    ```sql\n    *************************** 1. row ***************************\n    tidb_version(): Release Version: v5.0.0\n    Edition: Community\n    Git Commit Hash: 689a6b6439ae7835947fcaccf329a3fc303986cb\n    Git Branch: HEAD\n    UTC Build Time: 2020-05-28 11:09:45\n    GoVersion: go1.13.4\n    Race Enabled: false\n    TiKV Min Version: v3.0.0-60965b006877ca7234adaced7890d7b029ed1306\n    Check Table Before Drop: false\n    1 row in set (0.00 sec)\n    ```\n\n+ 创建 PingCAP database\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    create database pingcap;\n    ```\n\n    ```sql\n    Query OK, 0 rows affected (0.10 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    use pingcap;\n    ```\n\n    预期输出\n\n    ```sql\n    Database changed\n    ```\n\n+ 创建 `tab_tidb` 表\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    CREATE TABLE `tab_tidb` (\n    `id` int NOT NULL AUTO_INCREMENT,\n    `name` varchar(20) NOT NULL DEFAULT '',\n    `age` int NOT NULL DEFAULT 0,\n    `version` varchar(20) NOT NULL DEFAULT '',\n    PRIMARY KEY (`id`),\n    KEY `idx_age` (`age`));\n    ```\n\n    预期输出\n\n    ```sql\n    Query OK, 0 rows affected (0.11 sec)\n    ```\n\n+ 插入数据\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    insert into `tab_tidb` values (1,'TiDB',5,'TiDB-v5.0.0');\n    ```\n\n    预期输出\n\n    ```sql\n    Query OK, 1 row affected (0.03 sec)\n    ```\n\n+ 查看 `tab_tidb` 结果\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from tab_tidb;\n    ```\n\n    预期输出\n\n    ```sql\n    +----+------+-----+-------------+\n    | id | name | age | version     |\n    +----+------+-----+-------------+\n    |  1 | TiDB |   5 | TiDB-v5.0.0 |\n    +----+------+-----+-------------+\n    1 row in set (0.00 sec)\n    ```\n\n+ 查看 TiKV store 状态、`store_id`、存储情况以及启动时间\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select STORE_ID,ADDRESS,STORE_STATE,STORE_STATE_NAME,CAPACITY,AVAILABLE,UPTIME from INFORMATION_SCHEMA.TIKV_STORE_STATUS;\n    ```\n\n    预期输出\n\n    ```sql\n    +----------+--------------------+-------------+------------------+----------+-----------+--------------------+\n    | STORE_ID | ADDRESS            | STORE_STATE | STORE_STATE_NAME | CAPACITY | AVAILABLE | UPTIME             |\n    +----------+--------------------+-------------+------------------+----------+-----------+--------------------+\n    |        1 | 10.0.1.1:20160 |           0 | Up               | 49.98GiB | 46.3GiB   | 5h21m52.474864026s |\n    |        4 | 10.0.1.2:20160 |           0 | Up               | 49.98GiB | 46.32GiB  | 5h21m52.522669177s |\n    |        5 | 10.0.1.3:20160 |           0 | Up               | 49.98GiB | 45.44GiB  | 5h21m52.713660541s |\n    +----------+--------------------+-------------+------------------+----------+-----------+--------------------+\n    3 rows in set (0.00 sec)\n    ```\n\n+ 退出\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    exit\n    ```\n\n    预期输出\n\n    ```sql\n    Bye\n    ```\n"
        },
        {
          "name": "predicate-push-down.md",
          "type": "blob",
          "size": 9.8720703125,
          "content": "---\ntitle: 谓词下推\naliases: ['/docs-cn/dev/predicate-push-down/']\nsummary: TiDB 逻辑优化规则中的谓词下推旨在尽早完成数据过滤，减少数据传输或计算的开销。谓词下推适用于将过滤表达式计算下推到数据源，如示例 1、2、3。但对于存储层不支持的谓词、外连接中的谓词和包含用户变量的谓词则不能下推。\n---\n\n# 谓词下推\n\n本文档介绍 TiDB 逻辑优化规则中的谓词下推规则，旨在让读者对谓词下推形成理解，并了解常见的谓词下推适用及不适用的场景。\n\n谓词下推将查询语句中的过滤表达式计算尽可能下推到距离数据源最近的地方，以尽早完成数据的过滤，进而显著地减少数据传输或计算的开销。\n\n## 示例\n\n以下通过一些例子对谓词下推优化进行说明，其中示例 1、2、3 为谓词下推适用的案例，示例 4、5、6 为谓词下推不适用的案例。\n\n### 示例 1: 谓词下推到存储层\n\n```sql\ncreate table t(id int primary key, a int);\nexplain select * from t where a < 1;\n+-------------------------+----------+-----------+---------------+--------------------------------+\n| id                      | estRows  | task      | access object | operator info                  |\n+-------------------------+----------+-----------+---------------+--------------------------------+\n| TableReader_7           | 3323.33  | root      |               | data:Selection_6               |\n| └─Selection_6           | 3323.33  | cop[tikv] |               | lt(test.t.a, 1)                |\n|   └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo |\n+-------------------------+----------+-----------+---------------+--------------------------------+\n3 rows in set (0.00 sec)\n```\n\n在该查询中，将谓词 `a < 1` 下推到 TiKV 上对数据进行过滤，可以减少由于网络传输带来的开销。\n\n### 示例 2: 谓词下推到存储层\n\n```sql\ncreate table t(id int primary key, a int not null);\nexplain select * from t where a < substring('123', 1, 1);\n+-------------------------+----------+-----------+---------------+--------------------------------+\n| id                      | estRows  | task      | access object | operator info                  |\n+-------------------------+----------+-----------+---------------+--------------------------------+\n| TableReader_7           | 3323.33  | root      |               | data:Selection_6               |\n| └─Selection_6           | 3323.33  | cop[tikv] |               | lt(test.t.a, 1)                |\n|   └─TableFullScan_5     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo |\n+-------------------------+----------+-----------+---------------+--------------------------------+\n```\n\n该查询与示例 1 中的查询生成了完全一样的执行计划，这是因为谓词 `a < substring('123', 1, 1)` 的 `substring` 的入参均为常量，因此可以提前计算，进而简化得到等价的谓词 `a < 1`。进一步的，可以将 `a < 1` 下推至 TiKV 上。\n\n### 示例 3: 谓词下推到 join 下方\n\n```sql\ncreate table t(id int primary key, a int not null);\ncreate table s(id int primary key, a int not null);\nexplain select * from t join s on t.a = s.a where t.a < 1;\n+------------------------------+----------+-----------+---------------+--------------------------------------------+\n| id                           | estRows  | task      | access object | operator info                              |\n+------------------------------+----------+-----------+---------------+--------------------------------------------+\n| HashJoin_8                   | 4154.17  | root      |               | inner join, equal:[eq(test.t.a, test.s.a)] |\n| ├─TableReader_15(Build)      | 3323.33  | root      |               | data:Selection_14                          |\n| │ └─Selection_14             | 3323.33  | cop[tikv] |               | lt(test.s.a, 1)                            |\n| │   └─TableFullScan_13       | 10000.00 | cop[tikv] | table:s       | keep order:false, stats:pseudo             |\n| └─TableReader_12(Probe)      | 3323.33  | root      |               | data:Selection_11                          |\n|   └─Selection_11             | 3323.33  | cop[tikv] |               | lt(test.t.a, 1)                            |\n|     └─TableFullScan_10       | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo             |\n+------------------------------+----------+-----------+---------------+--------------------------------------------+\n7 rows in set (0.00 sec)\n```\n\n在该查询中，将谓词 `t.a < 1` 下推到 join 前进行过滤，可以减少 join 时的计算开销。\n\n此外，这条 SQL 执行的是内连接，且 `ON` 条件是 `t.a = s.a`，可以由 `t.a < 1` 推导出谓词 `s.a < 1`，并将其下推至 join 运算前对 `s` 表进行过滤，可以进一步减少 join 时的计算开销。\n\n### 示例 4: 存储层不支持的谓词无法下推\n\n```sql\ncreate table t(id int primary key, a varchar(10) not null);\ndesc select * from t where truncate(a, \" \") = '1';\n+-------------------------+----------+-----------+---------------+---------------------------------------------------+\n| id                      | estRows  | task      | access object | operator info                                     |\n+-------------------------+----------+-----------+---------------+---------------------------------------------------+\n| Selection_5             | 8000.00  | root      |               | eq(truncate(cast(test.t.a, double BINARY), 0), 1) |\n| └─TableReader_7         | 10000.00 | root      |               | data:TableFullScan_6                              |\n|   └─TableFullScan_6     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                    |\n+-------------------------+----------+-----------+---------------+---------------------------------------------------+\n```\n\n在该查询中，存在谓词 `truncate(a, \" \") = '1'`。\n\n从 explain 结果中可以看到，该谓词没有被下推到 TiKV 上进行计算，这是因为 TiKV coprocessor 中没有对 `truncate` 内置函数进行支持，因此无法将其下推到 TiKV 上。\n\n### 示例 5: 外连接中内表上的谓词不能下推\n\n```sql\ncreate table t(id int primary key, a int not null);\ncreate table s(id int primary key, a int not null);\nexplain select * from t left join s on t.a = s.a where s.a is null;\n+-------------------------------+----------+-----------+---------------+-------------------------------------------------+\n| id                            | estRows  | task      | access object | operator info                                   |\n+-------------------------------+----------+-----------+---------------+-------------------------------------------------+\n| Selection_7                   | 10000.00 | root      |               | isnull(test.s.a)                                |\n| └─HashJoin_8                  | 12500.00 | root      |               | left outer join, equal:[eq(test.t.a, test.s.a)] |\n|   ├─TableReader_13(Build)     | 10000.00 | root      |               | data:TableFullScan_12                           |\n|   │ └─TableFullScan_12        | 10000.00 | cop[tikv] | table:s       | keep order:false, stats:pseudo                  |\n|   └─TableReader_11(Probe)     | 10000.00 | root      |               | data:TableFullScan_10                           |\n|     └─TableFullScan_10        | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                  |\n+-------------------------------+----------+-----------+---------------+-------------------------------------------------+\n6 rows in set (0.00 sec)\n```\n\n在该查询中，内表 s 上存在谓词 `s.a is null`。\n\n从 explain 中可以看到，该谓词没有被下推到 join 前进行计算，这是因为外连接在不满足 on 条件时会对内表填充 NULL，而在该查询中 `s.a is null` 用来对 join 后的结果进行过滤，如果将其下推到 join 前在内表上进行过滤，则下推前后不等价，因此不可进行下推。\n\n### 示例 6: 谓词中包含用户变量时不能下推\n\n```sql\ncreate table t(id int primary key, a char);\nset @a = 1;\nexplain select * from t where a < @a;\n+-------------------------+----------+-----------+---------------+--------------------------------+\n| id                      | estRows  | task      | access object | operator info                  |\n+-------------------------+----------+-----------+---------------+--------------------------------+\n| Selection_5             | 8000.00  | root      |               | lt(test.t.a, getvar(\"a\"))      |\n| └─TableReader_7         | 10000.00 | root      |               | data:TableFullScan_6           |\n|   └─TableFullScan_6     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo |\n+-------------------------+----------+-----------+---------------+--------------------------------+\n3 rows in set (0.00 sec)\n```\n\n在该查询中，表 t 上存在谓词 `a < @a`，其中 `@a` 为值为 1 的用户变量。\n\n从 explain 中可以看到，该谓词没有像示例 2 中一样，将谓词简化为 `a < 1` 并下推到 TiKV 上进行计算。这是因为，用户变量 `@a` 的值可能会某些场景下在查询过程中发生改变，且 TiKV 对于用户变量 `@a` 的值不可知，因此 TiDB 不会将 `@a` 替换为 1，且不会下推至 TiKV 上进行计算。\n\n一个帮助理解的例子如下：\n\n```sql\ncreate table t(id int primary key, a int);\ninsert into t values(1, 1), (2,2);\nset @a = 1;\nselect id, a, @a:=@a+1 from t where a = @a;\n+----+------+----------+\n| id | a    | @a:=@a+1 |\n+----+------+----------+\n|  1 |    1 | 2        |\n|  2 |    2 | 3        |\n+----+------+----------+\n2 rows in set (0.00 sec)\n```\n\n可以从在该查询中看到，`@a` 的值会在查询过程中发生改变，因此如果将 `a = @a` 替换为 `a = 1` 并下推至 TiKV，则优化前后不等价。\n"
        },
        {
          "name": "privilege-management.md",
          "type": "blob",
          "size": 20.390625,
          "content": "---\ntitle: 权限管理\naliases: ['/docs-cn/dev/privilege-management/','/docs-cn/dev/reference/security/privilege-system/']\nsummary: TiDB 支持 MySQL 5.7 和 MySQL 8.0 的权限管理系统。权限相关操作包括授予权限、收回权限、查看用户权限和动态权限。权限系统的实现包括授权表和连接验证。权限生效时机是在 TiDB 启动时加载到内存，并且可以手动刷新。\n---\n\n# 权限管理\n\nTiDB 支持 MySQL 5.7 的权限管理系统，包括 MySQL 的语法和权限类型。同时 TiDB 还支持 MySQL 8.0 的以下特性：\n\n* 从 TiDB 3.0 开始，支持 SQL 角色。\n* 从 TiDB 5.1 开始，支持动态权限。\n\n本文档主要介绍 TiDB 权限相关操作、各项操作需要的权限以及权限系统的实现。\n\n## 权限相关操作\n\n### 授予权限\n\n[`GRANT`](/sql-statements/sql-statement-grant-privileges.md) 语句用于为 TiDB 中的用户分配权限。\n\n授予 `xxx` 用户对数据库 `test` 的读权限：\n\n```sql\nGRANT SELECT ON test.* TO 'xxx'@'%';\n```\n\n为 `xxx` 用户授予所有数据库，全部权限：\n\n```sql\nGRANT ALL PRIVILEGES ON *.* TO 'xxx'@'%';\n```\n\n默认情况下，如果指定的用户不存在，[`GRANT`](/sql-statements/sql-statement-grant-privileges.md) 语句将报错。该行为受 [SQL 模式](/system-variables.md#sql_mode)中的 `NO_AUTO_CREATE_USER` 控制。\n\n```sql\nSET sql_mode=DEFAULT;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nSELECT @@sql_mode;\n```\n\n```\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n| @@sql_mode                                                                                                                                |\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n| ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION |\n+-------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n```sql\nSELECT * FROM mysql.user WHERE user='idontexist';\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n```sql\nGRANT ALL PRIVILEGES ON test.* TO 'idontexist';\n```\n\n```\nERROR 1105 (HY000): You are not allowed to create a user with GRANT\n```\n\n```sql\nSELECT user,host,authentication_string FROM mysql.user WHERE user='idontexist';\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n在下面的例子中，由于没有将 SQL 模式设置为 `NO_AUTO_CREATE_USER`，用户 `idontexist` 会被自动创建且密码为空。**不推荐**使用这种方式，因为会带来安全风险：如果用户名拼写错误，会导致新用户被创建且密码为空。\n\n```sql\nSET @@sql_mode='ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION';\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nSELECT @@sql_mode;\n```\n\n```\n+-----------------------------------------------------------------------------------------------------------------------+\n| @@sql_mode                                                                                                            |\n+-----------------------------------------------------------------------------------------------------------------------+\n| ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_ENGINE_SUBSTITUTION |\n+-----------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n```sql\nSELECT * FROM mysql.user WHERE user='idontexist';\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n```sql\nGRANT ALL PRIVILEGES ON test.* TO 'idontexist';\n```\n\n```\nQuery OK, 1 row affected (0.05 sec)\n```\n\n```sql\nSELECT user,host,authentication_string FROM mysql.user WHERE user='idontexist';\n```\n\n```\n+------------+------+-----------------------+\n| user       | host | authentication_string |\n+------------+------+-----------------------+\n| idontexist | %    |                       |\n+------------+------+-----------------------+\n1 row in set (0.01 sec)\n```\n\n[`GRANT`](/sql-statements/sql-statement-grant-privileges.md) 还可以模糊匹配地授予用户数据库的权限：\n\n```sql\nGRANT ALL PRIVILEGES ON `te%`.* TO genius;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nSELECT user,host,db FROM mysql.db WHERE user='genius';\n```\n\n```\n+--------|------|-----+\n| user   | host | db  |\n+--------|------|-----+\n| genius | %    | te% |\n+--------|------|-----+\n1 row in set (0.00 sec)\n```\n\n这个例子中通过 `%` 模糊匹配，所有 `te` 开头的数据库，都被授予了权限。\n\n### 收回权限\n\n[`REVOKE`](/sql-statements/sql-statement-revoke-privileges.md) 语句允许系统管理员收回用户的权限。\n\n`REVOKE` 语句的作用与 `GRANT` 相反：\n\n```sql\nREVOKE ALL PRIVILEGES ON `test`.* FROM 'genius'@'localhost';\n```\n\n> **注意：**\n>\n> `REVOKE` 收回权限时只做精确匹配，若找不到记录则报错。而 `GRANT` 授予权限时可以使用模糊匹配。\n\n```sql\nREVOKE ALL PRIVILEGES ON `te%`.* FROM 'genius'@'%';\n```\n\n```\nERROR 1141 (42000): There is no such grant defined for user 'genius' on host '%'\n```\n\n关于模糊匹配和转义，字符串和 identifier：\n\n```sql\nGRANT ALL PRIVILEGES ON `te\\%`.* TO 'genius'@'localhost';\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n上述例子是精确匹配名为 `te%` 的数据库，注意使用 `\\` 转义字符。\n\n以单引号包含的部分，是一个字符串。以反引号包含的部分，是一个 identifier。注意下面的区别：\n\n```sql\nGRANT ALL PRIVILEGES ON 'test'.* TO 'genius'@'localhost';\n```\n\n```\nERROR 1064 (42000): You have an error in your SQL syntax; check the\nmanual that corresponds to your MySQL server version for the right\nsyntax to use near ''test'.* to 'genius'@'localhost'' at line 1\n```\n\n```sql\nGRANT ALL PRIVILEGES ON `test`.* TO 'genius'@'localhost';\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n如果想将一些特殊的关键字做为表名，可以用反引号包含起来。比如：\n\n```sql\nCREATE TABLE `select` (id int);\n```\n\n```\nQuery OK, 0 rows affected (0.27 sec)\n```\n\n### 查看为用户分配的权限\n\n`SHOW GRANTS` 语句可以查看为用户分配了哪些权限。例如：\n\n查看当前用户的权限：\n\n```sql\nSHOW GRANTS;\n```\n\n```\n+-------------------------------------------------------------+\n| Grants for User                                             |\n+-------------------------------------------------------------+\n| GRANT ALL PRIVILEGES ON *.* TO 'root'@'%' WITH GRANT OPTION |\n+-------------------------------------------------------------+\n```\n\n或者：\n\n```sql\nSHOW GRANTS FOR CURRENT_USER();\n```\n\n查看某个特定用户的权限：\n\n```sql\nSHOW GRANTS FOR 'user'@'host';\n```\n\n例如，创建一个用户 `rw_user@192.168.%` 并为其授予 `test.write_table` 表的写权限，和全局读权限。\n\n```sql\nCREATE USER `rw_user`@`192.168.%`;\nGRANT SELECT ON *.* TO `rw_user`@`192.168.%`;\nGRANT INSERT, UPDATE ON `test`.`write_table` TO `rw_user`@`192.168.%`;\n```\n\n查看用户 `rw_user@192.168.%` 的权限。\n\n```sql\nSHOW GRANTS FOR `rw_user`@`192.168.%`;\n```\n\n```\n+------------------------------------------------------------------+\n| Grants for rw_user@192.168.%                                     |\n+------------------------------------------------------------------+\n| GRANT Select ON *.* TO 'rw_user'@'192.168.%'                     |\n| GRANT Insert,Update ON test.write_table TO 'rw_user'@'192.168.%' |\n+------------------------------------------------------------------+\n```\n\n### 动态权限\n\n从 v5.1 开始，TiDB 支持 MySQL 8.0 中的动态权限特性。动态权限用于限制 `SUPER` 权限，实现对某些操作更细粒度的访问。例如，系统管理员可以使用动态权限来创建一个只能执行 `BACKUP` 和 `RESTORE` 操作的用户帐户。\n\n动态权限包括：\n\n* `BACKUP_ADMIN`\n* `RESTORE_ADMIN`\n* `SYSTEM_USER`\n* `SYSTEM_VARIABLES_ADMIN`\n* `ROLE_ADMIN`\n* `CONNECTION_ADMIN`\n* `PLACEMENT_ADMIN` 允许创建、删除和修改放置策略 (placement policy)。\n* `DASHBOARD_CLIENT` 允许登录 TiDB Dashboard。\n* `RESTRICTED_TABLES_ADMIN` 允许在 SEM 打开的情况下查看系统表。\n* `RESTRICTED_STATUS_ADMIN` 允许在 SEM 打开的情况下查看 [`SHOW [GLOBAL|SESSION] STATUS`](/sql-statements/sql-statement-show-status.md) 中的状态变量。\n* `RESTRICTED_VARIABLES_ADMIN` 允许在 SEM 打开的情况下查看所有系统变量。\n* `RESTRICTED_USER_ADMIN` 不允许在 SEM 打开的情况下使用 `SUPER` 用户撤销访问权限。\n* `RESTRICTED_CONNECTION_ADMIN` 允许 KILL 属于 `RESTRICTED_USER_ADMIN` 用户的连接。该权限对 `KILL` 和 `KILL TIDB` 语句生效。\n* `RESTRICTED_REPLICA_WRITER_ADMIN` 允许权限拥有者在 TiDB 集群开启了只读模式的情况下不受影响地执行写入或更新操作，详见 [`tidb_restricted_read_only` 配置项](/system-variables.md#tidb_restricted_read_only-从-v520-版本开始引入)。\n\n若要查看全部的动态权限，请执行 `SHOW PRIVILEGES` 语句。由于用户可使用插件来添加新的权限，因此可分配的权限列表可能因用户的 TiDB 安装情况而异。\n\n## `SUPER` 权限\n\n- 拥有 `SUPER` 权限的用户能完成几乎所有的操作，默认情况下只有 `root` 用户拥有该权限。请谨慎向其它用户授予 `SUPER` 权限。\n- `SUPER` 权限[在 MySQL 8.0 中被认为是过时的](https://dev.mysql.com/doc/refman/8.0/en/privileges-provided.html#dynamic-privileges-migration-from-super)，可以通过[动态权限](#动态权限)替代 `SUPER` 权限进行更细粒度的权限控制。\n\n## TiDB 各操作需要的权限\n\nTiDB 用户目前拥有的权限可以在 `INFORMATION_SCHEMA.USER_PRIVILEGES` 表中查找到。例如：\n\n```sql\nSELECT * FROM INFORMATION_SCHEMA.USER_PRIVILEGES WHERE grantee = \"'root'@'%'\";\n```\n\n```\n+------------+---------------+-------------------------+--------------+\n| GRANTEE    | TABLE_CATALOG | PRIVILEGE_TYPE          | IS_GRANTABLE |\n+------------+---------------+-------------------------+--------------+\n| 'root'@'%' | def           | Select                  | YES          |\n| 'root'@'%' | def           | Insert                  | YES          |\n| 'root'@'%' | def           | Update                  | YES          |\n| 'root'@'%' | def           | Delete                  | YES          |\n| 'root'@'%' | def           | Create                  | YES          |\n| 'root'@'%' | def           | Drop                    | YES          |\n| 'root'@'%' | def           | Process                 | YES          |\n| 'root'@'%' | def           | References              | YES          |\n| 'root'@'%' | def           | Alter                   | YES          |\n| 'root'@'%' | def           | Show Databases          | YES          |\n| 'root'@'%' | def           | Super                   | YES          |\n| 'root'@'%' | def           | Execute                 | YES          |\n| 'root'@'%' | def           | Index                   | YES          |\n| 'root'@'%' | def           | Create User             | YES          |\n| 'root'@'%' | def           | Create Tablespace       | YES          |\n| 'root'@'%' | def           | Trigger                 | YES          |\n| 'root'@'%' | def           | Create View             | YES          |\n| 'root'@'%' | def           | Show View               | YES          |\n| 'root'@'%' | def           | Create Role             | YES          |\n| 'root'@'%' | def           | Drop Role               | YES          |\n| 'root'@'%' | def           | CREATE TEMPORARY TABLES | YES          |\n| 'root'@'%' | def           | LOCK TABLES             | YES          |\n| 'root'@'%' | def           | CREATE ROUTINE          | YES          |\n| 'root'@'%' | def           | ALTER ROUTINE           | YES          |\n| 'root'@'%' | def           | EVENT                   | YES          |\n| 'root'@'%' | def           | SHUTDOWN                | YES          |\n| 'root'@'%' | def           | RELOAD                  | YES          |\n| 'root'@'%' | def           | FILE                    | YES          |\n| 'root'@'%' | def           | CONFIG                  | YES          |\n| 'root'@'%' | def           | REPLICATION CLIENT      | YES          |\n| 'root'@'%' | def           | REPLICATION SLAVE       | YES          |\n+------------+---------------+-------------------------+--------------+\n31 rows in set (0.00 sec)\n```\n\n### ALTER\n\n- 对于所有的 `ALTER` 语句，均需要用户对所操作的表拥有 `ALTER` 权限。\n- 除 `ALTER...DROP` 和 `ALTER...RENAME TO` 外，均需要对所操作表拥有 `INSERT` 和 `CREATE` 权限。\n- 对于 `ALTER...DROP` 语句，需要对表拥有 `DROP` 权限。\n- 对于 `ALTER...RENAME TO` 语句，需要对重命名前的表拥有 `DROP` 权限，对重命名后的表拥有 `CREATE` 和 `INSERT` 权限。\n\n> **注意：**\n>\n> 根据 MySQL 5.7 文档中的说明，对表进行 `ALTER` 操作需要 `INSERT` 和 `CREATE` 权限，但在 MySQL 5.7.25 版本实际情况中，该操作仅需要 `ALTER` 权限。目前，TiDB 中的 `ALTER` 权限与 MySQL 实际行为保持一致。\n\n### BACKUP\n\n需要拥有 `SUPER` 或者 `BACKUP_ADMIN` 权限。\n\n### CANCEL IMPORT JOB\n\n需要 `SUPER` 权限来取消属于其他用户的任务，否则只能取消当前用户创建的任务。\n\n### CREATE DATABASE\n\n需要拥有全局 `CREATE` 权限。\n\n### CREATE INDEX\n\n需要对所操作的表拥有 `INDEX` 权限。\n\n### CREATE TABLE\n\n需要对要创建的表所在的数据库拥有 `CREATE` 权限；若使用 `CREATE TABLE...LIKE...` 需要对相关的表拥有 `SELECT` 权限。\n\n### CREATE VIEW\n\n需要拥有 `CREATE VIEW` 权限。\n\n> **注意：**\n>\n> 如果当前登录用户与创建视图的用户不同，除需要 `CREATE VIEW` 权限外，还需要 `SUPER` 权限。\n\n### DROP DATABASE\n\n需要对数据库拥有 `DROP` 权限。\n\n### DROP INDEX\n\n需要对所操作的表拥有 `INDEX` 权限。\n\n### DROP TABLES\n\n需要对所操作的表拥有 `DROP` 权限。\n\n### IMPORT INTO\n\n需要对目标表拥有 `SELECT`、`UPDATE`、`INSERT`、`DELETE` 和 `ALTER` 权限。如果是导入存储在 TiDB 本地的文件，还需要有 `FILE` 权限。\n\n### LOAD DATA\n\n`LOAD DATA` 需要对所操作的表拥有 `INSERT` 权限。执行 `REPLACE INTO` 语句还需要对所操作的表拥有 `DELETE` 权限。\n\n### TRUNCATE TABLE\n\n需要对所操作的表拥有 `DROP` 权限。\n\n### RENAME TABLE\n\n需要对重命名前的表拥有 `ALTER` 和 `DROP` 权限，对重命名后的表拥有 `CREATE` 和 `INSERT` 权限。\n\n### ANALYZE TABLE\n\n需要对所操作的表拥有 `INSERT` 和 `SELECT` 权限。\n\n### LOCK STATS\n\n需要对所操作的表拥有 `INSERT` 和 `SELECT` 权限。\n\n### UNLOCK STATS\n\n需要对所操作的表拥有 `INSERT` 和 `SELECT` 权限。\n\n### SHOW\n\n`SHOW CREATE TABLE` 需要任意一种权限。\n\n`SHOW CREATE VIEW` 需要 `SHOW VIEW` 权限。\n\n`SHOW GRANTS` 需要拥有对 `mysql` 数据库的 `SELECT` 权限。如果是使用 `SHOW GRANTS` 查看当前用户权限，则不需要任何权限。\n\n`SHOW PROCESSLIST` 需要 `PROCESS` 权限来显示属于其他用户的连接。\n\n`SHOW IMPORT JOB` 需要 `SUPER` 权限来显示属于其他用户的任务，否则只能看到当前用户创建的任务。\n\n`SHOW STATS_LOCKED` 需要拥有 `mysql.stats_table_locked` 表的 `SELECT` 权限。\n\n### CREATE ROLE/USER\n\n`CREATE ROLE` 需要 `CREATE ROLE` 权限。\n\n`CREATE USER` 需要 `CREATE USER` 权限\n\n### DROP ROLE/USER\n\n`DROP ROLE` 需要 `DROP ROLE` 权限。\n\n`DROP USER` 需要 `CREATE USER` 权限\n\n### ALTER USER\n\n`ALTER USER` 需要 `CREATE USER` 权限。\n\n### GRANT\n\n`GRANT` 需要 `GRANT` 权限并且拥有 `GRANT` 所赋予的权限。\n\n如果在 `GRANTS` 语句中创建用户，需要有 `CREATE USER` 权限。\n\n`GRANT ROLE` 操作需要拥有 `SUPER` 或者 `ROLE_ADMIN` 权限。\n\n### REVOKE\n\n`REVOKE` 需要 `GRANT` 权限并且拥有 `REVOKE` 所指定要撤销的权限。\n\n`REVOKE ROLE` 操作需要拥有 `SUPER` 或者 `ROLE_ADMIN` 权限。\n\n### SET GLOBAL\n\n使用 `SET GLOBAL` 设置全局变量需要拥有 `SUPER` 或者 `SYSTEM_VARIABLES_ADMIN` 权限。\n\n### ADMIN\n\n需要拥有 `SUPER` 权限。\n\n### SET DEFAULT ROLE\n\n需要拥有 `SUPER` 权限。\n\n### KILL\n\n使用 `KILL` 终止其他用户的会话需要拥有 `SUPER` 或者 `CONNECTION_ADMIN` 权限。\n\n### CREATE RESOURCE GROUP\n\n需要拥有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 权限。\n\n### ALTER RESOURCE GROUP\n\n需要拥有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 权限。\n\n### DROP RESOURCE GROUP\n\n需要拥有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 权限。\n\n### CALIBRATE RESOURCE\n\n需要拥有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 权限。\n\n### SET RESOURCE GROUP\n\n当系统变量 [`tidb_resource_control_strict_mode`](/system-variables.md#tidb_resource_control_strict_mode-从-v820-版本开始引入) 设置为 `ON` 时，你需要有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 或者 `RESOURCE_GROUP_USER` 权限才能执行该语句。\n\n## 权限系统的实现\n\n### 授权表\n\n以下几张 [`mysql` 系统表](/mysql-schema/mysql-schema-user.md)是非常特殊的表，权限相关的数据全部存储在这几张表内。\n\n- `mysql.user`：用户账户，全局权限\n- `mysql.db`：数据库级别的权限\n- `mysql.tables_priv`：表级别的权限\n- `mysql.columns_priv`：列级别的权限，当前暂不支持\n\n这几张表包含了数据的生效范围和权限信息。例如，`mysql.user` 表的部分数据：\n\n```sql\nSELECT User,Host,Select_priv,Insert_priv FROM mysql.user LIMIT 1;\n```\n\n```\n+------|------|-------------|-------------+\n| User | Host | Select_priv | Insert_priv |\n+------|------|-------------|-------------+\n| root | %    | Y           | Y           |\n+------|------|-------------|-------------+\n1 row in set (0.00 sec)\n```\n\n这条记录中，`Host` 和 `User` 决定了 root 用户从任意主机 (%) 发送过来的连接请求可以被接受，而 `Select_priv` 和 `Insert_priv` 表示用户拥有全局的 `Select` 和 `Insert` 权限。`mysql.user` 这张表里面的生效范围是全局的。\n\n`mysql.db` 表里面包含的 `Host` 和 `User` 决定了用户可以访问哪些数据库，权限列的生效范围是数据库。\n\n理论上，所有权限管理相关的操作，都可以通过直接对授权表的 CRUD 操作完成。\n\n实现层面其实也只是包装了一层语法糖。例如删除用户会执行：\n\n```sql\nDELETE FROM mysql.user WHERE user='test';\n```\n\n但是，不推荐手动修改授权表，建议使用 `DROP USER` 语句：\n\n```sql\nDROP USER 'test';\n```\n\n### 连接验证\n\n当客户端发送连接请求时，TiDB 服务器会对登录操作进行验证。验证过程先检查 `mysql.user` 表，当某条记录的 `User` 和 `Host` 和连接请求匹配上了，再去验证 `authentication_string`。用户身份基于两部分信息，发起连接的客户端的 `Host`，以及用户名 `User`。如果 `User` 不为空，则用户名必须精确匹配。\n\nUser+Host 可能会匹配 `user` 表里面多行，为了处理这种情况，`user` 表的行是排序过的，客户端连接时会依次去匹配，并使用首次匹配到的那一行做权限验证。排序是按 `Host` 在前，`User` 在后。\n\n### 请求验证\n\n连接成功之后，请求验证会检测执行操作是否拥有足够的权限。\n\n对于数据库相关请求 (`INSERT`, `UPDATE`)，先检查 `mysql.user` 表里面的用户全局权限，如果权限够，则直接可以访问。如果全局权限不足，则再检查 `mysql.db` 表。\n\n`user` 表的权限是全局的，并且不管默认数据库是哪一个。比如 `user` 里面有 `DELETE` 权限，任何一行，任何的表，任何的数据库。\n\n`db`表里面，User 为空是匹配匿名用户，User 里面不能有通配符。Host 和 Db 列里面可以有 `%` 和 `_`，可以模式匹配。\n\n`user` 和 `db` 读到内存也是排序的。\n\n`tables_priv` 和 `columns_priv` 中使用 `%` 是类似的，但是在`Db`, `Table_name`, `Column_name` 这些列不能包含 `%`。加载进来时排序也是类似的。\n\n### 生效时机\n\nTiDB 启动时，会将一些权限检查的表加载到内存，之后使用缓存的数据来验证权限。执行权限管理语句（如 `GRANT`、`REVOKE`、`CREATE USER` 和 `DROP USER`）将立即生效。\n\n使用 `INSERT`、`DELETE`、`UPDATE` 等语句手动修改 `mysql.user` 等授权表不会立即生效。该行为与 MySQL 兼容。如需立即生效，可以手动执行 [`FLUSH PRIVILEGES`](/sql-statements/sql-statement-flush-privileges.md) 语句更新权限的缓存。"
        },
        {
          "name": "production-deployment-using-tiup.md",
          "type": "blob",
          "size": 18.0625,
          "content": "---\ntitle: 使用 TiUP 部署 TiDB 集群\nsummary: 了解如何使用 TiUP 部署 TiDB 集群。\naliases: ['/docs-cn/dev/production-offline-deployment-using-tiup/', '/zh/tidb/dev/production-offline-deployment-using-tiup','/docs-cn/dev/production-deployment-using-tiup/','/docs-cn/dev/how-to/deploy/orchestrated/tiup/','/docs-cn/dev/tiflash/deploy-tiflash/','/docs-cn/dev/reference/tiflash/deploy/','/zh/tidb/dev/deploy-test-cluster-using-docker-compose','/zh/tidb/dev/test-deployment-using-docker']\n---\n\n# 使用 TiUP 部署 TiDB 集群\n\n本指南介绍如何在生产环境中使用 [TiUP](https://github.com/pingcap/tiup) 部署 TiDB 集群。\n\nTiUP 是在 TiDB v4.0 中引入的集群运维工具，提供了使用 Golang 编写的集群管理组件 [TiUP cluster](https://github.com/pingcap/tiup/tree/master/components/cluster)。通过使用 TiUP cluster 组件，你可以轻松执行日常的数据库运维操作，包括部署、启动、关闭、销毁、弹性扩缩容、升级 TiDB 集群，以及管理 TiDB 集群参数。\n\nTiUP 还支持部署 TiDB、TiFlash、TiCDC 以及监控系统。本指南介绍了如何部署不同拓扑的 TiDB 集群。\n\n## 第 1 步：软硬件环境需求及前置检查\n\n务必阅读以下文档：\n\n- [软硬件环境需求](/hardware-and-software-requirements.md)\n- [环境与系统配置检查](/check-before-deployment.md)\n\n此外，建议阅读了解 [TiDB 安全配置最佳实践](/best-practices-for-security-configuration.md)。\n\n## 第 2 步：在中控机上部署 TiUP 组件\n\n在中控机上部署 TiUP 组件有两种方式：在线部署和离线部署。\n\n### 在线部署\n\n以普通用户身份登录中控机。以 `tidb` 用户为例，后续安装 TiUP 及集群管理操作均通过该用户完成：\n\n1. 执行如下命令安装 TiUP 工具：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh\n    ```\n\n2. 按如下步骤设置 TiUP 环境变量：\n\n    1. 重新声明全局环境变量：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        source .bash_profile\n        ```\n\n    2. 确认 TiUP 工具是否安装：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        which tiup\n        ```\n\n3. 安装 TiUP cluster 组件：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster\n    ```\n\n4. 如果已经安装，则更新 TiUP cluster 组件至最新版本：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup update --self && tiup update cluster\n    ```\n\n    预期输出 `“Updated successfully!”` 字样。\n\n5. 验证当前 TiUP cluster 版本信息。执行如下命令查看 TiUP cluster 组件版本：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup --binary cluster\n    ```\n\n### 离线部署\n\n离线部署 TiUP 组件的操作步骤如下。\n\n#### 准备 TiUP 离线组件包\n\n方式一：在[官方下载页面](https://cn.pingcap.com/product-community/)选择对应版本的 TiDB server 离线镜像包（包含 TiUP 离线组件包）。需要同时下载 TiDB-community-server 软件包和 TiDB-community-toolkit 软件包。\n\n方式二：使用 `tiup mirror clone` 命令手动打包离线组件包。步骤如下：\n\n1. 在在线环境中安装 TiUP 包管理器工具\n\n    1. 执行如下命令安装 TiUP 工具：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh\n        ```\n\n    2. 重新声明全局环境变量：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        source .bash_profile\n        ```\n\n    3. 确认 TiUP 工具是否安装：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        which tiup\n        ```\n\n2. 使用 TiUP 制作离线镜像\n\n    1. 在一台和外网相通的机器上拉取需要的组件：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```bash\n        tiup mirror clone tidb-community-server-${version}-linux-amd64 ${version} --os=linux --arch=amd64\n        ```\n\n        该命令会在当前目录下创建一个名叫 `tidb-community-server-${version}-linux-amd64` 的目录，里面包含 TiUP 管理的组件包。\n\n    2. 通过 tar 命令将该组件包打包然后发送到隔离环境的中控机：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```bash\n        tar czvf tidb-community-server-${version}-linux-amd64.tar.gz tidb-community-server-${version}-linux-amd64\n        ```\n\n        此时，`tidb-community-server-${version}-linux-amd64.tar.gz` 就是一个独立的离线环境包。\n\n3. 自定义制作的离线镜像，或调整已有离线镜像中的内容\n\n    如果从官网下载的离线镜像不满足你的具体需求，或者希望对已有的离线镜像内容进行调整，例如增加某个组件的新版本等，可以采取以下步骤进行操作：\n\n    1. 在制作离线镜像时，可通过参数指定具体的组件和版本等信息，获得不完整的离线镜像。例如，要制作一个只包括 v1.12.3 版本 TiUP 和 TiUP Cluster 的离线镜像，可执行如下命令：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```bash\n        tiup mirror clone tiup-custom-mirror-v1.12.3 --tiup v1.12.3 --cluster v1.12.3\n        ```\n\n        如果只需要某一特定平台的组件，也可以通过 `--os` 和 `--arch` 参数来指定。\n\n    2. 参考上文“使用 TiUP 制作离线镜像”第 2 步的方式，将此不完整的离线镜像传输到隔离环境的中控机。\n\n    3. 在隔离环境的中控机上，查看当前使用的离线镜像路径。较新版本的 TiUP 可以直接通过命令获取当前的镜像地址：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```bash\n        tiup mirror show\n        ```\n\n        以上命令如果提示 `show` 命令不存在，可能当前使用的是较老版本的 TiUP。此时可以通过查看 `$HOME/.tiup/tiup.toml` 获得正在使用的镜像地址。将此镜像地址记录下来，后续步骤中将以变量 `${base_mirror}` 指代此镜像地址。\n\n    4. 将不完整的离线镜像合并到已有的离线镜像中：\n\n        首先将当前离线镜像中的 `keys` 目录复制到 `$HOME/.tiup` 目录中：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```bash\n        cp -r ${base_mirror}/keys $HOME/.tiup/\n        ```\n\n        然后使用 TiUP 命令将不完整的离线镜像合并到当前使用的镜像中：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```bash\n        tiup mirror merge tiup-custom-mirror-v1.12.3\n        ```\n\n    5. 上述步骤完成后，通过 `tiup list` 命令检查执行结果。在本文例子中，使用 `tiup list tiup` 和 `tiup list cluster` 均应能看到对应组件的 `v1.12.3` 版本出现在结果中。\n\n#### 部署离线环境 TiUP 组件\n\n将离线包发送到目标集群的中控机后，执行以下命令安装 TiUP 组件：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntar xzvf tidb-community-server-${version}-linux-amd64.tar.gz && \\\nsh tidb-community-server-${version}-linux-amd64/local_install.sh && \\\nsource /home/tidb/.bash_profile\n```\n\n`local_install.sh` 脚本会自动执行 `tiup mirror set tidb-community-server-${version}-linux-amd64` 命令将当前镜像地址设置为 `tidb-community-server-${version}-linux-amd64`。\n\n#### 合并离线包\n\n如果是通过[官方下载页面](https://cn.pingcap.com/product-community/)下载的离线软件包，需要将 TiDB-community-server 软件包和 TiDB-community-toolkit 软件包合并到离线镜像中。如果是通过 `tiup mirror clone` 命令手动打包的离线组件包，不需要执行此步骤。\n\n执行以下命令合并离线组件到 server 目录下。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntar xf tidb-community-toolkit-${version}-linux-amd64.tar.gz\nls -ld tidb-community-server-${version}-linux-amd64 tidb-community-toolkit-${version}-linux-amd64\ncd tidb-community-server-${version}-linux-amd64/\ncp -rp keys ~/.tiup/\ntiup mirror merge ../tidb-community-toolkit-${version}-linux-amd64\n```\n\n若需将镜像切换到其他目录，可以通过手动执行 `tiup mirror set <mirror-dir>` 进行切换。如果需要切换到在线环境，可执行 `tiup mirror set https://tiup-mirrors.pingcap.com`。\n\n## 第 3 步：初始化集群拓扑文件\n\n执行如下命令，生成集群初始化配置文件：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster template > topology.yaml\n```\n\n针对两种常用的部署场景，也可以通过以下命令生成建议的拓扑模板：\n\n- 混合部署场景：单台机器部署多个实例，详情参见[混合部署拓扑架构](/hybrid-deployment-topology.md)。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster template --full > topology.yaml\n    ```\n\n- 跨机房部署场景：跨机房部署 TiDB 集群，详情参见[跨机房部署拓扑架构](/geo-distributed-deployment-topology.md)。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster template --multi-dc > topology.yaml\n    ```\n\n执行 vi topology.yaml，查看配置文件的内容：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\nserver_configs: {}\npd_servers:\n  - host: 10.0.1.4\n  - host: 10.0.1.5\n  - host: 10.0.1.6\ntidb_servers:\n  - host: 10.0.1.7\n  - host: 10.0.1.8\n  - host: 10.0.1.9\ntikv_servers:\n  - host: 10.0.1.1\n  - host: 10.0.1.2\n  - host: 10.0.1.3\nmonitoring_servers:\n  - host: 10.0.1.4\ngrafana_servers:\n  - host: 10.0.1.4\nalertmanager_servers:\n  - host: 10.0.1.4\n```\n\n下表列出了常用的 7 种场景，请根据链接中的拓扑说明以及配置文件模板配置`topology.yaml`。如果有其他组合场景的需求，请根据多个模板自行调整。\n\n| 场景 | 配置任务 | 配置文件模板 | 拓扑说明 |\n| :-- | :-- | :-- | :-- |\n| OLTP 业务 | [部署最小拓扑架构](/minimal-deployment-topology.md) | [简单最小配置模板](/minimal-deployment-topology.md#拓扑模版)<br/>[详细最小配置模板](/minimal-deployment-topology.md#拓扑模版) | 最小集群拓扑，包括 tidb-server、tikv-server、pd-server。 |\n| HTAP 业务 | [部署 TiFlash 拓扑架构](/tiflash-deployment-topology.md) | [简单 TiFlash 配置模版](/tiflash-deployment-topology.md#拓扑模版)<br/>[详细 TiFlash 配置模版](/tiflash-deployment-topology.md#拓扑模版) | 在最小拓扑的基础上部署 TiFlash。TiFlash 是列式存储引擎，已经逐步成为集群拓扑的标配。|\n| 使用 [TiCDC](/ticdc/ticdc-overview.md) 进行增量同步 | [部署 TiCDC 拓扑架构](/ticdc-deployment-topology.md) | [简单 TiCDC 配置模板](/ticdc-deployment-topology.md#拓扑模版)<br/>[详细 TiCDC 配置模板](/ticdc-deployment-topology.md#拓扑模版) | 在最小拓扑的基础上部署 TiCDC。TiCDC 支持多种下游：TiDB、MySQL、Kafka、MQ、Confluent 和存储服务。 |\n| 使用 Spark 的 OLAP 业务 | [部署 TiSpark 拓扑架构](/tispark-deployment-topology.md) | [简单 TiSpark 配置模板](/tispark-deployment-topology.md#拓扑模版)<br/>[详细 TiSpark 配置模板](/tispark-deployment-topology.md#拓扑模版) | 在最小拓扑的基础上部署 TiSpark 组件。TiSpark 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品。TiUP cluster 组件对 TiSpark 的支持目前为实验特性。 |\n| 单台机器，多个实例 | [混合部署拓扑架构](/hybrid-deployment-topology.md) | [简单混部配置模板](/hybrid-deployment-topology.md#拓扑模版)<br/>[详细混部配置模板](/hybrid-deployment-topology.md#拓扑模版) | 也适用于单机多实例需要额外增加目录、端口、资源配比、label 等配置的场景。 |\n| 跨机房部署 TiDB 集群 | [跨机房部署拓扑架构](/geo-distributed-deployment-topology.md) | [跨机房配置模板](/geo-distributed-deployment-topology.md#拓扑模版) | 以典型的两地三中心架构为例，介绍跨机房部署架构，以及需要注意的关键设置。 |\n\n> **注意：**\n>\n> - 对于需要全局生效的参数，请在配置文件中 `server_configs` 的对应组件下配置。\n>\n> - 对于需要某个节点生效的参数，请在具体节点的 `config` 中配置。\n>\n> - 配置的层次结构使用 `.` 表示。如：`log.slow-threshold`。更多格式参考 [TiUP 配置参数模版](https://github.com/pingcap/tiup/blob/master/embed/examples/cluster/topology.example.yaml)。\n>\n> - 如果需要指定在目标机创建的用户组名，可以参考[这个例子](https://github.com/pingcap/tiup/blob/master/embed/examples/cluster/topology.example.yaml#L7)。\n\n更多参数说明，请参考：\n\n- [TiDB `config.toml.example`](https://github.com/pingcap/tidb/blob/master/pkg/config/config.toml.example)\n- [TiKV `config.toml.example`](https://github.com/tikv/tikv/blob/master/etc/config-template.toml)\n- [PD `config.toml.example`](https://github.com/pingcap/pd/blob/master/conf/config.toml)\n- [TiFlash `config.toml.example`](https://github.com/pingcap/tiflash/blob/master/etc/config-template.toml)\n\n## 第 4 步：执行部署命令\n\n> **注意：**\n>\n> 通过 TiUP 进行集群部署可以使用密钥或者交互密码方式来进行安全认证：\n>\n> - 如果是密钥方式，可以通过 -i 或者 --identity_file 来指定密钥的路径。\n>\n> - 如果是密码方式，可以通过 -p 进入密码交互窗口。\n>\n> - 如果已经配置免密登录目标机，则不需填写认证。\n>\n> 一般情况下 TiUP 会在目标机器上创建 topology.yaml 中约定的用户和组，以下情况例外：\n>\n> - `topology.yaml` 中设置的用户名在目标机器上已存在。\n>\n> - 在命令行上使用了参数 --skip-create-user 明确指定跳过创建用户的步骤。\n\n执行部署命令前，先使用 `check` 及 `check --apply` 命令检查和自动修复集群存在的潜在风险：\n\n1. 检查集群存在的潜在风险：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster check ./topology.yaml --user root [-p] [-i /home/root/.ssh/gcp_rsa]\n    ```\n\n2. 自动修复集群存在的潜在风险：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster check ./topology.yaml --apply --user root [-p] [-i /home/root/.ssh/gcp_rsa]\n    ```\n\n3. 部署 TiDB 集群：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster deploy tidb-test v8.5.0 ./topology.yaml --user root [-p] [-i /home/root/.ssh/gcp_rsa]\n    ```\n\n以上部署示例中：\n\n- `tidb-test` 为部署的集群名称。\n- `v8.5.0` 为部署的集群版本，可以通过执行 `tiup list tidb` 来查看 TiUP 支持的最新可用版本。\n- 初始化配置文件为 `topology.yaml`。\n- `--user root` 表示通过 root 用户登录到目标主机完成集群部署，该用户需要有 ssh 到目标机器的权限，并且在目标机器有 sudo 权限。也可以用其他有 ssh 和 sudo 权限的用户完成部署。\n- [-i] 及 [-p] 为可选项，如果已经配置免密登录目标机，则不需填写。否则选择其一即可，[-i] 为可登录到目标机的 root 用户（或 --user 指定的其他用户）的私钥，也可使用 [-p] 交互式输入该用户的密码。\n\n预期日志结尾输出 ```Deployed cluster `tidb-test` successfully``` 关键词，表示部署成功。\n\n## 第 5 步：查看 TiUP 管理的集群情况\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster list\n```\n\nTiUP 支持管理多个 TiDB 集群，该命令会输出当前通过 TiUP cluster 管理的所有集群信息，包括集群名称、部署用户、版本、密钥信息等。\n\n## 第 6 步：检查部署的 TiDB 集群情况\n\n例如，执行如下命令检查 `tidb-test` 集群情况：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display tidb-test\n```\n\n预期输出包括 `tidb-test` 集群中实例 ID、角色、主机、监听端口和状态（由于还未启动，所以状态为 Down/inactive）、目录信息。\n\n## 第 7 步：启动集群\n\n安全启动是 TiUP cluster 从 v1.9.0 起引入的一种新的启动方式，采用该方式启动数据库可以提高数据库安全性。推荐使用安全启动。\n\n安全启动后，TiUP 会自动生成 TiDB root 用户的密码，并在命令行界面返回密码。\n\n> **注意：**\n>\n> - 使用安全启动方式后，不能通过无密码的 root 用户登录数据库，你需要记录命令行返回的密码进行后续操作。\n>\n> - 该自动生成的密码只会返回一次，如果没有记录或者忘记该密码，请参照[忘记 root 密码](/user-account-management.md#忘记-root-密码)修改密码。\n\n方式一：安全启动\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster start tidb-test --init\n```\n\n预期结果如下，表示启动成功。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\nStarted cluster `tidb-test` successfully.\nThe root password of TiDB database has been changed.\nThe new password is: 'y_+3Hwp=*AWz8971s6'.\nCopy and record it to somewhere safe, it is only displayed once, and will not be stored.\nThe generated password can NOT be got again in future.\n```\n\n方式二：普通启动\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster start tidb-test\n```\n\n预期结果输出 ```Started cluster `tidb-test` successfully```，表示启动成功。使用普通启动方式后，可通过无密码的 root 用户登录数据库。\n\n## 第 8 步：验证集群运行状态\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display tidb-test\n```\n\n预期结果输出：各节点 Status 状态信息为 `Up` 说明集群状态正常。\n\n## 探索更多\n\n如果你已同时部署了 [TiFlash](/tiflash/tiflash-overview.md)，接下来可参阅以下文档：\n\n- [使用 TiFlash](/tiflash/tiflash-overview.md#使用-tiflash)\n- [TiFlash 集群运维](/tiflash/maintain-tiflash.md)\n- [TiFlash 报警规则与处理方法](/tiflash/tiflash-alert-rules.md)\n- [TiFlash 常见问题](/tiflash/troubleshoot-tiflash.md)\n\n如果你已同时部署了 [TiCDC](/ticdc/ticdc-overview.md)，接下来可参阅以下文档：\n\n- [Changefeed 概述](/ticdc/ticdc-changefeed-overview.md)\n- [管理 Changefeed](/ticdc/ticdc-manage-changefeed.md)\n- [TiCDC 故障处理](/ticdc/troubleshoot-ticdc.md)\n- [TiCDC 常见问题](/ticdc/ticdc-faq.md)\n\n如果你想在不中断线上服务的情况下扩容或缩容 TiDB 集群，请参阅[使用 TiUP 扩容缩容 TiDB 集群](/scale-tidb-using-tiup.md)。\n"
        },
        {
          "name": "quick-start-with-htap.md",
          "type": "blob",
          "size": 8.8798828125,
          "content": "---\ntitle: HTAP 快速上手指南\nsummary: 本文介绍如何快速上手体验 TiDB 的 HTAP 功能。 \n---\n\n# HTAP 快速上手指南\n\n本指南介绍如何快速上手体验 TiDB 的一站式混合型在线事务与在线分析处理 (Hybrid Transactional and Analytical Processing, HTAP) 功能。\n\n> **注意：**\n>\n> 本指南中的步骤仅适用于快速上手体验，不适用于生产环境。如需探索 HTAP 更多功能，请参考[深入探索 HTAP](/explore-htap.md)。\n\n## 基础概念\n\n在试用前，你需要对 TiDB 面向在线事务处理的行存储引擎 [TiKV](/tikv-overview.md) 与面向实时分析场景的列存储引擎 [TiFlash](/tiflash/tiflash-overview.md) 有一些基本了解：\n\n- HTAP 存储引擎：行存 (Row-store) 与列存 (columnar-store) 同时存在，自动同步，保持强一致性。行存为在线事务处理 OLTP 提供优化，列存则为在线分析处理 OLAP 提供性能优化。\n- HTAP 数据一致性：作为一个分布式事务型的键值数据库，TiKV 提供了满足 ACID 约束的分布式事务接口，并通过 [Raft](https://raft.github.io/raft.pdf) 协议保证了多副本数据一致性以及高可用。TiFlash 通过 Multi-Raft Learner 协议实时从 TiKV 复制数据，确保与 TiKV 之间的数据强一致。\n- HTAP 数据隔离性：TiKV、TiFlash 可按需部署在不同的机器，解决 HTAP 资源隔离的问题。\n- MPP 计算引擎：从 v5.0 版本起，TiFlash 引入了分布式计算框架 [MPP](/tiflash/use-tiflash-mpp-mode.md)，允许节点之间的数据交换并提供高性能、高吞吐的 SQL 算法，可以大幅度缩短分析查询的执行时间。\n\n## 体验步骤\n\n本文的步骤以 [TPC-H](http://www.tpc.org/tpch/) 数据集为例，通过其中一个查询场景来体验 TiDB HTAP 的便捷性和高性能。TPC-H 是业界较为流行的决策支持（Decision Support）业务 Benchmark。它包含大数据量下，一个业务决策分析系统所需要响应的不同类型高复杂度的即席查询。如果需要体验 TPC-H 完整的 22 条 SQL，可以访问 [tidb-bench 仓库](https://github.com/pingcap/tidb-bench/tree/master/tpch/queries) 或者阅读 TPC-H 官网说明了解如何生成查询语句以及数据。\n\n### 第 1 步：部署试用环境\n\n在试用 TiDB HTAP 功能前，请按照 [TiDB 数据库快速上手指南](/quick-start-with-tidb.md)中的步骤准备 TiDB 本地测试环境，执行以下命令启动 TiDB 集群：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup playground\n```\n\n> **注意：**\n>\n> `tiup playground` 命令仅适用于快速上手体验，不适用于生产环境，也不适用于全面的功能测试和稳定性测试。\n\n### 第 2 步：准备试用数据\n\n通过以下步骤，将生成一个 [TPC-H](http://www.tpc.org/tpch/) 数据集用于体验 TiDB HTAP 功能。如果你对 TPC-H 感兴趣，可查看其[规格说明](http://tpc.org/tpc_documents_current_versions/pdf/tpc-h_v3.0.0.pdf)。\n\n> **注意：**\n>\n> 如果你想使用自己现有的数据进行分析查询，可以将[数据迁移到 TiDB](/migration-overview.md) 中；如果你想自己设计并生成数据，可以通过 SQL 语句或相关工具生成。\n\n1. 使用以下命令安装数据生成工具：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup install bench\n    ```\n\n2. 使用以下命令生成数据：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup bench tpch --sf=1 prepare\n    ```\n\n    当命令行输出 `Finished` 时，表示数据生成完毕。\n\n3. 运行以下 SQL 语句查看生成的数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    SELECT CONCAT(table_schema,'.',table_name) AS 'Table Name', table_rows AS 'Number of Rows', CONCAT(ROUND(data_length/(1024*1024*1024),4),'G') AS 'Data Size', CONCAT(ROUND(index_length/(1024*1024*1024),4),'G') AS 'Index Size', CONCAT(ROUND((data_length+index_length)/(1024*1024*1024),4),'G') AS'Total'FROM information_schema.TABLES WHERE table_schema LIKE 'test';\n    ```\n\n    从输出中可以看到，一共生成了八张表，最大的一张表数据量有 600 万行（由于数据是工具随机生成，所以实际的数据生成量以 SQL 实际查询到的值为准）。\n\n    ```sql\n    +---------------+----------------+-----------+------------+---------+\n    | Table Name    | Number of Rows | Data Size | Index Size | Total   |\n    +---------------+----------------+-----------+------------+---------+\n    | test.nation   |             25 | 0.0000G   | 0.0000G    | 0.0000G |\n    | test.region   |              5 | 0.0000G   | 0.0000G    | 0.0000G |\n    | test.part     |         200000 | 0.0245G   | 0.0000G    | 0.0245G |\n    | test.supplier |          10000 | 0.0014G   | 0.0000G    | 0.0014G |\n    | test.partsupp |         800000 | 0.1174G   | 0.0119G    | 0.1293G |\n    | test.customer |         150000 | 0.0242G   | 0.0000G    | 0.0242G |\n    | test.orders   |        1514336 | 0.1673G   | 0.0000G    | 0.1673G |\n    | test.lineitem |        6001215 | 0.7756G   | 0.0894G    | 0.8651G |\n    +---------------+----------------+-----------+------------+---------+\n    8 rows in set (0.06 sec)\n    ```\n\n    这是一个商业订购系统的数据库。其中，`test.nation` 表是国家信息、`test.region` 表是地区信息、`test.part` 表是零件信息、`test.supplier` 表是供货商信息、`test.partsupp` 表是供货商的零件信息、`test.customer` 表是消费者信息、`test.orders` 表是订单信息、`test.lineitem` 表是在线商品的信息。\n\n### 第 3 步：使用行存查询数据\n\n执行以下 SQL 语句，你可以体验当只使用行存（大多数数据库）时 TiDB 的表现：\n\n{{< copyable \"sql\" >}}\n\n```sql\nUSE test;\nSELECT\n    l_orderkey,\n    SUM(\n        l_extendedprice * (1 - l_discount)\n    ) AS revenue,\n    o_orderdate,\n    o_shippriority\nFROM\n    customer,\n    orders,\n    lineitem\nWHERE\n    c_mktsegment = 'BUILDING'\nAND c_custkey = o_custkey\nAND l_orderkey = o_orderkey\nAND o_orderdate < DATE '1996-01-01'\nAND l_shipdate > DATE '1996-02-01'\nGROUP BY\n    l_orderkey,\n    o_orderdate,\n    o_shippriority\nORDER BY\n    revenue DESC,\n    o_orderdate\nlimit 10;\n```\n\n这是一个运送优先权查询，用于给出在指定日期之前尚未运送的订单中收入最高订单的优先权和潜在的收入。潜在的收入被定义为 `l_extendedprice * (1-l_discount)` 的和。订单按照收入的降序列出。在本示例中，此查询将列出潜在查询收入在前 10 的尚未运送的订单。\n\n### 第 4 步：同步列存数据\n\nTiFlash 部署完成后并不会自动同步 TiKV 数据，你可以在 MySQL 客户端向 TiDB 发送以下 DDL 命令指定需要同步到 TiFlash 的表。指定后，TiDB 将创建对应的 TiFlash 副本。\n\n{{< copyable \"sql\" >}}\n\n```sql\nALTER TABLE test.customer SET TIFLASH REPLICA 1;\nALTER TABLE test.orders SET TIFLASH REPLICA 1;\nALTER TABLE test.lineitem SET TIFLASH REPLICA 1;\n```\n\n如需查询 TiFlash 表的同步状态，请使用以下 SQL 语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = 'test' and TABLE_NAME = 'customer';\nSELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = 'test' and TABLE_NAME = 'orders';\nSELECT * FROM information_schema.tiflash_replica WHERE TABLE_SCHEMA = 'test' and TABLE_NAME = 'lineitem';\n```\n\n以上查询结果中：\n\n- `AVAILABLE` 字段表示该表的 TiFlash 副本是否可用。1 代表可用，0 代表不可用。副本状态变为可用之后就不再改变。\n- `PROGRESS` 字段代表同步进度，在 0.0~1.0 之间，1 代表 TiFlash 副本已经完成同步。\n\n### 第 5 步：使用 HTAP 更快地分析数据\n\n再次执行[第 3 步](#第-3-步使用行存查询数据)中的 SQL 语句，你可以感受 TiDB HTAP 的表现。\n\n对于创建了 TiFlash 副本的表，TiDB 优化器会自动根据代价估算选择是否使用 TiFlash 副本。如需查看实际是否选择了 TiFlash 副本，可以使用 `desc` 或 `explain analyze` 语句，例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nUSE test;\nEXPLAIN ANALYZE SELECT\n    l_orderkey,\n    SUM(\n        l_extendedprice * (1 - l_discount)\n    ) AS revenue,\n    o_orderdate,\n    o_shippriority\nFROM\n    customer,\n    orders,\n    lineitem\nWHERE\n    c_mktsegment = 'BUILDING'\nAND c_custkey = o_custkey\nAND l_orderkey = o_orderkey\nAND o_orderdate < DATE '1996-01-01'\nAND l_shipdate > DATE '1996-02-01'\nGROUP BY\n    l_orderkey,\n    o_orderdate,\n    o_shippriority\nORDER BY\n    revenue DESC,\n    o_orderdate\nlimit 10;\n```\n\n如果结果中出现 ExchangeSender 和 ExchangeReceiver 算子，表明 MPP 已生效。\n\n此外，你也可以指定整个查询的各个计算部分都只使用 TiFlash 引擎，详情请参阅[使用 TiDB 读取 TiFlash](/tiflash/use-tidb-to-read-tiflash.md)。\n\n你可以对比两次的查询结果和查询性能。\n\n## 探索更多\n\n- [TiDB HTAP 形态架构](/tiflash/tiflash-overview.md#整体架构)\n- [深入探索 HTAP](/explore-htap.md)\n- [使用 TiFlash](/tiflash/tiflash-overview.md#使用-tiflash)\n"
        },
        {
          "name": "quick-start-with-tidb.md",
          "type": "blob",
          "size": 17.7958984375,
          "content": "---\ntitle: TiDB 数据库快速上手指南\nsummary: 了解如何快速上手使用 TiDB 数据库。\naliases: ['/docs-cn/dev/quick-start-with-tidb/','/docs-cn/dev/how-to/get-started/local-cluster/install-from-docker-compose/']\n---\n\n# TiDB 数据库快速上手指南\n\n本指南介绍如何快速上手体验 TiDB 数据库。对于非生产环境，你可以选择以下任意一种方式部署 TiDB 数据库：\n\n- [部署本地测试集群](#部署本地测试集群)（支持 macOS 和 Linux）\n- [在单机上模拟部署生产环境集群](#在单机上模拟部署生产环境集群)（支持 Linux）\n\n> **注意：**\n>\n> 本指南中的 TiDB 部署方式仅适用于快速上手体验，不适用于生产环境。\n>\n> - 如需在生产环境部署 TiDB，请参考[在生产环境中部署 TiDB 指南](/production-deployment-using-tiup.md)。\n> - 如需在 Kubernetes 上部署 TiDB，请参考[快速上手 TiDB Operator](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/get-started)。\n> - 如需在云上管理 TiDB，请参考 [TiDB Cloud 快速上手指南](https://docs.pingcap.com/tidbcloud/tidb-cloud-quickstart)。\n\n要快速了解 TiUP 的基本功能、使用 TiUP 快速搭建 TiDB 集群的方法与连接 TiDB 集群并执行 SQL 的方法，建议先观看下面的培训视频（时长 15 分钟）。注意本视频只作为学习参考，如需了解 [TiUP](/tiup/tiup-overview.md) 的具体使用方法和 [TiDB 快速上手具体操作步骤](#部署本地测试集群)，请以文档内容为准。\n\n<video src=\"https://download.pingcap.com/docs-cn%2FLesson07_quick_start.mp4\" width=\"100%\" height=\"100%\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_lesson7.png\"></video>\n\n## 部署本地测试集群\n\n- 适用场景：利用本地 macOS 或者单机 Linux 环境快速部署 TiDB 测试集群，体验 TiDB 集群的基本架构，以及 TiDB、TiKV、PD、监控等基础组件的运行。\n\n<SimpleTab>\n<div label=\"macOS\">\n\nTiDB 是一个分布式系统。最基础的 TiDB 测试集群通常由 2 个 TiDB 实例、3 个 TiKV 实例、3 个 PD 实例和可选的 TiFlash 实例构成。通过 TiUP Playground，可以快速搭建出上述的一套基础测试集群，步骤如下：\n\n1. 下载并安装 TiUP。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh\n    ```\n\n    安装完成后会提示如下信息：\n\n    ```log\n    Successfully set mirror to https://tiup-mirrors.pingcap.com\n    Detected shell: zsh\n    Shell profile:  /Users/user/.zshrc\n    /Users/user/.zshrc has been modified to add tiup to PATH\n    open a new terminal or source /Users/user/.zshrc to use it\n    Installed path: /Users/user/.tiup/bin/tiup\n    ===============================================\n    Have a try:     tiup playground\n    ===============================================\n    ```\n\n2. 声明全局环境变量。\n\n    > **注意：**\n    >\n    > TiUP 安装完成后会提示 Shell profile 文件的绝对路径。在执行以下 `source` 命令前，需要将 `${your_shell_profile}` 修改为 Shell profile 文件的实际位置。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    source ${your_shell_profile}\n    ```\n\n3. 在当前 session 执行以下命令启动集群。\n\n    > **注意：**\n    >\n    > - 如果按以下方式执行 playground，在结束部署测试后，TiUP 会自动清理掉原集群数据，重新执行命令会得到一个全新的集群。\n    > - 如果希望持久化数据，需要在启动集群时添加 TiUP 的 `--tag` 参数，详见[启动集群时指定 `tag` 以保留数据](/tiup/tiup-playground.md#启动集群时指定-tag-以保留数据)。\n    >\n    >     ```shell\n    >     tiup playground --tag ${tag_name}\n    >     ```\n\n    - 直接执行 `tiup playground` 命令会运行最新版本的 TiDB 集群，其中 TiDB、TiKV、PD 和 TiFlash 实例各 1 个：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        tiup playground\n        ```\n\n    - 也可以指定 TiDB 版本以及各组件实例个数，命令类似于：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        tiup playground v8.5.0 --db 2 --pd 3 --kv 3\n        ```\n\n        上述命令会在本地下载并启动某个版本的集群（例如 v8.5.0）。最新版本可以通过执行 `tiup list tidb` 来查看。运行结果将显示集群的访问方式：\n\n        ```log\n        CLUSTER START SUCCESSFULLY, Enjoy it ^-^\n        To connect TiDB: mysql --comments --host 127.0.0.1 --port 4001 -u root -p (no password)\n        To connect TiDB: mysql --comments --host 127.0.0.1 --port 4000 -u root -p (no password)\n        To view the dashboard: http://127.0.0.1:2379/dashboard\n        PD client endpoints: [127.0.0.1:2379 127.0.0.1:2382 127.0.0.1:2384]\n        To view the Prometheus: http://127.0.0.1:9090\n        To view the Grafana: http://127.0.0.1:3000\n        ```\n\n        > **注意：**\n        >\n        > v5.2.0 及以上版本的 TiDB 支持在 Apple M1 芯片的机器上运行 `tiup playground`。\n\n4. 新开启一个 session 以访问 TiDB 数据库。\n\n    + 使用 TiUP `client` 连接 TiDB：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        tiup client\n        ```\n\n    + 也可使用 MySQL 客户端连接 TiDB：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        mysql --host 127.0.0.1 --port 4000 -u root\n        ```\n\n5. 通过 <http://127.0.0.1:9090> 访问 TiDB 的 Prometheus 管理界面。\n\n6. 通过 <http://127.0.0.1:2379/dashboard> 访问 [TiDB Dashboard](/dashboard/dashboard-intro.md) 页面，默认用户名为 `root`，密码为空。\n\n7. 通过 <http://127.0.0.1:3000> 访问 TiDB 的 Grafana 界面，默认用户名和密码都为 `admin`。\n\n8. （可选）[将数据加载到 TiFlash](/tiflash/tiflash-overview.md#使用-tiflash) 进行分析。\n\n9. 测试完成之后，可以通过执行以下步骤来清理集群：\n\n    1. 按下 <kbd>Control+C</kbd> 键停掉上述启用的 TiDB 服务。\n\n    2. 等待服务退出操作完成后，执行以下命令：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        tiup clean --all\n        ```\n\n> **注意：**\n>\n> TiUP Playground 默认监听 `127.0.0.1`，服务仅本地可访问；若需要使服务可被外部访问，可使用 `--host` 参数指定监听网卡绑定外部可访问的 IP。\n\n</div>\n<div label=\"Linux\">\n\nTiDB 是一个分布式系统。最基础的 TiDB 测试集群通常由 2 个 TiDB 实例、3 个 TiKV 实例、3 个 PD 实例和可选的 TiFlash 实例构成。通过 TiUP Playground，可以快速搭建出上述的一套基础测试集群，步骤如下：\n\n1. 下载并安装 TiUP。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh\n    ```\n\n    安装完成后会提示如下信息：\n\n    ```log\n    Successfully set mirror to https://tiup-mirrors.pingcap.com\n    Detected shell: bash\n    Shell profile:  /home/user/.bashrc\n    /home/user/.bashrc has been modified to add tiup to PATH\n    open a new terminal or source /home/user/.bashrc to use it\n    Installed path: /home/user/.tiup/bin/tiup\n    ===============================================\n    Have a try:     tiup playground\n    ===============================================\n    ```\n\n2. 声明全局环境变量。\n\n    > **注意：**\n    >\n    > TiUP 安装完成后会提示 Shell profile 文件的绝对路径。在执行以下 `source` 命令前，需要将 `${your_shell_profile}` 修改为 Shell profile 文件的实际位置。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    source ${your_shell_profile}\n    ```\n\n3. 在当前 session 执行以下命令启动集群。\n\n    > **注意：**\n    >\n    > - 如果按以下方式执行 playground，在结束部署测试后，TiUP 会自动清理掉原集群数据，重新执行命令会得到一个全新的集群。\n    > - 如果希望持久化数据，需要在启动集群时添加 TiUP 的 `--tag` 参数，详见[启动集群时指定 `tag` 以保留数据](/tiup/tiup-playground.md#启动集群时指定-tag-以保留数据)。\n    >\n    >     ```shell\n    >     tiup playground --tag ${tag_name}\n    >     ```\n\n    - 直接运行 `tiup playground` 命令会运行最新版本的 TiDB 集群，其中 TiDB、TiKV、PD 和 TiFlash 实例各 1 个：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        tiup playground\n        ```\n\n    - 也可以指定 TiDB 版本以及各组件实例个数，命令类似于：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        tiup playground v8.5.0 --db 2 --pd 3 --kv 3\n        ```\n\n        上述命令会在本地下载并启动某个版本的集群（例如 v8.5.0）。最新版本可以通过执行 `tiup list tidb` 来查看。运行结果将显示集群的访问方式：\n\n        ```log\n        CLUSTER START SUCCESSFULLY, Enjoy it ^-^\n        To connect TiDB: mysql --host 127.0.0.1 --port 4000 -u root -p (no password) --comments\n        To view the dashboard: http://127.0.0.1:2379/dashboard\n        PD client endpoints: [127.0.0.1:2379]\n        To view the Prometheus: http://127.0.0.1:9090\n        To view the Grafana: http://127.0.0.1:3000\n        ```\n\n4. 新开启一个 session 以访问 TiDB 数据库。\n\n    + 使用 TiUP `client` 连接 TiDB：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        tiup client\n        ```\n\n    + 也可使用 MySQL 客户端连接 TiDB：\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        mysql --host 127.0.0.1 --port 4000 -u root\n        ```\n\n5. 通过 <http://127.0.0.1:9090> 访问 TiDB 的 Prometheus 管理界面。\n\n6. 通过 <http://127.0.0.1:2379/dashboard> 访问 [TiDB Dashboard](/dashboard/dashboard-intro.md) 页面，默认用户名为 `root`，密码为空。\n\n7. 通过 <http://127.0.0.1:3000> 访问 TiDB 的 Grafana 界面，默认用户名和密码都为 `admin`。\n\n8. （可选）[将数据加载到 TiFlash](/tiflash/tiflash-overview.md#使用-tiflash) 进行分析。\n\n9. 测试完成之后，可以通过执行以下步骤来清理集群：\n\n    1. 按下 <kbd>Control+C</kbd> 键停掉上述启用的 TiDB 服务。\n\n    2. 等待服务退出操作完成后，执行以下命令：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup clean --all\n    ```\n\n> **注意：**\n>\n> TiUP Playground 默认监听 `127.0.0.1`，服务仅本地可访问。若需要使服务可被外部访问，可使用 `--host` 参数指定监听网卡绑定外部可访问的 IP。\n\n</div>\n</SimpleTab>\n\n## 在单机上模拟部署生产环境集群\n\n- 适用场景：希望用单台 Linux 服务器，体验 TiDB 最小的完整拓扑的集群，并模拟生产环境下的部署步骤。\n\n本节介绍如何参照 TiUP 最小拓扑的一个 YAML 文件部署 TiDB 集群。\n\n### 准备环境\n\n开始部署 TiDB 集群前，准备一台部署主机，确保其软件满足需求：\n\n- 推荐安装 CentOS 7.3 及以上版本\n- 运行环境可以支持互联网访问，用于下载 TiDB 及相关软件安装包\n\n最小规模的 TiDB 集群拓扑包含以下实例：\n\n> **注意：**\n>\n> 下表中拓扑实例的 IP 为示例 IP。在实际部署时，请替换为实际的 IP。\n\n| 实例 | 个数 | IP | 配置 |\n|:-- | :-- | :-- | :-- |\n| TiKV | 3 | 10.0.1.1 <br/> 10.0.1.1 <br/> 10.0.1.1 | 避免端口和目录冲突 |\n| TiDB | 1 | 10.0.1.1 | 默认端口 <br/> 全局目录配置 |\n| PD | 1 | 10.0.1.1 | 默认端口 <br/> 全局目录配置 |\n| TiFlash | 1 | 10.0.1.1 | 默认端口 <br/> 全局目录配置 |\n| Monitor | 1 | 10.0.1.1 | 默认端口 <br/> 全局目录配置 |\n\n部署主机软件和环境要求如下：\n\n- 部署需要使用部署主机的 root 用户及密码\n- 部署主机[关闭防火墙](/check-before-deployment.md#检测及关闭目标部署机器的防火墙)或者开放 TiDB 集群的节点间所需端口\n- 目前 TiUP Cluster 支持在 x86_64（AMD64）和 ARM 架构上部署 TiDB 集群\n    - 在 AMD64 架构下，建议使用 CentOS 7.3 及以上版本 Linux 操作系统\n    - 在 ARM 架构下，建议使用 CentOS 7.6 1810 版本 Linux 操作系统\n\n### 实施部署\n\n> **注意：**\n>\n> 你可以使用 Linux 系统的任一普通用户或 root 用户登录主机，以下步骤以 root 用户为例。\n\n1. 下载并安装 TiUP：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    curl --proto '=https' --tlsv1.2 -sSf https://tiup-mirrors.pingcap.com/install.sh | sh\n    ```\n\n2. 声明全局环境变量：\n\n    > **注意：**\n    >\n    > TiUP 安装完成后会提示对应 Shell profile 文件的绝对路径。在执行以下 `source` 命令前，需要将 `${your_shell_profile}` 修改为 Shell profile 文件的实际位置。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    source ${your_shell_profile}\n    ```\n\n3. 安装 TiUP 的 cluster 组件：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster\n    ```\n\n4. 如果机器已经安装 TiUP cluster，需要更新软件版本：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup update --self && tiup update cluster\n    ```\n\n5. 由于模拟多机部署，需要通过 root 用户调大 sshd 服务的连接数限制：\n\n    1. 修改 `/etc/ssh/sshd_config` 将 `MaxSessions` 调至 20。\n    2. 重启 sshd 服务：\n\n        {{< copyable \"shell-root\" >}}\n\n        ```shell\n        service sshd restart\n        ```\n\n6. 创建并启动集群\n\n    按下面的配置模板，编辑配置文件，命名为 `topo.yaml`，其中：\n\n    - `user: \"tidb\"`：表示通过 `tidb` 系统用户（部署会自动创建）来做集群的内部管理，默认使用 22 端口通过 ssh 登录目标机器\n    - `replication.enable-placement-rules`：设置这个 PD 参数来确保 TiFlash 正常运行\n    - `host`：设置为本部署主机的 IP\n\n    配置模板如下：\n\n    {{< copyable \"\" >}}\n\n    ```yaml\n    # # Global variables are applied to all deployments and used as the default value of\n    # # the deployments if a specific deployment value is missing.\n    global:\n     user: \"tidb\"\n     ssh_port: 22\n     deploy_dir: \"/tidb-deploy\"\n     data_dir: \"/tidb-data\"\n\n    # # Monitored variables are applied to all the machines.\n    monitored:\n     node_exporter_port: 9100\n     blackbox_exporter_port: 9115\n\n    server_configs:\n     tidb:\n       instance.tidb_slow_log_threshold: 300\n     tikv:\n       readpool.storage.use-unified-pool: false\n       readpool.coprocessor.use-unified-pool: true\n     pd:\n       replication.enable-placement-rules: true\n       replication.location-labels: [\"host\"]\n     tiflash:\n       logger.level: \"info\"\n\n    pd_servers:\n     - host: 10.0.1.1\n\n    tidb_servers:\n     - host: 10.0.1.1\n\n    tikv_servers:\n     - host: 10.0.1.1\n       port: 20160\n       status_port: 20180\n       config:\n         server.labels: { host: \"logic-host-1\" }\n\n     - host: 10.0.1.1\n       port: 20161\n       status_port: 20181\n       config:\n         server.labels: { host: \"logic-host-2\" }\n\n     - host: 10.0.1.1\n       port: 20162\n       status_port: 20182\n       config:\n         server.labels: { host: \"logic-host-3\" }\n\n    tiflash_servers:\n     - host: 10.0.1.1\n\n    monitoring_servers:\n     - host: 10.0.1.1\n\n    grafana_servers:\n     - host: 10.0.1.1\n    ```\n\n7. 执行集群部署命令：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster deploy <cluster-name> <version> ./topo.yaml --user root -p\n    ```\n\n    - 参数 `<cluster-name>` 表示设置集群名称\n    - 参数 `<version>` 表示设置集群版本，例如 `v8.5.0`。可以通过 `tiup list tidb` 命令来查看当前支持部署的 TiDB 版本\n    - 参数 `-p` 表示在连接目标机器时使用密码登录\n\n        > **注意：**\n        >\n        > 如果主机通过密钥进行 SSH 认证，请使用 `-i` 参数指定密钥文件路径，`-i` 与 `-p` 不可同时使用。\n\n    按照引导，输入”y”及 root 密码，来完成部署：\n\n    ```log\n    Do you want to continue? [y/N]:  y\n    Input SSH password:\n    ```\n\n8. 启动集群：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster start <cluster-name>\n    ```\n\n9. 访问集群：\n\n    - 安装 MySQL 客户端。如果已安装 MySQL 客户端则可跳过这一步骤。\n\n        {{< copyable \"shell-regular\" >}}\n\n        ```shell\n        yum -y install mysql\n        ```\n\n    - 访问 TiDB 数据库，密码为空：\n\n        ```shell\n        mysql -h 10.0.1.1 -P 4000 -u root\n        ```\n\n    - 访问 TiDB 的 Grafana 监控：\n\n        通过 <http://{grafana-ip}:3000> 访问集群 Grafana 监控页面，默认用户名和密码均为 `admin`。\n\n    - 访问 TiDB 的 Dashboard：\n\n        通过 <http://{pd-ip}:2379/dashboard> 访问集群 [TiDB Dashboard](/dashboard/dashboard-intro.md) 监控页面，默认用户名为 `root`，密码为空。\n\n    - 执行以下命令确认当前已经部署的集群列表：\n\n        ```shell\n        tiup cluster list\n        ```\n\n    - 执行以下命令查看集群的拓扑结构和状态：\n\n        ```shell\n        tiup cluster display <cluster-name>\n        ```\n\n## 探索更多\n\n如果你刚刚部署好一套 TiDB 本地测试集群，你可以继续：\n\n- 学习 [TiDB SQL 操作](/basic-sql-operations.md)\n- [迁移数据到 TiDB](/migration-overview.md)\n\n如果你准备好在生产环境部署 TiDB，你可以继续：\n\n- [使用 TiUP 部署 TiDB 集群](/production-deployment-using-tiup.md)\n- [使用 TiDB Operator 在 Kubernetes 上部署 TiDB 集群](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable)\n\n如果你是应用开发者，想要快速使用 TiDB 构建应用，可参阅以下文档：\n\n- [开发者手册概览](/develop/dev-guide-overview.md)\n- [使用 TiDB Cloud Serverless 构建 TiDB 集群](/develop/dev-guide-build-cluster-in-cloud.md)\n- [示例程序](/develop/dev-guide-sample-application-java-jdbc.md)\n\n如果你想使用 TiFlash 作为数据分析的解决方案，可参阅以下文档：\n\n- [使用 TiFlash](/tiflash/tiflash-overview.md#使用-tiflash)\n- [TiFlash 简介](/tiflash/tiflash-overview.md)\n"
        },
        {
          "name": "read-historical-data.md",
          "type": "blob",
          "size": 7.3828125,
          "content": "---\ntitle: 通过系统变量 tidb_snapshot 读取历史数据\naliases: ['/docs-cn/dev/read-historical-data/','/docs-cn/dev/how-to/get-started/read-historical-data/']\nsummary: 本文介绍了通过系统变量 `tidb_snapshot` 读取历史数据的操作流程和历史数据的保留策略。TiDB 实现了通过标准 SQL 接口读取历史数据功能，无需特殊的 client 或者 driver。当数据被更新、删除后，依然可以通过 SQL 接口将更新 / 删除前的数据读取出来。历史数据保留策略使用 MVCC 管理版本，超过一定时间的历史数据会被彻底删除，以减小空间占用以及避免历史版本过多引入的性能开销。\n---\n\n# 通过系统变量 tidb_snapshot 读取历史数据\n\n本文档介绍如何通过系统变量 `tidb_snapshot` 读取历史数据，包括具体的操作流程以及历史数据的保存策略。\n\n> **注意：**\n>\n> 你还可以使用 [Stale Read](/stale-read.md) 功能读取历史数据。更推荐使用 Stale Read 读取历史数据。\n\n## 功能说明\n\nTiDB 实现了通过标准 SQL 接口读取历史数据功能，无需特殊的 client 或者 driver。当数据被更新、删除后，依然可以通过 SQL 接口将更新/删除前的数据读取出来。\n\n> **注意：**\n>\n> 读取历史数据时，即使当前数据的表结构相较于历史数据的表结构已经发生改变，历史数据也会使用当时的表结构来返回数据。\n\n## 操作流程\n\n为支持读取历史版本数据，TiDB 引入了一个新的系统变量 [`tidb_snapshot`](/system-variables.md#tidb_snapshot)：\n\n- 这个变量的作用域为 `SESSION`。\n- 你可以通过标准的 `SET` 语句修改这个变量的值。\n- 这个变量的数据类型为文本类型，能够存储 TSO 和日期时间。TSO 是从 PD 端获取的全局授时的时间戳，日期时间的格式为：\"2016-10-08 16:45:26.999\"，一般来说可以只写到秒，比如”2016-10-08 16:45:26”。\n- 当这个变量被设置时，TiDB 会按照设置的时间戳建立 Snapshot（没有开销，只是创建数据结构），随后所有的 `SELECT` 操作都会从这个 Snapshot 上读取数据。\n\n> **注意：**\n>\n> TiDB 的事务是通过 PD 进行全局授时，所以存储的数据版本也是以 PD 所授时间戳作为版本号。在生成 Snapshot 时，是以 tidb_snapshot 变量的值作为版本号，如果 TiDB Server 所在机器和 PD Server 所在机器的本地时间相差较大，需要以 PD 的时间为准。\n\n当读取历史版本操作结束后，可以结束当前 Session 或者是通过 `SET` 语句将 tidb_snapshot 变量的值设为 \"\"，即可读取最新版本的数据。\n\n## 历史数据保留策略\n\nTiDB 使用 MVCC 管理版本，当更新/删除数据时，不会做真正的数据删除，只会添加一个新版本数据，所以可以保留历史数据。历史数据不会全部保留，超过一定时间的历史数据会被彻底删除，以减小空间占用以及避免历史版本过多引入的性能开销。\n\nTiDB 使用周期性运行的 GC（Garbage Collection，垃圾回收）来进行清理，关于 GC 的详细介绍参见 [TiDB 垃圾回收 (GC)](/garbage-collection-overview.md)。\n\n这里需要重点关注的是：\n\n- 使用系统变量 [`tidb_gc_life_time`](/system-variables.md#tidb_gc_life_time-从-v50-版本开始引入) 可以配置历史版本的保留时间（默认值是 `10m0s`）。\n- 使用 SQL 语句 `SELECT * FROM mysql.tidb WHERE variable_name = 'tikv_gc_safe_point'` 可以查询当前的 safePoint，即当前可以读的最旧的快照。在每次 GC 开始运行时，safePoint 将自动更新。\n\n## 示例\n\n1. 初始化阶段，创建一个表，并插入几行数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    create table t (c int);\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    insert into t values (1), (2), (3);\n    ```\n\n    ```\n    Query OK, 3 rows affected (0.00 sec)\n    ```\n\n2. 查看表中的数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |    2 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n3. 查看当前时间：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select now();\n    ```\n\n    ```\n    +---------------------+\n    | now()               |\n    +---------------------+\n    | 2016-10-08 16:45:26 |\n    +---------------------+\n    1 row in set (0.00 sec)\n    ```\n\n4. 更新某一行数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    update t set c=22 where c=2;\n    ```\n\n    ```\n    Query OK, 1 row affected (0.00 sec)\n    ```\n\n5. 确认数据已经被更新：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |   22 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n6. 设置一个特殊的环境变量，这个是一个 session scope 的变量，其意义为读取这个时间之前的最新的一个版本。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    set @@tidb_snapshot=\"2016-10-08 16:45:26\";\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.00 sec)\n    ```\n\n    > **注意：**\n    >\n    > - 这里的时间设置的是 update 语句之前的那个时间。\n    > - 在 `tidb_snapshot` 前须使用 `@@` 而非 `@`，因为 `@@` 表示系统变量，`@` 表示用户变量。\n\n    这里读取到的内容即为 update 之前的内容，也就是历史版本：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |    2 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n7. 清空这个变量后，即可读取最新版本数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    set @@tidb_snapshot=\"\";\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.00 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |   22 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n    > **注意：**\n    >\n    > 在 `tidb_snapshot` 前须使用 `@@` 而非 `@`，因为 `@@` 表示系统变量，`@` 表示用户变量。\n\n## 历史数据恢复策略\n\n在恢复历史版本的数据之前，需要确保在对数据进行操作时，垃圾回收机制 (GC) 不会清除历史数据。如下所示，可以通过设置 `tidb_gc_life_time` 变量来调整 GC 清理的周期。不要忘记在恢复历史数据后将该变量设置回之前的值。\n\n```sql\nSET GLOBAL tidb_gc_life_time=\"60m\";\n```\n\n> **注意：**\n>\n> 将 GC life time 从默认的 10 分钟增加到半小时及以上，会导致同一行保留有多个版本并占用更多的磁盘空间，也可能会影响某些操作的性能，例如扫描。进行扫描操作时，TiDB 读取数据需要跳过这些有多个版本的同一行，从而影响到扫描性能。\n\n如果想要恢复历史版本的数据，可以使用以下任意一种方法进行设置：\n\n- 对于简单场景，在设置 `tidb_snapshot` 变量后使用 [`SELECT`](/sql-statements/sql-statement-select.md) 语句并复制粘贴输出结果，或者使用 `SELECT ... INTO OUTFILE` 语句并使用 [`LOAD DATA`](/sql-statements/sql-statement-load-data.md) 语句来导入数据。\n\n- 使用 [Dumpling](/dumpling-overview.md#导出-tidb-的历史数据快照) 导出 TiDB 的历史数据快照。Dumpling 在导出较大的数据集时有较好的性能。\n"
        },
        {
          "name": "releases",
          "type": "tree",
          "content": null
        },
        {
          "name": "replicate-between-primary-and-secondary-clusters.md",
          "type": "blob",
          "size": 13.693359375,
          "content": "---\ntitle: 搭建双集群主从复制\nsummary: 了解如何配置一个 TiDB 集群以及该集群的 TiDB 或 MySQL 从集群，并将增量数据实时从主集群同步到从集群，\n---\n\n# 搭建双集群主从复制\n\n本文档介绍如何配置一个 TiDB 集群以及该集群的 TiDB 或 MySQL 从集群，并将增量数据实时从主集群同步到从集群，主要包含以下内容：\n\n1. 配置一个 TiDB 集群以及该集群的 TiDB 或 MySQL 从集群。\n2. 将增量数据实时从主集群同步到从集群。\n3. 在主集群发生灾难利用 Redo log 恢复一致性数据。\n\n如果你需要配置一个运行中的 TiDB 集群和其从集群，以进行实时增量数据同步，可使用 [Backup & Restore (BR)](/br/backup-and-restore-overview.md) 和 [TiCDC](/ticdc/ticdc-overview.md)。\n\n## 第 1 步：搭建环境\n\n1. 部署集群。\n\n    使用 tiup playground 快速部署 TiDB 上下游测试集群。生产环境可以参考 [tiup 官方文档](/tiup/tiup-cluster.md)根据业务需求来部署集群。\n\n    为了方便展示和理解，我们简化部署结构，需要准备以下两台机器，分别来部署上游主集群和下游从集群。假设 IP 地址分别为:\n\n    - NodeA: `172.16.6.123`，部署上游 TiDB\n\n    - NodeB: `172.16.6.124`，部署下游 TiDB\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n\n    # 在 NodeA 上创建上游集群\n    tiup --tag upstream playground --host 0.0.0.0 --db 1 --pd 1 --kv 1 --tiflash 0 --ticdc 1\n    # 在 NodeB 上创建下游集群\n    tiup --tag downstream playground --host 0.0.0.0 --db 1 --pd 1 --kv 1 --tiflash 0 --ticdc 0\n    # 查看集群状态\n    tiup status\n    ```\n\n2. 初始化数据。\n\n    测试集群中默认创建了 test 数据库，因此可以使用 [sysbench](https://github.com/akopytov/sysbench#linux) 工具生成测试数据，用以模拟真实集群中的历史数据。\n\n    ```shell\n    sysbench oltp_write_only --config-file=./tidb-config --tables=10 --table-size=10000 prepare\n    ```\n\n    这里通过 sysbench 运行 oltp_write_only 脚本，其将在测试数据库中生成 10 张表，每张表包含 10000 行初始数据。tidb-config 的配置如下：\n\n    ```yaml\n    mysql-host=172.16.6.122 # 这里需要替换为实际上游集群 ip\n    mysql-port=4000\n    mysql-user=root\n    mysql-password=\n    db-driver=mysql         # 设置数据库驱动为 mysql\n    mysql-db=test           # 设置测试数据库为 test\n    report-interval=10      # 设置定期统计的时间间隔为 10 秒\n    threads=10              # 设置 worker 线程数量为 10\n    time=0                  # 设置脚本总执行时间，0 表示不限制\n    rate=100                # 设置平均事务速率 tps = 100\n    ```\n\n3. 模拟业务负载。\n\n    实际生产集群的数据迁移过程中，通常原集群还会写入新的业务数据，本文中可以通过 sysbench 工具模拟持续的写入负载，下面的命令会使用 10 个 worker 在数据库中的 sbtest1、sbtest2 和 sbtest3 三张表中持续写入数据，其总 tps 限制为 100。\n\n    ```shell\n    sysbench oltp_write_only --config-file=./tidb-config --tables=3 run\n    ```\n\n4. 准备外部存储。\n\n    在全量数据备份中，上下游集群均需访问备份文件，因此推荐使用[外部存储](/br/backup-and-restore-storages.md)存储备份文件，本文中通过 Minio 模拟兼容 S3 的存储服务：\n\n    ```shell\n    wget https://dl.min.io/server/minio/release/linux-amd64/minio\n    chmod +x minio\n    # 配置访问 minio 的 access-key access-secret-id\n    export `HOST_IP`='172.16.6.123' # 替换为实际部署 minio 的机器 ip\n    export** **MINIO_ROOT_USER**='**minio'\n    export MINIO_ROOT_PASSWORD='miniostorage'\n    # 创建 redo 和 backup 数据目录,  其中 redo, backup 为 bucket 名字\n    mkdir -p data/redo\n    mkdir -p data/backup\n    # 启动 minio, 暴露端口在 6060\n    nohup ./minio server ./data --address :6060 &\n    ```\n\n    上述命令行启动了一个单节点的 minio server 模拟 S3 服务，其相关参数为：\n\n    * Endpoint : `http://${HOST_IP}:6060/`\n    * Access-key : `minio`\n    * Secret-access-key: `miniostorage`\n    * Bucket: `redo`\n\n    其访问链接为如下:\n\n    ```shell\n    s3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://${HOST_IP}:6060&force-path-style=true\n    ```\n\n## 第 2 步：迁移全量数据\n\n搭建好测试环境后，可以使用 [BR](https://github.com/pingcap/tidb/tree/master/br) 工具的备份和恢复功能迁移全量数据。BR 工具有多种[使用方式](/br/br-use-overview.md#部署和使用-br)，本文中使用 SQL 语句 [`BACKUP`](/sql-statements/sql-statement-backup.md) 和 [`RESTORE`](/sql-statements/sql-statement-restore.md) 进行备份恢复。\n\n> **注意：**\n>\n> - `BACKUP` 和 `RESTORE` 语句目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n> - 在生产集群中，关闭 GC 机制和备份操作会一定程度上降低集群的读性能，建议在业务低峰期进行备份，并设置合适的 `RATE_LIMIT` 限制备份操作对线上业务的影响。\n> - 上下游集群版本不一致时，应检查 BR 工具的[兼容性](/br/backup-and-restore-overview.md#使用建议)。本文假设上下游集群版本相同。\n\n1. 关闭 GC。\n\n    为了保证增量迁移过程中新写入的数据不丢失，在开始备份之前，需要关闭上游集群的垃圾回收 (GC) 机制，以确保系统不再清理历史数据。\n\n    执行如下命令关闭 GC：\n\n    ```sql\n    MySQL [test]> SET GLOBAL tidb_gc_enable=FALSE;\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    查询 `tidb_gc_enable` 的取值，判断 GC 是否已关闭：\n\n    ```sql\n    MySQL [test]> SELECT @@global.tidb_gc_enable;\n    ```\n\n    ```\n    +-------------------------+\n    | @@global.tidb_gc_enable |\n    +-------------------------+\n    |                       0 |\n    +-------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n2. 备份数据。\n\n    在上游集群中执行 BACKUP 语句备份数据：\n\n    ```sql\n    MySQL [(none)]> BACKUP DATABASE * TO '`s3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://${HOST_IP}:6060&force-path-style=true`' RATE_LIMIT = 120 MB/SECOND;\n    ```\n\n    ```\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    | Destination          | Size     | BackupTS           | Queue Time          | Execution Time      |\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    | local:///tmp/backup/ | 10315858 | 431434047157698561 | 2022-02-25 19:57:59 | 2022-02-25 19:57:59 |\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    1 row in set (2.11 sec)\n    ```\n\n    备份语句提交成功后，TiDB 会返回关于备份数据的元信息，这里需要重点关注 BackupTS，它意味着该时间点之前数据会被备份，后边的教程中，本文将使用 BackupTS 作为**数据校验截止时间**和 **TiCDC 增量扫描的开始时间**。\n\n3. 恢复数据。\n\n    在下游集群中执行 RESTORE 语句恢复数据：\n\n    ```sql\n    mysql> RESTORE DATABASE * FROM '`s3://backup?access-key=minio&secret-access-key=miniostorage&endpoint=http://${HOST_IP}:6060&force-path-style=true`';\n    ```\n\n    ```\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    | Destination          | Size     | BackupTS           | Queue Time          | Execution Time      |\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    | local:///tmp/backup/ | 10315858 | 431434141450371074 | 2022-02-25 20:03:59 | 2022-02-25 20:03:59 |\n    +----------------------+----------+--------------------+---------------------+---------------------+\n    1 row in set (41.85 sec)\n    ```\n\n4. （可选）校验数据。\n\n    通过 [sync-diff-inspector](/sync-diff-inspector/sync-diff-inspector-overview.md) 工具，可以验证上下游数据在某个时间点的一致性。从上述备份和恢复命令的输出可以看到，上游集群备份的时间点为 431434047157698561，下游集群完成数据恢复的时间点为 431434141450371074。\n\n    ```shell\n    sync_diff_inspector -C ./config.yaml\n    ```\n\n    关于 sync-diff-inspector 的配置方法，请参考[配置文件说明](/sync-diff-inspector/sync-diff-inspector-overview.md#配置文件说明)。在本文中，相应的配置如下：\n\n    ```toml\n    # Diff Configuration.\n    ######################### Global config #########################\n    check-thread-count = 4\n    export-fix-sql = true\n    check-struct-only = false\n\n    ######################### Datasource config #########################\n    [data-sources]\n    [data-sources.upstream]\n        host = \"172.16.6.123\" # 替换为实际上游集群 ip\n        port = 4000\n        user = \"root\"\n        password = \"\"\n        snapshot = \"431434047157698561\" # 配置为实际的备份时间点\n    [data-sources.downstream]\n        host = \"172.16.6.124\" # 替换为实际下游集群 ip\n        port = 4000\n        user = \"root\"\n        password = \"\"\n        snapshot = \"431434141450371074\" # 配置为实际的恢复时间点\n\n    ######################### Task config #########################\n    [task]\n        output-dir = \"./output\"\n        source-instances = [\"upstream\"]\n        target-instance = \"downstream\"\n        target-check-tables = [\"*.*\"]\n    ```\n\n## 第 3 步：迁移增量数据\n\n1. 部署 TiCDC。\n\n    完成全量数据迁移后，就可以部署并配置 TiCDC 集群同步增量数据，实际生产集群中请参考 [TiCDC 部署](/ticdc/deploy-ticdc.md)。本文在创建测试集群时，已经启动了一个 TiCDC 节点，因此可以直接进行 changefeed 的配置。\n\n2. 创建同步任务。\n\n    创建 changefeed 配置文件并保存为 `changefeed.toml`。\n\n    ```toml\n    [consistent]\n    # 一致性级别，配置成 eventual 表示开启一致性复制\n    level = \"eventual\"\n    # 使用 S3 来存储 redo log, 其他可选为 local, nfs\n    storage = \"s3://redo?access-key=minio&secret-access-key=miniostorage&endpoint=http://172.16.6.125:6060&force-path-style=true\"\n    ```\n\n    在上游集群中，执行以下命令创建从上游到下游集群的同步链路：\n\n    ```shell\n    tiup cdc cli changefeed create --server=http://172.16.6.122:8300 --sink-uri=\"mysql://root:@172.16.6.125:4000\" --changefeed-id=\"primary-to-secondary\" --start-ts=\"431434047157698561\"\n    ```\n\n    以上命令中：\n\n    - `--server`：TiCDC 集群任意一节点的地址\n    - `--sink-uri`：同步任务下游的地址\n    - `--start-ts`：TiCDC 同步的起点，需要设置为实际的备份时间点（也就是[第 2 步：迁移全量数据](#第-2-步迁移全量数据)提到的 BackupTS）\n\n    更多关于 changefeed 的配置，请参考 [TiCDC Changefeed 配置参数](/ticdc/ticdc-changefeed-config.md)。\n\n3. 重新开启 GC。\n\n    TiCDC 可以保证未同步的历史数据不会被回收。因此，创建完从上游到下游集群的 changefeed 之后，就可以执行如下命令恢复集群的垃圾回收功能。详情请参考 [TiCDC GC safepoint 的完整行为](/ticdc/ticdc-faq.md#ticdc-gc-safepoint-的完整行为是什么)。\n\n   执行如下命令打开 GC：\n\n    ```sql\n    MySQL [test]> SET GLOBAL tidb_gc_enable=TRUE;\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    查询 `tidb_gc_enable` 的取值，判断 GC 是否已开启：\n\n    ```sql\n    MySQL [test]> SELECT @@global.tidb_gc_enable;\n    ```\n\n    ```\n    +-------------------------+\n    | @@global.tidb_gc_enable |\n    +-------------------------+\n    |                       1 |\n    +-------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n## 第 4 步：模拟主集群故障\n\n模拟在业务过程中上游 TiDB 发生灾难性故障无法再启动起来，这里可以直接使用 Ctrl + C 终止 tiup playground 进程。\n\n## 第 5 步：使用 redo log 确保数据一致性\n\n在正常同步过程中，为了提高 TiCDC 的吞吐能力，TiCDC 会将事务并行写入下游。因此，当 TiCDC 同步链路意外中断时，下游可能不会恰好停在与上游一致的状态。我们这里需要使用 TiCDC 的命令行工具来向下游重放 redo log，使下游达到最终一致性状态。\n\n```shell\ntiup cdc redo apply --storage \"s3://redo?access-key=minio&secret-access-key=miniostorage&endpoint=http://172.16.6.123:6060&force-path-style=true\" --tmp-dir /tmp/redo --sink-uri \"mysql://root:@172.16.6.124:4000\"\n```\n\n- `--storage`：指定 redo log 所在的 S3 位置以及 credential\n- `--tmp-dir`：为从 S3 下载 redo log 的缓存目录\n- `--sink-uri`：指定下游集群的地址\n\n## 第 6 步：恢复主集群及业务\n\n现在从集群有了某一时刻全部的一致性数据，你需要重新搭建主从集群来保证数据可靠性。\n\n1. 在 NodeA 重新搭建一个新的 TiDB 集群作为新的主集群。\n\n    ```shell\n    tiup --tag upstream playground v5.4.0 --host 0.0.0.0 --db 1 --pd 1 --kv 1 --tiflash 0 --ticdc 1\n    ```\n\n2. 使用 BR 将从集群数据全量备份恢复到主集群。\n\n    ```shell\n    # 全量备份从集群的数据\n    tiup br --pd http://172.16.6.124:2379 backup full --storage ./backup\n    # 全量恢复从集群的数据\n    tiup br --pd http://172.16.6.123:2379 restore full --storage ./backup\n    ```\n\n3. 创建一个 TiCDC 同步任务，备份主集群数据到从集群。\n\n    ```shell\n    # 创建 changefeed\n    tiup cdc cli changefeed create --server=http://172.16.6.122:8300 --sink-uri=\"mysql://root:@172.16.6.125:4000\" --changefeed-id=\"primary-to-secondary\"\n    ```\n"
        },
        {
          "name": "replicate-data-to-kafka.md",
          "type": "blob",
          "size": 8.0078125,
          "content": "---\ntitle: 与 Apache Kafka 和 Apache Flink 进行数据集成\nsummary: 了解如何使用 TiCDC 从 TiDB 同步数据至 Apache Kafka 和 Apache Flink。\naliases: ['/zh/tidb/dev/replicate-incremental-data-to-kafka/']\n---\n\n# 与 Apache Kafka 和 Apache Flink 进行数据集成\n\n本文档介绍如何使用 [TiCDC](/ticdc/ticdc-overview.md) 将 TiDB 的数据同步到 Apache Kafka。主要包含以下内容：\n\n1. 快速搭建 TiCDC 集群、Kafka 集群和 Flink 集群\n2. 创建 changefeed，将 TiDB 增量数据输出至 Kafka\n3. 使用 go-tpc 写入数据到上游 TiDB\n4. 使用 Kafka console consumer 观察数据被写入到指定的 Topic\n5. （可选）配置 Flink 集群消费 Kafka 内数据\n\n上述过程将会基于实验环境进行。你也可以参考上述执行步骤，搭建生产级别的集群。\n\n## 第 1 步：搭建环境\n\n1. 部署包含 TiCDC 的 TiDB 集群。\n\n    在实验或测试环境中，可以使用 TiUP Playground 功能，快速部署 TiCDC，命令如下：\n\n    ```shell\n    tiup playground --host 0.0.0.0 --db 1 --pd 1 --kv 1 --tiflash 0 --ticdc 1\n    # 查看集群状态\n    tiup status\n    ```\n\n    如果尚未安装 TiUP，可以参考[安装 TiUP](/tiup/tiup-overview.md)。在生产环境下，可以参考 [TiUP 安装部署 TiCDC 集群](/ticdc/deploy-ticdc.md)，完成 TiCDC 集群部署工作。\n\n2. 部署 Kafka 集群。\n\n    - 实验环境，可以参考 [Apache Kafka Quickstart](https://kafka.apache.org/quickstart) 启动 Kafka 集群。\n    - 生产环境，可以参考 [Running Kafka in Production](https://docs.confluent.io/platform/current/kafka/deployment.html) 完成 Kafka 集群搭建。\n\n3. （可选）部署 Flink 集群。\n\n    - 实验环境，可以参考 [Apache Flink First steps](https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/try-flink/local_installation/) 启动 Flink 集群。\n    - 生产环境，可以参考 [Apache Kafka Deployment](https://nightlies.apache.org/flink/flink-docs-release-1.15/docs/deployment/overview/) 部署 Flink 生产集群。\n\n## 第 2 步：创建 Kafka changefeed\n\n1. 创建 changefeed 配置文件。\n\n    根据 Flink 的要求和规范，每张表的增量数据需要发送到独立的 Topic 中，并且每个事件需要按照主键值分发 Partition。因此，需要创建一个名为 `changefeed.conf` 的配置文件，填写如下内容：\n\n    ```\n    [sink]\n    dispatchers = [\n    {matcher = ['*.*'], topic = \"tidb_{schema}_{table}\", partition=\"index-value\"},\n    ]\n    ```\n\n    关于配置文件中 dispatchers 的详细解释，参考[自定义 Kafka Sink 的 Topic 和 Partition 的分发规则](/ticdc/ticdc-sink-to-kafka.md#自定义-kafka-sink-的-topic-和-partition-的分发规则)。\n\n2. 创建一个 changefeed，将增量数据输出到 Kafka：\n\n    ```shell\n    tiup cdc:v<CLUSTER_VERSION> cli changefeed create --server=\"http://127.0.0.1:8300\" --sink-uri=\"kafka://127.0.0.1:9092/kafka-topic-name?protocol=canal-json\" --changefeed-id=\"kafka-changefeed\" --config=\"changefeed.conf\"\n    ```\n\n    - 如果命令执行成功，将会返回被创建的 changefeed 的相关信息，包含被创建的 changefeed 的 ID 以及相关信息，内容如下：\n\n        ```shell\n        Create changefeed successfully!\n        ID: kafka-changefeed\n        Info: {... changfeed info json struct ...}\n        ```\n\n    - 如果命令长时间没有返回，你需要检查当前执行命令所在服务器到 sink-uri 中指定的 Kafka 机器的网络可达性，保证二者之间的网络连接正常。\n\n    生产环境下 Kafka 集群通常有多个 broker 节点，你可以在 sink-uri 中配置多个 broker 的访问地址，这有助于提升 changefeed 到 Kafka 集群访问的稳定性，当部分被配置的 Kafka 节点故障的时候，changefeed 依旧可以正常工作。假设 Kafka 集群中有 3 个 broker 节点，地址分别为 127.0.0.1:9092 / 127.0.0.2:9092 / 127.0.0.3:9092，可以参考如下 sink-uri 创建 changefeed：\n\n    ```shell\n    tiup cdc:v<CLUSTER_VERSION> cli changefeed create --server=\"http://127.0.0.1:8300\" --sink-uri=\"kafka://127.0.0.1:9092,127.0.0.2:9092,127.0.0.3:9092/kafka-topic-name?protocol=canal-json&partition-num=3&replication-factor=1&max-message-bytes=1048576\" --config=\"changefeed.conf\"\n    ```\n\n3. Changefeed 创建成功后，执行如下命令，查看 changefeed 的状态：\n\n    ```shell\n    tiup cdc:v<CLUSTER_VERSION> cli changefeed list --server=\"http://127.0.0.1:8300\"\n    ```\n\n    可以参考[管理 Changefeed](/ticdc/ticdc-manage-changefeed.md)，对 changefeed 状态进行管理。\n\n## 第 3 步：写入数据以产生变更日志\n\n完成以上步骤后，TiCDC 会将上游 TiDB 的增量数据变更日志发送到 Kafka，下面对 TiDB 写入数据，以产生增量数据变更日志。\n\n1. 模拟业务负载。\n\n    在测试实验环境下，可以使用 go-tpc 向上游 TiDB 集群写入数据，以让 TiDB 产生事件变更数据。如下命令，首先在上游 TiDB 创建名为 `tpcc` 的数据库，然后使用 TiUP bench 写入数据到这个数据库中。\n\n    ```shell\n    tiup bench tpcc -H 127.0.0.1 -P 4000 -D tpcc --warehouses 4 prepare\n    tiup bench tpcc -H 127.0.0.1 -P 4000 -D tpcc --warehouses 4 run --time 300s\n    ```\n\n    关于 go-tpc 的更多详细内容，可以参考[如何对 TiDB 进行 TPC-C 测试](/benchmark/benchmark-tidb-using-tpcc.md)。\n\n2. 消费 Kafka Topic 中的数据。\n\n    changefeed 正常运行时，会向 Kafka Topic 写入数据，你可以通过由 Kafka 提供的 kafka-console-consumer.sh，观测到数据成功被写入到 Kafka Topic 中：\n\n    ```shell\n    ./bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --from-beginning --topic `${topic-name}`\n    ```\n\n至此，TiDB 的增量数据变更日志就实时地复制到了 Kafka。下一步，你可以使用 Flink 消费 Kafka 数据。当然，你也可以自行开发适用于业务场景的 Kafka 消费端。\n\n## 第 4 步：配置 Flink 消费 Kafka 数据（可选）\n\n1. 安装 Flink Kafka Connector。\n\n    在 Flink 生态中，Flink Kafka Connector 用于消费 Kafka 中的数据并输出到 Flink 中。Flink Kafka Connector 并不是内建的，因此在 Flink 安装完毕后，还需要将 Flink Kafka Connector 及其依赖项添加到 Flink 安装目录中。下载下列 jar 文件至 Flink 安装目录下的 lib 目录中，如果你已经运行了 Flink 集群，请重启集群以加载新的插件。\n\n    - [flink-connector-kafka-1.15.0.jar](https://repo.maven.apache.org/maven2/org/apache/flink/flink-connector-kafka/1.15.0/flink-connector-kafka-1.15.0.jar)\n    - [flink-sql-connector-kafka-1.15.0.jar](https://repo.maven.apache.org/maven2/org/apache/flink/flink-sql-connector-kafka/1.15.0/flink-sql-connector-kafka-1.15.0.jar)\n    - [kafka-clients-3.2.0.jar](https://repo.maven.apache.org/maven2/org/apache/kafka/kafka-clients/3.2.0/kafka-clients-3.2.0.jar)\n\n2. 创建一个表。\n\n    你可以在 Flink 的安装目录执行如下命令，启动 Flink SQL 交互式客户端：\n\n    ```shell\n    [root@flink flink-1.15.0]# ./bin/sql-client.sh\n    ```\n\n    随后，执行如下语句创建一个名为 `tpcc_orders` 的表：\n\n    ```sql\n    CREATE TABLE tpcc_orders (\n        o_id INTEGER,\n        o_d_id INTEGER,\n        o_w_id INTEGER,\n        o_c_id INTEGER,\n        o_entry_d STRING,\n        o_carrier_id INTEGER,\n        o_ol_cnt INTEGER,\n        o_all_local INTEGER\n    ) WITH (\n    'connector' = 'kafka',\n    'topic' = 'tidb_tpcc_orders',\n    'properties.bootstrap.servers' = '127.0.0.1:9092',\n    'properties.group.id' = 'testGroup',\n    'format' = 'canal-json',\n    'scan.startup.mode' = 'earliest-offset',\n    'properties.auto.offset.reset' = 'earliest'\n    )\n    ```\n\n    请将 `topic` 和 `properties.bootstrap.servers` 参数替换为环境中的实际值。\n\n3. 查询表内容。\n\n    执行如下命令，查询 `tpcc_orders` 表中的数据：\n\n    ```sql\n    SELECT * FROM tpcc_orders;\n    ```\n\n    执行成功后，可以观察到有数据输出，如下图：\n\n    ![SQL query result](/media/integrate/sql-query-result.png)\n\n至此，就完成了 TiDB 与 Flink 的数据集成。\n"
        },
        {
          "name": "resources",
          "type": "tree",
          "content": null
        },
        {
          "name": "role-based-access-control.md",
          "type": "blob",
          "size": 11.6083984375,
          "content": "---\ntitle: 基于角色的访问控制\naliases: ['/docs-cn/dev/role-based-access-control/','/docs-cn/dev/reference/security/role-based-access-control/']\nsummary: TiDB 的基于角色的访问控制 (RBAC) 系统类似于 MySQL 8.0 的 RBAC 系统。用户可以创建、删除和授予角色权限，也可以将角色授予其他用户。角色需要在用户启用后才能生效。用户可以通过 `SHOW GRANTS` 查看角色权限，也可以设置默认启用角色。角色授权具有原子性，失败会回滚。除了角色授权外，还有用户管理和权限管理相关操作。\n---\n\n# 基于角色的访问控制\n\nTiDB 的基于角色的访问控制 (RBAC) 系统的实现类似于 MySQL 8.0 的 RBAC 系统。TiDB 兼容大部分 MySQL RBAC 系统的语法。\n\n本文档主要介绍 TiDB 基于角色的访问控制相关操作及实现。\n\n## 角色访问控制相关操作\n\n角色是一系列权限的集合。用户可以创建角色、删除角色、将权限赋予角色；也可以将角色授予给其他用户，被授予的用户在启用角色后，可以得到角色所包含的权限。\n\n### 创建角色\n\n创建角色 app_developer，app_read 和 app_write：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE ROLE 'app_developer', 'app_read', 'app_write';\n```\n\n角色名的格式和规范可以参考 [TiDB 用户账户管理](/user-account-management.md)。\n\n角色会被保存在 `mysql.user` 表中，角色名称的主机名部分（如果省略）默认为 `'%'`。如果表中有同名角色或用户，角色会创建失败并报错。创建角色的用户需要拥有 `CREATE ROLE` 或 `CREATE USER` 权限。\n\n### 授予角色权限\n\n为角色授予权限和为用户授予权限操作相同，可参考 [TiDB 权限管理](/privilege-management.md)。\n\n为 `app_read` 角色授予数据库 `app_db` 的读权限：\n\n{{< copyable \"sql\" >}}\n\n```sql\nGRANT SELECT ON app_db.* TO 'app_read'@'%';\n```\n\n为 `app_write` 角色授予数据库 `app_db` 的写权限：\n\n{{< copyable \"sql\" >}}\n\n```sql\nGRANT INSERT, UPDATE, DELETE ON app_db.* TO 'app_write'@'%';\n```\n\n为 `app_developer` 角色授予 `app_db` 数据库的全部权限：\n\n{{< copyable \"sql\" >}}\n\n```sql\nGRANT ALL ON app_db.* TO 'app_developer';\n```\n\n### 将角色授予给用户\n\n假设有一个用户拥有开发者角色，可以对 `app_db` 的所有操作权限；另外有两个用户拥有 `app_db` 的只读权限；还有一个用户拥有 `app_db` 的读写权限。\n\n首先用 `CREATE USER` 来创建用户。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE USER 'dev1'@'localhost' IDENTIFIED BY 'dev1pass';\nCREATE USER 'read_user1'@'localhost' IDENTIFIED BY 'read_user1pass';\nCREATE USER 'read_user2'@'localhost' IDENTIFIED BY 'read_user2pass';\nCREATE USER 'rw_user1'@'localhost' IDENTIFIED BY 'rw_user1pass';\n```\n\n然后使用 `GRANT` 授予用户对应的角色。\n\n{{< copyable \"sql\" >}}\n\n```sql\nGRANT 'app_developer' TO 'dev1'@'localhost';\nGRANT 'app_read' TO 'read_user1'@'localhost', 'read_user2'@'localhost';\nGRANT 'app_read', 'app_write' TO 'rw_user1'@'localhost';\n```\n\n用户执行将角色授予给其他用户或者收回角色的命令，需要用户拥有 `SUPER` 权限。将角色授予给用户时并不会启用该角色，启用角色需要额外的操作。\n\n以下操作可能会形成一个“关系环”：\n\n```sql\nCREATE USER 'u1', 'u2';\nCREATE ROLE 'r1', 'r2';\n\nGRANT 'u1' TO 'u1';\nGRANT 'r1' TO 'r1';\n\nGRANT 'r2' TO 'u2';\nGRANT 'u2' TO 'r2';\n```\n\nTiDB 允许这种多层授权关系存在，可以使用多层授权关系实现权限继承。\n\n### 查看角色拥有的权限\n\n可以通过 `SHOW GRANTS` 语句查看用户被授予了哪些角色。当用户查看其他用户权限相关信息时，需要对 `mysql` 数据库拥有 `SELECT` 权限。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW GRANTS FOR 'dev1'@'localhost';\n```\n\n```\n+-------------------------------------------------+\n| Grants for dev1@localhost                       |\n+-------------------------------------------------+\n| GRANT USAGE ON *.* TO `dev1`@`localhost`        |\n| GRANT `app_developer`@`%` TO `dev1`@`localhost` |\n+-------------------------------------------------+\n```\n\n可以通过使用 `SHOW GRANTS` 的 `USING` 选项来查看角色对应的权限。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW GRANTS FOR 'dev1'@'localhost' USING 'app_developer';\n```\n\n```\n+----------------------------------------------------------+\n| Grants for dev1@localhost                                |\n+----------------------------------------------------------+\n| GRANT USAGE ON *.* TO `dev1`@`localhost`                 |\n| GRANT ALL PRIVILEGES ON `app_db`.* TO `dev1`@`localhost` |\n| GRANT `app_developer`@`%` TO `dev1`@`localhost`          |\n+----------------------------------------------------------+\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW GRANTS FOR 'rw_user1'@'localhost' USING 'app_read', 'app_write';\n```\n\n```\n+------------------------------------------------------------------------------+\n| Grants for rw_user1@localhost                                                |\n+------------------------------------------------------------------------------+\n| GRANT USAGE ON *.* TO `rw_user1`@`localhost`                                 |\n| GRANT SELECT, INSERT, UPDATE, DELETE ON `app_db`.* TO `rw_user1`@`localhost` |\n| GRANT `app_read`@`%`,`app_write`@`%` TO `rw_user1`@`localhost`               |\n+------------------------------------------------------------------------------+\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW GRANTS FOR 'read_user1'@'localhost' USING 'app_read';\n```\n\n```\n+--------------------------------------------------------+\n| Grants for read_user1@localhost                        |\n+--------------------------------------------------------+\n| GRANT USAGE ON *.* TO `read_user1`@`localhost`         |\n| GRANT SELECT ON `app_db`.* TO `read_user1`@`localhost` |\n| GRANT `app_read`@`%` TO `read_user1`@`localhost`       |\n+--------------------------------------------------------+\n```\n\n可以使用 `SHOW GRANTS` 或 `SHOW GRANTS FOR CURRENT_USER()` 查看当前用户的权限。这两个语句有细微的差异，`SHOW GRANTS` 会显示当前用户的启用角色的权限，而 `SHOW GRANTS FOR CURRENT_USER()` 则不会显示启用角色的权限。\n\n### 设置默认启用角色\n\n角色在授予给用户之后，并不会生效；只有在用户启用了某些角色之后，才可以使用角色拥有的权限。\n\n可以对用户设置默认启用的角色；用户在登录时，默认启用的角色会被自动启用。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET DEFAULT ROLE\n    {NONE | ALL | role [, role ] ...}\n    TO user [, user ]\n```\n\n比如将 `app_read` 和 `app_write` 设置为 `rw_user1@localhost` 的默认启用角色：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET DEFAULT ROLE app_read, app_write TO 'rw_user1'@'localhost';\n```\n\n将 `dev1@localhost` 的所有角色，设为其默认启用角色：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET DEFAULT ROLE ALL TO 'dev1'@'localhost';\n```\n\n关闭 `dev1@localhost` 的所有默认启用角色：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET DEFAULT ROLE NONE TO 'dev1'@'localhost';\n```\n\n需要注意的是，设置为默认启用角色的角色必须已经授予给那个用户。\n\n### 在当前 session 启用角色\n\n除了使用 `SET DEFAULT ROLE` 启用角色外，TiDB 还提供让用户在当前 session 启用某些角色的功能。\n\n```sql\nSET ROLE {\n    DEFAULT\n  | NONE\n  | ALL\n  | ALL EXCEPT role [, role ] ...\n  | role [, role ] ...\n}\n```\n\n例如，登录 `rw_user1` 后，为当前用户启用角色 `app_read` 和 `app_write`，仅在当前 session 有效：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET ROLE 'app_read', 'app_write';\n```\n\n启用当前用户的默认角色：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET ROLE DEFAULT\n```\n\n启用授予给当前用户的所有角色：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET ROLE ALL\n```\n\n不启用任何角色：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET ROLE NONE\n```\n\n启用除 `app_read` 外的角色：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET ROLE ALL EXCEPT 'app_read'\n```\n\n> **注意：**\n>\n> 使用 `SET ROLE` 启用的角色只有在当前 session 才会有效。\n\n### 查看当前启用角色\n\n当前用户可以通过 `CURRENT_ROLE()` 函数查看当前用户启用了哪些角色。\n\n例如，先对 `rw_user1'@'localhost` 设置默认角色：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET DEFAULT ROLE ALL TO 'rw_user1'@'localhost';\n```\n\n用 `rw_user1@localhost` 登录后：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT CURRENT_ROLE();\n```\n\n```\n+--------------------------------+\n| CURRENT_ROLE()                 |\n+--------------------------------+\n| `app_read`@`%`,`app_write`@`%` |\n+--------------------------------+\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET ROLE 'app_read'; SELECT CURRENT_ROLE();\n```\n\n```\n+----------------+\n| CURRENT_ROLE() |\n+----------------+\n| `app_read`@`%` |\n+----------------+\n```\n\n### 收回角色\n\n解除角色 `app_read` 与用户 `read_user1@localhost`、`read_user2@localhost` 的授权关系。\n\n{{< copyable \"sql\" >}}\n\n```sql\nREVOKE 'app_read' FROM 'read_user1'@'localhost', 'read_user2'@'localhost';\n```\n\n解除角色 `app_read`、`app_write` 与用户 `rw_user1@localhost` 的授权关系。\n\n{{< copyable \"sql\" >}}\n\n```sql\nREVOKE 'app_read', 'app_write' FROM 'rw_user1'@'localhost';\n```\n\n解除角色授权具有原子性，如果在撤销授权操作中失败会回滚。\n\n### 收回权限\n\n`REVOKE` 语句与 `GRANT` 对应，可以使用 `REVOKE` 来撤销 `app_write` 的权限。\n\n{{< copyable \"sql\" >}}\n\n```sql\nREVOKE INSERT, UPDATE, DELETE ON app_db.* FROM 'app_write';\n```\n\n具体可参考 [TiDB 权限管理](/privilege-management.md)。\n\n### 删除角色\n\n删除角色 `app_read` 和 `app_write`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nDROP ROLE 'app_read', 'app_write';\n```\n\n这个操作会清除角色在 `mysql.user` 表里面的记录项，并且清除在授权表里面的相关记录，解除和其相关的授权关系。执行删除角色的用户需要拥有 `DROP ROLE` 或 `DROP USER` 权限。\n\n### 授权表\n\n在原有的四张[系统权限表](/privilege-management.md#授权表)的基础上，角色访问控制引入了两张新的系统表：\n\n- `mysql.role_edges`：记录角色与用户的授权关系\n- `mysql.default_roles`：记录每个用户默认启用的角色\n\n以下是 `mysql.role_edges` 所包含的数据。\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from mysql.role_edges;\n```\n\n```\n+-----------+-----------+---------+---------+-------------------+\n| FROM_HOST | FROM_USER | TO_HOST | TO_USER | WITH_ADMIN_OPTION |\n+-----------+-----------+---------+---------+-------------------+\n| %         | r_1       | %       | u_1     | N                 |\n+-----------+-----------+---------+---------+-------------------+\n1 row in set (0.00 sec)\n```\n\n其中 `FROM_HOST` 和 `FROM_USER` 分别表示角色的主机名和用户名，`TO_HOST` 和 `TO_USER` 分别表示被授予角色的用户的主机名和用户名。\n\n`mysql.default_roles` 中包含了每个用户默认启用了哪些角色。\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from mysql.default_roles;\n```\n\n```\n+------+------+-------------------+-------------------+\n| HOST | USER | DEFAULT_ROLE_HOST | DEFAULT_ROLE_USER |\n+------+------+-------------------+-------------------+\n| %    | u_1  | %                 | r_1               |\n| %    | u_1  | %                 | r_2               |\n+------+------+-------------------+-------------------+\n2 rows in set (0.00 sec)\n```\n\n`HOST` 和 `USER` 分别表示用户的主机名和用户名，`DEFAULT_ROLE_HOST` 和 `DEFAULT_ROLE_USER` 分别表示默认启用的角色的主机名和用户名。\n\n### 其他\n\n由于基于角色的访问控制模块和用户管理以及权限管理结合十分紧密，因此需要参考一些操作的细节：\n\n- [TiDB 权限管理](/privilege-management.md)\n- [TiDB 用户账户管理](/user-account-management.md)\n"
        },
        {
          "name": "runtime-filter.md",
          "type": "blob",
          "size": 30.501953125,
          "content": "---\ntitle: Runtime Filter\nsummary: 介绍 Runtime Filter 的原理及使用方式。\n---\n\n# Runtime Filter\n\nRuntime Filter 是 TiDB v7.3 引入的新功能，旨在提升 MPP 场景下 Hash Join 的性能。它通过动态生成 Filter 来提前过滤 Hash Join 的数据，从而减少运行时的数据扫描量以及 Hash Join 的计算量，最终提升查询性能。\n\n## 名词解释\n\n- Hash Join：一种实现 Join 关系代数的方式。它通过在 Join 的一侧构建 Hash Table 并在另一侧不断匹配 Hash Table 来得到 Join 的结果。\n- Build Side：Hash Join 中用于构建 Hash Table 的一侧，称为 Build Side。本文默认以 Join 的右表作为 Build Side。\n- Probe Side：Hash Join 中用于不断匹配 Hash Table 的一侧，称为 Probe Side。本文默认以 Join 的左表作为 Probe Side。\n- Filter：也称谓词，在本文中指过滤条件。\n\n## Runtime Filter 的原理\n\nHash Join 通过将右表的数据构建为 Hash Table，并将左表的数据不断匹配 Hash Table 来完成 Join。如果在匹配过程中，发现一部分 Join Key 值无法命中 Hash Table，则说明这部分数据不存在于右表，也不会出现在最终的 Join 结果中。因此，如果能够在扫描时**提前过滤掉这部分 Join Key 的数据**，将减少扫描时间和网络开销，从而大幅提升 Join 效率。\n\nRuntime Filter 是在查询规划阶段生成的一种**动态取值谓词**。该谓词与 TiDB Selection 中的其他谓词具有相同作用，都应用于 Table Scan 操作上，用于筛选不满足谓词条件的行为。唯一的区别在于，Runtime Filter 中的参数取值来自于 Hash Join 构建过程中产生的结果。\n\n### 示例\n\n假设当前存在 `store_sales` 表与 `date_dim` 表的 Join 查询，它的 Join 方式为 Hash Join。`store_sales` 是一张事实表，主要存储门店销售数据，行数为 100 万。`date_dim` 是一张时间维度表，主要存储日期信息。当前想要查询 2001 年的销售数据，则 `date_dim` 表参与 Join 的数据量为 365 行。\n\n```sql\nSELECT * FROM store_sales, date_dim\nWHERE ss_date_sk = d_date_sk\n      AND d_year = 2001;\n```\n\nHash Join 通常情况下的执行方式为：\n\n```\n                 +-------------------+\n                 | PhysicalHashJoin  |\n        +------->|                   |<------+\n        |        +-------------------+       |\n        |                                    |\n        |                                    |\n  100w  |                                    | 365\n        |                                    |\n        |                                    |\n+-------+-------+                   +--------+-------+\n| TableFullScan |                   | TableFullScan  |\n|  store_sales  |                   |    date_dim    |\n+---------------+                   +----------------+\n```\n\n*（上图为示意图，省略了 exchange 等节点）*\n\nRuntime Filter 的执行方式如下：\n\n1. 扫描 `date_dim` 的数据。\n2. PhysicalHashJoin 根据 Build Side 数据计算出一个过滤条件，比如 `date_dim in (2001/01/01~2001/12/31)`。\n3. 将该过滤条件发送给等待扫描 `store_sales` 的 TableFullScan。\n4. `store_sales` 应用该过滤条件，并将过滤后的数据传递给 PhysicalHashJoin，从而减少 Probe Side 的扫表数据量以及匹配 Hash Table 的计算量。\n\n```\n                         2. Build RF values\n            +-------->+-------------------+\n            |         |PhysicalHashJoin   |<-----+\n            |    +----+                   |      |\n4. After RF |    |    +-------------------+      | 1. Scan T2\n    5000    |    |3. Send RF                     |      365\n            |    | filter data                   |\n            |    |                               |\n      +-----+----v------+                +-------+--------+\n      |  TableFullScan  |                | TableFullScan  |\n      |  store_sales    |                |    date_dim    |\n      +-----------------+                +----------------+\n```\n\n* (上图中的 RF 是 Runtime Filter 的缩写) *\n\n对比以上两个图可以看出，`store_sales` 的扫描量从 100 万减少到了 5000。通过减少 Table Full Scan 扫描的数据量，Runtime Filter 可以减少匹配 Hash Table 的次数，避免不必要的 I/O 和网络传输，从而显著提升了 Join 操作的效率。\n\n## 使用 Runtime Filter\n\n要使用 Runtime Filter，只需创建带 TiFlash 副本的表，并将 [`tidb_runtime_filter_mode`](/system-variables.md#tidb_runtime_filter_mode-从-v720-版本开始引入) 设置为 `LOCAL`。\n\n本小节以 TPC-DS 的数据集为例，使用 `catalog_sales` 表和 `date_dim` 表进行 Join 操作，说明如何使用 Runtime Filter 提升查询效率。\n\n### 第 1 步：创建表的 TiFlash 副本\n\n给 `catalog_sales` 表和 `date_dim` 表分别增加一个 TiFlash 副本。\n\n```sql\nALTER TABLE catalog_sales SET tiflash REPLICA 1;\nALTER TABLE date_dim SET tiflash REPLICA 1;\n```\n\n等待一段时间，并检查两个表的 TiFlash 副本已准备就绪，即副本的 `AVAILABLE` 字段和 `PROGRESS` 字段均为 `1`。\n\n```sql\nSELECT * FROM INFORMATION_SCHEMA.TIFLASH_REPLICA WHERE TABLE_NAME='catalog_sales';\n+--------------+---------------+----------+---------------+-----------------+-----------+----------+\n| TABLE_SCHEMA | TABLE_NAME    | TABLE_ID | REPLICA_COUNT | LOCATION_LABELS | AVAILABLE | PROGRESS |\n+--------------+---------------+----------+---------------+-----------------+-----------+----------+\n| tpcds50      | catalog_sales |     1055 |             1 |                 |         1 |        1 |\n+--------------+---------------+----------+---------------+-----------------+-----------+----------+\n\nmysql> SELECT * FROM INFORMATION_SCHEMA.TIFLASH_REPLICA WHERE TABLE_NAME='date_dim';\n+--------------+------------+----------+---------------+-----------------+-----------+----------+\n| TABLE_SCHEMA | TABLE_NAME | TABLE_ID | REPLICA_COUNT | LOCATION_LABELS | AVAILABLE | PROGRESS |\n+--------------+------------+----------+---------------+-----------------+-----------+----------+\n| tpcds50      | date_dim   |     1015 |             1 |                 |         1 |        1 |\n+--------------+------------+----------+---------------+-----------------+-----------+----------+\n```\n\n### 第 2 步：开启 Runtime Filter\n\n将系统变量 [`tidb_runtime_filter_mode`](/system-variables.md#tidb_runtime_filter_mode-从-v720-版本开始引入) 的值设置为 `LOCAL`，即开启 Runtime Filter。\n\n```sql\nSET tidb_runtime_filter_mode=\"LOCAL\";\n```\n\n查看是否更改成功：\n\n```sql\nSHOW VARIABLES LIKE \"tidb_runtime_filter_mode\";\n+--------------------------+-------+\n| Variable_name            | Value |\n+--------------------------+-------+\n| tidb_runtime_filter_mode | LOCAL |\n+--------------------------+-------+\n```\n\n系统变量的值显示为 `LOCAL`，则表示已成功开启 Runtime Filter。\n\n### 第 3 步：执行查询\n\n在进行查询之前，先查看一下查询计划。使用 [`EXPLAIN` 语句](/sql-statements/sql-statement-explain.md)来检查 Runtime Filter 是否已正确生效。\n\n```sql\nEXPLAIN SELECT cs_ship_date_sk FROM catalog_sales, date_dim\nWHERE d_date = '2002-2-01' AND\n     cs_ship_date_sk = d_date_sk;\n```\n\n当 Runtime Filter 启用时，可以看到 HashJoin 节点和 TableScan 节点上分别挂载了对应的 Runtime Filter，表示 Runtime Filter 规划成功。\n\n```\nTableFullScan: runtime filter:0[IN] -> tpcds50.catalog_sales.cs_ship_date_sk\n\nHashJoin: runtime filter:0[IN] <- tpcds50.date_dim.d_date_sk |\n```\n\n完整的查询规划如下：\n\n```\n+----------------------------------------+-------------+--------------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| id                                     | estRows     | task         | access object       | operator info                                                                                                                                 |\n+----------------------------------------+-------------+--------------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n| TableReader_53                         | 37343.19    | root         |                     | MppVersion: 1, data:ExchangeSender_52                                                                                                         |\n| └─ExchangeSender_52                    | 37343.19    | mpp[tiflash] |                     | ExchangeType: PassThrough                                                                                                                     |\n|   └─Projection_51                      | 37343.19    | mpp[tiflash] |                     | tpcds50.catalog_sales.cs_ship_date_sk                                                                                                         |\n|     └─HashJoin_48                      | 37343.19    | mpp[tiflash] |                     | inner join, equal:[eq(tpcds50.date_dim.d_date_sk, tpcds50.catalog_sales.cs_ship_date_sk)], runtime filter:0[IN] <- tpcds50.date_dim.d_date_sk |\n|       ├─ExchangeReceiver_29(Build)     | 1.00        | mpp[tiflash] |                     |                                                                                                                                               |\n|       │ └─ExchangeSender_28            | 1.00        | mpp[tiflash] |                     | ExchangeType: Broadcast, Compression: FAST                                                                                                    |\n|       │   └─TableFullScan_26           | 1.00        | mpp[tiflash] | table:date_dim      | pushed down filter:eq(tpcds50.date_dim.d_date, 2002-02-01 00:00:00.000000), keep order:false                                                  |\n|       └─Selection_31(Probe)            | 71638034.00 | mpp[tiflash] |                     | not(isnull(tpcds50.catalog_sales.cs_ship_date_sk))                                                                                            |\n|         └─TableFullScan_30             | 71997669.00 | mpp[tiflash] | table:catalog_sales | pushed down filter:empty, keep order:false, runtime filter:0[IN] -> tpcds50.catalog_sales.cs_ship_date_sk                                     |\n+----------------------------------------+-------------+--------------+---------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+\n9 rows in set (0.01 sec)\n```\n\n此时执行 SQL 查询，即可应用 Runtime Filter。\n\n```sql\nSELECT cs_ship_date_sk FROM catalog_sales, date_dim\nWHERE d_date = '2002-2-01' AND\n     cs_ship_date_sk = d_date_sk;\n```\n\n### 第 4 步：性能对比\n\n以 TPC-DS 的 50 GB 数据量为例，开启 Runtime Filter 后，查询时间从 0.38 秒减少到 0.17 秒，效率提升 50%。通过 `ANALYZE` 语句可以查看 Runtime Filter 生效后各个算子的执行时间。\n\n以下为未开启 Runtime Filter 时查询的执行信息：\n\n```sql\nEXPLAIN ANALYZE SELECT cs_ship_date_sk FROM catalog_sales, date_dim WHERE d_date = '2002-2-01' AND cs_ship_date_sk = d_date_sk;\n+----------------------------------------+-------------+----------+--------------+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------+------+\n| id                                     | estRows     | actRows  | task         | access object       | execution info                                                                                                                                                                                                                                                                                                                                                                                    | operator info                                                                                | memory  | disk |\n+----------------------------------------+-------------+----------+--------------+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------+------+\n| TableReader_53                         | 37343.19    | 59574    | root         |                     | time:379.7ms, loops:83, RU:0.000000, cop_task: {num: 48, max: 0s, min: 0s, avg: 0s, p95: 0s, copr_cache_hit_ratio: 0.00}                                                                                                                                                                                                                                                                          | MppVersion: 1, data:ExchangeSender_52                                                        | 12.0 KB | N/A  |\n| └─ExchangeSender_52                    | 37343.19    | 59574    | mpp[tiflash] |                     | tiflash_task:{proc max:377ms, min:375.3ms, avg: 376.1ms, p80:377ms, p95:377ms, iters:1160, tasks:2, threads:16}                                                                                                                                                                                                                                                                                   | ExchangeType: PassThrough                                                                    | N/A     | N/A  |\n|   └─Projection_51                      | 37343.19    | 59574    | mpp[tiflash] |                     | tiflash_task:{proc max:377ms, min:375.3ms, avg: 376.1ms, p80:377ms, p95:377ms, iters:1160, tasks:2, threads:16}                                                                                                                                                                                                                                                                                   | tpcds50.catalog_sales.cs_ship_date_sk                                                        | N/A     | N/A  |\n|     └─HashJoin_48                      | 37343.19    | 59574    | mpp[tiflash] |                     | tiflash_task:{proc max:377ms, min:375.3ms, avg: 376.1ms, p80:377ms, p95:377ms, iters:1160, tasks:2, threads:16}                                                                                                                                                                                                                                                                                   | inner join, equal:[eq(tpcds50.date_dim.d_date_sk, tpcds50.catalog_sales.cs_ship_date_sk)]    | N/A     | N/A  |\n|       ├─ExchangeReceiver_29(Build)     | 1.00        | 2        | mpp[tiflash] |                     | tiflash_task:{proc max:291.3ms, min:290ms, avg: 290.6ms, p80:291.3ms, p95:291.3ms, iters:2, tasks:2, threads:16}                                                                                                                                                                                                                                                                                  |                                                                                              | N/A     | N/A  |\n|       │ └─ExchangeSender_28            | 1.00        | 1        | mpp[tiflash] |                     | tiflash_task:{proc max:290.9ms, min:0s, avg: 145.4ms, p80:290.9ms, p95:290.9ms, iters:1, tasks:2, threads:1}                                                                                                                                                                                                                                                                                      | ExchangeType: Broadcast, Compression: FAST                                                   | N/A     | N/A  |\n|       │   └─TableFullScan_26           | 1.00        | 1        | mpp[tiflash] | table:date_dim      | tiflash_task:{proc max:3.88ms, min:0s, avg: 1.94ms, p80:3.88ms, p95:3.88ms, iters:1, tasks:2, threads:1}, tiflash_scan:{dtfile:{total_scanned_packs:2, total_skipped_packs:12, total_scanned_rows:16384, total_skipped_rows:97625, total_rs_index_load_time: 0ms, total_read_time: 0ms}, total_create_snapshot_time: 0ms, total_local_region_num: 1, total_remote_region_num: 0}                  | pushed down filter:eq(tpcds50.date_dim.d_date, 2002-02-01 00:00:00.000000), keep order:false | N/A     | N/A  |\n|       └─Selection_31(Probe)            | 71638034.00 | 71638034 | mpp[tiflash] |                     | tiflash_task:{proc max:47ms, min:34.3ms, avg: 40.6ms, p80:47ms, p95:47ms, iters:1160, tasks:2, threads:16}                                                                                                                                                                                                                                                                                        | not(isnull(tpcds50.catalog_sales.cs_ship_date_sk))                                           | N/A     | N/A  |\n|         └─TableFullScan_30             | 71997669.00 | 71997669 | mpp[tiflash] | table:catalog_sales | tiflash_task:{proc max:34ms, min:17.3ms, avg: 25.6ms, p80:34ms, p95:34ms, iters:1160, tasks:2, threads:16}, tiflash_scan:{dtfile:{total_scanned_packs:8893, total_skipped_packs:4007, total_scanned_rows:72056474, total_skipped_rows:32476901, total_rs_index_load_time: 8ms, total_read_time: 579ms}, total_create_snapshot_time: 0ms, total_local_region_num: 194, total_remote_region_num: 0} | pushed down filter:empty, keep order:false                                                   | N/A     | N/A  |\n+----------------------------------------+-------------+----------+--------------+---------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------+------+\n9 rows in set (0.38 sec)\n```\n\n以下为开启 Runtime Filter 后的查询 Summary：\n\n```sql\nEXPLAIN ANALYZE SELECT cs_ship_date_sk FROM catalog_sales, date_dim\n    -> WHERE d_date = '2002-2-01' AND\n    ->      cs_ship_date_sk = d_date_sk;\n+----------------------------------------+-------------+---------+--------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------+------+\n| id                                     | estRows     | actRows | task         | access object       | execution info                                                                                                                                                                                                                                                                                                                                                                                       | operator info                                                                                                                                 | memory  | disk |\n+----------------------------------------+-------------+---------+--------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------+------+\n| TableReader_53                         | 37343.19    | 59574   | root         |                     | time:162.1ms, loops:82, RU:0.000000, cop_task: {num: 47, max: 0s, min: 0s, avg: 0s, p95: 0s, copr_cache_hit_ratio: 0.00}                                                                                                                                                                                                                                                                             | MppVersion: 1, data:ExchangeSender_52                                                                                                         | 12.7 KB | N/A  |\n| └─ExchangeSender_52                    | 37343.19    | 59574   | mpp[tiflash] |                     | tiflash_task:{proc max:160.8ms, min:154.3ms, avg: 157.6ms, p80:160.8ms, p95:160.8ms, iters:86, tasks:2, threads:16}                                                                                                                                                                                                                                                                                  | ExchangeType: PassThrough                                                                                                                     | N/A     | N/A  |\n|   └─Projection_51                      | 37343.19    | 59574   | mpp[tiflash] |                     | tiflash_task:{proc max:160.8ms, min:154.3ms, avg: 157.6ms, p80:160.8ms, p95:160.8ms, iters:86, tasks:2, threads:16}                                                                                                                                                                                                                                                                                  | tpcds50.catalog_sales.cs_ship_date_sk                                                                                                         | N/A     | N/A  |\n|     └─HashJoin_48                      | 37343.19    | 59574   | mpp[tiflash] |                     | tiflash_task:{proc max:160.8ms, min:154.3ms, avg: 157.6ms, p80:160.8ms, p95:160.8ms, iters:86, tasks:2, threads:16}                                                                                                                                                                                                                                                                                  | inner join, equal:[eq(tpcds50.date_dim.d_date_sk, tpcds50.catalog_sales.cs_ship_date_sk)], runtime filter:0[IN] <- tpcds50.date_dim.d_date_sk | N/A     | N/A  |\n|       ├─ExchangeReceiver_29(Build)     | 1.00        | 2       | mpp[tiflash] |                     | tiflash_task:{proc max:132.3ms, min:130.8ms, avg: 131.6ms, p80:132.3ms, p95:132.3ms, iters:2, tasks:2, threads:16}                                                                                                                                                                                                                                                                                   |                                                                                                                                               | N/A     | N/A  |\n|       │ └─ExchangeSender_28            | 1.00        | 1       | mpp[tiflash] |                     | tiflash_task:{proc max:131ms, min:0s, avg: 65.5ms, p80:131ms, p95:131ms, iters:1, tasks:2, threads:1}                                                                                                                                                                                                                                                                                                | ExchangeType: Broadcast, Compression: FAST                                                                                                    | N/A     | N/A  |\n|       │   └─TableFullScan_26           | 1.00        | 1       | mpp[tiflash] | table:date_dim      | tiflash_task:{proc max:3.01ms, min:0s, avg: 1.51ms, p80:3.01ms, p95:3.01ms, iters:1, tasks:2, threads:1}, tiflash_scan:{dtfile:{total_scanned_packs:2, total_skipped_packs:12, total_scanned_rows:16384, total_skipped_rows:97625, total_rs_index_load_time: 0ms, total_read_time: 0ms}, total_create_snapshot_time: 0ms, total_local_region_num: 1, total_remote_region_num: 0}                     | pushed down filter:eq(tpcds50.date_dim.d_date, 2002-02-01 00:00:00.000000), keep order:false                                                  | N/A     | N/A  |\n|       └─Selection_31(Probe)            | 71638034.00 | 5308995 | mpp[tiflash] |                     | tiflash_task:{proc max:39.8ms, min:24.3ms, avg: 32.1ms, p80:39.8ms, p95:39.8ms, iters:86, tasks:2, threads:16}                                                                                                                                                                                                                                                                                       | not(isnull(tpcds50.catalog_sales.cs_ship_date_sk))                                                                                            | N/A     | N/A  |\n|         └─TableFullScan_30             | 71997669.00 | 5335549 | mpp[tiflash] | table:catalog_sales | tiflash_task:{proc max:36.8ms, min:23.3ms, avg: 30.1ms, p80:36.8ms, p95:36.8ms, iters:86, tasks:2, threads:16}, tiflash_scan:{dtfile:{total_scanned_packs:660, total_skipped_packs:12451, total_scanned_rows:5335549, total_skipped_rows:100905778, total_rs_index_load_time: 2ms, total_read_time: 47ms}, total_create_snapshot_time: 0ms, total_local_region_num: 194, total_remote_region_num: 0} | pushed down filter:empty, keep order:false, runtime filter:0[IN] -> tpcds50.catalog_sales.cs_ship_date_sk                                     | N/A     | N/A  |\n+----------------------------------------+-------------+---------+--------------+---------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------+---------+------+\n9 rows in set (0.17 sec)\n```\n\n对比两个查询的执行信息，可发现以下改进：\n\n* IO 减少：对比 TableFullScan 算子的 `total_scanned_rows` 可知，开启 Runtime Filter 后 TableFullScan 的扫描量减少了 2/3 。\n* Hash Join 性能提升：HashJoin 算子的执行耗时从 376.1ms 减少至 157.6ms。\n\n### 最佳实践\n\nRuntime Filter 适用于大表和小表进行 Join 的情况，比如事实表和维度表的关联查询。当维度表的命中的数据量较少时，意味着 Filter 的取值较少，事实表能更多地过滤掉不满足条件的数据。与默认情况下扫描整个事实表相比，这将显著提高查询性能。\n\n例如，在 TPC-DS 中，泛 `Sales` 表和 `date_dim` 表的 Join 就是一个典型例子。\n\n## 配置 Runtime Filter\n\n在使用 Runtime Filter 时，你可以配置 Runtime Filter 的模式和谓词的类型。\n\n### Runtime Filter Mode\n\nRuntime Filter Mode 指的是 Runtime Filter 的模式，即 **生成 Filter 算子** 和 **接收 Filter 算子**之间的关系。共有三种模式：`OFF`、`LOCAL`、`GLOBAL`。在 v7.3.0 中仅支持 `OFF` 和 `LOCAL` 模式，通过系统变量 [`tidb_runtime_filter_mode`](/system-variables.md#tidb_runtime_filter_mode-从-v720-版本开始引入) 控制。\n\n+ `OFF`：代表关闭 Runtime Filter。关闭后，查询行为和过去完全一致。\n+ `LOCAL`：开启 LOCAL 模式的 Runtime Filter。LOCAL 模式指的是**生成 Filter 的算子**和**接收 Filter 的算子**在同一个 MPP Task 中。简单来说，Runtime Filter 可应用于 HashJoin 算子和 TableScan 算子在同一个 Task 中的情况。目前 Runtime Filter 仅支持 LOCAL 模式，要开启该模式，设置为 `LOCAL` 即可。\n+ `GLOBAL`：目前不支持 GLOBAL 模式，不可设置为该模式。\n\n### Runtime Filter Type\n\nRuntime Filter Type 指的是 Runtime Filter 谓词的类型，即生成的 Filter 算子使用的谓词类型。目前只支持一种类型：`IN`，即生成的谓词类似于 `k1 in (xxx)`。通过系统变量 [`tidb_runtime_filter_type`](/system-variables.md#tidb_runtime_filter_type-从-v720-版本开始引入) 控制。\n\n+ `IN`：默认为 `IN` 类型。即生成的 Runtime Filter 使用 `IN` 类型的谓词。\n\n## 限制\n\n+ Runtime Filter 是 MPP 架构下的优化，仅可应用于下推到 TiFlash 的查询。\n+ Join 类型：Left outer、Full outer、Anti join（当左表为 Probe Side 时）均不支持生成 Runtime Filter。由于 Runtime Filter 会提前过滤参与 Join 的数据，这些类型的 Join 不会丢弃未匹配上的数据，所以无法使用该优化。\n+ Equal Join expression：当等值 Join 表达式中的 Probe 列为复杂表达式，或者其类型为 JSON、Blob、Array 等复合类型时，也不会生成 Runtime Filter。主要原因是这类 Column 很少作为 Equal Join 的关联列，并且即使生成了 Filter，过滤率通常很低。\n\n对于以上限制，如果你需要确认是否正确生成了 Runtime Filter，可以通过 [`EXPLAIN` 语句](/sql-statements/sql-statement-explain.md) 来验证。\n"
        },
        {
          "name": "scale-microservices-using-tiup.md",
          "type": "blob",
          "size": 9.673828125,
          "content": "---\ntitle: 使用 TiUP 扩容缩容 PD 微服务节点\nsummary: 介绍如何使用 TiUP 扩容缩容集群中的 PD 微服务节点，以及如何切换 PD 工作模式。\n---\n\n# 使用 TiUP 扩容缩容 PD 微服务节点\n\n本文介绍如何使用 TiUP 扩容缩容集群中的 [PD 微服务](/pd-microservices.md)节点（包括 TSO 节点和 Scheduling 节点），以及如何切换 PD 工作模式。\n\n你可以通过 `tiup cluster list` 查看当前的集群名称列表。\n\n例如，集群原拓扑结构如下所示：\n\n| 主机 IP   | 服务   |\n|:----|:----|\n| 10.0.1.4   | TiDB + PD   |\n| 10.0.1.5   | TiKV + Monitor   |\n| 10.0.1.1   | TiKV   |\n| 10.0.1.2   | TiKV   |\n| 10.0.1.6   | TSO   |\n| 10.0.1.7   | Scheduling   |\n\n## 扩容 TSO/Scheduling 节点\n\n> **注意：**\n>\n> 对于尚未开启 PD 微服务的集群，如需添加 TSO/Scheduling 节点，请参考[从常规模式切换为微服务模式](#从常规模式切换为微服务模式)中的步骤进行操作。\n\n对于已开启 PD 微服务的集群，如需添加一个 IP 地址为 10.0.1.8 的 TSO 节点和一个 IP 地址为 10.0.1.9 的 Scheduling 节点，可以按照如下步骤进行操作。\n\n### 1. 编写扩容拓扑配置\n\n> **注意：**\n>\n> - 默认情况下，可以不填写端口以及目录信息。但在单机多实例场景下，则需要分配不同的端口以及目录，如果有端口或目录冲突，会在部署或扩容时提醒。\n>\n> - 从 TiUP v1.0.0 开始，扩容配置会继承原集群配置的 global 部分。\n\n在 scale-out.yml 文件添加扩容拓扑配置：\n\n```shell\nvi scale-out.yml\n```\n\nTSO 配置参考：\n\n```ini\ntso_servers:\n  - host: 10.0.1.8\n    port: 3379\n```\n\nScheduling 配置参考：\n\n```ini\nscheduling_servers:\n  - host: 10.0.1.9\n    port: 3379\n```\n\n可以使用 `tiup cluster edit-config <cluster-name>` 查看当前集群的配置信息，因为其中的 `global` 和 `server_configs` 参数配置默认会被 `scale-out.yml` 继承，因此也会在 `scale-out.yml` 中生效。\n\n### 2. 执行扩容命令\n\n执行 scale-out 命令前，先使用 `check` 及 `check --apply` 命令，检查和自动修复集群存在的潜在风险：\n\n1. 检查集群存在的潜在风险：\n\n    ```shell\n    tiup cluster check <cluster-name> scale-out.yml --cluster --user root [-p] [-i /home/root/.ssh/gcp_rsa]\n    ```\n\n2. 自动修复集群存在的潜在风险：\n\n    ```shell\n    tiup cluster check <cluster-name> scale-out.yml --cluster --apply --user root [-p] [-i /home/root/.ssh/gcp_rsa]\n    ```\n\n3. 执行 `scale-out` 命令扩容 TiDB 集群：\n\n    ```shell\n    tiup cluster scale-out <cluster-name> scale-out.yml [-p] [-i /home/root/.ssh/gcp_rsa]\n    ```\n\n以上操作示例中：\n\n- 扩容配置文件为 `scale-out.yml`。\n- `--user root` 表示通过 root 用户登录到目标主机完成集群部署，该用户需要有 ssh 到目标机器的权限，并且在目标机器有 sudo 权限。也可以用其他有 ssh 和 sudo 权限的用户完成部署。\n- [-i] 及 [-p] 为可选项，如果已经配置免密登录目标机，则无需填写。否则选择其一即可，[-i] 为可登录到目标机的 root 用户（或 --user 指定的其他用户）的私钥，也可使用 [-p] 交互式输入该用户的密码。\n\n预期日志结尾输出 ```Scaled cluster `<cluster-name>` out successfully``` 信息，表示扩容操作成功。\n\n### 3. 查看集群状态\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n打开浏览器访问监控平台 <http://10.0.1.5:3000>，监控整个集群和新增节点的状态。\n\n扩容后，集群拓扑结构如下所示：\n\n| 主机 IP   | 服务   |\n|:----|:----|\n| 10.0.1.4   | TiDB + PD   |\n| 10.0.1.5   | TiKV + Monitor   |\n| 10.0.1.1   | TiKV   |\n| 10.0.1.2   | TiKV   |\n| 10.0.1.6   | TSO   |\n| 10.0.1.7   | Scheduling   |\n| 10.0.1.8   | TSO   |\n| 10.0.1.9   | Scheduling   |\n\n## 缩容 TSO/Scheduling 节点\n\n> **注意：**\n>\n> 对于已开启 PD 微服务的集群，如需切换为非微服务模式，请参考[从微服务模式切换为常规模式](#从微服务模式切换为常规模式)中的步骤进行操作。\n\n对于包含多个 TSO 或 Scheduling 节点的集群，如需移除 IP 地址为 10.0.1.8 的 TSO 节点和 IP 地址为 10.0.1.9 的 Scheduling 节点，可以按照如下步骤进行操作。\n\n### 1. 查看节点 ID 信息\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n```\nStarting /root/.tiup/components/cluster/v1.16/cluster display <cluster-name>\n\nTiDB Cluster: <cluster-name>\n\nTiDB Version: v8.5.0\n\nID       Role         Host    Ports                            Status  Data Dir        Deploy Dir\n\n--       ----         ----      -----                            ------  --------        ----------\n\n10.0.1.4:2379  pd           10.0.1.4    2379/2380                        Healthy data/pd-2379      deploy/pd-2379\n\n10.0.1.1:20160 tikv         10.0.1.1    20160/20180                      Up      data/tikv-20160     deploy/tikv-20160\n\n10.0.1.2:20160 tikv         10.0.1.2    20160/20180                      Up      data/tikv-20160     deploy/tikv-20160\n\n10.0.1.5:20160 tikv        10.0.1.5    20160/20180                     Up      data/tikv-20160     deploy/tikv-20160\n\n10.0.1.4:4000  tidb        10.0.1.4    4000/10080                      Up      -                 deploy/tidb-4000\n\n10.0.1.5:9090  prometheus   10.0.1.5    9090                             Up      data/prometheus-9090  deploy/prometheus-9090\n\n10.0.1.5:3000  grafana      10.0.1.5    3000                             Up      -            deploy/grafana-3000\n\n10.0.1.5:9093  alertmanager 10.0.1.5    9093/9094                        Up      data/alertmanager-9093 deploy/alertmanager-9093\n\n10.0.1.6:3379  tso          10.0.1.6    3379                            Up|P     data/tso-3379     deploy/tso-3379\n\n10.0.1.8:3379  tso          10.0.1.8    3379                            Up       data/tso-3379    deploy/tso-3379\n\n10.0.1.7:3379  scheduling   10.0.1.7    3379                            Up|P     data/scheduling-3379     deploy/scheduling-3379\n\n10.0.1.9:3379  scheduling   10.0.1.9    3379                            Up       data/scheduling-3379     deploy/scheduling-3379\n```\n\n### 2. 执行缩容操作\n\n```shell\ntiup cluster scale-in <cluster-name> --node 10.0.1.8:3379\ntiup cluster scale-in <cluster-name> --node 10.0.1.9:3379\n```\n\n其中 `--node` 参数为需要下线节点的 ID。\n\n预期输出 Scaled cluster `<cluster-name>` in successfully 信息，表示缩容操作成功。\n\n### 3. 查看集群状态\n\n执行如下命令检查节点是否下线成功：\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n打开浏览器访问监控平台 <http://10.0.1.5:3000>，监控整个集群的状态。\n\n调整后，拓扑结构如下：\n\n| 主机 IP   | 服务   |\n|:----|:----|\n| 10.0.1.4   | TiDB + PD   |\n| 10.0.1.5   | TiKV + Monitor   |\n| 10.0.1.1   | TiKV   |\n| 10.0.1.2   | TiKV   |\n| 10.0.1.6   | TSO   |\n| 10.0.1.7   | Scheduling   |\n\n## 切换 PD 工作模式\n\nPD 服务支持在以下两种工作模式之间进行切换：\n\n- 常规模式：由 PD 节点自身提供路由、时间戳分配和集群调度功能。\n- 微服务模式：支持将 PD 的时间戳分配和集群调度功能单独部署到 TSO 节点（提供 `tso` 微服务）和 Scheduling 节点（提供 `scheduling` 微服务），从而与 PD 的路由功能解耦，让 PD 专注于元数据的路由服务。\n\n> **注意：**\n>\n> 在模式切换期间，PD 服务会出现分钟级别不可用的情况。\n\n### 从常规模式切换为微服务模式\n\n对于尚未开启 PD 微服务的集群，如需切换为 PD 微服务模式，并添加一个 IP 地址为 10.0.1.8 的 TSO 节点和一个 IP 地址为 10.0.1.9 的 Scheduling 节点，可以按照如下步骤进行操作。\n\n1. 编写扩容拓扑配置：\n\n    ```shell\n    vi scale-out.yml\n    ```\n\n    配置参考：\n\n    ```ini\n    tso_servers:\n      - host: 10.0.1.8\n        port: 3379\n    scheduling_servers:\n      - host: 10.0.1.9\n        port: 3379\n    ```\n\n2. 修改集群配置，将集群切换为 PD 微服务模式：\n\n    ```shell\n    tiup cluster edit-config <cluster-name>\n    ```\n\n    在 `global` 中添加 `pd_mode: ms` 配置：\n\n    ```ini\n    global:\n      user: tidb\n      ssh_port: 22\n      listen_host: 0.0.0.0\n      deploy_dir: /tidb-deploy\n      data_dir: /tidb-data\n      os: linux\n      arch: amd64\n      systemd_mode: system\n      pd_mode: ms\n    ```\n\n3. 滚动更新 PD 节点配置：\n\n    ```shell\n    tiup cluster reload <cluster-name> -R pd\n    ```\n\n    > **注意：**\n    >\n    > 从执行 `reload` 命令开始到下一步的 `scale-out` 命令执行结束期间，PD 时间戳分配服务将不可用。\n\n4. 扩容 PD 微服务节点：\n\n    ```shell\n    tiup cluster scale-out <cluster-name> scale-out.yml\n    ```\n\n### 从微服务模式切换为常规模式\n\n对于已开启 PD 微服务的集群，假设该集群包含一个 IP 地址为 10.0.1.8 的 TSO 节点和一个 IP 地址为 10.0.1.9 的 Scheduling 节点，如需切换为非微服务模式，可以按照如下步骤进行操作。\n\n1. 修改集群配置，将集群切换为非 PD 微服务模式：\n\n    ```shell\n    tiup cluster edit-config <cluster-name>\n    ```\n\n    在 `global` 中删除 `pd_mode: ms` 配置：\n\n    ```ini\n    global:\n      user: tidb\n      ssh_port: 22\n      listen_host: 0.0.0.0\n      deploy_dir: /tidb-deploy\n      data_dir: /tidb-data\n      os: linux\n      arch: amd64\n      systemd_mode: system\n    ```\n\n2. 缩容集群中所有的 PD 微服务节点：\n\n    ```shell\n    tiup cluster scale-in <cluster-name> --node 10.0.1.8:3379,10.0.1.9:3379\n    ```\n\n    > **注意：**\n    >\n    > 从执行 `scale-in` 命令开始到下一步的 `reload` 命令执行结束期间，PD 时间戳分配服务将不可用。\n\n3. 滚动更新 PD 节点配置：\n\n    ```shell\n    tiup cluster reload <cluster-name> -R pd\n    ```"
        },
        {
          "name": "scale-tidb-using-tiup.md",
          "type": "blob",
          "size": 17.3291015625,
          "content": "---\ntitle: 使用 TiUP 扩容缩容 TiDB 集群\naliases: ['/docs-cn/dev/scale-tidb-using-tiup/','/docs-cn/dev/how-to/scale/with-tiup/','/docs-cn/dev/reference/tiflash/scale/']\nsummary: TiUP 可以在不中断线上服务的情况下扩容和缩容 TiDB 集群。使用 `tiup cluster list` 查看当前集群名称列表。扩容 TiDB/PD/TiKV 节点需要编写扩容拓扑配置，并执行扩容命令。扩容后，使用 `tiup cluster display <cluster-name>` 检查集群状态。缩容 TiDB/PD/TiKV 节点需要查看节点 ID 信息，执行缩容操作，然后检查集群状态。缩容 TiFlash/TiCDC 节点也需要执行相似的操作。\n---\n\n# 使用 TiUP 扩容缩容 TiDB 集群\n\nTiDB 集群可以在不中断线上服务的情况下进行扩容和缩容。\n\n本文介绍如何使用 TiUP 扩容缩容集群中的 TiDB、TiKV、PD、TiCDC 或者 TiFlash 节点。如未安装 TiUP，可参考[部署文档中的步骤](/production-deployment-using-tiup.md#第-2-步在中控机上部署-tiup-组件)。\n\n你可以通过 `tiup cluster list` 查看当前的集群名称列表。\n\n例如，集群原拓扑结构如下所示：\n\n| 主机 IP   | 服务   |\n|:----|:----|\n| 10.0.1.3   | TiDB + TiFlash  |\n| 10.0.1.4   | TiDB + PD   |\n| 10.0.1.5   | TiKV + Monitor   |\n| 10.0.1.1   | TiKV   |\n| 10.0.1.2   | TiKV   |\n\n## 扩容 TiDB/PD/TiKV 节点\n\n如果要添加一个 TiDB 节点，IP 地址为 10.0.1.5，可以按照如下步骤进行操作。\n\n> **注意：**\n>\n> 添加 PD 节点和添加 TiDB 节点的步骤类似。添加 TiKV 节点前，建议预先根据集群的负载情况调整 PD 调度参数。\n\n### 1. 编写扩容拓扑配置\n\n> **注意：**\n>\n> - 默认情况下，可以不填写端口以及目录信息。但在单机多实例场景下，则需要分配不同的端口以及目录，如果有端口或目录冲突，会在部署或扩容时提醒。\n>\n> - 从 TiUP v1.0.0 开始，扩容配置会继承原集群配置的 global 部分。\n\n在 scale-out.yml 文件添加扩容拓扑配置：\n\n```shell\nvi scale-out.yml\n```\n\n{{< copyable \"\" >}}\n\n```ini\ntidb_servers:\n  - host: 10.0.1.5\n    ssh_port: 22\n    port: 4000\n    status_port: 10080\n    deploy_dir: /tidb-deploy/tidb-4000\n    log_dir: /tidb-deploy/tidb-4000/log\n```\n\nTiKV 配置文件参考：\n\n{{< copyable \"\" >}}\n\n```ini\ntikv_servers:\n  - host: 10.0.1.5\n    ssh_port: 22\n    port: 20160\n    status_port: 20180\n    deploy_dir: /tidb-deploy/tikv-20160\n    data_dir: /tidb-data/tikv-20160\n    log_dir: /tidb-deploy/tikv-20160/log\n```\n\nPD 配置文件参考：\n\n{{< copyable \"\" >}}\n\n```ini\npd_servers:\n  - host: 10.0.1.5\n    ssh_port: 22\n    name: pd-1\n    client_port: 2379\n    peer_port: 2380\n    deploy_dir: /tidb-deploy/pd-2379\n    data_dir: /tidb-data/pd-2379\n    log_dir: /tidb-deploy/pd-2379/log\n```\n\n可以使用 `tiup cluster edit-config <cluster-name>` 查看当前集群的配置信息，因为其中的 `global` 和 `server_configs` 参数配置默认会被 `scale-out.yml` 继承，因此也会在 `scale-out.yml` 中生效。\n\n### 2. 执行扩容命令\n\n执行 scale-out 命令前，先使用 `check` 及 `check --apply` 命令，检查和自动修复集群存在的潜在风险：\n\n> **注意：**\n>\n> 针对 scale-out 命令的检查功能在 tiup cluster v1.9.3 及后续版本中支持，请操作前先升级 tiup cluster 版本。\n\n（1）检查集群存在的潜在风险：\n\n  {{< copyable \"shell-regular\" >}}\n\n  ```shell\n  tiup cluster check <cluster-name> scale-out.yml --cluster --user root [-p] [-i /home/root/.ssh/gcp_rsa]\n  ```\n\n（2）自动修复集群存在的潜在风险：\n\n  {{< copyable \"shell-regular\" >}}\n\n  ```shell\n  tiup cluster check <cluster-name> scale-out.yml --cluster --apply --user root [-p] [-i /home/root/.ssh/gcp_rsa]\n  ```\n\n（3）执行 scale-out 命令扩容 TiDB 集群：\n\n  {{< copyable \"shell-regular\" >}}\n\n  ```shell\n  tiup cluster scale-out <cluster-name> scale-out.yml [-p] [-i /home/root/.ssh/gcp_rsa]\n  ```\n\n以上操作示例中：\n\n- 扩容配置文件为 `scale-out.yml`。\n- `--user root` 表示通过 root 用户登录到目标主机完成集群部署，该用户需要有 ssh 到目标机器的权限，并且在目标机器有 sudo 权限。也可以用其他有 ssh 和 sudo 权限的用户完成部署。\n- [-i] 及 [-p] 为可选项，如果已经配置免密登录目标机，则不需填写。否则选择其一即可，[-i] 为可登录到目标机的 root 用户（或 --user 指定的其他用户）的私钥，也可使用 [-p] 交互式输入该用户的密码。\n\n预期日志结尾输出 ```Scaled cluster `<cluster-name>` out successfully``` 信息，表示扩容操作成功。\n\n### 3. 刷新集群配置\n\n> **注意：**\n>\n> 该操作仅需在扩容 PD 节点时执行，扩容 TiDB 或 TiKV 节点时无需执行。\n\n1. 更新集群配置：\n\n    ```shell\n    tiup cluster reload <cluster-name> --skip-restart\n    ```\n\n2. 更新 Prometheus 配置并重启：\n\n    > **注意：**\n    >\n    > 如果你使用的是 TiUP v1.15.0 及之后版本，请跳过此步骤；如果你使用的 TiUP 版本早于 v1.15.0，则需要执行以下命令来更新 Prometheus 配置并重启。\n\n    ```shell\n    tiup cluster reload <cluster-name> -R prometheus\n    ```\n\n### 4. 查看集群状态\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n打开浏览器访问监控平台 <http://10.0.1.5:3000>，监控整个集群和新增节点的状态。\n\n扩容后，集群拓扑结构如下所示：\n\n| 主机 IP   | 服务   |\n|:----|:----|\n| 10.0.1.3   | TiDB + TiFlash  |\n| 10.0.1.4   | TiDB + PD   |\n| 10.0.1.5   | **TiDB** + TiKV + Monitor   |\n| 10.0.1.1   | TiKV   |\n| 10.0.1.2   | TiKV   |\n\n## 扩容 TiFlash 节点\n\n如果要添加一个 TiFlash 节点，其 IP 地址为 `10.0.1.4`，可以按照如下步骤进行操作。\n\n> **注意：**\n>\n> 在原有 TiDB 集群上新增 TiFlash 组件需要注意：\n>\n> 1. 首先确认当前 TiDB 的版本支持 TiFlash，否则需要先升级 TiDB 集群至 v5.0 以上版本。\n> 2. 执行 `tiup ctl:v<CLUSTER_VERSION> pd -u http://<pd_ip>:<pd_port> config set enable-placement-rules true` 命令，以开启 PD 的 Placement Rules 功能。或通过 [pd-ctl](/pd-control.md) 执行对应的命令。\n\n### 1. 添加节点信息到 scale-out.yml 文件\n\n编写 scale-out.yml 文件，添加该 TiFlash 节点信息（目前只支持 ip，不支持域名）：\n\n{{< copyable \"\" >}}\n\n```ini\ntiflash_servers:\n  - host: 10.0.1.4\n```\n\n### 2. 运行扩容命令\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster scale-out <cluster-name> scale-out.yml\n```\n\n> **注意：**\n>\n> 此处假设当前执行命令的用户和新增的机器打通了互信，如果不满足已打通互信的条件，需要通过 `-p` 来输入新机器的密码，或通过 `-i` 指定私钥文件。\n\n### 3. 查看集群状态\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n打开浏览器访问监控平台 <http://10.0.1.5:3000>，监控整个集群和新增节点的状态。\n\n扩容后，集群拓扑结构如下所示：\n\n| 主机 IP   | 服务   |\n|:----|:----|\n| 10.0.1.3   | TiDB + TiFlash  |\n| 10.0.1.4   | TiDB + PD + **TiFlash**    |\n| 10.0.1.5   | TiDB+ TiKV + Monitor   |\n| 10.0.1.1   | TiKV   |\n| 10.0.1.2   | TiKV   |\n\n## 扩容 TiCDC 节点\n\n如果要添加 TiCDC 节点，IP 地址为 10.0.1.3、10.0.1.4，可以按照如下步骤进行操作。\n\n### 1. 添加节点信息到 scale-out.yml 文件\n\n编写 scale-out.yml 文件：\n\n{{< copyable \"\" >}}\n\n```ini\ncdc_servers:\n  - host: 10.0.1.3\n    gc-ttl: 86400\n    data_dir: /tidb-data/cdc-8300\n  - host: 10.0.1.4\n    gc-ttl: 86400\n    data_dir: /tidb-data/cdc-8300\n```\n\n### 2. 运行扩容命令\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster scale-out <cluster-name> scale-out.yml\n```\n\n> **注意：**\n>\n> 此处假设当前执行命令的用户和新增的机器打通了互信，如果不满足已打通互信的条件，需要通过 `-p` 来输入新机器的密码，或通过 `-i` 指定私钥文件。\n\n### 3. 查看集群状态\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n打开浏览器访问监控平台 <http://10.0.1.5:3000>，监控整个集群和新增节点的状态。\n\n扩容后，集群拓扑结构如下所示：\n\n| 主机 IP   | 服务   |\n|:----|:----|\n| 10.0.1.3   | TiDB + TiFlash + **TiCDC**  |\n| 10.0.1.4   | TiDB + PD + TiFlash + **TiCDC**  |\n| 10.0.1.5   | TiDB+ TiKV + Monitor   |\n| 10.0.1.1   | TiKV   |\n| 10.0.1.2   | TiKV   |\n\n## 缩容 TiDB/PD/TiKV 节点\n\n如果要移除 IP 地址为 10.0.1.5 的一个 TiKV 节点，可以按照如下步骤进行操作。\n\n> **注意：**\n>\n> - 移除 TiDB、PD 节点和移除 TiKV 节点的步骤类似。\n> - 由于 TiKV 和 TiFlash 组件是异步下线的，且下线过程耗时较长，所以 TiUP 对 TiKV 和 TiFlash 组件做了特殊处理，详情参考[下线特殊处理](/tiup/tiup-component-cluster-scale-in.md#下线特殊处理)。\n> - TiKV 中的 PD Client 会缓存 PD 节点的列表。当前版本的 TiKV 有定期自动更新 PD 节点的机制，可以降低 TiKV 缓存的 PD 节点列表过旧这一问题出现的概率。但你应尽量避免在扩容新 PD 后直接一次性缩容所有扩容前就已经存在的 PD 节点。如果需要，请确保在下线所有之前存在的 PD 节点前将 PD 的 leader 切换至新扩容的 PD 节点。\n\n### 1. 查看节点 ID 信息\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n```\nStarting /root/.tiup/components/cluster/v1.12.3/cluster display <cluster-name>\n\nTiDB Cluster: <cluster-name>\n\nTiDB Version: v8.5.0\n\nID       Role         Host    Ports                            Status  Data Dir        Deploy Dir\n\n--       ----         ----      -----                            ------  --------        ----------\n\n10.0.1.3:8300  cdc          10.0.1.3    8300                            Up      data/cdc-8300      deploy/cdc-8300\n\n10.0.1.4:8300  cdc          10.0.1.4    8300                            Up      data/cdc-8300      deploy/cdc-8300\n\n10.0.1.4:2379  pd           10.0.1.4    2379/2380                        Healthy data/pd-2379      deploy/pd-2379\n\n10.0.1.1:20160 tikv         10.0.1.1    20160/20180                      Up      data/tikv-20160     deploy/tikv-20160\n\n10.0.1.2:20160 tikv         10.0.1.2    20160/20180                      Up      data/tikv-20160     deploy/tikv-20160\n\n10.0.1.5:20160 tikv        10.0.1.5    20160/20180                     Up      data/tikv-20160     deploy/tikv-20160\n\n10.0.1.3:4000  tidb        10.0.1.3    4000/10080                      Up      -                 deploy/tidb-4000\n\n10.0.1.4:4000  tidb        10.0.1.4    4000/10080                      Up      -                 deploy/tidb-4000\n\n10.0.1.5:4000  tidb         10.0.1.5    4000/10080                       Up      -            deploy/tidb-4000\n\n10.0.1.3:9000   tiflash      10.0.1.3    9000/8123/3930/20170/20292/8234  Up      data/tiflash-9000       deploy/tiflash-9000\n\n10.0.1.4:9000   tiflash      10.0.1.4    9000/8123/3930/20170/20292/8234  Up      data/tiflash-9000       deploy/tiflash-9000\n\n10.0.1.5:9090  prometheus   10.0.1.5    9090                             Up      data/prometheus-9090  deploy/prometheus-9090\n\n10.0.1.5:3000  grafana      10.0.1.5    3000                             Up      -            deploy/grafana-3000\n\n10.0.1.5:9093  alertmanager 10.0.1.5    9093/9094                        Up      data/alertmanager-9093 deploy/alertmanager-9093\n```\n\n### 2. 执行缩容操作\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster scale-in <cluster-name> --node 10.0.1.5:20160\n```\n\n其中 `--node` 参数为需要下线节点的 ID。\n\n预期输出 Scaled cluster `<cluster-name>` in successfully 信息，表示缩容操作成功。\n\n### 3. 刷新集群配置\n\n> **注意：**\n>\n> 该操作仅需在缩容 PD 节点时执行，缩容 TiDB 或 TiKV 节点时无需执行。\n\n1. 更新集群配置：\n\n    ```shell\n    tiup cluster reload <cluster-name> --skip-restart\n    ```\n\n2. 更新 Prometheus 配置并重启：\n\n    > **注意：**\n    >\n    > 如果你使用的是 TiUP v1.15.0 及之后版本，请跳过此步骤；如果你使用的 TiUP 版本早于 v1.15.0，则需要执行以下命令来更新 Prometheus 配置并重启。\n\n    ```shell\n    tiup cluster reload <cluster-name> -R prometheus\n    ```\n\n### 4. 查看集群状态\n\n下线需要一定时间，下线节点的状态变为 Tombstone 就说明下线成功。\n\n执行如下命令检查节点是否下线成功：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n打开浏览器访问监控平台 <http://10.0.1.5:3000>，监控整个集群的状态。\n\n调整后，拓扑结构如下：\n\n| Host IP   | Service   |\n|:----|:----|\n| 10.0.1.3   | TiDB + TiFlash + TiCDC  |\n| 10.0.1.4   | TiDB + PD + TiFlash + TiCDC |\n| 10.0.1.5   | TiDB + Monitor**（TiKV 已删除）**   |\n| 10.0.1.1   | TiKV    |\n| 10.0.1.2   | TiKV    |\n\n## 缩容 TiFlash 节点\n\n如果要缩容 IP 地址为 10.0.1.4 的一个 TiFlash 节点，可以按照如下步骤进行操作。\n\n### 1. 根据 TiFlash 剩余节点数调整数据表的副本数\n\n1. 查询是否有数据表的 TiFlash 副本数大于缩容后的 TiFlash 节点数。`tobe_left_nodes` 表示缩容后的 TiFlash 节点数。如果查询结果为空，可以开始执行缩容。如果查询结果不为空，则需要修改相关表的 TiFlash 副本数。\n\n    ```sql\n    SELECT * FROM information_schema.tiflash_replica WHERE REPLICA_COUNT >  'tobe_left_nodes';\n    ```\n\n2. 对所有 TiFlash 副本数大于缩容后的 TiFlash 节点数的表执行以下语句，`new_replica_num` 必须小于等于 `tobe_left_nodes`：\n\n    ```sql\n    ALTER TABLE <db-name>.<table-name> SET tiflash replica 'new_replica_num';\n    ```\n\n    在执行该语句之后，TiDB 会相应地修改或删除 PD 的 [Placement Rules](/configure-placement-rules.md)，PD 再根据 Placement Rules 进行数据调度。\n\n3. 重新执行步骤 1，确保没有数据表的 TiFlash 副本数大于缩容后的 TiFlash 节点数。\n\n### 2. 执行缩容操作\n\n接下来，请任选下列方案其一进行缩容。\n\n#### 方案一：通过 TiUP 缩容 TiFlash 节点\n\n1. 通过以下命令确定需要下线的节点名称：\n\n    ```shell\n    tiup cluster display <cluster-name>\n    ```\n\n2. 执行 scale-in 命令来下线节点，假设步骤 1 中获得该节点名为 `10.0.1.4:9000`\n\n    ```shell\n    tiup cluster scale-in <cluster-name> --node 10.0.1.4:9000\n    ```\n\n3. 查看下线 TiFlash 节点的状态：\n\n    ```shell\n    tiup cluster display <cluster-name>\n    ```\n\n4. 等待下线 TiFlash 节点的状态变为 `Tombstone` 后，删除 TiUP 拓扑信息中已下线节点的信息（TiUP 会自动清理 `Tombstone` 状态节点的相关数据文件）：\n\n    ```shell\n    tiup cluster prune <cluster-name>\n    ```\n\n#### 方案二：手动缩容 TiFlash 节点\n\n在特殊情况下（比如需要强制下线节点），或者 TiUP 操作失败的情况下，可以使用以下方法手动下线 TiFlash 节点。\n\n1. 使用 pd-ctl 的 store 命令在 PD 中查看该 TiFlash 节点对应的 store id。\n\n    * 在 [pd-ctl](/pd-control.md)（tidb-ansible 目录下的 `resources/bin` 包含对应的二进制文件）中输入 store 命令。\n\n    * 若使用 TiUP 部署，可以调用以下命令代替 `pd-ctl`：\n\n        ```shell\n        tiup ctl:v<CLUSTER_VERSION> pd -u http://<pd_ip>:<pd_port> store\n        ```\n\n        > **注意：**\n        >\n        > 如果集群中有多个 PD 实例，只需在以上命令中指定一个活跃 PD 实例的 `IP:端口`即可。\n\n2. 在 pd-ctl 中下线该 TiFlash 节点。\n\n    * 在 pd-ctl 中输入 `store delete <store_id>`，其中 `<store_id>` 为上一步查到的该 TiFlash 节点对应的 store id。\n\n    * 若通过 TiUP 部署，可以调用以下命令代替 `pd-ctl`：\n\n        ```shell\n        tiup ctl:v<CLUSTER_VERSION> pd -u http://<pd_ip>:<pd_port> store delete <store_id>\n        ```\n\n        > **注意：**\n        >\n        > 如果集群中有多个 PD 实例，只需在以上命令中指定一个活跃 PD 实例的 `IP:端口`即可。\n\n3. 等待该 TiFlash 节点对应的 store 消失或者 state_name 变成 Tombstone 再关闭 TiFlash 进程。\n\n4. 删除 TiUP 拓扑信息中已下线节点的信息（TiUP 会自动清理 `Tombstone` 状态节点的相关数据文件）：\n\n    ```shell\n    tiup cluster prune <cluster-name>\n    ```\n\n### 3. 查看集群状态\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n打开浏览器访问监控平台 <http://10.0.1.5:3000>，监控整个集群的状态。\n\n调整后，拓扑结构如下：\n\n| Host IP   | Service   |\n|:----|:----|\n| 10.0.1.3   | TiDB + TiFlash + TiCDC  |\n| 10.0.1.4   | TiDB + PD + TiCDC **（TiFlash 已删除）**  |\n| 10.0.1.5   | TiDB + Monitor  |\n| 10.0.1.1   | TiKV    |\n| 10.0.1.2   | TiKV    |\n\n## 缩容 TiCDC 节点\n\n如果要缩容 IP 地址为 10.0.1.4 的一个 TiCDC 节点，可以按照如下步骤进行操作。\n\n### 1. 下线该 TiCDC 节点\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster scale-in <cluster-name> --node 10.0.1.4:8300\n```\n\n### 2. 查看集群状态\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n打开浏览器访问监控平台 <http://10.0.1.5:3000>，监控整个集群的状态。\n\n调整后，拓扑结构如下：\n\n| Host IP   | Service   |\n|:----|:----|\n| 10.0.1.3   | TiDB + TiFlash + TiCDC  |\n| 10.0.1.4   | TiDB + PD + **(TiCDC 已删除）**  |\n| 10.0.1.5   | TiDB + Monitor  |\n| 10.0.1.1   | TiKV    |\n| 10.0.1.2   | TiKV    |\n"
        },
        {
          "name": "schedule-replicas-by-topology-labels.md",
          "type": "blob",
          "size": 13.033203125,
          "content": "---\ntitle: 通过拓扑 label 进行副本调度\naliases: ['/docs-cn/dev/schedule-replicas-by-topology-labels/','/docs-cn/dev/how-to/deploy/geographic-redundancy/location-awareness/','/docs-cn/dev/location-awareness/']\nsummary: TiDB v5.3.0 引入了通过拓扑 label 进行副本调度的功能。为了提升集群的高可用性和数据容灾能力，推荐让 TiKV 节点在物理层面上尽可能分散。通过设置 TiKV 和 TiFlash 的 labels，可以标识它们的地理位置。同时，需要配置 PD 的 location-labels 和 isolation-level 来使 PD 理解 TiKV 节点拓扑并加强拓扑隔离要求。PD 在副本调度时会保证同一份数据的不同副本尽可能分散，以提高集群容灾能力。\n---\n\n# 通过拓扑 label 进行副本调度\n\n> **注意：**\n>\n> TiDB 在 v5.3.0 中引入了 [Placement Rules in SQL](/placement-rules-in-sql.md)。使用该功能，你可以更方便地配置表和分区的位置。在未来版本中，Placement Rules in SQL 可能取代通过 PD 配置放置规则的功能。\n\n为了提升 TiDB 集群的高可用性和数据容灾能力，我们推荐让 TiKV 节点尽可能在物理层面上分散，例如让 TiKV 节点分布在不同的机架甚至不同的可用区。PD 调度器根据 TiKV 的拓扑信息，会自动在后台通过调度使得 Region 的各个副本尽可能隔离，从而使得数据容灾能力最大化。\n\n要让这个机制生效，需要在部署时进行合理配置，把集群的拓扑信息（特别是 TiKV 的位置）上报给 PD。阅读本章前，请先确保阅读 [TiUP 部署方案](/production-deployment-using-tiup.md)。\n\n## 配置 TiKV、TiFlash 和 TiDB 的 labels\n\n你可以根据集群拓扑配置 TiKV、TiFlash 和 TiDB 的 labels。\n\n### 使用 TiUP 进行配置（推荐）\n\n如果使用 TiUP 部署集群，可以在[初始化配置文件](/production-deployment-using-tiup.md#第-3-步初始化集群拓扑文件)中统一进行 location 相关配置。TiUP 会负责在部署时生成对应的 TiDB、TiKV、PD 和 TiFlash 配置文件。\n\n下面的例子定义了 `zone` 和 `host` 两层拓扑结构。集群的 TiDB、TiKV 和 TiFlash 分布在三个 zone，z1、z2 和 z3。\n\n- 每个 zone 内有两台主机部署 TiDB 实例，TiDB 实例均为独占机器部署。\n- 每个 zone 内有两台主机部署 TiKV 实例，z1 每台主机同时部署两个 TiKV 实例，z2 和 z3 每台主机分别独立部署一个 TiKV 实例。\n- 每个 zone 内有两台主机部署 TiFlash 实例，TiFlash 实例均为独占机器部署。\n\n以下例子中 `tidb-host-machine-n` 代表第 n 个 TiDB 节点的 IP 地址，`tikv-host-machine-n` 代表第 n 个 TiKV 节点的 IP 地址，`tiflash-host-machine-n` 代表第 n 个 TiFlash 节点的 IP 地址。\n\n```\nserver_configs:\n  pd:\n    replication.location-labels: [\"zone\", \"host\"]\ntidb_servers:\n# z1\n  - host: tidb-host-machine-1\n    config:\n      labels:\n        zone: z1\n        host: tidb-host-machine-1\n  - host: tidb-host-machine-2\n    config:\n      labels:\n        zone: z1\n        host: tidb-host-machine-2\n# z2\n  - host: tidb-host-machine-3\n    config:\n      labels:\n        zone: z2\n        host: tidb-host-machine-3\n  - host: tikv-host-machine-4\n    config:\n      labels:\n        zone: z2\n        host: tidb-host-machine-4\n# z3\n  - host: tidb-host-machine-5\n    config:\n      labels:\n        zone: z3\n        host: tidb-host-machine-5\n  - host: tidb-host-machine-6\n    config:\n      labels:\n        zone: z3\n        host: tidb-host-machine-6\ntikv_servers:\n# z1\n  # machine-1 on z1\n  - host: tikv-host-machine-1\n    port：20160\n    config:\n      server.labels:\n        zone: z1\n        host: tikv-host-machine-1\n  - host: tikv-host-machine-1\n    port：20161\n    config:\n      server.labels:\n        zone: z1\n        host: tikv-host-machine-1\n  # machine-2 on z1\n  - host: tikv-host-machine-2\n    port：20160\n    config:\n      server.labels:\n        zone: z1\n        host: tikv-host-machine-2\n  - host: tikv-host-machine-2\n    port：20161\n    config:\n      server.labels:\n        zone: z1\n        host: tikv-host-machine-2\n# z2\n  - host: tikv-host-machine-3\n    config:\n      server.labels:\n        zone: z2\n        host: tikv-host-machine-3\n  - host: tikv-host-machine-4\n    config:\n      server.labels:\n        zone: z2\n        host: tikv-host-machine-4\n# z3\n  - host: tikv-host-machine-5\n    config:\n      server.labels:\n        zone: z3\n        host: tikv-host-machine-5\n  - host: tikv-host-machine-6\n    config:\n      server.labels:\n        zone: z3\n        host: tikv-host-machine-6\n\ntiflash_servers:\n# z1\n  - host: tiflash-host-machine-1\n    learner_config:\n      server.labels:\n        zone: z1\n        host: tiflash-host-machine-1\n  - host: tiflash-host-machine-2\n    learner_config:\n      server.labels:\n        zone: z1\n        host: tiflash-host-machine-2\n# z2\n  - host: tiflash-host-machine-3\n    learner_config:\n      server.labels:\n        zone: z2\n        host: tiflash-host-machine-3\n  - host: tiflash-host-machine-4\n    learner_config:\n      server.labels:\n        zone: z2\n        host: tiflash-host-machine-4\n# z3\n  - host: tiflash-host-machine-5\n    learner_config:\n      server.labels:\n        zone: z3\n        host: tiflash-host-machine-5\n  - host: tiflash-host-machine-6\n    learner_config:\n      server.labels:\n        zone: z3\n        host: tiflash-host-machine-6\n```\n\n详情参阅 [TiUP 跨数据中心部署拓扑](/geo-distributed-deployment-topology.md)。\n\n> **注意：**\n>\n> 如果你未在配置文件中配置 `replication.location-labels` 项，使用该拓扑配置文件部署集群时可能会报错。建议在部署集群前，确认 `replication.location-labels` 已配置。\n\n### 使用命令行或配置文件进行配置\n\n#### 设置 TiKV 和 TiFlash 的 `labels`\n\nTiKV 和 TiFlash 支持在命令行参数或者配置文件中以键值对的形式绑定一些属性，我们把这些属性叫做标签 (label)。TiKV 和 TiFlash 在启动后，会将自身的标签上报给 PD，因此可以使用标签来标识 TiKV 和 TiFlash 节点的地理位置。\n\n比如集群的拓扑结构分成四层：可用区 (zone) -> 数据中心 (dc) -> 机架 (rack) -> 主机 (host)，就可以使用这 4 个标签来设置 TiKV 和 TiFlash 的位置。\n\n使用命令行参数的方式启动一个 TiKV 实例：\n\n{{< copyable \"\" >}}\n\n```\ntikv-server --labels zone=<zone>,dc=<dc>,rack=<rack>,host=<host>\n```\n\n使用配置文件的方式：\n\n{{< copyable \"\" >}}\n\n```toml\n[server]\n[server.labels]\nzone = \"<zone>\"\ndc = \"<dc>\"\nrack = \"<rack>\"\nhost = \"<host>\"\n```\n\nTiFlash 支持通过 tiflash-learner.toml （tiflash-proxy 的配置文件）的方式设置 labels：\n\n{{< copyable \"\" >}}\n\n```toml\n[server]\n[server.labels]\nzone = \"<zone>\"\ndc = \"<dc>\"\nrack = \"<rack>\"\nhost = \"<host>\"\n```\n\n#### 设置 TiDB 的 `labels`（可选）\n\n如果需要使用 [Follower Read](/follower-read.md) 的优先读同一区域副本的功能，需要为 TiDB 节点配置相关的 `labels`。\n\nTiDB 支持使用配置文件的方式设置 `labels`：\n\n{{< copyable \"\" >}}\n\n```\n[labels]\nzone = \"<zone>\"\ndc = \"<dc>\"\nrack = \"<rack>\"\nhost = \"<host>\"\n```\n\n> **注意：**\n>\n> 目前，TiDB 依赖 `zone` 标签匹配选择同一区域的副本。如果需要使用此功能，需要在 PD [`location-labels` 配置](#设置-pd-的-isolation-level-配置)中包含 `zone`，并在 TiDB、TiKV 和 TiFlash 设置的 `labels` 中包含 `zone`。关于如何设置 TiKV 和 TiFlash 的 `labels`，可参考[设置 TiKV 和 TiFlash 的 `labels`](#设置-tikv-和-tiflash-的-labels)。\n\n## 设置 PD 的 `location-labels` 配置\n\n根据前面的描述，标签可以是用来描述 TiKV 属性的任意键值对，但 PD 无从得知哪些标签是用来标识地理位置的，而且也无从得知这些标签的层次关系。因此，PD 也需要一些配置来使得 PD 理解 TiKV 节点拓扑。\n\nPD 上的配置叫做 `location-labels`，是一个字符串数组。该配置的每一项与 TiKV `labels` 的 key 是对应的，而且其中每个 key 的顺序代表不同标签的级别关系（从左到右，隔离级别依次递减）。\n\n`location-labels` 没有默认值，你可以根据具体需求来设置该值，包括 `zone`、`rack`、`host` 等等。同时，`location-labels` 对标签级别的数量也**没有**限制（即不限定于 3 个），只要其级别与 TiKV 服务器的标签匹配，则可以配置成功。\n\n> **注意：**\n>\n> - 必须同时配置 PD 的 `location-labels` 和 TiKV 的 `labels` 参数，否则 PD 不会根据拓扑结构进行调度。\n> - 如果你使用 Placement Rules in SQL，只需要配置 TiKV 的 `labels` 即可。Placement Rules in SQL 目前不兼容 PD `location-labels` 设置，会忽略该设置。不建议 `location-labels` 与 Placement Rules in SQL 混用，否则可能产生非预期的结果。\n\n你可以根据集群状态来选择不同的配置方式：\n\n- 在集群初始化之前，可以通过 PD 的配置文件进行配置：\n\n    {{< copyable \"\" >}}\n\n    ```toml\n    [replication]\n    location-labels = [\"zone\", \"rack\", \"host\"]\n    ```\n\n- 如果需要在 PD 集群初始化完成后进行配置，则需要使用 pd-ctl 工具进行在线更改：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```bash\n    pd-ctl config set location-labels zone,rack,host\n    ```\n\n## 设置 PD 的 `isolation-level` 配置\n\n在配置了 `location-labels` 的前提下，用户可以还通过 `isolation-level` 配置来进一步加强对 TiKV 集群的拓扑隔离要求。假设按照上面的说明通过 `location-labels` 将集群的拓扑结构分成三层：可用区 (zone) -> 机架 (rack) -> 主机 (host)，并对 `isolation-level` 作如下配置：\n\n{{< copyable \"\" >}}\n\n```toml\n[replication]\nisolation-level = \"zone\"\n```\n\n当 PD 集群初始化完成后，需要使用 pd-ctl 工具进行在线更改：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config set isolation-level zone\n```\n\n其中，`isolation-level` 配置是一个字符串，需要与 `location-labels` 的其中一个 key 对应。该参数限制 TiKV 拓扑集群的最小且强制隔离级别要求。\n\n> **注意：**\n>\n> `isolation-level` 默认情况下为空，即不进行强制隔离级别限制，若要对其进行设置，必须先配置 PD 的 `location-labels` 参数，同时保证 `isolation-level` 的值一定为 `location-labels` 中的一个。\n\n## 基于拓扑 label 的 PD 调度策略\n\nPD 在副本调度时，会按照 label 层级，保证同一份数据的不同副本尽可能分散。\n\n下面以上一节的拓扑结构为例分析。\n\n假设集群副本数设置为 3 (`max-replicas=3`)，因为总共有 3 个 zone，PD 会保证每个 Region 的 3 个副本分别放置在 z1/z2/z3，这样当任何一个数据中心发生故障时，TiDB 集群依然是可用的。\n\n假如集群副本数设置为 5 (`max-replicas=5`)，因为总共只有 3 个 zone，在这一层级 PD 无法保证各个副本的隔离，此时 PD 调度器会退而求其次，保证在 host 这一层的隔离。也就是说，会出现一个 Region 的多个副本分布在同一个 zone 的情况，但是不会出现多个副本分布在同一台主机。\n\n在 5 副本配置的前提下，如果 z3 出现了整体故障或隔离，并且 z3 在一段时间后仍然不能恢复（由 `max-store-down-time` 控制），PD 会通过调度补齐 5 副本，此时可用的主机只有 4 个了，故而无法保证 host 级别的隔离，于是可能出现多个副本被调度到同一台主机的情况。\n\n但假如 `isolation-level` 设置不为空，值为 `zone`，这样就规定了 Region 副本在物理层面上的最低隔离要求，也就是说 PD 一定会保证同一 Region 的副本分散于不同的 zone 之上。即便遵循此隔离限制会无法满足 `max-replicas` 的多副本要求，PD 也不会进行相应的调度。例如，当前存在 TiKV 集群的三个可用区 z1/z2/z3，在三副本的设置下，PD 会将同一 Region 的三个副本分别分散调度至这三个可用区。若此时 z1 整个可用区发生了停电事故并在一段时间后（由 [`max-store-down-time`](/pd-configuration-file.md#max-store-down-time) 控制，默认为 30 分钟）仍然不能恢复，PD 会认为 z1 上的 Region 副本不再可用。但由于 `isolation-level` 设置为了 `zone`，PD 需要严格保证不同的 Region 副本不会落到同一 zone 上。此时的 z2 和 z3 均已存在副本，则 PD 在 `isolation-level` 的最小强制隔离级别限制下便不会进行任何调度，即使此时仅存在两个副本。\n\n类似地，`isolation-level` 为 `rack` 时，最小隔离级别便为同一可用区的不同 rack。在此设置下，如果能在 zone 级别保证隔离，会首先保证 zone 级别的隔离。只有在 zone 级别隔离无法完成时，才会考虑避免出现在同一 zone 同一 rack 的调度，并以此类推。\n\n总的来说，PD 能够根据当前的拓扑结构使得集群容灾能力最大化。所以如果用户希望达到某个级别的容灾能力，就需要根据拓扑结构在对应级别提供多于副本数 (`max-replicas`) 的机器。同时 TiDB 也提供了诸如 `isolation-level` 这样的强制隔离级别设置，以便更灵活地根据场景来控制对数据的拓扑隔离级别。\n"
        },
        {
          "name": "scheduling-configuration-file.md",
          "type": "blob",
          "size": 3.353515625,
          "content": "---\ntitle: Scheduling 配置文件描述\nsummary: Scheduling 配置文件包含了多个配置项，如节点名称、数据路径、节点 URL 等。\n---\n\n# Scheduling 配置文件描述\n\n<!-- markdownlint-disable MD001 -->\n\nScheduling 节点用于提供 PD 的 `scheduling` 微服务。本文档仅在 PD 开启微服务模式下适用。\n\n> **Tip:**\n>\n> 如果你需要调整配置项的值，请参考[修改配置参数](/maintain-tidb-using-tiup.md#修改配置参数)进行操作。\n\n### `name`\n\n- Scheduling 节点名称。\n- 默认值：`\"Scheduling\"`\n- 如果你需要启动多个 Scheduling 节点，请确保不同的 Scheduling 节点使用不同的名字。\n\n### `data-dir`\n\n- Scheduling 节点上的数据存储路径。\n- 默认值：`\"default.${name}\"`\n\n### `listen-addr`\n\n- Scheduling 节点监听的客户端 URL。\n- 默认值：`\"http://127.0.0.1:3379\"`\n- 部署集群时，`listen-addr` 必须指定当前主机的 IP 地址，例如 `\"http://192.168.100.113:3379\"`。如果运行在 Docker 中，则需要指定为 `\"http://0.0.0.0:3379\"`。\n\n### `advertise-listen-addr`\n\n- 用于外部访问 Scheduling 节点的 URL。\n- 默认值：`\"${listen-addr}\"`\n- 在某些情况下，例如 Docker 或者 NAT 网络环境，客户端并不能通过 Scheduling 节点自己监听的地址来访问 Scheduling 节点。此时，你可以设置 `advertise-listen-addr` 来让客户端访问。\n- 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 2379:2379`，那么可以设置 `advertise-listen-addr=\"http://192.168.100.113:2379\"`，然后客户端就可以通过 `http://192.168.100.113:2379` 来找到这个服务。\n\n### `backend-endpoints`\n\n- Scheduling 节点监听其他 Scheduling 节点的 URL 列表。\n- 默认值：`\"http://127.0.0.1:2379\"`\n\n### `lease`\n\n- Scheduling Primary Key 租约超时时间，超时系统重新选举 Primary。\n- 默认值：3\n- 单位：秒\n\n## security\n\n安全相关配置项。\n\n### `cacert-path`\n\n- CA 文件路径\n- 默认值：\"\"\n\n### `cert-path`\n\n- 包含 X.509 证书的 PEM 文件路径\n- 默认值：\"\"\n\n### `key-path`\n\n- 包含 X.509 key 的 PEM 文件路径\n- 默认值：\"\"\n\n### `redact-info-log`\n\n- 控制 Scheduling 节点日志脱敏的开关\n- 该配置项值设为 true 时将对 Scheduling 日志脱敏，遮蔽日志中的用户信息。\n- 默认值：false\n\n## log\n\n日志相关的配置项。\n\n### `level`\n\n- 指定日志的输出级别。\n- 可选值：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n- 默认值：\"info\"\n\n### `format`\n\n- 日志格式。\n- 可选值：\"text\"，\"json\"\n- 默认值：\"text\"\n\n### `disable-timestamp`\n\n- 是否禁用日志中自动生成的时间戳。\n- 默认值：false\n\n## log.file\n\n日志文件相关的配置项。\n\n### `max-size`\n\n- 单个日志文件最大大小，超过该值系统自动切分成多个文件。\n- 默认值：300\n- 单位：MiB\n- 最小值为 1\n\n### `max-days`\n\n- 日志保留的最长天数。\n- 如果未设置本参数或把本参数设置为默认值 `0`，Scheduling 不清理日志文件。\n- 默认：0\n\n### `max-backups`\n\n- 日志文件保留的最大个数。\n- 如果未设置本参数或把本参数设置为默认值 `0`，Scheduling 会保留所有的日志文件。\n- 默认：0\n\n## metric\n\n监控相关的配置项。\n\n### `interval`\n\n- 向 Prometheus 推送监控指标数据的间隔时间。\n- 默认：15s\n"
        },
        {
          "name": "schema-cache.md",
          "type": "blob",
          "size": 3.6142578125,
          "content": "---\ntitle: Schema 缓存\nsummary: TiDB 对于 schema 信息采用基于 LRU 的缓存机制，在大量数据库和表的场景下能够显著减少 schema 信息的内存占用以及提高性能。\n---\n\n# Schema 缓存\n\n在一些多租户的场景下，可能会存在几十万甚至上百万个数据库和表。这些数据库和表的 schema 信息如果全部加载到内存中，一方面会占用大量的内存，另一方面会导致相关的访问性能变差。为了解决这个问题，TiDB 引入了类似于 LRU 的 schema 缓存机制。只将最近用到的数据库和表的 schema 信息缓存到内存中。\n\n## 配置 schema 缓存\n\n可以通过配置系统变量 [`tidb_schema_cache_size`](/system-variables.md#tidb_schema_cache_size-从-v800-版本开始引入) 来打开 schema 缓存特性。\n\n## 最佳实践\n\n- 在大量数据库和表的场景下（例如 10 万以上的数据库和表数量）或者当数据库和表的数量大到影响系统性能时，建议打开 schema 缓存特性。\n- 可以通过观测 TiDB 监控中 **Schema load** 下的子面板 **Infoschema v2 Cache Operation** 来查看 schema 缓存的命中率。如果命中率较低，可以调大 [`tidb_schema_cache_size`](/system-variables.md#tidb_schema_cache_size-从-v800-版本开始引入)。\n- 可以通过观测 TiDB 监控中 **Schema load** 下的子面板 **Infoschema v2 Cache Size** 来查看当前使用的 schema 缓存的大小。\n- 建议关闭 [`performance.force-init-stats`](/tidb-configuration-file.md#force-init-stats-从-v657-和-v710-版本开始引入) 以减少 TiDB 的启动时间。\n- 如果需要创建大量的表（例如 10 万张以上），建议将参数 [`split-table`](/tidb-configuration-file.md#split-table) 设置为 `false` 以减少 Region 数量，从而降低 TiKV 的内存。\n\n## 已知限制\n\n在大量数据库和表的场景下，有以下已知问题：\n\n- 单个集群的表的数量不能超过 300 万张。\n- 当单个集群的表的数量超过 30 万张时，请不要将 [`tidb_schema_cache_size`](/system-variables.md#tidb_schema_cache_size-从-v800-版本开始引入) 取值设置为 `0`，否则可能导致 TiDB OOM。\n- 当使用外键时，可能会增加集群 DDL 的执行时长。\n- 当对表的访问没有规律，如 time1 访问一批表，time2 访问另外一批表，而且设置的 `tidb_schema_cache_size` 较小时，会导致这些 schema 信息被频繁地被逐出，频繁地被缓存，造成性能抖动。该特性比较适合被频繁访问的库和表是相对固定的场景。\n- 统计信息不一定能够及时收集。\n- 一些元数据信息的访问会变慢。\n- 切换 schema 缓存开关需要等待一段时间。\n- 全量列举所有元数据信息的相关操作会变慢，如：\n\n    - `SHOW FULL TABLES`\n    - `FLASHBACK`\n    - `ALTER TABLE ... SET TIFLASH MODE ...`\n- 对于设置了 [`AUTO_INCREMENT`](/auto-increment.md) 或 [`AUTO_RANDOM`](/auto-random.md) 属性的表，如果 schema 缓存设置过小，这些表的元信息可能会在内存中被频繁地缓存和淘汰。这种频繁的缓存变动可能导致未使用完的 ID 段失效，从而引发 ID 跳变。在写入量较大的场景下，甚至可能导致 ID 段耗尽。为减少此类问题并提升系统稳定性，建议采取以下措施：\n\n    - 通过监控面板查看 schema 缓存的命中率和大小，以评估缓存设置是否合理。并适当调大 schema 缓存大小，以减少频繁的缓存淘汰。\n    - 将 [`AUTO_ID_CACHE`](/auto-increment.md#auto_id_cache) 设置为 `1`，以防止 ID 跳变。\n    - 合理设置 `AUTO_RANDOM` 的分片位和保留位，避免可分配 ID 范围过小。\n"
        },
        {
          "name": "schema-object-names.md",
          "type": "blob",
          "size": 2.744140625,
          "content": "---\ntitle: Schema 对象名\nsummary: 本文介绍 TiDB SQL 语句中的模式对象名。\naliases: ['/docs-cn/dev/schema-object-names/','/docs-cn/dev/reference/sql/language-structure/schema-object-names/']\n---\n\n# Schema 对象名\n\n<!-- markdownlint-disable MD038 -->\n\n本文介绍 TiDB SQL 语句中的模式对象名。\n\n模式对象名用于命名 TiDB 中所有的模式对象，包括 database、table、index、column、alias 等等。在 SQL 语句中，可以通过标识符 (identifier) 来引用这些对象。\n\n标识符可以被反引号包裹，即 `SELECT * FROM t` 也可以写成 `` SELECT * FROM `t` ``。但如果标识符中存在至少一个特殊符号，或者它是一个保留关键字，那就必须使用反引号包裹来引用它所代表的模式对象。\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM `table` WHERE `table`.id = 20;\n```\n\n如果 SQL MODE 中设置了 `ANSI_QUOTES`，那么 TiDB 会将被双引号 `\"` 包裹的字符串识别为 identifier。\n\n```sql\nMySQL [test]> CREATE TABLE \"test\" (a varchar(10));\nERROR 1064 (42000): You have an error in your SQL syntax; check the manual that corresponds to your TiDB version for the right syntax to use line 1 column 19 near \"\"test\" (a varchar(10))\" \n\nMySQL [test]> SET SESSION sql_mode='ANSI_QUOTES';\nQuery OK, 0 rows affected (0.000 sec)\n\nMySQL [test]> CREATE TABLE \"test\" (a varchar(10));\nQuery OK, 0 rows affected (0.012 sec)\n```\n\n如果要在被引用的标识符中使用反引号这个字符，则需要重复两次反引号，例如创建一个表 a`b：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE `a``b` (a int);\n```\n\n在 select 语句中，alias 部分可以用标识符或者字符串：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT 1 AS `identifier`, 2 AS 'string';\n```\n\n```\n+------------+--------+\n| identifier | string |\n+------------+--------+\n|          1 |      2 |\n+------------+--------+\n1 row in set (0.00 sec)\n```\n\n更多细节，请参考 [MySQL 文档](https://dev.mysql.com/doc/refman/8.0/en/identifiers.html)。\n\n## Identifier Qualifiers\n\nObject Names（对象名字）有时可以被限定或者省略。例如在创建表的时候可以省略数据库限定名：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t (i int);\n```\n\n如果之前没有使用 `USE` 或者连接参数来设定数据库，会报 `ERROR 1046 (3D000): No database selected` 错误。此时可以指定数据库限定名：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE test.t (i int);\n```\n\n`.` 的左右两端可以出现空格，`table_name.col_name` 等于 `table_name . col_name`。\n\n如果要引用这个模式对象，那么请使用：\n\n```\n`table_name`.`col_name`\n```\n\n而不是：\n\n```\n`table_name.col_name`\n```\n\n更多细节，请参考 [MySQL 文档](https://dev.mysql.com/doc/refman/8.0/en/identifier-qualifiers.html)。\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "security-compatibility-with-mysql.md",
          "type": "blob",
          "size": 14.58203125,
          "content": "---\ntitle: 与 MySQL 安全特性差异\naliases: ['/docs-cn/dev/security-compatibility-with-mysql/','/docs-cn/dev/reference/security/compatibility/']\nsummary: TiDB 支持与 MySQL 5.7 类似的安全特性，同时也支持 MySQL 8.0 的部分安全特性。然而，在实现上存在一些差异，包括不支持列级别权限设置和部分权限属性。此外，TiDB 的密码过期策略和密码复杂度策略与 MySQL 存在一些差异。另外，TiDB 支持多种身份验证方式，包括 TLS 证书和 JWT。\n---\n\n# 与 MySQL 安全特性差异\n\nTiDB 支持与 MySQL 5.7 类似的安全特性，同时 TiDB 还支持 MySQL 8.0 的部分安全特性。TiDB 的安全特性在实现上与 MySQL 存在差异。\n\n## 不支持的安全功能特性\n\n- 不支持列级别权限设置。\n- 不支持权限属性 `max_questions`，`max_updated` 以及 `max_user_connections`。\n- 不支持密码修改验证策略，修改密码时需要验证当前密码。\n- 不支持双密码策略。\n- 不支持随机密码生成策略。\n- 不支持多因素身份验证。\n\n## 与 MySQL 有差异的安全特性详细说明\n\n### 密码过期策略\n\n针对密码过期策略功能，TiDB 与 MySQL 的比较如下：\n\n- MySQL 5.7 和 8.0 支持密码过期策略管理功能。\n- TiDB 从 v6.5.0 起支持密码过期策略管理功能。\n\nTiDB 的密码过期策略功能与 MySQL 保持一致，但是在密码过期处理机制上存在以下差异：\n\n- MySQL 5.7 和 8.0 在密码过期后是否将客户端的连接限制为“沙盒模式”，由客户端和服务端设置的组合确定。\n- TiDB 在密码过期后是否将客户端的连接限制为“沙盒模式”，仅由 TiDB 配置文件中的 `[security]` 部分的 [`disconnect-on-expired-password`](/tidb-configuration-file.md#disconnect-on-expired-password-从-v650-版本开始引入) 选项确定。\n\n### 密码复杂度策略\n\n针对密码复杂度策略功能，TiDB 与 MySQL 的比较如下：\n\n- MySQL 5.7 以 validate_password 插件的形式实现了密码复杂度策略管理功能。\n- MySQL 8.0 重新以 validate_password 组件的形式实现了密码复杂度策略管理功能。\n- TiDB 从 v6.5.0 起内置实现了密码复杂度策略管理功能。\n\n因此，功能实现上存在以下差异：\n\n- 密码复杂度策略功能如何启用：\n\n    + MySQL 5.7 以 validate_password 插件的形式实现，需要进行插件的安装以启用密码复杂度策略管理。\n    + MySQL 8.0 以 validate_password 组件的形式实现，需要进行组件的安装以启用密码复杂度策略管理。\n    + TiDB 内置实现了密码复杂度策略管理，支持通过系统变量 [`validate_password.enable`](/system-variables.md#validate_passwordenable-从-v650-版本开始引入) 启用密码复杂度策略管理。\n\n- 密码字典功能：\n\n    + MySQL 5.7 通过变量 `validate_password_dictionary_file` 指定一个文件路径，在文件中写入密码中不允许包含的单词。\n    + MySQL 8.0 通过变量 `validate_password.dictionary_file` 指定一个文件路径，在文件中写入密码中不允许包含的单词。\n    + TiDB 通过变量 [`validate_password.dictionary`](/system-variables.md#validate_passworddictionary-从-v650-版本开始引入) 指定一个字符串，在该字符串中写入密码中不允许包含的单词。\n\n### 密码连续错误限制登录策略\n\n针对密码连续错误限制登录策略功能，TiDB 与 MySQL 的比较如下：\n\n- MySQL 5.7 不支持密码连续错误限制登录策略管理功能。\n- MySQL 8.0 支持密码连续错误限制登录策略管理功能。\n- TiDB 从 v6.5.0 起支持密码连续错误限制登录策略管理功能。\n\n因为用户的失败尝试次数和锁定状态需要做到全局一致，而 TiDB 是分布式数据库，不能像 MySQL 在服务端的内存中记录失败尝试次数和锁定状态，所以实现机制存在以下差异：\n\n- 用户未被自动锁定，失败尝试次数的计数重置场景：\n\n    + MySQL 8.0：\n\n        - 服务器重启时，所有用户失败尝试次数的计数都会被重置。\n        - 执行 `FLUSH PRIVILEGES` 时，所有用户失败尝试次数的计数都会被重置。\n        - 对该用户执行 `ALTER USER ... ACCOUNT UNLOCK` 解锁命令时。\n        - 该用户登录成功时。\n\n    + TiDB：\n\n        - 对该用户执行 `ALTER USER ... ACCOUNT UNLOCK` 解锁命令时。\n        - 该用户登录成功时。\n\n- 账户被自动锁定后的解锁场景：\n\n    + MySQL 8.0：\n\n        - 服务器重启时，所有用户的自动锁定标识都会被重置。\n        - 执行 `FLUSH PRIVILEGES` 时，所有用户的自动锁定标识都会被重置。\n        - 该用户锁定时间结束，这种情况下，用户的自动锁定标识将在下次登录尝试时重置。\n        - 对该用户执行 `ALTER USER ... ACCOUNT UNLOCK` 解锁命令时。\n\n    + TiDB：\n\n        - 该用户锁定时间结束，这种情况下，用户的自动锁定标识将在下次登录尝试时重置。\n        - 对该用户执行 `ALTER USER ... ACCOUNT UNLOCK` 解锁命令时。\n\n### 密码重用策略\n\n针对密码重用策略功能，TiDB 与 MySQL 的比较如下：\n\n- MySQL 5.7 不支持密码重用策略管理功能。\n- MySQL 8.0 支持密码重用策略管理功能。\n- TiDB 从 v6.5.0 起支持密码重用策略管理功能。\n\nTiDB 的密码重用策略功能与 MySQL 一致，在实现密码重用策略时都增加了系统表 `mysql.password_history`，但 TiDB 与 MySQL 在删除 `mysql.user` 系统表中不存在的用户时存在以下差异：\n\n- 场景：没有正确创建用户（例如： `user01` ），而通过 `INSERT INTO mysql.password_history VALUES (...)` 命令直接向 `mysql.password_history` 系统表中添加一条 `user01` 的记录，此时系统表 `mysql.user` 中没有 `user01` 的记录。对该用户执行 `DROP USER` 操作时，TiDB 和 MySQL 状态不一致。\n- 差异点：\n\n    + MySQL：执行 `DROP USER user01` 时，在 `mysql.user` 和 `mysql.password_history` 系统表中匹配 `user01`，若在两个系统表或其中一个系统表中匹配成功，则 `DROP USER` 操作可以正常执行，不会报错。\n    + TiDB：执行 `DROP USER user01` 时，只在 `mysql.user` 系统表中匹配 `user01`，若没有匹配成功，则 `DROP USER` 操作执行失败，返回报错。此时如果需要成功执行 `DROP USER user01` 操作，删除 `mysql.password_history` 中 `user01` 的记录，请使用 `DROP USER IF EXISTS user01`。\n\n## 可用的身份验证插件\n\nTiDB 支持多种身份验证方式。通过使用 [`CREATE USER`](/sql-statements/sql-statement-create-user.md) 语句和 [`ALTER USER`](/sql-statements/sql-statement-alter-user.md) 语句，即可创建新用户或更改 TiDB 权限系统内的已有用户。TiDB 身份验证方式与 MySQL 兼容，其名称与 MySQL 保持一致。\n\nTiDB 目前支持的身份验证方式可在以下的表格中查找到。服务器和客户端建立连接时，如要指定服务器对外通告的默认验证方式，可通过 [`default_authentication_plugin`](/system-variables.md#default_authentication_plugin) 变量进行设置。`tidb_sm3_password` 为仅在 TiDB 支持的 SM3 身份验证方式，使用该方式登录的用户需要使用 [TiDB-JDBC](https://github.com/pingcap/mysql-connector-j/tree/release/8.0-sm3)。`tidb_auth_token` 用于 TiDB Cloud 内部的基于 JSON Web Token (JWT) 的认证，用户通过配置也可以用于自托管环境。\n\n针对 TLS 身份验证，TiDB 目前采用不同的配置方案。具体情况请参见[为 TiDB 客户端服务端间通信开启加密传输](/enable-tls-between-clients-and-servers.md)。\n\n| 身份验证方式    | 支持        |\n| :------------------------| :--------------- |\n| `mysql_native_password`  | 是              |\n| `sha256_password`        | 否               |\n| `caching_sha2_password`  | 是（5.2.0 版本起） |\n| `auth_socket`            | 是（5.3.0 版本起） |\n| `tidb_sm3_password`      | 是（6.3.0 版本起） |\n| `tidb_auth_token`        | 是（6.4.0 版本起） |\n| `authentication_ldap_sasl`   | 是（7.1.0 版本起） |\n| `authentication_ldap_simple` | 是（7.1.0 版本起） |\n| TLS 证书       | 是              |\n| LDAP                     | 是（7.1.0 版本起） |\n| PAM                      | 否               |\n| ed25519 (MariaDB)        | 否               |\n| GSSAPI (MariaDB)         | 否               |\n| FIDO                     | 否               |\n\n### `tidb_auth_token`\n\n`tidb_auth_token` 是一种基于 [JSON Web Token (JWT)](https://datatracker.ietf.org/doc/html/rfc7519) 的无密码认证方式。在 v6.4.0 中，`tidb_auth_token` 仅用于 TiDB Cloud 内部的用户认证，从 v6.5.0 起，你也可以将 `tidb_auth_token` 配置为 TiDB 自托管环境中用户的认证方式。不同于 `mysql_native_password`、`caching_sha2_password` 等使用密码的认证方式，`tidb_auth_token` 认证方式在创建用户时无需设置并保存自定义密码，在用户登录时只需使用一个签发的 token，从而简化用户的认证过程并提升安全性。\n\n#### JWT\n\nJWT 由 Header、Payload 和 Signature 三部分组成。这三部分分别通过 base64 编码后，使用点号（`.`）拼接成一个字符串，以便在客户端和服务器之间传输。\n\nHeader 描述 JWT 的元数据，包含 3 个属性：\n\n* `alg`：表示签名使用的算法，默认为 `RS256`。\n* `typ`：表示 token 的类型，为 `JWT`。\n* `kid`：表示用于生成 token 签名的 key ID。\n\nHeader 示例：\n\n```json\n{\n  \"alg\": \"RS256\",\n  \"kid\": \"the-key-id-0\",\n  \"typ\": \"JWT\"\n}\n```\n\nPayload 是 JWT 的主体部分，用于保存用户的信息。Payload 中的每个字段称为一个 claim（声明）。TiDB 用户认证要求提供的声明如下：\n\n* `iss`：如果[创建用户](/sql-statements/sql-statement-create-user.md)时未指定 `TOKEN_ISSUER` 或者将其设置为了空字符串，则可以不包含该声明；否则 `iss` 应该与 `TOKEN_ISSUER` 设置值相同。\n* `sub`：TiDB 中要求该值与待认证的用户名相同。\n* `iat`：发布 token 的时间戳。TiDB 中要求该值不得晚于认证时的时间，也不得早于认证前 15 分钟。\n* `exp`：token 到期的时间戳。如果 token 在认证时已经过期，则认证失败。\n* `email`：邮件地址。创建用户时可以通过 `ATTRIBUTE '{\"email\": \"xxxx@pingcap.com\"}'` 指定 email 信息。如果创建用户时未指定 email 信息，则该声明应设置为空字符串；否则该声明应该与创建用户时的设置值相同。\n\nPayload 示例：\n\n```json\n{\n  \"email\": \"user@pingcap.com\",\n  \"exp\": 1703305494,\n  \"iat\": 1703304594,\n  \"iss\": \"issuer-abc\",\n  \"sub\": \"user@pingcap.com\"\n}\n```\n\nSignature 用于对 Header 和 Payload 这两部分数据进行签名。\n\n> **警告：**\n>\n> - Header 与 Payload 使用 base64 进行编码的过程是可逆的，请勿在 Payload 中携带敏感数据。\n> - `tidb_auth_token` 认证方式要求客户端支持 [`mysql_clear_password`](https://dev.mysql.com/doc/refman/8.0/en/cleartext-pluggable-authentication.html) 插件，并将 token 以明文的方式发送至 TiDB，因此请[为 TiDB 开启加密传输](/enable-tls-between-clients-and-servers.md) 后再使用 `tidb_auth_token` 进行认证。\n\n#### 使用方法\n\n配置并使用 `tidb_auth_token` 作为 TiDB 自托管环境中用户的认证方式，有以下几个步骤：\n\n1. 在 TiDB 配置文件中设置 [`auth-token-jwks`](/tidb-configuration-file.md#auth-token-jwks-从-v640-版本开始引入) 和 [`auth-token-refresh-interval`](/tidb-configuration-file.md#auth-token-refresh-interval-从-v640-版本开始引入)。\n\n    例如，可以通过下列命令获取示例 JWKS：\n\n    ```bash\n    wget https://raw.githubusercontent.com/CbcWestwolf/generate_jwt/master/JWKS.json\n    ```\n\n    然后在 TiDB 的配置文件 `config.toml` 中配置上述 JWKS 文件的路径：\n\n    ```toml\n    [security]\n    auth-token-jwks = \"JWKS.json\"\n    ```\n\n2. 启动 `tidb-server`，并定期更新保存 JWKS 至 `auth-token-jwks` 指定的路径。\n\n3. 创建使用 `tidb_auth_token` 认证的用户，并根据需要通过 `REQUIRE TOKEN_ISSUER` 和 `ATTRIBUTE '{\"email\": \"xxxx@pingcap.com\"}` 指定 `iss` 与 `email` 信息。\n\n    例如，创建一个使用 `tidb_auth_token` 认证的用户 `user@pingcap.com`：\n\n    ```sql\n    CREATE USER 'user@pingcap.com' IDENTIFIED WITH 'tidb_auth_token' REQUIRE TOKEN_ISSUER 'issuer-abc' ATTRIBUTE '{\"email\": \"user@pingcap.com\"}';\n    ```\n\n4. 生成并签发用于认证的 token，通过 mysql 客户端的 `mysql_clear_text` 插件进行认证。\n\n    通过 `go install github.com/cbcwestwolf/generate_jwt` 安装 JWT 生成工具。该工具仅用于生成测试 `tidb_auth_token` 的 JWT。例如：\n\n    ```text\n    generate_jwt --kid \"the-key-id-0\" --sub \"user@pingcap.com\" --email \"user@pingcap.com\" --iss \"issuer-abc\"\n    ```\n\n    打印公钥和 token 形式如下：\n\n    ```text\n    -----BEGIN PUBLIC KEY-----\n    MIIBCgKCAQEAq8G5n9XBidxmBMVJKLOBsmdOHrCqGf17y9+VUXingwDUZxRp2Xbu\n    LZLbJtLgcln1lC0L9BsogrWf7+pDhAzWovO6Ai4Aybu00tJ2u0g4j1aLiDdsy0gy\n    vSb5FBoL08jFIH7t/JzMt4JpF487AjzvITwZZcnsrB9a9sdn2E5B/aZmpDGi2+Is\n    f5osnlw0zvveTwiMo9ba416VIzjntAVEvqMFHK7vyHqXbfqUPAyhjLO+iee99Tg5\n    AlGfjo1s6FjeML4xX7sAMGEy8FVBWNfpRU7ryTWoSn2adzyA/FVmtBvJNQBCMrrA\n    hXDTMJ5FNi8zHhvzyBKHU0kBTS1UNUbP9wIDAQAB\n    -----END PUBLIC KEY-----\n\n    eyJhbGciOiJSUzI1NiIsImtpZCI6InRoZS1rZXktaWQtMCIsInR5cCI6IkpXVCJ9.eyJlbWFpbCI6InVzZXJAcGluZ2NhcC5jb20iLCJleHAiOjE3MDMzMDU0OTQsImlhdCI6MTcwMzMwNDU5NCwiaXNzIjoiaXNzdWVyLWFiYyIsInN1YiI6InVzZXJAcGluZ2NhcC5jb20ifQ.T4QPh2hTB5on5xCuvtWiZiDTuuKvckggNHtNaovm1F4RvwUv15GyOqj9yMstE-wSoV5eLEcPC2HgE6eN1C6yH_f4CU-A6n3dm9F1w-oLbjts7aYCl8OHycVYnq609fNnb8JLsQAmd1Zn9C0JW899-WSOQtvjLqVSPe9prH-cWaBVDQXzUJKxwywQzk9v-Z1Njt9H3Rn9vvwwJEEPI16VnaNK38I7YG-1LN4fAG9jZ6Zwvz7vb_s4TW7xccFf3dIhWTEwOQ5jDPCeYkwraRXU8NC6DPF_duSrYJc7d7Nu9Z2cr-E4i1Rt_IiRTuIIzzKlcQGg7jd9AGEfGe_SowsA-w\n    ```\n\n    复制上面最后一行的 token 用于登录：\n\n    ```Shell\n    mycli -h 127.0.0.1 -P 4000 -u 'user@pingcap.com' -p '<the-token-generated>'\n    ```\n\n    注意这里使用的 mysql 客户端必须支持 `mysql_clear_password` 插件。[mycli](https://www.mycli.net/) 默认开启这一插件，如果使用 [mysql 命令行客户端](https://dev.mysql.com/doc/refman/8.0/en/mysql.html) 则需要 `--enable-cleartext-plugin` 选项来开启这个插件：\n\n    ```Shell\n    mysql -h 127.0.0.1 -P 4000 -u 'user@pingcap.com' -p'<the-token-generated>' --enable-cleartext-plugin\n    ```\n\n    如果在生成 token 的时候指定了错误的 `--sub`（比如 `--sub \"wronguser@pingcap.com\"`），则无法使用该 token 进行认证。\n\n可以使用 [jwt.io](https://jwt.io/) 提供的 debugger 对 token 进行编解码。\n"
        },
        {
          "name": "shard-row-id-bits.md",
          "type": "blob",
          "size": 1.0810546875,
          "content": "---\ntitle: SHARD_ROW_ID_BITS\nsummary: 介绍 TiDB 的 `SHARD_ROW_ID_BITS` 表属性。\naliases: ['/docs-cn/dev/shard-row-id-bits/']\n---\n\n# SHARD_ROW_ID_BITS\n\n本文介绍表属性 `SHARD_ROW_ID_BITS`，它用来设置隐式 `_tidb_rowid` 分片数量的 bit 位数。\n\n## 基本概念\n\n对于非[聚簇索引](/clustered-indexes.md)主键或没有主键的表，TiDB 会使用一个隐式的自增 rowid。大量执行 `INSERT` 插入语句时会把数据集中写入单个 Region，造成写入热点。\n\n通过设置 `SHARD_ROW_ID_BITS`，可以把 rowid 打散写入多个不同的 Region，缓解写入热点问题。\n\n- `SHARD_ROW_ID_BITS = 4` 表示 16 个分片\n- `SHARD_ROW_ID_BITS = 6` 表示 64 个分片\n- `SHARD_ROW_ID_BITS = 0` 表示默认值 1 个分片\n\n关于 `SHARD_ROW_ID_BITS` 的更多使用信息，可参考[使用 SHARD_ROW_ID_BITS 处理热点表](/troubleshoot-hot-spot-issues.md#使用-shard_row_id_bits-处理热点表)。\n\n## 语句示例\n\n```sql\nCREATE TABLE t (\n    id INT PRIMARY KEY NONCLUSTERED\n) SHARD_ROW_ID_BITS = 4;\n```\n\n```sql\nALTER TABLE t SHARD_ROW_ID_BITS = 4;\n```\n"
        },
        {
          "name": "smooth-upgrade-tidb.md",
          "type": "blob",
          "size": 6.359375,
          "content": "---\ntitle: 平滑升级 TiDB\nsummary: 本文介绍支持无需手动取消 DDL 的平滑升级集群功能。\n---\n\n# 平滑升级 TiDB\n\n本文档介绍 TiDB 的平滑升级集群功能，支持无需手动取消 DDL 的操作。\n\n从 v7.1.0 起，当将 TiDB 升级至更高的版本时，TiDB 支持平滑升级功能，取消了升级过程中的限制（你需要保证升级过程中无用户发起的 DDL 操作），提供更平滑的升级体验。\n\n## 版本支持情况\n\n依据是否需要开关控制，可分为两种支持方式：\n\n* 无需开关控制，默认开启此功能的方式。目前支持此方式的版本分别是 v7.1.0，v7.1.1，v7.2.0，和 v7.3.0。具体支持升级版本的情况：\n    * 从 v7.1.0 升级到 v7.1.1、v7.2.0 或 v7.3.0 版本\n    * 从 v7.1.1 升级到 v7.2.0 或 v7.3.0 版本\n    * 从 v7.2.0 升级到 v7.3.0 版本\n\n* 通过是否发送 `/upgrade/start` HTTP 请求控制此功能开关。即此功能默认关闭，可通过发送 `/upgrade/start` 请求，开启此功能。具体方式可以参考：[TiDB HTTP API 文档](https://github.com/pingcap/tidb/blob/master/docs/tidb_http_api.md)。具体版本情况：\n    * 从 v7.1.2 以及它之后的 v7.1 版本（即 >= v7.1.2）升级到 v7.4.0 及更高版本\n    * 从 v7.4.0 升级到更高的版本\n\n具体版本支持的升级方式，请参考下表：\n\n| 原版本 | 升级后版本 | 升级的升级方式 | 备注 |\n|------|--------|-------------|-------------|\n| < v7.1.0  | 任意版本                  | 不支持平滑升级方式 | |\n| v7.1.0    | v7.1.1、v7.2.0 或 v7.3.0  | 无需额外操作，自动支持平滑升级 | 实验特性。可能遇到 [#44760](https://github.com/pingcap/tidb/pull/44760) 问题 |\n| v7.1.1    | v7.2.0 或 v7.3.0         | 无需额外操作，自动支持平滑升级 | 实验特性 |\n| v7.2.0    | v7.3.0                   | 无需额外操作，自动支持平滑升级 | 实验特性 |\n| [v7.1.2, v7.2.0)                     | [v7.1.2, v7.2.0) | 通过发送 `/upgrade/start` HTTP 请求开启平滑升级，具体方式有两种：[TiUP 方式](#tiup-方式)；[其它方式](#其它方式) | 不开启平滑升级时，需确保升级时无 DDL 操作。 |\n| [v7.1.2, v7.2.0) 或 >= v7.4.0             | >= v7.4.0 | 通过发送 `/upgrade/start` HTTP 请求开启平滑升级，具体方式有两种：[TiUP 方式](#tiup-方式)；[其它方式](#其它方式)    | 不开启平滑升级时，需确保升级时无 DDL 操作。 |\n| v7.1.0、v7.1.1、v7.2.0、v7.3.0     | >= v7.4.0 | 不支持平滑升级方式 | |\n\n## 功能简介\n\nTiDB 引入平滑升级功能前，对于升级过程中的 DDL 操作有如下限制：\n\n- 在升级过程中执行 DDL 操作，TiDB 可能会出现未定义的行为。\n- 在 DDL 操作执行过程中升级 TiDB，TiDB 可能会出现未定义的行为。\n\n上述限制可概括为，你需要保证在升级过程中无用户发起的 DDL 操作。引入平滑升级后，TiDB 升级过程不再受此限制。\n\n更多详情，请参考[使用 TiUP 升级 TiDB](/upgrade-tidb-using-tiup.md#使用-tiup-升级-tidb) 中的警告部分。\n\n### 升级方式及步骤\n\n#### TiUP 方式\n\nTiUP 会在 v1.14.0 版本自适应支持此功能，即无需特殊操作，直接使用 `tiup cluster upgrade` 操作流程即可。注意目前不支持 `tiup cluster patch` 方式。\n\n#### TiDB Operator 方式\n\n目前不支持此功能，会尽早自适应支持此功能。\n\n#### 其它方式\n\n手动升级或者使用脚本升级的操作如下：\n\n1. 给集群中的任意一台 TiDB 发送 HTTP 升级开始请求：`curl -X POST http://{TiDBIP}:10080/upgrade/start`。\n   * TiDB 集群会进入 **Upgrading** 状态。\n   * 接下来将要执行的 DDL 操作都会被暂停。\n\n2. 替换 TiDB binary，并进行滚动升级。此过程和原升级过程一致。\n   * 执行升级过程中的系统 DDL 操作。\n\n3. 等集群中所有 TiDB 升级成功后，给任意一台 TiDB 发送 HTTP 升级结束请求：`curl -X POST http://{TiDBIP}:10080/upgrade/finish`。\n   * 恢复被暂停的用户的 DDL 操作。\n\n其中，恢复的 DDL job 仍会按升级前的顺序执行。\n\n## 使用限制\n\n使用平滑升级功能时，需要注意以下限制。\n\n> **注意：**\n>\n> 本小节中的使用限制不仅适用于使用平滑升级功能的场景，也适用于[使用 TiUP 升级 TiDB](/upgrade-tidb-using-tiup.md#使用-tiup-升级-tidb)的场景。\n\n### 用户操作限制\n\n* 在升级前有如下两种限制：\n\n    * 如果集群中存在正在处理的 canceling DDL job，即有正在被处理的 DDL job 被用户取消了，由于处于 canceling 状态的 job 无法被 `pause`，TiDB 会尝试重试。如果重试失败，会报错并退出升级。\n\n    * 如果当前集群版本 < v8.1.0，且 [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)已启用，请关闭 TiDB 分布式执行框架（即将 [`tidb_enable_dist_task`](/system-variables.md#tidb_enable_dist_task-从-v710-版本开始引入) 设置为 `OFF`），并确保所有分布式 `ADD INDEX` 和 `IMPORT INTO` 任务已完成，或者取消这些任务并等待升级完成后重新开始。否则，升级期间的 `ADD INDEX` 操作可能导致数据索引不一致。如果当前集群版本 >= v8.1.0，则无需关闭 TiDB 分布式执行框架，请忽略此限制。\n\n* 在使用 TiUP 进行升级的场景下，由于 TiUP 升级存在超时时间，如果在升级之前集群中有大量 DDL（超过 300 条）正在处理队列中等待执行，则此次升级可能会失败。\n\n* 在升级过程中，不允许以下操作：\n\n    * 对系统表（`mysql.*`、`information_schema.*`、`performance_schema.*`、`metrics_schema.*`）进行 DDL 操作。\n\n    * 执行手动取消 DDL job 操作：`ADMIN CANCEL DDL JOBS job_id [, job_id] ...;`。\n\n    * 导入数据。\n\n### 工具使用限制\n\n在升级过程中，不支持使用以下工具：\n\n* BR：BR 可能会将处于 paused 状态的 DDL 拷贝到 TiDB 中，而此状态的 DDL 不能自动 resume，可能导致后续 DDL 卡住的情况。\n\n* DM 和 TiCDC：如果在升级过程中使用 DM 和 TiCDC 向 TiDB 导入 SQL，并且其中包含 DDL 操作，则该导入操作会被阻塞，并可能出现未定义错误。\n\n### 插件使用限制\n\nTiDB 安装的插件可能自带 DDL 操作。然而，在升级过程中，如果这些插件自带的 DDL 操作针对非系统表进行，可能导致升级过程出现问题。"
        },
        {
          "name": "sql-logical-optimization.md",
          "type": "blob",
          "size": 1.2666015625,
          "content": "---\ntitle: 逻辑优化\naliases: ['/docs-cn/dev/sql-logical-optimization/']\nsummary: 本章节介绍了 TiDB 查询计划的关键逻辑改写，包括子查询优化、列裁剪、关联子查询去关联、Max/Min 消除、谓词下推、分区裁剪、TopN 和 Limit 下推以及 Join 重排序。这些改写帮助 TiDB 生成最终的查询计划，提高查询效率。\n---\n\n# 逻辑优化\n\n本章节将对一些比较关键的逻辑改写进行说明，帮助大家理解 TiDB 如何生成最终的查询计划。比如在 TiDB 输入 `select * from t where t.a in (select t1.a from t1 where t1.b=t.b)` 这个查询时，在最终的执行计划中将看不到这个 `t.a in (select t1.a from t1 where t1.b=t.b)` 这个 `IN` 子查询的存在，这便是因为 TiDB 对这里进行了一些改写。\n\n本章节会介绍如下几个关键改写：\n\n- [子查询相关的优化](/subquery-optimization.md)\n- [列裁剪](/column-pruning.md)\n- [关联子查询去关联](/correlated-subquery-optimization.md)\n- [Max/Min 消除](/max-min-eliminate.md)\n- [谓词下推](/predicate-push-down.md)\n- [分区裁剪](/partition-pruning.md)\n- [TopN 和 Limit 下推](/topn-limit-push-down.md)\n- [Join Reorder](/join-reorder.md)\n- [从窗口函数中推导 TopN 或 Limit](/derive-topn-from-window.md)\n"
        },
        {
          "name": "sql-mode.md",
          "type": "blob",
          "size": 7.2099609375,
          "content": "---\ntitle: SQL 模式\naliases: ['/docs-cn/dev/sql-mode/','/docs-cn/dev/reference/sql/sql-mode/']\nsummary: TiDB 服务器采用不同 SQL 模式来操作，可以使用 `SET [SESSION | GLOBAL] sql_mode='modes'` 语句设置 SQL 模式。`GLOBAL` 级别的 SQL 模式需要 `SUPER` 权限，影响新连接；`SESSION` 级别的 SQL 模式只影响当前客户端。重要的 sql_mode 值包括 `ANSI`、`STRICT_TRANS_TABLES` 和 `TRADITIONAL`。SQL mode 列表包括 `PIPES_AS_CONCAT`、`ANSI_QUOTES`、`IGNORE_SPACE`、`ONLY_FULL_GROUP_BY` 等。\n---\n\n# SQL 模式\n\nTiDB 服务器采用不同 SQL 模式来操作，且不同客户端可以应用不同模式。SQL 模式定义 TiDB 支持哪些 SQL 语法及执行哪种数据验证检查。\n\nTiDB 启动之后，你可以使用 `SET [ SESSION | GLOBAL ] sql_mode='modes'` 语句设置 SQL 模式。\n\n- 设置 `GLOBAL` 级别的 SQL 模式时用户需要有 `SUPER` 权限，并且只会影响到从设置 SQL 模式开始后续新建立的连接（注：老连接不受影响）。\n- `SESSION` 级别的 SQL 模式的变化只会影响当前的客户端。\n\n在该语句中，`modes` 是用逗号 (`,`) 间隔开的一系列不同的模式。使用 `SELECT @@sql_mode` 语句查询当前 SQL 模式，SQL 模式默认值：`ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION`。\n\n## 重要的 sql_mode 值\n\n* `ANSI`：符合标准 SQL，对数据进行校验，如果不符合定义类型或长度，对数据类型调整或截断保存，且返回 `warning` 警告。\n* `STRICT_TRANS_TABLES`：严格模式，对数据进行严格校验，当数据出现错误时，无法被插入到表中，并且返回错误。\n* `TRADITIONAL`：采用此模式使 TiDB 的行为像“传统” SQL 数据库系统，当在列中插入不正确的值时“给出错误而不是警告”，一旦发现错误立即放弃 `INSERT` 或 `UPDATE`。\n\n## SQL mode 列表，如下\n\n| 名称 | 含义 |\n| --- | --- |\n| `PIPES_AS_CONCAT` | 将 \"\\|\\|\" 视为字符串连接操作符 (`＋`)（同 `CONCAT()`），而不视为 `OR`（支持）|\n| `ANSI_QUOTES` | 将 `\"` 视为识别符，如果启用 `ANSI_QUOTES`，只单引号内的会被认为是 String Literals，双引号被解释为识别符，因此不能用双引号来引用字符串（支持）|\n| `IGNORE_SPACE` | 若开启该模式，系统忽略空格。例如：“user” 和 “user “ 是相同的（支持）|\n| `ONLY_FULL_GROUP_BY` | 如果未被聚合函数处理或未被 `GROUP BY` 的列出现在 `SELECT`、`HAVING`、`ORDER BY` 中，此 SQL 不合法，因为这种列被查询展示出来不合常规。该设置受系统变量 [`tidb_enable_new_only_full_group_by_check`](/system-variables.md#tidb_enable_new_only_full_group_by_check-从-v610-版本开始引入) 影响。（支持）|\n| `NO_UNSIGNED_SUBTRACTION` | 在减运算中，如果某个操作数没有符号，不要将结果标记为 `UNSIGNED`（支持）|\n| `NO_DIR_IN_CREATE` | 创建表时，忽视所有 `INDEX DIRECTORY` 和 `DATA DIRECTORY` 指令，该选项仅对从复制服务器有用 （仅语法支持）|\n| `NO_KEY_OPTIONS` | 使用 `SHOW CREATE TABLE` 时不会输出 MySQL 特有的语法部分，如 `ENGINE`，使用 mysqldump 跨 DB 种类迁移的时需要考虑此选项（仅语法支持）|\n| `NO_FIELD_OPTIONS` | 使用 `SHOW CREATE TABLE` 时不会输出 MySQL 特有的语法部分，如 `ENGINE`，使用 mysqldump 跨 DB 种类迁移的时需要考虑此选项（仅语法支持）|\n| `NO_TABLE_OPTIONS` | 使用 `SHOW CREATE TABLE` 时不会输出 MySQL 特有的语法部分，如 `ENGINE`，使用 mysqldump 跨 DB 种类迁移的时需要考虑此选项（仅语法支持）|\n| `NO_AUTO_VALUE_ON_ZERO` | 若启用该模式，在 [`AUTO_INCREMENT`](/auto-increment.md) 列传入的值是 `0` 或者具体数值时系统直接将该值写入此列，传入 `NULL` 时系统自动生成下一个序列号（支持）|\n| `NO_BACKSLASH_ESCAPES` | 若启用该模式，`\\` 反斜杠符号仅代表它自己（支持）|\n| `STRICT_TRANS_TABLES` | 对于事务存储引擎启用严格模式，insert非法值之后，回滚整条语句（支持）|\n| `STRICT_ALL_TABLES` | 对于事务型表，写入非法值之后，回滚整个事务语句（支持）|\n| `NO_ZERO_IN_DATE` | 在严格模式，不接受月或日部分为0的日期。如果使用IGNORE选项，我们为类似的日期插入'0000-00-00'。在非严格模式，可以接受该日期，但会生成警告（支持）\n| `NO_ZERO_DATE` | 在严格模式，不要将 '0000-00-00'做为合法日期。你仍然可以用 `IGNORE` 选项插入零日期。在非严格模式，可以接受该日期，但会生成警告（支持）|\n| `ALLOW_INVALID_DATES` | 不检查全部日期的合法性，仅检查月份值在 1 到 12 及日期值在 1 到 31 之间，仅适用于 `DATE` 和 `DATATIME` 列，`TIMESTAMP` 列需要全部检查其合法性（支持）|\n| `ERROR_FOR_DIVISION_BY_ZERO` | 若启用该模式，在 `INSERT` 或 `UPDATE` 过程中，被除数为 `0` 值时，系统产生错误 <br/> 若未启用该模式，被除数为 0 值时，系统产生警告，并用 `NULL` 代替（支持） |\n| `NO_AUTO_CREATE_USER` | 防止 `GRANT` 自动创建新用户，但指定密码除外（支持）|\n| `HIGH_NOT_PRECEDENCE` | NOT 操作符的优先级是表达式。例如：`NOT a BETWEEN b AND c` 被解释为 `NOT (a BETWEEN b AND c)`。在部份旧版本 MySQL 中，表达式被解释为 `(NOT a) BETWEEN b AND c`（支持） |\n| `NO_ENGINE_SUBSTITUTION` | 如果需要的存储引擎被禁用或未编译，可以防止自动替换存储引擎（仅语法支持）|\n| `PAD_CHAR_TO_FULL_LENGTH` | 若启用该模式，系统对于 `CHAR` 类型不会截断尾部空格（仅语法支持。该模式[在 MySQL 8.0 中已废弃](https://dev.mysql.com/doc/refman/8.0/en/sql-mode.html#sqlmode_pad_char_to_full_length)。）|\n| `REAL_AS_FLOAT` | 将 `REAL` 视为 `FLOAT` 的同义词，而不是 `DOUBLE` 的同义词（支持）|\n| `POSTGRESQL` | 等同于 `PIPES_AS_CONCAT`、`ANSI_QUOTES`、`IGNORE_SPACE`、`NO_KEY_OPTIONS`、`NO_TABLE_OPTIONS`、`NO_FIELD_OPTIONS`（仅语法支持）|\n| `MSSQL` | 等同于 `PIPES_AS_CONCAT`、`ANSI_QUOTES`、`IGNORE_SPACE`、`NO_KEY_OPTIONS`、`NO_TABLE_OPTIONS`、 `NO_FIELD_OPTIONS`（仅语法支持）|\n| `DB2` | 等同于 `PIPES_AS_CONCAT`、`ANSI_QUOTES`、`IGNORE_SPACE`、`NO_KEY_OPTIONS`、`NO_TABLE_OPTIONS`、`NO_FIELD_OPTIONS`（仅语法支持）|\n| `MAXDB` | 等同于 `PIPES_AS_CONCAT`、`ANSI_QUOTES`、`IGNORE_SPACE`、`NO_KEY_OPTIONS`、`NO_TABLE_OPTIONS`、`NO_FIELD_OPTIONS`、`NO_AUTO_CREATE_USER`（支持）|\n| `MySQL323` | 等同于 `NO_FIELD_OPTIONS`、`HIGH_NOT_PRECEDENCE`（仅语法支持）|\n| `MYSQL40` | 等同于 `NO_FIELD_OPTIONS`、`HIGH_NOT_PRECEDENCE`（仅语法支持）|\n| `ANSI` | 等同于 `REAL_AS_FLOAT`、`PIPES_AS_CONCAT`、`ANSI_QUOTES`、`IGNORE_SPACE`（仅语法支持）|\n| `TRADITIONAL` | 等同于 `STRICT_TRANS_TABLES`、`STRICT_ALL_TABLES`、`NO_ZERO_IN_DATE`、`NO_ZERO_DATE`、`ERROR_FOR_DIVISION_BY_ZERO`、`NO_AUTO_CREATE_USER`（仅语法支持）|\n| `ORACLE` | 等同于 `PIPES_AS_CONCAT`、`ANSI_QUOTES`、`IGNORE_SPACE`、`NO_KEY_OPTIONS`、`NO_TABLE_OPTIONS`、`NO_FIELD_OPTIONS`、`NO_AUTO_CREATE_USER`（仅语法支持）|\n"
        },
        {
          "name": "sql-non-prepared-plan-cache.md",
          "type": "blob",
          "size": 9.5830078125,
          "content": "---\ntitle: 非 Prepare 语句执行计划缓存\nsummary: 介绍 TiDB 中非 Prepare 语句执行计划缓存的原理、使用方法及示例。\n---\n\n# 非 Prepare 语句执行计划缓存\n\n对于某些非 `PREPARE` 语句，TiDB 可以像 [`Prepare`/`Execute` 语句](/sql-prepared-plan-cache.md)一样支持执行计划缓存。这可以让这些语句跳过优化器阶段，以提升性能。\n\n开启非 Prepare 语句执行计划缓存可能会带来额外的内存和 CPU 开销，并不一定适用于所有场景。建议参考[性能收益](#性能收益)和[内存监控](#监控)章节，根据具体的使用情况决定是否开启该功能。\n\n## 原理\n\nNon-Prepared Plan Cache 为会话级别，并且与 [Prepared Plan Cache](/sql-prepared-plan-cache.md) 共用一个缓存。Non-Prepared Plan Cache 功能的基本原理如下：\n\n1. 开启 Non-Prepared Plan Cache 后，TiDB 首先根据 AST（抽象语法树）对查询进行参数化。例如，将 `SELECT * FROM t WHERE b < 10 AND a = 1` 参数化为 `SELECT * FROM t WHERE b < ? and a = ?`。\n2. 然后，使用参数化后的查询在 Plan Cache 中查找。\n3. 如果能找到可以直接复用的计划，则直接使用，并跳过整个优化过程。\n4. 否则，继续进行查询优化，并在最后将生成的计划放回到缓存中，以便下次复用。\n\n## 使用方法\n\n目前，你可以通过 [`tidb_enable_non_prepared_plan_cache`](/system-variables.md#tidb_enable_non_prepared_plan_cache) 开启或关闭 Non-Prepared Plan Cache。同时，你还可以通过 [`tidb_session_plan_cache_size`](/system-variables.md#tidb_session_plan_cache_size-从-v710-版本开始引入) 来控制 Plan Cache 的大小。当缓存的计划数超过 `tidb_session_plan_cache_size` 时，TiDB 会使用 LRU (Least Recently Used) 策略进行逐出。\n\n从 v7.1.0 开始，你可以通过变量 [`tidb_plan_cache_max_plan_size`](/system-variables.md#tidb_plan_cache_max_plan_size-从-v710-版本开始引入) 来设置可以缓存的计划的最大大小，默认为 2 MB。超过该值的执行计划将不会被缓存到 Plan Cache 中。\n\n> **注意：**\n>\n> `tidb_session_plan_cache_size` 定义的内存会被 Prepared 和 Non-Prepared Plan Cache 共享。如果集群已经开启 Prepared Plan Cache，那么开启 Non-Prepared Plan Cache 可能降低原先 Prepared Plan Cache 的命中率。\n\n## 示例\n\n下面是一个使用示例：\n\n1. 创建用于测试的表 `t`：\n\n    ```sql\n    CREATE TABLE t (a INT, b INT, KEY(b));\n    ```\n\n2. 开启 Non-Prepared Plan Cache：\n\n    ```sql\n    SET tidb_enable_non_prepared_plan_cache = ON;\n    ```\n\n3. 依次执行以下查询：\n\n    ```sql\n    SELECT * FROM t WHERE b < 10 AND a = 1;\n    SELECT * FROM t WHERE b < 5 AND a = 2;\n    ```\n\n4. 查看第二个查询语句是否命中缓存：\n\n    ```sql\n    SELECT @@last_plan_from_cache;\n    ```\n\n    输出结果中 `last_plan_from_cache` 的值为 `1`，表示第二次执行的查询计划来自于缓存：\n\n    ```sql\n    +------------------------+\n    | @@last_plan_from_cache |\n    +------------------------+\n    |                      1 |\n    +------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n## 限制\n\n### 缓存限制\n\nTiDB 对参数化后形式相同的查询，只能缓存一个计划。例如，对于 `SELECT * FROM t WHERE a < 1` 和 `SELECT * FROM t WHERE a < 100000` 这两个查询语句，由于参数化后的形式相同，均为 `SELECT * FROM t WHERE a < ?`，因此它们会共用一个计划。\n\n如果由此产生性能问题，可以使用 `ignore_plan_cache()` Hint 忽略计划缓存中的计划，让优化器每次重新为 SQL 生成执行计划。如果无法修改 SQL，可以通过创建 binding 来解决，例如 `CREATE BINDING FOR SELECT ... USING SELECT /*+ ignore_plan_cache() */ ...`。\n\n### 使用限制\n\n由于上述风险以及执行计划缓存只在简单查询上有明显收益（如果查询较为复杂，查询本身执行时间较长，使用执行计划缓存收益不大），TiDB 目前对 Non-Prepared Plan Cache 的生效范围有严格的限制。具体限制如下：\n\n- [Prepared Plan Cache](/sql-prepared-plan-cache.md) 不支持的查询或者计划，Non-Prepared Plan Cache 也不支持。\n- 不支持包含 `Window` 或 `Having` 的查询。\n- 不支持包含三张表及以上 `Join` 或子查询的查询。\n- 不支持 `ORDER BY` 或者 `GROUP BY` 后直接带数字或者表达式的查询，如 `ORDER BY 1`、`GROUP BY a+1`。仅支持 `ORDER BY column_name` 和 `GROUP BY column_name`。\n- 不支持过滤条件中包含 `JSON`、`ENUM`、`SET` 或 `BIT` 类型的列的查询，例如 `SELECT * FROM t WHERE json_col = '{}'`。\n- 不支持过滤条件中出现 `NULL` 值的查询，例如 `SELECT * FROM t WHERE a is NULL`。\n- 默认不支持参数化后参数个数超过 200 个的查询，例如 `SELECT * FROM t WHERE a in (1, 2, 3, ... 201)`。从 v7.3.0 开始，你可以通过在 [`tidb_opt_fix_control`](/system-variables.md#tidb_opt_fix_control-从-v653-和-v710-版本开始引入) 系统变量中设置 [`44823`](/optimizer-fix-controls.md#44823-从-v730-版本开始引入) 这个 Fix 来调整该限制。\n- 不支持访问虚拟列、临时表、视图、或内存表的查询，例如 `SELECT * FROM INFORMATION_SCHEMA.COLUMNS`，其中 `COLUMNS` 为 TiDB 内存表。\n- 不支持带有 Hint 或有 Binding 的查询。\n- 默认不支持 DML 语句或包含 `FOR UPDATE` 的查询语句。若要启用支持，你可以执行 `SET tidb_enable_non_prepared_plan_cache_for_dml = ON`。\n\n开启此功能后，优化器会对查询进行快速判断，如果不满足 Non-Prepared Plan Cache 的支持条件，则会走正常的优化流程。\n\n## 性能收益\n\n在内部测试中，开启 Non-Prepared Plan Cache 功能在大多数 TP 场景下可以获得显著的性能收益。例如：TPCC 测试中性能提升约 4%，一些 Bank 负载上提升超过 10%，在 Sysbench RangeScan 上提升达到 15%。\n\n但是这个功能本身也会带来额外的 CPU 和内存开销，包括判断查询是否支持、对查询进行参数化、在 Plan Cache 中进行搜索等。如果负载中的多数查询无法被 Cache 命中，开启此功能反而可能影响性能。\n\n此时，你需要观察 Grafana 监控中的 **Queries Using Plan Cache OPS** 面板中的 `non-prepared` 指标和 **Plan Cache Miss OPS** 面板中的 `non-prepared-unsupported` 指标。如果大多数查询都无法被支持，只有少部分查询能命中 Plan Cache，此时你可以关闭此功能。\n\n![non-prepared-unsupported](/media/non-prepapred-plan-cache-unsupprot.png)\n\n## 诊断\n\n开启 Non-Prepared Plan Cache 后，可以使用 `EXPLAIN FORMAT='plan_cache' SELECT ...` 语句验证查询是否能够命中缓存。对于无法命中缓存的查询，系统会通过 warning 的方式返回无法命中的原因。\n\n需要注意的是，如果不加 `FORMAT='plan_cache'`，则 `EXPLAIN` 语句永远不会命中缓存。\n\n执行下面 `EXPLAIN FORMAT='plan_cache'` 语句，查看查询是否能够命中：\n\n```sql\nEXPLAIN FORMAT='plan_cache' SELECT * FROM (SELECT a+1 FROM t) t;\n```\n\n输出结果示例如下：\n\n```sql\n3 rows in set, 1 warning (0.00 sec)\n```\n\n通过 `SHOW warnings;` 查看无法命中缓存的查询信息：\n\n```sql\nSHOW warnings;\n```\n\n输出结果示例如下：\n\n```sql\n+---------+------+-------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                       |\n+---------+------+-------------------------------------------------------------------------------+\n| Warning | 1105 | skip non-prepared plan-cache: queries that have sub-queries are not supported |\n+---------+------+-------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n可以看到在上述例子中，由于 Non-Prepared Plan Cache 不支持 `+` 操作，所以无法命中缓存。\n\n## 监控\n\n开启 Non-Prepared Plan Cache 后，可以在以下几个面板中查看缓存的内存使用情况、缓存中计划的个数、缓存命中的情况等信息。\n\n![non-prepared-plan-cache](/media/tidb-non-prepared-plan-cache-metrics.png)\n\n`statements_summary` 表和慢查询日志也会体现缓存的命中情况。下面是查看 `statements_summary` 表中缓存命中情况的例子：\n\n1. 创建表 `t`：\n\n    ```sql\n    CREATE TABLE t (a int);\n    ```\n\n2. 打开 Non-Prepared Plan Cache 开关：\n\n    ```sql\n    SET @@tidb_enable_non_prepared_plan_cache = ON;\n    ```\n\n3. 依次执行以下三个查询：\n\n    ```sql\n    SELECT * FROM t WHERE a<1;\n    SELECT * FROM t WHERE a<2;\n    SELECT * FROM t WHERE a<3;\n    ```\n\n4. 查询 `statements_summary` 表查看查询命中缓存的情况：\n\n    ```sql\n    SELECT digest_text, query_sample_text, exec_count, plan_in_cache, plan_cache_hits FROM INFORMATION_SCHEMA.STATEMENTS_SUMMARY WHERE query_sample_text LIKE '%SELECT * FROM %';\n    ```\n\n    输出结果如下：\n\n    ```sql\n    +---------------------------------+------------------------------------------+------------+---------------+-----------------+\n    | digest_text                     | query_sample_text                        | exec_count | plan_in_cache | plan_cache_hits |\n    +---------------------------------+------------------------------------------+------------+---------------+-----------------+\n    | SELECT * FROM `t` WHERE `a` < ? | SELECT * FROM t WHERE a<1                |          3 |             1 |               2 |\n    +---------------------------------+------------------------------------------+------------+---------------+-----------------+\n    1 row in set (0.01 sec)\n    ```\n\n    可以看到，查询执行了三次且命中缓存两次。"
        },
        {
          "name": "sql-optimization-concepts.md",
          "type": "blob",
          "size": 1.58984375,
          "content": "---\ntitle: SQL 优化流程简介\naliases: ['/docs-cn/dev/sql-optimization-concepts/','/docs-cn/dev/reference/performance/sql-optimizer-overview/']\nsummary: TiDB 中的 SQL 优化流程包括查询文本解析、逻辑等价变化和最终执行计划生成。经过 parser 解析和合法性验证后，TiDB 会对查询进行逻辑上的等价变化，使得查询在逻辑执行计划上更易处理。之后根据数据分布和执行开销生成最终执行计划。同时，TiDB 在执行 PREPARE 语句时可以选择开启缓存来降低执行计划生成的开销。\n---\n\n# SQL 优化流程简介\n\n在 TiDB 中，从输入的查询文本到最终的执行计划执行结果的过程可以见下图。\n\n![SQL Optimization](/media/sql-optimization.png)\n\n在经过了 `parser` 对原始查询文本的解析以及一些简单的合法性验证后，TiDB 首先会对查询做一些逻辑上的等价变化，详细的变化可以查询[逻辑优化](/sql-logical-optimization.md)章节。\n\n通过这些等价变化，使得这个查询在逻辑执行计划上可以变得更易于处理。在等价变化结束之后，TiDB 会得到一个与原始查询等价的查询计划结构，之后根据数据分布、以及一个算子具体的执行开销，来获得一个最终的执行计划，这部分内容可以查询[物理优化](/sql-physical-optimization.md)章节。\n\n同时，TiDB 在执行 [`PREPARE`](/sql-statements/sql-statement-prepare.md) 语句时，可以选择开启缓存来降低 TiDB 生成执行计划的开销，这部分内容会在[执行计划缓存](/sql-prepared-plan-cache.md)一节中介绍。"
        },
        {
          "name": "sql-physical-optimization.md",
          "type": "blob",
          "size": 1.8896484375,
          "content": "---\ntitle: 物理优化\naliases: ['/docs-cn/dev/sql-physical-optimization/']\nsummary: 物理优化是基于代价的优化，为逻辑执行计划制定物理执行计划。优化器根据数据统计信息选择时间复杂度、资源消耗和物理属性最小的物理执行计划。TiDB 执行计划文档介绍了索引选择、统计信息、错误索引解决方案、Distinct 优化和代价模型。\n---\n\n# 物理优化\n\n物理优化是基于代价的优化，为上一阶段产生的逻辑执行计划制定物理执行计划。这一阶段中，优化器会为逻辑执行计划中的每个算子选择具体的物理实现。逻辑算子的不同物理实现有着不同的时间复杂度、资源消耗和物理属性等。在这个过程中，优化器会根据数据的统计信息来确定不同物理实现的代价，并选择整体代价最小的物理执行计划。\n\n[理解 TiDB 执行计划](/explain-overview.md)文档中对每个物理算子进行了一些介绍，本章节重点介绍以下方面：\n\n- [索引的选择](/choose-index.md)：介绍在一张表有多个索引时，TiDB 如何选择最优的索引来访问表。\n- [常规统计信息](/statistics.md)：介绍 TiDB 收集了哪些常规统计信息来获得表的数据分布情况。\n- [扩展统计信息](/extended-statistics.md)：介绍如何使用扩展统计信息指导优化器。\n- [错误索引的解决方案](/wrong-index-solution.md)：介绍当发现 TiDB 索引选错时，你应该使用哪些手段使其使用正确的索引。\n- [Distinct 优化](/agg-distinct-optimization.md)：介绍有关 `DISTINCT` 关键字的优化，包括其优缺点以及如何使用它。\n- [代价模型](/cost-model.md)：介绍在物理优化时，TiDB 怎么通过代价模型来选择一个最优的执行计划。\n- [Runtime Filter](/runtime-filter.md)：介绍如何通过动态生成 Filter 提升 MPP 场景下 Hash Join 的性能。"
        },
        {
          "name": "sql-plan-management.md",
          "type": "blob",
          "size": 49.8720703125,
          "content": "---\ntitle: 执行计划管理 (SPM)\nsummary: 介绍 TiDB 的执行计划管理 (SQL Plan Management) 功能。\naliases: ['/docs-cn/dev/sql-plan-management/','/docs-cn/dev/reference/performance/execution-plan-bind/','/docs-cn/dev/execution-plan-binding/']\n---\n\n# 执行计划管理 (SPM)\n\n执行计划管理，又称 SPM (SQL Plan Management)，是通过执行计划绑定，对执行计划进行人为干预的一系列功能，包括执行计划绑定、自动捕获绑定、自动演进绑定等。\n\n## 执行计划绑定 (SQL Binding)\n\n执行计划绑定是 SPM 的基础。在[优化器 Hints](/optimizer-hints.md) 中介绍了可以通过 Hint 的方式选择指定的执行计划，但有时需要在不修改 SQL 语句的情况下干预执行计划的选择。执行计划绑定功能使得可以在不修改 SQL 语句的情况下选择指定的执行计划。\n\n> **注意：**\n>\n> 要使用执行计划绑定，你需要拥有 `SUPER` 权限。如果在使用过程中系统提示权限不足，可参考[权限管理](/privilege-management.md)补充所需权限。\n\n### 创建绑定\n\n你可以根据 SQL 或者历史执行计划为指定的 SQL 语句创建绑定。\n\n#### 根据 SQL 创建绑定\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE [GLOBAL | SESSION] BINDING [FOR BindableStmt] USING BindableStmt;\n```\n\n该语句可以在 GLOBAL 或者 SESSION 作用域内为 SQL 绑定执行计划。目前，可创建执行计划绑定的 SQL 语句类型 (BindableStmt) 包括：`SELECT`、`DELETE`、`UPDATE` 和带有 `SELECT` 子查询的 `INSERT`/`REPLACE`。使用示例如下：\n\n```sql\nCREATE GLOBAL BINDING USING SELECT /*+ use_index(orders, orders_book_id_idx) */ * FROM orders;\nCREATE GLOBAL BINDING FOR SELECT * FROM orders USING SELECT /*+ use_index(orders, orders_book_id_idx) */ * FROM orders;\n```\n\n> **注意：**\n>\n> 绑定的优先级高于手工添加的 Hint，即在有绑定的时候执行带有 Hint 的语句时，该语句中控制优化器行为的 Hint 不会生效，但是其他类别的 Hint 仍然能够生效。\n\n其中，有两类特定的语法由于语法冲突不能创建执行计划绑定，创建时会报语法错误，例如：\n\n```sql\n-- 类型一：使用 `JOIN` 关键字但不通过 `USING` 关键字指定关联列的笛卡尔积\nCREATE GLOBAL BINDING for\n    SELECT * FROM orders o1 JOIN orders o2\nUSING\n    SELECT * FROM orders o1 JOIN orders o2;\n\n-- 类型二：包含了 `USING` 关键字的 `delete` 语句\nCREATE GLOBAL BINDING for\n    DELETE FROM users USING users JOIN orders ON users.id = orders.user_id\nUSING\n    DELETE FROM users USING users JOIN orders ON users.id = orders.user_id;\n```\n\n可以通过等价的 SQL 改写绕过这个语法冲突的问题。例如，上述两个例子可以改写为：\n\n```sql\n\n-- 类型一的改写：去掉 `JOIN` 关键字，用逗号代替\nCREATE GLOBAL BINDING for\n    SELECT * FROM orders o1, orders o2\nUSING\n    SELECT * FROM orders o1, orders o2;\n\n-- 类型二的改写：去掉 `DELETE` 语句中的 `USING` 关键字\nCREATE GLOBAL BINDING for\n    DELETE users FROM users JOIN orders ON users.id = orders.user_id\nUSING\n    DELETE users FROM users JOIN orders ON users.id = orders.user_id;\n```\n\n> **注意：**\n>\n> 在对带 `SELECT` 子查询的 `INSERT`/`REPLACE` 语句创建执行计划绑定时，需要将想要绑定的优化器 Hints 指定在 `SELECT` 子查询中，而不是 `INSERT`/`REPLACE` 关键字后，不然优化器 Hints 不会生效。\n\n例如：\n\n```sql\n-- Hint 能生效的用法\nCREATE GLOBAL BINDING for\n    INSERT INTO orders SELECT * FROM pre_orders WHERE status = 'VALID' AND created <= (NOW() - INTERVAL 1 HOUR)\nUSING\n    INSERT INTO orders SELECT /*+ use_index(@sel_1 pre_orders, idx_created) */ * FROM pre_orders WHERE status = 'VALID' AND created <= (NOW() - INTERVAL 1 HOUR);\n\n-- Hint 不能生效的用法\nCREATE GLOBAL BINDING for\n    INSERT INTO orders SELECT * FROM pre_orders WHERE status = 'VALID' AND created <= (NOW() - INTERVAL 1 HOUR)\nUSING\n    INSERT /*+ use_index(@sel_1 pre_orders, idx_created) */ INTO orders SELECT * FROM pre_orders WHERE status = 'VALID' AND created <= (NOW() - INTERVAL 1 HOUR);\n```\n\n如果在创建执行计划绑定时不指定作用域，隐式作用域 SESSION 会被使用。TiDB 优化器会将被绑定的 SQL 进行“标准化”处理，然后存储到系统表中。在处理 SQL 查询时，只要“标准化”后的 SQL 和系统表中某个被绑定的 SQL 语句一致，并且系统变量 [`tidb_use_plan_baselines`](/system-variables.md#tidb_use_plan_baselines-从-v40-版本开始引入) 的值为 `on`（其默认值为 `on`），即可使用相应的优化器 Hint。如果存在多个可匹配的执行计划，优化器会从中选择代价最小的一个进行绑定。\n\n`标准化`：把 SQL 中的常量变成变量参数，对空格和换行符等做标准化处理，并对查询引用到的表显式指定数据库。例如：\n\n```sql\nSELECT * FROM users WHERE balance >    100\n-- 以上语句标准化后如下：\nSELECT * FROM bookshop . users WHERE balance > ?\n```\n\n> **注意：**\n>\n> 在进行标准化的时候，`IN` 表达式中的 `?` 会被标准化为 `...`。\n>\n> 例如：\n>\n> ```sql\n> SELECT * FROM books WHERE type IN ('Novel')\n> SELECT * FROM books WHERE type IN ('Novel','Life','Education')\n> -- 以上语句标准化后如下：\n> SELECT * FROM bookshop . books WHERE type IN ( ... )\n> SELECT * FROM bookshop . books WHERE type IN ( ... )\n> ```\n>\n> 不同长度的 `IN` 表达式被标准化后，会被识别为同一条语句，因此只需要创建一条绑定，对这些表达式同时生效。\n>\n> 例如：\n>\n> ```sql\n> CREATE TABLE t (a INT, KEY(a));\n> CREATE BINDING FOR SELECT * FROM t WHERE a IN (?) USING SELECT /*+ use_index(t, idx_a) */ * FROM t WHERE a in (?);\n> \n> SELECT * FROM t WHERE a IN (1);\n> SELECT @@LAST_PLAN_FROM_BINDING;\n> +--------------------------+\n> | @@LAST_PLAN_FROM_BINDING |\n> +--------------------------+\n> |                        1 |\n> +--------------------------+\n>\n> SELECT * FROM t WHERE a IN (1, 2, 3);\n> SELECT @@LAST_PLAN_FROM_BINDING;\n> +--------------------------+\n> | @@LAST_PLAN_FROM_BINDING |\n> +--------------------------+\n> |                        1 |\n> +--------------------------+\n> ```\n> \n> 在 v7.4.0 之前版本的 TiDB 集群中创建的绑定可能会包含 `IN (?)`，在升级到 v7.4.0 或更高版本后，这些绑定会被统一修改为 `IN (...)`。\n> \n> 例如：\n>\n> ```sql\n> -- 在 v7.3.0 集群上创建绑定\n> mysql> CREATE GLOBAL BINDING FOR SELECT * FROM t WHERE a IN (1) USING SELECT /*+ use_index(t, idx_a) */ * FROM t WHERE a IN (1);\n> mysql> SHOW GLOBAL BINDINGS;\n> +-----------------------------------------------+------------------------------------------------------------------------+------------+---------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n> | Original_sql                                  | Bind_sql                                                               | Default_db | Status  | Create_time             | Update_time             | Charset | Collation          | Source | Sql_digest                                                       | Plan_digest |\n> +-----------------------------------------------+------------------------------------------------------------------------+------------+---------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n> | select * from `test` . `t` where `a` in ( ? ) | SELECT /*+ use_index(`t` `idx_a`)*/ * FROM `test`.`t` WHERE `a` IN (1) | test       | enabled | 2024-09-03 15:39:02.695 | 2024-09-03 15:39:02.695 | utf8mb4 | utf8mb4_general_ci | manual | 8b9c4e6ab8fad5ba29b034311dcbfc8a8ce57dde2e2d5d5b65313b90ebcdebf7 |             |\n> +-----------------------------------------------+------------------------------------------------------------------------+------------+---------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n> \n> -- 升级到 v7.4.0 或更高的版本后\n> mysql> SHOW GLOBAL BINDINGS;\n> +-------------------------------------------------+------------------------------------------------------------------------+------------+---------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n> | Original_sql                                    | Bind_sql                                                               | Default_db | Status  | Create_time             | Update_time             | Charset | Collation          | Source | Sql_digest                                                       | Plan_digest |\n> +-------------------------------------------------+------------------------------------------------------------------------+------------+---------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n> | select * from `test` . `t` where `a` in ( ... ) | SELECT /*+ use_index(`t` `idx_a`)*/ * FROM `test`.`t` WHERE `a` IN (1) | test       | enabled | 2024-09-03 15:35:59.861 | 2024-09-03 15:35:59.861 | utf8mb4 | utf8mb4_general_ci | manual | da38bf216db4a53e1a1e01c79ffa42306419442ad7238480bb7ac510723c8bdf |             |\n> +-------------------------------------------------+------------------------------------------------------------------------+------------+---------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n> ```\n\n值得注意的是，如果一条 SQL 语句在 GLOBAL 和 SESSION 作用域内都有与之绑定的执行计划，因为优化器在遇到 SESSION 绑定时会忽略 GLOBAL 绑定的执行计划，该语句在 SESSION 作用域内绑定的执行计划会屏蔽掉语句在 GLOBAL 作用域内绑定的执行计划。\n\n例如：\n\n```sql\n-- 创建一个 global binding，指定其使用 sort merge join\nCREATE GLOBAL BINDING for\n    SELECT * FROM t1, t2 WHERE t1.id = t2.id\nUSING\n    SELECT /*+ merge_join(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n\n-- 从该 SQL 的执行计划中可以看到其使用了 global binding 中指定的 sort merge join\nexplain SELECT * FROM t1, t2 WHERE t1.id = t2.id;\n\n-- 创建另一个 session binding，指定其使用 hash join\nCREATE BINDING for\n    SELECT * FROM t1, t2 WHERE t1.id = t2.id\nUSING\n    SELECT /*+ hash_join(t1, t2) */ * FROM t1, t2 WHERE t1.id = t2.id;\n\n-- 从该 SQL 的执行计划中可以看到其使用了 session binding 中指定的 hash join，而不是 global binding 中指定的 sort merge join\nexplain SELECT * FROM t1, t2 WHERE t1.id = t2.id;\n```\n\n第一个 `SELECT` 语句在执行时优化器会通过 GLOBAL 作用域内的绑定为其加上 `sm_join(t1, t2)` hint，`explain` 出的执行计划中最上层的节点为 MergeJoin。而第二个 `SELECT` 语句在执行时优化器则会忽视 GLOBAL 作用域内的绑定而使用 SESSION 作用域内的绑定为该语句加上 `hash_join(t1, t2)` hint，`explain` 出的执行计划中最上层的节点为 HashJoin。\n\n每个标准化的 SQL 只能同时有一个通过 `CREATE BINDING` 创建的绑定。对相同的标准化 SQL 创建多个绑定时，会保留最后一个创建的绑定，之前的所有绑定（创建的和演进出来的）都会被删除。但 session 绑定和 global 绑定仍然允许共存，不受这个逻辑影响。\n\n另外，创建绑定时，TiDB 要求 session 处于某个数据库上下文中，也就是执行过 `use ${database}` 或者客户端连接时指定了数据库。\n\n需要注意的是原始 SQL 和绑定 SQL 在参数化以及去掉 Hint 后文本必须相同，否则创建会失败，例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE BINDING FOR SELECT * FROM t WHERE a > 1 USING SELECT * FROM t use index(idx) WHERE a > 2;\n```\n\n可以创建成功，因为原始 SQL 和绑定 SQL 在参数化以及去掉 Hint 后文本都是 `SELECT * FROM test . t WHERE a > ?`，而\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE BINDING FOR SELECT * FROM t WHERE a > 1 USING SELECT * FROM t use index(idx) WHERE b > 2;\n```\n\n则不可以创建成功，因为原始 SQL 在经过处理后是 `SELECT * FROM test . t WHERE a > ?`，而绑定 SQL 在经过处理后是 `SELECT * FROM test . t WHERE b > ?`。\n\n> **注意：**\n>\n> 对于 `PREPARE`/`EXECUTE` 语句组，或者用二进制协议执行的查询，创建执行计划绑定的对象应当是查询语句本身，而不是 `PREPARE`/`EXECUTE` 语句。\n\n#### 根据历史执行计划创建绑定\n\n如需将 SQL 语句的执行计划固定为之前使用过的执行计划，可以使用 Plan Digest 为该 SQL 语句绑定一个历史的执行计划。相比于使用 SQL 创建绑定的方式，此方式更加简便，并且支持一次为多个语句绑定执行计划。详细说明和更多示例参见 [`CREATE [GLOBAL|SESSION] BINDING`](/sql-statements/sql-statement-create-binding.md)。\n\n以下为根据历史执行计划创建绑定的注意事项：\n\n- 该功能是根据历史的执行计划生成 hint 而实现的绑定，历史的执行计划来源是 [Statement Summary Tables](/statement-summary-tables.md)，因此在使用此功能之前需开启系统变量 [`tidb_enable_stmt_summary`](/system-variables.md#tidb_enable_stmt_summary-从-v304-版本开始引入)。\n- 对于包含子查询的查询、访问 TiFlash 的查询、3 张表或更多表进行 Join 的查询，自动生成的 hint 不够完备，可能导致无法完全固定住计划，对于这类情况在创建时会产生告警。\n- 原执行计划对应 SQL 语句中的 hint 也会被应用在创建的绑定中，如执行 `SELECT /*+ max_execution_time(1000) */ * FROM t` 后，使用其 Plan Digest 创建的绑定中会带上 `max_execution_time(1000)`。\n\n使用方式：\n\n```sql\nCREATE [GLOBAL | SESSION] BINDING FROM HISTORY USING PLAN DIGEST StringLiteralOrUserVariableList;\n```\n\n该语句使用 Plan Digest 为 SQL 语句绑定执行计划，在不指定作用域时默认作用域为 SESSION。所创建绑定的适用 SQL、优先级、作用域、生效条件等与[根据 SQL 创建绑定](#根据-sql-创建绑定)相同。\n\n使用此绑定方式时，你需要先从 `statements_summary` 中找到需要绑定的执行计划对应的 Plan Digest，再通过 Plan Digest 创建绑定。具体步骤如下：\n\n1. 从 `Statement Summary Tables` 的记录中查找执行计划对应的 Plan Digest。\n\n    例如：\n\n    ```sql\n    CREATE TABLE t(id INT PRIMARY KEY , a INT, KEY idx_a(a));\n    SELECT /*+ IGNORE_INDEX(t, idx_a) */ * FROM t WHERE a = 1;\n    SELECT * FROM INFORMATION_SCHEMA.STATEMENTS_SUMMARY WHERE QUERY_SAMPLE_TEXT = 'SELECT /*+ IGNORE_INDEX(t, idx_a) */ * FROM t WHERE a = 1'\\G\n    ```\n\n    以下为 `statements_summary` 部分查询结果：\n\n    ```\n    SUMMARY_BEGIN_TIME: 2022-12-01 19:00:00\n    ...........\n          DIGEST_TEXT: select * from `t` where `a` = ?\n    ...........\n          PLAN_DIGEST: 4e3159169cc63c14b139a4e7d72eae1759875c9a9581f94bb2079aae961189cb\n                 PLAN:  id                  task        estRows operator info                           actRows execution info                                                                                                                                             memory      disk\n                        TableReader_7       root        10      data:Selection_6                        0       time:4.05ms, loops:1, cop_task: {num: 1, max: 598.6µs, proc_keys: 0, rpc_num: 2, rpc_time: 609.8µs, copr_cache_hit_ratio: 0.00, distsql_concurrency: 15}   176 Bytes   N/A\n                        └─Selection_6       cop[tikv]   10      eq(test.t.a, 1)                         0       tikv_task:{time:560.8µs, loops:0}                                                                                                                          N/A         N/A\n                          └─TableFullScan_5 cop[tikv]   10000   table:t, keep order:false, stats:pseudo 0       tikv_task:{time:560.8µs, loops:0}                                                                                                                          N/A         N/A\n          BINARY_PLAN: 6QOYCuQDCg1UYWJsZVJlYWRlcl83Ev8BCgtTZWxlY3Rpb25fNhKOAQoPBSJQRnVsbFNjYW5fNSEBAAAAOA0/QSkAAQHwW4jDQDgCQAJKCwoJCgR0ZXN0EgF0Uh5rZWVwIG9yZGVyOmZhbHNlLCBzdGF0czpwc2V1ZG9qInRpa3ZfdGFzazp7dGltZTo1NjAuOMK1cywgbG9vcHM6MH1w////CQMEAXgJCBD///8BIQFzCDhVQw19BAAkBX0QUg9lcSgBfCAudC5hLCAxKWrmYQAYHOi0gc6hBB1hJAFAAVIQZGF0YTo9GgRaFAW4HDQuMDVtcywgCbYcMWKEAWNvcF8F2agge251bTogMSwgbWF4OiA1OTguNsK1cywgcHJvY19rZXlzOiAwLCBycGNfBSkAMgkMBVcQIDYwOS4pEPBDY29wcl9jYWNoZV9oaXRfcmF0aW86IDAuMDAsIGRpc3RzcWxfY29uY3VycmVuY3k6IDE1fXCwAXj///////////8BGAE=\n    ```\n\n    可以看到执行计划对应的 Plan Digest 为 `4e3159169cc63c14b139a4e7d72eae1759875c9a9581f94bb2079aae961189cb`。\n\n2. 使用 Plan Digest 创建绑定。\n\n    ```sql\n    CREATE BINDING FROM HISTORY USING PLAN DIGEST '4e3159169cc63c14b139a4e7d72eae1759875c9a9581f94bb2079aae961189cb';\n    ```\n\n创建完毕后可以[查看绑定](#查看绑定)，验证绑定是否生效。\n\n```sql\nSHOW BINDINGS\\G\n```\n\n```\n*************************** 1. row ***************************\nOriginal_sql: select * from `test` . `t` where `a` = ?\n    Bind_sql: SELECT /*+ use_index(@`sel_1` `test`.`t` ) ignore_index(`t` `idx_a`)*/ * FROM `test`.`t` WHERE `a` = 1\n       ...........\n  Sql_digest: 6909a1bbce5f64ade0a532d7058dd77b6ad5d5068aee22a531304280de48349f\n Plan_digest:\n1 row in set (0.01 sec)\n\nERROR:\nNo query specified\n```\n\n```sql\nSELECT * FROM t WHERE a = 1;\nSELECT @@LAST_PLAN_FROM_BINDING;\n```\n\n```\n+--------------------------+\n| @@LAST_PLAN_FROM_BINDING |\n+--------------------------+\n|                        1 |\n+--------------------------+\n1 row in set (0.00 sec)\n```\n\n### 删除绑定\n\n你可以根据 SQL 语句或者 SQL Digest 删除绑定。\n\n#### 根据 SQL 语句删除绑定\n\n{{< copyable \"sql\" >}}\n\n```sql\nDROP [GLOBAL | SESSION] BINDING FOR BindableStmt;\n```\n\n该语句可以在 GLOBAL 或者 SESSION 作用域内删除指定的执行计划绑定，在不指定作用域时默认作用域为 SESSION。\n\n一般来说，SESSION 作用域的绑定主要用于测试或在某些特殊情况下使用。若需要集群中所有的 TiDB 进程都生效，则需要使用 GLOBAL 作用域的绑定。SESSION 作用域对 GLOBAL 作用域绑定的屏蔽效果会持续到该 SESSION 结束。\n\n承接上面关于 SESSION 绑定屏蔽 GLOBAL 绑定的例子，继续执行：\n\n```sql\n-- 删除 session 中创建的 binding\nDROP session binding for SELECT * FROM t1, t2 WHERE t1.id = t2.id;\n\n-- 重新查看该 SQL 的执行计划\nexplain SELECT * FROM t1,t2 WHERE t1.id = t2.id;\n```\n\n在这里 SESSION 作用域内被删除掉的绑定会屏蔽 GLOBAL 作用域内相应的绑定，优化器不会为 `SELECT` 语句添加 `sm_join(t1, t2)` hint，`explain` 给出的执行计划中最上层节点并不被 hint 固定为 MergeJoin，而是由优化器经过代价估算后自主进行选择。\n\n#### 根据 SQL Digest 删除绑定\n\n你既可以根据 SQL 语句删除对应的绑定，也可以根据 SQL Digest 删除绑定。详细说明和更多示例参见 [`DROP [GLOBAL|SESSION] BINDING`](/sql-statements/sql-statement-drop-binding.md)。\n\n```sql\nDROP [GLOBAL | SESSION] BINDING FOR SQL DIGEST StringLiteralOrUserVariableList;\n```\n\n该语句用于在 GLOBAL 或者 SESSION 作用域内删除 SQL Digest 对应的的执行计划绑定，在不指定作用域时默认作用域为 SESSION。你可以通过[查看绑定](#查看绑定)语句获取 SQL Digest。\n\n> **注意：**\n>\n> 执行 `DROP GLOBAL BINDING` 会删除当前 tidb-server 实例缓存中的绑定，并将系统表中对应行的状态修改为 'deleted'。该语句不会直接删除系统表中的记录，因为其他 tidb-server 实例需要读取系统表中的 'deleted' 状态来删除其缓存中对应的绑定。对于这些系统表中状态为 'deleted' 的记录，后台线程每隔 100 个 `bind-info-lease`（默认值为 `3s`，合计 `300s`）会触发一次对 `update_time` 在 10 个 `bind-info-lease` 以前的绑定（确保所有 tidb-server 实例已经读取过这个 'deleted' 状态并更新完缓存）的回收清除操作。\n\n### 变更绑定状态\n\n#### 根据 SQL 语句变更绑定状态\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET BINDING [ENABLED | DISABLED] FOR BindableStmt;\n```\n\n该语句可以在 GLOBAL 作用域内变更指定执行计划的绑定状态，默认作用域为 GLOBAL，该作用域不可更改。\n\n使用时，只能将 `Disabled` 的绑定改为 `Enabled` 状态，或将 `Enabled` 的绑定改为 `Disabled` 状态。如果没有可以改变状态的绑定，则会输出一条内容为 `There are no bindings can be set the status. Please check the SQL text` 的警告。需要注意的是，当绑定被设置成 `Disabled` 状态时，查询语句不会使用该绑定。\n\n#### 根据 `sql_digest` 变更绑定状态\n\n除了可以根据 SQL 语句变更对应的绑定状态以外，也可以根据 `sql_digest` 变更绑定状态：\n\n```sql\nSET BINDING [ENABLED | DISABLED] FOR SQL DIGEST 'sql_digest';\n```\n\n使用 `sql_digest` 所能变更的绑定状态和生效情况与[根据 SQL 语句变更绑定状态](#根据-sql-语句变更绑定状态)相同。如果没有可以改变状态的绑定，则会输出一条内容为 `can't find any binding for 'sql_digest'` 的警告。\n\n### 查看绑定\n\n{{< copyable \"sql\" >}}\n\n```sql\nSHOW [GLOBAL | SESSION] BINDINGS [ShowLikeOrWhere];\n```\n\n该语句会按照绑定更新时间由新到旧的顺序输出 GLOBAL 或者 SESSION 作用域内的执行计划绑定，在不指定作用域时默认作用域为 SESSION。目前 `SHOW BINDINGS` 会输出 11 列，具体如下：\n\n| 列名 | 说明            |\n| -------- | ------------- |\n| original_sql  |  参数化后的原始 SQL |\n| bind_sql | 带 Hint 的绑定 SQL |\n| default_db | 默认数据库名 |\n| status | 状态，包括 enabled（可用，从 v6.0 开始取代之前版本的 using 状态）、disabled（不可用）、deleted（已删除）、 invalid（无效）、rejected（演进时被拒绝）和 pending verify（等待演进验证） |\n| create_time | 创建时间 |\n| update_time | 更新时间 |\n| charset | 字符集 |\n| collation | 排序规则 |\n| source | 创建方式，包括 manual（根据 SQL 创建绑定生成）、history（根据历史执行计划创建绑定生成）、capture（由 TiDB 自动创建生成）和 evolve （由 TiDB 自动演进生成） |\n| sql_digest | 归一化后的 SQL 的 digest |\n| plan_digest | 执行计划的 digest |\n\n### 排查绑定\n\n绑定的排查通常有两种方式：\n\n- 使用系统变量 [`last_plan_from_binding`](/system-variables.md#last_plan_from_binding-从-v40-版本开始引入) 显示上一条执行语句是否采用 binding 的执行计划。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    -- 创建一个 global binding\n\n    CREATE GLOBAL BINDING for\n        SELECT * FROM t\n    USING\n        SELECT /*+ USE_INDEX(t, idx_a) */ * FROM t;\n\n    SELECT * FROM t;\n    SELECT @@[SESSION.]last_plan_from_binding;\n    ```\n\n    ```sql\n    +--------------------------+\n    | @@last_plan_from_binding |\n    +--------------------------+\n    |                        1 |\n    +--------------------------+\n    1 row in set (0.00 sec)\n    ```\n\n- 使用 `explain format = 'verbose'` 语句查看 SQL 语句的查询计划。如果 SQL 语句使用了 binding，可以接着执行 `show warnings` 了解该 SQL 语句使用了哪一条 binding。\n\n    ```sql\n    -- 创建一个 global binding\n\n    CREATE GLOBAL BINDING for\n        SELECT * FROM t\n    USING\n        SELECT /*+ USE_INDEX(t, idx_a) */ * FROM t;\n\n    -- 使用 explain format = 'verbose' 语句查看 SQL 的执行计划\n\n    explain format = 'verbose' SELECT * FROM t;\n\n    -- 通过执行 `show warnings` 了解该 SQL 语句使用了哪一条 binding\n\n    show warnings;\n    ```\n\n    ```sql\n    +-------+------+--------------------------------------------------------------------------+\n    | Level | Code | Message                                                                  |\n    +-------+------+--------------------------------------------------------------------------+\n    | Note  | 1105 | Using the bindSQL: SELECT /*+ USE_INDEX(`t` `idx_a`)*/ * FROM `test`.`t` |\n    +-------+------+--------------------------------------------------------------------------+\n    1 row in set (0.01 sec)\n\n    ```\n\n### 对绑定进行缓存\n\n每个 TiDB 实例都有一个 LRU (Least Recently Used) Cache 对绑定进行缓存，缓存的容量由系统变量 [`tidb_mem_quota_binding_cache`](/system-variables.md#tidb_mem_quota_binding_cache-从-v600-版本开始引入) 进行控制。缓存会影响绑定的使用和查看，因此你只能使用和查看存在于缓存中的绑定。\n\n如需查看绑定的使用情况，可以执行 `SHOW binding_cache status` 语句。该语句无法指定作用域，默认作用域为 GLOBAL。该语句可查看缓存中可用绑定的数量、系统中所有可用绑定的数量、缓存中所有绑定的内存使用量及缓存的内存容量。\n\n{{< copyable \"sql\" >}}\n\n```sql\n\nSHOW binding_cache status;\n```\n\n```sql\n+-------------------+-------------------+--------------+--------------+\n| bindings_in_cache | bindings_in_table | memory_usage | memory_quota |\n+-------------------+-------------------+--------------+--------------+\n|                 1 |                 1 | 159 Bytes    | 64 MB        |\n+-------------------+-------------------+--------------+--------------+\n1 row in set (0.00 sec)\n```\n\n## 利用 Statement Summary 表获取需要绑定的查询\n\n[Statement Summary](/statement-summary-tables.md) 的表中存放了近期的 SQL 相关的执行信息，如延迟、执行次数、对应计划等。你可以通过查询 Statement Summary 表得到符合条件查询的 `plan_digest`，然后[根据历史执行计划创建绑定](/sql-plan-management.md#根据历史执行计划创建绑定)。\n\n以下示例查找过去两周执行次数超过 10 次、执行计划不稳定且未被绑定的 `SELECT` 语句，并按照执行次数排序，将执行次数前 100 的查询绑定到对应的查询延迟最低的计划上。\n\n```sql\nWITH stmts AS (                                                -- Gets all information\n  SELECT * FROM INFORMATION_SCHEMA.CLUSTER_STATEMENTS_SUMMARY\n  UNION ALL\n  SELECT * FROM INFORMATION_SCHEMA.CLUSTER_STATEMENTS_SUMMARY_HISTORY \n),\nbest_plans AS (\n  SELECT plan_digest, `digest`, avg_latency, \n  CONCAT('create global binding from history using plan digest \"', plan_digest, '\"') as binding_stmt \n  FROM stmts t1\n  WHERE avg_latency = (SELECT min(avg_latency) FROM stmts t2   -- The plan with the lowest query latency\n                       WHERE t2.`digest` = t1.`digest`)\n)\n\nSELECT any_value(digest_text) as query, \n       SUM(exec_count) as exec_count, \n       plan_hint, binding_stmt\nFROM stmts, best_plans\nWHERE stmts.`digest` = best_plans.`digest`\n  AND summary_begin_time > DATE_SUB(NOW(), interval 14 day)    -- Executed in the past 2 weeks\n  AND stmt_type = 'Select'                                     -- Only consider select statements\n  AND schema_name NOT IN ('INFORMATION_SCHEMA', 'mysql')       -- Not an internal query\n  AND plan_in_binding = 0                                      -- No binding yet\nGROUP BY stmts.`digest`\n  HAVING COUNT(DISTINCT(stmts.plan_digest)) > 1                -- This query is unstable. It has more than 1 plan.\n         AND SUM(exec_count) > 10                              -- High-frequency, and has been executed more than 10 times.\nORDER BY SUM(exec_count) DESC LIMIT 100;                       -- Top 100 high-frequency queries.\n```\n\n通过一些过滤条件得到满足条件的查询，然后直接运行 `binding_stmt` 列对应的语句即可创建相应的绑定。\n\n```\n+---------------------------------------------+------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| query                                       | exec_count | plan_hint                                                                   | binding_stmt                                                                                                            |\n+---------------------------------------------+------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n| select * from `t` where `a` = ? and `b` = ? |        401 | use_index(@`sel_1` `test`.`t` `a`), no_order_index(@`sel_1` `test`.`t` `a`) | create global binding from history using plan digest \"0d6e97fb1191bbd08dddefa7bd007ec0c422b1416b152662768f43e64a9958a6\" |\n| select * from `t` where `b` = ? and `c` = ? |        104 | use_index(@`sel_1` `test`.`t` `b`), no_order_index(@`sel_1` `test`.`t` `b`) | create global binding from history using plan digest \"80c2aa0aa7e6d3205755823aa8c6165092c8521fb74c06a9204b8d35fc037dd9\" |\n+---------------------------------------------+------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+\n```\n\n## 跨数据库绑定执行计划 (Cross-DB Binding)\n\n在创建绑定的 SQL 语句中，TiDB 支持使用通配符 `*` 表示数据库，实现跨数据库绑定。该功能自 v7.6.0 开始引入。要使用跨数据库绑定，首先需要开启 [`tidb_opt_enable_fuzzy_binding`](/system-variables.md#tidb_opt_enable_fuzzy_binding-从-v760-版本开始引入) 系统变量。\n\n当数据按数据库 (schema/db) 分类存储，同时各数据库具有相同的对象定义并且运行相似的业务逻辑时，跨数据库执行计划绑定能显著简化执行计划的固定过程。以下是一些常见的使用场景：\n\n* 用户在 TiDB 上运行 SaaS 或 PaaS 类服务，每个租户的数据存储于独立的数据库中，以便数据维护和管理。 \n* 用户在单一实例中进行分库操作，并在迁移到 TiDB 后保留了原有的数据库结构，即将原实例中的数据按数据库分类存储。 \n\n在这些场景中，跨数据库绑定能有效缓解由于用户数据和负载的不均衡及其快速变化所引发的 SQL 性能问题。SaaS 服务商可以通过跨数据库绑定，固定大数据量用户业务已验证的执行计划，从而避免因小数据量用户业务快速增长引起的潜在性能问题。\n\n使用跨数据库绑定，只需要在创建绑定的 SQL 语句中将数据库名用 `*` 表示，例如：\n\n```sql\nCREATE GLOBAL BINDING USING SELECT /*+ use_index(t, idx_a) */ * FROM t; -- 创建 GLOBAL 作用域的普通绑定\nCREATE GLOBAL BINDING USING SELECT /*+ use_index(t, idx_a) */ * FROM *.t; -- 创建 GLOBAL 作用域的跨数据库绑定\nSHOW GLOBAL BINDINGS;\n```\n\n输出结果示例如下：\n\n```sql\n+----------------------------+---------------------------------------------------+------------+---------+-------------------------+-------------------------+--------------+-----------------+--------+------------------------------------------------------------------+-------------+\n| Original_sql               | Bind_sql                                              | Default_db | Status  | Create_time             | Update_time             | Charset | Collation       | Source | Sql_digest                                                       | Plan_digest |\n+----------------------------+---------------------------------------------------+------------+---------+-------------------------+-------------------------+--------------+-----------------+--------+------------------------------------------------------------------+-------------+\n| select * from `test` . `t` | SELECT /*+ use_index(`t` `idx_a`)*/ * FROM `test`.`t` | test       | enabled | 2023-12-29 14:19:01.332 | 2023-12-29 14:19:01.332 | utf8    | utf8_general_ci | manual | 8b193b00413fdb910d39073e0d494c96ebf24d1e30b131ecdd553883d0e29b42 |             |\n| select * from `*` . `t`    | SELECT /*+ use_index(`t` `idx_a`)*/ * FROM `*`.`t`    |            | enabled | 2023-12-29 14:19:02.232 | 2023-12-29 14:19:02.232 | utf8    | utf8_general_ci | manual | 8b193b00413fdb910d39073e0d494c96ebf24d1e30b131ecdd553883d0e29b42 |             |\n+----------------------------+---------------------------------------------------+------------+---------+-------------------------+-------------------------+--------------+-----------------+--------+------------------------------------------------------------------+-------------+\n```\n\n在 `SHOW GLOBAL BINDINGS` 的输出结果中，跨数据库绑定的 `Default_db` 为空，且 `Original_sql` 和 `Bind_sql` 字段中的数据库名通过 `*` 表示。这条绑定会对所有 `select * from t` 查询生效，而不限于特定数据库。\n\n对于相同的查询，跨数据绑定与普通绑定可以同时存在，TiDB 匹配的优先级从高到低依次为：SESSION 级别的普通绑定 > SESSION 级别的跨数据库绑定 > GLOBAL 级别的普通绑定 > GLOBAL 级别的跨数据库绑定。\n\n除了创建绑定的 SQL 语句不同，跨数据库绑定的删除和状态变更语句与普通绑定相同。下面是一个详细的使用示例。\n\n1. 创建数据库 `db1` 和 `db2`，并在每个数据库中创建两张表：\n  \n    ```sql\n    CREATE DATABASE db1;\n    CREATE TABLE db1.t1 (a INT, KEY(a));\n    CREATE TABLE db1.t2 (a INT, KEY(a));\n    CREATE DATABASE db2;\n    CREATE TABLE db2.t1 (a INT, KEY(a));\n    CREATE TABLE db2.t2 (a INT, KEY(a));\n    ```\n\n2. 开启跨数据库绑定功能：\n\n    ```sql\n    SET tidb_opt_enable_fuzzy_binding=1;\n    ```\n\n3. 创建跨数据库绑定：\n\n    ```sql\n    CREATE GLOBAL BINDING USING SELECT /*+ use_index(t1, idx_a), use_index(t2, idx_a) */ * FROM *.t1, *.t2;\n    ```\n\n4. 执行查询并查看是否使用了绑定：\n\n    ```sql\n    SELECT * FROM db1.t1, db1.t2;\n    SELECT @@LAST_PLAN_FROM_BINDING;\n    +--------------------------+\n    | @@LAST_PLAN_FROM_BINDING |\n    +--------------------------+\n    |                        1 |\n    +--------------------------+\n    \n    SELECT * FROM db2.t1, db2.t2;\n    SELECT @@LAST_PLAN_FROM_BINDING;\n    +--------------------------+\n    | @@LAST_PLAN_FROM_BINDING |\n    +--------------------------+\n    |                        1 |\n    +--------------------------+\n    \n    SELECT * FROM db1.t1, db2.t2;\n    SELECT @@LAST_PLAN_FROM_BINDING;\n    +--------------------------+\n    | @@LAST_PLAN_FROM_BINDING |\n    +--------------------------+\n    |                        1 |\n    +--------------------------+\n\n    USE db1;\n    SELECT * FROM t1, db2.t2;\n    SELECT @@LAST_PLAN_FROM_BINDING;\n    +--------------------------+\n    | @@LAST_PLAN_FROM_BINDING |\n    +--------------------------+\n    |                        1 |\n    +--------------------------+\n    ```\n\n5. 查看绑定：\n\n    ```sql\n    SHOW GLOBAL BINDINGS;\n    +----------------------------------------------+------------------------------------------------------------------------------------------+------------+-----------------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n    | Original_sql                                 | Bind_sql                                                                                         | Default_db | Status  | Create_time             | Update_time             | Charset | Collation          | Source | Sql_digest                                                       | Plan_digest |\n    +----------------------------------------------+------------------------------------------------------------------------------------------+------------+-----------------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n    | select * from ( `*` . `t1` ) join `*` . `t2` | SELECT /*+ use_index(`t1` `idx_a`) use_index(`t2` `idx_a`)*/ * FROM (`*` . `t1`) JOIN `*` . `t2` |            | enabled | 2023-12-29 14:22:28.144 | 2023-12-29 14:22:28.144 | utf8    | utf8_general_ci    | manual | ea8720583e80644b58877663eafb3579700e5f918a748be222c5b741a696daf4 |             |\n    +----------------------------------------------+------------------------------------------------------------------------------------------+------------+-----------------+-------------------------+-------------------------+---------+--------------------+--------+------------------------------------------------------------------+-------------+\n    ```\n\n6. 删除绑定：\n\n    ```sql\n    DROP GLOBAL BINDING FOR SQL DIGEST 'ea8720583e80644b58877663eafb3579700e5f918a748be222c5b741a696daf4';\n    SHOW GLOBAL BINDINGS;\n    Empty set (0.00 sec)\n    ```\n\n## 自动捕获绑定 (Baseline Capturing)\n\n自动绑定会对符合捕获条件的查询进行捕获，为符合条件的查询生成相应的绑定。通常用于[升级时的计划回退防护](#升级时的计划回退防护)。\n\nPlan Baseline 是一组被允许用于 SQL 语句优化器的可接受计划。在典型的应用场景中，TiDB 仅在验证计划性能良好后才将其添加到 Baseline 中。这些计划包含优化器重新生成执行计划所需的所有信息（例如，SQL 计划标识符、提示集、绑定值、优化器环境）。 \n\n### 使用方式\n\n通过将 `tidb_capture_plan_baselines` 的值设置为 `on`（其默认值为 `off`）可以打开自动捕获绑定功能。\n\n> **注意：**\n>\n> 自动绑定功能依赖于 [Statement Summary](/statement-summary-tables.md)，因此在使用自动绑定之前需打开 Statement Summary 开关。\n\n开启自动绑定功能后，每隔 `bind-info-lease`（默认值为 `3s`）会遍历一次 Statement Summary 中的历史 SQL 语句，并为至少出现两次的 SQL 语句自动捕获绑定。绑定的执行计划为 Statement Summary 中记录执行这条语句时使用的执行计划。\n\n对于以下几种 SQL 语句，TiDB 不会自动捕获绑定：\n\n- EXPLAIN 和 EXPLAIN ANALYZE 语句；\n- TiDB 内部执行的 SQL 语句，比如统计信息自动加载使用的 SELECT 查询；\n- 存在 `Enabled` 或 `Disabled` 状态绑定的语句；\n- 满足捕获绑定黑名单过滤条件的语句。\n\n> **注意：**\n>\n> 当前，绑定通过生成一组 Hints 来固定查询语句生成的执行计划，从而确保执行计划不发生变化。对于大多数 OLTP 查询，TiDB 能够保证计划前后一致，如使用相同的索引、相同的 Join 方式（如 HashJoin、IndexJoin）等。但是，受限于当前 Hints 的完善程度，对于一些较为复杂的查询，如两个表以上的 Join 和复杂的 OLAP、MPP 类查询，TiDB 无法保证计划在绑定前后完全一致。\n\n对于 `PREPARE`/`EXECUTE` 语句组，或通过二进制协议执行的查询，TiDB 会为真正的查询（而不是 `PREPARE`/`EXECUTE` 语句）自动捕获绑定。\n\n> **注意：**\n>\n> 由于 TiDB 存在一些内嵌 SQL 保证一些功能的正确性，所以自动捕获绑定时会默认屏蔽内嵌 SQL。\n\n### 过滤捕获绑定\n\n使用本功能，你可以设置黑名单，将满足黑名单规则的查询排除在捕获范围之外。黑名单支持的过滤维度包括表名、频率和用户名。\n\n#### 使用方式\n\n将过滤规则插入到系统表 `mysql.capture_plan_baselines_blacklist` 中，该过滤规则即刻起会在整个集群范围内生效。\n\n{{< copyable \"sql\" >}}\n\n```sql\n-- 按照表名进行过滤\nINSERT INTO mysql.capture_plan_baselines_blacklist(filter_type, filter_value) VALUES('table', 'test.t');\n\n-- 通过通配符来实现按照数据库名和表名进行过滤\nINSERT INTO mysql.capture_plan_baselines_blacklist(filter_type, filter_value) VALUES('table', 'test.table_*');\nINSERT INTO mysql.capture_plan_baselines_blacklist(filter_type, filter_value) VALUES('table', 'db_*.table_*');\n\n-- 按照执行频率进行过滤\nINSERT INTO mysql.capture_plan_baselines_blacklist(filter_type, filter_value) VALUES('frequency', '2');\n\n-- 按照用户名进行过滤\nINSERT INTO mysql.capture_plan_baselines_blacklist(filter_type, filter_value) VALUES('user', 'user1');\n```\n\n| **维度名称** | **说明**                                                     | 注意事项                                                     |\n| :----------- | :----------------------------------------------------------- | ------------------------------------------------------------ |\n| table        | 按照表名进行过滤，每个过滤规则均采用 `db.table` 形式，支持通配符。详细规则可以参考[直接使用表名](/table-filter.md#直接使用表名)和[使用通配符](/table-filter.md#使用通配符)。 | 字母大小写不敏感，如果包含非法内容，日志会输出 `[sql-bind] failed to load mysql.capture_plan_baselines_blacklist` 警告。 |\n| frequency    | 按照频率进行过滤，默认捕获执行超过一次的语句。可以设置较大值来捕获执行频繁的语句。 | 插入的值小于 1 会被认为是非法值，同时，日志会输出 `[sql-bind] frequency threshold is less than 1, ignore it` 警告。如果插入了多条频率过滤规则，频率最大的值会被用作过滤条件。 |\n| user         | 按照用户名进行过滤，黑名单用户名执行的语句不会被捕获。                           | 如果多个用户执行同一条语句，只有当他们的用户名都在黑名单的时候，该语句才不会被捕获。 |\n\n> **注意：**\n>\n> - 修改黑名单需要数据库的 super privilege 权限。\n>\n> - 如果黑名单包含了非法的过滤内容时，TiDB 会在日志中输出 `[sql-bind] unknown capture filter type, ignore it` 进行提示。\n\n### 升级时的计划回退防护\n\n当需要升级 TiDB 集群时，你可以利用自动捕获绑定对潜在的计划回退风险进行一定程度的防护，具体流程为：\n\n1. 升级前打开自动捕获。\n\n    > **注意：**\n    >\n    > 经测试，长期打开自动捕获对集群负载的性能影响很小。尽量长期打开自动捕获，以确保重要的查询（出现过两次及以上）都能被捕获到。\n\n2. 进行 TiDB 集群的升级。在升级完成后，这些通过捕获的绑定会发挥作用，确保在升级后，查询的计划不会改变。\n3. 升级完成后，根据情况手动删除绑定。\n\n    - 通过[`SHOW GLOBAL BINDINGS`](#查看绑定)语句检查绑定来源：\n\n        根据输出中的 `Source` 字段对绑定的来源进行区分，确认是通过捕获 (`capture`) 生成还是通过手动创建 (`manual`) 生成。\n\n    - 确定 `capture` 的绑定是否需要保留：\n\n        ```\n        -- 查看绑定生效时的计划\n        SET @@SESSION.TIDB_USE_PLAN_BASELINES = true;\n        EXPLAIN FORMAT='VERBOSE' SELECT * FROM t1 WHERE ...;\n\n        -- 查看绑定不生效时的计划\n        SET @@SESSION.TIDB_USE_PLAN_BASELINES = false;\n        EXPLAIN FORMAT='VERBOSE' SELECT * FROM t1 WHERE ...;\n        ```\n\n        - 如果屏蔽绑定前后，查询得到的计划一致，则可以安全删除此绑定。\n\n        - 如果计划不一样，则可能需要对此计划变化的原因进行排查，如检查统计信息等操作。在这种情况下需要保留此绑定，确保计划不发生变化。\n\n## 自动演进绑定 (Baseline Evolution)\n\n自动演进绑定，在 TiDB 4.0 版本引入，是执行计划管理的重要功能之一。\n\n由于某些数据变更后，原先绑定的执行计划可能是一个不优的计划。为了解决该问题，引入自动演进绑定功能来自动优化已经绑定的执行计划。\n\n另外自动演进绑定还可以一定程度上避免统计信息改动后，对执行计划带来的抖动。\n\n### 使用方式\n\n通过以下语句可以开启自动演进绑定功能：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET GLOBAL tidb_evolve_plan_baselines = ON;\n```\n\n`tidb_evolve_plan_baselines` 的默认值为 `off`。\n\n> **警告：**\n>\n> - 自动演进功能目前为实验特性，存在未知风险，不建议在生产环境中使用。\n>\n> - 此变量开关已强制关闭，直到自动演进成为正式功能 GA (Generally Available)。如果你尝试打开开关，会产生报错。如果你已经在生产环境中使用了此功能，请尽快将它禁用。如发现 binding 状态不如预期，请从 PingCAP 官方或 TiDB 社区[获取支持](/support.md)。\n\n在打开自动演进功能后，如果优化器选出的最优执行计划不在之前绑定的执行计划之中，会将其记录为待验证的执行计划。每隔 `bind-info-lease`（默认值为 `3s`），会选出一个待验证的执行计划，将其和已经绑定的执行计划中代价最小的比较实际运行时间。如果待验证的运行时间更优的话（目前判断标准是运行时间小于等于已绑定执行计划运行时间的 2/3），会将其标记为可使用的绑定。以下示例描述上述过程。\n\n假如有表 `t` 定义如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t(a INT, b INT, KEY(a), KEY(b));\n```\n\n在表 `t` 上进行如下查询：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM t WHERE a < 100 AND b < 100;\n```\n\n表上满足条件 `a < 100` 的行很少。但由于某些原因，优化器没能选中使用索引 `a` 这个最优执行计划，而是误选了速度慢的全表扫，那么用户首先可以通过如下语句创建一个绑定：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE GLOBAL BINDING for SELECT * FROM t WHERE a < 100 AND b < 100 USING SELECT * FROM t use index(a) WHERE a < 100 AND b < 100;\n```\n\n当以上查询语句再次执行时，优化器会在刚创建绑定的干预下选择使用索引 `a`，进而降低查询时间。\n\n假如随着在表中进行插入和修改，表中满足条件 `a < 100` 的行变得越来越多，而满足条件 `b < 100` 的行变得越来越少，这时再在绑定的干预下使用索引 `a` 可能就不是最优了。\n\n绑定的演进可以解决这类问题。当优化器感知到表数据变化后，会对这条查询生成使用索引 `b` 的执行计划。但由于绑定的存在，这个执行计划不会被采纳和执行，不过它会被存在后台的演进列表里。在演进过程中，如果它被验证为执行时间明显低于使用索引 `a` 的执行时间（即当前绑定的执行计划），那么索引 `b` 会被加入到可用的绑定列表中。在此之后，当这条查询再次被执行时，优化器首先生成使用索引 `b` 的执行计划，并确认它在绑定列表中，所以会采纳它并执行，进而可以在数据变化后降低这条查询的执行时间。\n\n为了减少自动演进对集群的影响，可以通过设置 `tidb_evolve_plan_task_max_time` 来限制每个执行计划运行的最长时间，其默认值为 `600s`。实际在验证执行计划时，计划的最长运行时间还会被限制为不超过已验证执行计划的运行时间的两倍；通过 `tidb_evolve_plan_task_start_time` 和 `tidb_evolve_plan_task_end_time` 可以限制运行演进任务的时间窗口，默认值分别为 `00:00 +0000` 和 `23:59 +0000`。\n\n### 注意事项\n\n由于自动演进绑定会自动地创建新的绑定，当查询的环境发生变动时，自动创建的绑定可能会有多种行为的选择。这里列出一些注意事项：\n\n+ 自动演进只会对存在至少一个 global 绑定的标准化 SQL 进行演进。\n\n+ 由于创建新的绑定会删除之前所有绑定（对于一条标准化 SQL），自动演进的绑定也会在手动重新创建绑定后被删除。\n\n+ 所有和计算过程相关的 hint，在演进时都会被保留。计算过程相关的 hint 有如下几种：\n\n    | Hint | 说明            |\n    | :-------- | :------------- |\n    | memory_quota |  查询过程最多可以使用多少内存 |\n    | use_toja | 优化器是否考虑把子查询转化为 join |\n    | use_cascades | 是否使用 cascades 优化器 |\n    | no_index_merge | 优化器是否考虑将 index merge 作为一个读表选项 |\n    | read_consistent_replica | 是否强制读表时使用 follower read |\n    | max_execution_time | 查询过程最多消耗多少时间 |\n\n+ `read_from_storage` 是一个非常特别的 hint，因为它指定了读表时选择从 TiKV 读还是从 TiFlash 读。由于 TiDB 提供隔离读的功能，当隔离条件变化时，这个 hint 对演进出来的执行计划影响很大，所以当最初创建的绑定中存在这个 hint，TiDB 会无视其所有演进的绑定。\n\n## 升级检查 (Upgrade Checklist)\n\n执行计划管理功能 (SPM) 在版本升级过程中可能会出现一些兼容性问题导致升级失败，你需要在版本升级前做一些检查，确保版本顺利升级。\n\n* 当你尝试从 v5.2 以前的版本（即 v4.0、v5.0、v5.1）升级到当前版本，需要注意在升级前检查自动演进的开关 `tidb_evolve_plan_baselines` 是否已经关闭。如果尚未关闭，则需要将其关闭后再进行升级。具体操作如下所示：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    -- 在待升级的版本上检查自动演进的开关 `tidb_evolve_plan_baselines` 是否关闭。\n\n    SELECT @@global.tidb_evolve_plan_baselines;\n\n    -- 如果演进的开关 `tidb_evolve_plan_baselines` 尚未关闭，则需要将其关闭。\n\n    set global tidb_evolve_plan_baselines = off;\n    ```\n\n* 当你尝试从 v4.0 版本升级到当前版本，需要注意在升级前检查所有可用绑定对应的查询语句在新版本中是否存在语法错误。如果存在语法错误，则需要删除对应的绑定。\n\n    具体操作如下所示：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    -- 在待升级的版本上检查现有可用绑定对应的查询语句。\n\n    SELECT bind_sql FROM mysql.bind_info WHERE status = 'using';\n\n    -- 将上一条查询得到的结果，在新版本的测试环境中进行验证。\n\n    bind_sql_0;\n    bind_sql_1;\n    ...\n\n    -- 如果报错信息是语法错误（ERROR 1064 (42000): You have an error in your SQL syntax），则需要删除对应的绑定。\n    -- 如果是其他错误，如未找到表，则表示语法兼容，不需要进行额外的处理。\n    ```\n"
        },
        {
          "name": "sql-plan-replayer.md",
          "type": "blob",
          "size": 14.296875,
          "content": "---\ntitle: 使用 PLAN REPLAYER 保存和恢复集群现场信息\nsummary: 了解如何使用 PLAN REPLAY 命令保存和恢复集群现场信息。\n---\n\n# 使用 PLAN REPLAYER 保存和恢复集群现场信息\n\n用户在定位排查 TiDB 集群问题时，经常需要提供系统和查询计划相关的信息。为了帮助用户更方便地获取相关信息，更高效地排查集群问题，TiDB 在 v5.3.0 中引入了 `PLAN REPLAYER` 命令，用于“一键”保存和恢复现场问题的相关信息，提升查询计划问题诊断的效率，同时方便将问题归档管理。\n\n`PLAN REPLAYER` 主要功能如下：\n\n- 导出排查现场 TiDB 集群的相关信息，导出为 ZIP 格式的文件用于保存。\n- 在任意 TiDB 集群上导入另一 TiDB 集群现场信息的 ZIP 文件。\n\n## 使用 `PLAN REPLAYER` 导出集群信息\n\n你可以使用 `PLAN REPLAYER` 来保存 TiDB 集群的现场信息。导出接口如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nPLAN REPLAYER DUMP EXPLAIN [ANALYZE] [WITH STATS AS OF TIMESTAMP expression] sql-statement;\n```\n\nTiDB 根据 `sql-statement` 整理出以下集群现场信息：\n\n- TiDB 版本信息\n- TiDB 配置信息\n- TiDB Session 系统变量\n- TiDB 执行计划绑定信息（SQL Binding）\n- `sql-statement` 中所包含的表结构\n- `sql-statement` 中所包含表的统计信息\n- `EXPLAIN [ANALYZE] sql-statement` 的结果\n- 优化器进行查询优化的一些内部步骤的记录\n\n当[启用历史统计信息](/system-variables.md#tidb_enable_historical_stats)时，可以在 `PLAN REPLAYER` 语句中指定时间来获取对应时间的统计信息。该语法支持直接指定日期时间或指定时间戳。此时，TiDB 会查找指定时间之前的历史统计信息，并导出其中最新的一份。\n\n如果没有找到指定时间之前的历史统计信息，TiDB 会直接导出最新统计信息（和未指定时间时的行为一致），并且在导出的 `ZIP` 文件中的 `errors.txt` 中输出错误信息。\n\n> **注意：**\n>\n> `PLAN REPLAYER` **不会**导出表中数据\n\n### `PLAN REPLAYER` 导出示例\n\n{{< copyable \"sql\" >}}\n\n```sql\nuse test;\ncreate table t(a int, b int);\ninsert into t values(1,1), (2, 2), (3, 3);\nanalyze table t;\n\nplan replayer dump explain select * from t;\nplan replayer dump with stats as of timestamp '2023-07-17 12:00:00' explain select * from t;\nplan replayer dump with stats as of timestamp '442012134592479233' explain select * from t;\n```\n\n`PLAN REPLAYER DUMP` 会将以上信息打包整理成 `ZIP` 文件，并返回文件标识作为执行结果。\n\n> **注意：**\n>\n> `ZIP` 文件最多会在 TiDB 集群中保存一个小时，超时后 TiDB 会将其删除。\n\n```sql\nMySQL [test]> plan replayer dump explain select * from t;\n```\n\n```sql\n+------------------------------------------------------------------+\n| Dump_link                                                        |\n+------------------------------------------------------------------+\n| replayer_JOGvpu4t7dssySqJfTtS4A==_1635750890568691080.zip |\n+------------------------------------------------------------------+\n1 row in set (0.015 sec)\n```\n\n你同样可以通过 [`tidb_last_plan_replayer_token`](/system-variables.md#tidb_last_plan_replayer_token-从-v630-版本开始引入) 这个会话变量来获取上一次 `PLAN REPLAYER dump` 执行的结果。\n\n```sql\nSELECT @@tidb_last_plan_replayer_token;\n```\n\n```sql\n| @@tidb_last_plan_replayer_token                           |\n+-----------------------------------------------------------+\n| replayer_Fdamsm3C7ZiPJ-LQqgVjkA==_1663304195885090000.zip |\n+-----------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n对于多条 SQL 的情况，你可以通过文件的方式来获取 plan replayer dump 的结果，多条 SQL 语句在文件中以 `;` 进行分隔。\n\n```sql\nplan replayer dump explain 'sqls.txt';\n```\n\n```sql\nSELECT @@tidb_last_plan_replayer_token;\n```\n\n```sql\n+-----------------------------------------------------------+\n| @@tidb_last_plan_replayer_token                           |\n+-----------------------------------------------------------+\n| replayer_LEDKg8sb-K0u24QesiH8ig==_1663226556509182000.zip |\n+-----------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n因为 MySQL Client 无法下载文件，所以需要通过 TiDB HTTP 接口和文件标识下载文件：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\nhttp://${tidb-server-ip}:${tidb-server-status-port}/plan_replayer/dump/${file_token}\n```\n\n其中，`${tidb-server-ip}:${tidb-server-status-port}` 是集群中任意 TiDB server 的地址。示例如下：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ncurl http://127.0.0.1:10080/plan_replayer/dump/replayer_JOGvpu4t7dssySqJfTtS4A==_1635750890568691080.zip > plan_replayer.zip\n```\n\n## 使用 `PLAN REPLAYER` 导入集群信息\n\n> **警告：**\n>\n> `PLAN REPLAYER` 在一个 TiDB 集群上导入另一集群的现场信息，会修改导入集群的 TiDB Session 系统变量、执行计划绑定信息、表结构和统计信息。\n\n有 `PLAN REPLAYER` 导出的 `ZIP` 文件后，用户便可以通过 `PLAN REPLAYER` 导入接口在任意 TiDB 集群上恢复另一集群地现场信息。语法如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nPLAN REPLAYER LOAD 'file_name';\n```\n\n以上语句中，`file_name` 为要导入的 `ZIP` 文件名。\n\n示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nPLAN REPLAYER LOAD 'plan_replayer.zip';\n```\n\n> **注意：**\n>\n> 你需要禁止 `auto analyze`，否则导入的统计信息会被 `analyze` 覆盖。\n\n你可以通过将 [`tidb_enable_auto_analyze`](/system-variables.md#tidb_enable_auto_analyze-从-v610-版本开始引入) 系统变量设置为 `OFF` 来禁用 `auto analyze`。\n\n```sql\nset @@global.tidb_enable_auto_analyze = OFF;\n```\n\n导入完毕后，该 TiDB 集群就载入了所需要的表结构、统计信息等其他影响构造 Plan 所需要的信息。你可以通过以下方式查看执行计划以及验证统计信息:\n\n```sql\nmysql> desc t;\n+-------+------+------+------+---------+-------+\n| Field | Type | Null | Key  | Default | Extra |\n+-------+------+------+------+---------+-------+\n| a     | int  | YES  |      | NULL    |       |\n| b     | int  | YES  |      | NULL    |       |\n+-------+------+------+------+---------+-------+\n2 rows in set (0.01 sec)\n\nmysql> explain select * from t where a = 1 or b =1;\n+-------------------------+---------+-----------+---------------+--------------------------------------+\n| id                      | estRows | task      | access object | operator info                        |\n+-------------------------+---------+-----------+---------------+--------------------------------------+\n| TableReader_7           | 0.01    | root      |               | data:Selection_6                     |\n| └─Selection_6           | 0.01    | cop[tikv] |               | or(eq(test.t.a, 1), eq(test.t.b, 1)) |\n|   └─TableFullScan_5     | 6.00    | cop[tikv] | table:t       | keep order:false, stats:pseudo       |\n+-------------------------+---------+-----------+---------------+--------------------------------------+\n3 rows in set (0.00 sec)\n\nmysql> show stats_meta;\n+---------+------------+----------------+---------------------+--------------+-----------+\n| Db_name | Table_name | Partition_name | Update_time         | Modify_count | Row_count |\n+---------+------------+----------------+---------------------+--------------+-----------+\n| test    | t          |                | 2022-08-26 15:52:07 |            3 |         6 |\n+---------+------------+----------------+---------------------+--------------+-----------+\n1 row in set (0.04 sec)\n```\n\n加载并还原所需现场后，即可在该现场诊断和改进执行计划。\n\n## 使用 `PLAN REPLAYER CAPTURE` 抓取目标计划\n\n在用户定位 TiDB 执行计划的部分场景中，目标 SQL 语句与目标计划可能仅在查询中偶尔出现，无法使用 `PLAN REPLAYER` 直接抓取。此时你可以使用 `PLAN REPLAYER CAPTURE` 来帮助定向抓取目标 SQL 语句与目标计划的优化器信息。\n\n`PLAN REPLAYER CAPTURE` 主要功能如下：\n\n- 在 TiDB 集群内部提前注册目标 SQL 语句与执行计划的 Digest，并开始匹配目标查询。\n- 当目标查询匹配成功时，直接抓取其优化器相关信息，导出为 ZIP 格式的文件用于保存。\n- 针对匹配到的每组 SQL 和执行计划，信息只抓取一次。\n- 通过系统表显示正在进行的匹配任务，以及生成的文件。\n- 定时清理历史文件。\n\n### 开启 `PLAN REPLAYER CAPTURE`\n\n`PLAN REPLAYER CAPTURE` 功能通过系统变量 [`tidb_enable_plan_replayer_capture`](/system-variables.md#tidb_enable_plan_replayer_capture) 控制。要开启 `PLAN REPLAYER CAPTURE`，将变量值设为 `ON`。\n\n### 使用 `PLAN REPLAYER CAPTURE` 功能\n\n你可以通过以下方式向 TiDB 集群注册目标 SQL 语句和计划的 Digest:\n\n```sql\nPLAN REPLAYER CAPTURE 'sql_digest' 'plan_digest';\n```\n\n当你的目标 SQL 语句对应多种执行计划，且你想抓取所有执行计划时，你可以通过以下 SQL 语句一键注册:\n\n```sql\nPLAN REPLAYER CAPTURE 'sql_digest' '*';\n```\n\n### 查看 `PLAN REPLAYER CAPTURE` 抓取任务\n\n你可以通过以下方式查看集群中目前正在工作的 `PLAN REPLAYER CAPTURE` 的抓取任务:\n\n```sql\nmysql> PLAN REPLAYER CAPTURE 'example_sql' 'example_plan';\nQuery OK, 1 row affected (0.01 sec)\n\nmysql> SELECT * FROM mysql.plan_replayer_task;\n+-------------+--------------+---------------------+\n| sql_digest  | plan_digest  | update_time         |\n+-------------+--------------+---------------------+\n| example_sql | example_plan | 2023-01-28 11:58:22 |\n+-------------+--------------+---------------------+\n1 row in set (0.01 sec)\n```\n\n### 查看 `PLAN REPLAYER CAPTURE` 抓取结果\n\n当 `PLAN REPLAYER CAPTURE` 成功抓取到结果后，可以通过以下 SQL 语句查看用于下载的文件标识：\n\n```sql\nmysql> SELECT * FROM mysql.plan_replayer_status;\n+------------------------------------------------------------------+------------------------------------------------------------------+------------+-----------------------------------------------------------+---------------------+-------------+-----------------+\n| sql_digest                                                       | plan_digest                                                      | origin_sql | token                                                     | update_time         | fail_reason | instance        |\n+------------------------------------------------------------------+------------------------------------------------------------------+------------+-----------------------------------------------------------+---------------------+-------------+-----------------+\n| 086e3fbd2732f7671c17f299d4320689deeeb87ba031240e1e598a0ca14f808c | 042de2a6652a6d20afc629ff90b8507b7587a1c7e1eb122c3e0b808b1d80cc02 |            | replayer_Utah4nkz2sIEzkks7tIRog==_1668746293523179156.zip | 2022-11-18 12:38:13 | NULL        | 172.16.4.4:4022 |\n| b5b38322b7be560edb04f33f15b15a885e7c6209a22b56b0804622e397199b54 | 1770efeb3f91936e095f0344b629562bf1b204f6e46439b7d8f842319297c3b5 |            | replayer_Z2mUXNHDjU_WBmGdWQqifw==_1668746293560115314.zip | 2022-11-18 12:38:13 | NULL        | 172.16.4.4:4022 |\n| 96d00c0b3f08795fe94e2d712fa1078ab7809faf4e81d198f276c0dede818cf9 | 8892f74ac2a42c2c6b6152352bc491b5c07c73ac3ed66487b2c990909bae83e8 |            | replayer_RZcRHJB7BaCccxFfOIAhWg==_1668746293578282450.zip | 2022-11-18 12:38:13 | NULL        | 172.16.4.4:4022 |\n+------------------------------------------------------------------+------------------------------------------------------------------+------------+-----------------------------------------------------------+---------------------+-------------+-----------------+\n3 rows in set (0.00 sec)\n```\n\n下载 `PLAN REPLAYER CAPTURE` 的文件方法与 `PLAN REPLAYER` 相同，请参考 [`PLAN REPLAYER` 导出示例](#plan-replayer-导出示例)。\n\n> **注意：**\n>\n> `PLAN REPLAYER CAPTURE` 的结果文件最多会在 TiDB 集群中保存一周，超时后 TiDB 会将其删除。\n\n### 移除 `PLAN REPLAYER CAPTURE` 抓取任务\n\n不再需要某个 `PLAN REPLAYER CAPTURE` 抓取任务后，你可以通过 `PLAN REPLAYER CAPTURE REMOVE` 语句将其移除。示例如下：\n\n```sql\nmysql> PLAN REPLAYER CAPTURE '077a87a576e42360c95530ccdac7a1771c4efba17619e26be50a4cfd967204a0' '4838af52c1e07fc8694761ad193d16a689b2128bc5ced9d13beb31ae27b370ce';\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> SELECT * FROM mysql.plan_replayer_task;\n+------------------------------------------------------------------+------------------------------------------------------------------+---------------------+\n| sql_digest                                                       | plan_digest                                                      | update_time         |\n+------------------------------------------------------------------+------------------------------------------------------------------+---------------------+\n| 077a87a576e42360c95530ccdac7a1771c4efba17619e26be50a4cfd967204a0 | 4838af52c1e07fc8694761ad193d16a689b2128bc5ced9d13beb31ae27b370ce | 2024-05-21 11:26:10 |\n+------------------------------------------------------------------+------------------------------------------------------------------+---------------------+\n1 row in set (0.01 sec)\n\nmysql> PLAN REPLAYER CAPTURE REMOVE '077a87a576e42360c95530ccdac7a1771c4efba17619e26be50a4cfd967204a0' '4838af52c1e07fc8694761ad193d16a689b2128bc5ced9d13beb31ae27b370ce';\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> SELECT * FROM mysql.plan_replayer_task;\nEmpty set (0.01 sec)\n```\n\n## 使用 `PLAN REPLAYER CONTINUOUS CAPTURE`\n\n开启 `PLAN REPLAYER CONTINUOUS CAPTURE` 功能后，TiDB 将以 SQL DIGEST 和 PLAN DIGEST 为维度异步地将业务 SQL 语句以 `PLAN REPLAYER` 的方式进行记录，对于相同 DIGEST 的 SQL 语句与执行计划，`PLAN REPLAYER CONTINUOUS CAPTURE` 不会重复记录。\n\n### 开启 `PLAN REPLAYER CONTINUOUS CAPTURE`\n\n`PLAN REPLAYER CONTINUOUS CAPTURE` 功能通过系统变量 [`tidb_enable_plan_replayer_continuous_capture`](/system-variables.md#tidb_enable_plan_replayer_continuous_capture-从-v700-版本开始引入) 控制。要开启 `PLAN REPLAYER CONTINUOUS CAPTURE`，将变量值设为 `ON`。\n\n### 查看 `PLAN REPLAYER CONTINUOUS CAPTURE` 抓取结果\n\n查看 `PLAN REPLAYER CONTINUOUS CAPTURE` 抓取结果的方法同[查看 `PLAN REPLAYER CAPTURE` 抓取结果](#查看-plan-replayer-capture-抓取结果)。\n"
        },
        {
          "name": "sql-prepared-plan-cache.md",
          "type": "blob",
          "size": 19.5498046875,
          "content": "---\ntitle: Prepare 语句执行计划缓存\naliases: ['/docs-cn/dev/sql-prepare-plan-cache/','zh/tidb/dev/sql-prepare-plan-cache']\nsummary: Prepare 语句执行计划缓存功能默认打开，可通过变量启用或关闭。缓存功能仅针对 Prepare/Execute 请求，对普通查询无效。缓存功能会有一定内存开销，可通过监控查看内存使用情况。可手动清空计划缓存，但不支持一次性清空整个集群的计划缓存。忽略 COM_STMT_CLOSE 指令和 DEALLOCATE PREPARE 语句，可解决计划被立即清理的问题。监控 Queries Using Plan Cache OPS 和 Plan Cache Miss OPS，以确保 SQL 执行计划缓存正常工作。Prepared Statement Count 图表显示非零值，表示应用使用了预处理语句。\n---\n\n# Prepare 语句执行计划缓存\n\nTiDB 支持对 `Prepare`/`Execute` 请求的执行计划缓存。其中包括以下两种形式的预处理语句：\n\n- 使用 `COM_STMT_PREPARE` 和 `COM_STMT_EXECUTE` 的协议功能；\n- 执行 `Prepare`/`Execute` SQL 语句查询；\n\nTiDB 优化器对这两类查询的处理是一样的：`Prepare` 时将参数化的 SQL 查询解析成 AST（抽象语法树），每次 `Execute` 时根据保存的 AST 和具体的参数值生成执行计划。\n\n当开启执行计划缓存后，每条 `Prepare` 语句的第一次 `Execute` 会检查当前查询是否可以使用执行计划缓存，如果可以则将生成的执行计划放进一个由 LRU 链表构成的缓存中；在后续的 `Execute` 中，会先从缓存中获取执行计划，并检查是否可用，如果获取和检查成功则跳过生成执行计划这一步，否则重新生成执行计划并放入缓存中。\n\n对于某些非 `PREPARE` 语句，TiDB 可以像 `Prepare`/`Execute` 语句一样支持执行计划缓存，详情请参考[非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)。\n\n在当前版本中，当 `Prepare` 语句符合以下条件任何一条，查询或者计划不会被缓存：\n\n- `SELECT`、`UPDATE`、`INSERT`、`DELETE`、`Union`、`Intersect`、`Except` 以外的 SQL 语句；\n- 访问临时表、包含生成列的表的查询，或使用静态模式（即 [`tidb_partition_prune_mode`](/system-variables.md#tidb_partition_prune_mode-从-v51-版本开始引入) 设置为 `static`）访问分区表的查询；\n- 查询中包含非关联子查询，例如 `SELECT * FROM t1 WHERE t1.a > (SELECT 1 FROM t2 WHERE t2.b < 1)`；\n- 执行计划中带有 `PhysicalApply` 算子的关联子查询，例如 `SELECT * FROM t1 WHERE t1.a > (SELECT a FROM t2 WHERE t1.b > t2.b)`；\n- 包含 `ignore_plan_cache` 或 `set_var` 这两个 Hint 的查询，例如 `SELECT /*+ ignore_plan_cache() */ * FROM t` 或 `SELECT /*+ set_var(max_execution_time=1) */ * FROM t`；\n- 包含除 `?` 外其他变量（即系统变量或用户自定义变量）的查询，例如 `select * from t where a>? and b>@x`；\n- 查询包含无法被缓存函数。目前不能被缓存的函数有：`database()`、`current_user`、`current_role`、`user`、`connection_id`、`last_insert_id`、`row_count`、`version`、`like`；\n- `LIMIT` 后面带有变量（例如 `LIMIT ?` 或 `LIMIT 10, ?`）且变量值大于 10000 的执行计划不缓存；\n- `?` 直接在 `Order By` 后的查询，如 `Order By ?`，此时 `?` 表示根据 `Order By` 后第几列排序，排序列不同的查询使用同一个计划可能导致错误结果，故不缓存；如果是普通表达式，如 `Order By a+?` 则会缓存；\n- `?` 紧跟在 `Group by` 后的查询，如 `Group By ?`，此时 `?` 表示根据 `Group By` 后第几列聚合，聚合列不同的查询使用同一个计划可能导致错误结果，故不缓存；如果是普通表达式，如 `Group By a+?` 则会缓存；\n- `?` 出现在窗口函数 `Window Frame` 定义中的查询，如 `(partition by year order by sale rows ? preceding)`；如果 `?` 出现在窗口函数的其他位置，则会缓存；\n- 用参数进行 `int` 和 `string` 比较的查询，如 `c_int >= ?` 或者 `c_int in (?, ?)`等，其中 `?` 为字符串类型，如 `set @x='123'`；此时为了保证结果和 MySQL 兼容性，需要每次对参数进行调整，故不会缓存；\n- 会访问 `TiFlash` 的计划不会被缓存；\n- 大部分情况下计划中含有 `TableDual` 的计划将将不会被缓存，除非当前执行的 `Prepare` 语句不含参数，则对应的 `TableDual` 计划可以被缓存。\n- 访问 TiDB 系统视图的查询，如 `information_schema.columns`。不建议使用 `Prepare`/`Execute` 语句访问系统视图。\n\nTiDB 对 `?` 的个数有限制，如果超过了 65535 个，则会报错 `Prepared statement contains too many placeholders`。\n\nLRU 链表是设计成 session 级别的缓存，因为 `Prepare`/`Execute` 不能跨 session 执行。LRU 链表的每个元素是一个 key-value 对，value 是执行计划，key 由如下几部分组成：\n\n- 执行 `Execute` 时所在数据库的名字；\n- `Prepare` 语句的标识符，即紧跟在 `PREPARE` 关键字后的名字；\n- 当前的 schema 版本，每条执行成功的 DDL 语句会修改 schema 版本；\n- 执行 `Execute` 时的 SQL Mode；\n- 当前设置的时区，即系统变量 `time_zone` 的值；\n- 系统变量 `sql_select_limit` 的值；\n\nkey 中任何一项变动（如切换数据库、重命名 `Prepare` 语句、执行 DDL、修改 SQL Mode/`time_zone` 的值）、或 LRU 淘汰机制触发都会导致 `Execute` 时无法命中执行计划缓存。\n\n成功从缓存中获取到执行计划后，TiDB 会先检查执行计划是否依然合法，如果当前 `Execute` 在显式事务里执行，并且引用的表在事务前序语句中被修改，而缓存的执行计划对该表访问不包含 `UnionScan` 算子，则它不能被执行。\n\n在通过合法性检测后，会根据当前最新参数值，对执行计划的扫描范围做相应调整，再用它执行获取数据。\n\n关于执行计划缓存和查询性能有几点值得注意：\n\n- 不管计划是否已经被缓存，都会受到 SQL Binding 的影响。对于没有被缓存的计划，即在第一次执行 `Execute` 时，会受到已有 SQL Binding 的影响；而对于已经缓存的计划，如果有新的 SQL Binding 被创建产生，则原有已经被缓存的计划会失效。\n- 已经被缓存的计划不会受到统计信息更新、优化规则和表达式下推黑名单更新的影响，仍然会使用已经保存在缓存中的计划。\n- 重启 TiDB 实例时（如不停机滚动升级 TiDB 集群），`Prepare` 信息会丢失，此时执行 `execute stmt ...` 可能会遇到 `Prepared Statement not found` 的错误，此时需要再执行一次 `prepare stmt ...`。\n- 考虑到不同 `Execute` 的参数会不同，执行计划缓存为了保证适配性会禁止一些和具体参数值密切相关的激进查询优化手段，导致对特定的一些参数值，查询计划可能不是最优。比如查询的过滤条件为 `where a > ? and a < ?`，第一次 `Execute` 时参数分别为 2 和 1，考虑到这两个参数下次执行时可能会是 1 和 2，优化器不会生成对当前参数最优的 `TableDual` 执行计划。\n- 如果不考虑缓存失效和淘汰，一份执行计划缓存会对应各种不同的参数取值，理论上也会导致某些取值下执行计划非最优。比如查询过滤条件为 `where a < ?`，假如第一次执行 `Execute` 时用的参数值为 1，此时优化器生成最优的 `IndexScan` 执行计划放入缓存，在后续执行 `Execute` 时参数变为 10000，此时 `TableScan` 可能才是更优执行计划，但由于执行计划缓存，执行时还是会使用先前生成的 `IndexScan`。因此执行计划缓存更适用于查询较为简单（查询编译耗时占比较高）且执行计划较为固定的业务场景。\n\n自 v6.1.0 起，执行计划缓存功能默认打开，可以通过变量 [`tidb_enable_prepared_plan_cache`](/system-variables.md#tidb_enable_prepared_plan_cache-从-v610-版本开始引入) 启用或关闭这项功能。\n\n> **注意：**\n>\n> 系统变量 [`tidb_enable_prepared_plan_cache`](/system-variables.md#tidb_enable_prepared_plan_cache-从-v610-版本开始引入) 控制的执行计划缓存仅针对 `Prepare`/`Execute` 请求，对普通查询无效。普通查询的执行计划缓存参见[非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)。\n\n在开启了执行计划缓存功能后，可以通过 SESSION 级别的系统变量 [`last_plan_from_cache`](/system-variables.md#last_plan_from_cache-从-v40-版本开始引入) 查看上一条 `Execute` 语句是否使用了缓存的执行计划，例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nMySQL [test]> create table t(a int);\nQuery OK, 0 rows affected (0.00 sec)\n\nMySQL [test]> prepare stmt from 'select * from t where a = ?';\nQuery OK, 0 rows affected (0.00 sec)\n\nMySQL [test]> set @a = 1;\nQuery OK, 0 rows affected (0.00 sec)\n\n-- 第一次 execute 生成执行计划放入缓存\nMySQL [test]> execute stmt using @a;\nEmpty set (0.00 sec)\n\nMySQL [test]> select @@last_plan_from_cache;\n+------------------------+\n| @@last_plan_from_cache |\n+------------------------+\n| 0                      |\n+------------------------+\n1 row in set (0.00 sec)\n\n-- 第二次 execute 命中缓存\nMySQL [test]> execute stmt using @a;\nEmpty set (0.00 sec)\n\nMySQL [test]> select @@last_plan_from_cache;\n+------------------------+\n| @@last_plan_from_cache |\n+------------------------+\n| 1                      |\n+------------------------+\n1 row in set (0.00 sec)\n```\n\n如果发现某一组 `Prepare`/`Execute` 由于执行计划缓存导致了非预期行为，可以通过 SQL Hint `ignore_plan_cache()` 让该组语句不使用缓存。还是用上述的 `stmt` 为例：\n\n{{< copyable \"sql\" >}}\n\n```sql\nMySQL [test]> prepare stmt from 'select /*+ ignore_plan_cache() */ * from t where a = ?';\nQuery OK, 0 rows affected (0.00 sec)\n\nMySQL [test]> set @a = 1;\nQuery OK, 0 rows affected (0.00 sec)\n\nMySQL [test]> execute stmt using @a;\nEmpty set (0.00 sec)\n\nMySQL [test]> select @@last_plan_from_cache;\n+------------------------+\n| @@last_plan_from_cache |\n+------------------------+\n| 0                      |\n+------------------------+\n1 row in set (0.00 sec)\n\nMySQL [test]> execute stmt using @a;\nEmpty set (0.00 sec)\n\nMySQL [test]> select @@last_plan_from_cache;\n+------------------------+\n| @@last_plan_from_cache |\n+------------------------+\n| 0                      |\n+------------------------+\n1 row in set (0.00 sec)\n```\n\n## 诊断 Prepared Plan Cache\n\n### 通过 `SHOW WARNINGS` 诊断 \n\n对于无法进行缓存的查询或计划，可通过 `SHOW WARNINGS` 语句查看查询或计划是否被缓存。如果未被缓存，则可在结果中查看无法被缓存的原因。示例如下：\n\n```sql\nmysql> PREPARE st FROM 'SELECT * FROM t WHERE a > (SELECT MAX(a) FROM t)';  -- 该查询包含子查询，因此无法被缓存\nQuery OK, 0 rows affected, 1 warning (0.01 sec)\n\nmysql> SHOW WARNINGS;  -- 查看查询计划无法被缓存的原因\n+---------+------+-----------------------------------------------+\n| Level   | Code | Message                                       |\n+---------+------+-----------------------------------------------+\n| Warning | 1105 | skip plan-cache: sub-queries are un-cacheable |\n+---------+------+-----------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> PREPARE st FROM 'SELECT * FROM t WHERE a<?';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SET @a='1';\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> EXECUTE st USING @a;  -- 该优化中进行了非 INT 类型到 INT 类型的转换，产生的执行计划可能随着参数变化而存在风险，因此 TiDB 不缓存该计划\nEmpty set, 1 warning (0.01 sec)\n\nmysql> SHOW WARNINGS;\n+---------+------+----------------------------------------------+\n| Level   | Code | Message                                      |\n+---------+------+----------------------------------------------+\n| Warning | 1105 | skip plan-cache: '1' may be converted to INT |\n+---------+------+----------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n### 通过 `Statements Summary` 诊断\n\n在 `Statements Summary` 表中包含有 `plan_cache_unqualified` 和 `plan_cache_unqualified_last_reason` 两个字段，分别表示对应查询无法使用 Plan Cache 的次数和原因，可以通过这两个字段来进行诊断：\n\n```sql\nmysql> SELECT digest_text, plan_cache_unqualified,  plan_cache_unqualified_last_reason FROM information_schema.statements_summary WHERE plan_cache_unqualified > 0 ORDER BY plan_cache_unqualified DESC\nLIMIT 10;\n+---------------------------------+------------------------+----------------------------------------+\n| digest_text                     | plan_cache_unqualified | plan_cache_unqualified_last_reason     |\n+---------------------------------+------------------------+----------------------------------------+\n| select * from `t` where `a` < ? |                     10 | '1' may be converted to INT            |\n| select * from `t` order by ?    |                      4 | query has 'order by ?' is un-cacheable |\n| select database ( ) from `t`    |                      2 | query has 'database()' is un-cacheable |\n...\n+---------------------------------+------------------------+----------------------------------------+\n10 row in set (0.01 sec)\n```\n\n## Prepared Plan Cache 的内存管理\n\n使用 Prepared Plan Cache 会有一定的内存开销，可以通过 Grafana 中的 [`Plan Cache Memory Usage` 监控](/grafana-tidb-dashboard.md)查看每台 TiDB 实例上所有 `SESSION` 所缓存的计划占用的总内存。\n\n> **注意：**\n>\n> 考虑到 Golang 的内存回收机制以及部分未统计的内存结构，Grafana 中显示的内存与实际的堆内存使用量并不相等。经过实验验证存在约 ±20% 的误差。\n\n对于每台 TiDB 实例上所缓存的执行计划总数量，可以通过 Grafana 中的 [`Plan Cache Plan Num` 监控](/grafana-tidb-dashboard.md)查看。\n\nGrafana 中 `Plan Cache Memory Usage` 和 `Plan Cache Plan Num` 监控如下图所示：\n\n![grafana_panels](/media/planCache-memoryUsage-planNum-panels.png)\n\n从 v7.1.0 开始，你可以通过变量 [`tidb_session_plan_cache_size`](/system-variables.md#tidb_session_plan_cache_size-从-v710-版本开始引入) 来设置每个 `SESSION` 最多缓存的计划数量。针对不同的环境，推荐的设置如下，你可以结合监控进行调整：\n\n- TiDB Server 实例内存阈值 <= 64 GiB 时，`tidb_session_plan_cache_size = 50`\n- TiDB Server 实例内存阈值 > 64 GiB 时，`tidb_session_plan_cache_size = 100`\n\n从 v7.1.0 开始，你可以通过变量 [`tidb_plan_cache_max_plan_size`](/system-variables.md#tidb_plan_cache_max_plan_size-从-v710-版本开始引入) 来设置可以缓存的计划的最大大小，默认为 2 MB。超过该值的执行计划将不会被缓存到 Plan Cache 中。\n\n当 TiDB Server 的内存余量小于一定阈值时，会触发 Plan Cache 的内存保护机制，此时会对一些缓存的计划进行逐出。\n\n目前该阈值由变量 `tidb_prepared_plan_cache_memory_guard_ratio` 控制，默认为 0.1，即 10%，也就是当剩余内存不足 10%（使用内存超过 90%）时，会触发此机制。\n\n由于内存限制，Plan Cache 可能出现 Cache Miss 的情况，可以通过 Grafana 中的 [`Plan Cache Miss OPS` 监控](/grafana-tidb-dashboard.md)查看。\n\n## 手动清空计划缓存\n\n通过执行 `ADMIN FLUSH [SESSION | INSTANCE] PLAN_CACHE` 语句，你可以手动清空计划缓存。\n\n该语句中的作用域 `[SESSION | INSTANCE]` 用于指定需要清空的缓存级别，可以为 `SESSION` 或 `INSTANCE`。如果不指定作用域，该语句默认清空 `SESSION` 级别的缓存。\n\n下面是一个清空计划缓存的例子：\n\n{{< copyable \"sql\" >}}\n\n```sql\nMySQL [test]> create table t (a int);\nQuery OK, 0 rows affected (0.00 sec)\n\nMySQL [test]> prepare stmt from 'select * from t';\nQuery OK, 0 rows affected (0.00 sec)\n\nMySQL [test]> execute stmt;\nEmpty set (0.00 sec)\n\nMySQL [test]> execute stmt;\nEmpty set (0.00 sec)\n\nMySQL [test]> select @@last_plan_from_cache; -- 选择计划缓存\n+------------------------+\n| @@last_plan_from_cache |\n+------------------------+\n|                      1 |\n+------------------------+\n1 row in set (0.00 sec)\n\nMySQL [test]> admin flush session plan_cache; -- 清空当前 session 的计划缓存\nQuery OK, 0 rows affected (0.00 sec)\n\nMySQL [test]> execute stmt;\nEmpty set (0.00 sec)\n\nMySQL [test]> select @@last_plan_from_cache; -- 由于缓存被清空，此时无法再次选中\n+------------------------+\n| @@last_plan_from_cache |\n+------------------------+\n|                      0 |\n+------------------------+\n1 row in set (0.00 sec)\n```\n\nTiDB 暂不支持清空 `GLOBAL` 级别的计划缓存，即不支持一次性清空整个集群的计划缓存，使用时会报错：\n\n{{< copyable \"sql\" >}}\n\n```sql\nMySQL [test]> admin flush global plan_cache;\nERROR 1105 (HY000): Do not support the 'admin flush global scope.'\n```\n\n## 忽略 `COM_STMT_CLOSE` 指令和 `DEALLOCATE PREPARE` 语句\n\n为了减少每次执行 SQL 语句的语法分析，Prepared Statement 推荐的使用方式是，prepare 一次，然后 execute 多次，最后 deallocate prepare。例如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nMySQL [test]> prepare stmt from '...'; -- prepare 一次\nMySQL [test]> execute stmt using ...;  -- execute 一次\nMySQL [test]> ...\nMySQL [test]> execute stmt using ...;  -- execute 多次\nMySQL [test]> deallocate prepare stmt; -- 使用完成后释放\n```\n\n如果你习惯于在每次 execute 后都立即执行 deallocate prepare，如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nMySQL [test]> prepare stmt from '...'; -- 第一次 prepare\nMySQL [test]> execute stmt using ...;\nMySQL [test]> deallocate prepare stmt; -- 一次使用后立即释放\nMySQL [test]> prepare stmt from '...'; -- 第二次 prepare\nMySQL [test]> execute stmt using ...;\nMySQL [test]> deallocate prepare stmt; -- 再次释放\n```\n\n这样的使用方式会让第一次执行得到的计划被立即清理，不能在第二次被复用。\n\n为了兼容这样的使用方式，从 v6.0 起，TiDB 支持 [`tidb_ignore_prepared_cache_close_stmt`](/system-variables.md#tidb_ignore_prepared_cache_close_stmt-从-v600-版本开始引入) 变量。打开该变量后，TiDB 会忽略关闭 Prepare Statement 的信号，解决上述问题，如：\n\n{{< copyable \"sql\" >}}\n\n```sql\nmysql> set @@tidb_ignore_prepared_cache_close_stmt=1;  -- 打开开关\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> prepare stmt from 'select * from t'; -- 第一次 prepare\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> execute stmt;                        -- 第一次 execute\nEmpty set (0.00 sec)\n\nmysql> deallocate prepare stmt;             -- 第一次 execute 后立即释放\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> prepare stmt from 'select * from t'; -- 第二次 prepare\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> execute stmt;                        -- 第二次 execute\nEmpty set (0.00 sec)\n\nmysql> select @@last_plan_from_cache;       -- 因为开关打开，第二次依旧能复用上一次的计划\n+------------------------+\n| @@last_plan_from_cache |\n+------------------------+\n|                      1 |\n+------------------------+\n1 row in set (0.00 sec)\n```\n\n### 监控\n\n在 [Grafana 面板](/grafana-tidb-dashboard.md)的 TiDB 页面，**Executor** 部分包含“Queries Using Plan Cache OPS”和“Plan Cache Miss OPS”两个图表，用以检查 TiDB 和应用是否正确配置，以便 SQL 执行计划缓存能正常工作。TiDB 页面的 **Server** 部分还提供了“Prepared Statement Count”图表，如果应用使用了预处理语句，这个图表会显示非零值。通过数值变化，可以判断 SQL 执行计划缓存是否正常工作。\n\n![`sql_plan_cache`](/media/performance/sql_plan_cache.png)\n"
        },
        {
          "name": "sql-statements",
          "type": "tree",
          "content": null
        },
        {
          "name": "sql-tuning-overview.md",
          "type": "blob",
          "size": 1.4306640625,
          "content": "---\ntitle: SQL 性能调优\naliases: ['/docs-cn/dev/sql-tuning-overview/']\nsummary: SQL 性能调优是重要的，TiDB 会优化 SQL 语句的执行，以最省时的方式返回结果。这个过程类似于 GPS 导航，利用统计信息和实时交通信息规划最佳路线。了解 TiDB 执行计划、SQL 优化流程和控制执行计划可以帮助提高查询性能。\n---\n\n# SQL 性能调优\n\nSQL 是一种声明性语言。一条 SQL 语句描述的是最终结果应该如何，而非按顺序执行的步骤。TiDB 会优化 SQL 语句的执行，语义上允许以任何顺序执行查询的各部分，前提是能正确返回语句所描述的最终结果。\n\nSQL 性能优化的过程，可以理解为 GPS 导航的过程。你提供地址后，GPS 软件利用各种统计信息（例如以前的行程、速度限制等元数据，以及实时交通信息）规划出一条最省时的路线。这与 TiDB 中的 SQL 性能优化过程相对应。\n\n本章节包括以下文档，可帮助你更好地理解查询执行计划：\n\n- [理解 TiDB 执行计划](/explain-overview.md)介绍如何使用 `EXPLAIN` 语句来理解 TiDB 是如何执行某个查询的。\n- [SQL 优化流程概览](/sql-optimization-concepts.md)介绍 TiDB 可以使用的几种优化，以提高查询性能。\n- [控制执行计划](/control-execution-plan.md)介绍如何控制执行计划的生成。TiDB 的执行计划非最优时，建议控制执行计划。\n"
        },
        {
          "name": "stale-read.md",
          "type": "blob",
          "size": 5.49609375,
          "content": "---\ntitle: Stale Read 功能的使用场景\nsummary: 介绍 Stale Read 功能和使用场景。\n---\n\n# Stale Read 功能的使用场景\n\n本文档介绍 Stale Read 的使用场景。Stale Read 是一种读取历史数据版本的机制，读取 TiDB 中存储的历史数据版本。通过 Stale Read 功能，你能从指定时间点或时间范围内读取对应的历史数据，从而避免数据同步带来延迟。当使用 Stale Read 时，TiDB 默认会随机选择一个副本来读取数据，因此能利用所有副本。如果你的应用程序不能容忍读到非实时的数据，请勿使用 Stale Read，否则读到的数据可能不是最新成功写入的数据。\n\n## 场景描述\n\n+ 场景一：如果一个事务仅涉及只读操作，并且一定程度上可容忍牺牲实时性，你可以使用 Stale Read 功能来读取历史数据。由于牺牲了一定的实时性，使用 Stale Read 后，TiDB 可以让请求分发到任一个副本上，使得查询的执行获得更大的吞吐量。特别是在一些小表的查询场景中，如果使用了强一致性读，Leader 可能集中在某一个存储节点上，导致查询压力集中在该节点，成为整个查询的瓶颈。通过 Stale Read，可以提升了查询整体的吞吐能力，从而显著提升查询性能。\n\n+ 场景二：在部分跨数据中心部署的场景中，如果使用了强一致性的 Follower 读，为了读到的数据与 Leader 上的数据一致，会产生跨数据中心获取 `Readindex` 来校验的请求，导致整体查询的访问延迟增加。通过使用 Stale Read 功能，可以牺牲一定的实时性，就近访问对应数据所在当前中心的副本，避免跨数据中心的网络延迟，降低整体查询的访问延迟。详情参考[在三数据中心下就近读取数据](/best-practices/three-dc-local-read.md)。\n\n## 使用方法\n\nTiDB 提供语句级别、会话级别以及全局级别的 Stale Read 使用方式，具体使用方法如下：\n\n- 语句级别：\n    - 指定一个精确的时间点（**推荐**）：如需 TiDB 读取一个时间点上保证全局事务记录一致性的数据并且不破坏隔离级别，你可以指定这个时间点对应的时间戳。要使用该方式，请参阅 [`AS OF TIMESTAMP` 语法](/as-of-timestamp.md#语法方式)文档。\n    - 指定时间范围：如需 TiDB 读取在一个时间范围内尽可能新的数据并且不破坏隔离级别，你可以指定一个时间范围。在指定时间范围内，TiDB 会选择一个合适的时间戳，该时间戳能保证所访问的副本上不存在开始于这个时间戳之前且还没有提交的相关事务，即能保证在所访问的可用副本上可执行读取操作而且不会被阻塞。要使用该方式，请参阅 [`AS OF TIMESTAMP` 语法](/as-of-timestamp.md#语法方式)文档和该文档中 [`TIDB_BOUNDED_STALENESS` 函数](/as-of-timestamp.md#语法方式)部分的介绍。\n- 会话级别：\n    - 指定时间范围：在会话级别中，如需 TiDB 在后续的查询中读取一个时间范围内尽可能新的数据并且不破坏隔离级别，你可以通过设置一个 session 变量 `tidb_read_staleness` 来指定一个时间范围。要使用该方式，请参阅[通过系统变量 `tidb_read_staleness` 读取历史数据](/tidb-read-staleness.md)。\n\n除此以外，你也可以通过设置系统变量 [`tidb_external_ts`](/system-variables.md#tidb_external_ts-从-v640-版本开始引入) 来在某一会话或全局范围读取某一时间点前的历史数据。要使用该方式，请参阅[通过系统变量 `tidb_external_ts` 读取历史数据](/tidb-external-ts.md)。\n\n### 减少 Stale Read 延时\n\nStale Read 功能会定期推进 TiDB 集群的 Resolved TS 时间戳，该时间戳能保证 TiDB 读到满足事务一致性的数据。当 Stale Read 使用的时间戳（比如 `AS OF TIMESTAMP '2016-10-08 16:45:26'`）大于 Resolved TS 时，Stale Read 会先触发 TiDB 推进 Resolved TS，等待推进完成后再读取数据，从而导致延时上升。\n\n通过调整下面 TiKV 的配置项，你可以使 TiDB 加快 Resolved TS 推进，以减少 Stale Read 延时：\n\n```toml\n[resolved-ts]\nadvance-ts-interval = \"20s\" # 默认为 20 秒，可适当调小该值以加快 Resolved TS 推进，比如调整为 1 秒。\n```\n\n> **注意：**\n>\n> 调小该参数会增加 TiKV CPU 使用率和各节点之间的流量。\n\n关于 Resolved TS 的内部原理和诊断方法，请参阅[理解 TiKV 中的 Stale Read 和 safe-ts](/troubleshoot-stale-read.md)。\n\n## 限制\n\n当对表的 Stale Read 查询下推到 TiFlash 时，如果该表在 Stale Read 所指定的读取时间戳之后执行过 DDL 操作，此查询将会报错。原因是 TiFlash 只支持从最新的表结构读取数据。\n\n例如：\n\n```sql\ncreate table t1(id int);\nalter table t1 set tiflash replica 1;\n```\n\n一分钟后进行 DDL 操作：\n\n```sql\nalter table t1 add column c1 int not null;\n```\n\n然后使用 Stale Read 读取一分钟前的数据：\n\n```sql\nset @@session.tidb_enforce_mpp=1;\nselect * from t1 as of timestamp NOW() - INTERVAL 1 minute;\n```\n\n此时 TiFlash 会报错：\n\n```\nERROR 1105 (HY000): other error for mpp stream: From MPP<query:<query_ts:1673950975508472943, local_query_id:18, server_id:111947, start_ts:438816196526080000>,task_id:1>: Code: 0, e.displayText() = DB::TiFlashException: Table 323 schema version 104 newer than query schema version 100, e.what() = DB::TiFlashException,\n```\n\n把 Stale Read 指定的读取时间戳改成 DDL 操作完成之后的时间，即可避免该错误。\n"
        },
        {
          "name": "statement-summary-tables.md",
          "type": "blob",
          "size": 21.97265625,
          "content": "---\ntitle: Statement Summary Tables\naliases: ['/docs-cn/dev/statement-summary-tables/','/docs-cn/dev/reference/performance/statement-summary/']\nsummary: MySQL 的 `performance_schema` 提供了 `statement summary tables`，用于监控和统计 SQL 性能。TiDB 在 `information_schema` 中提供了类似功能的系统表，包括 `statements_summary`、`statements_summary_history`、`cluster_statements_summary` 和 `cluster_statements_summary_history`。这些表用于保存 SQL 监控指标聚合后的结果，帮助用户定位 SQL 问题。同时，还提供了参数配置来控制 statement summary 的功能，如清空周期、保存历史的数量等。\n---\n\n# Statement Summary Tables\n\n针对 SQL 性能相关的问题，MySQL 在 `performance_schema` 提供了 [statement summary tables](https://dev.mysql.com/doc/refman/8.0/en/performance-schema-statement-summary-tables.html)，用来监控和统计 SQL。例如其中的一张表 `events_statements_summary_by_digest`，提供了丰富的字段，包括延迟、执行次数、扫描行数、全表扫描次数等，有助于用户定位 SQL 问题。\n\n为此，从 4.0.0-rc.1 版本开始，TiDB 在 `information_schema`（_而不是_ `performance_schema`）中提供与 `events_statements_summary_by_digest` 功能相似的系统表：\n\n- `statements_summary`\n- `statements_summary_history`\n- `cluster_statements_summary`\n- `cluster_statements_summary_history`\n\n本文将详细介绍这些表，以及如何利用它们来排查 SQL 性能问题。\n\n## `statements_summary`\n\n`statements_summary` 是 `information_schema` 里的一张系统表，它把 SQL 按 所属资源组、SQL digest 和 plan digest 分组，统计每一组的 SQL 信息。\n\n此处的 SQL digest 与 slow log 里的 SQL digest 一样，是把 SQL 规一化后算出的唯一标识符。SQL 的规一化会忽略常量、空白符、大小写的差别。即语法一致的 SQL 语句，其 digest 也相同。\n\n例如：\n\n```sql\nSELECT * FROM employee WHERE id IN (1, 2, 3) AND salary BETWEEN 1000 AND 2000;\nselect * from EMPLOYEE where ID in (4, 5) and SALARY between 3000 and 4000;\n```\n\n归一化后都是：\n\n```sql\nselect * from employee where id in (...) and salary between ? and ?;\n```\n\n此处的 plan digest 是把执行计划规一化后算出的唯一标识符。执行计划的规一化会忽略常量的差别。由于相同的 SQL 可能产生不同的执行计划，所以可能分到多个组，同一个组内的执行计划是相同的。\n\n`statements_summary` 用于保存 SQL 监控指标聚合后的结果。一般来说，每一项监控指标都包含平均值和最大值。例如执行延时对应 `AVG_LATENCY` 和 `MAX_LATENCY` 两个字段，分别是平均延时和最大延时。\n\n为了监控指标的即时性，`statements_summary` 里的数据定期被清空，只展现最近一段时间内的聚合结果。清空周期由系统变量 `tidb_stmt_summary_refresh_interval` 设置。如果刚好在清空之后进行查询，显示的数据可能很少。\n\n以下为查询 `statements_summary` 的部分结果：\n\n```\n   SUMMARY_BEGIN_TIME: 2020-01-02 11:00:00\n     SUMMARY_END_TIME: 2020-01-02 11:30:00\n            STMT_TYPE: Select\n          SCHEMA_NAME: test\n               DIGEST: 0611cc2fe792f8c146cc97d39b31d9562014cf15f8d41f23a4938ca341f54182\n          DIGEST_TEXT: select * from employee where id = ?\n          TABLE_NAMES: test.employee\n          INDEX_NAMES: NULL\n          SAMPLE_USER: root\n           EXEC_COUNT: 3\n          SUM_LATENCY: 1035161\n          MAX_LATENCY: 399594\n          MIN_LATENCY: 301353\n          AVG_LATENCY: 345053\n    AVG_PARSE_LATENCY: 57000\n    MAX_PARSE_LATENCY: 57000\n  AVG_COMPILE_LATENCY: 175458\n  MAX_COMPILE_LATENCY: 175458\n  ...........\n              AVG_MEM: 103\n              MAX_MEM: 103\n              AVG_DISK: 65535\n              MAX_DISK: 65535\n    AVG_AFFECTED_ROWS: 0\n           FIRST_SEEN: 2020-01-02 11:12:54\n            LAST_SEEN: 2020-01-02 11:25:24\n    QUERY_SAMPLE_TEXT: select * from employee where id=3100\n     PREV_SAMPLE_TEXT:\n          PLAN_DIGEST: f415b8d52640b535b9b12a9c148a8630d2c6d59e419aad29397842e32e8e5de3\n                 PLAN:  Point_Get_1     root    1       table:employee, handle:3100\n```\n\n> **注意：**\n>\n> - 在 TiDB 中，statement summary tables 中字段的时间单位是纳秒 (ns)，而 MySQL 中的时间单位是皮秒 (ps)。\n> - 从 v7.5.1 和 v7.6.0 版本开始，对于开启[资源管控](/tidb-resource-control.md)的集群，`statements_summary` 会分资源组进行聚合，即在不同资源组执行的相同语句会被收集为不同的记录。\n\n## `statements_summary_history`\n\n`statements_summary_history` 的表结构与 `statements_summary` 完全相同，用于保存历史时间段的数据。通过历史数据，可以排查过去出现的异常，也可以对比不同时间的监控指标。\n\n字段 `SUMMARY_BEGIN_TIME` 和 `SUMMARY_END_TIME` 代表历史时间段的开始时间和结束时间。\n\n## `statements_summary_evicted`\n\n`statements_summary` 表的容量受 `tidb_stmt_summary_max_stmt_count` 配置控制，内部使用 LRU 算法，一旦接收到的 SQL 种类超过了 `tidb_stmt_summary_max_stmt_count`，表中最久未被命中的记录就会被驱逐出表。TiDB 引入了 `statements_summary_evicted` 表，该表记录了各个时段被驱逐 SQL 语句的具体数量。\n\n只有当 SQL 语句被 `statement summary` 表驱逐的时候，`statements_summary_evicted` 表的内容才会更新。`statements_summary_evicted` 表记录发生驱逐的时间段和被驱逐 SQL 的数量。\n\n## statement summary 的 cluster 表\n\n`statements_summary`、`statements_summary_history` 和 `statements_summary_evicted` 仅显示单台 TiDB server 的 statement summary 数据。若要查询整个集群的数据，需要查询 `cluster_statements_summary`、`cluster_statements_summary_history` 或 `cluster_statements_summary_evicted` 表。\n\n`cluster_statements_summary` 显示各台 TiDB server 的 `statements_summary` 数据，`cluster_statements_summary_history` 显示各台 TiDB server 的 `statements_summary_history` 数据，而 `cluster_statements_summary_evicted` 则显示各台 TiDB server 的 `statements_summary_evicted` 数据。这三张表用字段 `INSTANCE` 表示 TiDB server 的地址，其他字段与 `statements_summary`、`statements_summary_history` 和 `statements_summary_evicted` 表相同。\n\n## 参数配置\n\n以下系统变量用于控制 statement summary：\n\n- `tidb_enable_stmt_summary`：是否打开 statement summary 功能。1 代表打开，0 代表关闭，默认打开。statement summary 关闭后，系统表里的数据会被清空，下次打开后重新统计。经测试，打开后对性能几乎没有影响。\n- `tidb_stmt_summary_refresh_interval`：`statements_summary` 的清空周期，单位是秒 (s)，默认值是 `1800`。\n- `tidb_stmt_summary_history_size`：`statements_summary_history` 保存每种 SQL 的历史的数量，也是 `statements_summary_evicted` 的表容量，默认值是 `24`。\n- `tidb_stmt_summary_max_stmt_count`：statement summary tables 保存的 SQL 种类数量，默认 3000 条。当 SQL 种类超过该值时，会移除最近没有使用的 SQL。这些 SQL 将会被 `DIGEST` 为 `NULL` 的行和  `statements_summary_evicted` 统计记录。`DIGEST` 为 `NULL` 的行数据在 [TiDB Dashboard SQL 语句分析列表页面](/dashboard/dashboard-statement-list.md#others) 中显示为 `Others`。\n- `tidb_stmt_summary_max_sql_length`：字段 `DIGEST_TEXT` 和 `QUERY_SAMPLE_TEXT` 的最大显示长度，默认值是 4096。\n- `tidb_stmt_summary_internal_query`：是否统计 TiDB 的内部 SQL。1 代表统计，0 代表不统计，默认不统计。\n\n> **注意：**\n>\n> 当一种 SQL 因为达到 `tidb_stmt_summary_max_stmt_count` 限制要被移除时，TiDB 会移除该 SQL 语句种类在所有时间段的数据。因此，即使一个时间段内的 SQL 种类数量没有达到上限，显示的 SQL 语句数量也会比实际的少。如遇到该情况，对性能也有一些影响，建议调大 `tidb_stmt_summary_max_stmt_count` 的值。\n\nstatement summary 配置示例如下：\n\n```sql\nset global tidb_stmt_summary_max_stmt_count = 3000;\nset global tidb_enable_stmt_summary = true;\nset global tidb_stmt_summary_refresh_interval = 1800;\nset global tidb_stmt_summary_history_size = 24;\n```\n\n以上配置生效后，`statements_summary` 每 30 分钟清空一次，`statements_summary_history` 最多保存 3000 种 SQL 种类的数据，每种类型的 SQL 保存最近出现过的 24 个时间段的数据。`statements_summary_evicted` 保存最近 24 个发生了 evict 的时间段记录；`statements_summary_evicted` 则以 30 分钟为一个记录周期，表容量为 24 个时间段。\n\n> **注意：**\n>\n> - 假设某种 SQL 每分钟都出现，那 `statements_summary_history` 中会保存这种 SQL 最近 12 个小时的数据。但如果某种 SQL 只在每天 00:00 ~ 00:30 出现，则 `statements_summary_history` 中会保存这种 SQL 24 个时间段的数据，每个时间段的间隔都是 1 天，所以会有这种 SQL 最近 24 天的数据。\n> - `tidb_stmt_summary_history_size`、`tidb_stmt_summary_max_stmt_count`、`tidb_stmt_summary_max_sql_length` 这些配置都影响内存占用，建议根据实际情况调整（取决于 SQL 大小、SQL 数量、机器配置）不宜设置得过大。内存大小可通过 `tidb_stmt_summary_history_size` \\* `tidb_stmt_summary_max_stmt_count` \\* `tidb_stmt_summary_max_sql_length` \\* `3` 来进行估算。\n\n### 为 statement summary 设定合适的大小\n\n在系统运行一段时间后（视系统负载而定），可以查看 `statements_summary` 表检查是否发生了 evict，例如：\n\n```sql\nselect @@global.tidb_stmt_summary_max_stmt_count;\nselect count(*) from information_schema.statements_summary;\n```\n\n```\n+-------------------------------------------+\n| @@global.tidb_stmt_summary_max_stmt_count |\n+-------------------------------------------+\n| 3000                                      |\n+-------------------------------------------+\n1 row in set (0.001 sec)\n\n+----------+\n| count(*) |\n+----------+\n|     3001 |\n+----------+\n1 row in set (0.001 sec)\n```\n\n可以发现 `statements_summary` 表已经满了。再查看 `statements_summary_evicted` 表检查 evict 的数据。\n\n```sql\nselect * from information_schema.statements_summary_evicted;\n```\n\n```\n+---------------------+---------------------+---------------+\n| BEGIN_TIME          | END_TIME            | EVICTED_COUNT |\n+---------------------+---------------------+---------------+\n| 2020-01-02 16:30:00 | 2020-01-02 17:00:00 |            59 |\n+---------------------+---------------------+---------------+\n| 2020-01-02 16:00:00 | 2020-01-02 16:30:00 |            45 |\n+---------------------+---------------------+---------------+\n2 row in set (0.001 sec)\n```\n\n由上可知，对最多 59 种 SQL 发生了 evict。此时，建议将 `statements_summary` 表的容量至少增大 59 条记录，即至少增大至 3059 条。\n\n## 目前的限制\n\n由于 statement summary tables 默认都存储在内存中，TiDB server 重启后，statement summary 会全部丢失。\n\n为解决该问题，TiDB v6.6.0 实验性地引入了 [statement summary 持久化](#持久化-statements-summary)功能，该功能默认为关闭。开启该功能后，历史数据不再存储在内存内，而是直接写入磁盘。TiDB server 重启后，历史数据也依然可用。\n\n## 持久化 statements summary\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n如[目前的限制](#目前的限制)一节所描述，默认情况下 statements summary 只在内存中维护，一旦 TiDB server 发生重启，所有 statements summary 数据都会丢失。自 v6.6.0 起，TiDB 实验性地提供了配置项 [`tidb_stmt_summary_enable_persistent`](/tidb-configuration-file.md#tidb_stmt_summary_enable_persistent-从-v660-版本开始引入) 来允许用户控制是否开启 statements summary 持久化。\n\n如果要开启 statements summary 持久化，可以在 TiDB 配置文件中添加如下配置：\n\n```toml\n[instance]\ntidb_stmt_summary_enable_persistent = true\n# 以下配置为默认值，可根据需求调整。\n# tidb_stmt_summary_filename = \"tidb-statements.log\"\n# tidb_stmt_summary_file_max_days = 3\n# tidb_stmt_summary_file_max_size = 64 # MiB\n# tidb_stmt_summary_file_max_backups = 0\n```\n\n开启 statements summary 持久化后，内存中只维护当前的实时数据，不再维护历史数据。历史数据生成后直接被写入磁盘文件，写入周期参考[参数配置](#参数配置)一节所描述的 `tidb_stmt_summary_refresh_interval`。后续针对 `statements_summary_history` 或 `cluster_statements_summary_history` 表的查询将结合内存和磁盘两处数据返回结果。\n\n> **注意：**\n>\n> - 当开启持久化后，由于不再于内存中维护历史数据，因此[参数配置](#参数配置)一节所描述的 `tidb_stmt_summary_history_size` 将不再生效，而是由 [`tidb_stmt_summary_file_max_days`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_days-从-v660-版本开始引入)、[`tidb_stmt_summary_file_max_size`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_size-从-v660-版本开始引入) 和 [`tidb_stmt_summary_file_max_backups`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_backups-从-v660-版本开始引入) 这三项配置来决定历史数据在磁盘上的保留数量和时间。\n> - `tidb_stmt_summary_refresh_interval` 取值越小，数据写入到磁盘就越实时，但写入磁盘的冗余数据也会随之增多。\n\n## 排查示例\n\n下面用两个示例问题演示如何利用 statement summary 来排查。\n\n### SQL 延迟比较大，是不是服务端的问题？\n\n例如客户端显示 employee 表的点查比较慢，那么可以按 SQL 文本来模糊查询：\n\n```sql\nSELECT avg_latency, exec_count, query_sample_text\n    FROM information_schema.statements_summary\n    WHERE digest_text LIKE 'select * from employee%';\n```\n\n结果如下，`avg_latency` 是 1 ms 和 0.3 ms，在正常范围，所以可以判定不是服务端的问题，继而排查客户端或网络问题。\n\n```\n+-------------+------------+------------------------------------------+\n| avg_latency | exec_count | query_sample_text                        |\n+-------------+------------+------------------------------------------+\n|     1042040 |          2 | select * from employee where name='eric' |\n|      345053 |          3 | select * from employee where id=3100     |\n+-------------+------------+------------------------------------------+\n2 rows in set (0.00 sec)\n```\n\n### 哪类 SQL 的总耗时最高？\n\n假如上午 10:00 到 10:30 的 QPS 明显下降，可以从历史表中找出当时耗时最高的三类 SQL：\n\n```sql\nSELECT sum_latency, avg_latency, exec_count, query_sample_text\n    FROM information_schema.statements_summary_history\n    WHERE summary_begin_time='2020-01-02 10:00:00'\n    ORDER BY sum_latency DESC LIMIT 3;\n```\n\n结果显示以下三类 SQL 的总延迟最高，所以这些 SQL 需要重点优化。\n\n```\n+-------------+-------------+------------+-----------------------------------------------------------------------+\n| sum_latency | avg_latency | exec_count | query_sample_text                                                     |\n+-------------+-------------+------------+-----------------------------------------------------------------------+\n|     7855660 |     1122237 |          7 | select avg(salary) from employee where company_id=2013                |\n|     7241960 |     1448392 |          5 | select * from employee join company on employee.company_id=company.id |\n|     2084081 |     1042040 |          2 | select * from employee where name='eric'                              |\n+-------------+-------------+------------+-----------------------------------------------------------------------+\n3 rows in set (0.00 sec)\n```\n\n## 表的字段介绍\n\n### `statements_summary` 字段介绍\n\n下面介绍 `statements_summary` 表中各个字段的含义。\n\nSQL 的基础信息：\n\n- `STMT_TYPE`：SQL 语句的类型\n- `SCHEMA_NAME`：执行这类 SQL 的当前 schema\n- `DIGEST`：这类 SQL 的 digest\n- `DIGEST_TEXT`：规一化后的 SQL\n- `QUERY_SAMPLE_TEXT`：这类 SQL 的原 SQL 语句，多条语句只取其中一条\n- `TABLE_NAMES`：SQL 中涉及的所有表，多张表用 `,` 分隔\n- `INDEX_NAMES`：SQL 中使用的索引名，多个索引用 `,` 分隔\n- `SAMPLE_USER`：执行这类 SQL 的用户名，多个用户名只取其中一个\n- `PLAN_DIGEST`：执行计划的 digest\n- `PLAN`：原执行计划，多条语句只取其中一条的执行计划\n- `BINARY_PLAN`：以二进制格式编码后的原执行计划，存在多条语句时，只取其中一条语句的执行计划。用 [`SELECT tidb_decode_binary_plan('xxx...')`](/functions-and-operators/tidb-functions.md#tidb_decode_binary_plan) SQL 语句可以解析出具体的执行计划。\n- `PLAN_CACHE_HITS`：这类 SQL 语句命中 plan cache 的总次数\n- `PLAN_IN_CACHE`：这类 SQL 语句的上次执行是否命中了 plan cache\n- `PLAN_CACHE_UNQUALIFIED`：这类 SQL 语句没有命中 plan cache 的次数\n- `PLAN_CACHE_UNQUALIFIED_LAST_REASON`：这类 SQL 语句最后一次没有命中 plan cache 的原因\n\n执行时间相关的信息：\n\n- `SUMMARY_BEGIN_TIME`：当前统计的时间段的开始时间\n- `SUMMARY_END_TIME`：当前统计的时间段的结束时间\n- `FIRST_SEEN`：这类 SQL 的首次出现时间\n- `LAST_SEEN`：这类 SQL 的最后一次出现时间\n\n在 TiDB server 上的执行数据：\n\n- `EXEC_COUNT`：这类 SQL 的总执行次数\n- `SUM_ERRORS`：执行过程中遇到的 error 的总数\n- `SUM_WARNINGS`：执行过程中遇到的 warning 的总数\n- `SUM_LATENCY`：这类 SQL 的总延时\n- `MAX_LATENCY`：这类 SQL 的最大延时\n- `MIN_LATENCY`：这类 SQL 的最小延时\n- `AVG_LATENCY`：这类 SQL 的平均延时\n- `AVG_PARSE_LATENCY`：解析器的平均延时\n- `MAX_PARSE_LATENCY`：解析器的最大延时\n- `AVG_COMPILE_LATENCY`：优化器的平均延时\n- `MAX_COMPILE_LATENCY`：优化器的最大延时\n- `AVG_MEM`：使用的平均内存，单位 byte\n- `MAX_MEM`：使用的最大内存，单位 byte\n- `AVG_DISK`：使用的平均硬盘空间，单位 byte\n- `MAX_DISK`：使用的最大硬盘空间，单位 byte\n\n和 TiKV Coprocessor Task 相关的字段：\n\n- `SUM_COP_TASK_NUM`：发送 Coprocessor 请求的总数\n- `MAX_COP_PROCESS_TIME`：cop-task 的最大处理时间\n- `MAX_COP_PROCESS_ADDRESS`：执行时间最长的 cop-task 所在地址\n- `MAX_COP_WAIT_TIME`：cop-task 的最大等待时间\n- `MAX_COP_WAIT_ADDRESS`：等待时间最长的 cop-task 所在地址\n- `AVG_PROCESS_TIME`：SQL 在 TiKV 的平均处理时间\n- `MAX_PROCESS_TIME`：SQL 在 TiKV 的最大处理时间\n- `AVG_WAIT_TIME`：SQL 在 TiKV 的平均等待时间\n- `MAX_WAIT_TIME`：SQL 在 TiKV 的最大等待时间\n- `AVG_BACKOFF_TIME`：SQL 遇到需要重试的错误时在重试前的平均等待时间\n- `MAX_BACKOFF_TIME`：SQL 遇到需要重试的错误时在重试前的最大等待时间\n- `AVG_TOTAL_KEYS`：Coprocessor 扫过的 key 的平均数量\n- `MAX_TOTAL_KEYS`：Coprocessor 扫过的 key 的最大数量\n- `AVG_PROCESSED_KEYS`：Coprocessor 处理的 key 的平均数量。相比 `avg_total_keys`，`avg_processed_keys` 不包含 MVCC 的旧版本。如果 `avg_total_keys` 和 `avg_processed_keys` 相差很大，说明旧版本比较多\n- `MAX_PROCESSED_KEYS`：Coprocessor 处理的 key 的最大数量\n\n和事务相关的字段：\n\n- `AVG_PREWRITE_TIME`：prewrite 阶段消耗的平均时间\n- `MAX_PREWRITE_TIME` prewrite 阶段消耗的最大时间\n- `AVG_COMMIT_TIME`：commit 阶段消耗的平均时间\n- `MAX_COMMIT_TIME`：commit 阶段消耗的最大时间\n- `AVG_GET_COMMIT_TS_TIME`：获取 commit_ts 的平均时间\n- `MAX_GET_COMMIT_TS_TIME`：获取 commit_ts 的最大时间\n- `AVG_COMMIT_BACKOFF_TIME`：commit 时遇到需要重试的错误时在重试前的平均等待时间\n- `MAX_COMMIT_BACKOFF_TIME`：commit 时遇到需要重试的错误时在重试前的最大等待时间\n- `AVG_RESOLVE_LOCK_TIME`：解决事务的锁冲突的平均时间\n- `MAX_RESOLVE_LOCK_TIME`：解决事务的锁冲突的最大时间\n- `AVG_LOCAL_LATCH_WAIT_TIME`：本地事务等待的平均时间\n- `MAX_LOCAL_LATCH_WAIT_TIME`：本地事务等待的最大时间\n- `AVG_WRITE_KEYS`：写入 key 的平均数量\n- `MAX_WRITE_KEYS`：写入 key 的最大数量\n- `AVG_WRITE_SIZE`：写入的平均数据量，单位 byte\n- `MAX_WRITE_SIZE`：写入的最大数据量，单位 byte\n- `AVG_PREWRITE_REGIONS`：prewrite 涉及的平均 Region 数量\n- `MAX_PREWRITE_REGIONS`：prewrite 涉及的最大 Region 数量\n- `AVG_TXN_RETRY`：事务平均重试次数\n- `MAX_TXN_RETRY`：事务最大重试次数\n- `SUM_BACKOFF_TIMES`：这类 SQL 遇到需要重试的错误后的总重试次数\n- `BACKOFF_TYPES`：遇到需要重试的错误时的所有错误类型及每种类型重试的次数，格式为 `类型:次数`。如有多种错误则用 `,` 分隔，例如 `txnLock:2,pdRPC:1`\n- `AVG_AFFECTED_ROWS`：平均影响行数\n- `PREV_SAMPLE_TEXT`：当 SQL 是 `COMMIT` 时，该字段为 `COMMIT` 的前一条语句；否则该字段为空字符串。当 SQL 是 `COMMIT` 时，按 digest 和 `prev_sample_text` 一起分组，即不同 `prev_sample_text` 的 `COMMIT` 也会分到不同的行\n\n和资源管控相关的字段：\n\n- `AVG_REQUEST_UNIT_WRITE`：执行 SQL 语句平均消耗的写 RU\n- `MAX_REQUEST_UNIT_WRITE`：执行 SQL 语句最大消耗的写 RU\n- `AVG_REQUEST_UNIT_READ`：执行 SQL 语句平均消耗的读 RU\n- `MAX_REQUEST_UNIT_READ`：执行 SQL 语句最大消耗的读 RU\n- `AVG_QUEUED_RC_TIME`：执行 SQL 语句等待可用 RU 的平均耗时\n- `MAX_QUEUED_RC_TIME`：执行 SQL 语句等待可用 RU 的最大耗时\n- `RESOURCE_GROUP`：执行 SQL 语句绑定的资源组\n\n### `statements_summary_evicted` 字段介绍\n\n- `BEGIN_TIME`: 记录的开始时间；\n- `END_TIME`: 记录的结束时间；\n- `EVICTED_COUNT`：在记录的时间段内 evict 了多少种 SQL。\n"
        },
        {
          "name": "statistics.md",
          "type": "blob",
          "size": 52.681640625,
          "content": "---\ntitle: 常规统计信息\nsummary: 介绍 TiDB 中常规统计信息的收集和使用。\naliases: ['/docs-cn/dev/statistics/','/docs-cn/dev/reference/performance/statistics/']\n---\n\n# 常规统计信息\n\nTiDB 使用统计信息作为优化器的输入，用于估算 SQL 语句的执行计划中每个步骤处理的行数。优化器会估算每个可用执行计划的成本，包括[索引的选择](/choose-index.md)和表连接的顺序，并为每个可用执行计划生成成本。然后，优化器会选择总体成本最低的执行计划。\n\n## 收集统计信息\n\n本小节介绍收集统计信息的两种方式：自动更新和手动收集。\n\n### 自动更新\n\n对于 [`INSERT`](/sql-statements/sql-statement-insert.md)、[`DELETE`](/sql-statements/sql-statement-delete.md) 或 [`UPDATE`](/sql-statements/sql-statement-update.md) 语句，TiDB 会自动更新统计信息中表的总行数和修改的行数。\n\nTiDB 会定期持久化更新的统计信息，更新周期为 20 * [`stats-lease`](/tidb-configuration-file.md#stats-lease)。`stats-lease` 配置项的默认值为 `3s`，如果将其指定为 `0`，TiDB 将停止自动更新统计信息。\n\nTiDB 根据表的变更次数自动调度 [`ANALYZE`](/sql-statements/sql-statement-analyze-table.md) 来收集这些表的统计信息。统计信息的自动更新由 [`tidb_enable_auto_analyze`](/system-variables.md#tidb_enable_auto_analyze-从-v610-版本开始引入) 系统变量和以下 `tidb_auto_analyze%` 变量控制。\n\n|  系统变量名 | 默认值 | 功能描述 |\n| --------- | ----- | --------- |\n| [`tidb_enable_auto_analyze`](/system-variables.md#tidb_enable_auto_analyze-从-v610-版本开始引入) | `ON` | 是否启用自动更新表的统计信息 |\n| [`tidb_auto_analyze_ratio`](/system-variables.md#tidb_auto_analyze_ratio) | `0.5` | 自动更新阈值 |\n| [`tidb_auto_analyze_start_time`](/system-variables.md#tidb_auto_analyze_start_time) | `00:00 +0000` | 一天中能够进行自动更新的开始时间 |\n| [`tidb_auto_analyze_end_time`](/system-variables.md#tidb_auto_analyze_end_time) | `23:59 +0000` | 一天中能够进行自动更新的结束时间 |\n| [`tidb_auto_analyze_partition_batch_size`](/system-variables.md#tidb_auto_analyze_partition_batch_size-从-v640-版本开始引入)   | `128` | TiDB 自动 analyze 分区表（即自动更新分区表的统计信息）时，每次同时 analyze 分区的个数 |\n| [`tidb_enable_auto_analyze_priority_queue`](/system-variables.md#tidb_enable_auto_analyze_priority_queue-从-v800-版本开始引入) | `ON` | 是否启用优先队列来调度自动收集统计信息的任务。开启该变量后，TiDB 会优先收集那些更有收集价值的表，例如新创建的索引、发生分区变更的分区表等。同时，TiDB 也会优先处理那些健康度较低的表，将它们安排在队列的前端。 |\n\n当某个表 `tbl` 的修改行数与总行数的比值大于 `tidb_auto_analyze_ratio`，并且当前时间在 `tidb_auto_analyze_start_time` 和 `tidb_auto_analyze_end_time` 之间时，TiDB 会在后台执行 `ANALYZE TABLE tbl` 语句自动更新这个表的统计信息。\n\n为了避免小表因为少量数据修改而频繁触发自动更新，当表的行数小于 1000 时，TiDB 不会触发对此表的自动更新。你可以通过 `SHOW STATS_META` 语句来查看表的行数。\n\n> **注意：**\n>\n> 目前，自动更新不会记录手动 `ANALYZE` 时输入的配置项。因此，当你使用 [`WITH`](/sql-statements/sql-statement-analyze-table.md) 语法控制 `ANALYZE` 的收集行为时，需要手动设置定时任务来收集统计信息。\n\n### 手动收集\n\n目前 TiDB 收集统计信息为全量收集。你可以通过 `ANALYZE TABLE` 语句的以下语法来全量收集统计信息：\n\n- 收集 `TableNameList` 中所有表的统计信息：\n\n    ```sql\n    ANALYZE TABLE TableNameList [WITH NUM BUCKETS|TOPN|CMSKETCH DEPTH|CMSKETCH WIDTH]|[WITH NUM SAMPLES|WITH FLOATNUM SAMPLERATE];\n    ```\n\n- `WITH NUM BUCKETS` 用于指定生成直方图的桶数量上限。\n- `WITH NUM TOPN` 用于指定生成的 `TOPN` 数量的上限。\n- `WITH NUM CMSKETCH DEPTH` 用于指定 CM Sketch 的长。\n- `WITH NUM CMSKETCH WIDTH` 用于指定 CM Sketch 的宽。\n- `WITH NUM SAMPLES` 用于指定采样的数目。\n- `WITH FLOAT_NUM SAMPLERATE` 用于指定采样率。\n\n`WITH NUM SAMPLES` 与 `WITH FLOAT_NUM SAMPLERATE` 这两种设置对应了两种不同的收集采样的算法。\n\n相关详细解释参见[直方图](#直方图)、[Top-N 值](#top-n) and [CMSketch](#count-min-sketch) (Count-Min Sketch)。关于 `SAMPLES` 和 `SAMPLERATE`，参见[提升统计信息收集性能](#提升统计信息收集性能)。\n\n关于持久化 `ANALYZE` 配置以便后续沿用的更多信息，参见[持久化 `ANALYZE` 配置](#持久化-analyze-配置)。\n\n## 统计信息的类型\n\n本小节介绍统计信息的三种类型：直方图、Count-Min Sketch 和 Top-N。\n\n### 直方图\n\n直方图统计信息被优化器用于估算区间或范围谓词的选择，并可能用于确定列中不同值的数量，以估算 Version 2 统计信息（参见[统计信息版本](#统计信息版本)）中的等值查询或 `IN` 查询的谓词。\n\n直方图是对数据分布的近似表示。它将整个数值范围划分为一系列桶，并使用简单的数据来描述每个桶，例如落入该桶的数值数量。在 TiDB 中，会为每个表的具体列创建等深直方图，可用于估算区间查询。\n\n等深直方图，就是让落入每个桶里的数值数量尽量相等。例如，对于给定的集合 {1.6, 1.9, 1.9, 2.0, 2.4, 2.6, 2.7, 2.7, 2.8, 2.9, 3.4, 3.5} 生成 4 个桶，那么最终的等深直方图就会如下图所示，包含四个桶 [1.6, 1.9]，[2.0, 2.6]，[2.7, 2.8]，[2.9, 3.5]，其桶深均为 3。\n\n![等深直方图示例](/media/statistics-1.png)\n\n你可以通过 `WITH NUM BUCKETS` 参数控制直方图的桶数量上限，参见[手动收集](#手动收集)小节。桶数量越多，直方图的估算精度就越高，不过也会同时增加统计信息的内存使用。可以视具体情况来调整桶的数量上限。\n\n### Count-Min Sketch\n\n> **注意：**\n>\n> Count-Min Sketch 在统计信息 Version 1 仅用于等值查询或 `IN` 查询的谓词估算。在 Version 2 中，为了避免 Count-Min Sketch 可能带来的哈希冲突，TiDB 不再使用 Count-Min Sketch 统计信息，而是使用直方图估算等值查询或 `IN` 查询的谓词。\n\nCount-Min Sketch 是一种哈希结构，当处理等值查询（如 `a = 1`）或者 `IN` 查询（如 `a in (1, 2, 3)`）时，TiDB 便会使用这种数据结构来进行估算。\n\n由于 Count-Min Sketch 是一个哈希结构，就有出现哈希冲突的可能。当在 `EXPLAIN` 语句中发现等值查询的估算偏离实际值较大时，就可以认为是一个比较大的值和一个比较小的值被哈希到了一起。这时有以下两种方法来避免哈希冲突：\n\n- 修改 `WITH NUM TOPN` 参数。TiDB 会将出现频率前 x 的数据单独储存，之后的数据再储存到 Count-Min Sketch 中。因此，为了避免一个比较大的值和一个比较小的值被哈希到一起，可以调大 `WITH NUM TOPN` 的值。该参数的默认值是 `20`，最大值是 `1024`。关于该参数的更多信息，参见[手动收集](#手动收集)小节。\n- 修改 `WITH NUM CMSKETCH DEPTH` 和 `WITH NUM CMSKETCH WIDTH` 两个参数。这两个参数会影响哈希的桶数和冲突概率，可视具体情况适当调大这两个参数的值来减少冲突概率，不过调大后也会增加统计信息的内存使用。`WITH NUM CMSKETCH DEPTH` 的默认值是 `5`，`WITH NUM CMSKETCH WIDTH` 的默认值是 `2048`。关于这两个参数的更多信息，参见[手动收集](#手动收集)小节。\n\n### Top-N\n\nTop-N 值是列或索引中出现次数前 N 的值。Top-N 统计信息通常被称为频率统计信息或数据倾斜。\n\nTiDB 会记录 Top-N 的值和出现次数。参数 `WITH NUM TOPN` 控制 Top-N 值的数量，默认值是 `20`，表示收集出现频率最高的前 20 个值；最大值是 `1024`。关于该参数的详细信息，参见[手动收集](#手动收集)小节。\n\n## 选择性收集统计信息\n\n本小节介绍如何选择性地收集统计信息。\n\n### 收集索引的统计信息\n\n如果要收集 `TableName` 中 `IndexNameList` 里所有索引的统计信息，请使用以下语法：\n\n```sql\nANALYZE TABLE TableName INDEX [IndexNameList] [WITH NUM BUCKETS|TOPN|CMSKETCH DEPTH|CMSKETCH WIDTH]|[WITH NUM SAMPLES|WITH FLOATNUM SAMPLERATE];\n```\n\n当 `IndexNameList` 为空时，该语法将收集 `TableName` 中所有索引的统计信息。\n\n> **注意：**\n>\n> 为了保证收集前与收集后统计信息的一致性，当设置 `tidb_analyze_version = 2` 时，以上语法会收集表中索引列的统计信息和所有索引的统计信息。\n\n### 收集部分列的统计信息\n\n当 TiDB 执行 SQL 语句时，优化器在大多数情况下只会用到部分列的统计信息。例如，`WHERE`、`JOIN`、`ORDER BY`、`GROUP BY` 子句中出现的列，这些被用到的列称为 `PREDICATE COLUMNS`。\n\n如果一个表有很多列，收集所有列的统计信息会产生较大的开销。为了降低开销，你可以只收集选定列或者 `PREDICATE COLUMNS` 的统计信息供优化器使用。如果要持久化列配置以便将来沿用，参见[持久化列配置](#持久化列配置)。\n\n> **注意：**\n>\n> - 收集 `PREDICATE COLUMNS` 的统计信息的功能仅适用于 [`tidb_analyze_version = 2`](/system-variables.md#tidb_analyze_version-从-v510-版本开始引入) 的情况。\n> - TiDB v7.2.0 引入了系统变量 [`tidb_analyze_skip_column_types`](/system-variables.md#tidb_analyze_skip_column_types-从-v720-版本开始引入)，该变量可以控制在执行 `ANALYZE` 命令收集统计信息时，跳过哪些类型的列的统计信息收集。该变量仅适用于 `tidb_analyze_version = 2` 的情况。\n\n- 如果要收集指定列的统计信息，请使用以下语法：\n\n    ```sql\n    ANALYZE TABLE TableName COLUMNS ColumnNameList [WITH NUM BUCKETS|TOPN|CMSKETCH DEPTH|CMSKETCH WIDTH]|[WITH NUM SAMPLES|WITH FLOATNUM SAMPLERATE];\n    ```\n\n    其中，`ColumnNameList` 表示指定列的名称列表。如果需要指定多列，请使用用逗号 `,` 分隔列名。例如, `ANALYZE table t columns a, b`。该语法除了收集指定表中指定列的统计信息，将同时收集该表中索引列的统计信息以及所有索引的统计信息。\n\n- 如果要收集 `PREDICATE COLUMNS` 的统计信息，请使用以下语法：\n\n    ```sql\n    ANALYZE TABLE TableName PREDICATE COLUMNS [WITH NUM BUCKETS|TOPN|CMSKETCH DEPTH|CMSKETCH WIDTH]|[WITH NUM SAMPLES|WITH FLOATNUM SAMPLERATE];\n    ```\n\n    TiDB 将每隔 100 * [`stats-lease`](/tidb-configuration-file.md#stats-lease) 的时间将 `PREDICATE COLUMNS` 信息写入系统表 [`mysql.column_stats_usage`](/mysql-schema/mysql-schema.md#统计信息相关系统表)。\n\n    以上语法除了收集指定表中 `PREDICATE COLUMNS` 的统计信息之外，将同时收集该表中索引列的统计信息以及所有索引的统计信息。\n\n    > **注意：**\n    >\n    > - 如果系统表 [`mysql.column_stats_usage`](/mysql-schema/mysql-schema.md#统计信息相关系统表) 中没有关于该表的 `PREDICATE COLUMNS` 记录，执行以上语句会收集该表中索引列的统计信息以及所有索引的统计信息。\n    > - 对于任何被排除在此次统计信息收集（无论是手动列出列名，还是使用 `PREDICATE COLUMNS`）之外的列，它们的统计信息不会被覆盖。当执行新类型的 SQL 查询时，如果存在旧的统计信息，优化器将使用这些列的旧统计信息；如果从未收集过列的统计信息，则使用伪列统计信息。下一次使用 `PREDICATE COLUMNS` 的 `ANALYZE` 将收集这些列的统计信息。\n\n- 如果要收集所有列的统计信息以及所有索引的统计信息，请使用以下语法：\n\n    ```sql\n    ANALYZE TABLE TableName ALL COLUMNS [WITH NUM BUCKETS|TOPN|CMSKETCH DEPTH|CMSKETCH WIDTH]|[WITH NUM SAMPLES|WITH FLOATNUM SAMPLERATE];\n    ```\n\n### 收集分区的统计信息\n\n- 如果要收集 `TableName` 中 `PartitionNameList` 里所有分区的统计信息，请使用以下语法：\n\n    ```sql\n    ANALYZE TABLE TableName PARTITION PartitionNameList [WITH NUM BUCKETS|TOPN|CMSKETCH DEPTH|CMSKETCH WIDTH]|[WITH NUM SAMPLES|WITH FLOATNUM SAMPLERATE];\n    ```\n\n- 如果要收集 `TableName` 中 `PartitionNameList` 里所有分区的索引统计信息，请使用以下语法：\n\n    ```sql\n    ANALYZE TABLE TableName PARTITION PartitionNameList INDEX [IndexNameList] [WITH NUM BUCKETS|TOPN|CMSKETCH DEPTH|CMSKETCH WIDTH]|[WITH NUM SAMPLES|WITH FLOATNUM SAMPLERATE];\n    ```\n\n- 当收集分区的统计信息时，如果只需要[收集部分列的统计信息](/statistics.md#收集部分列的统计信息)，请使用以下语法：\n\n    > **警告：**\n    >\n    > 收集 `PREDICATE COLUMNS` 的统计信息目前为实验特性，不建议在生产环境中使用。\n\n    ```sql\n    ANALYZE TABLE TableName PARTITION PartitionNameList [COLUMNS ColumnNameList|PREDICATE COLUMNS|ALL COLUMNS] [WITH NUM BUCKETS|TOPN|CMSKETCH DEPTH|CMSKETCH WIDTH]|[WITH NUM SAMPLES|WITH FLOATNUM SAMPLERATE];\n    ```\n\n#### 收集动态裁剪模式下的分区表统计信息\n\n在分区表开启[动态裁剪模式](/partitioned-table.md#动态裁剪模式)（从 v6.3.0 开始，默认开启）的情况下，TiDB 将收集表级别的汇总统计信息，即分区表的全局统计信息。分区表的全局统计信息合并汇总了所有分区的统计信息。在动态裁剪模式下，表中任何分区的统计信息更新都可能触发该表全局统计信息的更新。\n\n如果某些分区的统计信息为空，或者某些分区中列的统计信息有缺失，那么统计信息收集行为将受 [`tidb_skip_missing_partition_stats`](/system-variables.md#tidb_skip_missing_partition_stats-从-v730-版本开始引入) 变量的控制：\n\n- 当触发全局统计信息更新且 [`tidb_skip_missing_partition_stats`](/system-variables.md#tidb_skip_missing_partition_stats-从-v730-版本开始引入) 为 `OFF` 时：\n\n    - 如果某些分区缺失统计信息（例如从未进行过 analyze 的新分区），全局统计信息生成会中断，并显示 warning 信息提示这些分区没有可用的统计信息。\n    - 如果某些分区中缺失某些列的统计信息（这些分区中指定了不同的列进行 analyze），当这些列的统计信息被合并汇总时，全局统计信息生成会中断，并显示 warning 信息提示某些分区中缺少某些列的统计信息。\n\n- 当触发全局统计信息更新且 [`tidb_skip_missing_partition_stats`](/system-variables.md#tidb_skip_missing_partition_stats-从-v730-版本开始引入) 为 `ON` 时：\n\n    - 如果某些分区缺失全部列或部分列的统计信息，TiDB 在生成全局统计信息时会跳过这些缺失的分区统计信息，不影响全局统计信息的生成。\n\n在动态裁剪模式下，分区和分区表的 `ANALYZE` 配置应保持一致。因此，如果在 `ANALYZE TABLE TableName PARTITION PartitionNameList` 语句后指定了 `COLUMNS` 配置或在 `WITH` 后指定了 `OPTIONS` 配置，TiDB 将忽略这些配置并返回 warning 信息提示。\n\n## 提升统计信息收集性能\n\n> **注意：**\n>\n> 在 TiDB 中执行 `ANALYZE TABLE` 语句可能比在 MySQL 或 InnoDB 中耗时更长。InnoDB 采样的只是少量页面，而 TiDB 默认会完全重构一套全面的统计信息。\n\nTiDB 提供了两种方法来提升统计信息收集的性能：\n\n- 收集列的子集的统计信息。参见[收集部分列的统计信息](#收集部分列的统计信息)。\n- 采样。参见[统计信息采样](#统计信息采样)。\n\n### 统计信息采样\n\n采样是通过 `ANALYZE` 语句的两个选项来实现的，每个选项对应一种不同的收集算法：\n\n- `WITH NUM SAMPLES` 指定了采样集的大小，在 TiDB 中是以蓄水池采样的方式实现。当表较大时，不推荐使用这种方式收集统计信息。因为蓄水池采样中间结果集会产生一定的冗余结果，会对内存等资源造成额外的压力。\n- `WITH FLOAT_NUM SAMPLERATE` 是从 v5.3.0 开始引入的采样方式，指定了采样率的大小，取值范围是 `(0, 1]`。在 TiDB 中是以伯努利采样的方式实现，更适合对较大的表进行采样，在收集效率和资源使用上更有优势。\n\n在 v5.3.0 之前，TiDB 采用蓄水池采样的方式收集统计信息。自 v5.3.0 版本起，TiDB Version 2 的统计信息默认会选取伯努利采样的方式收集统计信息。若要重新使用蓄水池采样的方式采样，可以使用 `WITH NUM SAMPLES` 语句。\n\n目前采样率基于自适应算法进行计算。当你通过 [`SHOW STATS_META`](/sql-statements/sql-statement-show-stats-meta.md) 可以观察到一个表的行数时，可通过这个行数去计算采集 10 万行所对应的采样率。如果你观察不到这个值，可通过表 [`SHOW TABLE REGIONS`](/sql-statements/sql-statement-show-table-regions.md) 结果中所有 `APPROXIMATE_KEYS` 列值的总和作为另一个参考来计算采样率。\n\n> **注意：**\n>\n> 通常情况下，`STATS_META` 比 `APPROXIMATE_KEYS` 更可信。但是，当 `STATS_META` 的结果远小于 `APPROXIMATE_KEYS` 的结果时，推荐使用 `APPROXIMATE_KEYS` 计算采样率。\n\n### 统计信息收集的内存限制\n\n> **警告：**\n>\n> 目前限制 `ANALYZE` 的内存使用量为实验特性，在生产环境中使用时可能存在内存统计有误差的情况。\n\nTiDB 从 v6.1.0 开始引入了统计信息收集的内存限制，你可以通过 [`tidb_mem_quota_analyze`](/system-variables.md#tidb_mem_quota_analyze-从-v610-版本开始引入) 变量来控制 TiDB 更新统计信息时的最大总内存占用。\n\n要合理地配置 `tidb_mem_quota_analyze` 的值，你需要考虑集群的数据规模。在使用默认采样率的情况下，主要考虑列的数量、列值的大小，以及 TiDB 的内存配置。你可参考以下建议来配置该变量的最大值和最小值：\n\n> **注意：**\n>\n> 以下配置建议仅供参考，实际配置需要在真实场景中测试确定。\n\n- 最小值：需要大于 TiDB 从集群上列最多的表收集统计信息时使用的最大内存。一个粗略的参考信息是，在测试集上，20 列的表在默认配置下，统计信息收集的最大内存使用量约为 800 MiB；160 列的表在默认配置下，统计信息收集的最大内存使用量约为 5 GiB。\n- 最大值：需要小于集群在不进行统计信息收集时的内存空余量。\n\n## 持久化 `ANALYZE` 配置\n\n从 v5.4.0 起，TiDB 支持 `ANALYZE` 配置持久化，方便后续收集统计信息时沿用已有配置。\n\nTiDB 支持以下 `ANALYZE` 配置的持久化：\n\n| 配置 | 对应的 `ANALYZE` 语法 |\n| --- | --- |\n| 直方图桶数 | `WITH NUM BUCKETS` |\n| TopN 个数 | `WITH NUM TOPN` |\n| 采样数 | `WITH NUM SAMPLES` |\n| 采样率 | `WITH FLOATNUM SAMPLERATE` |\n| `ANALYZE` 的列的类型 | AnalyzeColumnOption ::= ( 'ALL COLUMNS' \\| 'PREDICATE COLUMNS' \\| 'COLUMNS' ColumnNameList ) |\n| `ANALYZE` 的列 | ColumnNameList ::= Identifier ( ',' Identifier )* |\n\n### 开启 `ANALYZE` 配置持久化\n\n`ANALYZE` 配置持久化功能默认开启，即系统变量 `tidb_analyze_version` 为默认值 `2`，`tidb_persist_analyze_options` 为默认值 `ON`。\n\n`ANALYZE` 配置持久化功能可用于记录手动执行 `ANALYZE` 语句时指定的持久化配置。记录后，当 TiDB 下一次自动更新统计信息或者你手动收集统计信息但未指定配置时，TiDB 会按照记录的配置收集统计信息。\n\n如果要查询某张表上的持久化配置用于自动更新统计信息，使用以下 SQL 语句：\n\n```sql\nSELECT sample_num, sample_rate, buckets, topn, column_choice, column_ids FROM mysql.analyze_options opt JOIN information_schema.tables tbl ON opt.table_id = tbl.tidb_table_id WHERE tbl.table_schema = '{db_name}' AND tbl.table_name = '{table_name}';\n```\n\nTiDB 会使用最新的 `ANALYZE` 语句中指定的配置覆盖先前记录的持久化配置。例如，如果你运行 `ANALYZE TABLE t WITH 200 TOPN;` ，它将在 `ANALYZE` 语句中设置前 200 个值。随后，执行 `ANALYZE TABLE t WITH 0.1 SAMPLERATE;` 将为自动 `ANALYZE` 语句同时设置前 200 个值和 0.1 的采样率，类似于 `ANALYZE TABLE t WITH 200 TOPN, 0.1 SAMPLERATE;`。\n\n### 关闭 `ANALYZE` 配置持久化\n\n如果要关闭 `ANALYZE` 配置持久化功能，请将系统变量 `tidb_persist_analyze_options` 设置为 `OFF`。此外，由于 `ANALYZE` 配置持久化功能在 `tidb_analyze_version = 1` 的情况下不适用，因此设置 `tidb_analyze_version = 1` 同样会达到关闭配置持久化的效果。\n\n关闭 `ANALYZE` 配置持久化功能后，已持久化的配置记录不会被清除。因此，当再次开启该功能时，TiDB 会继续使用之前记录的持久化配置收集统计信息。\n\n> **注意：**\n>\n> 当再次开启 `ANALYZE` 配置持久化功能时，如果之前记录的持久化配置项已经不适用当前的数据，请手动执行 `ANALYZE` 语句并指定新的持久化配置。\n\n### 持久化列配置\n\n如果要持久化 `ANALYZE` 语句中列的配置（包括 `COLUMNS ColumnNameList`、`PREDICATE COLUMNS`、`ALL COLUMNS`），请将系统变量 [`tidb_persist_analyze_options`](/system-variables.md#tidb_persist_analyze_options-从-v540-版本开始引入) 的值设置为 `ON`，以开启[持久化 `ANALYZE` 配置](/statistics.md#持久化-analyze-配置)功能。开启 `ANALYZE` 配置持久化之后：\n\n- 当 TiDB 自动收集统计信息或者你手动执行 `ANALYZE` 语句收集统计信息但未指定列的配置时，TiDB 会继续沿用之前持久化的配置。\n- 当多次手动执行 `ANALYZE` 语句并指定列的配置时，TiDB 会使用最新一次 `ANALYZE` 指定的配置项覆盖上一次记录的持久化配置。\n\n如果要查看一个表中哪些列是 `PREDICATE COLUMNS`、哪些列的统计信息已经被收集，请使用 [`SHOW COLUMN_STATS_USAGE`](/sql-statements/sql-statement-show-column-stats-usage.md) 语句。\n\n在以下示例中，执行 `ANALYZE TABLE t PREDICATE COLUMNS;` 后，TiDB 将收集 `b`、`c`、`d` 列的统计信息，其中 `b` 列是 `PREDICATE COLUMN`，`c` 列和 `d` 列是索引列。\n\n```sql\nCREATE TABLE t (a INT, b INT, c INT, d INT, INDEX idx_c_d(c, d));\nQuery OK, 0 rows affected (0.00 sec)\n\n-- 在此查询中优化器用到了 b 列的统计信息。\nSELECT * FROM t WHERE b > 1;\nEmpty set (0.00 sec)\n\n-- 等待一段时间（100 * stats-lease）后，TiDB 将收集的 `PREDICATE COLUMNS` 写入 mysql.column_stats_usage。\n-- 指定 `last_used_at IS NOT NULL` 表示显示 TiDB 收集到的 `PREDICATE COLUMNS`。\nSHOW COLUMN_STATS_USAGE WHERE db_name = 'test' AND table_name = 't' AND last_used_at IS NOT NULL;\n+---------+------------+----------------+-------------+---------------------+------------------+\n| Db_name | Table_name | Partition_name | Column_name | Last_used_at        | Last_analyzed_at |\n+---------+------------+----------------+-------------+---------------------+------------------+\n| test    | t          |                | b           | 2022-01-05 17:21:33 | NULL             |\n+---------+------------+----------------+-------------+---------------------+------------------+\n1 row in set (0.00 sec)\n\nANALYZE TABLE t PREDICATE COLUMNS;\nQuery OK, 0 rows affected, 1 warning (0.03 sec)\n\n-- 指定 `last_analyzed_at IS NOT NULL` 表示显示收集过统计信息的列。\nSHOW COLUMN_STATS_USAGE WHERE db_name = 'test' AND table_name = 't' AND last_analyzed_at IS NOT NULL;\n+---------+------------+----------------+-------------+---------------------+---------------------+\n| Db_name | Table_name | Partition_name | Column_name | Last_used_at        | Last_analyzed_at    |\n+---------+------------+----------------+-------------+---------------------+---------------------+\n| test    | t          |                | b           | 2022-01-05 17:21:33 | 2022-01-05 17:23:06 |\n| test    | t          |                | c           | NULL                | 2022-01-05 17:23:06 |\n| test    | t          |                | d           | NULL                | 2022-01-05 17:23:06 |\n+---------+------------+----------------+-------------+---------------------+---------------------+\n3 rows in set (0.00 sec)\n```\n\n## 统计信息版本\n\n系统变量 [`tidb_analyze_version`](/system-variables.md#tidb_analyze_version-从-v510-版本开始引入) 用于控制 TiDB 收集统计信息的行为。目前 TiDB 支持两个版本的统计信息，即 `tidb_analyze_version = 1` 和 `tidb_analyze_version = 2`。\n\n- 从 v5.3.0 开始，变量 `tidb_analyze_version` 的默认值从 `1` 变为了 `2`。\n- 如果从 v5.3.0 之前版本的集群升级至 v5.3.0 或之后的版本，该变量的默认值不会发生变化。\n<!-- - TiDB Cloud 中，从 v6.5.0 开始，该变量的默认值从 `1` 变为了 `2`。-->\n\n更推荐选择 Version 2。Version 2 将继续增强，并最终完全取代 Version 1。与 Version 1 相比，Version 2 提高了大数据量场景下多项统计信息收集的准确性。此外，Version 2 在进行谓词选择率估算时不再需要收集 Count-Min Sketch 统计信息，并支持仅对选定列进行自动收集（参见[收集部分列的统计信息](#收集部分列的统计信息)），从而提高了收集性能。\n\n以下表格列出了两个统计信息版本为优化器估算收集的信息：\n\n| 信息 | Version 1 | Version 2|\n| --- | --- | ---|\n| 表的总行数 | ⎷ | ⎷ |\n| 等值查询或 `IN` 查询的谓词估算 | ⎷（列/索引 Top-N & Count-Min Sketch） | ⎷（列/索引 Top-N & 直方图） |\n| Range 范围谓词估算 | ⎷（列/索引 Top-N & 直方图） | ⎷（列/索引 Top-N & 直方图） |\n| `NULL` 谓词估算 | ⎷ | ⎷ |\n| 列的平均长度 | ⎷ | ⎷ |\n| 索引的平均长度 | ⎷ | ⎷ |\n\n### 切换统计信息版本\n\n建议确保所有表、索引（和分区）使用相同版本的统计信息收集功能。推荐使用 Version 2，但不建议在没有正当理由（例如使用中的版本出现问题）的情况下切换版本。版本之间的切换可能需要一段时间，在此期间可能没有统计信息，直到所有表都使用了新版本进行统计。如果没有统计信息，可能会影响优化器的计划选择。\n\n切换版本的正当理由可能包括：使用 Version 1 在收集 Count-Min Sketch 统计信息时，由于哈希冲突导致等值查询或 `IN` 查询谓词估算不准确。此时，你可以参考 [Count-Min Sketch](#count-min-sketch) 小节中描述的解决方案，或者设置 `tidb_analyze_version = 2` 并对所有对象重新运行 `ANALYZE`。在 Version 2 的早期阶段，执行 `ANALYZE` 后有内存溢出的风险，现在这个问题已经解决，但最初的解决方案是设置 `tidb_analyze_version = 1` 并对所有对象重新运行 `ANALYZE`。\n\n要为切换统计信息版本做好 `ANALYZE` 准备，请根据情况进行以下操作：\n\n- 如果 `ANALYZE` 语句是手动执行的，请手动统计每张需要统计的表：\n\n    ```sql\n    SELECT DISTINCT(CONCAT('ANALYZE TABLE ', table_schema, '.', table_name, ';'))\n    FROM information_schema.tables JOIN mysql.stats_histograms\n    ON table_id = tidb_table_id\n    WHERE stats_ver = 2;\n    ```\n\n- 如果 `ANALYZE` 语句是由 TiDB 自动执行的（当开启自动更新统计信息时），请执行以下语句生成 [`DROP STATS`](/sql-statements/sql-statement-drop-stats.md) 语句：\n\n    ```sql\n    SELECT DISTINCT(CONCAT('DROP STATS ', table_schema, '.', table_name, ';'))\n    FROM information_schema.tables ON mysql.stats_histograms\n    ON table_id = tidb_table_id\n    WHERE stats_ver = 2;\n    ```\n\n- 如果上一条语句的返回结果太长，不方便复制粘贴，可以将结果导出到临时文件后，再执行：\n\n    ```sql\n    SELECT DISTINCT ... INTO OUTFILE '/tmp/sql.txt';\n    mysql -h ${TiDB_IP} -u user -P ${TIDB_PORT} ... < '/tmp/sql.txt'\n    ```\n\n## 查看统计信息\n\n你可以使用一些 SQL 语句来查看 `ANALYZE` 的状态和统计信息的情况。\n\n### `ANALYZE` 状态\n\n在执行 `ANALYZE` 语句时，可以使用 [`SHOW ANALYZE STATUS`](/sql-statements/sql-statement-show-analyze-status.md) 语句来查看当前 `ANALYZE` 的状态。\n\n从 TiDB v6.1.0 起，执行 `SHOW ANALYZE STATUS` 语句将显示集群级别的任务，且 TiDB 重启后仍能看到重启之前的任务记录。在 TiDB v6.1.0 之前，执行 `SHOW ANALYZE STATUS` 语句仅显示实例级别的任务，且 TiDB 重启后任务记录会被清空。\n\n`SHOW ANALYZE STATUS` 仅显示最近的任务记录。从 TiDB v6.1.0 起，你可以通过系统表 `mysql.analyze_jobs` 查看过去 7 天内的历史记录。\n\n当设置了系统变量 [`tidb_mem_quota_analyze`](/system-variables.md#tidb_mem_quota_analyze-从-v610-版本开始引入) 且 TiDB 后台的统计信息自动更新任务的内存占用超过了这个阈值时，自动更新任务会重试。失败的任务和重试的任务都可以在 `SHOW ANALYZE STATUS` 语句的执行结果中查看。\n\n当 [`tidb_max_auto_analyze_time`](/system-variables.md#tidb_max_auto_analyze_time-从-v610-版本开始引入) 大于 `0` 时，如果后台统计信息自动更新任务的执行时间超过这个阈值，该任务会被终止。\n\n语法如下：\n\n```sql\nSHOW ANALYZE STATUS [ShowLikeOrWhere];\n```\n\n```\n+--------------+------------+----------------+-------------------------------------------------------------------------------------------+----------------+---------------------+---------------------+----------+-------------------------------------------------------------------------------|\n| Table_schema | Table_name | Partition_name | Job_info                                                                                  | Processed_rows | Start_time          | End_time            | State    | Fail_reason                                                                   |\n+--------------+------------+----------------+-------------------------------------------------------------------------------------------+----------------+---------------------+---------------------+----------+-------------------------------------------------------------------------------|\n| test         | sbtest1    |                | retry auto analyze table all columns with 100 topn, 0.055 samplerate                      |        2000000 | 2022-05-07 16:41:09 | 2022-05-07 16:41:20 | finished | NULL                                                                          |\n| test         | sbtest1    |                | auto analyze table all columns with 100 topn, 0.5 samplerate                              |              0 | 2022-05-07 16:40:50 | 2022-05-07 16:41:09 | failed   | analyze panic due to memory quota exceeds, please try with smaller samplerate |\n```\n\n### 表的元信息\n\n你可以使用 [`SHOW STATS_META`](/sql-statements/sql-statement-show-stats-meta.md) 语句来查看表的总行数以及修改的行数等信息。\n\n### 表的健康度信息\n\n你可以使用 [`SHOW STATS_HEALTHY`](/sql-statements/sql-statement-show-stats-healthy.md) 语句查看表的统计信息健康度，并粗略估计表上统计信息的准确度。当 `modify_count` >= `row_count` 时，健康度为 0；当 `modify_count` < `row_count` 时，健康度为 (1 - `modify_count`/`row_count`) * 100。\n\n### 列的元信息\n\n你可以使用 [`SHOW STATS_HISTOGRAMS`](/sql-statements/sql-statement-show-stats-histograms.md) 语句查看列的不同值数量以及 `NULL` 数量等信息。\n\n### 直方图桶的信息\n\n你可以使用 [`SHOW STATS_BUCKETS`](/sql-statements/sql-statement-show-stats-buckets.md) 语句查看直方图每个桶的信息。\n\n### Top-N 信息\n\n你可以使用 [`SHOW STATS_TOPN`](/sql-statements/sql-statement-show-stats-topn.md) 语句查看当前 TiDB 收集的 Top-N 值的信息。\n\n## 删除统计信息\n\n你可以通过执行 [`DROP STATS`](/sql-statements/sql-statement-drop-stats.md) 语句来删除统计信息。\n\n## 加载统计信息\n\n默认情况下，列的统计信息占用空间大小不同，TiDB 对统计信息的加载方式也会不同：\n\n- 对于 count、distinctCount、nullCount 等占用空间较小的统计信息，只要有数据更新，TiDB 就会自动将对应的统计信息加载进内存供 SQL 优化阶段使用。\n- 对于直方图、TopN、CMSketch 等占用空间较大的统计信息，为了确保 SQL 执行的性能，TiDB 会按需进行异步加载。例如，对于直方图，只有当某条 SQL 语句的优化阶段使用到了某列的直方图统计信息时，TiDB 才会将该列的直方图信息加载到内存。按需异步加载的优势是统计信息加载不会影响到 SQL 执行的性能，但在 SQL 优化时有可能使用不完整的统计信息。\n\n从 v5.4.0 开始，TiDB 引入了统计信息同步加载的特性，支持执行当前 SQL 语句时将直方图、TopN、CMSketch 等占用空间较大的统计信息同步加载到内存，提高该 SQL 语句优化时统计信息的完整性。\n\n要开启该特性，请将系统变量 [`tidb_stats_load_sync_wait`](/system-variables.md#tidb_stats_load_sync_wait-从-v540-版本开始引入) 的值设置为 SQL 优化可以等待的同步加载完整的列统计信息的最长超时时间（单位为毫秒）。该变量的默认值为 `100`，代表开启统计信息同步加载。\n\n开启同步加载统计信息特性后，你可以进一步配置该特性：\n\n- 通过修改系统变量 [`tidb_stats_load_pseudo_timeout`](/system-variables.md#tidb_stats_load_pseudo_timeout-从-v540-版本开始引入) 的值控制 SQL 优化等待超时后 TiDB 的行为。该变量默认值为 `ON`，表示超时后 SQL 优化过程不会使用任何列上的直方图、TopN 或 CMSketch。当该变量设置为 `OFF` 时，表示超时后 SQL 执行失败。\n- 通过修改 TiDB 配置项 [`stats-load-concurrency`](/tidb-configuration-file.md#stats-load-concurrency-从-v540-版本开始引入) 的值控制统计信息同步加载可以并发处理的最大列数。该配置项的默认值为 `5`。\n- 通过修改 TiDB 配置项 [`stats-load-queue-size`](/tidb-configuration-file.md#stats-load-queue-size-从-v540-版本开始引入) 的值设置统计信息同步加载最多可以缓存多少列的请求。该配置项的默认值为 `1000`。\n\n在 TiDB 启动阶段，初始统计信息加载完成之前执行的 SQL 可能有不合理的执行计划，从而影响性能。为了避免这种情况，从 v7.1.0 开始，TiDB 引入了配置参数 [`force-init-stats`](/tidb-configuration-file.md#force-init-stats-从-v657-和-v710-版本开始引入)。你可以使用该配置参数控制 TiDB 启动时是否在统计信息初始化完成后再对外提供服务。该配置参数从 v7.2.0 起默认开启。\n\n从 v7.1.0 开始，TiDB 引入了配置参数 [`lite-init-stats`](/tidb-configuration-file.md#lite-init-stats-从-v710-版本开始引入)，用于控制是否开启轻量级的统计信息初始化。\n\n- 当 `lite-init-stats` 设置为 `true` 时，统计信息初始化时列和索引的直方图、TopN、Count-Min Sketch 均不会加载到内存中。\n- 当 `lite-init-stats` 设置为 `false` 时，统计信息初始化时索引和主键的直方图、TopN、Count-Min Sketch 会被加载到内存中，非主键列的直方图、TopN、Count-Min Sketch 不会加载到内存中。当优化器需要某一索引或者列的直方图、TopN、Count-Min Sketch 时，这些统计信息会被同步或异步加载到内存中。\n\n`lite-init-stats` 的默认值为 `true`，即开启轻量级的统计信息初始化。将 `lite-init-stats` 设置为 `true` 可以加速统计信息初始化，避免加载不必要的统计信息，从而减少 TiDB 的内存使用。\n\n## 导出和导入统计信息\n\n本小节介绍如何导出和导入统计信息。\n\n### 导出统计信息\n\n统计信息的导出接口如下：\n\n+ 通过以下接口可以获取数据库 `${db_name}` 中的表 `${table_name}` 的 JSON 格式的统计信息：\n\n    ```\n    http://${tidb-server-ip}:${tidb-server-status-port}/stats/dump/${db_name}/${table_name}\n    ```\n\n    示例如下：\n\n    ```\n    curl -s http://127.0.0.1:10080/stats/dump/test/t1 -o /tmp/t1.json\n    ```\n\n+ 通过以下接口可以获取数据库 `${db_name}` 中的表 `${table_name}` 在指定时间上的 JSON 格式的统计信息。指定的时间应在 GC SafePoint 之后。\n\n    ```\n    http://${tidb-server-ip}:${tidb-server-status-port}/stats/dump/${db_name}/${table_name}/${yyyyMMddHHmmss}\n    ```\n\n### 导入统计信息\n\n> **注意：**\n>\n> 启动 MySQL 客户端时，请使用 `--local-infile=1` 参数。\n\n导入的统计信息一般指通过统计信息导出接口得到的 JSON 文件。你可以使用 [`LOAD STATS`](/sql-statements/sql-statement-load-stats.md) 语句来导入统计信息。\n\n语法如下：\n\n```sql\nLOAD STATS 'file_name';\n```\n\n`file_name` 为要导入的统计信息的文件名。\n\n## 锁定统计信息\n\n从 v6.5.0 开始，TiDB 支持锁定统计信息。当一张表或一个分区的统计信息被锁定以后，该表或分区的统计信息将无法被修改，也无法对该表进行 `ANALYZE` 操作。示例如下：\n\n创建表 `t`，并插入一些数据。在未锁定表 `t` 的统计信息时，可以成功执行 `ANALYZE` 语句：\n\n```sql\nmysql> CREATE TABLE t(a INT, b INT);\nQuery OK, 0 rows affected (0.03 sec)\n\nmysql> INSERT INTO t VALUES (1,2), (3,4), (5,6), (7,8);\nQuery OK, 4 rows affected (0.00 sec)\nRecords: 4  Duplicates: 0  Warnings: 0\n\nmysql> ANALYZE TABLE t;\nQuery OK, 0 rows affected, 1 warning (0.02 sec)\n\nmysql> SHOW WARNINGS;\n+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Level | Code | Message                                                                                                                                                                                                               |\n+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Note  | 1105 | Analyze use auto adjusted sample rate 1.000000 for table test.t, reason to use this rate is \"Row count in stats_meta is much smaller compared with the row count got by PD, use min(1, 15000/4) as the sample-rate=1\" |\n+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n锁定表 `t` 的统计信息，再执行 `ANALYZE` 语句，warning 提示跳过对表 `t` 的 `ANALYZE`：\n\n```sql\nmysql> LOCK STATS t;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SHOW STATS_LOCKED;\n+---------+------------+----------------+--------+\n| Db_name | Table_name | Partition_name | Status |\n+---------+------------+----------------+--------+\n| test    | t          |                | locked |\n+---------+------------+----------------+--------+\n1 row in set (0.01 sec)\n\nmysql> ANALYZE TABLE t;\nQuery OK, 0 rows affected, 2 warnings (0.00 sec)\n\nmysql> SHOW WARNINGS;\n+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                                                                 |\n+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------+\n| Note    | 1105 | Analyze use auto adjusted sample rate 1.000000 for table test.t, reason to use this rate is \"use min(1, 110000/8) as the sample-rate=1\" |\n| Warning | 1105 | skip analyze locked table: test.t                                                                                                       |\n+---------+------+-----------------------------------------------------------------------------------------------------------------------------------------+\n2 rows in set (0.00 sec)\n```\n\n解锁表 `t` 的统计信息，可以成功执行 `ANALYZE` 语句：\n\n```sql\nmysql> UNLOCK STATS t;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> ANALYZE TABLE t;\nQuery OK, 0 rows affected, 1 warning (0.03 sec)\n\nmysql> SHOW WARNINGS;\n+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------+\n| Level | Code | Message                                                                                                                                 |\n+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------+\n| Note  | 1105 | Analyze use auto adjusted sample rate 1.000000 for table test.t, reason to use this rate is \"use min(1, 110000/8) as the sample-rate=1\" |\n+-------+------+-----------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n另外，你也可以通过 `LOCK STATS` 语句锁定分区的统计信息。示例如下：\n\n创建分区表 `t`，并插入一些数据。在未锁定分区 `p1` 的统计信息时，可以成功执行 `ANALYZE` 语句：\n\n```sql\nmysql> CREATE TABLE t(a INT, b INT) PARTITION BY RANGE (a) (PARTITION p0 VALUES LESS THAN (10), PARTITION p1 VALUES LESS THAN (20), PARTITION p2 VALUES LESS THAN (30));\nQuery OK, 0 rows affected (0.03 sec)\n\nmysql> INSERT INTO t VALUES (1,2), (3,4), (5,6), (7,8);\nQuery OK, 4 rows affected (0.00 sec)\nRecords: 4  Duplicates: 0  Warnings: 0\n\nmysql> ANALYZE TABLE t;\nQuery OK, 0 rows affected, 6 warning (0.02 sec)\n\nmysql> SHOW WARNINGS;\n+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                                                                                                                                                              |\n+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Warning | 1105 | disable dynamic pruning due to t has no global stats                                                                                                                                                                                 |\n| Note    | 1105 | Analyze use auto adjusted sample rate 1.000000 for table test.t's partition p0, reason to use this rate is \"Row count in stats_meta is much smaller compared with the row count got by PD, use min(1, 15000/4) as the sample-rate=1\" |\n| Warning | 1105 | disable dynamic pruning due to t has no global stats                                                                                                                                                                                 |\n| Note    | 1105 | Analyze use auto adjusted sample rate 1.000000 for table test.t's partition p1, reason to use this rate is \"TiDB assumes that the table is empty, use sample-rate=1\"                                                                 |\n| Warning | 1105 | disable dynamic pruning due to t has no global stats                                                                                                                                                                                 |\n| Note    | 1105 | Analyze use auto adjusted sample rate 1.000000 for table test.t's partition p2, reason to use this rate is \"TiDB assumes that the table is empty, use sample-rate=1\"                                                                 |\n+---------+------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n6 rows in set (0.01 sec)\n```\n\n锁定分区 `p1` 的统计信息，再执行 `ANALYZE` 语句，warning 提示跳过对分区 `p1` 的 `ANALYZE`：\n\n```sql\nmysql> LOCK STATS t PARTITION p1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SHOW STATS_LOCKED;\n+---------+------------+----------------+--------+\n| Db_name | Table_name | Partition_name | Status |\n+---------+------------+----------------+--------+\n| test    | t          | p1             | locked |\n+---------+------------+----------------+--------+\n1 row in set (0.00 sec)\n\nmysql> ANALYZE TABLE t PARTITION p1;\nQuery OK, 0 rows affected, 2 warnings (0.01 sec)\n\nmysql> SHOW WARNINGS;\n+---------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                                                                                              |\n+---------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Note    | 1105 | Analyze use auto adjusted sample rate 1.000000 for table test.t's partition p1, reason to use this rate is \"TiDB assumes that the table is empty, use sample-rate=1\" |\n| Warning | 1105 | skip analyze locked table: test.t partition (p1)                                                                                                                     |\n+---------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n2 rows in set (0.00 sec)\n```\n\n解锁分区 `p1` 的统计信息，可以成功执行 `ANALYZE` 语句：\n\n```sql\nmysql> UNLOCK STATS t PARTITION p1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> ANALYZE TABLE t PARTITION p1;\nQuery OK, 0 rows affected, 1 warning (0.01 sec)\n\nmysql> SHOW WARNINGS;\n+-------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Level | Code | Message                                                                                                                                                              |\n+-------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Note  | 1105 | Analyze use auto adjusted sample rate 1.000000 for table test.t's partition p1, reason to use this rate is \"TiDB assumes that the table is empty, use sample-rate=1\" |\n+-------+------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n### 锁定统计信息的行为说明\n\n* 如果统计信息在分区表上锁定，那么该分区表上所有分区的统计信息就都保持锁定。\n* 如果表或者分区被 truncate，该表或分区上的统计信息锁定将会被解除。\n\n具体行为参见下面表格：\n\n|     | 删除整张表 | Truncate 整张表 | Truncate 某个分区 | 创建一个新分区 | 删除某个分区 | Reorganize 某个分区 | 交换某个分区 |\n|-----|----------|----------------|-----------------|--------------|-----------|-----------|-------|\n| 非分区表被锁定   | 锁定失效   | 锁定失效，因为 TiDB 删除了旧表，所以锁定信息也一起被删除  | /     | /       | /        | /          | /              |\n| 分区表并且整张表被锁定     | 锁定失效   | 锁定失效，因为 TiDB 删除了旧表，所以锁定信息也一起被删除       | 旧的分区锁定信息失效，自动锁定新的分区                         | 自动锁定新分区 | 被删除的分区锁定信息被清理，整张表锁继续生效 | 被删除的分区锁定信息被清理，新分区被自动锁定 | 锁定信息被转移到被交换表，新分区被自动锁定 |\n| 分区表并且只锁定了某些分区 | 锁定失效   | 锁定失效，因为 TiDB 删除了旧的分区表，所以锁定信息也一起被删除 | 锁定失效，因为 TiDB 删除了旧的分区表，所以锁定信息也一起被删除 | /              | 被删除的分区锁定信息被清理                   | 被删除的分区锁定信息被清理                   | 锁定信息被转移到被交换表 |\n\n## 管理 `ANALYZE` 任务与并发\n\n本小节介绍如何终止后台的 `ANALYZE` 任务，如何控制 `ANALYZE` 并发度。\n\n### 终止后台的 `ANALYZE` 任务\n\n从 TiDB v6.0 起，TiDB 支持通过 `KILL` 语句终止正在后台运行的 `ANALYZE` 任务。如果发现正在后台运行的 `ANALYZE` 任务消耗大量资源影响业务，你可以通过以下步骤终止该 `ANALYZE` 任务：\n\n1. 执行以下 SQL 语句：\n\n    ```sql\n    SHOW ANALYZE STATUS\n    ```\n\n    查看 `instance` 列和 `process_id` 列，获得正在执行后台 `ANALYZE` 任务的 TiDB 实例地址和任务 `ID`。\n\n2. 终止正在后台运行的 `ANALYZE` 任务。\n\n    - 如果 [`enable-global-kill`](/tidb-configuration-file.md#enable-global-kill-从-v610-版本开始引入) 的值为 `true`（默认为 `true`），你可以直接执行 `KILL TIDB ${id};` 语句。其中，`${id}` 为上一步中查询得到的后台 `ANALYZE` 任务的 `ID`。\n    - 如果 `enable-global-kill` 的值为 `false`，你需要先使用客户端连接到执行后台 `ANALYZE` 任务的 TiDB 实例，然后再执行 `KILL TIDB ${id};` 语句。如果使用客户端连接到其他 TiDB 实例，或者客户端和 TiDB 中间有代理，则 `KILL` 语句不能终止后台的 `ANALYZE` 任务。\n\n  关于 `KILL` 语句的更多信息，参见 [`KILL`](/sql-statements/sql-statement-kill.md)。\n\n### 控制 `ANALYZE` 并发度\n\n执行 `ANALYZE` 语句的时候，你可以通过一些系统变量来调整并发度，以控制对系统的影响。\n\n相关系统变量的关系如下图所示：\n\n![analyze_concurrency](/media/analyze_concurrency.png)\n\n`tidb_build_stats_concurrency`、`tidb_build_sampling_stats_concurrency` 和 `tidb_analyze_partition_concurrency` 为上下游关系。实际的总并发为：`tidb_build_stats_concurrency`* (`tidb_build_sampling_stats_concurrency` + `tidb_analyze_partition_concurrency`) 。所以在变更这些参数的时候，需要同时考虑这三个参数的值。建议按 `tidb_analyze_partition_concurrency`、`tidb_build_sampling_stats_concurrency`、`tidb_build_stats_concurrency` 的顺序逐个调节，并观察对系统的影响。这三个参数的值越大，对系统的资源开销就越大。\n\n#### `tidb_build_stats_concurrency`\n\n`ANALYZE` 任务在执行时会被切分成一个个小任务，每个任务只负责某一个列或者索引的统计信息收集。你可以使用 [`tidb_build_stats_concurrency`](/system-variables.md#tidb_build_stats_concurrency) 控制可以同时执行的小任务的数量，其默认值是 `2`。TiDB v7.4.0 及其之前版本中，默认值为 `4`。\n\n#### `tidb_build_sampling_stats_concurrency`\n\n在执行 `ANALYZE` 普通列任务的时候，你可以使用 [`tidb_build_sampling_stats_concurrency`](/system-variables.md#tidb_build_sampling_stats_concurrency-从-v750-版本开始引入) 控制执行采样任务的并发数量，其默认值是 `2`。\n\n#### `tidb_analyze_partition_concurrency`\n\n在执行 `ANALYZE` 任务的时候，你可以使用 [`tidb_analyze_partition_concurrency`](/system-variables.md#tidb_analyze_partition_concurrency) 控制对分区表统计信息进行读写的并发度，其默认值是 `2`。TiDB v7.4.0 及其之前版本中，默认值为 `1`。\n\n#### `tidb_distsql_scan_concurrency`\n\n在执行 `ANALYZE` 普通列任务的时候，你可以使用 [`tidb_distsql_scan_concurrency`](/system-variables.md#tidb_distsql_scan_concurrency) 控制一次读取的 Region 数量，其默认值是 `15`。修改该变量的值会影响查询性能，请谨慎调整。\n\n#### `tidb_index_serial_scan_concurrency`\n\n在执行 `ANALYZE` 索引列任务的时候，你可以使用 [`tidb_index_serial_scan_concurrency`](/system-variables.md#tidb_index_serial_scan_concurrency) 控制一次读取的 Region 数量，其默认值是 `1`。修改该变量的值会影响查询性能，请谨慎调整。\n\n## 另请参阅\n\n* [`LOAD STATS`](/sql-statements/sql-statement-load-stats.md)\n* [`DROP STATS`](/sql-statements/sql-statement-drop-stats.md)\n* [`LOCK STATS`](/sql-statements/sql-statement-lock-stats.md)\n* [`UNLOCK STATS`](/sql-statements/sql-statement-unlock-stats.md)\n* [`SHOW STATS_LOCKED`](/sql-statements/sql-statement-show-stats-locked.md)"
        },
        {
          "name": "status-variables.md",
          "type": "blob",
          "size": 2.8486328125,
          "content": "---\ntitle: 服务器状态变量\nsummary: 使用状态变量查看系统和会话状态。\n---\n\n# 服务器状态变量\n\n服务器状态变量提供有关服务器全局状态和 TiDB 中当前会话状态的信息。大多数变量与 MySQL 兼容。\n\n你可以使用 [SHOW GLOBAL STATUS](/sql-statements/sql-statement-show-status.md) 命令查看全局状态，使用 [SHOW SESSION STATUS](/sql-statements/sql-statement-show-status.md) 命令查看当前会话状态。  \n\n此外，[FLUSH STATUS](/sql-statements/sql-statement-flush-status.md) 命令与 MySQL 兼容。\n\n## 变量参考\n\n### Compression\n\n- 作用域：SESSION\n- 类型：布尔值\n- MySQL 是否使用压缩协议。\n\n### Compression_algorithm\n\n- 作用域：SESSION\n- 类型：字符串\n- MySQL 协议使用的压缩算法。\n\n### Compression_level\n\n- 作用域：SESSION\n- 类型：整数型\n- MySQL 协议使用的压缩等级。\n\n### Ssl_cipher\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- 正在使用的 TLS 加密套件。\n\n### Ssl_cipher_list\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- 服务器支持的 TLS 加密套件列表。\n\n### Ssl_server_not_after\n\n- 作用域：SESSION | GLOBAL\n- 类型：日期\n- 服务器用于 TLS 连接的 X.509 证书的过期时间。\n\n### Ssl_server_not_before\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- 服务器用于 TLS 连接的 X.509 证书的开始时间。\n\n### Ssl_verify_mode\n\n- 作用域：SESSION | GLOBAL\n- 类型：整数型\n- TLS 验证模式掩码。\n\n### Ssl_version\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- TLS 协议使用的版本。\n\n### Uptime\n\n- 作用域：SESSION | GLOBAL\n- 类型：整数型\n- 服务器正常运行时间（秒）。\n\n### ddl_schema_version\n\n- 作用域：SESSION | GLOBAL\n- 类型：整数型\n- DDL schema 使用的版本。\n\n### last_plan_binding_update_time <span class=\"version-mark\">从 v5.2.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 类型：时间戳\n- 最后一次计划绑定更新的日期时间。\n\n### server_id\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- 服务器的通用唯一识别码 (UUID)。\n\n### tidb_gc_last_run_time\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- 最近一次运行[垃圾回收 (GC)](/garbage-collection-overview.md) 的时间戳。\n\n### tidb_gc_leader_desc\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- [GC](/garbage-collection-overview.md) leader 的相关信息，包括主机名和进程 ID (PID)。\n\n### tidb_gc_leader_lease\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- [GC](/garbage-collection-overview.md) leader 的租约时间戳。\n\n### tidb_gc_leader_uuid\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- [GC](/garbage-collection-overview.md) leader 的 UUID。\n\n### tidb_gc_safe_point\n\n- 作用域：SESSION | GLOBAL\n- 类型：字符串\n- [GC](/garbage-collection-overview.md) safe point 的时间戳。\n"
        },
        {
          "name": "storage-engine",
          "type": "tree",
          "content": null
        },
        {
          "name": "subquery-optimization.md",
          "type": "blob",
          "size": 7.1513671875,
          "content": "---\ntitle: 子查询相关的优化\nsummary: 了解子查询相关的优化。\naliases: ['/docs-cn/dev/subquery-optimization/']\n---\n\n# 子查询相关的优化\n\n本文主要介绍子查询相关的优化。\n\n通常会遇到如下情况的子查询：\n\n- `NOT IN (SELECT ... FROM ...)`\n- `NOT EXISTS (SELECT ... FROM ...)`\n- `IN (SELECT ... FROM ..)`\n- `EXISTS (SELECT ... FROM ...)`\n- `... >/>=/</<=/=/!= (SELECT ... FROM ...)`\n\n有时，子查询中包含了非子查询中的列，如 `select * from t where t.a in (select * from t2 where t.b=t2.b)` 中，子查询中的 `t.b` 不是子查询中的列，而是从子查询外面引入的列。这种子查询通常会被称为`关联子查询`，外部引入的列会被称为`关联列`，关联子查询相关的优化参见[关联子查询去关联](/correlated-subquery-optimization.md)。本文主要关注不涉及关联列的子查询。\n\n子查询默认会以[理解 TiDB 执行计划](/explain-overview.md)中提到的 `semi join` 作为默认的执行方式，同时对于一些特殊的子查询，TiDB 会做一些逻辑上的替换使得查询可以获得更好的执行性能。\n\n## `... < ALL (SELECT ... FROM ...)` 或者 `... > ANY (SELECT ... FROM ...)`\n\n对于这种情况，可以将 `ALL` 或者 `ANY` 用 `MAX` 以及 `MIN` 来代替。不过由于在表为空时，`MAX(EXPR)` 以及 `MIN(EXPR)` 的结果会为 `NULL`，其表现形式和 `EXPR` 是有 `NULL` 值的结果一样。以及外部表达式结果为 `NULL` 时也会影响表达式的最终结果，因此这里完整的改写会是如下的形式：\n\n- `t.id < all(select s.id from s)` 会被改写为 `t.id < min(s.id) and if(sum(s.id is null) != 0, null, true)`。\n- `t.id < any (select s.id from s)` 会被改写为 `t.id < max(s.id) or if(sum(s.id is null) != 0, null, false)`。\n\n## `... != ANY (SELECT ... FROM ...)`\n\n对于这种情况，当子查询中不同值的个数只有一种的话，那只要和这个值对比就即可。如果子查询中不同值的个数多于 1 个，那么必然会有不相等的情况出现。因此这样的子查询可以采取如下的改写手段：\n\n- `select * from t where t.id != any (select s.id from s)` 会被改写为 `select t.* from t, (select s.id, count(distinct s.id) as cnt_distinct from s) where (t.id != s.id or cnt_distinct > 1)`\n\n## `... = ALL (SELECT ... FROM ...)`\n\n对于这种情况，当子查询中不同值的个数多于一种的话，那么这个表达式的结果必然为假。因此这样的子查询在 TiDB 中会改写为如下的形式：\n\n- `select * from t where t.id = all (select s.id from s)` 会被改写为 `select t.* from t, (select s.id, count(distinct s.id) as cnt_distinct from s) where (t.id = s.id and cnt_distinct <= 1)`\n\n## `... IN (SELECT ... FROM ...)`\n\n对于这种情况，会将其 `IN` 的子查询改写为 `SELECT ... FROM ... GROUP ...` 的形式，然后将 `IN` 改写为普通的 `JOIN` 的形式。如 `select * from t1 where t1.a in (select t2.a from t2)` 会被改写为 `select t1.* from t1, (select distinct(a) a from t2) t2 where t1.a = t2.a` 的形式。同时这里的 `DISTINCT` 可以在 `t2.a` 具有 `UNIQUE` 属性时被自动消去。\n\n{{< copyable \"sql\" >}}\n\n```sql\nexplain select * from t1 where t1.a in (select t2.a from t2);\n```\n\n```sql\n+------------------------------+---------+-----------+------------------------+----------------------------------------------------------------------------+\n| id                           | estRows | task      | access object          | operator info                                                              |\n+------------------------------+---------+-----------+------------------------+----------------------------------------------------------------------------+\n| IndexJoin_12                 | 9990.00 | root      |                        | inner join, inner:TableReader_11, outer key:test.t2.a, inner key:test.t1.a |\n| ├─HashAgg_21(Build)          | 7992.00 | root      |                        | group by:test.t2.a, funcs:firstrow(test.t2.a)->test.t2.a                   |\n| │ └─IndexReader_28           | 9990.00 | root      |                        | index:IndexFullScan_27                                                     |\n| │   └─IndexFullScan_27       | 9990.00 | cop[tikv] | table:t2, index:idx(a) | keep order:false, stats:pseudo                                             |\n| └─TableReader_11(Probe)      | 7992.00 | root      |                        | data:TableRangeScan_10                                                     |\n|   └─TableRangeScan_10        | 7992.00 | cop[tikv] | table:t1               | range: decided by [test.t2.a], keep order:false, stats:pseudo              |\n+------------------------------+---------+-----------+------------------------+----------------------------------------------------------------------------+\n```\n\n这个改写会在 `IN` 子查询相对较小，而外部查询相对较大时产生更好的执行性能。因为不经过改写的情况下，我们无法使用以 t2 为驱动表的 `index join`。同时这里的弊端便是，当改写生成的聚合无法被自动消去且 `t2` 表比较大时，反而会影响查询的性能。目前 TiDB 中使用 [tidb\\_opt\\_insubq\\_to\\_join\\_and\\_agg](/system-variables.md#tidb_opt_insubq_to_join_and_agg) 变量来控制这个优化的打开与否。当遇到不合适这个优化的情况可以手动关闭。\n\n## `EXISTS` 子查询以及 `... >/>=/</<=/=/!= (SELECT ... FROM ...)`\n\n当前对于这种场景的子查询，当它不是关联子查询时，TiDB 会在优化阶段提前展开它，将其直接替换为一个结果集直接判断结果。如下图中，`EXISTS` 会提前在优化阶段被执行为 `TRUE`，从而不会在最终的执行结果中看到它。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t1(a int);\ncreate table t2(a int);\ninsert into t2 values(1);\nexplain select * from t1 where exists (select * from t2);\n```\n\n```sql\n+------------------------+----------+-----------+---------------+--------------------------------+\n| id                     | estRows  | task      | access object | operator info                  |\n+------------------------+----------+-----------+---------------+--------------------------------+\n| TableReader_12         | 10000.00 | root      |               | data:TableFullScan_11          |\n| └─TableFullScan_11     | 10000.00 | cop[tikv] | table:t1      | keep order:false, stats:pseudo |\n+------------------------+----------+-----------+---------------+--------------------------------+\n```\n\n在上述优化中，优化器会自动优化语句执行。除以上情况外，你也可以在语句中添加 [`SEMI_JOIN_REWRITE`](/optimizer-hints.md#semi_join_rewrite) hint 进一步改写语句。\n\n如果不使用 `SEMI_JOIN_REWRITE` 进行改写，Semi Join 在选择 Hash Join 的执行方式时，只能够使用子查询构建哈希表，因此在子查询比外查询结果集大时，执行速度可能会不及预期。Semi Join 在选择 Index Join 的执行方式时，只能够使用外查询作为驱动表，因此在子查询比外查询结果集小时，执行速度可能会不及预期。\n\n使用 `SEMI_JOIN_REWRITE` 改写后，优化器便可以扩大选择范围，选择更好的执行方式。\n"
        },
        {
          "name": "support.md",
          "type": "blob",
          "size": 0.7392578125,
          "content": "---\ntitle: 支持资源\nsummary: 在使用 TiDB 时遇到问题，如何获取支持。\n---\n\n# 支持资源\n\n如果你在使用 TiDB 的过程中遇到了问题，你可以通过以下方式向 PingCAP 或 TiDB 社区寻求帮助：\n\n+ 从 PingCAP 获取支持（需要订阅 [TiDB 企业版](https://www.pingcap.cn/)）：\n\n    - [提交工单](https://support.pingcap.cn/)\n\n+ 从 TiDB 社区寻求帮助：\n\n    - [AskTUG 论坛](https://asktug.com/)\n    - [Stack Overflow](https://stackoverflow.com/questions/tagged/tidb)（在 #tidb 标签下提问）\n\n+ 报告 bug：\n\n    - 在 TiDB 仓库中[提交 issue](https://github.com/pingcap/tidb/issues/new/choose)\n\n+ 了解 TiDB 的实现和设计\n\n    - [TiDB Internals 论坛](https://internals.tidb.io/)\n"
        },
        {
          "name": "sync-diff-inspector",
          "type": "tree",
          "content": null
        },
        {
          "name": "sys-schema",
          "type": "tree",
          "content": null
        },
        {
          "name": "system-variable-reference.md",
          "type": "blob",
          "size": 150.2978515625,
          "content": "---\ntitle: 系统变量索引\nsummary: 查看 TiDB 所有的系统变量，以及引用这些变量的文档。\n---\n\n<!-- Note: The content of the reference lists in this file is organized in the following order:\n- For non-release-notes docs, put them in ascending alphabetical order.\n- For release notes docs, put them in descending order by version number.\n- Put release notes docs after non-release-notes docs. -->\n\n# 系统变量索引\n\n本文档列出了所有 TiDB 系统变量，以及引用这些变量的文档。你可以查看[系统变量](/system-variables.md)了解每个变量的详细信息。\n\n## 变量索引\n\n### allow_auto_random_explicit_insert\n\n引用该变量的文档：\n\n- [AUTO_RANDOM](/auto-random.md)\n- [SESSION_VARIABLES](/information-schema/information-schema-session-variables.md)\n- [VARIABLES_INFO](/information-schema/information-schema-variables-info.md)\n- [插入数据](/develop/dev-guide-insert-data.md)\n- [系统变量](/system-variables.md#allow_auto_random_explicit_insert-从-v403-版本开始引入)\n\n### authentication_ldap_sasl_auth_method_name\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_auth_method_name-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_bind_base_dn\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_bind_base_dn-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_bind_root_dn\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_bind_root_dn-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_bind_root_pwd\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_bind_root_pwd-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_ca_path\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_ca_path-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_init_pool_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_init_pool_size-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_max_pool_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_max_pool_size-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_server_host\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_server_host-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_server_port\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_server_port-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_sasl_tls\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_sasl_tls-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_auth_method_name\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_auth_method_name-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_bind_base_dn\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_bind_base_dn-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_bind_root_dn\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_bind_root_dn-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_bind_root_pwd\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_bind_root_pwd-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_ca_path\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_ca_path-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_init_pool_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_init_pool_size-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_max_pool_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_max_pool_size-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_server_host\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_server_host-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_server_port\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_server_port-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### authentication_ldap_simple_tls\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#authentication_ldap_simple_tls-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### auto_increment_increment\n\n引用该变量的文档：\n\n- [AUTO_INCREMENT](/auto-increment.md)\n- [AUTO_RANDOM](/auto-random.md)\n- [SESSION_VARIABLES](/information-schema/information-schema-session-variables.md)\n- [TiCDC 双向复制](/ticdc/ticdc-bidirectional-replication.md)\n- [VARIABLES_INFO](/information-schema/information-schema-variables-info.md)\n- [系统变量](/system-variables.md#auto_increment_increment)\n- [错误码与故障诊断](/error-codes.md)\n- [TiDB 7.1.6 Release Notes](/releases/release-7.1.6.md)\n- [TiDB 6.5.10 Release Notes](/releases/release-6.5.10.md)\n- [TiDB 3.0.9 Release Notes](/releases/release-3.0.9.md)\n\n### auto_increment_offset\n\n引用该变量的文档：\n\n- [AUTO_INCREMENT](/auto-increment.md)\n- [AUTO_RANDOM](/auto-random.md)\n- [SESSION_VARIABLES](/information-schema/information-schema-session-variables.md)\n- [TiCDC 双向复制](/ticdc/ticdc-bidirectional-replication.md)\n- [VARIABLES_INFO](/information-schema/information-schema-variables-info.md)\n- [系统变量](/system-variables.md#auto_increment_offset)\n- [错误码与故障诊断](/error-codes.md)\n- [TiDB 7.1.6 Release Notes](/releases/release-7.1.6.md)\n- [TiDB 6.5.10 Release Notes](/releases/release-6.5.10.md)\n- [TiDB 3.0.9 Release Notes](/releases/release-3.0.9.md)\n\n### autocommit\n\n引用该变量的文档：\n\n- [TiDB 事务概览](/transaction-overview.md)\n- [系统变量](/system-variables.md#autocommit)\n- [非事务 DML 语句](/non-transactional-dml.md)\n\n### block_encryption_mode\n\n引用该变量的文档：\n\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#block_encryption_mode)\n\n### character_set_client\n\n引用该变量的文档：\n\n- [GBK](/character-set-gbk.md)\n- [SET [NAMES|CHARACTER SET]](/sql-statements/sql-statement-set-names.md)\n- [VIEWS](/information-schema/information-schema-views.md)\n- [使用 Dumpling 导出数据](/dumpling-overview.md)\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#character_set_client)\n- [视图](/views.md)\n- [视图](/develop/dev-guide-use-views.md)\n\n### character_set_connection\n\n引用该变量的文档：\n\n- [GBK](/character-set-gbk.md)\n- [SET [NAMES|CHARACTER SET]](/sql-statements/sql-statement-set-names.md)\n- [使用 Dumpling 导出数据](/dumpling-overview.md)\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#character_set_connection)\n\n### character_set_database\n\n引用该变量的文档：\n\n- [SET [NAMES|CHARACTER SET]](/sql-statements/sql-statement-set-names.md)\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#character_set_database)\n\n### character_set_results\n\n引用该变量的文档：\n\n- [SET [NAMES|CHARACTER SET]](/sql-statements/sql-statement-set-names.md)\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#character_set_results)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n- [TiDB 2.1 RC1 Release Notes](/releases/release-2.1-rc.1.md)\n\n### character_set_server\n\n引用该变量的文档：\n\n- [SET [NAMES|CHARACTER SET]](/sql-statements/sql-statement-set-names.md)\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#character_set_server)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n\n### collation_connection\n\n引用该变量的文档：\n\n- [VIEWS](/information-schema/information-schema-views.md)\n- [字符串函数](/functions-and-operators/string-functions.md)\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#collation_connection)\n- [视图](/views.md)\n- [视图](/develop/dev-guide-use-views.md)\n\n### collation_database\n\n引用该变量的文档：\n\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#collation_database)\n\n### collation_server\n\n引用该变量的文档：\n\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#collation_server)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n- [TiDB 5.0.2 Release Notes](/releases/release-5.0.2.md)\n\n### cte_max_recursion_depth\n\n引用该变量的文档：\n\n- [TiDB 高并发写入场景最佳实践](/best-practices/high-concurrency-best-practices.md)\n- [系统变量](/system-variables.md#cte_max_recursion_depth)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n\n### datadir\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#datadir)\n\n### ddl_slow_threshold\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#ddl_slow_threshold)\n\n### default_authentication_plugin\n\n引用该变量的文档：\n\n- [TiDB 功能概览](/basic-features.md)\n- [与 MySQL 安全特性差异](/security-compatibility-with-mysql.md)\n- [系统变量](/system-variables.md#default_authentication_plugin)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n- [TiDB 5.2 Release Notes](/releases/release-5.2.0.md)\n\n### default_collation_for_utf8mb4\n\n引用该变量的文档：\n\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#default_collation_for_utf8mb4-从-v740-版本开始引入)\n- [TiDB 8.1.2 Release Notes](/releases/release-8.1.2.md)\n- [TiDB 7.5.5 Release Notes](/releases/release-7.5.5.md)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### default_password_lifetime\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [系统变量](/system-variables.md#default_password_lifetime-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### default_week_format\n\n引用该变量的文档：\n\n- [日期和时间函数](/functions-and-operators/date-and-time-functions.md)\n- [系统变量](/system-variables.md#default_week_format)\n- [TiDB 4.0.11 Release Notes](/releases/release-4.0.11.md)\n- [TiDB 2.1.7 Release Notes](/releases/release-2.1.7.md)\n\n### disconnect_on_expired_password\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#disconnect_on_expired_password-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### div_precision_increment\n\n引用该变量的文档：\n\n- [数值函数与操作符](/functions-and-operators/numeric-functions-and-operators.md)\n- [系统变量](/system-variables.md#div_precision_increment-从-v800-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### error_count\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#error_count)\n- [TiDB 2.1 RC1 Release Notes](/releases/release-2.1-rc.1.md)\n\n### foreign_key_checks\n\n引用该变量的文档：\n\n- [外键约束](/foreign-key.md)\n- [系统变量](/system-variables.md#foreign_key_checks)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### group_concat_max_len\n\n引用该变量的文档：\n\n- [GROUP BY 聚合函数](/functions-and-operators/aggregate-group-by-functions.md)\n- [系统变量](/system-variables.md#group_concat_max_len)\n- [TiDB 4.0.13 Release Notes](/releases/release-4.0.13.md)\n\n### have_openssl\n\n引用该变量的文档：\n\n- [TiDB 证书鉴权使用指南](/certificate-authentication.md)\n- [系统变量](/system-variables.md#have_openssl)\n\n### have_ssl\n\n引用该变量的文档：\n\n- [TiDB 证书鉴权使用指南](/certificate-authentication.md)\n- [系统变量](/system-variables.md#have_ssl)\n\n### hostname\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#hostname)\n\n### identity\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#identity-从-v530-版本开始引入)\n\n### init_connect\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#init_connect)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n- [TiDB 5.0.4 Release Notes](/releases/release-5.0.4.md)\n- [TiDB 4.0.14 Release Notes](/releases/release-4.0.14.md)\n\n### innodb_lock_wait_timeout\n\n引用该变量的文档：\n\n- [TiDB 悲观事务模式](/pessimistic-transaction.md)\n- [TiDB 锁冲突问题处理](/troubleshoot-lock-conflicts.md)\n- [TiKV 配置文件描述](/tikv-configuration-file.md)\n- [系统变量](/system-variables.md#innodb_lock_wait_timeout)\n- [TiDB 3.0.6 Release Notes](/releases/release-3.0.6.md)\n\n### interactive_timeout\n\n引用该变量的文档：\n\n- [TiDB 中的各种超时](/develop/dev-guide-timeouts-in-tidb.md)\n- [TiDB 集群管理常见问题](/faq/manage-cluster-faq.md)\n- [系统变量](/system-variables.md#interactive_timeout)\n- [TiDB 3.0 Beta Release Notes](/releases/release-3.0-beta.md)\n- [TiDB 3.0 GA Release Notes](/releases/release-3.0-ga.md)\n\n### last_insert_id\n\n引用该变量的文档：\n\n- [AUTO_RANDOM](/auto-random.md)\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [SHOW BUILTINS](/sql-statements/sql-statement-show-builtins.md)\n- [使用 mysql2 连接 TiDB](/develop/dev-guide-sample-application-ruby-mysql2.md)\n- [信息函数](/functions-and-operators/information-functions.md)\n- [系统变量](/system-variables.md#last_insert_id-从-v530-版本开始引入)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n- [TiDB 3.1 RC Release Notes](/releases/release-3.1.0-rc.md)\n- [TiDB 2.1.17 Release Notes](/releases/release-2.1.17.md)\n- [TiDB 2.1 RC2 Release Notes](/releases/release-2.1-rc.2.md)\n\n### last_plan_from_binding\n\n引用该变量的文档：\n\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#last_plan_from_binding-从-v40-版本开始引入)\n- [TiDB 4.0.12 Release Notes](/releases/release-4.0.12.md)\n\n### last_plan_from_cache\n\n引用该变量的文档：\n\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [系统变量](/system-variables.md#last_plan_from_cache-从-v40-版本开始引入)\n- [索引的选择](/choose-index.md)\n- [非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)\n- [TiDB 4.0.2 Release Notes](/releases/release-4.0.2.md)\n\n### last_sql_use_alloc\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#last_sql_use_alloc-从-v640-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### license\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#license)\n\n### max_allowed_packet\n\n引用该变量的文档：\n\n- [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)\n- [Data Migration 常见问题](/dm/dm-faq.md)\n- [TiDB Data Migration 故障及处理方法](/dm/dm-error-handling.md)\n- [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#max_allowed_packet-从-v610-版本开始引入)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n- [TiDB 5.2.4 Release Notes](/releases/release-5.2.4.md)\n- [TiDB 3.0.2 Release Notes](/releases/release-3.0.2.md)\n- [TiDB 2.1.5 Release Notes](/releases/release-2.1.5.md)\n- [TiDB 2.1 RC2 Release Notes](/releases/release-2.1-rc.2.md)\n- [TiDB 2.0.6 Release Notes](/releases/release-2.0.6.md)\n\n### max_connections\n\n引用该变量的文档：\n\n- [ProxySQL 集成指南](/develop/dev-guide-proxysql-integration.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [TiDB 集群管理常见问题](/faq/manage-cluster-faq.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [系统变量](/system-variables.md#max_connections)\n\n### max_execution_time\n\n引用该变量的文档：\n\n- [Optimizer Hints](/optimizer-hints.md)\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 中的各种超时](/develop/dev-guide-timeouts-in-tidb.md)\n- [开发 Java 应用使用 TiDB 的最佳实践](/best-practices/java-app-best-practices.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#max_execution_time)\n- [连接池与连接参数](/develop/dev-guide-connection-parameters.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 4.0.2 Release Notes](/releases/release-4.0.2.md)\n- [TiDB 2.1.14 Release Notes](/releases/release-2.1.14.md)\n\n### max_prepared_stmt_count\n\n引用该变量的文档：\n\n- [PREPARE](/sql-statements/sql-statement-prepare.md)\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [系统变量](/system-variables.md#max_prepared_stmt_count)\n- [TiDB 6.5.2 Release Notes](/releases/release-6.5.2.md)\n\n### mpp_exchange_compression_mode\n\n引用该变量的文档：\n\n- [用 EXPLAIN 查看 MPP 模式查询的执行计划](/explain-mpp.md)\n- [系统变量](/system-variables.md#mpp_exchange_compression_mode-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### mpp_version\n\n引用该变量的文档：\n\n- [用 EXPLAIN 查看 MPP 模式查询的执行计划](/explain-mpp.md)\n- [系统变量](/system-variables.md#mpp_version-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### password_history\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [`mysql` Schema](/mysql-schema/mysql-schema.md)\n- [与 MySQL 安全特性差异](/security-compatibility-with-mysql.md)\n- [系统变量](/system-variables.md#password_history-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### password_reuse_interval\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [系统变量](/system-variables.md#password_reuse_interval-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### pd_enable_follower_handle_region\n\n引用该变量的文档：\n\n- [Region 性能调优](/tune-region-performance.md)\n- [TiDB 功能概览](/basic-features.md)\n- [系统变量](/system-variables.md#pd_enable_follower_handle_region-从-v760-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n\n### plugin_dir\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#plugin_dir)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n\n### plugin_load\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#plugin_load)\n\n### port\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#port)\n\n### rand_seed1\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#rand_seed1)\n\n### rand_seed2\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#rand_seed2)\n\n### require_secure_transport\n\n引用该变量的文档：\n\n- [为 TiDB 客户端服务端间通信开启加密传输](/enable-tls-between-clients-and-servers.md)\n- [系统变量](/system-variables.md#require_secure_transport-从-v610-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.1.2 Release Notes](/releases/release-7.1.2.md)\n- [TiDB 6.5.6 Release Notes](/releases/release-6.5.6.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### skip_name_resolve\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#skip_name_resolve-从-v520-版本开始引入)\n\n### socket\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#socket)\n\n### sql_mode\n\n引用该变量的文档：\n\n- [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)\n- [GROUP BY 聚合函数](/functions-and-operators/aggregate-group-by-functions.md)\n- [SHOW ERRORS](/sql-statements/sql-statement-show-errors.md)\n- [SHOW WARNINGS](/sql-statements/sql-statement-show-warnings.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [SQL 模式](/sql-mode.md)\n- [Schema 对象名](/schema-object-names.md)\n- [TiDB Lightning 常见问题](/tidb-lightning/tidb-lightning-faq.md)\n- [TiDB 用户账户管理](/user-account-management.md)\n- [TiFlash 查询结果物化](/tiflash/tiflash-results-materialization.md)\n- [`SET [GLOBAL|SESSION] <variable>`](/sql-statements/sql-statement-set-variable.md)\n- [使用 TiDB 读取 TiFlash](/tiflash/use-tidb-to-read-tiflash.md)\n- [其他函数](/functions-and-operators/miscellaneous-functions.md)\n- [分区表](/partitioned-table.md)\n- [日期和时间类型](/data-type-date-and-time.md)\n- [权限管理](/privilege-management.md)\n- [精度数学](/functions-and-operators/precision-math.md)\n- [系统变量](/system-variables.md#sql_mode)\n- [结果集不稳定](/develop/dev-guide-unstable-result-set.md)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 7.5.5 Release Notes](/releases/release-7.5.5.md)\n- [TiDB 7.5.3 Release Notes](/releases/release-7.5.3.md)\n- [TiDB 7.5.2 Release Notes](/releases/release-7.5.2.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.1.5 Release Notes](/releases/release-7.1.5.md)\n- [TiDB 7.1.4 Release Notes](/releases/release-7.1.4.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 6.5.9 Release Notes](/releases/release-6.5.9.md)\n- [TiDB 6.5.7 Release Notes](/releases/release-6.5.7.md)\n- [TiDB 5.4.1 Release Notes](/releases/release-5.4.1.md)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n- [TiDB 5.0.2 Release Notes](/releases/release-5.0.2.md)\n- [TiDB 4.0.16 Release Notes](/releases/release-4.0.16.md)\n- [TiDB RC3 Release Notes](/releases/release-rc.3.md)\n- [TiDB RC2 Release Notes](/releases/release-rc.2.md)\n\n### sql_require_primary_key\n\n引用该变量的文档：\n\n- [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)\n- [系统变量](/system-variables.md#sql_require_primary_key-从-v630-版本开始引入)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### sql_select_limit\n\n引用该变量的文档：\n\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [系统变量](/system-variables.md#sql_select_limit-从-v402-版本开始引入)\n- [TiDB 4.0.2 Release Notes](/releases/release-4.0.2.md)\n\n### ssl_ca\n\n引用该变量的文档：\n\n- [TiDB 证书鉴权使用指南](/certificate-authentication.md)\n- [使用 Django 连接到 TiDB](/develop/dev-guide-sample-application-python-django.md)\n- [使用 MySQL Connector/Python 连接到 TiDB](/develop/dev-guide-sample-application-python-mysql-connector.md)\n- [使用 PyMySQL 连接到 TiDB](/develop/dev-guide-sample-application-python-pymysql.md)\n- [使用 Python 开始向量搜索](/vector-search/vector-search-get-started-using-python.md)\n- [使用 SQLAlchemy 连接到 TiDB](/develop/dev-guide-sample-application-python-sqlalchemy.md)\n- [使用 peewee 连接到 TiDB](/develop/dev-guide-sample-application-python-peewee.md)\n- [在 LangChain 中使用 TiDB 向量搜索](/vector-search/vector-search-integrate-with-langchain.md)\n- [在 LlamaIndex 中使用 TiDB 向量搜索](/vector-search/vector-search-integrate-with-llamaindex.md)\n- [在 SQLAlchemy 中使用 TiDB 向量搜索](/vector-search/vector-search-integrate-with-sqlalchemy.md)\n- [系统变量](/system-variables.md#ssl_ca)\n- [结合 Jina AI 嵌入模型 API 使用 TiDB 向量搜索](/vector-search/vector-search-integrate-with-jinaai-embedding.md)\n\n### ssl_cert\n\n引用该变量的文档：\n\n- [TiDB 证书鉴权使用指南](/certificate-authentication.md)\n- [系统变量](/system-variables.md#ssl_cert)\n\n### ssl_key\n\n引用该变量的文档：\n\n- [TiDB 证书鉴权使用指南](/certificate-authentication.md)\n- [系统变量](/system-variables.md#ssl_key)\n\n### system_time_zone\n\n引用该变量的文档：\n\n- [时区支持](/configure-time-zone.md)\n- [系统变量](/system-variables.md#system_time_zone)\n- [TiDB 3.0.8 Release Notes](/releases/release-3.0.8.md)\n\n### tidb_adaptive_closest_read_threshold\n\n引用该变量的文档：\n\n- [Follower Read](/follower-read.md)\n- [系统变量](/system-variables.md#tidb_adaptive_closest_read_threshold-从-v630-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_allow_batch_cop\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_allow_batch_cop-从-v40-版本开始引入)\n- [TiDB 4.0.3 Release Notes](/releases/release-4.0.3.md)\n- [TiDB 4.0 RC.2 Release Notes](/releases/release-4.0.0-rc.2.md)\n\n### tidb_allow_fallback_to_tikv\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_allow_fallback_to_tikv-从-v50-版本开始引入)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_allow_function_for_expression_index\n\n引用该变量的文档：\n\n- [CREATE INDEX](/sql-statements/sql-statement-create-index.md)\n- [TiDB 功能概览](/basic-features.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_allow_function_for_expression_index-从-v520-版本开始引入)\n\n### tidb_allow_mpp\n\n引用该变量的文档：\n\n- [HTAP 深入探索指南](/explore-htap.md)\n- [TiFlash 查询结果物化](/tiflash/tiflash-results-materialization.md)\n- [使用 MPP 模式](/tiflash/use-tiflash-mpp-mode.md)\n- [用 EXPLAIN 查看 MPP 模式查询的执行计划](/explain-mpp.md)\n- [系统变量](/system-variables.md#tidb_allow_mpp-从-v50-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_allow_remove_auto_inc\n\n引用该变量的文档：\n\n- [AUTO_INCREMENT](/auto-increment.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [与 MySQL 兼容性对比](/mysql-compatibility.md)\n- [已知的第三方工具兼容问题](/develop/dev-guide-third-party-tools-compatibility.md)\n- [系统变量](/system-variables.md#tidb_allow_remove_auto_inc-从-v2118-和-v304-版本开始引入)\n- [TiDB 3.0.4 Release Notes](/releases/release-3.0.4.md)\n- [TiDB 2.1.18 Release Notes](/releases/release-2.1.18.md)\n\n### tidb_allow_tiflash_cop\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_allow_tiflash_cop-从-v730-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n\n### tidb_analyze_column_options\n\n引用该变量的文档：\n\n- [使用 TiUP bench 组件压测 TiDB](/tiup/tiup-bench.md)\n- [如何对 TiDB 进行 CH-benCHmark 测试](/benchmark/benchmark-tidb-using-ch.md)\n- [系统变量](/system-variables.md#tidb_analyze_column_options-从-v830-版本开始引入)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n\n### tidb_analyze_distsql_scan_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_analyze_distsql_scan_concurrency-从-v760-版本开始引入)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n\n### tidb_analyze_partition_concurrency\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_analyze_partition_concurrency)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 8.1.1 Release Notes](/releases/release-8.1.1.md)\n- [TiDB 7.5.3 Release Notes](/releases/release-7.5.3.md)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n\n### tidb_analyze_skip_column_types\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_analyze_skip_column_types-从-v720-版本开始引入)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n\n### tidb_analyze_version\n\n引用该变量的文档：\n\n- [ANALYZE_STATUS](/information-schema/information-schema-analyze-status.md)\n- [SHOW ANALYZE STATUS](/sql-statements/sql-statement-show-analyze-status.md)\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_analyze_version-从-v510-版本开始引入)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n- [TiDB 5.2.4 Release Notes](/releases/release-5.2.4.md)\n- [TiDB 5.1.4 Release Notes](/releases/release-5.1.4.md)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n\n### tidb_auto_analyze_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_auto_analyze_concurrency-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_auto_analyze_end_time\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 集群问题导图](/tidb-troubleshooting-map.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_auto_analyze_end_time)\n- [读写延迟增加](/troubleshoot-cpu-issues.md)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n\n### tidb_auto_analyze_partition_batch_size\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_auto_analyze_partition_batch_size-从-v640-版本开始引入)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_auto_analyze_ratio\n\n引用该变量的文档：\n\n- [SHOW STATS_HEALTHY](/sql-statements/sql-statement-show-stats-healthy.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [TiDB 集群问题导图](/tidb-troubleshooting-map.md)\n- [使用 `EXPLAIN` 解读执行计划](/explain-walkthrough.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_auto_analyze_ratio)\n- [读写延迟增加](/troubleshoot-cpu-issues.md)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 2.0.1 Release Notes](/releases/release-2.0.1.md)\n\n### tidb_auto_analyze_start_time\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 集群问题导图](/tidb-troubleshooting-map.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_auto_analyze_start_time)\n- [读写延迟增加](/troubleshoot-cpu-issues.md)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n\n### tidb_auto_build_stats_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_auto_build_stats_concurrency-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_backoff_lock_fast\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_backoff_lock_fast)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_backoff_weight\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_backoff_weight)\n- [TiDB 3.0.3 Release Notes](/releases/release-3.0.3.md)\n\n### tidb_batch_commit\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_batch_commit)\n- [TiDB 3.0 Beta Release Notes](/releases/release-3.0-beta.md)\n- [TiDB 3.0 GA Release Notes](/releases/release-3.0-ga.md)\n\n### tidb_batch_delete\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_batch_delete)\n\n### tidb_batch_insert\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_batch_insert)\n- [TiDB 2.0.6 Release Notes](/releases/release-2.0.6.md)\n\n### tidb_batch_pending_tiflash_count\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_batch_pending_tiflash_count-从-v60-版本开始引入)\n\n### tidb_broadcast_join_threshold_count\n\n引用该变量的文档：\n\n- [TiFlash 性能调优](/tiflash/tune-tiflash-performance.md)\n- [使用 MPP 模式](/tiflash/use-tiflash-mpp-mode.md)\n- [用 EXPLAIN 查看 MPP 模式查询的执行计划](/explain-mpp.md)\n- [系统变量](/system-variables.md#tidb_broadcast_join_threshold_count-从-v50-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_broadcast_join_threshold_size\n\n引用该变量的文档：\n\n- [TiFlash 性能调优](/tiflash/tune-tiflash-performance.md)\n- [使用 MPP 模式](/tiflash/use-tiflash-mpp-mode.md)\n- [用 EXPLAIN 查看 MPP 模式查询的执行计划](/explain-mpp.md)\n- [系统变量](/system-variables.md#tidb_broadcast_join_threshold_size-从-v50-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_build_sampling_stats_concurrency\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_build_sampling_stats_concurrency-从-v750-版本开始引入)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n\n### tidb_build_stats_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_build_stats_concurrency)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n\n### tidb_capture_plan_baselines\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_capture_plan_baselines-从-v40-版本开始引入)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n- [TiDB 4.0 RC.1 Release Notes](/releases/release-4.0.0-rc.1.md)\n\n### tidb_cdc_write_source\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_cdc_write_source-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_check_mb4_value_in_utf8\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [升级与升级后常见问题](/faq/upgrade-faq.md)\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#tidb_check_mb4_value_in_utf8)\n\n### tidb_checksum_table_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)\n- [系统变量](/system-variables.md#tidb_checksum_table_concurrency)\n\n### tidb_cloud_storage_uri\n\n引用该变量的文档：\n\n- [IMPORT INTO](/sql-statements/sql-statement-import-into.md)\n- [TiDB 全局排序](/tidb-global-sort.md)\n- [系统变量](/system-variables.md#tidb_cloud_storage_uri-从-v740-版本开始引入)\n- [TiDB 8.1.1 Release Notes](/releases/release-8.1.1.md)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### tidb_committer_concurrency\n\n引用该变量的文档：\n\n- [TiDB 重要监控指标详解](/grafana-tidb-dashboard.md)\n- [系统变量](/system-variables.md#tidb_committer_concurrency-从-v610-版本开始引入)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_config\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_config)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n- [TiDB 1.1 Beta Release Notes](/releases/release-1.1-beta.md)\n\n### tidb_constraint_check_in_place\n\n引用该变量的文档：\n\n- [COMMIT](/sql-statements/sql-statement-commit.md)\n- [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 事务概览](/transaction-overview.md)\n- [系统变量](/system-variables.md#tidb_constraint_check_in_place)\n- [约束](/constraints.md)\n- [TiDB 2.1.5 Release Notes](/releases/release-2.1.5.md)\n\n### tidb_constraint_check_in_place_pessimistic\n\n引用该变量的文档：\n\n- [SAVEPOINT](/sql-statements/sql-statement-savepoint.md)\n- [TiDB 悲观事务模式](/pessimistic-transaction.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [系统变量](/system-variables.md#tidb_constraint_check_in_place_pessimistic-从-v630-版本开始引入)\n- [约束](/constraints.md)\n- [错误码与故障诊断](/error-codes.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_cost_model_version\n\n引用该变量的文档：\n\n- [代价模型](/cost-model.md)\n- [系统变量](/system-variables.md#tidb_cost_model_version-从-v620-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_current_ts\n\n引用该变量的文档：\n\n- [FLASHBACK CLUSTER](/sql-statements/sql-statement-flashback-cluster.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 中的 TimeStamp Oracle (TSO)](/tso.md)\n- [TiDB 主从集群数据校验和快照读](/ticdc/ticdc-upstream-downstream-check.md)\n- [TiDB 特有的函数](/functions-and-operators/tidb-functions.md)\n- [基于主备集群的容灾方案](/dr-secondary-cluster.md)\n- [系统变量](/system-variables.md#tidb_current_ts)\n- [通过系统变量 `tidb_external_ts` 读取历史数据](/tidb-external-ts.md)\n\n### tidb_ddl_disk_quota\n\n引用该变量的文档：\n\n- [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)\n- [系统变量](/system-variables.md#tidb_ddl_disk_quota-从-v630-版本开始引入)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_ddl_enable_fast_reorg\n\n引用该变量的文档：\n\n- [ADMIN SHOW DDL [JOBS|JOB QUERIES]](/sql-statements/sql-statement-admin-show-ddl.md)\n- [CREATE INDEX](/sql-statements/sql-statement-create-index.md)\n- [IMPORT INTO](/sql-statements/sql-statement-import-into.md)\n- [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)\n- [TiDB 功能概览](/basic-features.md)\n- [TiDB 环境与系统配置检查](/check-before-deployment.md)\n- [TiDB 软件和硬件环境需求](/hardware-and-software-requirements.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.1.2 Release Notes](/releases/release-8.1.2.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.5.5 Release Notes](/releases/release-7.5.5.md)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_ddl_error_count_limit\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)\n- [系统变量](/system-variables.md#tidb_ddl_error_count_limit)\n\n### tidb_ddl_flashback_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_ddl_flashback_concurrency-从-v630-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_ddl_reorg_batch_size\n\n引用该变量的文档：\n\n- [ADMIN ALTER DDL JOBS](/sql-statements/sql-statement-admin-alter-ddl.md)\n- [ADMIN SHOW DDL [JOBS|JOB QUERIES]](/sql-statements/sql-statement-admin-show-ddl.md)\n- [CREATE INDEX](/sql-statements/sql-statement-create-index.md)\n- [DDL 语句的执行原理及最佳实践](/ddl-introduction.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)\n- [使用 `EXPLAIN` 解读执行计划](/explain-walkthrough.md)\n- [性能调优最佳实践](/develop/dev-guide-optimize-sql-best-practices.md)\n- [系统变量](/system-variables.md#tidb_ddl_reorg_batch_size)\n- [读写延迟增加](/troubleshoot-cpu-issues.md)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 3.0.3 Release Notes](/releases/release-3.0.3.md)\n- [TiDB 2.1.4 Release Notes](/releases/release-2.1.4.md)\n\n### tidb_ddl_reorg_max_write_speed\n\n引用该变量的文档：\n\n- [ADMIN ALTER DDL JOBS](/sql-statements/sql-statement-admin-alter-ddl.md)\n- [ADMIN SHOW DDL [JOBS|JOB QUERIES]](/sql-statements/sql-statement-admin-show-ddl.md)\n- [系统变量](/system-variables.md#tidb_ddl_reorg_max_write_speed-从-v755-和-v850-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 7.5.5 Release Notes](/releases/release-7.5.5.md)\n\n### tidb_ddl_reorg_priority\n\n引用该变量的文档：\n\n- [CREATE INDEX](/sql-statements/sql-statement-create-index.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)\n- [系统变量](/system-variables.md#tidb_ddl_reorg_priority)\n- [读写延迟增加](/troubleshoot-cpu-issues.md)\n- [TiDB 2.1 RC1 Release Notes](/releases/release-2.1-rc.1.md)\n\n### tidb_ddl_reorg_worker_cnt\n\n引用该变量的文档：\n\n- [ADMIN ALTER DDL JOBS](/sql-statements/sql-statement-admin-alter-ddl.md)\n- [ADMIN SHOW DDL [JOBS|JOB QUERIES]](/sql-statements/sql-statement-admin-show-ddl.md)\n- [CREATE INDEX](/sql-statements/sql-statement-create-index.md)\n- [DDL 语句的执行原理及最佳实践](/ddl-introduction.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)\n- [使用 `EXPLAIN` 解读执行计划](/explain-walkthrough.md)\n- [性能调优最佳实践](/develop/dev-guide-optimize-sql-best-practices.md)\n- [系统变量](/system-variables.md#tidb_ddl_reorg_worker_cnt)\n- [读写延迟增加](/troubleshoot-cpu-issues.md)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 3.0.3 Release Notes](/releases/release-3.0.3.md)\n- [TiDB 2.1.4 Release Notes](/releases/release-2.1.4.md)\n\n### tidb_default_string_match_selectivity\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_default_string_match_selectivity-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_disable_txn_auto_retry\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 乐观事务模型](/optimistic-transaction.md)\n- [TiDB 事务概览](/transaction-overview.md)\n- [乐观事务模型下写写冲突问题排查](/troubleshoot-write-conflicts.md)\n- [如何用 Sysbench 测试 TiDB](/benchmark/benchmark-tidb-using-sysbench.md)\n- [系统变量](/system-variables.md#tidb_disable_txn_auto_retry)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 3.0.0-rc.2 Release Notes](/releases/release-3.0.0-rc.2.md)\n- [TiDB 3.0 GA Release Notes](/releases/release-3.0-ga.md)\n- [TiDB 2.1 Beta Release Notes](/releases/release-2.1-beta.md)\n- [TiDB 2.1 GA Release Notes](/releases/release-2.1-ga.md)\n- [TiDB 2.0.5 Release Notes](/releases/release-2.0.5.md)\n\n### tidb_distsql_scan_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [SLOW_QUERY](/information-schema/information-schema-slow-query.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [TiDB 最佳实践](/best-practices/tidb-best-practices.md)\n- [使用 Dumpling 导出数据](/dumpling-overview.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_distsql_scan_concurrency)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_dml_batch_size\n\n引用该变量的文档：\n\n- [LOAD DATA](/sql-statements/sql-statement-load-data.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_dml_batch_size)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n- [TiDB 4.0 Beta Release Notes](/releases/release-4.0.0-beta.md)\n\n### tidb_dml_type\n\n引用该变量的文档：\n\n- [IMPORT INTO 和 TiDB Lightning 对比](/tidb-lightning/import-into-vs-tidb-lightning.md)\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [TiDB 功能概览](/basic-features.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [定位消耗系统资源多的查询](/identify-expensive-queries.md)\n- [系统变量](/system-variables.md#tidb_dml_type-从-v800-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_enable_1pc\n\n引用该变量的文档：\n\n- [TiDB 事务概览](/transaction-overview.md)\n- [TiDB 功能概览](/basic-features.md)\n- [TiDB 集群管理常见问题](/faq/manage-cluster-faq.md)\n- [系统变量](/system-variables.md#tidb_enable_1pc-从-v50-版本开始引入)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_enable_analyze_snapshot\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_analyze_snapshot-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_enable_async_commit\n\n引用该变量的文档：\n\n- [TiDB 事务概览](/transaction-overview.md)\n- [TiDB 功能概览](/basic-features.md)\n- [TiDB 集群管理常见问题](/faq/manage-cluster-faq.md)\n- [系统变量](/system-variables.md#tidb_enable_async_commit-从-v50-版本开始引入)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n- [TiDB 5.0 RC Release Notes](/releases/release-5.0.0-rc.md)\n\n### tidb_enable_async_merge_global_stats\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_async_merge_global_stats-从-v750-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 8.1.1 Release Notes](/releases/release-8.1.1.md)\n- [TiDB 7.5.3 Release Notes](/releases/release-7.5.3.md)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n\n### tidb_enable_auto_analyze\n\n引用该变量的文档：\n\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [使用 PLAN REPLAYER 保存和恢复集群现场信息](/sql-plan-replayer.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_enable_auto_analyze-从-v610-版本开始引入)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_enable_auto_analyze_priority_queue\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_enable_auto_analyze_priority_queue-从-v800-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_enable_auto_increment_in_generated\n\n引用该变量的文档：\n\n- [CREATE INDEX](/sql-statements/sql-statement-create-index.md)\n- [系统变量](/system-variables.md#tidb_enable_auto_increment_in_generated)\n- [TiDB 5.2 Release Notes](/releases/release-5.2.0.md)\n\n### tidb_enable_batch_dml\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_batch_dml)\n\n### tidb_enable_cascades_planner\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 功能概览](/basic-features.md)\n- [系统变量](/system-variables.md#tidb_enable_cascades_planner)\n- [TiDB 3.0 Beta Release Notes](/releases/release-3.0-beta.md)\n\n### tidb_enable_check_constraint\n\n引用该变量的文档：\n\n- [CHECK\\_CONSTRAINTS](/information-schema/information-schema-check-constraints.md)\n- [TIDB\\_CHECK\\_CONSTRAINTS](/information-schema/information-schema-tidb-check-constraints.md)\n- [系统变量](/system-variables.md#tidb_enable_check_constraint-从-v720-版本开始引入)\n- [约束](/constraints.md)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n\n### tidb_enable_chunk_rpc\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_enable_chunk_rpc-从-v40-版本开始引入)\n\n### tidb_enable_clustered_index\n\n引用该变量的文档：\n\n- [TiDB Lightning 配置参数](/tidb-lightning/tidb-lightning-configuration.md)\n- [TiDB 备份与恢复概述](/br/backup-and-restore-overview.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [创建表](/develop/dev-guide-create-table.md)\n- [概述](/develop/dev-guide-schema-design-overview.md)\n- [系统变量](/system-variables.md#tidb_enable_clustered_index-从-v50-版本开始引入)\n- [聚簇索引](/clustered-indexes.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n- [TiDB 5.0 RC Release Notes](/releases/release-5.0.0-rc.md)\n\n### tidb_enable_collect_execution_info\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TIDB_INDEX_USAGE](/information-schema/information-schema-tidb-index-usage.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [慢查询日志](/identify-slow-queries.md)\n- [系统变量](/system-variables.md#tidb_enable_collect_execution_info)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 4.0.2 Release Notes](/releases/release-4.0.2.md)\n\n### tidb_enable_column_tracking\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_column_tracking-从-v540-版本开始引入)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_enable_ddl\n\n引用该变量的文档：\n\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [系统变量](/system-variables.md#tidb_enable_ddl-从-v630-版本开始引入)\n- [通过 TiUP 部署 TiDB 集群的拓扑文件配置](/tiup/tiup-cluster-topology-reference.md)\n\n### tidb_enable_dist_task\n\n引用该变量的文档：\n\n- [ADMIN ALTER DDL JOBS](/sql-statements/sql-statement-admin-alter-ddl.md)\n- [ADMIN SHOW DDL [JOBS|JOB QUERIES]](/sql-statements/sql-statement-admin-show-ddl.md)\n- [IMPORT INTO](/sql-statements/sql-statement-import-into.md)\n- [TiDB 全局排序](/tidb-global-sort.md)\n- [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)\n- [平滑升级 TiDB](/smooth-upgrade-tidb.md)\n- [系统变量](/system-variables.md#tidb_enable_dist_task-从-v710-版本开始引入)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_enable_enhanced_security\n\n引用该变量的文档：\n\n- [IMPORT INTO](/sql-statements/sql-statement-import-into.md)\n- [TiDB Dashboard 用户管理](/dashboard/dashboard-user.md)\n- [TiDB 功能概览](/basic-features.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_enable_enhanced_security)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n\n### tidb_enable_exchange_partition\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_exchange_partition)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_enable_extended_stats\n\n引用该变量的文档：\n\n- [扩展统计信息](/extended-statistics.md)\n- [系统变量](/system-variables.md#tidb_enable_extended_stats)\n\n### tidb_enable_external_ts_read\n\n引用该变量的文档：\n\n- [TiDB 主从集群数据校验和快照读](/ticdc/ticdc-upstream-downstream-check.md)\n- [基于主备集群的容灾方案](/dr-secondary-cluster.md)\n- [系统变量](/system-variables.md#tidb_enable_external_ts_read-从-v640-版本开始引入)\n- [通过系统变量 `tidb_external_ts` 读取历史数据](/tidb-external-ts.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_enable_fast_analyze\n\n引用该变量的文档：\n\n- [TiDB 功能概览](/basic-features.md)\n- [扩展统计信息](/extended-statistics.md)\n- [系统变量](/system-variables.md#tidb_enable_fast_analyze)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n- [TiDB 3.0.0-rc.1 Release Notes](/releases/release-3.0.0-rc.1.md)\n\n### tidb_enable_fast_create_table\n\n引用该变量的文档：\n\n- [提升 TiDB 建表性能](/accelerated-table-creation.md)\n- [系统变量](/system-variables.md#tidb_enable_fast_create_table-从-v800-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_enable_fast_table_check\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_fast_table_check-从-v720-版本开始引入)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n\n### tidb_enable_foreign_key\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_foreign_key-从-v630-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_enable_gc_aware_memory_track\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_gc_aware_memory_track)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_enable_global_index\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_global_index-从-v760-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n\n### tidb_enable_gogc_tuner\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_gogc_tuner-从-v640-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_enable_historical_stats\n\n引用该变量的文档：\n\n- [使用 PLAN REPLAYER 保存和恢复集群现场信息](/sql-plan-replayer.md)\n- [系统变量](/system-variables.md#tidb_enable_historical_stats)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_enable_historical_stats_for_capture\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_historical_stats_for_capture)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_enable_index_merge\n\n引用该变量的文档：\n\n- [Optimizer Hints](/optimizer-hints.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [用 EXPLAIN 查看索引合并的 SQL 执行计划](/explain-index-merge.md)\n- [系统变量](/system-variables.md#tidb_enable_index_merge-从-v40-版本开始引入)\n- [索引的选择](/choose-index.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_enable_index_merge_join\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_index_merge_join)\n\n### tidb_enable_inl_join_inner_multi_pattern\n\n引用该变量的文档：\n\n- [Optimizer Hints](/optimizer-hints.md)\n- [系统变量](/system-variables.md#tidb_enable_inl_join_inner_multi_pattern-从-v700-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_enable_instance_plan_cache\n\n引用该变量的文档：\n\n- [TiDB 功能概览](/basic-features.md)\n- [系统变量](/system-variables.md#tidb_enable_instance_plan_cache-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_enable_lazy_cursor_fetch\n\n引用该变量的文档：\n\n- [开发 Java 应用使用 TiDB 的最佳实践](/best-practices/java-app-best-practices.md)\n- [系统变量](/system-variables.md#tidb_enable_lazy_cursor_fetch-从-v830-版本开始引入)\n- [连接池与连接参数](/develop/dev-guide-connection-parameters.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n\n### tidb_enable_legacy_instance_scope\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_legacy_instance_scope-从-v600-版本开始引入)\n\n### tidb_enable_list_partition\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_list_partition-从-v50-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_enable_local_txn\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_local_txn)\n\n### tidb_enable_metadata_lock\n\n引用该变量的文档：\n\n- [元数据锁](/metadata-lock.md)\n- [系统变量](/system-variables.md#tidb_enable_metadata_lock-从-v630-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_enable_mutation_checker\n\n引用该变量的文档：\n\n- [数据索引一致性报错](/troubleshoot-data-inconsistency-errors.md)\n- [系统变量](/system-variables.md#tidb_enable_mutation_checker-从-v600-版本开始引入)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_enable_new_cost_interface\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_new_cost_interface-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_enable_new_only_full_group_by_check\n\n引用该变量的文档：\n\n- [SQL 模式](/sql-mode.md)\n- [系统变量](/system-variables.md#tidb_enable_new_only_full_group_by_check-从-v610-版本开始引入)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_enable_non_prepared_plan_cache\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_non_prepared_plan_cache)\n- [非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_enable_non_prepared_plan_cache_for_dml\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_non_prepared_plan_cache_for_dml-从-v710-版本开始引入)\n- [非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_enable_noop_functions\n\n引用该变量的文档：\n\n- [SELECT](/sql-statements/sql-statement-select.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 悲观事务模式](/pessimistic-transaction.md)\n- [信息函数](/functions-and-operators/information-functions.md)\n- [系统变量](/system-variables.md#tidb_enable_noop_functions-从-v40-版本开始引入)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_enable_noop_variables\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_noop_variables-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_enable_null_aware_anti_join\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_null_aware_anti_join-从-v630-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_enable_ordered_result_mode\n\n引用该变量的文档：\n\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [系统变量](/system-variables.md#tidb_enable_ordered_result_mode)\n- [TiDB 7.1.3 Release Notes](/releases/release-7.1.3.md)\n- [TiDB 6.5.6 Release Notes](/releases/release-6.5.6.md)\n\n### tidb_enable_outer_join_reorder\n\n引用该变量的文档：\n\n- [Join Reorder 算法简介](/join-reorder.md)\n- [系统变量](/system-variables.md#tidb_enable_outer_join_reorder-从-v610-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n- [TiDB 6.1.1 Release Notes](/releases/release-6.1.1.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_enable_paging\n\n引用该变量的文档：\n\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [系统变量](/system-variables.md#tidb_enable_paging-从-v540-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_enable_parallel_apply\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_parallel_apply-从-v50-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n- [TiDB 7.1.1 Release Notes](/releases/release-7.1.1.md)\n- [TiDB 6.5.4 Release Notes](/releases/release-6.5.4.md)\n- [TiDB 5.4.1 Release Notes](/releases/release-5.4.1.md)\n\n### tidb_enable_parallel_hashagg_spill\n\n引用该变量的文档：\n\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [系统变量](/system-variables.md#tidb_enable_parallel_hashagg_spill-从-v800-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 8.1.1 Release Notes](/releases/release-8.1.1.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_enable_pipelined_window_function\n\n引用该变量的文档：\n\n- [窗口函数](/functions-and-operators/window-functions.md)\n- [系统变量](/system-variables.md#tidb_enable_pipelined_window_function)\n\n### tidb_enable_plan_cache_for_param_limit\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_plan_cache_for_param_limit-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_enable_plan_cache_for_subquery\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_plan_cache_for_subquery-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_enable_plan_replayer_capture\n\n引用该变量的文档：\n\n- [使用 PLAN REPLAYER 保存和恢复集群现场信息](/sql-plan-replayer.md)\n- [系统变量](/system-variables.md#tidb_enable_plan_replayer_capture)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_enable_plan_replayer_continuous_capture\n\n引用该变量的文档：\n\n- [使用 PLAN REPLAYER 保存和恢复集群现场信息](/sql-plan-replayer.md)\n- [系统变量](/system-variables.md#tidb_enable_plan_replayer_continuous_capture-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_enable_prepared_plan_cache\n\n引用该变量的文档：\n\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [TiDB Dashboard 常见问题](/dashboard/dashboard-faq.md)\n- [如何用 Sysbench 测试 TiDB](/benchmark/benchmark-tidb-using-sysbench.md)\n- [系统变量](/system-variables.md#tidb_enable_prepared_plan_cache-从-v610-版本开始引入)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.1.4 Release Notes](/releases/release-7.1.4.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_enable_prepared_plan_cache_memory_monitor\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_prepared_plan_cache_memory_monitor-从-v640-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_enable_pseudo_for_outdated_stats\n\n引用该变量的文档：\n\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [系统变量](/system-variables.md#tidb_enable_pseudo_for_outdated_stats-从-v530-版本开始引入)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n\n### tidb_enable_rate_limit_action\n\n引用该变量的文档：\n\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [系统变量](/system-variables.md#tidb_enable_rate_limit_action)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_enable_resource_control\n\n引用该变量的文档：\n\n- [ALTER RESOURCE GROUP](/sql-statements/sql-statement-alter-resource-group.md)\n- [CREATE RESOURCE GROUP](/sql-statements/sql-statement-create-resource-group.md)\n- [DROP RESOURCE GROUP](/sql-statements/sql-statement-drop-resource-group.md)\n- [SET RESOURCE GROUP](/sql-statements/sql-statement-set-resource-group.md)\n- [TiKV 配置文件描述](/tikv-configuration-file.md)\n- [`CALIBRATE RESOURCE`](/sql-statements/sql-statement-calibrate-resource.md)\n- [使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)\n- [系统变量](/system-variables.md#tidb_enable_resource_control-从-v660-版本开始引入)\n- [错误码与故障诊断](/error-codes.md)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_enable_reuse_chunk\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_reuse_chunk-从-v640-版本开始引入)\n- [TiDB 6.5.1 Release Notes](/releases/release-6.5.1.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_enable_row_level_checksum\n\n引用该变量的文档：\n\n- [TiCDC 单行数据正确性校验](/ticdc/ticdc-integrity-check.md)\n- [TiDB 特有的函数](/functions-and-operators/tidb-functions.md)\n- [系统变量](/system-variables.md#tidb_enable_row_level_checksum-从-v710-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_enable_shared_lock_promotion\n\n引用该变量的文档：\n\n- [TiDB 悲观事务模式](/pessimistic-transaction.md)\n- [系统变量](/system-variables.md#tidb_enable_shared_lock_promotion-从-v830-版本开始引入)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n\n### tidb_enable_slow_log\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB Dashboard 慢查询页面](/dashboard/dashboard-slow-query.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [慢查询日志](/identify-slow-queries.md)\n- [系统变量](/system-variables.md#tidb_enable_slow_log)\n\n### tidb_enable_stats_owner\n\n引用该变量的文档：\n\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_enable_stats_owner-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_enable_stmt_summary\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_enable_stmt_summary-从-v304-版本开始引入)\n- [TiDB 3.0.4 Release Notes](/releases/release-3.0.4.md)\n\n### tidb_enable_strict_double_type_check\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_strict_double_type_check-从-v50-版本开始引入)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_enable_table_partition\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_enable_table_partition)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 4.0.13 Release Notes](/releases/release-4.0.13.md)\n- [TiDB 3.0.8 Release Notes](/releases/release-3.0.8.md)\n\n### tidb_enable_telemetry\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_enable_telemetry-从-v402-版本开始引入从-v810-版本开始废弃)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n- [TiDB 6.5.1 Release Notes](/releases/release-6.5.1.md)\n- [TiDB 6.1.5 Release Notes](/releases/release-6.1.5.md)\n\n### tidb_enable_tiflash_read_for_write_stmt\n\n引用该变量的文档：\n\n- [使用 TiDB 读取 TiFlash](/tiflash/use-tidb-to-read-tiflash.md)\n- [系统变量](/system-variables.md#tidb_enable_tiflash_read_for_write_stmt-从-v630-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_enable_tmp_storage_on_oom\n\n引用该变量的文档：\n\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [为 TiDB 落盘文件开启加密](/enable-disk-spill-encrypt.md)\n- [开发 Java 应用使用 TiDB 的最佳实践](/best-practices/java-app-best-practices.md)\n- [用 EXPLAIN 查看 JOIN 查询的执行计划](/explain-joins.md)\n- [系统变量](/system-variables.md#tidb_enable_tmp_storage_on_oom)\n- [连接池与连接参数](/develop/dev-guide-connection-parameters.md)\n\n### tidb_enable_top_sql\n\n引用该变量的文档：\n\n- [TiDB Dashboard Top SQL 页面](/dashboard/top-sql.md)\n- [系统变量](/system-variables.md#tidb_enable_top_sql-从-v540-版本开始引入)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_enable_tso_follower_proxy\n\n引用该变量的文档：\n\n- [PD 微服务](/pd-microservices.md)\n- [TiDB 功能概览](/basic-features.md)\n- [系统变量](/system-variables.md#tidb_enable_tso_follower_proxy-从-v530-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n\n### tidb_enable_unsafe_substitute\n\n引用该变量的文档：\n\n- [生成列](/generated-columns.md)\n- [系统变量](/system-variables.md#tidb_enable_unsafe_substitute-从-v630-版本开始引入)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_enable_vectorized_expression\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_enable_vectorized_expression-从-v40-版本开始引入)\n- [TiDB 5.4.1 Release Notes](/releases/release-5.4.1.md)\n- [TiDB 5.2.4 Release Notes](/releases/release-5.2.4.md)\n- [TiDB 5.1.4 Release Notes](/releases/release-5.1.4.md)\n- [TiDB 5.0.6 Release Notes](/releases/release-5.0.6.md)\n- [TiDB 4.0.16 Release Notes](/releases/release-4.0.16.md)\n\n### tidb_enable_window_function\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [窗口函数](/functions-and-operators/window-functions.md)\n- [系统变量](/system-variables.md#tidb_enable_window_function)\n- [TiDB 3.0.8 Release Notes](/releases/release-3.0.8.md)\n\n### tidb_enforce_mpp\n\n引用该变量的文档：\n\n- [HTAP 深入探索指南](/explore-htap.md)\n- [Stale Read 功能的使用场景](/stale-read.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [TiFlash 常见问题](/tiflash/troubleshoot-tiflash.md)\n- [TiFlash 性能调优](/tiflash/tune-tiflash-performance.md)\n- [TiFlash 查询结果物化](/tiflash/tiflash-results-materialization.md)\n- [使用 MPP 模式](/tiflash/use-tiflash-mpp-mode.md)\n- [系统变量](/system-variables.md#tidb_enforce_mpp-从-v51-版本开始引入)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n- [TiDB 5.4.2 Release Notes](/releases/release-5.4.2.md)\n- [TiDB 5.1.5 Release Notes](/releases/release-5.1.5.md)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n- [TiDB 5.0.4 Release Notes](/releases/release-5.0.4.md)\n\n### tidb_evolve_plan_baselines\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_evolve_plan_baselines-从-v40-版本开始引入)\n- [TiDB 5.2 Release Notes](/releases/release-5.2.0.md)\n\n### tidb_evolve_plan_task_end_time\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_evolve_plan_task_end_time-从-v40-版本开始引入)\n\n### tidb_evolve_plan_task_max_time\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_evolve_plan_task_max_time-从-v40-版本开始引入)\n\n### tidb_evolve_plan_task_start_time\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_evolve_plan_task_start_time-从-v40-版本开始引入)\n\n### tidb_executor_concurrency\n\n引用该变量的文档：\n\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [系统变量](/system-variables.md#tidb_executor_concurrency-从-v50-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_expensive_query_time_threshold\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [定位消耗系统资源多的查询](/identify-expensive-queries.md)\n- [系统变量](/system-variables.md#tidb_expensive_query_time_threshold)\n\n### tidb_expensive_txn_time_threshold\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_expensive_txn_time_threshold-从-v720-版本开始引入)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n\n### tidb_external_ts\n\n引用该变量的文档：\n\n- [Stale Read 功能的使用场景](/stale-read.md)\n- [TiDB 主从集群数据校验和快照读](/ticdc/ticdc-upstream-downstream-check.md)\n- [基于主备集群的容灾方案](/dr-secondary-cluster.md)\n- [系统变量](/system-variables.md#tidb_external_ts-从-v640-版本开始引入)\n- [通过系统变量 `tidb_external_ts` 读取历史数据](/tidb-external-ts.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_force_priority\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [系统变量](/system-variables.md#tidb_force_priority)\n- [TiDB 2.1.5 Release Notes](/releases/release-2.1.5.md)\n- [TiDB 2.1 RC3 Release Notes](/releases/release-2.1-rc.3.md)\n\n### tidb_gc_concurrency\n\n引用该变量的文档：\n\n- [GC 配置](/garbage-collection-configuration.md)\n- [系统变量](/system-variables.md#tidb_gc_concurrency-从-v50-版本开始引入)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_gc_enable\n\n引用该变量的文档：\n\n- [GC 配置](/garbage-collection-configuration.md)\n- [TiDB 集群问题导图](/tidb-troubleshooting-map.md)\n- [从 TiDB 集群迁移数据至兼容 MySQL 的数据库](/migrate-from-tidb-to-mysql.md)\n- [从 TiDB 集群迁移数据至另一 TiDB 集群](/migrate-from-tidb-to-tidb.md)\n- [基于主备集群的容灾方案](/dr-secondary-cluster.md)\n- [搭建双集群主从复制](/replicate-between-primary-and-secondary-clusters.md)\n- [系统变量](/system-variables.md#tidb_gc_enable-从-v50-版本开始引入)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_gc_life_time\n\n引用该变量的文档：\n\n- [FLASHBACK CLUSTER](/sql-statements/sql-statement-flashback-cluster.md)\n- [FLASHBACK DATABASE](/sql-statements/sql-statement-flashback-database.md)\n- [FLASHBACK TABLE](/sql-statements/sql-statement-flashback-table.md)\n- [GC 配置](/garbage-collection-configuration.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [Stale Read](/develop/dev-guide-use-stale-read.md)\n- [TiCDC 常见问题解答](/ticdc/ticdc-faq.md)\n- [TiDB 中的各种超时](/develop/dev-guide-timeouts-in-tidb.md)\n- [TiDB 增量备份与恢复使用指南](/br/br-incremental-guide.md)\n- [TiDB 集群问题导图](/tidb-troubleshooting-map.md)\n- [TiKV MVCC 内存引擎](/tikv-in-memory-engine.md)\n- [使用 Dumpling 导出数据](/dumpling-overview.md)\n- [同步数据到 Kafka](/ticdc/ticdc-sink-to-kafka.md)\n- [系统变量](/system-variables.md#tidb_gc_life_time-从-v50-版本开始引入)\n- [通过系统变量 tidb_snapshot 读取历史数据](/read-historical-data.md)\n- [错误码与故障诊断](/error-codes.md)\n- [TiDB 6.1.1 Release Notes](/releases/release-6.1.1.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_gc_max_wait_time\n\n引用该变量的文档：\n\n- [GC 配置](/garbage-collection-configuration.md)\n- [系统变量](/system-variables.md#tidb_gc_max_wait_time-从-v610-版本开始引入)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_gc_run_interval\n\n引用该变量的文档：\n\n- [GC 配置](/garbage-collection-configuration.md)\n- [系统变量](/system-variables.md#tidb_gc_run_interval-从-v50-版本开始引入)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_gc_scan_lock_mode\n\n引用该变量的文档：\n\n- [GC 机制简介](/garbage-collection-overview.md)\n- [GC 配置](/garbage-collection-configuration.md)\n- [TiDB 功能概览](/basic-features.md)\n- [系统变量](/system-variables.md#tidb_gc_scan_lock_mode-从-v50-版本开始引入)\n- [TiDB 5.0.4 Release Notes](/releases/release-5.0.4.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_general_log\n\n引用该变量的文档：\n\n- [Data Migration 常见问题](/dm/dm-faq.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 配置参数](/command-line-flags-for-tidb-configuration.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_general_log)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_generate_binary_plan\n\n引用该变量的文档：\n\n- [TiDB 特有的函数](/functions-and-operators/tidb-functions.md)\n- [系统变量](/system-variables.md#tidb_generate_binary_plan-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_gogc_tuner_max_value\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_gogc_tuner_max_value-从-v750-版本开始引入)\n- [TiDB 7.5.5 Release Notes](/releases/release-7.5.5.md)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n\n### tidb_gogc_tuner_min_value\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_gogc_tuner_min_value-从-v750-版本开始引入)\n- [TiDB 7.5.5 Release Notes](/releases/release-7.5.5.md)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n\n### tidb_gogc_tuner_threshold\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_gogc_tuner_threshold-从-v640-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.1.4 Release Notes](/releases/release-7.1.4.md)\n- [TiDB 6.5.9 Release Notes](/releases/release-6.5.9.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_guarantee_linearizability\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_guarantee_linearizability-从-v50-版本开始引入)\n\n### tidb_hash_exchange_with_new_collation\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_hash_exchange_with_new_collation)\n\n### tidb_hash_join_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [三节点混合部署的最佳实践](/best-practices/three-nodes-hybrid-deployment.md)\n- [分析慢查询](/analyze-slow-queries.md)\n- [用 EXPLAIN 查看 JOIN 查询的执行计划](/explain-joins.md)\n- [系统变量](/system-variables.md#tidb_hash_join_concurrency)\n\n### tidb_hash_join_version\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_hash_join_version-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_hashagg_final_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_hashagg_final_concurrency)\n\n### tidb_hashagg_partial_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_hashagg_partial_concurrency)\n\n### tidb_historical_stats_duration\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_historical_stats_duration-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_idle_transaction_timeout\n\n引用该变量的文档：\n\n- [TiDB 中的各种超时](/develop/dev-guide-timeouts-in-tidb.md)\n- [TiDB 功能概览](/basic-features.md)\n- [TiDB 集群管理常见问题](/faq/manage-cluster-faq.md)\n- [系统变量](/system-variables.md#tidb_idle_transaction_timeout-从-v760-版本开始引入)\n- [连接池与连接参数](/develop/dev-guide-connection-parameters.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n\n### tidb_ignore_prepared_cache_close_stmt\n\n引用该变量的文档：\n\n- [OLTP 负载性能优化实践](/performance-tuning-practices.md)\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [TiDB 性能分析和优化](/performance-tuning-methods.md)\n- [系统变量](/system-variables.md#tidb_ignore_prepared_cache_close_stmt-从-v600-版本开始引入)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_index_join_batch_size\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [用 EXPLAIN 查看 JOIN 查询的执行计划](/explain-joins.md)\n- [系统变量](/system-variables.md#tidb_index_join_batch_size)\n\n### tidb_index_join_double_read_penalty_cost_rate\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_index_join_double_read_penalty_cost_rate-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_index_lookup_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 最佳实践](/best-practices/tidb-best-practices.md)\n- [系统变量](/system-variables.md#tidb_index_lookup_concurrency)\n\n### tidb_index_lookup_join_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [三节点混合部署的最佳实践](/best-practices/three-nodes-hybrid-deployment.md)\n- [用 EXPLAIN 查看 JOIN 查询的执行计划](/explain-joins.md)\n- [系统变量](/system-variables.md#tidb_index_lookup_join_concurrency)\n\n### tidb_index_lookup_size\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 最佳实践](/best-practices/tidb-best-practices.md)\n- [系统变量](/system-variables.md#tidb_index_lookup_size)\n\n### tidb_index_merge_intersection_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_index_merge_intersection_concurrency-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_index_serial_scan_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 最佳实践](/best-practices/tidb-best-practices.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_index_serial_scan_concurrency)\n\n### tidb_init_chunk_size\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 事务隔离级别](/transaction-isolation-levels.md)\n- [系统变量](/system-variables.md#tidb_init_chunk_size)\n- [TiDB 3.0 Beta Release Notes](/releases/release-3.0-beta.md)\n\n### tidb_instance_plan_cache_max_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_instance_plan_cache_max_size-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_instance_plan_cache_reserved_percentage\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_instance_plan_cache_reserved_percentage-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_isolation_read_engines\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiFlash 兼容性说明](/tiflash/tiflash-compatibility.md)\n- [使用 FastScan 功能](/tiflash/use-fastscan.md)\n- [使用 TiDB 读取 TiFlash](/tiflash/use-tidb-to-read-tiflash.md)\n- [使用 TiUP bench 组件压测 TiDB](/tiup/tiup-bench.md)\n- [系统变量](/system-variables.md#tidb_isolation_read_engines-从-v40-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 4.0.2 Release Notes](/releases/release-4.0.2.md)\n\n### tidb_last_ddl_info\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_last_ddl_info-从-v600-版本开始引入)\n\n### tidb_last_plan_replayer_token\n\n引用该变量的文档：\n\n- [使用 PLAN REPLAYER 保存和恢复集群现场信息](/sql-plan-replayer.md)\n- [系统变量](/system-variables.md#tidb_last_plan_replayer_token-从-v630-版本开始引入)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_last_query_info\n\n引用该变量的文档：\n\n- [使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)\n- [系统变量](/system-variables.md#tidb_last_query_info-从-v4014-版本开始引入)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n\n### tidb_last_txn_info\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_last_txn_info-从-v409-版本开始引入)\n\n### tidb_load_based_replica_read_threshold\n\n引用该变量的文档：\n\n- [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md)\n- [系统变量](/system-variables.md#tidb_load_based_replica_read_threshold-从-v700-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_load_binding_timeout\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_load_binding_timeout-从-v800-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_lock_unchanged_keys\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_lock_unchanged_keys-从-v711-和-v730-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n- [TiDB 7.1.1 Release Notes](/releases/release-7.1.1.md)\n\n### tidb_log_file_max_days\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_log_file_max_days-从-v530-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_low_resolution_tso\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_low_resolution_tso)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 3.0.0-rc.2 Release Notes](/releases/release-3.0.0-rc.2.md)\n- [TiDB 3.0 GA Release Notes](/releases/release-3.0-ga.md)\n\n### tidb_low_resolution_tso_update_interval\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_low_resolution_tso_update_interval-从-v800-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_max_auto_analyze_time\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_max_auto_analyze_time-从-v610-版本开始引入)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_max_bytes_before_tiflash_external_group_by\n\n引用该变量的文档：\n\n- [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)\n- [系统变量](/system-variables.md#tidb_max_bytes_before_tiflash_external_group_by-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_max_bytes_before_tiflash_external_join\n\n引用该变量的文档：\n\n- [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)\n- [系统变量](/system-variables.md#tidb_max_bytes_before_tiflash_external_join-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_max_bytes_before_tiflash_external_sort\n\n引用该变量的文档：\n\n- [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)\n- [系统变量](/system-variables.md#tidb_max_bytes_before_tiflash_external_sort-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_max_chunk_size\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 事务隔离级别](/transaction-isolation-levels.md)\n- [系统变量](/system-variables.md#tidb_max_chunk_size)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.1.3 Release Notes](/releases/release-7.1.3.md)\n- [TiDB 6.5.7 Release Notes](/releases/release-6.5.7.md)\n- [TiDB 2.0.9 Release Notes](/releases/release-2.0.9.md)\n\n### tidb_max_delta_schema_count\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [SQL 操作常见问题](/faq/sql-faq.md)\n- [系统变量](/system-variables.md#tidb_max_delta_schema_count)\n- [TiDB 3.0.5 Release Notes](/releases/release-3.0.5.md)\n- [TiDB 2.1.18 Release Notes](/releases/release-2.1.18.md)\n\n### tidb_max_paging_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_max_paging_size-从-v630-版本开始引入)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_max_tiflash_threads\n\n引用该变量的文档：\n\n- [TiFlash 性能调优](/tiflash/tune-tiflash-performance.md)\n- [TiFlash 配置参数](/tiflash/tiflash-configuration.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [系统变量](/system-variables.md#tidb_max_tiflash_threads-从-v610-版本开始引入)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_mem_oom_action\n\n引用该变量的文档：\n\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [系统变量](/system-variables.md#tidb_mem_oom_action-从-v610-版本开始引入)\n- [非事务 DML 语句](/non-transactional-dml.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_mem_quota_analyze\n\n引用该变量的文档：\n\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_mem_quota_analyze-从-v610-版本开始引入)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 7.5.2 Release Notes](/releases/release-7.5.2.md)\n- [TiDB 7.1.6 Release Notes](/releases/release-7.1.6.md)\n- [TiDB 6.5.10 Release Notes](/releases/release-6.5.10.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_mem_quota_apply_cache\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_mem_quota_apply_cache-从-v50-版本开始引入)\n\n### tidb_mem_quota_binding_cache\n\n引用该变量的文档：\n\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_mem_quota_binding_cache-从-v600-版本开始引入)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_mem_quota_query\n\n引用该变量的文档：\n\n- [Optimizer Hints](/optimizer-hints.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [TiDB 集群问题导图](/tidb-troubleshooting-map.md)\n- [TiFlash 查询结果物化](/tiflash/tiflash-results-materialization.md)\n- [为 TiDB 落盘文件开启加密](/enable-disk-spill-encrypt.md)\n- [定位消耗系统资源多的查询](/identify-expensive-queries.md)\n- [用 EXPLAIN 查看 JOIN 查询的执行计划](/explain-joins.md)\n- [系统变量](/system-variables.md#tidb_mem_quota_query)\n- [错误码与故障诊断](/error-codes.md)\n- [非事务 DML 语句](/non-transactional-dml.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.1.2 Release Notes](/releases/release-8.1.2.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 7.5.4 Release Notes](/releases/release-7.5.4.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n- [TiDB 7.1.6 Release Notes](/releases/release-7.1.6.md)\n- [TiDB 7.1.4 Release Notes](/releases/release-7.1.4.md)\n- [TiDB 7.1.1 Release Notes](/releases/release-7.1.1.md)\n- [TiDB 6.5.11 Release Notes](/releases/release-6.5.11.md)\n- [TiDB 6.5.7 Release Notes](/releases/release-6.5.7.md)\n- [TiDB 6.5.4 Release Notes](/releases/release-6.5.4.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n- [TiDB 4.0.10 Release Notes](/releases/release-4.0.10.md)\n\n### tidb_memory_debug_mode_alarm_ratio\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_memory_debug_mode_alarm_ratio)\n\n### tidb_memory_debug_mode_min_heap_inuse\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_memory_debug_mode_min_heap_inuse)\n\n### tidb_memory_usage_alarm_keep_record_num\n\n引用该变量的文档：\n\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [系统变量](/system-variables.md#tidb_memory_usage_alarm_keep_record_num-从-v640-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_memory_usage_alarm_ratio\n\n引用该变量的文档：\n\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [系统变量](/system-variables.md#tidb_memory_usage_alarm_ratio)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_merge_join_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_merge_join_concurrency)\n\n### tidb_merge_partition_stats_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_merge_partition_stats_concurrency)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n- [TiDB 7.1.4 Release Notes](/releases/release-7.1.4.md)\n- [TiDB 6.5.9 Release Notes](/releases/release-6.5.9.md)\n\n### tidb_metric_query_range_duration\n\n引用该变量的文档：\n\n- [Metrics Schema](/metrics-schema.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_metric_query_range_duration-从-v40-版本开始引入)\n\n### tidb_metric_query_step\n\n引用该变量的文档：\n\n- [Metrics Schema](/metrics-schema.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_metric_query_step-从-v40-版本开始引入)\n\n### tidb_min_paging_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_min_paging_size-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_mpp_store_fail_ttl\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_mpp_store_fail_ttl)\n\n### tidb_multi_statement_mode\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_multi_statement_mode-从-v4011-版本开始引入)\n- [错误码与故障诊断](/error-codes.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.1.4 Release Notes](/releases/release-7.1.4.md)\n- [TiDB 6.5.8 Release Notes](/releases/release-6.5.8.md)\n- [TiDB 5.2 Release Notes](/releases/release-5.2.0.md)\n- [TiDB 5.1.1 Release Notes](/releases/release-5.1.1.md)\n- [TiDB 5.0.3 Release Notes](/releases/release-5.0.3.md)\n- [TiDB 4.0.14 Release Notes](/releases/release-4.0.14.md)\n\n### tidb_non_prepared_plan_cache_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_non_prepared_plan_cache_size)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_nontransactional_ignore_error\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_nontransactional_ignore_error-从-v610-版本开始引入)\n- [非事务 DML 语句](/non-transactional-dml.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_opt_advanced_join_hint\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_advanced_join_hint-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_opt_agg_push_down\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiFlash 性能调优](/tiflash/tune-tiflash-performance.md)\n- [系统变量](/system-variables.md#tidb_opt_agg_push_down)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n- [TiDB 7.1.1 Release Notes](/releases/release-7.1.1.md)\n- [TiDB 6.5.4 Release Notes](/releases/release-6.5.4.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 6.1.7 Release Notes](/releases/release-6.1.7.md)\n- [TiDB 5.4.2 Release Notes](/releases/release-5.4.2.md)\n- [TiDB 5.1.5 Release Notes](/releases/release-5.1.5.md)\n- [TiDB 4.0 GA Release Notes](/releases/release-4.0-ga.md)\n\n### tidb_opt_broadcast_cartesian_join\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_broadcast_cartesian_join)\n\n### tidb_opt_concurrency_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_concurrency_factor)\n\n### tidb_opt_copcpu_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_copcpu_factor)\n\n### tidb_opt_correlation_exp_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [扩展统计信息](/extended-statistics.md)\n- [系统变量](/system-variables.md#tidb_opt_correlation_exp_factor)\n- [TiDB 3.0.0-rc.1 Release Notes](/releases/release-3.0.0-rc.1.md)\n\n### tidb_opt_correlation_threshold\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [扩展统计信息](/extended-statistics.md)\n- [系统变量](/system-variables.md#tidb_opt_correlation_threshold)\n\n### tidb_opt_cpu_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_cpu_factor)\n\n### tidb_opt_derive_topn\n\n引用该变量的文档：\n\n- [从窗口函数中推导 TopN 或 Limit](/derive-topn-from-window.md)\n- [系统变量](/system-variables.md#tidb_opt_derive_topn-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_opt_desc_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_desc_factor)\n\n### tidb_opt_disk_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_disk_factor)\n\n### tidb_opt_distinct_agg_push_down\n\n引用该变量的文档：\n\n- [Distinct 优化](/agg-distinct-optimization.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [TiFlash 性能调优](/tiflash/tune-tiflash-performance.md)\n- [系统变量](/system-variables.md#tidb_opt_distinct_agg_push_down)\n\n### tidb_opt_enable_correlation_adjustment\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_enable_correlation_adjustment)\n- [TiDB 5.2 Release Notes](/releases/release-5.2.0.md)\n\n### tidb_opt_enable_fuzzy_binding\n\n引用该变量的文档：\n\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_opt_enable_fuzzy_binding-从-v760-版本开始引入)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n\n### tidb_opt_enable_hash_join\n\n引用该变量的文档：\n\n- [Optimizer Hints](/optimizer-hints.md)\n- [系统变量](/system-variables.md#tidb_opt_enable_hash_join-从-v656v712-和-v740-版本开始引入)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n- [TiDB 7.1.2 Release Notes](/releases/release-7.1.2.md)\n- [TiDB 6.5.6 Release Notes](/releases/release-6.5.6.md)\n\n### tidb_opt_enable_late_materialization\n\n引用该变量的文档：\n\n- [延迟物化](/tiflash/tiflash-late-materialization.md)\n- [系统变量](/system-variables.md#tidb_opt_enable_late_materialization-从-v700-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_opt_enable_mpp_shared_cte_execution\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_enable_mpp_shared_cte_execution-从-v720-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n\n### tidb_opt_enable_non_eval_scalar_subquery\n\n引用该变量的文档：\n\n- [EXPLAIN](/sql-statements/sql-statement-explain.md)\n- [使用 `EXPLAIN` 解读执行计划](/explain-walkthrough.md)\n- [系统变量](/system-variables.md#tidb_opt_enable_non_eval_scalar_subquery-从-v730-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n\n### tidb_opt_fix_control\n\n引用该变量的文档：\n\n- [Optimizer Fix Controls](/optimizer-fix-controls.md)\n- [控制执行计划](/control-execution-plan.md)\n- [系统变量](/system-variables.md#tidb_opt_fix_control-从-v653-和-v710-版本开始引入)\n- [非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_opt_force_inline_cte\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_force_inline_cte-从-v630-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_opt_insubq_to_join_and_agg\n\n引用该变量的文档：\n\n- [Optimizer Hints](/optimizer-hints.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [子查询相关的优化](/subquery-optimization.md)\n- [系统变量](/system-variables.md#tidb_opt_insubq_to_join_and_agg)\n- [TiDB 3.0 Beta Release Notes](/releases/release-3.0-beta.md)\n\n### tidb_opt_join_reorder_threshold\n\n引用该变量的文档：\n\n- [Join Reorder 算法简介](/join-reorder.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_join_reorder_threshold)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 3.0.0-rc.1 Release Notes](/releases/release-3.0.0-rc.1.md)\n\n### tidb_opt_limit_push_down_threshold\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_limit_push_down_threshold)\n- [TiDB 5.2 Release Notes](/releases/release-5.2.0.md)\n\n### tidb_opt_memory_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_memory_factor)\n\n### tidb_opt_mpp_outer_join_fixed_build_side\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_mpp_outer_join_fixed_build_side-从-v510-版本开始引入)\n\n### tidb_opt_network_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_network_factor)\n- [索引的选择](/choose-index.md)\n\n### tidb_opt_objective\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_objective-从-v740-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.5.2 Release Notes](/releases/release-7.5.2.md)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### tidb_opt_ordering_index_selectivity_ratio\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_ordering_index_selectivity_ratio-从-v800-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_opt_ordering_index_selectivity_threshold\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_ordering_index_selectivity_threshold-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_opt_prefer_range_scan\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_prefer_range_scan-从-v50-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_opt_prefix_index_single_scan\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_prefix_index_single_scan-从-v640-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_opt_projection_push_down\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_projection_push_down-从-v610-版本开始引入)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n\n### tidb_opt_range_max_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_range_max_size-从-v640-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_opt_scan_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_scan_factor)\n\n### tidb_opt_seek_factor\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_seek_factor)\n- [索引的选择](/choose-index.md)\n\n### tidb_opt_skew_distinct_agg\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_skew_distinct_agg-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_opt_three_stage_distinct_agg\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_three_stage_distinct_agg-从-v630-版本开始引入)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_opt_tiflash_concurrency_factor\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_opt_tiflash_concurrency_factor)\n\n### tidb_opt_write_row_id\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_opt_write_row_id)\n- [TiDB 2.1 RC5 Release Notes](/releases/release-2.1-rc.5.md)\n\n### tidb_optimizer_selectivity_level\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_optimizer_selectivity_level)\n\n### tidb_partition_prune_mode\n\n引用该变量的文档：\n\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [TiFlash 升级帮助](/tiflash-upgrade-guide.md)\n- [使用 MPP 模式](/tiflash/use-tiflash-mpp-mode.md)\n- [分区表](/partitioned-table.md)\n- [系统变量](/system-variables.md#tidb_partition_prune_mode-从-v51-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n- [TiDB 5.1 Release Notes](/releases/release-5.1.0.md)\n\n### tidb_persist_analyze_options\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_persist_analyze_options-从-v540-版本开始引入)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_pessimistic_txn_fair_locking\n\n引用该变量的文档：\n\n- [TiDB 锁冲突问题处理](/troubleshoot-lock-conflicts.md)\n- [系统变量](/system-variables.md#tidb_pessimistic_txn_fair_locking-从-v700-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_placement_mode\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_placement_mode-从-v600-版本开始引入)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_plan_cache_invalidation_on_fresh_stats\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_plan_cache_invalidation_on_fresh_stats-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_plan_cache_max_plan_size\n\n引用该变量的文档：\n\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [系统变量](/system-variables.md#tidb_plan_cache_max_plan_size-从-v710-版本开始引入)\n- [非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_pprof_sql_cpu\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_pprof_sql_cpu-从-v40-版本开始引入)\n- [TiDB 3.0.10 Release Notes](/releases/release-3.0.10.md)\n\n### tidb_pre_split_regions\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_pre_split_regions-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_prefer_broadcast_join_by_exchange_data_size\n\n引用该变量的文档：\n\n- [使用 MPP 模式](/tiflash/use-tiflash-mpp-mode.md)\n- [系统变量](/system-variables.md#tidb_prefer_broadcast_join_by_exchange_data_size-从-v710-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_prepared_plan_cache_memory_guard_ratio\n\n引用该变量的文档：\n\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [系统变量](/system-variables.md#tidb_prepared_plan_cache_memory_guard_ratio-从-v610-版本开始引入)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_prepared_plan_cache_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_prepared_plan_cache_size-从-v610-版本开始引入)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_projection_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_projection_concurrency)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_query_log_max_len\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [慢查询日志](/identify-slow-queries.md)\n- [系统变量](/system-variables.md#tidb_query_log_max_len)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n- [TiDB 2.1 GA Release Notes](/releases/release-2.1-ga.md)\n- [TiDB 2.1 RC5 Release Notes](/releases/release-2.1-rc.5.md)\n\n### tidb_rc_read_check_ts\n\n引用该变量的文档：\n\n- [OLTP 负载性能优化实践](/performance-tuning-practices.md)\n- [TiDB 事务隔离级别](/transaction-isolation-levels.md)\n- [系统变量](/system-variables.md#tidb_rc_read_check_ts-从-v600-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_rc_write_check_ts\n\n引用该变量的文档：\n\n- [TiDB 事务隔离级别](/transaction-isolation-levels.md)\n- [系统变量](/system-variables.md#tidb_rc_write_check_ts-从-v630-版本开始引入)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tidb_read_consistency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_read_consistency-从-v540-版本开始引入)\n\n### tidb_read_staleness\n\n引用该变量的文档：\n\n- [Stale Read](/develop/dev-guide-use-stale-read.md)\n- [Stale Read 功能的使用场景](/stale-read.md)\n- [系统变量](/system-variables.md#tidb_read_staleness-从-v540-版本开始引入)\n- [通过系统变量 `tidb_read_staleness` 读取历史数据](/tidb-read-staleness.md)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_record_plan_in_slow_log\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [系统变量](/system-variables.md#tidb_record_plan_in_slow_log)\n- [TiDB 3.0.5 Release Notes](/releases/release-3.0.5.md)\n\n### tidb_redact_log\n\n引用该变量的文档：\n\n- [TiDB 配置参数](/command-line-flags-for-tidb-configuration.md)\n- [慢查询日志](/identify-slow-queries.md)\n- [日志脱敏](/log-redaction.md)\n- [系统变量](/system-variables.md#tidb_redact_log)\n- [非事务 DML 语句](/non-transactional-dml.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.1.1 Release Notes](/releases/release-8.1.1.md)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.5.4 Release Notes](/releases/release-7.5.4.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n- [TiDB 5.0 RC Release Notes](/releases/release-5.0.0-rc.md)\n\n### tidb_regard_null_as_point\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_regard_null_as_point-从-v540-版本开始引入)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_remove_orderby_in_subquery\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_remove_orderby_in_subquery-从-v610-版本开始引入)\n- [TiDB 7.2.0 Release Notes](/releases/release-7.2.0.md)\n\n### tidb_replica_read\n\n引用该变量的文档：\n\n- [Follower Read](/follower-read.md)\n- [Follower Read](/develop/dev-guide-use-follower-read.md)\n- [Optimizer Hints](/optimizer-hints.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [只读存储节点最佳实践](/best-practices/readonly-nodes.md)\n- [在三数据中心下就近读取数据](/best-practices/three-dc-local-read.md)\n- [在公有云上部署 TiDB 的最佳实践](/best-practices-on-public-cloud.md)\n- [系统变量](/system-variables.md#tidb_replica_read-从-v40-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n- [TiDB 4.0.2 Release Notes](/releases/release-4.0.2.md)\n- [TiDB 3.1 RC Release Notes](/releases/release-3.1.0-rc.md)\n\n### tidb_request_source_type\n\n引用该变量的文档：\n\n- [使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)\n- [系统变量](/system-variables.md#tidb_request_source_type-从-v740-版本开始引入)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### tidb_resource_control_strict_mode\n\n引用该变量的文档：\n\n- [Optimizer Hints](/optimizer-hints.md)\n- [SET RESOURCE GROUP](/sql-statements/sql-statement-set-resource-group.md)\n- [使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)\n- [权限管理](/privilege-management.md)\n- [系统变量](/system-variables.md#tidb_resource_control_strict_mode-从-v820-版本开始引入)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n\n### tidb_restricted_read_only\n\n引用该变量的文档：\n\n- [同步数据到 MySQL 兼容数据库](/ticdc/ticdc-sink-to-mysql.md)\n- [权限管理](/privilege-management.md)\n- [系统变量](/system-variables.md#tidb_restricted_read_only-从-v520-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 5.4.1 Release Notes](/releases/release-5.4.1.md)\n- [TiDB 5.3.1 Release Notes](/releases/release-5.3.1.md)\n\n### tidb_retry_limit\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 乐观事务模型](/optimistic-transaction.md)\n- [TiDB 事务概览](/transaction-overview.md)\n- [TiDB 悲观事务模式](/pessimistic-transaction.md)\n- [TiDB 锁冲突问题处理](/troubleshoot-lock-conflicts.md)\n- [乐观事务模型下写写冲突问题排查](/troubleshoot-write-conflicts.md)\n- [系统变量](/system-variables.md#tidb_retry_limit)\n- [TiDB 2.1 Beta Release Notes](/releases/release-2.1-beta.md)\n- [TiDB 2.1 GA Release Notes](/releases/release-2.1-ga.md)\n\n### tidb_row_format_version\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_row_format_version)\n- [TiDB 4.0.13 Release Notes](/releases/release-4.0.13.md)\n\n### tidb_runtime_filter_mode\n\n引用该变量的文档：\n\n- [Runtime Filter](/runtime-filter.md)\n- [系统变量](/system-variables.md#tidb_runtime_filter_mode-从-v720-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n\n### tidb_runtime_filter_type\n\n引用该变量的文档：\n\n- [Runtime Filter](/runtime-filter.md)\n- [系统变量](/system-variables.md#tidb_runtime_filter_type-从-v720-版本开始引入)\n\n### tidb_scatter_region\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Split Region 使用文档](/sql-statements/sql-statement-split-region.md)\n- [TiDB 高并发写入场景最佳实践](/best-practices/high-concurrency-best-practices.md)\n- [系统变量](/system-variables.md#tidb_scatter_region)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 6.1.7 Release Notes](/releases/release-6.1.7.md)\n- [TiDB 2.1.15 Release Notes](/releases/release-2.1.15.md)\n\n### tidb_schema_cache_size\n\n引用该变量的文档：\n\n- [Schema 缓存](/schema-cache.md)\n- [系统变量](/system-variables.md#tidb_schema_cache_size-从-v800-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n\n### tidb_schema_version_cache_limit\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_schema_version_cache_limit-从-v740-版本开始引入)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### tidb_server_memory_limit\n\n引用该变量的文档：\n\n- [IMPORT INTO](/sql-statements/sql-statement-import-into.md)\n- [MEMORY_USAGE](/information-schema/information-schema-memory-usage.md)\n- [MEMORY_USAGE_OPS_HISTORY](/information-schema/information-schema-memory-usage-ops-history.md)\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_server_memory_limit-从-v640-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.1.4 Release Notes](/releases/release-7.1.4.md)\n- [TiDB 7.1.3 Release Notes](/releases/release-7.1.3.md)\n- [TiDB 6.5.9 Release Notes](/releases/release-6.5.9.md)\n- [TiDB 6.5.7 Release Notes](/releases/release-6.5.7.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_server_memory_limit_gc_trigger\n\n引用该变量的文档：\n\n- [IMPORT INTO](/sql-statements/sql-statement-import-into.md)\n- [TiDB 内存控制文档](/configure-memory-usage.md)\n- [系统变量](/system-variables.md#tidb_server_memory_limit_gc_trigger-从-v640-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_server_memory_limit_sess_min_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_server_memory_limit_sess_min_size-从-v640-版本开始引入)\n- [TiDB 6.5.2 Release Notes](/releases/release-6.5.2.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n\n### tidb_service_scope\n\n引用该变量的文档：\n\n- [ADMIN SHOW DDL [JOBS|JOB QUERIES]](/sql-statements/sql-statement-admin-show-ddl.md)\n- [IMPORT INTO 和 TiDB Lightning 对比](/tidb-lightning/import-into-vs-tidb-lightning.md)\n- [TiDB 全局排序](/tidb-global-sort.md)\n- [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)\n- [TiDB 功能概览](/basic-features.md)\n- [TiDB 配置参数](/command-line-flags-for-tidb-configuration.md)\n- [系统变量](/system-variables.md#tidb_service_scope-从-v740-版本开始引入)\n- [TiDB 8.1.0 Release Notes](/releases/release-8.1.0.md)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.5.0 Release Notes](/releases/release-7.5.0.md)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### tidb_session_alias\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_session_alias-从-v740-版本开始引入)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### tidb_session_plan_cache_size\n\n引用该变量的文档：\n\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [系统变量](/system-variables.md#tidb_session_plan_cache_size-从-v710-版本开始引入)\n- [非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n\n### tidb_shard_allocate_step\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_shard_allocate_step-从-v50-版本开始引入)\n\n### tidb_shard_row_id_bits\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_shard_row_id_bits-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_simplified_metrics\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_simplified_metrics)\n\n### tidb_skip_ascii_check\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_skip_ascii_check-从-v50-版本开始引入)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n\n### tidb_skip_isolation_level_check\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [事务概览](/develop/dev-guide-transaction-overview.md)\n- [已知的第三方工具兼容问题](/develop/dev-guide-third-party-tools-compatibility.md)\n- [系统变量](/system-variables.md#tidb_skip_isolation_level_check)\n- [错误码与故障诊断](/error-codes.md)\n- [TiDB 5.1.4 Release Notes](/releases/release-5.1.4.md)\n- [TiDB 3.0.0-rc.1 Release Notes](/releases/release-3.0.0-rc.1.md)\n- [TiDB 3.0 GA Release Notes](/releases/release-3.0-ga.md)\n\n### tidb_skip_missing_partition_stats\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_skip_missing_partition_stats-从-v730-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n\n### tidb_skip_utf8_check\n\n引用该变量的文档：\n\n- [DM 任务完整配置文件介绍](/dm/task-configuration-file-full.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 集群问题导图](/tidb-troubleshooting-map.md)\n- [升级与升级后常见问题](/faq/upgrade-faq.md)\n- [字符集和排序规则](/character-set-and-collation.md)\n- [系统变量](/system-variables.md#tidb_skip_utf8_check)\n\n### tidb_slow_log_threshold\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB Dashboard 慢查询页面](/dashboard/dashboard-slow-query.md)\n- [TiDB Dashboard 概况页面](/dashboard/dashboard-overview.md)\n- [TiDB 安装部署常见问题](/faq/deploy-and-maintain-faq.md)\n- [TiDB 数据库快速上手指南](/quick-start-with-tidb.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [在线修改集群配置](/dynamic-config.md)\n- [慢查询日志](/identify-slow-queries.md)\n- [系统变量](/system-variables.md#tidb_slow_log_threshold)\n- [TiDB 2.1 GA Release Notes](/releases/release-2.1-ga.md)\n- [TiDB 2.1 RC5 Release Notes](/releases/release-2.1-rc.5.md)\n\n### tidb_slow_query_file\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [慢查询日志](/identify-slow-queries.md)\n- [系统变量](/system-variables.md#tidb_slow_query_file)\n\n### tidb_snapshot\n\n引用该变量的文档：\n\n- [FLASHBACK CLUSTER](/sql-statements/sql-statement-flashback-cluster.md)\n- [FLUSH TABLES](/sql-statements/sql-statement-flush-tables.md)\n- [IMPORT INTO](/sql-statements/sql-statement-import-into.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 功能概览](/basic-features.md)\n- [临时表](/temporary-tables.md)\n- [使用 Dumpling 导出数据](/dumpling-overview.md)\n- [同步数据到 Kafka](/ticdc/ticdc-sink-to-kafka.md)\n- [系统变量](/system-variables.md#tidb_snapshot)\n- [缓存表](/cached-tables.md)\n- [通过系统变量 tidb_snapshot 读取历史数据](/read-historical-data.md)\n- [非事务 DML 语句](/non-transactional-dml.md)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n- [TiDB 5.2.2 Release Notes](/releases/release-5.2.2.md)\n- [TiDB 5.2 Release Notes](/releases/release-5.2.0.md)\n- [TiDB 5.1.5 Release Notes](/releases/release-5.1.5.md)\n- [TiDB 5.0.3 Release Notes](/releases/release-5.0.3.md)\n- [TiDB 4.0.12 Release Notes](/releases/release-4.0.12.md)\n- [TiDB 4.0.11 Release Notes](/releases/release-4.0.11.md)\n- [TiDB 3.0.11 Release Notes](/releases/release-3.0.11.md)\n- [TiDB 2.1.11 Release Notes](/releases/release-2.1.11.md)\n- [TiDB 2.1.10 Release Notes](/releases/release-2.1.10.md)\n- [TiDB 2.0.4 Release Notes](/releases/release-2.0.4.md)\n\n### tidb_source_id\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_source_id-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_stats_cache_mem_quota\n\n引用该变量的文档：\n\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [系统变量](/system-variables.md#tidb_stats_cache_mem_quota-从-v610-版本开始引入)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n- [TiDB 6.1.0 Release Notes](/releases/release-6.1.0.md)\n\n### tidb_stats_load_pseudo_timeout\n\n引用该变量的文档：\n\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_stats_load_pseudo_timeout-从-v540-版本开始引入)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_stats_load_sync_wait\n\n引用该变量的文档：\n\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [常规统计信息](/statistics.md)\n- [系统变量](/system-variables.md#tidb_stats_load_sync_wait-从-v540-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 6.4.0 Release Notes](/releases/release-6.4.0.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_stmt_summary_enable_persistent\n\n引用该变量的文档：\n\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_enable_persistent-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_stmt_summary_file_max_backups\n\n引用该变量的文档：\n\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_file_max_backups-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_stmt_summary_file_max_days\n\n引用该变量的文档：\n\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_file_max_days-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_stmt_summary_file_max_size\n\n引用该变量的文档：\n\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_file_max_size-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_stmt_summary_filename\n\n引用该变量的文档：\n\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_filename-从-v660-版本开始引入)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_stmt_summary_history_size\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_history_size-从-v40-版本开始引入)\n\n### tidb_stmt_summary_internal_query\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_internal_query-从-v40-版本开始引入)\n\n### tidb_stmt_summary_max_sql_length\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [TiDB Dashboard 慢查询页面](/dashboard/dashboard-slow-query.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_max_sql_length-从-v40-版本开始引入)\n\n### tidb_stmt_summary_max_stmt_count\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [TiDB Dashboard SQL 语句分析执行详情页面](/dashboard/dashboard-statement-list.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_max_stmt_count-从-v40-版本开始引入)\n- [TiDB 5.2 Release Notes](/releases/release-5.2.0.md)\n- [TiDB 5.1.1 Release Notes](/releases/release-5.1.1.md)\n- [TiDB 5.0.4 Release Notes](/releases/release-5.0.4.md)\n- [TiDB 4.0.14 Release Notes](/releases/release-4.0.14.md)\n\n### tidb_stmt_summary_refresh_interval\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Statement Summary Tables](/statement-summary-tables.md)\n- [系统变量](/system-variables.md#tidb_stmt_summary_refresh_interval-从-v40-版本开始引入)\n\n### tidb_store_batch_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_store_batch_size)\n- [TiDB 7.1.0 Release Notes](/releases/release-7.1.0.md)\n- [TiDB 6.6.0 Release Notes](/releases/release-6.6.0.md)\n\n### tidb_store_limit\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_store_limit-从-v304-和-v40-版本开始引入)\n- [错误码与故障诊断](/error-codes.md)\n- [TiDB 5.4 Release Notes](/releases/release-5.4.0.md)\n\n### tidb_streamagg_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_streamagg_concurrency)\n\n### tidb_super_read_only\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_super_read_only-从-v531-版本开始引入)\n- [TiDB 5.4.1 Release Notes](/releases/release-5.4.1.md)\n- [TiDB 5.3.1 Release Notes](/releases/release-5.3.1.md)\n\n### tidb_sysdate_is_now\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_sysdate_is_now-从-v600-版本开始引入)\n- [TiDB 8.0.0 Release Notes](/releases/release-8.0.0.md)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_sysproc_scan_concurrency\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_sysproc_scan_concurrency-从-v650-版本开始引入)\n- [TiDB 8.2.0 Release Notes](/releases/release-8.2.0.md)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_table_cache_lease\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_table_cache_lease-从-v600-版本开始引入)\n- [缓存表](/cached-tables.md)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_tmp_table_max_size\n\n引用该变量的文档：\n\n- [临时表](/temporary-tables.md)\n- [系统变量](/system-variables.md#tidb_tmp_table_max_size-从-v53-版本开始引入)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n\n### tidb_top_sql_max_meta_count\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_top_sql_max_meta_count-从-v600-版本开始引入)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_top_sql_max_time_series_count\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_top_sql_max_time_series_count-从-v600-版本开始引入)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_track_aggregate_memory_usage\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_track_aggregate_memory_usage)\n\n### tidb_tso_client_batch_max_wait_time\n\n引用该变量的文档：\n\n- [在公有云上部署 TiDB 的最佳实践](/best-practices-on-public-cloud.md)\n- [系统变量](/system-variables.md#tidb_tso_client_batch_max_wait_time-从-v530-版本开始引入)\n- [TiDB 5.3 Release Notes](/releases/release-5.3.0.md)\n\n### tidb_tso_client_rpc_mode\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_tso_client_rpc_mode-从-v840-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n\n### tidb_ttl_delete_batch_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_ttl_delete_batch_size-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_ttl_delete_rate_limit\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_ttl_delete_rate_limit-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_ttl_delete_worker_count\n\n引用该变量的文档：\n\n- [使用 TTL (Time to Live) 定期删除过期数据](/time-to-live.md)\n- [系统变量](/system-variables.md#tidb_ttl_delete_worker_count-从-v650-版本开始引入)\n- [TiDB 8.4.0 Release Notes](/releases/release-8.4.0.md)\n- [TiDB 8.1.2 Release Notes](/releases/release-8.1.2.md)\n- [TiDB 7.1.6 Release Notes](/releases/release-7.1.6.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_ttl_job_enable\n\n引用该变量的文档：\n\n- [使用 TTL (Time to Live) 定期删除过期数据](/time-to-live.md)\n- [系统变量](/system-variables.md#tidb_ttl_job_enable-从-v650-版本开始引入)\n- [TiDB 8.5.0 Release Notes](/releases/release-8.5.0.md)\n- [TiDB 8.1.2 Release Notes](/releases/release-8.1.2.md)\n- [TiDB 7.5.5 Release Notes](/releases/release-7.5.5.md)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_ttl_job_schedule_window_end_time\n\n引用该变量的文档：\n\n- [使用 TTL (Time to Live) 定期删除过期数据](/time-to-live.md)\n- [系统变量](/system-variables.md#tidb_ttl_job_schedule_window_end_time-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_ttl_job_schedule_window_start_time\n\n引用该变量的文档：\n\n- [使用 TTL (Time to Live) 定期删除过期数据](/time-to-live.md)\n- [系统变量](/system-variables.md#tidb_ttl_job_schedule_window_start_time-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_ttl_running_tasks\n\n引用该变量的文档：\n\n- [使用 TTL (Time to Live) 定期删除过期数据](/time-to-live.md)\n- [系统变量](/system-variables.md#tidb_ttl_running_tasks-从-v700-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n\n### tidb_ttl_scan_batch_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_ttl_scan_batch_size-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_ttl_scan_worker_count\n\n引用该变量的文档：\n\n- [使用 TTL (Time to Live) 定期删除过期数据](/time-to-live.md)\n- [系统变量](/system-variables.md#tidb_ttl_scan_worker_count-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### tidb_txn_assertion_level\n\n引用该变量的文档：\n\n- [数据索引一致性报错](/troubleshoot-data-inconsistency-errors.md)\n- [系统变量](/system-variables.md#tidb_txn_assertion_level-从-v600-版本开始引入)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tidb_txn_commit_batch_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tidb_txn_commit_batch_size-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tidb_txn_entry_size_limit\n\n引用该变量的文档：\n\n- [TiDB Lightning 故障处理](/tidb-lightning/troubleshoot-tidb-lightning.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [事务限制](/develop/dev-guide-transaction-restraints.md)\n- [系统变量](/system-variables.md#tidb_txn_entry_size_limit-从-v760-版本开始引入)\n- [TiDB 7.6.0 Release Notes](/releases/release-7.6.0.md)\n\n### tidb_txn_mode\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [TiDB 事务概览](/transaction-overview.md)\n- [TiDB 悲观事务模式](/pessimistic-transaction.md)\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [乐观事务和悲观事务](/develop/dev-guide-optimistic-and-pessimistic-transaction.md)\n- [系统变量](/system-variables.md#tidb_txn_mode)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n- [TiDB 5.0 Release Notes](/releases/release-5.0.0.md)\n- [TiDB 3.0.8 Release Notes](/releases/release-3.0.8.md)\n- [TiDB 3.0.4 Release Notes](/releases/release-3.0.4.md)\n\n### tidb_use_plan_baselines\n\n引用该变量的文档：\n\n- [CREATE [GLOBAL|SESSION] BINDING](/sql-statements/sql-statement-create-binding.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)\n- [系统变量](/system-variables.md#tidb_use_plan_baselines-从-v40-版本开始引入)\n\n### tidb_wait_split_region_finish\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Split Region 使用文档](/sql-statements/sql-statement-split-region.md)\n- [系统变量](/system-variables.md#tidb_wait_split_region_finish)\n\n### tidb_wait_split_region_timeout\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [Split Region 使用文档](/sql-statements/sql-statement-split-region.md)\n- [系统变量](/system-variables.md#tidb_wait_split_region_timeout)\n\n### tidb_window_concurrency\n\n引用该变量的文档：\n\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [系统变量](/system-variables.md#tidb_window_concurrency-从-v40-版本开始引入)\n\n### tiflash_fastscan\n\n引用该变量的文档：\n\n- [使用 FastScan 功能](/tiflash/use-fastscan.md)\n- [系统变量](/system-variables.md#tiflash_fastscan-从-v630-版本开始引入)\n- [TiDB 7.0.0 Release Notes](/releases/release-7.0.0.md)\n- [TiDB 6.3.0 Release Notes](/releases/release-6.3.0.md)\n\n### tiflash_fine_grained_shuffle_batch_size\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tiflash_fine_grained_shuffle_batch_size-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tiflash_fine_grained_shuffle_stream_count\n\n引用该变量的文档：\n\n- [TiFlash 性能调优](/tiflash/tune-tiflash-performance.md)\n- [系统变量](/system-variables.md#tiflash_fine_grained_shuffle_stream_count-从-v620-版本开始引入)\n- [TiDB 6.2.0 Release Notes](/releases/release-6.2.0.md)\n\n### tiflash_hashagg_preaggregation_mode\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tiflash_hashagg_preaggregation_mode-从-v830-版本开始引入)\n- [TiDB 8.3.0 Release Notes](/releases/release-8.3.0.md)\n\n### tiflash_mem_quota_query_per_node\n\n引用该变量的文档：\n\n- [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)\n- [系统变量](/system-variables.md#tiflash_mem_quota_query_per_node-从-v740-版本开始引入)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### tiflash_query_spill_ratio\n\n引用该变量的文档：\n\n- [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)\n- [系统变量](/system-variables.md#tiflash_query_spill_ratio-从-v740-版本开始引入)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### tiflash_replica_read\n\n引用该变量的文档：\n\n- [TiDB 功能概览](/basic-features.md)\n- [构建 TiFlash 副本](/tiflash/create-tiflash-replicas.md)\n- [系统变量](/system-variables.md#tiflash_replica_read-从-v730-版本开始引入)\n- [TiDB 7.3.0 Release Notes](/releases/release-7.3.0.md)\n\n### tikv_client_read_timeout\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tikv_client_read_timeout-从-v740-版本开始引入)\n- [TiDB 7.5.1 Release Notes](/releases/release-7.5.1.md)\n- [TiDB 7.4.0 Release Notes](/releases/release-7.4.0.md)\n\n### time_zone\n\n引用该变量的文档：\n\n- [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)\n- [SHOW [GLOBAL|SESSION] VARIABLES](/sql-statements/sql-statement-show-variables.md)\n- [日期和时间类型](/data-type-date-and-time.md)\n- [时区支持](/configure-time-zone.md)\n- [系统变量](/system-variables.md#time_zone)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n- [TiDB 2.1.8 Release Notes](/releases/release-2.1.8.md)\n\n### timestamp\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#timestamp)\n\n### transaction_isolation\n\n引用该变量的文档：\n\n- [SET TRANSACTION](/sql-statements/sql-statement-set-transaction.md)\n- [已知的第三方工具兼容问题](/develop/dev-guide-third-party-tools-compatibility.md)\n- [系统变量](/system-variables.md#transaction_isolation)\n- [TiDB 6.0.0 Release Notes](/releases/release-6.0.0-dmr.md)\n\n### tx_isolation\n\n引用该变量的文档：\n\n- [OLTP 负载性能优化实践](/performance-tuning-practices.md)\n- [已知的第三方工具兼容问题](/develop/dev-guide-third-party-tools-compatibility.md)\n- [系统变量](/system-variables.md#tx_isolation)\n\n### tx_isolation_one_shot\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tx_isolation_one_shot)\n\n### tx_read_ts\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#tx_read_ts)\n\n### txn_scope\n\n引用该变量的文档：\n\n- [TiDB 配置文件描述](/tidb-configuration-file.md)\n- [使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)\n- [系统变量](/system-variables.md#txn_scope)\n\n### validate_password.check_user_name\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#validate_passwordcheck_user_name-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### validate_password.dictionary\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [与 MySQL 安全特性差异](/security-compatibility-with-mysql.md)\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#validate_passworddictionary-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### validate_password.enable\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [与 MySQL 安全特性差异](/security-compatibility-with-mysql.md)\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#validate_passwordenable-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### validate_password.length\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#validate_passwordlength-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### validate_password.mixed_case_count\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#validate_passwordmixed_case_count-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### validate_password.number_count\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#validate_passwordnumber_count-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### validate_password.policy\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#validate_passwordpolicy-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### validate_password.special_char_count\n\n引用该变量的文档：\n\n- [TiDB 密码管理](/password-management.md)\n- [加密和压缩函数](/functions-and-operators/encryption-and-compression-functions.md)\n- [系统变量](/system-variables.md#validate_passwordspecial_char_count-从-v650-版本开始引入)\n- [TiDB 6.5.0 Release Notes](/releases/release-6.5.0.md)\n\n### version\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#version)\n\n### version_comment\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#version_comment)\n\n### version_compile_machine\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#version_compile_machine)\n\n### version_compile_os\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#version_compile_os)\n\n### wait_timeout\n\n引用该变量的文档：\n\n- [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)\n- [TiDB 中的各种超时](/develop/dev-guide-timeouts-in-tidb.md)\n- [TiDB 集群管理常见问题](/faq/manage-cluster-faq.md)\n- [TiProxy 简介](/tiproxy/tiproxy-overview.md)\n- [开发 Java 应用使用 TiDB 的最佳实践](/best-practices/java-app-best-practices.md)\n- [系统变量](/system-variables.md#wait_timeout)\n- [连接池与连接参数](/develop/dev-guide-connection-parameters.md)\n- [TiDB 3.0 Beta Release Notes](/releases/release-3.0-beta.md)\n- [TiDB 3.0 GA Release Notes](/releases/release-3.0-ga.md)\n\n### warning_count\n\n引用该变量的文档：\n\n- [系统变量](/system-variables.md#warning_count)\n- [TiDB 2.1 RC1 Release Notes](/releases/release-2.1-rc.1.md)\n\n### windowing_use_high_precision\n\n引用该变量的文档：\n\n- [窗口函数](/functions-and-operators/window-functions.md)\n- [系统变量](/system-variables.md#windowing_use_high_precision)\n"
        },
        {
          "name": "system-variables.md",
          "type": "blob",
          "size": 332.921875,
          "content": "---\ntitle: 系统变量\nsummary: 使用 TiDB 系统变量来优化性能或修改运行行为。\naliases: ['/docs-cn/dev/system-variables/','/docs-cn/dev/reference/configuration/tidb-server/mysql-variables/','/docs-cn/dev/tidb-specific-system-variables/','/docs-cn/dev/reference/configuration/tidb-server/tidb-specific-variables/','/zh/tidb/dev/tidb-specific-system-variables/']\n---\n\n# 系统变量\n\nTiDB 系统变量的行为与 MySQL 相似，变量的作用范围可以是会话级别有效 (Session Scope) 或全局范围有效 (Global Scope)。其中：\n\n- 对 `SESSION` 作用域变量的更改，设置后**只影响当前会话**。\n- 对 `GLOBAL` 作用域变量的更改，设置后立即生效。如果该变量也有 `SESSION` 作用域，已经连接的所有会话 (包括当前会话) 将继续使用会话当前的 `SESSION` 变量值。\n- 要设置变量值，可使用 [`SET` 语句](/sql-statements/sql-statement-set-variable.md)。\n\n```sql\n# 以下两个语句等价地改变一个 Session 变量\nSET tidb_distsql_scan_concurrency = 10;\nSET SESSION tidb_distsql_scan_concurrency = 10;\n\n# 以下两个语句等价地改变一个 Global 变量\nSET @@global.tidb_distsql_scan_concurrency = 10;\nSET GLOBAL tidb_distsql_scan_concurrency = 10;\n```\n\n> **注意：**\n>\n> 部分 `GLOBAL` 作用域的变量会持久化到 TiDB 集群中。文档中的变量有一个“是否持久化到集群”的说明，可以为“是”或者“否”。\n>\n> - 对于持久化到集群的变量，当该全局变量被修改后，会通知所有 TiDB 服务器刷新其系统变量缓存。在集群中增加一个新的 TiDB 服务器时，或者重启现存的 TiDB 服务器时，都将自动使用该持久化变量。\n> - 对于不持久化到集群的变量，对变量的修改只对当前连接的 TiDB 实例生效。如果需要保留设置过的值，需要在 `tidb.toml` 配置文件中声明。\n>\n> 此外，由于应用和连接器通常需要读取 MySQL 变量，为了兼容这一需求，在 TiDB 中，部分 MySQL 的变量既可读取也可设置。例如，尽管 JDBC 连接器不依赖于查询缓存 (query cache) 的行为，但仍然可以读取和设置查询缓存。\n\n> **注意：**\n>\n> 变量取较大值并不总会带来更好的性能。由于大部分变量对单个连接生效，设置变量时，还应考虑正在执行语句的并发连接数量。\n>\n> 确定安全值时，应考虑变量的单位：\n>\n> * 如果单位为线程，安全值通常取决于 CPU 核的数量。\n> * 如果单位为字节，安全值通常小于系统内存的总量。\n> * 如果单位为时间，单位可能为秒或毫秒。\n>\n> 单位相同的多个变量可能会争夺同一组资源。\n\n从 v7.4.0 开始，部分 `SESSION` 作用域的变量可以通过 [`SET_VAR`](/optimizer-hints.md#set_varvar_namevar_value) Hint 在语句执行期间临时修改变量的值。当语句执行完成后，系统变量将在当前会话中自动恢复为原始值。通过这个 Hint 可以修改一部分与优化器、执行器相关的系统变量行为。文档中的变量有一个“是否受 Hint [`SET_VAR`](/optimizer-hints.md#set_varvar_namevar_value) 控制”的说明，可以为“是”或者“否”。\n\n- 对于受 Hint SET_VAR 控制的变量，你可以在语句中使用 `/*+ SET_VAR(...) */` 修改语句执行期间变量的值。\n- 对于不受 Hint SET_VAR 控制的变量，你不能在语句中使用 `/*+ SET_VAR(...) */` 修改语句执行期间变量的值。\n\n关于 SET_VAR Hint 的更多说明，参考 [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value)。\n\n## 变量参考\n\n### `allow_auto_random_explicit_insert` <span class=\"version-mark\">从 v4.0.3 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 是否允许在 `INSERT` 语句中显式指定含有 `AUTO_RANDOM` 属性的列的值。\n\n### `authentication_ldap_sasl_auth_method_name` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`SCRAM-SHA-1`\n- 可选值：`SCRAM-SHA-1`、`SCRAM-SHA-256`、`GSSAPI`\n- LDAP SASL 身份验证中，验证方法的名称。\n\n### `authentication_ldap_sasl_bind_base_dn` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP SASL 身份验证中，搜索用户的范围。如果创建用户时没有通过 `AS ...` 指定 `dn`，TiDB 会自动在 LDAP Server 的该范围中根据用户名搜索用户 `dn`。例如 `dc=example,dc=org`。\n\n### `authentication_ldap_sasl_bind_root_dn` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP SASL 身份验证中，TiDB 登录 LDAP Server 搜索用户时使用的 `dn`。\n\n### `authentication_ldap_sasl_bind_root_pwd` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP SASL 身份验证中，TiDB 登录 LDAP Server 搜索用户时使用的密码。\n\n### `authentication_ldap_sasl_ca_path` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP SASL 身份验证中，TiDB 对 StartTLS 连接使用的 CA 证书的路径。\n\n### `authentication_ldap_sasl_init_pool_size` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`10`\n- 范围：`[1, 32767]`\n- LDAP SASL 身份验证中，TiDB 与 LDAP Server 间连接池的初始连接数。\n\n### `authentication_ldap_sasl_max_pool_size` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1000`\n- 范围：`[1, 32767]`\n- LDAP SASL 身份验证中，TiDB 与 LDAP Server 间连接池的最大连接数。\n\n### `authentication_ldap_sasl_server_host` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP SASL 身份验证中，LDAP Server 的主机名或地址。\n\n### `authentication_ldap_sasl_server_port` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`389`\n- 范围：`[1, 65535]`\n- LDAP SASL 身份验证中，LDAP Server 的 TCP/IP 端口号。\n\n### `authentication_ldap_sasl_tls` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- LDAP SASL 身份验证中，是否使用 StartTLS 对连接加密。\n\n### `authentication_ldap_simple_auth_method_name` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`SIMPLE`\n- 可选值：`SIMPLE`\n- LDAP simple 身份验证中，验证方法的名称。现在仅支持 `SIMPLE`。\n\n### `authentication_ldap_simple_bind_base_dn` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP simple 身份验证中，搜索用户的范围。如果创建用户时没有通过 `AS ...` 指定 `dn`，TiDB 会自动在 LDAP Server 的该范围中根据用户名搜索用户 `dn`。例如 `dc=example,dc=org`。\n\n### `authentication_ldap_simple_bind_root_dn` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP simple 身份验证中，TiDB 登录 LDAP Server 搜索用户时使用的 `dn`。\n\n### `authentication_ldap_simple_bind_root_pwd` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP simple 身份验证中，TiDB 登录 LDAP Server 搜索用户时使用的密码。\n\n### `authentication_ldap_simple_ca_path` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP simple 身份验证中，TiDB 对 StartTLS 连接使用的 CA 证书的路径。\n\n### `authentication_ldap_simple_init_pool_size` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`10`\n- 范围：`[1, 32767]`\n- LDAP simple 身份验证中，TiDB 与 LDAP Server 间连接池的初始连接数。\n\n### `authentication_ldap_simple_max_pool_size` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1000`\n- 范围：`[1, 32767]`\n- LDAP simple 身份验证中，TiDB 与 LDAP Server 间连接池的最大连接数。\n\n### `authentication_ldap_simple_server_host` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- LDAP simple 身份验证中，LDAP Server 的主机名或地址。\n\n### `authentication_ldap_simple_server_port` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`389`\n- 范围：`[1, 65535]`\n- LDAP simple 身份验证中，LDAP Server 的 TCP/IP 端口号。\n\n### `authentication_ldap_simple_tls` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- LDAP simple 身份验证中，是否使用 StartTLS 对连接加密。\n\n### `auto_increment_increment`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[1, 65535]`\n- 控制 `AUTO_INCREMENT` 自增值字段的自增步长和 `AUTO_RANDOM` ID 的分配规则。该变量常与 [`auto_increment_offset`](#auto_increment_offset) 一起使用。\n\n### `auto_increment_offset`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[1, 65535]`\n- 控制 `AUTO_INCREMENT` 自增值字段的初始值和 `AUTO_RANDOM` ID 的分配规则。该变量常与 [`auto_increment_increment`](#auto_increment_increment) 一起使用。示例如下：\n\n```sql\nmysql> CREATE TABLE t1 (a int not null primary key auto_increment);\nQuery OK, 0 rows affected (0.10 sec)\n\nmysql> set auto_increment_offset=1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> set auto_increment_increment=3;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> INSERT INTO t1 VALUES (),(),(),();\nQuery OK, 4 rows affected (0.04 sec)\nRecords: 4  Duplicates: 0  Warnings: 0\n\nmysql> SELECT * FROM t1;\n+----+\n| a  |\n+----+\n|  1 |\n|  4 |\n|  7 |\n| 10 |\n+----+\n4 rows in set (0.00 sec)\n```\n\n### `autocommit`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 用于设置在非显式事务时是否自动提交事务。更多信息，请参见[事务概述](/transaction-overview.md#自动提交)。\n\n### `block_encryption_mode`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`aes-128-ecb`\n- 可选值：`aes-128-ecb`、`aes-192-ecb`、`aes-256-ecb`、`aes-128-cbc`、`aes-192-cbc`、`aes-256-cbc`、`aes-128-ofb`、`aes-192-ofb`、`aes-256-ofb`、`aes-128-cfb`、`aes-192-cfb`、`aes-256-cfb`\n- 该变量用于设置 [`AES_ENCRYPT()`](/functions-and-operators/encryption-and-compression-functions.md#aes_encrypt) 和 [`AES_DECRYPT()`](/functions-and-operators/encryption-and-compression-functions.md#aes_decrypt) 函数的加密模式。\n\n### `character_set_client`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`utf8mb4`\n- 这个变量表示从客户端发出的数据所用的字符集。有关更多 TiDB 支持的字符集和排序规则，参阅[字符集和排序规则](/character-set-and-collation.md)文档。如果需要更改字符集，建议使用 [`SET NAMES`](/sql-statements/sql-statement-set-names.md) 语句。\n\n### `character_set_connection`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`utf8mb4`\n- 若没有为字符串常量指定字符集，该变量表示这些字符串常量所使用的字符集。\n\n### `character_set_database`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`utf8mb4`\n- 该变量表示当前默认在用数据库的字符集，**不建议设置该变量**。选择新的默认数据库后，服务器会更改该变量的值。\n\n### `character_set_results`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`utf8mb4`\n- 该变量表示数据发送至客户端时所使用的字符集。\n\n### `character_set_server`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`utf8mb4`\n- 当 `CREATE SCHEMA` 中没有指定字符集时，该变量表示这些新建的表结构所使用的字符集。\n\n### `collation_connection`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`utf8mb4_bin`\n- 该变量表示连接中所使用的排序规则。与 MySQL 中的 `collation_connection` 一致。\n\n### `collation_database`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`utf8mb4_bin`\n- 该变量表示当前数据库默认所使用的排序规则。与 MySQL 中的 `collation_database` 一致。**不建议设置此变量**，当前使用的数据库变动时，此变量会被 TiDB 修改。\n\n### `collation_server`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`utf8mb4_bin`\n- 该变量表示创建数据库时默认的排序规则。\n\n### `cte_max_recursion_depth`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`1000`\n- 范围：`[0, 4294967295]`\n- 这个变量用于控制公共表表达式的最大递归深度。\n\n### `datadir`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：使用的组件和部署方式不同，默认值也不同。\n    - `/tmp/tidb`：如果你将 [`--store`](/command-line-flags-for-tidb-configuration.md#--store) 设置为 `\"unistore\"` 或没有设置 `--store`，则默认值为 `/tmp/tidb`。\n    - `${pd-ip}:${pd-port}`：如果你设置的存储引擎是 TiKV（如果使用 TiUP 和 TiDB Operator 部署，则默认的存储引擎为 TiKV），则默认值为 `${pd-ip}:${pd-port}`。\n- 这个变量表示数据存储的位置，位置可以是本地路径 `/tmp/tidb`。如果数据存储在 TiKV 上，则可以是指向 PD 服务器的路径。变量值的格式为 `${pd-ip}:${pd-port}`，表示 TiDB 在启动时连接到的 PD 服务器。\n\n### `ddl_slow_threshold`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`300`\n- 取值范围：`[0, 2147483647]`\n- 单位：毫秒\n- 耗时超过该阈值的 DDL 操作会被输出到日志。\n\n### `default_authentication_plugin`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`mysql_native_password`\n- 可选值：`mysql_native_password`，`caching_sha2_password`，`tidb_sm3_password`，`tidb_auth_token`，`authentication_ldap_sasl` 或 `authentication_ldap_simple`。\n- 服务器和客户端建立连接时，这个变量用于设置服务器对外通告的默认身份验证方式。如要了解该变量的其他可选值，参见[可用的身份验证插件](/security-compatibility-with-mysql.md#可用的身份验证插件)。\n- 若要在用户登录时使用 `tidb_sm3_password` 插件，需要使用 [TiDB-JDBC](https://github.com/pingcap/mysql-connector-j/tree/release/8.0-sm3) 进行连接。\n\n### `default_collation_for_utf8mb4` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL | SESSION\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：`utf8mb4_bin`\n- 可选值：`utf8mb4_bin`、`utf8mb4_general_ci`、`utf8mb4_0900_ai_ci`\n- 该变量用于设置 utf8mb4 字符集的默认[排序规则](/character-set-and-collation.md)。它会影响以下语句的行为：\n    - [`SHOW COLLATION`](/sql-statements/sql-statement-show-collation.md) 和 [`SHOW CHARACTER SET`](/sql-statements/sql-statement-show-character-set.md) 语句显示的默认排序规则。\n    - [`CREATE TABLE`](/sql-statements/sql-statement-create-table.md) 和 [`ALTER TABLE`](/sql-statements/sql-statement-alter-table.md) 语句中对表或列使用 `CHARACTER SET` 语法明确指定 utf8mb4 字符集而未指定排序规则时，将使用该变量指定的排序规则。不影响未使用 `CHARACTER SET` 语法时的行为。\n    - [`CREATE DATABASE`](/sql-statements/sql-statement-create-database.md) 和 [`ALTER DATABASE`](/sql-statements/sql-statement-alter-database.md) 语句中使用 `CHARACTER SET` 语法明确指定 utf8mb4 字符集而未指定排序规则时，将使用该变量指定的排序规则。不影响未使用 `CHARACTER SET` 语法时的行为。\n    - 任何使用 `_utf8mb4'string'` 形式的字面量在未使用 `COLLATE` 语法指定排序规则时，将使用该变量指定的排序规则。\n\n### `default_password_lifetime` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 取值范围：`[0, 65535]`\n- 该变量用于设置全局自动密码过期策略，默认值为 `0`，即禁用全局自动密码过期。如果设置该变量的值为正整数 N，则表示允许的密码生存期为 N，即必须在 N 天之内更改密码。\n\n### `default_week_format`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 取值范围：`[0, 7]`\n- 设置 `WEEK()` 函数使用的周格式。\n\n### `disconnect_on_expired_password` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量是一个只读变量，用来显示 TiDB 是否会直接断开密码已过期用户的连接。当其值为 `ON`，表示 TiDB 会断开密码已过期用户的连接。当其值为 `OFF`，表示 TiDB 会将密码已过期用户的连接置于“沙盒模式”，允许该用户建立连接并执行密码重置操作。\n- 如果需要改变 TiDB 对密码已过期用户连接的处理方式，请在 TiDB 配置文件中的 `[security]` 部分修改 [`disconnect-on-expired-password`](/tidb-configuration-file.md#disconnect-on-expired-password-从-v650-版本开始引入) 选项。\n\n### `div_precision_increment` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`4`\n- 范围：`[0, 30]`\n- 这个变量用于控制使用运算符 `/` 执行除法操作时，结果增加的小数位数。该功能与 MySQL 保持一致。\n\n### `error_count`\n\n- 作用域：SESSION\n- 默认值：`0`\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 表示上一条生成消息的 SQL 语句中的错误数。该变量为只读变量。\n\n### `foreign_key_checks`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：在 v6.6.0 之前版本中为 `OFF`，在 v6.6.0 及之后的版本中为 `ON`。\n- 表示是否开启外键约束检查。\n\n### `group_concat_max_len`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1024`\n- 取值范围：`[4, 18446744073709551615]`\n- 表示 `GROUP_CONCAT()` 函数缓冲区的最大长度。\n\n### `have_openssl`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`DISABLED`\n- 用于 MySQL 兼容性的只读变量。当服务器启用 TLS 时，服务器将其设置为 `YES`。\n\n### `have_ssl`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`DISABLED`\n- 用于 MySQL 兼容性的只读变量。当服务器启用 TLS 时，服务器将其设置为 `YES`。\n\n### `hostname`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：（系统主机名）\n- 这个变量为只读变量，表示 TiDB server 的主机名。\n\n### `identity` <span class=\"version-mark\">从 v5.3.0 版本开始引入</span>\n\n- 该变量为变量 [`last_insert_id`](#last_insert_id-从-v530-版本开始引入) 的别名。\n\n### `init_connect`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 用户首次连接到 TiDB 服务器时，`init_connect` 特性允许 TiDB 自动执行一条或多条 SQL 语句。如果你有 `CONNECTION_ADMIN` 或者 `SUPER` 权限，这些 SQL 语句将不会被自动执行。如果这些语句执行报错，你的用户连接将被终止。\n\n### `innodb_lock_wait_timeout`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`50`\n- 范围：`[1, 3600]`\n- 单位：秒\n- 悲观事务语句等锁时间。\n\n### `interactive_timeout`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`28800`\n- 范围：`[1, 31536000]`\n- 单位：秒\n- 该变量表示交互式用户会话的空闲超时。交互式用户会话是指使用 `CLIENT_INTERACTIVE` 选项调用 [`mysql_real_connect()`](https://dev.mysql.com/doc/c-api/5.7/en/mysql-real-connect.html) API 建立的会话（例如：MySQL shell 和 MySQL client）。该变量与 MySQL 完全兼容。\n\n### `last_insert_id` <span class=\"version-mark\">从 v5.3.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 取值范围：`[0, 18446744073709551615]`\n- 返回由 `INSERT` 语句产生的最新 `AUTO_INSCRENT` 或者 `AUTO_RANDOM` 值，与 `LAST_INSERT_ID()` 的返回的结果相同。与 MySQL 中的 `last_insert_id` 一致。\n\n### `last_plan_from_binding` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量用来显示上一条执行的语句所使用的执行计划是否来自 binding 的[执行计划](/sql-plan-management.md)。\n\n### `last_plan_from_cache` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来显示上一个 `execute` 语句所使用的执行计划是不是直接从 plan cache 中取出来的。\n\n### `last_sql_use_alloc` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`OFF`\n- 这个变量是一个只读变量，用来显示上一个语句是否使用了缓存的 Chunk 对象 (Chunk allocation)。\n\n### `license`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`Apache License 2.0`\n- 这个变量表示 TiDB 服务器的安装许可证。\n\n### `max_connections`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 取值范围：`[0, 100000]`\n- 该变量表示 TiDB 中同时允许的最大客户端连接数，用于资源控制。\n- 默认情况下，该变量值为 `0` 表示不限制客户端连接数。当本变量的值大于 `0` 且客户端连接数到达此值时，TiDB 服务端将会拒绝新的客户端连接。\n\n### `max_execution_time`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 2147483647]`\n- 单位：毫秒\n- 语句最长执行时间。默认值 (0) 表示无限制。\n\n> **注意：**\n>\n> - 在 v6.4.0 之前，`max_execution_time` 对所有类型的语句生效。从 v6.4.0 开始，该变量只用于控制只读语句的最大执行时长。实际精度在 100ms 级别，而非更准确的毫秒级别。\n> - 对于使用了 [`MAX_EXECUTION_TIME`](/optimizer-hints.md#max_execution_timen) Hint 的 SQL 语句，这些语句的最长执行时间将不受该变量限制，而是由该 Hint 进行限制。你也可以使用该 Hint 来创建 SQL 绑定，详情请参考 [SQL 操作常见问题](/faq/sql-faq.md#如何阻止特定的-sql-语句执行或者将某个-sql-语句加入黑名单)。\n\n### `max_prepared_stmt_count`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[-1, 1048576]`\n- 指定当前实例中 [`PREPARE`](/sql-statements/sql-statement-prepare.md) 语句的最大数量。\n- 值为 `-1` 时表示不对实例中的 `PREPARE` 语句数量进行限制。\n- 如果将变量值设为超过上限 `1048576`，则使用上限值 `1048576`：\n\n```sql\nmysql> SET GLOBAL max_prepared_stmt_count = 1048577;\nQuery OK, 0 rows affected, 1 warning (0.01 sec)\n\nmysql> SHOW WARNINGS;\n+---------+------+--------------------------------------------------------------+\n| Level   | Code | Message                                                      |\n+---------+------+--------------------------------------------------------------+\n| Warning | 1292 | Truncated incorrect max_prepared_stmt_count value: '1048577' |\n+---------+------+--------------------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> SHOW GLOBAL VARIABLES LIKE 'max_prepared_stmt_count';\n+-------------------------+---------+\n| Variable_name           | Value   |\n+-------------------------+---------+\n| max_prepared_stmt_count | 1048576 |\n+-------------------------+---------+\n1 row in set (0.00 sec)\n```\n\n### `max_allowed_packet` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`67108864`\n- 取值范围：`[1024, 1073741824]`\n- 该变量取值应为 1024 的整数倍。若取值无法被 1024 整除，则会提示 warning 并向下取整。例如设置为 1025 时，则 TiDB 中的实际取值为 1024。\n- 服务器端和客户端在一次传送数据包的过程中所允许最大的数据包大小，单位为字节。\n- 在 `SESSION` 作用域下，该变量为只读变量。\n- 该变量的行为与 MySQL 兼容。\n\n### `mpp_exchange_compression_mode` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`UNSPECIFIED`\n- 可选值：`NONE`，`FAST`，`HIGH_COMPRESSION`，`UNSPECIFIED`\n- 该变量用于选择 MPP Exchange 算子的数据压缩模式，当 TiDB 选择版本号为 `1` 的 MPP 执行计划时生效。该变量值的含义如下：\n    - `UNSPECIFIED`：表示未指定，TiDB 将自动选择压缩模式，当前 TiDB 自动选择 `FAST` 模式\n    - `NONE`：不使用数据压缩\n    - `FAST`：快速模式，整体性能较好，压缩比小于 `HIGH_COMPRESSION`\n    - `HIGH_COMPRESSION`：高压缩比模式\n\n### `mpp_version` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`UNSPECIFIED`\n- 可选值：`UNSPECIFIED`，`0`，`1`，`2`\n- 该变量用于指定不同版本的 MPP 执行计划。指定后，TiDB 会选择指定版本的 MPP 执行计划。该变量值含义如下：\n    - `UNSPECIFIED`：表示未指定，此时 TiDB 自动选择最新版本 `2`。\n    - `0`：兼容所有 TiDB 集群版本，MPP 版本大于 `0` 的新特性均不会生效。\n    - `1`：从 v6.6.0 版本开始引入，用于开启 TiFlash 带压缩的数据交换，详情参见 [MPP Version 和 Exchange 数据压缩](/explain-mpp.md#mpp-version-和-exchange-数据压缩)。\n    - `2`：从 v7.3.0 版本开始引入，用于确保在 TiFlash 执行出错的情况下，获取到准确的报错信息。\n\n### `password_history` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 4294967295]`\n- 该变量用于建立密码重用策略，使 TiDB 基于密码更改次数限制密码的重复使用。该变量默认值为 `0`，表示禁用基于密码更改次数的密码重用策略。当设置该变量为一个正整数 N 时，表示不允许重复使用最近 N 次使用过的密码。\n\n### `password_reuse_interval` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 4294967295]`\n- 该变量用于建立密码重用策略，使 TiDB 基于经过时间限制密码重复使用。该变量默认值为 0，表示禁用基于密码经过时间的密码重用策略。当设置该变量为一个正整数 N 时，表示不允许重复使用最近 N 天内使用过的密码。\n\n### `pd_enable_follower_handle_region` <span class=\"version-mark\">从 v7.6.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启 Active PD Follower 特性（目前该特性只适用于处理获取 Region 信息的相关请求）。当该值为 `OFF` 时，TiDB 仅从 PD leader 获取 Region 信息。当该值为 `ON` 时，TiDB 在获取 Region 信息时会将请求均匀地发送到所有 PD 节点上，因此 PD follower 也可以处理 Region 信息请求，从而减轻 PD leader 的 CPU 压力。\n- 适合开启 Active PD Follower 的场景：\n    - 集群 Region 数量较多，PD leader 由于处理心跳和调度任务的开销大，导致 CPU 资源紧张。\n    - 集群中 TiDB 实例数量较多，Region 信息请求并发量较大，PD leader CPU 压力大。\n\n### `plugin_dir`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 指定加载插件的目录。\n\n### `plugin_load`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 指定 TiDB 启动时加载的插件，多个插件之间用逗号（,）分隔。\n\n### `port`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`4000`\n- 范围：`[0, 65535]`\n- 使用 MySQL 协议时 tidb-server 监听的端口。\n\n### `rand_seed1`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 2147483647]`\n- 该变量用于为 SQL 函数 `RAND()` 中使用的随机值生成器添加种子。\n- 该变量的行为与 MySQL 兼容。\n\n### `rand_seed2`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 2147483647]`\n- 该变量用于为 SQL 函数 `RAND()` 中使用的随机值生成器添加种子。\n- 该变量的行为与 MySQL 兼容。\n\n### `require_secure_transport` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制是否所有 TiDB 的连接都在本地 socket 上进行通信，或使用 TLS。详情见[为 TiDB 客户端服务端间通信开启加密传输](/enable-tls-between-clients-and-servers.md)。\n- 该变量设置为 `ON` 时，必须使用开启 TLS 的会话连接到 TiDB，防止在 TLS 配置不正确时出现锁定的情况。\n- 在 v6.1.0 之前这个开关通过 TiDB 配置文件 (`security.require-secure-transport`) 进行配置，升级到 v6.1.0 时会自动继承原有设置。\n- 从 v6.5.6、v7.1.2、v7.5.1 和 v8.0.0 起，当启用了安全增强模式 (SEM) 时，禁止将 `security.require-secure-transport` 设置为 `ON`。\n\n### `skip_name_resolve` <span class=\"version-mark\">从 v5.2.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制 `tidb-server` 实例是否将主机名作为连接握手的一部分来解析。\n- 当 DNS 不可靠时，可以启用该变量来提高网络性能。\n\n> **注意：**\n>\n> 当 `skip_name_resolve` 设置为 `ON` 时，身份信息中包含主机名的用户将无法登录服务器。例如：\n>\n> ```sql\n> CREATE USER 'appuser'@'apphost' IDENTIFIED BY 'app-password';\n> ```\n>\n> 该示例中，建议将 `apphost` 替换为 IP 地址或通配符（`%`）。\n\n### `socket`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 使用 MySQL 协议时，tidb-server 所监听的本地 unix 套接字文件。\n\n### `sql_mode`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`ONLY_FULL_GROUP_BY,STRICT_TRANS_TABLES,NO_ZERO_IN_DATE,NO_ZERO_DATE,ERROR_FOR_DIVISION_BY_ZERO,NO_AUTO_CREATE_USER,NO_ENGINE_SUBSTITUTION`\n- 这个变量控制许多 MySQL 兼容行为。详情见 [SQL 模式](/sql-mode.md)。\n\n### `sql_require_primary_key` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制表是否必须有主键。启用该变量后，如果在没有主键的情况下创建或修改表，将返回错误。\n- 该功能基于 MySQL 8.0 的特性 [`sql_require_primary_key`](https://dev.mysql.com/doc/refman/8.0/en/server-system-variables.html#sysvar_sql_require_primary_key)。\n- 强烈推荐在使用 TiCDC 时启用该变量，因为同步数据变更至 MySQL sink 时要求表必须有主键。\n- 如果启用了该变量，且使用了 TiDB Data Migration (DM) 来迁移数据，建议在 [DM 任务配置文件](/dm/task-configuration-file-full.md#完整配置文件示例)里的 `session` 中添加该系统变量 `sql_require_primary_key` 并设置为 `OFF`，否则会导致 DM 任务创建失败。\n\n### `sql_select_limit` <span class=\"version-mark\">从 v4.0.2 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`18446744073709551615`\n- 范围：`[0, 18446744073709551615]`\n- 单位：行\n- `SELECT` 语句返回的最大行数。\n\n### `ssl_ca`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 证书颁发机构 (CA) 文件的位置。若文件不存在，则变量值为空。该变量的值由 TiDB 配置项 [`ssl-ca`](/tidb-configuration-file.md#ssl-ca) 定义。\n\n### `ssl_cert`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 用于 SSL/TLS 连接的证书文件的位置。若文件不存在，则变量值为空。该变量的值由 TiDB 配置项 [`ssl-cert`](/tidb-configuration-file.md#ssl-cert) 定义。\n\n### `ssl_key`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 用于 SSL/TLS 连接的私钥文件的位置。若文件不存在，则变量值为空。该变量的值由 TiDB 配置项 [`ssl-key`](/tidb-configuration-file.md#ssl-cert) 定义。\n\n### `system_time_zone`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：（随系统）\n- 该变量显示首次引导启动 TiDB 时的系统时区。另请参阅 [`time_zone`](#time_zone)。\n\n### `tidb_adaptive_closest_read_threshold` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`4096`\n- 取值范围：`[0, 9223372036854775807]`\n- 单位：字节\n- 这个变量用于控制当 [`replica-read`](#tidb_replica_read-从-v40-版本开始引入) 设置为 `closest-adaptive` 时，优先将读请求发送至 TiDB server 所在区域副本的阈值。当读请求预估的返回结果的大小超过此阈值时，TiDB 会将读请求优先发送至同一可用区的副本，否则会发送至 leader 副本。\n\n### `tidb_allow_tiflash_cop` <span class=\"version-mark\">从 v7.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 当 TiDB 给 TiFlash 下推计算任务时，有三种方法（或协议）可供选择：Cop、BatchCop 和 MPP。相比于 Cop 和 BatchCop，MPP 协议更加成熟，提供更好的任务和资源管理。因此，更推荐使用 MPP 协议。\n\n    * `0` 或 `OFF`：优化器仅生成使用 TiFlash MPP 协议的计划。\n    * `1` 或 `ON`：优化器根据成本估算从 Cop、BatchCop 和 MPP 协议中选择一个用于生成执行计划。\n\n### `tidb_allow_batch_cop` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[0, 2]`\n- 这个变量用于控制 TiDB 向 TiFlash 发送 coprocessor 请求的方式，有以下几种取值：\n\n    * 0：从不批量发送请求\n    * 1：aggregation 和 join 的请求会进行批量发送\n    * 2：所有的 cop 请求都会批量发送\n\n### `tidb_allow_fallback_to_tikv` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：\"\"\n- 这个变量表示将 TiKV 作为备用存储引擎的存储引擎列表。当该列表中的存储引擎发生故障导致 SQL 语句执行失败时，TiDB 会使用 TiKV 作为存储引擎再次执行该 SQL 语句。目前支持设置该变量为 \"\" 或者 \"tiflash\"。如果设置该变量为 \"tiflash\"，当 TiFlash 返回超时错误（对应的错误码为 ErrTiFlashServerTimeout）时，TiDB 会使用 TiKV 作为存储引擎再次执行该 SQL 语句。\n\n### `tidb_allow_function_for_expression_index` <span class=\"version-mark\">从 v5.2.0 版本开始引入</span>\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`json_array, json_array_append, json_array_insert, json_contains, json_contains_path, json_depth, json_extract, json_insert, json_keys, json_length, json_merge_patch, json_merge_preserve, json_object, json_pretty, json_quote, json_remove, json_replace, json_schema_valid, json_search, json_set, json_storage_size, json_type, json_unquote, json_valid, lower, md5, reverse, tidb_shard, upper, vitess_hash`\n- 这个只读变量用于显示创建[表达式索引](/sql-statements/sql-statement-create-index.md#表达式索引)所允许使用的函数。\n\n### `tidb_allow_mpp` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否使用 TiFlash 的 MPP 模式执行查询，可以设置的值包括：\n    - `0` 或 `OFF`，代表从不使用 MPP 模式。如果在 v7.3.0 及之后的版本将该变量值设置为 `0` 或 `OFF`，你需要同时开启 [`tidb_allow_tiflash_cop`](/system-variables.md#tidb_allow_tiflash_cop-从-v730-版本开始引入) 变量，否则可能遇到查询报错。\n    - `1` 或 `ON`，代表由优化器根据代价估算选择是否使用 MPP 模式（默认）。\n\nMPP 是 TiFlash 引擎提供的分布式计算框架，允许节点之间的数据交换并提供高性能、高吞吐的 SQL 算法。MPP 模式选择的详细说明参见[控制是否选择 MPP 模式](/tiflash/use-tiflash-mpp-mode.md#控制是否选择-mpp-模式)。\n\n### `tidb_allow_remove_auto_inc` <span class=\"version-mark\">从 v2.1.18 和 v3.0.4 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制是否允许通过 `ALTER TABLE MODIFY` 或 `ALTER TABLE CHANGE` 来移除某个列的 `AUTO_INCREMENT` 属性。默认 (`OFF`) 为不允许。\n\n### tidb_analyze_column_options <span class=\"version-mark\">从 v8.3.0 版本开始引入</span>\n\n> **注意：**\n>\n> - 该变量只在 [`tidb_analyze_version`](#tidb_analyze_version-从-v510-版本开始引入) 设置为 `2` 时生效。\n> - 如果将 TiDB 集群从 v8.3.0 之前的版本升级至 v8.3.0 或更高版本，该变量会默认设置为 `ALL`，以保持原有行为。\n> - 从 v8.3.0 开始，对于新部署的 TiDB 集群，该变量默认设置为 `PREDICATE`。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`PREDICATE`\n- 可选值：`ALL`，`PREDICATE`\n- 该变量控制 `ANALYZE TABLE` 语句的行为。将其设置为 `PREDICATE` 表示仅收集 [predicate columns](/statistics.md#收集部分列的统计信息) 的统计信息；将其设置为 `ALL` 表示收集所有列的统计信息。在使用 OLAP 查询的场景中，建议将其设置为 `ALL`，否则查询性能可能会显著下降。\n\n### `tidb_analyze_distsql_scan_concurrency` <span class=\"version-mark\">从 v7.6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`4`\n- 范围：`[0, 4294967295]`。在 v8.2.0 之前版本中，最小值为 `1`。当设置为 `0` 时，TiDB 会根据集群规模自适应调整并发度。\n- 这个变量用来设置执行 `ANALYZE` 时 `scan` 操作的并发度。\n\n### `tidb_analyze_partition_concurrency`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`2`。TiDB v7.4.0 及其之前版本默认值为 `1`。\n- 范围：`[1, 128]`。在 v8.4.0 之前版本中，取值范围是 `[1, 18446744073709551615]`。\n- 这个变量用于 TiDB analyze 分区表时，写入分区表统计信息的并发度。\n\n### `tidb_analyze_version` <span class=\"version-mark\">从 v5.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`2`\n- 范围：`[1, 2]`\n- 这个变量用于控制 TiDB 收集统计信息的行为。\n- 在 v5.3.0 及之后的版本中，该变量的默认值为 `2`，具体可参照[常规统计信息](/statistics.md)文档。如果从 v5.3.0 之前版本的集群升级至 v5.3.0 及之后的版本，`tidb_analyze_version` 的默认值不发生变化。\n\n### `tidb_analyze_skip_column_types` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"json,blob,mediumblob,longblob,mediumtext,longtext\"。在 v8.2.0 之前，默认值为 \"json,blob,mediumblob,longblob\"。\n- 可选值：\"json,blob,mediumblob,longblob,text,mediumtext,longtext\"\n- 这个变量表示在执行 `ANALYZE` 命令收集统计信息时，跳过哪些类型的列的统计信息收集。该变量仅适用于 [`tidb_analyze_version = 2`](#tidb_analyze_version-从-v510-版本开始引入) 的情况。即使使用 `ANALYZE TABLE t COLUMNS c1, ..., cn` 语法指定列，如果指定的列的类型在 `tidb_analyze_skip_column_types` 中，也不会收集该列的统计信息。\n\n```sql\nmysql> SHOW CREATE TABLE t;\n+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Table | Create Table                                                                                                                                                                                                             |\n+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| t     | CREATE TABLE `t` (\n  `a` int DEFAULT NULL,\n  `b` varchar(10) DEFAULT NULL,\n  `c` json DEFAULT NULL,\n  `d` blob DEFAULT NULL,\n  `e` longblob DEFAULT NULL\n) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COLLATE=utf8mb4_bin |\n+-------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> SELECT @@tidb_analyze_skip_column_types;\n+----------------------------------+\n| @@tidb_analyze_skip_column_types |\n+----------------------------------+\n| json,blob,mediumblob,longblob    |\n+----------------------------------+\n1 row in set (0.00 sec)\n\nmysql> ANALYZE TABLE t;\nQuery OK, 0 rows affected, 1 warning (0.05 sec)\n\nmysql> SELECT job_info FROM mysql.analyze_jobs ORDER BY end_time DESC LIMIT 1;\n+---------------------------------------------------------------------+\n| job_info                                                            |\n+---------------------------------------------------------------------+\n| analyze table columns a, b with 256 buckets, 500 topn, 1 samplerate |\n+---------------------------------------------------------------------+\n1 row in set (0.00 sec)\n\nmysql> ANALYZE TABLE t COLUMNS a, c;\nQuery OK, 0 rows affected, 1 warning (0.04 sec)\n\nmysql> SELECT job_info FROM mysql.analyze_jobs ORDER BY end_time DESC LIMIT 1;\n+------------------------------------------------------------------+\n| job_info                                                         |\n+------------------------------------------------------------------+\n| analyze table columns a with 256 buckets, 500 topn, 1 samplerate |\n+------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n### `tidb_auto_analyze_concurrency` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[1, 2147483647]`\n- 这个变量用来设置单个自动统计信息收集任务内部的并发度。在 v8.4.0 之前的版本中，该并发度固定为 `1`。你可以根据集群资源情况提高该并发度，从而加快统计信息收集任务的执行速度。\n\n### `tidb_auto_analyze_end_time`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：时间\n- 默认值：`23:59 +0000`\n- 这个变量用来设置一天中允许自动 ANALYZE 更新统计信息的结束时间。例如，只允许在 UTC 时间的凌晨 1:00 至 3:00 之间自动更新统计信息，可以设置如下：\n\n    - `tidb_auto_analyze_start_time='01:00 +0000'`\n    - `tidb_auto_analyze_end_time='03:00 +0000'`\n\n### `tidb_auto_analyze_partition_batch_size` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`128`。对于 TiDB v7.6.0 之前的版本，默认值为 `1`。\n- 范围：`[1, 1024]`\n- 用于设置 TiDB [自动 analyze](/statistics.md#自动更新) 分区表（即自动收集分区表上的统计信息）时，每次同时 analyze 分区的个数。\n- 若该变量值小于分区表的分区数，则 TiDB 会分多批自动 analyze 该分区表的所有分区。若该变量值大于等于分区表的分区数，则 TiDB 会同时 analyze 该分区表的所有分区。\n- 若分区表个数远大于该变量值，且自动 analyze 花费时间较长，可调大该参数的值以减少耗时。\n\n### `tidb_auto_analyze_ratio`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：浮点数\n- 默认值：`0.5`\n- 范围：`(0, 1]`，v8.0.0 及之前版本范围为 `[0, 18446744073709551615]`。\n- 这个变量用来设置 TiDB 在后台自动执行 [`ANALYZE TABLE`](/sql-statements/sql-statement-analyze-table.md) 更新统计信息的阈值。`0.5` 指的是当表中超过 50% 的行被修改时，触发自动 ANALYZE 更新。可以指定 `tidb_auto_analyze_start_time` 和 `tidb_auto_analyze_end_time` 来限制自动 ANALYZE 的时间。\n\n> **注意：**\n>\n> 当系统变量 `tidb_enable_auto_analyze` 设置为 `ON` 时，TiDB 才会触发 `auto_analyze`。\n\n### `tidb_auto_analyze_start_time`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：时间\n- 默认值：`00:00 +0000`\n- 这个变量用来设置一天中允许自动 ANALYZE 更新统计信息的开始时间。例如，只允许在 UTC 时间的凌晨 1:00 至 3:00 之间自动更新统计信息，可以设置如下：\n\n    - `tidb_auto_analyze_start_time='01:00 +0000'`\n    - `tidb_auto_analyze_end_time='03:00 +0000'`\n\n### `tidb_auto_build_stats_concurrency` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[1, 256]`\n- 这个变量用来设置执行统计信息自动更新的并发度。\n\n### `tidb_backoff_lock_fast`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`10`\n- 范围：`[1, 2147483647]`\n- 这个变量用来设置读请求遇到锁的 backoff 时间。\n\n### `tidb_backoff_weight`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`2`\n- 范围：`[0, 2147483647]`\n- 这个变量用来给 TiDB 的 `backoff` 最大时间增加权重，即内部遇到网络或其他组件 (TiKV, PD) 故障时，发送重试请求的最大重试时间。可以通过这个变量来调整最大重试时间，最小值为 1。\n\n    例如，TiDB 向 PD 取 TSO 的基础超时时间是 15 秒，当 `tidb_backoff_weight = 2` 时，取 TSO 的最大超时时间为：基础时间 \\* 2 等于 30 秒。\n\n    在网络环境较差的情况下，适当增大该变量值可以有效缓解因为超时而向应用端报错的情况；而如果应用端希望更快地接到报错信息，则应该尽量减小该变量的值。\n\n### `tidb_batch_commit`\n\n> **警告：**\n>\n> **不建议**开启此变量。\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制是否启用已废弃的 batch-commit 特性。当该变量开启时，事务可能会通过分组一些语句被拆分为多个事务，并被非原子地提交。不推荐使用这种方式。\n\n### `tidb_batch_delete`\n\n> **警告：**\n>\n> 该变量与废弃的 batch-dml 特性相关，可能会导致数据损坏。因此，不建议开启该变量来使用 batch-dml。作为替代，请使用[非事务 DML 语句](/non-transactional-dml.md)。\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制是否启用已废弃的 batch-dml 特性中的 batch-delete 特性。当该变量开启时，`DELETE` 语句可能会被拆分为多个事务，并被非原子地提交。要使该特性生效，还需要开启 `tidb_enable_batch_dml` 并将 `tidb_dml_batch_size` 的值设置为正数。不推荐使用这种方式。\n\n### `tidb_batch_insert`\n\n> **警告：**\n>\n> 该变量与废弃的 batch-dml 特性相关，可能会导致数据损坏。因此，不建议开启该变量来使用 batch-dml。作为替代，请使用[非事务 DML 语句](/non-transactional-dml.md)。\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制是否启用已废弃的 batch-dml 特性中的 batch-insert 特性。当该变量开启时，`INSERT` 语句可能会被拆分为多个事务，并被非原子地提交。要使该特性生效，还需要开启 `tidb_enable_batch_dml` 并将 `tidb_dml_batch_size` 的值设置为正数。不推荐使用这种方式。\n\n### `tidb_batch_pending_tiflash_count` <span class=\"version-mark\">从 v6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`4000`\n- 范围：`[0, 4294967295]`\n- 使用 `ALTER DATABASE SET TIFLASH REPLICA` 语句为 TiFlash 添加副本时，能容许的不可用表的个数上限。如果超过该上限，则会停止或者以非常慢的速度为库中的剩余表设置 TiFlash 副本。\n\n### `tidb_broadcast_join_threshold_count` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`10240`\n- 范围：`[0, 9223372036854775807]`\n- 单位：行\n- 如果 join 的对象为子查询，优化器无法估计子查询结果集大小，在这种情况下通过结果集行数判断。如果子查询的行数估计值小于该变量，则选择 Broadcast Hash Join 算法。否则选择 Shuffled Hash Join 算法。\n- 开启 [`tidb_prefer_broadcast_join_by_exchange_data_size`](/system-variables.md#tidb_prefer_broadcast_join_by_exchange_data_size-从-v710-版本开始引入) 功能后，该变量将不再生效。\n\n### `tidb_broadcast_join_threshold_size` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`104857600` (100 MiB)\n- 范围：`[0, 9223372036854775807]`\n- 单位：字节\n- 如果表大小（字节数）小于该值，则选择 Broadcast Hash Join 算法。否则选择 Shuffled Hash Join 算法。\n- 开启 [`tidb_prefer_broadcast_join_by_exchange_data_size`](/system-variables.md#tidb_prefer_broadcast_join_by_exchange_data_size-从-v710-版本开始引入) 功能后，该变量将不再生效。\n\n### `tidb_build_stats_concurrency`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 单位：线程\n- 默认值：`2`。TiDB v7.4.0 及其之前版本默认值为 `4`。\n- 取值范围：`[1, 256]`\n- 这个变量用来设置 ANALYZE 语句执行时并发度。\n- 当这个变量被设置得更大时，会对其它的查询语句执行性能产生一定影响。\n\n### `tidb_build_sampling_stats_concurrency` <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 单位：线程\n- 默认值：`2`\n- 取值范围：`[1, 256]`\n- 这个变量用来设置 `ANALYZE` 过程中的采样并发度。\n- 当这个变量被设置得更大时，会对其它的查询语句执行性能产生一定影响。\n\n### `tidb_capture_plan_baselines` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启[自动捕获绑定](/sql-plan-management.md#自动捕获绑定-baseline-capturing)功能。该功能依赖 Statement Summary，因此在使用自动绑定之前需打开 Statement Summary 开关。\n- 开启该功能后会定期遍历一次 Statement Summary 中的历史 SQL 语句，并为至少出现两次的 SQL 语句自动创建绑定。\n\n### `tidb_cdc_write_source` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否持久化到集群：否\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值： `0`\n- 范围：`[0, 15]`\n- 当变量非 `0` 时，该 SESSION 写入的数据将被视为是由 TiCDC 写入的。这个变量仅由 TiCDC 设置，任何时候都不应该手动调整该变量。\n\n### `tidb_check_mb4_value_in_utf8`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 设置该变量为 `ON` 可强制只存储[基本多文种平面 (BMP)](https://zh.wikipedia.org/zh-hans/Unicode字符平面映射) 编码区段内的 `utf8` 字符值。若要存储 BMP 区段外的 `utf8` 值，推荐使用 `utf8mb4` 字符集。\n- 早期版本的 TiDB 中 (v2.1.x)，`utf8` 检查更为宽松。如果你的 TiDB 集群是从早期版本升级的，推荐关闭该变量，详情参阅[升级与升级后常见问题](/faq/upgrade-faq.md)。\n\n### `tidb_committer_concurrency` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`128`\n- 范围：`[1, 10000]`\n- 在单个事务的提交阶段，用于执行提交操作相关请求的 goroutine 数量。\n- 若提交的事务过大，事务提交时的流控队列等待耗时可能会过长。此时，可以通过调大该配置项来加速提交。\n- 在 v6.1.0 之前这个开关通过 TiDB 配置文件 (`performance.committer-concurrency`) 进行配置，升级到 v6.1.0 时会自动继承原有设置。\n\n### `tidb_checksum_table_concurrency`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`4`\n- 取值范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置 [`ADMIN CHECKSUM TABLE`](/sql-statements/sql-statement-admin-checksum-table.md) 语句执行时扫描索引的并发度。当这个变量被设置得更大时，会对其它的查询语句执行性能产生一定影响。\n\n### `tidb_config`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 这个变量是一个只读变量，用来获取当前 TiDB Server 的配置信息。\n\n### `tidb_constraint_check_in_place`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量仅适用于乐观事务模型。悲观事务模式中的行为由 [`tidb_constraint_check_in_place_pessimistic`](#tidb_constraint_check_in_place_pessimistic-从-v630-版本开始引入) 控制。\n- 当这个变量设置为 `OFF` 时，唯一索引的重复值检查会被推迟到事务提交时才进行。这有助于提高性能，但对于某些应用，可能导致非预期的行为。详情见[约束](/constraints.md#乐观事务)。\n\n    - 乐观事务模型下将 `tidb_constraint_check_in_place` 设置为 `OFF`：\n\n        {{< copyable \"sql\" >}}\n\n        ```sql\n        create table t (i int key);\n        insert into t values (1);\n        begin optimistic;\n        insert into t values (1);\n        ```\n\n        ```\n        Query OK, 1 row affected\n        ```\n\n        {{< copyable \"sql\" >}}\n\n        ```sql\n        tidb> commit; -- 事务提交时才检查\n        ```\n\n        ```\n        ERROR 1062 : Duplicate entry '1' for key 't.PRIMARY'\n        ```\n\n    - 乐观事务模型下将 `tidb_constraint_check_in_place` 设置为 `ON`：\n\n        {{< copyable \"sql\" >}}\n\n        ```sql\n        set @@tidb_constraint_check_in_place=ON;\n        begin optimistic;\n        insert into t values (1);\n        ```\n\n        ```\n        ERROR 1062 : Duplicate entry '1' for key 't.PRIMARY'\n        ```\n\n### `tidb_constraint_check_in_place_pessimistic` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：当配置项 [`pessimistic-txn.constraint-check-in-place-pessimistic`](/tidb-configuration-file.md#constraint-check-in-place-pessimistic-从-v640-版本开始引入) 为默认值 `true` 时，该变量的默认值为 `ON`。当配置项 [`pessimistic-txn.constraint-check-in-place-pessimistic`](/tidb-configuration-file.md#constraint-check-in-place-pessimistic-从-v640-版本开始引入) 为 `false` 时，该变量的默认值为 `OFF`。\n- 该变量仅适用于悲观事务模型。乐观事务模式中的行为由 [`tidb_constraint_check_in_place`](#tidb_constraint_check_in_place) 控制。\n- 当这个变量设置为 `OFF` 时，唯一约束检查会被推迟到下一次需要对这个索引加锁的语句执行时，或事务提交时才进行。这有助于提高性能，但对于某些应用，可能导致非预期的行为。详情见[约束](/constraints.md#悲观事务)。\n- 关闭该变量可能会导致悲观事务中返回 `LazyUniquenessCheckFailure` 报错。返回该错误时，TiDB 将会回滚当前事务。\n- 关闭该变量后，悲观事务中不支持使用 [`SAVEPOINT`](/sql-statements/sql-statement-savepoint.md) 功能。\n- 关闭该变量时，commit 语句可能会报出 `Write conflict` 错误或 `Duplicate entry` 错误，两种错误都意味着事务回滚。\n\n    - 悲观事务模型下将 `tidb_constraint_check_in_place_pessimistic` 设置为 `OFF`：\n\n        {{< copyable \"sql\" >}}\n\n        ```sql\n        set @@tidb_constraint_check_in_place_pessimistic=OFF;\n        create table t (i int key);\n        insert into t values (1);\n        begin pessimistic;\n        insert into t values (1);\n        ```\n\n        ```\n        Query OK, 1 row affected\n        ```\n\n        ```sql\n        tidb> commit; -- 事务提交时才检查\n        ```\n\n        ```\n        ERROR 1062 : Duplicate entry '1' for key 't.PRIMARY'\n        ```\n\n    - 悲观事务模型下将 `tidb_constraint_check_in_place_pessimistic` 设置为 `ON`：\n\n        ```sql\n        set @@tidb_constraint_check_in_place_pessimistic=ON;\n        begin pessimistic;\n        insert into t values (1);\n        ```\n\n        ```\n        ERROR 1062 : Duplicate entry '1' for key 't.PRIMARY'\n        ```\n\n### `tidb_cost_model_version` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n> **注意：**\n>\n> - 自 v6.5.0 开始，新创建的 TiDB 集群默认使用 Cost Model Version 2。如果从 v6.4.0 及之前版本的集群升级到 v6.5.0 及之后的版本，`tidb_cost_model_version` 的值不发生变化。\n> - 切换代价模型版本可能会引起查询计划的变动。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`2`\n- 取值范围：`[1, 2]`\n- 可选值：\n    - `1`：使用 Cost Model Version 1 代价模型。TiDB v6.4.0 及之前的版本默认使用 Cost Model Version 1。\n    - `2`：使用 Cost Model Version 2 代价模型。TiDB v6.5.0 正式发布了代价模型 [Cost Model Version 2](/cost-model.md#cost-model-version-2)，在内部测试中比 Version 1 版本的代价模型更加准确。\n- 代价模型会影响优化器对计划的选择，具体可见[代价模型](/cost-model.md)。\n\n### `tidb_current_ts`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 取值范围：`[0, 9223372036854775807]`\n- 这个变量是一个只读变量，用来获取当前事务的时间戳。\n\n### `tidb_ddl_disk_quota` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`107374182400` (100 GiB)\n- 范围：`[107374182400, 1125899906842624]` ([100 GiB, 1 PiB])\n- 单位：字节\n- 这个变量仅在 [`tidb_ddl_enable_fast_reorg`](#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入) 开启的情况下生效，用于设置创建索引的回填过程中本地存储空间的使用限制。\n\n### `tidb_ddl_enable_fast_reorg` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制是否开启添加索引加速功能，来提升创建索引回填过程的速度。开启该变量对于数据量较大的表有一定的性能提升。\n- TiDB v7.1.0 引入了快速加索引功能的检查点机制，即使 TiDB owner 因故障重启或者切换，也能够通过自动定期保存的检查点恢复部分进度。\n- 要验证已经完成的 `ADD INDEX` 操作是否使用了添加索引加速功能，可以执行 [`ADMIN SHOW DDL JOBS`](/sql-statements/sql-statement-admin-show-ddl.md#admin-show-ddl-jobs) 语句查看 `JOB_TYPE` 一列中是否含有 `ingest` 字样。\n\n> **注意：**\n>\n> * 要使用索引加速功能，你需要提供一个可写且具有足够空余空间的临时路径 [`temp-dir`](/tidb-configuration-file.md#temp-dir-从-v630-版本开始引入)。如果 `temp-dir` 无法使用，TiDB 会退回到非加速的索引创建方式。建议将 `temp-dir` 挂载在 SSD 磁盘上。\n>\n> * 在升级到 v6.5.0 及以上版本时，请确保 TiDB 的 [`temp-dir`](/tidb-configuration-file.md#temp-dir-从-v630-版本开始引入) 路径已正确挂载了 SSD 磁盘，并确保运行 TiDB 的操作系统用户对该目录有读写权限，否则在运行时可能产生不可预知的问题。该参数是 TiDB 的配置参数，设置后需要重启 TiDB 才能生效。因此，在升级前提前进行设置，可以避免再次重启。\n\n### `tidb_enable_dist_task` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`ON`\n- 这个变量用于控制是否开启 [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)。开启分布式执行框架后，DDL 和 Import 等将会由集群中多个 TiDB 节点共同完成。\n- 从 TiDB v7.1.0 开始，支持分布式执行分区表的 [`ADD INDEX`](/sql-statements/sql-statement-add-index.md)。\n- 从 TiDB v7.2.0 开始，支持分布式导入任务 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md)。\n- 从 TiDB v8.1.0 开始，该变量默认开启。如果要从低版本的集群升级到 v8.1.0 或更高版本，且该集群已开启分布式执行框架，为了避免升级期间 `ADD INDEX` 操作可能导致数据索引不一致的问题，请在升级前关闭分布式执行框架（即将 `tidb_enable_dist_task` 设置为 `OFF`），升级后再手动开启。\n- 该变量由 `tidb_ddl_distribute_reorg` 改名而来。\n\n### `tidb_cloud_storage_uri` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n> **注意：**\n>\n> 目前，[全局排序](/tidb-global-sort.md)会使用大量 TiDB 节点的计算与内存资源。对于在线增加索引等同时有用户业务在运行的场景，建议为集群添加新的 TiDB 节点，为这些 TiDB 节点设置 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入)，并连接到这些节点上创建任务。这样分布式框架就会将任务调度到这些节点上，将工作负载与其他 TiDB 节点隔离，以减少执行后端任务（如 `ADD INDEX` 和 `IMPORT INTO`）对用户业务的影响。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`\"\"`\n- 该变量用来指定[全局排序](/tidb-global-sort.md)中使用的 Amazon S3 云存储的 URI。在开启 [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md)后，你可以配置 URI 指向具有访问存储所需权限的云存储路径，以此来实现全局排序的功能。更多详情，参考 [Amazon S3 的 URI 格式](/external-storage-uri.md#amazon-s3-uri-格式)。\n- 以下语句支持全局排序功能：\n    - [`ADD INDEX`](/sql-statements/sql-statement-add-index.md) 语句。\n    - 用于将数据导入本地部署的 TiDB 的 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md) 语句。对于 TiDB Cloud，`IMPORT INTO` 语句不适用全局排序。\n\n### `tidb_ddl_error_count_limit`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`512`\n- 范围：`[0, 9223372036854775807]`\n- 这个变量用来控制 DDL 操作失败重试的次数。失败重试次数超过该参数的值后，会取消出错的 DDL 操作。\n\n### `tidb_ddl_flashback_concurrency` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`64`\n- 范围：`[1, 256]`\n- 这个变量用来控制 [`FLASHBACK CLUSTER`](/sql-statements/sql-statement-flashback-cluster.md) 的并发数。\n\n### `tidb_ddl_reorg_batch_size`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`256`\n- 范围：`[32, 10240]`\n- 单位：行\n- 这个变量用来设置 DDL 操作 `re-organize` 阶段的 batch size。比如 `ADD INDEX` 操作，需要回填索引数据，通过并发 `tidb_ddl_reorg_worker_cnt` 个 worker 一起回填数据，每个 worker 以 batch 为单位进行回填。\n\n    - 当设置 `tidb_ddl_enable_fast_reorg` 为 `OFF` 时，`ADD INDEX` 会通过事务的方式执行，执行时如果 `ADD INDEX` 的目标列有较多 `UPDATE` 或者 `REPLACE` 等更新操作，batch size 设置的值越大，事务冲突的概率也会越大。此时建议调小 batch size 的值，最小值是 32。\n    - 在没有事务冲突的情况下，或者当 `tidb_ddl_enable_fast_reorg` 为 `ON` 时，batch size 可设为较大值，这样回填数据的速度更快，但是 TiKV 的写入压力也会变大。设置 batch size 时需要参考 `tidb_ddl_reorg_worker_cnt` 的设置值，详情见[线上负载与 `ADD INDEX` 相互影响测试](/benchmark/online-workloads-and-add-index-operations.md)。\n    - 从 v8.3.0 版本开始，该参数支持 SESSION 级别的设置，因此修改 GLOBAL 级别的参数值不会影响当前正在运行的 DDL，而只会对新建 SESSION 中提交的 DDL 生效。\n\n### `tidb_ddl_reorg_priority`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`PRIORITY_LOW`\n- 可选值：`PRIORITY_LOW`、`PRIORITY_NORMAL`、`PRIORITY_HIGH`\n- 这个变量用来设置 `ADD INDEX` 操作 `re-organize` 阶段的执行优先级，可设置为 `PRIORITY_LOW`/`PRIORITY_NORMAL`/`PRIORITY_HIGH`。\n\n### `tidb_ddl_reorg_max_write_speed` <span class=\"version-mark\">从 v7.5.5 和 v8.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 1125899906842624]`（即最大可设置为 1 PiB）\n- 这个变量用于限制每个 TiKV 节点写入的带宽，仅在开启添加索引加速功能时生效（由变量 [`tidb_ddl_enable_fast_reorg`](#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入) 控制）。在数据量特别大的情况下（如数十亿行数据），降低加索引时写入 TiKV 节点的带宽可以有效减少对业务负载的影响。\n- 默认值 `0` 表示不限制写入带宽。默认单位为字节每秒，也可以通过 `'1GiB'`、`'256MiB'` 等格式设置该变量。\n\n### `tidb_ddl_reorg_worker_cnt`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`4`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置 DDL 操作 `re-organize` 阶段的并发度。\n- 从 v8.3.0 版本开始，该参数支持 SESSION 级别的设置，因此修改 GLOBAL 级别的参数值不会影响当前正在运行的 DDL，而只会对新建 SESSION 中提交的 DDL 生效。\n\n### `tidb_enable_fast_create_table` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`。在 v8.5.0 之前，默认值为 `OFF`。\n- 这个变量用于控制是否开启 [TiDB 加速建表](/accelerated-table-creation.md)。\n- 从 TiDB v8.0.0 开始，支持使用 `tidb_enable_fast_create_table` 加速建表 [`CREATE TABLE`](/sql-statements/sql-statement-create-table.md)。\n- 该变量是由 v7.6.0 中引入的 [`tidb_ddl_version`](https://docs.pingcap.com/zh/tidb/v7.6/system-variables#tidb_ddl_version-从-v760-版本开始引入) 更名而来。从 v8.0.0 开始，`tidb_ddl_version` 不再生效。\n- 从 TiDB v8.5.0 开始，新创建的集群默认开启 TiDB 加速建表功能，即 `tidb_enable_fast_create_table` 默认值为 `ON`。如果从 v8.4.0 及之前版本的集群升级至 v8.5.0 及之后的版本，`tidb_enable_fast_create_table` 的默认值不发生变化。\n\n### `tidb_default_string_match_selectivity` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点型\n- 默认值：`0.8`\n- 范围：`[0, 1]`\n- 这个变量用来设置过滤条件中的 `like`、`rlike`、`regexp` 函数在行数估算时的默认选择率，以及是否对这些函数启用 TopN 辅助估算。\n- TiDB 总是会尝试利用统计信息对过滤条件中的 `like` 进行估算，但是当 `like` 匹配的字符串太复杂时，或者面对 `rlike` 或 `regexp` 时，往往无法充分利用统计信息，转而使用 `0.8` 作为选择率，造成行数估算的误差较大。\n- 该变量可以用于修改这个行为，当变量被设为 `0` 以外的值时，会使用变量的值而不是默认的 `0.8` 作为选择率。\n- 如果将该变量的值设为 `0`，TiDB 在对上述三个函数进行行数估算时，会尝试利用统计信息中的 TopN 进行求值来提高估算精度，同时也会考虑统计信息中的 NULL 数。求值操作预计会造成少量性能损耗。这个功能生效的前提是统计信息是在 [`tidb_analyze_version`](#tidb_analyze_version-从-v510-版本开始引入) 设为 `2` 时收集的。\n- 当该变量的值被设为默认值以外的值的时候，会对 `not like`、`not rlike`、`not regexp` 的行数估算也进行相应的调整。\n\n### `tidb_disable_txn_auto_retry`\n\n> **警告：**\n>\n> 从 v8.0.0 开始，该变量被废弃。废弃后，TiDB 不再支持乐观事务的自动重试。作为替代，当使用乐观事务模式发生冲突时，请在应用里捕获错误并重试，或改用[悲观事务模式](/pessimistic-transaction.md)。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来设置是否禁用显式的乐观事务自动重试，设置为 `ON` 时，不会自动重试，如果遇到事务冲突需要在应用层重试。\n\n    如果将该变量的值设为 `OFF`，TiDB 将会自动重试事务，这样在事务提交时遇到的错误更少。需要注意的是，这样可能会导致数据更新丢失。\n\n    这个变量不会影响自动提交的隐式事务和 TiDB 内部执行的事务，它们依旧会根据 `tidb_retry_limit` 的值来决定最大重试次数。\n\n    关于是否需要禁用自动重试，请参考[重试的局限性](/optimistic-transaction.md#重试的局限性)。\n\n    该变量只适用于乐观事务，不适用于悲观事务。悲观事务的重试次数由 [`max_retry_count`](/tidb-configuration-file.md#max-retry-count) 控制。\n\n### `tidb_distsql_scan_concurrency`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`15`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置 scan 操作的并发度。\n- AP 类应用适合较大的值，TP 类应用适合较小的值。对于 AP 类应用，最大值建议不要超过所有 TiKV 节点的 CPU 核数。\n- 若表的分区较多可以适当调小该参数（取决于扫描数据量的大小以及扫描频率），避免 TiKV 内存溢出 (OOM)。\n- 对于仅包含 `LIMIT` 子句的简单查询，如果 `LIMIT` 行数小于 100000，该查询的 scan 操作被下推到 TiKV 时，会将该变量的值视为 `1` 进行处理，以提升执行效率。\n- 对于查询语句 `SELECT MAX/MIN(col) FROM ...`，如果 `col` 列有索引且该索引的顺序与 `MAX(col)` 或 `MIN(col)` 函数所需的顺序一致，TiDB 会将该查询改写为 `SELECT col FROM ... LIMIT 1` 进行处理，该变量的值也将视为 `1` 进行处理。例如，对于 `SELECT MIN(col) FROM ...`，如果 `col` 列有升序排列的索引，TiDB 通过将该查询改写为 `SELECT col FROM ... LIMIT 1`，可以直接读取该索引中第一条数据，从而快速得到 `MIN(col)` 值。\n- 在对 [`SLOW_QUERY`](/information-schema/information-schema-slow-query.md) 表进行查询时，此变量可以控制解析慢日志文件的并发度。\n\n### `tidb_dml_batch_size`\n\n> **警告：**\n>\n> 该变量与废弃的 batch-dml 特性相关，可能会导致数据损坏。因此，不建议开启该变量来使用 batch-dml。作为替代，请使用[非事务 DML 语句](/non-transactional-dml.md)。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 2147483647]`\n- 单位：行\n- 这个变量的值大于 `0` 时，TiDB 会将 `INSERT` 语句在更小的事务中批量提交。这样可减少内存使用，确保大批量修改时事务大小不会达到 `txn-total-size-limit` 限制。\n- 只有变量值为 `0` 时才符合 ACID 要求。否则无法保证 TiDB 的原子性和隔离性要求。\n- 要使该特性生效，还需要开启 `tidb_enable_batch_dml`，以及至少开启 `tidb_batch_insert` 和 `tidb_batch_delete` 中的一个。\n\n> **注意：**\n>\n> 自 v7.0.0 起，`tidb_dml_batch_size` 对 [`LOAD DATA` 语句](/sql-statements/sql-statement-load-data.md)不再生效。\n\n### `tidb_dml_type` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n> **警告：**\n>\n> 批量 DML 执行方式 (`tidb_dml_type = \"bulk\"`) 目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。在当前版本中，使用批量 DML 执行方式执行超大事务时，可能会影响 TiCDC、TiFlash 和 TiKV 的 resolved-ts 模块的内存使用和执行效率，可能引发 OOM 问题。此外，BR 在遇到锁时也可能被阻塞无法继续执行。因此，不建议在启用这些组件和功能时使用。\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：字符串\n- 默认值：`\"standard\"`\n- 可选值：`\"standard\"`、`\"bulk\"`\n- 该变量用来设置 DML 语句的执行方式。\n    - `\"standard\"` 表示使用标准的 DML 执行方式，TiDB 事务在提交前缓存在内存中。适用于处理高并发且可能存在冲突的事务场景，为默认推荐使用的执行方式。\n    - `\"bulk\"` 表示使用 Pipelined DML 执行方式，适合于处理因大量数据写入导致 TiDB 内存使用过多的情况。更多信息，请参考 [Pipelined DML](/pipelined-dml.md)。\n\n### `tidb_enable_1pc` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 指定是否在只涉及一个 Region 的事务上启用一阶段提交特性。比起传统两阶段提交，一阶段提交能大幅降低事务提交延迟并提升吞吐。\n\n> **注意：**\n>\n> - 对于新创建的集群，默认值为 ON。对于升级版本的集群，如果升级前是 v5.0 以下版本，升级后默认值为 `OFF`。\n> - 启用该参数仅意味着一阶段提交成为可选的事务提交模式，实际由 TiDB 自行判断选择最合适的提交模式进行事务提交。\n\n### `tidb_enable_analyze_snapshot` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制 `ANALYZE` 读取历史时刻的数据还是读取最新的数据。当该变量设置为 `ON` 时，`ANALYZE` 读取 `ANALYZE` 开始时刻的历史数据。当该变量设置为 `OFF` 时，`ANALYZE` 读取最新的数据。\n- 在 v5.2 之前，`ANALYZE` 读取最新的数据。v5.2 至 v6.1 版本 `ANALYZE` 读取 `ANALYZE` 开始时刻的历史数据。\n\n> **警告：**\n>\n> 如果 `ANALYZE` 读取 `ANALYZE` 开始时刻的历史数据，长时间的 `AUTO ANALYZE` 可能会因为历史数据被 GC 而出现 `GC life time is shorter than transaction duration` 的报错。\n\n### `tidb_enable_async_commit` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量控制是否启用 Async Commit 特性，使事务两阶段提交的第二阶段于后台异步进行。开启本特性能降低事务提交的延迟。\n\n> **注意：**\n>\n> - 对于新创建的集群，默认值为 ON。对于升级版本的集群，如果升级前是 v5.0 以下版本，升级后默认值为 `OFF`。\n> - 启用该参数仅意味着 Async Commit 成为可选的事务提交模式，实际由 TiDB 自行判断选择最合适的提交模式进行事务提交。\n\n### `tidb_enable_auto_analyze` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量控制 TiDB 是否以后台操作自动更新表的统计信息。\n- 在 v6.1.0 之前这个开关通过 TiDB 配置文件 (`performance.run-auto-analyze`) 进行配置，升级到 v6.1.0 时会自动继承原有设置。\n\n### `tidb_enable_auto_analyze_priority_queue` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量控制是否启用优先队列来调度自动收集统计信息的任务。开启该变量后，TiDB 会优先收集那些更有收集价值的表，例如新创建的索引、发生分区变更的分区表等。同时，TiDB 也会优先处理那些健康度较低的表，将它们安排在队列的前端。\n\n### `tidb_enable_auto_increment_in_generated`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否允许在创建生成列或者表达式索引时引用自增列。\n\n### `tidb_enable_batch_dml`\n\n> **警告：**\n>\n> 该变量与废弃的 batch-dml 特性相关，可能会导致数据损坏。因此，不建议开启该变量来使用 batch-dml。作为替代，请使用[非事务 DML 语句](/non-transactional-dml.md)。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制是否启用废弃的 batch-dml 特性。启用该变量后，部分语句可能会被拆分为多个事务执行，这是非原子性的，使用时需谨慎。使用 batch-dml 时，必须确保正在操作的数据没有并发操作。要使该变量生效，还需要为 `tidb_batch_dml_size` 指定一个正值，并启用 `tidb_batch_insert` 和 `tidb_batch_delete` 中的至少一个。\n\n### `tidb_enable_cascades_planner`\n\n> **警告：**\n>\n> 目前 cascades planner 为实验特性，不建议在生产环境中使用。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启 cascades planner。\n\n### `tidb_enable_check_constraint` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否启用 [`CHECK` 约束](/constraints.md#check-约束)。\n\n### `tidb_enable_chunk_rpc` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来设置是否启用 Coprocessor 的 `Chunk` 数据编码格式。\n\n### `tidb_enable_clustered_index` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`ON`\n- 可选值：`OFF`，`ON`，`INT_ONLY`\n- 这个变量用于控制默认情况下表的主键是否使用[聚簇索引](/clustered-indexes.md)。“默认情况”即不显式指定 `CLUSTERED`/`NONCLUSTERED` 关键字的情况。可设置为 `OFF`/`ON`/`INT_ONLY`。\n    - `OFF` 表示所有主键默认使用非聚簇索引。\n    - `ON` 表示所有主键默认使用聚簇索引。\n    - `INT_ONLY` 此时的行为受配置项 `alter-primary-key` 控制。如果该配置项取值为 `true`，则所有主键默认使用非聚簇索引；如果该配置项取值为 `false`，则由单个整数类型的列构成的主键默认使用聚簇索引，其他类型的主键默认使用非聚簇索引。\n\n### `tidb_enable_collect_execution_info`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否同时将各个执行算子的执行信息记录入 slow query log 中，以及是否维护[访问索引有关的统计信息](/information-schema/information-schema-tidb-index-usage.md)。\n\n### `tidb_enable_column_tracking` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n> **警告：**\n>\n> 从 v8.3.0 开始，该变量被废弃，TiDB 默认收集 [predicate columns](/glossary.md#predicate-columns) 的统计信息。更多信息，参见 [`tidb_analyze_column_options`](#tidb_analyze_column_options-从-v830-版本开始引入)。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启 TiDB 对 `PREDICATE COLUMNS` 的收集。关闭该变量后，之前收集的 `PREDICATE COLUMNS` 会被清除。详情见[收集部分列的统计信息](/statistics.md#收集部分列的统计信息)。\n\n### `tidb_enable_ddl` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`ON`\n- 可选值：`OFF`，`ON`\n- 用于设置该 TiDB 实例是否可以成为 DDL owner。若当前 TiDB 集群中只有一台 TiDB 实例，则不能禁止该实例成为 DDL owner，即不能设置为 `OFF`。\n\n### `tidb_enable_enhanced_security`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量表示所连接的 TiDB 服务器是否启用了安全增强模式 (SEM)。若要改变该变量值，你需要在 TiDB 服务器的配置文件中修改 `enable-sem` 项的值，并重启 TiDB 服务器。\n- 安全增强模式受[安全增强式 Linux](https://zh.wikipedia.org/wiki/安全增强式Linux) 等系统设计的启发，削减拥有 MySQL `SUPER` 权限的用户能力，转而使用细粒度的 `RESTRICTED` 权限作为替代。这些细粒度的 `RESTRICTED` 权限如下：\n    - `RESTRICTED_TABLES_ADMIN`：能够写入 `mysql` 库中的系统表，能查看 `information_schema` 表上的敏感列。\n    - `RESTRICTED_STATUS_ADMIN`：能够在 `SHOW STATUS` 命令中查看敏感内容。\n    - `RESTRICTED_VARIABLES_ADMIN`：能够在 `SHOW [GLOBAL] VARIABLES` 和 `SET` 命令中查看和设置包含敏感内容的变量。\n    - `RESTRICTED_USER_ADMIN`：能够阻止其他用户更改或删除用户帐户。\n    - `RESTRICTED_CONNECTION_ADMIN`：能够阻止其它用户使用 `KILL` 语句终止连接。\n\n### `tidb_enable_exchange_partition`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量用于设置是否启用 [`exchange partitions with tables`](/partitioned-table.md#分区管理) 特性。默认值为 `ON`，即默认开启该功能。\n- 该变量自 v6.3.0 开始废弃，其取值将固定为默认值 `ON`，即默认开启 `exchange partitions with tables`。\n\n### `tidb_enable_extended_stats`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量指定 TiDB 是否收集[扩展统计信息](/extended-statistics.md)来指导优化器。\n\n### `tidb_enable_external_ts_read` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 当此变量设置为 `ON` 时，TiDB 会读取 [`tidb_external_ts`](#tidb_external_ts-从-v640-版本开始引入) 指定时间戳前的历史数据。\n\n### `tidb_external_ts` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 当 [`tidb_enable_external_ts_read`](#tidb_enable_external_ts_read-从-v640-版本开始引入) 设置为 `ON` 时，TiDB 会依据该变量指定的时间戳读取历史数据。\n\n### `tidb_restricted_read_only` <span class=\"version-mark\">从 v5.2.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 可选值：`OFF` 和 `ON`\n- `tidb_restricted_read_only`和 [`tidb_super_read_only`](#tidb_super_read_only-从-v531-版本开始引入) 的作用相似。在大多数情况下，你只需要使用 [`tidb_super_read_only`](#tidb_super_read_only-从-v531-版本开始引入) 即可。\n- 拥有 `SUPER` 或 `SYSTEM_VARIABLES_ADMIN` 权限的用户可以修改该变量。如果 TiDB 开启了[安全增强模式](#tidb_enable_enhanced_security)，你还需要额外的 `RESTRICTED_VARIABLES_ADMIN` 权限才能读取或修改该变量。\n- `tidb_restricted_read_only` 的设置将影响 [`tidb_super_read_only`](#tidb_super_read_only-从-v531-版本开始引入) 的值：\n\n    - 当设置 `tidb_restricted_read_only` 为 `ON` 时，[`tidb_super_read_only`](#tidb_super_read_only-从-v531-版本开始引入) 的将自动被设置为 `ON`。\n    - 当设置 `tidb_restricted_read_only` 为 `OFF` 时，[`tidb_super_read_only`](#tidb_super_read_only-从-v531-版本开始引入) 的值将不受影响。\n    - 当 `tidb_restricted_read_only` 为 `ON` 时，[`tidb_super_read_only`](#tidb_super_read_only-从-v531-版本开始引入) 的值无法设置为 `OFF`。\n\n- 对于 TiDB 的 DBaaS 供应商，当 TiDB 为另一个数据库的下游数据库时，如果要将整个 TiDB 集群设置为只读模式，你需要开启[安全增强模式](#tidb_enable_enhanced_security) 并将 `tidb_restricted_read_only` 设置为 `ON`，从而防止你的用户通过 [`tidb_super_read_only`](#tidb_super_read_only-从-v531-版本开始引入) 将 TiDB 集群设置为可写。实现方法：首先开启[安全增强模式](#tidb_enable_enhanced_security)，然后由你（作为 DBaaS 的控制面）使用一个 admin 用户控制 `tidb_restricted_read_only`（需要拥有 `SYSTEM_VARIABLES_ADMIN` 和 `RESTRICTED_VARIABLES_ADMIN` 权限），由你的数据库用户使用 root 用户控制 [`tidb_super_read_only`](#tidb_super_read_only-从-v531-版本开始引入)（需要拥有 `SUPER` 权限）。\n- 该变量可以控制整个集群的只读状态。开启后（即该值为 `ON`），整个集群中的 TiDB 服务器都将进入只读状态，只有 `SELECT`、`USE`、`SHOW` 等不会修改数据的语句才能被执行，其他如 `INSERT`、`UPDATE` 等语句会被拒绝执行。\n- 该变量开启只读模式只保证整个集群最终进入只读模式，当变量修改状态还没被同步到其他 TiDB 服务器时，尚未同步的 TiDB 仍然停留在非只读模式。\n- 在执行 SQL 语句之前，TiDB 会检查集群的只读标志。从 v6.2.0 起，在提交 SQL 语句之前，TiDB 也会检查该标志，从而防止在服务器被置于只读模式后某些长期运行的 [auto commit](/transaction-overview.md#自动提交) 语句可能修改数据的情况。\n- 在变量开启时，对于尚未提交的事务：\n    - 如果有尚未提交的只读事务，可正常提交该事务。\n    - 如果尚未提交的事务为非只读事务，在事务内执行写入的 SQL 语句会被拒绝。\n    - 如果尚未提交的事务已经有数据改动，其提交也会被拒绝。\n- 当集群开启只读模式后，所有用户（包括 `SUPER` 用户）都无法执行可能写入数据的 SQL 语句，除非该用户被显式地授予了 `RESTRICTED_REPLICA_WRITER_ADMIN` 权限。\n\n### `tidb_enable_fast_analyze`\n\n> **警告：**\n>\n> 从 v7.5.0 开始，该变量被废弃。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制是否启用统计信息快速分析功能。默认值 0 表示不开启。\n- 快速分析功能开启后，TiDB 会随机采样约 10000 行的数据来构建统计信息。因此在数据分布不均匀或者数据量比较少的情况下，统计信息的准确度会比较低。这可能导致执行计划不优，比如选错索引。如果可以接受普通 `ANALYZE` 语句的执行时间，则推荐关闭快速分析功能。\n\n### `tidb_enable_fast_table_check` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n> **注意：**\n>\n> 该功能对[多值索引](/sql-statements/sql-statement-create-index.md#多值索引)和前缀索引不生效。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否使用基于校验和的方式来快速检查表中数据和索引的一致性。默认值 `ON` 表示该功能默认开启。\n- 开启后，TiDB 执行 [`ADMIN CHECK [TABLE|INDEX]`](/sql-statements/sql-statement-admin-check-table-index.md) 语句的速度更快。\n\n### `tidb_enable_foreign_key` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：在 v6.6.0 之前版本中为 `OFF`，在 v6.6.0 及之后的版本中为 `ON`。\n- 这个变量用于控制是否开启 `FOREIGN KEY` 特性。\n\n### `tidb_enable_gc_aware_memory_track`\n\n> **警告：**\n>\n> 该变量为 TiDB 内部调试变量，可能会在未来版本中删除，**请勿**设置该变量。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启可感知到垃圾回收的内存追踪 (GC-Aware memory track)。\n\n### `tidb_enable_global_index` <span class=\"version-mark\">从 v7.6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量控制是否支持为分区表创建[全局索引](/partitioned-table.md#全局索引)。启用此变量后，你可以通过在索引定义中添加 `GLOBAL` 选项创建不包含分区表达式中所有列的唯一索引。\n- 从 v8.4.0 开始，该变量被废弃。其值将固定为默认值 `ON`，即默认启用[全局索引](/partitioned-table.md#全局索引)。\n\n### `tidb_enable_lazy_cursor_fetch` <span class=\"version-mark\">从 v8.3.0 版本开始引入</span>\n\n> **警告：**\n>\n> 该变量控制的功能为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 可选值：`OFF`，`ON`\n- 这个变量用于控制 [Cursor Fetch](/develop/dev-guide-connection-parameters.md#使用-streamingresult-流式获取执行结果) 功能的行为。\n    - 当开启 Cursor Fetch 且该变量设置为 `OFF` 时，TiDB 会在语句开始执行时将所有数据读取完成并保存至 TiDB 内存，在后续客户端读取的过程中会依据客户端指定的 `FetchSize` 返回给客户端。如果结果集过大，可能会触发落盘临时将结果写入硬盘。\n    - 当开启 Cursor Fetch 且该变量设置为 `ON` 时，TiDB 不会一次把所有数据读取到 TiDB 节点，而是会随着客户端的读取不断将数据读到 TiDB 节点。\n- 该变量控制的功能存在以下限制：\n    - 不支持处于显式事务中的语句。\n    - 当前仅支持包含且仅包含 `TableReader`、`IndexReader`、`IndexLookUp`、`Projection`、`Selection` 算子的执行计划。\n    - 对于使用 Lazy Cursor Fetch 的语句，执行信息将不会出现在 [statements summary](/statement-summary-tables.md) 和[慢查询日志](/identify-slow-queries.md)中。\n- 对于暂不支持的场景，其行为与将变量设置为 `OFF` 时一致。\n\n### `tidb_enable_non_prepared_plan_cache`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制是否开启[非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)。\n- 开启此功能可能会带来额外的内存和 CPU 开销，并不一定适用于所有场景，请根据具体的使用情况决定是否开启该功能。\n\n### `tidb_enable_non_prepared_plan_cache_for_dml` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n> **警告：**\n>\n> 针对 DML 语句的非 Prepare 语句执行计划缓存目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制[非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)是否支持 DML 语句。\n\n### `tidb_enable_gogc_tuner` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量来用控制是否开启 GOGC Tuner。\n\n### `tidb_enable_historical_stats`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`。在 v8.2.0 之前，默认值为 `ON`。\n- 这个变量用来控制是否开启历史统计信息。默认值为 `OFF`，表示默认关闭历史统计信息。\n\n### `tidb_enable_historical_stats_for_capture`\n\n> **警告：**\n>\n> 当前版本中该变量控制的功能尚未完全生效，请保留默认值。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制 `PLAN REPLAYER CAPTURE` 抓取的内容是否默认带历史统计信息。默认值为 `OFF`，表示默认不带历史统计信息。\n\n### `tidb_enable_index_merge` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n> **注意：**\n>\n> - 当集群从 v4.0.0 以下版本升级到 v5.4.0 及以上版本时，该变量开关默认关闭，防止升级后计划发生变化导致回退。\n> - 当集群从 v4.0.0 及以上版本升级到 v5.4.0 及以上版本时，该变量开关保持升级前的状态。\n> - 对于 v5.4.0 及以上版本的新建集群，该变量开关默认开启。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否开启 index merge 功能。\n\n### `tidb_enable_index_merge_join`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 表示是否启用 `IndexMergeJoin` 算子。\n- 该变量为 TiDB 内部变量，**不推荐使用**，否则可能会造成数据正确性问题。\n\n### `tidb_enable_legacy_instance_scope` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于允许使用 `SET SESSION` 对 `INSTANCE` 作用域的变量进行设置，用法同 `SET GLOBAL`。\n- 为了兼容之前的 TiDB 版本，该变量值默认为 `ON`。\n\n### `tidb_enable_list_partition` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来设置是否开启 `LIST (COLUMNS) TABLE PARTITION` 特性。\n- 从 v8.4.0 开始，该变量被废弃。其值将固定为默认值 `ON`，即默认启用 [List 分区](/partitioned-table.md#list-分区)。\n\n### `tidb_enable_local_txn`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量用于一个未发布的特性，**请勿修改该变量值**。\n\n### `tidb_enable_metadata_lock` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来设置是否开启[元数据锁](/metadata-lock.md)特性。需要注意，在设置该变量时，集群中不能有 DDL 任务，以免造成非预期数据正确性、一致性问题。\n\n### `tidb_enable_mutation_checker` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于设置是否开启 mutation checker。mutation checker 是一项在 DML 语句执行过程中进行的数据索引一致性校验，校验报错会回滚当前语句。开启该校验会导致 CPU 使用轻微上升。详见[数据索引一致性报错](/troubleshoot-data-inconsistency-errors.md)。\n- 对于新创建的 v6.0.0 及以上的集群，默认值为 `ON`。对于升级版本的集群，如果升级前是低于 v6.0.0 的版本，升级后默认值为 `OFF`。\n\n### `tidb_enable_new_cost_interface` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- TiDB v6.2.0 对代价模型的实现进行了代码层面的重构，这个变量用来控制是否使用重构后的代价模型 [Cost Model Version 2](/cost-model.md#cost-model-version-2)。\n- 重构后的代价模型使用完全一样的代价公式，因此不会引起计划选择的变动，此开关默认打开。\n- 从 v6.1 升级至 v6.2 的用户，此开关保持升级前的 `OFF` 状态，此时建议直接打开；对于从 v6.1 之前版本升级至 v6.2 的用户，此开关默认为 `ON`。\n\n### `tidb_enable_new_only_full_group_by_check` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量用于控制 TiDB 执行 `ONLY_FULL_GROUP_BY` 检查时的行为。有关 `ONLY_FULL_GROUP_BY` 的信息可以参考 [MySQL 文档](https://dev.mysql.com/doc/refman/8.0/en/sql-mode.html#sqlmode_only_full_group_by)。在 v6.1 中 TiDB 对该项检查做了更严格正确的处理。\n- 由于可能存在版本升级造成的兼容性问题，在 v6.1 中该变量默认值是 `OFF`，即默认关闭。\n\n### `tidb_enable_noop_functions` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`OFF`\n- 可选值：`ON`、`OFF`、`WARN`\n- 默认情况下，用户尝试将某些语法用于尚未实现的功能时，TiDB 会报错。若将该变量值设为 `ON`，TiDB 则自动忽略此类功能不可用的情况，即不会报错。若用户无法更改 SQL 代码，可考虑将变量值设为 `ON`。\n- 启用 `noop` 函数可以控制以下行为：\n    * `LOCK IN SHARE MODE` 语法\n    * `SQL_CALC_FOUND_ROWS` 语法\n    * `START TRANSACTION READ ONLY` 和 `SET TRANSACTION READ ONLY` 语法\n    * `tx_read_only`、`transaction_read_only`、`offline_mode`、`super_read_only`、`read_only` 以及 `sql_auto_is_null` 系统变量\n    * `GROUP BY <expr> ASC|DESC` 语法\n\n> **警告：**\n>\n> 该变量只有在默认值 `OFF` 时，才算是安全的。因为设置 `tidb_enable_noop_functions=1` 后，TiDB 会自动忽略某些语法而不报错，这可能会导致应用程序出现异常行为。例如，允许使用语法 `START TRANSACTION READ ONLY` 时，事务仍会处于读写模式。\n\n### `tidb_enable_noop_variables` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 若该变量值为 `OFF`，TiDB 具有以下行为：\n    * 使用 `SET` 设置 `noop` 的系统变量时会报 `\"setting *variable_name* has no effect in TiDB\"` 的警告。\n    * `SHOW [SESSION | GLOBAL] VARIABLES` 的结果不显示 `noop` 的系统变量。\n    * 使用 `SELECT` 读取 `noop` 的系统变量时会报 `\"variable *variable_name* has no effect in TiDB\"` 的警告。\n- 你可以通过 `SELECT * FROM INFORMATION_SCHEMA.CLIENT_ERRORS_SUMMARY_GLOBAL;` 语句来检查 TiDB 实例是否曾设置和读取 `noop` 系统变量。\n\n### `tidb_enable_null_aware_anti_join` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：在 v7.0.0 之前版本中为 `OFF`，即默认关闭。在 v7.0.0 及之后的版本中为 `ON`，即默认开启。\n- 这个变量用于控制 TiDB 对特殊集合算子 `NOT IN` 和 `!= ALL` 引导的子查询产生的 ANTI JOIN 是否采用 Null Aware Hash Join 的执行方式。\n- 从旧版本升级到 v7.0.0 及之后版本，该功能自动开启，即该变量的值修改为默认值 `ON`。\n\n### `tidb_enable_outer_join_reorder` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 自 v6.1.0 起，TiDB 的 [Join Reorder 算法](/join-reorder.md)开始支持 Outer Join。该变量用于控制是否启用 Outer Join 的 Join Reorder。\n- 对于从较低版本升级到当前版本的 TiDB：\n\n    - 如果升级前 TiDB 的版本低于 v6.1.0，升级后该变量的默认值为 `ON`。\n    - 如果升级前 TiDB 的版本等于或大于 v6.1.0，升级后该变量的默认值跟随升级前的设定值。\n\n### `tidb_enable_inl_join_inner_multi_pattern` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`。TiDB v8.3.0 及之前版本默认值为 `OFF`。\n- 该变量用于控制当内表上有 `Selection`、`Projection` 或 `Aggregation` 算子时是否支持 Index Join。`OFF` 表示不支持。\n- 如果将集群从 v7.0.0 之前版本升级至 v8.4.0 或之后的版本，该变量默认值为 `OFF`，即默认不支持 Index Join。\n\n### `tidb_enable_instance_plan_cache` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n> **警告：**\n>\n> Instance Plan Cache 目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启 Instance Plan Cache 功能。该功能实现实例级执行计划缓存，允许同一个 TiDB 实例的所有会话共享执行计划缓存，从而提升内存利用率。开启该功能之前，建议关闭会话级别的 [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)和[非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)。\n\n### `tidb_enable_ordered_result_mode`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 指定是否对最终的输出结果进行自动排序。\n- 例如，开启该变量后，TiDB 会将 `SELECT a, MAX(b) FROM t GROUP BY a` 处理为 `SELECT a, MAX(b) FROM t GROUP BY a ORDER BY a, MAX(b)`。\n\n### `tidb_enable_paging` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否使用分页 (paging) 方式发送 Coprocessor 请求。对于 [v5.4.0, v6.2.0) 区间的 TiDB 版本，该变量只对 `IndexLookup` 算子生效；对于 v6.2.0 以及之后的版本，该变量对全局生效。从 v6.4.0 版本开始，该变量默认值由 `OFF` 改成 `ON`。\n- 适用场景：\n\n    - 推荐在所有偏 OLTP 的场景下使用 paging。\n    - 对于使用 `IndexLookUp` 和 `Limit` 并且 `Limit` 无法下推到 `IndexScan` 上的读请求，可能会出现读请求的延迟高、TiKV 的 Unified read pool CPU 使用率高的情况。在这种情况下，由于 `Limit` 算子只需要少部分数据，开启 [`tidb_enable_paging`](#tidb_enable_paging-从-v540-版本开始引入) 能够减少处理数据的数量，从而降低延迟、减少资源消耗。\n    - 对于 [Dumpling](/dumpling-overview.md) 数据导出或者全表扫描这类的场景，开启 paging 后可以有效降低 TiDB 进程的内存消耗。\n\n> **注意：**\n>\n> 对于偏 OLAP 的场景，并且以 TiKV 而非 TiFlash 作为存储引擎时，开启 paging 可能导致部分场景下性能回退。此时，你可以考虑通过该变量关闭 paging 或者通过系统变量 [`tidb_min_paging_size`](/system-variables.md#tidb_min_paging_size-从-v620-版本开始引入) 和 [`tidb_max_paging_size`](/system-variables.md#tidb_max_paging_size-从-v630-版本开始引入) 调整 paging size 的行数范围。\n\n### `tidb_enable_parallel_apply` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启 Apply 算子并发，并发数由 `tidb_executor_concurrency` 变量控制。Apply 算子用来处理关联子查询且默认无并发，所以执行速度较慢。打开 Apply 并发开关可增加并发度，提高执行速度。目前默认关闭。\n\n### `tidb_enable_parallel_hashagg_spill` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制 TiDB 是否支持并行 HashAgg 进行落盘。当该变量设置为 `ON` 时，在任意并发条件下，HashAgg 算子都可以根据内存使用情况自动触发数据落盘，从而兼顾性能和数据处理量。因此，不推荐将此变量修改为 `OFF`。从 v8.2.0 开始，将该变量设置为 `OFF` 时会产生警告。该变量将在未来版本中废弃。\n\n### `tidb_enable_pipelined_window_function`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量指定是否对[窗口函数](/functions-and-operators/window-functions.md)采用流水线的执行算法。\n\n### `tidb_enable_plan_cache_for_param_limit` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制 Prepared Plan Cache 是否缓存 `LIMIT` 后面带变量 (`LIMIT ?`) 的执行计划。目前不支持缓存 `LIMIT` 后面带变量且变量值大于 10000 的执行计划。\n\n### `tidb_enable_plan_cache_for_subquery` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制 Prepared Plan Cache 是否缓存包含子查询的查询。\n\n### `tidb_enable_plan_replayer_capture`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制是否开启 [`PLAN REPLAYER CAPTURE` 功能](/sql-plan-replayer.md#使用-plan-replayer-capture-抓取目标计划)。默认值 `ON` 代表开启 `PLAN REPLAYER CAPTURE` 功能。\n\n### `tidb_enable_plan_replayer_continuous_capture` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制是否开启 [`PLAN REPLAYER CONTINUOUS CAPTURE` 功能](/sql-plan-replayer.md#使用-plan-replayer-continuous-capture)。默认值 `OFF` 代表关闭功能。\n\n### `tidb_enable_prepared_plan_cache` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制是否开启 [Prepared Plan Cache](/sql-prepared-plan-cache.md)。开启后，对 `Prepare`、`Execute` 请求的执行计划会进行缓存，以便在后续执行时跳过查询计划优化这个步骤，获得性能上的提升。\n- 在 v6.1.0 之前这个开关通过 TiDB 配置文件 (`prepared-plan-cache.enabled`) 进行配置，升级到 v6.1.0 时会自动继承原有设置。\n\n### `tidb_enable_prepared_plan_cache_memory_monitor` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制是否统计 Prepared Plan Cache 中所缓存的执行计划占用的内存。具体可见 [Prepared Plan Cache 的内存管理](/sql-prepared-plan-cache.md#prepared-plan-cache-的内存管理)。\n\n### `tidb_enable_pseudo_for_outdated_stats` <span class=\"version-mark\">从 v5.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制优化器在一张表上的统计信息过期时的行为。\n- 统计信息过期的判断标准：最近一次对某张表执行 `ANALYZE` 获得统计信息后，该表数据被修改的行数大于该表总行数的 80%，便可判定该表的统计信息已过期。该比例可通过 [`pseudo-estimate-ratio`](/tidb-configuration-file.md#pseudo-estimate-ratio) 配置参数调整。\n- 默认情况下（即该变量值为 `OFF` 时），某张表上的统计信息过期后，优化器仍会使用该表上的统计信息。将该变量值设为 `ON` 时，当统计信息过期后，优化器认为该表上除总行数以外的统计信息不再可靠，转而使用 pseudo 统计信息。\n- 如果表数据修改较频繁，没有及时对表执行 `ANALYZE`，但又希望执行计划保持稳定，推荐将该变量值设为 `OFF`。\n\n### `tidb_enable_rate_limit_action`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量控制是否为读数据的算子开启动态内存控制功能。读数据的算子默认启用 [`tidb_distsql_scan_concurrency`](/system-variables.md#tidb_distsql_scan_concurrency) 所允许的最大线程数来读取数据。当单条 SQL 语句的内存使用每超过 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 一次，读数据的算子会停止一个线程。\n- 当读数据的算子只剩 1 个线程且当单条 SQL 语句的内存使用继续超过 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 时，该 SQL 语句会触发其它的内存控制行为，例如[落盘](/system-variables.md#tidb_enable_tmp_storage_on_oom)。\n- 该变量在单条查询仅涉及读数据的情况下，对内存控制效果较好。若还存在额外的计算操作（如连接、聚合等），打开该变量可能会导致内存不受 `tidb_mem_quota_query` 控制，加剧 OOM 风险。\n\n### `tidb_enable_resource_control` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`ON`\n- 类型：布尔型\n- 该变量是[资源管控特性](/tidb-resource-control.md)的开关。该变量设置为 `ON` 时，集群支持应用按照资源组做资源隔离。\n\n### `tidb_enable_reuse_chunk` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 可选值：`OFF`，`ON`\n- 该变量用于控制 TiDB 是否启用 Chunk 对象缓存。如果为 `ON`，则优先使用缓存中的 Chunk 对象，缓存中找不到申请的对象时才会从系统内存中申请。如果为 `OFF`，则直接从系统内存中申请 Chunk 对象。\n\n### `tidb_enable_shared_lock_promotion` <span class=\"version-mark\">从 v8.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量用于控制是否启用共享锁升级为排他锁的功能。TiDB 默认不支持 `SELECT LOCK IN SHARE MODE`，当该变量值为 `ON` 时，TiDB 会尝试将 `SELECT LOCK IN SHARE MODE` 语句升级为 `SELECT FOR UPDATE` 并真正加悲观锁。该变量默认值为 `OFF`，表示不启用共享锁升级为排他锁的功能。\n- 无论 [`tidb_enable_noop_functions`](#tidb_enable_noop_functions-从-v40-版本开始引入) 是否开启，启用该变量都会对 `SELECT LOCK IN SHARE MODE` 语句生效。\n\n### `tidb_enable_slow_log`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否开启 slow log 功能。\n\n### `tidb_enable_stats_owner` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`ON`\n- 可选值：`OFF`、`ON`\n- 用于设置该 TiDB 实例是否可以运行[统计信息自动更新](/statistics.md#自动更新)任务。若当前 TiDB 集群中只有一台 TiDB 实例，则不能禁止该实例运行统计信息自动更新，即不能设置为 `OFF`。\n\n### `tidb_enable_stmt_summary` <span class=\"version-mark\">从 v3.0.4 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制是否开启 statement summary 功能。如果开启，SQL 的耗时等执行信息将被记录到系统表 `information_schema.STATEMENTS_SUMMARY` 中，用于定位和排查 SQL 性能问题。\n\n### `tidb_enable_strict_double_type_check` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制是否可以用 `DOUBLE` 类型的无效定义创建表。该设置的目的是提供一个从 TiDB 早期版本升级的方法，因为早期版本在验证类型方面不太严格。\n- 该变量的默认值 `ON` 与 MySQL 兼容。\n\n例如，由于无法保证浮点类型的精度，现在将 `DOUBLE(10)` 类型视为无效。将 `tidb_enable_strict_double_type_check` 更改为 `OFF` 后，将会创建表。如下所示：\n\n```sql\nCREATE TABLE t1 (id int, c double(10));\nERROR 1149 (42000): You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use\nSET tidb_enable_strict_double_type_check = 'OFF';\nQuery OK, 0 rows affected (0.00 sec)\nCREATE TABLE t1 (id int, c double(10));\nQuery OK, 0 rows affected (0.09 sec)\n```\n\n> **注意：**\n>\n> 该设置仅适用于 `DOUBLE` 类型，因为 MySQL 允许为 `FLOAT` 类型指定精度。从 MySQL 8.0.17 开始已弃用此行为，不建议为 `FLOAT` 或 `DOUBLE` 类型指定精度。\n\n### `tidb_enable_table_partition`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`ON`\n- 类型：枚举型\n- 从 v8.4.0 开始，该变量被废弃。其值将固定为默认值 `ON`，即默认启用[分区表](/partitioned-table.md)。\n\n### `tidb_enable_telemetry` <span class=\"version-mark\">从 v4.0.2 版本开始引入，从 v8.1.0 版本开始废弃</span>\n\n> **警告：**\n>\n> 从 TiDB v8.1.0 开始，TiDB 已移除遥测功能，该变量已不再生效。保留该变量仅用于与之前版本兼容。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 在 v8.1.0 之前，这个变量用于动态地控制 TiDB 遥测功能是否开启。\n\n### `tidb_enable_tiflash_read_for_write_stmt` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制包含增删改的 SQL 语句中的读取操作能否下推到 TiFlash，比如：\n\n    - `INSERT INTO SELECT` 语句中的 `SELECT` 查询（典型应用场景为 [TiFlash 查询结果物化](/tiflash/tiflash-results-materialization.md)）\n    - `UPDATE` 和 `DELETE` 语句中的 `WHERE` 条件过滤\n- 从 v7.1.0 开始，该变量废弃。当 [`tidb_allow_mpp = ON`](/system-variables.md#tidb_allow_mpp-从-v50-版本开始引入) 时，优化器将根据 [SQL 模式](/sql-mode.md)及 TiFlash 副本的代价估算自行决定是否将查询下推至 TiFlash。需要注意的是，只有当前会话的 [SQL 模式](/sql-mode.md)为非严格模式（即 `sql_mode` 值不包含 `STRICT_TRANS_TABLES` 和 `STRICT_ALL_TABLES`）时，TiDB 才允许将包含增删改的 SQL 语句（如 `INSERT INTO SELECT`）中的读取操作下推至 TiFlash。\n\n### `tidb_enable_tmp_storage_on_oom`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 设置是否在单条 SQL 语句的内存使用超出系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 限制时为某些算子启用临时磁盘。\n- 在 v6.3.0 之前这个开关可通过 TiDB 配置文件中的 `oom-use-tmp-storage` 项进行配置。在升级到 v6.3.0 及更新的版本后，集群会自动使用原 `oom-use-tmp-storage` 的值来初始化该开关，配置文件中 `oom-use-tmp-storage` 的新设置不再影响该开关。\n\n### `tidb_enable_top_sql` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启 [Top SQL 特性](/dashboard/top-sql.md)。\n\n### `tidb_enable_tso_follower_proxy` <span class=\"version-mark\">从 v5.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启 TSO Follower Proxy 特性。当该值为 `OFF` 时，TiDB 仅会从 PD leader 获取 TSO。当该值为 `ON` 时，TiDB 在获取 TSO 时会将请求均匀地发送到所有 PD 节点上，因此 PD follower 也可以处理 TSO 请求，从而减轻 PD leader 的 CPU 压力。\n- 适合开启 TSO Follower Proxy 的场景：\n    * PD leader 因高压力的 TSO 请求而达到 CPU 瓶颈，导致 TSO RPC 请求的延迟较高。\n    * 集群中的 TiDB 实例数量较多，且调高 [`tidb_tso_client_batch_max_wait_time`](/system-variables.md#tidb_tso_client_batch_max_wait_time-从-v530-版本开始引入) 并不能缓解 TSO RPC 请求延迟高的问题。\n\n> **注意：**\n>\n> - 如果 PD leader 的 TSO RPC 延迟升高，但其现象并非由 CPU 使用率达到瓶颈而导致（可能存在网络等问题），此时，打开 TSO Follower Proxy 可能会导致 TiDB 的语句执行延迟上升，从而影响集群的 QPS 表现。\n> - 该功能与 [`tidb_tso_client_rpc_mode`](#tidb_tso_client_rpc_mode-从-v840-版本开始引入) 不兼容。启用该功能将导致 [`tidb_tso_client_rpc_mode`](#tidb_tso_client_rpc_mode-从-v840-版本开始引入) 不生效。\n\n### `tidb_enable_unsafe_substitute` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否对生成列中表达式替换使用不安全的替换方式。默认值为 `OFF`，即默认关闭不安全的替换方式。详情见[生成列](/generated-columns.md)。\n\n### `tidb_enable_vectorized_expression` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否开启向量化执行。\n\n### `tidb_enable_window_function`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制是否开启[窗口函数](/functions-and-operators/window-functions.md)的支持。\n- 由于窗口函数会使用一些保留关键字，可能导致原先可以正常执行的 SQL 语句在升级 TiDB 后无法被解析语法，此时可以将 `tidb_enable_window_function` 设置为 `OFF`。\n\n### `tidb_enable_row_level_checksum` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否开启 [TiCDC 单行数据正确性校验](/ticdc/ticdc-integrity-check.md)功能。\n- 你可以使用 [`TIDB_ROW_CHECKSUM()`](/functions-and-operators/tidb-functions.md#tidb_row_checksum) 函数查询行数据的 Checksum 值。\n\n### `tidb_enforce_mpp` <span class=\"version-mark\">从 v5.1 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`（表示关闭）。如需修改此变量的默认值，请配置 [`performance.enforce-mpp`](/tidb-configuration-file.md#enforce-mpp) 参数。\n- 这个变量用于控制是否忽略优化器代价估算，强制使用 TiFlash 的 MPP 模式执行查询，可以设置的值包括：\n    - 0 或 OFF，代表不强制使用 MPP 模式（默认）\n    - 1 或 ON，代表将忽略代价估算，强制使用 MPP 模式。注意：只有当 `tidb_allow_mpp=true` 时该设置才生效。\n\nMPP 是 TiFlash 引擎提供的分布式计算框架，允许节点之间的数据交换并提供高性能、高吞吐的 SQL 算法。MPP 模式选择的详细说明参见[控制是否选择 MPP 模式](/tiflash/use-tiflash-mpp-mode.md#控制是否选择-mpp-模式)。\n\n### `tidb_evolve_plan_baselines` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制是否启用自动演进绑定功能。该功能的详细介绍和使用方法可以参考[自动演进绑定](/sql-plan-management.md#自动演进绑定-baseline-evolution)。\n- 为了减少自动演进对集群的影响，可以进行以下配置：\n\n    - 设置 `tidb_evolve_plan_task_max_time`，限制每个执行计划运行的最长时间，其默认值为 600s；\n    - 设置`tidb_evolve_plan_task_start_time` 和 `tidb_evolve_plan_task_end_time`，限制运行演进任务的时间窗口，默认值分别为 `00:00 +0000` 和 `23:59 +0000`。\n\n### `tidb_evolve_plan_task_end_time` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：时间\n- 默认值：`23:59 +0000`\n- 这个变量用来设置一天中允许自动演进的结束时间。\n\n### `tidb_evolve_plan_task_max_time` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`600`\n- 范围：`[-1, 9223372036854775807]`\n- 单位：秒\n- 该变量用于限制自动演进功能中，每个执行计划运行的最长时间。\n\n### `tidb_evolve_plan_task_start_time` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：时间\n- 默认值：`00:00 +0000`\n- 这个变量用来设置一天中允许自动演进的开始时间。\n\n### `tidb_executor_concurrency` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`5`\n- 范围：`[1, 256]`\n- 单位：线程\n\n该变量用来统一设置各个 SQL 算子的并发度，包括：\n\n- `index lookup`\n- `index lookup join`\n- `hash join`\n- `hash aggregation`（partial 和 final 阶段）\n- `window`\n- `projection`\n- `sort`\n\n`tidb_executor_concurrency` 整合了已有的系统变量，方便管理。这些变量所列如下：\n\n+ `tidb_index_lookup_concurrency`\n+ `tidb_index_lookup_join_concurrency`\n+ `tidb_hash_join_concurrency`\n+ `tidb_hashagg_partial_concurrency`\n+ `tidb_hashagg_final_concurrency`\n+ `tidb_projection_concurrency`\n+ `tidb_window_concurrency`\n\nv5.0 后，用户仍可以单独修改以上系统变量（会有废弃警告），且修改只影响单个算子。后续通过 `tidb_executor_concurrency` 的修改也不会影响该算子。若要通过 `tidb_executor_concurrency` 来管理所有算子的并发度，需要将以上所列变量的值设置为 `-1`。\n\n对于从 v5.0 之前的版本升级到 v5.0 的系统，如果用户对上述所列变量的值没有做过改动（即 `tidb_hash_join_concurrency` 值为 `5`，其他值为 `4`），则会自动转为使用 `tidb_executor_concurrency` 来统一管理算子并发度。如果用户对上述变量的值做过改动，则沿用之前的变量对相应的算子做并发控制。\n\n### `tidb_expensive_query_time_threshold`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`60`\n- 范围：`[10, 2147483647]`\n- 单位：秒\n- 这个变量用来控制打印 expensive query 日志的阈值时间，默认值是 60 秒。expensive query 日志和慢日志的差别是，慢日志是在语句执行完后才打印，expensive query 日志可以把正在执行中的语句且执行时间超过阈值的语句及其相关信息打印出来。\n\n### `tidb_expensive_txn_time_threshold` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`600`\n- 范围：`[60, 2147483647]`\n- 单位：秒\n- 这个变量用来控制打印 expensive transaction 日志的阈值时间，默认值是 600 秒。expensive transaction 日志会将尚未 COMMIT 或 ROLLBACK 且持续时间超过该阈值的事务的相关信息打印出来。\n\n### `tidb_force_priority`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`NO_PRIORITY`\n- 可选值：`NO_PRIORITY`、`LOW_PRIORITY`、`DELAYED`、`HIGH_PRIORITY`\n- 这个变量用于改变 TiDB server 上执行的语句的默认优先级。例如，你可以通过设置该变量来确保正在执行 OLAP 查询的用户优先级低于正在执行 OLTP 查询的用户。\n- 默认值 `NO_PRIORITY` 表示不强制改变执行语句的优先级。\n\n> **注意：**\n>\n> TiDB 从 v6.6.0 版本开始支持[使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)功能。该功能可以将不同优先级的语句放在不同的资源组中执行，并为这些资源组分配不同的配额和优先级，可以达到更好的资源管控效果。在开启资源管控功能后，语句的调度主要受资源组的控制，`PRIORITY` 将不再生效。建议在支持资源管控的版本优先使用资源管控功能。\n\n### `tidb_gc_concurrency` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`-1` 或 `[1, 256]`\n- 单位：线程\n- 该变量用于控制[垃圾回收 (GC)](/garbage-collection-overview.md) 过程中 [Resolve Locks（清理锁）](/garbage-collection-overview.md#resolve-locks清理锁)的并发线程数。\n- 从 v8.3.0 开始，该变量也用于控制 GC 过程中 [Delete Range（删除区间）](/garbage-collection-overview.md#delete-ranges删除区间)的并发线程数。\n- 默认情况下，该变量值为 `-1`，TiDB 将根据负载情况自动决定适当的线程数。\n- 当设置为 `[1, 256]` 之间的数时：\n    - Resolve Locks（清理锁）直接使用该变量设定值作为线程数。\n    - Delete Range（删除区间）使用该变量设定值的 1/4 作为线程数。\n\n### `tidb_gc_enable` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否启用 TiKV 的垃圾回收 (GC) 机制。如果不启用 GC 机制，系统将不再清理旧版本的数据，因此会有损系统性能。\n\n### `tidb_gc_life_time` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：Duration\n- 默认值：`10m0s`\n- 范围：`[10m0s, 8760h0m0s]`\n- 这个变量用于指定每次进行垃圾回收 (GC) 时保留数据的时限。变量值为 Go 的 Duration 字符串格式。每次进行 GC 时，将以当前时间减去该变量的值作为 safe point。\n\n> **Note:**\n>\n> - 在数据频繁更新的场景下，将 `tidb_gc_life_time` 的值设置得过大（如数天甚至数月）可能会导致一些潜在的问题，如：\n>     - 占用更多的存储空间。\n>     - 大量的历史数据可能会在一定程度上影响系统性能，尤其是范围的查询（如 `select count(*) from t`）。\n> - 如果一个事务的运行时长超过了 `tidb_gc_life_time` 配置的值，在 GC 时，为了使这个事务可以继续正常运行，系统会保留从这个事务开始时间 `start_ts` 以来的数据。例如，如果 `tidb_gc_life_time` 的值配置为 10 分钟，且在一次 GC 时，集群正在运行的事务中最早开始的那个事务已经运行了 15 分钟，那么本次 GC 将保留最近 15 分钟的数据。\n\n### `tidb_gc_max_wait_time` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`86400`\n- 范围：`[600, 31536000]`\n- 单位：秒\n- 这个变量用于指定活跃事务阻碍 GC safe point 推进的最大时间。每次进行 GC 时，默认 GC safe point 不会超过正在执行中的事务的开始时间。如果活跃事务运行时间未超过该值，GC safe point 会一直被阻塞不更新，直到活跃事务运行时间超过该值 safe point 才会正常推进。\n\n### `tidb_gc_run_interval` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：Duration\n- 默认值：`10m0s`\n- 范围：`[10m0s, 8760h0m0s]`\n- 这个变量用于指定垃圾回收 (GC) 运行的时间间隔。变量值为 Go 的 Duration 字符串格式，如`\"1h30m\"`、`\"15m\"`等。\n\n### `tidb_gc_scan_lock_mode` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n> **警告：**\n>\n> Green GC 目前是实验性功能，不建议在生产环境中使用。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`LEGACY`\n- 可设置为：`PHYSICAL`，`LEGACY`\n    - `LEGACY`：使用旧的扫描方式，即禁用 Green GC。\n    - `PHYSICAL`：使用物理扫描方式，即启用 Green GC。\n- 这个变量用于指定垃圾回收 (GC) 的 Resolve Locks（清理锁）步骤中扫描锁的方式。当变量值设置为 `LEGACY` 时，TiDB 以 Region 为单位进行扫描。当变量值设置为 `PHYSICAL` 时，每个 TiKV 节点分别绕过 Raft 层直接扫描数据，可以有效地缓解在启用 [Hibernate Region](/tikv-configuration-file.md#hibernate-regions) 功能时，GC 唤醒全部 Region 的影响，从而提升 Resolve Locks（清理锁）这个步骤的执行速度。\n\n### `tidb_general_log`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置是否在[日志](/tidb-configuration-file.md#logfile)里记录所有的 SQL 语句。该功能默认关闭。如果系统运维人员在定位问题过程中需要追踪所有 SQL 记录，可考虑开启该功能。\n- 在 TiDB 配置项 [`log.level`](/tidb-configuration-file.md#level) 为 `\"info\"` 或 `\"debug\"` 时，通过查询 `\"GENERAL_LOG\"` 字符串可以定位到该功能在日志中的所有记录。日志会记录以下内容：\n    - `conn`：当前会话对应的 ID\n    - `user`：当前会话用户\n    - `schemaVersion`：当前 schema 版本\n    - `txnStartTS`：当前事务的开始时间戳\n    - `forUpdateTS`：事务模式为悲观事务时，SQL 语句的当前时间戳。悲观事务内发生写冲突时，会重试当前执行语句，该时间戳会被更新。重试次数由 [`max-retry-count`](/tidb-configuration-file.md#max-retry-count) 配置。事务模式为乐观事务时，该条目与 `txnStartTS` 等价。\n    - `isReadConsistency`：当前事务隔离级别是否是读已提交 (RC)\n    - `current_db`：当前数据库名\n    - `txn_mode`：事务模式。可选值：`OPTIMISTIC`（乐观事务模式），或 `PESSIMISTIC`（悲观事务模式）\n    - `sql`：当前查询对应的 SQL 语句\n\n### `tidb_non_prepared_plan_cache_size`\n\n> **警告：**\n>\n> 从 v7.1.0 开始，该变量被废弃。请使用 [`tidb_session_plan_cache_size`](#tidb_session_plan_cache_size-从-v710-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`100`\n- 范围：`[1, 100000]`\n- 这个变量用来控制[非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)最多能够缓存的计划数量。\n\n### `tidb_generate_binary_plan` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于指定是否在 slow log 和 statement summary 里包含以二进制格式编码的执行计划。\n- 开启该变量后，即可在 TiDB Dashboard 中查看查询的图形化执行计划。注意，TiDB Dashboard 只显示变量开启时产生的查询的执行计划。\n- 用 [`SELECT tidb_decode_binary_plan('xxx...')`](/functions-and-operators/tidb-functions.md#tidb_decode_binary_plan) SQL 语句可以从编码后的执行计划解析出具体的执行计划。\n\n### `tidb_gogc_tuner_max_value` <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`500`\n- 范围：`[10, 2147483647]`\n- 该变量用来控制 GOGC Tuner 可调节 GOGC 的最大值。\n\n### `tidb_gogc_tuner_min_value` <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`100`\n- 范围：`[10, 2147483647]`\n- 该变量用来控制 GOGC Tuner 可调节 GOGC 的最小值。\n\n### `tidb_gogc_tuner_threshold` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`0.6`\n- 范围：`[0, 0.9)`\n- 这个变量用来控制 GOGC Tuner 自动调节的最大内存阈值，超过阈值后 GOGC Tuner 会停止工作。\n\n### `tidb_guarantee_linearizability` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 此变量控制异步提交 (Async Commit) 中提交时间戳的计算方式。默认情况下（使用 `ON` 值），两阶段提交从 PD 服务器请求一个新的时间戳，并使用该时间戳计算最终提交的时间戳，这样可保证所有并发事务可线性化。\n- 如果将该变量值设为 `OFF`，从 PD 获取时间戳的操作会被省掉，这种情况下只保证因果一致性但不保证线性一致性。详情请参考 PingCAP 博文 [Async Commit 原理介绍](https://pingcap.com/zh/blog/async-commit-principle)。\n- 对于需要只保证因果一致性的场景，可将此变量设为 `OFF` 以提升性能。\n\n### `tidb_hash_exchange_with_new_collation`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该值表示是否在开启 new collation 的集群里生成 MPP hash partition exchange 算子。`true` 表示生成此算子，`false`表示不生成。\n- 该变量为 TiDB 内部变量，**不推荐设置该变量**。\n\n### `tidb_hash_join_concurrency`\n\n> **警告：**\n>\n> 从 v5.0 版本开始，该变量被废弃。请使用 [`tidb_executor_concurrency`](#tidb_executor_concurrency-从-v50-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置 hash join 算法的并发度。\n- 默认值 `-1` 表示使用 `tidb_executor_concurrency` 的值。\n\n### `tidb_hash_join_version` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n> **警告：**\n>\n> 该变量控制的功能为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：枚举型\n- 默认值：`legacy`\n- 可选值：`legacy`、`optimized`\n- 控制 TiDB 是否使用 Hash Join 算子的优化版。默认值为 `legacy`，代表不使用优化版。若设置为 `optimized`，TiDB 在执行 Hash Join 算子时将使用其优化版，以提升 Hash Join 性能。\n\n> **注意：**\n>\n> 目前，仅 Inner Join 和 Outer Join 类型的连接操作支持优化版的 Hash Join。对于其他类型的连接操作，即使将该变量设成 `optimized`，TiDB 也不会使用优化版的 Hash Join。\n\n### `tidb_hashagg_final_concurrency`\n\n> **警告：**\n>\n> 从 v5.0 版本开始，该变量被废弃。请使用 [`tidb_executor_concurrency`](#tidb_executor_concurrency-从-v50-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置并行 hash aggregation 算法 final 阶段的执行并发度。对于聚合函数参数不为 distinct 的情况，HashAgg 分为 partial 和 final 阶段分别并行执行。\n- 默认值 `-1` 表示使用 `tidb_executor_concurrency` 的值。\n\n### `tidb_hashagg_partial_concurrency`\n\n> **警告：**\n>\n> 从 v5.0 版本开始，该变量被废弃。请使用 [`tidb_executor_concurrency`](#tidb_executor_concurrency-从-v50-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置并行 hash aggregation 算法 partial 阶段的执行并发度。对于聚合函数参数不为 distinct 的情况，HashAgg 分为 partial 和 final 阶段分别并行执行。\n- 默认值 `-1` 表示使用 `tidb_executor_concurrency` 的值。\n\n### `tidb_historical_stats_duration` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：Duration\n- 默认值：`168h`，即 7 天\n- 这个变量用来控制历史统计信息在存储中的保留时间。\n\n### `tidb_idle_transaction_timeout` <span class=\"version-mark\">从 v7.6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 31536000]`\n- 单位：秒\n- 这个变量用来控制用户会话中事务的空闲超时。当用户会话处于事务状态且空闲时间超过该变量设定的值时，会话会被 Kill 掉。用户会话空闲是指没有正在执行的请求，处于等待请求的状态。\n- 默认值 `0` 表示没有时间限制。\n\n### `tidb_ignore_prepared_cache_close_stmt` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置是否忽略关闭 Prepared Statement 的指令。\n- 如果变量值设为 `ON`，Binary 协议的 `COM_STMT_CLOSE` 信号和文本协议的 [`DEALLOCATE PREPARE`](/sql-statements/sql-statement-deallocate.md) 语句都会被忽略。\n\n### `tidb_index_join_batch_size`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`25000`\n- 范围：`[1, 2147483647]`\n- 单位：行\n- 这个变量用来设置 index lookup join 操作的 batch 大小，AP 类应用适合较大的值，TP 类应用适合较小的值。\n\n### `tidb_index_join_double_read_penalty_cost_rate` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 默认值：`0`\n- 范围：`[0, 18446744073709551615]`\n- 这个变量用来设置是否给选择 index join 增加一些惩罚性的代价，以降低优化器选择 index join 操作的倾向，从而增加选择其他 join 方式的倾向，例如如选择 hash join 和 tiflash join 等。\n- 优化器选择 index join 可能触发较多的回表请求，造成较多的资源开销，此时可以通过设置这个变量，来减少优化器选择 index join 的倾向。\n- 这个变量只有在 [`tidb_cost_model_version`](/system-variables.md#tidb_cost_model_version-从-v620-版本开始引入) 设置为 `2` 时生效。\n\n### `tidb_index_lookup_concurrency`\n\n> **警告：**\n>\n> 从 v5.0 版本开始，该变量被废弃。请使用 [`tidb_executor_concurrency`](#tidb_executor_concurrency-从-v50-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`-1`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置 index lookup 操作的并发度，AP 类应用适合较大的值，TP 类应用适合较小的值。\n- 默认值 `-1` 表示使用 `tidb_executor_concurrency` 的值。\n\n### `tidb_index_lookup_join_concurrency`\n\n> **警告：**\n>\n> 从 v5.0 版本开始，该变量被废弃。请使用 [`tidb_executor_concurrency`](#tidb_executor_concurrency-从-v50-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`-1`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置 index lookup join 算法的并发度。\n- 默认值 `-1` 表示使用 `tidb_executor_concurrency` 的值。\n\n### `tidb_index_merge_intersection_concurrency` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`-1`\n- 范围：`[1, 256]`\n- 这个变量用来设置索引合并进行交集操作时的最大并发度，仅在以动态裁剪模式访问分区表时有效。实际并发度为 `tidb_index_merge_intersection_concurrency` 与分区表分区数目两者中较小的值。\n- 默认值 `-1` 表示使用 [`tidb_executor_concurrency`](#tidb_executor_concurrency-从-v50-版本开始引入) 的值。\n\n### `tidb_index_lookup_size`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`20000`\n- 范围：`[1, 2147483647]`\n- 单位：行\n- 这个变量用来设置 index lookup 操作的 batch 大小，AP 类应用适合较大的值，TP 类应用适合较小的值。\n\n### `tidb_index_serial_scan_concurrency`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用来设置顺序 scan 操作的并发度，AP 类应用适合较大的值，TP 类应用适合较小的值。\n\n### `tidb_init_chunk_size`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`32`\n- 范围：`[1, 32]`\n- 单位：行\n- 这个变量用来设置执行过程中初始 chunk 的行数。默认值是 32，可设置的范围是 1～32。chunk 行数直接影响单个查询所需的内存。可以按照查询中所有的列的总宽度和 chunk 行数来粗略估算单个 chunk 所需内存，并结合执行器的并发数来粗略估算单个查询所需内存总量。建议单个 chunk 内存总量不要超过 16 MiB。\n\n### `tidb_instance_plan_cache_reserved_percentage` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n> **警告：**\n>\n> Instance Plan Cache 目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：浮点型\n- 默认值：`0.1`\n- 范围：`[0, 1]`\n- 这个变量用于控制内存驱逐后 [Instance Plan Cache](/system-variables.md#tidb_enable_instance_plan_cache-从-v840-版本开始引入) 的空闲内存百分比。当 Instance Plan Cache 使用的内存达到 [`tidb_instance_plan_cache_max_size`](#tidb_instance_plan_cache_max_size-从-v840-版本开始引入) 设置的上限时，TiDB 会按照 Least Recently Used (LRU) 算法开始驱逐内存中的执行计划，直到空闲内存比例超过 [`tidb_instance_plan_cache_reserved_percentage`](#tidb_instance_plan_cache_reserved_percentage-从-v840-版本开始引入) 设定的值。\n\n### `tidb_instance_plan_cache_max_size` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n> **警告：**\n>\n> Instance Plan Cache 目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`125829120`（即 120 MiB）\n- 单位：字节\n- 这个变量用于设置 [Instance Plan Cache](/system-variables.md#tidb_enable_instance_plan_cache-从-v840-版本开始引入) 的最大内存使用量。\n\n### `tidb_isolation_read_engines` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`tikv,tiflash,tidb`\n- 这个变量用于设置 TiDB 在读取数据时可以使用的存储引擎列表。\n\n### `tidb_last_ddl_info` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 默认值：\"\"\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 该变量为只读变量，TiDB 内部使用该变量获取当前会话中上一个 DDL 操作的信息。\n    - \"query\"：上一个 DDL 查询字符串。\n    - \"seq_num\"：每个 DDL 操作的序列号，用于标识 DDL 操作的顺序。\n\n### `tidb_last_query_info` <span class=\"version-mark\">从 v4.0.14 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 这是一个只读变量。用于在 TiDB 内部查询上一条 DML 语句的事务信息。查询的事务信息包括：\n    - `txn_scope`：事务的作用域，可能为 `global` 或 `local`。\n    - `start_ts`：事务开始的时间戳。\n    - `for_update_ts`：先前执行的 DML 语句的 `for_update_ts` 信息。这是 TiDB 用于测试的内部术语。通常，你可以忽略此信息。\n    - `error`：错误消息（如果有）。\n    - `ru_consumption`：执行语句的 [RU](/tidb-resource-control.md#什么是-request-unit-ru) 消耗。\n\n### `tidb_last_txn_info` <span class=\"version-mark\">从 v4.0.9 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 此变量用于获取当前会话中最后一个事务的信息。这是一个只读变量。事务信息包括：\n    - 事务的范围\n    - 开始时间戳和提交时间戳\n    - 事务的提交模式，可能是两阶段提交，一阶段提交，或者异步提交\n    - 事务从异步提交或一阶段提交到两阶段提交的回退信息\n    - 遇到的错误\n\n### `tidb_last_plan_replayer_token` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 这个变量是一个只读变量，用于获取当前会话中最后一个 `PLAN REPLAYER dump` 的结果。\n\n### `tidb_load_based_replica_read_threshold` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`\"1s\"`\n- 范围：`[0s, 1h]`\n- 类型：字符串\n- 这个变量用来设置基于负载的 replica read 的触发阈值。当 leader 节点的预估排队时间超过阈值时，TiDB 会优先从 follower 节点读取数据。格式为时间，例如 `\"100ms\"` 或 `\"1s\"`。详情见 [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md#打散读热点)。\n\n### `tidb_load_binding_timeout` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`200`\n- 范围：`(0, 2147483647]`\n- 单位：毫秒\n- 这个变量用来控制加载 binding 的超时时间。当加载 binding 的执行时间超过该值时，会停止加载。\n\n### `tidb_lock_unchanged_keys` <span class=\"version-mark\">从 v7.1.1 和 v7.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量控制在以下场景是否对某些 key 加锁。设置为 `ON` 时，都加锁，设置为 `OFF` 时，都不加锁。\n    - 在 `INSERT IGNORE` 语句和 `REPLACE` 语句中值重复的 key。在 v6.1.6 之前版本中，这些 key 不加锁。这个问题已在 [#42121](https://github.com/pingcap/tidb/issues/42121) 修复。\n    - 在 `UPDATE` 语句中值没有改变的唯一索引 key。在 v6.5.2 之前版本中，这些 key 不加锁。这个问题已在 [#36438](https://github.com/pingcap/tidb/issues/36438) 修复。\n- 为保证事务行为的一致性和合理性，不推荐修改该值。如果在升级 TiDB 后因为这两项修复导致严重的性能问题，且可以接受不加锁的行为（见上述 Issue），可以将该变量设置为 `OFF`。\n\n### `tidb_log_file_max_days` <span class=\"version-mark\">从 v5.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 2147483647]`\n- 这个变量可以调整当前 TiDB 实例上日志的最大保留天数。默认值是实例配置文件中指定的值，见配置项 [`max-days`](/tidb-configuration-file.md#max-days)。此变量只影响当前 TiDB 实例上的配置，重启后丢失，且配置文件不受影响。\n\n### `tidb_low_resolution_tso`\n\n- 作用域：SESSION | GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置是否启用低精度 TSO 特性。开启该功能之后，TiDB 使用缓存 Timestamp 来读取数据。缓存 Timestamp 默认每 2 秒更新一次。从 v8.0.0 开始，你可以通过 [`tidb_low_resolution_tso_update_interval`](#tidb_low_resolution_tso_update_interval-从-v800-版本开始引入) 配置缓存 Timestamp 的更新时间间隔。\n- 主要场景是在可以容忍读到旧数据的情况下，降低小的只读事务获取 TSO 的开销。\n- 从 v8.3.0 版本开始，该变量支持 GLOBAL 作用域。\n\n### `tidb_low_resolution_tso_update_interval` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`2000`\n- 范围：`[10, 60000]`\n- 这个变量用来设置低精度 TSO 特性中使用的缓存 Timestamp 的更新时间间隔，单位为毫秒。\n- 该变量只在低精度 TSO 特性 [`tidb_low_resolution_tso`](#tidb_low_resolution_tso) 启用时有效。\n\n### `tidb_nontransactional_ignore_error` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置是否在非事务语句中立刻返回错误。当设为 `OFF` 时，在碰到第一个报错的 batch 时，非事务 DML 语句即中止，取消其后的所有 batch，返回错误。当设为 `ON` 时，当某个 batch 执行报错时，其后的 batch 会继续执行，直到所有 batch 执行完毕，返回结果时把这些错误合并后返回。\n\n### `tidb_max_auto_analyze_time` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`43200`\n- 范围：`[0, 2147483647]`\n- 单位：秒\n- 这个变量用于指定自动 ANALYZE 的最大执行时间。当执行时间超出指定的时间时，自动 ANALYZE 会被终止。当该变量值为 0 时，自动 ANALYZE 没有最大执行时间的限制。\n\n### `tidb_max_bytes_before_tiflash_external_group_by` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[-1, 9223372036854775807]`\n- 这个变量用于指定 TiFlash 中带有 `GROUP BY` 的 Hash Aggregation 算子的最大内存使用量，单位为 byte，超过该值之后 TiFlash 会触发 Hash Aggregation 算子的落盘。当该变量值为 -1 时，TiDB 不传递该变量给 TiFlash。只有该变量值大于等于 0 时，TiDB 才会传递该变量给 TiFlash。该变量为 0 时表示内存使用无限制，即 TiFlash Hash Aggregation 算子不会触发落盘。详情见 [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)。\n\n> **注意：**\n>\n> - 假设一个 TiDB 集群有多个 TiFlash 节点，Aggregation 通常会在多个 TiFlash 节点上分布式执行。该变量控制的是单个 TiFlash 节点中 Aggregation 算子的最大内存使用量。\n> - 当该变量设置为 -1 时，TiFlash 将根据自身配置项 [`max_bytes_before_external_group_by`](/tiflash/tiflash-configuration.md#tiflash-配置参数-1) 的值来决定 Aggregation 算子的最大内存使用量。\n\n### `tidb_max_bytes_before_tiflash_external_join` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[-1, 9223372036854775807]`\n- 这个变量用于指定 TiFlash 中带等值关联条件的 Hash Join 算子的最大内存使用量，单位为 byte，超过该值之后 TiFlash 会触发 Hash Join 算子的落盘。当该变量值为 -1 时，TiDB 不传递该变量给 TiFlash。只有该变量值大于等于 0 时，TiDB 才会传递该变量给 TiFlash。该变量为 0 时表示内存使用无限制，即 TiFlash Hash Join 算子不会触发落盘。详情见 [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)。\n\n> **注意：**\n>\n> - 假设一个 TiDB 集群有多个 TiFlash 节点，Join 通常会在多个 TiFlash 节点上分布式执行。该变量控制的是单个 TiFlash 节点中 Join 算子的最大内存使用量。\n> - 当该变量设置为 -1 时，TiFlash 将根据自身配置项 [`max_bytes_before_external_join`](/tiflash/tiflash-configuration.md#tiflash-配置参数-1) 的值来决定 Join 算子的最大内存使用量。\n\n### `tidb_max_bytes_before_tiflash_external_sort` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[-1, 9223372036854775807]`\n- 这个变量用于指定 TiFlash 中带 topN 和 sort 算子的最大内存使用量，单位为 byte，超过该值之后 TiFlash 会触发 topN 和 sort 算子的落盘。当该变量值为 -1 时，TiDB 不传递该变量给 TiFlash。只有该变量值大于等于 0 时，TiDB 才会传递该变量给 TiFlash。该变量为 0 时表示内存使用无限制，即 TiFlash topN 和 sort 算子不会触发落盘。详情见 [TiFlash 数据落盘](/tiflash/tiflash-spill-disk.md)。\n\n> **注意：**\n>\n> - 假设一个 TiDB 集群有多个 TiFlash 节点，TopN 和 Sort 通常会在多个 TiFlash 节点中分布式执行。该变量控制的是单个 TiFlash 节点中 TopN 和 Sort 算子的最大内存使用量。\n> - 当该变量设置为 -1 时，TiFlash 将根据自身配置项 [`max_bytes_before_external_sort`](/tiflash/tiflash-configuration.md#tiflash-配置参数-1) 的值来决定 TopN 和 Sort 算子的最大内存使用量。\n\n### `tidb_max_chunk_size`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1024`\n- 范围：`[32, 2147483647]`\n- 单位：行\n- 这个变量用来设置执行过程中一个 chunk 最大的行数，设置过大可能引起缓存局部性的问题，建议该变量不要超过 65536。chunk 行数直接影响单个查询所需的内存。可以按照查询中所有的列的总宽度和 chunk 行数来粗略估算单个 chunk 所需内存，并结合执行器的并发数来粗略估算单个查询所需内存总量。建议单个 chunk 内存总量不要超过 16 MiB。当查询涉及数据量较大、单个 chunk 无法处理所有数据时，TiDB 会进行多次处理，每次处理时将 chunk 行数翻倍，从 [`tidb_init_chunk_size`](#tidb_init_chunk_size) 开始，直到 chunk 行数达到最大值 `tidb_max_chunk_size`。\n\n### `tidb_max_delta_schema_count`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1024`\n- 范围：`[100, 16384]`\n- 这个变量用来设置缓存 schema 版本信息（对应版本修改的相关 table IDs）的个数限制，可设置的范围 100 - 16384。此变量在 2.1.18 及之后版本支持。\n\n### `tidb_max_paging_size` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`50000`\n- 范围：`[1, 9223372036854775807]`\n- 单位：行\n- 这个变量用来设置 coprocessor 协议中 paging size 的最大的行数。请合理设置该值，设置过小，TiDB 与 TiKV 的 RPC 交互会更频繁；设置过大，导数据和全表扫等特定场景会占用更多内存。该变量的默认值对于 OLTP 场景较友好，如果业务只使用了 TiKV 作为存储引擎，当执行偏 OLAP 的负载时，可以考虑将变量值调大，有可能获得更好的性能。\n\n### `tidb_max_tiflash_threads` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[-1, 256]`\n- 单位：线程\n- TiFlash 中 request 执行的最大并发度。默认值为 `-1`，表示该系统变量无效，此时最大并发度取决于 TiFlash 配置项 `profiles.default.max_threads` 的设置。`0` 表示由 TiFlash 系统自动设置该值。\n\n### `tidb_mem_oom_action` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`CANCEL`\n- 可选值：`CANCEL`，`LOG`\n- 该变量控制当单个查询使用的内存超过限制 (`tidb_mem_quota_query`) 且不能再利用临时磁盘时，TiDB 所采取的操作。详情见 [TiDB 内存控制](/configure-memory-usage.md)。\n- 该变量默认值为 `CANCEL`，但在 TiDB v4.0.2 及之前的版本中，默认值为 `LOG`。\n- 在 v6.1.0 之前这个开关通过 TiDB 配置文件 (`oom-action`) 进行配置，升级到 v6.1.0 时会自动继承原有设置。\n\n### `tidb_mem_quota_analyze` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n> **警告：**\n>\n> 目前限制 ANALYZE 的内存使用量为实验特性，在生产环境中使用时可能存在内存统计有误差的情况。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 单位：字节\n- 取值范围：`[-1, 9223372036854775807]`\n- 这个变量用来控制 TiDB 更新统计信息时的最大总内存占用，包括用户执行的 [`ANALYZE TABLE`](/sql-statements/sql-statement-analyze-table.md) 和 TiDB 后台自动执行的统计信息更新任务。当总的内存占用超过这个阈值时，用户执行的 `ANALYZE` 会被终止退出，并通过错误信息提示用户尝试更小的采样率或稍后重试。如果 TiDB 后台自动执行的统计信息更新任务因内存超限而退出，且使用的采样率高于默认值，则会使用默认采样率重试一次。当该变量值为负数或零时，TiDB 不对更新统计信息的前后台任务进行内存限制。\n\n> **注意：**\n>\n> 只有在 TiDB 的启动配置文件中开启了 `run-auto-analyze` 选项，该 TiDB 集群才会触发 `auto_analyze`。\n\n### `tidb_mem_quota_apply_cache` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`33554432` (32 MiB)\n- 范围：`[0, 9223372036854775807]`\n- 单位：字节\n- 这个变量用来设置 `Apply` 算子中局部 Cache 的内存使用阈值。\n- `Apply` 算子中局部 Cache 用来加速 `Apply` 算子的计算，该变量可以设置 `Apply` Cache 的内存使用阈值。设置变量值为 `0` 可以关闭 `Apply` Cache 功能。\n\n### `tidb_mem_quota_binding_cache` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`67108864` (64 MiB)\n- 范围：`[0, 2147483647]`\n- 单位：字节\n- 这个变量用来设置存放 `binding` 的缓存的内存使用阈值。\n- 如果一个系统创建或者捕获了过多的绑定，导致绑定所使用的内存空间超过该阈值，TiDB 会在日志中增加警告日志进行提示。这种情况下，缓存无法存放所有可用的绑定，并且无法保证哪些绑定存在于缓存中，因此，可能存在一些查询无法使用可用绑定的情况。此时，可以调大该变量的值，从而保证所有可用绑定都能正常使用。修改变量值以后，需要执行命令 `admin reload bindings` 重新加载绑定，确保变更生效。\n\n### `tidb_mem_quota_query`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1073741824` (1 GiB)\n- 范围：`[-1, 9223372036854775807]`\n- 单位：字节\n- 在 v6.1.0 之前的版本中，作用域为 `SESSION`。v6.1.0 及之后的版本，作用域变更为 `SESSION | GLOBAL`。\n- 在 v6.5.0 之前的版本中，该变量用来设置单条查询的内存使用限制，如果单条查询执行过程中使用的内存量超过该阈值，会触发系统变量 [`tidb_mem_oom_action`](#tidb_mem_oom_action-从-v610-版本开始引入) 中指定的行为。\n- 在 v6.5.0 及之后的版本中，该变量用来设置单个会话整体的内存使用限制，如果某个会话执行过程中使用的内存量超过该阈值，会触发系统变量 [`tidb_mem_oom_action`](#tidb_mem_oom_action-从-v610-版本开始引入) 中指定的行为。需要注意的是，自 v6.5.0 版本开始，会话的内存使用量包含会话中事务所消耗的内存。v6.5.0 及之后版本对事务内存的控制行为有所变化，详见 [txn-total-size-limit](/tidb-configuration-file.md#txn-total-size-limit)。\n- 当变量值为 `0` 或 `-1` 时，表示内存阈值为正无穷。此外，当变量值小于 128 时，将默认被设置为 `128`。\n\n### `tidb_memory_debug_mode_alarm_ratio`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：浮点型\n- 默认值：`0`\n- 该变量表示在 TiDB memory debug 模式下，允许的内存统计误差值。\n- 该变量用于 TiDB 内部测试，**不推荐修改该变量值**。\n\n### `tidb_memory_debug_mode_min_heap_inuse`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 该变量用于 TiDB 内部测试，**不推荐修改该变量值**，因为开启后会影响 TiDB 的性能。\n- 配置此参数后，TiDB 会进入 memory debug 模式进行内存追踪准确度的分析。TiDB 会在后续执行 SQL 语句的过程中频繁触发 GC，并将实际内存使用和内存统计值做对比。若当前内存使用大于 `tidb_memory_debug_mode_min_heap_inuse` 且内存统计误差超过 `tidb_memory_debug_mode_alarm_ratio`，则会输出相关内存信息到日志和文件中。\n\n### `tidb_memory_usage_alarm_ratio`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：浮点数\n- 默认值：`0.7`\n- 范围：`[0.0, 1.0]`\n- 这个变量用于设置触发 tidb-server 内存告警的内存使用比率。默认情况下，当 TiDB 内存使用量超过总内存的 70% 且满足[报警条件](/configure-memory-usage.md#tidb-server-内存占用过高时的报警)时，TiDB 会打印报警日志。\n- 当配置该变量的值为 `0` 或 `1` 时，表示关闭内存阈值报警功能。\n- 当配置该变量为 `0` 到 `1` 之间的值时，表示开启内存阈值报警功能：\n\n    - 如果系统变量 [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-从-v640-版本开始引入) 等于 0，则内存报警阈值为 `tidb_memory_usage_alarm_ratio * 系统内存大小`。\n    - 如果系统变量 `tidb_server_memory_limit` 被设置为大于 0，则内存报警阈值为 `tidb_memory_usage_alarm_ratio * tidb_server_memory_limit`。\n\n### `tidb_memory_usage_alarm_keep_record_num` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`5`\n- 范围：`[1, 10000]`\n- 当 tidb-server 内存占用超过内存报警阈值并触发报警时，TiDB 默认只保留最近 5 次报警时所生成的状态文件。你可以通过该变量调整该次数。\n\n### `tidb_merge_join_concurrency`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`1`\n- 取值范围：`[1, 256]`\n- 设置 `MergeJoin` 算子执行查询时的并发度。\n- **不推荐设置该变量**，修改该变量值可能会造成数据正确性问题。\n\n### `tidb_merge_partition_stats_concurrency`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`1`\n- 这个变量用于 TiDB analyze 分区表时，对分区表统计信息进行合并时的并发度。\n\n### `tidb_enable_async_merge_global_stats` <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`。从 v7.5.0 之前版本升级到 v7.5.0 或之后版本时，默认值为 `OFF`。\n- 这个变量用于设置 TiDB 使用异步方式合并统计信息，以避免 OOM 问题。\n\n### `tidb_metric_query_range_duration` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`60`\n- 范围：`[10, 216000]`\n- 单位：秒\n- 这个变量设置了查询 `METRIC_SCHEMA` 时生成的 Prometheus 语句的 range duration。\n\n### `tidb_metric_query_step` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`60`\n- 范围：`[10, 216000]`\n- 单位：秒\n- 这个变量设置了查询 `METRIC_SCHEMA` 时生成的 Prometheus 语句的 step。\n\n### `tidb_min_paging_size` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`128`\n- 范围：`[1, 9223372036854775807]`\n- 单位：行\n- 这个变量用来设置 coprocessor 协议中 paging size 的最小的行数。请合理设置该值，设置过小，TiDB 与 TiKV 的 RPC 交互会更频繁；设置过大，IndexLookup 带 Limit 场景会出现性能下降。该变量的默认值对于 OLTP 场景较友好，如果业务只使用了 TiKV 作为存储引擎，当执行偏 OLAP 的负载时，可以考虑将变量值调大，有可能获得更好的性能。\n\n![Paging size impact on TPCH](/media/paging-size-impact-on-tpch.png)\n\n开启 [`tidb_enable_paging`](#tidb_enable_paging-从-v540-版本开始引入) 时，`tidb_min_paging_size` 和 [`tidb_max_paging_size`](#tidb_max_paging_size-从-v630-版本开始引入) 对 TPCH 的性能影响如上图所示，纵轴是执行时间，越小越好。\n\n### `tidb_mpp_store_fail_ttl`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：Duration\n- 默认值：`60s`\n- 刚重启的 TiFlash 可能不能正常提供服务。为了防止查询失败，TiDB 会限制 tidb-server 向刚重启的 TiFlash 节点发送查询。这个变量表示刚重启的 TiFlash 不被发送请求的时间范围。\n\n### `tidb_multi_statement_mode` <span class=\"version-mark\">从 v4.0.11 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`OFF`\n- 可选值：`OFF`，`ON`，`WARN`\n- 该变量用于控制是否在同一个 `COM_QUERY` 调用中执行多个查询。\n- 为了减少 SQL 注入攻击的影响，TiDB 目前默认不允许在同一 `COM_QUERY` 调用中执行多个查询。该变量可用作早期 TiDB 版本的升级路径选项。该变量值与是否允许多语句行为的对照表如下：\n\n| 客户端设置         | `tidb_multi_statement_mode` 值 | 是否允许多语句 |\n|------------------------|-----------------------------------|--------------------------------|\n| Multiple Statements = ON  | OFF                               | 允许                            |\n| Multiple Statements = ON  | ON                                | 允许                            |\n| Multiple Statements = ON  | WARN                              | 允许                            |\n| Multiple Statements = OFF | OFF                               | 不允许                             |\n| Multiple Statements = OFF | ON                                | 允许                            |\n| Multiple Statements = OFF | WARN                              | 允许 + 警告提示        |\n\n> **注意：**\n>\n> 只有默认值 `OFF` 才是安全的。如果用户业务是专为早期 TiDB 版本而设计的，那么需要将该变量值设为 `ON`。如果用户业务需要多语句支持，建议用户使用客户端提供的设置，不要使用 `tidb_multi_statement_mode` 变量进行设置。\n\n>\n> * [go-sql-driver](https://github.com/go-sql-driver/mysql#multistatements) (`multiStatements`)\n> * [Connector/J](https://dev.mysql.com/doc/connector-j/en/connector-j-reference-configuration-properties.html) (`allowMultiQueries`)\n> * PHP [mysqli](https://www.php.net/manual/en/mysqli.quickstart.multiple-statement.php) (`mysqli_multi_query`)\n\n### `tidb_opt_agg_push_down`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置优化器是否执行聚合函数下推到 Join，Projection 和 UnionAll 之前的优化操作。当查询中聚合操作执行很慢时，可以尝试设置该变量为 ON。\n\n### `tidb_opt_broadcast_cartesian_join`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[0, 2]`\n- 表示是否允许 Broadcast Cartesian Join 算法。\n- 值为 `0` 时表示不允许使用 Broadcast Cartesian Join 算法。值为 `1` 时表示根据 [`tidb_broadcast_join_threshold_count`](#tidb_broadcast_join_threshold_count-从-v50-版本开始引入) 的行数阈值确定是否允许使用 Broadcast Cartesian Join 算法。值为 `2` 时表示总是允许 Broadcast Cartesian Join 算法，即使表的大小超过了该阈值。\n- 该变量是 TiDB 内部使用的变量，**不推荐**修改该变量的值。\n\n### `tidb_opt_concurrency_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 18446744073709551615]`\n- 默认值：`3.0`\n- 表示在 TiDB 中开启一个 Golang goroutine 的 CPU 开销。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n### `tidb_opt_copcpu_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 18446744073709551615]`\n- 默认值：`3.0`\n- 表示 TiKV 协处理器处理一行数据的 CPU 开销。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n### `tidb_opt_correlation_exp_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[0, 2147483647]`\n- 当交叉估算方法不可用时，会采用启发式估算方法。这个变量用来控制启发式方法的行为。当值为 0 时不用启发式估算方法，大于 0 时，该变量值越大，启发式估算方法越倾向 index scan，越小越倾向 table scan。\n\n### `tidb_opt_correlation_threshold`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 默认值：`0.9`\n- 范围：`[0, 1]`\n- 这个变量用来设置优化器启用交叉估算 row count 方法的阈值。如果列和 handle 列之间的顺序相关性超过这个阈值，就会启用交叉估算方法。\n- 交叉估算方法可以简单理解为，利用这个列的直方图来估算 handle 列需要扫的行数。\n\n### `tidb_opt_cpu_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 2147483647]`\n- 默认值：`3.0`\n- 表示 TiDB 处理一行数据的 CPU 开销。该变量是[代价模型](/cost-model.md)内部使用的变量，不建议修改该变量的值。\n\n### `tidb_opt_derive_topn` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 表示是否开启[从窗口函数中推导 TopN 或 Limit](/derive-topn-from-window.md) 的优化规则。\n\n### `tidb_opt_desc_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 18446744073709551615]`\n- 默认值：`3.0`\n- 表示降序扫描时，TiKV 在磁盘上扫描一行数据的开销。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n### `tidb_opt_disk_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 18446744073709551615]`\n- 默认值：`1.5`\n- 表示 TiDB 往临时磁盘读写一个字节数据的 I/O 开销。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n### `tidb_opt_distinct_agg_push_down`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置优化器是否执行带有 `Distinct` 的聚合函数（比如 `select count(distinct a) from t`）下推到 Coprocessor 的优化操作。当查询中带有 `Distinct` 的聚合操作执行很慢时，可以尝试设置该变量为 `1`。\n\n在以下示例中，`tidb_opt_distinct_agg_push_down` 开启前，TiDB 需要从 TiKV 读取所有数据，并在 TiDB 侧执行 `distinct`。`tidb_opt_distinct_agg_push_down` 开启后，`distinct a` 被下推到了 Coprocessor，在 `HashAgg_5` 里新增里一个 `group by` 列 `test.t.a`。\n\n```sql\nmysql> desc select count(distinct a) from test.t;\n+-------------------------+----------+-----------+---------------+------------------------------------------+\n| id                      | estRows  | task      | access object | operator info                            |\n+-------------------------+----------+-----------+---------------+------------------------------------------+\n| StreamAgg_6             | 1.00     | root      |               | funcs:count(distinct test.t.a)->Column#4 |\n| └─TableReader_10        | 10000.00 | root      |               | data:TableFullScan_9                     |\n|   └─TableFullScan_9     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo           |\n+-------------------------+----------+-----------+---------------+------------------------------------------+\n3 rows in set (0.01 sec)\n\nmysql> set session tidb_opt_distinct_agg_push_down = 1;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> desc select count(distinct a) from test.t;\n+---------------------------+----------+-----------+---------------+------------------------------------------+\n| id                        | estRows  | task      | access object | operator info                            |\n+---------------------------+----------+-----------+---------------+------------------------------------------+\n| HashAgg_8                 | 1.00     | root      |               | funcs:count(distinct test.t.a)->Column#3 |\n| └─TableReader_9           | 1.00     | root      |               | data:HashAgg_5                           |\n|   └─HashAgg_5             | 1.00     | cop[tikv] |               | group by:test.t.a,                       |\n|     └─TableFullScan_7     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo           |\n+---------------------------+----------+-----------+---------------+------------------------------------------+\n4 rows in set (0.00 sec)\n```\n\n### `tidb_opt_enable_correlation_adjustment`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制优化器是否开启交叉估算。\n\n### `tidb_opt_enable_hash_join` <span class=\"version-mark\">从 v6.5.6、v7.1.2 和 v7.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 控制优化器是否会选择表的哈希连接。默认打开 (`ON`)。设置为 `OFF` 时，优化器在生成执行计划时会避免选择表的哈希连接，除非没有其他连接方式可用。\n- 如果同时使用了 `tidb_opt_enable_hash_join` 和 `HASH_JOIN` Hint，则 `HASH_JOIN` Hint 优先级更高。即使 `tidb_opt_enable_hash_join` 被设置为 `OFF`，如果在查询中指定了 `HASH_JOIN` Hint，TiDB 优化器仍然会强制执行哈希连接计划。\n\n### `tidb_opt_enable_non_eval_scalar_subquery` <span class=\"version-mark\">从 v7.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制 `EXPLAIN` 语句是否禁止提前执行可以在优化阶段展开的常量子查询。该变量设置为 `OFF` 时，`EXPLAIN` 语句会在优化阶段提前展开子查询。该变量设置为 `ON` 时，`EXPLAIN` 语句不会在优化阶段展开子查询。更多信息请参考[禁止子查询提前展开](/explain-walkthrough.md#禁止子查询提前执行)。\n\n### `tidb_opt_enable_late_materialization` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制是否启用 [TiFlash 延迟物化](/tiflash/tiflash-late-materialization.md)功能。注意在 TiFlash [Fast Scan 模式](/tiflash/use-fastscan.md)下，延迟物化功能暂不可用。\n- 当设置该变量为 `OFF` 关闭 TiFlash 延迟物化功能时，如果 `SELECT` 语句中包含过滤条件（`WHERE` 子句），TiFlash 会先扫描查询所需列的全部数据后再进行过滤。当设置该变量为 `ON` 开启 TiFlash 延迟物化功能时，TiFlash 会先扫描下推到 TableScan 算子的过滤条件相关的列数据，过滤得到符合条件的行后，再扫描这些行的其他列数据，继续后续计算，从而减少 IO 扫描和数据处理的计算量。\n\n### `tidb_opt_enable_mpp_shared_cte_execution` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n> **警告：**\n>\n> 该变量控制的功能为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制非递归的[公共表表达式 (CTE)](/sql-statements/sql-statement-with.md) 是否可以在 TiFlash MPP 执行。默认情况下，未开启该变量时，CTE 在 TiDB 执行，相较于开启该功能，执行性能有较大差距。\n\n### `tidb_opt_enable_fuzzy_binding` <span class=\"version-mark\">从 v7.6.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量控制是否开启[跨数据库绑定执行计划](/sql-plan-management.md#跨数据库绑定执行计划-cross-db-binding)功能。\n\n### `tidb_opt_fix_control` <span class=\"version-mark\">从 v6.5.3 和 v7.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：字符串\n- 默认值：`\"\"`\n- 这个变量用来控制优化器的一些内部行为。\n- 一部分优化器行为的选择依赖用户场景或 SQL 编写方式。通过设置该变量，你可以更细粒度地控制优化器的行为，并且避免集群升级后优化器行为变化导致的性能回退。\n- 详细介绍请参考 [Optimizer Fix Controls](/optimizer-fix-controls.md)。\n\n### `tidb_opt_force_inline_cte` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制是否强制 inline CTE。默认值为 `OFF`，即默认不强制 inline CTE。注意，此时依旧可以通过 `MERGE()` hint 来开启个别 CTE 的 inline。如果设置为 `ON`，则当前 session 中所有查询的 CTE（递归 CTE 除外）都会 inline。\n\n### `tidb_opt_advanced_join_hint` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制包括 [`HASH_JOIN()` Hint](/optimizer-hints.md#hash_joint1_name--tl_name-)、[`MERGE_JOIN()` Hint](/optimizer-hints.md#merge_joint1_name--tl_name-) 等用于控制连接算法的 Join Method Hint 是否会影响 Join Reorder 的优化过程，包括 [`LEADING()` Hint](/optimizer-hints.md#leadingt1_name--tl_name-) 的使用。默认值为 `ON`，即默认不影响。如果设置为 `OFF`，在一些同时使用 Join Method Hint 和 `LEADING()` Hint 的场景下可能会产生冲突。\n\n> **注意：**\n>\n> v7.0.0 之前的版本行为和将该变量设置为 `OFF` 的行为一致。为确保向前兼容，从旧版本升级到 v7.0.0 及之后版本的集群，该变量会被设置成 `OFF`。为了获取更灵活的 Hint 行为，强烈建议在确保无性能回退的情况下，将该变量切换为 `ON`。\n\n### `tidb_opt_insubq_to_join_and_agg`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来设置是否开启优化规则：将子查询转成 join 和 aggregation。\n\n    例如，打开这个优化规则后，会将下面子查询做如下变化：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t where t.a in (select aa from t1);\n    ```\n\n    将子查询转成如下 join：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select t.* from t, (select aa from t1 group by aa) tmp_t where t.a = tmp_t.aa;\n    ```\n\n    如果 t1 在列 `aa` 上有 unique 且 not null 的限制，可以直接改写为如下，不需要添加 aggregation。\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select t.* from t, t1 where t.a=t1.aa;\n    ```\n\n### `tidb_opt_join_reorder_threshold`\n\n- 作用域: SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 2147483647]`\n- 这个变量用来控制 TiDB Join Reorder 算法的选择。当参与 Join Reorder 的节点个数大于该阈值时，TiDB 选择贪心算法，小于该阈值时 TiDB 选择动态规划 (dynamic programming) 算法。\n- 目前对于 OLTP 的查询，推荐保持默认值。对于 OLAP 的查询，推荐将变量值设为 10~15 来获得 AP 场景下更好的连接顺序。\n\n### `tidb_opt_limit_push_down_threshold`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`100`\n- 范围：`[0, 2147483647]`\n- 这个变量用来设置将 Limit 和 TopN 算子下推到 TiKV 的阈值。\n- 如果 Limit 或者 TopN 的取值小于等于这个阈值，则 Limit 和 TopN 算子会被强制下推到 TiKV。该变量可以解决部分由于估算误差导致 Limit 或者 TopN 无法被下推的问题。\n\n### `tidb_opt_memory_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 2147483647]`\n- 默认值：`0.001`\n- 表示 TiDB 存储一行数据的内存开销。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n### `tidb_opt_mpp_outer_join_fixed_build_side` <span class=\"version-mark\">从 v5.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 当该变量值为 `ON` 时，左连接始终使用内表作为构建端，右连接始终使用外表作为构建端。将该变量值设为 `OFF` 后，外连接可以灵活选择任意一边表作为构建端。\n\n### `tidb_opt_network_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 2147483647]`\n- 默认值：`1.0`\n- 表示传输 1 比特数据的网络净开销。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n### `tidb_opt_objective` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：枚举型\n- 默认值：`moderate`\n- 可选值：`moderate`、`determinate`\n- 该变量用于设置优化器优化目标。`moderate` 与 TiDB v7.4.0 之前版本的默认行为保持一致，优化器会利用更多信息尝试生成更优的计划。`determinate` 则倾向于保守，保持执行计划稳定。\n- 实时统计信息是 TiDB 在运行时根据 DML 语句自动更新的表的总行数以及修改的行数。该变量保持默认值 `moderate` 时，TiDB 会基于实时统计信息来生成执行计划。该变量设为 `determinate` 后，TiDB 在生成执行计划时将不再使用实时统计信息，这会让执行计划相对稳定。\n- 对于长期稳定的 OLTP 业务，或者如果用户对系统已有的执行计划非常确定，则推荐使用 `determinate` 模式减少执行计划跳变的可能。同时还可以结合 [`LOCK STATS`](/sql-statements/sql-statement-lock-stats.md) 来阻止统计信息的更新，进一步稳定执行计划。\n\n### `tidb_opt_ordering_index_selectivity_ratio` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 默认值：`-1`\n- 范围：`[-1, 1]`\n- 当一个索引满足 SQL 语句中的 `ORDER BY` 和 `LIMIT` 子句，但有部分过滤条件未被该索引覆盖时，该系统变量用于控制该索引的估算行数。\n- 该变量适用的场景与系统变量 [`tidb_opt_ordering_index_selectivity_threshold`](#tidb_opt_ordering_index_selectivity_threshold-从-v700-版本开始引入) 相同。\n- 与 `tidb_opt_ordering_index_selectivity_threshold` 的实现不同，该变量采用范围内符合条件的可能行数的比率或百分比。\n- 取值为 `-1`（默认值）或小于 `0` 时，禁用此变量。取值在 `0` 到 `1` 之间时，对应 0% 到 100% 的比率（例如，`0.5` 对应 `50%`）。\n- 在以下示例中，表 `t` 共有 1,000,000 行数据。示例使用相同查询，但应用了不同的 `tidb_opt_ordering_index_selectivity_ratio` 值。示例中的查询包含一个 `WHERE` 子句谓词，该谓词匹配少量行（1,000,000 中的 9,000 行）。存在一个支持 `ORDER BY a` 的索引（索引 `ia`），但是对 `b` 的过滤不在此索引中。根据实际的数据分布，满足 `WHERE` 子句和 `LIMIT 1` 的行可能在扫描非过滤索引时作为第一行访问到，也可能在几乎处理满足所有行后才找到。\n- 每个示例中都使用了一个索引 hint，用于展示对 estRows 的影响。最终计划选择取决于是否存在代价更低的其他计划。\n- 第一个示例使用默认值 `-1`，使用现有的估算公式。默认行为是，在找到符合条件的行之前，会扫描一小部分行进行估算。\n\n    ```sql\n    > SET SESSION tidb_opt_ordering_index_selectivity_ratio = -1;\n\n    > EXPLAIN SELECT * FROM t USE INDEX (ia) WHERE b <= 9000 ORDER BY a LIMIT 1;\n    +-----------------------------------+---------+-----------+-----------------------+---------------------------------+\n    | id                                | estRows | task      | access object         | operator info                   |\n    +-----------------------------------+---------+-----------+-----------------------+---------------------------------+\n    | Limit_12                          | 1.00    | root      |                       | offset:0, count:1               |\n    | └─Projection_22                   | 1.00    | root      |                       | test.t.a, test.t.b, test.t.c    |\n    |   └─IndexLookUp_21                | 1.00    | root      |                       |                                 |\n    |     ├─IndexFullScan_18(Build)     | 109.20  | cop[tikv] | table:t, index:ia(a)  | keep order:true                 |\n    |     └─Selection_20(Probe)         | 1.00    | cop[tikv] |                       | le(test.t.b, 9000)              |\n    |       └─TableRowIDScan_19         | 109.20  | cop[tikv] | table:t               | keep order:false                |\n    +-----------------------------------+---------+-----------+-----------------------+---------------------------------+\n    ```\n\n- 第二个示例使用 `0`，假设在找到符合条件的行之前，将扫描 0% 的行。\n\n    ```sql\n    > SET SESSION tidb_opt_ordering_index_selectivity_ratio = 0;\n\n    > EXPLAIN SELECT * FROM t USE INDEX (ia) WHERE b <= 9000 ORDER BY a LIMIT 1;\n    +-----------------------------------+---------+-----------+-----------------------+---------------------------------+\n    | id                                | estRows | task      | access object         | operator info                   |\n    +-----------------------------------+---------+-----------+-----------------------+---------------------------------+\n    | Limit_12                          | 1.00    | root      |                       | offset:0, count:1               |\n    | └─Projection_22                   | 1.00    | root      |                       | test.t.a, test.t.b, test.t.c    |\n    |   └─IndexLookUp_21                | 1.00    | root      |                       |                                 |\n    |     ├─IndexFullScan_18(Build)     | 1.00    | cop[tikv] | table:t, index:ia(a)  | keep order:true                 |\n    |     └─Selection_20(Probe)         | 1.00    | cop[tikv] |                       | le(test.t.b, 9000)              |\n    |       └─TableRowIDScan_19         | 1.00    | cop[tikv] | table:t               | keep order:false                |\n    +-----------------------------------+---------+-----------+-----------------------+---------------------------------+\n    ```\n\n- 第三个示例使用 `0.1`，假设在找到符合条件的行之前，将扫描 10% 的行。这个条件的过滤性较强，只有 1% 的行符合条件，因此最坏情况是找到这 1% 之前需要扫描 99% 的行。99% 中的 10% 大约是 9.9%，该数值会反映在 estRows 中。\n\n    ```sql\n    > SET SESSION tidb_opt_ordering_index_selectivity_ratio = 0.1;\n\n    > EXPLAIN SELECT * FROM t USE INDEX (ia) WHERE b <= 9000 ORDER BY a LIMIT 1;\n    +-----------------------------------+----------+-----------+-----------------------+---------------------------------+\n    | id                                | estRows  | task      | access object         | operator info                   |\n    +-----------------------------------+----------+-----------+-----------------------+---------------------------------+\n    | Limit_12                          | 1.00     | root      |                       | offset:0, count:1               |\n    | └─Projection_22                   | 1.00     | root      |                       | test.t.a, test.t.b, test.t.c    |\n    |   └─IndexLookUp_21                | 1.00     | root      |                       |                                 |\n    |     ├─IndexFullScan_18(Build)     | 99085.21 | cop[tikv] | table:t, index:ia(a)  | keep order:true                 |\n    |     └─Selection_20(Probe)         | 1.00     | cop[tikv] |                       | le(test.t.b, 9000)              |\n    |       └─TableRowIDScan_19         | 99085.21 | cop[tikv] | table:t               | keep order:false                |\n    +-----------------------------------+----------+-----------+-----------------------+---------------------------------+\n    ```\n\n- 第四个示例使用 `1.0`，假设在找到符合条件的行之前，将扫描 100% 的行。\n\n    ```sql\n    > SET SESSION tidb_opt_ordering_index_selectivity_ratio = 1;\n\n    > EXPLAIN SELECT * FROM t USE INDEX (ia) WHERE b <= 9000 ORDER BY a LIMIT 1;\n    +-----------------------------------+-----------+-----------+-----------------------+---------------------------------+\n    | id                                | estRows   | task      | access object         | operator info                   |\n    +-----------------------------------+-----------+-----------+-----------------------+---------------------------------+\n    | Limit_12                          | 1.00      | root      |                       | offset:0, count:1               |\n    | └─Projection_22                   | 1.00      | root      |                       | test.t.a, test.t.b, test.t.c    |\n    |   └─IndexLookUp_21                | 1.00      | root      |                       |                                 |\n    |     ├─IndexFullScan_18(Build)     | 990843.14 | cop[tikv] | table:t, index:ia(a)  | keep order:true                 |\n    |     └─Selection_20(Probe)         | 1.00      | cop[tikv] |                       | le(test.t.b, 9000)              |\n    |       └─TableRowIDScan_19         | 990843.14 | cop[tikv] | table:t               | keep order:false                |\n    +-----------------------------------+-----------+-----------+-----------------------+---------------------------------+\n    ```\n\n- 第五个示例也使用 `1.0`，但是增加了一个对 `a` 的谓词，限制了最坏情况下的扫描范围，因为 `WHERE a <= 9000` 匹配了索引，大约有 9,000 行符合条件。考虑到 `b` 上的过滤谓词不在索引中，所有大约 9,000 行在找到符合 `b <= 9000` 的行之前都会被扫描。\n\n    ```sql\n    > SET SESSION tidb_opt_ordering_index_selectivity_ratio = 1;\n\n    > EXPLAIN SELECT * FROM t USE INDEX (ia) WHERE a <= 9000 AND b <= 9000 ORDER BY a LIMIT 1;\n    +------------------------------------+---------+-----------+-----------------------+------------------------------------+\n    | id                                 | estRows | task      | access object         | operator info                      |\n    +------------------------------------+---------+-----------+-----------------------+------------------------------------+\n    | Limit_12                           | 1.00    | root      |                       | offset:0, count:1                  |\n    | └─Projection_22                    | 1.00    | root      |                       | test.t.a, test.t.b, test.t.c       |\n    |   └─IndexLookUp_21                 | 1.00    | root      |                       |                                    |\n    |     ├─IndexRangeScan_18(Build)     | 9074.99 | cop[tikv] | table:t, index:ia(a)  | range:[-inf,9000], keep order:true |\n    |     └─Selection_20(Probe)          | 1.00    | cop[tikv] |                       | le(test.t.b, 9000)                 |\n    |       └─TableRowIDScan_19          | 9074.99 | cop[tikv] | table:t               | keep order:false                   |\n    +------------------------------------+---------+-----------+-----------------------+------------------------------------+\n    ```\n\n### `tidb_opt_ordering_index_selectivity_threshold` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 默认值：`0`\n- 范围：`[0, 1]`\n- 用于当 SQL 中存在 `ORDER BY` 和 `LIMIT` 子句且带有过滤条件时，控制优化器选择索引的行为。\n- 对于此类查询，优化器会考虑选择对应的索引来满足 `ORDER BY` 和 `LIMIT` 子句（即使这个索引并不满足任何过滤条件）。但是由于数据分布的复杂性，优化器在这种场景下可能会选择不优的索引。\n- 该变量表示一个阈值。当存在索引能满足过滤条件，且其选择率估算值低于该阈值时，优化器会避免选择用于满足 `ORDER BY` 和 `LIMIT` 的索引，而优先选择用于满足过滤条件的索引。\n- 例如，当把该变量设为 `0` 时，优化器保持默认行为；当设为 `1` 时，优化器总是优先选择满足过滤条件的索引，避免选择满足 `ORDER BY` 和 `LIMIT` 的索引。\n- 在以下示例中，`t` 表共有 1,000,000 行数据。使用 `b` 列上的索引时，其估算行数是大约 8,748 行，因此其选择率估算值大约是 0.0087。默认情况下，优化器选择了 `a` 列上的索引。而将该变量设为 `0.01` 之后，由于 `b` 列上的索引的选择率 (0.0087) 低于 0.01，优化器选择了 `b` 列上的索引。\n\n```sql\n> EXPLAIN SELECT * FROM t WHERE b <= 9000 ORDER BY a LIMIT 1;\n+-----------------------------------+---------+-----------+----------------------+--------------------+\n| id                                | estRows | task      | access object        | operator info      |\n+-----------------------------------+---------+-----------+----------------------+--------------------+\n| Limit_12                          | 1.00    | root      |                      | offset:0, count:1  |\n| └─Projection_25                   | 1.00    | root      |                      | test.t.a, test.t.b |\n|   └─IndexLookUp_24                | 1.00    | root      |                      |                    |\n|     ├─IndexFullScan_21(Build)     | 114.30  | cop[tikv] | table:t, index:ia(a) | keep order:true    |\n|     └─Selection_23(Probe)         | 1.00    | cop[tikv] |                      | le(test.t.b, 9000) |\n|       └─TableRowIDScan_22         | 114.30  | cop[tikv] | table:t              | keep order:false   |\n+-----------------------------------+---------+-----------+----------------------+--------------------+\n\n> SET SESSION tidb_opt_ordering_index_selectivity_threshold = 0.01;\n\n> EXPLAIN SELECT * FROM t WHERE b <= 9000 ORDER BY a LIMIT 1;\n+----------------------------------+---------+-----------+----------------------+-------------------------------------+\n| id                               | estRows | task      | access object        | operator info                       |\n+----------------------------------+---------+-----------+----------------------+-------------------------------------+\n| TopN_9                           | 1.00    | root      |                      | test.t.a, offset:0, count:1         |\n| └─IndexLookUp_20                 | 1.00    | root      |                      |                                     |\n|   ├─IndexRangeScan_17(Build)     | 8748.62 | cop[tikv] | table:t, index:ib(b) | range:[-inf,9000], keep order:false |\n|   └─TopN_19(Probe)               | 1.00    | cop[tikv] |                      | test.t.a, offset:0, count:1         |\n|     └─TableRowIDScan_18          | 8748.62 | cop[tikv] | table:t              | keep order:false                    |\n+----------------------------------+---------+-----------+----------------------+-------------------------------------+\n```\n\n### `tidb_opt_prefer_range_scan` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n> **注意：**\n>\n> 从 v8.4.0 开始，此变量的默认值从 `OFF` 更改为 `ON`。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量值为 `ON` 时，对于没有统计信息的表（伪统计信息）或空表（零统计信息），优化器将优先选择区间扫描而不是全表扫描。\n- 在以下示例中，`tidb_opt_prefer_range_scan` 开启前，TiDB 优化器需要执行全表扫描。`tidb_opt_prefer_range_scan` 开启后，优化器选择了索引区间扫描。\n\n```sql\nexplain select * from t where age=5;\n+-------------------------+------------+-----------+---------------+-------------------+\n| id                      | estRows    | task      | access object | operator info     |\n+-------------------------+------------+-----------+---------------+-------------------+\n| TableReader_7           | 1048576.00 | root      |               | data:Selection_6  |\n| └─Selection_6           | 1048576.00 | cop[tikv] |               | eq(test.t.age, 5) |\n|   └─TableFullScan_5     | 1048576.00 | cop[tikv] | table:t       | keep order:false  |\n+-------------------------+------------+-----------+---------------+-------------------+\n3 rows in set (0.00 sec)\n\nset session tidb_opt_prefer_range_scan = 1;\n\nexplain select * from t where age=5;\n+-------------------------------+------------+-----------+-----------------------------+-------------------------------+\n| id                            | estRows    | task      | access object               | operator info                 |\n+-------------------------------+------------+-----------+-----------------------------+-------------------------------+\n| IndexLookUp_7                 | 1048576.00 | root      |                             |                               |\n| ├─IndexRangeScan_5(Build)     | 1048576.00 | cop[tikv] | table:t, index:idx_age(age) | range:[5,5], keep order:false |\n| └─TableRowIDScan_6(Probe)     | 1048576.00 | cop[tikv] | table:t                     | keep order:false              |\n+-------------------------------+------------+-----------+-----------------------------+-------------------------------+\n3 rows in set (0.00 sec)\n```\n\n### `tidb_opt_projection_push_down` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`。在 v8.3.0 之前，默认值为 `OFF`。\n- 指定是否允许优化器将 `Projection` 算子下推到 TiKV。开启后，优化器可能会将以下三种类型的 `Projection` 算子下推到 TiKV：\n    - 算子顶层表达式全部为 [JSON 查询类函数](/functions-and-operators/json-functions/json-functions-search.md)或 [JSON 值属性类函数](/functions-and-operators/json-functions/json-functions-return.md)，例如 `SELECT JSON_EXTRACT(data, '$.name') FROM users;`。\n    - 算子顶层表达式部分为 JSON 查询类函数或 JSON 值属性类函数，部分为直接的列读取，例如 `SELECT JSON_DEPTH(data), name FROM users;`。\n    - 算子顶层表达式全部为直接的列读取，且输出的列数量小于输入的列数量，例如 `SELECT name FROM users;`。\n- `Projection` 算子最终下推与否，还取决于优化器对查询代价的综合评估。\n- 对于从 v8.3.0 以前的版本升级到 v8.3.0 或更新版本的 TiDB 集群，该变量将默认为 `OFF`。\n\n### `tidb_opt_range_max_size` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`67108864` (64 MiB)\n- 取值范围：`[0, 9223372036854775807]`\n- 单位：字节\n- 该变量用于指定优化器构造扫描范围的内存用量上限。当该变量为 `0` 时，表示对扫描范围没有内存限制。如果构造精确的扫描范围会超出内存用量限制，优化器会使用更宽松的扫描范围（例如 `[[NULL,+inf]]`）。如果执行计划中未使用精确的扫描范围，可以调大该变量的值让优化器构造精确的扫描范围。\n\n该变量的使用示例如下：\n\n<details>\n<summary><code>tidb_opt_range_max_size</code> 使用示例</summary>\n\n查看该变量的默认值，即优化器构造扫描范围最多使用 64 MiB 内存。\n\n```sql\nSELECT @@tidb_opt_range_max_size;\n```\n\n```sql\n+----------------------------+\n| @@tidb_opt_range_max_size |\n+----------------------------+\n| 67108864                   |\n+----------------------------+\n1 row in set (0.01 sec)\n```\n\n```sql\nEXPLAIN SELECT * FROM t use index (idx) WHERE a IN (10,20,30) AND b IN (40,50,60);\n```\n\n在 64 MiB 的内存最大限制约束下，优化器构造出精确的扫描范围 `[10 40,10 40], [10 50,10 50], [10 60,10 60], [20 40,20 40], [20 50,20 50], [20 60,20 60], [30 40,30 40], [30 50,30 50], [30 60,30 60]`，见如下执行计划返回结果。\n\n```sql\n+-------------------------------+---------+-----------+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| id                            | estRows | task      | access object            | operator info                                                                                                                                                               |\n+-------------------------------+---------+-----------+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| IndexLookUp_7                 | 0.90    | root      |                          |                                                                                                                                                                             |\n| ├─IndexRangeScan_5(Build)     | 0.90    | cop[tikv] | table:t, index:idx(a, b) | range:[10 40,10 40], [10 50,10 50], [10 60,10 60], [20 40,20 40], [20 50,20 50], [20 60,20 60], [30 40,30 40], [30 50,30 50], [30 60,30 60], keep order:false, stats:pseudo |\n| └─TableRowIDScan_6(Probe)     | 0.90    | cop[tikv] | table:t                  | keep order:false, stats:pseudo                                                                                                                                              |\n+-------------------------------+---------+-----------+--------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n3 rows in set (0.00 sec)\n```\n\n现将优化器构造扫描范围的内存用量上限设为 1500 字节。\n\n```sql\nSET @@tidb_opt_range_max_size = 1500;\n```\n\n```sql\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nEXPLAIN SELECT * FROM t USE INDEX (idx) WHERE a IN (10,20,30) AND b IN (40,50,60);\n```\n\n在 1500 字节内存的最大限制约束下，优化器构造出了更宽松的扫描范围 `[10,10], [20,20], [30,30]`，并用 warning 提示用户构造精确的扫描范围所需的内存用量超出了 `tidb_opt_range_max_size` 的限制。\n\n```sql\n+-------------------------------+---------+-----------+--------------------------+-----------------------------------------------------------------+\n| id                            | estRows | task      | access object            | operator info                                                   |\n+-------------------------------+---------+-----------+--------------------------+-----------------------------------------------------------------+\n| IndexLookUp_8                 | 0.09    | root      |                          |                                                                 |\n| ├─Selection_7(Build)          | 0.09    | cop[tikv] |                          | in(test.t.b, 40, 50, 60)                                        |\n| │ └─IndexRangeScan_5          | 30.00   | cop[tikv] | table:t, index:idx(a, b) | range:[10,10], [20,20], [30,30], keep order:false, stats:pseudo |\n| └─TableRowIDScan_6(Probe)     | 0.09    | cop[tikv] | table:t                  | keep order:false, stats:pseudo                                  |\n+-------------------------------+---------+-----------+--------------------------+-----------------------------------------------------------------+\n4 rows in set, 1 warning (0.00 sec)\n```\n\n```sql\nSHOW WARNINGS;\n```\n\n```sql\n+---------+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                                                                     |\n+---------+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n| Warning | 1105 | Memory capacity of 1500 bytes for 'tidb_opt_range_max_size' exceeded when building ranges. Less accurate ranges such as full range are chosen |\n+---------+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n再将优化器构造扫描范围的内存用量上限设为 100 字节。\n\n```sql\nset @@tidb_opt_range_max_size = 100;\n```\n\n```sql\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n```sql\nEXPLAIN SELECT * FROM t USE INDEX (idx) WHERE a IN (10,20,30) AND b IN (40,50,60);\n```\n\n在 100 字节的内存最大限制约束下，优化器选择了 `IndexFullScan`，并用 warning 提示用户构造精确的扫描范围所需的内存超出了 `tidb_opt_range_max_size` 的限制。\n\n```sql\n+-------------------------------+----------+-----------+--------------------------+----------------------------------------------------+\n| id                            | estRows  | task      | access object            | operator info                                      |\n+-------------------------------+----------+-----------+--------------------------+----------------------------------------------------+\n| IndexLookUp_8                 | 8000.00  | root      |                          |                                                    |\n| ├─Selection_7(Build)          | 8000.00  | cop[tikv] |                          | in(test.t.a, 10, 20, 30), in(test.t.b, 40, 50, 60) |\n| │ └─IndexFullScan_5           | 10000.00 | cop[tikv] | table:t, index:idx(a, b) | keep order:false, stats:pseudo                     |\n| └─TableRowIDScan_6(Probe)     | 8000.00  | cop[tikv] | table:t                  | keep order:false, stats:pseudo                     |\n+-------------------------------+----------+-----------+--------------------------+----------------------------------------------------+\n4 rows in set, 1 warning (0.00 sec)\n```\n\n```sql\nSHOW WARNINGS;\n```\n\n```sql\n+---------+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n| Level   | Code | Message                                                                                                                                     |\n+---------+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n| Warning | 1105 | Memory capacity of 100 bytes for 'tidb_opt_range_max_size' exceeded when building ranges. Less accurate ranges such as full range are chosen |\n+---------+------+---------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n</details>\n\n### `tidb_opt_scan_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 2147483647]`\n- 默认值：`1.5`\n- 表示升序扫描时，TiKV 在磁盘上扫描一行数据的开销。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n### `tidb_opt_seek_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 2147483647]`\n- 默认值：`20`\n- 表示 TiDB 从 TiKV 请求数据的初始开销。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n### `tidb_opt_skew_distinct_agg` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n> **注意：**\n>\n> 开启该变量带来的查询性能优化仅对 TiFlash 有效。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置优化器是否将带有 `DISTINCT` 的聚合函数（例如 `SELECT b, count(DISTINCT a) FROM t GROUP BY b`）改写为两层聚合函数（例如 `SELECT b, count(a) FROM (SELECT b, a FROM t GROUP BY b, a) t GROUP BY b`）。当聚合列有严重的数据倾斜，且 `DISTINCT` 列有很多不同的值时，这种改写能够避免查询执行过程中的数据倾斜，从而提升查询性能。\n\n### `tidb_opt_three_stage_distinct_agg` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量用于控制在 MPP 模式下是否将 `COUNT(DISTINCT)` 聚合改写为三阶段分布式执行的聚合。\n- 该变量目前仅对只有一个 `COUNT(DISTINCT)` 的聚合生效。\n\n### `tidb_opt_tiflash_concurrency_factor`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：浮点数\n- 范围：`[0, 2147483647]`\n- 默认值：`24.0`\n- 表示 TiFlash 计算的并发数。该变量是[代价模型](/cost-model.md)内部使用的变量，**不建议**修改该变量的值。\n\n## `tidb_opt_use_invisible_indexes` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置是否允许优化器选择[不可见索引 (Invisible Index)](/sql-statements/sql-statement-create-index.md#不可见索引)。默认情况下，不可见索引由 DML 语句维护，不会被查询优化器使用。当修改变量为 `ON` 时，对该会话中的查询，优化器可以选择不可见索引进行查询优化。\n\n### `tidb_opt_write_row_id`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置是否允许 `INSERT`、`REPLACE` 和 `UPDATE` 操作 `_tidb_rowid` 列，默认是不允许操作。该选项仅用于 TiDB 工具导数据时使用。\n\n### `tidb_optimizer_selectivity_level`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 2147483647]`\n- 控制优化器估算逻辑的更迭。更改该变量值后，优化器的估算逻辑会产生较大的改变。目前该变量的有效值只有 `0`，不建议设为其它值。\n\n### `tidb_partition_prune_mode` <span class=\"version-mark\">从 v5.1 版本开始引入</span>\n\n> **警告：**\n>\n> 从 v8.5.0 开始，将该变量设置为 `static` 或 `static-only` 时会产生警告。该变量将在未来版本中废弃。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：枚举型\n- 默认值：`dynamic`\n- 可选值：`static`、`dynamic`、`static-only`、`dynamic-only`\n- 这个变量用来设置是否开启分区表动态裁剪模式。默认值为 `dynamic`。但是注意，`dynamic` 动态裁剪模式仅在表级别汇总统计信息（即分区表的全局统计信息）收集完成的情况下生效。如果在全局统计信息未收集完成的情况下启用 `dynamic` 动态裁剪模式，TiDB 仍然会维持 `static` 静态裁剪的状态，直到全局统计信息收集完成。关于全局统计信息的更多信息，请参考[动态裁剪模式下的分区表统计信息](/statistics.md#收集动态裁剪模式下的分区表统计信息)。关于动态裁剪模式的更多信息，请参考[分区表动态裁剪模式](/partitioned-table.md#动态裁剪模式)。\n\n### `tidb_persist_analyze_options` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否开启 [ANALYZE 配置持久化](/statistics.md#持久化-analyze-配置)特性。\n\n### `tidb_pessimistic_txn_fair_locking` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 是否对悲观锁启用加强的悲观锁唤醒模型。该模型可严格控制悲观锁单点冲突场景下事务的唤醒顺序，避免无效唤醒，大大降低原有唤醒机制中的随机性对事务延迟带来的不确定性。如果业务场景中遇到了单点悲观锁冲突频繁的情况（如高频更新同一行数据等），并进而引起语句重试频繁、尾延迟高，甚至偶尔发生 `pessimistic lock retry limit reached` 错误，可以尝试开启该变量来解决问题。\n- 对于从 v7.0.0 以前的版本升级到 v7.0.0 或更新版本的 TiDB 集群，该选项默认关闭。\n\n> **注意：**\n>\n> - 视具体业务场景的不同，启用该选项可能对存在频繁锁冲突的事务造成一定程度的吞吐下降（平均延迟上升）。\n> - 该选项目前仅对需要上锁单个 key 的语句有效。如果一个语句需要对多行同时上锁，则该选项不会对此类语句生效。\n> - 该功能从 v6.6.0 版本引入。在 v6.6.0 版本中，该功能由变量 [`tidb_pessimistic_txn_aggressive_locking`](https://docs.pingcap.com/zh/tidb/v6.6/system-variables#tidb_pessimistic_txn_aggressive_locking-%E4%BB%8E-v660-%E7%89%88%E6%9C%AC%E5%BC%80%E5%A7%8B%E5%BC%95%E5%85%A5) 控制，默认关闭。\n\n### `tidb_placement_mode` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`STRICT`\n- 可选值：`STRICT`，`IGNORE`\n- 该变量用于控制 DDL 语句是否忽略 [Placement Rules in SQL](/placement-rules-in-sql.md) 指定的放置规则。变量值为 `IGNORE` 时将忽略所有放置规则选项。\n- 该变量可由逻辑转储或逻辑恢复工具使用，确保即使绑定了不合适的放置规则，也始终可以成功创建表。这类似于 mysqldump 将 `SET FOREIGN_KEY_CHECKS=0;` 写入每个转储文件的开头部分。\n\n### `tidb_plan_cache_invalidation_on_fresh_stats` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量用于控制当某张表上的统计信息更新后，与该表相关的 Plan Cache 是否自动失效。\n- 开启此变量有助于 Plan Cache 更有效地利用可用的统计信息生成执行计划，例如：\n    - 有时 Plan Cache 会在统计信息尚不可用时生成执行计划。开启此变量后，Plan Cache 会在统计信息可用时重新生成执行计划。\n    - 当表上数据分布发生变化时，之前的最优执行计划可能对于现在不再是最优的。开启此变量后，Plan Cache 会在重新收集统计信息后重新生成执行计划。\n- 对于从 v7.1.0 以前的版本升级到 v7.1.0 及以上版本的 TiDB 集群，该选项默认关闭 (`OFF`)。\n\n### `tidb_plan_cache_max_plan_size` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`2097152`（即 2 MiB）\n- 取值范围：`[0, 9223372036854775807]`，单位为 Byte。支持带单位的内存格式 \"KiB|MiB|GiB|TiB\"。`0` 表示表示不设限制。\n- 这个变量用来控制可以缓存的 Prepare 或非 Prepare 语句执行计划的最大大小。超过该值的执行计划将不会被缓存到 Plan Cache 中。详情请参考 [Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md#prepared-plan-cache-的内存管理)和[非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md#使用方法)。\n\n### `tidb_pprof_sql_cpu` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 1]`\n- 这个变量用来控制是否在 profile 输出中标记出对应的 SQL 语句，用于定位和排查性能问题。\n\n### `tidb_opt_prefix_index_single_scan` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制 TiDB 优化器是否将某些过滤条件下推到前缀索引，尽量避免不必要的回表，从而提高查询性能。\n- 将该变量设置为 `ON` 时，会将过滤条件下推到前缀索引。此时，假设一张表中 `col` 列是索引前缀列，查询语句中的 `col is null` 或者 `col is not null` 条件会被归为索引上的过滤条件，而不是回表时的过滤条件，从而避免不必要的回表。\n\n<details>\n<summary>该变量的使用示例</summary>\n\n创建一张带前缀索引的表：\n\n```sql\nCREATE TABLE t (a INT, b VARCHAR(10), c INT, INDEX idx_a_b(a, b(5)));\n```\n\n此时关闭 `tidb_opt_prefix_index_single_scan`：\n\n```sql\nSET tidb_opt_prefix_index_single_scan = 'OFF';\n```\n\n对于以下查询，执行计划使用了前缀索引 `idx_a_b` 但需要回表（出现了 `IndexLookUp` 算子）。\n\n```sql\nEXPLAIN FORMAT='brief' SELECT COUNT(1) FROM t WHERE a = 1 AND b IS NOT NULL;\n+-------------------------------+---------+-----------+------------------------------+-------------------------------------------------------+\n| id                            | estRows | task      | access object                | operator info                                         |\n+-------------------------------+---------+-----------+------------------------------+-------------------------------------------------------+\n| HashAgg                       | 1.00    | root      |                              | funcs:count(Column#8)->Column#5                       |\n| └─IndexLookUp                 | 1.00    | root      |                              |                                                       |\n|   ├─IndexRangeScan(Build)     | 99.90   | cop[tikv] | table:t, index:idx_a_b(a, b) | range:[1 -inf,1 +inf], keep order:false, stats:pseudo |\n|   └─HashAgg(Probe)            | 1.00    | cop[tikv] |                              | funcs:count(1)->Column#8                              |\n|     └─Selection               | 99.90   | cop[tikv] |                              | not(isnull(test.t.b))                                 |\n|       └─TableRowIDScan        | 99.90   | cop[tikv] | table:t                      | keep order:false, stats:pseudo                        |\n+-------------------------------+---------+-----------+------------------------------+-------------------------------------------------------+\n6 rows in set (0.00 sec)\n```\n\n此时打开 `tidb_opt_prefix_index_single_scan`：\n\n```sql\nSET tidb_opt_prefix_index_single_scan = 'ON';\n```\n\n开启该变量后，对于以下查询，执行计划使用了前缀索引 `idx_a_b` 且不需要回表。\n\n```sql\nEXPLAIN FORMAT='brief' SELECT COUNT(1) FROM t WHERE a = 1 AND b IS NOT NULL;\n+--------------------------+---------+-----------+------------------------------+-------------------------------------------------------+\n| id                       | estRows | task      | access object                | operator info                                         |\n+--------------------------+---------+-----------+------------------------------+-------------------------------------------------------+\n| StreamAgg                | 1.00    | root      |                              | funcs:count(Column#7)->Column#5                       |\n| └─IndexReader            | 1.00    | root      |                              | index:StreamAgg                                       |\n|   └─StreamAgg            | 1.00    | cop[tikv] |                              | funcs:count(1)->Column#7                              |\n|     └─IndexRangeScan     | 99.90   | cop[tikv] | table:t, index:idx_a_b(a, b) | range:[1 -inf,1 +inf], keep order:false, stats:pseudo |\n+--------------------------+---------+-----------+------------------------------+-------------------------------------------------------+\n4 rows in set (0.00 sec)\n```\n\n</details>\n\n### `tidb_prefer_broadcast_join_by_exchange_data_size` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于设定 TiDB 选择 [MPP Hash Join 算法](/tiflash/use-tiflash-mpp-mode.md#mpp-模式的算法支持)时，是否使用最小网络交换的数据量策略。开启该变量后，TiDB 会估算 Broadcast Hash Join 和 Shuffled Hash Join 两种算法所需进行网络交换的数据量，并选择网络交换数据量较小的算法。\n- 该功能开启后 [`tidb_broadcast_join_threshold_count`](#tidb_broadcast_join_threshold_count-从-v50-版本开始引入) 和 [`tidb_broadcast_join_threshold_size`](#tidb_broadcast_join_threshold_size-从-v50-版本开始引入) 将不再生效。\n\n### `tidb_prepared_plan_cache_memory_guard_ratio` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：浮点数\n- 默认值：`0.1`\n- 范围：`[0, 1]`\n- 这个变量用来控制 Prepared Plan Cache 触发内存保护机制的阈值，具体可见 [Prepared Plan Cache 的内存管理](/sql-prepared-plan-cache.md#prepared-plan-cache-的内存管理)。\n- 在 v6.1.0 之前这个开关通过 TiDB 配置文件 (`prepared-plan-cache.memory-guard-ratio`) 进行配置，升级到 v6.1.0 时会自动继承原有设置。\n\n### `tidb_prepared_plan_cache_size` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n> **警告：**\n>\n> 从 v7.1.0 开始，该变量被废弃。请使用 [`tidb_session_plan_cache_size`](#tidb_session_plan_cache_size-从-v710-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`100`\n- 范围：`[1, 100000]`\n- 这个变量用来控制单个 `SESSION` 的 Prepared Plan Cache 最多能够缓存的计划数量，具体可见 [Prepared Plan Cache 的内存管理](/sql-prepared-plan-cache.md#prepared-plan-cache-的内存管理)。\n- 在 v6.1.0 之前这个开关通过 TiDB 配置文件 (`prepared-plan-cache.capacity`) 进行配置，升级到 v6.1.0 时会自动继承原有设置。\n\n### `tidb_pre_split_regions` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 15]`\n- 该变量用于设置新建表默认的行分裂分片数。当设置了该变量为非 0 值后，执行 `CREATE TABLE` 语句时，TiDB 会为允许使用 `PRE_SPLIT_REGIONS` 的表（例如 `NONCLUSTERED` 表）自动设定该属性。详见 [`PRE_SPLIT_REGIONS`](/sql-statements/sql-statement-split-region.md#pre_split_regions)。该变量通常与 [`tidb_shard_row_id_bits`](/system-variables.md#tidb_shard_row_id_bits-从-v840-版本开始引入) 配合使用，用于为新建表进行分片以及 Region 预分裂。\n\n### `tidb_projection_concurrency`\n\n> **警告：**\n>\n> 从 v5.0 版本开始，该变量被废弃。请使用 [`tidb_executor_concurrency`](#tidb_executor_concurrency-从-v50-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[-1, 256]`\n- 单位：线程\n- 这个变量用来设置 `Projection` 算子的并发度。\n- 默认值 `-1` 表示使用 `tidb_executor_concurrency` 的值。\n\n### `tidb_query_log_max_len`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`4096` (4 KiB)\n- 范围：`[0, 1073741824]`\n- 单位：字节\n- 该变量控制 SQL 语句输出的最大长度。当一条 SQL 语句的输出长度大于 `tidb_query_log_max_len` 时，输出将会被截断。\n- 在 v6.1.0 之前这个开关也可以通过 TiDB 配置文件 (`log.query-log-max-len`) 进行配置，升级到 v6.1.0 后仅可通过系统变量配置。\n\n### `tidb_rc_read_check_ts` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n> **警告：**\n>\n> - 该特性与 [`replica-read`](#tidb_replica_read-从-v40-版本开始引入) 尚不兼容，开启 `tidb_rc_read_check_ts` 的读请求无法使用 [`replica-read`](#tidb_replica_read-从-v40-版本开始引入)，请勿同时开启两项特性。\n> - 如果客户端使用游标操作，建议不开启 `tidb_rc_read_check_ts` 这一特性，避免前一批返回数据已经被客户端使用而语句最终会报错的情况。\n> - 自 v7.0.0 版本开始，该变量对于使用 prepared statement 协议下 cursor fetch read 游标模式不再生效。\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量用于优化时间戳的获取，适用于悲观事务 `READ-COMMITTED` 隔离级别下读写冲突较少的场景，开启此变量可以避免获取全局 timestamp 带来的延迟和开销，并优化事务内读语句延迟。\n- 如果读写冲突较为严重，开启此功能会增加额外开销和延迟，造成性能回退。更详细的说明，请参考[读已提交隔离级别 (Read Committed) 文档](/transaction-isolation-levels.md#读已提交隔离级别-read-committed)。\n\n### `tidb_rc_write_check_ts` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n> **警告：**\n>\n> 该特性与 [`replica-read`](#tidb_replica_read-从-v40-版本开始引入) 尚不兼容。开启本变量后，客户端发送的所有请求都将无法使用 `replica-read`，因此请勿同时开启 `tidb_rc_write_check_ts` 和 `replica-read`。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量用于优化时间戳的获取，适用于悲观事务 `READ-COMMITTED` 隔离级别下点写冲突较少的场景。开启此变量可以避免点写语句获取全局时间戳带来的延迟和开销。目前该变量适用的点写语句包括 `UPDATE`、`DELETE`、`SELECT ...... FOR UPDATE` 三种类型。点写语句是指将主键或者唯一键作为过滤条件且最终执行算子包含 `POINT-GET` 的写语句。\n- 如果点写冲突较为严重，开启此变量会增加额外开销和延迟，造成性能回退。更详细的说明，请参考[读已提交隔离级别 (Read Committed) 文档](/transaction-isolation-levels.md#读已提交隔离级别-read-committed)。\n\n### `tidb_read_consistency` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是（注意当存在[非事务 DML 语句](/non-transactional-dml.md)时，使用 hint 修改该变量的值可能不生效）\n- 类型：字符串\n- 默认值：`strict`\n- 此变量用于控制自动提交的读语句的读一致性。\n- 如果将变量值设置为 `weak`，则直接跳过读语句遇到的锁，读的执行可能会更快，这就是弱一致性读模式。但在该模式下，事务语义（例如原子性）和分布式一致性（线性一致性）并不能得到保证。\n- 如果用户场景中需要快速返回自动提交的读语句，并且可接受弱一致性的读取结果，则可以使用弱一致性读取模式。\n\n### `tidb_read_staleness` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围 `[-2147483648, 0]`\n- 这个变量用于设置当前会话允许读取的历史数据范围。设置后，TiDB 会从参数允许的范围内选出一个尽可能新的时间戳，并影响后继的所有读操作。比如，如果该变量的值设置为 `-5`，TiDB 会在 5 秒时间范围内，保证 TiKV 拥有对应历史版本数据的情况下，选择尽可能新的一个时间戳。\n\n### `tidb_record_plan_in_slow_log`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否在 slow log 里包含慢查询的执行计划。\n\n### `tidb_redact_log`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`OFF`\n- 可选值：`OFF`、`ON`、`MARKER`\n- 这个变量用于控制在记录 TiDB 日志和慢日志时，是否将 SQL 中的用户信息遮蔽。\n- 默认值为 `OFF`，即对用户输入的信息不做任何处理。\n- 将该变量设置为 `ON` 后，用户输入的信息被遮蔽。假设执行的 SQL 为 `INSERT INTO t VALUES (1,2)`，则日志中记录的 SQL 语句为 `INSERT INTO t VALUES (?,?)`。\n- 将该变量设置为 `MARKER` 后，用户输入的信息被标记符号 `‹ ›` 包裹。假设执行的 SQL 为 `INSERT INTO t VALUES (1,2)`，则日志中记录的 SQL 语句为 `INSERT INTO t VALUES (‹1›,‹2›)`。用户数据中的 `‹` 会转义成 `‹‹`，`›` 会转义成 `››`。基于标记后的日志，你可以在展示日志时决定是否对被标记信息进行脱敏处理。\n\n### `tidb_regard_null_as_point` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用来控制优化器是否可以将包含 null 的等值条件作为前缀条件来访问索引。\n- 该变量默认开启。开启后，该变量可以使优化器减少需要访问的索引数据量，从而提高查询的执行速度。例如，在有多列索引 `index(a, b)` 且查询条件为 `a<=>null and b=1` 的情况下，优化器可以同时使用查询条件中的 `a<=>null` 和 `b=1` 进行索引访问。如果关闭该变量，因为 `a<=>null and b=1` 包含 null 的等值条件，优化器不会使用 `b=1` 进行索引访问。\n\n### `tidb_remove_orderby_in_subquery` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：在 v7.2.0 之前版本中为 `OFF`，在 v7.2.0 及之后版本中为 `ON`。\n- 指定是否在子查询中移除 `ORDER BY` 子句。\n- 在 ISO/IEC SQL 标准中，`ORDER BY` 主要用于对顶层查询结果进行排序。对于子查询中的 `ORDER BY`，SQL 标准并不要求子查询结果按 `ORDER BY` 排序。\n- 如果需要对子查询结果排序，通常可以在外层查询中处理，例如使用窗口函数或在外层查询中再次使用 `ORDER BY`。这样做可以确保最终结果集的顺序。\n\n### `tidb_replica_read` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：枚举型\n- 默认值：`leader`\n- 可选值：`leader`、`follower`、`leader-and-follower`、`prefer-leader`、`closest-replicas`、`closest-adaptive` 和 `learner`。其中，`learner` 从 v6.6.0 开始引入。\n- 这个变量用于控制 TiDB 的 Follower Read 功能的行为。\n- 关于使用方式与实现原理，见 [Follower Read](/follower-read.md)。\n\n### `tidb_request_source_type` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：`\"\"`\n- 可选值：`\"ddl\"`、`\"stats\"`、`\"br\"`、`\"lightning\"`、`\"background\"`\n- 显式指定当前会话的任务类型，用于[资源管控](/tidb-resource-control.md)识别并控制。如 `SET @@tidb_request_source_type = \"background\"`。\n\n### `tidb_resource_control_strict_mode` <span class=\"version-mark\">从 v8.2.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 该变量是 [`SET RESOURCE GROUP`](/sql-statements/sql-statement-set-resource-group.md) 和优化器 [`RESOURCE_GROUP()`](/optimizer-hints.md#resource_groupresource_group_name) Hint 权限控制的开关。当此变量设置为 `ON` 时，你需要有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 或者 `RESOURCE_GROUP_USER` 权限才能使用这两种方式修改当前会话或当前语句绑定的资源组；当此变量设置为 `OFF` 时，则无需上述权限，其行为与不支持此变量的 TiDB 之前版本相同。\n- 从旧版本升级到 v8.2.0 及之后版本时，该功能默认关闭，此时该变量默认值为 `OFF`。\n\n### `tidb_retry_limit`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`10`\n- 范围：`[-1, 9223372036854775807]`\n- 这个变量用来设置乐观事务的最大重试次数。一个事务执行中遇到可重试的错误（例如事务冲突、事务提交过慢或表结构变更）时，会根据该变量的设置进行重试。注意当 `tidb_retry_limit = 0` 时，也会禁用自动重试。该变量仅适用于乐观事务，不适用于悲观事务。\n\n### `tidb_row_format_version`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`2`\n- 范围：`[1, 2]`\n- 控制新保存数据的表数据格式版本。TiDB v4.0 中默认使用版本号为 2 的[新表数据格式](https://github.com/pingcap/tidb/blob/master/docs/design/2018-07-19-row-format.md)保存新数据。\n\n- 但如果从 4.0.0 之前的版本升级到 4.0.0，不会改变表数据格式版本，TiDB 会继续使用版本为 1 的旧格式写入表中，即**只有新创建的集群才会默认使用新表数据格式**。\n\n- 需要注意的是修改该变量不会对已保存的老数据产生影响，只会对修改变量后的新写入数据使用对应版本格式保存。\n\n### `tidb_runtime_filter_mode` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：枚举型\n- 默认值：`OFF`\n- 可选值：`OFF`，`LOCAL`\n- 控制 Runtime Filter 的模式，即**生成 Filter 算子**和**接收 Filter 算子**之间的关系。当前可设置为两种模式：`OFF`、`LOCAL`。`OFF` 代表关闭 Runtime Filter，`LOCAL` 代表开启 `LOCAL` 模式的 Runtime Filter。详细说明见 [Runtime Filter Mode](/runtime-filter.md#runtime-filter-mode)。\n\n### `tidb_runtime_filter_type` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：枚举型\n- 默认值：`IN`\n- 可选值：`IN`\n- 控制 Runtime Filter 的类型，即生成的 Filter 算子使用的谓词类型。当前仅支持 `IN`，所以无需更改此设置。详细说明见 [Runtime Filter Type](/runtime-filter.md#runtime-filter-type)。\n\n### `tidb_scatter_region`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`\"\"`\n- 可选值：`\"\"`，`table`，`global`\n- 如果在建表时设置了 `SHARD_ROW_ID_BITS` 和 `PRE_SPLIT_REGIONS` 参数，则系统会在建表成功后自动将表均匀切分为指定数量的 Region。该变量用于控制这些分裂后的 Region 的打散策略。TiDB 将依据所选的打散策略对 Region 进行处理。需要特别说明的是，由于建表操作会等待 Region 打散完成后才返回成功状态，因此启用该变量可能会显著增加建表语句的执行时间，相较于未启用该变量的情况，执行时间可能会延长数倍。可选值描述如下：\n    - `\"\"`：默认值，表示建表后不打散表的 Region。\n    - `table`：表示在建表时，预分裂多个 Region 的场景下，会按表的粒度对这些表的 Region 进行打散。但是如果在建表时没有设置上述属性，需要快速创建大量表的场景，会导致这些表的 Region 集中在其中几个 TiKV 节点上，造成 Region 分布不均匀。\n    - `global`：表示 TiDB 会根据整个集群的数据分布情况来打散新建表的 Region。特别是快速创建大量表的时候，使用 `global` 可以有效避免 Region 过度集中在少数几个 TiKV 节点上，确保 Region 在集群中分布均匀。\n\n### `tidb_schema_cache_size` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`536870912` (512 MiB)\n- 取值范围：`0` 或 `[67108864, 9223372036854775807]`\n- 对 TiDB v8.4.0 以前的版本，该变量默认值为 `0`。\n- 从 TiDB v8.4.0 开始，默认值为 `536870912`（即 512 MiB）。从低版本升级到 v8.4.0 及更高版本后仍然会使用旧值。\n- 这个变量用来控制 TiDB schema 信息缓存的大小。单位为 byte。设置为 `0` 表示不打开缓存限制功能。如需开启，则需要将该变量的值设置在 `[67108864, 9223372036854775807]` 范围内，TiDB 将使用该变量的值做为可用的内存上限，并使用 Least Recently Used (LRU) 算法缓存所需的表，有效降低 schema 信息占用的内存。\n\n### `tidb_schema_version_cache_limit` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`16`\n- 取值范围：`[2, 255]`\n- 该变量用于限制 TiDB 实例可以缓存多少个历史版本的表结构信息。默认值为 `16`，即默认缓存 16 个历史版本的表结构信息。\n- 一般不需要修改该变量。当使用 [Stale Read](/stale-read.md) 功能且 DDL 执行非常频繁时，会导致表结构信息的版本号变更非常频繁，进而导致 Stale Read 在获取 Snapshot 的表结构信息时，可能会因为未命中表结构信息的缓存而需要消耗大量时间重新构建该信息。此时可以适当调大 `tidb_schema_version_cache_limit` 的值（例如 `32` ）来避免表结构信息的缓存不命中的问题。\n- 修改该变量会使 TiDB 的内存占用轻微上升。使用时请注意 TiDB 的内存占用，避免出现 OOM 问题。\n\n### `tidb_server_memory_limit` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`80%`\n- 取值范围：\n    - 你可以将该变量值设为百分比格式，表示内存用量占总内存的百分比，取值范围为 `[1%, 99%]`。\n    - 你还可以将变量值设为内存大小，取值范围为 `0` 以及 `[536870912, 9223372036854775807]`，单位为 Byte。支持带单位的内存格式 \"KiB|MiB|GiB|TiB\"。`0` 值表示不设内存限制。\n    - 当设置的内存值小于 512 MiB 且不为 0 时，TiDB 将会使用 512 MiB 作为替代。\n- 该变量指定 TiDB 实例的内存限制。TiDB 会在内存用量达到该限制时，对当前内存用量最高的 SQL 语句进行取消 (Cancel) 操作。在该 SQL 语句被成功 Cancel 掉后，TiDB 会尝试调用 Golang GC 立刻回收内存，以最快速度缓解内存压力。\n- 只有内存使用大于 `tidb_server_memory_limit_sess_min_size` 的 SQL 语句会被选定为最优先被 Cancel 的 SQL 语句。\n- 目前 TiDB 一次只能 Cancel 一条 SQL 语句。如果 TiDB 完全 Cancel 掉一条 SQL 语句并回收资源后，内存使用仍然大于该变量所设限制，TiDB 会开始下一次 Cancel 操作。\n\n### `tidb_server_memory_limit_gc_trigger` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`70%`\n- 取值范围：`[50%, 99%]`\n- TiDB 尝试触发 GC 的阈值。当 TiDB 的内存使用达到 `tidb_server_memory_limit` 值 \\* `tidb_server_memory_limit_gc_trigger` 值时，则会主动触发一次 Golang GC。在一分钟之内只会主动触发一次 GC。\n\n### `tidb_server_memory_limit_sess_min_size` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`134217728`（即 128 MiB）\n- 取值范围：`[128, 9223372036854775807]`，单位为 Byte。支持带单位的内存格式 \"KiB|MiB|GiB|TiB\"。\n- 开启内存限制后，TiDB 会终止当前实例上内存用量最高的 SQL 语句。本变量指定此情况下 SQL 语句被终止的最小内存用量。如果 TiDB 实例的内存超限是由许多内存使用量不明显的会话导致的，可以适当调小该变量值，使得更多会话成为 Cancel 的对象。\n\n### `tidb_service_scope` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：\"\"\n- 可选值：长度小于或等于 64 的字符串，可用合法字符包括数字 `0-9`、字母 `a-zA-Z`、下划线 `_` 和连字符 `-`\n- 该变量是一个实例级别的变量，用于控制 [TiDB 分布式执行框架](/tidb-distributed-execution-framework.md) 下各 TiDB 节点的服务范围。分布式执行框架会根据该变量的值决定将分布式任务调度到哪些 TiDB 节点上执行，具体规则请参考[任务调度](/tidb-distributed-execution-framework.md#任务调度)。\n\n### `tidb_session_alias` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：SESSION\n- 是否持久化到集群：否\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：字符串\n- 默认值：\"\"\n- 用来自定义当前会话相关日志中 `session_alias` 列的值，方便故障定位时识别该会话。此设置会对语句执行过程中涉及的多个节点的日志生效（包括 TiKV）。此变量限制长度最大为 64 个字符，超出的部分将会被自动截断。如果变量值的末尾存在空格，也会被自动去除。\n\n### `tidb_session_plan_cache_size` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`100`\n- 范围：`[1, 100000]`\n- 这个变量用来控制 Plan Cache 最多能够缓存的计划数量。其中，[Prepare 语句执行计划缓存](/sql-prepared-plan-cache.md)和[非 Prepare 语句执行计划缓存](/sql-non-prepared-plan-cache.md)共用一个缓存。\n- 从旧版本升级到 v7.1.0 及之后的版本，`tidb_session_plan_cache_size` 的值与 [`tidb_prepared_plan_cache_size`](#tidb_prepared_plan_cache_size-从-v610-版本开始引入) 保持一致。\n\n### `tidb_shard_allocate_step` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`9223372036854775807`\n- 范围：`[1, 9223372036854775807]`\n- 该变量设置为 [`AUTO_RANDOM`](/auto-random.md) 或 [`SHARD_ROW_ID_BITS`](/shard-row-id-bits.md) 属性列分配的最大连续 ID 数。通常，`AUTO_RANDOM` ID 或带有 `SHARD_ROW_ID_BITS` 属性的行 ID 在一个事务中是增量和连续的。你可以使用该变量来解决大事务场景下的热点问题。\n\n### `tidb_shard_row_id_bits` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 15]`\n- 该变量用于设置新建表默认的行 ID 的分片数。当设置了该变量为非 0 值后，执行 `CREATE TABLE` 语句时，TiDB 会为允许使用 `SHARD_ROW_ID_BITS` 的表（例如 `NONCLUSTERED` 表）自动设定该属性。详见 [`SHARD_ROW_ID_BITS`](/shard-row-id-bits.md)。\n\n### `tidb_simplified_metrics`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 该变量开启后，TiDB 将不会收集或记录 Grafana 面板未使用到的 metrics。\n\n### `tidb_skip_ascii_check` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置是否校验 ASCII 字符的合法性。\n- 校验 ASCII 字符会损耗些许性能。当你确认输入的字符串为有效的 ASCII 字符时，可以将其设置为 `ON`。\n\n### `tidb_skip_isolation_level_check`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 开启这个开关之后，如果对 `tx_isolation` 赋值一个 TiDB 不支持的隔离级别，不会报错，有助于兼容其他设置了（但不依赖于）不同隔离级别的应用。\n\n```sql\ntidb> set tx_isolation='serializable';\nERROR 8048 (HY000): The isolation level 'serializable' is not supported. Set tidb_skip_isolation_level_check=1 to skip this error\ntidb> set tidb_skip_isolation_level_check=1;\nQuery OK, 0 rows affected (0.00 sec)\n\ntidb> set tx_isolation='serializable';\nQuery OK, 0 rows affected, 1 warning (0.00 sec)\n```\n\n### `tidb_skip_missing_partition_stats` <span class=\"version-mark\">从 v7.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 分区表在开启[动态裁剪模式](/partitioned-table.md#动态裁剪模式)时，TiDB 会汇总各个分区的统计信息生成全局统计信息。这个变量用于控制当分区统计信息缺失时生成全局统计信息的行为。\n\n    - 当开启该变量时，TiDB 生成全局统计信息时会跳过缺失的分区统计信息，不影响全局统计信息的生成。\n    - 当关闭该变量时，遇到缺失的分区统计信息，TiDB 会停止生成全局统计信息。\n\n### `tidb_skip_utf8_check`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来设置是否校验 UTF-8 字符的合法性。\n- 校验 UTF-8 字符会损耗些许性能。当你确认输入的字符串为有效的 UTF-8 字符时，可以将其设置为 `ON`。\n\n> **注意：**\n>\n> 跳过字符检查可能会使 TiDB 检测不到应用写入的非法 UTF-8 字符，进一步导致执行 `ANALYZE` 时解码错误，以及引入其他未知的编码问题。如果应用不能保证写入字符串的合法性，不建议跳过该检查。\n\n### `tidb_slow_log_threshold`\n\n- 作用域：GLOBAL\n- 是否持久化到集群：否，仅作用于当前连接的 TiDB 实例\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`300`\n- 类型：整数型\n- 范围：`[-1, 9223372036854775807]`\n- 单位：毫秒\n- 输出慢日志的耗时阈值，默认为 300 ms。如果查询耗时大于这个值，会视作一个慢查询，并记录到慢查询日志。注意，当日志的输出级别 [`log.level`](/tidb-configuration-file.md#level) 是 `\"debug\"` 时，所有查询都会记录到慢日志，不受该变量的限制。\n\n### `tidb_slow_query_file`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 查询 `INFORMATION_SCHEMA.SLOW_QUERY` 只会解析配置文件中 `slow-query-file` 设置的慢日志文件名，默认是 \"tidb-slow.log\"。但如果想要解析其他的日志文件，可以通过设置 session 变量 `tidb_slow_query_file` 为具体的文件路径，然后查询 `INFORMATION_SCHEMA.SLOW_QUERY` 就会按照设置的路径去解析慢日志文件。更多详情可以参考 [SLOW_QUERY 文档](/identify-slow-queries.md)。\n\n### `tidb_snapshot`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 这个变量用来设置当前会话期待读取的历史数据所处时刻。比如当设置为 `\"2017-11-11 20:20:20\"` 时或者一个 TSO 数字 \"400036290571534337\"，当前会话将能读取到该时刻的数据。\n\n### `tidb_source_id` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值： `1`\n- 范围：`[1, 15]`\n- 这个变量用来设置在[双向复制](/ticdc/ticdc-bidirectional-replication.md)系统内不同集群的 ID。\n\n### `tidb_stats_cache_mem_quota` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 单位：字节\n- 默认值：`0`，表示自动设置统计信息缓存的内存使用上限为总内存的一半。\n- 范围：`[0, 1099511627776]`\n- 这个变量用于控制 TiDB 统计信息缓存的内存使用上限。\n\n### `tidb_stats_load_sync_wait` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`100`\n- 单位：毫秒\n- 范围：`[0, 2147483647]`\n- 这个变量用于控制是否开启统计信息的同步加载模式（为 `0` 代表不开启，即为异步加载模式），以及开启的情况下，SQL 执行同步加载完整统计信息等待多久后会超时。更多信息，请参考[统计信息的加载](/statistics.md#加载统计信息)。\n\n### `tidb_stats_load_pseudo_timeout` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制统计信息同步加载超时后，SQL 是执行失败（`OFF`），还是退回使用 pseudo 的统计信息（`ON`）。\n\n### `tidb_stmt_summary_enable_persistent` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 只读变量。表示是否开启 [statement summary tables 持久化](/statement-summary-tables.md#持久化-statements-summary)。该变量的值与配置文件中 [`tidb_stmt_summary_enable_persistent`](/tidb-configuration-file.md#tidb_stmt_summary_enable_persistent-从-v660-版本开始引入) 的取值相同。\n\n### `tidb_stmt_summary_filename` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：字符串\n- 默认值：`\"tidb-statements.log\"`\n- 只读变量。表示当开启 [statement summary tables 持久化](/statement-summary-tables.md#持久化-statements-summary)后持久化数据所写入的文件。该变量的值与配置文件中 [`tidb_stmt_summary_filename`](/tidb-configuration-file.md#tidb_stmt_summary_filename-从-v660-版本开始引入) 的取值相同。\n\n### `tidb_stmt_summary_file_max_backups` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 只读变量。表示当开启 [statement summary tables 持久化](/statement-summary-tables.md#持久化-statements-summary)后持久化数据文件的最大数量限制。该变量的值与配置文件中 [`tidb_stmt_summary_file_max_backups`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_backups-从-v660-版本开始引入) 的取值相同。\n\n### `tidb_stmt_summary_file_max_days` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`3`\n- 单位：天\n- 只读变量。表示当开启 [statement summary tables 持久化](/statement-summary-tables.md#持久化-statements-summary)后持久化数据文件所保留的最大天数。该变量的值与配置文件中 [`tidb_stmt_summary_file_max_days`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_days-从-v660-版本开始引入) 的取值相同。\n\n### `tidb_stmt_summary_file_max_size` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n- 作用域：GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`64`\n- 单位：MiB\n- 只读变量。表示当开启 [statement summary tables 持久化](/statement-summary-tables.md#持久化-statements-summary)后持久化数据单个文件的大小限制。该变量的值与配置文件中 [`tidb_stmt_summary_file_max_size`](/tidb-configuration-file.md#tidb_stmt_summary_file_max_size-从-v660-版本开始引入) 的取值相同。\n\n### `tidb_stmt_summary_history_size` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`24`\n- 范围：`[0, 255]`\n- 这个变量设置了 [statement summary tables](/statement-summary-tables.md) 的历史记录容量。\n\n### `tidb_stmt_summary_internal_query` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用来控制是否在 [statement summary tables](/statement-summary-tables.md) 中包含 TiDB 内部 SQL 的信息。\n\n### `tidb_stmt_summary_max_sql_length` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`4096`\n- 范围：`[0, 2147483647]`\n- 单位：字节\n- 这个变量用来控制 [Statement Summary Tables](/statement-summary-tables.md) 和 [TiDB Dashboard](/dashboard/dashboard-intro.md) 中显示的 SQL 字符串长度。\n\n### `tidb_stmt_summary_max_stmt_count` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`3000`\n- 范围：`[1, 32767]`\n- 这个变量设置了 [statement summary tables](/statement-summary-tables.md) 在内存中保存的语句的最大数量。\n\n### `tidb_stmt_summary_refresh_interval` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1800`\n- 范围：`[1, 2147483647]`\n- 单位：秒\n- 这个变量设置了 [statement summary tables](/statement-summary-tables.md) 的刷新时间。\n\n### `tidb_store_batch_size`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`4`\n- 范围：`[0, 25000]`\n- 设置 `IndexLookUp` 算子回表时多个 Coprocessor Task 的 batch 大小。`0` 代表不使用 batch。当 `IndexLookUp` 算子的回表 Task 数量特别多，出现极长的慢查询时，可以适当调大该参数以加速查询。\n\n### `tidb_streamagg_concurrency`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 设置 `StreamAgg` 算子执行查询时的并发度。\n- **不推荐设置该变量**，修改该变量值可能会造成数据正确性问题。\n\n### `tidb_top_sql_max_meta_count` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`5000`\n- 范围：`[1, 10000]`\n- 这个变量用于控制 [Top SQL](/dashboard/top-sql.md) 每分钟最多收集 SQL 语句类型的数量。\n\n### `tidb_top_sql_max_time_series_count` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`100`\n- 范围：`[1, 5000]`\n- 这个变量用于控制 [Top SQL](/dashboard/top-sql.md) 每分钟保留消耗负载最大的前多少条 SQL（即 Top N) 的数据。\n\n> **注意：**\n>\n> TiDB Dashboard 中的 Top SQL 页面目前只显示消耗负载最多的 5 类 SQL 查询，这与 `tidb_top_sql_max_time_series_count` 的配置无关。\n\n### `tidb_store_limit` <span class=\"version-mark\">从 v3.0.4 和 v4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 9223372036854775807]`\n- 这个变量用于限制 TiDB 同时向 TiKV 发送的请求的最大数量，0 表示没有限制。\n\n### `tidb_super_read_only` <span class=\"version-mark\">从 v5.3.1 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值: `OFF`。\n- `tidb_super_read_only` 用于实现对 MySQL 变量 `super_read_only` 的替代。然而，由于 TiDB 是一个分布式数据库，开启 `tidb_super_read_only` 后数据库各个 TiDB 服务器进入只读模式的时刻不是强一致的，而是最终一致的。\n- 拥有 `SUPER` 或 `SYSTEM_VARIABLES_ADMIN` 权限的用户可以修改该变量。\n- 该变量可以控制整个集群的只读状态。开启后（即该值为 `ON`），整个集群中的 TiDB 服务器都将进入只读状态，只有 `SELECT`、`USE`、`SHOW` 等不会修改数据的语句才能被执行，其他如 `INSERT`、`UPDATE` 等语句会被拒绝执行。\n- 该变量开启只读模式只保证整个集群最终进入只读模式，当变量修改状态还没被同步到其他 TiDB 服务器时，尚未同步的 TiDB 仍然停留在非只读模式。\n- 在执行 SQL 语句之前，TiDB 会检查集群的只读标志。从 v6.2.0 起，在提交 SQL 语句之前，TiDB 也会检查该标志，从而防止在服务器被置于只读模式后某些长期运行的 [auto commit](/transaction-overview.md#自动提交) 语句可能修改数据的情况。\n- 在变量开启时，对于尚未提交的事务：\n    - 如果有尚未提交的只读事务，可正常提交该事务。\n    - 如果尚未提交的事务为非只读事务，在事务内执行写入的 SQL 语句会被拒绝。\n    - 如果尚未提交的事务已经有数据改动，其提交也会被拒绝。\n- 当集群开启只读模式后，所有用户（包括 `SUPER` 用户）都无法执行可能写入数据的 SQL 语句，除非该用户被显式地授予了 `RESTRICTED_REPLICA_WRITER_ADMIN` 权限。\n- 当系统变量 [`tidb_restricted_read_only`](#tidb_restricted_read_only-从-v520-版本开始引入) 为 `ON` 时，`tidb_super_read_only` 的值会受到 [`tidb_restricted_read_only`](#tidb_restricted_read_only-从-v520-版本开始引入) 的影响。详情请参见[`tidb_restricted_read_only`](#tidb_restricted_read_only-从-v520-版本开始引入) 中的描述。\n\n### `tidb_sysdate_is_now` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`OFF`\n- 这个变量用于控制 `SYSDATE` 函数能否替换为 `NOW` 函数，其效果与 MYSQL 中的 [`sysdate-is-now`](https://dev.mysql.com/doc/refman/8.0/en/server-options.html#option_mysqld_sysdate-is-now) 一致。\n\n### `tidb_sysproc_scan_concurrency` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[0, 4294967295]`，在 v7.5.0 及之前版本中最大值为 `256`。在 v8.2.0 之前版本中，最小值为 `1`。当设置为 `0` 时，TiDB 会根据集群规模自适应调整并发度。\n- 这个变量用来设置 TiDB 执行内部 SQL 语句（例如统计信息自动更新）时 scan 操作的并发度。\n\n### `tidb_table_cache_lease` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`3`\n- 范围：`[1, 10]`\n- 单位：秒\n- 这个变量用来控制[缓存表](/cached-tables.md)的 lease 时间，默认值是 3 秒。该变量值的大小会影响缓存表的修改。在缓存表上执行修改操作后，最长可能出现 `tidb_table_cache_lease` 变量值时长的等待。如果业务表为只读表，或者能接受很高的写入延迟，则可以将该变量值调大，从而增加缓存的有效时间，减少 lease 续租的频率。\n\n### `tidb_tmp_table_max_size` <span class=\"version-mark\">从 v5.3 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`67108864`\n- 范围：`[1048576, 137438953472]`\n- 单位：字节\n- 这个变量用于限制单个[临时表](/temporary-tables.md)的最大大小，临时表超出该大小后报错。\n\n### `tidb_track_aggregate_memory_usage`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 本变量控制 TiDB 是否跟踪聚合函数的内存使用情况。\n\n> **警告：**\n>\n> 如果禁用该变量，TiDB 可能无法准确跟踪内存使用情况，并且无法控制对应 SQL 语句的内存使用。\n\n### `tidb_tso_client_batch_max_wait_time` <span class=\"version-mark\">从 v5.3.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：浮点数\n- 默认值：`0`\n- 范围：`[0, 10]`\n- 单位：毫秒\n- 这个变量用来设置 TiDB 向 PD 请求 TSO 时进行一次攒批操作的最大等待时长。默认值为 `0`，即不进行额外的等待。\n- 在向 PD 获取 TSO 请求时，TiDB 使用的 PD Client 会一次尽可能多地收集同一时刻的 TSO 请求，将其攒批合并成一个 RPC 请求后再发送给 PD，从而减轻 PD 的压力。\n- 将这个变量值设置为非 0 后，TiDB 会在每一次攒批结束前进行一个最大时长为其值的等待，目的是为了收集到更多的 TSO 请求，从而提高攒批效果。\n- 适合调高这个变量值的场景：\n    * PD leader 因高压力的 TSO 请求而达到 CPU 瓶颈，导致 TSO RPC 请求的延迟较高。\n    * 集群中 TiDB 实例的数量不多，但每一台 TiDB 实例上的并发量较高。\n- 在实际使用中，推荐将该变量尽可能设置为一个较小的值。\n\n> **注意：**\n>\n> - 如果 PD leader 的 TSO RPC 延迟升高，但其现象并非由 CPU 使用率达到瓶颈而导致（可能存在网络等问题），此时，调高 `tidb_tso_client_batch_max_wait_time` 可能会导致 TiDB 的语句执行延迟上升，影响集群的 QPS 表现。\n> - 该功能与 [`tidb_tso_client_rpc_mode`](#tidb_tso_client_rpc_mode-从-v840-版本开始引入) 不兼容。该变量设为非零值将导致 [`tidb_tso_client_rpc_mode`](#tidb_tso_client_rpc_mode-从-v840-版本开始引入) 不生效。\n\n### `tidb_tso_client_rpc_mode` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`DEFAULT`\n- 可选值：`DEFAULT`、`PARALLEL`、`PARALLEL-FAST`\n- 这个变量用来设置 TiDB 向 PD 发送 TSO RPC 请求时使用的模式。这里的模式将用于控制 TSO RPC 请求是否并行，调节获取 TS 时消耗在请求攒批阶段的时间，从而在某些场景中减少执行查询时等待 TS 阶段的时间。\n\n    - `DEFAULT`：默认模式。TiDB 会将一段时间内当前节点的所有取 TS 操作攒批到一个 TSO RPC 请求中发送给 PD 批量获取 TS，因而每次取 TS 操作的耗时由等待攒批的时间和进行 RPC 请求的时间组成。在默认模式下，不同的 TSO RPC 请求之间是串行进行的，每个取 TS 操作的平均耗时是实际 TSO RPC 耗时的 1.5 倍左右。\n    - `PARALLEL`：并行模式。在该模式下，TiDB 会尝试将每次攒批的时间缩短到默认模式的 1/2 左右，并尽可能保持两个 TSO RPC 请求同时进行。这样，每个取 TS 的操作的平均耗时理论上最多能缩短到实际 TSO RPC 耗时的 1.25 倍左右，即默认模式的 83% 左右。但是，攒批的效果会降低，TSO RPC 请求的数量会上升到默认模式的两倍左右。\n    - `PARALLEL-FAST`：快速并行模式。与 `PARALLEL` 模式类似，在该模式下，TiDB 会尝试将每次攒批的时间缩短到默认模式 1/4 左右，并尽可能保持 4 个 TSO RPC 请求同时进行。这样，每个取 TS 操作的平均耗时理论上最多能缩短到实际 TSO RPC 耗时的 1.125 倍左右，即默认模式的 75% 左右。但是，攒批的效果会进一步降低，TSO RPC 请求的数量会上升到默认模式的 4 倍左右。\n\n- 当满足以下条件时，可以考虑将该变量设置为 `PARALLEL` 或 `PARALLEL-FAST` 来获得一定的性能提升：\n\n    - TSO 等待时间在 SQL 查询的整体耗时中占比显著。\n    - PD 的 TSO 分配未达到瓶颈。\n    - PD 和 TiDB 节点的 CPU 资源比较充足。\n    - TiDB 到 PD 的网络延迟显著高于 PD 进行 TSO 分配的耗时，即 TSO RPC 请求的耗时主要由网络延迟构成。\n        - TSO RPC 请求的耗时可以通过 Grafana 的 TiDB 面板中 PD Client 分类下的 **PD TSO RPC Duration** 查看。\n        - PD 进行 TSO 分配的耗时可以通过 Grafana 的 PD 面板中 TiDB 分类下的 **PD server TSO handle duration** 查看。\n    - 可以接受 TiDB 到 PD 的 TSO RPC 请求的数量增加 2 倍（对于 `PARALLEL` 模式）或 4 倍（对于 `PARALLEL-FAST`）所带来的额外网络流量。\n\n> **注意：**\n>\n> - `PARALLEL` 和 `PARALLEL-FAST` 这两种模式与 [`tidb_tso_client_batch_max_wait_time`](#tidb_tso_client_batch_max_wait_time-从-v530-版本开始引入) 和 [`tidb_enable_tso_follower_proxy`](#tidb_enable_tso_follower_proxy-从-v530-版本开始引入) 不兼容。如果 [`tidb_tso_client_batch_max_wait_time`](#tidb_tso_client_batch_max_wait_time-从-v530-版本开始引入) 被设为非零值或者 [`tidb_enable_tso_follower_proxy`](#tidb_enable_tso_follower_proxy-从-v530-版本开始引入) 被启用，则 `tidb_tso_client_rpc_mode` 的设置不会生效，并按照 `DEFAULT` 模式执行。\n> - `PARALLEL` 和 `PARALLEL-FAST` 主要用于降低 TiDB 取 TS 操作的平均耗时。对于某些延迟波动较大的情况，如长尾、尖刺问题，这两种模式可能无法带来显著性能改善。\n\n### `tidb_ttl_delete_rate_limit` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 9223372036854775807]`\n- 这个变量用来对每个 TiDB 节点的 TTL 删除操作进行限流。其值代表了在 TTL 任务中单个节点每秒允许 `DELETE` 语句执行的最大次数。当此变量设置为 `0` 时，则表示不做限制。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_ttl_delete_batch_size` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`100`\n- 范围：`[1, 10240]`\n- 这个变量用于设置 TTL 任务中单个删除事务中允许删除的最大行数。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_ttl_delete_worker_count` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`4`\n- 范围：`[1, 256]`\n- 这个变量用于设置每个 TiDB 节点上 TTL 删除任务的最大并发数。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_ttl_job_enable` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`ON`\n- 类型：布尔型\n- 这个变量用于控制是否启动 TTL 后台清理任务。如果设置为 `OFF`，所有具有 TTL 属性的表会自动停止对过期数据的清理。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_ttl_scan_batch_size` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`500`\n- 范围：`[1, 10240]`\n- 这个变量用于设置 TTL 任务中用来扫描过期数据的每个 `SELECT` 语句的 `LIMIT` 的值。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_ttl_scan_worker_count` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`4`\n- 范围：`[1, 256]`\n- 这个变量用于设置每个 TiDB 节点 TTL 扫描任务的最大并发数。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_ttl_job_schedule_window_start_time` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：时间\n- 是否持久化到集群：是\n- 默认值：`00:00 +0000`\n- 这个变量用于控制 TTL 后台清理任务的调度窗口的起始时间。请谨慎调整此参数，过小的窗口有可能会造成过期数据的清理无法完成。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_ttl_job_schedule_window_end_time` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：时间\n- 是否持久化到集群：是\n- 默认值：`23:59 +0000`\n- 这个变量用于控制 TTL 后台清理任务的调度窗口的结束时间。请谨慎调整此参数，过小的窗口有可能会造成过期数据的清理无法完成。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_ttl_running_tasks` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`-1` 或 `[1, 256]`\n- 这个变量用于限制整个集群内 TTL 任务的并发量。`-1` 表示与 TiKV 节点的数量相同。更多信息，请参考 [Time to Live](/time-to-live.md)。\n\n### `tidb_txn_assertion_level` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`FAST`\n- 可选值：`OFF`，`FAST`，`STRICT`\n- 这个变量用于设置 assertion 级别。assertion 是一项在事务提交过程中进行的数据索引一致性校验，它对正在写入的 key 是否存在进行检查。如果不符则说明数据索引不一致，会导致事务 abort。详见[数据索引一致性报错](/troubleshoot-data-inconsistency-errors.md)。\n- 对于新创建的 v6.0.0 及以上的集群，默认值为 `FAST`。对于升级版本的集群，如果升级前是低于 v6.0.0 的版本，升级后默认值为 `OFF`。\n\n    - `OFF`: 关闭该检查。\n    - `FAST`: 开启大多数检查项，对性能几乎无影响。\n    - `STRICT`: 开启全部检查项，当系统负载较高时，对悲观事务的性能有较小影响。\n\n### `tidb_txn_commit_batch_size` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`16384`\n- 范围：`[1, 1073741824]`\n- 单位：字节\n- 这个变量用于控制 TiDB 向 TiKV 发送的事务提交请求的批量大小。如果业务负载的大部分事务都有大量的写操作，适当调大该变量可以提高批处理的效果。但需要注意的是，设置过大将会超过 TiKV 的 [`raft-entry-max-size`](/tikv-configuration-file.md#raft-entry-max-size) 限制，导致提交失败。\n\n### `tidb_txn_entry_size_limit` <span class=\"version-mark\">从 v7.6.0 版本开始引入</span>\n\n- 作用域：GLOBAL | SESSION\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 125829120]`\n- 单位：字节\n- 这个变量用于动态修改 TiDB 配置项 [`performance.txn-entry-size-limit`](/tidb-configuration-file.md#txn-entry-size-limit-从-v4010-和-v500-版本开始引入)，其含义与对应配置项相同，即用于限制 TiDB 单行数据的大小。该变量默认值为 `0`，表示默认使用配置项的值。当设置为非 `0` 值时，优先使用该变量的值作为 `txn-entry-size-limit` 的值。\n\n> **注意：**\n>\n> 使用 SESSION 作用域修改该变量时仅影响当前用户会话，不会影响 TiDB 的内部会话。这可能导致 TiDB 内部事务的单行数据大小超过配置项的限制，从而导致事务失败。如需在线调高限制，建议优先使用 GLOBAL 作用域修改该变量。\n\n### `tidb_txn_mode`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`pessimistic`\n- 可选值：`pessimistic`，`optimistic`\n- 这个变量用于设置事务模式。TiDB v3.0 支持了悲观事务，自 v3.0.8 开始，默认使用[悲观事务模式](/pessimistic-transaction.md)。\n- 但如果从 3.0.7 及之前的版本升级到 >= 3.0.8 的版本，不会改变默认事务模式，即**只有新创建的集群才会默认使用悲观事务模式**。\n- 将该变量设置为 \"optimistic\" 或 \"\" 时，将会使用[乐观事务模式](/optimistic-transaction.md)。\n\n### `tidb_use_plan_baselines` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制是否开启执行计划绑定功能，默认打开，可通过赋值 `OFF` 来关闭。关于执行计划绑定功能的使用可以参考[执行计划绑定文档](/sql-plan-management.md#创建绑定)。\n\n### `tidb_wait_split_region_finish`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 由于打散 Region 的时间可能比较长，主要由 PD 调度以及 TiKV 的负载情况所决定。这个变量用来设置在执行 `SPLIT REGION` 语句时，是否同步等待所有 Region 都打散完成后再返回结果给客户端。\n    - 默认 `ON` 代表等待打散完成后再返回结果\n    - `OFF` 代表不等待 Region 打散完成就返回。\n- 需要注意的是，在 Region 打散期间，对正在打散 Region 上的写入和读取的性能会有一定影响，对于批量写入、导数据等场景，还是建议等待 Region 打散完成后再开始导数据。\n\n### `tidb_wait_split_region_timeout`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`300`\n- 范围：`[1, 2147483647]`\n- 单位：秒\n- 这个变量用来设置 `SPLIT REGION` 语句的执行超时时间，默认值是 300 秒，如果超时还未完成，就返回一个超时错误。\n\n### `tidb_window_concurrency` <span class=\"version-mark\">从 v4.0 版本开始引入</span>\n\n> **警告：**\n>\n> 从 v5.0 版本开始，该变量被废弃。请使用 [`tidb_executor_concurrency`](#tidb_executor_concurrency-从-v50-版本开始引入) 进行设置。\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`-1`\n- 范围：`[1, 256]`\n- 单位：线程\n- 这个变量用于设置 window 算子的并行度。\n- 默认值 `-1` 表示使用 `tidb_executor_concurrency` 的值。\n\n### `tiflash_fastscan` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`OFF`\n- 类型：布尔型\n- 如果开启 [FastScan 功能](/tiflash/use-fastscan.md)（设置为 `ON` 时），TiFlash 可以提供更高效的查询性能，但不保证查询结果的精度和数据一致性。\n\n### `tiflash_fine_grained_shuffle_batch_size` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 默认值：`8192`\n- 范围：`[1, 18446744073709551615]`\n- 细粒度 shuffle 功能开启时，下推到 TiFlash 的窗口函数可以并行执行。该变量控制发送端发送数据的攒批大小。\n- 对性能影响：如果该值设置过小，例如极端值 1，会导致每个 Block 都进行一次网络传输。如果设置过大，例如极端值整个表的行数，会导致接收端大部分时间都在等待数据，无法流水线计算。可以观察 TiFlash 接收端收到的行数分布情况，如果大部分线程接收的行数很少，例如只有几百行，可以增加该值以达到减少网络开销的目的。\n\n### `tiflash_fine_grained_shuffle_stream_count` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[-1, 1024]`\n- 当窗口函数下推到 TiFlash 执行时，可以通过该变量控制窗口函数执行的并行度。不同取值含义：\n\n    * -1: 表示不使用细粒度 shuffle 功能，下推到 TiFlash 的窗口函数以单线程方式执行\n    * 0: 表示使用细粒度 shuffle 功能。如果 [`tidb_max_tiflash_threads`](/system-variables.md#tidb_max_tiflash_threads-从-v610-版本开始引入) 有效（大于 0），则 `tiflash_fine_grained_shuffle_stream_count` 会自动取值为 [`tidb_max_tiflash_threads`](/system-variables.md#tidb_max_tiflash_threads-从-v610-版本开始引入)，否则会根据 TiFlash 计算节点的 CPU 资源自动推算。最终在 TiFlash 上窗口函数的实际并发度为：min(`tiflash_fine_grained_shuffle_stream_count`，TiFlash 节点物理线程数)\n    * 大于 0: 表示使用细粒度 shuffle 功能，下推到 TiFlash 的窗口函数会以多线程方式执行，并发度为： min(`tiflash_fine_grained_shuffle_stream_count`, TiFlash 节点物理线程数)\n- 理论上窗口函数的性能会随着该值的增加线性提升。但是如果设置的值超过实际的物理线程数，反而会导致性能下降。\n\n### `tiflash_mem_quota_query_per_node` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[-1, 9223372036854775807]`\n- 用于设置单个查询在单个 TiFlash 节点上的内存使用上限，超过该限制时 TiFlash 会报错并终止该查询。`-1` 或者 `0` 表示无限制。当该变量的值大于 `0` 且 [`tiflash_query_spill_ratio`](/system-variables.md#tiflash_query_spill_ratio-从-v740-版本开始引入) 也设置为有效值时，TiFlash 将启用[查询级别的落盘机制](/tiflash/tiflash-spill-disk.md#查询级别的落盘)。\n\n### `tiflash_query_spill_ratio` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：浮点数\n- 默认值：`0.7`\n- 范围：`[0, 0.85]`\n- 用于控制 TiFlash [查询级别的落盘](/tiflash/tiflash-spill-disk.md#查询级别的落盘)机制的阈值：`0` 表示关闭查询级别的自动落盘机制；大于 `0` 时，如果查询使用的内存超过 [`tiflash_mem_quota_query_per_node`](/system-variables.md#tiflash_mem_quota_query_per_node-从-v740-版本开始引入) * `tiflash_query_spill_ratio`，TiFlash 会触发查询级别的落盘，即将查询中支持落盘的算子的数据按需进行落盘。\n\n> **注意：**\n>\n> - 该变量只在 [`tiflash_mem_quota_query_per_node`](/system-variables.md#tiflash_mem_quota_query_per_node-从-v740-版本开始引入) 大于 `0` 时生效，即如果 [tiflash_mem_quota_query_per_node](/system-variables.md#tiflash_mem_quota_query_per_node-从-v740-版本开始引入) 为 `0` 或 `-1`，即使 `tiflash_query_spill_ratio` 大于 `0` 也不会启用查询级别的落盘机制。\n> - 当 TiFlash 查询级别的落盘机制开启时，TiFlash 单个算子的落盘阈值会自动失效，即如果 [`tiflash_mem_quota_query_per_node`](/system-variables.md#tiflash_mem_quota_query_per_node-从-v740-版本开始引入) 和 `tiflash_query_spill_ratio` 均大于 0， [tidb_max_bytes_before_tiflash_external_sort](/system-variables.md#tidb_max_bytes_before_tiflash_external_sort-从-v700-版本开始引入)、[tidb_max_bytes_before_tiflash_external_group_by](/system-variables.md#tidb_max_bytes_before_tiflash_external_group_by-从-v700-版本开始引入)、[tidb_max_bytes_before_tiflash_external_join](/system-variables.md#tidb_max_bytes_before_tiflash_external_join-从-v700-版本开始引入) 这三个变量会自动失效，等效于被设置为 `0`。\n\n### `tiflash_replica_read` <span class=\"version-mark\">从 v7.3.0 版本开始引入</span>\n\n- 作用范围：SESSION | GLOBAL\n- 持久化至集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`all_replicas`\n- 可选值：`all_replicas`、`closest_adaptive`、`closest_replicas`\n- 该变量用于设置当查询需要使用 TiFlash 引擎时，TiFlash 副本的选择策略。\n    - `all_replicas` 表示使用所有的 TiFlash 副本进行分析计算。\n    - `closest_adaptive` 表示尽量使用与当前发起查询请求的 TiDB 节点相同区域的 TiFlash 副本进行分析计算。如果此区域的 TiFlash 副本未包含查询所需的全部数据，则再使用其他区域的 TiFlash 副本及对应的 TiFlash 节点。\n    - `closest_replicas` 表示仅使用与发起当前查询请求的 TiDB 节点相同区域的 TiFlash 副本进行分析计算。如果此区域的 TiFlash 副本未包含查询所需的全部数据，则查询将报错。\n\n> **注意：**\n>\n> - 如果 TiDB 节点未设置[区域属性](/schedule-replicas-by-topology-labels.md#设置-tidb-的-labels可选)，并且 TiFlash 副本选择策略不是 `all_replicas` 时，TiFlash 引擎将忽略 TiFlash 副本选择策略，使用所有 TiFlash 副本进行 TiFlash 查询，并且返回警告 `The variable tiflash_replica_read is ignored`。\n> - 如果 TiFlash 节点未设置[区域属性](/schedule-replicas-by-topology-labels.md#设置-tikv-和-tiflash-的-labels)，则将其视为不属于任何区域的节点。\n\n### `tiflash_hashagg_preaggregation_mode` <span class=\"version-mark\">从 v8.3.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：是\n- 类型：枚举型\n- 默认值：`force_preagg`\n- 可选值：`force_preagg`、`force_streaming`、`auto`\n- 该变量用于控制下推到 TiFlash 的两阶段或三阶段 HashAgg 在第一阶段采用哪种预聚合策略：\n    - `force_preagg`：TiFlash 在第一阶段的 HashAgg 中强制进行预聚合操作，与 v8.3.0 之前版本的行为一致\n    - `force_streaming`：TiFlash 直接将数据发送到下一阶段的 HashAgg，不进行预聚合操作\n    - `auto`：TiFlash 根据当前工作负载的聚合度自动选择是否进行预聚合操作\n\n### `tikv_client_read_timeout` <span class=\"version-mark\">从 v7.4.0 版本开始引入</span>\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`0`\n- 范围：`[0, 2147483647]`\n- 单位：毫秒\n- 该变量用于设置查询语句中 TiDB 发送 TiKV RPC 读请求的超时时间。当 TiDB 集群在网络不稳定或 TiKV 的 I/O 延迟抖动严重的环境下，且用户对查询 SQL 的延迟比较敏感时，可以通过设置 `tikv_client_read_timeout` 调小 TiKV RPC 读请求的超时时间，这样当某个 TiKV 节点出现 I/O 延迟抖动时，TiDB 侧可以快速超时并重新发送 TiKV RPC 请求给下一个 TiKV Region Peer 所在的 TiKV 节点。如果所有 TiKV Region Peer 都请求超时，则会用默认的超时时间（通常是 40 秒）进行新一轮的重试。\n- 你也可以在查询语句中使用 Optimizer Hint `/*+ SET_VAR(TIKV_CLIENT_READ_TIMEOUT=N) */` 来设置 TiDB 发送 TiKV RPC 读请求的超时时间。当同时设置了 Optimizer Hint 和该系统变量时，Optimizer Hint 的优先级更高。\n- 默认值 `0` 表示使用默认的超时时间（通常是 40 秒）。\n\n> **注意：**\n>\n> - 一个普通查询通常耗时几毫秒，但偶尔可能会出现某个 TiKV 节点的网络不稳定或 I/O 抖动，导致查询耗时超过 1 秒甚至 10 秒。此时，你可以尝试在查询语句中使用 Optimizer Hint `/*+ SET_VAR(TIKV_CLIENT_READ_TIMEOUT=100) */` 将 TiKV RPC 读请求超时设置为 100 毫秒，这样即使遇到某个 TiKV 节点查询慢，也可以快速超时然后重新发送 RPC 请求给下一个 TiKV Region Peer 所在的 TiKV 节点。由于两个 TiKV 节点同时出现 I/O 抖动的概率较低，所以该查询语句的耗时通常可以预期在几毫秒到 110 毫秒之间。\n> - 不建议将 `tikv_client_read_timeout` 的值设置的太小（例如，1 毫秒），否则 TiDB 集群在负载压力较大时会很容易导致请求超时，然后重试会进一步增加 TiDB 集群的压力。\n> - 如需为不同类型的查询语句设置不同的超时时间，建议使用 Optimizer Hint。\n\n### `time_zone`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`SYSTEM`\n- 数据库所使用的时区。这个变量值可以写成时区偏移的形式，如 '-8:00'，也可以写成一个命名时区，如 'America/Los_Angeles'。\n- 默认值 `SYSTEM` 表示时区应当与系统主机的时区相同。系统的时区可通过 [`system_time_zone`](#system_time_zone) 获取。\n\n### `timestamp`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：浮点数\n- 默认值：`0`\n- 取值范围：`[0, 2147483647]`\n- 一个 Unix 时间戳。变量值非空时，表示 `CURRENT_TIMESTAMP()`、`NOW()` 等函数的时间戳。该变量通常用于数据恢复或数据复制。\n\n### `transaction_isolation`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`REPEATABLE-READ`\n- 可选值：`READ-UNCOMMITTED`，`READ-COMMITTED`，`REPEATABLE-READ`，`SERIALIZABLE`\n- 这个变量用于设置事务隔离级别。TiDB 为了兼容 MySQL，支持可重复读 (`REPEATABLE-READ`)，但实际的隔离级别是快照隔离。详情见[事务隔离级别](/transaction-isolation-levels.md)。\n\n### `tx_isolation`\n\n这个变量是 `transaction_isolation` 的别名。\n\n### `tx_isolation_one_shot`\n\n> **注意：**\n>\n> 该变量仅用于 TiDB 内部实现，不推荐设置该变量。\n\n在 TiDB 内部实现中，TiDB 解释器会将 `SET TRANSACTION ISOLATION LEVEL [READ COMMITTED| REPEATABLE READ | ...]` 语句转化为 `SET @@SESSION.TX_ISOLATION_ONE_SHOT = [READ COMMITTED| REPEATABLE READ | ...]`。\n\n### `tx_read_ts`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 在 Stale Read 场景下，该会话变量用于帮助记录 Stable Read TS 值。\n- 该变量仅用于 TiDB 内部实现，**不推荐设置该变量**。\n\n### `txn_scope`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`global`\n- 可选值：`global` 和 `local`\n- 该变量用于设置当前会话下事务为全局事务（设为 `global`）还是局部事务（设为 `local`）。\n- 该变量仅用于 TiDB 内部实现，**不推荐设置该变量**。\n\n### `validate_password.check_user_name` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`ON`\n- 类型：布尔型\n- 该变量是密码复杂度策略检查中的一个检查项，用于进行密码与用户名匹配检查。只有 [`validate_password.enable`](/system-variables.md#validate_passwordenable-从-v650-版本开始引入) 开启时，该变量才生效。\n- 当该变量生效且为 `ON` 时，如果设置账户密码，TiDB 会将密码与当前会话账户的用户名部分（不包含主机名部分）进行比较，如果匹配则拒绝该密码。\n- 该变量独立于 [validate_password.policy](/system-variables.md#validate_passwordpolicy-从-v650-版本开始引入)，即不受密码复杂度检测强度的控制。\n\n### `validate_password.dictionary` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：\"\"\n- 类型：字符串\n- 该变量是密码复杂度策略检查中的一个检查项，用于进行密码与字典字符串匹配检查。只有当 [`validate_password.enable`](/system-variables.md#validate_passwordenable-从-v650-版本开始引入) 开启且 [validate_password.policy](/system-variables.md#validate_passwordpolicy-从-v650-版本开始引入) 设置为 `2` (STRONG) 时，该变量才生效。\n- 该变量是一个长字符串，长度不超过 1024，字符串内容可包含一个或多个在密码中不允许出现的单词，每个单词之间采用英文分号（`;`）分隔。\n- 默认情况下，该变量为空值，不执行字典检查。要进行字典检查，该变量值必须包含待匹配的单词。配置了该变量后，在设置账户密码时，TiDB 会将长度为 4 到 100 的密码的每个子字符串与该变量中配置的单词进行比较。任何匹配都会导致密码被拒绝。比较不区分大小写。\n\n### `validate_password.enable` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`OFF`\n- 类型：布尔型\n- 该变量是密码复杂度策略检查的开关。该变量设置为 `ON` 后，当设置账户密码时，TiDB 才会进行密码复杂度的各项检查。\n\n### `validate_password.length` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`8`\n- 范围：`[0, 2147483647]`\n- 该变量是密码复杂度策略检查中的一个检查项，用于限定密码的最小长度，默认最小长度为 8。只有 [`validate_password.enable`](/system-variables.md#validate_passwordenable-从-v650-版本开始引入) 开启时，该变量才生效。\n- 设置该变量时有最小值要求，最小值由其他几个相关的系统变量控制，即该变量的值不能设置为小于此表达式的值：`validate_password.number_count + validate_password.special_char_count + (2 * validate_password.mixed_case_count)`。\n- 当用户修改 `validate_password.number_count`、`validate_password.special_char_count`、`validate_password.mixed_case_count` 后导致表达式的值大于 `validate_password.length` 时，`validate_password.length` 将自动被修改为满足表达式的最小值。\n\n### `validate_password.mixed_case_count` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[0, 2147483647]`\n- 该变量是密码复杂度策略检查中的一个检查项，用于限定密码中至少需要包含多少个大写字符和小写字符。只有当 [`validate_password.enable`](/system-variables.md#validate_passwordenable-从-v650-版本开始引入) 开启且 [validate_password.policy](/system-variables.md#validate_passwordpolicy-从-v650-版本开始引入) 大于或等于 `1` (MEDIUM) 时，该变量才生效。\n- 对于给定的 `validate_password.mixed_case_count` 值，密码中的小写字符数和大写字符数都不能少于该值。例如，值为 1 时，密码中至少需要 1 个小写字母，至少需要 1 个大写字母。\n\n### `validate_password.number_count` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[0, 2147483647]`\n- 该变量是密码复杂度策略检查中的一个检查项，用于限定密码中至少需要包含多少个数字字符。只有当 [`validate_password.enable`](/system-variables.md#validate_passwordenable-从-v650-版本开始引入) 开启且 [validate_password.policy](/system-variables.md#validate_passwordpolicy-从-v650-版本开始引入) 大于或等于 `1` (MEDIUM) 时，该变量才生效。\n\n### `validate_password.policy` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：枚举型\n- 默认值：`1`\n- 可选值：`[0, 1, 2]`\n- 该变量是[密码复杂度策略检查](/password-management.md#密码复杂度策略)的强度策略，该变量影响其他密码复杂度系统变量（前缀为 `validate_password`）在密码检查时是否生效，但是 `validate_password.check_user_name` 除外。只有 [`validate_password.enable`](/system-variables.md#validate_passwordenable-从-v650-版本开始引入) 开启时，该变量才生效。\n- 该变量可以使用数值 0、1、2 或相应的符号值 LOW、MEDIUM、STRONG，密码强度策略对应的检查项如下：\n    - 0 或者 LOW：检查密码长度。\n    - 1 或者 MEDIUM：检查密码长度，检查密码中数字、小写字符、大写字符、特殊字符数量。\n    - 2 或者 STRONG：检查密码长度，检查密码中数字、小写字符、大写字符、特殊字符数量，检查密码字典匹配。\n\n### `validate_password.special_char_count` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n- 作用域：GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`1`\n- 范围：`[0, 2147483647]`\n- 该变量是密码[复杂度策略检查](/password-management.md#密码复杂度策略)中的一个检查项，用于限定密码中至少需要包含多少个特殊字符。只有当 [`validate_password.enable`](/system-variables.md#validate_passwordenable-从-v650-版本开始引入) 开启且 [validate_password.policy](/system-variables.md#validate_passwordpolicy-从-v650-版本开始引入) 大于或等于 `1` (MEDIUM) 时，该变量才生效。\n\n### `version`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`8.0.11-TiDB-(tidb version)`\n- 这个变量的值是 MySQL 的版本和 TiDB 的版本，例如 '8.0.11-TiDB-v8.5.0'。\n\n### `version_comment`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：(string)\n- 这个变量的值是 TiDB 版本号的其他信息，例如 'TiDB Server (Apache License 2.0) Community Edition, MySQL 8.0 compatible'。\n\n### `version_compile_os`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：(string)\n- 这个变量值是 TiDB 所在操作系统的名称。\n\n### `version_compile_machine`\n\n- 作用域：NONE\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：(string)\n- 这个变量值是运行 TiDB 的 CPU 架构的名称。\n\n### `wait_timeout`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：整数型\n- 默认值：`28800`\n- 范围：`[0, 31536000]`\n- 单位：秒\n- 这个变量表示用户会话的空闲超时。`0` 代表没有时间限制。\n\n### `warning_count`\n\n- 作用域：SESSION\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 默认值：`0`\n- 这个只读变量表示之前执行语句中出现的警告数。\n\n### `windowing_use_high_precision`\n\n- 作用域：SESSION | GLOBAL\n- 是否持久化到集群：是\n- 是否受 Hint [SET_VAR](/optimizer-hints.md#set_varvar_namevar_value) 控制：否\n- 类型：布尔型\n- 默认值：`ON`\n- 这个变量用于控制计算[窗口函数](/functions-and-operators/window-functions.md)时是否采用高精度模式。\n"
        },
        {
          "name": "table-attributes.md",
          "type": "blob",
          "size": 4.9375,
          "content": "---\ntitle: 表属性\nsummary: 介绍 TiDB 的 `ATTRIBUTES` 使用方法。\n---\n\n# 表属性\n\n表属性是 TiDB 从 5.3.0 版本开始引入的新特性，用于为表或分区添加特定的属性，以对表或分区执行相应属性对应的操作，例如可以利用表属性控制 Region 的合并。\n\n> **注意：**\n>\n> - 目前 TiDB 仅支持为表或分区添加 `merge_option` 属性，用于控制 Region 合并。该属性仅能处理部分热点问题。如需了解更多的热点问题处理相关内容，请参阅 [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md)。\n> - 当使用 TiCDC 进行同步或者使用 BR 进行增量备份时，同步和备份会跳过设置表属性的 DDL 语句。如需在下游或者备份集群使用表属性，需要在下游或者备份集群手动执行该 DDL 语句以设置表属性。\n\n## 使用方法\n\n表属性为 `key=value` 的形式，多个属性需要用逗号分隔。具体示例如下，其中 `t` 为所要修改的表名，`p` 为所要修改的分区名，`[]`内部为可选项。\n\n+ 设置表或分区属性\n\n    ```sql\n    ALTER TABLE t [PARTITION p] ATTRIBUTES [=] 'key=value[, key1=value1...]';\n    ```\n\n+ 重置表或分区属性\n\n    ```sql\n    ALTER TABLE t [PARTITION p] ATTRIBUTES [=] DEFAULT;\n    ```\n\n+ 查看全部表及分区属性\n\n    ```sql\n    SELECT * FROM information_schema.attributes;\n    ```\n\n+ 查看某一张表或分区配置的属性\n\n    ```sql\n    SELECT * FROM information_schema.attributes WHERE id='schema/t[/p]';\n    ```\n\n+ 查看拥有某属性的所有表及分区\n\n    ```sql\n    SELECT * FROM information_schema.attributes WHERE attributes LIKE '%key%';\n    ```\n\n## 覆盖关系\n\n为分区表配置的属性会对表的所有分区生效。一种例外情况是，如果分区表和分区都配置了相同属性但属性值不同，分区属性将覆盖分区表属性。例如，当分区表 `t` 配置属性 `key=value`，同时分区 `p` 配置属性 `key=value1` 时：\n\n```sql\nALTER TABLE t ATTRIBUTES[=]'key=value';\nALTER TABLE t PARTITION p ATTRIBUTES[=]'key=value1';\n```\n\n分区 `p` 实际生效的属性为 `key=value1`。\n\n## 使用表属性控制 Region 合并\n\n### 使用场景\n\n当写入或读取数据存在热点时，可以使用表属性控制 Region 合并，通过为表或分区添加 `merge_option` 属性，将其设置为 `deny` 来解决。以下介绍了两种使用场景。\n\n#### 新建表或分区的写入热点问题\n\n在对某张新建表或某个新建分区写入数据存在热点问题时，通常需要使用分裂打散 Region 的操作避免写入热点，但由于新建表或分区的分裂操作实际产生的是空 Region，如果分裂打散操作距离写入存在一定时间间隔，则 Region 可能会被合并，从而导致无法真正规避写入热点问题。此时可以为表或分区添加 `merge_option` 属性，设置为 `deny` 来解决问题。\n\n#### 只读场景下周期性读热点问题\n\n在只读场景下，如果是通过手动分裂 Region 缓解某张表或分区的周期性读热点问题，且不希望热点消失后手动分裂的 Region 被合并。此时可以为表或分区添加 `merge_option` 属性，设置为 `deny` 来解决问题。\n\n### 使用方法\n\n使用方法如下，其中 `t` 为所要修改的表名，`p` 为所要修改的分区名。\n\n+ 禁止属于某个表的 Region 被合并\n\n    ```sql\n    ALTER TABLE t ATTRIBUTES 'merge_option=deny';\n    ```\n\n+ 允许属于某个表的 Region 被合并\n\n    ```sql\n    ALTER TABLE t ATTRIBUTES 'merge_option=allow';\n    ```\n\n+ 重置某个表的属性\n\n    ```sql\n    ALTER TABLE t ATTRIBUTES DEFAULT;\n    ```\n\n+ 禁止属于某个分区的 Region 被合并\n\n    ```sql\n    ALTER TABLE t PARTITION p ATTRIBUTES 'merge_option=deny';\n    ```\n\n+ 允许属于某个分区的 Region 被合并\n\n    ```sql\n    ALTER TABLE t PARTITION p attributes 'merge_option=allow';\n    ```\n\n+ 查看所有配置了 `merge_option` 属性的表或分区\n\n    ```sql\n    SELECT * FROM information_schema.attributes WHERE attributes LIKE '%merge_option%';\n    ```\n\n### 覆盖关系\n\n```sql\nALTER TABLE t ATTRIBUTES 'merge_option=deny';\nALTER TABLE t PARTITION p ATTRIBUTES 'merge_option=allow';\n```\n\n同时配置上述两个属性时，实际分区 `p` 的 Region 可以被合并。当分区的属性被重置时，分区 `p` 则会继承表 `t` 的属性，Region 无法被合并。\n\n> **注意：** \n> \n> - 如果目前只存在分区表的属性，即使配置 `merge_option=allow`，分区也会默认按照实际分区数量切分成多个 Region。如需合并所有 Region，则需要[重置该分区表的属性](#使用方法)。\n> - 使用该属性需要注意 PD 的参数 [`split-merge-interval`](/pd-configuration-file.md#split-merge-interval) 的配置。如果没有配置 `merge_option`，Region 在超过 `split-merge-interval` 指定的时间后满足条件即可合并。如果配置了 `merge_option`，则超过指定时间后会根据 `merge_option` 的配置情况再决定是否可以合并。\n"
        },
        {
          "name": "table-filter.md",
          "type": "blob",
          "size": 7.1923828125,
          "content": "---\ntitle: 表库过滤\nsummary: 在 TiDB 数据迁移工具中使用表库过滤功能。\naliases: ['/docs-cn/dev/table-filter/','/docs-cn/dev/tidb-lightning/tidb-lightning-table-filter/','/docs-cn/dev/reference/tools/tidb-lightning/table-filter/','/zh/tidb/dev/tidb-lightning-table-filter/']\n---\n\n# 表库过滤\n\nTiDB 数据迁移工具默认情况下作用于所有数据库，但实际使用中，往往只需要作用于其中的部分子集。例如，用户只想处理 `foo*` 和 `bar*` 形式的表，而无需对其他表进行操作。\n\n从 TiDB 4.0 起，所有 TiDB 数据迁移工具都使用一个通用的过滤语法来定义子集。本文档介绍如何使用表库过滤功能。\n\n## 使用表库过滤\n\n### 命令行\n\n在命令行中使用多个 `-f` 或 `--filter` 参数，即可在 TiDB 数据迁移工具中应用表库过滤规则。每个过滤规则均采用 `db.table` 形式，支持通配符（详情见[下一节](#使用通配符)）。以下为各个工具中的使用示例：\n\n* [BR](/br/br-snapshot-manual.md#使用表库过滤功能备份多张表的数据)：\n\n    ```shell\n    tiup br backup full -f 'foo*.*' -f 'bar*.*' -s 'local:///tmp/backup'\n    ```\n\n    ```shell\n    tiup br restore full -f 'foo*.*' -f 'bar*.*' -s 'local:///tmp/backup'\n    ```\n\n* [Dumpling](/dumpling-overview.md)：\n\n    ```shell\n    tiup dumpling -f 'foo*.*' -f 'bar*.*' -P 3306 -o /tmp/data/\n    ```\n\n* [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md)：\n\n    ```shell\n    tiup tidb-lightning -f 'foo*.*' -f 'bar*.*' -d /tmp/data/ --backend tidb\n    ```\n\n### TOML 配置文件\n\n在 TOML 文件中，表库过滤规则以[字符串数组](https://toml.io/cn/v1.0.0-rc.1#%E6%95%B0%E7%BB%84)的形式指定。以下为各个工具中的使用示例：\n\n* TiDB Lightning：\n\n    ```toml\n    [mydumper]\n    filter = ['foo*.*', 'bar*.*']\n    ```\n\n* [TiCDC](/ticdc/ticdc-overview.md)：\n\n    ```toml\n    [filter]\n    rules = ['foo*.*', 'bar*.*']\n\n    [[sink.dispatchers]]\n    matcher = ['db1.*', 'db2.*', 'db3.*']\n    dispatcher = 'ts'\n    ```\n\n## 表库过滤语法\n\n### 直接使用表名\n\n每条表库过滤规则由“库”和“表”组成，两部分之间以英文句号 (`.`) 分隔。只有表名与规则完全相符的表才会被接受。\n\n```\ndb1.tbl1\ndb2.tbl2\ndb3.tbl3\n```\n\n表名只由有效的[标识符](/schema-object-names.md)组成，例如：\n\n* 数字（`0` 到 `9`）\n* 字母（`a` 到 `z`，`A` 到 `Z`）\n* `$`\n* `_`\n* 非 ASCII 字符（`U+0080` 到 `U+10FFFF`）\n\n其他 ASCII 字符均为保留字。部分标点符号有特殊含义，详情见下一节。\n\n### 使用通配符\n\n表名的两个部分均支持使用通配符（详情见 [fnmatch(3)](https://pubs.opengroup.org/onlinepubs/9699919799/utilities/V3_chap02.html#tag_18_13) ）。\n\n* `*`：匹配零个或多个字符。\n* `?`：匹配一个字符。\n* `[a-z]`：匹配 \"a\" 和 \"z\" 之间的一个字符。\n* `[!a-z]`：匹配不在 \"a\" 和 \"z\" 之间的一个字符。\n\n```\ndb[0-9].tbl[0-9a-f][0-9a-f]\ndata.*\n*.backup_*\n```\n\n此处，“字符”指的是一个 Unicode 码位，例如：\n\n* `U+00E9` \"é\" 是 1 个字符。\n* `U+0065，U+0301` \"é\" 是 2 个字符。\n* `U+1F926 U+1F3FF U+200D U+2640 U+FE0F` \"🤦🏿‍♀️\" 是 5 个字符。\n\n### 使用文件导入\n\n如需导入一个文件作为过滤规则，请在规则的开头加上一个 “@” 来指定文件名。库表过滤解析器将导入文件中的每一行都解析为一条额外的过滤规则。\n\n例如，`config/filter.txt` 文件有以下内容：\n\n```\nemployees.*\n*.WorkOrder\n```\n\n以下两条表库过滤命令是等价的：\n\n```bash\ntiup dumpling -f '@config/filter.txt'\ntiup dumpling -f 'employees.*' -f '*.WorkOrder'\n```\n\n导入的文件里不能使用过滤规则导入另一个文件。\n\n### 注释与空行\n\n导入的过滤规则文件中，每一行开头和结尾的空格都会被去除。此外，空行（空字符串）也将被忽略。\n\n行首的 `#` 表示该行是注释，会被忽略。而不在行首的 `#` 则会被认为是语法错误。\n\n```\n# 这是一行注释\ndb.table   # 这一部分不是注释，且可能引起错误\n```\n\n### 排除规则\n\n在一条过滤规则的开头加上 `!`，则表示符合这条规则的表不会被 TiDB 数据迁移工具处理。通过应用排除规则，库表过滤可以作为屏蔽名单来使用。\n\n```\n*.*\n#^ 注意：必须先添加 *.* 规则来包括所有表\n!*.Password\n!employees.salaries\n```\n\n### 转义字符\n\n如果需要将特殊字符转化为标识符，可以在特殊字符前加上反斜杠 `\\`。\n\n```\ndb\\.with\\.dots.*\n```\n\n为了简化语法并向上兼容，**不支持**下列字符序列：\n\n- 在行尾去除空格后使用 `\\`（使用 `[ ]` 来匹配行尾的空格）。\n- 在 `\\` 后使用数字或字母 (`[0-9a-zA-Z]`)。特别是类似 C 的转义序列，如 `\\0`、`\\r`、`\\n`、`\\t` 等序列，目前在表库过滤规则中无意义。\n\n### 引号包裹的标识符\n\n除了 `\\` 之外，还可以用 `\"` 和 `` ` `` 来控制特殊字符。\n\n```\n\"db.with.dots\".\"tbl\\1\"\n`db.with.dots`.`tbl\\2`\n```\n\n也可以通过输入两次引号，将引号包含在标识符内。\n\n```\n\"foo\"\"bar\".`foo``bar`\n# 等价于：\nfoo\\\"bar.foo\\`bar\n```\n\n用引号包裹的标识符不可以跨越多行。\n\n用引号只包裹标识符的一部分是无效的，例如：\n\n```\n\"this is \"invalid*.*\n```\n\n### 正则表达式\n\n如果你需要使用较复杂的过滤规则，可以将每个匹配模型写为正则表达式，以 `/` 为分隔符：\n\n```\n/^db\\d{2,}$/./^tbl\\d{2,}$/\n```\n\n这类正则表示使用 [Go dialect](https://pkg.go.dev/regexp/syntax?tab=doc)。只要标识符中有一个子字符串与正则表达式匹配，则视为匹配该模型。例如，`/b/` 匹配 `db01`。\n\n> **注意：**\n>\n> 正则表达式中的每一个 `/` 都需要转义为 `\\/`，包括在 `[...]` 里面的 `/`。不允许在 `\\Q...\\E` 之间放置一个未转义的 `/`。\n\n## 使用多个过滤规则\n\n当表的名称与过滤列表中所有规则均不匹配时，默认情况下这些表被忽略。\n\n要建立一个屏蔽名单，必须使用显式的 `*.*` 作为第一条过滤规则，否则所有表均被排除。\n\n```bash\n# 所有表均被过滤掉\ntiup dumpling -f '!*.Password'\n\n# 只有 “Password” 表被过滤掉，其余表仍保留\ntiup dumpling -f '*.*' -f '!*.Password'\n```\n\n如果一个表的名称与过滤列表中的多个规则匹配，则以最后匹配的规则为准。例如：\n\n```\n# rule 1\nemployees.*\n# rule 2\n!*.dep*\n# rule 3\n*.departments\n```\n\n过滤结果如下：\n\n| 表名            | 规则 1 | 规则 2 | 规则 3 | 结果          |\n|-----------------------|--------|--------|--------|------------------|\n| irrelevant.table      |        |        |        | 默认（拒绝） |\n| employees.employees   | ✓      |        |        | 规则 1（接受）  |\n| employees.dept_emp    | ✓      | ✓      |        | 规则 2（拒绝）  |\n| employees.departments | ✓      | ✓      | ✓      | 规则 3（接受）  |\n| else.departments      |        | ✓      | ✓      | 规则 3（接受）  |\n\n> **注意：**\n>\n> 在 TiDB 数据迁移工具的默认配置中，系统库总是被排除。系统库有以下六个：\n>\n> * `INFORMATION_SCHEMA`\n> * `PERFORMANCE_SCHEMA`\n> * `METRICS_SCHEMA`\n> * `INSPECTION_SCHEMA`\n> * `mysql`\n> * `sys`\n"
        },
        {
          "name": "telemetry.md",
          "type": "blob",
          "size": 3.515625,
          "content": "---\ntitle: 遥测\nsummary: 介绍遥测的场景，如何禁用功能和查看遥测状态。\naliases: ['/docs-cn/dev/telemetry/','/zh/tidb/dev/sql-statement-admin-show-telemetry']\n---\n\n# 遥测\n\n开启遥测后，TiUP 和 TiSpark 会收集使用情况信息，并将这些信息分享给 PingCAP 用于改善产品。\n\n> **注意：**\n>\n> - 从 TiUP v1.11.3 起，TiUP 遥测功能默认关闭，即 TiUP 默认不再收集使用情况信息。如果从 v1.11.3 之前的 TiUP 版本升级至 v1.11.3 或更高 TiUP 版本，遥测保持升级前的开启或关闭状态。\n> - 从 TiSpark v3.0.3 开始，TiSpark 遥测功能默认关闭，即 TiSpark 默认不收集使用情况信息。\n> - 从 TiDB v8.1.0 起，TiDB 和 TiDB Dashboard 移除了遥测功能。\n\n## 开启遥测后哪些使用情况信息会被收集？\n\n以下章节具体描述了 TiUP 和 TiSpark 收集并分享的使用情况信息。若收集的使用情况信息有变化，将在版本更新说明中告知。\n\n> **注意：**\n>\n> 在**任何情况**下，集群中用户存储的数据都**不会**被收集。另请参阅 [PingCAP 隐私声明](https://pingcap.com/zh/privacy-policy/)。\n\n### TiUP\n\n当 TiUP 遥测功能开启时，执行 TiUP 命令时会将使用情况信息分享给 PingCAP，包括（但不限于）：\n\n- 随机生成的遥测标示符\n- TiUP 命令的执行情况，如命令执行是否成功、命令执行耗时等\n- 使用 TiUP 进行部署的情况，如部署的目标机器硬件信息、组件版本号、修改过的部署配置名称等\n\n使用 TiUP 时，可通过设置 `TIUP_CLUSTER_DEBUG=enable` 环境变量输出执行命令时收集的使用情况信息，例如：\n\n```shell\nTIUP_CLUSTER_DEBUG=enable tiup cluster list\n```\n\n### TiSpark\n\n当 TiSpark 遥测功能开启时，Spark 在使用 TiSpark 时会发送会将使用情况信息分享给 PingCAP，包括（但不限于）：\n\n- 随机生成的遥测标示符\n- TiSpark 的部分配置信息，如读取引擎、是否开启流式读取等\n- 用户集群部署情况，包括 TiSpark 所在节点的机器硬件信息、操作系统信息和组件版本号等\n\n使用 TiSpark 时，可以通过查看 Spark 日志来了解 TiSpark 收集的使用情况，可将 Spark 日志级别调至 INFO 或更低，例如：\n\n```shell\ngrep \"Telemetry report\" {spark.log} | tail -n 1\n```\n\n## 开启遥测功能\n\n### 开启 TiUP 遥测\n\n可通过执行以下命令开启 TiUP 遥测功能：\n\n```shell\ntiup telemetry enable\n```\n\n### 开启 TiSpark 遥测\n\n可以通过在 Spark 配置文件设置 `spark.tispark.telemetry.enable = true` 来开启 TiSpark 的遥测功能。\n\n## 禁用遥测功能\n\n### 禁用 TiUP 遥测\n\n可通过执行以下命令禁用 TiUP 遥测功能：\n\n```shell\ntiup telemetry disable\n```\n\n### 禁用 TiSpark 遥测\n\n可以通过在 Spark 配置文件设置 `spark.tispark.telemetry.enable = false` 来禁用 TiSpark 的遥测功能。\n\n## 查看遥测启用状态\n\n对于 TiUP 遥测，可通过执行以下命令查看遥测状态：\n\n```shell\ntiup telemetry status\n```\n\n## 使用情况信息合规性\n\n为了满足不同国家或地区对于此类信息的合规性要求，使用情况信息会按照不同的操作者 IP 地址发送到位于不同国家的服务器，具体如下：\n\n- 若为中国大陆 IP 地址，使用情况信息将会发送并存储于中国大陆境内的公有云服务器。\n- 若为中国大陆以外 IP 地址，使用情况信息将会发送并存储于美国的公有云服务器。\n\n可参阅 [PingCAP 隐私声明](https://pingcap.com/zh/privacy-policy/)了解详情。\n"
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "temporary-tables.md",
          "type": "blob",
          "size": 10.3291015625,
          "content": "---\ntitle: 临时表\nsummary: 了解 TiDB 中的临时表功能，使用临时表存储业务中间数据，减少表管理开销，并提升性能。\n---\n\n# 临时表\n\nTiDB 在 v5.3.0 版本中引入了临时表功能。该功能针对业务中间计算结果的临时存储问题，让用户免于频繁地建表和删表等操作。用户可将业务上的中间计算数据存入临时表，用完数据后 TiDB 自动清理回收临时表。这避免了用户业务过于复杂，减少了表管理开销，并提升了性能。\n\n本文介绍了 TiDB 临时表的使用场景、临时表类型、使用示例、限制临时表内存占用的方法、与其他 TiDB 功能的兼容性限制。\n\n## 使用场景\n\nTiDB 临时表主要应用于以下业务场景：\n\n- 缓存业务的中间临时数据，计算完成后将数据转储至普通表，临时表会自动释放。\n- 短期内对同一数据进行多次 DML 操作。例如在电商购物车应用中，添加、修改、删除商品及完成结算，并移除购物车信息。\n- 快速批量导入中间临时数据，提升导入临时数据的性能。\n- 批量更新数据。将数据批量导入到数据库的临时表，修改完成后再导出到文件。\n\n## 临时表类型\n\nTiDB 的临时表分为本地临时表和全局临时表：\n\n- 本地临时表的表定义和表内数据只对当前会话可见，适用于暂存会话内的中间数据。\n- 全局临时表的表定义对整个 TiDB 集群可见，表内数据只对当前事务可见，适用于暂存事务内的中间数据。\n\n## 本地临时表\n\n本地临时表的语义与 MySQL 临时表一致，它有以下特性：\n\n- 本地临时表的表定义不持久化，只在创建该表的会话内可见，其他会话无法访问该本地临时表\n- 不同会话可以创建同名的本地临时表，各会话只会读写该会话内创建的本地临时表\n- 本地临时表的数据对会话内的所有事务可见\n- 在会话结束后，该会话创建的本地临时表会被自动删除\n- 本地临时表可以与普通表同名，此时在 DDL 和 DML 语句中，普通表被隐藏，直到本地临时表被删除\n\n用户可通过 `CREATE TEMPORARY TABLE` 语句创建本地临时表，通过 `DROP TABLE` 或 `DROP TEMPORARY TABLE` 语句删除本地临时表。\n\n不同于 MySQL，TiDB 本地临时表都是外部表，SQL 语句不会创建内部临时表。\n\n### 本地临时表使用示例\n\n> **注意：**\n>\n> - 使用 TiDB 中的临时表前，注意临时表[与其他 TiDB 功能的兼容性限制](#与其他-tidb-功能的兼容性限制)以及[与 MySQL 临时表的兼容性](#与-mysql-临时表的兼容性)。\n> - 如果在 v5.3.0 升级前创建了本地临时表，这些临时表实际为普通表，在升级后也会被 TiDB 当成普通表处理。\n\n假设已存在普通表 `users`:\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE users (\n    id BIGINT,\n    name VARCHAR(100),\n    PRIMARY KEY(id)\n);\n```\n\n在会话 A 中创建本地临时表 `users`，不会有名字冲突。会话 A 访问 `users` 时，访问的是本地临时表 `users`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TEMPORARY TABLE users (\n    id BIGINT,\n    name VARCHAR(100),\n    city VARCHAR(50),\n    PRIMARY KEY(id)\n);\n```\n\n```\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n此时将数据插入 `users`，插入到的是会话 A 中的本地临时表 `users`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO users(id, name, city) VALUES(1001, 'Davis', 'LosAngeles');\n```\n\n```\nQuery OK, 1 row affected (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM users;\n```\n\n```\n+------+-------+------------+\n| id   | name  | city       |\n+------+-------+------------+\n| 1001 | Davis | LosAngeles |\n+------+-------+------------+\n1 row in set (0.00 sec)\n```\n\n在会话 B 中创建本地临时表 `users`，不会与普通表 `users` 冲突，也不会与会话 A 中的本地临时表 `users` 冲突。会话 B 内访问 `users` 时，访问的是会话 B 内创建的本地临时表 `users` 数据。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TEMPORARY TABLE users (\n    id BIGINT,\n    name VARCHAR(100),\n    city VARCHAR(50),\n    PRIMARY KEY(id)\n);\n```\n\n```\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n此时将数据插入 `users`，插入到的是会话 B 中的本地临时表 `users`。\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO users(id, name, city) VALUES(1001, 'James', 'NewYork');\n```\n\n```\nQuery OK, 1 row affected (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM users;\n```\n\n```\n+------+-------+---------+\n| id   | name  | city    |\n+------+-------+---------+\n| 1001 | James | NewYork |\n+------+-------+---------+\n1 row in set (0.00 sec)\n```\n\n### 与 MySQL 临时表的兼容性\n\nTiDB 本地临时表的以下特性与限制与 MySQL 一致：\n\n- 创建、删除本地临时表时，不会自动提交当前事务\n- 删除本地临时表所在的 schema 后，临时表不会被删除，仍然可以读写\n- 创建本地临时表需要 `CREATE TEMPORARY TABLES` 权限，随后对该表的所有操作不需要权限\n- 本地临时表不支持外键和分区表\n- 不支持基于本地临时表创建视图\n- `SHOW [FULL] TABLES` 不显示本地临时表\n\nTiDB 本地临时表与 MySQL 临时表有以下方面不兼容：\n\n- TiDB 本地临时表不支持 `ALTER TABLE`\n- TiDB 本地临时表忽略 `ENGINE` 表选项，始终在 TiDB 内存中暂存临时表数据，并且有[内存限制](#限制临时表的内存占用)\n- 当声明存储引擎为 `MEMORY` 时，TiDB 本地临时表没有 `MEMORY` 存储引擎的限制\n- 当声明存储引擎为 `INNODB` 或 `MYISAM` 时，TiDB 本地临时表忽略 InnoDB 临时表特有的系统变量\n- MySQL 不允许在同一条 SQL 中多次引用同一张临时表，而 TiDB 本地临时表没有该限制\n- MySQL 中用于显示临时表的 `information_schema.INNODB_TEMP_TABLE_INFO` 表在 TiDB 中不存在。TiDB 暂无用于显示本地临时表的系统表。\n- TiDB 没有内部临时表，MySQL 针对内部临时表的系统变量对 TiDB 不生效\n\n## 全局临时表\n\n全局临时表是 TiDB 的扩展功能，它有以下特性：\n\n- 全局临时表的表定义会持久化，对所有会话可见\n- 全局临时表的数据只对当前的事务内可见，事务结束后数据自动清空\n- 全局临时表不能与普通表同名\n\n用户可通过 `CREATE GLOBAL TEMPORARY TABLE` 语句创建全局临时表，语句末尾要加上 `ON COMMIT DELETE ROWS`。可通过 `DROP TABLE` 或 `DROP GLOBAL TEMPORARY TABLE` 语句删除全局临时表。\n\n### 全局临时表使用示例\n\n> **注意：**\n>\n> - 使用 TiDB 中的临时表前，注意临时表[与其他 TiDB 功能的兼容性限制](#与其他-tidb-功能的兼容性限制)。\n> - 如果在 v5.3.0 或以上版本中创建了全局临时表，这些临时表在降级后会被当作普通表处理，导致数据错误。\n\n在会话 A 中创建全局临时表 `users`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE GLOBAL TEMPORARY TABLE users (\n    id BIGINT,\n    name VARCHAR(100),\n    city VARCHAR(50),\n    PRIMARY KEY(id)\n) ON COMMIT DELETE ROWS;\n```\n\n```\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n写入 `users` 的数据对当前事务可见：\n\n{{< copyable \"sql\" >}}\n\n```sql\nBEGIN;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nINSERT INTO users(id, name, city) VALUES(1001, 'Davis', 'LosAngeles');\n```\n\n```\nQuery OK, 1 row affected (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM users;\n```\n\n```\n+------+-------+------------+\n| id   | name  | city       |\n+------+-------+------------+\n| 1001 | Davis | LosAngeles |\n+------+-------+------------+\n1 row in set (0.00 sec)\n```\n\n事务结束后数据自动被清空：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCOMMIT;\n```\n\n```\nQuery OK, 0 rows affected (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM users;\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n会话 A 创建了 `users` 后，会话 B 也可以读写该表：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM users;\n```\n\n```\nEmpty set (0.00 sec)\n```\n\n> **注意：**\n>\n> 如果事务是自动提交的，插入的数据在 SQL 语句执行结束后会被自动清空，导致后续 SQL 执行查找不到结果。因此应该使用非自动提交的事务读写全局临时表。\n\n## 限制临时表的内存占用\n\n无论定义表时声明的 `ENGINE` 是哪种存储引擎，本地临时表和全局临时表的数据都只暂存在 TiDB 实例的内存中，不持久化。\n\n为了避免内存溢出，用户可通过系统变量 [`tidb_tmp_table_max_size`](/system-variables.md#tidb_tmp_table_max_size-从-v53-版本开始引入) 限制每张临时表的大小。当临时表大小超过限制后 TiDB 会报错。`tidb_tmp_table_max_size` 的默认值是 `64MB`。\n\n例如，将每张临时表的大小限制为 `256MB`：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET GLOBAL tidb_tmp_table_max_size=268435456;\n```\n\n## 与其他 TiDB 功能的兼容性限制\n\n以下是本地临时表和全局临时表都不支持的功能：\n\n- 不支持 `AUTO_RANDOM` 列\n- 不支持 `SHARD_ROW_ID_BITS` 和 `PRE_SPLIT_REGIONS` 表选项\n- 不支持分区表\n- 不支持 `SPLIT REGION` 语句\n- 不支持 `ADMIN CHECK TABLE` 和 `ADMIN CHECKSUM TABLE` 语句\n- 不支持 `FLASHBACK TABLE` 和 `RECOVER TABLE` 语句\n- 不支持以临时表为源表执行 `CREATE TABLE LIKE` 语句\n- 不支持 Stale Read\n- 不支持外键\n- 不支持 SQL binding\n- 不支持添加 TiFlash 副本\n- 不支持在临时表上创建视图\n- 不支持 Placement Rules\n- 包含临时表的执行计划不会被 prepared plan cache 缓存\n\n以下是只有本地临时表不支持的功能：\n\n- 不支持通过系统变量 `tidb_snapshot` 读取历史数据\n\n## TiDB 数据迁移工具支持\n\n本地临时表只对当前会话可见，因此本地临时表不会被 TiDB 数据迁移工具导出、备份、同步。\n\n全局临时表的表定义全局可见，因此全局临时表的表定义会被 TiDB 数据迁移工具导出、备份、同步，但不导出数据。\n\n> **注意：**\n>\n> - TiCDC 必须使用 v5.3.0 及以上版本同步，否则下游集群的表定义错误\n> - BR 必须使用 v5.3.0 及以上版本备份，否则备份后的表定义错误\n> - 导入的集群、恢复后的集群、同步的下游集群需要支持全局临时表，否则报错\n\n## 另请参阅\n\n* [CREATE TABLE](/sql-statements/sql-statement-create-table.md)\n* [CREATE TABLE LIKE](/sql-statements/sql-statement-create-table-like.md)\n* [DROP TABLE](/sql-statements/sql-statement-drop-table.md)\n"
        },
        {
          "name": "three-data-centers-in-two-cities-deployment.md",
          "type": "blob",
          "size": 9.037109375,
          "content": "---\ntitle: 双区域多 AZ 部署 TiDB\nsummary: 介绍在两个区域多个可用区部署 TiDB 的方式。\n---\n\n# 双区域多 AZ 部署 TiDB\n\n本文档简要介绍双区域多可用区 (Availability Zone, AZ) 部署的架构模型及配置。\n\n本文中的区域指的是地理隔离的不同位置，AZ 指的是区域内部划分的相互独立的资源集合，本文描述的方案同样适用于在两个城市部署三个数据中心（两地三中心）的场景。\n\n## 简介\n\n双区域多 AZ 架构，即生产数据 AZ、同区域灾备 AZ、跨区域灾备 AZ 的高可用容灾方案。在这种模式下，两个区域的三个 AZ 互联互通，如果一个 AZ 发生故障或灾难，其他 AZ 可以正常运行并对关键业务或全部业务实现接管。相比同区域多 AZ 方案，双区域三 AZ 具有跨区域级高可用能力，可以应对区域级自然灾害。\n\nTiDB 分布式数据库采用 Raft 算法，可以原生支持双区域三 AZ 架构，并保证集群数据的一致性和高可用。而且，同区域 AZ 网络延迟相对较小，可以把业务流量同时派发到同区域两个 AZ，并通过控制 Region Leader 和 PD Leader 分布实现同区域 AZ 共同负载业务流量。\n\n## 架构\n\n本文以北京和西安部署集群为例，阐述 TiDB 分布式数据库双区域三 AZ 架构的部署模型。\n\n假设北京有两个 AZ，AZ1 和 AZ2，西安有一个 AZ，AZ3。北京同区域两 AZ 之间网络延迟低于 3 ms，北京与西安之间的网络使用 ISP 专线，延迟约为 20 ms。\n\n下图为集群部署架构图，具体如下：\n\n- 集群采用双区域三 AZ 部署方式，分别为北京 AZ1，北京 AZ2，西安 AZ3。\n- 集群采用 5 副本模式，其中 AZ1 和 AZ2 分别放 2 份副本，AZ3 放 1 份副本；TiKV 按机柜设置 Label，即每个机柜上有 1 份副本。\n- 副本间通过 Raft 协议保证数据的一致性和高可用，对用户完全透明。\n\n![双区域三 AZ 集群架构图](/media/three-data-centers-in-two-cities-deployment-01.png)\n\n该架构具备高可用能力，同时通过 PD 调度保证 Region Leader 只出现在同区域的两个 AZ。相比于三 AZ，即 Region Leader 分布不受限制的方案，双区域三 AZ 方案有以下优缺点：\n\n- **优点**\n\n    - Region Leader 都在同区域 AZ，延迟低，数据写入速度更优。\n    - 双 AZ 可同时对外提供服务，资源利用率更高。\n    - 任一 AZ 失效后，另一 AZ 接管服务，业务可用并且不发生数据丢失。\n\n- **缺点**\n\n    - 因为数据一致性是基于 Raft 算法实现，当同区域两个 AZ 同时失效时，因为远程灾备 AZ 只剩下一份副本，不满足 Raft 算法大多数副本存活的要求。最终将导致集群暂时不可用，需要从一副本恢复集群，丢失少部分还没同步的热数据。这种情况出现概率较小。\n    - 由于使用了网络专线，该架构下网络设施成本较高。\n    - 双区域三 AZ 需设置 5 副本，数据冗余度增加，空间成本攀升。\n\n### 详细示例\n\n北京、西安双区域三 AZ 配置详解：\n\n![双区域三 AZ 配置详图](/media/three-data-centers-in-two-cities-deployment-02.png)\n\n如上图所示，北京有两个可用区 AZ1 和 AZ2，可用区 AZ1 有三套机架 rac1、rac2 和 rac3，可用区 AZ2 有两套机架 rac4 和 rac5；西安可用区 AZ3 有一套机架 rac6。\n\nAZ1 的 rac1 机架中，一台服务器部署了 TiDB 和 PD 服务，另外两台服务器部署了 TiKV 服务，其中，每台 TiKV 服务器部署了两个 TiKV 实例 (tikv-server)，rac2、rac4、rac5 和 rac6 类似。\n\n机架 rac3 上部署了 TiDB Server、中控及监控服务器。TiDB Server 用于日常管理维护和备份。中控和监控服务器上部署了 Prometheus、Grafana 以及恢复工具。\n\n## 配置\n\n### 示例\n\n以下为一个 `tiup topology.yaml` 文件示例：\n\n```\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/data/tidb_cluster/tidb-deploy\"\n  data_dir: \"/data/tidb_cluster/tidb-data\"\n\nserver_configs:\n  tikv:\n    server.grpc-compression-type: gzip\n  pd:\n    replication.location-labels:  [\"dc\",\"zone\",\"rack\",\"host\"]\n\npd_servers:\n  - host: 10.63.10.10\n    name: \"pd-10\"\n  - host: 10.63.10.11\n    name: \"pd-11\"\n  - host: 10.63.10.12\n    name: \"pd-12\"\n  - host: 10.63.10.13\n    name: \"pd-13\"\n  - host: 10.63.10.14\n    name: \"pd-14\"\n\ntidb_servers:\n  - host: 10.63.10.10\n  - host: 10.63.10.11\n  - host: 10.63.10.12\n  - host: 10.63.10.13\n  - host: 10.63.10.14\n\ntikv_servers:\n  - host: 10.63.10.30\n    config:\n      server.labels: { az: \"1\", replication zone: \"1\", rack: \"1\", host: \"30\" }\n  - host: 10.63.10.31\n    config:\n      server.labels: { az: \"1\", replication zone: \"2\", rack: \"2\", host: \"31\" }\n  - host: 10.63.10.32\n    config:\n      server.labels: { az: \"2\", replication zone: \"3\", rack: \"3\", host: \"32\" }\n  - host: 10.63.10.33\n    config:\n      server.labels: { az: \"2\", replication zone: \"4\", rack: \"4\", host: \"33\" }\n  - host: 10.63.10.34\n    config:\n      server.labels: { az: \"3\", replication zone: \"5\", rack: \"5\", host: \"34\" }\n      raftstore.raft-min-election-timeout-ticks: 50\n      raftstore.raft-max-election-timeout-ticks: 60\n\nmonitoring_servers:\n  - host: 10.63.10.60\n\ngrafana_servers:\n  - host: 10.63.10.60\n\nalertmanager_servers:\n  - host: 10.63.10.60\n```\n\n### Labels 设计\n\n在双区域三 AZ 部署方式下，对于 Labels 的设计需要充分考虑到系统的可用性和容灾能力，建议根据部署的物理结构来定义 AZ、replication zone、rack 和 host 四个等级。\n\n![Label 逻辑定义图](/media/three-data-centers-in-two-cities-deployment-03.png)\n\nPD 设置中添加 TiKV label 的等级配置。\n\n```\nserver_configs:\n  pd:\n    replication.location-labels:  [\"az\",\"replication zone\",\"rack\",\"host\"]\n```\n\ntikv_servers 设置基于 TiKV 真实物理部署位置的 Label 信息，方便 PD 进行全局管理和调度。\n\n```\ntikv_servers:\n  - host: 10.63.10.30\n    config:\n      server.labels: { az: \"1\", replication zone: \"1\", rack: \"1\", host: \"30\" }\n  - host: 10.63.10.31\n    config:\n      server.labels: { az: \"1\", replication zone: \"2\", rack: \"2\", host: \"31\" }\n  - host: 10.63.10.32\n    config:\n      server.labels: { az: \"2\", replication zone: \"3\", rack: \"3\", host: \"32\" }\n  - host: 10.63.10.33\n    config:\n      server.labels: { az: \"2\", replication zone: \"4\", rack: \"4\", host: \"33\" }\n  - host: 10.63.10.34\n    config:\n      server.labels: { az: \"3\", replication zone: \"5\", rack: \"5\", host: \"34\" }\n```\n\n### 参数配置优化\n\n在双区域三 AZ 的架构部署中，从性能优化的角度，除了常规参数配置外，还需要对集群中相关组件参数进行调整。\n\n- 启用 TiKV gRPC 消息压缩。由于需要在网络中传输集群数据，可开启 gRPC 消息压缩，降低网络流量。\n\n    ```\n    server.grpc-compression-type: gzip\n    ```\n\n- 优化跨区域 AZ3 的 TiKV 节点网络，修改 TiKV 的如下参数，拉长跨区域副本参与选举的时间，避免跨区域 TiKV 中的副本参与 Raft 选举。\n\n    ```\n    raftstore.raft-min-election-timeout-ticks: 50\n    raftstore.raft-max-election-timeout-ticks: 60\n    ```\n\n> **注意:**\n>\n> 通过 `raftstore.raft-min-election-timeout-ticks` 和 `raftstore.raft-max-election-timeout-ticks` 为 TiKV 节点配置较大的 election timeout tick 可以大幅降低该节点上的 Region 成为 Leader 的概率。但在发生灾难的场景中，如果部分 TiKV 节点宕机，而其它存活的 TiKV 节点 Raft 日志落后，此时只有这个配置了较大的 election timeout tick 的 TiKV 节点上的 Region 能成为 Leader。由于此 TiKV 节点上的 Region 需要至少等待 `raftstore.raft-min-election-timeout-ticks` 设置的时间后才能发起选举，因此尽量避免将此配置值设置得过大，以免在这种场景下影响集群的可用性。\n\n- 调度设置。在集群启动后，通过 `tiup ctl:v<CLUSTER_VERSION> pd` 工具进行调度策略修改。修改 TiKV Raft 副本数按照安装时规划好的副本数进行设置，在本例中为 5 副本。\n\n    ```\n    config set max-replicas 5\n    ```\n\n- 禁止向跨区域 AZ 调度 Raft Leader，当 Raft Leader 在跨区域 AZ 时，会造成不必要的本区域 AZ 与远程 AZ 间的网络消耗，同时，网络带宽和延迟也会对 TiDB 的集群性能产生影响。\n\n    ```\n    config set label-property reject-leader dc 3\n    ```\n\n    > **注意：**\n    >\n    > TiDB 5.2 及以上版本默认不支持 `label-property` 配置。若要设置副本策略，请使用 [Placement Rules](/configure-placement-rules.md)。\n\n- 设置 PD 的优先级，为了避免出现跨区域 AZ 的 PD 成为 Leader，可以将本区域 AZ 的 PD 优先级调高（数字越大，优先级越高），将跨区域的 PD 优先级调低。在可用的 PD 节点中，优先级数值最大的节点会直接当选 leader。\n\n    ```\n    member leader_priority PD-10 5\n    member leader_priority PD-11 5\n    member leader_priority PD-12 5\n    member leader_priority PD-13 5\n    member leader_priority PD-14 1\n    ```\n"
        },
        {
          "name": "ticdc-deployment-topology.md",
          "type": "blob",
          "size": 6.703125,
          "content": "---\ntitle: TiCDC 部署拓扑\nsummary: 介绍 TiCDC 部署 TiDB 集群的拓扑结构。\naliases: ['/docs-cn/dev/ticdc-deployment-topology/','/docs-cn/dev/reference/tools/ticdc/deploy/','/docs-cn/dev/ticdc/deploy-ticdc/']\n---\n\n# TiCDC 部署拓扑\n\n> **注意：**\n>\n> TiCDC 从 v4.0.6 起成为正式功能，可用于生产环境。\n\n本文介绍 [TiCDC](/ticdc/ticdc-overview.md) 部署的拓扑，以及如何在最小拓扑的基础上同时部署 TiCDC。TiCDC 是 4.0 版本开始支持的 TiDB 增量数据同步工具，支持多种下游（TiDB、MySQL、Kafka、MQ、存储服务等），有延迟低、天然高可用等优点。\n\n## 拓扑信息\n\n|实例 | 个数 | 物理机配置 | IP |配置 |\n| :-- | :-- | :-- | :-- | :-- |\n| TiDB |3 | 16 VCore 32GB * 1 | 10.0.1.1 <br/> 10.0.1.2 <br/> 10.0.1.3 | 默认端口 <br/>  全局目录配置 |\n| PD | 3 | 4 VCore 8GB * 1 |10.0.1.4 <br/> 10.0.1.5 <br/> 10.0.1.6 | 默认端口 <br/> 全局目录配置 |\n| TiKV | 3 | 16 VCore 32GB 2TB (nvme ssd) * 1 | 10.0.1.7 <br/> 10.0.1.8 <br/> 10.0.1.9 | 默认端口 <br/> 全局目录配置 |\n| CDC | 3 | 8 VCore 16GB * 1 | 10.0.1.11 <br/> 10.0.1.12 <br/> 10.0.1.13 | 默认端口 <br/> 全局目录配置 |\n| Monitoring & Grafana | 1 | 4 VCore 8GB * 1 500GB (ssd) | 10.0.1.11 | 默认端口 <br/> 全局目录配置 |\n\n### 拓扑模版\n\n<details>\n<summary>简单 TiCDC 配置模板</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\npd_servers:\n  - host: 10.0.1.4\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.1\n  - host: 10.0.1.2\n  - host: 10.0.1.3\n\ntikv_servers:\n  - host: 10.0.1.7\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\ncdc_servers:\n  - host: 10.0.1.7\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\nmonitoring_servers:\n  - host: 10.0.1.10\n\ngrafana_servers:\n  - host: 10.0.1.10\n\nalertmanager_servers:\n  - host: 10.0.1.10\n```\n\n</details>\n\n<details>\n<summary>详细 TiCDC 配置模板</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\n# # Monitored variables are applied to all the machines.\nmonitored:\n  node_exporter_port: 9100\n  blackbox_exporter_port: 9115\n  # deploy_dir: \"/tidb-deploy/monitored-9100\"\n  # data_dir: \"/tidb-data/monitored-9100\"\n  # log_dir: \"/tidb-deploy/monitored-9100/log\"\n\n# # Server configs are used to specify the runtime configuration of TiDB components.\n# # All configuration items can be found in TiDB docs:\n# # - TiDB: https://docs.pingcap.com/zh/tidb/stable/tidb-configuration-file\n# # - TiKV: https://docs.pingcap.com/zh/tidb/stable/tikv-configuration-file\n# # - PD: https://docs.pingcap.com/zh/tidb/stable/pd-configuration-file\n# # All configuration items use points to represent the hierarchy, e.g:\n# #   readpool.storage.use-unified-pool\n# #\n# # You can overwrite this configuration via the instance-level `config` field.\n\nserver_configs:\n  tidb:\n    log.slow-threshold: 300\n  tikv:\n    # server.grpc-concurrency: 4\n    # raftstore.apply-pool-size: 2\n    # raftstore.store-pool-size: 2\n    # rocksdb.max-sub-compactions: 1\n    # storage.block-cache.capacity: \"16GB\"\n    # readpool.unified.max-thread-count: 12\n    readpool.storage.use-unified-pool: false\n    readpool.coprocessor.use-unified-pool: true\n  pd:\n    schedule.leader-schedule-limit: 4\n    schedule.region-schedule-limit: 2048\n    schedule.replica-schedule-limit: 64\n  cdc:\n    # capture-session-ttl: 10\n    # sorter.sort-dir: \"/tmp/cdc_sort\"\n    # gc-ttl: 86400\n\npd_servers:\n  - host: 10.0.1.4\n    # ssh_port: 22\n    # name: \"pd-1\"\n    # client_port: 2379\n    # peer_port: 2380\n    # deploy_dir: \"/tidb-deploy/pd-2379\"\n    # data_dir: \"/tidb-data/pd-2379\"\n    # log_dir: \"/tidb-deploy/pd-2379/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.pd` values.\n    # config:\n    #   schedule.max-merge-region-size: 20\n    #   schedule.max-merge-region-keys: 200000\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.1\n    # ssh_port: 22\n    # port: 4000\n    # status_port: 10080\n    # deploy_dir: \"/tidb-deploy/tidb-4000\"\n    # log_dir: \"/tidb-deploy/tidb-4000/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tidb` values.\n    # config:\n    #   log.slow-query-file: tidb-slow-overwrited.log\n  - host: 10.0.1.2\n  - host: 10.0.1.3\n\ntikv_servers:\n  - host: 10.0.1.7\n    # ssh_port: 22\n    # port: 20160\n    # status_port: 20180\n    # deploy_dir: \"/tidb-deploy/tikv-20160\"\n    # data_dir: \"/tidb-data/tikv-20160\"\n    # log_dir: \"/tidb-deploy/tikv-20160/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tikv` values.\n    # config:\n    #   server.grpc-concurrency: 4\n    #   server.labels: { zone: \"zone1\", dc: \"dc1\", host: \"host1\" }\n\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\ncdc_servers:\n  - host: 10.0.1.1\n    port: 8300\n    deploy_dir: \"/tidb-deploy/cdc-8300\"\n    data_dir: \"/tidb-data/cdc-8300\"\n    log_dir: \"/tidb-deploy/cdc-8300/log\"\n    # gc-ttl: 86400\n    # ticdc_cluster_id: \"cluster1\"\n  - host: 10.0.1.2\n    port: 8300\n    deploy_dir: \"/tidb-deploy/cdc-8300\"\n    data_dir: \"/tidb-data/cdc-8300\"\n    log_dir: \"/tidb-deploy/cdc-8300/log\"\n    # gc-ttl: 86400\n    # ticdc_cluster_id: \"cluster1\"\n  - host: 10.0.1.3\n    port: 8300\n    deploy_dir: \"/tidb-deploy/cdc-8300\"\n    data_dir: \"/tidb-data/cdc-8300\"\n    log_dir: \"/tidb-deploy/cdc-8300/log\"\n    # gc-ttl: 86400\n    # ticdc_cluster_id: \"cluster2\"\n\nmonitoring_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # port: 9090\n    # deploy_dir: \"/tidb-deploy/prometheus-8249\"\n    # data_dir: \"/tidb-data/prometheus-8249\"\n    # log_dir: \"/tidb-deploy/prometheus-8249/log\"\n\ngrafana_servers:\n  - host: 10.0.1.10\n    # port: 3000\n    # deploy_dir: /tidb-deploy/grafana-3000\n\nalertmanager_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # web_port: 9093\n    # cluster_port: 9094\n    # deploy_dir: \"/tidb-deploy/alertmanager-9093\"\n    # data_dir: \"/tidb-data/alertmanager-9093\"\n    # log_dir: \"/tidb-deploy/alertmanager-9093/log\"\n```\n\n</details>\n\n以上 TiDB 集群拓扑文件中，详细的配置项说明见[通过 TiUP 部署 TiDB 集群的拓扑文件配置](/tiup/tiup-cluster-topology-reference.md#cdc_servers)。\n\n> **注意：**\n>\n> - 无需手动创建配置文件中的 `tidb` 用户，TiUP cluster 组件会在目标主机上自动创建该用户。可以自定义用户，也可以和中控机的用户保持一致。\n> - 如果部署目录配置为相对路径，会部署在用户的 Home 目录下。\n"
        },
        {
          "name": "ticdc-performance-tuning-methods.md",
          "type": "blob",
          "size": 5.3623046875,
          "content": "---\ntitle: TiCDC 性能分析和优化方法\nsummary: 本文介绍了 Performance Overview 面板中的 TiCDC 部分，帮助你了解和监控 TiCDC 工作负载。\n---\n\n# TiCDC 性能分析和优化方法\n\n本文介绍 TiCDC 资源使用率和关键的性能指标。你可以通过 Performance Overview 面板中的 [CDC 面板](/grafana-performance-overview-dashboard.md#cdc)来监控和评估 TiCDC 同步数据的性能。\n\n## TiCDC 集群的资源利用率\n\n通过以下三个指标，你可以快速判断 TiCDC 集群的资源使用率：\n\n- CPU usage：TiCDC 节点的 CPU 使用情况\n- Memory usage：TiCDC 节点的内存使用情况\n- Goroutine count：TiCDC 节点 Goroutine 的个数\n\n## TiCDC 数据同步关键指标\n\n### TiCDC 整体指标\n\n通过以下指标，你可以了解 TiCDC 数据同步的整体情况：\n\n- Changefeed checkpoint lag：同步任务上下游数据的进度差，以时间单位秒计算。\n\n    如果 TiCDC 消费数据的速度和写入下游的速度能跟上上游的数据变更，该指标将保持在较小的延迟范围内，通常是 10 秒以内。如果 TiCDC 消费数据的速度和写入下游的速度跟不上上游的数据变更，则该指标将持续增长。\n\n    该指标增长（即 TiCDC checkpoint lag 增长）的常见原因如下：\n\n    - 系统资源不足：如果 TiCDC 系统中的 CPU、内存或磁盘空间不足，可能会导致数据处理速度过慢，从而导致 TiCDC Changefeed checkpoint 过长。\n    - 网络问题：如果 TiCDC 系统中存在网络中断、延迟或带宽不足的问题，可能会影响数据的传输速度，从而导致 TiCDC Changefeed checkpoint 过长。\n    - 上游 QPS 过高：如果 TiCDC 系统需要处理的数据量过大，可能会导致数据处理超时，从而导致 TiCDC Changefeed checkpoint 增长，通常一个 TiCDC 节点处理的 QPS 上限为 60K 左右。\n    - 数据库问题：\n        - 上游 TiKV 集群 `min resolved ts` 和最新的 PD TSO 差距过大，通常是因为上游写入负载过大，TiKV 无法及时推进 resolved ts 造成。\n        - 下游数据库写入延迟高导致 TiCDC 无法及时将数据同步到下游。\n\n- Changefeed resolved ts lag：TiCDC 节点内部同步状态与上游的进度差，以时间单位秒计算。如果 TiCDC Changefeed resolved ts lag 值很高，可能意味着 TiCDC 系统的 Puller 或者 Sorter 模块数据处理能力不足，或者可能存在网络延迟或磁盘读写速度慢的问题。在这种情况下，需要采取适当的措施，例如增加 TiCDC 实例数量或优化网络配置，以确保 TiCDC 系统的高效和稳定运行。\n- The status of changefeeds：Changefeed 各状态的解释，请参考 [Changefeed 状态流转](/ticdc/ticdc-changefeed-overview.md)。\n\n示例 1：单个 TiCDC 节点上游 QPS 过高导致 checkpoint lag 过高\n\n如下图所示，因为上游 QPS 过高，该集群中只有单个 TiCDC 节点，TiCDC 节点处于过载状态，CPU 使用率较高，Changefeed checkpoint lag 和 Changefeed resolved ts lag 持续增长。Changefeeds 的状态间歇性地从 0 变为 1，意味着 changefeed 不断出错。你可尝试通过增加资源解决该问题：\n\n- 添加 TiCDC 节点：将 TiCDC 集群扩展到多个节点，以增加处理能力。\n- 优化 TiCDC 节点的资源：提高 TiCDC 节点的 CPU 和内存配置，以改善性能。\n\n![TiCDC overview](/media/performance/cdc/cdc-slow.png)\n\n### 数据流吞吐指标和下游延迟信息\n\n通过以下指标，你可以了解数据流的吞吐和下游延迟信息：\n\n- Puller output events/s：TiCDC 节点中 Puller 模块每秒输出到 Sorter 模块的数据变更行数\n- Sorter output events/s：TiCDC 节点中 Sorter 模块每秒输出到 Mounter 模块的行数\n- Mounter output events/s：TiCDC 节点中 Mounter 模块每秒输出到 Sink 模块的行数\n- Table sink output events/s：TiCDC 节点中 Table Sorter 模块每秒输出到 Sink 模块的行数\n- SinkV2 - Sink flush rows/s：TiCDC 节点中 Sink 模块每秒输出到下游的行数\n- Transaction Sink Full Flush Duration：TiCDC 节点中 MySQL Sink 写下游事务的平均延迟和 p999 延迟\n- MQ Worker Send Message Duration Percentile：下游为 Kafka 时 MQ worker 发送消息的延迟\n- Kafka Outgoing Bytes：MQ Workload 写下游事务的流量\n\n示例 2：下游数据库写入速度对 TiCDC 数据同步性能的影响\n\n如下图所示，该环境上下游都为 TiDB 集群。通过 `TiCDC Puller output events/s` 可以确认上游数据库的 QPS 值。通过 `Transaction Sink Full Flush Duration` 可以确认，第一段负载的下游数据库平均写入延迟高，第二段负载的下游平均写入延迟低。\n\n- 在第一段负载期间，由于下游 TiDB 集群写入数据缓慢，导致 TiCDC 消费数据的速度跟不上上游的 QPS，引起 Changefeed checkpoint lag 不断增长。然而，Changefeed resolved ts lag 仍然在 300 毫秒以内，说明同步延迟和吞吐瓶颈不在 puller 和 sorter 模块中，而在下游的 sink 模块。\n- 在第二段负载期间，因为下游 TiDB 集群的写入速度快，TiCDC 同步数据的速度完全追上了上游的速度，因此 Changefeed checkpoint lag 和 Changefeed resolved ts lag 保持在 500 毫秒以内，此时 TiCDC 的同步速度较为理想。\n\n![TiCDC overview](/media/performance/cdc/cdc-fast-1.png)\n\n![data flow and txn latency](/media/performance/cdc/cdc-fast-2.png)"
        },
        {
          "name": "ticdc",
          "type": "tree",
          "content": null
        },
        {
          "name": "tidb-architecture.md",
          "type": "blob",
          "size": 3.443359375,
          "content": "---\ntitle: TiDB 整体架构\nsummary: 了解 TiDB 的整体架构。\naliases: ['/docs-cn/dev/tidb-architecture/','/docs-cn/dev/architecture/']\n---\n\n# TiDB 整体架构\n\n推荐先观看以下视频（时长约 14 分钟），快速了解 TiDB 的整体架构。\n\n<video src=\"https://download.pingcap.com/docs-cn/tidb_architecture.mp4\" width=\"100%\" height=\"100%\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_tidb_architecture.png\"></video>\n\n与传统的单机数据库相比，TiDB 具有以下优势：\n\n* 纯分布式架构，拥有良好的扩展性，支持弹性的扩缩容\n* 支持 SQL，对外暴露 MySQL 的网络协议，并兼容大多数 MySQL 的语法，在大多数场景下可以直接替换 MySQL\n* 默认支持高可用，在少数副本失效的情况下，数据库本身能够自动进行数据修复和故障转移，对业务透明\n* 支持 ACID 事务，对于一些有强一致需求的场景友好，例如：银行转账\n* 具有丰富的工具链生态，覆盖数据迁移、同步、备份等多种场景\n\n在内核设计上，TiDB 分布式数据库将整体架构拆分成了多个模块，各模块之间互相通信，组成完整的 TiDB 系统。对应的架构图如下：\n\n![architecture](/media/tidb-architecture-v6.png)\n\n- [TiDB Server](/tidb-computing.md)：SQL 层，对外暴露 MySQL 协议的连接 endpoint，负责接受客户端的连接，执行 SQL 解析和优化，最终生成分布式执行计划。TiDB 层本身是无状态的，实践中可以启动多个 TiDB 实例，通过负载均衡组件（如 TiProxy、LVS、HAProxy、ProxySQL 或 F5）对外提供统一的接入地址，客户端的连接可以均匀地分摊在多个 TiDB 实例上以达到负载均衡的效果。TiDB Server 本身并不存储数据，只是解析 SQL，将实际的数据读取请求转发给底层的存储节点 TiKV（或 TiFlash）。\n\n- [PD (Placement Driver) Server](/tidb-scheduling.md)：整个 TiDB 集群的元信息管理模块，负责存储每个 TiKV 节点实时的数据分布情况和集群的整体拓扑结构，提供 TiDB Dashboard 管控界面，并为分布式事务分配事务 ID。PD 不仅存储元信息，同时还会根据 TiKV 节点实时上报的数据分布状态，下发数据调度命令给具体的 TiKV 节点，可以说是整个集群的“大脑”。此外，PD 本身也是由至少 3 个节点构成，拥有高可用的能力。建议部署奇数个 PD 节点。\n\n- 存储节点\n    - [TiKV Server](/tidb-storage.md)：负责存储数据，从外部看 TiKV 是一个分布式的提供事务的 Key-Value 存储引擎。存储数据的基本单位是 Region，每个 Region 负责存储一个 Key Range（从 StartKey 到 EndKey 的左闭右开区间）的数据，每个 TiKV 节点会负责多个 Region。TiKV 的 API 在 KV 键值对层面提供对分布式事务的原生支持，默认提供了 SI (Snapshot Isolation) 的隔离级别，这也是 TiDB 在 SQL 层面支持分布式事务的核心。TiDB 的 SQL 层做完 SQL 解析后，会将 SQL 的执行计划转换为对 TiKV API 的实际调用。所以，数据都存储在 TiKV 中。另外，TiKV 中的数据都会自动维护多副本（默认为三副本），天然支持高可用和自动故障转移。\n    - [TiFlash](/tiflash/tiflash-overview.md)：TiFlash 是一类特殊的存储节点。和普通 TiKV 节点不一样的是，在 TiFlash 内部，数据是以列式的形式进行存储，主要的功能是为分析型的场景加速。\n"
        },
        {
          "name": "tidb-computing.md",
          "type": "blob",
          "size": 9.63671875,
          "content": "---\ntitle: TiDB 数据库的计算\nsummary: 了解 TiDB 数据库的计算层。\naliases: ['/docs-cn/dev/tidb-computing/']\n---\n\n# TiDB 数据库的计算\n\nTiDB 在 TiKV 提供的分布式存储能力基础上，构建了兼具优异的交易处理能力与良好的数据分析能力的计算引擎。本文首先从数据映射算法入手介绍 TiDB 如何将库表中的数据映射到 TiKV 中的 (Key, Value) 键值对，然后描述 TiDB 元信息管理方式，最后介绍 TiDB SQL 层的主要架构。\n\n对于计算层依赖的存储方案，本文只介绍基于 TiKV 的行存储结构。针对分析型业务的特点，TiDB 推出了作为 TiKV 扩展的列存储方案 [TiFlash](/tiflash/tiflash-overview.md)。\n\n## 表数据与 Key-Value 的映射关系\n\n本小节介绍 TiDB 中数据到 (Key, Value) 键值对的映射方案。这里的数据主要包括以下两个方面：\n\n- 表中每一行的数据，以下简称表数据\n- 表中所有索引的数据，以下简称索引数据\n\n### 表数据与 Key-Value 的映射关系\n\n在关系型数据库中，一个表可能有很多列。要将一行中各列数据映射成一个 (Key, Value) 键值对，需要考虑如何构造 Key。首先，OLTP 场景下有大量针对单行或者多行的增、删、改、查等操作，要求数据库具备快速读取一行数据的能力。因此，对应的 Key 最好有一个唯一 ID（显示或隐式的 ID），以方便快速定位。其次，很多 OLAP 型查询需要进行全表扫描。如果能够将一个表中所有行的 Key 编码到一个区间内，就可以通过范围查询高效完成全表扫描的任务。\n\n基于上述考虑，TiDB 中的表数据与 Key-Value 的映射关系作了如下设计：\n\n- 为了保证同一个表的数据放在一起，方便查找，TiDB 会为每个表分配一个表 ID，用 `TableID` 表示。表 ID 是一个整数，在整个集群内唯一。\n- TiDB 会为表中每行数据分配一个行 ID，用 `RowID` 表示。行 ID 也是一个整数，在表内唯一。对于行 ID，TiDB 做了一个小优化，如果某个表有整数型的主键，TiDB 会使用主键的值当做这一行数据的行 ID。\n\n每行数据按照如下规则编码成 (Key, Value) 键值对：\n\n```\nKey:   tablePrefix{TableID}_recordPrefixSep{RowID}\nValue: [col1, col2, col3, col4]\n```\n\n其中 `tablePrefix` 和 `recordPrefixSep` 都是特定的字符串常量，用于在 Key 空间内区分其他数据。其具体值在后面的小结中给出。\n\n### 索引数据和 Key-Value 的映射关系\n\nTiDB 同时支持主键和二级索引（包括唯一索引和非唯一索引）。与表数据映射方案类似，TiDB 为表中每个索引分配了一个索引 ID，用 `IndexID` 表示。\n\n对于主键和唯一索引，需要根据键值快速定位到对应的 RowID，因此，按照如下规则编码成 (Key, Value) 键值对：\n\n```\nKey:   tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue\nValue: RowID\n```\n\n对于不需要满足唯一性约束的普通二级索引，一个键值可能对应多行，需要根据键值范围查询对应的 RowID。因此，按照如下规则编码成 (Key, Value) 键值对：\n\n```\nKey:   tablePrefix{TableID}_indexPrefixSep{IndexID}_indexedColumnsValue_{RowID}\nValue: null\n```\n\n### 映射关系小结\n\n上述所有编码规则中的 `tablePrefix`、`recordPrefixSep` 和 `indexPrefixSep` 都是字符串常量，用于在 Key 空间内区分其他数据，定义如下：\n\n```\ntablePrefix     = []byte{'t'}\nrecordPrefixSep = []byte{'r'}\nindexPrefixSep  = []byte{'i'}\n```\n\n另外请注意，上述方案中，无论是表数据还是索引数据的 Key 编码方案，一个表内所有的行都有相同的 Key 前缀，一个索引的所有数据也都有相同的前缀。这样具有相同的前缀的数据，在 TiKV 的 Key 空间内，是排列在一起的。因此只要小心地设计后缀部分的编码方案，保证编码前和编码后的比较关系不变，就可以将表数据或者索引数据有序地保存在 TiKV 中。采用这种编码后，一个表的所有行数据会按照 `RowID` 顺序地排列在 TiKV 的 Key 空间中，某一个索引的数据也会按照索引数据的具体的值（编码方案中的 `indexedColumnsValue`）顺序地排列在 Key 空间内。\n\n### Key-Value 映射关系示例\n\n最后通过一个简单的例子，来理解 TiDB 的 Key-Value 映射关系。假设 TiDB 中有如下这个表：\n\n```sql\nCREATE TABLE User (\n    ID int,\n    Name varchar(20),\n    Role varchar(20),\n    Age int,\n    PRIMARY KEY (ID),\n    KEY idxAge (Age)\n);\n```\n\n假设该表中有 3 行数据：\n\n```\n1, \"TiDB\", \"SQL Layer\", 10\n2, \"TiKV\", \"KV Engine\", 20\n3, \"PD\", \"Manager\", 30\n```\n\n首先每行数据都会映射为一个 (Key, Value) 键值对，同时该表有一个 `int` 类型的主键，所以 `RowID` 的值即为该主键的值。假设该表的 `TableID` 为 10，则其存储在 TiKV 上的表数据为：\n\n```\nt10_r1 --> [\"TiDB\", \"SQL Layer\", 10]\nt10_r2 --> [\"TiKV\", \"KV Engine\", 20]\nt10_r3 --> [\"PD\", \"Manager\", 30]\n```\n\n除了主键外，该表还有一个非唯一的普通二级索引 `idxAge`，假设这个索引的 `IndexID` 为 1，则其存储在 TiKV 上的索引数据为：\n\n```\nt10_i1_10_1 --> null\nt10_i1_20_2 --> null\nt10_i1_30_3 --> null\n```\n\n以上例子展示了 TiDB 中关系模型到 Key-Value 模型的映射规则，以及选择该方案背后的考量。\n\n## 元信息管理\n\nTiDB 中每个 `Database` 和 `Table` 都有元信息，也就是其定义以及各项属性。这些信息也需要持久化，TiDB 将这些信息也存储在了 TiKV 中。\n\n每个 `Database`/`Table` 都被分配了一个唯一的 ID，这个 ID 作为唯一标识，并且在编码为 Key-Value 时，这个 ID 都会编码到 Key 中，再加上 `m_` 前缀。这样可以构造出一个 Key，Value 中存储的是序列化后的元信息。\n\n除此之外，TiDB 还用一个专门的 (Key, Value) 键值对存储当前所有表结构信息的最新版本号。这个键值对是全局的，每次 DDL 操作的状态改变时其版本号都会加 1。目前，TiDB 把这个键值对持久化存储在 PD Server 中，其 Key 是 \"/tidb/ddl/global_schema_version\"，Value 是类型为 int64 的版本号值。TiDB 采用 Online Schema 变更算法，有一个后台线程在不断地检查 PD Server 中存储的表结构信息的版本号是否发生变化，并且保证在一定时间内一定能够获取版本的变化。\n\n## SQL 层简介\n\nTiDB 的 SQL 层，即 TiDB Server，负责将 SQL 翻译成 Key-Value 操作，将其转发给共用的分布式 Key-Value 存储层 TiKV，然后组装 TiKV 返回的结果，最终将查询结果返回给客户端。\n\n这一层的节点都是无状态的，节点本身并不存储数据，节点之间完全对等。\n\n### SQL 运算\n\n最简单的方案就是通过上一节所述的[表数据与 Key-Value 的映射关系](#表数据与-key-value-的映射关系)方案，将 SQL 查询映射为对 KV 的查询，再通过 KV 接口获取对应的数据，最后执行各种计算。\n\n比如 `select count(*) from user where name = \"TiDB\"` 这样一个 SQL 语句，它需要读取表中所有的数据，然后检查 `name` 字段是否是 `TiDB`，如果是的话，则返回这一行。具体流程如下：\n\n1. 构造出 Key Range：一个表中所有的 `RowID` 都在 `[0, MaxInt64)` 这个范围内，使用 `0` 和 `MaxInt64` 根据行数据的 `Key` 编码规则，就能构造出一个 `[StartKey, EndKey)`的左闭右开区间。\n2. 扫描 Key Range：根据上面构造出的 Key Range，读取 TiKV 中的数据。\n3. 过滤数据：对于读到的每一行数据，计算 `name = \"TiDB\"` 这个表达式，如果为真，则向上返回这一行，否则丢弃这一行数据。\n4. 计算 `Count(*)`：对符合要求的每一行，累计到 `Count(*)` 的结果上面。\n\n**整个流程示意图如下：**\n\n![naive sql flow](/media/tidb-computing-native-sql-flow.jpeg)\n\n这个方案是直观且可行的，但是在分布式数据库的场景下有一些显而易见的问题：\n\n- 在扫描数据的时候，每一行都要通过 KV 操作从 TiKV 中读取出来，至少有一次 RPC 开销，如果需要扫描的数据很多，那么这个开销会非常大。\n- 并不是所有的行都满足过滤条件 `name = \"TiDB\"`，如果不满足条件，其实可以不读取出来。\n- 此查询只要求返回符合要求行的数量，不要求返回这些行的值。\n\n### 分布式 SQL 运算\n\n为了解决上述问题，计算应该需要尽量靠近存储节点，以避免大量的 RPC 调用。首先，SQL 中的谓词条件 `name = \"TiDB\"` 应被下推到存储节点进行计算，这样只需要返回有效的行，避免无意义的网络传输。然后，聚合函数 `Count(*)` 也可以被下推到存储节点，进行预聚合，每个节点只需要返回一个 `Count(*)` 的结果即可，再由 SQL 层将各个节点返回的 `Count(*)` 的结果累加求和。\n\n以下是数据逐层返回的示意图：\n\n![dist sql flow](/media/tidb-computing-dist-sql-flow.png)\n\n### SQL 层架构\n\n通过上面的例子，希望大家对 SQL 语句的处理有一个基本的了解。实际上 TiDB 的 SQL 层要复杂得多，模块以及层次非常多，下图列出了重要的模块以及调用关系：\n\n![tidb sql layer](/media/tidb-computing-tidb-sql-layer.png)\n\n用户的 SQL 请求会直接或者通过 `Load Balancer` 发送到 TiDB Server，TiDB Server 会解析 `MySQL Protocol Packet`，获取请求内容，对 SQL 进行语法解析和语义分析，制定和优化查询计划，执行查询计划并获取和处理数据。数据全部存储在 TiKV 集群中，所以在这个过程中 TiDB Server 需要和 TiKV 交互，获取数据。最后 TiDB Server 需要将查询结果返回给用户。\n"
        },
        {
          "name": "tidb-configuration-file.md",
          "type": "blob",
          "size": 55.87109375,
          "content": "---\ntitle: TiDB 配置文件描述\nsummary: 介绍未包含在命令行参数中的 TiDB 配置文件选项。\naliases: ['/docs-cn/dev/tidb-configuration-file/','/docs-cn/dev/reference/configuration/tidb-server/configuration-file/']\n---\n\n<!-- markdownlint-disable MD001 -->\n\n# TiDB 配置文件描述\n\nTiDB 配置文件比命令行参数支持更多的选项。你可以在 [config/config.toml.example](https://github.com/pingcap/tidb/blob/master/pkg/config/config.toml.example) 找到默认值的配置文件，重命名为 `config.toml` 即可。本文档只介绍未包含在[命令行参数](/command-line-flags-for-tidb-configuration.md)中的参数。\n\n> **Tip:**\n>\n> 如果你需要调整配置项的值，请参考[修改配置参数](/maintain-tidb-using-tiup.md#修改配置参数)进行操作。\n\n### `split-table`\n\n+ 为每个 table 建立单独的 Region。\n+ 默认值：true\n+ 如果需要创建大量的表（例如 10 万张以上），建议将此参数设置为 false。\n\n### `tidb-max-reuse-chunk` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 用于控制每个连接最多缓存的 Chunk 对象数。配置过大会增加 OOM 的风险。\n+ 默认值：64\n+ 最小值：0\n+ 最大值：2147483647\n\n### `tidb-max-reuse-column` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 用于控制每个连接最多缓存的 column 对象数。配置过大会增加 OOM 的风险。\n+ 默认值：256\n+ 最小值：0\n+ 最大值：2147483647\n\n### `token-limit`\n\n+ 可以同时执行请求的 session 个数\n+ 类型：Integer\n+ 默认值：1000\n+ 最小值：1\n+ 最大值：`1048576`\n\n### `temp-dir` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n+ TiDB 用于存放临时数据的路径。如果一个功能需要使用 TiDB 节点的本地存储，TiDB 将把对应数据临时存放在这个目录下。\n+ 在创建索引的过程中，如果开启了[创建索引加速](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入)，那么新创建索引需要回填的数据会被先存放在 TiDB 本地临时存储路径，然后批量导入到 TiKV，从而提升索引创建速度。\n+ 在使用 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md) 导入数据时，排序后的数据会被先存放在 TiDB 本地临时存储路径，然后批量导入到 TiKV。\n+ 默认值：\"/tmp/tidb\"\n\n> **注意：**\n>\n> 如果目录不存在，TiDB 在启动时会自动创建该目录。如果目录创建失败，或者 TiDB 对该目录没有读写权限，[Fast Online DDL](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入) 在运行时可能产生不可预知的问题。\n\n### `oom-use-tmp-storage`\n\n> **警告：**\n>\n> 自 v6.3.0 起，该配置项被废弃，其功能由系统变量 [`tidb_enable_tmp_storage_on_oom`](/system-variables.md#tidb_enable_tmp_storage_on_oom) 代替。集群升级到 v6.3.0 及之后的版本后，会自动继承升级前的 `oom-use-tmp-storage` 设置，升级后再设置 `oom-use-tmp-storage` 将不生效。\n\n+ 设置是否在单条 SQL 语句的内存使用超出系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 限制时为某些算子启用临时磁盘。\n+ 默认值：true\n\n### `tmp-storage-path`\n\n+ 单条 SQL 语句的内存使用超出系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 限制时，某些算子的临时磁盘存储位置。\n+ 默认值：`<操作系统临时文件夹>/<操作系统用户ID>_tidb/MC4wLjAuMDo0MDAwLzAuMC4wLjA6MTAwODA=/tmp-storage`。其中 `MC4wLjAuMDo0MDAwLzAuMC4wLjA6MTAwODA=` 是对 `<host>:<port>/<statusHost>:<statusPort>` 进行 `Base64` 编码的输出结果。\n+ 此配置仅在系统变量 [`tidb_enable_tmp_storage_on_oom`](/system-variables.md#tidb_enable_tmp_storage_on_oom) 的值为 `ON` 时有效。\n\n### `tmp-storage-quota`\n\n+ `tmp-storage-path` 存储使用的限额。\n+ 单位：Byte\n+ 当单条 SQL 语句使用临时磁盘，导致 TiDB server 的总体临时磁盘总量超过 `tmp-storage-quota` 时，当前 SQL 操作会被取消，并返回 `Out Of Global Storage Quota!` 错误。\n+ 当 `tmp-storage-quota` 小于 0 时则没有上述检查与限制。\n+ 默认值：-1\n+ 当 `tmp-storage-path` 的剩余可用容量低于 `tmp-storage-quota` 所定义的值时，TiDB server 启动时将会报出错误并退出。\n\n### `lease`\n\n+ DDL 租约超时时间。\n+ 默认值：45s\n+ 单位：秒\n\n### `compatible-kill-query`\n\n+ 设置 `KILL` 语句的兼容性。\n+ 默认值：false\n+ `compatible-kill-query` 仅在 [`enable-global-kill`](#enable-global-kill-从-v610-版本开始引入) 为 `false` 时生效。\n+ 当 [`enable-global-kill`](#enable-global-kill-从-v610-版本开始引入) 为 `false` 时，`compatible-kill-query` 控制杀死一条查询时是否需要加上 `TIDB` 关键词。\n    - `compatible-kill-query` 为 `false` 时，TiDB 中 `KILL xxx` 的行为和 MySQL 中的行为不同。为杀死一条查询，在 TiDB 中需要加上 `TIDB` 关键词，即 `KILL TIDB xxx`。\n    - `compatible-kill-query` 为 `true` 时，为杀死一条查询，在 TiDB 中无需加上 `TIDB` 关键词。**强烈不建议**设置 `compatible-kill-query` 为 `true`，**除非**你确定客户端将始终连接到同一个 TiDB 节点。这是因为当你在默认的 MySQL 客户端按下 <kbd>Control</kbd>+<kbd>C</kbd> 时，客户端会开启一个新连接，并在这个新连接中执行 `KILL` 语句。此时，如果客户端和 TiDB 之间存在代理，新连接可能会被路由到其他 TiDB 节点，从而错误地终止其他会话。\n+ 当 [`enable-global-kill`](#enable-global-kill-从-v610-版本开始引入) 为 `true` 时，`KILL xxx` 和 `KILL TIDB xxx` 的作用相同。\n+ 关于 `KILL` 语句的更多信息，请参考 [KILL [TIDB]](/sql-statements/sql-statement-kill.md)。\n\n### `check-mb4-value-in-utf8`\n\n+ 开启检查 utf8mb4 字符的开关，如果开启此功能，字符集是 utf8，且在 utf8 插入 mb4 字符，系统将会报错。\n+ 默认值：true\n+ 自 v6.1.0 起，utf8mb4 字符检查改为通过 TiDB 配置项 `instance.tidb_check_mb4_value_in_utf8` 或系统变量 `tidb_check_mb4_value_in_utf8` 进行设置。`check-mb4-value-in-utf8` 仍可使用，但如果同时设置了 `check-mb4-value-in-utf8` 与 `instance.tidb_check_mb4_value_in_utf8`，TiDB 将采用 `instance.tidb_check_mb4_value_in_utf8` 的值。\n\n### `treat-old-version-utf8-as-utf8mb4`\n\n+ 将旧表中的 utf8 字符集当成 utf8mb4 的开关。\n+ 默认值：true\n\n### `alter-primary-key`（已废弃）\n\n+ 用于控制添加或者删除主键功能。\n+ 默认值：false\n+ 默认情况下，不支持增删主键。将此变量被设置为 true 后，支持增删主键功能。不过对在此开关开启前已经存在的表，且主键是整型类型时，即使之后开启此开关也不支持对此列表删除主键。\n\n> **注意：**\n>\n> 该配置项已被废弃，目前仅在 `@@tidb_enable_clustered_index` 取值为 `INT_ONLY` 时生效。如果需要增删主键，请在建表时使用 `NONCLUSTERED` 关键字代替。要了解关于 `CLUSTERED` 主键的详细信息，请参考[聚簇索引](/clustered-indexes.md)。\n\n### `server-version`\n\n+ 用来修改 TiDB 在以下情况下返回的版本号：\n    - 当使用内置函数 `VERSION()` 时。\n    - 当与客户端初始连接，TiDB 返回带有服务端版本号的初始握手包时。具体可以查看 MySQL 初始握手包的[描述](https://dev.mysql.com/doc/dev/mysql-server/latest/page_protocol_connection_phase.html#sect_protocol_connection_phase_initial_handshake)。\n+ 默认值：\"\"\n+ 默认情况下，TiDB 版本号格式为：`8.0.11-TiDB-${tidb_version}`。\n\n> **注意：**\n>\n> `server-version` 的值会被 TiDB 节点用于验证当前 TiDB 的版本。因此在进行 TiDB 集群升级前，请将 `server-version` 的值设置为空或者当前 TiDB 真实的版本值，避免出现非预期行为。\n\n### `repair-mode`\n\n+ 用于开启非可信修复模式，启动该模式后，可以过滤 `repair-table-list` 名单中坏表的加载。\n+ 默认值：false\n+ 默认情况下，不支持修复语法，默认启动时会加载所有表信息。\n\n### `repair-table-list`\n\n+ 配合 `repair-mode` 为 true 时使用，用于列出实例中需要修复的坏表的名单，该名单的写法为 [\"db.table1\",\"db.table2\", ……]。\n+ 默认值：[]\n+ 默认情况下，该 list 名单为空，表示没有所需修复的坏表信息。\n\n### `new_collations_enabled_on_first_bootstrap`\n\n+ 用于开启新的 collation 支持\n+ 默认值：true\n+ 注意：该配置项只有在初次初始化集群时生效，初始化集群后，无法通过更改该配置项打开或关闭新的 collation 框架。\n\n### `max-server-connections`\n\n+ TiDB 中同时允许的最大客户端连接数，用于资源控制。\n+ 默认值：0\n+ 默认情况下，TiDB 不限制客户端连接数。当本配置项的值大于 `0` 且客户端连接数到达此值时，TiDB 服务端将会拒绝新的客户端连接。\n+ 自 v6.2.0 起，客户端连接数已改用配置项 [`instance.max_connections`](/tidb-configuration-file.md#max_connections) 或系统变量 [`max_connections`](/system-variables.md#max_connections) 进行设置。`max-server-connections` 仍可使用，但如果同时设置了 `max-server-connections` 与 `instance.max_connections`，TiDB 将采用 `instance.max_connections` 的值。\n\n### `max-index-length`\n\n+ 用于设置新建索引的长度限制。\n+ 默认值：3072\n+ 单位：Byte\n+ 目前的合法值范围 `[3072, 3072*4]`。MySQL 和 TiDB v3.0.11 之前版本（不包含 v3.0.11）没有此配置项，不过都对新建索引的长度做了限制。MySQL 对此的长度限制为 `3072`，TiDB 在 v3.0.7 以及之前版本该值为 `3072*4`，在 v3.0.7 之后版本（包含 v3.0.8、v3.0.9 和 v3.0.10）的该值为 `3072`。为了与 MySQL 和 TiDB 之前版本的兼容，添加了此配置项。\n\n### `table-column-count-limit` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 用于设置单个表中列的数量限制\n+ 默认值：1017\n+ 目前的合法值范围 `[1017, 4096]`。\n\n### `index-limit` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 用于设置单个表中索引的数量限制\n+ 默认值：64\n+ 目前的合法值范围 `[64, 512]`。\n\n### `enable-telemetry` <span class=\"version-mark\">从 v4.0.2 版本开始引入，从 v8.1.0 版本开始废弃</span>\n\n> **警告：**\n>\n> 从 TiDB v8.1.0 开始，TiDB 已移除遥测功能，该配置项已不再生效。保留该配置项仅用于与之前版本兼容。\n\n+ 在 v8.1.0 之前，用于控制是否在 TiDB 实例上开启遥测功能。\n+ 默认值：false\n\n### `deprecate-integer-display-length`\n\n+ 当此配置项设置为 `true` 时，弃用整数类型的显示宽度。\n+ 默认值：`true`。在 v8.5.0 之前，默认值为 `false`。\n\n### `enable-tcp4-only` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 控制是否只监听 TCP4。\n+ 默认值：false\n+ 当使用 LVS 为 TiDB 做负载均衡时，可开启此配置项。这是因为 [LVS 的 TOA 模块](https://github.com/alibaba/LVS/tree/master/kernel/net/toa)可以通过 TCP4 协议从 TCP 头部信息中解析出客户端的真实 IP。\n\n### `enable-enum-length-limit` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 是否限制单个 `ENUM` 元素和单个 `SET` 元素的最大长度\n+ 默认值：true\n+ 当该配置项值为 `true` 时，`ENUM` 和 `SET` 单个元素的最大长度为 255 个字符，[与 MySQL 8 兼容](https://dev.mysql.com/doc/refman/8.0/en/string-type-syntax.html)；当该配置项值为 `false` 时，不对单个元素的长度进行限制，与 TiDB v5.0 之前的版本兼容。\n\n### `graceful-wait-before-shutdown` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n- 指定关闭服务器时 TiDB 等待的秒数，使得客户端有时间断开连接。\n- 默认值：0\n- 在 TiDB 等待服务器关闭期间，HTTP 状态会显示失败，使得负载均衡器可以重新路由流量。\n\n> **注意：**\n>\n> TiDB 在关闭服务器之前等待的时长也会受到以下参数的影响：\n>\n> - 当使用的平台采用了 SystemD 时，默认的停止超时为 90 秒。如果需要更长的超时时间，可以设置 [`TimeoutStopSec=`](https://www.freedesktop.org/software/systemd/man/latest/systemd.service.html#TimeoutStopSec=)。\n>\n> - 当使用 TiUP Cluster 组件时，默认的 [`--wait-timeout`](/tiup/tiup-component-cluster.md#--wait-timeoutuint默认-120) 为 120 秒。\n>\n> - 当使用 Kubernetes 时，默认的 [`terminationGracePeriodSeconds`](https://kubernetes.io/docs/reference/kubernetes-api/workload-resources/pod-v1/#lifecycle) 为 30 秒。\n\n### `enable-global-kill` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n+ 用于开启 Global Kill（跨节点终止查询或连接）功能。\n+ 默认值：true\n+ 当该配置项值为 `true` 时，`KILL` 语句和 `KILL TIDB` 语句均能跨节点终止查询或连接，无需担心错误地终止其他查询或连接。当你使用客户端连接到任何一个 TiDB 节点执行 `KILL` 语句或 `KILL TIDB` 语句时，该语句会被转发给对应的 TiDB 节点。当客户端和 TiDB 中间有代理时，`KILL` 语句或 `KILL TIDB` 语句也会被转发给对应的 TiDB 节点执行。关于 `KILL` 语句的更多信息，请参考 [`KILL [TIDB]`](/sql-statements/sql-statement-kill.md)。\n+ TiDB 从 v7.3.0 开始支持在 `enable-global-kill = true` 和 [`enable-32bits-connection-id = true`](#enable-32bits-connection-id-从-v730-版本开始引入) 时使用 MySQL 命令行 <kbd>Control+C</kbd> 终止查询或连接。\n\n### `enable-32bits-connection-id` <span class=\"version-mark\">从 v7.3.0 版本开始引入</span>\n\n+ 用于控制是否开启生成 32 位 connection ID 的功能。\n+ 默认值：`true`\n+ 当该配置项值以及 [`enable-global-kill`](#enable-global-kill-从-v610-版本开始引入) 为 `true` 时，生成 32 位 connection ID，从而支持在 MySQL 命令行中通过 <kbd>Control+C</kbd> 终止查询或连接。\n\n> **注意：**\n>\n> 当集群中 TiDB 实例数量超过 2048 或者单个 TiDB 实例的同时连接数超过 1048576 后，由于 32 位 connection ID 空间不足，将自动升级为 64 位 connection ID。升级过程中业务以及已建立的连接不受影响，但后续的新建连接将无法通过 MySQL 命令行 <kbd>Control+C</kbd> 终止。\n\n### `initialize-sql-file` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n+ 用于指定 TiDB 集群初次启动时执行的 SQL 脚本。\n+ 默认值：\"\"\n+ 脚本中的所有 SQL 语句将以最高权限执行，不会进行权限检查。若指定的 SQL 脚本无法正确执行，可能导致 TiDB 集群启动失败。\n+ 通常用于修改系统变量的值、创建用户或分配权限等。\n\n### `enable-forwarding` <span class=\"version-mark\">从 v5.0.0 版本开始引入</span>\n\n+ 控制 TiDB 中的 PD client 以及 TiKV client 在疑似网络隔离的情况下是否通过 follower 将请求转发给 leader。\n+ 默认值：false\n+ 如果确认环境存在网络隔离的可能，开启这个参数可以减少服务不可用的窗口期。\n+ 如果无法准确判断隔离、网络中断、宕机等情况，这个机制存在误判情况从而导致可用性、性能降低。如果网络中从未发生过网络故障，不推荐开启此选项。\n\n### `enable-table-lock` <span class=\"version-mark\">从 v4.0.0 版本开始引入</span>\n\n> **警告：**\n>\n> 表级锁 (Table Lock) 为实验特性，不建议在生产环境中使用。\n\n+ 控制是否开启表级锁特性。\n+ 默认值：false\n+ 表级锁用于协调多个 session 之间对同一张表的并发访问。目前已支持的锁种类包括 `READ`、`WRITE` 和 `WRITE LOCAL`。当该配置项为 `false` 时，执行 `LOCK TABLES` 和 `UNLOCK TABLES` 语句不会生效，并且会报 \"LOCK/UNLOCK TABLES is not supported\" 的警告。更多信息，请参考 [`LOCK TABLES` 和 `UNLOCK TABLES`](/sql-statements/sql-statement-lock-tables-and-unlock-tables.md)。\n\n### `labels`\n\n+ 指定服务器标签，例如 `{ zone = \"us-west-1\", dc = \"dc1\", rack = \"rack1\", host = \"tidb1\" }`。\n+ 默认值：`{}`\n\n> **注意：**\n>\n> - 标签 `zone` 在 TiDB 中具有特殊用途，用于指定服务器所在的区域信息，当设置 `zone` 为非空值时，对应的值会被自动用于 [`txn-score`](/system-variables.md#txn_scope) 和 [`Follower read`](/follower-read.md) 等功能。\n> - 标签 `group` 在 TiDB Operator 中具有特殊用途。对于使用 [TiDB Operator](/tidb-operator-overview.md) 部署的集群，建议不要手动指定此标签。\n\n## log\n\n日志相关的配置项。\n\n### `level`\n\n+ 指定日志的输出级别，可选项为 [debug, info, warn, error, fatal]\n+ 默认值：\"info\"\n\n### `format`\n\n+ 指定日志输出的格式，可选项为 [json, text]。\n+ 默认值：\"text\"\n\n### `enable-timestamp`\n\n+ 是否在日志中输出时间戳。\n+ 默认值：null\n+ 如果设置为 false，那么日志里面将不会输出时间戳。\n\n> **注意：**\n>\n> - 考虑后向兼容性，原来的配置项 `disable-timestamp` 仍然有效，但如果和 `enable-timestamp` 配置的值在语义上冲突（例如在配置中把 `enable-timestamp` 和 `disable-timestamp` 同时设置为 `true`），则 TiDB 会忽略 `disable-timestamp` 的值。\n> - 当前 TiDB 默认使用 `disable-timestamp` 来决定是否在日志中输出时间戳，此时 `enable-timestamp` 的值为 `null`。\n> - 在未来的版本中，`disable-timestamp` 配置项将被彻底移除，请废弃 `disable-timestamp` 的用法，使用语义上更易于理解的 `enable-timestamp`。\n\n### `enable-slow-log`\n\n+ 是否开启慢查询日志\n+ 默认值：true\n+ 可以设置成 `true` 或 `false` 来启用或禁用慢查询日志。\n+ 自 v6.1.0 起，已改用配置项 `instance.tidb_enable_slow_log` 或系统变量 `tidb_enable_slow_log` 来设置是否开启慢查询日志。`enable-slow-log` 仍可使用，但如果同时设置了 `enable-slow-log` 与 `instance.tidb_enable_slow_log`，TiDB 将采用 `instance.tidb_enable_slow_log` 的值。\n\n### `slow-query-file`\n\n+ 慢查询日志的文件名。\n+ 默认值：\"tidb-slow.log\"。注：由于 TiDB V2.1.8 更新了慢日志格式，所以将慢日志单独输出到了慢日志文件。V2.1.8 之前的版本，该变量的默认值是 \"\"。\n+ 设置后，慢查询日志会单独输出到该文件。\n\n### `slow-threshold`\n\n+ 输出慢日志的耗时阈值。\n+ 默认值：300\n+ 单位：毫秒\n+ 如果查询耗时大于这个值，会视作一个慢查询，并记录到慢查询日志。注意，当日志的输出级别 [`log.level`](#level) 是 `\"debug\"` 时，所有查询都会记录到慢日志，不受该参数的限制。\n+ 自 v6.1.0 起，已改用配置项 `instance.tidb_slow_log_threshold` 或系统变量 `tidb_slow_log_threshold` 来设置输出慢日志的耗时阈值。`slow-threshold` 仍可使用，但如果同时设置了 `slow-threshold` 与 `instance.tidb_slow_log_threshold`，TiDB 将采用 `instance.tidb_slow_log_threshold` 的值。\n\n### `record-plan-in-slow-log`\n\n+ 在慢日志中记录执行计划\n+ 默认值：1\n+ 自 v6.1.0 起，已改用配置项 [`instance.tidb_record_plan_in_slow_log`](/tidb-configuration-file.md#tidb_record_plan_in_slow_log) 或系统变量 [`tidb_record_plan_in_slow_log`](/system-variables.md#tidb_record_plan_in_slow_log) 来设置在慢日志中记录执行计划。`record-plan-in-slow-log` 仍可使用，但如果同时设置了 `record-plan-in-slow-log` 与 `instance.tidb_record_plan_in_slow_log`，TiDB 将采用 `instance.tidb_record_plan_in_slow_log` 的值。\n\n### `expensive-threshold`\n\n> **警告：**\n>\n> 自 v5.4.0 起，该配置项被废弃。请使用 [`tidb_expensive_query_time_threshold`](/system-variables.md#tidb_expensive_query_time_threshold) 系统变量进行设置。\n\n+ 输出 `expensive` 操作的行数阈值。\n+ 默认值：10000\n+ 当查询的行数（包括中间结果，基于统计信息）大于这个值，该操作会被认为是 `expensive` 查询，并输出一个前缀带有 `[EXPENSIVE_QUERY]` 的日志。\n\n### `general-log-file` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n+ 设置 [general log](/system-variables.md#tidb_general_log) 的文件名。\n+ 默认值：\"\"\n+ 如果设置了文件名，general log 会输出到指定的文件。如没有设置文件名，general log 会写入 TiDB 实例的日志文件中，该文件名通过 [`filename`](#filename) 指定。\n\n### `timeout` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n+ 用于设置 TiDB 写日志操作的超时时间。当磁盘故障导致日志无法写入时，该配置可以让 TiDB 进程崩溃而不是卡死。\n+ 默认值：0，表示不设置超时\n+ 单位：秒\n+ 在某些用户场景中，TiDB 日志可能是保存在热插拔盘或网络挂载盘上，这些磁盘可能会永久丢失。在这种场景下，TiDB 无法自动恢复，写日志操作会永久阻塞。尽管 TiDB 进程看起来仍在运行，但不会响应任何请求。该配置项用于处理这样的场景。\n\n## log.file\n\n日志文件相关的配置项。\n\n#### `filename`\n\n+ 一般日志文件名字。\n+ 默认值：\"\"\n+ 如果设置，会输出一般日志到这个文件。\n\n#### `max-size`\n\n+ 日志文件的大小限制。\n+ 默认值：300\n+ 单位：MB\n+ 最大设置上限为 4096。\n\n#### `max-days`\n\n+ 日志最大保留的天数。\n+ 默认值：0\n+ 默认不清理；如果设置了参数值，在 `max-days` 之后 TiDB 会清理过期的日志文件。\n\n#### `max-backups`\n\n+ 保留的日志的最大数量。\n+ 默认值：0\n+ 默认全部保存；如果设置为 7，会最多保留 7 个老的日志文件。\n\n#### `compression` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n+ 指定日志的压缩方式。\n+ 默认值：\"\"\n+ 可选值：\"\"、\"gzip\"\n+ 默认值为 \"\" ，即不压缩。修改为 \"gzip\" 可以使用 gzip 算法压缩数据。开启压缩后会影响所有的日志文件，包括 [`slow-query-file`](#slow-query-file)、[`general-log-file`](#general-log-file-从-v800-版本开始引入) 等。\n\n## security\n\n安全相关配置。\n\n### `enable-sem`\n\n- 启用安全增强模式 (SEM)。\n- 默认值：`false`\n- 可以通过系统变量 [`tidb_enable_enhanced_security`](/system-variables.md#tidb_enable_enhanced_security) 获取安全增强模式的状态。\n\n### `ssl-ca`\n\n+ PEM 格式的受信任 CA 的证书文件路径。\n+ 默认值：\"\"\n+ 当同时设置了该选项和 `--ssl-cert`、`--ssl-key` 选项时，TiDB 将在客户端出示证书的情况下根据该选项指定的受信任的 CA 列表验证客户端证书。若验证失败，则连接会被终止。\n+ 即使设置了该选项，若客户端没有出示证书，则安全连接仍然继续，不会进行客户端证书验证。\n\n### `ssl-cert`\n\n+ PEM 格式的 SSL 证书文件路径。\n+ 默认值：\"\"\n+ 当同时设置了该选项和 `--ssl-key` 选项时，TiDB 将接受（但不强制）客户端使用 TLS 安全地连接到 TiDB。\n+ 若指定的证书或私钥无效，则 TiDB 会照常启动，但无法接受安全连接。\n\n### `ssl-key`\n\n+ PEM 格式的 SSL 证书密钥文件路径，即 `--ssl-cert` 所指定的证书的私钥。\n+ 默认值：\"\"\n+ 目前 TiDB 不支持加载由密码保护的私钥。\n\n### `cluster-ssl-ca`\n\n+ CA 根证书，用于用 tls 连接 TiKV/PD\n+ 默认值：\"\"\n\n### `cluster-ssl-cert`\n\n+ ssl 证书文件路径，用于用 tls 连接 TiKV/PD\n+ 默认值：\"\"\n\n### `cluster-ssl-key`\n\n+ ssl 私钥文件路径，用于用 tls 连接 TiKV/PD\n+ 默认值：\"\"\n\n### `spilled-file-encryption-method`\n\n+ 内存落盘文件的加密方式。\n+ 默认值：`\"plaintext\"`，表示不进行加密。\n+ 可选值：`\"plaintext\"`、`\"aes128-ctr\"`。\n\n### `auto-tls`\n\n+ 控制 TiDB 启动时是否自动生成 TLS 证书。\n+ 默认值：`false`\n\n### `tls-version`\n\n> **警告：**\n>\n> TiDB v7.6.0 废弃了对 `\"TLSv1.0\"` 和 `\"TLSv1.1\"` 协议对支持，并从 v8.0.0 开始移除对这两个协议的支持。\n\n+ 设置用于连接 MySQL 协议的最低 TLS 版本。\n+ 默认值：\"\"，支持 TLSv1.2 及以上版本。在 v7.6.0 之前，TiDB 默认支持 TLSv1.1 及以上版本。\n+ 可选值：`\"TLSv1.2\"` 和 `\"TLSv1.3\"`。在 v8.0.0 之前，TiDB 也支持 `\"TLSv1.0\"` 和 `\"TLSv1.1\"`。\n\n### `auth-token-jwks` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 设置 [`tidb_auth_token`](/security-compatibility-with-mysql.md#tidb_auth_token) 认证方式的 JSON Web Key Sets (JWKS) 的本地文件路径。\n+ 默认值：\"\"\n\n### `auth-token-refresh-interval` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 设置 [`tidb_auth_token`](/security-compatibility-with-mysql.md#tidb_auth_token) 认证方式的 JWKS 刷新时间间隔。\n+ 默认值：1h\n\n### `disconnect-on-expired-password` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n+ 对于密码已过期的用户，通过 `disconnect-on-expired-password` 控制 TiDB 服务端是否直接断开该用户的连接。\n+ 默认值：`true`\n+ 默认值为 \"true\" 表示 TiDB 服务端将直接断开密码已过期用户的连接。设置为 \"false\" 时，TiDB 服务端将密码已过期用户的连接置于“沙盒模式”，允许该用户建立连接并执行密码重置操作。\n\n### `session-token-signing-cert` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 证书文件路径，用于 [TiProxy](/tiproxy/tiproxy-overview.md) 的会话迁移。\n+ 默认值：\"\"\n+ 空值将导致 TiProxy 会话迁移失败。要启用会话迁移，所有的 TiDB 节点必须设置相同的证书和密钥。因此你应该在每个 TiDB 节点上存储相同的证书和密钥。\n\n### `session-token-signing-key` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 密钥文件路径，用于 [TiProxy](/tiproxy/tiproxy-overview.md) 的会话迁移。\n+ 默认值：\"\"\n+ 参阅 [`session-token-signing-cert`](#session-token-signing-cert-从-v640-版本开始引入) 的描述。\n\n## performance\n\n性能相关配置。\n\n### `max-procs`\n\n+ TiDB 的 CPU 使用数量。\n+ 默认值：0\n+ 默认值为 0 表示使用机器上所有的 CPU；如果设置成 n，那么 TiDB 会使用 n 个 CPU 数量。\n\n### `server-memory-quota` <span class=\"version-mark\">从 v4.0.9 版本开始引入</span>\n\n> **警告：**\n>\n> 自 v6.5.0 起，该配置项被废弃。请使用 [`tidb_server_memory_limit`](/system-variables.md#tidb_server_memory_limit-从-v640-版本开始引入) 系统变量进行设置。\n\n+ 设置 tidb-server 实例的最大内存用量，单位为字节。\n+ 默认值：0\n+ 默认值为 0 表示无内存限制。\n\n### `txn-entry-size-limit` <span class=\"version-mark\">从 v4.0.10 和 v5.0.0 版本开始引入</span>\n\n+ TiDB 单行数据的大小限制\n+ 默认值：6291456\n+ 单位：Byte\n+ 事务中单个 key-value 记录的大小限制。若超出该限制，TiDB 将会返回 `entry too large` 错误。该配置项的最大值不超过 `125829120`（表示 120MB）。\n+ 从 v7.6.0 开始，你可以使用 [`tidb_txn_entry_size_limit`](/system-variables.md#tidb_txn_entry_size_limit-从-v760-版本开始引入) 系统变量动态修改该配置项的值。\n+ 注意，TiKV 有类似的限制。若单个写入请求的数据量大小超出 [`raft-entry-max-size`](/tikv-configuration-file.md#raft-entry-max-size)，默认为 8MB，TiKV 会拒绝处理该请求。当表的一行记录较大时，需要同时修改这两个配置。\n+ [`max_allowed_packet`](/system-variables.md#max_allowed_packet-从-v610-版本开始引入) (MySQL 协议的最大数据包大小) 的默认值为 `67108864`（64 MiB）。如果一行记录的大小超过 `max_allowed_packet`，该行记录会被截断。\n+ [`txn-total-size-limit`](#txn-total-size-limit)（TiDB 单个事务大小限制）的默认值为 100 MiB。如果将 `txn-entry-size-limit` 的值设置为 100 MiB 以上，需要相应地调大 `txn-total-size-limit` 的值。\n\n### `txn-total-size-limit`\n\n+ TiDB 单个事务大小限制\n+ 默认值：104857600\n+ 单位：Byte\n+ 单个事务中，所有 key-value 记录的总大小不能超过该限制。该配置项的最大值不超过 `1099511627776`（表示 1TB）。\n+ 在 v6.5.0 及之后的版本中，不再推荐使用该配置项，事务的内存大小会被累计计入所在会话的内存使用量中，并由 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 变量在单个会话内存超阈值时采取控制行为。为了向前兼容，由低版本升级至 v6.5.0 及更高版本时，该配置项的行为如下所述:\n    + 若该配置项未设置，或设置为默认值 (`104857600`)，升级后事务内存大小将会计入所在会话的内存使用中，由 `tidb_mem_quota_query` 变量控制。\n    + 若该配置项未设为默认值 (`104857600`)，升级前后该配置项仍生效，对单个事务大小的限制行为不会发生变化，事务内存大小不由 `tidb_mem_quota_query` 控制。\n+ 从 v8.0.0 开始，如果系统变量 [`tidb_dml_type`](/system-variables.md#tidb_dml_type-从-v800-版本开始引入) 以 `\"bulk\"` 方式执行事务时，事务的大小不受 TiDB 配置项 `txn-total-size-limit` 的限制。\n\n### `max-txn-ttl`\n\n+ 单个事务持锁的最长时间，超过该时间，该事务的锁可能会被其他事务清除，导致该事务无法成功提交。\n+ 默认值：3600000\n+ 单位：毫秒\n+ 超过此时间的事务只能执行提交或者回滚，提交不一定能够成功。\n+ 对于使用 [`\"bulk\"` DML 方式](/system-variables.md#tidb_dml_type-从-v800-版本开始引入)执行的事务，其最大 TTL 可以超过该配置项的限制，最大不超过 24 小时或该配置项值中的较大者。\n\n### `stmt-count-limit`\n\n+ TiDB 单个事务允许的最大语句条数限制。\n+ 默认值：5000\n+ 在一个事务中，超过 `stmt-count-limit` 条语句后还没有 rollback 或者 commit，TiDB 将会返回 `statement count 5001 exceeds the transaction limitation, autocommit = false` 错误。该限制只在可重试的乐观事务中生效，如果使用悲观事务或者关闭了[事务重试](/optimistic-transaction.md#事务的重试)，事务中的语句数将不受此限制。\n\n### `tcp-keep-alive`\n\n+ TiDB 在 TCP 层开启 keepalive。\n+ 默认值：true\n\n### `tcp-no-delay`\n\n+ 控制 TiDB 是否在 TCP 层开启 TCP_NODELAY。开启后，TiDB 将禁用 TCP/IP 协议中的 Nagle 算法，允许小数据包的发送，可以降低网络延时，适用于延时敏感型且数据传输量比较小的应用。\n+ 默认值：true\n\n### `cross-join`\n\n+ 默认值：true\n+ 默认可以执行在做 join 时两边表没有任何条件（where 字段）的语句；如果设置为 false，则有这样的 join 语句出现时，server 会拒绝执行\n\n> **注意：**\n>\n> 在创建集群时，不要将 `cross-join` 设置为 false，否则会导致集群启动失败。\n\n### `stats-lease`\n\n+ TiDB 重载统计信息，更新表行数，检查是否需要自动 analyze，利用 feedback 更新统计信息以及加载列的统计信息的时间间隔。\n+ 默认值：3s\n    - 每隔 `stats-lease` 时间，TiDB 会检查统计信息是否有更新，如果有会将其更新到内存中\n    - 每隔 `20 * stats-lease` 时间，TiDB 会将 DML 产生的总行数以及修改的行数变化更新到系统表中\n    - 每隔 `stats-lease` 时间，TiDB 会检查是否有表或者索引需要自动 analyze\n    - 每隔 `stats-lease` 时间，TiDB 会检查是否有列的统计信息需要被加载到内存中\n    - 每隔 `200 * stats-lease` 时间，TiDB 会将内存中缓存的 feedback 写入系统表中\n    - 每隔 `5 * stats-lease` 时间，TiDB 会读取系统表中的 feedback，更新内存中缓存的统计信息\n+ 当 `stats-lease` 为 0s 时，TiDB 会以 3s 的时间间隔周期性的读取系统表中的统计信息并更新内存中缓存的统计信息。但不会自动修改统计信息相关系统表，具体来说，TiDB 不再自动修改这些表：\n    - `mysql.stats_meta`：TiDB 不再自动记录事务中对某张表的修改行数，也不会更新到这个系统表中\n    - `mysql.stats_histograms`/`mysql.stats_buckets` 和 `mysql.stats_top_n`：TiDB 不再自动 analyze 和主动更新统计信息\n    - `mysql.stats_feedback`：TiDB 不再根据被查询的数据反馈的部分统计信息更新表和索引的统计信息\n\n### `pseudo-estimate-ratio`\n\n+ 修改过的行数/表的总行数的比值，超过该值时系统会认为统计信息已经过期，会采用 pseudo 的统计信息。\n+ 默认值：0.8\n+ 最小值：0\n+ 最大值：1\n\n### `force-priority`\n\n+ 把所有的语句优先级设置为 force-priority 的值。\n+ 默认值：NO_PRIORITY\n+ 可选值：默认值 NO_PRIORITY 表示不强制改变执行语句的优先级，其它优先级从低到高可设置为 LOW_PRIORITY、DELAYED 或 HIGH_PRIORITY。\n+ 自 v6.1.0 起，已改用配置项 [`instance.tidb_force_priority`](/tidb-configuration-file.md#tidb_force_priority) 或系统变量 [`tidb_force_priority`](/system-variables.md#tidb_force_priority) 来将所有语句优先级设为 force-priority 的值。`force-priority` 仍可使用，但如果同时设置了 `force-priority` 与 `instance.tidb_force_priority`，TiDB 将采用 `instance.tidb_force_priority` 的值。\n\n> **注意：**\n>\n> TiDB 从 v6.6.0 版本开始支持[使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)功能。该功能可以将不同优先级的语句放在不同的资源组中执行，并为这些资源组分配不同的配额和优先级，可以达到更好的资源管控效果。在开启资源管控功能后，语句的调度主要受资源组的控制，`PRIORITY` 将不再生效。建议在支持资源管控的版本优先使用资源管控功能。\n\n### `distinct-agg-push-down`\n\n+ 设置优化器是否执行将带有 `Distinct` 的聚合函数（比如 `select count(distinct a) from t`）下推到 Coprocessor 的优化操作。\n+ 默认值：false\n+ 该变量作为系统变量 [`tidb_opt_distinct_agg_push_down`](/system-variables.md#tidb_opt_distinct_agg_push_down) 的初始值。\n\n### `enforce-mpp`\n\n+ 用于控制是否忽略优化器代价估算，强制使用 TiFlash 的 MPP 模式执行查询。\n+ 默认值：false\n+ 该配置项可以控制系统变量 [`tidb_enforce_mpp`](/system-variables.md#tidb_enforce_mpp-从-v51-版本开始引入) 的初始值。例如，当设置该配置项为 true 时，`tidb_enforce_mpp` 的默认值为 ON。\n\n### `stats-load-concurrency` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ TiDB 统计信息同步加载功能可以并发处理的最大列数\n+ 默认值：`0`。在 v8.2.0 之前，默认值为 `5`。\n+ 目前的合法值范围：`[0, 128]`。`0` 为自动模式，根据服务器情况，自动调节并发度。在 v8.2.0 之前，最小值为 `1`。\n\n### `stats-load-queue-size` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ 用于设置 TiDB 统计信息同步加载功能最多可以缓存多少列的请求\n+ 默认值：1000\n+ 目前的合法值范围：`[1, 100000]`\n\n### `enable-stats-cache-mem-quota` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n+ 用于控制 TiDB 是否开启统计信息缓存的内存上限。\n+ 默认值：true\n\n### `concurrently-init-stats` <span class=\"version-mark\">从 v8.1.0 和 v7.5.2 版本开始引入</span>\n\n+ 用于控制 TiDB 启动时是否并发初始化统计信息。\n+ 默认值：`false`\n\n### `lite-init-stats` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n+ 用于控制 TiDB 启动时是否采用轻量级的统计信息初始化。\n+ 默认值：在 v7.2.0 之前版本中为 `false`，在 v7.2.0 及之后的版本中为 `true`。\n+ 当 `lite-init-stats` 为 `true` 时，统计信息初始化时列和索引的直方图、TopN、Count-Min Sketch 均不会加载到内存中。当 `lite-init-stats` 为 `false` 时，统计信息初始化时索引和主键的直方图、TopN、Count-Min Sketch 会被加载到内存中，非主键列的直方图、TopN、Count-Min Sketch 不会加载到内存中。当优化器需要某一索引或者列的直方图、TopN、Count-Min Sketch 时，这些统计信息会被同步或异步加载到内存中（由 [`tidb_stats_load_sync_wait`](/system-variables.md#tidb_stats_load_sync_wait-从-v540-版本开始引入) 控制）。\n+ 将 `lite-init-stats` 设置为 true，可以加速统计信息初始化，避免加载不必要的统计信息，从而降低 TiDB 的内存使用。详情请参考[统计信息的加载](/statistics.md#加载统计信息)。\n\n### `force-init-stats` <span class=\"version-mark\">从 v6.5.7 和 v7.1.0 版本开始引入</span>\n\n+ 用于控制 TiDB 启动时是否在统计信息初始化完成后再对外提供服务。\n+ 默认值：在 v7.2.0 之前版本中为 `false`，在 v7.2.0 及之后的版本中为 `true`。\n+ 当 `force-init-stats` 为 `true` 时，TiDB 启动时会等到统计信息初始化完成后再对外提供服务。需要注意的是，在表和分区数量较多且 [`lite-init-stats`](/tidb-configuration-file.md#lite-init-stats-从-v710-版本开始引入) 为 `false` 的情况下，`force-init-stats` 为 `true` 可能会导致 TiDB 从启动到开始对外提供服务的时间变长。\n+ 当 `force-init-stats` 为 `false` 时，TiDB 在统计信息初始化未完成时即可对外提供服务，但由于统计信息初始化未完成，优化器会用 pseudo 统计信息进行决策，可能会产生不合理的执行计划。\n\n## opentracing\n\nopentracing 的相关的设置。\n\n### `enable`\n\n+ 开启 opentracing 跟踪 TiDB 部分组件的调用开销。注意开启后会有一定的性能损失。\n+ 默认值：false\n\n### `rpc-metrics`\n\n+ 开启 rpc metrics。\n+ 默认值：false\n\n## opentracing.sampler\n\nopentracing.sampler 相关的设置。\n\n### `type`\n\n+ opentracing 采样器的类型。字符串取值大小写不敏感。\n+ 默认值：\"const\"\n+ 可选值：\"const\"，\"probabilistic\"，\"ratelimiting\"，remote\"\n\n### `param`\n\n+ 采样器参数。\n    - 对于 const 类型，可选值为 0 或 1，表示是否开启。\n    - 对于 probabilistic 类型，参数为采样概率，可选值为 0 到 1 之间的浮点数。\n    - 对于 ratelimiting 类型，参数为每秒采样 span 的个数。\n    - 对于 remote 类型，参数为采样概率，可选值为 0 到 1 之间的浮点数。\n+ 默认值：1.0\n\n### `sampling-server-url`\n\n+ jaeger-agent 采样服务器的 HTTP URL 地址。\n+ 默认值：\"\"\n\n### `max-operations`\n\n+ 采样器可追踪的最大操作数。如果一个操作没有被追踪，会启用默认的 probabilistic 采样器。\n+ 默认值：0\n\n### `sampling-refresh-interval`\n\n+ 控制远程轮询 jaeger-agent 采样策略的频率。\n+ 默认值：0\n\n## opentracing.reporter\n\nopentracing.reporter 相关的设置。\n\n### `queue-size`\n\n+ reporter 在内存中记录 spans 个数的队列容量。\n+ 默认值：0\n\n### `buffer-flush-interval`\n\n+ reporter 缓冲区的刷新频率。\n+ 默认值：0\n\n### `log-spans`\n\n+ 是否为所有提交的 span 打印日志。\n+ 默认值：false\n\n### `local-agent-host-port`\n\n+ reporter 向 jaeger-agent 发送 span 的地址。\n+ 默认值：\"\"\n\n## tikv-client\n\n### `grpc-connection-count`\n\n+ 跟每个 TiKV 之间建立的最大连接数。\n+ 默认值：4\n\n### `grpc-keepalive-time`\n\n+ TiDB 与 TiKV 节点之间 rpc 连接 keepalive 时间间隔，如果超过该值没有网络包，grpc client 会 ping 一下 TiKV 查看是否存活。\n+ 默认值：10\n+ 最小值：1\n+ 单位：秒\n\n### `grpc-keepalive-timeout`\n\n+ TiDB 与 TiKV 节点 rpc keepalive 检查的超时时间\n+ 默认值：3\n+ 最小值：0.05\n+ 单位：秒\n\n### `grpc-compression-type`\n\n+ 控制 TiDB 向 TiKV 节点传输数据使用的压缩算法类型。默认值为 \"none\" 即不压缩。修改为 \"gzip\" 可以使用 gzip 算法压缩数据。\n+ 默认值：\"none\"\n+ 可选值：\"none\", \"gzip\"\n\n> **注意：**\n>\n> TiKV 节点返回给 TiDB 的响应消息的压缩算法是由 TiKV 配置项 [`grpc-compression-type`](/tikv-configuration-file.md#grpc-compression-type) 控制的。\n\n### `commit-timeout`\n\n+ 执行事务提交时，最大的超时时间。\n+ 默认值：41s\n+ 这个值必须是大于两倍 Raft 选举的超时时间。\n\n### `batch-policy` <span class=\"version-mark\">从 v8.3.0 版本开始引入</span>\n\n+ 控制 TiDB 向 TiKV 发送请求时的批处理策略。TiDB 在向 TiKV 发送请求时，始终会将当前等待队列中的请求封装为 `BatchCommandsRequest` 并打包发送给 TiKV，这是基础的批处理机制。当 TiKV 负载吞吐较高时，TiDB 会根据 `batch-policy` 的配置决定是否在基础的批处理后额外等待一段时间，以在单个 `BatchCommandsRequest` 中封装更多的请求，即进行额外的批处理。\n+ 默认值：`\"standard\"`\n+ 可选值：\n    - `\"basic\"`：行为与 v8.3.0 之前的版本一致，即 TiDB 仅在 [`tikv-client.max-batch-wait-time`](#max-batch-wait-time) 大于 0 且 TiKV 的负载超过 [`tikv-client.overload-threshold`](#overload-threshold) 时进行额外的批处理。\n    - `\"standard\"`：TiDB 根据最近请求的到达时间间隔动态批处理，适用于高吞吐场景。\n    - `\"positive\"`：TiDB 始终进行额外的批处理，适用于高吞吐压测场景，以获得最佳性能。但在低负载场景下，该策略可能会引入不必要的批处理等待时间，从而导致性能下降。\n    - `\"custom{...}\"`：自定义批处理策略参数，仅用于 TiDB 内部测试，**不推荐用户使用**。\n\n### `max-batch-size`\n\n+ 批量发送 rpc 封包的最大数量，如果不为 0，将使用 BatchCommands api 发送请求到 TiKV，可以在并发度高的情况降低 rpc 的延迟，推荐不修改该值。\n+ 默认值：128\n\n### `max-batch-wait-time`\n\n+ 等待 `max-batch-wait-time` 纳秒批量将此期间的数据包封装成一个大包发送给 TiKV 节点，仅在 `tikv-client.max-batch-size` 值大于 0 时有效，不推荐修改该值。\n+ 默认值：0\n+ 单位：纳秒\n\n### `batch-wait-size`\n\n+ 批量向 TiKV 发送的封包最大数量，不推荐修改该值。\n+ 默认值：8\n+ 若此值为 0 表示关闭此功能。\n\n### `overload-threshold`\n\n+ TiKV 的负载阈值，如果超过此阈值，会收集更多的 batch 封包，来减轻 TiKV 的压力。仅在 `tikv-client.max-batch-size` 值大于 0 时有效，不推荐修改该值。\n+ 默认值：200\n\n### `copr-req-timeout` <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n> **警告：**\n>\n> 该配置项可能会在未来版本中废弃，**不要修改该配置**。\n\n+ 单个 Coprocessor Request 的超时时间\n+ 默认值：60\n+ 单位：秒\n\n### `enable-replica-selector-v2` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n> **警告：**\n>\n> 从 v8.2.0 开始，该配置项被废弃。给 TiKV 发送 RPC 请求时，默认使用新版本的 Region 副本选择器。\n\n+ 用于控制给 TiKV 发送 RPC 请求时，是否使用新版本的 Region 副本选择器。\n+ 默认值：true\n\n## tikv-client.copr-cache <span class=\"version-mark\">从 v4.0.0 版本开始引入</span>\n\n本部分介绍 Coprocessor Cache 相关的配置项。\n\n### `capacity-mb`\n\n+ 缓存的总数据量大小。当缓存空间满时，旧缓存条目将被逐出。值为 0.0 时表示关闭 Coprocessor Cache。\n+ 默认值：1000.0\n+ 单位：MB\n+ 类型：Float\n\n## txn-local-latches\n\n与事务锁存相关的配置项。这些配置项今后可能会废弃，不建议启用。\n\n### `enabled`\n\n- 控制是否开启事务的内存锁。\n- 默认值：`false`\n\n### `capacity`\n\n- Hash 对应的 slot 数量，自动向上调整为 2 的指数倍。每个 slot 占用 32 字节内存。如果设置过小，在数据写入范围较大的场景（如导入数据），可能会导致运行速度变慢，性能变差。\n- 默认值：`2048000`\n\n## status\n\nTiDB 服务状态相关配置。\n\n### `report-status`\n\n+ 开启 HTTP API 服务的开关。\n+ 默认值：true\n\n### `record-db-qps`\n\n+ 输出与 database 相关的 QPS metrics 到 Prometheus 的开关。\n+ 默认值：false\n\n### `record-db-label`\n\n- 控制是否向 Prometheus 传输与数据库相关的 QPS 指标。\n- 支持的指标类型比 `record-db-qps` 更多，比如 duration 和 statements。\n- 默认值：`false`\n\n## pessimistic-txn\n\n悲观事务使用方法请参考 [TiDB 悲观事务模式](/pessimistic-transaction.md)。\n\n### max-retry-count\n\n+ 悲观事务中单个语句最大重试次数，重试次数超过该限制，语句执行将会报错。\n+ 默认值：256\n\n### deadlock-history-capacity\n\n+ 单个 TiDB 节点的 [`INFORMATION_SCHEMA.DEADLOCKS`](/information-schema/information-schema-deadlocks.md) 表最多可记录的死锁事件个数。当表的容量已满时，如果再次发生死锁错误，最早的一次死锁错误的信息将从表中移除。\n+ 默认值：10\n+ 最小值：0\n+ 最大值：10000\n\n### deadlock-history-collect-retryable\n\n+ 控制 [`INFORMATION_SCHEMA.DEADLOCKS`](/information-schema/information-schema-deadlocks.md) 表中是否收集可重试的死锁错误信息。详见 `DEADLOCKS` 表文档的[可重试的死锁错误](/information-schema/information-schema-deadlocks.md#可重试的死锁错误)小节。\n+ 默认值：false\n\n### pessimistic-auto-commit\n\n+ 用来控制开启全局悲观事务模式下 (`tidb_txn_mode='pessimistic'`) 时，自动提交的事务使用的事务模式。默认情况下，即使开启全局悲观事务模式，自动提交事务依然使用乐观事务模式来执行。当开启该配置项后（设置为 `true`），在全局悲观事务模式下，自动提交事务将也使用悲观事务模式执行。行为与其他显式提交的悲观事务相同。\n+ 对于存在冲突的场景，开启本开关可以将自动提交事务纳入全局等锁管理中，从而避免死锁，改善冲突造成死锁带来的时延尖刺。\n+ 对于不存在冲突的场景，如果有大量自动提交事务（例如自动提交事务数量占业务数量的比例超过一半甚至更多，需要根据实际情况分析）且单个事务操作数据量较大的情况下，开启该配置项会造成性能回退。例如，自动提交的 `INSERT INTO SELECT` 语句。\n+ 当 SESSION 级系统变量 [`tidb_dml_type`](/system-variables.md#tidb_dml_type-从-v800-版本开始引入) 设置为 `\"bulk\"` 时，在该 SESSION 中，该配置项的效果等同于设置为 `false`。\n+ 默认值：false\n\n### constraint-check-in-place-pessimistic <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 用来控制系统变量 [`tidb_constraint_check_in_place_pessimistic`](/system-variables.md#tidb_constraint_check_in_place_pessimistic-从-v630-版本开始引入) 的默认值。\n+ 默认值：true\n\n## isolation-read\n\n读取隔离相关的配置项。\n\n### `engines`\n\n+ 用于控制 TiDB 节点允许从哪种类型的引擎读取数据。\n+ 默认值：[\"tikv\", \"tiflash\", \"tidb\"]，表示由优化器自动选择存储引擎。\n+ 可选值：\"tikv\", \"tiflash\", \"tidb\" 的组合，如：[\"tikv\", \"tidb\"]、[\"tiflash\", \"tidb\"]。\n\n## instance\n\n### `tidb_enable_collect_execution_info`\n\n+ 用于控制是否同时将各个执行算子的执行信息记录入 slow query log 中，以及是否维护[访问索引有关的统计信息](/information-schema/information-schema-tidb-index-usage.md)。\n+ 默认值：true\n+ 在 v6.1.0 之前，该功能通过配置项 `enable-collect-execution-info` 进行设置。\n\n### `tidb_enable_slow_log`\n\n+ 是否开启慢查询日志。\n+ 默认值：true\n+ 可以设置成 `true` 或 `false` 来启用或禁用慢查询日志。\n+ 在 v6.1.0 之前，该功能通过配置项 `enable-slow-log` 进行设置。\n\n### `tidb_slow_log_threshold`\n\n+ 输出慢日志的耗时阈值。\n+ 默认值：300\n+ 范围：`[-1, 9223372036854775807]`\n+ 单位：毫秒\n+ 如果查询耗时大于这个值，会视作一个慢查询，并记录到慢查询日志。注意，当日志的输出级别 [`log.level`](#level) 是 `\"debug\"` 时，所有查询都会记录到慢日志，不受该参数的限制。\n+ 在 v6.1.0 之前，该功能通过配置项 `slow-threshold` 进行设置。\n\n### `in-mem-slow-query-topn-num` <span class=\"version-mark\">从 v7.3.0 版本开始引入</span>\n\n+ 缓存在内存中的最慢的 slow query 个数。\n+ 默认值：30\n\n### `in-mem-slow-query-recent-num` <span class=\"version-mark\">从 v7.3.0 版本开始引入</span>\n\n+ 缓存在内存中的最近使用的 slow query 个数。\n+ 默认值：500\n\n### `tidb_expensive_query_time_threshold`\n\n+ 控制打印 expensive query 日志的阈值时间，默认值是 60 秒。expensive query 日志和慢日志的差别是，慢日志是在语句执行完后才打印，expensive query 日志可以把正在执行中且执行时间超过该阈值的语句及其相关信息打印出来。\n+ 默认值：60\n+ 范围：`[10, 2147483647]`\n+ 单位：秒\n+ 在 v5.4.0 之前，该功能通过配置项 `expensive-threshold` 进行设置。\n\n### `tidb_record_plan_in_slow_log`\n\n+ 在慢日志中记录执行计划。\n+ 默认值：1\n+ 0 表示关闭，1 表示开启，默认开启，该值作为系统变量 [`tidb_record_plan_in_slow_log`](/system-variables.md#tidb_record_plan_in_slow_log) 的初始值。\n+ 在 v6.1.0 之前，该功能通过配置项 `record-plan-in-slow-log` 进行设置。\n\n### `tidb_force_priority`\n\n+ 把所有的语句优先级设置为系统变量 `tidb_force_priority` 的值。\n+ 默认值：NO_PRIORITY\n+ 默认值 NO_PRIORITY 表示不强制改变执行语句的优先级，其它优先级从低到高可设置为 LOW_PRIORITY、DELAYED 或 HIGH_PRIORITY。\n+ 在 v6.1.0 之前，该功能通过配置项 `force-priority` 进行设置。\n\n> **注意：**\n>\n> TiDB 从 v6.6.0 版本开始支持[使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)功能。该功能可以将不同优先级的语句放在不同的资源组中执行，并为这些资源组分配不同的配额和优先级，可以达到更好的资源管控效果。在开启资源管控功能后，语句的调度主要受资源组的控制，`PRIORITY` 将不再生效。建议在支持资源管控的版本优先使用资源管控功能。\n\n### `max_connections`\n\n+ TiDB 中同时允许的最大客户端连接数，用于资源控制。\n+ 默认值：0\n+ 取值范围：`[0, 100000]`\n+ 默认情况下，TiDB 不限制客户端连接数。当本配置项的值大于 `0` 且客户端连接数到达此值时，TiDB 服务端将会拒绝新的客户端连接。\n+ 该值作为系统变量 [`max_connections`](/system-variables.md#max_connections) 的初始值。\n+ 在 v6.2.0 之前，该功能通过配置项 `max-server-connections` 进行设置。\n\n### `tidb_enable_ddl`\n\n+ 用于表示该 tidb-server 是否可以成为 DDL owner。\n+ 默认值：true\n+ 该值作为系统变量 [`tidb_enable_ddl`](/system-variables.md#tidb_enable_ddl-从-v630-版本开始引入) 的初始值。\n+ 在 v6.3.0 之前，该功能由配置项 `run-ddl` 进行设置。\n\n### `tidb_enable_stats_owner` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n+ 用于表示该 tidb-server 是否可以运行[统计信息自动更新](/statistics.md#自动更新)任务。\n+ 默认值：`true`\n+ 可选值：`true`、`false`\n+ 该值作为系统变量 [`tidb_enable_stats_owner`](/system-variables.md#tidb_enable_stats_owner-从-v840-版本开始引入) 的初始值。\n\n### `tidb_stmt_summary_enable_persistent` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 用于控制是否开启 statements summary 持久化。\n+ 默认值：false\n+ 详情参考[持久化 statements summary](/statement-summary-tables.md#持久化-statements-summary)。\n\n### `tidb_stmt_summary_filename` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 当开启了 statements summary 持久化时，该配置用于指定持久化数据所写入的文件。\n+ 默认值：\"tidb-statements.log\"\n\n### `tidb_stmt_summary_file_max_days` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 当开启了 statements summary 持久化时，该配置用于指定持久化数据文件所保留的最大天数。\n+ 默认值：3\n+ 单位：天\n+ 可结合数据保留时长需求与磁盘空间占用适当调整。\n\n### `tidb_stmt_summary_file_max_size` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 当开启了 statements summary 持久化时，该配置用于限制持久化数据单个文件的大小。\n+ 默认值：64\n+ 单位：MiB\n+ 可结合数据保留时长需求与磁盘空间占用适当调整。\n\n### `tidb_stmt_summary_file_max_backups` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> statements summary 持久化目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 当开启了 statements summary 持久化时，该配置用于限制持久化数据文件最大数量，`0` 表示不限制。\n+ 默认值：0\n+ 可结合数据保留时长需求与磁盘空间占用适当调整。\n\n## proxy-protocol\n\nPROXY 协议相关的配置项。\n\n### `networks`\n\n+ 允许使用 [PROXY 协议](https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt)连接 TiDB 的代理服务器地址列表。\n+ 默认值：\"\"\n+ 通常情况下，通过反向代理使用 TiDB 时，TiDB 会将反向代理服务器的 IP 地址视为客户端 IP 地址。对于支持 PROXY 协议的反向代理（如 HAProxy），开启 PROXY 协议后能让反向代理透传客户端真实的 IP 地址给 TiDB。\n+ 配置该参数后，TiDB 将允许配置的源 IP 地址使用 PROXY 协议连接到 TiDB，且拒绝这些源 IP 地址使用非 PROXY 协议连接。若该参数为空，则任何源 IP 地址都不能使用 PROXY 协议连接到 TiDB。地址可以使用 IP 地址格式 (192.168.1.50) 或者 CIDR 格式 (192.168.1.0/24)，并可用 `,` 分隔多个地址，或用 `*` 代表所有 IP 地址。\n\n> **警告：**\n>\n> 需谨慎使用 `*` 符号，因为 `*` 允许来自任何 IP 的客户端自行汇报其 IP 地址，从而可能引入安全风险。另外，`*` 可能导致部分直接连接 TiDB 的内部组件无法使用，例如 TiDB Dashboard。\n\n### `fallbackable` <span class=\"version-mark\">从 v6.5.1 版本开始引入</span>\n\n+ 用于控制是否启用 PROXY 协议回退模式。如果设置为 `true`，TiDB 可以接受属于 `proxy-protocol.networks` 的客户端使用非 PROXY 协议规范或者没有发送 PROXY 协议头的客户端连接。默认情况下，TiDB 仅接受属于 `proxy-protocol.networks` 的客户端发送 PROXY 协议头的客户端连接。\n+ 默认：`false`\n\n## experimental\n\nexperimental 部分为 TiDB 实验功能相关的配置。该部分从 v3.1.0 开始引入。\n\n### `allow-expression-index` <span class=\"version-mark\">从 v4.0.0 版本开始引入</span>\n\n+ 用于控制是否能创建表达式索引。自 v5.2.0 版本起，如果表达式中的函数是安全的，你可以直接基于该函数创建表达式索引，不需要打开该配置项。如果要创建基于其他函数的表达式索引，可以打开该配置项，但可能存在正确性问题。通过查询 `tidb_allow_function_for_expression_index` 变量可得到能直接用于创建表达式的安全函数。\n+ 默认值：false\n"
        },
        {
          "name": "tidb-control.md",
          "type": "blob",
          "size": 10.630859375,
          "content": "---\ntitle: TiDB Control 使用说明\naliases: ['/docs-cn/dev/tidb-control/','/docs-cn/dev/reference/tools/tidb-control/','/docs-cn/tools/tidb-controller/']\nsummary: TiDB Control 是 TiDB 的命令行工具，用于获取 TiDB 状态信息和调试。可通过 TiUP 安装或从源代码编译安装。使用介绍包括命令、选项和参数组成，以及全局参数和各子命令的功能。其中包括获取帮助信息、解码 base64 数据、解码 row key 和 value、操作 etcd、格式化日志文件，以及查询关键 key range 信息。注意：TiDB Control 主要用于诊断调试，不保证和 TiDB 未来引入的新特性完全兼容。\n---\n\n# TiDB Control 使用说明\n\nTiDB Control 是 TiDB 的命令行工具，用于获取 TiDB 状态信息，多用于调试。本文介绍了 TiDB Control 的主要功能和各个功能的使用方法。\n\n> **注意：**\n>\n> TiDB Control 主要用于诊断调试，不保证和 TiDB 未来引入的新特性完全兼容。因此不推荐客户在应用程序开发或工具开发中利用 TiDB Control 获取结果。 \n\n## 获取 TiDB Control\n\n本节提供了两种方式获取 TiDB Control 工具。\n\n> **注意：**\n>\n> 建议使用的 Control 工具版本与集群版本保持一致。\n\n### 通过 TiUP 安装\n\n在安装 TiUP 之后，可以使用 `tiup ctl:v<CLUSTER_VERSION> tidb` 命令来获取 TiDB Control 的二进制程序以及运行 TiDB Control。\n\n### 从源代码编译安装\n\n编译环境要求：[Go](https://golang.org/) 1.23 或以上版本\n\n编译步骤：在 [TiDB Control 项目](https://github.com/pingcap/tidb-ctl)根目录，使用 `make` 命令进行编译，生成 tidb-ctl。\n\n编译文档：帮助文档在 doc 文件夹下，如丢失或需要更新，可通过 `make doc` 命令生成帮助文档。\n\n## 使用介绍\n\n`tidb-ctl` 的使用由命令（包括子命令）、选项和参数组成。命令即不带 `-` 或者 `--` 的字符，选项即带有 `-` 或者 `--` 的字符，参数即命令或选项字符后紧跟的传递给命令和选项的字符。\n\n如：`tidb-ctl schema in mysql -n db`\n\n* schema: 命令\n* in: schema 的子命令\n* mysql: in 的参数\n* -n: 选项\n* db: -n 的参数\n\n目前，TiDB Control 包含以下子命令。\n\n* `tidb-ctl base64decode` 用于 BASE64 解码\n* `tidb-ctl decoder` 用于 KEY 解码\n* `tidb-ctl etcd` 用于操作 etcd\n* `tidb-ctl log` 用于格式化日志文件，将单行的堆栈信息展开\n* `tidb-ctl mvcc` 用于获取 MVCC 信息\n* `tidb-ctl region` 用于获取 Region 信息\n* `tidb-ctl schema` 用于获取 Schema 信息\n* `tidb-ctl table` 用于获取 Table 信息\n\n### 获取帮助\n\n`tidb-ctl -h/--help` 用于获取帮助信息。tidb-ctl 由多层命令组成，tidb-ctl 及其所有子命令都可以通过 `-h/--help` 来获取使用帮助。\n\n以获取 Schema 信息为例：\n\n通过 `tidb-ctl schema -h` 可以获取这个子命令的使用帮助。schema 有两个子命令——in 和 tid。in 用来通过数据库名获取数据库中所有表的表结构，tid 用来通过全数据库唯一的 table_id 获取表的表结构。\n\n### 全局参数\n\n`tidb-ctl` 有 4 个与连接相关的全局参数，分别为：\n\n- `--host` TiDB 服务地址\n- `--port` TiDB status 端口\n- `--pdhost` PD 服务地址\n- `--pdport` PD 服务端口\n- `--ca` 连接使用的 TLS CA 文件路径\n- `--ssl-key` 连接使用的 TLS 密钥文件路径\n- `--ssl-cert` 连接使用的 TLS 证书文件路径\n\n其中 `--pdhost` 和 `--pdport` 主要是用于 `etcd` 子命令，例如：`tidb-ctl etcd ddlinfo`。如不添加地址和端口将使用默认值，TiDB/PD 服务默认的地址是 127.0.0.1（服务地址只能使用 IP 地址），TiDB 服务端口默认的端口是 10080，PD 服务端口默认的端口是 2379 **连接选项是全局选项，适用于以下所有命令。**\n\n### schema 命令\n\n#### in 子命令\n\nin 子命令用来通过数据库名获取数据库中所有表的表结构。\n\n`tidb-ctl schema in {数据库名}`\n\n如：`tidb-ctl schema in mysql` 将得到以下结果\n\n```json\n[\n    {\n        \"id\": 13,\n        \"name\": {\n            \"O\": \"columns_priv\",\n            \"L\": \"columns_priv\"\n        },\n              ...\n        \"update_timestamp\": 399494726837600268,\n        \"ShardRowIDBits\": 0,\n        \"Partition\": null\n    }\n]\n```\n\n结果将以 json 形式展示，内容较长，这里做了截断。\n\n如希望指定表名，可以使用 `tidb-ctl schema in {数据库名} -n {表名}` 进行过滤。\n\n如：`tidb-ctl schema in mysql -n db` 将得到 mysql 库中 db 表的表结构。结果如下：\n\n```json\n{\n    \"id\": 9,\n    \"name\": {\n        \"O\": \"db\",\n        \"L\": \"db\"\n    },\n    ...\n    \"Partition\": null\n}\n```\n\n这里同样做了截断。\n\n如使用的 TiDB 地址不为默认地址和端口，可以使用命令行参数 `--host`, `--port` 选项，如：`tidb-ctl --host 172.16.55.88 --port 8898 schema in mysql -n db`。\n\n#### tid 子命令\n\ntid 子命令用来通过表的 id 获取数据库中表的表结构。\n\n通过使用 in 子命令查询到数据库中表的 id，之后可以通过 tid 子命令查看表的详细信息。\n\n例如，查询到 `mysql.stat_meta` 表的 id 是 21，可以通过 `tidb-ctl schema tid -i 21` 查看表的详细信息。\n\n```json\n{\n \"id\": 21,\n \"name\": {\n  \"O\": \"stats_meta\",\n  \"L\": \"stats_meta\"\n },\n \"charset\": \"utf8mb4\",\n \"collate\": \"utf8mb4_bin\",\n  ...\n}\n```\n\n同 in 子命令一样，如果使用的 TiDB 地址不是默认的地址和端口，需要通过 `--host` 和 `--port` 参数指定 TiDB 的地址和 status 端口。\n\n### base64decode 命令\n\n`base64decode` 用来解码 base64 数据。\n\n基本用法：\n\n```shell\ntidb-ctl base64decode [base64_data]\ntidb-ctl base64decode [db_name.table_name] [base64_data]\ntidb-ctl base64decode [table_id] [base64_data]\n```\n\n1. 准备环境，执行以下 SQL\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    use test;\n    create table t (a int, b varchar(20),c datetime default current_timestamp , d timestamp default current_timestamp, unique index(a));\n    insert into t (a,b,c) values(1,\"哈哈 hello\",NULL);\n    alter table t add column e varchar(20);\n    ```\n\n2. 用 HTTP API 接口获取 MVCC 数据\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    curl \"http://$IP:10080/mvcc/index/test/t/a/1?a=1\"\n    ```\n\n    ```\n    {\n     \"info\": {\n      \"writes\": [\n       {\n        \"start_ts\": 407306449994645510,\n        \"commit_ts\": 407306449994645513,\n        \"short_value\": \"AAAAAAAAAAE=\"    # unique index a 存的值是对应行的 handle id.\n       }\n      ]\n     }\n    }%\n    ```\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    curl \"http://$IP:10080/mvcc/key/test/t/1\"\n    ```\n\n    ```\n    {\n     \"info\": {\n      \"writes\": [\n       {\n        \"start_ts\": 407306588892692486,\n        \"commit_ts\": 407306588892692489,\n        \"short_value\": \"CAIIAggEAhjlk4jlk4ggaGVsbG8IBgAICAmAgIDwjYuu0Rk=\"  # handle id 为 1 的行数据。\n       }\n      ]\n     }\n    }%\n    ```\n\n3. 用 `base64decode` 解码 handle id (uint64).\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tidb-ctl base64decode AAAAAAAAAAE=\n    ```\n\n    ```\n    hex: 0000000000000001\n    uint64: 1\n    ```\n\n4. 用 `base64decode` 解码行数据。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    ./tidb-ctl base64decode test.t CAIIAggEAhjlk4jlk4ggaGVsbG8IBgAICAmAgIDwjYuu0Rk=\n    ```\n\n    ```\n    a:      1\n    b:      哈哈 hello\n    c is NULL\n    d:      2019-03-28 05:35:30\n    e not found in data\n    ```\n\n    如果 `test.t` 的 table id 是 60，你也可以使用下列命令获得同样结果：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    ./tidb-ctl base64decode 60 CAIIAggEAhjlk4jlk4ggaGVsbG8IBgAICAmAgIDwjYuu0Rk=\n    ```\n\n    ```\n    a:      1\n    b:      哈哈 hello\n    c is NULL\n    d:      2019-03-28 05:35:30\n    e not found in data\n    ```\n\n### decoder 命令\n\n* 以下示例解码 row key，index key 类似。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    ./tidb-ctl decoder \"t\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1c_r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xfa\"\n    ```\n\n    ```\n    format: table_row\n    table_id: -9223372036854775780\n    row_id: -9223372036854775558\n    ```\n\n* 以下示例解码 value\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    ./tidb-ctl decoder AhZoZWxsbyB3b3JsZAiAEA==\n    ```\n\n    ```\n    format: index_value\n    index_value[0]: {type: bytes, value: hello world}\n    index_value[1]: {type: bigint, value: 1024}\n    ```\n\n### etcd 命令\n\n* `tidb-ctl etcd ddlinfo` 获取 DDL 信息。\n* `tidb-ctl etcd putkey KEY VALUE` 添加 KEY VALUE 到 etcd（所有的 KEY 会添加到 `/tidb/ddl/all_schema_versions/` 之下）。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tidb-ctl etcd putkey \"foo\" \"bar\"\n    ```\n\n    实际是添加 KEY 为 `/tidb/ddl/all_schema_versions/foo`，VALUE 为 `bar` 的键值对到 etcd 中。\n\n* `tidb-ctl etcd delkey` 删除 etcd 中的 KEY，只有前缀以 `/tidb/ddl/fg/owner/` 和 `/tidb/ddl/all_schema_versions/` 开头才允许被删除。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tidb-ctl etcd delkey \"/tidb/ddl/fg/owner/foo\" &&\n    tidb-ctl etcd delkey \"/tidb/ddl/all_schema_versions/bar\"\n    ```\n\n### log 命令\n\nTiDB 错误日志的堆栈信息是一行的格式，可以使用 `tidb-ctl log` 将堆栈信息格式化成多行形式。\n\n### keyrange 命令\n\n`keyrange` 子命令用于查询全局或表相关的关键 key range 信息，以十六进制形式输出。\n\n* 使用 `tidb-ctl keyrange` 命令查看全局的关键 key range。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tidb-ctl keyrange\n    ```\n\n    ```\n    global ranges:\n      meta: (6d, 6e)\n      table: (74, 75)\n    ```\n\n* 添加 `--encode` 选项可以显示 encode 过的 key（与 TiKV 及 PD 中的格式相同）。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tidb-ctl keyrange --encode\n    ```\n\n    ```\n    global ranges:\n      meta: (6d00000000000000f8, 6e00000000000000f8)\n      table: (7400000000000000f8, 7500000000000000f8)\n    ```\n\n* 使用 `tidb-ctl keyrange --database={db} --table={tbl}` 命令查看全局和表相关的关键 key range。\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tidb-ctl keyrange --database test --table ttt\n    ```\n\n    ```\n    global ranges:\n      meta: (6d, 6e)\n      table: (74, 75)\n    table ttt ranges: (NOTE: key range might be changed after DDL)\n      table: (74800000000000002f, 748000000000000030)\n      table indexes: (74800000000000002f5f69, 74800000000000002f5f72)\n        index c2: (74800000000000002f5f698000000000000001, 74800000000000002f5f698000000000000002)\n        index c3: (74800000000000002f5f698000000000000002, 74800000000000002f5f698000000000000003)\n        index c4: (74800000000000002f5f698000000000000003, 74800000000000002f5f698000000000000004)\n      table rows: (74800000000000002f5f72, 748000000000000030)\n    ```\n"
        },
        {
          "name": "tidb-distributed-execution-framework.md",
          "type": "blob",
          "size": 8.1826171875,
          "content": "---\ntitle: TiDB 分布式执行框架\nsummary: 了解 TiDB 分布式执行框架的使用场景、限制、使用方法和实现原理。\n---\n\n# TiDB 分布式执行框架\n\nTiDB 采用计算存储分离架构，具有出色的扩展性和弹性的扩缩容能力。从 v7.1.0 开始，TiDB 引入了一个分布式执行框架，以进一步发挥分布式架构的资源优势。该框架的目标是对基于该框架的任务进行统一调度与分布式执行，并提供整体和单个任务两个维度的资源管理能力，更好地满足用户对于资源使用的预期。\n\n本文档介绍了 TiDB 分布式执行框架的使用场景、限制、使用方法和实现原理。\n\n## 使用场景\n\n在数据库中，除了核心的事务型负载任务 (TP) 和分析型查询任务 (AP)，也存在着其他重要任务，如 DDL 语句、[`IMPORT INTO`](/sql-statements/sql-statement-import-into.md)、[TTL](/time-to-live.md)、[`ANALYZE`](/sql-statements/sql-statement-analyze-table.md) 和 Backup/Restore 等。这些任务需要处理数据库对象（表）中的大量数据，通常具有如下特点：\n\n- 需要处理一个 schema 或者一个数据库对象（表）中的所有数据。\n- 可能需要周期执行，但频率较低。\n- 如果资源控制不当，容易对事务型任务和分析型任务造成影响，影响数据库的服务质量。\n\n启用 TiDB 分布式执行框架能够解决上述问题，并且具有以下三个优势：\n\n- 提供高扩展性、高可用性和高性能的统一能力支持。\n- 支持任务分布式执行，可以在整个 TiDB 集群可用的计算资源范围内进行灵活的调度，从而更好地利用 TiDB 集群内的计算资源。\n- 提供统一的资源使用和管理能力，从整体和单个任务两个维度提供资源管理的能力。\n\n目前，分布式执行框架支持分布式执行 [`ADD INDEX`](/sql-statements/sql-statement-add-index.md) 和 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md) 这两类任务。\n\n- [`ADD INDEX`](/sql-statements/sql-statement-add-index.md)，即 DDL 创建索引的场景。例如以下 SQL 语句：\n\n    ```sql\n    ALTER TABLE t1 ADD INDEX idx1(c1);\n    CREATE INDEX idx1 ON table t1(c1);\n    ```\n\n- [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md) 即通过该 SQL 语句将 CSV、SQL、PARQUET 等格式的数据导入到一张空表中。\n\n## 使用限制\n\n分布式执行框架最多同时调度 16 个任务（包括 [`ADD INDEX`](/sql-statements/sql-statement-add-index.md) 和 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md)）。\n\n## 启用前提\n\n如需使用分布式执行框架执行 [`ADD INDEX`](/sql-statements/sql-statement-add-index.md) 任务，需要先开启 [Fast Online DDL](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入) 模式。\n\n1. 调整 Fast Online DDL 相关的系统变量：\n\n    * [`tidb_ddl_enable_fast_reorg`](/system-variables.md#tidb_ddl_enable_fast_reorg-从-v630-版本开始引入)：从 TiDB v6.5.0 开始默认打开，用于启用快速模式。\n    * [`tidb_ddl_disk_quota`](/system-variables.md#tidb_ddl_disk_quota-从-v630-版本开始引入)：用于控制快速模式可使用的本地磁盘最大配额。\n\n2. 调整 Fast Online DDL 相关的配置项：\n\n    * [`temp-dir`](/tidb-configuration-file.md#temp-dir-从-v630-版本开始引入)：指定快速模式能够使用的本地盘路径。\n\n> **注意：**\n>\n> 建议 TiDB 的 `temp-dir` 目录至少有 100 GiB 的可用空间。\n\n## 启用步骤\n\n1. 启用分布式执行框架，只需将 [`tidb_enable_dist_task`](/system-variables.md#tidb_enable_dist_task-从-v710-版本开始引入) 设置为 `ON`。该变量从 v8.1.0 起默认开启，对于新建的 v8.1.0 或更高版本集群，可以跳过此步骤。\n\n    ```sql\n    SET GLOBAL tidb_enable_dist_task = ON;\n    ```\n\n    在运行任务时，框架支持的语句（如 [`ADD INDEX`](/sql-statements/sql-statement-add-index.md) 和 [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md)）会采用分布式方式执行。默认集群内部所有节点均会执行任务。\n\n2. 一般情况下，对于下列影响 DDL 任务分布式执行的系统变量，使用其默认值即可。\n\n    * [`tidb_ddl_reorg_worker_cnt`](/system-variables.md#tidb_ddl_reorg_worker_cnt)：使用默认值 `4` 即可，建议最大不超过 `16`。\n    * [`tidb_ddl_reorg_priority`](/system-variables.md#tidb_ddl_reorg_priority)\n    * [`tidb_ddl_error_count_limit`](/system-variables.md#tidb_ddl_error_count_limit)\n    * [`tidb_ddl_reorg_batch_size`](/system-variables.md#tidb_ddl_reorg_batch_size)：使用默认值即可，建议最大不超过 `1024`。\n\n## 任务调度\n\n默认情况下，分布式执行框架将会调度所有 TiDB 节点执行分布式任务。从 v7.4.0 起，你可以通过设置 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) 来控制分布式执行框架将会调度哪些 TiDB 节点执行分布式任务。\n\n- 在 v7.4.0 到 v8.0.0 及其之间的版本中，[`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) 的可选值为 `''` 或 `background`。如果当前集群存在 `tidb_service_scope = 'background'` 的 TiDB 节点，分布式执行框架会将该任务调度到 `tidb_service_scope = 'background'` 的节点上运行。如果当前集群不存在 `tidb_service_scope = 'background'` 的节点，无论是因为故障还是正常的缩容，分布式执行框架会将任务调度到 `tidb_service_scope = ''` 的节点上运行。\n\n- 从 v8.1.0 起，[`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) 可设置为任意合法值。当提交分布式任务时，该任务会绑定当前连接的 TiDB 节点的 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) 值，分布式执行框架只会将该任务调度到具有相同 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) 值的 TiDB 节点上运行。但是，为了兼容之前版本的配置，如果分布式任务是在 `tidb_service_scope = ''` 的节点上提交的，且当前集群存在 `tidb_service_scope = 'background'` 的节点，分布式执行框架会将该任务调度到 `tidb_service_scope = 'background'` 的 TiDB 节点上运行。\n\n从 v8.1.0 起，如果在任务运行过程中扩容新节点，分布式执行框架会根据上述规则决定是否将任务调度到新的节点来执行。如果不希望新扩容的节点运行任务，建议提前为这些节点设置 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入)，取值要和已经在运行分布式任务的 TiDB 节点不同。\n\n> **注意：**\n>\n> - 在 v7.4.0 到 v8.0.0 及其之间的版本中，对于包含多个 TiDB 节点的集群，强烈建议选择两个或更多的 TiDB 节点将 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) 设置为 `background`。若仅在单个 TiDB 节点上设置此变量，当该节点发生重启或故障时，任务会被重新调度到 `tidb_service_scope = ''` 的 TiDB 节点，会对这些 TiDB 节点的业务产生影响。\n> - 在分布式任务执行过程中，修改 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入) 的配置不会对当前任务生效，会从下次任务开始生效。\n\n## 实现原理\n\nTiDB 分布式执行框架的架构图如下：\n\n![分布式执行框架的架构](/media/dist-task/dist-task-architect.jpg)\n\n根据上图，分布式执行框架中任务的执行主要由以下模块负责：\n\n- Dispatcher：负责生成每个任务的分布式执行计划，管理执行过程，转换任务状态以及收集和反馈运行时任务信息等。\n- Scheduler：以 TiDB 节点为单位来同步分布式任务的执行，提高执行效率。\n- Subtask Executor：是实际的分布式子任务执行者，并将子任务的执行情况返回给 Scheduler，由 Scheduler 统一更新子任务的执行状态。\n- 资源池：通过对上述各种模块中计算资源进行池化，提供量化资源的使用与管理的基础。\n\n## 另请参阅\n\n* [DDL 执行原理及最佳实践](/ddl-introduction.md)\n"
        },
        {
          "name": "tidb-external-ts.md",
          "type": "blob",
          "size": 3.3623046875,
          "content": "---\ntitle: 通过系统变量 `tidb_external_ts` 读取历史数据\nsummary: 了解如何通过系统变量 `tidb_external_ts` 读取历史数据。\n---\n\n# 通过系统变量 `tidb_external_ts` 读取历史数据\n\n为了支持读取历史版本数据，TiDB 从 v6.4.0 起引入了一个新的系统变量 [`tidb_external_ts`](/system-variables.md#tidb_external_ts-从-v640-版本开始引入)。本文档介绍如何通过该系统变量读取历史数据，其中包括具体的操作流程。\n\n## 场景介绍\n\n通过配置让 TiDB 能够读取某一固定时间点的历史数据对于 TiCDC 等数据同步工具非常有用。在数据同步工具完成了某一时间点前的数据同步之后，可以通过设置下游 TiDB 的 `tidb_external_ts` 系统变量，使得下游 TiDB 的请求能够读取到该时间点前的数据。这将避免在同步过程中，下游 TiDB 读取到尚未完全同步而不一致的数据。\n\n## 功能介绍\n\n系统变量 [`tidb_external_ts`](/system-variables.md#tidb_external_ts-从-v640-版本开始引入) 用于指定启用 `tidb_enable_external_ts_read` 时，读取历史数据使用的时间戳。\n\n系统变量 [`tidb_enable_external_ts_read`](/system-variables.md#tidb_enable_external_ts_read-从-v640-版本开始引入) 控制着是否在当前会话或全局启用读取历史数据的功能。默认值为 `OFF`，这意味着该功能关闭，并且设置 `tidb_external_ts` 没有作用。当该变量被全局地设置为 `ON` 时，所有的请求都将读取到 `tidb_external_ts` 指定时间之前的历史数据。如果 `tidb_enable_external_ts_read` 仅在某一会话被设置为 `ON`，则只有该会话中的请求会读取到历史数据。\n\n当 `tidb_enable_external_ts_read` 被设置为 `ON` 时，TiDB 会进入只读模式，任何写请求都会失败并且返回错误 `ERROR 1836 (HY000): Running in read-only mode`。\n\n## 示例\n\n以下是一个使用该功能的示例：\n\n1. 创建一个表后，在表中插入几行数据：\n\n    ```sql\n    CREATE TABLE t (c INT);\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    ```sql\n    INSERT INTO t VALUES (1), (2), (3);\n    ```\n\n    ```\n    Query OK, 3 rows affected (0.00 sec)\n    ```\n\n2. 查看表中的数据：\n\n    ```sql\n    SELECT * FROM t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |    2 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n3. 将 `tidb_external_ts` 设置为 `@@tidb_current_ts`：\n\n    ```sql\n    START TRANSACTION;\n    SET GLOBAL tidb_external_ts = @@tidb_current_ts;\n    COMMIT;\n    ```\n\n4. 插入新的一行并确认新的一行已经被插入：\n\n    ```sql\n    INSERT INTO t VALUES (4);\n    ```\n\n    ```\n    Query OK, 1 row affected (0.001 sec)\n    ```\n\n    ```sql\n    SELECT * FROM t;\n    ```\n\n    ```\n    +------+\n    | id   |\n    +------+\n    |    1 |\n    |    2 |\n    |    3 |\n    |    4 |\n    +------+\n    4 rows in set (0.00 sec)\n    ```\n\n5. 将 `tidb_enable_external_ts_read` 设置为 `ON` 后，再次查询表中的数据：\n\n    ```sql\n    SET tidb_enable_external_ts_read = ON;\n    SELECT * FROM t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |    2 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n    因为 `tidb_external_ts` 被设置为插入这一行之前的时间，在启动 `tidb_enable_external_ts_read` 后，将读取不到新插入的行。\n"
        },
        {
          "name": "tidb-global-sort.md",
          "type": "blob",
          "size": 4.25390625,
          "content": "---\ntitle: TiDB 全局排序\nsummary: 了解 TiDB 全局排序功能的使用场景、限制、使用方法和实现原理。\n---\n\n# TiDB 全局排序\n\n> **注意：**\n>\n> - 目前，全局排序会使用大量 TiDB 节点的计算与内存资源。对于在线增加索引等同时有用户业务在运行的场景，建议为集群添加新的 TiDB 节点，为这些 TiDB 节点设置 [`tidb_service_scope`](/system-variables.md#tidb_service_scope-从-v740-版本开始引入)，并连接到这些节点上创建任务。这样分布式框架就会将任务调度到这些节点上，将工作负载与其他 TiDB 节点隔离，以减少执行后端任务（如 `ADD INDEX` 和 `IMPORT INTO`）对用户业务的影响。\n> - 当需要使用全局排序功能时，为避免 OOM，建议 TiDB 节点的规格至少为 16 核 CPU、32 GiB 内存。\n\n## 功能概览\n\nTiDB 全局排序功能增强了数据导入和 DDL（数据定义语言）操作的稳定性和执行效率。全局排序作为[分布式执行框架](/tidb-distributed-execution-framework.md)中的通用算子，通过分布式执行框架，在云上提供全局排序服务。\n\n全局排序目前支持使用 Amazon S3 作为云存储。\n\n## 目标\n\n全局排序功能旨在提高 `IMPORT INTO` 和 `CREATE INDEX` 的稳定性与效率。通过将任务需要处理的数据进行全局排序，可以提高数据写入 TiKV 的稳定性、可控性和可扩展性，从而提供更好的数据导入与 DDL 任务的用户体验及更高质量的服务。\n\n全局排序功能在分布式执行框架中执行任务，确保所需处理的数据在全局范围内保持有序。\n\n## 限制\n\n目前，全局排序功能不支持在查询过程中对查询结果进行排序。\n\n## 使用方法\n\n要开启全局排序功能，执行以下步骤：\n\n1. 将 [`tidb_enable_dist_task`](/system-variables.md#tidb_enable_dist_task-从-v710-版本开始引入) 的值设置为 `ON`，以开启分布式执行框架。该变量从 v8.1.0 起默认开启，对于新建的 v8.1.0 或更高版本集群，可以跳过此步骤。\n\n    ```sql\n    SET GLOBAL tidb_enable_dist_task = ON;\n    ```\n\n2. 将 [`tidb_cloud_storage_uri`](/system-variables.md#tidb_cloud_storage_uri-从-v740-版本开始引入) 设置为正确的云存储路径。参见[示例](/br/backup-and-restore-storages.md)。\n\n    ```sql\n    SET GLOBAL tidb_cloud_storage_uri = 's3://my-bucket/test-data?role-arn=arn:aws:iam::888888888888:role/my-role'\n    ```\n\n> **注意：**\n>\n> [`IMPORT INTO`](/sql-statements/sql-statement-import-into.md) 也可使用 [`CLOUD_STORAGE_URI`](/sql-statements/sql-statement-import-into.md#withoptions) 参数来控制云存储路径。如果 [`tidb_cloud_storage_uri`](/system-variables.md#tidb_cloud_storage_uri-从-v740-版本开始引入) 和 `CLOUD_STORAGE_URI` 都设置了有效的云存储路径，[`IMPORT INTO`](/sql-statements/sql-statement-import-into.md) 将以 `CLOUD_STORAGE_URI` 参数的配置为准。\n\n## 实现原理\n\n全局排序功能的算法如下图所示：\n\n![全局排序功能算法](/media/dist-task/global-sort.jpeg)\n\n详细的实现原理如下：\n\n### 第 1 步：扫描和准备数据\n\n1. TiDB 节点扫描特定范围的数据后（数据源可以是 CSV 数据或者 TiKV 中的表数据）：\n\n    1. TiDB 节点将扫描的数据编码为键值对。\n    2. TiDB 节点将键值对排序为多个块数据段（数据段局部有序），每个段是一个文件，并将这些文件上传到云存储中。\n\n2. TiDB 节点记录了每个段的连续实际键值范围（称为统计信息文件），这是可扩展排序实现的关键准备工作。这些文件随实际数据一起上传到云存储中。\n\n### 第 2 步：排序和分发数据\n\n从第一步中，全局排序的程序获取了一个已排序块的列表及其对应的统计信息文件，这些文件记录了本地已排序块的数量。此外，全局排序程序还记录了一个实际数据范围，供 PD 用于数据拆分和打散。接下来将执行以下步骤：\n\n1. 将统计信息文件中的记录排序，划分为大小相近的范围，每个范围将作为一个并行执行的子任务。\n2. 将子任务分发给 TiDB 节点执行。\n3. 每个 TiDB 节点独立地对子任务的数据进行排序，并在没有重叠的情况下将数据导入到 TiKV 中。\n"
        },
        {
          "name": "tidb-in-kubernetes.md",
          "type": "blob",
          "size": 1.314453125,
          "content": "---\ntitle: 在 Kubernetes 上部署 TiDB 集群\nsummary: 你可以使用 TiDB Operator 在 Kubernetes 上部署 TiDB。TiDB Operator 是 Kubernetes 上的 TiDB 集群自动运维系统，提供部署、升级、扩缩容、备份恢复、配置变更的 TiDB 全生命周期管理。借助 TiDB Operator，TiDB 可以无缝运行在公有云或自托管的 Kubernetes 集群上。TiDB Operator 的文档目前独立于 TiDB 文档。要查看如何在 Kubernetes 上部署 TiDB 的详细步骤，请参阅对应版本的 TiDB Operator 文档。\n---\n\n# 在 Kubernetes 上部署 TiDB 集群\n\n你可以使用 [TiDB Operator](https://github.com/pingcap/tidb-operator) 在 Kubernetes 上部署 TiDB 集群。TiDB Operator 是 Kubernetes 上的 TiDB 集群自动运维系统，提供包括部署、升级、扩缩容、备份恢复、配置变更的 TiDB 全生命周期管理。借助 TiDB Operator，TiDB 可以无缝运行在公有云或自托管的 Kubernetes 集群上。\n\nTiDB Operator 的文档目前独立于 TiDB 文档。要查看如何在 Kubernetes 上部署 TiDB 集群的详细步骤，请了解 [TiDB Operator 与 TiDB 版本的对应关系](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/tidb-operator-overview)，参阅相应版本的 [TiDB on Kubernetes 用户文档](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/)。\n"
        },
        {
          "name": "tidb-lightning",
          "type": "tree",
          "content": null
        },
        {
          "name": "tidb-limitations.md",
          "type": "blob",
          "size": 2.9990234375,
          "content": "---\ntitle: TiDB 使用限制\naliases: ['/docs-cn/dev/tidb-limitations/']\nsummary: TiDB 中的使用限制包括标识符长度限制、数据库、表、视图、连接总个数限制、单个数据库和表的限制、单行限制、数据类型限制、SQL 语句限制和 TiKV 版本限制。\n---\n\n# 使用限制\n\n本文会将详细描述 TiDB 中常见的使用限制，包括：标识符长度，最大支持的数据库、表、索引、分区表、序列等的个数。\n\n> **注意：**\n>\n> TiDB 高度兼容 MySQL 协议，也兼容了很多 MySQL 本身的限制，比如单个索引最多可包含 16 列。详细请参考[与 MySQL 兼容性对比](/mysql-compatibility.md) 和 MySQL 官方文档。\n\n## 标识符长度限制\n\n| 标识符类型 | 最大长度（字符）|\n|:---------|:--------------|\n| Database | 64 |\n| Table    | 64 |\n| Column   | 64 |\n| Index    | 64 |\n| View     | 64 |\n| Sequence | 64 |\n\n## Databases、Tables、Views、Connections 总个数限制\n\n| 类型  | 最大个数   |\n|:----------|:----------|\n| Databases | unlimited |\n| Tables    | unlimited |\n| Views     | unlimited |\n| Connections| unlimited|\n\n## 单个 Database 的限制\n\n| 类型       | 最大限制   |\n|:----------|:----------|\n| Tables    |unlimited  |\n\n## 单个 Table 的限制\n\n| 类型       | 最大限制（默认值）              |\n|:----------|:------------------------------|\n| Columns   | 默认为 1017，最大可调至 4096     |\n| Indexes   | 默认为 64，最大可调至 512        |\n| Rows      | 无限制                         |\n| Size      | 无限制                         |\n| Partitions| 8192                          |\n\n* Columns 的最大限制可通过 [`table-column-count-limit`](/tidb-configuration-file.md#table-column-count-limit-从-v50-版本开始引入) 修改。\n* Indexes 的最大限制可通过 [`index-limit`](/tidb-configuration-file.md#index-limit-从-v50-版本开始引入) 修改。\n\n## 单行的限制\n\n| 类型       | 最大限制（默认值）   |\n|:----------|:----------|\n| Size       | 默认为 6 MiB，可通过 [`txn-entry-size-limit`](/tidb-configuration-file.md#txn-entry-size-limit-从-v4010-和-v500-版本开始引入) 配置项调至 120 MiB |\n\n## 数据类型限制\n\n| 类型       | 最大限制   |\n|:----------|:----------|\n| CHAR       | 255 字符      |\n| BINARY     | 255 字节      |\n| VARBINARY  | 65535 字节    |\n| VARCHAR    | 16383 字符    |\n| TEXT       | 默认为 6291456 字节（即 6 MiB），可调至 125829120 字节（即 120 MiB）      |\n| BLOB       | 默认为 6291456 字节（即 6 MiB），可调至 125829120 字节（即 120 MiB）      |\n\n## SQL Statements 的限制\n\n| 类型       | 最大限制   |\n|:----------|:----------|\n| 单个事务最大语句数 |  在使用乐观事务并开启事务重试的情况下，默认限制 5000，可通过 [`stmt-count-limit`](/tidb-configuration-file.md#stmt-count-limit) 调整 |\n\n## TiKV 版本的限制\n\n在集群中，如果 TiDB 组件的版本为 v6.2.0 及以上，则 TiKV 组件版本不得低于 v6.2.0。\n"
        },
        {
          "name": "tidb-monitoring-api.md",
          "type": "blob",
          "size": 4.2841796875,
          "content": "---\ntitle: TiDB 集群监控 API\naliases: ['/docs-cn/dev/tidb-monitoring-api/']\nsummary: TiDB 提供状态接口和 Metrics 接口来监控集群状态。状态接口可获取 TiDB Server 的运行状态和存储信息，PD 的状态接口可查看整个 TiKV 集群的详细信息。Metrics 接口用于监控整个集群的状态和性能。部署 Prometheus 和 Grafana 后，配置 Grafana 即可使用 Metrics 接口。\n---\n\n# TiDB 集群监控 API\n\nTiDB 提供了以下几种接口来监控集群状态：\n\n- [状态接口](#使用状态接口)：通过 HTTP 接口对外汇报组件的信息。通过状态接口，你可获取当前 TiDB Server 的[运行状态](#运行状态)以及表的[存储信息](#存储信息)。\n- [Metrics 接口](#使用-metrics-接口)：使用 Prometheus 记录组件中各种操作的详细信息，使用 Grafana 进行可视化展示。\n\n## 使用状态接口\n\n状态接口用于监控组件的一些基本信息，并且可以作为 keepalive 的监测接口。另外，通过 PD 的状态接口可以看到整个 TiKV 集群的详细信息。\n\n### TiDB Server\n\n- TiDB API 地址：`http://${host}:${port}`\n- 默认端口：10080\n\n#### 运行状态\n\n以下示例中，通过访问 `http://${host}:${port}/status` 获取当前 TiDB Server 的状态，并判断该 TiDB Server 是否存活。结果以 **JSON** 格式返回：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ncurl http://127.0.0.1:10080/status\n```\n\n```\n{\n    connections: 0,  # 当前 TiDB Server 上的客户端连接数\n    version: \"8.0.11-TiDB-v8.5.0\",  # TiDB 版本号\n    git_hash: \"7267747ae0ec624dffc3fdedb00f1ed36e10284b\"  # TiDB 当前代码的 Git Hash\n}\n```\n\n#### 存储信息\n\n以下示例中，通过访问 `http://${host}:${port}/schema_storage/${db}/${table}` 获取指定数据表的存储信息。结果以 **JSON** 格式返回：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ncurl http://127.0.0.1:10080/schema_storage/mysql/stats_histograms\n```\n\n```\n{\n    \"table_schema\": \"mysql\",\n    \"table_name\": \"stats_histograms\",\n    \"table_rows\": 0,\n    \"avg_row_length\": 0,\n    \"data_length\": 0,\n    \"max_data_length\": 0,\n    \"index_length\": 0,\n    \"data_free\": 0\n}\n```\n\n```bash\ncurl http://127.0.0.1:10080/schema_storage/test\n```\n\n```\n[\n    {\n        \"table_schema\": \"test\",\n        \"table_name\": \"test\",\n        \"table_rows\": 0,\n        \"avg_row_length\": 0,\n        \"data_length\": 0,\n        \"max_data_length\": 0,\n        \"index_length\": 0,\n        \"data_free\": 0\n    }\n]\n```\n\n### PD Server\n\n- PD API 地址：`http://${host}:${port}/pd/api/v1/${api_name}`\n- 默认端口：2379\n- 各类 `api_name` 详细信息：参见 [PD API Doc](https://download.pingcap.com/pd-api-doc.html)\n\n通过该接口可以获取当前所有 TiKV 节点的状态以及负载均衡信息。下面以一个单节点的 TiKV 集群为例，说明用户需要了解的信息：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ncurl http://127.0.0.1:2379/pd/api/v1/stores\n```\n\n```\n{\n  \"count\": 1,  # TiKV 节点数量\n  \"stores\": [  # TiKV 节点的列表\n    # 集群中单个 TiKV 节点的信息\n    {\n      \"store\": {\n        \"id\": 1,\n        \"address\": \"127.0.0.1:20160\",\n        \"version\": \"4.0.0-rc.2\",\n        \"status_address\": \"172.16.5.90:20382\",\n        \"git_hash\": \"2fdb2804bf8ffaab4b18c4996970e19906296497\",\n        \"start_timestamp\": 1590029618,\n        \"deploy_path\": \"/data2/tidb_test/v4.0.rc.2/tikv-20372/bin\",\n        \"last_heartbeat\": 1590030038949235439,\n        \"state_name\": \"Up\"\n      },\n      \"status\": {\n        \"capacity\": \"3.581TiB\",  # 存储总容量\n        \"available\": \"3.552TiB\",  # 存储剩余容量\n        \"used_size\": \"31.77MiB\",\n        \"leader_count\": 174,\n        \"leader_weight\": 1,\n        \"leader_score\": 174,\n        \"leader_size\": 174,\n        \"region_count\": 531,\n        \"region_weight\": 1,\n        \"region_score\": 531,\n        \"region_size\": 531,\n        \"start_ts\": \"2020-05-21T10:53:38+08:00\",\n        \"last_heartbeat_ts\": \"2020-05-21T11:00:38.949235439+08:00\",\n        \"uptime\": \"7m0.949235439s\"\n      }\n    }\n  ]\n```\n\n## 使用 metrics 接口\n\nMetrics 接口用于监控整个集群的状态和性能。\n\n- 如果使用其他方式部署 TiDB 集群，在使用 metrics 接口前，需先[部署 Prometheus 和 Grafana](/deploy-monitoring-services.md)。\n\n成功部署 Prometheus 和 Grafana 之后，[配置 Grafana](/deploy-monitoring-services.md)。\n"
        },
        {
          "name": "tidb-monitoring-framework.md",
          "type": "blob",
          "size": 3.8505859375,
          "content": "---\ntitle: TiDB 监控框架概述\naliases: ['/docs-cn/dev/tidb-monitoring-framework/','/docs-cn/dev/how-to/monitor/overview/']\nsummary: TiDB 使用 Prometheus 作为监控和性能指标存储，Grafana 用于可视化展示。Prometheus 提供多个组件，包括 Prometheus Server、Client 代码库和 Alertmanager。Grafana 展示 TiDB 集群各组件的相关监控，分组包括备份恢复、Binlog、网络探活、磁盘性能、Kafka、TiDB Lightning 等。每个分组包含多个监控项页签，以及详细的监控指标看板。观看培训视频可快速了解监控与报警系统的体系、数据流转方式、系统管理方法和常用监控指标。\n---\n\n# TiDB 监控框架概述\n\nTiDB 使用开源时序数据库 [Prometheus](https://prometheus.io) 作为监控和性能指标信息存储方案，使用 [Grafana](https://grafana.com/grafana) 作为可视化组件进行展示。\n\n## Prometheus 在 TiDB 中的应用\n\nPrometheus 是一个拥有多维度数据模型的、灵活的查询语句的时序数据库。Prometheus 作为热门的开源项目，拥有活跃的社区及众多的成功案例。\n\nPrometheus 提供了多个组件供用户使用。目前，TiDB 使用了以下组件：\n\n- Prometheus Server：用于收集和存储时间序列数据。\n- Client 代码库：用于定制程序中需要的 Metric。\n- Alertmanager：用于实现报警机制。\n\n其结构如下图所示：\n\n![Prometheus in TiDB](/media/prometheus-in-tidb.png)\n\n## Grafana 在 TiDB 中的应用\n\nGrafana 是一个开源的 metric 分析及可视化系统。TiDB 使用 Grafana 来展示 TiDB 集群各组件的相关监控，监控项分组如下图所示：\n\n![Grafana monitored_groups](/media/grafana_monitored_groups.png)\n\n- {TiDB_Cluster_name}-Backup-Restore：备份恢复相关的监控项。\n- {TiDB_Cluster_name}-Blackbox_exporter：网络探活相关监控项。\n- {TiDB_Cluster_name}-Disk-Performance：磁盘性能相关监控项。\n- {TiDB_Cluster_name}-Kafka-Overview：Kafka 相关监控项。\n- {TiDB_Cluster_name}-Lightning：TiDB Lightning 组件相关监控项。\n- {TiDB_Cluster_name}-Node_exporter：操作系统相关监控项。\n- {TiDB_Cluster_name}-Overview：重要组件监控概览。\n- {TiDB_Cluster_name}-PD：PD server 组件相关监控项。\n- {TiDB_Cluster_name}-Performance-Read：读性能相关监控项。\n- {TiDB_Cluster_name}-Performance-Write：写性能相关监控项。\n- {TiDB_Cluster_name}-TiDB：TiDB server 组件详细监控项。\n- {TiDB_Cluster_name}-TiDB-Summary：TiDB server 相关监控项概览。\n- {TiDB_Cluster_name}-TiFlash-Proxy-Summary：数据同步到 TiFlash 的代理 server 监控项概览。\n- {TiDB_Cluster_name}-TiFlash-Summary：TiFlash server 相关监控项概览。\n- {TiDB_Cluster_name}-TiKV-Details：TiKV server 组件详细监控项。\n- {TiDB_Cluster_name}-TiKV-Summary：TiKV server 监控项概览。\n- {TiDB_Cluster_name}-TiKV-Trouble-Shooting：TiKV 错误诊断相关监控项。\n- {TiDB_Cluster_name}-TiCDC：TiCDC 组件详细监控项。\n- {TiDB_Cluster_name}-TiProxy-Summary：TiProxy 监控项概览。\n\n每个分组包含多个监控项页签，页签中包含多个详细的监控项信息。以 Overview 监控组为例，其中包含 5 个页签，每个页签内有相应的监控指标看板，如下图所示：\n\n![Grafana Overview](/media/grafana_monitor_overview.png)\n\n要快速了解 TiDB 监控与报警系统的体系、该系统背后的数据流转方式、系统管理方法、系统使用方法和常用监控指标，建议观看下面的培训视频（时长 29 分钟）。注意本视频只作为学习参考，具体的[监控指标与相关报警规则](/alert-rules.md#tidb-报警规则)，请以文档内容为准。\n\n<video src=\"https://download.pingcap.com/docs-cn%2FLesson13_monitor.mp4\" width=\"100%\" height=\"100%\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_lesson13.png\"></video>\n"
        },
        {
          "name": "tidb-operator-overview.md",
          "type": "blob",
          "size": 0.69140625,
          "content": "---\ntitle: TiDB Operator\nsummary: 了解 Kubernetes 上的 TiDB 集群自动部署运维工具 TiDB Operator。\n---\n\n# TiDB Operator\n\n[TiDB Operator](https://github.com/pingcap/tidb-operator) 是 Kubernetes 上的 TiDB 集群自动运维系统，提供包括部署、升级、扩缩容、备份恢复、配置变更的 TiDB 全生命周期管理。借助 TiDB Operator，TiDB 可以无缝运行在公有云或自托管的 Kubernetes 集群上。\n\nTiDB Operator 的文档目前独立于 TiDB 文档，文档名称为 **TiDB on Kubernetes 用户文档**。要访问 TiDB Operator 的文档，请点击以下链接：\n\n- [TiDB on Kubernetes 用户文档](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/)\n"
        },
        {
          "name": "tidb-read-staleness.md",
          "type": "blob",
          "size": 4.4130859375,
          "content": "---\ntitle: 通过系统变量 `tidb_read_staleness` 读取历史数据\nsummary: 了解如何通过系统变量 `tidb_read_staleness` 读取历史数据。\n---\n\n# 通过系统变量 `tidb_read_staleness` 读取历史数据\n\n为支持读取历史版本数据，TiDB 从 5.4 版本起引入了一个新的系统变量 `tidb_read_staleness`。本文档介绍如何通过该系统变量读取历史数据，其中包括具体的操作流程。\n\n## 功能介绍\n\n系统变量 `tidb_read_staleness` 用于设置当前会话允许读取的历史数据范围，其数据类型为 int，作用域为 `SESSION`。设置该变量后，TiDB 会从参数允许的范围内选出一个尽可能新的时间戳，并影响后继的所有读操作。比如，如果该变量的值设置为 `-5`，TiDB 会在 5 秒时间范围内，保证 TiKV 拥有对应历史版本数据的情况下，选择尽可能新的一个时间戳。\n\n开启 `tidb_read_staleness` 后，你仍可以进行以下操作：\n\n- 在当前会话中插入、修改、删除数据或进行 DML 操作。这些语句不会受到 `tidb_read_staleness` 的影响。\n- 在当前会话开启交互式事务。在该事务内的查询依旧是读取最新版本的数据。\n\n完成对历史版本数据的读取后，你可以通过以下两种方式来读取最新版本的数据。\n\n- 结束当前会话。\n- 使用 `SET` 语句，把 `tidb_read_staleness` 变量的值设为 `\"\"`。\n\n> **注意：**\n>\n> 你可以通过调整 TiKV 的 `advance-ts-interval` 配置项提高 Stale Read 数据的时效性（即减少延时），详情参见[减少 Stale Read 延时](/stale-read.md#减少-stale-read-延时)。\n\n## 示例\n\n本节通过具体操作示例介绍系统变量 `tidb_read_staleness`的使用方法。\n\n1. 初始化阶段。创建一个表后，在表中插入几行数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    create table t (c int);\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.01 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    insert into t values (1), (2), (3);\n    ```\n\n    ```\n    Query OK, 3 rows affected (0.00 sec)\n    ```\n\n2. 查看表中的数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |    2 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n3. 更新某一行数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    update t set c=22 where c=2;\n    ```\n\n    ```\n    Query OK, 1 row affected (0.00 sec)\n    ```\n\n4. 确认数据已经被更新：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |   22 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n5. 设置一个特殊的环境变量 `tidb_read_staleness`。\n\n    该变量的作用域为 `SESSION`，设置变量值后，TiDB 会读取变量值时间之前的最新一个版本的数据。\n\n    以下设置表示 TiDB 会从 5 秒前至现在的时间范围内选择一个尽可能新的时间戳，将其用作为历史数据读取的时间戳：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    set @@tidb_read_staleness=\"-5\";\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.00 sec)\n    ```\n\n    > **注意：**\n    >\n    > 必须在 `tidb_read_staleness` 前使用 `@@`，而非 `@`。因为 `@@` 表示系统变量，`@` 则表示用户变量。\n    > 你需要根据第 3 步到第 4 步所花费的时间，来设定你要读取的历史时间范围，即 `tidb_read_staleness` 的值。否则，查询结果中显示的会是最新数据，而非历史数据。因此，请根据自己的实际操作情况调整该时间范围。比如，在本示例中，由于设定的时间范围是 5 秒，你需要在 5 秒内完成第 3 步和第 4 步。\n\n    这里读取到的内容即为更新前的数据，也就是历史版本的数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |    2 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n\n6. 清空这个变量后，即可读取最新版本数据：\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    set @@tidb_read_staleness=\"\";\n    ```\n\n    ```\n    Query OK, 0 rows affected (0.00 sec)\n    ```\n\n    {{< copyable \"sql\" >}}\n\n    ```sql\n    select * from t;\n    ```\n\n    ```\n    +------+\n    | c    |\n    +------+\n    |    1 |\n    |   22 |\n    |    3 |\n    +------+\n    3 rows in set (0.00 sec)\n    ```\n"
        },
        {
          "name": "tidb-resource-control.md",
          "type": "blob",
          "size": 35.79296875,
          "content": "---\ntitle: 使用资源管控 (Resource Control) 实现资源隔离\nsummary: 介绍如何通过资源管控能力来实现对应用资源消耗的控制和有效调度。\n---\n\n# 使用资源管控 (Resource Control) 实现资源隔离\n\n使用资源管控特性，集群管理员可以定义资源组 (Resource Group)，通过资源组限定配额。\n\nTiDB 资源管控特性提供了两层资源管理能力，包括在 TiDB 层的流控能力和 TiKV 层的优先级调度的能力。两个能力可以单独或者同时开启，详情请参见[参数组合效果表](#相关参数)。将用户绑定到某个资源组后，TiDB 层会根据用户所绑定资源组设定的配额对用户的读写请求做流控，TiKV 层会根据配额映射的优先级来对请求做调度。通过流控和调度这两层控制，可以实现应用的资源隔离，满足服务质量 (QoS) 要求。\n\n- TiDB 流控：TiDB 流控使用[令牌桶算法](https://en.wikipedia.org/wiki/Token_bucket) 做流控。如果桶内令牌数不够，而且资源组没有指定 `BURSTABLE` 特性，属于该资源组的请求会等待令牌桶回填令牌并重试，重试可能会超时失败。\n- TiKV 调度：你可以为资源组设置绝对优先级 ([`PRIORITY`](/information-schema/information-schema-resource-groups.md#示例))，不同的资源按照 `PRIORITY` 的设置进行调度，`PRIORITY` 高的任务会被优先调度。如果没有设置绝对优先级 (`PRIORITY`)，TiKV 会将资源组的 `RU_PER_SEC` 取值映射成各自资源组读写请求的优先级，并基于各自的优先级在存储层使用优先级队列调度处理请求。\n\n从 v7.4.0 开始，TiDB 资源管控特性支持管控 TiFlash 资源，其原理与 TiDB 流控和 TiKV 调度类似：\n\n- TiFlash 流控：借助 [TiFlash Pipeline Model 执行模型](/tiflash/tiflash-pipeline-model.md)，可以更精确地获取不同查询的 CPU 消耗情况，并转换为 [Request Unit (RU)](#什么是-request-unit-ru) 进行扣除。流量控制通过令牌桶算法实现。\n- TiFlash 调度：当系统资源不足时，会根据优先级对多个资源组之间的 pipeline task 进行调度。具体逻辑是：首先判断资源组的优先级 `PRIORITY`，然后根据 CPU 使用情况，再结合 `RU_PER_SEC` 进行判断。最终效果是，如果 rg1 和 rg2 的 `PRIORITY` 一样，但是 rg2 的 `RU_PER_SEC` 是 rg1 的两倍，那么 rg2 可使用的 CPU 时间是 rg1 的两倍。\n\n## 使用场景\n\n资源管控特性的引入对 TiDB 具有里程碑的意义。它能够将一个分布式数据库集群划分成多个逻辑单元，即使个别单元对资源过度使用，也不会挤占其他单元所需的资源。利用该特性：\n\n- 你可以将多个来自不同系统的中小型应用合入一个 TiDB 集群中，个别应用的负载升高，不会影响其他业务的正常运行。而在系统负载较低的时候，繁忙的应用即使超过设定的读写配额，也仍然可以被分配到所需的系统资源，达到资源的最大化利用。\n- 你可以选择将所有测试环境合入一个集群，或者将消耗较大的批量任务编入一个单独的资源组，在保证重要应用获得必要资源的同时，提升硬件利用率，降低运行成本。\n- 当系统中存在多种业务负载时，可以将不同的负载分别放入各自的资源组。利用资源管控技术，确保交易类业务的响应时间不受数据分析或批量业务的影响。\n- 当集群遇到突发的 SQL 性能问题，可以结合 SQL Binding 和资源组，临时限制某个 SQL 的资源消耗。\n\n此外，合理利用资源管控特性可以减少集群数量，降低运维难度及管理成本。\n\n> **注意：**\n>\n> 推荐在部署资源相对独立的计算和存储节点上测试资源管控的效果，因为调度等对集群资源敏感的功能通常很难在单节点运行的 TiUP Playground 上表现出良好性能。\n\n## 使用限制\n\n资源管控将带来额外的调度开销。因此，开启该特性后，性能可能会有轻微下降（低于 5%）。\n\n## 什么是 Request Unit (RU)\n\nRequest Unit (RU) 是 TiDB 对 CPU、IO 等系统资源的统一抽象的计量单位，用于表示对数据库的单个请求消耗的资源量。请求消耗的 RU 数量取决于多种因素，例如操作类型或正在检索或修改的数据量。目前，RU 包含以下资源的统计信息：\n\n<table>\n    <thead>\n        <tr>\n            <th>资源类型</th>\n            <th>RU 消耗</th>\n        </tr>\n    </thead>\n    <tbody>\n        <tr>\n            <td rowspan=\"3\">Read</td>\n            <td>2 storage read batches 消耗 1 RU</td>\n        </tr>\n        <tr>\n            <td>8 storage read requests 消耗 1 RU</td>\n        </tr>\n        <tr>\n            <td>64 KiB read request payload 消耗 1 RU</td>\n        </tr>\n        <tr>\n            <td rowspan=\"3\">Write</td>\n            <td>1 storage write batch 消耗 1 RU</td>\n        </tr>\n        <tr>\n            <td>1 storage write request 消耗 1 RU</td>\n        </tr>\n        <tr>\n            <td>1 KiB write request payload 消耗 1 RU</td>\n        </tr>\n        <tr>\n            <td>CPU</td>\n            <td> 3 ms 消耗 1 RU</td>\n        </tr>\n    </tbody>\n</table>\n\n> **注意：**\n>\n> - 每个写操作最终都被会复制到所有副本（TiKV 默认 3 个数据副本），并且每次复制都被认为是一个不同的写操作。\n> - 上表只列举了本地部署的 TiDB 计算 RU 时涉及的相关资源，其中不包括网络和存储部分。TiDB Cloud Serverless 的 RU 可参考 [TiDB Cloud Serverless Pricing Details](https://www.pingcap.com/tidb-cloud-serverless-pricing-details/)。\n> - 目前 TiFlash 资源管控仅考虑 SQL CPU（即查询的 pipeline task 运行所占用的 CPU 时间）以及 read request payload。\n\n## 相关参数\n\n资源管控特性引入了如下系统变量或参数：\n\n- TiDB：通过配置全局变量 [`tidb_enable_resource_control`](/system-variables.md#tidb_enable_resource_control-从-v660-版本开始引入) 控制是否打开资源组流控。\n- TiKV：通过配置参数 [`resource-control.enabled`](/tikv-configuration-file.md#resource-control) 控制是否使用基于资源组配额的请求调度。\n- TiFlash：通过配置全局变量 [`tidb_enable_resource_control`](/system-variables.md#tidb_enable_resource_control-从-v660-版本开始引入) 和 TiFlash 配置项 [`enable_resource_control`](/tiflash/tiflash-configuration.md#配置文件-tiflashtoml)（v7.4.0 开始引入）控制是否开启 TiFlash 资源管控。\n\n从 v7.0.0 开始，`tidb_enable_resource_control` 和 `resource-control.enabled` 开关都被默认打开。这两个参数的组合效果见下表：\n\n| `resource-control.enabled`  | `tidb_enable_resource_control`= ON | `tidb_enable_resource_control`= OFF  |\n|:----------------------------|:-----------------------------------|:------------------------------------|\n| `resource-control.enabled`= true  | 流控和调度（推荐组合）                        | 无效配置                         |\n| `resource-control.enabled`= false | 仅流控（不推荐）                           |  特性被关闭                   |\n\n从 v7.4.0 开始，TiFlash 配置项 `enable_resource_control` 默认打开，与 `tidb_enable_resource_control` 一起控制 TiFlash 资源管控功能。只有二者都启用时，TiFlash 资源管控功能才能进行流控以及优先级调度。同时，在开启 `enable_resource_control` 时，TiFlash 会使用 [Pipeline Model 执行模型](/tiflash/tiflash-pipeline-model.md)。\n\n关于资源管控实现机制及相关参数的详细介绍，请参考 [RFC: Global Resource Control in TiDB](https://github.com/pingcap/tidb/blob/master/docs/design/2022-11-25-global-resource-control.md) 以及 [TiFlash Resource Control](https://github.com/pingcap/tiflash/blob/master/docs/design/2023-09-21-tiflash-resource-control.md)。\n\n## 使用方法\n\n下面介绍如何使用资源管控特性。\n\n### 预估集群容量\n\n在进行资源规划之前，你需要了解集群的整体容量。TiDB 提供了命令 [`CALIBRATE RESOURCE`](/sql-statements/sql-statement-calibrate-resource.md) 用来估算集群容量。目前提供两种估算方式：\n\n- [根据实际负载估算容量](/sql-statements/sql-statement-calibrate-resource.md#根据实际负载估算容量)\n- [基于硬件部署估算容量](/sql-statements/sql-statement-calibrate-resource.md#基于硬件部署估算容量)\n\n可通过 [TiDB Dashboard 资源管控页面](/dashboard/dashboard-resource-manager.md)进行查看。详情请参考 [`CALIBRATE RESOURCE` 预估方式](/sql-statements/sql-statement-calibrate-resource.md#预估方式)。\n\n### 管理资源组\n\n创建、修改、删除资源组，需要拥有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 权限。\n\n你可以通过 [`CREATE RESOURCE GROUP`](/sql-statements/sql-statement-create-resource-group.md) 在集群中创建资源组。\n\n对于已有的资源组，可以通过 [`ALTER RESOURCE GROUP`](/sql-statements/sql-statement-alter-resource-group.md) 修改资源组的配额，对资源组的配额修改会立即生效。\n\n可以使用 [`DROP RESOURCE GROUP`](/sql-statements/sql-statement-drop-resource-group.md) 删除资源组。\n\n### 创建资源组\n\n下面举例说明如何创建资源组。\n\n1. 创建 `rg1` 资源组，限额是每秒 500 RU，并且允许这个资源组的应用超额占用资源。\n\n    ```sql\n    CREATE RESOURCE GROUP IF NOT EXISTS rg1 RU_PER_SEC = 500 BURSTABLE;\n    ```\n\n2. 创建 `rg2` 资源组，RU 的回填速度是每秒 600 RU。在系统资源充足的时候，不允许这个资源组的应用超额占用资源。\n\n    ```sql\n    CREATE RESOURCE GROUP IF NOT EXISTS rg2 RU_PER_SEC = 600;\n    ```\n\n3. 创建 `rg3` 资源组，设置绝对优先级为 `HIGH`。绝对优先级目前支持 `LOW|MEDIUM|HIGH`，资源组的默认绝对优先级为 `MEDIUM`。\n\n    ```sql\n    CREATE RESOURCE GROUP IF NOT EXISTS rg3 RU_PER_SEC = 100 PRIORITY = HIGH;\n    ```\n\n### 绑定资源组\n\nTiDB 支持如下三个级别的资源组设置：\n\n- 用户级别。通过 [`CREATE USER`](/sql-statements/sql-statement-create-user.md) 或 [`ALTER USER`](/sql-statements/sql-statement-alter-user.md#修改用户绑定的资源组) 语句将用户绑定到特定的资源组。绑定后，对应的用户新创建的会话会自动绑定对应的资源组。\n- 会话级别。通过 [`SET RESOURCE GROUP`](/sql-statements/sql-statement-set-resource-group.md) 设置当前会话使用的资源组。\n- 语句级别。通过 [`RESOURCE_GROUP()`](/optimizer-hints.md#resource_groupresource_group_name) Optimizer Hint 设置当前语句使用的资源组。\n\n#### 将用户绑定到资源组\n\n下面的示例创建一个用户 `usr1` 并将其绑定到资源组 `rg1`。其中 `rg1` 为[创建资源组](#创建资源组)示例中创建的资源组。\n\n```sql\nCREATE USER 'usr1'@'%' IDENTIFIED BY '123' RESOURCE GROUP rg1;\n```\n\n下面示例使用 `ALTER USER` 将用户 `usr2` 绑定到资源组 `rg2`。其中 `rg2` 为[创建资源组](#创建资源组)示例中创建的资源组。\n\n```sql\nALTER USER usr2 RESOURCE GROUP rg2;\n```\n\n绑定用户后，用户新建立的会话对资源的占用会受到指定用量 (RU) 的限制。如果系统负载比较高，没有多余的容量，用户 `usr2` 的资源消耗速度会被严格控制不超过指定用量。由于 `usr1` 绑定的 `rg1` 配置了 `BURSTABLE`，所以 `usr1` 消耗速度允许超过指定用量。\n\n如果资源组对应的请求太多导致资源组的资源不足，客户端的请求处理会发生等待。如果等待时间过长，请求会报错。\n\n> **注意：**\n>\n> - 使用 `CREATE USER` 或者 `ALTER USER` 将用户绑定到资源组后，只会对该用户新建的会话生效，不会对该用户已有的会话生效。\n> - TiDB 集群在初始化时会自动创建 `default` 资源组，其 `RU_PER_SEC` 的默认值为 `UNLIMITED` (等同于 `INT` 类型最大值，即 `2147483647`)，且为 `BURSTABLE` 模式。对于没有绑定资源组的语句会自动绑定至此资源组。此资源组不支持删除，但允许修改其 RU 的配置。\n\n要解除用户与资源组的绑定，只需将其重新绑定到 `default` 资源组即可，如下所示：\n\n```sql\nALTER USER 'usr3'@'%' RESOURCE GROUP `default`;\n```\n\n更多信息，请参见 [`ALTER USER ... RESOURCE GROUP`](/sql-statements/sql-statement-alter-user.md#修改用户绑定的资源组)。\n\n#### 将当前会话绑定到资源组\n\n使用 [`SET RESOURCE GROUP`](/sql-statements/sql-statement-set-resource-group.md) 语句，可以修改当前会话绑定的资源组。通过把当前会话绑定到资源组，会话对资源的占用会受到指定配额 (RU) 的限制。\n\n当系统变量 [`tidb_resource_control_strict_mode`](/system-variables.md#tidb_resource_control_strict_mode-从-v820-版本开始引入) 设置为 `ON` 时，你需要有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 或者 `RESOURCE_GROUP_USER` 权限才能执行该语句。\n\n下面的示例将当前的会话绑定至资源组 `rg1`。\n\n```sql\nSET RESOURCE GROUP rg1;\n```\n\n#### 将语句绑定到资源组\n\n通过在 SQL 语句中添加 [`RESOURCE_GROUP(resource_group_name)`](/optimizer-hints.md#resource_groupresource_group_name) Hint，可以将该语句绑定到指定的资源组。此 Hint 支持 `SELECT`、`INSERT`、`UPDATE`、`DELETE` 四种语句。\n\n当系统变量 [`tidb_resource_control_strict_mode`](/system-variables.md#tidb_resource_control_strict_mode-从-v820-版本开始引入) 设置为 `ON` 时，你需要有 `SUPER` 或者 `RESOURCE_GROUP_ADMIN` 或者 `RESOURCE_GROUP_USER` 权限才能使用此 Hint。\n\n示例：\n\n```sql\nSELECT /*+ RESOURCE_GROUP(rg1) */ * FROM t limit 10;\n```\n\n### 管理资源消耗超出预期的查询 (Runaway Queries)\n\nRunaway Query 是指执行时间或消耗资源超出预期的查询（仅指 `SELECT` 语句）。下面使用 **Runaway Queries** 表示管理 Runaway Query 这一功能。\n\n- 自 v7.2.0 起，TiDB 资源管控引入了对 Runaway Queries 的管理。你可以针对某个资源组设置条件来识别 Runaway Queries，并自动发起应对操作，防止集群资源完全被 Runaway Queries 占用而影响其他正常查询。你可以在 [`CREATE RESOURCE GROUP`](/sql-statements/sql-statement-create-resource-group.md) 或者 [`ALTER RESOURCE GROUP`](/sql-statements/sql-statement-alter-resource-group.md) 中配置 `QUERY_LIMIT` 字段，通过规则识别来管理资源组的 Runaway Queries。\n- 自 v7.3.0 起，TiDB 资源管控引入了手动管理 Runaway Queries 监控列表的功能，将给定的 SQL 或者 Digest 添加到隔离监控列表，从而实现快速隔离 Runaway Queries。你可以执行语句 [`QUERY WATCH`](/sql-statements/sql-statement-query-watch.md)，手动管理资源组中的 Runaway Queries 监控列表。\n\n#### `QUERY_LIMIT` 参数说明\n\n如果查询超过以下任一限制，就会被识别为 Runaway Query：\n\n- `EXEC_ELAPSED`：检测查询执行的时间是否超限\n- `PROCESSED_KEYS`：检测 Coprocessor 处理的 key 的数量是否超限\n- `RU`：检测执行语句消耗的总读写 RU 是否超限\n\n支持的应对操作 (`ACTION`)：\n\n- `DRYRUN`：对执行 Query 不做任何操作，仅记录识别的 Runaway Query。主要用于观测设置条件是否合理。\n- `COOLDOWN`：将查询的执行优先级降到最低，查询仍旧会以低优先级继续执行，不占用其他操作的资源。\n- `KILL`：识别到的查询将被自动终止，报错 `Query execution was interrupted, identified as runaway query`。\n- `SWITCH_GROUP`：从 v8.4.0 开始引入，将识别到的查询切换到指定的资源组继续执行。该查询执行结束后，后续 SQL 仍保持在原资源组中执行。如果指定的资源组不存在，则不做任何动作。\n\n为了避免并发的 Runaway Query 过多导致系统资源耗尽，资源管控引入了 Runaway Query 监控机制，能够快速识别并隔离 Runaway Query。该功能通过 `WATCH` 子句实现，当某一个查询被识别为 Runaway Query 之后，会提取这个查询的匹配特征（由 `WATCH` 后的匹配方式参数决定），在接下来的一段时间里（由 `DURATION` 定义），这个 Runaway Query 的匹配特征会被加入到监控列表，TiDB 实例会将查询和监控列表进行匹配，匹配到的查询直接标记为 Runaway Query，而不再等待其被条件识别，并按照当前应对操作进行隔离。其中 `KILL` 会终止该查询，并报错 `Quarantined and interrupted because of being in runaway watch list`。\n\n`WATCH` 有三种匹配方式：\n\n- `EXACT` 表示完全相同的 SQL 才会被快速识别\n- `SIMILAR` 表示会忽略字面值 (Literal)，通过 SQL Digest 匹配所有模式 (Pattern) 相同的 SQL\n- `PLAN` 表示通过 Plan Digest 匹配所有模式 (Pattern) 相同的 SQL\n\n`WATCH` 中的 `DURATION` 选项，用于表示此识别项的持续时间，默认为无限长。\n\n添加监控项后，匹配特征和 `ACTION` 都不会随着 `QUERY_LIMIT` 配置的修改或删除而改变或删除。可以使用 `QUERY WATCH REMOVE` 来删除监控项。\n\n`QUERY_LIMIT` 具体格式如下：\n\n| 参数            | 含义           | 备注                                   |\n|---------------|--------------|--------------------------------------|\n| `EXEC_ELAPSED`  | 当查询执行时间超过该值时，会被识别为 Runaway Query | `EXEC_ELAPSED = 60s` 表示查询的执行时间超过 60 秒则被认为是 Runaway Query。 |\n| `PROCESSED_KEYS` | 当 Coprocessor 处理的 key 的数量超过该值时，查询会被识别为 Runaway Query | `PROCESSED_KEYS = 1000` 表示 Coprocessor 处理的 key 的数量超过 1000 则被认为是 Runaway Query。 |\n| `RU`  | 当查询消耗的总读写 RU 超过该值时，查询会被识别为 Runaway Query | `RU = 1000` 表示查询消耗的总读写 RU 超过 1000 则被认为是 Runaway Query。 |\n| `ACTION`    | 当识别到 Runaway Query 时进行的动作 | 可选值有 `DRYRUN`，`COOLDOWN`，`KILL`，`SWITCH_GROUP`。 |\n| `WATCH`   | 快速匹配已经识别到的 Runaway Query，即在一定时间内再碰到相同或相似查询直接进行相应动作 | 可选项，配置例如 `WATCH=SIMILAR DURATION '60s'`、`WATCH=EXACT DURATION '1m'`、`WATCH=PLAN`。 |\n\n> **注意：**\n>\n> 如果你想把 Runaway Queries 严格限制在一个资源组内，推荐将 `SWITCH_GROUP` 和 [`QUERY WATCH`](/tidb-resource-control.md#query-watch-语句说明) 语句一起搭配使用。因为 `QUERY_LIMIT` 只有在查询达到预设条件时才会触发，所以 `SWITCH_GROUP` 在此类场景下可能会出现无法及时将查询切换到目标资源组的情况。\n\n#### 示例\n\n1. 创建 `rg1` 资源组，限额是每秒 500 RU，并且定义超过 60 秒为 Runaway Query，并对 Runaway Query 降低优先级执行。\n\n    ```sql\n    CREATE RESOURCE GROUP IF NOT EXISTS rg1 RU_PER_SEC = 500 QUERY_LIMIT=(EXEC_ELAPSED='60s', ACTION=COOLDOWN);\n    ```\n\n2. 修改 `rg1` 资源组，对 Runaway Query 直接终止，并且在接下来的 10 分钟里，把相同模式的查询直接标记为 Runaway Query。\n\n    ```sql\n    ALTER RESOURCE GROUP rg1 QUERY_LIMIT=(EXEC_ELAPSED='60s', ACTION=KILL, WATCH=SIMILAR DURATION='10m');\n    ```\n\n3. 修改 `rg1` 资源组，取消 Runaway Queries 检查。\n\n    ```sql\n    ALTER RESOURCE GROUP rg1 QUERY_LIMIT=NULL;\n    ```\n\n#### `QUERY WATCH` 语句说明\n\n语法详见 [`QUERY WATCH`](/sql-statements/sql-statement-query-watch.md)。\n\n参数说明如下：\n\n- `RESOURCE GROUP` 用于指定资源组。此语句添加的 Runaway Queries 监控特征将添加到该资源组的监控列表中。此参数可以省略，省略时作用于 `default` 资源组。\n- `ACTION` 的含义与 `QUERY LIMIT` 相同。此参数可以省略，省略时表示识别后的对应操作采用此时资源组中 `QUERY LIMIT` 配置的 `ACTION`，且不会随着 `QUERY LIMIT` 配置的改变而改变。如果资源组没有配置 `ACTION`，会报错。\n- `QueryWatchTextOption` 参数有 `SQL DIGEST`、`PLAN DIGEST`、`SQL TEXT` 三种类型。\n    - `SQL DIGEST` 的含义与 `QUERY LIMIT` `WATCH` 类型中的 `SIMILAR` 相同，后面紧跟的参数可以是字符串、用户自定义变量以及其他计算结果为字符串的表达式。字符串长度必须为 64，与 TiDB 中关于 Digest 的定义一致。\n    - `PLAN DIGEST` 的含义与 `PLAN` 相同。输入参数为 Digest 字符串。\n    - `SQL TEXT` 可以根据后面紧跟的参数，将输入的 SQL 的原始字符串（使用 `EXACT` 选项）作为模式匹配项，或者经过解析和编译转化为 `SQL DIGEST`（使用 `SIMILAR` 选项）、`PLAN DIGEST`（使用 `PLAN` 选项）来作为模式匹配项。\n\n- 为默认资源组的 Runaway Queries 监控列表添加监控匹配特征（需要提前为默认资源组设置 `QUERY LIMIT`）。\n\n    ```sql\n    QUERY WATCH ADD ACTION KILL SQL TEXT EXACT TO 'select * from test.t2';\n    ```\n\n- 通过将 SQL 解析成 SQL Digest，为 `rg1` 资源组的 Runaway Queries 监控列表添加监控匹配特征。未指定 `ACTION` 时，使用 `rg1` 资源组已配置的 `ACTION`。\n\n    ```sql\n    QUERY WATCH ADD RESOURCE GROUP rg1 SQL TEXT SIMILAR TO 'select * from test.t2';\n    ```\n\n- 通过将 SQL 解析成 SQL Digest，为 `rg1` 资源组的 Runaway Queries 监控列表添加监控匹配特征，并指定 `ACTION` 为 `SWITCH_GROUP(rg2)`。\n\n    ```sql\n    QUERY WATCH ADD RESOURCE GROUP rg1 ACTION SWITCH_GROUP(rg2) SQL TEXT SIMILAR TO 'select * from test.t2';\n    ```\n\n- 通过 PLAN Digest 为 `rg1` 资源组的 Runaway Queries 监控列表添加监控匹配特征，并指定 `ACTION` 为 `KILL`。\n\n    ```sql\n    QUERY WATCH ADD RESOURCE GROUP rg1 ACTION KILL PLAN DIGEST 'd08bc323a934c39dc41948b0a073725be3398479b6fa4f6dd1db2a9b115f7f57';\n    ```\n\n- 通过查询 `INFORMATION_SCHEMA.RUNAWAY_WATCHES` 获取监控项 ID，删除该监控项。\n\n    ```sql\n    SELECT * FROM INFORMATION_SCHEMA.RUNAWAY_WATCHES ORDER BY id\\G\n    ```\n\n    ```sql\n    *************************** 1. row ***************************\n                     ID: 1\n    RESOURCE_GROUP_NAME: default\n             START_TIME: 2024-09-09 03:35:31\n               END_TIME: 2024-09-09 03:45:31\n                  WATCH: Exact\n            WATCH_TEXT: SELECT variable_name, variable_value FROM mysql.global_variables\n                 SOURCE: 127.0.0.1:4000\n                ACTION: Kill\n                RULE: ProcessedKeys = 666(10)\n    1 row in set (0.00 sec)\n    ```\n\n    ```sql\n    QUERY WATCH REMOVE 1;\n    ```\n\n#### 可观测性\n\n可以通过以下系统表和 `INFORMATION_SCHEMA` 表获得 Runaway 相关的更多信息：\n\n+ `mysql.tidb_runaway_queries` 表中包含了过去 7 天内所有识别到的 Runaway Queries 的历史记录。以其中一行为例：\n\n    ```sql\n    MySQL [(none)]> SELECT * FROM mysql.tidb_runaway_queries LIMIT 1\\G\n    *************************** 1. row ***************************\n    resource_group_name: default\n         start_time: 2024-09-09 17:43:42\n            repeats: 2\n         match_type: watch\n             action: kill\n         sample_sql: select sleep(2) from t\n         sql_digest: 4adbc838b86c573265d4b39a3979d0a362b5f0336c91c26930c83ab187701a55\n        plan_digest: 5d094f78efbce44b2923733b74e1d09233cb446318293492901c5e5d92e27dbc\n        tidb_server: 127.0.0.1:4000\n    ```\n\n    字段解释：\n\n    - `start_time` 为该 Runaway Query 被识别的时间。\n    - `repeats` 为该 Runaway Query 从 `start_time` 开始后被识别的次数。\n    - `match_type` 为该 Runaway Query 的来源，其值如下：\n        - `identify` 表示命中条件。\n        - `watch` 表示被快速识别机制命中。\n\n+ `information_schema.runaway_watches` 表中包含了 Runaway Queries 的快速识别规则记录。详见 [`RUNAWAY_WATCHES`](/information-schema/information-schema-runaway-watches.md)。\n\n### 管理后台任务\n\n> **警告：**\n>\n> 该功能目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请[提交 issue](/support.md) 反馈。\n>\n> 资源管控的后台任务管理是基于 TiKV 的 CPU/IO 的资源利用率动态调整资源配额的，因此它依赖各个实例可用资源上限 (Quota)。如果在单个服务器混合部署多个组件或实例，需要通过 cgroup 为各个实例设置合适的资源上限 (Quota)。TiUP Playground 这类共享资源的配置很难表现出预期效果。\n\n后台任务是指那些优先级不高但是需要消耗大量资源的任务，如数据备份和自动统计信息收集等。这些任务通常定期或不定期触发，在执行的时候会消耗大量资源，从而影响在线的高优先级任务的性能。\n\n自 v7.4.0 开始，TiDB 资源管控引入了对后台任务的管理。当一种任务被标记为后台任务时，TiKV 会动态地限制该任务的资源使用，以尽量避免此类任务在执行时对其他前台任务的性能产生影响。TiKV 通过实时地监测所有前台任务所消耗的 CPU 和 IO 等资源，并根据实例总的资源上限计算出后台任务可使用的资源阈值，所有后台任务在执行时会受此阈值的限制。\n\n#### `BACKGROUND` 参数说明\n\n- `TASK_TYPES`：设置需要作为后台任务管理的任务类型，多个任务类型以 `,` 分隔。\n- `UTILIZATION_LIMIT`：限制每个 TiKV 节点上后台任务最大可以使用的资源百分比 (0-100)。默认情况下，TiKV 会根据节点的总资源以及当前前台任务所占用的资源，来计算后台任务的可用资源。如果设置此限制，则实际分配给后台任务的资源不会超过此限制的比例。\n\n目前 TiDB 支持如下几种后台任务的类型：\n\n- `lightning`：使用 [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md) 执行导入任务。同时支持 TiDB Lightning 的物理和逻辑导入模式。\n- `br`：使用 [BR](/br/backup-and-restore-overview.md) 执行数据备份和恢复。目前不支持 PITR。\n- `ddl`：对于 Reorg DDL，控制批量数据回写阶段的资源使用。\n- `stats`：对应手动执行或系统自动触发的[收集统计信息](/statistics.md#收集统计信息)任务。\n- `background`：预留的任务类型，可使用 [`tidb_request_source_type`](/system-variables.md#tidb_request_source_type-从-v740-版本开始引入) 系统变量指定当前会话的任务类型为 `background`。\n\n默认情况下，被标记为后台任务的任务类型为 `\"\"`，此时后台任务的管理功能处于关闭状态。如需开启后台任务管理功能，你需要手动修改 `default` 资源组的后台任务类型以开启后台任务管理。后台任务类型被识别匹配后，资源管控会自动进行，即当系统资源紧张时，后台任务会自动降为最低优先级，保证前台任务的执行。\n\n> **注意：**\n>\n> 目前，所有资源组的后台任务默认都会绑定到默认资源组 `default` 下进行管控，你可以通过 `default` 全局管控后台任务类型。暂不支持将后台任务绑定到其他资源组。\n\n#### 示例\n\n1. 修改 `default` 资源组，将 `br` 和 `ddl` 标记为后台任务，并配置后台任务最多可使用 TiKV 节点总资源的 30%。\n\n    ```sql\n    ALTER RESOURCE GROUP `default` BACKGROUND=(TASK_TYPES='br,ddl', UTILIZATION_LIMIT=30);\n    ```\n\n2. 修改 `default` 资源组，将后台任务的类型还原为默认值。\n\n    ```sql\n    ALTER RESOURCE GROUP `default` BACKGROUND=NULL;\n    ```\n\n3. 修改 `default` 资源组，将后台任务的类型设置为空，此时此资源组的所有任务类型都不会作为后台任务处理。\n\n    ```sql\n    ALTER RESOURCE GROUP `default` BACKGROUND=(TASK_TYPES=\"\");\n    ```\n\n4. 查看 `default` 资源组的后台任务类型。\n\n    ```sql\n    SELECT * FROM information_schema.resource_groups WHERE NAME=\"default\";\n    ```\n\n    输出结果如下：\n\n    ```\n    +---------+------------+----------+-----------+-------------+-------------------------------------------+\n    | NAME    | RU_PER_SEC | PRIORITY | BURSTABLE | QUERY_LIMIT | BACKGROUND                                |\n    +---------+------------+----------+-----------+-------------+-------------------------------------------+\n    | default | UNLIMITED  | MEDIUM   | YES       | NULL        | TASK_TYPES='br,ddl', UTILIZATION_LIMIT=30 |\n    +---------+------------+----------+-----------+-------------+-------------------------------------------+\n    ```\n\n5. 如果希望将当前会话里的任务显式标记为后台类型，你可以使用 `tidb_request_source_type` 显式指定任务类型，如：\n\n    ``` sql\n    SET @@tidb_request_source_type=\"background\";\n    /* 添加 background 任务类型 */\n    ALTER RESOURCE GROUP `default` BACKGROUND=(TASK_TYPES=\"background\");\n    /* 在当前会话中执行 LOAD DATA */\n    LOAD DATA INFILE \"s3://resource-control/Lightning/test.customer.aaaa.csv\"\n    ```\n\n## 关闭资源管控特性\n\n1. 执行以下命令关闭资源管控特性：\n\n    ```sql\n    SET GLOBAL tidb_enable_resource_control = 'OFF';\n    ```\n\n2. 将 TiKV 参数 [`resource-control.enabled`](/tikv-configuration-file.md#resource-control) 设为 `false`，关闭按照资源组配额调度。\n\n3. 将 TiFlash 参数 [`enable_resource_control`](/tiflash/tiflash-configuration.md#配置文件-tiflashtoml) 设为 `false`，关闭 TiFlash 资源管控。\n\n## 查看 RU 消耗\n\n你可以查看 RU 消耗的相关信息。\n\n### 查看 SQL 的 RU 消耗\n\n你可以通过以下方式查询 SQL 消耗的 RU：\n\n- 系统变量 `tidb_last_query_info`\n- `EXPLAIN ANALYZE`\n- 慢查询及对应的系统表\n- `statements_summary`\n\n#### 使用系统变量 `tidb_last_query_info` 查询执行上一条 SQL 语句的 RU 消耗\n\nTiDB 提供系统变量 [`tidb_last_query_info`](/system-variables.md#tidb_last_query_info-从-v4014-版本开始引入)，记录上一条 DML 语句执行的信息，其中包含 SQL 执行消耗的 RU。\n\n使用示例：\n\n1. 执行 `UPDATE` 语句：\n\n    ```sql\n    UPDATE sbtest.sbtest1 SET k = k + 1 WHERE id = 1;\n    ```\n\n    ```\n    Query OK, 1 row affected (0.01 sec)\n    Rows matched: 1  Changed: 1  Warnings: 0\n    ```\n\n2. 通过查询系统变量 `tidb_last_query_info`，查看上条执行的语句的相关信息：\n\n    ```sql\n    SELECT @@tidb_last_query_info;\n    ```\n\n    ```\n    +------------------------------------------------------------------------------------------------------------------------+\n    | @@tidb_last_query_info                                                                                                 |\n    +------------------------------------------------------------------------------------------------------------------------+\n    | {\"txn_scope\":\"global\",\"start_ts\":446809472210829315,\"for_update_ts\":446809472210829315,\"ru_consumption\":4.34885578125} |\n    +------------------------------------------------------------------------------------------------------------------------+\n    1 row in set (0.01 sec)\n    ```\n\n    返回结果中的 `ru_consumption` 即为执行此 SQL 语句消耗的 RU。\n\n#### 使用 `EXPLAIN ANALYZE` 查询 SQL 执行时所消耗的 RU\n\n你也可以通过 [`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md#ru-request-unit-消耗) 语句获取到 SQL 执行时所消耗的 RU。注意 RU 的大小会受缓存的影响（比如[下推计算结果缓存](/coprocessor-cache.md)），多次执行同一条 SQL 所消耗的 RU 可能会有不同。因此这个 RU 值并不代表每次执行的精确值，但可以作为估算的参考。\n\n#### 慢查询及对应的系统表\n\n在开启资源管控时，TiDB 的[慢查询日志](/identify-slow-queries.md)以及对应系统表 [`INFORMATION_SCHEMA.SLOW_QUERY`](/information-schema/information-schema-slow-query.md) 中均包含对应 SQL 所属的资源组、等待可用 RU 的耗时、以及真实 RU 消耗等相关信息。\n\n#### 通过 `statements_summary` 查询 RU 相关的统计信息\n\nTiDB 的系统表 [`INFORMATION_SCHEMA.statements_summary`](/statement-summary-tables.md#statements_summary) 中保存了 SQL 语句归一化聚合后的各种统计信息，可以用于查看分析各个 SQL 语句的执行性能。其中也包含资源管控相关的统计信息，包括资源组名、RU 消耗、等待可用 RU 的耗时等信息。具体请参考[`statements_summary` 字段介绍](/statement-summary-tables.md#statements_summary-字段介绍)。\n\n### 查看资源组的 RU 消耗\n\n从 v7.6.0 版本开始，TiDB 提供系统表 [`mysql.request_unit_by_group`](/mysql-schema/mysql-schema.md#资源管控相关系统表) 存放各个资源组每日消耗的 RU 的历史记录。\n\n示例：\n\n```sql\nSELECT * FROM request_unit_by_group LIMIT 5;\n```\n\n```\n+----------------------------+----------------------------+----------------+----------+\n| start_time                 | end_time                   | resource_group | total_ru |\n+----------------------------+----------------------------+----------------+----------+\n| 2024-01-01 00:00:00.000000 | 2024-01-02 00:00:00.000000 | default        |   334147 |\n| 2024-01-01 00:00:00.000000 | 2024-01-02 00:00:00.000000 | rg1            |     4172 |\n| 2024-01-01 00:00:00.000000 | 2024-01-02 00:00:00.000000 | rg2            |    34028 |\n| 2024-01-02 00:00:00.000000 | 2024-01-03 00:00:00.000000 | default        |   334088 |\n| 2024-01-02 00:00:00.000000 | 2024-01-03 00:00:00.000000 | rg1            |     3850 |\n+----------------------------+----------------------------+----------------+----------+\n5 rows in set (0.01 sec)\n```\n\n> **注意：**\n>\n> `mysql.request_unit_by_group` 的数据由 TiDB 的定时任务在每天结束后自动导入。如果某个资源组当天的 RU 消耗为 0，则不会产生一条记录。此表默认只存放最近 3 个月（最多 92 天）的数据。超过此期限的数据会自动被清理。\n\n## 监控与图表\n\nTiDB 会定时采集资源管控的运行时信息，并在 Grafana 的 **Resource Control Dashboard** 中提供了相关指标的可视化图表，详见 [Resource Control 监控指标详解](/grafana-resource-control-dashboard.md)。\n\nTiKV 中也记录了来自于不同资源组的请求 QPS，详见 [TiKV 监控指标详解](/grafana-tikv-dashboard.md#grpc)。\n\nTiDB Dashboard 中可以查看当前 [`RESOURCE_GROUPS`](/information-schema/information-schema-resource-groups.md) 表中资源组的数据。详见 [TiDB Dashboard 资源管控页面](/dashboard/dashboard-resource-manager.md)。\n\n## 工具兼容性\n\n资源管控不影响数据导入导出以及其他同步工具的正常使用，BR、TiDB Lightning、TiCDC 等工具不支持对资源管控相关 DDL 的处理，这些工具的资源消耗也不受资源管控的限制。\n\n## 常见问题\n\n1. 如果我暂时不想使用资源组对资源进行管控，是否一定要关闭这个特性？\n\n    不需要。没有指定任何资源组的用户，将被放入系统预定义的 `default` 资源组，而 `default` 资源组默认拥有无限用量。当所有用户都属于 `default` 资源组时，资源分配方式与关闭资源管控时相同。\n\n2. 一个数据库用户是否可以绑定到不同的资源组？\n\n    不能。一个数据库用户只能绑定到一个资源组。但是，在会话运行的过程中，可以通过 [`SET RESOURCE GROUP`](/sql-statements/sql-statement-set-resource-group.md) 设置当前会话使用的资源组。你也可以通过优化器 [`RESOURCE_GROUP()`](/optimizer-hints.md#resource_groupresource_group_name) Hint 为运行的语句设置资源组。\n\n3. 当各个资源组设置的用量 (`RU_PER_SEC`) 总和超出系统容量会发生什么？\n\n    TiDB 在创建资源组时不会检查容量。只要系统有足够的空闲资源，TiDB 就会满足每个资源组的用量设置。当系统资源超过限制时，TiDB 会优先满足高优先级 (PRIORITY) 资源组的请求。如果同一优先级的请求无法全部满足，TiDB 会根据用量 (`RU_PER_SEC`) 的大小按比例分配。\n\n## 另请参阅\n\n* [CREATE RESOURCE GROUP](/sql-statements/sql-statement-create-resource-group.md)\n* [ALTER RESOURCE GROUP](/sql-statements/sql-statement-alter-resource-group.md)\n* [DROP RESOURCE GROUP](/sql-statements/sql-statement-drop-resource-group.md)\n* [RESOURCE GROUP RFC](https://github.com/pingcap/tidb/blob/master/docs/design/2022-11-25-global-resource-control.md)\n"
        },
        {
          "name": "tidb-roadmap.md",
          "type": "blob",
          "size": 14.6171875,
          "content": "---\ntitle: TiDB 路线图\nsummary: 了解 TiDB 未来的发展方向，包括新特性和改进提升。\n---\n\n# TiDB 路线图\n\nTiDB 路线图展示了 TiDB 未来的计划。随着我们发布长期稳定版本 (LTS)，这个路线图将会持续更新。通过路线图，你可以预先了解 TiDB 的未来规划，以便你关注进度，了解关键里程碑，并对开发工作提出反馈。\n\n在开发过程中，路线图可能会根据用户需求和反馈进行调整，请不要根据路线图的内容制定上线计划。如果你有功能需求，或者想提高某个特性的优先级，请在 [GitHub](https://github.com/pingcap/tidb/issues) 上提交 issue。\n\n> **注意：**\n> \n> - 下表中未注明 GA (Generally Available) 的特性，均为实验特性。\n> - 下表中并未列出所有计划发布的内容。\n> - 不同的服务订阅版本中的功能可能有所不同。\n\n## TiDB 重要特性规划\n\n<table>\n  <thead>\n    <tr>\n      <th>类别</th>\n      <th>2024 年底版本</th>\n      <th>2025 年中版本</th>\n      <th>未来版本</th>\n    </tr>\n  </thead>\n  <tbody valign=\"top\">\n    <tr>\n      <td>\n        <b>可扩展性与性能</b>\n        <br />提供更强的扩展能力和更快的性能，支持超大规模的工作负载，优化资源利用，提升集群性能。\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>TiKV 数据缓存 </b>\n            <br />TiKV 在内存中维护数据的最近版本，减少对多版本数据的反复扫描，进而提升性能。\n          </li>\n          <br />\n          <li>\n            <b>自动配置统计信息收集的并行度 (GA)</b>\n            <br />TiDB 根据部署的节点数以及硬件规格自动设置统计信息收集任务的并行度和扫描并发度，提升收集速度。\n          </li>\n          <br />\n          <li>\n            <b>加速数据库恢复</b>\n            <br />缩短全量数据库恢复和 Point in Time Recovery (PITR) 所需的时间。\n          </li>\n          <br />\n          <li>\n            <b>支持不限大小的事务</b>\n            <br />未提交事务所处理的数据量，不再依赖 TiDB 节点的可用内存大小。提升事务及批量任务的成功率。\n          </li>\n          <br />\n          <li>\n            <b>TiProxy 根据负载转发流量(GA)</b>\n            <br />TiProxy 依据目标 TiDB 的负载对流量进行转发，以此充分利用硬件资源。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>PD 的路由功能微服务化</b>\n            <br />实现 PD 路由服务（Region 元数据的访问、更新）的独立部署，路由服务完全改造为无状态服务（无强领导者）、易于扩展，避免 PD 成为集群资源瓶颈。\n          </li>\n          <br />\n          <li>\n            <b>减少统计信息收集时的 I/O 消耗 (GA)</b>\n            <br />当收集统计信息时，允许 TiKV 上仅扫描部分数据样本，以减少统计信息收集所消耗的时间和资源。\n          </li>\n          <br />\n          <li>\n            <b>移除将 Limit 算子下推到 TiKV 的已知限制</b>\n            <br />移除将 Limit 运算符从 TiDB 下推到 TiKV 的限制，从而可以直接在存储层进行更高效的查询处理。\n          </li>\n          <br />\n          <li>\n            <b>Cascades optimizer </b>\n            <br />引入更成熟强大的优化器框架，扩展当前优化器的基础能力。\n          </li>\n          <br />\n          <li>\n            <b>增强 DDL 执行框架</b>\n            <br />提供可扩展的并行 DDL 执行框架，提升 DDL 的性能和稳定性。\n          </li>\n          <br />\n          <li>\n            <b>增强 TiCDC 的扩展性</b>\n            <br />推出新的 TiCDC 架构，提升 TiCDC 的扩展性以及性能。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>表级别的负载均衡</b>\n            <br />PD 根据每个表上各 Region 的负载决定数据的调度策略。\n          </li>\n          <br />\n          <li>\n            <b>系统表性能优化</b>\n            <br />当系统表中存有大量数据时，提升查询系统表的查询性能。\n          </li>\n          <br />\n          <li>\n            <b>增强 Region 元数据存储的可扩展性</b>\n            <br />拆分专用的无状态路由器服务（区域元数据读/写），并将区域元数据存储从 PD 迁移到 TiKV。 元数据存储层将轻松实现无限扩展。\n          </li>\n          <br />\n        </ul>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        <b>SQL 功能</b>\n        <br />提供前沿的 SQL 功能，提升了兼容性、灵活性和易用性，助力复杂查询和现代应用程序的高效运行。\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>支持向量搜索功能</b>\n            <br />支持向量数据类型、向量索引及高性能向量搜索能力，同时具备向量和关系数据混合查询能力。\n          </li>\n          <br />\n          <li>\n            <b>外键成为正式功能 (GA)</b>\n          </li>\n          <br />\n          <li>\n            <b>分区表全局索引成为正式功能 (GA)</b>\n            <br />解除分区表唯一键必须包含分区键的限制，提升分区表非分区列的查询性能。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>支持修改分区表的列类型</b>\n            <br />你可以修改分区中列的类型，无论是否为分区键。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>支持物化视图</b>\n            <br />支持物化视图功能，改进预处理能力，优化计算效率，进一步提升数据分析性能。\n          </li>\n          <br />\n        </ul>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        <b>稳定性与高可用</b>\n        <br />确保持续运行，提升系统容错能力，为用户提供稳定可靠的使用体验。\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>限制备份任务的内存消耗</b>\n          </li>\n          <br />\n          <li>\n            <b>限制统计信息收集的内存消耗 (GA)</b>\n          </li>\n          <br />\n          <li>\n            <b>管理大量的 SQL Binding (GA)</b>\n            <br />提升 SQL Binding 的使用体验，方便用户创建和管理大量的执行计划，以稳定数据库性能。\n          </li>\n          <br />\n          <li>\n            <b>资源组增强对复杂 SQL 的控制 (GA)</b>\n            <br />在复杂 SQL 执行完成前，间歇性衡量 SQL 的 RU 消耗，避免在 SQL 执行期间对系统产生过大影响。\n          </li>\n          <br />\n          <li>\n            <b>自动为超预期查询切换资源组 (GA)</b>\n            <br />当一个查询被认定为 Runaway Query，你可以选择将其置入一个特定资源组，为其资源消耗设置上限。\n          </li>\n          <br />\n          <li>\n            <b>限制表元信息的内存消耗 (GA)</b>\n            <br />减少大规模集群下表的元信息对内存的消耗，提升大规模集群的稳定性。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>更可靠的数据备份 </b>\n            <br />减少数据备份过程中可能出现的内存不足等问题，并确保备份数据的可用性。\n          </li>\n          <br />\n          <li>\n            <b>常用算子均可落盘</b>\n            <br />HashAgg、Sort、TopN、HashJoin、WindowFunction、IndexJoin 和 IndexHashJoin 等常用算子均可落盘，进一步降低 OOM 风险。\n          </li>\n          <br />\n          <li>\n            <b>实例级执行计划缓存 (GA)</b>\n            <br />同一个 TiDB 实例的所有会话可以共享执行计划缓存，提升内存利用率。\n          </li>\n          <br />\n          <li>\n            <b>资源组优先满足限额内定义的用量 (RU) (GA)</b>\n            <br />动态调整 Burstable 资源组使用的资源上限。在不影响其他资源组限额的情况下，充分利用剩余资源。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>自适应资源组</b>\n            <br />资源组根据过往的运行情况自动调整资源组的 RU 设定。\n          </li>\n          <br />\n          <li>\n            <b>强化的内存保护</b>\n            <br />TiDB 主动对所有模块的内存使用进行监控，阻止一切可能影响系统稳定性的内存操作。\n          </li>\n          <br />\n          <li>\n            <b>自动 SQL 绑定</b>\n            <br />通过对 SQL 运行指标的收集和分析，对一部分执行计划自动创建绑定，提升 OLTP 类系统的执行计划稳定性。\n          </li>\n          <br />\n          <li>\n            <b>多版本统计信息</b>\n            <br />当统计信息被更新后，你可以查看统计信息的过往版本，并能够选择恢复过去某个版本的统计信息。\n          </li>\n          <br />\n          <li>\n            <b>分布式统计信息收集</b>\n            <br />统计信息收集支持在多个 TiDB 节点上并行进行，提升收集效率。\n          </li>\n          <br />\n        </ul>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        <b>数据库管理与可观测性</b>\n        <br />通过主动监控和管理，确保系统平稳运行。\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>可靠的 SQL 终止操作 (GA)</b>\n            <br />正在运行中的 SQL 语句能够被立即终止，并从 TiDB 和 TiKV 中释放相应的资源。\n          </li>\n          <br />\n          <li>\n            <b>切换资源组的权限控制 (GA)</b>\n            <br />只有被授予特定权限的用户，才可以切换自身的资源组，防止资源被滥用。\n          </li>\n          <br />\n          <li>\n            <b>增加对 TiDB 和 TiKV CPU 时间的观测 (GA)</b>\n            <br />在 statements 记录、慢日志中增加 TiDB 和 TiKV CPU 时间的指标，方便快速定位造成 TiDB 或者 TiKV CPU 飙升的 SQL 语句。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>细粒度定制统计信息收集策略 (GA)</b>\n            <br />你可以为特定的表定制统计策略，调整健康度和并发度等参数。\n          </li>\n          <br />\n          <li>\n            <b>Workload Repository (GA)</b>\n            <br />TiDB 持久化内存中记录的负载信息，包括累计统计数据和实时统计数据，有助于故障排查和分析。\n          </li>\n          <br />\n          <li>\n            <b>自动索引推荐 (GA)</b>\n            <br />TiDB 自动分析有优化价值的 SQL，推荐创建新索引或删除已有索引。\n          </li>\n          <br />\n          <li>\n            <b>标准时间模型 (GA)</b>\n            <br />对 SQL 的运行时间进行标准化定义，以此为基础定义数据库负载。通过观测 statements 记录、慢日志、聚合的集群指标，用户能够准确发现产生异常负载的节点及 SQL。\n          </li>\n          <br />\n          <li>\n            <b>增加对 TiFlash CPU 时间的观测 (GA)</b>\n            <br />在 statements 记录和慢日志中增加 TiFlash CPU 时间的指标，方便快速定位造成 TiFlash CPU 飙升的 SQL 语句。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>负载分析</b>\n            <br />分析 Workload Repository 中的过往负载数据，根据分析结果提出优化建议，例如 SQL 调优和统计信息收集策略调整。\n          </li>\n          <br />\n          <li>\n            <b>全链路监控</b>\n            <br />跟踪单条 SQL 语句在其运行的整个生命周期的时间消耗，包括 TiDB、PD、TiKV 和 TiFlash。\n          </li>\n          <br />\n        </ul>\n      </td>\n    </tr>\n    <tr>\n      <td>\n        <b>全面的数据安全和隐私保护</b>\n        <br />强化保护敏感数据的安全措施，提供顶级加密保障，并确保符合不断演进的隐私法规\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>Google Cloud KMS (GA)</b>\n            <br />完善静态加密基于 Google Cloud KMS 的密钥管理机制，使其成为正式功能。\n          </li>\n          <br />\n          <li>\n            <b>Azure Key Vault</b>\n            <br />基于 Azure Key Vault 增强静态加密的密钥管理机制。\n          </li>\n          <br />\n          <li>\n            <b>基于标记的日志脱敏</b>\n            <br />支持在集群日志中标记敏感信息，然后可以根据使用场景决定是否对其进行脱敏。\n          </li>\n          <br />\n          <li>\n            <b>列级权限管理 (GA)</b>\n            <br />支持兼容 MySQL 的列级权限管理机制。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>AWS 的 IAM 认证</b>\n            <br />支持与 AWS IAM 的第三方 ARN 集成，以实现安全的访问控制。\n          </li>\n          <br />\n          <li>\n            <b>Kerberos 认证 (GA)</b>\n            <br />支持基于 Kerberos 的身份验证。\n          </li>\n          <br />\n          <li>\n            <b>MFA</b>\n            <br />增加对多因素认证的支持，增强用户对多因素认证机制的验证。\n          </li>\n          <br />\n          <li>\n            <b>组件之间的 TLS 改进 (GA)</b>\n            <br />确保 TiDB 集群的所有组件之间的连接支持加密传输。\n          </li>\n          <br />\n          <li>\n            <b>完善动态权限</b>\n            <br />完善动态权限设计，限制 Super 权限的实现。\n          </li>\n          <br />\n          <li>\n            <b>FIPS (GA)</b>\n            <br />加密场景符合 FIPS 标准。\n          </li>\n          <br />\n        </ul>\n      </td>\n      <td>\n        <ul>\n          <li>\n            <b>基于标签的访问控制机制</b>\n            <br />支持通过配置标签的方式对数据进行访问控制。\n          </li>\n          <br />\n          <li>\n            <b>增强的客户端加密</b>\n            <br />支持客户端对关键字段加密，增强数据安全性。\n          </li>\n          <br />\n          <li>\n            <b>业务数据动态脱敏</b>\n            <br />基于不同数据应用场景的数据脱敏，保证重要领域的数据安全。\n          </li>\n          <br />\n        </ul>\n      </td>\n    </tr>\n  </tbody>\n</table>\n"
        },
        {
          "name": "tidb-scheduling.md",
          "type": "blob",
          "size": 11.7607421875,
          "content": "---\ntitle: TiDB 数据库的调度\naliases: ['/docs-cn/dev/tidb-scheduling/']\nsummary: TiDB 数据库的调度由 PD（Placement Driver）模块负责管理和实时调度集群数据。PD 需要收集节点和 Region 的状态信息，并根据调度策略制定调度计划，包括增加 / 删除副本、迁移 Leader 角色等基本操作。调度需满足副本数量、位置分布、负载均衡、存储空间利用等需求。PD 通过心跳包收集信息，并根据策略生成调度操作序列，但具体执行由 Region Leader 决定。\n---\n\n# TiDB 数据库的调度\n\n[PD](https://github.com/tikv/pd) (Placement Driver) 是 TiDB 集群的管理模块，同时也负责集群数据的实时调度。本文档介绍一下 PD 的设计思想和关键概念。\n\n## 场景描述\n\nTiKV 集群是 TiDB 数据库的分布式 KV 存储引擎，数据以 Region 为单位进行复制和管理，每个 Region 会有多个副本 (Replica)，这些副本会分布在不同的 TiKV 节点上，其中 Leader 负责读/写，Follower 负责同步 Leader 发来的 Raft log。\n\n需要考虑以下场景：\n\n* 为了提高集群的空间利用率，需要根据 Region 的空间占用对副本进行合理的分布。\n* 集群进行跨机房部署的时候，要保证一个机房掉线，不会丢失 Raft Group 的多个副本。\n* 添加一个节点进入 TiKV 集群之后，需要合理地将集群中其他节点上的数据搬到新增节点。\n* 当一个节点掉线时，需要考虑快速稳定地进行容灾。\n    * 从节点的恢复时间来看\n        * 如果节点只是短暂掉线（重启服务），是否需要进行调度。\n        * 如果节点是长时间掉线（磁盘故障，数据全部丢失），如何进行调度。\n    * 假设集群需要每个 Raft Group 有 N 个副本，从单个 Raft Group 的副本个数来看\n        * 副本数量不够（例如节点掉线，失去副本），需要选择适当的机器的进行补充。\n        * 副本数量过多（例如掉线的节点又恢复正常，自动加入集群），需要合理的删除多余的副本。\n* 读/写通过 Leader 进行，Leader 的分布只集中在少量几个节点会对集群造成影响。\n* 并不是所有的 Region 都被频繁的访问，可能访问热点只在少数几个 Region，需要通过调度进行负载均衡。\n* 集群在做负载均衡的时候，往往需要搬迁数据，这种数据的迁移可能会占用大量的网络带宽、磁盘 IO 以及 CPU，进而影响在线服务。\n\n以上问题和场景如果多个同时出现，就不太容易解决，因为需要考虑全局信息。同时整个系统也是在动态变化的，因此需要一个中心节点，来对系统的整体状况进行把控和调整，所以有了 PD 这个模块。\n\n## 调度的需求\n\n对以上的问题和场景进行分类和整理，可归为以下两类：\n\n**第一类：作为一个分布式高可用存储系统，必须满足的需求，包括几种**\n\n* 副本数量不能多也不能少\n* 副本需要根据拓扑结构分布在不同属性的机器上\n* 节点宕机或异常能够自动合理快速地进行容灾\n\n**第二类：作为一个良好的分布式系统，需要考虑的地方包括**\n\n* 维持整个集群的 Leader 分布均匀\n* 维持每个节点的储存容量均匀\n* 维持访问热点分布均匀\n* 控制负载均衡的速度，避免影响在线服务\n* 管理节点状态，包括手动上线/下线节点\n\n满足第一类需求后，整个系统将具备强大的容灾功能。满足第二类需求后，可以使得系统整体的资源利用率更高且合理，具备良好的扩展性。\n\n为了满足这些需求，首先需要收集足够的信息，比如每个节点的状态、每个 Raft Group 的信息、业务访问操作的统计等；其次需要设置一些策略，PD 根据这些信息以及调度的策略，制定出尽量满足前面所述需求的调度计划；最后需要一些基本的操作，来完成调度计划。\n\n## 调度的基本操作\n\n调度的基本操作指的是为了满足调度的策略。上述调度需求可整理为以下三个操作：\n\n* 增加一个副本\n* 删除一个副本\n* 将 Leader 角色在一个 Raft Group 的不同副本之间 transfer（迁移）\n\n刚好 Raft 协议通过 `AddReplica`、`RemoveReplica`、`TransferLeader` 这三个命令，可以支撑上述三种基本操作。\n\n## 信息收集\n\n调度依赖于整个集群信息的收集，简单来说，调度需要知道每个 TiKV 节点的状态以及每个 Region 的状态。TiKV 集群会向 PD 汇报两类消息，TiKV 节点信息和 Region 信息：\n\n**每个 TiKV 节点会定期向 PD 汇报节点的状态信息**\n\nTiKV 节点 (Store) 与 PD 之间存在心跳包，一方面 PD 通过心跳包检测每个 Store 是否存活，以及是否有新加入的 Store；另一方面，心跳包中也会携带这个 [Store 的状态信息](https://github.com/pingcap/kvproto/blob/master/proto/pdpb.proto#L473)，主要包括：\n\n* 总磁盘容量\n* 可用磁盘容量\n* 承载的 Region 数量\n* 数据写入/读取速度\n* 发送/接受的 Snapshot 数量（副本之间可能会通过 Snapshot 同步数据）\n* 是否过载\n* labels 标签信息（标签是具备层级关系的一系列 Tag，能够[感知拓扑信息](/schedule-replicas-by-topology-labels.md)）\n\n通过使用 `pd-ctl` 可以查看到 TiKV Store 的状态信息。TiKV Store 的状态具体分为 Up，Disconnect，Offline，Down，Tombstone。各状态的关系如下：\n\n+ **Up**：表示当前的 TiKV Store 处于提供服务的状态。\n+ **Disconnect**：当 PD 和 TiKV Store 的心跳信息丢失超过 20 秒后，该 Store 的状态会变为 Disconnect 状态，当时间超过 `max-store-down-time` 指定的时间后，该 Store 会变为 Down 状态。\n+ **Down**：表示该 TiKV Store 与集群失去连接的时间已经超过了 `max-store-down-time` 指定的时间，默认 30 分钟。超过该时间后，对应的 Store 会变为 Down，并且开始在存活的 Store 上补足各个 Region 的副本。\n+ **Offline**：当对某个 TiKV Store 通过 PD Control 进行手动下线操作，该 Store 会变为 Offline 状态。该状态只是 Store 下线的中间状态，处于该状态的 Store 会将其上的所有 Region 搬离至其它满足搬迁条件的 Up 状态 Store。当该 Store 的 `leader_count` 和 `region_count` (在 PD Control 中获取) 均显示为 0 后，该 Store 会由 Offline 状态变为 Tombstone 状态。在 Offline 状态下，禁止关闭该 Store 服务以及其所在的物理服务器。下线过程中，如果集群里不存在满足搬迁条件的其它目标 Store（例如没有足够的 Store 能够继续满足集群的副本数量要求），该 Store 将一直处于 Offline 状态。\n+ **Tombstone**：表示该 TiKV Store 已处于完全下线状态，可以使用 `remove-tombstone` 接口安全地清理该状态的 TiKV。\n\n![TiKV store status relationship](/media/tikv-store-status-relationship.png)\n\n**每个 Raft Group 的 Leader 会定期向 PD 汇报 Region 的状态信息**\n\n每个 Raft Group 的 Leader 和 PD 之间存在心跳包，用于汇报这个 [Region 的状态](https://github.com/pingcap/kvproto/blob/master/proto/pdpb.proto#L312)，主要包括下面几点信息：\n\n* Leader 的位置\n* Followers 的位置\n* 掉线副本的个数\n* 数据写入/读取的速度\n\nPD 不断的通过这两类心跳消息收集整个集群的信息，再以这些信息作为决策的依据。\n\n除此之外，PD 还可以通过扩展的接口接受额外的信息，用来做更准确的决策。比如当某个 Store 的心跳包中断的时候，PD 并不能判断这个节点是临时失效还是永久失效，只能经过一段时间的等待（默认是 30 分钟），如果一直没有心跳包，就认为该 Store 已经下线，再决定需要将这个 Store 上面的 Region 都调度走。\n\n但是有的时候，是运维人员主动将某台机器下线，这个时候，可以通过 PD 的管理接口通知 PD 该 Store 不可用，PD 就可以马上判断需要将这个 Store 上面的 Region 都调度走。\n\n## 调度的策略\n\nPD 收集了这些信息后，还需要一些策略来制定具体的调度计划。\n\n**一个 Region 的副本数量正确**\n\n当 PD 通过某个 Region Leader 的心跳包发现这个 Region 的副本数量不满足要求时，需要通过 Add/Remove Replica 操作调整副本数量。出现这种情况的可能原因是：\n\n* 某个节点掉线，上面的数据全部丢失，导致一些 Region 的副本数量不足\n* 某个掉线节点又恢复服务，自动接入集群，这样之前已经补足了副本的 Region 的副本数量过多，需要删除某个副本\n* 管理员调整副本策略，修改了 [max-replicas](https://github.com/pingcap/pd/blob/v4.0.0-beta/conf/config.toml#L95) 的配置\n\n**一个 Raft Group 中的多个副本不在同一个位置**\n\n注意这里用的是『同一个位置』而不是『同一个节点』。在一般情况下，PD 只会保证多个副本不落在一个节点上，以避免单个节点失效导致多个副本丢失。在实际部署中，还可能出现下面这些需求：\n\n* 多个节点部署在同一台物理机器上\n* TiKV 节点分布在多个机架上，希望单个机架掉电时，也能保证系统可用性\n* TiKV 节点分布在多个 IDC 中，希望单个机房掉电时，也能保证系统可用性\n\n这些需求本质上都是某一个节点具备共同的位置属性，构成一个最小的『容错单元』，希望这个单元内部不会存在一个 Region 的多个副本。这个时候，可以给节点配置 [labels](https://github.com/tikv/tikv/blob/v4.0.0-beta/etc/config-template.toml#L140) 并且通过在 PD 上配置 [location-labels](https://github.com/pingcap/pd/blob/v4.0.0-beta/conf/config.toml#L100) 来指名哪些 label 是位置标识，需要在副本分配的时候尽量保证一个 Region 的多个副本不会分布在具有相同的位置标识的节点上。\n\n**副本在 Store 之间的分布均匀分配**\n\n由于每个 Region 的副本中存储的数据容量上限是固定的，通过维持每个节点上面副本数量的均衡，使得各节点间承载的数据更均衡。\n\n**Leader 数量在 Store 之间均匀分配**\n\nRaft 协议要求读取和写入都通过 Leader 进行，所以计算的负载主要在 Leader 上面，PD 会尽可能将 Leader 在节点间分散开。\n\n**访问热点数量在 Store 之间均匀分配**\n\n每个 Store 以及 Region Leader 在上报信息时携带了当前访问负载的信息，比如 Key 的读取/写入速度。PD 会检测出访问热点，且将其在节点之间分散开。\n\n**各个 Store 的存储空间占用大致相等**\n\n每个 Store 启动的时候都会指定一个 `Capacity` 参数，表明这个 Store 的存储空间上限，PD 在做调度的时候，会考虑节点的存储空间剩余量。\n\n**控制调度速度，避免影响在线服务**\n\n调度操作需要耗费 CPU、内存、磁盘 IO 以及网络带宽，需要避免对线上服务造成太大影响。PD 会对当前正在进行的操作数量进行控制，默认的速度控制是比较保守的，如果希望加快调度（比如停服务升级或者增加新节点，希望尽快调度），那么可以通过调节 PD 参数动态加快调度速度。\n\n## 调度的实现\n\n本节介绍调度的实现\n\nPD 不断地通过 Store 或者 Leader 的心跳包收集整个集群信息，并且根据这些信息以及调度策略生成调度操作序列。每次收到 Region Leader 发来的心跳包时，PD 都会检查这个 Region 是否有待进行的操作，然后通过心跳包的回复消息，将需要进行的操作返回给 Region Leader，并在后面的心跳包中监测执行结果。\n\n注意这里的操作只是给 Region Leader 的建议，并不保证一定能得到执行，具体是否会执行以及什么时候执行，由 Region Leader 根据当前自身状态来定。\n"
        },
        {
          "name": "tidb-storage.md",
          "type": "blob",
          "size": 8.1416015625,
          "content": "---\ntitle: TiDB 数据库的存储\nsummary: 了解 TiDB 数据库的存储层。\naliases: ['/docs-cn/dev/tidb-storage/']\n---\n\n# TiDB 数据库的存储\n\n本文主要介绍 [TiKV](https://github.com/tikv/tikv) 的一些设计思想和关键概念。\n\n![storage-architecture](/media/tidb-storage-architecture-1.png)\n\n## Key-Value Pairs（键值对）\n\n作为保存数据的系统，首先要决定的是数据的存储模型，也就是数据以什么样的形式保存下来。TiKV 的选择是 Key-Value 模型，并且提供有序遍历方法。\n\nTiKV 数据存储的两个关键点：\n\n1. 这是一个巨大的 Map（可以类比一下 C++ 的 std::map），也就是存储的是 Key-Value Pairs（键值对）\n2. 这个 Map 中的 Key-Value pair 按照 Key 的二进制顺序有序，也就是可以 Seek 到某一个 Key 的位置，然后不断地调用 Next 方法以递增的顺序获取比这个 Key 大的 Key-Value。\n\n注意，本文所说的 **TiKV 的 KV 存储模型和 SQL 中的 Table 无关**。本文不讨论 SQL 中的任何概念，专注于讨论如何实现 TiKV 这样一个高性能、高可靠性、分布式的 Key-Value 存储。\n\n## 本地存储 (RocksDB)\n\n任何持久化的存储引擎，数据终归要保存在磁盘上，TiKV 也不例外。但是 TiKV 没有选择直接向磁盘上写数据，而是把数据保存在 RocksDB 中，具体的数据落地由 RocksDB 负责。这个选择的原因是开发一个单机存储引擎工作量很大，特别是要做一个高性能的单机引擎，需要做各种细致的优化，而 RocksDB 是由 Facebook 开源的一个非常优秀的单机 KV 存储引擎，可以满足 TiKV 对单机引擎的各种要求。这里可以简单的认为 RocksDB 是一个单机的持久化 Key-Value Map。\n\n## Raft 协议\n\n接下来 TiKV 的实现面临一件更难的事情：如何保证单机失效的情况下，数据不丢失，不出错？\n\n简单来说，需要想办法把数据复制到多台机器上，这样一台机器无法服务了，其他的机器上的副本还能提供服务；复杂来说，还需要这个数据复制方案是可靠和高效的，并且能处理副本失效的情况。TiKV 选择了 Raft 算法。Raft 是一个一致性协议，本文只会对 Raft 做一个简要的介绍，细节问题可以参考它的[论文](https://raft.github.io/raft.pdf)。Raft 提供几个重要的功能：\n\n- Leader（主副本）选举\n- 成员变更（如添加副本、删除副本、转移 Leader 等操作）\n- 日志复制\n\nTiKV 利用 Raft 来做数据复制，每个数据变更都会落地为一条 Raft 日志，通过 Raft 的日志复制功能，将数据安全可靠地同步到复制组的每一个节点中。不过在实际写入中，根据 Raft 的协议，只需要同步复制到多数节点，即可安全地认为数据写入成功。\n\n![Raft in TiDB](/media/tidb-storage-1.png)\n\n总结一下，通过单机的 RocksDB，TiKV 可以将数据快速地存储在磁盘上；通过 Raft，将数据复制到多台机器上，以防单机失效。数据的写入是通过 Raft 这一层的接口写入，而不是直接写 RocksDB。通过实现 Raft，TiKV 变成了一个分布式的 Key-Value 存储，少数几台机器宕机也能通过原生的 Raft 协议自动把副本补全，可以做到对业务无感知。\n\n## Region\n\n首先，为了便于理解，在此节，假设所有的数据都只有一个副本。前面提到，TiKV 可以看做是一个巨大的有序的 KV Map，那么为了实现存储的水平扩展，数据将被分散在多台机器上。对于一个 KV 系统，将数据分散在多台机器上有两种比较典型的方案：\n\n* Hash：按照 Key 做 Hash，根据 Hash 值选择对应的存储节点。\n* Range：按照 Key 分 Range，某一段连续的 Key 都保存在一个存储节点上。\n\nTiKV 选择了第二种方式，将整个 Key-Value 空间分成很多段，每一段是一系列连续的 Key，将每一段叫做一个 Region，可以用 [StartKey，EndKey) 这样一个左闭右开区间来描述。每个 Region 中保存的数据量默认维持在 256 MiB 左右（可以通过配置修改）。\n\n![Region in TiDB](/media/tidb-storage-2.png)\n\n注意，这里的 Region 还是和 SQL 中的表没什么关系。 这里的讨论依然不涉及 SQL，只和 KV 有关。\n\n将数据划分成 Region 后，TiKV 将会做两件重要的事情：\n\n* 以 Region 为单位，将数据分散在集群中所有的节点上，并且尽量保证每个节点上服务的 Region 数量差不多。\n* 以 Region 为单位做 Raft 的复制和成员管理。\n\n这两点非常重要：\n\n* 先看第一点，数据按照 Key 切分成很多 Region，每个 Region 的数据只会保存在一个节点上面（暂不考虑多副本）。TiDB 系统会有一个组件 (PD) 来负责将 Region 尽可能均匀的散布在集群中所有的节点上，这样一方面实现了存储容量的水平扩展（增加新的节点后，会自动将其他节点上的 Region 调度过来），另一方面也实现了负载均衡（不会出现某个节点有很多数据，其他节点上没什么数据的情况）。同时为了保证上层客户端能够访问所需要的数据，系统中也会有一个组件 (PD) 记录 Region 在节点上面的分布情况，也就是通过任意一个 Key 就能查询到这个 Key 在哪个 Region 中，以及这个 Region 目前在哪个节点上（即 Key 的位置路由信息）。至于负责这两项重要工作的组件 (PD)，会在后续介绍。\n* 对于第二点，TiKV 是以 Region 为单位做数据的复制，也就是一个 Region 的数据会保存多个副本，TiKV 将每一个副本叫做一个 Replica。Replica 之间是通过 Raft 来保持数据的一致，一个 Region 的多个 Replica 会保存在不同的节点上，构成一个 Raft Group。其中一个 Replica 会作为这个 Group 的 Leader，其他的 Replica 作为 Follower。默认情况下，所有的读和写都是通过 Leader 进行，读操作在 Leader 上即可完成，而写操作再由 Leader 复制给 Follower。\n\n大家理解了 Region 之后，应该可以理解下面这张图：\n\n![TiDB Storage](/media/tidb-storage-3.png)\n\n以 Region 为单位做数据的分散和复制，TiKV 就成为了一个分布式的具备一定容灾能力的 KeyValue 系统，不用再担心数据存不下，或者是磁盘故障丢失数据的问题。\n\n## MVCC\n\nTiKV 支持多版本并发控制 (Multi-Version Concurrency Control, MVCC)。假设有这样一种场景：某客户端 A 在写一个 Key，另一个客户端 B 同时在对这个 Key 进行读操作。如果没有数据的多版本控制机制，那么这里的读写操作必然互斥。在分布式场景下，这种情况可能会导致性能问题和死锁问题。有了 MVCC，只要客户端 B 执行的读操作的逻辑时间早于客户端 A，那么客户端 B 就可以在客户端 A 写入的同时正确地读原有的值。即使该 Key 被多个写操作修改过多次，客户端 B 也可以按照其逻辑时间读到旧的值。\n\nTiKV 的 MVCC 是通过在 Key 后面添加版本号来实现的。没有 MVCC 时，可以把 TiKV 看作如下的 Key-Value 对：\n\n```\nKey1 -> Value\nKey2 -> Value\n……\nKeyN -> Value\n```\n\n有了 MVCC 之后，TiKV 的 Key-Value 排列如下：\n\n```\nKey1_Version3 -> Value\nKey1_Version2 -> Value\nKey1_Version1 -> Value\n……\nKey2_Version4 -> Value\nKey2_Version3 -> Value\nKey2_Version2 -> Value\nKey2_Version1 -> Value\n……\nKeyN_Version2 -> Value\nKeyN_Version1 -> Value\n……\n```\n\n注意，对于同一个 Key 的多个版本，版本号较大的会被放在前面，版本号小的会被放在后面（见 [Key-Value](#key-value-pairs键值对) 一节，Key 是有序的排列），这样当用户通过一个 Key + Version 来获取 Value 的时候，可以通过 Key 和 Version 构造出 MVCC 的 Key，也就是 Key_Version。然后可以直接通过 RocksDB 的 SeekPrefix(Key_Version) API，定位到第一个大于等于这个 Key_Version 的位置。\n\n## 分布式 ACID 事务\n\nTiKV 的事务采用的是 Google 在 BigTable 中使用的事务模型：[Percolator](https://research.google.com/pubs/pub36726.html)，TiKV 根据这篇论文实现，并做了大量的优化。详细介绍参见[事务概览](/transaction-overview.md)。\n"
        },
        {
          "name": "tidb-troubleshooting-map.md",
          "type": "blob",
          "size": 42.3759765625,
          "content": "---\ntitle: TiDB 集群问题导图\nsummary: 了解如何处理 TiDB 集群常见问题。\naliases: ['/docs-cn/dev/tidb-troubleshooting-map/','/docs-cn/dev/how-to/troubleshoot/diagnose-map/']\n---\n\n# TiDB 集群问题导图\n\n本篇文档总结了使用 TiDB 及其组件时的常见错误。遇到相关错误时，可以通过本文档的问题导图来排查错误原因并进行处理。\n\n## 1. 服务不可用\n\n### 1.1 客户端报 `Region is Unavailable` 错误\n\n- 1.1.1 `Region is Unavailable` 一般是由于 Region 在一段时间不可用（可能会遇到 `TiKV server is busy`；或者发送给 TiKV 的请求由于 `not leader` 或者 `epoch not match` 等原因被打回；又或者请求 TiKV 超时等），TiDB 内部会进行 `backoff` 重试。`backoff` 的时间超过一定阈值（默认 20s）后就会报错给客户端。如果 `backoff` 在阈值内，客户端对该错误无感知。\n\n- 1.1.2 多台 TiKV 同时内存不足 (OOM)，导致 Region OOM 期间内没有 Leader，见案例 [case-991](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case991.md)。\n\n- 1.1.3 TiKV 报 `TiKV server is busy` 错误，超过 `backoff` 时间，参考 [4.3 客户端报 `server is busy` 错误](#43-客户端报-server-is-busy-错误)。`TiKV server is busy` 属于内部流控机制，后续可能不计入 `backoff` 时间。\n\n- 1.1.4 多台 TiKV 启动不了，导致 Region 没有 Leader。单台物理主机部署多个 TiKV 实例，一个物理机挂掉，由于 label 配置错误导致 Region 没有 Leader，见案例 [case-228](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case228.md)。\n\n- 1.1.5 follower apply 落后，成为 Leader 之后把收到的请求以 `epoch not match` 理由打回，见案例 [case-958](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case958.md)（TiKV 内部需要优化该机制）。\n\n### 1.2 PD 异常导致服务不可用\n\n查看本文档 [5. PD 问题](#5-pd-问题)。\n\n## 2. 延迟明显升高\n\n### 2.1 延迟短暂升高\n\n- 2.1.1 TiDB 执行计划不对导致延迟升高，请参考 [3.3 执行计划不对](#33-执行计划不对)。\n- 2.1.2 PD 出现选举问题或者 OOM 问题，请参考 [5.2 PD 选举问题](#52-pd-选举问题)和 [5.3 PD OOM 问题](#53-pd-oom)。\n- 2.1.3 某些 TiKV 大量掉 Leader，请参考 [4.4 某些 TiKV 大量掉 Leader](#44-某些-tikv-大量掉-leader)。\n- 2.1.4 其他原因，请参考[读写延迟增加](/troubleshoot-cpu-issues.md)。\n\n### 2.2 Latency 持续升高\n\n- 2.2.1 TiKV 单线程瓶颈\n\n    - 单个 TiKV Region 过多，导致单个 gRPC 线程成为瓶颈（查看监控：**Grafana** -> **TiKV-details** -> **Thread CPU**/**gRPC CPU Per Thread**），v3.x 以上版本可以开启 Hibernate Region 特性解决该问题，见案例 [case-612](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case612.md)。\n\n    - v3.0 之前版本 Raftstore 单线程或者 apply 单线程到达瓶颈（查看监控：**Grafana** -> **TiKV-details** -> **Thread CPU**/**raft store CPU** 和 **Async apply CPU** 超过 `80%`）。可以选择扩容 TiKV（v2.x 版本）实例，或者升级到多线程模型的 v3.x 版本。<!-- 见案例 [case-517](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case517.md)。-->\n\n- 2.2.2 CPU load 升高。\n\n- 2.2.3 TiKV 写入慢，请参考 [4.5 TiKV 写入慢](#45-tikv-写入慢)。\n\n- 2.2.4 TiDB 执行计划不对，请参考 [3.3 执行计划不对](#33-执行计划不对)。\n\n- 2.2.5 其他原因，请参考[读写延迟增加](/troubleshoot-cpu-issues.md)。\n\n## 3. TiDB 问题\n\n### 3.1 DDL\n\n- 3.1.1 修改 `decimal` 字段长度时报错 `\"ERROR 1105 (HY000): unsupported modify decimal column precision\"`。<!-- 见案例 [case-1004](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case1004.md)，-->TiDB 暂时不支持修改 `decimal` 字段长度。\n\n- 3.1.2 TiDB DDL job 卡住不动/执行很慢（通过 `admin show ddl jobs` 可以查看 DDL 进度）：\n\n    - 原因 1：TiDB 在 v6.3.0 中引入[元数据锁](/metadata-lock.md)，并在 v6.5.0 及之后的版本默认打开。如果 DDL 涉及的表与当前未提交事务涉及的表存在交集，则会阻塞 DDL 操作，直到事务提交或者回滚。\n\n    - 原因 2：与外部组件 (PD/TiKV) 的网络问题。\n\n    - 原因 3：早期版本（v3.0.8 之前）TiDB 内部自身负载很重（高并发下可能产生了很多协程）。\n\n    - 原因 4：早期版本（v2.1.15 & v3.0.0-rc1 之前）PD 实例删除 TiDB key 无效的问题，会导致每次 DDL 变更都需要等 2 个 lease（很慢）。\n\n    - 其他未知原因，请[上报 bug](https://github.com/pingcap/tidb/issues/new?labels=type%2Fbug&template=bug-report.md)。\n\n    - 解决方法：原因 1 需要检查与外部组件的网络问题；原因 2 和 3 已经修复，需要升级到高版本；其他原因，可选择以下兜底方案进行 DDL owner 迁移。\n\n    - DDL owner 迁移方案：\n\n        - 如果与该 TiDB 集群可以网络互通，执行重新进行 owner 选举命令：`curl -X POST http://{TiDBIP}:10080/ddl/owner/resign`\n\n        - 如果与该 TiDB 集群不可以网络互通，需旁路下线，通过 `tidb-ctl` 工具，从 PD 集群的 etcd 中直接删除 DDL owner，之后也会重新选举：`tidb-ctl etcd delowner [LeaseID] [flags] + ownerKey`\n\n- 3.1.3 TiDB 日志中报 `information schema is changed` 的错误：\n\n    - 报错的详细原因以及解决办法参见[触发 Information schema is changed 错误的原因](/faq/sql-faq.md#触发-information-schema-is-changed-错误的原因)。\n\n    - 背景知识：`schema version` 的增长数量与每个 DDL 变更操作的 `schema state` 个数一致，例如 `create table` 操作会有 1 个版本变更，`add column` 操作会有 4 个版本变更（详情可以参考 [online schema change](https://static.googleusercontent.com/media/research.google.com/zh-CN//pubs/archive/41376.pdf)），所以太多的 column 变更操作会导致 `schema version` 增长得很快。\n\n- 3.1.4 TiDB 日志中报 `information schema is out of date` 的错误：\n\n    - 原因 1：执行 DML 的 TiDB 被 `graceful kill` 后准备退出，且此 DML 对应的事务执行时间超过一个 DDL lease，在事务提交的时候会报此错。\n\n    - 原因 2：TiDB 在执行 DML 时，有一段时间连不上 PD 和 TiKV，导致 TiDB 在一个 DDL Lease（默认 `45s`）内没有加载新的 schema，或者 TiDB 断开与 PD 之间带 `keep alive` 设置的连接。\n\n    - 原因 3：TiKV 压力大或网络超时，通过监控 **Grafana** -> TiDB 和 TiKV 节点的负载情况来确认是否是该原因。\n\n    - 解决方法：第 1 种原因，在 TiDB 启动时手动重试该 DML 即可；第 2 种原因，需要检查 TiDB 实例和 PD 及 TiKV 的网络波动情况；第 3 种原因，需要检查 TiKV 为什么繁忙，参考 [4. TiKV 问题](#4-tikv-问题)。\n\n### 3.2 OOM 问题\n\n- 3.2.1 现象\n\n    - 客户端：客户端收到 TiDB server 报错 `ERROR 2013 (HY000): Lost connection to MySQL server during query`\n\n    - 日志：\n\n        - `dmesg -T | grep tidb-server` 结果中有事故发生附近时间点的 OOM-killer 的日志。\n\n        - tidb.log 中可以 `grep` 到事故发生后附近时间的 `\"Welcome to TiDB\"` 的日志（即 TiDB server 发生重启）。\n\n        - tidb_stderr.log 中能 `grep` 到 `fatal error: \"runtime: out of memory\"` 或 `\"cannot allocate memory\"`。\n\n        - v2.1.8 及其之前的版本，tidb_stderr.log 中能 `grep` 到 `fatal error: stack overflow`。\n\n    - 监控：TiDB server 实例所在机器可用内存迅速回升\n\n- 3.2.2 定位造成 OOM 的 SQL（目前所有版本都无法完成精准定位，需要在发现 SQL 后再做进一步分析，确认 OOM 是否的确由该 SQL 造成）：\n\n    - `> = v3.0.0` 的版本，可以在 tidb.log 中 `grep \"expensive_query\"`，该 log 会记录运行超时、或使用内存超过阈值的 SQL。\n    - `< v3.0.0` 的版本，通过 `grep \"memory exceeds quota\"` 定位运行时内存超限的 SQL。\n\n    > **注意：**\n    >\n    > 单条 SQL 内存阈值的默认值为 `1GB`，可通过系统变量 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 进行设置。\n\n- 3.2.3 缓解 OOM 问题\n\n    - 通过开启 `SWAP` 的方式，可以缓解由于大查询使用内存过多而造成的 OOM 问题。但该方法由于存在 I/O 开销，会在内存空间不足时对大查询性能造成一定影响。性能回退程度受剩余内存量、读写盘速度影响。\n\n- 3.2.4 OOM 常见原因\n\n    - SQL 中包含 join，通过 `explain` 查看发现该 join 选用 `HashJoin` 算法且 `inner` 端的表很大。\n\n    - 单条 `UPDATE`/`DELETE` 涉及的查询数据量太大，见案例 [case-882](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case882.md)。\n\n    - SQL 中包含 `Union` 连接的多条子查询，见案例 [case-1828](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case1828.md)。\n\n更多 OOM 的排查方法，请参考 [TiDB OOM 故障排查](/troubleshoot-tidb-oom.md)。\n\n### 3.3 执行计划不对\n\n- 3.3.1 现象\n\n    - SQL 相比于之前的执行时间有较大程度变慢，执行计划突然发生改变。如果慢日志中输出了执行计划，可以直接对比执行计划。\n\n    - SQL 执行时间相比于其他数据库（例如 MySQL）有较大差距。可以对比其他数据库执行计划，例如 `Join Order` 是否不同。\n\n    - 慢日志中 SQL 执行时间 `Scan Keys` 数目较大。\n\n- 3.3.2 排查执行计划问题\n\n    - `explain analyze {SQL}` 在执行时间可以接受的情况下，对比 `explain analyze` 结果中 `count` 和 execution info 中 `rows` 的数目差距。如果在 `TableScan`/`IndexScan` 行上发现比较大的差距，很大可能是统计信息出问题；如果在其他行上发现较大差距，则也有可能是非统计信息问题。\n\n    - `select count(*)` 在执行计划中包含 `join` 等情况下，explain analyze 可能耗时过长；此时可以通过对 `TableScan`/`IndexScan` 上的条件进行 `select count(*)`，并对比 `explain` 结果中的 `row count` 信息，确定是不是统计信息的问题。\n\n- 3.3.3 缓解问题\n\n    - v3.0 及以上版本可以使用 `SQL Bind` 功能固定执行计划。\n\n    - 更新统计信息。在大致确定问题是由统计信息导致的情况下，先 [dump 统计信息](/statistics.md#导出统计信息)保留现场。如果是由于统计信息过期导致，例如 `show stats_meta` 中 modify count/row count 大于某个值（例如 0.3）或者表中存在时间列的索引情况下，可以先尝试 analyze table 恢复；如果配置了 auto analyze，可以查看系统变量 `tidb_auto_analyze_ratio` 是否过大（例如大于 0.3），以及当前时间是否在 `tidb_auto_analyze_start_time` 和 `tidb_auto_analyze_end_time` 范围内。\n\n    - 其他情况，请[上报 bug](https://github.com/pingcap/tidb/issues/new?labels=type%2Fbug&template=bug-report.md)。\n\n### 3.4 SQL 执行报错\n\n- 3.4.1 客户端报 `ERROR 1265(01000) Data Truncated` 错误。原因是 TiDB 内在计算 `Decimal` 类型处理精度的时候，和 MySQL 不兼容。该错误已于 v3.0.10 中修复 ([#14438](https://github.com/pingcap/tidb/pull/14438))，具体原因如下：\n\n    在 MySQL 内，如果两个大精度 `Decimal` 做除法运算，超出最大小数精度时(`30`)，会只保留 `30` 位且不报错。TiDB 在计算结果上，也是这样实现的，但是在内部表示 `Decimal` 的结构体内，有一个表示小数精度的字段，还是保留的真实精度。\n\n    比如 `(0.1^30) / 10`，TiDB 和 MySQL 的结果都为 0，是正确的，因为精度最多 `30`；但是 TiDB 内表示精度的那个字段，还是 31；\n\n    多次 `Decimal` 除法计算后，虽然结果正确，但是这个精度可能越来越大，最终超过 TiDB 内的另一个阈值 72，此时就会报 `Data Truncated` 的错误；`Decimal` 的乘法计算就不会有这个问题，因为绕过越界，会直接把精度设置为最大精度限制。\n\n    解决方法：可以通过手动加 `Cast(xx as decimal(a, b))` 来绕过这个问题，a 和 b 就是目标的精度。\n\n### 3.5 慢查询问题\n\n要定位慢查询，参阅[慢查询日志](/identify-slow-queries.md)。要处理慢查询，参阅[分析慢查询](/analyze-slow-queries.md)。\n\n### 3.6 热点问题\n\nTiDB 作为分布式数据库，内建负载均衡机制，尽可能将业务负载均匀地分布到不同计算或存储节点上，更好地利用上整体系统资源。然而，机制不是万能的，在一些场景下仍会有部分业务负载不能被很好地分散，影响性能，形成单点的过高负载，也称为热点。\n\nTiDB 提供了完整的方案用于排查、解决或规避这类热点。通过均衡负载热点，可以提升整体性能，包括提高 QPS 和降低延迟等。详情参见 [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md)。\n\n### 3.7 TiDB 磁盘 I/O 过高\n\n当出现系统响应变慢的时候，如果已经排查了 CPU 的瓶颈、数据事务冲突的瓶颈后，问题仍存在，就需要从 I/O 来入手来辅助判断目前的系统瓶颈点。参考 [TiDB 磁盘 I/O 过高的处理办法](/troubleshoot-high-disk-io.md)了解如何定位和处理 TiDB 存储 I/O 过高的问题。\n\n### 3.8 锁冲突问题\n\nTiDB 支持完整的分布式事务，自 v3.0 版本起，提供乐观事务与悲观事务两种事务模式。要了解如何排查锁相关的问题，以及如何处理乐观和悲观锁冲突的问题，请参考 [TiDB 锁冲突问题处理](/troubleshoot-lock-conflicts.md)。\n\n### 3.9 数据索引一致性报错\n\n当执行事务或执行 `ADMIN CHECK [TABLE|INDEX]` 命令时，TiDB 会对数据索引的一致性进行检查。如果检查发现 record key-value 和 index key-value 不一致，即存储行数据的键值对和存储其对应索引的键值对之间不一致（例如多索引或缺索引），TiDB 会报数据索引一致性错误，并在日志文件中打印相关错误日志。\n\n要了解更多数据索引一致性报错信息以及如何绕过检查，请参考[数据索引一致性报错](/troubleshoot-data-inconsistency-errors.md)。\n\n## 4. TiKV 问题\n\n### 4.1 TiKV panic 启动不了\n\n- 4.1.1 `sync-log = false`，机器断电之后出现 `unexpected raft log index: last_index X < applied_index Y` 错误。符合预期，需通过 `tikv-ctl` 工具恢复 Region。\n\n- 4.1.2 虚拟机部署 TiKV，`kill` 虚拟机或物理机断电，出现 `entries[X, Y] is unavailable from storage` 错误。符合预期，虚拟机的 fsync 不可靠，需通过 `tikv-ctl` 工具恢复 Region。\n\n- 4.1.3 其他原因（非预期，[需报 bug](https://github.com/tikv/tikv/issues/new?template=bug-report.md)）。\n\n### 4.2 TiKV OOM\n\n- 4.2.1 `block-cache` 配置太大导致 OOM：\n\n    - 在监控 **Grafana** -> **TiKV-details** 选中对应的 instance 后，查看 RocksDB 的 `block cache size` 监控来确认是否是该问题。\n\n    - 同时，请检查 `[storage.block-cache] capacity = # \"1GB\"` 参数是否设置合理，默认情况下 TiKV 的 `block-cache` 设置为机器总内存的 `45%`；在 container 部署时，需要显式指定该参数，因为 TiKV 获取的是物理机的内存，可能会超出 container 的内存限制。\n\n- 4.2.2 Coprocessor 收到大量大查询，返回的数据量太大，gRPC 的发送速度跟不上 Coprocessor 往外输出数据的速度，导致 OOM：\n\n    - 可以通过检查监控：**Grafana** -> **TiKV-details** -> **coprocessor overview** 的 `response size` 是否超过 `network outbound` 流量来确认是否属于这种情况。\n\n- 4.2.3 其他部分占用太多内存（非预期，[需报 bug](https://github.com/tikv/tikv/issues/new?template=bug-report.md)）。\n\n### 4.3 客户端报 `server is busy` 错误\n\n通过查看监控：**Grafana** -> **TiKV** -> **errors** 确认具体 busy 原因。`server is busy` 是 TiKV 自身的流控机制，TiKV 通过这种方式告知 `tidb/ti-client` 当前 TiKV 的压力过大，稍后再尝试。\n\n- 4.3.1 TiKV RocksDB 出现 `write stall`。一个 TiKV 包含两个 RocksDB 实例，一个用于存储 Raft 日志，位于 `data/raft`。另一个用于存储真正的数据，位于 `data/db`。通过 `grep \"Stalling\" RocksDB` 日志可以查看 stall 的具体原因，RocksDB 日志是 `LOG` 开头的文件，`LOG` 为当前日志。`write stall` 是一个 RocksDB 原生内建的性能降级机制。当 RocksDB 发生 `write stall` 时，系统的整体性能会急剧下降。在 v5.2.0 之前，当发生 `write stall` 时，TiDB 通过直接给客户端返回 `ServerIsBusy` 错误来阻挡所有的写请求，但这容易导致 QPS 性能急剧下降。自 v5.2.0 起，TiKV 引入了新的流控机制，通过前置在调度层实现动态延迟写请求来抑制写入，以替代之前遇到 `write stall` 时就给客户端返回 `server is busy` 来抑制写入的机制。新的流控机制默认配置开启，TiKV 会自动关闭 `KvDB` 和 `RaftDB` (memtable 除外) 的 `write stall` 机制。但是，当 pending 的请求量超过一定阈值时，流控机制仍然会生效，开始拒绝部分或所有的写入请求，并返回客户端 `server is busy` 报错，表现如下。详细的说明和阈值可参考[流控配置说明](/tikv-configuration-file.md#storageflow-control)。\n\n    - 如果 pending compaction bytes 太多触发 `server is busy` 报错，可以通过调大 [`soft-pending-compaction-bytes-limit`](/tikv-configuration-file.md#soft-pending-compaction-bytes-limit) 和 [`hard-pending-compaction-bytes-limit`](/tikv-configuration-file.md#hard-pending-compaction-bytes-limit) 参数的值来缓解。\n\n        - 如果 pending compaction bytes 达到 `soft-pending-compaction-bytes-limit` 参数的值（默认为 `192GiB`），流控就会开始拒绝一部分的写请求（通过给客户端返回 `ServerIsBusy`）。此时，可以调大该参数的值，例如，`[storage.flow-control] soft-pending-compaction-bytes-limit = \"384GiB\"`。\n\n        - 如果 pending compaction bytes 达到 `hard-pending-compaction-bytes-limit` 参数的值（默认为 `1024GiB`），流控就会开始拒绝所有的写请求（通过给客户端返回 `ServerIsBusy`）。通常不太可能触发该情况，因为在达到 `soft-pending-compaction-bytes-limit` 的阈值之后，流控机制就会介入而放慢写入速度。如果触发，可以调大该参数的值，例如，`[storage.flow-control] hard-pending-compaction-bytes-limit = \"2048GiB\"`。\n\n        - 如果磁盘 IO 能力持续跟不上写入，建议扩容。如果磁盘的吞吐达到了上限（例如 SATA SSD 的吞吐相对 NVME SSD 会低很多）导致 write stall，但是 CPU 资源又比较充足，可以尝试采用压缩率更高的压缩算法来缓解磁盘的压力，用 CPU 资源换磁盘资源。\n\n        - 比如 default cf compaction 压力比较大，调整参数 `[rocksdb.defaultcf] compression-per-level = [\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]` 改成 `compression-per-level = [\"no\", \"no\", \"zstd\", \"zstd\", \"zstd\", \"zstd\", \"zstd\"]`。\n\n    - memtable 太多导致 stall。该问题一般发生在瞬间写入量比较大，并且 memtable flush 到磁盘的速度比较慢的情况下。如果磁盘写入速度不能改善，并且只有业务峰值会出现这种情况，可以通过调大对应 CF 的 `max-write-buffer-number` 来缓解：\n\n        - 例如 `[rocksdb.defaultcf] max-write-buffer-number = 8` （默认值 `5`），同时请求注意在高峰期可能会占用更多的内存，因为可能存在于内存中的 memtable 会更多。\n\n- 4.3.2 `scheduler too busy`\n\n    - 写入冲突严重，`latch wait duration` 比较高，查看监控：**Grafana** -> **TiKV-details** -> **scheduler prewrite** 或者 **scheduler commit** 的 `latch wait duration`。scheduler 写入任务堆积，导致超过了 `[storage] scheduler-pending-write-threshold = \"100MB\"` 设置的阈值。可通过查看 `MVCC_CONFLICT_COUNTER` 对应的 metric 来确认是否属于该情况。\n    - 写入慢导致写入堆积，该 TiKV 正在写入的数据超过了 `[storage] scheduler-pending-write-threshold = \"100MB\"` 设置的阈值。请参考 [4.5 TiKV 写入慢](#45-tikv-写入慢)。\n\n- 4.3.3 `raftstore is busy`，主要是消息的处理速度没有跟上接收消息的速度。短时间的 `channel full` 不会影响服务，长时间持续出现该错误可能会导致 Leader 切换走。\n\n    - `append log` 遇到了 stall，参考 [4.3.1 客户端报 `server is busy` 错误](#43-客户端报-server-is-busy-错误)。\n    - `append log duration` 比较高，导致处理消息不及时，可以参考 [4.5 TiKV 写入慢](#45-tikv-写入慢)分析为什么 `append log duration` 比较高。\n    - 瞬间收到大量消息（查看 TiKV Raft messages 面板），Raftstore 没处理过来，通常情况下短时间的 `channel full` 不会影响服务。\n\n- 4.3.4 TiKV Coprocessor 排队，任务堆积超过了 `Coprocessor 线程数 * readpool.coprocessor.max-tasks-per-worker-[normal|low|high]`。大量大查询导致 Coprocessor 出现了堆积情况，需要确认是否由于执行计划变化而导致了大量扫表操作，请参考 [3.3 执行计划不对](#33-执行计划不对)。\n\n### 4.4 某些 TiKV 大量掉 Leader\n\n- 4.4.1 TiKV 重启，导致重新选举。\n\n    - TiKV `panic` 之后又被 systemd 重新拉起正常运行，可以通过查看 TiKV 的日志来确认是否有 `panic`，这种情况属于非预期，[需报 bug](https://github.com/tikv/tikv/issues/new?template=bug-report.md)。\n    - 被第三者 `stop/kill`，被 systemd 重新拉起。查看 `dmesg` 和 `TiKV log` 确认原因。\n    - TiKV 发生 OOM 导致重启了，参考 [4.2 TiKV OOM 问题](#42-tikv-oom)。\n    - 动态调整 `THP` 导致 hung 住，见案例 [case-500](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case500.md)。\n\n- 4.4.2 查看监控：**Grafana** -> **TiKV-details** -> **errors** 面板 `server is busy`，看到 TiKV RocksDB 出现 write stall 导致发生重新选举，请参考 [4.3.1](#43-客户端报-server-is-busy-错误)。\n\n- 4.4.3 网络隔离导致重新选举。\n\n### 4.5 TiKV 写入慢\n\n- 4.5.1 通过查看 TiKV gRPC 的 `prewrite`/`commit`/`raw-put`（仅限 raw kv 集群）duration 确认确实是 TiKV 写入慢了。通常情况下可以按照 [performance-map](https://github.com/pingcap/tidb-map/blob/master/maps/performance-map.png) 来定位到底哪个阶段慢了，下面列出几种常见的情况。\n\n- 4.5.2 scheduler CPU 繁忙（仅限 transaction kv）。prewrite/commit 的 `scheduler command duration` 比 `scheduler latch wait duration` + `storage async write duration` 更长，并且 scheduler worker CPU 比较高，例如超过 `scheduler-worker-pool-size` * 100% 的 80%，并且或者整个机器的 CPU 资源比较紧张。如果写入量很大，确认下是否 `[storage] scheduler-worker-pool-size` 配置得太小。其他情况，[需报 bug](https://github.com/tikv/tikv/issues/new?template=bug-report.md)。\n\n- 4.5.3 Append log 慢。TiKV Grafana 的 **Raft IO**/`append log duration` 比较高，通常情况下是由于写盘慢了，可以检查 RocksDB - Raft 的 `WAL Sync Duration max` 值来确认，否则可能[需要报 bug](https://github.com/tikv/tikv/issues/new?template=bug-report.md)。\n\n- 4.5.4 Raftstore 线程繁忙。TiKV Grafana 的 **Raft Propose**/`propose wait duration` 明显高于 `append log duration`。请查看以下情况：\n\n    - `[raftstore] store-pool-size` 配置是否过小（该值建议在 [1,5] 之间，不建议太大）。\n    - 机器的 CPU 是不是不够。\n\n- 4.5.5 apply 慢了。TiKV Grafana 的 **Raft IO**/`apply log duration` 比较高，通常会伴随着 **Raft Propose**/`apply wait duration` 比较高。可能是以下原因引起的：\n\n    - `[raftstore] apply-pool-size` 配置过小（建议在 [1, 5] 之间，不建议太大），**Thread CPU**/`apply cpu` 比较高；\n    - 机器的 CPU 资源不够了；\n    - Region 写入热点问题，单个 apply 线程 CPU 使用率比较高（通过修改 Grafana 表达式，加上 `by (instance, name)` 来看各个线程的 CPU 使用情况），暂时对于单个 Region 的热点写入没有很好的方式，最近在优化该场景；\n    - 写 RocksDB 比较慢，**RocksDB kv**/`max write duration` 比较高（单个 Raft log 可能包含很多个 kv，写 RocksDB 的时候会把 128 个 kv 放在一个 write batch 写入到 RocksDB，所以一次 apply log 可能涉及到多次 RocksDB 的 write）；\n    - 其他情况，[需报 bug](https://github.com/tikv/tikv/issues/new?template=bug-report.md)。\n\n- 4.5.6 Raft commit log 慢了。\n\n    - TiKV Grafana 的 **Raft IO**/`commit log duration` 比较高（4.x 版本的 Grafana 才有该 metric）。每个 Region 对应一个独立的 Raft group，Raft 本身是有流控机制的，类似 TCP 的滑动窗口机制，通过参数 `[raftstore] raft-max-inflight-msgs = 256` 来控制滑动窗口的大小，如果有热点写入并且 `commit log duration` 比较高可以适度调大该参数，比如 1024。\n\n- 4.5.7 其他情况，请参考 [Performance Map](https://github.com/pingcap/tidb-map/blob/master/maps/performance-map.png) 上的写入路径来分析。\n\n## 5. PD 问题\n\n### 5.1 PD 调度问题\n\n- 5.1.1 merge 问题：\n\n    - 跨表空 Region 无法 merge，需要修改 TiKV 的 `[coprocessor] split-region-on-table = false` 参数来解决，4.x 版本该参数默认为 false，见案例 [case-896](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case896.md)。\n\n    - Region merge 慢，可检查监控 **Grafana** -> **PD** -> **operator** 面板是否有 merge 的 operator 产生，可以适当调大 `merge-schedule-limit` 参数来加速 merge。\n\n- 5.1.2 补副本/上下线问题：\n\n    - TiKV 磁盘使用 `80%` 容量，PD 不会进行补副本操作，miss peer 数量上升，需要扩容 TiKV，见案例 [case-801](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case801.md)。\n\n    - 下线 TiKV，有 Region 长时间迁移不走。v3.0.4 版本已经修复该问题，见 [#5526](https://github.com/tikv/tikv/pull/5526) 和案例 [case-870](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case870.md)。\n\n- 5.1.3 Balance 问题：\n\n    - Leader/Region count 分布不均，见案例 [case-394](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case394.md), [case-759](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case759.md)。主要原因是 balance 是依赖 Region/leader 的 size 去调度的，所以可能会造成 count 数量的不均衡，v4.0 新增了一个参数 `[leader-schedule-policy]`，可以调整 Leader 的调度策略，根据 \"count\" 或者是 \"size\" 进行调度。\n\n### 5.2 PD 选举问题\n\n- 5.2.1 PD 发生 Leader 切换：\n\n    - 磁盘问题，PD 所在的节点 I/O 被打满，排查是否有其他 I/O 高的组件与 PD 混部以及盘的健康情况，可通过监控 **Grafana** -> **disk performance** -> **latency** 和 **load** 等指标进行验证，必要时可以使用 fio 工具对盘进行检测，见案例 [case-292](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case292.md)。\n\n    - 网络问题，PD 日志中有 `lost the TCP streaming connection`，排查 PD 之间网络是否有问题，可通过监控 **Grafana** -> **PD** -> **etcd** 的 `round trip` 来验证，见案例 [case-177](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case177.md)。\n\n    - 系统 load 高，日志中能看到 `server is likely overloaded`，见案例 [case-214](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case214.md)。\n\n- 5.2.2 PD 选不出 Leader 或者选举慢：\n\n    - 选不出 Leader，PD 日志中有 `lease is not expired`，见 [#10355](https://github.com/etcd-io/etcd/issues/10355)。v3.0.x 版本和 v2.1.19 版本已修复该问题，见案例 [case-875](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case875.md)。\n\n    - 选举慢，Region 加载时间长。从 PD 日志中 `grep \"regions cost\"`（例如日志中可能是 \"load 460927 regions cost 11.77099s\"），如果出现秒级，则说明较慢，v3.0 版本可开启 Region storage（设置 `use-region-storage` 为 `true`），该特性能极大缩短加载 Region 的时间，见案例 [case-429](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case429.md)。\n\n- 5.2.3 TiDB 执行 SQL 时报 PD timeout：\n\n    - PD 没 Leader 或者有切换，参考 [5.2.1 PD 选举问题](#52-pd-选举问题)和 [5.2.2 PD 选举问题](#52-pd-选举问题)。\n\n    - 网络问题，排查网络相关情况。通过监控 **Grafana** -> **blackbox_exporter** -> **ping latency** 确定 TiDB 到 PD Leader 的网络是否正常。\n\n    - PD panic，[需报 bug](https://github.com/pingcap/pd/issues/new?labels=kind%2Fbug&template=bug-report.md)。\n\n    - PD OOM，参考 [5.3 PD OOM 问题](#53-pd-oom)。\n\n    - 其他原因，通过 `curl http://127.0.0.1:2379/debug/pprof/goroutine?debug=2` 抓 goroutine，[报 bug](https://github.com/pingcap/pd/issues/new?labels=kind%2Fbug&template=bug-report.md)。\n\n- 5.2.4 其他问题\n\n    - PD 报 `FATAL` 错误，日志中有 `range failed to find revision pair`，v3.0.8 已经修复该问题，见 [#2040](https://github.com/pingcap/pd/pull/2040)。详情参考案例 [case-947](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case947.md)。\n\n    - 其他原因，[需报 bug](https://github.com/pingcap/pd/issues/new?labels=kind%2Fbug&template=bug-report.md)。\n\n### 5.3 PD OOM\n\n- 5.3.1 使用 `/api/v1/regions` 接口时 Region 数量过多，可能会导致 PD OOM，在 v3.0.8 版本中修复，见 [#1986](https://github.com/pingcap/pd/pull/1986)。\n\n- 5.3.2 滚动升级的时候 PD OOM，gRPC 消息大小没限制，监控可看到 TCP InSegs 较大，在 v3.0.6 版本中修复，见 [#1952](https://github.com/pingcap/pd/pull/1952)。<!-- 详情请参考案例 [case-852](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case852.md)。-->\n\n### 5.4 Grafana 显示问题\n\n- 5.4.1 监控 **Grafana** -> **PD** -> **cluster** -> **role** 显示 follower，Grafana 表达式问题，在 v3.0.8 版本修复。\n\n## 6. 生态 Tools 问题\n\n### 6.1 DM 问题\n\n- 6.1.1 TiDB Data Migration (DM) 是能将 MySQL/MariaDB 的数据迁移到 TiDB 的迁移工具，详情见 [DM 简介](/dm/dm-overview.md)。\n\n- 6.1.2 执行 `query-status` 或查看日志时出现 `Access denied for user 'root'@'172.31.43.27' (using password: YES)`。\n\n    - 在所有 DM 配置文件中，数据库相关的密码都必须使用经 dmctl 加密后的密文（若数据库密码为空，则无需加密）。在 v1.0.6 及以后的版本可使用明文密码。\n\n    - 在 DM 运行过程中，上下游数据库的用户必须具备相应的读写权限。在启动同步任务过程中，DM 会自动进行[相应权限的检查](/dm/dm-precheck.md)。\n\n    - 同一套 DM 集群，混合部署不同版本的 DM-worker/DM-master/dmctl，见案例 [AskTUG-1049](https://asktug.com/t/dm1-0-0-ga-access-denied-for-user/1049/5)。\n\n- 6.1.3 DM 同步任务中断并包含 `driver: bad connection` 错误。\n\n    - 发生 `driver: bad connection` 错误时，通常表示 DM 到下游 TiDB 的数据库连接出现了异常（如网络故障、TiDB 重启等）且当前请求的数据暂时未能发送到 TiDB。\n\n        - 1.0.0 GA 之前的版本，DM 发生该类型错误时，需要先使用 `stop-task` 命令停止任务后再使用 `start-task` 命令重启任务。\n\n        - 1.0.0 GA 版本，增加对此类错误的自动重试机制，见 [#265](https://github.com/pingcap/dm/pull/265)。\n\n- 6.1.4 同步任务中断并包含 `invalid connection` 错误。\n\n    - 发生 `invalid connection` 错误时，通常表示 DM 到下游 TiDB 的数据库连接出现了异常（如网络故障、TiDB 重启、TiKV busy 等）且当前请求已有部分数据发送到了 TiDB。由于 DM 中存在同步任务并发向下游复制数据的特性，因此在任务中断时可能同时包含多个错误（可通过 `query-status` 或 `query-error` 查询当前错误）：\n\n        - 如果错误中仅包含 `invalid connection` 类型的错误，且当前处于增量复制阶段，则 DM 会自动进行重试。\n\n        - 如果 DM 由于版本问题（v1.0.0-rc.1 后引入自动重试）等未自动进行重试或自动重试未能成功，则可尝试先使用 `stop-task` 停止任务，然后再使用 `start-task` 重启任务。\n\n- 6.1.5 Relay 处理单元报错 `event from * in * diff from passed-in event *` 或同步任务中断并包含 `get binlog error ERROR 1236 (HY000)`、`binlog checksum mismatch, data may be corrupted` 等 binlog 获取或解析失败错误。\n\n    - 在 DM 进行 relay log 拉取与增量同步过程中，如果遇到了上游超过 4 GB 的 binlog 文件，就可能出现这两个错误。原因是 DM 在写 relay log 时需要依据 binlog position 及文件大小对 event 进行验证，且需要保存同步的 binlog position 信息作为 checkpoint。但是 MySQL binlog position 官方定义使用 uint32 存储，所以超过 4 GB 部分的 binlog position 的 offset 值会溢出，进而出现上面的错误。\n\n        - 对于 relay 处理单元，可通过官网步骤进行[手动处理](/dm/dm-error-handling.md)。\n\n        - 对于 binlog replication 处理单元，可通过官网步骤进行[手动处理](/dm/dm-error-handling.md)。\n\n- 6.1.6 DM 同步中断，日志报错 `ERROR 1236 (HY000) The slave is connecting using CHANGE MASTER TO MASTER_AUTO_POSITION = 1, but the master has purged binary logs containing GTIDs that the slave requires.`。\n\n    - 检查 master 的 binlog 是否被 purge。\n\n    - 检查 relay.meta 中记录的位点信息。\n\n        - relay.meta 中记录空的 GTID 信息，DM-worker 进程在退出时、以及定时 (30s) 会把内存中的 GTID 信息保存到 relay.meta 中，在没有获取到上游 GTID 信息的情况下，把空的 GTID 信息保存到了 relay.meta 中。见案例 [case-772](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case772.md)。\n\n        - relay.meta 中记录的 binlog event 不完整触发 recover 流程后记录错误的 GTID 信息，该问题可能会在 1.0.2 之前的版本遇到，已在 1.0.2 版本修复。<!-- 见案例 [case-764](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case764.md)。-->\n\n- 6.1.7 DM 同步报错 `Error 1366: incorrect utf8 value eda0bdedb29d(\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd\\ufffd)`。\n\n    - 该值 MySQL 8.0 和 TiDB 都不能写入成功，但是 MySQL 5.7 可以写入成功。可以开启 TiDB 动态参数 `tidb_skip_utf8_check` 参数，跳过数据格式检查。\n\n### 6.2 TiDB Lightning 问题\n\n- 6.2.1 TiDB Lightning 是快速的全量数据导入工具，见 [TiDB Lightning on GitHub](https://github.com/pingcap/tidb/tree/master/lightning)。\n\n- 6.2.2 导入速度太慢。\n\n    - `region-concurrency` 设定太高，线程间争用资源反而减低了效率。排查方法如下：\n\n        - 从日志的开头搜寻 `region-concurrency` 能知道 TiDB Lightning 读到的参数是多少；\n        - 如果 TiDB Lightning 与其他服务（如 TiKV Importer）共用一台服务器，必需手动将 `region-concurrency` 设为该服务器 CPU 数量的 `75%`；\n        - 如果 CPU 设有限额（例如从 Kubernetes 指定的上限），TiDB Lightning 可能无法自动判断出来，此时亦需要手动调整 `region-concurrency`。\n\n    - 表结构太复杂。每条索引都会额外增加 KV 对，如果有 N 条索引，实际导入的大小就差不多是 [Dumpling](/dumpling-overview.md) 文件的 N+1 倍。如果索引不太重要，可以考虑先从 schema 去掉，待导入完成后再使用 `CREATE INDEX` 加回去。\n\n    - TiDB Lightning 版本太旧。尝试使用最新的版本，可能会有改善。\n\n- 6.2.3 `checksum failed: checksum mismatched remote vs local`\n\n    - 原因 1：这张表可能本身已有数据，影响最终结果。\n\n    - 原因 2：如果目标数据库的校验和全是 0，表示没有发生任何导入，有可能是集群太忙无法接收任何数据。\n\n    - 原因 3：如果数据源是由机器生成而不是从 [Dumpling](/dumpling-overview.md) 备份的，需确保数据符合表的限制。例如：\n\n        - 自增 (AUTO_INCREMENT) 的列需要为正数，不能为 0。\n        - 单一键和主键 (UNIQUE and PRIMARY KEYs) 不能有重复的值。\n\n    - 解决办法：参考[官网步骤处理](/tidb-lightning/troubleshoot-tidb-lightning.md#checksum-failed-checksum-mismatched-remote-vs-local)。\n\n- 6.2.4 `Checkpoint for … has invalid status:(错误码)`\n\n    - 原因：断点续传已启用。TiDB Lightning 或 TiKV Importer 之前发生了异常退出。为了防止数据意外损坏，TiDB Lightning 在错误解决以前不会启动。错误码是小于 25 的整数，可能的取值是 0、3、6、9、12、14、15、17、18、20、21。整数越大，表示异常退出所发生的步骤在导入流程中越晚。\n\n    - 解决办法：参考[官网步骤](/tidb-lightning/troubleshoot-tidb-lightning.md#checkpoint-for--has-invalid-status错误码)处理。\n\n- 6.2.5 `cannot guess encoding for input file, please convert to UTF-8 manually`\n\n    - 原因：TiDB Lightning 只支持 UTF-8 和 GB-18030 编码的表架构。此错误代表数据源不是这里任一个编码。也有可能是文件中混合了不同的编码，例如在不同的环境运行过 `ALTER TABLE`，使表架构同时出现 UTF-8 和 GB-18030 的字符。\n\n    - 解决办法：参考[官网步骤](/tidb-lightning/troubleshoot-tidb-lightning.md#cannot-guess-encoding-for-input-file-please-convert-to-utf-8-manually)处理。\n\n- 6.2.6 `[sql2kv] sql encode error = [types:1292]invalid time format: '{1970 1 1 0 45 0 0}'`\n\n    - 原因：一个 timestamp 类型的时间戳记录了不存在的时间值。时间值不存在是由于夏令时切换或超出支持的范围（1970 年 1 月 1 日至 2038 年 1 月 19 日）。\n\n    - 解决办法：参考[官网步骤](/tidb-lightning/troubleshoot-tidb-lightning.md#sql2kv-sql-encode-error--types1292invalid-time-format-1970-1-1-)处理。\n\n## 7. 常见日志分析\n\n### 7.1 TiDB\n\n- 7.1.1 `GC life time is shorter than transaction duration`。事务执行时间太长，超过了 GC lifetime（默认为 10 分钟），可以通过修改系统变量 [`tidb_gc_life_time`](/system-variables.md#tidb_gc_life_time-从-v50-版本开始引入) 来延长 life time，通常情况下不建议修改，因为延长时限可能导致大量老版本数据的堆积（如果有大量 `UPDATE` 和 `DELETE` 语句）。\n\n- 7.1.2 `txn takes too much time`。事务太长时间（超过 590s）没有提交，准备提交的时候报该错误。可以通过调大 `[tikv-client] max-txn-time-use = 590` 参数，以及调大 `GC life time` 来绕过该问题（如果确实有这个需求）。通常情况下，建议看看业务是否真的需要执行这么长时间的事务。\n\n- 7.1.3 coprocessor.go 报 `request outdated`。发往 TiKV 的 Coprocessor 请求在 TiKV 端排队时间超过了 60s，直接返回该错误。需要排查 TiKV Coprocessor 为什么排队这么严重。\n\n- 7.1.4 region_cache.go 大量报 `switch region peer to next due to send request fail` 且 error 信息是 `context deadline exceeded`。请求 TiKV 超时触发 region cache 切换请求到其他节点，可以对日志中的 addr 字段继续 `grep \"<addr> cancelled\"`，根据 grep 结果：\n\n    - `send request is cancelled`。请求发送阶段超时，可以排查监控 **Grafana** -> **TiDB** -> **Batch Client**/`Pending Request Count by TiKV` 是否大于 128，确定是否因发送远超 KV 处理能力导致发送堆积。如果 Pending Request 不多，需要排查日志确认是否因为对应 KV 有运维变更，导致短暂报出；否则非预期，[需报 bug](https://github.com/pingcap/tidb/issues/new?labels=type%2Fbug&template=bug-report.md)。\n\n    - `wait response is cancelled`。请求发送到 TiKV 后超时未收到 TiKV 响应。需要排查对应地址 TiKV 的响应时间和对应 Region 在当时的 PD 和 KV 日志，确定为什么 KV 未及时响应。\n\n- 7.1.5 distsql.go 报 `inconsistent index`。数据索引疑似发生不一致，首先对报错的信息中 index 所在表执行 `admin check table <TableName>` 命令，如果检查失败，则先通过以下命令禁用 GC，然后[报 bug](https://github.com/pingcap/tidb/issues/new?labels=type%2Fbug&template=bug-report.md)。\n\n    ```sql\n    SET GLOBAL tidb_gc_enable = 0;\n    ```\n\n### 7.2 TiKV\n\n- 7.2.1 `key is locked` 读写冲突，读请求碰到还未提交的数据，需要等待其提交之后才能读。少量这个错误对业务无影响，大量出现这个错误说明业务读写冲突比较严重。\n\n- 7.2.2 `write conflict` 乐观事务中的写写冲突，同时多个事务对相同的 key 进行修改，只有一个事务会成功，其他事务会自动重取 timestamp 然后进行重试，不影响业务。如果业务冲突很严重可能会导致重试多次之后事务失败，这种情况下建议使用悲观锁。报错以及解决方法详情，参考[乐观事务模型下写写冲突问题排查](/troubleshoot-write-conflicts.md)。\n\n- 7.2.3 `TxnLockNotFound` 事务提交太慢，过了 TTL (Time To Live) 时间之后被其他事务回滚了，该事务会自动重试，通常情况下对业务无感知。对于 0.25 MB 以内的小事务，TTL 默认时间为 3 秒。详情参见[锁被清除 (LockNotFound) 错误](/troubleshoot-lock-conflicts.md#锁被清除-locknotfound-错误)。\n\n- 7.2.4 `PessimisticLockNotFound` 类似 `TxnLockNotFound`，悲观事务提交太慢被其他事务回滚了。\n\n- 7.2.5 `stale_epoch` 请求的 epoch 太旧了，TiDB 会更新路由之后再重新发送请求，业务无感知。epoch 在 Region 发生 split/merge 以及迁移副本的时候会变化。\n\n- 7.2.6 `peer is not leader` 请求发到了非 Leader 的副本上，TiDB 会根据该错误更新本地路由（如果错误 response 里携带了最新 Leader 是哪个副本这一信息），并且重新发送请求到最新 Leader，一般情况下业务无感知。在 v3.0 后 TiDB 在原 Leader 请求失败时会尝试其他 peer，也会导致 TiKV 频繁出现 `not leader` 日志，可以通过查看 TiDB 对应 Region 的 `switch region peer to next due to send request fail` 日志，排查发送失败根本原因，参考 [7.1.4 TiDB](#71-tidb)。另外也可能是由于其他原因导致一些 Region 一直没有 Leader，请参考 [4.4 某些 TiKV 大量掉 Leader](#44-某些-tikv-大量掉-leader)。\n"
        },
        {
          "name": "tiflash-deployment-topology.md",
          "type": "blob",
          "size": 8.306640625,
          "content": "---\ntitle: TiFlash 部署拓扑\nsummary: 了解在部署最小拓扑集群的基础上，部署 TiFlash 的拓扑结构。\naliases: ['/docs-cn/dev/tiflash-deployment-topology/']\n---\n\n# TiFlash 部署拓扑\n\n本文介绍在部署最小拓扑集群的基础上，部署 [TiFlash](/tiflash/tiflash-overview.md) 的拓扑结构。TiFlash 是列式的存储引擎，已经成为集群拓扑的标配，适合 Real-Time HTAP 业务。\n\n## 拓扑信息\n\n|实例 | 个数 | 物理机配置 | IP |配置 |\n| :-- | :-- | :-- | :-- | :-- |\n| TiDB |3 | 16 VCore 32GB * 1 | 10.0.1.7 <br/> 10.0.1.8 <br/> 10.0.1.9 | 默认端口 <br/>  全局目录配置 |\n| PD | 3 | 4 VCore 8GB * 1 |10.0.1.4 <br/> 10.0.1.5 <br/> 10.0.1.6 | 默认端口 <br/> 全局目录配置 |\n| TiKV | 3 | 16 VCore 32GB 2TB (nvme ssd) * 1 | 10.0.1.1 <br/> 10.0.1.2 <br/> 10.0.1.3 | 默认端口 <br/> 全局目录配置 |\n| TiFlash | 1 | 32 VCore 64 GB 2TB (nvme ssd) * 1  | 10.0.1.11 | 默认端口 <br/> 全局目录配置 |\n| Monitoring & Grafana | 1 | 4 VCore 8GB * 1 500GB (ssd) | 10.0.1.10 | 默认端口 <br/> 全局目录配置 |\n\n### 拓扑模版\n\n<details>\n<summary>简单 TiFlash 配置模版</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\nserver_configs:\n  pd:\n    replication.enable-placement-rules: true\n\npd_servers:\n  - host: 10.0.1.4\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.7\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\ntikv_servers:\n  - host: 10.0.1.1\n  - host: 10.0.1.2\n  - host: 10.0.1.3\n\ntiflash_servers:\n  - host: 10.0.1.11\n    data_dir: /tidb-data/tiflash-9000\n    deploy_dir: /tidb-deploy/tiflash-9000\n\nmonitoring_servers:\n  - host: 10.0.1.10\n\ngrafana_servers:\n  - host: 10.0.1.10\n\nalertmanager_servers:\n  - host: 10.0.1.10\n```\n\n</details>\n\n<details>\n<summary>详细 TiFlash 配置模版</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\n# # Monitored variables are applied to all the machines.\nmonitored:\n  node_exporter_port: 9100\n  blackbox_exporter_port: 9115\n  # deploy_dir: \"/tidb-deploy/monitored-9100\"\n  # data_dir: \"/tidb-data/monitored-9100\"\n  # log_dir: \"/tidb-deploy/monitored-9100/log\"\n\n# # Server configs are used to specify the runtime configuration of TiDB components.\n# # All configuration items can be found in TiDB docs:\n# # - TiDB: https://docs.pingcap.com/zh/tidb/stable/tidb-configuration-file\n# # - TiKV: https://docs.pingcap.com/zh/tidb/stable/tikv-configuration-file\n# # - PD: https://docs.pingcap.com/zh/tidb/stable/pd-configuration-file\n# # All configuration items use points to represent the hierarchy, e.g:\n# #   readpool.storage.use-unified-pool\n# #\n# # You can overwrite this configuration via the instance-level `config` field.\n\nserver_configs:\n  tidb:\n    log.slow-threshold: 300\n  tikv:\n    # server.grpc-concurrency: 4\n    # raftstore.apply-pool-size: 2\n    # raftstore.store-pool-size: 2\n    # rocksdb.max-sub-compactions: 1\n    # storage.block-cache.capacity: \"16GB\"\n    # readpool.unified.max-thread-count: 12\n    readpool.storage.use-unified-pool: false\n    readpool.coprocessor.use-unified-pool: true\n  pd:\n    schedule.leader-schedule-limit: 4\n    schedule.region-schedule-limit: 2048\n    schedule.replica-schedule-limit: 64\n    replication.enable-placement-rules: true\n  tiflash:\n    # Maximum memory usage for processing a single query. Zero means unlimited.\n    profiles.default.max_memory_usage: 0\n    # Maximum memory usage for processing all concurrently running queries on the server. Zero means unlimited.\n    profiles.default.max_memory_usage_for_all_queries: 0\n  tiflash-learner:\n    # The allowable number of threads in the pool that flushes Raft data to storage.\n    raftstore.apply-pool-size: 4\n    # The allowable number of threads that process Raft, which is the size of the Raftstore thread pool.\n    raftstore.store-pool-size: 4\npd_servers:\n  - host: 10.0.1.4\n    # ssh_port: 22\n    # name: \"pd-1\"\n    # client_port: 2379\n    # peer_port: 2380\n    # deploy_dir: \"/tidb-deploy/pd-2379\"\n    # data_dir: \"/tidb-data/pd-2379\"\n    # log_dir: \"/tidb-deploy/pd-2379/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.pd` values.\n    # config:\n    #   schedule.max-merge-region-size: 20\n    #   schedule.max-merge-region-keys: 200000\n  - host: 10.0.1.5\n  - host: 10.0.1.6\ntidb_servers:\n  - host: 10.0.1.7\n    # ssh_port: 22\n    # port: 4000\n    # status_port: 10080\n    # deploy_dir: \"/tidb-deploy/tidb-4000\"\n    # log_dir: \"/tidb-deploy/tidb-4000/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tidb` values.\n    # config:\n    #   log.slow-query-file: tidb-slow-overwrited.log\n  - host: 10.0.1.8\n  - host: 10.0.1.9\ntikv_servers:\n  - host: 10.0.1.1\n    # ssh_port: 22\n    # port: 20160\n    # status_port: 20180\n    # deploy_dir: \"/tidb-deploy/tikv-20160\"\n    # data_dir: \"/tidb-data/tikv-20160\"\n    # log_dir: \"/tidb-deploy/tikv-20160/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tikv` values.\n    # config:\n    #   server.grpc-concurrency: 4\n    #   server.labels:\n    #     zone: \"zone1\"\n    #     dc: \"dc1\"\n    #     host: \"host1\"\n  - host: 10.0.1.2\n  - host: 10.0.1.3\n\ntiflash_servers:\n  - host: 10.0.1.11\n    # ssh_port: 22\n    # tcp_port: 9000\n    # flash_service_port: 3930\n    # flash_proxy_port: 20170\n    # flash_proxy_status_port: 20292\n    # metrics_port: 8234\n    # deploy_dir: \"/tidb-deploy/tiflash-9000\"\n    ## The `data_dir` will be overwritten if you define `storage.main.dir` configurations in the `config` section.\n    # data_dir: \"/tidb-data/tiflash-9000\"\n    # log_dir: \"/tidb-deploy/tiflash-9000/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tiflash` values.\n    # config:\n    #   logger.level: \"info\"\n    #   ## Multi-disk deployment introduced in v4.0.9\n    #   ## Check https://docs.pingcap.com/tidb/stable/tiflash-configuration#multi-disk-deployment for more details.\n    #   ## Example1:\n    #   # storage.main.dir: [ \"/nvme_ssd0_512/tiflash\", \"/nvme_ssd1_512/tiflash\" ]\n    #   # storage.main.capacity: [ 536870912000, 536870912000 ]\n    #   ## Example2:\n    #   # storage.main.dir: [ \"/sata_ssd0_512/tiflash\", \"/sata_ssd1_512/tiflash\", \"/sata_ssd2_512/tiflash\" ]\n    #   # storage.latest.dir: [ \"/nvme_ssd0_150/tiflash\" ]\n    #   # storage.main.capacity: [ 536870912000, 536870912000, 536870912000 ]\n    #   # storage.latest.capacity: [ 161061273600 ]\n    # learner_config:\n    #   log-level: \"info\"\n    #   server.labels:\n    #     zone: \"zone2\"\n    #     dc: \"dc2\"\n    #     host: \"host2\"\n  # - host: 10.0.1.12\n  # - host: 10.0.1.13\n\nmonitoring_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # port: 9090\n    # deploy_dir: \"/tidb-deploy/prometheus-8249\"\n    # data_dir: \"/tidb-data/prometheus-8249\"\n    # log_dir: \"/tidb-deploy/prometheus-8249/log\"\n\ngrafana_servers:\n  - host: 10.0.1.10\n    # port: 3000\n    # deploy_dir: /tidb-deploy/grafana-3000\n\nalertmanager_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # web_port: 9093\n    # cluster_port: 9094\n    # deploy_dir: \"/tidb-deploy/alertmanager-9093\"\n    # data_dir: \"/tidb-data/alertmanager-9093\"\n    # log_dir: \"/tidb-deploy/alertmanager-9093/log\"\n```\n\n</details>\n\n以上 TiDB 集群拓扑文件中，详细的配置项说明见[通过 TiUP 部署 TiDB 集群的拓扑文件配置](/tiup/tiup-cluster-topology-reference.md#tiflash_servers)。\n\n### 关键参数介绍\n\n- 需要将配置模板中 `replication.enable-placement-rules` 设置为 `true`，以开启 PD 的 [Placement Rules](/configure-placement-rules.md) 功能。\n\n- `tiflash_servers` 实例级别配置 `\"-host\"` 目前只支持 IP，不支持域名。\n\n- TiFlash 具体的参数配置介绍可参考 [TiFlash 参数配置](/tiflash/tiflash-configuration.md)。\n\n> **注意：**\n>\n> - 无需手动创建配置文件中的 `tidb` 用户，TiUP cluster 组件会在目标主机上自动创建该用户。可以自定义用户，也可以和中控机的用户保持一致。\n> - 如果部署目录配置为相对路径，会部署在用户的 Home 目录下。\n"
        },
        {
          "name": "tiflash-performance-tuning-methods.md",
          "type": "blob",
          "size": 7.451171875,
          "content": "---\ntitle: TiFlash 性能分析和优化方法\nsummary: 本文介绍了 Performance Overview 面板中 TiFlash 部分，帮助你了解和监控 TiFlash 的工作负载。\n---\n\n# TiFlash 性能分析和优化方法\n\n本文介绍 TiFlash 资源使用率和关键的性能指标。你可以通过 Performance Overview 面板中的 [TiFlash 面板](/grafana-performance-overview-dashboard.md#tiflash)，来监控和评估 TiFlash 集群的性能。\n\n## TiFlash 集群资源利用率\n\n通过以下三个指标，你可以快速判断 TiFlash 集群的资源使用率：\n\n- CPU：每个 TiFlash 实例的 CPU 使用率\n- Memory：每个 TiFlash 实例内存的使用情况\n- IO utilization：每个 TiFlash 实例的 IO 使用率\n\n示例：[CH-benCHmark 负载](/benchmark/benchmark-tidb-using-ch.md)资源使用率\n\n该 TiFlash 集群包含两个节点，每个节点配置均为 16 核、48G 内存。当 CH-benCHmark 负载运行时，CPU 利用率最高可达到 1500%，内存占用最大可达 20 GB，IO 利用率达到 91%。这表明 TiFlash 节点资源接近饱和状态。\n\n![CH-TiFlash-MPP](/media/performance/tiflash/tiflash-resource-usage.png) \n\n## TiFlash 关键性能指标\n\n### 吞吐指标\n\n通过以下指标，你可以了解 TiFlash 的吞吐情况：\n\n- MPP Query count：每个 TiFlash 实例 MPP 查询数量的瞬时值，表示当前 TiFlash 实例需要处理的 MPP 查询数量（包括正在处理的以及还没被调度到的）。\n- Request QPS：所有 TiFlash 实例收到的 coprocessor 请求数量。\n    - `run_mpp_task`、`dispatch_mpp_task` 和 `mpp_establish_conn` 为 MPP 请求。\n    - `batch`：batch 请求数量。\n    - `cop`：直接通过 coprocessor 接口发送的 coprocessor 请求数量。\n    - `cop_execution`：正在执行的 coprocessor 请求数量。\n    - `remote_read`、`remote_read_constructed` 和 `remote_read_sent` 为 remote read 相关指标，remote read 增多一般意味着系统出现了问题。\n- Executor QPS：所有 TiFlash 实例收到的请求中，每种 dag 算子的数量，其中 `table_scan` 是扫表算子，`selection` 是过滤算子，`aggregation` 是聚合算子，`top_n` 是 TopN 算子，`limit` 是 limit 算子，`join` 为关联算子，`exchange_sender` 和 `exchange_receiver` 为数据发送和接收算子。\n\n### 延迟指标\n\n通过以下指标，你可以了解 TiFlash 的延迟处理情况：\n\n- Request Duration Overview：每秒所有 TiFlash 实例处理所有请求类型的总时长堆叠图。\n\n    - 如果请求类型为 `run_mpp_task`、`dispatch_mpp_task` 或 `mpp_establish_conn`，说明 SQL 语句的执行已经部分或者完全下推到 TiFlash 上进行，通常包含 join 和数据分发的操作，这是 TiFlash 最常见的请求类型。\n    - 如果请求类型为 `cop`，说明整个语句并没有完全下推到 TiFlash，通常 TiDB 会将全表扫描算子下推到 TiFlash 上进行数据访问和过滤。在堆叠图中，如果 `cop` 占据主导地位，需要仔细权衡是否合理。\n\n        - 如果 SQL 访问的数据量很大，优化器可能根据成本模型估算 TiFlash 全表扫描的成本更低。\n        - 如果表结构缺少合适的索引，即使访问的数据量很少，优化器也只能将查询下推到 TiFlash 进行全表扫描。在这种情况下，创建合适的索引，通过 TiKV 访问数据更加高效。\n\n- Request Duration：所有 TiFlash 实例每种 MPP 和 coprocessor 请求类型的总处理时间，包含平均和 P99 处理延迟。\n- Request Handle Duration：指 `cop` 和 `batch cop` 从开始执行到执行结束的时间，不包括等待时间，只包含 `cop` 和 `batch cop` 两种类型，包含平均和 P99 延迟。\n\n示例 1 ：TiFlash MPP 请求处理时间概览\n\n如下图所示，在此负载中，`run_mpp_task` 和 `mpp_establish_conn` 请求的处理时间占比最高，表明大部分请求都是完全下推到 TiFlash 上执行的 MPP 任务。\n\n而 `cop` 请求处理时间占比较小，说明存在一部分请求是通过 coprocessor 下推到 TiFlash 上进行数据访问和过滤的。\n\n![CH-TiFlash-MPP](/media/performance/tiflash/ch-2tiflash-op.png)\n\n示例 2 ：TiFlash `cop` 请求处理时间占比高\n\n如下图所示，在此负载中，`cop` 请求的处理时间占比最高，可以通过查看 SQL 执行计划来确认 `cop` 请求产生的原因。\n\n![Cop](/media/performance/tiflash/tiflash_request_duration_by_type.png)\n\n### Raft 相关指标\n\n通过以下指标，你可以了解 TiFlash 的 Raft 同步情况：\n\n- Raft Wait Index Duration：所有 TiFlash 实例等待本地 Region index >= read_index 所花费的时间，即进行 wait_index 操作的延迟。如果 Wait Index 延迟过高，说明 TiKV 和 TiFlash 之间数据同步存在明显的延迟，可能的原因包括：\n\n    - TiKV 资源过载\n    - TiFlash 资源过载，特别是 IO 资源\n    - TiKV 和 TiFlash 之间存在网络瓶颈\n\n- Raft Batch Read Index Duration：所有 TiFlash 实例 `read_index` 的延迟。如果该指标过高，说明 TiFlash 和 TiKV 之间的交互速度较慢，可能的原因包括：\n\n    - TiFlash 资源过载\n    - TiKV 资源过载\n    - TiFlash 和 TiKV 之间存在网络瓶颈\n\n### IO 流量指标\n\n通过以下指标，你可以了解 TiFlash 的 IO 流量情况：\n\n- Write Throughput By Instance：每个 TiFlash 实例写入数据的吞吐量，包括 apply Raft 数据日志以及 Raft 快照的写入吞吐量。\n- Write flow：所有 TiFlash 实例磁盘写操作的流量。\n\n    - File Descriptor：TiFlash 所使用的 DeltaTree 存储引擎的稳定层。\n    - Page：指 Pagestore，TiFlash 所使用的 DeltaTree 存储引擎的 Delta 变更层。\n\n- Read flow：所有 TiFlash 实例磁盘读操作的流量。\n\n    - File Descriptor：TiFlash 所使用的 DeltaTree 存储引擎的稳定层。\n    - Page：指 Pagestore，TiFlash 所使用的 DeltaTree 存储引擎的 Delta 变更层。\n\n你可以通过 `(Read flow + Write flow) ÷ 总的 Write Throughput By Instance` 计算出整个 TiFlash 集群的写放大倍数。\n\n示例 1 ：[CH-benCHmark 负载](/benchmark/benchmark-tidb-using-ch.md)本地部署环境 Raft 和 IO 指标\n\n如下图所示，该 TiFlash 集群的 Raft Wait Index Duration 和 Raft Batch Read Index Duration 的 99 分位数较高，分别为 3.24 秒和 753 毫秒。这是因为该集群的 TiFlash 负载较高，数据同步存在延迟。\n\n该集群包含两个 TiFlash 节点，每秒 TiKV 同步到 TiFlash 的增量数据约为 28 MB。稳定层 (File Descriptor) 的文件描述符最大写流量为 939 MB/s，最大读流量为 1.1 GiB/s，而 Delta 层 (Page) 最大写流量为 74 MB/s，最大读流量为 111 MB/s。该环境中的 TiFlash 使用独立的 NVME 盘，具有较强的 IO 吞吐能力。\n\n![CH-2TiFlash-OP](/media/performance/tiflash/ch-2tiflash-raft-io-flow.png)\n\n示例 2 ：[CH-benCHmark 负载](/benchmark/benchmark-tidb-using-ch.md) 公有云环境 Raft 和 IO 指标\n\n如下图所示，Raft Wait Index Duration 等待时间 99 分位数最高为 438 毫秒，Raft Batch Read Index Duration 等待时间 99 分位数最高为 125 毫秒。该集群只有一个 TiFlash 节点，每秒 TiKV 同步到 TiFlash 的增量数据约为 5 MB。稳定层 (File Descriptor) 的最大写入流量为 78 MB/s，最大读取流量为 221 MB/s，Delta 层 (Page) 最大写入流量为 8 MB/s，最大读取流量为 18 MB/s。这个环境中的 TiFlash 使用的是 AWS EBS 云盘，其 IO 吞吐能力相对较弱。\n\n![CH-TiFlash-MPP](/media/performance/tiflash/ch-1tiflash-raft-io-flow-cloud.png)"
        },
        {
          "name": "tiflash-upgrade-guide.md",
          "type": "blob",
          "size": 7.2822265625,
          "content": "---\ntitle: TiFlash 升级帮助\nsummary: 了解升级 TiFlash 时的注意事项。\naliases: ['/zh/tidb/dev/tiflash-620-upgrade-guide']\n---\n\n# TiFlash 升级帮助\n\n本文介绍 TiFlash 升级时功能模块的变化，以及推荐的应对方法。\n\n如需了解标准升级流程，请参考如下文档：\n\n- [使用 TiUP 升级 TiDB](/upgrade-tidb-using-tiup.md)\n- [使用 TiDB Operator 升级 TiDB](https://docs.pingcap.com/zh/tidb-in-kubernetes/stable/upgrade-a-tidb-cluster)\n\n> **注意：**\n>\n> - v6.2.0 新增了一项名为 [FastScan](/tiflash/use-fastscan.md) 的实验功能，该功能在 v7.0.0 GA。FastScan 在牺牲强一致性保证的前提下可以大幅提升扫表性能。\n>\n> - 不推荐跨主干版本升级包含 TiFlash 的 TiDB 集群，如从 v4.x 升级至 v6.x，请先升级至 v5.x，然后再升级至 v6.x。\n>\n> - v4.x. 已接近产品周期尾声，请尽早升级到 v5.x 及以上版本。具体的版本周期请参考 [TiDB 版本周期支持策略](https://pingcap.com/zh/tidb-release-support-policy)。\n>\n> - v6.0 作为非 LTS 版本，不会推出后续的 bug 修复版，请尽量使用 v6.1 及之后的 LTS 版本。\n\n## 使用 TiUP 升级\n\n如需将 TiFlash 从 v5.3.0 之前的版本升级到 v5.3.0 及之后的版本，必须进行 TiFlash 的停机升级。使用 TiUP 进行升级时：\n\n- 如果 TiUP Cluster 版本大于或等于 v1.12.0，则无法进行 TiFlash 的停机升级。如果目标版本要求的 TiUP Cluster 版本大于或等于 v1.12.0，则建议先使用 `tiup cluster:v1.11.3 <subcommand>` 将 TiFlash 升级到某个中间版本，然后进行 TiDB 集群的在线升级，之后升级 TiUP 版本，最后对 TiDB 集群进行不停机升级至目标版本。\n- 如果 TiUP Cluster 版本小于 v1.12.0，则执行以下步骤进行升级 TiFlash。\n\n参考如下步骤，可以在确保其他组件正常运行的情况下，使用 TiUP 升级 TiFlash：\n\n1. 关闭 TiFlash 实例：\n\n    ```shell\n    tiup cluster stop <cluster-name> -R tiflash\n    ```\n\n2. 使用 `--offline` 参数在不重启（只更新文件）的情况下升级集群：\n\n    ```shell \n    tiup cluster upgrade <cluster-name> <version> --offline\n    ```\n    \n    例如： \n    \n    ```shell     \n    tiup cluster upgrade <cluster-name> v5.3.0 --offline\n    ```\n\n3. 重新加载整个集群。此时，TiFlash 也会正常启动，无需额外操作。\n\n    ```shell \n    tiup cluster reload <cluster-name>\n    ```\n\n## 从 v5.x 或 v6.0 升级至 v6.1\n\n从 v5.x 或 v6.0 升级至 v6.1 时，需要注意 TiFlash Proxy 和动态分区裁剪功能的变化。\n\n### TiFlash Proxy\n\nTiFlash 在 v6.1.0 对 Proxy 做了升级（与 TiKV v6.0.0 对齐）。新的 Proxy 版本升级了 RocksDB 版本，在升级过程中会自动将数据格式转换为新版本。\n\n正常升级时，不会有明显风险。如果特殊场景（如测试验证）需要降级，请注意，v6.1 降级到之前的任意版本时，会无法解析新版 RocksDB 配置，从而导致 TiFlash 重启失败。请做好升级验证工作，并尽可能准备应急方案。\n\n**测试环境及特殊回退需求下的对策**\n\n强制缩容 TiFlash 节点，并重新同步数据。操作步骤详见[缩容 TiFlash 节点](/scale-tidb-using-tiup.md#缩容-tiflash-节点)。\n\n### 动态分区裁剪\n\n如果你没有也不打算开启动[态分区裁剪](/partitioned-table.md#动态裁剪模式)，可略过本部分。\n\n- TiDB v6.1 全新安装：默认开启动态分区裁剪 (Dynamic Pruning)。\n\n- TiDB v6.0 及之前版本：默认关闭动态分区裁剪。旧版本升级遵循已有设定，不会自动开启（相对的也不会关闭）此功能。\n\n    升级完成之后，如果要启用动态分区裁剪特性，请确保 `tidb_partition_prune_mode` 的值为 `dynamic`，并手动更新分区表的全局统计信息。关于如何手动更新统计信息，参见[动态裁剪模式](/partitioned-table.md#动态裁剪模式)。\n\n## 从 v5.x 或 v6.0 升级至 v6.2\n\nTiFlash 在 v6.2.0 将数据格式升级到 V3 版本，因此，从 v5.x 或 v6.0 升级至 v6.2 时，除了需要注意 [TiFlash Proxy](#tiflash-proxy) 和[动态分区裁剪](#动态分区裁剪)的变化，还应注意 PageStorage 变更数据版本带来的影响，具体如下：\n\n- 已有节点升级 v6.2 后，随着数据不断写入，旧版本的数据会逐步转换成新版本数据。\n- 新旧版本的数据格式不能做到完全的转换，这会带来一定系统开销（通常不影响业务，但需要注意）。因此升级完成后，建议使用 [`COMPACT` 命令](/sql-statements/sql-statement-alter-table-compact.md)触发数据整理 (Compaction) 将相关表的数据转成新版本格式。操作步骤如下：\n\n    1. 对每张有 TiFlash 副本（replica）的表执行如下命令：\n\n        ```sql\n        ALTER TABLE <table_name> COMPACT tiflash replica;\n        ```\n\n    2. 重启 TiFlash 节点。\n\n你可以在 Grafana 监控查看是否还有表使用旧的数据版本：**TiFlash-Summary** > **Storage Pool** > **Storage Pool Run Mode**\n\n- Only V2：使用 PageStorage V2 的表数量（包括分区数）\n- Only V3：使用 PageStorage V3 的表数量（包括分区数）\n- Mix Mode：从 V2 迁移到 V3 的表数量（包括分区数）\n\n**测试环境及特殊回退需求下的对策**\n\n强制缩容 TiFlash 节点，并重新同步数据。操作步骤详见[缩容 TiFlash 节点](/scale-tidb-using-tiup.md#缩容-tiflash-节点)。\n\n## 从 v6.1 升级至 v6.2\n\n从 v6.1 升级至 v6.2 时，需要注意 PageStorage 变更数据版本带来的影响。具体请参考[从 v5.x 或 v6.0 升级至 v6.2](#从-v5x-或-v60-升级至-v62) 中关于 PageStorage 的描述。\n\n## 从 v6.x 或 v7.x 升级至 v7.3，并且设置了 `storage.format_version = 5`\n\n从 v7.3 开始，TiFlash 支持新的 DTFile 版本 V3（实验特性），可以将多个小文件合并成一个大文件，减少文件数量。DTFile 在 v7.3 的默认版本是 V2，如需使用 V3，可通过 [TiFlash 配置参数](/tiflash/tiflash-configuration.md) `storage.format_version = 5` 来设置。设置后，TiFlash 仍可以读 V2 版本的 DTFile，并且在后续的数据整理 (Compaction) 中会将这些 V2 版本的 DMFile 逐步重新写为 V3 版本的 DTFile。\n\n在 TiFlash 升级到 v7.3 并且使用了 V3 版本的 DTFile 后，如需回退到之前的 TiFlash 版本，可以通过 DTTool 离线将 DTFile 重新写回 V2 版本，详见 [DTTool 迁移工具](/tiflash/tiflash-command-line-flags.md#dttool-migrate)。\n\n## 从 v6.x 或 v7.x 升级至 v7.4 或以上版本\n\n从 v7.4 开始，为了减少数据整理时产生的读、写放大，PageStorage V3 数据整理时逻辑进行了优化，导致底层部分存储文件名发生改动。因此，升级 TiFlash 到 v7.4 或以上版本后，不支持原地降级到之前的版本。\n\n## 从 v7.x 升级至 v8.4 或以上版本\n\n从 v8.4 开始，为了支持[向量搜索功能](/vector-search/vector-search-index.md)，TiFlash 底层存储格式发生改动。因此，升级 TiFlash 到 v8.4 或以上版本后，不支持原地降级到之前的版本。\n\n**测试环境及特殊回退需求下的对策**\n\n如果在测试环境下或者其他有特殊回退需求的场景下，可以强制缩容 TiFlash 节点，并重新同步数据。操作步骤详见[缩容 TiFlash 节点](/scale-tidb-using-tiup.md#缩容-tiflash-节点)。\n"
        },
        {
          "name": "tiflash",
          "type": "tree",
          "content": null
        },
        {
          "name": "tikv-configuration-file.md",
          "type": "blob",
          "size": 102.3466796875,
          "content": "---\ntitle: TiKV 配置文件描述\nsummary: 了解 TiKV 的配置文件参数。\naliases: ['/docs-cn/dev/tikv-configuration-file/','/docs-cn/dev/reference/configuration/tikv-server/configuration-file/']\n---\n\n# TiKV 配置文件描述\n\n<!-- markdownlint-disable MD001 -->\n\nTiKV 配置文件比命令行参数支持更多的选项。你可以在 [etc/config-template.toml](https://github.com/tikv/tikv/blob/master/etc/config-template.toml) 找到默认值的配置文件，重命名为 config.toml 即可。\n\n本文档只阐述未包含在命令行参数中的参数，命令行参数参见 [TiKV 配置参数](/command-line-flags-for-tikv-configuration.md)。\n\n> **Tip:**\n>\n> 如果你需要调整配置项的值，请参考[修改配置参数](/maintain-tidb-using-tiup.md#修改配置参数)进行操作。\n\n<!-- markdownlint-disable MD001 -->\n\n## 全局配置\n\n### `abort-on-panic`\n\n+ 设置 TiKV panic 时是否调用 `abort()` 退出进程。此选项影响 TiKV 是否允许系统生成 core dump 文件。\n\n    + 如果此配置项值为 false，当 TiKV panic 时，TiKV 调用 `exit()` 退出进程。\n    + 如果此配置项值为 true，当 TiKV panic 时，TiKV 调用 `abort()` 退出进程。此时 TiKV 允许系统在退出时生成 core dump 文件。要生成 core dump 文件，你还需要进行 core dump 相关的系统配置（比如打开 `ulimit -c` 和配置 core dump 路径，不同操作系统配置方式不同）。建议将 core dump 生成路径设置在 TiKV 数据的不同磁盘分区，避免 core dump 文件占用磁盘空间过大，造成 TiKV 磁盘空间不足。\n\n+ 默认值：false\n\n### `slow-log-file`\n\n+ 存储慢日志的文件。\n+ 如果未设置本项但设置了 `log.file.filename`，慢日志将输出至 `log.file.filename` 指定的日志文件中。\n+ 如果本项和 `log.file.filename` 均未设置，所有日志默认输出到 `\"stderr\"`。\n+ 如果同时设置了两项，普通日志会输出至 `log.file.filename` 指定的日志文件中，而慢日志则会输出至本配置项指定的日志文件中。\n+ 默认值：\"\"\n\n### `slow-log-threshold`\n\n+ 输出慢日志的阈值。处理时间超过该阈值后会输出慢日志。\n+ 默认值：\"1s\"\n\n### `memory-usage-limit`\n\n+ TiKV 实例的内存使用限制。当 TiKV 的内存使用量接近此阈值时，内部缓存会被清除以释放内存。\n+ 在大多数情况下，TiKV 实例被设置为占系统可用总内存的 75%，因此你不需要显式指定此配置项。剩余 25% 的内存用于操作系统的页缓存，详情参见 [`storage.block-cache.capacity`](#capacity)。\n+ 在单个物理机上部署多个 TiKV 节点时，你也不需要设置此配置项。在这种情况下，TiKV 实例使用 `5/3 * block-cache.capacity` 的内存。\n+ 不同系统内存容量的默认值如下：\n\n    + system=8G    block-cache=3.6G    memory-usage-limit=6G   page-cache=2G\n    + system=16G   block-cache=7.2G    memory-usage-limit=12G  page-cache=4G\n    + system=32G   block-cache=14.4G   memory-usage-limit=24G  page-cache=8G\n\n## log <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n日志相关的配置项。\n\n自 v5.4.0 版本起，废弃原 log 参数 `log-rotation-timespan`，并将 `log-level`、`log-format`、`log-file`、`log-rotation-size` 变更为下列参数，与 TiDB 的 log 参数保持一致。如果只设置了原参数、且把其值设为非默认值，原参数与新参数会保持兼容；如果同时设置了原参数和新参数，则会使用新参数。\n\n### `level` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ 日志等级。\n+ 可选值：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n+ 默认值：\"info\"\n\n### `format` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ 日志的格式。\n+ 可选值：\"json\"，\"text\"\n+ 默认值：\"text\"\n\n### `enable-timestamp` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ 是否开启日志中的时间戳。\n+ 可选值：\"true\"，\"false\"\n+ 默认值：\"true\"\n\n## log.file <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n日志文件相关的配置项。\n\n### `filename` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ log 文件。如果未设置该参数，日志会默认输出到 `\"stderr\"`；如果设置了该参数，log 会输出到对应的文件中。\n+ 默认值：\"\"\n\n### `max-size` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ 单个 log 文件最大大小，超过设定的参数值后，系统自动切分成多个文件。\n+ 默认值：300\n+ 最大值：4096\n+ 单位：MiB\n\n### `max-days` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ 保留 log 文件的最长天数。\n    + 如果未设置本参数或把此参数设置为默认值 `0`，TiKV 不清理 log 文件。\n    + 如果把此参数设置为非 `0` 的值，在 `max-days` 之后，TiKV 会清理过期的日志文件。\n+ 默认值：0\n\n### `max-backups` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ 可保留的 log 文件的最大数量。\n    + 如果未设置本参数或把此参数设置为默认值 `0`，TiKV 会保存所有的 log 文件；\n    + 如果把此参数设置为非 `0` 的值，TiKV 最多会保留 `max-backups` 中指定的数量的旧日志文件。比如，如果该值设置为 `7`，TiKV 最多会保留 7 个旧的日志文件。\n+ 默认值：0\n\n## server\n\n服务器相关的配置项。\n\n### `addr`\n\n+ 服务器监听的 IP 地址和端口号。\n+ 默认值：`\"127.0.0.1:20160\"`\n\n### `advertise-addr`\n\n+ 用于客户端通信的对外访问地址。\n+ 如果没有设置该配置项，则使用 `addr` 的值。\n+ 默认值：`\"\"`\n\n### `status-addr`\n\n+ 通过 HTTP 直接报告 TiKV 状态的地址。\n\n    > **警告：**\n    >\n    > 如果该值暴露在公网，TiKV 服务器的状态可能会泄露。\n\n+ 要禁用 `status-addr`，请将该值设置为 `\"\"`。\n+ 默认值：`\"127.0.0.1:20180\"`\n\n### `status-thread-pool-size`\n\n+ HTTP API 服务的工作线程数量。\n+ 默认值：1\n+ 最小值：1\n\n### `grpc-compression-type`\n\n+ gRPC 消息的压缩算法。它会影响 TiKV 节点之间的 gRPC 消息以及 TiKV 发送给 TiDB 的 gRPC（响应）消息。\n+ 可选值：`\"none\"`、`\"deflate\"`、`\"gzip\"`\n+ 默认值：`\"none\"`\n\n### `grpc-concurrency`\n\n+ gRPC 工作线程的数量。调整 gRPC 线程池的大小时，请参考 [TiKV 线程池调优](/tune-tikv-thread-performance.md#tikv-线程池调优)。\n+ 默认值：5\n+ 最小值：1\n\n### `grpc-concurrent-stream`\n\n+ 一个 gRPC 链接中最多允许的并发请求数量。\n+ 默认值：1024\n+ 最小值：1\n\n### `grpc-memory-pool-quota`\n\n+ gRPC 可使用的内存大小限制。\n+ 默认值：无限制\n+ 建议仅在出现内存不足 (OOM) 的情况下限制内存使用。需要注意，限制内存使用可能会导致卡顿。\n\n### `grpc-raft-conn-num`\n\n+ TiKV 节点之间用于 Raft 通信的连接最大数量。\n+ 默认值：1\n+ 最小值：1\n\n### `max-grpc-send-msg-len`\n\n+ 设置可发送的最大 gRPC 消息长度。\n+ 默认值：10485760\n+ 单位：Bytes\n+ 最大值：2147483647\n\n### `grpc-stream-initial-window-size`\n\n+ gRPC stream 的 window 大小。\n+ 默认值：2MiB\n+ 单位：KiB|MiB|GiB\n+ 最小值：1KiB\n\n### `grpc-keepalive-time`\n\n+ gRPC 发送 keep alive ping 消息的间隔时长。\n+ 默认值：10s\n+ 最小值：1s\n\n### `grpc-keepalive-timeout`\n\n+ 关闭 gRPC 链接的超时时长。\n+ 默认值：3s\n+ 最小值：1s\n\n### `concurrent-send-snap-limit`\n\n+ 同时发送 snapshot 的最大个数。\n+ 默认值：32\n+ 最小值：1\n\n### `concurrent-recv-snap-limit`\n\n+ 同时接受 snapshot 的最大个数。\n+ 默认值：32\n+ 最小值：1\n\n### `end-point-recursion-limit`\n\n+ endpoint 下推查询请求解码消息时，最多允许的递归层数。\n+ 默认值：1000\n+ 最小值：1\n\n### `end-point-request-max-handle-duration`\n\n+ endpoint 下推查询请求处理任务最长允许的时长。\n+ 默认值：60s\n+ 最小值：1s\n\n### `snap-io-max-bytes-per-sec`\n\n+ 处理 snapshot 时最大允许使用的磁盘带宽。\n+ 默认值：100MiB\n+ 单位：KiB|MiB|GiB\n+ 最小值：1KiB\n\n### `enable-request-batch`\n\n+ 控制是否开启批处理请求。\n+ 默认值：`true`\n\n### `labels`\n\n+ 指定服务器属性，例如 `{ zone = \"us-west-1\", disk = \"ssd\" }`。\n+ 默认值：`{}`\n\n### `background-thread-count`\n\n+ 后台线程池的工作线程数量，包括 endpoint 线程、BR 线程、split check 线程、Region 线程以及其他延迟不敏感的任务线程。\n+ 默认值：当 CPU 核数小于 16 时，默认值为 `2`。否则，默认值为 `3`。\n\n### `end-point-slow-log-threshold`\n\n+ endpoint 下推查询请求输出慢日志的阈值，处理时间超过阈值后会输出慢日志。\n+ 默认值：1s\n+ 最小值：0\n\n### `raft-client-queue-size`\n\n+ 该配置项指定 TiKV 中发送 Raft 消息的缓冲区大小。如果存在消息发送不及时导致缓冲区满、消息被丢弃的情况，可以适当调大该配置项值以提升系统运行的稳定性。\n+ 默认值：16384\n\n### `simplify-metrics` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 是否精简返回的监控指标 Metrics 数据。设置为 `true` 后，TiKV 可以通过过滤部分 Metrics 采样数据以减少每次请求返回的 Metrics 数据量。\n+ 默认值：false\n\n### `forward-max-connections-per-address` <span class=\"version-mark\">从 v5.0.0 版本开始引入</span>\n\n+ 设置服务与转发请求的连接池大小。设置过小会影响请求的延迟和负载均衡。\n+ 默认值：4\n\n## readpool.unified\n\n统一处理读请求的线程池相关的配置项。该线程池自 4.0 版本起取代原有的 storage 和 coprocessor 线程池。\n\n### `min-thread-count`\n\n+ 统一处理读请求的线程池最少的线程数量。\n+ 默认值：1\n\n### `max-thread-count`\n\n+ 统一处理读请求的线程池最多的线程数量，即 UnifyReadPool 线程池的大小。调整该线程池的大小时，请参考 [TiKV 线程池调优](/tune-tikv-thread-performance.md#tikv-线程池调优)。\n+ 可调整范围：`[min-thread-count, MAX(4, CPU quota * 10)]`。其中，`MAX(4, CPU quota * 10)` 表示：如果 CPU 配额乘 10 小于 `4`，取 `4`；如果 CPU 配额乘 10 大于 `4`，即 CPU 配额大于 `0.4`，则取 CPU 配额乘 10。\n+ 默认值：MAX(4, CPU quota * 0.8)\n\n> **注意：**\n>\n> 增加线程数量会导致上下文切换增多，可能会导致性能下降，因此不推荐修改此配置。\n\n### `stack-size`\n\n+ 统一处理读请求的线程池中线程的栈大小。\n+ 类型：整数 + 单位\n+ 默认值：10MiB\n+ 单位：KiB|MiB|GiB\n+ 最小值：2MiB\n+ 最大值：在系统中执行 `ulimit -sH` 命令后，输出的千字节数。\n\n### `max-tasks-per-worker`\n\n+ 统一处理读请求的线程池中单个线程允许积压的最大任务数量，超出后会返回 Server Is Busy。\n+ 默认值：2000\n+ 最小值：2\n\n### `auto-adjust-pool-size` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n+ 是否开启自动调整线程池的大小。开启此配置可以基于当前的 CPU 使用情况，自动调整统一处理读请求的线程池 (UnifyReadPool) 的大小，优化 TiKV 的读性能。目前线程池自动调整的范围为：`[max-thread-count, MAX(4, CPU)]`(上限与 [`max-thread-count`](#max-thread-count) 可设置的最大值相同)。\n+ 默认值：false\n\n## readpool.storage\n\n存储线程池相关的配置项。\n\n### `use-unified-pool`\n\n+ 是否使用统一的读取线程池（在 [`readpool.unified`](#readpoolunified) 中配置）处理存储请求。该选项值为 false 时，使用单独的存储线程池。通过本节 (`readpool.storage`) 中的其余配置项配置单独的线程池。\n+ 默认值：如果本节 (`readpool.storage`) 中没有其他配置，默认为 true。否则，为了升级兼容性，默认为 false，请根据需要更改 [`readpool.unified`](#readpoolunified) 中的配置后再启用该选项。\n\n### `high-concurrency`\n\n+ 处理高优先级读请求的线程池线程数量。\n+ 当 `8` ≤ `cpu num` ≤ `16` 时，默认值为 `cpu_num * 0.5`；当 `cpu num` 小于 `8` 时，默认值为 `4`；当 `cpu num` 大于 `16` 时，默认值为 `8`。\n+ 最小值：`1`\n\n### `normal-concurrency`\n\n+ 处理普通优先级读请求的线程池线程数量。\n+ 当 `8` ≤ `cpu num` ≤ `16` 时，默认值为 `cpu_num * 0.5`；当 `cpu num` 小于 `8` 时，默认值为 `4`；当 `cpu num` 大于 `16` 时，默认值为 `8`。\n+ 最小值：`1`\n\n### `low-concurrency`\n\n+ 处理低优先级读请求的线程池线程数量。\n+ 当 `8` ≤ `cpu num` ≤ `16` 时，默认值为 `cpu_num * 0.5`；当 `cpu num` 小于 `8` 时，默认值为 `4`；当 `cpu num` 大于 `16` 时，默认值为 `8`。\n+ 最小值：`1`\n\n### `max-tasks-per-worker-high`\n\n+ 高优先级线程池中单个线程允许积压的最大任务数量，超出后会返回 Server Is Busy。\n+ 默认值：2000\n+ 最小值：2\n\n### `max-tasks-per-worker-normal`\n\n+ 普通优先级线程池中单个线程允许积压的最大任务数量，超出后会返回 Server Is Busy。\n+ 默认值：2000\n+ 最小值：2\n\n### `max-tasks-per-worker-low`\n\n+ 低优先级线程池中单个线程允许积压的最大任务数量，超出后会返回 Server Is Busy。\n+ 默认值：2000\n+ 最小值：2\n\n### `stack-size`\n\n+ Storage 读线程池中线程的栈大小。\n+ 类型：整数 + 单位\n+ 默认值：10MiB\n+ 单位：KiB|MiB|GiB\n+ 最小值：2MiB\n+ 最大值：在系统中执行 `ulimit -sH` 命令后，输出的千字节数。\n\n## readpool.coprocessor\n\n协处理器线程池相关的配置项。\n\n### `use-unified-pool`\n\n+ 是否使用统一的读取线程池（在 [`readpool.unified`](#readpoolunified) 中配置）处理协处理器请求。该选项值为 false 时，使用单独的协处理器线程池。通过本节 (`readpool.coprocessor`) 中的其余配置项配置单独的线程池。\n+ 默认值：如果本节 (`readpool.coprocessor`) 中没有其他配置，默认为 true。否则，为了升级兼容性，默认为 false，请根据需要更改 [`readpool.unified`](#readpoolunified) 中的配置后再启用该选项。\n\n### `high-concurrency`\n\n+ 处理高优先级 Coprocessor 请求（如点查）的线程池线程数量。\n+ 默认值：CPU * 0.8\n+ 最小值：1\n\n### `normal-concurrency`\n\n+ 处理普通优先级 Coprocessor 请求的线程池线程数量。\n+ 默认值：CPU * 0.8\n+ 最小值：1\n\n### `low-concurrency`\n\n+ 处理低优先级 Coprocessor 请求（如扫表）的线程池线程数量。\n+ 默认值：CPU * 0.8\n+ 最小值：1\n\n### `max-tasks-per-worker-high`\n\n+ 高优先级线程池中单个线程允许积压的任务数量，超出后会返回 Server Is Busy。\n+ 默认值：2000\n+ 最小值：2\n\n### `max-tasks-per-worker-normal`\n\n+ 普通优先级线程池中单个线程允许积压的任务数量，超出后会返回 Server Is Busy。\n+ 默认值：2000\n+ 最小值：2\n\n### `max-tasks-per-worker-low`\n\n+ 低优先级线程池中单个线程允许积压的任务数量，超出后会返回 Server Is Busy。\n+ 默认值：2000\n+ 最小值：2\n\n### `stack-size`\n\n+ Coprocessor 线程池中线程的栈大小。\n+ 默认值：10MiB\n+ 单位：KiB|MiB|GiB\n+ 最小值：2MiB\n+ 最大值：在系统中执行 `ulimit -sH` 命令后，输出的千字节数。\n\n## storage\n\n存储相关的配置项。\n\n### `data-dir`\n\n+ RocksDB 存储路径。\n+ 默认值：`\"./\"`\n\n### `engine` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> 该功能目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 设置存储引擎类型。该配置只能在创建新集群时指定，且后续无法更改。\n+ 默认值：`\"raft-kv\"`\n+ 可选值：\n\n    + `\"raft-kv\"`：TiDB v6.6.0 之前版本的默认存储引擎。\n    + `\"partitioned-raft-kv\"`：TiDB v6.6.0 新引入的存储引擎。\n\n### `scheduler-concurrency`\n\n+ scheduler 内置一个内存锁机制，防止同时对一个 key 进行操作。每个 key hash 到不同的槽。\n+ 默认值：524288\n+ 最小值：1\n\n### `scheduler-worker-pool-size`\n\n+ Scheduler 线程池中线程的数量。Scheduler 线程主要负责写入之前的事务一致性检查工作。如果 CPU 核心数量大于等于 16，默认为 8；否则默认为 4。调整 scheduler 线程池的大小时，请参考 [TiKV 线程池调优](/tune-tikv-thread-performance.md#tikv-线程池调优)。\n+ 默认值：4\n+ 可调整范围：`[1, MAX(4, CPU)]`。其中，`MAX(4, CPU)` 表示：如果 CPU 核心数量小于 `4`，取 `4`；如果 CPU 核心数量大于 `4`，则取 CPU 核心数量。\n\n### `scheduler-pending-write-threshold`\n\n+ 写入数据队列的最大值，超过该值之后对于新的写入 TiKV 会返回 Server Is Busy 错误。\n+ 默认值：100MiB\n+ 单位：MiB|GiB\n\n### `enable-async-apply-prewrite`\n\n+ 控制异步提交 (Async Commit) 事务在应用 prewrite 请求之前是否响应 TiKV 客户端。开启该配置项可以降低 apply 耗时较高时的延迟，或者减少 apply 耗时不稳定时的延迟抖动。\n+ 默认值：`false`\n\n### `reserve-space`\n\n+ TiKV 启动时会预留一块空间用于保护磁盘空间。当磁盘剩余空间小于该预留空间时，TiKV 会限制部分写操作。预留空间形式上分为两个部分：预留空间的 80% 用作磁盘空间不足时的运维操作所需要的额外磁盘空间，剩余的 20% 为磁盘临时文件。在回收空间的过程中，如果额外使用的磁盘空间过多，导致存储耗尽时，该临时文件会成为恢复服务的最后一道防御。\n+ 临时文件名为 `space_placeholder_file`，位于 `storage.data-dir` 目录下。当 TiKV 因磁盘空间耗尽而下线时，重启 TiKV 会自动删除该临时文件，并自动尝试回收空间。\n+ 当剩余空间不足时，TiKV 不会创建该临时文件。防御的有效性与预留空间的大小有关。预留空间大小的计算方式为磁盘容量的 5% 与该配置项之间的最大值。当该配置项的值为 `0MiB` 时，TiKV 会关闭磁盘防护功能。\n+ 默认值：5GiB\n+ 单位：MiB|GiB\n\n### `enable-ttl`\n\n> **警告：**\n>\n> - 你**只能**在部署新的 TiKV 集群时将 `enable-ttl` 的值设置为 `true` 或 `false`，**不能**在已有的 TiKV 集群中修改该配置项的值。由于该配置项为 `true` 和 `false` 的 TiKV 集群所存储的数据格式不相同，如果你在已有的 TiKV 集群中修改该配置项的值，会造成不同格式的数据存储在同一个集群，导致重启对应的 TiKV 集群时 TiKV 报 \"can't enable ttl on a non-ttl instance\" 错误。\n> - 你**只能**在 TiKV 集群中使用 `enable-ttl`，**不能**在有 TiDB 节点的集群中使用该配置项（即在此类集群中把 `enable-ttl` 设置为 `true`），否则会导致数据损坏、TiDB 集群升级失败等严重后果。\n\n+ TTL 即 Time to live。数据超过 TTL 时间后会被自动删除。用户需在客户端写入请求中指定 TTL。不指定 TTL 即表明相应数据不会被自动删除。\n+ 默认值：false\n\n### `ttl-check-poll-interval`\n\n+ 回收数据物理空间的检查周期。如果数据超过了 TTL 时间，数据的物理空间会在检查时被强制回收。\n+ 默认值：12h\n+ 最小值：0s\n\n### `background-error-recovery-window` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n+ RocksDB 检测到可恢复的后台错误后，所允许的最长恢复时间。如果后台 SST 文件出现损坏，RocksDB 在检测到故障 SST 文件所属的 Peer 后，会通过心跳上报到 PD。PD 随后会进行调度操作移除该 Peer。最后故障 SST 文件将会被直接删除，随后 TiKV 后台恢复正常。\n+ 在恢复操作完成之前，损坏的 SST 文件将一直存在。此时 RocksDB 可以继续写入新的内容，但读到损坏的数据范围时会返回错误。\n+ 如果恢复操作未能在该时间窗口内完成，TiKV 会崩溃。\n+ 默认值：1h\n\n### `api-version` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n+ TiKV 作为 RawKV 存储数据时使用的存储格式与接口版本。\n+ 可选值：\n    + `1`：使用 API V1。不对客户端传入的数据进行编码，而是原样存储。在 v6.1.0 之前的版本，TiKV 都使用 API V1。\n    + `2`：使用 API V2：\n        + 数据采用多版本并发控制 (MVCC) 方式存储，其中时间戳由 tikv-server 从 PD 获取（即 TSO）。\n        + 数据根据使用方式划分范围，支持单一集群 TiDB、事务 KV、RawKV 应用共存。\n        + 需要同时设置 `storage.enable-ttl = true`。由于 API V2 支持 TTL 特性，因此强制要求打开 `enable-ttl` 以避免这个参数出现歧义。\n        + 启用 API V2 后需要在集群中额外部署至少一个 tidb-server 以回收过期数据。该 tidb-server 可同时提供数据库读写服务。可以部署多个 tidb-server 以保证高可用。\n        + 需要客户端的支持。请参考对应客户端的 API V2 使用说明。\n        + 从 v6.2.0 版本开始，你可以通过 [RawKV CDC](https://tikv.org/docs/latest/concepts/explore-tikv-features/cdc/cdc-cn/) 组件实现 RawKV 的 Change Data Capture (CDC)。\n+ 默认值：1\n\n> **警告：**\n>\n> - 由于 API V1 和 API V2 底层存储格式不同，因此**仅当** TiKV 中只有 TiDB 数据时，可以平滑启用或关闭 API V2。其他情况下，需要新建集群，并使用 [TiKV Backup & Restore](https://tikv.org/docs/latest/concepts/explore-tikv-features/backup-restore-cn/) 工具进行数据迁移。\n> - 启用 API V2 后，**不能**将 TiKV 集群回退到 v6.1.0 之前的版本，否则可能导致数据损坏。\n\n## storage.block-cache\n\nRocksDB 多个 CF 之间共享 block cache 的配置选项。\n\n### `capacity`\n\n+ 共享 block cache 的大小。\n+ 默认值：\n\n    + 当 `storage.engine=\"raft-kv\"` 时，默认值为系统总内存大小的 45%。\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为系统总内存大小的 30%。\n\n+ 单位：KiB|MiB|GiB\n\n## storage.flow-control\n\n在 scheduler 层进行流量控制代替 RocksDB 的 write stall 机制，可以避免 write stall 机制卡住 Raftstore 或 Apply 线程导致的次生问题。本节介绍 TiKV 流量控制机制相关的配置项。\n\n### `enable`\n\n+ 是否开启流量控制机制。开启后，TiKV 会自动关闭 KvDB 的 write stall 机制，还会关闭 RaftDB 中除 memtable 以外的 write stall 机制。\n+ 默认值：true\n\n### `memtables-threshold`\n\n+ 当 KvDB 的 memtable 的个数达到该阈值时，流控机制开始工作。当 `enable` 的值为 `true` 时，会覆盖 `rocksdb.(defaultcf|writecf|lockcf).max-write-buffer-number` 的配置。\n+ 默认值：5\n\n### `l0-files-threshold`\n\n+ 当 KvDB 的 L0 文件个数达到该阈值时，流控机制开始工作。当 `enable` 的值为 `true` 时，会覆盖 `rocksdb.(defaultcf|writecf|lockcf).level0-slowdown-writes-trigger`的配置。\n+ 默认值：20\n\n### `soft-pending-compaction-bytes-limit`\n\n+ 当 KvDB 的 pending compaction bytes 达到该阈值时，流控机制开始拒绝部分写入请求，报错 `ServerIsBusy`。当 `enable` 的值为 `true` 时，会覆盖 `rocksdb.(defaultcf|writecf|lockcf).soft-pending-compaction-bytes-limit` 的配置。\n+ 默认值：\"192GiB\"\n\n### `hard-pending-compaction-bytes-limit`\n\n+ 当 KvDB 的 pending compaction bytes 达到该阈值时，流控机制拒绝所有写入请求，报错 `ServerIsBusy`。当 `enable` 的值为 `true` 时，会覆盖 `rocksdb.(defaultcf|writecf|lockcf).hard-pending-compaction-bytes-limit` 的配置。\n+ 默认值：\"1024GiB\"\n\n## storage.io-rate-limit\n\nI/O rate limiter 相关的配置项。\n\n### `max-bytes-per-sec`\n\n+ 限制服务器每秒从磁盘读取数据或写入数据的最大 I/O 字节数，I/O 类型由下面的 `mode` 配置项决定。达到该限制后，TiKV 倾向于放缓后台操作为前台操作节流。该配置项值应设为磁盘的最佳 I/O 带宽，例如云盘厂商指定的最大 I/O 带宽。\n+ 默认值：\"0MiB\"\n\n### `mode`\n\n+ 确定哪些类型的 I/O 操作被计数并受 `max-bytes-per-sec` 阈值的限流。当前 TiKV 只支持 write-only 只写模式。\n+ 可选值：`\"read-only\"`，`\"write-only\"`，`\"all-io\"`\n+ 默认值：`\"write-only\"`\n\n## pd\n\n### `enable-forwarding` <span class=\"version-mark\">从 v5.0.0 版本开始引入</span>\n\n+ 控制 TiKV 中的 PD client 在疑似网络隔离的情况下是否通过 follower 将请求转发给 leader。\n+ 默认值：false\n+ 如果确认环境存在网络隔离的可能，开启这个参数可以减少服务不可用的窗口期。\n+ 如果无法准确判断隔离、网络中断、宕机等情况，这个机制存在误判情况从而导致可用性、性能降低。如果网络中从未发生过网络故障，不推荐开启此选项。\n\n### `endpoints`\n\n+ PD 的地址。当指定多个地址时，需要用逗号 `,` 分隔。\n+ 默认值：`[\"127.0.0.1:2379\"]`\n\n### `retry-interval`\n\n+ 设置 PD 连接的重试间隔。\n+ 默认值：`\"300ms\"`\n\n### `retry-log-every`\n\n+ 指定 PD 客户端在观察到错误时跳过报错的频率。例如，当配置项值为 `5` 时，每次 PD 观察到错误时，将跳过 4 次报错，直到第 5 次错误时才报告。\n+ 要禁用此功能，请将值设置为 `1`。\n+ 默认值：`10`\n\n### `retry-max-count`\n\n+ 初始化 PD 连接的最大重试次数。\n+ 要禁用重试，请将该值设置为 `0`。要解除重试次数的限制，请将该值设置为 `-1`。\n+ 默认值：`-1`\n\n## raftstore\n\nraftstore 相关的配置项。\n\n### `prevote`\n\n+ 开启 Prevote 的开关，开启有助于减少隔离恢复后对系统造成的抖动。\n+ 默认值：true\n\n### `capacity`\n\n+ 存储容量，即允许的最大数据存储大小。如果没有设置，则使用当前磁盘容量。如果要将多个 TiKV 实例部署在同一块物理磁盘上，需要在 TiKV 配置中添加该参数，参见[混合部署的关键参数介绍](/hybrid-deployment-topology.md#混合部署的关键参数介绍)。\n+ 默认值：0\n+ 单位：KiB|MiB|GiB\n\n### `raftdb-path`\n\n+ raft 库的路径，默认存储在 storage.data-dir/raft 下。\n+ 默认值：\"\"\n\n### `raft-base-tick-interval`\n\n> **注意：**\n>\n> 该配置项不支持通过 SQL 语句查询，但支持在配置文件中进行配置。\n\n+ 状态机 tick 一次的间隔时间。\n+ 默认值：1s\n+ 最小值：大于 0\n\n### `raft-heartbeat-ticks`\n\n> **注意：**\n>\n> 该配置项不支持通过 SQL 语句查询，但支持在配置文件中进行配置。\n\n+ 发送心跳时经过的 tick 个数，即每隔 raft-base-tick-interval * raft-heartbeat-ticks 时间发送一次心跳。\n+ 默认值：2\n+ 最小值：大于 0\n\n### `raft-election-timeout-ticks`\n\n> **注意：**\n>\n> 该配置项不支持通过 SQL 语句查询，但支持在配置文件中进行配置。\n\n+ 发起选举时经过的 tick 个数，即如果处于无主状态，大约经过 raft-base-tick-interval * raft-election-timeout-ticks 时间以后发起选举。\n+ 默认值：10\n+ 最小值：raft-heartbeat-ticks\n\n### `raft-min-election-timeout-ticks`\n\n> **注意：**\n>\n> 该配置项不支持通过 SQL 语句查询，但支持在配置文件中进行配置。\n\n+ 发起选举时至少经过的 tick 个数，如果为 0，则表示使用 raft-election-timeout-ticks，不能比 raft-election-timeout-ticks 小。\n+ 默认值：0\n+ 最小值：0\n\n### `raft-max-election-timeout-ticks`\n\n> **注意：**\n>\n> 该配置项不支持通过 SQL 语句查询，但支持在配置文件中进行配置。\n\n+ 发起选举时最多经过的 tick 个数，如果为 0，则表示使用 raft-election-timeout-ticks * 2。\n+ 默认值：0\n+ 最小值：0\n\n### `raft-max-size-per-msg`\n\n+ 产生的单个消息包的大小限制，软限制。\n+ 默认值：1MiB\n+ 最小值：大于 0\n+ 最大值: 3GiB\n+ 单位：KiB|MiB|GiB\n\n### `raft-max-inflight-msgs`\n\n+ 待确认的日志个数，如果超过这个数量，Raft 状态机会减缓发送日志的速度。\n+ 默认值：256\n+ 最小值：大于 0\n+ 最大值: 16384\n\n### `raft-entry-max-size`\n\n+ 单个日志最大大小，硬限制。\n+ 默认值：8MiB\n+ 最小值：0\n+ 单位：MiB|GiB\n\n### `raft-log-compact-sync-interval` <span class=\"version-mark\">从 v5.3 版本开始引入</span>\n\n+ 压缩非必要 Raft 日志的时间间隔。\n+ 默认值：\"2s\"\n+ 最小值：\"0s\"\n\n### `raft-log-gc-tick-interval`\n\n+ 删除 Raft 日志的轮询任务调度间隔时间，0 表示不启用。\n+ 默认值：\"3s\"\n+ 最小值：\"0s\"\n\n### `raft-log-gc-threshold`\n\n+ 允许残余的 Raft 日志个数，这是一个软限制。\n+ 默认值：50\n+ 最小值：1\n\n### `raft-log-gc-count-limit`\n\n+ 允许残余的 Raft 日志个数，这是一个硬限制。\n+ 默认值：3/4 Region 大小所能容纳的日志个数，按照每个日志 1 MiB 计算\n+ 最小值：0\n\n### `raft-log-gc-size-limit`\n\n+ 允许残余的 Raft 日志大小，这是一个硬限制。\n+ 默认值：Region 大小的 3/4\n+ 最小值：大于 0\n\n### `raft-log-reserve-max-ticks` <span class=\"version-mark\">从 v5.3 版本开始引入</span>\n\n+ 超过本配置项设置的的 tick 数后，即使剩余 Raft 日志的数量没有达到 `raft-log-gc-threshold` 设置的值，TiKV 也会进行 GC 操作。\n+ 默认值：6\n+ 最小值：大于 0\n\n### `raft-engine-purge-interval`\n\n+ 清除旧的 TiKV 日志文件的间隔时间，以尽快回收磁盘空间。Raft 引擎是可替换的组件，因此某些功能或优化的实现需要清除 TiKV 日志文件。\n+ 默认值：`\"10s\"`\n\n### `raft-entry-cache-life-time`\n\n+ 内存中日志 cache 允许的最长残留时间。\n+ 默认值：30s\n+ 最小值：0\n\n### `max-apply-unpersisted-log-limit` <span class=\"version-mark\">从 v8.1.0 版本开始引入</span>\n\n+ 允许 apply 已经 `commit` 但尚未持久化的 Raft 日志的最大数量。\n\n    + 将此配置项设置为大于 0 的值将使该 TiKV 节点能够提前 apply 已 `commit` 但尚未持久化的 Raft 日志，从而有效降低该节点上因 IO 抖动导致的长尾延迟。但这也可能会增加 TiKV 内存使用量和 Raft 日志占用的磁盘容量。\n    + 将此配置项设置为 0 则表示关闭此特性，此时 TiKV 需要等待 Raft 日志被 `commit` 且持久化之后才能对其进行 apply，此行为与 v8.2.0 之前版本的行为一致。\n\n+ 默认值：1024\n+ 最小值：0\n\n### `hibernate-regions`\n\n+ 打开或关闭静默 Region。打开后，如果 Region 长时间处于非活跃状态，即被自动设置为静默状态。静默状态的 Region 可以降低 Leader 和 Follower 之间心跳信息的系统开销。可以通过 `peer-stale-state-check-interval` 调整 Leader 和 Follower 之间的心跳间隔。\n+ 默认值：v5.0.2 及以后版本默认值为 true，v5.0.2 以前的版本默认值为 false\n\n### `split-region-check-tick-interval`\n\n+ 检查 Region 是否需要分裂的时间间隔，0 表示不启用。\n+ 默认值：10s\n+ 最小值：0\n\n### `region-split-check-diff`\n\n+ 允许 Region 数据超过指定大小的最大值。\n+ 默认值：Region 大小的 1/16\n+ 最小值：0\n\n### `region-compact-check-interval`\n\n+ 检查是否需要人工触发 RocksDB compaction 的时间间隔，0 表示不启用。\n+ 默认值：5m\n+ 最小值：0\n\n### `region-compact-check-step`\n\n+ 每轮校验人工 compaction 时，一次性检查的 Region 个数。\n+ 默认值：\n    + 当 `storage.engine=\"raft-kv\"` 时，默认值为 100。\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为 5。\n\n### `region-compact-min-tombstones`\n\n+ 触发 RocksDB compaction 需要的 tombstone 个数。\n+ 默认值：10000\n+ 最小值：0\n\n### `region-compact-tombstones-percent`\n\n+ 触发 RocksDB compaction 需要的 tombstone 所占比例。\n+ 默认值：30\n+ 最小值：1\n+ 最大值：100\n\n### `region-compact-min-redundant-rows` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n+ 触发 RocksDB compaction 需要的冗余的 MVCC 数据行数。\n+ 默认值：`50000`\n+ 最小值：`0`\n\n### `region-compact-redundant-rows-percent` <span class=\"version-mark\">从 v7.1.0 版本开始引入</span>\n\n+ 触发 RocksDB compaction 需要的冗余的 MVCC 数据行所占比例。\n+ 默认值：`20`\n+ 最小值：`1`\n+ 最大值：`100`\n\n### `report-region-buckets-tick-interval` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n> **警告：**\n>\n> `report-region-buckets-tick-interval` 是 TiDB 在 v6.1.0 中引入的实验特性，不建议在生产环境中使用。\n\n+ 启用 `enable-region-bucket` 后，该配置项设置 TiKV 向 PD 上报 bucket 信息的间隔时间。\n+ 默认值：10s\n\n### `pd-heartbeat-tick-interval`\n\n+ 触发 Region 对 PD 心跳的时间间隔，0 表示不启用。\n+ 默认值：1m\n+ 最小值：0\n\n### `pd-store-heartbeat-tick-interval`\n\n+ 触发 store 对 PD 心跳的时间间隔，0 表示不启用。\n+ 默认值：10s\n+ 最小值：0\n\n### `snap-mgr-gc-tick-interval`\n\n+ 触发回收过期 snapshot 文件的时间间隔，0 表示不启用。\n+ 默认值：1m\n+ 最小值：0\n\n### `snap-gc-timeout`\n\n+ snapshot 文件的最长保存时间。\n+ 默认值：4h\n+ 最小值：0\n\n### `snap-generator-pool-size` <span class=\"version-mark\">从 v5.4.0 版本开始引入</span>\n\n+ 用于配置 `snap-generator` 线程池的大小。\n+ 为了让 TiKV 在恢复场景下加快 Region 生成 Snapshot 的速度，需要调大对应 Worker 的 `snap-generator` 线程数量。可通过本配置项调大对应线程的数量。\n+ 默认值：`2`\n+ 最小值：`1`\n\n### `lock-cf-compact-interval`\n\n+ 触发对 lock CF compact 检查的时间间隔。\n+ 默认值：10m\n+ 最小值：0\n\n### `lock-cf-compact-bytes-threshold`\n\n+ 触发对 lock CF 进行 compact 的大小。\n+ 默认值：256MiB\n+ 最小值：0\n+ 单位：MiB\n\n### `notify-capacity`\n\n+ Region 消息队列的最长长度。\n+ 默认值：40960\n+ 最小值：0\n\n### `messages-per-tick`\n\n+ 每轮处理的消息最大个数。\n+ 默认值：4096\n+ 最小值：0\n\n### `max-peer-down-duration`\n\n+ 副本允许的最长未响应时间，超过将被标记为 down，后续 PD 会尝试将其删掉。\n+ 默认值：10m\n+ 最小值：当 Hibernate Region 功能启用时，为 peer-stale-state-check-interval * 2；Hibernate Region 功能关闭时，为 0。\n\n### `max-leader-missing-duration`\n\n+ 允许副本处于无主状态的最长时间，超过将会向 PD 校验自己是否已经被删除。\n+ 默认值：2h\n+ 最小值：> abnormal-leader-missing-duration\n\n### `abnormal-leader-missing-duration`\n\n+ 允许副本处于无主状态的时间，超过将视为异常，标记在 metrics 和日志中。\n+ 默认值：10m\n+ 最小值：> peer-stale-state-check-interval\n\n### `peer-stale-state-check-interval`\n\n+ 触发检验副本是否处于无主状态的时间间隔。\n+ 默认值：5m\n+ 最小值：> 2 * election-timeout\n\n### `leader-transfer-max-log-lag`\n\n+ 尝试转移领导权时被转移者允许的最大日志缺失个数。\n+ 默认值：128\n+ 最小值：10\n\n### `max-snapshot-file-raw-size` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n+ 当 snapshot 文件大于该配置项指定的大小时，snapshot 文件会被切割为多个文件。\n+ 默认值：100MiB\n+ 最小值：100MiB\n\n### `snap-apply-batch-size`\n\n+ 当导入 snapshot 文件需要写数据时，内存写缓存的大小。\n+ 默认值：10MiB\n+ 最小值：0\n+ 单位：MiB\n\n### `consistency-check-interval`\n\n> **警告：**\n>\n> 开启一致性检查对集群性能有影响，并且和 TiDB GC 操作不兼容，不建议在生产环境中使用。\n\n+ 触发一致性检查的时间间隔，0 表示不启用。\n+ 默认值：0s\n+ 最小值：0\n\n### `raft-store-max-leader-lease`\n\n+ Region 主可信任期的最长时间。\n+ 默认值：9s\n+ 最小值：0\n\n### `right-derive-when-split`\n\n+ 指定 Region 分裂时新 Region 的起始 key。当此配置项设置为 `true` 时，起始 key 为最大分裂 key；当此配置项设置为 `false` 时，起始 key 为原 Region 的起始 key。\n+ 默认值：true\n\n### `merge-max-log-gap`\n\n+ 进行 merge 时，允许的最大日志缺失个数。\n+ 默认值：10\n+ 最小值：> raft-log-gc-count-limit\n\n### `merge-check-tick-interval`\n\n+ 触发 merge 完成检查的时间间隔。\n+ 默认值：2s\n+ 最小值：大于 0\n\n### `use-delete-range`\n\n+ 开启 rocksdb delete_range 接口删除数据的开关。\n+ 默认值：false\n\n### `cleanup-import-sst-interval`\n\n+ 触发检查过期 SST 文件的时间间隔，0 表示不启用。\n+ 默认值：10m\n+ 最小值：0\n\n### `local-read-batch-size`\n\n+ 一轮处理读请求的最大个数。\n+ 默认值：1024\n+ 最小值：大于 0\n\n### `apply-yield-write-size` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ Apply 线程每一轮处理单个状态机写入的最大数据量，这是个软限制。\n+ 默认值：32KiB\n+ 最小值：大于 0\n+ 单位：KiB|MiB|GiB\n\n### `apply-max-batch-size`\n\n+ Raft 状态机由 BatchSystem 批量执行数据写入请求，该配置项指定每批可执行请求的最多 Raft 状态机个数。\n+ 默认值：256\n+ 最小值：大于 0\n+ 最大值: 10240\n\n### `apply-pool-size`\n\n+ Apply 线程池负责把数据落盘至磁盘。该配置项为 Apply 线程池中线程的数量，即 Apply 线程池的大小。调整 Apply 线程池的大小时，请参考 [TiKV 线程池调优](/tune-tikv-thread-performance.md#tikv-线程池调优)。\n+ 默认值：2\n+ 可调整范围：[1, CPU * 10]\n\n### `store-max-batch-size`\n\n+ Raft 状态机由 BatchSystem 批量执行把日志落盘至磁盘的请求，该配置项指定每批可执行请求的最多 Raft 状态机个数。\n+ 如果开启 `hibernate-regions`，默认值为 256；如果关闭 `hibernate-regions`，默认值为 1024\n+ 最小值：大于 0\n+ 最大值: 10240\n\n### `store-pool-size`\n\n+ 表示处理 Raft 的线程池中线程的数量，即 Raftstore 线程池的大小。调整该线程池的大小时，请参考 [TiKV 线程池调优](/tune-tikv-thread-performance.md#tikv-线程池调优)。\n+ 默认值：2\n+ 可调整范围：[1, CPU * 10]\n\n### `store-io-pool-size` <span class=\"version-mark\">从 v5.3.0 版本开始引入</span>\n\n+ 表示处理 Raft I/O 任务的线程池中线程的数量，即 StoreWriter 线程池的大小。调整该线程池的大小时，请参考 [TiKV 线程池调优](/tune-tikv-thread-performance.md#tikv-线程池调优)。\n+ 默认值：1（对于 TiDB v8.0.0 之前的版本，默认值为 0）\n+ 最小值：0\n\n### `future-poll-size`\n\n+ 驱动 future 的线程池中线程的数量。\n+ 默认值：1\n+ 最小值：大于 0\n\n### `cmd-batch`\n\n+ 对请求进行攒批的控制开关，开启后可显著提升写入性能。\n+ 默认值：true\n\n### `inspect-interval`\n\n+ TiKV 每隔一段时间会检测 Raftstore 组件的延迟情况，该配置项设置检测的时间间隔。当检测的延迟超过该时间，该检测会被记为超时。\n+ 根据超时的检测延迟的比例计算判断 TiKV 是否为慢节点。\n+ 默认值：100ms\n+ 最小值：1ms\n\n### `raft-write-size-limit` <span class=\"version-mark\">从 v5.3.0 版本开始引入</span>\n\n+ 触发 Raft 数据写入的阈值。当数据大小超过该配置项值，数据会被写入磁盘。当 `store-io-pool-size` 的值为 `0` 时，该配置项不生效。\n+ 默认值：1MiB\n+ 最小值：0\n\n### `report-min-resolved-ts-interval` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n+ 设置 PD leader 收到 Resolved TS 的间隔时间。如果该值设置为 `0`，表示禁用该功能。\n+ 默认值：在 v6.3.0 之前版本中为 `\"0s\"`，在 v6.3.0 及之后的版本中为 `\"1s\"`，即最小正值。\n+ 最小值：0\n+ 单位：秒\n\n### `evict-cache-on-memory-ratio` <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n+ 当 TiKV 的内存使用超过系统可用内存的 90%，并且 Raft 缓存条目占用的内存超过已使用内存 * `evict-cache-on-memory-ratio` 时，TiKV 会逐出 Raft 缓存条目。\n+ 设置为 `0` 表示禁用该功能。\n+ 默认值：0.1\n+ 最小值：0\n\n### `periodic-full-compact-start-times` <span class=\"version-mark\">从 v7.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> 周期性全量数据整理目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 设置 TiKV 启动周期性全量数据整理 (Compaction) 的时间。你可以在数组中指定一个或多个时间计划。例如：\n    + `periodic-full-compact-start-times = [\"03:00\", \"23:00\"]` 表示 TiKV 基于 TiKV 节点的本地时区，在每天凌晨 3 点和晚上 11 点进行全量数据整理。\n    + `periodic-full-compact-start-times = [\"03:00 +0000\", \"23:00 +0000\"]` 表示 TiKV 在每天 UTC 时间的凌晨 3 点和晚上 11 点进行全量数据整理。\n    + `periodic-full-compact-start-times = [\"03:00 +0800\", \"23:00 +0800\"]` 表示 TiKV 在每天 UTC+08:00 时间的凌晨 3 点和晚上 11 点进行全量数据整理。\n+ 默认值：`[]`，表示默认情况下禁用周期性全量数据整理。\n\n### `periodic-full-compact-start-max-cpu` <span class=\"version-mark\">从 v7.6.0 版本开始引入</span>\n\n+ 控制 TiKV 执行周期性全量数据整理时的 CPU 使用率阈值。\n+ 默认值：`0.1`，表示全量数据整理进程的最大 CPU 使用率为 10%。\n\n## coprocessor\n\nCoprocessor 相关的配置项。\n\n### `split-region-on-table`\n\n+ 开启按 table 分裂 Region 的开关，建议仅在 TiDB 模式下使用。\n+ 默认值：false\n\n### `batch-split-limit`\n\n+ 批量分裂 Region 的阈值，调大该值可加速分裂 Region。\n+ 默认值：10\n+ 最小值：1\n\n### `region-max-size`\n\n+ Region 容量空间最大值，超过时系统分裂成多个 Region。\n+ 默认值：`region-split-size / 2 * 3`\n+ 单位：KiB|MiB|GiB\n\n### `region-split-size`\n\n+ 分裂后新 Region 的大小，此值属于估算值。\n+ 默认值：`\"256MiB\"`。在 v8.4.0 之前，默认值为 `\"96MiB\"`。\n+ 单位：KiB|MiB|GiB\n\n### `region-max-keys`\n\n+ Region 最多允许的 key 的个数，超过时系统分裂成多个 Region。\n+ 默认值：`region-split-keys / 2 * 3`\n\n### `region-split-keys`\n\n+ 分裂后新 Region 的 key 的个数，此值属于估算值。\n+ 默认值：`2560000`。在 v8.4.0 之前，默认值为 `960000`。\n\n### `consistency-check-method`\n\n+ 指定数据一致性检查的方法。\n+ 要对 MVCC 数据进行一致性检查，设置该值为 `\"mvcc\"`。要对原始数据进行一致性检查，设置该值为 `\"raw\"`。\n+ 默认值：`\"mvcc\"`\n\n## coprocessor-v2\n\n### `coprocessor-plugin-directory`\n\n+ 已编译 coprocessor 插件所在目录的路径。TiKV 会自动加载该目录下的插件。\n+ 如果未设置该配置项，则 coprocessor 插件会被禁用。\n+ 默认值：无\n\n### `enable-region-bucket` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n+ 是否将 Region 划分为更小的区间 bucket，并且以 bucket 作为并发查询单位，以提高扫描数据的并发度。bucket 的详细设计可见 [Dynamic size Region](https://github.com/tikv/rfcs/blob/master/text/0082-dynamic-size-region.md)。\n+ 默认值：无，表示默认关闭。\n\n> **警告：**\n>\n> - `enable-region-bucket` 是 TiDB 在 v6.1.0 中引入的实验特性，不建议在生产环境中使用。\n> - 这个参数仅在 `region-split-size` 调到两倍 `region-bucket-size` 及以上时才有意义，否则不会真正生成 bucket。\n> - 将 `region-split-size` 调大可能会有潜在的性能回退、数据调度缓慢的风险。\n\n### `region-bucket-size` <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n+ 设置 `enable-region-bucket` 启用时 bucket 的预期大小。\n+ 默认值：从 v7.3.0 起，默认值从 `96MiB` 变更为 `50MiB`。\n\n> **警告：**\n>\n> `region-bucket-size` 是 TiDB 在 v6.1.0 中引入的实验特性，不建议在生产环境中使用。\n\n## rocksdb\n\nRocksDB 相关的配置项。\n\n### `max-background-jobs`\n\n+ RocksDB 后台线程个数。调整 RocksDB 线程池的大小时，请参考 [TiKV 线程池调优](/tune-tikv-thread-performance.md#tikv-线程池调优)。\n+ 默认值：\n    + CPU 核数为 10 时，默认值为 `9`\n    + CPU 核数为 8 时，默认值为 `7`\n    + CPU 核数为 `N` 时，默认值为 `max(2, min(N - 1, 9))`\n+ 最小值：2\n\n### `max-background-flushes`\n\n+ RocksDB 用于刷写 memtable 的最大后台线程数量。\n+ 默认值：\n    + CPU 核数为 10 时，默认值为 `3`\n    + CPU 核数为 8 时，默认值为 `2`\n    + CPU 核数为 `N` 时，默认值为 `[(max-background-jobs + 3) / 4]`\n+ 最小值：1\n\n### `max-sub-compactions`\n\n+ RocksDB 进行 subcompaction 的并发个数。\n+ 默认值：3\n+ 最小值：1\n\n### `max-open-files`\n\n+ RocksDB 可以打开的文件总数。\n+ 默认值：40960\n+ 最小值：-1\n\n### `max-manifest-file-size`\n\n+ RocksDB Manifest 文件最大大小。\n+ 默认值：128MiB\n+ 最小值：0\n+ 单位：B|KiB|MiB|GiB\n\n### `create-if-missing`\n\n+ 自动创建 DB 开关。\n+ 默认值：true\n\n### `wal-recovery-mode`\n\n+ 预写式日志 (WAL, Write Ahead Log) 的恢复模式。\n+ 可选值：\n    + `\"tolerate-corrupted-tail-records\"`：容忍并丢弃位于日志尾部的不完整的数据 (trailing data)。\n    + `\"absolute-consistency\"`：当发现待恢复的日志中有被损坏的日志时，放弃恢复所有日志。\n    + `\"point-in-time\"`：按顺序恢复日志。遇到第一个损坏的日志时，停止恢复剩余的日志。\n    + `\"skip-any-corrupted-records\"`：灾难后恢复。跳过日志中的损坏记录，尽可能多地恢复数据。\n+ 默认值：`\"point-in-time\"`\n\n### `wal-dir`\n\n+ WAL 存储目录，若未指定，WAL 将存储在数据目录。\n+ 默认值：`\"\"`\n\n### `wal-ttl-seconds`\n\n+ 归档 WAL 生存周期，超过该值时，系统会删除相关 WAL。\n+ 默认值：0\n+ 最小值：0\n+ 单位：秒\n\n### `wal-size-limit`\n\n+ 归档 WAL 大小限制，超过该值时，系统会删除相关 WAL。\n+ 默认值：0\n+ 最小值：0\n+ 单位：B|KiB|MiB|GiB\n\n### `max-total-wal-size`\n\n+ RocksDB WAL 总大小限制，即 `data-dir` 目录下 `*.log` 文件的大小总和。\n+ 默认值：\n    + 当 `storage.engine=\"raft-kv\"` 时，默认值为 `\"4GiB\"`\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为 `1`\n\n### `stats-dump-period`\n\n+ 将统计信息输出到日志中的间隔时间。\n+ 默认值：\n\n    + 当 `storage.engine=\"raft-kv\"` 时，默认值为 `\"10m\"`。\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为 `\"0\"`。\n\n### `compaction-readahead-size`\n\n+ 开启 RocksDB compaction 过程中的预读功能，该项指定预读数据的大小。如果使用的是机械磁盘，建议该值至少为 2MiB。\n+ 默认值：0\n+ 最小值：0\n+ 单位：B|KiB|MiB|GiB\n\n### `writable-file-max-buffer-size`\n\n+ WritableFileWrite 所使用的最大的 buffer 大小。\n+ 默认值：1MiB\n+ 最小值：0\n+ 单位：B|KiB|MiB|GiB\n\n### `use-direct-io-for-flush-and-compaction`\n\n+ 决定后台 flush 或者 compaction 的读写是否设置 O_DIRECT 的标志。该选项对性能的影响：开启 O_DIRECT 可以绕过并防止污染操作系统 buffer cache，但后续文件读取需要把内容重新读到 buffer cache。\n+ 默认值：false\n\n### `rate-bytes-per-sec`\n\n+ 未开启 Titan 时，限制 RocksDB Compaction 的 I/O 速率，以达到在流量高峰时，限制 RocksDB Compaction 减少其 I/O 带宽和 CPU 消耗对前台读写性能的影响。开启 Titan 时，限制 RocksDB Compaction 和 Titan GC 的 I/O 速率总和。当发现在流量高峰时 RocksDB Compaction 和 Titan GC 的 I/O 和/或 CPU 消耗过大，可以根据磁盘 I/O 带宽和实际写入流量适当配置这个选项。\n+ 默认值：10GiB\n+ 最小值：0\n+ 单位：B|KiB|MiB|GiB\n\n### `rate-limiter-refill-period`\n\n+ 控制 I/O 令牌的刷新频率。较小的值可以减少 I/O 尖刺，但会增加 CPU 开销。\n+ 默认值：`\"100ms\"`\n\n### `rate-limiter-mode`\n\n+ RocksDB 的 compaction rate limiter 模式。\n+ 可选值：\"read-only\"，\"write-only\"，\"all-io\"\n+ 默认值：\"write-only\"\n\n### `rate-limiter-auto-tuned` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 控制是否依据最近的负载量自动优化 RocksDB 的 compaction rate limiter 配置。此配置项开启后，compaction pending bytes 监控指标值会比一般情况下稍微高些。\n+ 默认值：true\n\n### `enable-pipelined-write`\n\n+ 控制是否开启 Pipelined Write。开启时会使用旧的 Pipelined Write，关闭时会使用新的 Pipelined Commit 机制。\n+ 默认值：false\n\n### `bytes-per-sync`\n\n+ 异步 Sync 限速速率。\n+ 默认值：1MiB\n+ 最小值：0\n+ 单位：B|KiB|MiB|GiB\n\n### `wal-bytes-per-sync`\n\n+ WAL Sync 限速速率。\n+ 默认值：512KiB\n+ 最小值：0\n+ 单位：B|KiB|MiB|GiB\n\n### `info-log-max-size`\n\n> **警告：**\n>\n> 自 v5.4.0 起，RocksDB 的日志改为由 TiKV 的日志模块进行管理，因此该配置项被废弃，其功能由配置参数 [`log.file.max-size`](#max-size-从-v540-版本开始引入) 代替。\n\n+ Info 日志的最大大小。\n+ 默认值：1GiB\n+ 最小值：0\n+ 单位：B|KiB|MiB|GiB\n\n### `info-log-roll-time`\n\n> **警告：**\n>\n> 自 v5.4.0 起，RocksDB 的日志改为由 TiKV 的日志模块进行管理，因此该配置项被废弃。TiKV 不再支持按照时间自动切分日志，请使用配置参数 [`log.file.max-size`](#max-size-从-v540-版本开始引入) 配置按照文件大小自动切分日志的阈值。\n\n+ 日志截断间隔时间，如果为 0s 则不截断。\n+ 默认值：0s\n\n### `info-log-keep-log-file-num`\n\n> **警告：**\n>\n> 自 v5.4.0 起，RocksDB 的日志改为由 TiKV 的日志模块进行管理，因此该配置项被废弃，其功能由配置参数 [`log.file.max-backups`](#max-backups-从-v540-版本开始引入) 代替。\n\n+ 保留日志文件最大个数。\n+ 默认值：10\n+ 最小值：0\n\n### `info-log-dir`\n\n+ 日志存储目录。\n+ 默认值：\"\"\n\n### `info-log-level`\n\n> **警告：**\n>\n> 自 v5.4.0 起，RocksDB 的日志改为由 TiKV 的日志模块进行管理，因此该配置项被废弃，其功能由配置参数 [`log.level`](#level-从-v540-版本开始引入) 代替。\n\n+ RocksDB 的日志级别。\n+ 默认值：`\"info\"`\n\n### `write-buffer-flush-oldest-first` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> 该功能目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 设置当 RocksDB 当前 memtable 内存占用达到阈值之后的 Flush 策略。\n+ 默认值：`false`\n+ 可选值：\n    + `false`：Flush 策略是优先选择数据量大的 memtable 落盘到 SST。\n    + `true`：Flush 策略是优先选择最早的 memtable 落盘到 SST。该策略可以清除冷数据的 memtable，用于有明显冷热数据的场景。\n\n### `write-buffer-limit` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n> **警告：**\n>\n> 该功能目前为实验特性，不建议在生产环境中使用。该功能可能会在未事先通知的情况下发生变化或删除。如果发现 bug，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n\n+ 设置单个 TiKV 中所有 RocksDB 实例使用的 memtable 的总内存上限。`0` 表示不设限制。\n+ 默认值：\n\n    + 当 `storage.engine=\"raft-kv\"` 时，无默认值，即不限制。\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为本机内存的 20%。\n\n+ 单位：KiB|MiB|GiB\n\n### `track-and-verify-wals-in-manifest` <span class=\"version-mark\">从 v6.5.9、v7.1.5、v7.5.2、v8.0.0 版本开始引入</span>\n\n+ 控制是否在 RocksDB 的 MANIFEST 文件中记录 WAL (Write Ahead Log) 文件的信息，以及在启动时是否验证 WAL 文件的完整性。详情请参考 RocksDB [Track WAL in MANIFEST](https://github.com/facebook/rocksdb/wiki/Track-WAL-in-MANIFEST)。\n+ 默认值：`true`\n+ 可选值：\n    + `true`：在 MANIFEST 文件中记录 WAL 文件的信息，并在启动时验证 WAL 文件的完整性。\n    + `false`：不在 MANIFEST 文件中记录 WAL 文件的信息，而且不在启动时验证 WAL 文件的完整性。\n\n## rocksdb.titan\n\nTitan 相关的配置项。\n\n### `enabled`\n\n> **注意：**\n>\n> - 从 TiDB v7.6.0 开始，参数默认值从 `false` 变更为 `true`，即新集群默认开启 Titan，以更好地支持 TiDB 宽表写入场景和 JSON。\n> - 如果集群在升级到 TiDB v7.6.0 或更高版本之前未启用 Titan，则升级后将保持原有配置，继续使用 RocksDB，不会启用 Titan。\n> - 如果集群在升级到 TiDB v7.6.0 或更高版本之前已经启用了 Titan，则升级后将维持原有配置，保持启用 Titan 引擎，并保留升级前 [`min-blob-size`](/tikv-configuration-file.md#min-blob-size) 的配置。如果升级前没有显式配置该值，则升级后仍然保持了老版本默认值 `1KiB`，以确保升级后集群配置的稳定性。\n\n+ 开启 Titan 开关。\n+ 默认值：`true`\n\n### `dirname`\n\n+ Titan Blob 文件存储目录。\n+ 默认值：titandb\n\n### `disable-gc`\n\n+ 关闭 Titan 对 Blob 文件的 GC 的开关。\n+ 默认值：false\n\n### `max-background-gc`\n\n+ Titan 后台 GC 的线程个数，当从 **TiKV Details** > **Thread CPU** > **RocksDB CPU** 监控中观察到 Titan GC 线程长期处于满负荷状态时，应该考虑增加 Titan GC 线程池大小。\n+ 默认值：4\n+ 最小值：1\n\n## rocksdb.defaultcf | rocksdb.writecf | rocksdb.lockcf\n\nrocksdb defaultcf、rocksdb writecf 和 rocksdb lockcf 相关的配置项。\n\n### `block-size`\n\n+ 一个 RocksDB block 的默认大小。\n+ `defaultcf` 默认值：32KiB\n+ `writecf` 默认值：32KiB\n+ `lockcf` 默认值：16KiB\n+ 最小值：1KiB\n+ 单位：KiB|MiB|GiB\n\n### `block-cache-size`\n\n> **警告：**\n>\n> 从 v6.6.0 起，该配置项被废弃。\n\n+ 一个 RocksDB block 的默认缓存大小。\n+ `defaultcf` 默认值：机器总内存 * 25%\n+ `writecf` 默认值：机器总内存 * 15%\n+ `lockcf` 默认值：机器总内存 * 2%\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n\n### `disable-block-cache`\n\n+ 开启 block cache 开关。\n+ 默认值：false\n\n### `cache-index-and-filter-blocks`\n\n+ 开启缓存 index 和 filter 的开关。\n+ 默认值：true\n\n### `pin-l0-filter-and-index-blocks`\n\n+ 控制第 0 层 SST 文件的 index block 和 filter block 是否常驻在内存中的开关。\n+ 默认值：true\n\n### `use-bloom-filter`\n\n+ 开启 bloom filter 的开关。\n+ 默认值：true\n\n### `optimize-filters-for-hits`\n\n+ 开启优化 filter 的命中率的开关。\n+ `defaultcf` 默认值：`true`\n+ `writecf` 默认值：`false`\n+ `lockcf` 默认值：`false`\n\n### `optimize-filters-for-memory` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n+ 控制是否生成能够最小化内存碎片的 Bloom/Ribbon filter。\n+ 只有当 [`format-version`](#format-version-从-v620-版本开始引入) >= 5 时，该配置项才生效。\n+ 默认值：`false`\n\n### `whole-key-filtering`\n\n+ 开启将整个 key 放到 bloom filter 中的开关。\n+ `defaultcf` 默认值：`true`\n+ `writecf` 默认值：`false`\n+ `lockcf` 默认值：`true`\n\n### `bloom-filter-bits-per-key`\n\n+ bloom filter 为每个 key 预留的长度。\n+ 默认值：10\n+ 单位：字节\n\n### `block-based-bloom-filter`\n\n+ 开启每个 block 建立 bloom filter 的开关。\n+ 默认值：false\n\n### `ribbon-filter-above-level` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n+ 控制是否对于大于等于该值的 level 使用 Ribbon filter，对于小于该值的 level，使用非 block-based bloom filter。当该配置开启时，[`block-based-bloom-filter`](#block-based-bloom-filter) 将被忽略。\n+ 只有当 [`format-version`](#format-version-从-v620-版本开始引入) >= 5 时，该配置项才生效。\n+ 默认值：无，默认关闭。\n\n### `read-amp-bytes-per-bit`\n\n+ 开启读放大统计的开关，0：不开启，> 0 开启。\n+ 默认值：0\n+ 最小值：0\n\n### `compression-per-level`\n\n+ 每一层默认压缩算法。\n+ `defaultcf` 的默认值：[\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]\n+ `writecf` 的默认值：[\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]\n+ `lockcf` 的默认值：[\"no\", \"no\", \"no\", \"no\", \"no\", \"no\", \"no\"]\n\n### `bottommost-level-compression`\n\n+ 设置最底层的压缩算法。该设置将覆盖 `compression-per-level` 的设置。\n+ 因为最底层并非从数据开始写入 LSM-tree 起就直接采用 `compression-per-level` 数组中的最后一个压缩算法，使用 `bottommost-level-compression` 可以让最底层从一开始就使用压缩效果最好的压缩算法。\n+ 如果不想设置最底层的压缩算法，可以将该配置项的值设为 `disable`。\n+ 默认值：\"zstd\"\n\n### `write-buffer-size`\n\n+ memtable 大小。\n+ `defaultcf` 默认值：`\"128MiB\"`\n+ `writecf` 默认值：`\"128MiB\"`\n+ `lockcf` 默认值：\n    + 当 `storage.engine=\"raft-kv\"` 时，默认值为 `\"32MiB\"`\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为 `\"4MiB\"`\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n\n### `max-write-buffer-number`\n\n+ 最大 memtable 个数。当 `storage.flow-control.enable` 的值为 `true` 时，`storage.flow-control.memtables-threshold` 会覆盖此配置。\n+ 默认值：5\n+ 最小值：0\n\n### `min-write-buffer-number-to-merge`\n\n+ 触发 flush 的最小 memtable 个数。\n+ 默认值：1\n+ 最小值：0\n\n### `max-bytes-for-level-base`\n\n+ base level (L1) 最大字节数，一般设置为 memtable 大小 4 倍。当 L1 的数据量大小达到 `max-bytes-for-level-base` 限定的值的时候，会触发 L1 的 SST 文件和 L2 中有 overlap 的 SST 文件进行 compaction。\n+ `defaultcf` 默认值：`\"512MiB\"`\n+ `writecf` 默认值：`\"512MiB\"`\n+ `lockcf` 默认值：`\"128MiB\"`\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n+ 建议 `max-bytes-for-level-base` 的取值和 L0 的数据量大致相等，以减少不必要的 compaction。假如压缩方式为 \"no:no:lz4:lz4:lz4:lz4:lz4\"，那么 `max-bytes-for-level-base` 的值应该是 `write-buffer-size * 4`，因为 L0 和 L1 均没有压缩，且 L0 触发 compaction 的条件是 SST 文件的个数到达 4（默认值）。当 L0 和 L1 都发生了 compaction 时，需要分析 RocksDB 的日志了解由一个 memtable 压缩成的 SST 文件的大小。如果文件大小为 32MiB，那么 `max-bytes-for-level-base` 的值建议设为 32MiB * 4 = 128MiB。\n\n### `target-file-size-base`\n\n+ base level 的目标文件大小。当 `enable-compaction-guard` 的值为 `true` 时，`compaction-guard-max-output-file-size` 会覆盖此配置。\n+ 默认值：无，表示默认 8MiB\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n\n### `level0-file-num-compaction-trigger`\n\n+ 触发 compaction 的 L0 文件最大个数。\n+ `defaultcf` 默认值：`4`\n+ `writecf` 默认值：`4`\n+ `lockcf` 默认值：`1`\n+ 最小值：`0`\n\n### `level0-slowdown-writes-trigger`\n\n+ 触发 write stall 的 L0 文件最大个数。当 `storage.flow-control.enable` 的值为 `true` 时，`storage.flow-control.l0-files-threshold` 会覆盖此配置。\n+ 默认值：20\n+ 最小值：0\n\n### `level0-stop-writes-trigger`\n\n+ 完全阻停写入的 L0 文件最大个数。\n+ 默认值：36\n+ 最小值：0\n\n### `max-compaction-bytes`\n\n+ 一次 compaction 最大写入字节数。\n+ 默认值：2GiB\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n\n### `compaction-pri`\n\n+ 优先处理 compaction 的类型。\n+ 可选值：\n    + `\"by-compensated-size\"`：根据大小顺序，优先对大文件进行 compaction。\n    + `\"oldest-largest-seq-first\"`：根据时间顺序，优先对数据更新时间晚的文件进行 compaction。当你只在小范围内更新部分热点键 (hot keys) 时，可以使用此配置。\n    + `\"oldest-smallest-seq-first\"`：根据时间顺序，优先对长时间没有被 compact 到下一级的文件进行 compaction。如果你在大范围内随机更新了部分热点键，使用该配置可以轻微缓解写放大。\n    + `\"min-overlapping-ratio\"`：根据重叠比例，优先对在不同层之间文件重叠比例高的文件进行 compaction，即一个文件在 `下一层的大小`/`本层的大小` 的值越小，compaction 的优先级越高。在诸多场景下，该配置可以有效缓解写放大。\n+ 默认值：\n    + `defaultcf` 和 `writecf` 的默认值：`\"min-overlapping-ratio\"`\n    + `lockcf` 的默认值：`\"by-compensated-size\"`\n\n### `dynamic-level-bytes`\n\n+ 开启 dynamic level bytes 优化的开关。\n+ 默认值：true\n\n### `num-levels`\n\n+ RocksDB 文件最大层数。\n+ 默认值：7\n\n### `max-bytes-for-level-multiplier`\n\n+ 每一层的默认放大倍数。\n+ 默认值：10\n\n### `compaction-style`\n\n+ compaction 方法。\n+ 可选值：\"level\"，\"universal\"，\"fifo\"\n+ 默认值：\"level\"\n\n### `disable-auto-compactions`\n\n+ 是否关闭自动 compaction。\n+ 默认值：false\n\n### `soft-pending-compaction-bytes-limit`\n\n+ pending compaction bytes 的软限制。当 `storage.flow-control.enable` 的值为 `true` 时，`storage.flow-control.soft-pending-compaction-bytes-limit` 会覆盖此配置。\n+ 默认值：192GiB\n+ 单位：KiB|MiB|GiB\n\n### `hard-pending-compaction-bytes-limit`\n\n+ pending compaction bytes 的硬限制。当 `storage.flow-control.enable` 的值为 `true` 时，`storage.flow-control.hard-pending-compaction-bytes-limit` 会覆盖此配置。\n+ 默认值：256GiB\n+ 单位：KiB|MiB|GiB\n\n### `enable-compaction-guard`\n\n+ 设置 compaction guard 的启用状态。compaction guard 优化通过使用 TiKV Region 边界分割 SST 文件，帮助降低 compaction I/O，让 TiKV 能够输出较大的 SST 文件，并且在迁移 Region 时及时清理过期数据。\n+ `defaultcf` 默认值：`true`\n+ `writecf` 默认值：`true`\n+ `lockcf` 默认值：无，表示默认关闭\n\n### `compaction-guard-min-output-file-size`\n\n+ 设置 compaction guard 启用时 SST 文件大小的最小值，防止 SST 文件过小。\n+ 默认值：`\"8MiB\"`\n+ 单位：KiB|MiB|GiB\n\n### `compaction-guard-max-output-file-size`\n\n+ 设置 compaction guard 启用时 SST 文件大小的最大值，防止 SST 文件过大。对于同一列族，此配置项的值会覆盖 `target-file-size-base`。\n+ 默认值：128MiB\n+ 单位：KiB|MiB|GiB\n\n### `format-version` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 设置 SST 文件的格式版本。该配置项只影响新写入的表，对于已经存在的表，版本信息会从 footer 中读取。\n+ 可选值：\n    - `0`：适用于所有 TiKV 版本。默认 checksum 类型为 CRC32。该版本不支持修改 checksum 类型。\n    - `1`：适用于所有 TiKV 版本。支持使用非默认的 checksum 类型，例如 xxHash。只有在 checksum 类型不是 CRC32 时，RocksDB 才会写入数据。（`0` 版本会自动升级）\n    - `2`：适用于所有 TiKV 版本。更改了压缩块的编码方式，使用 LZ4、BZip2 和 Zlib 压缩。\n    - `3`：适用于 TiKV v2.1 及以上版本。更改了索引块中 key 的编码方式。\n    - `4`：适用于 TiKV v3.0 及以上版本。更改了索引块中 value 的编码方式。\n    - `5`：适用于 TiKV v6.1 及以上版本。全量和分区 filter 采用一种具有不同模式的、更快、更准确的 Bloom filter 实现。\n+ 默认值：\n\n    + 当 `storage.engine=\"raft-kv\"` 时，默认值为 `2`。\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为 `5`。\n\n### `ttl` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n+ 设置 SST 文件被自动选中执行 compaction 的 TTL 时间。更新时间超过此值的 SST 文件将被选中并进行 compaction。在执行 compaction 时，这些 SST 文件通常以级联的方式进行压缩，以便被压缩到最底层或最底层的文件中。\n+ 默认值：无，表示默认不选择任何 SST 文件。\n+ 单位：s(second)|h(hour)|d(day)\n\n### `periodic-compaction-seconds` <span class=\"version-mark\">从 v7.2.0 版本开始引入</span>\n\n+ 设置周期性 compaction 的时间。更新时间超过此值的 SST 文件将被选中进行 compaction，并被重新写入这些 SST 文件所在的层级。\n+ 默认值：无，表示默认不触发此 compaction。\n+ 单位：s(second)|h(hour)|d(day)\n\n## rocksdb.defaultcf.titan\n\n> **注意：**\n>\n> 仅支持在 `rocksdb.defaultcf` 启用 Titan，不支持在 `rocksdb.writecf` 启用 Titan。\n\nrocksdb defaultcf titan 相关的配置项。\n\n### `min-blob-size`\n\n> **注意：**\n>\n> - 为了提高宽表和 JSON 数据写入和点查性能，TiDB 从 v7.6.0 版本起默认启用 Titan，并将写入 Titan 的阈值参数 `min-blob-size` 的默认值从之前版本的 `1KiB` 调整为 `32KiB`，即当数据的 value 超过 `32KiB` 时，将存储在 Titan 中，而其他数据则继续存储在 RocksDB 中。\n> - 为了保证配置的连续性，已有集群升级到 TiDB v7.6.0 版本或者更高版本后，如果升级前用户未显式设置 `min-blob-size`，则维持使用老版本默认值 `1KiB`，以确保升级后集群配置的稳定性。\n> - 当参数被设置为小于 `32KiB` 时，TiKV 大范围扫描性能会受到一些影响。然而，如果负载主要是写入和点查为主，你可以适当调小 `min-blob-size` 的值以获取更好的写入和点查性能。\n\n+ 最小存储在 Blob 文件中 value 大小，低于该值的 value 还是存在 LSM-Tree 中。\n+ 默认值：无，表示默认 32KiB。\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n\n### `blob-file-compression`\n\n> **注意：**\n>\n> - Snappy 压缩文件必须遵循[官方 Snappy 格式](https://github.com/google/snappy)。不支持其他非官方压缩格式。\n> - TiDB v7.5.0 及更早的版本，参数默认值为 `lz4`。TiDB v7.6.0 及更高版本，参数默认值调整为 `zstd`。\n\n+ Blob 文件所使用的压缩算法，可选值：no、snappy、zlib、bz2、lz4、lz4hc、zstd。\n+ 默认值：zstd\n\n### `zstd-dict-size`\n\n+ 指定 zstd 字典大小，默认为 `\"0KiB\"`，表示关闭 zstd 字典压缩，也就是说 Titan 中压缩的是单个 value 值，而 RocksDB 压缩以 Block（默认值为 `32KiB`）为单位。因此当关闭字典压缩、且 value 平均小于 `32KiB` 时，Titan 的压缩率低于 RocksDB。以 JSON 内容为例，Titan 的 Store Size 可能比 RocksDB 高 30% 至 50%。实际压缩率还取决于 value 内容是否适合压缩，以及不同 value 之间的相似性。你可以通过设置 `zstd-dict-size`（比如 `16KiB`）启用 zstd 字典以大幅提高压缩率（实际 Store Size 可以低于 RocksDB），但 zstd 字典压缩在有些负载下会有 10% 左右的性能损失。\n+ 默认值：`\"0KiB\"`\n+ 单位：KiB|MiB|GiB\n\n### `blob-cache-size`\n\n+ Blob 文件的 cache 大小。\n+ 默认值：0GiB\n+ 最小值：0\n+ 推荐值：0。从 v8.0.0 开始，TiKV 引入了 `shared-blob-cache` 配置项并默认开启，因此无需再单独设置 `blob-cache-size`。只有当 `shared-blob-cache` 设置为 `false` 时，`blob-cache-size` 的设置才生效。\n+ 单位：KiB|MiB|GiB\n\n### `shared-blob-cache` <span class=\"version-mark\">从 v8.0.0 版本开始引入</span>\n\n+ 是否启用 Titan Blob 文件和 RocksDB Block 文件的共享缓存\n+ 默认值：`true`。当开启共享缓存时，Block 文件具有更高的优先级，TiKV 将优先满足 Block 文件的缓存需求，然后将剩余的缓存用于 Blob 文件。\n\n### `min-gc-batch-size`\n\n+ 做一次 GC 所要求的最低 Blob 文件大小总和。\n+ 默认值：16MiB\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n\n### `max-gc-batch-size`\n\n+ 做一次 GC 所要求的最高 Blob 文件大小总和。\n+ 默认值：64MiB\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n\n### `discardable-ratio`\n\n+ 当一个 blob file 中无用数据（相应的 key 已经被更新或删除）比例超过以下阈值时，将会触发 Titan GC。将此文件有用的数据重写到另一个文件。这个值可以估算 Titan 的写放大和空间放大的上界（假设关闭压缩）。公式是：\n\n    写放大上界 = 1 / `discardable-ratio`\n\n    空间放大上界 = 1 / (1 - `discardable-ratio`)\n\n    可以看到，减少这个阈值可以减少空间放大，但是会造成 Titan 更频繁 GC；增加这个值可以减少 Titan GC，减少相应的 I/O 带宽和 CPU 消耗，但是会增加磁盘空间占用。\n\n+ 默认值：0.5\n+ 最小值：0\n+ 最大值：1\n\n### `sample-ratio`\n\n+ 进行 GC 时，对 Blob 文件进行采样时读取数据占整个文件的比例。\n+ 默认值：0.1\n+ 最小值：0\n+ 最大值：1\n\n### `merge-small-file-threshold`\n\n+ Blob 文件的大小小于该值时，无视 discardable-ratio 仍可能被 GC 选中。\n+ 默认值：8MiB\n+ 最小值：0\n+ 单位：KiB|MiB|GiB\n\n### `blob-run-mode`\n\n+ Titan 的运行模式选择。\n+ 可选值：\n    + \"normal\"：value size 超过 [`min-blob-size`](#min-blob-size) 的数据会写入到 blob 文件。\n    + \"read-only\"：不再写入新数据到 blob，原有 blob 内的数据仍然可以读取。\n    + \"fallback\"：将 blob 内的数据写回 LSM。\n+ 默认值：\"normal\"\n\n### `level-merge`\n\n+ 是否通过开启 level-merge 来提升读性能，副作用是写放大会比不开启更大。\n+ 默认值：false\n\n## raftdb\n\nraftdb 相关配置项。\n\n### `max-background-jobs`\n\n+ RocksDB 后台线程个数。调整 RocksDB 线程池的大小时，请参考 [TiKV 线程池调优](/tune-tikv-thread-performance.md#tikv-线程池调优)。\n+ 默认值：4\n+ 最小值：2\n\n### `max-sub-compactions`\n\n+ RocksDB 进行 subcompaction 的并发数。\n+ 默认值：2\n+ 最小值：1\n\n### `max-open-files`\n\n+ RocksDB 可以打开的文件总数。\n+ 默认值：`40960`\n+ 最小值：`-1`\n\n### `max-manifest-file-size`\n\n+ 单个 RocksDB Manifest 文件的最大大小。\n+ 默认值：`\"20MiB\"`\n+ 最小值：`0`\n+ 单位：B|KiB|MiB|GiB\n\n### `create-if-missing`\n\n+ 如果值为 `true`，当数据库不存在时将自动创建。\n+ 默认值：`true`\n\n### `stats-dump-period`\n\n+ 输出统计信息到日志的时间间隔。\n+ 默认值：`10m`\n\n### `wal-dir`\n\n+ 存储 Raft RocksDB WAL 文件的目录，即 WAL 的绝对路径。**请勿**将该配置项设置为与 [`rocksdb.wal-dir`](#wal-dir) 相同的值。\n+ 如果未设置该配置项，日志文件将存储在与数据相同的目录中。\n+ 如果机器上有两个磁盘，将 RocksDB 数据和 WAL 日志存储在不同磁盘上可以提高性能。\n+ 默认值：`\"\"`\n\n### `wal-ttl-seconds`\n\n+ 归档的 WAL 文件的保留时间。当超过该值时，系统将删除这些文件。\n+ 默认值：`0`\n+ 最小值：`0`\n+ 单位：秒\n\n### `wal-size-limit`\n\n+ 归档 WAL 文件的大小限制。当超过该值时，系统将删除这些文件。\n+ 默认值：`0`\n+ 最小值：`0`\n+ 单位：B|KiB|MiB|GiB\n\n### `max-total-wal-size`\n\n+ RocksDB WAL 文件的最大总大小。\n+ 默认值：\n    + 当 `storage.engine=\"raft-kv\"` 时，默认值为 `\"4GiB\"`\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为 `1`\n\n### `compaction-readahead-size`\n\n+ 控制在 RocksDB compaction 时是否开启预读取功能，并指定预读取数据的大小。\n+ 如果使用机械硬盘，建议将该值至少设置为 `2MiB`。\n+ 默认值：`0`\n+ 最小值：`0`\n+ 单位：B|KiB|MiB|GiB\n\n### `writable-file-max-buffer-size`\n\n+ WriteableFileWrite 中使用的最大缓冲区大小。\n+ 默认值：`\"1MiB\"`\n+ 最小值：`0`\n+ 单位：B|KiB|MiB|GiB\n\n### `use-direct-io-for-flush-and-compaction`\n\n+ 控制是否在后台刷新和 compaction 时使用 `O_DIRECT` 进行读写。启用 `O_DIRECT` 的性能影响：它可以绕过和防止操作系统缓存污染，但是后续文件读取需要重新读取内容到缓存中。\n+ 默认值：`false`\n\n### `enable-pipelined-write`\n\n+ 控制是否开启 Pipelined Write。开启时会使用旧的 Pipelined Write，关闭时会使用新的 Pipelined Commit 机制。\n+ 默认值：`true`\n\n### `allow-concurrent-memtable-write`\n\n+ 控制是否开启并发 memtable 写入。\n+ 默认值：`true`\n\n### `bytes-per-sync`\n\n+ 异步 Sync 限速速率。\n+ 默认值：`\"1MiB\"`\n+ 最小值：`0`\n+ 单位：B|KiB|MiB|GiB\n\n### `wal-bytes-per-sync`\n\n+ WAL Sync 限速速率。\n+ 默认值：`\"512KiB\"`\n+ 最小值：`0`\n+ 单位：B|KiB|MiB|GiB\n\n### `info-log-max-size`\n\n> **警告：**\n>\n> 自 v5.4.0 起，RocksDB 的日志改为由 TiKV 的日志模块进行管理，因此该配置项被废弃，其功能由配置参数 [`log.file.max-size`](#max-size-从-v540-版本开始引入) 代替。\n\n+ Info 日志的最大大小。\n+ 默认值：`\"1GiB\"`\n+ 最小值：`0`\n+ 单位：B|KiB|MiB|GiB\n\n### `info-log-roll-time`\n\n> **警告：**\n>\n> 自 v5.4.0 起，RocksDB 的日志改为由 TiKV 的日志模块进行管理，因此该配置项被废弃。TiKV 不再支持按照时间自动切分日志，请使用配置参数 [`log.file.max-size`](#max-size-从-v540-版本开始引入) 配置按照文件大小自动切分日志的阈值。\n\n+ Info 日志截断间隔时间，如果为 `\"0s\"` 则不截断。\n+ 默认值：`\"0s\"`\n\n### `info-log-keep-log-file-num`\n\n> **警告：**\n>\n> 自 v5.4.0 起，RocksDB 的日志改为由 TiKV 的日志模块进行管理，因此该配置项被废弃，其功能由配置参数 [`log.file.max-backups`](#max-backups-从-v540-版本开始引入) 代替。\n\n+ RaftDB 中保存的 Info 日志文件的最大数量。\n+ 默认值：`10`\n+ 最小值：`0`\n\n### `info-log-dir`\n\n+ Info 日志存储的目录。\n+ 默认值：`\"\"`\n\n### `info-log-level`\n\n> **警告：**\n>\n> 自 v5.4.0 起，RocksDB 的日志改为由 TiKV 的日志模块进行管理，因此该配置项被废弃，其功能由配置参数 [`log.level`](#level-从-v540-版本开始引入) 代替。\n\n+ RaftDB 的日志级别。\n+ 默认值：`\"info\"`\n\n## raft-engine\n\nRaft Engine 相关的配置项。\n\n> **注意：**\n>\n> - 第一次开启 Raft Engine 时，TiKV 会将原有的 RocksDB 数据转移至 Raft Engine 中。因此，TiKV 的启动时间会比较长，你需要额外等待几十秒。\n> - 如果你要将 TiDB 集群降级至 v5.4.0 以前的版本（不含 v5.4.0），你需要在降级**之前**先关闭 Raft Engine（即把 `enable` 配置项设置为 `false`，并重启 TiKV 使配置生效），否则会导致集群降级后无法正常开启。\n\n### `enable`\n\n+ 决定是否使用 Raft Engine 来存储 Raft 日志。开启该配置项后，`raftdb` 的配置不再生效。\n+ 默认值：`true`\n\n### `dir`\n\n+ 存储 Raft 日志文件的目录。如果该目录不存在，则在启动 TiKV 时创建该目录。\n+ 如果未设置此配置，则使用 `{data-dir}/raft-engine`。\n+ 如果你的机器上有多个磁盘，建议将 Raft Engine 的数据存储在单独的磁盘上，以提高 TiKV 性能。\n+ 默认值：`\"\"`\n\n### `spill-dir` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n+ 存储 Raft 日志文件的辅助目录，当 `dir` 目录所在盘数据写满后，新的 Raft 日志将存储在该目录下。如果该目录配置后不存在，则在 TiKV 启动时自动创建该目录。\n+ 如果未设置此配置，则表示不启用辅助目录。\n\n> **注意：**\n>\n> - 该配置仅在 Raft Engine 的 `dir` 和 `spill-dir` 分别指定为**不同盘符**时才有效。\n> - 在配置该功能后，若想要关闭该功能，你需要在重启 TiKV **之前**执行如下操作，否则将**无法启动** TiKV：\n>     1. 关闭 TiKV。\n>     2. 将 `spill-dir` 目录下的所有 Raft Log 复制到 [`dir`](/tikv-configuration-file.md#dir) 目录下。\n>     3. 从 TiKV 配置文件中删除该配置。\n>     4. 重启 TiKV。\n\n### `batch-compression-threshold`\n\n+ 指定日志批处理的阈值大小。大于此配置的日志批次将被压缩。如果将此配置项设置为 `0`，则禁用压缩。\n+ 默认值：`\"8KiB\"`\n\n### `bytes-per-sync`\n\n> **警告：**\n>\n> 从 v6.5.0 起，Raft Engine 在写入日志时不会缓存而是直接落盘，因此该配置项被废弃，且不再生效。\n\n+ 指定缓存写入的最大累积大小。当超过此配置值时，缓存的写入将被刷写到磁盘。\n+ 如果将此配置项设置为 `0`，则禁用增量同步。\n+ 在 v6.5.0 之前的版本中，默认值为 `\"4MiB\"`。\n\n### `target-file-size`\n\n+ 指定日志文件的最大大小。当日志文件大于此值时，将对其进行轮转。\n+ 默认值：`\"128MiB\"`\n\n### `purge-threshold`\n\n+ 指定主日志队列的阈值大小。当超过此配置值时，将对主日志队列执行垃圾回收。\n+ 此参数可用于调整 Raft Engine 的空间占用大小。\n+ 默认值：`\"10GiB\"`\n\n### `recovery-mode`\n\n+ 确定在日志恢复过程中如何处理文件损坏。\n+ 可选值：`\"absolute-consistency\"`, `\"tolerate-tail-corruption\"`, `\"tolerate-any-corruption\"`\n+ 默认值：`\"tolerate-tail-corruption\"`\n\n### `recovery-read-block-size`\n\n+ 恢复期间读取日志文件的最小 I/O 大小。\n+ 默认值：`\"16KiB\"`\n+ 最小值：`\"512B\"`\n\n### `recovery-threads`\n\n+ 用于扫描和恢复日志文件的线程数。\n+ 默认值：`4`\n+ 最小值：`1`\n\n### `memory-limit`\n\n+ 指定 Raft Engine 使用内存的上限。\n+ 当该配置项未设置时，Raft Engine 默认使用系统总内存的 15%。\n+ 默认值：`系统总内存 * 15%`\n\n### `format-version` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n> **注意：**\n>\n> `format-version` 的值设置为 `2` 后，如果你需要将 TiKV 集群降级至 v6.3.0 以前的版本，你需要在降级**之前**执行如下操作：\n>\n> 1. 关闭 Raft Engine。将 [`enable`](/tikv-configuration-file.md#enable-1) 配置项设置为 `false`，并重启 TiKV 使配置生效。\n> 2. 将 `format-version` 的值重新设置为 `1`。\n> 3. 重新打开 Raft Engine，即把 `enable` 配置项重设为 `true`，并重启 TiKV 使配置生效。\n\n+ 指定 Raft Engine 的日志文件格式版本。\n+ 可选值：\n    + `1`：v6.3.0 以前的默认日志文件格式。v6.1.0 及以后版本的 TiKV 可以读取该格式。\n    + `2`：支持日志回收。v6.3.0 及以后版本的 TiKV 可以读取该格式。\n+ 默认值：\n    + 当 `storage.engine=\"raft-kv\"` 时，默认值为 `2`\n    + 当 `storage.engine=\"partitioned-raft-kv\"` 时，默认值为 `5`\n\n### `enable-log-recycle` <span class=\"version-mark\">从 v6.3.0 版本开始引入</span>\n\n> **注意：**\n>\n> 仅在 [`format-version`](#format-version-从-v630-版本开始引入) 的值大于等于 2 时，该配置项才生效。\n\n+ 控制 Raft Engine 是否回收过期的日志文件。该配置项启用时，Raft Engine 将保留逻辑上被清除的日志文件，用于日志回收，减少写负载的长尾延迟。\n+ 默认值：`true`\n\n### `prefill-for-recycle` <span class=\"version-mark\">从 v7.0.0 版本开始引入</span>\n\n> **注意：**\n>\n> 仅在 [`enable-log-recycle`](#enable-log-recycle-从-v630-版本开始引入) 的值为 `true` 时，该配置项才生效。\n\n+ 控制 Raft Engine 是否自动生成空的日志文件用于日志回收。该配置项启用时，Raft Engine 将在初始化时自动填充一批空日志文件用于日志回收，保证日志回收在初始化后立即生效。\n+ 默认值：`false`\n\n## security\n\n安全相关配置项。\n\n### `ca-path`\n\n+ CA 文件路径。\n+ 默认值：\"\"\n\n### `cert-path`\n\n+ 包含 X.509 证书的 PEM 文件路径。\n+ 默认值：\"\"\n\n### `key-path`\n\n+ 包含 X.509 key 的 PEM 文件路径。\n+ 默认值：\"\"\n\n### `cert-allowed-cn`\n\n+ 客户端提供的证书中，可接受的 X.509 通用名称列表。仅当提供的通用名称与列表中的条目之一完全匹配时，才会允许其请求。\n+ 默认值：`[]`。这意味着默认情况下禁用客户端证书 CN 检查。\n\n### `redact-info-log` <span class=\"version-mark\">从 v4.0.8 版本开始引入</span>\n\n+ 控制是否开启日志脱敏。可选值为 `true`、`\"on\"`、`false`、`\"off\"` 和 `\"marker\"`。其中，`\"on\"`、`\"off\"` 和 `\"marker\"` 从 v8.3.0 开始支持。\n+ 若设置为 `false` 或 `\"off\"`，即对用户日志不做处理。\n+ 若设置为 `true` 或 `\"on\"`，日志中的用户数据会以 `?` 代替显示。\n+ 若设置为 `\"marker\"`，日志中的用户数据会被标记符号 `‹ ›` 包裹。用户数据中的 `‹` 会转义成 `‹‹`，`›` 会转义成 `››`。基于标记后的日志，你可以在展示日志时决定是否对被标记信息进行脱敏处理。\n+ 默认值：`false`\n+ 具体使用方法参见[日志脱敏](/log-redaction.md#tikv-组件日志脱敏)。\n\n## security.encryption\n\n[静态加密](/encryption-at-rest.md) (TDE) 有关的配置项。\n\n### `data-encryption-method`\n\n+ 数据文件的加密方法。\n+ 可选值：`\"plaintext\"`，`\"aes128-ctr\"`，`\"aes192-ctr\"`，`\"aes256-ctr\"`，`\"sm4-ctr\"`（从 v6.3.0 开始支持）\n+ 选择 `\"plaintext\"` 以外的值则表示启用加密功能。此时必须指定主密钥。\n+ 默认值：`\"plaintext\"`\n\n### `data-key-rotation-period`\n\n+ 指定 TiKV 轮换数据密钥的频率。\n+ 默认值：`7d`\n\n### `enable-file-dictionary-log`\n\n+ 启用优化，以减少 TiKV 管理加密元数据时的 I/O 操作和互斥锁竞争。\n+ 此配置参数默认启用，为避免可能出现的兼容性问题，请参考[静态加密 - TiKV 版本间兼容性](/encryption-at-rest.md#tikv-版本间兼容性)。\n+ 默认值：`true`\n\n### `master-key`\n\n+ 指定启用加密时的主密钥。若要了解如何配置主密钥，可以参考[静态加密 - 配置加密](/encryption-at-rest.md#配置加密)。\n\n### `previous-master-key`\n\n+ 指定轮换新主密钥时的旧主密钥。旧主密钥的配置格式与主密钥相同。若要了解如何配置主密钥，可以参考[静态加密 - 配置加密](/encryption-at-rest.md#配置加密)。\n\n## import\n\n用于 TiDB Lightning 导入及 BR 恢复相关的配置项。\n\n### `num-threads`\n\n+ 处理 RPC 请求的线程数量。\n+ 默认值：8\n+ 最小值：1\n\n### `stream-channel-window`\n\n+ Stream channel 的窗口大小。当 channel 满时，Stream 会被阻塞。\n+ 默认值：`128`\n\n### `memory-use-ratio` <span class=\"version-mark\">从 v6.5.0 版本开始引入</span>\n\n+ 从 v6.5.0 开始，PITR 支持直接将备份日志文件读取到缓存中，然后进行恢复。此配置项用来配置 PITR 恢复中可用内存与系统总内存的占比。\n+ 可调整范围：[0.0, 0.5]\n+ 默认值：`0.3`，表示系统 30% 的内存可用于 PITR 恢复；当为 `0.0` 时，表示通过下载日志文件到本地进行 PITR 恢复。\n\n> **注意：**\n>\n> 在小于 v6.5.0 的版本中，PITR 仅支持将备份文件下载到本地进行恢复。\n\n## gc\n\n### `batch-keys`\n\n+ 一次 GC 操作中的 key 的数量。\n+ 默认值：`512`\n\n### `max-write-bytes-per-sec`\n\n+ GC 工作线程每秒可以写入 RocksDB 的最大字节数。\n+ 如果设置为 `0`，则没有限制。\n+ 默认值：`\"0\"`\n\n### `enable-compaction-filter` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 是否开启 GC in Compaction Filter 特性。\n+ 默认值：true\n\n### `ratio-threshold`\n\n+ 触发 GC 的垃圾比例阈值。\n+ 默认值：`1.1`\n\n### `num-threads` <span class=\"version-mark\">从 v6.5.8、v7.1.4、v7.5.1 和 v7.6.0 版本开始引入</span>\n\n+ 当 `enable-compaction-filter` 为 `false` 时 GC 线程个数。\n+ 默认值：1\n\n## backup\n\n用于 BR 备份相关的配置项。\n\n### `num-threads`\n\n+ 处理备份的工作线程数量。\n+ 默认值：CPU * 0.5，但最大为 8\n+ 可调整范围：[1, CPU]\n+ 最小值：1\n\n### `batch-size`\n\n+ 一次备份的数据范围数量。\n+ 默认值：`8`\n\n### `sst-max-size`\n\n+ 备份 SST 文件大小的阈值。如果 TiKV Region 中备份文件的大小超过该阈值，则将该文件备份到 Region 分割的多个 Region 文件中，每个分割 Region 中的文件大小均为 `sst-max-size`（或略大）。\n+ 例如，当 Region `[a,e)` 中备份文件大小超过 `sst-max-size` 时，该文件会被备份到多个 Region 范围中，分别为 Region `[a,b)`、`[b,c)`、`[c,d)` 和 `[d,e)`，并且 `[a,b)`、`[b,c)` 和 `[c,d)` 的大小均为 `sst-max-size`（或略大）。\n+ 默认值：`\"384MiB\"`。在 v8.4.0 之前，默认值为 `\"144MiB\"`。\n\n### `enable-auto-tune` <span class=\"version-mark\">从 v5.4 版本开始引入</span>\n\n+ 在集群资源占用率较高的情况下，是否允许 BR 自动限制备份使用的资源，减少对集群的影响。详情见[自动调节](/br/br-auto-tune.md)。\n+ 默认值：true\n\n### `s3-multi-part-size` <span class=\"version-mark\">从 v5.3.2 版本开始引入</span>\n\n> **注意：**\n>\n> 引入该配置项是为了解决备份期间遇到的 S3 限流导致备份失败的问题。该问题已通过[优化 BR 备份数据存储的目录结构](/br/br-snapshot-architecture.md#备份文件目录结构)得到解决。因此，该配置项自 v6.1.1 起开始废弃，不再推荐使用。\n\n+ 备份阶段 S3 分块上传的块大小。可通过调整该参数来控制备份时发往 S3 的请求数量。\n+ TiKV 备份数据到 S3 时，如果备份文件大于该配置项的值，会自动进行[分块上传](https://docs.aws.amazon.com/zh_cn/AmazonS3/latest/API/API_UploadPart.html)。根据压缩率的不同，96 MiB Region 产生的备份文件大约在 10 MiB~30 MiB 之间。\n+ 默认值：5MiB\n\n## backup.hadoop\n\n### `home`\n\n+ 指定 HDFS shell 命令的位置，并且允许 TiKV 找到该 shell 命令。该配置项与环境变量 `$HADOOP_HOME` 有相同的效果。\n+ 默认值：`\"\"`\n\n### `linux-user`\n\n+ 指定 TiKV 运行 HDFS shell 命令的 Linux 用户。\n+ 如果未设置该配置项，TiKV 会使用当前 Linux 用户。\n+ 默认值：`\"\"`\n\n## log-backup\n\n用于日志备份相关的配置项。\n\n### `enable` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 用于开启日志备份功能。\n+ 默认值：true\n\n### `file-size-limit` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 日志备份任务中，保存到存储的备份文件大小。\n+ 默认值：256MiB\n+ 注意：一般情况下，`file-size-limit` 的值会大于外部存储上显示的备份文件大小，这是因为备份文件在上传时会被压缩。\n\n### `initial-scan-pending-memory-quota` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 日志备份任务在扫描增量数据时，用于存放扫描数据的缓存大小。\n+ 默认值：`min(机器总内存 * 10%, 512 MiB)`\n\n### `initial-scan-rate-limit` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 日志备份任务在扫描增量数据时的吞吐限流参数，表示每秒最多从硬盘读出的数据量。注意，如果仅指定数字（如 `60`），则单位为 Byte 而不是 KiB。\n+ 默认值：60MiB\n+ 最小值：1MiB\n\n### `max-flush-interval` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 日志备份任务将备份数据写入到外部存储的最大间隔时间。\n+ 默认值：3min\n\n### `num-threads` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 日志备份功能占用的线程数目。\n+ 默认值：CPU * 0.5\n+ 可调整范围：[2, 12]\n\n### `temp-path` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 日志文件存放的临时目录，日志文件预先写入临时目录，然后 flush 到外部存储中。\n+ 默认值：`${deploy-dir}/data/log-backup-temp`\n\n## cdc\n\n用于 TiCDC 捕捉变更数据相关的配置项。\n\n### `min-ts-interval`\n\n+ 定期推进 Resolved TS 的时间间隔。\n+ 默认值：`\"1s\"`\n\n> **注意：**\n>\n> 在 v6.5.0 中，`min-ts-interval` 的默认值从 `\"1s\"` 更改为 `\"200ms\"`，以减少 CDC 的延迟。从 v6.5.1 开始，该默认值更改回 `\"1s\"`，以减少网络流量。\n\n### `old-value-cache-memory-quota`\n\n+ 缓存在内存中的 TiCDC Old Value 的条目占用内存的上限。\n+ 默认值：512MiB\n\n### `sink-memory-quota`\n\n+ 缓存在内存中的 TiCDC 数据变更事件占用内存的上限。\n+ 默认值：512MiB\n\n### `incremental-scan-speed-limit`\n\n+ 增量扫描历史数据的速度上限。\n+ 默认值：128MiB，即 128MiB 每秒。\n\n### `incremental-scan-threads`\n\n+ 增量扫描历史数据任务的线程个数。\n+ 默认值：4，即 4 个线程\n\n### `incremental-scan-concurrency`\n\n+ 增量扫描历史数据任务的最大并发执行个数。\n+ 默认值：6，即最多并发执行 6 个任务\n+ 注意：`incremental-scan-concurrency` 需要大于等于 `incremental-scan-threads`，否则 TiKV 启动会报错。\n\n## resolved-ts\n\n用于维护 Resolved TS 以服务 Stale Read 请求的相关配置项。\n\n### `enable`\n\n+ 是否为所有 Region 维护 Resolved TS。\n+ 默认值：true\n\n### `advance-ts-interval`\n\n+ 定期推进 Resolved TS 的时间间隔。\n+ 默认值：20s\n\n### `scan-lock-pool-size`\n\n+ 初始化 Resolved TS 时 TiKV 扫描 MVCC（多版本并发控制）锁数据的线程个数。\n+ 默认值：2，即 2 个线程\n\n## pessimistic-txn\n\n悲观事务使用方法请参考 [TiDB 悲观事务模式](/pessimistic-transaction.md)。\n\n### `wait-for-lock-timeout`\n\n+ 悲观事务在 TiKV 中等待其他事务释放锁的最长时间。若超时则会返回错误给 TiDB 并由 TiDB 重试加锁，语句最长等锁时间由 `innodb_lock_wait_timeout` 控制。\n+ 默认值：1s\n+ 最小值：1ms\n\n### `wake-up-delay-duration`\n\n+ 悲观事务释放锁时，只会唤醒等锁事务中 `start_ts` 最小的事务，其他事务将会延迟 `wake-up-delay-duration` 之后被唤醒。\n+ 默认值：20ms\n\n### `pipelined`\n\n+ 开启流水线式加悲观锁流程。开启该功能后，TiKV 在检测数据满足加锁要求后，立刻通知 TiDB 执行后面的请求，并异步写入悲观锁，从而降低大部分延迟，显著提升悲观事务的性能。但有较低概率出现悲观锁异步写入失败的情况，可能会导致悲观事务提交失败。\n+ 默认值：true\n\n### `in-memory` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n+ 开启内存悲观锁功能。开启该功能后，悲观事务会尽可能在 TiKV 内存中存储悲观锁，而不将悲观锁写入磁盘，也不将悲观锁同步给其他副本，从而提升悲观事务的性能。但有较低概率出现悲观锁丢失的情况，可能会导致悲观事务提交失败。\n+ 默认值：true\n+ 注意：`in-memory` 仅在 `pipelined` 为 true 时生效。\n\n### `in-memory-peer-size-limit` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n+ 控制单个 Region [内存悲观锁](/pessimistic-transaction.md#内存悲观锁)的内存使用上限。超过此限制时，悲观锁将回退到持久化方式写入磁盘。\n+ 默认值：512KiB\n+ 单位：KiB|MiB|GiB\n\n### `in-memory-instance-size-limit` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n+ 控制单个 TiKV 实例[内存悲观锁](/pessimistic-transaction.md#内存悲观锁)的内存使用上限。超过此限制时，悲观锁将回退到持久化方式写入磁盘。\n+ 默认值：100MiB\n+ 单位：KiB|MiB|GiB\n\n## quota\n\n用于请求限流 (Quota Limiter) 相关的配置项。\n\n### `max-delay-duration` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n+ 单次读写请求被强制等待的最大时间。\n+ 默认值：500ms\n+ 推荐设置：一般使用默认值即可。如果实例出现了内存溢出或者是剧烈的性能抖动，可以设置为 1S，使得请求被延迟调节的时间不超过 1 秒。\n\n### 前台限流\n\n用于前台限流相关的配置项。\n\n当 TiKV 部署的机型资源有限（如 4v CPU，16 G 内存）时，如果 TiKV 前台处理的读写请求量过大，以至于占用 TiKV 后台处理请求所需的 CPU 资源，最终影响 TiKV 性能的稳定性。此时，你可以使用前台限流相关的 quota 配置项以限制前台各类请求占用的 CPU 资源。触发该限制的请求会被强制等待一段时间以让出 CPU 资源。具体等待时间与新增请求量相关，最多不超过 [`max-delay-duration`](#max-delay-duration-从-v600-版本开始引入) 的值。\n\n#### `foreground-cpu-time` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n+ 限制处理 TiKV 前台读写请求所使用的 CPU 资源使用量，这是一个软限制。\n+ 默认值：0（即无限制）\n+ 单位：millicpu （当该参数值为 `1500` 时，前端请求会消耗 1.5v CPU）。\n+ 推荐设置：对于 4 核以上的实例，使用默认值 `0` 即可；对 4 核实例，设置为 `1000` 到 `1500` 之间的值能取得比较均衡的效果；对 2 核实例，则不要超过 `1200`。\n\n#### `foreground-write-bandwidth` <span class=\"version-mark\">从 v6.0.0 版本开始引入</span>\n\n+ 限制前台事务写入的带宽，这是一个软限制。\n+ 默认值：0KiB（即无限制）\n+ 推荐设置：除非因为 `foreground-cpu-time` 设置不足以对写带宽做限制，一般情况下本配置项使用默认值 `0` 即可；否则，在 4 核及 4 核以下规格实例上，建议设置在 `50MiB` 以下。\n\n#### `foreground-read-bandwidth` <span class=\"version-mark\">从 v6.0.0 版本开始引入 </span>\n\n+ 限制前台事务读取数据和 Coprocessor 读取数据的带宽，这是一个软限制。\n+ 默认值：0KiB（即无限制）\n+ 推荐设置：除非因为 `foreground-cpu-time` 设置不足以对读带宽做限制，一般情况本配置项使用默认值 `0` 即可；否则，在 4 核及 4 核以下规格实例上，建议设置在 `20MiB` 以内。\n\n### 后台限流\n\n用于后台限流相关的配置项。\n\n当 TiKV 部署的机型资源有限（如 4v CPU，16 G 内存）时，如果 TiKV 后台处理的计算或者读写请求量过大，以至于占用 TiKV 前台处理请求所需的 CPU 资源，最终影响 TiKV 性能的稳定性。此时，你可以使用后台限流相关的 quota 配置项以限制后台各类请求占用的 CPU 资源。触发该限制的请求会被强制等待一段时间以让出 CPU 资源。具体等待时间与新增请求量相关，最多不超过 [`max-delay-duration`](#max-delay-duration-从-v600-版本开始引入) 的值。\n\n> **警告：**\n>\n> - 后台限流是 TiDB 在 v6.2.0 中引入的实验特性，不建议在生产环境中使用。\n> - 该功能仅适合在资源有限的环境中使用，以保证 TiKV 在该环境下可以长期稳定地运行。如果在资源丰富的机型环境中开启该功能，可能会导致读写请求量达到峰值时 TiKV 的性能下降的问题。\n\n#### `background-cpu-time` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 限制处理 TiKV 后台读写请求所使用的 CPU 资源使用量，这是一个软限制。\n+ 默认值：0（即无限制）\n+ 单位：millicpu（当该参数值为 `1500` 时，后端请求会消耗 1.5v CPU）。\n\n#### `background-write-bandwidth` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n> **注意：**\n>\n> 该配置项可以通过 `SHOW CONFIG` 查询到，但暂未生效。设置该配置项的值不生效。\n\n+ 限制后台事务写入的带宽，这是一个软限制。\n+ 默认值：0KiB（即无限制）\n\n#### `background-read-bandwidth` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n> **注意：**\n>\n> 该配置项可以通过 `SHOW CONFIG` 查询到，但暂未生效。设置该配置项的值不生效。\n\n+ 限制后台事务读取数据和 Coprocessor 读取数据的带宽，这是一个软限制。\n+ 默认值：0KiB（即无限制）\n\n#### `enable-auto-tune` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 是否支持 quota 动态调整。如果打开该配置项，TiKV 会根据 TiKV 实例的负载情况动态调整对后台请求的限制 quota。\n+ 默认值：false（即关闭动态调整）\n\n## causal-ts <span class=\"version-mark\">从 v6.1.0 版本开始引入</span>\n\n用于 TiKV API V2（`storage.api-version = 2`）中时间戳获取相关的配置项。\n\n为了降低写请求延迟，TiKV 会定期获取一批时间戳缓存在本地，避免频繁访问 PD，并容忍短时间的 TSO 服务故障。\n\n### `alloc-ahead-buffer` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 预分配 TSO 缓存大小（以时长计算）。\n+ 表示 TiKV 将按照这个参数指定的时长，预分配 TSO 缓存。TiKV 会根据前一周期的使用情况，预估并请求满足 `alloc-ahead-buffer` 时长所需要的 TSO 数量，缓存在本地。\n+ 这个参数通常用于提高 TiKV API V2 (`storage.api-version = 2`) 对 PD 故障的容忍度。\n+ 调大这个参数会增加 TSO 消耗，并增加 TiKV 的内存开销。为了获得足够的 TSO，建议同时调小 PD 的 [`tso-update-physical-interval`](/pd-configuration-file.md#tso-update-physical-interval) 参数。\n+ 根据测试，默认配置下，当 PD 主节点由于故障切换到其节点时，写请求会短暂出现延迟增大和 QPS 下降（幅度约 15%）。\n+ 如果希望业务不受影响，可以尝试采用以下配置：\n    + `causal-ts.alloc-ahead-buffer = \"6s\"`\n    + `causal-ts.renew-batch-max-size = 65536`\n    + `causal-ts.renew-batch-min-size = 2048`\n    + 在 PD 中配置 `tso-update-physical-interval = \"1ms\"`\n+ 默认值：3s\n\n### `renew-interval`\n\n+ 更新本地缓存时间戳的周期。\n+ TiKV 会每隔 `renew-interval` 发起一次时间戳更新，并根据前一周期的使用情况以及 `alloc-ahead-buffer` 参数，来调整时间戳的缓存数量。这个参数配置过大会导致不能及时反映最新的 TiKV 负载变化。而配置过小则会增加 PD 的负载。如果写流量剧烈变化、频繁出现时间戳耗尽、写延迟增加，可以适当调小这个参数，但需要同时关注 PD 的负载情况。\n+ 默认值：100ms\n\n### `renew-batch-min-size`\n\n+ 单次时间戳请求的最小数量。\n+ TiKV 会根据前一周期的使用情况以及 `alloc-ahead-buffer` 参数设置，来调整时间戳的缓存数量。如果 TSO 需求量较低，TiKV 会降低单次 TSO 请求量，直至等于 `renew-batch-min-size`。如果业务中经常出现突发的大流量写入，可以适当调大这个参数。注意这个参数是单个 tikv-server 的缓存大小，如果配置过大、而同时集群中 tikv-server 较多，会导致 TSO 消耗过快。\n+ Grafana **TiKV-Raw** 面板下 **Causal timestamp** 中的 **TSO batch size** 是根据业务负载动态调整后的本地缓存数量。可以参考该监控指标值调整这个参数的大小。\n+ 默认值：100\n\n### `renew-batch-max-size` <span class=\"version-mark\">从 v6.4.0 版本开始引入</span>\n\n+ 单次时间戳请求的最大数量。\n+ 在默认的一个 TSO 物理时钟更新周期内 (50ms)，PD 最多提供 262144 个 TSO，超过这个数量后 PD 会暂缓 TSO 请求的处理。这个配置用于避免 PD 的 TSO 消耗殆尽、影响其他业务的使用。如果增大这个参数，建议同时减小 PD 的 [`tso-update-physical-interval`](/pd-configuration-file.md#tso-update-physical-interval) 参数，以获得足够的 TSO。\n+ 默认值：8192\n\n## resource-control\n\n资源控制 (Resource Control) 在 TiKV 存储层相关的配置项。\n\n### `enabled` <span class=\"version-mark\">从 v6.6.0 版本开始引入</span>\n\n+ 是否支持对用户前台的读写请求按照对应的资源组配额做优先级调度。有关 TiDB 资源组和资源管控的信息，请参考 [TiDB 资源管控](/tidb-resource-control.md)\n+ 在 TiDB 侧开启 [`tidb_enable_resource_control`](/system-variables.md#tidb_enable_resource_control-从-v660-版本开始引入) 全局变量的情况下，开启这个配置项才有意义。此配置参数开启后，TiKV 会使用优先级队列对排队的用户前台读写请求做调度，调度的优先级和请求所在资源组已经消费的资源量反相关，和对应资源组的配额正相关。\n+ 默认值：true（即开启按照资源组配额调度）\n\n### `priority-ctl-strategy` <span class=\"version-mark\">从 v8.4.0 版本开始引入</span>\n\n+ 配置低优先级任务的流量管控策略。TiKV 通过对低优先级的任务进行流量控制来确保优先执行高优先级任务。\n+ 可选值：\n    + `aggressive`：此策略会优先保证高优先级任务的性能，确保高优先级任务的吞吐和延迟基本不受影响，但低优先级任务的执行会较慢。\n    + `moderate`：此策略会为低优先级任务施加较平衡的流控限制，并对高优先级任务有较低影响。\n    + `conservative`：此策略会优先确保系统资源被充分利用，允许低优先级任务根据需要充分使用系统可用资源，因此对高优先级任务的性能影响更大。\n+ 默认值：`moderate`\n\n## split\n\n[Load Base Split](/configure-load-base-split.md) 相关的配置项。\n\n### `byte-threshold` <span class=\"version-mark\">从 v5.0 版本开始引入</span>\n\n+ 控制某个 Region 被识别为热点 Region 的流量阈值。\n+ 默认值：\n\n    + 当 [`region-split-size`](#region-split-size) 小于 4 GiB 时，默认值为每秒 `30MiB` 流量。\n    + 当 [`region-split-size`](#region-split-size) 大于或等于 4 GiB 时，默认值为每秒 `100MiB` 流量。\n\n### `qps-threshold`\n\n+ 控制某个 Region 被识别为热点 Region 的 QPS 阈值。\n+ 默认值：\n\n    + 当 [`region-split-size`](#region-split-size) 小于 4 GiB 时，默认值为每秒 `3000` QPS。\n    + 当 [`region-split-size`](#region-split-size) 大于或等于 4 GiB 时，默认值为每秒 `7000` QPS。\n\n### `region-cpu-overload-threshold-ratio` <span class=\"version-mark\">从 v6.2.0 版本开始引入</span>\n\n+ 控制某个 Region 被识别为热点 Region 的 CPU 使用率阈值。\n+ 默认值：\n\n    + 当 [`region-split-size`](#region-split-size) 小于 4 GiB 时，默认值为 `0.25`。\n    + 当 [`region-split-size`](#region-split-size) 大于或等于 4 GiB 时，默认值为 `0.75`。\n\n## memory <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n### `enable-heap-profiling` <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n+ 控制是否开启 TiKV 堆内存分析功能，以跟踪 TiKV 的内存使用情况。\n+ 默认值：true\n\n### `profiling-sample-per-bytes` <span class=\"version-mark\">从 v7.5.0 版本开始引入</span>\n\n+ 设置 TiKV 堆内存分析每次采样的数据量，以 2 的指数次幂向上取整。\n+ 默认值：512KiB\n\n## in-memory-engine <span class=\"version-mark\">从 v8.5.0 版本开始引入</span>\n\nTiKV MVCC 内存引擎 (In-Memory Engine) 在 TiKV 存储层相关的配置项。\n\n### `enable` <span class=\"version-mark\">从 v8.5.0 版本开始引入</span>\n\n> **注意：**\n>\n> 该配置项支持在配置文件中进行配置，但不支持通过 SQL 语句查询。\n\n+ 是否开启内存引擎以加速多版本查询。关于内存引擎的详细信息，参见 [TiKV MVCC 内存引擎](/tikv-in-memory-engine.md)。\n+ 默认值：false（即关闭内存引擎）\n\n### `capacity` <span class=\"version-mark\">从 v8.5.0 版本开始引入</span>\n\n> **注意：**\n>\n> + 开启内存引擎后，`block-cache.capacity` 会自动减少 10%。\n> + 手动配置 `capacity` 时，`block-cache.capacity` 不会自动减少，需手动调整为合适的值以避免 OOM。\n\n+ 配置内存引擎可使用的内存大小。最大值为 5 GiB。你可以手动调整配置以使用更多内存。\n+ 默认值：系统内存的 10%。\n\n### `gc-run-interval` <span class=\"version-mark\">从 v8.5.0 版本开始引入</span>\n\n+ 控制内存引擎 GC 缓存 MVCC 版本的时间间隔。调小该参数可加快 GC 频率，减少 MVCC 记录，但会增加 GC 的 CPU 消耗，以及增加内存引擎失效的概率。\n+ 默认值：3m\n\n### `mvcc-amplification-threshold` <span class=\"version-mark\">从 v8.5.0 版本开始引入</span>\n\n+ 控制内存引擎选取加载 Region 时 MVCC 读放大的阈值。默认为 `10`，表示在某个 Region 中读一行记录需要处理的 MVCC 版本数量超过 10 个时，有可能会被加载到内存引擎中。\n+ 默认值：10\n"
        },
        {
          "name": "tikv-control.md",
          "type": "blob",
          "size": 30.150390625,
          "content": "---\ntitle: TiKV Control 使用说明\naliases: ['/docs-cn/dev/tikv-control/','/docs-cn/dev/reference/tools/tikv-control/']\nsummary: TiKV Control（tikv-ctl）是 TiKV 的命令行工具，用于管理 TiKV 集群。它的安装目录在 `~/.tiup/components/ctl/{VERSION}/` 目录下。通过 TiUP 使用 TiKV Control，可以调用 `tikv-ctl` 工具。通用参数包括远程模式和本地模式，以及两个简单的命令 `--to-hex` 和 `--to-escaped`。其他子命令包括查看 Raft 状态机的信息、查看 Region 的大小、扫描查看给定范围的 MVCC、查看给定 key 的 MVCC、扫描 raw key、打印某个 key 的值、打印 Region 的 properties 信息、手动 compact 单个 TiKV 的数据、手动 compact 整个 TiKV 集群的数据、设置一个 Region 副本为 tombstone 状态、向 TiKV 发出 consistency-check 请求、Dump snapshot 元文件、打印 Raft 状态机出错的 Region、动态修改 TiKV 的配置、强制 Region 从多副本失败状态恢复服务、恢复损坏的 MVCC 数据、Ldb 命令、打印加密元数据、打印损坏的 SST 文件信息、获取一个 Region 的 RegionReadProgress 状态。\n---\n\n# TiKV Control 使用说明\n\nTiKV Control（以下简称 tikv-ctl）是 TiKV 的命令行工具，用于管理 TiKV 集群。它的安装目录如下：\n\n+ 如果是使用 TiUP 部署的集群，在 `~/.tiup/components/ctl/{VERSION}/` 目录下。\n\n## 通过 TiUP 使用 TiKV Control\n\n> **注意：**\n>\n> 建议使用的 Control 工具版本与集群版本保持一致。\n\n`tikv-ctl` 也集成在了 `tiup` 命令中。执行以下命令，即可调用 `tikv-ctl` 工具：\n\n```shell\ntiup ctl:v<CLUSTER_VERSION> tikv\n```\n\n```\nStarting component `ctl`: /home/tidb/.tiup/components/ctl/v4.0.8/ctl tikv\nTiKV Control (tikv-ctl)\nRelease Version:   4.0.8\nEdition:           Community\nGit Commit Hash:   83091173e960e5a0f5f417e921a0801d2f6635ae\nGit Commit Branch: heads/refs/tags/v4.0.8\nUTC Build Time:    2020-10-30 08:40:33\nRust Version:      rustc 1.42.0-nightly (0de96d37f 2019-12-19)\nEnable Features:   jemalloc mem-profiling portable sse protobuf-codec\nProfile:           dist_release\n\nA tool for interacting with TiKV deployments.\n\nUSAGE:\n    TiKV Control (tikv-ctl) [FLAGS] [OPTIONS] [SUBCOMMAND]\n\nFLAGS:\n    -h, --help                    Prints help information\n        --skip-paranoid-checks    Skip paranoid checks when open rocksdb\n    -V, --version                 Prints version information\n\nOPTIONS:\n        --ca-path <ca-path>              Set the CA certificate path\n        --cert-path <cert-path>          Set the certificate path\n        --config <config>                TiKV config path, by default it's <deploy-dir>/conf/tikv.toml\n        --data-dir <data-dir>            TiKV data directory path, check <deploy-dir>/scripts/run.sh to get it\n        --decode <decode>                Decode a key in escaped format\n        --encode <encode>                Encode a key in escaped format\n        --to-hex <escaped-to-hex>        Convert an escaped key to hex key\n        --to-escaped <hex-to-escaped>    Convert a hex key to escaped key\n        --host <host>                    Set the remote host\n        --key-path <key-path>            Set the private key path\n        --log-level <log-level>          Set the log level [default:warn]\n        --pd <pd>                        Set the address of pd\n\nSUBCOMMANDS:\n    bad-regions           Get all regions with corrupt raft\n    cluster               Print the cluster id\n    compact               Compact a column family in a specified range\n    compact-cluster       Compact the whole cluster in a specified range in one or more column families\n    consistency-check     Force a consistency-check for a specified region\n    decrypt-file          Decrypt an encrypted file\n    diff                  Calculate difference of region keys from different dbs\n    dump-snap-meta        Dump snapshot meta file\n    encryption-meta       Dump encryption metadata\n    fail                  Inject failures to TiKV and recovery\n    help                  Prints this message or the help of the given subcommand(s)\n    metrics               Print the metrics\n    modify-tikv-config    Modify tikv config, eg. tikv-ctl --host ip:port modify-tikv-config -n\n                          rocksdb.defaultcf.disable-auto-compactions -v true\n    mvcc                  Print the mvcc value\n    print                 Print the raw value\n    raft                  Print a raft log entry\n    raw-scan              Print all raw keys in the range\n    recover-mvcc          Recover mvcc data on one node by deleting corrupted keys\n    recreate-region       Recreate a region with given metadata, but alloc new id for it\n    region-properties     Show region properties\n    scan                  Print the range db range\n    size                  Print region size\n    split-region          Split the region\n    store                 Print the store id\n    tombstone             Set some regions on the node to tombstone by manual\n    unsafe-recover        Unsafely recover the cluster when the majority replicas are failed\n```\n\n你可以在 `tiup ctl:v<CLUSTER_VERSION> tikv` 后面再接上相应的参数与子命令。\n\n## 通用参数\n\ntikv-ctl 提供以下两种运行模式：\n\n- **远程模式**。通过 `--host` 选项接受 TiKV 的服务地址作为参数。在此模式下，如果 TiKV 启用了 SSL，则 tikv-ctl 也需要指定相关的证书文件，例如：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tikv-ctl --ca-path ca.pem --cert-path client.pem --key-path client-key.pem --host 127.0.0.1:20160 <subcommands>\n    ```\n\n    某些情况下，tikv-ctl 与 PD 进行通信，而不与 TiKV 通信。此时你需要使用 `--pd` 选项而非 `--host` 选项，例如：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tikv-ctl --pd 127.0.0.1:2379 compact-cluster\n    ```\n\n    ```\n    store:\"127.0.0.1:20160\" compact db:KV cf:default range:([], []) success!\n    ```\n\n- **本地模式**：\n\n    - 通过 `--data-dir` 选项来指定本地 TiKV 数据的目录路径。\n    - 通过 `--config` 选项来指定本地 TiKV 配置文件到路径。\n\n  在此模式下，需要停止正在运行的 TiKV 实例。\n\n以下如无特殊说明，所有命令都同时支持这两种模式。\n\n除此之外，tikv-ctl 还有两个简单的命令 `--to-hex` 和 `--to-escaped`，用于对 key 的形式作简单的变换。一般使用 `escaped` 形式，示例如下：\n\n```shell\ntikv-ctl --to-escaped 0xaaff\n```\n\n```\n\\252\\377\n```\n\n```shell\ntikv-ctl --to-hex \"\\252\\377\"\n```\n\n```\nAAFF\n```\n\n> **注意：**\n>\n> 在命令行上指定 `escaped` 形式的 key 时，需要用双引号引起来，否则 bash 会将反斜杠吃掉，导致结果错误。\n\n## 各项子命令及部分参数、选项\n\n下面逐一对 tikv-ctl 支持的子命令进行举例说明。有的子命令支持很多可选参数，要查看全部细节，可运行 `tikv-ctl --help <subcommand>`。\n\n### 查看 Raft 状态机的信息\n\n`raft` 子命令可以查看 Raft 状态机在某一时刻的状态。状态信息包括 **RegionLocalState**、**RaftLocalState** 和 **RegionApplyState** 三个结构体，及某一条 log 对应的 Entries。\n\n可以使用 `region` 和 `log` 两个子命令分别查询以上信息。两条子命令都同时支持远程模式和本地模式。\n\n对于 `region` 子命令：\n\n- 要查看指定的 Region，可在命令中使用 `-r` 参数，多个 Region 以 `,` 分隔。也可以使用 `--all-regions` 参数来返回所有 Region（`-r` 与 `--all-regions` 不能同时使用）\n- 要限制输出的 Region 的数量，可在命令中使用 `--limit` 参数（默认为 `16`）\n- 要查询某个 key 范围中包含哪些 Region，可在命令中使用 `--start` 和 `--end` 参数（默认不限范围，采用 Hex 格式）\n\n需要输出 ID 为 `1239` 的 Region 时，用法及输出内容如下所示：\n\n```shell\ntikv-ctl --host 127.0.0.1:20160 raft region -r 1239\n```\n\n```\n\"region id\": 1239\n\"region state\": { \n    id: 1239, \n    start_key: 7480000000000000FF4E5F728000000000FF1443770000000000FA, \n    end_key: 7480000000000000FF4E5F728000000000FF21C4420000000000FA, \n    region_epoch: {conf_ver: 1 version: 43}, \n    peers: [ {id: 1240 store_id: 1 role: Voter} ] \n}\n\"raft state\": {\n    hard_state {term: 8 vote: 5 commit: 7} \n    last_index: 8)\n}\n\"apply state\": {\n    applied_index: 8 commit_index: 8 commit_term: 8\n    truncated_state {index: 5 term: 5} \n}\n```\n\n需要查询某个 key 范围中包含哪些 Region 时，用法及输出内容如下所示：\n\n- 当 key 范围包含在某个 Region 中时，将会输出该 Region 信息。\n- 当 key 范围精准到某个 Region 的范围时，以上述 Region `1239` 为例：当给定的 key 范围为 Region `1239` 的范围时，由于 Region 范围为左闭右开区间，并且 Region `1009` 以 Region `1239` 的 `end_key` 作为 `start_key`，因此会同时输出 Region `1009` 和 Region `1239` 的信息。\n\n```shell\ntikv-ctl --host 127.0.0.1:20160 raft region --start 7480000000000000FF4E5F728000000000FF1443770000000000FA --end 7480000000000000FF4E5F728000000000FF21C4420000000000FA\n```\n\n```\n\"region state\": { \n    id: 1009\n    start_key: 7480000000000000FF4E5F728000000000FF21C4420000000000FA, \n    end_key: 7480000000000000FF5000000000000000F8, \n    ...\n}\n\"region state\": { \n    id: 1239\n    start_key: 7480000000000000FF4E5F728000000000FF06C6D60000000000FA, \n    end_key: 7480000000000000FF4E5F728000000000FF1443770000000000FA, \n    ...\n}\n```\n\n### 查看 Region 的大小\n\n使用 `size` 命令可以查看 Region 的大小：\n\n```shell\ntikv-ctl --data-dir /path/to/tikv size -r 2\n```\n\n```\nregion id: 2\ncf default region size: 799.703 MB\ncf write region size: 41.250 MB\ncf lock region size: 27616\n```\n\n### 扫描查看给定范围的 MVCC\n\n`scan` 命令的 `--from` 和 `--to` 参数接受两个 escaped 形式的 raw key，并用 `--show-cf` 参数指定只需要查看哪些列族。\n\n```shell\ntikv-ctl --data-dir /path/to/tikv scan --from 'zm' --limit 2 --show-cf lock,default,write\n```\n\n```\nkey: zmBootstr\\377a\\377pKey\\000\\000\\377\\000\\000\\373\\000\\000\\000\\000\\000\\377\\000\\000s\\000\\000\\000\\000\\000\\372\n         write cf value: start_ts: 399650102814441473 commit_ts: 399650102814441475 short_value: \"20\"\nkey: zmDB:29\\000\\000\\377\\000\\374\\000\\000\\000\\000\\000\\000\\377\\000H\\000\\000\\000\\000\\000\\000\\371\n         write cf value: start_ts: 399650105239273474 commit_ts: 399650105239273475 short_value: \"\\000\\000\\000\\000\\000\\000\\000\\002\"\n         write cf value: start_ts: 399650105199951882 commit_ts: 399650105213059076 short_value: \"\\000\\000\\000\\000\\000\\000\\000\\001\"\n```\n\n### 查看给定 key 的 MVCC\n\n与 `scan` 命令类似，`mvcc` 命令可以查看给定 key 的 MVCC：\n\n```shell\ntikv-ctl --data-dir /path/to/tikv mvcc -k \"zmDB:29\\000\\000\\377\\000\\374\\000\\000\\000\\000\\000\\000\\377\\000H\\000\\000\\000\\000\\000\\000\\371\" --show-cf=lock,write,default\n```\n\n```\nkey: zmDB:29\\000\\000\\377\\000\\374\\000\\000\\000\\000\\000\\000\\377\\000H\\000\\000\\000\\000\\000\\000\\371\n         write cf value: start_ts: 399650105239273474 commit_ts: 399650105239273475 short_value: \"\\000\\000\\000\\000\\000\\000\\000\\002\"\n         write cf value: start_ts: 399650105199951882 commit_ts: 399650105213059076 short_value: \"\\000\\000\\000\\000\\000\\000\\000\\001\"\n```\n\n> **注意：**\n>\n> 该命令中，key 同样需要是 escaped 形式的 raw key。\n\n### 扫描 raw key\n\n使用 `raw-scan` 命令，TiKV 可直接在 RocksDB 中扫描 raw key。\n\n> **注意：**\n>\n> 如果要扫描数据 key，需要在 key 前添加 `'z'` 前缀。\n\n- 要指定扫描范围，可在 `raw-scan` 命令中使用 `--from` 和 `--to` 参数（默认不限范围）\n- 要限制能够打印出的 key 的数量（默认为 `30`），可在命令中使用 `--limit` 参数\n- 要指定扫描的 CF，可在命令中使用 `--cf` 参数（可选值为 `default`，`write`，`lock`）\n\n```shell\ntikv-ctl --data-dir /var/lib/tikv raw-scan --from 'zt' --limit 2 --cf default\n```\n\n```\nkey: \"zt\\200\\000\\000\\000\\000\\000\\000\\377\\005_r\\200\\000\\000\\000\\000\\377\\000\\000\\001\\000\\000\\000\\000\\000\\372\\372b2,^\\033\\377\\364\", value: \"\\010\\002\\002\\002%\\010\\004\\002\\010root\\010\\006\\002\\000\\010\\010\\t\\002\\010\\n\\t\\002\\010\\014\\t\\002\\010\\016\\t\\002\\010\\020\\t\\002\\010\\022\\t\\002\\010\\024\\t\\002\\010\\026\\t\\002\\010\\030\\t\\002\\010\\032\\t\\002\\010\\034\\t\\002\\010\\036\\t\\002\\010 \\t\\002\\010\\\"\\t\\002\\010s\\t\\002\\010&\\t\\002\\010(\\t\\002\\010*\\t\\002\\010,\\t\\002\\010.\\t\\002\\0100\\t\\002\\0102\\t\\002\\0104\\t\\002\"\nkey: \"zt\\200\\000\\000\\000\\000\\000\\000\\377\\025_r\\200\\000\\000\\000\\000\\377\\000\\000\\023\\000\\000\\000\\000\\000\\372\\372b2,^\\033\\377\\364\", value: \"\\010\\002\\002&slow_query_log_file\\010\\004\\002P/usr/local/mysql/data/localhost-slow.log\"\n\nTotal scanned keys: 2\n```\n\n### 打印某个 key 的值\n\n打印某个 key 的值需要用到 `print` 命令。示例从略。\n\n### 打印 Region 的 properties 信息\n\n为了记录 Region 的状态信息，TiKV 将一些数据写入 Region 的 SST 文件中。你可以用子命令 `region-properties` 运行 tikv-ctl 来查看这些 properties 信息。例如：\n\n```shell\ntikv-ctl --host localhost:20160 region-properties -r 2\n```\n\n```\nnum_files: 0\nnum_entries: 0\nnum_deletes: 0\nmvcc.min_ts: 18446744073709551615\nmvcc.max_ts: 0\nmvcc.num_rows: 0\nmvcc.num_puts: 0\nmvcc.num_versions: 0\nmvcc.max_row_versions: 0\nmiddle_key_by_approximate_size:\n```\n\n这些 properties 信息可以用于检查某个 Region 是否健康或者修复不健康的 Region。例如，使用 `middle_key_approximate_size` 可以手动分裂 Region。\n\n### 手动 compact 单个 TiKV 的数据\n\n`compact` 命令可以对单个 TiKV 进行手动 compact。\n\n- `--from` 和 `--to` 选项以 escaped raw key 形式指定 compact 的范围。如果没有设置，表示 compact 整个 TiKV。\n- `--region` 选项指定 compact Region 的范围。如果设置，则 `--from` 和 `--to` 选项会被忽略。\n- `-c` 选项指定 column family 名称，默认值为 `default`，可选值为 `default`、`lock` 和 `write`。\n- `-d` 选项指定要 compact 的 RocksDB，默认值为 `kv`，可选值为 `kv` 和 `raft`。\n- `--threads` 选项可以指定 compact 的并发数，默认值是 8。一般来说，并发数越大，compact 的速度越快，但是也会对服务造成影响，所以需要根据情况选择合适的并发数。\n- `--bottommost` 选项可以指定 compact 是否包括最下层的文件。可选值为 `default`、`skip` 和 `force`，默认为 `default`。\n    - `default` 表示只有开启了 Compaction Filter 时 compact 才会包括最下层文件。\n    - `skip` 表示 compact 不包括最下层文件。\n    - `force` 表示 compact 总是包括最下层文件。\n\n- 在本地模式 compact data，执行如下命令：\n\n    ```shell\n    tikv-ctl --data-dir /path/to/tikv compact -d kv\n    ```\n\n- 在远程模式 compact data，执行如下命令：\n\n    ```shell\n    tikv-ctl --host ip:port compact -d kv\n    ```\n\n### 手动 compact 整个 TiKV 集群的数据\n\n`compact-cluster` 命令可以对整个 TiKV 集群进行手动 compact。该命令参数的含义和使用与 `compact` 命令一样，唯一的区别如下：\n\n- 使用 `compact-cluster` 命令时，通过 `--pd` 指定 PD 所在的地址，以便 `tikv-ctl` 可以找到集群中的所有 TiKV 节点作为 compact 目标。\n- 使用 `compact` 命令时，通过 `--data-dir` 或者 `--host` 指定单个 TiKV 作为 compact 目标。\n\n### 设置一个 Region 副本为 tombstone 状态\n\n`tombstone` 命令常用于没有开启 sync-log，因为机器掉电导致 Raft 状态机丢失部分写入的情况。它可以在一个 TiKV 实例上将一些 Region 的副本设置为 Tombstone 状态，从而在重启时跳过这些 Region，避免因为这些 Region 的副本的 Raft 状态机损坏而无法启动服务。这些 Region 应该在其他 TiKV 上有足够多的健康的副本以便能够继续通过 Raft 机制进行读写。\n\n一般情况下，可以先在 PD 上将 Region 的副本通过 `remove-peer` 命令删除掉：\n\n{{< copyable \"\" >}}\n\n```shell\npd-ctl>> operator add remove-peer <region_id> <store_id>\n```\n\n然后再用 tikv-ctl 在那个 TiKV 实例上将 Region 的副本标记为 tombstone 以便跳过启动时对他的健康检查：\n\n```shell\ntikv-ctl --data-dir /path/to/tikv tombstone -p 127.0.0.1:2379 -r <region_id>\n```\n\n```\nsuccess!\n```\n\n但是有些情况下，当不能方便地从 PD 上移除这个副本时，可以指定 tikv-ctl 的 `--force` 选项来强制设置它为 tombstone：\n\n```shell\ntikv-ctl --data-dir /path/to/tikv tombstone -p 127.0.0.1:2379 -r <region_id>,<region_id> --force\n```\n\n```\nsuccess!\n```\n\n> **注意：**\n>\n> - **该命令只支持本地模式**\n> - `-p` 选项的参数指定 PD 的 endpoints，无需 `http` 前缀。指定 PD 的 endpoints 是为了询问 PD 是否可以安全切换至 Tombstone 状态。\n\n### 向 TiKV 发出 consistency-check 请求\n\n`consistency-check` 命令用于在某个 Region 对应的 Raft 副本之间进行一致性检查。如果检查失败，TiKV 自身会 panic。如果 `--host` 指定的 TiKV 不是这个 Region 的 Leader，则会报告错误。\n\n```shell\ntikv-ctl --host 127.0.0.1:20160 consistency-check -r 2\n```\n\n```\nsuccess!\n```\n\n```shell\ntikv-ctl --host 127.0.0.1:20161 consistency-check -r 2\n```\n\n```\nDebugClient::check_region_consistency: RpcFailure(RpcStatus { status: Unknown, details: Some(\"StringError(\\\"Leader is on store 1\\\")\") })\n```\n\n> **注意：**\n>\n> - 目前 consistency-check 与 TiDB GC 操作不兼容，存在误报错误的可能，因此不建议使用该命令。\n> - **该命令只支持远程模式**。\n> - 即使该命令返回了成功信息，也需要检查是否有 TiKV panic 了。因为该命令只是向 Leader 请求进行一致性检查，但整个检查流程是否成功并不能在客户端知道。\n\n### Dump snapshot 元文件\n\n这条子命令可以用于解析指定路径下的 Snapshot 元文件并打印结果。\n\n### 打印 Raft 状态机出错的 Region\n\n前面 `tombstone` 命令可以将 Raft 状态机出错的 Region 设置为 Tombstone 状态，避免 TiKV 启动时对它们进行检查。在运行 `tombstone` 命令之前，可使用 `bad-regions` 命令找到出错的 Region，以便将多个工具组合起来进行自动化的处理。\n\n```shell\ntikv-ctl --data-dir /path/to/tikv bad-regions\n```\n\n```\nall regions are healthy\n```\n\n命令执行成功后会打印以上信息，否则会打印出有错误的 Region 列表。目前可以检出的错误包括 `last index`、`commit index` 和 `apply index` 之间的不匹配，以及 Raft log 的丢失。其他一些情况，比如 Snapshot 文件损坏等仍然需要后续的支持。\n\n### 查看 Region 属性\n\n- 本地查看部署在 `/path/to/tikv` 的 TiKV 上面 Region 2 的 properties 信息：\n\n    ```shell\n    tikv-ctl --data-dir /path/to/tikv/data region-properties -r 2\n    ```\n\n- 在线查看运行在 `127.0.0.1:20160` 的 TiKV 上面 Region 2 的 properties 信息：\n\n    ```shell\n    tikv-ctl --host 127.0.0.1:20160 region-properties -r 2\n    ```\n\n### 动态修改 TiKV 的配置\n\n使用 `modify-tikv-config` 命令可以动态修改配置参数。目前可动态修改的 TiKV 配置与具体的修改行为与 SQL 动态修改配置功能相同，可参考[在线修改 TiKV 配置](/dynamic-config.md#在线修改-tikv-配置)。\n\n- `-n` 用于指定完整的配置名。支持动态修改的配置名可以参考[在线修改 TiKV 配置](/dynamic-config.md#在线修改-tikv-配置)中支持的配置项列表。\n- `-v` 用于指定配置值。\n\n设置 `shared block cache` 的大小：\n\n```shell\ntikv-ctl --host ip:port modify-tikv-config -n storage.block-cache.capacity -v 10GB\n```\n\n```\nsuccess\n```\n\n当禁用 `shared block cache` 时，为 `write` CF 设置 `block cache size`：\n\n```shell\ntikv-ctl --host ip:port modify-tikv-config -n rocksdb.writecf.block-cache-size -v 256MB\n```\n\n```\nsuccess\n```\n\n```shell\ntikv-ctl --host ip:port modify-tikv-config -n raftdb.defaultcf.disable-auto-compactions -v true\n```\n\n```\nsuccess\n```\n\n```shell\ntikv-ctl --host ip:port modify-tikv-config -n raftstore.sync-log -v false\n```\n\n```\nsuccess\n```\n\n如果 compaction 的流量控制导致待 compact 数据量 (compaction pending bytes) 堆积，可以禁用 `rate-limiter-auto-tuned` 配置项或调高 compaction 相关的流量阈值。示例如下：\n\n```shell\ntikv-ctl --host ip:port modify-tikv-config -n rocksdb.rate-limiter-auto-tuned -v false\n```\n\n```\nsuccess\n```\n\n```shell\ntikv-ctl --host ip:port modify-tikv-config -n rocksdb.rate-bytes-per-sec -v \"1GB\"\n```\n\n```\nsuccess\n```\n\n### 强制 Region 从多副本失败状态恢复服务（弃用）\n\n> **警告：**\n>\n> 不推荐使用该功能，恢复需求可通过 `pd-ctl` 的 Online Unsafe Recovery 功能实现。它提供了一键式自动恢复的能力，无需停止服务等额外操作，具体使用方式请参考 [Online Unsafe Recovery 使用文档](/online-unsafe-recovery.md)。\n\n`unsafe-recover remove-fail-stores` 命令可以将故障机器从指定 Region 的 peer 列表中移除。运行命令之前，需要目标 TiKV 先停掉服务以便释放文件锁。\n\n`-s` 选项接受多个以逗号分隔的 `store_id`，并使用 `-r` 参数来指定包含的 Region。如果要对某一个 store 上的全部 Region 都执行这个操作，可简单指定 `--all-regions`。\n\n> **警告：**\n>\n> - 此功能使用不当可能导致集群难以恢复，存在风险。请悉知潜在的风险，尽量避免在生产环境中使用。\n> - 如果使用 `--all-regions`，必须在剩余所有连入集群的 store 上执行此命令。需要保证这些健康的 store 都停掉服务后再进行恢复，否则期间 Region 副本之间的 peer 列表不一致会导致执行 `split-region` 或者 `remove-peer` 时报错进而引起其他元数据的不一致，最终引发 Region 不可用。\n> - 一旦执行了 `remove-fail-stores`，不可再重新启动被移除的节点并将其加入集群，否则会导致元数据的不一致，最终引发 Region 不可用。\n\n```shell\ntikv-ctl --data-dir /path/to/tikv unsafe-recover remove-fail-stores -s 3 -r 1001,1002\n```\n\n```\nsuccess!\n```\n\n```shell\ntikv-ctl --data-dir /path/to/tikv unsafe-recover remove-fail-stores -s 4,5 --all-regions\n```\n\n之后启动 TiKV，这些 Region 便可以使用剩下的健康副本继续提供服务了。此命令常用于多个 TiKV store 损坏或被删除的情况。\n\n> **注意：**\n>\n> - 一般来说，您需要为指定 Region 的 peers 所在的每个 store 运行此命令。\n> - 该命令只支持本地模式。在运行成功后，会打印 `success!`。\n\n### 恢复损坏的 MVCC 数据\n\n`recover-mvcc` 命令用于 MVCC 数据损坏导致 TiKV 无法正常运行的情况。为了从不同种类的不一致情况中恢复，该命令会交叉检查 3 个 CF (\"default\", \"write\", \"lock\")。\n\n- `-r` 选项可以通过 `region_id` 指定包含的 Region。\n- `-p` 选项可以指定 PD 的 endpoints。\n\n```shell\ntikv-ctl --data-dir /path/to/tikv recover-mvcc -r 1001,1002 -p 127.0.0.1:2379\n```\n\n```\nsuccess!\n```\n\n> **注意：**\n>\n> - 该命令只支持本地模式。在运行成功后，会打印 `success!`。\n> - `-p` 选项指定 PD 的 endpoint，不使用 `http` 前缀，用于查询指定的 `region_id` 是否有效。\n> - 对于指定 Region 的 peers 所在的每个 store，均须执行该命令。\n\n### Ldb 命令\n\n`ldb` 命令行工具提供多种数据访问以及数据库管理命令。下方列出了一些示例用法。详细信息请在运行 `tikv-ctl ldb` 命令时查看帮助消息或查阅 RocksDB 文档。\n\n数据访问序列的示例如下：\n\n用 HEX 格式 dump 现有 RocksDB 数据：\n\n```shell\ntikv-ctl ldb --hex --db=/tmp/db dump\n```\n\nDump 现有 RocksDB 的声明：\n\n```shell\ntikv-ctl ldb --hex manifest_dump --path=/tmp/db/MANIFEST-000001\n```\n\n您可以通过 `--column_family=<string>` 指定查询的目标列族。\n\n通过 `--try_load_options` 命令加载数据库选项文件以打开数据库。在数据库运行时，建议您保持该命令为开启的状态。如果您使用默认配置打开数据库，LSM-tree 存储组织可能会出现混乱，且无法自动恢复。\n\n### 打印加密元数据\n\n`encryption-meta` 命令用于打印加密元数据。该子命令可以打印两种加密元数据：数据文件的加密信息，以及所有的数据加密密钥。\n\n使用 `encryption-meta dump-file` 子命令打印数据文件的加密信息。你需要创建一个 TiKV 配置文件用以指定 TiKV 的数据目录：\n\n```\n# conf.toml\n[storage]\ndata-dir = \"/path/to/tikv/data\"\n```\n\n`--path` 选项可以指定数据文件的绝对或者相对路径。如果指定的文件是明文存储的，本命令有可能没有输出。如果不指定 `--path` 选项，本命令打印所有数据文件的加密信息。\n\n```shell\ntikv-ctl --config=./conf.toml encryption-meta dump-file --path=/path/to/tikv/data/db/CURRENT\n/path/to/tikv/data/db/CURRENT: key_id: 9291156302549018620 iv: E3C2FDBF63FC03BFC28F265D7E78283F method: Aes128Ctr\n```\n\n使用 `encryption-meta dump-key` 打印数据加密密钥。使用本命令的时候，除了在 TiKV 配置文件中指定 TiKV 的数据目录以外，还需要指定当前的主加密密钥。请参阅[静态加密](/encryption-at-rest.md)文档关于配置 TiKV 主加密密钥的说明。使用本命令时 `security.encryption.previous-master-key` 配置项不生效，即使配置文件中使用了该配置，本命令也不会触发更换主加密密钥。\n\n```\n# conf.toml\n[storage]\ndata-dir = \"/path/to/tikv/data\"\n\n[security.encryption.master-key]\ntype = \"kms\"\nkey-id = \"0987dcba-09fe-87dc-65ba-ab0987654321\"\nregion = \"us-west-2\"\n```\n\n注意如果使用了 AWS KMS 作为主加密密钥，使用本命令时 `tikv-ctl` 需要该 KMS 密钥的访问权限。KMS 访问权限可以通过环境变量、AWS 默认配置文件或 IAM 的方式传递给 `tikv-ctl`。详情请参阅相关 AWS 文档。\n\n`--ids` 选项可以指定以逗号分隔的数据加密密钥 id 列表。如果不指定 `--ids` 选项，本命令打印所有的数据加密密钥，以及最新的数据加密密钥的 id。\n\n本命令会输出一个警告，提示本命令会泄漏敏感数据。根据提示输入 \"I consent\" 即可。\n\n```shell\ntikv-ctl --config=./conf.toml encryption-meta dump-key\nThis action will expose encryption key(s) as plaintext. Do not output the result in file on disk.\nType \"I consent\" to continue, anything else to exit: I consent\ncurrent key id: 9291156302549018620\n9291156302549018620: key: 8B6B6B8F83D36BE2467ED55D72AE808B method: Aes128Ctr creation_time: 1592938357\n```\n\n```shell\ntikv-ctl --config=./conf.toml encryption-meta dump-key --ids=9291156302549018620\nThis action will expose encryption key(s) as plaintext. Do not output the result in file on disk.\nType \"I consent\" to continue, anything else to exit: I consent\n9291156302549018620: key: 8B6B6B8F83D36BE2467ED55D72AE808B method: Aes128Ctr creation_time: 1592938357\n```\n\n> **注意：**\n>\n> 本命令会以明文方式打印数据加密密钥。在生产环境中，请勿将本命令的输出重定向到磁盘文件中。即使使用以后删除该文件也不能保证文件内容从磁盘中干净清除。\n\n### 打印损坏的 SST 文件信息\n\nTiKV 中损坏的 SST 文件会导致 TiKV 进程崩溃。在 TiDB v6.1.0 之前，损坏的 SST 文件会导致 TiKV 进程立即崩溃。从 TiDB v6.1.0 起，TiKV 进程会在 SST 文件损坏 1 小时后崩溃。\n\n为了方便清理掉这些 SST 文件，你可以先使用 `bad-ssts` 命令打印出损坏的 SST 文件信息。\n\n> **注意：**\n>\n> 执行此命令前，请保证关闭当前运行的 TiKV 实例。\n\n```shell\ntikv-ctl --data-dir </path/to/tikv> bad-ssts --pd <endpoint>\n```\n\n```shell\n--------------------------------------------------------\ncorruption info:\ndata/tikv-21107/db/000014.sst: Corruption: Bad table magic number: expected 9863518390377041911, found 759105309091689679 in data/tikv-21107/db/000014.sst\nsst meta:\n14:552997[1 .. 5520]['0101' seq:1, type:1 .. '7A7480000000000000FF0F5F728000000000FF0002160000000000FAFA13AB33020BFFFA' seq:2032, type:1] at level 0 for Column family \"default\"  (ID 0)\nit isn't easy to handle local data, start key:0101\noverlap region:\nRegionInfo { region: id: 4 end_key: 7480000000000000FF0500000000000000F8 region_epoch { conf_ver: 1 version: 2 } peers { id: 5 store_id: 1 }, leader: Some(id: 5 store_id: 1) }\nrefer operations:\ntikv-ctl ldb --db=/path/to/tikv/db unsafe_remove_sst_file 000014\ntikv-ctl --data-dir=/path/to/tikv tombstone -r 4 --pd <endpoint>\n--------------------------------------------------------\ncorruption analysis has completed\n```\n\n通过上面的输出，你可以看到损坏的 SST 文件和损坏原因等信息先被打印出，然后是相关的元信息。\n\n+ 在 `sst meta` 输出部分，`14` 表示 SST 文件号，`552997` 表示文件大小，紧随其后的是最小和最大的序列号 (seq) 等其它元信息。\n+ `overlap region` 部分为损坏 SST 文件所在 Region 的信息。该信息是从 PD 组件获取的。\n+ `suggested operations` 部分为你清理损坏的 SST 文件提供建议操作。你可以参考这些建议的命令，清理文件，并重新启动该 TiKV 实例。\n\n### 获取一个 Region 的 `RegionReadProgress` 状态\n\n从 v6.5.4 和 v7.3.0 开始，TiKV 引入 `get-region-read-progress` 子命令，用于获取 resolver 和 `RegionReadProgress` 的最新状态。你需要指定一个 Region ID 和一个 TiKV，这可以从 Grafana（`Min Resolved TS Region` 和 `Min Safe TS Region`）或 `DataIsNotReady` 日志中获得。\n\n- `--log`（可选）：如果指定，TiKV 会在 `INFO` 日志级别下记录该 TiKV 中 Region 的 resolver 中最小的锁 `start_ts`。该选项有助于提前识别可能阻塞 resolved-ts 的锁。\n\n- `--min-start-ts`（可选）：如果指定，TiKV 会在日志中过滤掉 `start_ts` 小于该值的锁。你可以使用该选项指定一个感兴趣的事务，以便在日志中记录。默认值为 `0`，表示不过滤。\n\n下面是一个使用示例：\n\n```\n./tikv-ctl --host 127.0.0.1:20160 get-region-read-progress -r 14 --log --min-start-ts 0\n```\n\n输出结果如下：\n\n```\nRegion read progress:\n    exist: true,\n    safe_ts: 0,\n    applied_index: 92,\n    pending front item (oldest) ts: 0,\n    pending front item (oldest) applied index: 0,\n    pending back item (latest) ts: 0,\n    pending back item (latest) applied index: 0,\n    paused: false,\nResolver:\n    exist: true,\n    resolved_ts: 0,\n    tracked index: 92,\n    number of locks: 0,\n    number of transactions: 0,\n    stopped: false,\n```\n\n该子命令有助于诊断与 Stale Read 和 safe-ts 相关的问题。详情请参阅[理解 TiKV 中的 Stale Read 和 safe-ts](/troubleshoot-stale-read.md)。\n"
        },
        {
          "name": "tikv-in-memory-engine.md",
          "type": "blob",
          "size": 7.8779296875,
          "content": "---\ntitle: TiKV MVCC 内存引擎\nsummary: 了解内存引擎的适用场景和工作原理，使用内存引擎加速多版本记录查询。\n---\n\n# TiKV MVCC 内存引擎\n\nTiKV MVCC 内存引擎 (In-Memory Engine, IME) 主要用于加速需要扫描大量 MVCC 历史版本的查询，即[查询扫描的总共版本数量 (`total_keys`) 远大于处理的版本数量 (`processed_keys`)](/analyze-slow-queries.md#过期-mvcc-版本和-key-过多)。\n\nTiKV MVCC 内存引擎适用于以下场景：\n\n- 业务需要查询频繁更新或删除的记录。\n- 业务需要调整 [`tidb_gc_life_time`](/garbage-collection-configuration.md#gc-配置)，使 TiDB 保留较长时间的历史版本（比如 24 小时）。\n\n## 工作原理\n\nTiKV MVCC 内存引擎在内存中缓存最近写入的 MVCC 版本，并实现独立于 TiDB 的 MVCC GC 机制，使其可快速 GC 内存中的 MVCC 记录，从而减少查询时扫描版本的个数，以达到降低请求延时和减少 CPU 开销的效果。\n\n下图为 TiKV 如何组织 MVCC 版本的示意图：\n\n![IME 通过缓存近期的版本以减少 CPU 开销](/media/tikv-ime-data-organization.png)\n\n以上示意图中共有 2 行记录，每行记录各有 9 个 MVCC 版本。在开启内存引擎和未开启内存引擎的情况下，行为对比如下：\n\n- 左侧（未开启内存引擎）：表中记录按主键升序保存在 RocksDB 中，相同行的 MVCC 版本紧邻在一起。\n- 右侧（开启了内存引擎）：RocksDB 中的数据与左侧一致，同时内存引擎缓存了 2 行记录最新的 2 个 MVCC 版本。\n- 当 TiKV 处理一个范围为 `[k1, k2]`，开始时间戳为 `8` 的扫描请求时：\n    - 左侧未开启内存引擎时需要处理 11 个 MVCC 版本。\n    - 右侧开启内存引擎时只需处理 4 个 MVCC 版本，因此减少了请求延时和 CPU 消耗。\n- 当 TiKV 处理一个范围为 `[k1, k2]`，开始时间戳为 `7` 的扫描请求时：\n    - 由于右侧缺少需要读取的历史版本，因此内存引擎缓存失效，回退到读取 RocksDB 中的数据。\n\n## 使用方式\n\n如果要开启 TiKV MVCC 内存引擎 (IME) 功能，需要调整 TiKV 配置并重启 TiKV。以下是配置说明：\n\n```toml\n[in-memory-engine]\n# 该参数为内存引擎功能的开关，默认为 false，调整为 true 即可开启。\nenable = false\n\n# 该参数控制内存引擎可使用的内存大小。默认值为系统内存的 10%，同时最大值为 5 GiB，\n# 可通过手动调整配置以使用更多内存。\n# 注意：当内存引擎开启后，block-cache.capacity 会减少 10%。\ncapacity = \"5GiB\"\n\n# 该参数控制内存引擎 GC 缓存 MVCC 的版本的时间间隔。\n# 默认为 3 分钟，代表每 3 分钟 GC 一次缓存的 MVCC 版本。\n# 调小该参数可加快 GC 频率，减少 MVCC 记录，但会增加 GC CPU 的消耗和增加内存引擎失效的概率。\ngc-run-interval = \"3m\"\n\n# 该参数控制内存引擎选取加载 Region 时 MVCC 读放大的阈值。\n# 默认为 10，表示在某个 Region 中读一行记录需要处理的 MVCC 版本数量超过 10 个时，将有可能会被加载到内存引擎中。\nmvcc-amplification-threshold = 10\n```\n\n> **注意：**\n>\n> + 内存引擎默认关闭，并且从关闭状态修改为开启状态后，需要重启 TiKV。\n> + 除 `enable` 之外，其他配置都可以动态调整。\n\n### 自动加载\n\n开启内存引擎之后，TiKV 会根据 Region 的读流量和 MVCC 放大程度，选择要自动加载的 Region。具体流程如下：\n\n1. Region 按照最近时间段的 `next` (RocksDB Iterator next API) 和 `prev` (RocksDB Iterator prev API) 次数进行排序。\n2. 使用 `mvcc-amplification-threshold` 配置项对 Region 进行过滤，该配置项的默认值为 `10`。MVCC amplification 衡量读放大程度，计算公式为 (`next` + `prev`) / `processed_keys`）。\n3. 载入前 N 个 MVCC 放大严重的 Region，其中 N 基于内存估算而来。\n\n内存引擎也会定期驱逐 Region。具体流程如下：\n\n1. 内存引擎会驱逐那些读流量过小或者 MVCC 放大程度过低的 Region。\n2. 如果内存使用达到了 `capacity` 的 90%，并且有新的 Region 需要被载入，那么内存引擎会根据读取流量来筛选 Region 并进行驱逐。\n\n## 兼容性\n\n+ [BR](/br/br-use-overview.md)：内存引擎与 BR 可同时使用，但 BR restore 会驱逐内存引擎中涉及恢复的 Region，BR restore 完成后，如果对应 Region 还是热点，则会被内存引擎自动加载。\n+ [TiDB Lightning](/tidb-lightning/tidb-lightning-overview.md)：内存引擎与 TiDB Lightning 可同时使用，但 TiDB Lightning 的物理导入模式会驱逐内存引擎中涉及恢复的 Region，TiDB Lightning 使用物理导入模式完成导入数据后，如果对应 Region 还是热点，则会被内存引擎自动加载。\n+ [Follower Read](/develop/dev-guide-use-follower-read.md) 与 [Stale Read](/develop/dev-guide-use-stale-read.md)：内存引擎可与这两个特性同时开启，但内存引擎只能加速 Leader 上的 coprocessor 请求，无法加速 Follower Read 和 Stale Read。\n+ [`FLASHBACK CLUSTER`](/sql-statements/sql-statement-flashback-cluster.md)：内存引擎与 Flashback 可同时使用，但 Flashback 会导致内存引擎缓存失效。Flashback 完成后，内存引擎会自动加载热点 Region。\n\n## FAQ\n\n### 内存引擎能否减少写入延时，提高写入吞吐？\n\n不能。内存引擎只能加速扫描了大量 MVCC 版本的读请求。\n\n### 如何判断内存引擎是否能改善我的场景？\n\n可以通过执行以下 SQL 语句查看是否存在 `Total_keys` 远大于 `Process_keys` 的慢查询：\n\n```sql\nSELECT\n    Time,\n    DB,\n    Index_names,\n    Process_keys,\n    Total_keys,\n    CONCAT(\n        LEFT(REGEXP_REPLACE(Query, '\\\\s+', ' '), 20),\n        '...',\n        RIGHT(REGEXP_REPLACE(Query, '\\\\s+', ' '), 10)\n    ) as Query,\n    Query_time,\n    Cop_time,\n    Process_time\nFROM\n    INFORMATION_SCHEMA.SLOW_QUERY\nWHERE\n    Is_internal = 0\n    AND Cop_time > 1\n    AND Process_keys > 0\n    AND Total_keys / Process_keys >= 10\n    AND Time >= NOW() - INTERVAL 10 MINUTE\nORDER BY Total_keys DESC\nLIMIT 5;\n```\n\n示例：\n\n以下结果显示 `db1.tbl1` 表上存在 MVCC 放大严重的查询，TiKV 在处理 1358517 个 MVCC 版本后，仅返回了 2 个版本。\n\n```\n+----------------------------+-----+-------------------+--------------+------------+-----------------------------------+--------------------+--------------------+--------------------+\n| Time                       | DB  | Index_names       | Process_keys | Total_keys | Query                             | Query_time         | Cop_time           | Process_time       |\n+----------------------------+-----+-------------------+--------------+------------+-----------------------------------+--------------------+--------------------+--------------------+\n| 2024-11-18 11:56:10.303228 | db1 | [tbl1:some_index] |            2 |    1358517 |  SELECT * FROM tbl1 ... LIMIT 1 ; | 1.2581352350000001 |         1.25651062 |        1.251837479 |\n| 2024-11-18 11:56:11.556257 | db1 | [tbl1:some_index] |            2 |    1358231 |  SELECT * FROM tbl1 ... LIMIT 1 ; |        1.252694002 |        1.251129038 |        1.240532546 |\n| 2024-11-18 12:00:10.553331 | db1 | [tbl1:some_index] |            2 |    1342914 |  SELECT * FROM tbl1 ... LIMIT 1 ; |        1.473941872 | 1.4720495900000001 | 1.3666103170000001 |\n| 2024-11-18 12:01:52.122548 | db1 | [tbl1:some_index] |            2 |    1128064 |  SELECT * FROM tbl1 ... LIMIT 1 ; |        1.058942591 |        1.056853228 |        1.023483875 |\n| 2024-11-18 12:01:52.107951 | db1 | [tbl1:some_index] |            2 |    1128064 |  SELECT * FROM tbl1 ... LIMIT 1 ; |        1.044847031 |        1.042546122 |        0.934768555 |\n+----------------------------+-----+-------------------+--------------+------------+-----------------------------------+--------------------+--------------------+--------------------+\n5 rows in set (1.26 sec)\n```\n"
        },
        {
          "name": "tikv-overview.md",
          "type": "blob",
          "size": 3.9443359375,
          "content": "---\ntitle: TiKV 简介\naliases: ['/docs-cn/dev/tikv-overview/']\nsummary: TiKV 是一个分布式事务型的键值数据库，通过 Raft 协议保证了多副本数据一致性和高可用。整体架构采用 multi-raft-group 的副本机制，保证数据和读写负载均匀分散在各个 TiKV 上。TiKV 支持分布式事务，通过两阶段提交保证了 ACID 约束。同时，通过协处理器可以为 TiDB 分担一部分计算。\n---\n\n# TiKV 简介\n\nTiKV 是一个分布式事务型的键值数据库，提供了满足 ACID 约束的分布式事务接口，并且通过 [Raft 协议](https://raft.github.io/raft.pdf)保证了多副本数据一致性以及高可用。TiKV 作为 TiDB 的存储层，为用户写入 TiDB 的数据提供了持久化以及读写服务，同时还存储了 TiDB 的统计信息数据。\n\n## 整体架构\n\n与传统的整节点备份方式不同，TiKV 参考 Spanner 设计了 multi-raft-group 的副本机制。将数据按照 key 的范围划分成大致相等的切片（下文统称为 Region），每一个切片会有多个副本（通常是 3 个），其中一个副本是 Leader，提供读写服务。TiKV 通过 PD 对这些 Region 以及副本进行调度，以保证数据和读写负载都均匀地分散在各个 TiKV 上，这样的设计保证了整个集群资源的充分利用并且可以随着机器数量的增加水平扩展。\n\n![TiKV 架构](/media/tikv-arch.png)\n\n### Region 与 RocksDB\n\n虽然 TiKV 将数据按照范围切割成了多个 Region，但是同一个节点的所有 Region 数据仍然是不加区分地存储于同一个 RocksDB 实例上，而用于 Raft 协议复制所需要的日志则存储于另一个 RocksDB 实例。这样设计的原因是因为随机 I/O 的性能远低于顺序 I/O，所以 TiKV 使用同一个 RocksDB 实例来存储这些数据，以便不同 Region 的写入可以合并在一次 I/O 中。\n\n### Region 与 Raft 协议\n\nRegion 与副本之间通过 Raft 协议来维持数据一致性，任何写请求都只能在 Leader 上写入，并且需要写入多数副本后（默认配置为 3 副本，即所有请求必须至少写入两个副本成功）才会返回客户端写入成功。\n\nTiKV 会尽量保持每个 Region 中保存的数据在一个合适的大小，目前默认是 256 MB，这样更有利于 PD 进行调度决策。当某个 Region 的大小超过一定限制（默认是 384 MiB）后，TiKV 会将它分裂为两个或者更多个 Region。同样，当某个 Region 因为大量的删除请求而变得太小时（默认是 54 MiB），TiKV 会将比较小的两个相邻 Region 合并为一个。\n\n当 PD 需要把某个 Region 的一个副本从一个 TiKV 节点调度到另一个上面时，PD 会先为这个 Raft Group 在目标节点上增加一个 Learner 副本（虽然会复制 Leader 的数据，但是不会计入写请求的多数副本中）。当这个 Learner 副本的进度大致追上 Leader 副本时，Leader 会将它变更为 Follower，之后再移除操作节点的 Follower 副本，这样就完成了 Region 副本的一次调度。\n\nLeader 副本的调度原理也类似，不过需要在目标节点的 Learner 副本变为 Follower 副本后，再执行一次 Leader Transfer，让该 Follower 主动发起一次选举成为新 Leader，之后新 Leader 负责删除旧 Leader 这个副本。\n\n## 分布式事务\n\nTiKV 支持分布式事务，用户（或者 TiDB）可以一次性写入多个 key-value 而不必关心这些 key-value 是否处于同一个数据切片 (Region) 上，TiKV 通过两阶段提交保证了这些读写请求的 ACID 约束，详见 [TiDB 乐观事务模型](/optimistic-transaction.md)。\n\n## 计算加速\n\nTiKV 通过协处理器 (Coprocessor) 可以为 TiDB 分担一部分计算：TiDB 会将可以由存储层分担的计算下推。能否下推取决于 TiKV 是否可以支持相关下推。计算单元仍然是以 Region 为单位，即 TiKV 的一个 Coprocessor 计算请求中不会计算超过一个 Region 的数据。\n"
        },
        {
          "name": "time-to-live.md",
          "type": "blob",
          "size": 16.2109375,
          "content": "---\ntitle: 使用 TTL (Time to Live) 定期删除过期数据\nsummary: Time to Live (TTL) 提供了行级别的生命周期控制策略。本篇文档介绍如何通过 TTL (Time to Live) 来管理表数据的生命周期。\n---\n\n# 使用 TTL (Time to Live) 定期删除过期数据\n\nTime to Live (TTL) 提供了行级别的生命周期控制策略。通过为表设置 TTL 属性，TiDB 可以周期性地自动检查并清理表中的过期数据。此功能在一些场景可以有效节省存储空间、提升性能。\n\nTTL 常见的使用场景：\n\n* 定期删除验证码、短网址记录\n* 定期删除不需要的历史订单\n* 自动删除计算的中间结果\n\nTTL 设计的目标是在不影响在线读写负载的前提下，帮助用户周期性且及时地清理不需要的数据。TTL 会以表为单位，并发地分发不同的任务到不同的 TiDB Server 节点上，进行并行删除处理。TTL 并不保证所有过期数据立即被删除，也就是说即使数据过期了，客户端仍然有可能在这之后的一段时间内读到过期的数据，直到其真正的被后台处理任务删除。\n\n## 语法\n\n你可以通过 [`CREATE TABLE`](/sql-statements/sql-statement-create-table.md) 或 [`ALTER TABLE`](/sql-statements/sql-statement-alter-table.md) 语句来配置表的 TTL 功能。\n\n### 创建具有 TTL 属性的表\n\n- 创建一个具有 TTL 属性的表：\n\n    ```sql\n    CREATE TABLE t1 (\n        id int PRIMARY KEY,\n        created_at TIMESTAMP\n    ) TTL = `created_at` + INTERVAL 3 MONTH;\n    ```\n\n    上面的例子创建了一张表 `t1`，并指定了 `created_at` 为 TTL 的时间列，表示数据的创建时间。同时，它还通过 `INTERVAL 3 MONTH` 设置了表中行的最长存活时间为 3 个月。超过了此时长的过期数据会在之后被删除。\n\n- 设置 `TTL_ENABLE` 属性来开启或关闭清理过期数据的功能：\n\n    ```sql\n    CREATE TABLE t1 (\n        id int PRIMARY KEY,\n        created_at TIMESTAMP\n    ) TTL = `created_at` + INTERVAL 3 MONTH TTL_ENABLE = 'OFF';\n    ```\n\n    如果 `TTL_ENABLE` 被设置成了 `OFF`，则即使设置了其他 TTL 选项，当前表也不会自动清理过期数据。对于一个设置了 TTL 属性的表，`TTL_ENABLE` 在缺省条件下默认为 `ON`。\n\n- 为了与 MySQL 兼容，你也可以使用注释语法来设置 TTL：\n\n    ```sql\n    CREATE TABLE t1 (\n        id int PRIMARY KEY,\n        created_at TIMESTAMP\n    ) /*T![ttl] TTL = `created_at` + INTERVAL 3 MONTH TTL_ENABLE = 'OFF'*/;\n    ```\n\n    在 TiDB 环境中，使用表的 TTL 属性和注释语法来配置 TTL 是等价的。在 MySQL 环境中，会自动忽略注释中的内容，并创建普通的表。\n\n### 修改表的 TTL 属性\n\n- 修改表的 TTL 属性：\n\n    ```sql\n    ALTER TABLE t1 TTL = `created_at` + INTERVAL 1 MONTH;\n    ```\n\n    上面的语句既支持修改已配置 TTL 属性的表，也支持为一张非 TTL 的表添加 TTL 属性。\n\n- 单独修改 TTL 表的 `TTL_ENABLE` 值：\n\n    ```sql\n    ALTER TABLE t1 TTL_ENABLE = 'OFF';\n    ```\n\n- 清除一张表的所有 TTL 属性：\n\n    ```sql\n    ALTER TABLE t1 REMOVE TTL;\n    ```\n\n### TTL 和数据类型的默认值\n\nTTL 可以和[数据类型的默认值](/data-type-default-values.md)一起使用。以下是两种常见的用法示例：\n\n* 使用 `DEFAULT CURRENT_TIMESTAMP` 来指定某一列的默认值为该行的创建时间，并用这一列作为 TTL 的时间列，创建时间超过 3 个月的数据将被标记为过期：\n\n    ```sql\n    CREATE TABLE t1 (\n        id int PRIMARY KEY,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n    ) TTL = `created_at` + INTERVAL 3 MONTH;\n    ```\n\n* 指定某一列的默认值为该行的创建时间或更新时间，并用这一列作为 TTL 的时间列，创建时间或更新时间超过 3 个月的数据将被标记为过期：\n\n    ```sql\n    CREATE TABLE t1 (\n        id int PRIMARY KEY,\n        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP\n    ) TTL = `created_at` + INTERVAL 3 MONTH;\n    ```\n\n### TTL 和生成列\n\nTTL 可以和[生成列](/generated-columns.md)一起使用，用来表达更加复杂的过期规则。例如：\n\n```sql\nCREATE TABLE message (\n    id int PRIMARY KEY,\n    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n    image bool,\n    expire_at TIMESTAMP AS (IF(image,\n            created_at + INTERVAL 5 DAY,\n            created_at + INTERVAL 30 DAY\n    ))\n) TTL = `expire_at` + INTERVAL 0 DAY;\n```\n\n上述语句的消息以 `expire_at` 列来作为过期时间，并按照消息类型来设定。如果是图片，则 5 天后过期，不然就 30 天后过期。\n\nTTL 还可以和 [JSON 类型](/data-type-json.md) 一起使用。例如：\n\n```sql\nCREATE TABLE orders (\n    id INT NOT NULL AUTO_INCREMENT PRIMARY KEY,\n    order_info JSON,\n    created_at DATE AS (JSON_EXTRACT(order_info, '$.created_at')) VIRTUAL\n) TTL = `created_at` + INTERVAL 3 month;\n```\n\n## TTL 任务\n\n对于每张设置了 TTL 属性的表，TiDB 内部会定期调度后台任务来清理过期的数据。你可以通过给表设置 `TTL_JOB_INTERVAL` 属性来自定义任务的执行周期，比如通过下面的语句将后台清理任务设置为每 24 小时执行一次：\n\n```sql\nALTER TABLE orders TTL_JOB_INTERVAL = '24h';\n```\n\n`TTL_JOB_INTERVAL` 的默认值是 `1h`。\n\n在执行 TTL 任务时，TiDB 会基于 Region 的数量将表拆分为最多 64 个子任务。这些子任务会被分发到不同的 TiDB 节点中执行。你可以通过设置系统变量 [`tidb_ttl_running_tasks`](/system-variables.md#tidb_ttl_running_tasks-从-v700-版本开始引入) 来限制整个集群中同时执行的 TTL 子任务数量。然而，并非所有表的 TTL 任务都可以被拆分为子任务。请参考[使用限制](#使用限制)以了解哪些表的 TTL 任务不能被拆分。\n\n如果想禁止 TTL 任务的执行，除了可以设置表属性 `TTL_ENABLE='OFF'` 外，也可以通过设置全局变量 `tidb_ttl_job_enable` 关闭整个集群的 TTL 任务的执行。\n\n```sql\nSET @@global.tidb_ttl_job_enable = OFF;\n```\n\n在某些场景下，你可能希望只允许在每天的某个时间段内调度后台的 TTL 任务，此时可以设置全局变量 [`tidb_ttl_job_schedule_window_start_time`](/system-variables.md#tidb_ttl_job_schedule_window_start_time-从-v650-版本开始引入) 和 [`tidb_ttl_job_schedule_window_end_time`](/system-variables.md#tidb_ttl_job_schedule_window_end_time-从-v650-版本开始引入) 来指定时间窗口，比如：\n\n```sql\nSET @@global.tidb_ttl_job_schedule_window_start_time = '01:00 +0000';\nSET @@global.tidb_ttl_job_schedule_window_end_time = '05:00 +0000';\n```\n\n上述语句只允许在 UTC 时间的凌晨 1 点到 5 点调度 TTL 任务。默认情况下的时间窗口设置为 `00:00 +0000` 到 `23:59 +0000`，即允许所有时段的任务调度。\n\n## TTL 的可观测性\n\nTiDB 会定时采集 TTL 的运行时信息，并在 Grafana 中提供了相关指标的可视化图表。你可以在 TiDB -> TTL 的面板下看到这些信息。指标详情见 [TiDB 重要监控指标详解](/grafana-tidb-dashboard.md) 中的 `TTL` 部分。\n\n同时，可以通过以下三个系统表获得 TTL 任务执行的更多信息：\n\n+ `mysql.tidb_ttl_table_status` 表中包含了所有 TTL 表的上一次执行与正在执行的 TTL 任务的信息。以其中一行为例：\n\n    ```sql\n    TABLE mysql.tidb_ttl_table_status LIMIT 1\\G\n    ```\n\n    ```sql\n    *************************** 1. row ***************************\n                          table_id: 85\n                   parent_table_id: 85\n                  table_statistics: NULL\n                       last_job_id: 0b4a6d50-3041-4664-9516-5525ee6d9f90\n               last_job_start_time: 2023-02-15 20:43:46\n              last_job_finish_time: 2023-02-15 20:44:46\n               last_job_ttl_expire: 2023-02-15 19:43:46\n                  last_job_summary: {\"total_rows\":4369519,\"success_rows\":4369519,\"error_rows\":0,\"total_scan_task\":64,\"scheduled_scan_task\":64,\"finished_scan_task\":64}\n                    current_job_id: NULL\n              current_job_owner_id: NULL\n            current_job_owner_addr: NULL\n         current_job_owner_hb_time: NULL\n            current_job_start_time: NULL\n            current_job_ttl_expire: NULL\n                 current_job_state: NULL\n                current_job_status: NULL\n    current_job_status_update_time: NULL\n    1 row in set (0.040 sec)\n    ```\n\n    其中列 `table_id` 为分区表 ID，而 `parent_table_id` 为表的 ID，与 [`information_schema.tables`](/information-schema/information-schema-tables.md) 表中的 ID 对应。如果表不是分区表，则 `table_id` 与 `parent_table_id` 总是相等。\n\n    列 `{last, current}_job_{start_time, finish_time, ttl_expire}` 分别描述了过去和当前 TTL 任务的开始时间、结束时间和过期时间。`last_job_summary` 列描述了上一次 TTL 任务的执行情况，包括总行数、成功行数、失败行数。\n\n+ `mysql.tidb_ttl_task` 表中包含了正在执行的 TTL 子任务。单个 TTL 任务会被拆分为多个子任务，该表中记录了正在执行的这些子任务的信息。\n+ `mysql.tidb_ttl_job_history` 表中记录了 TTL 任务的执行历史。TTL 任务的历史记录将被保存 90 天。以一行为例：\n\n    ```sql\n    TABLE mysql.tidb_ttl_job_history LIMIT 1\\G\n    ```\n\n    ```\n    *************************** 1. row ***************************\n              job_id: f221620c-ab84-4a28-9d24-b47ca2b5a301\n            table_id: 85\n     parent_table_id: 85\n        table_schema: test_schema\n          table_name: TestTable\n      partition_name: NULL\n         create_time: 2023-02-15 17:43:46\n         finish_time: 2023-02-15 17:45:46\n          ttl_expire: 2023-02-15 16:43:46\n        summary_text: {\"total_rows\":9588419,\"success_rows\":9588419,\"error_rows\":0,\"total_scan_task\":63,\"scheduled_scan_task\":63,\"finished_scan_task\":63}\n        expired_rows: 9588419\n        deleted_rows: 9588419\n   error_delete_rows: 0\n              status: finished\n    ```\n\n  其中列 `table_id` 为分区表 ID，而 `parent_table_id` 为表的 ID，与 `information_schema.tables` 表中的 ID 对应。`table_schema`、`table_name`、`partition_name` 分别对应表示数据库、表名、分区名。`create_time`、`finish_time`、`ttl_expire` 分别表示 TTL 任务的创建时间、结束时间和过期时间。`expired_rows` 与 `deleted_rows` 表示过期行数与成功删除的行数。\n\n## TiDB 数据迁移工具兼容性\n\nTTL 功能能够与 TiDB 的迁移、备份、恢复工具一同使用。\n\n| 工具名称 | 最低兼容版本 | 说明 |\n| --- | --- | --- |\n| Backup & Restore (BR) | v6.6.0 | 恢复时会自动将表的 `TTL_ENABLE` 属性设置为 `OFF`，关闭 TTL。这样可以防止 TiDB 在备份恢复后立即删除过期的数据。此时你需要手动重新配置 `TTL_ENABLE` 属性来重新开启各个表的 TTL。 |\n| TiDB Lightning | v6.6.0 | 导入后如果表中有 TTL 属性，会自动将表的 `TTL_ENABLE` 属性设置为 `OFF`，关闭 TTL。这样可以防止 TiDB 在导入后立即删除过期的数据。此时你需要手动重新配置 `TTL_ENABLE` 属性来重新开启各个表的 TTL。 |\n| TiCDC | v7.0.0 | 上游的 TTL 删除将会同步至下游。因此，为了防止重复删除，下游表的 `TTL_ENABLE` 属性将被强制设置为 `OFF`。 |\n\n## 与 TiDB 其他特性的兼容性\n\n| 特性名称 | 说明 |\n| :-- | :---- |\n| [`FLASHBACK TABLE`](/sql-statements/sql-statement-flashback-table.md) |  `FLASHBACK TABLE` 语句会将每个表的 `TTL_ENABLE` 属性强制设置为 `OFF`。这样可以防止 TiDB 在 FLASHBACK 后立即删除过期的数据。此时你需要手动重新配置 `TTL_ENABLE` 属性来重新开启各个表的 TTL。 |\n| [`FLASHBACK DATABASE`](/sql-statements/sql-statement-flashback-database.md) | `FLASHBACK DATABASE` 语句会将每个表的 `TTL_ENABLE` 属性强制设置为 `OFF`。这样可以防止 TiDB 在 FLASHBACK 后立即删除过期的数据。此时你需要手动重新配置 `TTL_ENABLE` 属性来重新开启各个表的 TTL。 |\n| [`FLASHBACK CLUSTER`](/sql-statements/sql-statement-flashback-cluster.md) | `FLASHBACK CLUSTER` 会将 [`TIDB_TTL_JOB_ENABLE`](/system-variables.md#tidb_ttl_job_enable-从-v650-版本开始引入) 系统变量设置为 `OFF`，同时表的 `TTL_ENABLE` 属性将保持原样。 |\n\n## 使用限制\n\n目前，TTL 特性具有以下限制:\n\n* 不允许在临时表上设置 TTL 属性，包括本地临时表和全局临时表。\n* 具有 TTL 属性的表不支持作为外键约束的主表被其他表引用。\n* 不保证所有过期数据立即被删除，过期数据被删除的时间取决于后台清理任务的调度周期和调度窗口。\n* 对于使用[聚簇索引](/clustered-indexes.md)的表，仅支持在以下场景中将 TTL 任务拆分成多个子任务：\n    - 主键或者复合主键的第一列为整数或二进制字符串类型。其中，二进制字符串类型主要指下面几种：\n        - `CHAR(N) CHARACTER SET BINARY`\n        - `VARCHAR(N) CHARACTER SET BINARY`\n        - `BINARY(N)`\n        - `VARBINARY(N)`\n        - `BIT(N)`\n    - 主键或者复合主键的第一列的字符集为 `utf8` 或者 `utf8mb4`，且排序规则设置为 `utf8_bin`、 `utf8mb4_bin` 或者 `utf8mb4_0900_bin`。\n* 对于主键第一列的字符集类型是 `utf8` 或者 `utf8mb4` 的表，仅会根据 ASCII 可见字符的范围进行子任务拆分。如果大量的主键值具有相同的 ASCII 前缀，可能会造成任务拆分不均匀。\n* 对于不支持拆分 TTL 子任务的表，TTL 任务只能在一个 TiDB 节点上按顺序执行。此时如果表中的数据量较大，TTL 任务的执行可能会变得缓慢。\n\n## 常见问题\n\n- 如何判断删除的速度是否够快，能够保持数据总量相对稳定？\n\n    在 [Grafana `TiDB` 面板](/grafana-tidb-dashboard.md)中，监控项 `TTL Insert Rows Per Hour` 记录了前一小时总共插入数据的数量。相应的 `TTL Delete Rows Per Hour` 记录了前一小时 TTL 任务总共删除的数据总量。如果 `TTL Insert Rows Per Hour` 长期高于 `TTL Delete Rows Per Hour`， 说明插入的速度高于删除的速度，数据总量将会上升。例如：\n\n    ![insert fast example](/media/ttl/insert-fast.png)\n\n    值得注意的是，由于 TTL 并不能保证数据立即被删除，且当前插入的数据将会在将来的 TTL 任务中才会被删除，哪怕短时间内 TTL 删除的速度低于插入的速度，也不能说明 TTL 的效率一定过慢。需要结合具体情况分析。\n\n- 如何判断 TTL 任务的瓶颈在扫描还是删除？\n\n    观察面板中 `TTL Scan Worker Time By Phase` 与 `TTL Delete Worker Time By Phase` 监控项。如果 scan worker 处于 `dispatch` 状态的时间有很大占比，且 delete worker 很少处于 `idle` 状态，那么说明 scan worker 在等待 delete worker 完成删除工作，如果此时集群资源仍然较为宽松，可以考虑提高 `tidb_ttl_delete_worker_count` 来提高删除的 worker 数量。例如：\n\n    ![scan fast example](/media/ttl/scan-fast.png)\n\n    与之相对，如果 scan worker 很少处于 `dispatch` 的状态，且 delete worker 长期处于 `idle` 阶段，那么说明 delete worker 闲置，且 scan worker 较为忙碌。例如：\n\n    ![delete fast example](/media/ttl/delete-fast.png)\n\n    TTL 任务中扫描与删除的占比与机器配置、数据分布都有关系，所以每一时刻的数据只能代表正在执行的 TTL Job 的情况。用户可以通过查询表 `mysql.tidb_ttl_job_history` 来判断某一时刻运行的 TTL Job 对应哪一张表。\n\n- 如何合理配置 `tidb_ttl_scan_worker_count` 和 `tidb_ttl_delete_worker_count`？\n\n    1. 可以参考问题 \"如何判断 TTL 任务的瓶颈在扫描还是删除？\" 来考虑提升 `tidb_ttl_scan_worker_count` 还是 `tidb_ttl_delete_worker_count`。\n    2. 如果 TiKV 节点数量较多，提升 `tidb_ttl_scan_worker_count` 能够使 TTL 任务负载更加均匀。\n\n    由于过高的 TTL worker 数量将会造成较大的压力，所以需要综合观察 TiDB 的 CPU 水平与 TiKV 的磁盘与 CPU 使用量。根据不同场景和需求（需要尽量加速 TTL，或是需要减少 TTL 对其他请求的影响）来调整 `tidb_ttl_scan_worker_count` 与 `tidb_ttl_delete_worker_count`，从而提升 TTL 扫描和删除数据的速度，或降低 TTL 任务对性能的影响。\n"
        },
        {
          "name": "tiproxy",
          "type": "tree",
          "content": null
        },
        {
          "name": "tispark-deployment-topology.md",
          "type": "blob",
          "size": 8.63671875,
          "content": "---\ntitle: TiSpark 部署拓扑\nsummary: 介绍 TiUP 部署包含 TiSpark 组件的 TiDB 集群的拓扑结构。\naliases: ['/docs-cn/dev/tispark-deployment-topology/']\n---\n\n# TiSpark 部署拓扑\n\n本文介绍 TiSpark 部署的拓扑，以及如何在最小拓扑的基础上同时部署 TiSpark。TiSpark 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品。它借助 Spark 平台，同时融合 TiKV 分布式集群的优势，和 TiDB 一起为用户一站式解决 HTAP (Hybrid Transactional/Analytical Processing) 的需求。\n\n关于 TiSpark 的架构介绍与使用，参见 [TiSpark 用户指南](/tispark-overview.md)。\n\n> **警告：**\n>\n> TiUP Cluster 的 TiSpark 支持目前为废弃状态，不建议使用。\n\n## 拓扑信息\n\n|实例 | 个数 | 物理机配置 | IP |配置 |\n| :-- | :-- | :-- | :-- | :-- |\n| TiDB |3 | 16 VCore 32GB * 1 | 10.0.1.1 <br/> 10.0.1.2 <br/> 10.0.1.3 | 默认端口 <br/>  全局目录配置 |\n| PD | 3 | 4 VCore 8GB * 1 |10.0.1.4 <br/> 10.0.1.5 <br/> 10.0.1.6 | 默认端口 <br/> 全局目录配置 |\n| TiKV | 3 | 16 VCore 32GB 2TB (nvme ssd) * 1 | 10.0.1.7 <br/> 10.0.1.8 <br/> 10.0.1.9 | 默认端口 <br/> 全局目录配置 |\n| TiSpark | 3 | 8 VCore 16GB * 1 | 10.0.1.21 (master) <br/> 10.0.1.22 (worker) <br/> 10.0.1.23 (worker) | 默认端口 <br/> 全局目录配置 |\n| Monitoring & Grafana | 1 | 4 VCore 8GB * 1 500GB (ssd) | 10.0.1.11 | 默认端口 <br/> 全局目录配置 |\n\n### 拓扑模版\n\n<details>\n<summary>简单 TiSpark 配置模板</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\npd_servers:\n  - host: 10.0.1.4\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.1\n  - host: 10.0.1.2\n  - host: 10.0.1.3\n\ntikv_servers:\n  - host: 10.0.1.7\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\n\n# NOTE: TiSpark support is an experimental feature, it's not recommend to be used in\n# production at present.\n# To use TiSpark, you need to manually install Java Runtime Environment (JRE) 8 on the\n# host, see the OpenJDK doc for a reference: https://openjdk.java.net/install/\n# NOTE: Only 1 master node is supported for now\ntispark_masters:\n  - host: 10.0.1.21\n\n# NOTE: multiple worker nodes on the same host is not supported by Spark\ntispark_workers:\n  - host: 10.0.1.22\n  - host: 10.0.1.23\n\nmonitoring_servers:\n  - host: 10.0.1.10\n\ngrafana_servers:\n  - host: 10.0.1.10\n\nalertmanager_servers:\n  - host: 10.0.1.10\n```\n\n</details>\n\n<details>\n<summary>详细 TiSpark 配置模板</summary>\n\n```yaml\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/tidb-deploy\"\n  data_dir: \"/tidb-data\"\n\n# # Monitored variables are applied to all the machines.\nmonitored:\n  node_exporter_port: 9100\n  blackbox_exporter_port: 9115\n  # deploy_dir: \"/tidb-deploy/monitored-9100\"\n  # data_dir: \"/tidb-data/monitored-9100\"\n  # log_dir: \"/tidb-deploy/monitored-9100/log\"\n\n# # Server configs are used to specify the runtime configuration of TiDB components.\n# # All configuration items can be found in TiDB docs:\n# # - TiDB: https://docs.pingcap.com/zh/tidb/stable/tidb-configuration-file\n# # - TiKV: https://docs.pingcap.com/zh/tidb/stable/tikv-configuration-file\n# # - PD: https://docs.pingcap.com/zh/tidb/stable/pd-configuration-file\n# # All configuration items use points to represent the hierarchy, e.g:\n# #   readpool.storage.use-unified-pool\n# #\n# # You can overwrite this configuration via the instance-level `config` field.\n\nserver_configs:\n  tidb:\n    log.slow-threshold: 300\n  tikv:\n    # server.grpc-concurrency: 4\n    # raftstore.apply-pool-size: 2\n    # raftstore.store-pool-size: 2\n    # rocksdb.max-sub-compactions: 1\n    # storage.block-cache.capacity: \"16GB\"\n    # readpool.unified.max-thread-count: 12\n    readpool.storage.use-unified-pool: false\n    readpool.coprocessor.use-unified-pool: true\n  pd:\n    schedule.leader-schedule-limit: 4\n    schedule.region-schedule-limit: 2048\n    schedule.replica-schedule-limit: 64\n\npd_servers:\n  - host: 10.0.1.4\n    # ssh_port: 22\n    # name: \"pd-1\"\n    # client_port: 2379\n    # peer_port: 2380\n    # deploy_dir: \"/tidb-deploy/pd-2379\"\n    # data_dir: \"/tidb-data/pd-2379\"\n    # log_dir: \"/tidb-deploy/pd-2379/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.pd` values.\n    # config:\n    #   schedule.max-merge-region-size: 20\n    #   schedule.max-merge-region-keys: 200000\n  - host: 10.0.1.5\n  - host: 10.0.1.6\n\ntidb_servers:\n  - host: 10.0.1.1\n    # ssh_port: 22\n    # port: 4000\n    # status_port: 10080\n    # deploy_dir: \"/tidb-deploy/tidb-4000\"\n    # log_dir: \"/tidb-deploy/tidb-4000/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tidb` values.\n    # config:\n    #   log.slow-query-file: tidb-slow-overwrited.log\n  - host: 10.0.1.2\n  - host: 10.0.1.3\n\ntikv_servers:\n  - host: 10.0.1.7\n    # ssh_port: 22\n    # port: 20160\n    # status_port: 20180\n    # deploy_dir: \"/tidb-deploy/tikv-20160\"\n    # data_dir: \"/tidb-data/tikv-20160\"\n    # log_dir: \"/tidb-deploy/tikv-20160/log\"\n    # numa_node: \"0,1\"\n    # # The following configs are used to overwrite the `server_configs.tikv` values.\n    # config:\n    #   server.grpc-concurrency: 4\n    #   server.labels: { zone: \"zone1\", dc: \"dc1\", host: \"host1\" }\n\n  - host: 10.0.1.8\n  - host: 10.0.1.9\n\n# NOTE: TiSpark support is an experimental feature, it's not recommend to be used in\n# production at present.\n# To use TiSpark, you need to manually install Java Runtime Environment (JRE) 8 on the\n# host, see the OpenJDK doc for a reference: https://openjdk.java.net/install/\n# If you have already installed JRE 1.8 at a location other than the default of system's\n# package management system, you may use the \"java_home\" field to set the JAVA_HOME variable.\n# NOTE: Only 1 master node is supported for now\ntispark_masters:\n  - host: 10.0.1.21\n    # ssh_port: 22\n    # port: 7077\n    # web_port: 8080\n    # deploy_dir: \"/tidb-deploy/tispark-master-7077\"\n    # java_home: \"/usr/local/bin/java-1.8.0\"\n    # spark_config:\n    #   spark.driver.memory: \"2g\"\n    #   spark.eventLog.enabled: \"False\"\n    #   spark.tispark.grpc.framesize: 268435456\n    #   spark.tispark.grpc.timeout_in_sec: 100\n    #   spark.tispark.meta.reload_period_in_sec: 60\n    #   spark.tispark.request.command.priority: \"Low\"\n    #   spark.tispark.table.scan_concurrency: 256\n    # spark_env:\n    #   SPARK_EXECUTOR_CORES: 5\n    #   SPARK_EXECUTOR_MEMORY: \"10g\"\n    #   SPARK_WORKER_CORES: 5\n    #   SPARK_WORKER_MEMORY: \"10g\"\n\n# NOTE: multiple worker nodes on the same host is not supported by Spark\ntispark_workers:\n  - host: 10.0.1.22\n    # ssh_port: 22\n    # port: 7078\n    # web_port: 8081\n    # deploy_dir: \"/tidb-deploy/tispark-worker-7078\"\n    # java_home: \"/usr/local/bin/java-1.8.0\"\n  - host: 10.0.1.23\n\nmonitoring_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # port: 9090\n    # deploy_dir: \"/tidb-deploy/prometheus-8249\"\n    # data_dir: \"/tidb-data/prometheus-8249\"\n    # log_dir: \"/tidb-deploy/prometheus-8249/log\"\n\ngrafana_servers:\n  - host: 10.0.1.10\n    # port: 3000\n    # deploy_dir: /tidb-deploy/grafana-3000\n\nalertmanager_servers:\n  - host: 10.0.1.10\n    # ssh_port: 22\n    # web_port: 9093\n    # cluster_port: 9094\n    # deploy_dir: \"/tidb-deploy/alertmanager-9093\"\n    # data_dir: \"/tidb-data/alertmanager-9093\"\n    # log_dir: \"/tidb-deploy/alertmanager-9093/log\"\n```\n\n</details>\n\n以上 TiDB 集群拓扑文件中，详细的配置项说明见[通过 TiUP 部署 TiDB 集群的拓扑文件配置](/tiup/tiup-cluster-topology-reference.md#tispark_masters)。\n\n> **注意：**\n>\n> - 无需手动创建配置文件中的 `tidb` 用户，TiUP cluster 组件会在目标主机上自动创建该用户。可以自定义用户，也可以和中控机的用户保持一致。\n> - 如果部署目录配置为相对路径，会部署在用户的 Home 目录下。\n\n## 环境要求\n\n由于 TiSpark 基于 Apache Spark 集群，在启动包含 TiSpark 组件的 TiDB 集群前，需要在部署了 TiSpark 组件的服务器上安装 Java 运行时环境(JRE) 8，否则将无法启动相关组件。\n\nTiUP 不提供自动安装 JRE 的支持，该操作需要用户自行完成。JRE 8 的安装方法可以参考 [OpenJDK 的文档说明](https://openjdk.java.net/install/)。\n\n如果部署服务器上已经安装有 JRE 8，但不在系统的默认包管理工具路径中，可以通过在拓扑配置中设置 `java_home` 参数来指定要使用的 JRE 环境所在的路径。该参数对应系统环境变量 `JAVA_HOME`。\n"
        },
        {
          "name": "tispark-overview.md",
          "type": "blob",
          "size": 33.0546875,
          "content": "---\ntitle: TiSpark 用户指南\nsummary: 使用 TiSpark 一站式解决用户的 HTAP 需求。\naliases: ['/docs-cn/dev/tispark-overview/','/docs-cn/dev/reference/tispark/','/zh/tidb/dev/get-started-with-tispark/','/docs-cn/dev/get-started-with-tispark/','/docs-cn/dev/how-to/get-started/tispark/']\n---\n\n# TiSpark 用户指南\n\n![TiSpark 架构](/media/tispark-architecture.png)\n\n## TiSpark vs TiFlash\n\n[TiSpark](https://github.com/pingcap/tispark) 是 PingCAP 为解决用户复杂 OLAP 需求而推出的产品。它借助 Spark 平台，同时融合 TiKV 分布式集群的优势，和 TiDB 一起为用户一站式解决 HTAP (Hybrid Transactional/Analytical Processing) 的需求。\n\n[TiFlash](/tiflash/tiflash-overview.md) 也是一个解决 HTAP 需求的产品。TiFlash 和 TiSpark 都允许使用多个主机在 OLTP 数据上执行 OLAP 查询。TiFlash 是列式存储，这提供了更高效的分析查询。TiFlash 和 TiSpark 可以同时使用。\n\n## TiSpark 是什么\n\nTiSpark 依赖于 TiKV 集群和 Placement Driver (PD)，也需要你搭建一个 Spark 集群。本文简单介绍如何部署和使用 TiSpark。本文假设你对 Spark 有基本认知。你可以参阅 [Apache Spark 官网](https://spark.apache.org/docs/latest/index.html)了解 Spark 的相关信息。\n\nTiSpark 深度整合了 Spark Catalyst 引擎，可以对计算进行精确的控制，使 Spark 能够高效地读取 TiKV 中的数据。TiSpark 还提供索引支持，帮助实现高速点查。\n\nTiSpark 通过将计算下推到 TiKV 中提升了数据查询的效率，减少了 Spark SQL 需要处理的数据量，通过 TiDB 内置的统计信息选择最优的查询计划。\n\nTiSpark 和 TiDB 可以让用户无需创建和维护 ETL，直接在同一个平台上进行事务和分析两种任务。这简化了系统架构，降低了运维成本。\n\n用户可以在 TiDB 上使用 Spark 生态圈的多种工具进行数据处理，例如：\n\n- TiSpark：数据分析和 ETL。\n- TiKV：数据检索。\n- 调度系统：生成报表。\n\n除此之外，TiSpark 还提供了分布式写入 TiKV 的功能。与使用 Spark 结合 JDBC 写入 TiDB 的方式相比，分布式写入 TiKV 能够实现事务（要么全部数据写入成功，要么全部都写入失败）。\n\n> **警告：**\n>\n> 由于 TiSpark 直接访问 TiKV，所以 TiDB Server 使用的访问控制机制并不适用于 TiSpark。TiSpark v2.5.0 及以上版本实现了部分鉴权与授权功能，具体信息请参考[安全](/tispark-overview.md#安全)。\n\n## 版本要求\n\n- TiSpark 支持 Spark 2.3 或以上版本。\n- TiSpark 需要 JDK 1.8 以及 Scala 2.11/2.12 版本。\n- TiSpark 可以运行在任何 Spark 模式上，如 `YARN`、`Mesos` 以及 `Standalone`。\n\n## 推荐配置\n\n> **警告：**\n>\n> [此文](/tispark-deployment-topology.md)所描述的使用 TiUP 部署 TiSpark 的方式已被废弃。\n\nTiSpark 作为 Spark 的 TiDB 连接器，需要 Spark 集群的支持。本文仅提供部署 Spark 的参考建议，对于硬件以及部署细节，请参考 [Spark 官方文档](https://spark.apache.org/docs/latest/hardware-provisioning.html)。\n\n对于独立部署的 Spark 集群，可以参考如下建议配置：\n\n- 建议为 Spark 分配 32G 以上的内存，并为操作系统和缓存保留至少 25% 的内存。\n- 建议每台机器至少为 Spark 分配 8 到 16 核 CPU。起初，你可以设定将所有 CPU 核分配给 Spark。\n\n可以参考如下的 spark-env.sh 配置文件：\n\n```\nSPARK_EXECUTOR_MEMORY = 32g\nSPARK_WORKER_MEMORY = 32g\nSPARK_WORKER_CORES = 8\n```\n\n## 获取 TiSpark\n\nTiSpark 是 Spark 的第三方 jar 包，提供读写 TiKV 的能力。\n\n### 获取 mysql-connector-j\n\n由于 GPL 许可证的限制，TiSpark 不再提供 `mysql-connector-java` 的依赖。以下版本将不再包含 `mysql-connector-java`：\n\n- TiSpark > 3.0.1\n- TiSpark > 2.5.1 (TiSpark 2.5.x)\n- TiSpark > 2.4.3 (TiSpark 2.4.x)\n\n在使用 TiSpark 写入与鉴权时，仍需要 `mysql-connector-java` 依赖，因此你需要手动下载，并使用以下方式引入：\n\n- 将 `mysql-connector-java` 放入 Spark jars 包中。\n- 在你提交 Spark 任务时，引入 `mysql-connector-java`，详见以下示例:\n\n  ```\n  spark-submit --jars tispark-assembly-3.0_2.12-3.1.0-SNAPSHOT.jar,mysql-connector-java-8.0.29.jar\n  ```\n\n### 选择 TiSpark 版本\n\n你可以根据 TiDB 和 Spark 版本选择相应的 TiSpark 版本。\n\n| TiSpark 版本       | TiDB、TiKV、PD 版本 | Spark 版本                   | Scala 版本 |\n|------------------|-----------------|----------------------------|----------|\n| 2.4.x-scala_2.11 | 5.x, 4.x        | 2.3.x, 2.4.x               | 2.11     |\n| 2.4.x-scala_2.12 | 5.x, 4.x        | 2.4.x                      | 2.12     |\n| 2.5.x            | 5.x, 4.x        | 3.0.x, 3.1.x               | 2.12     |\n| 3.0.x            | 5.x, 4.x        | 3.0.x, 3.1.x, 3.2.x        | 2.12     |\n| 3.1.x            | 6.x, 5.x, 4.x   | 3.0.x, 3.1.x, 3.2.x, 3.3.x | 2.12     |\n| 3.2.x            | 6.x, 5.x, 4.x   | 3.0.x, 3.1.x, 3.2.x, 3.3.x | 2.12     |\n\n推荐使用 TiSpark 的最新稳定版本，包括 2.4.4、2.5.3、3.0.3、3.1.7 和 3.2.3。\n\n> **Note:**\n>\n> TiSpark 不保证与 TiDB v7.0.0 及之后版本兼容。\n> TiSpark 不保证与 Spark v3.4.0 及之后版本兼容。\n\n## 获取 TiSpark jar 包\n\n你能用以下方式获取 jar 包：\n\n- 从 [maven 中央仓库](https://search.maven.org/)获取，你可以搜索 [`pingcap`](http://search.maven.org/#search%7Cga%7C1%7Cpingcap) 关键词。\n- 从 [TiSpark releases](https://github.com/pingcap/tispark/releases) 获取。\n- 通过以下步骤从源码构建：\n\n1. 下载 TiSpark 源码：\n\n    ```\n    git clone https://github.com/pingcap/tispark.git\n    cd tisapark\n    ```\n\n2. 在 TiSpark 根目录运行如下命令：\n\n    ```\n    // add -Dmaven.test.skip=true to skip the tests\n    mvn clean install -Dmaven.test.skip=true\n    // or you can add properties to specify spark version\n    mvn clean install -Dmaven.test.skip=true -Pspark3.2.1\n    ```\n\n> **注意：**\n>\n> 目前，你只能使用 java8 构架 TiSpark。运行 `mvn -version` 来检查 java 版本。\n\n### TiSpark jar 包的 artifact ID\n\n注意不同版本的 TiSpark artifact ID 也不同：\n\n| TiSpark 版本                     | Artifact ID                                        |\n|--------------------------------| -------------------------------------------------- |\n| 2.4.x-\\${scala_version}, 2.5.0 | tispark-assembly                                   |\n| 2.5.1                          | tispark-assembly-\\${spark_version}                  |\n| 3.0.x, 3.1.x, 3.2.x            | tispark-assembly-\\${spark_version}-\\${scala_version} |\n\n## 快速开始\n\n本章节将以 spark-shell 为例，介绍如何使用 TiSpark。请保证您已下载 Spark。\n\n### 启动 spark-shell\n\n在 `spark-defaults.conf` 中添加如下配置：\n\n```\nspark.sql.extensions  org.apache.spark.sql.TiExtensions\nspark.tispark.pd.addresses  ${your_pd_address}\nspark.sql.catalog.tidb_catalog  org.apache.spark.sql.catalyst.catalog.TiCatalog\nspark.sql.catalog.tidb_catalog.pd.addresses  ${your_pd_address}\n```\n\n启动 spark-shell：\n\n```\nspark-shell --jars tispark-assembly-{version}.jar\n```\n\n### 获取 TiSpark 版本\n\n在 spark-shell 中运行如下命令，可获取 TiSpark 版本信息：\n\n```scala\nspark.sql(\"select ti_version()\").collect\n```\n\n### 使用 TiSpark 读取数据\n\n可以通过 Spark SQL 从 TiKV 读取数据：\n\n```scala\nspark.sql(\"use tidb_catalog\")\nspark.sql(\"select count(*) from ${database}.${table}\").show\n```\n\n### 使用 TiSpark 写入数据\n\n通过 Spark DataSource API，可以在保证 ACID 前提下写入数据到 TiKV：\n\n```scala\nval tidbOptions: Map[String, String] = Map(\n  \"tidb.addr\" -> \"127.0.0.1\",\n  \"tidb.password\" -> \"\",\n  \"tidb.port\" -> \"4000\",\n  \"tidb.user\" -> \"root\"\n)\n\nval customerDF = spark.sql(\"select * from customer limit 100000\")\n\ncustomerDF.write\n.format(\"tidb\")\n.option(\"database\", \"tpch_test\")\n.option(\"table\", \"cust_test_select\")\n.options(tidbOptions)\n.mode(\"append\")\n.save()\n```\n\n详见 [Data Source API User Guide](https://github.com/pingcap/tispark/blob/master/docs/features/datasource_api_userguide.md)。\n\nTiSpark 3.1 及之后版本支持通过 Spark SQL 写入数据到 TiKV。详见 [Insert SQL](https://github.com/pingcap/tispark/blob/master/docs/features/insert_sql_userguide.md)。\n\n### 通过 JDBC 数据源写入数据\n\n你同样可以在不使用 TiSpark 的情况下使用 Spark JDBC 数据源写入 TiDB。\n\n由于这超过了 TiSpark 的范畴，本文仅简单提供示例，详情请参考 [JDBC To Other Databases](https://spark.apache.org/docs/latest/sql-data-sources-jdbc.html)。\n\n```scala\nimport org.apache.spark.sql.execution.datasources.jdbc.JDBCOptions\n\nval customer = spark.sql(\"select * from customer limit 100000\")\n// 为了平衡各节点以及提高并发数，你可以将数据源重新分区\nval df = customer.repartition(32)\ndf.write\n.mode(saveMode = \"append\")\n.format(\"jdbc\")\n.option(\"driver\", \"com.mysql.jdbc.Driver\")\n// 替换为你的主机名和端口地址，并确保开启了重写批处理\n.option(\"url\", \"jdbc:mysql://127.0.0.1:4000/test?rewriteBatchedStatements=true\")\n.option(\"useSSL\", \"false\")\n// 作为测试，建议设置为 150\n.option(JDBCOptions.JDBC_BATCH_INSERT_SIZE, 150)\n.option(\"dbtable\", s\"cust_test_select\") // 数据库名和表名\n.option(\"isolationLevel\", \"NONE\") // 如果需要写入较大 Dataframe，推荐将 isolationLevel 设置为 NONE\n.option(\"user\", \"root\") // TiDB 用户名\n.save()\n```\n\n为了避免大事务导致 OOM 以及 TiDB 报 `ISOLATION LEVEL does not support` 错误（TiDB 目前仅支持 `REPEATABLE-READ`)，推荐设置 `isolationLevel` 为 `NONE`。\n\n### 使用 TiSpark 删除数据\n\n可以使用 Spark SQL 删除 TiKV 数据：\n\n```\nspark.sql(\"use tidb_catalog\")\nspark.sql(\"delete from ${database}.${table} where xxx\")\n```\n\n详情请参考 [delete feature](https://github.com/pingcap/tispark/blob/master/docs/features/delete_userguide.md)。\n\n### 与其他数据源一起使用\n\n你可以使用多个 catalog 从不同数据源读取数据：\n\n```\n// 从 Hive 读取\nspark.sql(\"select * from spark_catalog.default.t\").show\n\n// Join Hive 表和 TiDB 表\nspark.sql(\"select t1.id,t2.id from spark_catalog.default.t t1 left join tidb_catalog.test.t t2\").show\n```\n\n## TiSpark 配置\n\n可以将如下参数配置在 `spark-defaults.conf` 中，也可以参考 Spark 其他配置，用同样的方式传入。\n\n| Key                                             | Default value    | Description                                                                                                                                                                                                                                                                            |\n|-------------------------------------------------|------------------|----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| `spark.tispark.pd.addresses`                    | `127.0.0.1:2379` | PD 集群的地址，通过逗号分隔。                                                                                                                                                                                                                                                                       |\n| `spark.tispark.grpc.framesize`                  | `2147483647`     | gRPC 的最大回复大小，单位 bytes（默认 2G）。                                                                                                                                                                                                                                                          |\n| `spark.tispark.grpc.timeout_in_sec`             | `10`             | gRPC 超时时间，单位秒。                                                                                                                                                                                                                                                                         |\n| `spark.tispark.plan.allow_agg_pushdown`         | `true`           | 是否运行聚合下推（为了避免 TiKV 节点繁忙）。                                                                                                                                                                                                                                                              |\n| `spark.tispark.plan.allow_index_read`           | `true`           | 是否在执行计划中开启 index（可能导致 TiKV 压力过大）。                                                                                                                                                                                                                                                      |\n| `spark.tispark.index.scan_batch_size`           | `20000`          | 在 index scan 的一次 batch 中 row keys 的数量。                                                                                                                                                                                                                                                 |\n| `spark.tispark.index.scan_concurrency`          | `5`              | 在 index scan 中获取 row keys 的最大线程数（每个 JVM 的所有任务共享）。                                                                                                                                                                                                                                      |\n| `spark.tispark.table.scan_concurrency`          | `512`            | 在 table scan 中的最大线程数（每个 JVM 的所有任务共享）。                                                                                                                                                                                                                                                  |\n| `spark.tispark.request.command.priority`        | `Low`            | 可选项有 `Low`、`Normal`和`High`。修改配置会影响影响 TiKV 的资源分配。建议使用 `Low`，因为 OLTP 的负载不会被影响。                                                                                                                                                                                                           |\n| `spark.tispark.coprocess.codec_format`          | `chblock`        | Coprocessor 的编码格式。可选项为 `default`、`chblock` 和 `chunk`。                                                                                                                                                                                                                                  |\n| `spark.tispark.coprocess.streaming`             | `false`          | 是否在获取响应时使用 streaming（实验性质）。                                                                                                                                                                                                                                                            |\n| `spark.tispark.plan.unsupported_pushdown_exprs` |                  | 表达式清单，多个表达式用逗号分隔。为了防止老版本的 TiKV 不支持某些表达式，你可以禁止下推它们。                                                                                                                                                                                                                                     |\n| `spark.tispark.plan.downgrade.index_threshold`  | `1000000000`     | 如果 index scan 请求的范围超过了此限制，该 Region 请求会被降级为 table scan。降级默认关闭。                                                                                                                                                                                                                         |\n| `spark.tispark.show_rowid`                      | `false`          | 是否在 row_id 存在时显示它。                                                                                                                                                                                                                                                                     |\n| `spark.tispark.db_prefix`                       |                  | TiDB数据库的前缀。该配置可用于在 TiSpark 2.4 中区分同名的 TiDB 和 Hive 数据库。                                                                                                                                                                                                                                 |\n| `spark.tispark.request.isolation.level`         | `SI`             | 是否为底层的 TiKV 解锁。当你使用 \"RC\" 级别，你将忽略锁，得到比 `tso` 更小的最新版本数据。当你使用 \"SI\" 级别，你将进行解锁并根据该锁对应的事务提交与否获取数据。                                                                                                                                                                                         |\n| `spark.tispark.coprocessor.chunk_batch_size`    | `1024`           | 从 coprocessor 获取的一个 batch 的 Row 数量。                                                                                                                                                                                                                                                    |\n| `spark.tispark.isolation_read_engines`          | `tikv,tiflash`   | TiSpark 读引擎，多个引擎使用逗号分隔。未配置的存储引擎不会被读取。                                                                                                                                                                                                                                                  |\n| `spark.tispark.stale_read`                      | optional         | stale read 时间戳。详情请参考 [stale read](https://github.com/pingcap/tispark/blob/master/docs/features/stale_read.md)。                                                                                                                                                                        |\n| `spark.tispark.tikv.tls_enable`                 | `false`          | 是否开启 TiSpark TLS。                                                                                                                                                                                                                                                                      |\n| `spark.tispark.tikv.trust_cert_collection`      |                  | TiKV Client 的受信任根证书，用于验证 PD 证书。如 `/home/tispark/config/root.pem`，该文件需包含 X.509 证书。                                                                                                                                                                                                     |\n| `spark.tispark.tikv.key_cert_chain`             |                  | TiKV Client 的 X.509 格式的客户端证书链。如 `/home/tispark/config/client.pem`。                                                                                                                                                                                                                     |\n| `spark.tispark.tikv.key_file`                   |                  | TiKV Client 的 PKCS#8 私钥文件。如 `/home/tispark/client_pkcs8.key`。                                                                                                                                                                                                                          |\n| `spark.tispark.tikv.jks_enable`                 | `false`          | 是否使用 `JAVA key store` 而不是 X.509 证书。                                                                                                                                                                                                                                                    |\n| `spark.tispark.tikv.jks_trust_path`             |                  | TiKV Client JKS 格式的受信任根证书。由 `keytool` 生成，如 `/home/tispark/config/tikv-truststore`。                                                                                                                                                                                                   |\n| `spark.tispark.tikv.jks_trust_password`         |                  | `spark.tispark.tikv.jks_trust_path` 的密码。                                                                                                                                                                                                                                               |\n| `spark.tispark.tikv.jks_key_path`               |                  | TiKV Client JKS 格式的客户端证书。由 `keytool` 生成，如 `/home/tispark/config/tikv-clientstore`。                                                                                                                                                                                                   |\n| `spark.tispark.tikv.jks_key_password`           |                  | `spark.tispark.tikv.jks_key_path` 的密码。                                                                                                                                                                                                                                                 |\n| `spark.tispark.jdbc.tls_enable`                 | `false`          | 是否开启 JDBC connector TLS。                                                                                                                                                                                                                                                               |\n| `spark.tispark.jdbc.server_cert_store`          |                  | JDBC 的受信任根证书。由 `keytool` 生产的 Java keystore (JKS) 格式的证书。如 `/home/tispark/config/jdbc-truststore`。默认值为 \"\"，表示 TiSpark 不会校验 TiDB 服务端。                                                                                                                                                  |\n| `spark.tispark.jdbc.server_cert_password`       |                  | `spark.tispark.jdbc.server_cert_store` 的密码。                                                                                                                                                                                                                                            |\n| `spark.tispark.jdbc.client_cert_store`          |                  | JDBC 的 PKCS#12 格式的客户端证书。这是由 `keytool` 生成的 JKS 格式证书。如 `/home/tispark/config/jdbc-clientstore`。默认值为 \"\"，表示 TiDB 不会校验 TiSpark。                                                                                                                                                          |\n| `spark.tispark.jdbc.client_cert_password`       |                  | `spark.tispark.jdbc.client_cert_store` 的密码。                                                                                                                                                                                                                                             |\n| `spark.tispark.tikv.tls_reload_interval`        | `10s`            | 重载证书的时间间隔。默认值为 `10s`。                                                                                                                                                                                                                                                                 |\n| `spark.tispark.tikv.conn_recycle_time`          | `60s`            | 清理 TiKV 失效连接的时间间隔。默认时间为 `60s`。当重载证书开启时此配置才会生效。                                                                                                                                                                                                                                         |\n| `spark.tispark.host_mapping`                    |                  | 路由映射配置。用于配置公有 IP 地址和私有 IP 地址的映射。当 TiDB 在私有网络上运行时，你可以将一系列内部 IP 地址映射到公网 IP 地址以便 Spark 集群访问。其格式为 `{Intranet IP1}:{Public IP1};{Intranet IP2}:{Public IP2}`，例如 `192.168.0.2:8.8.8.8;192.168.0.3:9.9.9.9`。                                                                                  |\n| `spark.tispark.new_collation_enable`            |                  | 当 TiDB 开启 [new collation](https://docs.pingcap.com/tidb/stable/character-set-and-collation#new-framework-for-collations)，推荐将此配置设为`true`。当 TiDB 关闭 `new collation`，推荐将此配置设置为 `false`。在未配置的情况下，TiSpark 会依据 TiDB 版本自动配置 `new collation`。其规则为：当 TiDB 版本大于等于 v6.0.0 时为 `true`；否则为 `false`。 |\n| `spark.tispark.replica_read`                   | `leader`         | 读取副本的类型。可选值为 `leader`、`follower`、`learner`。可以同时指定多个类型，TiSpark 会根据顺序选择。  |\n| `spark.tispark.replica_read.label`             |                  | 目标 TiKV 节点的标签。格式为 `label_x=value_x,label_y=value_y`，各项之间为“逻辑与”的关系。 |\n\n### TLS 配置\n\nTiSpark TLS 分为两部分：TiKV Client TLS 以及 JDBC connector TLS。 前者用于创建和 TiKV/PD 的 TLS 连接，后者用于创建与 TiDB 的 TLS 连接。\n\n当配置 TiKV Client TLS 时，你需要以 X.509 格式的证书配置 `tikv.trust_cert_collection`、`tikv.key_cert_chain` 和 `tikv.key_file`；或者以 JKS 格式的证书配置 `tikv.jks_enable`，`tikv.jks_trust_path` 和 `tikv.jks_key_path`。\n\n当配置 JDBC connector TLS 时，你需要配置 `spark.tispark.jdbc.tls_enable`，而 `jdbc.server_cert_store` 和 `jdbc.client_cert_store` 则是可选的。\n\nTiSpark 目前仅持 TLS 1.2 and TLS 1.3。\n\n* 如下是使用 X.509 证书配置 TiKV Client TLS 的例子：\n\n```\nspark.tispark.tikv.tls_enable                                  true\nspark.tispark.tikv.trust_cert_collection                       /home/tispark/root.pem\nspark.tispark.tikv.key_cert_chain                              /home/tispark/client.pem\nspark.tispark.tikv.key_file                                    /home/tispark/client.key\n```\n\n* 如下是使用 JKS 配置 TiKV Client TLS 的例子：\n\n```\nspark.tispark.tikv.tls_enable                                  true\nspark.tispark.tikv.jks_enable                                  true\nspark.tispark.tikv.jks_key_path                                /home/tispark/config/tikv-truststore\nspark.tispark.tikv.jks_key_password                            tikv_trustore_password\nspark.tispark.tikv.jks_trust_path                              /home/tispark/config/tikv-clientstore\nspark.tispark.tikv.jks_trust_password                          tikv_clientstore_password\n```\n\n当你同时配置 JKS 和 X.509 证书时，JKS 优先级更高。因此，当你只想使用普通的 pem 证书时，不要同时设置 `spark.tispark.tikv.jks_enable=true`。\n\n* 下面是一个配置 JDBC connector TLS 的例子：\n\n```\nspark.tispark.jdbc.tls_enable                                  true\nspark.tispark.jdbc.server_cert_store                           /home/tispark/jdbc-truststore\nspark.tispark.jdbc.server_cert_password                        jdbc_truststore_password\nspark.tispark.jdbc.client_cert_store                           /home/tispark/jdbc-clientstore\nspark.tispark.jdbc.client_cert_password                        jdbc_clientstore_password\n```\n\n- 对于如何开启 TiDB TLS，请参考 [Enable TLS between TiDB Clients and Servers](/enable-tls-between-clients-and-servers.md)。\n- 对于如何生成 JAVA key store，请参考 [Connecting Securely Using SSL](https://dev.mysql.com/doc/connector-j/en/connector-j-reference-using-ssl.html)。\n\n### 时区配置\n\n使用 `-Duser.timezone` 系统参数来配置时区（比如 `-Duser.timezone=GMT-7`）。时区会影响 `Timestamp` 数据类型。\n\n请不要使用 `spark.sql.session.timeZone`。\n\n## 特性\n\nTiSpark 的主要特性如下：\n\n| 特性支持                            | TiSpark 2.4.x | TiSpark 2.5.x | TiSpark 3.0.x | TiSpark 3.1.x |\n|---------------------------------| ------------- | ------------- | ----------- |---------------|\n| SQL select without tidb_catalog | ✔           | ✔           |             |               |\n| SQL select with tidb_catalog    |               | ✔           | ✔         | ✔             |\n| DataFrame append                | ✔           | ✔           | ✔         | ✔             |\n| DataFrame reads                 | ✔           | ✔           | ✔         | ✔             |\n| SQL show databases              | ✔           | ✔           | ✔         | ✔             |\n| SQL show tables                 | ✔           | ✔           | ✔         | ✔             |\n| SQL auth                        |               | ✔           | ✔         | ✔             |\n| SQL delete                      |               |               | ✔         | ✔             |\n| SQL insert                      |               |               |           | ✔              |\n| TLS                             |               |               | ✔         | ✔             |\n| DataFrame auth                  |               |               |             | ✔             |\n\n### Expression index 支持\n\nTiDB v5.0 开始支持 [expression index](/sql-statements/sql-statement-create-index.md#表达式索引)。\n\nTiSpark 目前支持从 `expression index` 的表中获取数据，但 `expression index` 不会被 TiSpark 执行计划使用。\n\n### TiFlash 支持\n\nTiSpark 能够通过配置 `spark.tispark.isolation_read_engines` 从 TiFlash 读取数据。\n\n### 分区表支持\n\n**读分区表**\n\nTiSpark 目前支持读取 range 与 hash 分区表。\n\nTiSpark 目前不支持 `partition table` 语法 `select col_name from table_name partition(partition_name)`。但是，你仍可以使用 `where` 条件过滤分区。\n\nTiSpark 会根据分区类型、分区表达式以及具体 SQL 决定是否进行分区裁剪。目前，TiSpark 仅支持在 range 分区下，且在下列任一条件下进行分区裁剪：\n\n+ 列表达式，如 `partition by col1`。\n+ 形如 `YEAR($col)` 的 year 函数，其中 col 为列名且类型为 datetime、date 或能被解析为 datetime、date 的 string 字面量。\n+ 形如 `TO_DAYS($col)` 的 to_days 函数，其中 col 为列名且类型为 datetime、date 或能被解析为 datetime、date 的 string 字面量。\n\n如果分区裁剪未被应用，TiSpark 将会读取所有分区表。\n\n**写分区表**\n\n目前, TiSpark 仅支持写入 range 与 hash 分区表，且需满足以下任一条件：\n\n+ 列表达式，如 `partition by col1`。\n+ 形如 `YEAR($col)` 的 year 函数，其中 col 为列名且类型为 datetime、date 或能被解析为 datetime、date 的 string 字面量。\n\n> **注意：**\n>\n> 目前，TiSpark 只支持在 utf8mb4_bin 字符集下写入分区表。\n\n有两种方式写入分区表：\n\n+ 使用支持 replace 和 append 语义的 Datasource API 写入分区表。\n+ 使用 Spark SQL 删除语句。\n\n### 安全\n\n从 TiSpark v2.5.0 起，你可以通过 TiDB 对 TiSpark 进行鉴权与授权。\n\n该功能默认关闭。要开启该功能，请在 Spark 配置文件 `spark-defaults.conf` 中添加以下配置项：\n\n```\n// 开启鉴权与授权功能\nspark.sql.auth.enable true\n\n// 配置 TiDB 信息\nspark.sql.tidb.addr $your_tidb_server_address\nspark.sql.tidb.port $your_tidb_server_port\nspark.sql.tidb.user $your_tidb_server_user\nspark.sql.tidb.password $your_tidb_server_password\n```\n\n更多详细信息，请参考 [TiSpark 鉴权与授权指南](https://github.com/pingcap/tispark/blob/master/docs/features/authorization_userguide.md)。\n\n### 其他特性\n\n- [下推](https://github.com/pingcap/tispark/blob/master/docs/features/push_down.md)\n- [TiSpark 删除数据](https://github.com/pingcap/tispark/blob/master/docs/features/delete_userguide.md)\n- [历史读](https://github.com/pingcap/tispark/blob/master/docs/features/stale_read.md)\n- [TiSpark with multiple catalogs](https://github.com/pingcap/tispark/wiki/TiSpark-with-multiple-catalogs)\n- [TiSpark TLS](#tls-配置)\n- [TiSpark 执行计划](https://github.com/pingcap/tispark/blob/master/docs/features/query_execution_plan_in_TiSpark.md)\n\n## 统计信息\n\nTiSpark 可以使用 TiDB 的统计信息：\n\n- 选择代价最低的索引访问\n- 估算数据大小以决定是否进行广播优化\n\n如果你希望 TiSpark 使用统计信息支持，需要确保所涉及的表已经被分析。参考[常规统计信息](/statistics.md)了解如何进行表分析。\n\n从 TiSpark 2.0 开始，统计信息将会默认被读取。\n\n## FAQ\n\n详情请参考 [TiSpark FAQ](https://github.com/pingcap/tispark/wiki/TiSpark-FAQ)。\n"
        },
        {
          "name": "tiup",
          "type": "tree",
          "content": null
        },
        {
          "name": "topn-limit-push-down.md",
          "type": "blob",
          "size": 8.7373046875,
          "content": "---\ntitle: TopN 和 Limit 下推\naliases: ['/docs-cn/dev/topn-limit-push-down/']\nsummary: TiDB 中的 LIMIT 子句对应 Limit 算子节点，ORDER BY 子句对应 Sort 算子节点。相邻的 Limit 和 Sort 算子组合成 TopN 算子节点，表示按排序规则提取记录的前 N 项。TopN 下推将尽可能下推到数据源附近，减少数据传输或计算的开销。可参考 [优化规则及表达式下推的黑名单](/blocklist-control-plan.md) 中的关闭方法。TopN 可下推到存储层 Coprocessor，减少计算开销。TopN 无法下推过 Join，排序规则仅依赖于外表列时可下推。TopN 也可转换成 Limit，简化排序操作。\n---\n\n# TopN 和 Limit 下推\n\nSQL 中的 LIMIT 子句在 TiDB 查询计划树中对应 Limit 算子节点，ORDER BY 子句在查询计划树中对应 Sort 算子节点，此外，我们会将相邻的 Limit 和 Sort 算子组合成 TopN 算子节点，表示按某个排序规则提取记录的前 N 项。从另一方面来说，Limit 节点等价于一个排序规则为空的 TopN 节点。\n\n和谓词下推类似，TopN（及 Limit，下同）下推将查询计划树中的 TopN 计算尽可能下推到距离数据源最近的地方，以尽早完成数据的过滤，进而显著地减少数据传输或计算的开销。\n\n如果要关闭这个规则，可参照[优化规则及表达式下推的黑名单](/blocklist-control-plan.md)中的关闭方法。\n\n## 示例\n\n以下通过一些例子对 TopN 下推进行说明。\n\n### 示例 1：下推到存储层 Coprocessor\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t(id int primary key, a int not null);\nexplain select * from t order by a limit 10;\n```\n\n```\n+----------------------------+----------+-----------+---------------+--------------------------------+\n| id                         | estRows  | task      | access object | operator info                  |\n+----------------------------+----------+-----------+---------------+--------------------------------+\n| TopN_7                     | 10.00    | root      |               | test.t.a, offset:0, count:10   |\n| └─TableReader_15           | 10.00    | root      |               | data:TopN_14                   |\n|   └─TopN_14                | 10.00    | cop[tikv] |               | test.t.a, offset:0, count:10   |\n|     └─TableFullScan_13     | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo |\n+----------------------------+----------+-----------+---------------+--------------------------------+\n4 rows in set (0.00 sec)\n```\n\n在该查询中，将 TopN 算子节点下推到 TiKV 上对数据进行过滤，每个 Coprocessor 只向 TiDB 传输 10 条记录。在 TiDB 将数据整合后，再进行最终的过滤。\n\n### 示例 2：TopN 下推过 Join 的情况（排序规则仅依赖于外表中的列）\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t(id int primary key, a int not null);\ncreate table s(id int primary key, a int not null);\nexplain select * from t left join s on t.a = s.a order by t.a limit 10;\n```\n\n```\n+----------------------------------+----------+-----------+---------------+-------------------------------------------------+\n| id                               | estRows  | task      | access object | operator info                                   |\n+----------------------------------+----------+-----------+---------------+-------------------------------------------------+\n| TopN_12                          | 10.00    | root      |               | test.t.a, offset:0, count:10                    |\n| └─HashJoin_17                    | 12.50    | root      |               | left outer join, equal:[eq(test.t.a, test.s.a)] |\n|   ├─TopN_18(Build)               | 10.00    | root      |               | test.t.a, offset:0, count:10                    |\n|   │ └─TableReader_26             | 10.00    | root      |               | data:TopN_25                                    |\n|   │   └─TopN_25                  | 10.00    | cop[tikv] |               | test.t.a, offset:0, count:10                    |\n|   │     └─TableFullScan_24       | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo                  |\n|   └─TableReader_30(Probe)        | 10000.00 | root      |               | data:TableFullScan_29                           |\n|     └─TableFullScan_29           | 10000.00 | cop[tikv] | table:s       | keep order:false, stats:pseudo                  |\n+----------------------------------+----------+-----------+---------------+-------------------------------------------------+\n8 rows in set (0.01 sec)\n```\n\n在该查询中，TopN 算子的排序规则仅依赖于外表 t 中的列，可以将 TopN 下推到 Join 之前进行一次计算，以减少 Join 时的计算开销。除此之外，TiDB 同样将 TopN 下推到了存储层中。\n\n### 示例 3：TopN 不能下推过 Join 的情况\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t(id int primary key, a int not null);\ncreate table s(id int primary key, a int not null);\nexplain select * from t join s on t.a = s.a order by t.id limit 10;\n```\n\n```\n+-------------------------------+----------+-----------+---------------+--------------------------------------------+\n| id                            | estRows  | task      | access object | operator info                              |\n+-------------------------------+----------+-----------+---------------+--------------------------------------------+\n| TopN_12                       | 10.00    | root      |               | test.t.id, offset:0, count:10              |\n| └─HashJoin_16                 | 12500.00 | root      |               | inner join, equal:[eq(test.t.a, test.s.a)] |\n|   ├─TableReader_21(Build)     | 10000.00 | root      |               | data:TableFullScan_20                      |\n|   │ └─TableFullScan_20        | 10000.00 | cop[tikv] | table:s       | keep order:false, stats:pseudo             |\n|   └─TableReader_19(Probe)     | 10000.00 | root      |               | data:TableFullScan_18                      |\n|     └─TableFullScan_18        | 10000.00 | cop[tikv] | table:t       | keep order:false, stats:pseudo             |\n+-------------------------------+----------+-----------+---------------+--------------------------------------------+\n6 rows in set (0.00 sec)\n```\n\nTopN 无法下推过 Inner Join。以上面的查询为例，如果先 Join 得到 100 条记录，再做 TopN 可以剩余 10 条记录。而如果在 TopN 之前就过滤到剩余 10 条记录，做完 Join 之后可能就剩下 5 条了，导致了结果的差异。\n\n同理，TopN 无法下推到 Outer Join 的内表上。在 TopN 的排序规则涉及多张表上的列时，也无法下推，如 `t.a+s.a`。只有当 TopN 的排序规则仅依赖于外表上的列时，才可以下推。\n\n### 示例 4：TopN 转换成 Limit 的情况\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t(id int primary key, a int not null);\ncreate table s(id int primary key, a int not null);\nexplain select * from t left join s on t.a = s.a order by t.id limit 10;\n```\n\n```\n+----------------------------------+----------+-----------+---------------+-------------------------------------------------+\n| id                               | estRows  | task      | access object | operator info                                   |\n+----------------------------------+----------+-----------+---------------+-------------------------------------------------+\n| TopN_12                          | 10.00    | root      |               | test.t.id, offset:0, count:10                   |\n| └─HashJoin_17                    | 12.50    | root      |               | left outer join, equal:[eq(test.t.a, test.s.a)] |\n|   ├─Limit_21(Build)              | 10.00    | root      |               | offset:0, count:10                              |\n|   │ └─TableReader_31             | 10.00    | root      |               | data:Limit_30                                   |\n|   │   └─Limit_30                 | 10.00    | cop[tikv] |               | offset:0, count:10                              |\n|   │     └─TableFullScan_29       | 10.00    | cop[tikv] | table:t       | keep order:true, stats:pseudo                   |\n|   └─TableReader_35(Probe)        | 10000.00 | root      |               | data:TableFullScan_34                           |\n|     └─TableFullScan_34           | 10000.00 | cop[tikv] | table:s       | keep order:false, stats:pseudo                  |\n+----------------------------------+----------+-----------+---------------+-------------------------------------------------+\n8 rows in set (0.00 sec)\n\n```\n\n在上面的查询中，TopN 首先推到了外表 t 上。然后因为它要对 `t.id` 进行排序，而 `t.id` 是表 t 的主键，可以直接按顺序读出 (`keep order:true`)，从而省略了 TopN 中的排序，将其简化为 Limit。\n"
        },
        {
          "name": "transaction-isolation-levels.md",
          "type": "blob",
          "size": 8.0966796875,
          "content": "---\ntitle: TiDB 事务隔离级别\nsummary: 了解 TiDB 事务的隔离级别。\naliases: ['/docs-cn/dev/transaction-isolation-levels/','/docs-cn/dev/reference/transactions/transaction-isolation/']\n---\n\n# TiDB 事务隔离级别\n\n事务隔离级别是数据库事务处理的基础，[ACID](/glossary.md#acid) 中的 “I”，即 Isolation，指的就是事务的隔离性。\n\nSQL-92 标准定义了 4 种隔离级别：读未提交 (READ UNCOMMITTED)、读已提交 (READ COMMITTED)、可重复读 (REPEATABLE READ)、串行化 (SERIALIZABLE)。详见下表：\n\n| Isolation Level  | Dirty Write  | Dirty Read   | Fuzzy Read   | Phantom      |\n| ---------------- | ------------ | ------------ | ------------ | ------------ |\n| READ UNCOMMITTED | Not Possible | Possible     | Possible     | Possible     |\n| READ COMMITTED   | Not Possible | Not possible | Possible     | Possible     |\n| REPEATABLE READ  | Not Possible | Not possible | Not possible | Possible     |\n| SERIALIZABLE     | Not Possible | Not possible | Not possible | Not possible |\n\nTiDB 实现了快照隔离 (Snapshot Isolation, SI) 级别的一致性。为与 MySQL 保持一致，又称其为“可重复读” (REPEATABLE READ)。该隔离级别不同于 [ANSI 可重复读隔离级别](#与-ansi-可重复读隔离级别的区别)和 [MySQL 可重复读隔离级别](#与-mysql-可重复读隔离级别的区别)。\n\n> **注意：**\n>\n> 在 TiDB v3.0 中，事务的自动重试功能默认为禁用状态。不建议开启自动重试功能，因为可能导致**事务隔离级别遭到破坏**。更多关于事务自动重试的文档说明，请参考[事务重试](/optimistic-transaction.md#重试机制)。\n>\n> 从 TiDB [v3.0.8](/releases/release-3.0.8.md#tidb) 版本开始，新创建的 TiDB 集群会默认使用[悲观事务模式](/pessimistic-transaction.md)，悲观事务中的当前读（for update 读）为**不可重复读**，关于悲观事务使用注意事项，请参考[悲观事务模式](/pessimistic-transaction.md)\n\n## 可重复读隔离级别 (Repeatable Read)\n\n当事务隔离级别为可重复读时，只能读到该事务启动时已经提交的其他事务修改的数据，未提交的数据或在事务启动后其他事务提交的数据是不可见的。对于本事务而言，事务语句可以看到之前的语句做出的修改。\n\n对于运行于不同节点的事务而言，不同事务启动和提交的顺序取决于从 PD 获取时间戳的顺序。\n\n处于可重复读隔离级别的事务不能并发的更新同一行，当事务提交时发现该行在该事务启动后，已经被另一个已提交的事务更新过，那么该事务会回滚。示例如下：\n\n```sql\ncreate table t1(id int);\ninsert into t1 values(0);\n\nstart transaction;              |               start transaction;\nselect * from t1;               |               select * from t1;\nupdate t1 set id=id+1;          |               update t1 set id=id+1; -- 如果使用悲观事务，则后一个执行的 update 语句会等锁，直到持有锁的事务提交或者回滚释放行锁。\ncommit;                         |\n                                |               commit; -- 事务提交失败，回滚。如果使用悲观事务，可以提交成功。\n```\n\n### 与 ANSI 可重复读隔离级别的区别\n\n尽管名称是可重复读隔离级别，但是 TiDB 中可重复读隔离级别和 ANSI 可重复隔离级别是不同的。按照 [A Critique of ANSI SQL Isolation Levels](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/02/tr-95-51.pdf) 论文中的标准，TiDB 实现的是论文中的快照隔离级别。该隔离级别不会出现狭义上的幻读 (A3)，但不会阻止广义上的幻读 (P3)，同时，SI 还会出现写偏斜，而 ANSI 可重复读隔离级别不会出现写偏斜，会出现幻读。\n\n### 与 MySQL 可重复读隔离级别的区别\n\nMySQL 可重复读隔离级别在更新时并不检验当前版本是否可见，也就是说，即使该行在事务启动后被更新过，同样可以继续更新。这种情况在 TiDB 使用乐观事务时会导致事务回滚，导致事务最终失败，而 TiDB 默认的悲观事务和 MySQL 是可以更新成功的。\n\n## 读已提交隔离级别 (Read Committed)\n\n从 TiDB [v4.0.0-beta](/releases/release-4.0.0-beta.md#tidb) 版本开始，TiDB 支持使用 Read Committed 隔离级别。由于历史原因，当前主流数据库的 Read Committed 隔离级别本质上都是 Oracle 定义的[一致性读隔离级别](https://docs.oracle.com/cd/B19306_01/server.102/b14220/consist.htm)。TiDB 为了适应这一历史原因，悲观事务中的 Read Committed 隔离级别的实质行为也是一致性读。\n\n> **注意：**\n>\n> Read Committed 隔离级别仅在[悲观事务模式](/pessimistic-transaction.md)下生效。在[乐观事务模式](/optimistic-transaction.md)下设置事务隔离级别为 Read Committed 将不会生效，事务将仍旧使用可重复读隔离级别。\n\n从 v6.0.0 版本开始，TiDB 支持使用系统变量 [`tidb_rc_read_check_ts`](/system-variables.md#tidb_rc_read_check_ts-从-v600-版本开始引入) 对读写冲突较少情况下优化时间戳的获取。开启此变量后，`SELECT` 语句会尝试使用前一个有效的时间戳进行数据读取，初始值为事务的 `start_ts`。\n\n- 如果整个读取过程没有遇到更新的数据版本，则返回结果给客户端且 `SELECT` 语句执行成功。\n- 如果读取过程中遇到更新的数据版本：\n    - 如果当前 TiDB 尚未向客户端回复数据，则尝试重新获取一个新的时间戳重试此语句。\n    - 如果 TiDB 已经向客户端返回部分数据，则 TiDB 会向客户端报错。每次向客户端回复的数据量受 `tidb_init_chunk_size` 和 `tidb_max_chunk_size` 控制。\n\n在使用 `READ-COMMITTED` 隔离级别且单个事务中 `SELECT` 语句较多、读写冲突较少的场景，可通过开启此变量来避免获取全局 timestamp 带来的延迟和开销。\n\n从 v6.3.0 版本开始，TiDB 支持通过开启系统变量 [`tidb_rc_write_check_ts`](/system-variables.md#tidb_rc_write_check_ts-从-v630-版本开始引入) 对点写冲突较少情况下优化时间戳的获取。开启此变量后，点写语句会尝试使用当前事务有效的时间戳进行数据读取和加锁操作，且在读取数据时按照开启 [`tidb_rc_read_check_ts`](/system-variables.md#tidb_rc_read_check_ts-从-v600-版本开始引入) 的方式读取数据。目前该变量适用的点写语句包括 `UPDATE`、`DELETE`、`SELECT ...... FOR UPDATE` 三种类型。点写语句是指将主键或者唯一键作为过滤条件且最终执行算子包含 `POINT-GET` 的写语句。目前这三种点写语句的共同点是会先根据 key 值做点查，如果 key 存在再加锁，如果不存在则直接返回空集。\n\n- 如果点写语句的整个读取过程中没有遇到更新的数据版本，则继续使用当前事务的时间戳进行加锁。\n    - 如果加锁过程中遇到因时间戳旧而导致写冲突，则重新获取最新的全局时间戳进行加锁。\n    - 如果加锁过程中没有遇到写冲突或其他错误，则加锁成功。\n- 如果读取过程中遇到更新的数据版本，则尝试重新获取一个新的时间戳重试此语句。\n\n在使用 `READ-COMMITTED` 隔离级别且单个事务中点写语句较多、点写冲突较少的场景，可通过开启此变量来避免获取全局时间戳带来的延迟和开销。\n\n### 与 MySQL Read Committed 隔离级别的区别\n\nMySQL 的 Read Committed 隔离级别大部分符合一致性读特性，但其中存在某些特例，如半一致性读 ([semi-consistent read](https://dev.mysql.com/doc/refman/8.0/en/innodb-transaction-isolation-levels.html))，TiDB 没有兼容这个特殊行为。\n\n## 更多阅读\n\n- [TiDB 的乐观事务模型](https://pingcap.com/blog-cn/best-practice-optimistic-transaction/)\n- [TiDB 新特性漫谈-悲观事务](https://pingcap.com/blog-cn/pessimistic-transaction-the-new-features-of-tidb/)\n- [TiDB 新特性-白话悲观锁](https://pingcap.com/blog-cn/tidb-4.0-pessimistic-lock/)\n- [TiKV 的 MVCC (Multi-Version Concurrency Control) 机制](https://pingcap.com/blog-cn/mvcc-in-tikv/)\n"
        },
        {
          "name": "transaction-overview.md",
          "type": "blob",
          "size": 13.9951171875,
          "content": "---\ntitle: TiDB 事务概览\nsummary: 了解 TiDB 中的事务。\naliases: ['/docs-cn/dev/transaction-overview/','/docs-cn/dev/reference/transactions/overview/']\n---\n\n# TiDB 事务概览\n\nTiDB 支持分布式事务，提供[乐观事务](/optimistic-transaction.md)与[悲观事务](/pessimistic-transaction.md)两种事务模式。TiDB 3.0.8 及以后版本，TiDB 默认采用悲观事务模式。\n\n本文主要介绍涉及事务的常用语句、显式/隐式事务、事务的隔离级别和惰性检查，以及事务大小的限制。\n\n常用的变量包括 [`autocommit`](#自动提交)、[`tidb_disable_txn_auto_retry`](/system-variables.md#tidb_disable_txn_auto_retry)、[`tidb_retry_limit`](/system-variables.md#tidb_retry_limit) 以及 [`tidb_txn_mode`](/system-variables.md#tidb_txn_mode)。\n\n> **注意：**\n>\n> 变量 [`tidb_disable_txn_auto_retry`](/system-variables.md#tidb_disable_txn_auto_retry) 和 [`tidb_retry_limit`](/system-variables.md#tidb_retry_limit) 仅适用于乐观事务，不适用于悲观事务。\n\n## 常用事务语句\n\n### 开启事务\n\n要显式地开启一个新事务，既可以使用 [`BEGIN`](/sql-statements/sql-statement-begin.md) 语句，也可以使用 [`START TRANSACTION`](/sql-statements/sql-statement-start-transaction.md) 语句，两者效果相同。\n\n语法：\n\n{{< copyable \"sql\" >}}\n\n```sql\nBEGIN;\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSTART TRANSACTION;\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSTART TRANSACTION WITH CONSISTENT SNAPSHOT;\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSTART TRANSACTION WITH CAUSAL CONSISTENCY ONLY;\n```\n\n如果执行以上语句时，当前 Session 正处于一个事务的中间过程，那么系统会先自动提交当前事务，再开启一个新的事务。\n\n> **注意：**\n>\n> 与 MySQL 不同的是，TiDB 在执行完上述语句后即会获取当前数据库快照，而 MySQL 的 `BEGIN` 和 `START TRANSACTION` 是在开启事务后的第一个从 InnoDB 读数据的 `SELECT` 语句（非 `SELECT FOR UPDATE`）后获取快照，`START TRANSACTION WITH CONSISTENT SNAPSHOT` 是语句执行时获取快照。因此，TiDB 中的 `BEGIN`、`START TRANSACTION` 和 `START TRANSACTION WITH CONSISTENT SNAPSHOT` 都等效为 MySQL 中的 `START TRANSACTION WITH CONSISTENT SNAPSHOT`。\n\n### 提交事务\n\n[`COMMIT`](/sql-statements/sql-statement-commit.md) 语句用于提交 TiDB 在当前事务中进行的所有修改。\n\n语法：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCOMMIT;\n```\n\n> **建议：**\n>\n> 启用[乐观事务](/optimistic-transaction.md)前，请确保应用程序可正确处理 `COMMIT` 语句可能返回的错误。如果不确定应用程序将会如何处理，建议改为使用[悲观事务](/pessimistic-transaction.md)。\n\n### 回滚事务\n\n[`ROLLBACK`](/sql-statements/sql-statement-rollback.md) 语句用于回滚并撤销当前事务的所有修改。\n\n语法：\n\n{{< copyable \"sql\" >}}\n\n```sql\nROLLBACK;\n```\n\n如果客户端连接中止或关闭，也会自动回滚该事务。\n\n## 自动提交\n\n为满足 MySQL 兼容性的要求，在默认情况下，TiDB 将在执行语句后立即进行 _autocommit_（自动提交）。\n\n举例：\n\n```sql\nmysql> CREATE TABLE t1 (\n          id INT NOT NULL PRIMARY KEY auto_increment,\n          pad1 VARCHAR(100)\n         );\nQuery OK, 0 rows affected (0.09 sec)\n\nmysql> SELECT @@autocommit;\n+--------------+\n| @@autocommit |\n+--------------+\n| 1            |\n+--------------+\n1 row in set (0.00 sec)\n\nmysql> INSERT INTO t1 VALUES (1, 'test');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> ROLLBACK;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> SELECT * FROM t1;\n+----+------+\n| id | pad1 |\n+----+------+\n|  1 | test |\n+----+------+\n1 row in set (0.00 sec)\n```\n\n以上示例中，`ROLLBACK` 语句没产生任何效果。由于 `INSERT` 语句是在自动提交的情况下执行的，等同于以下单语句事务：\n\n```sql\nSTART TRANSACTION;\nINSERT INTO t1 VALUES (1, 'test');\nCOMMIT;\n```\n\n如果已显式地启动事务，则不适用自动提交。以下示例，`ROLLBACK` 语句成功撤回了 `INSERT` 语句：\n\n```sql\nmysql> CREATE TABLE t2 (\n          id INT NOT NULL PRIMARY KEY auto_increment,\n          pad1 VARCHAR(100)\n         );\nQuery OK, 0 rows affected (0.10 sec)\n\nmysql> SELECT @@autocommit;\n+--------------+\n| @@autocommit |\n+--------------+\n| 1            |\n+--------------+\n1 row in set (0.00 sec)\n\nmysql> START TRANSACTION;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> INSERT INTO t2 VALUES (1, 'test');\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> ROLLBACK;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> SELECT * FROM t2;\nEmpty set (0.00 sec)\n```\n\n[`autocommit`](/system-variables.md#autocommit) 是一个系统变量，可以基于 Session 或 Global 进行[修改](/sql-statements/sql-statement-set-variable.md)。\n\n举例：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET autocommit = 0;\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSET GLOBAL autocommit = 0;\n```\n\n## 显式事务和隐式事务\n\n> **注意：**\n>\n> 有些语句是隐式提交的。例如，执行 `[BEGIN|START TRANCATION]` 语句时，TiDB 会隐式提交上一个事务，并开启一个新的事务以满足 MySQL 兼容性的需求。详情参见 [implicit commit](https://dev.mysql.com/doc/refman/8.0/en/implicit-commit.html)。\n\nTiDB 可以显式地使用事务（通过 `[BEGIN|START TRANSACTION]`/`COMMIT` 语句定义事务的开始和结束）或者隐式地使用事务 (`SET autocommit = 1`)。\n\n在自动提交状态下，使用 `[BEGIN|START TRANSACTION]` 语句会显式地开启一个事务，同时也会禁用自动提交，使隐式事务变成显式事务。直到执行 `COMMIT` 或 `ROLLBACK` 语句时才会恢复到此前默认的自动提交状态。\n\n对于 DDL 语句，会自动提交并且不能回滚。如果运行 DDL 的时候，正在一个事务的中间过程中，会先自动提交当前事务，再执行 DDL。\n\n## 惰性检查\n\n执行 DML 语句时，乐观事务默认不会检查[主键约束](/constraints.md#主键约束)或[唯一约束](/constraints.md#唯一约束)，而是在 `COMMIT` 事务时进行这些检查。\n\n举例：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t1 (id INT NOT NULL PRIMARY KEY);\nINSERT INTO t1 VALUES (1);\nBEGIN OPTIMISTIC;\nINSERT INTO t1 VALUES (1); -- MySQL 返回错误；TiDB 返回成功。\nINSERT INTO t1 VALUES (2);\nCOMMIT; -- MySQL 提交成功；TiDB 返回错误，事务回滚。\nSELECT * FROM t1; -- MySQL 返回 1 2；TiDB 返回 1。\n```\n\n```sql\nmysql> CREATE TABLE t1 (id INT NOT NULL PRIMARY KEY);\nQuery OK, 0 rows affected (0.10 sec)\n\nmysql> INSERT INTO t1 VALUES (1);\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> BEGIN OPTIMISTIC;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> INSERT INTO t1 VALUES (1); -- MySQL 返回错误；TiDB 返回成功。\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> INSERT INTO t1 VALUES (2);\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> COMMIT; -- MySQL 提交成功；TiDB 返回错误，事务回滚。\nERROR 1062 (23000): Duplicate entry '1' for key 't1.PRIMARY'\nmysql> SELECT * FROM t1; -- MySQL 返回 1 2；TiDB 返回 1。\n+----+\n| id |\n+----+\n|  1 |\n+----+\n1 row in set (0.01 sec)\n```\n\n惰性检查优化通过批处理约束检查并减少网络通信来提升性能。可以通过设置 [`tidb_constraint_check_in_place = ON`](/system-variables.md#tidb_constraint_check_in_place) 禁用该行为。\n\n> **注意：**\n>\n> + 本优化仅适用于乐观事务。\n> + 本优化仅对普通的 `INSERT` 语句生效，对 `INSERT IGNORE` 和 `INSERT ON DUPLICATE KEY UPDATE` 不会生效。\n\n## 语句回滚\n\nTiDB 支持语句执行失败后的原子性回滚。如果语句报错，则所做的修改将不会生效。该事务将保持打开状态，并且在发出 `COMMIT` 或 `ROLLBACK` 语句之前可以进行其他修改。\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE test (id INT NOT NULL PRIMARY KEY);\nBEGIN;\nINSERT INTO test VALUES (1);\nINSERT INTO tset VALUES (2);  -- tset 拼写错误，使该语句执行出错。\nINSERT INTO test VALUES (1),(2);  -- 违反 PRIMARY KEY 约束，语句不生效。\nINSERT INTO test VALUES (3);\nCOMMIT;\nSELECT * FROM test;\n```\n\n```sql\nmysql> CREATE TABLE test (id INT NOT NULL PRIMARY KEY);\nQuery OK, 0 rows affected (0.09 sec)\n\nmysql> BEGIN;\nQuery OK, 0 rows affected (0.00 sec)\n\nmysql> INSERT INTO test VALUES (1);\nQuery OK, 1 row affected (0.02 sec)\n\nmysql> INSERT INTO tset VALUES (2);  -- tset 拼写错误，使该语句执行出错。\nERROR 1146 (42S02): Table 'test.tset' doesn't exist\nmysql> INSERT INTO test VALUES (1),(2);  -- 违反 PRIMARY KEY 约束，语句不生效。\nERROR 1062 (23000): Duplicate entry '1' for key 'test.PRIMARY'\nmysql> INSERT INTO test VALUES (3);\nQuery OK, 1 row affected (0.00 sec)\n\nmysql> COMMIT;\nQuery OK, 0 rows affected (0.01 sec)\n\nmysql> SELECT * FROM test;\n+----+\n| id |\n+----+\n|  1 |\n|  3 |\n+----+\n2 rows in set (0.00 sec)\n```\n\n以上例子中，`INSERT` 语句执行失败之后，事务保持打开状态。最后的 `INSERT` 语句执行成功，并且提交了修改。\n\n## 事务限制\n\n由于底层存储引擎的限制，TiDB 要求单行不超过 6 MB。可以将一行的所有列根据类型转换为字节数并加和来估算单行大小。\n\nTiDB 同时支持乐观事务与悲观事务，其中乐观事务是悲观事务的基础。由于乐观事务是先将修改缓存在私有内存中，因此，TiDB 对于单个事务的容量做了限制。\n\nTiDB 中，单个事务的总大小默认不超过 100 MB，这个默认值可以通过配置文件中的配置项 `txn-total-size-limit` 进行修改，最大支持 1 TB。单个事务的实际大小限制还取决于服务器剩余可用内存的大小，执行事务时 TiDB 进程的内存消耗相对于事务大小会存在一定程度的放大，最大可能达到提交事务大小的 2 到 3 倍以上。\n\n在 4.0 以前的版本，TiDB 限制了单个事务的键值对的总数量不超过 30 万条，从 4.0 版本起 TiDB 取消了这项限制。\n\n## 因果一致性事务\n\n> **注意：**\n>\n> 因果一致性事务只在启用 Async Commit 特性和一阶段提交特性时生效。关于这两个特性的启用情况，请参见 [`tidb_enable_async_commit` 系统变量介绍](/system-variables.md#tidb_enable_async_commit-从-v50-版本开始引入)和 [`tidb_enable_1pc` 系统变量介绍](/system-variables.md#tidb_enable_1pc-从-v50-版本开始引入)。\n\nTiDB 支持开启因果一致性的事务。因果一致性的事务在提交时无需向 PD 获取时间戳，所以提交延迟更低。开启因果一致性事务的语法为：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSTART TRANSACTION WITH CAUSAL CONSISTENCY ONLY;\n```\n\n默认情况下，TiDB 保证线性一致性。在线性一致性的情况下，如果事务 2 在事务 1 提交完成后提交，逻辑上事务 2 就应该在事务 1 后发生。\n\n因果一致性弱于线性一致性。在因果一致性的情况下，只有事务 1 和事务 2 加锁或写入的数据有交集时（即事务 1 和事务 2 存在数据库可知的因果关系时），才能保证事务的提交顺序与事务的发生顺序保持一致。目前暂不支持传入数据库外部的因果关系。\n\n采用因果一致性的两个事务有以下特性：\n\n+ [有潜在因果关系的事务之间的逻辑顺序与物理提交顺序一致](#有潜在因果关系的事务之间的逻辑顺序与物理提交顺序一致)\n+ [无因果关系的事务之间的逻辑顺序与物理提交顺序不保证一致](#无因果关系的事务之间的逻辑顺序与物理提交顺序不保证一致)\n+ [不加锁的读取不产生因果关系](#不加锁的读取不产生因果关系)\n\n### 有潜在因果关系的事务之间的逻辑顺序与物理提交顺序一致\n\n假设事务 1 和事务 2 都采用因果一致性，并先后执行如下语句：\n\n| 事务 1 | 事务 2 |\n|-------|-------|\n| START TRANSACTION WITH CAUSAL CONSISTENCY ONLY | START TRANSACTION WITH CAUSAL CONSISTENCY ONLY |\n| x = SELECT v FROM t WHERE id = 1 FOR UPDATE | |\n| UPDATE t set v = $(x + 1) WHERE id = 2 | |\n| COMMIT | |\n| | UPDATE t SET v = 2 WHERE id = 1 |\n| | COMMIT |\n\n上面的例子中，事务 1 对 `id = 1` 的记录加了锁，事务 2 的事务对 `id = 1` 的记录进行了修改，所以事务 1 和事务 2 有潜在的因果关系。所以即使用因果一致性开启事务，只要事务 2 在事务 1 提交成功后才提交，逻辑上事务 2 就必定比事务 1 晚发生。因此，不存在某个事务读到了事务 2 对 `id = 1` 记录的修改，但却没有读到事务 1 对 `id = 2` 记录的修改的情况。\n\n### 无因果关系的事务之间的逻辑顺序与物理提交顺序不保证一致\n\n假设 `id = 1` 和 `id = 2` 的记录最初值都为 0，事务 1 和事务 2 都采用因果一致性，并先后执行如下语句：\n\n| 事务 1 | 事务 2 | 事务 3 |\n|-------|-------|-------|\n| START TRANSACTION WITH CAUSAL CONSISTENCY ONLY | START TRANSACTION WITH CAUSAL CONSISTENCY ONLY | |\n| UPDATE t set v = 3 WHERE id = 2 | | |\n| | UPDATE t SET v = 2 WHERE id = 1 | |\n| | | BEGIN |\n| COMMIT | | |\n| | COMMIT | |\n| | | SELECT v FROM t WHERE id IN (1, 2) |\n\n在本例中，事务 1 不读取 `id = 1` 的记录。此时事务 1 和事务 2 没有数据库可知的因果关系。如果使用因果一致性开启事务，即使物理时间上事务 2 在事务 1 提交完成后才开始提交，TiDB 也不保证逻辑上事务 2 比事务 1 晚发生。\n\n此时如果有一个事务 3 在事务 1 提交前开启，并在事务 2 提交后读取 `id = 1` 和 `id = 2` 的记录，事务 3 可能读到 `id = 1` 的值为 2 但是 `id = 2` 的值为 0。\n\n### 不加锁的读取不产生因果关系\n\n假设事务 1 和事务 2 都采用因果一致性，并先后执行如下语句：\n\n| 事务 1 | 事务 2 |\n|-------|-------|\n| START TRANSACTION WITH CAUSAL CONSISTENCY ONLY | START TRANSACTION WITH CAUSAL CONSISTENCY ONLY |\n| | UPDATE t SET v = 2 WHERE id = 1 |\n| SELECT v FROM t WHERE id = 1 | |\n| UPDATE t set v = 3 WHERE id = 2 | |\n| | COMMIT |\n| COMMIT | |\n\n如本例所示，不加锁的读取不产生因果关系。事务 1 和事务 2 产生了写偏斜的异常，如果他们有业务上的因果关系，则是不合理的。所以本例中，使用因果一致性的事务 1 和事务 2 没有确定的逻辑顺序。\n"
        },
        {
          "name": "troubleshoot-cpu-issues.md",
          "type": "blob",
          "size": 10.44140625,
          "content": "---\ntitle: 读写延迟增加\nsummary: 介绍读写延时增加、抖动时的排查思路，可能的原因和解决方法。\naliases: ['/docs-cn/dev/troubleshoot-cpu-issues/']\n---\n\n# 读写延迟增加\n\n本文档介绍读写延迟增加、抖动时的排查思路，可能的原因和解决方法。\n\n## 常见原因\n\n### TiDB 执行计划不对导致延迟增高\n\n查询语句的执行计划不稳定，偶尔执行计划选择错误的索引，导致查询延迟增加。\n\n**现象：**\n\n* 如果慢日志中输出了执行计划，可以直接查看执行计划。用 `select tidb_decode_plan('xxx...')` 语句可以解析出具体的执行计划。\n* 监控中的 key 扫描异常升高；慢日志中 SQL 执行时间 `Scan Keys` 数目较大。\n* SQL 执行时间相比于其他数据库（例如 MySQL）有较大差距。可以对比其他数据库执行计划，例如 `Join Order` 是否不同。\n\n**可能的原因：**\n\n* 统计信息不准确\n\n**解决方案：**\n\n* 更新统计信息\n    * 手动 `analyze table`，配合 crontab 定期 `analyze`，维持统计信息准确度。\n    * 自动 `auto analyze`，调低 `analyze ratio` 阈值，提高收集频次，并设置运行时间窗口。示例如下：\n        * `set global tidb_auto_analyze_ratio=0.2;`\n        * `set global tidb_auto_analyze_start_time='00:00 +0800';`\n        * `set global tidb_auto_analyze_end_time='06:00 +0800';`\n* 绑定执行计划\n    * 修改业务 SQL，使用 `use index` 固定使用列上的索引。\n    * 3.0 版本下，业务可以不用修改 SQL，使用 `create global binding` 创建 `force index` 的绑定 SQL。\n    * 4.0 版本支持 SQL Plan Management，可以避免因执行计划不稳定导致的性能下降。\n\n### PD 异常\n\n**现象：**\n\n监控中 PD TSO 的 **wait duration** 异常升高。**wait duration** 代表从开始等待 PD 返回，到等待结束的时间。\n\n**可能的原因：**\n\n* 磁盘问题。PD 所在的节点 I/O 被占满，排查是否有其他 I/O 高的组件与 PD 混合部署以及磁盘的健康情况，可通过监控 Grafana -> **disk performance** -> **latency** 和 **load** 等指标进行验证，必要时可以使用 fio 工具对盘进行检测，见案例 [case-292](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case292.md)。\n\n* PD 之间的网络问题。PD 日志中有 `\"lost the TCP streaming connection\"`，排查 PD 之间网络是否有问题，可通过监控 Grafana -> **PD** -> **etcd** 的 **round trip** 来验证，见案例 [case-177](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case177.md)。\n\n* 系统负载高，日志中能看到 `\"server is likely overloaded\"`，见案例 [case-214](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case214.md)。\n\n* 选举不出 leader。PD 日志中有 `\"lease is not expired\"`，见 issues [https://github.com/etcd-io/etcd/issues/10355](https://github.com/etcd-io/etcd/issues/10355)。v3.0.x 版本和 v2.1.19 版本已解决该问题，见案例 [case-875](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case875.md)。\n\n* 选举慢。Region 加载时间长，从 PD 日志中 `grep \"regions cost\"`（例如日志中可能是 `load 460927 regions cost 11.77099s`），如果出现秒级，则说明较慢，v3.0 版本可开启 Region Storage（设置 `use-region-storage` 为 `true`），该特性能极大缩短加载 Region 的时间，见案例 [case-429](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case429.md)。\n\n* TiDB 与 PD 之间的网络问题，应排查网络相关情况。通过监控 Grafana -> **blackbox_exporter** -> **ping latency** 确定 TiDB 到 PD leader 的网络是否正常。\n\n* PD 报 `FATAL` 错误，日志中有 `\"range failed to find revision pair\"`。v3.0.8 中已经解决问题，见 PR [https://github.com/pingcap/pd/pull/2040](https://github.com/pingcap/pd/pull/2040)。详情请参考案例 [case-947](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case947.md)。\n\n* 使用 `/api/v1/regions` 接口时 Region 数量过多可能会导致 PD OOM。已于 v3.0.8 版本修复，见 [https://github.com/pingcap/pd/pull/1986](https://github.com/pingcap/pd/pull/1986)。\n\n* 滚动升级的时候 PD OOM，gRPC 消息大小没限制，监控可看到 TCP InSegs 较大，已于 v3.0.6 版本修复，见 [https://github.com/pingcap/pd/pull/1952](https://github.com/pingcap/pd/pull/1952)。\n\n* PD panic，请[提交 bug](https://github.com/tikv/pd/issues/new?labels=kind/bug&template=bug-report.md)。\n\n* 其他原因，通过 `curl http://127.0.0.1:2379/debug/pprof/goroutine?debug=2` 抓取 goroutine，并[提交 bug](https://github.com/pingcap/pd/issues/new?labels=kind%2Fbug&template=bug-report.md)。\n\n### TiKV 异常\n\n**现象：**\n\n监控中 **KV Cmd Duration** 异常升高。KV Cmd Duration 是 TiDB 发送请求给 TiKV 到收到回复的延迟。\n\n**可能的原因：**\n\n* 查看 gRPC duration。gRPC duration 是请求在 TiKV 端的总耗时。通过对比 TiKV 的 gRPC duration 以及 TiDB 中的 KV duration 可以发现潜在的网络问题。比如 gRPC duration 很短但是 TiDB 的 KV duration 显示很长，说明 TiDB 和 TiKV 之间网络延迟可能很高，或者 TiDB 和 TiKV 之间的网卡带宽被占满。\n\n* TiKV 重启了导致重新选举\n    * TiKV panic 之后又被 `systemd` 重新拉起正常运行，可以通过查看 TiKV 的日志来确认是否有 `panic`，这种情况属于非预期，需要报 bug。\n    * 被第三者 `stop`/`kill`，被 `systemd` 重新拉起。查看 `dmesg` 和 `TiKV log` 确认原因。\n    * TiKV 发生 OOM 导致重启了。\n    * 动态调整 `THP` 导致 hung 住，见案例 [case-500](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case500.md)。\n\n* 查看监控：Grafana -> **TiKV-details** -> **errors** 面板 `server is busy` 看到 TiKV RocksDB 出现 write stall 导致发生重新选举。\n\n* TiKV 发生网络隔离导致重新选举。\n\n* `block-cache` 配置太大导致 OOM，在监控 Grafana -> **TiKV-details** 选中对应的 instance 之后查看 RocksDB 的 `block cache size` 监控来确认是否是该问题。同时请检查 `[storage.block-cache] capacity = # \"1GB\"` 参数是否设置合理，默认情况下 TiKV 的 `block-cache` 设置为机器总内存的 `45%`。在容器化部署时需要显式指定该参数，因为 TiKV 获取的是物理机的内存，可能会超出单个 container 的内存限制。\n\n* Coprocessor 收到大量大查询，返回的数据量太大，gRPC 发送速度跟不上 Coprocessor 向客户端输出数据的速度导致 OOM。可以通过检查监控：Grafana -> **TiKV-details** -> **coprocessor overview** 的 `response size` 是否超过 `network outbound` 流量来确认是否属于这种情况。\n\n### TiKV 单线程瓶颈\n\nTiKV 中存在一些单线程线程，可能会成为瓶颈。\n\n* 单个 TiKV Region 过多导致单个 gRPC 线程成为瓶颈（查看 Grafana -> TiKV-details -> `Thread CPU/gRPC CPU Per Thread` 监控），v3.x 以上版本可以开启 Hibernate Region 特性来解决，见案例 [case-612](https://github.com/pingcap/tidb-map/blob/master/maps/diagnose-case-study/case612.md)。\n* v3.0 之前版本 Raftstore 单线程或者 Apply 单线程到达瓶颈（Grafana -> TiKV-details -> `Thread CPU/raft store CPU 和 Async apply CPU` 超过 `80%`），可以选择扩容 TiKV（v2.x 版本）实例或者升级到多线程模型的 v3.x 版本。\n\n### CPU Load 升高\n\n**现象：**\n\nCPU 资源使用到达瓶颈\n\n**可能的原因：**\n\n* 热点问题。\n* 整体负载高，排查 TiDB 的 slow query 和 expensive query。对运行的 query 进行优化，如果缺索引就加索引，如果可以批量执行就批量执行。另一个方案是对集群进行扩容。\n\n## 其它原因\n\n### 集群维护\n\n通常大多数的线上集群有 3 或 5 个 PD 节点，如果维护的主机上有 PD 组件，需要具体考虑节点是 leader 还是 follower，关闭 follower 对集群运行没有任何影响，关闭 leader 需要先切换，并在切换时有 3 秒左右的性能抖动。\n\n### 少数派副本离线\n\nTiDB 集群默认配置为 3 副本，每一个 Region 都会在集群中保存 3 份，它们之间通过 Raft 协议来选举 Leader 并同步数据。Raft 协议可以保证在数量小于副本数（注意：不是节点数）一半的节点挂掉或者隔离的情况下，仍然能够提供服务，并且不丢失任何数据。对于 3 副本集群，挂掉一个节点可能会导致性能抖动，可用性和正确性理论上不会受影响。\n\n### 新增索引\n\n由于创建索引在扫表回填索引的时候会消耗大量资源，甚至与一些频繁更新的字段会发生冲突导致正常业务受到影响。大表创建索引的过程往往会持续很长时间，所以要尽可能地平衡执行时间和集群性能之间的关系，比如选择非高频更新时间段。\n\n**参数调整：**\n\n目前主要使用 `tidb_ddl_reorg_worker_cnt` 和 `tidb_ddl_reorg_batch_size` 这两个参数来动态调整索引创建速度，通常来说它们的值越小对系统影响越小，但是执行时间越长。\n\n一般情况下，先将值保持为默认的 `4` 和 `256`，观察集群资源使用情况和响应速度，再逐渐调大 `tidb_ddl_reorg_worker_cnt` 参数来增加并发，观察监控如果系统没有发生明显的抖动，再逐渐调大 `tidb_ddl_reorg_batch_size` 参数，但如果索引涉及的列更新很频繁的话就会造成大量冲突造成失败重试。\n\n另外还可以通过调整参数 `tidb_ddl_reorg_priority` 为 `PRIORITY_HIGH` 来让创建索引的任务保持高优先级来提升速度，但在通用 OLTP 系统上，一般建议保持默认。\n\n### GC 压力大\n\nTiDB 的事务的实现采用了 MVCC（多版本并发控制）机制，当新写入的数据覆盖旧的数据时，旧的数据不会被替换掉，而是与新写入的数据同时保留，并以时间戳来区分版本。GC 的任务便是清理不再需要的旧数据。\n\n* Resolve Locks 阶段在 TiKV 一侧会产生大量的 scan_lock 请求，可以在 gRPC 相关的 metrics 中观察到。`scan_lock` 请求会对全部的 Region 调用。\n* Delete Ranges 阶段会往 TiKV 发送少量的 `unsafe_destroy_range` 请求，也可能没有。可以在 gRPC 相关的 metrics 中和 GC 分类下的 GC tasks 中观察到。\n* Do GC 阶段，默认每台 TiKV 会自动扫描本机上的 leader Region 并对每一个 leader 进行 GC，这一活动可以在 GC 分类下的 GC tasks 中观察到。\n"
        },
        {
          "name": "troubleshoot-data-inconsistency-errors.md",
          "type": "blob",
          "size": 6.38671875,
          "content": "---\ntitle: 数据索引一致性错误\nsummary: TiDB 在执行事务或执行 ADMIN CHECK 命令时会检查数据索引的一致性。如果发现不一致，会报错并记录相关错误日志。报错处理可通过改写 SQL 或关闭错误检查来绕过。对于特定错误代码，可通过设置 @@tidb_enable_mutation_checker=0 或 @@tidb_txn_assertion_level=OFF 来跳过检查。需注意关闭开关会关闭所有 SQL 语句的对应检查。\n---\n\n# 数据索引一致性报错\n\n当执行事务或执行 [`ADMIN CHECK [TABLE|INDEX]`](/sql-statements/sql-statement-admin-check-table-index.md) 命令时，TiDB 会对数据索引的一致性进行检查。如果检查发现 record key-value 和 index key-value 不一致，即存储行数据的键值对和存储其对应索引的键值对之间不一致（例如多索引或缺索引），TiDB 会报数据索引一致性错误，并在日志文件中打印相关错误日志。\n\n本文对数据索引一致性的报错信息进行了说明，并提供了一些绕过检查的方法。遇到报错时，你可以前往 [AskTUG 论坛](https://asktug.com/)，与社区用户交流；如果是订阅用户，请联系 [PingCAP 服务与支持](https://cn.pingcap.com/support/)。\n\n## 错误样例解读\n\n当数据索引不一致时，你可以通过查看 TiDB 的报错信息了解行数据和索引数据在哪一项不一致，或者查看相关错误日志进行判断。\n\n### 执行事务中的报错\n\n本节列出了 TiDB 在执行事务过程中可能出现的数据索引不一致性的报错，并通过举例对这些信息的含义进行了解释。\n\n#### Error 8133\n\n`ERROR 8133 (HY000): data inconsistency in table: t, index: k2, index-count:1 != record-count:0`\n\n上述错误表明，对于表 `t` 中的 `k2` 索引，表中索引数量为 1，行记录的数量为 0，数量不一致。\n\n#### Error 8138\n\n`ERROR 8138 (HY000): writing inconsistent data in table: t, expected-values:{KindString green} != record-values:{KindString GREEN}`\n\n上述错误表明，事务试图写入的行值有误。即将写入的数据中，编码后的行数据与编码前的原始数据不符。\n\n#### Error 8139\n\n`ERROR 8139 (HY000): writing inconsistent data in table: t, index: i1, index-handle:4 != record-handle:3, index: tables.mutation{key:kv.Key{0x74, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x49, 0x5f, 0x69, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x1, 0x1, 0x68, 0x65, 0x6c, 0x6c, 0x6f, 0x0, 0x0, 0x0, 0xfc, 0x1, 0x68, 0x65, 0x6c, 0x6c, 0x6f, 0x0, 0x0, 0x0, 0xfc, 0x3, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x4}, flags:0x0, value:[]uint8{0x30}, indexID:1}, record: tables.mutation{key:kv.Key{0x74, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x49, 0x5f, 0x72, 0x80, 0x0, 0x0, 0x0, 0x0, 0x0, 0x0, 0x3}, flags:0xd, value:[]uint8{0x80, 0x0, 0x2, 0x0, 0x0, 0x0, 0x1, 0x2, 0x5, 0x0, 0xa, 0x0, 0x68, 0x65, 0x6c, 0x6c, 0x6f, 0x68, 0x65, 0x6c, 0x6c, 0x6f}, indexID:0}`\n\n上述错误表明，即将写入的数据中，handle（即行数据的 key）值不一致。对于表 `t` 中的 `i1` 索引，该事务即将写入的某行在索引键值对中的 handle 值是 4，在行记录键值对中的 handle 值是 3。这行数据将不会被写入。\n\n#### Error 8140\n\n`ERROR 8140 (HY000): writing inconsistent data in table: t, index: i2, col: c1, indexed-value:{KindString hellp} != record-value:{KindString hello}`\n\n上述错误表明，事务试图写入的行和索引的值不一致。对于表 `t` 中的 `i2` 索引，该事务即将写入的某行在索引键值对中的数据是 `hellp`，在行记录键值对中的数据是`hello`。这行数据将不会被写入。\n\n#### Error 8141\n\n`ERROR 8141 (HY000): assertion failed: key: 7480000000000000405f72013300000000000000f8, assertion: NotExist, start_ts: 430590532931813377, existing start ts: 430590532931551233, existing commit ts: 430590532931551234`\n\n上述错误表明，事务提交时断言失败。根据数据索引一致的假设，TiDB 断言 key `7480000000000000405f72013300000000000000f8` 不存在，提交事务时发现该 key 存在，是由 start ts 为 `430590532931551233` 的事务写入的。TiDB 会将该 key 的 MVCC (Multi-Version Concurrency Control) 历史输出到日志。\n\n### Admin check 中的报错\n\n本节列出了执行 [`ADMIN CHECK [TABLE|INDEX]`](/sql-statements/sql-statement-admin-check-table-index.md) 系列语句时 TiDB 可能出现的数据索引不一致报错，并通过举例对这些信息的含义进行了解释。\n\n#### Error 8003\n\n`ERROR 8003 (HY000): table count 3 != index(idx) count 2`\n\n上述错误表明，在 [`ADMIN CHECK`](/sql-statements/sql-statement-admin-check-table-index.md) 语句所执行的表上有 3 个行键值对，但只有 2 个索引键值对。\n\n#### Error 8134\n\n`ERROR 8134 (HY000): data inconsistency in table: t, index: c2, col: c2, handle: \"2\", index-values:\"KindInt64 13\" != record-values:\"KindInt64 12\", compare err:<nil>`\n\n上述错误表明，对于表 `t` 中的 `c2` 索引，handle 为 2 的行对应的索引键值对中列 c2 的值是 13，行记录键值对中列 c2 的值是 12。\n\n#### Error 8223\n\n`ERROR 8223 (HY000): data inconsistency in table: t2, index: i1, handle: {hello, hello}, index-values:\"\" != record-values:\"handle: {hello, hello}, values: [KindString hello KindString hello]\"`\n\n上述错误表明，`index-values` 为空，`record-values` 不为空，说明不存在对应的索引，但存在对应的行。\n\n## 报错处理\n\n发生报错时，不要自行处理，请从 PingCAP 官方或 TiDB 社区[获取支持](/support.md)。如果业务急需跳过此类报错，可以使用以下方法绕过检查。\n\n### 改写 SQL\n\n如果只有某一条 SQL 语句报错，可以尝试将其改写为其它等价的 SQL 形式，以使用不同的执行算子来尝试绕过。\n\n### 关闭错误检查\n\n对于事务执行中出现的一些报错，可以使用以下方法绕过检查：\n\n- 对于错误代码为 8138、8139 和 8140 的错误，可以通过设置 `set @@tidb_enable_mutation_checker=0` 跳过检查。\n- 对于错误代码为 8141 的错误，可以通过设置 `set @@tidb_txn_assertion_level=OFF` 跳过检查。\n\n> **注意：**\n>\n> 关闭 `tidb_enable_mutation_checker` 和 `tidb_txn_assertion_level` 开关会关闭对所有 SQL 语句的对应检查。\n\n对于其它错误代码，包括执行 [`ADMIN CHECK [TABLE|INDEX]`](/sql-statements/sql-statement-admin-check-table-index.md) 系列语句或执行事务中的报错，由于数据中已经存在不一致，无法跳过对应的检查。\n"
        },
        {
          "name": "troubleshoot-high-disk-io.md",
          "type": "blob",
          "size": 6.8017578125,
          "content": "---\ntitle: TiDB 磁盘 I/O 过高的处理办法\nsummary: 了解如何定位和处理 TiDB 存储 I/O 过高的问题。\naliases: ['/docs-cn/dev/troubleshoot-high-disk-io/']\n---\n\n# TiDB 磁盘 I/O 过高的处理办法\n\n本文主要介绍如何定位和处理 TiDB 存储 I/O 过高的问题。\n\n## 确认当前 I/O 指标\n\n当出现系统响应变慢的时候，如果已经排查了 CPU 的瓶颈、数据事务冲突的瓶颈后，就需要从 I/O 来入手来辅助判断目前的系统瓶颈点。\n\n### 从监控定位 I/O 问题\n\n最快速的定位手段是从监控来查看整体的 I/O 情况，可以从集群部署工具 (TiUP) 默认会部署的监控组件 Grafana 来查看对应的 I/O 监控，跟 I/O 相关的 Dashboard 有 `Overview`，`Node_exporter`，`Disk-Performance`。\n\n#### 第一类面板\n\n在 `Overview` > `System Info` > `IO Util` 中，可以看到集群中每个机器的 I/O 情况，该指标和 Linux iostat 监控中的 util 类似，百分比越高代表磁盘 I/O 占用越高：\n\n- 如果监控中只有一台机器的 I/O 高，那么可以辅助判断当前有读写热点。\n- 如果监控中大部分机器的 I/O 都高，那么集群现在有高 I/O 负载存在。\n\n如果发现某台机器的 I/O 比较高，可以从监控 `Disk-Performance Dashboard` 进一步观察 I/O 的使用情况，结合 `Disk Latency`，`Disk Load` 等 metric 判断是否存在异常，必要时可以使用 fio 工具对磁盘进行检测。\n\n#### 第二类面板\n\nTiDB 集群主要的持久化组件是 TiKV 集群，一个 TiKV 包含两个 RocksDB 实例：一个用于存储 Raft 日志，位于 data/raft，一个用于存储真正的数据，位于 data/db。\n\n在 `TiKV-Details` > `Raft IO` 中，可以看到这两个实例磁盘写入的相关 metric：\n\n- `Append log duration`：该监控表明了存储 Raft 日志的 RocksDB 写入的响应时间，99% 的响应应该在 50ms 以内。\n- `Apply log duration`：该监控表明了存储真正数据的 RocksDB 写入的响应时间，99% 的响应应该在 100ms 以内。\n\n这两个监控还有 `.. per server` 的监控面板来提供辅助查看热点写入的情况。\n\n#### 第三类面板\n\n在 `TiKV-Details` > `Storage` 中，有关于 storage 相关情况的监控：\n\n- `Storage command total`：收到的不同命令的个数。\n- `Storage async write duration`：包括了磁盘 sync duration 等监控项，可能和 Raft IO 有关。如遇到异常情况，需要通过 log 来检查相关组件的工作状态是否正常。\n\n#### 其他面板\n\n此外，可能还需要一些其它内容来辅助确认瓶颈是否为 I/O，并可以尝试调整一些参数。通过查看 TiKV gRPC 的 prewrite/commit/raw-put（仅限 raw kv 集群）duration，确认确实是 TiKV 写入慢了。常见的几种情况如下：\n\n- append log 慢。TiKV Grafana 的 Raft I/O 和 append log duration 比较高，通常情况下是由于写盘慢了，可以检查 RocksDB - raft 的 WAL Sync Duration max 值来确认，否则可能需要报 bug。\n- raftstore 线程繁忙。TiKV grafana 的 Raft Propose/propose wait duration 明显高于 append log duration。请查看以下两点：\n\n    - `[raftstore]` 的 `store-pool-size` 配置是否过小（该值建议在[1,5] 之间，不建议太大）。\n    - 机器的 CPU 是不是不够了。\n\n- apply log 慢。TiKV Grafana 的 Raft I/O 和 apply log duration 比较高，通常会伴随着 Raft Propose/apply wait duration 比较高。可能的情况如下：\n  \n    - `[raftstore]` 的 `apply-pool-size` 配置过小（建议在 [1, 5] 之间，不建议太大），Thread CPU/apply cpu 比较高；\n    - 机器的 CPU 资源不够了。\n    - Region 写入热点问题，单个 apply 线程 CPU 使用率比较高（通过修改 Grafana 表达式，加上 by (instance, name) 来看各个线程的 cpu 使用情况），暂时对于单个 Region 的热点写入没有很好的方式，最近在优化该场景。\n    - 写 RocksDB 比较慢，RocksDB kv/max write duration 比较高（单个 Raft log 可能包含很多个 kv，写 RocksDB 的时候会把 128 个 kv 放在一个 write batch 写入到 RocksDB，所以一次 apply log 可能涉及到多次 RocksDB 的 write）。\n    - 其他情况，需要报 bug。\n\n- raft commit log 慢。TiKV Grafana 的 Raft I/O 和 commit log duration 比较高（4.x 版本的 Grafana 才有该 metric）。每个 Region 对应一个独立的 Raft group，Raft 本身是有流控机制的，类似 TCP 的滑动窗口机制，通过参数 [raftstore] raft-max-inflight-msgs = 256 来控制滑动窗口的大小，如果有热点写入并且 commit log duration 比较高可以适度调大改参数，比如 1024。\n\n### 从 log 定位 I/O 问题\n\n- 如果客户端报 `server is busy` 错误，特别是 `raftstore is busy` 的错误信息，会和 I/O 有相关性。\n\n    可以通过查看监控：grafana -> TiKV -> errors 监控确认具体 busy 原因。其中，`server is busy` 是 TiKV 自身的流控机制，TiKV 通过这种方式告知 `tidb/ti-client` 当前 TiKV 的压力过大，过一会儿再尝试。\n\n- TiKV RocksDB 日志出现 `write stall`。\n\n    可能是 `level0 sst` 太多导致 stall。可以添加参数 `[rocksdb] max-sub-compactions = 2（或者 3）` 加快 level0 sst 往下 compact 的速度，该参数的意思是将从 level0 到 level1 的 compaction 任务最多切成 `max-sub-compactions` 个子任务交给多线程并发执行。\n\n    如果磁盘 I/O 能力持续跟不上写入，建议扩容。如果磁盘的吞吐达到了上限（例如 SATA SSD 的吞吐相对 NVME SSD 会低很多）导致 write stall，但是 CPU 资源又比较充足，可以尝试采用压缩率更高的压缩算法来缓解磁盘的压力，用 CPU 资源换磁盘资源。\n\n    比如 `default cf compaction` 压力比较大时，可以调整参数 `[rocksdb.defaultcf] compression-per-level = [\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]` 改成 `compression-per-level = [\"no\", \"no\", \"zstd\", \"zstd\", \"zstd\", \"zstd\", \"zstd\"]`。\n\n### 从告警发现 I/O 问题\n\n集群部署工具 (TiUP) 默认部署的告警组件，官方已经预置了相关的告警项目和阈值，I/O 相关项包括：\n\n- TiKV_write_stall\n- TiKV_raft_log_lag\n- TiKV_async_request_snapshot_duration_seconds\n- TiKV_async_request_write_duration_seconds\n- TiKV_raft_append_log_duration_secs\n- TiKV_raft_apply_log_duration_secs\n\n## I/O 问题处理方案\n\n1. 当确认为热点 I/O 问题的时候，需要参考 [TiDB 热点问题处理](/troubleshoot-hot-spot-issues.md)来消除相关的热点 I/O 情况。\n2. 当确认整体 I/O 已经到达瓶颈的时候，且从业务侧能够判断 I/O 的能力会持续的跟不上，那么就可以利用分布式数据库的 scale 的能力，采用扩容 TiKV 节点数量的方案来获取更大的整体 I/O 吞吐量。\n3. 调整上述说明中的一些参数，使用计算/内存资源来换取磁盘的存储资源。"
        },
        {
          "name": "troubleshoot-hot-spot-issues.md",
          "type": "blob",
          "size": 9.7294921875,
          "content": "---\ntitle: TiDB 热点问题处理\naliases: ['/docs-cn/dev/troubleshoot-hot-spot-issues/']\nsummary: TiDB 热点问题处理：介绍定位和解决读写热点问题，包括常见热点场景、确定存在热点问题的方法、使用 TiDB Dashboard 定位热点表、使用 SHARD_ROW_ID_BITS 处理热点表、使用 AUTO_RANDOM 处理自增主键热点表、小表热点的优化、打散读热点。\n---\n\n# TiDB 热点问题处理\n\n本文介绍如何定位和解决读写热点问题。\n\nTiDB 作为分布式数据库，内建负载均衡机制，尽可能将业务负载均匀地分布到不同计算或存储节点上，更好地利用上整体系统资源。然而，机制不是万能的，在一些场景下仍会有部分业务负载不能被很好地分散，影响性能，形成单点的过高负载，也称为热点。\n\nTiDB 提供了完整的方案用于排查、解决或规避这类热点。通过均衡负载热点，可以提升整体性能，包括提高 QPS 和降低延迟等。\n\n## 常见热点场景\n\n### TiDB 编码规则回顾\n\nTiDB 对每个表分配一个 TableID，每一个索引都会分配一个 IndexID，每一行分配一个 RowID（默认情况下，如果表使用整数型的 Primary Key，那么会用 Primary Key 的值当做 RowID）。其中 TableID 在整个集群内唯一，IndexID/RowID 在表内唯一，这些 ID 都是 int64 类型。\n\n每行数据按照如下规则进行编码成 Key-Value pair：\n\n```text\nKey: tablePrefix{tableID}_recordPrefixSep{rowID}\nValue: [col1, col2, col3, col4]\n```\n\n其中 Key 的 `tablePrefix` 和 `recordPrefixSep` 都是特定的字符串常量，用于在 KV 空间内区分其他数据。\n\n对于 Index 数据，会按照如下规则编码成 Key-Value pair：\n\n```text\nKey: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue\nValue: rowID\n```\n\nIndex 数据还需要考虑 Unique Index 和非 Unique Index 两种情况，对于 Unique Index，可以按照上述编码规则。但是对于非 Unique Index，通过这种编码并不能构造出唯一的 Key，因为同一个 Index 的 `tablePrefix{tableID}_indexPrefixSep{indexID}` 都一样，可能有多行数据的 `ColumnsValue` 是一样的，所以对于非 Unique Index 的编码做了一点调整：\n\n```text\nKey: tablePrefix{tableID}_indexPrefixSep{indexID}_indexedColumnsValue_rowID\nValue: null\n```\n\n### 表热点\n\n从 TiDB 编码规则可知，同一个表的数据会在以表 ID 开头为前缀的一个 range 中，数据的顺序按照 RowID 的值顺序排列。在表 insert 的过程中如果 RowID 的值是递增的，则插入的行只能在末端追加。当 Region 达到一定的大小之后会进行分裂，分裂之后还是只能在 range 范围的末端追加，永远只能在一个 Region 上进行 insert 操作，形成热点。\n\n常见的 increment 类型自增主键就是顺序递增的，默认情况下，在主键为整数型时，会用主键值当做 RowID，此时 RowID 为顺序递增，在大量 insert 时形成表的写入热点。\n\n同时，TiDB 中 RowID 默认也按照自增的方式顺序递增，主键不为整数类型时，同样会遇到写入热点的问题。\n\n此外，当写入或读取数据存在热点时，即出现新建表或分区的写入热点问题和只读场景下周期性读热点问题时，你可以使用表属性控制 Region 合并。具体的热点场景描述和解决方法可以查看[使用表属性控制 Region 合并的使用场景](/table-attributes.md#使用场景)。\n\n### 索引热点\n\n索引热点与表热点类似，常见的热点场景出现在时间顺序单调递增的字段，或者插入大量重复值的场景。\n\n## 确定存在热点问题\n\n性能问题不一定是热点造成的，也可能存在多个因素共同影响，在排查前需要先确认是否与热点相关。\n\n- 判断写热点依据：打开监控面板 TiKV-Trouble-Shooting 中 Hot Write 面板，观察 Raftstore CPU 监控是否存在个别 TiKV 节点的指标明显高于其他节点的现象。\n\n- 判断读热点依据：打开监控面板 TIKV-Details 中 Thread_CPU，查看 coprocessor cpu 有没有明显的某个 TiKV 特别高。\n\n## 使用 TiDB Dashboard 定位热点表\n\n[TiDB Dashboard](/dashboard/dashboard-intro.md) 中的[流量可视化](/dashboard/dashboard-key-visualizer.md)功能可帮助用户缩小热点排查范围到表级别。以下是流量可视化功能展示的一个热力图样例，该图横坐标是时间，纵坐标是各个表和索引，颜色越亮代表其流量越大。可在工具栏中切换显示读或写流量。\n\n![Dashboard 示例1](/media/troubleshoot-hot-spot-issues-1.png)\n\n当图中写入流量图出现以下明亮斜线（斜向上或斜向下）时，由于写入只出现在末端，随着表 Region 数量变多，呈现出阶梯状。此时说明该表构成了写入热点：\n\n![Dashboard 示例2](/media/troubleshoot-hot-spot-issues-2.png)\n\n对于读热点，在热力图中一般表现为一条明亮的横线，通常是有大量访问的小表，如下图所示：\n\n![Dashboard 示例3](/media/troubleshoot-hot-spot-issues-3.png)\n\n将鼠标移到亮色块上，即可看到是什么表或索引具有大流量，如下图所示：\n\n![Dashboard 示例4](/media/troubleshoot-hot-spot-issues-4.png)\n\n## 使用 SHARD_ROW_ID_BITS 处理热点表\n\n对于非聚簇索引主键或没有主键的表，TiDB 会使用一个隐式的自增 RowID，大量 `INSERT` 时会把数据集中写入单个 Region，造成写入热点。\n\n通过设置 [`SHARD_ROW_ID_BITS`](/shard-row-id-bits.md)，可以把 RowID 打散写入多个不同的 Region，缓解写入热点问题。\n\n```\nSHARD_ROW_ID_BITS = 4 表示 16 个分片\nSHARD_ROW_ID_BITS = 6 表示 64 个分片\nSHARD_ROW_ID_BITS = 0 表示默认值 1 个分片\n```\n\n语句示例：\n\n```sql\nCREATE TABLE：CREATE TABLE t (c int) SHARD_ROW_ID_BITS = 4;\nALTER TABLE：ALTER TABLE t SHARD_ROW_ID_BITS = 4;\n```\n\n`SHARD_ROW_ID_BITS` 的值可以动态修改，每次修改之后，只对新写入的数据生效。\n\n对于含有 `CLUSTERED` 主键的表，TiDB 会使用表的主键作为 RowID，因为 `SHARD_ROW_ID_BITS` 会改变 RowID 生成规则，所以此时无法使用 `SHARD_ROW_ID_BITS` 选项。而对于使用 `NONCLUSTERED` 主键的表，TiDB 会使用自动分配的 64 位整数作为 RowID，此时也可以使用 `SHARD_ROW_ID_BITS` 特性。要了解关于 `CLUSTERED` 主键的详细信息，请参考[聚簇索引](/clustered-indexes.md)。\n\n以下是两张无主键情况下使用 `SHARD_ROW_ID_BITS` 打散热点后的流量图，第一张展示了打散前的情况，第二张展示了打散后的情况。\n\n![Dashboard 示例5](/media/troubleshoot-hot-spot-issues-5.png)\n\n![Dashboard 示例6](/media/troubleshoot-hot-spot-issues-6.png)\n\n从流量图可见，设置 `SHARD_ROW_ID_BITS` 后，流量热点由之前的只在一个 Region 上变得很分散。\n\n## 使用 AUTO_RANDOM 处理自增主键热点表\n\n使用 `AUTO_RANDOM` 处理自增主键热点表，适用于代替自增主键，解决自增主键带来的写入热点。\n\n使用该功能后，将由 TiDB 生成随机分布且空间耗尽前不重复的主键，达到离散写入、打散写入热点的目的。\n\n注意 TiDB 生成的主键不再是自增的主键，可使用 `LAST_INSERT_ID()` 获取上次分配的主键值。\n\n将建表语句中的 `AUTO_INCREMENT` 改为 `AUTO_RANDOM` 即可使用该功能，适用于主键只需要保证唯一，不包含业务意义的场景。示例如下：\n\n{{< copyable \"sql\" >}}\n\n```sql\nCREATE TABLE t (a BIGINT PRIMARY KEY AUTO_RANDOM, b varchar(255));\nINSERT INTO t (b) VALUES (\"foo\");\nSELECT * FROM t;\n```\n\n```sql\n+------------+---+\n| a          | b |\n+------------+---+\n| 1073741825 | b |\n+------------+---+\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT LAST_INSERT_ID();\n```\n\n```sql\n+------------------+\n| LAST_INSERT_ID() |\n+------------------+\n| 1073741825       |\n+------------------+\n```\n\n以下是将 `AUTO_INCREMENT` 表改为 `AUTO_RANDOM` 打散热点后的流量图，第一张是 `AUTO_INCREMENT`，第二张是 `AUTO_RANDOM`。\n\n![Dashboard 示例7](/media/troubleshoot-hot-spot-issues-7.png)\n\n![Dashboard 示例8](/media/troubleshoot-hot-spot-issues-8.png)\n\n由流量图可见，使用 `AUTO_RANDOM` 代替 `AUTO_INCREMENT` 能很好地打散热点。\n\n更详细的说明参见 [`AUTO_RANDOM`](/auto-random.md) 文档。\n\n## 小表热点的优化\n\nTiDB 的 Coprocessor Cache 功能支持下推计算结果缓存。开启该功能后，将在 TiDB 实例侧缓存下推给 TiKV 计算的结果，对于小表读热点能起到比较好的效果。\n\n更详细的说明参见[下推计算结果缓存](/coprocessor-cache.md#配置)文档。\n\n**其他相关资料**：\n\n+ [TiDB 高并发写入场景最佳实践](/best-practices/high-concurrency-best-practices.md)\n+ [Split Region 使用文档](/sql-statements/sql-statement-split-region.md)\n\n## 打散读热点\n\n在读热点场景中，热点 TiKV 无法及时处理读请求，导致读请求排队。但是，此时并非所有 TiKV 资源都已耗尽。为了降低延迟，TiDB v7.1.0 引入了负载自适应副本读取功能，允许从其他 TiKV 节点读取副本，而无需在热点 TiKV 节点排队等待。你可以通过 [`tidb_load_based_replica_read_threshold`](/system-variables.md#tidb_load_based_replica_read_threshold-从-v700-版本开始引入) 系统变量控制读请求的排队长度。当 leader 节点的预估排队时间超过该阈值时，TiDB 会优先从 follower 节点读取数据。在读热点的情况下，与不打散读热点相比，该功能可提高读取吞吐量 70%～200%。\n\n## 使用 TiKV MVCC 内存引擎缓解因多版本导致的读热点\n\n在 GC 历史版本数据的保留时间过长、频繁更新或删除时，可能会因扫描大量 MVCC 版本而导致读热点。针对这类热点，可通过开启 [TiKV MVCC 内存引擎](/tikv-in-memory-engine.md)功能缓解。\n"
        },
        {
          "name": "troubleshoot-lock-conflicts.md",
          "type": "blob",
          "size": 22.0498046875,
          "content": "---\ntitle: TiDB 锁冲突问题处理\nsummary: 了解 TiDB 锁冲突问题以及处理方式。\naliases: ['/docs-cn/dev/troubleshoot-lock-conflicts/']\n---\n\n# TiDB 锁冲突问题处理\n\nTiDB 支持完整的分布式事务，自 v3.0 版本起，提供[乐观事务](/optimistic-transaction.md)与[悲观事务](/pessimistic-transaction.md)两种事务模式。本文介绍如何使用 Lock View 排查锁相关的问题，以及如何处理使用乐观事务或者悲观事务的过程中常见的锁冲突问题。\n\n## 使用 Lock View 排查锁相关的问题\n\n自 v5.1 版本起，TiDB 支持 Lock View 功能。该功能在 `information_schema` 中内置了若干系统表，用于提供更多关于锁冲突和锁等待的信息。\n\n> **注意：**\n>\n> Lock View 功能目前仅提供悲观锁的冲突和等待信息。\n\n关于这些系统表的详细说明，请参考以下文档：\n\n* [`TIDB_TRX` 与 `CLUSTER_TIDB_TRX`](/information-schema/information-schema-tidb-trx.md)：提供当前 TiDB 节点上或整个集群上所有运行中的事务的信息，包括事务是否处于等锁状态、等锁时间和事务曾经执行过的语句的 Digest 等信息。\n* [`DATA_LOCK_WAITS`](/information-schema/information-schema-data-lock-waits.md)：提供关于 TiKV 内的悲观锁等锁信息，包括阻塞和被阻塞的事务的 `start_ts`、被阻塞的 SQL 语句的 Digest 和发生等待的 key。\n* [`DEADLOCKS` 与 `CLUSTER_DEADLOCKS`](/information-schema/information-schema-deadlocks.md)：提供当前 TiDB 节点上或整个集群上最近发生过的若干次死锁的相关信息，包括死锁环中事务之间的等待关系、事务当前正在执行的语句的 Digest 和发生等待的 key。\n\n> **注意：**\n>\n> Lock View 所属的系统表中展示的 SQL 语句为归一化的 SQL 语句（即去除了格式和参数的 SQL 语句），通过内部查询从 SQL Digest 获得，因而无法获取包括格式和参数在内的完整语句。有关 SQL Digest 和归一化 SQL 语句的详细介绍，请参阅 [Statement Summary Tables](/statement-summary-tables.md)。\n\n以下为排查部分问题的示例。\n\n### 死锁错误\n\n要获取最近发生的死锁错误的信息，可查询 `DEADLOCKS` 或 `CLUSTER_DEADLOCKS` 表。\n\n以查询 `DEADLOCKS` 表为例，请执行以下 SQL 语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from information_schema.deadlocks;\n```\n\n示例输出：\n\n```sql\n+-------------+----------------------------+-----------+--------------------+------------------------------------------------------------------+-----------------------------------------+----------------------------------------+----------------------------------------------------------------------------------------------------+--------------------+\n| DEADLOCK_ID | OCCUR_TIME                 | RETRYABLE | TRY_LOCK_TRX_ID    | CURRENT_SQL_DIGEST                                               | CURRENT_SQL_DIGEST_TEXT                 | KEY                                    | KEY_INFO                                                                                           | TRX_HOLDING_LOCK   |\n+-------------+----------------------------+-----------+--------------------+------------------------------------------------------------------+-----------------------------------------+----------------------------------------+----------------------------------------------------------------------------------------------------+--------------------+\n|           1 | 2021-08-05 11:09:03.230341 |         0 | 426812829645406216 | 22230766411edb40f27a68dadefc63c6c6970d5827f1e5e22fc97be2c4d8350d | update `t` set `v` = ? where `id` = ? ; | 7480000000000000355F728000000000000002 | {\"db_id\":1,\"db_name\":\"test\",\"table_id\":53,\"table_name\":\"t\",\"handle_type\":\"int\",\"handle_value\":\"2\"} | 426812829645406217 |\n|           1 | 2021-08-05 11:09:03.230341 |         0 | 426812829645406217 | 22230766411edb40f27a68dadefc63c6c6970d5827f1e5e22fc97be2c4d8350d | update `t` set `v` = ? where `id` = ? ; | 7480000000000000355F728000000000000001 | {\"db_id\":1,\"db_name\":\"test\",\"table_id\":53,\"table_name\":\"t\",\"handle_type\":\"int\",\"handle_value\":\"1\"} | 426812829645406216 |\n+-------------+----------------------------+-----------+--------------------+------------------------------------------------------------------+-----------------------------------------+----------------------------------------+----------------------------------------------------------------------------------------------------+--------------------+\n```\n\n查询结果会显示死锁错误中多个事务之间的等待关系和各个事务当前正在执行的 SQL 语句的归一化形式（即去掉参数和格式的形式），以及发生冲突的 key 及其从 key 中解读出的一些信息。\n\n例如在上述例子中，第一行意味着 ID 为 `426812829645406216` 的事务当前正在执行形如 ``update `t` set `v` = ? where `id` = ? ;`` 的语句，被另一个 ID 为 `426812829645406217` 的事务阻塞；而 `426812829645406217` 同样也在执行一条形如 ``update `t` set `v` = ? where `id` = ? ;`` 的语句，并被 ID 为 `426812829645406216` 的事务阻塞，两个事务因而构成死锁。\n\n### 少数热点 key 造成锁排队\n\n`DATA_LOCK_WAITS` 系统表提供 TiKV 节点上的等锁情况。查询该表时，TiDB 将自动从所有 TiKV 节点上获取当前时刻的等锁信息。当少数热点 key 频繁被上锁并阻塞较多事务时，你可以查询 `DATA_LOCK_WAITS` 表并按 key 对结果进行聚合，以尝试找出经常发生问题的 key：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect `key`, count(*) as `count` from information_schema.data_lock_waits group by `key` order by `count` desc;\n```\n\n示例输出：\n\n```sql\n+----------------------------------------+-------+\n| key                                    | count |\n+----------------------------------------+-------+\n| 7480000000000000415F728000000000000001 |     2 |\n| 7480000000000000415F728000000000000002 |     1 |\n+----------------------------------------+-------+\n```\n\n为避免偶然性，你可考虑进行多次查询。\n\n如果已知频繁出问题的 key，可尝试从 `TIDB_TRX` 或 `CLUSTER_TIDB_TRX` 表中获取试图上锁该 key 的事务的信息。\n\n需要注意 `TIDB_TRX` 和 `CLUSTER_TIDB_TRX` 表所展示的信息也是对其进行查询的时刻正在运行的事务的信息，并不展示已经结束的事务。如果并发的事务数量很大，该查询的结果集也可能很大，可以考虑添加 limit 子句，或用 where 子句筛选出等锁时间较长的事务。需要注意，对 Lock View 中的多张表进行 join 时，不同表之间的数据并不保证在同一时刻获取，因而不同表中的信息可能并不同步。\n\n以用 where 子句筛选出等锁时间较长的事务为例，请执行以下 SQL 语句：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect trx.* from information_schema.data_lock_waits as l left join information_schema.cluster_tidb_trx as trx on l.trx_id = trx.id where l.key = \"7480000000000000415F728000000000000001\"\\G\n```\n\n示例输出：\n\n```sql\n*************************** 1. row ***************************\n               INSTANCE: 127.0.0.1:10080\n                     ID: 426831815660273668\n             START_TIME: 2021-08-06 07:16:00.081000\n     CURRENT_SQL_DIGEST: 06da614b93e62713bd282d4685fc5b88d688337f36e88fe55871726ce0eb80d7\nCURRENT_SQL_DIGEST_TEXT: update `t` set `v` = `v` + ? where `id` = ? ;\n                  STATE: LockWaiting\n     WAITING_START_TIME: 2021-08-06 07:16:00.087720\n        MEM_BUFFER_KEYS: 0\n       MEM_BUFFER_BYTES: 0\n             SESSION_ID: 77\n                   USER: root\n                     DB: test\n        ALL_SQL_DIGESTS: [\"0fdc781f19da1c6078c9de7eadef8a307889c001e05f107847bee4cfc8f3cdf3\",\"06da614b93e62713bd282d4685fc5b88d688337f36e88fe55871726ce0eb80d7\"]\n*************************** 2. row ***************************\n               INSTANCE: 127.0.0.1:10080\n                     ID: 426831818019569665\n             START_TIME: 2021-08-06 07:16:09.081000\n     CURRENT_SQL_DIGEST: 06da614b93e62713bd282d4685fc5b88d688337f36e88fe55871726ce0eb80d7\nCURRENT_SQL_DIGEST_TEXT: update `t` set `v` = `v` + ? where `id` = ? ;\n                  STATE: LockWaiting\n     WAITING_START_TIME: 2021-08-06 07:16:09.290271\n        MEM_BUFFER_KEYS: 0\n       MEM_BUFFER_BYTES: 0\n             SESSION_ID: 75\n                   USER: root\n                     DB: test\n        ALL_SQL_DIGESTS: [\"0fdc781f19da1c6078c9de7eadef8a307889c001e05f107847bee4cfc8f3cdf3\",\"06da614b93e62713bd282d4685fc5b88d688337f36e88fe55871726ce0eb80d7\"]\n2 rows in set (0.00 sec)\n```\n\n### 事务被长时间阻塞\n\n如果已知一个事务被另一事务（或多个事务）阻塞，且已知当前事务的 `start_ts`（即事务 ID），则可使用如下方式获取导致该事务阻塞的事务的信息。注意对 Lock View 中的多张表进行 join 时，不同表之间的数据并不保证在同一时刻获取，因而可能不同表中的信息可能并不同步。\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect l.key, trx.*, tidb_decode_sql_digests(trx.all_sql_digests) as sqls from information_schema.data_lock_waits as l join information_schema.cluster_tidb_trx as trx on l.current_holding_trx_id = trx.id where l.trx_id = 426831965449355272\\G\n```\n\n示例输出：\n\n```sql\n*************************** 1. row ***************************\n                    key: 74800000000000004D5F728000000000000001\n               INSTANCE: 127.0.0.1:10080\n                     ID: 426832040186609668\n             START_TIME: 2021-08-06 07:30:16.581000\n     CURRENT_SQL_DIGEST: 06da614b93e62713bd282d4685fc5b88d688337f36e88fe55871726ce0eb80d7\nCURRENT_SQL_DIGEST_TEXT: update `t` set `v` = `v` + ? where `id` = ? ;\n                  STATE: LockWaiting\n     WAITING_START_TIME: 2021-08-06 07:30:16.592763\n        MEM_BUFFER_KEYS: 1\n       MEM_BUFFER_BYTES: 19\n             SESSION_ID: 113\n                   USER: root\n                     DB: test\n        ALL_SQL_DIGESTS: [\"0fdc781f19da1c6078c9de7eadef8a307889c001e05f107847bee4cfc8f3cdf3\",\"a4e28cc182bdd18288e2a34180499b9404cd0ba07e3cc34b6b3be7b7c2de7fe9\",\"06da614b93e62713bd282d4685fc5b88d688337f36e88fe55871726ce0eb80d7\"]\n                   sqls: [\"begin ;\",\"select * from `t` where `id` = ? for update ;\",\"update `t` set `v` = `v` + ? where `id` = ? ;\"]\n1 row in set (0.01 sec)\n```\n\n上述查询中，对 `CLUSTER_TIDB_TRX` 表的 `ALL_SQL_DIGESTS` 列使用了 [`TIDB_DECODE_SQL_DIGESTS`](/functions-and-operators/tidb-functions.md#tidb_decode_sql_digests) 函数，目的是将该列（内容为一组 SQL Digest）转换为其对应的归一化 SQL 语句，便于阅读。\n\n如果当前事务的 `start_ts` 未知，可以尝试从 `TIDB_TRX` / `CLUSTER_TIDB_TRX` 表或者 [`PROCESSLIST` / `CLUSTER_PROCESSLIST`](/information-schema/information-schema-processlist.md) 表中的信息进行判断。\n\n### 元数据锁\n\n如果一个会话在等待 schema 更改，这可能是元数据锁引起的。更多详细信息，参见[元数据锁](/metadata-lock.md)。\n\n## 处理乐观锁冲突问题\n\n以下介绍乐观事务模式下常见的锁冲突问题的处理方式。\n\n### 读写冲突\n\n在 TiDB 中，读取数据时，会获取一个包含当前物理时间且全局唯一递增的时间戳作为当前事务的 start_ts。事务在读取时，需要读到目标 key 的 commit_ts 小于这个事务的 start_ts 的最新的数据版本。当读取时发现目标 key 上存在 lock 时，因为无法知道上锁的那个事务是在 Commit 阶段还是 Prewrite 阶段，所以就会出现读写冲突的情况，如下图：\n\n![读写冲突](/media/troubleshooting-lock-pic-04.png)\n\n分析：\n\nTxn0 完成了 Prewrite，在 Commit 的过程中 Txn1 对该 key 发起了读请求，Txn1 需要读取 start_ts > commit_ts 最近的 key 的版本。此时，Txn1 的 `start_ts > Txn0` 的 lock_ts，需要读取的 key 上的锁信息仍未清理，故无法判断 Txn0 是否提交成功，因此 Txn1 与 Txn0 出现读写冲突。\n\n你可以通过如下两种途径来检测当前环境中是否存在读写冲突：\n\n1. TiDB 监控及日志\n\n    * 通过 TiDB Grafana 监控分析：\n\n        观察 KV Errors 下 Lock Resolve OPS 面板中的 not_expired/resolve 监控项以及 KV Backoff OPS 面板中的 tikvLockFast 监控项，如果有较为明显的上升趋势，那么可能是当前的环境中出现了大量的读写冲突。其中，not_expired 是指对应的锁还没有超时，resolve 是指尝试清锁的操作，tikvLockFast 代表出现了读写冲突。\n\n        ![KV-backoff-txnLockFast-optimistic](/media/troubleshooting-lock-pic-09.png)\n        ![KV-Errors-resolve-optimistic](/media/troubleshooting-lock-pic-08.png)\n\n    * 通过 TiDB 日志分析：\n\n        在 TiDB 的日志中可以看到下列信息：\n\n        ```log\n        [INFO] [coprocessor.go:743] [\"[TIME_COP_PROCESS] resp_time:406.038899ms txnStartTS:416643508703592451 region_id:8297 store_addr:10.8.1.208:20160 backoff_ms:255 backoff_types:[txnLockFast,txnLockFast] kv_process_ms:333 scan_total_write:0 scan_processed_write:0 scan_total_data:0 scan_processed_data:0 scan_total_lock:0 scan_processed_lock:0\"]\n        ```\n\n        * txnStartTS：发起读请求的事务的 start_ts，如上面示例中的 416643508703592451\n        * backoff_types：读写发生了冲突，并且读请求进行了 backoff 重试，重试的类型为 txnLockFast\n        * backoff_ms：读请求 backoff 重试的耗时，单位为 ms，如上面示例中的 255\n        * region_id：读请求访问的目标 region 的 id\n\n2. 通过 TiKV 日志分析：\n\n    在 TiKV 的日志可以看到下列信息：\n\n    ```log\n    [ERROR] [endpoint.rs:454] [error-response] [err=\"\"locked primary_lock:7480000000000004D35F6980000000000000010380000000004C788E0380000000004C0748 lock_version: 411402933858205712 key: 7480000000000004D35F7280000000004C0748 lock_ttl: 3008 txn_size: 1\"\"]\n    ```\n\n    这段报错信息表示出现了读写冲突，当读数据时发现 key 有锁阻碍读，这个锁包括未提交的乐观锁和未提交的 prewrite 后的悲观锁。\n\n    * primary_lock：锁对应事务的 primary lock。\n    * lock_version：锁对应事务的 start_ts。\n    * key：表示被锁的 key。\n    * lock_ttl: 锁的 TTL。\n    * txn_size：锁所在事务在其 Region 的 key 数量，指导清锁方式。\n\n处理建议：\n\n* 在遇到读写冲突时会有 backoff 自动重试机制，如上述示例中 Txn1 会进行 backoff 重试，单次初始 10 ms，单次最大 3000 ms，总共最大 20000 ms\n\n* 可以使用 TiDB Control 的子命令 [decoder](/tidb-control.md#decoder-命令) 来查看指定 key 对应的行的 table id 以及 rowid：\n\n    ```sh\n    ./tidb-ctl decoder \"t\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\x1c_r\\x00\\x00\\x00\\x00\\x00\\x00\\x00\\xfa\"\n    format: table_row\n    table_id: -9223372036854775780\n    row_id: -9223372036854775558\n    ```\n\n### KeyIsLocked 错误\n\n事务在 Prewrite 阶段的第一步就会检查是否有写写冲突，第二步会检查目标 key 是否已经被另一个事务上锁。当检测到该 key 被 lock 后，会在 TiKV 端报出 KeyIsLocked。目前该报错信息没有打印到 TiDB 以及 TiKV 的日志中。与读写冲突一样，在出现 KeyIsLocked 时，后台会自动进行 backoff 重试。\n\n你可以通过 TiDB Grafana 监控检测 KeyIsLocked 错误：\n\n观察 KV Errors 下 Lock Resolve OPS 面板中的 resolve 监控项以及 KV Backoff OPS 面板中的 txnLock 监控项，会有比较明显的上升趋势，其中 resolve 是指尝试清锁的操作，txnLock 代表出现了写冲突。\n\n![KV-backoff-txnLockFast-optimistic-01](/media/troubleshooting-lock-pic-07.png)\n![KV-Errors-resolve-optimistic-01](/media/troubleshooting-lock-pic-08.png)\n\n处理建议：\n\n* 监控中出现少量 txnLock，无需过多关注。后台会自动进行 backoff 重试，单次初始 100 ms，单次最大 3000 ms。\n* 如果出现大量的 txnLock，需要从业务的角度评估下冲突的原因。\n* 使用悲观锁模式。\n\n### 锁被清除 (LockNotFound) 错误\n\nTxnLockNotFound 错误是由于事务提交的慢了，超过了 TTL 的时间。当要提交时，发现被其他事务给 Rollback 掉了。在开启 TiDB [自动重试事务](/system-variables.md#tidb_retry_limit)的情况下，会自动在后台进行事务重试（注意显示和隐式事务的差别）。\n\n你可以通过如下两种途径来查看 LockNotFound 报错信息：\n\n1. 查看 TiDB 日志\n\n    如果出现了 TxnLockNotFound 的报错，会在 TiDB 的日志中看到下面的信息：\n\n    ```log\n    [WARN] [session.go:446] [\"commit failed\"] [conn=149370] [\"finished txn\"=\"Txn{state=invalid}\"] [error=\"[kv:6]Error: KV error safe to retry tikv restarts txn: Txn(Mvcc(TxnLockNotFound{ start_ts: 412720515987275779, commit_ts: 412720519984971777, key: [116, 128, 0, 0, 0, 0, 1, 111, 16, 95, 114, 128, 0, 0, 0, 0, 0, 0, 2] })) [try again later]\"]\n    ```\n\n    * start_ts：出现 TxnLockNotFound 报错的事务的 start_ts，如上例中的 412720515987275779\n    * commit_ts：出现 TxnLockNotFound 报错的事务的 commit_ts，如上例中的 412720519984971777\n\n2. 查看 TiKV 日志\n\n    如果出现了 TxnLockNotFound 的报错，在 TiKV 的日志中同样可以看到相应的报错信息：\n\n    ```log\n    Error: KV error safe to retry restarts txn: Txn(Mvcc(TxnLockNotFound)) [ERROR [Kv.rs:708] [\"KvService::batch_raft send response fail\"] [err=RemoteStoped]\n    ```\n\n处理建议：\n\n* 通过检查 start_ts 和 commit_ts 之间的提交间隔，可以确认是否超过了默认的 TTL 的时间。\n\n    查看提交间隔：\n\n    ```shell\n    tiup ctl:v<CLUSTER_VERSION> pd tso [start_ts]\n    tiup ctl:v<CLUSTER_VERSION> pd tso [commit_ts]\n    ```\n\n* 建议检查下是否是因为写入性能的缓慢导致事务提交的效率差，进而出现了锁被清除的情况。\n\n* 在关闭 TiDB 事务重试的情况下，需要在应用端捕获异常，并进行重试。\n\n## 处理悲观锁冲突问题\n\n以下介绍悲观事务模式下常见的锁冲突问题的处理方式。\n\n> **注意：**\n>\n> 即使设置了悲观事务模式，autocommit 事务仍然会优先尝试使用乐观事务模式进行提交，并在发生冲突后、自动重试时切换为悲观事务模式。\n\n### 读写冲突\n\n报错信息以及处理建议同乐观锁模式。\n\n### pessimistic lock retry limit reached\n\n在冲突非常严重的场景下，或者当发生 write conflict 时，乐观事务会直接终止，而悲观事务会尝试用最新数据重试该语句直到没有 write conflict。因为 TiDB 的加锁操作是一个写入操作，且操作过程是先读后写，需要 2 次 RPC。如果在这中间发生了 write conflict，那么会重试。每次重试都会打印日志，不用特别关注。重试次数由 [pessimistic-txn.max-retry-count](/tidb-configuration-file.md#max-retry-count) 定义。\n\n可通过查看 TiDB 日志查看报错信息：\n\n悲观事务模式下，如果发生 write conflict，并且重试的次数达到了上限，那么在 TiDB 的日志中会出现含有下述关键字的报错信息。如下：\n\n```log\nerr=\"pessimistic lock retry limit reached\"\n```\n\n处理建议：\n\n* 如果上述报错出现的比较频繁，建议从业务的角度进行调整。\n* 如果业务中包含对同一行（同一个 key）的高并发上锁而频繁冲突，可以尝试启用系统变量 [`tidb_pessimistic_txn_fair_locking`](/system-variables.md#tidb_pessimistic_txn_fair_locking-从-v700-版本开始引入)。需要注意启用该选项可能对存在锁冲突的事务带来一定程度的吞吐下降（平均延迟上升）的代价。对于新部署的集群，该选项默认启用 (`ON`) 。\n\n### Lock wait timeout exceeded\n\n在悲观锁模式下，事务之间出现会等锁的情况。等锁的超时时间由 TiDB 的 [innodb_lock_wait_timeout](/system-variables.md#innodb_lock_wait_timeout) 参数来定义，这个是 SQL 语句层面的最大允许等锁时间，即一个 SQL 语句期望加锁，但锁一直获取不到，超过这个时间，TiDB 不会再尝试加锁，会向客户端返回相应的报错信息。\n\n可通过查看 TiDB 日志查看报错信息：\n\n当出现等锁超时的情况时，会向客户端返回下述报错信息：\n\n```log\nERROR 1205 (HY000): Lock wait timeout exceeded; try restarting transaction\n```\n\n处理建议：\n\n* 如果出现的次数非常频繁，建议从业务逻辑的角度来进行调整。\n\n### TTL manager has timed out\n\n除了有不能超出 GC 时间的限制外，悲观锁的 TTL 有上限，默认为 1 小时，所以执行时间超过 1 小时的悲观事务有可能提交失败。这个超时时间由 TiDB 参数 [`performance.max-txn-ttl`](https://github.com/pingcap/tidb/blob/master/pkg/config/config.toml.example) 指定。\n\n可通过查看 TiDB 日志查看报错信息：\n\n当悲观锁的事务执行时间超过 TTL 时，会出现下述报错：\n\n```log\nTTL manager has timed out, pessimistic locks may expire, please commit or rollback this transaction\n```\n\n处理建议：\n\n* 当遇到该报错时，建议确认下业务逻辑是否可以进行优化，如将大事务拆分为小事务。在未使用[大事务](/tidb-configuration-file.md#txn-total-size-limit)的前提下，大事务可能会触发 TiDB 的事务限制。\n\n* 可适当调整相关参数，使其符合事务要求。\n\n### Deadlock found when trying to get lock\n\n死锁是指两个或两个以上的事务在执行过程中，由于竞争资源而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去，将永远在互相等待。此时，需要终止其中一个事务使其能够继续推进下去。\n\nTiDB 在使用悲观锁的情况下，多个事务之间出现了死锁，必定有一个事务 abort 来解开死锁。在客户端层面行为和 MySQL 一致，在客户端返回表示死锁的 Error 1213。如下：\n\n```log\n[err=\"[executor:1213]Deadlock found when trying to get lock; try restarting transaction\"]\n```\n\n处理建议：\n\n* 如果难以确认产生死锁的原因，对于 v5.1 及以后的版本，建议尝试查询 `INFORMATION_SCHEMA.DEADLOCKS` 或 `INFORMATION_SCHEMA.CLUSTER_DEADLOCKS` 系统表来获取死锁的等待链信息。详情请参考[死锁错误](#死锁错误)小节和 [`DEADLOCKS` 表](/information-schema/information-schema-deadlocks.md)文档。\n* 如果死锁出现非常频繁，需要调整业务代码来降低发生概率。\n"
        },
        {
          "name": "troubleshoot-stale-read.md",
          "type": "blob",
          "size": 13.5947265625,
          "content": "---\ntitle: 理解 TiKV 中的 Stale Read 和 safe-ts\nsummary: TiKV 中的 Stale Read 依赖于 safe-ts，保证读取历史数据版本的安全性。safe-ts 由每个 Region 中的 peer 维护，resolved-ts 则由 Region leader 维护。诊断 Stale Read 问题可通过 Grafana、tikv-ctl 和日志。常见原因包括事务提交时间过长、事务存在时间过长以及 CheckLeader 信息推送延迟。处理慢事务提交可通过识别锁所属的事务和检查应用程序逻辑。处理长事务可通过识别事务、检查应用程序逻辑和处理慢查询。解决 CheckLeader 问题可通过检查网络和监控面板指标。\n---\n\n# 理解 TiKV 中的 Stale Read 和 safe-ts\n\n在本文档中，你可以了解 TiKV 中 Stale Read 和 safe-ts 的原理以及如何诊断与 Stale Read 相关的常见问题。\n\n## Stale Read 和 safe-ts 概述\n\n[Stale Read](/stale-read.md) 是一种读取历史数据版本的机制，读取 TiDB 中存储的历史数据版本。在 TiKV 中，Stale Read 依赖 [safe-ts](#什么是-safe-ts)。如果一个 Region peer 上的读请求的时间戳 (timestamp, ts) 小于等于 Region 的 safe-ts，TiDB 可以安全地从 peer 上读取数据。TiKV 通过保证 safe-ts 总是小于等于 [resolved-ts](#什么是-resolved-ts) 来保证这种安全性。\n\n## 理解 safe-ts 和 resolved-ts\n\n本章节介绍 safe-ts 和 resolved-ts 的概念和维护方式。\n\n### 什么是 safe-ts？\n\nsafe-ts 是一个由 Region 中的每个 peer 维护的时间戳，它保证所有时间戳小于等于 safe-ts 的事务已经被 peer apply，从而实现本地 Stale Read。\n\n### 什么是 resolved-ts？\n\nresolved-ts 是一个时间戳，它保证所有时间戳小于该值的事务已经被 leader apply。与 safe-ts 不同，resolved-ts 只由 Region leader 维护。Follower 可能有一个比 leader 更小的 apply index，因此 resolved-ts 不能直接被当作 safe-ts。\n\n### safe-ts 的维护\n\n`RegionReadProgress` 模块维护 safe-ts。Region leader 维护 resolved-ts，并定期通过 CheckLeader RPC 将 resolved-ts、最小的（使 resolved-ts 生效的）apply index和 Region 本身发送给所有副本的 `RegionReadProgerss` 模块。\n\n当一个 peer apply 数据时，它会更新 apply index，并检查是否有 pending resolved-ts 可以成为新的 safe-ts。\n\n### resolved-ts 的维护\n\nRegion leader 使用一个 resolver 来管理 resolved-ts。该 resolver 通过接收 Raft apply 时的变更日志来跟踪 LOCK CF (Column Family) 中的锁。当初始化时，resolver 会扫描整个 Region 来跟踪锁。\n\n## 诊断 Stale Read 问题\n\n本章节介绍如何使用 Grafana、`tikv-ctl` 和日志诊断 Stale Read 问题。\n\n### 识别问题\n\n在 [Grafana > TiDB dashboard > **KV Request** 监控面板](/grafana-tidb-dashboard.md#kv-request)中，以下面板显示了 Stale Read 的命中率、OPS 和流量：\n\n![Stale Read Hit/Miss OPS](/media/stale-read/metrics-hit-miss.png)\n\n![Stale Read Req OPS](/media/stale-read/metrics-ops.png)\n\n![Stale Read Req Traffic](/media/stale-read/traffic.png)\n\n关于上述监控项的更多信息，参考 [TiDB 监控指标](/grafana-tidb-dashboard.md#kv-request)。\n\n当 Stale Read 问题发生时，你可能会注意到上述监控项的变化。最直接的指标是 TiDB 的 WARN 日志，它会报告 `DataIsNotReady` 和 Region ID，以及它遇到的 `safe-ts`。\n\n### 常见原因\n\n下面是影响 Stale Read 有效性的常见原因：\n\n- 事务提交时间过长。\n- 事务在提交前存在太长时间。\n- 从 leader 到 follower 推送 CheckLeader 信息的延迟。\n\n### 使用 Grafana 诊断\n\n在 [**TiKV-Details** > **Resolved-TS** 监控面板](/grafana-tikv-dashboard.md#resolved-ts)中，你可以识别每个 TiKV 上 resolved-ts 和 safe-ts 最小的 Region。如果这些时间戳明显落后于实时时间，你需要使用 `tikv-ctl` 检查这些 Region 的详细信息。\n\n### 使用 `tikv-ctl` 诊断\n\n`tikv-ctl` 提供了 resolver 和 `RegionReadProgress` 的最新详细信息。更多信息，参考[获取 Region 的 `RegionReadProgress` 状态](/tikv-control.md#获取一个-region-的-regionreadprogress-状态)。\n\n下面是一个使用示例：\n\n```bash\n./tikv-ctl --host 127.0.0.1:20160 get-region-read-progress -r 14 --log --min-start-ts 0\n```\n\n输出结果如下：\n\n```log\nRegion read progress:\n    exist: true,\n    safe_ts: 0,\n    applied_index: 92,\n    pending front item (oldest) ts: 0,\n    pending front item (oldest) applied index: 0,\n    pending back item (latest) ts: 0,\n    pending back item (latest) applied index: 0,\n    paused: false,\nResolver:\n    exist: true,\n    resolved_ts: 0,\n    tracked index: 92,\n    number of locks: 0,\n    number of transactions: 0,\n    stopped: false,\n```\n\n上面的输出结果可以帮助你判断：\n\n- 锁是否阻塞了 resolved-ts。\n- apply index 是否太小而无法更新 safe-ts。\n- 当存在 follower peer 时，leader 是否发送了更新的 resolved-ts。\n\n### 使用日志诊断\n\nTiKV 每 10 秒检查以下监控项：\n\n- resolved-ts 最小的 Region leader\n- resolved-ts 最小的 Region follower\n- safe-ts 最小的 Region follower\n\n如果这些时间戳中的任何一个异常地小，TiKV 就会打印日志。\n\n当你想要诊断一个已经不存在的历史问题时，这些日志尤其有用。\n\n下面是日志的示例：\n\n```log\n[2023/08/29 16:48:18.118 +08:00] [INFO] [endpoint.rs:505] [\"the max gap of leader resolved-ts is large\"] [last_resolve_attempt=\"Some(LastAttempt { success: false, ts: TimeStamp(443888082736381953), reason: \\\"lock\\\", lock: Some(7480000000000000625F728000000002512B5C) })\"] [duration_to_last_update_safe_ts=10648ms] [min_memory_lock=None] [txn_num=0] [lock_num=0] [min_lock=None] [safe_ts=443888117326544897] [gap=110705ms] [region_id=291]\n\n[2023/08/29 16:48:18.118 +08:00] [INFO] [endpoint.rs:526] [\"the max gap of follower safe-ts is large\"] [oldest_candidate=None] [latest_candidate=None] [applied_index=3276] [duration_to_last_consume_leader=11460ms] [resolved_ts=443888117117353985] [safe_ts=443888117117353985] [gap=111503ms] [region_id=273]\n\n[2023/08/29 16:48:18.118 +08:00] [INFO] [endpoint.rs:547] [\"the max gap of follower resolved-ts is large; it's the same region that has the min safe-ts\"]\n```\n\n## 诊断建议\n\n### 处理慢事务提交\n\n提交时间长的事务通常是大事务。这个慢事务的 prewrite 阶段会留下一些锁，但是在 commit 阶段清理掉锁之前需要很长时间。为了解决这个问题，你可以尝试识别锁所属的事务，并找出它们存在的原因，例如使用日志。\n\n下面是一些你可以采取的措施：\n\n- 在 `tikv-ctl` 命令中指定 `--log` 选项，并在 TiKV 日志中通过 start_ts 查找相应的锁。\n- 在 TiDB 和 TiKV 日志中搜索 start_ts，以识别事务的问题。\n\n    如果一个查询花费超过 60 秒，就会打印一个带有 SQL 语句的 `expensive_query` 日志。你可以使用 start_ts 值匹配日志。下面是一个示例：\n\n    ```log\n    [2023/07/17 19:32:09.403 +08:00] [WARN] [expensivequery.go:145] [expensive_query] [cost_time=60.025022732s] [cop_time=0.00346666s] [process_time=8.358409508s] [wait_time=0.013582596s] [request_count=278] [total_keys=9943616] [process_keys=9943360] [num_cop_tasks=278] [process_avg_time=0.030066221s] [process_p90_time=0.045296042s] [process_max_time=0.052828934s] [process_max_addr=192.168.31.244:20160] [wait_avg_time=0.000048858s] [wait_p90_time=0.00006057s] [wait_max_time=0.00040991s] [wait_max_addr=192.168.31.244:20160] [stats=t:442916666913587201] [conn=2826881778407440457] [user=root] [database=test] [table_ids=\"[100]\"] [**txn_start_ts**=442916790435840001] [mem_max=\"2514229289 Bytes (2.34 GB)\"] [sql=\"update t set b = b + 1\"]\n    ```\n\n- 如果你无法从日志中获取关于锁的足够信息，可以使用 [`CLUSTER_TIDB_TRX`](/information-schema/information-schema-tidb-trx.md#cluster_tidb_trx) 表查找活跃的事务。\n- 执行 [`SHOW PROCESSLIST`](/sql-statements/sql-statement-show-processlist.md) 查看当前连接到同一个 TiDB 服务器的会话及其在当前语句上花费的时间。但是它不会显示 start_ts。\n\n如果锁是由于正在进行的大事务而存在的，考虑修改你的应用程序逻辑，因为这些锁会阻碍 resolve-ts 的进度。\n\n如果锁不属于任何正在进行的事务，可能是由于协调器 (TiDB) 在预写锁之后崩溃。在这种情况下，TiDB 会自动解决锁。除非问题持续存在，否则不需要采取任何措施。\n\n### 处理长事务\n\n长时间保持活跃的事务，即使最终提交了，也可能会阻塞 resolved-ts 的进度。这是因为这些长期存在的事务的 start-ts 用于计算 resolved-ts。\n\n要解决这个问题，你可以：\n\n- 识别事务：首先识别与锁相关的事务，了解它们存在的原因。你可以使用日志帮助识别。\n\n- 检查应用程序逻辑：如果长时间的事务持续时间是由于应用程序逻辑导致的，考虑修改应用程序以防止这种情况发生。\n\n- 处理慢查询：如果事务的持续时间由于慢查询而延长，优先解决这些查询以缓解问题。\n\n### 解决 CheckLeader 问题\n\n为了解决 CheckLeader 问题，你可以检查网络和 [**TiKV-Details** > **Resolved-TS** 监控面板](/grafana-tikv-dashboard.md#resolved-ts)中的 **Check Leader Duration** 指标。\n\n## 示例\n\n如果你观察到 **Stale Read OPS** 的 miss rate 增加，如下所示：\n\n![Example: Stale Read OPS](/media/stale-read/example-ops.png)\n\n首先，你可以检查 [**TiKV-Details** > **Resolved-TS** 监控面板](/grafana-tikv-dashboard.md#resolved-ts)中的 **Max Resolved TS gap** 和 **Min Resolved TS Region** 指标：\n\n![Example: Max Resolved TS gap](/media/stale-read/example-ts-gap.png)\n\n从上述指标中，你可以发现 Region `3121` 和其他一些 Region 没有及时更新 resolved-ts。\n\n为了获取 Region `3121` 的更多详细信息，你可以执行以下命令：\n\n```bash\n./tikv-ctl --host 127.0.0.1:20160 get-region-read-progress -r 3121 --log\n```\n\n输出结果如下：\n\n```log\nRegion read progress:\n    exist: true,\n    safe_ts: 442918444145049601,\n    applied_index: 2477,\n    read_state.ts: 442918444145049601,\n    read_state.apply_index: 1532,\n    pending front item (oldest) ts: 0,\n    pending front item (oldest) applied index: 0,\n    pending back item (latest) ts: 0,\n    pending back item (latest) applied index: 0,\n    paused: false,\n    discarding: false,\nResolver:\n    exist: true,\n    resolved_ts: 442918444145049601,\n    tracked index: 2477,\n    number of locks: 480000,\n    number of transactions: 1,\n    stopped: false,\n```\n\n值得注意的是，`applied_index` 等于 resolver 中的 `tracked index`，均为 `2477`。因此，resolver 可能是这个问题的根源。你还可以看到，有 1 个事务在这个 Region 中留下了 480000 个锁，这可能是问题的原因。\n\n为了获取确切的事务和一些锁的 keys，你可以检查 TiKV 日志并搜索 `locks with`。输出结果如下：\n\n```log\n[2023/07/17 21:16:44.257 +08:00] [INFO] [resolver.rs:213] [\"locks with the minimum start_ts in resolver\"] [keys=\"[74800000000000006A5F7280000000000405F6, ... , 74800000000000006A5F72800000000000EFF6, 74800000000000006A5F7280000000000721D9, 74800000000000006A5F72800000000002F691]\"] [start_ts=442918429687808001] [region_id=3121]\n```\n\n从 TiKV 日志中，你可以获取事务的 start_ts，即 `442918429687808001`。为了获取关于语句和事务的更多信息，你可以在 TiDB 日志中搜索这个时间戳。找到结果如下：\n\n```log\n[2023/07/17 21:16:18.287 +08:00] [INFO] [2pc.go:685] [\"[BIG_TXN]\"] [session=2826881778407440457] [\"key sample\"=74800000000000006a5f728000000000000000] [size=319967171] [keys=10000000] [puts=10000000] [dels=0] [locks=0] [checks=0] [txnStartTS=442918429687808001]\n\n[2023/07/17 21:16:22.703 +08:00] [WARN] [expensivequery.go:145] [expensive_query] [cost_time=60.047172498s] [cop_time=0.004575113s] [process_time=15.356963423s] [wait_time=0.017093811s] [request_count=397] [total_keys=20000398] [process_keys=10000000] [num_cop_tasks=397] [process_avg_time=0.038682527s] [process_p90_time=0.082608262s] [process_max_time=0.116321331s] [process_max_addr=192.168.31.244:20160] [wait_avg_time=0.000043057s] [wait_p90_time=0.00004007s] [wait_max_time=0.00075014s] [wait_max_addr=192.168.31.244:20160] [stats=t:442918428521267201] [conn=2826881778407440457] [user=root] [database=test] [table_ids=\"[106]\"] [txn_start_ts=442918429687808001] [mem_max=\"2513773983 Bytes (2.34 GB)\"] [sql=\"update t set b = b + 1\"]\n```\n\n接着，你可以基本定位导致问题的语句。为了进一步检查，你可以执行 [`SHOW PROCESSLIST`](/sql-statements/sql-statement-show-processlist.md) 语句。输出结果如下：\n\n```sql\n+---------------------+------+---------------------+--------+---------+------+------------+---------------------------+\n| Id                  | User | Host                | db     | Command | Time | State      | Info                      |\n+---------------------+------+---------------------+--------+---------+------+------------+---------------------------+\n| 2826881778407440457 | root | 192.168.31.43:58641 | test   | Query   | 48   | autocommit | update t set b = b + 1    |\n| 2826881778407440613 | root | 127.0.0.1:45952     | test   | Execute | 0    | autocommit | select * from t where a=? |\n| 2826881778407440619 | root | 192.168.31.43:60428 | <null> | Query   | 0    | autocommit | show processlist          |\n+---------------------+------+---------------------+--------+---------+------+------------+---------------------------+\n```\n\n输出结果显示，有程序正在执行一个意外的 `UPDATE` 语句 (`update t set b = b + 1`)，这导致了一个大事务并阻塞了 Stale Read。\n\n你可以停止执行这个 `UPDATE` 语句的应用程序来解决这个问题。\n"
        },
        {
          "name": "troubleshoot-tidb-cluster.md",
          "type": "blob",
          "size": 6.3330078125,
          "content": "---\ntitle: TiDB 集群故障诊断\naliases: ['/docs-cn/dev/troubleshoot-tidb-cluster/','/docs-cn/dev/how-to/troubleshoot/cluster-setup/']\nsummary: TiDB 集群故障诊断包括收集出错信息、组件状态、日志信息、机器配置和 dmesg 中的问题。解决数据库连接问题需要确认服务是否启动，查看 tidb-server 日志并清空数据重新部署服务。解决 tidb-server 启动报错需检查参数、端口占用和 pd-server 连接。解决 tikv-server 启动报错需检查参数、端口占用和 pd-server 连接。解决 pd-server 启动报错需检查参数和端口占用。进程异常退出需检查是否在前台启动，使用 nohup+& 方式运行或写在脚本中。TiKV 进程异常重启需检查 OOM 信息和 panic log。连接被拒绝需确保网络参数正确。解决文件打开过多问题需确保 ulimit -n 足够大。数据库访问超时需检查拓扑结构、硬件配置、其他服务、操作、CPU 线程、网络 /IO 监控数据。\n---\n\n# TiDB 集群故障诊断\n\n当试用 TiDB 遇到问题时，请先参考本篇文档。如果问题未解决，请收集以下信息并通过 [TiDB 支持资源](/support.md)解决：\n\n+ 具体的出错信息以及正在执行的操作\n+ 当前所有组件的状态\n+ 出问题组件 log 中的 error/fatal/panic 信息\n+ 机器配置以及部署拓扑\n+ dmesg 中 TiDB 组件相关的问题\n\n## 数据库连接不上\n\n首先请确认集群的各项服务是否已经启动，包括 tidb-server、pd-server、tikv-server。请用 ps 命令查看所有进程是否在。如果某个组件的进程已经不在了，请参考对应的章节排查错误。\n\n如果所有的进程都在，请查看 tidb-server 的日志，看是否有报错？常见的错误包括：\n\n+ InformationSchema is out of date\n\n    无法连接 tikv-server，请检查 pd-server 以及 tikv-server 的状态和日志。\n\n+ panic\n\n    程序有错误，请将具体的 panic log [提供给 TiDB 开发者](https://github.com/pingcap/tidb/issues/new/choose)。\n\n    如果是清空数据并重新部署服务，请确认以下信息：\n\n+ pd-server、tikv-server 数据都已清空\n\n    tikv-server 存储具体的数据，pd-server 存储 tikv-server 中数据的元信息。如果只清空 pd-server 或只清空 tikv-server 的数据，会导致两边数据不匹配。\n\n+ 清空 pd-server 和 tikv-server 的数据并重启后，也需要重启 tidb-server\n\n    集群 ID 是由 pd-server 在集群初始化时随机分配，所以重新部署集群后，集群 ID 会发生变化。tidb-server 业务需要重启以获取新的集群 ID。\n\n## tidb-server 启动报错\n\ntidb-server 无法启动的常见情况包括：\n\n+ 启动参数错误\n\n    请参考 [TiDB 命令行参数](/command-line-flags-for-tidb-configuration.md)。\n\n+ 端口被占用：`lsof -i:port`\n\n    请确保 tidb-server 启动所需要的端口未被占用。\n\n+ 无法连接 pd-server\n\n    首先检查 pd-server 的进程状态和日志，确保 pd-server 成功启动，对应端口已打开：`lsof -i:port`。\n\n    若 pd-server 正常，则需要检查 tidb-server 机器和 pd-server 对应端口之间的连通性，确保网段连通且对应服务端口已添加到防火墙白名单中，可通过 nc 或 curl 工具检查。\n\n    例如，假设 tidb 服务位于 `192.168.1.100`，无法连接的 pd 位于 `192.168.1.101`，且 2379 为其 client port，则可以在 tidb 机器上执行 `nc -v -z 192.168.1.101 2379`，测试是否可以访问端口。或使用 `curl -v 192.168.1.101:2379/pd/api/v1/leader` 直接检查 pd 是否正常服务。\n\n## tikv-server 启动报错\n\n+ 启动参数错误\n\n    请参考 [TiKV 启动参数](/command-line-flags-for-tikv-configuration.md)文档。\n\n+ 端口被占用：`lsof -i:port`\n\n    请确保 tikv-server 启动所需要的端口未被占用：`lsof -i:port`。\n\n+ 无法连接 pd-server\n\n    首先检查 pd-server 的进程状态和日志。确保 pd-server 成功启动，对应端口已打开：`lsof -i:port`。\n\n    若 pd-server 正常，则需要检查 tikv-server 机器和 pd-server 对应端口之间的连通性，确保网段连通且对应服务端口已添加到防火墙白名单中，可通过 nc 或 curl 工具检查。具体命令参考上一节。\n\n+ 文件被占用\n\n    不要在一个数据库文件目录上打开两个 tikv。\n\n## pd-server 启动报错\n\n+ 启动参数错误\n\n    请参考 [PD 命令行参数](/command-line-flags-for-pd-configuration.md)文档。\n\n+ 端口被占用：`lsof -i:port`\n\n    请确保 pd-server 启动所需要的端口未被占用：`lsof -i:port`。\n\n## TiDB/TiKV/PD 进程异常退出\n\n+ 进程是否是启动在前台\n\n    当前终端退出给其所有子进程发送 HUP 信号，从而导致进程退出。\n\n+ 是否是在命令行用过 `nohup+&` 方式直接运行\n\n    这样依然可能导致进程因终端连接突然中断，作为终端 SHELL 的子进程被杀掉。\n\n    推荐将启动命令写在脚本中，通过脚本运行（相当于二次 fork 启动）。\n\n## TiKV 进程异常重启\n\n+ 检查 dmesg 或者 syslog 里面是否有 OOM 信息\n\n    如果有 OOM 信息并且杀掉的进程为 TiKV，请减少 TiKV 的 RocksDB 的各个 CF 的 `block-cache-size` 值。\n\n+ 检查 TiKV 日志是否有 panic 的 log\n\n    提交 Issue 并附上 panic 的 log。\n\n## TiDB panic\n\n请提供 panic 的 log。\n\n## 连接被拒绝\n\n+ 请确保操作系统的网络参数正确，包括但不限于\n    - 连接字符串中的端口和 tidb-server 启动的端口需要一致\n    - 请保证防火墙的配置正确\n\n## Too many open files\n\n在启动进程之前，请确保 `ulimit -n` 的结果足够大，推荐设为 unlimited 或者是大于 1000000。\n\n## 数据库访问超时，系统负载高\n\n首先检查 [SLOW-QUERY](/identify-slow-queries.md) 日志，判断是否是因为某条 SQL 语句导致。如果未能解决，请提供如下信息：\n\n+ 部署的拓扑结构\n    - tidb-server/pd-server/tikv-server 部署了几个实例\n    - 这些实例在机器上是如何分布的\n+ 机器的硬件配置\n    - CPU 核数\n    - 内存大小\n    - 硬盘类型（SSD 还是机械硬盘）\n    - 是实体机还是虚拟机\n+ 机器上除了 TiDB 集群之外是否还有其他服务\n+ pd-server 和 tikv-server 是否分开部署\n+ 目前正在进行什么操作\n+ 用 `top -H` 命令查看当前占用 CPU 的线程名\n+ 最近一段时间的网络/IO 监控数据是否有异常\n"
        },
        {
          "name": "troubleshoot-tidb-oom.md",
          "type": "blob",
          "size": 10.8359375,
          "content": "---\ntitle: TiDB OOM 故障排查\nsummary: TiDB OOM 故障排查总结了 OOM 常见问题的解决思路、故障现象、原因、解决方法和需要收集的诊断信息。排查思路包括确认是否属于 OOM 问题和进一步排查触发 OOM 的原因。常见故障原因包括部署问题、数据库问题和客户端问题。处理 OOM 问题需要收集操作系统内存配置、数据库版本和内存配置、Grafana TiDB 内存使用情况等信息。详细排查方法请参考相关章节。\n---\n\n# TiDB OOM 故障排查\n\n本文总结了 TiDB Out of Memory (OOM) 常见问题的解决思路、故障现象、故障原因、解决方法，以及需要收集的诊断信息。在遇到 OOM 问题时，你可以参考本文档来排查错误原因并进行处理。\n\n## 常见故障现象\n\nOOM 常见的故障现象包括（但不限于）：\n\n- 客户端报错：`SQL error, errno = 2013, state = 'HY000': Lost connection to MySQL server during query`\n\n- 查看 Grafana 监控，发现以下现象：\n    - **TiDB** > **Server** > **Memory Usage** 显示 process/heapInUse 持续升高，达到阈值后掉零\n    - **TiDB** > **Server** > **Uptime** 显示为掉零\n    - **TiDB-Runtime** > **Memory Usage** 显示 estimate-inuse 持续升高\n\n- 查看 `tidb.log`，可发现如下日志条目：\n    - OOM 相关的 Alarm：`[WARN] [memory_usage_alarm.go:139] [\"tidb-server has the risk of OOM because of memory usage exceeds alarm ratio. Running SQLs and heap profile will be recorded in record path\"]`。关于该日志的详细说明，请参考 [`memory-usage-alarm-ratio`](/system-variables.md#tidb_memory_usage_alarm_ratio)。\n    - 重启相关的日志条目：`[INFO] [printer.go:33] [\"Welcome to TiDB.\"]`。\n\n## 整体排查思路\n\n在排查 OOM 问题时，整体遵循以下排查思路：\n\n1. 确认是否属于 OOM 问题。\n\n    执行以下命令查看操作系统日志。如果故障发生的时间点附近存在 `oom-killer` 的日志，则可以确定是 OOM 问题。\n\n    ```shell\n    dmesg -T | grep tidb-server\n    ```\n\n    下面是包含 `oom-killer` 的日志输出示例：\n\n    ```shell\n    ......\n    Mar 14 16:55:03 localhost kernel: tidb-server invoked oom-killer: gfp_mask=0x201da, order=0, oom_score_adj=0\n    Mar 14 16:55:03 localhost kernel: tidb-server cpuset=/ mems_allowed=0\n    Mar 14 16:55:03 localhost kernel: CPU: 14 PID: 21966 Comm: tidb-server Kdump: loaded Not tainted 3.10.0-1160.el7.x86_64 #1\n    Mar 14 16:55:03 localhost kernel: Hardware name: QEMU Standard PC (i440FX + PIIX, 1996), BIOS rel-1.14.0-0-g155821a1990b-prebuilt.qemu.org 04/01/2014\n    ......\n    Mar 14 16:55:03 localhost kernel: Out of memory: Kill process 21945 (tidb-server) score 956 or sacrifice child\n    Mar 14 16:55:03 localhost kernel: Killed process 21945 (tidb-server), UID 1000, total-vm:33027492kB, anon-rss:31303276kB, file-rss:0kB, shmem-rss:0kB\n    Mar 14 16:55:07 localhost systemd: tidb-4000.service: main process exited, code=killed, status=9/KILL\n    ......\n    ```\n\n2. 确认是 OOM 问题之后，可以进一步排查触发 OOM 的原因是部署问题还是数据库问题。\n\n    - 如果是部署问题触发 OOM，需要排查资源配置、混合部署的影响。\n    - 如果是数据库问题触发 OOM，常见原因有：\n        - TiDB 处理较大的数据流量，如：大查询、大写入、数据导入等。\n        - TiDB 的高并发场景，多条 SQL 并发消耗资源，或者算子并发高。\n        - TiDB 内存泄露，资源没有释放。\n\n    具体排查方法请参考下面的章节。\n\n## 常见故障原因和解决方法\n\n根据 OOM 出现的原因，一般可以分为以下几种情况：\n\n- [部署问题](#部署问题)\n- [数据库问题](#数据库问题)\n- [客户端问题](#客户端问题)\n\n### 部署问题\n\n如果是由于部署不当导致的 OOM 问题，常见的原因有：\n\n- 操作系统内存容量规划偏小，导致内存不足。\n- TiUP [`resource_control`](/tiup/tiup-cluster-topology-reference.md#global) 配置不合理。\n- 在混合部署的情况下（指 TiDB 和其他应用程序部署在同一台服务器上），其他应用程序抢占资源导致 TiDB 被 `oom-killer` 关闭。\n\n### 数据库问题\n\n本节介绍由数据库问题导致的 OOM 问题和解决办法。\n\n> **注意：**\n>\n> 如果 SQL 返回 `ERROR 1105 (HY000): Out Of Memory Quota![conn_id=54]`，是由于配置了 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 导致，数据库的内存使用控制行为会触发该报错。此报错为正常行为。\n\n#### 执行 SQL 语句时消耗太多内存\n\n可以根据以下不同的触发 OOM 的原因，采取相应的措施减少 SQL 的内存使用：\n\n- 如果 SQL 的执行计划不优，比如由于缺少合适的索引、统计信息过期、优化器 bug 等原因，会导致选错 SQL 的执行计划，进而出现巨大的中间结果集累积在内存中。这种情况下可以考虑采取以下措施：\n    - 添加合适的索引\n    - 使用[算子的数据落盘功能](/configure-memory-usage.md#数据落盘)\n    - 调整表之间的 JOIN 顺序\n    - 使用 hint 进行调优\n\n- 一些算子和函数不支持下推到存储层，导致出现巨大的中间结果集累积。此时可能需要改写业务 SQL，或使用 hint 进行调优，来使用可下推的函数或算子。\n\n- 执行计划中存在算子 HashAgg。HashAgg 是多线程并发执行，虽然执行速度较快，但会消耗较多内存。可以尝试使用 `STREAM_AGG()` 替代。\n\n- 调小同时读取的 Region 的数量，或降低算子并发度，以避免因高并发导致的内存问题。对应的系统变量包括：\n    - [`tidb_distsql_scan_concurrency`](/system-variables.md#tidb_distsql_scan_concurrency)\n    - [`tidb_index_serial_scan_concurrency`](/system-variables.md#tidb_index_serial_scan_concurrency)\n    - [`tidb_executor_concurrency`](/system-variables.md#tidb_executor_concurrency-从-v50-版本开始引入)\n\n- 问题发生时间附近，session 的并发度过高，此时可能需要添加节点进行扩容。\n\n#### 大事务或大写入消耗太多内存\n\n需要提前进行内存的容量规划，这是因为执行事务时 TiDB 进程的内存消耗相对于事务大小会存在一定程度的放大，最大可能达到提交事务大小的 2 到 3 倍以上。\n\n针对单个大事务，可以通过拆分的方式调小事务大小。\n\n#### 收集和加载统计信息的过程中消耗太多内存\n\nTiDB 节点启动后需要加载统计信息到内存中。统计信息的收集过程会消耗内存，可以通过以下方式控制内存使用量：\n\n- 使用指定采样率、指定只收集特定列的统计信息、减少 `ANALYZE` 并发度等手段减少内存使用。\n- TiDB v6.1.0 开始引入了系统变量 [`tidb_stats_cache_mem_quota`](/system-variables.md#tidb_stats_cache_mem_quota-从-v610-版本开始引入)，可以对统计信息的内存使用进行限制。\n- TiDB v6.1.0 开始引入了系统变量 [`tidb_mem_quota_analyze`](/system-variables.md#tidb_mem_quota_analyze-从-v610-版本开始引入)，用于控制 TiDB 更新统计信息时的最大总内存占用。\n\n更多信息请参见[常规统计信息](/statistics.md)。\n\n#### 预处理语句 (Prepared Statement) 使用过量\n\n客户端不断创建预处理语句但未执行 [`deallocate prepare stmt`](/sql-prepared-plan-cache.md#忽略-com_stmt_close-指令和-deallocate-prepare-语句) 会导致内存持续上涨，最终触发 TiDB OOM。原因是预处理语句占用的内存要在 session 关闭后才会释放。这一点在长连接下尤需注意。\n\n要解决该问题，可以考虑采取以下措施：\n\n- 调整 session 的生命周期。\n- 调整[连接池的 `wait_timeout` 和 `max_execution_time` 时长](/develop/dev-guide-connection-parameters.md#超时参数)。\n- 使用系统变量 [`max_prepared_stmt_count`](/system-variables.md#max_prepared_stmt_count) 进行限制。\n\n#### 系统变量配置不当\n\n系统变量 [`tidb_enable_rate_limit_action`](/system-variables.md#tidb_enable_rate_limit_action) 在单条查询仅涉及读数据的情况下，对内存控制效果较好。若还存在额外的计算操作（如连接、聚合等），启动该变量可能会导致内存不受 [`tidb_mem_quota_query`](/system-variables.md#tidb_mem_quota_query) 控制，加剧 OOM 风险。\n\n建议关闭该变量。从 TiDB v6.3.0 开始，该变量默认关闭。\n\n### 客户端问题\n\n若客户端发生 OOM，则需要排查以下方面：\n\n- 观察 **Grafana TiDB Details** > **Server** > **Client Data Traffic** 的趋势和速度，查看是否存在网络阻塞。\n- 检查是否存在错误的 JDBC 配置参数导致的应用 OOM。例如流式读取的相关参数 `defaultFetchSize` 配置有误，会造成数据在客户端大量缓存。\n\n## 处理 OOM 问题需要收集的诊断信息\n\n为定位 OOM 故障，通常需要收集以下信息：\n\n- 操作系统的内存相关配置：\n    - TiUP 上的配置：`resource_control.memory_limit`\n    - 操作系统的配置：\n        - 内存信息：`cat /proc/meminfo`\n        - 相关内核参数：`vm.overcommit_memory`\n    - NUMA 相关信息：\n        - `numactl --hardware`\n        - `numactl --show`\n\n- 数据库的版本和内存相关配置：\n    - TiDB 版本\n    - `tidb_mem_quota_query`\n    - `memory-usage-alarm-ratio`\n    - `mem-quota-query`\n    - `oom-action`\n    - `tidb_enable_rate_limit_action`\n    - `tidb_server_memory_limit`\n    - `oom-use-tmp-storage`\n    - `tmp-storage-path`\n    - `tmp-storage-quota`\n    - `tidb_analyze_version`\n\n- 在 Grafana 查看 TiDB 内存的日常使用情况：**TiDB** > **Server** > **Memory Usage**\n\n- 查看内存消耗较多的 SQL 语句：\n\n    - 可以从 TiDB Dashboard 中查看 SQL 语句分析、慢查询，查看内存使用量\n    - 查看 `INFORMATION_SCHEMA` 中的 `SLOW_QUERY` 和 `CLUSTER_SLOW_QUERY`\n    - 各个 TiDB 节点的 `tidb_slow_query.log`\n    - 执行 `grep \"expensive_query\" tidb.log` 查看对应的日志条目\n    - 执行 `EXPLAIN ANALYZE` 查看算子的内存消耗\n    - 执行 `SELECT * FROM information_schema.processlist;` 查看 SQL 对应的 `MEM` 列的值\n\n- 执行以下命令收集内存使用率高的时候 TiDB 的 Profile 信息：\n\n    ```shell\n    curl -G \"http://{TiDBIP}:10080/debug/zip?seconds=10\" > profile.zip\n    ```\n\n- 执行 `grep \"tidb-server has the risk of OOM\" tidb.log` 查看 TiDB Server 收集的告警文件路径，例如：\n\n    ```shell\n    [\"tidb-server has the risk of OOM because of memory usage exceeds alarm ratio. Running SQLs and heap profile will be recorded in record path\"] [\"is tidb_server_memory_limit set\"=false] [\"system memory total\"=14388137984] [\"system memory usage\"=11897434112] [\"tidb-server memory usage\"=11223572312] [memory-usage-alarm-ratio=0.8] [\"record path\"=\"/tmp/0_tidb/MC4wLjAuMDo0MDAwLzAuMC4wLjA6MTAwODA=/tmp-storage/record\"]\n    ```\n\n## 探索更多\n\n- [TiDB 内存调优](/configure-memory-usage.md)\n- [TiKV 内存调优](/tune-tikv-memory-performance.md)\n"
        },
        {
          "name": "troubleshoot-write-conflicts.md",
          "type": "blob",
          "size": 5.615234375,
          "content": "---\ntitle: 乐观事务模型下写写冲突问题排查\nsummary: 介绍 TiDB 中乐观锁下写写冲突出现的原因以及解决方案。\naliases: ['/docs-cn/dev/troubleshoot-write-conflicts/']\n---\n\n# 乐观事务模型下写写冲突问题排查\n\n本文介绍 TiDB 中乐观锁下写写冲突出现的原因以及解决方案。\n\n在 v3.0.8 版本之前，TiDB 默认采用乐观事务模型，在事务执行过程中并不会做冲突检测，而是在事务最终 COMMIT 提交时触发两阶段提交，并检测是否存在写写冲突。当出现写写冲突，并且开启了事务重试机制，则 TiDB 会在限定次数内进行重试，最终重试成功或者达到重试次数上限后，会给客户端返回结果。因此，如果 TiDB 集群中存在大量的写写冲突情况，容易导致集群的 Duration 比较高。\n\n## 出现写写冲突的原因\n\nTiDB 中使用 [Percolator](https://www.usenix.org/legacy/event/osdi10/tech/full_papers/Peng.pdf) 事务模型来实现 TiDB 中的事务。Percolator 总体上就是一个二阶段提交的实现。具体的二阶段提交过程可参考[乐观事务文档](/optimistic-transaction.md)。\n\n当客户端发起 `COMMIT` 请求的时候，TiDB 开始两阶段提交：\n\n1. TiDB 从所有要写入的 Key 中选择一个作为当前事务的 Primary Key\n2. TiDB 向所有的本次提交涉及到的 TiKV 发起 prewrite 请求，TiKV 判断是否所有 Key 都可以 prewrite 成功\n3. TiDB 收到所有 Key 都 prewrite 成功的消息\n4. TiDB 向 PD 请求 commit_ts\n5. TiDB 向 Primary Key 发起第二阶段提交。Primary Key 所在的 TiKV 收到 commit 操作后，检查数据合法性，清理 prewrite 阶段留下的锁\n6. TiDB 收到两阶段提交成功的信息\n\n写写冲突发生在 prewrite 阶段，当发现有其他的事务在写当前 Key (data.commit_ts > txn.start_ts)，则会发生写写冲突。\n\nTiDB 会根据 `tidb_disable_txn_auto_retry` 和 `tidb_retry_limit` 参数设置的情况决定是否进行重试，如果设置了不重试，或者重试次数达到上限后还是没有 prewrite 成功，则向 TiDB 返回 `Write Conflict` 错误。\n\n## 如何判断当前集群存在写写冲突\n\n可以通过 Grafana 监控查看集群写写冲突的情况：\n\n* 通过 TiDB 监控面板中 KV Errors 监控栏中 KV Backoff OPS 监控指标项，查看 TiKV 中返回错误信息的数量\n\n    ![kv-backoff-ops](/media/troubleshooting-write-conflict-kv-backoff-ops.png)\n\n    txnlock 表示集群中存在写写冲突，txnLockFast 表示集群中存在读写冲突。\n\n* 通过 TiDB 监控面板中 KV Errors 监控栏中 Lock Resolve OPS 监控指标项，查看事务冲突相关的数量\n\n    ![lock-resolve-ops](/media/troubleshooting-write-conflict-lock-resolve-ops.png)\n\n    expired、not_expired、wait_expired 表示对应的 lock 状态\n\n* 通过 TiDB 监控面板中 KV Errors 监控栏中 KV Retry Duration 监控指标项，查看 KV 重试请求的时间\n\n    ![kv-retry-duration](/media/troubleshooting-write-conflict-kv-retry-duration.png)\n\n也可以通过 TiDB 日志查看是否有 `[kv:9007]Write conflict` 关键字，如果搜索到对应关键字，则可以表明集群中存在写写冲突。\n\n## 如何解决写写冲突问题\n\n如果通过以上方式判断出集群中存在大量的写写冲突，建议找到冲突的数据，以及写写冲突的原因，看是否能从应用程序修改逻辑，加上重试的逻辑。当出现写写冲突的时候，可以在 TiDB 日志中看到类似的日志：\n\n```log\n[2020/05/12 15:17:01.568 +08:00] [WARN] [session.go:446] [\"commit failed\"] [conn=3] [\"finished txn\"=\"Txn{state=invalid}\"] [error=\"[kv:9007]Write conflict, txnStartTS=416617006551793665, conflictStartTS=416617018650001409, conflictCommitTS=416617023093080065, key={tableID=47, indexID=1, indexValues={string, }} primary={tableID=47, indexID=1, indexValues={string, }} [try again later]\"]\n```\n\n关于日志的解释如下：\n\n* `[kv:9007]Write conflict`：表示出现了写写冲突\n* `txnStartTS=416617006551793665`：表示当前事务的 start_ts 时间戳，可以通过 pd-ctl 工具将时间戳转换为具体时间\n* `conflictStartTS=416617018650001409`：表示冲突事务的 start_ts 时间戳，可以通过 pd-ctl 工具将时间戳转换为具体时间\n* `conflictCommitTS=416617023093080065`：表示冲突事务的 commit_ts 时间戳，可以通过 pd-ctl 工具将时间戳转换为具体时间\n* `key={tableID=47, indexID=1, indexValues={string, }}`：表示当前事务中冲突的数据，tableID 表示发生冲突的表的 ID，indexID 表示是索引数据发生了冲突。如果是数据发生了冲突，会打印 `handle=x` 表示对应哪行数据发生了冲突，indexValues 表示发生冲突的索引数据\n* `primary={tableID=47, indexID=1, indexValues={string, }}`：表示当前事务中的 Primary Key 信息\n\n通过 pd-ctl 将时间戳转换为可读时间：\n\n{{< copyable \"\" >}}\n\n```shell\ntiup ctl:v<CLUSTER_VERSION> pd -u https://127.0.0.1:2379 tso {TIMESTAMP}\n```\n\n通过 tableID 查找具体的表名：\n\n{{< copyable \"\" >}}\n\n```shell\ncurl http://{TiDBIP}:10080/db-table/{tableID}\n```\n\n通过 indexID 查找具体的索引名：\n\n{{< copyable \"sql\" >}}\n\n```sql\nSELECT * FROM INFORMATION_SCHEMA.TIDB_INDEXES WHERE TABLE_SCHEMA='{db_name}' AND TABLE_NAME='{table_name}' AND INDEX_ID={indexID};\n```\n\n另外在 v3.0.8 及之后版本默认使用悲观事务模式，从而避免在事务提交的时候因为冲突而导致失败，无需修改应用程序。悲观事务模式下会在每个 DML 语句执行的时候，加上悲观锁，用于防止其他事务修改相同 Key，从而保证在最后提交的 prewrite 阶段不会出现写写冲突的情况。\n"
        },
        {
          "name": "tso-configuration-file.md",
          "type": "blob",
          "size": 3.751953125,
          "content": "---\ntitle: TSO 配置文件描述\nsummary: TSO 配置文件包含了多个配置项，如节点名称、数据路径、节点 URL 等。\n---\n\n# TSO 配置文件描述\n\n<!-- markdownlint-disable MD001 -->\n\nTSO 节点用于提供 PD 的 `tso` 微服务。本文档仅在 PD 开启微服务模式下适用。\n\n> **Tip:**\n>\n> 如果你需要调整配置项的值，请参考[修改配置参数](/maintain-tidb-using-tiup.md#修改配置参数)进行操作。\n\n### `name`\n\n- TSO 节点名称。\n- 默认值：`\"TSO\"`\n- 如果你需要启动多个 TSO 节点，请确保不同的 TSO 节点使用不同的名字。\n\n### `data-dir`\n\n- TSO 节点上的数据存储路径。\n- 默认值：`\"default.${name}\"`\n\n### `listen-addr`\n\n- TSO 节点监听的客户端 URL。\n- 默认值：`\"http://127.0.0.1:3379\"`\n- 部署集群时，`listen-addr` 必须指定当前主机的 IP 地址，例如 `\"http://192.168.100.113:3379\"`。如果运行在 Docker 中，则需要指定为 `\"http://0.0.0.0:3379\"`。\n\n### `advertise-listen-addr`\n\n- 用于外部访问 TSO 节点的 URL。\n- 默认值：`\"${listen-addr}\"`\n- 在某些情况下，例如 Docker 或者 NAT 网络环境，客户端并不能通过 TSO 节点自己监听的地址来访问 TSO 节点。此时，你可以设置 `advertise-listen-addr` 来让客户端访问。\n- 例如，Docker 内部 IP 地址为 `172.17.0.1`，而宿主机的 IP 地址为 `192.168.100.113` 并且设置了端口映射 `-p 3379:3379`，那么可以设置 `advertise-listen-addr=\"http://192.168.100.113:3379\"`，然后客户端就可以通过 `http://192.168.100.113:3379` 来找到这个服务。\n\n### `backend-endpoints`\n\n- TSO 节点监听其他 TSO 节点的 URL 列表。\n- 默认值：`\"http://127.0.0.1:2379\"`\n\n### `lease`\n\n- TSO Primary Key 租约超时时间，超时系统重新选举 Primary。\n- 默认值：3\n- 单位：秒\n\n### `tso-update-physical-interval`\n\n- TSO 物理时钟更新周期。\n- 在默认的一个 TSO 物理时钟更新周期内 (50ms)，TSO server 最多提供 262144 个 TSO。如果需要更多的 TSO，可以将这个参数调小。最小值为 `1ms`。\n- 缩短这个参数会增加 TSO server 的 CPU 消耗。根据测试，相比 `50ms` 更新周期，更新周期为 `1ms` 时，TSO server 的 CPU 占用率 ([CPU usage](https://man7.org/linux/man-pages/man1/top.1.html)) 将增加约 10%。\n- 默认值：50ms\n- 最小值：1ms\n\n## security\n\n安全相关配置项。\n\n### `cacert-path`\n\n- CA 文件路径\n- 默认值：\"\"\n\n### `cert-path`\n\n- 包含 X.509 证书的 PEM 文件路径\n- 默认值：\"\"\n\n### `key-path`\n\n- 包含 X.509 key 的 PEM 文件路径\n- 默认值：\"\"\n\n### `redact-info-log`\n\n- 控制 TSO 节点日志脱敏的开关\n- 该配置项值设为 true 时将对 TSO 节点的日志脱敏，遮蔽日志中的用户信息。\n- 默认值：false\n\n## log\n\n日志相关的配置项。\n\n### `level`\n\n- 指定日志的输出级别。\n- 可选值：\"debug\"，\"info\"，\"warn\"，\"error\"，\"fatal\"\n- 默认值：\"info\"\n\n### `format`\n\n- 日志格式。\n- 可选值：\"text\"，\"json\"\n- 默认值：\"text\"\n\n### `disable-timestamp`\n\n- 是否禁用日志中自动生成的时间戳。\n- 默认值：false\n\n## log.file\n\n日志文件相关的配置项。\n\n### `max-size`\n\n- 单个日志文件最大大小，超过该值系统自动切分成多个文件。\n- 默认值：300\n- 单位：MiB\n- 最小值为 1\n\n### `max-days`\n\n- 日志保留的最长天数。\n- 如果未设置本参数或把本参数设置为默认值 `0`，TSO 节点不清理日志文件。\n- 默认：0\n\n### `max-backups`\n\n- 日志文件保留的最大个数。\n- 如果未设置本参数或把本参数设置为默认值 `0`，TSO 节点会保留所有的日志文件。\n- 默认：0\n\n## metric\n\n监控相关的配置项。\n\n### `interval`\n\n- 向 Prometheus 推送监控指标数据的间隔时间。\n- 默认：15s\n"
        },
        {
          "name": "tso.md",
          "type": "blob",
          "size": 4.1689453125,
          "content": "---\ntitle: TiDB 中的 TimeStamp Oracle (TSO)\nsummary: 了解 TiDB 中的 TimeStamp Oracle (TSO)。\n---\n\n# TiDB 中的 TimeStamp Oracle (TSO)\n\n在 TiDB 中，Placement Driver (PD) 承担着 TSO 时间戳分配器的角色，负责为集群内各组件分配时间戳。这些时间戳用于为事务和数据分配时间标记。该分配机制对于在 TiDB 中启用 [Percolator](https://research.google.com/pubs/pub36726.html) 模型至关重要。Percolator 模型用于支持多版本并发控制（Multi-Version Concurrency Control, MVCC）和[事务管理](/transaction-overview.md)。\n\n下面示例显示了如何获取 TiDB 当前的 TSO：\n\n```sql\nBEGIN; SET @ts := @@tidb_current_ts; ROLLBACK;\nQuery OK, 0 rows affected (0.0007 sec)\nQuery OK, 0 rows affected (0.0002 sec)\nQuery OK, 0 rows affected (0.0001 sec)\n\nSELECT @ts;\n+--------------------+\n| @ts                |\n+--------------------+\n| 443852055297916932 |\n+--------------------+\n1 row in set (0.00 sec)\n```\n\n注意由于 TSO 时间戳是按事务分配的，所以需要从包含 `BEGIN; ...; ROLLBACK` 的事务中获取时间戳。\n\n从上例中得到的 TSO 时间戳是一个十进制数。你可以使用以下 SQL 函数来解析时间戳：\n\n- [`TIDB_PARSE_TSO()`](/functions-and-operators/tidb-functions.md#tidb_parse_tso)\n- [`TIDB_PARSE_TSO_LOGICAL()`](/functions-and-operators/tidb-functions.md)\n\n```sql\nSELECT TIDB_PARSE_TSO(443852055297916932);\n+------------------------------------+\n| TIDB_PARSE_TSO(443852055297916932) |\n+------------------------------------+\n| 2023-08-27 20:33:41.687000         |\n+------------------------------------+\n1 row in set (0.00 sec)\n```\n\n```sql\nSELECT TIDB_PARSE_TSO_LOGICAL(443852055297916932);\n+--------------------------------------------+\n| TIDB_PARSE_TSO_LOGICAL(443852055297916932) |\n+--------------------------------------------+\n|                                          4 |\n+--------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n下面示例展示了 TSO 时间戳的二进制细节：\n\n```shell\n0000011000101000111000010001011110111000110111000000000000000100  ← 该值是二进制形式的 443852055297916932\n0000011000101000111000010001011110111000110111                    ← 前 46 位是物理时间戳\n                                              000000000000000100  ← 后 18 位是逻辑时间戳\n```\n\nTSO 时间戳由两部分组成：\n\n- 物理时间戳：自 1970 年 1 月 1 日以来的 UNIX 时间戳，单位为毫秒。\n- 逻辑时间戳：递增计数器，用于需要在一毫秒内使用多个时间戳的情况，或某些事件可能触发时钟进程逆转的情况。在这些情况下，物理时间戳会保持不变，而逻辑时间戳保持递增。该机制可以确保 TSO 时间戳的完整性，确保时间戳始终递增而不会倒退。\n\n你可以通过 SQL 语句更深入地查看 TSO 时间戳，示例如下：\n\n```sql\nSELECT @ts, UNIX_TIMESTAMP(NOW(6)), (@ts >> 18)/1000, FROM_UNIXTIME((@ts >> 18)/1000), NOW(6), @ts & 0x3FFFF\\G\n*************************** 1. row ***************************\n                            @ts: 443852055297916932\n         UNIX_TIMESTAMP(NOW(6)): 1693161835.502954\n               (@ts >> 18)/1000: 1693161221.6870\nFROM_UNIXTIME((@ts >> 18)/1000): 2023-08-27 20:33:41.6870\n                         NOW(6): 2023-08-27 20:43:55.502954\n                  @ts & 0x3FFFF: 4\n1 row in set (0.00 sec)\n```\n\n`>> 18` 操作表示按位[右移](/functions-and-operators/bit-functions-and-operators.md#右移) 18 位，用于提取物理时间戳。由于物理时间戳以毫秒为单位，与更常见的以秒为单位的 UNIX 时间戳格式不同，因此需要除以 1000 将其转换为与 [`FROM_UNIXTIME()`](/functions-and-operators/date-and-time-functions.md) 兼容的格式。这个转换过程与 `TIDB_PARSE_TSO()` 的功能一致。\n\n你还可以将二进制中的逻辑时间戳 `000000000000000100`（即十进制中的 `4`）提取出来。\n\n你也可以通过 CLI 工具解析时间戳，命令如下：\n\n```shell\n$ tiup ctl:v7.1.0 pd tso 443852055297916932\n```\n\n```\nsystem:  2023-08-27 20:33:41.687 +0200 CEST\nlogic:   4\n```\n\n可以看到，物理时间戳在以 `system:` 开头的行中，逻辑时间戳在以 `logic:` 开头的行中。\n"
        },
        {
          "name": "tune-operating-system.md",
          "type": "blob",
          "size": 9.7529296875,
          "content": "---\ntitle: 操作系统性能参数调优\nsummary: 了解如何进行 CentOS 7 系统的性能调优。\naliases: ['/docs-cn/dev/tune-operating-system/']\n---\n\n# 操作系统性能参数调优\n\n本文档仅用于描述如何优化 CentOS 7 的各个子系统。\n\n> **注意：**\n>\n> + CentOS 7 操作系统的默认配置适用于中等负载下运行的大多数服务。调整特定子系统的性能可能会对其他子系统产生负面影响。因此在调整系统之前，请备份所有用户数据和配置信息；\n> + 请在测试环境下对所有修改做好充分测试后，再应用到生产环境中。\n\n## 性能分析工具\n\n系统调优需要根据系统性能分析的结果做指导，因此本文先列出常用的性能分析方法。\n\n### 60 秒分析法\n\n[60 秒分析法](http://www.brendangregg.com/Articles/Netflix_Linux_Perf_Analysis_60s.pdf)由《性能之巅》的作者 Brendan Gregg 及其所在的 Netflix 性能工程团队公布。所用到的工具均可从发行版的官方源获取，通过分析以下清单中的输出，可定位大部分常见的性能问题。\n\n+ `uptime`\n+ `dmesg | tail`\n+ `vmstat 1`\n+ `mpstat -P ALL 1`\n+ `pidstat 1`\n+ `iostat -xz 1`\n+ `free -m`\n+ `sar -n DEV 1`\n+ `sar -n TCP,ETCP 1`\n+ `top`\n\n具体用法可查询相应 `man` 手册。\n\n### perf\n\nperf 是 Linux 内核提供的一个重要的性能分析工具，它涵盖硬件级别（CPU/PMU 和性能监视单元）功能和软件功能（软件计数器和跟踪点）。详细用法请参考 [perf Examples](http://www.brendangregg.com/perf.html#Background)。\n\n### BCC/bpftrace\n\nCentOS 从 7.6 版本起，内核已实现对 BPF (Berkeley Packet Filter) 的支持，因此可根据[上述清单](#60-秒分析法)的结果，选取适当的工具进行深入分析。相比 perf/ftrace，BPF 提供了可编程能力和更小的性能开销。相比 kprobe，BPF 提供了更高的安全性，更适合在生产环境上使用。关于 BCC 工具集的使用请参考 [BPF Compiler Collection (BCC)](https://github.com/iovisor/bcc/blob/master/README.md)。\n\n## 性能调优\n\n性能调优将根据内核子系统进行分类描述。\n\n### 处理器——动态节能技术\n\ncpufreq 是一个动态调整 CPU 频率的模块，可支持五种模式。为保证服务性能应选用 performance 模式，将 CPU 频率固定工作在其支持的最高运行频率上，不进行动态调节，操作命令为 `cpupower frequency-set --governor performance`。\n\n### 处理器——中断亲和性\n\n- 自动平衡：可通过 `irqbalance` 服务实现。\n- 手动平衡：\n    - 确定需要平衡中断的设备，从 CentOS 7.5 开始，系统会自动为某些设备及其驱动程序配置最佳的中断关联性。不能再手动配置其亲和性。目前已知的有使用 `be2iscsi` 驱动的设备，以及 NVMe 设置；\n    - 对于其他设备，可查询其芯片手册，是否支持分发中断。\n        - 若不支持，则该设备的所有中断会路由到同一个 CPU 上，无法对其进行修改。\n        - 若支持，则计算 `smp_affinity` 掩码并设置对应的配置文件，具体请参考[内核 IRQ-affinity 文档](https://www.kernel.org/doc/Documentation/IRQ-affinity.txt)。\n\n### NUMA 绑核\n\n为尽可能的避免跨 NUMA (Non-Uniform Memory Access) 访问内存，可以通过设置线程的 CPU 亲和性来实现 NUMA 绑核。对于普通程序，可使用 `numactl` 命令来绑定，具体用法请查询 `man` 手册。对于网卡中断，请参考下文[网络](#网络)章节。\n\n### 内存——透明大页\n\n对于数据库应用，**不推荐**使用 THP，因为数据库往往具有稀疏而不是连续的内存访问模式，且当高阶内存碎片化比较严重时，分配 THP 页面会出现较大的延迟。若开启针对 THP 的直接内存规整功能，也会出现系统 CPU 使用率激增的现象，因此建议关闭 THP。\n\n```shell\necho never > /sys/kernel/mm/transparent_hugepage/enabled\necho never > /sys/kernel/mm/transparent_hugepage/defrag\n```\n\n### 内存——虚拟内存参数\n\n- `dirty_ratio` 百分比值。当脏的 page cache 总量达到系统内存总量的这一百分比后，系统将开始使用 `pdflush` 操作将脏的 page cache 写入磁盘。默认值为 20％，通常不需调整。对于高性能 SSD，比如 NVMe 设备来说，降低其值有利于提高内存回收时的效率。\n- `dirty_background_ratio` 百分比值。当脏的 page cache 总量达到系统内存总量的这一百分比后，系统开始在后台将脏的 page cache 写入磁盘。默认值为 10％，通常不需调整。对于高性能 SSD，比如 NVMe 设备来说，设置较低的值有利于提高内存回收时的效率。\n\n### 存储及文件系统\n\n内核 I/O 栈链路较长，包含了文件系统层、块设备层和驱动层。\n\n#### I/O 调度器\n\nI/O 调度程序确定 I/O 操作何时在存储设备上运行以及持续多长时间。也称为 I/O 升降机。对于 SSD 设备，宜设置为 `noop`。\n\n```shell\necho noop > /sys/block/${SSD_DEV_NAME}/queue/scheduler\n```\n\n#### 格式化参数——块大小\n\n块 (block) 是文件系统的工作单元。块大小决定了单个块中可以存储多少数据，因此决定了一次写入或读取的最小数据量。\n\n默认块大小适用于大多数使用情况。但是，如果块大小（或多个块的大小）与通常一次读取或写入的数据量相同或稍大，则文件系统将性能更好，数据存储效率更高。小文件仍将使用整个块。文件可以分布在多个块中，但这会增加运行时开销。\n\n使用 `mkfs` 命令格式化设备时，将块大小指定为文件系统选项的一部分。指定块大小的参数随文件系统的不同而不同。有关详细信息，请查询对应文件系统的 `mkfs` 手册页，比如 `man mkfs.ext4`。\n\n#### 挂载参数\n\n`noatime` 读取文件时，将禁用对元数据的更新。它还启用了 nodiratime 行为，该行为会在读取目录时禁用对元数据的更新。\n\n### 网络\n\n网络子系统由具有敏感连接的许多不同部分组成。因此，CentOS 7 网络子系统旨在为大多数工作负载提供最佳性能，并自动优化其性能。因此，通常无需手动调整网络性能。\n\n网络问题通常由硬件或相关设施出现问题导致的，因此在调优协议栈前，请先排除硬件问题。\n\n尽管网络堆栈在很大程度上是自我优化的。但是在网络数据包处理过程中，以下方面可能会成为瓶颈并降低性能：\n\n- 网卡硬件缓存：正确观察硬件层面的丢包方法是使用 `ethtool -S ${NIC_DEV_NAME}` 命令观察 `drops` 字段。当出现丢包现象时，主要考虑是硬/软中断的处理速度跟不上网卡接收速度。若接收缓存小于最大限制时，也可尝试增加 RX 缓存来防止丢包。查询命令为：`ethtool -g ${NIC_DEV_NAME}`，修改命令为 `ethtool -G ${NIC_DEV_NAME}`。\n- 硬中断：若网卡支持 Receive-Side Scaling（RSS 也称为多网卡接收）功能，则观察 `/proc/interrupts` 网卡中断，如果出现了中断不均衡的情况，请参考[处理器动态节能技术](#处理器动态节能技术)，[处理器调优](#处理器中断亲和性)，[NUMA 绑核](#numa-绑核)。若不支持 RSS 或 RSS 数量远小于物理 CPU 核数，则可配置 Receive Packet Steering（RPS，可以看作 RSS 的软件实现），及 RPS 的扩展 Receive Flow Steering (RFS)。具体设置请参考[内核文档](https://www.kernel.org/doc/Documentation/networking/scaling.txt)。\n- 软中断：观察 `/proc/net/softnet_stat` 监控。如果除第三列的其他列的数值在增长，则应适度调大 `net.core.netdev_budget` 或 `net.core.dev_weight` 值，使 `softirq` 可以获得更多的 CPU 时间。除此之外，也需要检查 CPU 使用情况，确定哪些任务在频繁占用 CPU，能否优化。\n- 应用的套接字接收队列：监控 `ss -nmp` 的 `Recv-q` 列，若队列已满，则应考虑增大应用程序套接字的缓存大小或使用自动调整缓存的方式。除此之外，也要考虑能否优化应用层的架构，降低读取套接字的间隔。\n- 以太网流控：若网卡和交换机支持流控功能，可通过使能此功能，给内核一些时间来处理网卡队列中的数据，来规避网卡缓存溢出的问题。对于网卡测，可通过 `ethtool -a ${NIC_DEV_NAME}` 命令检查是否支持/使能，并通过 `ethtool -A ${NIC_DEV_NAME}` 命令开启。对于交换机，请查询其手册。\n- 中断合并：过于频繁的硬件中断会降低系统性能，而过晚的硬件中断会导致丢包。对于较新的网卡支持中断合并功能，并允许驱动自动调节硬件中断数。可通过 `ethtool -c ${NIC_DEV_NAME}` 命令检查，`ethtool -C ${NIC_DEV_NAME}` 命令开启。自适应模式使网卡可以自动调节中断合并。在自适应模式下，驱动程序将检查流量模式和内核接收模式，并实时评估合并设置，以防止数据包丢失。不同品牌的网卡具有不同的功能和默认配置，具体请参考网卡手册。\n- 适配器队列：在协议栈处理之前，内核利用此队列缓存网卡接收的数据，每个 CPU 都有各自的 backlog 队列。此队列可缓存的最大 packets 数量为 `netdev_max_backlog`。观察 `/proc/net/softnet_stat` 第二列，当某行的第二列持续增加，则意味着 CPU [row-1] 队列已满，数据包被丢失，可通过持续加倍 `net.core.netdev_max_backlog` 值来解决。\n- 发送队列：发送队列长度值确定在发送之前可以排队的数据包数量。默认值是 1000，对于 10 Gbps 足够。但若从 `ip -s link` 的输出中观察到 `TX errors` 值时，可尝试加倍该数据包数量：`ip link set dev ${NIC_DEV_NAME} txqueuelen 2000`。\n- 驱动：网卡驱动通常也会提供调优参数，请查询设备硬件手册及其驱动文档。\n"
        },
        {
          "name": "tune-region-performance.md",
          "type": "blob",
          "size": 4.51953125,
          "content": "---\ntitle: Region 性能调优\nsummary: 了解如何通过调整 Region 大小等方法对 Region 进行性能调优以及如何在大 Region 下使用 bucket 进行并发查询优化。\n---\n\n# Region 性能调优\n\n本文介绍了如何通过调整 Region 大小等方法对 Region 进行性能调优以及如何在大 Region 下使用 bucket 进行并发查询优化。同时，本文还介绍了通过开启 Active PD Follower 特性来提升 PD 为 TiDB 节点提供 Region 信息的服务能力。\n\n## 概述\n\nTiKV 自动将底层数据进行[分片](/best-practices/tidb-best-practices.md#数据分片)，所有数据按照 key 的范围划分为若干个 Region。当某个 Region 的大小超过一定限制后，TiKV 会将它分裂为多个 Region。\n\n在大量数据的场景下，如果 Region 较小，可能会出现 Region 数量过多的情况，从而带来更多的资源开销和导致[性能回退](/best-practices/massive-regions-best-practices.md#性能问题)的问题。\n\n> **说明：**\n>\n> - 在 v6.1.0 中，TiDB 支持设置自定义的 Region 大小，该特性为实验特性。\n> - 从 v6.5.0 开始，该特性成为正式功能（GA）。\n> - 从 v8.4.0 开始，Region 默认的大小从 96 MiB 调整为 256 MiB，将其调大可以减少 Region 个数。\n\n开启 [Hibernate Region](/best-practices/massive-regions-best-practices.md#方法四开启-hibernate-region-功能) 或 [`Region Merge`](/best-practices/massive-regions-best-practices.md#方法五开启-region-merge) 也可以减少过多 Region 带来的性能开销。\n\n## 使用 `region-split-size` 调整 Region 大小\n\n> **注意：**\n>\n> Region 大小的推荐范围为 [48 MiB, 256 MiB]，常用的大小包括 96 MiB、128 MiB、256 MiB。不推荐将 Region 大小设置超过 1 GiB，强烈建议不超过 10 GiB。过大的 Region 可能带来以下影响：\n>\n> + 性能抖动。\n> + 查询性能回退，尤其是大范围数据查询的性能会有回退。\n> + 调度变慢。\n\nRegion 的大小可以通过 [`coprocessor.region-split-size`](/tikv-configuration-file.md#region-split-size) 进行设置。如果你使用了 TiFlash 或 Dumpling 工具，则 Region 大小不能超过 1 GiB。Region 调大以后，使用 Dumpling 工具时，需要降低并发，否则 TiDB 会有 OOM 的风险。\n\n## 使用 bucket 增加并发\n\n> **警告：**\n>\n> 当前该功能为实验特性，不建议在生产环境中使用。\n\nRegion 调大以后，如需进一步提高查询的并发度，可以设置 [`coprocessor.enable-region-bucket`](/tikv-configuration-file.md#enable-region-bucket-从-v610-版本开始引入) 为 `true`。这个配置会将每个 Region 划分为更小的区间 bucket，并且以这个更小的区间作为并发查询单位，以提高扫描数据的并发度。bucket 的大小通过 [`coprocessor.region-bucket-size`](/tikv-configuration-file.md#region-bucket-size-从-v610-版本开始引入) 来控制。\n\n## 通过 Active PD Follower 提升 PD Region 信息查询服务的扩展能力\n\n当集群的 Region 数量较多时，PD leader 处理心跳和调度任务的开销也较大，可能导致 CPU 资源紧张。如果同时集群中的 TiDB 实例数量较多，查询 Region 信息请求并发量较大，PD leader CPU 压力将变得更大，可能会造成 PD 服务不可用。\n\n为确保服务的高可用性，PD leader 会将 Region 信息实时同步给 PD follower。PD follower 在内存中维护保存 Region 信息，从而具备处理 Region 信息请求的能力。你可以通过设置系统变量 [`pd_enable_follower_handle_region`](/system-variables.md#pd_enable_follower_handle_region-从-v760-版本开始引入) 开启 Active PD Follower 特性。启用该特性后，TiDB 在获取 Region 信息时会将请求均匀地发送到所有 PD 节点上，使 PD follower 也可以直接处理 Region 请求，从而降低减轻 PD leader 的 CPU 压力。\n\nPD 通过维护 Region 同步流的状态，并结合 TiKV client-go 的 fallback 机制，确保 TiDB 中的 Region 信息始终是最新的。\n\n- 当 PD leader 与 follower 之间网络不稳定或 follower 不可用导致 Region 同步流断开时，PD follower 将拒绝处理 Region 请求。此时，TiDB 会自动向 PD leader 重试请求，并将该 follower 暂时标记为不可用状态。\n- 当网络稳定时，由于 PD leader 和 follower 之间的同步可能存在延迟，从 follower 获取的部分 Region 信息可能是过时的。在这种情况下，如果 Region 对应的 KV Request 失败，TiDB 会重新向 PD leader 请求最新的 Region 信息，并再次向 TiKV 发送 KV Request。"
        },
        {
          "name": "tune-tikv-memory-performance.md",
          "type": "blob",
          "size": 13.9296875,
          "content": "---\ntitle: TiKV 内存参数性能调优\naliases: ['/docs-cn/dev/tune-tikv-memory-performance/','/docs-cn/dev/reference/performance/tune-tikv/', '/docs-cn/dev/tune-tikv-performance/']\nsummary: TiKV 内存参数性能调优，根据机器配置情况调整参数以达到最佳性能。TiKV 使用 RocksDB 作为持久化存储，配置项包括 block-cache 大小和 write-buffer 大小。除此之外，系统内存还会被用于 page cache 和处理大查询时的数据结构生成。推荐将 TiKV 部署在 CPU 核数不低于 8 或内存不低于 32GiB 的机器上，对写入吞吐要求高时使用吞吐能力较好的磁盘，对读写延迟要求高时使用 IOPS 较高的 SSD 盘。\n---\n\n# TiKV 内存参数性能调优\n\n本文档用于描述如何根据机器配置情况来调整 TiKV 的参数，使 TiKV 的性能达到最优。你可以在 [etc/config-template.toml](https://github.com/tikv/tikv/blob/master/etc/config-template.toml) 找到配置文件模版，参考[使用 TiUP 修改配置参数](/maintain-tidb-using-tiup.md#修改配置参数)进行操作，部分配置项可以通过[在线修改 TiKV 配置](/dynamic-config.md#在线修改-tikv-配置)方式在线更新。具体配置项的含义可参考 [TiKV 配置文件描述](/tikv-configuration-file.md)。\n\nTiKV 最底层使用的是 RocksDB 做为持久化存储，所以 TiKV 的很多性能相关的参数都是与 RocksDB 相关的。TiKV 使用了两个 RocksDB 实例，默认 RocksDB 实例存储 KV 数据，Raft RocksDB 实例（简称 RaftDB）存储 Raft 数据。\n\nTiKV 使用了 RocksDB 的 `Column Families` (CF) 特性。\n\n- 默认 RocksDB 实例将 KV 数据存储在内部的 `default`、`write` 和 `lock` 3 个 CF 内。\n\n    - `default` CF 存储的是真正的数据，与其对应的参数位于 `[rocksdb.defaultcf]` 项中；\n    - `write` CF 存储的是数据的版本信息 (MVCC) 以及索引相关的数据，相关的参数位于 `[rocksdb.writecf]` 项中；\n    - `lock` CF 存储的是锁信息，系统使用默认参数。\n\n- Raft RocksDB 实例存储 Raft log。\n\n    - `default` CF 主要存储的是 Raft log，与其对应的参数位于 `[raftdb.defaultcf]` 项中。\n\n所有的 CF 默认共同使用一个 block cache 实例。通过在 `[storage.block-cache]` 下设置 `capacity` 参数，你可以配置该 block cache 的大小。block cache 越大，能够缓存的热点数据越多，读取数据越容易，同时占用的系统内存也越多。\n\n> **注意：**\n>\n> 在 TiKV 3.0 之前的版本中，不支持使用 `shared block cache`，需要为每个 CF 单独配置 block cache。\n\n每个 CF 有各自的 `write-buffer`，大小通过 `write-buffer-size` 控制。\n\n## 参数说明\n\n```toml\n# 日志级别，可选值为：trace，debug，warn，error，info，off\nlog-level = \"info\"\n\n[server]\n# 监听地址\n# addr = \"127.0.0.1:20160\"\n\n# gRPC 线程池大小\n# grpc-concurrency = 4\n# TiKV 每个实例之间的 gRPC 连接数\n# grpc-raft-conn-num = 10\n\n# TiDB 过来的大部分读请求都会发送到 TiKV 的 Coprocessor 进行处理，该参数用于设置\n# coprocessor 线程的个数，如果业务是读请求比较多，增加 coprocessor 的线程数，但应比系统的\n# CPU 核数小。例如：TiKV 所在的机器有 32 core，在重读的场景下甚至可以将该参数设置为 30。在没有\n# 设置该参数的情况下，TiKV 会自动将该值设置为 CPU 总核数乘以 0.8。\n# end-point-concurrency = 8\n\n# 可以给 TiKV 实例打标签，用于副本的调度\n# labels = {zone = \"cn-east-1\", host = \"118\", disk = \"ssd\"}\n\n[storage]\n# 数据目录\n# data-dir = \"/tmp/tikv/store\"\n\n# 通常情况下使用默认值就可以了。在导数据的情况下建议将该参数设置为 1024000。\n# scheduler-concurrency = 102400\n# 该参数控制写入线程的个数，当写入操作比较频繁的时候，需要把该参数调大。使用 top -H -p tikv-pid\n# 发现名称为 sched-worker-pool 的线程都特别忙，这个时候就需要将 scheduler-worker-pool-size\n# 参数调大，增加写线程的个数。\n# scheduler-worker-pool-size = 4\n\n[storage.block-cache]\n## 是否为 RocksDB 的所有 CF 都创建一个 `shared block cache`。\n##\n## RocksDB 使用 block cache 来缓存未压缩的数据块。较大的 block cache 可以加快读取速度。\n## 推荐开启 `shared block cache` 参数。这样只需要设置全部缓存大小，使配置过程更加方便。\n## 在大多数情况下，可以通过 LRU 算法在各 CF 间自动平衡缓存用量。\n##\n## `storage.block-cache` 会话中的其余配置仅在开启 `shared block cache` 时起作用。\n## 从 v6.6.0 开始，该选项永远开启且无法关闭。\n# shared = true\n## `shared block cache` 的大小。正常情况下应设置为系统全部内存的 30%-50%。\n## 如果未设置该参数，则由以下字段或其默认值的总和决定。\n##\n##   * rocksdb.defaultcf.block-cache-size 或系统全部内存的 25%\n##   * rocksdb.writecf.block-cache-size 或系统全部内存的 15%\n##   * rocksdb.lockcf.block-cache-size 或系统全部内存的 2%\n##   * raftdb.defaultcf.block-cache-size 或系统全部内存的 2%\n##\n## 要在单个物理机上部署多个 TiKV 节点，需要显式配置该参数。\n## 否则，TiKV 中可能会出现 OOM 错误。\n# capacity = \"1GB\"\n\n[pd]\n# pd 的地址\n# endpoints = [\"127.0.0.1:2379\",\"127.0.0.2:2379\",\"127.0.0.3:2379\"]\n\n[metric]\n# 将 metrics 推送给 Prometheus pushgateway 的时间间隔\ninterval = \"15s\"\n# Prometheus pushgateway 的地址\naddress = \"\"\njob = \"tikv\"\n\n[raftstore]\n# Raft RocksDB 目录。默认值是 [storage.data-dir] 的 raft 子目录。\n# 如果机器上有多块磁盘，可以将 Raft RocksDB 的数据放在不同的盘上，提高 TiKV 的性能。\n# raftdb-path = \"/tmp/tikv/store/raft\"\n\n# 当 Region 写入的数据量超过该阈值的时候，TiKV 会检查该 Region 是否需要分裂。为了减少检查过程\n# 中扫描数据的成本，导入数据过程中可以将该值设置为 32 MB，正常运行状态下使用默认值即可。\nregion-split-check-diff = \"32MiB\"\n\n[coprocessor]\n\n## 当区间为 [a,e) 的 Region 的大小超过 `region_max_size`，TiKV 会尝试分裂该 Region，例如分裂成 [a,b)、[b,c)、[c,d)、[d,e) 等区间的 Region 后\n## 这些 Region [a,b), [b,c), [c,d) 的大小为 `region_split_size` (或者稍大于 `region_split_size`）\n# region-max-size = \"384MiB\"\n# region-split-size = \"256MiB\"\n\n[rocksdb]\n# RocksDB 进行后台任务的最大线程数，后台任务包括 compaction 和 flush。具体 RocksDB 为什么需要进行 compaction，\n# 请参考 RocksDB 的相关资料。在写流量比较大的时候（例如导数据），建议开启更多的线程，\n# 但应小于 CPU 的核数。例如在导数据的时候，32 核 CPU 的机器，可以设置成 28。\n# max-background-jobs = 8\n\n# RocksDB 能够打开的最大文件句柄数。\n# max-open-files = 40960\n\n# RocksDB MANIFEST 文件的大小限制.# 更详细的信息请参考：https://github.com/facebook/rocksdb/wiki/MANIFEST\nmax-manifest-file-size = \"20MiB\"\n\n# RocksDB write-ahead logs 目录。如果机器上有两块盘，可以将 RocksDB 的数据和 WAL 日志放在\n# 不同的盘上，提高 TiKV 的性能。\n# wal-dir = \"/tmp/tikv/store\"\n\n# 下面两个参数用于怎样处理 RocksDB 归档 WAL。\n# 更多详细信息请参考：https://github.com/facebook/rocksdb/wiki/How-to-persist-in-memory-RocksDB-database%3F\n# wal-ttl-seconds = 0\n# wal-size-limit = 0\n\n# RocksDB WAL 日志的最大总大小，通常情况下使用默认值就可以了。\n# max-total-wal-size = \"4GB\"\n\n# 开启 RocksDB compaction 过程中的预读功能，如果使用的是机械磁盘，建议该值至少为2MiB。\n# compaction-readahead-size = \"2MiB\"\n\n[rocksdb.defaultcf]\n# 数据块大小。RocksDB 是按照 block 为单元对数据进行压缩的，同时 block 也是缓存在 block-cache\n# 中的最小单元（类似其他数据库的 page 概念）。\nblock-size = \"64KB\"\n\n# RocksDB 每一层数据的压缩方式，可选的值为：no,snappy,zlib,bzip2,lz4,lz4hc,zstd。注意 Snappy 压缩文件必须遵循[官方 Snappy 格式](https://github.com/google/snappy)。不支持其他非官方压缩格式。\n# no:no:lz4:lz4:lz4:zstd:zstd 表示 level0 和 level1 不压缩，level2 到 level4 采用 lz4 压缩算法,\n# level5 和 level6 采用 zstd 压缩算法,。\n# no 表示没有压缩，lz4 是速度和压缩比较为中庸的压缩算法，zlib 的压缩比很高，对存储空间比较友\n# 好，但是压缩速度比较慢，压缩的时候需要占用较多的 CPU 资源。不同的机器需要根据 CPU 以及 I/O 资\n# 源情况来配置怎样的压缩方式。例如：如果采用的压缩方式为\"no:no:lz4:lz4:lz4:zstd:zstd\"，在大量\n# 写入数据的情况下（导数据），发现系统的 I/O 压力很大（使用 iostat 发现 %util 持续 100% 或者使\n# 用 top 命令发现 iowait 特别多），而 CPU 的资源还比较充裕，这个时候可以考虑将 level0 和\n# level1 开启压缩，用 CPU 资源换取 I/O 资源。如果采用的压缩方式\n# 为\"no:no:lz4:lz4:lz4:zstd:zstd\"，在大量写入数据的情况下，发现系统的 I/O 压力不大，但是 CPU\n# 资源已经吃光了，top -H 发现有大量的 bg 开头的线程（RocksDB 的 compaction 线程）在运行，这\n# 个时候可以考虑用 I/O 资源换取 CPU 资源，将压缩方式改成\"no:no:no:lz4:lz4:zstd:zstd\"。总之，目\n# 的是为了最大限度地利用系统的现有资源，使 TiKV 的性能在现有的资源情况下充分发挥。\ncompression-per-level = [\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]\n\n# RocksDB memtable 的大小。\nwrite-buffer-size = \"128MiB\"\n\n# 最多允许几个 memtable 存在。写入到 RocksDB 的数据首先会记录到 WAL 日志里面，然后会插入到\n# memtable 里面，当 memtable 的大小到达了 write-buffer-size 限定的大小的时候，当前的\n# memtable 会变成只读的，然后生成一个新的 memtable 接收新的写入。只读的 memtable 会被\n# RocksDB 的 flush 线程（max-background-flushes 参数能够控制 flush 线程的最大个数）\n# flush 到磁盘，成为 level0 的一个 sst 文件。当 flush 线程忙不过来，导致等待 flush 到磁盘的\n# memtable 的数量到达 max-write-buffer-number 限定的个数的时候，RocksDB 会将新的写入\n# stall 住，stall 是 RocksDB 的一种流控机制。在导数据的时候可以将 max-write-buffer-number\n# 的值设置的更大一点，例如 10。\nmax-write-buffer-number = 5\n\n# 当 level0 的 sst 文件个数到达 level0-slowdown-writes-trigger 指定的限度的时候，\n# RocksDB 会尝试减慢写入的速度。因为 level0 的 sst 太多会导致 RocksDB 的读放大上升。\n# level0-slowdown-writes-trigger 和 level0-stop-writes-trigger 是 RocksDB 进行流控的\n# 另一个表现。当 level0 的 sst 的文件个数到达 4（默认值），level0 的 sst 文件会和 level1 中\n# 有 overlap 的 sst 文件进行 compaction，缓解读放大的问题。\nlevel0-slowdown-writes-trigger = 20\n\n# 当 level0 的 sst 文件个数到达 level0-stop-writes-trigger 指定的限度的时候，RocksDB 会\n# stall 住新的写入。\nlevel0-stop-writes-trigger = 36\n\n# 当 level1 的数据量大小达到 max-bytes-for-level-base 限定的值的时候，会触发 level1 的\n# sst 和 level2 种有 overlap 的 sst 进行 compaction。\n# 黄金定律：max-bytes-for-level-base 的设置的第一参考原则就是保证和 level0 的数据量大致相\n# 等，这样能够减少不必要的 compaction。例如压缩方式为\"no:no:lz4:lz4:lz4:lz4:lz4\"，那么\n# max-bytes-for-level-base 的值应该是 write-buffer-size 的大小乘以 4，因为 level0 和\n# level1 都没有压缩，而且 level0 触发 compaction 的条件是 sst 的个数到达 4（默认值）。在\n# level0 和 level1 都采取了压缩的情况下，就需要分析下 RocksDB 的日志，看一个 memtable 的压\n# 缩成一个 sst 文件的大小大概是多少，例如 32MB，那么 max-bytes-for-level-base 的建议值就应\n# 该是 32MiB * 4 = 128MiB。\nmax-bytes-for-level-base = \"512MiB\"\n\n# sst 文件的大小。level0 的 sst 文件的大小受 write-buffer-size 和 level0 采用的压缩算法的\n# 影响，target-file-size-base 参数用于控制 level1-level6 单个 sst 文件的大小。\ntarget-file-size-base = \"32MiB\"\n\n[rocksdb.writecf]\n# 保持和 rocksdb.defaultcf.compression-per-level 一致。\ncompression-per-level = [\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]\n\n# 保持和 rocksdb.defaultcf.write-buffer-size 一致。\nwrite-buffer-size = \"128MiB\"\nmax-write-buffer-number = 5\nmin-write-buffer-number-to-merge = 1\n\n# 保持和 rocksdb.defaultcf.max-bytes-for-level-base 一致。\nmax-bytes-for-level-base = \"512MiB\"\ntarget-file-size-base = \"32MiB\"\n\n[raftdb]\n# RaftDB 能够打开的最大文件句柄数。\n# max-open-files = 40960\n\n# 开启 RaftDB compaction 过程中的预读功能，如果使用的是机械磁盘，建议该值至少为2MiB。\n# compaction-readahead-size = \"2MiB\"\n\n[raftdb.defaultcf]\n# 保持和 rocksdb.defaultcf.compression-per-level 一致。\ncompression-per-level = [\"no\", \"no\", \"lz4\", \"lz4\", \"lz4\", \"zstd\", \"zstd\"]\n\n# 保持和 rocksdb.defaultcf.write-buffer-size 一致。\nwrite-buffer-size = \"128MiB\"\nmax-write-buffer-number = 5\nmin-write-buffer-number-to-merge = 1\n\n# 保持和 rocksdb.defaultcf.max-bytes-for-level-base 一致。\nmax-bytes-for-level-base = \"512MiB\"\ntarget-file-size-base = \"32MiB\"\n```\n\n## TiKV 内存使用情况\n\n除了以上列出的 `block-cache` 以及 `write-buffer` 会占用系统内存外：\n\n1. 需预留一些内存作为系统的 page cache\n2. TiKV 在处理大的查询的时候（例如 `select * from ...`）会读取数据然后在内存中生成对应的数据结构返回给 TiDB，这个过程中 TiKV 会占用一部分内存\n\n## TiKV 机器配置推荐\n\n1. 生产环境中，不建议将 TiKV 部署在 CPU 核数小于 8 或内存低于 32GiB 的机器上\n2. 如果对写入吞吐要求比较高，建议使用吞吐能力比较好的磁盘\n3. 如果对读写的延迟要求非常高，建议使用 IOPS 比较高的 SSD 盘\n"
        },
        {
          "name": "tune-tikv-thread-performance.md",
          "type": "blob",
          "size": 8.8056640625,
          "content": "---\ntitle: TiKV 线程池性能调优\nsummary: 了解 TiKV 线程池性能调优。\naliases: ['/docs-cn/dev/tune-tikv-thread-performance/']\n---\n\n# TiKV 线程池性能调优\n\n本文主要介绍 TiKV 线程池性能调优的主要手段，以及 TiKV 内部线程池的主要用途。\n\n## 线程池介绍\n\n在 TiKV 中，线程池主要由 gRPC、Scheduler、UnifyReadPool、Raftstore、StoreWriter、Apply、RocksDB 以及其它一些占用 CPU 不多的定时任务与检测组件组成，这里主要介绍几个占用 CPU 比较多且会对用户读写请求的性能产生影响的线程池。\n\n* gRPC 线程池：负责处理所有网络请求，它会把不同任务类型的请求转发给不同的线程池。\n* Scheduler 线程池：负责检测写事务冲突，把事务的两阶段提交、悲观锁上锁、事务回滚等请求转化为 key-value 对数组，然后交给 Raftstore 线程进行 Raft 日志复制。\n* Raftstore 线程池：\n    * 处理所有的 Raft 消息以及添加新日志的提议 (Propose)。\n    * 处理 Raft 日志。如果 [`store-io-pool-size`](/tikv-configuration-file.md#store-io-pool-size-从-v530-版本开始引入) 配置项的值为 `0`，Raftstore 线程将日志写入到磁盘；如果该值不为 `0`，Raftstore 线程将日志发送给 StoreWriter 线程处理。\n    * 当日志在多数副本中达成一致后，Raftstore 线程把该日志发送给 Apply 线程处理。\n* StoreWriter 线程池：负责将所有 Raft 日志写入到磁盘，再把结果返回到 Raftstore 线程。\n* Apply 线程池：当收到从 Raftstore 线程池发来的已提交日志后，负责将其解析为 key-value 请求，然后写入 RocksDB 并且调用回调函数通知 gRPC 线程池中的写请求完成，返回结果给客户端。\n* RocksDB 线程池：RocksDB 进行 Compact 和 Flush 任务的线程池，关于 RocksDB 的架构与 Compact 操作请参考 [RocksDB: A Persistent Key-Value Store for Flash and RAM Storage](https://github.com/facebook/rocksdb)。\n* UnifyReadPool 线程池：由 Coprocessor 线程池与 Storage Read Pool 合并而来，所有的读取请求包括 kv get、kv batch get、raw kv get、coprocessor 等都会在这个线程池中执行。\n\n## TiKV 的只读请求\n\nTiKV 的读取请求分为两类：\n\n- 一类是指定查询某一行或者某几行的简单查询，这类查询会运行在 Storage Read Pool 中。\n- 另一类是复杂的聚合计算、范围查询，这类请求会运行在 Coprocessor Read Pool 中。\n\n从 TiKV 5.0 版本起，默认所有的读取请求都通过统一的线程池进行查询。如果是从 TiKV 4.0 升级上来的 TiKV 集群且升级前未打开 `readpool.storage` 的 `use-unified-pool` 配置，则升级后所有的读取请求仍然继续使用独立的线程池进行查询，可以将 `readpool.storage.use-unified-pool` 设置为 `true` 使所有的读取请求通过统一的线程池进行查询。\n\n## TiKV 线程池调优\n\n* gRPC 线程池的大小默认配置 (`server.grpc-concurrency`) 是 5。由于 gRPC 线程池几乎不会有多少计算开销，它主要负责网络 IO、反序列化请求，因此该配置通常不需要调整。\n\n    - 如果部署的机器 CPU 核数特别少（小于等于 8），可以考虑将该配置 (`server.grpc-concurrency`) 设置为 2。\n    - 如果机器配置很高，并且 TiKV 承担了非常大量的读写请求，观察到 Grafana 上的监控 Thread CPU 的 gRPC poll CPU 的数值超过了 server.grpc-concurrency 大小的 80%，那么可以考虑适当调大 `server.grpc-concurrency` 以控制该线程池使用率在 80% 以下（即 Grafana 上的指标低于 `80% * server.grpc-concurrency` 的值）。\n\n* Scheduler 线程池的大小配置 (`storage.scheduler-worker-pool-size`) 在 TiKV 检测到机器 CPU 核数大于等于 16 时默认为 8，小于 16 时默认为 4。它主要用于将复杂的事务请求转化为简单的 key-value 读写。但是 **scheduler 线程池本身不进行任何写操作**。\n\n    - 如果检测到有事务冲突，那么它会提前返回冲突结果给客户端。\n    - 如果未检测到事务冲突，那么它会把需要写入的 key-value 合并成一条 Raft 日志交给 Raftstore 线程进行 Raft 日志复制。\n    \n    通常来说为了避免过多的线程切换，最好确保 scheduler 线程池的利用率保持在 50%～75% 之间。（如果线程池大小为 8 的话，那么 Grafana 上的 TiKV-Details.Thread CPU.scheduler worker CPU 应当在 400%～600% 之间较为合理）\n\n* Raftstore 线程池是 TiKV 中最复杂的一个线程池，默认大小 (由 `raftstore.store-pool-size` 控制) 为 2。StoreWriter 线程池的默认大小 (由 `raftstore.store-io-pool-size` 控制) 为 1。\n\n    * 当 StoreWriter 线程池大小为 0 时，所有的写请求都会由 Raftstore 线程以 fsync 的方式写入 RocksDB。此时建议采取如下调优操作：\n        * 将 Raftstore 线程的整体 CPU 使用率控制在 60% 以下。当把 Raftstore 线程数设为默认值 2 时，将 Grafana 监控上 **TiKV-Details**、**Thread CPU**、**Raft store CPU** 面版上的数值控制在 120% 以内。由于存在 I/O 请求，理论上 Raftstore 线程的 CPU 使用率总是低于 100%。\n        * 不建议为了提升写性能而盲目增大 Raftstore 线程池大小，这样可能会适得其反，增加磁盘负担，导致性能变差。\n    * 当 StoreWriter 线程池大小不为 0 时，所有写请求都由 StoreWriter 线程以 fsync 的方式写入 RocksDB。此时建议采取如下调优操作：\n        * 仅在整体 CPU 资源比较充裕的情况下启用 StoreWriter 线程池，并将 StoreWriter 线程和 Raftstore 线程的 CPU 使用率控制在 80% 以下。\n\n            与写请求在 Raftstore 线程完成的情况相比，理论上 StoreWriter 线程处理写请求能够显著地降低写延迟和读的尾延迟。然而，写入速度变得更快意味着 Raft 日志也变得更多，从而导致 Raftstore 线程、Apply 线程和 gRPC 线程的 CPU 开销增多。在这种情况下，CPU 资源不足可能会抵消优化效果，反而还可能比原来的写速度更慢，因此若是 CPU 资源不充裕则不建议开启 StoreWriter 线程。由于 Raftstore 线程把绝大部分的 I/O 请求交给 StoreWriter，因此 Raftstore 线程的 CPU 使用率控制在 80% 以下即可。\n\n        * 大多数情况下将 StoreWriter 线程池的大小设为 1 或 2 即可。这是因为 StoreWriter 线程池的大小会影响 Raft 日志数量，所以该值不宜过大。如果 CPU 使用率高于 80%，可以考虑再增加其大小。\n        * 注意 Raft 日志增多对其他线程池 CPU 开销的影响，必要的时候需要相应地增加 Raftstore 线程、Apply 线程和 gRPC 线程的数量。\n\n* UnifyReadPool 负责处理所有的读取请求。默认配置 (`readpool.unified.max-thread-count`) 大小为机器 CPU 数的 80% （如机器为 16 核，则默认线程池大小为 12）。\n\n    通常建议根据业务负载特性调整其 CPU 使用率在线程池大小的 60%～90% 之间（如果用户 Grafana 上 TiKV-Details.Thread CPU.Unified read pool CPU 的峰值不超过 800%，那么建议用户将 `readpool.unified.max-thread-count` 设置为 10，过多的线程数会造成更频繁的线程切换，并且抢占其他线程池的资源）。\n\n    TiKV 从 v6.3.0 开始支持根据统一读线程池 (UnifyReadPool) 的 CPU 利用率自动动态调整线程池的线程数量，可以通过配置 [`readpool.unified.auto-adjust-pool-size`](/tikv-configuration-file.md#auto-adjust-pool-size-从-v630-版本开始引入) 开启此功能。对于重读并且峰值 CPU 利用率超过 80% 的集群，建议开启此配置。\n\n* RocksDB 线程池是 RocksDB 进行 Compact 和 Flush 任务的线程池，通常不需要配置。\n\n    * 如果机器 CPU 核数较少，可将 `rocksdb.max-background-jobs` 与 `raftdb.max-background-jobs` 同时设置为 4。\n    * 如果遇到了 Write Stall，可查看 Grafana 监控上 **RocksDB-kv** 中的 Write Stall Reason 有哪些指标不为 0。\n        * 如果是由 pending compaction bytes 相关原因引起的，可将 `rocksdb.max-sub-compactions` 设置为 2 或者 3（该配置表示单次 compaction job 允许使用的子线程数量，TiKV 4.0 版本默认值为 3，3.0 版本默认值为 1）。\n        * 如果原因是 memtable count 相关，建议调大所有列的 `max-write-buffer-number`（默认为 5）。\n        * 如果原因是 level0 file limit 相关，建议调大如下参数为 64 或者更高：\n\n            ```\n            rocksdb.defaultcf.level0-slowdown-writes-trigger\n            rocksdb.writecf.level0-slowdown-writes-trigger\n            rocksdb.lockcf.level0-slowdown-writes-trigger\n            rocksdb.defaultcf.level0-stop-writes-trigger\n            rocksdb.writecf.level0-stop-writes-trigger\n            rocksdb.lockcf.level0-stop-writes-trigger\n            ```\n"
        },
        {
          "name": "two-data-centers-in-one-city-deployment.md",
          "type": "blob",
          "size": 11.259765625,
          "content": "---\ntitle: 单区域双 AZ 部署 TiDB\nsummary: 了解单个区域两个可用区自适应同步模式部署方式。\n---\n\n# 单区域双 AZ 部署 TiDB\n\n本文介绍单区域双可用区 (Availability Zone, AZ) 的部署模式，包括方案架构、示例配置、副本方法及启用该模式的方法。\n\n本文中的区域指的是地理隔离的不同位置，AZ 指的是区域内部划分的相互独立的资源集合，本文所描述的方案同样适用于同一城市两个数据中心（同城双中心）的场景。\n\n## 简介\n\nTiDB 通常采用多 AZ 部署方案保证集群高可用和容灾能力。多 AZ 部署方案包括单区域多 AZ 部署模式、双区域多 AZ 部署模式等多种部署模式。本文介绍单区域双 AZ 部署方案，即在同一区域部署两个 AZ，成本更低，同样能满足高可用和容灾要求。该部署方案采用自适应同步模式，即 Data Replication Auto Synchronous，简称 DR Auto-Sync。\n\n单区域双 AZ 部署方案下，两个 AZ 通常位于同一个城市或两个相邻城市（例如北京和廊坊），相距 50 km 以内，AZ 间的网络连接延迟小于 1.5 ms，带宽大于 10 Gbps。\n\n## 部署架构\n\n本文以某市为例，市里有两个 AZ，AZ1 和 AZ2，AZ1 为主用区域，AZ2 为从属区域，分别位于城东和城西。\n\n下图为集群部署架构图，具体如下：\n\n- 集群采用推荐的 6 副本模式，其中 AZ1 中放 3 个 Voter，AZ2 中放 2 个 Follower 副本和 1 个 Learner 副本。TiKV 按机房的实际情况打上合适的 Label。\n- 副本间通过 Raft 协议保证数据的一致性和高可用，对用户完全透明。\n\n![单区域双 AZ 集群架构图](/media/two-dc-replication-1.png)\n\n该部署方案定义了三种状态来控制和标示集群的同步状态，该状态约束了 TiKV 的同步方式。集群的复制模式可以自动在三种状态之间自适应切换。要了解切换过程，请参考[状态转换](#状态转换)。\n\n- **sync**：同步复制模式，此时从 AZ (DR) 至少有一个副本与主 AZ (PRIMARY) 进行同步，Raft 算法保证每条日志按 Label 同步复制到 DR。\n- **async**：异步复制模式，此时不保证 DR 与 PRIMARY 完全同步，Raft 算法使用经典的 majority 方式复制日志。\n- **sync-recover**：恢复同步，此时不保证 DR 与 PRIMARY 完全同步，Raft 逐步切换成 Label 复制，切换成功后汇报给 PD。\n\n## 配置\n\n### 示例\n\n以下 `tiup topology.yaml` 示例拓扑文件为单区域双 AZ 典型的拓扑配置：\n\n```\n# # Global variables are applied to all deployments and used as the default value of\n# # the deployments if a specific deployment value is missing.\nglobal:\n  user: \"tidb\"\n  ssh_port: 22\n  deploy_dir: \"/data/tidb_cluster/tidb-deploy\"\n  data_dir: \"/data/tidb_cluster/tidb-data\"\nserver_configs:\n  pd:\n    replication.location-labels:  [\"az\",\"rack\",\"host\"]\npd_servers:\n  - host: 10.63.10.10\n    name: \"pd-10\"\n  - host: 10.63.10.11\n    name: \"pd-11\"\n  - host: 10.63.10.12\n    name: \"pd-12\"\ntidb_servers:\n  - host: 10.63.10.10\n  - host: 10.63.10.11\n  - host: 10.63.10.12\ntikv_servers:\n  - host: 10.63.10.30\n    config:\n      server.labels: { az: \"east\", rack: \"east-1\", host: \"30\" }\n  - host: 10.63.10.31\n    config:\n      server.labels: { az: \"east\", rack: \"east-2\", host: \"31\" }\n  - host: 10.63.10.32\n    config:\n      server.labels: { az: \"east\", rack: \"east-3\", host: \"32\" }\n  - host: 10.63.10.33\n    config:\n      server.labels: { az: \"west\", rack: \"west-1\", host: \"33\" }\n  - host: 10.63.10.34\n    config:\n      server.labels: { az: \"west\", rack: \"west-2\", host: \"34\" }\n  - host: 10.63.10.35\n    config:\n      server.labels: { az: \"west\", rack: \"west-3\", host: \"35\" }\nmonitoring_servers:\n  - host: 10.63.10.60\ngrafana_servers:\n  - host: 10.63.10.60\nalertmanager_servers:\n  - host: 10.63.10.60\n```\n\n### Placement Rules 规划\n\n为了按照规划的集群拓扑进行部署，你需要使用 [Placement Rules](/configure-placement-rules.md) 来规划集群副本的放置位置。以 6 副本（3 个 Voter 副本在主 AZ，2 个 Follower 副本和 1 个 Learner 副本在从 AZ）的部署方式为例，可使用 Placement Rules 进行如下副本配置：\n\n```\ncat rule.json\n[\n  {\n    \"group_id\": \"pd\",\n    \"group_index\": 0,\n    \"group_override\": false,\n    \"rules\": [\n      {\n        \"group_id\": \"pd\",\n        \"id\": \"az-east\",\n        \"start_key\": \"\",\n        \"end_key\": \"\",\n        \"role\": \"voter\",\n        \"count\": 3,\n        \"label_constraints\": [\n          {\n            \"key\": \"az\",\n            \"op\": \"in\",\n            \"values\": [\n              \"east\"\n            ]\n          }\n        ],\n        \"location_labels\": [\n          \"az\",\n          \"rack\",\n          \"host\"\n        ]\n      },\n      {\n        \"group_id\": \"pd\",\n        \"id\": \"az-west-1\",\n        \"start_key\": \"\",\n        \"end_key\": \"\",\n        \"role\": \"follower\",\n        \"count\": 2,\n        \"label_constraints\": [\n          {\n            \"key\": \"az\",\n            \"op\": \"in\",\n            \"values\": [\n              \"west\"\n            ]\n          }\n        ],\n        \"location_labels\": [\n          \"az\",\n          \"rack\",\n          \"host\"\n        ]\n      },\n      {\n        \"group_id\": \"pd\",\n        \"id\": \"az-west-2\",\n        \"start_key\": \"\",\n        \"end_key\": \"\",\n        \"role\": \"learner\",\n        \"count\": 1,\n        \"label_constraints\": [\n          {\n            \"key\": \"az\",\n            \"op\": \"in\",\n            \"values\": [\n              \"west\"\n            ]\n          }\n        ],\n        \"location_labels\": [\n          \"az\",\n          \"rack\",\n          \"host\"\n        ]\n      }\n    ]\n  }\n]\n```\n\n如果需要使用 `rule.json` 中的配置，你可以使用以下命令把原有的配置备份到 `default.json` 文件，再使用 `rule.json` 中的配置覆盖原有配置：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\npd-ctl config placement-rules rule-bundle load --out=\"default.json\"\npd-ctl config placement-rules rule-bundle save --in=\"rule.json\"\n```\n\n如果需要回退配置，你可以还原备份的 `default.json` 文件或者手动编写如下的 json 文件并将其覆盖到现有的配置文件中：\n\n```\ncat default.json\n[\n  {\n    \"group_id\": \"pd\",\n    \"group_index\": 0,\n    \"group_override\": false,\n    \"rules\": [\n      {\n        \"group_id\": \"pd\",\n        \"id\": \"default\",\n        \"start_key\": \"\",\n        \"end_key\": \"\",\n        \"role\": \"voter\",\n        \"count\": 5\n      }\n    ]\n  }\n]\n```\n\n### 启用自适应同步模式\n\n副本的复制模式由 PD 节点控制。如果要使用 DR Auto-sync 自适应同步模式，需要按照以下任一方法修改 PD 的配置。\n\n+ 方法一：先配置 PD 的配置文件，然后部署集群。\n\n    {{< copyable \"\" >}}\n\n    ```toml\n    [replication-mode]\n    replication-mode = \"dr-auto-sync\"\n    [replication-mode.dr-auto-sync]\n    label-key = \"az\"\n    primary = \"east\"\n    dr = \"west\"\n    primary-replicas = 3\n    dr-replicas = 2\n    wait-store-timeout = \"1m\"\n    wait-recover-timeout = \"0s\"\n    pause-region-split = false\n    ```\n\n+ 方法二：如果已经部署了集群，则使用 pd-ctl 命令修改 PD 的配置。\n\n    {{< copyable \"\" >}}\n\n    ```shell\n    config set replication-mode dr-auto-sync\n    config set replication-mode dr-auto-sync label-key az\n    config set replication-mode dr-auto-sync primary east\n    config set replication-mode dr-auto-sync dr west\n    config set replication-mode dr-auto-sync primary-replicas 3\n    config set replication-mode dr-auto-sync dr-replicas 2\n    ```\n\n配置项说明：\n\n+ `replication-mode` 为待启用的复制模式，以上示例中设置为 `dr-auto-sync`。默认使用 majority 算法。\n+ `label-key` 用于区分不同的 AZ，需要和 Placement Rules 相匹配。其中主 AZ 为 \"east\"，从 AZ 为 \"west\"。\n+ `primary-replicas` 是主 AZ 上 Voter 副本的数量。\n+ `dr-replicas` 是从 AZ 上 Voter 副本的数量。\n+ `wait-store-timeout` 是当出现网络隔离或者故障时，切换到异步复制模式的等待时间。如果超过这个时间还没恢复，则自动切换到异步复制模式。默认时间为 60 秒。\n+ `wait-recover-timeout` 是当网络恢复后，切换回 `sync-recover` 状态的等待时间。默认为 0 秒。\n+ `pause-region-split` 用于控制在 `async_wait` 和 `async` 状态下是否暂停 Region 的 split 操作。暂停 Region split 可以防止在 `sync-recover` 状态同步数据时从属 AZ 出现短暂的部分数据缺失。默认为 `false`。\n\n如果需要检查当前集群的复制状态，可以通过以下 API 获取：\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ncurl http://pd_ip:pd_port/pd/api/v1/replication_mode/status\n```\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\n{\n  \"mode\": \"dr-auto-sync\",\n  \"dr-auto-sync\": {\n    \"label-key\": \"az\",\n    \"state\": \"sync\"\n  }\n}\n```\n\n#### 状态转换\n\n简单来讲，集群的复制模式可以自动在三种状态之间自适应的切换：\n\n- 当集群一切正常时，会进入同步复制模式来最大化地保障灾备 AZ 的数据完整性。\n- 当 AZ 网络断连或灾备 AZ 发生整体故障时，在经过一段提前设置好的保护窗口之后，集群会进入异步复制状态，来保障业务的可用性。\n- 当 AZ 网络重连或灾备 AZ 整体恢复之后，灾备 AZ 的 TiKV 节点会重新加入到集群，逐步同步数据并最终转为同步复制模式。\n\n状态转换的细节过程如下：\n\n1. **初始化**：集群在初次启动时处于 sync（同步复制）模式，PD 会下发信息给 TiKV，所有 TiKV 节点会严格按照 sync 模式的要求进行工作。\n\n2. **同步切异步**：PD 通过定时检查 TiKV 的心跳信息来判断 TiKV 是否宕机或断连。如果宕机数超过 Primary 和 DR 各自副本的数量 `primary-replicas` 和 `dr-replicas`，意味着无法完成同步复制，需要切换状态。当宕机时间超过了 `wait-store-timeout` 设定的时间，PD 将集群状态切换成 async（异步复制）模式。然后 PD 再将 async 状态下发到所有 TiKV 节点，TiKV 的复制模式由双 AZ 同步方式转为原生的 Raft 大多数落实方式 (majority)。\n\n3. **异步切同步**：PD 通过定时检查 TiKV 的心跳信息来判断 TiKV 是否恢复连接，如果宕机数小于 Primary 和 DR 各自副本的数量，意味着可以切回同步。PD 会将集群复制状态先切换至 sync-recover，再将该状态下发给所有 TiKV 节点。TiKV 的所有 Region 逐步切换成双 AZ 同步复制模式，切换成功后通过心跳将状态同步信息给 PD。PD 记录 TiKV 上 Region 的状态并统计恢复进度。当 TiKV 的所有 Region 都完成了同步复制模式的切换，PD 将集群复制状态切换为 sync。\n\n### 灾难恢复\n\n本节介绍单区域双 AZ 部署提供的容灾恢复方案。\n\n当处于同步复制状态的集群发生了灾难，可进行 `RPO = 0` 的数据恢复：\n\n- 如果主 AZ 发生故障，丢失了大多数 Voter 副本，但是从 AZ 有完整的数据，可在从 AZ 恢复数据。此时需要人工介入，通过专业工具恢复。如需获取支持，请联系 [PingCAP 服务与支持](https://cn.pingcap.com/support/)。\n\n- 如果从 AZ 发生故障，丢失了少数 Voter 副本，能自动切换成 async 异步复制模式。\n\n当不处于同步复制状态的集群发生了灾难，不能保证满足 `RPO = 0` 进行数据恢复：\n\n- 如果丢失了大多数 Voter 副本，需要人工介入，通过专业工具恢复（恢复方式请联系 TiDB 团队）。\n"
        },
        {
          "name": "upgrade-monitoring-services.md",
          "type": "blob",
          "size": 6.6064453125,
          "content": "---\ntitle: 升级集群监控组件\nsummary: 介绍如何升级 TiDB 集群监控组件 Prometheus、Grafana 和 Alertmanager。\n---\n\n# 升级 TiDB 集群监控组件\n\n使用 TiUP 部署 TiDB 集群时，TiUP 会同时自动部署 Prometheus、Grafana 和 Alertmanager 等监控组件，并且在集群扩容中自动为新增节点添加监控配置。通过 TiUP 自动部署的监控组件并不是这些三方组件的最新版本，如果你需要使用最新的三方组件，可以按照本文的方法升级所需的监控组件。\n\n当管理集群时，TiUP 会使用自己的配置参数覆盖监控组件的配置。如果你直接通过替换监控组件配置文件的方式升级监控组件，在之后对集群进行 `deploy`、`scale-out`、`scale-in`、`reload` 等 TiUP 操作时，该升级可能被 TiUP 所覆盖，导致升级出错。如果需要升级 Prometheus、Grafana 和 Alertmanager，请参考本文介绍的升级步骤，而不是直接替换配置文件。\n\n> **注意：**\n>\n> - 如果现有的监控组件是[手动部署](/deploy-monitoring-services.md)的，而不是由 TiUP 部署的，你可以直接升级监控组件，无需参考本文。\n> - TiDB 并未对监控组件新版本的兼容性进行测试，可能存在升级后部分功能无法正常使用的问题。如果遇到问题，请在 GitHub 上提 [issue](https://github.com/pingcap/tidb/issues) 反馈。\n> - 本文所述功能在 TiUP v1.9.0 及后续版本支持，使用本功能前请检查 TiUP 版本号。\n> - 使用 TiUP 升级 TiDB 群集时，TiUP 会将监控组件重新部署为其默认版本。因此，你需要在升级 TiDB 后重新升级监控组件。\n\n## 升级 Prometheus\n\n为了更好地兼容 TiDB，推荐使用 TiDB 官方安装包中自带的 Prometheus 组件安装包，该组件包中的 Prometheus 版本是固定的。如果你需要使用更高版本的 Prometheus，可以在 Prometheus 官网的 [Release Note 页面](https://github.com/prometheus/prometheus/releases)查看新版本特性，选择适合你生产环境的版本，或者咨询 PingCAP 技术支持服务寻求版本建议。\n\n在以下升级步骤中，你需要先从 Prometheus 官网下载所需版本的软件安装包，然后将其构造为可被 TiUP 使用的 Prometheus 组件安装包。\n\n### 第 1 步：从 Prometheus 官网下载新版本安装包\n\n从 [Prometheus 官网下载页面](https://prometheus.io/download/)下载组件安装包，并解压。\n\n### 第 2 步：下载 TiDB 官方 Prometheus 安装包\n\n1. 在 [TiDB 官网下载页面](https://cn.pingcap.com/product/#SelectProduct)下载 `TiDB-community-server` 软件包，并解压。\n2. 在解压文件中，找到 `prometheus-v{version}-linux-amd64.tar.gz`，并解压。\n\n    ```bash\n    tar -xzf prometheus-v{version}-linux-amd64.tar.gz\n    ```\n\n### 第 3 步：构造新的适用于 TiUP 的 Prometheus 组件包\n\n1. 复制第 1 步中解压的文件，替换第 2 步解压后的 `./prometheus-v{version}-linux-amd64/prometheus` 目录下的对应文件。\n2. 重新压缩替换文件后的 `./prometheus-v{version}-linux-amd64` 目录，并将新的压缩包命名为 `prometheus-v{new-version}.tar.gz`。其中，`{new-version}` 可以由你自行指定。\n\n    ```bash\n    cd prometheus-v{version}-linux-amd64\n    tar -zcvf ../prometheus-v{new-version}.tar.gz ./\n    ```\n\n### 第 4 步：使用新的组件包升级 Prometheus\n\n执行以下命令升级 Prometheus。\n\n```bash\ntiup cluster patch <cluster-name> prometheus-v{new-version}.tar.gz -R prometheus --overwrite\n```\n\n升级完成后，可以打开 Prometheus 主页（地址通常是 `http://<Prometheus-server-host-name>:9090`），点击顶部导航菜单中的 **Status**，然后打开 **Runtime & Build Information** 页面，查看 Prometheus 的版本信息，确认升级成功。\n\n## 升级 Grafana\n\n为了更好地兼容 TiDB，推荐使用 TiDB 官方安装包中自带的 Grafana 组件安装包，该组件包中的 Grafana 版本是固定的。如果你需要使用更高版本的 Grafana，可以在 Grafana 官网的 [Release Note 页面](https://grafana.com/docs/grafana/latest/whatsnew/)查看新版本特性，选择适合你生产环境的版本，或者咨询 PingCAP 技术支持服务寻求版本建议。\n\n在以下升级步骤中，你需要先从 Grafana 官网下载所需版本的软件安装包，然后将其构造为可被 TiUP 使用的 Grafana 组件安装包。\n\n### 第 1 步：从 Grafana 官网的下载新版本安装包\n\n1. 从 [Grafana 官网下载页面](https://grafana.com/grafana/download?pg=get&plcmt=selfmanaged-box1-cta1)下载组件安装包。你可以根据需要选择下载 `OSS` 版或 `Enterprise` 版。\n2. 解压下载的软件包。\n\n### 第 2 步：下载 TiDB 官方 Grafana 安装包\n\n1. 在 [TiDB 官网下载页面](https://cn.pingcap.com/product/#SelectProduct)下载 `TiDB-community-server` 软件包，并解压。\n2. 在解压文件中，找到 `grafana-v{version}-linux-amd64.tar.gz`，并解压。\n\n    ```bash\n    tar -xzf grafana-v{version}-linux-amd64.tar.gz\n    ```\n\n### 第 3 步：构造新的适用于 TiUP 的 Grafana 组件包\n\n1. 复制第 1 步中解压的文件，替换第 2 步解压后的 `./grafana-v{version}-linux-amd64/` 目录下的对应文件。\n2. 重新压缩替换文件后的 `./grafana-v{version}-linux-amd64` 目录，并将新的压缩包命名为 `grafana-v{new-version}.tar.gz`。其中，`{new-version}` 可以由你自行指定。\n\n    ```bash\n    cd grafana-v{version}-linux-amd64\n    tar -zcvf ../grafana-v{new-version}.tar.gz ./\n    ```\n\n### 第 4 步：使用新的组件包升级 Grafana\n\n执行以下命令升级 Grafana。\n\n```bash\ntiup cluster patch <cluster-name> grafana-v{new-version}.tar.gz -R grafana --overwrite\n```\n\n升级完成后，可以打开 Grafana 主页（地址通常是 `http://<Grafana-server-host-name>:3000`），查看 Grafana 的版本信息，确认升级成功。\n\n## 升级 Alertmanager\n\nTiDB 安装包中直接使用了 Alertmanager 官方组件包，因此升级 Alertmanager 时你只需要下载并安装新版本的官方组件包。\n\n### 第 1 步：从 Prometheus 官网下载新版本安装包\n\n从 [Prometheus 官网下载页面](https://prometheus.io/download/#alertmanager)下载 `alertmanager` 组件安装包。\n\n### 第 2 步：使用新的组件包升级 Alertmanager\n\n执行以下命令升级 Alertmanager：\n\n```bash\ntiup cluster patch <cluster-name> alertmanager-v{new-version}-linux-amd64.tar.gz -R alertmanager --overwrite\n```\n\n升级完成后，可以打开 Alertmanager 主页（地址通常是 `http://<Alertmanager-server-host-name>:9093`），点击顶部导航菜单中的 **Status**，然后查看 Alertmanager 的版本信息，确认升级成功。"
        },
        {
          "name": "upgrade-tidb-using-tiup.md",
          "type": "blob",
          "size": 18.310546875,
          "content": "---\ntitle: 使用 TiUP 升级 TiDB\naliases: ['/docs-cn/dev/upgrade-tidb-using-tiup/','/docs-cn/dev/how-to/upgrade/using-tiup/','/docs-cn/dev/upgrade-tidb-using-tiup-offline/', '/zh/tidb/dev/upgrade-tidb-using-tiup-offline']\nsummary: TiUP 可用于 TiDB 升级。升级过程中需注意不支持 TiFlash 组件从 5.3 之前的老版本在线升级至 5.3 及之后的版本，只能采用停机升级。在升级过程中，不要执行 DDL 语句，避免出现行为未定义的问题。升级前需查看集群中是否有正在进行的 DDL Job，并等待其完成或取消后再进行升级。升级完成后，可使用 TiUP 安装对应版本的 `ctl` 组件来更新相关工具版本。\n---\n\n# 使用 TiUP 升级 TiDB\n\n本文档适用于从以下版本升级到 TiDB v8.5.0：v6.1.x、v6.5.x、v7.1.x、v7.5.x、v8.1.x、v8.2.0、v8.3.0、v8.4.0\n\n> **警告：**\n>\n> 1. 从 v8.4.0 版本开始，TiDB 已结束对 CentOS 7 和 Red Hat Enterprise Linux 7 的支持，建议使用 Rocky Linux 9.1 及以上的版本。如果将运行在 CentOS 7 或 Red Hat Enterprise Linux 7 上的 TiDB 集群升级到 v8.4.0 或之后版本，将导致集群不可用。升级 TiDB 前，请务必确保你的操作系统版本符合[操作系统及平台要求](/hardware-and-software-requirements.md#操作系统及平台要求)。\n> 2. 不支持将 TiFlash 组件从 5.3 之前的老版本在线升级至 5.3 及之后的版本，只能采用停机升级。如果集群中其他组件（如 tidb，tikv）不能停机升级，参考[不停机升级](#不停机升级)中的注意事项。\n> 3. 在升级 TiDB 集群的过程中，**请勿执行** DDL 语句，否则可能会出现行为未定义的问题。\n> 4. 集群中有 DDL 语句正在被执行时（通常为 `ADD INDEX` 和列类型变更等耗时较久的 DDL 语句），**请勿进行**升级操作。在升级前，建议使用 [`ADMIN SHOW DDL`](/sql-statements/sql-statement-admin-show-ddl.md) 命令查看集群中是否有正在进行的 DDL Job。如需升级，请等待 DDL 执行完成或使用 [`ADMIN CANCEL DDL`](/sql-statements/sql-statement-admin-cancel-ddl.md) 命令取消该 DDL Job 后再进行升级。\n> 5. 从 TiDB v7.1 版本升级至更高的版本时，可以不遵循上面的限制 3 和 4，请参考[平滑升级 TiDB 的限制](/smooth-upgrade-tidb.md#使用限制)。\n> 6. 在使用 TiUP 升级 TiDB 集群之前，务必阅读[用户操作限制](/smooth-upgrade-tidb.md#用户操作限制)。\n\n> **注意：**\n>\n> - 如果原集群是 6.2 之前的版本，升级到 6.2 及以上版本时，部分场景会遇到升级卡住的情况，你可以参考[如何解决升级卡住的问题](#42-升级到-v620-及以上版本时如何解决升级卡住的问题)。\n> - 配置参数 [`server-version`](/tidb-configuration-file.md#server-version) 的值会被 TiDB 节点用于验证当前 TiDB 的版本。因此在进行 TiDB 集群升级前，请将 `server-version` 的值设置为空或者当前 TiDB 真实的版本值，避免出现非预期行为。\n> - 配置项 [`performance.force-init-stats`](/tidb-configuration-file.md#force-init-stats-从-v657-和-v710-版本开始引入) 设置为 `ON` 会延长 TiDB 的启动时间，这可能会造成启动超时，升级失败。为避免这种情况，建议为 TiUP 设置更长的等待超时。\n>     - 可能受影响的场景：\n>         - 原集群版本低于 v6.5.7、v7.1.0（尚未支持 `performance.force-init-stats`），目标版本为 v7.2.0 及更高。\n>         - 原集群版本高于或等于 v6.5.7、v7.1.0，且配置项 `performance.force-init-stats` 被设置为 `ON`。\n>     - 查看配置项 `performance.force-init-stats` 的值：\n>\n>         ```\n>         SHOW CONFIG WHERE type = 'tidb' AND name = 'performance.force-init-stats';\n>         ```\n>\n>     - 通过增加命令行选项 [`--wait-timeout`](/tiup/tiup-component-cluster.md#--wait-timeoutuint默认-120) 可以延长 TiUP 超时等待。如下命令可将超时等待设置为 1200 秒（即 20 分钟）。\n>\n>         ```shell\n>         tiup update cluster --wait-timeout 1200 [other options]\n>         ```\n>\n>         通常情况下，20 分钟超时等待能满足绝大部分场景的需求。如果需要更准确的预估，可以在 TiDB 日志中搜索 `init stats info time` 关键字，获取上次启动的统计信息加载时间作为参考。例如：\n>\n>         ```\n>         [domain.go:2271] [\"init stats info time\"] [lite=true] [\"take time\"=2.151333ms]\n>         ```\n>\n>          如果原集群是 v7.1.0 或更早的版本，升级到 v7.2.0 或以上版本时，由于 [`performance.lite-init-stats`](/tidb-configuration-file.md#lite-init-stats-从-v710-版本开始引入) 的引入，统计信息加载时间会大幅减少。这个情况下，升级前的 `init stats info time` 会比升级后加载所需的时间偏长。\n>     - 如果想要缩短 TiDB 滚动升级的时间，并且在升级过程中能够承受初始统计信息缺失带来的潜在性能影响，可以在升级前[用 TiUP 修改目标实例的配置](/maintain-tidb-using-tiup.md#修改配置参数)，将 `performance.force-init-stats` 设置为 `OFF`。升级完成后可酌情改回。\n\n## 1. 升级兼容性说明\n\n- TiDB 目前暂不支持版本降级或升级后回退。\n- 支持 TiCDC，TiFlash 等组件版本的升级。\n- 将 v6.3.0 之前的 TiFlash 升级至 v6.3.0 及之后的版本时，需要特别注意：在 Linux AMD64 架构的硬件平台部署 TiFlash 时，CPU 必须支持 AVX2 指令集。而在 Linux ARM64 架构的硬件平台部署 TiFlash 时，CPU 必须支持 ARMv8 架构。具体请参考 [6.3.0 版本 Release Notes](/releases/release-6.3.0.md#其他) 中的描述。\n- 具体不同版本的兼容性说明，请查看各个版本的 [Release Note](/releases/release-notes.md)。请根据各个版本的 Release Note 的兼容性更改调整集群的配置。\n- 升级 v5.3 之前版本的集群到 v5.3 及后续版本时，默认部署的 Prometheus 生成的 Alert 存在时间格式变化。该格式变化是从 Prometheus v2.27.1 开始引入的，详情见 [Prometheus commit](https://github.com/prometheus/prometheus/commit/7646cbca328278585be15fa615e22f2a50b47d06)。\n\n## 2. 升级前准备\n\n本部分介绍实际开始升级前需要进行的更新 TiUP 和 TiUP Cluster 组件版本等准备工作。\n\n### 2.1 查阅兼容性变更\n\n查阅 TiDB v8.5.0 release notes 中的[兼容性变更](/releases/release-8.5.0.md#兼容性变更)。如果有任何变更影响到了你的升级，请采取相应的措施。\n\n### 2.2 升级 TiUP 或更新 TiUP 离线镜像\n\n#### 升级 TiUP 和 TiUP Cluster\n\n> **注意：**\n>\n> 如果原集群中控机不能访问 `https://tiup-mirrors.pingcap.com` 地址，可跳过本步骤，然后[更新 TiUP 离线镜像](#更新-tiup-离线镜像)。\n\n1. 先升级 TiUP 版本（建议 `tiup` 版本不低于 `1.11.3`）：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup update --self\n    tiup --version\n    ```\n\n2. 再升级 TiUP Cluster 版本（建议 `tiup cluster` 版本不低于 `1.11.3`）：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup update cluster\n    tiup cluster --version\n    ```\n\n#### 更新 TiUP 离线镜像\n\n> **注意：**\n>\n> 如果原集群不是通过离线部署方式部署的，可忽略此步骤。\n\n可以参考[使用 TiUP 部署 TiDB 集群](/production-deployment-using-tiup.md)的步骤下载部署新版本的 TiUP 离线镜像，上传到中控机。在执行 `local_install.sh` 后，TiUP 会完成覆盖升级。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntar xzvf tidb-community-server-${version}-linux-amd64.tar.gz\nsh tidb-community-server-${version}-linux-amd64/local_install.sh\nsource /home/tidb/.bash_profile\n```\n\n> **建议：**\n>\n> 关于 `TiDB-community-server` 软件包和 `TiDB-community-toolkit` 软件包的内容物，请查阅 [TiDB 离线包](/binary-package.md)。\n\n覆盖升级完成后，需将 server 和 toolkit 两个离线镜像合并，执行以下命令合并离线组件到 server 目录下。\n\n{{< copyable \"shell-regular\" >}}\n\n```bash\ntar xf tidb-community-toolkit-${version}-linux-amd64.tar.gz\nls -ld tidb-community-server-${version}-linux-amd64 tidb-community-toolkit-${version}-linux-amd64\ncd tidb-community-server-${version}-linux-amd64/\ncp -rp keys ~/.tiup/\ntiup mirror merge ../tidb-community-toolkit-${version}-linux-amd64\n```\n\n离线镜像合并后，执行下列命令升级 Cluster 组件：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup update cluster\n```\n\n此时离线镜像已经更新成功。如果覆盖后发现 TiUP 运行报错，可能是 manifest 未更新导致，可尝试 `rm -rf ~/.tiup/manifests/*` 后再使用。\n\n### 2.3 编辑 TiUP Cluster 拓扑配置文件\n\n> **注意：**\n>\n> 以下情况可跳过此步骤：\n>\n> - 原集群没有修改过配置参数，或通过 tiup cluster 修改过参数但不需要调整。\n> - 升级后对未修改过的配置项希望使用 `8.5.0` 默认参数。\n\n1. 进入拓扑文件的 `vi` 编辑模式：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster edit-config <cluster-name>\n    ```\n\n2. 参考 [topology](https://github.com/pingcap/tiup/blob/master/embed/examples/cluster/topology.example.yaml) 配置模板的格式，将希望修改的参数填到拓扑文件的 `server_configs` 下面。\n\n修改完成后 `:wq` 保存并退出编辑模式，输入 `Y` 确认变更。\n\n### 2.4 检查当前集群的 DDL 和 Backup 情况\n\n为避免升级过程中出现未定义行为或其他故障，建议检查以下指标后再进行升级操作。\n\n- 集群 DDL 情况：\n\n    - 如果[平滑升级](/smooth-upgrade-tidb.md)到 v8.1.0 或更高版本，且开启了[分布式执行框架](/tidb-distributed-execution-framework.md)，建议关闭分布式执行框架后再升级，否则升级期间添加的索引可能会出现和数据不一致的问题，导致升级失败。 \n    - 如果你没有采用[平滑升级](/smooth-upgrade-tidb.md)，则建议使用 [`ADMIN SHOW DDL`](/sql-statements/sql-statement-admin-show-ddl.md) 语句查看集群中是否存在正在进行的 DDL job。如果存在，请等待 DDL job 执行完成或使用 [`ADMIN CANCEL DDL`](/sql-statements/sql-statement-admin-cancel-ddl.md) 语句取消该 DDL job 后再进行升级。\n\n- 集群 Backup 情况：建议使用 [`SHOW [BACKUPS|RESTORES]`](/sql-statements/sql-statement-show-backups.md) 命令查看集群中是否有正在进行的 Backup 或者 Restore 任务。如需升级，请等待 Backup 执行完成后，得到一个有效的备份后再执行升级。\n\n### 2.5 检查当前集群的健康状况\n\n为避免升级过程中出现未定义行为或其他故障，建议在升级前对集群当前的 region 健康状态进行检查，此操作可通过 `check` 子命令完成。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster check <cluster-name> --cluster\n```\n\n执行结束后，最后会输出 region status 检查结果。如果结果为 \"All regions are healthy\"，则说明当前集群中所有 region 均为健康状态，可以继续执行升级；如果结果为 \"Regions are not fully healthy: m miss-peer, n pending-peer\" 并提示 \"Please fix unhealthy regions before other operations.\"，则说明当前集群中有 region 处在异常状态，应先排除相应异常状态，并再次检查结果为 \"All regions are healthy\" 后再继续升级。\n\n## 3. 升级 TiDB 集群\n\n本部分介绍如何滚动升级 TiDB 集群以及如何进行升级后的验证。\n\n### 3.1 将集群升级到指定版本\n\n升级的方式有两种：不停机升级和停机升级。TiUP Cluster 默认的升级 TiDB 集群的方式是不停机升级，即升级过程中集群仍然可以对外提供服务。升级时会对各节点逐个迁移 leader 后再升级和重启，因此对于大规模集群需要较长时间才能完成整个升级操作。如果业务有维护窗口可供数据库停机维护，则可以使用停机升级的方式快速进行升级操作。\n\n#### 不停机升级\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster upgrade <cluster-name> <version>\n```\n\n以升级到 v8.5.0 版本为例：\n\n{{< copyable \"shell-regular\" >}}\n\n```\ntiup cluster upgrade <cluster-name> v8.5.0\n```\n\n> **注意：**\n>\n> - 滚动升级会逐个升级所有的组件。升级 TiKV 期间，会逐个将 TiKV 上的所有 leader 切走再停止该 TiKV 实例。默认超时时间为 5 分钟（300 秒），超时后会直接停止该实例。\n> - 使用 `--force` 参数可以在不驱逐 leader 的前提下快速升级集群至新版本，但是该方式会忽略所有升级中的错误，在升级失败后得不到有效提示，请谨慎使用。\n> - 如果希望保持性能稳定，则需要保证 TiKV 上的所有 leader 驱逐完成后再停止该 TiKV 实例，可以指定 `--transfer-timeout` 为一个更大的值，如 `--transfer-timeout 3600`，单位为秒。\n> - 如需将 TiFlash 从 v5.3.0 之前的版本升级到 v5.3.0 及之后的版本，必须进行 TiFlash 的停机升级，且 TiUP 版本小于 v1.12.0。具体升级步骤，请参考[使用 TiUP 升级](/tiflash-upgrade-guide.md#使用-tiup-升级)。\n\n#### 升级时指定组件版本\n\n从 tiup-cluster v1.14.0 开始，支持在升级集群的时候指定其中某些组件到特定版本。指定的组件在后续升级中保持固定版本，除非重新指定版本。\n\n> **注意：**\n>\n> 对于 TiDB、TiKV、PD、TiCDC 等共用版本号的组件，尚未有完整的测试保证它们在跨版本混合部署的场景下能正常工作。请仅在测试场景或在[获取支持](/support.md)的情况下使用此配置。\n\n```shell\ntiup cluster upgrade -h | grep \"version\"\n      --alertmanager-version string        Fix the version of alertmanager and no longer follows the cluster version.\n      --blackbox-exporter-version string   Fix the version of blackbox-exporter and no longer follows the cluster version.\n      --cdc-version string                 Fix the version of cdc and no longer follows the cluster version.\n      --ignore-version-check               Ignore checking if target version is bigger than current version.\n      --node-exporter-version string       Fix the version of node-exporter and no longer follows the cluster version.\n      --pd-version string                  Fix the version of pd and no longer follows the cluster version.\n      --tidb-dashboard-version string      Fix the version of tidb-dashboard and no longer follows the cluster version.\n      --tiflash-version string             Fix the version of tiflash and no longer follows the cluster version.\n      --tikv-cdc-version string            Fix the version of tikv-cdc and no longer follows the cluster version.\n      --tikv-version string                Fix the version of tikv and no longer follows the cluster version.\n      --tiproxy-version string             Fix the version of tiproxy and no longer follows the cluster version.\n```\n\n#### 停机升级\n\n在停机升级前，首先需要将整个集群关停。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster stop <cluster-name>\n```\n\n之后通过 `upgrade` 命令添加 `--offline` 参数来进行停机升级，其中 `<cluster-name>` 为集群名，`<version>` 为升级的目标版本，例如 `v8.5.0`。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster upgrade <cluster-name> <version> --offline\n```\n\n升级完成后集群不会自动启动，需要使用 `start` 命令来启动集群。\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster start <cluster-name>\n```\n\n### 3.2 升级后验证\n\n执行 `display` 命令来查看最新的集群版本 `TiDB Version`：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster display <cluster-name>\n```\n\n```\nCluster type:       tidb\nCluster name:       <cluster-name>\nCluster version:    v8.5.0\n```\n\n## 4. 升级 FAQ\n\n本部分介绍使用 TiUP 升级 TiDB 集群遇到的常见问题。\n\n### 4.1 升级时报错中断，处理完报错后，如何继续升级\n\n重新执行 `tiup cluster upgrade` 命令进行升级，升级操作会重启之前已经升级完成的节点。如果不希望重启已经升级过的节点，可以使用 `replay` 子命令来重试操作，具体方法如下：\n\n1. 使用 `tiup cluster audit` 命令查看操作记录：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster audit\n    ```\n\n    在其中找到失败的升级操作记录，并记下该操作记录的 ID，下一步中将使用 `<audit-id>` 表示操作记录 ID 的值。\n\n2. 使用 `tiup cluster replay <audit-id>` 命令重试对应操作：\n\n    {{< copyable \"shell-regular\" >}}\n\n    ```shell\n    tiup cluster replay <audit-id>\n    ```\n\n### 4.2 升级到 v6.2.0 及以上版本时，如何解决升级卡住的问题\n\n从 v6.2.0 开始，TiDB 默认开启[并发 DDL 框架](/ddl-introduction.md#tidb-在线-ddl-异步变更的原理)执行并发 DDL。该框架改变了 DDL 作业存储方式，由 KV 队列变为表队列。这一变化可能会导致部分升级场景卡住。下面是一些会触发该问题的场景及解决方案：\n\n- 加载插件导致的卡住\n\n    升级过程中加载部分插件时需要执行 DDL 语句，此时会卡住升级。\n\n    **解决方案**：升级过程中避免加载插件。待升级完成后再执行插件加载。\n\n- 使用 `kill -9` 命令停机升级导致的卡住\n\n    - 预防措施：避免使用 `kill -9` 命令停机升级。如需使用，应在 2 分钟后再启动新版本 TiDB 节点。\n    - 如果升级已经被卡住：重启受影响的 TiDB 节点。如果问题刚发生，建议等待 2 分钟后再重启。\n\n- DDL Owner 变更导致的卡住\n\n    在多 TiDB 实例场景升级时，网络或机器故障可能引起 DDL Owner 变更。如果此时存在未完成的升级阶段 DDL 语句，升级可能会卡住。\n\n    **解决方案**：\n\n    1. 先 Kill 卡住的 TiDB 节点（避免使用 `kill -9`）。\n    2. 重新启动新版本 TiDB 节点。\n\n### 4.3 升级过程中 evict leader 等待时间过长，如何跳过该步骤快速升级\n\n可以指定 `--force`，升级时会跳过 `PD transfer leader` 和 `TiKV evict leader` 过程，直接重启并升级版本，对线上运行的集群性能影响较大。命令如下，其中 `<version>` 为升级的目标版本，例如 `v8.5.0`：\n\n{{< copyable \"shell-regular\" >}}\n\n```shell\ntiup cluster upgrade <cluster-name> <version> --force\n```\n\n### 4.4 升级完成后，如何更新 pd-ctl 等周边工具版本\n\n可通过 TiUP 安装对应版本的 `ctl` 组件来更新相关工具版本：\n\n{{< copyable \"\" >}}\n\n```\ntiup install ctl:v8.5.0\n```\n"
        },
        {
          "name": "user-account-management.md",
          "type": "blob",
          "size": 9.474609375,
          "content": "---\ntitle: TiDB 用户账户管理\naliases: ['/docs-cn/dev/user-account-management/','/docs-cn/dev/reference/security/user-account-management/']\nsummary: TiDB 用户账户管理主要包括用户名和密码设置、添加用户、删除用户、保留用户账户、设置资源限制、设置密码、忘记密码处理和刷新权限。用户可以通过 SQL 语句或图形化界面工具进行用户管理，同时可以使用 `FLUSH PRIVILEGES` 命令立即生效修改。 TiDB 在数据库初始化时会生成一个默认账户。\n---\n\n# TiDB 用户账户管理\n\n本文档主要介绍如何管理 TiDB 用户账户。\n\n要快速了解 TiDB 如何进行认证与赋权并创建与管理用户账户，建议先观看下面的培训视频（时长 22 分钟）。注意本视频只作为学习参考，如需了解具体的用户账户管理方法，请参考本文档的内容。\n\n<video src=\"https://download.pingcap.com/docs-cn%2FLesson11_security.mp4\" width=\"600px\" height=\"450px\" controls=\"controls\" poster=\"https://download.pingcap.com/docs-cn/poster_lesson11.png\"></video>\n\n## 用户名和密码\n\nTiDB 将用户账户存储在 [`mysql.user`](/mysql-schema/mysql-schema-user.md) 系统表里面。每个账户由用户名和 host 作为标识。每个账户可以设置一个密码。每个用户名最长为 32 个字符。\n\n通过 MySQL 客户端连接到 TiDB 服务器，通过指定的账户和密码登录：\n\n```shell\nmysql --port 4000 --user xxx --password\n```\n\n使用缩写的命令行参数则是：\n\n```shell\nmysql -P 4000 -u xxx -p\n```\n\n## 添加用户\n\n添加用户有两种方式：\n\n* 通过标准的用户管理的 SQL 语句创建用户以及授予权限，比如 [`CREATE USER`](/sql-statements/sql-statement-create-user.md) 和 [`GRANT`](/sql-statements/sql-statement-grant-privileges.md)。\n* 直接通过[`INSERT`](/sql-statements/sql-statement-insert.md)、[`UPDATE`](/sql-statements/sql-statement-update.md) 和 [`DELETE`](/sql-statements/sql-statement-delete.md) 操作授权表，然后执行 [`FLUSH PRIVILEGES`](/sql-statements/sql-statement-flush-privileges.md)。不推荐使用这种方式添加或修改用户，因为容易导致修改不完整。\n\n除以上两种方法外，你还可以使用[第三方图形化界面工具](/develop/dev-guide-third-party-support.md#gui)来添加用户。\n\n```sql\nCREATE USER [IF NOT EXISTS] user [IDENTIFIED BY 'auth_string'];\n```\n\n设置登录密码后，`auth_string` 会被 TiDB 加密并存储在 [`mysql.user`](/mysql-schema/mysql-schema-user.md) 表中。\n\n```sql\nCREATE USER 'test'@'127.0.0.1' IDENTIFIED BY 'xxx';\n```\n\nTiDB 的用户账户名由一个用户名和一个主机名组成。账户名的语法为 `'user_name'@'host_name'`。\n\n- `user_name` 大小写敏感。\n- `host_name` 可以是一个主机名或 IP 地址。主机名或 IP 地址中允许使用通配符 `%` 和 `_`。例如，名为 `'%'` 的主机名可以匹配所有主机，`'192.168.1.%'` 可以匹配子网中的所有主机。\n\nhost 支持模糊匹配，比如：\n\n```sql\nCREATE USER 'test'@'192.168.10.%';\n```\n\n允许 `test` 用户从 `192.168.10` 子网的任何一个主机登录。\n\n如果没有指定 host，则默认是所有 IP 均可登录。如果没有指定密码，默认为空：\n\n```sql\nCREATE USER 'test';\n```\n\n等价于：\n\n```sql\nCREATE USER 'test'@'%' IDENTIFIED BY '';\n```\n\n为一个不存在的用户授权时，是否会自动创建用户的行为受 [`sql_mode`](/system-variables.md#sql_mode) 影响。如果 `sql_mode` 中包含 `NO_AUTO_CREATE_USER`，则 `GRANT` 不会自动创建用户并报错。\n\n假设 `sql_mode` 不包含 `NO_AUTO_CREATE_USER`，下面的例子用 `CREATE USER` 和 `GRANT` 语句创建了四个账户：\n\n```sql\nCREATE USER 'finley'@'localhost' IDENTIFIED BY 'some_pass';\n```\n\n```sql\nGRANT ALL PRIVILEGES ON *.* TO 'finley'@'localhost' WITH GRANT OPTION;\n```\n\n```sql\nCREATE USER 'finley'@'%' IDENTIFIED BY 'some_pass';\n```\n\n```sql\nGRANT ALL PRIVILEGES ON *.* TO 'finley'@'%' WITH GRANT OPTION;\n```\n\n```sql\nCREATE USER 'admin'@'localhost' IDENTIFIED BY 'admin_pass';\n```\n\n```sql\nGRANT RELOAD,PROCESS ON *.* TO 'admin'@'localhost';\n```\n\n```sql\nCREATE USER 'dummy'@'localhost';\n```\n\n使用 [`SHOW GRANTS`](/sql-statements/sql-statement-show-grants.md) 可以看到为一个用户授予的权限：\n\n```sql\nSHOW GRANTS FOR 'admin'@'localhost';\n```\n\n```\n+-----------------------------------------------------+\n| Grants for admin@localhost                          |\n+-----------------------------------------------------+\n| GRANT RELOAD, PROCESS ON *.* TO 'admin'@'localhost' |\n+-----------------------------------------------------+\n```\n\n使用 [`SHOW CREATE USER`](/sql-statements/sql-statement-show-create-user.md) 查看用户的定义语句：\n\n```sql\nSHOW CREATE USER 'admin'@'localhost';\n```\n\n```\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| CREATE USER for admin@localhost                                                                                                                                                                                                      |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| CREATE USER 'admin'@'localhost' IDENTIFIED WITH 'mysql_native_password' AS '*14E65567ABDB5135D0CFD9A70B3032C179A49EE7' REQUIRE NONE PASSWORD EXPIRE DEFAULT ACCOUNT UNLOCK PASSWORD HISTORY DEFAULT PASSWORD REUSE INTERVAL DEFAULT  |\n+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n1 row in set (0.00 sec)\n```\n\n## 删除用户\n\n使用 [`DROP USER`](/sql-statements/sql-statement-drop-user.md) 语句可以删除用户，例如：\n\n```sql\nDROP USER 'test'@'localhost';\n```\n\n这个操作会清除用户在 [`mysql.user`](/mysql-schema/mysql-schema-user.md) 表里面的记录项，并且清除在授权表里面的相关记录。\n\n## 保留用户账户\n\nTiDB 在数据库初始化时会生成一个 `'root'@'%'` 的默认账户。\n\n## 设置资源限制\n\nTiDB 可以利用资源组对用户消耗的资源进行限制，详情参见[使用资源管控 (Resource Control) 实现资源隔离](/tidb-resource-control.md)。\n\n## 设置密码\n\nTiDB 将密码存在 [`mysql.user`](/mysql-schema/mysql-schema-user.md) 系统表里面。只有拥有 `CREATE USER` 权限，或者拥有 `mysql` 数据库权限（`INSERT` 权限用于创建，`UPDATE` 权限用于更新）的用户才能够设置或修改密码。\n\n- 在 [`CREATE USER`](/sql-statements/sql-statement-create-user.md) 创建用户时通过 `IDENTIFIED BY` 指定密码：\n\n    ```sql\n    CREATE USER 'test'@'localhost' IDENTIFIED BY 'mypass';\n    ```\n\n- 为一个已存在的账户修改密码，可以通过 [`SET PASSWORD FOR`](/sql-statements/sql-statement-set-password.md) 或者 [`ALTER USER`](/sql-statements/sql-statement-alter-user.md) 语句完成：\n\n    ```sql\n    SET PASSWORD FOR 'root'@'%' = 'xxx';\n    ```\n\n    或者：\n\n    ```sql\n    ALTER USER 'test'@'localhost' IDENTIFIED BY 'mypass';\n    ```\n\n## 忘记 `root` 密码\n\n1. 修改 TiDB 配置文件：\n\n    1. 登录其中一台 tidb-server 实例所在的机器。\n    2. 进入 TiDB 节点的部署目录下的 `conf` 目录，找到 `tidb.toml` 配置文件。\n    3. 在配置文件的 [`security`](/tidb-configuration-file.md#security) 部分添加配置项 [`skip-grant-table`](/tidb-configuration-file.md)。如无 `security` 部分，则将以下两行内容添加至 `tidb.toml` 配置文件尾部：\n\n        ```\n        [security]\n        skip-grant-table = true\n        ```\n\n2. 终止该 tidb-server 的进程：\n\n    1. 查看 tidb-server 的进程：\n\n        ```bash\n        ps aux | grep tidb-server\n        ```\n\n    2. 找到 tidb-server 对应的进程 ID (PID) 并使用 `kill` 命令停掉该进程：\n\n        ```bash\n        kill -9 <pid>\n        ```\n\n3. 使用修改之后的配置启动 TiDB：\n\n    > **注意：**\n    >\n    > 设置 `skip-grant-table` 之后，启动 TiDB 进程会增加操作系统用户检查，只有操作系统的 `root` 用户才能启动 TiDB 进程。\n\n    1. 进入 TiDB 节点部署目录下的 `scripts` 目录。\n    2. 切换到操作系统 `root` 账号。\n    3. 在前台执行目录中的 `run_tidb.sh` 脚本。\n    4. 在新的终端窗口中使用 `root` 登录后修改密码：\n\n        ```bash\n        mysql -h 127.0.0.1 -P 4000 -u root\n        ```\n\n4. 停止运行 `run_tidb.sh` 脚本，并去掉第 1 步中在 TiDB 配置文件中添加的内容，等待 tidb-server 自启动。\n\n## `FLUSH PRIVILEGES`\n\n用户以及权限相关的信息都存储在 TiKV 服务器中，TiDB 在进程内部会缓存这些信息。一般通过 [`CREATE USER`](/sql-statements/sql-statement-create-user.md)、[`GRANT`](/sql-statements/sql-statement-grant-privileges.md) 等语句来修改相关信息时，可在整个集群迅速生效。如果遇到网络或者其它因素影响，由于 TiDB 会周期性地更新缓存信息，正常情况下，最多 15 分钟左右生效。\n\n如果授权表已被直接修改，则不会通知 TiDB 节点更新缓存，执行 [`FLUSH PRIVILEGES`](/sql-statements/sql-statement-flush-privileges.md) 可使改动立即生效。\n\n详情参见[权限管理](/privilege-management.md)。\n"
        },
        {
          "name": "user-defined-variables.md",
          "type": "blob",
          "size": 3.4892578125,
          "content": "---\ntitle: 用户自定义变量\nsummary: 本文介绍 TiDB 的用户自定义变量。\naliases: ['/docs-cn/dev/user-defined-variables/','/docs-cn/dev/reference/sql/language-structure/user-defined-variables/']\n---\n\n# 用户自定义变量\n\n> **警告：**\n>\n> 当前该功能为实验特性，不建议在生产环境中使用。\n\n本文介绍 TiDB 的用户自定义变量的概念，以及设置和读取用户自定义变量的方法。\n\n用户自定义变量格式为 `@var_name`。组成 `var_name` 的字符可以是任何能够组成标识符 (identifier) 的字符，包括数字 `0-9`、字母 `a-zA-Z`、下划线 `_`、美元符号 `$` 以及 UTF-8 字符。此外，还包括英文句号 `.`。用户自定义变量是大小写不敏感的。\n\n用户自定义变量跟 session 绑定，当前设置的用户变量只在当前连接中可见，其他客户端连接无法查看。\n\n## 设置用户自定义变量\n\n用 [`SET` 语句](/sql-statements/sql-statement-set-variable.md)可以设置用户自定义变量，语法为 `SET @var_name = expr [, @var_name = expr] ...;`。例如：\n\n```sql\nSET @favorite_db = 'TiDB';\n```\n\n```sql\nSET @a = 'a', @b = 'b', @c = 'c';\n```\n\n其中赋值符号还可以使用 `:=`。例如：\n\n```sql\nSET @favorite_db := 'TiDB';\n```\n\n赋值符号右边的内容可以是任意合法的表达式。例如：\n\n```sql\nSET @c = @a + @b;\n```\n\n```sql\nSET @c = b'1000001' + b'1000001';\n```\n\n## 读取用户自定义变量\n\n要读取一个用户自定义变量，可以使用 `SELECT` 语句查询：\n\n```sql\nSELECT @a1, @a2, @a3\n```\n\n```\n+------+------+------+\n| @a1  | @a2  | @a3  |\n+------+------+------+\n|    1 |    2 |    4 |\n+------+------+------+\n```\n\n还可以在 `SELECT` 语句中赋值：\n\n```sql\nSELECT @a1, @a2, @a3, @a4 := @a1+@a2+@a3;\n```\n\n```\n+------+------+------+--------------------+\n| @a1  | @a2  | @a3  | @a4 := @a1+@a2+@a3 |\n+------+------+------+--------------------+\n|    1 |    2 |    4 |                  7 |\n+------+------+------+--------------------+\n```\n\n其中变量 `@a4` 在被修改或关闭连接之前，值始终为 `7`。\n\n如果设置用户变量时用了十六进制字面量或者二进制字面量，TiDB 会把它当成二进制字符串。如果要将其设置成数字，那么可以手动加上 `CAST` 转换，或者在表达式中使用数字的运算符：\n\n```sql\nSET @v1 = b'1000001';\nSET @v2 = b'1000001'+0;\nSET @v3 = CAST(b'1000001' AS UNSIGNED);\n```\n\n```sql\nSELECT @v1, @v2, @v3;\n```\n\n```\n+------+------+------+\n| @v1  | @v2  | @v3  |\n+------+------+------+\n| A    | 65   | 65   |\n+------+------+------+\n```\n\n如果获取一个没有设置过的变量，会返回一个 NULL：\n\n```sql\nSELECT @not_exist;\n```\n\n```\n+------------+\n| @not_exist |\n+------------+\n| NULL       |\n+------------+\n```\n\n除了 `SELECT` 读取用户自定义变量以外，常见的用法还有 `PREPARE` 语句，例如：\n\n```sql\nSET @s = 'SELECT SQRT(POW(?,2) + POW(?,2)) AS hypotenuse';\nPREPARE stmt FROM @s;\nSET @a = 6;\nSET @b = 8;\nEXECUTE stmt USING @a, @b;\n```\n\n```\n+------------+\n| hypotenuse |\n+------------+\n|         10 |\n+------------+\n```\n\n用户自定义变量的内容不会在 SQL 语句中被当成标识符，例如：\n\n```sql\nSELECT * FROM t;\n```\n\n```\n+---+\n| a |\n+---+\n| 1 |\n+---+\n```\n\n```sql\nSET @col = \"`a`\";\nSELECT @col FROM t;\n```\n\n```\n+------+\n| @col |\n+------+\n| `a`  |\n+------+\n```\n\n## MySQL 兼容性\n\n除 `SELECT ... INTO <variable>` 外，MySQL 和 TiDB 支持的语法相同。\n\n更多细节，请参考 [MySQL 文档](https://dev.mysql.com/doc/refman/8.0/en/user-variables.html)。\n"
        },
        {
          "name": "vector-search",
          "type": "tree",
          "content": null
        },
        {
          "name": "views.md",
          "type": "blob",
          "size": 7.203125,
          "content": "---\ntitle: 视图\naliases: ['/docs-cn/dev/views/','/docs-cn/dev/reference/sql/view/']\nsummary: TiDB 支持视图，视图是虚拟表，结构由创建时的 SELECT 语句定义。使用视图可保证数据安全，简化复杂查询。查询视图类似查询表，TiDB 执行查询时会展开视图。可通过 SHOW CREATE TABLE 或 SHOW CREATE VIEW 查看视图创建语句及相关信息。也可查询 INFORMATION_SCHEMA.VIEWS 表或访问 HTTP API 获取视图元信息。视图有局限性，不支持物化视图，且为只读视图，不支持写入操作。已创建的视图仅支持 DROP 操作。\n---\n\n# 视图\n\nTiDB 支持视图，视图是一张虚拟表，该虚拟表的结构由创建视图时的 `SELECT` 语句定义。使用视图一方面可以对用户只暴露安全的字段及数据，进而保证底层表的敏感字段及数据的安全。另一方面，将频繁出现的复杂查询定义为视图，可以使复杂查询更加简单便捷。\n\n## 查询视图\n\n查询一个视图和查询一张普通表类似。但是 TiDB 在真正执行查询视图时，会将视图展开成创建视图时定义的 `SELECT` 语句，进而执行展开后的查询语句。\n\n## 查看视图的相关信息\n\n通过以下方式，可以查看 view 相关的信息。\n\n### 使用 `SHOW CREATE TABLE view_name` 或 `SHOW CREATE VIEW view_name` 语句\n\n示例：\n\n{{< copyable \"sql\" >}}\n\n```sql\nshow create view v;\n```\n\n使用该语句可以查看 view 对应的创建语句，及创建 view 时对应的 `character_set_client` 及 `collation_connection` 系统变量值。\n\n```sql\n+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n| View | Create View                                                                                                                                                         | character_set_client | collation_connection |\n+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n| v    | CREATE ALGORITHM=UNDEFINED DEFINER=`root`@`127.0.0.1` SQL SECURITY DEFINER VIEW `v` (`a`) AS SELECT `s`.`a` FROM `test`.`t` LEFT JOIN `test`.`s` ON `t`.`a`=`s`.`a` | utf8                 | utf8_general_ci      |\n+------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------+----------------------+\n1 row in set (0.00 sec)\n```\n\n### 查询 `INFORMATION_SCHEMA.VIEWS` 表\n\n示例：\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from information_schema.views;\n```\n\n通过查询该表可以查看 view 的相关元信息，如 `TABLE_CATALOG`、`TABLE_SCHEMA`、`TABLE_NAME`、`VIEW_DEFINITION`、`CHECK_OPTION`、`IS_UPDATABLE`、`DEFINER`、`SECURITY_TYPE`、`CHARACTER_SET_CLIENT`、`COLLATION_CONNECTION` 等。\n\n```sql\n+---------------+--------------+------------+------------------------------------------------------------------------+--------------+--------------+----------------+---------------+----------------------+----------------------+\n| TABLE_CATALOG | TABLE_SCHEMA | TABLE_NAME | VIEW_DEFINITION                                                        | CHECK_OPTION | IS_UPDATABLE | DEFINER        | SECURITY_TYPE | CHARACTER_SET_CLIENT | COLLATION_CONNECTION |\n+---------------+--------------+------------+------------------------------------------------------------------------+--------------+--------------+----------------+---------------+----------------------+----------------------+\n| def           | test         | v          | SELECT `s`.`a` FROM `test`.`t` LEFT JOIN `test`.`s` ON `t`.`a`=`s`.`a` | CASCADED     | NO           | root@127.0.0.1 | DEFINER       | utf8                 | utf8_general_ci      |\n+---------------+--------------+------------+------------------------------------------------------------------------+--------------+--------------+----------------+---------------+----------------------+----------------------+\n1 row in set (0.00 sec)\n```\n\n### 查询 HTTP API\n\n示例：\n\n{{< copyable \"\" >}}\n\n```\ncurl http://127.0.0.1:10080/schema/test/v\n```\n\n通过访问 `http://{TiDBIP}:10080/schema/{db}/{view}` 可以得到对应 view 的所有元信息。\n\n```\n{\n \"id\": 122,\n \"name\": {\n  \"O\": \"v\",\n  \"L\": \"v\"\n },\n \"charset\": \"utf8\",\n \"collate\": \"utf8_general_ci\",\n \"cols\": [\n  {\n   \"id\": 1,\n   \"name\": {\n    \"O\": \"a\",\n    \"L\": \"a\"\n   },\n   \"offset\": 0,\n   \"origin_default\": null,\n   \"default\": null,\n   \"default_bit\": null,\n   \"default_is_expr\": false,\n   \"generated_expr_string\": \"\",\n   \"generated_stored\": false,\n   \"dependences\": null,\n   \"type\": {\n    \"Tp\": 0,\n    \"Flag\": 0,\n    \"Flen\": 0,\n    \"Decimal\": 0,\n    \"Charset\": \"\",\n    \"Collate\": \"\",\n    \"Elems\": null\n   },\n   \"state\": 5,\n   \"comment\": \"\",\n   \"hidden\": false,\n   \"version\": 0\n  }\n ],\n \"index_info\": null,\n \"fk_info\": null,\n \"state\": 5,\n \"pk_is_handle\": false,\n \"is_common_handle\": false,\n \"comment\": \"\",\n \"auto_inc_id\": 0,\n \"auto_id_cache\": 0,\n \"auto_rand_id\": 0,\n \"max_col_id\": 1,\n \"max_idx_id\": 0,\n \"update_timestamp\": 416801600091455490,\n \"ShardRowIDBits\": 0,\n \"max_shard_row_id_bits\": 0,\n \"auto_random_bits\": 0,\n \"pre_split_regions\": 0,\n \"partition\": null,\n \"compression\": \"\",\n \"view\": {\n  \"view_algorithm\": 0,\n  \"view_definer\": {\n   \"Username\": \"root\",\n   \"Hostname\": \"127.0.0.1\",\n   \"CurrentUser\": false,\n   \"AuthUsername\": \"root\",\n   \"AuthHostname\": \"%\"\n  },\n  \"view_security\": 0,\n  \"view_select\": \"SELECT `s`.`a` FROM `test`.`t` LEFT JOIN `test`.`s` ON `t`.`a`=`s`.`a`\",\n  \"view_checkoption\": 1,\n  \"view_cols\": null\n },\n \"sequence\": null,\n \"Lock\": null,\n \"version\": 3,\n \"tiflash_replica\": null\n}\n```\n\n## 示例\n\n以下例子将创建一个视图，并在该视图上进行查询，最后删除该视图。\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table t(a int, b int);\n```\n\n```\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\ninsert into t values(1, 1),(2,2),(3,3);\n```\n\n```\nQuery OK, 3 rows affected (0.00 sec)\nRecords: 3  Duplicates: 0  Warnings: 0\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate table s(a int);\n```\n\n```\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\ninsert into s values(2),(3);\n```\n\n```\nQuery OK, 2 rows affected (0.01 sec)\nRecords: 2  Duplicates: 0  Warnings: 0\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\ncreate view v as select s.a from t left join s on t.a = s.a;\n```\n\n```\nQuery OK, 0 rows affected (0.01 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\nselect * from v;\n```\n\n```\n+------+\n| a    |\n+------+\n| NULL |\n|    2 |\n|    3 |\n+------+\n3 rows in set (0.00 sec)\n```\n\n{{< copyable \"sql\" >}}\n\n```sql\ndrop view v;\n```\n\n```\nQuery OK, 0 rows affected (0.02 sec)\n```\n\n## 局限性\n\n目前 TiDB 中的视图有以下局限性：\n\n- 不支持物化视图。\n- TiDB 中视图为只读视图，不支持对视图进行 `UPDATE`、`INSERT`、`DELETE`、`TRUNCATE` 等写入操作。\n- 对已创建的视图仅支持 `DROP` 的 DDL 操作，即 `DROP [VIEW | TABLE]`。\n\n## 扩展阅读\n\n- [创建视图](/sql-statements/sql-statement-create-view.md)\n- [删除视图](/sql-statements/sql-statement-drop-view.md)\n"
        },
        {
          "name": "wrong-index-solution.md",
          "type": "blob",
          "size": 4.3125,
          "content": "---\ntitle: 错误索引的解决方案\nsummary: 了解如何处理错误索引问题。\naliases: ['/docs-cn/dev/wrong-index-solution/']\n---\n\n# 错误索引的解决方案\n\n在观察到某个查询的执行速度达不到预期时，可能是它的索引使用有误。\n\n可能造成 TiDB 优化器选择非预期索引的原因包括：\n\n- **统计信息过时**：优化器依赖统计信息来估算查询成本。如果统计信息过时，可能导致优化器作出次优选择。\n- **统计信息不匹配**：即使统计信息是最新的，也可能无法准确反映数据分布情况，导致成本估算偏差。\n- **成本计算不准确**：当查询的结构复杂或数据分布不均时，优化器有可能会错误地估算使用某个索引的成本。\n- **存储引擎选择不当**：在某些场景下，优化器选择的存储引擎可能不适合当前查询。\n- **函数下推限制**：部分函数或操作无法下推到存储引擎执行，可能会影响查询性能。\n\n## 统计信息健康度\n\n可以先使用[表的健康度信息](/statistics.md#表的健康度信息)来查看统计信息的健康度。根据健康度可以分为以下两种情况处理。\n\n### 健康度较低\n\n这意味着距离 TiDB 上次执行 `ANALYZE` 已经很久了。这时可以先使用 `ANALYZE` 命令对统计信息进行更新。更新之后如果仍在使用错误的索引，可以参考下一小节。\n\n### 健康度接近 100%\n\n这时意味着刚刚结束 `ANALYZE` 命令或者结束后不久。这时可能和 TiDB 对行数的估算逻辑有关。\n\n对于等值查询，错误索引可能是由 [Count-Min Sketch](/statistics.md#count-min-sketch) 引起的。这时可以先检查是不是这种特殊情况，然后进行对应的处理。\n\n如果经过检查发现不是上面的可能情况，可以使用 [Optimizer Hints](/optimizer-hints.md#use_indext1_name-idx1_name--idx2_name-) 中提到的 `USE_INDEX` 或者 `use index` 来强制选择索引。同时也可以使用[执行计划管理](/sql-plan-management.md)中提到的方式来非侵入地更改查询的行为。\n\n### 其他情况\n\n除去上述情况外，也存在因为数据的更新导致现有所有索引都不再适合的情况。这时就需要对条件和数据分布进行分析，查看是否有新的索引可以加快查询速度，然后使用 [`ADD INDEX`](/sql-statements/sql-statement-add-index.md) 命令增加新的索引。\n\n## 统计信息不匹配\n\n当数据分布特别不均衡时，统计信息可能无法准确反映实际数据分布。此时，可以尝试配置 [`ANALYZE TABLE`](/sql-statements/sql-statement-analyze-table.md) 语句的不同选项来提高统计信息的准确性，以更准确地匹配索引。\n\n例如，假设你有一个 `orders` 表，其中 `customer_id` 列上有一个索引，但超过 50% 的订单都具有相同的 `customer_id`。对于该表，统计信息可能无法很好地反映数据分布，从而影响查询性能。\n\n## 成本信息\n\n如需查看执行成本的详细信息，可以在执行 [`EXPLAIN`](/sql-statements/sql-statement-explain.md) 和 [`EXPLAIN ANALYZE`](/sql-statements/sql-statement-explain-analyze.md) 语句时带上 `FORMAT=verbose` 选项。通过这些信息，可以了解不同执行路径之间的成本差异。\n\n## 引擎选择\n\n默认情况下，TiDB 会基于成本估算选择使用 TiKV 或 TiFlash 访问数据表。你可以通过配置 Engine 隔离的方式，尝试使用不同的存储引擎执行同一查询。\n\n更多信息，请参考 [Engine 隔离](/tiflash/use-tidb-to-read-tiflash.md#engine-隔离)。\n\n## 函数下推\n\n为了提升查询性能，TiDB 会将某些函数下推到 TiKV 或 TiFlash 存储引擎中执行。然而，部分函数不支持下推，这可能会限制可用的执行计划，进而影响查询性能。\n\n关于支持下推的表达式，请参考 [TiKV 支持的下推计算](/functions-and-operators/expressions-pushed-down.md) 和 [TiFlash 支持的下推计算](/tiflash/tiflash-supported-pushdown-calculations.md)。\n\n需要注意的是，你也可以禁用特定表达式的下推。更多信息，请参考[优化规则和表达式下推的黑名单](/blocklist-control-plan.md)。\n\n## 另请参阅\n\n- [常规统计信息](/statistics.md)\n- [索引选择](/choose-index.md)\n- [Optimizer Hints](/optimizer-hints.md)\n- [执行计划管理 (SPM)](/sql-plan-management.md)"
        }
      ]
    }
  ]
}