{
  "metadata": {
    "timestamp": 1736568566419,
    "page": 601,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYwOQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "joeknock90/Single-GPU-Passthrough",
      "stars": 1513,
      "defaultBranch": "master",
      "files": [
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.623046875,
          "content": "# Single GPU Passthrough on Linux  \nThis guide is to help people through the process of using GPU Passthrough via libvirt/virt-manager on systems that only have one GPU. \n\n## Special Thanks to:\n### [The Passthrough post](https://passthroughpo.st)\nFor hosting news and information about VFIO passthrough, and for the libvirt/qemu hook helper in this guide.\n\n### andre-ritcher\nFor providing the vfio-pci-bind tool. A tool that is no longer used in this guide, but was previously used and he still deserves thanks.\n\n### Matoking\nFor the Nvidia ROM patcher. Making passing the boot gpu to the VM without GPU bios problems.\nPatching the rom is no longer required but I never would have written this guide without the original work so I'm keeping them here. \n\n### Sporif\nFor diagnosing, developing, and testing methods to successfully rebind the EFI-Framebuffer when passing the video card back to the host OS.\n\n### droidman\nFor instructions on manually editing the vBIOS hex for use with VFIO passthrough\n\n### [Yuri Alek](https://gitlab.com/YuriAlek/vfio)\nA guide that is no doubt better than mine. Learning a few things from his implementation that can help me out a bit. This guide depends on libvirt at the base where as his has implementations that do not. \n\n#### So many other people and organizations I need to thank. If feel your name should be here, please contact me. Credit where credit is due is very important to me, and to making the Linux community a better place.\n\n## Contents\n\n1. [Disclaimer](#disclaimer) \n2. [Background](#background)\n3. [Advantages](#advantages)\n4. [Disadvantages](#disadvantages)\n3. [Prerequisites](#prerequisites) and [Assumptions](#assumptions)\n4. [Procedure](#procedure)\n\n# Disclaimer\nYou are completely responsible for your hardware and software. This guide makes no guarentees that the process will work for you, or will not void your waranty on various parts or break your computer in some way. Everything from here on out is at your own risk. \n\n# Background\nHistorically, VFIO passthrough has been built on a very specific model. I.E.\n\n* 2 GPUs, 1 for the host, and one for the VM\n* 2 monitors *OR* a monitor with 2 inputs *OR* a KVM switch\n\nI personally, as well as some of you out there, might not have those things available. Maybe You've got a Mini-ITX build with no iGPU. Or maybe you're poor like me, and can't shell out for new computer components without some financial  planning before hand.\n\nWhatever your reason is. VFIO is still possible. But with caveats. Here's some advantages and disadvantages of this model.\n\nThis setup model is a lot like dual booting, without actually rebooting.\n\n# Advantages\n* As already stated, this model only requires one GPU\n* The ability to switch back and forth between different OSes with FULL use of a discrete graphics processor (Linux on Host with full GPU, Windows 10 Guest with Full GPU, MacOS guest with full GPU)\n* Bragging rights\n* Could be faster than dual booting (this depends on your system)\n* Using virtual disk images (like qcow) gives you management of snapshots, making breaking your guest os easy to recover from.\n\n# Disadvantages\n* Can only use one OS at a time.\n\t- Once the VM is running, it's basically like running that as your main OS. You  will be logged out of your user on the host, but will be unable to manage the host locally at all. You can still use ssh/vnc/xrdp to manage the host.\n* There are still some quirks (I need your help to iron these out!)\n* Using virtual disk images could be a performance hit\n\t- You can still use raw partitions/lvm/pass through raw disks, but loose the more robust snapshot and management features\n* If you DO have a second video card, solutions like looking-glass are WAYYY more convenient and need active testing and development.\n* All VMs must be run as root. There are security considerations to be made there. This model requires a level of risk acceptance.\n\nFor my personal use case. This model is worth it to me and it might be for you too!\n\n# Prerequisites and Assumptions\n\n## Assumptions\nThis guide is going to assume a few things\n\n1. You have a system capable of VFIO passthrough. I.E. a processors that supports IOMMU, sane IOMMU groups, and etc.\n2. I am going to start in a place where you have a working libvirt config, or qemu script, that boots a guest OS without PCI devices passed through.\n\nI am not going to cover the basic setup of VFIO passthrough here. There are a lot of guides out there that cover the process from beginning to end.\n\nWhat I will say is that using the [Arch Wiki][arch_wiki] is your best bet.\n\nFollow the instructions found [here][arch_wiki]\n\n[arch_wiki]: https://wiki.archlinux.org/index.php/PCI_passthrough_via_OVMF\n\n**Skip the Isolating the GPU section** We are not going to do that in this method as we still want the host to have access to it. I will cover this again in the procedure section.\n\n## Prerequisites\n\n1. A working Libvirt VM or Qemu script for your guest OS.\n2. IOMMU enabled and Sane IOMMU groups\n3. The Following Tools\n\t* (If using Libvirt) [The Libvirt Hook Helper](https://passthroughpo.st/simple-per-vm-libvirt-hooks-with-the-vfio-tools-hook-helper/)\n\t* (Optional) Another machine to SSH/VNC to your host with for testing might be useful\n\nWith all this ready. Let's move on to how to actually do this.\n\n# Procedure\n\n## Setting up Libvirt hooks\n\nUsing libvirt hooks will allow us to automatically run scripts before the VM is started and after the VM has stopped.\n\nUsing the instructions [here](https://passthroughpo.st/simple-per-vm-libvirt-hooks-with-the-vfio-tools-hook-helper/) to install the base scripts, you'll find a directory structure that now looks like this:\n\n```\n/etc/libvirt/hooks\n├── qemu <- The script that does the magic\n└── qemu.d\n    └── {VM Name}\n        ├── prepare\n        │   └── begin\n        │       └── start.sh\n        └── release\n            └── end\n                └── revert.sh\n```\n\nAnything in the directory ````/etc/libvirt/hooks/qemu.d/{VM Name}/prepare/begin```` will run when starting your VM\n\nAnything in the directory ````/etc/libvirt/hooks/qemu.d/{VM Name}/release/end```` will run when your VM is stopped\n\n### Libvirt Hook Scripts]\n#### Do not copy my scripts. Use them as a template, but write your own. \n\nI've made my start script ```/etc/libvirt/hooks/qemu.d/{VMName}/prepare/begin/start.sh```\n\n\n### Start Script\n```\n#!/bin/bash\n# Helpful to read output when debugging\nset -x\n\n# Stop display manager\nsystemctl stop display-manager.service\n## Uncomment the following line if you use GDM\n#killall gdm-x-session\n\n# Unbind VTconsoles\necho 0 > /sys/class/vtconsole/vtcon0/bind\necho 0 > /sys/class/vtconsole/vtcon1/bind\n\n# Unbind EFI-Framebuffer\necho efi-framebuffer.0 > /sys/bus/platform/drivers/efi-framebuffer/unbind\n\n# Avoid a Race condition by waiting 2 seconds. This can be calibrated to be shorter or longer if required for your system\nsleep 2\n\n# Unbind the GPU from display driver\nvirsh nodedev-detach pci_0000_0c_00_0\nvirsh nodedev-detach pci_0000_0c_00_1\n\n# Load VFIO Kernel Module  \nmodprobe vfio-pci  \n```\nNOTE: Gnome/GDM users. You have to uncommment the line ````killall gdm-x-session```` in order for the script to work properly. Killing GDM does not destroy all users sessions like other display managers do. \n\n\n### VM Stop script\nMy stop script is ```/etc/libvirt/hooks/qemu.d/{VMName}/release/end/revert.sh```\n```\n#!/bin/bash\nset -x\n  \n# Re-Bind GPU to Nvidia Driver\nvirsh nodedev-reattach pci_0000_0c_00_1\nvirsh nodedev-reattach pci_0000_0c_00_0\n\n# Reload nvidia modules\nmodprobe nvidia\nmodprobe nvidia_modeset\nmodprobe nvidia_uvm\nmodprobe nvidia_drm\n\n# Rebind VT consoles\necho 1 > /sys/class/vtconsole/vtcon0/bind\n# Some machines might have more than 1 virtual console. Add a line for each corresponding VTConsole\n#echo 1 > /sys/class/vtconsole/vtcon1/bind\n\nnvidia-xconfig --query-gpu-info > /dev/null 2>&1\necho \"efi-framebuffer.0\" > /sys/bus/platform/drivers/efi-framebuffer/bind\n\n# Restart Display Manager\nsystemctl start display-manager.service\n\n```\n\n# Running the VM\nWhen running the VM, the scripts should now automatically stop your display manager, unbind your GPU from all drivers currently using it and pass control over the libvirt. Libvirt handles binding the card to VFIO-PCI automatically. \n\nWhen the VM is stopped, Libvirt will also handle removing the card from VFIO-PCI. The stop script will then rebind the card to Nvidia and SHOULD rebind your vtconsoles and EFI-Framebuffer. \n\n# Troubleshooting\nFirst of all. If you ask for help, then tell me you skipped some step... I'm gonna be a little annoyed. So before moving on to troubleshooting, and DEFINATELY before asking for help, make sure you've follwed ALL of the steps of this guide. They are all here for a reason. \n\n## Logs\nLogs can be found under /var/log/libvirt/qemu/[VM name].log\n\n## Common issues\n### Black Screen on VM Activation\n1. Make sure you've removed the Spice Video and QXL video adapter on the VM\n2. It can be extremely helpful to SSH into the host to check if scripts have executed properly, and that the VM is running. Try these in this order.\n\t1. SSH into the host, and manually run the start script. If the start script runs properly, the host monitors should go completely black, and the terminal should return you to the prompt. \n\t2. If all goes well there, try running the vm manually using `sudo virsh start {vmname}`\n\t3. If there is a problem here, typically the command will hang. That would signify a problem with the VM libvirt configuration. \n\t4. If you are returned to the prompt, check if the vm is in a running state by using `sudo virsh list`\n\t5. If it's running fine, and you've made sure that you are not having the issue in step 1 and 2, yell at me in the issue tracker or reddit\n\n### Audio\nCheck out the ArchWIKI entry for tips on audio. I've used both Pulseaudio Passthrough but am currently using a Scream IVSHMEM device on the VM. \n\n## NOTE\nEither of these will require a user systemd service. You can keep user systemd services running by enabling linger for your user account like so:\n`sudo loginctl enable-linger {username}`\nThis will keep services running even when your account is not logged in. I do not know the security implications of this. My assumption is that it's not a great idea, but oh well. \n\n# Tips and Tricks\n## Personal Touches\nHere's a few things I do to make managing the host easier. \n\n1. Start a VNC server on the host in the start script\n2. Set pulseaudio volume to 100%\n3. Anything you want the host to do upon VM activation.\n\n\n# Let me know what works and what doesnt!\nLet me know your success and failure stories. \n\n\n#### [Fuel my coffee addiction or help me test new hardware](https://www.paypal.com/donate?business=87AQBT5TGFRJS&item_name=Github+Testing&currency_code=USD)\n#### Always appreciated, never required.\n\n"
        },
        {
          "name": "example-revert.sh",
          "type": "blob",
          "size": 0.62109375,
          "content": "#!/bin/bash\nset -x\n  \n# Re-Bind GPU to Nvidia Driver\nvirsh nodedev-reattach pci_0000_0c_00_1\nvirsh nodedev-reattach pci_0000_0c_00_0\n\n# Reload nvidia modules\nmodprobe nvidia\nmodprobe nvidia_modeset\nmodprobe nvidia_uvm\nmodprobe nvidia_drm\n\n# Rebind VT consoles\necho 1 > /sys/class/vtconsole/vtcon0/bind\n# Some machines might have more than 1 virtual console. Add a line for each corresponding VTConsole\n#echo 1 > /sys/class/vtconsole/vtcon1/bind\n\nnvidia-xconfig --query-gpu-info > /dev/null 2>&1\necho \"efi-framebuffer.0\" > /sys/bus/platform/drivers/efi-framebuffer/bind\n\n# Restart Display Manager\nsystemctl start display-manager.service\n"
        },
        {
          "name": "example-start.sh",
          "type": "blob",
          "size": 0.626953125,
          "content": "#!/bin/bash\n# Helpful to read output when debugging\nset -x\n\n# Stop display manager\nsystemctl stop display-manager.service\n## Uncomment the following line if you use GDM\n#killall gdm-x-session\n\n# Unbind VTconsoles\necho 0 > /sys/class/vtconsole/vtcon0/bind\necho 0 > /sys/class/vtconsole/vtcon1/bind\n\n# Unbind EFI-Framebuffer\necho efi-framebuffer.0 > /sys/bus/platform/drivers/efi-framebuffer/unbind\n\n# Avoid a Race condition by waiting 2 seconds. This can be calibrated to be shorter or longer if required for your system\nsleep 2\n\n# Unbind the GPU from display driver\nvirsh nodedev-detach pci_0000_0c_00_0\nvirsh nodedev-detach pci_0000_0c_00_1\n"
        }
      ]
    }
  ]
}