{
  "metadata": {
    "timestamp": 1736568714755,
    "page": 815,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgxOQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "crazyhottommy/getting-started-with-genomics-tools-and-resources",
      "stars": 1206,
      "defaultBranch": "master",
      "files": [
        {
          "name": "2013_Book_AppliedPredictiveModeling.pdf",
          "type": "blob",
          "size": 16742.9853515625,
          "content": ""
        },
        {
          "name": "20140312-GARP-LL-CODING_Projects_-Experiments_Files_Data.pdf",
          "type": "blob",
          "size": 750.5615234375,
          "content": null
        },
        {
          "name": "Bash_Strict_Mode.pdf",
          "type": "blob",
          "size": 641.2958984375,
          "content": null
        },
        {
          "name": "Curating Research Assets- A Tutorial on the Git Version Control System.pdf",
          "type": "blob",
          "size": 3754.83203125,
          "content": null
        },
        {
          "name": "DataONE_BP_Primer_020212.pdf",
          "type": "blob",
          "size": 206.490234375,
          "content": null
        },
        {
          "name": "FAIR_Open_GettingStarted.pdf",
          "type": "blob",
          "size": 64.025390625,
          "content": null
        },
        {
          "name": "GENIEDataGuide.pdf",
          "type": "blob",
          "size": 289.3544921875,
          "content": null
        },
        {
          "name": "How_to_share_data_collabration.pdf",
          "type": "blob",
          "size": 253.6845703125,
          "content": null
        },
        {
          "name": "Next Generation Sequencing spreadsheet - T.tsv",
          "type": "blob",
          "size": 4.6064453125,
          "content": "Platform\tReads x run: (M) \tRead length: (paired-end*, Half of data in reads**)\tRun time: (d) \tYield: (Gb) \tRate: (Gb/d) \tReagents: ($K)\tper-Gb: ($)\thg-30x: ($)\tMachine: ($)\tInstall base\tMax theoretical output per day (Gb)\tMax total output per day (Gb) from current installbase\t#\tPlatform\tInstall base\tMax theoretical output per day (Gb)\tMax total output per day (Gb) from current installbase\t#\tPlatform\tlength\tyield\r\niSeq 100 1fcell\t4\t250*\t077-1.28\t1.2-2\t1.56\t0.625\t521\t62500\t19.9K\t10\t1.56\t15.6\t--\tiSeq 100 1fcell\t10\t1.56\t15.6\t--\tiSeq 100 1fcell\t250*\t1.2-2\r\nMiniSeq 1fcell\t25\t150*\t1\t7.5\t7.5\t1.75\t233\t28000\t49.5K\t600\t7.5\t4500\t--\tMiniSeq 1fcell\t600\t7.5\t4500\t--\tMiniSeq 1fcell\t150*\t7.5\r\nMiSeq 1fcell\t25\t300*\t2\t15\t7.5\t1\t66\t8000\t99K\t6000\t7.5\t45000\t--\tMiSeq 1fcell\t6000\t7.5\t45000\t--\tMiSeq 1fcell\t300*\t15\r\nNextSeq 550 1fcell\t400\t150*\t1.2\t120\t100\t5\t50\t5000\t250K\t2400\t100\t240000\t--\tNextSeq 550 1fcell\t2400\t100\t240000\t--\tNextSeq 550 1fcell\t150*\t120\r\nHiSeq 2500 RR 2fcells\t600\t100*\t1.125\t120\t106.6\t6.145\t51.2\t6144\t740K\t0\t106.6\t0\t--\tHiSeq 2500 V4 2fcells\t1500\t166\t249000\t--\tHiSeq 2500 RR 2fcells\t100*\t120\r\nHiSeq 2500 V3 2fcells\t3000\t100*\t11\t600\t55\t23.47\t39.1\t4692\t690K\t0\t55\t0\t--\tHiSeq 4000 2fcells\t400\t400\t160000\t--\tHiSeq 2500 V3 2fcells\t100*\t600\r\nHiSeq 2500 V4 2fcells\t4000\t125*\t6\t1000\t166\t29.9\t31.7\t3804\t690K\t1500\t166\t249000\t--\tHiSeq X 2fcells\t400\t600\t240000\t--\tHiSeq 2500 V4 2fcells\t125*\t1000\r\nHiSeq 4000 2fcells\t5000\t150*\t3.5\t1500\t400\t--\t20.5\t2460\t900K\t400\t400\t160000\t--\tNovaSeq S4 2fcells\t285\t1200\t342000\t--\tHiSeq 4000 2fcells\t150*\t1500\r\nHiSeq X 2fcells\t6000\t150*\t3\t1800\t600\t--\t7.08\t849.6\t1M\t400\t600\t240000\t--\tIllumina PacBio RSII\t150\t2.8\t420\t--\tHiSeq X 2fcells\t150*\t1800\r\nNovaSeq S1 2fcells\t3300\t150*\t1.66\t1000\t600\t18.75\t18.75\t1800\t999K\t0\t600\t0\t--\tIllumina PacBio Sequel 16cells v6.0 2018\t275\t24.2\t6655\t--\tNovaSeq S1 2fcells\t150*\t1000\r\nNovaSeq S2 2fcells\t6600\t150*\t1.66\t2000\t1200\t35\t17.5\t1564\t999K\t0\t0\t0\t--\tMinION RevD 1fcell\t6000\t15\t90000\t--\tNovaSeq S2 2fcells\t150*\t2000\r\nNovaSeq S4 2fcells\t20000\t150*\t1.66\t6000\t3600\t64\t10.67\t700\t999K\t285\t1200\t342000\t--\tGridION X5 5fcells\t100\t75\t7500\t--\tNovaSeq S4 2fcells\t150*\t6000\r\nIllumina PacBio RSII\t0.88\t20K**\t4.3\t12\t2.8\t2.4\t200\t24000\t695K\t150\t2.8\t420\t--\tPromethION 24fcells\t45\t2800\t126000\t--\tIllumina PacBio RSII\t20K**\t12\r\nIllumina PacBio Sequel 16cells v6.0 2018\t6.4\t45K**\t6.6\t160-320\t24-48\t--\t80\t9600\t350K\t275\t24.2\t6655\t--\tMGISEQ T7 4fcells\t1\t6000\t6000\t--\tIllumina PacBio Sequel 16cells v6.0 2018\t45K**\t160-320\r\n Illumina PacBio Q1 2019\t--\t45K**\t--\t192\t--\t1\t6.6\t1000\t350K\t0\t0\t0\t--\tBGISEQ 500\t700\t37\t25900\t--\t Illumina PacBio Q1 2019\t45K**\t192\r\n5500 XL\t1400\t60\t7\t180\t30\t10.5\t58.33\t7000\t595K\t0\t0\t0\t--\tMGISEQ 2000\t20\t600\t12000\t--\t5500 XL\t60\t180\r\nIon S5 510 1chip\t2 - 3\t200-400\t0.21\t1\t4.8\t0.95\t950\t114000\t65K\t0\t0\t0\t--\tMIGSEQ 200\t10\t30\t300\t--\tIon S5 510 1chip\t200-400\t1\r\nIon S5 520 1chip\t3 - 6\t200-600\t0.23\t1\t4.3\t1\t500\t60000\t65K\t0\t0\t0\t--\t\t\t\t\t--\tIon S5 520 1chip\t200-600\t1\r\nIon S5 530 1chip\t20\t200-600\t0.29\t4\t13.8\t1.2\t150\t18000\t65K\t0\t0\t0\t--\t\t\t\t\t--\tIon S5 530 1chip\t200-600\t4\r\nIon S5 540 1chip\t80\t200\t0.42\t15\t35.7\t1.4\t93.3\t11196\t65k\t0\t0\t0\t--\t\t\t\t\t--\tIon S5 540 1chip\t200\t15\r\nIon S5 550 1chip\t130\t200\t0.5\t25\t50\t1.67\t66.8\t8016\t65k\t0\t0\t0\t--\t\t\t\t\t--\tIon S5 550 1chip\t200\t25\r\nSmidgION 1fcell\t--\t500-2000000\tTBC\tTBC\tTBC\tTBC\tTBC\t--\t--\t0\t0\t0\t--\t\t\t\t\t--\tSmidgION 1fcell\t500-2000000\tTBC\r\nFlongle 1fcell\t--\t500-2000000\t0.7\t1-3.3\t--\t--\t90-30\t8100-2700\t--\t0\t0\t0\t--\t\t\t\t\t--\tFlongle 1fcell\t500-2000000\t1-3.3\r\nMinION RevD 1fcell\t--\t500-2000000\t2\t17-40\t--\t--\t30-12.5\t2700-1125\t--\t6000\t15\t90000\t--\t\t\t\t\t--\tMinION RevD 1fcell\t500-2000000\t17-40\r\nGridION X5 5fcells\t--\t500-2000000\t2\t85-200\t--\t--\t17.5-7.5\t1575-675\t--\t100\t75\t7500\t--\t\t\t\t\t--\tGridION X5 5fcells\t500-2000000\t85-200\r\nPromethION 24fcells\t--\t500-2000000\t3\t3000/6000-10000\t--\t--\t42.5-1.6\t625-3825\t--\t45\t2800\t126000\t--\t\t\t\t\t--\tPromethION 24fcells\t500-2000000\t3000/6000-10000\r\nQiaGen GeneReader\t48\t100-150\t--\t7.2\t--\t0.5\t--\t--\t--\t0\t0\t0\t--\t\t\t\t\t--\tQiaGen GeneReader\t100-150\t7.2\r\nMGISEQ T7 4fcells\t20000\t150*\t1\t6000\t6000\t4.8\t0.8\t100\t--\t1\t6000\t6000\t\t\t\t\t\t\tMGISEQ T7 4fcells\t150*\t6000\r\nBGISEQ 500\t1600\t150*\t7\t260\t37\t--\t--\t600?\t240K\t700\t37\t25900\t--\t\t\t\t\t--\tBGISEQ 500\t150*\t260\r\nBGISEQ 50\t1600\t50*\t0.4\t8\t20\t--\t--\t--\t--\t0\t0\t0\t--\t\t\t\t\t--\tBGISEQ 50\t50*\t8\r\nMGISEQ 2000\t3000\t100*\t2\t600\t300\t4.8\t8\t960\t320K\t20\t600\t12000\t--\t\t\t\t\t--\tMGISEQ 2000\t100*\t600\r\nMIGSEQ 200\t300\t100*\t2\t60\t30\t--\t--\t--\t160K\t10\t30\t300\t--\t\t\t\t\t--\tMIGSEQ 200\t100*\t60\r\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nTotal (Gb/day)\t1555290.6\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nHuman genomes (30x)/day\t17,281\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nHuman genomes (30x)/year\t6,307,567\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nYears to sequence 7.6B people\t1205 yrs\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\r\nNumber of NovaSeqs or PromethIONs needed to sequence 7.6B people in 10 years\t156,164\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t"
        },
        {
          "name": "Principles and best practices in data versioning for all data sets big and small V1.0.pdf",
          "type": "blob",
          "size": 260.08203125,
          "content": null
        },
        {
          "name": "Programming Paradigms.pdf",
          "type": "blob",
          "size": 630.6005859375,
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 83.1142578125,
          "content": "\n<details>\n<summary>Table of content</summary>\n\n## Table of content\n\n- [General](#general)\n- [Courses](#courses)\n- [Some biology](#some-biology)\n- [Some statistics](#some-statistics)\n- [linear algebra](#linear-algebra)\n  - [Bayesian Statistics](#bayesian-statistics)\n- [Learning Latex](#learning-latex)\n- [Linux commands](#linux-commands)\n- [Do not give me excel files!](#do-not-give-me-excel-files)\n- [How to name files](#how-to-name-files)\n- [parallelization](#parallelization)\n- [Statistics](#statistics)\n- [Data transfer](#data-transfer)\n- [Website](#website)\n- [profile R code](#profile-r-code)\n- [updating R](#updating-r)\n- [Better R code](#better-r-code)\n- [Shiny App](#shiny-app)\n- [R tools for data wrangling, tidying and visualizing.](#r-tools-for-data-wrangling-tidying-and-visualizing)\n- [Genomic data visulization](#genomic-data-visulization)\n- [Sankey graph](#sankey-graph)\n- [Handling big data in R](#handling-big-data-in-r)\n- [Write your own R package](#write-your-own-r-package)\n- [Documentation](#documentation)\n- [handling arguments at the command line](#handling-arguments-at-the-command-line)\n- [visualization in general](#visualization-in-general)\n- [Javascript](#javascript)\n- [python tips and tools](#python-tips-and-tools)\n- [machine learning](#machine-learning)\n- [Amazon cloud computing](#amazon-cloud-computing)\n- [Genomics-visualization-tools](#genomics-visualization-tools)\n- [Databases](#databases)\n- [Large data consortium data mining](#large-data-consortium-data-mining)\n- [Integrative analysis](#integrative-analysis)\n- [Interactive visualization](#interactive-visualization)\n- [Tutorials](#tutorials)\n- [MOOC(Massive Open Online Courses)](#moocmassive-open-online-courses)\n- [git and version control](#git-and-version-control)\n- [blogs](#blogs)\n- [data management](#data-management)\n- [Automate your workflow, open science and reproducible research](#automate-your-workflow-open-science-and-reproducible-research)\n- [Survival curve](#survival-curve)\n- [Organize research for a group](#organize-research-for-a-group)\n- [Clustering](#clustering)\n- [CRISPR related](#crispr-related)\n- [vector arts for life sciences](#vector-arts-for-life-sciences)\n\n</details>\n\n\n\n### General\n\n* [So you want to be a computational biologist?](http://www.nature.com/nbt/journal/v31/n11/full/nbt.2740.html)\n* [Ten simple rules for biologists learning to program](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005871)\n* [Scientific computing: Code alert](http://www.nature.com/naturejobs/science/articles/10.1038/nj7638-563a?WT.mc_id=TWT_NatureNews) Nature News.\n* [Some drawings about programming](https://drawings.jvns.ca/) Very nice cartoon demonstrating useful concepts. https://wizardzines.com/\n* [Practical computing for biologist](http://practicalcomputing.org/). One of my first books to get me started in coding.\n* [ModernDive An Introduction to Statistical and Data Sciences via R](https://ismayc.github.io/moderndiver-book/index.html)\n* [DevOps for Data Science](https://do4ds.com/) A free ebook on DevOps for data science.\n* [Introduction to Data Science](https://rafalab.github.io/dsbook/) by Rafael A. Irizarry.\n* [Learning Statistics with R](https://learningstatisticswithr.com/)\n* [Hands-on Machine Learning with R](https://bradleyboehmke.github.io/HOML/)\n* [Reproducible Research Workflows with Snakemake and R](https://lachlandeer.github.io/snakemake-econ-r-tutorial/)\n* [The Biologist’s Guide to Computing](http://book.biologistsguide2computing.com/en/stable/) A book written by @tjelvar_olsson\n* [A Primer for Computational Biology](http://library.open.oregonstate.edu/computationalbiology/) A nice book from Oregon State University. You can get a hard copy on Amazon https://www.amazon.com/Primer-Computational-Biology-Shawn-ONeil/dp/0870719262.\n* [Computational Genomics With R](http://compgenomr.github.io/book/) A nice book from Altuna Akalin.\n* [Modern Statistics for Modern Biology](http://web.stanford.edu/class/bios221/book/) written by Prof. Susan Holmes from Stanford. I plan to read through it. a nice book using R for modern biology! looks awesome!\n* [Introduction to Data Science](https://ubc-dsci.github.io/introduction-to-datascience/index.html) A book by Tiffany-Anne. TimbersTrevor and CampbellMelissa Lee.\n* [An Introduction To Applied Bioinformatics](https://github.com/caporaso-lab/An-Introduction-To-Applied-Bioinformatics) Interactive lessons in bioinformatics. http://www.readiab.org/introduction.html\n* [Feature Engineering and Selection: A Practical Approach for Predictive Models](https://github.com/topepo/FES) by Kuhn and Johnson https://bookdown.org/max/FES\n* [Agile Data Science with R](https://edwinth.github.io/ADSwR/index.html)\n* [Offensieve programming book](https://neonira.github.io/offensiveProgrammingBook_v1.2.1/) in R.\n* [The Biostar Handbook: A Beginner's Guide to Bioinformatics](http://read.biostarhandbook.com/?q=) I am honored to be a co-author of this book. My ChIP-seq section was released by the mid of 2017.\n* [Beginner's Handbook to Next Generation Sequencing](https://genohub.com/next-generation-sequencing-handbook/) Everything you need to know about starting a sequencing project\n* [Another Book on Data Science:Learn R and Python in Parallel](https://www.anotherbookondatascience.com/) compares R and python side by side.\n* [A New Online Computational Biology Curriculum](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003662) PLOS genetics paper.\n* [Bioinformatics core competencies for undergraduate life sciences education](http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0196878)\n* [PH525x series - Biomedical Data Science](http://genomicsclass.github.io/book/) The best course to get you started with genomics using R. I have taken 3 times for the same course to get a deep understanding of the concepts and R commands. Now everything can be found here from Rafael Irizarry lab: http://rafalab.github.io/pages/harvardx.html\n* [The Bioconductor 2018 Workshop Compilation](https://bioconductor.github.io/BiocWorkshops/) very rich!\n* [Bioconductor for Genomics Data sciences](https://kasperdanielhansen.github.io/genbioconductor/) Coursera course.\n* [bioc workflow genomic annotation](https://www.bioconductor.org/packages/release/workflows/html/annotation.html)\n* [Expanding the computational toolbox for mining cancer genomes](http://www.nature.com/nrg/journal/v15/n8/full/nrg3767.html) Nature Review.\n* [some repos from command line to rstats and github](https://github.com/info-201)\n* 2016 review [Coming of age: ten years of next-generation sequencing technologies](http://www.nature.com/nrg/journal/v17/n6/full/nrg.2016.49.html)\n* [Cancer genomics — from bench to bedside: review papers from Nature](http://www.nature.com/collections/dswwtfkdty?BAN_NRG_1602_CANCERCOLLECTION_PORTFOLIO)\n* [SequencEnG: an Interactive Knowledge Base of Sequencing Techniques\n](http://education.knoweng.org/sequenceng/)\n* [Research Software Engineering with Python](https://merely-useful.tech/py-rse/) Building software that makes research possible. From Greg Wilson and Carpentries folks.\n* [Research Software Engineering with R](https://merely-useful.tech/r-rse/index.html) Building software that makes research possible\n\n### Courses\n\n* [The Missing Semester of Your CS Education](https://missing.csail.mit.edu/) These MIT Classes teach you all about advanced topics within CS, from operating systems to machine learning, but there’s one critical subject that’s rarely covered, and is instead left to students to figure out on their own: proficiency with their tools. We’ll teach you how to master the command-line, use a powerful text editor, use fancy features of version control systems, and much more!\n* Bioinformatics training materials from https://bioinformatics.babraham.ac.uk/training.html I like the Inkscape tutorial too\n* [applied computational genomics](https://github.com/quinlan-lab/applied-computational-genomics#course-lecture-slides) by Aaron Quinlan, the creator of bedtools and many other cool tools.\n* [BMMB 852: Applied Bioinformatics (Fall, 2016)](https://www.ialbert.me/) by Istvan Albert, the creator of [biostars](https://www.biostars.org/).\n* [JHU EN.600.649: Computational Genomics: Applied Comparative Genomics](https://github.com/schatzlab/appliedgenomics) by Michael Schatz.\n* [Introduction to Computational Biology](https://biodatascience.github.io/compbio/) by Mike Love.\n* [Advanced Data Science](http://jtleek.com/advdatasci/index.html) by Jeff Leek.\n* [Data Science for Biological, Medical and Health Research: Notes for 431](https://thomaselove.github.io/2018-431-book/data-science.html): R focused\n* Various [TeachingMaterial](https://github.com/lgatto/TeachingMaterial) collected by Laurent Gatto.\n* [NGS sequence analysis](https://bioinf.comav.upv.es/courses/sequence_analysis/index.html)\n* [bioinformatics-workbook](https://isugenomics.github.io/bioinformatics-workbook/)\n* [Reproducible Quantitative Methods](https://cbahlai.github.io/rqm-template/) from Mozilla science lab.\n* [bio-info courses](https://edu.t-bio.info/lp-courses/)\n* [MIT Computational Biology: Genomes, Networks, Evolution, Health - Fall 2018 - 6.047/6.878/HST.507](https://www.youtube.com/playlist?list=PLypiXJdtIca6GBQwDTo4bIEDV8F4RcAgt)by Manolis Kellis\n* [MIT machine learning in Genomics](https://www.youtube.com/playlist?list=PLypiXJdtIca6U5uQOCHjP9Op3gpa177fK) by Manolis Kellis.\n* [MIT linear algebra course by Gilbert Strang ](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/)\n* [A 2020 Vision of Linear Algebra](https://ocw.mit.edu/resources/res-18-010-a-2020-vision-of-linear-algebra-spring-2020/index.htm) by Gilbert Strang\n* [Generalized Additive Models in R](https://noamross.github.io/gams-in-r-course/) This short course will teach you how to use these flexible, powerful tools to model data and solve data science problems. GAMs offer offer a middle ground between simple linear models and complex machine-learning techniques, allowing you to model and understand complex systems.\n* [Feature Engineering and Selection: A Practical Approach for Predictive Models](https://bookdown.org/max/FES/)\n* [Tidy Modeling with R](https://www.tmwr.org/)\n\n\n### Some biology\n\nIf you are from fields outside of biology, places to get you started:\n\n* [An Owner's Guide to the Human Genome: an introduction to human population genetics, variation and disease](https://web.stanford.edu/group/pritchardlab/HGbook.html) by Jonathan Pritchard, Stanford University\n* [Tales from the Genome](https://www.udacity.com/course/tales-from-the-genome--bio110) A course by Udacity and 23andMe.\n* [The Biology of Cancer](http://garlandscience.com/product/isbn/9780815342205) A classic text book by Robert A. Weinberg. A must read for all cancer biologists.\n* [Molecular Biology of the Cell](https://www.amazon.com/Molecular-Biology-Cell-Bruce-Alberts/dp/0815341059/ref=mt_hardcover?_encoding=UTF8&me=) A text book\n* [Learn Genetics](http://learn.genetics.utah.edu/) from University of Utah learning center.\n* [iBiology](https://www.ibiology.org) offers several different types of courses\n* [courses](https://www.khanacademy.org/science/biology) from khanacademy.org\n* [genomics for software engineer](https://learngenomics.dev/docs/biological-foundations/cells-genomes-dna-chromosomes)\n\n### Some statistics\n\n* [Elements of Statistical Modeling for Experimental Biology](https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/) by Jeffrey A. Walker.I plan to read this one!!\n* [seeing theory](http://students.brown.edu/seeing-theory/index.html) The goal of the project is to make statistics more accessible to a wider range of students through interactive visualizations.\n* [Points of Significance: Interpreting P values](http://www.nature.com/nmeth/journal/v14/n3/full/nmeth.4210.html)\n* [statistics for biologists](http://www.nature.com/collections/qghhqm/pointsofsignificance)\n* [Advanced Statistical Computing](https://bookdown.org/rdpeng/advstatcomp/) by Roger Peng.\n* [fiveMinuteStats](http://stephens999.github.io/fiveMinuteStats/index.html#inference)\n* [Learning Statistics with R](https://learningstatisticswithr.com/)\n* [Statistical Modeling of High Dimensional Counts](https://mikelove.github.io/counts-model/) by Mike love on RNAseq counts modeling.\n* [Mixed models in R: a primer](https://arbor-analytics.com/post/mixed-models-a-primer/)\n* [Introduction to linear mixed models](https://gkhajduk.github.io/2017-03-09-mixed-models/)\n* [MIXED EFFECTS COX REGRESSION](https://stats.idre.ucla.edu/r/dae/mixed-effects-cox-regression/)\n* [GLMM FAQ](https://bbolker.github.io/mixedmodels-misc/glmmFAQ.html)\n* [mixed model/hierachical model visualized](http://mfviz.com/hierarchical-models/)\n* [A brief introduction to mixed effects modelling and multi-model inference in ecology](https://peerj.com/articles/4794/)\n\n### linear algebra\n\n* [Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab) by threebrownoneblue\n* [18.06 from Gilbert Strang](https://ocw.mit.edu/courses/mathematics/18-06-linear-algebra-spring-2010/video-lectures/)\n* [Matrix Methods in Data Analysis, Signal Processing, and Machine Learning from Gilbert Strang](https://ocw.mit.edu/courses/mathematics/18-065-matrix-methods-in-data-analysis-signal-processing-and-machine-learning-spring-2018/index.htm)\n* [Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares](http://vmls-book.stanford.edu/)using Julia language.\n* [Common statistical tests are linear models (or: how to teach stats)](https://lindeloev.github.io/tests-as-linear/)\n* [Course materials for applied regression STATS191 stanford](https://pratheepaj.github.io/teaching/stats191/)\n\n#### Bayesian Statistics\n\n* [Bayes Rules! An Introduction to Bayesian Modeling with R](https://www.bayesrulesbook.com/)\n* [Introduction to Bayesian Statistics](https://www.youtube.com/playlist?list=PLuRpZIQQRQedb2GM2WhKSEzojGN-BIIR9) STATS331 from Brendon Brewer.  \n* [Introduction to Empirical Bayes](http://varianceexplained.org/r/empirical-bayes-book/) by David Robinson using baseball examples.  \n* [Statistical Rethinking](http://xcelab.net/rm/statistical-rethinking/) github link https://github.com/rmcelreath/statrethinking_winter2019 Julia code https://shmuma.github.io/rethinking-2ed-julia/\n* [Bayesian Data Analysis demos for R](https://github.com/avehtari/BDA_R_demos)\n* [Doing Bayesian Data Analysis in brms and the tidyverse](https://bookdown.org/ajkurz/DBDA_recoded/) a book.\n* [Probabilistic-Programming-and-Bayesian-Methods-for-Hackers](https://github.com/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers)\n\n### Learning Latex\n\n* [draw your symbols](http://detexify.kirelabs.org/classify.html)\n* [The Best Way to Support LaTeX Math in Markdown with MathJax](https://yihui.org/en/2018/07/latex-math-markdown/)\n* [TinyTeX](https://yihui.org/tinytex/) A lightweight, cross-platform, portable, and easy-to-maintain LaTeX distribution based on TeX Live\n* Math expression http://www.math.mcgill.ca/yyang/regression/RMarkdown/example.html\n* [intro to Latex2](http://ctan.mirrors.hoobly.com/info/lshort/english/lshort.pdf) chapter 3\n* [The Bates LaTeX Manual](https://www.bates.edu/mathematics/resources/latex-manual/)\n\n### Linux commands\n\n* [Greg Wilson's youtube videos on unix shell](https://www.youtube.com/watch?v=U3iNcBtycaQ)\n* [A Bioinformatician's UNIX Toolbox](http://lh3lh3.users.sourceforge.net/biounix.shtml#xargs) from Heng Li  \n* [Linux command line exercises for NGS data processing](http://userweb.eng.gla.ac.uk/umer.ijaz/bioinformatics/linux.html)  \n* [command line bootcamp](http://rik.smith-unna.com/command_line_bootcamp/?id=rca84m6nsz6c9ngnugt8uayvi) teaches you unix command step by step\n* [Unix in your browser](https://browsix.org/). Maybe useful for teaching bash?\n* sshx A secure web-based, collaborative terminal https://sshx.io/ useful for teaching\n* [A Book for Anyone to Get Started with Unix](https://github.com/seankross/the-unix-workbench)\n* [bash one-liners for bioinformatics](https://github.com/crazyhottommy/oneliners)  \n* [some of my bash one-liner collections](https://github.com/crazyhottommy/scripts-general-use/blob/master/Shell/bioingformatics_one_liner.md)\n* [Use the Unofficial Bash Strict Mode (Unless You Looove Debugging)](http://redsymbol.net/articles/unofficial-bash-strict-mode/)\n* [Defensive BASH Programming](http://www.kfirlavi.com/blog/2012/11/14/defensive-bash-programming) very good read for bash programming.\n* [Better Bash Scripting in 15 Minutes](http://robertmuth.blogspot.com/2012/08/better-bash-scripting-in-15-minutes.html?m=1)\n* [bash pitfalls](http://mywiki.wooledge.org/BashPitfalls)\n* [Advancing in the Bash Shell](http://samrowe.com/wordpress/advancing-in-the-bash-shell/)\n* [Bash tips](http://jvns.ca/blog/2017/03/26/bash-quirks/)\n![](https://cloud.githubusercontent.com/assets/4106146/24389198/68cee218-1345-11e7-98a1-93ba78542daf.jpg)  \n* [Bash by example](https://www.ibm.com/developerworks/library/l-bash/)\n* process substitution: [Using Names Pipes and Process Substitution in Bioinformatics](http://vincebuffalo.org/blog/2013/08/08/using-names-pipes-and-process-substitution-in-bioinformatics.html) [Handy Bash feature: Process Substitution](https://medium.com/@joewalnes/handy-bash-feature-process-substitution-8eb6dce68133#.uz5pj9yer)\n* [NGS Advanced Beginner/Intermediate Shell](https://github.com/ngs-docs/2016-adv-begin-shell-genomics)\n* Commonly used commands for PBS scheduler:[Monitoring and Managing Your Job](https://www.osc.edu/supercomputing/batch-processing-at-osc/monitoring-and-managing-your-job)\n* test your unix skills at [cmd challenge](https://cmdchallenge.com)\n* people say awk is not part of bioinformats :) Still very useful parsing plain text files. [Steve's Awk Academy](http://troubleshooters.com/codecorn/awk/index.htm)\n* [intro-bioinformatics: Website and slides for intro to bioinformatics class at Fred Hutch](https://github.com/fredhutchio/intro-bioinformatics)  \n![](https://cloud.githubusercontent.com/assets/4106146/17654247/872f8716-6266-11e6-887d-cebd009dde6a.png)\n* [tmate](https://tmate.io/):Instant terminal sharing\n* [tmux](https://tmux.github.io/) is a terminal multiplexer similar to [`screen`](https://www.gnu.org/software/screen/manual/screen.html) but have more features.\n[tmux cheatsheet](https://gist.github.com/MohamedAlaa/2961058)  \n[tmux config](https://github.com/tony/tmux-config)    \n[tmux install without root](https://gist.github.com/ryin/3106801)\n\n\n* [All about redirection](http://tldp.org/HOWTO/Bash-Prog-Intro-HOWTO-3.html)\n\n**Theory and quick reference**\n\nThere are 3 file descriptors, stdin, stdout and stderr (std=standard).\n\nBasically you can:\n\nredirect stdout to a file\nredirect stderr to a file\nredirect stdout to a stderr\nredirect stderr to a stdout\nredirect stderr and stdout to a file\nredirect stderr and stdout to stdout\nredirect stderr and stdout to stderr\n1 'represents' stdout and 2 stderr.\nA little note for seeing this things: with the less command you can view both stdout (which will remain on the buffer) and the stderr that will be printed on the screen, but erased as you try to 'browse' the buffer.\n\n* stdout 2 file\n\nThis will cause the ouput of a program to be written to a file.\n\n         ls -l > ls-l.txt\n\nHere, a file called 'ls-l.txt' will be created and it will contain what you would see on the screen if you type the command 'ls -l' and execute it.\n\n* stderr 2 file  \n\nThis will cause the stderr ouput of a program to be written to a file.\n\n         grep da * 2> grep-errors.txt\n\nHere, a file called 'grep-errors.txt' will be created and it will contain what you would see the stderr portion of the output of the 'grep da *' command.  \n\n* stdout 2 stderr\n\nThis will cause the stderr ouput of a program to be written to the same filedescriptor than stdout.\n\n         grep da * 1>&2\n\nHere, the stdout portion of the command is sent to stderr, you may notice that in differen ways.  \n\n* stderr 2 stdout\n\nThis will cause the stderr ouput of a program to be written to the same filedescriptor than stdout.\n\n         grep * 2>&1\n\nHere, the stderr portion of the command is sent to stdout, if you pipe to less, you'll see that lines that normally 'dissapear' (as they are written to stderr) are being kept now (because they're on stdout).  \n\n* stderr and stdout 2 file\n\nThis will place every output of a program to a file. This is suitable sometimes for cron entries, if you want a command to pass in absolute silence.\n\n         rm -f $(find / -name core) &> /dev/null\n\nThis (thinking on the cron entry) will delete every file called 'core' in any directory. Notice that you should be pretty sure of what a command is doing if you are going to wipe it's output.\n\n* change permissions of files  \neach digit is for: user, group and other.  \n\n`chmod 754 myfile`: this means the user has read, write and execute permssion; member in the same group has read and execute permission but no write permission; other people in the world only has read permission.  \n\n4 stands for \"read\",    \n2 stands for \"write\",    \n1 stands for \"execute\", and    \n0 stands for \"no permission.\"    \nSo 7 is the combination of permissions 4+2+1 (read, write, and execute), 5 is 4+0+1 (read, no write, and execute), and 4 is 4+0+0 (read, no write, and no execute).    \n\nIt is sometimes hard to remember. one can use the letter:The letters u, g, and o stand for \"user\", \"group\", and \"other\"; \"r\", \"w\", and \"x\" stand for \"read\", \"write\", and \"execute\", respectively.    \n\n`chmod u+x myfile`    \n`chmod g+r myfile`  \n\n\n### Do not give me excel files!\n\n* [scary-excel-stories](https://github.com/jennybc/scary-excel-stories/blob/master/README.md)\n* [VisiData](https://www.visidata.org/)is an interactive multitool for tabular data. It combines the clarity of a spreadsheet, the efficiency of the terminal, and the power of Python, into a lightweight utility which can handle millions of rows with ease.\n* [convert xlsx to csv: xlsx2csv](https://github.com/dilshod/xlsx2csv)\n* [csvkit](http://csvkit.readthedocs.io/en/latest/index.html#)\n* [csvtk](https://bioinf.shenwei.me/csvtk/usage/) A complete .csv/.tsv toolkit including join command.\n* [GNU datamash](https://www.gnu.org/software/datamash/)\n* [tabtk](https://github.com/lh3/tabtk) Toolkit for processing TAB-delimited format from Heng Li, the author of `Samtools`, `BWA` and many others.\n* [miller](https://miller.readthedocs.io/en/latest/) is a command-line tool for querying, shaping, and reformatting data files in various formats including CSV, TSV, JSON, and JSON Lines.\n* [xsv](https://github.com/BurntSushi/xsv) A fast CSV toolkit written in Rust.\n* [Going from a human readable Excel file to a machine-readable csv with {tidyxl}](https://www.brodrigues.co/blog/2018-09-11-human_to_machine/)\n* eBay's TSV Utilities: Command line tools for large, tabular data files. Filtering, statistics, sampling, joins and more. https://ebay.github.io/tsv-utils/\n\n### How to name files\n\nIt is really important to name your files correctly! see a [ppt](https://rawgit.com/Reproducible-Science-Curriculum/rr-organization1/master/organization-01-slides.html) by Jenny Bryan.\n\nThree principles for (file) names:\n* Machine readable (do not put special characters and space in the name)  \n* Human readable (Easy to figure out what the heck something is, based on its name, add slug)  \n* Plays well with default ordering:   \n\n1. Put something numeric first  \n\n2. Use the ISO 8601 standard for dates (YYYY-MM-DD)\n\n3. Left pad other numbers with zeros  \n\n![](https://cloud.githubusercontent.com/assets/4106146/17389870/5dfc54c4-59cd-11e6-9293-a1f8789c8352.png)\n\n![](https://cloud.githubusercontent.com/assets/4106146/17389869/5df7f6f4-59cd-11e6-8715-86645243d70c.png)\n\n**If you have to rename the files...**\n\n* [brename](https://github.com/shenwei356/brename) A cross-platform command-line tool for safely batch renaming files/directories via regular expression (supporting Windows, Linux and OS X) from ShenWei is very useful!\n\n**Good naming of your files can help you to extract meta data from the file name**  \n* [dirdf](https://github.com/ropenscilabs/dirdf) Create tidy data frames of file metadata from directory and file names.\n\n```r\n> dir(\"examples/dataset_1/\")\n[1] \"2013-06-26_BRAFWTNEG_Plasmid-Cellline-100_A01.csv\"\n[2] \"2013-06-26_BRAFWTNEG_Plasmid-Cellline-100_A02.csv\"\n[3] \"2014-02-26_BRAFWTNEG_FFPEDNA-CRC-1-41_D08.csv\"\n[4] \"2014-03-05_BRAFWTNEG_FFPEDNA-CRC-REPEAT_H03.csv\"\n[5] \"2016-04-01_BRAFWTNEG_FFPEDNA-CRC-1-41_E12.csv\"\n\n> library(\"dirdf\")\n> dirdf(\"examples/dataset_1/\", template=\"date_assay_experiment_well.ext\")\n        date     assay           experiment well ext                                          pathname\n1 2013-06-26 BRAFWTNEG Plasmid-Cellline-100  A01 csv 2013-06-26_BRAFWTNEG_Plasmid-Cellline-100_A01.csv\n2 2013-06-26 BRAFWTNEG Plasmid-Cellline-100  A02 csv 2013-06-26_BRAFWTNEG_Plasmid-Cellline-100_A02.csv\n3 2014-02-26 BRAFWTNEG     FFPEDNA-CRC-1-41  D08 csv     2014-02-26_BRAFWTNEG_FFPEDNA-CRC-1-41_D08.csv\n4 2014-03-05 BRAFWTNEG   FFPEDNA-CRC-REPEAT  H03 csv   2014-03-05_BRAFWTNEG_FFPEDNA-CRC-REPEAT_H03.csv\n```\n\n### parallelization\n\nUsing these tool will greatly improve your working efficiency and get rid of most of your `for loops`.  \n1. [xargs](http://www.cyberciti.biz/faq/linux-unix-bsd-xargs-construct-argument-lists-utility/)  \n2. [GNU parallel](https://www.gnu.org/software/parallel/). one of my post [here](http://crazyhottommy.blogspot.com/2016/03/the-most-powerful-uniux-commands-i.html)  \n3. [gxargs](https://github.com/brentp/gargs) by Brent Pedersen. Written in GO.  \n4. [rush](https://github.com/shenwei356/rush) A cross-platform command-line tool for executing jobs in parallel by Shen Wei. I use his other tools such as `brename` and `csvtk`.\n5. [future: Unified Parallel and Distributed Processing in R for Everyone](https://cran.r-project.org/web/packages/future/index.html)\n6. [furrr](https://github.com/DavisVaughan/furrr) Apply Mapping Functions in Parallel using Futures\n\n### Statistics\n\n* [Essence of linear algebra](https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab)\n* [statistics for biologists](http://www.nature.com/collections/qghhqm) A collection of Nature articles on statistics in biology.\n\n### Data transfer\n\n* keep an eye on the [dat project](https://datproject.org/)! https://blog.datproject.org/2018/04/24/data-sharing-at-institutions-and-beyond-with-dat/\n\na blog post by Mark Ziemann http://genomespot.blogspot.com/2018/03/share-and-backup-data-sets-with-dat.html\n\n### Website\n\n* [rmarkdown website](https://rmarkdown.rstudio.com/rmarkdown_websites.html)\n* [A step by step tutorial](https://gupsych.github.io/acadweb/index.html)\n* [Up and running with blogdown](https://alison.rbind.io/post/up-and-running-with-blogdown/)\n* [summer of blogdown](https://summer-of-blogdown.netlify.com/)\n* [bookdown advanced slide](https://arm.rbind.io/slides/bookdown.html#1)\n* [make a hugo blog from scratch](https://zwbetz.com/make-a-hugo-blog-from-scratch/) to understand Hugo if you use blogdown.\n* [Tips for using the Hugo academic theme](https://lmyint.github.io/post/hugo-academic-tips/)\n* [Custom domain hosting with Github and Namecheap](http://blog.brooke.science/posts/custom-domain-hosting-with-github-and-namecheap/)\n* [MkDocs](https://www.mkdocs.org/) is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file.\n\n\n### updating R\n\n* [R upgrading can be a smooth process](http://dscinomics.com/post/2017-04-28-upgrade-to-r-3-4-0/)\n* [updating R](http://lcolladotor.github.io/2017/05/04/Updating-R/#.WSEQPrzyuqA) a blog post by L. Collado-Torres.\n* [update your R version in a breeze ( on OSX)](https://github.com/AndreaCirilloAC/updateR)\n* [updating R](https://whattheyforgot.org/maintaining-r.html#how-to-transfer-your-library-when-updating-r)\n\n```r\n# Install new version of R (lets say 3.5.0 in this example)\n\n# Create a new directory for the version of R\nfs::dir_create(\"~/Library/R/3.5/library\")\n\n# Re-start R so the .libPaths are updated\n\n# Lookup what packages were in your old package library\npkgs <- fs::dirname(fs::dir_ls(\"~/Library/R/3.4/library\"))\n\n# Filter these packages as needed\n\n# Install the packages in the new version\ninstall.packages(pkgs)\n\n```\n\n### Better R code\n\n* [assertr](https://github.com/ropensci/assertr)\n* [Tools for Working with ...](https://ellipsis.r-lib.org)\n* [here](https://github.com/jennybc/here_here)\n* [Inline testthat tests with roxygen2:roxytest](https://github.com/mikldk/roxytest)\n* [Non-invasive pretty printing of R code: styler](https://styler.r-lib.org)\n* [Static Code Analysis for R: lintr](https://github.com/jimhester/lintr) It checks adherence to a given style, syntax errors and possible semantic issues\n* [Make R a little bit stricter: strict](https://github.com/hadley/strict)\nalso read[offensive programming Book](https://neonira.github.io/offensiveProgrammingBook_v1.2.1/)\n### Shiny App\n\n* [Omicsplayground[https://github.com/bigomics/omicsplayground]\n* A Framework for Building Robust Shiny Apps [golem](https://thinkr-open.github.io/golem/)\n* [bootstrapllib}(https://rstudio.github.io/bootstraplib/) Tools for styling shiny and rmarkdown from R via Bootstrap (3 or 4) Sass\n* [The Shiny AWS Book](https://business-science.github.io/shiny-production-with-aws-book/) How to set up Shiny in AWS\n* [imola](https://github.com/pedrocoutinhosilva/imola)Bridging the gap between R/shiny and CSS layouts (grid and flexbox)!\n\n\n### profile R code\n\n* [profvis](https://rstudio.github.io/profvis/) Interactive Visualizations for Profiling R Code.\n* [proffer](https://github.com/r-prof/proffer) The proffer package profiles R code to find bottlenecks.\n* [rco - The R Code Optimizer](https://jcrodriguez1989.github.io/rco/index.html) Make your R code run faster! rco analyzes your code and applies different optimization strategies that return an R code that runs faster.\n\n### R tools for data wrangling, tidying and visualizing.  \n\n* [Common statistical tests are linear models (or: how to teach stats)](https://lindeloev.github.io/tests-as-linear/)\n* [What They Forgot to Teach You About R](https://whattheyforgot.org/) by Jennifer Bryan, Jim Hester. you know it is good.\nRstudio2020 https://rstudio-conf-2020.github.io/what-they-forgot/\n*  An R package for simple but efficient rowwise jobs https://courtiol.github.io/lay/\n* [Fundamentals of Data Visualization](http://serialmentor.com/dataviz/) by Claus O. Wilke.\n* [from data to vis](https://www.data-to-viz.com) From Data to Viz leads you to the most appropriate graph for your data. It links to the code to build it and lists common caveats you should avoid.\n* [Rmarkdown cookbook](https://bookdown.org/yihui/rmarkdown-cookbook)\n* [Data Visualization: A practical introduction](http://socviz.co/) A book by Kieran Healy from Duke University. Nice one to have!\n* [Functional programming and unit testing for data munging with R](http://www.brodrigues.co/fput/)\n* [R workshops](https://github.com/nuitrcs/rworkshops) some resources for R related materials.\n* [RStartHere](https://github.com/rstudio/RStartHere) A guide to some of the most useful R Packages that we know about, organized by their role in data science.\n* [biobroom](https://www.bioconductor.org/packages/release/bioc/html/biobroom.html):Turn Bioconductor objects into tidy data frames  \n* [readr](https://github.com/hadley/readr)  \n* [visdat](https://github.com/ropensci/visdat) visualizing your missing data and more.\n* [tidyr](https://github.com/hadley/tidyr)  \n* [stringr](https://github.com/tidyverse/stringr)\n* [glue](https://github.com/tidyverse/glue#usage) Glue strings to data in R. Small, fast, dependency free interpreted string literals\n* [purrr tutorial](https://jennybc.github.io/purrr-tutorial/index.html) by jenny bryan. functional programming in R.\n* [Row-oriented workflows in R with the tidyverse](https://github.com/jennybc/row-oriented-workflows) `pmap` is your friend :)\n* [janitor](https://github.com/sfirke/janitor) simple tools for data cleaning in R.\n* [tidyeval resources](http://maraaverick.rbind.io/2017/08/tidyeval-resource-roundup/)\n* Rstudio [tidyeval video](https://www.rstudio.com/resources/webinars/tidy-eval/)\n* [Tidy evaluation, most common actions](https://edwinth.github.io/blog/dplyr-recipes/)\n* [Tidy Eval Meets ggplot2](http://www.onceupondata.com/2018/07/06/ggplot-tidyeval/) a blog post.\n* [Tidy evaluation in ggplot2](https://www.tidyverse.org/articles/2018/07/ggplot2-tidy-evaluation/) from tidyverse.\n* [tidyeval patterns](https://www.tidyverse.org/articles/2019/06/rlang-0-4-0/)\n* [Tidy eval now supports glue strings](https://www.tidyverse.org/blog/2020/02/glue-strings-and-tidy-eval/)\n* [Non-standard evaluation, how tidy eval builds on base R](https://edwinth.github.io/blog/nse/)\n* [My First Steps into The World of Tidy Eval](http://www.onceupondata.com/2017/08/12/my-first-steps-into-the-world-of-tidyeval/)\n* [tidyeval shiny app](https://ijlyttle.shinyapps.io/tidyeval/)\n* [tidyeval bookdown](https://tidyeval.tidyverse.org/)\n* [reusing tidyverse code](https://speakerdeck.com/lionelhenry/reusing-tidyverse-code)\n* [dplry](https://github.com/hadley/dplyr)\n* [set_na_where(): a nonstandard evaluation use case](https://tjmahr.github.io/set-na-where-nonstandard-evaluation-use-case/)\n* [programming with dplyr](http://dplyr.tidyverse.org/articles/programming.html) A great read on non-standard evaluation, quoating  and qusiquotation. then the following two packages help you to deal with that.\n* [best practices for programming with ggplot2](https://fishandwhistle.net/slides/rstudioconf2020/#1)\n* [replyr](https://github.com/WinVector/replyr) An R package for fluid use of dplyr.\n* [Introduction of Parameterized dplyr expression](http://blog.eighty20.co.za//package%20exploration/2017/02/16/replyr-dplyr/) using replyr\n* [wrapr](https://github.com/WinVector/wrapr) wraps R functions debugging and better standard evaluation. `Let` function. blog post [wrapr: for sweet R code](http://www.win-vector.com/blog/2017/03/wrapr-for-sweet-r-code/)\n* [Easy machine learning pipelines with pipelearner: intro and call for contributors](https://drsimonj.svbtle.com/easy-machine-learning-pipelines-with-pipelearner-intro-and-call-for-contributors?utm_content=buffer7ef93&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) [github page](https://github.com/drsimonj/pipelearner)\n* [plot ROC with tidyverse](https://sydykova.com/post/2019-03-12-make-roc-curves-tidyverse/)\n* [csv fingerprint](http://setosa.io/blog/2014/08/03/csv-fingerprints/)\n* [ggplot2](https://github.com/hadley/ggplot2)\n* [ggplot2 tips](http://t-redactyl.io/tag/ggplot2.html)\n* [Demystifying ggplot2](https://rud.is/books/creating-ggplot2-extensions/demystifying-ggplot2.html) Learn how to write ggplot2 extensions.\n* [A List of ggplot2 extensions](https://www.ggplot2-exts.org/)\n* [using ggplot2 in packages](https://ggplot2.tidyverse.org/dev/articles/ggplot2-in-packages.html)\n>If you already know the mapping in advance (like the above example) you should use the .data pronoun from rlang to make it explicit that you are referring to the drv in the layer data and not some other variable named drv (which may or may not exist elsewhere). To avoid a similar note from the CMD check about .data, use #' @importFrom rlang .data in any roxygen code block (typically this should be in the package documentation as generated by usethis::use_package_doc()).\n\n> * If you know the mapping or facet specification is col in advance, use aes(.data$col) or vars(.data$col).  \n> * If col is a variable that contains the column name as a character vector, use aes(.data[[col]] or vars(.data[[col]]).\n> * If you would like the behaviour of col to look and feel like it would within aes() and vars(), use aes({{ col }}) or vars({{ col }}).\n\n* [gghighlight](https://github.com/yutannihilation/gghighlight): Highlight ggplot's Lines and Points with Predicates\n* [Anatomy of gghighlight](https://yutani.rbind.io/post/2018-06-03-anatomy-of-gghighlight/)\n* [nice ggplot themes](https://github.com/hrbrmstr/hrbrthemes)\n* [ggsci](https://ggsci.net/) offers a collection of ggplot2 color palettes inspired by scientific journals, data visualization libraries, science fiction movies, and TV shows.\n* The goal of [paletteer](https://github.com/EmilHvitfeldt/paletteer) is to be a comprehensize collection (666!)of color palettes in R using a common interface\n* [randomcolR](https://github.com/ronammar/randomcoloR) An R package for generating attractive and distinctive colors.\n* [colourpicker](https://github.com/daattali/colourpicker) A colour picker tool for Shiny and for selecting colours in plots (in R). [R blogger post](https://www.r-bloggers.com/plot-colour-helper-finally-an-easy-way-to-pick-colours-for-your-r-plots/amp/)\n* [ggforce](https://github.com/thomasp85/ggforce/tree/facets): facet_zoom() to zoom in part of the figure! and many more.\n* [ggpubr](http://www.sthda.com/english/rpkgs/ggpubr/): ‘ggplot2’ Based Publication Ready Plots. add pvalues. this saves me from customerizing my ggplot2 figures.\n* [op 50 ggplot2 Visualizations - The Master List (With Full R Code)](http://r-statistics.co/Top50-Ggplot2-Visualizations-MasterList-R-Code.html)\n* [kableExtra](https://github.com/haozhu233/kableExtra) Construct Complex Table with knitr::kable() + pipe.\n* [ggedit](https://www.r-statistics.com/2016/11/ggedit-interactive-ggplot-aesthetic-and-theme-editor/?utm_content=buffer62da5&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer) – interactive ggplot aesthetic and theme editor.\n* [trelliscopejs](http://ryanhafen.com/blog/trelliscopejs) is an R package that brings faceted visualizations to life while plugging in to common analytical workflows like ggplot2 or the “tidyverse”.\n* [Plotting background data for groups with ggplot2](https://drsimonj.svbtle.com/plotting-background-data-for-groups-with-ggplot2?utm_campaign=Data%2BElixir&utm_medium=email&utm_source=Data_Elixir_92)\n* [Ordering categories within ggplot2 facets](https://drsimonj.svbtle.com/ordering-categories-within-ggplot2-facets)\n* [plotly for R](https://cpsievert.github.io/plotly_book/)\n* [rematch2](https://github.com/MangoTheCat/rematch2#readme)Tidy output from regular expression matches\n* [Make waffle (square pie) charts in R](https://github.com/hrbrmstr/waffle)\n* Bring the power of R to the command line: [littler](http://dirk.eddelbuettel.com/blog/2016/08/07/#littler-0.3.1)  [Rio](https://github.com/jeroenjanssens/data-science-at-the-command-line/blob/master/tools/Rio) A wrapper by Jeroen Janssens, the author of [data science at the command line](http://datascienceatthecommandline.com/)\n* [Complexheatmap](https://jokergoo.github.io/ComplexHeatmap-reference/book/) my go-to package for static heatmaps.\n* [tidyheatmap](https://stemangiola.github.io/tidyHeatmap/articles/introduction.html) based on complexheatmap.\n* [htmlwidgets for R](http://www.htmlwidgets.org/showcase_d3heatmap.html) including `d3heatmap` for interactive heatmaps.\n* [focus() on correlations of some variables with many others](https://drsimonj.svbtle.com/how-does-one-variable-correlate-with-all-others)\n* Explore correlations in R with [corrr](https://github.com/drsimonj/corrr)\n* [Unit test in R](http://www.johnmyleswhite.com/notebook/2010/08/17/unit-testing-in-r-the-bare-minimum/)\n* [sinaplot](https://cran.r-project.org/web/packages/sinaplot/vignettes/SinaPlot.html): an enhanced chart for simple and truthful representation of single observations over multiple classes. `ggforce` has `geom_sina` for the same purpose.\n* [complexHeatmaps](https://bioconductor.org/packages/release/bioc/html/ComplexHeatmap.html)  \n* [superheat](https://github.com/rlbarter/superheat) Another heatmap package worth learning besides `ComplexHeatmap`. Not as flexiable as ComplexHeatmap, but can be handy when the function you want has been implemented.\n* [iheatmapr](https://github.com/AliciaSchep/iheatmapr) is an R package for building complex, interactive heatmaps using modular building blocks.\n* [heatmap:gapmap](https://cran.rstudio.com/web/packages/gapmap/vignettes/simple_example.html)\n* [dendsort](https://cran.r-project.org/web/packages/dendsort/index.html):Modular Leaf Ordering Methods for Dendrogram Nodes\n* [dendextend](https://cran.r-project.org/web/packages/dendextend/vignettes/introduction.html#changing-a-dendrograms-structure)\n* [Interactive Heat Maps for R Using plotly](https://github.com/talgalili/heatmaply)\n* [Multiple plots on a page](https://stat545-ubc.github.io/block020_multiple-plots-on-a-page.html)\n* [ggExtra](http://deanattali.com/2015/03/29/ggExtra-r-package/)\n* [cowplot](https://github.com/wilkelab/cowplot) -- An add-on to the ggplot2 plotting package\n* [ggplot2 - Easy way to mix multiple graphs on the same page - R software and data visualization](http://www.sthda.com/english/wiki/ggplot2-easy-way-to-mix-multiple-graphs-on-the-same-page-r-software-and-data-visualization)\n* [Extract Tables from PDFs](https://github.com/leeper/tabulizer)\n* Alternative to venndiagram! [upSetR](https://github.com/hms-dbmi/UpSetR)\n* [hierarchicalSets](https://github.com/thomasp85/hierarchicalSets)\n* [Intervene](https://bitbucket.org/CBGR/intervene) is a tool for intersection and visualization of multiple gene or genomic region sets.\n* [In-depth introduction to machine learning in 15 hours of expert videos](http://www.dataschool.io/15-hours-of-expert-machine-learning-videos/)\n* [Data Analysis and Visualization Using R](http://varianceexplained.org/RData/)This is a course that combines video, HTML and interactive elements to teach the statistical programming language R.\n* [dabestr](https://cran.r-project.org/web/packages/dabestr/vignettes/using-dabestr.html) dabestr is a package for Data Analysis using Bootstrap-Coupled ESTimation. https://github.com/ACCLAB/dabestr\n* [These are the course notes for the Monash Bioinformatics Platform’s “R More” course](https://monashbioinformaticsplatform.github.io/r-more/)\n* [gitbook: Getting used to R, RStudio, and R Markdown](https://ismayc.github.io/rbasics-book/index.html)\n* [Technical Foundations of Informatics](https://info201-s17.github.io/book/) a free book to teach you R and many others.\n* [Efficient R programming](https://csgillespie.github.io/efficientR/)\n* [R for Data Science](http://r4ds.had.co.nz/) by Garrett Grolemund and Hadley Wickham\n\n### Genomic data visulization\n\n* [karyoploteR](https://bernatgel.github.io/karyoploter_tutorial/Tutorial/PlotCoverage/PlotCoverage.html) Really powerful and versatile tool.\n* [Bentobox](https://phanstiellab.github.io/BentoBox/index.html) BentoBox empowers users to programmatically and flexibly generate multi-panel figures. \n\n\n### Sankey graph\n\n* [ggalluvial](http://corybrunson.github.io/ggalluvial/index.html)\n* [ggforce](https://github.com/thomasp85/ggforce/tree/sankey) `geom_parallel_sets()`\n* [easyalluvial](https://erblast.github.io/easyalluvial/)\n\n### Handling big data in R\n\n* [A data.table and dplyr tour](https://atrebas.github.io/post/2019-03-03-datatable-dplyr/) A blog post compare dplyr and data.table side by side.\n* [Lightning Fast Serialization of Data Frames for R](https://github.com/fstpackage/fst) faster than `data.table`, `feather`.\n* [Rpub post: Handling large data sets in R](https://rpubs.com/msundar/large_data_analysis)\n* [The disk.frame package aims to be the answer to the question: how do I manipulate structured tabular data that doesn’t fit into Random Access Memory (RAM)](https://github.com/xiaodaigh/disk.frame)\n* [`dtplyr` and `tidyfast` are teaming up (well, at least in this blog post)](https://tysonbarrett.com//jekyll/update/2019/12/03/workflow_dtplyr_tidyfast/)\n* [Fast reading of delimited files with vroom](https://vroom.r-lib.org) The fastest delimited reader for R, 1.40 GB/sec/sec.\n* [stash: Naive on-disk caching in R](https://github.com/iqis/stash)\n* [qs: Quick serialization of R objects](https://github.com/traversc/qs)\n* [The fst package](https://www.fstpackage.org/) for R provides a fast, easy and flexible way to serialize data frames. With access speeds of multiple GB/s, fst is specifically designed to unlock the potential of high speed solid state disks that can be found in most modern computers. Data frames stored in the fst format have full random access, both in column and rows.\n* The [arrow](https://github.com/apache/arrow/tree/master/r) package exposes an interface to the Arrow C++ library to access many of its features in R. This includes support for analyzing large, multi-file datasets (open_dataset()), working with individual Parquet (read_parquet(), write_parquet()) and Feather (read_feather(), write_feather()) files, as well as lower-level access to Arrow memory and messages.\n* [dm: Working with relational data models in R](https://github.com/krlmlr/dm). Use it today (if only like a list of tables). Build data models tomorrow. Deploy the data models to your organization’s RDBMS the day after.\n\n### Write your own R package\n\n* [R packages](https://r-pkgs.org/) book from Hadely.\n* [rpackages](https://github.com/jtleek/rpackages) from Jeff Leek.\n* [WRITING R PACKAGES IN RSTUDIO TUTORIAL ADAPTED FROM STIRLINGCODINGCLUB.GITHUB.IO](https://ourcodingclub.github.io/tutorials/writing-r-package/)\n* [R package development](https://combine-australia.github.io/r-pkg-dev/) This workshop was created by COMBINE, an association for Australian students in bioinformatics, computational biology and related fields.\n* [usethis workflow for package development](https://www.hvitfeldt.me/2018/09/usethis-workflow-for-package-development/)\n* [Developing R Packages with usethis and GitLab CI: Part I](https://blog.methodsconsultants.com/posts/developing-r-packages-using-gitlab-ci-part-i/)\n* [Writing an R package from scratch](https://r-mageddon.netlify.com/post/writing-an-r-package-from-scratch/) a blog post.\n* [available](https://github.com/ropenscilabs/available) helps you name your R package\n* [goodpractice](https://cran.r-project.org/web/packages/goodpractice/index.html) An R package on Advice on R packages.\n* [R package primer: a minimal tutorial](http://kbroman.org/pkg_primer/)\n* [Write your own R package](http://stat545.com/packages06_foofactors-package.html)\n* [R packages](http://r-pkgs.had.co.nz/) a book by Hadley Wickham.\n* [Developing R packages](https://github.com/jtleek/rpackages/blob/master/README.md) from Jeff leek.\n* [Sinew](https://github.com/metrumresearchgroup/sinew) is a R package that generates a roxygen2 skeleton populated with information scraped from the function script.\n* [Automatic tools for improving R packages](http://www.masalmon.eu/2017/06/17/automatictools/) `devtools:spell_check()` `goodpractice:gp()` and `pkgdown:build_site()`.\n* blog post [How to develop good R packages (for open science)](http://www.masalmon.eu/2017/12/11/goodrpackages/)\n* Easy and efficient debugging for R packages: [debugme](https://github.com/r-lib/debugme)\n* [Non-invasive pretty printing of R code](http://styler.r-lib.org)\n* [usethis](https://github.com/r-lib/usethis) The goal of usethis is to automate many common package and analysis setup tasks.\n* [Mastering Software Development in R](https://bookdown.org/rdpeng/RProgDA/) by Roger Peng et.al.\n* [The tidyverse style guide](http://style.tidyverse.org/) by Hadley Wickham.\n* [submitting your package to bioconductor](https://github.com/kuwisdelu/BiocMeetup/blob/master/2019-Jan/BioC-git-and-Github.pdf)\n\n### Documentation\n\n* This is a must read for writing good documentations: A blog [post](https://www.divio.com/blog/documentation/). I saved it to a pdf and uploaded to this repo.\n\n### handling arguments at the command line\n\n* [docopt.R](https://github.com/docopt/docopt.R) [tutorial](http://www.slideshare.net/EdwindeJonge1/docopt-user2014)\n* [python version](http://docopt.org/)\n* [Generate a CLI tool from a Python module/function](https://github.com/bharadwaj-raju/cligenerator)\n* [Introducing Python Fire, a library for automatically generating command line interfaces](https://opensource.googleblog.com/2017/03/python-fire-command-line.html)\n* [Patterns and anti-patterns for writing command-line bioinformatics software](https://github.com/ctb/titus-blog/blob/add/command_line_patterns/src/2018-our-command-line-patterns.md) by Titus.\n\n### visualization in general\n\n* [Nature Methods point of view data visulization](http://blogs.nature.com/methagora/2013/07/data-visualization-points-of-view.html)\n* [A tutorial for the free Inkscape cross-platform vector graphics editor](https://github.com/fredhutchio/inkscape-tutorial)\n* [gimp](https://www.gimp.org/downloads/) for bit-map based figures.\n* [data vis resource from Sabah](https://sabahzero.github.io/dataviz/resources)\n* [Ten simple rules to colorize biological data visualization](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008259)\n\n### Javascript\n\n* [JavaScript versus Research Computing](https://gvwilson.github.io/js-vs-rc/) from Greg Wilson, the founder of software carpentry.\n\n### python tips and tools\n\n* [some nice free python books: Think python etc](http://greenteapress.com/wp/)\n* [python learning resources](https://learnbyexample.github.io/py_resources/)\n* [Interactive python](http://interactivepython.org/runestone/default/user/login?_next=/runestone/default/index) nice interactive books help you learn python.\n* [30 Python Language Features and Tricks You May Not Know About](http://sahandsaba.com/thirty-python-language-features-and-tricks-you-may-not-know.html#id1)\n* [intermediatePython](https://github.com/crazyhottommy/intermediatePython)\n* [The Hitchhiker’s Guide to Python!](http://docs.python-guide.org/en/latest/)\n* [Python 3 for Scientists](http://python-3-for-scientists.readthedocs.io/en/latest/)\n* [Python FAQ: Why should I use Python 3?](https://eev.ee/blog/2016/07/31/python-faq-why-should-i-use-python-3/)\n* [gitbook: Computational and Inferential Thinking; The Foundations of Data Science](https://www.gitbook.com/book/ds8/textbook/details)\n* [A collection of python courses online](http://bafflednerd.com/learn-python-online/)\n* [tpot:A Python tool that automatically creates and optimizes machine learning pipelines using genetic programming.](https://github.com/rhiever/tpot)\n* [Easy to use Python API wrapper to plot charts with matplotlib, plotly, bokeh and more](https://github.com/cuemacro/chartpy):chartpy creates a simple easy to use API to plot in a number of great Python chart libraries like plotly (via cufflinks), bokeh and matplotlib, with a unified interface. You simply need to change a single keyword to change which chart engine to use (see below), rather than having to learn the low level details of each library.\n* [Top 8 resources for learning data analysis with pandas](http://www.dataschool.io/best-python-pandas-resources/)\n* [Jupyter Notebooks for the Python Data Science Handbook](https://github.com/jakevdp/PythonDataScienceHandbook)\n* [kite](https://kite.com/)The smart copilot for programmers. works with atom, sublime, vim and emacs!\n\n### machine learning\n\n* [Practical Machine Learning with Python: standford crowd course](http://crowdcourse.stanford.edu/ml.html)\n* [tidy modeling with R](https://www.tmwr.org/)\n\n### Amazon cloud computing\n\n[Intro to AWS Cloud Computing](https://github.com/griffithlab/rnaseq_tutorial/wiki/Intro-to-AWS-Cloud-Computing#necessary-steps-for-launching-an-instance)\n\n### Genomics-visualization-tools\n\nThere are many online web based tools for visualization of (cancer) genomic data. I put my collections here. I use R for visulization.\nsee a nice post by using python by Radhouane Aniba:[Genomic Data Visualization in Python](http://fullstackdatascientist.io/2016-03-15-genomic-data-visualization-using-python/)\n\n* [UCSC cancer genome browser](https://genome-cancer.ucsc.edu/proj/site/hgHeatmap/) It has many data including TCGA data buit in, and can be very handy for both bench scientist and bioinformaticians.  \n* [UCSC Xena](http://xena.ucsc.edu/). A new tool developed by UCSC team as well. Poteintially very useful, but need more tutorials to follow.\n* [UCSC genome browser](http://genome.ucsc.edu/). One of the most famous genome browser and my favoriate. **Every person** studying genetics, genomics and molecular biology needs to know how to use it. [Tutorials from OpenHelix](http://blog.openhelix.eu/?p=22649).  \n* [Epiviz 3](http://epiviz.github.io/index.html) is an interactive visualization tool for functional genomics data. It supports genome navigation like other genome browsers, but allows multiple visualizations of data within genomic regions using scatterplots, heatmaps and other user-supplied visualizations.\n* Mutation Annotation & Genome Interpretation TCGA: [MAGA](http://magi.brown.edu/)\n* [GeneProteinViz (GPViz)](http://icbi.at/software/gpviz/gpviz.shtml) is a versatile Java-based software for dynamic gene-centered visualization of genomic regions and/or variants.\n* [ProteinPaint: Web Application for Visualizing Genomic Data](https://pecan.stjude.org/proteinpaint/TP53/) The software developed for this project highlights critical attributes about the mutations, including the form of protein variant (e.g. the new amino acid as a result of missense mutation), the name of sample from which the mutation was identified, whether the mutation is somatic or germline,\n\n### Databases\n\n* [protein-protein interaction databases](http://startbioinfo.com/cgi-bin/simpleresources.pl?tn=PPI_AR)\n* [A compilation of protein-protein interaction resources Akhilesh Bajpai and Sravanthi Davuluri (Correspondence: Acharya KK, kshitish@ibab.ac.in)](http://startbioinfo.com/cgi-bin/simpleresources.pl?tn=PPI_AR)\n* [DisGeNET](http://www.disgenet.org/web/DisGeNET/menu/home?utm_source=twitterfeed&utm_medium=twitter) is a discovery platform integrating information on gene-disease associations (GDAs) from several public data sources and the literature\n* [Cancer3D](http://cancer3d.org/) is a database that unites information on somatic missense mutations from TCGA and CCLE, allowing users to explore two different cancer-related problems at the same time: drug sensitivity/biomarker identification and prediction of cancer drivers\n* [UCSCXenaTools](https://github.com/ropensci/UCSCXenaTools) An R package for accessing genomics data from UCSC Xena platform, from cancer multi-omics to single-cell RNA-seq\n* [PharmacoGx](https://bioconductor.org/packages/release/bioc/html/PharmacoGx.html) Contains a set of functions to perform large-scale analysis of pharmacogenomic data. public data sets such as CCLE can be easily downloaded!\n* [clinical intepretations of variants in cancer](https://civic.genome.wustl.edu/#/home)\n* [R Wrapper for DGIdb](http://bioconductor.org/packages/devel/bioc/html/rDGIdb.html) Drug-gene interaction database.\n* [BioGrid](http://thebiogrid.org/) Welcome to the Biological General Repository for Interaction Datasets\n* [The IUPHAR/BPS Guide to PHARMACOLOGY in 2016: towards curated quantitative interactions between 1300 protein targets and 6000 ligands](http://nar.oxfordjournals.org/content/early/2015/10/11/nar.gkv1037.short?rss=1)\n* [Public data and open source tools for multi-assay genomic investigation of disease](http://bib.oxfordjournals.org/content/early/2015/10/10/bib.bbv080.long)\n* [cancer cell metabolism genes](http://bioinfo.mc.vanderbilt.edu/ccmGDB/index.html)\n* [oncogenes and tumor suppressors](https://www.biostars.org/p/15890/) biostar post and [TSgene](http://bioinfo.mc.vanderbilt.edu/TSGene/index.html)  \n* [DriverDB: A database for cancer driver gene/mutation](http://ngs.ym.edu.tw/driverdb)\n* Interaction of genes: [GENEMANIA](http://genemania.org/)\n* [DATA DISCOVERY PLATFORM:Designed for researchers who use, share and collaborate on human genomic data](http://discover.repositive.io/)\n* [zenodo: research shared](https://zenodo.org/collection/datasets)\n* [dataMed](https://datamed.org/) biomedical and healthCAre Data Discovery Index Ecosystem.\n* [repostive](https://repositive.io/) Discover a better way of searching for genomic data.\n* The NCI's [Genomic Data Commons](https://gdc.nci.nih.gov/) (GDC) provides the cancer research community with a unified data repository that enables data sharing across cancer genomic studies in support of precision medicine. A copy of TCGA and TARGET data? [Data Release Notes](https://gdc-docs.nci.nih.gov/Data/Release_Notes/Data_Release_Notes/?platform=hootsuite)\n* [OASIS genomics](http://www.oasis-genomics.org/) from Pfizer. processed data from TCGA, CCLE, GTEx.\n* [TCGA alternative splicing](http://bioinformatics.mdanderson.org/TCGASpliceSeq)\n* [ISOexpresso](http://wiki.tgilab.org/ISOexpresso/): a web-based platform for isoform-level expression analysis in human cancer\n* [omics databse](http://www.omicsdi.org/#/) The Omics Discovery Index (OmicsDI) provides dataset discovery across a heterogeneous, distributed group of Transcriptomics, Genomics, Proteomics and Metabolomics data resources spanning eight repositories in three continents and six organisations, including both open and controlled access data resources. The resource provides a short description of every dataset: accession, description, sample/data protocols biological evidences, publication, etc. Based on these metadata, OmicsDI provides extensive search capabilities, as well as identification of related datasets by metadata and data content where possible. In particular, OmicsDI identifies groups of related, multi-omics datasets across repositories by shared identifiers.\n* [MAGI](http://magi.brown.edu/) Mutation Annotation &Genome Interpretation for TCGA data.\n* [How to successfully apply for access to dbGaP](http://blog.repositive.io/how-to-successfully-apply-for-access-to-dbgap/)\n* [Human cell Atlas](https://www.humancellatlas.org/) some preview data sets https://preview.data.humancellatlas.org/\n* [DepMap](https://depmap.org/portal/depmap/) A Cancer Dependency Map to systematically identify genetic and pharmacologic dependencies and the biomarkers that predict them.\n\n### Large data consortium data mining\n\n* [AnnotationHub](http://bioconductor.org/packages/devel/bioc/vignettes/AnnotationHub/inst/doc/AnnotationHub-HOWTO.html#roadmap-epigenomics-project) bioconductor package for TCGA and epigenome roadmap, ENCODE project.  \n* [TCGAbiolinks](http://bioconductor.org/packages/devel/bioc/vignettes/TCGAbiolinks/inst/doc/tcgaBiolinks.html) bioconductor package.\n* [GenomicDataCommons](https://github.com/Bioconductor/GenomicDataCommons) bioc package to acess GDC.\n* [RTCGA bioconductor](http://bioconductor.org/packages/devel/bioc/html/RTCGA.html)\n* [f1000 workflow paper TCGA Workflow: Analyze cancer genomics and epigenomics data using Bioconductor packages](http://f1000research.com/articles/5-1542/v1)\n* paper [Data mining The Cancer Genome Atlas in the era of precision cancer medicine](http://www.smw.ch/content/smw-2015-14183/)\n* [CrossHub](http://sourceforge.net/p/crosshub/): a tool for multi-way analysis of The Cancer Genome Atlas (TCGA) in the context of gene expression regulation mechanisms.\n* [Ferret, a User-Friendly Java Tool to Extract Data from the 1000 Genomes Project](http://limousophie35.github.io/Ferret/)\n* [EGA:European Genome-phenome Archive](https://www.ebi.ac.uk/ega/datasets)\n* [survival curves for TCGA data: a simple web tool](http://www.oncolnc.org/)\n*  Genetic determinants of cancer patient survival http://survival.cshl.edu/.  https://twitter.com/jsheltzer/status/1150828456340574209?s=12\n\"..in some papers and presentations, biologists will use TCGA survival curves showing that their favorite gene is associated with poor prognosis to argue that their gene is super-important. This is weak evidence. **Prognostic biomarkers are not necessarily strong cancer drivers**\"\n* [AACR Project GENIE](https://www.synapse.org/#!Synapse:syn7222066/wiki/405659) [data guide](https://github.com/crazyhottommy/getting-started-with-genomics-tools-and-resources/blob/master/GENIEDataGuide.pdf)\n\n### Integrative analysis\n\n* [High-dimensional genomic data bias correction and data integration using MANCIE](http://www.nature.com/ncomms/2016/160413/ncomms11305/full/ncomms11305.html)  correct batch effects for data from different sequencing methods. (RNAseq vs ChIPseq)\n*\n\n### Interactive visualization\n\n* [Vega-lite](https://github.com/vega/vega-lite) A high-level grammar for visual analysis, built on top of Vega. Looks awesome!\n* [Introducing altair, an R interface to the Altair Python Package](https://vegawidget.rbind.io/post/2018-05-20-introducing-altair/)  which you can use to build and render Vega-Lite chart-specifications.\n* The goal of [g(r)osling](https://github.com/gosling-lang/grosling) is to help you build interactive genomics visualizations with [Gosling](https://github.com/gosling-lang). This package uses reticulate to provide an interface to the Gos Python package.\n\n### Tutorials\n\n* [Ten quick tips for effective dimensionality reduction](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006907) by Susan Holmes.\n* [PH525x series - Biomedical Data Science](http://genomicsclass.github.io/book/). Learn R and bioconductor.\n* [Principal Component Analysis Explained Visually](http://setosa.io/ev/principal-component-analysis/)  \n* [PCA, MDS, k-means, Hierarchical clustering and heatmap](https://rpubs.com/crazyhottommy/PCA_MDS). I wrote it.\n* [A tale of two heatmaps](https://rpubs.com/crazyhottommy/a-tale-of-two-heatmap-functions). I wrote it.\n* [Heatmap demystified](https://rpubs.com/crazyhottommy/heatmap_demystified). I wrote it.\n* [Cluster Analysis in R - Unsupervised machine learning](http://www.sthda.com/english/wiki/cluster-analysis-in-r-unsupervised-machine-learning#at_pco=smlre-1.0&at_si=58765a95fcb21379&at_ab=per-2&at_pos=3&at_tot=4) very practical intro on STHDA website.\n* [I wrote on PCA, and heatmaps on Rpub](https://rpubs.com/crazyhottommy)\n* A most read for clustering analysis for high-dimentional biological data:[Avoiding common pitfalls when clustering\nbiological data](http://stke.sciencemag.org/content/9/432/re6)\n* [How does gene expression clustering work?](http://www.nature.com/nbt/journal/v23/n12/full/nbt1205-1499.html) A must read for\nclustering.\n* [How to read PCA plots for scRNAseq](http://www.nxn.se/valent/2017/6/12/how-to-read-pca-plots) by VALENTINE SVENSSON.\n\n<blockquote class=\"twitter-tweet\" data-lang=\"en\"><p lang=\"en\" dir=\"ltr\">See <a href=\"https://t.co/yxCb85ctL1\">https://t.co/yxCb85ctL1</a>: &quot;MDS best choice for preserving outliers,  PCA for variance, &amp; T-SNE for clusters&quot; <a href=\"https://twitter.com/mikelove\">@mikelove</a> <a href=\"https://twitter.com/AndrewLBeam\">@AndrewLBeam</a></p>&mdash; Rileen Sinha (@RileenSinha) <a href=\"https://twitter.com/RileenSinha/status/768873620521250816\">August 25, 2016</a></blockquote>\n<script async src=\"//platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n\n[paper: Outlier Preservation by Dimensionality Reduction Techniques](http://oai.cwi.nl/oai/asset/22628/22628B.pdf)\n>\"MDS best choice for preserving outliers, PCA for variance, & T-SNE for clusters\"\n\n* [How to Use t-SNE Effectively](http://distill.pub/2016/misread-tsne/)\n* [Rtsne](https://github.com/jkrijthe/Rtsne) R package for T-SNE\n* [rtsne](https://github.com/jdonaldson/rtsne) An R package for t-SNE (t-Distributed Stochastic Neighbor Embedding)\na bug was in `rtsne`: https://gist.github.com/mikelove/74bbf5c41010ae1dc94281cface90d32\n* [t-SNE-Heatmaps](https://github.com/KlugerLab/t-SNE-Heatmaps) Beta version of 1D t-SNE heatmaps to visualize expression patterns of hundreds of genes simultaneously in scRNA-seq.\n* [PHATE dimensionality reduction method](https://github.com/KrishnaswamyLab/PHATE) paper: http://biorxiv.org/content/early/2017/03/24/120378\n* [Uniform Manifold Approximation and Projection (UMAP)](https://github.com/lmcinnes/umap) is a dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. The algorithm is founded on three assumptions about the data. Run from R: https://gist.github.com/crazyhottommy/caa5a4a4b07ee7f08f7d0649780832ef\n* [umapr](https://github.com/ropenscilabs/umapr) UMAP dimensionality reduction in R\n* [Understanding UMAP](https://pair-code.github.io/understanding-umap/) very nice one to read!\n\n* [Survival analysis of TCGA patients integrating gene expression (RNASeq) data](https://www.biostars.org/p/153013/)\n* [Tutorial: Machine Learning For Cancer Classification](https://www.biostars.org/p/85124/). It has four parts.\n* [Learning bash scripting for beginners](http://www.cyberciti.biz/open-source/learning-bash-scripting-for-beginners/)  \n* [Bedtools tutorial](http://quinlanlab.org/tutorials/cshl2013/bedtools.html)  \n* [Gemini](http://gemini.readthedocs.org/en/latest/#tutorials) explores your vcf, and [slides](https://speakerdeck.com/arq5x).\n* [GNU parallel](https://www.biostars.org/p/63816/)  \n* [A Tutorial on Principal Component Analysis](http://arxiv.org/abs/1404.1100)  \n* [StatQuest: PCA clearly explained](https://www.youtube.com/watch?v=fRiEZWQ-WT8)\n* [Computing Workflows for Biologists: A Roadmap](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1002303)  \n* [Best Practices for Scientific Computing](http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.1001745)\n* [Google's R Style Guide](https://google.github.io/styleguide/Rguide.xml)\n\n### MOOC(Massive Open Online Courses)\n\n* [The Open Source Data Science Masters](http://datasciencemasters.org/)\n* [Path to a free self-taught education in Data Science!](https://github.com/open-source-society/data-science)\n* [Path to a free self-taught education in Bioinformatics!](https://github.com/open-source-society/bioinformatics)\n* [CODING CLUB TUTORIALS](https://ourcodingclub.github.io/tutorials/)\n* [Udacity](https://www.udacity.com/)\n* [Coursera](https://www.coursera.org/)\n* [edx](https://www.edx.org/)\n\n### git and version control\n\n* [git intro by github](https://github.github.io/on-demand/)\n* [How to Write a Git Commit Message](https://chris.beams.io/posts/git-commit/)\n* [Happy Git and GitHub for the useR](http://happygitwithr.com/) A book by Jenny Bryan.\n* [learn git branching](http://learngitbranching.js.org/)\n* [A Git Workflow Walkthrough Series](http://vallandingham.me/git-workflow.html)\n* [paper:A Quick Introduction to Version Control with Git and GitHub](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004668)\n* [paper:Ten Simple Rules for Taking Advantage of Git and GitHub](http://journals.plos.org/ploscompbiol/article?id=10.1371%2Fjournal.pcbi.1004947)\n* [software carpentry git novice lesson](http://swcarpentry.github.io/git-novice/)\n* [git best practise](https://sethrobertson.github.io/GitBestPractices/)\n* [git-hub cheatsheet](https://github.com/tiimgreen/github-cheat-sheet#readme)\n* [oh shit git!](http://ohshitgit.com/) Git is hard: screwing up is easy, and figuring out how to fix your mistakes is fucking impossible. Git documentation has this chicken and egg problem where you can't search for how to get yourself out of a mess, unless you already know the name of the thing you need to know about in order to fix your problem.\n* [How to undo (almost) anything with Git](https://github.com/blog/2019-how-to-undo-almost-anything-with-git)\n* [A guide for astronauts (now, programmers using Git) about what to do when things go wrong: git flight rules](https://github.com/k88hudson/git-flight-rules)\n* An opinionated intermediate/advanced Git book: [git in practise](https://github.com/GitInPractice/GitInPractice#readme)\n* [shournal](https://github.com/tycho-kirchner/shournal) Note: for a more formal introduction please read Bashing irreproducibility with shournal on [bioRxiv](https://www.biorxiv.org/content/10.1101/2020.08.03.232843v1). Save your bash history, record how a file is generated!\n\n### blogs\n\n* [blogdown](https://github.com/rstudio/blogdown) from yihui xie.\n* [Jekyll Jupyter Notebook plugin](https://github.com/red-data-tools/jekyll-jupyter-notebook)\n* [How to Use Plotly with Jekyll and Github Pages](http://ryankuhn.net/blog/How-To-Use-Plotly-With-Jekyll)\n* [render Rmd pages into blog posts using updated rmarkdown::render function](https://sbamin.com/blog/2017/05/hello-r-jekyll/)\n\n### data management\n\n* [youtube video from softwarecarpentry](https://www.youtube.com/watch?v=3MEJ38BO6Mo)\n* [research data management: the-turing-way](https://the-turing-way.netlify.app/reproducible-research/rdm/rdm-fair.html)\n* [How to FAIR](https://howtofair.dk/)\n* [The FAIR Guiding Principles for scientific data management and stewardship](https://www.nature.com/articles/sdata201618)\n* [Developing a modern data workflow for living data](https://www.biorxiv.org/content/early/2018/06/12/344804)\n* [online course CN-2559-BEST-PRACTICES-BIOMEDICAL-RESEARCH-DATA-MANAGEMENT](https://learn.canvas.net/courses/1854)\n* [Ten Simple Rules for Creating a Good Data Management Plan](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1004525)\n* [Nine simple ways to make it easier to (re)use your data](https://ojs.library.queensu.ca/index.php/IEE/article/view/4608)\n* [Dataone best practise Practices](https://www.dataone.org/best-practices)\n* [Research Data Management: A Primer Publication of the National Information Standards Organization](https://groups.niso.org/apps/group_public/download.php/15375/PrimerRDM-2015-0727%E2%80%A6)\n* [Data management for biologists](https://www.tjelvarolsson.com/blog/data-management-for-biologists/) A blog post by Tjelvar Olsson. Also check his [dtool](https://dtool.readthedocs.io/en/latest/philosophy.html)\n* [peppy]([http://code.databio.org/peppy/) is a python package that provides an API for handling standardized project and sample metadata. If you define your project in Portable Encapsulated Project (PEP) format.\n\n### Automate your workflow, open science and reproducible research\n\n**Automation wins in the long run.**\n\n![](https://cloud.githubusercontent.com/assets/4106146/20045655/b58467e6-a468-11e6-8d63-b44da6a276b1.png)\n\n**STEP 6 is usually missing!**  \n\n![](https://cloud.githubusercontent.com/assets/4106146/19217628/807b5bba-8da5-11e6-8387-5f701d7a9ead.jpg)\n\nThe pic was downloaded from http://biobungalow.weebly.com/bio-bungalow-blog/everybody-knows-the-scientific-method\n\n#### Workflow languages\n\n##### Reviews\n\n* [Streamlining Data-Intensive Biology With Workflow Systems](https://dib-lab.github.io/2020-workflows-paper/) Nice read from Titus Brown Group.\n* A [blog post](https://jmazz.me/blog/NGS-Workflows) comparing bash script, make, snakemake and nextflow.\n* [paper:A review of bioinformatic pipeline frameworks](http://bib.oxfordjournals.org/content/early/2016/03/23/bib.bbw020.long)\n* [Building Infrastructure and Workflows for Clinical Bioinformatics Pipelines](https://www.sciencedirect.com/science/article/pii/S2589408020300156)\n* [Existing Workflow systems](https://github.com/common-workflow-language/common-workflow-language/wiki/Existing-Workflow-systems)\n* [Workflow management software for pipeline development in NGS](https://www.biostars.org/p/115745/)\n* [Awesome pipeline toolkit list](https://github.com/pditommaso/awesome-pipeline)\n\n##### Snakemake\n\n* [Snakemake](https://github.com/snakemake/snakemake) [[Docs](https://snakemake.readthedocs.io)] [[Publication](https://academic.oup.com/bioinformatics/article/28/19/2520/290322)]\n* [Snakemake tutorial from Titus Brown 2019](https://github.com/ctb/2019-snakemake-ucdavis)\n* [Snakemake tutorial from Titus Brown 2020](https://hackmd.io/jXwbvOyQTqWqpuWwrpByHQ?view)\n* [snakePipes: facilitating flexible, scalable and integrative epigenomic analysis](https://github.com/maxplanck-ie/snakepipes) [[Publication](https://academic.oup.com/bioinformatics/article/35/22/4757/5499080)]\n* [Snk: A Snakemake CLI and Workflow Management System](https://joss.theoj.org/papers/10.21105/joss.07410)\n\nI am using snakemake and so far is very happy about it!\n\n##### Nextflow\n\n* [Nextflow](https://www.nextflow.io/) [[Docs](https://www.nextflow.io/docs/latest/index.html)] [[Publication](https://www.nature.com/articles/nbt.3820)]\n* [Nextflow DSL 2 modular syntax](https://www.nextflow.io/docs/latest/dsl2.html) [[Original GitHub issue](https://github.com/nextflow-io/nextflow/issues/984)]\n* [Nextflow Camp DSL 2 tutorial 2019](https://github.com/nextflow-io/nfcamp-tutorial)\n* [CZ Biohub Nextflow tutorial 2019](https://github.com/czbiohub/nextflow-tutorial-2019)\n* [Nextflow workshop tutorial 2018](https://nextflow-io.github.io/nf-hack18/)\n* [Nextflow pipeline examples](https://www.nextflow.io/example1.html)\n* [The nf-core framework for community-curated bioinformatics pipelines](https://nf-co.re/) [[Existing Workflows](https://nf-co.re/pipelines)] [[Publication](https://rdcu.be/b1GjZ)]\n* [Curated list of Nextflow pipelines](https://github.com/nextflow-io/awesome-nextflow)\n* [NGS pipelines by nextflow core](https://github.com/nf-core)\n* [nextflow tower](https://tower.nf/)\n* [A Nextflow pipeline assembler for genomics](https://github.com/assemblerflow/assemblerflow) and [flowcraft](https://github.com/assemblerflow/flowcraft) Now you can track both the execution of a nextflowio pipeline AND the reports that it generates in real-time! You can even follow the reports (https://tinyurl.com/y854vftf ) and the pipeline execution.\n\n#### Reproducible research\n\n* [Data Skills for Reproducible Research](https://psyteachr.github.io/reprores-v3/)\n* [pracpac: Practical R Packaging with Docker](https://arxiv.org/abs/2303.07876)\n* [rix: Reproducible Environments with Nix](https://docs.ropensci.org/rix/) not only for R package versions but also R versions and operating systems. I will try it!\n* [Awesome youtube video for reproducible workflow](https://www.youtube.com/watch?v=s3JldKoA0zw&feature=youtu.be)\n*  A great book Building reproducible analytical pipelines with R https://raps-with-r.dev/preface.html\n* [A Realistic Guide to Making Data Available Alongside Code to Improve Reproducibility](https://github.com/karthik/ddd)\n* A must read: [Parallel sequencing lives, or what makes large sequencing projects successful ](https://academic.oup.com/gigascience/article/6/11/gix100/4557140)\n* [A Reproducible Data Analysis Workflow with R Markdown, Git, Make, and Docker](https://psyarxiv.com/8xzqy/) https://github.com/aaronpeikert/reproducible-research\n* [Reproducibility starts at home](http://www.jonzelner.net/statistics/make/docker/reproducibility/2016/05/31/reproducibility-pt-1/) A series of blog posts by Jon Zelner.\n* [docker intro](https://staph-b.github.io/docker-builds/)\n* [cyverse Reproducibility Tour](https://learning.cyverse.org/projects/cyverse-cyverse-reproducbility-tutorial/en/latest/index.html#)\n* [Conda hacks for data science efficiency](http://ericmjl.com/blog/2018/12/25/conda-hacks-for-data-science-efficiency/)\n* [Practical Computational Reproducibility in the Life Sciences](https://www.cell.com/cell-systems/fulltext/S2405-4712(18)30140-6?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS2405471218301406%3Fshowall%3Dtrue) from Cell Systems.\n* [Analysis validation has been neglected in the Age of Reproducibility](https://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.3000070)\n* [The Life & Times of a Reproducible Clinical Project](https://github.com/jenniferthompson/RMedicine2018) https://jenthompson.me/slides/rmedicine2018/rmedicine2018#1\n* [github Actions for R](https://speakerdeck.com/jimhester/github-actions-for-r)\n* [Automate testing of your R package using Travis CI, Codecov, and testthat](https://jef.works/blog/2019/02/17/automate-testing-of-your-R-package/) by Jean Fan.\n* [Reproducible computational environments using containers](https://dme26.github.io/docker-introduction/)\n* [docker intro by Cyverse](https://cyverse-cybercarpentry-container-workshop-2018.readthedocs-hosted.com/en/latest/docker/dockerintro.html) and [singularity](https://cyverse-container-camp-workshop-2018.readthedocs-hosted.com/en/latest/index.html) by upendra devisetty. I met him in UC Davis during 2018 ANGUS :)\n* [rocker/binder](https://github.com/rocker-org/binder) Adds binder abilities on top of the rocker/tidyverse images.\n* [Embedding containerized workflows inside data science notebooks enhances reproducibility](https://www.biorxiv.org/content/early/2018/05/02/309567)\n* [workflowr](https://jdblischak.github.io/workflowr/index.html): organized + reproducible + shareable data science in R\n* [Singularity](http://singularity.lbl.gov/) Singularity enables users to have full control of their environment. Singularity containers can be used to package entire scientific workflows, software and libraries, and even data. This means that you don’t have to ask your cluster admin to install anything for you - you can put it in a Singularity container and run.\n* [EMBL-bioIT singularity workshop](https://git.embl.de/grp-bio-it/singularity-training-2019)\n* [countinous analysis](https://github.com/greenelab/continuous_analysis) [Reproducibility of computational workflows is automated using continuous analysis](http://www.nature.com/nbt/journal/v35/n4/full/nbt.3780.html)  \n* [The hard road to reproducibility](http://science.sciencemag.org/content/354/6308/142) commentary on Science Magzine.\n* [Five selfish reasons to work reproducibly](http://genomebiology.biomedcentral.com/articles/10.1186/s13059-015-0850-7) Genome Biology paper.\n* [Make lessons from software carpentry](http://swcarpentry.github.io/make-novice/)\n* [biomake](https://github.com/evoldoers/biomake) GNU-Make-like utility for managing builds and complex workflows.\n* [drake](https://github.com/ropensci/drake) An R-focused pipeline toolkit for reproducibility and high-performance computing. Snakemake in R.\n* [STAT545 Automating data analysis pipelines](https://stat545-ubc.github.io/automation00_index.html)\n* [biostar post:Job Manager to parallelize otherwise consecutive bash scripts](https://www.biostars.org/p/174468/)\n* [initial steps toward reproducible research](http://kbroman.org/steps2rr/#TAGC16)\n* [JupyterLab: the next generation of the Jupyter Notebook](http://blog.jupyter.org/2016/07/14/jupyter-lab-alpha/)\n* [Deepnote](https://www.deepnote.com) - Better UI for Jupyter and enables collaboration & working online without installing anything.\n* [R notebook](http://data-steve.github.io/setting-up-r-notebook/)\n* [CoCAL](https://cocalc.com/) Collaborative Calculation in the Cloud\n* [BEAKER THE DATA SCIENTIST'S LABORATORY](http://beakernotebook.com/)\n* [nteract] notebook (https://nteract.io/)\n* A video by  Dr.Keith A. Baggerly from MD Anderson [The Importance of Reproducible Research in High-Throughput Biology](https://www.youtube.com/watch?v=7gYIs7uYbMo)  very interesting, and Keith is really a fun guy!\n* [paper: Ten Simple Rules for Reproducible Computational Research](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1003285)\n* [open-research](https://github.com/svaksha/aksh/blob/master/open-research.md#arr)\n* [Best Practice Data Life Cycle Approaches for the Life Sciences](http://www.biorxiv.org/content/early/2017/07/24/167619)\n* [Good Enough Practices in Scientific Computing](http://arxiv.org/abs/1609.00037) We present a set of computing tools and techniques that every researcher can and should adopt. These recommendations synthesize inspiration from our own work, from the experiences of the thousands of people who have taken part in Software Carpentry and Data Carpentry workshops over the past six years, and from a variety of other guides. Unlike some other guides, our recommendations are aimed specifically at people who are new to research computing.  **Well worth reading!**\n* [A Quick Guide to Organizing Computational Biology Projects](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000424) A must read for computational biologists!\n* [Ten Simple Rules for Digital Data Storage](http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005097)\n* avoid `setwd()` in your R script. [`here_here()`](https://github.com/jennybc/here_here#readme) comes to rescue.\n\n* **Have you ever had problem to reuse one of your own published figures due to copyright of the journal?**\nHere is the [solution](https://storify.com/LorenaABarba/reactions-to-my-tip-on-how-i-use-figshare)! from @LorenaABarba\n\n>As an early adopter of the Figshare repository, I came up with a strategy that serves both our open-science and our reproducibility goals, and also helps with this problem: for the main results in any new paper, we would share the data, plotting script and figure under a CC-BY license, by first uploading them to Figshare.\n\n### Survival curve\n\n* tidymodels survival analysis with [censored](https://censored.tidymodels.org/) https://hfrick.github.io/rstudio-conf-2022/#/section\n* [Survival Analysis in R](http://www.emilyzabor.com/tutorials/survival_analysis_in_r_tutorial.html) This tutorial was originally presented at the Memorial Sloan Kettering Cancer Center R-Presenters series on August 30, 2018 by [Emily](http://www.emilyzabor.com)\n* [Survival plots have never been so informative: survminer package](http://r-addict.com/2016/05/23/Informative-Survival-Plots.html)\n* posts for survival analysis:  \n** [Survival Analysis - 1](http://justanotherdatablog.blogspot.com/2015/08/survival-analysis-1.html)  KM estimator  \n** [Survival Analysis - 2](http://justanotherdatablog.blogspot.com/2015/08/survival-analysis-2.html)  Cox's proportional hazards model  \n** [Overall Survival Curves for TCGA and Tothill by RD Status](http://bioinformatics.mdanderson.org/Supplements/ResidualDisease/Reports/osCurves.html)  \n** [Survival analysis of TCGA patients integrating gene expression (RNASeq) data](https://www.biostars.org/p/153013/)  \n* [survminer](http://www.sthda.com/english/wiki/survminer-0-3-0)\n* [survival analysis with TCGA](http://bioconnector.org/r-survival.html)\n* [Kaplan Meier Mistakes](https://towardsdatascience.com/kaplan-meier-mistakes-48cd9e168b09) a blog post by https://twitter.com/BencomoTomas\n* [TCGA survival](https://tcga-survival.com/)\n\n### Organize research for a group\n\n* [slack](https://slack.com/):A messaging app for teams.\n* [Ryver](http://www.ryver.com/ryver-vs-slack/).\n* [Trello](https://trello.com/) lets you work more collaboratively and get more done.\n\n\n### Clustering\n\n* [densityCut](http://m.bioinformatics.oxfordjournals.org/content/early/2016/04/23/bioinformatics.btw227.short?rss=1): an efficient and versatile topological approach for automatic clustering of biological data\n* [Interactive visualisation and fast computation of the solution path: convex bi-clustering](https://www.youtube.com/watch?v=2g-akN6q8aI) by [Genevera Allen](http://www.stat.rice.edu/~gallen/software.html)\n[cvxbiclustr](https://cran.r-project.org/web/packages/cvxbiclustr/index.html) and the clustRviz package coming.\n* [optCluster](https://cran.r-project.org/web/packages/optCluster/index.html): An R Package for Determining the Optimal Clustering Algorithm.\n* [iClusterPlus](https://www.bioconductor.org/packages/release/bioc/html/iClusterPlus.html) Integrative clustering of multiple genomic data using a joint latent variable model.  \n* [ConsensusClusterPlus](https://bioconductor.org/packages/release/bioc/html/ConsensusClusterPlus.html) algorithm for determining cluster count and membership by stability evidence in unsupervised analysis.\n\n### CRISPR related\n\n* [CRISPR GENOME EDITING MADE EASY](https://www.deskgen.com/landing/)\n* [CRISPR design from Japan](http://crispr.dbcls.jp/)\n* [CRISPResso](http://crispresso.rocks/):Analysis of CRISPR-Cas9 genome editing outcomes from deep sequencing data\n* [CRISPR-DO](http://cistrome.org/crispr/): A whole genome CRISPR designer and optimizer in human and mouse\n* [CCTop](http://crispr.cos.uni-heidelberg.de/) - CRISPR/Cas9 target online predictor\n* [DESKGEN](https://horizon.deskgen.com/landing/#/)\n* [Genome-wide Unbiased Identifications of DSBs Evaluated by Sequencing (GUIDE-seq) is a novel method the Joung lab has developed to identify the off-target sites of CRISPR-Cas RNA-guided Nucleases](http://www.jounglab.org/guideseq)\n* [WTSI Genome Editing (WGE) is a website that provides tools to aid with genome editing of human and mouse genomes](http://www.sanger.ac.uk/htgt/wge/)\n\n### vector arts for life sciences\n\n* [biorender](https://biorender.io/)\n* [The Noun Project](https://thenounproject.com/)\n* https://bioicons.com/\n* https://healthicons.org/\n* [reactome icon](https://reactome.org/icon-lib)\n* [Inovative genomic Institute glossary](https://innovativegenomics.org/resources/educational-materials/glossary/)\\  \n* https://smart.servier.com/category/cellular-biology/nucleic-acids/\n* https://www.vecteezy.com/\n* https://www.freepik.com/\n* https://pixabay.com/\n* make workflow diagram https://app.diagrams.net/\n"
        },
        {
          "name": "R_inferno.pdf",
          "type": "blob",
          "size": 925.587890625,
          "content": null
        },
        {
          "name": "R_reproducible_workflow-2018-12-14.pdf",
          "type": "blob",
          "size": 681.787109375,
          "content": null
        },
        {
          "name": "R_tricks.md",
          "type": "blob",
          "size": 40.34375,
          "content": "### add system env to check if you accidentally used && when comparing 2 vectors\n\nhttps://twitter.com/milesmcbain/status/1194029161490202625?s=12\n\n`Sys.setenv(\"_R_CHECK_LENGTH_1_LOGIC2_\" = verbose)` and\n`Sys.setenv(\"_R_CHECK_LENGTH_1_CONDITION_\" = TRUE)` \n\n\n### add a unique id for rows with the same values on columns \n\nhttp://stackoverflow.com/questions/41231209/r-define-distinct-pattern-from-values-of-multiple-variables\n```r\ndplyr::group_indices()\n\ndata.frame(x=c(0,0,0,1,1,1), y=c(0,0,1,0,1,1))\n\ngroup_indices(df, x, y)\n\ndf %>% mutate(pattern = group_indices_(df, .dots = c('x', 'y')))\n\n```\n\n### alternative to nested `ifelse`:\n```r\nlibrary(dplyr)\ncase_when\n\ncars %>% as_tibble %>% mutate(case_code = case_when(\n                              speed == 4 & dist == 2 ~ \"this\",\n                              dist > 6 & dist == 10 ~ \"is\",\n                              speed >=10 & dist >= 18 ~ \"awesome\"))\n                   \n\n```\n\n### choose the max value of a group\n\n```r\ndf %>% group_by(A, B) %>% top_n(n=1, wt= C)\ndf %>% group_by(A,B) %>% slice(which.max(C))\ndf %>% group_by(A, B) %>% filter(value == max(C)) \n```\n### filter by group\n```r\ndf %>% group_by(A,B) %>% filter(all(C>10))\ndf %>% group_by(A,B) %>% filter(any(C>10))\n```\n\n### first and last row in a group\n```r\ndf %>% arrange(stopSequence) %>% group_by(id) %>% slice(c(1,n()))\n```\n### change all the factor columns to characters\n```r\nlibrary(purrr)\nlibrary(dplyr)\nbob %>% map_if(is.factor, as.character)\n\nbob %>% mutate_if(is.factor, as.character)\n```\n### cut groups on the fly in ggplot2\n\n```r\np <- ggplot(diamonds, aes(x = carat, y = price))\n\n# Use cut_interval\np + geom_boxplot(aes(group = cut_interval(carat, n=10)))\n\n# Use cut_number\np + geom_boxplot(aes(group = cut_number(carat, n =10)))\n\n# Use cut_width\np + geom_boxplot(aes(group = cut_width(carat, width = 0.25)))\n```\n\n### dplyr cut_width\n\n```r\ndiamonds %>% count(cut_width(carat, 0.5))\n```\n### reorder boxplot by median\n\n```r\nggplot(mpg) + geom_boxplot(aes(x = reoder(class, hwy, FUN = median), y = hwy)) \n\n```\n### weight box plot and violin plot by number of observations\n```r\nggplot(diamonds, aes(x = cut, y = price)) + geom_boxplot(varwidth = TRUE)\n\nlibrary(dplyr)\nmammals2 <- mammals %>%\n  group_by(vore) %>%\n  mutate(n = n()/nrow(mammals))\n  \nggplot(mammals2, aes(x = vore, y = sleep_total, fill = vore)) +\n  geom_violin(aes(weight = n), col = NA)\n```\n\n### Reorder rows using custom order\n\n```r\nlibrary(tibble)\ndf<- tibble(category=LETTERS[1:3], b=1:3)\nx<- c(\"C\", \"A\", \"B\")\n\n# reorder\ndf %>% slice(match(x, category))\n# A tibble: 3 × 2\n  category     b\n     <chr> <int>\n1        C     3\n2        A     1\n3        B     2\n\n```\n\nhttps://stackoverflow.com/questions/46129322/arranging-rows-in-custom-order-using-dplyr\n\n\n```{r}\n> df<- data.frame(num = c(1,3,4,5,3,2), type = c(\"A\", \"B\", \"C\", \"C\", \"A\", \"B\"))\n> df\n  num type\n1   1    A\n2   3    B\n3   4    C\n4   5    C\n5   3    A\n6   2    B\n\n> df %>% arrange(match(type, c(\"C\", \"A\", \"B\")), desc(num))\n  num type\n1   5    C\n2   4    C\n3   3    A\n4   1    A\n5   3    B\n6   2    B\n```\n\n### filter out groups of rows by dplyr if a column in a group are all smaller than a number\n\n```r\ndf %>% group_by(A,B) %>% filter(all(C >10)) \n```\n\n### ggplot geom_density()\n\n>The default combined density plot extends the range of all values to the total extent of the entire dataset, which may be a bit confusing. In the fourth plot, adjust for this by setting trim = TRUE inside `geom_density()`. However, be cautious. Since the distributions are cut off at the extreme ends, the area under the curve technically is not equal to one anymore.\n\n### parallel coordinate plot\n\n```r\nrequire(GGally)\nggparcoord(iris, columns = 1:4, groupColumn = 5, scale = \"globalminmax\", order = \"anyClass\", alphaLines = 0.4) \n```\n![](https://cloud.githubusercontent.com/assets/4106146/21956921/a139fc92-da50-11e6-9630-f56805ebd5d3.png)\n\n### plot MDS with ggfortify\n\nAs you can probably imagine, distance matrices (class dist) contain the measured distance between all pair-wise combinations of many points. For example, the eurodist dataset contains the distances between major European cities. dist objects lend themselves well to `ggfortify::autoplot()`.\n\nThe `stats::cmdscale()` function performs Classical Multi-Dimensional Scaling and returns point coodinates as a matrix. Although autoplot will work on this object, it will produce a heatmap, and not a scatter plot. However, if either `eig = TRUE`, `add = TRUE` or `x.ret = TRUE` is specified, stats::cmdscale() will return a list instead of matrix. In these cases, `ggfortify::autoplot` can deal with the output. Details on these arguments can be found in the docs (?cmdscale).\n\n```r\n# ggfortify and eurodist are available\n\n# Autoplot + ggplot2 tweaking\nautoplot(eurodist) + \nlabs( x = \"\", y = \"\") + \ncoord_fixed() +\ntheme(axis.text.x = element_text(angle = 90, hjust =1, vjust = 0.5))\n\n# Autoplot of MDS\nautoplot(cmdscale(eurodist, eig = TRUE), label = TRUE, label.size =3, size = 0)\n\n```\n\n### build multiple plots\nalso check `purrr`, Hadely has not used `plyr` for long time. ref...twitter\n```r\nlibrary(plyr)\nmyplots<- dlplyr(mtcars, .(cyl), function(df){\n        ggplot(df, aes(mpg, wt)) +\n                geom_point() +\n                xlim(range(mtcars$mpg)) +\n                ylim(range(mtcars$wt)) +\n                ggtilte(paste(df$cyl[1], \"cylinders\"))})\n# by position                \nmyplots[[2]]\n\n# by name\nmyplots[[\"4\"]]\nlibrary(gridExtra)\n\ngrid.arrange(myplots[[1]], myplots[[2]], ncol = 2)\ndo.call(grid.arrange, myplots)\n\n```\n\n### plot k-means result with ggfortify\n\n```r\nlibrary(ggfortify)\n# perform clustering\niris_k<- kmeans(iris[-5], center = 3)\n\n# autplot: coloring according to cluster\nautoplot(iris_k, data = iris, frame = TRUE)\n\n# autoplot: coloring according to species\nautoplot(iris_k, data = iris, frame = TRUE, col = \"Species\")\n\n```\n### join, filter multiple datasets\n\n```r\ndf1 %>% left_join(df2) %>% left_join(df3)....\n\nlibrary(purrr)\n\ntables<- list(df1,df2,df3)\nreduce(tables, left_join, by = \"key\")\n\nlist(more_artists, more_bands, supergroups) %>% \n  # Return rows of more_artists in all three datasets\n  reduce(semi_join, by = c(\"first\", \"last\"))\n```\n### I need more color for my ggplot2\nread http://novyden.blogspot.com/2013/09/how-to-expand-color-palette-with-ggplot.html\n\n```r\ncolorRampPalette(brewer.pal(9, \"Set1\"))(26)\n [1] \"#E41A1C\" \"#AC3A4D\" \"#755A7F\" \"#3D7AB1\" \"#3D8B99\" \"#449B75\" \"#4BAB52\" \"#5F975F\" \"#77787B\" \"#8F5998\"\n[11] \"#AC5782\" \"#CD674E\" \"#EE771A\" \"#FF9308\" \"#FFBC18\" \"#FFE528\" \"#F4EA31\" \"#D7B42E\" \"#BB7E2A\" \"#AC5934\"\n[21] \"#C66764\" \"#E07494\" \"#F381BD\" \"#D589B1\" \"#B791A5\" \"#999999\"\n\n```\n### balloon plot alternative to heatmap\nread here https://datascience.blog.wzb.eu/2017/01/24/creating-a-balloon-plot-as-alternative-to-a-heat-map-with-ggplot2/\n\n![](https://cloud.githubusercontent.com/assets/4106146/22408099/80c69cd8-e638-11e6-8acc-4964031eadd9.png)\n```r\n# Create a \"balloon plot\" as alternative to a heatmap with ggplot2\n# \n# January 2017\n# Author: Markus Konrad <markus.konrad@wzb.eu>, WZB Berlin Social Science Center\n\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(ggplot2)\n\n# define the variables that will be displayed in the columns\nvars <- c('awake', 'sleep_total', 'sleep_rem')\n\n# prepare the data: we use the \"msleep\" dataset which comes with ggplot2\ndf <- msleep[!is.na(msleep$vore), c('name', 'vore', vars)] %>%  # only select the columns we need from the msleep dataset\n  group_by(vore) %>% sample_n(5) %>% ungroup() %>%              # select 5 random rows from each \"vore\" group as subset\n  gather(key = variable, value = value, -name, -vore) %>%       # make a long table format: gather columns in rows\n  filter(!is.na(value)) %>%                                     # remove rows with NA-values -> those will be empty spots in the plot\n  arrange(vore, name)                                           # order by vore and name\n\n# add a \"row\" column which will be the y position in the plot: group by vore and name, then set \"row\" as group index\ndf <- df %>% mutate(row = group_indices_(df, .dots=c('vore', 'name')))\n# add a \"col\" column which will be the x position in the plot: group by variable, then set \"col\" as group index\ndf <- df %>% mutate(col = group_indices_(df, .dots=c('variable')))\n\n# get character vector of variable names for the x axis. the order is important, hence arrange(col)!\nvars_x_axis <- c(df %>% arrange(col) %>% select(variable) %>% distinct())$variable\n# get character vector of observation names for the y axis. again, the order is important but \"df\" is already ordered\nnames_y_axis <- c(df %>% group_by(row) %>% distinct(name) %>% ungroup() %>% select(name))$name\n\n# now plot\n# make color dependent on vore, size and alpha dependent on value\n# x and y must be set as factor() otherwise scale_x/y_discrete() won't work\nggplot(df, aes(x=factor(col), y=factor(row), color=vore, size=value, alpha=value)) +\n  geom_point() +    # plot as points\n  geom_text(aes(label=value, x=col + 0.25), alpha=1.0, size=3) +   # display the value next to the \"balloons\"\n  scale_alpha_continuous(range=c(0.3, 0.7)) +\n  scale_size_area(max_size = 5) +\n  scale_x_discrete(breaks=1:length(vars_x_axis), labels=vars_x_axis, position='top') +   # set the labels on the X axis\n  scale_y_discrete(breaks=1:length(names_y_axis), labels=names_y_axis) +                 # set the labels on the Y axis\n  theme_bw() +\n  theme(axis.line = element_blank(),            # disable axis lines\n        axis.title = element_blank(),           # disable axis titles\n        panel.border = element_blank(),         # disable panel border\n        panel.grid.major.x = element_blank(),   # disable lines in grid on X-axis\n        panel.grid.minor.x = element_blank())   # disable lines in grid on X-axis\n```\n\n### write a list of dataframe to files.\n\n```r\ndf_list<- split(df, df$A)\nsapply(names(df_list), function (x) write.table(df_list[[x]], file=paste(x, \"txt\", sep=\".\")))\n```\n\n### read in a list of data frames from the current directory \n\n```r\nfiles<- as.list(dir(\".\", pattern= \".tsv\"))\n\n## need to add the file name into a column\ndatlist <- lapply(mix.files, function(f) {\n        dat = read.table(f, header =T, sep =\"\\t\", quote = \"\\\"\")\n        dat$sample = gsub(\".tsv\", \"\", f)\n        return(dat)\n})\n\ndata<- do.call(rbind, datlist)\n## or use dplyr: bind_rows(datlist, .id = \"sample\")\n\n## if each file has a common column, e.g. RNAseq HTSeq counts for many samples, and you want to make a big dataframe with first column\n## is the gene-id and columns of raw counts\nCCLE_counts<- reduce(datlist, left_join, by = \"GeneID\")\n```\n\nor https://github.com/vsbuffalo/devnotes/wiki/Data-Analysis-Patterns by Vince Buffalo.\n\n```r\n### example setup:\nDIR <- 'path/to/data' # change to directory you can write files to.\n# filenames to make example work:\nfiles <- c('sampleA_rep01.tsv', 'sampleA_rep02.tsv','sampleB_rep01.tsv', \n           'sampleB_rep02.tsv', 'sampleC_rep01.tsv', 'sampleC_rep02.tsv')\n\n# write test files for example (iris a bunch of times)\nwalk(files, ~ write_tsv(iris, file.path(DIR, .)))\n\n### Pattern:\n# grab all files programmatically: \ninput_files <- list.files(DIR, \n                          pattern='sample.*\\\\.tsv', full.names=TRUE)\n\n# data loading pattern:\nall_data <- tibble(file=input_files) %>% \n   # read data in (note: in general, best to \n   # pass col_names and col_types to map)\n   mutate(data=map(file, read_tsv)) %>% \n   # get the file basename (no path); if \n   # your metadata is in the path, change accordingly!\n   mutate(basename=basename(file)) %>% \n   # extract out the metadata from the base filename\n   extract(basename, into=c('sample', 'rep'), \n           regex='sample([^_]+)_rep([^_]+)\\\\.tsv') %>% \n   unnest(data)  # optional, depends on what you need.\n```\nor use `purrr::map_df`\n\n```{r}\nf <- list.files(\n  \"my_folder\",\n   pattern = \"*.csv\",\n   full.names = TRUE)\n\nd <- purrr::map_df(f, readr::read_csv, .id = \"id\")\n\nAlso check\n\n```\n?purrr::map_dfr and ?purrr::map_dfc\n```\n```\n### gather multiple columns\nread http://stackoverflow.com/questions/41880796/grouped-multicolumn-gather-with-dplyr-tidyr-purrr\n```r\nhave\n#> # A tibble: 4 × 8\n#>    gene sample genotype1 genotype2 genotype3 freq1 freq2 freq3\n#>   <chr>  <chr>     <chr>     <chr>     <chr> <dbl> <dbl> <dbl>\n#> 1    gX     s1        AA        AC        CC   0.8  0.15  0.05\n#> 2    gX     s2        AA        AC        CC   0.9  0.10  0.00\n#> 3    gY     s1        GG        GT        TT   0.7  0.20  0.10\n#> 4    gY     s2        GG        GT        TT   0.6  0.35  0.05\n\nto\n\nwant\n#> # A tibble: 12 × 4\n#>     gene sample genotype  freq\n#>    <chr>  <chr>    <chr> <dbl>\n#> 1     gX     s1       AA  0.80\n#> 2     gX     s1       AC  0.15\n#> 3     gX     s1       CC  0.05\n#> 4     gX     s2       AA  0.90\n#> 5     gX     s2       AC  0.10\n#> 6     gX     s2       CC  0.00\n#> 7     gY     s1       GG  0.70\n#> 8     gY     s1       GT  0.20\n#> 9     gY     s1       TT  0.10\n#> 10    gY     s2       GG  0.60\n#> 11    gY     s2       GT  0.35\n#> 12    gY     s2       TT  0.05\n\nlibrary(sjmisc)\nto_long(have, keys = \"genos\", values = c(\"genotype\", \"freq\"),\n       c(\"genotype1\", \"genotype2\", \"genotype3\"),\n       c(\"freq1\", \"freq2\", \"freq3\"))\n\n##  A tibble: 12 × 5\n##     gene sample     genos genotype  freq\n##    <chr>  <chr>     <chr>    <chr> <dbl>\n## 1     gX     s1 genotype1       AA  0.80\n## 2     gX     s2 genotype1       AA  0.90\n## 3     gY     s1 genotype1       GG  0.70\n## 4     gY     s2 genotype1       GG  0.60\n## 5     gX     s1 genotype2       AC  0.15\n## 6     gX     s2 genotype2       AC  0.10\n## 7     gY     s1 genotype2       GT  0.20\n## 8     gY     s2 genotype2       GT  0.35\n## 9     gX     s1 genotype3       CC  0.05\n## 10    gX     s2 genotype3       CC  0.00\n## 11    gY     s1 genotype3       TT  0.10\n## 12    gY     s2 genotype3       TT  0.05\n\nlibrary(data.table)\nmelt(setDT(have), id = 1:2, measure = patterns(\"genotype\", \"freq\"))\n\n```\n\n### mutate_at()\n\n```r\n> iris %>% as_tibble()\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl>  <fctr>\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n# ... with 140 more rows\n\n# convert columns to characters\n>iris %>% as_tibble() %>% mutate_at(vars(Sepal.Length:Petal.Width), as.character) %>% head()\n# A tibble: 6 × 5\n  Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n         <chr>       <chr>        <chr>       <chr>  <fctr>\n1          5.1         3.5          1.4         0.2  setosa\n2          4.9           3          1.4         0.2  setosa\n3          4.7         3.2          1.3         0.2  setosa\n4          4.6         3.1          1.5         0.2  setosa\n5            5         3.6          1.4         0.2  setosa\n6          5.4         3.9          1.7         0.4  setosa\n\n```\n\n### mutate_if()\n\nconvert character columns back to double \n```r\niris %>% as_tibble() %>% mutate_at(vars(Sepal.Length:Petal.Width), as.character) %>% mutate_if(is.character, as.double)\n# A tibble: 150 × 5\n   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n          <dbl>       <dbl>        <dbl>       <dbl>  <fctr>\n1           5.1         3.5          1.4         0.2  setosa\n2           4.9         3.0          1.4         0.2  setosa\n3           4.7         3.2          1.3         0.2  setosa\n4           4.6         3.1          1.5         0.2  setosa\n5           5.0         3.6          1.4         0.2  setosa\n6           5.4         3.9          1.7         0.4  setosa\n7           4.6         3.4          1.4         0.3  setosa\n8           5.0         3.4          1.5         0.2  setosa\n9           4.4         2.9          1.4         0.2  setosa\n10          4.9         3.1          1.5         0.1  setosa\n# ... with 140 more rows\n\n```\n\n### diff, lag and lead\n\n```r\n# diff minus the previous number in sequence\n> a<- c(1,2,5,7,9,14)\n> diff(a)\n[1] 1 3 2 2 5\n\n## the long way\n> a\n[1]  1  2  5  7  9 14\n> lag(a, 1)\n[1] NA  1  2  5  7  9\n> a - lag(a,1)\n[1] NA  1  3  2  2  5\n```\n### Window functions and grouped mutate/filter\n\nhttps://cran.r-project.org/web/packages/dplyr/vignettes/window-functions.html\n>A window function is a variation on an aggregation function. Where an aggregation function, like sum() and mean(), takes n inputs and return a single value, a window function returns n values. The output of a window function depends on all its input values, so window functions don’t include functions that work element-wise, like + or round(). Window functions include variations on aggregate functions, like cumsum() and cummean(), functions for ranking and ordering, like rank(), and functions for taking offsets, like lead() and lag().\n\n### move variables to the front of the dataframe with the everything() helper function\n\n```r\nselect(flights, time_hour, air_time, everything())\n```\n\n### plot a table together with a ggplot2 figure\nsee http://www.magesblog.com/2015/04/plotting-tables-alsongside-charts-in-r.html\n\n```r\n# Create some sample data\nCV_1 <- 0.2\nCV_2 <- 0.3\nMean <- 65\nsigma_1 <- sqrt(log(1 + CV_1^2))\nmu_1 <- log(Mean) - sigma_1^2 / 2\nsigma_2 <- sqrt(log(1 + CV_2^2))\nmu_2 <- log(Mean) - sigma_2^2 / 2\nq <- c(0.25, 0.5, 0.75, 0.9, 0.95) \nSummaryTable <- data.frame(\n  Quantile=paste0(100*q,\"%ile\"), \n  Loss_1=round(qlnorm(q, mu_1, sigma_1),1),\n  Loss_2=round(qlnorm(q, mu_2, sigma_2),1)\n  )\n# Create a plot \nlibrary(ggplot2)\nplt <- ggplot(data.frame(x=c(20, 150)), aes(x)) + \n  stat_function(fun=function(x) dlnorm(x, mu_1, sigma_1), \n                aes(colour=\"CV_1\")) + \n  stat_function(fun=function(x) dlnorm(x, mu_2, sigma_2), \n                aes(colour=\"CV_2\")) +\n  scale_colour_discrete(name = \"CV\", \n                        labels=c(expression(CV[1]), expression(CV[2]))) +\n  xlab(\"Loss\") +  \n  ylab(\"Density\") +\n  ggtitle(paste0(\"Two log-normal distributions with same mean of \",\n                 Mean,\", but different CVs\")) \n# Create a table plot\nlibrary(gridExtra)\nnames(SummaryTable) <- c(\"Quantile\", \n              expression(Loss(CV[1])),\n              expression(Loss(CV[2])))\n# Set theme to allow for plotmath expressions\ntt <- ttheme_default(colhead=list(fg_params = list(parse=TRUE)))\ntbl <- tableGrob(SummaryTable, rows=NULL, theme=tt)\n# Plot chart and table into one object\ngrid.arrange(plt, tbl,\n             nrow=2,\n             as.table=TRUE,\n             heights=c(3,1))\n```\n\n![](https://cloud.githubusercontent.com/assets/4106146/25259762/abebc1f8-260d-11e7-9c50-f983dfd51dbf.png)\n\n### remove columns with all NAs\n\n```r\n... %>%\n  select_if(~ !all(is.na(.)))\n  \n # OR equivalent\n  select_if(function(.) !all(is.na(.)))\n  \n janitor::remove_empty_cols()\n\n```\n### replace all NAs with 0 in a df\n\nhttps://stackoverflow.com/questions/45576805/how-to-replace-all-na-in-a-dataframe-using-tidyrreplace-na\n\n```r\n\ndf %>% replace(is.na(.), 0)\ndf %>% %>% mutate_all(coalesce, 0)\n```\n\n### add a new column with rank based on two or more columns of a df\n\n```r\ndf %>% arrange(var1, var2) %>% mutate(my_rank = 1: n())\n\ndf %>% arrange(var1, var2) %>% mutate(my_rank = row_number())\n```\n\n### less know useful functions\n\nhttps://twitter.com/robinson_es/status/953432465514876928\n```r\nrlang::set_names() = purrr::set_names() \n\nrlang::set_names(), tibble::rowid_to_column(), modelr::seq_range(), the .data pronoun, purrr::safely(), dplyr::pull(), stringr::str_replace_all() with a named vector\n\nenframe, deframe, fct_reorder, fct_reorder2\n```\n### ggplot boxplot with whiskers\n\nhttps://stats.stackexchange.com/questions/8137/how-to-add-horizontal-lines-to-ggplot2-boxplot\n\n```r\nbp <- ggplot(iris, aes(factor(Species), Sepal.Width, fill = Species)) +  stat_boxplot(geom ='errorbar')\nbp + geom_boxplot()\n```\n\n### raincloud plots\n\nhttps://micahallen.org/blog-neuroconscience/\n\n```r\nlibrary(readr)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(Hmisc)\nlibrary(plyr)\nlibrary(RColorBrewer)\nlibrary(reshape2)\n\nsource(\"https://gist.githubusercontent.com/benmarwick/2a1bb0133ff568cbe28d/raw/fb53bd97121f7f9ce947837ef1a4c65a73bffb3f/geom_flat_violin.R\")\n\nmy_data<-read.csv(url(\"https://data.bris.ac.uk/datasets/112g2vkxomjoo1l26vjmvnlexj/2016.08.14_AnxietyPaper_Data%20Sheet.csv\"))\n\nhead(X)\nlibrary(reshape2)\nmy_datal <- melt(my_data, id.vars = c(\"Participant\"), measure.vars = c(\"AngerUH\", \"DisgustUH\", \"FearUH\", \"HappyUH\"), variable.name = \"EmotionCondition\", value.name = \"Sensitivity\")\n\nhead(my_datal)\n\nraincloud_theme = theme(\ntext = element_text(size = 10),\naxis.title.x = element_text(size = 16),\naxis.title.y = element_text(size = 16),\naxis.text = element_text(size = 14),\naxis.text.x = element_text(angle = 45, vjust = 0.5),\nlegend.title=element_text(size=16),\nlegend.text=element_text(size=16),\nlegend.position = \"right\",\nplot.title = element_text(lineheight=.8, face=\"bold\", size = 16),\npanel.border = element_blank(),\npanel.grid.minor = element_blank(),\npanel.grid.major = element_blank(),\naxis.line.x = element_line(colour = 'black', size=0.5, linetype='solid'),\naxis.line.y = element_line(colour = 'black', size=0.5, linetype='solid'))\n\nlb <- function(x) mean(x) - sd(x)\nub <- function(x) mean(x) + sd(x)\n\nsumld<- ddply(my_datal, ~EmotionCondition, summarise, mean = mean(Sensitivity), median = median(Sensitivity), lower = lb(Sensitivity), upper = ub(Sensitivity))\n\nhead(sumld)\n\ng <- ggplot(data = my_datal, aes(y = Sensitivity, x = EmotionCondition, fill = EmotionCondition)) +\ngeom_flat_violin(position = position_nudge(x = .2, y = 0), alpha = .8) +\ngeom_point(aes(y = Sensitivity, color = EmotionCondition), position = position_jitter(width = .15), size = .5, alpha = 0.8) +\ngeom_boxplot(width = .1, guides = FALSE, outlier.shape = NA, alpha = 0.5) +\nexpand_limits(x = 5.25) +\nguides(fill = FALSE) +\nguides(color = FALSE) +\nscale_color_brewer(palette = \"Spectral\") +\nscale_fill_brewer(palette = \"Spectral\") +\n# coord_flip() +\ntheme_bw() +\nraincloud_theme\n\ng\n\n```\n\n### ggrepel label points\n\n```r\ndata<- data.frame( \n        x = 1:10,\n        y = rnorm(10),\n        name = c(\"Apple\", \"Banana\", \"Kiwi\", \"Orange\", \"Watermelon\", \n                 \"Grapes\", \"Pear\", \"Cantelope\", \"Tomato\", \"Satusma\")\n)\n\nmy_data<- mutate(data, name_poor = case_when(\n        y < 0 ~ name,\n        TRUE ~ \"\"\n))\n\nggplot(my_data, aes(x = x, y = y)) + \n               geom_point(size = 5) +\n               geom_text_repel(aes(label = name_poor), point.padding = 2)\n```\n### convert a tidy df to a nested json\n\nhttps://stackoverflow.com/questions/50477156/convert-a-tidy-table-to-deeply-nested-list-using-r-and-tidyverse\n\n```r\nlibrary(tidyverse)\nlibrary(stringi)\n\nn_patient = 2\nn_samples = 3\nn_readgroup = 4\nn_mate = 2\n\ndf = data.frame(patient   = rep(rep(LETTERS[1:n_patient], n_samples),2),\n                sample    = rep(rep(seq(1:n_samples), each = n_patient),2),\n                readgroup = rep(stri_rand_strings(n_patient * n_samples * n_readgroup, 6, '[A-Z]'),2),\n                mate      = rep(1:n_mate, each = n_patient * n_samples * n_readgroup)) %>%\n  mutate(file = sprintf(\"%s.%s.%s_%s\", patient, sample, readgroup, mate)) %>%\n  arrange(file)\n\n> head(df)\n  patient sample readgroup mate         file\n1       A      1    FCSDRJ    1 A.1.FCSDRJ_1\n2       A      1    FCSDRJ    2 A.1.FCSDRJ_2\n3       A      1    IAXDPR    1 A.1.IAXDPR_1\n4       A      1    IAXDPR    2 A.1.IAXDPR_2\n5       A      1    MLDBKZ    1 A.1.MLDBKZ_1\n6       A      1    MLDBKZ    2 A.1.MLDBKZ_2\n\n\njson2 <- df %>% nest(-(1:2),.key=readgroups) %>% nest(-1,.key=samples)\njson3 <- df %>% nest(-(1:3),.key=mate) %>% nest(-(1:2),.key=readgroups) %>% nest(-1,.key=samples)\n\njsonlite::toJSON(json3,pretty=T)\n\n# output\n[\n  {\n    \"patient\": \"A\",\n    \"samples\": [\n      {\n        \"sample\": 1,\n        \"readgroups\": [\n          {\n            \"readgroup\": \"FUPEYR\",\n            \"mate\": [\n              {\n                \"mate\": 1,\n                \"file\": \"A.1.FUPEYR_1\"\n              },\n              {\n                \"mate\": 2,\n                \"file\": \"A.1.FUPEYR_2\"\n              }\n...\n```\nAnd if necessary, generalize it:\n\n```r\nvars <- names(df)[-1] # or whatever variables you want to nest, order matters!\nvar_pairs <- map((length(vars)-1):1,~vars[.x:(.x+1)])\njson4 <- reduce(var_pairs,~{nm<-.y[1];nest(.x,.y,.key=!!enquo(nm))},.init=df)\n\njsonlite::toJSON(json4,pretty=T)\n\n[\n  {\n    \"patient\": \"A\",\n    \"sample\": [\n      {\n        \"sample\": 1,\n        \"readgroup\": [\n          {\n            \"readgroup\": \"FUPEYR\",\n            \"mate\": [\n              {\n                \"mate\": 1,\n                \"file\": \"A.1.FUPEYR_1\"\n              },\n              {\n                \"mate\": 2,\n                \"file\": \"A.1.FUPEYR_2\"\n              }\n...\n```\n### reorder within facet ggplot2\n\nhttps://github.com/dgrtwo/drlib/blob/master/R/reorder_within.R\n\n```r\n#' Reorder an x or y axis within facets\n#'\n#' Reorder a column before plotting with faceting, such that the values are ordered\n#' within each facet. This requires two functions: \\code{reorder_within} applied to\n#' the column, then either \\code{scale_x_reordered} or \\code{scale_y_reordered} added\n#' to the plot.\n#' This is implemented as a bit of a hack: it appends ___ and then the facet\n#' at the end of each string.\n#'\n#' @param x Vector to reorder.\n#' @param by Vector of the same length, to use for reordering.\n#' @param within Vector of the same length that will later be used for faceting\n#' @param fun Function to perform within each subset to determine the resulting\n#' ordering. By default, mean.\n#' @param sep Separator to distinguish the two. You may want to set this manually\n#' if ___ can exist within one of your labels.\n#' @param ... In \\code{reorder_within} arguments passed on to \\code{\\link{reorder}}.\n#' In the scale functions, extra arguments passed on to\n#' \\code{\\link[ggplot2]{scale_x_discrete}} or \\code{\\link[ggplot2]{scale_y_discrete}}.\n#'\n#' @source \"Ordering categories within ggplot2 Facets\" by Tyler Rinker:\n#' \\url{https://trinkerrstuff.wordpress.com/2016/12/23/ordering-categories-within-ggplot2-facets/}\n#'\n#' @examples\n#'\n#' library(tidyr)\n#' library(ggplot2)\n#'\n#' iris_gathered <- gather(iris, metric, value, -Species)\n#'\n#' # reordering doesn't work within each facet (see Sepal.Width):\n#' ggplot(iris_gathered, aes(reorder(Species, value), value)) +\n#'   geom_boxplot() +\n#'   facet_wrap(~ metric)\n#'\n#' # reorder_within and scale_x_reordered work.\n#' # (Note that you need to set scales = \"free_x\" in the facet)\n#' ggplot(iris_gathered, aes(reorder_within(Species, value, metric), value)) +\n#'   geom_boxplot() +\n#'   scale_x_reordered() +\n#'   facet_wrap(~ metric, scales = \"free_x\")\n#'\n#' @export\nreorder_within <- function(x, by, within, fun = mean, sep = \"___\", ...) {\n  new_x <- paste(x, within, sep = sep)\n  stats::reorder(new_x, by, FUN = fun)\n}\n\n\n#' @rdname reorder_within\n#' @export\nscale_x_reordered <- function(..., sep = \"___\") {\n  reg <- paste0(sep, \".+$\")\n  ggplot2::scale_x_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n}\n\n\n#' @rdname reorder_within\n#' @export\nscale_y_reordered <- function(..., sep = \"___\") {\n  reg <- paste0(sep, \".+$\")\n  ggplot2::scale_y_discrete(labels = function(x) gsub(reg, \"\", x), ...)\n}\n```\n\n### separate multiple values in a field\n\n```r\nlibrary(tidyverse)\n> test_scores<- data_frame(student = c(\"Amy\", \"Belle\", \"Candice\"), \n+                          score= c(\"75-81-86\",\"87-89-90\",\"92-93-99\"))\n> test_scores\n# A tibble: 3 x 2\n  student score   \n  <chr>   <chr>   \n1 Amy     75-81-86\n2 Belle   87-89-90\n3 Candice 92-93-99\n> test_scores %>% separate(score, c(\"s1\", \"s2\", \"s3\")) %>%\n+         gather(key, score, -student) %>% select(-key)\n# A tibble: 9 x 2\n  student score\n  <chr>   <chr>\n1 Amy     75   \n2 Belle   87   \n3 Candice 92   \n4 Amy     81   \n5 Belle   89   \n6 Candice 93   \n7 Amy     86   \n8 Belle   90   \n9 Candice 99   \n> \n> separate_rows(test_scores, score)\n# A tibble: 9 x 2\n  student score\n  <chr>   <chr>\n1 Amy     75   \n2 Amy     81   \n3 Amy     86   \n4 Belle   87   \n5 Belle   89   \n6 Belle   90   \n7 Candice 92   \n8 Candice 93   \n9 Candice 99 \n```\n\n### preview ggplot2 without saving to a file\n\nfrom https://twitter.com/tjmahr/status/1083094031826124800?s=12\n\n```r\nlibrary(ggplot2)\nggpreview <- function (..., device = \"png\") {\n    fname <- tempfile(fileext = paste0(\".\", device))\n    ggplot2::ggsave(filename = fname, device = device, ...)\n    system2(\"open\", fname)\n    invisible(NULL)\n}\n\ng<- ggplot(mtcars, aes(x = hp, y = mpg)) + geom_point()\n\nggpreview(g, width = 5, height = 6, device = \"pdf\")\n\n```\n\n### group_split() and group_map(), group_walk()\n\ndplyr >= 0.8.0 see this post https://www.johnmackintosh.com/2019-02-28-first-look-at-mapping-and-splitting-in-dplyr/\nand this tweethttps://twitter.com/coolbutuseless/status/1101447111978205184?s=12\n\n(a) group_split() + walk()\n(b) group_by() + group_walk()\n\n```r\nlibrary(tidyverse)\n> mtcars %>% group_split(cyl) %>% walk(~print(head(.x,2)))\n# A tibble: 2 x 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  22.8     4  108     93  3.85  2.32  18.6     1     1     4     1\n2  24.4     4  147.    62  3.69  3.19  20       1     0     4     2\n# A tibble: 2 x 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1    21     6   160   110   3.9  2.62  16.5     0     1     4     4\n2    21     6   160   110   3.9  2.88  17.0     0     1     4     4\n# A tibble: 2 x 11\n    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  18.7     8   360   175  3.15  3.44  17.0     0     0     3     2\n2  14.3     8   360   245  3.21  3.57  15.8     0     0     3     4\n\n## the cyl variable is not in the dataframe\n\n> mtcars %>% group_by(cyl) %>% group_walk(~print(head(.x,2)))\n# A tibble: 2 x 10\n    mpg  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  22.8  108     93  3.85  2.32  18.6     1     1     4     1\n2  24.4  147.    62  3.69  3.19  20       1     0     4     2\n# A tibble: 2 x 10\n    mpg  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1    21   160   110   3.9  2.62  16.5     0     1     4     4\n2    21   160   110   3.9  2.88  17.0     0     1     4     4\n# A tibble: 2 x 10\n    mpg  disp    hp  drat    wt  qsec    vs    am  gear  carb\n  <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl> <dbl>\n1  18.7   360   175  3.15  3.44  17.0     0     0     3     2\n2  14.3   360   245  3.21  3.57  15.8     0     0     3     4\n```\n\n### hacking R lib path\n\nhttps://milesmcbain.xyz/hacking-r-library-paths/\n\n```r\nset_lib_paths <- function(lib_vec) {\n\n  lib_vec <- normalizePath(lib_vec, mustWork = TRUE)\n\n  shim_fun <- .libPaths\n  shim_env <- new.env(parent = environment(shim_fun))\n  shim_env$.Library <- character()\n  shim_env$.Library.site <- character()\n\n  environment(shim_fun) <- shim_env\n  shim_fun(lib_vec)\n\n}\n\n> .libPaths()\n[1] \"/home/miles/R/x86_64-pc-linux-gnu-library/3.6\"\n[2] \"/usr/local/lib/R/site-library\"                \n[3] \"/usr/lib/R/site-library\"                      \n[4] \"/usr/lib/R/library\"    \n\n> set_lib_paths(\"~/code/library\")\n> .libPaths()\n[1] \"/home/miles/code/library\"\n```\n### Sample from groups, n varies by group\n\nhttps://jennybc.github.io/purrr-tutorial/ls12_different-sized-samples.html\n\n```r\niris %>%\n  group_by(Species) %>% \n  nest() %>%            \n  mutate(n = c(2, 5, 3)) %>% \n  mutate(samp = map2(data, n, sample_n)) %>% \n  select(Species, samp) %>%\n  unnest()\n#> # A tibble: 10 x 5\n#>    Species    Sepal.Length Sepal.Width Petal.Length Petal.Width\n#>    <fct>             <dbl>       <dbl>        <dbl>       <dbl>\n#>  1 setosa              5.4         3.4          1.7         0.2\n#>  2 setosa              5.5         3.5          1.3         0.2\n#>  3 versicolor          6.6         2.9          4.6         1.3\n#>  4 versicolor          6.9         3.1          4.9         1.5\n#>  5 versicolor          5.8         2.7          3.9         1.2\n#>  6 versicolor          6           2.7          5.1         1.6\n#>  7 versicolor          6.2         2.9          4.3         1.3\n#>  8 virginica           6.4         3.2          5.3         2.3\n#>  9 virginica           6.5         3            5.5         1.8\n#> 10 virginica           6.1         3            4.9         1.8\n```\n\nalso check `dplyr::sample_n()` and `dplyr::sample_frac()`\n\n### ggplot2 reorder factor within facet\n\nhttps://juliasilge.com/blog/reorder-within/\n\n```r\nlibrary(tidyverse)\nlibrary(babynames)\n\ntop_names <- babynames %>%\n    filter(year >= 1950,\n           year < 1990) %>%\n    mutate(decade = (year %/% 10) * 10) %>%\n    group_by(decade) %>%\n    count(name, wt = n, sort = TRUE) %>%\n    ungroup\n\ntop_names\n\ntop_names %>%\n    group_by(decade) %>%\n    top_n(15) %>%\n    ungroup %>%\n    mutate(decade = as.factor(decade),\n           name = reorder_within(name, n, decade)) %>%\n    ggplot(aes(name, n, fill = decade)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~decade, scales = \"free_y\") +\n    coord_flip() +\n    scale_x_reordered() +\n    scale_y_continuous(expand = c(0,0)) +\n    labs(y = \"Number of babies per decade\",\n         x = NULL,\n         title = \"What were the most common baby names in each decade?\",\n         subtitle = \"Via US Social Security Administration\")\n```\n\n### add statistics sig to ggplot2\n\nhttps://indrajeetpatil.github.io/pairwiseComparisons/\nand https://cran.r-project.org/web/packages/ggsignif/vignettes/intro.html\n\n```r\nlibrary(pairwiseComparisons)\nlibrary(ggsignif)\nlibrary(ggplot2)\nmtcars$cyl<- as.factor(mtcars$cyl)\ndf<- pairwise_comparisons(mtcars, cyl, wt, type = \"parametric\") %>%\n        dplyr::mutate(.data = ., groups = purrr::pmap(.l = list(group1, group2), .f = c)) %>%\n        dplyr::arrange(.data = . , group1)\n\np<- ggplot(mtcars, aes(cyl, wt)) +geom_boxplot()\n\np + ggsignif::geom_signif(\n        comparisons = df$groups,\n        map_signif_level = TRUE,\n        y_position = c(5.5,5.75,6),\n        annotations = df$label,\n        test = NULL,\n        na.rm = TRUE,\n        parse = TRUE\n)\n```\n![](https://user-images.githubusercontent.com/4106146/65054432-8b9b0100-d93b-11e9-9ef0-4ccff12a1c22.png)\n\n### flip the ggplot2 color\n\nThanks [Shila Ghazanfar](https://twitter.com/shazanfar) for the tip!\n\n```{r}\nlibrary(ggplot2)\nlibrary(patchwork)\n\ndf = data.frame(x = c(\"yes\", \"no\", \"maybe\"))\n\ng1 = ggplot(df, aes(x = x, fill = x)) + geom_bar()\ng2 = g1 + scale_fill_discrete(limits = rev(levels(df$x))) \n\ng1 + g2\n\n```\n\n### Emulate ggplot2 default color palette\n\nhttps://stackoverflow.com/questions/8197559/emulate-ggplot2-default-color-palette\n\n```{r}\ngg_color_hue <- function(n) {\n  hues = seq(15, 375, length = n + 1)\n  hcl(h = hues, l = 65, c = 100)[1:n]\n}\n\n## 4 colors \nn = 4\ncols = gg_color_hue(n)\n```\n### split delimited strings in a column and insert as new rows\n\nhttps://stackoverflow.com/questions/15347282/split-delimited-strings-in-a-column-and-insert-as-new-rows\n\n\n```{r}\n\n> library(tidyr)\n> library(dplyr)\n> mydf\n\n  V1    V2\n2  1 a,b,c\n3  2   a,c\n4  3   b,d\n5  4   e,f\n6  .     .\n\n\n> mydf %>% \n    mutate(V2 = strsplit(as.character(V2), \",\")) %>% \n    unnest(V2)\n\n   V1 V2\n1   1  a\n2   1  b\n3   1  c\n4   2  a\n5   2  c\n6   3  b\n7   3  d\n8   4  e\n9   4  f\n```\n\nor use `seperate_rows`:\n\n```{r}\n> head(mydf)\ngeneid              chrom    start  end strand  length  gene_count\nENSG00000223972.5   chr1;chr1;chr1;chr1;chr1;chr1;chr1;chr1;chr1    11869;12010;12179;12613;12613;12975;13221;13221;13453   12227;12057;12227;12721;12697;13052;13374;14409;13670   +;+;+;+;+;+;+;+;+   1735    11\nENSG00000227232.5   chr1;chr1;chr1;chr1;chr1;chr1;chr1;chr1;chr1;chr1;chr1  14404;15005;15796;16607;16858;17233;17606;17915;18268;24738;29534   14501;15038;15947;16765;17055;17368;17742;18061;18366;24891;29570   -;-;-;-;-;-;-;-;-;-;-   1351    380\nENSG00000278267.1   chr1    17369   17436   -   68  14\nENSG00000243485.4   chr1;chr1;chr1;chr1;chr1    29554;30267;30564;30976;30976   30039;30667;30667;31097;31109   +;+;+;+;+   1021    22\nENSG00000237613.2   chr1;chr1;chr1  34554;35277;35721   35174;35481;36081   -;-;-   1187    24\nENSG00000268020.3   chr1    52473   53312   +   840 14\n\n\n> mydf %>% separate_rows(strand, chrom, gene_start, gene_end)\ngeneid  length  gene_count  strand  chrom   start   end\nENSG00000223972.5   1735    11  +   chr1    11869   12227\nENSG00000223972.5   1735    11  +   chr1    12010   12057\nENSG00000223972.5   1735    11  +   chr1    12179   12227\nENSG00000223972.5   1735    11  +   chr1    12613   12721\nENSG00000223972.5   1735    11  +   chr1    12613   12697\nENSG00000223972.5   1735    11  +   chr1    12975   13052\nENSG00000223972.5   1735    11  +   chr1    13221   13374\nENSG00000223972.5   1735    11  +   chr1    13221   14409\nENSG00000223972.5   1735    11  +   chr1    13453   13670\nENSG00000227232.5   1351    380 -   chr1    14404   14501\nENSG00000227232.5   1351    380 -   chr1    15005   15038\nENSG00000227232.5   1351    380 -   chr1    15796   15947\nENSG00000227232.5   1351    380 -   chr1    16607   16765\nENSG00000227232.5   1351    380 -   chr1    16858   17055\nENSG00000227232.5   1351    380 -   chr1    17233   17368\nENSG00000227232.5   1351    380 -   chr1    17606   17742\nENSG00000227232.5   1351    380 -   chr1    17915   18061\nENSG00000227232.5   1351    380 -   chr1    18268   18366\nENSG00000227232.5   1351    380 -   chr1    24738   24891\nENSG00000227232.5   1351    380 -   chr1    29534   29570\nENSG00000278267.1   68  5   -   chr1    17369   17436\nENSG00000243485.4   1021    8   +   chr1    29554   30039\nENSG00000243485.4   1021    8   +   chr1    30267   30667\nENSG00000243485.4   1021    8   +   chr1    30564   30667\nENSG00000243485.4   1021    8   +   chr1    30976   31097\nENSG00000243485.4   1021    8   +   chr1    30976   31109\nENSG00000237613.2   1187    24  -   chr1    34554   35174\nENSG00000237613.2   1187    24  -   chr1    35277   35481\nENSG00000237613.2   1187    24  -   chr1    35721   36081\nENSG00000268020.3   840 0   +   chr1    52473   53312\n```\n\nIf you concern about the speed see `data.table` solutions.\nhttps://stackoverflow.com/questions/13773770/split-comma-separated-strings-in-a-column-into-separate-rows\n\n```{r}\nlibrary(data.table)\n# method 1 (preferred)\nsetDT(v)[, lapply(.SD, function(x) unlist(tstrsplit(x, \",\", fixed=TRUE))), by = AB\n         ][!is.na(director)]\n# method 2\nsetDT(v)[, strsplit(as.character(director), \",\", fixed=TRUE), by = .(AB, director)\n         ][,.(director = V1, AB)]\n```\n\nor even base R\n\n```{r}\n# if 'director' is a character-column:\nstack(setNames(strsplit(df$director,','), df$AB))\n\n# if 'director' is a factor-column:\nstack(setNames(strsplit(as.character(df$director),','), df$AB))\n```\n\n### rowwise tidyverse\n\n```{r}\nlibrary(tidyverse) #dplyr version >=0.8.99.9000\nworld_total_pop<- world_bank_pop %>%\n        filter(indicator == \"SP.POP.TOTL\")\n\nhead(world_total_pop)\ncountry indicator `2000` `2001` `2002` `2003` `2004` `2005` `2006` `2007`\n  <chr>   <chr>      <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>  <dbl>\n1 ABW     SP.POP.T… 9.09e4 9.29e4 9.50e4 9.70e4 9.87e4 1.00e5 1.01e5 1.01e5\n2 AFG     SP.POP.T… 2.01e7 2.10e7 2.20e7 2.31e7 2.41e7 2.51e7 2.59e7 2.66e7\n3 AGO     SP.POP.T… 1.64e7 1.70e7 1.76e7 1.82e7 1.89e7 1.96e7 2.03e7 2.10e7\n....\n\n\n## calculate the mean of 2000 to 2007\ntidyr_way<- world_total_pop %>%\n        pivot_longer(starts_with(\"20\")) %>%\n        group_by(country) %>%\n        mutate(mean = mean(value, na.rm = TRUE)) %>%\n        pivot_wider(names_from = name)\n\npurrr_across_way<- world_total_pop %>%\n        mutate(mean = pmap_dbl(across(starts_with(\"20\")), \n                                      ~mean(c(...), na.rm = TRUE)))\n\n# or  https://github.com/jennybc/row-oriented-workflows/blob/master/ex09_row-summaries.md\npurrr_across_way<- world_total_pop %>%\n        mutate(mean = pmap_dbl(select(., starts_with(\"20\")), \n                                      ~mean(c(...), na.rm = TRUE)))\nrowwise_flat_way<- world_total_pop %>%\n        rowwise() %>%\n        mutate(mean = mean(flatten_dbl(across(starts_with(\"20\"))), na.rm =TRUE))\n\ntidybase_way<- world_total_pop %>% \n        mutate(mean=rowMeans(across(starts_with(\"20\")), na.rm = TRUE))\n```\n\ncheck https://github.com/jennybc/row-oriented-workflows as well. \nhttps://github.com/jennybc/row-oriented-workflows/blob/master/ex09_row-summaries.md\n\n### learning rowwise() tidyverse\nhttps://tladeras.shinyapps.io/learning_rowwise/\n\n### ggplot layout\nManual facets: that base-R `layout()`goodness coming to ggplot2: https://teunbrand.github.io/ggh4x/articles/Facets.html#manual-facets-1\n\n### add margins to the table\n\n```\ntable(mtcars$cyl, mtcars$am) %>% addmargins()\n```\n\n### more control of facet in ggplot\n\nggh4x https://github.com/teunbrand/ggh4x\n"
        },
        {
          "name": "Rcode_style.pdf",
          "type": "blob",
          "size": 227.4736328125,
          "content": null
        },
        {
          "name": "The importance of metadata in genomics and the FAIR principles ebook.pdf",
          "type": "blob",
          "size": 321.103515625,
          "content": null
        },
        {
          "name": "awk_and_sed",
          "type": "tree",
          "content": null
        },
        {
          "name": "bash_associate_array.md",
          "type": "blob",
          "size": 2.78125,
          "content": "### The problem\nI am running [pyclone](http://compbio.bccrc.ca/software/pyclone/) recently to determine the clonality of our collected tumor samples.\nIt needs tumor purity (estimated from sequenza) for input. I have 24 samples, and I want to generate pyclone commands for all of them.\n\n### The solution\nI usually generate command by bash and then use another [bash wrapper](https://gist.github.com/crazyhottommy/60b42c15b74b261afe13871f058f43d2) \nto generate pbs files on HPC ((Thanks to @SBAmin).\nnow for each patient, I have two samples. How should I generate the commands? I am still better in R and Unix than python,\nso I used the associated array in bash.\n\nFirst, generate a file containing the tumor purity for each tumor:  \n\n```bash\nhead -6 all_tumor_purity_no_header.txt\n0.69\tPa25T1\n0.26\tPa25T2\n0.49\tPa26T1\n0.37\tPa26T2\n0.9\tPa27T1\n0.92\tPa27T2\n```\nThis bash script uses associate array to contain tumor purity.\nread more at http://www.artificialworlds.net/blog/2012/10/17/bash-associative-array-examples/\n\n`make_commands.sh`:\n\n```bash\n#! /bin/bash\nset -euo pipefail\n\n## build the array to contain the tumor purity, like a python dict\n## have to declare by -A\ndeclare -A cols\n\nwhile read purity sample\ndo\n    cols[$sample]=$purity\ndone < all_purity_no_header.txt \n\necho ${cols[@]}\n\n## generate commands\nfor i in Pa{25..37}\ndo\n   echo PyClone run_analysis_pipeline --in_file ${i}T1_pyclone.tsv ${i}T2_pyclone.tsv --tumour_contents ${cols[${i}T1]} ${cols[${i}T2]} --samples ${i}T1 ${i}T2 --density pyclone_binomial --working_dir ${i}T_pyclone_analysis --min_cluster_size 2 --seed 123 --num_iters 50000 > ${i}_pyclone_commands.txt\ndone\n```\n```bash\nchmod u+x make_commands.sh\n./make_commands.sh\n```\n\nwhat you get:\n`cat *commands.txt`\n```\nPyClone run_analysis_pipeline --in_file Pa25T1_pyclone.tsv Pa25T2_pyclone.tsv --tumour_contents 0.69 0.26 --samples Pa25T1 Pa25T2 --density pyclone_binomial --working_dir Pa25T_pyclone_analysis --min_cluster_size 2 --seed 123 --num_iters 50000\nPyClone run_analysis_pipeline --in_file Pa26T1_pyclone.tsv Pa26T2_pyclone.tsv --tumour_contents 0.49 0.37 --samples Pa26T1 Pa26T2 --density pyclone_binomial --working_dir Pa26T_pyclone_analysis --min_cluster_size 2 --seed 123 --num_iters 50000\nPyClone run_analysis_pipeline --in_file Pa27T1_pyclone.tsv Pa27T2_pyclone.tsv --tumour_contents 0.9 0.92 --samples Pa27T1 Pa27T2 --density pyclone_binomial --working_dir Pa27T_pyclone_analysis --min_cluster_size 2 --seed 123 --num_iters 50000\n....\n```\n\nthen:\n\n```bash\n\nfind *commands.txt | parallel 'makemsub -a {} -j {/.} -o a -c 4 -t \"48:00:00\" -m 16g > {/.}.pbs'\n\nfor pbs in *pbs\ndo\n  msub $pbs\n  sleep 1\ndone\n```\nI usually do it for simple tasks. e.g. only one or two commands are evoked.\nFor more complex workflows, a workflow tool such as [snakemake](https://bitbucket.org/snakemake/snakemake/wiki/Home) is better.\n"
        },
        {
          "name": "bring_R_to_command_line.md",
          "type": "blob",
          "size": 3.859375,
          "content": "If you ever wants to use R on the command line, now you have several opitions.\n\n### [litter](https://github.com/eddelbuettel/littler)\n\nI wanted to use `littler` for a long time on my mac, but was intimidated by the complex installation process. Since version `0.3.0`, it is on CRAN,and can be installed within R console or Rstudio by:\n\n```r\n> install.packages(\"littler\")\nInstalling package into ‘/Users/mtang1/Library/R/3.3/library’\n(as ‘lib’ is unspecified)\ntrying URL 'https://cran.rstudio.com/bin/macosx/mavericks/contrib/3.3/littler_0.3.1.tgz'\nContent type 'application/x-gzip' length 50351 bytes (49 KB)\n==================================================\ndownloaded 49 KB\n\n\nThe downloaded binary packages are in\n\t/var/folders/79/x06wz9v560q10gw881n9c_z0x7m0vl/T//RtmphlFEcc/downloaded_packages\n> library(littler)\nThe littler package provides 'r' as a binary.\nSee 'vignette(\"littler-examples\") for some illustrations.\nOn OS X, 'r' and 'R' are the same so 'lr' is an alternate name for littler.\nYou could link to the 'r' binary installed in\n'/Users/mtang1/Library/R/3.3/library/littler/bin/r'\nas '/usr/local/bin/lr' in order to use 'lr' for scripting.\n```\nyou just need to make a symbolic link by:\n\n```bash\nln -s /Users/mtang1/Library/R/3.3/library/littler/bin/r  /usr/local/bin/lr\n```\nNow you can start to use R as `lr` at the command line:\n\n```bash\nlr -h\n\nUsage: r [options] [-|file]\n\nLaunch GNU R to execute the R commands supplied in the specified file, or\nfrom stdin if '-' is used. Suitable for so-called shebang '#!/'-line scripts.\n\nOptions:\n  -h, --help           Give this help list\n      --usage          Give a short usage message\n  -V, --version        Show the version number\n  -v, --vanilla        Pass the '--vanilla' option to R\n  -t, --rtemp          Use per-session temporary directory as R does\n  -i, --interactive    Let interactive() return 'true' rather than 'false'\n  -q, --quick          Skip autoload / delayed assign of default libraries\n  -p, --verbose        Print the value of expressions to the console\n  -l, --packages list  Load the R packages from the comma-separated 'list'\n  -d, --datastdin      Prepend command to load 'X' as csv from stdin\n  -L, --libpath dir    Add directory to library path via '.libPaths(dir)'\n  -e, --eval expr      Let R evaluate 'expr'\n```\nGo to `littler`'s page for [more usage examples](http://dirk.eddelbuettel.com/code/littler.html).\n\nFrom [this issue](https://github.com/eddelbuettel/littler/issues/11) on github, I found one can install littler on mac by `brew` (this is really should be the way to manage software installation on mac)\n\n```bash\nbrew tap homebrew/science\nbrew install littler\n```\nIt will be installed as `littler` to avoid confusions.   \n**NOTE** I tried `brew install` as well, but it seems to be isolated with my installed R packages as it complains `dplyr` is not installed when I do `littler -l dplyr,tidyr`.\n\n### [`rscl`](https://github.com/jeroenjanssens/rscl)  and [`Rio`](https://github.com/jeroenjanssens/data-science-at-the-command-line/blob/master/tools/Rio). \n\nBoth are from the author of `Data science at the command line`: Jeroen Janssens.\n\n```r\nif (packageVersion(\"devtools\") < 1.6) {\n  install.packages(\"devtools\")\n}\ndevtools::install_github(\"jeroenjanssens/rscl\")\n\nrscl::setup()\n# If you want to invoke `rscl` from anywhere on your filesystem, add the line\n# below to ~/.bashrc, ~/.zshrc, or whathever file your shell sources at startup.\nexport PATH=\"$PATH:/Users/mtang1/Library/R/3.3/library/rscl/bin\"\n> \n```\non command line:  \n\n```bash\nln -s /Users/mtang1/Library/R/3.3/library/rscl/bin/\n```\nwhen I execute `rscl`, I got an segment fault error. I opened [an issue](https://github.com/jeroenjanssens/rscl/issues/4) on github.\n\nFor usage of `Rio`, see my previous blog post: [csvkit to manipulate csv at command line, Rio to interact with R at command line](http://crazyhottommy.blogspot.com/2014/11/csvkit-to-manipulate-csv-at-command.html)\n"
        },
        {
          "name": "file_naming_and_coding_GARP-example_20151211.pdf",
          "type": "blob",
          "size": 638.0048828125,
          "content": null
        },
        {
          "name": "gaussian_process_ML.pdf",
          "type": "blob",
          "size": 3957.31640625,
          "content": null
        },
        {
          "name": "genohub_seq_recommendations.xlsx",
          "type": "blob",
          "size": 13.2099609375,
          "content": null
        },
        {
          "name": "guide-to-reproducible-code.pdf",
          "type": "blob",
          "size": 2287.4755859375,
          "content": null
        },
        {
          "name": "how_to_write_documentation.pdf",
          "type": "blob",
          "size": 576.55078125,
          "content": null
        },
        {
          "name": "idioms_of_R_programming.pdf",
          "type": "blob",
          "size": 325.857421875,
          "content": null
        },
        {
          "name": "learning_statistics_with_r.pdf",
          "type": "blob",
          "size": 7441.9951171875,
          "content": ""
        },
        {
          "name": "moving_cli.png",
          "type": "blob",
          "size": 49.4599609375,
          "content": null
        },
        {
          "name": "parallel01.pdf",
          "type": "blob",
          "size": 607.2607421875,
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "tmux_scroll_mode.md",
          "type": "blob",
          "size": 2.5908203125,
          "content": "\n### tmux config file\nyou can copy https://github.com/crazyhottommy/getting-started-with-genomics-tools-and-resources/blob/master/scripts/.tmux.conf\nto your home directory.\n\nit changed the key binding from `control + b` to `control + a` if you are familiar with the `screen` shortcuts.\n\n`control + a + c` will create a new window.\n`control + a + Space` will move to previous window.\n`control + a + n` will move to next window.\n\n```\ncontrol + a + ?\n```\nwill show you all the shortcuts.\n\n\n### scroll mode\n\nOne problem with `screen` or `tmux` is that you have to press `control + a + [` to enter the copy mode, and and `control + a + ]` to paste it.\nI want to just use the mouse to scroll up and down and copy/paste.\n\nread this long thread github issue: https://github.com/tmux/tmux/issues/145\nThe solution that worked for me:\n\n* download the [Tmux Plugin Manager](https://github.com/tmux-plugins/tpm)\n\n```bash\ngit clone https://github.com/tmux-plugins/tpm ~/.tmux/plugins/tpm\n```\nPut this at the bottom of .tmux.conf:\n\n```\n# List of plugins\nset -g @plugin 'tmux-plugins/tpm'\nset -g @plugin 'tmux-plugins/tmux-sensible'\n\n# Other examples:\n# set -g @plugin 'github_username/plugin_name'\n# set -g @plugin 'git@github.com/user/plugin'\n# set -g @plugin 'git@bitbucket.com/user/plugin'\n\n# Initialize TMUX plugin manager (keep this line at the very bottom of tmux.conf)\nrun '~/.tmux/plugins/tpm/tpm'\n\n```\n* Now install [tmux-better-mouse-mode](https://github.com/NHDaly/tmux-better-mouse-mode) plugin.\n\nopen your `.tmux.conf`.\n\nTo enable mouse-mode in tmux 2.1+, put the following line in your ~/.tmux.conf:\n\n```\nset-option -g mouse on\n\n```\n\nthen add the following line to your .tmux.conf file:\n\n```\nset -g @plugin 'nhdaly/tmux-better-mouse-mode'\n```\n\n*  install it\n\n```\n# start a new session\ntmux\n\n# source the config\ntmux source ~/.tmux.conf\n\n# install plugin\n`control + a + I (captial)` to install all the plugins.\n\n```\nNow if you scroll up with your mouse, you will enter into copy mode automatically, and when you scroll down to the end of the current screen,\nyou will exit the copy mode automatically. \n\n*  copy and paste  \n\nIf you scroll up and select the text you want to copy by left-click and drag, you will exit\nthe copy mode instantly, and the content you selected will be copied in the buffer. You just need to `control + a + ]` to paste it.\nvery cool!\n\n### add tmux-yank plugin\n\nadd [tmux-yank](https://github.com/tmux-plugins/tmux-yank) plugin to enable copy text in tmux session on the clip-borad of the system.\n\nadd `set -g @plugin 'tmux-plugins/tmux-yank'` to .tmux.conf\n# install plugin\n`control + a + I (captial)` to install all the plugins.\n"
        },
        {
          "name": "understanding_statistics_and_experiment_design.pdf",
          "type": "blob",
          "size": 3444.83984375,
          "content": null
        },
        {
          "name": "wget_specific_files.md",
          "type": "blob",
          "size": 5.466796875,
          "content": "I want to download some files from a ftp site, and I only want to download some files with names matching a pattern.\nHow can I do it?  \nUse `wget` !  It is a very versatile command and I just got to know several tricks.\n\n**When there are many levels of folder, you want to search down to all the folders:**   \n >-r\n       --recursive\n           Turn on recursive retrieving.\n\n       -l depth\n       --level=depth\n           Specify recursion maximum depth level depth.  The default maximum depth is 5.\n\n**You can specify what files you want to download or reject using wild cards:**   \n > Recursive Accept/Reject Options  \n       -A acclist --accept acclist    \n       -R rejlist --reject rejlist    \n           Specify comma-separated lists of file name suffixes or patterns to accept or\n           reject. Note that if any of the wildcard characters, *, ?, [ or ], appear in\n           an element of acclist or rejlist, it will be treated as a pattern, rather\n           than a suffix.\n           \n**If you want to save the file to a different name:**  \n> -O file\n       --output-document=file  \n           The documents will not be written to the appropriate files, but all will be\n           concatenated together and written to file.  If - is used as file, documents\n           will be printed to standard output, disabling link conversion.  (Use ./- to\n           print to a file literally named -.)\n\n           Use of -O is not intended to mean simply \"use the name file instead of the\n           one in the URL;\" rather, it is analogous to shell redirection: wget -O file\n           http://foo is intended to work like wget -O - http://foo > file; file will be\n           truncated immediately, and all downloaded content will be written there.\n\n           For this reason, -N (for timestamp-checking) is not supported in combination\n           with -O: since file is always newly created, it will always have a very new\n           timestamp. A warning will be issued if this combination is used.\n\n           Similarly, using -r or -p with -O may not work as you expect: Wget won’t just\n           download the first file to file and then download the rest to their normal\n           names: all downloaded content will be placed in file. This was disabled in\n           version 1.11, but has been reinstated (with a warning) in 1.11.2, as there\n           are some cases where this behavior can actually have some use.\n\n           Note that a combination with -k is only permitted when downloading a single\n           document, as in that case it will just convert all relative URIs to external\n           ones; -k makes no sense for multiple URIs when they’re all being downloaded\n           to a single file.\n\n**If you do not need the folder structure:**\n\n> -nd\n       --no-directories  \n           Do not create a hierarchy of directories when retrieving\n           recursively.  With this option turned on, all files will get saved\n           to the current directory, without clobbering (if a name shows up\n           more than once, the filenames will get extensions .n).\n           \n\n\n**one alternative way is to specify `-nH` and `--cut-dirs=10` together**   \n>-nH\n       --no-host-directories  \n           Disable generation of host-prefixed directories.  By default,\n           invoking Wget with -r http://fly.srk.fer.hr/ will create a\n           structure of directories beginning with fly.srk.fer.hr/.  This\n           option disables such behavior.\n           \n           \n>--cut-dirs=number  \n           Ignore number directory components.  This is useful for getting a\n           fine-grained control over the directory where recursive retrieval\n           will be saved.  \n\n           Take, for example, the directory at\n           ftp://ftp.xemacs.org/pub/xemacs/.  If you retrieve it with -r, it\n           will be saved locally under ftp.xemacs.org/pub/xemacs/.  While the\n           -nH option can remove the ftp.xemacs.org/ part, you are still stuck\n           with pub/xemacs.  This is where --cut-dirs comes in handy; it makes\n           Wget not \"see\" number remote directory components.  Here are\n           several examples of how --cut-dirs option works.\n\n                   No options        -> ftp.xemacs.org/pub/xemacs/\n                   -nH               -> pub/xemacs/\n                   -nH --cut-dirs=1  -> xemacs/\n                   -nH --cut-dirs=2  -> .\n\n                   --cut-dirs=1      -> ftp.xemacs.org/xemacs/\n                   ...\n\n           If you just want to get rid of the directory structure, this option\n           is similar to a combination of -nd and -P.  However, unlike -nd,\n           --cut-dirs does not lose with subdirectories---for instance, with\n           -nH --cut-dirs=1, a beta/ subdirectory will be placed to\n           xemacs/beta, as one would expect.\n\n**If you want to save files to a different folder name:**            \n> -P prefix\n       --directory-prefix=prefix  \n           Set directory prefix to prefix.  The directory prefix is the\n           directory where all other files and subdirectories will be saved\n           to, i.e. the top of the retrieval tree.  The default is . (the\n           current directory).\n\n**Continue to download a file**:           \n> -c\n       --continue  \n           Continue getting a partially-downloaded file.  This is useful when\n           you want to finish up a download started by a previous instance of\n           Wget, or by another program\n\n\nThere are so many different options, just `man wget` to see all of them! I am impressed on how versatile this command is!\n"
        }
      ]
    }
  ]
}