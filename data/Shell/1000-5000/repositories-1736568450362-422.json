{
  "metadata": {
    "timestamp": 1736568450362,
    "page": 422,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQyOQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "stefanprodan/swarmprom",
      "stars": 1866,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0927734375,
          "content": "# Denote all files that are truly binary and should not be modified.\n*.png binary\n*.jpg binary\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2861328125,
          "content": "# Binaries for programs and plugins\n*.exe\n*.dll\n*.so\n*.dylib\n\n# Test binary, build with `go test -c`\n*.test\n\n# Output of the go coverage tool, specifically when used with LiteIDE\n*.out\n\n# Project-local glide cache, RE: https://github.com/Masterminds/glide/issues/736\n.glide/\n\n.idea/\n.DS_Store\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.5791015625,
          "content": "sudo: required\n\nservices:\n  - docker\n\nbefore_install:\n  - curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo apt-key add -\n  - sudo add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable\"\n  - sudo apt-get update\n  - sudo apt-get -y install docker-ce\n  - sudo service docker restart\n\nscript:\n  - cd prometheus && docker build -t stefanprodan/swarmprom-prometheus:$TRAVIS_BUILD_NUMBER .\n  - cd .. && cd node-exporter && docker build -t stefanprodan/swarmprom-node-exporter:$TRAVIS_BUILD_NUMBER .\n  - cd .. && cd alertmanager && docker build -t stefanprodan/swarmprom-alertmanager:$TRAVIS_BUILD_NUMBER .\n  - cd .. && cd grafana && docker build -t stefanprodan/swarmprom-grafana:$TRAVIS_BUILD_NUMBER .\n\nafter_success:\n  - if [ -z \"$DOCKER_USER\" ]; then\n      echo \"PR build, skipping Docker Hub push\";\n    else\n    docker login -u \"$DOCKER_USER\" -p \"$DOCKER_PASS\";\n    docker tag stefanprodan/swarmprom-prometheus:$TRAVIS_BUILD_NUMBER stefanprodan/swarmprom-prometheus:v2.5.0;\n    docker push stefanprodan/swarmprom-prometheus:v2.5.0;\n    docker tag stefanprodan/swarmprom-node-exporter:$TRAVIS_BUILD_NUMBER stefanprodan/swarmprom-node-exporter:v0.16.0;\n    docker push stefanprodan/swarmprom-node-exporter:v0.16.0;\n    docker tag stefanprodan/swarmprom-alertmanager:$TRAVIS_BUILD_NUMBER stefanprodan/swarmprom-alertmanager:v0.15.3;\n    docker push stefanprodan/swarmprom-alertmanager:v0.15.3;\n    docker tag stefanprodan/swarmprom-grafana:$TRAVIS_BUILD_NUMBER stefanprodan/swarmprom-grafana:5.3.4;\n    docker push stefanprodan/swarmprom-grafana:5.3.4;\n    fi\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.044921875,
          "content": "MIT License\n\nCopyright (c) 2017 Stefan Prodan\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.4375,
          "content": "# swarmprom\n\nSwarmprom is a starter kit for Docker Swarm monitoring with [Prometheus](https://prometheus.io/),\n[Grafana](http://grafana.org/),\n[cAdvisor](https://github.com/google/cadvisor),\n[Node Exporter](https://github.com/prometheus/node_exporter),\n[Alert Manager](https://github.com/prometheus/alertmanager)\nand [Unsee](https://github.com/cloudflare/unsee).\n\n## Install\n\nClone this repository and run the monitoring stack:\n\n```bash\n$ git clone https://github.com/stefanprodan/swarmprom.git\n$ cd swarmprom\n\nADMIN_USER=admin \\\nADMIN_PASSWORD=admin \\\nSLACK_URL=https://hooks.slack.com/services/TOKEN \\\nSLACK_CHANNEL=devops-alerts \\\nSLACK_USER=alertmanager \\\ndocker stack deploy -c docker-compose.yml mon\n```\n\nPrerequisites:\n\n* Docker CE 17.09.0-ce or Docker EE 17.06.2-ee-3\n* Swarm cluster with one manager and a worker node\n* Docker engine experimental enabled and metrics address set to `0.0.0.0:9323`\n\nServices:\n\n* prometheus (metrics database) `http://<swarm-ip>:9090`\n* grafana (visualize metrics) `http://<swarm-ip>:3000`\n* node-exporter (host metrics collector)\n* cadvisor (containers metrics collector)\n* dockerd-exporter (Docker daemon metrics collector, requires Docker experimental metrics-addr to be enabled)\n* alertmanager (alerts dispatcher) `http://<swarm-ip>:9093`\n* unsee (alert manager dashboard) `http://<swarm-ip>:9094`\n* caddy (reverse proxy and basic auth provider for prometheus, alertmanager and unsee)\n\n\n## Alternative install with Traefik and HTTPS\n\nIf you have a Docker Swarm cluster with a global Traefik set up as described in [DockerSwarm.rocks](https://dockerswarm.rocks), you can deploy Swarmprom integrated with that global Traefik proxy.\n\nThis way, each Swarmprom service will have its own domain, and each of them will be served using HTTPS, with certificates generated (and renewed) automatically.\n\n### Requisites\n\nThese instructions assume you already have Traefik set up following that guide above, in short:\n\n* With automatic HTTPS certificate generation.\n* A Docker Swarm network `traefik-public`.\n* Filtering to only serve containers with a label `traefik.constraint-label=traefik-public`.\n\n### Instructions\n\n* Clone this repository and enter into the directory:\n\n```bash\n$ git clone https://github.com/stefanprodan/swarmprom.git\n$ cd swarmprom\n```\n\n* Set and export an `ADMIN_USER` environment variable:\n\n```bash\nexport ADMIN_USER=admin\n```\n\n* Set and export an `ADMIN_PASSWORD` environment variable:\n\n\n```bash\nexport ADMIN_PASSWORD=changethis\n```\n\n* Set and export a hashed version of the `ADMIN_PASSWORD` using `openssl`, it will be used by Traefik's HTTP Basic Auth for most of the services:\n\n```bash\nexport HASHED_PASSWORD=$(openssl passwd -apr1 $ADMIN_PASSWORD)\n```\n\n* You can check the contents with:\n\n```bash\necho $HASHED_PASSWORD\n```\n\nit will look like:\n\n```\n$apr1$89eqM5Ro$CxaFELthUKV21DpI3UTQO.\n```\n\n* Create and export an environment variable `DOMAIN`, e.g.:\n\n```bash\nexport DOMAIN=example.com\n```\n\nand make sure that the following sub-domains point to your Docker Swarm cluster IPs:\n\n* `grafana.example.com`\n* `alertmanager.example.com`\n* `unsee.example.com`\n* `prometheus.example.com`\n\n(and replace `example.com` with your actual domain).\n\n**Note**: You can also use a subdomain, like `swarmprom.example.com`. Just make sure that the subdomains point to (at least one of) your cluster IPs. Or set up a wildcard subdomain (`*`).\n\n* If you are using Slack and want to integrate it, set the following environment variables:\n\n```bash\nexport SLACK_URL=https://hooks.slack.com/services/TOKEN\nexport SLACK_CHANNEL=devops-alerts\nexport SLACK_USER=alertmanager\n```\n\n**Note**: by using `export` when declaring all the environment variables above, the next command will be able to use them.\n\n* Deploy the Traefik version of the stack:\n\n\n```bash\ndocker stack deploy -c docker-compose.traefik.yml swarmprom\n```\n\nTo test it, go to each URL:\n\n* `https://grafana.example.com`\n* `https://alertmanager.example.com`\n* `https://unsee.example.com`\n* `https://prometheus.example.com`\n\n\n## Setup Grafana\n\nNavigate to `http://<swarm-ip>:3000` and login with user ***admin*** password ***admin***.\nYou can change the credentials in the compose file or\nby supplying the `ADMIN_USER` and `ADMIN_PASSWORD` environment variables at stack deploy.\n\nSwarmprom Grafana is preconfigured with two dashboards and Prometheus as the default data source:\n\n* Name: Prometheus\n* Type: Prometheus\n* Url: http://prometheus:9090\n* Access: proxy\n\nAfter you login, click on the home drop down, in the left upper corner and you'll see the dashboards there.\n\n***Docker Swarm Nodes Dashboard***\n\n![Nodes](https://raw.githubusercontent.com/stefanprodan/swarmprom/master/grafana/screens/swarmprom-nodes-dash-v3.png)\n\nURL: `http://<swarm-ip>:3000/dashboard/db/docker-swarm-nodes`\n\nThis dashboard shows key metrics for monitoring the resource usage of your Swarm nodes and can be filtered by node ID:\n\n* Cluster up-time, number of nodes, number of CPUs, CPU idle gauge\n* System load average graph, CPU usage graph by node\n* Total memory, available memory gouge, total disk space and available storage gouge\n* Memory usage graph by node (used and cached)\n* I/O usage graph (read and write Bps)\n* IOPS usage (read and write operation per second) and CPU IOWait\n* Running containers graph by Swarm service and node\n* Network usage graph (inbound Bps, outbound Bps)\n* Nodes list (instance, node ID, node name)\n\n***Docker Swarm Services Dashboard***\n\n![Nodes](https://raw.githubusercontent.com/stefanprodan/swarmprom/master/grafana/screens/swarmprom-services-dash-v3.png)\n\nURL: `http://<swarm-ip>:3000/dashboard/db/docker-swarm-services`\n\nThis dashboard shows key metrics for monitoring the resource usage of your Swarm stacks and services, can be filtered by node ID:\n\n* Number of nodes, stacks, services and running container\n* Swarm tasks graph by service name\n* Health check graph (total health checks and failed checks)\n* CPU usage graph by service and by container (top 10)\n* Memory usage graph by service and by container (top 10)\n* Network usage graph by service (received and transmitted)\n* Cluster network traffic and IOPS graphs\n* Docker engine container and network actions by node\n* Docker engine list (version, node id, OS, kernel, graph driver)\n\n***Prometheus Stats Dashboard***\n\n![Nodes](https://raw.githubusercontent.com/stefanprodan/swarmprom/master/grafana/screens/swarmprom-prometheus-dash-v3.png)\n\nURL: `http://<swarm-ip>:3000/dashboard/db/prometheus`\n\n* Uptime, local storage memory chunks and series\n* CPU usage graph\n* Memory usage graph\n* Chunks to persist and persistence urgency graphs\n* Chunks ops and checkpoint duration graphs\n* Target scrapes, rule evaluation duration, samples ingested rate and scrape duration graphs\n\n\n## Prometheus service discovery\n\nIn order to collect metrics from Swarm nodes you need to deploy the exporters on each server.\nUsing global services you don't have to manually deploy the exporters. When you scale up your\ncluster, Swarm will launch a cAdvisor, node-exporter and dockerd-exporter instance on the newly created nodes.\nAll you need is an automated way for Prometheus to reach these instances.\n\nRunning Prometheus on the same overlay network as the exporter services allows you to use the DNS service\ndiscovery. Using the exporters service name, you can configure DNS discovery:\n\n```yaml\nscrape_configs:\n  - job_name: 'node-exporter'\n    dns_sd_configs:\n    - names:\n      - 'tasks.node-exporter'\n      type: 'A'\n      port: 9100\n  - job_name: 'cadvisor'\n    dns_sd_configs:\n    - names:\n      - 'tasks.cadvisor'\n      type: 'A'\n      port: 8080\n  - job_name: 'dockerd-exporter'\n    dns_sd_configs:\n    - names:\n      - 'tasks.dockerd-exporter'\n      type: 'A'\n      port: 9323\n```\n\nWhen Prometheus runs the DNS lookup, Docker Swarm will return a list of IPs for each task.\nUsing these IPs, Prometheus will bypass the Swarm load-balancer and will be able to scrape each exporter\ninstance.\n\nThe problem with this approach is that you will not be able to tell which exporter runs on which node.\nYour Swarm nodes' real IPs are different from the exporters IPs since exporters IPs are dynamically\nassigned by Docker and are part of the overlay network.\nSwarm doesn't provide any records for the tasks DNS, besides the overlay IP.\nIf Swarm provides SRV records with the nodes hostname or IP, you can re-label the source\nand overwrite the overlay IP with the real IP.\n\nIn order to tell which host a node-exporter instance is running, I had to create a prom file inside\nthe node-exporter containing the hostname and the Docker Swarm node ID.\n\nWhen a node-exporter container starts `node-meta.prom` is generated with the following content:\n\n```bash\n\"node_meta{node_id=\\\"$NODE_ID\\\", node_name=\\\"$NODE_NAME\\\"} 1\"\n```\n\nThe node ID value is supplied via `{{.Node.ID}}` and the node name is extracted from the `/etc/hostname`\nfile that is mounted inside the node-exporter container.\n\n```yaml\n  node-exporter:\n    image: stefanprodan/swarmprom-node-exporter\n    environment:\n      - NODE_ID={{.Node.ID}}\n    volumes:\n      - /etc/hostname:/etc/nodename\n    command:\n      - '-collector.textfile.directory=/etc/node-exporter/'\n```\n\nUsing the textfile command, you can instruct node-exporter to collect the `node_meta` metric.\nNow that you have a metric containing the Docker Swarm node ID and name, you can use it in promql queries.\n\nLet's say you want to find the available memory on each node, normally you would write something like this:\n\n```\nsum(node_memory_MemAvailable) by (instance)\n\n{instance=\"10.0.0.5:9100\"} 889450496\n{instance=\"10.0.0.13:9100\"} 1404162048\n{instance=\"10.0.0.15:9100\"} 1406574592\n```\n\nThe above result is not very helpful since you can't tell what Swarm node is behind the instance IP.\nSo let's write that query taking into account the node_meta metric:\n\n```sql\nsum(node_memory_MemAvailable * on(instance) group_left(node_id, node_name) node_meta) by (node_id, node_name)\n\n{node_id=\"wrdvtftteo0uaekmdq4dxrn14\",node_name=\"swarm-manager-1\"} 889450496\n{node_id=\"moggm3uaq8tax9ptr1if89pi7\",node_name=\"swarm-worker-1\"} 1404162048\n{node_id=\"vkdfx99mm5u4xl2drqhnwtnsv\",node_name=\"swarm-worker-2\"} 1406574592\n```\n\nThis is much better. Instead of overlay IPs, now I can see the actual Docker Swarm nodes ID and hostname. Knowing the hostname of your nodes is useful for alerting as well.\n\nYou can define an alert when available memory reaches 10%. You also will receive the hostname in the alert message\nand not some overlay IP that you can't correlate to a infrastructure item.\n\nMaybe you are wondering why you need the node ID if you have the hostname. The node ID will help you match\nnode-exporter instances to cAdvisor instances. All metrics exported by cAdvisor have a label named `container_label_com_docker_swarm_node_id`,\nand this label can be used to filter containers metrics by Swarm nodes.\n\nLet's write a query to find out how many containers are running on a Swarm node.\nKnowing from the `node_meta` metric all the nodes IDs you can define a filter with them in Grafana.\nAssuming the filter is `$node_id` the container count query should look like this:\n\n```\ncount(rate(container_last_seen{container_label_com_docker_swarm_node_id=~\"$node_id\"}[5m]))\n```\n\nAnother use case for node ID is filtering the metrics provided by the Docker engine daemon.\nDocker engine doesn't have a label with the node ID attached on every metric, but there is a `swarm_node_info`\nmetric that has this label.  If you want to find out the number of failed health checks on a Swarm node\nyou would write a query like this:\n\n```\nsum(engine_daemon_health_checks_failed_total) * on(instance) group_left(node_id) swarm_node_info{node_id=~\"$node_id\"})\n```\n\nFor now the engine metrics are still experimental. If you want to use dockerd-exporter you have to enable\nthe experimental feature and set the metrics address to `0.0.0.0:9323`.\n\nIf you are running Docker with systemd create or edit\n/etc/systemd/system/docker.service.d/docker.conf file like so:\n\n```\n[Service]\nExecStart=\nExecStart=/usr/bin/dockerd \\\n  --storage-driver=overlay2 \\\n  --dns 8.8.4.4 --dns 8.8.8.8 \\\n  --experimental=true \\\n  --metrics-addr 0.0.0.0:9323\n```\n\nApply the config changes with `systemctl daemon-reload && systemctl restart docker` and\ncheck if the docker_gwbridge ip address is 172.18.0.1:\n\n```bash\nip -o addr show docker_gwbridge\n```\n\nReplace 172.18.0.1 with your docker_gwbridge address in the compose file:\n\n```yaml\n  dockerd-exporter:\n    image: stefanprodan/caddy\n    environment:\n      - DOCKER_GWBRIDGE_IP=172.18.0.1\n```\n\nCollecting Docker Swarm metrics with Prometheus is not a smooth process, and\nbecause of `group_left` queries tend to become more complex.\nIn the future I hope Swarm DNS will contain the SRV record for hostname and Docker engine\nmetrics will expose container metrics replacing cAdvisor all together.\n\n## Configure Prometheus\n\nI've set the Prometheus retention period to 24h, you can change these values in the\ncompose file or using the env variable `PROMETHEUS_RETENTION`.\n\n```yaml\n  prometheus:\n    image: stefanprodan/swarmprom-prometheus\n    command:\n      - '-storage.tsdb.retention=24h'\n    deploy:\n      resources:\n        limits:\n          memory: 2048M\n        reservations:\n          memory: 1024M\n```\n\nWhen using host volumes you should ensure that Prometheus doesn't get scheduled on different nodes. You can\npin the Prometheus service on a specific host with placement constraints.\n\n```yaml\n  prometheus:\n    image: stefanprodan/swarmprom-prometheus\n    volumes:\n      - prometheus:/prometheus\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.labels.monitoring.role == prometheus\n```\n\n## Configure alerting\n\nThe Prometheus swarmprom comes with the following alert rules:\n\n***Swarm Node CPU Usage***\n\nAlerts when a node CPU usage goes over 80% for five minutes.\n\n```\nALERT node_cpu_usage\n  IF 100 - (avg(irate(node_cpu{mode=\"idle\"}[1m])  * on(instance) group_left(node_name) node_meta * 100) by (node_name)) > 80\n  FOR 5m\n  LABELS      { severity=\"warning\" }\n  ANNOTATIONS {\n      summary = \"CPU alert for Swarm node '{{ $labels.node_name }}'\",\n      description = \"Swarm node {{ $labels.node_name }} CPU usage is at {{ humanize $value}}%.\",\n  }\n```\n***Swarm Node Memory Alert***\n\nAlerts when a node memory usage goes over 80% for five minutes.\n\n```\nALERT node_memory_usage\n  IF sum(((node_memory_MemTotal - node_memory_MemAvailable) / node_memory_MemTotal) * on(instance) group_left(node_name) node_meta * 100) by (node_name) > 80\n  FOR 5m\n  LABELS      { severity=\"warning\" }\n  ANNOTATIONS {\n      summary = \"Memory alert for Swarm node '{{ $labels.node_name }}'\",\n      description = \"Swarm node {{ $labels.node_name }} memory usage is at {{ humanize $value}}%.\",\n  }\n```\n***Swarm Node Disk Alert***\n\nAlerts when a node storage usage goes over 85% for five minutes.\n\n```\nALERT node_disk_usage\n  IF ((node_filesystem_size{mountpoint=\"/rootfs\"} - node_filesystem_free{mountpoint=\"/rootfs\"}) * 100 / node_filesystem_size{mountpoint=\"/rootfs\"}) * on(instance) group_left(node_name) node_meta > 85\n  FOR 5m\n  LABELS      { severity=\"warning\" }\n  ANNOTATIONS {\n      summary = \"Disk alert for Swarm node '{{ $labels.node_name }}'\",\n      description = \"Swarm node {{ $labels.node_name }} disk usage is at {{ humanize $value}}%.\",\n  }\n```\n\n***Swarm Node Disk Fill Rate Alert***\n\nAlerts when a node storage is going to remain out of free space in six hours.\n\n```\nALERT node_disk_fill_rate_6h\n  IF predict_linear(node_filesystem_free{mountpoint=\"/rootfs\"}[1h], 6*3600) * on(instance) group_left(node_name) node_meta < 0\n  FOR 1h\n  LABELS      { severity=\"critical\" }\n  ANNOTATIONS {\n      summary = \"Disk fill alert for Swarm node '{{ $labels.node_name }}'\",\n      description = \"Swarm node {{ $labels.node_name }} disk is going to fill up in 6h.\",\n  }\n```\n\nYou can add alerts to\n[swarm_node](https://github.com/stefanprodan/swarmprom/blob/master/prometheus/rules/swarm_node.rules)\nand [swarm_task](https://github.com/stefanprodan/swarmprom/blob/master/prometheus/rules/swarm_task.rules)\nfiles and rerun stack deploy to update them. Because these files are mounted inside the Prometheus\ncontainer at run time as [Docker configs](https://docs.docker.com/engine/swarm/configs/)\nyou don't have to bundle them with the image.\n\nThe Alertmanager swarmprom image is configured with the Slack receiver.\nIn order to receive alerts on Slack you have to provide the Slack API url,\nusername and channel via environment variables:\n\n```yaml\n  alertmanager:\n    image: stefanprodan/swarmprom-alertmanager\n    environment:\n      - SLACK_URL=${SLACK_URL}\n      - SLACK_CHANNEL=${SLACK_CHANNEL}\n      - SLACK_USER=${SLACK_USER}\n```\n\nYou can install the `stress` package with apt and test out the CPU alert, you should receive something like this:\n\n![Alerts](https://raw.githubusercontent.com/stefanprodan/swarmprom/master/grafana/screens/alertmanager-slack-v2.png)\n\nCloudflare has made a great dashboard for managing alerts.\nUnsee can aggregate alerts from multiple Alertmanager instances, running either in HA mode or separate.\nYou can access unsee at `http://<swarm-ip>:9094` using the admin user/password set via compose up:\n\n![Unsee](https://raw.githubusercontent.com/stefanprodan/swarmprom/master/grafana/screens/unsee.png)\n\n## Monitoring applications and backend services\n\nYou can extend swarmprom with special-purpose exporters for services like MongoDB, PostgreSQL, Kafka,\nRedis and also instrument your own applications using the Prometheus client libraries.\n\nIn order to scrape other services you need to attach those to the `mon_net` network so Prometheus\ncan reach them. Or you can attach the `mon_prometheus` service to the networks where your services are running.\n\nOnce your services are reachable by Prometheus you can add the dns name and port of those services to the\nPrometheus config using the `JOBS` environment variable:\n\n```yaml\n  prometheus:\n    image: stefanprodan/swarmprom-prometheus\n    environment:\n      - JOBS=mongo-exporter:9216 kafka-exporter:9216 redis-exporter:9216\n```\n\n## Monitoring production systems\n\nThe swarmprom project is meant as a starting point in developing your own monitoring solution. Before running this\nin production you should consider building and publishing your own Prometheus, node exporter and alert manager\nimages. Docker Swarm doesn't play well with locally built images, the first step would be to setup a secure Docker\nregistry that your Swarm has access to and push the images there. Your CI system should assign version tags to each\nimage. Don't rely on the latest tag for continuous deployments, Prometheus will soon reach v2 and the data store\nwill not be backwards compatible with v1.x.\n\nAnother thing you should consider is having redundancy for Prometheus and alert manager.\nYou could run them as a service with two replicas pinned on different nodes, or even better,\nuse a service like Weave Cloud Cortex to ship your metrics outside of your current setup.\nYou can use Weave Cloud not only as a backup of your\nmetrics database but you can also define alerts and use it as a data source for your Grafana dashboards.\nHaving the alerting and monitoring system hosted on a different platform other than your production\nis good practice that will allow you to react quickly and efficiently when a major disaster strikes.\n\nSwarmprom comes with built-in [Weave Cloud](https://www.weave.works/product/cloud/) integration,\nwhat you need to do is run the weave-compose stack with your Weave service token:\n\n```bash\nTOKEN=<WEAVE-TOKEN> \\\nADMIN_USER=admin \\\nADMIN_PASSWORD=admin \\\ndocker stack deploy -c weave-compose.yml mon\n```\n\nThis will deploy Weave Scope and Prometheus with Weave Cortex as remote write.\nThe local retention is set to 24h so even if your internet connection drops you'll not lose data\nas Prometheus will retry pushing data to Weave Cloud when the connection is up again.\n\nYou can define alerts and notifications routes in Weave Cloud in the same way you would do with alert manager.\n\nTo use Grafana with Weave Cloud you have to reconfigure the Prometheus data source like this:\n\n* Name: Prometheus\n* Type: Prometheus\n* Url: https://cloud.weave.works/api/prom\n* Access: proxy\n* Basic auth: use your service token as password, the user value is ignored\n\nWeave Scope automatically generates a map of your application, enabling you to intuitively understand,\nmonitor, and control your microservices based application.\nYou can view metrics, tags and metadata of the running processes, containers and hosts.\nScope offers remote access to the Swarm’s nods and containers, making it easy to diagnose issues in real-time.\n\n![Scope](https://raw.githubusercontent.com/stefanprodan/swarmprom/master/grafana/screens/weave-scope.png)\n\n![Scope Hosts](https://raw.githubusercontent.com/stefanprodan/swarmprom/master/grafana/screens/weave-scope-hosts-v2.png)\n"
        },
        {
          "name": "alertmanager",
          "type": "tree",
          "content": null
        },
        {
          "name": "caddy",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose.traefik.yml",
          "type": "blob",
          "size": 8.2978515625,
          "content": "version: \"3.3\"\n\nnetworks:\n  net:\n    driver: overlay\n    attachable: true\n  traefik-public:\n    external: true\n\nvolumes:\n    prometheus: {}\n    grafana: {}\n    alertmanager: {}\n\nconfigs:\n  dockerd_config:\n    file: ./dockerd-exporter/Caddyfile\n  node_rules:\n    file: ./prometheus/rules/swarm_node.rules.yml\n  task_rules:\n    file: ./prometheus/rules/swarm_task.rules.yml\n\nservices:\n  dockerd-exporter:\n    image: stefanprodan/caddy\n    networks:\n      - net\n    environment:\n      - DOCKER_GWBRIDGE_IP=172.18.0.1\n    configs:\n      - source: dockerd_config\n        target: /etc/caddy/Caddyfile\n    deploy:\n      mode: global\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n\n  cadvisor:\n    image: google/cadvisor\n    networks:\n      - net\n    command: -logtostderr -docker_only\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /:/rootfs:ro\n      - /var/run:/var/run\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n    deploy:\n      mode: global\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n\n  grafana:\n    image: stefanprodan/swarmprom-grafana:5.3.4\n    networks:\n      - default\n      - net\n      - traefik-public\n    environment:\n      - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}\n      - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}\n      - GF_USERS_ALLOW_SIGN_UP=false\n      #- GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-localhost}\n      #- GF_SMTP_ENABLED=${GF_SMTP_ENABLED:-false}\n      #- GF_SMTP_FROM_ADDRESS=${GF_SMTP_FROM_ADDRESS:-grafana@test.com}\n      #- GF_SMTP_FROM_NAME=${GF_SMTP_FROM_NAME:-Grafana}\n      #- GF_SMTP_HOST=${GF_SMTP_HOST:-smtp:25}\n      #- GF_SMTP_USER=${GF_SMTP_USER}\n      #- GF_SMTP_PASSWORD=${GF_SMTP_PASSWORD}\n    volumes:\n      - grafana:/var/lib/grafana\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n      labels:\n        - traefik.enable=true\n        - traefik.docker.network=traefik-public\n        - traefik.constraint-label=traefik-public\n        - traefik.http.routers.swarmprom-grafana-http.rule=Host(`grafana.${DOMAIN?Variable DOMAIN not set}`)\n        - traefik.http.routers.swarmprom-grafana-http.entrypoints=http\n        - traefik.http.routers.swarmprom-grafana-http.middlewares=https-redirect\n        - traefik.http.routers.swarmprom-grafana-https.rule=Host(`grafana.${DOMAIN?Variable DOMAIN not set}`)\n        - traefik.http.routers.swarmprom-grafana-https.entrypoints=https\n        - traefik.http.routers.swarmprom-grafana-https.tls=true\n        - traefik.http.routers.swarmprom-grafana-https.tls.certresolver=le\n        - traefik.http.services.swarmprom-grafana.loadbalancer.server.port=3000\n\n  alertmanager:\n    image: stefanprodan/swarmprom-alertmanager:v0.14.0\n    networks:\n      - default\n      - net\n      - traefik-public\n    environment:\n      - SLACK_URL=${SLACK_URL:-https://hooks.slack.com/services/TOKEN}\n      - SLACK_CHANNEL=${SLACK_CHANNEL:-general}\n      - SLACK_USER=${SLACK_USER:-alertmanager}\n    command:\n      - '--config.file=/etc/alertmanager/alertmanager.yml'\n      - '--storage.path=/alertmanager'\n    volumes:\n      - alertmanager:/alertmanager\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n      labels:\n        - traefik.enable=true\n        - traefik.docker.network=traefik-public\n        - traefik.constraint-label=traefik-public\n        - traefik.http.routers.swarmprom-alertmanager-http.rule=Host(`alertmanager.${DOMAIN?Variable DOMAIN not set}`)\n        - traefik.http.routers.swarmprom-alertmanager-http.entrypoints=http\n        - traefik.http.routers.swarmprom-alertmanager-http.middlewares=https-redirect\n        - traefik.http.routers.swarmprom-alertmanager-https.rule=Host(`alertmanager.${DOMAIN?Variable DOMAIN not set}`)\n        - traefik.http.routers.swarmprom-alertmanager-https.entrypoints=https\n        - traefik.http.routers.swarmprom-alertmanager-https.tls=true\n        - traefik.http.routers.swarmprom-alertmanager-https.tls.certresolver=le\n        - traefik.http.services.swarmprom-alertmanager.loadbalancer.server.port=9093\n        - traefik.http.middlewares.swarmprom-alertmanager-auth.basicauth.users=${ADMIN_USER?Variable ADMIN_USER not set}:${HASHED_PASSWORD?Variable HASHED_PASSWORD not set}\n        - traefik.http.routers.swarmprom-alertmanager-https.middlewares=swarmprom-alertmanager-auth\n\n  unsee:\n    image: cloudflare/unsee:v0.8.0\n    networks:\n      - default\n      - net\n      - traefik-public\n    environment:\n      - \"ALERTMANAGER_URIS=default:http://alertmanager:9093\"\n    deploy:\n      mode: replicated\n      replicas: 1\n      labels:\n        - traefik.enable=true\n        - traefik.docker.network=traefik-public\n        - traefik.constraint-label=traefik-public\n        - traefik.http.routers.swarmprom-unsee-http.rule=Host(`unsee.${DOMAIN?Variable DOMAIN not set}`)\n        - traefik.http.routers.swarmprom-unsee-http.entrypoints=http\n        - traefik.http.routers.swarmprom-unsee-http.middlewares=https-redirect\n        - traefik.http.routers.swarmprom-unsee-https.rule=Host(`unsee.${DOMAIN?Variable DOMAIN not set}`)\n        - traefik.http.routers.swarmprom-unsee-https.entrypoints=https\n        - traefik.http.routers.swarmprom-unsee-https.tls=true\n        - traefik.http.routers.swarmprom-unsee-https.tls.certresolver=le\n        - traefik.http.services.swarmprom-unsee.loadbalancer.server.port=8080\n        - traefik.http.middlewares.swarmprom-unsee-auth.basicauth.users=${ADMIN_USER?Variable ADMIN_USER not set}:${HASHED_PASSWORD?Variable HASHED_PASSWORD not set}\n        - traefik.http.routers.swarmprom-unsee-https.middlewares=swarmprom-unsee-auth\n\n  node-exporter:\n    image: stefanprodan/swarmprom-node-exporter:v0.16.0\n    networks:\n      - net\n    environment:\n      - NODE_ID={{.Node.ID}}\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n      - /etc/hostname:/etc/nodename\n    command:\n      - '--path.sysfs=/host/sys'\n      - '--path.procfs=/host/proc'\n      - '--collector.textfile.directory=/etc/node-exporter/'\n      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'\n      - '--no-collector.ipvs'\n    deploy:\n      mode: global\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n\n  prometheus:\n    image: stefanprodan/swarmprom-prometheus:v2.5.0\n    networks:\n      - default\n      - net\n      - traefik-public\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention=${PROMETHEUS_RETENTION:-24h}'\n    volumes:\n      - prometheus:/prometheus\n    configs:\n      - source: node_rules\n        target: /etc/prometheus/swarm_node.rules.yml\n      - source: task_rules\n        target: /etc/prometheus/swarm_task.rules.yml\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n      resources:\n        limits:\n          memory: 2048M\n        reservations:\n          memory: 128M\n      labels:\n        - traefik.enable=true\n        - traefik.docker.network=traefik-public\n        - traefik.constraint-label=traefik-public\n        - traefik.http.routers.swarmprom-prometheus-http.rule=Host(`prometheus.${DOMAIN?Variable DOMAIN not set}`)\n        - traefik.http.routers.swarmprom-prometheus-http.entrypoints=http\n        - traefik.http.routers.swarmprom-prometheus-http.middlewares=https-redirect\n        - traefik.http.routers.swarmprom-prometheus-https.rule=Host(`prometheus.${DOMAIN?Variable DOMAIN not set}`)\n        - traefik.http.routers.swarmprom-prometheus-https.entrypoints=https\n        - traefik.http.routers.swarmprom-prometheus-https.tls=true\n        - traefik.http.routers.swarmprom-prometheus-https.tls.certresolver=le\n        - traefik.http.services.swarmprom-prometheus.loadbalancer.server.port=9090\n        - traefik.http.middlewares.swarmprom-prometheus-auth.basicauth.users=${ADMIN_USER?Variable ADMIN_USER not set}:${HASHED_PASSWORD?Variable HASHED_PASSWORD not set}\n        - traefik.http.routers.swarmprom-prometheus-https.middlewares=swarmprom-prometheus-auth\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 4.8544921875,
          "content": "version: \"3.3\"\n\nnetworks:\n  net:\n    driver: overlay\n    attachable: true\n\nvolumes:\n    prometheus: {}\n    grafana: {}\n    alertmanager: {}\n\nconfigs:\n  caddy_config:\n    file: ./caddy/Caddyfile\n  dockerd_config:\n    file: ./dockerd-exporter/Caddyfile\n  node_rules:\n    file: ./prometheus/rules/swarm_node.rules.yml\n  task_rules:\n    file: ./prometheus/rules/swarm_task.rules.yml\n\nservices:\n  dockerd-exporter:\n    image: stefanprodan/caddy\n    networks:\n      - net\n    environment:\n      - DOCKER_GWBRIDGE_IP=172.18.0.1\n    configs:\n      - source: dockerd_config\n        target: /etc/caddy/Caddyfile\n    deploy:\n      mode: global\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n\n  cadvisor:\n    image: google/cadvisor\n    networks:\n      - net\n    command: -logtostderr -docker_only\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /:/rootfs:ro\n      - /var/run:/var/run\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n    deploy:\n      mode: global\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n\n  grafana:\n    image: stefanprodan/swarmprom-grafana:5.3.4\n    networks:\n      - net\n    environment:\n      - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}\n      - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}\n      - GF_USERS_ALLOW_SIGN_UP=false\n      #- GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-localhost}\n      #- GF_SMTP_ENABLED=${GF_SMTP_ENABLED:-false}\n      #- GF_SMTP_FROM_ADDRESS=${GF_SMTP_FROM_ADDRESS:-grafana@test.com}\n      #- GF_SMTP_FROM_NAME=${GF_SMTP_FROM_NAME:-Grafana}\n      #- GF_SMTP_HOST=${GF_SMTP_HOST:-smtp:25}\n      #- GF_SMTP_USER=${GF_SMTP_USER}\n      #- GF_SMTP_PASSWORD=${GF_SMTP_PASSWORD}\n    volumes:\n      - grafana:/var/lib/grafana\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n\n  alertmanager:\n    image: stefanprodan/swarmprom-alertmanager:v0.14.0\n    networks:\n      - net\n    environment:\n      - SLACK_URL=${SLACK_URL:-https://hooks.slack.com/services/TOKEN}\n      - SLACK_CHANNEL=${SLACK_CHANNEL:-general}\n      - SLACK_USER=${SLACK_USER:-alertmanager}\n    command:\n      - '--config.file=/etc/alertmanager/alertmanager.yml'\n      - '--storage.path=/alertmanager'\n    volumes:\n      - alertmanager:/alertmanager\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n\n  unsee:\n    image: cloudflare/unsee:v0.8.0\n    networks:\n      - net\n    environment:\n      - \"ALERTMANAGER_URIS=default:http://alertmanager:9093\"\n    deploy:\n      mode: replicated\n      replicas: 1\n\n  node-exporter:\n    image: stefanprodan/swarmprom-node-exporter:v0.16.0\n    networks:\n      - net\n    environment:\n      - NODE_ID={{.Node.ID}}\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n      - /etc/hostname:/etc/nodename\n    command:\n      - '--path.sysfs=/host/sys'\n      - '--path.procfs=/host/proc'\n      - '--collector.textfile.directory=/etc/node-exporter/'\n      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'\n      - '--no-collector.ipvs'\n    deploy:\n      mode: global\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n\n  prometheus:\n    image: stefanprodan/swarmprom-prometheus:v2.5.0\n    networks:\n      - net\n    command:\n      - '--config.file=/etc/prometheus/prometheus.yml'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention=${PROMETHEUS_RETENTION:-24h}'\n    volumes:\n      - prometheus:/prometheus\n    configs:\n      - source: node_rules\n        target: /etc/prometheus/swarm_node.rules.yml\n      - source: task_rules\n        target: /etc/prometheus/swarm_task.rules.yml\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n      resources:\n        limits:\n          memory: 2048M\n        reservations:\n          memory: 128M\n\n  caddy:\n    image: stefanprodan/caddy\n    ports:\n      - \"3000:3000\"\n      - \"9090:9090\"\n      - \"9093:9093\"\n      - \"9094:9094\"\n    networks:\n      - net\n    environment:\n      - ADMIN_USER=${ADMIN_USER:-admin}\n      - ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}\n    configs:\n      - source: caddy_config\n        target: /etc/caddy/Caddyfile\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n      resources:\n        limits:\n          memory: 128M\n        reservations:\n          memory: 64M\n    healthcheck:\n      test: [\"CMD\", \"curl\", \"-f\", \"http://localhost:3000\"]\n      interval: 5s\n      timeout: 1s\n      retries: 5\n"
        },
        {
          "name": "dockerd-exporter",
          "type": "tree",
          "content": null
        },
        {
          "name": "grafana",
          "type": "tree",
          "content": null
        },
        {
          "name": "node-exporter",
          "type": "tree",
          "content": null
        },
        {
          "name": "prometheus",
          "type": "tree",
          "content": null
        },
        {
          "name": "test-compose.yml",
          "type": "blob",
          "size": 0.6240234375,
          "content": "version: \"3.3\"\n\nnetworks:\n  net:\n    driver: overlay\n    attachable: true\n  mon_net:\n    external: true\n\nservices:\n\n  mongo:\n    image: healthcheck/mongo:latest\n    networks:\n      - net\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role != manager\n\n  mongo-exporter:\n    image: forekshub/percona-mongodb-exporter:latest\n    networks:\n      - net\n      - mon_net\n    ports:\n      - \"9216:9216\"\n    environment:\n      - MONGODB_URL=mongodb://mongo:27017\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n"
        },
        {
          "name": "weave-compose.yml",
          "type": "blob",
          "size": 3.6640625,
          "content": "version: \"3.3\"\n\nnetworks:\n  net:\n    driver: overlay\n    attachable: true\n\nvolumes:\n    prometheus: {}\n    grafana: {}\n\nconfigs:\n  caddy_config:\n    file: ./caddy/Caddyfile\n  dockerd_config:\n    file: ./dockerd-exporter/Caddyfile\n\nservices:\n  dockerd-exporter:\n    image: stefanprodan/caddy\n    networks:\n      - net\n    environment:\n      - DOCKER_GWBRIDGE_IP=172.18.0.1\n    configs:\n      - source: dockerd_config\n        target: /etc/caddy/Caddyfile\n    deploy:\n      mode: global\n\n  cadvisor:\n    image: google/cadvisor\n    networks:\n      - net\n    command: -logtostderr -docker_only\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock:ro\n      - /:/rootfs:ro\n      - /var/run:/var/run\n      - /sys:/sys:ro\n      - /var/lib/docker/:/var/lib/docker:ro\n    deploy:\n      mode: global\n\n  grafana:\n    image: stefanprodan/swarmprom-grafana:4.6.3\n    networks:\n      - net\n    environment:\n      - GF_SECURITY_ADMIN_USER=${ADMIN_USER:-admin}\n      - GF_SECURITY_ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}\n      - GF_USERS_ALLOW_SIGN_UP=false\n      #- GF_SERVER_ROOT_URL=${GF_SERVER_ROOT_URL:-localhost}\n      #- GF_SMTP_ENABLED=${GF_SMTP_ENABLED:-false}\n      #- GF_SMTP_FROM_ADDRESS=${GF_SMTP_FROM_ADDRESS:-grafana@test.com}\n      #- GF_SMTP_FROM_NAME=${GF_SMTP_FROM_NAME:-Grafana}\n      #- GF_SMTP_HOST=${GF_SMTP_HOST:-smtp:25}\n      #- GF_SMTP_USER=${GF_SMTP_USER}\n      #- GF_SMTP_PASSWORD=${GF_SMTP_PASSWORD}\n    volumes:\n      - grafana:/var/lib/grafana\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n\n  node-exporter:\n    image: stefanprodan/swarmprom-node-exporter:v0.15.2\n    networks:\n      - net\n    environment:\n      - NODE_ID={{.Node.ID}}\n    volumes:\n      - /proc:/host/proc:ro\n      - /sys:/host/sys:ro\n      - /:/rootfs:ro\n      - /etc/hostname:/etc/nodename\n    command:\n      - '--path.sysfs=/host/sys'\n      - '--path.procfs=/host/proc'\n      - '--collector.textfile.directory=/etc/node-exporter/'\n      - '--collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)'\n      # no collectors are explicitely enabled here, because the defaults are just fine,\n      # see https://github.com/prometheus/node_exporter\n      # disable ipvs collector because it barfs the node-exporter logs full with errors on my centos 7 vm's\n      - '--no-collector.ipvs'\n    deploy:\n      mode: global\n\n  caddy:\n    image: stefanprodan/caddy\n    ports:\n      - \"3000:3000\"\n      - \"9090:9090\"\n    networks:\n      - net\n    environment:\n      - ADMIN_USER=${ADMIN_USER:-admin}\n      - ADMIN_PASSWORD=${ADMIN_PASSWORD:-admin}\n    configs:\n      - source: caddy_config\n        target: /etc/caddy/Caddyfile\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n\n  prometheus:\n    image: stefanprodan/swarmprom-prometheus:v2.2.0-rc.0\n    networks:\n      - net\n    environment:\n      - WEAVE_TOKEN=$TOKEN\n      #- JOBS=mongo-exporter:9216\n    command:\n      - '--config.file=/etc/prometheus/weave-cortex.yml'\n      - '--web.console.libraries=/etc/prometheus/console_libraries'\n      - '--web.console.templates=/etc/prometheus/consoles'\n      - '--storage.tsdb.path=/prometheus'\n      - '--storage.tsdb.retention=${PROMETHEUS_RETENTION:-24h}'\n    volumes:\n      - prometheus:/prometheus\n    deploy:\n      mode: replicated\n      replicas: 1\n      placement:\n        constraints:\n          - node.role == manager\n\n  scope-launcher:\n    image: weaveworks/scope-swarm-launcher\n    networks:\n      - net\n    command: scope launch --service-token=$TOKEN\n    volumes:\n      - /var/run/docker.sock:/var/run/docker.sock\n    deploy:\n      mode: global\n      restart_policy:\n        condition: none\n"
        }
      ]
    }
  ]
}