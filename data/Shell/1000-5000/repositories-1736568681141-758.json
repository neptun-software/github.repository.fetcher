{
  "metadata": {
    "timestamp": 1736568681141,
    "page": 758,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc2OQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVIDIA/deepops",
      "stars": 1279,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.154296875,
          "content": "# ansible\n*.retry\n\n# misc.\n.*.swp\n\n# project-specific\n/admin.conf\n/config*/\n!/config.example/\n/roles/galaxy/\n/collections/*\n/k8s-config/\n/kubectl\n/tridentctl\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.2109375,
          "content": "[submodule \"kubespray\"]\n\tpath = submodules/kubespray\n\turl = https://github.com/kubernetes-sigs/kubespray.git\n[submodule \"packer-maas\"]\n\tpath = submodules/packer-maas\n\turl = https://github.com/DeepOps/packer-maas.git\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.134765625,
          "content": "# Contribute to NVIDIA DeepOps\n\nAll PRs submitted to the DeepOps project must be signed, see below for more information\n\n## Sign your work\n\nThe sign-off is a simple line at the end of the explanation for the patch. Your\nsignature certifies that you wrote the patch or otherwise have the right to pass\nit on as an open-source patch. The rules are pretty simple: if you can certify\nthe below (from [developercertificate.org](http://developercertificate.org/)):\n\n```\nDeveloper Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n1 Letterman Drive\nSuite D4700\nSan Francisco, CA, 94129\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n```\n\nThen you just add a line to every git commit message:\n\n    Signed-off-by: Joe Smith <joe.smith@email.com>\n\nUse your real name (sorry, no pseudonyms or anonymous contributions.)\n\nIf you set your `user.name` and `user.email` git configs, you can sign your\ncommit automatically with `git commit -s`.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.482421875,
          "content": "BSD 3-Clause License\n\nCopyright (c) 2018, NVIDIA Corporation\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n* Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n* Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.533203125,
          "content": "# DeepOps\n\nInfrastructure automation tools for Kubernetes and Slurm clusters with NVIDIA GPUs.\n\n## Table of Contents\n\n- [DeepOps](#deepops)\n  - [Table of Contents](#table-of-contents)\n  - [Overview](#overview)\n  - [Deployment Requirements](#deployment-requirements)\n    - [Provisioning System](#provisioning-system)\n    - [Cluster System](#cluster-system)\n    - [Kubernetes](#kubernetes)\n    - [Slurm](#slurm)\n    - [Hybrid clusters](#hybrid-clusters)\n    - [Virtual](#virtual)\n  - [Updating DeepOps](#updating-deepops)\n  - [Copyright and License](#copyright-and-license)\n  - [Issues](#issues)\n  - [Contributing](#contributing)\n\n## Overview\n\nThe DeepOps project encapsulates best practices in the deployment of GPU server clusters and sharing single powerful nodes (such as [NVIDIA DGX Systems](https://www.nvidia.com/en-us/data-center/dgx-systems/)). DeepOps may also be adapted or used in a modular fashion to match site-specific cluster needs. For example:\n\n- An on-prem data center of NVIDIA DGX servers where DeepOps provides end-to-end capabilities to set up the entire cluster management stack\n- An existing cluster running Kubernetes where DeepOps scripts are used to deploy KubeFlow and connect NFS storage\n- An existing cluster that needs a resource manager / batch scheduler, where DeepOps is used to install Slurm or Kubernetes\n- A single machine where no scheduler is desired, only NVIDIA drivers, Docker, and the NVIDIA Container Runtime\n\nLatest release: [DeepOps 23.08 Release](https://github.com/NVIDIA/deepops/releases/tag/23.08)\n\nIt is recommended to use the latest release branch for stable code (linked above). All development takes place on the master branch, which is generally [functional](docs/deepops/testing.md) but may change significantly between releases.\n\n## Deployment Requirements\n\n### Provisioning System\n\nThe provisioning system is used to orchestrate the running of all playbooks and one will be needed when instantiating Kubernetes or Slurm clusters. Supported operating systems which are tested and supported include:\n\n- NVIDIA DGX OS 4, 5\n- Ubuntu 18.04 LTS, 20.04, 22.04 LTS\n- CentOS 7, 8\n\n### Cluster System\n\nThe cluster nodes will follow the requirements described by Slurm or Kubernetes. You may also use a cluster node as a provisioning system but it is not required.\n\n- NVIDIA DGX OS 4, 5\n- Ubuntu 18.04 LTS, 20.04, 22.04 LTS\n- CentOS 7, 8\n\nYou may also install a supported operating system on all servers via a 3rd-party solution (i.e. [MAAS](https://maas.io/), [Foreman](https://www.theforeman.org/)) or utilize the provided [OS install container](docs/pxe/minimal-pxe-container.md).\n\n### Kubernetes\n\nKubernetes (K8s) is an open-source system for automating deployment, scaling, and management of containerized applications. The instantiation of a Kubernetes cluster is done by [Kubespray](submodules/kubespray). Kubespray runs on bare metal and most clouds, using Ansible as its substrate for provisioning and orchestration. For people with familiarity with Ansible, existing Ansible deployments or the desire to run a Kubernetes cluster across multiple platforms, Kubespray is a good choice. Kubespray does generic configuration management tasks from the \"OS operators\" ansible world, plus some initial K8s clustering (with networking plugins included) and control plane bootstrapping. DeepOps provides additional playbooks for orchestration and optimization of GPU environments.\n\nConsult the [DeepOps Kubernetes Deployment Guide](docs/k8s-cluster/) for instructions on building a GPU-enabled Kubernetes cluster using DeepOps.\n\nFor more information on Kubernetes in general, refer to the [official Kubernetes docs](https://kubernetes.io/docs/concepts/overview/what-is-kubernetes/).\n\n### Slurm\n\nSlurm is an open-source cluster resource management and job scheduling system that strives to be simple, scalable, portable, fault-tolerant, and interconnect agnostic. Slurm currently has been tested only under Linux.\n\nAs a cluster resource manager, Slurm provides three key functions. First, it allocates exclusive and/or non-exclusive access to resources (compute nodes) to users for some duration of time so they can perform work. Second, it provides a framework for starting, executing, and monitoring work (normally a parallel job) on the set of allocated nodes. Finally, it arbitrates conflicting requests for resources by managing a queue of pending work. Slurm cluster instantiation is achieved through [SchedMD](https://slurm.schedmd.com/download.html)\n\nConsult the [DeepOps Slurm Deployment Guide](docs/slurm-cluster/) for instructions on building a GPU-enabled Slurm cluster using DeepOps.\n\nFor more information on Slurm in general, refer to the [official Slurm docs](https://slurm.schedmd.com/overview.html).\n\n### Hybrid clusters\n\n**DeepOps does not test or support a configuration where both Kubernetes and Slurm are deployed on the same physical cluster.**\n\n[NVIDIA Bright Cluster Manager](https://www.brightcomputing.com/brightclustermanager) is recommended as an enterprise solution which enables managing multiple workload managers within a single cluster, including Kubernetes, Slurm, Univa Grid Engine, and PBS Pro.\n\n**DeepOps does not test or support a configuration where nodes have a heterogenous OS running.**\nAdditional modifications are needed if you plan to use unsupported operating systems such as RHEL.\n\n### Virtual\n\nTo try DeepOps before deploying it on an actual cluster, a virtualized version of DeepOps may be deployed on a single node using Vagrant. This can be used for testing, adding new features, or configuring DeepOps to meet deployment-specific needs.\n\nConsult the [Virtual DeepOps Deployment Guide](virtual/README.md) to build a GPU-enabled virtual cluster with DeepOps.\n\n## Updating DeepOps\n\nTo update from a previous version of DeepOps to a newer release, please consult the [DeepOps Update Guide](docs/deepops/update-deepops.md).\n\n## Copyright and License\n\nThis project is released under the [BSD 3-clause license](https://github.com/NVIDIA/deepops/blob/master/LICENSE).\n\n## Issues\n\nNVIDIA DGX customers should file an NVES ticket via [NVIDIA Enterprise Services](https://nvid.nvidia.com/enterpriselogin/).\n\nOtherwise, bugs and feature requests can be made by [filing a GitHub Issue](https://github.com/NVIDIA/deepops/issues/new).\n\n## Contributing\n\nTo contribute, please issue a [signed](https://raw.githubusercontent.com/NVIDIA/deepops/master/CONTRIBUTING.md) [pull request](https://help.github.com/articles/using-pull-requests/) against the master branch from a local fork. See the [contribution document](https://raw.githubusercontent.com/NVIDIA/deepops/master/CONTRIBUTING.md) for more information.\n"
        },
        {
          "name": "ansible.cfg",
          "type": "blob",
          "size": 0.806640625,
          "content": "[defaults]\ncollections_paths = ./collections\nroles_path = ./roles/galaxy:./roles:./submodules/kubespray/roles\nlibrary = ./submodules/kubespray/library\ninventory = ./config/inventory\nhost_key_checking = False\ngathering = smart\nfact_caching = jsonfile\nfact_caching_connection = /var/tmp/ansible_cache\nfact_caching_timeout = 86400\ndeprecation_warnings = False\n#vault_password_file = ./config/.vault-pass\ntimeout=60\nstdout_callback = yaml\nbin_ansible_callbacks = True\nlocal_tmp=/tmp\nremote_tmp=/tmp\nforks = 25\nforce_valid_group_names = ignore\nansible_python_interpreter = /usr/bin/python3\n\n[galaxy]\nserver = https://old-galaxy.ansible.com/\n\n[ssh_connection]\npipelining = True\nssh_args = -o ControlMaster=auto -o ControlPersist=5m -o ConnectionAttempts=100 -o UserKnownHostsFile=/dev/null\ncontrol_path = ~/.ssh/ansible-%%r@%%h:%%p\n"
        },
        {
          "name": "config.example",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "playbooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "roles",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "submodules",
          "type": "tree",
          "content": null
        },
        {
          "name": "virtual",
          "type": "tree",
          "content": null
        },
        {
          "name": "workloads",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}