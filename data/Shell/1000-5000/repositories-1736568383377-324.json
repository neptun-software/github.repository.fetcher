{
  "metadata": {
    "timestamp": 1736568383377,
    "page": 324,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "spujadas/elk-docker",
      "stars": 2160,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.013671875,
          "content": "* text eol=lf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 6.1923828125,
          "content": "# Dockerfile for ELK stack\n# Elasticsearch, Logstash, Kibana 8.15.1\n\n# Build with:\n# docker build -t <repo-user>/elk .\n\n# Run with:\n# docker run -p 5601:5601 -p 9200:9200 -p 5044:5044 -it --name elk <repo-user>/elk\n\n# replace with master-arm64 for ARM64\nARG IMAGE=focal-1.1.0\n\nFROM phusion/baseimage:${IMAGE}\nMAINTAINER Sebastien Pujadas http://pujadas.net\nENV \\\n REFRESHED_AT=2020-06-20\n\n\n###############################################################################\n#                                INSTALLATION\n###############################################################################\n\n### install prerequisites (cURL, gosu, tzdata, JDK for Logstash)\n\nRUN set -x \\\n && apt update -qq \\\n && apt install -qqy --no-install-recommends ca-certificates curl gosu tzdata openjdk-11-jdk-headless \\\n && apt clean \\\n && rm -rf /var/lib/apt/lists/* \\\n && gosu nobody true \\\n && set +x\n\n\n### set current package version\n\nARG ELK_VERSION=8.15.1\n\n# base version (i.e. remove OSS prefix) for Elasticsearch and Kibana (no OSS version since 7.11.0)\nARG ELK_BASE_VERSION=8.15.1\n\n# replace with aarch64 for ARM64 systems\nARG ARCH=x86_64 \n\n\n### install Elasticsearch\n\n# predefine env vars, as you can't define an env var that references another one in the same block\nENV \\\n ES_VERSION=${ELK_BASE_VERSION} \\\n ES_HOME=/opt/elasticsearch\n\nENV \\\n ES_PACKAGE=elasticsearch-${ES_VERSION}-linux-${ARCH}.tar.gz \\\n ES_GID=991 \\\n ES_UID=991 \\\n ES_PATH_CONF=/etc/elasticsearch \\\n ES_PATH_BACKUP=/var/backups\n\nRUN DEBIAN_FRONTEND=noninteractive \\\n && mkdir ${ES_HOME} \\\n && curl -O https://artifacts.elastic.co/downloads/elasticsearch/${ES_PACKAGE} \\\n && tar xzf ${ES_PACKAGE} -C ${ES_HOME} --strip-components=1 \\\n && rm -f ${ES_PACKAGE} \\\n && groupadd -r elasticsearch -g ${ES_GID} \\\n && useradd -r -s /usr/sbin/nologin -M -d ${ES_HOME} -c \"Elasticsearch service user\" -u ${ES_UID} -g elasticsearch elasticsearch \\\n && mkdir -p /var/log/elasticsearch ${ES_PATH_CONF} ${ES_PATH_CONF}/scripts ${ES_PATH_CONF}/jvm.options.d /var/lib/elasticsearch ${ES_PATH_BACKUP} \\\n && chown -R elasticsearch:elasticsearch ${ES_HOME} /var/log/elasticsearch /var/lib/elasticsearch ${ES_PATH_CONF} ${ES_PATH_BACKUP}\n\n\n### install Logstash\n\nENV \\\n LOGSTASH_VERSION=${ELK_VERSION} \\\n LOGSTASH_HOME=/opt/logstash\n\nENV \\\n LOGSTASH_PACKAGE=logstash-${LOGSTASH_VERSION}-linux-${ARCH}.tar.gz \\\n LOGSTASH_GID=992 \\\n LOGSTASH_UID=992 \\\n LOGSTASH_PATH_CONF=/etc/logstash \\\n LOGSTASH_PATH_SETTINGS=${LOGSTASH_HOME}/config\n\nRUN mkdir ${LOGSTASH_HOME} \\\n && curl -O https://artifacts.elastic.co/downloads/logstash/${LOGSTASH_PACKAGE} \\\n && tar xzf ${LOGSTASH_PACKAGE} -C ${LOGSTASH_HOME} --strip-components=1 \\\n && rm -f ${LOGSTASH_PACKAGE} \\\n && groupadd -r logstash -g ${LOGSTASH_GID} \\\n && useradd -r -s /usr/sbin/nologin -M -d ${LOGSTASH_HOME} -c \"Logstash service user\" -u ${LOGSTASH_UID} -g logstash logstash \\\n && mkdir -p /var/log/logstash ${LOGSTASH_PATH_CONF}/conf.d \\\n && chown -R logstash:logstash ${LOGSTASH_HOME} /var/log/logstash ${LOGSTASH_PATH_CONF}\n\n\n### install Kibana\n\nENV \\\n KIBANA_VERSION=${ELK_BASE_VERSION} \\\n KIBANA_HOME=/opt/kibana\n\nENV \\\n KIBANA_PACKAGE=kibana-${KIBANA_VERSION}-linux-${ARCH}.tar.gz \\\n KIBANA_GID=993 \\\n KIBANA_UID=993\n\nRUN mkdir ${KIBANA_HOME} \\\n && curl -O https://artifacts.elastic.co/downloads/kibana/${KIBANA_PACKAGE} \\\n && tar xzf ${KIBANA_PACKAGE} -C ${KIBANA_HOME} --strip-components=1 \\\n && rm -f ${KIBANA_PACKAGE} \\\n && groupadd -r kibana -g ${KIBANA_GID} \\\n && useradd -r -s /usr/sbin/nologin -d ${KIBANA_HOME} -c \"Kibana service user\" -u ${KIBANA_UID} -g kibana kibana \\\n && mkdir -p /var/log/kibana \\\n && chown -R kibana:kibana ${KIBANA_HOME} /var/log/kibana\n\n\n###############################################################################\n#                              START-UP SCRIPTS\n###############################################################################\n\n### Elasticsearch\n\nADD ./elasticsearch-init /etc/init.d/elasticsearch\nRUN sed -i -e 's#^ES_HOME=$#ES_HOME='$ES_HOME'#' /etc/init.d/elasticsearch \\\n && chmod +x /etc/init.d/elasticsearch\n\n\n### Logstash\n\nADD ./logstash-init /etc/init.d/logstash\nRUN sed -i -e 's#^LS_HOME=$#LS_HOME='$LOGSTASH_HOME'#' /etc/init.d/logstash \\\n && chmod +x /etc/init.d/logstash\n\n\n### Kibana\n\nADD ./kibana-init /etc/init.d/kibana\nRUN sed -i -e 's#^KIBANA_HOME=$#KIBANA_HOME='$KIBANA_HOME'#' /etc/init.d/kibana \\\n && chmod +x /etc/init.d/kibana\n\n\n###############################################################################\n#                               CONFIGURATION\n###############################################################################\n\n### configure Elasticsearch\n\nADD ./elasticsearch.yml ${ES_PATH_CONF}/elasticsearch.yml\nADD ./elasticsearch-default /etc/default/elasticsearch\nRUN cp ${ES_HOME}/config/log4j2.properties ${ES_HOME}/config/jvm.options \\\n    ${ES_PATH_CONF} \\\n && chown -R elasticsearch:elasticsearch ${ES_PATH_CONF} \\\n && chmod -R +r ${ES_PATH_CONF}\n\n\n### configure Logstash\n\n# certs/keys for Beats and Lumberjack input\nRUN mkdir -p /etc/pki/tls/{certs,private}\nADD ./logstash-beats.crt /etc/pki/tls/certs/logstash-beats.crt\nADD ./logstash-beats.key /etc/pki/tls/private/logstash-beats.key\n\n# pipelines\nADD pipelines.yml ${LOGSTASH_PATH_SETTINGS}/pipelines.yml\n\n# filters\nADD ./logstash-conf/*.conf ${LOGSTASH_PATH_CONF}/conf.d/\n\n# patterns\nADD ./nginx.pattern ${LOGSTASH_HOME}/patterns/nginx\nRUN chown -R logstash:logstash ${LOGSTASH_HOME}/patterns\n\n# Fix permissions\nRUN chmod -R +r ${LOGSTASH_PATH_CONF} ${LOGSTASH_PATH_SETTINGS} \\\n && chown -R logstash:logstash ${LOGSTASH_PATH_SETTINGS}\n\n\n### configure logrotate\n\nADD ./elasticsearch-logrotate /etc/logrotate.d/elasticsearch\nADD ./logstash-logrotate /etc/logrotate.d/logstash\nADD ./kibana-logrotate /etc/logrotate.d/kibana\nRUN chmod 644 /etc/logrotate.d/elasticsearch \\\n && chmod 644 /etc/logrotate.d/logstash \\\n && chmod 644 /etc/logrotate.d/kibana\n\n\n### configure Kibana\n\nADD ./kibana.yml ${KIBANA_HOME}/config/kibana.yml\n\n\n###############################################################################\n#                                   START\n###############################################################################\n\nADD ./start.sh /usr/local/bin/start.sh\nRUN chmod +x /usr/local/bin/start.sh\n\nEXPOSE 5601 9200 9300 9600 5044\nVOLUME /var/lib/elasticsearch\n\nCMD [ \"/usr/local/bin/start.sh\" ]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 0.544921875,
          "content": "Copyright 2015 SÃ©bastien Pujadas\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License."
        },
        {
          "name": "README-short.txt",
          "type": "blob",
          "size": 0.1015625,
          "content": "Collect, search and visualise log data with ELK (Elasticsearch 8.15.1, Logstash 8.15.1, Kibana 8.15.1).\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.833984375,
          "content": "# Elasticsearch, Logstash, Kibana (ELK) Docker image\n\n[![](https://images.microbadger.com/badges/image/sebp/elk.svg)](https://microbadger.com/images/sebp/elk \"Get your own image badge on microbadger.com\") [![Documentation Status](https://readthedocs.org/projects/elk-docker/badge/?version=latest)](http://elk-docker.readthedocs.io/?badge=latest)\n\nThis Docker image provides a convenient centralised log server and log management web interface, by packaging Elasticsearch, Logstash, and Kibana, collectively known as ELK.\n\n### Documentation\n\nSee the [ELK Docker image documentation web page](http://elk-docker.readthedocs.io/) for complete instructions on how to use this image.\n\n### Docker Hub\n\nThis image is hosted on Docker Hub at [https://hub.docker.com/r/sebp/elk/](https://hub.docker.com/r/sebp/elk/).\n\nThe following tags are available:\n\n- `latest`, `8.15.1`: ELK 8.15.1.\n\n- `oss-8.15.1` (ELK OSS 8.15.1), `8.15.0` (8.15.0), `oss-8.15.0` (OSS 8.15.0), `8.14.3` (8.14.3), `8.14.1` (8.14.1), `oss-8.14.1` (OSS 8.14.1), `8.14.0` (8.14.0), `oss-8.14.0` (OSS 8.14.0), `8.13.4` (8.13.4), `oss-8.13.4` (OSS 8.13.4), `8.13.3` (8.13.3), `oss-8.13.3` (OSS 8.13.3), `8.13.2` (8.13.2), `oss-8.13.2` (OSS 8.13.2), `8.13.1` (8.13.1), `oss-8.13.1` (OSS 8.13.1), `8.13.0` (8.13.0), `oss-8.13.0` (OSS 8.13.0), `8.12.2` (8.12.2), `oss-8.12.2` (OSS 8.12.2), `8.12.1` (8.12.1), `oss-8.12.1` (OSS 8.12.1), `8.12.0` (8.12.0), `oss-8.12.0` (OSS 8.12.0), `8.11.4` (8.11.4), `oss-8.11.4` (OSS 8.11.4), `8.11.3` (8.11.3), `oss-8.11.3` (OSS 8.11.3), `8.11.2` (8.11.2), `oss-8.11.2` (OSS 8.11.2), `8.11.1` (8.11.1), `oss-8.11.1` (OSS 8.11.1), `8.10.4` (8.10.4), `oss-8.10.4` (OSS 8.10.4), `8.9.2` (8.9.2), `oss-8.9.2` (OSS 8.9.2), `8.9.1` (8.9.1), `oss-8.9.1` (OSS 8.9.1), `8.9.0` (8.9.0), `oss-8.9.0` (OSS 8.9.0), `8.8.2` (8.8.2), `oss-8.8.2` (OSS 8.8.2), `8.8.1` (8.8.1), `oss-8.8.1` (OSS 8.8.1), `8.8.0` (8.8.0), `oss-8.8.0` (OSS 8.8.0), `8.7.1` (8.7.1), `oss-8.7.1` (OSS 8.7.1), `8.7.0` (8.7.0), `oss-8.7.0` (OSS 8.7.0), `8.6.2` (8.6.2), `oss-8.6.2` (OSS 8.6.2), `8.6.1` (8.6.1), `oss-8.6.1` (OSS 8.6.1), `8.6.0` (8.6.0), `oss-8.6.0` (OSS 8.6.0), `8.5.3` (8.5.3), `oss-8.5.3` (OSS 8.5.3), `8.5.2` (8.5.2), `oss-8.5.2` (OSS 8.5.2), `8.5.1` (8.5.1), `oss-8.5.1` (OSS 8.5.1), `8.5.0` (8.5.0), `oss-8.5.0` (OSS 8.5.0), `8.4.3` (8.4.3), `oss-8.4.3` (OSS 8.4.3), `8.4.2` (8.4.2), `oss-8.4.2` (OSS 8.4.2), `8.4.1` (8.4.1), `oss-8.4.1` (OSS 8.4.1), `8.4.0` (8.4.0), `oss-8.4.0` (OSS 8.4.0), `8.3.3` (8.3.3), `oss-8.3.3` (OSS 8.3.3), `8.3.2` (8.3.2), `oss-8.3.2` (OSS 8.3.2), `8.3.1` (8.3.1), `oss-8.3.1` (OSS 8.3.1), `8.3.0` (8.3.0), `oss-8.3.0` (OSS 8.3.0), `8.2.3` (8.2.3), `oss-8.2.3` (OSS 8.2.3), `8.2.2` (8.2.2), `oss-8.2.2` (OSS 8.2.2), `8.2.1` (8.2.1), `oss-8.2.1` (OSS 8.2.1), `8.2.0` (8.2.0), `oss-8.2.0` (OSS 8.2.0), `8.1.3` (8.1.3), `oss-8.1.3` (OSS 8.1.3), `8.1.2` (8.1.2), `oss-8.1.2` (OSS 8.1.2), `8.1.1` (8.1.1), `oss-8.1.1` (OSS 8.1.1), `8.1.0` (8.1.0), `oss-8.1.0` (OSS 8.1.0), `8.0.1` (8.0.1), `oss-8.0.1` (OSS 8.0.1), `8.0.0` (8.0.0), `oss-8.0.0` (OSS 8.0.0).\n\n- `7.17.1` (ELK 7.17.5), `oss-7.17.1` (OSS 7.17.5), `7.17.1` (7.17.1), `oss-7.17.1` (OSS 7.17.1), `7.17.0` (7.17.0), `oss-7.17.0` (OSS 7.17.0), `7.16.3` (7.16.3), `oss-7.16.3` (OSS 7.16.3), `7.16.2` (7.16.2), `oss-7.16.2` (OSS 7.16.2), `7.16.1` (7.16.1), `oss-7.16.1` (OSS 7.16.1), `7.16.0` (7.16.0), `oss-7.16.0` (OSS 7.16.0), `7.15.2` (7.15.2), `oss-7.15.2` (OSS 7.15.2), `7.15.1` (7.15.1), `oss-7.15.1` (OSS 7.15.1), `7.15.0` (7.15.0), `oss-7.15.0` (OSS 7.15.0), `7.14.2` (7.14.2), `oss-7.14.2` (OSS 7.14.2), `7.14.1` (7.14.1), `oss-7.14.1` (OSS 7.14.1), `7.14.0` (7.14.0), `oss-7.14.0` (OSS 7.14.0), `7.13.4` (7.13.4), `oss-7.13.4` (OSS 7.13.4), `7.13.3` (7.13.3), `oss-7.13.3` (OSS 7.13.3), `7.13.2` (7.13.2), `oss-7.13.2` (OSS 7.13.2), `7.13.1` (7.13.1), `oss-7.13.1` (OSS 7.13.1), `7.13.0` (7.13.0), `oss-7.13.0` (OSS 7.13.0), `7.12.1` (7.12.1), `oss-7.12.1` (OSS 7.12.1), `7.12.0` (7.12.0), `oss-7.12.0` (OSS 7.12.0), `7.11.2` (7.11.2), `oss-7.11.2` (OSS 7.11.2), `7.11.1` (7.11.1), `oss-7.11.1` (OSS 7.11.1), `oss-7.11.0` (OSS 7.11.0), `7.11.0` (7.11.0), `oss-7.10.2` (OSS 7.10.2), `7.10.2` (7.10.2), `oss-7.10.1` (OSS 7.10.1), `7.10.1` (7.10.1), `oss-7.10.0` (OSS 7.10.0), `7.10.0` (7.10.0), `oss-793` (OSS 7.9.3), `793` (7.9.3), `oss-792` (OSS 7.9.2), `792` (7.9.2), `oss-791` (OSS 7.9.1), `791` (7.9.1), `oss-790` (OSS 7.9.0), `790` (7.9.0), `oss-781` (OSS 7.8.1), `781` (7.8.1), `oss-780` (OSS 7.8.0), `780` (7.8.0), `771` (7.7.1), `770` (7.7.0), `762` (7.6.2), `761` (7.6.1), `760` (7.6.0), `752` (7.5.2), `751` (7.5.1), `750` (7.5.0), `742` (7.4.2), `741` (7.4.1), `740` (7.4.0), `732` (7.3.2), `731` (7.3.1), `730` (7.3.0), `721` (7.2.1), `720` (7.2.0), `711` (7.1.1), `710` (7.1.0), `701` (7.0.1), `700` (7.0.0).\n\n- `6.8.22` (ELK 6.8.22), `683` (6.8.3), `681` (ELK 6.8.2), `681` (ELK 6.8.1), `680` (ELK 6.8.0), `672` (ELK 6.7.2), `671` (ELK 6.7.1), `670` (6.7.0), `662` (6.6.2), `661` (6.6.1), `660` (6.6.0), `651` (6.5.1), `650` (6.5.0), `643` (6.4.3), `642` (6.4.2), `641` (6.4.1), `640` (6.4.0), `632` (6.3.2), `631` (6.3.1), `630` (6.3.0), `624` (6.2.4), `623` (6.2.3), `622` (6.2.2), `621` (6.2.1), `620` (6.2.0), `613` (6.1.3), `612` (6.1.2), `611` (6.1.1), `610` (6.1.0), `601` (6.0.1), `600` (6.0.0).\n\n- `5615` (ELK version 5.6.15), `568` (5.6.8), `564` (5.6.4), `563` (5.6.3), `562` (5.6.2), `561` (5.6.1), `560` (5.6.0), `553` (5.5.3), `552` (5.5.2), `551` (5.5.1), `550` (5.5.0), `543` (5.4.3), `542` (5.4.2), `541` (5.4.1), `540` (5.4.0), `532` (5.3.2), `531` (5.3.1), `530` (5.3.0), `522` (5.2.2), `521` (5.2.1), `520` (5.2.0), `512` (5.1.2), `511` (5.1.1), `502` (5.0.2), `es501_l501_k501` (5.0.1), `es500_l500_k500` (5.0.0).\n\n- `es241_l240_k461`: Elasticsearch 2.4.1, Logstash 2.4.0, and Kibana 4.6.1.\n\n- `es240_l240_k460`: Elasticsearch 2.4.0, Logstash 2.4.0, and Kibana 4.6.0.\n\n- `es235_l234_k454`: Elasticsearch 2.3.5, Logstash 2.3.4, and Kibana 4.5.4.\n\n- `es234_l234_k453`: Elasticsearch 2.3.4, Logstash 2.3.4, and Kibana 4.5.3.\n\n- `es234_l234_k452`: Elasticsearch 2.3.4, Logstash 2.3.4, and Kibana 4.5.2.\n\n- `es233_l232_k451`: Elasticsearch 2.3.3, Logstash 2.3.2, and Kibana 4.5.1.\n\n- `es232_l232_k450`: Elasticsearch 2.3.2, Logstash 2.3.2, and Kibana 4.5.0.\n\n- `es231_l231_k450`: Elasticsearch 2.3.1, Logstash 2.3.1, and Kibana 4.5.0.\n\n- `es230_l230_k450`: Elasticsearch 2.3.0, Logstash 2.3.0, and Kibana 4.5.0.\n\n- `es221_l222_k442`: Elasticsearch 2.2.1, Logstash 2.2.2, and Kibana 4.4.2.\n\n- `es220_l222_k441`: Elasticsearch 2.2.0, Logstash 2.2.2, and Kibana 4.4.1.\n\n- `es220_l220_k440`: Elasticsearch 2.2.0, Logstash 2.2.0, and Kibana 4.4.0.\n\n- `E1L1K4`: Elasticsearch 1.7.3, Logstash 1.5.5, and Kibana 4.1.2.\n\n**Note** â See the documentation page for more information on pulling specific combinations of versions of Elasticsearch, Logstash and Kibana.\n\n### About\n\nWritten by [SÃ©bastien Pujadas](https://pujadas.net), released under the [Apache 2 license](https://www.apache.org/licenses/LICENSE-2.0).\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.1865234375,
          "content": "# Docker Compose file for ELK stack\n\n# Build with:\n# docker-compose build elk\n\n# Run with:\n# docker-compose up\n\nelk:\n  build: .\n  ports:\n    - \"5601:5601\"\n    - \"9200:9200\"\n    - \"5044:5044\"\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "elasticsearch-default",
          "type": "blob",
          "size": 1.55859375,
          "content": "################################\n# Elasticsearch\n################################\n\n# Elasticsearch home directory\n#ES_HOME=/usr/share/elasticsearch\n\n# Elasticsearch Java path\n#ES_JAVA_HOME=\n\n# Elasticsearch configuration directory\n#ES_PATH_CONF=/etc/elasticsearch\n\n# Elasticsearch PID directory\n#PID_DIR=/var/run/elasticsearch\n\n# Additional Java OPTS\n#ES_JAVA_OPTS=\n\n# Configure restart on package upgrade (true, every other setting will lead to not restarting)\n#RESTART_ON_UPGRADE=true\n\n################################\n# Elasticsearch service\n################################\n\n# SysV init.d\n#\n# The number of seconds to wait before checking if Elasticsearch started successfully as a daemon process\nES_STARTUP_SLEEP_TIME=5\n\n################################\n# System properties\n################################\n\n# Specifies the maximum file descriptor number that can be opened by this process\n# When using Systemd, this setting is ignored and the LimitNOFILE defined in\n# /usr/lib/systemd/system/elasticsearch.service takes precedence\n#MAX_OPEN_FILES=65536\n\n# The maximum number of bytes of memory that may be locked into RAM\n# Set to \"unlimited\" if you use the 'bootstrap.memory_lock: true' option\n# in elasticsearch.yml.\n# When using Systemd, the LimitMEMLOCK property must be set\n# in /usr/lib/systemd/system/elasticsearch.service\n#MAX_LOCKED_MEMORY=unlimited\n\n# Maximum number of VMA (Virtual Memory Areas) a process can own\n# When using Systemd, this setting is ignored and the 'vm.max_map_count'\n# property is set at boot time in /usr/lib/sysctl.d/elasticsearch.conf\n#MAX_MAP_COUNT=262144"
        },
        {
          "name": "elasticsearch-init",
          "type": "blob",
          "size": 4.9208984375,
          "content": "#!/bin/bash\n#\n# /etc/init.d/elasticsearch -- startup script for Elasticsearch\n#\n### BEGIN INIT INFO\n# Provides:          elasticsearch\n# Required-Start:    $network $remote_fs $named\n# Required-Stop:     $network $remote_fs $named\n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description: Starts elasticsearch\n# Description:       Starts elasticsearch using start-stop-daemon\n### END INIT INFO\n\nPATH=/bin:/usr/bin:/sbin:/usr/sbin\nNAME=elasticsearch\nDESC=\"Elasticsearch Server\"\nDEFAULT=/etc/default/$NAME\n\nif [ `id -u` -ne 0 ]; then\n    echo \"You need root privileges to run this script\"\n    exit 1\nfi\n\n\n. /lib/lsb/init-functions\n\nif [ -r /etc/default/rcS ]; then\n        . /etc/default/rcS\nfi\n\n\n# The following variables can be overwritten in $DEFAULT\n\n# Run Elasticsearch as this user ID and group ID\nES_USER=elasticsearch\nES_GROUP=elasticsearch\n\n# Directory where the Elasticsearch binary distribution resides\nES_HOME=\n\n# Directory containing Java\nES_JAVA_HOME=$ES_HOME/jdk\n\n# Additional Java OPTS\n#ES_JAVA_OPTS=\n\n# Maximum number of open files\n#MAX_OPEN_FILES=65536\n\n# Maximum amount of locked memory\n#MAX_LOCKED_MEMORY=\n\n# Elasticsearch log directory\nLOG_DIR=/var/log/$NAME\n\n# Elasticsearch data directory\nDATA_DIR=/var/lib/$NAME\n\n# Elasticsearch configuration directory\nES_PATH_CONF=/etc/$NAME\n\n# Maximum number of VMA (Virtual Memory Areas) a process can own\n#MAX_MAP_COUNT=262144\n\n# Elasticsearch PID file directory\nPID_DIR=\"/var/run/elasticsearch\"\n\n# End of variables that can be overwritten in $DEFAULT\n\n# overwrite settings from default file\nif [ -f \"$DEFAULT\" ]; then\n    . \"$DEFAULT\"\nfi\n\n# Define other required variables\nPID_FILE=\"$PID_DIR/$NAME.pid\"\nDAEMON=$ES_HOME/bin/elasticsearch\nDAEMON_OPTS=\"-d -p $PID_FILE -Epath.logs=$LOG_DIR -Epath.data=$DATA_DIR\"\nDAEMON_ENV_VARS=\"ES_PATH_CONF=$ES_PATH_CONF\"\n\nexport ES_JAVA_OPTS\nexport JAVA_HOME\nexport ES_INCLUDE\nexport ES_JVM_OPTIONS\n\nif [ ! -x \"$DAEMON\" ]; then\n    echo \"The elasticsearch startup script does not exists or it is not executable, tried: $DAEMON\"\n    exit 1\nfi\n\ncheckJava() {\n    if [ -x \"$JAVA_HOME/bin/java\" ]; then\n        JAVA=\"$JAVA_HOME/bin/java\"\n    else\n        JAVA=`which java`\n    fi\n\n    if [ ! -x \"$JAVA\" ]; then\n        echo \"Could not find any executable java binary. Please install java in your PATH or set JAVA_HOME\"\n        exit 1\n    fi\n}\n\ncase \"$1\" in\n  start)\n        checkJava\n\n        log_daemon_msg \"Starting $DESC\"\n\n        pid=`pidofproc -p $PID_FILE elasticsearch`\n        if [ -n \"$pid\" ] ; then\n            log_begin_msg \"Already running.\"\n            log_end_msg 0\n            exit 0\n        fi\n\n        # Ensure that the PID_DIR exists (it is cleaned at OS startup time)\n        if [ -n \"$PID_DIR\" ] && [ ! -e \"$PID_DIR\" ]; then\n            mkdir -p \"$PID_DIR\" && chown \"$ES_USER\":\"$ES_GROUP\" \"$PID_DIR\"\n        fi\n        if [ -n \"$PID_FILE\" ] && [ ! -e \"$PID_FILE\" ]; then\n            touch \"$PID_FILE\" && chown \"$ES_USER\":\"$ES_GROUP\" \"$PID_FILE\"\n        fi\n\n        if [ -n \"$MAX_OPEN_FILES\" ]; then\n            ulimit -n $MAX_OPEN_FILES\n        fi\n\n        if [ -n \"$MAX_LOCKED_MEMORY\" ]; then\n            ulimit -l $MAX_LOCKED_MEMORY\n        fi\n\n        if [ -n \"$MAX_MAP_COUNT\" ]; then\n            sysctl -q -w vm.max_map_count=$MAX_MAP_COUNT\n        fi\n\n        # Start Daemon\n        start-stop-daemon -d $ES_HOME --start --user \"$ES_USER\" -c \"$ES_USER\" --pidfile \"$PID_FILE\" \\\n            --exec /usr/bin/env $DAEMON_ENV_VARS $DAEMON -- $DAEMON_OPTS\n        return=$?\n        if [ $return -eq 0 ]; then\n            i=0\n            timeout=10\n            # Wait for the process to be properly started before exiting\n            until { kill -0 `cat \"$PID_FILE\"`; } >/dev/null 2>&1\n            do\n                sleep 1\n                i=$(($i + 1))\n                if [ $i -gt $timeout ]; then\n                    log_end_msg 1\n                    exit 1\n                fi\n                done\n        fi\n        log_end_msg $return\n        exit $return\n        ;;\n  stop)\n        log_daemon_msg \"Stopping $DESC\"\n\n        if [ -f \"$PID_FILE\" ]; then\n            start-stop-daemon --stop --pidfile \"$PID_FILE\" \\\n                --user \"$ES_USER\" \\\n                --quiet \\\n                --retry TERM/60/KILL/5 > /dev/null\n            if [ $? -eq 1 ]; then\n                log_progress_msg \"$DESC is not running but pid file exists, cleaning up\"\n            elif [ $? -eq 3 ]; then\n                PID=\"`cat $PID_FILE`\"\n                log_failure_msg \"Failed to stop $DESC (pid $PID)\"\n                exit 1\n            fi\n            rm -f \"$PID_FILE\"\n        else\n            log_progress_msg \"(not running)\"\n        fi\n        log_end_msg 0\n        ;;\n  status)\n        status_of_proc -p $PID_FILE elasticsearch elasticsearch && exit 0 || exit $?\n        ;;\n  restart|force-reload)\n        if [ -f \"$PID_FILE\" ]; then\n            $0 stop\n        fi\n        $0 start\n        ;;\n  *)\n        log_success_msg \"Usage: $0 {start|stop|restart|force-reload|status}\"\n        exit 1\n        ;;\nesac\n\nexit 0\n"
        },
        {
          "name": "elasticsearch-logrotate",
          "type": "blob",
          "size": 0.1298828125,
          "content": "/var/log/elasticsearch/*.log {\n    daily\n    rotate 7\n    copytruncate\n    compress\n    delaycompress\n    missingok\n    notifempty\n}\n"
        },
        {
          "name": "elasticsearch.yml",
          "type": "blob",
          "size": 3.009765625,
          "content": "# ======================== Elasticsearch Configuration =========================\n#\n# NOTE: Elasticsearch comes with reasonable defaults for most settings.\n#       Before you set out to tweak and tune the configuration, make sure you\n#       understand what are you trying to accomplish and the consequences.\n#\n# The primary way of configuring a node is via this file. This template lists\n# the most important settings you may want to configure for a production cluster.\n#\n# Please consult the documentation for further information on configuration options:\n# https://www.elastic.co/guide/en/elasticsearch/reference/index.html\n#\n# ---------------------------------- Cluster -----------------------------------\n#\n# Use a descriptive name for your cluster:\n#\n#cluster.name: my-application\n#\n# ------------------------------------ Node ------------------------------------\n#\n# Use a descriptive name for the node:\n#\nnode.name: elk\n#\n# Add custom attributes to the node:\n#\n#node.attr.rack: r1\n#\n# ----------------------------------- Paths ------------------------------------\n#\n# Path to directory where to store the data (separate multiple locations by comma):\n#\n#path.data: /path/to/data\n#\n# Path to log files:\n#\n#path.logs: /path/to/logs\n#\n# Path to snapshots for backups:\n#\npath.repo: /var/backups\n#\n# ----------------------------------- Memory -----------------------------------\n#\n# Lock the memory on startup:\n#\n#bootstrap.memory_lock: true\n#\n# Make sure that the heap size is set to about half the memory available\n# on the system and that the owner of the process is allowed to use this\n# limit.\n#\n# Elasticsearch performs poorly when the system is swapping the memory.\n#\n# ---------------------------------- Network -----------------------------------\n#\n# Set the bind address to a specific IP (IPv4 or IPv6):\n#\n#network.host: 192.168.0.1\nnetwork.host: 0.0.0.0\n#\n# Set a custom port for HTTP:\n#\n#http.port: 9200\n#\n# For more information, consult the network module documentation.\n#\n# --------------------------------- Discovery ----------------------------------\n#\n# Pass an initial list of hosts to perform discovery when this node is started:\n# The default list of hosts is [\"127.0.0.1\", \"[::1]\"]\n#\n#discovery.seed_hosts: [\"127.0.0.1\", \"[::1]\"]\n#\n# Bootstrap the cluster using an initial set of master-eligible nodes:\n#\ncluster.initial_master_nodes: [\"elk\"]\n#\n# For more information, consult the discovery and cluster formation module documentation.\n#\n# ---------------------------------- Gateway -----------------------------------\n#\n# Block initial recovery after a full cluster restart until N nodes are started:\n#\n#gateway.recover_after_nodes: 3\n#\n# For more information, consult the gateway module documentation.\n#\n# ---------------------------------- Various -----------------------------------\n#\n# Require explicit names when deleting indices:\n#\n#action.destructive_requires_name: true\n#\n# ---------------------------------- Security ----------------------------------\n#\n# Enable/disable security (enabled by default since version 8.0)\n#\nxpack.security.enabled: false\n"
        },
        {
          "name": "kibana-init",
          "type": "blob",
          "size": 2.474609375,
          "content": "#!/bin/sh\n#\n# /etc/init.d/kibana5_init -- startup script for kibana5\n# bsmith@the408.com 2015-02-20; used elasticsearch init script as template\n# https://github.com/akabdog/scripts/edit/master/kibana4_init\n# spujadas 2015-04-09; updated to run as non-root user\n#\n### BEGIN INIT INFO\n# Provides:          kibana5_init\n# Required-Start:    $network $remote_fs $named\n# Required-Stop:     $network $remote_fs $named\n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description: Starts kibana5_init\n# Description:       Starts kibana5_init using start-stop-daemon\n### END INIT INFO\n\nKIBANA_HOME=\nKIBANA_BIN=${KIBANA_HOME}/bin\n\nNAME=kibana5\nPID_FILE=/var/run/$NAME.pid\nPATH=/bin:/usr/bin:/sbin:/usr/sbin:$KIBANA_BIN\nDAEMON=$KIBANA_BIN/kibana\nKIBANA_USER=kibana\nKIBANA_GROUP=kibana\nKIBANA_LOG_DIR=/var/log/kibana\nKIBANA_LOG_FILE=\"${KIBANA_LOG_DIR}/$NAME.log\"\nDAEMON_OPTS=\"-l ${KIBANA_LOG_FILE}\"\nDESC=\"Kibana5\"\nNODE_OPTIONS=\"--max-old-space-size=4096\"\n\nif [ $(id -u) -ne 0 ]; then\n  echo \"You need root privileges to run this script\"\n  exit 1\nfi\n\n. /lib/lsb/init-functions\n\nif [ -r /etc/default/rcS ]; then\n  . /etc/default/rcS\nfi\n\ncase \"$1\" in\n  start)\n    log_daemon_msg \"Starting $DESC\"\n\n    pid=$(pidofproc -p $PID_FILE kibana)\n    if [ -n \"$pid\" ]; then\n      log_begin_msg \"Already running.\"\n      log_end_msg 0\n      exit 0\n    fi\n\n    touch ${KIBANA_LOG_FILE}\n    chown ${KIBANA_USER}:${KIBANA_GROUP} ${KIBANA_LOG_FILE}\n\n    # Start Daemon\n    NODE_OPTIONS=\"$NODE_OPTIONS\" start-stop-daemon --start --user $KIBANA_USER -c $KIBANA_USER \\\n      --group $KIBANA_GROUP --pidfile \"$PID_FILE\" --make-pidfile \\\n      --background --exec $DAEMON -- $DAEMON_OPTS\n    log_end_msg $?\n    ;;\n\n  stop)\n    log_daemon_msg \"Stopping $DESC\"\n\n    if [ -f \"$PID_FILE\" ]; then\n      start-stop-daemon --stop --pidfile \"$PID_FILE\" \\\n        --retry TERM/60/KILL/5 >/dev/null\n      if [ $? -eq 1 ]; then\n        log_progress_msg \"$DESC is not running but pid file exists, cleaning up\"\n      elif [ $? -eq 3 ]; then\n        PID=\"$(cat $PID_FILE)\"\n        log_failure_msg \"Failed to stop $DESC (pid $PID)\"\n        exit 1\n      fi\n      rm -f \"$PID_FILE\"\n    else\n      log_progress_msg \"(not running)\"\n    fi\n    log_end_msg 0\n    ;;\n\n  status)\n    status_of_proc -p $PID_FILE kibana kibana && exit 0 || exit $?\n    ;;\n\n  restart|force-reload)\n    if [ -f \"$PID_FILE\" ]; then\n      $0 stop\n      sleep 1\n    fi\n    $0 start\n    ;;\n\n  *)\n    log_success_msg \"Usage: $0 {start|stop|restart|force-reload|status}\"\n    exit 1\n    ;;\nesac\n\nexit 0\n"
        },
        {
          "name": "kibana-logrotate",
          "type": "blob",
          "size": 0.12890625,
          "content": "/var/log/kibana/kibana5.log {\n    daily\n    rotate 7\n    copytruncate\n    compress\n    delaycompress\n    missingok\n    notifempty\n}\n"
        },
        {
          "name": "kibana.yml",
          "type": "blob",
          "size": 7.0048828125,
          "content": "# For more configuration options see the configuration guide for Kibana in\n# https://www.elastic.co/guide/index.html\n\n# =================== System: Kibana Server ===================\n# Kibana is served by a back end server. This setting specifies the port to use.\n#server.port: 5601\n\n# Specifies the address to which the Kibana server will bind. IP addresses and host names are both valid values.\n# The default is 'localhost', which usually means remote machines will not be able to connect.\n# To allow connections from remote users, set this parameter to a non-loopback address.\nserver.host: \"0.0.0.0\"\n\n# Enables you to specify a path to mount Kibana at if you are running behind a proxy.\n# Use the `server.rewriteBasePath` setting to tell Kibana if it should remove the basePath\n# from requests it receives, and to prevent a deprecation warning at startup.\n# This setting cannot end in a slash.\n#server.basePath: \"\"\n\n# Specifies whether Kibana should rewrite requests that are prefixed with\n# `server.basePath` or require that they are rewritten by your reverse proxy.\n# Defaults to `false`.\n#server.rewriteBasePath: false\n\n# Specifies the public URL at which Kibana is available for end users. If\n# `server.basePath` is configured this URL should end with the same basePath.\n#server.publicBaseUrl: \"\"\n\n# The maximum payload size in bytes for incoming server requests.\n#server.maxPayload: 1048576\n\n# The Kibana server's name. This is used for display purposes.\n#server.name: \"your-hostname\"\n\n# =================== System: Kibana Server (Optional) ===================\n# Enables SSL and paths to the PEM-format SSL certificate and SSL key files, respectively.\n# These settings enable SSL for outgoing requests from the Kibana server to the browser.\n#server.ssl.enabled: false\n#server.ssl.certificate: /path/to/your/server.crt\n#server.ssl.key: /path/to/your/server.key\n\n# =================== System: Elasticsearch ===================\n# The URLs of the Elasticsearch instances to use for all your queries.\n#elasticsearch.hosts: [\"http://localhost:9200\"]\n\n# If your Elasticsearch is protected with basic authentication, these settings provide\n# the username and password that the Kibana server uses to perform maintenance on the Kibana\n# index at startup. Your Kibana users still need to authenticate with Elasticsearch, which\n# is proxied through the Kibana server.\n#elasticsearch.username: \"kibana_system\"\n#elasticsearch.password: \"pass\"\n\n# Kibana can also authenticate to Elasticsearch via \"service account tokens\".\n# Service account tokens are Bearer style tokens that replace the traditional username/password based configuration.\n# Use this token instead of a username/password.\n# elasticsearch.serviceAccountToken: \"my_token\"\n\n# Time in milliseconds to wait for Elasticsearch to respond to pings. Defaults to the value of\n# the elasticsearch.requestTimeout setting.\n#elasticsearch.pingTimeout: 1500\n\n# Time in milliseconds to wait for responses from the back end or Elasticsearch. This value\n# must be a positive integer.\n#elasticsearch.requestTimeout: 30000\n\n# List of Kibana client-side headers to send to Elasticsearch. To send *no* client-side\n# headers, set this value to [] (an empty list).\n#elasticsearch.requestHeadersWhitelist: [ authorization ]\n\n# Header names and values that are sent to Elasticsearch. Any custom headers cannot be overwritten\n# by client-side headers, regardless of the elasticsearch.requestHeadersWhitelist configuration.\n#elasticsearch.customHeaders: {}\n\n# Time in milliseconds for Elasticsearch to wait for responses from shards. Set to 0 to disable.\n#elasticsearch.shardTimeout: 30000\n\n# =================== System: Elasticsearch (Optional) ===================\n# These files are used to verify the identity of Kibana to Elasticsearch and are required when\n# xpack.security.http.ssl.client_authentication in Elasticsearch is set to required.\n#elasticsearch.ssl.certificate: /path/to/your/client.crt\n#elasticsearch.ssl.key: /path/to/your/client.key\n\n# Enables you to specify a path to the PEM file for the certificate\n# authority for your Elasticsearch instance.\n#elasticsearch.ssl.certificateAuthorities: [ \"/path/to/your/CA.pem\" ]\n\n# To disregard the validity of SSL certificates, change this setting's value to 'none'.\n#elasticsearch.ssl.verificationMode: full\n\n# =================== System: Logging ===================\n# Set the value of this setting to off to suppress all logging output, or to debug to log everything. Defaults to 'error'\n#logging.root.level: debug\n\n# Enables you to specify a file where Kibana stores log output.\n#logging.appenders.default:\n#  type: file\n#  fileName: /var/logs/kibana.log\n#  layout:\n#    type: json\n\n# Logs queries sent to Elasticsearch.\n#logging.loggers:\n#  - name: elasticsearch.query\n#    level: debug\n\n# Logs http responses.\n#logging.loggers:\n#  - name: http.server.response\n#    level: debug\n\n# Logs system usage information.\n#logging.loggers:\n#  - name: metrics.ops\n#    level: debug\n\n# =================== System: Other ===================\n# The path where Kibana stores persistent data not saved in Elasticsearch. Defaults to data\n#path.data: data\n\n# Specifies the path where Kibana creates the process ID file.\n#pid.file: /run/kibana/kibana.pid\n\n# Set the interval in milliseconds to sample system and process performance\n# metrics. Minimum is 100ms. Defaults to 5000.\n#ops.interval: 5000\n\n# Specifies locale to be used for all localizable strings, dates and number formats.\n# Supported languages are the following: English - en , by default , Chinese - zh-CN .\n#i18n.locale: \"en\"\n\n# =================== Frequently used (Optional)===================\n\n# =================== Saved Objects: Migrations ===================\n# Saved object migrations run at startup. If you run into migration-related issues, you might need to adjust these settings.\n\n# The number of documents migrated at a time.\n# If Kibana can't start up or upgrade due to an Elasticsearch `circuit_breaking_exception`,\n# use a smaller batchSize value to reduce the memory pressure. Defaults to 1000\n# migrations.batchSize: 1000\n\n# The maximum payload size for indexing batches of upgraded saved objects.\n# To avoid migrations failing due to a 413 Request Entity Too Large response from Elasticsearch.\n# This value should be lower than or equal to your Elasticsearch clusterâs `http.max_content_length`\n# configuration option. Default: 100mb\n# migrations.maxBatchSizeBytes: 100mb\n\n# The number of times to retry temporary migration failures. Increase the setting\n# if migrations fail frequently with a message such as `Unable to complete the [...] step after\n# 15 attempts, terminating`. Defaults to 15\n# migrations.retryAttempts: 15\n\n# =================== Search Autocomplete ===================\n# Time in milliseconds to wait for autocomplete suggestions from Elasticsearch.\n# This value must be a whole number greater than zero. Defaults to 1000\n# data.autocomplete.valueSuggestions.timeout: 1000\n\n# Maximum number of documents loaded by each shard to generate autocomplete suggestions.\n# This value must be a whole number greater than zero. Defaults to 100000\n# data.autocomplete.valueSuggestions.terminateAfter: 100000\n"
        },
        {
          "name": "logstash-beats.crt",
          "type": "blob",
          "size": 1.048828125,
          "content": "-----BEGIN CERTIFICATE-----\nMIIC6zCCAdOgAwIBAgIJANPZwuf+5wTLMA0GCSqGSIb3DQEBCwUAMAwxCjAIBgNV\nBAMMASowHhcNMTUxMjI4MTA0NTMyWhcNMjUxMjI1MTA0NTMyWjAMMQowCAYDVQQD\nDAEqMIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAp+jHFvhyYKiPXc7k\n0c33f2QV+1hHNyW/uwcJbp5jG82cuQ41v70Z1+b2veBW4sUlDY3yAIEOPSUD8ASt\n9m72CAo4xlwYKDvm/Sa3KJtDk0NrQiz6PPyBUFsY+Bj3xn6Nz1RW5YaP+Q1Hjnks\nPEyQu4vLgfTSGYBHLD4gvs8wDWY7aaKf8DfuP7Ov74Qlj2GOxnmiDEF4tirlko0r\nqQcvBgujCqA7rNoG+QDmkn3VrxtX8mKF72bxQ7USCyoxD4cWV2mU2HD2Maed3KHj\nKAvDAzSyBMjI+qi9IlPN5MR7rVqUV0VlSKXBVPct6NG7x4WRwnoKjTXnr3CRADD0\n4uvbQQIDAQABo1AwTjAdBgNVHQ4EFgQUVFurgDwdcgnCYxszc0dWMWhB3DswHwYD\nVR0jBBgwFoAUVFurgDwdcgnCYxszc0dWMWhB3DswDAYDVR0TBAUwAwEB/zANBgkq\nhkiG9w0BAQsFAAOCAQEAaLSytepMb5LXzOPr9OiuZjTk21a2C84k96f4uqGqKV/s\nokTTKD0NdeY/IUIINMq4/ERiqn6YDgPgHIYvQheWqnJ8ir69ODcYCpsMXIPau1ow\nT8c108BEHqBMEjkOQ5LrEjyvLa/29qJ5JsSSiULHvS917nVgY6xhcnRZ0AhuJkiI\nARKXwpO5tqJi6BtgzX/3VDSOgVZbvX1uX51Fe9gWwPDgipnYaE/t9TGzJEhKwSah\nkNr+7RM+Glsv9rx1KcWcx4xxY3basG3/KwvsGAFPvk5tXbZ780VuNFTTZw7q3p8O\nGk1zQUBOie0naS0afype5qFMPp586SF/2xAeb68gLg==\n-----END CERTIFICATE-----\n"
        },
        {
          "name": "logstash-beats.key",
          "type": "blob",
          "size": 1.6640625,
          "content": "-----BEGIN PRIVATE KEY-----\nMIIEvAIBADANBgkqhkiG9w0BAQEFAASCBKYwggSiAgEAAoIBAQCn6McW+HJgqI9d\nzuTRzfd/ZBX7WEc3Jb+7BwlunmMbzZy5DjW/vRnX5va94FbixSUNjfIAgQ49JQPw\nBK32bvYICjjGXBgoO+b9Jrcom0OTQ2tCLPo8/IFQWxj4GPfGfo3PVFblho/5DUeO\neSw8TJC7i8uB9NIZgEcsPiC+zzANZjtpop/wN+4/s6/vhCWPYY7GeaIMQXi2KuWS\njSupBy8GC6MKoDus2gb5AOaSfdWvG1fyYoXvZvFDtRILKjEPhxZXaZTYcPYxp53c\noeMoC8MDNLIEyMj6qL0iU83kxHutWpRXRWVIpcFU9y3o0bvHhZHCegqNNeevcJEA\nMPTi69tBAgMBAAECggEAGh1xQZhYqcHtsmhoXF1NfinB5XrAcMpVPLCGfgbyYTOk\niX+1SmIN7++DNtr6iICjF62ZEwz/evET4LPJnsd5SpzUYb2XIELY1Uy9NfqYEwJs\nXzmBnhSjxCy3AHdZqiyqv7FdZot8Pv8avwUHpUU/SXwfpdG/D6pM54uuKh8tWRfp\n6Fun0x3tFLhr1iY/jwxXx+V5zZ1A7AyHnelSv1u7gnxd2WPNJsyoYp7iWkSbdmjr\nfThE8CsTSgL/ndKOmxnPLs7l7ZUipyBgwjmUoZxCZk1I4w9njXY9mti67+6/SAj7\ni26/c7p6H31C+FAqksdKOHWh+zCg8zf1kMW3x3P3MQKBgQDUncHjIZ2rDuoWrOIc\nng6IbuyuSHjUDCs59Z2HQbAVe/0IgDKNspdfddgC5XSN8q5GIWvNQtJjQzuCBRJC\nSxKkncOcTH6J3eQ4e4+sqDbHIagHQwuSBQRYkjd/KN63HEEJoh5UB/r/77CRdOhT\nm6dBm7QvrlXUpcm+z2V3TwvahQKBgQDKK7Qg0mBKm8QVneZUqGRbzaLcOsDefzdP\nIKRhWphAia70z6eoPHgR5MaqCFTPAajFaX6xBzIkPT8g8YUMPHhnw5eJRh/58hbU\n84KBw9jNGjE+H+OTT8+qLicP9EoMOeSVknYIX/zPj7xww0w2mF6tYroKKsiQHZhv\neB16YjqAjQKBgD1BCffm2mbK0DQiMK5v9t3lnziC1pS4wMdc9Lpf+VvnMbn+PRJH\nroapC8eh1ZeDoCPCQy2Kn9RLLVzDG0SQHlngvddMznPnwnVnW7gxaj6qep9E+JNj\n8KGX1ndDDg8RC8e7tiMdfXm401TEqp5TzLcBJcNK5Z1y+hGH7MKXumGFAoGATmkI\n4bn2Url7IY8uKCNvWRO2WIgJCcJ5Zx0X5BJI/q7nxldLhTp+ryH10ziL/AV+uaIi\n2vIZhmiitVo26foCEOyRN1KVUFGOfWU8dqvIyDOiaZ/gmd/YgP6Jc+yhU4CYoVI+\nqRzhZncu9OUqB/qsrb6evRa+1vZDiughNrgmTHkCgYBggzulPfHEBqhlcg6PCsUj\nW6YcxEEPojkPPBsG/aMkGOmCr75I2w81lcjyUv54AiVXpSKqgvs+Zg3nF1WRxMVG\nvevXsCc4wdJPn669J68uh34eHvMBJQ8I00P7tBcW1RjXpvaH/HLUMO51vnfMwBtR\nOrW1ssSF9AvUG1VUmnMSgA==\n-----END PRIVATE KEY-----\n"
        },
        {
          "name": "logstash-conf",
          "type": "tree",
          "content": null
        },
        {
          "name": "logstash-init",
          "type": "blob",
          "size": 3.6435546875,
          "content": "#!/bin/sh\n# Init script for logstash\n# Maintained by Elasticsearch\n# Generated by pleaserun.\n# Implemented based on LSB Core 3.1:\n#   * Sections: 20.2, 20.3\n# spujadas 2015-05-21; emptied LS_HOME (updated by Dockerfile)\n#\n### BEGIN INIT INFO\n# Provides:          logstash\n# Required-Start:    $remote_fs $syslog\n# Required-Stop:     $remote_fs $syslog\n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description:\n# Description:        Starts Logstash as a daemon.\n### END INIT INFO\n\nPATH=/sbin:/usr/sbin:/bin:/usr/bin\nexport PATH\n\nif [ $(id -u) -ne 0 ]; then\n   echo \"You need root privileges to run this script\"\n   exit 1\nfi\n\nname=logstash\npidfile=\"/var/run/$name.pid\"\n\nLS_USER=logstash\nLS_GROUP=logstash\nLS_HOME=\nLS_HEAP_SIZE=\"500m\"\nLS_JAVA_OPTS=\"-Djava.io.tmpdir=${LS_HOME}\"\nLS_LOG_DIR=/var/log/logstash\nLS_LOG_FILE=\"${LS_LOG_DIR}/${name}-plain.log\"\nLS_OPEN_FILES=16384\nLS_NICE=19\nLS_OPTS=\n\n[ -r /etc/default/$name ] && . /etc/default/$name\n[ -r /etc/sysconfig/$name ] && . /etc/sysconfig/$name\n\nprogram=/opt/logstash/bin/logstash\nargs=\"--path.logs ${LS_LOG_DIR} ${LS_OPTS}\"\n\nstart() {\n  HOME=${LS_HOME}\n  \n  ## removing/updating next lines as overriding JAVA_OPTS prevents Logstash\n  ## from starting\n  #JAVA_OPTS=${LS_JAVA_OPTS}\n  #export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING\n  \n  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING\n\n  touch ${LS_LOG_FILE}\n  chown ${LS_USER}:${LS_GROUP} ${LS_LOG_FILE}\n\n  # set ulimit as (root, presumably) first, before we drop privileges\n  ulimit -n ${LS_OPEN_FILES}\n\n  # Run the program!\n  nice -n ${LS_NICE} chroot --userspec $LS_USER:$LS_GROUP / sh -c \"\n    cd $LS_HOME\n    ulimit -n ${LS_OPEN_FILES}\n    exec \\\"$program\\\" $args\n  \" > \"${LS_LOG_DIR}/$name.stdout\" 2> \"${LS_LOG_DIR}/$name.err\" &\n\n  # Generate the pidfile from here. If we instead made the forked process\n  # generate it there will be a race condition between the pidfile writing\n  # and a process possibly asking for status.\n  echo $! > $pidfile\n\n  echo \"$name started.\"\n  return 0\n}\n\nstop() {\n  # Try a few times to kill TERM the program\n  if status; then\n    pid=$(cat \"$pidfile\")\n    echo \"Killing $name (pid $pid) with SIGTERM\"\n    kill -TERM $pid\n    # Wait for it to exit.\n    for i in 1 2 3 4 5; do\n      echo \"Waiting for $name (pid $pid) to die...\"\n      status || break\n      sleep 1\n    done\n    if status; then\n      echo \"$name stop failed; still running.\"\n    else\n      echo \"$name stopped.\"\n      rm -f $pidfile\n    fi\n  fi\n}\n\nstatus() {\n  if [ -f \"$pidfile\" ] ; then\n    pid=$(cat \"$pidfile\")\n    if kill -0 $pid > /dev/null 2> /dev/null; then\n      # process by this pid is running.\n      # It may not be our pid, but that's what you get with just pidfiles.\n      # TODO(sissel): Check if this process seems to be the same as the one we\n      # expect. It'd be nice to use flock here, but flock uses fork, not exec,\n      # so it makes it quite awkward to use in this case.\n      return 0\n    else\n      return 2 # program is dead but pid file exists\n    fi\n  else\n    return 3 # program is not running\n  fi\n}\n\nforce_stop() {\n  if status; then\n    stop\n    status && kill -KILL $(cat \"$pidfile\")\n    rm -f $pidfile\n  fi\n}\n\n\ncase \"$1\" in\n  start)\n    status\n    code=$?\n    if [ $code -eq 0 ]; then\n      echo \"$name is already running\"\n    else\n      start\n      code=$?\n    fi\n    exit $code\n    ;;\n\n  stop) stop ;;\n\n  force-stop) force_stop ;;\n\n  status)\n    status\n    code=$?\n    if [ $code -eq 0 ]; then\n      echo \"$name is running\"\n    else\n      echo \"$name is not running\"\n    fi\n    exit $code\n    ;;\n\n  restart) stop && start ;;\n\n  *)\n    echo \"Usage: $SCRIPTNAME {start|stop|force-stop|status|restart}\" >&2\n    exit 3\n    ;;\nesac\n\nexit $?\n"
        },
        {
          "name": "logstash-logrotate",
          "type": "blob",
          "size": 0.1748046875,
          "content": "/var/log/logstash/*.err /var/log/logstash/*.log /var/log/logstash/*.stdout {\n    daily\n    rotate 7\n    copytruncate\n    compress\n    delaycompress\n    missingok\n    notifempty\n}\n"
        },
        {
          "name": "nginx-filebeat",
          "type": "tree",
          "content": null
        },
        {
          "name": "nginx.pattern",
          "type": "blob",
          "size": 0.3046875,
          "content": "NGUSERNAME [a-zA-Z\\.\\@\\-\\+_%]+\nNGUSER %{NGUSERNAME}\nNGINXACCESS %{IPORHOST:clientip} %{NGUSER:ident} %{NGUSER:auth} \\[%{HTTPDATE:timestamp}\\] \"%{WORD:verb} %{URIPATHPARAM:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:%{NUMBER:bytes:int}|-) (?:\"(?:%{URI:referrer}|-)\"|%{QS:referrer}) %{QS:agent}\n"
        },
        {
          "name": "pipelines.yml",
          "type": "blob",
          "size": 0.2783203125,
          "content": "# This file is where you define your pipelines. You can define multiple.\n# For more information on multiple pipelines, see the documentation:\n#   https://www.elastic.co/guide/en/logstash/current/multiple-pipelines.html\n\n- pipeline.id: main\n  path.config: \"/etc/logstash/conf.d/*.conf\"\n"
        },
        {
          "name": "start.sh",
          "type": "blob",
          "size": 9.0634765625,
          "content": "#!/bin/bash\n#\n# /usr/local/bin/start.sh\n# Start Elasticsearch, Logstash and Kibana services\n\n# WARNING - This script assumes that the ELK services are not running, and is\n#   only expected to be run once, when the container is started.\n#   Do not attempt to run this script if the ELK services are running (or be\n#   prepared to reap zombie processes).\n\n\n## handle termination gracefully\n\n_term() {\n  echo \"Terminating ELK\"\n\n  # shut down services\n  timeout 60 service elasticsearch stop &\n  timeout 60 service logstash stop &\n  timeout 60 service kibana stop &\n  trap - SIGTERM SIGINT\n\n  # wait for all services to stop\n  wait\n\n  # kill script PGID so all the child processes are killed, to avoid zombies\n  kill -TERM -- -$$ 2>/dev/null\n  exit 0\n}\n\ntrap _term SIGTERM SIGINT\n\n\n## remove pidfiles in case previous graceful termination failed\n# NOTE - This is the reason for the WARNING at the top - it's a bit hackish,\n#   but if it's good enough for Fedora (https://goo.gl/88eyXJ), it's good\n#   enough for me :)\n\nrm -f /var/run/elasticsearch/elasticsearch.pid /var/run/logstash.pid \\\n  /var/run/kibana5.pid\n\n## initialise list of log files to stream in console (initially empty)\nOUTPUT_LOGFILES=\"\"\n\n\n## override default time zone (Etc/UTC) if TZ variable is set\nif [ ! -z \"$TZ\" ]; then\n  ln -snf /usr/share/zoneinfo/$TZ /etc/localtime && echo $TZ > /etc/timezone\nfi\n\n\n## run pre-hooks\nif [ -x /usr/local/bin/elk-pre-hooks.sh ]; then\n  . /usr/local/bin/elk-pre-hooks.sh\nfi\n\n\n## start services as needed\n\n### crond\n\nservice cron start\n\n\n### Elasticsearch\n\nif [ -z \"$ELASTICSEARCH_START\" ]; then\n  ELASTICSEARCH_START=1\nfi\nif [ \"$ELASTICSEARCH_START\" -ne \"1\" ]; then\n  echo \"ELASTICSEARCH_START is set to something different from 1, not starting...\"\nelse\n  # update permissions of ES data directory\n  chown -R elasticsearch:elasticsearch /var/lib/elasticsearch\n\n  # override JVM heap size using custom JVM options file if ES_HEAP_SIZE variable is set\n  if [ ! -z \"$ES_HEAP_SIZE\" ]; then\n    cat << EOF > ${ES_PATH_CONF}/jvm.options.d/heap_size.options\n-Xmx$ES_HEAP_SIZE\n-Xms$ES_HEAP_SIZE\nEOF\n  fi\n\n  # disable JVM HeapDumpOnOutOfMemoryError if ES_HEAP_DISABLE variable is set\n  if [ ! -z \"$ES_HEAP_DISABLE\" ]; then\n    echo \"-XX:-HeapDumpOnOutOfMemoryError\" > ${ES_PATH_CONF}/jvm.options.d/heap_dump.options\n  fi\n\n  # override ES_JAVA_OPTS variable if set\n  if [ ! -z \"$ES_JAVA_OPTS\" ]; then\n    awk -v LINE=\"ES_JAVA_OPTS=\\\"$ES_JAVA_OPTS\\\"\" '{ sub(/^#?ES_JAVA_OPTS=.*/, LINE); print; }' /etc/default/elasticsearch \\\n        > /etc/default/elasticsearch.new && mv /etc/default/elasticsearch.new /etc/default/elasticsearch\n  fi\n\n  # override MAX_OPEN_FILES variable if set\n  if [ ! -z \"$MAX_OPEN_FILES\" ]; then\n    awk -v LINE=\"MAX_OPEN_FILES=$MAX_OPEN_FILES\" '{ sub(/^#?MAX_OPEN_FILES=.*/, LINE); print; }' /etc/init.d/elasticsearch \\\n        > /etc/init.d/elasticsearch.new && mv /etc/init.d/elasticsearch.new /etc/init.d/elasticsearch \\\n        && chmod +x /etc/init.d/elasticsearch\n  fi\n\n  # override MAX_MAP_COUNT variable if set\n  if [ ! -z \"$MAX_MAP_COUNT\" ]; then\n    awk -v LINE=\"MAX_MAP_COUNT=$MAX_MAP_COUNT\" '{ sub(/^#?MAX_MAP_COUNT=.*/, LINE); print; }' /etc/init.d/elasticsearch \\\n        > /etc/init.d/elasticsearch.new && mv /etc/init.d/elasticsearch.new /etc/init.d/elasticsearch \\\n        && chmod +x /etc/init.d/elasticsearch\n  fi\n\n  service elasticsearch start\n\n  # wait for Elasticsearch to start up before either starting Kibana (if enabled)\n  # or attempting to stream its log file\n  # - https://github.com/elasticsearch/kibana/issues/3077\n\n  # set number of retries (default: 30, override using ES_CONNECT_RETRY env var)\n  re_is_numeric='^[0-9]+$'\n  if ! [[ $ES_CONNECT_RETRY =~ $re_is_numeric ]] ; then\n     ES_CONNECT_RETRY=30\n  fi\n\n  if [ -z \"$ELASTICSEARCH_URL\" ]; then\n    ELASTICSEARCH_URL=${ES_PROTOCOL:-http}://localhost:9200\n  fi\n\n  counter=0\n  while [ ! \"$(curl -k ${ELASTICSEARCH_URL} 2> /dev/null)\" -a $counter -lt $ES_CONNECT_RETRY  ]; do\n    sleep 1\n    ((counter++))\n    echo \"waiting for Elasticsearch to be up ($counter/$ES_CONNECT_RETRY)\"\n  done\n  if [ ! \"$(curl -k ${ELASTICSEARCH_URL} 2> /dev/null)\" ]; then\n    echo \"Couldn't start Elasticsearch. Exiting.\"\n    echo \"Elasticsearch log follows below.\"\n    cat /var/log/elasticsearch/elasticsearch.log\n    exit 1\n  fi\n\n  # wait for cluster to respond before getting its name\n  counter=0\n  while [ -z \"$CLUSTER_NAME\" -a $counter -lt 30 ]; do\n    sleep 1\n    ((counter++))\n    CLUSTER_NAME=$(curl -k ${ELASTICSEARCH_URL}/_cat/health?h=cluster 2> /dev/null | tr -d '[:space:]')\n    echo \"Waiting for Elasticsearch cluster to respond ($counter/30)\"\n  done\n\n  if [ -z \"$CLUSTER_NAME\" ]; then\n    echo \"Couldn't get name of cluster. Exiting.\"\n    echo \"Elasticsearch log follows.\"\n    cat /var/log/elasticsearch/elasticsearch.log\n    exit 1\n  elif [[ \"$CLUSTER_NAME\" =~ \"master_not_discovered_exception\" ]]; then\n    # If we got a JSON error back, don't treat it like the literal name of the cluster.\n    # Example of what this error looks like:\n    # [{\"error\":{\"root_cause\":[{\"type\":\"master_not_discovered_exception\",\"reason\":null}]\n    # We don't know the cluster name, so we'll just glob it.\n    echo \"Failed to contact a healthy master in cluster.\"\n    echo \"Elasticsearch logs follow.\"\n    cat /var/log/elasticsearch/*.log\n    exit 1\n  fi\n  OUTPUT_LOGFILES+=\"/var/log/elasticsearch/${CLUSTER_NAME}.log \"\nfi\n\n\n### Logstash\n\nif [ -z \"$LOGSTASH_START\" ]; then\n  LOGSTASH_START=1\nfi\nif [ \"$LOGSTASH_START\" -ne \"1\" ]; then\n  echo \"LOGSTASH_START is set to something different from 1, not starting...\"\nelse\n  # override LS_HEAP_SIZE variable if set\n  if [ ! -z \"$LS_HEAP_SIZE\" ]; then\n    awk -v LINE=\"-Xmx$LS_HEAP_SIZE\" '{ sub(/^.Xmx.*/, LINE); print; }' ${LOGSTASH_PATH_SETTINGS}/jvm.options \\\n        > ${LOGSTASH_PATH_SETTINGS}/jvm.options.new && mv ${LOGSTASH_PATH_SETTINGS}/jvm.options.new ${LOGSTASH_PATH_SETTINGS}/jvm.options\n    awk -v LINE=\"-Xms$LS_HEAP_SIZE\" '{ sub(/^.Xms.*/, LINE); print; }' ${LOGSTASH_PATH_SETTINGS}/jvm.options \\\n        > ${LOGSTASH_PATH_SETTINGS}/jvm.options.new && mv ${LOGSTASH_PATH_SETTINGS}/jvm.options.new ${LOGSTASH_PATH_SETTINGS}/jvm.options\n  fi\n\n  if [ ! -z \"$LS_HEAP_DISABLE\" ]; then\n    awk -v LINE=\"#-XX:+HeapDumpOnOutOfMemoryError\" '{ sub(/^-XX:\\+HeapDumpOnOutOfMemoryError.*/, LINE); print; }' ${LOGSTASH_PATH_SETTINGS}/jvm.options \\\n        > ${LOGSTASH_PATH_SETTINGS}/jvm.options.new && mv ${LOGSTASH_PATH_SETTINGS}/jvm.options.new ${LOGSTASH_PATH_SETTINGS}/jvm.options\n  fi\n\n  # override LS_OPTS variable if set\n  if [ ! -z \"$LS_OPTS\" ]; then\n    awk -v LINE=\"LS_OPTS=\\\"$LS_OPTS\\\"\" '{ sub(/^LS_OPTS=.*/, LINE); print; }' /etc/init.d/logstash \\\n        > /etc/init.d/logstash.new && mv /etc/init.d/logstash.new /etc/init.d/logstash && chmod +x /etc/init.d/logstash\n  fi\n\n  service logstash start\n  OUTPUT_LOGFILES+=\"/var/log/logstash/logstash-plain.log \"\nfi\n\n\n### Kibana\n\nif [ -z \"$KIBANA_START\" ]; then\n  KIBANA_START=1\nfi\nif [ \"$KIBANA_START\" -ne \"1\" ]; then\n  echo \"KIBANA_START is set to something different from 1, not starting...\"\nelse\n  # override NODE_OPTIONS variable if set\n  if [ ! -z \"$NODE_OPTIONS\" ]; then\n    awk -v LINE=\"NODE_OPTIONS=\\\"$NODE_OPTIONS\\\"\" '{ sub(/^NODE_OPTIONS=.*/, LINE); print; }' /etc/init.d/kibana \\\n        > /etc/init.d/kibana.new && mv /etc/init.d/kibana.new /etc/init.d/kibana && chmod +x /etc/init.d/kibana\n  fi\n\n  service kibana start\n  OUTPUT_LOGFILES+=\"/var/log/kibana/kibana5.log \"\nfi\n\n# Exit if nothing has been started\nif [ \"$ELASTICSEARCH_START\" -ne \"1\" ] && [ \"$LOGSTASH_START\" -ne \"1\" ] \\\n  && [ \"$KIBANA_START\" -ne \"1\" ]; then\n  >&2 echo \"No services started. Exiting.\"\n  exit 1\nfi\n\n\n## run post-hooks\nif [ -x /usr/local/bin/elk-post-hooks.sh ]; then\n  ### if Kibana was started...\n  if [ \"$KIBANA_START\" -eq \"1\" ]; then\n\n  ### ... then wait for Kibana to be up first to ensure that .kibana index is\n  ### created before the post-hooks are executed\n    # set number of retries (default: 30, override using KIBANA_CONNECT_RETRY env var)\n    if ! [[ $KIBANA_CONNECT_RETRY =~ $re_is_numeric ]] ; then\n       KIBANA_CONNECT_RETRY=30\n    fi\n\n    if [ -z \"$KIBANA_URL\" ]; then\n      KIBANA_URL=http://localhost:5601\n    fi\n\n    counter=0\n    while [ ! \"$(curl ${KIBANA_URL} 2> /dev/null)\" -a $counter -lt $KIBANA_CONNECT_RETRY  ]; do\n      sleep 1\n      ((counter++))\n      echo \"waiting for Kibana to be up ($counter/$KIBANA_CONNECT_RETRY)\"\n    done\n    if [ ! \"$(curl ${KIBANA_URL} 2> /dev/null)\" ]; then\n      echo \"Couldn't start Kibana. Exiting.\"\n      echo \"Kibana log follows below.\"\n      cat /var/log/kibana/kibana5.log\n      exit 1\n    fi\n    # wait for Kibana to not only be up but to return 200 OK\n    counter=0\n    while [[ \"$(curl -s -o /dev/null -w ''%{http_code}'' ${KIBANA_URL}/api/status)\" != \"200\" && $counter -lt 30 ]]; do\n      sleep 1\n      ((counter++))\n      echo \"waiting for Kibana to respond ($counter/30)\"\n    done\n    if [[ \"$(curl -s -o /dev/null -w ''%{http_code}'' ${KIBANA_URL}/api/status)\" != \"200\" ]]; then\n      echo \"Timed out waiting for Kibana to respond. Exiting.\"\n      echo \"Kibana log follows below.\"\n      cat /var/log/kibana/kibana5.log\n      exit 1\n    fi\n  fi\n\n  . /usr/local/bin/elk-post-hooks.sh\nfi\n\n\ntouch $OUTPUT_LOGFILES\ntail -f $OUTPUT_LOGFILES &\nwait\n"
        }
      ]
    }
  ]
}