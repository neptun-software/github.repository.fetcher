{
  "metadata": {
    "timestamp": 1736568820417,
    "page": 984,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk4OQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lework/kainstall",
      "stars": 1026,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".tgitconfig",
          "type": "blob",
          "size": 0.064453125,
          "content": "[hook \"precommit\"]\n\tcmdline = chmod.bat\n\twait = true\n\tshow = true\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0380859375,
          "content": "MIT License\n\nCopyright (c) 2020 Lework\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.9365234375,
          "content": "# kainstall  =  kubeadm install kubernetes\n\n[![GitHub Super-Linter](https://github.com/lework/kainstall/workflows/Lint%20Code%20Base/badge.svg)](https://github.com/marketplace/actions/super-linter)\n\n使用 shell 脚本, 基于 kubeadm 一键部署 kubernetes HA 集群\n\n## 为什么\n\n**为什么要搞这个？Ansible PlayBook 不好么？**\n\n**因为懒**，Ansible PlayBook 编排是非常给力的，不过需要安装 Python 和 Ansible, 且需要下载多个 yaml 文件 。**因为懒**，我想要个更简单的方式来**快速部署**一个分布式的 **Kubernetes HA** 集群， 使用 **shell** 脚本可以不借助外力直接在服务器上运行，省时省力。 并且 shell 脚本只有一个文件，文件大小**100 KB 左右**，非常小巧，可以实现一条命令安装集群的超快体验，而且配合**离线安装包**，可以在不联网的环境下安装集群，这体验真的**非常爽**啊。\n\n## 要求\n\nOS: `centos 7.x x64` , `centos 8.x x64`,  `debian 9.x x64` , `debian 10.x x64`, `ubuntu 20.04 x64`, `ubuntu 20.10 x64`, `ubuntu 21.04 x64`\n\nCPU: `2C`\n\nMEM: `4G`\n\n认证: 集群节点需**统一认证**; 使用密码认证时，集群节点需使用同一用户名和密码，使用密钥认证时，集群节点需使用同一个密钥文件登陆。\n\n> 未指定离线包时，需要连通外网，用于下载 kube 组件和 docker 镜像。\n\n## 架构\n\n![k8s-node-ha](./images/k8s-node-ha.png)\n\n> 如需按照步骤安装集群，可参考 [https://lework.github.io/2019/10/01/kubeadm-install/](https://lework.github.io/2019/10/01/kubeadm-install/)\n\n## 功能\n\n- 服务器初始化。\n  - 关闭 `selinux`\n  - 关闭 `swap`\n  - 关闭 `firewalld`\n  - 配置 `epel` 源\n  - 修改 `limits`\n  - 配置内核参数\n  - 配置 `history` 记录\n  - 配置 `journal` 日志\n  - 配置 `chrony` 时间同步\n  - 添加 `ssh-login-info` 信息\n  - 配置 `audit` 审计\n  - 安装 `ipvs` 模块\n  - 更新内核\n- 安装`kube`组件。\n- 初始化`kubernetes`集群,以及增加或删除节点。\n- 安装`ingress`组件，可选`nginx`，`traefik`。\n- 安装`network`组件，可选`flannel`，`calico`，`cilium`。\n- 安装`monitor`组件，可选`prometheus`。\n- 安装`log`组件，可选`elasticsearch`。\n- 安装`storage`组件，可选`rook`，`longhorn`。\n- 安装`web ui`组件，可选`dashboard`, `kubesphere`。\n- 安装`addon`组件，可选`metrics-server`, `nodelocaldns`。\n- 安装`cri`组件，可选`docker`, `containerd`, `cri-o`\n- 升级到`kubernetes`指定版本。\n- 更新集群证书。\n- 添加运维操作，如备份etcd快照。\n- 支持**离线部署**。\n- 支持**sudo特权**。\n- 支持**10年证书期限**。\n- 支持脚本更新。\n\n## 默认版本\n\n| 分类                                           | 软件                                             | kainstall 默认版本 | 软件最新版本                                                 |\n| ------------------------------------------------ | ------------------ | ------------------------------------------------------------ | ------------------------------------------------ |\n| common | [containerd](https://github.com/containerd/containerd) | latest             | ![docker-ce release](https://img.shields.io/github/v/release/containerd/containerd?sort=semver) |\n| common | [kubernetes](https://github.com/kubernetes/kubernetes) | latest             | ![kubernetes release](https://img.shields.io/github/v/release/kubernetes/kubernetes?sort=semver) |\n| network | [flannel](https://github.com/coreos/flannel) | 0.24.0         | ![flannel release](https://img.shields.io/github/v/release/coreos/flannel) |\n| network | [calico](https://github.com/projectcalico/calico) | 3.27.0 | ![calico release ](https://img.shields.io/github/v/release/projectcalico/calico?sort=semver) |\n| network | [cilium](https://github.com/cilium/cilium) | 1.14.5 | ![cilium release ](https://img.shields.io/github/v/release/cilium/cilium?sort=semver) |\n| addons | [metrics server](https://github.com/kubernetes-sigs/metrics-server) | 0.6.4             | ![metrics-server release](https://img.shields.io/github/v/release/kubernetes-sigs/metrics-server) |\n| addons | [nodelocaldns](https://github.com/kubernetes/dns/tree/master/cmd/node-cache) | latest           | 1.22.28                                                      |\n| ingress | [ingress nginx controller](https://github.com/kubernetes/ingress-nginx) | 1.9.5        | ![ingress-nginx release](https://img.shields.io/github/v/release/kubernetes/ingress-nginx) |\n| ingress | [traefik](https://github.com/traefik/traefik) | 2.10.7      | ![traefik release ](https://img.shields.io/github/v/release/traefik/traefik?sort=semver) |\n| monitor | [kube_prometheus](https://github.com/prometheus-operator/kube-prometheus) | 0.13.0             | ![kube-prometheus release](https://img.shields.io/github/v/release/prometheus-operator/kube-prometheus) |\n| log | [elasticsearch](https://github.com/elastic/elasticsearch) | 8.11.3      | ![elasticsearch release](https://img.shields.io/github/v/release/elastic/elasticsearch?sort=semver) |\n| storage | [rook](https://github.com/rook/rook) | 1.13.1 | ![rook release](https://img.shields.io/github/v/release/rook/rook?sort=semver) |\n| storage | [longhorn](https://github.com/longhorn/longhorn) | 1.5.3 | ![longhorn release](https://img.shields.io/github/v/release/longhorn/longhorn?sort=semver) |\n| ui | [kubernetes_dashboard](https://github.com/kubernetes/dashboard) | 2.7.0            | ![kubernetes dashboard release](https://img.shields.io/github/v/release/kubernetes/dashboard?sort=semver) |\n| ui | [kubesphere](https://github.com/kubesphere/kubesphere) | 3.3.0            | ![kubesphere release](https://img.shields.io/github/v/release/kubesphere/kubesphere?sort=semver) |\n\n除 **kube组件** 版本可以通过参数(`--version`) 指定外，其他的软件版本需在脚本中指定。\n\n## 使用\n\n> 案例使用请见：[https://lework.github.io/2020/09/26/kainstall](https://lework.github.io/2020/09/26/kainstall)\n\n### 下载脚本\n\n```bash\n# centos\nwget https://ghproxy.com/https://raw.githubusercontent.com/lework/kainstall/master/kainstall-centos.sh\n\n# debian\nwget https://ghproxy.com/https://raw.githubusercontent.com/lework/kainstall/master/kainstall-debian.sh\n\n# ubuntu\nwget https://ghproxy.com/https://raw.githubusercontent.com/lework/kainstall/master/kainstall-ubuntu.sh\n```\n\n### 帮助信息\n\n```bash\n# bash kainstall-centos.sh\n\n\nInstall kubernetes cluster using kubeadm.\n\nUsage:\n  kainstall-centos.sh [command]\n\nAvailable Commands:\n  init            Init Kubernetes cluster.\n  reset           Reset Kubernetes cluster.\n  add             Add nodes to the cluster.\n  del             Remove node from the cluster.\n  renew-cert      Renew all available certificates.\n  upgrade         Upgrading kubeadm clusters.\n  update          Update script file.\n\nFlag:\n  -m,--master          master node, default: ''\n  -w,--worker          work node, default: ''\n  -u,--user            ssh user, default: root\n  -p,--password        ssh password\n     --private-key     ssh private key\n  -P,--port            ssh port, default: 22\n  -v,--version         kube version, default: latest\n  -n,--network         cluster network, choose: [flannel,calico,cilium], default: flannel\n  -i,--ingress         ingress controller, choose: [nginx,traefik], default: nginx\n  -ui,--ui             cluster web ui, choose: [dashboard,kubesphere], default: dashboard\n  -a,--addon           cluster add-ons, choose: [metrics-server,nodelocaldns], default: metrics-server\n  -M,--monitor         cluster monitor, choose: [prometheus]\n  -l,--log             cluster log, choose: [elasticsearch]\n  -s,--storage         cluster storage, choose: [rook,longhorn]\n     --cri             cri runtime, choose: [docker,containerd,cri-o], default: containerd\n     --cri-version     cri version, default: latest\n     --cri-endpoint    cri endpoint, default: /var/run/dockershim.sock\n  -U,--upgrade-kernel  upgrade kernel\n  -of,--offline-file   specify the offline package file to load\n      --10years        the certificate period is 10 years.\n      --sudo           sudo mode\n      --sudo-user      sudo user\n      --sudo-password  sudo user password\n\nExample:\n  [init cluster]\n  kainstall-centos.sh init \\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\n  --worker 192.168.77.133,192.168.77.134,192.168.77.135 \\\n  --user root \\\n  --password 123456 \\\n  --version 1.20.6\n\n  [reset cluster]\n  kainstall-centos.sh reset \\\n  --user root \\\n  --password 123456\n\n  [add node]\n  kainstall-centos.sh add \\\n  --master 192.168.77.140,192.168.77.141 \\\n  --worker 192.168.77.143,192.168.77.144 \\\n  --user root \\\n  --password 123456 \\\n  --version 1.20.6\n\n  [del node]\n  kainstall-centos.sh del \\\n  --master 192.168.77.140,192.168.77.141 \\\n  --worker 192.168.77.143,192.168.77.144 \\\n  --user root \\\n  --password 123456\n \n  [other]\n  kainstall-centos.sh renew-cert --user root --password 123456\n  kainstall-centos.sh upgrade --version 1.20.6 --user root --password 123456\n  kainstall-centos.sh update\n  kainstall-centos.sh add --ingress traefik\n  kainstall-centos.sh add --monitor prometheus\n  kainstall-centos.sh add --log elasticsearch\n  kainstall-centos.sh add --storage rook\n  kainstall-centos.sh add --ui dashboard\n  kainstall-centos.sh add --addon nodelocaldns\n```\n\n### 初始化集群\n\n```bash\n# 使用脚本参数\nbash kainstall-centos.sh init \\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\n  --worker 192.168.77.133,192.168.77.134 \\\n  --user root \\\n  --password 123456 \\\n  --port 22 \\\n  --version 1.20.6\n\n# 使用环境变量\nexport MASTER_NODES=\"192.168.77.130,192.168.77.131,192.168.77.132\"\nexport WORKER_NODES=\"192.168.77.133,192.168.77.134\"\nexport SSH_USER=\"root\"\nexport SSH_PASSWORD=\"123456\"\nexport SSH_PORT=\"22\"\nexport KUBE_VERSION=\"1.20.6\"\nbash kainstall-centos.sh init\n```\n\n> 默认情况下，除了初始化集群外，还会安装 `ingress: nginx` , `ui: dashboard` 两个组件。\n\n还可以使用一键安装方式, 连下载都省略了。\n\n```bash\nbash -c \"$(curl -sSL https://ghproxy.com/https://raw.githubusercontent.com/lework/kainstall/master/kainstall-centos.sh)\"  \\\n  - init \\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\n  --worker 192.168.77.133,192.168.77.134 \\\n  --user root \\\n  --password 123456 \\\n  --port 22 \\\n  --version 1.20.6\n```\n\n### 增加节点\n\n> 操作需在 k8s master 节点上操作，ssh连接信息非默认时请指定\n\n```bash\n# 增加单个master节点\nbash kainstall-centos.sh add --master 192.168.77.135\n\n# 增加单个worker节点\nbash kainstall-centos.sh add --worker 192.168.77.134\n\n# 同时增加\nbash kainstall-centos.sh add --master 192.168.77.135,192.168.77.136 --worker 192.168.77.137,192.168.77.138\n```\n\n### 删除节点\n\n> 操作需在 k8s master 节点上操作，ssh连接信息非默认时请指定\n\n```bash\n# 删除单个master节点\nbash kainstall-centos.sh del --master 192.168.77.135\n\n# 删除单个worker节点\nbash kainstall-centos.sh del --worker 192.168.77.134\n\n# 同时删除\nbash kainstall-centos.sh del --master 192.168.77.135,192.168.77.136 --worker 192.168.77.137,192.168.77.138\n```\n\n### 重置集群\n\n```bash\nbash kainstall-centos.sh reset \\\n  --user root \\\n  --password 123456 \\\n  --port 22 \\\n```\n\n### 其他操作\n\n> 操作需在 k8s master 节点上操作，ssh连接信息非默认时请指定\n> **注意：** 添加组件时请保持节点的内存和cpu至少为`2C4G`的空闲。否则会导致节点下线且服务器卡死。\n\n```bash\n# 添加 nginx ingress\nbash kainstall-centos.sh add --ingress nginx\n\n# 添加 prometheus\nbash kainstall-centos.sh add --monitor prometheus\n\n# 添加 elasticsearch\nbash kainstall-centos.sh add --log elasticsearch\n\n# 添加 rook\nbash kainstall-centos.sh add --storage rook\n\n# 添加 nodelocaldns\nbash kainstall-centos.sh add --addon nodelocaldns\n\n# 升级版本\nbash kainstall-centos.sh upgrade --version 1.20.6\n\n# 重新颁发证书\nbash kainstall-centos.sh renew-cert\n\n# debug模式\nDEBUG=1 bash kainstall-centos.sh\n\n# 更新脚本\nbash kainstall-centos.sh update\n\n# 使用 cri-o containerd runtime\nbash kainstall-centos.sh init \\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\n  --worker 192.168.77.133,192.168.77.134,192.168.77.135 \\\n  --user root \\\n  --password 123456 \\\n  --cri containerd\n  \n# 使用 cri-o cri runtime\nbash kainstall-centos.sh init \\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\n  --worker 192.168.77.133,192.168.77.134,192.168.77.135 \\\n  --user root \\\n  --password 123456 \\\n  --cri cri-o\n```\n\n### 默认设置\n\n> **注意:** 以下变量都在脚本文件的`environment configuration`部分。可根据需要自行修改，或者为变量设置同名的**环境变量**修改其默认内容。\n\n```bash\n# 版本\nKUBE_VERSION=\"${KUBE_VERSION:-latest}\"\nFLANNEL_VERSION=\"${FLANNEL_VERSION:-0.24.0}\"\nMETRICS_SERVER_VERSION=\"${METRICS_SERVER_VERSION:-0.6.4}\"\nINGRESS_NGINX=\"${INGRESS_NGINX:-1.9.5}\"\nTRAEFIK_VERSION=\"${TRAEFIK_VERSION:-2.10.7}\"\nCALICO_VERSION=\"${CALICO_VERSION:-3.27.0}\"\nCILIUM_VERSION=\"${CILIUM_VERSION:-1.14.5}\"\nKUBE_PROMETHEUS_VERSION=\"${KUBE_PROMETHEUS_VERSION:-0.13.0}\"\nELASTICSEARCH_VERSION=\"${ELASTICSEARCH_VERSION:-8.11.3}\"\nROOK_VERSION=\"${ROOK_VERSION:-1.9.13}\"\nLONGHORN_VERSION=\"${LONGHORN_VERSION:-1.5.3}\"\nKUBERNETES_DASHBOARD_VERSION=\"${KUBERNETES_DASHBOARD_VERSION:-2.7.0}\"\nKUBESPHERE_VERSION=\"${KUBESPHERE_VERSION:-3.3.2}\"\n\n# 集群配置\nKUBE_DNSDOMAIN=\"${KUBE_DNSDOMAIN:-cluster.local}\"\nKUBE_APISERVER=\"${KUBE_APISERVER:-apiserver.$KUBE_DNSDOMAIN}\"\nKUBE_POD_SUBNET=\"${KUBE_POD_SUBNET:-10.244.0.0/16}\"\nKUBE_SERVICE_SUBNET=\"${KUBE_SERVICE_SUBNET:-10.96.0.0/16}\"\nKUBE_IMAGE_REPO=\"${KUBE_IMAGE_REPO:-registry.cn-hangzhou.aliyuncs.com/kainstall}\"\nKUBE_NETWORK=\"${KUBE_NETWORK:-flannel}\"\nKUBE_INGRESS=\"${KUBE_INGRESS:-nginx}\"\nKUBE_MONITOR=\"${KUBE_MONITOR:-prometheus}\"\nKUBE_STORAGE=\"${KUBE_STORAGE:-rook}\"\nKUBE_LOG=\"${KUBE_LOG:-elasticsearch}\"\nKUBE_UI=\"${KUBE_UI:-dashboard}\"\nKUBE_ADDON=\"${KUBE_ADDON:-metrics-server}\"\nKUBE_FLANNEL_TYPE=\"${KUBE_FLANNEL_TYPE:-vxlan}\"\nKUBE_CRI=\"${KUBE_CRI:-containerd}\"\nKUBE_CRI_VERSION=\"${KUBE_CRI_VERSION:-latest}\"\nKUBE_CRI_ENDPOINT=\"${KUBE_CRI_ENDPOINT:-unix:///run/containerd/containerd.sock}\"\n\n# 定义的master和worker节点地址，以逗号分隔\nMASTER_NODES=\"${MASTER_NODES:-}\"\nWORKER_NODES=\"${WORKER_NODES:-}\"\n\n# 定义在哪个节点上进行设置\nMGMT_NODE=\"${MGMT_NODE:-127.0.0.1}\"\n\n# 节点的连接信息\nSSH_USER=\"${SSH_USER:-root}\"\nSSH_PASSWORD=\"${SSH_PASSWORD:-}\"\nSSH_PRIVATE_KEY=\"${SSH_PRIVATE_KEY:-}\"\nSSH_PORT=\"${SSH_PORT:-22}\"\nSUDO_USER=\"${SUDO_USER:-root}\"\n\n# 节点设置\nHOSTNAME_PREFIX=\"${HOSTNAME_PREFIX:-k8s}\"\n\n# 脚本设置\nTMP_DIR=\"$(rm -rf /tmp/kainstall* && mktemp -d -t kainstall.XXXXXXXXXX)\"\nLOG_FILE=\"${TMP_DIR}/kainstall.log\"\nSSH_OPTIONS=\"-o ConnectTimeout=600 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\"\nERROR_INFO=\"\\n\\033[31mERROR Summary: \\033[0m\\n  \"\nACCESS_INFO=\"\\n\\033[32mACCESS Summary: \\033[0m\\n  \"\nCOMMAND_OUTPUT=\"\"\nSCRIPT_PARAMETER=\"$*\"\nOFFLINE_DIR=\"/tmp/kainstall-offline-file/\"\nOFFLINE_FILE=\"\"\nOS_SUPPORT=\"centos7 centos8\"\nGITHUB_PROXY=\"${GITHUB_PROXY:-https://mirror.ghproxy.com/}\"\nGCR_PROXY=\"${GCR_PROXY:-k8sgcr.lework.workers.dev}\"\nSKIP_UPGRADE_PLAN=${SKIP_UPGRADE_PLAN:-false}\nSKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n```\n\n### 离线部署\n\n> **注意**, 脚本执行的宿主机上，需要安装 `tar` 命令，用于解压离线包。\n> 详细部署请见: [https://lework.github.io/2020/10/18/kainstall-offline/](https://lework.github.io/2020/10/18/kainstall-offline/)\n\n1. 下载指定版本的离线包\n\n    ```bash\n    wget https://github.com/lework/kainstall-offline/releases/download/1.20.6/1.20.6_centos7.tgz\n    ```\n\n    > 更多离线包信息，见 [kainstall-offline](https://github.com/lework/kainstall-offline) 仓库\n\n2. 初始化集群\n\n    > 指定 `--offline-file` 参数。\n\n    ```bash\n    bash kainstall-centos.sh init \\\n      --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\n      --worker 192.168.77.133,192.168.77.134 \\\n      --user root \\\n      --password 123456 \\\n      --version 1.20.6 \\\n      --upgrade-kernel \\\n      --10years \\\n      --offline-file 1.20.6_centos7.tgz\n    ```\n\n3. 添加节点\n\n    > 指定 --offline-file 参数。\n\n    ```bash\n    bash kainstall-centos.sh add \\\n      --master 192.168.77.135 \\\n      --worker 192.168.77.136 \\\n      --user root \\\n      --password 123456 \\\n      --version 1.20.6 \\\n      --offline-file 1.20.6_centos7.tgz\n    ```\n\n### sudo 特权\n\n创建 sudo 用户\n\n```bash\nuseradd test\npasswd test --stdin <<< \"12345678\"\necho 'test    ALL=(ALL)   NOPASSWD:ALL' >> /etc/sudoers\n```\n\nsudo 参数\n\n- `--sudo` 开启 sudo 特权\n- `--sudo-user` 指定 sudo 用户, 默认是 `root`\n- `--sudo-password` 指定 sudo 密码\n\n示例\n\n```bash\n# 初始化\nbash kainstall-centos.sh init \\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\n  --worker 192.168.77.133,192.168.77.134 \\\n  --user test \\\n  --password 12345678 \\\n  --port 22 \\\n  --version 1.20.6 \\\n  --sudo \\\n  --sudo-user root \\\n  --sudo-password 12345678\n\n# 添加\nbash kainstall-centos.sh add \\\n  --master 192.168.77.135 \\\n  --worker 192.168.77.136 \\\n  --user test \\\n  --password 12345678 \\\n  --port 22 \\\n  --version 1.20.6 \\\n  --sudo \\\n  --sudo-user root \\\n  --sudo-password 12345678\n\n# 更新脚本文件\nbash kainstall-centos.sh update\n```\n\n### 10年证书期限\n\n**注意:** 此操作需要联网下载。\n\n使用 [kubeadm-certs](https://github.com/lework/kubeadm-certs) 项目编译的 `kubeadm` 客户端， 其修改了 `kubeadm` 源码，将 1 年期限修改成 10 年期限，具体信息见仓库介绍。\n\n在初始化或添加时，加上 `--10years` 参数，就可以使用`kubeadm` 10 years 的客户端\n\n示例\n\n```bash\n# 初始化\nbash kainstall-centos.sh init \\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\n  --worker 192.168.77.133,192.168.77.134 \\\n  --user root \\\n  --password 123456 \\\n  --port 22 \\\n  --version 1.20.6 \\\n  --10years\n  \n# 添加\nbash kainstall-centos.sh add \\\n  --master 192.168.77.135 \\\n  --worker 192.168.77.136 \\\n  --user root \\\n  --password 123456 \\\n  --port 22 \\\n  --version 1.20.6 \\\n  --10years\n```\n\n## 联系方式\n\n- [QQ群](https://qm.qq.com/cgi-bin/qm/qr?k=HwpkLUcmroLKNv37TlrHY-D3SXuLKMOd&jump_from=webapi)\n- [论坛](https://leops.cn/topics/node51)\n\n## License\n\nMIT\n"
        },
        {
          "name": "chmod.bat",
          "type": "blob",
          "size": 0.216796875,
          "content": "git config core.filemode false\r\ngit update-index --chmod=+x kainstall-centos.sh\r\ngit update-index --chmod=+x kainstall-debian.sh\r\ngit update-index --chmod=+x kainstall-ubuntu.sh\r\ngit ls-files --stage\r\ntimeout /nobreak /t 5"
        },
        {
          "name": "images",
          "type": "tree",
          "content": null
        },
        {
          "name": "kainstall-centos.sh",
          "type": "blob",
          "size": 129.7548828125,
          "content": "#!/usr/bin/env bash\n###################################################################\n#Script Name    : kainstall-centos.sh\n#Description    : Install kubernetes cluster using kubeadm.\n#Create Date    : 2020-09-17\n#Author         : lework\n#Email          : lework@yeah.net\n###################################################################\n\n\n[[ -n $DEBUG ]] && set -x\nset -o errtrace         # Make sure any error trap is inherited\nset -o nounset          # Disallow expansion of unset variables\nset -o pipefail         # Use last non-zero exit code in a pipeline\n\n\n######################################################################################################\n# environment configuration\n######################################################################################################\n\n# 版本\nKUBE_VERSION=\"${KUBE_VERSION:-latest}\"\nFLANNEL_VERSION=\"${FLANNEL_VERSION:-0.24.0}\"\nMETRICS_SERVER_VERSION=\"${METRICS_SERVER_VERSION:-0.6.4}\"\nINGRESS_NGINX=\"${INGRESS_NGINX:-1.9.5}\"\nTRAEFIK_VERSION=\"${TRAEFIK_VERSION:-2.10.7}\"\nCALICO_VERSION=\"${CALICO_VERSION:-3.27.0}\"\nCILIUM_VERSION=\"${CILIUM_VERSION:-1.14.5}\"\nKUBE_PROMETHEUS_VERSION=\"${KUBE_PROMETHEUS_VERSION:-0.13.0}\"\nELASTICSEARCH_VERSION=\"${ELASTICSEARCH_VERSION:-8.11.3}\"\nROOK_VERSION=\"${ROOK_VERSION:-1.9.13}\"\nLONGHORN_VERSION=\"${LONGHORN_VERSION:-1.5.3}\"\nKUBERNETES_DASHBOARD_VERSION=\"${KUBERNETES_DASHBOARD_VERSION:-2.7.0}\"\nKUBESPHERE_VERSION=\"${KUBESPHERE_VERSION:-3.3.2}\"\n\n# 集群配置\nKUBE_DNSDOMAIN=\"${KUBE_DNSDOMAIN:-cluster.local}\"\nKUBE_APISERVER=\"${KUBE_APISERVER:-apiserver.$KUBE_DNSDOMAIN}\"\nKUBE_POD_SUBNET=\"${KUBE_POD_SUBNET:-10.244.0.0/16}\"\nKUBE_SERVICE_SUBNET=\"${KUBE_SERVICE_SUBNET:-10.96.0.0/16}\"\nKUBE_IMAGE_REPO=\"${KUBE_IMAGE_REPO:-registry.cn-hangzhou.aliyuncs.com/kainstall}\"\nKUBE_NETWORK=\"${KUBE_NETWORK:-flannel}\"\nKUBE_INGRESS=\"${KUBE_INGRESS:-nginx}\"\nKUBE_MONITOR=\"${KUBE_MONITOR:-prometheus}\"\nKUBE_STORAGE=\"${KUBE_STORAGE:-rook}\"\nKUBE_LOG=\"${KUBE_LOG:-elasticsearch}\"\nKUBE_UI=\"${KUBE_UI:-dashboard}\"\nKUBE_ADDON=\"${KUBE_ADDON:-metrics-server}\"\nKUBE_FLANNEL_TYPE=\"${KUBE_FLANNEL_TYPE:-vxlan}\"\nKUBE_CRI=\"${KUBE_CRI:-containerd}\"\nKUBE_CRI_VERSION=\"${KUBE_CRI_VERSION:-latest}\"\nKUBE_CRI_ENDPOINT=\"${KUBE_CRI_ENDPOINT:-unix:///run/containerd/containerd.sock}\"\n\n# 定义的master和worker节点地址，以逗号分隔\nMASTER_NODES=\"${MASTER_NODES:-}\"\nWORKER_NODES=\"${WORKER_NODES:-}\"\n\n# 定义在哪个节点上进行设置\nMGMT_NODE=\"${MGMT_NODE:-127.0.0.1}\"\n\n# 节点的连接信息\nSSH_USER=\"${SSH_USER:-root}\"\nSSH_PASSWORD=\"${SSH_PASSWORD:-}\"\nSSH_PRIVATE_KEY=\"${SSH_PRIVATE_KEY:-}\"\nSSH_PORT=\"${SSH_PORT:-22}\"\nSUDO_USER=\"${SUDO_USER:-root}\"\n\n# 节点设置\nHOSTNAME_PREFIX=\"${HOSTNAME_PREFIX:-k8s}\"\n\n# 脚本设置\nTMP_DIR=\"$(rm -rf /tmp/kainstall* && mktemp -d -t kainstall.XXXXXXXXXX)\"\nLOG_FILE=\"${TMP_DIR}/kainstall.log\"\nSSH_OPTIONS=\"-o ConnectTimeout=600 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\"\nERROR_INFO=\"\\n\\033[31mERROR Summary: \\033[0m\\n  \"\nACCESS_INFO=\"\\n\\033[32mACCESS Summary: \\033[0m\\n  \"\nCOMMAND_OUTPUT=\"\"\nSCRIPT_PARAMETER=\"$*\"\nOFFLINE_DIR=\"/tmp/kainstall-offline-file/\"\nOFFLINE_FILE=\"\"\nOS_SUPPORT=\"centos7 centos8\"\nGITHUB_PROXY=\"${GITHUB_PROXY:-https://mirror.ghproxy.com/}\"\nGCR_PROXY=\"${GCR_PROXY:-k8sgcr.lework.workers.dev}\"\nSKIP_UPGRADE_PLAN=${SKIP_UPGRADE_PLAN:-false}\nSKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n\ntrap trap::info 1 2 3 15 EXIT\n\n######################################################################################################\n# function\n######################################################################################################\n\nfunction trap::info() {\n  # 信号处理\n  \n  [[ ${#ERROR_INFO} -gt 37 ]] && echo -e \"$ERROR_INFO\"\n  [[ ${#ACCESS_INFO} -gt 38 ]] && echo -e \"$ACCESS_INFO\"\n  [ -f \"$LOG_FILE\" ] && echo -e \"\\n\\n  See detailed log >>> $LOG_FILE \\n\\n\"\n  trap '' EXIT\n  exit\n}\n\n\nfunction log::error() {\n  # 错误日志\n  \n  local item; item=\"[$(date +'%Y-%m-%dT%H:%M:%S.%N%z')]: \\033[31mERROR:   \\033[0m$*\"\n  ERROR_INFO=\"${ERROR_INFO}${item}\\n  \"\n  echo -e \"${item}\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::info() {\n  # 基础日志\n  \n  printf \"[%s]: \\033[32mINFO:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::warning() {\n  # 警告日志\n  \n  printf \"[%s]: \\033[33mWARNING: \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::access() {\n  # 访问信息\n  \n  ACCESS_INFO=\"${ACCESS_INFO}$*\\n  \"\n  printf \"[%s]: \\033[32mINFO:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::exec() {\n  # 执行日志\n  \n  printf \"[%s]: \\033[34mEXEC:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" >> \"$LOG_FILE\"\n}\n\n\nfunction utils::version_to_number() {\n  # 版本号转数字\n\n  echo \"$@\" | awk -F. '{ printf(\"%d%03d%03d%03d\\n\", $1,$2,$3,$4); }';\n}\n\n\nfunction utils::retry {\n  # 重试\n\n  local retries=$1\n  shift\n\n  local count=0\n  until eval \"$*\"; do\n    exit=$?\n    wait=$((2 ** count))\n    count=$((count + 1))\n    if [ \"$count\" -lt \"$retries\" ]; then\n      echo \"Retry $count/$retries exited $exit, retrying in $wait seconds...\"\n      sleep $wait\n    else\n      echo \"Retry $count/$retries exited $exit, no more retries left.\"\n      return $exit\n    fi\n  done\n  return 0\n}\n\n\nfunction utils::quote() {\n  # 转义引号\n\n  # shellcheck disable=SC2046 \n  if [ $(echo \"$*\" | tr -d \"\\n\" | wc -c) -eq 0 ]; then\n    echo \"''\"\n  elif [ $(echo \"$*\" | tr -d \"[a-z][A-Z][0-9]:,.=~_/\\n-\" | wc -c) -gt 0 ]; then\n    printf \"%s\" \"$*\" | sed -e \"1h;2,\\$H;\\$!d;g\" -e \"s/'/\\'\\\"\\'\\\"\\'/g\" | sed -e \"1h;2,\\$H;\\$!d;g\" -e \"s/^/'/g\" -e \"s/$/'/g\"\n  else\n    echo \"$*\"\n  fi\n}\n\n\nfunction utils::download_file() {\n  # 下载文件\n  \n  local url=\"$1\"\n  local dest=\"$2\"\n  local unzip_tag=\"${3:-1}\"\n  \n  local dest_dirname; dest_dirname=$(dirname \"$dest\")\n  local filename; filename=$(basename \"$dest\")\n  \n  log::info \"[download]\" \"${filename}\"\n  command::exec \"${MGMT_NODE}\" \"\n    set -e\n    if [ ! -f \\\"${dest}\\\" ]; then\n      [ ! -d \\\"${dest_dirname}\\\" ] && mkdir -pv \\\"${dest_dirname}\\\" \n      wget --timeout=10 --waitretry=3 --tries=5 --retry-connrefused --no-check-certificate \\\"${url}\\\" -O \\\"${dest}\\\"\n      if [[ \\\"${unzip_tag}\\\" == \\\"unzip\\\" ]]; then\n        command -v unzip 2>/dev/null || yum install -y unzip\n        unzip -o \\\"${dest}\\\" -d \\\"${dest_dirname}\\\"\n      fi\n    else\n      echo \\\"${dest} is exists!\\\"\n    fi\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"download\" \"${filename}\" \"exit\"\n  return \"$status\"\n}\n\n\nfunction utils::is_element_in_array() {\n  # 判断是否在数组中存在元素\n\n  local -r element=\"${1}\"\n  local -r array=(\"${@:2}\")\n\n  local walker=''\n\n  for walker in \"${array[@]}\"\n  do\n    [[ \"${walker}\" = \"${element}\" ]] && return 0\n  done\n\n  return 1\n}\n\n\nfunction command::exec() {\n  # 执行命令\n\n  local host=${1:-}\n  shift\n  local command=\"$*\"\n  \n  if [[ \"${SUDO_TAG:-}\" == \"1\" ]]; then\n    sudo_options=\"sudo -H -n -u ${SUDO_USER}\"\n  \n    if [[ \"${SUDO_PASSWORD:-}\" != \"\" ]]; then\n       sudo_options=\"${sudo_options// -n/} -p \\\"\\\" -S <<< \\\"${SUDO_PASSWORD}\\\"\"\n    fi\n    command=\"$sudo_options bash -c $(utils::quote \"$command\")\"\n  fi\n  \n  command=\"$(utils::quote \"$command\")\"\n  \n  if [[ \"${host}\" == \"127.0.0.1\" ]]; then\n    # 本地执行\n    log::exec \"[command]\" \"bash -c $(printf \"%s\" \"${command//${SUDO_PASSWORD:-}/zzzzzz}\")\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval bash -c \"${command}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  else\n    # 远程执行\n    local ssh_cmd=\"ssh\"\n    if [[ \"${SSH_PASSWORD}\" != \"\" ]]; then\n      ssh_cmd=\"sshpass -p \\\"${SSH_PASSWORD}\\\" ${ssh_cmd}\"\n    elif [[ \"$SSH_PRIVATE_KEY\" != \"\" ]]; then\n      [ -f \"${SSH_PRIVATE_KEY}\" ] || { log::error \"[exec]\" \"ssh private_key:${SSH_PRIVATE_KEY} not found.\"; exit 1; }\n      ssh_cmd=\"${ssh_cmd} -i $SSH_PRIVATE_KEY\"\n    fi\n    log::exec \"[command]\" \"${ssh_cmd//${SSH_PASSWORD:-}/zzzzzz} ${SSH_OPTIONS} ${SSH_USER}@${host} -p ${SSH_PORT} bash -c $(printf \"%s\" \"${command//${SUDO_PASSWORD:-}/zzzzzz}\")\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval \"${ssh_cmd} ${SSH_OPTIONS} ${SSH_USER}@${host} -p ${SSH_PORT}\" bash -c '\"${command}\"' 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  fi\n  return $status\n}\n\n\nfunction command::scp() {\n  # 拷贝文件\n\n  local host=${1:-}\n  local src=${2:-}\n  local dest=${3:-/tmp/}\n  \n  if [[ \"${host}\" == \"127.0.0.1\" ]]; then\n    local command=\"cp -rf ${src} ${dest}\"\n    log::exec \"[command]\" \"bash -c \\\"${command}\\\"\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(bash -c \"${command}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  else\n    local scp_cmd=\"scp\"\n    if [[ \"${SSH_PASSWORD}\" != \"\" ]]; then\n      scp_cmd=\"sshpass -p \\\"${SSH_PASSWORD}\\\" ${scp_cmd}\"\n    elif [[ \"$SSH_PRIVATE_KEY\" != \"\" ]]; then\n      [ -f \"${SSH_PRIVATE_KEY}\" ] || { log::error \"[exec]\" \"ssh private_key:${SSH_PRIVATE_KEY} not found.\"; exit 1; }\n      scp_cmd=\"${scp_cmd} -i $SSH_PRIVATE_KEY\"\n    fi\n    log::exec \"[command]\" \"${scp_cmd} ${SSH_OPTIONS} -P ${SSH_PORT} -r ${src} ${SSH_USER}@${host}:${dest}\" >> \"$LOG_FILE\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval \"${scp_cmd} ${SSH_OPTIONS} -P ${SSH_PORT} -r ${src} ${SSH_USER}@${host}:${dest}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  fi\n  return $status\n}\n\n\nfunction script::init_node() {\n  # 节点初始化脚本\n  \n  # clean\n  sed -i -e \"/$KUBE_APISERVER/d\" -e '/-worker-/d' -e '/-master-/d' /etc/hosts\n  sed -i '/## Kainstall managed start/,/## Kainstall managed end/d' /etc/security/limits.conf /etc/systemd/system.conf /etc/bashrc /etc/rc.local /etc/audit/rules.d/audit.rules  \n\n  # Disable selinux\n  sed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/config\n  setenforce 0\n  \n  # Disable swap\n  swapoff -a && sysctl -w vm.swappiness=0\n  sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n\n  # Disable firewalld\n  for target in firewalld python-firewall firewalld-filesystem iptables; do\n    systemctl stop $target &>/dev/null || true\n    systemctl disable $target &>/dev/null || true\n  done\n\n  # repo\n  [[ -f /etc/yum.repos.d/CentOS-Base.repo && \"${SKIP_SET_OS_REPO,,}\" == \"false\" ]] && sed -e 's!^#baseurl=!baseurl=!g' \\\n    -e 's!^mirrorlist=!#mirrorlist=!g' \\\n    -e 's!mirror.centos.org!mirrors.aliyun.com!g' \\\n    -i /etc/yum.repos.d/CentOS-Base.repo\n  \n  [[ \"${OFFLINE_TAG:-}\" != \"1\" && \"${SKIP_SET_OS_REPO,,}\" == \"false\" ]] && yum install -y epel-release\n  \n  [[ -f /etc/yum.repos.d/epel.repo && \"${SKIP_SET_OS_REPO,,}\" == \"false\" ]] && sed -e 's!^mirrorlist=!#mirrorlist=!g' \\\n    -e 's!^metalink=!#metalink=!g' \\\n    -e 's!^#baseurl=!baseurl=!g' \\\n    -e 's!//download.*/pub!//mirrors.aliyun.com!g' \\\n    -e 's!http://mirrors\\.aliyun!https://mirrors.aliyun!g' \\\n    -i /etc/yum.repos.d/epel.repo\n\n  yum clean all\n\n  # Change limits\n  [ ! -f /etc/security/limits.conf_bak ] && cp /etc/security/limits.conf{,_bak}\n  cat << EOF >> /etc/security/limits.conf\n## Kainstall managed start\nroot soft nofile 655360\nroot hard nofile 655360\nroot soft nproc 655360\nroot hard nproc 655360\nroot soft core unlimited\nroot hard core unlimited\n\n* soft nofile 655360\n* hard nofile 655360\n* soft nproc 655360\n* hard nproc 655360\n* soft core unlimited\n* hard core unlimited\n## Kainstall managed end\nEOF\n\n  [ -f /etc/security/limits.d/20-nproc.conf ] && sed -i 's#4096#655360#g' /etc/security/limits.d/20-nproc.conf\n  cat << EOF >> /etc/systemd/system.conf\n## Kainstall managed start\nDefaultLimitCORE=infinity\nDefaultLimitNOFILE=655360\nDefaultLimitNPROC=655360\nDefaultTasksMax=75%\n## Kainstall managed end\nEOF\n\n   # Change sysctl\n   cat << EOF >  /etc/sysctl.d/99-kube.conf\n# https://www.kernel.org/doc/Documentation/sysctl/\n#############################################################################################\n# 调整虚拟内存\n#############################################################################################\n\n# Default: 30\n# 0 - 任何情况下都不使用swap。\n# 1 - 除非内存不足（OOM），否则不使用swap。\nvm.swappiness = 0\n\n# 内存分配策略\n#0 - 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。\n#1 - 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。\n#2 - 表示内核允许分配超过所有物理内存和交换空间总和的内存\nvm.overcommit_memory=1\n\n# OOM时处理\n# 1关闭，等于0时，表示当内存耗尽时，内核会触发OOM killer杀掉最耗内存的进程。\nvm.panic_on_oom=0\n\n# vm.dirty_background_ratio 用于调整内核如何处理必须刷新到磁盘的脏页。\n# Default value is 10.\n# 该值是系统内存总量的百分比，在许多情况下将此值设置为5是合适的。\n# 此设置不应设置为零。\nvm.dirty_background_ratio = 5\n\n# 内核强制同步操作将其刷新到磁盘之前允许的脏页总数\n# 也可以通过更改 vm.dirty_ratio 的值（将其增加到默认值30以上（也占系统内存的百分比））来增加\n# 推荐 vm.dirty_ratio 的值在60到80之间。\nvm.dirty_ratio = 60\n\n# vm.max_map_count 计算当前的内存映射文件数。\n# mmap 限制（vm.max_map_count）的最小值是打开文件的ulimit数量（cat /proc/sys/fs/file-max）。\n# 每128KB系统内存 map_count应该大约为1。 因此，在32GB系统上，max_map_count为262144。\n# Default: 65530\nvm.max_map_count = 2097152\n\n#############################################################################################\n# 调整文件\n#############################################################################################\n\nfs.may_detach_mounts = 1\n\n# 增加文件句柄和inode缓存的大小，并限制核心转储。\nfs.file-max = 2097152\nfs.nr_open = 2097152\nfs.suid_dumpable = 0\n\n# 同时可以拥有的的异步IO请求数目\nfs.aio-max-nr = 10000000\nfs.aio-nr = 75552\n\n# 文件监控\nfs.inotify.max_user_instances=8192\nfs.inotify.max_user_watches=524288\nfs.inotify.max_queued_events=16384\n\n#############################################################################################\n# 调整网络设置\n#############################################################################################\n\n# 为每个套接字的发送和接收缓冲区分配的默认内存量。\nnet.core.wmem_default = 25165824\nnet.core.rmem_default = 25165824\n\n# 为每个套接字的发送和接收缓冲区分配的最大内存量。\nnet.core.wmem_max = 25165824\nnet.core.rmem_max = 25165824\n\n# 除了套接字设置外，发送和接收缓冲区的大小\n# 必须使用net.ipv4.tcp_wmem和net.ipv4.tcp_rmem参数分别设置TCP套接字。\n# 使用三个以空格分隔的整数设置这些整数，分别指定最小，默认和最大大小。\n# 最大大小不能大于使用net.core.wmem_max和net.core.rmem_max为所有套接字指定的值。\n# 合理的设置是最小4KiB，默认64KiB和最大2MiB缓冲区。\nnet.ipv4.tcp_wmem = 20480 12582912 25165824\nnet.ipv4.tcp_rmem = 20480 12582912 25165824\n\n# 增加最大可分配的总缓冲区空间\n# 以页为单位（4096字节）进行度量\nnet.ipv4.tcp_mem = 65536 25165824 262144\nnet.ipv4.udp_mem = 65536 25165824 262144\n\n# 为每个套接字的发送和接收缓冲区分配的最小内存量。\nnet.ipv4.udp_wmem_min = 16384\nnet.ipv4.udp_rmem_min = 16384\n\n# 启用TCP窗口缩放，客户端可以更有效地传输数据，并允许在代理方缓冲该数据。\nnet.ipv4.tcp_window_scaling = 1\n\n# 提高同时接受连接数。\nnet.ipv4.tcp_max_syn_backlog = 10240\n\n# 将net.core.netdev_max_backlog的值增加到大于默认值1000\n# 可以帮助突发网络流量，特别是在使用数千兆位网络连接速度时，\n# 通过允许更多的数据包排队等待内核处理它们。\nnet.core.netdev_max_backlog = 65536\n\n# 增加选项内存缓冲区的最大数量\nnet.core.optmem_max = 25165824\n\n# 被动TCP连接的SYNACK次数。\nnet.ipv4.tcp_synack_retries = 2\n\n# 允许的本地端口范围。\nnet.ipv4.ip_local_port_range = 2048 65535\n\n# 防止TCP时间等待\n# Default: net.ipv4.tcp_rfc1337 = 0\nnet.ipv4.tcp_rfc1337 = 1\n\n# 减少tcp_fin_timeout连接的时间默认值\nnet.ipv4.tcp_fin_timeout = 15\n\n# 积压套接字的最大数量。\n# Default is 128.\nnet.core.somaxconn = 32768\n\n# 打开syncookies以进行SYN洪水攻击保护。\nnet.ipv4.tcp_syncookies = 1\n\n# 避免Smurf攻击\n# 发送伪装的ICMP数据包，目的地址设为某个网络的广播地址，源地址设为要攻击的目的主机，\n# 使所有收到此ICMP数据包的主机都将对目的主机发出一个回应，使被攻击主机在某一段时间内收到成千上万的数据包\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\n\n# 为icmp错误消息打开保护\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\n\n# 启用自动缩放窗口。\n# 如果延迟证明合理，这将允许TCP缓冲区超过其通常的最大值64K。\nnet.ipv4.tcp_window_scaling = 1\n\n# 打开并记录欺骗，源路由和重定向数据包\nnet.ipv4.conf.all.log_martians = 1\nnet.ipv4.conf.default.log_martians = 1\n\n# 告诉内核有多少个未附加的TCP套接字维护用户文件句柄。 万一超过这个数字，\n# 孤立的连接会立即重置，并显示警告。\n# Default: net.ipv4.tcp_max_orphans = 65536\nnet.ipv4.tcp_max_orphans = 65536\n\n# 不要在关闭连接时缓存指标\nnet.ipv4.tcp_no_metrics_save = 1\n\n# 启用RFC1323中定义的时间戳记：\n# Default: net.ipv4.tcp_timestamps = 1\nnet.ipv4.tcp_timestamps = 1\n\n# 启用选择确认。\n# Default: net.ipv4.tcp_sack = 1\nnet.ipv4.tcp_sack = 1\n\n# 增加 tcp-time-wait 存储桶池大小，以防止简单的DOS攻击。\n# net.ipv4.tcp_tw_recycle 已从Linux 4.12中删除。请改用net.ipv4.tcp_tw_reuse。\nnet.ipv4.tcp_max_tw_buckets = 14400\nnet.ipv4.tcp_tw_reuse = 1\n\n# accept_source_route 选项使网络接口接受设置了严格源路由（SSR）或松散源路由（LSR）选项的数据包。\n# 以下设置将丢弃设置了SSR或LSR选项的数据包。\nnet.ipv4.conf.all.accept_source_route = 0\nnet.ipv4.conf.default.accept_source_route = 0\n\n# 打开反向路径过滤\nnet.ipv4.conf.all.rp_filter = 1\nnet.ipv4.conf.default.rp_filter = 1\n\n# 禁用ICMP重定向接受\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.all.secure_redirects = 0\nnet.ipv4.conf.default.secure_redirects = 0\n\n# 禁止发送所有IPv4 ICMP重定向数据包。\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.default.send_redirects = 0\n\n# 开启IP转发.\nnet.ipv4.ip_forward = 1\n\n# 禁止IPv6\nnet.ipv6.conf.lo.disable_ipv6=1\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n\n# 要求iptables不对bridge的数据进行处理\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-arptables = 1\n\n# arp缓存\n# 存在于 ARP 高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是 128\nnet.ipv4.neigh.default.gc_thresh1=2048\n# 保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512\nnet.ipv4.neigh.default.gc_thresh2=4096\n# 保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是 1024\nnet.ipv4.neigh.default.gc_thresh3=8192\n\n# 持久连接\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_intvl = 30\nnet.ipv4.tcp_keepalive_probes = 10\n\n# conntrack表\nnet.nf_conntrack_max=1048576\nnet.netfilter.nf_conntrack_max=1048576\nnet.netfilter.nf_conntrack_buckets=262144\nnet.netfilter.nf_conntrack_tcp_timeout_fin_wait=30\nnet.netfilter.nf_conntrack_tcp_timeout_time_wait=30\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait=15\nnet.netfilter.nf_conntrack_tcp_timeout_established=300\n\n#############################################################################################\n# 调整内核参数\n#############################################################################################\n\n# 地址空间布局随机化（ASLR）是一种用于操作系统的内存保护过程，可防止缓冲区溢出攻击。\n# 这有助于确保与系统上正在运行的进程相关联的内存地址不可预测，\n# 因此，与这些流程相关的缺陷或漏洞将更加难以利用。\n# Accepted values: 0 = 关闭, 1 = 保守随机化, 2 = 完全随机化\nkernel.randomize_va_space = 2\n\n# 调高 PID 数量\nkernel.pid_max = 65536\nkernel.threads-max=30938\n\n# coredump\nkernel.core_pattern=core\n\n# 决定了检测到soft lockup时是否自动panic，缺省值是0\nkernel.softlockup_all_cpu_backtrace=1\nkernel.softlockup_panic=1\nEOF\n\n  # history\n  cat << EOF >> /etc/bashrc\n## Kainstall managed start\n# history actions record，include action time, user, login ip\nHISTFILESIZE=5000\nHISTSIZE=5000\nUSER_IP=\\$(who -u am i 2>/dev/null | awk '{print \\$NF}' | sed -e 's/[()]//g')\nif [ -z \\$USER_IP ]\nthen\n  USER_IP=\\$(hostname -i)\nfi\nHISTTIMEFORMAT=\"%Y-%m-%d %H:%M:%S \\$USER_IP:\\$(whoami) \"\nexport HISTFILESIZE HISTSIZE HISTTIMEFORMAT\n\n# PS1\nPS1='\\[\\033[0m\\]\\[\\033[1;36m\\][\\u\\[\\033[0m\\]@\\[\\033[1;32m\\]\\h\\[\\033[0m\\] \\[\\033[1;31m\\]\\w\\[\\033[0m\\]\\[\\033[1;36m\\]]\\[\\033[33;1m\\]\\\\$ \\[\\033[0m\\]'\n## Kainstall managed end\nEOF\n\n   # journal\n   mkdir -p /var/log/journal /etc/systemd/journald.conf.d\n   cat << EOF > /etc/systemd/journald.conf.d/99-prophet.conf\n[Journal]\n# 持久化保存到磁盘\nStorage=persistent\n# 压缩历史日志\nCompress=yes\nSyncIntervalSec=5m\nRateLimitInterval=30s\nRateLimitBurst=1000\n# 最大占用空间 10G\nSystemMaxUse=10G\n# 单日志文件最大 200M\nSystemMaxFileSize=200M\n# 日志保存时间 3 周\nMaxRetentionSec=3week\n# 不将日志转发到 syslog\nForwardToSyslog=no\nEOF\n\n  # motd\n  cat << EOF > /etc/profile.d/zz-ssh-login-info.sh\n#!/bin/sh\n#\n# @Time    : 2020-02-04\n# @Author  : lework\n# @Desc    : ssh login banner\n\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:\\$PATH\nshopt -q login_shell && : || return 0\necho -e \"\\033[0;32m\n ██╗  ██╗ █████╗ ███████╗\n ██║ ██╔╝██╔══██╗██╔════╝\n █████╔╝ ╚█████╔╝███████╗\n ██╔═██╗ ██╔══██╗╚════██║\n ██║  ██╗╚█████╔╝███████║\n ╚═╝  ╚═╝ ╚════╝ ╚══════ by kainstall\\033[0m\"\n\n# os\nupSeconds=\"\\$(cut -d. -f1 /proc/uptime)\"\nsecs=\\$((\\${upSeconds}%60))\nmins=\\$((\\${upSeconds}/60%60))\nhours=\\$((\\${upSeconds}/3600%24))\ndays=\\$((\\${upSeconds}/86400))\nUPTIME_INFO=\\$(printf \"%d days, %02dh %02dm %02ds\" \"\\$days\" \"\\$hours\" \"\\$mins\" \"\\$secs\")\n\nif [ -f /etc/redhat-release ] ; then\n    PRETTY_NAME=\\$(< /etc/redhat-release)\n\nelif [ -f /etc/debian_version ]; then\n   DIST_VER=\\$(</etc/debian_version)\n   PRETTY_NAME=\"\\$(grep PRETTY_NAME /etc/os-release | sed -e 's/PRETTY_NAME=//g' -e  's/\"//g') (\\$DIST_VER)\"\n\nelse\n    PRETTY_NAME=\\$(cat /etc/*-release | grep \"PRETTY_NAME\" | sed -e 's/PRETTY_NAME=//g' -e 's/\"//g')\nfi\n\nif [[ -d \"/system/app/\" && -d \"/system/priv-app\" ]]; then\n    model=\"\\$(getprop ro.product.brand) \\$(getprop ro.product.model)\"\n\nelif [[ -f /sys/devices/virtual/dmi/id/product_name ||\n        -f /sys/devices/virtual/dmi/id/product_version ]]; then\n    model=\"\\$(< /sys/devices/virtual/dmi/id/product_name)\"\n    model+=\" \\$(< /sys/devices/virtual/dmi/id/product_version)\"\n\nelif [[ -f /sys/firmware/devicetree/base/model ]]; then\n    model=\"\\$(< /sys/firmware/devicetree/base/model)\"\n\nelif [[ -f /tmp/sysinfo/model ]]; then\n    model=\"\\$(< /tmp/sysinfo/model)\"\nfi\n\nMODEL_INFO=\\${model}\nKERNEL=\\$(uname -srmo)\nUSER_NUM=\\$(who -u | wc -l)\nRUNNING=\\$(ps ax | wc -l | tr -d \" \")\n\n# disk\ntotaldisk=\\$(df -h -x devtmpfs -x tmpfs -x debugfs -x aufs -x overlay --total 2>/dev/null | tail -1)\ndisktotal=\\$(awk '{print \\$2}' <<< \"\\${totaldisk}\")\ndiskused=\\$(awk '{print \\$3}' <<< \"\\${totaldisk}\")\ndiskusedper=\\$(awk '{print \\$5}' <<< \"\\${totaldisk}\")\nDISK_INFO=\"\\033[0;33m\\${diskused}\\033[0m of \\033[1;34m\\${disktotal}\\033[0m disk space used (\\033[0;33m\\${diskusedper}\\033[0m)\"\n\n# cpu\ncpu=\\$(awk -F':' '/^model name/ {print \\$2}' /proc/cpuinfo | uniq | sed -e 's/^[ \\t]*//')\ncpun=\\$(grep -c '^processor' /proc/cpuinfo)\ncpuc=\\$(grep '^cpu cores' /proc/cpuinfo | tail -1 | awk '{print \\$4}')\ncpup=\\$(grep '^physical id' /proc/cpuinfo | wc -l)\nCPU_INFO=\"\\${cpu} \\${cpup}P \\${cpuc}C \\${cpun}L\"\n\n# get the load averages\nread one five fifteen rest < /proc/loadavg\nLOADAVG_INFO=\"\\033[0;33m\\${one}\\033[0m / \\${five} / \\${fifteen} with \\033[1;34m\\$(( cpun*cpuc ))\\033[0m core(s) at \\033[1;34m\\$(grep '^cpu MHz' /proc/cpuinfo | tail -1 | awk '{print \\$4}')\\033 MHz\"\n\n# mem\nMEM_INFO=\"\\$(cat /proc/meminfo | awk '/MemTotal:/{total=\\$2/1024/1024;next} /MemAvailable:/{use=total-\\$2/1024/1024; printf(\"\\033[0;33m%.2fGiB\\033[0m of \\033[1;34m%.2fGiB\\033[0m RAM used (\\033[0;33m%.2f%%\\033[0m)\",use,total,(use/total)*100);}')\"\n\n# network\n# extranet_ip=\" and \\$(curl -s ip.cip.cc)\"\nIP_INFO=\"\\$(ip a|grep -E '^[0-9]+: em*|^[0-9]+: eno*|^[0-9]+: enp*|^[0-9]+: ens*|^[0-9]+: eth*|^[0-9]+: wlp*' -A2|grep inet|awk -F ' ' '{print \\$2}'|cut -f1 -d/|xargs echo)\"\n\n# Container info\nCONTAINER_INFO=\"\\$(sudo /usr/bin/crictl ps -a -o yaml 2> /dev/null | awk '/^  state: /{gsub(\"CONTAINER_\", \"\", \\$NF) ++S[\\$NF]}END{for(m in S) printf \"%s%s:%s \",substr(m,1,1),tolower(substr(m,2)),S[m]}')Images:\\$(sudo /usr/bin/crictl images -q 2> /dev/null | wc -l)\"\n\n# info\necho -e \"\n Information as of: \\033[1;34m\\$(date +\"%Y-%m-%d %T\")\\033[0m\n \n \\033[0;1;31mProduct\\033[0m............: \\${MODEL_INFO}\n \\033[0;1;31mOS\\033[0m.................: \\${PRETTY_NAME}\n \\033[0;1;31mKernel\\033[0m.............: \\${KERNEL}\n \\033[0;1;31mCPU\\033[0m................: \\${CPU_INFO}\n\n \\033[0;1;31mHostname\\033[0m...........: \\033[1;34m\\$(hostname)\\033[0m\n \\033[0;1;31mIP Addresses\\033[0m.......: \\033[1;34m\\${IP_INFO}\\033[0m\n\n \\033[0;1;31mUptime\\033[0m.............: \\033[0;33m\\${UPTIME_INFO}\\033[0m\n \\033[0;1;31mMemory\\033[0m.............: \\${MEM_INFO}\n \\033[0;1;31mLoad Averages\\033[0m......: \\${LOADAVG_INFO}\n \\033[0;1;31mDisk Usage\\033[0m.........: \\${DISK_INFO} \n\n \\033[0;1;31mUsers online\\033[0m.......: \\033[1;34m\\${USER_NUM}\\033[0m\n \\033[0;1;31mRunning Processes\\033[0m..: \\033[1;34m\\${RUNNING}\\033[0m\n \\033[0;1;31mContainer Info\\033[0m.....: \\${CONTAINER_INFO}\n\"\nEOF\n\n  chmod +x /etc/profile.d/zz-ssh-login-info.sh\n  echo 'ALL ALL=(ALL) NOPASSWD:/usr/bin/crictl' > /etc/sudoers.d/crictl\n\n  # time sync\n  ntpd --help >/dev/null 2>&1 && yum remove -y ntp\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && yum install -y chrony \n  [ ! -f /etc/chrony.conf_bak ] && cp /etc/chrony.conf{,_bak} #备份默认配置\n  cat << EOF > /etc/chrony.conf\nserver ntp.aliyun.com iburst\nserver cn.ntp.org.cn iburst\nserver ntp.shu.edu.cn iburst\nserver 0.cn.pool.ntp.org iburst\nserver 1.cn.pool.ntp.org iburst\nserver 2.cn.pool.ntp.org iburst\nserver 3.cn.pool.ntp.org iburst\n\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nlogdir /var/log/chrony\nEOF\n\n  timedatectl set-timezone Asia/Shanghai\n  chronyd -q -t 1 'server cn.pool.ntp.org iburst maxsamples 1'\n  systemctl enable chronyd\n  systemctl start chronyd\n  chronyc sources -v\n  chronyc sourcestats\n  hwclock --systohc\n\n  # package\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && yum install -y curl wget\n\n  # ipvs\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && yum install -y ipvsadm ipset sysstat conntrack libseccomp\n  module=(\n  ip_vs\n  ip_vs_rr\n  ip_vs_wrr\n  ip_vs_sh\n  overlay\n  nf_conntrack\n  br_netfilter\n  )\n  [ -f /etc/modules-load.d/ipvs.conf ] && cp -f /etc/modules-load.d/ipvs.conf{,_bak}\n  for kernel_module in \"${module[@]}\";do\n     /sbin/modinfo -F filename \"$kernel_module\" |& grep -qv ERROR && echo \"$kernel_module\" >> /etc/modules-load.d/ipvs.conf\n  done\n  systemctl restart systemd-modules-load\n  systemctl enable systemd-modules-load\n  sysctl --system\n\n  # audit\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && yum install -y audit audit-libs\ncat << EOF >> /etc/audit/rules.d/audit.rules\n## Kainstall managed start\n# Ignore errors\n-i\n\n# SYSCALL\n-a always,exit -F arch=b64 -S kill,tkill,tgkill -F a1=9 -F key=trace_kill_9\n-a always,exit -F arch=b64 -S kill,tkill,tgkill -F a1=15 -F key=trace_kill_15\n\n# docker\n-w /usr/bin/dockerd -k docker\n-w /var/lib/docker -k docker\n-w /etc/docker -k docker\n-w /usr/lib/systemd/system/docker.service -k docker\n-w /etc/systemd/system/docker.service -k docker\n-w /usr/lib/systemd/system/docker.socket -k docker\n-w /etc/default/docker -k docker\n-w /etc/sysconfig/docker -k docker\n-w /etc/docker/daemon.json -k docker\n\n# containerd\n-w /usr/bin/containerd -k containerd\n-w /var/lib/containerd -k containerd\n-w /usr/lib/systemd/system/containerd.service -k containerd\n-w /etc/containerd/config.toml -k containerd\n\n# cri-o\n-w /usr/bin/crio -k cri-o\n-w /etc/crio -k cri-o\n\n# runc \n-w /usr/bin/runc -k runc\n\n# kube\n-w /usr/bin/kubeadm -k kubeadm\n-w /usr/bin/kubelet -k kubelet\n-w /usr/bin/kubectl -k kubectl\n-w /var/lib/kubelet -k kubelet\n-w /etc/kubernetes -k kubernetes\n## Kainstall managed end\nEOF\n  chmod 600 /etc/audit/rules.d/audit.rules\n  sed -i 's#max_log_file =.*#max_log_file = 80#g' /etc/audit/auditd.conf \n  if [ -f /usr/libexec/initscripts/legacy-actions/auditd/restart ]; then\n     /usr/libexec/initscripts/legacy-actions/auditd/restart\n  else\n     systemctl stop auditd && systemctl start auditd\n  fi\n  systemctl enable auditd\n\n  grep single-request-reopen /etc/resolv.conf || sed -i '1ioptions timeout:2 attempts:3 rotate single-request-reopen' /etc/resolv.conf\n\n  ipvsadm --clear\n  iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n}\n\n\nfunction script::upgrade_kernel() {\n  # 升级内核\n\n  local ver; ver=$(rpm --eval \"%{centos_ver}\")\n\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && yum install -y \"https://www.elrepo.org/elrepo-release-${ver}.el${ver}.elrepo.noarch.rpm\"\n  sed -e \"s/^mirrorlist=/#mirrorlist=/g\" \\\n      -e \"s/elrepo.org\\/linux/mirrors.tuna.tsinghua.edu.cn\\/elrepo/g\" \\\n      -i /etc/yum.repos.d/elrepo.repo\n  \n  if ! command -v perl; then yum install -y perl;fi\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && if ! yum install -y --disablerepo=\"*\" --enablerepo=elrepo-kernel kernel-ml{,-devel}; then exit 1;fi\n  \n  grub2-set-default 0 && grub2-mkconfig -o /etc/grub2.cfg\n  grubby --default-kernel\n  grubby --args=\"cgroup_enable=cpuset cgroup_memory=1 cgroup_enable=memory user_namespace.enable=1\" --update-kernel=\"$(grubby --default-kernel)\"\n}\n\n\nfunction script::upgrage_kube() {\n  # 节点软件升级\n\n  local role=${1:-init}\n  local version=\"-${2:-latest}\"\n  version=\"${version#-latest}\"\n\n  set -e\n  echo '[install] kubeadm'\n  kubeadm version\n  yum install -y \"kubeadm${version}\" --disableexcludes=kubernetes\n  kubeadm version\n\n  echo '[upgrade]'\n  if [[ \"$role\" == \"init\" ]]; then\n    local plan_info; plan_info=$(kubeadm upgrade plan)\n    local v; v=$(printf \"%s\" \"$plan_info\" | grep 'kubeadm upgrade apply ' | awk '{print $4}'| tail -1 )\n    printf \"%s\\n\" \"${plan_info}\"\n    kubeadm upgrade apply \"${v}\" -y\n  else\n    kubeadm upgrade node\n  fi\n\n  echo '[install] kubelet kubectl'\n  kubectl version --client=true\n  yum install -y \"kubelet${version}\" \"kubectl${version}\" --disableexcludes=kubernetes\n  kubectl version --client=true\n\n  [ -f /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf ] && \\\n    sed -i 's#^\\[Service\\]#[Service]\\nCPUAccounting=true\\nMemoryAccounting=true#g' /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\n\n  systemctl daemon-reload\n  systemctl restart kubelet\n}\n\n\nfunction script::install_docker() {\n  # 安装 docker\n  \n  local version=\"-${1:-latest}\"\n  version=\"${version#-latest}\"\n\n  cat << EOF > /etc/yum.repos.d/docker-ce.repo\n[docker-ce-stable]\nname=Docker CE Stable - \\$basearch\nbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$(rpm --eval '%{centos_ver}')/\\$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg\nEOF\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which docker)\" ]  && yum remove -y docker-ce docker-ce-cli containerd.io\n\n    yum install -y \"docker-ce${version}\" \"docker-ce-cli${version}\" containerd.io bash-completion\n  fi\n\n  [ -f /usr/share/bash-completion/completions/docker ] && \\\n    cp -f /usr/share/bash-completion/completions/docker /etc/bash_completion.d/\n\n  [ ! -d /etc/docker ] && mkdir /etc/docker\n  cat << EOF > /etc/docker/daemon.json\n{\n  \"data-root\": \"/var/lib/docker\",\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"200m\",\n    \"max-file\": \"5\"\n  },\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 655360,\n      \"Soft\": 655360\n    },\n    \"nproc\": {\n      \"Name\": \"nproc\",\n      \"Hard\": 655360,\n      \"Soft\": 655360\n    }\n  },\n  \"live-restore\": true,\n  \"oom-score-adjust\": -1000,\n  \"max-concurrent-downloads\": 10,\n  \"max-concurrent-uploads\": 10,\n  \"registry-mirrors\": [\n    \"https://yssx4sxy.mirror.aliyuncs.com/\"\n  ]\n}\nEOF\n  sed -i 's|#oom_score = 0|oom_score = -999|' /etc/containerd/config.toml\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///var/run/dockershim.sock\nimage-endpoint: unix:///var/run/dockershim.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n  \n  systemctl enable containerd\n  systemctl restart containerd\n\n  systemctl enable docker\n  systemctl restart docker\n}\n\n\nfunction script::install_containerd() {\n  # 安装 containerd\n  \n  local version=\"-${1:-latest}\"\n  version=\"${version#-latest}\"\n\n  cat << EOF > /etc/yum.repos.d/docker-ce.repo\n[docker-ce-stable]\nname=Docker CE Stable - \\$basearch\nbaseurl=https://mirrors.aliyun.com/docker-ce/linux/centos/$(rpm --eval '%{centos_ver}')/\\$basearch/stable\nenabled=1\ngpgcheck=1\ngpgkey=https://mirrors.aliyun.com/docker-ce/linux/centos/gpg\nEOF\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which runc)\" ]  && yum remove -y runc\n    [ -f \"$(which containerd)\" ]  && yum remove -y containerd.io\n    yum install -y containerd.io\"${version}\" containernetworking bash-completion\n  fi\n\n  [ -d /etc/bash_completion.d ] && crictl completion bash > /etc/bash_completion.d/crictl\n\n  containerd config default > /etc/containerd/config.toml\n  sed -i -e \"s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#registry.k8s.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#https://registry-1.docker.io#https://yssx4sxy.mirror.aliyuncs.com#g\" \\\n         -e \"s#SystemdCgroup = false#SystemdCgroup = true#g\" \\\n         -e \"s#oom_score = 0#oom_score = -999#\" \\\n         -e \"s#max_container_log_line_size = 16384#max_container_log_line_size = 65535#\" \\\n         -e \"s#max_concurrent_downloads = 3#max_concurrent_downloads = 10#g\" /etc/containerd/config.toml\n\n  grep docker.io /etc/containerd/config.toml ||  sed -i -e \"/registry.mirrors]/a\\ \\ \\ \\ \\ \\ \\ \\ [plugins.\\\"io.containerd.grpc.v1.cri\\\".registry.mirrors.\\\"docker.io\\\"]\\n           endpoint = [\\\"https://yssx4sxy.mirror.aliyuncs.com\\\"]\" \\\n       /etc/containerd/config.toml\n\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n  \n  systemctl restart containerd\n  systemctl enable containerd\n}\n\nfunction script::install_cri-o() {\n  # 安装 cri-o\n  \n  local version=\"${1:-latest}\"\n  version=\"${version##latest}\"\n  os=\"CentOS_$(rpm --eval '%{centos_ver}')\" && echo \"${os}\"\n\n  cat << EOF > /etc/yum.repos.d/devel_kubic_libcontainers_stable.repo\n[devel_kubic_libcontainers_stable]\nname=Stable Releases of Upstream github.com/containers packages\ntype=rpm-md\nbaseurl=https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/${os}/\ngpgcheck=1\ngpgkey=https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/${os}/repodata/repomd.xml.key\nenabled=1\n\n[devel_kubic_libcontainers_stable_cri-o]\nname=devel:kubic:libcontainers:stable:cri-o\ntype=rpm-md\nbaseurl=https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/${version}/${os}/\ngpgcheck=1\ngpgkey=https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/${version}/${os}/repodata/repomd.xml.key\nenabled=1\nEOF\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which runc)\" ]  && yum remove -y runc\n    [ -f \"$(which crio)\" ]  && yum remove -y cri-o\n    [ -f \"$(which docker)\" ]  && yum remove -y docker-ce docker-ce-cli containerd.io\n\n    yum install -y runc cri-o bash-completion --disablerepo=docker-ce-stable || if ! yum install -y runc cri-o bash-completion; then exit 1;fi\n  fi\n\n  [ -d /etc/bash_completion.d ] && \\\n    { crictl completion bash >  /etc/bash_completion.d/crictl; \\\n      crio completion bash > /etc/bash_completion.d/crio; \\\n      crio-status completion bash > /etc/bash_completion.d/crio-status; }\n\n  crio config --default > /etc/crio/crio.conf\n  sed -i -e \"s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#registry.k8s.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e 's|#registries = \\[|registries = [\"docker.io\", \"quay.io\"]|g' /etc/crio/crio.conf\n\n  [ -d /etc/containers/registries.conf.d ] && cat << EOF > /etc/containers/registries.conf.d/000-dockerio.conf\n[[registry]]\nprefix = \"docker.io\"\ninsecure = false\nblocked = false\nlocation = \"docker.io\"\n\n[[registry.mirror]]\nlocation = \"yssx4sxy.mirror.aliyuncs.com\"\ninsecure = true\nEOF\n\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///var/run/crio/crio.sock\nimage-endpoint: unix:///var/run/crio/crio.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n\n  sed -i \"s#10.85.0.0/16#${KUBE_POD_SUBNET:-10.85.0.0/16}#g\" /etc/cni/net.d/100-crio-bridge.conf\n  cat << EOF > /etc/cni/net.d/10-crio.conf\n{\n$(grep cniVersion /etc/cni/net.d/100-crio-bridge.conf)\n    \"name\": \"crio\",\n    \"type\": \"flannel\"\n}\nEOF\n  mv /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/10-crio.conf /etc/cni/net.d/200-loopback.conf /tmp/\n  systemctl restart crio\n  systemctl enable crio\n}\n\n\nfunction script::install_kube() {\n  # 安装kube组件\n  \n  local version=\"-${1:-latest}\"\n  version=\"${version#-latest}\"\n\n  repo=\"${version%.*}\"\n  repo=\"${repo//-}\"\n  [ \"${repo}\" == \"\" ] && repo=\"1.29\"\n  \n  cat <<EOF > /etc/yum.repos.d/kubernetes.repo\n[kubernetes]\nname=Kubernetes\nbaseurl=https://pkgs.k8s.io/core:/stable:/v${repo}/rpm/\nenabled=1\ngpgcheck=1\ngpgkey=https://pkgs.k8s.io/core:/stable:/v${repo}/rpm/repodata/repomd.xml.key\nexclude=kubelet kubeadm kubectl cri-tools kubernetes-cni\nEOF\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f /usr/bin/kubeadm ]  && yum remove -y kubeadm\n    [ -f /usr/bin/kubelet ]  && yum remove -y kubelet\n    [ -f /usr/bin/kubectl ]  && yum remove -y kubectl\n\n    yum install -y \"kubeadm${version}\" \"kubelet${version}\" \"kubectl${version}\" --disableexcludes=kubernetes\n  fi\n\n  [ -d /etc/bash_completion.d ] && \\\n    { kubectl completion bash > /etc/bash_completion.d/kubectl; \\\n      kubeadm completion bash > /etc/bash_completion.d/kubadm; }\n\n  [ ! -d /usr/lib/systemd/system/kubelet.service.d ] && mkdir -p /usr/lib/systemd/system/kubelet.service.d\n  cat << EOF > /usr/lib/systemd/system/kubelet.service.d/11-cgroup.conf\n[Service]\nCPUAccounting=true\nMemoryAccounting=true\nBlockIOAccounting=true\nExecStartPre=/bin/bash -c '/bin/mkdir -p /sys/fs/cgroup/{cpuset,memory,hugetlb,systemd,pids,\"cpu,cpuacct\"}/{system,kube,kubepods}.slice||:'\nSlice=kube.slice\nEOF\n  systemctl daemon-reload\n\n  systemctl enable kubelet\n  systemctl restart kubelet\n}\n\n\nfunction script::install_haproxy() {\n  # 安装haproxy\n   \n  local api_servers=\"$*\"\n   \n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f /usr/bin/haproxy ] && yum remove -y haproxy\n    yum install -y haproxy rsyslog\n  fi\n\n  [ ! -f /etc/haproxy/haproxy.cfg_bak ] && cp /etc/haproxy/haproxy.cfg{,_bak}\ncat << EOF > /etc/haproxy/haproxy.cfg\nglobal\n  log /dev/log    local0\n  log /dev/log    local1 notice\n  tune.ssl.default-dh-param 2048\n\ndefaults\n  log global\n  mode http\n  option dontlognull\n  timeout connect 5000ms\n  timeout client 600000ms\n  timeout server 600000ms\n\nlisten stats\n    bind :19090\n    mode http\n    balance\n    stats uri /haproxy_stats\n    stats auth admin:admin123\n    stats admin if TRUE\n\nfrontend kube-apiserver-https\n   mode tcp\n   option tcplog\n   bind :6443\n   default_backend kube-apiserver-backend\n\nbackend kube-apiserver-backend\n    mode tcp\n    balance roundrobin\n    stick-table type ip size 200k expire 30m\n    stick on src\n$(index=1;for h in $api_servers;do echo \"    server apiserver${index} $h:6443 check\";index=$((index+1));done)\nEOF\n\ncat <<EOF > /etc/rsyslog.d/haproxy.conf\nlocal0.* /var/log/haproxy.log\nlocal1.* /var/log/haproxy.log\nEOF\n\n  systemctl enable haproxy\n  systemctl restart haproxy\n  systemctl enable rsyslog\n  systemctl restart rsyslog\n}\n\n\nfunction check::command_exists() {\n  # 检查命令是否存在\n  \n  local cmd=${1}\n  local package=${2}\n\n  if command -V \"$cmd\" > /dev/null 2>&1; then\n    log::info \"[check]\" \"$cmd command exists.\"\n  else\n    log::warning \"[check]\" \"I require $cmd but it's not installed.\"\n    log::warning \"[check]\" \"install $package package.\"\n    command::exec \"127.0.0.1\" \"yum install -y ${package}\"\n    check::exit_code \"$?\" \"check\" \"$package install\" \"exit\"\n  fi\n}\n\n\nfunction check::command() {\n  # 检查用到的命令\n  \n  check::command_exists ssh openssh-clients\n  check::command_exists sshpass sshpass\n  check::command_exists wget wget\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && check::command_exists tar tar\n}\n\n\nfunction check::ssh_conn() {\n  # 检查ssh连通性\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    [ \"$host\" == \"127.0.0.1\" ] && continue\n    command::exec \"${host}\" \"echo 0\"\n    check::exit_code \"$?\" \"check\" \"ssh $host connection\" \"exit\"\n  done\n}\n\n\nfunction check::os() {\n  # 检查os系统支持\n\n  log::info \"[check]\" \"os support: ${OS_SUPPORT}\"\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"\n      [ -f /etc/os-release ] && source /etc/os-release\n      echo client_os:\\${ID:-}\\${VERSION_ID:-}\n      if [[ \\\"${OS_SUPPORT}\\\" == *\\\"\\${ID:-}\\${VERSION_ID:-}\\\"* ]]; then\n        exit 0\n      fi\n      exit 1\n    \"\n    check::exit_code \"$?\" \"check\" \"$host os support\" \"exit\"\n  done\n}\n\n\nfunction check::kernel() {\n  # 检查os kernel 版本\n\n  local version=${1:-}\n  log::info \"[check]\" \"kernel version not less than ${version}\"\n  version=$(echo \"${version}\" | awk -F. '{ printf(\"%d%03d%03d\\n\", $1,$2,$3); }')\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"\n      kernel_version=\\$(uname -r)\n      kernel_version=\\$(echo \\${kernel_version/-*} | awk -F. '{ printf(\\\"%d%03d%03d\\n\\\", \\$1,\\$2,\\$3); }') \n      echo kernel_version \\${kernel_version}\n      [[ \\${kernel_version} -ge ${version} ]] && exit 0 || exit 1\n    \"                                                                                                                                                 \n    check::exit_code \"$?\" \"check\" \"$host kernel version\" \"exit\"\n  done\n\n}\n\nfunction check::apiserver_conn() {\n  # 检查apiserver连通性\n\n  command::exec \"${MGMT_NODE}\" \"kubectl get node\"\n  check::exit_code \"$?\" \"check\" \"conn apiserver\" \"exit\"\n}\n\n\nfunction check::exit_code() {\n  # 检查返回码\n\n  local code=${1:-}\n  local app=${2:-}\n  local desc=${3:-}\n  local exit_script=${4:-}\n\n  if [[ \"${code}\" == \"0\" ]]; then\n    log::info \"[${app}]\" \"${desc} succeeded.\"\n  else\n    log::error \"[${app}]\" \"${desc} failed.\"\n    [[ \"$exit_script\" == \"exit\" ]] && exit \"$code\"\n  fi\n}\n\n\nfunction check::preflight() {\n  # 预检\n  \n  # check command\n  check::command\n\n  # check ssh conn\n  check::ssh_conn\n\n  # check os\n  check::os\n\n  # check os kernel\n  [[ \"${KUBE_NETWORK:-}\" == \"cilium\" && \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]] && check::kernel 4.9.17\n\n  # check apiserver conn\n  if [[ $(( ${ADD_TAG:-0} + ${DEL_TAG:-0} + ${UPGRADE_TAG:-0} + ${RENEW_CERT_TAG:-0} )) -gt 0 ]]; then\n    check::apiserver_conn\n  fi\n\n  # check docker cri\n  [[ \"${KUBE_CRI}\" == \"docker\" && (\"${KUBE_VERSION}\" == \"latest\" || \"${KUBE_VERSION}\" =~ 1.24) ]] && { log::error \"[limit]\" \"$KUBE_CRI is not supported, only [containerd,cri-o]\"; exit 1; }\n}\n\n\nfunction install::package() {\n  # 安装包\n \n  if [[ \"${KUBE_CRI}\" == \"cri-o\" && \"${KUBE_CRI_VERSION}\" == \"latest\" ]]; then\n    KUBE_CRI_VERSION=\"${KUBE_VERSION}\"\n    if [[ \"${KUBE_CRI_VERSION}\" == \"latest\" ]]; then\n      if command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"; then\n        KUBE_CRI_VERSION=\"${COMMAND_OUTPUT#v}\"\n      else\n        log::error \"[install]\" \"get kubernetes stable version error. Please specify the version!\"\n        exit 1\n      fi\n    fi\n    KUBE_CRI_VERSION=\"${KUBE_CRI_VERSION%.*}\"\n  fi\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    # install cri\n    log::info \"[install]\" \"install ${KUBE_CRI} on $host.\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_\"${KUBE_CRI}\")\n      script::install_${KUBE_CRI} $KUBE_CRI_VERSION\n    \"\n    check::exit_code \"$?\" \"install\" \"install ${KUBE_CRI} on $host\"\n\n    # install kube\n    log::info \"[install]\" \"install kube on $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_kube)\n      script::install_kube $KUBE_VERSION\n    \"\n    check::exit_code \"$?\" \"install\" \"install kube on $host\"\n  done\n\n  local apiservers=$MASTER_NODES\n  if [[ \"$apiservers\" == \"127.0.0.1\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"ip -o route get to 8.8.8.8 | sed -n 's/.*src \\([0-9.]\\+\\).*/\\1/p'\"\n    get::command_output \"apiservers\" \"$?\"\n  fi\n\n  if [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n    \"\n    get::command_output \"apiservers\" \"$?\"\n  fi\n  \n  for host in $WORKER_NODES\n  do\n    # install haproxy\n    log::info \"[install]\" \"install haproxy on $host\"\n  command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_haproxy)\n      script::install_haproxy \\\"$apiservers\\\"\n  \"\n    check::exit_code \"$?\" \"install\" \"install haproxy on $host\"\n  done\n  \n  # 10年证书\n  if [[ \"${CERT_YEAR_TAG:-}\" == \"1\" ]]; then\n    local version=\"${KUBE_VERSION}\"\n    \n    if [[ \"${version}\" == \"latest\" ]]; then\n      if command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"; then\n        version=\"${COMMAND_OUTPUT#v}\"\n      else\n        log::error \"[install]\" \"get kubernetes stable version error. Please specify the version!\"\n        exit 1\n      fi\n    fi\n    \n    log::info \"[install]\" \"download kubeadm 10 years certs client\"\n    local certs_file=\"${OFFLINE_DIR}/bins/kubeadm-linux-amd64\"\n    MGMT_NODE=\"127.0.0.1\" utils::download_file \"${GITHUB_PROXY}https://github.com/lework/kubeadm-certs/releases/download/v${version}/kubeadm-linux-amd64\" \"${certs_file}\"\n    \n    for host in $MASTER_NODES $WORKER_NODES\n    do\n      log::info \"[install]\" \"scp kubeadm client to $host\"\n      command::scp \"${host}\" \"${certs_file}\" \"/tmp/kubeadm-linux-amd64\"\n      check::exit_code \"$?\" \"install\" \"scp kubeadm client to $host\" \"exit\"\n\n      command::exec \"${host}\" \"\n        set -e\n        if [[ -f /tmp/kubeadm-linux-amd64 ]]; then\n        [[ -f /usr/bin/kubeadm && ! -f /usr/bin/kubeadm_src ]] && mv -fv /usr/bin/kubeadm{,_src}\n          mv -fv /tmp/kubeadm-linux-amd64 /usr/bin/kubeadm\n          chmod +x /usr/bin/kubeadm\n        else\n          echo \\\"not found /tmp/kubeadm-linux-amd64\\\"\n          exit 1\n        fi\n    \"\n      check::exit_code \"$?\" \"install\" \"$host: use kubeadm 10 years certs client\"\n    done\n  fi\n}\n\n\nfunction init::upgrade_kernel() {\n  # 升级节点内核\n\n  [[ \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]] && return\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    log::info \"[init]\" \"upgrade kernel: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::upgrade_kernel)\n      script::upgrade_kernel\n    \"\n    check::exit_code \"$?\" \"init\" \"upgrade kernel $host\" \"exit\"\n  done\n  \n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"bash -c 'sleep 15 && reboot' &>/dev/null &\"\n    check::exit_code \"$?\" \"init\" \"$host: Wait for 15s to restart\"\n  done\n\n  log::info \"[notice]\" \"Please execute the command again!\" \n  log::access \"[command]\" \"bash $0 ${SCRIPT_PARAMETER// --upgrade-kernel/}\"\n  exit 0\n}\n\n\nfunction cert::renew_node() {\n # 节点证书续期\n \n  local role=\"${1:-control-plane}\"\n  local hosts=\"\"\n  local kubelet_config=\"\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='node-role.kubernetes.io/${role}' -o jsonpath='{range.items[*]}{.metadata.name } {end}'\n  \"\n  get::command_output \"hosts\" \"$?\"\n  \n  for host in ${hosts}\n  do\n    log::info \"[cert]\" \"drain $host\"\n    command::exec \"${MGMT_NODE}\" \"kubectl drain $host --force --ignore-daemonsets --delete-local-data\"\n    check::exit_code \"$?\" \"cert\" \"$host: drain\" \"exit\"\n    sleep 5\n    \n    if [[ \"${role}\" == \"master\" || \"${role}\" == \"control-plane\" ]]; then\n      command::exec \"${host}\" \"cp -rf /etc/kubernetes /etc/kubernetes_\\$(date +%Y-%m-%d)\"\n      check::exit_code \"$?\" \"cert\" \"$host: backup kubernetes config\" \"exit\"\n      \n      command::exec \"${host}\" \"kubeadm certs renew all 2>/dev/null|| kubeadm alpha certs renew all\"\n      check::exit_code \"$?\" \"cert\" \"$host: renew certs\" \"exit\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof etcd) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:2379 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart etcd\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-apiserver) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:6443 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-apiserver\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-controller-manager) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10257 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n       \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-controller-manager\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-scheduler) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10259 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-scheduler\"\n    fi\n\n    log::info \"[cert]\" \"get kubelet config\"\n    command::exec \"${MGMT_NODE}\" \"\n      kubeadm kubeconfig user --org system:nodes --client-name system:node:${host}  --config /etc/kubernetes/kubeadmcfg.yaml || kubeadm alpha kubeconfig user --org system:nodes --client-name system:node:${host}  --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    get::command_output \"kubelet_config\" \"$?\" \"exit\"\n\n    if [[ \"$kubelet_config\" != \"\" ]]; then\n      log::info \"[cert]\" \"copy kubelet config\"\n      command::exec \"${host}\" \"\n        cp /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf_bak\n        echo '$(printf \"%s\" \"${kubelet_config}\" | sed 's#https://.*:#https://127.0.0.1:#g')' > /etc/kubernetes/kubelet.conf\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: copy kubelet config\" \"exit\"\n\n      command::exec \"${host}\" \"rm -rfv /var/lib/kubelet/pki/*\"\n      check::exit_code \"$?\" \"cert\" \"$host: delete kubelet pki files\" \"exit\"\n\n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        systemctl restart kubelet && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10250 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      local status=\"$?\"\n      check::exit_code \"${status}\" \"cert\" \"$host: restart kubelet\"\n      if [[ \"${status}\" == \"0\" ]]; then\n        sleep 5\n        command::exec \"${MGMT_NODE}\" \"kubectl uncordon ${host}\"\n        check::exit_code \"$?\" \"cert\" \"uncordon ${host} node\" \"exit\"\n      fi\n    fi\n  done\n}\n\n\nfunction cert::renew() {\n  # 证书续期\n \n  log::info \"[cert]\" \"renew cluster cert\"\n  cert::renew_node \"control-plane\"\n  cert::renew_node \"worker\"\n \n  log::info \"[cert]\" \"cluster status\"\n  command::exec \"${MGMT_NODE}\" \"\n    echo\n    kubectl get node\n    echo\n    kubeadm certs check-expiration 2>/dev/null || kubeadm alpha certs check-expiration\n  \" && printf \"%s\" \"${COMMAND_OUTPUT}\"\n}\n\n\nfunction init::node_config() {\n  # 初始化节点配置\n\n  local master_index=${master_index:-1}\n  local worker_index=${worker_index:-1}\n  \n  log::info \"[init]\" \"Get $MGMT_NODE InternalIP.\"\n  command::exec \"${MGMT_NODE}\" \"\n    ip -4 route get 8.8.8.8 2>/dev/null | head -1 | awk '{print \\$7}'\n  \"\n  get::command_output \"MGMT_NODE_IP\" \"$?\" \"exit\"\n\n  # master\n  for host in $MASTER_NODES\n  do\n    log::info \"[init]\" \"master: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0} KUBE_APISERVER=${KUBE_APISERVER} SKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n      $(declare -f script::init_node)\n      script::init_node\n   \"\n    check::exit_code \"$?\" \"init\" \"init master $host\" \"exit\"\n\n    # 设置主机名和解析\n    command::exec \"${host}\" \"\n      printf \\\"\\\\n${MGMT_NODE_IP} $KUBE_APISERVER\\\\n$node_hosts\\\" >> /etc/hosts\n      hostnamectl set-hostname ${HOSTNAME_PREFIX}-master-node${master_index}\n    \"\n    check::exit_code \"$?\" \"init\" \"$host set hostname and hostname resolution\"\n\n    # set audit-policy\n    log::info \"[init]\" \"$host: set audit-policy file.\"\n    command::exec \"${host}\" \"\n      [ ! -d etc/kubernetes ] && mkdir -p /etc/kubernetes\n      cat << EOF > /etc/kubernetes/audit-policy.yaml\n# Log all requests at the Metadata level.\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n- level: Metadata\nEOF\n    \"\n    check::exit_code \"$?\" \"init\" \"$host: set audit-policy file\" \"exit\"\n    master_index=$((master_index + 1))\n  done\n   \n  # worker\n  for host in $WORKER_NODES\n  do\n    log::info \"[init]\" \"worker: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0} KUBE_APISERVER=${KUBE_APISERVER} SKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n      $(declare -f script::init_node)\n      script::init_node\n    \"\n    check::exit_code \"$?\" \"init\" \"init worker $host\" \"exit\"\n\n    # 设置主机名和解析\n    command::exec \"${host}\" \"\n      printf \\\"\\\\n127.0.0.1 $KUBE_APISERVER\\\\n$node_hosts\\\" >> /etc/hosts\n      hostnamectl set-hostname ${HOSTNAME_PREFIX}-worker-node${worker_index}\n    \"\n    worker_index=$((worker_index + 1))\n  done\n}\n\n\nfunction init::node() {\n  # 初始化节点\n  \n  init::upgrade_kernel\n\n  local node_hosts=\"\"\n  local i=1\n  for h in $MASTER_NODES\n  do\n    node_hosts=\"${node_hosts}\\n$h ${HOSTNAME_PREFIX}-master-node${i}\"\n    i=$((i + 1))\n  done\n  \n  local i=1\n  for h in $WORKER_NODES\n  do\n    node_hosts=\"${node_hosts}\\n$h ${HOSTNAME_PREFIX}-worker-node${i}\"\n    i=$((i + 1))\n  done\n\n  init::node_config\n}\n\n\nfunction init::add_node() {\n  # 初始化添加的节点\n  \n  init::upgrade_kernel\n\n  local master_index=0\n  local worker_index=0\n  local node_hosts=\"\"\n  local add_node_hosts=\"\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address } {end}' | awk '{print \\$1}'\n  \"\n  get::command_output \"MGMT_NODE\" \"$?\" \"exit\"\n\n  # 获取现有集群节点主机名\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.metadata.name }\\\\n{end}'\n  \"\n  get::command_output \"node_hosts\" \"$?\" \"exit\"\n  \n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    if [[ $node_hosts == *\"$host\"* ]]; then\n      log::error \"[init]\" \"The host $host is already in the cluster!\"\n      exit 1\n    fi\n  done\n  \n  if [[ \"$MASTER_NODES\" != \"\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].metadata.name}' |grep -Eo 'node[0-9]*'|grep -Eo '[0-9]*'|awk -F ' ' 'BEGIN {max = 0} {if (\\$0+0 > max+0) max=\\$0} END {print max}'\n    \"\n    get::command_output \"master_index\" \"$?\" \"exit\"\n    master_index=$(( master_index + 1 ))\n    local i=$master_index\n    for host in $MASTER_NODES\n    do\n      add_node_hosts=\"${add_node_hosts}\\n${host:-} ${HOSTNAME_PREFIX}-master-node${i}\"\n      i=$((i + 1))\n    done\n  fi\n\n  if [[ \"$WORKER_NODES\" != \"\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].metadata.name}'| grep -Eo 'node[0-9]*'|grep -Eo '[0-9]*'|awk 'BEGIN {max = 0} {if (\\$0+0 > max+0) max=\\$0} END {print max}' || echo 0\n    \"\n    get::command_output \"worker_index\" \"$?\" \"exit\"\n    worker_index=$(( worker_index + 1 ))\n    local i=$worker_index\n    for host in $WORKER_NODES\n    do\n      add_node_hosts=\"${add_node_hosts}\\n${host:-} ${HOSTNAME_PREFIX}-worker-node${i}\"\n      i=$((i + 1))\n    done\n  fi\n  #向集群节点添加新增的节点主机名解析 \n  for host in $(echo -ne \"$node_hosts\" | awk '{print $1}')\n  do\n     command::exec \"${host}\" \"\n       printf \\\"$add_node_hosts\\\" >> /etc/hosts\n     \"\n     check::exit_code \"$?\" \"init\" \"$host add new node hostname resolution\"\n  done\n\n  node_hosts=\"${node_hosts}\\n${add_node_hosts}\"\n  init::node_config\n}\n\n\nfunction kubeadm::init() {\n  # 集群初始化\n  \n  log::info \"[kubeadm init]\" \"kubeadm init on ${MGMT_NODE}\"\n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: set kubeadmcfg.yaml\"\n  command::exec \"${MGMT_NODE}\" \"\n    PAUSE_VERSION=$(kubeadm config images list 2>/dev/null | awk -F: '/pause/ {print $2}')\n    cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: InitConfiguration\n${kubelet_nodeRegistration}\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\nipvs:\n  minSyncPeriod: 5s\n  syncPeriod: 5s\n  # ipvs 负载策略\n  scheduler: 'wrr'\n\n---\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nmaxPods: 200\ncgroupDriver: systemd\nruntimeRequestTimeout: 5m\n# 此配置保证了 kubelet 能在 swap 开启的情况下启动\nfailSwapOn: false\nnodeStatusUpdateFrequency: 5s\nrotateCertificates: true\nimageGCLowThresholdPercent: 70\nimageGCHighThresholdPercent: 80\n# 软驱逐阀值\nevictionSoft:\n  imagefs.available: 15%\n  memory.available: 512Mi\n  nodefs.available: 15%\n  nodefs.inodesFree: 10%\n# 达到软阈值之后，持续时间超过多久才进行驱逐\nevictionSoftGracePeriod:\n  imagefs.available: 3m\n  memory.available: 1m\n  nodefs.available: 3m\n  nodefs.inodesFree: 1m\n# 硬驱逐阀值\nevictionHard:\n  imagefs.available: 10%\n  memory.available: 256Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionMaxPodGracePeriod: 30\n# 节点资源预留\nkubeReserved:\n  cpu: 200m\\$(if [[ \\$(cat /proc/meminfo | awk '/MemTotal/ {print \\$2}') -gt 3670016 ]]; then echo -e '\\n  memory: 256Mi';fi)\n  ephemeral-storage: 1Gi\nsystemReserved:\n  cpu: 300m\\$(if [[ \\$(cat /proc/meminfo | awk '/MemTotal/ {print \\$2}') -gt 3670016 ]]; then echo -e '\\n  memory: 512Mi';fi)\n  ephemeral-storage: 1Gi\nkubeReservedCgroup: /kube.slice\nsystemReservedCgroup: /system.slice\nenforceNodeAllocatable: \n- pods\n\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\nkubernetesVersion: $KUBE_VERSION\ncontrolPlaneEndpoint: $KUBE_APISERVER:6443\nnetworking:\n  dnsDomain: $KUBE_DNSDOMAIN\n  podSubnet: $KUBE_POD_SUBNET\n  serviceSubnet: $KUBE_SERVICE_SUBNET\nimageRepository: $KUBE_IMAGE_REPO\napiServer:\n  certSANs:\n  - 127.0.0.1\n  - $KUBE_APISERVER\n$(for h in $MASTER_NODES;do echo \"  - $h\";done)\n  extraArgs:\n    event-ttl: '720h'\n    service-node-port-range: '30000-50000'\n    # 审计日志相关配置\n    audit-log-maxage: '20'\n    audit-log-maxbackup: '10'\n    audit-log-maxsize: '100'\n    audit-log-path: /var/log/kube-audit/audit.log\n    audit-policy-file: /etc/kubernetes/audit-policy.yaml\n  extraVolumes:\n  - name: audit-config\n    hostPath: /etc/kubernetes/audit-policy.yaml\n    mountPath: /etc/kubernetes/audit-policy.yaml\n    readOnly: true\n    pathType: File\n  - name: audit-log\n    hostPath: /var/log/kube-audit\n    mountPath: /var/log/kube-audit\n    pathType: DirectoryOrCreate\n  - name: localtime\n    hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    readOnly: true\n    pathType: File\ncontrollerManager:\n  extraArgs:\n    bind-address: 0.0.0.0\n    node-cidr-mask-size: '24'\n    node-monitor-grace-period: '20s'\n    terminated-pod-gc-threshold: '30'\n    cluster-signing-duration: 87600h\n    feature-gates: RotateKubeletServerCertificate=true\n  extraVolumes:\n  - hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    name: localtime\n    readOnly: true\n    pathType: File\nscheduler:\n  extraArgs:\n    bind-address: 0.0.0.0\n  extraVolumes:\n  - hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    name: localtime\n    readOnly: true\n    pathType: File\n$(if [[ \"${KUBE_VERSION}\" == \"1.21.1\" ]]; then\necho \"dns:\n  type: CoreDNS\n  imageRepository: docker.io\n  imageTag: 1.8.0\"\nfi)\nEOF\n\"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: set kubeadmcfg.yaml\" \"exit\"\n  \n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: kubeadm init start.\"\n  command::exec \"${MGMT_NODE}\" \"kubeadm init --config=/etc/kubernetes/kubeadmcfg.yaml --upload-certs\"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: kubeadm init\" \"exit\"\n  \n  sleep 3\n  \n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: set kube config.\"\n  command::exec \"${MGMT_NODE}\" \"\n     mkdir -p \\$HOME/.kube\n     sudo cp -f /etc/kubernetes/admin.conf \\$HOME/.kube/config\n  \"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: set kube config\" \"exit\"\n  if [[ \"$(echo \"$MASTER_NODES\" | wc -w)\" == \"1\" ]]; then\n    log::info \"[kubeadm init]\" \"${MGMT_NODE}: delete master taint\"\n    command::exec \"${MGMT_NODE}\" \"kubectl taint nodes --all node-role.kubernetes.io/master- || kubectl taint nodes --all node-role.kubernetes.io/control-plane-\"\n    check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: delete master taint\"\n  fi\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --user=kubelet-bootstrap\n    kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes\n    kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes\n  \"\n  check::exit_code \"$?\" \"kubeadm init\" \"Auto-Approve kubelet cert csr\" \"exit\"\n}\n\n\nfunction kubeadm::join() {\n  # 加入集群\n\n  log::info \"[kubeadm join]\" \"master: get join token and cert info\"\n  command::exec \"${MGMT_NODE}\" \"\n    openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'\n  \"\n  get::command_output \"CACRT_HASH\" \"$?\" \"exit\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm init phase upload-certs --upload-certs --config /etc/kubernetes/kubeadmcfg.yaml 2>> /dev/null | tail -1\n  \"\n  get::command_output \"INTI_CERTKEY\" \"$?\" \"exit\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm token create\n  \"\n  get::command_output \"INIT_TOKEN\" \"$?\" \"exit\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm config images list 2>/dev/null | awk -F: '/pause/ {print \\$2}'\n  \"\n  get::command_output \"PAUSE_VERSION\" \"$?\"\n\n  for host in $MASTER_NODES\n  do\n    [[ \"${MGMT_NODE}\" == \"$host\" ]] && continue\n    log::info \"[kubeadm join]\" \"master $host join cluster.\"\n    command::exec \"${host}\" \"\n      cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: $KUBE_APISERVER:6443\n    caCertHashes:\n    - sha256:${CACRT_HASH:-}\n    token: ${INIT_TOKEN}\n  timeout: 5m0s\ncontrolPlane:\n  certificateKey: ${INTI_CERTKEY:-}\n${kubelet_nodeRegistration}\nEOF\n      kubeadm join --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"master $host join cluster\"\n\n    log::info \"[kubeadm join]\" \"$host: set kube config.\"\n    command::exec \"${host}\" \"\n      mkdir -p \\$HOME/.kube\n      sudo cp -f /etc/kubernetes/admin.conf \\$HOME/.kube/config\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"$host: set kube config\" \"exit\"\n    \n    command::exec \"${host}\" \"\n      sed -i 's#.*$KUBE_APISERVER#127.0.0.1 $KUBE_APISERVER#g' /etc/hosts\n    \"\n  done\n\n  for host in $WORKER_NODES\n  do\n    log::info \"[kubeadm join]\" \"worker $host join cluster.\"\n    command::exec \"${host}\" \"\n      mkdir -p /etc/kubernetes/manifests\n      cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: $KUBE_APISERVER:6443\n    caCertHashes:\n    - sha256:${CACRT_HASH:-}\n    token: ${INIT_TOKEN}\n  timeout: 5m0s\n${kubelet_nodeRegistration}\nEOF\n      kubeadm join --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"worker $host join cluster\"\n  \n    log::info \"[kubeadm join]\" \"set $host worker node role.\"\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/master,!node-role.kubernetes.io/control-plane' | grep '<none>' | awk '{print \\\"kubectl label node \\\" \\$1 \\\" node-role.kubernetes.io/worker= --overwrite\\\" }' | bash\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"set $host worker node role\"\n  done\n}\n\n\nfunction kube::wait() {\n  # 等待资源完成\n\n  local app=$1\n  local namespace=$2\n  local resource=$3\n  local selector=${4:-}\n\n  sleep 3\n  log::info \"[waiting]\" \"waiting $app\"\n  command::exec \"${MGMT_NODE}\" \"\n    $(declare -f utils::retry)\n    utils::retry 6 kubectl wait --namespace ${namespace} \\\n    --for=condition=ready ${resource} \\\n    --selector=$selector \\\n    --timeout=60s\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"waiting\" \"$app ${resource} ready\"\n  return \"$status\"\n}\n\n\nfunction kube::apply() {\n  # 应用manifest\n\n  local file=$1\n\n  log::info \"[apply]\" \"$file\"\n  command::exec \"${MGMT_NODE}\" \"\n    $(declare -f utils::retry)\n    if [ -f \\\"$file\\\" ]; then\n      utils::retry 6 kubectl apply --wait=true --timeout=10s -f \\\"$file\\\"\n    else\n      utils::retry 6 \\\"cat <<EOF | kubectl apply --wait=true --timeout=10s -f -\n\\$(printf \\\"%s\\\" \\\"${2:-}\\\")\nEOF\n      \\\"\n    fi\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"apply\" \"add $file\" \"exit\"\n  return \"$status\"\n}\n\n\nfunction kube::status() {\n  # 集群状态\n  \n  sleep 5\n  log::info \"[cluster]\" \"cluster status\"\n  command::exec \"${MGMT_NODE}\" \"\n     echo\n     kubectl get node -o wide\n     echo\n     kubectl get pods -A\n  \" && printf \"%s\" \"${COMMAND_OUTPUT}\"\n}\n\n\nfunction config::haproxy_backend() {\n  # 添加或删除haproxy的后端server\n\n  local action=${1:-add}\n  local action_cmd=\"\"\n  local master_nodes\n  \n  if [[ \"$MASTER_NODES\" == \"\" || \"$MASTER_NODES\" == \"127.0.0.1\" ]]; then\n    return\n  fi\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n  \"\n  get::command_output \"master_nodes\" \"$?\" \"exit\"\n  \n  for m in $MASTER_NODES\n  do\n    if [[ \"${action}\" == \"add\" ]]; then\n      num=$(echo \"${m}\"| awk -F'.' '{print $4}')\n      action_cmd=\"${action_cmd}\\necho \\\"    server apiserver${num} ${m}:6443 check\\\" >> /etc/haproxy/haproxy.cfg\"\n    else\n      [[ \"${master_nodes}\" == *\"${m}\"* ]] || return\n      action_cmd=\"${action_cmd}\\n sed -i -e \\\"/${m}/d\\\" /etc/haproxy/haproxy.cfg\"\n    fi\n  done\n        \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n  \"\n  get::command_output \"worker_nodes\" \"$?\"\n  \n  for host in ${worker_nodes:-}\n  do\n    log::info \"[config]\" \"worker ${host}: ${action} apiserver from haproxy\"\n    command::exec \"${host}\" \"\n      $(echo -ne \"${action_cmd}\")\n      haproxy -c -f /etc/haproxy/haproxy.cfg && systemctl reload haproxy\n    \"\n    check::exit_code \"$?\" \"config\" \"worker ${host}: ${action} apiserver(${m}) from haproxy\"\n  done\n}\n\n\nfunction config::etcd_snapshot() {\n  # 更新 etcd 备份副本\n\n  command::exec \"${MGMT_NODE}\" \"\n    count=\\$(kubectl get node --selector='!node-role.kubernetes.io/worker' --no-headers | wc -l)\n    kubectl -n kube-system patch cronjobs etcd-snapshot --patch \\\"\nspec:\n  jobTemplate:\n    spec:\n      completions: \\${count:-1}\n      parallelism: \\${count:-1}\n\\\"\n  \"\n  check::exit_code \"$?\" \"config\" \"etcd-snapshot completions options\"\n}\n\n\nfunction get::command_output() {\n   # 获取命令的返回值\n\n   local app=\"$1\"\n   local status=\"$2\"\n   local is_exit=\"${3:-}\"\n   \n   if [[ \"$status\" == \"0\" && \"${COMMAND_OUTPUT}\" != \"\" ]]; then\n     log::info \"[command]\" \"get $app value succeeded.\"\n     eval \"$app=\\\"${COMMAND_OUTPUT}\\\"\"\n   else\n     log::error \"[command]\" \"get $app value failed.\"\n     [[ \"$is_exit\" == \"exit\" ]] && exit \"$status\"\n   fi\n   return \"$status\"\n}\n\n\nfunction get::ingress_conn(){\n  # 获取ingress连接地址\n\n  local port=\"${1:-80}\"\n  local ingress_name=\"${2:-ingress-${KUBE_INGRESS}-controller}\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range .items[*]}{ .status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.status.conditions[?(@.status == \\\"True\\\")].status}{\\\"\\\\n\\\"}{end}' | awk '{if(\\$2==\\\"True\\\")a=\\$1}END{print a}'\n  \"\n  get::command_output \"node_ip\" \"$?\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get svc --all-namespaces -o go-template=\\\"{{range .items}}{{if eq .metadata.name \\\\\\\"${ingress_name}\\\\\\\"}}{{range.spec.ports}}{{if eq .port ${port}}}{{.nodePort}}{{end}}{{end}}{{end}}{{end}}\\\"\n  \"\n  \n  get::command_output \"node_port\" \"$?\"\n \n  INGRESS_CONN=\"${node_ip:-nodeIP}:${node_port:-nodePort}\"\n}\n\n\nfunction add::ingress() {\n  # 添加ingress组件\n\n  local add_ingress_demo=0\n\n  if [[ \"$KUBE_INGRESS\" == \"nginx\" ]]; then\n    log::info \"[ingress]\" \"add ingress-nginx\"\n    \n    local ingress_nginx_file=\"${OFFLINE_DIR}/manifests/ingress-nginx.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v${INGRESS_NGINX}/deploy/static/provider/baremetal/deploy.yaml\" \"${ingress_nginx_file}\"\n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#k8s.gcr.io/ingress-nginx#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#registry.k8s.io/ingress-nginx#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#@sha256:.*\\$##g' '${ingress_nginx_file}'\n    \"\n    check::exit_code \"$?\" \"ingress\" \"change ingress-nginx manifests\"\n    kube::apply \"${ingress_nginx_file}\"\n\n    kube::wait \"ingress-nginx\" \"ingress-nginx\" \"pod\" \"app.kubernetes.io/component=controller\" && add_ingress_demo=1\n\n    command::exec \"${MGMT_NODE}\" \"kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission\"\n    check::exit_code \"$?\" \"ingress\" \"delete ingress-ngin ValidatingWebhookConfiguration\"\n\n\n    log::info \"[ingress]\"  \"set nginx is default ingress class\"\n    command::exec \"${MGMT_NODE}\" \"\n      default_class=\\\"\\$(kubectl get ingressclass -A -o jsonpath='{.items[?(@.metadata.annotations.ingressclass\\\\.kubernetes\\\\.io/is-default-class==\\\\\\\"true\\\\\\\")].metadata.name}')\\\"\n      if [ \\\"\\${default_class:-}\\\" != \\\"\\\" ]; then\n         kubectl patch ingressclass \\${default_class} -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"ingressclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\n      fi\n      kubectl patch ingressclass nginx -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"ingressclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\n    \"\n    check::exit_code \"$?\" \"ingress\" \"set nginx is default ingress class\"\n\n  elif [[ \"$KUBE_INGRESS\" == \"traefik\" ]]; then\n    log::info \"[ingress]\" \"add ingress-traefik\"\n    kube::apply \"traefik\" \"\"\"\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: ingress-traefik-controller\nrules:\n  - apiGroups:\n      - ''\n    resources:\n      - services\n      - endpoints\n      - secrets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n      - networking.k8s.io\n    resources:\n      - ingresses\n      - ingressclasses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: ingress-traefik-controller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: ingress-traefik-controller\nsubjects:\n  - kind: ServiceAccount\n    name: ingress-traefik-controller\n    namespace: default\n--- \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: ingress-traefik-controller\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ingress-traefik-controller\n  labels:\n    app: ingress-traefik-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ingress-traefik-controller\n  template:\n    metadata:\n      labels:\n        app: ingress-traefik-controller\n    spec:\n      serviceAccountName: ingress-traefik-controller\n      containers:\n        - name: traefik\n          image: traefik:v${TRAEFIK_VERSION}\n          args:\n            - --api.debug=true\n            - --api.insecure=true\n            - --log=true\n            - --log.level=debug\n            - --ping=true\n            - --accesslog=true\n            - --entrypoints.http.Address=:80\n            - --entrypoints.https.Address=:443\n            - --entrypoints.traefik.Address=:8080\n            - --providers.kubernetesingress\n            - --serverstransport.insecureskipverify=true\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: admin\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 2\n            httpGet:\n              path: /ping\n              port: admin\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /ping\n              port: admin\n              scheme: HTTP\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          resources:\n            limits:\n              cpu: 250m\n              memory: 128Mi\n            requests:\n              cpu: 100m\n              memory: 64Mi\n          securityContext:\n            capabilities:\n              add:\n              - NET_BIND_SERVICE\n              drop:\n              - ALL\n      restartPolicy: Always\n      serviceAccount: ingress-traefik-controller\n      serviceAccountName: ingress-traefik-controller\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-traefik-controller\nspec:\n  type: NodePort\n  selector:\n    app: ingress-traefik-controller\n  ports:\n    - protocol: TCP\n      port: 80\n      name: http\n      targetPort: 80\n    - protocol: TCP\n      port: 443\n      name: https\n      targetPort: 443\n    - protocol: TCP\n      port: 8080\n      name: admin\n      targetPort: 8080\n\"\"\"\n    kube::wait \"traefik\" \"default\" \"pod\" \"app=ingress-traefik-controller\" && add_ingress_demo=1\n  else\n    log::warning \"[ingress]\" \"No $KUBE_INGRESS config.\"\n  fi\n\n  if [[ \"$add_ingress_demo\" == \"1\" ]]; then\n    log::info \"[ingress]\" \"add ingress default-http-backend\"\n    kube::apply \"default-http-backend\" \"\"\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app.kubernetes.io/name: default-http-backend\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: default-http-backend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: default-http-backend\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: default-http-backend\n        image: ${KUBE_IMAGE_REPO}/defaultbackend-amd64:1.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: default-http-backend\n\"\"\"\n    log::info \"[ingress]\" \"add ingress app demo\"\n    kube::apply \"ingress-demo-app\" \"\"\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-demo-app\n  labels:\n    app: ingress-demo-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ingress-demo-app\n  template:\n    metadata:\n      labels:\n        app: ingress-demo-app\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami:v1.10.1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-demo-app\nspec:\n  type: ClusterIP\n  selector:\n    app: ingress-demo-app\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-demo-app\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: app.demo.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: ingress-demo-app\n            port:\n              number: 80\n\"\"\"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:app.demo.com' http://${INGRESS_CONN}\"\n    fi\n  fi\n}\n\n\nfunction add::network() {\n  # 添加network组件\n\n  if [[ \"$KUBE_NETWORK\" == \"flannel\" ]]; then\n    log::info \"[network]\" \"add flannel\"\n    \n    local flannel_file=\"${OFFLINE_DIR}/manifests/kube-flannel.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/flannel-io/flannel/v${FLANNEL_VERSION}/Documentation/kube-flannel.yml\" \"${flannel_file}\"\n    \n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#10.244.0.0/16#${KUBE_POD_SUBNET}#g' \\\n             -e 's#quay.io/coreos#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#docker.io/flannel#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#namespace: kube-system#namespace: kube-flannel#g' \\\n             -e 's#\\\"Type\\\": \\\"vxlan\\\"#\\\"Type\\\": \\\"${KUBE_FLANNEL_TYPE}\\\"#g' \\\"${flannel_file}\\\"\n      if [[ \\\"${KUBE_FLANNEL_TYPE}\\\" == \\\"vxlan\\\" ]]; then\n        sed -i 's#\\\"Type\\\": \\\"vxlan\\\"#\\\"Type\\\": \\\"vxlan\\\", \\\"DirectRouting\\\": true#g' \\\"${flannel_file}\\\"\n      fi\n    \"\n    check::exit_code \"$?\" \"flannel\" \"change flannel pod subnet\"\n    kube::apply \"${flannel_file}\"\n    kube::wait \"flannel\" \"kube-flannel\" \"pods\" \"app=flannel\" \n\n  elif [[ \"$KUBE_NETWORK\" == \"calico\" ]]; then\n    log::info \"[network]\" \"add calico\"\n    utils::download_file \"https://raw.githubusercontent.com/projectcalico/calico/v${CALICO_VERSION}/manifests/calico.yaml\" \"${OFFLINE_DIR}/manifests/calico.yaml\"\n    utils::download_file \"https://raw.githubusercontent.com/projectcalico/calico/v${CALICO_VERSION}/manifests/calicoctl.yaml\" \"${OFFLINE_DIR}/manifests/calicoctl.yaml\"\n    \n    command::exec \"${MGMT_NODE}\" \"\n      sed -i 's#value: \\\"Always\\\"#value: \\\"CrossSubnet\\\"#g' \\\"${OFFLINE_DIR}/manifests/calico.yaml\\\"\n    \"\n    check::exit_code \"$?\" \"network\" \"change calico version to ${CALICO_VERSION}\"\n    \n    kube::apply \"${OFFLINE_DIR}/manifests/calico.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/calicoctl.yaml\"\n    kube::wait \"calico-kube-controllers\" \"kube-system\" \"pods\" \"k8s-app=calico-kube-controllers\"\n    kube::wait \"calico-node\" \"kube-system\" \"pods\" \"k8s-app=calico-node\"\n\n  elif [[ \"$KUBE_NETWORK\" == \"cilium\" ]]; then \n    log::info \"[network]\" \"add cilium\"\n\n    CILIUM_CLI_VERSION=$(curl -s \"${GITHUB_PROXY}https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt\")\n    CLI_ARCH=amd64\n    if [ \"$(uname -m)\" = \"aarch64\" ]; then CLI_ARCH=arm64; fi\n    utils::download_file \"${GITHUB_PROXY}https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz\"  \"${OFFLINE_DIR}/cilium-linux-${CLI_ARCH}.tar.gz\"\n    tar xzvfC \"${OFFLINE_DIR}/cilium-linux-${CLI_ARCH}.tar.gz\" /usr/local/bin\n    [ -d /etc/bash_completion.d ] && cilium completion bash > /etc/bash_completion.d/cilium\n\n    cilium install --version \"${CILIUM_VERSION}\" \\\n                   --set ipam.mode=cluster-pool \\\n                   --set ipam.Operator.clusterPoolIPv4PodCIDRList=[\"{KUBE_POD_SUBNET}\"] \\\n                   --set ipam.Operator.clusterPoolIPv4MaskSize=24 \\\n                   --set kubeProxyReplacement=false\n\n    cilium status --wait\n    cilium hubble enable --ui\n   \n    log::info \"[monitor]\" \"add hubble-ui ingress\"\n    kube::apply \"hubble-ui ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hubble-ui\n  namespace: kube-system\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: hubble-ui.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: hubble-ui\n            port:\n              number: 80\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then                                                                                                                                            \n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:hubble-ui.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[network]\" \"No $KUBE_NETWORK config.\"\n  fi\n}\n\n\nfunction add::addon() {\n  # 添加addon组件\n\n  if [[ \"$KUBE_ADDON\" == \"metrics-server\" ]]; then\n    log::info \"[addon]\" \"download metrics-server manifests\"\n    local metrics_server_file=\"${OFFLINE_DIR}/manifests/metrics-server.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/kubernetes-sigs/metrics-server/releases/download/v${METRICS_SERVER_VERSION}/components.yaml\" \"${metrics_server_file}\"\n  \n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#k8s.gcr.io/metrics-server#$KUBE_IMAGE_REPO#g' \\\n             -e 's#registry.k8s.io/metrics-server#$KUBE_IMAGE_REPO#g' \\\n             -e '/--kubelet-preferred-address-types=.*/d' \\\n             -e 's/\\\\(.*\\\\)- --secure-port=\\\\(.*\\\\)/\\\\1- --secure-port=\\\\2\\\\n\\\\1- --kubelet-insecure-tls\\\\n\\\\1- --kubelet-preferred-address-types=InternalIP,InternalDNS,ExternalIP,ExternalDNS,Hostname/g' \\\n             \\\"${metrics_server_file}\\\"\n    \"\n    check::exit_code \"$?\" \"addon\" \"change metrics-server parameter\"\n    kube::apply \"${metrics_server_file}\"\n    kube::wait \"metrics-server\" \"kube-system\" \"pod\" \"k8s-app=metrics-server\"\n  elif [[ \"$KUBE_ADDON\" == \"nodelocaldns\" ]]; then\n    log::info \"[addon]\" \"download nodelocaldns manifests\"\n    local nodelocaldns_file=\"${OFFLINE_DIR}/manifests/nodelocaldns.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml\" \"${nodelocaldns_file}\"\n  \n    command::exec \"${MGMT_NODE}\" \"\n      cluster_dns=\\$(kubectl -n kube-system get svc kube-dns -o jsonpath={.spec.clusterIP})\n      sed -i -e \\\"s#k8s.gcr.io/dns#${KUBE_IMAGE_REPO}#g\\\" \\\n             -e \\\"s#registry.k8s.io/dns#${KUBE_IMAGE_REPO}#g\\\" \\\n             -e \\\"s/__PILLAR__CLUSTER__DNS__/\\$cluster_dns/g\\\" \\\n             -e \\\"s/__PILLAR__UPSTREAM__SERVERS__/\\$cluster_dns/g\\\" \\\n             -e \\\"s/__PILLAR__LOCAL__DNS__/169.254.20.10/g\\\" \\\n             -e \\\"s/[ |,]__PILLAR__DNS__SERVER__//g\\\" \\\n             -e \\\"s/__PILLAR__DNS__DOMAIN__/$KUBE_DNSDOMAIN/g\\\" \\\n             \\\"${nodelocaldns_file}\\\"\n    \"\n    check::exit_code \"$?\" \"addon\" \"change nodelocaldns parameter\"\n    kube::apply \"${nodelocaldns_file}\"\n    kube::wait \"node-local-dns\" \"kube-system\" \"pod\" \"k8s-app=node-local-dns\"\n  else\n    log::warning \"[addon]\" \"No $KUBE_ADDON config.\"\n  fi\n}\n\n\nfunction add::monitor() {\n  # 添加监控组件\n  \n  if [[ \"$KUBE_MONITOR\" == \"prometheus\" ]]; then\n    log::info \"[monitor]\" \"add prometheus\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/prometheus-operator/kube-prometheus/archive/v${KUBE_PROMETHEUS_VERSION}.zip\" \"${OFFLINE_DIR}/manifests/prometheus.zip\" \"unzip\"\n   \n    log::info \"[monitor]\" \"apply prometheus manifests\"\n    command::exec \"${MGMT_NODE}\" \"\n      $(declare -f utils::retry)\n      cd \\\"${OFFLINE_DIR}/manifests/kube-prometheus-${KUBE_PROMETHEUS_VERSION}\\\" \\\n      && sed -i -e \\\"s#registry.k8s.io/prometheus-adapter#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#registry.k8s.io/kube-state-metrics#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/brancz#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/prometheus-operator#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/prometheus#${KUBE_IMAGE_REPO}#g\\\" \\\n                   manifests/*.yaml  \\\n      && utils::retry 6 kubectl apply --server-side --wait=true --timeout=10s -f manifests/setup/ \\\n      && until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo ''; done \\\n      && utils::retry 6 kubectl apply  --wait=true --timeout=10s -f manifests/\n    \"\n    check::exit_code \"$?\" \"apply\" \"add prometheus\"\n    kube::wait \"prometheus\" \"monitoring\" \"pods --all\"\n\n    kube::apply \"controller-manager and scheduler prometheus discovery service\" \"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: kube-system\n  name: kube-scheduler-prometheus-discovery\n  labels:\n    k8s-app: kube-scheduler\nspec:\n  selector:\n    component: kube-scheduler\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: https-metrics\n    port: 10259\n    targetPort: 10259\n    protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: kube-system\n  name: kube-controller-manager-prometheus-discovery\n  labels:\n    k8s-app: kube-controller-manager\nspec:\n  selector:\n    component: kube-controller-manager\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: https-metrics\n    port: 10257\n    targetPort: 10257\n    protocol: TCP\n    \"\n    \n    log::info \"[monitor]\" \"add prometheus ingress\"\n    kube::apply \"prometheus ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: grafana.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: grafana\n            port:\n              number: 3000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: prometheus.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: prometheus-k8s\n            port:\n              number: 9090\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: alertmanager\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: alertmanager.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: alertmanager-main\n            port:\n              number: 9093\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:grafana.monitoring.cluster.local' http://${INGRESS_CONN}; auth: admin/admin\"\n      log::access \"[ingress]\" \"curl -H 'Host:prometheus.monitoring.cluster.local' http://${INGRESS_CONN}\"\n      log::access \"[ingress]\" \"curl -H 'Host:alertmanager.monitoring.cluster.local' http://${INGRESS_CONN}\"\n    fi\n    \n  else\n    log::warning \"[addon]\" \"No $KUBE_MONITOR config.\"\n  fi\n}\n\n\nfunction add::log() {\n  # 添加log组件\n\n  if [[ \"$KUBE_LOG\" == \"elasticsearch\" ]]; then\n    log::info \"[log]\" \"add elasticsearch\"\n    kube::apply \"elasticsearch\" \"\n---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: kube-logging\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch\n  namespace: kube-logging\n  labels:\n    app: elasticsearch\nspec:\n  selector:\n    app: elasticsearch\n  clusterIP: None\n  ports:\n    - port: 9200\n      name: rest\n    - port: 9300\n      name: inter-node\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-cluster\n  namespace: kube-logging\nspec:\n  serviceName: elasticsearch\n  replicas: 3\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - {key: app,operator: In,values: [\\\"elasticsearch\\\"]}\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: elasticsearch\n        image: ${KUBE_IMAGE_REPO}/elasticsearch:${ELASTICSEARCH_VERSION}\n        resources:\n            limits:\n              cpu: 1000m\n            requests:\n              cpu: 100m\n        ports:\n        - containerPort: 9200\n          name: rest\n          protocol: TCP\n        - containerPort: 9300\n          name: inter-node\n          protocol: TCP\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        env:\n          - name: cluster.name\n            value: k8s-logs\n          - name: node.name\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: discovery.seed_hosts\n            value: 'es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch'\n          - name: cluster.initial_master_nodes\n            value: 'es-cluster-0,es-cluster-1,es-cluster-2'\n          - name: xpack.security.enabled\n            value: 'false'\n          - name: ES_JAVA_OPTS\n            value: '-Xms512m -Xmx512m'\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n      initContainers:\n      - name: increase-vm-max-map\n        image: alpine:3.9\n        command: ['sysctl', '-w', 'vm.max_map_count=262144']\n        securityContext:\n          privileged: true\n      - name: increase-fd-ulimit\n        image: alpine:3.9\n        command: ['sh', '-c', 'ulimit -n 65536']\n        securityContext:\n          privileged: true\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: elasticsearch\n  namespace: kube-logging\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: elasticsearch.logging.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: elasticsearch\n            port:\n              number: 9200\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  labels:\n    app: kibana\nspec:\n  ports:\n  - port: 5601\n  selector:\n    app: kibana\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  labels:\n    app: kibana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kibana\n  template:\n    metadata:\n      labels:\n        app: kibana\n    spec:\n      containers:\n      - name: kibana\n        image: ${KUBE_IMAGE_REPO}/kibana:${ELASTICSEARCH_VERSION}\n        resources:\n          limits:\n            cpu: 1000m\n          requests:\n            cpu: 100m\n        env:\n          - name: ELASTICSEARCH_URL\n            value: http://elasticsearch:9200\n        ports:\n        - containerPort: 5601\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: kibana.logging.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kibana\n            port:\n              number: 5601\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: fluentd\n  namespace: kube-logging\n  labels:\n    app: fluentd\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: fluentd\n  labels:\n    app: fluentd\nrules:\n- apiGroups:\n  - ''\n  resources:\n  - pods\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: fluentd\nroleRef:\n  kind: ClusterRole\n  name: fluentd\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: fluentd\n  namespace: kube-logging\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: kube-logging\n  labels:\n    app: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: fluentd\n  template:\n    metadata:\n      labels:\n        app: fluentd\n    spec:\n      serviceAccount: fluentd\n      serviceAccountName: fluentd\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n      containers:\n      - name: fluentd\n        image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-elasticsearch8-1\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: FLUENT_ELASTICSEARCH_HOST\n          value: elasticsearch.kube-logging.svc.${KUBE_DNSDOMAIN}\n        - name: FLUENT_ELASTICSEARCH_PORT\n          value: '9200'\n        - name: FLUENT_ELASTICSEARCH_SCHEME\n          value: http\n        - name: FLUENTD_SYSTEMD_CONF\n          value: disable\n        - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH\n          value: /var/log/containers/fluent*\n        - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE\n          value: cri\n        - name: FLUENT_CONTAINER_TAIL_PARSER_TIME_FORMAT\n          value: '%Y-%m-%dT%H:%M:%S.%L%z'\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: localtime\n          mountPath: /etc/localtime\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n      - name: varlog\n        hostPath:\n          path: /var/log\n    \" \n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      kube::wait \"elasticsearch\" \"kube-logging\" \"pods --all\"\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:kibana.logging.cluster.local' http://${INGRESS_CONN}\"\n      log::access \"[ingress]\" \"curl -H 'Host:elasticsearch.logging.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[log]\" \"No $KUBE_LOG config.\"\n  fi\n}\n\n\nfunction add::storage() {\n  # 添加存储 \n\n  if [[ \"$KUBE_STORAGE\" == \"rook\" ]]; then\n\n    log::info \"[storage]\" \"add rook\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/rook/rook/archive/v${ROOK_VERSION}.zip\" \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}.zip\" \"unzip\"\n\n    command::exec \"${MGMT_NODE}\" \"\n      cd '${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples' \\\n      && sed -i -e 's/# ROOK_CSI_\\\\(.*\\\\)_IMAGE/ROOK_CSI_\\\\1_IMAGE/g' \\\n                -e 's#\\\\k8s\\\\.gcr\\\\.io/sig-storage#${KUBE_IMAGE_REPO}#g' \\\n                -e 's#\\\\quay\\\\.io/cephcsi#${KUBE_IMAGE_REPO}#g' operator.yaml \\\n      && sed -i 's#quay.io/ceph#${KUBE_IMAGE_REPO}#g' cluster.yaml\n    \" \n    check::exit_code \"$?\" \"storage\" \"image using proxy\"\n\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/crds.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/common.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/operator.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/cluster.yaml\"\n\n  elif [[ \"$KUBE_STORAGE\" == \"longhorn\" ]]; then\n    log::info \"[storage]\" \"add longhorn\"\n    log::info \"[storage]\" \"get cluster node hosts\"\n    if [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl get node -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n      \"\n      get::command_output \"cluster_nodes\" \"$?\" \"exit\"\n    else\n      cluster_nodes=\"${MASTER_NODES} ${WORKER_NODES}\"\n    fi\n    for host in ${cluster_nodes:-}\n    do\n      log::info \"[storage]\"  \"${host}: install iscsi-initiator-utils\"\n      command::exec \"${host}\" \"\n        yum install -y iscsi-initiator-utils\n      \"\n      check::exit_code \"$?\" \"storage\" \"${host}: install iscsi-initiator-utils\" \"exit\"\n    done\n    \n    local longhorn_file=\"${OFFLINE_DIR}/manifests/longhorn.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/longhorn/longhorn/v${LONGHORN_VERSION}/deploy/longhorn.yaml\" \"${longhorn_file}\"\n\n    command::exec \"${MGMT_NODE}\" \"\n      sed -i 's#numberOfReplicas: \\\"3\\\"#numberOfReplicas: \\\"1\\\"#g' \\\"${longhorn_file}\\\"\n    \"\n    check::exit_code \"$?\" \"storage\" \"set longhorn numberOfReplicas is 1\"\n\n    kube::apply \"${longhorn_file}\"\n    kube::wait \"longhorn\" \"longhorn-system\" \"pods --all\"\n    \n    log::info \"[storage]\"  \"set longhorn is default storage class\"\n    command::exec \"${MGMT_NODE}\" \"\n      default_class=\\\"\\$(kubectl get storageclass -A -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\\\.kubernetes\\\\.io/is-default-class==\\\\\\\"true\\\\\\\")].metadata.name}')\\\"\n      if [ \\\"\\${default_class:-}\\\" != \\\"\\\" ]; then\n         kubectl patch storageclass \\${default_class} -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\n      fi\n      kubectl patch storageclass longhorn -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\n    \"\n    check::exit_code \"$?\" \"storage\" \"set longhorn is default storage class\"\n    \n    kube::apply \"longhorn ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: longhorn-ingress\n  namespace: longhorn-system\nspec:\n  rules:\n  - host: longhorn.storage.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: longhorn-frontend\n            port:\n              number: 80\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:longhorn.storage.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[storage]\" \"No $KUBE_STORAGE config.\"\n  fi\n}\n\n\nfunction add::ui() {\n  # 添加用户界面\n\n  if [[ \"$KUBE_UI\" == \"dashboard\" ]]; then\n    log::info \"[ui]\" \"add kubernetes dashboard\"\n    local dashboard_file=\"${OFFLINE_DIR}/manifests/kubernetes-dashboard.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/dashboard/v${KUBERNETES_DASHBOARD_VERSION}/aio/deploy/recommended.yaml\" \"${dashboard_file}\"\n    kube::apply \"${dashboard_file}\"\n    kube::wait \"kubernetes-dashboard\" \"kubernetes-dashboard\" \"pod\" \"k8s-app=kubernetes-dashboard\"\n    kube::apply \"kubernetes dashboard ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\n$( if [[ $KUBE_INGRESS == \"nginx\" ]]; then\necho \"\"\"\n    nginx.ingress.kubernetes.io/secure-backends: 'true'\n    nginx.ingress.kubernetes.io/backend-protocol: 'HTTPS'\n    nginx.ingress.kubernetes.io/ssl-passthrough: 'true'\n\"\"\";\nelif [[ $KUBE_INGRESS == \"traefik\" ]]; then \necho \"\"\"\n    traefik.ingress.kubernetes.io/frontend-entry-points: https\n    traefik.ingress.kubernetes.io/auth-type: 'basic'\n    traefik.ingress.kubernetes.io/auth-secret: 'kubernetes-dashboard-auth'\n    traefik.ingress.kubernetes.io/ssl-redirect: 'true'\n\"\"\";\nfi\n)\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard \nspec:\n$( if [[ $KUBE_INGRESS == \"nginx\" ]]; then\necho \"\"\"\n  tls:\n  - hosts:\n    - kubernetes-dashboard.cluster.local\n    secretName: kubernetes-dashboard-certs\n\"\"\"\nelif [[ $KUBE_INGRESS == \"traefik\" ]]; then \necho \"\"\"\n\"\"\"\nfi\n)\n  rules:\n  - host: kubernetes-dashboard.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn \"443\"\n      log::access \"[ingress]\" \"curl --insecure -H 'Host:kubernetes-dashboard.cluster.local' https://${INGRESS_CONN}\"\n\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl create serviceaccount kubernetes-dashboard-admin-sa -n kubernetes-dashboard  --dry-run -o yaml | kubectl apply -f -\n        kubectl apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubernetes-dashboard-admin-sa-token\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: kubernetes-dashboard-admin-sa\ntype: kubernetes.io/service-account-token\nEOF\n        kubectl create clusterrolebinding kubernetes-dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:kubernetes-dashboard-admin-sa -n kubernetes-dashboard  --dry-run -o yaml | kubectl apply -f -\n      \"\n      local s=\"$?\"\n      check::exit_code \"$s\" \"ui\" \"create kubernetes dashboard admin service account\"\n      local dashboard_token=\"\"\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl describe secrets \\$(kubectl describe sa kubernetes-dashboard-admin-sa -n kubernetes-dashboard | awk '/Tokens/ {print \\$2}') -n kubernetes-dashboard | awk '/token:/{print \\$2}'\n      \"\n      get::command_output \"dashboard_token\" \"$?\"\n      [[ \"$dashboard_token\" != \"\" ]] && log::access \"[Token]\" \"${dashboard_token}\"\n    fi\n  elif [[ \"$KUBE_UI\" == \"kubesphere\" ]]; then\n    log::info \"[ui]\" \"add kubesphere\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubesphere/ks-installer/v${KUBESPHERE_VERSION}/deploy/kubesphere-installer.yaml\" \"${OFFLINE_DIR}/manifests/kubesphere-installer.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubesphere/ks-installer/v${KUBESPHERE_VERSION}/deploy/cluster-configuration.yaml\" \"${OFFLINE_DIR}/manifests/cluster-configuration.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/kubesphere-installer.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/cluster-configuration.yaml\"\n\n    sleep 60\n    kube::wait \"ks-installer\" \"kubesphere-system\" \"pods\" \"app=ks-install\"\n    command::exec \"${MGMT_NODE}\" \"\n      $(declare -f utils::retry) \n      utils::retry 10 kubectl -n kubesphere-system get pods redis-ha-server-0 \\\n        && { kubectl -n kubesphere-system get sts redis-ha-server -o yaml | sed 's#node-role.kubernetes.io/master#node-role.kubernetes.io/worker#g' | kubectl replace --force -f -; } \\\n        && echo not replace redis-ha-server\n      utils::retry 10 kubectl -n kubesphere-system get pods openldap-0 \\\n        && { kubectl -n kubesphere-system get sts openldap -o yaml | sed 's#node-role.kubernetes.io/master#node-role.kubernetes.io/worker#g' | kubectl replace --force -f -; } \\\n        && echo not replace openldap\n    \"\n    check::exit_code \"$?\" \"ui\" \"set statefulset to worker node\"\n\n    sleep 60\n    kube::wait \"kubesphere-system\" \"kubesphere-system\" \"pods --all\"\n    kube::wait \"kubesphere-controls-system\" \"kubesphere-controls-system\" \"pods --all\" \n    kube::wait \"kubesphere-monitoring-system\" \"kubesphere-monitoring-system\" \"pods --all\" \n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address } {end}' | awk '{print \\$1}'\n      \"\n      get::command_output \"node_ip\" \"$?\"\n      log::access \"[service]\" \"curl http://${node_ip:-NodeIP}:30880;  auth: admin/P@88w0rd\"\n    fi\n  else\n    log::warning \"[ui]\" \"No $KUBE_UI config.\"\n  fi\n}\n\n\nfunction add::ops() {\n  # 运维操作\n   \n  local master_num\n  master_num=$(awk '{print NF}' <<< \"${MASTER_NODES}\")\n  \n  log::info \"[ops]\" \"add anti-affinity strategy to coredns\"\n  command::exec \"${MGMT_NODE}\" \"\"\"\n    kubectl -n kube-system patch deployment coredns --patch '{\\\"spec\\\": {\\\"template\\\": {\\\"spec\\\": {\\\"affinity\\\":{\\\"podAntiAffinity\\\":{\\\"preferredDuringSchedulingIgnoredDuringExecution\\\":[{\\\"weight\\\":100,\\\"podAffinityTerm\\\":{\\\"labelSelector\\\":{\\\"matchExpressions\\\":[{\\\"key\\\":\\\"k8s-app\\\",\\\"operator\\\":\\\"In\\\",\\\"values\\\":[\\\"kube-dns\\\"]}]},\\\"topologyKey\\\":\\\"kubernetes.io/hostname\\\"}}]}}}}}}' --record\n  \"\"\"\n  check::exit_code \"$?\" \"ops\" \"add anti-affinity strategy to coredns\"\n\n  log::info \"[ops]\" \"add etcd snapshot cronjob\"\n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm config images list --config=/etc/kubernetes/kubeadmcfg.yaml 2>/dev/null | grep etcd:\n  \"\n  get::command_output \"etcd_image\" \"$?\"\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' --no-headers | wc -l\n  \"\n  get::command_output \"master_num\" \"$?\"\n\n  [[ \"${master_num:-0}\" == \"0\" ]] && master_num=1\n  kube::apply \"etcd-snapshot\" \"\"\"\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etcd-snapshot\n  namespace: kube-system\nspec:\n  schedule: '0 */6 * * *'\n  successfulJobsHistoryLimit: 3\n  suspend: false\n  concurrencyPolicy: Allow\n  failedJobsHistoryLimit: 3\n  jobTemplate:\n    spec:\n      backoffLimit: 6\n      parallelism: ${master_num}\n      completions: ${master_num}\n      template:\n        metadata:\n          labels:\n            app: etcd-snapshot\n        spec:\n          affinity:\n            podAntiAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n              - labelSelector:\n                  matchExpressions:\n                  - key: app\n                    operator: In\n                    values:\n                    - etcd-snapshot\n                topologyKey: 'kubernetes.io/hostname'\n          containers:\n          - name: etcd-snapshot\n            image: ${etcd_image:-${KUBE_IMAGE_REPO}/etcd:3.4.13-0}\n            imagePullPolicy: IfNotPresent\n            args:\n            - -c\n            - etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt\n              --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\n              snapshot save /backup/etcd-snapshot-\\\\\\\\\\\\\\$(date +%Y-%m-%d_%H:%M:%S_%Z).db\n              && echo 'delete old backups' && { find /backup -type f -mtime +30 -exec rm -fv {} \\\\; || echo error; }\n            command:\n            - /usr/bin/bash\n            env:\n            - name: ETCDCTL_API\n              value: '3'\n            resources: {}\n            terminationMessagePath: /dev/termination-log\n            terminationMessagePolicy: File\n            volumeMounts:\n            - name: etcd-certs\n              mountPath: /etc/kubernetes/pki/etcd\n              readOnly: true\n            - name: backup\n              mountPath: /backup\n            - name: etc\n              mountPath: /etc\n            - name: bin\n              mountPath: /usr/bin\n            - name: lib64\n              mountPath: /lib64\n          dnsPolicy: ClusterFirst\n          hostNetwork: true\n          nodeSelector:\n            node-role.kubernetes.io/control-plane: ''\n          tolerations:\n          - effect: NoSchedule\n            operator: Exists\n          restartPolicy: OnFailure\n          schedulerName: default-scheduler\n          securityContext: {}\n          terminationGracePeriodSeconds: 30\n          volumes:\n          - name: etcd-certs\n            hostPath:\n              path: /etc/kubernetes/pki/etcd\n              type: DirectoryOrCreate\n          - name: backup\n            hostPath:\n              path: /var/lib/etcd/backups\n              type: DirectoryOrCreate\n          - name: etc\n            hostPath:\n              path: /etc\n          - name: bin\n            hostPath:\n              path: /usr/bin\n          - name: lib64\n            hostPath:\n              path: /lib64\n\"\"\"\n  # shellcheck disable=SC2181\n  [[ \"$?\" == \"0\" ]] && log::access \"[ops]\" \"etcd backup directory: /var/lib/etcd/backups\"\n  command::exec \"${MGMT_NODE}\" \"\n    jobname=\\\"etcd-snapshot-$(date +%s)\\\"\n    kubectl create job --from=cronjob/etcd-snapshot \\${jobname} -n kube-system && \\\n    kubectl wait --for=condition=complete job/\\${jobname} -n kube-system\n  \"\n  check::exit_code \"$?\" \"ops\" \"trigger etcd backup\"\n}\n\n\nfunction reset::node() {\n  # 重置节点\n\n  local host=$1\n  log::info \"[reset]\" \"node $host\"\n  command::exec \"${host}\" \"\n    set +ex\n    cri_socket=\\\"\\\"\n    [ -S /var/run/crio/crio.sock ] && cri_socket=\\\"--cri-socket /var/run/crio/crio.sock\\\"\n    [ -S /run/containerd/containerd.sock ] && cri_socket=\\\"--cri-socket /run/containerd/containerd.sock\\\"\n    kubeadm reset -f \\$cri_socket\n    [ -f \\\"\\$(which kubelet)\\\" ] && { systemctl stop kubelet; find /var/lib/kubelet | xargs -n 1 findmnt -n -o TARGET -T | sort | uniq | xargs -r umount -v; yum remove -y kubeadm kubelet kubectl; }\n    [ -d /etc/kubernetes ] && rm -rf /etc/kubernetes/* /var/lib/kubelet/* /var/lib/etcd/* \\$HOME/.kube /etc/cni/net.d/* /var/lib/dockershim/* /var/lib/cni/* /var/run/kubernetes/*\n\n    [ -f \\\"\\$(which docker)\\\" ] && { docker rm -f -v \\$(docker ps | grep kube | awk '{print \\$1}'); systemctl stop docker; rm -rf \\$HOME/.docker /etc/docker/* /var/lib/docker/*; yum remove -y docker; }\n    [ -f \\\"\\$(which containerd)\\\" ] && { crictl rm \\$(crictl ps -a -q); systemctl stop containerd; rm -rf /etc/containerd/* /var/lib/containerd/*; yum remove -y containerd.io; }\n    [ -f \\\"\\$(which crio)\\\" ] && { crictl rm \\$(crictl ps -a -q); systemctl stop crio; rm -rf /etc/crictl.yaml /etc/crio/* /var/run/crio/*; yum remove -y cri-o; }\n    [ -f \\\"\\$(which runc)\\\" ] && { find /run/containers/ /var/lib/containers/ | xargs -n 1 findmnt -n -o TARGET -T | sort | uniq | xargs -r umount -v; rm -rf /var/lib/containers/* /var/run/containers/*; yum remove -y runc; }\n    [ -f \\\"\\$(which haproxy)\\\" ] && { systemctl stop haproxy; rm -rf /etc/haproxy/* /etc/rsyslog.d/haproxy.conf; yum remove -y haproxy; }\n\n    sed -i -e \\\"/$KUBE_APISERVER/d\\\" -e '/-worker-/d' -e '/-master-/d' /etc/hosts\n    sed -i '/## Kainstall managed start/,/## Kainstall managed end/d' /etc/security/limits.conf /etc/systemd/system.conf /etc/bashrc /etc/rc.local /etc/audit/rules.d/audit.rules\n    \n    [ -d /var/lib/elasticsearch ] && rm -rf /var/lib/elasticsearch/*\n    [ -d /var/lib/longhorn ] &&  rm -rf /var/lib/longhorn/*\n    [ -d \\\"${OFFLINE_DIR:-/tmp/abc}\\\" ] && rm -rf \\\"${OFFLINE_DIR:-/tmp/abc}\\\"\n    for repo in kubernetes.repo docker-ce.repo devel_kubic_libcontainers_stable.repo\n    do\n      [ -f /etc/yum.repos.d/\\${repo} ] && rm -f /etc/yum.repos.d/\\${repo}\n    done\n    ipvsadm --clear\n    iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n    for int in kube-ipvs0 cni0 docker0 dummy0 flannel.1 cilium_host cilium_net cilium_vxlan lxc_health nodelocaldns \n    do\n      [ -d /sys/class/net/\\${int} ] && ip link delete \\${int}\n    done\n    modprobe -r ipip\n    echo done.\n  \"\n  check::exit_code \"$?\" \"reset\" \"$host: reset\"\n}\n\n\nfunction reset::cluster() {\n  # 重置所有节点\n  \n  local all_node=\"\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {end}'\n  \"\n  get::command_output \"all_node\" \"$?\"\n  \n  all_node=$(echo \"${WORKER_NODES} ${MASTER_NODES} ${all_node}\" | awk '{for (i=1;i<=NF;i++) if (!a[$i]++) printf(\"%s%s\",$i,FS)}')\n\n  for host in $all_node\n  do\n    reset::node \"$host\"\n  done\n\n}\n\n\nfunction offline::load() {\n  # 节点加载离线包\n \n  local role=\"${1:-}\"\n  local hosts=\"\"\n  \n  if [[ \"${role}\" == \"master\" ]]; then\n     hosts=\"${MASTER_NODES}\"\n  elif [[ \"${role}\" == \"worker\" ]]; then\n     hosts=\"${WORKER_NODES}\"\n  fi\n \n  for host in ${hosts}\n  do\n    log::info \"[offline]\" \"${role} ${host}: load offline file\"\n    command::exec \"${host}\"  \"[[ ! -d \\\"${OFFLINE_DIR}\\\" ]] && { mkdir -pv \\\"${OFFLINE_DIR}\\\"; chmod 777 \\\"${OFFLINE_DIR}\\\"; } ||:\"\n    check::exit_code \"$?\" \"offline\" \"$host: mkdir offline dir\" \"exit\"\n\n    if [[ \"${UPGRADE_KERNEL_TAG:-}\" == \"1\" ]]; then\n      command::scp \"${host}\" \"${TMP_DIR}/packages/kernel/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp kernel file to $host\" \"exit\"\n    else\n      log::info \"[offline]\" \"${role} ${host}: copy offline file\"\n      command::scp \"${host}\" \"${TMP_DIR}/packages/kubeadm/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp kube file to $host\" \"exit\"\n      command::scp \"${host}\" \"${TMP_DIR}/packages/all/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp all file to $host\" \"exit\"\n\n      if [[ \"${role}\" == \"worker\" ]]; then\n        command::scp \"${host}\" \"${TMP_DIR}/packages/worker/*\" \"${OFFLINE_DIR}\"\n        check::exit_code \"$?\" \"offline\" \"scp worker file to $host\" \"exit\"\n      fi \n\n      command::scp \"${host}\" \"${TMP_DIR}/images/${role}.tgz\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp ${role} images to $host\" \"exit\"\n      command::scp \"${host}\" \"${TMP_DIR}/images/all.tgz\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp all images to $host\" \"exit\"\n    fi\n\n    \n    log::info \"[offline]\" \"${role} ${host}: install package\"\n    command::exec \"${host}\" \"yum localinstall -y --skip-broken ${OFFLINE_DIR}/*.rpm\"\n    check::exit_code \"$?\" \"offline\" \"${role} ${host}: install package\" \"exit\"\n  \n    if [[ \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]]; then\n      command::exec \"${host}\" \"\n        set -e\n        for target in firewalld python-firewall firewalld-filesystem iptables; do\n          systemctl stop \\$target &>/dev/null || true\n          systemctl disable \\$target &>/dev/null || true\n        done\n        systemctl start docker && \\\n        cd ${OFFLINE_DIR} && \\\n        gzip -d -c ${1}.tgz | docker load && gzip -d -c all.tgz | docker load\n      \"\n      check::exit_code \"$?\" \"offline\" \"$host: load images\" \"exit\"  \n    fi\n    command::exec \"${host}\" \"rm -rf ${OFFLINE_DIR:-/tmp/abc}\"\n    check::exit_code \"$?\" \"offline\" \"$host: clean offline file\"  \n  done\n\n  command::scp \"${MGMT_NODE}\" \"${TMP_DIR}/manifests\" \"${OFFLINE_DIR}\"\n  check::exit_code \"$?\" \"offline\" \"scp manifests file to ${MGMT_NODE}\" \"exit\"\n\n  command::scp \"${MGMT_NODE}\" \"${TMP_DIR}/bins\" \"${OFFLINE_DIR}\"\n  check::exit_code \"$?\" \"offline\" \"scp bins file to ${MGMT_NODE}\" \"exit\"\n}\n\n\nfunction offline::cluster() {\n  # 集群节点加载离线包\n\n  [ ! -f \"${OFFLINE_FILE}\" ] && { log::error \"[offline]\" \"not found ${OFFLINE_FILE}\" ; exit 1; }\n\n  log::info \"[offline]\" \"Unzip offline package on local.\"\n  tar zxf \"${OFFLINE_FILE}\"  -C \"${TMP_DIR}/\"\n  check::exit_code \"$?\" \"offline\"  \"Unzip offline package\"\n \n  offline::load \"master\"\n  offline::load \"worker\"\n}\n\n\nfunction init::cluster() {\n  # 初始化集群\n\n  MGMT_NODE=$(echo \"${MASTER_NODES}\" | awk '{print $1}')\n\n  # 加载离线包\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && offline::cluster\n  \n  # 1. 初始化节点\n  init::node\n  # 2. 安装包\n  install::package\n  # 3. 初始化kubeadm\n  kubeadm::init\n  # 4. 加入集群\n  kubeadm::join\n  # 5. 添加network\n  add::network\n  # 6. 安装addon\n  add::addon\n  # 7. 添加ingress\n  add::ingress\n  # 8. 添加storage\n  [[ \"${STORAGE_TAG:-}\" == \"1\" ]] && add::storage\n  # 9. 添加web ui\n  add::ui\n  # 10. 添加monitor\n  [[ \"${MONITOR_TAG:-}\" == \"1\" ]] && add::monitor\n  # 11. 添加log\n  [[ \"${LOG_TAG:-}\" == \"1\" ]] && add::log\n  # 12. 运维操作\n  add::ops\n  # 13. 查看集群状态\n  kube::status\n}\n\n\nfunction add::node() {\n  # 添加节点\n  \n  # 加载离线包\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && offline::cluster\n\n  # KUBE_VERSION未指定时，获取集群的版本\n  if [[ \"${KUBE_VERSION}\" == \"\" || \"${KUBE_VERSION}\" == \"latest\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.nodeInfo.kubeletVersion } {end}' | awk -F'v| ' '{print \\$2}'\n  \"\n    get::command_output \"KUBE_VERSION\" \"$?\" \"exit\"\n  fi\n\n  # 1. 初始化节点\n  init::add_node\n  # 2. 安装包\n  install::package\n  # 3. 加入集群\n  kubeadm::join\n  # 4. haproxy添加apiserver\n  config::haproxy_backend \"add\"\n  # 5. 更新 etcd snapshot 副本\n  config::etcd_snapshot\n  # 6. 查看集群状态\n  kube::status\n}\n\n\nfunction del::node() {\n  # 删除节点\n \n  config::haproxy_backend \"remove\"\n\n  local cluster_nodes=\"\"\n  local del_hosts_cmd=\"\"\n  command::exec \"${MGMT_NODE}\" \"\n     kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.metadata.name }\\\\n{end}'\n  \"\n  get::command_output \"cluster_nodes\" \"$?\" exit\n\n  for host in $MASTER_NODES\n  do\n     command::exec \"${MGMT_NODE}\" \"\n       etcd_pod=\\$(kubectl -n kube-system get pods -l component=etcd --field-selector=status.phase=Running -o jsonpath='{\\$.items[0].metadata.name}')\n       etcd_node=\\$(kubectl -n kube-system exec \\$etcd_pod -- sh -c \\\"export ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key ETCDCTL_ENDPOINTS=https://127.0.0.1:2379; etcdctl member list\\\"| grep $host | awk -F, '{print \\$1}')\n       echo \\\"\\$etcd_pod \\$etcd_node\\\"\n       kubectl -n kube-system exec \\$etcd_pod -- sh -c \\\"export ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key ETCDCTL_ENDPOINTS=https://127.0.0.1:2379; etcdctl member remove \\$etcd_node; etcdctl member list\\\"\n     \"\n     check::exit_code \"$?\" \"del\" \"remove $host etcd member\"\n  done\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    log::info \"[del]\" \"node $host\"\n\n    local node_name; node_name=$(echo -ne \"${cluster_nodes}\" | grep \"${host}\" | awk '{print $2}')\n    if [[ \"${node_name}\" == \"\" ]]; then\n      log::warning \"[del]\" \"node $host not found.\"\n      read -r -t 10 -n 1 -p \"Do you need to reset the node (y/n)? \" answer\n      [[ -z \"$answer\" || \"$answer\" != \"y\" ]] && exit || echo\n    else\n      log::info \"[del]\" \"drain $host\"\n      command::exec \"${MGMT_NODE}\" \"kubectl drain $node_name --force --ignore-daemonsets --delete-local-data\"\n      check::exit_code \"$?\" \"del\" \"$host: drain\"\n\n      log::info \"[del]\" \"delete node $host\"\n      command::exec \"${MGMT_NODE}\" \"kubectl delete node $node_name\"\n      check::exit_code \"$?\" \"del\" \"$host: delete\"\n      sleep 3\n    fi\n    reset::node \"$host\"\n    del_hosts_cmd=\"${del_hosts_cmd}\\nsed -i \"/$host/d\" /etc/hosts\"\n  done\n\n  for host in $(echo -ne \"${cluster_nodes}\" | awk '{print $1}')\n  do\n     log::info \"[del]\" \"$host: remove del node hostname resolution\"\n     command::exec \"${host}\" \"\n       $(echo -ne \"${del_hosts_cmd}\")\n     \"\n     check::exit_code \"$?\" \"del\" \"remove del node hostname resolution\"\n  done\n  [ \"$MASTER_NODES\" != \"\" ] && config::etcd_snapshot\n  kube::status\n}\n\n\nfunction upgrade::cluster() {\n  # 升级集群\n\n  log::info \"[upgrade]\" \"upgrade to $KUBE_VERSION\"\n  log::info \"[upgrade]\" \"backup cluster\"\n  add::ops\n\n  local stable_version=\"2\"\n  command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"\n  get::command_output \"stable_version\" \"$?\" && stable_version=\"${stable_version#v}\"\n\n  local node_hosts=\"$MASTER_NODES $WORKER_NODES\"\n  if [[ \"$node_hosts\" == \" \" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node -o jsonpath='{range.items[*]}{.metadata.name } {end}'\n    \"\n    get::command_output \"node_hosts\" \"$?\" exit\n  fi\n\n  local skip_plan=${SKIP_UPGRADE_PLAN,,}\n  for host in ${node_hosts}\n  do\n    log::info \"[upgrade]\" \"node: $host\"\n    local local_version=\"\"\n    command::exec \"${host}\" \"kubectl version --client --output=yaml | awk '/gitVersion:/ {print \\$2}'\"\n    get::command_output \"local_version\" \"$?\" && local_version=\"${local_version#v}\"\n\n    if [[ \"${KUBE_VERSION}\" != \"latest\" ]]; then\n      if [[ \"${KUBE_VERSION}\" == \"${local_version}\" ]];then\n        log::warning \"[check]\" \"The specified version(${KUBE_VERSION}) is consistent with the local version(${local_version})!\"\n        continue\n      fi\n\n      if [[ $(utils::version_to_number \"$KUBE_VERSION\") -lt $(utils::version_to_number \"${local_version}\") ]];then\n        log::warning \"[check]\" \"The specified version($KUBE_VERSION) is less than the local version(${local_version})!\"\n        continue\n      fi\n\n      if [[ $(utils::version_to_number \"$KUBE_VERSION\") -gt $(utils::version_to_number \"${stable_version}\") ]];then\n        log::warning \"[check]\" \"The specified version($KUBE_VERSION) is more than the stable version(${stable_version})!\"\n        continue\n      fi\n    else\n      if [[ $(utils::version_to_number \"${local_version}\") -ge $(utils::version_to_number \"${stable_version}\") ]];then\n        log::warning \"[check]\" \"The local version($local_version) is greater or equal to the stable version(${stable_version})!\"\n        continue\n      fi\n    fi\n\n    command::exec \"${MGMT_NODE}\" \"kubectl drain ${host} --ignore-daemonsets --delete-local-data\"\n    check::exit_code \"$?\" \"upgrade\" \"drain ${host} node\" \"exit\"\n    sleep 5\n\n    if [[ \"${skip_plan}\" == \"false\" ]]; then\n      command::exec \"${host}\" \"$(declare -f script::upgrage_kube); script::upgrage_kube 'init' '$KUBE_VERSION'\"\n      check::exit_code \"$?\" \"upgrade\" \"plan and upgrade cluster on ${host}\" \"exit\"\n      command::exec \"${host}\" \"$(declare -f utils::retry); utils::retry 10 kubectl get node\"\n      check::exit_code \"$?\" \"upgrade\" \"${host}: upgrade\" \"exit\"\n      skip_plan=true\n    else\n      command::exec \"${host}\" \"$(declare -f script::upgrage_kube); script::upgrage_kube 'node' '$KUBE_VERSION'\"\n      check::exit_code \"$?\" \"upgrade\" \"upgrade ${host} node\" \"exit\"\n    fi\n\n    command::exec \"${MGMT_NODE}\" \"kubectl wait --for=condition=Ready node/${host} --timeout=120s\"\n    check::exit_code \"$?\" \"upgrade\" \"${host} ready\"\n    sleep 5\n    command::exec \"${MGMT_NODE}\" \"$(declare -f utils::retry); utils::retry 6 kubectl uncordon ${host}\"\n    check::exit_code \"$?\" \"upgrade\" \"uncordon ${host} node\"\n    sleep 5\n  done\n  \n  kube::status\n}\n\n\nfunction update::self {\n  # 脚本文件更新\n  \n  log::info \"[update]\" \"download kainstall script to $0\"\n  command::exec \"127.0.0.1\" \"\n    wget --timeout=10 --waitretry=3 --tries=5 --retry-connrefused ${GITHUB_PROXY}https://raw.githubusercontent.com/lework/kainstall/master/kainstall-centos.sh -O /tmp/kainstall-centos.sh || exit 1\n    /bin/cp -fv $0 /tmp/$0-bakup\n    /bin/mv -fv /tmp/kainstall-centos.sh \\\"$0\\\"\n    chmod +x \\\"$0\\\"\n  \"\n  check::exit_code \"$?\" \"update\" \"kainstall script\"\n}\n\n\nfunction transform::data {\n  # 数据处理及限制\n\n  MASTER_NODES=$(echo \"${MASTER_NODES}\" | tr ',' ' ')\n  WORKER_NODES=$(echo \"${WORKER_NODES}\" | tr ',' ' ')\n\n  if ! utils::is_element_in_array \"$KUBE_CRI\" docker containerd cri-o ; then\n    log::error \"[limit]\" \"$KUBE_CRI is not supported, only [docker,containerd,cri-o]\"\n    exit 1\n  fi\n\n  [[ \"$KUBE_CRI\" != \"containerd\" && \"${OFFLINE_TAG:-}\" == \"1\" ]] && { log::error \"[limit]\" \"$KUBE_CRI is not supported offline, only containerd\"; exit 1; }\n  [[ \"$KUBE_CRI\" == \"docker\" && \"${KUBE_CRI_ENDPOINT}\" == \"unix:///run/containerd/containerd.sock\" ]] && KUBE_CRI_ENDPOINT=\"/var/run/dockershim.sock\"\n  [[ \"$KUBE_CRI\" == \"cri-o\" && \"${KUBE_CRI_ENDPOINT}\" == \"unix:///run/containerd/containerd.sock\"  ]] && KUBE_CRI_ENDPOINT=\"unix:///var/run/crio/crio.sock\"\n\n  kubelet_nodeRegistration=\"nodeRegistration:\n  criSocket: ${KUBE_CRI_ENDPOINT:-/run/containerd/containerd.sock}\n  kubeletExtraArgs:\n    runtime-cgroups: /system.slice/${KUBE_CRI//-/}.service\n    pod-infra-container-image: ${KUBE_IMAGE_REPO}/pause:${PAUSE_VERSION:-3.7}\n\"\n}\n\n\nfunction help::usage {\n  # 使用帮助\n  \n  cat << EOF\n\nInstall kubernetes cluster using kubeadm.\n\nUsage:\n  $(basename \"$0\") [command]\n\nAvailable Commands:\n  init            Init Kubernetes cluster.\n  reset           Reset Kubernetes cluster.\n  add             Add nodes to the cluster.\n  del             Remove node from the cluster.\n  renew-cert      Renew all available certificates.\n  upgrade         Upgrading kubeadm clusters.\n  update          Update script file.\n\nFlag:\n  -m,--master          master node, default: ''\n  -w,--worker          work node, default: ''\n  -u,--user            ssh user, default: ${SSH_USER}\n  -p,--password        ssh password\n     --private-key     ssh private key\n  -P,--port            ssh port, default: ${SSH_PORT}\n  -v,--version         kube version, default: ${KUBE_VERSION}\n  -n,--network         cluster network, choose: [flannel,calico,cilium], default: ${KUBE_NETWORK}\n  -i,--ingress         ingress controller, choose: [nginx,traefik], default: ${KUBE_INGRESS}\n  -ui,--ui             cluster web ui, choose: [dashboard,kubesphere], default: ${KUBE_UI}\n  -a,--addon           cluster add-ons, choose: [metrics-server,nodelocaldns], default: ${KUBE_ADDON}\n  -M,--monitor         cluster monitor, choose: [prometheus]\n  -l,--log             cluster log, choose: [elasticsearch]\n  -s,--storage         cluster storage, choose: [rook,longhorn]\n     --cri             cri tools, choose: [docker,containerd,cri-o], default: ${KUBE_CRI}\n     --cri-version     cri version, default: ${KUBE_CRI_VERSION}\n     --cri-endpoint    cri endpoint, default: ${KUBE_CRI_ENDPOINT}\n  -U,--upgrade-kernel  upgrade kernel\n  -of,--offline-file   specify the offline package file to load\n      --10years        the certificate period is 10 years.\n      --sudo           sudo mode\n      --sudo-user      sudo user\n      --sudo-password  sudo user password\n\nExample:\n  [init cluster]\n  $0 init \\\\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\\\n  --worker 192.168.77.133,192.168.77.134,192.168.77.135 \\\\\n  --user root \\\\\n  --password 123456 \\\\\n  --version 1.20.4\n\n  [reset cluster]\n  $0 reset \\\\\n  --user root \\\\\n  --password 123456\n\n  [add node]\n  $0 add \\\\\n  --master 192.168.77.140,192.168.77.141 \\\\\n  --worker 192.168.77.143,192.168.77.144 \\\\\n  --user root \\\\\n  --password 123456 \\\\\n  --version 1.20.4\n\n  [del node]\n  $0 del \\\\\n  --master 192.168.77.140,192.168.77.141 \\\\\n  --worker 192.168.77.143,192.168.77.144 \\\\\n  --user root \\\\\n  --password 123456\n \n  [other]\n  $0 renew-cert --user root --password 123456\n  $0 upgrade --version 1.20.4 --user root --password 123456\n  $0 update\n  $0 add --ingress traefik\n  $0 add --monitor prometheus\n  $0 add --log elasticsearch\n  $0 add --storage rook\n  $0 add --ui dashboard\n  $0 add --addon nodelocaldns\n\nEOF\n  exit 1\n}\n\n\n######################################################################################################\n# main\n######################################################################################################\n\n\n[ \"$#\" == \"0\" ] && help::usage\n\nwhile [ \"${1:-}\" != \"\" ]; do\n  case $1 in\n    init  )                 INIT_TAG=1\n                            ;;\n    reset )                 RESET_TAG=1\n                            ;;\n    add )                   ADD_TAG=1\n                            ;;\n    del )                   DEL_TAG=1\n                            ;;\n    renew-cert )            RENEW_CERT_TAG=1\n                            ;;\n    upgrade )               UPGRADE_TAG=1\n                            ;;\n    update )                UPDATE_TAG=1\n                            ;;\n    -m | --master )         shift\n                            MASTER_NODES=${1:-$MASTER_NODES}\n                            ;;\n    -w | --worker )         shift\n                            WORKER_NODES=${1:-$WORKER_NODES}\n                            ;;\n    -u | --user )           shift\n                            SSH_USER=${1:-$SSH_USER}\n                            ;;\n    -p | --password )       shift\n                            SSH_PASSWORD=${1:-$SSH_PASSWORD}\n                            ;;\n    --private-key )         shift\n                            SSH_PRIVATE_KEY=${1:-$SSH_SSH_PRIVATE_KEY}\n                            ;;\n    -P | --port )           shift\n                            SSH_PORT=${1:-$SSH_PORT}\n                            ;;\n    -v | --version )        shift\n                            KUBE_VERSION=${1:-$KUBE_VERSION}\n                            ;;\n    -n | --network )        shift\n                            NETWORK_TAG=1\n                            KUBE_NETWORK=${1:-$KUBE_NETWORK}\n                            ;;\n    -i | --ingress )        shift\n                            INGRESS_TAG=1\n                            KUBE_INGRESS=${1:-$KUBE_INGRESS}\n                            ;;\n    -M | --monitor )        shift\n                            MONITOR_TAG=1\n                            KUBE_MONITOR=${1:-$KUBE_MONITOR}\n                            ;;\n    -l | --log )            shift\n                            LOG_TAG=1\n                            KUBE_LOG=${1:-$KUBE_LOG}\n                            ;;\n    -s | --storage )        shift\n                            STORAGE_TAG=1\n                            KUBE_STORAGE=${1:-$KUBE_STORAGE}\n                            ;;\n    -ui | --ui )            shift\n                            UI_TAG=1\n                            KUBE_UI=${1:-$KUBE_UI}\n                            ;;\n    -a | --addon )          shift\n                            ADDON_TAG=1\n                            KUBE_ADDON=${1:-$KUBE_ADDON}\n                            ;;\n    --cri )                 shift\n                            KUBE_CRI=${1:-$KUBE_CRI}\n                            ;;\n    --cri-version )         shift\n                            KUBE_CRI_VERSION=${1:-$KUBE_CRI_VERSION}\n                            ;;\n    --cri-endpoint )        shift\n                            KUBE_CRI_ENDPOINT=${1:-$KUBE_CRI_ENDPOINT}\n                            ;;\n    -U | --upgrade-kernel ) UPGRADE_KERNEL_TAG=1\n                            ;;\n    -of | --offline-file )  shift\n                            OFFLINE_TAG=1\n                            OFFLINE_FILE=${1:-$OFFLINE_FILE}\n                            ;;\n    --10years )             CERT_YEAR_TAG=1\n                            ;;\n    --sudo )                SUDO_TAG=1\n                            ;;\n    --sudo-user )           shift\n                            SUDO_USER=${1:-$SUDO_USER}\n                            ;;\n    --sudo-password )       shift\n                            SUDO_PASSWORD=${1:-}\n                            ;;\n    * )                     help::usage\n  esac\n  shift\ndone\n\n# 开始\nlog::info \"[start]\" \"bash $0 ${SCRIPT_PARAMETER//${SSH_PASSWORD:-${SUDO_PASSWORD:-}}/zzzzzz}\"\n\n# 数据处理\ntransform::data\n\n# 预检\ncheck::preflight\n\n# 动作\nif [[ \"${INIT_TAG:-}\" == \"1\" ]]; then\n  [[ \"$MASTER_NODES\" == \"\" ]] && MASTER_NODES=\"127.0.0.1\"\n  init::cluster\nelif [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n  [[ \"${NETWORK_TAG:-}\" == \"1\" ]] && { add::network; add=1; }\n  [[ \"${INGRESS_TAG:-}\" == \"1\" ]] && { add::ingress; add=1; }\n  [[ \"${STORAGE_TAG:-}\" == \"1\" ]] && { add::storage; add=1; }\n  [[ \"${MONITOR_TAG:-}\" == \"1\" ]] && { add::monitor; add=1; }\n  [[ \"${LOG_TAG:-}\" == \"1\" ]] && { add::log; add=1; }\n  [[ \"${UI_TAG:-}\" == \"1\" ]] && { add::ui; add=1; }\n  [[ \"${ADDON_TAG:-}\" == \"1\" ]] && { add::addon; add=1; }\n  [[ \"$MASTER_NODES\" != \"\" || \"$WORKER_NODES\" != \"\" ]] && { add::node; add=1; }\n  [[ \"${add:-}\" != \"1\" ]] && help::usage\nelif [[ \"${DEL_TAG:-}\" == \"1\" ]]; then\n  if [[ \"$MASTER_NODES\" != \"\" || \"$WORKER_NODES\" != \"\" ]]; then del::node; else help::usage; fi\nelif [[ \"${RESET_TAG:-}\" == \"1\" ]]; then\n  reset::cluster\nelif [[ \"${RENEW_CERT_TAG:-}\" == \"1\" ]]; then\n  cert::renew\nelif [[ \"${UPGRADE_TAG:-}\" == \"1\" ]]; then\n  upgrade::cluster\nelif [[ \"${UPDATE_TAG:-}\" == \"1\" ]]; then\n  update::self\nelse\n  help::usage\nfi\n"
        },
        {
          "name": "kainstall-debian.sh",
          "type": "blob",
          "size": 129.9599609375,
          "content": "#!/usr/bin/env bash\n###################################################################\n#Script Name    : kainstall-debian.sh\n#Description    : Install kubernetes cluster using kubeadm.\n#Create Date    : 2021-04-18\n#Author         : lework\n#Email          : lework@yeah.net\n###################################################################\n\n\n[[ -n $DEBUG ]] && set -x\nset -o errtrace         # Make sure any error trap is inherited\nset -o nounset          # Disallow expansion of unset variables\nset -o pipefail         # Use last non-zero exit code in a pipeline\n\n\n######################################################################################################\n# environment configuration\n######################################################################################################\n\n# 版本\nKUBE_VERSION=\"${KUBE_VERSION:-latest}\"\nFLANNEL_VERSION=\"${FLANNEL_VERSION:-0.24.0}\"\nMETRICS_SERVER_VERSION=\"${METRICS_SERVER_VERSION:-0.6.4}\"\nINGRESS_NGINX=\"${INGRESS_NGINX:-1.9.5}\"\nTRAEFIK_VERSION=\"${TRAEFIK_VERSION:-2.10.7}\"\nCALICO_VERSION=\"${CALICO_VERSION:-3.27.0}\"\nCILIUM_VERSION=\"${CILIUM_VERSION:-1.14.5}\"\nKUBE_PROMETHEUS_VERSION=\"${KUBE_PROMETHEUS_VERSION:-0.13.0}\"\nELASTICSEARCH_VERSION=\"${ELASTICSEARCH_VERSION:-8.11.3}\"\nROOK_VERSION=\"${ROOK_VERSION:-1.9.13}\"\nLONGHORN_VERSION=\"${LONGHORN_VERSION:-1.5.3}\"\nKUBERNETES_DASHBOARD_VERSION=\"${KUBERNETES_DASHBOARD_VERSION:-2.7.0}\"\nKUBESPHERE_VERSION=\"${KUBESPHERE_VERSION:-3.3.2}\"\n\n# 集群配置\nKUBE_DNSDOMAIN=\"${KUBE_DNSDOMAIN:-cluster.local}\"\nKUBE_APISERVER=\"${KUBE_APISERVER:-apiserver.$KUBE_DNSDOMAIN}\"\nKUBE_POD_SUBNET=\"${KUBE_POD_SUBNET:-10.244.0.0/16}\"\nKUBE_SERVICE_SUBNET=\"${KUBE_SERVICE_SUBNET:-10.96.0.0/16}\"\nKUBE_IMAGE_REPO=\"${KUBE_IMAGE_REPO:-registry.cn-hangzhou.aliyuncs.com/kainstall}\"\nKUBE_NETWORK=\"${KUBE_NETWORK:-flannel}\"\nKUBE_INGRESS=\"${KUBE_INGRESS:-nginx}\"\nKUBE_MONITOR=\"${KUBE_MONITOR:-prometheus}\"\nKUBE_STORAGE=\"${KUBE_STORAGE:-rook}\"\nKUBE_LOG=\"${KUBE_LOG:-elasticsearch}\"\nKUBE_UI=\"${KUBE_UI:-dashboard}\"\nKUBE_ADDON=\"${KUBE_ADDON:-metrics-server}\"\nKUBE_FLANNEL_TYPE=\"${KUBE_FLANNEL_TYPE:-vxlan}\"\nKUBE_CRI=\"${KUBE_CRI:-containerd}\"\nKUBE_CRI_VERSION=\"${KUBE_CRI_VERSION:-latest}\"\nKUBE_CRI_ENDPOINT=\"${KUBE_CRI_ENDPOINT:-unix:///run/containerd/containerd.sock}\"\n\n# 定义的master和worker节点地址，以逗号分隔\nMASTER_NODES=\"${MASTER_NODES:-}\"\nWORKER_NODES=\"${WORKER_NODES:-}\"\n\n# 定义在哪个节点上进行设置\nMGMT_NODE=\"${MGMT_NODE:-127.0.0.1}\"\n\n# 节点的连接信息\nSSH_USER=\"${SSH_USER:-root}\"\nSSH_PASSWORD=\"${SSH_PASSWORD:-}\"\nSSH_PRIVATE_KEY=\"${SSH_PRIVATE_KEY:-}\"\nSSH_PORT=\"${SSH_PORT:-22}\"\nSUDO_USER=\"${SUDO_USER:-root}\"\n\n# 节点设置\nHOSTNAME_PREFIX=\"${HOSTNAME_PREFIX:-k8s}\"\n\n# 脚本设置\nTMP_DIR=\"$(rm -rf /tmp/kainstall* && mktemp -d -t kainstall.XXXXXXXXXX)\"\nLOG_FILE=\"${TMP_DIR}/kainstall.log\"\nSSH_OPTIONS=\"-o ConnectTimeout=600 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\"\nERROR_INFO=\"\\n\\033[31mERROR Summary: \\033[0m\\n  \"\nACCESS_INFO=\"\\n\\033[32mACCESS Summary: \\033[0m\\n  \"\nCOMMAND_OUTPUT=\"\"\nSCRIPT_PARAMETER=\"$*\"\nOFFLINE_DIR=\"/tmp/kainstall-offline-file/\"\nOFFLINE_FILE=\"\"\nOS_SUPPORT=\"debian9 debian10\"\nGITHUB_PROXY=\"${GITHUB_PROXY:-https://mirror.ghproxy.com/}\"\nGCR_PROXY=\"${GCR_PROXY:-k8sgcr.lework.workers.dev}\"\nSKIP_UPGRADE_PLAN=${SKIP_UPGRADE_PLAN:-false}\nSKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n\ntrap trap::info 1 2 3 15 EXIT\n\n######################################################################################################\n# function\n######################################################################################################\n\nfunction trap::info() {\n  # 信号处理\n  \n  [[ ${#ERROR_INFO} -gt 37 ]] && echo -e \"$ERROR_INFO\"\n  [[ ${#ACCESS_INFO} -gt 38 ]] && echo -e \"$ACCESS_INFO\"\n  [ -f \"$LOG_FILE\" ] && echo -e \"\\n\\n  See detailed log >>> $LOG_FILE \\n\\n\"\n  trap '' EXIT\n  exit\n}\n\n\nfunction log::error() {\n  # 错误日志\n  \n  local item; item=\"[$(date +'%Y-%m-%dT%H:%M:%S.%N%z')]: \\033[31mERROR:   \\033[0m$*\"\n  ERROR_INFO=\"${ERROR_INFO}${item}\\n  \"\n  echo -e \"${item}\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::info() {\n  # 基础日志\n  \n  printf \"[%s]: \\033[32mINFO:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::warning() {\n  # 警告日志\n  \n  printf \"[%s]: \\033[33mWARNING: \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::access() {\n  # 访问信息\n  \n  ACCESS_INFO=\"${ACCESS_INFO}$*\\n  \"\n  printf \"[%s]: \\033[32mINFO:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::exec() {\n  # 执行日志\n  \n  printf \"[%s]: \\033[34mEXEC:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" >> \"$LOG_FILE\"\n}\n\n\nfunction utils::version_to_number() {\n  # 版本号转数字\n\n  echo \"$@\" | awk -F. '{ printf(\"%d%03d%03d%03d\\n\", $1,$2,$3,$4); }';\n}\n\n\nfunction utils::retry {\n  # 重试\n\n  local retries=$1\n  shift\n\n  local count=0\n  until eval \"$*\"; do\n    exit=$?\n    wait=$((2 ** count))\n    count=$((count + 1))\n    if [ \"$count\" -lt \"$retries\" ]; then\n      echo \"Retry $count/$retries exited $exit, retrying in $wait seconds...\"\n      sleep $wait\n    else\n      echo \"Retry $count/$retries exited $exit, no more retries left.\"\n      return $exit\n    fi\n  done\n  return 0\n}\n\n\nfunction utils::quote() {\n  # 转义引号\n\n  # shellcheck disable=SC2046 \n  if [ $(echo \"$*\" | tr -d \"\\n\" | wc -c) -eq 0 ]; then\n    echo \"''\"\n  elif [ $(echo \"$*\" | tr -d \"[a-z][A-Z][0-9]:,.=~_/\\n-\" | wc -c) -gt 0 ]; then\n    printf \"%s\" \"$*\" | sed -e \"1h;2,\\$H;\\$!d;g\" -e \"s/'/\\'\\\"\\'\\\"\\'/g\" | sed -e \"1h;2,\\$H;\\$!d;g\" -e \"s/^/'/g\" -e \"s/$/'/g\"\n  else\n    echo \"$*\"\n  fi\n}\n\n\nfunction utils::download_file() {\n  # 下载文件\n  \n  local url=\"$1\"\n  local dest=\"$2\"\n  local unzip_tag=\"${3:-1}\"\n  \n  local dest_dirname; dest_dirname=$(dirname \"$dest\")\n  local filename; filename=$(basename \"$dest\")\n  \n  log::info \"[download]\" \"${filename}\"\n  command::exec \"${MGMT_NODE}\" \"\n    set -e\n    if [ ! -f \\\"${dest}\\\" ]; then\n      [ ! -d \\\"${dest_dirname}\\\" ] && mkdir -pv \\\"${dest_dirname}\\\" \n      wget --timeout=10 --waitretry=3 --tries=5 --retry-connrefused --no-check-certificate \\\"${url}\\\" -O \\\"${dest}\\\"\n      if [[ \\\"${unzip_tag}\\\" == \\\"unzip\\\" ]]; then\n        command -v unzip 2>/dev/null || apt-get install -y unzip\n        unzip -o \\\"${dest}\\\" -d \\\"${dest_dirname}\\\"\n      fi\n    else\n      echo \\\"${dest} is exists!\\\"\n    fi\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"download\" \"${filename}\" \"exit\"\n  return \"$status\"\n}\n\n\nfunction utils::is_element_in_array() {\n  # 判断是否在数组中存在元素\n\n  local -r element=\"${1}\"\n  local -r array=(\"${@:2}\")\n\n  local walker=''\n\n  for walker in \"${array[@]}\"\n  do\n    [[ \"${walker}\" = \"${element}\" ]] && return 0\n  done\n\n  return 1\n}\n\n\nfunction command::exec() {\n  # 执行命令\n\n  local host=${1:-}\n  shift\n  local command=\"$*\"\n  \n  if [[ \"${SUDO_TAG:-}\" == \"1\" ]]; then\n    sudo_options=\"sudo -H -n -u ${SUDO_USER}\"\n  \n    if [[ \"${SUDO_PASSWORD:-}\" != \"\" ]]; then\n       sudo_options=\"${sudo_options// -n/} -p \\\"\\\" -S <<< \\\"${SUDO_PASSWORD}\\\"\"\n    fi\n    command=\"$sudo_options bash -c $(utils::quote \"$command\")\"\n  fi\n  \n  command=\"$(utils::quote \"$command\")\"\n  \n  if [[ \"${host}\" == \"127.0.0.1\" ]]; then\n    # 本地执行\n    log::exec \"[command]\" \"bash -c $(printf \"%s\" \"${command//${SUDO_PASSWORD:-}/zzzzzz}\")\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval bash -c \"${command}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  else\n    # 远程执行\n    local ssh_cmd=\"ssh\"\n    if [[ \"${SSH_PASSWORD}\" != \"\" ]]; then\n      ssh_cmd=\"sshpass -p \\\"${SSH_PASSWORD}\\\" ${ssh_cmd}\"\n    elif [[ \"$SSH_PRIVATE_KEY\" != \"\" ]]; then\n      [ -f \"${SSH_PRIVATE_KEY}\" ] || { log::error \"[exec]\" \"ssh private_key:${SSH_PRIVATE_KEY} not found.\"; exit 1; }\n      ssh_cmd=\"${ssh_cmd} -i $SSH_PRIVATE_KEY\"\n    fi\n    log::exec \"[command]\" \"${ssh_cmd//${SSH_PASSWORD:-}/zzzzzz} ${SSH_OPTIONS} ${SSH_USER}@${host} -p ${SSH_PORT} bash -c $(printf \"%s\" \"${command//${SUDO_PASSWORD:-}/zzzzzz}\")\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval \"${ssh_cmd} ${SSH_OPTIONS} ${SSH_USER}@${host} -p ${SSH_PORT}\" bash -c '\"${command}\"' 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  fi\n  return $status\n}\n\n\nfunction command::scp() {\n  # 拷贝文件\n\n  local host=${1:-}\n  local src=${2:-}\n  local dest=${3:-/tmp/}\n  \n  if [[ \"${host}\" == \"127.0.0.1\" ]]; then\n    local command=\"cp -rf ${src} ${dest}\"\n    log::exec \"[command]\" \"bash -c \\\"${command}\\\"\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(bash -c \"${command}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  else\n    local scp_cmd=\"scp\"\n    if [[ \"${SSH_PASSWORD}\" != \"\" ]]; then\n      scp_cmd=\"sshpass -p \\\"${SSH_PASSWORD}\\\" ${scp_cmd}\"\n    elif [[ \"$SSH_PRIVATE_KEY\" != \"\" ]]; then\n      [ -f \"${SSH_PRIVATE_KEY}\" ] || { log::error \"[exec]\" \"ssh private_key:${SSH_PRIVATE_KEY} not found.\"; exit 1; }\n      scp_cmd=\"${scp_cmd} -i $SSH_PRIVATE_KEY\"\n    fi\n    log::exec \"[command]\" \"${scp_cmd} ${SSH_OPTIONS} -P ${SSH_PORT} -r ${src} ${SSH_USER}@${host}:${dest}\" >> \"$LOG_FILE\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval \"${scp_cmd} ${SSH_OPTIONS} -P ${SSH_PORT} -r ${src} ${SSH_USER}@${host}:${dest}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  fi\n  return $status\n}\n\n\nfunction script::init_node() {\n  # 节点初始化脚本\n  \n  # clean\n  sed -i -e \"/$KUBE_APISERVER/d\" -e '/-worker-/d' -e '/-master-/d' /etc/hosts\n  sed -i '/## Kainstall managed start/,/## Kainstall managed end/d' /etc/security/limits.conf /etc/systemd/system.conf /etc/bash.bashrc /etc/audit/rules.d/audit.rules  \n\n  # Disable selinux\n  sed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/config\n  setenforce 0\n  \n  # Disable swap\n  swapoff -a && sysctl -w vm.swappiness=0\n  sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n\n  # Disable firewalld\n  for target in firewalld python-firewall firewalld-filesystem iptables; do\n    systemctl stop $target &>/dev/null || true\n    systemctl disable $target &>/dev/null || true\n  done\n\n  # repo\n  local codename; codename=\"$(dpkg --status tzdata|grep Provides|cut -f2 -d'-')\"\n  [[ \"${SKIP_SET_OS_REPO,,}\" == \"false\" ]] && cp -fv /etc/apt/sources.list{,.bak}\n  [[ \"${SKIP_SET_OS_REPO,,}\" == \"false\" ]] && cat << EOF > /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/debian/ ${codename} main contrib non-free\ndeb-src http://mirrors.aliyun.com/debian/  ${codename} main contrib non-free\n\ndeb http://mirrors.aliyun.com/debian/  ${codename}-updates main contrib non-free\ndeb-src http://mirrors.aliyun.com/debian/  ${codename}-updates main contrib non-free\n\ndeb http://mirrors.aliyun.com/debian-security/  ${codename}/updates main contrib non-free\ndeb-src http://mirrors.aliyun.com/debian-security/  ${codename}/updates main contrib non-free\nEOF\n  apt update\n\n  echo -e '#!/bin/sh\\nexit 101' | install -m 755 /dev/stdin /usr/sbin/policy-rc.d\n\n  systemctl mask apt-daily.service apt-daily-upgrade.service\n  systemctl stop apt-daily.timer apt-daily-upgrade.timer\n  systemctl disable apt-daily.timer apt-daily-upgrade.timer\n  systemctl kill --kill-who=all apt-daily.service\n\ncat << EOF > /etc/apt/apt.conf.d/10cloudinit-disable\nAPT::Periodic::Enable \"0\";\n// undo what's in 20auto-upgrade\nAPT::Periodic::Update-Package-Lists \"0\";\nAPT::Periodic::Unattended-Upgrade \"0\";\nEOF\n\n  # Change limits\n  [ ! -f /etc/security/limits.conf_bak ] && cp /etc/security/limits.conf{,_bak}\n  cat << EOF >> /etc/security/limits.conf\n## Kainstall managed start\nroot soft nofile 655360\nroot hard nofile 655360\nroot soft nproc 655360\nroot hard nproc 655360\nroot soft core unlimited\nroot hard core unlimited\n\n* soft nofile 655360\n* hard nofile 655360\n* soft nproc 655360\n* hard nproc 655360\n* soft core unlimited\n* hard core unlimited\n## Kainstall managed end\nEOF\n\n  [ -f /etc/security/limits.d/20-nproc.conf ] && sed -i 's#4096#655360#g' /etc/security/limits.d/20-nproc.conf\n  cat << EOF >> /etc/systemd/system.conf\n## Kainstall managed start\nDefaultLimitCORE=infinity\nDefaultLimitNOFILE=655360\nDefaultLimitNPROC=655360\nDefaultTasksMax=75%\n## Kainstall managed end\nEOF\n\n   # Change sysctl\n   cat << EOF >  /etc/sysctl.d/99-kube.conf\n# https://www.kernel.org/doc/Documentation/sysctl/\n#############################################################################################\n# 调整虚拟内存\n#############################################################################################\n\n# Default: 30\n# 0 - 任何情况下都不使用swap。\n# 1 - 除非内存不足（OOM），否则不使用swap。\nvm.swappiness = 0\n\n# 内存分配策略\n#0 - 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。\n#1 - 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。\n#2 - 表示内核允许分配超过所有物理内存和交换空间总和的内存\nvm.overcommit_memory=1\n\n# OOM时处理\n# 1关闭，等于0时，表示当内存耗尽时，内核会触发OOM killer杀掉最耗内存的进程。\nvm.panic_on_oom=0\n\n# vm.dirty_background_ratio 用于调整内核如何处理必须刷新到磁盘的脏页。\n# Default value is 10.\n# 该值是系统内存总量的百分比，在许多情况下将此值设置为5是合适的。\n# 此设置不应设置为零。\nvm.dirty_background_ratio = 5\n\n# 内核强制同步操作将其刷新到磁盘之前允许的脏页总数\n# 也可以通过更改 vm.dirty_ratio 的值（将其增加到默认值30以上（也占系统内存的百分比））来增加\n# 推荐 vm.dirty_ratio 的值在60到80之间。\nvm.dirty_ratio = 60\n\n# vm.max_map_count 计算当前的内存映射文件数。\n# mmap 限制（vm.max_map_count）的最小值是打开文件的ulimit数量（cat /proc/sys/fs/file-max）。\n# 每128KB系统内存 map_count应该大约为1。 因此，在32GB系统上，max_map_count为262144。\n# Default: 65530\nvm.max_map_count = 2097152\n\n#############################################################################################\n# 调整文件\n#############################################################################################\n\nfs.may_detach_mounts = 1\n\n# 增加文件句柄和inode缓存的大小，并限制核心转储。\nfs.file-max = 2097152\nfs.nr_open = 2097152\nfs.suid_dumpable = 0\n\n# 同时可以拥有的的异步IO请求数目\nfs.aio-max-nr = 10000000\nfs.aio-nr = 75552\n\n# 文件监控\nfs.inotify.max_user_instances=8192\nfs.inotify.max_user_watches=524288\nfs.inotify.max_queued_events=16384\n\n#############################################################################################\n# 调整网络设置\n#############################################################################################\n\n# 为每个套接字的发送和接收缓冲区分配的默认内存量。\nnet.core.wmem_default = 25165824\nnet.core.rmem_default = 25165824\n\n# 为每个套接字的发送和接收缓冲区分配的最大内存量。\nnet.core.wmem_max = 25165824\nnet.core.rmem_max = 25165824\n\n# 除了套接字设置外，发送和接收缓冲区的大小\n# 必须使用net.ipv4.tcp_wmem和net.ipv4.tcp_rmem参数分别设置TCP套接字。\n# 使用三个以空格分隔的整数设置这些整数，分别指定最小，默认和最大大小。\n# 最大大小不能大于使用net.core.wmem_max和net.core.rmem_max为所有套接字指定的值。\n# 合理的设置是最小4KiB，默认64KiB和最大2MiB缓冲区。\nnet.ipv4.tcp_wmem = 20480 12582912 25165824\nnet.ipv4.tcp_rmem = 20480 12582912 25165824\n\n# 增加最大可分配的总缓冲区空间\n# 以页为单位（4096字节）进行度量\nnet.ipv4.tcp_mem = 65536 25165824 262144\nnet.ipv4.udp_mem = 65536 25165824 262144\n\n# 为每个套接字的发送和接收缓冲区分配的最小内存量。\nnet.ipv4.udp_wmem_min = 16384\nnet.ipv4.udp_rmem_min = 16384\n\n# 启用TCP窗口缩放，客户端可以更有效地传输数据，并允许在代理方缓冲该数据。\nnet.ipv4.tcp_window_scaling = 1\n\n# 提高同时接受连接数。\nnet.ipv4.tcp_max_syn_backlog = 10240\n\n# 将net.core.netdev_max_backlog的值增加到大于默认值1000\n# 可以帮助突发网络流量，特别是在使用数千兆位网络连接速度时，\n# 通过允许更多的数据包排队等待内核处理它们。\nnet.core.netdev_max_backlog = 65536\n\n# 增加选项内存缓冲区的最大数量\nnet.core.optmem_max = 25165824\n\n# 被动TCP连接的SYNACK次数。\nnet.ipv4.tcp_synack_retries = 2\n\n# 允许的本地端口范围。\nnet.ipv4.ip_local_port_range = 2048 65535\n\n# 防止TCP时间等待\n# Default: net.ipv4.tcp_rfc1337 = 0\nnet.ipv4.tcp_rfc1337 = 1\n\n# 减少tcp_fin_timeout连接的时间默认值\nnet.ipv4.tcp_fin_timeout = 15\n\n# 积压套接字的最大数量。\n# Default is 128.\nnet.core.somaxconn = 32768\n\n# 打开syncookies以进行SYN洪水攻击保护。\nnet.ipv4.tcp_syncookies = 1\n\n# 避免Smurf攻击\n# 发送伪装的ICMP数据包，目的地址设为某个网络的广播地址，源地址设为要攻击的目的主机，\n# 使所有收到此ICMP数据包的主机都将对目的主机发出一个回应，使被攻击主机在某一段时间内收到成千上万的数据包\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\n\n# 为icmp错误消息打开保护\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\n\n# 启用自动缩放窗口。\n# 如果延迟证明合理，这将允许TCP缓冲区超过其通常的最大值64K。\nnet.ipv4.tcp_window_scaling = 1\n\n# 打开并记录欺骗，源路由和重定向数据包\nnet.ipv4.conf.all.log_martians = 1\nnet.ipv4.conf.default.log_martians = 1\n\n# 告诉内核有多少个未附加的TCP套接字维护用户文件句柄。 万一超过这个数字，\n# 孤立的连接会立即重置，并显示警告。\n# Default: net.ipv4.tcp_max_orphans = 65536\nnet.ipv4.tcp_max_orphans = 65536\n\n# 不要在关闭连接时缓存指标\nnet.ipv4.tcp_no_metrics_save = 1\n\n# 启用RFC1323中定义的时间戳记：\n# Default: net.ipv4.tcp_timestamps = 1\nnet.ipv4.tcp_timestamps = 1\n\n# 启用选择确认。\n# Default: net.ipv4.tcp_sack = 1\nnet.ipv4.tcp_sack = 1\n\n# 增加 tcp-time-wait 存储桶池大小，以防止简单的DOS攻击。\n# net.ipv4.tcp_tw_recycle 已从Linux 4.12中删除。请改用net.ipv4.tcp_tw_reuse。\nnet.ipv4.tcp_max_tw_buckets = 14400\nnet.ipv4.tcp_tw_reuse = 1\n\n# accept_source_route 选项使网络接口接受设置了严格源路由（SSR）或松散源路由（LSR）选项的数据包。\n# 以下设置将丢弃设置了SSR或LSR选项的数据包。\nnet.ipv4.conf.all.accept_source_route = 0\nnet.ipv4.conf.default.accept_source_route = 0\n\n# 打开反向路径过滤\nnet.ipv4.conf.all.rp_filter = 1\nnet.ipv4.conf.default.rp_filter = 1\n\n# 禁用ICMP重定向接受\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.all.secure_redirects = 0\nnet.ipv4.conf.default.secure_redirects = 0\n\n# 禁止发送所有IPv4 ICMP重定向数据包。\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.default.send_redirects = 0\n\n# 开启IP转发.\nnet.ipv4.ip_forward = 1\n\n# 禁止IPv6\nnet.ipv6.conf.lo.disable_ipv6=1\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n\n# 要求iptables不对bridge的数据进行处理\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-arptables = 1\n\n# arp缓存\n# 存在于 ARP 高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是 128\nnet.ipv4.neigh.default.gc_thresh1=2048\n# 保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512\nnet.ipv4.neigh.default.gc_thresh2=4096\n# 保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是 1024\nnet.ipv4.neigh.default.gc_thresh3=8192\n\n# 持久连接\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_intvl = 30\nnet.ipv4.tcp_keepalive_probes = 10\n\n# conntrack表\nnet.nf_conntrack_max=1048576\nnet.netfilter.nf_conntrack_max=1048576\nnet.netfilter.nf_conntrack_buckets=262144\nnet.netfilter.nf_conntrack_tcp_timeout_fin_wait=30\nnet.netfilter.nf_conntrack_tcp_timeout_time_wait=30\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait=15\nnet.netfilter.nf_conntrack_tcp_timeout_established=300\n\n#############################################################################################\n# 调整内核参数\n#############################################################################################\n\n# 地址空间布局随机化（ASLR）是一种用于操作系统的内存保护过程，可防止缓冲区溢出攻击。\n# 这有助于确保与系统上正在运行的进程相关联的内存地址不可预测，\n# 因此，与这些流程相关的缺陷或漏洞将更加难以利用。\n# Accepted values: 0 = 关闭, 1 = 保守随机化, 2 = 完全随机化\nkernel.randomize_va_space = 2\n\n# 调高 PID 数量\nkernel.pid_max = 65536\nkernel.threads-max=30938\n\n# coredump\nkernel.core_pattern=core\n\n# 决定了检测到soft lockup时是否自动panic，缺省值是0\nkernel.softlockup_all_cpu_backtrace=1\nkernel.softlockup_panic=1\nEOF\n\n  # history\n  cat << EOF >> /etc/bash.bashrc\n## Kainstall managed start\n# history actions record，include action time, user, login ip\nHISTFILESIZE=5000\nHISTSIZE=5000\nUSER_IP=\\$(who -u am i 2>/dev/null | awk '{print \\$NF}' | sed -e 's/[()]//g')\nif [ -z \\$USER_IP ]\nthen\n  USER_IP=\\$(hostname -i)\nfi\nHISTTIMEFORMAT=\"%Y-%m-%d %H:%M:%S \\$USER_IP:\\$(whoami) \"\nexport HISTFILESIZE HISTSIZE HISTTIMEFORMAT\n\n# PS1\nPS1='\\[\\033[0m\\]\\[\\033[1;36m\\][\\u\\[\\033[0m\\]@\\[\\033[1;32m\\]\\h\\[\\033[0m\\] \\[\\033[1;31m\\]\\w\\[\\033[0m\\]\\[\\033[1;36m\\]]\\[\\033[33;1m\\]\\\\$ \\[\\033[0m\\]'\n## Kainstall managed end\nEOF\n\n   # journal\n   mkdir -p /var/log/journal /etc/systemd/journald.conf.d\n   cat << EOF > /etc/systemd/journald.conf.d/99-prophet.conf\n[Journal]\n# 持久化保存到磁盘\nStorage=persistent\n# 压缩历史日志\nCompress=yes\nSyncIntervalSec=5m\nRateLimitInterval=30s\nRateLimitBurst=1000\n# 最大占用空间 10G\nSystemMaxUse=10G\n# 单日志文件最大 200M\nSystemMaxFileSize=200M\n# 日志保存时间 3 周\nMaxRetentionSec=3week\n# 不将日志转发到 syslog\nForwardToSyslog=no\nEOF\n\n  # motd\n  cat << EOF > /etc/profile.d/zz-ssh-login-info.sh\n#!/bin/sh\n#\n# @Time    : 2020-02-04\n# @Author  : lework\n# @Desc    : ssh login banner\n\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:\\$PATH\nshopt -q login_shell && : || return 0\necho -e \"\\033[0;32m\n ██╗  ██╗ █████╗ ███████╗\n ██║ ██╔╝██╔══██╗██╔════╝\n █████╔╝ ╚█████╔╝███████╗\n ██╔═██╗ ██╔══██╗╚════██║\n ██║  ██╗╚█████╔╝███████║\n ╚═╝  ╚═╝ ╚════╝ ╚══════ by kainstall\\033[0m\"\n\n# os\nupSeconds=\"\\$(cut -d. -f1 /proc/uptime)\"\nsecs=\\$((\\${upSeconds}%60))\nmins=\\$((\\${upSeconds}/60%60))\nhours=\\$((\\${upSeconds}/3600%24))\ndays=\\$((\\${upSeconds}/86400))\nUPTIME_INFO=\\$(printf \"%d days, %02dh %02dm %02ds\" \"\\$days\" \"\\$hours\" \"\\$mins\" \"\\$secs\")\n\nif [ -f /etc/redhat-release ] ; then\n    PRETTY_NAME=\\$(< /etc/redhat-release)\n\nelif [ -f /etc/debian_version ]; then\n   DIST_VER=\\$(</etc/debian_version)\n   PRETTY_NAME=\"\\$(grep PRETTY_NAME /etc/os-release | sed -e 's/PRETTY_NAME=//g' -e  's/\"//g') (\\$DIST_VER)\"\n\nelse\n    PRETTY_NAME=\\$(cat /etc/*-release | grep \"PRETTY_NAME\" | sed -e 's/PRETTY_NAME=//g' -e 's/\"//g')\nfi\n\nif [[ -d \"/system/app/\" && -d \"/system/priv-app\" ]]; then\n    model=\"\\$(getprop ro.product.brand) \\$(getprop ro.product.model)\"\n\nelif [[ -f /sys/devices/virtual/dmi/id/product_name ||\n        -f /sys/devices/virtual/dmi/id/product_version ]]; then\n    model=\"\\$(< /sys/devices/virtual/dmi/id/product_name)\"\n    model+=\" \\$(< /sys/devices/virtual/dmi/id/product_version)\"\n\nelif [[ -f /sys/firmware/devicetree/base/model ]]; then\n    model=\"\\$(< /sys/firmware/devicetree/base/model)\"\n\nelif [[ -f /tmp/sysinfo/model ]]; then\n    model=\"\\$(< /tmp/sysinfo/model)\"\nfi\n\nMODEL_INFO=\\${model}\nKERNEL=\\$(uname -srmo)\nUSER_NUM=\\$(who -u | wc -l)\nRUNNING=\\$(ps ax | wc -l | tr -d \" \")\n\n# disk\ntotaldisk=\\$(df -h -x devtmpfs -x tmpfs -x debugfs -x aufs -x overlay --total 2>/dev/null | tail -1)\ndisktotal=\\$(awk '{print \\$2}' <<< \"\\${totaldisk}\")\ndiskused=\\$(awk '{print \\$3}' <<< \"\\${totaldisk}\")\ndiskusedper=\\$(awk '{print \\$5}' <<< \"\\${totaldisk}\")\nDISK_INFO=\"\\033[0;33m\\${diskused}\\033[0m of \\033[1;34m\\${disktotal}\\033[0m disk space used (\\033[0;33m\\${diskusedper}\\033[0m)\"\n\n# cpu\ncpu=\\$(awk -F':' '/^model name/ {print \\$2}' /proc/cpuinfo | uniq | sed -e 's/^[ \\t]*//')\ncpun=\\$(grep -c '^processor' /proc/cpuinfo)\ncpuc=\\$(grep '^cpu cores' /proc/cpuinfo | tail -1 | awk '{print \\$4}')\ncpup=\\$(grep '^physical id' /proc/cpuinfo | wc -l)\nCPU_INFO=\"\\${cpu} \\${cpup}P \\${cpuc}C \\${cpun}L\"\n\n# get the load averages\nread one five fifteen rest < /proc/loadavg\nLOADAVG_INFO=\"\\033[0;33m\\${one}\\033[0m / \\${five} / \\${fifteen} with \\033[1;34m\\$(( cpun*cpuc ))\\033[0m core(s) at \\033[1;34m\\$(grep '^cpu MHz' /proc/cpuinfo | tail -1 | awk '{print \\$4}')\\033 MHz\"\n\n# mem\nMEM_INFO=\"\\$(cat /proc/meminfo | awk '/MemTotal:/{total=\\$2/1024/1024;next} /MemAvailable:/{use=total-\\$2/1024/1024; printf(\"\\033[0;33m%.2fGiB\\033[0m of \\033[1;34m%.2fGiB\\033[0m RAM used (\\033[0;33m%.2f%%\\033[0m)\",use,total,(use/total)*100);}')\"\n\n# network\n# extranet_ip=\" and \\$(curl -s ip.cip.cc)\"\nIP_INFO=\"\\$(ip a|grep -E '^[0-9]+: em*|^[0-9]+: eno*|^[0-9]+: enp*|^[0-9]+: ens*|^[0-9]+: eth*|^[0-9]+: wlp*' -A2|grep inet|awk -F ' ' '{print \\$2}'|cut -f1 -d/|xargs echo)\"\n\n# Container info\nCONTAINER_INFO=\"\\$(sudo /usr/bin/crictl ps -a -o yaml 2> /dev/null | awk '/^  state: /{gsub(\"CONTAINER_\", \"\", \\$NF) ++S[\\$NF]}END{for(m in S) printf \"%s%s:%s \",substr(m,1,1),tolower(substr(m,2)),S[m]}')Images:\\$(sudo /usr/bin/crictl images -q 2> /dev/null | wc -l)\"\n\n# info\necho -e \"\n Information as of: \\033[1;34m\\$(date +\"%Y-%m-%d %T\")\\033[0m\n \n \\033[0;1;31mProduct\\033[0m............: \\${MODEL_INFO}\n \\033[0;1;31mOS\\033[0m.................: \\${PRETTY_NAME}\n \\033[0;1;31mKernel\\033[0m.............: \\${KERNEL}\n \\033[0;1;31mCPU\\033[0m................: \\${CPU_INFO}\n\n \\033[0;1;31mHostname\\033[0m...........: \\033[1;34m\\$(hostname)\\033[0m\n \\033[0;1;31mIP Addresses\\033[0m.......: \\033[1;34m\\${IP_INFO}\\033[0m\n\n \\033[0;1;31mUptime\\033[0m.............: \\033[0;33m\\${UPTIME_INFO}\\033[0m\n \\033[0;1;31mMemory\\033[0m.............: \\${MEM_INFO}\n \\033[0;1;31mLoad Averages\\033[0m......: \\${LOADAVG_INFO}\n \\033[0;1;31mDisk Usage\\033[0m.........: \\${DISK_INFO} \n\n \\033[0;1;31mUsers online\\033[0m.......: \\033[1;34m\\${USER_NUM}\\033[0m\n \\033[0;1;31mRunning Processes\\033[0m..: \\033[1;34m\\${RUNNING}\\033[0m\n \\033[0;1;31mContainer Info\\033[0m.....: \\${CONTAINER_INFO}\n\"\nEOF\n\n  chmod +x /etc/profile.d/zz-ssh-login-info.sh\n  echo 'ALL ALL=(ALL) NOPASSWD:/usr/bin/crictl' > /etc/sudoers.d/crictl\n\n  # time sync\n  ntpd --help >/dev/null 2>&1 && apt-get remove -y ntp\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && apt-get install -y chrony \n  [ ! -f /etc/chrony.conf_bak ] && cp /etc/chrony.conf{,_bak} #备份默认配置\n  cat << EOF > /etc/chrony.conf\nserver ntp.aliyun.com iburst\nserver cn.ntp.org.cn iburst\nserver ntp.shu.edu.cn iburst\nserver 0.cn.pool.ntp.org iburst\nserver 1.cn.pool.ntp.org iburst\nserver 2.cn.pool.ntp.org iburst\nserver 3.cn.pool.ntp.org iburst\n\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nlogdir /var/log/chrony\nEOF\n\n  timedatectl set-timezone Asia/Shanghai\n  chronyd -q -t 1 'server cn.pool.ntp.org iburst maxsamples 1'\n  systemctl enable chrony\n  systemctl start chrony\n  chronyc sources -v\n  chronyc sourcestats\n  hwclock --systohc\n\n  # package\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && apt-get install -y apt-transport-https ca-certificates curl wget gnupg lsb-release\n\n  # ipvs\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && apt-get install -y ipvsadm ipset sysstat conntrack libseccomp2\n  module=(\n  ip_vs\n  ip_vs_rr\n  ip_vs_wrr\n  ip_vs_sh\n  overlay\n  nf_conntrack\n  br_netfilter\n  )\n  [ -f /etc/modules-load.d/ipvs.conf ] && cp -f /etc/modules-load.d/ipvs.conf{,_bak}\n  for kernel_module in \"${module[@]}\";do\n     /sbin/modinfo -F filename \"$kernel_module\" |& grep -qv ERROR && echo \"$kernel_module\" >> /etc/modules-load.d/ipvs.conf\n  done\n  systemctl restart systemd-modules-load\n  systemctl enable systemd-modules-load\n  sysctl --system\n\n  # audit\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && apt-get install -y auditd audispd-plugins\ncat << EOF >> /etc/audit/rules.d/audit.rules\n## Kainstall managed start\n# Ignore errors\n-i\n\n# SYSCALL\n-a always,exit -F arch=b64 -S kill,tkill,tgkill -F a1=9 -F key=trace_kill_9\n-a always,exit -F arch=b64 -S kill,tkill,tgkill -F a1=15 -F key=trace_kill_15\n\n# docker\n-w /usr/bin/dockerd -k docker\n-w /var/lib/docker -k docker\n-w /etc/docker -k docker\n-w /usr/lib/systemd/system/docker.service -k docker\n-w /etc/systemd/system/docker.service -k docker\n-w /usr/lib/systemd/system/docker.socket -k docker\n-w /etc/default/docker -k docker\n-w /etc/sysconfig/docker -k docker\n-w /etc/docker/daemon.json -k docker\n\n# containerd\n-w /usr/bin/containerd -k containerd\n-w /var/lib/containerd -k containerd\n-w /usr/lib/systemd/system/containerd.service -k containerd\n-w /etc/containerd/config.toml -k containerd\n\n# cri-o\n-w /usr/bin/crio -k cri-o\n-w /etc/crio -k cri-o\n\n# runc \n-w /usr/bin/runc -k runc\n\n# kube\n-w /usr/bin/kubeadm -k kubeadm\n-w /usr/bin/kubelet -k kubelet\n-w /usr/bin/kubectl -k kubectl\n-w /var/lib/kubelet -k kubelet\n-w /etc/kubernetes -k kubernetes\n## Kainstall managed end\nEOF\n  chmod 600 /etc/audit/rules.d/audit.rules\n  sed -i 's#max_log_file =.*#max_log_file = 80#g' /etc/audit/auditd.conf \n  if [ -f /usr/libexec/initscripts/legacy-actions/auditd/restart ]; then\n     /usr/libexec/initscripts/legacy-actions/auditd/restart\n  else\n     systemctl stop auditd && systemctl start auditd\n  fi\n  systemctl enable auditd\n\n  grep single-request-reopen /etc/resolv.conf || sed -i '1ioptions timeout:2 attempts:3 rotate single-request-reopen' /etc/resolv.conf\n\n  ipvsadm --clear\n  iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n}\n\n\nfunction script::upgrade_kernel() {\n  # 升级内核\n\n  local codename; codename=\"$(dpkg --status tzdata|grep Provides|cut -f2 -d'-')\"\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]]; then\n    echo \"deb [trusted=yes] http://mirrors.aliyun.com/debian ${codename}-backports main\" > /etc/apt/sources.list.d/backports.list\n    apt update\n    apt -t \"${codename}-backports\" install linux-image-amd64 linux-headers-amd64 -y\n  fi \n}\n\n\nfunction script::upgrage_kube() {\n  # 节点软件升级\n\n  local role=${1:-init}\n  local version=\"=${2:-latest}-00\"\n  version=\"${version#=latest-00}\"\n\n  set -e\n  echo '[install] kubeadm'\n  kubeadm version\n  apt-get update\n  apt-get install -y \"kubeadm${version}\"\n  kubeadm version\n\n  echo '[upgrade]'\n  if [[ \"$role\" == \"init\" ]]; then\n    local plan_info; plan_info=$(kubeadm upgrade plan)\n    local v; v=$(printf \"%s\" \"$plan_info\" | grep 'kubeadm upgrade apply ' | awk '{print $4}'| tail -1 )\n    printf \"%s\\n\" \"${plan_info}\"\n    kubeadm upgrade apply \"${v}\" -y\n  else\n    kubeadm upgrade node\n  fi\n\n  echo '[install] kubelet kubectl'\n  kubectl version --client=true\n  apt-get install -y \"kubelet${version}\" \"kubectl${version}\"\n  kubectl version --client=true\n\n  [ -f /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf ] && \\\n    sed -i 's#^\\[Service\\]#[Service]\\nCPUAccounting=true\\nMemoryAccounting=true#g' /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\n\n  systemctl daemon-reload\n  systemctl restart kubelet\n}\n\n\nfunction script::install_docker() {\n  # 安装 docker\n  \n  local version=\"=${1:-latest}-00\"\n  version=\"${version#=latest-00}\"\n\n  wget -qO - http://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo apt-key add -\n  echo \"deb [trusted=yes] http://mirrors.aliyun.com/docker-ce/linux/debian $(lsb_release -cs) stable\" > /etc/apt/sources.list.d/docker-ce.list\n  apt-get update\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which docker)\" ]  && apt remove -y docker-ce docker-ce-cli containerd.io\n    apt-get install -y \"docker-ce${version}\" \"docker-ce-cli${version}\" containerd.io bash-completion\n  fi\n\n  apt-mark hold docker-ce docker-ce-cli containerd.io\n\n  [ -f /usr/share/bash-completion/completions/docker ] && \\\n    cp -f /usr/share/bash-completion/completions/docker /etc/bash_completion.d/\n\n  [ ! -d /etc/docker ] && mkdir /etc/docker\n  cat << EOF > /etc/docker/daemon.json\n{\n  \"data-root\": \"/var/lib/docker\",\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"200m\",\n    \"max-file\": \"5\"\n  },\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 655360,\n      \"Soft\": 655360\n    },\n    \"nproc\": {\n      \"Name\": \"nproc\",\n      \"Hard\": 655360,\n      \"Soft\": 655360\n    }\n  },\n  \"live-restore\": true,\n  \"oom-score-adjust\": -1000,\n  \"max-concurrent-downloads\": 10,\n  \"max-concurrent-uploads\": 10,\n  \"registry-mirrors\": [\n    \"https://yssx4sxy.mirror.aliyuncs.com/\"\n  ]\n}\nEOF\n  sed -i 's|#oom_score = 0|oom_score = -999|' /etc/containerd/config.toml\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///var/run/dockershim.sock\nimage-endpoint: unix:///var/run/dockershim.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n  \n  systemctl enable containerd\n  systemctl restart containerd\n\n  systemctl enable docker\n  systemctl restart docker\n}\n\n\nfunction script::install_containerd() {\n  # 安装 containerd\n  \n  local version=\"=${1:-latest}-00\"\n  version=\"${version#=latest-00}\"\n\n  wget -qO - http://mirrors.aliyun.com/docker-ce/linux/debian/gpg | sudo apt-key add -\n  echo \"deb [trusted=yes] http://mirrors.aliyun.com/docker-ce/linux/debian $(lsb_release -cs) stable\" > /etc/apt/sources.list.d/docker-ce.list\n  apt-get update\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which runc)\" ]  && apt remove -y runc\n    [ -f \"$(which containerd)\" ]  && apt remove -y containerd.io\n    aapt-get install -y containerd.io\"${version}\" containernetworking bash-completion || apt-get install -y containerd.io\"${version}\" bash-completion\n  fi\n\n  [ -d /etc/bash_completion.d ] && crictl completion bash > /etc/bash_completion.d/crictl\n\n  containerd config default > /etc/containerd/config.toml\n  sed -i -e \"s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#registry.k8s.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#https://registry-1.docker.io#https://yssx4sxy.mirror.aliyuncs.com#g\" \\\n         -e \"s#SystemdCgroup = false#SystemdCgroup = true#g\" \\\n         -e \"s#oom_score = 0#oom_score = -999#\" \\\n         -e \"s#max_container_log_line_size = 16384#max_container_log_line_size = 65535#\" \\\n         -e \"s#max_concurrent_downloads = 3#max_concurrent_downloads = 10#g\" /etc/containerd/config.toml\n\n  grep docker.io /etc/containerd/config.toml ||  sed -i -e \"/registry.mirrors]/a\\ \\ \\ \\ \\ \\ \\ \\ [plugins.\\\"io.containerd.grpc.v1.cri\\\".registry.mirrors.\\\"docker.io\\\"]\\n           endpoint = [\\\"https://yssx4sxy.mirror.aliyuncs.com\\\"]\" \\\n       /etc/containerd/config.toml\n\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n  \n  systemctl restart containerd\n  systemctl enable containerd\n}\n\nfunction script::install_cri-o() {\n  # 安装 cri-o\n  \n  local version=\"${1:-latest}\"\n  version=\"${version##latest}\"\n  os=\"Debian_Unstable\"\n\n  echo \"deb [trusted=yes] http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$os/ /\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\n  echo \"deb [trusted=yes] http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$version/$os/ /\" > \"/etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$version.list\"\n\n  wget -qO - \"https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$version/$os/Release.key\" | apt-key add -\n  wget -qO - \"https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$os/Release.key\" | apt-key add -\n\n  apt-get update\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which runc)\" ]  && apt remove -y runc\n    [ -f \"$(which crio)\" ]  && apt remove -y cri-o\n    [ -f \"$(which docker)\" ]  && apt remove -y docker-ce docker-ce-cli containerd.io\n    apt-get install -y cri-o runc bash-completion\n  fi\n\n  [ -d /etc/bash_completion.d ] && \\\n    { crictl completion bash >  /etc/bash_completion.d/crictl; \\\n      crio completion bash > /etc/bash_completion.d/crio; \\\n      crio-status completion bash > /etc/bash_completion.d/crio-status; }\n\n  crio config --default > /etc/crio/crio.conf\n  sed -i -e \"s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#registry.k8s.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e 's|#registries = \\[|registries = [\"docker.io\", \"quay.io\"]|g' /etc/crio/crio.conf\n\n  [ -d /etc/containers/registries.conf.d ] && cat << EOF > /etc/containers/registries.conf.d/000-dockerio.conf\n[[registry]]\nprefix = \"docker.io\"\ninsecure = false\nblocked = false\nlocation = \"docker.io\"\n\n[[registry.mirror]]\nlocation = \"yssx4sxy.mirror.aliyuncs.com\"\ninsecure = true\nEOF\n\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///var/run/crio/crio.sock\nimage-endpoint: unix:///var/run/crio/crio.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n\n  sed -i \"s#10.85.0.0/16#${KUBE_POD_SUBNET:-10.85.0.0/16}#g\" /etc/cni/net.d/100-crio-bridge.conf\n  cat << EOF > /etc/cni/net.d/10-crio.conf\n{\n$(grep cniVersion /etc/cni/net.d/100-crio-bridge.conf)\n    \"name\": \"crio\",\n    \"type\": \"flannel\"\n}\nEOF\n  mv /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/10-crio.conf /etc/cni/net.d/200-loopback.conf /tmp/\n  [ ! -d /usr/lib/cri-o-runc/sbin/ ] && mkdir -p /usr/lib/cri-o-runc/sbin/\n  [ ! -f /usr/sbin/runc ] && ln -sv \"$(which runc)\" /usr/sbin/runc\n  [ ! -f /usr/lib/cri-o-runc/sbin/runc ] && ln -sv \"$(which runc)\" /usr/lib/cri-o-runc/sbin/runc\n  systemctl restart crio\n  systemctl enable crio\n}\n\n\nfunction script::install_kube() {\n  # 安装kube组件\n  \n  local version=\"=${1:-latest}-00\"\n  version=\"${version#=latest-00}\"\n  \n  repo=\"${version%.*}\"\n  repo=\"${repo//-}\"\n  [ \"${repo}\" == \"\" ] && repo=\"1.29\"\n\n  echo \"deb [trusted=yes] https://pkgs.k8s.io/core:/stable:/v${repo}/deb/ /\" > /etc/apt/sources.list.d/kubernetes.list\n  apt-get update\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which kubeadm)\" ]  && apt remove -y kubeadm\n    [ -f \"$(which kubelet)\" ]  && apt remove -y kubelet\n    [ -f \"$(which kubectl)\" ]  && apt remove -y kubectl\n    apt-get install -y \"kubeadm${version}\" \"kubelet${version}\" \"kubectl${version}\" kubernetes-cni\n  fi\n\n  [ -d /etc/bash_completion.d ] && \\\n    { kubectl completion bash > /etc/bash_completion.d/kubectl; \\\n      kubeadm completion bash > /etc/bash_completion.d/kubadm; }\n\n  [ ! -d /usr/lib/systemd/system/kubelet.service.d ] && mkdir -p /usr/lib/systemd/system/kubelet.service.d\n  cat << EOF > /usr/lib/systemd/system/kubelet.service.d/11-cgroup.conf\n[Service]\nCPUAccounting=true\nMemoryAccounting=true\nBlockIOAccounting=true\nExecStartPre=/bin/bash -c '/bin/mkdir -p /sys/fs/cgroup/{cpuset,memory,hugetlb,systemd,pids,\"cpu,cpuacct\"}/{system,kube,kubepods}.slice||:'\nSlice=kube.slice\nEOF\n  systemctl daemon-reload\n\n  systemctl enable kubelet\n  systemctl restart kubelet\n}\n\n\nfunction script::install_haproxy() {\n  # 安装haproxy\n   \n  local api_servers=\"$*\"\n   \n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which haproxy)\" ] && apt remove -y haproxy\n    apt-get install -y haproxy rsyslog\n  fi\n\n  [ ! -f /etc/haproxy/haproxy.cfg_bak ] && cp /etc/haproxy/haproxy.cfg{,_bak}\ncat << EOF > /etc/haproxy/haproxy.cfg\nglobal\n  log /dev/log    local0\n  log /dev/log    local1 notice\n  tune.ssl.default-dh-param 2048\n\ndefaults\n  log global\n  mode http\n  option dontlognull\n  timeout connect 5000ms\n  timeout client 600000ms\n  timeout server 600000ms\n\nlisten stats\n    bind :19090\n    mode http\n    balance\n    stats uri /haproxy_stats\n    stats auth admin:admin123\n    stats admin if TRUE\n\nfrontend kube-apiserver-https\n   mode tcp\n   option tcplog\n   bind :6443\n   default_backend kube-apiserver-backend\n\nbackend kube-apiserver-backend\n    mode tcp\n    balance roundrobin\n    stick-table type ip size 200k expire 30m\n    stick on src\n$(index=1;for h in $api_servers;do echo \"    server apiserver${index} $h:6443 check\";index=$((index+1));done)\nEOF\n\ncat <<EOF > /etc/rsyslog.d/haproxy.conf\nlocal0.* /var/log/haproxy.log\nlocal1.* /var/log/haproxy.log\nEOF\n\n  systemctl enable rsyslog\n  systemctl restart rsyslog\n  systemctl enable haproxy\n  systemctl restart haproxy\n}\n\n\nfunction check::command_exists() {\n  # 检查命令是否存在\n  \n  local cmd=${1}\n  local package=${2}\n\n  if command -V \"$cmd\" > /dev/null 2>&1; then\n    log::info \"[check]\" \"$cmd command exists.\"\n  else\n    log::warning \"[check]\" \"I require $cmd but it's not installed.\"\n    log::warning \"[check]\" \"install $package package.\"\n    command::exec \"127.0.0.1\" \"apt-get install -y ${package}\"\n    check::exit_code \"$?\" \"check\" \"$package install\" \"exit\"\n  fi\n}\n\n\nfunction check::command() {\n  # 检查用到的命令\n  \n  check::command_exists ssh openssh-clients\n  check::command_exists sshpass sshpass\n  check::command_exists wget wget\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && check::command_exists tar tar\n}\n\n\nfunction check::ssh_conn() {\n  # 检查ssh连通性\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    [ \"$host\" == \"127.0.0.1\" ] && continue\n    command::exec \"${host}\" \"echo 0\"\n    check::exit_code \"$?\" \"check\" \"ssh $host connection\" \"exit\"\n  done\n}\n\n\nfunction check::os() {\n  # 检查os系统支持\n\n  log::info \"[check]\" \"os support: ${OS_SUPPORT}\"\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"\n      [ -f /etc/os-release ] && source /etc/os-release\n      echo client_os:\\${ID:-}\\${VERSION_ID:-}\n      if [[ \\\"${OS_SUPPORT}\\\" == *\\\"\\${ID:-}\\${VERSION_ID:-}\\\"* ]]; then\n        exit 0\n      fi\n      exit 1\n    \"\n    check::exit_code \"$?\" \"check\" \"$host os support\" \"exit\"\n  done\n}\n\n\nfunction check::kernel() {\n  # 检查os kernel 版本\n\n  local version=${1:-}\n  log::info \"[check]\" \"kernel version not less than ${version}\"\n  version=$(echo \"${version}\" | awk -F. '{ printf(\"%d%03d%03d\\n\", $1,$2,$3); }')\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"\n      kernel_version=\\$(uname -r)\n      kernel_version=\\$(echo \\${kernel_version/-*} | awk -F. '{ printf(\\\"%d%03d%03d\\n\\\", \\$1,\\$2,\\$3); }') \n      echo kernel_version \\${kernel_version}\n      [[ \\${kernel_version} -ge ${version} ]] && exit 0 || exit 1\n    \"                                                                                                                                                 \n    check::exit_code \"$?\" \"check\" \"$host kernel version\" \"exit\"\n  done\n\n}\n\nfunction check::apiserver_conn() {\n  # 检查apiserver连通性\n\n  command::exec \"${MGMT_NODE}\" \"kubectl get node\"\n  check::exit_code \"$?\" \"check\" \"conn apiserver\" \"exit\"\n}\n\n\nfunction check::exit_code() {\n  # 检查返回码\n\n  local code=${1:-}\n  local app=${2:-}\n  local desc=${3:-}\n  local exit_script=${4:-}\n\n  if [[ \"${code}\" == \"0\" ]]; then\n    log::info \"[${app}]\" \"${desc} succeeded.\"\n  else\n    log::error \"[${app}]\" \"${desc} failed.\"\n    [[ \"$exit_script\" == \"exit\" ]] && exit \"$code\"\n  fi\n}\n\n\nfunction check::preflight() {\n  # 预检\n  \n  # check command\n  check::command\n\n  # check ssh conn\n  check::ssh_conn\n\n  # check os\n  check::os\n\n  # check os kernel\n  [[ \"${KUBE_NETWORK:-}\" == \"cilium\" && \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]] && check::kernel 4.9.17\n\n  # check apiserver conn\n  if [[ $(( ${ADD_TAG:-0} + ${DEL_TAG:-0} + ${UPGRADE_TAG:-0} + ${RENEW_CERT_TAG:-0} )) -gt 0 ]]; then\n    check::apiserver_conn\n  fi\n  \n  # check docker cri\n  [[ \"${KUBE_CRI}\" == \"docker\" && (\"${KUBE_VERSION}\" == \"latest\" || \"${KUBE_VERSION}\" =~ 1.24 ) ]] && { log::error \"[limit]\" \"$KUBE_CRI is not supported, only [containerd,cri-o]\"; exit 1; }\n}\n\n\nfunction install::package() {\n  # 安装包\n \n  if [[ \"${KUBE_CRI}\" == \"cri-o\" && \"${KUBE_CRI_VERSION}\" == \"latest\" ]]; then\n    KUBE_CRI_VERSION=\"${KUBE_VERSION}\"\n    if [[ \"${KUBE_CRI_VERSION}\" == \"latest\" ]]; then\n      if command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"; then\n        KUBE_CRI_VERSION=\"${COMMAND_OUTPUT#v}\"\n      else\n        log::error \"[install]\" \"get kubernetes stable version error. Please specify the version!\"\n        exit 1\n      fi\n    fi\n    KUBE_CRI_VERSION=\"${KUBE_CRI_VERSION%.*}\"\n  fi\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    # install cri\n    log::info \"[install]\" \"install ${KUBE_CRI} on $host.\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_\"${KUBE_CRI}\")\n      script::install_${KUBE_CRI} $KUBE_CRI_VERSION\n    \"\n    check::exit_code \"$?\" \"install\" \"install ${KUBE_CRI} on $host\"\n\n    # install kube\n    log::info \"[install]\" \"install kube on $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_kube)\n      script::install_kube $KUBE_VERSION\n    \"\n    check::exit_code \"$?\" \"install\" \"install kube on $host\"\n  done\n\n  local apiservers=$MASTER_NODES\n  if [[ \"$apiservers\" == \"127.0.0.1\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"ip -o route get to 8.8.8.8 | sed -n 's/.*src \\([0-9.]\\+\\).*/\\1/p'\"\n    get::command_output \"apiservers\" \"$?\"\n  fi\n\n  if [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n    \"\n    get::command_output \"apiservers\" \"$?\"\n  fi\n  \n  for host in $WORKER_NODES\n  do\n    # install haproxy\n    log::info \"[install]\" \"install haproxy on $host\"\n  command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_haproxy)\n      script::install_haproxy \\\"$apiservers\\\"\n  \"\n    check::exit_code \"$?\" \"install\" \"install haproxy on $host\"\n  done\n  \n  # 10年证书\n  if [[ \"${CERT_YEAR_TAG:-}\" == \"1\" ]]; then\n    local version=\"${KUBE_VERSION}\"\n    \n    if [[ \"${version}\" == \"latest\" ]]; then\n      if command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"; then\n        version=\"${COMMAND_OUTPUT#v}\"\n      else\n        log::error \"[install]\" \"get kubernetes stable version error. Please specify the version!\"\n        exit 1\n      fi\n    fi\n    \n    log::info \"[install]\" \"download kubeadm 10 years certs client\"\n    local certs_file=\"${OFFLINE_DIR}/bins/kubeadm-linux-amd64\"\n    MGMT_NODE=\"127.0.0.1\" utils::download_file \"${GITHUB_PROXY}https://github.com/lework/kubeadm-certs/releases/download/v${version}/kubeadm-linux-amd64\" \"${certs_file}\"\n    \n    for host in $MASTER_NODES $WORKER_NODES\n    do\n      log::info \"[install]\" \"scp kubeadm client to $host\"\n      command::scp \"${host}\" \"${certs_file}\" \"/tmp/kubeadm-linux-amd64\"\n      check::exit_code \"$?\" \"install\" \"scp kubeadm client to $host\" \"exit\"\n\n      command::exec \"${host}\" \"\n        set -e\n        if [[ -f /tmp/kubeadm-linux-amd64 ]]; then\n        [[ -f /usr/bin/kubeadm && ! -f /usr/bin/kubeadm_src ]] && mv -fv /usr/bin/kubeadm{,_src}\n          mv -fv /tmp/kubeadm-linux-amd64 /usr/bin/kubeadm\n          chmod +x /usr/bin/kubeadm\n        else\n          echo \\\"not found /tmp/kubeadm-linux-amd64\\\"\n          exit 1\n        fi\n    \"\n      check::exit_code \"$?\" \"install\" \"$host: use kubeadm 10 years certs client\"\n    done\n  fi\n}\n\n\nfunction init::upgrade_kernel() {\n  # 升级节点内核\n\n  [[ \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]] && return\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    log::info \"[init]\" \"upgrade kernel: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::upgrade_kernel)\n      script::upgrade_kernel\n    \"\n    check::exit_code \"$?\" \"init\" \"upgrade kernel $host\" \"exit\"\n  done\n  \n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"bash -c 'sleep 15 && reboot' &>/dev/null &\"\n    check::exit_code \"$?\" \"init\" \"$host: Wait for 15s to restart\"\n  done\n\n  log::info \"[notice]\" \"Please execute the command again!\" \n  log::access \"[command]\" \"bash $0 ${SCRIPT_PARAMETER// --upgrade-kernel/}\"\n  exit 0\n}\n\n\nfunction cert::renew_node() {\n # 节点证书续期\n \n  local role=\"${1:-control-plane}\"\n  local hosts=\"\"\n  local kubelet_config=\"\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='node-role.kubernetes.io/${role}' -o jsonpath='{range.items[*]}{.metadata.name } {end}'\n  \"\n  get::command_output \"hosts\" \"$?\"\n  \n  for host in ${hosts}\n  do\n    log::info \"[cert]\" \"drain $host\"\n    command::exec \"${MGMT_NODE}\" \"kubectl drain $host --force --ignore-daemonsets --delete-local-data\"\n    check::exit_code \"$?\" \"cert\" \"$host: drain\" \"exit\"\n    sleep 5\n    \n    if [[ \"${role}\" == \"master\" || \"${role}\" == \"control-plane\" ]]; then\n      command::exec \"${host}\" \"cp -rf /etc/kubernetes /etc/kubernetes_\\$(date +%Y-%m-%d)\"\n      check::exit_code \"$?\" \"cert\" \"$host: backup kubernetes config\" \"exit\"\n      \n      command::exec \"${host}\" \"kubeadm certs renew all 2>/dev/null|| kubeadm alpha certs renew all\"\n      check::exit_code \"$?\" \"cert\" \"$host: renew certs\" \"exit\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof etcd) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:2379 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart etcd\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-apiserver) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:6443 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-apiserver\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-controller-manager) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10257 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n       \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-controller-manager\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-scheduler) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10259 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-scheduler\"\n    fi\n\n    log::info \"[cert]\" \"get kubelet config\"\n    command::exec \"${MGMT_NODE}\" \"\n      kubeadm kubeconfig user --org system:nodes --client-name system:node:${host}  --config /etc/kubernetes/kubeadmcfg.yaml || kubeadm alpha kubeconfig user --org system:nodes --client-name system:node:${host}  --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    get::command_output \"kubelet_config\" \"$?\" \"exit\"\n\n    if [[ \"$kubelet_config\" != \"\" ]]; then\n      log::info \"[cert]\" \"copy kubelet config\"\n      command::exec \"${host}\" \"\n        cp /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf_bak\n        echo '$(printf \"%s\" \"${kubelet_config}\" | sed 's#https://.*:#https://127.0.0.1:#g')' > /etc/kubernetes/kubelet.conf\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: copy kubelet config\" \"exit\"\n\n      command::exec \"${host}\" \"rm -rfv /var/lib/kubelet/pki/*\"\n      check::exit_code \"$?\" \"cert\" \"$host: delete kubelet pki files\" \"exit\"\n\n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        systemctl restart kubelet && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10250 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      local status=\"$?\"\n      check::exit_code \"${status}\" \"cert\" \"$host: restart kubelet\"\n      if [[ \"${status}\" == \"0\" ]]; then\n        sleep 5\n        command::exec \"${MGMT_NODE}\" \"kubectl uncordon ${host}\"\n        check::exit_code \"$?\" \"cert\" \"uncordon ${host} node\" \"exit\"\n      fi\n    fi\n  done\n}\n\n\nfunction cert::renew() {\n  # 证书续期\n \n  log::info \"[cert]\" \"renew cluster cert\"\n  cert::renew_node \"control-plan\"\n  cert::renew_node \"worker\"\n \n  log::info \"[cert]\" \"cluster status\"\n  command::exec \"${MGMT_NODE}\" \"\n    echo\n    kubectl get node\n    echo\n    kubeadm certs check-expiration 2>/dev/null || kubeadm alpha certs check-expiration\n  \" && printf \"%s\" \"${COMMAND_OUTPUT}\"\n}\n\n\nfunction init::node_config() {\n  # 初始化节点配置\n\n  local master_index=${master_index:-1}\n  local worker_index=${worker_index:-1}\n  \n  log::info \"[init]\" \"Get $MGMT_NODE InternalIP.\"\n  command::exec \"${MGMT_NODE}\" \"\n    ip -4 route get 8.8.8.8 2>/dev/null | head -1 | awk '{print \\$7}'\n  \"\n  get::command_output \"MGMT_NODE_IP\" \"$?\" \"exit\"\n\n  # master\n  for host in $MASTER_NODES\n  do\n    log::info \"[init]\" \"master: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0} KUBE_APISERVER=${KUBE_APISERVER} SKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n      $(declare -f script::init_node)\n      script::init_node\n   \"\n    check::exit_code \"$?\" \"init\" \"init master $host\" \"exit\"\n\n    # 设置主机名和解析\n    command::exec \"${host}\" \"\n      printf \\\"\\\\n${MGMT_NODE_IP} $KUBE_APISERVER\\\\n$node_hosts\\\" >> /etc/hosts\n      hostnamectl set-hostname ${HOSTNAME_PREFIX}-master-node${master_index}\n    \"\n    check::exit_code \"$?\" \"init\" \"$host set hostname and hostname resolution\"\n\n    # set audit-policy\n    log::info \"[init]\" \"$host: set audit-policy file.\"\n    command::exec \"${host}\" \"\n      [ ! -d etc/kubernetes ] && mkdir -p /etc/kubernetes\n      cat << EOF > /etc/kubernetes/audit-policy.yaml\n# Log all requests at the Metadata level.\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n- level: Metadata\nEOF\n    \"\n    check::exit_code \"$?\" \"init\" \"$host: set audit-policy file\" \"exit\"\n    master_index=$((master_index + 1))\n  done\n   \n  # worker\n  for host in $WORKER_NODES\n  do\n    log::info \"[init]\" \"worker: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0} KUBE_APISERVER=${KUBE_APISERVER} SKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n      $(declare -f script::init_node)\n      script::init_node\n    \"\n    check::exit_code \"$?\" \"init\" \"init worker $host\" \"exit\"\n\n    # 设置主机名和解析\n    command::exec \"${host}\" \"\n      printf \\\"\\\\n127.0.0.1 $KUBE_APISERVER\\\\n$node_hosts\\\" >> /etc/hosts\n      hostnamectl set-hostname ${HOSTNAME_PREFIX}-worker-node${worker_index}\n    \"\n    worker_index=$((worker_index + 1))\n  done\n}\n\n\nfunction init::node() {\n  # 初始化节点\n  \n  init::upgrade_kernel\n\n  local node_hosts=\"\"\n  local i=1\n  for h in $MASTER_NODES\n  do\n    node_hosts=\"${node_hosts}\\n$h ${HOSTNAME_PREFIX}-master-node${i}\"\n    i=$((i + 1))\n  done\n  \n  local i=1\n  for h in $WORKER_NODES\n  do\n    node_hosts=\"${node_hosts}\\n$h ${HOSTNAME_PREFIX}-worker-node${i}\"\n    i=$((i + 1))\n  done\n\n  init::node_config\n}\n\n\nfunction init::add_node() {\n  # 初始化添加的节点\n  \n  init::upgrade_kernel\n\n  local master_index=0\n  local worker_index=0\n  local node_hosts=\"\"\n  local add_node_hosts=\"\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address } {end}' | awk '{print \\$1}'\n  \"\n  get::command_output \"MGMT_NODE\" \"$?\" \"exit\"\n\n  # 获取现有集群节点主机名\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.metadata.name }\\\\n{end}'\n  \"\n  get::command_output \"node_hosts\" \"$?\" \"exit\"\n  \n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    if [[ $node_hosts == *\"$host\"* ]]; then\n      log::error \"[init]\" \"The host $host is already in the cluster!\"\n      exit 1\n    fi\n  done\n  \n  if [[ \"$MASTER_NODES\" != \"\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].metadata.name}' |grep -Eo 'node[0-9]*'|grep -Eo '[0-9]*'|awk -F ' ' 'BEGIN {max = 0} {if (\\$0+0 > max+0) max=\\$0} END {print max}'\n    \"\n    get::command_output \"master_index\" \"$?\" \"exit\"\n    master_index=$(( master_index + 1 ))\n    local i=$master_index\n    for host in $MASTER_NODES\n    do\n      add_node_hosts=\"${add_node_hosts}\\n${host:-} ${HOSTNAME_PREFIX}-master-node${i}\"\n      i=$((i + 1))\n    done\n  fi\n\n  if [[ \"$WORKER_NODES\" != \"\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].metadata.name}'| grep -Eo 'node[0-9]*'|grep -Eo '[0-9]*'|awk 'BEGIN {max = 0} {if (\\$0+0 > max+0) max=\\$0} END {print max}' || echo 0\n    \"\n    get::command_output \"worker_index\" \"$?\" \"exit\"\n    worker_index=$(( worker_index + 1 ))\n    local i=$worker_index\n    for host in $WORKER_NODES\n    do\n      add_node_hosts=\"${add_node_hosts}\\n${host:-} ${HOSTNAME_PREFIX}-worker-node${i}\"\n      i=$((i + 1))\n    done\n  fi\n  #向集群节点添加新增的节点主机名解析 \n  for host in $(echo -ne \"$node_hosts\" | awk '{print $1}')\n  do\n     command::exec \"${host}\" \"\n       printf \\\"$add_node_hosts\\\" >> /etc/hosts\n     \"\n     check::exit_code \"$?\" \"init\" \"$host add new node hostname resolution\"\n  done\n\n  node_hosts=\"${node_hosts}\\n${add_node_hosts}\"\n  init::node_config\n}\n\n\nfunction kubeadm::init() {\n  # 集群初始化\n  \n  log::info \"[kubeadm init]\" \"kubeadm init on ${MGMT_NODE}\"\n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: set kubeadmcfg.yaml\"\n  command::exec \"${MGMT_NODE}\" \"\n    PAUSE_VERSION=$(kubeadm config images list 2>/dev/null | awk -F: '/pause/ {print $2}')\n    cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: InitConfiguration\n${kubelet_nodeRegistration}\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\nipvs:\n  minSyncPeriod: 5s\n  syncPeriod: 5s\n  # ipvs 负载策略\n  scheduler: 'wrr'\n\n---\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nmaxPods: 200\ncgroupDriver: systemd\nruntimeRequestTimeout: 5m\n# 此配置保证了 kubelet 能在 swap 开启的情况下启动\nfailSwapOn: false\nnodeStatusUpdateFrequency: 5s\nrotateCertificates: true\nimageGCLowThresholdPercent: 70\nimageGCHighThresholdPercent: 80\n# 软驱逐阀值\nevictionSoft:\n  imagefs.available: 15%\n  memory.available: 512Mi\n  nodefs.available: 15%\n  nodefs.inodesFree: 10%\n# 达到软阈值之后，持续时间超过多久才进行驱逐\nevictionSoftGracePeriod:\n  imagefs.available: 3m\n  memory.available: 1m\n  nodefs.available: 3m\n  nodefs.inodesFree: 1m\n# 硬驱逐阀值\nevictionHard:\n  imagefs.available: 10%\n  memory.available: 256Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionMaxPodGracePeriod: 30\n# 节点资源预留\nkubeReserved:\n  cpu: 200m\\$(if [[ \\$(cat /proc/meminfo | awk '/MemTotal/ {print \\$2}') -gt 3670016 ]]; then echo -e '\\n  memory: 256Mi';fi)\n  ephemeral-storage: 1Gi\nsystemReserved:\n  cpu: 300m\\$(if [[ \\$(cat /proc/meminfo | awk '/MemTotal/ {print \\$2}') -gt 3670016 ]]; then echo -e '\\n  memory: 512Mi';fi)\n  ephemeral-storage: 1Gi\nkubeReservedCgroup: /kube.slice\nsystemReservedCgroup: /system.slice\nenforceNodeAllocatable: \n- pods\n\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\nkubernetesVersion: $KUBE_VERSION\ncontrolPlaneEndpoint: $KUBE_APISERVER:6443\nnetworking:\n  dnsDomain: $KUBE_DNSDOMAIN\n  podSubnet: $KUBE_POD_SUBNET\n  serviceSubnet: $KUBE_SERVICE_SUBNET\nimageRepository: $KUBE_IMAGE_REPO\napiServer:\n  certSANs:\n  - 127.0.0.1\n  - $KUBE_APISERVER\n$(for h in $MASTER_NODES;do echo \"  - $h\";done)\n  extraArgs:\n    event-ttl: '720h'\n    service-node-port-range: '30000-50000'\n    # 审计日志相关配置\n    audit-log-maxage: '20'\n    audit-log-maxbackup: '10'\n    audit-log-maxsize: '100'\n    audit-log-path: /var/log/kube-audit/audit.log\n    audit-policy-file: /etc/kubernetes/audit-policy.yaml\n  extraVolumes:\n  - name: audit-config\n    hostPath: /etc/kubernetes/audit-policy.yaml\n    mountPath: /etc/kubernetes/audit-policy.yaml\n    readOnly: true\n    pathType: File\n  - name: audit-log\n    hostPath: /var/log/kube-audit\n    mountPath: /var/log/kube-audit\n    pathType: DirectoryOrCreate\n  - name: localtime\n    hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    readOnly: true\n    pathType: File\ncontrollerManager:\n  extraArgs:\n    bind-address: 0.0.0.0\n    node-cidr-mask-size: '24'\n    node-monitor-grace-period: '20s'\n    terminated-pod-gc-threshold: '30'\n    cluster-signing-duration: 87600h\n    feature-gates: RotateKubeletServerCertificate=true\n  extraVolumes:\n  - hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    name: localtime\n    readOnly: true\n    pathType: File\nscheduler:\n  extraArgs:\n    bind-address: 0.0.0.0\n  extraVolumes:\n  - hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    name: localtime\n    readOnly: true\n    pathType: File\n$(if [[ \"${KUBE_VERSION}\" == \"1.21.1\" ]]; then\necho \"dns:\n  type: CoreDNS\n  imageRepository: docker.io\n  imageTag: 1.8.0\"\nfi)\nEOF\n\"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: set kubeadmcfg.yaml\" \"exit\"\n  \n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: kubeadm init start.\"\n  command::exec \"${MGMT_NODE}\" \"kubeadm init --config=/etc/kubernetes/kubeadmcfg.yaml --upload-certs\"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: kubeadm init\" \"exit\"\n  \n  sleep 3\n  \n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: set kube config.\"\n  command::exec \"${MGMT_NODE}\" \"\n     mkdir -p \\$HOME/.kube\n     sudo cp -f /etc/kubernetes/admin.conf \\$HOME/.kube/config\n  \"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: set kube config\" \"exit\"\n  if [[ \"$(echo \"$MASTER_NODES\" | wc -w)\" == \"1\" ]]; then\n    log::info \"[kubeadm init]\" \"${MGMT_NODE}: delete master taint\"\n    command::exec \"${MGMT_NODE}\" \"kubectl taint nodes --all node-role.kubernetes.io/master- || kubectl taint nodes --all node-role.kubernetes.io/control-plane-\"\n    check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: delete master taint\"\n  fi\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --user=kubelet-bootstrap\n    kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes\n    kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes\n  \"\n  check::exit_code \"$?\" \"kubeadm init\" \"Auto-Approve kubelet cert csr\" \"exit\"\n}\n\n\nfunction kubeadm::join() {\n  # 加入集群\n\n  log::info \"[kubeadm join]\" \"master: get join token and cert info\"\n  command::exec \"${MGMT_NODE}\" \"\n    openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'\n  \"\n  get::command_output \"CACRT_HASH\" \"$?\" \"exit\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm init phase upload-certs --upload-certs --config /etc/kubernetes/kubeadmcfg.yaml 2>> /dev/null | tail -1\n  \"\n  get::command_output \"INTI_CERTKEY\" \"$?\" \"exit\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm token create\n  \"\n  get::command_output \"INIT_TOKEN\" \"$?\" \"exit\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm config images list 2>/dev/null | awk -F: '/pause/ {print \\$2}'\n  \"\n  get::command_output \"PAUSE_VERSION\" \"$?\"\n\n  for host in $MASTER_NODES\n  do\n    [[ \"${MGMT_NODE}\" == \"$host\" ]] && continue\n    log::info \"[kubeadm join]\" \"master $host join cluster.\"\n    command::exec \"${host}\" \"\n      cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta2\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: $KUBE_APISERVER:6443\n    caCertHashes:\n    - sha256:${CACRT_HASH:-}\n    token: ${INIT_TOKEN}\n  timeout: 5m0s\ncontrolPlane:\n  certificateKey: ${INTI_CERTKEY:-}\n${kubelet_nodeRegistration}\nEOF\n      kubeadm join --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"master $host join cluster\"\n\n    log::info \"[kubeadm join]\" \"$host: set kube config.\"\n    command::exec \"${host}\" \"\n      mkdir -p \\$HOME/.kube\n      sudo cp -f /etc/kubernetes/admin.conf \\$HOME/.kube/config\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"$host: set kube config\" \"exit\"\n    \n    command::exec \"${host}\" \"\n      sed -i 's#.*$KUBE_APISERVER#127.0.0.1 $KUBE_APISERVER#g' /etc/hosts\n    \"\n  done\n\n  for host in $WORKER_NODES\n  do\n    log::info \"[kubeadm join]\" \"worker $host join cluster.\"\n    command::exec \"${host}\" \"\n      mkdir -p /etc/kubernetes/manifests\n      cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta2\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: $KUBE_APISERVER:6443\n    caCertHashes:\n    - sha256:${CACRT_HASH:-}\n    token: ${INIT_TOKEN}\n  timeout: 5m0s\n${kubelet_nodeRegistration}\nEOF\n      kubeadm join --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"worker $host join cluster\"\n  \n    log::info \"[kubeadm join]\" \"set $host worker node role.\"\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/master,!node-role.kubernetes.io/control-plane' | grep '<none>' | awk '{print \\\"kubectl label node \\\" \\$1 \\\" node-role.kubernetes.io/worker= --overwrite\\\" }' | bash\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"set $host worker node role\"\n  done\n}\n\n\nfunction kube::wait() {\n  # 等待资源完成\n\n  local app=$1\n  local namespace=$2\n  local resource=$3\n  local selector=${4:-}\n\n  sleep 3\n  log::info \"[waiting]\" \"waiting $app\"\n  command::exec \"${MGMT_NODE}\" \"\n    $(declare -f utils::retry)\n    utils::retry 6 kubectl wait --namespace ${namespace} \\\n    --for=condition=ready ${resource} \\\n    --selector=$selector \\\n    --timeout=60s\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"waiting\" \"$app ${resource} ready\"\n  return \"$status\"\n}\n\n\nfunction kube::apply() {\n  # 应用manifest\n\n  local file=$1\n\n  log::info \"[apply]\" \"$file\"\n  command::exec \"${MGMT_NODE}\" \"\n    $(declare -f utils::retry)\n    if [ -f \\\"$file\\\" ]; then\n      utils::retry 6 kubectl apply --wait=true --timeout=10s -f \\\"$file\\\"\n    else\n      utils::retry 6 \\\"cat <<EOF | kubectl apply --wait=true --timeout=10s -f -\n\\$(printf \\\"%s\\\" \\\"${2:-}\\\")\nEOF\n      \\\"\n    fi\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"apply\" \"add $file\" \"exit\"\n  return \"$status\"\n}\n\n\nfunction kube::status() {\n  # 集群状态\n  \n  sleep 5\n  log::info \"[cluster]\" \"cluster status\"\n  command::exec \"${MGMT_NODE}\" \"\n     echo\n     kubectl get node -o wide\n     echo\n     kubectl get pods -A\n  \" && printf \"%s\" \"${COMMAND_OUTPUT}\"\n}\n\n\nfunction config::haproxy_backend() {\n  # 添加或删除haproxy的后端server\n\n  local action=${1:-add}\n  local action_cmd=\"\"\n  local master_nodes\n  \n  if [[ \"$MASTER_NODES\" == \"\" || \"$MASTER_NODES\" == \"127.0.0.1\" ]]; then\n    return\n  fi\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n  \"\n  get::command_output \"master_nodes\" \"$?\" \"exit\"\n  \n  for m in $MASTER_NODES\n  do\n    if [[ \"${action}\" == \"add\" ]]; then\n      num=$(echo \"${m}\"| awk -F'.' '{print $4}')\n      action_cmd=\"${action_cmd}\\necho \\\"    server apiserver${num} ${m}:6443 check\\\" >> /etc/haproxy/haproxy.cfg\"\n    else\n      [[ \"${master_nodes}\" == *\"${m}\"* ]] || return\n      action_cmd=\"${action_cmd}\\n sed -i -e \\\"/${m}/d\\\" /etc/haproxy/haproxy.cfg\"\n    fi\n  done\n        \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n  \"\n  get::command_output \"worker_nodes\" \"$?\"\n  \n  for host in ${worker_nodes:-}\n  do\n    log::info \"[config]\" \"worker ${host}: ${action} apiserver from haproxy\"\n    command::exec \"${host}\" \"\n      $(echo -ne \"${action_cmd}\")\n      haproxy -c -f /etc/haproxy/haproxy.cfg && systemctl reload haproxy\n    \"\n    check::exit_code \"$?\" \"config\" \"worker ${host}: ${action} apiserver(${m}) from haproxy\"\n  done\n}\n\n\nfunction config::etcd_snapshot() {\n  # 更新 etcd 备份副本\n\n  command::exec \"${MGMT_NODE}\" \"\n    count=\\$(kubectl get node --selector='!node-role.kubernetes.io/worker' --no-headers | wc -l)\n    kubectl -n kube-system patch cronjobs etcd-snapshot --patch \\\"\nspec:\n  jobTemplate:\n    spec:\n      completions: \\${count:-1}\n      parallelism: \\${count:-1}\n\\\"\n  \"\n  check::exit_code \"$?\" \"config\" \"etcd-snapshot completions options\"\n}\n\n\nfunction get::command_output() {\n   # 获取命令的返回值\n\n   local app=\"$1\"\n   local status=\"$2\"\n   local is_exit=\"${3:-}\"\n   \n   if [[ \"$status\" == \"0\" && \"${COMMAND_OUTPUT}\" != \"\" ]]; then\n     log::info \"[command]\" \"get $app value succeeded.\"\n     eval \"$app=\\\"${COMMAND_OUTPUT}\\\"\"\n   else\n     log::error \"[command]\" \"get $app value failed.\"\n     [[ \"$is_exit\" == \"exit\" ]] && exit \"$status\"\n   fi\n   return \"$status\"\n}\n\n\nfunction get::ingress_conn(){\n  # 获取ingress连接地址\n\n  local port=\"${1:-80}\"\n  local ingress_name=\"${2:-ingress-${KUBE_INGRESS}-controller}\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range .items[*]}{ .status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.status.conditions[?(@.status == \\\"True\\\")].status}{\\\"\\\\n\\\"}{end}' | awk '{if(\\$2==\\\"True\\\")a=\\$1}END{print a}'\n  \"\n  get::command_output \"node_ip\" \"$?\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get svc --all-namespaces -o go-template=\\\"{{range .items}}{{if eq .metadata.name \\\\\\\"${ingress_name}\\\\\\\"}}{{range.spec.ports}}{{if eq .port ${port}}}{{.nodePort}}{{end}}{{end}}{{end}}{{end}}\\\"\n  \"\n  \n  get::command_output \"node_port\" \"$?\"\n \n  INGRESS_CONN=\"${node_ip:-nodeIP}:${node_port:-nodePort}\"\n}\n\n\nfunction add::ingress() {\n  # 添加ingress组件\n\n  local add_ingress_demo=0\n\n  if [[ \"$KUBE_INGRESS\" == \"nginx\" ]]; then\n    log::info \"[ingress]\" \"add ingress-nginx\"\n    \n    local ingress_nginx_file=\"${OFFLINE_DIR}/manifests/ingress-nginx.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v${INGRESS_NGINX}/deploy/static/provider/baremetal/deploy.yaml\" \"${ingress_nginx_file}\"\n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#k8s.gcr.io/ingress-nginx#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#registry.k8s.io/ingress-nginx#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#@sha256:.*\\$##g' '${ingress_nginx_file}'\n    \"\n    check::exit_code \"$?\" \"ingress\" \"change ingress-nginx manifests\"\n    kube::apply \"${ingress_nginx_file}\"\n\n    kube::wait \"ingress-nginx\" \"ingress-nginx\" \"pod\" \"app.kubernetes.io/component=controller\" && add_ingress_demo=1\n\n    command::exec \"${MGMT_NODE}\" \"kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission\"\n    check::exit_code \"$?\" \"ingress\" \"delete ingress-ngin ValidatingWebhookConfiguration\"\n\n    log::info \"[ingress]\"  \"set nginx is default ingress class\"\n    command::exec \"${MGMT_NODE}\" \"\n      default_class=\\\"\\$(kubectl get ingressclass -A -o jsonpath='{.items[?(@.metadata.annotations.ingressclass\\\\.kubernetes\\\\.io/is-default-class==\\\\\\\"true\\\\\\\")].metadata.name}')\\\"\n      if [ \\\"\\${default_class:-}\\\" != \\\"\\\" ]; then\n         kubectl patch ingressclass \\${default_class} -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"ingressclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\n      fi\n      kubectl patch ingressclass nginx -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"ingressclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\n    \"\n    check::exit_code \"$?\" \"ingress\" \"set nginx is default ingress class\"\n\n  elif [[ \"$KUBE_INGRESS\" == \"traefik\" ]]; then\n    log::info \"[ingress]\" \"add ingress-traefik\"\n    kube::apply \"traefik\" \"\"\"\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: ingress-traefik-controller\nrules:\n  - apiGroups:\n      - ''\n    resources:\n      - services\n      - endpoints\n      - secrets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n      - networking.k8s.io\n    resources:\n      - ingresses\n      - ingressclasses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: ingress-traefik-controller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: ingress-traefik-controller\nsubjects:\n  - kind: ServiceAccount\n    name: ingress-traefik-controller\n    namespace: default\n--- \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: ingress-traefik-controller\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ingress-traefik-controller\n  labels:\n    app: ingress-traefik-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ingress-traefik-controller\n  template:\n    metadata:\n      labels:\n        app: ingress-traefik-controller\n    spec:\n      serviceAccountName: ingress-traefik-controller\n      containers:\n        - name: traefik\n          image: traefik:v${TRAEFIK_VERSION}\n          args:\n            - --api.debug=true\n            - --api.insecure=true\n            - --log=true\n            - --log.level=debug\n            - --ping=true\n            - --accesslog=true\n            - --entrypoints.http.Address=:80\n            - --entrypoints.https.Address=:443\n            - --entrypoints.traefik.Address=:8080\n            - --providers.kubernetesingress\n            - --serverstransport.insecureskipverify=true\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: admin\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 2\n            httpGet:\n              path: /ping\n              port: admin\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /ping\n              port: admin\n              scheme: HTTP\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          resources:\n            limits:\n              cpu: 250m\n              memory: 128Mi\n            requests:\n              cpu: 100m\n              memory: 64Mi\n          securityContext:\n            capabilities:\n              add:\n              - NET_BIND_SERVICE\n              drop:\n              - ALL\n      restartPolicy: Always\n      serviceAccount: ingress-traefik-controller\n      serviceAccountName: ingress-traefik-controller\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-traefik-controller\nspec:\n  type: NodePort\n  selector:\n    app: ingress-traefik-controller\n  ports:\n    - protocol: TCP\n      port: 80\n      name: http\n      targetPort: 80\n    - protocol: TCP\n      port: 443\n      name: https\n      targetPort: 443\n    - protocol: TCP\n      port: 8080\n      name: admin\n      targetPort: 8080\n\"\"\"\n    kube::wait \"traefik\" \"default\" \"pod\" \"app=ingress-traefik-controller\" && add_ingress_demo=1\n  else\n    log::warning \"[ingress]\" \"No $KUBE_INGRESS config.\"\n  fi\n\n  if [[ \"$add_ingress_demo\" == \"1\" ]]; then\n    log::info \"[ingress]\" \"add ingress default-http-backend\"\n    kube::apply \"default-http-backend\" \"\"\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app.kubernetes.io/name: default-http-backend\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: default-http-backend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: default-http-backend\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: default-http-backend\n        image: ${KUBE_IMAGE_REPO}/defaultbackend-amd64:1.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: default-http-backend\n\"\"\"\n    log::info \"[ingress]\" \"add ingress app demo\"\n    kube::apply \"ingress-demo-app\" \"\"\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-demo-app\n  labels:\n    app: ingress-demo-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ingress-demo-app\n  template:\n    metadata:\n      labels:\n        app: ingress-demo-app\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami:v1.10.1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-demo-app\nspec:\n  type: ClusterIP\n  selector:\n    app: ingress-demo-app\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-demo-app\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: app.demo.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: ingress-demo-app\n            port:\n              number: 80\n\"\"\"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:app.demo.com' http://${INGRESS_CONN}\"\n    fi\n  fi\n}\n\n#compare version numbers\nfunction is::larger_version_than() {\n  local version1=$1;\n  local version2=$2;  \n  return \"$(echo \"$version1\" \"$version2\" | awk '{if ($1 >= $2) print 1; else print 0}')\"\n}\n\nfunction add::network() {\n  # 添加network组件\n\n  if [[ \"$KUBE_NETWORK\" == \"flannel\" ]]; then\n    log::info \"[network]\" \"add flannel\"\n    \n    local flannel_file=\"${OFFLINE_DIR}/manifests/kube-flannel.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/flannel-io/flannel/v${FLANNEL_VERSION}/Documentation/kube-flannel.yml\" \"${flannel_file}\"\n    \n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#10.244.0.0/16#${KUBE_POD_SUBNET}#g' \\\n             -e 's#quay.io/coreos#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#docker.io/flannel#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#namespace: kube-system#namespace: kube-flannel#g' \\\n             -e 's#\\\"Type\\\": \\\"vxlan\\\"#\\\"Type\\\": \\\"${KUBE_FLANNEL_TYPE}\\\"#g' \\\"${flannel_file}\\\"\n      if [[ \\\"${KUBE_FLANNEL_TYPE}\\\" == \\\"vxlan\\\" ]]; then\n        sed -i 's#\\\"Type\\\": \\\"vxlan\\\"#\\\"Type\\\": \\\"vxlan\\\", \\\"DirectRouting\\\": true#g' \\\"${flannel_file}\\\"\n      fi\n    \"\n    check::exit_code \"$?\" \"flannel\" \"change flannel pod subnet\"\n    kube::apply \"${flannel_file}\"\n    kube::wait \"flannel\" \"kube-flannel\" \"pods\" \"app=flannel\"\n\n  elif [[ \"$KUBE_NETWORK\" == \"calico\" ]]; then\n    log::info \"[network]\" \"add calico\"\n    \n    utils::download_file \"https://raw.githubusercontent.com/projectcalico/calico/v${CALICO_VERSION}/manifests/calico.yaml\" \"${OFFLINE_DIR}/manifests/calico.yaml\"\n    utils::download_file \"https://raw.githubusercontent.com/projectcalico/calico/v${CALICO_VERSION}/manifests/calicoctl.yaml\" \"${OFFLINE_DIR}/manifests/calicoctl.yaml\"\n    \n    command::exec \"${MGMT_NODE}\" \"\n      sed -i 's#value: \\\"Always\\\"#value: \\\"CrossSubnet\\\"#g' \\\"${OFFLINE_DIR}/manifests/calico.yaml\\\"\n    \"\n    check::exit_code \"$?\" \"network\" \"change calico version to ${CALICO_VERSION}\"\n    \n    kube::apply \"${OFFLINE_DIR}/manifests/calico.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/calicoctl.yaml\"\n    kube::wait \"calico-kube-controllers\" \"kube-system\" \"pods\" \"k8s-app=calico-kube-controllers\"\n    kube::wait \"calico-node\" \"kube-system\" \"pods\" \"k8s-app=calico-node\"\n\n  elif [[ \"$KUBE_NETWORK\" == \"cilium\" ]]; then \n    log::info \"[network]\" \"add cilium\"\n\n    CILIUM_CLI_VERSION=$(curl -s \"${GITHUB_PROXY}https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt\")\n    CLI_ARCH=amd64\n    if [ \"$(uname -m)\" = \"aarch64\" ]; then CLI_ARCH=arm64; fi\n    utils::download_file \"${GITHUB_PROXY}https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz\"  \"${OFFLINE_DIR}/cilium-linux-${CLI_ARCH}.tar.gz\"\n    tar xzvfC \"${OFFLINE_DIR}/cilium-linux-${CLI_ARCH}.tar.gz\" /usr/local/bin\n    [ -d /etc/bash_completion.d ] && cilium completion bash > /etc/bash_completion.d/cilium\n    cilium install --version \"${CILIUM_VERSION}\" \\\n                   --set ipam.mode=cluster-pool \\\n                   --set ipam.Operator.clusterPoolIPv4PodCIDRList=[\"{KUBE_POD_SUBNET}\"] \\\n                   --set ipam.Operator.clusterPoolIPv4MaskSize=24 \\\n                   --set kubeProxyReplacement=false\n    cilium status --wait\n    cilium hubble enable --ui\n   \n    log::info \"[monitor]\" \"add hubble-ui ingress\"\n    kube::apply \"hubble-ui ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hubble-ui\n  namespace: kube-system\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: hubble-ui.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: hubble-ui\n            port:\n              number: 80\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then                                                                                                                                            \n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:hubble-ui.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[network]\" \"No $KUBE_NETWORK config.\"\n  fi\n}\n\n\nfunction add::addon() {\n  # 添加addon组件\n\n  if [[ \"$KUBE_ADDON\" == \"metrics-server\" ]]; then\n    log::info \"[addon]\" \"download metrics-server manifests\"\n    local metrics_server_file=\"${OFFLINE_DIR}/manifests/metrics-server.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/kubernetes-sigs/metrics-server/releases/download/v${METRICS_SERVER_VERSION}/components.yaml\" \"${metrics_server_file}\"\n  \n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#k8s.gcr.io/metrics-server#$KUBE_IMAGE_REPO#g' \\\n             -e 's#registry.k8s.io/metrics-server#$KUBE_IMAGE_REPO#g' \\\n             -e '/--kubelet-preferred-address-types=.*/d' \\\n             -e 's/\\\\(.*\\\\)- --secure-port=\\\\(.*\\\\)/\\\\1- --secure-port=\\\\2\\\\n\\\\1- --kubelet-insecure-tls\\\\n\\\\1- --kubelet-preferred-address-types=InternalIP,InternalDNS,ExternalIP,ExternalDNS,Hostname/g' \\\n             \\\"${metrics_server_file}\\\"\n    \"\n    check::exit_code \"$?\" \"addon\" \"change metrics-server parameter\"\n    kube::apply \"${metrics_server_file}\"\n    kube::wait \"metrics-server\" \"kube-system\" \"pod\" \"k8s-app=metrics-server\"\n\n  elif [[ \"$KUBE_ADDON\" == \"nodelocaldns\" ]]; then\n    log::info \"[addon]\" \"download nodelocaldns manifests\"\n    local nodelocaldns_file=\"${OFFLINE_DIR}/manifests/nodelocaldns.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml\" \"${nodelocaldns_file}\"\n  \n    command::exec \"${MGMT_NODE}\" \"\n      cluster_dns=\\$(kubectl -n kube-system get svc kube-dns -o jsonpath={.spec.clusterIP})\n      sed -i -e \\\"s#k8s.gcr.io/dns#${KUBE_IMAGE_REPO}#g\\\" \\\n             -e \\\"s#registry.k8s.io/dns#${KUBE_IMAGE_REPO}#g\\\" \\\n             -e \\\"s/__PILLAR__CLUSTER__DNS__/\\$cluster_dns/g\\\" \\\n             -e \\\"s/__PILLAR__UPSTREAM__SERVERS__/\\$cluster_dns/g\\\" \\\n             -e \\\"s/__PILLAR__LOCAL__DNS__/169.254.20.10/g\\\" \\\n             -e \\\"s/[ |,]__PILLAR__DNS__SERVER__//g\\\" \\\n             -e \\\"s/__PILLAR__DNS__DOMAIN__/$KUBE_DNSDOMAIN/g\\\" \\\n             \\\"${nodelocaldns_file}\\\"\n    \"\n    check::exit_code \"$?\" \"addon\" \"change nodelocaldns parameter\"\n    kube::apply \"${nodelocaldns_file}\"\n    kube::wait \"node-local-dns\" \"kube-system\" \"pod\" \"k8s-app=node-local-dns\"\n\n  else\n    log::warning \"[addon]\" \"No $KUBE_ADDON config.\"\n  fi\n}\n\n\nfunction add::monitor() {\n  # 添加监控组件\n  \n  if [[ \"$KUBE_MONITOR\" == \"prometheus\" ]]; then\n    log::info \"[monitor]\" \"add prometheus\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/prometheus-operator/kube-prometheus/archive/v${KUBE_PROMETHEUS_VERSION}.zip\" \"${OFFLINE_DIR}/manifests/prometheus.zip\" \"unzip\"\n   \n    log::info \"[monitor]\" \"apply prometheus manifests\"\n    command::exec \"${MGMT_NODE}\" \"\n      $(declare -f utils::retry)\n      cd \\\"${OFFLINE_DIR}/manifests/kube-prometheus-${KUBE_PROMETHEUS_VERSION}\\\" \\\n      && sed -i -e \\\"s#registry.k8s.io/prometheus-adapter#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#registry.k8s.io/kube-state-metrics#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/brancz#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/prometheus-operator#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/prometheus#${KUBE_IMAGE_REPO}#g\\\" \\\n                   manifests/*.yaml  \\\n      && utils::retry 6 kubectl apply --server-side --wait=true --timeout=10s -f manifests/setup/ \\\n      && until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo ''; done \\\n      && utils::retry 6 kubectl apply --wait=true --timeout=10s -f manifests/\n    \"\n    check::exit_code \"$?\" \"apply\" \"add prometheus\"\n    kube::wait \"prometheus\" \"monitoring\" \"pods --all\"\n\n    kube::apply \"controller-manager and scheduler prometheus discovery service\" \"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: kube-system\n  name: kube-scheduler-prometheus-discovery\n  labels:\n    k8s-app: kube-scheduler\nspec:\n  selector:\n    component: kube-scheduler\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: https-metrics\n    port: 10259\n    targetPort: 10259\n    protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: kube-system\n  name: kube-controller-manager-prometheus-discovery\n  labels:\n    k8s-app: kube-controller-manager\nspec:\n  selector:\n    component: kube-controller-manager\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: https-metrics\n    port: 10257\n    targetPort: 10257\n    protocol: TCP\n    \"\n    \n    log::info \"[monitor]\" \"add prometheus ingress\"\n    kube::apply \"prometheus ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: grafana.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: grafana\n            port:\n              number: 3000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: prometheus.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: prometheus-k8s\n            port:\n              number: 9090\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: alertmanager\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: alertmanager.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: alertmanager-main\n            port:\n              number: 9093\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:grafana.monitoring.cluster.local' http://${INGRESS_CONN}; auth: admin/admin\"\n      log::access \"[ingress]\" \"curl -H 'Host:prometheus.monitoring.cluster.local' http://${INGRESS_CONN}\"\n      log::access \"[ingress]\" \"curl -H 'Host:alertmanager.monitoring.cluster.local' http://${INGRESS_CONN}\"\n    fi\n    \n  else\n    log::warning \"[addon]\" \"No $KUBE_MONITOR config.\"\n  fi\n}\n\n\nfunction add::log() {\n  # 添加log组件\n\n  if [[ \"$KUBE_LOG\" == \"elasticsearch\" ]]; then\n    log::info \"[log]\" \"add elasticsearch\"\n    kube::apply \"elasticsearch\" \"\n---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: kube-logging\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch\n  namespace: kube-logging\n  labels:\n    app: elasticsearch\nspec:\n  selector:\n    app: elasticsearch\n  clusterIP: None\n  ports:\n    - port: 9200\n      name: rest\n    - port: 9300\n      name: inter-node\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-cluster\n  namespace: kube-logging\nspec:\n  serviceName: elasticsearch\n  replicas: 3\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - {key: app,operator: In,values: [\\\"elasticsearch\\\"]}\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: elasticsearch\n        image: ${KUBE_IMAGE_REPO}/elasticsearch:${ELASTICSEARCH_VERSION}\n        resources:\n            limits:\n              cpu: 1000m\n            requests:\n              cpu: 100m\n        ports:\n        - containerPort: 9200\n          name: rest\n          protocol: TCP\n        - containerPort: 9300\n          name: inter-node\n          protocol: TCP\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        env:\n          - name: cluster.name\n            value: k8s-logs\n          - name: node.name\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: discovery.seed_hosts\n            value: 'es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch'\n          - name: cluster.initial_master_nodes\n            value: 'es-cluster-0,es-cluster-1,es-cluster-2'\n          - name: xpack.security.enabled\n            value: 'false'\n          - name: ES_JAVA_OPTS\n            value: '-Xms512m -Xmx512m'\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n      initContainers:\n      - name: increase-vm-max-map\n        image: alpine:3.9\n        command: ['sysctl', '-w', 'vm.max_map_count=262144']\n        securityContext:\n          privileged: true\n      - name: increase-fd-ulimit\n        image: alpine:3.9\n        command: ['sh', '-c', 'ulimit -n 65536']\n        securityContext:\n          privileged: true\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: elasticsearch\n  namespace: kube-logging\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: elasticsearch.logging.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: elasticsearch\n            port:\n              number: 9200\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  labels:\n    app: kibana\nspec:\n  ports:\n  - port: 5601\n  selector:\n    app: kibana\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  labels:\n    app: kibana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kibana\n  template:\n    metadata:\n      labels:\n        app: kibana\n    spec:\n      containers:\n      - name: kibana\n        image: ${KUBE_IMAGE_REPO}/kibana:${ELASTICSEARCH_VERSION}\n        resources:\n          limits:\n            cpu: 1000m\n          requests:\n            cpu: 100m\n        env:\n          - name: ELASTICSEARCH_URL\n            value: http://elasticsearch:9200\n        ports:\n        - containerPort: 5601\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: kibana.logging.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kibana\n            port:\n              number: 5601\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: fluentd\n  namespace: kube-logging\n  labels:\n    app: fluentd\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: fluentd\n  labels:\n    app: fluentd\nrules:\n- apiGroups:\n  - ''\n  resources:\n  - pods\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: fluentd\nroleRef:\n  kind: ClusterRole\n  name: fluentd\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: fluentd\n  namespace: kube-logging\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: kube-logging\n  labels:\n    app: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: fluentd\n  template:\n    metadata:\n      labels:\n        app: fluentd\n    spec:\n      serviceAccount: fluentd\n      serviceAccountName: fluentd\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n      containers:\n      - name: fluentd\n        image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-elasticsearch8-1\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: FLUENT_ELASTICSEARCH_HOST\n          value: elasticsearch.kube-logging.svc.${KUBE_DNSDOMAIN}\n        - name: FLUENT_ELASTICSEARCH_PORT\n          value: '9200'\n        - name: FLUENT_ELASTICSEARCH_SCHEME\n          value: http\n        - name: FLUENTD_SYSTEMD_CONF\n          value: disable\n        - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH\n          value: /var/log/containers/fluent*\n        - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE\n          value: cri\n        - name: FLUENT_CONTAINER_TAIL_PARSER_TIME_FORMAT\n          value: '%Y-%m-%dT%H:%M:%S.%L%z'\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: localtime\n          mountPath: /etc/localtime\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: varlog\n        hostPath:\n          path: /var/log\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n    \" \n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      kube::wait \"elasticsearch\" \"kube-logging\" \"pods --all\"\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:kibana.logging.cluster.local' http://${INGRESS_CONN}\"\n      log::access \"[ingress]\" \"curl -H 'Host:elasticsearch.logging.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[log]\" \"No $KUBE_LOG config.\"\n  fi\n}\n\n\nfunction add::storage() {\n  # 添加存储 \n\n  if [[ \"$KUBE_STORAGE\" == \"rook\" ]]; then\n\n    log::info \"[storage]\" \"add rook\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/rook/rook/archive/v${ROOK_VERSION}.zip\" \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}.zip\" \"unzip\"\n\n    command::exec \"${MGMT_NODE}\" \"\n      cd '${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples' \\\n      && sed -i -e 's/# ROOK_CSI_\\\\(.*\\\\)_IMAGE/ROOK_CSI_\\\\1_IMAGE/g' \\\n                -e 's#\\\\k8s\\\\.gcr\\\\.io/sig-storage#${KUBE_IMAGE_REPO}#g' \\\n                -e 's#\\\\quay\\\\.io/cephcsi#${KUBE_IMAGE_REPO}#g' operator.yaml \\\n      && sed -i 's#quay.io/ceph#${KUBE_IMAGE_REPO}#g' cluster.yaml\n    \" \n    check::exit_code \"$?\" \"storage\" \"image using proxy\"\n\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/crds.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/common.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/operator.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/cluster.yaml\"\n\n  elif [[ \"$KUBE_STORAGE\" == \"longhorn\" ]]; then\n    log::info \"[storage]\" \"add longhorn\"\n    log::info \"[storage]\" \"get cluster node hosts\"\n    if [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl get node -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n      \"\n      get::command_output \"cluster_nodes\" \"$?\" \"exit\"\n    else\n      cluster_nodes=\"${MASTER_NODES} ${WORKER_NODES}\"\n    fi\n    for host in ${cluster_nodes:-}\n    do\n      log::info \"[storage]\"  \"${host}: install iscsi-initiator-utils\"\n      command::exec \"${host}\" \"\n        apt-get install -y open-iscsi\n      \"\n      check::exit_code \"$?\" \"storage\" \"${host}: install iscsi-initiator-utils\" \"exit\"\n    done\n    \n    local longhorn_file=\"${OFFLINE_DIR}/manifests/longhorn.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/longhorn/longhorn/v${LONGHORN_VERSION}/deploy/longhorn.yaml\" \"${longhorn_file}\"\n\n    command::exec \"${MGMT_NODE}\" \"\n      sed -i 's#numberOfReplicas: \\\"3\\\"#numberOfReplicas: \\\"1\\\"#g' \\\"${longhorn_file}\\\"\n    \"\n    check::exit_code \"$?\" \"storage\" \"set longhorn numberOfReplicas is 1\"\n\n    kube::apply \"${longhorn_file}\"\n    kube::wait \"longhorn\" \"longhorn-system\" \"pods --all\"\n    \n    log::info \"[storage]\"  \"set longhorn is default storage class\"\n    command::exec \"${MGMT_NODE}\" \"\n      default_class=\\\"\\$(kubectl get storageclass -A -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\\\.kubernetes\\\\.io/is-default-class==\\\\\\\"true\\\\\\\")].metadata.name}')\\\"\n      if [ \\\"\\${default_class:-}\\\" != \\\"\\\" ]; then\n         kubectl patch storageclass \\${default_class} -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\n      fi\n      kubectl patch storageclass longhorn -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\n    \"\n    check::exit_code \"$?\" \"storage\" \"set longhorn is default storage class\"\n    \n    kube::apply \"longhorn ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: longhorn-ingress\n  namespace: longhorn-system\nspec:\n  rules:\n  - host: longhorn.storage.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: longhorn-frontend\n            port:\n              number: 80\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:longhorn.storage.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[storage]\" \"No $KUBE_STORAGE config.\"\n  fi\n}\n\n\nfunction add::ui() {\n  # 添加用户界面\n\n  if [[ \"$KUBE_UI\" == \"dashboard\" ]]; then\n    log::info \"[ui]\" \"add kubernetes dashboard\"\n    local dashboard_file=\"${OFFLINE_DIR}/manifests/kubernetes-dashboard.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/dashboard/v${KUBERNETES_DASHBOARD_VERSION}/aio/deploy/recommended.yaml\" \"${dashboard_file}\"\n    kube::apply \"${dashboard_file}\"\n    kube::wait \"kubernetes-dashboard\" \"kubernetes-dashboard\" \"pod\" \"k8s-app=kubernetes-dashboard\"\n    kube::apply \"kubernetes dashboard ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\n$( if [[ $KUBE_INGRESS == \"nginx\" ]]; then\necho \"\"\"\n    nginx.ingress.kubernetes.io/secure-backends: 'true'\n    nginx.ingress.kubernetes.io/backend-protocol: 'HTTPS'\n    nginx.ingress.kubernetes.io/ssl-passthrough: 'true'\n\"\"\";\nelif [[ $KUBE_INGRESS == \"traefik\" ]]; then \necho \"\"\"\n    traefik.ingress.kubernetes.io/frontend-entry-points: https\n    traefik.ingress.kubernetes.io/auth-type: 'basic'\n    traefik.ingress.kubernetes.io/auth-secret: 'kubernetes-dashboard-auth'\n    traefik.ingress.kubernetes.io/ssl-redirect: 'true'\n\"\"\";\nfi\n)\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard \nspec:\n$( if [[ $KUBE_INGRESS == \"nginx\" ]]; then\necho \"\"\"\n  tls:\n  - hosts:\n    - kubernetes-dashboard.cluster.local\n    secretName: kubernetes-dashboard-certs\n\"\"\"\nelif [[ $KUBE_INGRESS == \"traefik\" ]]; then \necho \"\"\"\n\"\"\"\nfi\n)\n  rules:\n  - host: kubernetes-dashboard.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn \"443\"\n      log::access \"[ingress]\" \"curl --insecure -H 'Host:kubernetes-dashboard.cluster.local' https://${INGRESS_CONN}\"\n\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl create serviceaccount kubernetes-dashboard-admin-sa -n kubernetes-dashboard\n        kubectl apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubernetes-dashboard-admin-sa-token\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: kubernetes-dashboard-admin-sa\ntype: kubernetes.io/service-account-token\nEOF\n        kubectl create clusterrolebinding kubernetes-dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:kubernetes-dashboard-admin-sa -n kubernetes-dashboard\n      \"\n      local s=\"$?\"\n      check::exit_code \"$s\" \"ui\" \"create kubernetes dashboard admin service account\"\n      local dashboard_token=\"\"\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl describe secrets \\$(kubectl describe sa kubernetes-dashboard-admin-sa -n kubernetes-dashboard | awk '/Tokens/ {print \\$2}') -n kubernetes-dashboard | awk '/token:/{print \\$2}'\n      \"\n      get::command_output \"dashboard_token\" \"$?\"\n      [[ \"$dashboard_token\" != \"\" ]] && log::access \"[Token]\" \"${dashboard_token}\"\n    fi\n  elif [[ \"$KUBE_UI\" == \"kubesphere\" ]]; then\n    log::info \"[ui]\" \"add kubesphere\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubesphere/ks-installer/v${KUBESPHERE_VERSION}/deploy/kubesphere-installer.yaml\" \"${OFFLINE_DIR}/manifests/kubesphere-installer.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubesphere/ks-installer/v${KUBESPHERE_VERSION}/deploy/cluster-configuration.yaml\" \"${OFFLINE_DIR}/manifests/cluster-configuration.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/kubesphere-installer.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/cluster-configuration.yaml\"\n\n    sleep 60\n    kube::wait \"ks-installer\" \"kubesphere-system\" \"pods\" \"app=ks-install\"\n    command::exec \"${MGMT_NODE}\" \"\n      $(declare -f utils::retry) \n      utils::retry 10 kubectl -n kubesphere-system get pods redis-ha-server-0 \\\n        && { kubectl -n kubesphere-system get sts redis-ha-server -o yaml | sed 's#!node-role.kubernetes.io/worker#node-role.kubernetes.io/worker#g' | kubectl replace --force -f -; } \\\n        && echo not replace redis-ha-server\n      utils::retry 10 kubectl -n kubesphere-system get pods openldap-0 \\\n        && { kubectl -n kubesphere-system get sts openldap -o yaml | sed 's#!node-role.kubernetes.io/worker#node-role.kubernetes.io/worker#g' | kubectl replace --force -f -; } \\\n        && echo not replace openldap\n    \"\n    check::exit_code \"$?\" \"ui\" \"set statefulset to worker node\"\n\n    sleep 60\n    kube::wait \"kubesphere-system\" \"kubesphere-system\" \"pods --all\"\n    kube::wait \"kubesphere-controls-system\" \"kubesphere-controls-system\" \"pods --all\" \n    kube::wait \"kubesphere-monitoring-system\" \"kubesphere-monitoring-system\" \"pods --all\" \n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address } {end}' | awk '{print \\$1}'\n      \"\n      get::command_output \"node_ip\" \"$?\"\n      log::access \"[service]\" \"curl http://${node_ip:-NodeIP}:30880;  auth: admin/P@88w0rd\"\n    fi\n  else\n    log::warning \"[ui]\" \"No $KUBE_UI config.\"\n  fi\n}\n\n\nfunction add::ops() {\n  # 运维操作\n   \n  local master_num\n  master_num=$(awk '{print NF}' <<< \"${MASTER_NODES}\")\n  \n  log::info \"[ops]\" \"add anti-affinity strategy to coredns\"\n  command::exec \"${MGMT_NODE}\" \"\"\"\n    kubectl -n kube-system patch deployment coredns --patch '{\\\"spec\\\": {\\\"template\\\": {\\\"spec\\\": {\\\"affinity\\\":{\\\"podAntiAffinity\\\":{\\\"preferredDuringSchedulingIgnoredDuringExecution\\\":[{\\\"weight\\\":100,\\\"podAffinityTerm\\\":{\\\"labelSelector\\\":{\\\"matchExpressions\\\":[{\\\"key\\\":\\\"k8s-app\\\",\\\"operator\\\":\\\"In\\\",\\\"values\\\":[\\\"kube-dns\\\"]}]},\\\"topologyKey\\\":\\\"kubernetes.io/hostname\\\"}}]}}}}}}' --record\n  \"\"\"\n  check::exit_code \"$?\" \"ops\" \"add anti-affinity strategy to coredns\"\n\n  log::info \"[ops]\" \"add etcd snapshot cronjob\"\n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm config images list --config=/etc/kubernetes/kubeadmcfg.yaml 2>/dev/null | grep etcd:\n  \"\n  get::command_output \"etcd_image\" \"$?\"\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' --no-headers | wc -l\n  \"\n  get::command_output \"master_num\" \"$?\"\n\n  [[ \"${master_num:-0}\" == \"0\" ]] && master_num=1\n  kube::apply \"etcd-snapshot\" \"\"\"\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etcd-snapshot\n  namespace: kube-system\nspec:\n  schedule: '0 */6 * * *'\n  successfulJobsHistoryLimit: 3\n  suspend: false\n  concurrencyPolicy: Allow\n  failedJobsHistoryLimit: 3\n  jobTemplate:\n    spec:\n      backoffLimit: 6\n      parallelism: ${master_num}\n      completions: ${master_num}\n      template:\n        metadata:\n          labels:\n            app: etcd-snapshot\n        spec:\n          affinity:\n            podAntiAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n              - labelSelector:\n                  matchExpressions:\n                  - key: app\n                    operator: In\n                    values:\n                    - etcd-snapshot\n                topologyKey: 'kubernetes.io/hostname'\n          containers:\n          - name: etcd-snapshot\n            image: ${etcd_image:-${KUBE_IMAGE_REPO}/etcd:3.4.13-0}\n            imagePullPolicy: IfNotPresent\n            args:\n            - -c\n            - etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt\n              --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\n              snapshot save /backup/etcd-snapshot-\\\\\\\\\\\\\\$(date +%Y-%m-%d_%H:%M:%S_%Z).db\n              && echo 'delete old backups' && { find /backup -type f -mtime +30 -exec rm -fv {} \\\\; || echo error; }\n            command:\n            - /usr/bin/bash\n            env:\n            - name: ETCDCTL_API\n              value: '3'\n            resources: {}\n            terminationMessagePath: /dev/termination-log\n            terminationMessagePolicy: File\n            volumeMounts:\n            - name: etcd-certs\n              mountPath: /etc/kubernetes/pki/etcd\n              readOnly: true\n            - name: backup\n              mountPath: /backup\n            - name: etc\n              mountPath: /etc\n            - name: bin\n              mountPath: /usr/bin\n            - name: lib\n              mountPath: /lib\n            - name: lib64\n              mountPath: /lib64\n          dnsPolicy: ClusterFirst\n          hostNetwork: true\n          nodeSelector:\n            node-role.kubernetes.io/control-plane: ''\n          tolerations:\n          - effect: NoSchedule\n            operator: Exists\n          restartPolicy: OnFailure\n          schedulerName: default-scheduler\n          securityContext: {}\n          terminationGracePeriodSeconds: 30\n          volumes:\n          - name: etcd-certs\n            hostPath:\n              path: /etc/kubernetes/pki/etcd\n              type: DirectoryOrCreate\n          - name: backup\n            hostPath:\n              path: /var/lib/etcd/backups\n              type: DirectoryOrCreate\n          - name: etc\n            hostPath:\n              path: /etc\n          - name: bin\n            hostPath:\n              path: /usr/bin\n          - name: lib\n            hostPath:\n              path: /lib\n          - name: lib64\n            hostPath:\n              path: /lib64\n\"\"\"\n  # shellcheck disable=SC2181\n  [[ \"$?\" == \"0\" ]] && log::access \"[ops]\" \"etcd backup directory: /var/lib/etcd/backups\"\n  command::exec \"${MGMT_NODE}\" \"\n    jobname=\\\"etcd-snapshot-$(date +%s)\\\"\n    kubectl create job --from=cronjob/etcd-snapshot \\${jobname} -n kube-system && \\\n    kubectl wait --for=condition=complete job/\\${jobname} -n kube-system\n  \"\n  check::exit_code \"$?\" \"ops\" \"trigger etcd backup\"\n}\n\n\nfunction reset::node() {\n  # 重置节点\n\n  local host=$1\n  log::info \"[reset]\" \"node $host\"\n  command::exec \"${host}\" \"\n    set +ex\n    cri_socket=\\\"\\\"\n    [ -S /var/run/crio/crio.sock ] && cri_socket=\\\"--cri-socket /var/run/crio/crio.sock\\\"\n    [ -S /run/containerd/containerd.sock ] && cri_socket=\\\"--cri-socket /run/containerd/containerd.sock\\\"\n    kubeadm reset -f \\$cri_socket\n    [ -f \\\"\\$(which kubelet)\\\" ] && { systemctl stop kubelet; find /var/lib/kubelet | xargs -n 1 findmnt -n -o TARGET -T | sort | uniq | xargs -r umount -v; apt remove -y kubeadm kubelet kubectl; }\n    [ -d /etc/kubernetes ] && rm -rf /etc/kubernetes/* /var/lib/kubelet/* /var/lib/etcd/* \\$HOME/.kube /etc/cni/net.d/* /var/lib/dockershim/* /var/lib/cni/* /var/run/kubernetes/*\n\n    [ -f \\\"\\$(which docker)\\\" ] && { docker rm -f -v \\$(docker ps | grep kube | awk '{print \\$1}'); systemctl stop docker; rm -rf \\$HOME/.docker /etc/docker/* /var/lib/docker/*; apt remove -y docker; }\n    [ -f \\\"\\$(which containerd)\\\" ] && { crictl rm \\$(crictl ps -a -q); systemctl stop containerd; rm -rf /etc/containerd/* /var/lib/containerd/*; apt remove -y containerd.io; }\n    [ -f \\\"\\$(which crio)\\\" ] && { crictl rm \\$(crictl ps -a -q); systemctl stop crio; rm -rf /etc/crictl.yaml /etc/crio/* /var/run/crio/*; apt remove -y cri-o; }\n    [ -f \\\"\\$(which runc)\\\" ] && { find /run/containers/ /var/lib/containers/ | xargs -n 1 findmnt -n -o TARGET -T | sort | uniq | xargs -r umount -v; rm -rf /var/lib/containers/* /var/run/containers/*; apt remove -y runc; }\n    [ -f \\\"\\$(which haproxy)\\\" ] && { systemctl stop haproxy; rm -rf /etc/haproxy/* /etc/rsyslog.d/haproxy.conf; yum remove -y haproxy; }\n\n    sed -i -e \\\"/$KUBE_APISERVER/d\\\" -e '/-worker-/d' -e '/-master-/d' /etc/hosts\n    sed -i '/## Kainstall managed start/,/## Kainstall managed end/d' /etc/security/limits.conf /etc/systemd/system.conf /etc/bash.bashrc /etc/audit/rules.d/audit.rules\n    \n    [ -d /var/lib/elasticsearch ] && rm -rf /var/lib/elasticsearch/*\n    [ -d /var/lib/longhorn ] &&  rm -rf /var/lib/longhorn/*\n    [ -d \\\"${OFFLINE_DIR:-/tmp/abc}\\\" ] && rm -rf \\\"${OFFLINE_DIR:-/tmp/abc}\\\"\n\n    ipvsadm --clear\n    iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n    for int in kube-ipvs0 cni0 docker0 dummy0 flannel.1 cilium_host cilium_net cilium_vxlan lxc_health nodelocaldns \n    do\n      [ -d /sys/class/net/\\${int} ] && ip link delete \\${int}\n    done\n    modprobe -r ipip\n    echo done.\n  \"\n  check::exit_code \"$?\" \"reset\" \"$host: reset\"\n}\n\n\nfunction reset::cluster() {\n  # 重置所有节点\n  \n  local all_node=\"\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {end}'\n  \"\n  get::command_output \"all_node\" \"$?\"\n  \n  all_node=$(echo \"${WORKER_NODES} ${MASTER_NODES} ${all_node}\" | awk '{for (i=1;i<=NF;i++) if (!a[$i]++) printf(\"%s%s\",$i,FS)}')\n\n  for host in $all_node\n  do\n    reset::node \"$host\"\n  done\n\n}\n\n\nfunction offline::load() {\n  # 节点加载离线包\n \n  local role=\"${1:-}\"\n  local hosts=\"\"\n  \n  if [[ \"${role}\" == \"master\" ]]; then\n     hosts=\"${MASTER_NODES}\"\n  elif [[ \"${role}\" == \"worker\" ]]; then\n     hosts=\"${WORKER_NODES}\"\n  fi\n \n  for host in ${hosts}\n  do\n    log::info \"[offline]\" \"${role} ${host}: load offline file\"\n    command::exec \"${host}\"  \"[[ ! -d \\\"${OFFLINE_DIR}\\\" ]] && { mkdir -pv \\\"${OFFLINE_DIR}\\\"; chmod 777 \\\"${OFFLINE_DIR}\\\"; } ||:\"\n    check::exit_code \"$?\" \"offline\" \"$host: mkdir offline dir\" \"exit\"\n\n    if [[ \"${UPGRADE_KERNEL_TAG:-}\" == \"1\" ]]; then\n      command::scp \"${host}\" \"${TMP_DIR}/packages/kernel/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp kernel file to $host\" \"exit\"\n    else\n      log::info \"[offline]\" \"${role} ${host}: copy offline file\"\n      command::scp \"${host}\" \"${TMP_DIR}/packages/kubeadm/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp kube file to $host\" \"exit\"\n      command::scp \"${host}\" \"${TMP_DIR}/packages/all/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp all file to $host\" \"exit\"\n\n      if [[ \"${role}\" == \"worker\" ]]; then\n        command::scp \"${host}\" \"${TMP_DIR}/packages/worker/*\" \"${OFFLINE_DIR}\"\n        check::exit_code \"$?\" \"offline\" \"scp worker file to $host\" \"exit\"\n      fi \n\n      command::scp \"${host}\" \"${TMP_DIR}/images/${role}.tgz\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp ${role} images to $host\" \"exit\"\n      command::scp \"${host}\" \"${TMP_DIR}/images/all.tgz\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp all images to $host\" \"exit\"\n    fi\n\n    \n    log::info \"[offline]\" \"${role} ${host}: install package\"\n    command::exec \"${host}\" \"dpkg --force-all -i ${OFFLINE_DIR}/*.deb; DEBIAN_FRONTEND=noninteractive apt-get install -f -q -y\"\n    check::exit_code \"$?\" \"offline\" \"${role} ${host}: install package\" \"exit\"\n  \n    if [[ \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]]; then\n      command::exec \"${host}\" \"\n        set -e\n        for target in firewalld python-firewall firewalld-filesystem iptables; do\n          systemctl stop \\$target &>/dev/null || true\n          systemctl disable \\$target &>/dev/null || true\n        done\n        systemctl start docker && \\\n        cd ${OFFLINE_DIR} && \\\n        gzip -d -c ${1}.tgz | docker load && gzip -d -c all.tgz | docker load\n      \"\n      check::exit_code \"$?\" \"offline\" \"$host: load images\" \"exit\"  \n    fi\n    command::exec \"${host}\" \"rm -rf ${OFFLINE_DIR:-/tmp/abc}\"\n    check::exit_code \"$?\" \"offline\" \"$host: clean offline file\"  \n  done\n\n  command::scp \"${MGMT_NODE}\" \"${TMP_DIR}/manifests\" \"${OFFLINE_DIR}\"\n  check::exit_code \"$?\" \"offline\" \"scp manifests file to ${MGMT_NODE}\" \"exit\"\n\n  command::scp \"${MGMT_NODE}\" \"${TMP_DIR}/bins\" \"${OFFLINE_DIR}\"\n  check::exit_code \"$?\" \"offline\" \"scp bins file to ${MGMT_NODE}\" \"exit\"\n}\n\n\nfunction offline::cluster() {\n  # 集群节点加载离线包\n\n  [ ! -f \"${OFFLINE_FILE}\" ] && { log::error \"[offline]\" \"not found ${OFFLINE_FILE}\" ; exit 1; }\n\n  log::info \"[offline]\" \"Unzip offline package on local.\"\n  tar zxf \"${OFFLINE_FILE}\"  -C \"${TMP_DIR}/\"\n  check::exit_code \"$?\" \"offline\"  \"Unzip offline package\"\n \n  offline::load \"master\"\n  offline::load \"worker\"\n}\n\n\nfunction init::cluster() {\n  # 初始化集群\n\n  MGMT_NODE=$(echo \"${MASTER_NODES}\" | awk '{print $1}')\n\n  # 加载离线包\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && offline::cluster\n  \n  # 1. 初始化节点\n  init::node\n  # 2. 安装包\n  install::package\n  # 3. 初始化kubeadm\n  kubeadm::init\n  # 4. 加入集群\n  kubeadm::join\n  # 5. 添加network\n  add::network\n  # 6. 安装addon\n  add::addon\n  # 7. 添加ingress\n  add::ingress\n  # 8. 添加storage\n  [[ \"${STORAGE_TAG:-}\" == \"1\" ]] && add::storage\n  # 9. 添加web ui\n  add::ui\n  # 10. 添加monitor\n  [[ \"${MONITOR_TAG:-}\" == \"1\" ]] && add::monitor\n  # 11. 添加log\n  [[ \"${LOG_TAG:-}\" == \"1\" ]] && add::log\n  # 12. 运维操作\n  add::ops\n  # 13. 查看集群状态\n  kube::status\n}\n\n\nfunction add::node() {\n  # 添加节点\n  \n  # 加载离线包\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && offline::cluster\n\n  # KUBE_VERSION未指定时，获取集群的版本\n  if [[ \"${KUBE_VERSION}\" == \"\" || \"${KUBE_VERSION}\" == \"latest\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.nodeInfo.kubeletVersion } {end}' | awk -F'v| ' '{print \\$2}'\n  \"\n    get::command_output \"KUBE_VERSION\" \"$?\" \"exit\"\n  fi\n\n  # 1. 初始化节点\n  init::add_node\n  # 2. 安装包\n  install::package\n  # 3. 加入集群\n  kubeadm::join\n  # 4. haproxy添加apiserver\n  config::haproxy_backend \"add\"\n  # 5. 更新 etcd snapshot 副本\n  config::etcd_snapshot\n  # 6. 查看集群状态\n  kube::status\n}\n\n\nfunction del::node() {\n  # 删除节点\n \n  config::haproxy_backend \"remove\"\n\n  local cluster_nodes=\"\"\n  local del_hosts_cmd=\"\"\n  command::exec \"${MGMT_NODE}\" \"\n     kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.metadata.name }\\\\n{end}'\n  \"\n  get::command_output \"cluster_nodes\" \"$?\" exit\n\n  for host in $MASTER_NODES\n  do\n     command::exec \"${MGMT_NODE}\" \"\n       etcd_pod=\\$(kubectl -n kube-system get pods -l component=etcd --field-selector=status.phase=Running -o jsonpath='{\\$.items[0].metadata.name}')\n       etcd_node=\\$(kubectl -n kube-system exec \\$etcd_pod -- sh -c \\\"export ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key ETCDCTL_ENDPOINTS=https://127.0.0.1:2379; etcdctl member list\\\"| grep $host | awk -F, '{print \\$1}')\n       echo \\\"\\$etcd_pod \\$etcd_node\\\"\n       kubectl -n kube-system exec \\$etcd_pod -- sh -c \\\"export ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key ETCDCTL_ENDPOINTS=https://127.0.0.1:2379; etcdctl member remove \\$etcd_node; etcdctl member list\\\"\n     \"\n     check::exit_code \"$?\" \"del\" \"remove $host etcd member\"\n  done\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    log::info \"[del]\" \"node $host\"\n\n    local node_name; node_name=$(echo -ne \"${cluster_nodes}\" | grep \"${host}\" | awk '{print $2}')\n    if [[ \"${node_name}\" == \"\" ]]; then\n      log::warning \"[del]\" \"node $host not found.\"\n      read -r -t 10 -n 1 -p \"Do you need to reset the node (y/n)? \" answer\n      [[ -z \"$answer\" || \"$answer\" != \"y\" ]] && exit || echo\n    else\n      log::info \"[del]\" \"drain $host\"\n      command::exec \"${MGMT_NODE}\" \"kubectl drain $node_name --force --ignore-daemonsets --delete-local-data\"\n      check::exit_code \"$?\" \"del\" \"$host: drain\"\n\n      log::info \"[del]\" \"delete node $host\"\n      command::exec \"${MGMT_NODE}\" \"kubectl delete node $node_name\"\n      check::exit_code \"$?\" \"del\" \"$host: delete\"\n      sleep 3\n    fi\n    reset::node \"$host\"\n    del_hosts_cmd=\"${del_hosts_cmd}\\nsed -i \"/$host/d\" /etc/hosts\"\n  done\n\n  for host in $(echo -ne \"${cluster_nodes}\" | awk '{print $1}')\n  do\n     log::info \"[del]\" \"$host: remove del node hostname resolution\"\n     command::exec \"${host}\" \"\n       $(echo -ne \"${del_hosts_cmd}\")\n     \"\n     check::exit_code \"$?\" \"del\" \"remove del node hostname resolution\"\n  done\n  [ \"$MASTER_NODES\" != \"\" ] && config::etcd_snapshot\n  kube::status\n}\n\n\nfunction upgrade::cluster() {\n  # 升级集群\n\n  log::info \"[upgrade]\" \"upgrade to $KUBE_VERSION\"\n  log::info \"[upgrade]\" \"backup cluster\"\n  add::ops\n\n  local stable_version=\"2\"\n  command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"\n  get::command_output \"stable_version\" \"$?\" && stable_version=\"${stable_version#v}\"\n\n  local node_hosts=\"$MASTER_NODES $WORKER_NODES\"\n  if [[ \"$node_hosts\" == \" \" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node -o jsonpath='{range.items[*]}{.metadata.name } {end}'\n    \"\n    get::command_output \"node_hosts\" \"$?\" exit\n  fi\n\n  local skip_plan=${SKIP_UPGRADE_PLAN,,}\n  for host in ${node_hosts}\n  do\n    log::info \"[upgrade]\" \"node: $host\"\n    local local_version=\"\"\n    command::exec \"${host}\" \"kubectl version --client --output=yaml | awk '/gitVersion:/ {print \\$2}'\"\n    get::command_output \"local_version\" \"$?\" && local_version=\"${local_version#v}\"\n\n    if [[ \"${KUBE_VERSION}\" != \"latest\" ]]; then\n      if [[ \"${KUBE_VERSION}\" == \"${local_version}\" ]];then\n        log::warning \"[check]\" \"The specified version(${KUBE_VERSION}) is consistent with the local version(${local_version})!\"\n        continue\n      fi\n\n      if [[ $(utils::version_to_number \"$KUBE_VERSION\") -lt $(utils::version_to_number \"${local_version}\") ]];then\n        log::warning \"[check]\" \"The specified version($KUBE_VERSION) is less than the local version(${local_version})!\"\n        continue\n      fi\n\n      if [[ $(utils::version_to_number \"$KUBE_VERSION\") -gt $(utils::version_to_number \"${stable_version}\") ]];then\n        log::warning \"[check]\" \"The specified version($KUBE_VERSION) is more than the stable version(${stable_version})!\"\n        continue\n      fi\n    else\n      if [[ $(utils::version_to_number \"${local_version}\") -ge $(utils::version_to_number \"${stable_version}\") ]];then\n        log::warning \"[check]\" \"The local version($local_version) is greater or equal to the stable version(${stable_version})!\"\n        continue\n      fi\n    fi\n\n    command::exec \"${MGMT_NODE}\" \"kubectl drain ${host} --ignore-daemonsets --delete-local-data\"\n    check::exit_code \"$?\" \"upgrade\" \"drain ${host} node\" \"exit\"\n    sleep 5\n\n    if [[ \"${skip_plan}\" == \"false\" ]]; then\n      command::exec \"${host}\" \"$(declare -f script::upgrage_kube); script::upgrage_kube 'init' '$KUBE_VERSION'\"\n      check::exit_code \"$?\" \"upgrade\" \"plan and upgrade cluster on ${host}\" \"exit\"\n      command::exec \"${host}\" \"$(declare -f utils::retry); utils::retry 10 kubectl get node\"\n      check::exit_code \"$?\" \"upgrade\" \"${host}: upgrade\" \"exit\"\n      skip_plan=true\n    else\n      command::exec \"${host}\" \"$(declare -f script::upgrage_kube); script::upgrage_kube 'node' '$KUBE_VERSION'\"\n      check::exit_code \"$?\" \"upgrade\" \"upgrade ${host} node\" \"exit\"\n    fi\n\n    command::exec \"${MGMT_NODE}\" \"kubectl wait --for=condition=Ready node/${host} --timeout=120s\"\n    check::exit_code \"$?\" \"upgrade\" \"${host} ready\"\n    sleep 5\n    command::exec \"${MGMT_NODE}\" \"$(declare -f utils::retry); utils::retry 6 kubectl uncordon ${host}\"\n    check::exit_code \"$?\" \"upgrade\" \"uncordon ${host} node\"\n    sleep 5\n  done\n  \n  kube::status\n}\n\n\nfunction update::self {\n  # 脚本文件更新\n  \n  log::info \"[update]\" \"download kainstall script to $0\"\n  command::exec \"127.0.0.1\" \"\n    wget --timeout=10 --waitretry=3 --tries=5 --retry-connrefused ${GITHUB_PROXY}https://raw.githubusercontent.com/lework/kainstall/master/kainstall-debian.sh -O /tmp/kainstall-debian.sh || exit 1\n    /bin/cp -fv $0 /tmp/$0-bakup\n    /bin/mv -fv /tmp/kainstall-debian.sh \\\"$0\\\"\n    chmod +x \\\"$0\\\"\n  \"\n  check::exit_code \"$?\" \"update\" \"kainstall script\"\n}\n\n\nfunction transform::data {\n  # 数据处理及限制\n\n  MASTER_NODES=$(echo \"${MASTER_NODES}\" | tr ',' ' ')\n  WORKER_NODES=$(echo \"${WORKER_NODES}\" | tr ',' ' ')\n\n  if ! utils::is_element_in_array \"$KUBE_CRI\" docker containerd cri-o ; then\n    log::error \"[limit]\" \"$KUBE_CRI is not supported, only [docker,containerd,cri-o]\"\n    exit 1\n  fi\n\n  [[ \"$KUBE_CRI\" != \"containerd\" && \"${OFFLINE_TAG:-}\" == \"1\" ]] && { log::error \"[limit]\" \"$KUBE_CRI is not supported offline, only containerd\"; exit 1; }\n  [[ \"$KUBE_CRI\" == \"docker\" && \"${KUBE_CRI_ENDPOINT}\" == \"unix:///run/containerd/containerd.sock\" ]] && KUBE_CRI_ENDPOINT=\"/var/run/dockershim.sock\"\n  [[ \"$KUBE_CRI\" == \"cri-o\" && \"${KUBE_CRI_ENDPOINT}\" == \"unix:///run/containerd/containerd.sock\"  ]] && KUBE_CRI_ENDPOINT=\"unix:///var/run/crio/crio.sock\"\n\n  kubelet_nodeRegistration=\"nodeRegistration:\n  criSocket: ${KUBE_CRI_ENDPOINT:-/run/containerd/containerd.sock}\n  kubeletExtraArgs:\n    runtime-cgroups: /system.slice/${KUBE_CRI//-/}.service\n    pod-infra-container-image: ${KUBE_IMAGE_REPO}/pause:${PAUSE_VERSION:-3.7}\n\"\n}\n\n\nfunction help::usage {\n  # 使用帮助\n  \n  cat << EOF\n\nInstall kubernetes cluster using kubeadm.\n\nUsage:\n  $(basename \"$0\") [command]\n\nAvailable Commands:\n  init            Init Kubernetes cluster.\n  reset           Reset Kubernetes cluster.\n  add             Add nodes to the cluster.\n  del             Remove node from the cluster.\n  renew-cert      Renew all available certificates.\n  upgrade         Upgrading kubeadm clusters.\n  update          Update script file.\n\nFlag:\n  -m,--master          master node, default: ''\n  -w,--worker          work node, default: ''\n  -u,--user            ssh user, default: ${SSH_USER}\n  -p,--password        ssh password\n     --private-key     ssh private key\n  -P,--port            ssh port, default: ${SSH_PORT}\n  -v,--version         kube version, default: ${KUBE_VERSION}\n  -n,--network         cluster network, choose: [flannel,calico,cilium], default: ${KUBE_NETWORK}\n  -i,--ingress         ingress controller, choose: [nginx,traefik], default: ${KUBE_INGRESS}\n  -ui,--ui             cluster web ui, choose: [dashboard,kubesphere], default: ${KUBE_UI}\n  -a,--addon           cluster add-ons, choose: [metrics-server,nodelocaldns], default: ${KUBE_ADDON}\n  -M,--monitor         cluster monitor, choose: [prometheus]\n  -l,--log             cluster log, choose: [elasticsearch]\n  -s,--storage         cluster storage, choose: [rook,longhorn]\n     --cri             cri tools, choose: [docker,containerd,cri-o], default: ${KUBE_CRI}\n     --cri-version     cri version, default: ${KUBE_CRI_VERSION}\n     --cri-endpoint    cri endpoint, default: ${KUBE_CRI_ENDPOINT}\n  -U,--upgrade-kernel  upgrade kernel\n  -of,--offline-file   specify the offline package file to load\n      --10years        the certificate period is 10 years.\n      --sudo           sudo mode\n      --sudo-user      sudo user\n      --sudo-password  sudo user password\n\nExample:\n  [init cluster]\n  $0 init \\\\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\\\n  --worker 192.168.77.133,192.168.77.134,192.168.77.135 \\\\\n  --user root \\\\\n  --password 123456 \\\\\n  --version 1.20.4\n\n  [reset cluster]\n  $0 reset \\\\\n  --user root \\\\\n  --password 123456\n\n  [add node]\n  $0 add \\\\\n  --master 192.168.77.140,192.168.77.141 \\\\\n  --worker 192.168.77.143,192.168.77.144 \\\\\n  --user root \\\\\n  --password 123456 \\\\\n  --version 1.20.4\n\n  [del node]\n  $0 del \\\\\n  --master 192.168.77.140,192.168.77.141 \\\\\n  --worker 192.168.77.143,192.168.77.144 \\\\\n  --user root \\\\\n  --password 123456\n \n  [other]\n  $0 renew-cert --user root --password 123456\n  $0 upgrade --version 1.20.4 --user root --password 123456\n  $0 update\n  $0 add --ingress traefik\n  $0 add --monitor prometheus\n  $0 add --log elasticsearch\n  $0 add --storage rook\n  $0 add --ui dashboard\n  $0 add --addon nodelocaldns\n\nEOF\n  exit 1\n}\n\n\n######################################################################################################\n# main\n######################################################################################################\n\n\n[ \"$#\" == \"0\" ] && help::usage\n\nwhile [ \"${1:-}\" != \"\" ]; do\n  case $1 in\n    init  )                 INIT_TAG=1\n                            ;;\n    reset )                 RESET_TAG=1\n                            ;;\n    add )                   ADD_TAG=1\n                            ;;\n    del )                   DEL_TAG=1\n                            ;;\n    renew-cert )            RENEW_CERT_TAG=1\n                            ;;\n    upgrade )               UPGRADE_TAG=1\n                            ;;\n    update )                UPDATE_TAG=1\n                            ;;\n    -m | --master )         shift\n                            MASTER_NODES=${1:-$MASTER_NODES}\n                            ;;\n    -w | --worker )         shift\n                            WORKER_NODES=${1:-$WORKER_NODES}\n                            ;;\n    -u | --user )           shift\n                            SSH_USER=${1:-$SSH_USER}\n                            ;;\n    -p | --password )       shift\n                            SSH_PASSWORD=${1:-$SSH_PASSWORD}\n                            ;;\n    --private-key )         shift\n                            SSH_PRIVATE_KEY=${1:-$SSH_SSH_PRIVATE_KEY}\n                            ;;\n    -P | --port )           shift\n                            SSH_PORT=${1:-$SSH_PORT}\n                            ;;\n    -v | --version )        shift\n                            KUBE_VERSION=${1:-$KUBE_VERSION}\n                            ;;\n    -n | --network )        shift\n                            NETWORK_TAG=1\n                            KUBE_NETWORK=${1:-$KUBE_NETWORK}\n                            ;;\n    -i | --ingress )        shift\n                            INGRESS_TAG=1\n                            KUBE_INGRESS=${1:-$KUBE_INGRESS}\n                            ;;\n    -M | --monitor )        shift\n                            MONITOR_TAG=1\n                            KUBE_MONITOR=${1:-$KUBE_MONITOR}\n                            ;;\n    -l | --log )            shift\n                            LOG_TAG=1\n                            KUBE_LOG=${1:-$KUBE_LOG}\n                            ;;\n    -s | --storage )        shift\n                            STORAGE_TAG=1\n                            KUBE_STORAGE=${1:-$KUBE_STORAGE}\n                            ;;\n    -ui | --ui )            shift\n                            UI_TAG=1\n                            KUBE_UI=${1:-$KUBE_UI}\n                            ;;\n    -a | --addon )          shift\n                            ADDON_TAG=1\n                            KUBE_ADDON=${1:-$KUBE_ADDON}\n                            ;;\n    --cri )                 shift\n                            KUBE_CRI=${1:-$KUBE_CRI}\n                            ;;\n    --cri-version )         shift\n                            KUBE_CRI_VERSION=${1:-$KUBE_CRI_VERSION}\n                            ;;\n    --cri-endpoint )        shift\n                            KUBE_CRI_ENDPOINT=${1:-$KUBE_CRI_ENDPOINT}\n                            ;;\n    -U | --upgrade-kernel ) UPGRADE_KERNEL_TAG=1\n                            ;;\n    -of | --offline-file )  shift\n                            OFFLINE_TAG=1\n                            OFFLINE_FILE=${1:-$OFFLINE_FILE}\n                            ;;\n    --10years )             CERT_YEAR_TAG=1\n                            ;;\n    --sudo )                SUDO_TAG=1\n                            ;;\n    --sudo-user )           shift\n                            SUDO_USER=${1:-$SUDO_USER}\n                            ;;\n    --sudo-password )       shift\n                            SUDO_PASSWORD=${1:-}\n                            ;;\n    * )                     help::usage\n  esac\n  shift\ndone\n\n# 开始\nlog::info \"[start]\" \"bash $0 ${SCRIPT_PARAMETER//${SSH_PASSWORD:-${SUDO_PASSWORD:-}}/zzzzzz}\"  \n\n# 数据处理\ntransform::data\n\n# 预检\ncheck::preflight\n\n# 动作\nif [[ \"${INIT_TAG:-}\" == \"1\" ]]; then\n  [[ \"$MASTER_NODES\" == \"\" ]] && MASTER_NODES=\"127.0.0.1\"\n  init::cluster\nelif [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n  [[ \"${NETWORK_TAG:-}\" == \"1\" ]] && { add::network; add=1; }\n  [[ \"${INGRESS_TAG:-}\" == \"1\" ]] && { add::ingress; add=1; }\n  [[ \"${STORAGE_TAG:-}\" == \"1\" ]] && { add::storage; add=1; }\n  [[ \"${MONITOR_TAG:-}\" == \"1\" ]] && { add::monitor; add=1; }\n  [[ \"${LOG_TAG:-}\" == \"1\" ]] && { add::log; add=1; }\n  [[ \"${UI_TAG:-}\" == \"1\" ]] && { add::ui; add=1; }\n  [[ \"${ADDON_TAG:-}\" == \"1\" ]] && { add::addon; add=1; }\n  [[ \"$MASTER_NODES\" != \"\" || \"$WORKER_NODES\" != \"\" ]] && { add::node; add=1; }\n  [[ \"${add:-}\" != \"1\" ]] && help::usage\nelif [[ \"${DEL_TAG:-}\" == \"1\" ]]; then\n  if [[ \"$MASTER_NODES\" != \"\" || \"$WORKER_NODES\" != \"\" ]]; then del::node; else help::usage; fi\nelif [[ \"${RESET_TAG:-}\" == \"1\" ]]; then\n  reset::cluster\nelif [[ \"${RENEW_CERT_TAG:-}\" == \"1\" ]]; then\n  cert::renew\nelif [[ \"${UPGRADE_TAG:-}\" == \"1\" ]]; then\n  upgrade::cluster\nelif [[ \"${UPDATE_TAG:-}\" == \"1\" ]]; then\n  update::self\nelse\n  help::usage\nfi\n"
        },
        {
          "name": "kainstall-ubuntu.sh",
          "type": "blob",
          "size": 130.1796875,
          "content": "#!/usr/bin/env bash\n###################################################################\n#Script Name    : kainstall-ubuntu.sh\n#Description    : Install kubernetes cluster using kubeadm.\n#Create Date    : 2021-08-01\n#Author         : lework\n#Email          : lework@yeah.net\n###################################################################\n\n\n[[ -n $DEBUG ]] && set -x\nset -o errtrace         # Make sure any error trap is inherited\nset -o nounset          # Disallow expansion of unset variables\nset -o pipefail         # Use last non-zero exit code in a pipeline\n\n\n######################################################################################################\n# environment configuration\n######################################################################################################\n\n# 版本\nKUBE_VERSION=\"${KUBE_VERSION:-latest}\"\nFLANNEL_VERSION=\"${FLANNEL_VERSION:-0.24.0}\"\nMETRICS_SERVER_VERSION=\"${METRICS_SERVER_VERSION:-0.6.4}\"\nINGRESS_NGINX=\"${INGRESS_NGINX:-1.9.5}\"\nTRAEFIK_VERSION=\"${TRAEFIK_VERSION:-2.10.7}\"\nCALICO_VERSION=\"${CALICO_VERSION:-3.27.0}\"\nCILIUM_VERSION=\"${CILIUM_VERSION:-1.14.5}\"\nKUBE_PROMETHEUS_VERSION=\"${KUBE_PROMETHEUS_VERSION:-0.13.0}\"\nELASTICSEARCH_VERSION=\"${ELASTICSEARCH_VERSION:-8.11.3}\"\nROOK_VERSION=\"${ROOK_VERSION:-1.9.13}\"\nLONGHORN_VERSION=\"${LONGHORN_VERSION:-1.5.3}\"\nKUBERNETES_DASHBOARD_VERSION=\"${KUBERNETES_DASHBOARD_VERSION:-2.7.0}\"\nKUBESPHERE_VERSION=\"${KUBESPHERE_VERSION:-3.3.2}\"\n\n# 集群配置\nKUBE_DNSDOMAIN=\"${KUBE_DNSDOMAIN:-cluster.local}\"\nKUBE_APISERVER=\"${KUBE_APISERVER:-apiserver.$KUBE_DNSDOMAIN}\"\nKUBE_POD_SUBNET=\"${KUBE_POD_SUBNET:-10.244.0.0/16}\"\nKUBE_SERVICE_SUBNET=\"${KUBE_SERVICE_SUBNET:-10.96.0.0/16}\"\nKUBE_IMAGE_REPO=\"${KUBE_IMAGE_REPO:-registry.cn-hangzhou.aliyuncs.com/kainstall}\"\nKUBE_NETWORK=\"${KUBE_NETWORK:-flannel}\"\nKUBE_INGRESS=\"${KUBE_INGRESS:-nginx}\"\nKUBE_MONITOR=\"${KUBE_MONITOR:-prometheus}\"\nKUBE_STORAGE=\"${KUBE_STORAGE:-rook}\"\nKUBE_LOG=\"${KUBE_LOG:-elasticsearch}\"\nKUBE_UI=\"${KUBE_UI:-dashboard}\"\nKUBE_ADDON=\"${KUBE_ADDON:-metrics-server}\"\nKUBE_FLANNEL_TYPE=\"${KUBE_FLANNEL_TYPE:-vxlan}\"\nKUBE_CRI=\"${KUBE_CRI:-containerd}\"\nKUBE_CRI_VERSION=\"${KUBE_CRI_VERSION:-latest}\"\nKUBE_CRI_ENDPOINT=\"${KUBE_CRI_ENDPOINT:-unix:///run/containerd/containerd.sock}\"\n\n# 定义的master和worker节点地址，以逗号分隔\nMASTER_NODES=\"${MASTER_NODES:-}\"\nWORKER_NODES=\"${WORKER_NODES:-}\"\n\n# 定义在哪个节点上进行设置\nMGMT_NODE=\"${MGMT_NODE:-127.0.0.1}\"\n\n# 节点的连接信息\nSSH_USER=\"${SSH_USER:-root}\"\nSSH_PASSWORD=\"${SSH_PASSWORD:-}\"\nSSH_PRIVATE_KEY=\"${SSH_PRIVATE_KEY:-}\"\nSSH_PORT=\"${SSH_PORT:-22}\"\nSUDO_USER=\"${SUDO_USER:-root}\"\n\n# 节点设置\nHOSTNAME_PREFIX=\"${HOSTNAME_PREFIX:-k8s}\"\n\n# 脚本设置\nTMP_DIR=\"$(rm -rf /tmp/kainstall* && mktemp -d -t kainstall.XXXXXXXXXX)\"\nLOG_FILE=\"${TMP_DIR}/kainstall.log\"\nSSH_OPTIONS=\"-o ConnectTimeout=600 -o StrictHostKeyChecking=no -o UserKnownHostsFile=/dev/null\"\nERROR_INFO=\"\\n\\033[31mERROR Summary: \\033[0m\\n  \"\nACCESS_INFO=\"\\n\\033[32mACCESS Summary: \\033[0m\\n  \"\nCOMMAND_OUTPUT=\"\"\nSCRIPT_PARAMETER=\"$*\"\nOFFLINE_DIR=\"/tmp/kainstall-offline-file/\"\nOFFLINE_FILE=\"\"\nOS_SUPPORT=\"ubuntu20.04 ubuntu20.10 ubuntu21.04\"\nGITHUB_PROXY=\"${GITHUB_PROXY:-https://mirror.ghproxy.com/}\"\nGCR_PROXY=\"${GCR_PROXY:-k8sgcr.lework.workers.dev}\"\nSKIP_UPGRADE_PLAN=${SKIP_UPGRADE_PLAN:-false}\nSKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n\ntrap trap::info 1 2 3 15 EXIT\n\n######################################################################################################\n# function\n######################################################################################################\n\nfunction trap::info() {\n  # 信号处理\n  \n  [[ ${#ERROR_INFO} -gt 37 ]] && echo -e \"$ERROR_INFO\"\n  [[ ${#ACCESS_INFO} -gt 38 ]] && echo -e \"$ACCESS_INFO\"\n  [ -f \"$LOG_FILE\" ] && echo -e \"\\n\\n  See detailed log >>> $LOG_FILE \\n\\n\"\n  trap '' EXIT\n  exit\n}\n\n\nfunction log::error() {\n  # 错误日志\n  \n  local item; item=\"[$(date +'%Y-%m-%dT%H:%M:%S.%N%z')]: \\033[31mERROR:   \\033[0m$*\"\n  ERROR_INFO=\"${ERROR_INFO}${item}\\n  \"\n  echo -e \"${item}\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::info() {\n  # 基础日志\n  \n  printf \"[%s]: \\033[32mINFO:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::warning() {\n  # 警告日志\n  \n  printf \"[%s]: \\033[33mWARNING: \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::access() {\n  # 访问信息\n  \n  ACCESS_INFO=\"${ACCESS_INFO}$*\\n  \"\n  printf \"[%s]: \\033[32mINFO:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" | tee -a \"$LOG_FILE\"\n}\n\n\nfunction log::exec() {\n  # 执行日志\n  \n  printf \"[%s]: \\033[34mEXEC:    \\033[0m%s\\n\" \"$(date +'%Y-%m-%dT%H:%M:%S.%N%z')\" \"$*\" >> \"$LOG_FILE\"\n}\n\n\nfunction utils::version_to_number() {\n  # 版本号转数字\n\n  echo \"$@\" | awk -F. '{ printf(\"%d%03d%03d%03d\\n\", $1,$2,$3,$4); }';\n}\n\n\nfunction utils::retry {\n  # 重试\n\n  local retries=$1\n  shift\n\n  local count=0\n  until eval \"$*\"; do\n    exit=$?\n    wait=$((2 ** count))\n    count=$((count + 1))\n    if [ \"$count\" -lt \"$retries\" ]; then\n      echo \"Retry $count/$retries exited $exit, retrying in $wait seconds...\"\n      sleep $wait\n    else\n      echo \"Retry $count/$retries exited $exit, no more retries left.\"\n      return $exit\n    fi\n  done\n  return 0\n}\n\n\nfunction utils::quote() {\n  # 转义引号\n\n  # shellcheck disable=SC2046 \n  if [ $(echo \"$*\" | tr -d \"\\n\" | wc -c) -eq 0 ]; then\n    echo \"''\"\n  elif [ $(echo \"$*\" | tr -d \"[a-z][A-Z][0-9]:,.=~_/\\n-\" | wc -c) -gt 0 ]; then\n    printf \"%s\" \"$*\" | sed -e \"1h;2,\\$H;\\$!d;g\" -e \"s/'/\\'\\\"\\'\\\"\\'/g\" | sed -e \"1h;2,\\$H;\\$!d;g\" -e \"s/^/'/g\" -e \"s/$/'/g\"\n  else\n    echo \"$*\"\n  fi\n}\n\n\nfunction utils::download_file() {\n  # 下载文件\n  \n  local url=\"$1\"\n  local dest=\"$2\"\n  local unzip_tag=\"${3:-1}\"\n  \n  local dest_dirname; dest_dirname=$(dirname \"$dest\")\n  local filename; filename=$(basename \"$dest\")\n  \n  log::info \"[download]\" \"${filename}\"\n  command::exec \"${MGMT_NODE}\" \"\n    set -e\n    if [ ! -f \\\"${dest}\\\" ]; then\n      [ ! -d \\\"${dest_dirname}\\\" ] && mkdir -pv \\\"${dest_dirname}\\\" \n      wget --timeout=10 --waitretry=3 --tries=5 --retry-connrefused --no-check-certificate \\\"${url}\\\" -O \\\"${dest}\\\"\n      if [[ \\\"${unzip_tag}\\\" == \\\"unzip\\\" ]]; then\n        command -v unzip 2>/dev/null || apt-get install -y unzip\n        unzip -o \\\"${dest}\\\" -d \\\"${dest_dirname}\\\"\n      fi\n    else\n      echo \\\"${dest} is exists!\\\"\n    fi\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"download\" \"${filename}\" \"exit\"\n  return \"$status\"\n}\n\n\nfunction utils::is_element_in_array() {\n  # 判断是否在数组中存在元素\n\n  local -r element=\"${1}\"\n  local -r array=(\"${@:2}\")\n\n  local walker=''\n\n  for walker in \"${array[@]}\"\n  do\n    [[ \"${walker}\" = \"${element}\" ]] && return 0\n  done\n\n  return 1\n}\n\n\nfunction command::exec() {\n  # 执行命令\n\n  local host=${1:-}\n  shift\n  local command=\"$*\"\n  \n  if [[ \"${SUDO_TAG:-}\" == \"1\" ]]; then\n    sudo_options=\"sudo -H -n -u ${SUDO_USER}\"\n  \n    if [[ \"${SUDO_PASSWORD:-}\" != \"\" ]]; then\n       sudo_options=\"${sudo_options// -n/} -p \\\"\\\" -S <<< \\\"${SUDO_PASSWORD}\\\"\"\n    fi\n    command=\"$sudo_options bash -c $(utils::quote \"$command\")\"\n  fi\n  \n  command=\"$(utils::quote \"$command\")\"\n  \n  if [[ \"${host}\" == \"127.0.0.1\" ]]; then\n    # 本地执行\n    log::exec \"[command]\" \"bash -c $(printf \"%s\" \"${command//${SUDO_PASSWORD:-}/zzzzzz}\")\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval bash -c \"${command}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  else\n    # 远程执行\n    local ssh_cmd=\"ssh\"\n    if [[ \"${SSH_PASSWORD}\" != \"\" ]]; then\n      ssh_cmd=\"sshpass -p \\\"${SSH_PASSWORD}\\\" ${ssh_cmd}\"\n    elif [[ \"$SSH_PRIVATE_KEY\" != \"\" ]]; then\n      [ -f \"${SSH_PRIVATE_KEY}\" ] || { log::error \"[exec]\" \"ssh private_key:${SSH_PRIVATE_KEY} not found.\"; exit 1; }\n      ssh_cmd=\"${ssh_cmd} -i $SSH_PRIVATE_KEY\"\n    fi\n    log::exec \"[command]\" \"${ssh_cmd//${SSH_PASSWORD:-}/zzzzzz} ${SSH_OPTIONS} ${SSH_USER}@${host} -p ${SSH_PORT} bash -c $(printf \"%s\" \"${command//${SUDO_PASSWORD:-}/zzzzzz}\")\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval \"${ssh_cmd} ${SSH_OPTIONS} ${SSH_USER}@${host} -p ${SSH_PORT}\" bash -c '\"${command}\"' 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  fi\n  return $status\n}\n\n\nfunction command::scp() {\n  # 拷贝文件\n\n  local host=${1:-}\n  local src=${2:-}\n  local dest=${3:-/tmp/}\n  \n  if [[ \"${host}\" == \"127.0.0.1\" ]]; then\n    local command=\"cp -rf ${src} ${dest}\"\n    log::exec \"[command]\" \"bash -c \\\"${command}\\\"\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(bash -c \"${command}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  else\n    local scp_cmd=\"scp\"\n    if [[ \"${SSH_PASSWORD}\" != \"\" ]]; then\n      scp_cmd=\"sshpass -p \\\"${SSH_PASSWORD}\\\" ${scp_cmd}\"\n    elif [[ \"$SSH_PRIVATE_KEY\" != \"\" ]]; then\n      [ -f \"${SSH_PRIVATE_KEY}\" ] || { log::error \"[exec]\" \"ssh private_key:${SSH_PRIVATE_KEY} not found.\"; exit 1; }\n      scp_cmd=\"${scp_cmd} -i $SSH_PRIVATE_KEY\"\n    fi\n    log::exec \"[command]\" \"${scp_cmd} ${SSH_OPTIONS} -P ${SSH_PORT} -r ${src} ${SSH_USER}@${host}:${dest}\" >> \"$LOG_FILE\"\n    # shellcheck disable=SC2094\n    COMMAND_OUTPUT=$(eval \"${scp_cmd} ${SSH_OPTIONS} -P ${SSH_PORT} -r ${src} ${SSH_USER}@${host}:${dest}\" 2>> \"$LOG_FILE\" | tee -a \"$LOG_FILE\")\n    local status=$?\n  fi\n  return $status\n}\n\n\nfunction script::init_node() {\n  # 节点初始化脚本\n  \n  # clean\n  sed -i -e \"/$KUBE_APISERVER/d\" -e '/-worker-/d' -e '/-master-/d' /etc/hosts\n  sed -i '/## Kainstall managed start/,/## Kainstall managed end/d' /etc/security/limits.conf /etc/systemd/system.conf ~/.bashrc /etc/audit/rules.d/audit.rules  \n\n  # Disable selinux\n  sed -i '/SELINUX/s/enforcing/disabled/' /etc/selinux/config\n  setenforce 0\n  \n  # Disable swap\n  swapoff -a && sysctl -w vm.swappiness=0\n  sed -ri '/^[^#]*swap/s@^@#@' /etc/fstab\n\n  # Disable firewalld\n  for target in firewalld iptables ufw; do\n    systemctl stop $target &>/dev/null || true\n    systemctl disable $target &>/dev/null || true\n  done\n\n  # repo\n  local codename; codename=\"$(awk -F'=' '/UBUNTU_CODENAME/ {print $2}' /etc/os-release)\"\n  [[ \"${SKIP_SET_OS_REPO,,}\" == \"false\" ]] && cp -fv /etc/apt/sources.list{,.bak}\n  [[ \"${SKIP_SET_OS_REPO,,}\" == \"false\" ]] && cat << EOF > /etc/apt/sources.list\ndeb http://mirrors.aliyun.com/ubuntu/ ${codename} main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ ${codename}-security main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ ${codename}-updates main restricted universe multiverse\ndeb http://mirrors.aliyun.com/ubuntu/ ${codename}-proposed main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ ${codename} main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ ${codename}-security main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ ${codename}-updates main restricted universe multiverse\ndeb-src http://mirrors.aliyun.com/ubuntu/ ${codename}-proposed main restricted universe multiverse\nEOF\n  apt update\n\n  echo -e '#!/bin/sh\\nexit 101' | install -m 755 /dev/stdin /usr/sbin/policy-rc.d\n\n  systemctl mask apt-daily.service apt-daily-upgrade.service\n  systemctl stop apt-daily.timer apt-daily-upgrade.timer\n  systemctl disable apt-daily.timer apt-daily-upgrade.timer\n  systemctl kill --kill-who=all apt-daily.service\n\ncat << EOF > /etc/apt/apt.conf.d/10cloudinit-disable\nAPT::Periodic::Enable \"0\";\n// undo what's in 20auto-upgrade\nAPT::Periodic::Update-Package-Lists \"0\";\nAPT::Periodic::Unattended-Upgrade \"0\";\nEOF\n\n  # Change limits\n  [ ! -f /etc/security/limits.conf_bak ] && cp /etc/security/limits.conf{,_bak}\n  cat << EOF >> /etc/security/limits.conf\n## Kainstall managed start\nroot soft nofile 655360\nroot hard nofile 655360\nroot soft nproc 655360\nroot hard nproc 655360\nroot soft core unlimited\nroot hard core unlimited\n\n* soft nofile 655360\n* hard nofile 655360\n* soft nproc 655360\n* hard nproc 655360\n* soft core unlimited\n* hard core unlimited\n## Kainstall managed end\nEOF\n\n  [ -f /etc/security/limits.d/20-nproc.conf ] && sed -i 's#4096#655360#g' /etc/security/limits.d/20-nproc.conf\n  cat << EOF >> /etc/systemd/system.conf\n## Kainstall managed start\nDefaultLimitCORE=infinity\nDefaultLimitNOFILE=655360\nDefaultLimitNPROC=655360\nDefaultTasksMax=75%\n## Kainstall managed end\nEOF\n\n   # Change sysctl\n   cat << EOF >  /etc/sysctl.d/99-kube.conf\n# https://www.kernel.org/doc/Documentation/sysctl/\n#############################################################################################\n# 调整虚拟内存\n#############################################################################################\n\n# Default: 30\n# 0 - 任何情况下都不使用swap。\n# 1 - 除非内存不足（OOM），否则不使用swap。\nvm.swappiness = 0\n\n# 内存分配策略\n#0 - 表示内核将检查是否有足够的可用内存供应用进程使用；如果有足够的可用内存，内存申请允许；否则，内存申请失败，并把错误返回给应用进程。\n#1 - 表示内核允许分配所有的物理内存，而不管当前的内存状态如何。\n#2 - 表示内核允许分配超过所有物理内存和交换空间总和的内存\nvm.overcommit_memory=1\n\n# OOM时处理\n# 1关闭，等于0时，表示当内存耗尽时，内核会触发OOM killer杀掉最耗内存的进程。\nvm.panic_on_oom=0\n\n# vm.dirty_background_ratio 用于调整内核如何处理必须刷新到磁盘的脏页。\n# Default value is 10.\n# 该值是系统内存总量的百分比，在许多情况下将此值设置为5是合适的。\n# 此设置不应设置为零。\nvm.dirty_background_ratio = 5\n\n# 内核强制同步操作将其刷新到磁盘之前允许的脏页总数\n# 也可以通过更改 vm.dirty_ratio 的值（将其增加到默认值30以上（也占系统内存的百分比））来增加\n# 推荐 vm.dirty_ratio 的值在60到80之间。\nvm.dirty_ratio = 60\n\n# vm.max_map_count 计算当前的内存映射文件数。\n# mmap 限制（vm.max_map_count）的最小值是打开文件的ulimit数量（cat /proc/sys/fs/file-max）。\n# 每128KB系统内存 map_count应该大约为1。 因此，在32GB系统上，max_map_count为262144。\n# Default: 65530\nvm.max_map_count = 2097152\n\n#############################################################################################\n# 调整文件\n#############################################################################################\n\nfs.may_detach_mounts = 1\n\n# 增加文件句柄和inode缓存的大小，并限制核心转储。\nfs.file-max = 2097152\nfs.nr_open = 2097152\nfs.suid_dumpable = 0\n\n# 同时可以拥有的的异步IO请求数目\nfs.aio-max-nr = 10000000\nfs.aio-nr = 75552\n\n# 文件监控\nfs.inotify.max_user_instances=8192\nfs.inotify.max_user_watches=524288\nfs.inotify.max_queued_events=16384\n\n#############################################################################################\n# 调整网络设置\n#############################################################################################\n\n# 为每个套接字的发送和接收缓冲区分配的默认内存量。\nnet.core.wmem_default = 25165824\nnet.core.rmem_default = 25165824\n\n# 为每个套接字的发送和接收缓冲区分配的最大内存量。\nnet.core.wmem_max = 25165824\nnet.core.rmem_max = 25165824\n\n# 除了套接字设置外，发送和接收缓冲区的大小\n# 必须使用net.ipv4.tcp_wmem和net.ipv4.tcp_rmem参数分别设置TCP套接字。\n# 使用三个以空格分隔的整数设置这些整数，分别指定最小，默认和最大大小。\n# 最大大小不能大于使用net.core.wmem_max和net.core.rmem_max为所有套接字指定的值。\n# 合理的设置是最小4KiB，默认64KiB和最大2MiB缓冲区。\nnet.ipv4.tcp_wmem = 20480 12582912 25165824\nnet.ipv4.tcp_rmem = 20480 12582912 25165824\n\n# 增加最大可分配的总缓冲区空间\n# 以页为单位（4096字节）进行度量\nnet.ipv4.tcp_mem = 65536 25165824 262144\nnet.ipv4.udp_mem = 65536 25165824 262144\n\n# 为每个套接字的发送和接收缓冲区分配的最小内存量。\nnet.ipv4.udp_wmem_min = 16384\nnet.ipv4.udp_rmem_min = 16384\n\n# 启用TCP窗口缩放，客户端可以更有效地传输数据，并允许在代理方缓冲该数据。\nnet.ipv4.tcp_window_scaling = 1\n\n# 提高同时接受连接数。\nnet.ipv4.tcp_max_syn_backlog = 10240\n\n# 将net.core.netdev_max_backlog的值增加到大于默认值1000\n# 可以帮助突发网络流量，特别是在使用数千兆位网络连接速度时，\n# 通过允许更多的数据包排队等待内核处理它们。\nnet.core.netdev_max_backlog = 65536\n\n# 增加选项内存缓冲区的最大数量\nnet.core.optmem_max = 25165824\n\n# 被动TCP连接的SYNACK次数。\nnet.ipv4.tcp_synack_retries = 2\n\n# 允许的本地端口范围。\nnet.ipv4.ip_local_port_range = 2048 65535\n\n# 防止TCP时间等待\n# Default: net.ipv4.tcp_rfc1337 = 0\nnet.ipv4.tcp_rfc1337 = 1\n\n# 减少tcp_fin_timeout连接的时间默认值\nnet.ipv4.tcp_fin_timeout = 15\n\n# 积压套接字的最大数量。\n# Default is 128.\nnet.core.somaxconn = 32768\n\n# 打开syncookies以进行SYN洪水攻击保护。\nnet.ipv4.tcp_syncookies = 1\n\n# 避免Smurf攻击\n# 发送伪装的ICMP数据包，目的地址设为某个网络的广播地址，源地址设为要攻击的目的主机，\n# 使所有收到此ICMP数据包的主机都将对目的主机发出一个回应，使被攻击主机在某一段时间内收到成千上万的数据包\nnet.ipv4.icmp_echo_ignore_broadcasts = 1\n\n# 为icmp错误消息打开保护\nnet.ipv4.icmp_ignore_bogus_error_responses = 1\n\n# 启用自动缩放窗口。\n# 如果延迟证明合理，这将允许TCP缓冲区超过其通常的最大值64K。\nnet.ipv4.tcp_window_scaling = 1\n\n# 打开并记录欺骗，源路由和重定向数据包\nnet.ipv4.conf.all.log_martians = 1\nnet.ipv4.conf.default.log_martians = 1\n\n# 告诉内核有多少个未附加的TCP套接字维护用户文件句柄。 万一超过这个数字，\n# 孤立的连接会立即重置，并显示警告。\n# Default: net.ipv4.tcp_max_orphans = 65536\nnet.ipv4.tcp_max_orphans = 65536\n\n# 不要在关闭连接时缓存指标\nnet.ipv4.tcp_no_metrics_save = 1\n\n# 启用RFC1323中定义的时间戳记：\n# Default: net.ipv4.tcp_timestamps = 1\nnet.ipv4.tcp_timestamps = 1\n\n# 启用选择确认。\n# Default: net.ipv4.tcp_sack = 1\nnet.ipv4.tcp_sack = 1\n\n# 增加 tcp-time-wait 存储桶池大小，以防止简单的DOS攻击。\n# net.ipv4.tcp_tw_recycle 已从Linux 4.12中删除。请改用net.ipv4.tcp_tw_reuse。\nnet.ipv4.tcp_max_tw_buckets = 14400\nnet.ipv4.tcp_tw_reuse = 1\n\n# accept_source_route 选项使网络接口接受设置了严格源路由（SSR）或松散源路由（LSR）选项的数据包。\n# 以下设置将丢弃设置了SSR或LSR选项的数据包。\nnet.ipv4.conf.all.accept_source_route = 0\nnet.ipv4.conf.default.accept_source_route = 0\n\n# 打开反向路径过滤\nnet.ipv4.conf.all.rp_filter = 1\nnet.ipv4.conf.default.rp_filter = 1\n\n# 禁用ICMP重定向接受\nnet.ipv4.conf.all.accept_redirects = 0\nnet.ipv4.conf.default.accept_redirects = 0\nnet.ipv4.conf.all.secure_redirects = 0\nnet.ipv4.conf.default.secure_redirects = 0\n\n# 禁止发送所有IPv4 ICMP重定向数据包。\nnet.ipv4.conf.all.send_redirects = 0\nnet.ipv4.conf.default.send_redirects = 0\n\n# 开启IP转发.\nnet.ipv4.ip_forward = 1\n\n# 禁止IPv6\nnet.ipv6.conf.lo.disable_ipv6=1\nnet.ipv6.conf.all.disable_ipv6 = 1\nnet.ipv6.conf.default.disable_ipv6 = 1\n\n# 要求iptables不对bridge的数据进行处理\nnet.bridge.bridge-nf-call-ip6tables = 1\nnet.bridge.bridge-nf-call-iptables = 1\nnet.bridge.bridge-nf-call-arptables = 1\n\n# arp缓存\n# 存在于 ARP 高速缓存中的最少层数，如果少于这个数，垃圾收集器将不会运行。缺省值是 128\nnet.ipv4.neigh.default.gc_thresh1=2048\n# 保存在 ARP 高速缓存中的最多的记录软限制。垃圾收集器在开始收集前，允许记录数超过这个数字 5 秒。缺省值是 512\nnet.ipv4.neigh.default.gc_thresh2=4096\n# 保存在 ARP 高速缓存中的最多记录的硬限制，一旦高速缓存中的数目高于此，垃圾收集器将马上运行。缺省值是 1024\nnet.ipv4.neigh.default.gc_thresh3=8192\n\n# 持久连接\nnet.ipv4.tcp_keepalive_time = 600\nnet.ipv4.tcp_keepalive_intvl = 30\nnet.ipv4.tcp_keepalive_probes = 10\n\n# conntrack表\nnet.nf_conntrack_max=1048576\nnet.netfilter.nf_conntrack_max=1048576\nnet.netfilter.nf_conntrack_buckets=262144\nnet.netfilter.nf_conntrack_tcp_timeout_fin_wait=30\nnet.netfilter.nf_conntrack_tcp_timeout_time_wait=30\nnet.netfilter.nf_conntrack_tcp_timeout_close_wait=15\nnet.netfilter.nf_conntrack_tcp_timeout_established=300\n\n#############################################################################################\n# 调整内核参数\n#############################################################################################\n\n# 地址空间布局随机化（ASLR）是一种用于操作系统的内存保护过程，可防止缓冲区溢出攻击。\n# 这有助于确保与系统上正在运行的进程相关联的内存地址不可预测，\n# 因此，与这些流程相关的缺陷或漏洞将更加难以利用。\n# Accepted values: 0 = 关闭, 1 = 保守随机化, 2 = 完全随机化\nkernel.randomize_va_space = 2\n\n# 调高 PID 数量\nkernel.pid_max = 65536\nkernel.threads-max=30938\n\n# coredump\nkernel.core_pattern=core\n\n# 决定了检测到soft lockup时是否自动panic，缺省值是0\nkernel.softlockup_all_cpu_backtrace=1\nkernel.softlockup_panic=1\nEOF\n\n  # history\n  cat << EOF >> ~/.bashrc\n## Kainstall managed start\n# history actions record，include action time, user, login ip\nHISTFILESIZE=5000\nHISTSIZE=5000\nUSER_IP=\\$(who -u am i 2>/dev/null | awk '{print \\$NF}' | sed -e 's/[()]//g')\nif [ -z \\$USER_IP ]\nthen\n  USER_IP=\\$(hostname -i)\nfi\nHISTTIMEFORMAT=\"%Y-%m-%d %H:%M:%S \\$USER_IP:\\$(whoami) \"\nexport HISTFILESIZE HISTSIZE HISTTIMEFORMAT\n\n# PS1\nPS1='\\[\\033[0m\\]\\[\\033[1;36m\\][\\u\\[\\033[0m\\]@\\[\\033[1;32m\\]\\h\\[\\033[0m\\] \\[\\033[1;31m\\]\\w\\[\\033[0m\\]\\[\\033[1;36m\\]]\\[\\033[33;1m\\]\\\\$ \\[\\033[0m\\]'\n## Kainstall managed end\nEOF\n\n   # journal\n   mkdir -p /var/log/journal /etc/systemd/journald.conf.d\n   cat << EOF > /etc/systemd/journald.conf.d/99-prophet.conf\n[Journal]\n# 持久化保存到磁盘\nStorage=persistent\n# 压缩历史日志\nCompress=yes\nSyncIntervalSec=5m\nRateLimitInterval=30s\nRateLimitBurst=1000\n# 最大占用空间 10G\nSystemMaxUse=10G\n# 单日志文件最大 200M\nSystemMaxFileSize=200M\n# 日志保存时间 3 周\nMaxRetentionSec=3week\n# 不将日志转发到 syslog\nForwardToSyslog=no\nEOF\n\n  # motd\n  chmod -x /etc/update-motd.d/*\n  cat << EOF > /etc/profile.d/zz-ssh-login-info.sh\n#!/bin/sh\n#\n# @Time    : 2020-02-04\n# @Author  : lework\n# @Desc    : ssh login banner\n\nexport PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:\\$PATH\nshopt -q login_shell && : || return 0\necho -e \"\\033[0;32m\n ██╗  ██╗ █████╗ ███████╗\n ██║ ██╔╝██╔══██╗██╔════╝\n █████╔╝ ╚█████╔╝███████╗\n ██╔═██╗ ██╔══██╗╚════██║\n ██║  ██╗╚█████╔╝███████║\n ╚═╝  ╚═╝ ╚════╝ ╚══════ by kainstall\\033[0m\"\n\n# os\nupSeconds=\"\\$(cut -d. -f1 /proc/uptime)\"\nsecs=\\$((\\${upSeconds}%60))\nmins=\\$((\\${upSeconds}/60%60))\nhours=\\$((\\${upSeconds}/3600%24))\ndays=\\$((\\${upSeconds}/86400))\nUPTIME_INFO=\\$(printf \"%d days, %02dh %02dm %02ds\" \"\\$days\" \"\\$hours\" \"\\$mins\" \"\\$secs\")\n\nif [ -f /etc/redhat-release ] ; then\n    PRETTY_NAME=\\$(< /etc/redhat-release)\n\nelif [ -f /etc/debian_version ]; then\n   DIST_VER=\\$(</etc/debian_version)\n   PRETTY_NAME=\"\\$(grep PRETTY_NAME /etc/os-release | sed -e 's/PRETTY_NAME=//g' -e  's/\"//g') (\\$DIST_VER)\"\n\nelse\n    PRETTY_NAME=\\$(cat /etc/*-release | grep \"PRETTY_NAME\" | sed -e 's/PRETTY_NAME=//g' -e 's/\"//g')\nfi\n\nif [[ -d \"/system/app/\" && -d \"/system/priv-app\" ]]; then\n    model=\"\\$(getprop ro.product.brand) \\$(getprop ro.product.model)\"\n\nelif [[ -f /sys/devices/virtual/dmi/id/product_name ||\n        -f /sys/devices/virtual/dmi/id/product_version ]]; then\n    model=\"\\$(< /sys/devices/virtual/dmi/id/product_name)\"\n    model+=\" \\$(< /sys/devices/virtual/dmi/id/product_version)\"\n\nelif [[ -f /sys/firmware/devicetree/base/model ]]; then\n    model=\"\\$(< /sys/firmware/devicetree/base/model)\"\n\nelif [[ -f /tmp/sysinfo/model ]]; then\n    model=\"\\$(< /tmp/sysinfo/model)\"\nfi\n\nMODEL_INFO=\\${model}\nKERNEL=\\$(uname -srmo)\nUSER_NUM=\\$(who -u | wc -l)\nRUNNING=\\$(ps ax | wc -l | tr -d \" \")\n\n# disk\ntotaldisk=\\$(df -h -x devtmpfs -x tmpfs -x debugfs -x aufs -x overlay --total 2>/dev/null | tail -1)\ndisktotal=\\$(awk '{print \\$2}' <<< \"\\${totaldisk}\")\ndiskused=\\$(awk '{print \\$3}' <<< \"\\${totaldisk}\")\ndiskusedper=\\$(awk '{print \\$5}' <<< \"\\${totaldisk}\")\nDISK_INFO=\"\\033[0;33m\\${diskused}\\033[0m of \\033[1;34m\\${disktotal}\\033[0m disk space used (\\033[0;33m\\${diskusedper}\\033[0m)\"\n\n# cpu\ncpu=\\$(awk -F':' '/^model name/ {print \\$2}' /proc/cpuinfo | uniq | sed -e 's/^[ \\t]*//')\ncpun=\\$(grep -c '^processor' /proc/cpuinfo)\ncpuc=\\$(grep '^cpu cores' /proc/cpuinfo | tail -1 | awk '{print \\$4}')\ncpup=\\$(grep '^physical id' /proc/cpuinfo | wc -l)\nCPU_INFO=\"\\${cpu} \\${cpup}P \\${cpuc}C \\${cpun}L\"\n\n# get the load averages\nread one five fifteen rest < /proc/loadavg\nLOADAVG_INFO=\"\\033[0;33m\\${one}\\033[0m / \\${five} / \\${fifteen} with \\033[1;34m\\$(( cpun*cpuc ))\\033[0m core(s) at \\033[1;34m\\$(grep '^cpu MHz' /proc/cpuinfo | tail -1 | awk '{print \\$4}')\\033 MHz\"\n\n# mem\nMEM_INFO=\"\\$(cat /proc/meminfo | awk '/MemTotal:/{total=\\$2/1024/1024;next} /MemAvailable:/{use=total-\\$2/1024/1024; printf(\"\\033[0;33m%.2fGiB\\033[0m of \\033[1;34m%.2fGiB\\033[0m RAM used (\\033[0;33m%.2f%%\\033[0m)\",use,total,(use/total)*100);}')\"\n\n# network\n# extranet_ip=\" and \\$(curl -s ip.cip.cc)\"\nIP_INFO=\"\\$(ip a|grep -E '^[0-9]+: em*|^[0-9]+: eno*|^[0-9]+: enp*|^[0-9]+: ens*|^[0-9]+: eth*|^[0-9]+: wlp*' -A2|grep inet|awk -F ' ' '{print \\$2}'|cut -f1 -d/|xargs echo)\"\n\n# Container info\nCONTAINER_INFO=\"\\$(sudo /usr/bin/crictl ps -a -o yaml 2> /dev/null | awk '/^  state: /{gsub(\"CONTAINER_\", \"\", \\$NF) ++S[\\$NF]}END{for(m in S) printf \"%s%s:%s \",substr(m,1,1),tolower(substr(m,2)),S[m]}')Images:\\$(sudo /usr/bin/crictl images -q 2> /dev/null | wc -l)\"\n\n# info\necho -e \"\n Information as of: \\033[1;34m\\$(date +\"%Y-%m-%d %T\")\\033[0m\n \n \\033[0;1;31mProduct\\033[0m............: \\${MODEL_INFO}\n \\033[0;1;31mOS\\033[0m.................: \\${PRETTY_NAME}\n \\033[0;1;31mKernel\\033[0m.............: \\${KERNEL}\n \\033[0;1;31mCPU\\033[0m................: \\${CPU_INFO}\n\n \\033[0;1;31mHostname\\033[0m...........: \\033[1;34m\\$(hostname)\\033[0m\n \\033[0;1;31mIP Addresses\\033[0m.......: \\033[1;34m\\${IP_INFO}\\033[0m\n\n \\033[0;1;31mUptime\\033[0m.............: \\033[0;33m\\${UPTIME_INFO}\\033[0m\n \\033[0;1;31mMemory\\033[0m.............: \\${MEM_INFO}\n \\033[0;1;31mLoad Averages\\033[0m......: \\${LOADAVG_INFO}\n \\033[0;1;31mDisk Usage\\033[0m.........: \\${DISK_INFO} \n\n \\033[0;1;31mUsers online\\033[0m.......: \\033[1;34m\\${USER_NUM}\\033[0m\n \\033[0;1;31mRunning Processes\\033[0m..: \\033[1;34m\\${RUNNING}\\033[0m\n \\033[0;1;31mContainer Info\\033[0m.....: \\${CONTAINER_INFO}\n\"\nEOF\n\n  chmod +x /etc/profile.d/zz-ssh-login-info.sh\n  echo 'ALL ALL=(ALL) NOPASSWD:/usr/bin/crictl' > /etc/sudoers.d/crictl\n\n  # time sync\n  ntpd --help >/dev/null 2>&1 && apt-get remove -y ntp\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && apt-get install -y chrony \n  [ ! -f /etc/chrony.conf_bak ] && cp /etc/chrony.conf{,_bak} #备份默认配置\n  cat << EOF > /etc/chrony.conf\nserver ntp.aliyun.com iburst\nserver cn.ntp.org.cn iburst\nserver ntp.shu.edu.cn iburst\nserver 0.cn.pool.ntp.org iburst\nserver 1.cn.pool.ntp.org iburst\nserver 2.cn.pool.ntp.org iburst\nserver 3.cn.pool.ntp.org iburst\n\ndriftfile /var/lib/chrony/drift\nmakestep 1.0 3\nlogdir /var/log/chrony\nEOF\n\n  timedatectl set-timezone Asia/Shanghai\n  chronyd -q -t 1 'server cn.pool.ntp.org iburst maxsamples 1'\n  systemctl enable chrony\n  systemctl start chrony\n  chronyc sources -v\n  chronyc sourcestats\n  hwclock --systohc\n\n  # package\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && apt-get install -y apt-transport-https ca-certificates curl wget gnupg lsb-release\n\n  # ipvs\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && apt-get install -y ipvsadm ipset sysstat conntrack libseccomp2\n  module=(\n  ip_vs\n  ip_vs_rr\n  ip_vs_wrr\n  ip_vs_sh\n  overlay\n  nf_conntrack\n  br_netfilter\n  )\n  [ -f /etc/modules-load.d/ipvs.conf ] && cp -f /etc/modules-load.d/ipvs.conf{,_bak}\n  for kernel_module in \"${module[@]}\";do\n     /sbin/modinfo -F filename \"$kernel_module\" |& grep -qv ERROR && echo \"$kernel_module\" >> /etc/modules-load.d/ipvs.conf\n  done\n  systemctl restart systemd-modules-load\n  systemctl enable systemd-modules-load\n  sysctl --system\n\n  # audit\n  [[ \"${OFFLINE_TAG:-}\" != \"1\" ]] && apt-get install -y auditd audispd-plugins\ncat << EOF >> /etc/audit/rules.d/audit.rules\n## Kainstall managed start\n# Ignore errors\n-i\n\n# SYSCALL\n-a always,exit -F arch=b64 -S kill,tkill,tgkill -F a1=9 -F key=trace_kill_9\n-a always,exit -F arch=b64 -S kill,tkill,tgkill -F a1=15 -F key=trace_kill_15\n\n# docker\n-w /usr/bin/dockerd -k docker\n-w /var/lib/docker -k docker\n-w /etc/docker -k docker\n-w /usr/lib/systemd/system/docker.service -k docker\n-w /etc/systemd/system/docker.service -k docker\n-w /usr/lib/systemd/system/docker.socket -k docker\n-w /etc/default/docker -k docker\n-w /etc/sysconfig/docker -k docker\n-w /etc/docker/daemon.json -k docker\n\n# containerd\n-w /usr/bin/containerd -k containerd\n-w /var/lib/containerd -k containerd\n-w /usr/lib/systemd/system/containerd.service -k containerd\n-w /etc/containerd/config.toml -k containerd\n\n# cri-o\n-w /usr/bin/crio -k cri-o\n-w /etc/crio -k cri-o\n\n# runc \n-w /usr/bin/runc -k runc\n\n# kube\n-w /usr/bin/kubeadm -k kubeadm\n-w /usr/bin/kubelet -k kubelet\n-w /usr/bin/kubectl -k kubectl\n-w /var/lib/kubelet -k kubelet\n-w /etc/kubernetes -k kubernetes\n## Kainstall managed end\nEOF\n  chmod 600 /etc/audit/rules.d/audit.rules\n  sed -i 's#max_log_file =.*#max_log_file = 80#g' /etc/audit/auditd.conf \n  if [ -f /usr/libexec/initscripts/legacy-actions/auditd/restart ]; then\n     /usr/libexec/initscripts/legacy-actions/auditd/restart\n  else\n     systemctl stop auditd && systemctl start auditd\n  fi\n  systemctl enable auditd\n\n  grep single-request-reopen /etc/resolv.conf || sed -i '1ioptions timeout:2 attempts:3 rotate single-request-reopen' /etc/resolv.conf\n\n  ipvsadm --clear\n  iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n}\n\n\nfunction script::upgrade_kernel() {\n  # 升级内核\n\n  local codename; codename=\"$(awk -F'=' '/UBUNTU_CODENAME/ {print $2}' /etc/os-release)\"\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]]; then\n    echo \"deb [trusted=yes] http://mirrors.aliyun.com/ubuntu ${codename}-backports main\" > /etc/apt/sources.list.d/backports.list\n    apt update\n    apt -t \"${codename}-backports\" install linux-headers-generic linux-image-generic -y\n  fi \n}\n\n\nfunction script::upgrage_kube() {\n  # 节点软件升级\n\n  local role=${1:-init}\n  local version=\"=${2:-latest}-00\"\n  version=\"${version#=latest-00}\"\n\n  set -e\n  echo '[install] kubeadm'\n  kubeadm version\n  apt-get update\n  apt-get install -y \"kubeadm${version}\"\n  kubeadm version\n\n  echo '[upgrade]'\n  if [[ \"$role\" == \"init\" ]]; then\n    local plan_info; plan_info=$(kubeadm upgrade plan)\n    local v; v=$(printf \"%s\" \"$plan_info\" | grep 'kubeadm upgrade apply ' | awk '{print $4}'| tail -1 )\n    printf \"%s\\n\" \"${plan_info}\"\n    kubeadm upgrade apply \"${v}\" -y\n  else\n    kubeadm upgrade node\n  fi\n\n  echo '[install] kubelet kubectl'\n  kubectl version --client=true\n  apt-get install -y \"kubelet${version}\" \"kubectl${version}\"\n  kubectl version --client=true\n\n  [ -f /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf ] && \\\n    sed -i 's#^\\[Service\\]#[Service]\\nCPUAccounting=true\\nMemoryAccounting=true#g' /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf\n\n  systemctl daemon-reload\n  systemctl restart kubelet\n}\n\n\nfunction script::install_docker() {\n  # 安装 docker\n  \n  local version=\"=${1:-latest}-00\"\n  version=\"${version#=latest-00}\"\n\n  wget -qO - http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n  echo \"deb [trusted=yes] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" > /etc/apt/sources.list.d/docker-ce.list\n  apt-get update\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which docker)\" ]  && apt remove -y docker-ce docker-ce-cli containerd.io\n    apt-get install -y \"docker-ce${version}\" \"docker-ce-cli${version}\" containerd.io bash-completion\n  fi\n\n  apt-mark hold docker-ce docker-ce-cli containerd.io\n\n  [ -f /usr/share/bash-completion/completions/docker ] && \\\n    cp -f /usr/share/bash-completion/completions/docker /etc/bash_completion.d/\n\n  [ ! -d /etc/docker ] && mkdir /etc/docker\n  cat << EOF > /etc/docker/daemon.json\n{\n  \"data-root\": \"/var/lib/docker\",\n  \"log-driver\": \"json-file\",\n  \"log-opts\": {\n    \"max-size\": \"200m\",\n    \"max-file\": \"5\"\n  },\n  \"default-ulimits\": {\n    \"nofile\": {\n      \"Name\": \"nofile\",\n      \"Hard\": 655360,\n      \"Soft\": 655360\n    },\n    \"nproc\": {\n      \"Name\": \"nproc\",\n      \"Hard\": 655360,\n      \"Soft\": 655360\n    }\n  },\n  \"live-restore\": true,\n  \"oom-score-adjust\": -1000,\n  \"max-concurrent-downloads\": 10,\n  \"max-concurrent-uploads\": 10,\n  \"registry-mirrors\": [\n    \"https://yssx4sxy.mirror.aliyuncs.com/\"\n  ]\n}\nEOF\n  sed -i 's|#oom_score = 0|oom_score = -999|' /etc/containerd/config.toml\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///var/run/dockershim.sock\nimage-endpoint: unix:///var/run/dockershim.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n  \n  systemctl enable containerd\n  systemctl restart containerd\n\n  systemctl enable docker\n  systemctl restart docker\n}\n\n\nfunction script::install_containerd() {\n  # 安装 containerd\n  \n  local version=\"=${1:-latest}-00\"\n  version=\"${version#=latest-00}\"\n\n  wget -qO - http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add -\n  echo \"deb [trusted=yes] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs) stable\" > /etc/apt/sources.list.d/docker-ce.list\n  apt-get update\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which runc)\" ]  && apt remove -y runc\n    [ -f \"$(which containerd)\" ]  && apt remove -y containerd.io\n    apt-get install -y containerd.io\"${version}\" bash-completion\n  fi\n\n  [ -d /etc/bash_completion.d ] && crictl completion bash > /etc/bash_completion.d/crictl\n\n  containerd config default > /etc/containerd/config.toml\n  sed -i -e \"s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#registry.k8s.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#https://registry-1.docker.io#https://yssx4sxy.mirror.aliyuncs.com#g\" \\\n         -e \"s#SystemdCgroup = false#SystemdCgroup = true#g\" \\\n         -e \"s#oom_score = 0#oom_score = -999#\" \\\n         -e \"s#max_container_log_line_size = 16384#max_container_log_line_size = 65535#\" \\\n         -e \"s#max_concurrent_downloads = 3#max_concurrent_downloads = 10#g\" /etc/containerd/config.toml\n\n  grep docker.io /etc/containerd/config.toml ||  sed -i -e \"/registry.mirrors]/a\\ \\ \\ \\ \\ \\ \\ \\ [plugins.\\\"io.containerd.grpc.v1.cri\\\".registry.mirrors.\\\"docker.io\\\"]\\n           endpoint = [\\\"https://yssx4sxy.mirror.aliyuncs.com\\\"]\" \\\n       /etc/containerd/config.toml\n\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///run/containerd/containerd.sock\nimage-endpoint: unix:///run/containerd/containerd.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n  \n  systemctl restart containerd\n  systemctl enable containerd\n}\n\nfunction script::install_cri-o() {\n  # 安装 cri-o\n  \n  local version=\"${1:-latest}\"\n  version=\"${version##latest}\"\n  os=\"xUbuntu_$(lsb_release -rs)\"\n\n  echo \"deb [trusted=yes] http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$os/ /\" > /etc/apt/sources.list.d/devel:kubic:libcontainers:stable.list\n  echo \"deb [trusted=yes] http://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable:/cri-o:/$version/${os}/ /\" > \"/etc/apt/sources.list.d/devel:kubic:libcontainers:stable:cri-o:$version.list\"\n\n  wget -qO - \"https://download.opensuse.org/repositories/devel:kubic:libcontainers:stable:cri-o:$version/${os}/Release.key\" | apt-key add -\n  wget -qO - \"https://download.opensuse.org/repositories/devel:/kubic:/libcontainers:/stable/$os/Release.key\" | apt-key add -\n\n  apt-get update\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which runc)\" ]  && apt remove -y runc\n    [ -f \"$(which crio)\" ]  && apt remove -y cri-o\n    [ -f \"$(which docker)\" ]  && apt remove -y docker-ce docker-ce-cli containerd.io\n    apt-get install -y cri-o runc bash-completion\n  fi\n\n  [ -d /etc/bash_completion.d ] && \\\n    { crictl completion bash >  /etc/bash_completion.d/crictl; \\\n      crio completion bash > /etc/bash_completion.d/crio; \\\n      crio-status completion bash > /etc/bash_completion.d/crio-status; }\n\n  crio config --default > /etc/crio/crio.conf\n  sed -i -e \"s#k8s.gcr.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e \"s#registry.k8s.io#registry.cn-hangzhou.aliyuncs.com/kainstall#g\" \\\n         -e 's|#registries = \\[|registries = [\"docker.io\", \"quay.io\"]|g' /etc/crio/crio.conf\n\n  [ -d /etc/containers/registries.conf.d ] && cat << EOF > /etc/containers/registries.conf.d/000-dockerio.conf\n[[registry]]\nprefix = \"docker.io\"\ninsecure = false\nblocked = false\nlocation = \"docker.io\"\n\n[[registry.mirror]]\nlocation = \"yssx4sxy.mirror.aliyuncs.com\"\ninsecure = true\nEOF\n\n  cat << EOF > /etc/crictl.yaml\nruntime-endpoint: unix:///var/run/crio/crio.sock\nimage-endpoint: unix:///var/run/crio/crio.sock\ntimeout: 2\ndebug: false\npull-image-on-create: true\ndisable-pull-on-run: false\nEOF\n\n  sed -i \"s#10.85.0.0/16#${KUBE_POD_SUBNET:-10.85.0.0/16}#g\" /etc/cni/net.d/100-crio-bridge.conf\n  cat << EOF > /etc/cni/net.d/10-crio.conf\n{\n$(grep cniVersion /etc/cni/net.d/100-crio-bridge.conf)\n    \"name\": \"crio\",\n    \"type\": \"flannel\"\n}\nEOF\n  mv /etc/cni/net.d/100-crio-bridge.conf /etc/cni/net.d/10-crio.conf /etc/cni/net.d/200-loopback.conf /tmp/\n  [ ! -d /usr/lib/cri-o-runc/sbin/ ] && mkdir -p /usr/lib/cri-o-runc/sbin/\n  [ ! -f /usr/sbin/runc ] && ln -sv \"$(which runc)\" /usr/sbin/runc\n  [ ! -f /usr/lib/cri-o-runc/sbin/runc ] && ln -sv \"$(which runc)\" /usr/lib/cri-o-runc/sbin/runc\n  systemctl restart crio\n  systemctl enable crio\n}\n\n\nfunction script::install_kube() {\n  # 安装kube组件\n  \n  local version=\"=${1:-latest}-00\"\n  version=\"${version#=latest-00}\"\n\n  repo=\"${version%.*}\"\n  repo=\"${repo//-}\"\n  [ \"${repo}\" == \"\" ] && repo=\"1.29\"\n\n  echo \"deb [trusted=yes] https://pkgs.k8s.io/core:/stable:/v${repo}/deb/ /\" > /etc/apt/sources.list.d/kubernetes.list\n  apt-get update\n\n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which kubeadm)\" ]  && apt remove -y kubeadm\n    [ -f \"$(which kubelet)\" ]  && apt remove -y kubelet\n    [ -f \"$(which kubectl)\" ]  && apt remove -y kubectl\n    apt-get install -y \"kubeadm${version}\" \"kubelet${version}\" \"kubectl${version}\" kubernetes-cni\n  fi\n\n  [ -d /etc/bash_completion.d ] && \\\n    { kubectl completion bash > /etc/bash_completion.d/kubectl; \\\n      kubeadm completion bash > /etc/bash_completion.d/kubadm; }\n\n  [ ! -d /usr/lib/systemd/system/kubelet.service.d ] && mkdir -p /usr/lib/systemd/system/kubelet.service.d\n  cat << EOF > /usr/lib/systemd/system/kubelet.service.d/11-cgroup.conf\n[Service]\nCPUAccounting=true\nMemoryAccounting=true\nBlockIOAccounting=true\nExecStartPre=/bin/bash -c '/bin/mkdir -p /sys/fs/cgroup/{cpuset,memory,hugetlb,systemd,pids,\"cpu,cpuacct\"}/{system,kube,kubepods}.slice||:'\nSlice=kube.slice\nEOF\n  systemctl daemon-reload\n\n  systemctl enable kubelet\n  systemctl restart kubelet\n}\n\n\nfunction script::install_haproxy() {\n  # 安装haproxy\n   \n  local api_servers=\"$*\"\n   \n  if [[ \"${OFFLINE_TAG:-}\" != \"1\" ]];then\n    [ -f \"$(which haproxy)\" ] && apt remove -y haproxy\n    apt-get install -y haproxy rsyslog\n  fi\n\n  [ ! -f /etc/haproxy/haproxy.cfg_bak ] && cp /etc/haproxy/haproxy.cfg{,_bak}\ncat << EOF > /etc/haproxy/haproxy.cfg\nglobal\n  log /dev/log    local0\n  log /dev/log    local1 notice\n  tune.ssl.default-dh-param 2048\n\ndefaults\n  log global\n  mode http\n  option dontlognull\n  timeout connect 5000ms\n  timeout client 600000ms\n  timeout server 600000ms\n\nlisten stats\n    bind :19090\n    mode http\n    balance\n    stats uri /haproxy_stats\n    stats auth admin:admin123\n    stats admin if TRUE\n\nfrontend kube-apiserver-https\n   mode tcp\n   option tcplog\n   bind :6443\n   default_backend kube-apiserver-backend\n\nbackend kube-apiserver-backend\n    mode tcp\n    balance roundrobin\n    stick-table type ip size 200k expire 30m\n    stick on src\n$(index=1;for h in $api_servers;do echo \"    server apiserver${index} $h:6443 check\";index=$((index+1));done)\nEOF\n\ncat <<EOF > /etc/rsyslog.d/haproxy.conf\nlocal0.* /var/log/haproxy.log\nlocal1.* /var/log/haproxy.log\nEOF\n\n  systemctl enable rsyslog\n  systemctl restart rsyslog\n  systemctl enable haproxy\n  systemctl restart haproxy\n}\n\n\nfunction check::command_exists() {\n  # 检查命令是否存在\n  \n  local cmd=${1}\n  local package=${2}\n\n  if command -V \"$cmd\" > /dev/null 2>&1; then\n    log::info \"[check]\" \"$cmd command exists.\"\n  else\n    log::warning \"[check]\" \"I require $cmd but it's not installed.\"\n    log::warning \"[check]\" \"install $package package.\"\n    command::exec \"127.0.0.1\" \"apt-get install -y ${package}\"\n    check::exit_code \"$?\" \"check\" \"$package install\" \"exit\"\n  fi\n}\n\n\nfunction check::command() {\n  # 检查用到的命令\n  \n  check::command_exists ssh openssh-clients\n  check::command_exists sshpass sshpass\n  check::command_exists wget wget\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && check::command_exists tar tar\n}\n\n\nfunction check::ssh_conn() {\n  # 检查ssh连通性\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    [ \"$host\" == \"127.0.0.1\" ] && continue\n    command::exec \"${host}\" \"echo 0\"\n    check::exit_code \"$?\" \"check\" \"ssh $host connection\" \"exit\"\n  done\n}\n\n\nfunction check::os() {\n  # 检查os系统支持\n\n  log::info \"[check]\" \"os support: ${OS_SUPPORT}\"\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"\n      [ -f /etc/os-release ] && source /etc/os-release\n      echo client_os:\\${ID:-}\\${VERSION_ID:-}\n      if [[ \\\"${OS_SUPPORT}\\\" == *\\\"\\${ID:-}\\${VERSION_ID:-}\\\"* ]]; then\n        exit 0\n      fi\n      exit 1\n    \"\n    check::exit_code \"$?\" \"check\" \"$host os support\" \"exit\"\n  done\n}\n\n\nfunction check::kernel() {\n  # 检查os kernel 版本\n\n  local version=${1:-}\n  log::info \"[check]\" \"kernel version not less than ${version}\"\n  version=$(echo \"${version}\" | awk -F. '{ printf(\"%d%03d%03d\\n\", $1,$2,$3); }')\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"\n      kernel_version=\\$(uname -r)\n      kernel_version=\\$(echo \\${kernel_version/-*} | awk -F. '{ printf(\\\"%d%03d%03d\\n\\\", \\$1,\\$2,\\$3); }') \n      echo kernel_version \\${kernel_version}\n      [[ \\${kernel_version} -ge ${version} ]] && exit 0 || exit 1\n    \"                                                                                                                                                 \n    check::exit_code \"$?\" \"check\" \"$host kernel version\" \"exit\"\n  done\n\n}\n\nfunction check::apiserver_conn() {\n  # 检查apiserver连通性\n\n  command::exec \"${MGMT_NODE}\" \"kubectl get node\"\n  check::exit_code \"$?\" \"check\" \"conn apiserver\" \"exit\"\n}\n\n\nfunction check::exit_code() {\n  # 检查返回码\n\n  local code=${1:-}\n  local app=${2:-}\n  local desc=${3:-}\n  local exit_script=${4:-}\n\n  if [[ \"${code}\" == \"0\" ]]; then\n    log::info \"[${app}]\" \"${desc} succeeded.\"\n  else\n    log::error \"[${app}]\" \"${desc} failed.\"\n    [[ \"$exit_script\" == \"exit\" ]] && exit \"$code\"\n  fi\n}\n\n\nfunction check::preflight() {\n  # 预检\n  \n  # check command\n  check::command\n\n  # check ssh conn\n  check::ssh_conn\n\n  # check os\n  check::os\n\n  # check os kernel\n  [[ \"${KUBE_NETWORK:-}\" == \"cilium\" && \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]] && check::kernel 4.9.17\n\n  # check apiserver conn\n  if [[ $(( ${ADD_TAG:-0} + ${DEL_TAG:-0} + ${UPGRADE_TAG:-0} + ${RENEW_CERT_TAG:-0} )) -gt 0 ]]; then\n    check::apiserver_conn\n  fi\n  \n  # check docker cri\n  [[ \"${KUBE_CRI}\" == \"docker\" && (\"${KUBE_VERSION}\" == \"latest\" || \"${KUBE_VERSION}\" =~ 1.24) ]] && { log::error \"[limit]\" \"$KUBE_CRI is not supported, only [containerd,cri-o]\"; exit 1; }\n}\n\n\nfunction install::package() {\n  # 安装包\n \n  if [[ \"${KUBE_CRI}\" == \"cri-o\" && \"${KUBE_CRI_VERSION}\" == \"latest\" ]]; then\n    KUBE_CRI_VERSION=\"${KUBE_VERSION}\"\n    if [[ \"${KUBE_CRI_VERSION}\" == \"latest\" ]]; then\n      if command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"; then\n        KUBE_CRI_VERSION=\"${COMMAND_OUTPUT#v}\"\n      else\n        log::error \"[install]\" \"get kubernetes stable version error. Please specify the version!\"\n        exit 1\n      fi\n    fi\n    KUBE_CRI_VERSION=\"${KUBE_CRI_VERSION%.*}\"\n  fi\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    # install cri\n    log::info \"[install]\" \"install ${KUBE_CRI} on $host.\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_\"${KUBE_CRI}\")\n      script::install_${KUBE_CRI} $KUBE_CRI_VERSION\n    \"\n    check::exit_code \"$?\" \"install\" \"install ${KUBE_CRI} on $host\"\n\n    # install kube\n    log::info \"[install]\" \"install kube on $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_kube)\n      script::install_kube $KUBE_VERSION\n    \"\n    check::exit_code \"$?\" \"install\" \"install kube on $host\"\n  done\n\n  local apiservers=$MASTER_NODES\n  if [[ \"$apiservers\" == \"127.0.0.1\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"ip -o route get to 8.8.8.8 | sed -n 's/.*src \\([0-9.]\\+\\).*/\\1/p'\"\n    get::command_output \"apiservers\" \"$?\"\n  fi\n\n  if [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n    \"\n    get::command_output \"apiservers\" \"$?\"\n  fi\n  \n  for host in $WORKER_NODES\n  do\n    # install haproxy\n    log::info \"[install]\" \"install haproxy on $host\"\n  command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::install_haproxy)\n      script::install_haproxy \\\"$apiservers\\\"\n  \"\n    check::exit_code \"$?\" \"install\" \"install haproxy on $host\"\n  done\n  \n  # 10年证书\n  if [[ \"${CERT_YEAR_TAG:-}\" == \"1\" ]]; then\n    local version=\"${KUBE_VERSION}\"\n    \n    if [[ \"${version}\" == \"latest\" ]]; then\n      if command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"; then\n        version=\"${COMMAND_OUTPUT#v}\"\n      else\n        log::error \"[install]\" \"get kubernetes stable version error. Please specify the version!\"\n        exit 1\n      fi\n    fi\n    \n    log::info \"[install]\" \"download kubeadm 10 years certs client\"\n    local certs_file=\"${OFFLINE_DIR}/bins/kubeadm-linux-amd64\"\n    MGMT_NODE=\"127.0.0.1\" utils::download_file \"${GITHUB_PROXY}https://github.com/lework/kubeadm-certs/releases/download/v${version}/kubeadm-linux-amd64\" \"${certs_file}\"\n    \n    for host in $MASTER_NODES $WORKER_NODES\n    do\n      log::info \"[install]\" \"scp kubeadm client to $host\"\n      command::scp \"${host}\" \"${certs_file}\" \"/tmp/kubeadm-linux-amd64\"\n      check::exit_code \"$?\" \"install\" \"scp kubeadm client to $host\" \"exit\"\n\n      command::exec \"${host}\" \"\n        set -e\n        if [[ -f /tmp/kubeadm-linux-amd64 ]]; then\n        [[ -f /usr/bin/kubeadm && ! -f /usr/bin/kubeadm_src ]] && mv -fv /usr/bin/kubeadm{,_src}\n          mv -fv /tmp/kubeadm-linux-amd64 /usr/bin/kubeadm\n          chmod +x /usr/bin/kubeadm\n        else\n          echo \\\"not found /tmp/kubeadm-linux-amd64\\\"\n          exit 1\n        fi\n    \"\n      check::exit_code \"$?\" \"install\" \"$host: use kubeadm 10 years certs client\"\n    done\n  fi\n}\n\n\nfunction init::upgrade_kernel() {\n  # 升级节点内核\n\n  [[ \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]] && return\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    log::info \"[init]\" \"upgrade kernel: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0}\n      $(declare -f script::upgrade_kernel)\n      script::upgrade_kernel\n    \"\n    check::exit_code \"$?\" \"init\" \"upgrade kernel $host\" \"exit\"\n  done\n  \n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    command::exec \"${host}\" \"bash -c 'sleep 15 && reboot' &>/dev/null &\"\n    check::exit_code \"$?\" \"init\" \"$host: Wait for 15s to restart\"\n  done\n\n  log::info \"[notice]\" \"Please execute the command again!\" \n  log::access \"[command]\" \"bash $0 ${SCRIPT_PARAMETER// --upgrade-kernel/}\"\n  exit 0\n}\n\n\nfunction cert::renew_node() {\n # 节点证书续期\n \n  local role=\"${1:-control-plane}\"\n  local hosts=\"\"\n  local kubelet_config=\"\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='node-role.kubernetes.io/${role}' -o jsonpath='{range.items[*]}{.metadata.name } {end}'\n  \"\n  get::command_output \"hosts\" \"$?\"\n  \n  for host in ${hosts}\n  do\n    log::info \"[cert]\" \"drain $host\"\n    command::exec \"${MGMT_NODE}\" \"kubectl drain $host --force --ignore-daemonsets --delete-local-data\"\n    check::exit_code \"$?\" \"cert\" \"$host: drain\" \"exit\"\n    sleep 5\n    \n    if [[ \"${role}\" == \"master\" || \"${role}\" == \"control-plane\" ]]; then\n      command::exec \"${host}\" \"cp -rf /etc/kubernetes /etc/kubernetes_\\$(date +%Y-%m-%d)\"\n      check::exit_code \"$?\" \"cert\" \"$host: backup kubernetes config\" \"exit\"\n      \n      command::exec \"${host}\" \"kubeadm certs renew all 2>/dev/null|| kubeadm alpha certs renew all\"\n      check::exit_code \"$?\" \"cert\" \"$host: renew certs\" \"exit\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof etcd) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:2379 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart etcd\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-apiserver) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:6443 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-apiserver\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-controller-manager) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10257 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n       \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-controller-manager\"\n      \n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        kill -s SIGHUP \\$(pidof kube-scheduler) && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10259 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: restart kube-scheduler\"\n    fi\n\n    log::info \"[cert]\" \"get kubelet config\"\n    command::exec \"${MGMT_NODE}\" \"\n      kubeadm kubeconfig user --org system:nodes --client-name system:node:${host}  --config /etc/kubernetes/kubeadmcfg.yaml || kubeadm alpha kubeconfig user --org system:nodes --client-name system:node:${host}  --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    get::command_output \"kubelet_config\" \"$?\" \"exit\"\n\n    if [[ \"$kubelet_config\" != \"\" ]]; then\n      log::info \"[cert]\" \"copy kubelet config\"\n      command::exec \"${host}\" \"\n        cp /etc/kubernetes/kubelet.conf /etc/kubernetes/kubelet.conf_bak\n        echo '$(printf \"%s\" \"${kubelet_config}\" | sed 's#https://.*:#https://127.0.0.1:#g')' > /etc/kubernetes/kubelet.conf\n      \"\n      check::exit_code \"$?\" \"cert\" \"$host: copy kubelet config\" \"exit\"\n\n      command::exec \"${host}\" \"rm -rfv /var/lib/kubelet/pki/*\"\n      check::exit_code \"$?\" \"cert\" \"$host: delete kubelet pki files\" \"exit\"\n\n      command::exec \"${host}\" \"\n        $(declare -f utils::retry)\n        systemctl restart kubelet && \\\n        utils::retry 10 \\\"echo -n | openssl s_client -connect localhost:10250 2>&1 | sed -ne '/-BEGIN CERTIFICATE-/,/-END CERTIFICATE-/p' | openssl x509 -text -noout | grep Not\\\"\n      \"\n      local status=\"$?\"\n      check::exit_code \"${status}\" \"cert\" \"$host: restart kubelet\"\n      if [[ \"${status}\" == \"0\" ]]; then\n        sleep 5\n        command::exec \"${MGMT_NODE}\" \"kubectl uncordon ${host}\"\n        check::exit_code \"$?\" \"cert\" \"uncordon ${host} node\" \"exit\"\n      fi\n    fi\n  done\n}\n\n\nfunction cert::renew() {\n  # 证书续期\n \n  log::info \"[cert]\" \"renew cluster cert\"\n  cert::renew_node \"control-plan\"\n  cert::renew_node \"worker\"\n \n  log::info \"[cert]\" \"cluster status\"\n  command::exec \"${MGMT_NODE}\" \"\n    echo\n    kubectl get node\n    echo\n    kubeadm certs check-expiration 2>/dev/null || kubeadm alpha certs check-expiration\n  \" && printf \"%s\" \"${COMMAND_OUTPUT}\"\n}\n\n\nfunction init::node_config() {\n  # 初始化节点配置\n\n  local master_index=${master_index:-1}\n  local worker_index=${worker_index:-1}\n  \n  log::info \"[init]\" \"Get $MGMT_NODE InternalIP.\"\n  command::exec \"${MGMT_NODE}\" \"\n    ip -4 route get 8.8.8.8 2>/dev/null | head -1 | awk '{print \\$7}'\n  \"\n  get::command_output \"MGMT_NODE_IP\" \"$?\" \"exit\"\n\n  # master\n  for host in $MASTER_NODES\n  do\n    log::info \"[init]\" \"master: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0} KUBE_APISERVER=${KUBE_APISERVER} SKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n      $(declare -f script::init_node)\n      script::init_node\n   \"\n    check::exit_code \"$?\" \"init\" \"init master $host\" \"exit\"\n\n    # 设置主机名和解析\n    command::exec \"${host}\" \"\n      printf \\\"\\\\n${MGMT_NODE_IP} $KUBE_APISERVER\\\\n$node_hosts\\\" >> /etc/hosts\n      hostnamectl set-hostname ${HOSTNAME_PREFIX}-master-node${master_index}\n    \"\n    check::exit_code \"$?\" \"init\" \"$host set hostname and hostname resolution\"\n\n    # set audit-policy\n    log::info \"[init]\" \"$host: set audit-policy file.\"\n    command::exec \"${host}\" \"\n      [ ! -d etc/kubernetes ] && mkdir -p /etc/kubernetes\n      cat << EOF > /etc/kubernetes/audit-policy.yaml\n# Log all requests at the Metadata level.\napiVersion: audit.k8s.io/v1\nkind: Policy\nrules:\n- level: Metadata\nEOF\n    \"\n    check::exit_code \"$?\" \"init\" \"$host: set audit-policy file\" \"exit\"\n    master_index=$((master_index + 1))\n  done\n   \n  # worker\n  for host in $WORKER_NODES\n  do\n    log::info \"[init]\" \"worker: $host\"\n    command::exec \"${host}\" \"\n      export OFFLINE_TAG=${OFFLINE_TAG:-0} KUBE_APISERVER=${KUBE_APISERVER} SKIP_SET_OS_REPO=${SKIP_SET_OS_REPO:-false}\n      $(declare -f script::init_node)\n      script::init_node\n    \"\n    check::exit_code \"$?\" \"init\" \"init worker $host\" \"exit\"\n\n    # 设置主机名和解析\n    command::exec \"${host}\" \"\n      printf \\\"\\\\n127.0.0.1 $KUBE_APISERVER\\\\n$node_hosts\\\" >> /etc/hosts\n      hostnamectl set-hostname ${HOSTNAME_PREFIX}-worker-node${worker_index}\n    \"\n    worker_index=$((worker_index + 1))\n  done\n}\n\n\nfunction init::node() {\n  # 初始化节点\n  \n  init::upgrade_kernel\n\n  local node_hosts=\"\"\n  local i=1\n  for h in $MASTER_NODES\n  do\n    node_hosts=\"${node_hosts}\\n$h ${HOSTNAME_PREFIX}-master-node${i}\"\n    i=$((i + 1))\n  done\n  \n  local i=1\n  for h in $WORKER_NODES\n  do\n    node_hosts=\"${node_hosts}\\n$h ${HOSTNAME_PREFIX}-worker-node${i}\"\n    i=$((i + 1))\n  done\n\n  init::node_config\n}\n\n\nfunction init::add_node() {\n  # 初始化添加的节点\n  \n  init::upgrade_kernel\n\n  local master_index=0\n  local worker_index=0\n  local node_hosts=\"\"\n  local add_node_hosts=\"\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address } {end}' | awk '{print \\$1}'\n  \"\n  get::command_output \"MGMT_NODE\" \"$?\" \"exit\"\n\n  # 获取现有集群节点主机名\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.metadata.name }\\\\n{end}'\n  \"\n  get::command_output \"node_hosts\" \"$?\" \"exit\"\n  \n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    if [[ $node_hosts == *\"$host\"* ]]; then\n      log::error \"[init]\" \"The host $host is already in the cluster!\"\n      exit 1\n    fi\n  done\n  \n  if [[ \"$MASTER_NODES\" != \"\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].metadata.name}' |grep -Eo 'node[0-9]*'|grep -Eo '[0-9]*'|awk -F ' ' 'BEGIN {max = 0} {if (\\$0+0 > max+0) max=\\$0} END {print max}'\n    \"\n    get::command_output \"master_index\" \"$?\" \"exit\"\n    master_index=$(( master_index + 1 ))\n    local i=$master_index\n    for host in $MASTER_NODES\n    do\n      add_node_hosts=\"${add_node_hosts}\\n${host:-} ${HOSTNAME_PREFIX}-master-node${i}\"\n      i=$((i + 1))\n    done\n  fi\n\n  if [[ \"$WORKER_NODES\" != \"\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].metadata.name}'| grep -Eo 'node[0-9]*'|grep -Eo '[0-9]*'|awk 'BEGIN {max = 0} {if (\\$0+0 > max+0) max=\\$0} END {print max}' || echo 0\n    \"\n    get::command_output \"worker_index\" \"$?\" \"exit\"\n    worker_index=$(( worker_index + 1 ))\n    local i=$worker_index\n    for host in $WORKER_NODES\n    do\n      add_node_hosts=\"${add_node_hosts}\\n${host:-} ${HOSTNAME_PREFIX}-worker-node${i}\"\n      i=$((i + 1))\n    done\n  fi\n  #向集群节点添加新增的节点主机名解析 \n  for host in $(echo -ne \"$node_hosts\" | awk '{print $1}')\n  do\n     command::exec \"${host}\" \"\n       printf \\\"$add_node_hosts\\\" >> /etc/hosts\n     \"\n     check::exit_code \"$?\" \"init\" \"$host add new node hostname resolution\"\n  done\n\n  node_hosts=\"${node_hosts}\\n${add_node_hosts}\"\n  init::node_config\n}\n\n\nfunction kubeadm::init() {\n  # 集群初始化\n  \n  log::info \"[kubeadm init]\" \"kubeadm init on ${MGMT_NODE}\"\n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: set kubeadmcfg.yaml\"\n  command::exec \"${MGMT_NODE}\" \"\n    PAUSE_VERSION=$(kubeadm config images list 2>/dev/null | awk -F: '/pause/ {print $2}')\n    cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: InitConfiguration\n${kubelet_nodeRegistration}\n---\napiVersion: kubeproxy.config.k8s.io/v1alpha1\nkind: KubeProxyConfiguration\nmode: ipvs\nipvs:\n  minSyncPeriod: 5s\n  syncPeriod: 5s\n  # ipvs 负载策略\n  scheduler: 'wrr'\n\n---\napiVersion: kubelet.config.k8s.io/v1beta1\nkind: KubeletConfiguration\nmaxPods: 200\ncgroupDriver: systemd\nruntimeRequestTimeout: 5m\n# 此配置保证了 kubelet 能在 swap 开启的情况下启动\nfailSwapOn: false\nnodeStatusUpdateFrequency: 5s\nrotateCertificates: true\nimageGCLowThresholdPercent: 70\nimageGCHighThresholdPercent: 80\n# 软驱逐阀值\nevictionSoft:\n  imagefs.available: 15%\n  memory.available: 512Mi\n  nodefs.available: 15%\n  nodefs.inodesFree: 10%\n# 达到软阈值之后，持续时间超过多久才进行驱逐\nevictionSoftGracePeriod:\n  imagefs.available: 3m\n  memory.available: 1m\n  nodefs.available: 3m\n  nodefs.inodesFree: 1m\n# 硬驱逐阀值\nevictionHard:\n  imagefs.available: 10%\n  memory.available: 256Mi\n  nodefs.available: 10%\n  nodefs.inodesFree: 5%\nevictionMaxPodGracePeriod: 30\n# 节点资源预留\nkubeReserved:\n  cpu: 200m\\$(if [[ \\$(cat /proc/meminfo | awk '/MemTotal/ {print \\$2}') -gt 3670016 ]]; then echo -e '\\n  memory: 256Mi';fi)\n  ephemeral-storage: 1Gi\nsystemReserved:\n  cpu: 300m\\$(if [[ \\$(cat /proc/meminfo | awk '/MemTotal/ {print \\$2}') -gt 3670016 ]]; then echo -e '\\n  memory: 512Mi';fi)\n  ephemeral-storage: 1Gi\nkubeReservedCgroup: /kube.slice\nsystemReservedCgroup: /system.slice\nenforceNodeAllocatable: \n- pods\n\n---\napiVersion: kubeadm.k8s.io/v1beta3\nkind: ClusterConfiguration\nkubernetesVersion: $KUBE_VERSION\ncontrolPlaneEndpoint: $KUBE_APISERVER:6443\nnetworking:\n  dnsDomain: $KUBE_DNSDOMAIN\n  podSubnet: $KUBE_POD_SUBNET\n  serviceSubnet: $KUBE_SERVICE_SUBNET\nimageRepository: $KUBE_IMAGE_REPO\napiServer:\n  certSANs:\n  - 127.0.0.1\n  - $KUBE_APISERVER\n$(for h in $MASTER_NODES;do echo \"  - $h\";done)\n  extraArgs:\n    event-ttl: '720h'\n    service-node-port-range: '30000-50000'\n    # 审计日志相关配置\n    audit-log-maxage: '20'\n    audit-log-maxbackup: '10'\n    audit-log-maxsize: '100'\n    audit-log-path: /var/log/kube-audit/audit.log\n    audit-policy-file: /etc/kubernetes/audit-policy.yaml\n  extraVolumes:\n  - name: audit-config\n    hostPath: /etc/kubernetes/audit-policy.yaml\n    mountPath: /etc/kubernetes/audit-policy.yaml\n    readOnly: true\n    pathType: File\n  - name: audit-log\n    hostPath: /var/log/kube-audit\n    mountPath: /var/log/kube-audit\n    pathType: DirectoryOrCreate\n  - name: localtime\n    hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    readOnly: true\n    pathType: File\ncontrollerManager:\n  extraArgs:\n    bind-address: 0.0.0.0\n    node-cidr-mask-size: '24'\n    node-monitor-grace-period: '20s'\n    pod-eviction-timeout: '2m'\n    terminated-pod-gc-threshold: '30'\n    cluster-signing-duration: 87600h\n    feature-gates: RotateKubeletServerCertificate=true\n  extraVolumes:\n  - hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    name: localtime\n    readOnly: true\n    pathType: File\nscheduler:\n  extraArgs:\n    bind-address: 0.0.0.0\n  extraVolumes:\n  - hostPath: /usr/share/zoneinfo/Asia/Shanghai\n    mountPath: /etc/localtime\n    name: localtime\n    readOnly: true\n    pathType: File\n$(if [[ \"${KUBE_VERSION}\" == \"1.21.1\" ]]; then\necho \"dns:\n  type: CoreDNS\n  imageRepository: docker.io\n  imageTag: 1.8.0\"\nfi)\nEOF\n\"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: set kubeadmcfg.yaml\" \"exit\"\n  \n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: kubeadm init start.\"\n  command::exec \"${MGMT_NODE}\" \"kubeadm init --config=/etc/kubernetes/kubeadmcfg.yaml --upload-certs\"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: kubeadm init\" \"exit\"\n  \n  sleep 3\n  \n  log::info \"[kubeadm init]\" \"${MGMT_NODE}: set kube config.\"\n  command::exec \"${MGMT_NODE}\" \"\n     mkdir -p \\$HOME/.kube\n     sudo cp -f /etc/kubernetes/admin.conf \\$HOME/.kube/config\n  \"\n  check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: set kube config\" \"exit\"\n  if [[ \"$(echo \"$MASTER_NODES\" | wc -w)\" == \"1\" ]]; then\n    log::info \"[kubeadm init]\" \"${MGMT_NODE}: delete master taint\"\n    command::exec \"${MGMT_NODE}\" \"kubectl taint nodes --all node-role.kubernetes.io/master- || kubectl taint nodes --all node-role.kubernetes.io/control-plane-\"\n    check::exit_code \"$?\" \"kubeadm init\" \"${MGMT_NODE}: delete master taint\"\n  fi\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl create clusterrolebinding node-client-auto-approve-csr --clusterrole=system:certificates.k8s.io:certificatesigningrequests:nodeclient --user=kubelet-bootstrap\n    kubectl create clusterrolebinding node-client-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeclient --group=system:nodes\n    kubectl create clusterrolebinding node-server-auto-renew-crt --clusterrole=system:certificates.k8s.io:certificatesigningrequests:selfnodeserver --group=system:nodes\n  \"\n  check::exit_code \"$?\" \"kubeadm init\" \"Auto-Approve kubelet cert csr\" \"exit\"\n}\n\n\nfunction kubeadm::join() {\n  # 加入集群\n\n  log::info \"[kubeadm join]\" \"master: get join token and cert info\"\n  command::exec \"${MGMT_NODE}\" \"\n    openssl x509 -pubkey -in /etc/kubernetes/pki/ca.crt | openssl rsa -pubin -outform der 2>/dev/null | openssl dgst -sha256 -hex | sed 's/^.* //'\n  \"\n  get::command_output \"CACRT_HASH\" \"$?\" \"exit\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm init phase upload-certs --upload-certs --config /etc/kubernetes/kubeadmcfg.yaml 2>> /dev/null | tail -1\n  \"\n  get::command_output \"INTI_CERTKEY\" \"$?\" \"exit\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm token create\n  \"\n  get::command_output \"INIT_TOKEN\" \"$?\" \"exit\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm config images list 2>/dev/null | awk -F: '/pause/ {print \\$2}'\n  \"\n  get::command_output \"PAUSE_VERSION\" \"$?\"\n\n  for host in $MASTER_NODES\n  do\n    [[ \"${MGMT_NODE}\" == \"$host\" ]] && continue\n    log::info \"[kubeadm join]\" \"master $host join cluster.\"\n    command::exec \"${host}\" \"\n      cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta2\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: $KUBE_APISERVER:6443\n    caCertHashes:\n    - sha256:${CACRT_HASH:-}\n    token: ${INIT_TOKEN}\n  timeout: 5m0s\ncontrolPlane:\n  certificateKey: ${INTI_CERTKEY:-}\n${kubelet_nodeRegistration}\nEOF\n      kubeadm join --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"master $host join cluster\"\n\n    log::info \"[kubeadm join]\" \"$host: set kube config.\"\n    command::exec \"${host}\" \"\n      mkdir -p \\$HOME/.kube\n      sudo cp -f /etc/kubernetes/admin.conf \\$HOME/.kube/config\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"$host: set kube config\" \"exit\"\n    \n    command::exec \"${host}\" \"\n      sed -i 's#.*$KUBE_APISERVER#127.0.0.1 $KUBE_APISERVER#g' /etc/hosts\n    \"\n  done\n\n  for host in $WORKER_NODES\n  do\n    log::info \"[kubeadm join]\" \"worker $host join cluster.\"\n    command::exec \"${host}\" \"\n      mkdir -p /etc/kubernetes/manifests\n      cat << EOF > /etc/kubernetes/kubeadmcfg.yaml\n---\napiVersion: kubeadm.k8s.io/v1beta2\nkind: JoinConfiguration\ndiscovery:\n  bootstrapToken:\n    apiServerEndpoint: $KUBE_APISERVER:6443\n    caCertHashes:\n    - sha256:${CACRT_HASH:-}\n    token: ${INIT_TOKEN}\n  timeout: 5m0s\n${kubelet_nodeRegistration}\nEOF\n      kubeadm join --config /etc/kubernetes/kubeadmcfg.yaml\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"worker $host join cluster\"\n  \n    log::info \"[kubeadm join]\" \"set $host worker node role.\"\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/master,!node-role.kubernetes.io/control-plane' | grep '<none>' | awk '{print \\\"kubectl label node \\\" \\$1 \\\" node-role.kubernetes.io/worker= --overwrite\\\" }' | bash\n    \"\n    check::exit_code \"$?\" \"kubeadm join\" \"set $host worker node role\"\n  done\n}\n\n\nfunction kube::wait() {\n  # 等待资源完成\n\n  local app=$1\n  local namespace=$2\n  local resource=$3\n  local selector=${4:-}\n\n  sleep 3\n  log::info \"[waiting]\" \"waiting $app\"\n  command::exec \"${MGMT_NODE}\" \"\n    $(declare -f utils::retry)\n    utils::retry 6 kubectl wait --namespace ${namespace} \\\n    --for=condition=ready ${resource} \\\n    --selector=$selector \\\n    --timeout=60s\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"waiting\" \"$app ${resource} ready\"\n  return \"$status\"\n}\n\n\nfunction kube::apply() {\n  # 应用manifest\n\n  local file=$1\n\n  log::info \"[apply]\" \"$file\"\n  command::exec \"${MGMT_NODE}\" \"\n    $(declare -f utils::retry)\n    if [ -f \\\"$file\\\" ]; then\n      utils::retry 6 kubectl apply --wait=true --timeout=10s -f \\\"$file\\\"\n    else\n      utils::retry 6 \\\"cat <<EOF | kubectl apply --wait=true --timeout=10s -f -\n\\$(printf \\\"%s\\\" \\\"${2:-}\\\")\nEOF\n      \\\"\n    fi\n  \"\n  local status=\"$?\"\n  check::exit_code \"$status\" \"apply\" \"add $file\" \"exit\"\n  return \"$status\"\n}\n\n\nfunction kube::status() {\n  # 集群状态\n  \n  sleep 5\n  log::info \"[cluster]\" \"cluster status\"\n  command::exec \"${MGMT_NODE}\" \"\n     echo\n     kubectl get node -o wide\n     echo\n     kubectl get pods -A\n  \" && printf \"%s\" \"${COMMAND_OUTPUT}\"\n}\n\n\nfunction config::haproxy_backend() {\n  # 添加或删除haproxy的后端server\n\n  local action=${1:-add}\n  local action_cmd=\"\"\n  local master_nodes\n  \n  if [[ \"$MASTER_NODES\" == \"\" || \"$MASTER_NODES\" == \"127.0.0.1\" ]]; then\n    return\n  fi\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n  \"\n  get::command_output \"master_nodes\" \"$?\" \"exit\"\n  \n  for m in $MASTER_NODES\n  do\n    if [[ \"${action}\" == \"add\" ]]; then\n      num=$(echo \"${m}\"| awk -F'.' '{print $4}')\n      action_cmd=\"${action_cmd}\\necho \\\"    server apiserver${num} ${m}:6443 check\\\" >> /etc/haproxy/haproxy.cfg\"\n    else\n      [[ \"${master_nodes}\" == *\"${m}\"* ]] || return\n      action_cmd=\"${action_cmd}\\n sed -i -e \\\"/${m}/d\\\" /etc/haproxy/haproxy.cfg\"\n    fi\n  done\n        \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='node-role.kubernetes.io/worker' -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n  \"\n  get::command_output \"worker_nodes\" \"$?\"\n  \n  for host in ${worker_nodes:-}\n  do\n    log::info \"[config]\" \"worker ${host}: ${action} apiserver from haproxy\"\n    command::exec \"${host}\" \"\n      $(echo -ne \"${action_cmd}\")\n      haproxy -c -f /etc/haproxy/haproxy.cfg && systemctl reload haproxy\n    \"\n    check::exit_code \"$?\" \"config\" \"worker ${host}: ${action} apiserver(${m}) from haproxy\"\n  done\n}\n\n\nfunction config::etcd_snapshot() {\n  # 更新 etcd 备份副本\n\n  command::exec \"${MGMT_NODE}\" \"\n    count=\\$(kubectl get node --selector='!node-role.kubernetes.io/worker' --no-headers | wc -l)\n    kubectl -n kube-system patch cronjobs etcd-snapshot --patch \\\"\nspec:\n  jobTemplate:\n    spec:\n      completions: \\${count:-1}\n      parallelism: \\${count:-1}\n\\\"\n  \"\n  check::exit_code \"$?\" \"config\" \"etcd-snapshot completions options\"\n}\n\n\nfunction get::command_output() {\n   # 获取命令的返回值\n\n   local app=\"$1\"\n   local status=\"$2\"\n   local is_exit=\"${3:-}\"\n   \n   if [[ \"$status\" == \"0\" && \"${COMMAND_OUTPUT}\" != \"\" ]]; then\n     log::info \"[command]\" \"get $app value succeeded.\"\n     eval \"$app=\\\"${COMMAND_OUTPUT}\\\"\"\n   else\n     log::error \"[command]\" \"get $app value failed.\"\n     [[ \"$is_exit\" == \"exit\" ]] && exit \"$status\"\n   fi\n   return \"$status\"\n}\n\n\nfunction get::ingress_conn(){\n  # 获取ingress连接地址\n\n  local port=\"${1:-80}\"\n  local ingress_name=\"${2:-ingress-${KUBE_INGRESS}-controller}\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range .items[*]}{ .status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.status.conditions[?(@.status == \\\"True\\\")].status}{\\\"\\\\n\\\"}{end}' | awk '{if(\\$2==\\\"True\\\")a=\\$1}END{print a}'\n  \"\n  get::command_output \"node_ip\" \"$?\"\n\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get svc --all-namespaces -o go-template=\\\"{{range .items}}{{if eq .metadata.name \\\\\\\"${ingress_name}\\\\\\\"}}{{range.spec.ports}}{{if eq .port ${port}}}{{.nodePort}}{{end}}{{end}}{{end}}{{end}}\\\"\n  \"\n  \n  get::command_output \"node_port\" \"$?\"\n \n  INGRESS_CONN=\"${node_ip:-nodeIP}:${node_port:-nodePort}\"\n}\n\n\nfunction add::ingress() {\n  # 添加ingress组件\n\n  local add_ingress_demo=0\n\n  if [[ \"$KUBE_INGRESS\" == \"nginx\" ]]; then\n    log::info \"[ingress]\" \"add ingress-nginx\"\n    \n    local ingress_nginx_file=\"${OFFLINE_DIR}/manifests/ingress-nginx.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v${INGRESS_NGINX}/deploy/static/provider/baremetal/deploy.yaml\" \"${ingress_nginx_file}\"\n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#k8s.gcr.io/ingress-nginx#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#registry.k8s.io/ingress-nginx#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#@sha256:.*\\$##g' '${ingress_nginx_file}'\n    \"\n    check::exit_code \"$?\" \"ingress\" \"change ingress-nginx manifests\"\n    kube::apply \"${ingress_nginx_file}\"\n\n    kube::wait \"ingress-nginx\" \"ingress-nginx\" \"pod\" \"app.kubernetes.io/component=controller\" && add_ingress_demo=1\n\n    command::exec \"${MGMT_NODE}\" \"kubectl delete -A ValidatingWebhookConfiguration ingress-nginx-admission\"\n    check::exit_code \"$?\" \"ingress\" \"delete ingress-ngin ValidatingWebhookConfiguration\"\n\n    log::info \"[ingress]\"  \"set nginx is default ingress class\"\n    command::exec \"${MGMT_NODE}\" \"\n      default_class=\\\"\\$(kubectl get ingressclass -A -o jsonpath='{.items[?(@.metadata.annotations.ingressclass\\\\.kubernetes\\\\.io/is-default-class==\\\\\\\"true\\\\\\\")].metadata.name}')\\\"\n      if [ \\\"\\${default_class:-}\\\" != \\\"\\\" ]; then\n         kubectl patch ingressclass \\${default_class} -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"ingressclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\n      fi\n      kubectl patch ingressclass nginx -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"ingressclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\n    \"\n    check::exit_code \"$?\" \"ingress\" \"set nginx is default ingress class\"\n\n  elif [[ \"$KUBE_INGRESS\" == \"traefik\" ]]; then\n    log::info \"[ingress]\" \"add ingress-traefik\"\n    kube::apply \"traefik\" \"\"\"\n---\nkind: ClusterRole\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: ingress-traefik-controller\nrules:\n  - apiGroups:\n      - ''\n    resources:\n      - services\n      - endpoints\n      - secrets\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n      - networking.k8s.io\n    resources:\n      - ingresses\n      - ingressclasses\n    verbs:\n      - get\n      - list\n      - watch\n  - apiGroups:\n      - extensions\n    resources:\n      - ingresses/status\n    verbs:\n      - update\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: ingress-traefik-controller\nroleRef:\n  apiGroup: rbac.authorization.k8s.io\n  kind: ClusterRole\n  name: ingress-traefik-controller\nsubjects:\n  - kind: ServiceAccount\n    name: ingress-traefik-controller\n    namespace: default\n--- \napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: ingress-traefik-controller\n---\nkind: Deployment\napiVersion: apps/v1\nmetadata:\n  name: ingress-traefik-controller\n  labels:\n    app: ingress-traefik-controller\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: ingress-traefik-controller\n  template:\n    metadata:\n      labels:\n        app: ingress-traefik-controller\n    spec:\n      serviceAccountName: ingress-traefik-controller\n      containers:\n        - name: traefik\n          image: traefik:v${TRAEFIK_VERSION}\n          args:\n            - --api.debug=true\n            - --api.insecure=true\n            - --log=true\n            - --log.level=debug\n            - --ping=true\n            - --accesslog=true\n            - --entrypoints.http.Address=:80\n            - --entrypoints.https.Address=:443\n            - --entrypoints.traefik.Address=:8080\n            - --providers.kubernetesingress\n            - --serverstransport.insecureskipverify=true\n          ports:\n            - name: http\n              containerPort: 80\n              protocol: TCP\n            - name: https\n              containerPort: 443\n              protocol: TCP\n            - name: admin\n              containerPort: 8080\n              protocol: TCP\n          livenessProbe:\n            failureThreshold: 2\n            httpGet:\n              path: /ping\n              port: admin\n              scheme: HTTP\n            initialDelaySeconds: 10\n            periodSeconds: 5\n            successThreshold: 1\n            timeoutSeconds: 1\n          readinessProbe:\n            failureThreshold: 3\n            httpGet:\n              path: /ping\n              port: admin\n              scheme: HTTP\n            periodSeconds: 10\n            successThreshold: 1\n            timeoutSeconds: 1\n          resources:\n            limits:\n              cpu: 250m\n              memory: 128Mi\n            requests:\n              cpu: 100m\n              memory: 64Mi\n          securityContext:\n            capabilities:\n              add:\n              - NET_BIND_SERVICE\n              drop:\n              - ALL\n      restartPolicy: Always\n      serviceAccount: ingress-traefik-controller\n      serviceAccountName: ingress-traefik-controller\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-traefik-controller\nspec:\n  type: NodePort\n  selector:\n    app: ingress-traefik-controller\n  ports:\n    - protocol: TCP\n      port: 80\n      name: http\n      targetPort: 80\n    - protocol: TCP\n      port: 443\n      name: https\n      targetPort: 443\n    - protocol: TCP\n      port: 8080\n      name: admin\n      targetPort: 8080\n\"\"\"\n    kube::wait \"traefik\" \"default\" \"pod\" \"app=ingress-traefik-controller\" && add_ingress_demo=1\n  else\n    log::warning \"[ingress]\" \"No $KUBE_INGRESS config.\"\n  fi\n\n  if [[ \"$add_ingress_demo\" == \"1\" ]]; then\n    log::info \"[ingress]\" \"add ingress default-http-backend\"\n    kube::apply \"default-http-backend\" \"\"\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: default-http-backend\n  labels:\n    app.kubernetes.io/name: default-http-backend\n  namespace: kube-system\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app.kubernetes.io/name: default-http-backend\n  template:\n    metadata:\n      labels:\n        app.kubernetes.io/name: default-http-backend\n    spec:\n      terminationGracePeriodSeconds: 60\n      containers:\n      - name: default-http-backend\n        image: ${KUBE_IMAGE_REPO}/defaultbackend-amd64:1.5\n        livenessProbe:\n          httpGet:\n            path: /healthz\n            port: 8080\n            scheme: HTTP\n          initialDelaySeconds: 30\n          timeoutSeconds: 5\n        ports:\n        - containerPort: 8080\n        resources:\n          limits:\n            cpu: 10m\n            memory: 20Mi\n          requests:\n            cpu: 10m\n            memory: 20Mi\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: default-http-backend\n  namespace: kube-system\n  labels:\n    app.kubernetes.io/name: default-http-backend\nspec:\n  ports:\n  - port: 80\n    targetPort: 8080\n  selector:\n    app.kubernetes.io/name: default-http-backend\n\"\"\"\n    log::info \"[ingress]\" \"add ingress app demo\"\n    kube::apply \"ingress-demo-app\" \"\"\"\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: ingress-demo-app\n  labels:\n    app: ingress-demo-app\nspec:\n  replicas: 2\n  selector:\n    matchLabels:\n      app: ingress-demo-app\n  template:\n    metadata:\n      labels:\n        app: ingress-demo-app\n    spec:\n      containers:\n      - name: whoami\n        image: traefik/whoami:v1.10.1\n        ports:\n        - containerPort: 80\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: ingress-demo-app\nspec:\n  type: ClusterIP\n  selector:\n    app: ingress-demo-app\n  ports:\n    - name: http\n      port: 80\n      targetPort: 80\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: ingress-demo-app\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: app.demo.com\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: ingress-demo-app\n            port:\n              number: 80\n\"\"\"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:app.demo.com' http://${INGRESS_CONN}\"\n    fi\n  fi\n}\n\n#compare version numbers\nfunction is::larger_version_than() {\n  local version1=$1;\n  local version2=$2;  \n  return \"$(echo \"$version1\" \"$version2\" | awk '{if ($1 >= $2) print 1; else print 0}')\"\n}\n\nfunction add::network() {\n  # 添加network组件\n\n  if [[ \"$KUBE_NETWORK\" == \"flannel\" ]]; then\n    log::info \"[network]\" \"add flannel\"\n    \n    local flannel_file=\"${OFFLINE_DIR}/manifests/kube-flannel.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/flannel-io/flannel/v${FLANNEL_VERSION}/Documentation/kube-flannel.yml\" \"${flannel_file}\"\n    \n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#10.244.0.0/16#${KUBE_POD_SUBNET}#g' \\\n             -e 's#quay.io/coreos#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#docker.io/flannel#${KUBE_IMAGE_REPO}#g' \\\n             -e 's#namespace: kube-system#namespace: kube-flannel#g' \\\n             -e 's#\\\"Type\\\": \\\"vxlan\\\"#\\\"Type\\\": \\\"${KUBE_FLANNEL_TYPE}\\\"#g' \\\"${flannel_file}\\\"\n      if [[ \\\"${KUBE_FLANNEL_TYPE}\\\" == \\\"vxlan\\\" ]]; then\n        sed -i 's#\\\"Type\\\": \\\"vxlan\\\"#\\\"Type\\\": \\\"vxlan\\\", \\\"DirectRouting\\\": true#g' \\\"${flannel_file}\\\"\n      fi\n    \"\n    check::exit_code \"$?\" \"flannel\" \"change flannel pod subnet\"\n    kube::apply \"${flannel_file}\"\n    kube::wait \"flannel\" \"kube-flannel\" \"pods\" \"app=flannel\"\n\n  elif [[ \"$KUBE_NETWORK\" == \"calico\" ]]; then\n    log::info \"[network]\" \"add calico\"\n    \n    utils::download_file \"https://raw.githubusercontent.com/projectcalico/calico/v${CALICO_VERSION}/manifests/calico.yaml\" \"${OFFLINE_DIR}/manifests/calico.yaml\"\n    utils::download_file \"https://raw.githubusercontent.com/projectcalico/calico/v${CALICO_VERSION}/manifests/calicoctl.yaml\" \"${OFFLINE_DIR}/manifests/calicoctl.yaml\"\n    command::exec \"${MGMT_NODE}\" \"\n      sed -i 's#value: \\\"Always\\\"#value: \\\"CrossSubnet\\\"#g' \\\"${OFFLINE_DIR}/manifests/calico.yaml\\\"\n    \"\n    check::exit_code \"$?\" \"network\" \"change calico version to ${CALICO_VERSION}\"\n    \n    kube::apply \"${OFFLINE_DIR}/manifests/calico.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/calicoctl.yaml\"\n    kube::wait \"calico-kube-controllers\" \"kube-system\" \"pods\" \"k8s-app=calico-kube-controllers\"\n    kube::wait \"calico-node\" \"kube-system\" \"pods\" \"k8s-app=calico-node\"\n\n  elif [[ \"$KUBE_NETWORK\" == \"cilium\" ]]; then \n    log::info \"[network]\" \"add cilium\"\n\n    CILIUM_CLI_VERSION=$(curl -s \"${GITHUB_PROXY}https://raw.githubusercontent.com/cilium/cilium-cli/main/stable.txt\")\n    CLI_ARCH=amd64\n    if [ \"$(uname -m)\" = \"aarch64\" ]; then CLI_ARCH=arm64; fi\n    utils::download_file \"${GITHUB_PROXY}https://github.com/cilium/cilium-cli/releases/download/${CILIUM_CLI_VERSION}/cilium-linux-${CLI_ARCH}.tar.gz\"  \"${OFFLINE_DIR}/cilium-linux-${CLI_ARCH}.tar.gz\"\n    tar xzvfC \"${OFFLINE_DIR}/cilium-linux-${CLI_ARCH}.tar.gz\" /usr/local/bin\n    [ -d /etc/bash_completion.d ] && cilium completion bash > /etc/bash_completion.d/cilium\n    cilium install --version \"${CILIUM_VERSION}\" \\\n                   --set ipam.mode=cluster-pool \\\n                   --set ipam.Operator.clusterPoolIPv4PodCIDRList=[\"{KUBE_POD_SUBNET}\"] \\\n                   --set ipam.Operator.clusterPoolIPv4MaskSize=24 \\\n                   --set kubeProxyReplacement=false\n    cilium status --wait\n    cilium hubble enable --ui\n   \n    log::info \"[monitor]\" \"add hubble-ui ingress\"\n    kube::apply \"hubble-ui ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: hubble-ui\n  namespace: kube-system\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: hubble-ui.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: hubble-ui\n            port:\n              number: 80\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then                                                                                                                                            \n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:hubble-ui.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[network]\" \"No $KUBE_NETWORK config.\"\n  fi\n}\n\n\nfunction add::addon() {\n  # 添加addon组件\n\n  if [[ \"$KUBE_ADDON\" == \"metrics-server\" ]]; then\n    log::info \"[addon]\" \"download metrics-server manifests\"\n    local metrics_server_file=\"${OFFLINE_DIR}/manifests/metrics-server.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/kubernetes-sigs/metrics-server/releases/download/v${METRICS_SERVER_VERSION}/components.yaml\" \"${metrics_server_file}\"\n  \n    command::exec \"${MGMT_NODE}\" \"\n      sed -i -e 's#k8s.gcr.io/metrics-server#$KUBE_IMAGE_REPO#g' \\\n             -e 's#registry.k8s.io/metrics-server#$KUBE_IMAGE_REPO#g' \\\n             -e '/--kubelet-preferred-address-types=.*/d' \\\n             -e 's/\\\\(.*\\\\)- --secure-port=\\\\(.*\\\\)/\\\\1- --secure-port=\\\\2\\\\n\\\\1- --kubelet-insecure-tls\\\\n\\\\1- --kubelet-preferred-address-types=InternalIP,InternalDNS,ExternalIP,ExternalDNS,Hostname/g' \\\n             \\\"${metrics_server_file}\\\"\n    \"\n    check::exit_code \"$?\" \"addon\" \"change metrics-server parameter\"\n    kube::apply \"${metrics_server_file}\"\n    kube::wait \"metrics-server\" \"kube-system\" \"pod\" \"k8s-app=metrics-server\"\n\n  elif [[ \"$KUBE_ADDON\" == \"nodelocaldns\" ]]; then\n    log::info \"[addon]\" \"download nodelocaldns manifests\"\n    local nodelocaldns_file=\"${OFFLINE_DIR}/manifests/nodelocaldns.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/kubernetes/master/cluster/addons/dns/nodelocaldns/nodelocaldns.yaml\" \"${nodelocaldns_file}\"\n  \n    command::exec \"${MGMT_NODE}\" \"\n      cluster_dns=\\$(kubectl -n kube-system get svc kube-dns -o jsonpath={.spec.clusterIP})\n      sed -i -e \\\"s#k8s.gcr.io/dns#${KUBE_IMAGE_REPO}#g\\\" \\\n             -e \\\"s#registry.k8s.io/dns#${KUBE_IMAGE_REPO}#g\\\" \\\n             -e \\\"s/__PILLAR__CLUSTER__DNS__/\\$cluster_dns/g\\\" \\\n             -e \\\"s/__PILLAR__UPSTREAM__SERVERS__/\\$cluster_dns/g\\\" \\\n             -e \\\"s/__PILLAR__LOCAL__DNS__/169.254.20.10/g\\\" \\\n             -e \\\"s/[ |,]__PILLAR__DNS__SERVER__//g\\\" \\\n             -e \\\"s/__PILLAR__DNS__DOMAIN__/$KUBE_DNSDOMAIN/g\\\" \\\n             \\\"${nodelocaldns_file}\\\"\n    \"\n    check::exit_code \"$?\" \"addon\" \"change nodelocaldns parameter\"\n    kube::apply \"${nodelocaldns_file}\"\n    kube::wait \"node-local-dns\" \"kube-system\" \"pod\" \"k8s-app=node-local-dns\"\n\n  else\n    log::warning \"[addon]\" \"No $KUBE_ADDON config.\"\n  fi\n}\n\n\nfunction add::monitor() {\n  # 添加监控组件\n  \n  if [[ \"$KUBE_MONITOR\" == \"prometheus\" ]]; then\n    log::info \"[monitor]\" \"add prometheus\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/prometheus-operator/kube-prometheus/archive/v${KUBE_PROMETHEUS_VERSION}.zip\" \"${OFFLINE_DIR}/manifests/prometheus.zip\" \"unzip\"\n   \n    log::info \"[monitor]\" \"apply prometheus manifests\"\n    command::exec \"${MGMT_NODE}\" \"\n      $(declare -f utils::retry)\n      cd \\\"${OFFLINE_DIR}/manifests/kube-prometheus-${KUBE_PROMETHEUS_VERSION}\\\" \\\n      && sed -i -e \\\"s#registry.k8s.io/prometheus-adapter#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#registry.k8s.io/kube-state-metrics#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/brancz#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/prometheus-operator#${KUBE_IMAGE_REPO}#g\\\" \\\n                -e \\\"s#quay.io/prometheus#${KUBE_IMAGE_REPO}#g\\\" \\\n                   manifests/*.yaml  \\\n      && utils::retry 6 kubectl apply --server-side --wait=true --timeout=10s -f manifests/setup/ \\\n      && until kubectl get servicemonitors --all-namespaces ; do date; sleep 1; echo ''; done \\\n      && utils::retry 6 kubectl apply --wait=true --timeout=10s -f manifests/\n    \"\n    check::exit_code \"$?\" \"apply\" \"add prometheus\"\n    kube::wait \"prometheus\" \"monitoring\" \"pods --all\"\n\n    kube::apply \"controller-manager and scheduler prometheus discovery service\" \"\n---\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: kube-system\n  name: kube-scheduler-prometheus-discovery\n  labels:\n    k8s-app: kube-scheduler\nspec:\n  selector:\n    component: kube-scheduler\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: https-metrics\n    port: 10259\n    targetPort: 10259\n    protocol: TCP\n---\napiVersion: v1\nkind: Service\nmetadata:\n  namespace: kube-system\n  name: kube-controller-manager-prometheus-discovery\n  labels:\n    k8s-app: kube-controller-manager\nspec:\n  selector:\n    component: kube-controller-manager\n  type: ClusterIP\n  clusterIP: None\n  ports:\n  - name: https-metrics\n    port: 10257\n    targetPort: 10257\n    protocol: TCP\n    \"\n    \n    log::info \"[monitor]\" \"add prometheus ingress\"\n    kube::apply \"prometheus ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: grafana\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: grafana.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: grafana\n            port:\n              number: 3000\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: prometheus\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: prometheus.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: prometheus-k8s\n            port:\n              number: 9090\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: alertmanager\n  namespace: monitoring\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: alertmanager.monitoring.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: alertmanager-main\n            port:\n              number: 9093\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:grafana.monitoring.cluster.local' http://${INGRESS_CONN}; auth: admin/admin\"\n      log::access \"[ingress]\" \"curl -H 'Host:prometheus.monitoring.cluster.local' http://${INGRESS_CONN}\"\n      log::access \"[ingress]\" \"curl -H 'Host:alertmanager.monitoring.cluster.local' http://${INGRESS_CONN}\"\n    fi\n    \n  else\n    log::warning \"[addon]\" \"No $KUBE_MONITOR config.\"\n  fi\n}\n\n\nfunction add::log() {\n  # 添加log组件\n\n  if [[ \"$KUBE_LOG\" == \"elasticsearch\" ]]; then\n    log::info \"[log]\" \"add elasticsearch\"\n    kube::apply \"elasticsearch\" \"\n---\nkind: Namespace\napiVersion: v1\nmetadata:\n  name: kube-logging\n---\nkind: Service\napiVersion: v1\nmetadata:\n  name: elasticsearch\n  namespace: kube-logging\n  labels:\n    app: elasticsearch\nspec:\n  selector:\n    app: elasticsearch\n  clusterIP: None\n  ports:\n    - port: 9200\n      name: rest\n    - port: 9300\n      name: inter-node\n---\napiVersion: apps/v1\nkind: StatefulSet\nmetadata:\n  name: es-cluster\n  namespace: kube-logging\nspec:\n  serviceName: elasticsearch\n  replicas: 3\n  selector:\n    matchLabels:\n      app: elasticsearch\n  template:\n    metadata:\n      labels:\n        app: elasticsearch\n    spec:\n      affinity:\n        podAntiAffinity:\n          preferredDuringSchedulingIgnoredDuringExecution:\n          - weight: 100\n            podAffinityTerm:\n              labelSelector:\n                matchExpressions:\n                - {key: app,operator: In,values: [\\\"elasticsearch\\\"]}\n              topologyKey: kubernetes.io/hostname\n      containers:\n      - name: elasticsearch\n        image: ${KUBE_IMAGE_REPO}/elasticsearch:${ELASTICSEARCH_VERSION}\n        resources:\n            limits:\n              cpu: 1000m\n            requests:\n              cpu: 100m\n        ports:\n        - containerPort: 9200\n          name: rest\n          protocol: TCP\n        - containerPort: 9300\n          name: inter-node\n          protocol: TCP\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n        env:\n          - name: cluster.name\n            value: k8s-logs\n          - name: node.name\n            valueFrom:\n              fieldRef:\n                fieldPath: metadata.name\n          - name: discovery.seed_hosts\n            value: 'es-cluster-0.elasticsearch,es-cluster-1.elasticsearch,es-cluster-2.elasticsearch'\n          - name: cluster.initial_master_nodes\n            value: 'es-cluster-0,es-cluster-1,es-cluster-2'\n          - name: xpack.security.enabled\n            value: 'false'\n          - name: ES_JAVA_OPTS\n            value: '-Xms512m -Xmx512m'\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n      initContainers:\n      - name: increase-vm-max-map\n        image: alpine:3.9\n        command: ['sysctl', '-w', 'vm.max_map_count=262144']\n        securityContext:\n          privileged: true\n      - name: increase-fd-ulimit\n        image: alpine:3.9\n        command: ['sh', '-c', 'ulimit -n 65536']\n        securityContext:\n          privileged: true\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: elasticsearch\n  namespace: kube-logging\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: elasticsearch.logging.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: elasticsearch\n            port:\n              number: 9200\n---\napiVersion: v1\nkind: Service\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  labels:\n    app: kibana\nspec:\n  ports:\n  - port: 5601\n  selector:\n    app: kibana\n\n---\napiVersion: apps/v1\nkind: Deployment\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  labels:\n    app: kibana\nspec:\n  replicas: 1\n  selector:\n    matchLabels:\n      app: kibana\n  template:\n    metadata:\n      labels:\n        app: kibana\n    spec:\n      containers:\n      - name: kibana\n        image: ${KUBE_IMAGE_REPO}/kibana:${ELASTICSEARCH_VERSION}\n        resources:\n          limits:\n            cpu: 1000m\n          requests:\n            cpu: 100m\n        env:\n          - name: ELASTICSEARCH_URL\n            value: http://elasticsearch:9200\n        ports:\n        - containerPort: 5601\n        volumeMounts:\n        - name: localtime\n          mountPath: /etc/localtime\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: kibana\n  namespace: kube-logging\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\nspec:\n  rules:\n  - host: kibana.logging.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kibana\n            port:\n              number: 5601\n---\napiVersion: v1\nkind: ServiceAccount\nmetadata:\n  name: fluentd\n  namespace: kube-logging\n  labels:\n    app: fluentd\n---\napiVersion: rbac.authorization.k8s.io/v1\nkind: ClusterRole\nmetadata:\n  name: fluentd\n  labels:\n    app: fluentd\nrules:\n- apiGroups:\n  - ''\n  resources:\n  - pods\n  - namespaces\n  verbs:\n  - get\n  - list\n  - watch\n---\nkind: ClusterRoleBinding\napiVersion: rbac.authorization.k8s.io/v1\nmetadata:\n  name: fluentd\nroleRef:\n  kind: ClusterRole\n  name: fluentd\n  apiGroup: rbac.authorization.k8s.io\nsubjects:\n- kind: ServiceAccount\n  name: fluentd\n  namespace: kube-logging\n---\napiVersion: apps/v1\nkind: DaemonSet\nmetadata:\n  name: fluentd\n  namespace: kube-logging\n  labels:\n    app: fluentd\nspec:\n  selector:\n    matchLabels:\n      app: fluentd\n  template:\n    metadata:\n      labels:\n        app: fluentd\n    spec:\n      serviceAccount: fluentd\n      serviceAccountName: fluentd\n      tolerations:\n      - key: node-role.kubernetes.io/master\n        effect: NoSchedule\n      - key: node-role.kubernetes.io/control-plane\n        effect: NoSchedule\n      containers:\n      - name: fluentd\n        image: fluent/fluentd-kubernetes-daemonset:v1.16-debian-elasticsearch8-1\n        env:\n        - name: K8S_NODE_NAME\n          valueFrom:\n            fieldRef:\n              apiVersion: v1\n              fieldPath: spec.nodeName\n        - name: FLUENT_ELASTICSEARCH_HOST\n          value: elasticsearch.kube-logging.svc.${KUBE_DNSDOMAIN}\n        - name: FLUENT_ELASTICSEARCH_PORT\n          value: '9200'\n        - name: FLUENT_ELASTICSEARCH_SCHEME\n          value: http\n        - name: FLUENTD_SYSTEMD_CONF\n          value: disable\n        - name: FLUENT_CONTAINER_TAIL_EXCLUDE_PATH\n          value: /var/log/containers/fluent*\n        - name: FLUENT_CONTAINER_TAIL_PARSER_TYPE\n          value: cri\n        - name: FLUENT_CONTAINER_TAIL_PARSER_TIME_FORMAT\n          value: '%Y-%m-%dT%H:%M:%S.%L%z'\n        resources:\n          limits:\n            memory: 512Mi\n          requests:\n            cpu: 100m\n            memory: 200Mi\n        volumeMounts:\n        - name: varlog\n          mountPath: /var/log\n        - name: localtime\n          mountPath: /etc/localtime\n      terminationGracePeriodSeconds: 30\n      volumes:\n      - name: localtime\n        hostPath:\n          path: /usr/share/zoneinfo/Asia/Shanghai\n      - name: varlog\n        hostPath:\n          path: /var/log\n    \" \n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      kube::wait \"elasticsearch\" \"kube-logging\" \"pods --all\"\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:kibana.logging.cluster.local' http://${INGRESS_CONN}\"\n      log::access \"[ingress]\" \"curl -H 'Host:elasticsearch.logging.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[log]\" \"No $KUBE_LOG config.\"\n  fi\n}\n\n\nfunction add::storage() {\n  # 添加存储 \n\n  if [[ \"$KUBE_STORAGE\" == \"rook\" ]]; then\n\n    log::info \"[storage]\" \"add rook\"\n    utils::download_file \"${GITHUB_PROXY}https://github.com/rook/rook/archive/v${ROOK_VERSION}.zip\" \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}.zip\" \"unzip\"\n\n    command::exec \"${MGMT_NODE}\" \"\n      cd '${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples' \\\n      && sed -i -e 's/# ROOK_CSI_\\\\(.*\\\\)_IMAGE/ROOK_CSI_\\\\1_IMAGE/g' \\\n                -e 's#\\\\k8s\\\\.gcr\\\\.io/sig-storage#${KUBE_IMAGE_REPO}#g' \\\n                -e 's#\\\\quay\\\\.io/cephcsi#${KUBE_IMAGE_REPO}#g' operator.yaml \\\n      && sed -i 's#quay.io/ceph#${KUBE_IMAGE_REPO}#g' cluster.yaml\n    \" \n    check::exit_code \"$?\" \"storage\" \"image using proxy\"\n\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/crds.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/common.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/operator.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/rook-${ROOK_VERSION}/deploy/examples/cluster.yaml\"\n\n  elif [[ \"$KUBE_STORAGE\" == \"longhorn\" ]]; then\n    log::info \"[storage]\" \"add longhorn\"\n    log::info \"[storage]\" \"get cluster node hosts\"\n    if [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl get node -o jsonpath='{\\$.items[*].status.addresses[?(@.type==\\\"InternalIP\\\")].address}'\n      \"\n      get::command_output \"cluster_nodes\" \"$?\" \"exit\"\n    else\n      cluster_nodes=\"${MASTER_NODES} ${WORKER_NODES}\"\n    fi\n    for host in ${cluster_nodes:-}\n    do\n      log::info \"[storage]\"  \"${host}: install iscsi-initiator-utils\"\n      command::exec \"${host}\" \"\n        apt-get install -y open-iscsi\n      \"\n      check::exit_code \"$?\" \"storage\" \"${host}: install iscsi-initiator-utils\" \"exit\"\n    done\n    \n    local longhorn_file=\"${OFFLINE_DIR}/manifests/longhorn.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/longhorn/longhorn/v${LONGHORN_VERSION}/deploy/longhorn.yaml\" \"${longhorn_file}\"\n\n    command::exec \"${MGMT_NODE}\" \"\n      sed -i 's#numberOfReplicas: \\\"3\\\"#numberOfReplicas: \\\"1\\\"#g' \\\"${longhorn_file}\\\"\n    \"\n    check::exit_code \"$?\" \"storage\" \"set longhorn numberOfReplicas is 1\"\n\n    kube::apply \"${longhorn_file}\"\n    kube::wait \"longhorn\" \"longhorn-system\" \"pods --all\"\n    \n    log::info \"[storage]\"  \"set longhorn is default storage class\"\n    command::exec \"${MGMT_NODE}\" \"\n      default_class=\\\"\\$(kubectl get storageclass -A -o jsonpath='{.items[?(@.metadata.annotations.storageclass\\\\.kubernetes\\\\.io/is-default-class==\\\\\\\"true\\\\\\\")].metadata.name}')\\\"\n      if [ \\\"\\${default_class:-}\\\" != \\\"\\\" ]; then\n         kubectl patch storageclass \\${default_class} -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"false\\\"}}}'\n      fi\n      kubectl patch storageclass longhorn -p '{\\\"metadata\\\": {\\\"annotations\\\":{\\\"storageclass.kubernetes.io/is-default-class\\\":\\\"true\\\"}}}'\n    \"\n    check::exit_code \"$?\" \"storage\" \"set longhorn is default storage class\"\n    \n    kube::apply \"longhorn ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  name: longhorn-ingress\n  namespace: longhorn-system\nspec:\n  rules:\n  - host: longhorn.storage.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: longhorn-frontend\n            port:\n              number: 80\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn\n      log::access \"[ingress]\" \"curl -H 'Host:longhorn.storage.cluster.local' http://${INGRESS_CONN}\"\n    fi\n  else\n    log::warning \"[storage]\" \"No $KUBE_STORAGE config.\"\n  fi\n}\n\n\nfunction add::ui() {\n  # 添加用户界面\n\n  if [[ \"$KUBE_UI\" == \"dashboard\" ]]; then\n    log::info \"[ui]\" \"add kubernetes dashboard\"\n    local dashboard_file=\"${OFFLINE_DIR}/manifests/kubernetes-dashboard.yml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubernetes/dashboard/v${KUBERNETES_DASHBOARD_VERSION}/aio/deploy/recommended.yaml\" \"${dashboard_file}\"\n    kube::apply \"${dashboard_file}\"\n    kube::wait \"kubernetes-dashboard\" \"kubernetes-dashboard\" \"pod\" \"k8s-app=kubernetes-dashboard\"\n    kube::apply \"kubernetes dashboard ingress\" \"\n---\napiVersion: networking.k8s.io/v1\nkind: Ingress\nmetadata:\n  annotations:\n    kubernetes.io/ingress.class: ${KUBE_INGRESS}\n$( if [[ $KUBE_INGRESS == \"nginx\" ]]; then\necho \"\"\"\n    nginx.ingress.kubernetes.io/secure-backends: 'true'\n    nginx.ingress.kubernetes.io/backend-protocol: 'HTTPS'\n    nginx.ingress.kubernetes.io/ssl-passthrough: 'true'\n\"\"\";\nelif [[ $KUBE_INGRESS == \"traefik\" ]]; then \necho \"\"\"\n    traefik.ingress.kubernetes.io/frontend-entry-points: https\n    traefik.ingress.kubernetes.io/auth-type: 'basic'\n    traefik.ingress.kubernetes.io/auth-secret: 'kubernetes-dashboard-auth'\n    traefik.ingress.kubernetes.io/ssl-redirect: 'true'\n\"\"\";\nfi\n)\n  name: kubernetes-dashboard\n  namespace: kubernetes-dashboard \nspec:\n$( if [[ $KUBE_INGRESS == \"nginx\" ]]; then\necho \"\"\"\n  tls:\n  - hosts:\n    - kubernetes-dashboard.cluster.local\n    secretName: kubernetes-dashboard-certs\n\"\"\"\nelif [[ $KUBE_INGRESS == \"traefik\" ]]; then \necho \"\"\"\n\"\"\"\nfi\n)\n  rules:\n  - host: kubernetes-dashboard.cluster.local\n    http:\n      paths:\n      - path: /\n        pathType: Prefix\n        backend:\n          service:\n            name: kubernetes-dashboard\n            port:\n              number: 443\n    \"\n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      get::ingress_conn \"443\"\n      log::access \"[ingress]\" \"curl --insecure -H 'Host:kubernetes-dashboard.cluster.local' https://${INGRESS_CONN}\"\n\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl create serviceaccount kubernetes-dashboard-admin-sa -n kubernetes-dashboard\n        kubectl apply -f - <<EOF\napiVersion: v1\nkind: Secret\nmetadata:\n  name: kubernetes-dashboard-admin-sa-token\n  namespace: kubernetes-dashboard\n  annotations:\n    kubernetes.io/service-account.name: kubernetes-dashboard-admin-sa\ntype: kubernetes.io/service-account-token\nEOF\n        kubectl create clusterrolebinding kubernetes-dashboard-admin-sa --clusterrole=cluster-admin --serviceaccount=kubernetes-dashboard:kubernetes-dashboard-admin-sa -n kubernetes-dashboard\n      \"\n      local s=\"$?\"\n      check::exit_code \"$s\" \"ui\" \"create kubernetes dashboard admin service account\"\n      local dashboard_token=\"\"\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl describe secrets \\$(kubectl describe sa kubernetes-dashboard-admin-sa -n kubernetes-dashboard | awk '/Tokens/ {print \\$2}') -n kubernetes-dashboard | awk '/token:/{print \\$2}'\n      \"\n      get::command_output \"dashboard_token\" \"$?\"\n      [[ \"$dashboard_token\" != \"\" ]] && log::access \"[Token]\" \"${dashboard_token}\"\n    fi\n  elif [[ \"$KUBE_UI\" == \"kubesphere\" ]]; then\n    log::info \"[ui]\" \"add kubesphere\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubesphere/ks-installer/v${KUBESPHERE_VERSION}/deploy/kubesphere-installer.yaml\" \"${OFFLINE_DIR}/manifests/kubesphere-installer.yaml\"\n    utils::download_file \"${GITHUB_PROXY}https://raw.githubusercontent.com/kubesphere/ks-installer/v${KUBESPHERE_VERSION}/deploy/cluster-configuration.yaml\" \"${OFFLINE_DIR}/manifests/cluster-configuration.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/kubesphere-installer.yaml\"\n    kube::apply \"${OFFLINE_DIR}/manifests/cluster-configuration.yaml\"\n\n    sleep 60\n    kube::wait \"ks-installer\" \"kubesphere-system\" \"pods\" \"app=ks-install\"\n    command::exec \"${MGMT_NODE}\" \"\n      $(declare -f utils::retry) \n      utils::retry 10 kubectl -n kubesphere-system get pods redis-ha-server-0 \\\n        && { kubectl -n kubesphere-system get sts redis-ha-server -o yaml | sed 's#!node-role.kubernetes.io/worker#node-role.kubernetes.io/worker#g' | kubectl replace --force -f -; } \\\n        && echo not replace redis-ha-server\n      utils::retry 10 kubectl -n kubesphere-system get pods openldap-0 \\\n        && { kubectl -n kubesphere-system get sts openldap -o yaml | sed 's#!node-role.kubernetes.io/worker#node-role.kubernetes.io/worker#g' | kubectl replace --force -f -; } \\\n        && echo not replace openldap\n    \"\n    check::exit_code \"$?\" \"ui\" \"set statefulset to worker node\"\n\n    sleep 60\n    kube::wait \"kubesphere-system\" \"kubesphere-system\" \"pods --all\"\n    kube::wait \"kubesphere-controls-system\" \"kubesphere-controls-system\" \"pods --all\" \n    kube::wait \"kubesphere-monitoring-system\" \"kubesphere-monitoring-system\" \"pods --all\" \n    # shellcheck disable=SC2181\n    if [[ \"$?\" == \"0\" ]]; then\n      command::exec \"${MGMT_NODE}\" \"\n        kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address } {end}' | awk '{print \\$1}'\n      \"\n      get::command_output \"node_ip\" \"$?\"\n      log::access \"[service]\" \"curl http://${node_ip:-NodeIP}:30880;  auth: admin/P@88w0rd\"\n    fi\n  else\n    log::warning \"[ui]\" \"No $KUBE_UI config.\"\n  fi\n}\n\n\nfunction add::ops() {\n  # 运维操作\n   \n  local master_num\n  master_num=$(awk '{print NF}' <<< \"${MASTER_NODES}\")\n  \n  log::info \"[ops]\" \"add anti-affinity strategy to coredns\"\n  command::exec \"${MGMT_NODE}\" \"\"\"\n    kubectl -n kube-system patch deployment coredns --patch '{\\\"spec\\\": {\\\"template\\\": {\\\"spec\\\": {\\\"affinity\\\":{\\\"podAntiAffinity\\\":{\\\"preferredDuringSchedulingIgnoredDuringExecution\\\":[{\\\"weight\\\":100,\\\"podAffinityTerm\\\":{\\\"labelSelector\\\":{\\\"matchExpressions\\\":[{\\\"key\\\":\\\"k8s-app\\\",\\\"operator\\\":\\\"In\\\",\\\"values\\\":[\\\"kube-dns\\\"]}]},\\\"topologyKey\\\":\\\"kubernetes.io/hostname\\\"}}]}}}}}}' --record\n  \"\"\"\n  check::exit_code \"$?\" \"ops\" \"add anti-affinity strategy to coredns\"\n\n  log::info \"[ops]\" \"add etcd snapshot cronjob\"\n  command::exec \"${MGMT_NODE}\" \"\n    kubeadm config images list --config=/etc/kubernetes/kubeadmcfg.yaml 2>/dev/null | grep etcd:\n  \"\n  get::command_output \"etcd_image\" \"$?\"\n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node --selector='!node-role.kubernetes.io/worker' --no-headers | wc -l\n  \"\n  get::command_output \"master_num\" \"$?\"\n\n  [[ \"${master_num:-0}\" == \"0\" ]] && master_num=1\n  kube::apply \"etcd-snapshot\" \"\"\"\n---\napiVersion: batch/v1\nkind: CronJob\nmetadata:\n  name: etcd-snapshot\n  namespace: kube-system\nspec:\n  schedule: '0 */6 * * *'\n  successfulJobsHistoryLimit: 3\n  suspend: false\n  concurrencyPolicy: Allow\n  failedJobsHistoryLimit: 3\n  jobTemplate:\n    spec:\n      backoffLimit: 6\n      parallelism: ${master_num}\n      completions: ${master_num}\n      template:\n        metadata:\n          labels:\n            app: etcd-snapshot\n        spec:\n          affinity:\n            podAntiAffinity:\n              requiredDuringSchedulingIgnoredDuringExecution:\n              - labelSelector:\n                  matchExpressions:\n                  - key: app\n                    operator: In\n                    values:\n                    - etcd-snapshot\n                topologyKey: 'kubernetes.io/hostname'\n          containers:\n          - name: etcd-snapshot\n            image: ${etcd_image:-${KUBE_IMAGE_REPO}/etcd:3.4.13-0}\n            imagePullPolicy: IfNotPresent\n            args:\n            - -c\n            - etcdctl --endpoints=https://127.0.0.1:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt\n              --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\n              snapshot save /backup/etcd-snapshot-\\\\\\\\\\\\\\$(date +%Y-%m-%d_%H:%M:%S_%Z).db\n              && echo 'delete old backups' && { find /backup -type f -mtime +30 -exec rm -fv {} \\\\; || echo error; }\n            command:\n            - /usr/bin/bash\n            env:\n            - name: ETCDCTL_API\n              value: '3'\n            resources: {}\n            terminationMessagePath: /dev/termination-log\n            terminationMessagePolicy: File\n            volumeMounts:\n            - name: etcd-certs\n              mountPath: /etc/kubernetes/pki/etcd\n              readOnly: true\n            - name: backup\n              mountPath: /backup\n            - name: etc\n              mountPath: /etc\n            - name: bin\n              mountPath: /usr/bin\n            - name: lib\n              mountPath: /lib\n            - name: lib64\n              mountPath: /lib64\n          dnsPolicy: ClusterFirst\n          hostNetwork: true\n          nodeSelector:\n            node-role.kubernetes.io/control-plane: ''\n          tolerations:\n          - effect: NoSchedule\n            operator: Exists\n          restartPolicy: OnFailure\n          schedulerName: default-scheduler\n          securityContext: {}\n          terminationGracePeriodSeconds: 30\n          volumes:\n          - name: etcd-certs\n            hostPath:\n              path: /etc/kubernetes/pki/etcd\n              type: DirectoryOrCreate\n          - name: backup\n            hostPath:\n              path: /var/lib/etcd/backups\n              type: DirectoryOrCreate\n          - name: etc\n            hostPath:\n              path: /etc\n          - name: bin\n            hostPath:\n              path: /usr/bin\n          - name: lib\n            hostPath:\n              path: /lib\n          - name: lib64\n            hostPath:\n              path: /lib64\n\"\"\"\n  # shellcheck disable=SC2181\n  [[ \"$?\" == \"0\" ]] && log::access \"[ops]\" \"etcd backup directory: /var/lib/etcd/backups\"\n  command::exec \"${MGMT_NODE}\" \"\n    jobname=\\\"etcd-snapshot-$(date +%s)\\\"\n    kubectl create job --from=cronjob/etcd-snapshot \\${jobname} -n kube-system && \\\n    kubectl wait --for=condition=complete job/\\${jobname} -n kube-system\n  \"\n  check::exit_code \"$?\" \"ops\" \"trigger etcd backup\"\n}\n\n\nfunction reset::node() {\n  # 重置节点\n\n  local host=$1\n  log::info \"[reset]\" \"node $host\"\n  command::exec \"${host}\" \"\n    set +ex\n    cri_socket=\\\"\\\"\n    [ -S /var/run/crio/crio.sock ] && cri_socket=\\\"--cri-socket /var/run/crio/crio.sock\\\"\n    [ -S /run/containerd/containerd.sock ] && cri_socket=\\\"--cri-socket /run/containerd/containerd.sock\\\"\n    kubeadm reset -f \\$cri_socket\n    [ -f \\\"\\$(which kubelet)\\\" ] && { systemctl stop kubelet; find /var/lib/kubelet | xargs -n 1 findmnt -n -o TARGET -T | sort | uniq | xargs -r umount -v; apt remove -y kubeadm kubelet kubectl; }\n    [ -d /etc/kubernetes ] && rm -rf /etc/kubernetes/* /var/lib/kubelet/* /var/lib/etcd/* \\$HOME/.kube /etc/cni/net.d/* /var/lib/dockershim/* /var/lib/cni/* /var/run/kubernetes/*\n\n    [ -f \\\"\\$(which docker)\\\" ] && { docker rm -f -v \\$(docker ps | grep kube | awk '{print \\$1}'); systemctl stop docker; rm -rf \\$HOME/.docker /etc/docker/* /var/lib/docker/*; apt remove -y docker; }\n    [ -f \\\"\\$(which containerd)\\\" ] && { crictl rm \\$(crictl ps -a -q); systemctl stop containerd; rm -rf /etc/containerd/* /var/lib/containerd/*; apt remove -y containerd.io; }\n    [ -f \\\"\\$(which crio)\\\" ] && { crictl rm \\$(crictl ps -a -q); systemctl stop crio; rm -rf /etc/crictl.yaml /etc/crio/* /var/run/crio/*; apt remove -y cri-o; }\n    [ -f \\\"\\$(which runc)\\\" ] && { find /run/containers/ /var/lib/containers/ | xargs -n 1 findmnt -n -o TARGET -T | sort | uniq | xargs -r umount -v; rm -rf /var/lib/containers/* /var/run/containers/*; apt remove -y runc; }\n    [ -f \\\"\\$(which haproxy)\\\" ] && { systemctl stop haproxy; rm -rf /etc/haproxy/* /etc/rsyslog.d/haproxy.conf; yum remove -y haproxy; }\n\n    sed -i -e \\\"/$KUBE_APISERVER/d\\\" -e '/-worker-/d' -e '/-master-/d' /etc/hosts\n    sed -i '/## Kainstall managed start/,/## Kainstall managed end/d' /etc/security/limits.conf /etc/systemd/system.conf ~/.bashrc /etc/audit/rules.d/audit.rules\n    \n    [ -d /var/lib/elasticsearch ] && rm -rf /var/lib/elasticsearch/*\n    [ -d /var/lib/longhorn ] &&  rm -rf /var/lib/longhorn/*\n    [ -d \\\"${OFFLINE_DIR:-/tmp/abc}\\\" ] && rm -rf \\\"${OFFLINE_DIR:-/tmp/abc}\\\"\n\n    ipvsadm --clear\n    iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X\n    for int in kube-ipvs0 cni0 docker0 dummy0 flannel.1 cilium_host cilium_net cilium_vxlan lxc_health nodelocaldns \n    do\n      [ -d /sys/class/net/\\${int} ] && ip link delete \\${int}\n    done\n    modprobe -r ipip\n    echo done.\n  \"\n  check::exit_code \"$?\" \"reset\" \"$host: reset\"\n}\n\n\nfunction reset::cluster() {\n  # 重置所有节点\n  \n  local all_node=\"\"\n  \n  command::exec \"${MGMT_NODE}\" \"\n    kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {end}'\n  \"\n  get::command_output \"all_node\" \"$?\"\n  \n  all_node=$(echo \"${WORKER_NODES} ${MASTER_NODES} ${all_node}\" | awk '{for (i=1;i<=NF;i++) if (!a[$i]++) printf(\"%s%s\",$i,FS)}')\n\n  for host in $all_node\n  do\n    reset::node \"$host\"\n  done\n\n}\n\n\nfunction offline::load() {\n  # 节点加载离线包\n \n  local role=\"${1:-}\"\n  local hosts=\"\"\n  \n  if [[ \"${role}\" == \"master\" ]]; then\n     hosts=\"${MASTER_NODES}\"\n  elif [[ \"${role}\" == \"worker\" ]]; then\n     hosts=\"${WORKER_NODES}\"\n  fi\n \n  for host in ${hosts}\n  do\n    log::info \"[offline]\" \"${role} ${host}: load offline file\"\n    command::exec \"${host}\"  \"[[ ! -d \\\"${OFFLINE_DIR}\\\" ]] && { mkdir -pv \\\"${OFFLINE_DIR}\\\"; chmod 777 \\\"${OFFLINE_DIR}\\\"; } ||:\"\n    check::exit_code \"$?\" \"offline\" \"$host: mkdir offline dir\" \"exit\"\n\n    if [[ \"${UPGRADE_KERNEL_TAG:-}\" == \"1\" ]]; then\n      command::scp \"${host}\" \"${TMP_DIR}/packages/kernel/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp kernel file to $host\" \"exit\"\n    else\n      log::info \"[offline]\" \"${role} ${host}: copy offline file\"\n      command::scp \"${host}\" \"${TMP_DIR}/packages/kubeadm/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp kube file to $host\" \"exit\"\n      command::scp \"${host}\" \"${TMP_DIR}/packages/all/*\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp all file to $host\" \"exit\"\n\n      if [[ \"${role}\" == \"worker\" ]]; then\n        command::scp \"${host}\" \"${TMP_DIR}/packages/worker/*\" \"${OFFLINE_DIR}\"\n        check::exit_code \"$?\" \"offline\" \"scp worker file to $host\" \"exit\"\n      fi \n\n      command::scp \"${host}\" \"${TMP_DIR}/images/${role}.tgz\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp ${role} images to $host\" \"exit\"\n      command::scp \"${host}\" \"${TMP_DIR}/images/all.tgz\" \"${OFFLINE_DIR}\"\n      check::exit_code \"$?\" \"offline\" \"scp all images to $host\" \"exit\"\n    fi\n\n    \n    log::info \"[offline]\" \"${role} ${host}: install package\"\n    command::exec \"${host}\" \"dpkg --force-all -i ${OFFLINE_DIR}/*.deb; DEBIAN_FRONTEND=noninteractive apt-get install -f -q -y\"\n    check::exit_code \"$?\" \"offline\" \"${role} ${host}: install package\" \"exit\"\n  \n    if [[ \"${UPGRADE_KERNEL_TAG:-}\" != \"1\" ]]; then\n      command::exec \"${host}\" \"\n        set -e\n        for target in firewalld python-firewall firewalld-filesystem iptables; do\n          systemctl stop \\$target &>/dev/null || true\n          systemctl disable \\$target &>/dev/null || true\n        done\n        systemctl start docker && \\\n        cd ${OFFLINE_DIR} && \\\n        gzip -d -c ${1}.tgz | docker load && gzip -d -c all.tgz | docker load\n      \"\n      check::exit_code \"$?\" \"offline\" \"$host: load images\" \"exit\"  \n    fi\n    command::exec \"${host}\" \"rm -rf ${OFFLINE_DIR:-/tmp/abc}\"\n    check::exit_code \"$?\" \"offline\" \"$host: clean offline file\"  \n  done\n\n  command::scp \"${MGMT_NODE}\" \"${TMP_DIR}/manifests\" \"${OFFLINE_DIR}\"\n  check::exit_code \"$?\" \"offline\" \"scp manifests file to ${MGMT_NODE}\" \"exit\"\n\n  command::scp \"${MGMT_NODE}\" \"${TMP_DIR}/bins\" \"${OFFLINE_DIR}\"\n  check::exit_code \"$?\" \"offline\" \"scp bins file to ${MGMT_NODE}\" \"exit\"\n}\n\n\nfunction offline::cluster() {\n  # 集群节点加载离线包\n\n  [ ! -f \"${OFFLINE_FILE}\" ] && { log::error \"[offline]\" \"not found ${OFFLINE_FILE}\" ; exit 1; }\n\n  log::info \"[offline]\" \"Unzip offline package on local.\"\n  tar zxf \"${OFFLINE_FILE}\"  -C \"${TMP_DIR}/\"\n  check::exit_code \"$?\" \"offline\"  \"Unzip offline package\"\n \n  offline::load \"master\"\n  offline::load \"worker\"\n}\n\n\nfunction init::cluster() {\n  # 初始化集群\n\n  MGMT_NODE=$(echo \"${MASTER_NODES}\" | awk '{print $1}')\n\n  # 加载离线包\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && offline::cluster\n  \n  # 1. 初始化节点\n  init::node\n  # 2. 安装包\n  install::package\n  # 3. 初始化kubeadm\n  kubeadm::init\n  # 4. 加入集群\n  kubeadm::join\n  # 5. 添加network\n  add::network\n  # 6. 安装addon\n  add::addon\n  # 7. 添加ingress\n  add::ingress\n  # 8. 添加storage\n  [[ \"${STORAGE_TAG:-}\" == \"1\" ]] && add::storage\n  # 9. 添加web ui\n  add::ui\n  # 10. 添加monitor\n  [[ \"${MONITOR_TAG:-}\" == \"1\" ]] && add::monitor\n  # 11. 添加log\n  [[ \"${LOG_TAG:-}\" == \"1\" ]] && add::log\n  # 12. 运维操作\n  add::ops\n  # 13. 查看集群状态\n  kube::status\n}\n\n\nfunction add::node() {\n  # 添加节点\n  \n  # 加载离线包\n  [[ \"${OFFLINE_TAG:-}\" == \"1\" ]] && offline::cluster\n\n  # KUBE_VERSION未指定时，获取集群的版本\n  if [[ \"${KUBE_VERSION}\" == \"\" || \"${KUBE_VERSION}\" == \"latest\" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node --selector='!node-role.kubernetes.io/worker' -o jsonpath='{range.items[*]}{.status.nodeInfo.kubeletVersion } {end}' | awk -F'v| ' '{print \\$2}'\n  \"\n    get::command_output \"KUBE_VERSION\" \"$?\" \"exit\"\n  fi\n\n  # 1. 初始化节点\n  init::add_node\n  # 2. 安装包\n  install::package\n  # 3. 加入集群\n  kubeadm::join\n  # 4. haproxy添加apiserver\n  config::haproxy_backend \"add\"\n  # 5. 更新 etcd snapshot 副本\n  config::etcd_snapshot\n  # 6. 查看集群状态\n  kube::status\n}\n\n\nfunction del::node() {\n  # 删除节点\n \n  config::haproxy_backend \"remove\"\n\n  local cluster_nodes=\"\"\n  local del_hosts_cmd=\"\"\n  command::exec \"${MGMT_NODE}\" \"\n     kubectl get node -o jsonpath='{range.items[*]}{.status.addresses[?(@.type==\\\"InternalIP\\\")].address} {.metadata.name }\\\\n{end}'\n  \"\n  get::command_output \"cluster_nodes\" \"$?\" exit\n\n  for host in $MASTER_NODES\n  do\n     command::exec \"${MGMT_NODE}\" \"\n       etcd_pod=\\$(kubectl -n kube-system get pods -l component=etcd --field-selector=status.phase=Running -o jsonpath='{\\$.items[0].metadata.name}')\n       etcd_node=\\$(kubectl -n kube-system exec \\$etcd_pod -- sh -c \\\"export ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key ETCDCTL_ENDPOINTS=https://127.0.0.1:2379; etcdctl member list\\\"| grep $host | awk -F, '{print \\$1}')\n       echo \\\"\\$etcd_pod \\$etcd_node\\\"\n       kubectl -n kube-system exec \\$etcd_pod -- sh -c \\\"export ETCDCTL_API=3 ETCDCTL_CACERT=/etc/kubernetes/pki/etcd/ca.crt ETCDCTL_CERT=/etc/kubernetes/pki/etcd/server.crt ETCDCTL_KEY=/etc/kubernetes/pki/etcd/server.key ETCDCTL_ENDPOINTS=https://127.0.0.1:2379; etcdctl member remove \\$etcd_node; etcdctl member list\\\"\n     \"\n     check::exit_code \"$?\" \"del\" \"remove $host etcd member\"\n  done\n\n  for host in $MASTER_NODES $WORKER_NODES\n  do\n    log::info \"[del]\" \"node $host\"\n\n    local node_name; node_name=$(echo -ne \"${cluster_nodes}\" | grep \"${host}\" | awk '{print $2}')\n    if [[ \"${node_name}\" == \"\" ]]; then\n      log::warning \"[del]\" \"node $host not found.\"\n      read -r -t 10 -n 1 -p \"Do you need to reset the node (y/n)? \" answer\n      [[ -z \"$answer\" || \"$answer\" != \"y\" ]] && exit || echo\n    else\n      log::info \"[del]\" \"drain $host\"\n      command::exec \"${MGMT_NODE}\" \"kubectl drain $node_name --force --ignore-daemonsets --delete-local-data\"\n      check::exit_code \"$?\" \"del\" \"$host: drain\"\n\n      log::info \"[del]\" \"delete node $host\"\n      command::exec \"${MGMT_NODE}\" \"kubectl delete node $node_name\"\n      check::exit_code \"$?\" \"del\" \"$host: delete\"\n      sleep 3\n    fi\n    reset::node \"$host\"\n    del_hosts_cmd=\"${del_hosts_cmd}\\nsed -i \"/$host/d\" /etc/hosts\"\n  done\n\n  for host in $(echo -ne \"${cluster_nodes}\" | awk '{print $1}')\n  do\n     log::info \"[del]\" \"$host: remove del node hostname resolution\"\n     command::exec \"${host}\" \"\n       $(echo -ne \"${del_hosts_cmd}\")\n     \"\n     check::exit_code \"$?\" \"del\" \"remove del node hostname resolution\"\n  done\n  [ \"$MASTER_NODES\" != \"\" ] && config::etcd_snapshot\n  kube::status\n}\n\n\nfunction upgrade::cluster() {\n  # 升级集群\n\n  log::info \"[upgrade]\" \"upgrade to $KUBE_VERSION\"\n  log::info \"[upgrade]\" \"backup cluster\"\n  add::ops\n\n  local stable_version=\"2\"\n  command::exec \"127.0.0.1\" \"wget https://storage.googleapis.com/kubernetes-release/release/stable.txt -q -O -\"\n  get::command_output \"stable_version\" \"$?\" && stable_version=\"${stable_version#v}\"\n\n  local node_hosts=\"$MASTER_NODES $WORKER_NODES\"\n  if [[ \"$node_hosts\" == \" \" ]]; then\n    command::exec \"${MGMT_NODE}\" \"\n      kubectl get node -o jsonpath='{range.items[*]}{.metadata.name } {end}'\n    \"\n    get::command_output \"node_hosts\" \"$?\" exit\n  fi\n\n  local skip_plan=${SKIP_UPGRADE_PLAN,,}\n  for host in ${node_hosts}\n  do\n    log::info \"[upgrade]\" \"node: $host\"\n    local local_version=\"\"\n    command::exec \"${host}\" \"kubectl version --client --output=yaml | awk '/gitVersion:/ {print \\$2}'\"\n    get::command_output \"local_version\" \"$?\" && local_version=\"${local_version#v}\"\n\n    if [[ \"${KUBE_VERSION}\" != \"latest\" ]]; then\n      if [[ \"${KUBE_VERSION}\" == \"${local_version}\" ]];then\n        log::warning \"[check]\" \"The specified version(${KUBE_VERSION}) is consistent with the local version(${local_version})!\"\n        continue\n      fi\n\n      if [[ $(utils::version_to_number \"$KUBE_VERSION\") -lt $(utils::version_to_number \"${local_version}\") ]];then\n        log::warning \"[check]\" \"The specified version($KUBE_VERSION) is less than the local version(${local_version})!\"\n        continue\n      fi\n\n      if [[ $(utils::version_to_number \"$KUBE_VERSION\") -gt $(utils::version_to_number \"${stable_version}\") ]];then\n        log::warning \"[check]\" \"The specified version($KUBE_VERSION) is more than the stable version(${stable_version})!\"\n        continue\n      fi\n    else\n      if [[ $(utils::version_to_number \"${local_version}\") -ge $(utils::version_to_number \"${stable_version}\") ]];then\n        log::warning \"[check]\" \"The local version($local_version) is greater or equal to the stable version(${stable_version})!\"\n        continue\n      fi\n    fi\n\n    command::exec \"${MGMT_NODE}\" \"kubectl drain ${host} --ignore-daemonsets --delete-local-data\"\n    check::exit_code \"$?\" \"upgrade\" \"drain ${host} node\" \"exit\"\n    sleep 5\n\n    if [[ \"${skip_plan}\" == \"false\" ]]; then\n      command::exec \"${host}\" \"$(declare -f script::upgrage_kube); script::upgrage_kube 'init' '$KUBE_VERSION'\"\n      check::exit_code \"$?\" \"upgrade\" \"plan and upgrade cluster on ${host}\" \"exit\"\n      command::exec \"${host}\" \"$(declare -f utils::retry); utils::retry 10 kubectl get node\"\n      check::exit_code \"$?\" \"upgrade\" \"${host}: upgrade\" \"exit\"\n      skip_plan=true\n    else\n      command::exec \"${host}\" \"$(declare -f script::upgrage_kube); script::upgrage_kube 'node' '$KUBE_VERSION'\"\n      check::exit_code \"$?\" \"upgrade\" \"upgrade ${host} node\" \"exit\"\n    fi\n\n    command::exec \"${MGMT_NODE}\" \"kubectl wait --for=condition=Ready node/${host} --timeout=120s\"\n    check::exit_code \"$?\" \"upgrade\" \"${host} ready\"\n    sleep 5\n    command::exec \"${MGMT_NODE}\" \"$(declare -f utils::retry); utils::retry 6 kubectl uncordon ${host}\"\n    check::exit_code \"$?\" \"upgrade\" \"uncordon ${host} node\"\n    sleep 5\n  done\n  \n  kube::status\n}\n\n\nfunction update::self {\n  # 脚本文件更新\n  \n  log::info \"[update]\" \"download kainstall script to $0\"\n  command::exec \"127.0.0.1\" \"\n    wget --timeout=10 --waitretry=3 --tries=5 --retry-connrefused ${GITHUB_PROXY}https://raw.githubusercontent.com/lework/kainstall/master/kainstall-ubuntu.sh -O /tmp/kainstall-ubuntu.sh || exit 1\n    /bin/cp -fv $0 /tmp/$0-bakup\n    /bin/mv -fv /tmp/kainstall-ubuntu.sh \\\"$0\\\"\n    chmod +x \\\"$0\\\"\n  \"\n  check::exit_code \"$?\" \"update\" \"kainstall script\"\n}\n\n\nfunction transform::data {\n  # 数据处理及限制\n\n  MASTER_NODES=$(echo \"${MASTER_NODES}\" | tr ',' ' ')\n  WORKER_NODES=$(echo \"${WORKER_NODES}\" | tr ',' ' ')\n\n  if ! utils::is_element_in_array \"$KUBE_CRI\" docker containerd cri-o ; then\n    log::error \"[limit]\" \"$KUBE_CRI is not supported, only [docker,containerd,cri-o]\"\n    exit 1\n  fi\n\n  [[ \"$KUBE_CRI\" != \"containerd\" && \"${OFFLINE_TAG:-}\" == \"1\" ]] && { log::error \"[limit]\" \"$KUBE_CRI is not supported offline, only containerd\"; exit 1; }\n  [[ \"$KUBE_CRI\" == \"docker\" && \"${KUBE_CRI_ENDPOINT}\" == \"unix:///run/containerd/containerd.sock\" ]] && KUBE_CRI_ENDPOINT=\"/var/run/dockershim.sock\"\n  [[ \"$KUBE_CRI\" == \"cri-o\" && \"${KUBE_CRI_ENDPOINT}\" == \"unix:///run/containerd/containerd.sock\"  ]] && KUBE_CRI_ENDPOINT=\"unix:///var/run/crio/crio.sock\"\n\n  kubelet_nodeRegistration=\"nodeRegistration:\n  criSocket: ${KUBE_CRI_ENDPOINT:-/run/containerd/containerd.sock}\n  kubeletExtraArgs:\n    runtime-cgroups: /system.slice/${KUBE_CRI//-/}.service\n    pod-infra-container-image: ${KUBE_IMAGE_REPO}/pause:${PAUSE_VERSION:-3.7}\n\"\n}\n\n\nfunction help::usage {\n  # 使用帮助\n  \n  cat << EOF\n\nInstall kubernetes cluster using kubeadm.\n\nUsage:\n  $(basename \"$0\") [command]\n\nAvailable Commands:\n  init            Init Kubernetes cluster.\n  reset           Reset Kubernetes cluster.\n  add             Add nodes to the cluster.\n  del             Remove node from the cluster.\n  renew-cert      Renew all available certificates.\n  upgrade         Upgrading kubeadm clusters.\n  update          Update script file.\n\nFlag:\n  -m,--master          master node, default: ''\n  -w,--worker          work node, default: ''\n  -u,--user            ssh user, default: ${SSH_USER}\n  -p,--password        ssh password\n     --private-key     ssh private key\n  -P,--port            ssh port, default: ${SSH_PORT}\n  -v,--version         kube version, default: ${KUBE_VERSION}\n  -n,--network         cluster network, choose: [flannel,calico,cilium], default: ${KUBE_NETWORK}\n  -i,--ingress         ingress controller, choose: [nginx,traefik], default: ${KUBE_INGRESS}\n  -ui,--ui             cluster web ui, choose: [dashboard,kubesphere], default: ${KUBE_UI}\n  -a,--addon           cluster add-ons, choose: [metrics-server,nodelocaldns], default: ${KUBE_ADDON}\n  -M,--monitor         cluster monitor, choose: [prometheus]\n  -l,--log             cluster log, choose: [elasticsearch]\n  -s,--storage         cluster storage, choose: [rook,longhorn]\n     --cri             cri tools, choose: [docker,containerd,cri-o], default: ${KUBE_CRI}\n     --cri-version     cri version, default: ${KUBE_CRI_VERSION}\n     --cri-endpoint    cri endpoint, default: ${KUBE_CRI_ENDPOINT}\n  -U,--upgrade-kernel  upgrade kernel\n  -of,--offline-file   specify the offline package file to load\n      --10years        the certificate period is 10 years.\n      --sudo           sudo mode\n      --sudo-user      sudo user\n      --sudo-password  sudo user password\n\nExample:\n  [init cluster]\n  $0 init \\\\\n  --master 192.168.77.130,192.168.77.131,192.168.77.132 \\\\\n  --worker 192.168.77.133,192.168.77.134,192.168.77.135 \\\\\n  --user root \\\\\n  --password 123456 \\\\\n  --version 1.20.4\n\n  [reset cluster]\n  $0 reset \\\\\n  --user root \\\\\n  --password 123456\n\n  [add node]\n  $0 add \\\\\n  --master 192.168.77.140,192.168.77.141 \\\\\n  --worker 192.168.77.143,192.168.77.144 \\\\\n  --user root \\\\\n  --password 123456 \\\\\n  --version 1.20.4\n\n  [del node]\n  $0 del \\\\\n  --master 192.168.77.140,192.168.77.141 \\\\\n  --worker 192.168.77.143,192.168.77.144 \\\\\n  --user root \\\\\n  --password 123456\n \n  [other]\n  $0 renew-cert --user root --password 123456\n  $0 upgrade --version 1.20.4 --user root --password 123456\n  $0 update\n  $0 add --ingress traefik\n  $0 add --monitor prometheus\n  $0 add --log elasticsearch\n  $0 add --storage rook\n  $0 add --ui dashboard\n  $0 add --addon nodelocaldns\n\nEOF\n  exit 1\n}\n\n\n######################################################################################################\n# main\n######################################################################################################\n\n\n[ \"$#\" == \"0\" ] && help::usage\n\nwhile [ \"${1:-}\" != \"\" ]; do\n  case $1 in\n    init  )                 INIT_TAG=1\n                            ;;\n    reset )                 RESET_TAG=1\n                            ;;\n    add )                   ADD_TAG=1\n                            ;;\n    del )                   DEL_TAG=1\n                            ;;\n    renew-cert )            RENEW_CERT_TAG=1\n                            ;;\n    upgrade )               UPGRADE_TAG=1\n                            ;;\n    update )                UPDATE_TAG=1\n                            ;;\n    -m | --master )         shift\n                            MASTER_NODES=${1:-$MASTER_NODES}\n                            ;;\n    -w | --worker )         shift\n                            WORKER_NODES=${1:-$WORKER_NODES}\n                            ;;\n    -u | --user )           shift\n                            SSH_USER=${1:-$SSH_USER}\n                            ;;\n    -p | --password )       shift\n                            SSH_PASSWORD=${1:-$SSH_PASSWORD}\n                            ;;\n    --private-key )         shift\n                            SSH_PRIVATE_KEY=${1:-$SSH_SSH_PRIVATE_KEY}\n                            ;;\n    -P | --port )           shift\n                            SSH_PORT=${1:-$SSH_PORT}\n                            ;;\n    -v | --version )        shift\n                            KUBE_VERSION=${1:-$KUBE_VERSION}\n                            ;;\n    -n | --network )        shift\n                            NETWORK_TAG=1\n                            KUBE_NETWORK=${1:-$KUBE_NETWORK}\n                            ;;\n    -i | --ingress )        shift\n                            INGRESS_TAG=1\n                            KUBE_INGRESS=${1:-$KUBE_INGRESS}\n                            ;;\n    -M | --monitor )        shift\n                            MONITOR_TAG=1\n                            KUBE_MONITOR=${1:-$KUBE_MONITOR}\n                            ;;\n    -l | --log )            shift\n                            LOG_TAG=1\n                            KUBE_LOG=${1:-$KUBE_LOG}\n                            ;;\n    -s | --storage )        shift\n                            STORAGE_TAG=1\n                            KUBE_STORAGE=${1:-$KUBE_STORAGE}\n                            ;;\n    -ui | --ui )            shift\n                            UI_TAG=1\n                            KUBE_UI=${1:-$KUBE_UI}\n                            ;;\n    -a | --addon )          shift\n                            ADDON_TAG=1\n                            KUBE_ADDON=${1:-$KUBE_ADDON}\n                            ;;\n    --cri )                 shift\n                            KUBE_CRI=${1:-$KUBE_CRI}\n                            ;;\n    --cri-version )         shift\n                            KUBE_CRI_VERSION=${1:-$KUBE_CRI_VERSION}\n                            ;;\n    --cri-endpoint )        shift\n                            KUBE_CRI_ENDPOINT=${1:-$KUBE_CRI_ENDPOINT}\n                            ;;\n    -U | --upgrade-kernel ) UPGRADE_KERNEL_TAG=1\n                            ;;\n    -of | --offline-file )  shift\n                            OFFLINE_TAG=1\n                            OFFLINE_FILE=${1:-$OFFLINE_FILE}\n                            ;;\n    --10years )             CERT_YEAR_TAG=1\n                            ;;\n    --sudo )                SUDO_TAG=1\n                            ;;\n    --sudo-user )           shift\n                            SUDO_USER=${1:-$SUDO_USER}\n                            ;;\n    --sudo-password )       shift\n                            SUDO_PASSWORD=${1:-}\n                            ;;\n    * )                     help::usage\n  esac\n  shift\ndone\n\n# 开始\nlog::info \"[start]\" \"bash $0 ${SCRIPT_PARAMETER//${SSH_PASSWORD:-${SUDO_PASSWORD:-}}/zzzzzz}\"  \n\n# 数据处理\ntransform::data\n\n# 预检\ncheck::preflight\n\n# 动作\nif [[ \"${INIT_TAG:-}\" == \"1\" ]]; then\n  [[ \"$MASTER_NODES\" == \"\" ]] && MASTER_NODES=\"127.0.0.1\"\n  init::cluster\nelif [[ \"${ADD_TAG:-}\" == \"1\" ]]; then\n  [[ \"${NETWORK_TAG:-}\" == \"1\" ]] && { add::network; add=1; }\n  [[ \"${INGRESS_TAG:-}\" == \"1\" ]] && { add::ingress; add=1; }\n  [[ \"${STORAGE_TAG:-}\" == \"1\" ]] && { add::storage; add=1; }\n  [[ \"${MONITOR_TAG:-}\" == \"1\" ]] && { add::monitor; add=1; }\n  [[ \"${LOG_TAG:-}\" == \"1\" ]] && { add::log; add=1; }\n  [[ \"${UI_TAG:-}\" == \"1\" ]] && { add::ui; add=1; }\n  [[ \"${ADDON_TAG:-}\" == \"1\" ]] && { add::addon; add=1; }\n  [[ \"$MASTER_NODES\" != \"\" || \"$WORKER_NODES\" != \"\" ]] && { add::node; add=1; }\n  [[ \"${add:-}\" != \"1\" ]] && help::usage\nelif [[ \"${DEL_TAG:-}\" == \"1\" ]]; then\n  if [[ \"$MASTER_NODES\" != \"\" || \"$WORKER_NODES\" != \"\" ]]; then del::node; else help::usage; fi\nelif [[ \"${RESET_TAG:-}\" == \"1\" ]]; then\n  reset::cluster\nelif [[ \"${RENEW_CERT_TAG:-}\" == \"1\" ]]; then\n  cert::renew\nelif [[ \"${UPGRADE_TAG:-}\" == \"1\" ]]; then\n  upgrade::cluster\nelif [[ \"${UPDATE_TAG:-}\" == \"1\" ]]; then\n  update::self\nelse\n  help::usage\nfi\n"
        }
      ]
    }
  ]
}