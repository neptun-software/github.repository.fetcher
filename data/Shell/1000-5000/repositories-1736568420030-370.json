{
  "metadata": {
    "timestamp": 1736568420030,
    "page": 370,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjM3OQ==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "lensesio/fast-data-dev",
      "stars": 2025,
      "defaultBranch": "fdd/main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0224609375,
          "content": "*#\n*~\n.idea\n.vscode\ntmp"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 24.279296875,
          "content": "FROM debian:bullseye as compile-lkd\nMAINTAINER Marios Andreopoulos <marios@lenses.io>\nARG TARGETARCH TARGETOS\n\nRUN printenv \\\n    && apt-get update \\\n    && apt-get install -y \\\n         unzip \\\n         wget \\\n\t file \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && echo \"progress = dot:giga\" | tee /etc/wgetrc \\\n    && mkdir -p /mnt /opt /data \\\n    && wget https://github.com/andmarios/duphard/releases/download/v1.1/duphard-${TARGETOS}-${TARGETARCH} -O /bin/duphard \\\n    && chmod +x /bin/duphard\n\nSHELL [\"/bin/bash\", \"-c\"]\nWORKDIR /\n\n# Login args for development archives\nARG DEVARCH_USER\nARG DEVARCH_PASS\nARG ARCHIVE_SERVER=https://archive.lenses.io\nARG LKD_VERSION=3.6.1-L0\n\n############\n# Add kafka/\n############\n\n# Add Apache Kafka (includes Connect and Zookeeper)\nARG KAFKA_VERSION=3.6.1\nARG KAFKA_LVERSION=\"${KAFKA_VERSION}-L0\"\nARG KAFKA_URL=\"${ARCHIVE_SERVER}/lkd/packages/kafka/kafka-2.13-${KAFKA_LVERSION}-lkd.tar.gz\"\n\nRUN wget $DEVARCH_USER $DEVARCH_PASS \"$KAFKA_URL\" -O /opt/kafka.tar.gz \\\n    && tar --no-same-owner -xzf /opt/kafka.tar.gz -C /opt \\\n    && mkdir /opt/lensesio/kafka/logs && chmod 1777 /opt/lensesio/kafka/logs \\\n    && rm -rf /opt/kafka.tar.gz\n\n# Add Schema Registry and REST Proxy\nARG REGISTRY_VERSION=7.5.3-lkd-r0\nARG REGISTRY_URL=\"${ARCHIVE_SERVER}/lkd/packages/schema-registry/schema-registry-${REGISTRY_VERSION}.tar.gz\"\nRUN wget $DEVARCH_USER $DEVARCH_PASS \"$REGISTRY_URL\" -O /opt/registry.tar.gz \\\n    && tar --no-same-owner -xzf /opt/registry.tar.gz -C /opt/ \\\n    && rm -rf /opt/registry.tar.gz\n\nARG REST_VERSION=7.5.3-lkd-r0\nARG REST_URL=\"${ARCHIVE_SERVER}/lkd/packages/rest-proxy/rest-proxy-${REST_VERSION}.tar.gz\"\nRUN wget $DEVARCH_USER $DEVARCH_PASS \"$REST_URL\" -O /opt/rest.tar.gz \\\n    && tar --no-same-owner -xzf /opt/rest.tar.gz -C /opt/ \\\n    && rm -rf /opt/rest.tar.gz\n\n# Configure Connect and Confluent Components to support CORS\nRUN echo -e 'access.control.allow.methods=GET,POST,PUT,DELETE,OPTIONS\\naccess.control.allow.origin=*' \\\n         | tee -a /opt/lensesio/kafka/etc/schema-registry/schema-registry.properties \\\n         | tee -a /opt/lensesio/kafka/etc/kafka-rest/kafka-rest.properties \\\n         | tee -a /opt/lensesio/kafka/etc/schema-registry/connect-avro-distributed.properties\n\n\n#################\n# Add connectors/\n#################\n\n# Add Stream Reactor and needed components\nARG STREAM_REACTOR_VERSION=7.4.1\nARG STREAM_REACTOR_URL=\"https://archive.lenses.io/lkd/packages/connectors/stream-reactor/stream-reactor-${STREAM_REACTOR_VERSION}.tar.gz\"\nARG ACTIVEMQ_VERSION=5.12.3\n\nRUN wget $DEVARCH_USER $DEVARCH_PASS \"${STREAM_REACTOR_URL}\" -O /stream-reactor.tar.gz \\\n    && mkdir -p /opt/lensesio/connectors/stream-reactor \\\n    && tar -xf /stream-reactor.tar.gz \\\n           --no-same-owner \\\n           --strip-components=1 \\\n           -C /opt/lensesio/connectors/stream-reactor \\\n    && rm /stream-reactor.tar.gz \\\n    && rm -rf /opt/lensesio/connectors/stream-reactor/kafka-connect-hive-1.1 \\\n    && wget https://repo1.maven.org/maven2/org/apache/activemq/activemq-all/${ACTIVEMQ_VERSION}/activemq-all-${ACTIVEMQ_VERSION}.jar \\\n            -P /opt/lensesio/connectors/stream-reactor/kafka-connect-jms \\\n    && mkdir -p /opt/lensesio/kafka/share/java/lensesio-common \\\n    && export _NUM_CONNECTORS=$(ls /opt/lensesio/connectors/stream-reactor | wc -l) \\\n    && for file in $(find /opt/lensesio/connectors/stream-reactor -maxdepth 2 -type f -exec basename {} \\; | grep -Ev \"scala-logging|kafka-connect-common|scala-\" | grep jar | grep -v log4j-over-slf4j | sort | uniq -c | grep -E \"^\\s+${_NUM_CONNECTORS} \" | awk '{print $2}' ); do \\\n         cp /opt/lensesio/connectors/stream-reactor/kafka-connect-aws-s3/$file /opt/lensesio/kafka/share/java/lensesio-common/; \\\n         rm -f /opt/lensesio/connectors/stream-reactor/kafka-connect-*/$file; \\\n       done \\\n    && for file in $(find /opt/lensesio/kafka/share/java/{kafka,lensesio-common} -maxdepth 1 -type f -exec basename {} \\; | sort | uniq -c | grep -E \"^\\s+2 \" | awk '{print $2}' ); do \\\n         echo \"Removing duplicate /opt/lensesio/kafka/share/java/lensesio-common/$file.\"; \\\n         rm -f /opt/lensesio/kafka/share/java/lensesio-common/$file; \\\n       done \\\n    && rm -f /opt/lensesio/connectors/stream-reactor/*/*{javadoc,scaladoc,sources}.jar \\\n    && echo \"plugin.path=/opt/lensesio/connectors/stream-reactor,/opt/lensesio/connectors/third-party\" \\\n            >> /opt/lensesio/kafka/etc/schema-registry/connect-avro-distributed.properties\n\n# Add Secrets Provider\nARG SECRET_PROVIDER_VERSION=2.3.0\nARG SECRET_PROVIDER_URL=\"https://github.com/lensesio/secret-provider/releases/download/${SECRET_PROVIDER_VERSION}/secret-provider-${SECRET_PROVIDER_VERSION}-all.jar\"\nRUN mkdir -p /opt/lensesio/connectors/stream-reactor/kafka-connect-secret-provider \\\n    && wget \"${SECRET_PROVIDER_URL}\" -P \"/opt/lensesio/connectors/stream-reactor/kafka-connect-secret-provider\"\n\n# Add Kafka Connect SMTs\nARG KAFKA_CONNECT_VERSION=1.0.2\nARG KAFKA_CONNECT_URL=\"https://github.com/lensesio/kafka-connect-smt/releases/download/v${KAFKA_CONNECT_VERSION}/kafka-connect-smt-${KAFKA_CONNECT_VERSION}.jar\"\nRUN mkdir -p /opt/lensesio/connectors/stream-reactor/kafka-connect-smt \\\n    && wget \"${KAFKA_CONNECT_URL}\" -P \"/opt/lensesio/connectors/stream-reactor/kafka-connect-smt\"\n\n# Add Third Party Connectors\n\n## Filesource\nRUN mkdir -p /opt/lensesio/connectors/third-party/kafka-connect-file \\\n    && ln -s /opt/lensesio/kafka/share/java/kafka/connect-file-${KAFKA_LVERSION}.jar /opt/lensesio/connectors/third-party/kafka-connect-file/connect-file-${KAFKA_LVERSION}.jar\n\n## Twitter\nARG TWITTER_CONNECTOR_URL=\"https://archive.lenses.io/third-party/kafka-connect-twitter/kafka-connect-twitter-0.1-master-33331ea-connect-1.0.0-jar-with-dependencies.jar\"\nRUN mkdir -p /opt/lensesio/connectors/third-party/kafka-connect-twitter \\\n    && wget \"$TWITTER_CONNECTOR_URL\" -P /opt/lensesio/connectors/third-party/kafka-connect-twitter\n\n## Kafka Connect JDBC\nARG KAFKA_CONNECT_JDBC_VERSION=10.7.4-lkd-r0\nARG KAFKA_CONNECT_JDBC_URL=\"${ARCHIVE_SERVER}/lkd/packages/connectors/third-party/kafka-connect-jdbc/kafka-connect-jdbc-${KAFKA_CONNECT_JDBC_VERSION}.tar.gz\"\nRUN wget $DEVARCH_USER $DEVARCH_PASS \"$KAFKA_CONNECT_JDBC_URL\" \\\n         -O /opt/kafka-connect-jdbc.tar.gz \\\n    && mkdir -p /opt/lensesio/connectors/third-party/ \\\n    && tar --no-same-owner -xf /opt/kafka-connect-jdbc.tar.gz \\\n           -C /opt/lensesio/connectors/third-party/ \\\n    && rm -rf /opt/kafka-connect-jdbc.tar.gz\n\n## Kafka Connect ELASTICSEARCH\nARG KAFKA_CONNECT_ELASTICSEARCH_VERSION=14.0.12-lkd-r0\nARG KAFKA_CONNECT_ELASTICSEARCH_URL=\"${ARCHIVE_SERVER}/lkd/packages/connectors/third-party/kafka-connect-elasticsearch/kafka-connect-elasticsearch-${KAFKA_CONNECT_ELASTICSEARCH_VERSION}.tar.gz\"\nRUN wget $DEVARCH_USER $DEVARCH_PASS \"$KAFKA_CONNECT_ELASTICSEARCH_URL\" \\\n         -O /opt/kafka-connect-elasticsearch.tar.gz \\\n    && mkdir -p /opt/lensesio/connectors/third-party/ \\\n    && tar --no-same-owner -xf /opt/kafka-connect-elasticsearch.tar.gz \\\n           -C /opt/lensesio/connectors/third-party/ \\\n    && rm -rf /opt/kafka-connect-elasticsearch.tar.gz\n\n## Kafka Connect HDFS\nARG KAFKA_CONNECT_HDFS_VERSION=10.2.5-lkd-r0\nARG KAFKA_CONNECT_HDFS_URL=\"${ARCHIVE_SERVER}/lkd/packages/connectors/third-party/kafka-connect-hdfs/kafka-connect-hdfs-${KAFKA_CONNECT_HDFS_VERSION}.tar.gz\"\nRUN wget $DEVARCH_USER $DEVARCH_PASS \"$KAFKA_CONNECT_HDFS_URL\" \\\n         -O /opt/kafka-connect-hdfs.tar.gz \\\n    && mkdir -p /opt/lensesio/connectors/third-party/ \\\n    && tar --no-same-owner -xf /opt/kafka-connect-hdfs.tar.gz \\\n           -C /opt/lensesio/connectors/third-party/ \\\n    && rm -rf /opt/kafka-connect-hdfs.tar.gz\n\n# Kafka Connect Couchbase\nARG KAFKA_CONNECT_COUCHBASE_VERSION=4.1.9\nARG KAFKA_CONNECT_COUCHBASE_URL=\"http://packages.couchbase.com/clients/kafka/${KAFKA_CONNECT_COUCHBASE_VERSION}/couchbase-kafka-connect-couchbase-${KAFKA_CONNECT_COUCHBASE_VERSION}.zip\"\nRUN wget $DEVARCH_USER $DEVARCH_PASS \"$KAFKA_CONNECT_COUCHBASE_URL\" \\\n         -O /couchbase.zip \\\n    && mkdir -p /couchbase /opt/lensesio/connectors/third-party/kafka-connect-couchbase \\\n    && unzip /couchbase.zip -d /couchbase \\\n    && cp -ax /couchbase/couchbase-kafka-connect-couchbase-${KAFKA_CONNECT_COUCHBASE_VERSION}/* \\\n          /opt/lensesio/connectors/third-party/kafka-connect-couchbase \\\n    && chown -R root:root /opt/lensesio/connectors/third-party/kafka-connect-couchbase \\\n    && rm -rf /couchbase.zip /couchbase\n\n# Kafka Connect Debezium MongoDB / MySQL / Postgres / MsSQL\nARG KAFKA_CONNECT_DEBEZIUM_MONGODB_VERSION=2.4.2.Final\nARG KAFKA_CONNECT_DEBEZIUM_MONGODB_URL=\"https://search.maven.org/remotecontent?filepath=io/debezium/debezium-connector-mongodb/${KAFKA_CONNECT_DEBEZIUM_MONGODB_VERSION}/debezium-connector-mongodb-${KAFKA_CONNECT_DEBEZIUM_MONGODB_VERSION}-plugin.tar.gz\"\nARG KAFKA_CONNECT_DEBEZIUM_MYSQL_VERSION=2.4.2.Final\nARG KAFKA_CONNECT_DEBEZIUM_MYSQL_URL=\"https://search.maven.org/remotecontent?filepath=io/debezium/debezium-connector-mysql/${KAFKA_CONNECT_DEBEZIUM_MYSQL_VERSION}/debezium-connector-mysql-${KAFKA_CONNECT_DEBEZIUM_MYSQL_VERSION}-plugin.tar.gz\"\nARG KAFKA_CONNECT_DEBEZIUM_POSTGRES_VERSION=2.4.2.Final\nARG KAFKA_CONNECT_DEBEZIUM_POSTGRES_URL=\"https://search.maven.org/remotecontent?filepath=io/debezium/debezium-connector-postgres/${KAFKA_CONNECT_DEBEZIUM_POSTGRES_VERSION}/debezium-connector-postgres-${KAFKA_CONNECT_DEBEZIUM_POSTGRES_VERSION}-plugin.tar.gz\"\nARG KAFKA_CONNECT_DEBEZIUM_SQLSERVER_VERSION=2.4.2.Final\nARG KAFKA_CONNECT_DEBEZIUM_SQLSERVER_URL=\"https://search.maven.org/remotecontent?filepath=io/debezium/debezium-connector-sqlserver/${KAFKA_CONNECT_DEBEZIUM_SQLSERVER_VERSION}/debezium-connector-sqlserver-${KAFKA_CONNECT_DEBEZIUM_SQLSERVER_VERSION}-plugin.tar.gz\"\nRUN mkdir -p /opt/lensesio/connectors/third-party/kafka-connect-debezium-{mongodb,mysql,postgres,sqlserver} \\\n    && wget \"$KAFKA_CONNECT_DEBEZIUM_MONGODB_URL\" -O /debezium-mongodb.tgz \\\n    && file /debezium-mongodb.tgz \\\n    && tar -xf /debezium-mongodb.tgz \\\n           --owner=root --group=root --strip-components=1 \\\n           -C  /opt/lensesio/connectors/third-party/kafka-connect-debezium-mongodb \\\n    && wget \"$KAFKA_CONNECT_DEBEZIUM_MYSQL_URL\" -O /debezium-mysql.tgz \\\n    && file /debezium-mysql.tgz \\\n    && tar -xf /debezium-mysql.tgz \\\n           --owner=root --group=root --strip-components=1 \\\n           -C  /opt/lensesio/connectors/third-party/kafka-connect-debezium-mysql \\\n    && wget \"$KAFKA_CONNECT_DEBEZIUM_POSTGRES_URL\" -O /debezium-postgres.tgz \\\n    && file /debezium-postgres.tgz \\\n    && tar -xf /debezium-postgres.tgz \\\n           --owner=root --group=root --strip-components=1 \\\n           -C  /opt/lensesio/connectors/third-party/kafka-connect-debezium-postgres \\\n    && wget \"$KAFKA_CONNECT_DEBEZIUM_SQLSERVER_URL\" -O /debezium-sqlserver.tgz \\\n    && tar -xf /debezium-sqlserver.tgz \\\n           --owner=root --group=root --strip-components=1 \\\n           -C  /opt/lensesio/connectors/third-party/kafka-connect-debezium-sqlserver \\\n    && rm -rf /debezium-{mongodb,mysql,postgres,sqlserver}.tgz\n\n# Kafka Connect Splunk\nARG KAFKA_CONNECT_SPLUNK_VERSION=\"2.2.0\"\nARG KAFKA_CONNECT_SPLUNK_URL=\"https://github.com/splunk/kafka-connect-splunk/releases/download/v${KAFKA_CONNECT_SPLUNK_VERSION}/splunk-kafka-connect-v${KAFKA_CONNECT_SPLUNK_VERSION}.jar\"\nRUN mkdir -p /opt/lensesio/connectors/third-party/kafka-connect-splunk \\\n    && wget \"$KAFKA_CONNECT_SPLUNK_URL\" \\\n       -O /opt/lensesio/connectors/third-party/kafka-connect-splunk/splunk-kafka-connect-v${KAFKA_CONNECT_SPLUNK_VERSION}.jar\n\n############\n# Add tools/\n############\n\n# Add Coyote\nARG COYOTE_VERSION=1.5\nARG COYOTE_URL=\"https://github.com/lensesio/coyote/releases/download/v${COYOTE_VERSION}/coyote-${COYOTE_VERSION}-${TARGETOS}-${TARGETARCH}\"\nRUN mkdir -p /opt/lensesio/tools/bin /opt/lensesio/tools/share/coyote/examples \\\n    && wget \"$COYOTE_URL\" -O /opt/lensesio/tools/bin/coyote \\\n    && chmod +x /opt/lensesio/tools/bin/coyote\nADD lkd/simple-integration-tests.yml /opt/lensesio/tools/share/coyote/examples/\n\n# Add Kafka Topic UI, Schema Registry UI, Kafka Connect UI\nARG KAFKA_TOPICS_UI_VERSION=0.9.4\nARG KAFKA_TOPICS_UI_URL=\"https://github.com/lensesio/kafka-topics-ui/releases/download/v${KAFKA_TOPICS_UI_VERSION}/kafka-topics-ui-${KAFKA_TOPICS_UI_VERSION}.tar.gz\"\nARG SCHEMA_REGISTRY_UI_VERSION=0.9.5\nARG SCHEMA_REGISTRY_UI_URL=\"https://github.com/lensesio/schema-registry-ui/releases/download/v.${SCHEMA_REGISTRY_UI_VERSION}/schema-registry-ui-${SCHEMA_REGISTRY_UI_VERSION}.tar.gz\"\nARG KAFKA_CONNECT_UI_VERSION=0.9.7\nARG KAFKA_CONNECT_UI_URL=\"https://github.com/lensesio/kafka-connect-ui/releases/download/v.${KAFKA_CONNECT_UI_VERSION}/kafka-connect-ui-${KAFKA_CONNECT_UI_VERSION}.tar.gz\"\nRUN mkdir -p /opt/lensesio/tools/share/kafka-topics-ui/ \\\n             /opt/lensesio/tools/share/schema-registry-ui/ \\\n             /opt/lensesio/tools/share/kafka-connect-ui/ \\\n    && wget \"$KAFKA_TOPICS_UI_URL\" -O /kafka-topics-ui.tar.gz \\\n    && tar xvf /kafka-topics-ui.tar.gz -C /opt/lensesio/tools/share/kafka-topics-ui \\\n    && mv /opt/lensesio/tools/share/kafka-topics-ui/env.js /opt/lensesio/tools/share/kafka-topics-ui/env.js.sample \\\n    && wget \"$SCHEMA_REGISTRY_UI_URL\" -O /schema-registry-ui.tar.gz \\\n    && tar xvf /schema-registry-ui.tar.gz -C /opt/lensesio/tools/share/schema-registry-ui \\\n    && mv /opt/lensesio/tools/share/schema-registry-ui/env.js /opt/lensesio/tools/share/schema-registry-ui/env.js.sample \\\n    && wget \"$KAFKA_CONNECT_UI_URL\" -O /kafka-connect-ui.tar.gz \\\n    && tar xvf /kafka-connect-ui.tar.gz -C /opt/lensesio/tools/share/kafka-connect-ui \\\n    && mv /opt/lensesio/tools/share/kafka-connect-ui/env.js /opt/lensesio/tools/share/kafka-connect-ui/env.js.sample \\\n    && rm -f /kafka-topics-ui.tar.gz /schema-registry-ui.tar.gz /kafka-connect-ui.tar.gz\n\n# Add Kafka Autocomplete\nARG KAFKA_AUTOCOMPLETE_VERSION=0.3\nARG KAFKA_AUTOCOMPLETE_URL=\"https://github.com/lensesio/kafka-autocomplete/releases/download/${KAFKA_AUTOCOMPLETE_VERSION}/kafka\"\nRUN mkdir -p /opt/lensesio/tools/share/kafka-autocomplete \\\n             /opt/lensesio/tools/share/bash-completion/completions \\\n    && wget \"$KAFKA_AUTOCOMPLETE_URL\" \\\n            -O /opt/lensesio/tools/share/kafka-autocomplete/kafka \\\n    && wget \"$KAFKA_AUTOCOMPLETE_URL\" \\\n            -O /opt/lensesio/tools/share/bash-completion/completions/kafka\n\n# Enable jline for Zookeeper\nRUN TJLINE=\"$(find /opt/lensesio/kafka -name \"jline-0*.jar\" | head -n1)\" \\\n    && if [[ -n $TJLINE ]]; then sed \"s|^exec.*|export CLASSPATH=\\\"\\$CLASSPATH:$TJLINE\\\"\\n&|\" -i /opt/lensesio/kafka/bin/zookeeper-shell; fi\n\n# Add normcat\nARG NORMCAT_VERSION=1.1.1\nARG NORMCAT_URL=\"https://github.com/andmarios/normcat/releases/download/${NORMCAT_VERSION}/normcat-${NORMCAT_VERSION}-${TARGETOS}-${TARGETARCH}\"\nRUN wget \"$NORMCAT_URL\"-lowmem.tar.gz -O /normcat-linux.tgz \\\n    && tar -xf /normcat-linux.tgz -C /opt/lensesio/tools/bin \\\n    && chmod +x /opt/lensesio/tools/bin/normcat \\\n    && rm -f /normcat-linux.tgz\n\n# Add connect-cli\nARG CONNECT_CLI_VERSION=1.0.9\nARG CONNECT_CLI_URL=\"https://github.com/lensesio/kafka-connect-tools/releases/download/v${CONNECT_CLI_VERSION}/connect-cli\"\nRUN wget \"$CONNECT_CLI_URL\" -O /opt/lensesio/tools/bin/connect-cli && chmod +x /opt/lensesio/tools/bin/connect-cli\n\n##########\n# Finalize\n##########\n\nRUN echo    \"LKD_VERSION=${LKD_VERSION}\"                               | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_VERSION=${KAFKA_LVERSION}\"                          | tee -a /opt/lensesio/build.info \\\n    && echo \"CONNECT_VERSION=${KAFKA_LVERSION}\"                        | tee -a /opt/lensesio/build.info \\\n    && echo \"SCHEMA_REGISTRY_VERSION=${REGISTRY_VERSION}\"              | tee -a /opt/lensesio/build.info \\\n    && echo \"REST_PROXY_VERSION=${REST_VERSION}\"                       | tee -a /opt/lensesio/build.info \\\n    && echo \"STREAM_REACTOR_VERSION=${STREAM_REACTOR_VERSION}\"         | tee -a /opt/lensesio/build.info \\\n    && echo \"SECRET_PROVIDER_VERSION=${SECRET_PROVIDER_VERSION}\"       | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_JDBC_VERSION=${KAFKA_CONNECT_JDBC_VERSION}\" | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_ELASTICSEARCH_VERSION=${KAFKA_CONNECT_ELASTICSEARCH_VERSION}\" \\\n                                                                       | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_HDFS_VERSION=${KAFKA_CONNECT_HDFS_VERSION}\" | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_COUCHBASE_VERSION=${KAFKA_CONNECT_COUCHBASE_VERSION}\" \\\n                                                                       | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_DEBEZIUM_MONGODB_VERSION=${KAFKA_CONNECT_DEBEZIUM_MONGODB_VERSION}\" \\\n                                                                       | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_DEBEZIUM_MYSQL_VERSION=${KAFKA_CONNECT_DEBEZIUM_MYSQL_VERSION}\" \\\n                                                                       | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_DEBEZIUM_SQLSERVER_VERSION=${KAFKA_CONNECT_DEBEZIUM_SQLSERVER_VERSION}\" \\\n                                                                       | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_DEBEZIUM_POSTGRES_VERSION=${KAFKA_CONNECT_DEBEZIUM_POSTGRES_VERSION}\" \\\n                                                                       | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_SPLUNK_VERSION=${KAFKA_CONNECT_SPLUNK_VERSION}\" \\\n                                                                       | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_TOPICS_UI_VERSION=${KAFKA_TOPICS_UI_VERSION}\"       | tee -a /opt/lensesio/build.info \\\n    && echo \"SCHEMA_REGISTRY_UI_VERSION=${SCHEMA_REGISTRY_UI_VERSION}\" | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_CONNECT_UI_VERSION=${KAFKA_CONNECT_UI_VERSION}\"     | tee -a /opt/lensesio/build.info \\\n    && echo \"COYOTE_VERSION=${COYOTE_VERSION}\"                         | tee -a /opt/lensesio/build.info \\\n    && echo \"KAFKA_AUTOCOMPLETE_VERSION=${KAFKA_AUTOCOMPLETE_VERSION}\" | tee -a /opt/lensesio/build.info \\\n    && echo \"NORMCAT_VERSION=${NORMCAT_VERSION}\"                       | tee -a /opt/lensesio/build.info \\\n    && echo \"CONNECT_CLI_VERSION=${CONNECT_CLI_VERSION}\"               | tee -a /opt/lensesio/build.info\n\n# duphard (replace duplicates with hard links) and create archive\n# We run as two separate commands because otherwise the build fails in docker hub (but not locally)\nRUN duphard -d=0 /opt/lensesio\nRUN tar -czf /LKD-${LKD_VERSION}.tar.gz \\\n           --owner=root \\\n           --group=root \\\n           -C /opt \\\n           lensesio \\\n    && rm -rf /opt/lensesio\n# Unfortunately we have to make this a separate step in order for docker to understand the change to hardlinks\n# Good thing: final image that people download is much smaller (~200MB).\nRUN tar xf /LKD-${LKD_VERSION}.tar.gz -C /opt \\\n    && rm /LKD-${LKD_VERSION}.tar.gz\n\nENV LKD_VERSION=${LKD_VERSION}\n# If this stage is run as container and you mount `/mnt`, we will create the LKD archive there.\nCMD [\"bash\", \"-c\", \"tar -czf /mnt/LKD-${LKD_VERSION}.tar.gz -C /opt lensesio; chown --reference=/mnt /mnt/LKD-${LKD_VERSION}.tar.gz\"]\n\nFROM debian:bullseye-slim\nMAINTAINER Marios Andreopoulos <marios@lenses.io>\nCOPY --from=compile-lkd /opt /opt\nARG TARGETOS TARGETARCH\n\n# Update, install tooling and some basic setup\nRUN apt-get update \\\n    && apt-get install -y \\\n        bash-completion \\\n        bzip2 \\\n        coreutils \\\n        curl \\\n        default-jre-headless \\\n        dumb-init \\\n        gettext \\\n        gzip \\\n        jq \\\n        locales \\\n        netcat \\\n        openssl \\\n        sqlite3 \\\n        supervisor \\\n        wget \\\n    && rm -rf /var/lib/apt/lists/* \\\n    && echo \"progress = dot:giga\" | tee /etc/wgetrc \\\n    && mkdir -p /opt \\\n    && mkdir /extra-connect-jars /connectors \\\n    && mkdir /etc/supervisord.d /etc/supervisord.templates.d \\\n    && rm -rf /var/log/*\n\nSHELL [\"/bin/bash\", \"-c\"]\nWORKDIR /\n\n# Install external tooling\n# checkport: checks for ports that are already in use, useful when we run with\n#            '--net=host so we have an easy way to detect if our ports are free\n# quickcert: a small tool we use to create a CA and key-cert pairs so we can easily\n#            setup SSL on the brokers with autogenerated keys and certs\n# glibc    : alpine linux has an embedded libc which misses some functions that are\n#            needed by some apps (e.g jvm's rocksdb jni — HDFS connector, Lenses, etc),\n#            so we add glibc to make them work. Also now we can add en_US.UTF-8 locale.\n#            https://github.com/sgerrand/alpine-pkg-glibc\n# caddy    : an excellent web server we use to serve fast-data-dev UI, proxy various REST\n#            endpoints, etc\n#            https://github.com/mholt/caddy\nARG CHECKPORT_URL=\"https://github.com/andmarios/checkport/releases/download/0.1/checkport-${TARGETOS}-${TARGETARCH}\"\nARG QUICKCERT_URL=\"https://github.com/andmarios/quickcert/releases/download/1.1/quickcert-1.1-${TARGETOS}-${TARGETARCH}\"\nARG CADDY_URL=https://github.com/caddyserver/caddy/releases/download/v0.11.5/caddy_v0.11.5_${TARGETOS}_${TARGETARCH}.tar.gz\nARG GOTTY_URL_AMD64=https://github.com/yudai/gotty/releases/download/v1.0.1/gotty_linux_amd64.tar.gz\nARG GOTTY_URL_ARM64=https://github.com/yudai/gotty/releases/download/v1.0.1/gotty_linux_arm.tar.gz\nRUN wget \"$CHECKPORT_URL\" -O /usr/local/bin/checkport \\\n    && wget \"$QUICKCERT_URL\" -O /usr/local/bin/quickcert \\\n    && chmod 0755 /usr/local/bin/quickcert /usr/local/bin/checkport \\\n    && wget \"$CADDY_URL\" -O /caddy.tgz \\\n    && mkdir -p /opt/caddy \\\n    && tar xzf /caddy.tgz -C /opt/caddy \\\n    && rm -f /caddy.tgz \\\n    && if [[ $TARGETARCH == amd64 ]]; then GOTTY_URL=$GOTTY_URL_AMD64; elif [[ $TARGETARCH == arm64 ]]; then GOTTY_URL=$GOTTY_URL_ARM64; fi \\\n    && wget \"$GOTTY_URL\" -O /gotty.tar.gz \\\n    && mkdir -p /opt/gotty \\\n    && tar xzf gotty.tar.gz -C /opt/gotty \\\n    && rm -f gotty.tar.gz \\\n    && localedef -i en_US -f UTF-8 en_US.UTF-8\nENV LANG=en_US.UTF-8 LANGUAGE=en_US.UTF-8 LC_ALL=en_US.UTF-8\n\nCOPY /filesystem /\nRUN chmod +x /usr/local/bin/{smoke-tests,logs-to-kafka,nullsink}.sh \\\n             /usr/local/share/lensesio/sample-data/*.sh\n\n# Create system symlinks to Kafka binaries\nRUN bash -c 'for i in $(find /opt/lensesio/kafka/bin /opt/lensesio/tools/bin -maxdepth 1 -type f); do ln -s $i /usr/local/bin/$(echo $i | sed -e \"s>.*/>>\"); done'\n\n# Compatibility directories that were used with LKD 3.3.1 and older. FDD does\n# not need them, but we add them in case users have scripts or docker-compose\n# files that rely on them.\nRUN ln -s /opt/lensesio /opt/landoop \\\n    && ln -s /usr/local/share/lensesio /usr/local/share/landoop\n\n# Add kafka ssl principal builder\nRUN wget https://archive.lenses.io/third-party/kafka-custom-principal-builder/kafka-custom-principal-builder-1.0-SNAPSHOT.jar \\\n         -P /opt/lensesio/kafka/share/java/kafka \\\n    && mkdir -p /opt/lensesio/kafka/share/docs/kafka-custom-principal-builder \\\n    && wget https://archive.lenses.io/third-party/kafka-custom-principal-builder/LICENSE \\\n         -P /opt/lensesio/kafka/share/docs/kafka-custom-principal-builder \\\n    && wget https://archive.lenses.io/third-party/kafka-custom-principal-builder/README.md \\\n         -P /opt/lensesio/kafka/share/docs/kafka-custom-principal-builder\n\n# Setup Kafka Topics UI, Schema Registry UI, Kafka Connect UI\nRUN mkdir -p \\\n      /var/www/kafka-topics-ui \\\n      /var/www/schema-registry-ui \\\n      /var/www/kafka-connect-ui \\\n    && cp -ax /opt/lensesio/tools/share/kafka-topics-ui/* /var/www/kafka-topics-ui/ \\\n    && cp -ax /opt/lensesio/tools/share/schema-registry-ui/* /var/www/schema-registry-ui/ \\\n    && cp -ax /opt/lensesio/tools/share/kafka-connect-ui/* /var/www/kafka-connect-ui/\n\nRUN ln -s /var/log /var/www/logs\n\n# Add executables, settings and configuration\nADD setup-and-run.sh /usr/local/bin/\nRUN chmod +x /usr/local/bin/setup-and-run.sh \\\n    && rm -f /root/.bashrc \\\n    && ln -s /usr/local/share/lensesio/etc/bashrc /root/.bashrc\n\nVOLUME [\"/data\"]\n\nARG BUILD_BRANCH\nARG BUILD_COMMIT\nARG BUILD_TIME\nARG DOCKER_REPO=local\nRUN echo \"BUILD_BRANCH=${BUILD_BRANCH}\"    | tee /build.info \\\n    && echo \"BUILD_COMMIT=${BUILD_COMMIT}\" | tee -a /build.info \\\n    && echo \"BUILD_TIME=${BUILD_TIME}\"     | tee -a /build.info \\\n    && echo \"DOCKER_REPO=${DOCKER_REPO}\"   | tee -a /build.info \\\n    && echo \"TARGETPLATFORM=${TARGETOS}/${TARGETARCH}\" | tee -a /build.info \\\n    && sed -e 's/^/FDD_/' /opt/lensesio/build.info      | tee -a /build.info\n\nEXPOSE 2181 3030 3031 8081 8082 8083 9092\nENTRYPOINT [\"/usr/bin/dumb-init\", \"--\"]\nCMD [\"/usr/local/bin/setup-and-run.sh\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "NOTICE",
          "type": "blob",
          "size": 0.8564453125,
          "content": "Lenses Box / fast-data-dev\nCopyright 2016-2024, Lenses.io, Ltd\n\nThis product includes software developed at\nLenses.io, Ltd (https://lenses.io)\n\nSoftware included in the Docker image produced by this repository includes\nsoftware by third parties and other Lenses.io repositories.\n\nWe do not claim ownership of third party software and each component retains\nits original license.\n\nThe most important software components and their license are:\n\nBy Lenses.io:\nLenses.io Stream Reactor        : Apache 2.0\nLenses.io Kafka Connect UI      : BSL (http://www.landoop.com/bsl/)\nLenses.io Kafka Topics UI       : BSL (http://www.landoop.com/bsl/)\nLenses.io Schema Registry UI    : BSL (http://www.landoop.com/bsl/)\n\nThird Party:\nCaddy                                  : Apache 2.0\nApache Kafka (incl Connect, Zookeeper) : Apache 2.0\nConfluent Schema Registry, REST Proxy  : Apache 2.0\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 21.6806640625,
          "content": "# Lenses Box / fast-data-dev #\nlensesio/box (lensesio/box)\n[![docker](https://img.shields.io/docker/pulls/lensesio/box.svg?style=flat)](https://hub.docker.com/r/lensesio/box/)\n[![](https://images.microbadger.com/badges/image/lensesio/box.svg)](http://microbadger.com/images/lensesio/box)\n\nlensesio/fast-data-dev\n[![docker](https://img.shields.io/docker/pulls/lensesio/fast-data-dev.svg?style=flat)](https://hub.docker.com/r/lensesio/fast-data-dev/)\n[![](https://images.microbadger.com/badges/image/lensesio/fast-data-dev.svg)](http://microbadger.com/images/lensesio/fast-data-dev)\n\n[Join the Slack Lenses.io Community!](https://launchpass.com/lensesio)\n\n[Apache Kafka](http://kafka.apache.org/) docker image for developers; with\nLenses\n([lensesio/box](https://hub.docker.com/r/lensesio/box))\nor Lenses.io's open source UI tools\n([lensesio/fast-data-dev](https://hub.docker.com/r/lensesio/fast-data-dev)). Have\na full fledged Kafka installation up and running in seconds and top it off with\na modern streaming platform (only for kafka-lenses-dev), intuitive UIs and extra\ngoodies. Also includes Kafka Connect, Schema Registry, Lenses.io's Stream Reactor\n25+ Connectors and more.\n\n> **[Get a free license for Lenses Box](https://lenses.io/box/)**\n\n### Introduction\n\nWhen you need:\n\n1. **A Kafka distribution** with Apache Kafka, Kafka Connect, Zookeeper, Confluent Schema Registry and REST Proxy\n2. **Lenses.io** Lenses or kafka-topics-ui, schema-registry-ui, kafka-connect-ui\n3. **Lenses.io** Stream Reactor, 25+ Kafka Connectors to simplify ETL processes\n4. Integration testing and examples embedded into the docker\n\njust run:\n\n    docker run --rm --net=host lensesio/fast-data-dev\n\nThat's it. Visit <http://localhost:3030> to get into the fast-data-dev environment\n\n<img src=\"https://storage.googleapis.com/wch/fast-data-dev-ports.png\" alt=\"fast-data-dev web UI screenshot\" type=\"image/png\" width=\"320\">\n\nAll the service ports are exposed, and can be used from localhost / or within\nyour IntelliJ.  The kafka broker is exposed by default at port `9092`, zookeeper\nat port `2181`, schema registry at `8081`, connect at `8083`.  As an example, to\naccess the JMX data of the broker run:\n\n    jconsole localhost:9581\n\nIf you want to have the services remotely accessible, then you may need to pass\nin your machine's IP address or hostname that other machines can use to access\nit:\n\n    docker run --rm --net=host -e ADV_HOST=<IP> lensesio/fast-data-dev\n\n> Hit **control+c** to stop and remove everything\n\n<img src=\"https://storage.googleapis.com/wch/fast-data-dev-ui.png\" alt=\"fast-data-dev web UI screenshot\" type=\"image/png\" width=\"900\">\n\n### Mac and Windows users (docker-machine)\n\nCreate a VM with 4+GB RAM using Docker Machine:\n\n    docker-machine create --driver virtualbox --virtualbox-memory 4096 lensesio\n\n\nRun `docker-machine ls` to verify that the Docker Machine is running correctly. The command's output should be similar to:\n\n\n    $ docker-machine ls\n    NAME        ACTIVE   DRIVER       STATE     URL                         SWARM   DOCKER        ERRORS\n    lensesio     *        virtualbox   Running   tcp://192.168.99.100:2376           v17.03.1-ce\n\nConfigure your terminal to be able to use the new Docker Machine named lensesio:\n\n    eval $(docker-machine env lensesio)\n\nAnd run the Kafka Development Environment. Define ports, advertise the hostname and use extra parameters:\n\n    docker run --rm -p 2181:2181 -p 3030:3030 -p 8081-8083:8081-8083 \\\n           -p 9581-9585:9581-9585 -p 9092:9092 -e ADV_HOST=192.168.99.100 \\\n           lensesio/fast-data-dev:latest\n\nThat's it. Visit <http://192.168.99.100:3030> to get into the fast-data-dev environment\n\n### Run on the Cloud\n\nYou may want to quickly run a Kafka instance in GCE or AWS and access it from your local\ncomputer. Fast-data-dev has you covered.\n\nStart a VM in the respective cloud. You can use the OS of your choice, provided it has\na docker package. CoreOS is a nice choice as you get docker out of the box.\n\nNext you have to open the firewall, both for your machines but also *for the VM itself*.\nThis is important!\n\nOnce the firewall is open try:\n\n    docker run -d --net=host -e ADV_HOST=[VM_EXTERNAL_IP] \\\n               -e RUNNING_SAMPLEDATA=1 lensesio/fast-data-dev\n\nAlternatively just export the ports you need. E.g:\n\n    docker run -d -p 2181:2181 -p 3030:3030 -p 8081-8083:8081-8083 \\\n               -p 9581-9585:9581-9585 -p 9092:9092 -e ADV_HOST=[VM_EXTERNAL_IP] \\\n               -e RUNNING_SAMPLEDATA=1 lensesio/fast-data-dev\n\nEnjoy Kafka, Schema Registry, Connect, Lensesio UIs and Stream Reactor.\n\n### Customize execution\n\nFast-data-dev and kafka-lenses-dev support custom configuration and extra features\nvia environment variables.\n\n#### fast-data-dev / kafka-lenses-dev advanced configuration\n\n Optional Parameters              | Description\n--------------------------------- | ------------------------------------------------------------------------------------------------------------\n `CONNECT_HEAP=3G`                | Configure the maximum (`-Xmx`) heap size allocated to Kafka Connect. Useful when you want to start many connectors.\n `<SERVICE>_PORT=<PORT>`          | Custom port `<PORT>` for service, `0` will disable it. `<SERVICE>` one of `ZK`, `BROKER`, `BROKER_SSL`, `REGISTRY`, `REST`, `CONNECT`.\n `<SERVICE>_JMX_PORT=<PORT>`      | Custom JMX port `<PORT>` for service, `0` will disable it. `<SERVICE>` one of `ZK`, `BROKER`, `BROKER_SSL`, `REGISTRY`, `REST`, `CONNECT`.\n `USER=username`                  | Run in combination with `PASSWORD` to specify the username to use on basic auth.\n `PASSWORD=password`              | Protect the fast-data-dev UI when running publicly. If `USER` is not set, the default username is `kafka`.\n `SAMPLEDATA=0`                   | Do not create topics with sample avro and json records; (e.g do not create topics `sea_vessel_position_reports`, `reddit_posts`).\n `RUNNING_SAMPLEDATA=1`           | In the sample topics send a continuous (yet very low) flow of messages, so you can develop against live data.\n `RUNTESTS=0`                     | Disable the (coyote) integration tests from running when container starts.\n `FORWARDLOGS=0`                  | Disable running the file source connector that brings broker logs into a Kafka topic.\n `RUN_AS_ROOT=1`                  | Run kafka as `root` user - useful to i.e. test HDFS connector.\n `DISABLE_JMX=1`                  | Disable JMX - enabled by default on ports 9581 - 9585. You may also disable it individually for services.\n `ENABLE_SSL=1`                   | Generate a CA, key-certificate pairs and enable a SSL port on the broker.\n `SSL_EXTRA_HOSTS=IP1,host2`      | If SSL is enabled, extra hostnames and IP addresses to include to the broker certificate.\n `CONNECTORS=<CONNECTOR>[,<CON2>]`| Explicitly set which connectors* will be enabled. E.g `hbase`, `elastic` (Stream Reactor version)\n `DISABLE=<CONNECTOR>[,<CON2>]`   | Disable one or more connectors*. E.g `hbase`, `elastic` (Stream Reactor version), `elasticsearch` (Confluent version)\n `BROWSECONFIGS=1`                | Expose service configuration in the UI. Useful to see how Kafka is setup.\n `DEBUG=1`                        | Print stdout and stderr of all processes to container's stdout. Useful for debugging early container exits.\n `SUPERVISORWEB=1`                | Enable supervisor web interface on port 9001 (adjust via `SUPERVISORWEB_PORT`) in order to control services, run `tail -f`, etc.\n\n*Available connectors are: azure-documentdb, blockchain, bloomberg, cassandra,\ncoap, druid, elastic, elastic5, ftp, hazelcast, hbase, influxdb, jms, kudu,\nmongodb, mqtt, pulsar, redis, rethink, voltdb, couchbase, dbvisitreplicate,\ndebezium-mongodb, debezium-mysql, debezium-postgres, elasticsearch, hdfs,\njdbc, s3, twitter.\n\nTo programmatically get a list, run:\n\n    docker run --rm -it lensesio/fast-data-dev \\\n           find /opt/lensesio/connectors -type d -maxdepth 2 -name \"kafka-connect-*\"\n\nOptional Parameters (unsupported) | Description\n----------------------------------|---------------------------------------------------------------------------------------------------------\n`WEB_ONLY=1`                      | Run in combination with `--net=host` and docker will connect to the kafka services running on the local host. Please use our UI docker images instead.\n`TOPIC_DELETE=0`                  | Configure whether you can delete topics. By default topics can be deleted. Please use `KAFKA_DELETE_TOPIC_ENABLE=false` instead.\n\n\n#### Configure Kafka Components\n\nYou may configure any Kafka component (broker, schema registry, connect, rest proxy) by converting the configuration option to uppercase, replace dots with underscores and prepend with\n`<SERVICE>_`.\n\nAs example:\n\n- To set the `log.retention.bytes` for the broker, you would set the environment\n  variable `KAFKA_LOG_RETENTION_BYTES=1073741824`.\n- To set the `kafkastore.topic` for the schema registry, you would set\n  `SCHEMA_REGISTRY_KAFKASTORE_TOPIC=_schemas`.\n- To set the `plugin.path` for the connect worker, you would set\n  `CONNECT_PLUGIN_PATH=/var/run/connect/connectors/stream-reactor,/var/run/connect/connectors/third-party,/connectors`.\n- To set the `schema.registry.url` for the rest proxy, you would set\n  `KAFKA_REST_SCHEMA_REGISTRY_URL=http://localhost:8081`.\n\nWe also support the variables that set JVM options, such as `KAFKA_OPTS`, `SCHEMA_REGISTRY_JMX_OPTS`, etc.\n\nLensesio's Kafka Distribution (LKD) supports a few extra flags as well. Since in\nthe Apache Kafka build, both the broker and the connect worker expect JVM\noptions at the default `KAFKA_OPTS`, LKD supports using `BROKER_OPTS`, etc for\nthe broker and `CONNECT_OPTS`, etc for the connect worker. Of course\n`KAFKA_OPTS` are still supported and apply to both applications (and the\nembedded zookeeper).\n\nAnother LKD addition are the `VANILLA_CONNECT`, `SERDE_TOOLS` and\n`LENSESIO_COMMON` flags for Kafka Connect.  By default we load into the Connect\nClasspath the Schema Registry and Serde Tools by Confluent in order to support\navro and our own base jars in order to support avro and our connectors. You can\nchoose to run a completely vanilla kafka connect, the same that comes from the\nofficial distribution, without avro support by setting `VANILLA_CONNECT=1`.\nPlease note that most if not all the connectors will fail to load, so it would\nbe wise to disable them.  `SERDE_TOOLS=0` will disable Confluent's jars and\n`LENSESIO_COMMON=0` will disable our jars. Any of these is enough to support\navro, but disabling `LENSESIO_COMMON` will render Stream Reactor inoperable.\n\n### Versions\n\nThe latest version of this docker image tracks our latest stable tag (1.0.1). Our\nimages include:\n\n Version                       | Kafka Distro  | Lensesio tools | Apache Kafka  | Connectors\n-------------------------------| ------------- | -------------- | ------------- | --------------\nlensesio/fast-data-dev:3.6.1   | LKD 3.6.1-L0  |       ✓        |    3.6.1      | 20+ connectors\nlensesio/fast-data-dev:3.3.1   | LKD 3.3.1-L0  |       ✓        |    3.3.1      | 20+ connectors\nlensesio/fast-data-dev:2.6.2   | LKD 2.6.2-L0  |       ✓        |    2.6.2      | 30+ connectors\nlensesio/fast-data-dev:2.5.1   | LKD 2.5.1-L0  |       ✓        |    2.5.1      | 30+ connectors\nlensesio/fast-data-dev:2.4.1   | LKD 2.4.1-L0  |       ✓        |    2.4.1      | 30+ connectors\nlensesio/fast-data-dev:2.3.2   | LKD 2.3.2-L0  |       ✓        |    2.3.2      | 30+ connectors\nlensesio/fast-data-dev:2.2.1   | LKD 2.2.1-L0  |       ✓        |    2.2.1      | 30+ connectors\nlensesio/fast-data-dev:2.1.1   | LKD 2.1.1-L0  |       ✓        |    2.1.1      | 30+ connectors\nlensesio/fast-data-dev:2.0.1   | LKD 2.0.1-L0  |       ✓        |    2.0.1      | 30+ connectors\nlandoop/fast-data-dev:1.1.1    | LKD 1.1.1-L0  |       ✓        |    1.1.1      | 30+ connectors\nlandoop/fast-data-dev:1.0.1    | LKD 1.0.1-L0  |       ✓        |    1.0.1      | 30+ connectors\nlandoop/fast-data-dev:cp3.3.0  | CP 3.3.0 OSS  |       ✓        |    0.11.0.0   | 30+ connectors\nlandoop/fast-data-dev:cp3.2.2  | CP 3.2.2 OSS  |       ✓        |    0.10.2.1   | 24+ connectors\nlandoop/fast-data-dev:cp3.1.2  | CP 3.1.2 OSS  |       ✓        |    0.10.1.1   | 20+ connectors\nlandoop/fast-data-dev:cp3.0.1  | CP 3.0.1 OSS  |       ✓        |    0.10.0.1   | 20+ connectors\n\n*LKD stands for Lenses.io's Kafka Distribution. We build and package Apache Kafka with Kafka Connect\nand Apache Zookeeper, Confluent Schema Registry and REST Proxy and a collection of third party\nKafka Connectors as well as our own Stream Reactor collection.\n\nPlease note the [BSL license](https://lensesio.com/bsl/) of the tools. To use them on a PROD\ncluster with > 3 Kafka nodes, you should contact us.\n\n### Building it\n\nFast-data-dev and Lenses Box require a recent version of docker which supports\nmultistage builds. Optionally you should also enable the buildx plugin to enable\nmulti-arch builds, even if you just use the default builder.\n\nTo build it just run:\n\n    docker build -t lensesio-local/fast-data-dev .\n\nPeriodically pull from docker hub to refresh your cache.\n\nIf your docker version does not support multi-arch builds, or you don't have the\nbuildx plugin installed, use the build args demonstrated below to emulate\nmulti-arch support:\n\n    docker build --build-arg TARGETOS=linux --build-arg TARGETARCH=amd64 -t lensesio-local/fast-data-dev .\n\n\n### Advanced Features and Settings\n\n#### Custom Ports\n\nTo use custom ports for the various services, you can take advantage of the\n`ZK_PORT`, `BROKER_PORT`, `REGISTRY_PORT`, `REST_PORT`, `CONNECT_PORT` and\n`WEB_PORT` environment variables. One catch is that you can't swap ports; e.g\nto assign 8082 (default REST Proxy port) to the brokers.\n\n    docker run --rm -it \\\n               -p 3181:3181 -p 3040:3040 -p 7081:7081 \\\n               -p 7082:7082 -p 7083:7083 -p 7092:7092 \\\n               -e ZK_PORT=3181 -e WEB_PORT=3040 -e REGISTRY_PORT=8081 \\\n               -e REST_PORT=7082 -e CONNECT_PORT=7083 -e BROKER_PORT=7092 \\\n               -e ADV_HOST=127.0.0.1 \\\n               lensesio/fast-data-dev\n\nA port of `0` will disable the service.\n\n#### Execute kafka command line tools\n\nDo you need to execute kafka related console tools? Whilst your Kafka containers is running,\ntry something like:\n\n    docker run --rm -it --net=host lensesio/fast-data-dev kafka-topics --zookeeper localhost:2181 --list\n\nOr enter the container to use any tool as you like:\n\n    docker run --rm -it --net=host lensesio/fast-data-dev bash\n\n#### View logs\n\nYou can view the logs from the web interface. If you prefer the command line,\nevery application stores its logs under `/var/log` inside the container.\nIf you have your container's ID, or name, you could do something like:\n\n    docker exec -it <ID> cat /var/log/broker.log\n\n#### Enable SSL on Broker\n\nDo you want to test your application over an authenticated TLS connection to the\nbroker? We got you covered. Enable TLS via `-e ENABLE_SSL=1`:\n\n    docker run --rm --net=host \\\n               -e ENABLE_SSL=1 \\\n               lensesio/fast-data-dev\n\nWhen fast-data-dev spawns, it will create a self-signed CA. From that it will\ncreate a truststore and two signed key-certificate pairs, one for the broker,\none for your client. You can access the truststore and the client's keystore\nfrom our Web UI, under `/certs` (e.g http://localhost:3030/certs). The password\nfor both the keystores and the TLS key is `fastdata`.\nThe SSL port of the broker is `9093`, configurable via the `BROKER_SSL_PORT`\nvariable.\n\nHere is a simple example of how the SSL functionality can be used. Let's spawn\na fast-data-dev to act as the server:\n\n    docker run --rm --net=host -e ENABLE_SSL=1 -e RUNTESTS=0 lensesio/fast-data-dev\n\nOn a new console, run another instance of fast-data-dev only to get access to\nKafka command line utilities and use TLS to connect to the broker of the former\ncontainer:\n\n    docker run --rm -it --net=host --entrypoint bash lensesio/fast-data-dev\n    root@fast-data-dev / $ wget localhost:3030/certs/truststore.jks\n    root@fast-data-dev / $ wget localhost:3030/certs/client.jks\n    root@fast-data-dev / $ kafka-producer-perf-test --topic tls_test \\\n      --throughput 100000 --record-size 1000 --num-records 2000 \\\n      --producer-props bootstrap.servers=\"localhost:9093\" security.protocol=SSL \\\n      ssl.keystore.location=client.jks ssl.keystore.password=fastdata \\\n      ssl.key.password=fastdata ssl.truststore.location=truststore.jks \\\n      ssl.truststore.password=fastdata\n\nSince the plaintext port is also available, you can test both and find out\nwhich is faster and by how much. ;)\n\n\n### Advanced Connector settings\n\n#### Explicitly Enable Connectors\n\nThe number of connectors present significantly affects Kafka Connect's\nstartup time, as well as its memory usage. You can enable connectors\nexplicitly using the `CONNECTORS` environment variable:\n\n    docker run --rm -it --net=host \\\n               -e CONNECTORS=jdbc,elastic,hbase \\\n               lensesio/fast-data-dev\n\nPlease note that if you don't enable jdbc, some tests will fail.\nThis doesn't affect fast-data-dev's operation.\n\n#### Explicitly Disable Connectors\n\nFollowing the same logic as in the paragraph above, you can instead choose to\nexplicitly disable certain connectors using the `DISABLE` environment\nvariable. It takes a comma separated list of connector names you want to\ndisable:\n\n    docker run --rm -it --net=host \\\n               -e DISABLE=elastic,hbase \\\n               lensesio/fast-data-dev\n\nIf you disable the jdbc connector, some tests will fail to run.\n\n#### Enable additional connectors\n\nIf you have a custom connector you would like to use, you can mount it at folder\n`/connectors`. `plugin.path` variable for Kafka Connect is set up to include\n`/connectors/`, so it will use any single-jar connectors it will find inside this\ndirectory and any multi-jar connectors it will find in subdirectories of this directory.\n\n    docker run --rm -it --net=host \\\n               -v /path/to/my/connector/connector.jar:/connectors/connector.jar \\\n               -v /path/to/my/multijar-connector-directory:/connectors/multijar-connector-directory \\\n               lensesio/fast-data-dev\n\n### FAQ\n\n- Lensesio's Fast Data Web UI tools and integration test requires some time\n  till they fully work. Especially the tests and Kafka Connect UI will need\n  a few minutes.\n  \n  That is because the services (Kafka, Schema Registry, Kafka Connect, REST Proxy)\n  have to start and initialize before the UIs can read data.\n- What resources does this container need?\n  \n  An idle, fresh container will need about 3GiB of RAM. As at least 5 JVM\n  applications will be working in it, your mileage will vary. In our\n  experience Kafka Connect usually requires a lot of memory. It's heap size is\n  set by default to 640MiB but you'll might need more than that.\n- Fast-data-dev does not start properly, broker fails with:\n  > [2016-08-23 15:54:36,772] FATAL [Kafka Server 0], Fatal error during\n  > KafkaServer startup. Prepare to shutdown (kafka.server.KafkaServer)\n  > java.net.UnknownHostException: [HOSTNAME]: [HOSTNAME]: unknown error\n  \n  JVM based apps tend to be a bit sensitive to hostname issues.\n  Either run the image without `--net=host` and expose all ports\n  (2181, 3030, 8081, 8082, 8083, 9092) to the same port at the host, or\n  better yet make sure your hostname resolve to the localhost address\n  (127.0.0.1). Usually to achieve this, you need to add your hostname (case\n  sensitive) at `/etc/hosts` as the first name after 127.0.0.1. E.g:\n  \n      127.0.0.1 MyHost localhost\n\n### Detailed configuration options\n\n#### Web Only Mode\n\n*Note:* Web only mode will be deprecated in the future.\n\nThis is a special mode only for Linux hosts, where *only* Lensesio's Web UIs\nare started and kafka services are expected to be running on the local\nmachine. It must be run with `--net=host` flag, thus the Linux only\nrequisite:\n\n    docker run --rm -it --net=host \\\n               -e WEB_ONLY=true \\\n               lensesio/fast-data-dev\n\nThis is useful if you already have a Kafka cluster and want just the additional Lensesio Fast Data web UI.\n_Please note that we provide separate, lightweight docker images for each UI component\nand we strongly encourage to use these over fast-data-dev._\n\n#### Connect Heap Size\n\nYou can configure Connect's heap size via the environment variable\n`CONNECT_HEAP`. The default is `640M`:\n\n    docker run -e CONNECT_HEAP=3G -d lensesio/fast-data-dev\n\n#### Basic Auth (password)\n\nWe have included a web server to serve Lensesio UIs and proxy the schema registry\nand kafa REST proxy services, in order to share your docker over the web.\nIf you want some basic protection, pass the `PASSWORD` variable and the web\nserver will be protected by user `kafka` with your password. If you want to\nsetup the username too, set the `USER` variable.\n\n     docker run --rm -it -p 3030:3030 \\\n                -e PASSWORD=password \\\n                lensesio/fast-data-dev\n\n#### Disable tests\n\nBy default this docker runs a set of coyote tests, to ensure that your container\nand development environment is all set up. You can disable running the `coyote` tests\nusing the flag:\n\n    -e RUNTESTS=0\n\n#### Run Kafka as root\n\nIn the recent versions of fast-data-dev, we switched to running Kafka as user\n`nobody` instead of `root` since it was a bad practice. The old behaviour may\nstill be desirable, for example on our\n[HDFS connector tests](http://coyote.lensesio.com/connect/kafka-connect-hdfs/),\nConnect worker needs to run as the root user in order to be able to write to the\nHDFS. To switch to the old behaviour, use:\n\n    -e RUN_AS_ROOT=1\n\n#### JMX Metrics\n\nJMX metrics are enabled by default. If you want to disable them for some\nreason (e.g you need the ports for other purposes), use the `DISABLE_JMX`\nenvironment variable:\n\n    docker run --rm -it --net=host \\\n               -e DISABLE_JMX=1 \\\n               lensesio/fast-data-dev\n\nJMX ports are hardcoded to `9581` for the broker, `9582` for schema registry,\n`9583` for REST proxy and `9584` for connect distributed. Zookeeper is exposed\nat `9585`.\n"
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "filesystem",
          "type": "tree",
          "content": null
        },
        {
          "name": "hooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "lkd",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup-and-run.sh",
          "type": "blob",
          "size": 27.2958984375,
          "content": "#!/usr/bin/env bash\n\nTRUE_REG='^([tT][rR][uU][eE]|[yY]|[yY][eE][sS]|1)$'\nFALSE_REG='^([fF][aA][lL][sS][eE]|[nN]|[nN][oO]|0)$'\n\nDEBUG_SCRIPT=${DEBUG_SCRIPT:-false}\nif [[ $DEBUG_SCRIPT =~ $TRUE_REG ]]; then\n    set -o xtrace\n    printenv\nfi\n\nSTRICT_SCRIPT=${STRICT_SCRIPT:-true}\nif [[ $STRICT_SCRIPT =~ $TRUE_REG ]]; then\n    set -o errexit\n    set -o nounset\n    set -o pipefail\nfi\n\n# Run pre-setup user provided scripts.\nexport PRE_SETUP=${PRE_SETUP:-}\nexport PRE_SETUP_FILE=${PRE_SETUP_FILE:-}\nexport PRE_SETUP_URL=${PRE_SETUP_URL:-}\nif [[ -n \"$PRE_SETUP\" ]]; then\n    $PRE_SETUP\nfi\nif [[ -n \"$PRE_SETUP_FILE\" ]]; then\n    if [[ ! -f \"$PRE_SETUP_FILE\" ]]; then\n        echo \"Although PRE_SETUP_FILE is set, I cannot find the file.\"\n    fi\n    source $PRE_SETUP_FILE\nfi\nif [[ -n \"$PRE_SETUP_URL\" ]]; then\n    curl \"$PRE_SETUP_URL\" --silent --show-error --fail --output /tmp/PRE_SETUP_URL\n    source /tmp/PRE_SETUP_URL\n    rm /tmp/PRE_SETUP_URL\nfi\n\n# Default values\nexport ZK_PORT=${ZK_PORT:-2181}\nexport ZK_JMX_PORT=${ZK_JMX_PORT:-9585}\nexport BROKER_PORT=${BROKER_PORT:-9092}\nexport BROKER_JMX_PORT=${BROKER_JMX_PORT:-9581}\nexport BROKER_SSL_PORT=${BROKER_SSL_PORT:-9093}\nexport REGISTRY_PORT=${REGISTRY_PORT:-8081}\nexport REGISTRY_JMX_PORT=${REGISTRY_JMX_PORT:-9582}\nexport CONNECT_PORT=${CONNECT_PORT:-8083}\nexport CONNECT_JMX_PORT=${CONNECT_JMX_PORT:-9584}\nexport REST_PORT=${REST_PORT:-8082}\nexport REST_JMX_PORT=${REST_JMX_PORT:-9583}\nexport WEB_PORT=${WEB_PORT:-3030}\nRUN_AS_ROOT=${RUN_AS_ROOT:-false}\nDISABLE_JMX=${DISABLE_JMX:-false}\nENABLE_SSL=${ENABLE_SSL:-false}\nSSL_EXTRA_HOSTS=${SSL_EXTRA_HOSTS:-}\nDEBUG=${DEBUG:-false}\nexport SAMPLEDATA=${SAMPLEDATA:-1}\nexport RUNNING_SAMPLEDATA=${RUNNING_SAMPLEDATA:-0}\nDISABLE=${DISABLE:-}\nCONNECTORS=${CONNECTORS:-}\nexport ADV_HOST=${ADV_HOST:-}\nexport ADV_HOST_JMX=${ADV_HOST_JMX:-${ADV_HOST}}\nexport ADV_HOST_JMX=${ADV_HOST_JMX:-127.0.0.1}\nCONNECT_HEAP=${CONNECT_HEAP:-}\nWEB_ONLY=${WEB_ONLY:-0}\nexport FORWARDLOGS=${FORWARDLOGS:-1}\nexport RUNTESTS=${RUNTESTS:-1}\nexport BROWSECONFIGS=${BROWSECONFIGS:-1}\nexport SUPERVISORWEB=${SUPERVISORWEB:-0}\nexport SUPERVISORWEB_PORT=${SUPERVISORWEB_PORT:-9001}\nexport WEB_TERMINAL_PORT=${WEB_TERMINAL_PORT:-0}\nexport DEBUG_AUTH=${DEBUG_AUTH:-0}\nexport WAIT_SCRIPT_BROKER=${WAIT_SCRIPT_BROKER:-/usr/local/share/lensesio/wait-scripts/wait-for-zookeeper.sh}\nexport WAIT_SCRIPT_REGISTRY=${WAIT_SCRIPT_REGISTRY:-/usr/local/share/lensesio/wait-scripts/wait-for-kafka.sh}\nexport WAIT_SCRIPT_CONNECT=${WAIT_SCRIPT_CONNECT:-/usr/local/share/lensesio/wait-scripts/wait-for-registry.sh}\nexport WAIT_SCRIPT_RESTPROXY=${WAIT_SCRIPT_RESTPROXY:-/usr/local/share/lensesio/wait-scripts/wait-for-registry.sh}\n\n# These ports are always used.\nPORTS=\"$ZK_PORT $BROKER_PORT $REGISTRY_PORT $REST_PORT $CONNECT_PORT $WEB_PORT\"\n\n# Export versions so envsubst will work\nsource build.info\n# shellcheck disable=SC2046\nexport $(cut -d= -f1 /build.info)\n\n# Set env vars to configure Kafka\nexport KAFKA_BROKER_ID=${KAFKA_BROKER_ID:-0}\nexport KAFKA_NUM_NETWORK_THREADS=${KAFKA_NUM_NETWORK_THREADS:-2}\nexport KAFKA_NUM_IO_THREADS=${KAFKA_NUM_IO_THREADS:-4}\nexport KAFKA_LOG_DIRS=${KAFKA_LOG_DIRS:-/data/kafka/logdir}\n#export KAFKA_NUM_PARTITIONS=${KAFKA_NUM_PARTITIONS:-1}\nexport KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR=${KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR:-1}\nexport KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR=${KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR:-1}\nexport KAFKA_TRANSACTION_STATE_LOG_MIN_ISR=${KAFKA_TRANSACTION_STATE_LOG_MIN_ISR:-1}\nexport KAFKA_LOG_RETENTION_HOURS=${KAFKA_LOG_RETENTION_HOURS:-168}\nexport KAFKA_LOG_SEGMENT_BYTES=${KAFKA_LOG_SEGMENT_BYTES:-104857600}\nexport KAFKA_ZOOKEEPER_CONNECT=${KAFKA_ZOOKEEPER_CONNECT:-127.0.0.1:$ZK_PORT}\nexport KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS=${KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS:-1000}\nexport KAFKA_LISTENERS=${KAFKA_LISTENERS:-PLAINTEXT://:$BROKER_PORT}\nexport KAFKA_DELETE_TOPIC_ENABLE=${KAFKA_DELETE_TOPIC_ENABLE:-true}\nexport KAFKA_ADVERTISED_LISTENERS=${KAFKA_ADVERTISED_LISTENERS:-}\nexport BROKER_JMX_OPTS=${BROKER_JMX_OPTS:--Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Djava.rmi.server.hostname=$ADV_HOST_JMX -Dcom.sun.management.jmxremote.rmi.port=$BROKER_JMX_PORT}\nexport BROKER_LOG4J_OPTS=${BROKER_LOG4J_OPTS:--Dlog4j.configuration=file:/var/run/broker/log4j.properties}\n\n# Set env vars to configure Schema Registry\nexport SCHEMA_REGISTRY_LISTENERS=${SCHEMA_REGISTRY_LISTENERS:-http://0.0.0.0:$REGISTRY_PORT}\nexport SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS=${SCHEMA_REGISTRY_KAFKASTORE_BOOTSTRAP_SERVERS:-PLAINTEXT://:$BROKER_PORT}\nexport SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS=${SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_METHODS:-GET,POST,PUT,DELETE,OPTIONS}\nexport SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN=${SCHEMA_REGISTRY_ACCESS_CONTROL_ALLOW_ORIGIN:-*}\nexport SCHEMA_REGISTRY_JMX_OPTS=${SCHEMA_REGISTRY_JMX_OPTS:--Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Djava.rmi.server.hostname=$ADV_HOST_JMX -Dcom.sun.management.jmxremote.rmi.port=$REGISTRY_JMX_PORT}\nexport SCHEMA_REGISTRY_LOG4J_OPTS=${SCHEMA_REGISTRY_JMX_OPTS:--Dlog4j.configuration=file:/var/run/schema-registry/log4j.properties}\n\n# Set env vars for Kafka Connect Distributed\nexport CONNECT_BOOTSTRAP_SERVERS=${CONNECT_BOOTSTRAP_SERVERS:-PLAINTEXT://127.0.0.1:$BROKER_PORT}\nexport CONNECT_GROUP_ID=${CONNECT_GROUP_ID:-connect-fast-data}\nexport CONNECT_KEY_CONVERTER=${CONNECT_KEY_CONVERTER:-io.confluent.connect.avro.AvroConverter}\nexport CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL=${CONNECT_KEY_CONVERTER_SCHEMA_REGISTRY_URL:-http://127.0.0.1:$REGISTRY_PORT}\nexport CONNECT_VALUE_CONVERTER=${CONNECT_VALUE_CONVERTER:-io.confluent.connect.avro.AvroConverter}\nexport CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL=${CONNECT_VALUE_CONVERTER_SCHEMA_REGISTRY_URL:-http://127.0.0.1:$REGISTRY_PORT}\nexport CONNECT_CONFIG_STORAGE_TOPIC=${CONNECT_CONFIG_STORAGE_TOPIC:-connect-configs}\nexport CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR=${CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR:-1}\nexport CONNECT_CONFIG_STORAGE_PARTITIONS=1\nexport CONNECT_OFFSET_STORAGE_TOPIC=${CONNECT_OFFSET_STORAGE_TOPIC:-connect-offsets}\nexport CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR=${CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR:-1}\nexport CONNECT_OFFSET_STORAGE_PARTITIONS=${CONNECT_OFFSET_STORAGE_PARTITIONS:-5}\nexport CONNECT_STATUS_STORAGE_TOPIC=${CONNECT_STATUS_STORAGE_TOPIC:-connect-statuses}\nexport CONNECT_STATUS_STORAGE_REPLICATION_FACTOR=${CONNECT_STATUS_STORAGE_REPLICATION_FACTOR:-1}\nexport CONNECT_STATUS_STORAGE_PARTITIONS=${CONNECT_STATUS_STORAGE_PARTITIONS:-3}\nexport CONNECT_ACCESS_CONTROL_ALLOW_METHODS=${CONNECT_ACCESS_CONTROL_ALLOW_METHODS:-GET,POST,PUT,DELETE,OPTIONS}\nexport CONNECT_ACCESS_CONTROL_ALLOW_ORIGIN=${CONNECT_ACCESS_CONTROL_ALLOW_ORIGIN:-*}\nexport CONNECT_PLUGIN_PATH=${CONNECT_PLUGIN_PATH:-/var/run/connect/connectors/stream-reactor,/var/run/connect/connectors/third-party,/connectors}\nexport CONNECT_REST_PORT=${CONNECT_REST_PORT:-$CONNECT_PORT}\nexport CONNECT_REST_ADVERTISED_HOST_NAME=${CONNECT_REST_ADVERTISED_HOST_NAME:-}\nexport CONNECT_JMX_OPTS=${CONNECT_JMX_OPTS:--Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Djava.rmi.server.hostname=$ADV_HOST_JMX -Dcom.sun.management.jmxremote.rmi.port=$CONNECT_JMX_PORT}\nexport CONNECT_LOG4J_OPTS=${CONNECT_LOG4J_OPTS:--Dlog4j.configuration=file:/var/run/connect/connect-log4j.properties}\n\n# Set env vars for REST Proxy\nexport KAFKA_REST_BOOTSTRAP_SERVERS=${KAFKA_REST_BOOTSTRAP_SERVERS:-PLAINTEXT://127.0.0.1:$BROKER_PORT}\nexport KAFKA_REST_ACCESS_CONTROL_ALLOW_METHODS=${KAFKA_REST_ACCESS_CONTROL_ALLOW_METHODS:-GET,POST,PUT,DELETE,OPTIONS}\nexport KAFKA_REST_ACCESS_CONTROL_ALLOW_ORIGIN=${KAFKA_REST_ACCESS_CONTROL_ALLOW_ORIGIN:-*}\nexport KAFKA_REST_LISTENERS=${KAFKA_REST_LISTENERS:-http://0.0.0.0:$REST_PORT}\nexport KAFKA_REST_SCHEMA_REGISTRY_URL=${KAFKA_REST_SCHEMA_REGISTRY_URL:-http://127.0.0.1:$REGISTRY_PORT}\n# Next two lines are a fix for REST Proxy\nexport KAFKA_REST_CONSUMER_REQUEST_TIMEOUT_MS=${KAFKA_REST_CONSUMER_REQUEST_TIMEOUT_MS:-20000}\nexport KAFKA_REST_CONSUMER_MAX_POLL_INTERVAL_MS=${KAFKA_REST_CONSUMER_MAX_POLL_INTERVAL_MS:-18000}\nexport KAFKA_REST_ZOOKEEPER_CONNECT=${KAFKA_REST_ZOOKEEPER_CONNECT:-127.0.0.1:$ZK_PORT}\nexport KAFKAREST_JMX_OPTS=${KAFKA_REST_JMX_OPTS:-}\nexport KAFKAREST_JMX_OPTS=${KAFKAREST_JMX_OPTS:--Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Djava.rmi.server.hostname=$ADV_HOST_JMX -Dcom.sun.management.jmxremote.rmi.port=$REST_JMX_PORT}\nexport KAFKAREST_LOG4J_OPTS=${KAFKA_REST_LOG4J_OPTS:-}\nexport KAFKAREST_LOG4J_OPTS=${KAFKAREST_LOG4J_OPTS:--Dlog4j.configuration=file:/var/run/rest-proxy/log4j.properties}\n\n# Set env vars for ZOOKEEPER\nexport ZOOKEEPER_dataDir=${ZOOKEEPER_dataDir:-/data/zookeeper}\nexport ZOOKEEPER_clientPort=${ZOOKEEPER_clientPort:-$ZK_PORT}\nexport ZOOKEEPER_maxClientCnxns=${ZOOKEEPER_maxClientCnxnxs:-0}\nZOOKEEPER_OPTS=${ZOOKEEPER_OPTS:-}\nexport ZOOKEEPER_OPTS=\"${ZOOKEEPER_OPTS} -Dzookeeper.4lw.commands.whitelist=ruok,dump\"\nexport ZOOKEEPER_LOG4J_OPTS=${ZOOKEEPER_LOG4J_OPTS:--Dlog4j.configuration=file:/var/run/zookeeper/log4j.properties}\nexport ZOOKEEPER_JMX_OPTS=${ZOOKEEPER_JMX_OPTS:--Dcom.sun.management.jmxremote -Dcom.sun.management.jmxremote.authenticate=false -Dcom.sun.management.jmxremote.ssl=false -Dcom.sun.management.jmxremote.local.only=false -Djava.rmi.server.hostname=$ADV_HOST_JMX -Dcom.sun.management.jmxremote.rmi.port=$ZK_JMX_PORT}\n\n# Set env vars for generator\nexport GENERATOR_BROKER=${GENERATOR_BROKER:-127.0.0.1:$BROKER_PORT}\nexport GENERATOR_ZK_HOST=${GENERATOR_ZK_HOST:-127.0.0.1}\nexport GENERATOR_SCHEMA_REGISTRY_URL=${GENERATOR_SCHEMA_REGISTRY_URL:-http://127.0.0.1:$REGISTRY_PORT}\n\n# Set memory limits\n# Set connect heap size if needed\nif [[ -n $CONNECT_HEAP ]]; then CONNECT_HEAP=\"-Xmx$CONNECT_HEAP\"; fi\nCONNECT_HEAP_OPTS=${CONNECT_HEAP_OPTS:-$CONNECT_HEAP}\nexport CONNECT_HEAP_OPTS=${CONNECT_HEAP_OPTS:--Xmx640M -Xms128M}\nexport BROKER_HEAP_OPTS=${BROKER_HEAP_OPTS:--Xmx320M -Xms320M}\nexport ZOOKEEPER_HEAP_OPTS=${ZOOKEEPER_HEAP_OPTS:--Xmx256M -Xms64M}\nexport SCHEMA_REGISTRY_HEAP_OPTS=${SCHEMA_REGISTRY_HEAP_OPTS:--Xmx256M -Xms128M}\nexport KAFKA_REST_HEAP_OPTS=${KAFKA_REST_HEAP_OPTS:--Xmx256M -Xms128M}\n\n# Configure JMX if needed or disable it.\nif [[ ! $DISABLE_JMX =~ $TRUE_REG ]]; then\n    # If JMX is not disabled, we should check for port availability\n    PORTS=\"$PORTS $BROKER_JMX_PORT $REGISTRY_JMX_PORT $REST_JMX_PORT $CONNECT_JMX_PORT $ZK_JMX_PORT\"\nelse\n    # This does not really disable JMX, but each service will start JMX\n    # in an ephemeral port, so it won't cause issues to the process.\n    export ZK_JMX_PORT=0\n    export BROKER_JMX_PORT=0\n    export REGISTRY_JMX_PORT=0\n    export CONNECT_JMX_PORT=0\n    export REST_JMX_PORT=0\nfi\n\n# Create run directories for various services and initialize where applicable with configuration files.\nmkdir -p \\\n      /var/run/zookeeper \\\n      /var/run/broker \\\n      /var/run/schema-registry \\\n      /var/run/connect \\\n      /var/run/connect/connectors/{stream-reactor,third-party} \\\n      /var/run/rest-proxy \\\n      /var/run/coyote \\\n      /var/run/caddy \\\n      /data/{zookeeper,kafka}\nchmod 777 /data/{zookeeper,kafka}\n\n# Copy log4j files\ncp /opt/lensesio/kafka/etc/kafka/log4j.properties \\\n   /var/run/zookeeper/\ncp /opt/lensesio/kafka/etc/kafka/log4j.properties \\\n   /var/run/broker/\ncp /opt/lensesio/kafka/etc/schema-registry/log4j.properties \\\n   /var/run/schema-registry/\ncp /opt/lensesio/kafka/etc/kafka/connect-log4j.properties \\\n   /var/run/connect/\ncp /opt/lensesio/kafka/etc/kafka-rest/log4j.properties \\\n   /var/run/rest-proxy/\n\n# Copy tests\n# This differs in that we need to adjust it later\ncp /opt/lensesio/tools/share/coyote/examples/simple-integration-tests.yml \\\n   /var/run/coyote/simple-integration-tests.yml\n## Fix ports for integration-tests\nsed -e \"s/3030/$WEB_PORT/\" \\\n    -e \"s/2181/$ZK_PORT/\" \\\n    -e \"s/9092/$BROKER_PORT/\" \\\n    -e \"s/8081/$REGISTRY_PORT/\" \\\n    -e \"s/8082/$REST_PORT/\" \\\n    -e \"s/8083/$CONNECT_PORT/\" \\\n    -i /var/run/coyote/simple-integration-tests.yml\n\n# Copy other templated files (caddy, logs-to-kafka, env.js)\nenvsubst < /usr/local/share/lensesio/etc/Caddyfile               > /var/run/caddy/Caddyfile\nenvsubst < /usr/local/share/lensesio/etc/fast-data-dev-ui/env.js > /var/www/env.js\n\n# Set ADV_HOST if needed\nif [[ -n ${ADV_HOST} ]]; then\n    echo -e \"\\e[92mSetting advertised host to \\e[96m${ADV_HOST}\\e[34m\\e[92m.\\e[34m\"\n    if [[ -z ${KAFKA_ADVERTISED_LISTENERS} ]]; then\n        export KAFKA_ADVERTISED_LISTENERS=\"PLAINTEXT://${ADV_HOST}:${BROKER_PORT}\"\n        # If SSL is enabled we need to know if we should update the advertised.listeners\n        TMP_ADV_LISTENERS_SET=true\n    else\n        TMP_ADV_LISTENERS_SET=false\n    fi\n    if [[ -z $CONNECT_REST_ADVERTISED_HOST_NAME ]]; then\n        export CONNECT_REST_ADVERTISED_HOST_NAME=${ADV_HOST}\n    fi\n    sed -e \"s#127.0.0.1#${ADV_HOST}#g\" -i /var/run/coyote/simple-integration-tests.yml /var/www/env.js\nfi\n\n# setup Kafka (and components)\nsource /usr/local/share/lensesio/config_kafka.sh\n\n# setup supervisord\nfor service in /usr/local/share/lensesio/etc/supervisord.templates.d/*.conf; do\n    # shellcheck disable=SC2094\n    envsubst < \"$service\" > /etc/supervisord.d/\"$(basename \"$service\")\"\ndone\n# Disable services if asked\nif [[ $ZK_PORT == 0 ]] || [[ $GENERATOR_ZK_HOST != \"127.0.0.1\" ]];       then rm /etc/supervisord.d/*zookeeper.conf; fi\nif [[ $BROKER_PORT == 0 ]];   then rm /etc/supervisord.d/*broker.conf; fi\nif [[ $REGISTRY_PORT == 0 ]]; then rm /etc/supervisord.d/*schema-registry.conf; fi\nif [[ $CONNECT_PORT == 0 ]];  then rm /etc/supervisord.d/*connect-distributed.conf; fi\nif [[ $REST_PORT == 0 ]];     then rm /etc/supervisord.d/*rest-proxy.conf; fi\nif [[ $WEB_PORT == 0 ]];      then rm /etc/supervisord.d/*caddy.conf; fi\nif [[ $WEB_TERMINAL_PORT == 0 ]];      then rm /etc/supervisord.d/*gotty-web-terminal.conf; fi\nif [[ $FORWARDLOGS =~ $FALSE_REG ]]; then rm /etc/supervisord.d/*logs-to-kafka.conf; fi\nif [[ $RUNTESTS =~ $FALSE_REG ]]; then\n    rm /etc/supervisord.d/*smoke-tests.conf\n    cat <<EOF > /var/www/coyote-tests/results\n{\n  \"passed\": -1,\n  \"failed\": 0\n}\nEOF\nfi\n\n# Set webserver basicauth username and password\nUSER=${USER:-kafka}\nPASSWORD=${PASSWORD:-}\nexport USER\nif [[ ! -z $PASSWORD ]]; then\n    echo -e \"\\e[92mEnabling login credentials '\\e[96m${USER}\\e[34m\\e[92m' '\\e[96mxxxxxxxx'\\e[34m\\e[92m.\\e[34m\"\n    echo \"basicauth / \\\"${USER}\\\" \\\"${PASSWORD}\\\"\" >> /var/run/caddy/Caddyfile\nfi\n# If BROWSECONFIGS, expose configs under /config\nif [[ $BROWSECONFIGS =~ $TRUE_REG ]]; then\n    rm -f /var/www/config\n    ln -s /var/run /var/www/config\n    echo \"browse /config\" >> /var/run/caddy/Caddyfile\n    sed -e 's/browseconfigs/\"enabled\" : true/' -i /var/www/env.js\nelse\n    sed -e 's/browseconfigs/\"enabled\" : false/' -i /var/www/env.js\nfi\n# If SUPERVISORWEB, enable supervisor control and proxy it\nif [[ $SUPERVISORWEB =~ $TRUE_REG ]]; then\n    cat <<EOF > /etc/supervisord.d/99-supervisorctl.conf\n[inet_http_server]\nport=*:$SUPERVISORWEB_PORT\nEOF\n    PORTS=\"$PORTS $SUPERVISORWEB_PORT\"\n    if [[ ! -z $PASSWORD ]]; then\n        echo -e \"\\e[92Adding login credentials to supervisor '\\e[96m${USER}\\e[34m\\e[92m' '\\e[96mxxxxxxxx'\\e[34m\\e[92m.\\e[34m\"\n        echo \"username=$USER\" >> /etc/supervisord.d/99-supervisorctl.conf\n        echo \"password=$PASSWORD\" >> /etc/supervisord.d/99-supervisorctl.conf\n    fi\n\n    # These does not work, because supervisor server\n    # can only live under webroot.\n#     cat <<EOF >> /var/run/caddy/Caddyfile\n# proxy /control 0.0.0.0:$SUPERVISORWEB_PORT {\n#     without /control\n# }\n# EOF\n    sed -e 's/supervisorweb/\"enabled\" : true/' -i  /var/www/env.js\nelse\n    sed -e 's/supervisorweb/\"enabled\" : false/' -i  /var/www/env.js\nfi\n\n# Cleanup previous starts\nrm -f /var/run/connect/connectors/{stream-reactor,third-party}/*\n# Disable Connectors\nOLD_IFS=$IFS\nIFS=,\nif [[ -z $CONNECTORS ]] && [[ -z $DISABLE ]]; then\n    DISABLE=\"random_string_hope_not_a_connector_name\"\nfi\nif [[ -n $DISABLE ]]; then\n    DISABLE=\" ${DISABLE//,/ } \"\n    CONNECTOR_LIST=\"$(find /opt/lensesio/connectors/stream-reactor -maxdepth 1 -name \"kafka-connect-*\" -type d | sed -e 's/.*kafka-connect-//' | tr '\\n' ',')\"\n    for connector in $CONNECTOR_LIST; do\n        connectorTest=\" $connector \"\n        if [[ $DISABLE =~ $connectorTest ]]; then\n            echo \"Skipping connector: kafka-connect-${connector}\"\n        else\n            ln -s /opt/lensesio/connectors/stream-reactor/kafka-connect-\"${connector}\" \\\n               /var/run/connect/connectors/stream-reactor/kafka-connect-\"${connector}\"\n        fi\n    done\n    CONNECTOR_LIST=\"$(find /opt/lensesio/connectors/third-party -maxdepth 1 -name \"kafka-connect-*\" -type d | sed -e 's/.*kafka-connect-//' | tr '\\n' ',')\"\n    for connector in $CONNECTOR_LIST; do\n        connectorTest=\" $connector \"\n        if [[ $DISABLE =~ $connectorTest ]]; then\n            echo \"Skipping connector: kafka-connect-${connector}\"\n        else\n            ln -s /opt/lensesio/connectors/third-party/kafka-connect-\"${connector}\" \\\n               /var/run/connect/connectors/third-party/kafka-connect-\"${connector}\"\n        fi\n    done\nfi\n# Enable Connectors\nif [[ -n $CONNECTORS ]]; then\n    CONNECTORS=\" ${CONNECTORS//,/ } \"\n    CONNECTOR_LIST=\"$(find /opt/lensesio/connectors/stream-reactor -maxdepth 1 -name \"kafka-connect-*\" -type d | sed -e 's/.*kafka-connect-//' | tr '\\n' ',')\"\n    for connector in $CONNECTOR_LIST; do\n        connectorTest=\" $connector \"\n        if [[ $CONNECTORS =~ $connectorTest ]]; then\n            echo \"Enabling connector: kafka-connect-${connector}\"\n            ln -s /opt/lensesio/connectors/stream-reactor/kafka-connect-\"${connector}\" \\\n               /var/run/connect/connectors/stream-reactor/kafka-connect-\"${connector}\"\n        fi\n    done\n    CONNECTOR_LIST=\"$(find /opt/lensesio/connectors/third-party -maxdepth 1 -name \"kafka-connect-*\" -type d | sed -e 's/.*kafka-connect-//' | tr '\\n' ',')\"\n    for connector in $CONNECTOR_LIST; do\n        connectorTest=\" $connector \"\n        if [[ $CONNECTORS =~ $connectorTest ]]; then\n            echo \"Enabling connector: kafka-connect-${connector}\"\n            ln -s /opt/lensesio/connectors/third-party/kafka-connect-\"${connector}\" \\\n               /var/run/connect/connectors/third-party/kafka-connect-\"${connector}\"\n        fi\n    done\nfi\nIFS=\"$OLD_IFS\"\n\n# Enable root-mode if needed\nif [[ $RUN_AS_ROOT =~ $TRUE_REG ]]; then\n    sed -e 's/user=nobody/;user=nobody/' -i /etc/supervisord.d/*\n    echo -e \"\\e[92mRunning Kafka as root.\\e[34m\"\nfi\n\n# SSL setup\nif [[ $ENABLE_SSL =~ $TRUE_REG ]]; then\n    PORTS=\"$PORTS $BROKER_SSL_PORT\"\n    echo -e \"\\e[92mTLS enabled.\\e[34m\"\n    if [[ -f /tmp/certs/kafka.jks ]] \\\n           && [[ -f /tmp/certs/client.jks ]] \\\n           && [[ -f /tmp/certs/truststore.jks ]]; then\n        echo -e \"\\e[92mOld keystores and truststore found, skipping creation of new ones.\\e[34m\"\n        {\n            pushd /tmp/certs\n            mkdir -p /var/www/certs/\n            cp client.jks truststore.jks /var/www/certs/\n            popd\n        } >>/var/log/ssl-setup.log 2>&1\n    else\n        echo -e \"\\e[92mCreating CA and key-cert pairs.\\e[34m\"\n        {\n            mkdir -p /tmp/certs\n            pushd /tmp/certs\n            # Create Lensesio Fast Data Dev CA\n            quickcert -ca -out lfddca. -CN \"Lensesio's Fast Data Dev Self Signed Certificate Authority\"\n            SSL_HOSTS=\"127.0.0.1,127.0.0.1,192.168.99.100\"\n            HOSTNAME=${HOSTNAME:-} # This come from the container, so let's not permit it be unbound\n            if [[ ! -z $HOSTNAME ]]; then SSL_HOSTS=\"$SSL_HOSTS,$HOSTNAME\"; fi\n            if [[ ! -z $ADV_HOST ]]; then SSL_HOSTS=\"$SSL_HOSTS,$ADV_HOST\"; fi\n            if [[ ! -z $SSL_EXTRA_HOSTS ]]; then SSL_HOSTS=\"$SSL_HOSTS,$SSL_EXTRA_HOSTS\"; fi\n\n            # Create Key-Certificate pairs for Kafka and user\n            for cert in kafka client clientA clientB; do\n                quickcert -cacert lfddca.crt.pem -cakey lfddca.key.pem -out $cert. -CN \"$cert\" -hosts \"$SSL_HOSTS\" -duration 3650\n\n                openssl pkcs12 -export \\\n                        -in \"$cert.crt.pem\" \\\n                        -inkey \"$cert.key.pem\" \\\n                        -out \"$cert.p12\" \\\n                        -name \"$cert\" \\\n                        -passout pass:fastdata\n\n                keytool -importkeystore \\\n                        -noprompt -v \\\n                        -srckeystore \"$cert.p12\" \\\n                        -srcstoretype PKCS12 \\\n                        -srcstorepass fastdata \\\n                        -alias \"$cert\" \\\n                        -deststorepass fastdata \\\n                        -destkeypass fastdata \\\n                        -destkeystore \"$cert.jks\"\n            done\n\n            keytool -importcert \\\n                    -noprompt \\\n                    -keystore truststore.jks \\\n                    -alias LensesioFastDataDevCA \\\n                    -file lfddca.crt.pem \\\n                    -storepass fastdata\n\n            mkdir -p /var/www/certs/\n            cp client.jks clientA.jks clientB.jks truststore.jks /var/www/certs/\n\n            popd\n        } >/var/log/ssl-setup.log 2>&1\n    fi\n    # Setup the broker with SSL\n    cat <<EOF >>/var/run/broker/server.properties\nssl.client.auth=required\nssl.key.password=fastdata\nssl.keystore.location=/tmp/certs/kafka.jks\nssl.keystore.password=fastdata\nssl.truststore.location=/tmp/certs/truststore.jks\nssl.truststore.password=fastdata\nssl.protocol=TLS\nssl.enabled.protocols=TLSv1.2,TLSv1.1,TLSv1\nssl.keystore.type=JKS\nssl.truststore.type=JKS\nEOF\n    sed -r -e \"s|^(listeners=.*)|\\1,SSL://:${BROKER_SSL_PORT}|\" \\\n        -i /var/run/broker/server.properties\n    if [[ -n ${ADV_HOST} ]] && [[ $TMP_ADV_LISTENERS_SET =~ $TRUE_REG ]]; then\n        sed -r \\\n            -e \"s|^(advertised.listeners=.*)|\\1,SSL://${ADV_HOST}:${BROKER_SSL_PORT}|\" \\\n            -i /var/run/broker/server.properties\n    fi\n\n    # Log authorization requests\n    if [[ $DEBUG_AUTH =~ $TRUE_REG ]]; then\n        touch /var/log/kafka-authorizer.log\n        chmod 666 /var/log/kafka-authorizer.log\n        cat <<EOF >> /var/run/broker/log4j.properties\nlog4j.appender.authorizerAppender=org.apache.log4j.DailyRollingFileAppender\nlog4j.appender.authorizerAppender.DatePattern='.'yyyy-MM-dd-HH\nlog4j.appender.authorizerAppender.File=/var/log/kafka-authorizer.log\nlog4j.appender.authorizerAppender.layout=org.apache.log4j.PatternLayout\nlog4j.appender.authorizerAppender.layout.ConversionPattern=[%d] %p %m (%c)%n\nlog4j.logger.kafka.authorizer.logger=INFO, authorizerAppender\nlog4j.additivity.kafka.authorizer.logger=false\nEOF\n    fi\n\n    sed -e 's/ssl_browse/\"enabled\" : true/' -i /var/www/env.js\nelse\n    sed -r -e \"s|$BROKER_SSL_PORT||\" -i /var/www/env.js\n    sed -e 's/ssl_browse/\"enabled\" : false/' -i /var/www/env.js\nfi\n\n# Set web-only mode if needed\nif [[ $WEB_ONLY =~ $TRUE_REG ]]; then\n    PORTS=\"$WEB_PORT\"\n    echo -e \"\\e[92mWeb only mode. Kafka services will be disabled.\\e[39m\"\n    rm -rf /etc/supervisord.d/*\n    cp /usr/local/share/etc/lensesio/supervisord.d/supervisord-web-only.conf /etc/supervisord.d/\n    envsubst < /usr/local/share/lensesio/etc/fast-data-dev-ui/env-webonly.js > /var/www/env.js\n    export RUNTESTS=\"${RUNTESTS:-0}\"\nfi\n\n# Set supervisord to output all logs to stdout\nif [[ $DEBUG =~ $TRUE_REG ]]; then\n    sed -e 's/loglevel=info/loglevel=debug/' -i /etc/supervisord.d/*\nfi\n\n# Check for port availability\nfor port in $PORTS; do\n    if [[ $port == 0 ]]; then\n        continue\n    elif ! /usr/local/bin/checkport -port \"$port\"; then\n        echo \"Could not successfully bind to port $port. Maybe some other service\"\n        echo \"in your system is using it? Please free the port and try again.\"\n        echo \"Exiting.\"\n        exit 1\n    fi\ndone\n\n# Check for Container's Memory Limit\nif [[ -f /sys/fs/cgroup/memory/memory.limit_in_bytes ]]; then\n    MLB=\"$(cat /sys/fs/cgroup/memory/memory.limit_in_bytes)\"\n    MLMB=\"$(( MLB / 1024 / 1024 ))\"\n    MLREC=3584\n    if [[ \"$MLMB\" -lt \"$MLREC\" ]]; then\n        echo -e \"\\e[91mMemory limit for container is \\e[93m${MLMB} MiB\\e[91m, which is less than the lowest\"\n        echo -e \"recommended of \\e[93m${MLREC} MiB\\e[91m. You will probably experience instability issues.\\e[39m\"\n    fi\nfi\n\n# Check for Available RAM\nset +o errexit\nRAKB=\"$(grep MemA /proc/meminfo | sed -r -e 's/.* ([0-9]+) kB/\\1/')\"\nif [[ $STRICT_SCRIPT =~ $TRUE_REG ]]; then set -o errexit; fi\nif [[ -z \"$RAKB\" ]]; then\n        echo -e \"\\e[91mCould not detect available RAM, probably due to very old Linux Kernel.\"\n        echo -e \"\\e[91mPlease make sure you have the recommended minimum of \\e[93m4096 MiB\\e[91m RAM available for fast-data-dev.\\e[39m\"\nelse\n    RAMB=\"$(( RAKB / 1024 ))\"\n    RAREC=4096\n    if [[ \"$RAMB\" -lt \"$RAREC\" ]]; then\n        echo -e \"\\e[91mOperating system RAM available is \\e[93m${RAMB} MiB\\e[91m, which is less than the lowest\"\n        echo -e \"recommended of \\e[93m${RAREC} MiB\\e[91m. Your system performance may be seriously impacted.\\e[39m\"\n    fi\nfi\n# Check for Available Disk\nDAM=\"$(df /tmp --output=avail -BM | tail -n1 | sed -r -e 's/M//' -e 's/[ ]*([0-9]+)[ ]*/\\1/')\"\nif [[ -z \"$DAM\" ]] || ! [[ \"$DAM\" =~ ^[0-9]+$ ]]; then\n    echo -e \"\\e[91mCould not detect available Disk space.\"\n    echo -e \"\\e[91mPlease make sure you have the recommended minimum of \\e[93m256 MiB\\e[91m disk space available for '/tmp' directory.\\e[39m\"\nelse\n    DAREC=256\n    if [[ \"$DAM\" -lt $DAREC ]]; then\n        echo -e \"\\e[91mDisk space available for the '/tmp' directory is just \\e[93m${DAM} MiB\\e[91m which is less than the lowest\"\n        echo -e \"recommended of \\e[93m${DAREC} MiB\\e[91m. The container’s services may fail to start.\\e[39m\"\n    fi\nfi\n\nPRINT_HOST=${ADV_HOST:-127.0.0.1}\nexport PRINT_HOST\nWEB_TERMINAL_CREDS=${WEB_TERMINAL_CREDS:-admin:admin}\nexport WEB_TERMINAL_CREDS\n# shellcheck disable=SC1091\n[[ -f /build.info ]] && source /build.info\necho -e \"\\e[92mStarting services.\\e[39m\"\necho -e \"\\e[92mThis is Lenses.io’s fast-data-dev. Kafka ${FDD_KAFKA_VERSION} (Lenses.io's Kafka Distribution).\\e[39m\"\necho -e \"\\e[34mYou may visit \\e[96mhttp://${PRINT_HOST}:${WEB_PORT}\\e[34m in about \\e[96ma minute\\e[34m.\\e[39m\"\n\n# Set sample data if needed\nif [[ $RUNNING_SAMPLEDATA =~ $TRUE_REG ]] && [[ $SAMPLEDATA =~ $TRUE_REG ]]; then\n    cp /usr/local/share/lensesio/etc/supervisord.d/99-supervisord-running-sample-data.conf /etc/supervisord.d/\nelif [[ $SAMPLEDATA =~ $TRUE_REG ]]; then\n    # This should be added only if we don't have running data, because it sets\n    # retention period to 10 years (as the data is so few in this case).\n    cp /usr/local/share/lensesio/etc/supervisord.d/99-supervisord-sample-data.conf /etc/supervisord.d/\nelse\n    # If SAMPLEDATA=0 and FORWARDLOGS connector not explicitly requested\n    export FORWARDLOGS=0\nfi\n\n# Run post-setup user provided scripts.\nexport POST_SETUP=${POST_SETUP:-}\nexport POST_SETUP_FILE=${POST_SETUP_FILE:-}\nexport POST_SETUP_URL=${POST_SETUP_URL:-}\nif [[ -n \"$POST_SETUP\" ]]; then\n    $POST_SETUP\nfi\nif [[ -n \"$POST_SETUP_FILE\" ]]; then\n    if [[ ! -f \"$POST_SETUP_FILE\" ]]; then\n        echo \"Although POST_SETUP_FILE is set, I cannot find the file.\"\n    fi\n    source $POST_SETUP_FILE\nfi\nif [[ -n \"$POST_SETUP_URL\" ]]; then\n    curl \"$POST_SETUP_URL\" --silent --show-error --fail --output /tmp/POST_SETUP_URL\n    source /tmp/POST_SETUP_URL\n    rm /tmp/POST_SETUP_URL\nfi\n\nexec /usr/bin/supervisord -c /etc/supervisord.conf\n"
        }
      ]
    }
  ]
}