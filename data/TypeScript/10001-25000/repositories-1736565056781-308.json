{
  "metadata": {
    "timestamp": 1736565056781,
    "page": 308,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "getumbrel/llama-gpt",
      "stars": 10905,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.0419921875,
          "content": "* text=auto\n*.sh      eol=lf\n*.yml     text"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.046875,
          "content": "**/.DS_Store\nmodels/*.bin\nmodels/*.gguf\n**/.todo"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.076171875,
          "content": "MIT License\n\nCopyright (c) 2023 Umbrel, Inc.\nCopyright (c) 2023 Mckay Wrigley\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 9.744140625,
          "content": "<p align=\"center\">\n  <a href=\"https://apps.umbrel.com/app/llama-gpt\">\n    <img width=\"150\" height=\"150\" src=\"https://i.imgur.com/LI59cui.png\" alt=\"LlamaGPT\" width=\"200\" />\n  </a>\n</p>\n<p align=\"center\">\n  <h1 align=\"center\">LlamaGPT</h1>\n  <p align=\"center\">\n    A self-hosted, offline, ChatGPT-like chatbot, powered by Llama 2. 100% private, with no data leaving your device.\n    <br/>\n    <strong>New: Support for Code Llama models and Nvidia GPUs.</strong>\n    <br />\n    <br />\n    <a href=\"https://umbrel.com\"><strong>umbrel.com (we're hiring) Â»</strong></a>\n    <br />\n    <br />\n    <a href=\"https://twitter.com/umbrel\">\n      <img src=\"https://img.shields.io/twitter/follow/umbrel?style=social\" />\n    </a>\n    <a href=\"https://t.me/getumbrel\">\n      <img src=\"https://img.shields.io/badge/community-chat-%235351FB\">\n    </a>\n    <a href=\"https://reddit.com/r/getumbrel\">\n      <img src=\"https://img.shields.io/reddit/subreddit-subscribers/getumbrel?style=social\">\n    </a>\n    <a href=\"https://community.umbrel.com\">\n      <img src=\"https://img.shields.io/badge/community-forum-%235351FB\">\n    </a>\n  </p>\n</p>\n<p align=\"center\">\n  <a href=\"https://umbrel.com/#start\">\n    <img src=\"https://i.imgur.com/sj5vqEG.jpg\" width=\"100%\" />\n  </a>\n</p>\n\n## Contents\n\n1. [Demo](#demo)\n2. [Supported Models](#supported-models)\n3. [How to install](#how-to-install)\n   - [On umbrelOS home server](#install-llamagpt-on-your-umbrelos-home-server)\n   - [On M1/M2 Mac](#install-llamagpt-on-m1m2-mac)\n   - [Anywhere else with Docker](#install-llamagpt-anywhere-else-with-docker)\n   - [Kubernetes](#install-llamagpt-with-kubernetes)\n4. [OpenAI-compatible API](#openai-compatible-api)\n5. [Benchmarks](#benchmarks)\n6. [Roadmap and contributing](#roadmap-and-contributing)\n7. [Acknowledgements](#acknowledgements)\n\n## Demo\n\nhttps://github.com/getumbrel/llama-gpt/assets/10330103/5d1a76b8-ed03-4a51-90bd-12ebfaf1e6cd\n\n## Supported models\n\nCurrently, LlamaGPT supports the following models. Support for running custom models is on the roadmap.\n\n| Model name                               | Model size | Model download size | Memory required |\n| ---------------------------------------- | ---------- | ------------------- | --------------- |\n| Nous Hermes Llama 2 7B Chat (GGML q4_0)  | 7B         | 3.79GB              | 6.29GB          |\n| Nous Hermes Llama 2 13B Chat (GGML q4_0) | 13B        | 7.32GB              | 9.82GB          |\n| Nous Hermes Llama 2 70B Chat (GGML q4_0) | 70B        | 38.87GB             | 41.37GB         |\n| Code Llama 7B Chat (GGUF Q4_K_M)         | 7B         | 4.24GB              | 6.74GB          |\n| Code Llama 13B Chat (GGUF Q4_K_M)        | 13B        | 8.06GB              | 10.56GB         |\n| Phind Code Llama 34B Chat (GGUF Q4_K_M)  | 34B        | 20.22GB             | 22.72GB         |\n\n## How to install\n\n### Install LlamaGPT on your umbrelOS home server\n\nRunning LlamaGPT on an [umbrelOS](https://umbrel.com) home server is one click. Simply install it from the [Umbrel App Store](https://apps.umbrel.com/app/llama-gpt).\n\n[![LlamaGPT on Umbrel App Store](https://apps.umbrel.com/app/llama-gpt/badge-light.svg)](https://apps.umbrel.com/app/llama-gpt)\n\n### Install LlamaGPT on M1/M2 Mac\n\nMake sure your have Docker and Xcode installed.\n\nThen, clone this repo and `cd` into it:\n\n```\ngit clone https://github.com/getumbrel/llama-gpt.git\ncd llama-gpt\n```\n\nRun LlamaGPT with the following command:\n\n```\n./run-mac.sh --model 7b\n```\n\nYou can access LlamaGPT at http://localhost:3000.\n\n> To run 13B or 70B chat models, replace `7b` with `13b` or `70b` respectively.\n> To run 7B, 13B or 34B Code Llama models, replace `7b` with `code-7b`, `code-13b` or `code-34b` respectively.\n\nTo stop LlamaGPT, do `Ctrl + C` in Terminal.\n\n### Install LlamaGPT anywhere else with Docker\n\nYou can run LlamaGPT on any x86 or arm64 system. Make sure you have Docker installed.\n\nThen, clone this repo and `cd` into it:\n\n```\ngit clone https://github.com/getumbrel/llama-gpt.git\ncd llama-gpt\n```\n\nRun LlamaGPT with the following command:\n\n```\n./run.sh --model 7b\n```\n\nOr if you have an Nvidia GPU, you can run LlamaGPT with CUDA support using the `--with-cuda` flag, like:\n\n```\n./run.sh --model 7b --with-cuda\n```\n\nYou can access LlamaGPT at `http://localhost:3000`.\n\n> To run 13B or 70B chat models, replace `7b` with `13b` or `70b` respectively.\n> To run Code Llama 7B, 13B or 34B models, replace `7b` with `code-7b`, `code-13b` or `code-34b` respectively.\n\nTo stop LlamaGPT, do `Ctrl + C` in Terminal.\n\n> Note: On the first run, it may take a while for the model to be downloaded to the `/models` directory. You may also see lots of output like this for a few minutes, which is normal:\n>\n> ```\n> llama-gpt-llama-gpt-ui-1       | [INFO  wait] Host [llama-gpt-api-13b:8000] not yet available...\n> ```\n>\n> After the model has been automatically downloaded and loaded, and the API server is running, you'll see an output like:\n>\n> ```\n> llama-gpt-ui_1   | ready - started server on 0.0.0.0:3000, url: http://localhost:3000\n> ```\n>\n> You can then access LlamaGPT at http://localhost:3000.\n\n---\n\n### Install LlamaGPT with Kubernetes\n\nFirst, make sure you have a running Kubernetes cluster and `kubectl` is configured to interact with it.\n\nThen, clone this repo and `cd` into it.\n\nTo deploy to Kubernetes first create a namespace:\n\n```bash\nkubectl create ns llama\n```\n\nThen apply the manifests under the `/deploy/kubernetes` directory with\n\n```bash\nkubectl apply -k deploy/kubernetes/. -n llama\n```\n\nExpose your service however you would normally do that.\n\n## OpenAI compatible API\n\nThanks to llama-cpp-python, a drop-in replacement for OpenAI API is available at `http://localhost:3001`. Open http://localhost:3001/docs to see the API documentation.\n\n## Benchmarks\n\nWe've tested LlamaGPT models on the following hardware with the default system prompt, and user prompt: \"How does the universe expand?\" at temperature 0 to guarantee deterministic results. Generation speed is averaged over the first 10 generations.\n\nFeel free to add your own benchmarks to this table by opening a pull request.\n\n#### Nous Hermes Llama 2 7B Chat (GGML q4_0)\n\n| Device                              | Generation speed |\n| ----------------------------------- | ---------------- |\n| M1 Max MacBook Pro (64GB RAM)       | 54 tokens/sec    |\n| GCP c2-standard-16 vCPU (64 GB RAM) | 16.7 tokens/sec  |\n| Ryzen 5700G 4.4GHz 4c (16 GB RAM)   | 11.50 tokens/sec |\n| GCP c2-standard-4 vCPU (16 GB RAM)  | 4.3 tokens/sec   |\n| Umbrel Home (16GB RAM)              | 2.7 tokens/sec   |\n| Raspberry Pi 4 (8GB RAM)            | 0.9 tokens/sec   |\n\n#### Nous Hermes Llama 2 13B Chat (GGML q4_0)\n\n| Device                              | Generation speed |\n| ----------------------------------- | ---------------- |\n| M1 Max MacBook Pro (64GB RAM)       | 20 tokens/sec    |\n| GCP c2-standard-16 vCPU (64 GB RAM) | 8.6 tokens/sec   |\n| GCP c2-standard-4 vCPU (16 GB RAM)  | 2.2 tokens/sec   |\n| Umbrel Home (16GB RAM)              | 1.5 tokens/sec   |\n\n#### Nous Hermes Llama 2 70B Chat (GGML q4_0)\n\n| Device                              | Generation speed |\n| ----------------------------------- | ---------------- |\n| M1 Max MacBook Pro (64GB RAM)       | 4.8 tokens/sec   |\n| GCP e2-standard-16 vCPU (64 GB RAM) | 1.75 tokens/sec  |\n| GCP c2-standard-16 vCPU (64 GB RAM) | 1.62 tokens/sec  |\n\n#### Code Llama 7B Chat (GGUF Q4_K_M)\n\n| Device                        | Generation speed |\n| ----------------------------- | ---------------- |\n| M1 Max MacBook Pro (64GB RAM) | 41 tokens/sec    |\n\n#### Code Llama 13B Chat (GGUF Q4_K_M)\n\n| Device                        | Generation speed |\n| ----------------------------- | ---------------- |\n| M1 Max MacBook Pro (64GB RAM) | 25 tokens/sec    |\n\n#### Phind Code Llama 34B Chat (GGUF Q4_K_M)\n\n| Device                        | Generation speed |\n| ----------------------------- | ---------------- |\n| M1 Max MacBook Pro (64GB RAM) | 10.26 tokens/sec |\n\n## Roadmap and contributing\n\nWe're looking to add more features to LlamaGPT. You can see the roadmap [here](https://github.com/getumbrel/llama-gpt/issues/8#issuecomment-1681321145). The highest priorities are:\n\n- [x] Moving the model out of the Docker image and into a separate volume.\n- [x] Add Metal support for M1/M2 Macs.\n- [x] Add support for Code Llama models.\n- [x] Add CUDA support for NVIDIA GPUs.\n- [ ] Add ability to load custom models.\n- [ ] Allow users to switch between models.\n\nIf you're a developer who'd like to help with any of these, please open an issue to discuss the best way to tackle the challenge. If you're looking to help but not sure where to begin, check out [these issues](https://github.com/getumbrel/llama-gpt/labels/good%20first%20issue) that have specifically been marked as being friendly to new contributors.\n\n## Acknowledgements\n\nA massive thank you to the following developers and teams for making LlamaGPT possible:\n\n- [Mckay Wrigley](https://github.com/mckaywrigley) for building [Chatbot UI](https://github.com/mckaywrigley).\n- [Georgi Gerganov](https://github.com/ggerganov) for implementing [llama.cpp](https://github.com/ggerganov/llama.cpp).\n- [Andrei](https://github.com/abetlen) for building the [Python bindings for llama.cpp](https://github.com/abetlen/llama-cpp-python).\n- [NousResearch](https://nousresearch.com) for [fine-tuning the Llama 2 7B and 13B models](https://huggingface.co/NousResearch).\n- [Phind](https://www.phind.com/) for [fine-tuning the Code Llama 34B model](https://www.phind.com/blog/code-llama-beats-gpt4).\n- [Tom Jobbins](https://huggingface.co/TheBloke) for [quantizing the Llama 2 models](https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML).\n- [Meta](https://ai.meta.com/llama) for releasing Llama 2 and Code Llama under a permissive license.\n\n---\n\n[![License](https://img.shields.io/github/license/getumbrel/llama-gpt?color=%235351FB)](https://github.com/getumbrel/llama-gpt/blob/master/LICENSE.md)\n\n[umbrel.com](https://umbrel.com)\n"
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "cuda",
          "type": "tree",
          "content": null
        },
        {
          "name": "deploy",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker-compose-cuda-ggml.yml",
          "type": "blob",
          "size": 1.4248046875,
          "content": "version: '3.6'\n\nservices:\n  llama-gpt-api-cuda-ggml:\n    build:\n      context: ./cuda\n      dockerfile: ggml.Dockerfile\n    restart: on-failure\n    volumes:\n      - './models:/models'\n      - './cuda:/cuda'\n    ports:\n      - 3001:8000\n    environment:\n      MODEL: '/models/${MODEL_NAME:-llama-2-7b-chat.bin}'\n      MODEL_DOWNLOAD_URL: '${MODEL_DOWNLOAD_URL:-https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML/resolve/main/nous-hermes-llama-2-7b.ggmlv3.q4_0.bin}'\n      N_GQA: '${N_GQA:-1}'\n      USE_MLOCK: 1\n    cap_add:\n      - IPC_LOCK\n      - SYS_RESOURCE\n    command: '/bin/sh /cuda/run.sh'\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n  llama-gpt-ui:\n    # TODO: Use this image instead of building from source after the next release\n    # image: 'ghcr.io/getumbrel/llama-gpt-ui:latest'\n    build:\n      context: ./ui\n      dockerfile: Dockerfile\n    ports:\n      - 3000:3000\n    restart: on-failure\n    environment:\n      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'\n      - 'OPENAI_API_HOST=http://llama-gpt-api-cuda-ggml:8000'\n      - 'DEFAULT_MODEL=/models/${MODEL_NAME:-llama-2-7b-chat.bin}'\n      - 'NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT=${DEFAULT_SYSTEM_PROMPT:-\"You are a helpful and friendly AI assistant. Respond very concisely.\"}'\n      - 'WAIT_HOSTS=llama-gpt-api-cuda-ggml:8000'\n      - 'WAIT_TIMEOUT=${WAIT_TIMEOUT:-3600}'\n"
        },
        {
          "name": "docker-compose-cuda-gguf.yml",
          "type": "blob",
          "size": 1.4306640625,
          "content": "version: '3.6'\n\nservices:\n  llama-gpt-api-cuda-gguf:\n    build:\n      context: ./cuda\n      dockerfile: gguf.Dockerfile\n    restart: on-failure\n    volumes:\n      - './models:/models'\n      - './cuda:/cuda'\n    ports:\n      - 3001:8000\n    environment:\n      MODEL: '/models/${MODEL_NAME:-code-llama-2-7b-chat.gguf}'\n      MODEL_DOWNLOAD_URL: '${MODEL_DOWNLOAD_URL:-https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf}'\n      N_GQA: '${N_GQA:-1}'\n      USE_MLOCK: 1\n    cap_add:\n      - IPC_LOCK\n      - SYS_RESOURCE\n    command: '/bin/sh /cuda/run.sh'\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: 1\n              capabilities: [gpu]\n\n  llama-gpt-ui:\n    # TODO: Use this image instead of building from source after the next release\n    # image: 'ghcr.io/getumbrel/llama-gpt-ui:latest'\n    build:\n      context: ./ui\n      dockerfile: Dockerfile\n    ports:\n      - 3000:3000\n    restart: on-failure\n    environment:\n      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'\n      - 'OPENAI_API_HOST=http://llama-gpt-api-cuda-gguf:8000'\n      - 'DEFAULT_MODEL=/models/${MODEL_NAME:-code-llama-2-7b-chat.gguf}'\n      - 'NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT=${DEFAULT_SYSTEM_PROMPT:-\"You are a helpful and friendly AI assistant. Respond very concisely.\"}'\n      - 'WAIT_HOSTS=llama-gpt-api-cuda-gguf:8000'\n      - 'WAIT_TIMEOUT=${WAIT_TIMEOUT:-3600}'\n"
        },
        {
          "name": "docker-compose-gguf.yml",
          "type": "blob",
          "size": 1.328125,
          "content": "version: '3.6'\n\nservices:\n  llama-gpt-api:\n    # Pin to llama-cpp-python 0.1.80 with GGUF support\n    image: ghcr.io/abetlen/llama-cpp-python:latest@sha256:de0fd227f348b5e43d4b5b7300f1344e712c14132914d1332182e9ecfde502b2\n    restart: on-failure\n    volumes:\n      - './models:/models'\n      - './api:/api'\n    ports:\n      - 3001:8000\n    environment:\n      MODEL: '/models/${MODEL_NAME:-code-llama-2-7b-chat.gguf}'\n      MODEL_DOWNLOAD_URL: '${MODEL_DOWNLOAD_URL:-https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf}'\n      N_GQA: '${N_GQA:-1}'\n      USE_MLOCK: 1\n    cap_add:\n      - IPC_LOCK\n    command: '/bin/sh /api/run.sh'\n\n  llama-gpt-ui:\n    # TODO: Use this image instead of building from source after the next release\n    # image: 'ghcr.io/getumbrel/llama-gpt-ui:latest'\n    build:\n      context: ./ui\n      dockerfile: Dockerfile\n    ports:\n      - 3000:3000\n    restart: on-failure\n    environment:\n      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'\n      - 'OPENAI_API_HOST=http://llama-gpt-api:8000'\n      - 'DEFAULT_MODEL=/models/${MODEL_NAME:-llama-2-7b-chat.bin}'\n      - 'NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT=${DEFAULT_SYSTEM_PROMPT:-\"You are a helpful and friendly AI assistant. Respond very concisely.\"}'\n      - 'WAIT_HOSTS=llama-gpt-api:8000'\n      - 'WAIT_TIMEOUT=${WAIT_TIMEOUT:-3600}'\n"
        },
        {
          "name": "docker-compose-mac.yml",
          "type": "blob",
          "size": 0.4931640625,
          "content": "version: '3.6'\n\nservices:\n  llama-gpt-ui-mac:\n    build:\n      context: ./ui\n      dockerfile: no-wait.Dockerfile\n    ports:\n      - 3000:3000\n    restart: on-failure\n    environment:\n      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'\n      - 'OPENAI_API_HOST=http://host.docker.internal:3001'\n      - 'DEFAULT_MODEL=$MODEL'\n      - 'NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT=${DEFAULT_SYSTEM_PROMPT:-\"You are a helpful and friendly AI assistant. Respond very concisely and use markdown if responding with code.\"}'\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 1.3583984375,
          "content": "version: '3.6'\n\nservices:\n  llama-gpt-api:\n    # Pin the image to llama-cpp-python 0.1.78 to avoid ggml => gguf breaking changes\n    image: ghcr.io/abetlen/llama-cpp-python:latest@sha256:b6d21ff8c4d9baad65e1fa741a0f8c898d68735fff3f3cd777e3f0c6a1839dd4\n    restart: on-failure\n    volumes:\n      - './models:/models'\n      - './api:/api'\n    ports:\n      - 3001:8000\n    environment:\n      MODEL: '/models/${MODEL_NAME:-llama-2-7b-chat.bin}'\n      MODEL_DOWNLOAD_URL: '${MODEL_DOWNLOAD_URL:-https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML/resolve/main/nous-hermes-llama-2-7b.ggmlv3.q4_0.bin}'\n      N_GQA: '${N_GQA:-1}'\n      USE_MLOCK: 1\n    cap_add:\n      - IPC_LOCK\n    command: '/bin/sh /api/run.sh'\n\n  llama-gpt-ui:\n    # TODO: Use this image instead of building from source after the next release\n    # image: 'ghcr.io/getumbrel/llama-gpt-ui:latest'\n    build:\n      context: ./ui\n      dockerfile: Dockerfile\n    ports:\n      - 3000:3000\n    restart: on-failure\n    environment:\n      - 'OPENAI_API_KEY=sk-XXXXXXXXXXXXXXXXXXXX'\n      - 'OPENAI_API_HOST=http://llama-gpt-api:8000'\n      - 'DEFAULT_MODEL=/models/${MODEL_NAME:-llama-2-7b-chat.bin}'\n      - 'NEXT_PUBLIC_DEFAULT_SYSTEM_PROMPT=${DEFAULT_SYSTEM_PROMPT:-\"You are a helpful and friendly AI assistant. Respond very concisely.\"}'\n      - 'WAIT_HOSTS=llama-gpt-api:8000'\n      - 'WAIT_TIMEOUT=${WAIT_TIMEOUT:-3600}'\n"
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "run-mac.sh",
          "type": "blob",
          "size": 8.2197265625,
          "content": "#!/bin/bash\nset -e\n\n# Define a function to refresh the source of .zshrc or .bashrc\nsource_shell_rc() {\n    # Source .zshrc or .bashrc\n    if [ -f ~/.zshrc ]; then\n        source ~/.zshrc\n    elif [ -f ~/.bashrc ]; then\n        source ~/.bashrc\n    else\n        echo \"No .bashrc or .zshrc file found.\"\n    fi\n}\n\n# Define a function to install conda with Miniforge3\ninstall_conda() {\n    # Download Miniforge3\n    curl -L -o /tmp/Miniforge3.sh https://github.com/conda-forge/miniforge/releases/latest/download/Miniforge3-MacOSX-arm64.sh\n    bash /tmp/Miniforge3.sh\n    source_shell_rc\n}\n\n# Define a function to install a specific version of llama-cpp-python\ninstall_llama_cpp_python() {\n    local model_type=$1\n    local version=$2\n    local installed_version=$(pip3 show llama-cpp-python | grep -i version | awk '{print $2}')\n\n    if [[ \"$installed_version\" != \"$version\" ]]; then\n        echo \"llama-cpp-python version is not $version. Installing $version version for $model_type support...\"\n        pip3 uninstall llama-cpp-python -y\n        CMAKE_ARGS=\"-DLLAMA_METAL=on\" FORCE_CMAKE=1 pip3 install llama-cpp-python==$version --no-cache-dir\n        pip3 install 'llama-cpp-python[server]'\n    else\n        echo \"llama-cpp-python version is $version.\"\n    fi\n}\n\nsource_shell_rc\n\n# Check if the platform is MacOS and the architecture is arm64\nif [[ \"$(uname)\" != \"Darwin\" ]] || [[ \"$(uname -m)\" != \"arm64\" ]]; then\n    echo \"This script is intended to be run on MacOS with M1/M2 chips. Exiting...\"\n    exit 1\nfi\n\n# Check if Docker is installed\nif ! command -v docker &> /dev/null; then\n    echo \"Docker is not installed. Exiting...\"\n    exit 1\nfi\n\n# Check if python3 is installed\nif ! command -v python3 &> /dev/null; then\n    echo \"Python3 is not installed. Exiting...\"\n    exit 1\nfi\n\n# Check if Xcode is installed\nxcode_path=$(xcode-select -p 2>/dev/null)\nif [ -z \"$xcode_path\" ]; then\n    echo \"Xcode is not installed. Installing (this may take a long time)...\"\n    xcode-select --install\nelse\n    echo \"Xcode is installed at $xcode_path\"\nfi\n\n# Check if conda is installed\nif ! command -v conda &> /dev/null; then\n    echo \"Conda is not installed. Installing Miniforge3 which includes conda...\"\n    install_conda\nelse\n    echo \"Conda is installed.\"\n    # TODO: Check if the conda version for MacOS that supports Metal GPU is installed\n    # conda_version=$(conda --version)\n    # if [[ $conda_version != *\"Miniforge3\"* ]]; then\n    #     echo \"Conda version that supports Metal GPU is not installed. Installing...\"\n    #     install_conda\n    # else\n    #     echo \"Conda version that supports M1/M2 is installed.\"\n    # fi\nfi\n\n\n# Check if the conda environment 'llama-gpt' exists\nif conda env list | grep -q 'llama-gpt'; then\n    echo \"Conda environment 'llama-gpt' already exists.\"\nelse\n    echo \"Creating a conda environment called 'llama-gpt'...\"\n    conda create -n llama-gpt python=$(python3 --version | cut -d ' ' -f 2)\nfi\n\n# Check if the conda environment 'llama-gpt' is active\nif [[ \"$(conda info --envs | grep '*' | awk '{print $1}')\" != \"llama-gpt\" ]]; then\n    echo \"Activating the conda environment 'llama-gpt'...\"\n    conda activate llama-gpt\nelse\n    echo \"Conda environment 'llama-gpt' is already active.\"\nfi\n\n# Parse command line arguments for --model\nwhile [[ \"$#\" -gt 0 ]]; do\n    case $1 in\n        --model) MODEL=\"$2\"; shift ;;\n        *) echo \"Unknown parameter passed: $1\"; exit 1 ;;\n    esac\n    shift\ndone\n\n# If no model is passed, default to 7b model\nif [[ -z \"$MODEL\" ]]; then\n    echo \"No model value provided. Defaulting to 7b. If you want to change the model, exit the script and use --model to provide the model value.\"\n    echo \"Supported models are 7b, 13b, 70b, code-7b, code-13b, code-34b.\"\n    MODEL=\"7b\"\nfi\n\n# Get the number of available CPU cores and subtract 2\nn_threads=$(($(sysctl -n hw.logicalcpu) - 2))\n\n# Define context window\nn_ctx=4096\n\n# Define batch size\nn_batch=2096\n\n# Define number of GPU layers\nn_gpu_layers=1\n\n# Define grouping factor\nN_GQA=1\n\nmodel_type=\"gguf\"\n\n# Set values for MODEL and MODEL_DOWNLOAD_URL based on the model passed\ncase $MODEL in\n    7b) \n        MODEL=\"./models/llama-2-7b-chat.bin\"\n        MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML/resolve/main/nous-hermes-llama-2-7b.ggmlv3.q4_0.bin\"\n        model_type=\"ggml\"\n        ;;\n    13b) \n        MODEL=\"./models/llama-2-13b-chat.bin\"\n        MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GGML/resolve/main/nous-hermes-llama2-13b.ggmlv3.q4_0.bin\"\n        model_type=\"ggml\"\n        n_gpu_layers=2\n        ;;\n    70b) \n        MODEL=\"./models/llama-2-70b-chat.bin\"\n        MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/Llama-2-70B-Chat-GGML/resolve/main/llama-2-70b-chat.ggmlv3.q4_0.bin\"\n        model_type=\"ggml\"\n        n_gpu_layers=3\n        # Llama 2 70B's grouping factor is 8 compared to 7B and 13B's 1.\n        N_GQA=8\n        ;;\n    code-7b)\n        MODEL=\"./models/code-llama-7b-chat.gguf\"\n        MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf\"\n        DEFAULT_SYSTEM_PROMPT=\"You are a helpful coding assistant. Use markdown when responding with code.\"\n        n_gpu_layers=1\n        n_ctx=8192\n        ;;\n    code-13b)\n        MODEL=\"./models/code-llama-13b-chat.gguf\"\n        MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf\"\n        DEFAULT_SYSTEM_PROMPT=\"You are a helpful coding assistant. Use markdown when responding with code.\"\n        n_gpu_layers=2\n        n_ctx=8192\n        ;;\n    code-34b)\n        MODEL=\"./models/code-llama-34b-chat.gguf\"\n        MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q4_K_M.gguf\"\n        DEFAULT_SYSTEM_PROMPT=\"You are a helpful coding assistant. Use markdown when responding with code.\"\n        n_gpu_layers=3\n        n_ctx=8192\n        # Code Llama 34B's grouping factor is 8 compared to 7B and 13B's 1.\n        N_GQA=8\n        ;;\n    *) \n        echo \"Invalid model passed: $MODEL\"; exit 1 \n        ;;\nesac\n\n# Check if llama-cpp-python is already installed\nllama_cpp_python_installed=$(pip3 list | grep -q llama-cpp-python && echo \"installed\" || echo \"not installed\")\nif [[ \"$llama_cpp_python_installed\" == \"not installed\" ]]; then\n    echo \"llama-cpp-python is not installed. Installing...\"\n    if [[ \"$model_type\" == \"ggml\" ]]; then\n        install_llama_cpp_python \"GGML\" \"0.1.78\"\n    else\n        install_llama_cpp_python \"GGUF\" \"0.1.80\"\n    fi\nelse\n    echo \"llama-cpp-python is installed.\"\n    if [[ \"$model_type\" == \"ggml\" ]]; then\n        install_llama_cpp_python \"GGML\" \"0.1.78\"\n    else\n        install_llama_cpp_python \"GGUF\" \"0.1.80\"\n    fi\nfi\n\n\n# Check if the model file exists\nif [ ! -f $MODEL ]; then\n    echo \"Model file not found. Downloading...\"\n    # Download the model file with a custom progress bar showing percentage, download speed, downloaded, total size, and estimated time remaining\n    curl -L -o $MODEL $MODEL_DOWNLOAD_URL\n    if [ $? -ne 0 ]; then\n        echo \"Download failed. Trying with TLS 1.2...\"\n        curl -L --tlsv1.2 -o $MODEL $MODEL_DOWNLOAD_URL\n    fi\nelse\n    echo \"$MODEL model found.\"\nfi\n\n# Display configuration information\necho \"Initializing server with:\"\necho \"Batch size: $n_batch\"\necho \"Number of CPU threads: $n_threads\"\necho \"Number of GPU layers: $n_gpu_layers\"\necho \"Context window: $n_ctx\"\necho \"GQA: $n_gqa\"\n\n# Export environment variables\nexport MODEL\nexport N_GQA\nexport DEFAULT_SYSTEM_PROMPT\n\n# Run docker-compose with the macOS yml file\ndocker compose -f ./docker-compose-mac.yml up --remove-orphans --build &\n\n# Run the server\npython3 -m llama_cpp.server --n_ctx $n_ctx --n_threads $n_threads --n_gpu_layers $n_gpu_layers --n_batch $n_batch --model $MODEL --port 3001 &\n\n# Define a function to stop docker-compose and the python3 command\nstop_commands() {\n    echo \"Stopping docker-compose...\"\n    docker compose -f ./docker-compose-mac.yml down\n    echo \"Stopping python server...\"\n    pkill -f \"python3 -m llama_cpp.server\"\n    echo \"Deactivating conda environment...\"\n    conda deactivate\n    echo \"All processes stopped.\"\n}\n\n# Set a trap to catch SIGINT and stop the commands\ntrap stop_commands SIGINT\n\n# Wait for both commands to finish\nwait $DOCKER_COMPOSE_PID\nwait $PYTHON_PID\n\n\n"
        },
        {
          "name": "run.sh",
          "type": "blob",
          "size": 4.1259765625,
          "content": "#!/bin/bash\n\n# Check if docker compose is installed\nif ! command -v docker &> /dev/null\nthen\n    echo \"Docker could not be found. Please install Docker and try again.\"\n    exit\nfi\n\n# Parse command line arguments for model value and check for --with-cuda flag\nwith_cuda=0\nwhile [[ \"$#\" -gt 0 ]]; do\n    case $1 in\n        --model) model=\"$2\"; shift ;;\n        --with-cuda) with_cuda=1 ;;\n        *) echo \"Unknown parameter passed: $1\"; exit 1 ;;\n    esac\n    shift\ndone\n\n# Check if model value is provided\nif [ -z \"$model\" ]\nthen\n    echo \"No model value provided. Defaulting to 7b. If you want to change the model, exit the script and use --model to provide the model value.\"\n    echo \"Supported models are 7b, 13b, 70b, code-7b, code-13b, code-34b.\"\n    model=\"7b\"\nfi\n\nmodel_type=\"gguf\"\n\n# Export the model value as an environment variable\ncase $model in\n    7b)\n        export MODEL_NAME=\"llama-2-7b-chat.bin\"\n        export MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/Nous-Hermes-Llama-2-7B-GGML/resolve/main/nous-hermes-llama-2-7b.ggmlv3.q4_0.bin\"\n        export WAIT_TIMEOUT=3600\n        export N_GQA=1\n        model_type=\"ggml\"\n        ;;\n    13b)\n        export MODEL_NAME=\"llama-2-13b-chat.bin\"\n        export MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/Nous-Hermes-Llama2-GGML/resolve/main/nous-hermes-llama2-13b.ggmlv3.q4_0.bin\"\n        export WAIT_TIMEOUT=10800\n        export N_GQA=1\n        model_type=\"ggml\"\n        ;;\n    70b)\n        export MODEL_NAME=\"llama-2-70b-chat.bin\"\n        export MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/Nous-Hermes-Llama2-70B-GGML/resolve/main/nous-hermes-llama2-70b.ggmlv3.Q4_0.bin\"\n        export WAIT_TIMEOUT=21600\n        # Llama 2 70B's grouping factor is 8 compared to 7B and 13B's 1. Currently,\n        # it's not possible to change this using --n_gqa with llama-cpp-python in\n        # run.sh, so we expose it as an environment variable.\n        # See: https://github.com/abetlen/llama-cpp-python/issues/528\n        # and: https://github.com/facebookresearch/llama/issues/407\n        export N_GQA=8\n        model_type=\"ggml\"\n        ;;\n    code-7b)\n        export MODEL_NAME=\"code-llama-7b-chat.gguf\"\n        export MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/CodeLlama-7B-Instruct-GGUF/resolve/main/codellama-7b-instruct.Q4_K_M.gguf\"\n        export WAIT_TIMEOUT=3600\n        export DEFAULT_SYSTEM_PROMPT=\"You are a helpful coding assistant. Use markdown when responding with code.\"\n        export N_GQA=1\n        ;;\n    code-13b)\n        export MODEL_NAME=\"code-llama-13b-chat.gguf\"\n        export MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/CodeLlama-13B-Instruct-GGUF/resolve/main/codellama-13b-instruct.Q4_K_M.gguf\"\n        export DEFAULT_SYSTEM_PROMPT=\"You are a helpful coding assistant. Use markdown when responding with code.\"\n        export WAIT_TIMEOUT=10800\n        export N_GQA=1\n        ;;\n    code-34b)\n        export MODEL_NAME=\"code-llama-34b-chat.gguf\"\n        export MODEL_DOWNLOAD_URL=\"https://huggingface.co/TheBloke/Phind-CodeLlama-34B-v1-GGUF/resolve/main/phind-codellama-34b-v1.Q4_K_M.gguf\"\n        export DEFAULT_SYSTEM_PROMPT=\"You are a helpful coding assistant. Use markdown when responding with code.\"\n        export WAIT_TIMEOUT=21600\n        # Code Llama 34B's grouping factor is 8 compared to 7B and 13B's 1. Currently,\n        # it's not possible to change this using --n_gqa with llama-cpp-python in\n        # run.sh, so we expose it as an environment variable.\n        # See: https://github.com/abetlen/llama-cpp-python/issues/528\n        export N_GQA=8\n        ;;\n    *)\n        echo \"Invalid model value provided. Supported models are 7b, 13b, 70b, code-7b, code-13b, code-34b.\"\n        exit 1\n        ;;\nesac\n\n# Run docker compose with docker-compose-ggml.yml or docker-compose-gguf.yml\n\nif [ \"$with_cuda\" -eq 1 ]\nthen\n    if [ \"$model_type\" = \"ggml\" ]\n    then\n        docker compose -f docker-compose-cuda-ggml.yml up --build\n    else\n        docker compose -f docker-compose-cuda-gguf.yml up --build\n    fi\nelse\n    if [ \"$model_type\" = \"ggml\" ]\n    then\n        docker compose -f docker-compose.yml up --build\n    else\n        docker compose -f docker-compose-gguf.yml up --build\n    fi\nfi\n"
        },
        {
          "name": "ui",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}