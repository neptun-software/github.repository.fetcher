{
  "metadata": {
    "timestamp": 1736564305758,
    "page": 893,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "ianarawjo/ChainForge",
      "stars": 2444,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.5703125,
          "content": "*.DS_Store\nchainforge/cache\nchainforge/examples/oaievals/\nchainforge/react-server/node_modules\nchainforge/react-server/build\n\n# == Below was generated by https://www.toptal.com/developers/gitignore/api/python ==\n# Edit at https://www.toptal.com/developers/gitignore?templates=python\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# Docker\npackages/\njobs/\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n#.idea/\n\n### Python Patch ###\n# Poetry local configuration file - https://python-poetry.org/docs/configuration/#local-configuration\npoetry.toml\n\n# ruff\n.ruff_cache/\n\n# LSP config files\npyrightconfig.json\n\n# End of https://www.toptal.com/developers/gitignore/api/python"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 0.193359375,
          "content": "FROM python:3.12-slim AS builder\n\nRUN pip install --upgrade pip\nRUN pip install chainforge --no-cache-dir\n\nWORKDIR /chainforge\n\nEXPOSE 8000\nENTRYPOINT [ \"chainforge\", \"serve\", \"--host\", \"0.0.0.0\" ]\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2023 Ian Arawjo\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.0771484375,
          "content": "graft chainforge/react-server/build\ngraft chainforge/examples\ninclude README.md"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 11.568359375,
          "content": "# ‚õìÔ∏èüõ†Ô∏è ChainForge\n\n**An open-source visual programming environment for battle-testing prompts to LLMs.**\n\n<img width=\"1517\" alt=\"banner\" src=\"https://github.com/ianarawjo/ChainForge/assets/5251713/570879ef-ef8a-4e00-b37c-b49bc3c1a370\">\n\nChainForge is a data flow prompt engineering environment for analyzing and evaluating LLM responses. It is geared towards early-stage, quick-and-dirty exploration of prompts, chat responses, and response quality that goes beyond ad-hoc chatting with individual LLMs. With ChainForge, you can:\n\n- **Query multiple LLMs at once** to test prompt ideas and variations quickly and effectively.\n- **Compare response quality across prompt permutations, across models, and across model settings** to choose the best prompt and model for your use case.\n- **Setup evaluation metrics** (scoring function) and immediately visualize results across prompts, prompt parameters, models, and model settings.\n- **Hold multiple conversations at once across template parameters and chat models.** Template not just prompts, but follow-up chat messages, and inspect and evaluate outputs at each turn of a chat conversation.\n\n[Read the docs to learn more.](https://chainforge.ai/docs/) ChainForge comes with a number of example evaluation flows to give you a sense of what's possible, including 188 example flows generated from benchmarks in OpenAI evals.\n\n**This is an open beta of Chainforge.** We support model providers OpenAI, HuggingFace, Anthropic, Google PaLM2, Azure OpenAI endpoints, and [Dalai](https://github.com/cocktailpeanut/dalai)-hosted models Alpaca and Llama. You can change the exact model and individual model settings. Visualization nodes support numeric and boolean evaluation metrics. Try it and let us know what you think! :)\n\nChainForge is built on [ReactFlow](https://reactflow.dev) and [Flask](https://flask.palletsprojects.com/en/2.3.x/).\n\n# Table of Contents\n\n- [Documentation](https://chainforge.ai/docs/)\n- [Installation](#installation)\n- [Example Experiments](#example-experiments)\n- [Share with Others](#share-with-others)\n- [Features](#features) (see the [docs](https://chainforge.ai/docs/nodes/) for more comprehensive info)\n- [Development and How to Cite](#development)\n\n# Installation\n\nYou can install ChainForge locally, or try it out on the web at **https://chainforge.ai/play/**. The web version of ChainForge has a limited feature set. In a locally installed version you can load API keys automatically from environment variables, write Python code to evaluate LLM responses, or query locally-run Alpaca/Llama models hosted via Dalai.\n\nTo install Chainforge on your machine, make sure you have Python 3.8 or higher, then run\n\n```bash\npip install chainforge\n```\n\nOnce installed, do\n\n```bash\nchainforge serve\n```\n\nOpen [localhost:8000](http://localhost:8000/) in a Google Chrome, Firefox, Microsoft Edge, or Brave browser.\n\nYou can set your API keys by clicking the Settings icon in the top-right corner. If you prefer to not worry about this everytime you open ChainForge, we recommend that save your OpenAI, Anthropic, Google PaLM API keys and/or Amazon AWS credentials to your local environment. For more details, see the [How to Install](https://chainforge.ai/docs/getting_started/).\n\n## Run using Docker\n\nYou can use our [Dockerfile](/Dockerfile) to run `ChainForge` locally using `Docker Desktop`:\n\n- Build the `Dockerfile`:\n  ```shell\n  docker build -t chainforge .\n  ```\n\n- Run the image:\n  ```shell\n  docker run -p 8000:8000 chainforge\n  ```\n\nNow you can open the browser of your choice and open `http://127.0.0.1:8000`.\n\n# Supported providers\n\n- OpenAI\n- Anthropic\n- Google (Gemini, PaLM2)\n- HuggingFace (Inference and Endpoints)\n- [Ollama](https://github.com/jmorganca/ollama) (locally-hosted models)\n- Microsoft Azure OpenAI Endpoints\n- [AlephAlpha](https://app.aleph-alpha.com/)\n- Foundation models via Amazon Bedrock on-demand inference, including Anthropic Claude 3\n- ...and any other provider through [custom provider scripts](https://chainforge.ai/docs/custom_providers/)!\n\n# Example experiments\n\nWe've prepared a couple example flows to give you a sense of what's possible with Chainforge.\nClick the \"Example Flows\" button on the top-right corner and select one. Here is a basic comparison example, plotting the length of responses across different models and arguments for the prompt parameter `{game}`:\n\n<img width=\"1593\" alt=\"basic-compare\" src=\"https://github.com/ianarawjo/ChainForge/assets/5251713/43c87ab7-aabd-41ba-8d9b-e7e9ebe25c75\">\n\nYou can also conduct **ground truth evaluations** using Tabular Data nodes. For instance, we can compare each LLM's ability to answer math problems by comparing each response to the expected answer:\n\n<img width=\"1775\" alt=\"Screen Shot 2023-07-04 at 9 21 50 AM\" src=\"https://github.com/ianarawjo/ChainForge/assets/5251713/6d842f7a-f747-44f9-b317-95bec73653c5\">\n\n# Compare responses across models and prompts\n\nCompare across models and prompt variables with an interactive response inspector, including a formatted table and exportable data:\n\n<img width=\"1460\" alt=\"Screen Shot 2023-07-19 at 5 03 55 PM\" src=\"https://github.com/ianarawjo/ChainForge/assets/5251713/6aca2bd7-7820-4256-9e8b-3a87795f3e50\">\n\nHere's [a tutorial to get started comparing across prompt templates](https://chainforge.ai/docs/compare_prompts/).\n\n# Share with others\n\nThe web version of ChainForge (https://chainforge.ai/play/) includes a Share button.\n\nSimply click Share to generate a unique link for your flow and copy it to your clipboard:\n\n![ezgif-2-a4d8048bba](https://github.com/ianarawjo/ChainForge/assets/5251713/1c69900b-5a0f-4055-bbd3-ea191e93ecde)\n\nFor instance, here's a experiment I made that tries to get an LLM to reveal a secret key: https://chainforge.ai/play/?f=28puvwc788bog\n\n> **Note**\n> To prevent abuse, you can only share up to 10 flows at a time, and each flow must be <5MB after compression.\n> If you share more than 10 flows, the oldest link will break, so make sure to always Export important flows to `cforge` files,\n> and use Share to only pass data ephemerally.\n\nFor finer details about the features of specific nodes, check out the [List of Nodes](https://chainforge.ai/docs/nodes/).\n\n# Features\n\nA key goal of ChainForge is facilitating **comparison** and **evaluation** of prompts and models. Basic features are:\n\n- **Prompt permutations**: Setup a prompt template and feed it variations of input variables. ChainForge will prompt all selected LLMs with all possible permutations of the input prompt, so that you can get a better sense of prompt quality. You can also chain prompt templates at arbitrary depth (e.g., to compare templates).\n- **Chat turns**: Go beyond prompts and template follow-up chat messages, just like prompts. You can test how the wording of the user's query might change an LLM's output, or compare quality of later responses across multiple chat models (or the same chat model with different settings!).\n- **Model settings**: Change the settings of supported models, and compare across settings. For instance, you can measure the impact of a system message on ChatGPT by adding several ChatGPT models, changing individual settings, and nicknaming each one. ChainForge will send out queries to each version of the model.\n- **Evaluation nodes**: Probe LLM responses in a chain and test them (classically) for some desired behavior. At a basic level, this is Python script based. We plan to add preset evaluator nodes for common use cases in the near future (e.g., name-entity recognition). Note that you can also chain LLM responses into prompt templates to help evaluate outputs cheaply before more extensive evaluation methods.\n- **Visualization nodes**: Visualize evaluation results on plots like grouped box-and-whisker (for numeric metrics) and histograms (for boolean metrics). Currently we only support numeric and boolean metrics. We aim to provide users more control and options for plotting in the future.\n\nTaken together, these features let you easily:\n\n- **Compare across prompts and prompt parameters**: Choose the best set of prompts that maximizes your eval target metrics (e.g., lowest code error rate). Or, see how changing parameters in a prompt template affects the quality of responses.\n- **Compare across models**: Compare responses for every prompt across models and different model settings.\n\nWe've also found that some users simply want to use ChainForge to make tons of parametrized queries to LLMs (e.g., chaining prompt templates into prompt templates), possibly score them, and then output the results to a spreadsheet (Excel `xlsx`). To do this, attach an Inspect node to the output of a Prompt node and click `Export Data`.\n\nFor more specific details, see our [documentation](https://chainforge.ai/docs/nodes/).\n\n---\n\n# Development\n\nChainForge was created by [Ian Arawjo](http://ianarawjo.com/index.html), a postdoctoral scholar in Harvard HCI's [Glassman Lab](http://glassmanlab.seas.harvard.edu/) with support from the Harvard HCI community. Collaborators include PhD students [Priyan Vaithilingam](https://priyan.info) and [Chelse Swoopes](https://seas.harvard.edu/person/chelse-swoopes), Harvard undergraduate [Sean Yang](https://shawsean.com), and faculty members [Elena Glassman](http://glassmanlab.seas.harvard.edu/glassman.html) and [Martin Wattenberg](https://www.bewitched.com/about.html). Additional collaborators include UC Berkeley PhD student Shreya Shankar and Universit√© de Montr√©al undergraduate Cassandre Hamel.\n\nThis work was partially funded by the NSF grants IIS-2107391, IIS-2040880, and IIS-1955699. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.\n\nWe provide ongoing releases of this tool in the hopes that others find it useful for their projects.\n\n## Inspiration and Links\n\nChainForge is meant to be general-purpose, and is not developed for a specific API or LLM back-end. Our ultimate goal is integration into other tools for the systematic evaluation and auditing of LLMs. We hope to help others who are developing prompt-analysis flows in LLMs, or otherwise auditing LLM outputs. This project was inspired by own our use case, but also shares some comraderie with two related (closed-source) research projects, both led by [Sherry Wu](https://www.cs.cmu.edu/~sherryw/):\n\n- \"PromptChainer: Chaining Large Language Model Prompts through Visual Programming\" (Wu et al., CHI ‚Äô22 LBW) [Video](https://www.youtube.com/watch?v=p6MA8q19uo0)\n- \"AI Chains: Transparent and Controllable Human-AI Interaction by Chaining Large Language Model Prompts\" (Wu et al., CHI ‚Äô22)\n\nUnlike these projects, we are focusing on supporting evaluation across prompts, prompt parameters, and models.\n\n## How to collaborate?\n\nWe welcome open-source collaborators. If you want to report a bug or request a feature, open an [Issue](https://github.com/ianarawjo/ChainForge/issues). We also encourage users to implement the requested feature / bug fix and submit a Pull Request.\n\n---\n\n# Cite Us\n\nIf you use ChainForge for research purposes, whether by building upon the source code or investigating LLM behavior using the tool, we ask that you cite our [CHI research paper](https://dl.acm.org/doi/full/10.1145/3613904.3642016) in any related publications. The BibTeX you can use is:\n\n```bibtex\n@inproceedings{arawjo2024chainforge,\n  title={ChainForge: A Visual Toolkit for Prompt Engineering and LLM Hypothesis Testing},\n  author={Arawjo, Ian and Swoopes, Chelse and Vaithilingam, Priyan and Wattenberg, Martin and Glassman, Elena L},\n  booktitle={Proceedings of the CHI Conference on Human Factors in Computing Systems},\n  pages={1--18},\n  year={2024}\n}\n```\n\n# License\n\nChainForge is released under the MIT License.\n"
        },
        {
          "name": "chainforge",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 1.4912109375,
          "content": "from setuptools import setup, find_packages\n\ndef readme():\n    with open('README.md', encoding='utf-8') as f:\n        return f.read()\n\nsetup(\n    name=\"chainforge\",\n    version=\"0.3.2.8\",\n    packages=find_packages(),\n    author=\"Ian Arawjo\",\n    description=\"A Visual Programming Environment for Prompt Engineering\",\n    long_description=readme(),\n    long_description_content_type=\"text/markdown\",\n    keywords=\"prompt engineering LLM response evaluation\",\n    license=\"MIT\",\n    url=\"https://github.com/ianarawjo/ChainForge/\",\n    install_requires=[\n        # Package dependencies\n        \"flask>=2.2.3\",\n        \"flask[async]\",\n        \"flask_cors\",\n        \"requests\",\n        \"urllib3==1.26.6\",\n        \"openai\",\n        \"anthropic\",\n        \"google-generativeai\",\n        \"dalaipy>=2.0.2\",\n        \"mistune>=2.0\",  # for LLM response markdown parsing\n    ],\n    entry_points={\n        \"console_scripts\": [\n            \"chainforge = chainforge.app:main\",\n        ],\n    },\n    classifiers=[\n        # Package classifiers\n        \"Development Status :: 3 - Alpha\",\n        \"Intended Audience :: Developers\",\n        \"License :: OSI Approved :: MIT License\",\n        \"Programming Language :: Python :: 3\",\n        \"Programming Language :: Python :: 3.8\",\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n    ],\n    python_requires=\">=3.8\",\n    include_package_data=True,\n)\n"
        }
      ]
    }
  ]
}