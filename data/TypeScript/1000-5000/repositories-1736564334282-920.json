{
  "metadata": {
    "timestamp": 1736564334282,
    "page": 920,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjkyMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "run-llama/sec-insights",
      "stars": 2409,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".devcontainer",
          "type": "tree",
          "content": null
        },
        {
          "name": "FAQ.md",
          "type": "blob",
          "size": 4.669921875,
          "content": "# Frequently Asked Questions üîç\n\nHere we will go over a list of commonly asked questions and/or concerns regarding this project. You may run into some of these questions yourself when reviewing the project!\n\n## How do I add more SEC documents beyond the selected pool of SEC filings?\nYou can do this by using our [seed script](https://github.com/run-llama/sec-insights/tree/main/backend#seed-db-script-)!\n\nYou can run the seed script with the `--ciks` CLI arg *(e.g. `python scripts/seed_db.py --ciks '[\"1640147\"]'`)*. The `ciks` arg allows you to define which companies you want to download SEC filings for. You can search for the CIK value for a given company using the SECs search tool on [this website](https://www.sec.gov/edgar/searchedgar/companysearch).\n\nAlternatively, you may also just add the CIKs you want to include in your project by modifying the `DEFAULT_CIKS` list [here](https://github.com/run-llama/sec-insights/blob/main/backend/scripts/download_sec_pdf.py#L12).\n\nJust make sure you follow the setup instructions as a pre-requisite to running the seed script :)\n\n## How do I use different types of documents besides SEC filings? e.g. Research papers, internal documents, etc.\nThis can be done!\n\nWhile our frontend is fairly specific to the SEC filing use-case, our backend is setup to be very flexible in terms of the types of documents you can ingest and start asking questions about.\n\nAn in-depth walkthrough on doing this can be found in [our YouTube tutorial](https://youtu.be/2O52Tfj79T4?si=kiRxB2dLES0Gaad7&t=1311).\n\nHere are some high level steps:\n1. Insert the PDF document into your database by using the script in `scripts/upsert_document.py`\n   * The script will print out the newly inserted document's UUID. Make sure to copy this to your clipboard for later!\n1. Start the backend service locally using `make run`\n1. Start the shell-based Chat REPL using `make chat`\n1. Within the REPL:\n   1. First, run `pick_docs`\n   1. Then run `select_id <paste the document UUID you copied earlier>` e.g. `select_id 421b8099-6155-2f6e-8c5b-674ee0ab0e7d`\n   1. Type `finish` to wrap up document selection\n   1. Create your conversation by typing `create`\n   1. Send a message within the newly created conversation with `message <your message here>` e.g. `message What is the document about?`\n      * The first time that there is a message for a newly inserted document, the backend will need to go through the embedding + indexing process for that document which can take some time.\n   1. Start chatting away! The platform should now be ready for questions regarding this document within this Chat REPL.\n\nYou will also find that some of the prompts used in the application are specific to the SEC Insights use case. These will need to be changed to fit your particular use case. Here's an initial list of places in the codebase that may need to be changed to tune the prompts to your use case:\n* [Custom Response Synth prompt](https://github.com/run-llama/sec-insights/blob/e81c839/backend/app/chat/qa_response_synth.py#L15-L48)\n* [Vector Index tool descriptions](https://github.com/run-llama/sec-insights/blob/e81c83958a428e2aa02e8cb1280c3a17c55c4aa9/backend/app/chat/engine.py#L295-L296)\n* System Message ([template](https://github.com/run-llama/sec-insights/blob/e81c83958a428e2aa02e8cb1280c3a17c55c4aa9/backend/app/chat/constants.py#L3-L17) and [construction](https://github.com/run-llama/sec-insights/blob/e81c83958a428e2aa02e8cb1280c3a17c55c4aa9/backend/app/chat/engine.py#L336))\n* [User Message Prefix](https://github.com/run-llama/sec-insights/blob/e81c83958a428e2aa02e8cb1280c3a17c55c4aa9/backend/app/chat/messaging.py#L143-L145)\n\n## How do I completely refresh my database?\nDuring development, you may find it useful or necessary to completely wipe out your database and start fresh with empty tables.\n\nTo make this process simple, we have included a `make refresh_db` command in `backend/Makefile`. To use it, just do the following:\n- `cd` into the `backend/` folder if you're not already in it\n- Run `set -a` then `source .env`\n   - See instructions in `README.md` for more information on what this step does\n- Run `make refresh_db`\n   - This will ask for confirmation first and run as soon as you type either `y` or `N`.\n\n**What is this script doing?**\n\nWhen you run the database in the `db` container using `docker compose` and the various `make` commands, the container shares a data volume with your local machine. This ensures that the data in this local database is persisted even as the `db` container is started and stopped. As such, to completely refresh this database, you would first need to stop your DB container, delete these volumes, re-create the DB container, and re-apply the alembic migrations. That's what `make refresh_db` does.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2023 LlamaIndex\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 7.4697265625,
          "content": "# SEC Insights üè¶\n<a href=\"https://www.producthunt.com/posts/sec-insights-ai?utm_source=badge-top-post-badge&utm_medium=badge&utm_souce=badge-sec&#0045;insights&#0045;ai\" target=\"_blank\"><img src=\"https://api.producthunt.com/widgets/embed-image/v1/top-post-badge.svg?post_id=410213&theme=light&period=daily\" alt=\"SEC&#0032;Insights&#0032;AI - Revolutionizing&#0032;SEC&#0032;document&#0032;analysis | Product Hunt\" style=\"width: 250px; height: 54px;\" width=\"250\" height=\"54\" /></a>\n\n[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://codespaces.new/run-llama/sec-insights)\n\n[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)\n\nSEC Insights uses the Retrieval Augmented Generation (RAG) capabilities of [LlamaIndex](https://github.com/jerryjliu/llama_index) to answer questions about SEC 10-K & 10-Q documents.\n\nYou can start using the application now at [secinsights.ai](https://www.secinsights.ai/)\n\nYou can also check out our [End-to-End tutorial guide on YouTube](https://youtu.be/2O52Tfj79T4?si=CYUcaBkc9P9g_m0P) for this project! This video covers product features, system architecture, development environment setup, and how to use this application with your own custom documents *(beyond just SEC filings!)*. The video has chapters so you can skip to the section most relevant to you.\n\n## Why did we make this? ü§î\nAs RAG applications look to move increasingly from prototype to production, we thought our developer community would find value in having a complete example of a working real-world RAG application.\n\nSEC Insights works as well locally as it does in the cloud. It also comes with many product features that will be immediately applicable to most RAG applications.\n\nUse this repository as a reference when building out your own RAG application or fork it entirely to start your project off with a solid foundation.\n\n## Product Features üòé\n- Chat-based Document Q&A against a pool of documents\n- Citation of source data that LLM response was based on\n- PDF Viewer with highlighting of citations\n- Use of API-based tools ([polygon.io](https://polygon.io/)) for answering quantitative questions\n- Token-level streaming of LLM responses via [Server-Sent Events](https://developer.mozilla.org/en-US/docs/Web/API/Server-sent_events)\n- Streaming of Reasoning Steps (Sub-Questions) within Chat\n\n## Development Features ü§ì\n- Infrastructure-as-code for deploying directly to [Vercel](https://vercel.com/) & [Render](https://render.com/)\n- Continuous deployments provided by Vercel & Render.com. Shipping changes is as easy as merging into your `main` branch.\n- Production & Preview environments for both Frontend & Backend deployments! Easily try your changes before release.\n- Robust local environment setup making use of [LocalStack](https://localstack.cloud/) & [Docker](https://www.docker.com/) compose\n- Monitoring & Profiling provided by [Sentry](https://sentry.io/welcome/)\n- Load Testing provided by [Loader.io](https://loader.io/)\n- Variety of python scripts for REPL-based chat & data management\n\n## Tech Stack ‚öíÔ∏è\n- Frontend\n    - [React](https://react.dev/) / [Next.js](https://nextjs.org/)\n    - [Tailwind CSS](https://tailwindcss.com/)\n- Backend\n    - [FastAPI](https://fastapi.tiangolo.com/)\n    - [Docker](https://www.docker.com/)\n    - [SQLAlchemy](https://www.sqlalchemy.org/)\n    - [OpenAI](https://openai.com/) (gpt-3.5-turbo + text-embedding-ada-002)\n    - [PGVector](https://github.com/pgvector/pgvector)\n    - [LlamaIndex ü¶ô](https://www.llamaindex.ai/)\n- Infrastructure\n    - [Render.com](https://render.com/)\n        - Backend hosting\n        - [Postgres 15](https://www.postgresql.org/)\n    - [Vercel](https://vercel.com/)\n        - Frontend Hosting\n    - [AWS](https://aws.amazon.com/)\n        - [Cloudfront](https://aws.amazon.com/cloudfront/)\n        - [S3](https://aws.amazon.com/s3/)\n\n### System Architecture\n[![System Architecture](https://www.plantuml.com/plantuml/png/jLJ1RjD04BtxAuPmo2bLsgGIaH0YYMqe0XhL4HoggjhOKsVRzMoqEsuR4F_EncxTDEjGX8GFbdRUcpTldZVfGeXNaX2KMEkI8PC6KvQQRF0ggv7FKJo_d9zUdfry-3WFWgR3wiAzUAtS6vabvJQmDv9MmeW2LYAz4Jd2pm3SCt6dtEYIigbMsi3hy70wZ4O0NKYGOT70a5OuQoW4fqlW9O8mHj_LG2scJORcGMXGFLKzriI9_85mE6pEFYjXDAXvlS8jFAuU3s_qsf1gyubMsGuuLZ8dI95S9VWLR6MIAbrc_psHez6R_cJKdi1pFvbWiH1sxqUAmsWIzlq9uU1usE__pOJQQ2t_R4-lUJWS7KTLTRwKwGsXjN3qN8nqji_gt0YoZeN4EtPzx0NB1bCMbAkzgKJZA8p2bjodW-Zu3way2NVEa5pVGQgB3WWBzV5XtdaiB8zd9zLW1rpKrQdH19_qeZusNswcBUS6xMP0VRqwu-y998FEezoiN2YPmYoCOL8wHNuGd1bvAnWXOMr4ZbDDZFVSS9xqedj6Gq91WkPMfcWRwIIQTYr4MIuCECSNyBQNwJlgxRXrixHQvveEf8POag1KEhbGiDXfQryzGMAptZH_qIHP6qdvfadX5UzjEbqXZKyUFRyumwTxcxX47l_KEj_GfAYQ8Bwwv0wkBSIEp4wq8dSXSNpd5KHsNLekaDX2QJULfSmofFhdOGE_7thdDUMYpR5NsQOtDwAnlWstteTsvaitfDLskUgzynstKXsnpOpNN36RhThXFLxz3Vsv7kMV51j_mNjdgYnKy1i0)](https://www.plantuml.com/plantuml/uml/jLJ1RjD04BtxAuPmo2bLsgGIaH0YYMqe0XhL4HoggjhOKsVRzMoqEsuR4F_EncxTDEjGX8GFbdRUcpTldZVfGeXNaX2KMEkI8PC6KvQQRF0ggv7FKJo_d9zUdfry-3WFWgR3wiAzUAtS6vabvJQmDv9MmeW2LYAz4Jd2pm3SCt6dtEYIigbMsi3hy70wZ4O0NKYGOT70a5OuQoW4fqlW9O8mHj_LG2scJORcGMXGFLKzriI9_85mE6pEFYjXDAXvlS8jFAuU3s_qsf1gyubMsGuuLZ8dI95S9VWLR6MIAbrc_psHez6R_cJKdi1pFvbWiH1sxqUAmsWIzlq9uU1usE__pOJQQ2t_R4-lUJWS7KTLTRwKwGsXjN3qN8nqji_gt0YoZeN4EtPzx0NB1bCMbAkzgKJZA8p2bjodW-Zu3way2NVEa5pVGQgB3WWBzV5XtdaiB8zd9zLW1rpKrQdH19_qeZusNswcBUS6xMP0VRqwu-y998FEezoiN2YPmYoCOL8wHNuGd1bvAnWXOMr4ZbDDZFVSS9xqedj6Gq91WkPMfcWRwIIQTYr4MIuCECSNyBQNwJlgxRXrixHQvveEf8POag1KEhbGiDXfQryzGMAptZH_qIHP6qdvfadX5UzjEbqXZKyUFRyumwTxcxX47l_KEj_GfAYQ8Bwwv0wkBSIEp4wq8dSXSNpd5KHsNLekaDX2QJULfSmofFhdOGE_7thdDUMYpR5NsQOtDwAnlWstteTsvaitfDLskUgzynstKXsnpOpNN36RhThXFLxz3Vsv7kMV51j_mNjdgYnKy1i0)\n\n## Usage üíª\nSee `README.md` files in `frontend/` & `backend/` folders for individual setup instructions for each. As mentioned above, we also have a YouTube tutorial [here](https://youtu.be/2O52Tfj79T4?si=1Tm3zvuqna5ei4Cu&t=677) that covers how to setup this project's development environment.\n\nWe've also included a config for a [GitHub Codespace](https://github.com/features/codespaces) in [`.devcontainer/devcontainer.json`](https://github.com/run-llama/sec-insights/blob/main/.devcontainer/devcontainer.json). If you choose to use GitHub Codespaces, your codespace will come pre-configured with a lot of the libraries and system dependencies that are needed to run this project. This is probably the fastest way to get this project up and running! Having said that, developers have successfully set-up this project in Linux, macOS, and Windows environments!\n\nIf you have any questions when trying to run this project, you may find your answer quickly by reviewing our [FAQ](./FAQ.md) or by searching through our [GitHub issues](https://github.com/run-llama/sec-insights/issues)! If you don't see a satisfactory answer to your question, feel free to [open a GitHub issue](https://github.com/run-llama/sec-insights/issues/new) so we may assist you!\n\nWe also have a dedicated [#sec-insights channel on our Discord](https://discord.com/channels/1059199217496772688/1150942525968879636) where we may be able to assist with smaller issues more instantaneously.\n\n## Caveats üßê\n- The frontend currently doesn't support Mobile\n- Our main goal with this project is to provide a solid foundation for full-stack RAG apps. There is still room for improvement in terms of RAG performance!\n\n## Contributing üí°\nWe remain very open to contributions! We're looking forward to seeing the ideas and improvements the LlamaIndex community is able to provide.\n\nHuge shoutout to [**@Evanc123**](https://github.com/Evanc123) for his fantastic work building the frontend for this project!\n"
        },
        {
          "name": "backend",
          "type": "tree",
          "content": null
        },
        {
          "name": "frontend",
          "type": "tree",
          "content": null
        },
        {
          "name": "render.yaml",
          "type": "blob",
          "size": 3.3759765625,
          "content": "previewsEnabled: true\ndatabases:\n  - name: llama-app-db\n    databaseName: llama_app_db\n    plan: pro\n    previewPlan: starter\n\nservices:\n  # A Docker web service\n  # Docs for Render blueprints:\n  # https://render.com/docs/blueprint-spec\n  - type: web\n    name: llama-app-backend\n    runtime: docker\n    repo: https://github.com/run-llama/sec-insights.git\n    region: oregon\n    plan: standard\n    rootDir: ./backend\n    # https://render.com/docs/blueprint-spec#scaling\n    scaling:\n      minInstances: 2\n      maxInstances: 10\n      targetMemoryPercent: 75 # optional if targetCPUPercent is set (valid: 1-90)\n      targetCPUPercent: 75 # optional if targetMemory is set (valid: 1-90)\n    healthCheckPath: /api/health/\n    initialDeployHook: make seed_db_based_on_env\n    envVars:\n      - key: DATABASE_URL\n        fromDatabase:\n          name: llama-app-db\n          property: connectionString\n      - fromGroup: general-settings\n      - fromGroup: prod-web-secrets\n      - fromGroup: preview-web-secrets\n  # A Docker cron service\n  # Runs the seed_db job which should only be upserts and otherwise idempotent\n  - type: cron\n    name: llama-app-cron\n    runtime: docker\n    repo: https://github.com/run-llama/sec-insights.git\n    region: oregon\n    plan: standard\n    rootDir: ./backend\n    # set to the fake date of Feb 31st so it never runs. Meant to be manually triggered.\n    schedule: \"0 5 31 2 ?\"\n    dockerCommand: make seed_db_based_on_env\n    envVars:\n      - key: DATABASE_URL\n        fromDatabase:\n          name: llama-app-db\n          property: connectionString\n      - fromGroup: general-settings\n      - fromGroup: prod-web-secrets\n      - fromGroup: preview-web-secrets\nenvVarGroups:\n- name: general-settings\n  envVars:\n    - key: IS_PREVIEW_ENV\n      value: false\n      previewValue: true\n    - key: LOG_LEVEL\n      value: INFO\n      previewValue: DEBUG\n    - key: BACKEND_CORS_ORIGINS\n      value: '[\"http://localhost\", \"http://localhost:8000\", \"http://localhost:3000\", \"http://127.0.0.1:3000\", \"https://llama-app-backend.onrender.com\", \"https://llama-app-frontend.vercel.app\", \"http://secinsights.ai\", \"http://www.secinsights.ai\", \"https://secinsights.ai\", \"https://www.secinsights.ai\"]'\n    # S3_BUCKET_NAME is the bucket used for the StorageContext of the backend's LlamaIndex chat engine\n    - key: S3_BUCKET_NAME\n      value: llama-app-backend-prod\n      previewValue: llama-app-backend-preview\n    # S3_ASSET_BUCKET_NAME is the bucket used for app assets (e.g. document PDFs)\n    - key: S3_ASSET_BUCKET_NAME\n      value: llama-app-web-assets-prod\n      previewValue: llama-app-web-assets-preview\n    - key: CDN_BASE_URL\n      value: https://d687lz8k56fia.cloudfront.net\n      previewValue: https://dl94gqvzlh4k8.cloudfront.net\n    - key: SENTRY_DSN\n      sync: false\n- name: prod-web-secrets\n  envVars:\n    # Manually add a prod value for OPENAI_API_KEY in Render dashboard\n    - key: OPENAI_API_KEY\n      sync: false\n    - key: AWS_KEY\n      sync: false\n    - key: AWS_SECRET\n      sync: false\n    - key: POLYGON_IO_API_KEY\n      sync: false\n- name: preview-web-secrets\n  envVars:\n    # All env vars in this group should be prefixed with \"PREVIEW_\"\n    # Manually add a preview value for PREVIEW_OPENAI_API_KEY in Render dashboard\n    - key: PREVIEW_OPENAI_API_KEY\n      sync: false\n    - key: PREVIEW_AWS_KEY\n      sync: false\n    - key: PREVIEW_AWS_SECRET\n      sync: false\n    - key: PREVIEW_POLYGON_IO_API_KEY\n      sync: false\n"
        }
      ]
    }
  ]
}