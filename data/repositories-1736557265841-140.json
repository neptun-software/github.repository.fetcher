{
  "metadata": {
    "timestamp": 1736557265841,
    "page": 140,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "abi/screenshot-to-code",
      "stars": 66705,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.06,
          "content": "# Auto detect text files and perform LF normalization\n* text=auto\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.17,
          "content": ".aider*\n\n# Project-related files\n\n# Run logs\nbackend/run_logs/*\n\n# Weird Docker setup related files\nbackend/backend/*\n\n# Env vars\nfrontend/.env.local\n.env\n\n# Mac files\n.DS_Store\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "Evaluation.md",
          "type": "blob",
          "size": 1.17,
          "content": "## Evaluating models and prompts\n\nEvaluation dataset consists of 16 screenshots. A Python script for running screenshot-to-code on the dataset and a UI for rating outputs is included. With this set up, we can compare and evaluate various models and prompts.\n\n### Running evals\n\n- Input screenshots should be located at `backend/evals_data/inputs` and the outputs will be `backend/evals_data/outputs`. If you want to modify this, modify `EVALS_DIR` in `backend/evals/config.py`. You can download the input screenshot dataset here: TODO.\n- Set a stack and model (`STACK` var, `MODEL` var) in `backend/run_evals.py`\n- Run `OPENAI_API_KEY=sk-... python run_evals.py` - this runs the screenshot-to-code on the input dataset in parallel but it will still take a few minutes to complete.\n- Once the script is done, you can find the outputs in `backend/evals_data/outputs`.\n\n### Rating evals\n\nIn order to view and rate the outputs, visit your front-end at `/evals`.\n\n- Rate each output on a scale of 1-4\n- You can also print the page as PDF to share your results with others.\n\nGenerally, I run three tests for each model/prompt + stack combo and take the average score out of those tests to evaluate.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.04,
          "content": "MIT License\n\nCopyright (c) 2023 Abi Raja\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.41,
          "content": "# screenshot-to-code\n\nA simple tool to convert screenshots, mockups and Figma designs into clean, functional code using AI. **Now supporting Claude Sonnet 3.5 and Gemini 2.0 Flash!**\n\nhttps://github.com/abi/screenshot-to-code/assets/23818/6cebadae-2fe3-4986-ac6a-8fb9db030045\n\nSupported stacks:\n\n- HTML + Tailwind\n- HTML + CSS\n- React + Tailwind\n- Vue + Tailwind\n- Bootstrap\n- Ionic + Tailwind\n- SVG\n\nSupported AI models:\n\n- Claude Sonnet 3.5 - Best model!\n- GPT-4o - also recommended!\n- DALL-E 3 or Flux Schnell (using Replicate) for image generation\n\nSee the [Examples](#-examples) section below for more demos.\n\nWe also just added experimental support for taking a video/screen recording of a website in action and turning that into a functional prototype.\n\n![google in app quick 3](https://github.com/abi/screenshot-to-code/assets/23818/8758ffa4-9483-4b9b-bb66-abd6d1594c33)\n\n[Learn more about video here](https://github.com/abi/screenshot-to-code/wiki/Screen-Recording-to-Code).\n\n[Follow me on Twitter for updates](https://twitter.com/_abi_).\n\n## üåç  Hosted Version\n\n[Try it live on the hosted version (paid)](https://screenshottocode.com). If you're a large or medium enterprise (50+ employees), [book a meeting to explore custom enterprise plans](https://cal.com/abi-raja-wy2pfh/30min).\n\n## üõ† Getting Started\n\nThe app has a React/Vite frontend and a FastAPI backend.\n\nKeys needed:\n\n- [OpenAI API key with access to GPT-4](https://github.com/abi/screenshot-to-code/blob/main/Troubleshooting.md) or Anthropic key (optional)\n- Both are recommended so you can compare results from both Claude and GPT4o\n\nIf you'd like to run the app with Ollama open source models (not recommended due to poor quality results), [follow this comment](https://github.com/abi/screenshot-to-code/issues/354#issuecomment-2435479853).\n\nRun the backend (I use Poetry for package management - `pip install poetry` if you don't have it):\n\n```bash\ncd backend\necho \"OPENAI_API_KEY=sk-your-key\" > .env\necho \"ANTHROPIC_API_KEY=your-key\" > .env\npoetry install\npoetry shell\npoetry run uvicorn main:app --reload --port 7001\n```\nYou can also set up the keys using the settings dialog on the front-end (click the gear icon after loading the frontend).\n\nRun the frontend:\n\n```bash\ncd frontend\nyarn\nyarn dev\n```\n\nOpen http://localhost:5173 to use the app.\n\nIf you prefer to run the backend on a different port, update VITE_WS_BACKEND_URL in `frontend/.env.local`\n\nFor debugging purposes, if you don't want to waste GPT4-Vision credits, you can run the backend in mock mode (which streams a pre-recorded response):\n\n```bash\nMOCK=true poetry run uvicorn main:app --reload --port 7001\n```\n\n## Docker\n\nIf you have Docker installed on your system, in the root directory, run:\n\n```bash\necho \"OPENAI_API_KEY=sk-your-key\" > .env\ndocker-compose up -d --build\n```\n\nThe app will be up and running at http://localhost:5173. Note that you can't develop the application with this setup as the file changes won't trigger a rebuild.\n\n## üôã‚Äç‚ôÇÔ∏è FAQs\n\n- **I'm running into an error when setting up the backend. How can I fix it?** [Try this](https://github.com/abi/screenshot-to-code/issues/3#issuecomment-1814777959). If that still doesn't work, open an issue.\n- **How do I get an OpenAI API key?** See https://github.com/abi/screenshot-to-code/blob/main/Troubleshooting.md\n- **How can I configure an OpenAI proxy?** - If you're not able to access the OpenAI API directly (due to e.g. country restrictions), you can try a VPN or you can configure the OpenAI base URL to use a proxy: Set OPENAI_BASE_URL in the `backend/.env` or directly in the UI in the settings dialog. Make sure the URL has \"v1\" in the path so it should look like this: `https://xxx.xxxxx.xxx/v1`\n- **How can I update the backend host that my front-end connects to?** - Configure VITE_HTTP_BACKEND_URL and VITE_WS_BACKEND_URL in front/.env.local For example, set VITE_HTTP_BACKEND_URL=http://124.10.20.1:7001\n- **Seeing UTF-8 errors when running the backend?** - On windows, open the .env file with notepad++, then go to Encoding and select UTF-8.\n- **How can I provide feedback?** For feedback, feature requests and bug reports, open an issue or ping me on [Twitter](https://twitter.com/_abi_).\n\n## üìö Examples\n\n**NYTimes**\n\n| Original                                                                                                                                                        | Replica                                                                                                                                                         |\n| --------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n| <img width=\"1238\" alt=\"Screenshot 2023-11-20 at 12 54 03 PM\" src=\"https://github.com/abi/screenshot-to-code/assets/23818/3b644dfa-9ca6-4148-84a7-3405b6671922\"> | <img width=\"1414\" alt=\"Screenshot 2023-11-20 at 12 59 56 PM\" src=\"https://github.com/abi/screenshot-to-code/assets/23818/26201c9f-1a28-4f35-a3b1-1f04e2b8ce2a\"> |\n\n**Instagram page (with not Taylor Swift pics)**\n\nhttps://github.com/abi/screenshot-to-code/assets/23818/503eb86a-356e-4dfc-926a-dabdb1ac7ba1\n\n**Hacker News** but it gets the colors wrong at first so we nudge it\n\nhttps://github.com/abi/screenshot-to-code/assets/23818/3fec0f77-44e8-4fb3-a769-ac7410315e5d\n"
        },
        {
          "name": "Troubleshooting.md",
          "type": "blob",
          "size": 1.81,
          "content": "### Getting an OpenAI API key with GPT-4 model access\n\nYou don't need a ChatGPT Pro account. Screenshot to code uses API keys from your OpenAI developer account. In order to get access to the GPT4 Vision model, log into your OpenAI account and then, follow these instructions:\n\n1. Open [OpenAI Dashboard](https://platform.openai.com/)\n1. Go to Settings > Billing\n1. Click at the Add payment details\n<img width=\"900\" alt=\"285636868-c80deb92-ab47-45cd-988f-deee67fbd44d\" src=\"https://github.com/abi/screenshot-to-code/assets/23818/4e0f4b77-9578-4f9a-803c-c12b1502f3d7\">\n\n4. You have to buy some credits. The minimum is $5.\n5. Go to Settings > Limits and check at the bottom of the page, your current tier has to be \"Tier 1\" to have GPT4 access\n<img width=\"900\" alt=\"285636973-da38bd4d-8a78-4904-8027-ca67d729b933\" src=\"https://github.com/abi/screenshot-to-code/assets/23818/8d07cd84-0cf9-4f88-bc00-80eba492eadf\">\n\n6. Navigate to OpenAI [api keys](https://platform.openai.com/api-keys) page and create and copy a new secret key.\n7. Go to Screenshot to code and paste it in the Settings dialog under OpenAI key (gear icon). Your key is only stored in your browser. Never stored on our servers.\n\n## Still not working?\n\n- Some users have also reported that it can take upto 30 minutes after your credit purchase for the GPT4 vision model to be activated.\n- You need to add credits to your account AND set it to renew when credits run out in order to be upgraded to Tier 1. Make sure your \"Settings > Limits\" page shows that you are at Tier 1.\n\nIf you've followed these steps, and it still doesn't work, feel free to open a Github issue. We only provide support for the open source version since we don't have debugging logs on the hosted version. If you're looking to use the hosted version, we recommend getting a paid subscription on screenshottocode.com\n"
        },
        {
          "name": "backend",
          "type": "tree",
          "content": null
        },
        {
          "name": "blog",
          "type": "tree",
          "content": null
        },
        {
          "name": "design-docs.md",
          "type": "blob",
          "size": 0.19,
          "content": "## Version History\n\nVersion history is stored as a tree on the client-side.\n\n![Screenshot to Code](https://github.com/abi/screenshot-to-code/assets/23818/e35644aa-b90a-4aa7-8027-b8732796fd7c)\n"
        },
        {
          "name": "docker-compose.yml",
          "type": "blob",
          "size": 0.59,
          "content": "version: '3.9'\n\nservices:\n  backend:\n    build: \n      context: ./backend\n      dockerfile: Dockerfile\n    \n    env_file:\n      - .env\n    \n    # or \n    # environment:\n      #- BACKEND_PORT=7001   # if you change the port, make sure to also change the VITE_WS_BACKEND_URL at frontend/.env.local\n      # - OPENAI_API_KEY=your_openai_api_key\n    \n    ports:\n      - \"${BACKEND_PORT:-7001}:${BACKEND_PORT:-7001}\"\n\n    command: poetry run uvicorn main:app --host 0.0.0.0 --port ${BACKEND_PORT:-7001}\n  \n  frontend:\n    build:\n      context: ./frontend\n      dockerfile: Dockerfile\n    ports:\n      - \"5173:5173\"\n"
        },
        {
          "name": "frontend",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}