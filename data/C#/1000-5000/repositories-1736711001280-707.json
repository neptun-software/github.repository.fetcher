{
  "metadata": {
    "timestamp": 1736711001280,
    "page": 707,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjcxMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "openai/openai-dotnet",
      "stars": 1684,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 8.81640625,
          "content": "# editorconfig.org\n\n# top-most EditorConfig file\nroot = true\n\n[*]\ntrim_trailing_whitespace = true\n\n# Default settings:\n# A newline ending every file\n# Use 4 spaces as indentation\n[sdk/*/{Azure.*,System.*}/**]\ninsert_final_newline = true\nindent_style = space\nindent_size = 4\n\n# C# files\n[src/*/{Custom.*}/**.cs]\n# New line preferences\ncsharp_new_line_before_open_brace = all # vs-default: any\ncsharp_new_line_before_else = true # vs-default: true\ncsharp_new_line_before_catch = true # vs-default: true\ncsharp_new_line_before_finally = true # vs-default: true\ncsharp_new_line_before_members_in_object_initializers = true # vs-default: true\ncsharp_new_line_before_members_in_anonymous_types = true # vs-default: true\ncsharp_new_line_between_query_expression_clauses = true # vs-default: true\n\n# Indentation preferences\ncsharp_indent_block_contents = true # vs-default: true\ncsharp_indent_braces = false # vs-default: false\ncsharp_indent_case_contents = true # vs-default: true\ncsharp_indent_case_contents_when_block = true\ncsharp_indent_switch_labels = true # vs-default: true\ncsharp_indent_labels = one_less_than_current # vs-default: one_less_than_current\n\n# Modifier preferences\ncsharp_preferred_modifier_order = public,private,protected,internal,static,extern,new,virtual,abstract,sealed,override,readonly,unsafe,volatile,async:suggestion\n\n# avoid this. unless absolutely necessary\ndotnet_style_qualification_for_field = false:suggestion # vs-default: false:none\ndotnet_style_qualification_for_property = false:suggestion # vs-default: false:none\ndotnet_style_qualification_for_method = false:suggestion # vs-default: false:none\ndotnet_style_qualification_for_event = false:suggestion # vs-default: false:none\n\n# only use var when it's obvious what the variable type is\ncsharp_style_var_for_built_in_types = false:none # vs-default: true:none\ncsharp_style_var_when_type_is_apparent = false:none # vs-default: true:none\ncsharp_style_var_elsewhere = false:suggestion # vs-default: true:none\n\n# use language keywords instead of BCL types\ndotnet_style_predefined_type_for_locals_parameters_members = true:suggestion # vs-default: true:none\ndotnet_style_predefined_type_for_member_access = true:suggestion # vs-default: true:none\n\n# name all constant fields using PascalCase\ndotnet_naming_rule.constant_fields_should_be_pascal_case.severity = suggestion\ndotnet_naming_rule.constant_fields_should_be_pascal_case.symbols = constant_fields\ndotnet_naming_rule.constant_fields_should_be_pascal_case.style = pascal_case_style\n\ndotnet_naming_symbols.constant_fields.applicable_kinds = field\ndotnet_naming_symbols.constant_fields.required_modifiers = const\n\ndotnet_naming_style.pascal_case_style.capitalization = pascal_case\n\n# static fields should have s_ prefix\ndotnet_naming_rule.static_fields_should_have_prefix.severity = suggestion\ndotnet_naming_rule.static_fields_should_have_prefix.symbols = static_fields\ndotnet_naming_rule.static_fields_should_have_prefix.style = static_prefix_style\n\ndotnet_naming_symbols.static_fields.applicable_kinds = field\ndotnet_naming_symbols.static_fields.required_modifiers = static\ndotnet_naming_symbols.static_fields.applicable_accessibilities = private, internal, private_protected\ndotnet_naming_style.static_prefix_style.required_prefix = s_\ndotnet_naming_style.static_prefix_style.capitalization = camel_case\n\n# internal and private fields should be _camelCase\ndotnet_naming_rule.camel_case_for_private_internal_fields.severity = suggestion\ndotnet_naming_rule.camel_case_for_private_internal_fields.symbols = private_internal_fields\ndotnet_naming_rule.camel_case_for_private_internal_fields.style = camel_case_underscore_style\n\ndotnet_naming_symbols.private_internal_fields.applicable_kinds = field\ndotnet_naming_symbols.private_internal_fields.applicable_accessibilities = private, internal\n\ndotnet_naming_style.camel_case_underscore_style.required_prefix = _\ndotnet_naming_style.camel_case_underscore_style.capitalization = camel_case\n\n# Code style defaults\ncsharp_using_directive_placement = outside_namespace:suggestion\ndotnet_sort_system_directives_first = true # vs-default: true\ncsharp_prefer_braces = true:refactoring\ncsharp_preserve_single_line_blocks = true # vs-default: true\ncsharp_preserve_single_line_statements = false # vs-default: true\ncsharp_prefer_static_local_function = true:suggestion\ncsharp_prefer_simple_using_statement = false:none\ncsharp_style_prefer_switch_expression = true:suggestion\n\n# Code quality\ndotnet_style_readonly_field = true:suggestion\ndotnet_code_quality_unused_parameters = non_public:suggestion\n\n# Expression-level preferences\ndotnet_style_object_initializer = true:suggestion # vs-default: true:suggestion\ndotnet_style_collection_initializer = true:suggestion # vs-default: true:suggestion\ndotnet_style_explicit_tuple_names = true:suggestion # vs-default: true:suggestion\ndotnet_style_coalesce_expression = true:suggestion # vs-default: true:suggestion\ndotnet_style_null_propagation = true:suggestion # vs-default: true:suggestion\ndotnet_style_prefer_is_null_check_over_reference_equality_method = true:suggestion\ndotnet_style_prefer_inferred_tuple_names = true:suggestion\ndotnet_style_prefer_inferred_anonymous_type_member_names = true:suggestion\ndotnet_style_prefer_auto_properties = true:suggestion\ndotnet_style_prefer_conditional_expression_over_assignment = true:refactoring\ndotnet_style_prefer_conditional_expression_over_return = true:refactoring\ncsharp_prefer_simple_default_expression = true:suggestion\n\n# Expression-bodied members\ncsharp_style_expression_bodied_methods = false:none # vs-default: false:none\ncsharp_style_expression_bodied_constructors = false:none # vs-default: false:none\ncsharp_style_expression_bodied_operators = false:none # vs-default: false:none\ncsharp_style_expression_bodied_properties = true:none # vs-default: true:none\ncsharp_style_expression_bodied_indexers = true:none # vs-default: true:none\ncsharp_style_expression_bodied_accessors = true:none # vs-default: true:none\ncsharp_style_expression_bodied_lambdas = true:refactoring\ncsharp_style_expression_bodied_local_functions = true:refactoring\n\n# Pattern matching\ncsharp_style_pattern_matching_over_is_with_cast_check = true:suggestion # vs-default: true:suggestion\ncsharp_style_pattern_matching_over_as_with_null_check = true:suggestion # vs-default: true:suggestion\ncsharp_style_inlined_variable_declaration = true:suggestion # vs-default: true:suggestion\n\n# Null checking preferences\ncsharp_style_throw_expression = true:suggestion # vs-default: true:suggestion\ncsharp_style_conditional_delegate_call = true:suggestion # vs-default: true:suggestion\n\n# Other features\ncsharp_style_prefer_index_operator = false:none\ncsharp_style_prefer_range_operator = false:none\ncsharp_style_pattern_local_over_anonymous_function = false:none\n\n# Space preferences\ncsharp_space_after_cast = false # vs-default: false\ncsharp_space_after_colon_in_inheritance_clause = true # vs-default: true\ncsharp_space_after_comma = true # vs-default: true\ncsharp_space_after_dot = false # vs-default: false\ncsharp_space_after_keywords_in_control_flow_statements = true # vs-default: true\ncsharp_space_after_semicolon_in_for_statement = true # vs-default: true\ncsharp_space_around_binary_operators = before_and_after # vs-default: before_and_after\ncsharp_space_around_declaration_statements = do_not_ignore # vs-default: false\ncsharp_space_before_colon_in_inheritance_clause = true # vs-default: true\ncsharp_space_before_comma = false # vs-default: false\ncsharp_space_before_dot = false # vs-default: false\ncsharp_space_before_open_square_brackets = false # vs-default: false\ncsharp_space_before_semicolon_in_for_statement = false # vs-default: false\ncsharp_space_between_empty_square_brackets = false # vs-default: false\ncsharp_space_between_method_call_empty_parameter_list_parentheses = false # vs-default: false\ncsharp_space_between_method_call_name_and_opening_parenthesis = false # vs-default: false\ncsharp_space_between_method_call_parameter_list_parentheses = false # vs-default: false\ncsharp_space_between_method_declaration_empty_parameter_list_parentheses = false # vs-default: false\ncsharp_space_between_method_declaration_name_and_open_parenthesis = false # vs-default: false\ncsharp_space_between_method_declaration_parameter_list_parentheses = false # vs-default: false\ncsharp_space_between_parentheses = false # vs-default: false\ncsharp_space_between_square_brackets = false # vs-default: false\n\n# Require accessibility modifiers\ndotnet_style_require_accessibility_modifiers = for_non_interface_members:suggestion # vs-default: for_non_interface_members:none\n\n# Analyzers\ndotnet_code_quality.ca1802.api_surface = private, internal\n\n# Xml project files\n[*.{csproj,vcxproj,vcxproj.filters,proj,nativeproj,locproj}]\nindent_size = 2\n\n# Xml build files\n[*.builds]\nindent_size = 2\n\n# Xml files\n[*.{xml,stylecop,resx,ruleset}]\nindent_size = 2\n\n# Xml config files\n[*.{props,targets,config,nuspec}]\nindent_size = 2\n\n# Shell scripts\n[*.sh]\nend_of_line = lf\n[*.{cmd, bat}]\nend_of_line = crlf"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.380859375,
          "content": "## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n\n# User-specific files\n*.suo\n*.user\n*.sln.docstates\n.vs/\n*.lock.json\ndeveloper/\nlaunch.json\nlaunchSettings.json\n\n# Default Assets restore directory\n.assets\n\n# Build results\n/artifacts\nbinaries/\n[Dd]ebug*/\n[Rr]elease/\nbuild/\nrestoredPackages/\nPolicheckOutput/\ntools/net46/\ntools/SdkBuildTools/\ntools/Microsoft.WindowsAzure.Build.Tasks/packages/\nPublishedNugets/\nsrc/NuGet.Config\ntools/7-zip/\n#tools/LocalNugetFeed/Microsoft.Internal.NetSdkBuild.Mgmt.Tools.*.nupkg\n\n[Tt]est[Rr]esult\n[Bb]uild[Ll]og.*\n\n*_i.c\n*_p.c\n*.ilk\n*.meta\n*.obj\n*.pch\n*.pdb\n*.pgc\n*.pgd\n*.rsp\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp\n*.vspscc\n*.vssscc\n.builds\n\n*.pidb\n\n*.log\n*.scc\n# Visual C++ cache files\nipch/\n*.aps\n*.ncb\n*.opensdf\n*.sdf\n\n# Visual Studio profiler\n*.psess\n*.vsp\n\n# VS Code\n**/.vscode/*\n!.vscode/cspell.json\n\n# Code analysis\n*.CodeAnalysisLog.xml\n\n# Guidance Automation Toolkit\n*.gpState\n\n# ReSharper is a .NET coding add-in\n_ReSharper*/\n\n*.[Rr]e[Ss]harper\n\n# Rider IDE\n.idea\n\n# NCrunch\n*.ncrunch*\n.*crunch*.local.xml\n\n# Installshield output folder\n[Ee]xpress\n\n# DocProject is a documentation generator add-in\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\n\n# Click-Once directory\npublish\n\n# Publish Web Output\n*.Publish.xml\n\n# Others\n[Bb]in\n[Oo]bj\nTestResults\n[Tt]est[Rr]esult*\n*.Cache\nClientBin\n~$*\n*.dbmdl\n\n*.[Pp]ublish.xml\n\nGenerated_Code #added for RIA/Silverlight projects\n\n# Build tasks\ntools/*.dll\n\n# Sensitive files\n*.keys\n!Azure.Extensions.AspNetCore.DataProtection.Keys\n!Azure.Security.KeyVault.Keys\n*.pfx\nTestConfigurations.xml\n*.json.env\n*.bicep.env\n\n# Backup & report files from converting an old project file to a newer\n# Visual Studio version. Backup files are not needed, because we have git ;-)\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\n\n# NuGet\npackages\npackages/repositories.config\ntestPackages\n\n# Mac development\n.DS_Store\n\n# Specification DLLs\n*.Specification.dll\n\n# Generated readme.txt files #\nsrc/*/readme.txt\n\nbuild.out\n.nuget/\n\n# Azure Project\ncsx/\n*.GhostDoc.xml\npingme.txt\n\n# TS/Node files\ndist/\nnode_modules/\n\n# MSBuild binary log files\nmsbuild.binlog\n\n# BenchmarkDotNet\nBenchmarkDotNet.Artifacts\n\nartifacts\n.assets\n\n# Temporary typespec folders for typespec generation\nTempTypeSpecFiles/\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 39.6591796875,
          "content": "# Release History\n\n## 2.1.0 (2024-12-04)\n\n### Features added\n\n- OpenAI.Assistants:\n  - Added a `Content` property to `RunStepFileSearchResult` ([`step_details.tool_calls.file_search.results.content` in the REST API](https://platform.openai.com/docs/api-reference/run-steps/step-object)). ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n    - When using an Assistant with the File Search tool, you can use this property to retrieve the contents of the File Search results that were used by the model.\n  - Added `FileSearchRankingOptions` and `FileSearchResults` properties to `RunStepDetailsUpdate`. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n\n### Breaking Changes in Preview APIs\n\n- OpenAI.RealtimeConversation:\n  - Renamed the `From*()` factory methods on `ConversationContentPart` to `Create*Part()` for consistency. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n  - Removed an extraneous `toolCallId` parameter from `ConversationItem.CreateSystemMessage()`. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n- OpenAI.Assistants:\n  - Renamed `RunStepType` to `RunStepKind`. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n  - Changed `RunStepKind` from an \"extensible enum\" to a regular enum. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n  - Renamed the `ToolCallId` property of `RunStepToolCall` to `Id`. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n  - Renamed the `ToolKind` property of `RunStepToolCall` to `Kind`. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n  - Replaced the `FileSearchRanker` and `FileSearchScoreThreshold` properties of `RunStepToolCall` with a new `FileSearchRankingOptions` property that contains both values to make it clearer how they are related. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n\n### Bugs fixed\n\n- OpenAI.RealtimeConversation:\n  - Fixed serialization issues with `ConversationItem` creation of system and assistant messages. ([bf3f0ed](https://github.com/openai/openai-dotnet/commit/bf3f0eddeda1957a998491e36d7fb551e99be916))\n  - Fixed an issue causing a deadlock when calling the `RealtimeConversationSession`'s `SendInputAudio` method overload that takes a `BinaryData` parameter. ([f491c2d](https://github.com/openai/openai-dotnet/commit/f491c2d5a3894953e0bc112431ea3844a64496da))\n\n## 2.1.0-beta.2 (2024-11-04)\n\n### Features added\n\n- OpenAI.Chat:\n  - Added a `StoredOutputEnabled` property to `ChatCompletionOptions` ([`store` in the REST API](https://platform.openai.com/docs/api-reference/chat/create#chat-create-store)). ([b0f9e5c](https://github.com/openai/openai-dotnet/commit/b0f9e5c3b9708a802afa6ce7489636d2084e7d61))\n    - Use this property to indicate whether or not to store the output of the chat completion for use in model distillation or evals.\n  - Added a `Metadata` property to `ChatCompletionOptions` ([`metadata` in the REST API](https://platform.openai.com/docs/api-reference/chat/create#chat-create-metadata)). ([b0f9e5c](https://github.com/openai/openai-dotnet/commit/b0f9e5c3b9708a802afa6ce7489636d2084e7d61))\n    - Use this property to add custom tags and values to the chat completions for filtering in the OpenAI dashboard.\n  - Added an `InputTokenDetails` property to `ChatTokenUsage` ([`usage.prompt_token_details` in the REST API](https://platform.openai.com/docs/api-reference/chat/object#chat/object-usage)). ([b0f9e5c](https://github.com/openai/openai-dotnet/commit/b0f9e5c3b9708a802afa6ce7489636d2084e7d61))\n    - The property is of a new type called `ChatInputTokenUsageDetails`, which contains properties for `AudioTokenCount` and `CachedTokenCount` for usage with supported models.\n  - Added an `AudioTokenCount` property to `ChatOutputTokenUsageDetails` ([`usage.completion_token_details` in the REST API](https://platform.openai.com/docs/api-reference/chat/object#chat/object-usage)). Audio support in chat completions is coming soon. ([b0f9e5c](https://github.com/openai/openai-dotnet/commit/b0f9e5c3b9708a802afa6ce7489636d2084e7d61))\n- OpenAI.Moderations:\n  - Added `Illicit` and `IllicitViolent` properties `ModerationResult` to represent these two new moderation categories. ([b0f9e5c](https://github.com/openai/openai-dotnet/commit/b0f9e5c3b9708a802afa6ce7489636d2084e7d61))\n\n### Breaking Changes in Preview APIs\n\n- OpenAI.RealtimeConversation:\n  - Made improvements to the experimental Realtime API. Please note this features area is currently under rapid development and not all changes may be reflected here. ([b0f9e5c](https://github.com/openai/openai-dotnet/commit/b0f9e5c3b9708a802afa6ce7489636d2084e7d61))\n    - Several types have been renamed for consistency and clarity.\n    - `ConversationRateLimitsUpdate` (previously `ConversationRateLimitsUpdatedUpdate`) now includes named `RequestDetails` and `TokenDetails` properties, mapping to the corresponding named items in the underlying `rate_limits` command payload.\n\n### Bugs Fixed\n\n- OpenAI.RealtimeConversation:\n  - Fixed serialization and deserialization of `ConversationToolChoice` literal values (such as `\"required\"`). ([9de3709](https://github.com/openai/openai-dotnet/commit/9de37095eaad6f1e2e87c201fd693ac1d9757142))\n\n### Other Changes\n\n- Updated the `System.ClientModel` dependency to version `1.2.1`. ([b0f9e5c](https://github.com/openai/openai-dotnet/commit/b0f9e5c3b9708a802afa6ce7489636d2084e7d61))\n  - This updates the `System.Text.Json` transitive dependency to version `6.0.10`, which includes a security compliance fix for [CVE-2024-43485](https://github.com/advisories/GHSA-8g4q-xg66-9fp4). Please note that the OpenAI library was not impacted by this vulnerability since it does not use the `[JsonExtensionData]` feature.\n\n## 2.1.0-beta.1 (2024-10-01)\n\n> [!NOTE]\n> With this updated preview library release, we're excited to bring early support for the newly-announced `/realtime` beta API. You can read more about `/realtime` here: https://openai.com/index/introducing-the-realtime-api/. Given the scope and recency of the feature area, the new `RealtimeConversationClient` is subject to substantial refinement and change over the coming weeks -- this release is purely intended to empower early development against `gpt-4o-realtime-preview` as quickly and efficiently as possible.\n\n### Features Added\n\n- Added a new `RealtimeConversationClient` in a corresponding scenario namespace. ([ff75da4](https://github.com/openai/openai-dotnet/commit/ff75da4167bc83fa85eb69ac142cab88a963ed06))\n  - This maps to the new `/realtime` beta endpoint and is thus marked with a new `[Experimental(\"OPENAI002\")]` diagnostic tag. \n  - This is a very early version of the convenience surface and thus subject to significant change\n  - Documentation and samples will arrive soon; in the interim, see [the scenario test files](/tests/RealtimeConversation) for basic usage\n  - You can also find an external sample employing this client, together with Azure OpenAI support, at https://github.com/Azure-Samples/aoai-realtime-audio-sdk/tree/main/dotnet/samples/console\n\n## 2.0.0 (2024-09-30)\n\n> [!NOTE]\n> First stable version of the official OpenAI library for .NET.\n\n### Features Added\n\n- Support for OpenAI's latest flagship models, including GPT-4o, GPT-4o mini, o1-preview, and o1-mini\n- Support for the entire OpenAI REST API, including:\n  - Structured outputs\n  - Reasoning tokens\n  - Experimental support for Assistants beta v2\n- Support for sync and async APIs\n- Convenient APIs to facilitate working with streaming chat completions and assistants\n- Tons of other quality-of-life features for ease of use and productivity\n\n### Breaking Changes\n\n> [!NOTE]\n> The following breaking changes only apply when upgrading from the previous 2.0.0-beta.* versions.\n\n- Implemented `ChatMessageContent` to encapsulate the representation of content parts in `ChatMessage`, `ChatCompletion`, and `StreamingChatCompletionUpdate`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n- Changed the representation of function arguments to `BinaryData` in `ChatToolCall`, `StreamingChatToolCallUpdate`, `ChatFunctionCall`, and `StreamingChatFunctionCallUpdate`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n- Renamed `OpenAIClientOptions`'s `ApplicationId` to `UserAgentApplicationId`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n- Renamed `StreamingChatToolCallUpdate`'s `Id` to `ToolCallId`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n- Renamed `StreamingChatCompletionUpdate`'s `Id` to `CompletionId`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n- Replaced `Auto` and `None` in the deprecated `ChatFunctionChoice` with `CreateAutoChoice()` and `CreateNoneChoice()`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n- Replaced the deprecated `ChatFunctionChoice(ChatFunction)` constructor with `CreateNamedChoice(string functionName)`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n- Renamed `FileClient` to `OpenAIFileClient` and the corresponding `GetFileClient()` method in `OpenAIClient` to `GetOpenAIFileClient()`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n- Renamed `ModelClient` to `OpenAIModelClient` and the corresponding `GetModelClient()` method in `OpenAIClient` to `GetOpenAIModelClient()`. ([31c2ba6](https://github.com/openai/openai-dotnet/commit/31c2ba63c625b1b4fc2640ddf378a97e89b89167))\n\n## 2.0.0-beta.13 (2024-09-27)\n\n### Breaking Changes\n\n- Refactored `ModerationResult` by merging `ModerationCategories` and `ModerationCategoryScores` into individual `ModerationCategory` properties, each with `Flagged` and `Score` properties. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Renamed type `OpenAIFileInfo` to `OpenAIFile` and `OpenAIFileInfoCollection` to `OpenAIFileCollection`. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Renamed type `OpenAIModelInfo` to `OpenAIModel` and `OpenAIModelInfoCollection` to `OpenAIModelCollection`. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Renamed type `Embedding` to `OpenAIEmbedding` and `EmbeddingCollection` to `OpenAIEmbeddingCollection`. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Renamed property `ImageUrl` to `ImageUri` and method `FromImageUrl` to `FromImageUri` in the `MessageContent` type. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Renamed property `ParallelToolCallsEnabled` to `AllowParallelToolCalls` in the `RunCreationOptions`, `ThreadRun`, and `ChatCompletionOptions` types. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Renamed properties `PromptTokens` to `InputTokenCount`, `CompletionTokens` to `OutputTokenCount`, and `TotalTokens` to `TotalTokenCount` in the `RunTokenUsage` and `RunStepTokenUsage` types. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Renamed properties `InputTokens` to `InputTokenCount` and `TotalTokens` to `TotalTokenCount` in the `EmbeddingTokenUsage` type. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Renamed properties `MaxPromptTokens` to `MaxInputTokenCount` and `MaxCompletionTokens` to `MaxOutputTokenCount` in the `ThreadRun`, `RunCreationOptions`, and `RunIncompleteReason` types. ([19ceae4](https://github.com/openai/openai-dotnet/commit/19ceae44172fdc17af1f47aa30edf4a3bddcb9d6))\n- Removed the `virtual` keyword from the `Pipeline` property across all clients. ([75eded5](https://github.com/openai/openai-dotnet/commit/75eded51db8c8bcec41cd894f3575374e40a4103))\n- Renamed the `Granularities` property of `AudioTranscriptionOptions` to `TimestampGranularities`. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `AudioTranscriptionFormat` from an enum to an \"extensible enum\". ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `AudioTranslationFormat` from an enum to an \"extensible enum\". ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `GenerateImageFormat` from an enum to an \"extensible enum\". ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `GeneratedImageQuality` from an enum to an \"extensible enum\". ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `GeneratedImageStyle` from an enum to an \"extensible enum\". ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Removed method overloads in `AssistantClient` and `VectorStoreClient` that take complex parameters in favor of methods that take simple string IDs. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Updated the `TokenIds` property type in the `TranscribedSegment` type from `IReadOnlyList<int>` to `ReadOnlyMemory<int>`. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Updated the `inputs` parameter type in the `GenerateEmbeddings` and `GenerateEmbeddingsAsync` methods of `EmbeddingClient` from `IEnumerable<IEnumerable<int>>` to `IEnumerable<ReadOnlyMemory<int>>`. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `ChatMessageContentPartKind` from an extensible enum to an enum. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `ChatToolCallKind` from an extensible enum to an enum. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `ChatToolKind` from an extensible enum to an enum. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `OpenAIFilePurpose` from an extensible enum to an enum. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Changed `OpenAIFileStatus` from an extensible enum to an enum. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Renamed `OpenAIFilePurpose` to `FilePurpose`. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Renamed `OpenAIFileStatus` to `FileStatus`. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n- Removed constructors that take string API key and options. ([a330c2e](https://github.com/openai/openai-dotnet/commit/a330c2e703e48179991905e991b0f4186a017198))\n\n## 2.0.0-beta.12 (2024-09-20)\n\n### Features Added\n\n- The library now includes support for the new [OpenAI o1](https://openai.com/o1/) model family. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n  - `ChatCompletionOptions` will automatically apply its `MaxOutputTokenCount` value (renamed from `MaxTokens`) to the new `max_completion_tokens` request body property\n  - `Usage` includes a new `OutputTokenDetails` property with a `ReasoningTokenCount` value that will reflect `o1` model use of this new subcategory of output tokens.\n  - Note that `OutputTokenCount` (`completion_tokens`) is the *sum* of displayed tokens generated by the model *and* (when applicable) these new reasoning tokens\n- Assistants file search now includes support for `RankingOptions`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n  - Use of the `include[]` query string parameter and retrieval of run step detail result content is currently only available via protocol methods\n- Added support for the Uploads API in `FileClient`. This `Experimental` feature allows uploading large files in multiple parts. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n  - The feature is supported by the `CreateUpload`, `AddUploadPart`, `CompleteUpload`, and `CancelUpload` protocol methods.\n\n### Breaking Changes\n\n- Renamed `ChatMessageContentPart`'s `CreateTextMessageContentPart` factory method to `CreateTextPart`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed `ChatMessageContentPart`'s `CreateImageMessageContentPart` factory method to `CreateImagePart`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed `ChatMessageContentPart`'s `CreateRefusalMessageContentPart` factory method to `CreateRefusalPart`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed `ImageChatMessageContentPartDetail` to `ChatImageDetailLevel`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Removed `ChatMessageContentPart`'s `ToString` overload. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed the `MaxTokens` property in `ChatCompletionOptions` to `MaxOutputTokenCount`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed properties in `ChatTokenUsage`:\n  - `InputTokens` is renamed to `InputTokenCount`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n  - `OutputTokens` is renamed to `OutputTokenCount`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n  - `TotalTokens` is renamed to `TotalTokenCount`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Removed the common `ListOrder` enum from the top-level `OpenAI` namespace in favor of individual enums in their corresponding sub-namespace. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed the `PageSize` property to `PageSizeLimit`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Updated deletion methods to return a result object instead of a `bool`. Affected methods:\n  - `DeleteAssitant`, `DeleteMessage`, and `DeleteThread` in `AssistantClient`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n  - `DeleteVectorStore` and `RemoveFileFromStore` in `VectorStoreClient`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n  - `DeleteModel` in `ModelClient`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n  - `DeleteFile` in `FileClient`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Removed setters from collection properties. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed `ChatTokenLogProbabilityInfo` to `ChatTokenLogProbabilityDetails`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed `ChatTokenTopLogProbabilityInfo` to `ChatTokenTopLogProbabilityDetails`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed the `Utf8ByteValues` properties of `ChatTokenLogProbabilityDetails` and `ChatTokenTopLogProbabilityDetails` to `Utf8Bytes` and changed their type from `IReadOnlyList<int>` to `ReadOnlyMemory<byte>?`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed the `Start` and `End` properties of `TranscribedSegment` and `TranscribedWord` to `StartTime` and `EndTime`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Changed the type of `TranscribedSegment`'s `AverageLogProbability` and `NoSpeechProbability` properties from `double` to `float`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Changed the type of `TranscribedSegment`'s `SeekOffset` property from `long` to `int`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Changed the type of `TranscribedSegment`'s `TokenIds` property from `IReadOnlyList<long>` to `IReadOnlyList<int>`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Updated the `Embedding.Vector` property to the `Embedding.ToFloats()` method. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Removed the optional parameter from the constructors of `VectorStoreCreationHelper`, `AssistantChatMessage`, and `ChatFunction`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Removed the optional `purpose` parameter from `FileClient.GetFilesAsync` and `FileClient.GetFiles` methods, and added overloads where `purpose` is required. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Renamed `ModerationClient`'s `ClassifyTextInput` methods to `ClassifyText`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Removed duplicated `Created` property from `GeneratedImageCollection`. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n\n### Bugs Fixed\n\n- Addressed an issue that caused multi-page queries of fine-tuning jobs, checkpoints, and events to fail. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- `ChatCompletionOptions` can now be serialized via `ModelReaderWriter.Write()` prior to calling `CompleteChat` using the options. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n\n### Other Changes\n\n- Added support for `CancellationToken` to `ModelClient` methods. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n- Applied the `Obsolete` attribute where appropriate to align with the existing deprecations in the REST API. ([2ab1a94](https://github.com/openai/openai-dotnet/commit/2ab1a94269125e6bed45d134a402ad8addd8fea4))\n\n## 2.0.0-beta.11 (2024-09-03)\n\n### Features Added\n\n- Added the `OpenAIChatModelFactory` in the `OpenAI.Chat` namespace (a static class that can be used to instantiate OpenAI models for mocking in non-live test scenarios). ([79014ab](https://github.com/openai/openai-dotnet/commit/79014abc01a00e13d5a334d3f6529ed590b8ee98))\n\n### Breaking Changes\n\n- Updated fine-tuning pagination methods `GetJobs`, `GetEvents`, and `GetJobCheckpoints` to return `IEnumerable<ClientResult>` instead of `ClientResult`. ([5773292](https://github.com/openai/openai-dotnet/commit/57732927575c6c48f30bded0afb9f5b16d4f30da))\n- Updated the batching pagination method `GetBatches` to return `IEnumerable<ClientResult>` instead of `ClientResult`. ([5773292](https://github.com/openai/openai-dotnet/commit/57732927575c6c48f30bded0afb9f5b16d4f30da))\n- Changed `GeneratedSpeechVoice` from an enum to an \"extensible enum\". ([79014ab](https://github.com/openai/openai-dotnet/commit/79014abc01a00e13d5a334d3f6529ed590b8ee98))\n- Changed `GeneratedSpeechFormat` from an enum to an \"extensible enum\". ([cc9169a](https://github.com/openai/openai-dotnet/commit/cc9169ad2ff92bb7312eed3b7e64e45da5da1d18))\n- Renamed `SpeechGenerationOptions`'s `Speed` property to `SpeedRatio`. ([cc9169a](https://github.com/openai/openai-dotnet/commit/cc9169ad2ff92bb7312eed3b7e64e45da5da1d18))\n\n### Bugs Fixed\n\n- Corrected an internal deserialization issue that caused recent updates to Assistants `file_search` to fail when streaming a run. Strongly typed support for `ranking_options` is not included but will arrive soon. ([cc9169a](https://github.com/openai/openai-dotnet/commit/cc9169ad2ff92bb7312eed3b7e64e45da5da1d18))\n- Mitigated a .NET runtime issue that prevented `ChatResponseFormat` from serializing correct on targets including Unity. ([cc9169a](https://github.com/openai/openai-dotnet/commit/cc9169ad2ff92bb7312eed3b7e64e45da5da1d18))\n\n### Other Changes\n\n- Reverted the removal of the version path parameter \"v1\" from the default endpoint URL. ([583e9f6](https://github.com/openai/openai-dotnet/commit/583e9f6f519feeee0e2907e80bf7d5bf8302d93f))\n- Added the `Experimental` attribute to the following APIs:\n  - All public APIs in the `OpenAI.Assistants` namespace. ([79014ab](https://github.com/openai/openai-dotnet/commit/79014abc01a00e13d5a334d3f6529ed590b8ee98))\n  - All public APIs in the `OpenAI.VectorStores` namespace. ([79014ab](https://github.com/openai/openai-dotnet/commit/79014abc01a00e13d5a334d3f6529ed590b8ee98))\n  - All public APIs in the `OpenAI.Batch` namespace. ([0f5e024](https://github.com/openai/openai-dotnet/commit/0f5e0249cffd42755fc9a820e65fb025fd4f986c))\n  - All public APIs in the `OpenAI.FineTuning` namespace. ([0f5e024](https://github.com/openai/openai-dotnet/commit/0f5e0249cffd42755fc9a820e65fb025fd4f986c))\n  - The `ChatCompletionOptions.Seed` property. ([0f5e024](https://github.com/openai/openai-dotnet/commit/0f5e0249cffd42755fc9a820e65fb025fd4f986c))\n\n## 2.0.0-beta.10 (2024-08-26)\n\n### Breaking Changes\n\n- Renamed `AudioClient`'s `GenerateSpeechFromText` methods to simply `GenerateSpeech`. ([d84bf54](https://github.com/openai/openai-dotnet/commit/d84bf54df14ddac4c49f6efd61467b600d34ecd7))\n- Changed the type of `OpenAIFileInfo`'s `SizeInBytes` property from `long?` to `int?`. ([d84bf54](https://github.com/openai/openai-dotnet/commit/d84bf54df14ddac4c49f6efd61467b600d34ecd7)) \n\n### Bugs Fixed\n\n- Fixed a newly introduced bug ([#185](https://github.com/openai/openai-dotnet/pull/185)) where providing `OpenAIClientOptions` to a top-level `OpenAIClient` did not carry over to scenario clients (e.g. `ChatClient`) created via that top-level client ([d84bf54](https://github.com/openai/openai-dotnet/commit/d84bf54df14ddac4c49f6efd61467b600d34ecd7))\n\n### Other Changes\n\n- Removed the version path parameter \"v1\" from the default endpoint URL. ([d84bf54](https://github.com/openai/openai-dotnet/commit/d84bf54df14ddac4c49f6efd61467b600d34ecd7))\n\n## 2.0.0-beta.9 (2024-08-23)\n\n### Features Added\n\n- Added support for the new [structured outputs](https://platform.openai.com/docs/guides/structured-outputs/introduction) response format feature, which enables chat completions, assistants, and tools on each of those clients to provide a specific JSON Schema that generated content should adhere to. ([3467b53](https://github.com/openai/openai-dotnet/commit/3467b535c918e72237a4c0dc36d4bda5548edb7a))\n  - To enable top-level structured outputs for response content, use `ChatResponseFormat.CreateJsonSchemaFormat()` and `AssistantResponseFormat.CreateJsonSchemaFormat()` as the `ResponseFormat` in method options like `ChatCompletionOptions`\n  - To enable structured outputs for function tools, set `StrictParameterSchemaEnabled` to `true` on the tool definition\n  - For more information, please see [the new section in readme.md](readme.md#how-to-use-structured-outputs)\n- Chat completions: the request message types of `AssistantChatMessage`, `SystemChatMessage`, and `ToolChatMessage` now support array-based content part collections in addition to simple string input. ([3467b53](https://github.com/openai/openai-dotnet/commit/3467b535c918e72237a4c0dc36d4bda5548edb7a))\n- Added the following model factories (static classes that can be used to instantiate OpenAI models for mocking in non-live test scenarios):\n  - `OpenAIAudioModelFactory` in the `OpenAI.Audio` namespace ([3284295](https://github.com/openai/openai-dotnet/commit/3284295e7fd9922a3395d921513473bcb483655e))\n  - `OpenAIEmbeddingsModelFactory` in the `OpenAI.Embeddings` namespace ([3284295](https://github.com/openai/openai-dotnet/commit/3284295e7fd9922a3395d921513473bcb483655e))\n  - `OpenAIFilesModelFactory` in the `OpenAI.Files` namespace ([b1ce397](https://github.com/openai/openai-dotnet/commit/b1ce397ff4f9a55db797167be9e86e138ed5d403))\n  - `OpenAIImagesModelFactory` in the `OpenAI.Images` namespace ([3284295](https://github.com/openai/openai-dotnet/commit/3284295e7fd9922a3395d921513473bcb483655e))\n  - `OpenAIModelsModelFactory` in the `OpenAI.Models` namespace ([b1ce397](https://github.com/openai/openai-dotnet/commit/b1ce397ff4f9a55db797167be9e86e138ed5d403))\n  - `OpenAIModerationsModelFactory` in the `OpenAI.Moderations` namespace ([b1ce397](https://github.com/openai/openai-dotnet/commit/b1ce397ff4f9a55db797167be9e86e138ed5d403))\n\n### Breaking Changes\n\n- Removed client constructors that do not explicitly take an API key parameter or an endpoint via an `OpenAIClientOptions` parameter, making it clearer how to appropriately instantiate a client. ([13a9c68](https://github.com/openai/openai-dotnet/commit/13a9c68647c8d54475f1529a63b13ad711bd4ba6))\n- Removed the endpoint parameter from all client constructors, making it clearer that an alternative endpoint must be specified via the `OpenAIClientOptions` parameter. ([13a9c68](https://github.com/openai/openai-dotnet/commit/13a9c68647c8d54475f1529a63b13ad711bd4ba6))\n- Removed `OpenAIClient`'s `Endpoint` `protected` property. ([13a9c68](https://github.com/openai/openai-dotnet/commit/13a9c68647c8d54475f1529a63b13ad711bd4ba6))\n- Made `OpenAIClient`'s constructor that takes a `ClientPipeline` parameter `protected internal` instead of just `protected`. ([13a9c68](https://github.com/openai/openai-dotnet/commit/13a9c68647c8d54475f1529a63b13ad711bd4ba6))\n- Renamed the `User` property in applicable Options classes to `EndUserId`, making its purpose clearer. ([13a9c68](https://github.com/openai/openai-dotnet/commit/13a9c68647c8d54475f1529a63b13ad711bd4ba6))\n\n### Bugs Fixed\n\n- The `Assistants` namespace `VectorStoreCreationHelper` type now properly includes a `ChunkingStrategy` property. ([3467b53](https://github.com/openai/openai-dotnet/commit/3467b535c918e72237a4c0dc36d4bda5548edb7a))\n\n### Other Changes\n\n- `ChatCompletion.ToString()` will no longer throw an exception when no content is present, as is the case for tool calls. Additionally, if a tool call is present with no content, `ToString()` will return the serialized form of the first available tool call. ([3467b53](https://github.com/openai/openai-dotnet/commit/3467b535c918e72237a4c0dc36d4bda5548edb7a))\n\n## 2.0.0-beta.8 (2024-07-31)\n\n### Breaking Changes\n\n- Changed name of return types from methods returning streaming collections from `ResultCollection` to `CollectionResult`. ([7bdecfd](https://github.com/openai/openai-dotnet/commit/7bdecfd8d294be933c7779c7e5b6435ba8a8eab0))\n- Changed return types from methods returning paginated collections from `PageableCollection` to `PageCollection`. ([7bdecfd](https://github.com/openai/openai-dotnet/commit/7bdecfd8d294be933c7779c7e5b6435ba8a8eab0))\n- Users must now call `GetAllValues` on the collection of pages to enumerate collection items directly. Corresponding protocol methods return `IEnumerable<ClientResult>` where each collection item represents a single service response holding a page of values. ([7bdecfd](https://github.com/openai/openai-dotnet/commit/7bdecfd8d294be933c7779c7e5b6435ba8a8eab0))\n- Updated `VectorStoreFileCounts` and `VectorStoreFileAssociationError` types from `readonly struct` to `class`. ([58f93c8](https://github.com/openai/openai-dotnet/commit/58f93c8d5ea080adfee8b37ae3cc034ebb06c79f))\n\n### Bugs Fixed\n\n- ([#49](https://github.com/openai/openai-dotnet/issues/49)) Fixed a bug with extensible enums implementing case-insensitive equality but case-sensitive hash codes. ([0c12500](https://github.com/openai/openai-dotnet/commit/0c125002ffd791594597ef837f4d10582bdff004))\n- ([#57](https://github.com/openai/openai-dotnet/issues/57)) Fixed a bug with requests URIs with query string parameter potentially containing a malformed double question mark (`??`) on .NET Framework (net481). ([0c12500](https://github.com/openai/openai-dotnet/commit/0c125002ffd791594597ef837f4d10582bdff004))\n- Added optional `CancellationToken` parameters to methods for `AssistantClient` and `VectorStore` client, consistent with past changes in [19a65a0](https://github.com/openai/openai-dotnet/commit/19a65a0a943fa3bef1ec8504708aaa526a1ee03a). ([d77539c](https://github.com/openai/openai-dotnet/commit/d77539ca04467c166f848953eb866012a265555c))\n- Fixed Assistants `FileSearchToolDefinition`'s `MaxResults` parameter to appropriately serialize and deserialize the value ([d77539c](https://github.com/openai/openai-dotnet/commit/d77539ca04467c166f848953eb866012a265555c))\n- Added missing `[EditorBrowsable(EditorBrowsableState.Never)]` attributes to `AssistantClient` protocol methods, which should improve discoverability of the strongly typed methods. ([d77539c](https://github.com/openai/openai-dotnet/commit/d77539ca04467c166f848953eb866012a265555c))\n\n### Other Changes\n\n- Removed the usage of `init` and updated properties to use `set`. ([58f93c8](https://github.com/openai/openai-dotnet/commit/58f93c8d5ea080adfee8b37ae3cc034ebb06c79f))\n\n## 2.0.0-beta.7 (2024-06-24)\n\n### Bugs Fixed\n\n- ([#84](https://github.com/openai/openai-dotnet/issues/84)) Fixed a `NullReferenceException` thrown when adding the custom headers policy while `OpenAIClientOptions` is null ([0b97311](https://github.com/openai/openai-dotnet/commit/0b97311f58dfb28bd883d990f68d548da040a807))\n\n## 2.0.0-beta.6 (2024-06-21)\n\n### Features Added\n\n- `OrganizationId` and `ProjectId` are now present on `OpenAIClientOptions`. When instantiating a client, providing an instance of `OpenAIClientOptions` with these properties set will cause the client to add the appropriate request headers for org/project, eliminating the need to manually configure the headers. ([9ee7dff](https://github.com/openai/openai-dotnet/commit/9ee7dff064a9412c069a793ff62096b8db4aa43d))\n\n### Bugs Fixed\n\n- ([#72](https://github.com/openai/openai-dotnet/issues/72)) Fixed `filename` request encoding in operations using `multipart/form-data`, including `files` and `audio` ([2ba8e69](https://github.com/openai/openai-dotnet/commit/2ba8e69512e187ea0b761edda8bce2cd5c79c58a))\n- ([#79](https://github.com/openai/openai-dotnet/issues/79)) Fixed hard-coded `user` role for caller-created Assistants API messages on threads ([d665b61](https://github.com/openai/openai-dotnet/commit/d665b61fc7ef1ada00a8ef5c00d1a47d276be032))\n- Fixed non-streaming Assistants API run step details not reporting code interpreter logs when present ([d665b61](https://github.com/openai/openai-dotnet/commit/d665b61fc7ef1ada00a8ef5c00d1a47d276be032))\n\n### Breaking Changes\n\n**Assistants (beta)**:\n\n- `AssistantClient.CreateMessage()` and the explicit constructor for `ThreadInitializationMessage` now require a `MessageRole` parameter. This properly enables the ability to create an Assistant message representing conversation history on a new thread. ([d665b61](https://github.com/openai/openai-dotnet/commit/d665b61fc7ef1ada00a8ef5c00d1a47d276be032))\n\n## 2.0.0-beta.5 (2024-06-14)\n\n### Features Added\n\n- API updates, current to [openai/openai-openapi@dd73070b](https://github.com/openai/openai-openapi/commit/dd73070b1d507645d24c249a63ebebd3ec38c0cb) ([1af6569](https://github.com/openai/openai-dotnet/commit/1af6569e2ceae9d840b8826e42d7e3b2569b43f6))\n  - This includes `MaxResults` for Assistants `FileSearchToolDefinition`, `ParallelToolCallsEnabled` for function tools in Assistants and Chat, and `FileChunkingStrategy` for Assistants VectorStores\n- Optional `CancellationToken` parameters are now directly present on most methods, eliminating the need to use protocol methods ([19a65a0](https://github.com/openai/openai-dotnet/commit/19a65a0a943fa3bef1ec8504708aaa526a1ee03a))\n\n### Bugs Fixed\n\n- ([#30](https://github.com/openai/openai-dotnet/issues/30)) Mitigated a .NET runtime issue that prevented chat message content and several other types from serializing correct on targets including mono and wasm ([896b9e0](https://github.com/openai/openai-dotnet/commit/896b9e0bc60f0ace90bd0d1af1254cf2680f8df6))\n- Assistants: Fixed an issue that threw an exception when receiving code interpreter run step logs when streaming a run ([207d597](https://github.com/openai/openai-dotnet/commit/207d59762e7eeb666b7ab2728a0bbee7c0cdd918))\n- Fixed a concurrency condition that could cause `multipart/form-data` requests to no longer generate random content part boundaries (no direct scenario impact) ([7cacdee](https://github.com/openai/openai-dotnet/commit/7cacdee2366df3cfa7e6c43bb050da54d23f8db9))\n\n### Breaking Changes\n\n**Assistants (beta)**:\n\n- `InputQuote` is removed from Assistants `TextAnnotation` and `TextAnnotationUpdate`, per [openai/openai-openapi@dd73070b](https://github.com/openai/openai-openapi/commit/dd73070b1d507645d24c249a63ebebd3ec38c0cb) ([1af6569](https://github.com/openai/openai-dotnet/commit/1af6569e2ceae9d840b8826e42d7e3b2569b43f6))\n\n### Other Changes\n\n- Added an environment-variable-based test project to the repository with baseline scenario coverage ([db6328a](https://github.com/openai/openai-dotnet/commit/db6328accdd7927f19915cdc5412eb841f2447c1))\n- Build/analyzer warnings cleaned up throughout the project ([45fc4d7](https://github.com/openai/openai-dotnet/commit/45fc4d72c12314aea83264ebe2e1dc18870e5c06), [b1fa082](https://github.com/openai/openai-dotnet/commit/b1fa0828a875906ef33ffe43ff1cd1a85fbd1a60), [22ab606](https://github.com/openai/openai-dotnet/commit/22ab606b867bbe0ea7f6918843dbc5e11dfe78eb))\n- Proactively aligned library's implementation of server-sent event (SSE) handling with the source of the incoming `System.Net.ServerSentEvents` namespace ([674e0f7](https://github.com/openai/openai-dotnet/commit/674e0f773b26a22eb039e879539c4c7a44fdffdd))\n\n## 2.0.0-beta.4 (2024-06-10)\n\n### Features Added\n\n- Added new, built-in helpers to simplify the use of text-only message content ([1c40de6](https://github.com/openai/openai-dotnet/commit/1c40de673a67ddf834b673aaabb94b2c42076e03))\n\n### Bugs Fixed\n\n- Optimized embedding deserialization and addressed correctness on big endian systems ([e28b5a7](https://github.com/openai/openai-dotnet/commit/e28b5a7787df4b1baa772406b09a0f650a45c77f))\n- Optimized b64_json message parsing via regex ([efd76b5](https://github.com/openai/openai-dotnet/commit/efd76b50f094c585350240aea051ba342c6f6622))\n\n## 2.0.0-beta.3 (2024-06-07)\n\n### Bugs Fixed\n\n- Removed a vestigial package reference ([5874f53](https://github.com/openai/openai-dotnet/commit/5874f533722ab46a3e077dacb6c3474e0ecca96e))\n\n## 2.0.0-beta.2 (2024-06-06)\n\n### Bugs Fixed\n\n- Addressed an assembly properties issue ([bf21eb5](https://github.com/openai/openai-dotnet/commit/bf21eb5ad367aaac418dbbf320f98187fee5089a))\n- Added migration guide to package ([f150666](https://github.com/openai/openai-dotnet/commit/f150666cd2ed552720207098b3b604a8e1ca73df))\n\n## 2.0.0-beta.1 (2024-06-06)\n\n### Features Added\n\nThis is the official OpenAI client library for C# / .NET. It provides convenient access to the OpenAI REST API from .NET applications and supports all the latest features. It is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) in collaboration with Microsoft.\n\n### Breaking Changes\n\nIf you are a user migrating from version 1.11.0 or earlier, we will soon share a migration guide to help you get started.\n\n- ***Addendum:** the [migration guide](https://github.com/openai/openai-dotnet/blob/main/MigrationGuide.md) is now available.*"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0693359375,
          "content": "The MIT License (MIT)\n\nCopyright (c) 2024 OpenAI (https://openai.com)\n\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n"
        },
        {
          "name": "MigrationGuide.md",
          "type": "blob",
          "size": 11.052734375,
          "content": "# Guide for migrating to OpenAI 2.0.0-beta.1 or higher from OpenAI 1.11.0\n\nThis guide is intended to assist in the migration to the official OpenAI library (2.0.0-beta.1 or higher) from [OpenAI 1.11.0][openai_1110], focusing on side-by-side comparisons for similar operations between libraries. Version 2.0.0-beta.1 will be used for comparison with 1.11.0 but this guide can still be safely used when migrating to higher versions.\n\nPrior to 2.0.0-beta.1, the OpenAI package was a community library not officially supported by OpenAI. See the [CHANGELOG][changelog] for more details.\n\nFamiliarity with the OpenAI 1.11.0 package is assumed. For those new to any OpenAI library for .NET, see the [README][readme] rather than this guide.\n\n## Table of contents\n- [Client usage](#client-usage)\n- [Authentication](#authentication)\n- [Highlighted scenarios](#highlighted-scenarios)\n    - [Chat Completions: Text generation](#chat-completions-text-generation)\n    - [Chat Completions: Streaming](#chat-completions-streaming)\n    - [Chat Completions: JSON mode](#chat-completions-json-mode)\n    - [Chat Completions: Vision](#chat-completions-vision)\n    - [Audio: Speech-to-text](#audio-speech-to-text)\n    - [Audio: Text-to-speech](#audio-text-to-speech)\n    - [Image: Image generation](#image-image-generation)\n- [Additional examples](#additional-examples)\n\n## Client usage\n\nThe client usage has considerably changed between libraries. While the OpenAI 1.11.0 had a single client, `OpenAIAPI`, from which multiple APIs could be accessed, OpenAI 2.0.0-beta.1 keeps a separate client per API. The following snippets illustrate this difference when invoking the image generation capability from the Image API:\n\nOpenAI 1.11.0:\n```cs\nOpenAIAPI api = new OpenAIAPI(\"<api-key>\");\nImageResult result = await api.ImageGenerations.CreateImageAsync(\"Draw a quick brown fox jumping over a lazy dog.\", Model.DALLE3);\n```\n\nOpenAI 2.0.0-beta.1:\n```cs\nImageClient client = new ImageClient(\"dall-e-3\", \"<api-key>\");\nClientResult<GeneratedImage> result = await client.GenerateImageAsync(\"Draw a quick brown fox jumping over a lazy dog.\");\n```\n\nAnother major difference highlighted in the snippets above is that OpenAI 2.0.0-beta.1 requires the model to be explicitly set during client instantiation, while the `OpenAIAPI` client allows a model to be specified per call.\n\nThe table below illustrates to which client each endpoint of `OpenAIAPI` was ported. Note that the deprecated Completions API is not supported in 2.0.0-beta.1:\n\nOld library's endpoint|New library's client\n|-|-\n|Chat | ChatClient\n|ImageGenerations | ImageClient\n|TextToSpeech | AudioClient\n|Transcriptions | AudioClient\n|Translations | AudioClient\n|Moderation | ModerationClient\n|Embeddings | EmbeddingClient\n|Files | OpenAIFileClient\n|Models | OpenAIModelClient\n|Completions | Not supported\n\n## Authentication\n\nTo authenticate to OpenAI, you must set an API key when creating a client.\n\nOpenAI 1.11.0 allowed setting the API key in 3 different ways:\n- Directly from a string\n- From an environment variable\n- From a configuration file\n\n```cs\nOpenAIAPI api;\n\n// Sets the API key directly from a string.\napi = new OpenAIAPI(\"<api-key>\");\n\n// Attempts to load the API key from environment variables OPENAI_KEY and OPENAI_API_KEY.\napi = new OpenAIAPI(APIAuthentication.LoadFromEnv());\n\n// Attempts to load the API key from a configuration file.\napi = new OpenAIAPI(APIAuthentication.LoadFromPath(\"<directory>\", \"<filename>\"));\n```\n\nOpenAI 2.0.0-beta.1 only supports setting it from a string or from an environment variable. The following snippet illustrates the behavior with the `ChatClient`, but other clients behave the same:\n\n```cs\nChatClient client;\n\n// Sets the API key directly from a string.\nclient = new ChatClient(\"gpt-3.5-turbo\", \"<api-key>\");\n\n// When no API key string is specified, attempts to load the API key from the environment variable OPENAI_API_KEY.\nclient = new ChatClient(\"gpt-3.5-turbo\");\n```\n\nNote that, unlike the OpenAI 1.11.0, OpenAI 2.0.0-beta.1 will never attempt to load the API key from the `OPENAI_KEY` environment variable. Only `OPENAI_API_KEY` is supported.\n\n## Highlighted scenarios\n\nThe following sections illustrate side-by-side comparisons for similar operations between the two libraries, highlighting common scenarios.\n\n### Chat Completions: Text generation\n\nOpenAI 1.11.0:\n```cs\nOpenAIAPI api = new OpenAIAPI(\"<api-key>\");\nConversation conversation = api.Chat.CreateConversation();\n\nconversation.Model = Model.ChatGPTTurbo;\nconversation.AppendSystemMessage(\"You are a helpful assistant.\");\nconversation.AppendUserInput(\"When was the Nobel Prize founded?\");\n\nawait conversation.GetResponseFromChatbotAsync();\n\nconversation.AppendUserInput(\"Who was the first person to be awarded one?\");\n\nawait conversation.GetResponseFromChatbotAsync();\n\nforeach (ChatMessage message in conversation.Messages)\n{\n    Console.WriteLine($\"{message.Role}: {message.TextContent}\");\n}\n```\n\nOpenAI 2.0.0-beta.1:\n```cs\nChatClient client = new ChatClient(\"gpt-3.5-turbo\", \"<api-key>\");\nList<ChatMessage> messages = new List<ChatMessage>()\n{\n    new SystemChatMessage(\"You are a helpful assistant.\"),\n    new UserChatMessage(\"When was the Nobel Prize founded?\")\n};\n\nClientResult<ChatCompletion> result = await client.CompleteChatAsync(messages);\n\nmessages.Add(new AssistantChatMessage(result));\nmessages.Add(new UserChatMessage(\"Who was the first person to be awarded one?\"));\n\nresult = await client.CompleteChatAsync(messages);\n\nmessages.Add(new AssistantChatMessage(result));\n\nforeach (ChatMessage message in messages)\n{\n    string role = message.GetType().Name;\n    string text = message.Content[0].Text;\n\n    Console.WriteLine($\"{role}: {text}\");\n}\n```\n\n### Chat Completions: Streaming\n\nOpenAI 1.11.0:\n```cs\nOpenAIAPI api = new OpenAIAPI(\"<api-key>\");\nConversation conversation = api.Chat.CreateConversation();\n\nconversation.Model = Model.ChatGPTTurbo;\nconversation.AppendUserInput(\"Give me a list of Nobel Prize winners of the last 5 years.\");\n\nawait foreach (string response in conversation.StreamResponseEnumerableFromChatbotAsync())\n{\n    Console.Write(response);\n}\n```\n\nOpenAI 2.0.0-beta.1:\n```cs\nChatClient client = new ChatClient(\"gpt-3.5-turbo\", \"<api-key>\");\nList<ChatMessage> messages = new List<ChatMessage>()\n{\n    new UserChatMessage(\"Give me a list of Nobel Prize winners of the last 5 years.\")\n};\n\nawait foreach (StreamingChatCompletionUpdate chatUpdate in client.CompleteChatStreamingAsync(messages))\n{\n    if (chatUpdate.ContentUpdate.Count > 0)\n    {\n        Console.Write(chatUpdate.ContentUpdate[0].Text);\n    }\n}\n```\n\n### Chat Completions: JSON mode\n\nOpenAI 1.11.0:\n```cs\nOpenAIAPI api = new OpenAIAPI(\"<api-key>\");\nChatRequest request = new ChatRequest()\n{\n    Model = Model.ChatGPTTurbo,\n    ResponseFormat = request.ResponseFormats.JsonObject,\n    Messages = new List<ChatMessage>()\n    {\n        new ChatMessage(ChatMessageRole.System, \"You are a helpful assistant designed to output JSON.\"),\n        new ChatMessage(ChatMessageRole.User, \"Give me a JSON object listing Nobel Prize winners of the last 5 years.\")\n    }\n};\n\nChatResult result = await api.Chat.CreateChatCompletionAsync(request);\n\nConsole.WriteLine(result);\n```\n\nOpenAI 2.0.0-beta.1:\n```cs\nChatClient client = new ChatClient(\"gpt-3.5-turbo\", \"<api-key>\");\nList<ChatMessage> messages = new List<ChatMessage>()\n{\n    new SystemChatMessage(\"You are a helpful assistant designed to output JSON.\"),\n    new UserChatMessage(\"Give me a JSON object listing Nobel Prize winners of the last 5 years.\")\n};\nChatCompletionOptions options = new ChatCompletionOptions()\n{\n    ResponseFormat = ChatResponseFormat.JsonObject\n};\n\nClientResult<ChatCompletion> result = await client.CompleteChatAsync(messages, options);\nstring text = result.Value.Content[0].Text;\n\nConsole.WriteLine(text);\n```\n\n### Chat Completions: Vision\n\nOpenAI 1.11.0:\n```cs\nOpenAIAPI api = new OpenAIAPI(\"<api-key>\");\nConversation conversation = api.Chat.CreateConversation();\nbyte[] imageData = await File.ReadAllBytesAsync(\"<file-path>\");\n\nconversation.Model = Model.GPT4_Vision;\nconversation.AppendUserInput(\"Describe this image.\", ImageInput.FromImageBytes(imageData));\n\nstring response = await conversation.GetResponseFromChatbotAsync();\n\nConsole.WriteLine(response);\n```\n\nOpenAI 2.0.0-beta.1:\n```cs\nChatClient client = new ChatClient(\"gpt-4-vision-preview\", \"<api-key>\");\nusing FileStream file = File.OpenRead(\"<file-path>\");\nBinaryData imageData = await BinaryData.FromStreamAsync(file);\nList<ChatMessage> messages = new List<ChatMessage>()\n{\n    new UserChatMessage(\n        ChatMessageContentPart.CreateTextMessageContentPart(\"Describe this image.\"),\n        ChatMessageContentPart.CreateImageMessageContentPart(imageData, \"image/png\"))\n};\n\nClientResult<ChatCompletion> result = await client.CompleteChatAsync(messages);\nstring text = result.Value.Content[0].Text;\n\nConsole.WriteLine(text);\n```\n\n### Audio: Speech-to-text\n\nOpenAI 1.11.0:\n```cs\nOpenAIAPI api = new OpenAIAPI(\"<api-key>\");\nstring result = await api.Transcriptions.GetTextAsync(\"<file-path>\", \"fr\");\n\nConsole.WriteLine(result);\n```\n\nOpenAI 2.0.0-beta.1:\n```cs\nAudioClient client = new AudioClient(\"whisper-1\", \"<api-key>\");\nAudioTranscriptionOptions options = new AudioTranscriptionOptions()\n{\n    Language = \"fr\"\n};\n\nClientResult<AudioTranscription> result = await client.TranscribeAudioAsync(\"<file-path>\", options);\nstring text = result.Value.Text;\n\nConsole.WriteLine(text);\n```\n\n### Audio: Text-to-speech\n\nOpenAI 1.11.0:\n```cs\nOpenAIAPI api = new OpenAIAPI(\"<api-key>\");\nTextToSpeechRequest request = new TextToSpeechRequest()\n{\n    Input = \"Hasta la vista, baby.\",\n    Model = Model.TTS_Speed,\n    Voice = \"alloy\"\n};\n\nawait api.TextToSpeech.SaveSpeechToFileAsync(request, \"<file-path>\");\n```\n\nOpenAI 2.0.0-beta.1:\n```cs\nAudioClient client = new AudioClient(\"tts-1\", \"<api-key>\");\n\nClientResult<BinaryData> result = await client.GenerateSpeechFromTextAsync(\"Hasta la vista, baby.\", GeneratedSpeechVoice.Alloy);\nBinaryData data = result.Value;\n\nawait File.WriteAllBytesAsync(\"<file-path>\", data.ToArray());\n```\n\n### Image: Image generation\n\nOpenAI 1.11.0:\n```cs\nOpenAIAPI api = new OpenAIAPI(\"<api-key>\");\nImageGenerationRequest request = new ImageGenerationRequest()\n{\n    Prompt = \"Draw a quick brown fox jumping over a lazy dog.\",\n    Model = Model.DALLE3,\n    Quality = \"standard\",\n    Size = ImageSize._1024\n};\n\nImageResult result = await api.ImageGenerations.CreateImageAsync(request);\n\nConsole.WriteLine(result.Data[0].Url);\n```\n\nOpenAI 2.0.0-beta.1:\n```cs\nImageClient client = new ImageClient(\"dall-e-3\", \"<api-key>\");\nImageGenerationOptions options = new ImageGenerationOptions()\n{\n    Quality = GeneratedImageQuality.Standard,\n    Size = GeneratedImageSize.W1024xH1024\n};\n\nClientResult<GeneratedImage> result = await client.GenerateImageAsync(\"Draw a quick brown fox jumping over a lazy dog.\", options);\nUri imageUri = result.Value.ImageUri;\n\nConsole.WriteLine(imageUri.AbsoluteUri);\n```\n\n## Additional examples\n\nFor additional examples, see [OpenAI Examples][examples].\n\n[readme]: https://github.com/openai/openai-dotnet/blob/main/README.md\n[changelog]: https://github.com/openai/openai-dotnet/blob/main/CHANGELOG.md\n[examples]: https://github.com/openai/openai-dotnet/tree/main/examples\n[openai_1110]: https://aka.ms/openai1110\n"
        },
        {
          "name": "OpenAI.sln",
          "type": "blob",
          "size": 2.005859375,
          "content": "Microsoft Visual Studio Solution File, Format Version 12.00\n# Visual Studio Version 17\nVisualStudioVersion = 17.9.34902.65\nMinimumVisualStudioVersion = 10.0.40219.1\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"OpenAI\", \"src\\OpenAI.csproj\", \"{28FF4005-4467-4E36-92E7-DEA27DEB1519}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"OpenAI.Examples\", \"examples\\OpenAI.Examples.csproj\", \"{1F1CD1D4-9932-4B73-99D8-C252A67D4B46}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"OpenAI.Tests\", \"tests\\OpenAI.Tests.csproj\", \"{6F156401-2544-41D7-B204-3148C51C1D09}\"\nEndProject\nGlobal\n\tGlobalSection(SolutionConfigurationPlatforms) = preSolution\n\t\tDebug|Any CPU = Debug|Any CPU\n\t\tRelease|Any CPU = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(ProjectConfigurationPlatforms) = postSolution\n\t\t{28FF4005-4467-4E36-92E7-DEA27DEB1519}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{28FF4005-4467-4E36-92E7-DEA27DEB1519}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{28FF4005-4467-4E36-92E7-DEA27DEB1519}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{28FF4005-4467-4E36-92E7-DEA27DEB1519}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{1F1CD1D4-9932-4B73-99D8-C252A67D4B46}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{1F1CD1D4-9932-4B73-99D8-C252A67D4B46}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{1F1CD1D4-9932-4B73-99D8-C252A67D4B46}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{1F1CD1D4-9932-4B73-99D8-C252A67D4B46}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{6F156401-2544-41D7-B204-3148C51C1D09}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{6F156401-2544-41D7-B204-3148C51C1D09}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{6F156401-2544-41D7-B204-3148C51C1D09}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{6F156401-2544-41D7-B204-3148C51C1D09}.Release|Any CPU.Build.0 = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(SolutionProperties) = preSolution\n\t\tHideSolutionNode = FALSE\n\tEndGlobalSection\n\tGlobalSection(ExtensibilityGlobals) = postSolution\n\t\tSolutionGuid = {A97F4B90-2591-4689-B1F8-5F21FE6D6CAE}\n\tEndGlobalSection\nEndGlobal\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 39.595703125,
          "content": "# OpenAI .NET API library\n\n[![NuGet stable version](https://img.shields.io/nuget/v/openai.svg)](https://www.nuget.org/packages/OpenAI) [![NuGet preview version](https://img.shields.io/nuget/vpre/openai.svg)](https://www.nuget.org/packages/OpenAI/absoluteLatest)\n\nThe OpenAI .NET library provides convenient access to the OpenAI REST API from .NET applications.\n\nIt is generated from our [OpenAPI specification](https://github.com/openai/openai-openapi) in collaboration with Microsoft.\n\n## Table of Contents\n\n- [Getting started](#getting-started)\n  - [Prerequisites](#prerequisites)\n  - [Install the NuGet package](#install-the-nuget-package)\n- [Using the client library](#using-the-client-library)\n  - [Namespace organization](#namespace-organization)\n  - [Using the async API](#using-the-async-api)\n  - [Using the `OpenAIClient` class](#using-the-openaiclient-class)\n- [How to use chat completions with streaming](#how-to-use-chat-completions-with-streaming)\n- [How to use chat completions with tools and function calling](#how-to-use-chat-completions-with-tools-and-function-calling)\n- [How to use chat completions with structured outputs](#how-to-use-chat-completions-with-structured-outputs)\n- [How to generate text embeddings](#how-to-generate-text-embeddings)\n- [How to generate images](#how-to-generate-images)\n- [How to transcribe audio](#how-to-transcribe-audio)\n- [How to use assistants with retrieval augmented generation (RAG)](#how-to-use-assistants-with-retrieval-augmented-generation-rag)\n- [How to use assistants with streaming and vision](#how-to-use-assistants-with-streaming-and-vision)\n- [How to work with Azure OpenAI](#how-to-work-with-azure-openai)\n- [Advanced scenarios](#advanced-scenarios)\n  - [Using protocol methods](#using-protocol-methods)\n  - [Mock a client for testing](#mock-a-client-for-testing)\n  - [Automatically retrying errors](#automatically-retrying-errors)\n  - [Observability](#observability)\n\n## Getting started\n\n### Prerequisites\n\nTo call the OpenAI REST API, you will need an API key. To obtain one, first [create a new OpenAI account](https://platform.openai.com/signup) or [log in](https://platform.openai.com/login). Next, navigate to the [API key page](https://platform.openai.com/account/api-keys) and select \"Create new secret key\", optionally naming the key. Make sure to save your API key somewhere safe and do not share it with anyone.\n\n### Install the NuGet package\n\nAdd the client library to your .NET project by installing the [NuGet](https://www.nuget.org/) package via your IDE or by running the following command in the .NET CLI:\n\n```cli\ndotnet add package OpenAI\n```\n\nIf you would like to try the latest preview version, remember to append the `--prerelease` command option.\n\nNote that the code examples included below were written using [.NET 8](https://dotnet.microsoft.com/download/dotnet/8.0). The OpenAI .NET library is compatible with all .NET Standard 2.0 applications, but the syntax used in some of the code examples in this document may depend on newer language features.\n\n## Using the client library\n\nThe full API of this library can be found in the [OpenAI.netstandard2.0.cs](https://github.com/openai/openai-dotnet/blob/main/api/OpenAI.netstandard2.0.cs) file, and there are many [code examples](https://github.com/openai/openai-dotnet/tree/main/examples) to help. For instance, the following snippet illustrates the basic use of the chat completions API:\n\n```csharp\nusing OpenAI.Chat;\n\nChatClient client = new(model: \"gpt-4o\", apiKey: Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n\nChatCompletion completion = client.CompleteChat(\"Say 'this is a test.'\");\n\nConsole.WriteLine($\"[ASSISTANT]: {completion.Content[0].Text}\");\n```\n\nWhile you can pass your API key directly as a string, it is highly recommended that you keep it in a secure location and instead access it via an environment variable or configuration file as shown above to avoid storing it in source control.\n\n### Namespace organization\n\nThe library is organized into namespaces by feature areas in the OpenAI REST API. Each namespace contains a corresponding client class.\n\n| Namespace                     | Client class                 | Notes                                                             |\n| ------------------------------|------------------------------|-------------------------------------------------------------------|\n| `OpenAI.Assistants`           | `AssistantClient`            | ![Experimental](https://img.shields.io/badge/experimental-purple) |\n| `OpenAI.Audio`                | `AudioClient`                |                                                                   |\n| `OpenAI.Batch`                | `BatchClient`                | ![Experimental](https://img.shields.io/badge/experimental-purple) |\n| `OpenAI.Chat`                 | `ChatClient`                 |                                                                   |\n| `OpenAI.Embeddings`           | `EmbeddingClient`            |                                                                   |\n| `OpenAI.FineTuning`           | `FineTuningClient`           | ![Experimental](https://img.shields.io/badge/experimental-purple) |\n| `OpenAI.Files`                | `OpenAIFileClient`           |                                                                   |\n| `OpenAI.Images`               | `ImageClient`                |                                                                   |\n| `OpenAI.Models`               | `OpenAIModelClient`          |                                                                   |\n| `OpenAI.Moderations`          | `ModerationClient`           |                                                                   |\n| `OpenAI.VectorStores`         | `VectorStoreClient`          | ![Experimental](https://img.shields.io/badge/experimental-purple) |\n\n### Using the async API\n\nEvery client method that performs a synchronous API call has an asynchronous variant in the same client class. For instance, the asynchronous variant of the `ChatClient`'s `CompleteChat` method is `CompleteChatAsync`. To rewrite the call above using the asynchronous counterpart, simply `await` the call to the corresponding async variant:\n\n```csharp\nChatCompletion completion = await client.CompleteChatAsync(\"Say 'this is a test.'\");\n```\n\n### Using the `OpenAIClient` class\n\nIn addition to the namespaces mentioned above, there is also the parent `OpenAI` namespace itself:\n\n```csharp\nusing OpenAI;\n```\n\nThis namespace contains the `OpenAIClient` class, which offers certain conveniences when you need to work with multiple feature area clients. Specifically, you can use an instance of this class to create instances of the other clients and have them share the same implementation details, which might be more efficient.\n\nYou can create an `OpenAIClient` by specifying the API key that all clients will use for authentication:\n\n```csharp\nOpenAIClient client = new(Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n```\n\nNext, to create an instance of an `AudioClient`, for example, you can call the `OpenAIClient`'s `GetAudioClient` method by passing the OpenAI model that the `AudioClient` will use, just as if you were using the `AudioClient` constructor directly. If necessary, you can create additional clients of the same type to target different models.\n\n```csharp\nAudioClient ttsClient = client.GetAudioClient(\"tts-1\");\nAudioClient whisperClient = client.GetAudioClient(\"whisper-1\");\n```\n\n## How to use chat completions with streaming\n\nWhen you request a chat completion, the default behavior is for the server to generate it in its entirety before sending it back in a single response. Consequently, long chat completions can require waiting for several seconds before hearing back from the server. To mitigate this, the OpenAI REST API supports the ability to stream partial results back as they are being generated, allowing you to start processing the beginning of the completion before it is finished.\n\nThe client library offers a convenient approach to working with streaming chat completions. If you wanted to re-write the example from the previous section using streaming, rather than calling the `ChatClient`'s `CompleteChat` method, you would call its `CompleteChatStreaming` method instead:\n\n```csharp\nCollectionResult<StreamingChatCompletionUpdate> completionUpdates = client.CompleteChatStreaming(\"Say 'this is a test.'\");\n```\n\nNotice that the returned value is a `CollectionResult<StreamingChatCompletionUpdate>` instance, which can be enumerated to process the streaming response chunks as they arrive:\n\n```csharp\nConsole.Write($\"[ASSISTANT]: \");\nforeach (StreamingChatCompletionUpdate completionUpdate in completionUpdates)\n{\n    if (completionUpdate.ContentUpdate.Count > 0)\n    {\n        Console.Write(completionUpdate.ContentUpdate[0].Text);\n    }\n}\n```\n\nAlternatively, you can do this asynchronously by calling the `CompleteChatStreamingAsync` method to get an `AsyncCollectionResult<StreamingChatCompletionUpdate>` and enumerate it using `await foreach`:\n\n```csharp\nAsyncCollectionResult<StreamingChatCompletionUpdate> completionUpdates = client.CompleteChatStreamingAsync(\"Say 'this is a test.'\");\n\nConsole.Write($\"[ASSISTANT]: \");\nawait foreach (StreamingChatCompletionUpdate completionUpdate in completionUpdates)\n{\n    if (completionUpdate.ContentUpdate.Count > 0)\n    {\n        Console.Write(completionUpdate.ContentUpdate[0].Text);\n    }\n}\n```\n\n## How to use chat completions with tools and function calling\n\nIn this example, you have two functions. The first function can retrieve a user's current geographic location (e.g., by polling the location service APIs of the user's device), while the second function can query the weather in a given location (e.g., by making an API call to some third-party weather service). You want the model to be able to call these functions if it deems it necessary to have this information in order to respond to a user request as part of generating a chat completion. For illustrative purposes, consider the following:\n\n```csharp\nprivate static string GetCurrentLocation()\n{\n    // Call the location API here.\n    return \"San Francisco\";\n}\n\nprivate static string GetCurrentWeather(string location, string unit = \"celsius\")\n{\n    // Call the weather API here.\n    return $\"31 {unit}\";\n}\n```\n\nStart by creating two `ChatTool` instances using the static `CreateFunctionTool` method to describe each function:\n\n```csharp\nprivate static readonly ChatTool getCurrentLocationTool = ChatTool.CreateFunctionTool(\n    functionName: nameof(GetCurrentLocation),\n    functionDescription: \"Get the user's current location\"\n);\n\nprivate static readonly ChatTool getCurrentWeatherTool = ChatTool.CreateFunctionTool(\n    functionName: nameof(GetCurrentWeather),\n    functionDescription: \"Get the current weather in a given location\",\n    functionParameters: BinaryData.FromBytes(\"\"\"\n        {\n            \"type\": \"object\",\n            \"properties\": {\n                \"location\": {\n                    \"type\": \"string\",\n                    \"description\": \"The city and state, e.g. Boston, MA\"\n                },\n                \"unit\": {\n                    \"type\": \"string\",\n                    \"enum\": [ \"celsius\", \"fahrenheit\" ],\n                    \"description\": \"The temperature unit to use. Infer this from the specified location.\"\n                }\n            },\n            \"required\": [ \"location\" ]\n        }\n        \"\"\"u8.ToArray())\n);\n```\n\nNext, create a `ChatCompletionOptions` instance and add both to its `Tools` property. You will pass the `ChatCompletionOptions` as an argument in your calls to the `ChatClient`'s `CompleteChat` method.\n\n```csharp\nList<ChatMessage> messages = \n[\n    new UserChatMessage(\"What's the weather like today?\"),\n];\n\nChatCompletionOptions options = new()\n{\n    Tools = { getCurrentLocationTool, getCurrentWeatherTool },\n};\n```\n\nWhen the resulting `ChatCompletion` has a `FinishReason` property equal to `ChatFinishReason.ToolCalls`, it means that the model has determined that one or more tools must be called before the assistant can respond appropriately. In those cases, you must first call the function specified in the `ChatCompletion`'s `ToolCalls` and then call the `ChatClient`'s `CompleteChat` method again while passing the function's result as an additional `ChatRequestToolMessage`. Repeat this process as needed.\n\n```csharp\nbool requiresAction;\n\ndo\n{\n    requiresAction = false;\n    ChatCompletion completion = client.CompleteChat(messages, options);\n\n    switch (completion.FinishReason)\n    {\n        case ChatFinishReason.Stop:\n            {\n                // Add the assistant message to the conversation history.\n                messages.Add(new AssistantChatMessage(completion));\n                break;\n            }\n\n        case ChatFinishReason.ToolCalls:\n            {\n                // First, add the assistant message with tool calls to the conversation history.\n                messages.Add(new AssistantChatMessage(completion));\n\n                // Then, add a new tool message for each tool call that is resolved.\n                foreach (ChatToolCall toolCall in completion.ToolCalls)\n                {\n                    switch (toolCall.FunctionName)\n                    {\n                        case nameof(GetCurrentLocation):\n                            {\n                                string toolResult = GetCurrentLocation();\n                                messages.Add(new ToolChatMessage(toolCall.Id, toolResult));\n                                break;\n                            }\n\n                        case nameof(GetCurrentWeather):\n                            {\n                                // The arguments that the model wants to use to call the function are specified as a\n                                // stringified JSON object based on the schema defined in the tool definition. Note that\n                                // the model may hallucinate arguments too. Consequently, it is important to do the\n                                // appropriate parsing and validation before calling the function.\n                                using JsonDocument argumentsJson = JsonDocument.Parse(toolCall.FunctionArguments);\n                                bool hasLocation = argumentsJson.RootElement.TryGetProperty(\"location\", out JsonElement location);\n                                bool hasUnit = argumentsJson.RootElement.TryGetProperty(\"unit\", out JsonElement unit);\n\n                                if (!hasLocation)\n                                {\n                                    throw new ArgumentNullException(nameof(location), \"The location argument is required.\");\n                                }\n\n                                string toolResult = hasUnit\n                                    ? GetCurrentWeather(location.GetString(), unit.GetString())\n                                    : GetCurrentWeather(location.GetString());\n                                messages.Add(new ToolChatMessage(toolCall.Id, toolResult));\n                                break;\n                            }\n\n                        default:\n                            {\n                                // Handle other unexpected calls.\n                                throw new NotImplementedException();\n                            }\n                    }\n                }\n\n                requiresAction = true;\n                break;\n            }\n\n        case ChatFinishReason.Length:\n            throw new NotImplementedException(\"Incomplete model output due to MaxTokens parameter or token limit exceeded.\");\n\n        case ChatFinishReason.ContentFilter:\n            throw new NotImplementedException(\"Omitted content due to a content filter flag.\");\n\n        case ChatFinishReason.FunctionCall:\n            throw new NotImplementedException(\"Deprecated in favor of tool calls.\");\n\n        default:\n            throw new NotImplementedException(completion.FinishReason.ToString());\n    }\n} while (requiresAction);\n```\n\n## How to use chat completions with structured outputs\n\nBeginning with the `gpt-4o-mini`, `gpt-4o-mini-2024-07-18`, and `gpt-4o-2024-08-06` model snapshots, structured outputs are available for both top-level response content and tool calls in the chat completion and assistants APIs. For information about the feature, see [the Structured Outputs guide](https://platform.openai.com/docs/guides/structured-outputs/introduction).\n\nTo use structured outputs to constrain chat completion content, set an appropriate `ChatResponseFormat` as in the following example:\n\n```csharp\nList<ChatMessage> messages =\n[\n    new UserChatMessage(\"How can I solve 8x + 7 = -23?\"),\n];\n\nChatCompletionOptions options = new()\n{\n    ResponseFormat = ChatResponseFormat.CreateJsonSchemaFormat(\n        jsonSchemaFormatName: \"math_reasoning\",\n        jsonSchema: BinaryData.FromBytes(\"\"\"\n            {\n                \"type\": \"object\",\n                \"properties\": {\n                \"steps\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"explanation\": { \"type\": \"string\" },\n                        \"output\": { \"type\": \"string\" }\n                    },\n                    \"required\": [\"explanation\", \"output\"],\n                    \"additionalProperties\": false\n                    }\n                },\n                \"final_answer\": { \"type\": \"string\" }\n                },\n                \"required\": [\"steps\", \"final_answer\"],\n                \"additionalProperties\": false\n            }\n            \"\"\"u8.ToArray()),\n        jsonSchemaIsStrict: true)\n};\n\nChatCompletion completion = client.CompleteChat(messages, options);\n\nusing JsonDocument structuredJson = JsonDocument.Parse(completion.Content[0].Text);\n\nConsole.WriteLine($\"Final answer: {structuredJson.RootElement.GetProperty(\"final_answer\")}\");\nConsole.WriteLine(\"Reasoning steps:\");\n\nforeach (JsonElement stepElement in structuredJson.RootElement.GetProperty(\"steps\").EnumerateArray())\n{\n    Console.WriteLine($\"  - Explanation: {stepElement.GetProperty(\"explanation\")}\");\n    Console.WriteLine($\"    Output: {stepElement.GetProperty(\"output\")}\");\n}\n```\n\n## How to generate text embeddings\n\nIn this example, you want to create a trip-planning website that allows customers to write a prompt describing the kind of hotel that they are looking for and then offers hotel recommendations that closely match this description. To achieve this, it is possible to use text embeddings to measure the relatedness of text strings. In summary, you can get embeddings of the hotel descriptions, store them in a vector database, and use them to build a search index that you can query using the embedding of a given customer's prompt.\n\nTo generate a text embedding, use `EmbeddingClient` from the `OpenAI.Embeddings` namespace:\n\n```csharp\nusing OpenAI.Embeddings;\n\nEmbeddingClient client = new(\"text-embedding-3-small\", Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n\nstring description = \"Best hotel in town if you like luxury hotels. They have an amazing infinity pool, a spa,\"\n    + \" and a really helpful concierge. The location is perfect -- right downtown, close to all the tourist\"\n    + \" attractions. We highly recommend this hotel.\";\n\nOpenAIEmbedding embedding = client.GenerateEmbedding(description);\nReadOnlyMemory<float> vector = embedding.ToFloats();\n```\n\nNotice that the resulting embedding is a list (also called a vector) of floating point numbers represented as an instance of `ReadOnlyMemory<float>`. By default, the length of the embedding vector will be 1536 when using the `text-embedding-3-small` model or 3072 when using the `text-embedding-3-large` model. Generally, larger embeddings perform better, but using them also tends to cost more in terms of compute, memory, and storage. You can reduce the dimensions of the embedding by creating an instance of the `EmbeddingGenerationOptions` class, setting the `Dimensions` property, and passing it as an argument in your call to the `GenerateEmbedding` method:\n\n```csharp\nEmbeddingGenerationOptions options = new() { Dimensions = 512 };\n\nOpenAIEmbedding embedding = client.GenerateEmbedding(description, options);\n```\n\n## How to generate images\n\nIn this example, you want to build an app to help interior designers prototype new ideas based on the latest design trends. As part of the creative process, an interior designer can use this app to generate images for inspiration simply by describing the scene in their head as a prompt. As expected, high-quality, strikingly dramatic images with finer details deliver the best results for this application.\n\nTo generate an image, use `ImageClient` from the `OpenAI.Images` namespace:\n\n```csharp\nusing OpenAI.Images;\n\nImageClient client = new(\"dall-e-3\", Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n```\n\nGenerating an image always requires a `prompt` that describes what should be generated. To further tailor the image generation to your specific needs, you can create an instance of the `ImageGenerationOptions` class and set the `Quality`, `Size`, and `Style` properties accordingly. Note that you can also set the `ResponseFormat` property of `ImageGenerationOptions` to `GeneratedImageFormat.Bytes` in order to receive the resulting PNG as `BinaryData` (instead of the default remote `Uri`) if this is convenient for your use case.\n\n```csharp\nstring prompt = \"The concept for a living room that blends Scandinavian simplicity with Japanese minimalism for\"\n    + \" a serene and cozy atmosphere. It's a space that invites relaxation and mindfulness, with natural light\"\n    + \" and fresh air. Using neutral tones, including colors like white, beige, gray, and black, that create a\"\n    + \" sense of harmony. Featuring sleek wood furniture with clean lines and subtle curves to add warmth and\"\n    + \" elegance. Plants and flowers in ceramic pots adding color and life to a space. They can serve as focal\"\n    + \" points, creating a connection with nature. Soft textiles and cushions in organic fabrics adding comfort\"\n    + \" and softness to a space. They can serve as accents, adding contrast and texture.\";\n\nImageGenerationOptions options = new()\n{\n    Quality = GeneratedImageQuality.High,\n    Size = GeneratedImageSize.W1792xH1024,\n    Style = GeneratedImageStyle.Vivid,\n    ResponseFormat = GeneratedImageFormat.Bytes\n};\n```\n\nFinally, call the `ImageClient`'s `GenerateImage` method by passing the prompt and the `ImageGenerationOptions` instance as arguments:\n\n```csharp\nGeneratedImage image = client.GenerateImage(prompt, options);\nBinaryData bytes = image.ImageBytes;\n```\n\nFor illustrative purposes, you could then save the generated image to local storage:\n\n```csharp\nusing FileStream stream = File.OpenWrite($\"{Guid.NewGuid()}.png\");\nbytes.ToStream().CopyTo(stream);\n```\n\n## How to transcribe audio\n\nIn this example, an audio file is transcribed using the Whisper speech-to-text model, including both word- and audio-segment-level timestamp information.\n\n```csharp\nusing OpenAI.Audio;\n\nAudioClient client = new(\"whisper-1\", Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n\nstring audioFilePath = Path.Combine(\"Assets\", \"audio_houseplant_care.mp3\");\n\nAudioTranscriptionOptions options = new()\n{\n    ResponseFormat = AudioTranscriptionFormat.Verbose,\n    TimestampGranularities = AudioTimestampGranularities.Word | AudioTimestampGranularities.Segment,\n};\n\nAudioTranscription transcription = client.TranscribeAudio(audioFilePath, options);\n\nConsole.WriteLine(\"Transcription:\");\nConsole.WriteLine($\"{transcription.Text}\");\n\nConsole.WriteLine();\nConsole.WriteLine($\"Words:\");\nforeach (TranscribedWord word in transcription.Words)\n{\n    Console.WriteLine($\"  {word.Word,15} : {word.StartTime.TotalMilliseconds,5:0} - {word.EndTime.TotalMilliseconds,5:0}\");\n}\n\nConsole.WriteLine();\nConsole.WriteLine($\"Segments:\");\nforeach (TranscribedSegment segment in transcription.Segments)\n{\n    Console.WriteLine($\"  {segment.Text,90} : {segment.StartTime.TotalMilliseconds,5:0} - {segment.EndTime.TotalMilliseconds,5:0}\");\n}\n```\n\n## How to use assistants with retrieval augmented generation (RAG)\n\nIn this example, you have a JSON document with the monthly sales information of different products, and you want to build an assistant capable of analyzing it and answering questions about it.\n\nTo achieve this, use both `OpenAIFileClient` from the `OpenAI.Files` namespace and `AssistantClient` from the `OpenAI.Assistants` namespace.\n\nImportant: The Assistants REST API is currently in beta. As such, the details are subject to change, and correspondingly the `AssistantClient` is attributed as `[Experimental]`. To use it, you must suppress the `OPENAI001` warning first.\n\n```csharp\nusing OpenAI.Assistants;\nusing OpenAI.Files;\n\nOpenAIClient openAIClient = new(Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\nOpenAIFileClient fileClient = openAIClient.GetOpenAIFileClient();\nAssistantClient assistantClient = openAIClient.GetAssistantClient();\n```\n\nHere is an example of what the JSON document might look like:\n\n```csharp\nusing Stream document = BinaryData.FromBytes(\"\"\"\n    {\n        \"description\": \"This document contains the sale history data for Contoso products.\",\n        \"sales\": [\n            {\n                \"month\": \"January\",\n                \"by_product\": {\n                    \"113043\": 15,\n                    \"113045\": 12,\n                    \"113049\": 2\n                }\n            },\n            {\n                \"month\": \"February\",\n                \"by_product\": {\n                    \"113045\": 22\n                }\n            },\n            {\n                \"month\": \"March\",\n                \"by_product\": {\n                    \"113045\": 16,\n                    \"113055\": 5\n                }\n            }\n        ]\n    }\n    \"\"\"u8.ToArray()).ToStream();\n```\n\nUpload this document to OpenAI using the `OpenAIFileClient`'s `UploadFile` method, ensuring that you use `FileUploadPurpose.Assistants` to allow your assistant to access it later:\n\n```csharp\nOpenAIFile salesFile = fileClient.UploadFile(\n    document,\n    \"monthly_sales.json\",\n    FileUploadPurpose.Assistants);\n```\n\nCreate a new assistant using an instance of the `AssistantCreationOptions` class to customize it. Here, we use:\n\n- A friendly `Name` for the assistant, as will display in the Playground\n- Tool definition instances for the tools that the assistant should have access to; here, we use `FileSearchToolDefinition` to process the sales document we just uploaded and `CodeInterpreterToolDefinition` so we can analyze and visualize the numeric data\n- Resources for the assistant to use with its tools, here using the `VectorStoreCreationHelper` type to automatically make a new vector store that indexes the sales file; alternatively, you could use `VectorStoreClient` to manage the vector store separately\n\n```csharp\nAssistantCreationOptions assistantOptions = new()\n{\n    Name = \"Example: Contoso sales RAG\",\n    Instructions =\n        \"You are an assistant that looks up sales data and helps visualize the information based\"\n        + \" on user queries. When asked to generate a graph, chart, or other visualization, use\"\n        + \" the code interpreter tool to do so.\",\n    Tools =\n    {\n        new FileSearchToolDefinition(),\n        new CodeInterpreterToolDefinition(),\n    },\n    ToolResources = new()\n    {\n        FileSearch = new()\n        {\n            NewVectorStores =\n            {\n                new VectorStoreCreationHelper([salesFile.Id]),\n            }\n        }\n    },\n};\n\nAssistant assistant = assistantClient.CreateAssistant(\"gpt-4o\", assistantOptions);\n```\n\nNext, create a new thread. For illustrative purposes, you could include an initial user message asking about the sales information of a given product and then use the `AssistantClient`'s `CreateThreadAndRun` method to get it started:\n\n```csharp\nThreadCreationOptions threadOptions = new()\n{\n    InitialMessages = { \"How well did product 113045 sell in February? Graph its trend over time.\" }\n};\n\nThreadRun threadRun = assistantClient.CreateThreadAndRun(assistant.Id, threadOptions);\n```\n\nPoll the status of the run until it is no longer queued or in progress:\n\n```csharp\ndo\n{\n    Thread.Sleep(TimeSpan.FromSeconds(1));\n    threadRun = assistantClient.GetRun(threadRun.ThreadId, threadRun.Id);\n} while (!threadRun.Status.IsTerminal);\n```\n\nIf everything went well, the terminal status of the run will be `RunStatus.Completed`.\n\nFinally, you can use the `AssistantClient`'s `GetMessages` method to retrieve the messages associated with this thread, which now include the responses from the assistant to the initial user message.\n\nFor illustrative purposes, you could print the messages to the console and also save any images produced by the assistant to local storage:\n\n```csharp\nCollectionResult<ThreadMessage> messages\n    = assistantClient.GetMessages(threadRun.ThreadId, new MessageCollectionOptions() { Order = MessageCollectionOrder.Ascending });\n\nforeach (ThreadMessage message in messages)\n{\n    Console.Write($\"[{message.Role.ToString().ToUpper()}]: \");\n    foreach (MessageContent contentItem in message.Content)\n    {\n        if (!string.IsNullOrEmpty(contentItem.Text))\n        {\n            Console.WriteLine($\"{contentItem.Text}\");\n\n            if (contentItem.TextAnnotations.Count > 0)\n            {\n                Console.WriteLine();\n            }\n\n            // Include annotations, if any.\n            foreach (TextAnnotation annotation in contentItem.TextAnnotations)\n            {\n                if (!string.IsNullOrEmpty(annotation.InputFileId))\n                {\n                    Console.WriteLine($\"* File citation, file ID: {annotation.InputFileId}\");\n                }\n                if (!string.IsNullOrEmpty(annotation.OutputFileId))\n                {\n                    Console.WriteLine($\"* File output, new file ID: {annotation.OutputFileId}\");\n                }\n            }\n        }\n        if (!string.IsNullOrEmpty(contentItem.ImageFileId))\n        {\n            OpenAIFile imageInfo = fileClient.GetFile(contentItem.ImageFileId);\n            BinaryData imageBytes = fileClient.DownloadFile(contentItem.ImageFileId);\n            using FileStream stream = File.OpenWrite($\"{imageInfo.Filename}.png\");\n            imageBytes.ToStream().CopyTo(stream);\n\n            Console.WriteLine($\"<image: {imageInfo.Filename}.png>\");\n        }\n    }\n    Console.WriteLine();\n}\n```\n\nAnd it would yield something like this:\n\n```text\n[USER]: How well did product 113045 sell in February? Graph its trend over time.\n\n[ASSISTANT]: Product 113045 sold 22 units in February4:0monthly_sales.json.\n\nNow, I will generate a graph to show its sales trend over time.\n\n* File citation, file ID: file-hGOiwGNftMgOsjbynBpMCPFn\n\n[ASSISTANT]: <image: 015d8e43-17fe-47de-af40-280f25452280.png>\nThe sales trend for Product 113045 over the past three months shows that:\n\n- In January, 12 units were sold.\n- In February, 22 units were sold, indicating significant growth.\n- In March, sales dropped slightly to 16 units.\n\nThe graph above visualizes this trend, showing a peak in sales during February.\n```\n\n## How to use assistants with streaming and vision\n\nThis example shows how to use the v2 Assistants API to provide image data to an assistant and then stream the run's response.\n\nAs before, you will use a `OpenAIFileClient` and an `AssistantClient`:\n\n```csharp\nOpenAIClient openAIClient = new(Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\nOpenAIFileClient fileClient = openAIClient.GetOpenAIFileClient();\nAssistantClient assistantClient = openAIClient.GetAssistantClient();\n```\n\nFor this example, we will use both image data from a local file as well as an image located at a URL. For the local data, we upload the file with the `Vision` upload purpose, which would also allow it to be downloaded and retrieved later.\n\n```csharp\nOpenAIFile pictureOfAppleFile = fileClient.UploadFile(\n    Path.Combine(\"Assets\", \"images_apple.png\"),\n    FileUploadPurpose.Vision);\n\nUri linkToPictureOfOrange = new(\"https://raw.githubusercontent.com/openai/openai-dotnet/refs/heads/main/examples/Assets/images_orange.png\");\n```\n\nNext, create a new assistant with a vision-capable model like `gpt-4o` and a thread with the image information referenced:\n\n```csharp\nAssistant assistant = assistantClient.CreateAssistant(\n    \"gpt-4o\",\n    new AssistantCreationOptions()\n    {\n        Instructions = \"When asked a question, attempt to answer very concisely. \"\n            + \"Prefer one-sentence answers whenever feasible.\"\n    });\n\nAssistantThread thread = assistantClient.CreateThread(new ThreadCreationOptions()\n{\n    InitialMessages =\n        {\n            new ThreadInitializationMessage(\n                MessageRole.User,\n                [\n                    \"Hello, assistant! Please compare these two images for me:\",\n                    MessageContent.FromImageFileId(pictureOfAppleFile.Id),\n                    MessageContent.FromImageUri(linkToPictureOfOrange),\n                ]),\n        }\n});\n```\n\nWith the assistant and thread prepared, use the `CreateRunStreaming` method to get an enumerable `CollectionResult<StreamingUpdate>`. You can then iterate over this collection with `foreach`. For async calling patterns, use `CreateRunStreamingAsync` and iterate over the `AsyncCollectionResult<StreamingUpdate>` with `await foreach`, instead. Note that streaming variants also exist for `CreateThreadAndRunStreaming` and `SubmitToolOutputsToRunStreaming`.\n\n```csharp\nCollectionResult<StreamingUpdate> streamingUpdates = assistantClient.CreateRunStreaming(\n    thread.Id,\n    assistant.Id,\n    new RunCreationOptions()\n    {\n        AdditionalInstructions = \"When possible, try to sneak in puns if you're asked to compare things.\",\n    });\n```\n\nFinally, to handle the `StreamingUpdates` as they arrive, you can use the `UpdateKind` property on the base `StreamingUpdate` and/or downcast to a specifically desired update type, like `MessageContentUpdate` for `thread.message.delta` events or `RequiredActionUpdate` for streaming tool calls.\n\n```csharp\nforeach (StreamingUpdate streamingUpdate in streamingUpdates)\n{\n    if (streamingUpdate.UpdateKind == StreamingUpdateReason.RunCreated)\n    {\n        Console.WriteLine($\"--- Run started! ---\");\n    }\n    if (streamingUpdate is MessageContentUpdate contentUpdate)\n    {\n        Console.Write(contentUpdate.Text);\n    }\n}\n```\n\nThis will yield streamed output from the run like the following:\n\n```text\n--- Run started! ---\nThe first image depicts a multicolored apple with a blend of red and green hues, while the second image shows an orange with a bright, textured orange peel; one might say its comparing apples to oranges!\n```\n\n## How to work with Azure OpenAI\n\nFor Azure OpenAI scenarios use the [Azure SDK](https://github.com/Azure/azure-sdk-for-net) and more specifically the [Azure OpenAI client library for .NET](https://github.com/Azure/azure-sdk-for-net/blob/main/sdk/openai/Azure.AI.OpenAI/README.md). \n\nThe Azure OpenAI client library for .NET is a companion to this library and all common capabilities between OpenAI and Azure OpenAI share the same scenario clients, methods, and request/response types. It is designed to make Azure specific scenarios straightforward, with extensions for Azure-specific concepts like Responsible AI content filter results and On Your Data integration.\n\n```c#\nAzureOpenAIClient azureClient = new(\n    new Uri(\"https://your-azure-openai-resource.com\"),\n    new DefaultAzureCredential());\nChatClient chatClient = azureClient.GetChatClient(\"my-gpt-35-turbo-deployment\");\n\nChatCompletion completion = chatClient.CompleteChat(\n    [\n        // System messages represent instructions or other guidance about how the assistant should behave\n        new SystemChatMessage(\"You are a helpful assistant that talks like a pirate.\"),\n        // User messages represent user input, whether historical or the most recen tinput\n        new UserChatMessage(\"Hi, can you help me?\"),\n        // Assistant messages in a request represent conversation history for responses\n        new AssistantChatMessage(\"Arrr! Of course, me hearty! What can I do for ye?\"),\n        new UserChatMessage(\"What's the best way to train a parrot?\"),\n    ]);\n\nConsole.WriteLine($\"{completion.Role}: {completion.Content[0].Text}\");\n```\n\n## Advanced scenarios\n\n### Using protocol methods\n\nIn addition to the client methods that use strongly-typed request and response objects, the .NET library also provides _protocol methods_ that enable more direct access to the REST API. Protocol methods are \"binary in, binary out\" accepting `BinaryContent` as request bodies and providing `BinaryData` as response bodies.\n\nFor example, to use the protocol method variant of the `ChatClient`'s `CompleteChat` method, pass the request body as `BinaryContent`:\n\n```csharp\nChatClient client = new(\"gpt-4o\", Environment.GetEnvironmentVariable(\"OPENAI_API_KEY\"));\n\nBinaryData input = BinaryData.FromBytes(\"\"\"\n    {\n       \"model\": \"gpt-4o\",\n       \"messages\": [\n           {\n               \"role\": \"user\",\n               \"content\": \"Say 'this is a test.'\"\n           }\n       ]\n    }\n    \"\"\"u8.ToArray());\n\nusing BinaryContent content = BinaryContent.Create(input);\nClientResult result = client.CompleteChat(content);\nBinaryData output = result.GetRawResponse().Content;\n\nusing JsonDocument outputAsJson = JsonDocument.Parse(output.ToString());\nstring message = outputAsJson.RootElement\n    .GetProperty(\"choices\"u8)[0]\n    .GetProperty(\"message\"u8)\n    .GetProperty(\"content\"u8)\n    .GetString();\n\nConsole.WriteLine($\"[ASSISTANT]: {message}\");\n```\n\nNotice how you can then call the resulting `ClientResult`'s `GetRawResponse` method and retrieve the response body as `BinaryData` via the `PipelineResponse`'s `Content` property.\n\n### Mock a client for testing\n\nThe OpenAI .NET library has been designed to support mocking, providing key features such as:\n\n- Client methods made virtual to allow overriding.\n- Model factories to assist in instantiating API output models that lack public constructors.\n\nTo illustrate how mocking works, suppose you want to validate the behavior of the following method using the [Moq](https://github.com/devlooped/moq) library. Given the path to an audio file, it determines whether it contains a specified secret word:\n\n```csharp\npublic bool ContainsSecretWord(AudioClient client, string audioFilePath, string secretWord)\n{\n    AudioTranscription transcription = client.TranscribeAudio(audioFilePath);\n    return transcription.Text.Contains(secretWord);\n}\n```\n\nCreate mocks of `AudioClient` and `ClientResult<AudioTranscription>`, set up methods and properties that will be invoked, then test the behavior of the `ContainsSecretWord` method. Since the `AudioTranscription` class does not provide public constructors, it must be instantiated by the `OpenAIAudioModelFactory` static class:\n\n```csharp\n// Instantiate mocks and the AudioTranscription object.\n\nMock<AudioClient> mockClient = new();\nMock<ClientResult<AudioTranscription>> mockResult = new(null, Mock.Of<PipelineResponse>());\nAudioTranscription transcription = OpenAIAudioModelFactory.AudioTranscription(text: \"I swear I saw an apple flying yesterday!\");\n\n// Set up mocks' properties and methods.\n\nmockResult\n    .SetupGet(result => result.Value)\n    .Returns(transcription);\n\nmockClient.Setup(client => client.TranscribeAudio(\n        It.IsAny<string>(),\n        It.IsAny<AudioTranscriptionOptions>()))\n    .Returns(mockResult.Object);\n\n// Perform validation.\n\nAudioClient client = mockClient.Object;\nbool containsSecretWord = ContainsSecretWord(client, \"<audioFilePath>\", \"apple\");\n\nAssert.That(containsSecretWord, Is.True);\n```\n\nAll namespaces have their corresponding model factory to support mocking with the exception of the `OpenAI.Assistants` and `OpenAI.VectorStores` namespaces, for which model factories are coming soon.\n\n### Automatically retrying errors\n\nBy default, the client classes will automatically retry the following errors up to three additional times using exponential backoff:\n\n- 408 Request Timeout\n- 429 Too Many Requests\n- 500 Internal Server Error\n- 502 Bad Gateway\n- 503 Service Unavailable\n- 504 Gateway Timeout\n\n### Observability\n\nOpenAI .NET library supports experimental distributed tracing and metrics with OpenTelemetry. Check out [Observability with OpenTelemetry](./docs/observability.md) for more details."
        },
        {
          "name": "api",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "global.json",
          "type": "blob",
          "size": 0.0712890625,
          "content": "{\n  \"sdk\": {\n    \"version\": \"8.0.100\",\n    \"rollForward\": \"feature\"\n  }\n}"
        },
        {
          "name": "nuget.config",
          "type": "blob",
          "size": 0.2763671875,
          "content": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<configuration>\n  <packageSources>\n    <!--To inherit the global NuGet package sources remove the <clear/> line below -->\n    <clear />\n    <add key=\"nuget\" value=\"https://api.nuget.org/v3/index.json\" />\n  </packageSources>\n</configuration>\n"
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}