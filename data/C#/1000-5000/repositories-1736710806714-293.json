{
  "metadata": {
    "timestamp": 1736710806714,
    "page": 293,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjMwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "SciSharp/LLamaSharp",
      "stars": 2820,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 4.80078125,
          "content": "\n[*]\ncharset = utf-8\nend_of_line = lf\ntrim_trailing_whitespace = false\ninsert_final_newline = false\nindent_style = space\nindent_size = 4\n\n# Microsoft .NET properties\ncsharp_new_line_before_members_in_object_initializers = false\ncsharp_preferred_modifier_order = public, private, protected, internal, file, new, static, abstract, virtual, sealed, readonly, override, extern, unsafe, volatile, async, required:suggestion\ncsharp_style_prefer_utf8_string_literals = true:suggestion\ncsharp_style_var_elsewhere = true:suggestion\ncsharp_style_var_for_built_in_types = true:suggestion\ncsharp_style_var_when_type_is_apparent = true:suggestion\ndotnet_naming_rule.enum_member_rule.import_to_resharper = True\ndotnet_naming_rule.enum_member_rule.resharper_description = Enum members\ndotnet_naming_rule.enum_member_rule.resharper_guid = 8b8504e3-f0be-4c14-9103-c732f2bddc15\ndotnet_naming_rule.enum_member_rule.resharper_style = AA_BB, AaBb\ndotnet_naming_rule.enum_member_rule.severity = warning\ndotnet_naming_rule.enum_member_rule.style = all_upper_style\ndotnet_naming_rule.enum_member_rule.symbols = enum_member_symbols\ndotnet_naming_rule.unity_serialized_field_rule.import_to_resharper = True\ndotnet_naming_rule.unity_serialized_field_rule.resharper_description = Unity serialized field\ndotnet_naming_rule.unity_serialized_field_rule.resharper_guid = 5f0fdb63-c892-4d2c-9324-15c80b22a7ef\ndotnet_naming_rule.unity_serialized_field_rule.severity = warning\ndotnet_naming_rule.unity_serialized_field_rule.style = lower_camel_case_style\ndotnet_naming_rule.unity_serialized_field_rule.symbols = unity_serialized_field_symbols\ndotnet_naming_style.all_upper_style.capitalization = all_upper\ndotnet_naming_style.all_upper_style.word_separator = _\ndotnet_naming_style.lower_camel_case_style.capitalization = camel_case\ndotnet_naming_symbols.enum_member_symbols.applicable_accessibilities = *\ndotnet_naming_symbols.enum_member_symbols.applicable_kinds = \ndotnet_naming_symbols.enum_member_symbols.resharper_applicable_kinds = enum_member\ndotnet_naming_symbols.enum_member_symbols.resharper_required_modifiers = any\ndotnet_naming_symbols.unity_serialized_field_symbols.applicable_accessibilities = *\ndotnet_naming_symbols.unity_serialized_field_symbols.applicable_kinds = \ndotnet_naming_symbols.unity_serialized_field_symbols.resharper_applicable_kinds = unity_serialised_field\ndotnet_naming_symbols.unity_serialized_field_symbols.resharper_required_modifiers = instance\ndotnet_style_parentheses_in_arithmetic_binary_operators = never_if_unnecessary:none\ndotnet_style_parentheses_in_other_binary_operators = always_for_clarity:none\ndotnet_style_parentheses_in_relational_binary_operators = never_if_unnecessary:none\ndotnet_style_predefined_type_for_locals_parameters_members = true:suggestion\ndotnet_style_predefined_type_for_member_access = true:suggestion\ndotnet_style_qualification_for_event = false:suggestion\ndotnet_style_qualification_for_field = false:suggestion\ndotnet_style_qualification_for_method = false:suggestion\ndotnet_style_qualification_for_property = false:suggestion\ndotnet_style_require_accessibility_modifiers = for_non_interface_members:suggestion\n\n# ReSharper properties\nresharper_autodetect_indent_settings = true\nresharper_formatter_off_tag = @formatter:off\nresharper_formatter_on_tag = @formatter:on\nresharper_formatter_tags_enabled = true\nresharper_use_indent_from_vs = false\n\n# ReSharper inspection severities\nresharper_arrange_redundant_parentheses_highlighting = hint\nresharper_arrange_this_qualifier_highlighting = hint\nresharper_arrange_type_member_modifiers_highlighting = hint\nresharper_arrange_type_modifiers_highlighting = hint\nresharper_built_in_type_reference_style_for_member_access_highlighting = hint\nresharper_built_in_type_reference_style_highlighting = hint\nresharper_razor_assembly_not_resolved_highlighting = warning\nresharper_redundant_base_qualifier_highlighting = warning\nresharper_suggest_var_or_type_built_in_types_highlighting = hint\nresharper_suggest_var_or_type_elsewhere_highlighting = hint\nresharper_suggest_var_or_type_simple_types_highlighting = hint\nresharper_web_config_module_not_resolved_highlighting = warning\nresharper_web_config_type_not_resolved_highlighting = warning\nresharper_web_config_wrong_module_highlighting = warning\n\n[{*.har,*.jsb2,*.jsb3,*.json,*.jsonc,*.postman_collection,*.postman_collection.json,*.postman_environment,*.postman_environment.json,.babelrc,.eslintrc,.prettierrc,.stylelintrc,bowerrc,jest.config}]\nindent_style = space\nindent_size = 2\n\n[*.map]\nindent_style = space\nindent_size = 2\n\n[*.{appxmanifest,asax,ascx,aspx,axaml,build,c,c++,c++m,cc,ccm,cginc,compute,cp,cpp,cppm,cs,cshtml,cu,cuh,cxx,cxxm,dtd,fs,fsi,fsscript,fsx,fx,fxh,h,hh,hlsl,hlsli,hlslinc,hpp,hxx,inc,inl,ino,ipp,ixx,master,ml,mli,mpp,mq4,mq5,mqh,mxx,nuspec,paml,razor,resw,resx,shader,skin,tpp,usf,ush,uxml,vb,xaml,xamlx,xoml,xsd}]\nindent_style = space\nindent_size = 4\ntab_width = 4\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 5.8544921875,
          "content": "## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n##\n## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore\n\n# User-specific files\n*.suo\n*.user\n*.userosscache\n*.sln.docstates\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n*.userprefs\n\n# Build results\n[Dd]ebug/\n[Dd]ebugPublic/\n[Rr]elease/\n[Rr]eleases/\nx64/\nx86/\nbld/\n[Bb]in/\n[Oo]bj/\n[Ll]og/\n\n# Visual Studio 2015/2017 cache/options directory\n.vs/\n# Uncomment if you have tasks that create the project's static files in wwwroot\n#wwwroot/\n\n# Visual Studio 2017 auto generated files\nGenerated\\ Files/\n\n# MSTest test Results\n[Tt]est[Rr]esult*/\n[Bb]uild[Ll]og.*\n\n# NUNIT\n*.VisualState.xml\nTestResult.xml\n\n# Build Results of an ATL Project\n[Dd]ebugPS/\n[Rr]eleasePS/\ndlldata.c\n\n# Benchmark Results\nBenchmarkDotNet.Artifacts/\n\n# .NET Core\nproject.lock.json\nproject.fragment.lock.json\nartifacts/\n**/Properties/launchSettings.json\n\n# StyleCop\nStyleCopReport.xml\n\n# Files built by Visual Studio\n*_i.c\n*_p.c\n*_i.h\n*.ilk\n*.obj\n*.iobj\n*.pch\n*.pdb\n*.ipdb\n*.pgc\n*.pgd\n*.rsp\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp\n*.tmp_proj\n*.log\n*.vspscc\n*.vssscc\n.builds\n*.pidb\n*.svclog\n*.scc\n\n# Chutzpah Test files\n_Chutzpah*\n\n# Visual C++ cache files\nipch/\n*.aps\n*.ncb\n*.opendb\n*.opensdf\n*.sdf\n*.cachefile\n*.VC.db\n*.VC.VC.opendb\n\n# Visual Studio profiler\n*.psess\n*.vsp\n*.vspx\n*.sap\n\n# Visual Studio Trace Files\n*.e2e\n\n# TFS 2012 Local Workspace\n$tf/\n\n# Guidance Automation Toolkit\n*.gpState\n\n# ReSharper is a .NET coding add-in\n_ReSharper*/\n*.[Rr]e[Ss]harper\n*.DotSettings.user\n\n# JustCode is a .NET coding add-in\n.JustCode\n\n# TeamCity is a build add-in\n_TeamCity*\n\n# DotCover is a Code Coverage Tool\n*.dotCover\n\n# AxoCover is a Code Coverage Tool\n.axoCover/*\n!.axoCover/settings.json\n\n# Visual Studio code coverage results\n*.coverage\n*.coveragexml\n\n# NCrunch\n_NCrunch_*\n.*crunch*.local.xml\nnCrunchTemp_*\n\n# MightyMoose\n*.mm.*\nAutoTest.Net/\n\n# Web workbench (sass)\n.sass-cache/\n\n# Installshield output folder\n[Ee]xpress/\n\n# DocProject is a documentation generator add-in\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\n\n# Click-Once directory\npublish/\n\n# Publish Web Output\n*.[Pp]ublish.xml\n*.azurePubxml\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n*.pubxml\n*.publishproj\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\nPublishScripts/\n\n# NuGet Packages\n*.nupkg\n# The packages folder can be ignored because of Package Restore\n**/[Pp]ackages/*\n# except build/, which is used as an MSBuild target.\n!**/[Pp]ackages/build/\n# Uncomment if necessary however generally it will be regenerated when needed\n#!**/[Pp]ackages/repositories.config\n# NuGet v3's project.json files produces more ignorable files\n*.nuget.props\n*.nuget.targets\n\n# Microsoft Azure Build Output\ncsx/\n*.build.csdef\n\n# Microsoft Azure Emulator\necf/\nrcf/\n\n# Windows Store app package directories and files\nAppPackages/\nBundleArtifacts/\nPackage.StoreAssociation.xml\n_pkginfo.txt\n*.appx\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n*.[Cc]ache\n# but keep track of directories ending in .cache\n!*.[Cc]ache/\n\n# Others\nClientBin/\n~$*\n*~\n*.dbmdl\n*.dbproj.schemaview\n*.jfm\n*.pfx\n*.publishsettings\norleans.codegen.cs\n\n# Including strong name files can present a security risk \n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n#*.snk\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n#bower_components/\n\n# RIA/Silverlight projects\nGenerated_Code/\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\nUpgradeLog*.htm\nServiceFabricBackup/\n*.rptproj.bak\n\n# SQL Server files\n*.mdf\n*.ldf\n*.ndf\n\n# Business Intelligence projects\n*.rdl.data\n*.bim.layout\n*.bim_*.settings\n*.rptproj.rsuser\n\n# Microsoft Fakes\nFakesAssemblies/\n\n# GhostDoc plugin setting file\n*.GhostDoc.xml\n\n# Node.js Tools for Visual Studio\n.ntvs_analysis.dat\nnode_modules/\n\n# Visual Studio 6 build log\n*.plg\n\n# Visual Studio 6 workspace options file\n*.opt\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n*.vbw\n\n# Visual Studio LightSwitch build output\n**/*.HTMLClient/GeneratedArtifacts\n**/*.DesktopClient/GeneratedArtifacts\n**/*.DesktopClient/ModelManifest.xml\n**/*.Server/GeneratedArtifacts\n**/*.Server/ModelManifest.xml\n_Pvt_Extensions\n\n# Paket dependency manager\n.paket/paket.exe\npaket-files/\n\n# FAKE - F# Make\n.fake/\n\n# JetBrains Rider\n.idea/\n*.sln.iml\n\n# CodeRush\n.cr/\n\n# Python Tools for Visual Studio (PTVS)\n__pycache__/\n*.pyc\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n*.tss\n\n# Telerik's JustMock configuration file\n*.jmconfig\n\n# BizTalk build output\n*.btp.cs\n*.btm.cs\n*.odx.cs\n*.xsd.cs\n\n# OpenCover UI analysis results\nOpenCover/\n\n# Azure Stream Analytics local run output \nASALocalRun/\n\n# MSBuild Binary and Structured Log\n*.binlog\n\n# NVidia Nsight GPU debugger configuration file\n*.nvuser\n\n# MFractors (Xamarin productivity tool) working folder \n.mfractor/\n/docs/build\nsrc/TensorFlowNET.Native/bazel-*\nsrc/TensorFlowNET.Native/c_api.h\n/.vscode\ntest/TensorFlowNET.Examples/mnist\n\n\n# training model resources\n.resources\n/redist\n*.xml\n*.xsd\n\n# docs\nsite/\n\n/LLama.Unittest/Models/*.bin\n/LLama.Unittest/Models/*.gguf\n\n/LLama.Benchmark/Models/*.bin\n/LLama.Benchmark/Models/*.gguf\n\n**/appsettings.Local.json\n/LLama/runtimes/deps\n/LLama/runtimes/deps.zip\n/LLama/runtimes/release_id.txt"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.08984375,
          "content": "[submodule \"llama.cpp\"]\n\tpath = llama.cpp\n\turl = https://github.com/ggerganov/llama.cpp.git\n"
        },
        {
          "name": "Assets",
          "type": "tree",
          "content": null
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 7.708984375,
          "content": "# LLamaSharp Contributing Guide\n\nHi, welcome to develop LLamaSharp with us together! We are always open for every contributor and any format of contributions! If you want to maintain this library actively together, please contact us to get the write access after some PRs. (Email: AsakusaRinne@gmail.com)\n\nIn this page, we introduce how to make contributions here easily. 😊\n\n## The goal of LLamaSharp\n\nAt the beginning, LLamaSharp is a C# binding of [llama.cpp](https://github.com/ggerganov/llama.cpp). It provided only some wrappers for llama.cpp to let C#/.NET users could run LLM models on their local device efficiently even if without any experience with C++. After around a year of development, more tools and integrations has been added to LLamaSharp, significantly expanding the application of LLamaSharp. Though llama.cpp is still the only backend of LLamaSharp, the goal of this repository is more likely to be an efficient and easy-to-use library of LLM inference, rather than just a binding of llama.cpp.\n\nIn this way, our development of LLamaSharp is divided into two main directions:\n\n1. To make LLamaSharp more efficient. For example, `BatchedExecutor` could accept multiple queries and generate the response for them at the same time, which significantly improves the throughput. This part is always related with native APIs and executors in LLamaSharp.\n2. To make it easier to use LLamaSharp. We believe the best library is to let users build powerful functionalities with simple code. Higher-level APIs and integrations with other libraries are the key points of it.\n\n\n## How to compile the native library from source\n\nIf you want to contribute to the first direction of our goal, you may need to compile the native library yourself.\n\nFirstly, please follow the instructions in [llama.cpp readme](https://github.com/ggerganov/llama.cpp#build) to configure your local environment. Most importantly, CMake with version higher than 3.14 should be installed on your device.\n\nSecondly, clone the llama.cpp repositories. You could manually clone it and checkout to the right commit according to [Map of LLamaSharp and llama.cpp versions](https://github.com/SciSharp/LLamaSharp?tab=readme-ov-file#map-of-llamasharp-and-llama.cpp-versions), or use clone the submodule of LLamaSharp when cloning LLamaSharp.\n\n```shell\ngit clone --recursive https://github.com/SciSharp/LLamaSharp.git\n```\n\nIf you want to support cublas in the compilation, please make sure that you've installed it. If you are using Intel CPU, please check the highest AVX ([Advanced Vector Extensions](https://en.wikipedia.org/wiki/Advanced_Vector_Extensions)) level that is supported by your device.\n\nAs shown in [llama.cpp cmake file](https://github.com/ggerganov/llama.cpp/blob/master/CMakeLists.txt), there are many options that could be enabled or disabled when building the library. The following ones are commonly used when using it as a native library of LLamaSharp.\n\n```cpp\noption(BUILD_SHARED_LIBS                \"build shared libraries\") // Please always enable it \noption(LLAMA_NATIVE                     \"llama: enable -march=native flag\") // Could be disabled\noption(LLAMA_AVX                        \"llama: enable AVX\") // Enable it if the highest supported avx level is AVX\noption(LLAMA_AVX2                       \"llama: enable AVX2\") // Enable it if the highest supported avx level is AVX2\noption(LLAMA_AVX512                     \"llama: enable AVX512\") // Enable it if the highest supported avx level is AVX512\noption(LLAMA_BLAS                       \"llama: use BLAS\") // Enable it if you want to use BLAS library to accelerate the computation on CPU\noption(LLAMA_CUDA                       \"llama: use CUDA\") // Enable it if you have CUDA device\noption(LLAMA_CLBLAST                    \"llama: use CLBlast\") // Enable it if you have a device with CLBLast or OpenCL support, for example, some AMD GPUs.\noption(LLAMA_VULKAN                     \"llama: use Vulkan\") // Enable it if you have a device with Vulkan support\noption(LLAMA_METAL                      \"llama: use Metal\") // Enable it if you are using a MAC with Metal device.\noption(LLAMA_BUILD_TESTS                \"llama: build tests\") // Please disable it.\noption(LLAMA_BUILD_EXAMPLES             \"llama: build examples\") // Please disable it.\noption(LLAMA_BUILD_SERVER               \"llama: build server example\")// Please disable it.\n```\n\nMost importantly, `-DBUILD_SHARED_LIBS=ON` must be added to the cmake instruction and other options depends on you. For example, when building with cublas but without openblas, use the following instruction:\n\n```bash\nmkdir build && cd build\ncmake .. -DLLAMA_CUBLAS=ON -DBUILD_SHARED_LIBS=ON\ncmake --build . --config Release\n```\n\nNow you could find the `llama.dll`, `libllama.so` or `llama.dylib` in your build directory (or `build/bin`). \n\nTo load the compiled native library, please add the following code to the very beginning of your code.\n\n```cs\nNativeLibraryConfig.Instance.WithLibrary(\"<Your native library path>\");\n```\n\n\n## Add a new feature to LLamaSharp\n\nAfter refactoring the framework in `v0.4.0`, LLamaSharp will try to maintain the backward compatibility. However, in the following cases a breaking change will be required:\n\n1. Due to some break changes in [llama.cpp](https://github.com/ggerganov/llama.cpp), making a breaking change will help to maintain the good abstraction and friendly user APIs.\n2. An important feature cannot be implemented unless refactoring some parts.\n3. After some discussions, an agreement was reached that making the break change is reasonable.\n\nIf a new feature could be added without introducing any break change, please **open a PR** rather than open an issue first. We will never refuse the PR but help to improve it, unless it's malicious.\n\nWhen adding the feature, please take care of the namespace and the naming convention. For example, if you are adding an integration for WPF, please put the code under namespace `LLama.WPF` or `LLama.Integration.WPF` instead of putting it under the root namespace. The naming convention of LLamaSharp follows the pascal naming convention, but in some parts that are invisible to users, you can do whatever you want.\n\n## Find the problem and fix the BUG\n\nIf the issue is related to the LLM internal behaviour, such as endless generating the response, the best way to find the problem is to do comparison test between llama.cpp and LLamaSharp.\n\nYou could use exactly the same prompt, the same model and the same parameters to run the inference in llama.cpp and LLamaSharp respectively to see if it's really a problem caused by the implementation in LLamaSharp.\n\nIf the experiment showed that it worked well in llama.cpp but didn't in LLamaSharp, a search for the problem could be started. While the reason of the problem could be various, the best way I think is to add log-print in the code of llama.cpp and use it in LLamaSharp after compilation. Thus, when running LLamaSharp, you could see what happened in the native library.\n\nDuring the BUG fix process, please don't hesitate to discuss together when you are blocked.\n\n## Add integrations\n\nAll kinds of integration are welcomed here! Currently the following integrations have been added but still need improvement:\n\n1. semantic-kernel\n2. kernel-memory\n3. BotSharp (maintained in SciSharp/BotSharp repo)\n4. Langchain (maintained in tryAGI/LangChain repo)\n\nIf you find another library that is good to be integrated, please open an issue to let us know!\n\n\n## Add examples\n\nThere're mainly two ways to add an example:\n\n1. Add the example to `LLama.Examples` of the repository.\n2. Put the example in another repository and add the link to the readme or docs of LLamaSharp.\n\n## Add documents\n\nLLamaSharp uses [mkdocs](https://github.com/mkdocs/mkdocs) to build the documentation, please follow the tutorial of mkdocs to add or modify documents in LLamaSharp.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.044921875,
          "content": "MIT License\n\nCopyright (c) 2023 SciSharp STACK\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "LLama.Benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLama.Examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLama.Experimental",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLama.KernelMemory",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLama.SemanticKernel",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLama.Unittest",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLama.Web",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLama.WebAPI",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLama",
          "type": "tree",
          "content": null
        },
        {
          "name": "LLamaSharp.sln",
          "type": "blob",
          "size": 14.599609375,
          "content": "﻿\nMicrosoft Visual Studio Solution File, Format Version 12.00\n# Visual Studio Version 17\nVisualStudioVersion = 17.5.33424.131\nMinimumVisualStudioVersion = 10.0.40219.1\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLama.Unittest\", \"LLama.Unittest\\LLama.Unittest.csproj\", \"{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLama.Examples\", \"LLama.Examples\\LLama.Examples.csproj\", \"{BD1909AD-E1F8-476E-BC49-E394FF0470CE}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLamaSharp\", \"LLama\\LLamaSharp.csproj\", \"{01A12D68-DE95-425E-AEEE-2D099305036D}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLama.WebAPI\", \"LLama.WebAPI\\LLama.WebAPI.csproj\", \"{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLama.Web\", \"LLama.Web\\LLama.Web.csproj\", \"{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLamaSharp.SemanticKernel\", \"LLama.SemanticKernel\\LLamaSharp.SemanticKernel.csproj\", \"{D98F93E3-B344-4F9D-86BB-FDBF6768B587}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLamaSharp.KernelMemory\", \"LLama.KernelMemory\\LLamaSharp.KernelMemory.csproj\", \"{E5589AE7-B86F-4343-A1CC-8E5D34596E52}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLama.Experimental\", \"LLama.Experimental\\LLama.Experimental.csproj\", \"{BE4F977B-D4D9-472F-B506-EAE17542A810}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LLama.Benchmark\", \"LLama.Benchmark\\LLama.Benchmark.csproj\", \"{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}\"\nEndProject\nGlobal\n\tGlobalSection(SolutionConfigurationPlatforms) = preSolution\n\t\tDebug|Any CPU = Debug|Any CPU\n\t\tDebug|Arm64 = Debug|Arm64\n\t\tDebug|x64 = Debug|x64\n\t\tGPU|Any CPU = GPU|Any CPU\n\t\tGPU|Arm64 = GPU|Arm64\n\t\tGPU|x64 = GPU|x64\n\t\tRelease|Any CPU = Release|Any CPU\n\t\tRelease|Arm64 = Release|Arm64\n\t\tRelease|x64 = Release|x64\n\tEndGlobalSection\n\tGlobalSection(ProjectConfigurationPlatforms) = postSolution\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Debug|Arm64.ActiveCfg = Debug|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Debug|Arm64.Build.0 = Debug|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Debug|x64.ActiveCfg = Debug|x64\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Debug|x64.Build.0 = Debug|x64\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.GPU|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.GPU|Any CPU.Build.0 = Release|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.GPU|Arm64.ActiveCfg = Release|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.GPU|Arm64.Build.0 = Release|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.GPU|x64.ActiveCfg = Release|x64\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.GPU|x64.Build.0 = Release|x64\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Release|Arm64.ActiveCfg = Release|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Release|Arm64.Build.0 = Release|Any CPU\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Release|x64.ActiveCfg = Release|x64\n\t\t{BAC1CFA9-E6AC-4BD0-A548-A8066D3C467E}.Release|x64.Build.0 = Release|x64\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Debug|Arm64.ActiveCfg = Debug|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Debug|Arm64.Build.0 = Debug|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Debug|x64.ActiveCfg = Debug|x64\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Debug|x64.Build.0 = Debug|x64\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.GPU|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.GPU|Any CPU.Build.0 = Release|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.GPU|Arm64.ActiveCfg = Release|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.GPU|Arm64.Build.0 = Release|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.GPU|x64.ActiveCfg = Release|x64\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.GPU|x64.Build.0 = Release|x64\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Release|Arm64.ActiveCfg = Release|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Release|Arm64.Build.0 = Release|Any CPU\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Release|x64.ActiveCfg = Release|x64\n\t\t{BD1909AD-E1F8-476E-BC49-E394FF0470CE}.Release|x64.Build.0 = Release|x64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Debug|Arm64.ActiveCfg = Debug|Arm64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Debug|Arm64.Build.0 = Debug|Arm64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Debug|x64.ActiveCfg = Debug|x64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Debug|x64.Build.0 = Debug|x64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.GPU|Any CPU.ActiveCfg = GPU|Any CPU\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.GPU|Any CPU.Build.0 = GPU|Any CPU\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.GPU|Arm64.ActiveCfg = GPU|Arm64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.GPU|Arm64.Build.0 = GPU|Arm64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.GPU|x64.ActiveCfg = GPU|x64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.GPU|x64.Build.0 = GPU|x64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Release|Arm64.ActiveCfg = Release|Arm64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Release|Arm64.Build.0 = Release|Arm64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Release|x64.ActiveCfg = Release|x64\n\t\t{01A12D68-DE95-425E-AEEE-2D099305036D}.Release|x64.Build.0 = Release|x64\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Debug|Arm64.ActiveCfg = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Debug|Arm64.Build.0 = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Debug|x64.ActiveCfg = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Debug|x64.Build.0 = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.GPU|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.GPU|Any CPU.Build.0 = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.GPU|Arm64.ActiveCfg = Release|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.GPU|Arm64.Build.0 = Release|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.GPU|x64.ActiveCfg = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.GPU|x64.Build.0 = Debug|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Release|Arm64.ActiveCfg = Release|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Release|Arm64.Build.0 = Release|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Release|x64.ActiveCfg = Release|Any CPU\n\t\t{D3CEC57A-9027-4DA4-AAAC-612A1EB50ADF}.Release|x64.Build.0 = Release|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Debug|Arm64.ActiveCfg = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Debug|Arm64.Build.0 = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Debug|x64.ActiveCfg = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Debug|x64.Build.0 = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.GPU|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.GPU|Any CPU.Build.0 = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.GPU|Arm64.ActiveCfg = Release|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.GPU|Arm64.Build.0 = Release|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.GPU|x64.ActiveCfg = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.GPU|x64.Build.0 = Debug|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Release|Arm64.ActiveCfg = Release|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Release|Arm64.Build.0 = Release|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Release|x64.ActiveCfg = Release|Any CPU\n\t\t{C3531DB2-1B2B-433C-8DE6-3541E3620DB1}.Release|x64.Build.0 = Release|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Debug|Arm64.ActiveCfg = Debug|Arm64\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Debug|Arm64.Build.0 = Debug|Arm64\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Debug|x64.ActiveCfg = Debug|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Debug|x64.Build.0 = Debug|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.GPU|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.GPU|Any CPU.Build.0 = Debug|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.GPU|Arm64.ActiveCfg = GPU|Arm64\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.GPU|Arm64.Build.0 = GPU|Arm64\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.GPU|x64.ActiveCfg = Debug|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.GPU|x64.Build.0 = Debug|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Release|Arm64.ActiveCfg = Release|Arm64\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Release|Arm64.Build.0 = Release|Arm64\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Release|x64.ActiveCfg = Release|Any CPU\n\t\t{D98F93E3-B344-4F9D-86BB-FDBF6768B587}.Release|x64.Build.0 = Release|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Debug|Arm64.ActiveCfg = Debug|Arm64\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Debug|Arm64.Build.0 = Debug|Arm64\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Debug|x64.ActiveCfg = Debug|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Debug|x64.Build.0 = Debug|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.GPU|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.GPU|Any CPU.Build.0 = Debug|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.GPU|Arm64.ActiveCfg = GPU|Arm64\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.GPU|Arm64.Build.0 = GPU|Arm64\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.GPU|x64.ActiveCfg = Debug|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.GPU|x64.Build.0 = Debug|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Release|Arm64.ActiveCfg = Release|Arm64\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Release|Arm64.Build.0 = Release|Arm64\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Release|x64.ActiveCfg = Release|Any CPU\n\t\t{E5589AE7-B86F-4343-A1CC-8E5D34596E52}.Release|x64.Build.0 = Release|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Debug|Arm64.ActiveCfg = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Debug|Arm64.Build.0 = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Debug|x64.ActiveCfg = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Debug|x64.Build.0 = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.GPU|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.GPU|Any CPU.Build.0 = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.GPU|Arm64.ActiveCfg = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.GPU|Arm64.Build.0 = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.GPU|x64.ActiveCfg = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.GPU|x64.Build.0 = Debug|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Release|Arm64.ActiveCfg = Release|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Release|Arm64.Build.0 = Release|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Release|x64.ActiveCfg = Release|Any CPU\n\t\t{BE4F977B-D4D9-472F-B506-EAE17542A810}.Release|x64.Build.0 = Release|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Debug|Arm64.ActiveCfg = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Debug|Arm64.Build.0 = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Debug|x64.ActiveCfg = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Debug|x64.Build.0 = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.GPU|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.GPU|Any CPU.Build.0 = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.GPU|Arm64.ActiveCfg = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.GPU|Arm64.Build.0 = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.GPU|x64.ActiveCfg = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.GPU|x64.Build.0 = Debug|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Release|Arm64.ActiveCfg = Release|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Release|Arm64.Build.0 = Release|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Release|x64.ActiveCfg = Release|Any CPU\n\t\t{90D38FEE-68EA-459E-A4EE-268B9DFA1CD5}.Release|x64.Build.0 = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(SolutionProperties) = preSolution\n\t\tHideSolutionNode = FALSE\n\tEndGlobalSection\n\tGlobalSection(ExtensibilityGlobals) = postSolution\n\t\tSolutionGuid = {87746B65-249E-4AD9-882B-7B919D079367}\n\tEndGlobalSection\nEndGlobal\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.7099609375,
          "content": "﻿![logo](Assets/LLamaSharpLogo.png)\n\n[![Discord](https://img.shields.io/discord/1106946823282761851?label=Discord)](https://discord.gg/7wNVU65ZDY)\n[![QQ Group](https://img.shields.io/static/v1?label=QQ&message=加入QQ群&color=brightgreen)](http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&k=sN9VVMwbWjs5L0ATpizKKxOcZdEPMrp8&authKey=RLDw41bLTrEyEgZZi%2FzT4pYk%2BwmEFgFcrhs8ZbkiVY7a4JFckzJefaYNW6Lk4yPX&noverify=0&group_code=985366726)\n[![LLamaSharp Badge](https://img.shields.io/nuget/v/LLamaSharp?label=LLamaSharp)](https://www.nuget.org/packages/LLamaSharp)\n[![LLamaSharp Badge](https://img.shields.io/nuget/v/LLamaSharp.Backend.Cpu?label=LLamaSharp.Backend.Cpu)](https://www.nuget.org/packages/LLamaSharp.Backend.Cpu)\n[![LLamaSharp Badge](https://img.shields.io/nuget/v/LLamaSharp.Backend.Cuda11?label=LLamaSharp.Backend.Cuda11)](https://www.nuget.org/packages/LLamaSharp.Backend.Cuda11)\n[![LLamaSharp Badge](https://img.shields.io/nuget/v/LLamaSharp.Backend.Cuda12?label=LLamaSharp.Backend.Cuda12)](https://www.nuget.org/packages/LLamaSharp.Backend.Cuda12)\n[![LLamaSharp Badge](https://img.shields.io/nuget/v/LLamaSharp.semantic-kernel?label=LLamaSharp.semantic-kernel)](https://www.nuget.org/packages/LLamaSharp.semantic-kernel)\n[![LLamaSharp Badge](https://img.shields.io/nuget/v/LLamaSharp.kernel-memory?label=LLamaSharp.kernel-memory)](https://www.nuget.org/packages/LLamaSharp.kernel-memory)\n[![LLamaSharp Badge](https://img.shields.io/nuget/v/LLamaSharp.Backend.Vulkan?label=LLamaSharp.Backend.Vulkan)](https://www.nuget.org/packages/LLamaSharp.Backend.Vulkan)\n\n\n**LLamaSharp is a cross-platform library to run 🦙LLaMA/LLaVA model (and others) on your local device. Based on [llama.cpp](https://github.com/ggerganov/llama.cpp), inference with LLamaSharp is efficient on both CPU and GPU. With the higher-level APIs and RAG support, it's convenient to deploy LLMs (Large Language Models) in your application with LLamaSharp.**\n\n**Please star the repo to show your support for this project!🤗**\n\n---\n\n\n<details>\n  <summary>Table of Contents</summary>\n  <ul>\n    <li><a href=\"#Documentation\">Documentation</a></li>\n    <li><a href=\"#Console Demo\">Console Demo</a></li>\n    <li><a href=\"#Integrations & Examples\">Integrations & Examples</a></li>\n    <li><a href=\"#Get started\">Get started</a></li>\n    <li><a href=\"#FAQ\">FAQ</a></li>\n    <li><a href=\"#Contributing\">Contributing</a></li>\n    <li><a href=\"#Join the community\">Join the community</a></li>\n    <li><a href=\"#Star history\">Star history</a></li>\n    <li><a href=\"#Contributor wall of fame\">Contributor wall of fame</a></li>\n    <li><a href=\"#Map of LLamaSharp and llama.cpp versions\">Map of LLamaSharp and llama.cpp versions</a></li>\n  </ul>\n</details>\n\n## 📖Documentation\n\n- [Quick start](https://scisharp.github.io/LLamaSharp/latest/QuickStart/)\n- [FAQ](https://scisharp.github.io/LLamaSharp/latest/FAQ/)\n- [Tutorial](https://scisharp.github.io/LLamaSharp/latest/Tutorials/NativeLibraryConfig/)\n- [Full documentation](https://scisharp.github.io/LLamaSharp/latest/)\n- [API reference](https://scisharp.github.io/LLamaSharp/latest/xmldocs/)\n\n\n## 📌Console Demo\n\n<table class=\"center\">\n    <tr style=\"line-height: 0\">\n    <td width=50% height=30 style=\"border: none; text-align: center\">LLaMA</td>\n    <td width=50% height=30 style=\"border: none; text-align: center\">LLaVA</td>\n    </tr>\n    <tr>\n    <td width=25% style=\"border: none\"><img src=\"Assets/console_demo.gif\" style=\"width:100%\"></td>\n    <td width=25% style=\"border: none\"><img src=\"Assets/llava_demo.gif\" style=\"width:100%\"></td>\n    </tr>\n</table>\n\n\n## 🔗Integrations & Examples\n\nThere are integrations for the following libraries, making it easier to develop your APP. Integrations for semantic-kernel and kernel-memory are developed in the LLamaSharp repository, while others are developed in their own repositories.\n\n- [semantic-kernel](https://github.com/microsoft/semantic-kernel): an SDK that integrates LLMs like OpenAI, Azure OpenAI, and Hugging Face.\n- [kernel-memory](https://github.com/microsoft/kernel-memory): a multi-modal AI Service specialized in the efficient indexing of datasets through custom continuous data hybrid pipelines, with support for RAG ([Retrieval Augmented Generation](https://en.wikipedia.org/wiki/Prompt_engineering#Retrieval-augmented_generation)), synthetic memory, prompt engineering, and custom semantic memory processing.\n- [BotSharp](https://github.com/SciSharp/BotSharp): an open source machine learning framework for AI Bot platform builder.\n- [Langchain](https://github.com/tryAGI/LangChain): a framework for developing applications powered by language models.\n\n\nThe following examples show how to build APPs with LLamaSharp.\n\n- [Official Console Examples](./LLama.Examples/)\n- [Unity Demo](https://github.com/eublefar/LLAMASharpUnityDemo)\n- [LLamaStack (with WPF and Web demo)](https://github.com/saddam213/LLamaStack)\n- [Blazor Demo (with Model Explorer)](https://github.com/alexhiggins732/BLlamaSharp.ChatGpt.Blazor)\n- [ASP.NET Demo](./LLama.Web/)\n- [LLamaWorker (ASP.NET Web API like OAI and Function Calling Support)](https://github.com/sangyuxiaowu/LLamaWorker)\n- [VirtualPet (Desktop Application)](https://github.com/AcoranGonzalezMoray/VirtualPet-WindowsEdition)\n\n![LLamaSharp-Integrations](./Assets/LLamaSharp-Integrations.png)\n\n\n## 🚀Get started\n\n### Installation\n\nTo gain high performance, LLamaSharp interacts with native libraries compiled from c++, these are called `backends`. We provide backend packages for Windows, Linux and Mac with CPU, CUDA, Metal and Vulkan. You **don't** need to compile any c++, just install the backend packages.\n\nIf no published backend matches your device, please open an issue to let us know. If compiling c++ code is not difficult for you, you could also follow [this guide](./docs/ContributingGuide.md) to compile a backend and run LLamaSharp with it.\n\n1.  Install [LLamaSharp](https://www.nuget.org/packages/LLamaSharp) package on NuGet:\n\n```\nPM> Install-Package LLamaSharp\n```\n\n2. Install one or more of these backends, or use a self-compiled backend.\n\n   - [`LLamaSharp.Backend.Cpu`](https://www.nuget.org/packages/LLamaSharp.Backend.Cpu): Pure CPU for Windows, Linux & Mac. Metal (GPU) support for Mac.\n   - [`LLamaSharp.Backend.Cuda11`](https://www.nuget.org/packages/LLamaSharp.Backend.Cuda11): CUDA 11 for Windows & Linux.\n   - [`LLamaSharp.Backend.Cuda12`](https://www.nuget.org/packages/LLamaSharp.Backend.Cuda12): CUDA 12 for Windows & Linux.\n   - [`LLamaSharp.Backend.Vulkan`](https://www.nuget.org/packages/LLamaSharp.Backend.Vulkan): Vulkan for Windows & Linux.\n\n3. (optional) For [Microsoft semantic-kernel](https://github.com/microsoft/semantic-kernel) integration, install the [LLamaSharp.semantic-kernel](https://www.nuget.org/packages/LLamaSharp.semantic-kernel) package.\n4. (optional) To enable RAG support, install the [LLamaSharp.kernel-memory](https://www.nuget.org/packages/LLamaSharp.kernel-memory) package (this package only supports `net6.0` or higher yet), which is based on [Microsoft kernel-memory](https://github.com/microsoft/kernel-memory) integration.\n\n### Model preparation\n\nThere are two popular formats of model file of LLMs, these are PyTorch format (.pth) and Huggingface format (.bin). LLamaSharp uses a `GGUF` format file, which can be converted from these two formats. To get a `GGUF` file, there are two options:\n\n1. Search model name + 'gguf' in [Huggingface](https://huggingface.co), you will find lots of model files that have already been converted to GGUF format. Please take note of the publishing time of them because some old ones may only work with older versions of LLamaSharp.\n\n2. Convert PyTorch or Huggingface format to GGUF format yourself. Please follow the instructions from [this part of llama.cpp readme](https://github.com/ggerganov/llama.cpp?tab=readme-ov-file#prepare-and-quantize) to convert them with python scripts.\n\nGenerally, we recommend downloading models with quantization rather than fp16, because it significantly reduces the required memory size while only slightly impacting the generation quality.\n\n\n### Example of LLaMA chat session\n\nHere is a simple example to chat with a bot based on a LLM in LLamaSharp. Please replace the model path with yours.\n\n```cs\nusing LLama.Common;\nusing LLama;\n\nstring modelPath = @\"<Your Model Path>\"; // change it to your own model path.\n\nvar parameters = new ModelParams(modelPath)\n{\n    ContextSize = 1024, // The longest length of chat as memory.\n    GpuLayerCount = 5 // How many layers to offload to GPU. Please adjust it according to your GPU memory.\n};\nusing var model = LLamaWeights.LoadFromFile(parameters);\nusing var context = model.CreateContext(parameters);\nvar executor = new InteractiveExecutor(context);\n\n// Add chat histories as prompt to tell AI how to act.\nvar chatHistory = new ChatHistory();\nchatHistory.AddMessage(AuthorRole.System, \"Transcript of a dialog, where the User interacts with an Assistant named Bob. Bob is helpful, kind, honest, good at writing, and never fails to answer the User's requests immediately and with precision.\");\nchatHistory.AddMessage(AuthorRole.User, \"Hello, Bob.\");\nchatHistory.AddMessage(AuthorRole.Assistant, \"Hello. How may I help you today?\");\n\nChatSession session = new(executor, chatHistory);\n\nInferenceParams inferenceParams = new InferenceParams()\n{\n    MaxTokens = 256, // No more than 256 tokens should appear in answer. Remove it if antiprompt is enough for control.\n    AntiPrompts = new List<string> { \"User:\" }, // Stop generation once antiprompts appear.\n\n    SamplingPipeline = new DefaultSamplingPipeline(),\n};\n\nConsole.ForegroundColor = ConsoleColor.Yellow;\nConsole.Write(\"The chat session has started.\\nUser: \");\nConsole.ForegroundColor = ConsoleColor.Green;\nstring userInput = Console.ReadLine() ?? \"\";\n\nwhile (userInput != \"exit\")\n{\n    await foreach ( // Generate the response streamingly.\n        var text\n        in session.ChatAsync(\n            new ChatHistory.Message(AuthorRole.User, userInput),\n            inferenceParams))\n    {\n        Console.ForegroundColor = ConsoleColor.White;\n        Console.Write(text);\n    }\n    Console.ForegroundColor = ConsoleColor.Green;\n    userInput = Console.ReadLine() ?? \"\";\n}\n```\n\nFor more examples, please refer to [LLamaSharp.Examples](./LLama.Examples).\n\n\n## 💡FAQ\n\n#### Why is my GPU not used when I have installed CUDA?\n\n1. If you are using backend packages, please make sure you have installed the CUDA backend package which matches the CUDA version installed on your system.\n2. Add the following line to the very beginning of your code. The log will show which native library file is loaded. If the CPU library is loaded, please try to compile the native library yourself and open an issue for that. If the CUDA library is loaded, please check if `GpuLayerCount > 0` when loading the model weight.\n\n```cs\n    NativeLibraryConfig.Instance.WithLogCallback(delegate (LLamaLogLevel level, string message) { Console.Write($\"{level}: {message}\"); } )\n```\n\n\n#### Why is the inference so slow?\n\nFirstly, due to the large size of LLM models, it requires more time to generate output than other models, especially when you are using models larger than 30B parameters.\n\nTo see if that's a LLamaSharp performance issue, please follow the two tips below.\n\n1. If you are using CUDA, Metal or Vulkan, please set `GpuLayerCount` as large as possible.\n2. If it's still slower than you expect it to be, please try to run the same model with same setting in [llama.cpp examples](https://github.com/ggerganov/llama.cpp/tree/master/examples). If llama.cpp outperforms LLamaSharp significantly, it's likely a LLamaSharp BUG and please report that to us.\n\n\n#### Why does the program crash before any output is generated?\n\nGenerally, there are two possible cases for this problem:\n\n1. The native library (backend) you are using is not compatible with the LLamaSharp version. If you compiled the native library yourself, please make sure you have checked-out llama.cpp to the corresponding commit of LLamaSharp, which can be found at the bottom of README.\n2. The model file you are using is not compatible with the backend. If you are using a GGUF file downloaded from huggingface, please check its publishing time.\n\n#### Why is my model generating output infinitely?\n\nPlease set anti-prompt or max-length when executing the inference.\n\n\n## 🙌Contributing\n\nAll contributions are welcome! There's a TODO list in [LLamaSharp Dev Project](https://github.com/orgs/SciSharp/projects/5) and you can pick an interesting one to start. Please read the [contributing guide](./CONTRIBUTING.md) for more information. \n\nYou can also do one of the following to help us make LLamaSharp better:\n\n- Submit a feature request.\n- Star and share LLamaSharp to let others know about it.\n- Write a blog or demo about LLamaSharp.\n- Help to develop Web API and UI integration.\n- Just open an issue about the problem you've found!\n\n## Join the community\n\nJoin our chat on [Discord](https://discord.gg/7wNVU65ZDY) (please contact Rinne to join the dev channel if you want to be a contributor).\n\nJoin [QQ group](http://qm.qq.com/cgi-bin/qm/qr?_wv=1027&k=sN9VVMwbWjs5L0ATpizKKxOcZdEPMrp8&authKey=RLDw41bLTrEyEgZZi%2FzT4pYk%2BwmEFgFcrhs8ZbkiVY7a4JFckzJefaYNW6Lk4yPX&noverify=0&group_code=985366726)\n\n## Star history\n\n[![Star History Chart](https://api.star-history.com/svg?repos=SciSharp/LLamaSharp)](https://star-history.com/#SciSharp/LLamaSharp&Date)\n\n## Contributor wall of fame\n\n[![LLamaSharp Contributors](https://contrib.rocks/image?repo=SciSharp/LLamaSharp)](https://github.com/SciSharp/LLamaSharp/graphs/contributors)\n\n## Map of LLamaSharp and llama.cpp versions\nIf you want to compile llama.cpp yourself you **must** use the exact commit ID listed for each version.\n\n| LLamaSharp | Verified Model Resources | llama.cpp commit id |\n| - | -- | - |\n| v0.2.0 | This version is not recommended to use. | - |\n| v0.2.1 | [WizardLM](https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/previous_llama), [Vicuna (filenames with \"old\")](https://huggingface.co/eachadea/ggml-vicuna-13b-1.1/tree/main) | - |\n| v0.2.2, v0.2.3 | [WizardLM](https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/previous_llama_ggmlv2), [Vicuna (filenames without \"old\")](https://huggingface.co/eachadea/ggml-vicuna-13b-1.1/tree/main) | `63d2046` |\n| v0.3.0, v0.4.0 | [LLamaSharpSamples v0.3.0](https://huggingface.co/AsakusaRinne/LLamaSharpSamples/tree/v0.3.0), [WizardLM](https://huggingface.co/TheBloke/wizardLM-7B-GGML/tree/main) | `7e4ea5b` |\n| v0.4.1-preview | [Open llama 3b](https://huggingface.co/SlyEcho/open_llama_3b_ggml), [Open Buddy](https://huggingface.co/OpenBuddy/openbuddy-llama-ggml)| `aacdbd4` |\n|v0.4.2-preview | [Llama2 7B (GGML)](https://huggingface.co/TheBloke/llama-2-7B-Guanaco-QLoRA-GGML)| `3323112` |\n| v0.5.1 | [Llama2 7B (GGUF)](https://huggingface.co/TheBloke/llama-2-7B-Guanaco-QLoRA-GGUF)| `6b73ef1` |\n| v0.6.0 | | [`cb33f43`](https://github.com/ggerganov/llama.cpp/commit/cb33f43a2a9f5a5a5f8d290dd97c625d9ba97a2f) |\n| v0.7.0, v0.8.0 | [Thespis-13B](https://huggingface.co/TheBloke/Thespis-13B-v0.5-GGUF/tree/main?not-for-all-audiences=true), [LLaMA2-7B](https://huggingface.co/TheBloke/llama-2-7B-Guanaco-QLoRA-GGUF) | [`207b519`](https://github.com/ggerganov/llama.cpp/commit/207b51900e15cc7f89763a3bb1c565fe11cbb45d) |\n| v0.8.1 | | [`e937066`](https://github.com/ggerganov/llama.cpp/commit/e937066420b79a757bf80e9836eb12b88420a218) |\n| v0.9.0, v0.9.1 | [Mixtral-8x7B](https://huggingface.co/TheBloke/Mixtral-8x7B-v0.1-GGUF) | [`9fb13f9`](https://github.com/ggerganov/llama.cpp/blob/9fb13f95840c722ad419f390dc8a9c86080a3700) |\n| v0.10.0 | [Phi2](https://huggingface.co/TheBloke/phi-2-GGUF) | [`d71ac90`](https://github.com/ggerganov/llama.cpp/tree/d71ac90985854b0905e1abba778e407e17f9f887) |\n| v0.11.1, v0.11.2 | [LLaVA-v1.5](https://hf-mirror.com/jartine/llava-v1.5-7B-GGUF/blob/main/llava-v1.5-7b-mmproj-Q4_0.gguf), [Phi2](https://huggingface.co/TheBloke/phi-2-GGUF)| [`3ab8b3a`](https://github.com/ggerganov/llama.cpp/tree/3ab8b3a92ede46df88bc5a2dfca3777de4a2b2b6) |\n| v0.12.0 | LLama3 | [`a743d76`](https://github.com/ggerganov/llama.cpp/tree/a743d76a01f23038b2c85af1e9048ee836767b44) |\n| v0.13.0 | | [`1debe72`](https://github.com/ggerganov/llama.cpp/tree/1debe72737ea131cb52975da3d53ed3a835df3a6) |\n| v0.14.0 | Gemma2 | [`36864569`](https://github.com/ggerganov/llama.cpp/tree/368645698ab648e390dcd7c00a2bf60efa654f57) |\n| v0.15.0 | LLama3.1 | [`345c8c0c`](https://github.com/ggerganov/llama.cpp/tree/345c8c0c87a97c1595f9c8b14833d531c8c7d8df) |\n| v0.16.0 |  | [`11b84eb4`](https://github.com/ggerganov/llama.cpp/tree/11b84eb4578864827afcf956db5b571003f18180) |\n| v0.17.0 |  | [`c35e586e`](https://github.com/ggerganov/llama.cpp/tree/c35e586ea57221844442c65a1172498c54971cb0) |\n| v0.18.0 |  | [`c35e586e`](https://github.com/ggerganov/llama.cpp/tree/c35e586ea57221844442c65a1172498c54971cb0) |\n| v0.19.0 |  | [`958367bf`](https://github.com/ggerganov/llama.cpp/tree/958367bf530d943a902afa1ce1c342476098576b) |\n\n## License\n\nThis project is licensed under the terms of the MIT license.\n\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "llama.cpp",
          "type": "commit",
          "content": null
        },
        {
          "name": "mkdocs.yml",
          "type": "blob",
          "size": 15.2177734375,
          "content": "site_name: LLamaSharp Documentation\nnav:\n    - Overview: index.md\n    - Quick Start: QuickStart.md\n    - Architecture: Architecture.md\n    - FAQ: FAQ.md\n    - Contributing Guide: ContributingGuide.md\n    - Tutorials:\n        - Configure the native library loading: Tutorials/NativeLibraryConfig.md\n        - Use executors: Tutorials/Executors.md\n        - Use ChatSession: Tutorials/ChatSession.md\n        - Understand LLamaContext: Tutorials/UnderstandLLamaContext.md\n        - Get embeddings: Tutorials/GetEmbeddings.md\n        - Quantize the model: Tutorials/Quantization.md\n\n    - Advanced Tutorials:\n        - Customize the native library loading: AdvancedTutorials/CustomizeNativeLibraryLoading.md\n\n    - Integrations:\n        - semantic-kernel integration: Integrations/semantic-kernel.md\n        - kernel-memory integration: Integrations/kernel-memory.md\n        - BotSharp integration: Integrations/BotSharp.md\n        - Langchain integration: Integrations/Langchain.md\n\n    - Examples:\n        - Bacthed executor - multi-output to one input: Examples/BatchedExecutorFork.md\n        - Batched executor - basic guidance: Examples/BatchedExecutorGuidance.md\n        - Batched executor - rewinding to an earlier state: Examples/BatchedExecutorRewind.md\n        - Chinese LLM - with GB2312 encoding: Examples/ChatChineseGB2312.md\n        - ChatSession - stripping role names: Examples/ChatSessionStripRoleName.md\n        - ChatSession - with history: Examples/ChatSessionWithHistory.md\n        - ChatSession - restarting: Examples/ChatSessionWithRestart.md\n        - ChatSession - Basic: Examples/ChatSessionWithRoleName.md\n        - Coding assistant: Examples/CodingAssistant.md\n        - Get embeddings: Examples/GetEmbeddings.md\n        - Grammar - json response: Examples/GrammarJsonResponse.md\n        - Instruct executor - basic: Examples/InstructModeExecute.md\n        - Interactive executor - basic: Examples/InteractiveModeExecute.md\n        - Kernel memory integration - basic: Examples/KernelMemory.md\n        - Kernel-memory - save & load: Examples/KernelMemorySaveAndLoad.md\n        - LLaVA - basic: Examples/LLavaInteractiveModeExecute.md\n        - ChatSession - load & save: Examples/LoadAndSaveSession.md\n        - Executor - save/load state: Examples/LoadAndSaveState.md\n        - Quantization: Examples/QuantizeModel.md\n        - Semantic-kernel - chat: Examples/SemanticKernelChat.md\n        - Semantic-kernel - with kernel-memory: Examples/SemanticKernelMemory.md\n        - Semantic-kernel - basic: Examples/SemanticKernelPrompt.md\n        - Stateless executor: Examples/StatelessModeExecute.md\n        - Talk to yourself: Examples/TalkToYourself.md\n\n    - API Reference:\n        - index: ./xmldocs/index.md\n        - llama.abstractions.adaptercollection: ./xmldocs/llama.abstractions.adaptercollection.md\n        - llama.abstractions.icontextparams: ./xmldocs/llama.abstractions.icontextparams.md\n        - llama.abstractions.ihistorytransform: ./xmldocs/llama.abstractions.ihistorytransform.md\n        - llama.abstractions.iinferenceparams: ./xmldocs/llama.abstractions.iinferenceparams.md\n        - llama.abstractions.illamaexecutor: ./xmldocs/llama.abstractions.illamaexecutor.md\n        - llama.abstractions.illamaparams: ./xmldocs/llama.abstractions.illamaparams.md\n        - llama.abstractions.imodelparams: ./xmldocs/llama.abstractions.imodelparams.md\n        - llama.abstractions.itextstreamtransform: ./xmldocs/llama.abstractions.itextstreamtransform.md\n        - llama.abstractions.itexttransform: ./xmldocs/llama.abstractions.itexttransform.md\n        - llama.abstractions.loraadapter: ./xmldocs/llama.abstractions.loraadapter.md\n        - llama.abstractions.metadataoverride: ./xmldocs/llama.abstractions.metadataoverride.md\n        - llama.abstractions.metadataoverrideconverter: ./xmldocs/llama.abstractions.metadataoverrideconverter.md\n        - llama.abstractions.tensorsplitscollection: ./xmldocs/llama.abstractions.tensorsplitscollection.md\n        - llama.abstractions.tensorsplitscollectionconverter: ./xmldocs/llama.abstractions.tensorsplitscollectionconverter.md\n        - llama.antipromptprocessor: ./xmldocs/llama.antipromptprocessor.md\n        - llama.batched.alreadypromptedconversationexception: ./xmldocs/llama.batched.alreadypromptedconversationexception.md\n        - llama.batched.batchedexecutor: ./xmldocs/llama.batched.batchedexecutor.md\n        - llama.batched.cannotforkwhilerequiresinferenceexception: ./xmldocs/llama.batched.cannotforkwhilerequiresinferenceexception.md\n        - llama.batched.cannotmodifywhilerequiresinferenceexception: ./xmldocs/llama.batched.cannotmodifywhilerequiresinferenceexception.md\n        - llama.batched.cannotsamplerequiresinferenceexception: ./xmldocs/llama.batched.cannotsamplerequiresinferenceexception.md\n        - llama.batched.cannotsamplerequirespromptexception: ./xmldocs/llama.batched.cannotsamplerequirespromptexception.md\n        - llama.batched.conversation: ./xmldocs/llama.batched.conversation.md\n        - llama.batched.conversationextensions: ./xmldocs/llama.batched.conversationextensions.md\n        - llama.batched.experimentalbatchedexecutorexception: ./xmldocs/llama.batched.experimentalbatchedexecutorexception.md\n        - llama.chatsession-1: ./xmldocs/llama.chatsession-1.md\n        - llama.chatsession: ./xmldocs/llama.chatsession.md\n        - llama.common.authorrole: ./xmldocs/llama.common.authorrole.md\n        - llama.common.chathistory: ./xmldocs/llama.common.chathistory.md\n        - llama.common.fixedsizequeue-1: ./xmldocs/llama.common.fixedsizequeue-1.md\n        - llama.common.inferenceparams: ./xmldocs/llama.common.inferenceparams.md\n        - llama.common.mirostattype: ./xmldocs/llama.common.mirostattype.md\n        - llama.common.modelparams: ./xmldocs/llama.common.modelparams.md\n        - llama.exceptions.grammarexpectedname: ./xmldocs/llama.exceptions.grammarexpectedname.md\n        - llama.exceptions.grammarexpectednext: ./xmldocs/llama.exceptions.grammarexpectednext.md\n        - llama.exceptions.grammarexpectedprevious: ./xmldocs/llama.exceptions.grammarexpectedprevious.md\n        - llama.exceptions.grammarformatexception: ./xmldocs/llama.exceptions.grammarformatexception.md\n        - llama.exceptions.grammarunexpectedcharaltelement: ./xmldocs/llama.exceptions.grammarunexpectedcharaltelement.md\n        - llama.exceptions.grammarunexpectedcharrngelement: ./xmldocs/llama.exceptions.grammarunexpectedcharrngelement.md\n        - llama.exceptions.grammarunexpectedendelement: ./xmldocs/llama.exceptions.grammarunexpectedendelement.md\n        - llama.exceptions.grammarunexpectedendofinput: ./xmldocs/llama.exceptions.grammarunexpectedendofinput.md\n        - llama.exceptions.grammarunexpectedhexcharscount: ./xmldocs/llama.exceptions.grammarunexpectedhexcharscount.md\n        - llama.exceptions.grammarunknownescapecharacter: ./xmldocs/llama.exceptions.grammarunknownescapecharacter.md\n        - llama.exceptions.llamadecodeerror: ./xmldocs/llama.exceptions.llamadecodeerror.md\n        - llama.exceptions.loadweightsfailedexception: ./xmldocs/llama.exceptions.loadweightsfailedexception.md\n        - llama.exceptions.runtimeerror: ./xmldocs/llama.exceptions.runtimeerror.md\n        - llama.extensions.icontextparamsextensions: ./xmldocs/llama.extensions.icontextparamsextensions.md\n        - llama.extensions.imodelparamsextensions: ./xmldocs/llama.extensions.imodelparamsextensions.md\n        - llama.grammars.grammar: ./xmldocs/llama.grammars.grammar.md\n        - llama.grammars.grammarrule: ./xmldocs/llama.grammars.grammarrule.md\n        - llama.ichatmodel: ./xmldocs/llama.ichatmodel.md\n        - llama.llamacache: ./xmldocs/llama.llamacache.md\n        - llama.llamaembedder: ./xmldocs/llama.llamaembedder.md\n        - llama.llamamodel: ./xmldocs/llama.llamamodel.md\n        - llama.llamamodelv1: ./xmldocs/llama.llamamodelv1.md\n        - llama.llamaparams: ./xmldocs/llama.llamaparams.md\n        - llama.llamaquantizer: ./xmldocs/llama.llamaquantizer.md\n        - llama.llamastate: ./xmldocs/llama.llamastate.md\n        - llama.llamatransforms: ./xmldocs/llama.llamatransforms.md\n        - llama.llavaweights: ./xmldocs/llama.llavaweights.md\n        - llama.native.decoderesult: ./xmldocs/llama.native.decoderesult.md\n        - llama.native.ggmltype: ./xmldocs/llama.native.ggmltype.md\n        - llama.native.gpusplitmode: ./xmldocs/llama.native.gpusplitmode.md\n        - llama.native.llamabatch: ./xmldocs/llama.native.llamabatch.md\n        - llama.native.llamabeamsstate: ./xmldocs/llama.native.llamabeamsstate.md\n        - llama.native.llamabeamview: ./xmldocs/llama.native.llamabeamview.md\n        - llama.native.llamachatmessage: ./xmldocs/llama.native.llamachatmessage.md\n        - llama.native.llamacontextparams: ./xmldocs/llama.native.llamacontextparams.md\n        - llama.native.llamaftype: ./xmldocs/llama.native.llamaftype.md\n        - llama.native.llamagrammarelement: ./xmldocs/llama.native.llamagrammarelement.md\n        - llama.native.llamagrammarelementtype: ./xmldocs/llama.native.llamagrammarelementtype.md\n        - llama.native.llamakvcacheview: ./xmldocs/llama.native.llamakvcacheview.md\n        - llama.native.llamakvcacheviewcell: ./xmldocs/llama.native.llamakvcacheviewcell.md\n        - llama.native.llamakvcacheviewsafehandle: ./xmldocs/llama.native.llamakvcacheviewsafehandle.md\n        - llama.native.llamaloglevel: ./xmldocs/llama.native.llamaloglevel.md\n        - llama.native.llamamodelkvoverridetype: ./xmldocs/llama.native.llamamodelkvoverridetype.md\n        - llama.native.llamamodelmetadataoverride: ./xmldocs/llama.native.llamamodelmetadataoverride.md\n        - llama.native.llamamodelparams: ./xmldocs/llama.native.llamamodelparams.md\n        - llama.native.llamamodelquantizeparams: ./xmldocs/llama.native.llamamodelquantizeparams.md\n        - llama.native.llamanativebatch: ./xmldocs/llama.native.llamanativebatch.md\n        - llama.native.llamapoolingtype: ./xmldocs/llama.native.llamapoolingtype.md\n        - llama.native.llamapos: ./xmldocs/llama.native.llamapos.md\n        - llama.native.llamaropetype: ./xmldocs/llama.native.llamaropetype.md\n        - llama.native.llamaseqid: ./xmldocs/llama.native.llamaseqid.md\n        - llama.native.llamatoken: ./xmldocs/llama.native.llamatoken.md\n        - llama.native.llamatokendata: ./xmldocs/llama.native.llamatokendata.md\n        - llama.native.llamatokendataarray: ./xmldocs/llama.native.llamatokendataarray.md\n        - llama.native.llamatokendataarraynative: ./xmldocs/llama.native.llamatokendataarraynative.md\n        - llama.native.llamatokentype: ./xmldocs/llama.native.llamatokentype.md\n        - llama.native.llamavocabtype: ./xmldocs/llama.native.llamavocabtype.md\n        - llama.native.llavaimageembed: ./xmldocs/llama.native.llavaimageembed.md\n        - llama.native.nativeapi: ./xmldocs/llama.native.nativeapi.md\n        - llama.native.nativelibraryconfig: ./xmldocs/llama.native.nativelibraryconfig.md\n        - llama.native.ropescalingtype: ./xmldocs/llama.native.ropescalingtype.md\n        - llama.native.safellamacontexthandle: ./xmldocs/llama.native.safellamacontexthandle.md\n        - llama.native.safellamagrammarhandle: ./xmldocs/llama.native.safellamagrammarhandle.md\n        - llama.native.safellamahandlebase: ./xmldocs/llama.native.safellamahandlebase.md\n        - llama.native.safellamamodelhandle: ./xmldocs/llama.native.safellamamodelhandle.md\n        - llama.native.safellavaimageembedhandle: ./xmldocs/llama.native.safellavaimageembedhandle.md\n        - llama.native.safellavamodelhandle: ./xmldocs/llama.native.safellavamodelhandle.md\n        - llama.quantizer: ./xmldocs/llama.quantizer.md\n        - llama.sampling.basesamplingpipeline: ./xmldocs/llama.sampling.basesamplingpipeline.md\n        - llama.sampling.defaultsamplingpipeline: ./xmldocs/llama.sampling.defaultsamplingpipeline.md\n        - llama.sampling.greedysamplingpipeline: ./xmldocs/llama.sampling.greedysamplingpipeline.md\n        - llama.sampling.isamplingpipeline: ./xmldocs/llama.sampling.isamplingpipeline.md\n        - llama.sampling.isamplingpipelineextensions: ./xmldocs/llama.sampling.isamplingpipelineextensions.md\n        - llama.sampling.mirostate2samplingpipeline: ./xmldocs/llama.sampling.mirostate2samplingpipeline.md\n        - llama.sampling.mirostatesamplingpipeline: ./xmldocs/llama.sampling.mirostatesamplingpipeline.md\n        - llama.sessionstate: ./xmldocs/llama.sessionstate.md\n        - llama.streamingtokendecoder: ./xmldocs/llama.streamingtokendecoder.md\n        - llama.types.chatcompletion: ./xmldocs/llama.types.chatcompletion.md\n        - llama.types.chatcompletionchoice: ./xmldocs/llama.types.chatcompletionchoice.md\n        - llama.types.chatcompletionchunk: ./xmldocs/llama.types.chatcompletionchunk.md\n        - llama.types.chatcompletionchunkchoice: ./xmldocs/llama.types.chatcompletionchunkchoice.md\n        - llama.types.chatcompletionchunkdelta: ./xmldocs/llama.types.chatcompletionchunkdelta.md\n        - llama.types.chatcompletionmessage: ./xmldocs/llama.types.chatcompletionmessage.md\n        - llama.types.chatmessagerecord: ./xmldocs/llama.types.chatmessagerecord.md\n        - llama.types.chatrole: ./xmldocs/llama.types.chatrole.md\n        - llama.types.completion: ./xmldocs/llama.types.completion.md\n        - llama.types.completionchoice: ./xmldocs/llama.types.completionchoice.md\n        - llama.types.completionchunk: ./xmldocs/llama.types.completionchunk.md\n        - llama.types.completionlogprobs: ./xmldocs/llama.types.completionlogprobs.md\n        - llama.types.completionusage: ./xmldocs/llama.types.completionusage.md\n        - llama.types.embedding: ./xmldocs/llama.types.embedding.md\n        - llama.types.embeddingdata: ./xmldocs/llama.types.embeddingdata.md\n        - llama.types.embeddingusage: ./xmldocs/llama.types.embeddingusage.md\n        - logger: ./xmldocs/logger.md\n\ntheme:\n  name: material\n  static_templates:\n    - 404.html\n  language: 'en'\n  palette:\n    # Palette toggle for light mode\n    - media: \"(prefers-color-scheme: light)\"\n      scheme: default\n      primary: white\n      accent: red\n      toggle:\n        icon: material/weather-sunny\n        name: Switch to dark mode\n\n    # Palette toggle for dark mode\n    - media: \"(prefers-color-scheme: dark)\"\n      scheme: slate\n      primary: blue\n      accent: blue\n      toggle:\n        icon: material/weather-night\n        name: Switch to light mode\n  include_search_page: false\n  search_index_only: true\n  favicon: 'media/icon128.png'\n  icon:\n    logo: 'material/file-document'\n  features:\n    - content.action.edit\n    - navigation.instant\n  font:\n    text: 'Fira Sans'\n    code: 'Fira Mono'\n\n\nextra:\n  version:\n    provider: mike\n\nextra_css:\n  - 'css/extra.css?v=14'\n\nmarkdown_extensions:\n  - admonition\n  - def_list\n  - footnotes\n  - meta\n  - toc:\n      permalink: \"\"\n      slugify: !!python/name:pymdownx.slugs.uslugify\n  - pymdownx.arithmatex:\n      generic: true\n  - pymdownx.caret\n  - pymdownx.critic\n  - pymdownx.details\n  - pymdownx.emoji:\n      emoji_generator: !!python/name:pymdownx.emoji.to_svg\n  - pymdownx.highlight:\n      linenums: true\n  - pymdownx.inlinehilite\n  - pymdownx.keys\n  - pymdownx.magiclink\n  - pymdownx.mark\n  - pymdownx.snippets:\n      check_paths: true\n  - pymdownx.progressbar\n  - pymdownx.smartsymbols\n  - pymdownx.superfences:\n      custom_fences:\n        - name: math\n          class: arithmatex\n          format: !!python/name:pymdownx.arithmatex.fence_mathjax_format\n  - pymdownx.tasklist:\n      custom_checkbox: true\n  - pymdownx.tilde\n  - pymdownx.tabbed:\n      alternate_style: true"
        }
      ]
    }
  ]
}