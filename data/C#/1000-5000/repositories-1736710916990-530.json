{
  "metadata": {
    "timestamp": 1736710916990,
    "page": 530,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dotnet/spark",
      "stars": 2036,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".config",
          "type": "tree",
          "content": null
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 5.7626953125,
          "content": "# editorconfig.org\n\n# top-most EditorConfig file\nroot = true\n\n# Default settings:\n# A newline ending every file\n# Use 4 spaces as indentation\n[*]\ninsert_final_newline = true\nindent_style = space\nindent_size = 4\n\n[project.json]\nindent_size = 2\n\n# C# files\n[*.cs]\n# New line preferences\ncsharp_new_line_before_open_brace = all\ncsharp_new_line_before_else = true\ncsharp_new_line_before_catch = true\ncsharp_new_line_before_finally = true\ncsharp_new_line_before_members_in_object_initializers = true\ncsharp_new_line_before_members_in_anonymous_types = true\ncsharp_new_line_between_query_expression_clauses = true\n\n# Indentation preferences\ncsharp_indent_block_contents = true\ncsharp_indent_braces = false\ncsharp_indent_case_contents = true\ncsharp_indent_switch_labels = true\ncsharp_indent_labels = one_less_than_current\n\n# avoid this. unless absolutely necessary\ndotnet_style_qualification_for_field = false:suggestion\ndotnet_style_qualification_for_property = false:suggestion\ndotnet_style_qualification_for_method = false:suggestion\ndotnet_style_qualification_for_event = false:suggestion\n\n# only use var when it's obvious what the variable type is\ncsharp_style_var_for_built_in_types = false:none\ncsharp_style_var_when_type_is_apparent = false:none\ncsharp_style_var_elsewhere = false:suggestion\n\n# use language keywords instead of BCL types\ndotnet_style_predefined_type_for_locals_parameters_members = true:suggestion\ndotnet_style_predefined_type_for_member_access = true:suggestion\n\n# name all constant fields using PascalCase\ndotnet_naming_rule.constant_fields_should_be_pascal_case.severity = suggestion\ndotnet_naming_rule.constant_fields_should_be_pascal_case.symbols  = constant_fields\ndotnet_naming_rule.constant_fields_should_be_pascal_case.style    = pascal_case_style\n\ndotnet_naming_symbols.constant_fields.applicable_kinds   = field\ndotnet_naming_symbols.constant_fields.required_modifiers = const\n\ndotnet_naming_style.pascal_case_style.capitalization = pascal_case\n\n# static fields should have s_ prefix\ndotnet_naming_rule.static_fields_should_have_prefix.severity = suggestion\ndotnet_naming_rule.static_fields_should_have_prefix.symbols  = static_fields\ndotnet_naming_rule.static_fields_should_have_prefix.style    = static_prefix_style\n\ndotnet_naming_symbols.static_fields.applicable_kinds   = field\ndotnet_naming_symbols.static_fields.required_modifiers = static\n\ndotnet_naming_style.static_prefix_style.required_prefix = s_\ndotnet_naming_style.static_prefix_style.capitalization = camel_case \n\n# internal and private fields should be _camelCase\ndotnet_naming_rule.camel_case_for_private_internal_fields.severity = suggestion\ndotnet_naming_rule.camel_case_for_private_internal_fields.symbols  = private_internal_fields\ndotnet_naming_rule.camel_case_for_private_internal_fields.style    = camel_case_underscore_style\n\ndotnet_naming_symbols.private_internal_fields.applicable_kinds = field\ndotnet_naming_symbols.private_internal_fields.applicable_accessibilities = private, internal\n\ndotnet_naming_style.camel_case_underscore_style.required_prefix = _\ndotnet_naming_style.camel_case_underscore_style.capitalization = camel_case \n\n# Code style defaults\ndotnet_sort_system_directives_first = true\ncsharp_preserve_single_line_blocks = true\ncsharp_preserve_single_line_statements = false\n\n# Expression-level preferences\ndotnet_style_object_initializer = true:suggestion\ndotnet_style_collection_initializer = true:suggestion\ndotnet_style_explicit_tuple_names = true:suggestion\ndotnet_style_coalesce_expression = true:suggestion\ndotnet_style_null_propagation = true:suggestion\n\n# Expression-bodied members\ncsharp_style_expression_bodied_methods = false:none\ncsharp_style_expression_bodied_constructors = false:none\ncsharp_style_expression_bodied_operators = false:none\ncsharp_style_expression_bodied_properties = true:none\ncsharp_style_expression_bodied_indexers = true:none\ncsharp_style_expression_bodied_accessors = true:none\n\n# Pattern matching\ncsharp_style_pattern_matching_over_is_with_cast_check = true:suggestion\ncsharp_style_pattern_matching_over_as_with_null_check = true:suggestion\ncsharp_style_inlined_variable_declaration = true:suggestion\n\n# Null checking preferences\ncsharp_style_throw_expression = true:suggestion\ncsharp_style_conditional_delegate_call = true:suggestion\n\n# Space preferences\ncsharp_space_after_cast = false\ncsharp_space_after_colon_in_inheritance_clause = true\ncsharp_space_after_comma = true\ncsharp_space_after_dot = false\ncsharp_space_after_keywords_in_control_flow_statements = true\ncsharp_space_after_semicolon_in_for_statement = true\ncsharp_space_around_binary_operators = before_and_after\ncsharp_space_around_declaration_statements = do_not_ignore\ncsharp_space_before_colon_in_inheritance_clause = true\ncsharp_space_before_comma = false\ncsharp_space_before_dot = false\ncsharp_space_before_open_square_brackets = false\ncsharp_space_before_semicolon_in_for_statement = false\ncsharp_space_between_empty_square_brackets = false\ncsharp_space_between_method_call_empty_parameter_list_parentheses = false\ncsharp_space_between_method_call_name_and_opening_parenthesis = false\ncsharp_space_between_method_call_parameter_list_parentheses = false\ncsharp_space_between_method_declaration_empty_parameter_list_parentheses = false\ncsharp_space_between_method_declaration_name_and_open_parenthesis = false\ncsharp_space_between_method_declaration_parameter_list_parentheses = false\ncsharp_space_between_parentheses = false\ncsharp_space_between_square_brackets = false\n\n# Blocks are allowed\ncsharp_prefer_braces = true:silent\n\n# Xml project files\n[*.{csproj,vcxproj,vcxproj.filters,proj,nativeproj,locproj}]\nindent_size = 2\n\n# Xml build files\n[*.builds]\nindent_size = 2\n\n# Xml files\n[*.{xml,stylecop,resx,ruleset}]\nindent_size = 2\n\n# Xml config files\n[*.{props,targets,config,nuspec}]\nindent_size = 2\n\n# Shell scripts\n[*.sh]\nend_of_line = lf\n[*.{cmd, bat}]\nend_of_line = crlf\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.3896484375,
          "content": "###############################################################################\n# Set default behavior to automatically normalize line endings.\n###############################################################################\n* text=auto\n\n# Force bash scripts to always use lf line endings so that if a repo is accessed\n# in Unix via a file share from Windows, the scripts will work.\n*.sh text eol=lf\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 6.017578125,
          "content": "## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n##\n## Get latest from https://github.com/github/gitignore/blob/master/VisualStudio.gitignore\n\n# User-specific files\n*.rsuser\n*.suo\n*.user\n*.userosscache\n*.sln.docstates\n\n# User-specific files (MonoDevelop/Xamarin Studio)\n*.userprefs\n\n# Build results\n[Dd]ebug/\n[Dd]ebugPublic/\n[Rr]elease/\n[Rr]eleases/\nx64/\nx86/\nbld/\n[Bb]in/\n[Oo]bj/\n[Ll]og/\n\n# Visual Studio 2015/2017 cache/options directory\n.vs/\n# Uncomment if you have tasks that create the project's static files in wwwroot\n#wwwroot/\n\n# Visual Studio 2017 auto generated files\nGenerated\\ Files/\n\n# MSTest test Results\n[Tt]est[Rr]esult*/\n[Bb]uild[Ll]og.*\n\n# NUNIT\n*.VisualState.xml\nTestResult.xml\n\n# Build Results of an ATL Project\n[Dd]ebugPS/\n[Rr]eleasePS/\ndlldata.c\n\n# Benchmark Results\nBenchmarkDotNet.Artifacts/\n\n# .NET\nproject.lock.json\nproject.fragment.lock.json\nartifacts/\n.dotnet/\n\n# StyleCop\nStyleCopReport.xml\n\n# Files built by Visual Studio\n*_i.c\n*_p.c\n*_h.h\n*.ilk\n*.meta\n*.obj\n*.iobj\n*.pch\n*.pdb\n*.ipdb\n*.pgc\n*.pgd\n*.rsp\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp\n*.tmp_proj\n*_wpftmp.csproj\n*.log\n*.vspscc\n*.vssscc\n.builds\n*.pidb\n*.svclog\n*.scc\n\n# Chutzpah Test files\n_Chutzpah*\n\n# Visual C++ cache files\nipch/\n*.aps\n*.ncb\n*.opendb\n*.opensdf\n*.sdf\n*.cachefile\n*.VC.db\n*.VC.VC.opendb\n\n# Visual Studio profiler\n*.psess\n*.vsp\n*.vspx\n*.sap\n\n# Visual Studio Trace Files\n*.e2e\n\n# TFS 2012 Local Workspace\n$tf/\n\n# Guidance Automation Toolkit\n*.gpState\n\n# ReSharper is a .NET coding add-in\n_ReSharper*/\n*.[Rr]e[Ss]harper\n*.DotSettings.user\n\n# JustCode is a .NET coding add-in\n.JustCode\n\n# TeamCity is a build add-in\n_TeamCity*\n\n# DotCover is a Code Coverage Tool\n*.dotCover\n\n# AxoCover is a Code Coverage Tool\n.axoCover/*\n!.axoCover/settings.json\n\n# Visual Studio code coverage results\n*.coverage\n*.coveragexml\n\n# NCrunch\n_NCrunch_*\n.*crunch*.local.xml\nnCrunchTemp_*\n\n# MightyMoose\n*.mm.*\nAutoTest.Net/\n\n# Web workbench (sass)\n.sass-cache/\n\n# Installshield output folder\n[Ee]xpress/\n\n# DocProject is a documentation generator add-in\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\n\n# Click-Once directory\npublish/\n\n# Publish Web Output\n*.[Pp]ublish.xml\n*.azurePubxml\n# Note: Comment the next line if you want to checkin your web deploy settings,\n# but database connection strings (with potential passwords) will be unencrypted\n*.pubxml\n*.publishproj\n\n# Microsoft Azure Web App publish settings. Comment the next line if you want to\n# checkin your Azure Web App publish settings, but sensitive information contained\n# in these scripts will be unencrypted\nPublishScripts/\n\n# NuGet Packages\n*.nupkg\n# NuGet Symbol Packages\n*.snupkg\n# The packages folder can be ignored because of Package Restore\n**/[Pp]ackages/*\n# except build/, which is used as an MSBuild target.\n!**/[Pp]ackages/build/\n# Uncomment if necessary however generally it will be regenerated when needed\n#!**/[Pp]ackages/repositories.config\n# NuGet v3's project.json files produces more ignorable files\n*.nuget.props\n*.nuget.targets\n\n# Microsoft Azure Build Output\ncsx/\n*.build.csdef\n\n# Microsoft Azure Emulator\necf/\nrcf/\n\n# Windows Store app package directories and files\nAppPackages/\nBundleArtifacts/\nPackage.StoreAssociation.xml\n_pkginfo.txt\n*.appx\n\n# Visual Studio cache files\n# files ending in .cache can be ignored\n*.[Cc]ache\n# but keep track of directories ending in .cache\n!*.[Cc]ache/\n\n# Others\nClientBin/\n~$*\n*~\n*.dbmdl\n*.dbproj.schemaview\n*.jfm\n*.pfx\n*.publishsettings\norleans.codegen.cs\n\n# Including strong name files can present a security risk\n# (https://github.com/github/gitignore/pull/2483#issue-259490424)\n#*.snk\n\n# Since there are multiple workflows, uncomment next line to ignore bower_components\n# (https://github.com/github/gitignore/pull/1529#issuecomment-104372622)\n#bower_components/\n\n# RIA/Silverlight projects\nGenerated_Code/\n\n# Backup & report files from converting an old project file\n# to a newer Visual Studio version. Backup files are not needed,\n# because we have git ;-)\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\nUpgradeLog*.htm\nServiceFabricBackup/\n*.rptproj.bak\n\n# SQL Server files\n*.mdf\n*.ldf\n*.ndf\n\n# Business Intelligence projects\n*.rdl.data\n*.bim.layout\n*.bim_*.settings\n*.rptproj.rsuser\n\n# Microsoft Fakes\nFakesAssemblies/\n\n# GhostDoc plugin setting file\n*.GhostDoc.xml\n\n# Node.js Tools for Visual Studio\n.ntvs_analysis.dat\nnode_modules/\n\n# Visual Studio 6 build log\n*.plg\n\n# Visual Studio 6 workspace options file\n*.opt\n\n# Visual Studio 6 auto-generated workspace file (contains which files were open etc.)\n*.vbw\n\n# Visual Studio LightSwitch build output\n**/*.HTMLClient/GeneratedArtifacts\n**/*.DesktopClient/GeneratedArtifacts\n**/*.DesktopClient/ModelManifest.xml\n**/*.Server/GeneratedArtifacts\n**/*.Server/ModelManifest.xml\n_Pvt_Extensions\n\n# Paket dependency manager\n.paket/paket.exe\npaket-files/\n\n# FAKE - F# Make\n.fake/\n\n# JetBrains Rider\n.idea/\n*.sln.iml\n\n# CodeRush personal settings\n.cr/personal\n\n# Python Tools for Visual Studio (PTVS)\n__pycache__/\n*.pyc\n\n# Cake - Uncomment if you are using it\n# tools/**\n# !tools/packages.config\n\n# Tabs Studio\n*.tss\n\n# Telerik's JustMock configuration file\n*.jmconfig\n\n# BizTalk build output\n*.btp.cs\n*.btm.cs\n*.odx.cs\n*.xsd.cs\n\n# OpenCover UI analysis results\nOpenCover/\n\n# Azure Stream Analytics local run output\nASALocalRun/\n\n# MSBuild Binary and Structured Log\n*.binlog\n\n# NVidia Nsight GPU debugger configuration file\n*.nvuser\n\n# MFractors (Xamarin productivity tool) working folder\n.mfractor/\n\n# Local History for Visual Studio\n.localhistory/\n\n# Below is for ignore files for Java taken from github/gitignore.\n\n# Compiled class file\n*.class\n\n# Log file\n*.log\n\n# BlueJ files\n*.ctxt\n\n# Mobile Tools for Java (J2ME)\n.mtj.tmp/\n\n# Package Files #\n*.jar\n*.war\n*.nar\n*.ear\n*.zip\n*.tar.gz\n*.rar\n\n# virtual machine crash logs, see http://www.java.com/en/download/help/error_hotspot.xml\nhs_err_pid*\n\n# IntelliJ file\n*.iml\n\n# The target folder contains the output of building\n**/target/**\n\n# F# vs code \n.ionide/\n\n# Mac dev\n.DS_Store"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 0.26953125,
          "content": "This project has adopted the code of conduct defined by the [Contributor Covenant](https://contributor-covenant.org/) to clarify expected behavior in our community.\nFor more information, see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 7.7734375,
          "content": "# Contributing to .NET for Apache Spark!\n\nIf you are here, it means you are interested in helping us out. A hearty welcome and thank you! There are many ways you can contribute to the .NET for Apache Spark project:\n\n* Offer PR's to fix bugs or implement new features\n* Review currently [open PRs](https://github.com/dotnet/spark/pulls)\n* Give us feedback and bug reports regarding the software or the documentation\n* Improve our examples, tutorials, and documentation\n\nPlease start by browsing the [issues](https://github.com/dotnet/spark/issues) and leave a comment to engage us if any of them interests you. And don't forget to take a look at the project [roadmap](https://github.com/dotnet/spark/blob/master/ROADMAP.md).\n\nHere are a few things to consider:\n\n* Before starting working on a major feature or bug fix, please open a GitHub issue describing the work you are proposing. We will make sure no one else is already working on it and the work aligns with the project [roadmap](https://github.com/dotnet/spark/blob/master/ROADMAP.md).\n* A \"major\" feature or bug fix is defined as any change that is > 100 lines of code (not including tests) or changes user-facing behavior (e.g., breaking API changes). Please read [Proposing Major Changes to .NET for Apache Spark](#proposing-major-changes-to-net-for-apache-spark) before you begin any major work.\n* Once you are ready, you can create a PR and the committers will help reviewing your PR.\n\n**Coding Style**: Please review our [coding guidelines](https://github.com/rapoth/spark-2/blob/master/docs/contributing.md). \n\n## Getting started\n\nPlease make sure to take a look at the project [roadmap](ROADMAP.md).\n\n### Pull requests\n\nIf you are new to GitHub [here](https://help.github.com/categories/collaborating-with-issues-and-pull-requests/) is a detailed help source on getting involved with development on GitHub.\n\nAs a first time contributor, you will be invited to sign the Contributor License Agreement (CLA). Please follow the instructions of the dotnet foundation bot reviewer on your PR to sign the agreement indicating that you have appropriate rights to your contribution.\n\nYour pull request needs to reference a filed issue. Please fill in the template that is populated for the pull request. Only pull requests addressing small typos can have no issues associated with them.\n\nA .NET for Apache Spark team member will be assigned to your pull request once the continuous integration checks have passed successfully.\n\nAll commits in a pull request will be squashed to a single commit with the original creator as author.\n\n### Contributing\n\nSee [Contributing](docs/contributing.md) for information about coding styles, source structure, making pull requests, and more.\n\n### Developers\n\nSee the [Developer Guide](docs/developer-guide.md) for details about developing in this repo.\n\n## Proposing major changes to .NET for Apache Spark\n\nThe development process in .NET for Apache Spark is design-driven. If you intend of making any significant changes, please consider discussing with the .NET for Apache Spark community first (and sometimes formally documented), before you open a PR.\n\nThe rest of this document describes the process for proposing, documenting and implementing changes to the .NET for Apache Spark project.\n\nTo learn about the motivation behind .NET for Apache Spark, see the following talk:\n  - [Introducing .NET Bindings for Apache Spark](https://databricks.com/session/introducing-net-bindings-for-apache-spark) from Spark+AI Summit 2019.\n  - [.NET for Apache Spark](https://databricks.com/session_eu19/net-for-apache-spark) from Spark+AI Europe Summit 2019.\n\n### The Proposal Process\n\nThe process outlined below is for reviewing a proposal and reaching a decision about whether to accept/decline a proposal.\t\n\n  1. The proposal author [creates a brief issue](https://github.com/dotnet/spark/issues/new?assignees=&labels=untriaged%2C+proposal&template=design-template.md&title=%5BPROPOSAL%5D%3A+) describing the proposal.\n  2. A discussion on the issue will aim to triage the proposal into one of three outcomes:\n     - Accept proposal\n     - Decline proposal\n     - Ask for a detailed doc\n     If the proposal is accepted/declined, the process is done. Otherwise, the discussion is expected to identify concerns that should be addressed in a more detailed design.\n  3. The proposal author follows up with a detailed description to work out details of the proposed design and address the concerns raised in the initial discussion.\n  4. Once comments and revisions on the design are complete, there is a final discussion on the issue to reach one of two outcomes:\n     - Accept proposal\n     - Decline proposal\n\nAfter the proposal is accepted or declined (e.g., after Step 2 or Step 4), implementation work proceeds in the same way as any other contribution. \n\n> **Tip:** If you are an experienced committer and are certain that a design description will be required for a particular proposal, you can skip Step 2.\n\n### Writing a Design Document\n\nAs noted [above](#the-proposal-process), some (but not all) proposals need to be elaborated in a design description.\t\n\n  - The design description should follow the template outlined below\n  ```\n      Proposal:\n    \n      Rationale:\n      \n      Compatibility:\n      \n      Design:\n      \n      Implementation:\n      \n      Impact on Performance (if applicable):\n      \n      Open issues (if applicable):\n  ``` \n\n  - Once you have the design description ready and have addressed any specific concerns raised during the initial discussion, please reply back to the original issue. \n  - Address any additional feedback/questions and update your design description as needed. \n\n### Proposal Review\n\nA group of .NET for Apache Spark team members will review your proposal and CC the relevant developers, raising important questions, pinging lapsed discussions, and generally trying to guide the discussion toward agreement about the outcome. The discussion itself is expected to happen on the issue, so that anyone can take part.\t\n\n### Consensus and Disagreement\n\nThe goal of the proposal process is to reach general consensus about the outcome in a timely manner.\t\n\nIf general consensus cannot be reached, the proposal review group decides the next step by reviewing and discussing the issue and reaching a consensus among themselves. \n\n## Becoming a .NET for Apache Spark Committer\n\nThe .NET for Apache Spark team will add new committers from the active contributors, based on their contributions to the .NET for Apache Spark project. The qualifications for new committers are derived from [Apache Spark Contributor Guide](https://spark.apache.org/contributing.html):\n\n  - **Sustained contributions to .NET for Apache Spark**: Committers should have a history of major contributions to .NET for Apache Spark. An ideal committer will have contributed broadly throughout the project, and have contributed at least one major component where they have taken an “ownership” role. An ownership role means that existing contributors feel that they should run patches for this component by this person.\n  - **Quality of contributions**: Committers more than any other community member should submit simple, well-tested, and well-designed patches. In addition, they should show sufficient expertise to be able to review patches, including making sure they fit within .NET for Apache Spark’s engineering practices (testability, documentation, API stability, code style, etc). The committership is collectively responsible for the software quality and maintainability of .NET for Apache Spark. \n  - **Community involvement**: Committers should have a constructive and friendly attitude in all community interactions. They should also be active on the dev and user list and help mentor newer contributors and users. In design discussions, committers should maintain a professional and diplomatic approach, even in the face of disagreement.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.046875,
          "content": "MIT License\n\nCopyright (c) 2019 .NET Foundation\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "NuGet.config",
          "type": "blob",
          "size": 0.5947265625,
          "content": "<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<configuration>\n  <packageSources>\n    <clear />\n    <add key=\"dotnet-public\" value=\"https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-public/nuget/v3/index.json\" />\n    <add key=\"dotnet-tools\" value=\"https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-tools/nuget/v3/index.json\" />\n    <add key=\"dotnet-eng\" value=\"https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet-eng/nuget/v3/index.json\" />\n    <add key=\"dotnet5\" value=\"https://pkgs.dev.azure.com/dnceng/public/_packaging/dotnet5/nuget/v3/index.json\" />\n  </packageSources>\n</configuration>\n"
        },
        {
          "name": "PULL_REQUEST_TEMPLATE.md",
          "type": "blob",
          "size": 0.6240234375,
          "content": "We are excited to review your PR.\n\nSo we can do the best job, please check:\n\n- [ ] There's a descriptive title that will make sense to other developers some time from now. \n- [ ] There's associated issues. All PR's should have issue(s) associated - unless a trivial self-evident change such as fixing a typo. You can use the format `Fixes #nnnn` in your description to cause GitHub to automatically close the issue(s) when your PR is merged.\n- [ ] Your change description explains what the change does, why you chose your approach, and anything else that reviewers should know.\n- [ ] You have included any necessary tests in the same PR.\n\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.5947265625,
          "content": "[![NuGet Badge](https://buildstats.info/nuget/Microsoft.Spark)](https://www.nuget.org/packages/Microsoft.Spark)\n\n![Icon](docs/img/dotnetsparklogo-6.png)\n\n# .NET for Apache® Spark™\n\n.NET for Apache Spark provides high performance APIs for using [Apache Spark](https://spark.apache.org/) from C# and F#. With these .NET APIs, you can access the most popular Dataframe and SparkSQL aspects of Apache Spark, for working with structured data, and Spark Structured Streaming, for working with streaming data. \n\n.NET for Apache Spark is compliant with .NET Standard - a formal specification of .NET APIs that are common across .NET implementations. This means you can use .NET for Apache Spark anywhere you write .NET code allowing you to reuse all the knowledge, skills, code, and libraries you already have as a .NET developer. \n\n.NET for Apache Spark runs on Windows, Linux, and macOS using .NET 6, or Windows using .NET Framework. It also runs on all major cloud providers including [Azure HDInsight Spark](deployment/README.md#azure-hdinsight-spark), [Amazon EMR Spark](deployment/README.md#amazon-emr-spark), [AWS](deployment/README.md#databricks) & [Azure](deployment/README.md#databricks) Databricks.\n\n**Note**: We currently have a Spark Project Improvement Proposal JIRA at [SPIP: .NET bindings for Apache Spark](https://issues.apache.org/jira/browse/SPARK-27006) to work with the community towards getting .NET support by default into Apache Spark. We highly encourage you to participate in the discussion. \n\n## Table of Contents\n\n- [Supported Apache Spark](#supported-apache-spark)\n- [Releases](#releases)\n- [Get Started](#get-started)\n- [Build Status](#build-status)\n- [Building from Source](#building-from-source)\n- [Samples](#samples)\n- [Contributing](#contributing)\n- [Inspiration and Special Thanks](#inspiration-and-special-thanks)\n- [How to Engage, Contribute and Provide Feedback](#how-to-engage-contribute-and-provide-feedback)\n- [Support](#support)\n- [.NET Foundation](#net-foundation)\n- [Code of Conduct](#code-of-conduct)\n- [License](#license)\n\n## Supported Apache Spark\n\n<table>\n    <thead>\n        <tr>\n            <th>Apache Spark</th>\n            <th>.NET for Apache Spark</th>\n        </tr>\n    </thead>\n    <tbody align=\"center\">\n        <tr>\n            <td>2.4*</td>\n            <td rowspan=4><a href=\"https://github.com/dotnet/spark/releases/tag/v2.1.1\">v2.1.1</a></td>\n        </tr>\n        <tr>\n            <td>3.0</td>\n        </tr>\n        <tr>\n            <td>3.1</td>\n        </tr>\n        <tr>\n            <td>3.2</td>\n        </tr>\n    </tbody>\n</table>\n\n*2.4.2 is <a href=\"https://github.com/dotnet/spark/issues/60\">not supported</a>.\n\n## Releases\n\n.NET for Apache Spark releases are available [here](https://github.com/dotnet/spark/releases) and NuGet packages are available [here](https://www.nuget.org/packages/Microsoft.Spark).\n\n## Get Started\nThese instructions will show you how to run a .NET for Apache Spark app using .NET 6.\n- [Windows Instructions](docs/getting-started/windows-instructions.md)\n- [Ubuntu Instructions](docs/getting-started/ubuntu-instructions.md)\n- [MacOs Instructions](docs/getting-started/macos-instructions.md)\n\n## Build Status\n\n| ![Ubuntu icon](docs/img/ubuntu-icon-32.png) | ![Windows icon](docs/img/windows-icon-32.png) |\n| :---:         |          :---: |\n| Ubuntu | Windows |\n| | [![Build Status](https://dnceng.visualstudio.com/public/_apis/build/status/dotnet.spark?branchName=main)](https://dev.azure.com/dnceng/public/_build?definitionId=459&branchName=main)|\n\n## Building from Source\n\nBuilding from source is very easy and the whole process (from cloning to being able to run your app) should take less than 15 minutes!\n\n| |  | Instructions |\n| :---: | :---         |      :--- |\n| ![Windows icon](docs/img/windows-icon-32.png) | **Windows**    | <ul><li>Local - [.NET Framework 4.6.1](docs/building/windows-instructions.md#using-visual-studio-for-net-framework-461)</li><li>Local - [.NET 6](docs/building/windows-instructions.md#using-net-core-cli-for-net-core)</li><ul>    |\n| ![Ubuntu icon](docs/img/ubuntu-icon-32.png) | **Ubuntu**     | <ul><li>Local - [.NET 6](docs/building/ubuntu-instructions.md)</li><li>[Azure HDInsight Spark - .NET 6](deployment/README.md)</li></ul>      |\n\n<a name=\"samples\"></a>\n## Samples\n\nThere are two types of samples/apps in the .NET for Apache Spark repo:\n\n* ![Icon](docs/img/app-type-getting-started.png) Getting Started - .NET for Apache Spark code focused on simple and minimalistic scenarios.\n\n* ![Icon](docs/img/app-type-e2e.png)  End-End apps/scenarios - Real world examples of industry standard benchmarks, usecases and business applications implemented using .NET for Apache Spark. \n\nWe welcome contributions to both categories!\n\n<table>\n <tr>\n   <td width=\"25%\">\n      <h4><b>Analytics Scenario</b></h4>\n  </td>\n  <td>\n      <h4 width=\"35%\"><b>Description</b></h4>\n  </td>\n  <td>\n      <h4><b>Scenarios</b></h4>\n  </td>\n </tr>\n <tr>\n   <td width=\"25%\">\n      <h5>Dataframes and SparkSQL</h5>\n  </td>\n  <td width=\"35%\">\n  Simple code snippets to help you get familiarized with the programmability experience of .NET for Apache Spark.\n  </td>\n    <td>\n      <h5>Basic &nbsp;&nbsp;&nbsp;\n      <a href=\"examples/Microsoft.Spark.CSharp.Examples/Sql/Batch/Basic.cs\">C#</a> &nbsp; &nbsp; <a href=\"examples/Microsoft.Spark.FSharp.Examples/Sql/Basic.fs\">F#</a>&nbsp;&nbsp;&nbsp;<a href=\"#\"><img src=\"docs/img/app-type-getting-started.png\" alt=\"Getting started icon\"></a></h5>\n  </td>\n </tr>\n <tr>\n   <td width=\"25%\">\n      <h5>Structured Streaming</h5>\n  </td>\n  <td width=\"35%\">\n      Code snippets to show you how to utilize Apache Spark's Structured Streaming (<a href=\"https://spark.apache.org/docs/2.3.1/structured-streaming-programming-guide.html\">2.3.1</a>, <a href=\"https://spark.apache.org/docs/2.3.2/structured-streaming-programming-guide.html\">2.3.2</a>, <a href=\"https://spark.apache.org/docs/2.4.1/structured-streaming-programming-guide.html\">2.4.1</a>, <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\">Latest</a>)\n  </td>\n  <td>\n      <h5>Word Count &nbsp;&nbsp;&nbsp;\n      <a href=\"examples/Microsoft.Spark.CSharp.Examples/Sql/Streaming/StructuredNetworkWordCount.cs\">C#</a> &nbsp;&nbsp;&nbsp;<a href=\"examples/Microsoft.Spark.FSharp.Examples/Sql/Streaming/StructuredNetworkWordCount.fs\">F#</a> &nbsp;&nbsp;&nbsp;<a href=\"#\"><img src=\"docs/img/app-type-getting-started.png\" alt=\"Getting started icon\"></a></h5>\n      <h5>Windowed Word Count &nbsp;&nbsp;&nbsp;<a href=\"examples/Microsoft.Spark.CSharp.Examples/Sql/Streaming/StructuredNetworkWordCountWindowed.cs\">C#</a> &nbsp; &nbsp;<a href=\"examples/Microsoft.Spark.FSharp.Examples/Sql/Streaming/StructuredNetworkWordCountWindowed.fs\">F#</a> &nbsp;&nbsp;&nbsp;<a href=\"#\"><img src=\"docs/img/app-type-getting-started.png\" alt=\"Getting started icon\"></a></h5>      \n      <h5>Word Count on data from <a href=\"https://kafka.apache.org/\">Kafka</a> &nbsp;&nbsp;&nbsp;<a href=\"examples/Microsoft.Spark.CSharp.Examples/Sql/Streaming/StructuredKafkaWordCount.cs\">C#</a> &nbsp;&nbsp;&nbsp;<a href=\"examples/Microsoft.Spark.FSharp.Examples/Sql/Streaming/StructuredKafkaWordCount.fs\">F#</a> &nbsp; &nbsp;&nbsp;<a href=\"#\"><img src=\"docs/img/app-type-getting-started.png\" alt=\"Getting started icon\"></a></h5>\n  </td>\n </tr>\n <tr>\n   <td width=\"25%\">\n      <h4>TPC-H Queries</h4>\n  </td>\n  <td width=\"35%\">\n  Code to show you how to author complex queries using .NET for Apache Spark.\n  </td>\n  <td>\n      <h5>TPC-H Functional &nbsp;&nbsp;&nbsp;\n      <a href=\"benchmark/csharp/Tpch/TpchFunctionalQueries.cs\">C#</a> &nbsp;&nbsp;&nbsp;<a href=\"#\"><img src=\"docs/img/app-type-e2e.png\" alt=\"End-to-end app icon\"></a></h5>\n      <h5>TPC-H SparkSQL &nbsp;&nbsp;&nbsp;\n      <a href=\"benchmark/csharp/Tpch/TpchSqlQueries.cs\">C#</a>  &nbsp;&nbsp;&nbsp;<a href=\"#\"><img src=\"docs/img/app-type-e2e.png\" alt=\"End-to-end app icon\"></a></h5>\n  </td>\n</tr>\n </tr> \n </table>\n\n## Contributing\n\nWe welcome contributions! Please review our [contribution guide](CONTRIBUTING.md).\n\n## Inspiration and Special Thanks\n\nThis project would not have been possible without the outstanding work from the following communities:\n\n- [Apache Spark](https://spark.apache.org/): Unified Analytics Engine for Big Data, the underlying backend execution engine for .NET for Apache Spark\n- [Mobius](https://github.com/Microsoft/Mobius): C# and F# language binding and extensions to Apache Spark, a pre-cursor project to .NET for Apache Spark from the same Microsoft group.\n- [PySpark](https://spark.apache.org/docs/latest/api/python/index.html): Python bindings for Apache Spark, one of the implementations .NET for Apache Spark derives inspiration from. \n- [sparkR](https://spark.apache.org/docs/latest/sparkr.html): one of the implementations .NET for Apache Spark derives inspiration from.\n- [Apache Arrow](https://arrow.apache.org/): A cross-language development platform for in-memory data. This library provides .NET for Apache Spark with efficient ways to transfer column major data between the JVM and .NET CLR.\n- [Pyrolite](https://github.com/irmen/Pyrolite) - Java and .NET interface to Python's pickle and Pyro protocols. This library provides .NET for Apache Spark with efficient ways to transfer row major data between the JVM and .NET CLR. \n- [Databricks](https://databricks.com/): Unified analytics platform. Many thanks to all the suggestions from them towards making .NET for Apache Spark run on Azure and AWS Databricks.\n\n## How to Engage, Contribute and Provide Feedback\n\nThe .NET for Apache Spark team encourages [contributions](docs/contributing.md), both issues and PRs. The first step is finding an [existing issue](https://github.com/dotnet/spark/issues) you want to contribute to or if you cannot find any, [open an issue](https://github.com/dotnet/spark/issues?utf8=%E2%9C%93&q=is%3Aissue+is%3Aopen+).\n\n## Support\n\n[.NET for Apache Spark](https://github.com/dotnet/spark) is an open source project under the [.NET Foundation](https://dotnetfoundation.org/) and \ndoes not come with Microsoft Support unless otherwise noted by the specific product. For issues with or questions about .NET for Apache Spark, please [create an issue](https://github.com/dotnet/spark/issues). The community is active and is monitoring submissions.\n\n## .NET Foundation\n\nThe .NET for Apache Spark project is part of the [.NET Foundation](http://www.dotnetfoundation.org).\n\n## Code of Conduct\n\nThis project has adopted the code of conduct defined by the [Contributor Covenant](https://contributor-covenant.org/)\nto clarify expected behavior in our community.\nFor more information, see the [.NET Foundation Code of Conduct](https://dotnetfoundation.org/code-of-conduct).\n\n<a name=\"license\"></a>\n## License\n\n.NET for Apache Spark is licensed under the [MIT license](LICENSE).\n"
        },
        {
          "name": "ROADMAP.md",
          "type": "blob",
          "size": 1.9921875,
          "content": "# .NET for Apache Spark Roadmap\n\nThe goal of the .NET for Apache Spark project is to provide an easy to use, .NET-friendly integration to the popular big data platform, Apache Spark. This document describes the tentative plan for the project in the short and long-term. \n\n.NET for Apache Spark is a community effort and we welcome community feedback on our plans. The best way to give feedback is to open an issue in this repo. We are also excited to receive contributions (check out the [contribution guide](docs/contributing.md)). It's always a good idea to open an issue for a discussion before embarking on a large code change to make sure there is not duplicated effort. Where we do know that efforts are already underway, we have used the (*) marker below.\n\n## Short Term\n\n### User Experience\n* 1:1 API compatibility for Dataframes with Apache Spark 2.3.x, Apache Spark 2.4.x and Apache Spark 3.0.x (*)\n\n### Performance Optimizations\n* Improvements to C# Pickling Library\n* Improvements to Arrow .NET Library\n* Exploiting .NET Vectorization (*)\n* Micro-benchmarking framework for Interop\n\n### Benchmarks\n* Benchmarking scripts for all languages that include generating the dataset and running queries against it (*)\n* Published reproducible benchmarks against [TPC-H](http://www.tpc.org/tpch/) (industry-standard database benchmark) (*)\n\n### Tooling Improvements\n* VS Code support (*)\n* Apache Jupyter integration with C# & F# Notebook Support (*)\n* Improved user experience for .NET app submission to a remote Spark cluster\n\n## Longer Term\n\n### User Experience\n* Idiomatic C# and F# APIs\n\n### Performance Optimizations\n* Contribute extensible interop layer to Apache Spark\n\n### Benchmarks\n* Published reproducible benchmarks against [TPC-DS](http://www.tpc.org/tpcds/default.asp) (industry-standard database benchmark)\n\n### Tooling Improvements\n* Visual Studio Extension for .NET app submission to a remote Spark cluster\n* Visual Studio Extension for .NET app debugging\n* Make it easy to copy/paste Scala examples into Visual Studio\n"
        },
        {
          "name": "THIRD-PARTY-NOTICES.TXT",
          "type": "blob",
          "size": 0.369140625,
          "content": ".NET for Apache Spark uses third-party libraries or other resources that may be\ndistributed under licenses different than the .NET for Apache Spark software.\n\nIn the event that we accidentally failed to list a required notice, please\nbring it to our attention. Post an issue or email us:\n\n           dotnet@microsoft.com\n\nThe attached notices are provided for information only.\n"
        },
        {
          "name": "azure-pipelines-e2e-tests-template.yml",
          "type": "blob",
          "size": 9.763671875,
          "content": "parameters:\n- name: tests\n  type: object\n  default: {}\n- name: backwardCompatibleRelease\n  type: string\n  default: ''\n- name: forwardCompatibleRelease\n  type: string\n  default: ''\n\n\nstages:\n- ${{ each test in parameters.tests }}:\n  - stage: E2E_Tests_${{ replace(test.version, '.', '_') }}\n    displayName: E2E tests for Spark ${{ test.version }}\n    dependsOn: Build\n    jobs:\n    - ${{ each option in test.jobOptions }}:\n      - job: Run_${{ replace(option.pool, ' ', '_') }}\n        ${{ if eq(lower(option.pool), 'windows') }}:\n          pool:\n            name: Cosmos2MT-AzureDevOps-AgentPool\n            image: 1es-pt-windows-2019\n            os: windows\n        ${{ else }}:\n          pool:\n            name: Cosmos2MT-AzureDevOps-AgentPool\n            image: 1es-pt-ubuntu-22\n            os: linux\n\n        steps:\n        - task: JavaToolInstaller@0\n          condition: eq( variables['Agent.OS'], 'Linux')\n          displayName: '[ENV] Install JDK 8'\n          inputs:\n            versionSpec: '8'\n            jdkArchitectureOption: 'x64'\n            jdkSourceOption: 'PreInstalled'\n\n        - task: PowerShell@2\n          condition: eq( variables['Agent.OS'], 'Linux')\n          displayName: Install Maven for Linux agent\n          inputs:\n            workingDirectory: $(Build.BinariesDirectory)\n            pwsh: true\n            targetType: inline\n            script: |\n              $numRetry = 0\n              $maxRetry = 60*10\n\n              while($numRetry -le $maxRetry)\n              {\n                  sudo fuser -v /var/lib/dpkg/lock-frontend 2>&1\n\n                  if ($LASTEXITCODE -ne 0) { Break }\n                  \n                  sleep 1\n                  $numRetry++\n                  echo \"Waited $numRetry s for release of dpkg locks\"\n              }\n              \n              sudo apt update\n              sudo apt -y install maven\n              mvn -version\n\n        - task: UseDotNet@2\n          displayName: 'Use .NET 6 sdk'\n          inputs:\n            packageType: sdk\n            version: 6.x\n            installationPath: $(Agent.ToolsDirectory)/dotnet\n\n        - task: DownloadPipelineArtifact@2\n          displayName: Download Build Artifacts\n          inputs:\n            targetPath: $(Build.ArtifactStagingDirectory)/Microsoft.Spark.Binaries\n            artifactName: Microsoft.Spark.Binaries\n\n        - pwsh: |\n            $framework = \"net6.0\"\n\n            if ($env:AGENT_OS -eq 'Windows_NT') {\n              $runtimeIdentifier = \"win-x64\"\n            } else {\n              $runtimeIdentifier = \"linux-x64\"\n            }\n\n            $pathSeparator = [IO.Path]::DirectorySeparatorChar\n            $artifactPath = \"$(Build.ArtifactStagingDirectory)${pathSeparator}Microsoft.Spark.Binaries\"\n            echo \"##vso[task.setvariable variable=PATH_SEPARATOR]$pathSeparator\"\n            echo \"##vso[task.setvariable variable=ArtifactPath]$artifactPath\"\n\n            $backwardCompatibleRelease = \"${{ parameters.backwardCompatibleRelease }}\"\n            echo \"##vso[task.setvariable variable=BACKWARD_COMPATIBLE_DOTNET_WORKER_DIR]$(Build.BinariesDirectory)${pathSeparator}Microsoft.Spark.Worker-${backwardCompatibleRelease}\"\n            echo \"##vso[task.setvariable variable=BACKWARD_COMPATIBLE_WORKER_URL]https://github.com/dotnet/spark/releases/download/v${backwardCompatibleRelease}/Microsoft.Spark.Worker.${framework}.${runtimeIdentifier}-${backwardCompatibleRelease}.zip\"\n\n            $dotnetWorkerDir = \"${artifactPath}${pathSeparator}Microsoft.Spark.Worker${pathSeparator}${framework}${pathSeparator}${runtimeIdentifier}\"\n            echo \"##vso[task.setvariable variable=CURRENT_DOTNET_WORKER_DIR]$dotnetWorkerDir\"\n            if ($env:AGENT_OS -eq 'Linux') {\n              chmod +x \"${dotnetWorkerDir}${pathSeparator}Microsoft.Spark.Worker\"\n            }\n          displayName: 'Setup Variables and Permissions'\n\n        - checkout: self\n          path: s$(PATH_SEPARATOR)dotnet-spark\n\n        - task: CopyFiles@2\n          displayName: Copy jars\n          inputs:\n            sourceFolder: $(ArtifactPath)$(PATH_SEPARATOR)Jars\n            contents: '**$(PATH_SEPARATOR)*.jar'\n            targetFolder: $(Build.SourcesDirectory)$(PATH_SEPARATOR)dotnet-spark$(PATH_SEPARATOR)src$(PATH_SEPARATOR)scala\n\n        - task: PowerShell@2\n          condition: eq( variables['Agent.OS'], 'Windows_NT' )\n          displayName: Download Winutils.exe\n          inputs:\n            workingDirectory: $(Build.BinariesDirectory)\n            pwsh: true\n            targetType: inline\n            script: |\n              echo \"Download Hadoop utils for Windows.\"\n              $hadoopBinaryUrl = \"https://github.com/steveloughran/winutils/releases/download/tag_2017-08-29-hadoop-2.8.1-native/hadoop-2.8.1.zip\"\n              # Spark 3.3.0+ version binary uses Hadoop3 dependency\n              if ([version]\"3.3.0\" -le [version]\"${{ test.version }}\") {\n                $hadoopBinaryUrl = \"https://github.com/SparkSnail/winutils/releases/download/hadoop-3.3.5/hadoop-3.3.5.zip\"\n              }\n              curl -k -L -o hadoop.zip $hadoopBinaryUrl\n              Expand-Archive -Path hadoop.zip -Destination .\n              New-Item -ItemType Directory -Force -Path hadoop\\bin\n              if ([version]\"3.3.0\" -le [version]\"${{ test.version }}\") {\n                cp hadoop-3.3.5\\winutils.exe hadoop\\bin\n                # Hadoop 3.3 need to add hadoop.dll to environment varibles to avoid UnsatisfiedLinkError\n                cp hadoop-3.3.5\\hadoop.dll hadoop\\bin\n                cp hadoop-3.3.5\\hadoop.dll C:\\Windows\\System32\n                [System.Environment]::SetEnvironmentVariable(\"PATH\", $Env:Path + \";$(Build.BinariesDirectory)$(PATH_SEPARATOR)hadoop\", [System.EnvironmentVariableTarget]::Machine)\n              } else {\n                cp hadoop-2.8.1\\winutils.exe hadoop\\bin\n              }\n\n        - pwsh: |\n            echo \"Downloading Spark ${{ test.version }}\"\n            $sparkBinaryName = \"spark-${{ test.version }}-bin-hadoop2.7\"\n            # Spark 3.3.0+ uses Hadoop3\n            if ([version]\"3.3.0\" -le [version]\"${{ test.version }}\") {\n                $sparkBinaryName = \"spark-${{ test.version }}-bin-hadoop3\"\n            }\n            curl -k -L -o spark-${{ test.version }}.tgz https://archive.apache.org/dist/spark/spark-${{ test.version }}/${sparkBinaryName}.tgz\n            tar xzvf spark-${{ test.version }}.tgz\n            move $sparkBinaryName spark-${{ test.version }}-bin-hadoop\n          displayName: 'Download Spark Distro ${{ test.version }}'\n          workingDirectory: $(Build.BinariesDirectory)\n\n        - task: DotNetCoreCLI@2\n          displayName: 'E2E tests'\n          inputs:\n            command: test\n            projects: '**/Microsoft.Spark*.E2ETest/*.csproj'\n            arguments: '--configuration $(buildConfiguration) ${{ option.testOptions }}'\n            workingDirectory: $(Build.SourcesDirectory)$(PATH_SEPARATOR)dotnet-spark\n          env:\n            HADOOP_HOME: $(Build.BinariesDirectory)$(PATH_SEPARATOR)hadoop\n            SPARK_HOME: $(Build.BinariesDirectory)$(PATH_SEPARATOR)spark-${{ test.version }}-bin-hadoop\n            DOTNET_WORKER_DIR: $(CURRENT_DOTNET_WORKER_DIR)\n\n        - pwsh: |\n            echo \"Downloading ${env:BACKWARD_COMPATIBLE_WORKER_URL}\"\n            curl -k -L -o Microsoft.Spark.Worker-${{ parameters.backwardCompatibleRelease }}.zip ${env:BACKWARD_COMPATIBLE_WORKER_URL}\n            unzip Microsoft.Spark.Worker-${{ parameters.backwardCompatibleRelease }}.zip -d $([System.IO.Directory]::GetParent($env:BACKWARD_COMPATIBLE_DOTNET_WORKER_DIR).FullName)\n\n            if ($env:AGENT_OS -eq 'Linux') {\n              chmod +x \"${env:BACKWARD_COMPATIBLE_DOTNET_WORKER_DIR}${env:PATH_SEPARATOR}Microsoft.Spark.Worker\"\n            }\n          condition: ${{ test.enableBackwardCompatibleTests }}\n          displayName: 'Setup Backward Compatible Microsoft Spark Worker ${{ parameters.backwardCompatibleRelease }}'\n          workingDirectory: $(Build.BinariesDirectory)\n\n        - task: DotNetCoreCLI@2\n          displayName: 'E2E Backward Compatibility Tests'\n          condition: ${{ test.enableBackwardCompatibleTests }}\n          inputs:\n            command: test\n            projects: '**/Microsoft.Spark*.E2ETest/*.csproj'\n            arguments: '--configuration $(buildConfiguration) ${{ option.backwardCompatibleTestOptions }}'\n            workingDirectory: $(Build.SourcesDirectory)$(PATH_SEPARATOR)dotnet-spark\n          env:\n            HADOOP_HOME: $(Build.BinariesDirectory)$(PATH_SEPARATOR)hadoop\n            SPARK_HOME: $(Build.BinariesDirectory)$(PATH_SEPARATOR)spark-${{ test.version }}-bin-hadoop\n            DOTNET_WORKER_DIR: $(BACKWARD_COMPATIBLE_DOTNET_WORKER_DIR)\n\n        - checkout: forwardCompatibleRelease\n          path: s$(PATH_SEPARATOR)dotnet-spark-${{ parameters.forwardCompatibleRelease }}\n\n        - task: Maven@3\n          displayName: 'Maven build src for forward compatible release v${{ parameters.forwardCompatibleRelease }}'\n          condition: ${{ test.enableForwardCompatibleTests }}\n          inputs:\n            mavenPomFile: $(Build.SourcesDirectory)$(PATH_SEPARATOR)dotnet-spark-${{ parameters.forwardCompatibleRelease }}$(PATH_SEPARATOR)src$(PATH_SEPARATOR)scala$(PATH_SEPARATOR)pom.xml\n\n        - task: DotNetCoreCLI@2\n          displayName: 'E2E Forward Compatibility Tests'\n          condition: ${{ test.enableForwardCompatibleTests }}\n          inputs:\n            command: test\n            projects: '**/Microsoft.Spark*.E2ETest/*.csproj'\n            arguments: '--configuration $(buildConfiguration) ${{ option.forwardCompatibleTestOptions }}'\n            workingDirectory: $(Build.SourcesDirectory)$(PATH_SEPARATOR)dotnet-spark-${{ parameters.forwardCompatibleRelease }}\n          env:\n            HADOOP_HOME: $(Build.BinariesDirectory)$(PATH_SEPARATOR)hadoop\n            SPARK_HOME: $(Build.BinariesDirectory)$(PATH_SEPARATOR)spark-${{ test.version }}-bin-hadoop\n            DOTNET_WORKER_DIR: $(CURRENT_DOTNET_WORKER_DIR)\n"
        },
        {
          "name": "azure-pipelines-pr.yml",
          "type": "blob",
          "size": 6.0927734375,
          "content": "# Spark .NET build\n\ntrigger:\n  batch: true\n  branches:\n    include:\n    - main\n\nvariables:\n  buildConfiguration: 'Release'\n  _SignType: real\n  _TeamName: DotNetSpark\n  MSBUILDSINGLELOADCONTEXT: 1\n  ArtifactPath: '$(Build.ArtifactStagingDirectory)\\Microsoft.Spark.Binaries'\n\n  backwardCompatibleRelease: '2.0.0'\n  forwardCompatibleRelease: '2.0.0'\n\n  backwardCompatibleTestOptions_Windows_2_4: \"\"\n  forwardCompatibleTestOptions_Windows_2_4: \"\"\n  backwardCompatibleTestOptions_Linux_2_4: \"\"\n  forwardCompatibleTestOptions_Linux_2_4: \"\"\n\n  backwardCompatibleTestOptions_Windows_3_0: \"\"\n  forwardCompatibleTestOptions_Windows_3_0: \"\"\n  backwardCompatibleTestOptions_Linux_3_0: \"\"\n  forwardCompatibleTestOptions_Linux_3_0: \"\"\n\n  backwardCompatibleTestOptions_Windows_3_1: \"\"\n  forwardCompatibleTestOptions_Windows_3_1: \"\"\n  backwardCompatibleTestOptions_Linux_3_1: \"\"\n  forwardCompatibleTestOptions_Linux_3_1: \"\"\n\n  # Skip all forward/backward compatibility tests since Spark 3.2 and 3.5 are not supported before this release.\n  backwardCompatibleTestOptions_Windows_3_2: \"--filter FullyQualifiedName=NONE\"\n  forwardCompatibleTestOptions_Windows_3_2: $(backwardCompatibleTestOptions_Windows_3_2)\n  backwardCompatibleTestOptions_Linux_3_2: $(backwardCompatibleTestOptions_Windows_3_2)\n  forwardCompatibleTestOptions_Linux_3_2: $(backwardCompatibleTestOptions_Windows_3_2)\n\n  backwardCompatibleTestOptions_Windows_3_3: \"--filter FullyQualifiedName=NONE\"\n  forwardCompatibleTestOptions_Windows_3_3: $(backwardCompatibleTestOptions_Windows_3_3)\n  backwardCompatibleTestOptions_Linux_3_3: $(backwardCompatibleTestOptions_Windows_3_3)\n  forwardCompatibleTestOptions_Linux_3_3: $(backwardCompatibleTestOptions_Windows_3_3)\n  \n  backwardCompatibleTestOptions_Windows_3_5: \"--filter FullyQualifiedName=NONE\"\n  forwardCompatibleTestOptions_Windows_3_5: $(backwardCompatibleTestOptions_Windows_3_5)\n  backwardCompatibleTestOptions_Linux_3_5: $(backwardCompatibleTestOptions_Windows_3_5)\n  forwardCompatibleTestOptions_Linux_3_5: $(backwardCompatibleTestOptions_Windows_3_5)\n\n  # Azure DevOps variables are transformed into environment variables, with these variables we\n  # avoid the first time experience and telemetry to speed up the build.\n  DOTNET_CLI_TELEMETRY_OPTOUT: 1\n  DOTNET_SKIP_FIRST_TIME_EXPERIENCE: 1\n\nparameters:\n# List of Spark versions to run E2E tests\n- name: listOfE2ETestsSparkVersions\n  type: object\n  default:\n  - '2.4.0'\n  - '2.4.1'\n  - '2.4.3'\n  - '2.4.4'\n  - '2.4.5'\n  - '2.4.6'\n  - '2.4.7'\n  - '2.4.8'\n  - '3.0.0'\n  - '3.0.1'\n  - '3.0.2'\n  - '3.1.1'\n  - '3.1.2'\n  - '3.2.1'\n  - '3.2.2'\n  - '3.2.3'\n  - '3.3.0'\n  - '3.3.1'\n  - '3.3.2'\n  - '3.3.3'\n  - '3.3.4'\n  - '3.5.0'\n  - '3.5.1'\n  - '3.5.2'\n  - '3.5.3'\n\n# List of OS types to run E2E tests, run each test in both 'Windows' and 'Linux' environments\n- name: listOfE2ETestsPoolTypes\n  type: object\n  default:\n  - 'Windows'\n  - 'Linux'\n\nresources:\n  repositories:\n  - repository: forwardCompatibleRelease\n    type: github\n    endpoint: dotnet.spark\n    name: dotnet/spark\n    ref: refs/tags/v$(forwardCompatibleRelease)\n  \n  - repository: 1ESPipelineTemplates\n    type: git\n    name: 1ESPipelineTemplates/1ESPipelineTemplates\n    ref: refs/tags/release\n\nextends:\n  template: v1/1ES.Official.PipelineTemplate.yml@1ESPipelineTemplates\n  parameters:\n    settings:\n      skipBuildTagsForGitHubPullRequests: true\n    sdl:\n      spotBugs:\n        enabled: false\n      PSScriptAnalyzer:\n        enabled: false\n      credscan:\n        enabled: false\n      sourceRepositoriesToScan:\n        exclude:\n          - repository: forwardCompatibleRelease\n    pool:\n      name: Cosmos2MT-AzureDevOps-AgentPool\n      image: 1es-pt-windows-2019\n      os: windows\n    customBuildTags:\n      - ES365AIMigrationTooling\n          \n    stages:\n    - stage: Build\n      displayName: Build Sources\n      jobs:\n      - job: Build\n        templateContext:\n          outputs:\n            - output: pipelineArtifact\n              artifactName: Microsoft.Spark.Binaries\n              targetPath: $(Build.ArtifactStagingDirectory)\n              displayName: '[PUBLISH] Microsoft.Spark.Binaries'\n\n        steps:\n        - task: Maven@3\n          displayName: 'Maven build src'\n          inputs:\n            mavenPomFile: src/scala/pom.xml\n\n        - task: Maven@3\n          displayName: 'Maven build benchmark'\n          inputs:\n            mavenPomFile: benchmark/scala/pom.xml\n\n        - script: build.cmd -pack\n                    -c $(buildConfiguration)\n                    -ci\n                    $(_OfficialBuildIdArgs)\n                    /p:PublishSparkWorker=true\n                    /p:SparkWorkerPublishDir=$(Build.ArtifactStagingDirectory)\\Microsoft.Spark.Worker\n          displayName: '.NET build'\n\n        - task: DotNetCoreCLI@2\n          displayName: '.NET unit tests'\n          inputs:\n            command: test\n            projects: '**/*UnitTest/*.csproj'\n            arguments: '--configuration $(buildConfiguration)'\n\n        - task: CopyFiles@2\n          displayName: Stage Maven build jars\n          inputs:\n            sourceFolder: $(Build.SourcesDirectory)/src/scala\n            contents: '**/*.jar'\n            targetFolder: $(Build.ArtifactStagingDirectory)/Jars\n\n    - template: azure-pipelines-e2e-tests-template.yml\n      parameters:\n        backwardCompatibleRelease: $(backwardCompatibleRelease)\n        forwardCompatibleRelease: $(forwardCompatibleRelease)\n        tests:\n        - ${{ each version in parameters.listOfE2ETestsSparkVersions }} :\n          - version: ${{ version }}\n            # Set compatible test to false for all test currently,\n            # will need to refactor and update values in parameter if need to enalbe it per version.\n            enableForwardCompatibleTests: false\n            enableBackwardCompatibleTests: false\n            jobOptions:\n            - ${{ each pool in parameters.listOfE2ETestsPoolTypes }}:\n              - pool: ${{ pool }}\n                backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_${{ pool }}_${{ split(version, '.')[0] }}_${{ split(version, '.')[1] }})\n                forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_${{ pool }}_${{ split(version, '.')[0] }}_${{ split(version, '.')[1] }})\n"
        },
        {
          "name": "azure-pipelines.yml",
          "type": "blob",
          "size": 19.1650390625,
          "content": "# Spark .NET build\n\ntrigger:\n  batch: true\n  branches:\n    include:\n    - main\n\nvariables:\n  buildConfiguration: 'Release'\n  _SignType: real\n  _TeamName: DotNetSpark\n  MSBUILDSINGLELOADCONTEXT: 1\n  ArtifactPath: '$(Build.ArtifactStagingDirectory)\\Microsoft.Spark.Binaries'\n\n  backwardCompatibleRelease: '2.0.0'\n  forwardCompatibleRelease: '2.0.0'\n\n  backwardCompatibleTestOptions_Windows_2_4: \"\"\n  forwardCompatibleTestOptions_Windows_2_4: \"\"\n  backwardCompatibleTestOptions_Linux_2_4: \"\"\n  forwardCompatibleTestOptions_Linux_2_4: \"\"\n\n  backwardCompatibleTestOptions_Windows_3_0: \"\"\n  forwardCompatibleTestOptions_Windows_3_0: \"\"\n  backwardCompatibleTestOptions_Linux_3_0: \"\"\n  forwardCompatibleTestOptions_Linux_3_0: \"\"\n\n  backwardCompatibleTestOptions_Windows_3_1: \"\"\n  forwardCompatibleTestOptions_Windows_3_1: \"\"\n  backwardCompatibleTestOptions_Linux_3_1: \"\"\n  forwardCompatibleTestOptions_Linux_3_1: \"\"\n\n  # Skip all forward/backward compatibility tests since Spark 3.2 and 3.5 are not supported before this release.\n  backwardCompatibleTestOptions_Windows_3_2: \"--filter FullyQualifiedName=NONE\"\n  forwardCompatibleTestOptions_Windows_3_2: $(backwardCompatibleTestOptions_Windows_3_2)\n  backwardCompatibleTestOptions_Linux_3_2: $(backwardCompatibleTestOptions_Windows_3_2)\n  forwardCompatibleTestOptions_Linux_3_2: $(backwardCompatibleTestOptions_Windows_3_2)\n\n  backwardCompatibleTestOptions_Windows_3_5: \"--filter FullyQualifiedName=NONE\"\n  forwardCompatibleTestOptions_Windows_3_5: $(backwardCompatibleTestOptions_Windows_3_5)\n  backwardCompatibleTestOptions_Linux_3_5: $(backwardCompatibleTestOptions_Windows_3_5)\n  forwardCompatibleTestOptions_Linux_3_5: $(backwardCompatibleTestOptions_Windows_3_5)\n\n  # Azure DevOps variables are transformed into environment variables, with these variables we\n  # avoid the first time experience and telemetry to speed up the build.\n  DOTNET_CLI_TELEMETRY_OPTOUT: 1\n  DOTNET_SKIP_FIRST_TIME_EXPERIENCE: 1\n\nresources:\n  repositories:\n  - repository: forwardCompatibleRelease\n    type: github\n    endpoint: public\n    name: dotnet/spark\n    ref: refs/tags/v$(forwardCompatibleRelease)\n\nstages:\n- stage: Build\n  displayName: Build Sources\n  jobs:\n  - job: Build\n    pool: \n      vmImage: 'windows-2022'\n\n    variables:\n      ${{ if and(ne(variables['System.TeamProject'], 'public'), notin(variables['Build.Reason'], 'PullRequest')) }}:\n        _OfficialBuildIdArgs: /p:OfficialBuildId=$(BUILD.BUILDNUMBER)\n\n    steps:\n    - task: Maven@3\n      displayName: 'Maven build src'\n      inputs:\n        mavenPomFile: src/scala/pom.xml\n\n    - task: Maven@3\n      displayName: 'Maven build benchmark'\n      inputs:\n        mavenPomFile: benchmark/scala/pom.xml\n\n    - script: build.cmd -pack\n                -c $(buildConfiguration)\n                -ci\n                $(_OfficialBuildIdArgs)\n                /p:PublishSparkWorker=true\n                /p:SparkWorkerPublishDir=$(Build.ArtifactStagingDirectory)\\Microsoft.Spark.Worker\n      displayName: '.NET build'\n\n    - task: DotNetCoreCLI@2\n      displayName: '.NET unit tests'\n      inputs:\n        command: test\n        projects: '**/*UnitTest/*.csproj'\n        arguments: '--configuration $(buildConfiguration)'\n\n    - task: CopyFiles@2\n      displayName: Stage Maven build jars\n      inputs:\n        sourceFolder: $(Build.SourcesDirectory)/src/scala\n        contents: '**/*.jar'\n        targetFolder: $(Build.ArtifactStagingDirectory)/Jars\n\n    - ${{ if and(ne(variables['System.TeamProject'], 'public'), notin(variables['Build.Reason'], 'PullRequest')) }}:\n      - task: CopyFiles@2\n        displayName: Stage .NET artifacts\n        inputs:\n          sourceFolder: $(Build.SourcesDirectory)/artifacts/packages/$(buildConfiguration)/Shipping\n          contents: |\n            **/*.nupkg\n            **/*.snupkg\n          targetFolder: $(Build.ArtifactStagingDirectory)/BuildArtifacts/artifacts/packages/$(buildConfiguration)/Shipping\n\n      - task: CopyFiles@2\n        displayName: Stage build logs\n        inputs:\n          sourceFolder: $(Build.SourcesDirectory)/artifacts/log\n          targetFolder: $(Build.ArtifactStagingDirectory)/BuildArtifacts/artifacts/log\n\n    - task: PublishBuildArtifacts@1\n      inputs:\n        pathtoPublish: '$(Build.ArtifactStagingDirectory)'\n        artifactName:  Microsoft.Spark.Binaries\n\n  - ${{ if and(ne(variables['System.TeamProject'], 'public'), notin(variables['Build.Reason'], 'PullRequest')) }}:\n    - job: Sign\n      dependsOn:\n        - Build\n      displayName: Sign Artifacts\n      pool:\n        name: NetCore1ESPool-Internal\n        demands: ImageOverride -equals build.windows.10.amd64.vs2019\n\n      variables:\n        ${{ if and(ne(variables['System.TeamProject'], 'public'), notin(variables['Build.Reason'], 'PullRequest')) }}:\n          _OfficialBuildIdArgs: /p:OfficialBuildId=$(BUILD.BUILDNUMBER)\n\n      steps:\n      - task: DownloadBuildArtifacts@0\n        displayName: Download Build Artifacts\n        inputs:\n          artifactName: Microsoft.Spark.Binaries\n          downloadPath: $(Build.ArtifactStagingDirectory)\n\n      - task: MicroBuildSigningPlugin@2\n        displayName: Install MicroBuild plugin\n        inputs:\n          signType: $(_SignType)\n          zipSources: false\n          feedSource: https://dnceng.pkgs.visualstudio.com/_packaging/MicroBuildToolset/nuget/v3/index.json\n        env:\n          TeamName: $(_TeamName)\n        condition: and(succeeded(), in(variables['_SignType'], 'real', 'test'), eq(variables['Agent.Os'], 'Windows_NT'))\n\n      - task: PowerShell@2\n        displayName: Sign artifacts\n        inputs:\n          filePath: eng\\common\\build.ps1\n          arguments: -restore -sign\n                     -c $(buildConfiguration)\n                     -ci\n                     $(_OfficialBuildIdArgs)\n                     /p:DotNetSignType=$(_SignType)\n                     /p:SparkPackagesDir=$(ArtifactPath)\\BuildArtifacts\\artifacts\\packages\n                     /p:SparkWorkerPublishDir=$(ArtifactPath)\\Microsoft.Spark.Worker\n\n      - task: PublishBuildArtifacts@1\n        inputs:\n          pathtoPublish: '$(ArtifactPath)'\n          artifactName:  Microsoft.Spark.Binaries\n\n  # The \"Publish\" stage is separated out from the \"Sign\" stage because we need to install powershell module\n  # to zip files correctly for macOS; installing the module is not allowed in NetCoreInternal-Pool.\n  - ${{ if and(ne(variables['System.TeamProject'], 'public'), notin(variables['Build.Reason'], 'PullRequest')) }}:\n    - job: Publish\n      dependsOn:\n        - Sign\n      displayName: Publish Artifacts\n      pool: \n        vmImage: 'windows-2022'\n\n      variables:\n        ${{ if and(ne(variables['System.TeamProject'], 'public'), notin(variables['Build.Reason'], 'PullRequest')) }}:\n          _OfficialBuildIdArgs: /p:OfficialBuildId=$(BUILD.BUILDNUMBER)\n\n      steps:\n      # The following module needs to be installed to zip files correctly for macOS.\n      - powershell: Install-Module -Name Microsoft.PowerShell.Archive -Scope CurrentUser -Force -AllowClobber -Verbose -MinimumVersion 1.2.5\n        displayName: Install Microsoft.PowerShell.Archive\n\n      - task: DownloadBuildArtifacts@0\n        displayName: Download Signed Artifacts\n        inputs:\n          artifactName: Microsoft.Spark.Binaries\n          downloadPath: $(Build.ArtifactStagingDirectory)\n\n      - task: PowerShell@2\n        displayName: Package Microsoft.Spark.Worker\n        inputs:\n          filePath: eng\\common\\build.ps1\n          arguments: -restore -publish\n                     -c $(buildConfiguration)\n                     -ci\n                     $(_OfficialBuildIdArgs)\n                     /p:SparkWorkerPublishDir=$(ArtifactPath)\\Microsoft.Spark.Worker\n                     /p:SparkWorkerPackageOutputDir=$(ArtifactPath)\n\n      - task: PublishBuildArtifacts@1\n        inputs:\n          pathtoPublish: '$(ArtifactPath)'\n          artifactName:  Microsoft.Spark.Binaries\n\n- template: azure-pipelines-e2e-tests-template.yml\n  parameters:\n    backwardCompatibleRelease: $(backwardCompatibleRelease)\n    forwardCompatibleRelease: $(forwardCompatibleRelease)\n    tests:\n    - version: '2.4.0'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_2_4)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_2_4)\n    - version: '2.4.1'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_2_4)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_2_4)\n    - version: '2.4.3'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_2_4)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_2_4)\n    - version: '2.4.4'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_2_4)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_2_4)\n    - version: '2.4.5'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_2_4)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_2_4)\n    - version: '2.4.6'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_2_4)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_2_4)\n    - version: '2.4.7'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_2_4)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_2_4)\n    - version: '2.4.8'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_2_4)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_2_4)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_2_4)\n    - version: '3.0.0'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_0)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_0)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_0)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_0)\n    - version: '3.0.1'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_0)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_0)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_0)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_0)\n    - version: '3.0.2'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_0)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_0)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_0)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_0)\n    - version: '3.1.1'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_1)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_1)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_1)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_1)\n    - version: '3.1.2'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_1)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_1)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_1)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_1)\n    - version: '3.2.0'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_2)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_2)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_2)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_2)\n    - version: '3.2.1'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_2)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_2)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_2)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_2)\n    - version: '3.2.2'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_2)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_2)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_2)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_2)\n    - version: '3.2.3'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_2)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_2)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_2)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_2)\n    - version: '3.5.0'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_5)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_5)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_5)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_5)\n    - version: '3.5.1'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_5)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_5)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_5)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_5)\n    - version: '3.5.2'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_5)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_5)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_5)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_5)\n    - version: '3.5.3'\n      enableForwardCompatibleTests: false\n      enableBackwardCompatibleTests: false\n      jobOptions:\n      - pool: 'Windows'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Windows_3_5)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Windows_3_5)\n      - pool: 'Linux'\n        testOptions: \"\"\n        backwardCompatibleTestOptions: $(backwardCompatibleTestOptions_Linux_3_5)\n        forwardCompatibleTestOptions: $(forwardCompatibleTestOptions_Linux_3_5)"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.cmd",
          "type": "blob",
          "size": 0.1376953125,
          "content": "@echo off\npowershell -ExecutionPolicy ByPass -NoProfile -command \"& \"\"\"%~dp0eng\\common\\Build.ps1\"\"\" -restore -build %*\"\nexit /b %ErrorLevel%\n"
        },
        {
          "name": "build.sh",
          "type": "blob",
          "size": 0.50390625,
          "content": "#!/usr/bin/env bash\n\nsource=\"${BASH_SOURCE[0]}\"\n\n# resolve $SOURCE until the file is no longer a symlink\nwhile [[ -h $source ]]; do\n  scriptroot=\"$( cd -P \"$( dirname \"$source\" )\" && pwd )\"\n  source=\"$(readlink \"$source\")\"\n\n  # if $source was a relative symlink, we need to resolve it relative to the path where the\n  # symlink file was located\n  [[ $source != /* ]] && source=\"$scriptroot/$source\"\ndone\n\nscriptroot=\"$( cd -P \"$( dirname \"$source\" )\" && pwd )\"\n\"$scriptroot/eng/common/build.sh\" --build --restore $@\n"
        },
        {
          "name": "deployment",
          "type": "tree",
          "content": null
        },
        {
          "name": "dev",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "eng",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "global.json",
          "type": "blob",
          "size": 0.123046875,
          "content": "{\n  \"tools\": {\n    \"dotnet\": \"6.0.400\"\n  },\n  \"msbuild-sdks\": {\n    \"Microsoft.DotNet.Arcade.Sdk\": \"1.0.0-beta.20230.5\"\n  }\n}\n"
        },
        {
          "name": "script",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}