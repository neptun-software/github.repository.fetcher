{
  "metadata": {
    "timestamp": 1736710879263,
    "page": 448,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "sjdirect/abot",
      "stars": 2262,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.08984375,
          "content": "Abot.Tests.Unit/*.html linguist-documentation\nAbot2.Tests.Unit/*.html linguist-documentation"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.6787109375,
          "content": "# Build Folders (you can keep bin if you'd like, to store dlls and pdbs)\n[Bb]in/\n[Oo]bj/\n\n# mstest test results\nTestResults\n\n## Ignore Visual Studio temporary files, build results, and\n## files generated by popular Visual Studio add-ons.\n\n# User-specific files\n*.suo\n*.user\n*.sln.docstates\n\n# Build results\n[Dd]ebug/\n[Rr]elease/\nx64/\n*_i.c\n*_p.c\n*.ilk\n*.meta\n*.obj\n*.pch\n*.pdb\n*.pgc\n*.pgd\n# *.rsp # Needed for Appveyor to pass msbuild parameters\n*.sbr\n*.tlb\n*.tli\n*.tlh\n*.tmp\n*.log\n*.vspscc\n*.vssscc\n.builds\n\n# Visual C++ cache files\nipch/\n*.aps\n*.ncb\n*.opensdf\n*.sdf\n\n# Visual Studio profiler\n*.psess\n*.vsp\n*.vspx\n\n# Guidance Automation Toolkit\n*.gpState\n\n# ReSharper is a .NET coding add-in\n_ReSharper*\n\n# NCrunch\n*.ncrunch*\n.*crunch*.local.xml\n\n# Installshield output folder \n[Ee]xpress\n\n# DocProject is a documentation generator add-in\nDocProject/buildhelp/\nDocProject/Help/*.HxT\nDocProject/Help/*.HxC\nDocProject/Help/*.hhc\nDocProject/Help/*.hhk\nDocProject/Help/*.hhp\nDocProject/Help/Html2\nDocProject/Help/html\n\n# Click-Once directory\npublish\n\n# Publish Web Output\n*.Publish.xml\n\n# NuGet Packages Directory\npackages\n\n# Windows Azure Build Output\ncsx\n*.build.csdef\n\n# Windows Store app package directory\nAppPackages/\n\n# Others\n[Bb]in\n[Oo]bj\nsql\nTestResults\n[Tt]est[Rr]esult*\n*.Cache\nClientBin\n[Ss]tyle[Cc]op.*\n~$*\n*.dbmdl\nGenerated_Code #added for RIA/Silverlight projects\n\n# Backup & report files from converting an old project file to a newer\n# Visual Studio version. Backup files are not needed, because we have git ;-)\n_UpgradeReport_Files/\nBackup*/\nUpgradeLog*.XML\nSettings.StyleCop\n.vs/config/applicationhost.config\n/.vs/Abot/v15\n\n\\.vs/Abot/DesignTimeBuild/\n\n\\.vs/\n/.idea\n/Abot2/Properties/launchSettings.json\n"
        },
        {
          "name": ".nuget",
          "type": "tree",
          "content": null
        },
        {
          "name": "Abot.sln",
          "type": "blob",
          "size": 4.6494140625,
          "content": "ï»¿\nMicrosoft Visual Studio Solution File, Format Version 12.00\n# Visual Studio Version 16\nVisualStudioVersion = 16.0.29215.179\nMinimumVisualStudioVersion = 10.0.40219.1\nProject(\"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\") = \".nuget\", \".nuget\", \"{2E5344D9-2733-4C6E-8B95-1A7715D23B61}\"\n\tProjectSection(SolutionItems) = preProject\n\t\t.nuget\\NuGet.Config = .nuget\\NuGet.Config\n\t\t.nuget\\NuGet.exe = .nuget\\NuGet.exe\n\t\t.nuget\\NuGet.targets = .nuget\\NuGet.targets\n\tEndProjectSection\nEndProject\nProject(\"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\") = \"Solution Items\", \"Solution Items\", \"{2BA10448-7631-4E76-8673-0AEE6D1525DB}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Abot2\", \"Abot2\\Abot2.csproj\", \"{A1CB064A-18C6-4964-AC23-93426CCB2288}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Abot2.Tests.Unit\", \"Abot2.Tests.Unit\\Abot2.Tests.Unit.csproj\", \"{C97EACF1-D791-438D-AA05-CB60DCCF9310}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Abot2.Demo\", \"Abot2.Demo\\Abot2.Demo.csproj\", \"{4324C969-FCD5-467C-BBD1-DA6BBCF98767}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Abot2.Tests.Integration\", \"Abot2.Tests.Integration\\Abot2.Tests.Integration.csproj\", \"{54A40E42-CF28-4A13-8268-49D48920280D}\"\nEndProject\nGlobal\n\tGlobalSection(SolutionConfigurationPlatforms) = preSolution\n\t\tDebug|Any CPU = Debug|Any CPU\n\t\tDeploy-Dev|Any CPU = Deploy-Dev|Any CPU\n\t\tDeploy-Release|Any CPU = Deploy-Release|Any CPU\n\t\tRelease|Any CPU = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(ProjectConfigurationPlatforms) = postSolution\n\t\t{A1CB064A-18C6-4964-AC23-93426CCB2288}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{A1CB064A-18C6-4964-AC23-93426CCB2288}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{A1CB064A-18C6-4964-AC23-93426CCB2288}.Deploy-Dev|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{A1CB064A-18C6-4964-AC23-93426CCB2288}.Deploy-Dev|Any CPU.Build.0 = Debug|Any CPU\n\t\t{A1CB064A-18C6-4964-AC23-93426CCB2288}.Deploy-Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{A1CB064A-18C6-4964-AC23-93426CCB2288}.Deploy-Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{A1CB064A-18C6-4964-AC23-93426CCB2288}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{A1CB064A-18C6-4964-AC23-93426CCB2288}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{C97EACF1-D791-438D-AA05-CB60DCCF9310}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{C97EACF1-D791-438D-AA05-CB60DCCF9310}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{C97EACF1-D791-438D-AA05-CB60DCCF9310}.Deploy-Dev|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{C97EACF1-D791-438D-AA05-CB60DCCF9310}.Deploy-Dev|Any CPU.Build.0 = Debug|Any CPU\n\t\t{C97EACF1-D791-438D-AA05-CB60DCCF9310}.Deploy-Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{C97EACF1-D791-438D-AA05-CB60DCCF9310}.Deploy-Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{C97EACF1-D791-438D-AA05-CB60DCCF9310}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{C97EACF1-D791-438D-AA05-CB60DCCF9310}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{4324C969-FCD5-467C-BBD1-DA6BBCF98767}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{4324C969-FCD5-467C-BBD1-DA6BBCF98767}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{4324C969-FCD5-467C-BBD1-DA6BBCF98767}.Deploy-Dev|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{4324C969-FCD5-467C-BBD1-DA6BBCF98767}.Deploy-Dev|Any CPU.Build.0 = Debug|Any CPU\n\t\t{4324C969-FCD5-467C-BBD1-DA6BBCF98767}.Deploy-Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{4324C969-FCD5-467C-BBD1-DA6BBCF98767}.Deploy-Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{4324C969-FCD5-467C-BBD1-DA6BBCF98767}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{4324C969-FCD5-467C-BBD1-DA6BBCF98767}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{54A40E42-CF28-4A13-8268-49D48920280D}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{54A40E42-CF28-4A13-8268-49D48920280D}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{54A40E42-CF28-4A13-8268-49D48920280D}.Deploy-Dev|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{54A40E42-CF28-4A13-8268-49D48920280D}.Deploy-Dev|Any CPU.Build.0 = Debug|Any CPU\n\t\t{54A40E42-CF28-4A13-8268-49D48920280D}.Deploy-Release|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{54A40E42-CF28-4A13-8268-49D48920280D}.Deploy-Release|Any CPU.Build.0 = Debug|Any CPU\n\t\t{54A40E42-CF28-4A13-8268-49D48920280D}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{54A40E42-CF28-4A13-8268-49D48920280D}.Release|Any CPU.Build.0 = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(SolutionProperties) = preSolution\n\t\tHideSolutionNode = FALSE\n\tEndGlobalSection\n\tGlobalSection(ExtensibilityGlobals) = postSolution\n\t\tEnterpriseLibraryConfigurationToolBinariesPath = packages\\EnterpriseLibrary.Common.5.0.505.0\\lib\\NET35;packages\\Unity.Interception.2.1.505.0\\lib\\NET35;packages\\Unity.2.1.505.0\\lib\\NET35;packages\\Unity.2.1.505.2\\lib\\NET35\n\t\tSolutionGuid = {CE61BA35-B8AD-494A-B9AA-BC177A23EFDA}\n\tEndGlobalSection\nEndGlobal\n"
        },
        {
          "name": "Abot2.Demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "Abot2.Tests.Integration",
          "type": "tree",
          "content": null
        },
        {
          "name": "Abot2.Tests.Unit",
          "type": "tree",
          "content": null
        },
        {
          "name": "Abot2",
          "type": "tree",
          "content": null
        },
        {
          "name": "License.txt",
          "type": "blob",
          "size": 8.9267578125,
          "content": "Apache License\n\nVersion 2.0, January 2004\n\nhttp://www.apache.org/licenses/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION \n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\n\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\n\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\n\nIf the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License. You may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\n\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.1923828125,
          "content": "# Abot [![Build Status](https://ci.appveyor.com/api/projects/status/nr84t6dpneg5tmb6?svg=true)](https://ci.appveyor.com/project/sjdirect/abot2) [![NuGet](https://img.shields.io/nuget/v/Abot.svg)](https://www.nuget.org/packages/Abot/)\n\n*Please star this project!!*\n\n###### C# web crawler built for speed and flexibility.\n\nAbot is an open source C# web crawler framework built for speed and flexibility. It takes care of the low level plumbing (multithreading, http requests, scheduling, link parsing, etc..). You just register for events to process the page data. You can also plugin your own implementations of core interfaces to take complete control over the crawl process. Abot Nuget package version >= 2.0 targets Dotnet Standard 2.0 and Abot Nuget package version < 2.0 targets .NET version 4.0 which makes it highly compatible with many .net framework/core implementations.\n\n###### What's So Great About It?\n  * Open Source (Free for commercial and personal use)\n  * It's fast, really fast!!\n  * Easily customizable (Pluggable architecture allows you to decide what gets crawled and how)\n  * Heavily unit tested (High code coverage)\n  * Very lightweight (not over engineered)\n  * No out of process dependencies (no databases, no installed services, etc...)\n\n###### Links of Interest\n\n  * [Ask a question](http://groups.google.com/group/abot-web-crawler), please search for similar questions first!!!\n  * [Report a bug](https://github.com/sjdirect/abot/issues)\n  * [Learn how you can contribute](https://github.com/sjdirect/abot/wiki/Contribute)\n  * [Need expert Abot customization?](https://github.com/sjdirect/abot/wiki/Custom-Development)\n  * [Take the usage survey](https://www.surveymonkey.com/s/JS5826F) to help prioritize features/improvements\n  * [Consider making a donation](https://www.paypal.com/cgi-bin/webscr?cmd=_s-xclick&hosted_button_id=G6ZY6BZNBFVQJ)\n\n###### Use [AbotX](https://github.com/sjdirect/abotx/blob/master/README.md) for more powerful extensions/wrappers\n\n  * [Crawl multiple sites concurrently](https://github.com/sjdirect/abotx/blob/master/README.md#parallel-crawler-engine)\n  * [Execute/Render Javascript](https://github.com/sjdirect/abotx/blob/master/README.md#javascript-rendering)\n  * [Avoid getting blocked by sites](https://github.com/sjdirect/abotx/blob/master/README.md#auto-throttling)\n  * [Auto Tuning](https://github.com/sjdirect/abotx/blob/master/README.md#auto-tuning)\n  * [Auto Throttling](https://github.com/sjdirect/abotx/blob/master/README.md#auto-throttling)\n  * [Pause/Resume live crawls](https://github.com/sjdirect/abotx/blob/master/README.md#pause-and-resume)\n  * [Simplified pluggability/extensibility](https://github.com/sjdirect/abotx/blob/master/README.md#easy-override)\n\n<br /><br />\n<hr />\n\n## Quick Start \n\n###### Installing Abot\n  * Install Abot using [Nuget](https://www.nuget.org/packages/Abot/)\n  \n```command\nPM> Install-Package Abot\n```\n\n###### Using Abot \n```c#\nusing System;\nusing System.Threading.Tasks;\nusing Abot2.Core;\nusing Abot2.Crawler;\nusing Abot2.Poco;\nusing Serilog;\n\nnamespace TestAbotUse\n{\n    class Program\n    {\n        static async Task Main(string[] args)\n        {\n            Log.Logger = new LoggerConfiguration()\n                .MinimumLevel.Information()\n                .WriteTo.Console()\n                .CreateLogger();\n\n            Log.Logger.Information(\"Demo starting up!\");\n\n            await DemoSimpleCrawler();\n            await DemoSinglePageRequest();\n        }\n\n        private static async Task DemoSimpleCrawler()\n        {\n            var config = new CrawlConfiguration\n            {\n                MaxPagesToCrawl = 10, //Only crawl 10 pages\n                MinCrawlDelayPerDomainMilliSeconds = 3000 //Wait this many millisecs between requests\n            };\n            var crawler = new PoliteWebCrawler(config);\n\n            crawler.PageCrawlCompleted += PageCrawlCompleted;//Several events available...\n\n            var crawlResult = await crawler.CrawlAsync(new Uri(\"http://!!!!!!!!YOURSITEHERE!!!!!!!!!.com\"));\n        }\n\n        private static async Task DemoSinglePageRequest()\n        {\n            var pageRequester = new PageRequester(new CrawlConfiguration(), new WebContentExtractor());\n\n            var crawledPage = await pageRequester.MakeRequestAsync(new Uri(\"http://google.com\"));\n            Log.Logger.Information(\"{result}\", new\n            {\n                url = crawledPage.Uri,\n                status = Convert.ToInt32(crawledPage.HttpResponseMessage.StatusCode)\n            });\n        }\n\n        private static void PageCrawlCompleted(object sender, PageCrawlCompletedArgs e)\n        {\n            var httpStatus = e.CrawledPage.HttpResponseMessage.StatusCode;\n            var rawPageText = e.CrawledPage.Content.Text;\n        }\n    }\n}\n\n```\n\n## Abot Configuration\nAbot's Abot2.Poco.CrawlConfiguration class has a ton of configuration options. You can see what effect each config value has on the crawl by looking at the [code comments ](https://github.com/sjdirect/abot/blob/master/Abot2/Poco/CrawlConfiguration.cs).\n    \n```c#\nvar crawlConfig = new CrawlConfiguration();\ncrawlConfig.CrawlTimeoutSeconds = 100;\ncrawlConfig.MaxConcurrentThreads = 10;\ncrawlConfig.MaxPagesToCrawl = 1000;\ncrawlConfig.UserAgentString = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36\";\ncrawlConfig.ConfigurationExtensions.Add(\"SomeCustomConfigValue1\", \"1111\");\ncrawlConfig.ConfigurationExtensions.Add(\"SomeCustomConfigValue2\", \"2222\");\netc...\n```\n\n## Abot Events\nRegister for events and create processing methods\n```c#\ncrawler.PageCrawlStarting += crawler_ProcessPageCrawlStarting;\ncrawler.PageCrawlCompleted += crawler_ProcessPageCrawlCompleted;\ncrawler.PageCrawlDisallowed += crawler_PageCrawlDisallowed;\ncrawler.PageLinksCrawlDisallowed += crawler_PageLinksCrawlDisallowed;\n```\n```c#\nvoid crawler_ProcessPageCrawlStarting(object sender, PageCrawlStartingArgs e)\n{\n\tPageToCrawl pageToCrawl = e.PageToCrawl;\n\tConsole.WriteLine($\"About to crawl link {pageToCrawl.Uri.AbsoluteUri} which was found on page {pageToCrawl.ParentUri.AbsoluteUri}\");\n}\n\nvoid crawler_ProcessPageCrawlCompleted(object sender, PageCrawlCompletedArgs e)\n{\n\tCrawledPage crawledPage = e.CrawledPage;\t\n\tif (crawledPage.HttpRequestException != null || crawledPage.HttpResponseMessage.StatusCode != HttpStatusCode.OK)\n\t\tConsole.WriteLine($\"Crawl of page failed {crawledPage.Uri.AbsoluteUri}\");\n\telse\n\t\tConsole.WriteLine($\"Crawl of page succeeded {crawledPage.Uri.AbsoluteUri}\");\n\n\tif (string.IsNullOrEmpty(crawledPage.Content.Text))\n\t\tConsole.WriteLine($\"Page had no content {crawledPage.Uri.AbsoluteUri}\");\n\n\tvar angleSharpHtmlDocument = crawledPage.AngleSharpHtmlDocument; //AngleSharp parser\n}\n\nvoid crawler_PageLinksCrawlDisallowed(object sender, PageLinksCrawlDisallowedArgs e)\n{\n\tCrawledPage crawledPage = e.CrawledPage;\n\tConsole.WriteLine($\"Did not crawl the links on page {crawledPage.Uri.AbsoluteUri} due to {e.DisallowedReason}\");\n}\n\nvoid crawler_PageCrawlDisallowed(object sender, PageCrawlDisallowedArgs e)\n{\n\tPageToCrawl pageToCrawl = e.PageToCrawl;\n\tConsole.WriteLine($\"Did not crawl page {pageToCrawl.Uri.AbsoluteUri} due to {e.DisallowedReason}\");\n}\n```\n\n## Custom objects and the dynamic crawl bag\nAdd any number of custom objects to the dynamic crawl bag or page bag. These objects will be available in the CrawlContext.CrawlBag object, PageToCrawl.PageBag object or CrawledPage.PageBag object.\n```c#\nvar crawler crawler = new PoliteWebCrawler();\ncrawler.CrawlBag.MyFoo1 = new Foo();\ncrawler.CrawlBag.MyFoo2 = new Foo();\ncrawler.PageCrawlStarting += crawler_ProcessPageCrawlStarting;\n...\n```\n```c#\nvoid crawler_ProcessPageCrawlStarting(object sender, PageCrawlStartingArgs e)\n{\n    //Get your Foo instances from the CrawlContext object\n    var foo1 = e.CrawlConext.CrawlBag.MyFoo1;\n    var foo2 = e.CrawlConext.CrawlBag.MyFoo2;\n\n    //Also add a dynamic value to the PageToCrawl or CrawledPage\n    e.PageToCrawl.PageBag.Bar = new Bar();\n}\n```\n\n## Cancellation\n```c#\nCancellationTokenSource cancellationTokenSource = new CancellationTokenSource();\n\nvar crawler = new PoliteWebCrawler();\nvar result = await crawler.CrawlAsync(new Uri(\"addurihere\"), cancellationTokenSource);\n```\n\n<br /><br /><br />\n<hr />\n\n## Customizing Crawl Behavior\n\nAbot was designed to be as pluggable as possible. This allows you to easily alter the way it works to suite your needs.\n\nThe easiest way to change Abot's behavior for common features is to change the config values that control them. See the [Quick Start](#quick-start) page for examples on the different ways Abot can be configured.\n\n#### CrawlDecision Callbacks/Delegates\nSometimes you don't want to create a class and go through the ceremony of extending a base class or implementing the interface directly. For all you lazy developers out there Abot provides a shorthand method to easily add your custom crawl decision logic. NOTE: The ICrawlDecisionMaker's corresponding method is called first and if it does not \"allow\" a decision, these callbacks will not be called.\n\n```c#\nvar crawler = new PoliteWebCrawler();\n\ncrawler.ShouldCrawlPageDecisionMaker = (pageToCrawl, crawlContext) => \n{\n\tvar decision = new CrawlDecision{ Allow = true };\n\tif(pageToCrawl.Uri.Authority == \"google.com\")\n\t\treturn new CrawlDecision{ Allow = false, Reason = \"Dont want to crawl google pages\" };\n\t\n\treturn decision;\n};\n\ncrawler.ShouldDownloadPageContentDecisionMaker = (crawledPage, crawlContext) =>\n{\n\tvar decision = new CrawlDecision{ Allow = true };\n\tif (!crawledPage.Uri.AbsoluteUri.Contains(\".com\"))\n\t\treturn new CrawlDecision { Allow = false, Reason = \"Only download raw page content for .com tlds\" };\n\n\treturn decision;\n};\n\ncrawler.ShouldCrawlPageLinksDecisionMaker = (crawledPage, crawlContext) =>\n{\n\tvar decision = new CrawlDecision{ Allow = true };\n\tif (crawledPage.Content.Bytes.Length < 100)\n\t\treturn new CrawlDecision { Allow = false, Reason = \"Just crawl links in pages that have at least 100 bytes\" };\n\n\treturn decision;\n};\n```\n\n#### Custom Implementations\nPoliteWebCrawler is the master of orchestrating the crawl. Its job is to coordinate all the utility classes to \"crawl\" a site. PoliteWebCrawler accepts an alternate implementation for all its dependencies through its constructor.\n \n```c#\nvar crawler = new PoliteWebCrawler(\n    \tnew CrawlConfiguration(),\n\tnew YourCrawlDecisionMaker(),\n\tnew YourThreadMgr(), \n\tnew YourScheduler(), \n\tnew YourPageRequester(), \n\tnew YourHyperLinkParser(), \n\tnew YourMemoryManager(), \n    \tnew YourDomainRateLimiter,\n\tnew YourRobotsDotTextFinder());\n```\n\nPassing null for any implementation will use the default. The example below will use your custom implementation for the IPageRequester and IHyperLinkParser but will use the default for all others.\n\n```c#\nvar crawler = new PoliteWebCrawler(\n\tnull, \n\tnull, \n    \tnull,\n    \tnull,\n\tnew YourPageRequester(), \n\tnew YourHyperLinkParser(), \n\tnull,\n    \tnull, \n\tnull);\n```\n\nThe following are explanations of each interface that PoliteWebCrawler relies on to do the real work.\n\n###### ICrawlDecisionMaker\nThe callback/delegate shortcuts are great to add a small amount of logic but if you are doing anything more heavy you will want to pass in your custom implementation of ICrawlDecisionMaker. The crawler calls this implementation to see whether a page should be crawled, whether the page's content should be downloaded and whether a crawled page's links should be crawled.\n\n[CrawlDecisionMaker.cs](https://github.com/sjdirect/abot/blob/master/Abot2/Core/CrawlDecisionMaker.cs) is the default ICrawlDecisionMaker used by Abot. This class takes care of common checks like making sure the config value MaxPagesToCrawl is not exceeded. Most users will only need to create a class that extends CrawlDecision maker and just add their custom logic. However, you are completely free to create a class that implements ICrawlDecisionMaker and pass it into PoliteWebCrawlers constructor.\n\n```c#\n/// <summary>\n/// Determines what pages should be crawled, whether the raw content should be downloaded and if the links on a page should be crawled\n/// </summary>\npublic interface ICrawlDecisionMaker\n{\n\t/// <summary>\n\t/// Decides whether the page should be crawled\n\t/// </summary>\n\tCrawlDecision ShouldCrawlPage(PageToCrawl pageToCrawl, CrawlContext crawlContext);\n\n\t/// <summary>\n\t/// Decides whether the page's links should be crawled\n\t/// </summary>\n\tCrawlDecision ShouldCrawlPageLinks(CrawledPage crawledPage, CrawlContext crawlContext);\n\n\t/// <summary>\n\t/// Decides whether the page's content should be dowloaded\n\t/// </summary>\n\tCrawlDecision ShouldDownloadPageContent(CrawledPage crawledPage, CrawlContext crawlContext);\n}\n```\n\n\n###### IThreadManager\nThe IThreadManager interface deals with the multithreading details. It is used by the crawler to manage concurrent http requests. \n\n[TaskThreadManager.cs](https://github.com/sjdirect/abot/blob/master/Abot2/Util/TaskThreadManager.cs) is the default IThreadManager used by Abot. \n\n\n```c#\n/// <summary>\n/// Handles the multithreading implementation details\n/// </summary>\npublic interface IThreadManager : IDisposable\n{\n\t/// <summary>\n\t/// Max number of threads to use.\n\t/// </summary>\n\tint MaxThreads { get; }\n\n\t/// <summary>\n\t/// Will perform the action asynchrously on a seperate thread\n\t/// </summary>\n\t/// <param name=\"action\">The action to perform</param>\n\tvoid DoWork(Action action);\n\n\t/// <summary>\n\t/// Whether there are running threads\n\t/// </summary>\n\tbool HasRunningThreads();\n\n\t/// <summary>\n\t/// Abort all running threads\n\t/// </summary>\n\tvoid AbortAll();\n}\n```\n\n\n###### IScheduler\nThe IScheduler interface deals with managing what pages need to be crawled. The crawler gives the links it finds to and gets the pages to crawl from the IScheduler implementation. A common use cases for writing your own implementation might be to distribute crawls across multiple machines which could be managed by a DistributedScheduler.\n\n[Scheduler.cs](https://github.com/sjdirect/abot/blob/master/Abot2/Core/Scheduler.cs) is the default IScheduler used by the crawler and by default is constructed with in memory collection to determine what pages have been crawled and which need to be crawled. \n\n```c#\n/// <summary>\n/// Handles managing the priority of what pages need to be crawled\n/// </summary>\npublic interface IScheduler\n{\n\t/// <summary>\n\t/// Count of remaining items that are currently scheduled\n\t/// </summary>\n\tint Count { get; }\n\n\t/// <summary>\n\t/// Schedules the param to be crawled\n\t/// </summary>\n\tvoid Add(PageToCrawl page);\n\n\t/// <summary>\n\t/// Schedules the param to be crawled\n\t/// </summary>\n\tvoid Add(IEnumerable<PageToCrawl> pages);\n\n\t/// <summary>\n\t/// Gets the next page to crawl\n\t/// </summary>\n\tPageToCrawl GetNext();\n\n\t/// <summary>\n\t/// Clear all currently scheduled pages\n\t/// </summary>\n\tvoid Clear();\n}\n```\n\n\n###### IPageRequester\nThe IPageRequester interface deals with making the raw http requests.\n\n[PageRequester.cs](https://github.com/sjdirect/abot/blob/master/Abot2/Core/PageRequester.cs) is the default IPageRequester used by the crawler. \n\n```c#\npublic interface IPageRequester : IDisposable\n{\n\t/// <summary>\n\t/// Make an http web request to the url and download its content\n\t/// </summary>\n\tTask<CrawledPage> MakeRequestAsync(Uri uri);\n\n\t/// <summary>\n\t/// Make an http web request to the url and download its content based on the param func decision\n\t/// </summary>\n\tTask<CrawledPage> MakeRequestAsync(Uri uri, Func<CrawledPage, CrawlDecision> shouldDownloadContent);\n}\n```\n\n###### IHyperLinkParser\nThe IHyperLinkParser interface deals with parsing the links out of raw html.\n\n[AngleSharpHyperLinkParser.cs](https://github.com/sjdirect/abot/blob/master/Abot2/Core/AngleSharpHyperLinkParser.cs) is the default IHyperLinkParser used by the crawler. It uses the well known [AngleSharp](https://github.com/AngleSharp/AngleSharp) to do the html parsing. AngleSharp uses a css style selector like jquery but all in c#. \n\n```c#\n/// <summary>\n/// Handles parsing hyperlikns out of the raw html\n/// </summary>\npublic interface IHyperLinkParser\n{\n\t/// <summary>\n\t/// Parses html to extract hyperlinks, converts each into an absolute url\n\t/// </summary>\n\tIEnumerable<Uri> GetLinks(CrawledPage crawledPage);\n}\n```\n\n###### IMemoryManager\nThe IMemoryManager handles memory monitoring. This feature is still experimental and could be removed in a future release if found to be unreliable. \n\n[MemoryManager.cs](https://github.com/sjdirect/abot/blob/master/Abot2/Util/MemoryManager.cs) is the default implementation used by the crawler. \n\n```c#\n/// <summary>\n/// Handles memory monitoring/usage\n/// </summary>\npublic interface IMemoryManager : IMemoryMonitor, IDisposable\n{\n\t/// <summary>\n\t/// Whether the current process that is hosting this instance is allocated/using above the param value of memory in mb\n\t/// </summary>\n\tbool IsCurrentUsageAbove(int sizeInMb);\n\n\t/// <summary>\n\t/// Whether there is at least the param value of available memory in mb\n\t/// </summary>\n\tbool IsSpaceAvailable(int sizeInMb);\n}\n```\n\n###### IDomainRateLimiter\nThe IDomainRateLimiter handles domain rate limiting. It will handle determining how much time needs to elapse before it is ok to make another http request to the domain.\n\n[DomainRateLimiter.cs](https://github.com/sjdirect/abot/blob/master/Abot2/Core/DomainRateLimiter.cs) is the default implementation used by the crawler. \n\n```c#\n/// <summary>\n/// Rate limits or throttles on a per domain basis\n/// </summary>\npublic interface IDomainRateLimiter\n{\n\t/// <summary>\n\t/// If the domain of the param has been flagged for rate limiting, it will be rate limited according to the configured minimum crawl delay\n\t/// </summary>\n\tvoid RateLimit(Uri uri);\n\n\t/// <summary>\n\t/// Add a domain entry so that domain may be rate limited according the the param minumum crawl delay\n\t/// </summary>\n\tvoid AddDomain(Uri uri, long minCrawlDelayInMillisecs);\n}\n```\n\n\n###### IRobotsDotTextFinder\nThe IRobotsDotTextFinder is responsible for retrieving the robots.txt file for every domain (if isRespectRobotsDotTextEnabled=\"true\") and building the robots.txt abstraction which implements the IRobotsDotText interface. \n\n[RobotsDotTextFinder.cs](https://github.com/sjdirect/abot/blob/master/Abot2/Core/RobotsDotTextFinder.cs) is the default implementation used by the crawler. \n\n```c#\n/// <summary>\n/// Finds and builds the robots.txt file abstraction\n/// </summary>\npublic interface IRobotsDotTextFinder\n{\n\t/// <summary>\n\t/// Finds the robots.txt file using the rootUri. \n        /// \n\tIRobotsDotText Find(Uri rootUri);\n}\n```\n\n<br /><br /><br />\n<hr />\n"
        },
        {
          "name": "TestResponses.saz",
          "type": "blob",
          "size": 626.1875,
          "content": null
        },
        {
          "name": "azure-pipelines.yml",
          "type": "blob",
          "size": 0.451171875,
          "content": "# Starter pipeline\n# Start with a minimal pipeline that you can customize to build and deploy your code.\n# Add steps that build, run tests, deploy, and more:\n# https://aka.ms/yaml\n\ntrigger:\n- master\n\npool:\n  vmImage: 'ubuntu-latest'\n\nsteps:\n- script: echo Hello, world!\n  displayName: 'Run a one-line script'\n\n- script: |\n    echo Add other tasks to build, test, and deploy your project.\n    echo See https://aka.ms/yaml\n  displayName: 'Run a multi-line script'\n"
        },
        {
          "name": "msbuild.rsp",
          "type": "blob",
          "size": 0.013671875,
          "content": "/p:NoWarn=1591"
        }
      ]
    }
  ]
}