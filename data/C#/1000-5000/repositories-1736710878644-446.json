{
  "metadata": {
    "timestamp": 1736710878644,
    "page": 446,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQ1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "apache/lucenenet",
      "stars": 2263,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".asf.yaml",
          "type": "blob",
          "size": 0.638671875,
          "content": "# Apache YAML Features for Git Repositories\n# See the documentation at: https://cwiki.apache.org/confluence/display/INFRA/git+-+.asf.yaml+features\n\ngithub:\n  description: \"Apache Lucene.NET\"\n  homepage: https://lucenenet.apache.org/\n  labels:\n    - lucenenet\n    - lucene\n    - text\n    - search\n    - information\n    - retrieval\n    - analysis\n    - index\n    - query\n    - apache\n    - hacktoberfest\n    \n  features:\n    # Enable wiki for documentation\n    wiki: false\n    # Enable issues management\n    issues: true\n    # Enable projects for project management boards\n    projects: false\n\n    # Disallow forced pushes\n  protected_branches:\n    master\n"
        },
        {
          "name": ".build",
          "type": "tree",
          "content": null
        },
        {
          "name": ".config",
          "type": "tree",
          "content": null
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 7.5048828125,
          "content": "﻿# You can modify the rules from these initially generated values to suit your own policies\n# You can learn more about editorconfig here: https://docs.microsoft.com/en-us/visualstudio/ide/editorconfig-code-style-settings-reference\n\n[*]\ncharset = utf-8-bom\ntrim_trailing_whitespace = true\ninsert_final_newline = true\nresharper_enforce_empty_line_at_end_of_file = true\n\n[*.md]\nindent_style = space\nindent_size = 4\n\n# C# files\n[*.cs]\nindent_style = space\nindent_size = 4\n\n#### Core EditorConfig Options ####\n\n#Formatting - indentation\n\n#use soft tabs (spaces) for indentation\nindent_style = space\n\n#Formatting - indentation options\n\n#indent switch case contents.\ncsharp_indent_case_contents = true\n#indent switch labels\ncsharp_indent_switch_labels = true\n\n#Formatting - new line options\n\n#place catch statements on a new line\ncsharp_new_line_before_catch = true\n#place else statements on a new line\ncsharp_new_line_before_else = true\n#require finally statements to be on a new line after the closing brace\ncsharp_new_line_before_finally = true\n#require members of object initializers to be on the same line\ncsharp_new_line_before_members_in_object_initializers = false\n#require braces to be on a new line for control_blocks, types, properties, and methods (also known as \"Allman\" style)\ncsharp_new_line_before_open_brace = accessors, anonymous_methods, control_blocks, lambdas, methods, object_collection_array_initializers, properties, types\n\n#Formatting - organize using options\n\n#do not place System.* using directives before other using directives\ndotnet_sort_system_directives_first = false\n\n#Formatting - spacing options\n\n#require NO space between a cast and the value\ncsharp_space_after_cast = false\n#require a space before the colon for bases or interfaces in a type declaration\ncsharp_space_after_colon_in_inheritance_clause = true\n#require a space after a keyword in a control flow statement such as a for loop\ncsharp_space_after_keywords_in_control_flow_statements = true\n#require a space before the colon for bases or interfaces in a type declaration\ncsharp_space_before_colon_in_inheritance_clause = true\n#remove space within empty argument list parentheses\ncsharp_space_between_method_call_empty_parameter_list_parentheses = false\n#remove space between method call name and opening parenthesis\ncsharp_space_between_method_call_name_and_opening_parenthesis = false\n#do not place space characters after the opening parenthesis and before the closing parenthesis of a method call\ncsharp_space_between_method_call_parameter_list_parentheses = false\n#remove space within empty parameter list parentheses for a method declaration\ncsharp_space_between_method_declaration_empty_parameter_list_parentheses = false\n#place a space character after the opening parenthesis and before the closing parenthesis of a method declaration parameter list.\ncsharp_space_between_method_declaration_parameter_list_parentheses = false\n\n#Formatting - wrapping options\n\n#leave code block on separate lines\ncsharp_preserve_single_line_blocks = true\n#leave statements and member declarations on the same line\ncsharp_preserve_single_line_statements = true\n\n#Style - Code block preferences\n\n#prefer curly braces even for one line of code\ncsharp_prefer_braces = when_multiline:silent\n\n#Style - expression bodied member options\n\n#prefer expression-bodied members for accessors\ncsharp_style_expression_bodied_accessors = when_on_single_line:suggestion\n#prefer block bodies for constructors\ncsharp_style_expression_bodied_constructors = false:suggestion\n#prefer expression-bodied members for indexers\ncsharp_style_expression_bodied_indexers = true:suggestion\n#prefer block bodies for methods\ncsharp_style_expression_bodied_methods = when_on_single_line:silent\n#prefer expression-bodied members for properties\ncsharp_style_expression_bodied_properties = when_on_single_line:suggestion\n\n#Style - expression level options\n\n#prefer out variables to be declared inline in the argument list of a method call when possible\ncsharp_style_inlined_variable_declaration = true:suggestion\n#prefer the language keyword for member access expressions, instead of the type name, for types that have a keyword to represent them\ndotnet_style_predefined_type_for_member_access = true:suggestion\n\n#Style - Expression-level  preferences\n\n#prefer objects to not be initialized using object initializers, but do not warn\ndotnet_style_object_initializer = true:silent\n#prefer objects to use auto properties, but turn off the warnings (we want to keep backing fields from Java for the most part)\ndotnet_style_prefer_auto_properties = true:silent\n\n#Style - implicit and explicit types\n\n#prefer explicit type over var in all cases, unless overridden by another code style rule\ncsharp_style_var_elsewhere = false:silent\n#prefer explicit type over var to declare variables with built-in system types such as int\ncsharp_style_var_for_built_in_types = false:silent\n#prefer explicit type over var when the type is already mentioned on the right-hand side of a declaration\ncsharp_style_var_when_type_is_apparent = false:silent\n\n#Style - language keyword and framework type options\n\n#prefer the language keyword for local variables, method parameters, and class members, instead of the type name, for types that have a keyword to represent them\ndotnet_style_predefined_type_for_locals_parameters_members = true:suggestion\n\n#Style - modifier options\n\n#prefer accessibility modifiers to be declared except for public interface members. This will currently not differ from always and will act as future proofing for if C# adds default interface methods.\ndotnet_style_require_accessibility_modifiers = for_non_interface_members:suggestion\n\n#Style - Modifier preferences\n\n#when this rule is set to a list of modifiers, prefer the specified ordering.\ncsharp_preferred_modifier_order = public,private,protected,internal,virtual,readonly,override,static,abstract,new,sealed,volatile:silent\n\n#Style - Pattern matching\n\n#prefer pattern matching instead of is expression with type casts\ncsharp_style_pattern_matching_over_as_with_null_check = true:suggestion\n\n#Style -Pattern matcing preferences\n\n#prefer expression-style for switch case\ncsharp_style_prefer_switch_expression = false:suggestion\n\n#Style - qualification options\n\n#prefer fields not to be prefaced with this. or Me. in Visual Basic\ndotnet_style_qualification_for_field = false:none\n#prefer methods not to be prefaced with this. or Me. in Visual Basic\ndotnet_style_qualification_for_method = false:none\n#prefer properties not to be prefaced with this. or Me. in Visual Basic\ndotnet_style_qualification_for_property = false:none\n\n#Style - assignment options\n#prefer compound asignment x += 1 rather than x = x + 1.\ndotnet_style_prefer_compound_assignment = true:silent\n\n#### General Code Quality Preferences ####\n\n# Warn about any performance category issues across the entire API\ndotnet_code_quality.Performance.api_surface = all:warning\n\n# CA1031: Do not catch general exception types\ndotnet_diagnostic.CA1031.severity = none\n\n# CA1034: Do not nest types\ndotnet_diagnostic.CA1034.severity = none\n\n#### Style ####\n\n# IDE0090: Simplify new expression\ndotnet_diagnostic.IDE0090.severity=none\n\n#### Usage ####\n# CA2249: Consider using String.Contains instead of String.IndexOf\ndotnet_diagnostic.CA2249.severity=none\n\n\n# Features that require .NET Standard 2.1+\n\n# IDE0056: Use index operator\ndotnet_diagnostic.IDE0056.severity = none\n\n# IDE0057: Use range operator\ndotnet_diagnostic.IDE0057.severity = none\n\n# IDE0070: Use 'System.HashCode.Combine'\ndotnet_diagnostic.IDE0070.severity = none\n\n\n### SonarCloud Issues ###\n\n# S907: Remove this use of 'goto'\ndotnet_diagnostic.S907.severity = none\n"
        },
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.55078125,
          "content": "﻿# Auto detect text files and perform LF normalization\n* text=auto\n\n# Custom for Visual Studio\n*.cs     eol=crlf diff=csharp\n*.sln    merge=union\n*.csproj merge=union\n*.vbproj merge=union\n*.fsproj merge=union\n*.dbproj merge=union\n\n# Standard to msysgit\n*.doc\t diff=astextplain\n*.DOC\t diff=astextplain\n*.docx diff=astextplain\n*.DOCX diff=astextplain\n*.dot  diff=astextplain\n*.DOT  diff=astextplain\n*.pdf  diff=astextplain\n*.PDF\t diff=astextplain\n*.rtf\t diff=astextplain\n*.RTF\t diff=astextplain\n\n# Batch Files\nbuild eol=lf\n*.sh eol=lf\n*.bat eol=crlf\n*.cmd eol=crlf"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.158203125,
          "content": "﻿# this is the git pattern ignore file for the project.\n# Git can be used with svn http://code.google.com/p/msysgit/\nbin\nBin\nobj\n*.user\n*.suo\n*.bak\n*.vs10x\n*.VisualState.xml\n*.userprefs\n*.pidb\n*.ide\n*.project.lock.json\ntest-results\nbuild/artifacts\nbuild/bin\n_ReSharper*\n*.orig\n*.cache\nsrc/core/Messages/\ntest/core/Messages/\nsrc/core/QueryParser/\ntest/core/QueryParser/\nsrc/contrib/\ntest/contrib/\nsrc/core/obj/\nsrc/core/bin/\ntest/core/obj/\ntest/core/bin/\nbin/\nobj/\ndoc/\nsrc/demo/\npackages/\n.vs/\n*.lock.json\nTestResults/\ntest-files/analysis/data/\n[Nn]u[Gg]et[Pp]ackages/\nout.dot\nrelease/\n_artifacts/\n\n# NuGet v3's project.json files produces more ignoreable files\n*.nuget.props\n*.nuget.targets\n\n# For the build tools that are downloading when building Lucene.NET\n.tools/\n\n# NUnit test result file produced by nunit3-console.exe\n[Tt]est[Rr]esult.xml\nwebsites/**/_site/*\nwebsites/**/tools/*\nwebsites/**/_exported_templates/*\nwebsites/**/api/.manifest\nwebsites/**/docfx.log\nwebsites/**/lucenetemplate/plugins/*\nwebsites/apidocs/api/**/*.yml\nwebsites/apidocs/api/**/*.manifest\n!websites/apidocs/api/toc.yml\n\n# Apache Releases on Subversion\nsvn-*/\n\n# vscode files\n.vscode/\n.idea/**/misc.xml\n"
        },
        {
          "name": ".idea",
          "type": "tree",
          "content": null
        },
        {
          "name": ".rat-excludes",
          "type": "blob",
          "size": 0.8115234375,
          "content": "﻿# Note: these patterns are applied to single files or directories, not full paths\n# coverage/* will ignore any coverage dir, but airflow/www/static/coverage/* will match nothing\n\n.rat-excludes\n.gitignore\n.gitattributes\n\n# Exclude build assets\nlib/*\nobj/*\nbin/*\n_artifacts/*\n_site/*\n\n# Exclude SVN release folders\nsvn-dev/*\nsvn-release/*\n\n# Exclude auto-generated designers\n.*\\.Designer\\.cs\n\n# For testing - must not change\nLuceneResourcesWikiPage\\.html\n\n# Covered under a different license\npsake/*\nKStemData\\d+\\.cs\nSnowball/*\nEgothor.Stemmer/*\nEvents/*\nSax/*\nJaspellTernarySearchTrie\\.cs\nRectangularArrays\\.cs\nLimitedConcurrencyLevelTaskScheduler\\.cs\nAutomaton/*\nConcurrentHashSet\\.cs\nNullableAttributes\\.cs\nConfigurationReloadToken\\.cs\nConfigurationRoot\\.cs\nConfigurationSection\\.cs\nDateTimeOffsetUtil\\.cs\nFileStreamOptions\\.cs"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "ACKNOWLEDGEMENTS.txt",
          "type": "blob",
          "size": 0.5869140625,
          "content": "Contributors to this port of Lucene 4.8.0 to C# were Russell Trupiano and David Wan.\nAll changes are to be submitted under the apache 2.0 license.\n\nThe snowball stemmers in contrib/Snowball.Net/Snowball.Net/SF/Snowball\nwere developed by Martin Porter and Richard Boulton.\n\nThe full snowball package is available from http://snowball.tartarus.org/\n\nApache Lucene.Net is a port of Jakarta Lucene to C#.  \nThe port from Java to C# of versions 1.4.0, 1.4.3, 1.9, 1.9.1, 2.0 and 2.1 were done \nprimarily by George Aroush. To contact George Aroush please visit http://www.aroush.net/.\nMuch thanks to George\n"
        },
        {
          "name": "CHANGES.txt",
          "type": "blob",
          "size": 208.466796875,
          "content": "﻿ =================== Release 4.8.0-beta00006 =====================\n\nBug\n•\tLucene.Net.Support.Collections::Equals<T>(): Fixed comparison to include a check whether Count\n\tmatches.\n•[Pull Request #215] - Lucene.Net.Analysis.Common.Analysis.Core.UpperCaseFilter::UpperCaseFilter():\n\tRemoved redundant termAtt initialization in UpperCaseFilter constructor\n•[LUCENENET-597] - Lucene.Net.Search.Spans.SpanNearQuery::ToString(): Fixed ordering problem when appending\n\tstatements to StringBuilder.\n•[Pull Request #224] - GOOD_FAST_HASH_SEED thread safety - avoid static constructor\n•\tIntermittent failures of Lucene.Net.Facet.Taxonomy.WriterCache.TestCharBlockArray.TestArray().\n\tThe test was not setup to with encoders that fallback to '?' for unmapped/invalid characters.\n\tAlso, the BinaryReader/BinaryWriter was too strict with regard to validating surrogate pairs\n\tfor this type of serialization, so implemented custom extension methods over Stream that do\n\tnot use encoding.\n•[LUCENENET-602] - Some platforms fail to load codecs seemingly because their types are discovered \n\tby using reflection. Changed to using hard-coded codec lists rather than reflection to load the\n\tinternal codec types.\n\tregression test to show it is no longer an issue).\n•\tLucene.Net.Tests.Search.TestMultiTermContsantScore: Safely call Dispose by ensuring the \n\treference variable is not null\n•\tLucene.Net.Tests.Search.BaseTestRangeFilter: Safely call Dispose by ensuring the \n\treference variable is not null\n•\tLucene.Net.Support.IO.FileSupport::fileCanonPathCache changed to ConcurrentDictionary\n\tto make it thread-safe\n•\tLucene.Net.Store.NativeFSLockFactory: Fixed locking/disposal of lock instances to be thread-safe\n•[Pull Request #225] - Changed \"Improve this doc\" button to point to the GitHub repo\n•\tFixed constructor of Lucene.Net.Support.HashMap to use the passed in comparer rather than \n\tEqualityComparer<TKey>.Default.\n•[LUCENENET-603] - Fixed ConcurrentMergeScheduler and TaskMergeScheduler so they don't throw exceptions\n\ton background threads and properly throw exceptions on the calling thread during merge failures.\n•\tLucene.Net.TestFramework: Removed dependency on local file path location of \n\teuroparl.lines.txt.gz and embedded the file.\n•\tLucene.Net.Suggest.Suggest.FileDictionary - Fixed conversion of string to number to be\n\tculture insensitive (it caused the tests in FileDictionaryTest to fail randomly)\n•\tLucene.Net.Tests.Cli - Fixed issue with xplat root directory specification \n\t(all platforms were trying to set the directory to C:\\)\n•\tLucene.Net.Benchmark.ByTask.Utils.Config: Fixed FormatException caused by converting\n\tnumber to string in ambient culture and parsing it back to a number in invariant culture\n•\tLucene.Net.Analysis.Common.Analysis.Util.AbstractAnalysisFactory: Fixed parsing issue \n\tconverting string to int in ambient culture\n•\tLucene.Net.Analysis.Common.Analysis.Miscellaneous.TruncateTokenFilterFactory - Fixed\n\tissue converting string to sbyte in ambient culture\n•\tLucene.Net.Util.CommandLineUtil.AdjustDirectoryName - IndexOf comparison must be \n\tStringComparison.Ordinal (or in this case, a single char) to be compatible with all\n\tcultures/platforms.\n•\tLucene.Net.TestFramework.Util.LuceneTestCase.NewFSDirectory - When resolving a type, \n\twe were expecting an exception if the type does not subclass FSDirectory, however, in .NET this\n\twon't happen. We need to explicitly check whether the resolved type is assignable from FSDirectory\n\tor if the type name is nonsense.\n•\tLucene.Net.Util.StringHelper: - Fixed parsing issue converting string to int in ambient culture\n•\tLucene.Net.Index.CheckIndex - Fixed issue with converting int to string using ambient context \n\ton VersionInfo comparison\n•\tLucene.Net.Expressions: Corrected casing on app.config to lower (xplat problem)\n•\tLucene.Net.Analysis.SmartCn: Corrected casing of folder paths on bigramdict.mem, coredict.mem, \n\tand package.md (xplat problem)\n•\tLucene.Net.Tests.Support.TestTreeSet: Passing null instead of CultureInfo.InvariantCulture causes\n\tthe test to randomly fail depending on the culture of the current thread (which is\n\trandomly selected by LuceneTestCase).\n•\tLucene.Net.TestFramework.Util.TestUtil.NextLong: The result of the method was always the value of\n\tstart when start == long.MinValue and end == long.MaxValue. As a result, many tests\n\twere not actually random.\n•\tLucene.Net.TestFramework.Index.AlcoholicMergePolicy: The value chosen for Hour was supposed to be\n\trandom, but it was setup to be a constant by a mistranslation from Java to .NET\n•\tLucene.Net.Tests.Index.TestTransactionRollback: Number was failing due to the fact the data that was\n\tbeing populated wasn't being converted from int to string in invariant culture. Switched back to\n\toriginal logic, using LastIndexOf(char) rather than LastIndexOf(string).\n•\tLucene.Net.Grouping.TopGroups - check collection equality if the generic type is a reference\n\ttype (as is the default behavior in Java)\n•\tSWEEP: Added StringComparison.Ordinal to all of the string.StartsWith() and string.EndsWith()\n\tmethods where it was missing\n•\tLucene.Net.Tests.QueryParser.Flexible.Precedence.TestPrecedenceQueryParser: Specify short date\n\tformat by using DateTime.ParseExact instead of DateTime.Parse\n•\tLucene.Net.Support.CultureContext: Fixed minor issue with unused variable\n•\tLucene.Net.TestFramework.JavaCompatibility.SystemTypesHelpers: Overloads of append that take\n\tnumeric types need to be converted to the invariant culture. Removed the overloads for\n\tdecimal, double, and float, as those need to be dealt with on a case by case basis.\n•\tLucene.Net.Tests.Analysis.Common.Analysis.Pattern.TestPatternTokenizer.TestSplitting: int.Parse\n\tmust be setup in the invariant culture to consistently recognize inputs\n•\tSWEEP: Ensure all enumerators are disposed of properly (except in some cases where enumerators\n\tare set to field variables, see LUCENENET-611)\n•\tLucene.Net.Highlighter.VectorHighlight.FieldQuery: List<T> replacement for LinkedHashSet<T> preserves insertion\n\torder, but we need to explicitly check to ensure no duplicate values are added\n•\tLucene.Net.Tests.Search.TestFieldCacheRangeFilter.TestSparseIndex: formatting value must be done in invariant culture\n•\tLucene.Net.Util.StringHelper - Use Time.CurrentTimeMilliseconds() instead of DateTime.Now.Millisecond. The latter is \n•\ta mis-translation from Java which contains only numbers 0 to 999, the former returns a long based on\n\tStopwatch.GetTimestamp() that has several orders of magnitude more possible values.\n•\tSWEEP: Ensure all enumerators are disposed of properly (except in some cases where enumerators are set \n\tto field variables, see LUCENENET-611)\n•\tLucene.Net.TestFramework.Codecs.RAMOnly.RAMOnlyPostingsFormat - string comparison must be done using ordinal to match Java\n\nImprovement\n•[Pull Request #206] - Website & API Doc site generator using DocFx script\n•[Pull Request #223] - Website updates - DOAP file and copy changes\n•[LUCENENET-588] - Made lucene-cli into a dotnet tool NuGet package and updated the documentation on how to install and use it\n•\tFixed solution and project files so builds can be done cross-platform (in the IDE or via dotnet build)\n•\tSwitched to the .snupkg debugy symbols format\n•\tChanged build.ps1 script to install and use only version 2.2.300 of the .NET Core SDK to prevent\n\tbuild failures due to version drift\n•[Pull Request #227 & LUCENENET-608] - Added strong naming to Lucene.Net assemblies to comply with Microsoft guidelines\n•\tAdded missing guard clauses for Lucene.Net.Support.HashMap and Lucene.Net.Support.LinkedHashMap constructors\n•\tUpgraded build script to latest dotnet-install.ps1\n•\tChanged NuGet dependency from the unofficial SharpZipLib.NETStandard to the official SharpZipLib\n\tand upgraded to version 1.1.0 from 0.86.0\n•\tRemoved hard-coded failure, since we are no longer getting crashes due to background threads \n\tthrowing exceptions\n•\tSWEEP: Re-evaluated test times and decorated all tests 5 seconds or over with the LongRunningTestAttribute,\n\tremoving the attribute where it was no longer necessary\n•\tSWEEP: Removed the TimeoutAttribute from all tests that are known to run in a short duration\n•\tUpgraded test projects to use Microsoft.NET.Test.Sdk version 16.2.0\n•\tUpgraded test projects to use NUnit3TestAdapter version 3.13.0\n•\tUpgraded test projects to use NUnit version 3.9.0\n•\tLucene.Net.Analysis.Analyzer: Implemented dispose pattern\n•\tLucene.Net.Benchmark.ByTask.Tasks.PerfTask: added IDisposable so the\n\tclass can be used with a using block (it already had Dispose())\n•\tSetup build.ps1 to run tests in parallel using background jobs\n•\tbuild.ps1: Added function to summarize the test results on the console\n•\tRemoved Version.proj file and moved the version properties into the root Directory.Build.props file\n•\tRenamed TestTargetFramework.proj to TestTargetFramework.props (Some editions of VS2019 don't seem \n\tto like the .proj extension)\n•\tLucene.Net.TestFramework: Implemented dispose pattern where applicable\n•\tBroke Lucene.Net.Tests project into Lucene.Net.Tests._A-I, Lucene.Net.Tests._J-U, and \n\tLucene.Net.Tests._U-Z to cut the time it takes to run the tests in the project by about 2/3, running\n\tin parallel\n•[Pull Request #216] - Added .NET Standard 2.0 target to projects where it was missing\n•\tLucene.Net.TestFramework.Util.LuceneTestCase: Throw explicit exception if Directory type cannot be resolved\n•\tLucene.Net.Benchmark: Use AssemblyQualifiedName for StandardAnalyzer for better reliability with .NET Reflection\n•\tbuild.ps1: Added option to specify maximum number of parallel jobs to use during testing\n•\tAdded .vscode/settings.json file to locate tests and ignore docs path in Visual Studio Code\n•\tSWEEP: Added StringComparison.Ordinal to all string.Equals() calls, as per \n\thttps://docs.microsoft.com/en-us/dotnet/standard/base-types/best-practices-strings#recommendations-for-string-usage\n•\tLucene.Net.TestFramework.Util.LuceneTestCase: Added try catch blocks to write stack traces to the\n\tconsole if exceptions occur during OneTimeSetUp or OneTimeTearDown\n•[LUCENENET-435] SWEEP: CA2200: Rethrow to preserve stack details\n\t(https://docs.microsoft.com/en-us/visualstudio/code-quality/ca2200-rethrow-to-preserve-stack-details)\n•\tLucene.Net.Support.Search.ReferenceContext: Sealed the class, as none of its\n\tmembers are virtual anyway.\n•\tMoved TaskMergeScheduler/TestTaskMergeScheduler to the Support folders\n•\tUpgraded NuGet package dependency of Spatial4n to version 0.4.1\n\nNew Feature\n•[LUCENENET-566 & LUCENENET-573] - Lucene.Net.ICU: Added all missing functionality and tests (100% passing) and changed the NuGet \n\tpackage dependency from icu.net to ICU4N.\n•\tCreated azure-pipelines.yml for Azure DevOps that anybody can use.\n\n=================== Release 4.8.0-beta00005 =====================\n\nBug\n•\tAdded [Obsolete] attribute to Lucene.Net.Field extension methods that are only for\n\tpre-4.0 backward compatibility.\n•\tBREAKING: Lucene.Net.Search.Similarites.BasicStats: Changed m_field from protected to \n\tprivate (to match Lucene).\n•\tBREAKING: Lucene.Net.Util.MapOfSets: Changed Map property from IDictionary<TKey, HashSet<TValue>> \n\tto IDictionary<TKey, ISet<TValue>> (to match Lucene).\n•\tBREAKING: Lucene.Net.Util.OfflineSorter: Changed setter of BufferSize property to private\n\t(to match Lucene).\n•\tBREAKING: Lucene.Net.Util.RamUsageEstimator.IdentityHashSet<KType>: Changed accessibility \n\tfrom public to internal (to match Lucene)\n•\tBREAKING: Lucene.Net.Search.BitsFilteredDocIdSet: Changed constructor to throw ArgumentNullException \n\tinstead of NullReferenceException (.NET convention)\n•\tLucene.Net.Search.ConstantScoreQuery.ConstantWeight: Fixed initialization issue with commented code \n\tin GetValueForNormalization() (that diverged from Lucene)\n•\tLucene.Net.Util.OfflineSorter: Added line to delete the output file (that existed in Lucene).\n•\tBREAKING: Lucene.Net.Analysis.Common.Analysis.Util.AbstractAnalysisFactory: Changed return \n\ttype of GetSet from ICollection<T> to ISet<T>.\n•[LUCENENET-544] - Turkish stemmer causes an IndexOutOfRange (Reported in prior version, added\n\tregression test to show it is no longer an issue).\n•[LUCENENET-544] - replace java doc notation with ms style xml comments notation.\n•\tLucene.Net.Misc: Removed unnecessary dependency on Lucene.Net.Analysis.Common\n•\tLucene.Net.Highlighter: Removed unnecessary dependency on Lucene.Net.Analysis.Common\n•\tBREAKING: Lucene.Net.Facet.Taxonomy.TaxonomyReader: Implemented IDisposable and proper dispose pattern.\n\tPublic Dispose(bool) method removed.\n•\tLucene.Net.Misc.IndexMergeTool: Added try-finally block to properly dispose of directories.\n•[LUCENENET-521] - Concurrency bug with MMapDirectory (added regression test to confirm fix).\n•[LUCENENET-530] - Create a truly Memory Mapped Directory type as MMapDirectory is not really memory mapped\n\tLucene.Net.Store: Fixed MMapDirectory concurrency using solution suggested by Vincent \n\tVan Den Berghe http://apache.markmail.org/message/hafnuhq2ydhfjmi2.\n•\tLucene.Net.Store.LockVerifyServer: Read/write 1 byte instead of 1 int (4 bytes). \n\tAlso, we don't need 2 streams in .NET for input/output (solution provided by Vincent Van Den Berghe).\n•\tLucene.Net.TestFramework.Store.MockDirectoryWrapper.IndexInputSlicerAnonymousClass: Fixed Dispose() method.\n•\tLucene.Net.Index.IndexWriter: Fixed string formatting of numeric values in InfoStream message.\n•\tLucene.Net.Tests.Querys.CommonTermsQueryTest: Added missing TestRandomIndex() test\n•\tLucene.Net.Analysis.Common.Analysis.CharFilter.MappingCharFilterFactory: fixed escaping problem in parsing regex\n•\tLucene.Net.Search.SearcherLifetimeManager: Corrected bug in Lifetime reference count\n•\tLucene.Net.Spatial: Removed unnecessary dependencies on GeoAPI and NetTopologySuite\n•[LUCENENET-593] - Refactored Lucene.Net.Util.Constants so OS identification, processor architecture, and \n\tframework identification are more reliable. Searching for environment variables rather than using well-known APIs \n\twas causing a null reference exception on Linux.\n•\tLazily initialize codecs to ensure the correct type is loaded if overridden\n•\tLucene.Net.Search.FieldCacheRangeFilter.AnonymousStringFieldCacheRangeFilter: Fixed Debug.Assert condition that \n\twas causing assert to fail.\n•\tLucene.Net.Tests.Spatial.SpatialArgsTest.CalcDistanceFromErrPct(): Test fails because floating point asserts \n\tdidn't contain any delta and the implementation has changed in .NET Core 2.0 so it is no longer on the nose \n\t(but still well within tolerance for floating point numbers).\n•\tLucene.Net.Tests.Index.TestConcurrentMergeScheduler: Fixed FailOnlyOnFlush class to match the original, \n\twhich was causing TestFlushExceptions() to fail. Also removed throw statement on a background thread that \n\twas causing a crash.\n•\tLucene.Net.Tests.Search.TestMutiTermConstantScore: Made Small and Reader variables instance members, since \n\tthey are being set by instance methods. When they were static, tests could cross threads on the instance.\n•\tLucene.Net.TestFramework.Util.LuceneTestCase: Added missing catch block for UnauthorizedAccessException, \n\twhich does not subclass IOException in .NET as was the case in Java.\n•\tLucene.Net.Tests.Index.TestTransactionRollback: Fixed issue where the value passed to substring could \n\tpotentially go beyond the length of the string.\n•\tLucene.Net.Benchmark.ByTask.Tasks.TestPerfTasksLogic.TestLocale(): Original test was using no-NO which \n\tis not consistently supported across platforms on .NET. Changed the test (and the documentation) to use nb-NO instead.\n•\tLucene.Net.Tests.Benchmark.ByTask.TestPerfTasksLogic.TestCollator(): Changed culture from no-NO to nb-NO to ensure \n\tit runs consistently between dev and the CI server.\n•\tLucene.Net.Support.DictionaryExtensions: Fixed Store() method to save the date using the InvariantCulture so the \n\tformat is unaffected by the ambient culture.\n•\tLucene.Net.TestFramework: Having sequential folder names creates situations where multiple threads are doing \n\toperations on the same folder at the same time. Changed implementation to use GetRandomFileName() to append \n\ta random string instead of an incremental number.\n•\tLucene.Net.Automaton (BasicAutomata + BasicOperations + MinimizationOperations + SpecialOperations): Corrected \n\taccessibility from internal to public and removed InternalsVisibleTo attributes unneeded as a result of these changes.\n•\tLucene.Net.Tests.Expressions.TestExpressionSorts: Added missing Collections.Shuffle call\n•\tPatched behavior of all implementations of String.Split() and Regex.Split() using the .TrimEnd() extension method. \n\tIn Java, only the empty entries at the end of the array are removed, not all empty entries.\n•\tLucene.Net.Tests.QueryParser.Flexible.Precedence.TestPrecedenceQueryParser: Fixed test to always use \n\tGregorianCalendar and local time zone.\n•\tAdded TimeZoneInfo.ConvertTime() to corresponding locations where time zone had been set in Lucene.\n•\tLucene.Net.Tests.Search.TestControlledRealTimeReopenThread.DoAfterWriter(): Enabled Thread priority for \n\t.NET Core 2.0 tests\n•\tLucene.Net.Tests.Search.TestMultiTermConstantScore: Added check to ensure a null instance variable \n\tdoesn't cause the AfterClass method to fail\n•\tLucene.Net.Index (ConcurrentMergeScheduler + TaskMergeScheduler): Fixed null reference exception \n\tdue to synchronization of list across threads.\n•[LUCENENET-592] - Lucene.Net.QueryParser.Flexible.Core.Util.UnescapedCharSequence: Fixed loop condition that \n\twas preventing the ToStringEscaped() method from returning any results.\n\t\nImprovement\n•\tBREAKING: Changed namespace of Collections class from Lucene.Net to Lucene.Net.Support.\n•\tBREAKING: Changed namespace of IcuBreakIterator class from Lucene.Net to Lucene.Net.Support.\n•\tFixed several broken XML documentation comment issues.\n•\tLucene.Net.Codecs.MultiLevelSkipListReader: Implemented proper dispose pattern.\n•\tBREAKING: Lucene.Net.Index.SegmentCommitInfo: Renamed Files() method to GetFiles().\n•\tBREAKING: Lucene.Net.Index.SegmentInfos: Renamed Files() method to GetFiles().\n•\tBREAKING: Lucene.Net.Search.Similarities.LMSimilarity.ICollectionModel: Changed Name \n\tproperty to GetName() method (consistency).\n•\tBREAKING: Lucene.Net.Util (PagedBytes + PagedBytes.PagedBytesDataInput + PagedBytes.PagedBytesDataOutput): \n\tChanged Pointer > GetPointer(), Position > GetPosition()\n•\tLucene.Net.Util.PrintStreamInfoStream: Marked obsolete and replaced with class named TextWriterInfoStream.\n•\tLucene.Net.Util.RamUsageEstimator: Added SizeOf() overloads for ulong, uint, and ushort\n•\tBREAKING: Lucene.Net.MultiTermQuery: Removed nested ConstantScoreAutoRewrite class, since it is exactly \n\tthe same as the non-nested ConstantScoreAutoRewrite class. Made public constructor for ConstantScoreAutoRewrite.\n•\tBREAKING: Lucene.Net.Index.SegmentReader.ICoreClosedListener: Renamed ICoreDisposedListener, OnClose() > OnDispose()\n•\tLucene.Net.Search.ReferenceManager: Implemented proper dispose pattern.\n•\tLucene.Net.Util.IOUtils: Added Dispose() and DisposeWhileHandlingException() overloads\n\tand marked Close() and CloseWhileHandlingException() overloads [Obsolete].\n•\tLucene.Net.Search.BooleanQuery: Added documentation to show .NET usage of collection initializer.\n•\tLucene.Net.Search.MultiPhraseQuery: Implemented IEnumerable<T> so collection initializer can be used and \n\tadded documentation to show usage of collection initializer.\n•\tLucene.Net.Search.PhraseQuery: Implemented IEnumerable<T> so collection initializer can be used and \n\tadded documentation to show usage of collection initializer.\n•\tLucene.Net.Search.NGramPhraseQuery: Added documentation to show usage of collection initializer.\n•\tLucene.Net.Queries.CommonTermsQuery: Implemented IEnumerable<T> so collection initializer can be used and \n\tadded documentation to show usage of collection initializer.\n•\tLucene.Net.Search.DisjunctionMaxQuery: Added documentation to show usage of collection initializer.\n•\tLucene.Net.Facet.Range.DoubleRangeFacetCounts: Added missing params keyword on ranges constructor argument.\n•\tBREAKING: Lucene.Net.Support.MathExtension: Renamed MathExtensions and added overloads of ToRadians() \n\tfor decimal and int, and added the ToDegrees() method overloads.\n•\tLucene.Net.Analysis.Stempel: Modified Egothor.Stemmer Compile and DiffIt programs to accept file \n\tencoding on the command line and cleaned up implementation.\n•[Pull Request #207] - Added ReferenceManager<G>.GetContext(), which is similar to ReferenceManager<G>.Acquire() but can be \n\tused in a using block to implicitly dereference instead of having to do it explicitly in a finally block.\n•\tLucene.Net.Support.Document: Added extension methods to make casting to the correct IIndexableField-derived type simpler.\n•\tBREAKING: Lucene.Net.Store.FSDirectory: Removed Fsync() method and m_staleFiles variable and all references to them.\n•\tLucene.Net.Store.NativeFSLockFactory: Refactored implementation to utilize locking/sharing features of FileStream in .NET\n\ton Windows only - fallback to a different locking strategy on other plaforms. (solution mostly provided by Vincent Van Den Burghe).\n•\tPorted StreamTokenizer from Apache Harmony.\n•\tMoved SystemProperties class from Lucene.Net.TestFramework to Lucene.Net so the more Java System.properties-like \n\tdefault value and security exception handling can be used globally\n•\tLucene.Net.Support.Character: Ported Digit(char, int) method from Apache Harmony for use in Lucene.Net.Benchmark.\n•\tLucene.Net.Support.DictionaryExtensions: Added Load and Store methods from Apache Harmony, so an \n\tIDictionary<string, string> can be used the same way the Properties class is used in Java \n\t(saving and loading the same file format).\n•\tLucene.Net.Support.StringTokenizer: Did a fresh port from Apache Harmony and ported tests.\n•\tLucene.Net.Support: Added a SystemConsole class as a stand-in for System.Console, but with the ability to \n\tswap Out and Error to a different destination than System.Out and System.Error.\n•\tLucene.Net.Support.StringExtensions: Added a TrimEnd() method that can be used on string arrays. \n\tThis is to mimic Java's Split() method that removes only the null or empty elements from the end \n\tof the array that is returned, but leaves any prior empty elements intact.\n•\tLucene.Net.Support.StringBuilderExtensions: Added IndexOf() extension methods and tests from Apache Harmony.\n•\tLucene.Net.Util.SPIClassIterator: Factored out code to get all non-Microsoft referenced assemblies into a \n\tnew class in Support named AssemblyUtils\n•\tBREAKING: Lucene.Net.Index.IIndexableField: Renamed FieldType > IndexableFieldType and added additional FieldType \n\tproperty on Lucene.Net.Documents.Field that returns FieldType rather than IIndexableFieldType so we can avoid casting.\n•\tLucene.Net.Documents.Field: Added similar Number value types as in Java so the numeric types can be stored as object \n\twithout boxing/unboxing. Also added overloads for numeric GetXXXValue() fields to IIndexableField so numeric values \n\tcan be retrieved without boxing/unboxing.\n•\tLucene.Net.Documents.Field: Added NumericType property that returns an enum so the Field type can be determined without boxing/unboxing. \n•\tLucene.Net.Documents.Field: Added extension methods GetXXXValueOrDefault() to easily retrieve the numeric value if it is not a \n\tconcern that it could be null.\n•\tBREAKING: Lucene.Net.Facet.Taxonomy.WriterCache.CharBlockArray: Refactored to use BinaryReader/BinaryWriter for serialzation \n\tand eliminated the 2 serialization support classes StreamUtils and CharBlockArrayConverter\n•\tBREAKING: Removed indescriminate use of [Serializable] and chose specific targets to make serializable,\n\tnamely types that maintain a single value, collection, or array.\n•\tBREAKING: Added ICloneable to all places where it was used in Lucene, but made it a compilation option that can be used\n\tin custom builds only, since Microsoft discourages use of this interface.\n•\tMoved Intern() functionality to StringExtensions rather than using string.Intern() directly.\n•\tEliminated [Debuggable] attribute and added [MethodImpl(MethodImplOptions.NoInlining)] to each potential match \n\tfor the StackTraceHelper, which allows tests that use it to work in release mode. Solution provided by Vincent Van Den Berghe.\n•\tChanged to new .csproj format and merged Lucene.Net.sln and Lucene.Net.Portable.sln files into one Lucene.Net.sln file.\n•\tRequire VS 2017+ to load solution\n•\tLucene.Net.Search.Suggest.Analyzing.FSTUtil.Path<T>.Output: Changed accessibility to public. This wasn't made public until \n\tLucene 5.1, but doing it for 4.8 since it is required by end users.\n•\tBREAKING: Lucene.Net.MMapDirectory: Removed UnmapHack/UNMAP_SUPPORTED features since these are not needed in .NET.\n•\tLucene.Net.Suggest.Analyzing.FreeTextSuggester: Changed to use Path.GetRandomFileName() instead of using random \n\tintegers to make the file name. Changed to delete the folder using System.IO.Directory.Delete and rearranged \n\ttry catch statements so the Lucene Directory disposes before deleting the OS directory.\n•\tLucene.Net.TestFramework.Index.BasePostingsFormatTestCase + Lucene.Net.Suggest.Analyzing \n\t(AnalyzingInfixSuggesterTest + TestFreeTextSuggester) + Lucene.Net.Tests.Index.TestCodecs: \n\tAdded using blocks to make the tests run more reliably.\n•\tBREAKING: Lucene.Net.Support.IO.FileSupport: Removed unused GetFiles(), GetLuceneIndexFiles(), and Sync() methods\n•\tLucene.Net.Support.IO.FileSupport: Made class static\n•\tLucene.Net.Support.IO.FileSupport: Fixed several issues with CreateTempFile() implementation\n•\tLucene.Net.Support.IO.FileSupport: Added GetCanonicalPath() method + tests\n•\tSwapped GetCanonicalPath() call into each of the locations where it was originally used in Lucene\n•\tLucene.Net + Lucene.Net.Facet + Lucene.Net.ICU: Added extension methods to Document class for adding\n\tnon-obsolete project-related Field types\n•\tBREAKING: Lucene.Net.Facet: De-nested DrillSidewaysResult from DrillSideways class\n•\tBREAKING: Lucene.Net.Support: Removed CharAt() method from StringCharSequenceWrapper\n•\tLucene.Net.Support: Added StringBuilderCharSequenceWrapper class and StringBuilder.ToCharSequence()\n\textension method.\n•[LUCENENET-592] - Lucene.Net.QueryParser.Flexible: Added an overload of type StringBuilder for all \n\tICharSequence-based methods and constructors.\n•\tBREAKING: Changed .NET Standard from 1.5 to 1.6 due to missing required API in Microsoft.Extensions.DependencyModel\n\nNew Feature\n•\tLucene.Net.Search.Filter: Added NewAnonymous() method for easy creation of anonymous classes via delegate methods.\n•\tLucene.Net.Search.DocIdSet: Added NewAnonymous() method for easy creation of anonymous classes via delegate methods.\n•\tLucene.Net.Search.Collector: Added Collector.NewAnonymous() method for easy creation of anonymous classes via delegate methods.\n•[LUCENENET-514] - Ported Lucene.Net.Analysis.SmartCn (Smart Chinese Analyzer)\n•[LUCENENET-569] - Ported Lucene.Net.Analysis.Phonetic\n•[LUCENENET-563] - Ported Lucene.Net.Demo (part of lucene-cli utility)\n•[LUCENENET-577] - Port Lock Stress Test CLI Utility (part of lucene-cli utility)\n•[LUCENENET-576] - Port IndexUpgrader CLI Utility (part of lucene-cli utility)\n•[LUCENENET-575] - Port CheckIndex CLI Utility (part of lucene-cli utility)\n•[LUCENENET-582] - Port Index Splitter CLI Utility (part of lucene-cli utility)\n•[LUCENENET-585] - Port High Freq Terms CLI Utility (part of lucene-cli utility)\n•[LUCENENET-584] - Port Get Term Info CLI Utility (part of lucene-cli utility)\n•[LUCENENET-581] - Port Compound File Extractor CLI Utility (part of lucene-cli utility)\n•[LUCENENET-586] - Port Index Merge Tool CLI Utility (part of lucene-cli utility)\n•[LUCENENET-583] - Port Multi-Pass Index Splitter CLI Utility (part of lucene-cli utility)\n•[LUCENENET-579] - Port Print Taxonomy Stats CLI Utility (part of lucene-cli utility)\n•[LUCENENET-578] - Port Lock Verify Server CLI Utility (part of lucene-cli utility)\n•[LUCENENET-588] - Create unified CLI tool (lucene-cli) to wrap all Lucene maintenance tools and demos for .NET\n•[LUCENENET-565 & Pull Request #209] - Port Lucene.Net.Replicator\n•[LUCENENET-567] - Port Lucene.Net.Analysis.Kuromoji\n•\tAdded Collation features of Lucene.Net.Analysis.ICU to Lucene.Net.ICU (as linked files).\n•[LUCENENET-564] - Port Lucene.Net.Benchmarks\n•\tAdded .NET Standard 2.0 support\n•\tAdded .NET Framework 4.5 support\n•\tCreated JavaDocToMarkdownConverter utility to assist with converting java docs to markdown docs for docfx.\n\n=================== Release 4.8.0-beta00004 =====================\n\nBug\n•\tAssemblyVersion and other metadata not being set on .NET Standard assemblies \n\tfor Lucene.Net.QueryParser and Lucene.Net.Expressions.\n•\tAttempting to load default codec in static Codec constructor causes failure\n\tin environments that don't allow Reflection to be used that early in the lifecycle.\n\tThis also improves performance when a custom DefaultCodecFactory is used.\n\n=================== Release 4.8.0-beta00003 =====================\n\nBug\n•\tBREAKING CHANGE: Fixed codec issue with incorrect calculation on x86 platforms that produced the wrong\n\tresult. This fix means this version or any future version may not be able to read indexes from any \n\tprior 4.8.x build that has indexes with binary doc values that were generated using an x86 platform.\n•\tAdded missing null check for values retrieved from table of Lucene45DocValuesProducer.\n•\tFixed assertion failure in FstEnum due to missing check for IsLast. This may have also produced\n\tincorrect runtime behavior.\n•\tFixed assertion failure in UnicodeUtil due to incorrect cast.\n\nImprovement\n•\tAdded overloads of Analyzer.NewAnonymous() that accept a delegate method for InitReader.\n•\tAdded constructor overloads on MMapDirectory, NIOFSDirectory, and SimpleFSDirectory that accept\n\tstring rather than DirectoryInfo to specify the directory.\n•\tAdded test to verify index compatibility with Lucene when using binary doc values.\n\n=================== Release 4.8.0-beta00002 =====================\n\nBug\n•[Pull Request #205] - Made FSDirectory stale files set synchronized\n\nImprovement\n•\tAdded InstallSDK and Restore tasks to the build script, and made them dependencies of the Test task\n\twhich will make the test more reliable from the CLI/CI server.\n•\tAdded ability to test from CLI in release.\n•\tUpdated syntax of build script to use the standard - (for short command) and -- (for long command),\n\ti.e. \"build -t\" or \"build --test\".\n\n=================== Release 4.8.0-beta00001 =====================\n\nBug\n•[LUCENENET-516] - ChainedFilter class not available in Lucene.net build 3.0.3\n•[LUCENENET-571] - Lucene.Net.QueryParser.Flexible Not Fully Implemented in .NET Core\n•[LUCENENET-558] - Some possible null reference exceptions in ListExtensions.cs\n•[LUCENENET-542] - Snowball Analyser - stemming issue\n\nImprovement\n•[LUCENENET-572] - Lucene.Net.Expressions - removed dependency on .NET Core configuration packages\n\nTask\n•[LUCENENET-540] - Make changes to Contrib\\Analyzers\\Miscellaneous to sync with Java 4.3 version\n\nMay 6th 2017 - Completed most of v4.8\nSeptember 6th 2014 - Started work on revamping the project code and structure towards a v4.8 release\n\n=================== 3.0.3 trunk (not yet released) =====================\n\nBug\n•[LUCENENET-54] - ArgumentOurOfRangeException caused by SF.Snowball.Ext.DanishStemmer\n•[LUCENENET-420] - String.StartsWith has culture in it.\n•[LUCENENET-423] - QueryParser differences between Java and .NET when parsing range queries involving dates\n•[LUCENENET-445] - Lucene.Net.Index.TestIndexWriter.TestFutureCommit() Fails\n•[LUCENENET-464] - The Lucene.Net.FastVectorHighligher.dll of the latest release 2.9.4 breaks any ASP.NET application\n•[LUCENENET-472] - Operator == on Parameter does not check for null arguments\n•[LUCENENET-473] - Fix linefeeds in more than 600 files\n•[LUCENENET-474] - Missing License Headers in trunk after 3.0.3 merge\n•[LUCENENET-475] - DanishStemmer doesn't work.\n•[LUCENENET-476] - ScoreDocs in TopDocs is ambiguos when using Visual Basic .Net\n•[LUCENENET-477] - NullReferenceException in ThreadLocal when Lucene.Net compiled for .Net 2.0\n•[LUCENENET-478] - Parts of QueryParser are outdated or weren't previously ported correctly\n•[LUCENENET-479] - QueryParser.SetEnablePositionIncrements(false) doesn't work\n•[LUCENENET-483] - Spatial Search skipping records when one location is close to origin, another one is away and radius is wider\n•[LUCENENET-484] - Some possibly major tests intermittently fail \n•[LUCENENET-485] - IndexOutOfRangeException in FrenchStemmer\n•[LUCENENET-490] - QueryParser is culture-sensitive\n•[LUCENENET-493] - Make lucene.net culture insensitive (like the java version)\n•[LUCENENET-494] - Port error in FieldCacheRangeFilter\n•[LUCENENET-495] - Use of DateTime.Now causes huge amount of System.Globalization.DaylightTime object allocations\n•[LUCENENET-500] - Lucene fails to run in medium trust ASP.NET Application\n\nImprovement\n•[LUCENENET-179] - SnowballFilter speed improvment\n•[LUCENENET-407] - Signing the assembly\n•[LUCENENET-408] - Mark assembly as CLS compliant; make AlreadyClosedException serializable\n•[LUCENENET-466] - optimisation for the GermanStemmer.vb‏\n•[LUCENENET-504] - FastVectorHighlighter - support for prefix query\n•[LUCENENET-506] - FastVectorHighlighter should use Query.ExtractTerms as fallback\n\nNew Feature\n•[LUCENENET-463] - Would like to be able to use a SimpleSpanFragmenter for extrcting whole sentances \n•[LUCENENET-481] - Port Contrib.MemoryIndex\n\nTask\n•[LUCENENET-446] - Make Lucene.Net CLS Compliant\n•[LUCENENET-471] - Remove Package.html and Overview.html artifacts\n•[LUCENENET-480] - Investigate what needs to happen to make both .NET 3.5 and 4.0 builds possible\n•[LUCENENET-487] - Remove Obsolete Members, Fields that are marked as obsolete and to be removed in 3.0\n•[LUCENENET-503] - Update binary names\n\nSub-task\n•[LUCENENET-468] - Implement the Dispose pattern properly in classes with Close\n•[LUCENENET-470] - Change Getxxx() and Setxxx() methods to .NET Properties\n\n\n=================== 2.9.4 trunk =====================\n\nBug fixes\n\n * LUCENENET-355  [LUCENE-2387]: Don't hang onto Fieldables from the last doc indexed,\n   in IndexWriter, nor the Reader in Tokenizer after close is\n   called. (digy) [Ruben Laguna, Uwe Schindler, Mike McCandless]\n\n\nChange Log Copied from Lucene \n======================= Release 2.9.2 2010-02-26 =======================\n\nBug fixes\n\n * LUCENE-2045: Fix silly FileNotFoundException hit if you enable\n   infoStream on IndexWriter and then add an empty document and commit\n   (Shai Erera via Mike McCandless)\n\n * LUCENE-2088: addAttribute() should only accept interfaces that\n   extend Attribute. (Shai Erera, Uwe Schindler)\n\n * LUCENE-2092: BooleanQuery was ignoring disableCoord in its hashCode\n   and equals methods, cause bad things to happen when caching\n   BooleanQueries.  (Chris Hostetter, Mike McCandless)\n\n * LUCENE-2095: Fixes: when two threads call IndexWriter.commit() at\n   the same time, it's possible for commit to return control back to\n   one of the threads before all changes are actually committed.\n   (Sanne Grinovero via Mike McCandless)\n\n * LUCENE-2166: Don't incorrectly keep warning about the same immense\n    term, when IndexWriter.infoStream is on.  (Mike McCandless)\n\n * LUCENE-2158: At high indexing rates, NRT reader could temporarily\n   lose deletions.  (Mike McCandless)\n  \n * LUCENE-2182: DEFAULT_ATTRIBUTE_FACTORY was failing to load\n   implementation class when interface was loaded by a different\n   class loader.  (Uwe Schindler, reported on java-user by Ahmed El-dawy)\n  \n * LUCENE-2257: Increase max number of unique terms in one segment to\n   termIndexInterval (default 128) * ~2.1 billion = ~274 billion.\n   (Tom Burton-West via Mike McCandless)\n\n * LUCENE-2260: Fixed AttributeSource to not hold a strong\n   reference to the Attribute/AttributeImpl classes which prevents\n   unloading of custom attributes loaded by other classloaders\n   (e.g. in Solr plugins).  (Uwe Schindler)\n \n * LUCENE-1941: Fix Min/MaxPayloadFunction returns 0 when\n   only one payload is present.  (Erik Hatcher, Mike McCandless\n   via Uwe Schindler)\n\n * LUCENE-2270: Queries consisting of all zero-boost clauses\n   (for example, text:foo^0) sorted incorrectly and produced\n   invalid docids. (yonik)\n\n * LUCENE-2422: Don't reuse byte[] in IndexInput/Output -- it gains\n   little performance, and ties up possibly large amounts of memory\n   for apps that index large docs.  (Ross Woolf via Mike McCandless)\n\nAPI Changes\n\n * LUCENE-2190: Added a new class CustomScoreProvider to function package\n   that can be subclassed to provide custom scoring to CustomScoreQuery.\n   The methods in CustomScoreQuery that did this before were deprecated\n   and replaced by a method getCustomScoreProvider(IndexReader) that\n   returns a custom score implementation using the above class. The change\n   is necessary with per-segment searching, as CustomScoreQuery is\n   a stateless class (like all other Queries) and does not know about\n   the currently searched segment. this API works similar to Filter's\n   getDocIdSet(IndexReader).  (Paul chez Jamespot via Mike McCandless,\n   Uwe Schindler)\n\n * LUCENE-2080: Deprecate Version.LUCENE_CURRENT, as using this constant\n   will cause backwards compatibility problems when upgrading Lucene. See\n   the Version javadocs for additional information.\n   (Robert Muir)\n\nOptimizations\n\n * LUCENE-2086: When resolving deleted terms, do so in term sort order\n   for better performance (Bogdan Ghidireac via Mike McCandless)\n\n * LUCENE-2258: Remove unneeded synchronization in FuzzyTermEnum.\n   (Uwe Schindler, Robert Muir)\n\nTest Cases\n\n * LUCENE-2114: Change TestFilteredSearch to test on multi-segment\n   index as well. (Simon Willnauer via Mike McCandless)\n\n * LUCENE-2211: Improves BaseTokenStreamTestCase to use a fake attribute\n   that checks if clearAttributes() was called correctly.\n   (Uwe Schindler, Robert Muir)\n\n * LUCENE-2207, LUCENE-2219: Improve BaseTokenStreamTestCase to check if\n   end() is implemented correctly.  (Koji Sekiguchi, Robert Muir)\n\nDocumentation\n\n * LUCENE-2114: Improve javadocs of Filter to call out that the\n   provided reader is per-segment (Simon Willnauer via Mike\n   McCandless)\n\n======================= Release 2.9.1 2009-11-06 =======================\n\nChanges in backwards compatibility policy\n\n * LUCENE-2002: Add required Version matchVersion argument when\n   constructing QueryParser or MultiFieldQueryParser and, default (as\n   of 2.9) enablePositionIncrements to true to match\n   StandardAnalyzer's 2.9 default (Uwe Schindler, Mike McCandless)\n\nBug fixes\n\n * LUCENE-1974: Fixed nasty bug in BooleanQuery (when it used\n   BooleanScorer for scoring), whereby some matching documents fail to\n   be collected.  (Fulin Tang via Mike McCandless)\n\n * LUCENE-1124: Make sure FuzzyQuery always matches the precise term.\n   (stefatwork@gmail.com via Mike McCandless)\n\n * LUCENE-1976: Fix IndexReader.isCurrent() to return the right thing\n   when the reader is a near real-time reader.  (Jake Mannix via Mike\n   McCandless)\n\n * LUCENE-1986: Fix NPE when scoring PayloadNearQuery (Peter Keegan,\n   Mark Miller via Mike McCandless)\n\n * LUCENE-1992: Fix thread hazard if a merge is committing just as an\n   exception occurs during sync (Uwe Schindler, Mike McCandless)\n\n * LUCENE-1995: Note in javadocs that IndexWriter.setRAMBufferSizeMB\n   cannot exceed 2048 MB, and throw IllegalArgumentException if it\n   does.  (Aaron McKee, Yonik Seeley, Mike McCandless)\n\n * LUCENE-2004: Fix Constants.LUCENE_MAIN_VERSION to not be inlined\n   by client code.  (Uwe Schindler)\n\n * LUCENE-2016: Replace illegal U+FFFF character with the replacement\n   char (U+FFFD) during indexing, to prevent silent index corruption.\n   (Peter Keegan, Mike McCandless)\n\nAPI Changes\n\n * Un-deprecate search(Weight weight, Filter filter, int n) from\n   Searchable interface (deprecated by accident).  (Uwe Schindler)\n\n * Un-deprecate o.a.l.util.Version constants.  (Mike McCandless)\n\n * LUCENE-1987: Un-deprecate some ctors of Token, as they will not\n   be removed in 3.0 and are still useful. Also add some missing\n   o.a.l.util.Version constants for enabling invalid acronym\n   settings in StandardAnalyzer to be compatible with the coming\n   Lucene 3.0.  (Uwe Schindler)\n\n * LUCENE-1973: Un-deprecate IndexSearcher.setDefaultFieldSortScoring,\n   to allow controlling per-IndexSearcher whether scores are computed\n   when sorting by field.  (Uwe Schindler, Mike McCandless)\n   \nDocumentation\n\n * LUCENE-1955: Fix Hits deprecation notice to point users in right\n   direction. (Mike McCandless, Mark Miller)\n   \n * Fix javadoc about score tracking done by search methods in Searcher \n   and IndexSearcher.  (Mike McCandless)\n\n * LUCENE-2008: Javadoc improvements for TokenStream/Tokenizer/Token\n   (Luke Nezda via Mike McCandless)\n\n======================= Release 2.9.0 2009-09-23 =======================\n\nChanges in backwards compatibility policy\n\n * LUCENE-1575: Searchable.search(Weight, Filter, int, Sort) no\n    longer computes a document score for each hit by default.  If\n    document score tracking is still needed, you can call\n    IndexSearcher.setDefaultFieldSortScoring(true, true) to enable\n    both per-hit and maxScore tracking; however, this is deprecated\n    and will be removed in 3.0.\n\n    Alternatively, use Searchable.search(Weight, Filter, Collector)\n    and pass in a TopFieldCollector instance, using the following code\n    sample:\n \n    <code>\n      TopFieldCollector tfc = TopFieldCollector.create(sort, numHits, fillFields, \n                                                       true /* trackDocScores */,\n                                                       true /* trackMaxScore */,\n                                                       false /* docsInOrder */);\n      searcher.search(query, tfc);\n      TopDocs results = tfc.topDocs();\n    </code>\n\n    Note that your Sort object cannot use SortField.AUTO when you\n    directly instantiate TopFieldCollector.\n\n    Also, the method search(Weight, Filter, Collector) was added to\n    the Searchable interface and the Searcher abstract class to\n    replace the deprecated HitCollector versions.  If you either\n    implement Searchable or extend Searcher, you should change your\n    code to implement this method.  If you already extend\n    IndexSearcher, no further changes are needed to use Collector.\n    \n    Finally, the values Float.NaN and Float.NEGATIVE_INFINITY are not\n    valid scores.  Lucene uses these values internally in certain\n    places, so if you have hits with such scores, it will cause\n    problems. (Shai Erera via Mike McCandless)\n\n * LUCENE-1687: All methods and parsers from the interface ExtendedFieldCache\n    have been moved into FieldCache. ExtendedFieldCache is now deprecated and\n    contains only a few declarations for binary backwards compatibility. \n    ExtendedFieldCache will be removed in version 3.0. Users of FieldCache and \n    ExtendedFieldCache will be able to plug in Lucene 2.9 without recompilation.\n    The auto cache (FieldCache.getAuto) is now deprecated. Due to the merge of\n    ExtendedFieldCache and FieldCache, FieldCache can now additionally return\n    long[] and double[] arrays in addition to int[] and float[] and StringIndex.\n    \n    The interface changes are only notable for users implementing the interfaces,\n    which was unlikely done, because there is no possibility to change\n    Lucene's FieldCache implementation.  (Grant Ingersoll, Uwe Schindler)\n    \n * LUCENE-1630, LUCENE-1771: Weight, previously an interface, is now an abstract \n    class. Some of the method signatures have changed, but it should be fairly\n    easy to see what adjustments must be made to existing code to sync up\n    with the new API. You can find more detail in the API Changes section.\n    \n    Going forward Searchable will be kept for convenience only and may\n    be changed between minor releases without any deprecation\n    process. It is not recommended that you implement it, but rather extend\n    Searcher.  \n    (Shai Erera, Chris Hostetter, Martin Ruckli, Mark Miller via Mike McCandless)\n\n * LUCENE-1422, LUCENE-1693: The new Attribute based TokenStream API (see below)\n    has some backwards breaks in rare cases. We did our best to make the \n    transition as easy as possible and you are not likely to run into any problems. \n    If your tokenizers still implement next(Token) or next(), the calls are \n    automatically wrapped. The indexer and query parser use the new API \n    (eg use incrementToken() calls). All core TokenStreams are implemented using \n    the new API. You can mix old and new API style TokenFilters/TokenStream. \n    Problems only occur when you have done the following:\n    You have overridden next(Token) or next() in one of the non-abstract core\n    TokenStreams/-Filters. These classes should normally be final, but some\n    of them are not. In this case, next(Token)/next() would never be called.\n    To fail early with a hard compile/runtime error, the next(Token)/next()\n    methods in these TokenStreams/-Filters were made final in this release.\n    (Michael Busch, Uwe Schindler)\n\n * LUCENE-1763: MergePolicy now requires an IndexWriter instance to\n    be passed upon instantiation. As a result, IndexWriter was removed\n    as a method argument from all MergePolicy methods. (Shai Erera via\n    Mike McCandless)\n    \n * LUCENE-1748: LUCENE-1001 introduced PayloadSpans, but this was a back\n    compat break and caused custom SpanQuery implementations to fail at runtime\n    in a variety of ways. this issue attempts to remedy things by causing\n    a compile time break on custom SpanQuery implementations and removing \n    the PayloadSpans class, with its functionality now moved to Spans. To\n    help in alleviating future back compat pain, Spans has been changed from\n    an interface to an abstract class.\n    (Hugh Cayless, Mark Miller)\n    \n * LUCENE-1808: Query.createWeight has been changed from protected to\n    public. this will be a back compat break if you have overridden this\n    method - but you are likely already affected by the LUCENE-1693 (make Weight \n    abstract rather than an interface) back compat break if you have overridden \n    Query.creatWeight, so we have taken the opportunity to make this change.\n    (Tim Smith, Shai Erera via Mark Miller)\n\n * LUCENE-1708 - IndexReader.document() no longer checks if the document is \n    deleted. You can call IndexReader.isDeleted(n) prior to calling document(n).\n    (Shai Erera via Mike McCandless)\n\n \nChanges in runtime behavior\n\n * LUCENE-1424: QueryParser now by default uses constant score auto\n    rewriting when it generates a WildcardQuery and PrefixQuery (it\n    already does so for TermRangeQuery, as well).  Call\n    setMultiTermRewriteMethod(MultiTermQuery.SCORING_BOOLEAN_QUERY_REWRITE)\n    to revert to slower BooleanQuery rewriting method.  (Mark Miller via Mike\n    McCandless)\n    \n * LUCENE-1575: As of 2.9, the core collectors as well as\n    IndexSearcher's search methods that return top N results, no\n    longer filter documents with scores <= 0.0. If you rely on this\n    functionality you can use PositiveScoresOnlyCollector like this:\n\n    <code>\n      TopDocsCollector tdc = new TopScoreDocCollector(10);\n      Collector c = new PositiveScoresOnlyCollector(tdc);\n      searcher.search(query, c);\n      TopDocs hits = tdc.topDocs();\n      ...\n    </code>\n\n * LUCENE-1604: IndexReader.norms(String field) is now allowed to\n    return null if the field has no norms, as long as you've\n    previously called IndexReader.setDisableFakeNorms(true).  this\n    setting now defaults to false (to preserve the fake norms back\n    compatible behavior) but in 3.0 will be hardwired to true.  (Shon\n    Vella via Mike McCandless).\n\n * LUCENE-1624: If you open IndexWriter with create=true and\n    autoCommit=false on an existing index, IndexWriter no longer\n    writes an empty commit when it's created.  (Paul Taylor via Mike\n    McCandless)\n\n * LUCENE-1593: When you call Sort() or Sort.setSort(String field,\n    boolean reverse), the resulting SortField array no longer ends\n    with SortField.FIELD_DOC (it was unnecessary as Lucene breaks ties\n    internally by docID). (Shai Erera via Michael McCandless)\n\n * LUCENE-1542: When the first token(s) have 0 position increment,\n    IndexWriter used to incorrectly record the position as -1, if no\n    payload is present, or Integer.MAX_VALUE if a payload is present.\n    this causes positional queries to fail to match.  The bug is now\n    fixed, but if your app relies on the buggy behavior then you must\n    call IndexWriter.setAllowMinus1Position().  That API is deprecated\n    so you must fix your application, and rebuild your index, to not\n    rely on this behavior by the 3.0 release of Lucene. (Jonathan\n    Mamou, Mark Miller via Mike McCandless)\n\n\n * LUCENE-1715: Finalizers have been removed from the 4 core classes\n    that still had them, since they will cause GC to take longer, thus\n    tying up memory for longer, and at best they mask buggy app code.\n    DirectoryReader (returned from IndexReader.open) & IndexWriter\n    previously released the write lock during finalize.\n    SimpleFSDirectory.FSIndexInput closed the descriptor in its\n    finalizer, and NativeFSLock released the lock.  It's possible\n    applications will be affected by this, but only if the application\n    is failing to close reader/writers.  (Brian Groose via Mike\n    McCandless)\n\n * LUCENE-1717: Fixed IndexWriter to account for RAM usage of\n    buffered deletions.  (Mike McCandless)\n\n * LUCENE-1727: Ensure that fields are stored & retrieved in the\n    exact order in which they were added to the document.  this was\n    true in all Lucene releases before 2.3, but was broken in 2.3 and\n    2.4, and is now fixed in 2.9.  (Mike McCandless)\n\n * LUCENE-1678: The addition of Analyzer.reusableTokenStream\n    accidentally broke back compatibility of external analyzers that\n    subclassed core analyzers that implemented tokenStream but not\n    reusableTokenStream.  this is now fixed, such that if\n    reusableTokenStream is invoked on such a subclass, that method\n    will forcefully fallback to tokenStream.  (Mike McCandless)\n    \n * LUCENE-1801: Token.clear() and Token.clearNoTermBuffer() now also clear\n    startOffset, endOffset and type. this is not likely to affect any\n    Tokenizer chains, as Tokenizers normally always set these three values.\n    this change was made to be conform to the new AttributeImpl.clear() and\n    AttributeSource.clearAttributes() to work identical for Token as one for all\n    AttributeImpl and the 6 separate AttributeImpls. (Uwe Schindler, Michael Busch)\n\n * LUCENE-1483: When searching over multiple segments, a new Scorer is now created \n    for each segment. Searching has been telescoped out a level and IndexSearcher now\n    operates much like MultiSearcher does. The Weight is created only once for the top \n    level Searcher, but each Scorer is passed a per-segment IndexReader. this will \n    result in doc ids in the Scorer being internal to the per-segment IndexReader. It \n    has always been outside of the API to count on a given IndexReader to contain every \n    doc id in the index - and if you have been ignoring MultiSearcher in your custom code \n    and counting on this fact, you will find your code no longer works correctly. If a \n    custom Scorer implementation uses any caches/filters that rely on being based on the \n    top level IndexReader, it will need to be updated to correctly use contextless \n    caches/filters eg you can't count on the IndexReader to contain any given doc id or \n    all of the doc ids. (Mark Miller, Mike McCandless)\n\n * LUCENE-1846: DateTools now uses the US locale to format the numbers in its\n    date/time strings instead of the default locale. For most locales there will\n    be no change in the index format, as DateFormatSymbols is using ASCII digits.\n    The usage of the US locale is important to guarantee correct ordering of\n    generated terms.  (Uwe Schindler)\n\n * LUCENE-1860: MultiTermQuery now defaults to\n    CONSTANT_SCORE_AUTO_REWRITE_DEFAULT rewrite method (previously it\n    was SCORING_BOOLEAN_QUERY_REWRITE).  this means that PrefixQuery\n    and WildcardQuery will now produce constant score for all matching\n    docs, equal to the boost of the query.  (Mike McCandless)\n\nAPI Changes\n\n * LUCENE-1419: Add expert API to set custom indexing chain. this API is \n   package-protected for now, so we don't have to officially support it.\n   Yet, it will give us the possibility to try out different consumers\n   in the chain. (Michael Busch)\n\n * LUCENE-1427: DocIdSet.iterator() is now allowed to throw\n   IOException.  (Paul Elschot, Mike McCandless)\n\n * LUCENE-1422, LUCENE-1693: New TokenStream API that uses a new class called \n   AttributeSource instead of the Token class, which is now a utility class that\n   holds common Token attributes. All attributes that the Token class had have \n   been moved into separate classes: TermAttribute, OffsetAttribute, \n   PositionIncrementAttribute, PayloadAttribute, TypeAttribute and FlagsAttribute. \n   The new API is much more flexible; it allows to combine the Attributes \n   arbitrarily and also to define custom Attributes. The new API has the same \n   performance as the old next(Token) approach. For conformance with this new \n   API Tee-/SinkTokenizer was deprecated and replaced by a new TeeSinkTokenFilter. \n   (Michael Busch, Uwe Schindler; additional contributions and bug fixes by \n   Daniel Shane, Doron Cohen)\n\n * LUCENE-1467: Add nextDoc() and next(int) methods to OpenBitSetIterator.\n   These methods can be used to avoid additional calls to doc(). \n   (Michael Busch)\n\n * LUCENE-1468: Deprecate Directory.list(), which sometimes (in\n   FSDirectory) filters out files that don't look like index files, in\n   favor of new Directory.listAll(), which does no filtering.  Also,\n   listAll() will never return null; instead, it throws an IOException\n   (or subclass).  Specifically, FSDirectory.listAll() will throw the\n   newly added NoSuchDirectoryException if the directory does not\n   exist.  (Marcel Reutegger, Mike McCandless)\n\n * LUCENE-1546: Add IndexReader.flush(Map commitUserData), allowing\n   you to record an opaque commitUserData (maps String -> String) into\n   the commit written by IndexReader.  this matches IndexWriter's\n   commit methods.  (Jason Rutherglen via Mike McCandless)\n\n * LUCENE-652: Added org.apache.lucene.document.CompressionTools, to\n   enable compressing & decompressing binary content, external to\n   Lucene's indexing.  Deprecated Field.Store.COMPRESS.\n\n * LUCENE-1561: Renamed Field.omitTf to Field.omitTermFreqAndPositions\n    (Otis Gospodnetic via Mike McCandless)\n  \n * LUCENE-1500: Added new InvalidTokenOffsetsException to Highlighter methods\n    to denote issues when offsets in TokenStream tokens exceed the length of the\n    provided text.  (Mark Harwood)\n    \n * LUCENE-1575, LUCENE-1483: HitCollector is now deprecated in favor of \n    a new Collector abstract class. For easy migration, people can use\n    HitCollectorWrapper which translates (wraps) HitCollector into\n    Collector. Note that this class is also deprecated and will be\n    removed when HitCollector is removed.  Also TimeLimitedCollector\n    is deprecated in favor of the new TimeLimitingCollector which\n    extends Collector.  (Shai Erera, Mark Miller, Mike McCandless)\n\n * LUCENE-1592: The method TermsEnum.skipTo() was deprecated, because\n    it is used nowhere in core/contrib and there is only a very ineffective\n    default implementation available. If you want to position a TermEnum\n    to another Term, create a new one using IndexReader.terms(Term).\n    (Uwe Schindler)\n\n * LUCENE-1621: MultiTermQuery.getTerm() has been deprecated as it does\n    not make sense for all subclasses of MultiTermQuery. Check individual\n    subclasses to see if they support getTerm().  (Mark Miller)\n\n * LUCENE-1636: Make TokenFilter.input final so it's set only\n    once. (Wouter Heijke, Uwe Schindler via Mike McCandless).\n\n * LUCENE-1658, LUCENE-1451: Renamed FSDirectory to SimpleFSDirectory\n    (but left an FSDirectory base class).  Added an FSDirectory.open\n    static method to pick a good default FSDirectory implementation\n    given the OS. FSDirectories should now be instantiated using\n    FSDirectory.open or with public constructors rather than\n    FSDirectory.getDirectory(), which has been deprecated.\n    (Michael McCandless, Uwe Schindler, yonik)\n\n * LUCENE-1665: Deprecate SortField.AUTO, to be removed in 3.0.\n    Instead, when sorting by field, the application should explicitly\n    state the type of the field.  (Mike McCandless)\n\n * LUCENE-1660: StopFilter, StandardAnalyzer, StopAnalyzer now\n    require up front specification of enablePositionIncrement (Mike\n    McCandless)\n\n * LUCENE-1614: DocIdSetIterator's next() and skipTo() were deprecated in favor\n    of the new nextDoc() and advance(). The new methods return the doc Id they \n    landed on, saving an extra call to doc() in most cases.\n    For easy migration of the code, you can change the calls to next() to \n    nextDoc() != DocIdSetIterator.NO_MORE_DOCS and similarly for skipTo(). \n    However it is advised that you take advantage of the returned doc ID and not \n    call doc() following those two.\n    Also, doc() was deprecated in favor of docID(). docID() should return -1 or \n    NO_MORE_DOCS if nextDoc/advance were not called yet, or NO_MORE_DOCS if the \n    iterator has exhausted. Otherwise it should return the current doc ID.\n    (Shai Erera via Mike McCandless)\n\n * LUCENE-1672: All ctors/opens and other methods using String/File to\n    specify the directory in IndexReader, IndexWriter, and IndexSearcher\n    were deprecated. You should instantiate the Directory manually before\n    and pass it to these classes (LUCENE-1451, LUCENE-1658).\n    (Uwe Schindler)\n\n * LUCENE-1407: Move RemoteSearchable, RemoteCachingWrapperFilter out\n    of Lucene's core into new contrib/remote package.  Searchable no\n    longer extends java.rmi.Remote (Simon Willnauer via Mike\n    McCandless)\n\n * LUCENE-1677: The global property\n    org.apache.lucene.SegmentReader.class, and\n    ReadOnlySegmentReader.class are now deprecated, to be removed in\n    3.0.  src/gcj/* has been removed. (Earwin Burrfoot via Mike\n    McCandless)\n\n * LUCENE-1673: Deprecated NumberTools in favour of the new\n    NumericRangeQuery and its new indexing format for numeric or\n    date values.  (Uwe Schindler)\n    \n * LUCENE-1630, LUCENE-1771: Weight is now an abstract class, and adds\n    a scorer(IndexReader, boolean /* scoreDocsInOrder */, boolean /*\n    topScorer */) method instead of scorer(IndexReader). IndexSearcher uses \n    this method to obtain a scorer matching the capabilities of the Collector \n    wrt orderedness of docIDs. Some Scorers (like BooleanScorer) are much more\n    efficient if out-of-order documents scoring is allowed by a Collector.  \n    Collector must now implement acceptsDocsOutOfOrder. If you write a \n    Collector which does not care about doc ID orderness, it is recommended \n    that you return true.  Weight has a scoresDocsOutOfOrder method, which by \n    default returns false.  If you create a Weight which will score documents \n    out of order if requested, you should override that method to return true. \n    BooleanQuery's setAllowDocsOutOfOrder and getAllowDocsOutOfOrder have been \n    deprecated as they are not needed anymore. BooleanQuery will now score docs \n    out of order when used with a Collector that can accept docs out of order.\n    Finally, Weight#explain now takes a sub-reader and sub-docID, rather than\n    a top level reader and docID.\n    (Shai Erera, Chris Hostetter, Martin Ruckli, Mark Miller via Mike McCandless)\n \t\n * LUCENE-1466, LUCENE-1906: Added CharFilter and MappingCharFilter, which allows\n    chaining & mapping of characters before tokenizers run. CharStream (subclass of\n    Reader) is the base class for custom java.io.Reader's, that support offset\n    correction. Tokenizers got an additional method correctOffset() that is passed\n    down to the underlying CharStream if input is a subclass of CharStream/-Filter.\n    (Koji Sekiguchi via Mike McCandless, Uwe Schindler)\n\n * LUCENE-1703: Add IndexWriter.waitForMerges.  (Tim Smith via Mike\n    McCandless)\n\n * LUCENE-1625: CheckIndex's programmatic API now returns separate\n    classes detailing the status of each component in the index, and\n    includes more detailed status than previously.  (Tim Smith via\n    Mike McCandless)\n\n * LUCENE-1713: Deprecated RangeQuery and RangeFilter and renamed to\n    TermRangeQuery and TermRangeFilter. TermRangeQuery is in constant\n    score auto rewrite mode by default. The new classes also have new\n    ctors taking field and term ranges as Strings (see also\n    LUCENE-1424).  (Uwe Schindler)\n\n * LUCENE-1609: The termInfosIndexDivisor must now be specified\n    up-front when opening the IndexReader.  Attempts to call\n    IndexReader.setTermInfosIndexDivisor will hit an\n    UnsupportedOperationException.  this was done to enable removal of\n    all synchronization in TermInfosReader, which previously could\n    cause threads to pile up in certain cases. (Dan Rosher via Mike\n    McCandless)\n    \n * LUCENE-1688: Deprecate static final String stop word array in and \n    StopAnalzyer and replace it with an immutable implementation of \n    CharArraySet.  (Simon Willnauer via Mark Miller)\n\n * LUCENE-1742: SegmentInfos, SegmentInfo and SegmentReader have been\n    made public as expert, experimental APIs.  These APIs may suddenly\n    change from release to release (Jason Rutherglen via Mike\n    McCandless).\n    \n * LUCENE-1754: QueryWeight.scorer() can return null if no documents\n    are going to be matched by the query. Similarly,\n    Filter.getDocIdSet() can return null if no documents are going to\n    be accepted by the Filter. Note that these 'can' return null,\n    however they don't have to and can return a Scorer/DocIdSet which\n    does not match / reject all documents.  this is already the\n    behavior of some QueryWeight/Filter implementations, and is\n    documented here just for emphasis. (Shai Erera via Mike\n    McCandless)\n\n * LUCENE-1705: Added IndexWriter.deleteAllDocuments.  (Tim Smith via\n    Mike McCandless)\n\n * LUCENE-1460: Changed TokenStreams/TokenFilters in contrib to\n    use the new TokenStream API. (Robert Muir, Michael Busch)\n\n * LUCENE-1748: LUCENE-1001 introduced PayloadSpans, but this was a back\n    compat break and caused custom SpanQuery implementations to fail at runtime\n    in a variety of ways. this issue attempts to remedy things by causing\n    a compile time break on custom SpanQuery implementations and removing \n    the PayloadSpans class, with its functionality now moved to Spans. To\n    help in alleviating future back compat pain, Spans has been changed from\n    an interface to an abstract class.\n    (Hugh Cayless, Mark Miller)\n    \n * LUCENE-1808: Query.createWeight has been changed from protected to\n    public. (Tim Smith, Shai Erera via Mark Miller)\n\n * LUCENE-1826: Add constructors that take AttributeSource and\n    AttributeFactory to all Tokenizer implementations.\n    (Michael Busch)\n    \n * LUCENE-1847: Similarity#idf for both a Term and Term Collection have\n    been deprecated. New versions that return an IDFExplanation have been\n    added.  (Yasoja Seneviratne, Mike McCandless, Mark Miller)\n    \n * LUCENE-1877: Made NativeFSLockFactory the default for\n    the new FSDirectory API (open(), FSDirectory subclass ctors).\n    All FSDirectory system properties were deprecated and all lock\n    implementations use no lock prefix if the locks are stored inside\n    the index directory. Because the deprecated String/File ctors of\n    IndexWriter and IndexReader (LUCENE-1672) and FSDirectory.getDirectory()\n    still use the old SimpleFSLockFactory and the new API\n    NativeFSLockFactory, we strongly recommend not to mix deprecated\n    and new API. (Uwe Schindler, Mike McCandless)\n\n * LUCENE-1911: Added a new method isCacheable() to DocIdSet. this method\n    should return true, if the underlying implementation does not use disk\n    I/O and is fast enough to be directly cached by CachingWrapperFilter.\n    OpenBitSet, SortedVIntList, and DocIdBitSet are such candidates.\n    The default implementation of the abstract DocIdSet class returns false.\n    In this case, CachingWrapperFilter copies the DocIdSetIterator into an\n    OpenBitSet for caching.  (Uwe Schindler, Thomas Becker)\n\nBug fixes\n\n * LUCENE-1415: MultiPhraseQuery has incorrect hashCode() and equals()\n   implementation - Leads to Solr Cache misses. \n   (Todd Feak, Mark Miller via yonik)\n\n * LUCENE-1327: Fix TermSpans#skipTo() to behave as specified in javadocs\n   of Terms#skipTo(). (Michael Busch)\n\n * LUCENE-1573: Do not ignore InterruptedException (caused by\n   Thread.interrupt()) nor enter deadlock/spin loop. Now, an interrupt\n   will cause a RuntimeException to be thrown.  In 3.0 we will change\n   public APIs to throw InterruptedException.  (Jeremy Volkman via\n   Mike McCandless)\n\n * LUCENE-1590: Fixed stored-only Field instances do not change the\n   value of omitNorms, omitTermFreqAndPositions in FieldInfo; when you\n   retrieve such fields they will now have omitNorms=true and\n   omitTermFreqAndPositions=false (though these values are unused).\n   (Uwe Schindler via Mike McCandless)\n\n * LUCENE-1587: RangeQuery#equals() could consider a RangeQuery\n   without a collator equal to one with a collator.\n   (Mark Platvoet via Mark Miller) \n\n * LUCENE-1600: Don't call String.intern unnecessarily in some cases\n   when loading documents from the index.  (P Eger via Mike\n   McCandless)\n\n * LUCENE-1611: Fix case where OutOfMemoryException in IndexWriter\n   could cause \"infinite merging\" to happen.  (Christiaan Fluit via\n   Mike McCandless)\n\n * LUCENE-1623: Properly handle back-compatibility of 2.3.x indexes that\n   contain field names with non-ascii characters.  (Mike Streeton via\n   Mike McCandless)\n\n * LUCENE-1593: MultiSearcher and ParallelMultiSearcher did not break ties (in \n   sort) by doc Id in a consistent manner (i.e., if Sort.FIELD_DOC was used vs. \n   when it wasn't). (Shai Erera via Michael McCandless)\n\n * LUCENE-1647: Fix case where IndexReader.undeleteAll would cause\n    the segment's deletion count to be incorrect. (Mike McCandless)\n\n * LUCENE-1542: When the first token(s) have 0 position increment,\n    IndexWriter used to incorrectly record the position as -1, if no\n    payload is present, or Integer.MAX_VALUE if a payload is present.\n    this causes positional queries to fail to match.  The bug is now\n    fixed, but if your app relies on the buggy behavior then you must\n    call IndexWriter.setAllowMinus1Position().  That API is deprecated\n    so you must fix your application, and rebuild your index, to not\n    rely on this behavior by the 3.0 release of Lucene. (Jonathan\n    Mamou, Mark Miller via Mike McCandless)\n\n * LUCENE-1658: Fixed MMapDirectory to correctly throw IOExceptions\n    on EOF, removed numeric overflow possibilities and added support\n    for a hack to unmap the buffers on closing IndexInput.\n    (Uwe Schindler)\n    \n * LUCENE-1681: Fix infinite loop caused by a call to DocValues methods \n    getMinValue, getMaxValue, getAverageValue. (Simon Willnauer via Mark Miller)\n\n * LUCENE-1599: Add clone support for SpanQuerys. SpanRegexQuery counts\n    on this functionality and does not work correctly without it.\n    (Billow Gao, Mark Miller)\n\n * LUCENE-1718: Fix termInfosIndexDivisor to carry over to reopened\n    readers (Mike McCandless)\n    \n * LUCENE-1583: SpanOrQuery skipTo() doesn't always move forwards as Spans\n\tdocumentation indicates it should.  (Moti Nisenson via Mark Miller)\n\n * LUCENE-1566: Sun JVM Bug\n    http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6478546 causes\n    invalid OutOfMemoryError when reading too many bytes at once from\n    a file on 32bit JVMs that have a large maximum heap size.  this\n    fix adds set/getReadChunkSize to FSDirectory so that large reads\n    are broken into chunks, to work around this JVM bug.  On 32bit\n    JVMs the default chunk size is 100 MB; on 64bit JVMs, which don't\n    show the bug, the default is Integer.MAX_VALUE. (Simon Willnauer\n    via Mike McCandless)\n    \n * LUCENE-1448: Added TokenStream.end() to perform end-of-stream\n    operations (ie to return the end offset of the tokenization).  \n    this is important when multiple fields with the same name are added\n    to a document, to ensure offsets recorded in term vectors for all \n    of the instances are correct.  \n    (Mike McCandless, Mark Miller, Michael Busch)\n\n * LUCENE-1805: CloseableThreadLocal did not allow a null Object in get(), \n    although it does allow it in set(Object). Fix get() to not assert the object\n    is not null. (Shai Erera via Mike McCandless)\n    \n * LUCENE-1801: Changed all Tokenizers or TokenStreams in core/contrib)\n    that are the source of Tokens to always call\n    AttributeSource.clearAttributes() first. (Uwe Schindler)\n    \n * LUCENE-1819: MatchAllDocsQuery.toString(field) should produce output\n    that is parsable by the QueryParser.  (John Wang, Mark Miller)\n\n * LUCENE-1836: Fix localization bug in the new query parser and add \n    new LocalizedTestCase as base class for localization junit tests.\n    (Robert Muir, Uwe Schindler via Michael Busch)\n\n * LUCENE-1847: PhraseQuery/TermQuery/SpanQuery use IndexReader specific stats \n    in their Weight#explain methods - these stats should be corpus wide.\n    (Yasoja Seneviratne, Mike McCandless, Mark Miller)\n\n * LUCENE-1885: Fix the bug that NativeFSLock.isLocked() did not work,\n    if the lock was obtained by another NativeFSLock(Factory) instance.\n    Because of this IndexReader.isLocked() and IndexWriter.isLocked() did\n    not work correctly.  (Uwe Schindler)\n\n * LUCENE-1899: Fix O(N^2) CPU cost when setting docIDs in order in an\n    OpenBitSet, due to an inefficiency in how the underlying storage is\n    reallocated.  (Nadav Har'El via Mike McCandless)\n\n * LUCENE-1918: Fixed cases where a ParallelReader would\n   generate exceptions on being passed to\n   IndexWriter.addIndexes(IndexReader[]).  First case was when the\n   ParallelReader was empty.  Second case was when the ParallelReader\n   used to contain documents with TermVectors, but all such documents\n   have been deleted. (Christian Kohlschütter via Mike McCandless)\n\nNew features\n\n * LUCENE-1411: Added expert API to open an IndexWriter on a prior\n    commit, obtained from IndexReader.listCommits.  this makes it\n    possible to rollback changes to an index even after you've closed\n    the IndexWriter that made the changes, assuming you are using an\n    IndexDeletionPolicy that keeps past commits around.  this is useful\n    when building transactional support on top of Lucene.  (Mike\n    McCandless)\n\n * LUCENE-1382: Add an optional arbitrary Map (String -> String)\n    \"commitUserData\" to IndexWriter.commit(), which is stored in the\n    segments file and is then retrievable via\n    IndexReader.getCommitUserData instance and static methods.\n    (Shalin Shekhar Mangar via Mike McCandless)\n\n * LUCENE-1420: Similarity now has a computeNorm method that allows\n    custom Similarity classes to override how norm is computed.  It's\n    provided a FieldInvertState instance that contains details from\n    inverting the field.  The default impl is boost *\n    lengthNorm(numTerms), to be backwards compatible.  Also added\n    {set/get}DiscountOverlaps to DefaultSimilarity, to control whether\n    overlapping tokens (tokens with 0 position increment) should be\n    counted in lengthNorm.  (Andrzej Bialecki via Mike McCandless)\n\n * LUCENE-1424: Moved constant score query rewrite capability into\n    MultiTermQuery, allowing TermRangeQuery, PrefixQuery and WildcardQuery\n    to switch between constant-score rewriting or BooleanQuery\n    expansion rewriting via a new setRewriteMethod method.\n    Deprecated ConstantScoreRangeQuery (Mark Miller via Mike\n    McCandless)\n\n * LUCENE-1461: Added FieldCacheRangeFilter, a RangeFilter for\n    single-term fields that uses FieldCache to compute the filter.  If\n    your documents all have a single term for a given field, and you\n    need to create many RangeFilters with varying lower/upper bounds,\n    then this is likely a much faster way to create the filters than\n    RangeFilter.  FieldCacheRangeFilter allows ranges on all data types,\n    FieldCache supports (term ranges, byte, short, int, long, float, double).\n    However, it comes at the expense of added RAM consumption and slower\n    first-time usage due to populating the FieldCache.  It also does not\n    support collation  (Tim Sturge, Matt Ericson via Mike McCandless and\n    Uwe Schindler)\n\n * LUCENE-1296: add protected method CachingWrapperFilter.docIdSetToCache \n    to allow subclasses to choose which DocIdSet implementation to use\n    (Paul Elschot via Mike McCandless)\n    \n * LUCENE-1390: Added ASCIIFoldingFilter, a Filter that converts \n    alphabetic, numeric, and symbolic Unicode characters which are not in \n    the first 127 ASCII characters (the \"Basic Latin\" Unicode block) into \n    their ASCII equivalents, if one exists. ISOLatin1AccentFilter, which\n    handles a subset of this filter, has been deprecated.\n    (Andi Vajda, Steven Rowe via Mark Miller)\n\n * LUCENE-1478: Added new SortField constructor allowing you to\n    specify a custom FieldCache parser to generate numeric values from\n    terms for a field.  (Uwe Schindler via Mike McCandless)\n\n * LUCENE-1528: Add support for Ideographic Space to the queryparser.\n    (Luis Alves via Michael Busch)\n\n * LUCENE-1487: Added FieldCacheTermsFilter, to filter by multiple\n    terms on single-valued fields.  The filter loads the FieldCache\n    for the field the first time it's called, and subsequent usage of\n    that field, even with different Terms in the filter, are fast.\n    (Tim Sturge, Shalin Shekhar Mangar via Mike McCandless).\n\n * LUCENE-1314: Add clone(), clone(boolean readOnly) and\n    reopen(boolean readOnly) to IndexReader.  Cloning an IndexReader\n    gives you a new reader which you can make changes to (deletions,\n    norms) without affecting the original reader.  Now, with clone or\n    reopen you can change the readOnly of the original reader.  (Jason\n    Rutherglen, Mike McCandless)\n\n * LUCENE-1506: Added FilteredDocIdSet, an abstract class which you\n    subclass to implement the \"match\" method to accept or reject each\n    docID.  Unlike ChainedFilter (under contrib/misc),\n    FilteredDocIdSet never requires you to materialize the full\n    bitset.  Instead, match() is called on demand per docID.  (John\n    Wang via Mike McCandless)\n\n * LUCENE-1398: Add ReverseStringFilter to contrib/analyzers, a filter\n    to reverse the characters in each token.  (Koji Sekiguchi via yonik)\n\n * LUCENE-1551: Add expert IndexReader.reopen(IndexCommit) to allow\n    efficiently opening a new reader on a specific commit, sharing\n    resources with the original reader.  (Torin Danil via Mike\n    McCandless)\n\n * LUCENE-1434: Added org.apache.lucene.util.IndexableBinaryStringTools,\n    to encode byte[] as String values that are valid terms, and\n    maintain sort order of the original byte[] when the bytes are\n    interpreted as unsigned.  (Steven Rowe via Mike McCandless)\n\n * LUCENE-1543: Allow MatchAllDocsQuery to optionally use norms from\n    a specific fields to set the score for a document.  (Karl Wettin\n    via Mike McCandless)\n\n * LUCENE-1586: Add IndexReader.getUniqueTermCount().  (Mike\n    McCandless via Derek)\n\n * LUCENE-1516: Added \"near real-time search\" to IndexWriter, via a\n    new expert getReader() method.  this method returns a reader that\n    searches the full index, including any uncommitted changes in the\n    current IndexWriter session.  this should result in a faster\n    turnaround than the normal approach of commiting the changes and\n    then reopening a reader.  (Jason Rutherglen via Mike McCandless)\n\n * LUCENE-1603: Added new MultiTermQueryWrapperFilter, to wrap any\n    MultiTermQuery as a Filter.  Also made some improvements to\n    MultiTermQuery: return DocIdSet.EMPTY_DOCIDSET if there are no\n    terms in the enum; track the total number of terms it visited\n    during rewrite (getTotalNumberOfTerms).  FilteredTermEnum is also\n    more friendly to subclassing.  (Uwe Schindler via Mike McCandless)\n\n * LUCENE-1605: Added BitVector.subset().  (Jeremy Volkman via Mike\n    McCandless)\n    \n * LUCENE-1618: Added FileSwitchDirectory that enables files with\n    specified extensions to be stored in a primary directory and the\n    rest of the files to be stored in the secondary directory.  For\n    example, this can be useful for the large doc-store (stored\n    fields, term vectors) files in FSDirectory and the rest of the\n    index files in a RAMDirectory. (Jason Rutherglen via Mike\n    McCandless)\n\n * LUCENE-1494: Added FieldMaskingSpanQuery which can be used to\n    cross-correlate Spans from different fields.\n    (Paul Cowan and Chris Hostetter)\n\n * LUCENE-1634: Add calibrateSizeByDeletes to LogMergePolicy, to take\n    deletions into account when considering merges.  (Yasuhiro Matsuda\n    via Mike McCandless)\n\n * LUCENE-1550: Added new n-gram based String distance measure for spell checking.\n    See the Javadocs for NGramDistance.java for a reference paper on why\n    this is helpful (Tom Morton via Grant Ingersoll)\n\n * LUCENE-1470, LUCENE-1582, LUCENE-1602, LUCENE-1673, LUCENE-1701, LUCENE-1712:\n    Added NumericRangeQuery and NumericRangeFilter, a fast alternative to\n    RangeQuery/RangeFilter for numeric searches. They depend on a specific\n    structure of terms in the index that can be created by indexing\n    using the new NumericField or NumericTokenStream classes. NumericField\n    can only be used for indexing and optionally stores the values as\n    string representation in the doc store. Documents returned from\n    IndexReader/IndexSearcher will return only the String value using\n    the standard Fieldable interface. NumericFields can be sorted on\n    and loaded into the FieldCache.  (Uwe Schindler, Yonik Seeley,\n    Mike McCandless)\n\n * LUCENE-1405: Added support for Ant resource collections in contrib/ant\n    <index> task.  (Przemyslaw Sztoch via Erik Hatcher)\n\n * LUCENE-1699: Allow setting a TokenStream on Field/Fieldable for indexing\n    in conjunction with any other ways to specify stored field values,\n    currently binary or string values.  (yonik)\n    \n * LUCENE-1701: Made the standard FieldCache.Parsers public and added\n    parsers for fields generated using NumericField/NumericTokenStream.\n    All standard parsers now also implement Serializable and enforce\n    their singleton status.  (Uwe Schindler, Mike McCandless)\n    \n * LUCENE-1741: User configurable maximum chunk size in MMapDirectory.\n    On 32 bit platforms, the address space can be very fragmented, so\n    one big ByteBuffer for the whole file may not fit into address space.\n    (Eks Dev via Uwe Schindler)\n\n * LUCENE-1644: Enable 4 rewrite modes for queries deriving from\n    MultiTermQuery (WildcardQuery, PrefixQuery, TermRangeQuery,\n    NumericRangeQuery): CONSTANT_SCORE_FILTER_REWRITE first creates a\n    filter and then assigns constant score (boost) to docs;\n    CONSTANT_SCORE_BOOLEAN_QUERY_REWRITE create a BooleanQuery but\n    uses a constant score (boost); SCORING_BOOLEAN_QUERY_REWRITE also\n    creates a BooleanQuery but keeps the BooleanQuery's scores;\n    CONSTANT_SCORE_AUTO_REWRITE tries to pick the most performant\n    constant-score rewrite method.  (Mike McCandless)\n    \n * LUCENE-1448: Added TokenStream.end(), to perform end-of-stream\n    operations.  this is currently used to fix offset problems when \n    multiple fields with the same name are added to a document.\n    (Mike McCandless, Mark Miller, Michael Busch)\n \n * LUCENE-1776: Add an option to not collect payloads for an ordered\n    SpanNearQuery. Payloads were not lazily loaded in this case as\n    the javadocs implied. If you have payloads and want to use an ordered\n    SpanNearQuery that does not need to use the payloads, you can\n    disable loading them with a new constructor switch.  (Mark Miller)\n\n * LUCENE-1341: Added PayloadNearQuery to enable SpanNearQuery functionality\n    with payloads (Peter Keegan, Grant Ingersoll, Mark Miller)\n\n * LUCENE-1790: Added PayloadTermQuery to enable scoring of payloads\n    based on the maximum payload seen for a document.\n    Slight refactoring of Similarity and other payload queries (Grant Ingersoll, Mark Miller)\n\n * LUCENE-1749: Addition of FieldCacheSanityChecker utility, and\n    hooks to use it in all existing Lucene Tests.  this class can\n    be used by any application to inspect the FieldCache and provide\n    diagnostic information about the possibility of inconsistent\n    FieldCache usage.  Namely: FieldCache entries for the same field\n    with different datatypes or parsers; and FieldCache entries for\n    the same field in both a reader, and one of it's (descendant) sub\n    readers. \n    (Chris Hostetter, Mark Miller)\n\n * LUCENE-1789: Added utility class\n    oal.search.function.MultiValueSource to ease the transition to\n    segment based searching for any apps that directly call\n    oal.search.function.* APIs.  this class wraps any other\n    ValueSource, but takes care when composite (multi-segment) are\n    passed to not double RAM usage in the FieldCache.  (Chris\n    Hostetter, Mark Miller, Mike McCandless)\n   \nOptimizations\n\n * LUCENE-1427: Fixed QueryWrapperFilter to not waste time computing\n    scores of the query, since they are just discarded.  Also, made it\n    more efficient (single pass) by not creating & populating an\n    intermediate OpenBitSet (Paul Elschot, Mike McCandless)\n\n * LUCENE-1443: Performance improvement for OpenBitSetDISI.inPlaceAnd()\n    (Paul Elschot via yonik)\n\n * LUCENE-1484: Remove synchronization of IndexReader.document() by\n    using CloseableThreadLocal internally.  (Jason Rutherglen via Mike\n    McCandless).\n    \n * LUCENE-1124: Short circuit FuzzyQuery.rewrite when input token length \n    is small compared to minSimilarity. (Timo Nentwig, Mark Miller)\n\n * LUCENE-1316: MatchAllDocsQuery now avoids the synchronized\n    IndexReader.isDeleted() call per document, by directly accessing\n    the underlying deleteDocs BitVector.  this improves performance\n    with non-readOnly readers, especially in a multi-threaded\n    environment.  (Todd Feak, Yonik Seeley, Jason Rutherglen via Mike\n    McCandless)\n\n * LUCENE-1483: When searching over multiple segments we now visit\n    each sub-reader one at a time.  this speeds up warming, since\n    FieldCache entries (if required) can be shared across reopens for\n    those segments that did not change, and also speeds up searches\n    that sort by relevance or by field values.  (Mark Miller, Mike\n    McCandless)\n    \n * LUCENE-1575: The new Collector class decouples collect() from\n    score computation.  Collector.setScorer is called to establish the\n    current Scorer in-use per segment.  Collectors that require the\n    score should then call Scorer.score() per hit inside\n    collect(). (Shai Erera via Mike McCandless)\n\n * LUCENE-1596: MultiTermDocs speedup when set with\n    MultiTermDocs.seek(MultiTermEnum) (yonik)\n    \n * LUCENE-1653: Avoid creating a Calendar in every call to \n    DateTools#dateToString, DateTools#timeToString and\n    DateTools#round.  (Shai Erera via Mark Miller)\n    \n * LUCENE-1688: Deprecate static final String stop word array and \n    replace it with an immutable implementation of CharArraySet.\n    Removes conversions between Set and array.\n    (Simon Willnauer via Mark Miller)\n\n * LUCENE-1754: BooleanQuery.queryWeight.scorer() will return null if\n    it won't match any documents (e.g. if there are no required and\n    optional scorers, or not enough optional scorers to satisfy\n    minShouldMatch).  (Shai Erera via Mike McCandless)\n\n * LUCENE-1607: To speed up string interning for commonly used\n    strings, the StringHelper.intern() interface was added with a\n    default implementation that uses a lockless cache.\n    (Earwin Burrfoot, yonik)\n\n * LUCENE-1800: QueryParser should use reusable TokenStreams. (yonik)\n    \n\nDocumentation\n\n * LUCENE-1908: Scoring documentation imrovements in Similarity javadocs. \n   (Mark Miller, Shai Erera, Ted Dunning, Jiri Kuhn, Marvin Humphrey, Doron Cohen)\n    \n * LUCENE-1872: NumericField javadoc improvements\n    (Michael McCandless, Uwe Schindler)\n \n * LUCENE-1875: Make TokenStream.end javadoc less confusing.\n    (Uwe Schindler)\n\n * LUCENE-1862: Rectified duplicate package level javadocs for\n    o.a.l.queryParser and o.a.l.analysis.cn.\n    (Chris Hostetter)\n\n * LUCENE-1886: Improved hyperlinking in key Analysis javadocs\n    (Bernd Fondermann via Chris Hostetter)\n\n * LUCENE-1884: massive javadoc and comment cleanup, primarily dealing with\n    typos.\n    (Robert Muir via Chris Hostetter)\n    \n * LUCENE-1898: Switch changes to use bullets rather than numbers and \n    update changes-to-html script to handle the new format. \n    (Steven Rowe, Mark Miller)\n    \n * LUCENE-1900: Improve Searchable Javadoc.\n    (Nadav Har'El, Doron Cohen, Marvin Humphrey, Mark Miller)\n    \n * LUCENE-1896: Improve Similarity#queryNorm javadocs.\n    (Jiri Kuhn, Mark Miller)\n\nBuild\n\n * LUCENE-1440: Add new targets to build.xml that allow downloading\n    and executing the junit testcases from an older release for\n    backwards-compatibility testing. (Michael Busch)\n\n * LUCENE-1446: Add compatibility tag to common-build.xml and run \n    backwards-compatibility tests in the nightly build. (Michael Busch)\n\n * LUCENE-1529: Properly test \"drop-in\" replacement of jar with \n    backwards-compatibility tests. (Mike McCandless, Michael Busch)\n\n * LUCENE-1851: Change 'javacc' and 'clean-javacc' targets to build\n    and clean contrib/surround files. (Luis Alves via Michael Busch)\n\n * LUCENE-1854: tar task should use longfile=\"gnu\" to avoid false file\n    name length warnings.  (Mark Miller)\n\nTest Cases\n\n * LUCENE-1791: Enhancements to the QueryUtils and CheckHits utility \n    classes to wrap IndexReaders and Searchers in MultiReaders or \n    MultiSearcher when possible to help exercise more edge cases.\n    (Chris Hostetter, Mark Miller)\n\n * LUCENE-1852: Fix localization test failures. \n    (Robert Muir via Michael Busch)\n    \n * LUCENE-1843: Refactored all tests that use assertAnalyzesTo() & others\n    in core and contrib to use a new BaseTokenStreamTestCase\n    base class. Also rewrote some tests to use this general analysis assert\n    functions instead of own ones (e.g. TestMappingCharFilter).\n    The new base class also tests tokenization with the TokenStream.next()\n    backwards layer enabled (using Token/TokenWrapper as attribute\n    implementation) and disabled (default for Lucene 3.0)\n    (Uwe Schindler, Robert Muir)\n    \n * LUCENE-1836: Added a new LocalizedTestCase as base class for localization\n    junit tests.  (Robert Muir, Uwe Schindler via Michael Busch)\n\n======================= Release 2.4.1 2009-03-09 =======================\n\nAPI Changes\n\n1. LUCENE-1186: Add Analyzer.close() to free internal ThreadLocal\n   resources.  (Christian Kohlschütter via Mike McCandless)\n\nBug fixes\n\n1. LUCENE-1452: Fixed silent data-loss case whereby binary fields are\n   truncated to 0 bytes during merging if the segments being merged\n   are non-congruent (same field name maps to different field\n   numbers).  this bug was introduced with LUCENE-1219.  (Andrzej\n   Bialecki via Mike McCandless).\n\n2. LUCENE-1429: Don't throw incorrect IllegalStateException from\n   IndexWriter.close() if you've hit an OOM when autoCommit is true.\n   (Mike McCandless)\n\n3. LUCENE-1474: If IndexReader.flush() is called twice when there were\n   pending deletions, it could lead to later false AssertionError\n   during IndexReader.open.  (Mike McCandless)\n\n4. LUCENE-1430: Fix false AlreadyClosedException from IndexReader.open\n   (masking an actual IOException) that takes String or File path.\n   (Mike McCandless)\n\n5. LUCENE-1442: Multiple-valued NOT_ANALYZED fields can double-count\n   token offsets.  (Mike McCandless)\n\n6. LUCENE-1453: Ensure IndexReader.reopen()/clone() does not result in\n   incorrectly closing the shared FSDirectory. this bug would only\n   happen if you use IndexReader.open() with a File or String argument.\n   The returned readers are wrapped by a FilterIndexReader that\n   correctly handles closing of directory after reopen()/clone(). \n   (Mark Miller, Uwe Schindler, Mike McCandless)\n\n7. LUCENE-1457: Fix possible overflow bugs during binary\n   searches. (Mark Miller via Mike McCandless)\n\n8. LUCENE-1459: Fix CachingWrapperFilter to not throw exception if\n   both bits() and getDocIdSet() methods are called. (Matt Jones via\n   Mike McCandless)\n\n9. LUCENE-1519: Fix int overflow bug during segment merging.  (Deepak\n   via Mike McCandless)\n\n10. LUCENE-1521: Fix int overflow bug when flushing segment.\n    (Shon Vella via Mike McCandless).\n\n11. LUCENE-1544: Fix deadlock in IndexWriter.addIndexes(IndexReader[]).\n    (Mike McCandless via Doug Sale)\n\n12. LUCENE-1547: Fix rare thread safety issue if two threads call\n    IndexWriter commit() at the same time.  (Mike McCandless)\n\n13. LUCENE-1465: NearSpansOrdered returns payloads from first possible match \n    rather than the correct, shortest match; Payloads could be returned even\n    if the max slop was exceeded; The wrong payload could be returned in \n    certain situations. (Jonathan Mamou, Greg Shackles, Mark Miller)\n\n14. LUCENE-1186: Add Analyzer.close() to free internal ThreadLocal\n    resources.  (Christian Kohlschütter via Mike McCandless)\n\n15. LUCENE-1552: Fix IndexWriter.addIndexes(IndexReader[]) to properly\n    rollback IndexWriter's internal state on hitting an\n    exception. (Scott Garland via Mike McCandless)\n\n======================= Release 2.4.0 2008-10-06 =======================\n\nChanges in backwards compatibility policy\n\n1. LUCENE-1340: In a minor change to Lucene's backward compatibility\n   policy, we are now allowing the Fieldable interface to have\n   changes, within reason, and made on a case-by-case basis.  If an\n   application implements it's own Fieldable, please be aware of\n   this.  Otherwise, no need to be concerned.  this is in effect for\n   all 2.X releases, starting with 2.4.  Also note, that in all\n   likelihood, Fieldable will be changed in 3.0.\n\n\nChanges in runtime behavior\n\n 1. LUCENE-1151: Fix StandardAnalyzer to not mis-identify host names\n    (eg lucene.apache.org) as an ACRONYM.  To get back to the pre-2.4\n    backwards compatible, but buggy, behavior, you can either call\n    StandardAnalyzer.setDefaultReplaceInvalidAcronym(false) (static\n    method), or, set system property\n    Lucene.Net.Analysis.standard.StandardAnalyzer.replaceInvalidAcronym\n    to \"false\" on JVM startup.  All StandardAnalyzer instances created\n    after that will then show the pre-2.4 behavior.  Alternatively,\n    you can call setReplaceInvalidAcronym(false) to change the\n    behavior per instance of StandardAnalyzer.  this backwards\n    compatibility will be removed in 3.0 (hardwiring the value to\n    true).  (Mike McCandless)\n\n 2. LUCENE-1044: IndexWriter with autoCommit=true now commits (such\n    that a reader can see the changes) far less often than it used to.\n    Previously, every flush was also a commit.  You can always force a\n    commit by calling IndexWriter.commit().  Furthermore, in 3.0,\n    autoCommit will be hardwired to false (IndexWriter constructors\n    that take an autoCommit argument have been deprecated) (Mike\n    McCandless)\n\n 3. LUCENE-1335: IndexWriter.addIndexes(Directory[]) and\n    addIndexesNoOptimize no longer allow the same Directory instance\n    to be passed in more than once.  Internally, IndexWriter uses\n    Directory and segment name to uniquely identify segments, so\n    adding the same Directory more than once was causing duplicates\n    which led to problems (Mike McCandless)\n\n 4. LUCENE-1396: Improve PhraseQuery.toString() so that gaps in the\n    positions are indicated with a ? and multiple terms at the same\n    position are joined with a |.  (Andrzej Bialecki via Mike\n    McCandless)\n\nAPI Changes\n\n 1. LUCENE-1084: Changed all IndexWriter constructors to take an\n    explicit parameter for maximum field size.  Deprecated all the\n    pre-existing constructors; these will be removed in release 3.0.\n    NOTE: these new constructors set autoCommit to false.  (Steven\n    Rowe via Mike McCandless)\n\n 2. LUCENE-584: Changed Filter API to return a DocIdSet instead of a\n    java.util.BitSet. this allows using more efficient data structures\n    for Filters and makes them more flexible. this deprecates\n    Filter.bits(), so all filters that implement this outside\n    the Lucene code base will need to be adapted. See also the javadocs\n    of the Filter class. (Paul Elschot, Michael Busch)\n\n 3. LUCENE-1044: Added IndexWriter.commit() which flushes any buffered\n    adds/deletes and then commits a new segments file so readers will\n    see the changes.  Deprecate IndexWriter.flush() in favor of\n    IndexWriter.commit().  (Mike McCandless)\n\n 4. LUCENE-325: Added IndexWriter.expungeDeletes methods, which\n    consult the MergePolicy to find merges necessary to merge away all\n    deletes from the index.  this should be a somewhat lower cost\n    operation than optimize.  (John Wang via Mike McCandless)\n\n 5. LUCENE-1233: Return empty array instead of null when no fields\n    match the specified name in these methods in Document:\n    getFieldables, getFields, getValues, getBinaryValues.  (Stefan\n    Trcek vai Mike McCandless)\n\n 6. LUCENE-1234: Make BoostingSpanScorer protected.  (Andi Vajda via Grant Ingersoll)\n\n 7. LUCENE-510: The index now stores strings as true UTF-8 bytes\n    (previously it was Java's modified UTF-8).  If any text, either\n    stored fields or a token, has illegal UTF-16 surrogate characters,\n    these characters are now silently replaced with the Unicode\n    replacement character U+FFFD.  this is a change to the index file\n    format.  (Marvin Humphrey via Mike McCandless)\n\n 8. LUCENE-852: Let the SpellChecker caller specify IndexWriter mergeFactor\n    and RAM buffer size.  (Otis Gospodnetic)\n\t\n 9. LUCENE-1290: Deprecate org.apache.lucene.search.Hits, Hit and HitIterator\n    and remove all references to these classes from the core. Also update demos\n    and tutorials. (Michael Busch)\n\n10. LUCENE-1288: Add getVersion() and getGeneration() to IndexCommit.\n    getVersion() returns the same value that IndexReader.getVersion()\n    returns when the reader is opened on the same commit.  (Jason\n    Rutherglen via Mike McCandless)\n\n11. LUCENE-1311: Added IndexReader.listCommits(Directory) static\n    method to list all commits in a Directory, plus IndexReader.open\n    methods that accept an IndexCommit and open the index as of that\n    commit.  These methods are only useful if you implement a custom\n    DeletionPolicy that keeps more than the last commit around.\n    (Jason Rutherglen via Mike McCandless)\n\n12. LUCENE-1325: Added IndexCommit.isOptimized().  (Shalin Shekhar\n    Mangar via Mike McCandless)\n\n13. LUCENE-1324: Added TokenFilter.reset(). (Shai Erera via Mike\n    McCandless)\n\n14. LUCENE-1340: Added Fieldable.omitTf() method to skip indexing term\n    frequency, positions and payloads.  this saves index space, and\n    indexing/searching time.  (Eks Dev via Mike McCandless)\n\n15. LUCENE-1219: Add basic reuse API to Fieldable for binary fields:\n    getBinaryValue/Offset/Length(); currently only lazy fields reuse\n    the provided byte[] result to getBinaryValue.  (Eks Dev via Mike\n    McCandless)\n\n16. LUCENE-1334: Add new constructor for Term: Term(String fieldName)\n    which defaults term text to \"\".  (DM Smith via Mike McCandless)\n\n17. LUCENE-1333: Added Token.reinit(*) APIs to re-initialize (reuse) a\n    Token.  Also added term() method to return a String, with a\n    performance penalty clearly documented.  Also implemented\n    hashCode() and equals() in Token, and fixed all core and contrib\n    analyzers to use the re-use APIs.  (DM Smith via Mike McCandless)\n\n18. LUCENE-1329: Add optional readOnly boolean when opening an\n    IndexReader.  A readOnly reader is not allowed to make changes\n    (deletions, norms) to the index; in exchanged, the isDeleted\n    method, often a bottleneck when searching with many threads, is\n    not synchronized.  The default for readOnly is still false, but in\n    3.0 the default will become true.  (Jason Rutherglen via Mike\n    McCandless)\n\n19. LUCENE-1367: Add IndexCommit.isDeleted().  (Shalin Shekhar Mangar\n    via Mike McCandless)\n\n20. LUCENE-1061: Factored out all \"new XXXQuery(...)\" in\n    QueryParser.java into protected methods newXXXQuery(...) so that\n    subclasses can create their own subclasses of each Query type.\n    (John Wang via Mike McCandless)\n\n21. LUCENE-753: Added new Directory implementation\n    org.apache.lucene.store.NIOFSDirectory, which uses java.nio's\n    FileChannel to do file reads.  On most non-Windows platforms, with\n    many threads sharing a single searcher, this may yield sizable\n    improvement to query throughput when compared to FSDirectory,\n    which only allows a single thread to read from an open file at a\n    time.  (Jason Rutherglen via Mike McCandless)\n\n22. LUCENE-1371: Added convenience method TopDocs Searcher.search(Query query, int n).\n    (Mike McCandless)\n    \n23. LUCENE-1356: Allow easy extensions of TopDocCollector by turning\n    constructor and fields from package to protected. (Shai Erera\n    via Doron Cohen) \n\n24. LUCENE-1375: Added convenience method IndexCommit.getTimestamp,\n    which is equivalent to\n    getDirectory().fileModified(getSegmentsFileName()).  (Mike McCandless)\n\n23. LUCENE-1366: Rename Field.Index options to be more accurate:\n    TOKENIZED becomes ANALYZED;  UN_TOKENIZED becomes NOT_ANALYZED;\n    NO_NORMS becomes NOT_ANALYZED_NO_NORMS and a new ANALYZED_NO_NORMS\n    is added.  (Mike McCandless)\n\n24. LUCENE-1131: Added numDeletedDocs method to IndexReader (Otis Gospodnetic)\n\nBug fixes\n    \n 1. LUCENE-1134: Fixed BooleanQuery.rewrite to only optimize a single \n    clause query if minNumShouldMatch<=0. (Shai Erera via Michael Busch)\n\n 2. LUCENE-1169: Fixed bug in IndexSearcher.search(): searching with\n    a filter might miss some hits because scorer.skipTo() is called\n    without checking if the scorer is already at the right position.\n    scorer.skipTo(scorer.doc()) is not a NOOP, it behaves as \n    scorer.next(). (Eks Dev, Michael Busch)\n\n 3. LUCENE-1182: Added scorePayload to SimilarityDelegator (Andi Vajda via Grant Ingersoll)\n \n 4. LUCENE-1213: MultiFieldQueryParser was ignoring slop in case\n    of a single field phrase. (Trejkaz via Doron Cohen)\n\n 5. LUCENE-1228: IndexWriter.commit() was not updating the index version and as\n    result IndexReader.reopen() failed to sense index changes. (Doron Cohen)\n\n 6. LUCENE-1267: Added numDocs() and maxDoc() to IndexWriter;\n    deprecated docCount().  (Mike McCandless)\n\n 7. LUCENE-1274: Added new prepareCommit() method to IndexWriter,\n    which does phase 1 of a 2-phase commit (commit() does phase 2).\n    this is needed when you want to update an index as part of a\n    transaction involving external resources (eg a database).  Also\n    deprecated abort(), renaming it to rollback().  (Mike McCandless)\n\n 8. LUCENE-1003: Stop RussianAnalyzer from removing numbers.\n    (TUSUR OpenTeam, Dmitry Lihachev via Otis Gospodnetic)\n\n 9. LUCENE-1152: SpellChecker fix around clearIndex and indexDictionary\n    methods, plus removal of IndexReader reference.\n    (Naveen Belkale via Otis Gospodnetic)\n\n10. LUCENE-1046: Removed dead code in SpellChecker\n    (Daniel Naber via Otis Gospodnetic)\n\t\n11. LUCENE-1189: Fixed the QueryParser to handle escaped characters within \n    quoted terms correctly. (Tomer Gabel via Michael Busch)\n\n12. LUCENE-1299: Fixed NPE in SpellChecker when IndexReader is not null and field is (Grant Ingersoll)\n\n13. LUCENE-1303: Fixed BoostingTermQuery's explanation to be marked as a Match \n    depending only upon the non-payload score part, regardless of the effect of \n    the payload on the score. Prior to this, score of a query containing a BTQ \n    differed from its explanation. (Doron Cohen)\n    \n14. LUCENE-1310: Fixed SloppyPhraseScorer to work also for terms repeating more \n    than twice in the query. (Doron Cohen)\n\n15. LUCENE-1351: ISOLatin1AccentFilter now cleans additional ligatures (Cedrik Lime via Grant Ingersoll)\n\n16. LUCENE-1383: Workaround a nasty \"leak\" in Java's builtin\n    ThreadLocal, to prevent Lucene from causing unexpected\n    OutOfMemoryError in certain situations (notably J2EE\n    applications).  (Chris Lu via Mike McCandless)\n\nNew features\n\n 1. LUCENE-1137: Added Token.set/getFlags() accessors for passing more information about a Token through the analysis\n    process.  The flag is not indexed/stored and is thus only used by analysis.\n\n 2. LUCENE-1147: Add -segment option to CheckIndex tool so you can\n    check only a specific segment or segments in your index.  (Mike\n    McCandless)\n\n 3. LUCENE-1045: Reopened this issue to add support for short and bytes. \n \n 4. LUCENE-584: Added new data structures to o.a.l.util, such as \n    OpenBitSet and SortedVIntList. These extend DocIdSet and can \n    directly be used for Filters with the new Filter API. Also changed\n    the core Filters to use OpenBitSet instead of java.util.BitSet.\n    (Paul Elschot, Michael Busch)\n\n 5. LUCENE-494: Added QueryAutoStopWordAnalyzer to allow for the automatic removal, from a query of frequently occurring terms.\n    this Analyzer is not intended for use during indexing. (Mark Harwood via Grant Ingersoll)\n\n 6. LUCENE-1044: Change Lucene to properly \"sync\" files after\n    committing, to ensure on a machine or OS crash or power cut, even\n    with cached writes, the index remains consistent.  Also added\n    explicit commit() method to IndexWriter to force a commit without\n    having to close.  (Mike McCandless)\n    \n 7. LUCENE-997: Add search timeout (partial) support.\n    A TimeLimitedCollector was added to allow limiting search time.\n    It is a partial solution since timeout is checked only when \n    collecting a hit, and therefore a search for rare words in a \n    huge index might not stop within the specified time.\n    (Sean Timm via Doron Cohen) \n\n 8. LUCENE-1184: Allow SnapshotDeletionPolicy to be re-used across\n    close/re-open of IndexWriter while still protecting an open\n    snapshot (Tim Brennan via Mike McCandless)\n\n 9. LUCENE-1194: Added IndexWriter.deleteDocuments(Query) to delete\n    documents matching the specified query.  Also added static unlock\n    and isLocked methods (deprecating the ones in IndexReader).  (Mike\n    McCandless)\n\n10. LUCENE-1201: Add IndexReader.getIndexCommit() method. (Tim Brennan\n    via Mike McCandless)\n\n11. LUCENE-550:  Added InstantiatedIndex implementation.  Experimental \n    Index store similar to MemoryIndex but allows for multiple documents \n    in memory.  (Karl Wettin via Grant Ingersoll)\n\n12. LUCENE-400: Added word based n-gram filter (in contrib/analyzers) called ShingleFilter and an Analyzer wrapper\n    that wraps another Analyzer's token stream with a ShingleFilter (Sebastian Kirsch, Steve Rowe via Grant Ingersoll) \n\n13. LUCENE-1166: Decomposition tokenfilter for languages like German and Swedish (Thomas Peuss via Grant Ingersoll)\n\n14. LUCENE-1187: ChainedFilter and BooleanFilter now work with new Filter API\n    and DocIdSetIterator-based filters. Backwards-compatibility with old \n    BitSet-based filters is ensured. (Paul Elschot via Michael Busch)\n\n15. LUCENE-1295: Added new method to MoreLikethis for retrieving interesting terms and made retrieveTerms(int) public. (Grant Ingersoll)\n\n16. LUCENE-1298: MoreLikethis can now accept a custom Similarity (Grant Ingersoll)\n\n17. LUCENE-1297: Allow other string distance measures for the SpellChecker\n    (Thomas Morton via Otis Gospodnetic)\n\n18. LUCENE-1001: Provide access to Payloads via Spans.  All existing Span Query implementations in Lucene implement. (Mark Miller, Grant Ingersoll)\n\n19. LUCENE-1354: Provide programmatic access to CheckIndex (Grant Ingersoll, Mike McCandless)\n\n20. LUCENE-1279: Add support for Collators to RangeFilter/Query and Query Parser.  (Steve Rowe via Grant Ingersoll) \n\nOptimizations\n\n 1. LUCENE-705: When building a compound file, use\n    RandomAccessFile.setLength() to tell the OS/filesystem to\n    pre-allocate space for the file.  this may improve fragmentation\n    in how the CFS file is stored, and allows us to detect an upcoming\n    disk full situation before actually filling up the disk.  (Mike\n    McCandless)\n\n 2. LUCENE-1120: Speed up merging of term vectors by bulk-copying the\n    raw bytes for each contiguous range of non-deleted documents.\n    (Mike McCandless)\n\t\n 3. LUCENE-1185: Avoid checking if the TermBuffer 'scratch' in \n    SegmentTermEnum is null for every call of scanTo().\n    (Christian Kohlschuetter via Michael Busch)\n\n 4. LUCENE-1217: Internal to Field.java, use isBinary instead of\n    runtime type checking for possible speedup of binaryValue().\n    (Eks Dev via Mike McCandless)\n\n 5. LUCENE-1183: Optimized TRStringDistance class (in contrib/spell) that uses\n    less memory than the previous version.  (Cédrik LIME via Otis Gospodnetic)\n\n 6. LUCENE-1195: Improve term lookup performance by adding a LRU cache to the\n    TermInfosReader. In performance experiments the speedup was about 25% on \n    average on mid-size indexes with ~500,000 documents for queries with 3 \n    terms and about 7% on larger indexes with ~4.3M documents. (Michael Busch)\n\nDocumentation\n\n  1. LUCENE-1236:  Added some clarifying remarks to EdgeNGram*.java (Hiroaki Kawai via Grant Ingersoll)\n  \n  2. LUCENE-1157 and LUCENE-1256: HTML changes log, created automatically \n     from CHANGES.txt. this HTML file is currently visible only via developers page.     \n     (Steven Rowe via Doron Cohen)\n\n  3. LUCENE-1349: Fieldable can now be changed without breaking backward compatibility rules (within reason.  See the note at\n  the top of this file and also on Fieldable.java).  (Grant Ingersoll)\n  \n  4. LUCENE-1873: Update documentation to reflect current Contrib area status.\n     (Steven Rowe, Mark Miller)\n\nBuild\n\n  1. LUCENE-1153: Added JUnit JAR to new lib directory.  Updated build to rely on local JUnit instead of ANT/lib.\n  \n  2. LUCENE-1202: Small fixes to the way Clover is used to work better\n     with contribs.  Of particular note: a single clover db is used\n     regardless of whether tests are run globally or in the specific\n     contrib directories. \n     \n  3. LUCENE-1353: Javacc target in contrib/miscellaneous for \n     generating the precedence query parser. \n\nTest Cases\n\n 1. LUCENE-1238: Fixed intermittent failures of TestTimeLimitedCollector.testTimeoutMultiThreaded.\n    Within this fix, \"greedy\" flag was added to TimeLimitedCollector, to allow the wrapped \n    collector to collect also the last doc, after allowed-tTime passed. (Doron Cohen)   \n\t\n 2. LUCENE-1348: relax TestTimeLimitedCollector to not fail due to \n    timeout exceeded (just because test machine is very busy).\n\t\n======================= Release 2.3.2 2008-05-05 =======================\n\nBug fixes\n\n 1. LUCENE-1191: On hitting OutOfMemoryError in any index-modifying\n    methods in IndexWriter, do not commit any further changes to the\n    index to prevent risk of possible corruption.  (Mike McCandless)\n\n 2. LUCENE-1197: Fixed issue whereby IndexWriter would flush by RAM\n    too early when TermVectors were in use.  (Mike McCandless)\n\n 3. LUCENE-1198: Don't corrupt index if an exception happens inside\n    DocumentsWriter.init (Mike McCandless)\n\n 4. LUCENE-1199: Added defensive check for null indexReader before\n    calling close in IndexModifier.close() (Mike McCandless)\n\n 5. LUCENE-1200: Fix rare deadlock case in addIndexes* when\n    ConcurrentMergeScheduler is in use (Mike McCandless)\n\n 6. LUCENE-1208: Fix deadlock case on hitting an exception while\n    processing a document that had triggered a flush (Mike McCandless)\n\n 7. LUCENE-1210: Fix deadlock case on hitting an exception while\n    starting a merge when using ConcurrentMergeScheduler (Mike McCandless)\n\n 8. LUCENE-1222: Fix IndexWriter.doAfterFlush to always be called on\n    flush (Mark Ferguson via Mike McCandless)\n\t\n 9. LUCENE-1226: Fixed IndexWriter.addIndexes(IndexReader[]) to commit\n    successfully created compound files. (Michael Busch)\n\n10. LUCENE-1150: Re-expose StandardTokenizer's constants publicly;\n    this was accidentally lost with LUCENE-966.  (Nicolas Lalevée via\n    Mike McCandless)\n\n11. LUCENE-1262: Fixed bug in BufferedIndexReader.refill whereby on\n    hitting an exception in readInternal, the buffer is incorrectly\n    filled with stale bytes such that subsequent calls to readByte()\n    return incorrect results.  (Trejkaz via Mike McCandless)\n\n12. LUCENE-1270: Fixed intermittent case where IndexWriter.close()\n    would hang after IndexWriter.addIndexesNoOptimize had been\n    called.  (Stu Hood via Mike McCandless)\n\t\nBuild\n\n 1. LUCENE-1230: Include *pom.xml* in source release files. (Michael Busch)\n\n \n======================= Release 2.3.1 2008-02-22 =======================\n\nBug fixes\n    \n 1. LUCENE-1168: Fixed corruption cases when autoCommit=false and\n    documents have mixed term vectors (Suresh Guvvala via Mike\n    McCandless).\n\n 2. LUCENE-1171: Fixed some cases where OOM errors could cause\n    deadlock in IndexWriter (Mike McCandless).\n\n 3. LUCENE-1173: Fixed corruption case when autoCommit=false and bulk\n    merging of stored fields is used (Yonik via Mike McCandless).\n\n 4. LUCENE-1163: Fixed bug in CharArraySet.contains(char[] buffer, int\n    offset, int len) that was ignoring offset and thus giving the\n    wrong answer.  (Thomas Peuss via Mike McCandless)\n\t\n 5. LUCENE-1177: Fix rare case where IndexWriter.optimize might do too\n    many merges at the end.  (Mike McCandless)\n\t\n 6. LUCENE-1176: Fix corruption case when documents with no term\n    vector fields are added before documents with term vector fields.\n    (Mike McCandless)\n\t\n 7. LUCENE-1179: Fixed assert statement that was incorrectly\n    preventing Fields with empty-string field name from working.\n    (Sergey Kabashnyuk via Mike McCandless)\n\n======================= Release 2.3.0 2008-01-21 =======================\n\nChanges in runtime behavior\n\n 1. LUCENE-994: Defaults for IndexWriter have been changed to maximize\n    out-of-the-box indexing speed.  First, IndexWriter now flushes by\n    RAM usage (16 MB by default) instead of a fixed doc count (call\n    IndexWriter.setMaxBufferedDocs to get backwards compatible\n    behavior).  Second, ConcurrentMergeScheduler is used to run merges\n    using background threads (call IndexWriter.setMergeScheduler(new\n    SerialMergeScheduler()) to get backwards compatible behavior).\n    Third, merges are chosen based on size in bytes of each segment\n    rather than document count of each segment (call\n    IndexWriter.setMergePolicy(new LogDocMergePolicy()) to get\n    backwards compatible behavior).\n\n    NOTE: users of ParallelReader must change back all of these\n    defaults in order to ensure the docIDs \"align\" across all parallel\n    indices.\n\n    (Mike McCandless)\n\n 2. LUCENE-1045: SortField.AUTO didn't work with long. When detecting\n    the field type for sorting automatically, numbers used to be\n    interpreted as int, then as float, if parsing the number as an int\n    failed. Now the detection checks for int, then for long,\n    then for float. (Daniel Naber)\n\nAPI Changes\n\n 1. LUCENE-843: Added IndexWriter.setRAMBufferSizeMB(...) to have\n    IndexWriter flush whenever the buffered documents are using more\n    than the specified amount of RAM.  Also added new APIs to Token\n    that allow one to set a char[] plus offset and length to specify a\n    token (to avoid creating a new String() for each Token).  (Mike\n    McCandless)\n\n 2. LUCENE-963: Add setters to Field to allow for re-using a single\n    Field instance during indexing.  this is a sizable performance\n    gain, especially for small documents.  (Mike McCandless)\n\n 3. LUCENE-969: Add new APIs to Token, TokenStream and Analyzer to\n    permit re-using of Token and TokenStream instances during\n    indexing.  Changed Token to use a char[] as the store for the\n    termText instead of String.  this gives faster tokenization\n    performance (~10-15%).  (Mike McCandless)\n\n 4. LUCENE-847: Factored MergePolicy, which determines which merges\n    should take place and when, as well as MergeScheduler, which\n    determines when the selected merges should actually run, out of\n    IndexWriter.  The default merge policy is now\n    LogByteSizeMergePolicy (see LUCENE-845) and the default merge\n    scheduler is now ConcurrentMergeScheduler (see\n    LUCENE-870). (Steven Parkes via Mike McCandless)\n\n 5. LUCENE-1052: Add IndexReader.setTermInfosIndexDivisor(int) method\n    that allows you to reduce memory usage of the termInfos by further\n    sub-sampling (over the termIndexInterval that was used during\n    indexing) which terms are loaded into memory.  (Chuck Williams,\n    Doug Cutting via Mike McCandless)\n    \n 6. LUCENE-743: Add IndexReader.reopen() method that re-opens an\n    existing IndexReader (see New features -> 8.) (Michael Busch)\n\n 7. LUCENE-1062: Add setData(byte[] data), \n    setData(byte[] data, int offset, int length), getData(), getOffset()\n    and clone() methods to o.a.l.index.Payload. Also add the field name \n    as arg to Similarity.scorePayload(). (Michael Busch)\n\n 8. LUCENE-982: Add IndexWriter.optimize(int maxNumSegments) method to\n    \"partially optimize\" an index down to maxNumSegments segments.\n    (Mike McCandless)\n\n 9. LUCENE-1080: Changed Token.DEFAULT_TYPE to be public.\n\n10. LUCENE-1064: Changed TopDocs constructor to be public. \n     (Shai Erera via Michael Busch)\n\n11. LUCENE-1079: DocValues cleanup: constructor now has no params,\n    and getInnerArray() now throws UnsupportedOperationException (Doron Cohen)\n\n12. LUCENE-1089: Added PriorityQueue.insertWithOverflow, which returns\n    the Object (if any) that was bumped from the queue to allow\n    re-use.  (Shai Erera via Mike McCandless)\n    \n13. LUCENE-1101: Token reuse 'contract' (defined LUCENE-969)\n    modified so it is token producer's responsibility\n    to call Token.clear(). (Doron Cohen)   \n\n14. LUCENE-1118: Changed StandardAnalyzer to skip too-long (default >\n    255 characters) tokens.  You can increase this limit by calling\n    StandardAnalyzer.setMaxTokenLength(...).  (Michael McCandless)\n\n\nBug fixes\n\n 1. LUCENE-933: QueryParser fixed to not produce empty sub \n    BooleanQueries \"()\" even if the Analyzer produced no \n    tokens for input. (Doron Cohen)\n\n 2. LUCENE-955: Fixed SegmentTermPositions to work correctly with the\n    first term in the dictionary. (Michael Busch)\n\n 3. LUCENE-951: Fixed NullPointerException in MultiLevelSkipListReader\n    that was thrown after a call of TermPositions.seek(). \n    (Rich Johnson via Michael Busch)\n    \n 4. LUCENE-938: Fixed cases where an unhandled exception in\n    IndexWriter's methods could cause deletes to be lost.\n    (Steven Parkes via Mike McCandless)\n      \n 5. LUCENE-962: Fixed case where an unhandled exception in\n    IndexWriter.addDocument or IndexWriter.updateDocument could cause\n    unreferenced files in the index to not be deleted\n    (Steven Parkes via Mike McCandless)\n  \n 6. LUCENE-957: RAMDirectory fixed to properly handle directories\n    larger than Integer.MAX_VALUE. (Doron Cohen)\n\n 7. LUCENE-781: MultiReader fixed to not throw NPE if isCurrent(),\n    isOptimized() or getVersion() is called. Separated MultiReader\n    into two classes: MultiSegmentReader extends IndexReader, is\n    package-protected and is created automatically by IndexReader.open()\n    in case the index has multiple segments. The public MultiReader \n    now extends MultiSegmentReader and is intended to be used by users\n    who want to add their own subreaders. (Daniel Naber, Michael Busch)\n\n 8. LUCENE-970: FilterIndexReader now implements isOptimized(). Before\n    a call of isOptimized() would throw a NPE. (Michael Busch)\n\n 9. LUCENE-832: ParallelReader fixed to not throw NPE if isCurrent(),\n    isOptimized() or getVersion() is called. (Michael Busch)\n      \n10. LUCENE-948: Fix FNFE exception caused by stale NFS client\n    directory listing caches when writers on different machines are\n    sharing an index over NFS and using a custom deletion policy (Mike\n    McCandless)\n\n11. LUCENE-978: Ensure TermInfosReader, FieldsReader, and FieldsReader\n    close any streams they had opened if an exception is hit in the\n    constructor.  (Ning Li via Mike McCandless)\n\n12. LUCENE-985: If an extremely long term is in a doc (> 16383 chars),\n    we now throw an IllegalArgumentException saying the term is too\n    long, instead of cryptic ArrayIndexOutOfBoundsException.  (Karl\n    Wettin via Mike McCandless)\n\n13. LUCENE-991: The explain() method of BoostingTermQuery had errors\n    when no payloads were present on a document.  (Peter Keegan via\n    Grant Ingersoll)\n\n14. LUCENE-992: Fixed IndexWriter.updateDocument to be atomic again\n    (this was broken by LUCENE-843).  (Ning Li via Mike McCandless)\n\n15. LUCENE-1008: Fixed corruption case when document with no term\n    vector fields is added after documents with term vector fields.\n    this bug was introduced with LUCENE-843.  (Grant Ingersoll via\n    Mike McCandless)\n\n16. LUCENE-1006: Fixed QueryParser to accept a \"\" field value (zero\n    length quoted string.)  (yonik)\n\n17. LUCENE-1010: Fixed corruption case when document with no term\n    vector fields is added after documents with term vector fields.\n    this case is hit during merge and would cause an EOFException.\n    this bug was introduced with LUCENE-984.  (Andi Vajda via Mike\n    McCandless)\n\n19. LUCENE-1009: Fix merge slowdown with LogByteSizeMergePolicy when\n    autoCommit=false and documents are using stored fields and/or term\n    vectors.  (Mark Miller via Mike McCandless)\n\n20. LUCENE-1011: Fixed corruption case when two or more machines,\n    sharing an index over NFS, can be writers in quick succession.\n    (Patrick Kimber via Mike McCandless)\n\n21. LUCENE-1028: Fixed Weight serialization for few queries:\n    DisjunctionMaxQuery, ValueSourceQuery, CustomScoreQuery.\n    Serialization check added for all queries.\n    (Kyle Maxwell via Doron Cohen)\n\n22. LUCENE-1048: Fixed incorrect behavior in Lock.obtain(...) when the\n    timeout argument is very large (eg Long.MAX_VALUE).  Also added\n    Lock.LOCK_OBTAIN_WAIT_FOREVER constant to never timeout.  (Nikolay\n    Diakov via Mike McCandless)\n\n23. LUCENE-1050: Throw LockReleaseFailedException in\n    Simple/NativeFSLockFactory if we fail to delete the lock file when\n    releasing the lock.  (Nikolay Diakov via Mike McCandless)\n\n24. LUCENE-1071: Fixed SegmentMerger to correctly set payload bit in \n    the merged segment. (Michael Busch)\n\n25. LUCENE-1042: Remove throwing of IOException in getTermFreqVector(int, String, TermVectorMapper) to be consistent\n    with other getTermFreqVector calls.  Also removed the throwing of the other IOException in that method to be consistent.  (Karl Wettin via Grant Ingersoll)\n    \n26. LUCENE-1096: Fixed Hits behavior when hits' docs are deleted \n    along with iterating the hits. Deleting docs already retrieved \n    now works seamlessly. If docs not yet retrieved are deleted \n    (e.g. from another thread), and then, relying on the initial \n    Hits.length(), an application attempts to retrieve more hits \n    than actually exist , a ConcurrentMidificationException \n    is thrown.  (Doron Cohen)\n\n27. LUCENE-1068: Changed StandardTokenizer to fix an issue with it marking\n  the type of some tokens incorrectly.  this is done by adding a new flag named\n  replaceInvalidAcronym which defaults to false, the current, incorrect behavior.  Setting\n  this flag to true fixes the problem.  this flag is a temporary fix and is already\n  marked as being deprecated.  3.x will implement the correct approach.  (Shai Erera via Grant Ingersoll)\n  LUCENE-1140: Fixed NPE caused by 1068 (Alexei Dets via Grant Ingersoll)\n    \n28. LUCENE-749: ChainedFilter behavior fixed when logic of \n    first filter is ANDNOT.  (Antonio Bruno via Doron Cohen)\n\n29. LUCENE-508: Make sure SegmentTermEnum.prev() is accurate (= last\n    term) after next() returns false.  (Steven Tamm via Mike\n    McCandless)\n\n    \nNew features\n\n 1. LUCENE-906: Elision filter for French.\n    (Mathieu Lecarme via Otis Gospodnetic)\n\n 2. LUCENE-960: Added a SpanQueryFilter and related classes to allow for\n    not only filtering, but knowing where in a Document a Filter matches\n    (Grant Ingersoll)\n\n 3. LUCENE-868: Added new Term Vector access features.  New callback\n    mechanism allows application to define how and where to read Term\n    Vectors from disk. this implementation contains several extensions\n    of the new abstract TermVectorMapper class.  The new API should be\n    back-compatible.  No changes in the actual storage of Term Vectors\n    has taken place.\n 3.1 LUCENE-1038: Added setDocumentNumber() method to TermVectorMapper\n     to provide information about what document is being accessed.\n     (Karl Wettin via Grant Ingersoll)\n\n 4. LUCENE-975: Added PositionBasedTermVectorMapper that allows for\n    position based lookup of term vector information.\n    See item #3 above (LUCENE-868).\n\n 5. LUCENE-1011: Added simple tools (all in org.apache.lucene.store)\n    to verify that locking is working properly.  LockVerifyServer runs\n    a separate server to verify locks.  LockStressTest runs a simple\n    tool that rapidly obtains and releases locks.\n    VerifyingLockFactory is a LockFactory that wraps any other\n    LockFactory and consults the LockVerifyServer whenever a lock is\n    obtained or released, throwing an exception if an illegal lock\n    obtain occurred.  (Patrick Kimber via Mike McCandless)\n\n 6. LUCENE-1015: Added FieldCache extension (ExtendedFieldCache) to\n    support doubles and longs.  Added support into SortField for sorting\n    on doubles and longs as well.  (Grant Ingersoll)\n\n 7. LUCENE-1020: Created basic index checking & repair tool\n    (o.a.l.index.CheckIndex).  When run without -fix it does a\n    detailed test of all segments in the index and reports summary\n    information and any errors it hit.  With -fix it will remove\n    segments that had errors.  (Mike McCandless)\n\n 8. LUCENE-743: Add IndexReader.reopen() method that re-opens an\n    existing IndexReader by only loading those portions of an index\n    that have changed since the reader was (re)opened. reopen() can\n    be significantly faster than open(), depending on the amount of\n    index changes. SegmentReader, MultiSegmentReader, MultiReader,\n    and ParallelReader implement reopen(). (Michael Busch) \n\n 9. LUCENE-1040: CharArraySet useful for efficiently checking\n    set membership of text specified by char[]. (yonik)\n\n10. LUCENE-1073: Created SnapshotDeletionPolicy to facilitate taking a\n    live backup of an index without pausing indexing.  (Mike\n    McCandless)\n    \n11. LUCENE-1019: CustomScoreQuery enhanced to support multiple \n    ValueSource queries. (Kyle Maxwell via Doron Cohen)\n    \n12. LUCENE-1095: Added an option to StopFilter to increase \n    positionIncrement of the token succeeding a stopped token.\n    Disabled by default. Similar option added to QueryParser \n    to consider token positions when creating PhraseQuery \n    and MultiPhraseQuery. Disabled by default (so by default\n    the query parser ignores position increments).\n    (Doron Cohen)\n\n13. LUCENE-1380: Added TokenFilter for setting position increment in special cases related to the ShingleFilter (Mck SembWever, Steve Rowe, Karl Wettin via Grant Ingersoll)\n\n\n\nOptimizations\n\n 1. LUCENE-937: CachingTokenFilter now uses an iterator to access the \n    Tokens that are cached in the LinkedList. this increases performance \n    significantly, especially when the number of Tokens is large. \n    (Mark Miller via Michael Busch)\n\n 2. LUCENE-843: Substantial optimizations to improve how IndexWriter\n    uses RAM for buffering documents and to speed up indexing (2X-8X\n    faster).  A single shared hash table now records the in-memory\n    postings per unique term and is directly flushed into a single\n    segment.  (Mike McCandless)\n \n 3. LUCENE-892: Fixed extra \"buffer to buffer copy\" that sometimes\n    takes place when using compound files.  (Mike McCandless)\n\n 4. LUCENE-959: Remove synchronization in Document (yonik)\n\n 5. LUCENE-963: Add setters to Field to allow for re-using a single\n    Field instance during indexing.  this is a sizable performance\n    gain, especially for small documents.  (Mike McCandless)\n\n 6. LUCENE-939: Check explicitly for boundary conditions in FieldInfos\n    and don't rely on exceptions. (Michael Busch)\n\n 7. LUCENE-966: Very substantial speedups (~6X faster) for\n    StandardTokenizer (StandardAnalyzer) by using JFlex instead of\n    JavaCC to generate the tokenizer.\n    (Stanislaw Osinski via Mike McCandless)\n\n 8. LUCENE-969: Changed core tokenizers & filters to re-use Token and\n    TokenStream instances when possible to improve tokenization\n    performance (~10-15%). (Mike McCandless)\n\n 9. LUCENE-871: Speedup ISOLatin1AccentFilter (Ian Boston via Mike\n    McCandless)\n\n10. LUCENE-986: Refactored SegmentInfos from IndexReader into the new\n    subclass DirectoryIndexReader. SegmentReader and MultiSegmentReader\n    now extend DirectoryIndexReader and are the only IndexReader \n    implementations that use SegmentInfos to access an index and \n    acquire a write lock for index modifications. (Michael Busch)\n\n11. LUCENE-1007: Allow flushing in IndexWriter to be triggered by\n    either RAM usage or document count or both (whichever comes\n    first), by adding symbolic constant DISABLE_AUTO_FLUSH to disable\n    one of the flush triggers.  (Ning Li via Mike McCandless)\n\n12. LUCENE-1043: Speed up merging of stored fields by bulk-copying the\n    raw bytes for each contiguous range of non-deleted documents.\n    (Robert Engels via Mike McCandless)\n\n13. LUCENE-693: Speed up nested conjunctions (~2x) that match many\n    documents, and a slight performance increase for top level\n    conjunctions.  (yonik)\n\n14. LUCENE-1098: Make inner class StandardAnalyzer.SavedStreams static \n    and final. (Nathan Beyer via Michael Busch)\n\nDocumentation\n\n 1. LUCENE-1051: Generate separate javadocs for core, demo and contrib\n    classes, as well as an unified view. Also add an appropriate menu \n    structure to the website. (Michael Busch)\n\n 2. LUCENE-746: Fix error message in AnalyzingQueryParser.getPrefixQuery.\n    (Ronnie Kolehmainen via Michael Busch)\n\nBuild\n\n 1. LUCENE-908: Improvements and simplifications for how the MANIFEST\n    file and the META-INF dir are created. (Michael Busch)\n\n 2. LUCENE-935: Various improvements for the maven artifacts. Now the\n    artifacts also include the sources as .jar files. (Michael Busch)\n\n 3. Added apply-patch target to top-level build.  Defaults to looking for\n    a patch in ${basedir}/../patches with name specified by -Dpatch.name.\n    Can also specify any location by -Dpatch.file property on the command\n    line.  this should be helpful for easy application of patches, but it\n    is also a step towards integrating automatic patch application with\n    JIRA and Hudson, and is thus subject to change.  (Grant Ingersoll)\n \n 4. LUCENE-935: Defined property \"m2.repository.url\" to allow setting\n    the url to a maven remote repository to deploy to. (Michael Busch)\n\n 5. LUCENE-1051: Include javadocs in the maven artifacts. (Michael Busch)\n\n 6. LUCENE-1055: Remove gdata-server from build files and its sources \n    from trunk. (Michael Busch)\n\n 7. LUCENE-935: Allow to deploy maven artifacts to a remote m2 repository\n    via scp and ssh authentication. (Michael Busch)\n\t\n 8. LUCENE-1123: Allow overriding the specification version for \n    MANIFEST.MF (Michael Busch)\n\nTest Cases\n\n 1. LUCENE-766: Test adding two fields with the same name but different \n    term vector setting.  (Nicolas Lalevée via Doron Cohen)  \n    \n======================= Release 2.2.0 2007-06-19 =======================\n\nChanges in runtime behavior\n\nAPI Changes\n\n 1. LUCENE-793: created new exceptions and added them to throws clause\n    for many methods (all subclasses of IOException for backwards\n    compatibility): index.StaleReaderException,\n    index.CorruptIndexException, store.LockObtainFailedException.\n    this was done to better call out the possible root causes of an\n    IOException from these methods.  (Mike McCandless)\n\n 2. LUCENE-811: make SegmentInfos class, plus a few methods from related\n    classes, package-private again (they were unnecessarily made public\n    as part of LUCENE-701).  (Mike McCandless)\n\n 3. LUCENE-710: added optional autoCommit boolean to IndexWriter\n    constructors.  When this is false, index changes are not committed\n    until the writer is closed.  this gives explicit control over when\n    a reader will see the changes.  Also added optional custom\n    deletion policy to explicitly control when prior commits are\n    removed from the index.  this is intended to allow applications to\n    share an index over NFS by customizing when prior commits are\n    deleted. (Mike McCandless)\n\n 4. LUCENE-818: changed most public methods of IndexWriter,\n    IndexReader (and its subclasses), FieldsReader and RAMDirectory to\n    throw AlreadyClosedException if they are accessed after being\n    closed.  (Mike McCandless)\n\n 5. LUCENE-834: Changed some access levels for certain Span classes to allow them\n    to be overridden.  They have been marked expert only and not for public\n    consumption. (Grant Ingersoll) \n\n 6. LUCENE-796: Removed calls to super.* from various get*Query methods in\n    MultiFieldQueryParser, in order to allow sub-classes to override them.\n    (Steven Parkes via Otis Gospodnetic)\n\n 7. LUCENE-857: Removed caching from QueryFilter and deprecated QueryFilter\n    in favour of QueryWrapperFilter or QueryWrapperFilter + CachingWrapperFilter\n    combination when caching is desired.\n    (Chris Hostetter, Otis Gospodnetic)\n\n 8. LUCENE-869: Changed FSIndexInput and FSIndexOutput to inner classes of FSDirectory\n    to enable extensibility of these classes. (Michael Busch)\n\n 9. LUCENE-580: Added the public method reset() to TokenStream. this method does\n    nothing by default, but may be overwritten by subclasses to support consuming\n    the TokenStream more than once. (Michael Busch)\n\n10. LUCENE-580: Added a new constructor to Field that takes a TokenStream as\n    argument, available as tokenStreamValue(). this is useful to avoid the need of \n    \"dummy analyzers\" for pre-analyzed fields. (Karl Wettin, Michael Busch)\n\n11. LUCENE-730: Added the new methods to BooleanQuery setAllowDocsOutOfOrder() and\n    getAllowDocsOutOfOrder(). Deprecated the methods setUseScorer14() and \n    getUseScorer14(). The optimization patch LUCENE-730 (see Optimizations->3.) \n    improves performance for certain queries but results in scoring out of docid \n    order. this patch reverse this change, so now by default hit docs are scored\n    in docid order if not setAllowDocsOutOfOrder(true) is explicitly called.\n    this patch also enables the tests in QueryUtils again that check for docid\n    order. (Paul Elschot, Doron Cohen, Michael Busch)\n\n12. LUCENE-888: Added Directory.openInput(File path, int bufferSize)\n    to optionally specify the size of the read buffer.  Also added\n    BufferedIndexInput.setBufferSize(int) to change the buffer size.\n    (Mike McCandless)\n\n13. LUCENE-923: Make SegmentTermPositionVector package-private. It does not need\n    to be public because it implements the public interface TermPositionVector.\n    (Michael Busch)\n\nBug fixes\n\n 1. LUCENE-804: Fixed build.xml to pack a fully compilable src dist.  (Doron Cohen)\n\n 2. LUCENE-813: Leading wildcard fixed to work with trailing wildcard.\n    Query parser modified to create a prefix query only for the case \n    that there is a single trailing wildcard (and no additional wildcard \n    or '?' in the query text).  (Doron Cohen)\n\n 3. LUCENE-812: Add no-argument constructors to NativeFSLockFactory\n    and SimpleFSLockFactory.  this enables all 4 builtin LockFactory\n    implementations to be specified via the System property\n    org.apache.lucene.store.FSDirectoryLockFactoryClass.  (Mike McCandless)\n\n 4. LUCENE-821: The new single-norm-file introduced by LUCENE-756\n    failed to reduce the number of open descriptors since it was still\n    opened once per field with norms. (yonik)\n\n 5. LUCENE-823: Make sure internal file handles are closed when\n    hitting an exception (eg disk full) while flushing deletes in\n    IndexWriter's mergeSegments, and also during\n    IndexWriter.addIndexes.  (Mike McCandless)\n\n 6. LUCENE-825: If directory is removed after\n    FSDirectory.getDirectory() but before IndexReader.open you now get\n    a FileNotFoundException like Lucene pre-2.1 (before this fix you\n    got an NPE).  (Mike McCandless)\n\n 7. LUCENE-800: Removed backslash from the TERM_CHAR list in the queryparser, \n    because the backslash is the escape character. Also changed the ESCAPED_CHAR\n    list to contain all possible characters, because every character that \n    follows a backslash should be considered as escaped. (Michael Busch)\n\n 8. LUCENE-372: QueryParser.parse() now ensures that the entire input string \n    is consumed. Now a ParseException is thrown if a query contains too many\n    closing parentheses. (Andreas Neumann via Michael Busch)\n\n 9. LUCENE-814: javacc build targets now fix line-end-style of generated files.\n    Now also deleting all javacc generated files before calling javacc.\n    (Steven Parkes, Doron Cohen)\n    \n10. LUCENE-829: close readers in contrib/benchmark. (Karl Wettin, Doron Cohen)\n\n11. LUCENE-828: Minor fix for Term's equal().\n    (Paul Cowan via Otis Gospodnetic)\n\n12. LUCENE-846: Fixed: if IndexWriter is opened with autoCommit=false,\n    and you call addIndexes, and hit an exception (eg disk full) then\n    when IndexWriter rolls back its internal state this could corrupt\n    the instance of IndexWriter (but, not the index itself) by\n    referencing already deleted segments.  this bug was only present\n    in 2.2 (trunk), ie was never released.  (Mike McCandless)\n    \n13. LUCENE-736: Sloppy phrase query with repeating terms matches wrong docs.\n    For example query \"B C B\"~2 matches the doc \"A B C D E\". (Doron Cohen)\n    \n14. LUCENE-789: Fixed: custom similarity is ignored when using MultiSearcher (problem reported \n    by Alexey Lef). Now the similarity applied by MultiSearcer.setSimilarity(sim) is being used. \n    Note that as before this fix, creating a multiSearcher from Searchers for whom custom similarity \n    was set has no effect - it is masked by the similarity of the MultiSearcher. this is as \n    designed, because MultiSearcher operates on Searchables (not Searchers). (Doron Cohen)\n\n15. LUCENE-880: Fixed DocumentWriter to close the TokenStreams after it\n    has written the postings. Then the resources associated with the \n    TokenStreams can safely be released. (Michael Busch)\n\n16. LUCENE-883: consecutive calls to Spellchecker.indexDictionary()\n    won't insert terms twice anymore. (Daniel Naber)\n\n17. LUCENE-881: QueryParser.escape() now also escapes the characters\n    '|' and '&' which are part of the queryparser syntax. (Michael Busch)\n\n18. LUCENE-886: Spellchecker clean up: exceptions aren't printed to STDERR\n    anymore and ignored, but re-thrown. Some javadoc improvements.\n    (Daniel Naber)\n\n19. LUCENE-698: FilteredQuery now takes the query boost into account for \n    scoring. (Michael Busch)\n\n20. LUCENE-763: Spellchecker: LuceneDictionary used to skip first word in \n    enumeration. (Christian Mallwitz via Daniel Naber)\n    \n21. LUCENE-903: FilteredQuery explanation inaccuracy with boost.\n    Explanation tests now \"deep\" check the explanation details.\n    (Chris Hostetter, Doron Cohen)\n    \n22. LUCENE-912: DisjunctionMaxScorer first skipTo(target) call ignores the \n    skip target param and ends up at the first match.\n    (Sudaakeran B. via Chris Hostetter & Doron Cohen)\n    \n23. LUCENE-913: Two consecutive score() calls return different \n    scores for Boolean Queries. (Michael Busch, Doron Cohen)\n\n24. LUCENE-1013: Fix IndexWriter.setMaxMergeDocs to work \"out of the\n    box\", again, by moving set/getMaxMergeDocs up from\n    LogDocMergePolicy into LogMergePolicy.  this fixes the API\n    breakage (non backwards compatible change) caused by LUCENE-994.\n    (Yonik Seeley via Mike McCandless)\n\nNew features\n\n 1. LUCENE-759: Added two n-gram-producing TokenFilters.\n    (Otis Gospodnetic)\n\n 2. LUCENE-822: Added FieldSelector capabilities to Searchable for use with\n    RemoteSearcher, and other Searchable implementations. (Mark Miller, Grant Ingersoll)\n\n 3. LUCENE-755: Added the ability to store arbitrary binary metadata in the posting list.\n    These metadata are called Payloads. For every position of a Token one Payload in the form\n    of a variable length byte array can be stored in the prox file.\n    Remark: The APIs introduced with this feature are in experimental state and thus\n            contain appropriate warnings in the javadocs.\n    (Michael Busch)\n\n 4. LUCENE-834: Added BoostingTermQuery which can boost scores based on the\n    values of a payload (see #3 above.) (Grant Ingersoll)\n\n 5. LUCENE-834: Similarity has a new method for scoring payloads called\n    scorePayloads that can be overridden to take advantage of payload\n    storage (see #3 above)\n\n 6. LUCENE-834: Added isPayloadAvailable() onto TermPositions interface and\n    implemented it in the appropriate places (Grant Ingersoll)\n\n 7. LUCENE-853: Added RemoteCachingWrapperFilter to enable caching of Filters\n    on the remote side of the RMI connection.\n    (Matt Ericson via Otis Gospodnetic)\n\n 8. LUCENE-446: Added Solr's search.function for scores based on field \n    values, plus CustomScoreQuery for simple score (post) customization.\n    (Yonik Seeley, Doron Cohen)\n\n 9. LUCENE-1058: Added new TeeTokenFilter (like the UNIX 'tee' command) and SinkTokenizer which can be used to share tokens between two or more\n    Fields such that the other Fields do not have to go through the whole Analysis process over again.  For instance, if you have two\n    Fields that share all the same analysis steps except one lowercases tokens and the other does not, you can coordinate the operations\n    between the two using the TeeTokenFilter and the SinkTokenizer.  See TeeSinkTokenTest.java for examples.\n    (Grant Ingersoll, Michael Busch, Yonik Seeley)\n \nOptimizations\n\n 1. LUCENE-761: The proxStream is now cloned lazily in SegmentTermPositions\n    when nextPosition() is called for the first time. this allows using instances\n    of SegmentTermPositions instead of SegmentTermDocs without additional costs.\n    (Michael Busch)\n\n 2. LUCENE-431: RAMInputStream and RAMOutputStream extend IndexInput and\n    IndexOutput directly now. this avoids further buffering and thus avoids \n    unnecessary array copies. (Michael Busch)\n\n 3. LUCENE-730: Updated BooleanScorer2 to make use of BooleanScorer in some\n    cases and possibly improve scoring performance.  Documents can now be\n    delivered out-of-order as they are scored (e.g. to HitCollector).\n    N.B. A bit of code had to be disabled in QueryUtils in order for\n    TestBoolean2 test to keep passing.\n    (Paul Elschot via Otis Gospodnetic)\n\n 4. LUCENE-882: Spellchecker doesn't store the ngrams anymore but only indexes\n    them to keep the spell index small. (Daniel Naber)\n\n 5. LUCENE-430: Delay allocation of the buffer after a clone of BufferedIndexInput.\n    Together with LUCENE-888 this will allow to adjust the buffer size\n    dynamically. (Paul Elschot, Michael Busch)\n \n 6. LUCENE-888: Increase buffer sizes inside CompoundFileWriter and\n    BufferedIndexOutput.  Also increase buffer size in\n    BufferedIndexInput, but only when used during merging.  Together,\n    these increases yield 10-18% overall performance gain vs the\n    previous 1K defaults.  (Mike McCandless)\n\n 7. LUCENE-866: Adds multi-level skip lists to the posting lists. this speeds \n    up most queries that use skipTo(), especially on big indexes with large posting \n    lists. For average AND queries the speedup is about 20%, for queries that \n    contain very frequent and very unique terms the speedup can be over 80%.\n    (Michael Busch)\n\nDocumentation\n\n 1. LUCENE 791 && INFRA-1173: Infrastructure moved the Wiki to\n    http://wiki.apache.org/lucene-java/   Updated the links in the docs and\n    wherever else I found references.  (Grant Ingersoll, Joe Schaefer)\n\n 2. LUCENE-807: Fixed the javadoc for ScoreDocComparator.compare() to be \n    consistent with java.util.Comparator.compare(): Any integer is allowed to \n    be returned instead of only -1/0/1.\n    (Paul Cowan via Michael Busch)\n \n 3. LUCENE-875: Solved javadoc warnings & errors under jdk1.4. \n    Solved javadoc errors under jdk5 (jars in path for gdata).\n    Made \"javadocs\" target depend on \"build-contrib\" for first downloading\n    contrib jars configured for dynamic downloaded. (Note: when running\n    behind firewall, a firewall prompt might pop up) (Doron Cohen)\n\n 4. LUCENE-740: Added SNOWBALL-LICENSE.txt to the snowball package and a\n    remark about the license to NOTICE.TXT. (Steven Parkes via Michael Busch)\n\n 5. LUCENE-925: Added analysis package javadocs. (Grant Ingersoll and Doron Cohen)\n\n 6. LUCENE-926: Added document package javadocs. (Grant Ingersoll)\n\nBuild\n\n 1. LUCENE-802: Added LICENSE.TXT and NOTICE.TXT to Lucene jars.\n    (Steven Parkes via Michael Busch)\n\n 2. LUCENE-885: \"ant test\" now includes all contrib tests.  The new\n    \"ant test-core\" target can be used to run only the Core (non\n    contrib) tests. \n    (Chris Hostetter)\n    \n 3. LUCENE-900: \"ant test\" now enables Java assertions (in Lucene packages).\n    (Doron Cohen)\n\n 4. LUCENE-894: Add custom build file for binary distributions that includes\n    targets to build the demos. (Chris Hostetter, Michael Busch)\n\n 5. LUCENE-904: The \"package\" targets in build.xml now also generate .md5\n    checksum files. (Chris Hostetter, Michael Busch)\n\n 6. LUCENE-907: Include LICENSE.TXT and NOTICE.TXT in the META-INF dirs of\n    demo war, demo jar, and the contrib jars. (Michael Busch)\n    \n 7. LUCENE-909: Demo targets for running the demo. (Doron Cohen)\n\n 8. LUCENE-908: Improves content of MANIFEST file and makes it customizable\n    for the contribs. Adds SNOWBALL-LICENSE.txt to META-INF of the snowball\n    jar and makes sure that the lucli jar contains LICENSE.txt and NOTICE.txt.\n    (Chris Hostetter, Michael Busch)\n\n 9. LUCENE-930: Various contrib building improvements to ensure contrib\n    dependencies are met, and test compilation errors fail the build.\n    (Steven Parkes, Chris Hostetter)\n\n10. LUCENE-622: Add ant target and pom.xml files for building maven artifacts \n    of the Lucene core and the contrib modules. \n    (Sami Siren, Karl Wettin, Michael Busch)\n\n======================= Release 2.1.0 2007-02-14 =======================\n\nChanges in runtime behavior\n\n 1. 's' and 't' have been removed from the list of default stopwords\n    in StopAnalyzer (also used in by StandardAnalyzer). Having e.g. 's'\n    as a stopword meant that 's-class' led to the same results as 'class'.\n    Note that this problem still exists for 'a', e.g. in 'a-class' as\n    'a' continues to be a stopword.\n    (Daniel Naber)\n\n 2. LUCENE-478: Updated the list of Unicode code point ranges for CJK\n    (now split into CJ and K) in StandardAnalyzer.  (John Wang and\n    Steven Rowe via Otis Gospodnetic)\n\n 3. Modified some CJK Unicode code point ranges in StandardTokenizer.jj,\n    and added a few more of them to increase CJK character coverage.\n    Also documented some of the ranges.\n    (Otis Gospodnetic)\n\n 4. LUCENE-489: Add support for leading wildcard characters (*, ?) to\n    QueryParser.  Default is to disallow them, as before.\n    (Steven Parkes via Otis Gospodnetic)\n\n 5. LUCENE-703: QueryParser changed to default to use of ConstantScoreRangeQuery\n    for range queries. Added useOldRangeQuery property to QueryParser to allow\n    selection of old RangeQuery class if required.\n    (Mark Harwood)\n\n 6. LUCENE-543: WildcardQuery now performs a TermQuery if the provided term\n    does not contain a wildcard character (? or *), when previously a\n    StringIndexOutOfBoundsException was thrown.\n    (Michael Busch via Erik Hatcher)\n\n 7. LUCENE-726: Removed the use of deprecated doc.fields() method and\n    Enumeration.\n    (Michael Busch via Otis Gospodnetic)\n\n 8. LUCENE-436: Removed finalize() in TermInfosReader and SegmentReader,\n    and added a call to enumerators.remove() in TermInfosReader.close().\n    The finalize() overrides were added to help with a pre-1.4.2 JVM bug\n    that has since been fixed, plus we no longer support pre-1.4.2 JVMs.\n    (Otis Gospodnetic)\n\n 9. LUCENE-771: The default location of the write lock is now the\n    index directory, and is named simply \"write.lock\" (without a big\n    digest prefix).  The system properties \"org.apache.lucene.lockDir\"\n    nor \"java.io.tmpdir\" are no longer used as the global directory\n    for storing lock files, and the LOCK_DIR field of FSDirectory is\n    now deprecated.  (Mike McCandless)\n\nNew features\n\n 1. LUCENE-503: New ThaiAnalyzer and ThaiWordFilter in contrib/analyzers\n    (Samphan Raruenrom via Chris Hostetter)\n\n 2. LUCENE-545: New FieldSelector API and associated changes to\n    IndexReader and implementations.  New Fieldable interface for use\n    with the lazy field loading mechanism.  (Grant Ingersoll and Chuck\n    Williams via Grant Ingersoll)\n\n 3. LUCENE-676: Move Solr's PrefixFilter to Lucene core. (Yura\n    Smolsky, Yonik Seeley)\n\n 4. LUCENE-678: Added NativeFSLockFactory, which implements locking\n    using OS native locking (via java.nio.*).  (Michael McCandless via\n    Yonik Seeley)\n\n 5. LUCENE-544: Added the ability to specify different boosts for\n    different fields when using MultiFieldQueryParser (Matt Ericson\n    via Otis Gospodnetic)\n\n 6. LUCENE-528: New IndexWriter.addIndexesNoOptimize() that doesn't\n    optimize the index when adding new segments, only performing\n    merges as needed.  (Ning Li via Yonik Seeley)\n\n 7. LUCENE-573: QueryParser now allows backslash escaping in\n    quoted terms and phrases. (Michael Busch via Yonik Seeley)\n\n 8. LUCENE-716: QueryParser now allows specification of Unicode\n    characters in terms via a unicode escape of the form \\uXXXX\n    (Michael Busch via Yonik Seeley)\n\n 9. LUCENE-709: Added RAMDirectory.sizeInBytes(), IndexWriter.ramSizeInBytes()\n    and IndexWriter.flushRamSegments(), allowing applications to\n    control the amount of memory used to buffer documents.\n    (Chuck Williams via Yonik Seeley)\n\n10. LUCENE-723: QueryParser now parses *:* as MatchAllDocsQuery\n    (Yonik Seeley)\n\n11. LUCENE-741: Command-line utility for modifying or removing norms\n    on fields in an existing index.  this is mostly based on LUCENE-496\n    and lives in contrib/miscellaneous.\n    (Chris Hostetter, Otis Gospodnetic)\n\n12. LUCENE-759: Added NGramTokenizer and EdgeNGramTokenizer classes and\n    their passing unit tests.\n    (Otis Gospodnetic)\n\n13. LUCENE-565: Added methods to IndexWriter to more efficiently\n    handle updating documents (the \"delete then add\" use case).  this\n    is intended to be an eventual replacement for the existing\n    IndexModifier.  Added IndexWriter.flush() (renamed from\n    flushRamSegments()) to flush all pending updates (held in RAM), to\n    the Directory.  (Ning Li via Mike McCandless)\n\n14. LUCENE-762: Added in SIZE and SIZE_AND_BREAK FieldSelectorResult options\n    which allow one to retrieve the size of a field without retrieving the\n    actual field. (Chuck Williams via Grant Ingersoll)\n\n15. LUCENE-799: Properly handle lazy, compressed fields.\n    (Mike Klaas via Grant Ingersoll)\n\nAPI Changes\n\n 1. LUCENE-438: Remove \"final\" from Token, implement Cloneable, allow\n    changing of termText via setTermText().  (Yonik Seeley)\n\n 2. Lucene.Net.Analysis.nl.WordlistLoader has been deprecated\n    and is supposed to be replaced with the WordlistLoader class in\n    package Lucene.Net.Analysis (Daniel Naber)\n\n 3. LUCENE-609: Revert return type of Document.getField(s) to Field\n    for backward compatibility, added new Document.getFieldable(s)\n    for access to new lazy loaded fields. (Yonik Seeley)\n\n 4. LUCENE-608: Document.fields() has been deprecated and a new method\n    Document.getFields() has been added that returns a List instead of\n    an Enumeration (Daniel Naber)\n\n 5. LUCENE-605: New Explanation.isMatch() method and new ComplexExplanation\n    subclass allows explain methods to produce Explanations which model\n    \"matching\" independent of having a positive value.\n    (Chris Hostetter)\n\n 6. LUCENE-621: New static methods IndexWriter.setDefaultWriteLockTimeout\n    and IndexWriter.setDefaultCommitLockTimeout for overriding default\n    timeout values for all future instances of IndexWriter (as well\n    as for any other classes that may reference the static values,\n    ie: IndexReader).\n    (Michael McCandless via Chris Hostetter)\n\n 7. LUCENE-638: FSDirectory.list() now only returns the directory's\n    Lucene-related files. Thanks to this change one can now construct\n    a RAMDirectory from a file system directory that contains files\n    not related to Lucene.\n    (Simon Willnauer via Daniel Naber)\n\n 8. LUCENE-635: Decoupling locking implementation from Directory\n    implementation.  Added set/getLockFactory to Directory and moved\n    all locking code into subclasses of abstract class LockFactory.\n    FSDirectory and RAMDirectory still default to their prior locking\n    implementations, but now you can mix & match, for example using\n    SingleInstanceLockFactory (ie, in memory locking) locking with an\n    FSDirectory.  Note that now you must call setDisableLocks before\n    the instantiation a FSDirectory if you wish to disable locking\n    for that Directory.\n    (Michael McCandless, Jeff Patterson via Yonik Seeley)\n\n 9. LUCENE-657: Made FuzzyQuery non-final and inner ScoreTerm protected.\n    (Steven Parkes via Otis Gospodnetic)\n\n10. LUCENE-701: Lockless commits: a commit lock is no longer required\n    when a writer commits and a reader opens the index.  this includes\n    a change to the index file format (see docs/fileformats.html for\n    details).  It also removes all APIs associated with the commit\n    lock & its timeout.  Readers are now truly read-only and do not\n    block one another on startup.  this is the first step to getting\n    Lucene to work correctly over NFS (second step is\n    LUCENE-710). (Mike McCandless)\n\n11. LUCENE-722: DEFAULT_MIN_DOC_FREQ was misspelled DEFALT_MIN_DOC_FREQ\n    in Similarity's MoreLikethis class. The misspelling has been\n    replaced by the correct spelling.\n    (Andi Vajda via Daniel Naber)\n\n12. LUCENE-738: Reduce the size of the file that keeps track of which\n    documents are deleted when the number of deleted documents is\n    small.  this changes the index file format and cannot be\n    read by previous versions of Lucene.  (Doron Cohen via Yonik Seeley)\n\n13. LUCENE-756: Maintain all norms in a single .nrm file to reduce the\n    number of open files and file descriptors for the non-compound index\n    format.  this changes the index file format, but maintains the\n    ability to read and update older indices. The first segment merge\n    on an older format index will create a single .nrm file for the new\n    segment.  (Doron Cohen via Yonik Seeley)\n\n14. LUCENE-732: DateTools support has been added to QueryParser, with\n    setters for both the default Resolution, and per-field Resolution.\n    For backwards compatibility, DateField is still used if no Resolutions\n    are specified. (Michael Busch via Chris Hostetter)\n\n15. Added isOptimized() method to IndexReader.\n    (Otis Gospodnetic)\n\n16. LUCENE-773: Deprecate the FSDirectory.getDirectory(*) methods that\n    take a boolean \"create\" argument.  Instead you should use\n    IndexWriter's \"create\" argument to create a new index.\n    (Mike McCandless)\n\n17. LUCENE-780: Add a static Directory.copy() method to copy files\n    from one Directory to another.  (Jiri Kuhn via Mike McCandless)\n\n18. LUCENE-773: Added Directory.clearLock(String name) to forcefully\n    remove an old lock.  The default implementation is to ask the\n    lockFactory (if non null) to clear the lock.  (Mike McCandless)\n\n19. LUCENE-795: Directory.renameFile() has been deprecated as it is\n    not used anymore inside Lucene.  (Daniel Naber)\n\nBug fixes\n\n 1. Fixed the web application demo (built with \"ant war-demo\") which\n    didn't work because it used a QueryParser method that had\n    been removed (Daniel Naber)\n\n 2. LUCENE-583: ISOLatin1AccentFilter fails to preserve positionIncrement\n    (Yonik Seeley)\n\n 3. LUCENE-575: SpellChecker min score is incorrectly changed by suggestSimilar\n    (Karl Wettin via Yonik Seeley)\n\n 4. LUCENE-587: Explanation.toHtml was producing malformed HTML\n    (Chris Hostetter)\n\n 5. Fix to allow MatchAllDocsQuery to be used with RemoteSearcher (Yonik Seeley)\n\n 6. LUCENE-601: RAMDirectory and RAMFile made Serializable\n    (Karl Wettin via Otis Gospodnetic)\n\n 7. LUCENE-557: Fixes to BooleanQuery and FilteredQuery so that the score\n    Explanations match up with the real scores.\n    (Chris Hostetter)\n\n 8. LUCENE-607: ParallelReader's TermEnum fails to advance properly to\n    new fields (Chuck Williams, Christian Kohlschuetter via Yonik Seeley)\n\n 9. LUCENE-610,LUCENE-611: Simple syntax changes to allow compilation with ecj:\n    disambiguate inner class scorer's use of doc() in BooleanScorer2,\n    other test code changes.  (DM Smith via Yonik Seeley)\n\n10. LUCENE-451: All core query types now use ComplexExplanations so that\n    boosts of zero don't confuse the BooleanWeight explain method.\n    (Chris Hostetter)\n\n11. LUCENE-593: Fixed LuceneDictionary's inner Iterator\n    (Kåre Fiedler Christiansen via Otis Gospodnetic)\n\n12. LUCENE-641: fixed an off-by-one bug with IndexWriter.setMaxFieldLength()\n    (Daniel Naber)\n\n13. LUCENE-659: Make PerFieldAnalyzerWrapper delegate getPositionIncrementGap()\n    to the correct analyzer for the field. (Chuck Williams via Yonik Seeley)\n\n14. LUCENE-650: Fixed NPE in Locale specific String Sort when Document\n    has no value.\n    (Oliver Hutchison via Chris Hostetter)\n\n15. LUCENE-683: Fixed data corruption when reading lazy loaded fields.\n    (Yonik Seeley)\n\n16. LUCENE-678: Fixed bug in NativeFSLockFactory which caused the same\n    lock to be shared between different directories.\n    (Michael McCandless via Yonik Seeley)\n\n17. LUCENE-690: Fixed thread unsafe use of IndexInput by lazy loaded fields.\n    (Yonik Seeley)\n\n18. LUCENE-696: Fix bug when scorer for DisjunctionMaxQuery has skipTo()\n    called on it before next().  (Yonik Seeley)\n\n19. LUCENE-569: Fixed SpanNearQuery bug, for 'inOrder' queries it would fail\n    to recognize ordered spans if they overlapped with unordered spans.\n    (Paul Elschot via Chris Hostetter)\n\n20. LUCENE-706: Updated fileformats.xml|html concerning the docdelta value\n    in the frequency file. (Johan Stuyts, Doron Cohen via Grant Ingersoll)\n\n21. LUCENE-715: Fixed private constructor in IndexWriter.java to\n    properly release the acquired write lock if there is an\n    IOException after acquiring the write lock but before finishing\n    instantiation. (Matthew Bogosian via Mike McCandless)\n\n22. LUCENE-651: Multiple different threads requesting the same\n    FieldCache entry (often for Sorting by a field) at the same\n    time caused multiple generations of that entry, which was\n    detrimental to performance and memory use.\n    (Oliver Hutchison via Otis Gospodnetic)\n\n23. LUCENE-717: Fixed build.xml not to fail when there is no lib dir.\n    (Doron Cohen via Otis Gospodnetic)\n\n24. LUCENE-728: Removed duplicate/old MoreLikethis and SimilarityQueries\n    classes from contrib/similarity, as their new home is under\n    contrib/queries.\n    (Otis Gospodnetic)\n\n25. LUCENE-669: Do not double-close the RandomAccessFile in\n    FSIndexInput/Output during finalize().  Besides sending an\n    IOException up to the GC, this may also be the cause intermittent\n    \"The handle is invalid\" IOExceptions on Windows when trying to\n    close readers or writers. (Michael Busch via Mike McCandless)\n\n26. LUCENE-702: Fix IndexWriter.addIndexes(*) to not corrupt the index\n    on any exceptions (eg disk full).  The semantics of these methods\n    is now transactional: either all indices are merged or none are.\n    Also fixed IndexWriter.mergeSegments (called outside of\n    addIndexes(*) by addDocument, optimize, flushRamSegments) and\n    IndexReader.commit() (called by close) to clean up and keep the\n    instance state consistent to what's actually in the index (Mike\n    McCandless).\n\n27. LUCENE-129: Change finalizers to do \"try {...} finally\n    {super.finalize();}\" to make sure we don't miss finalizers in\n    classes above us. (Esmond Pitt via Mike McCandless)\n\n28. LUCENE-754: Fix a problem introduced by LUCENE-651, causing\n    IndexReaders to hang around forever, in addition to not\n    fixing the original FieldCache performance problem.\n    (Chris Hostetter, Yonik Seeley)\n\n29. LUCENE-140: Fix IndexReader.deleteDocument(int docNum) to\n    correctly raise ArrayIndexOutOfBoundsException when docNum is too\n    large.  Previously, if docNum was only slightly too large (within\n    the same multiple of 8, ie, up to 7 ints beyond maxDoc), no\n    exception would be raised and instead the index would become\n    silently corrupted.  The corruption then only appears much later,\n    in mergeSegments, when the corrupted segment is merged with\n    segment(s) after it. (Mike McCandless)\n\n30. LUCENE-768: Fix case where an Exception during deleteDocument,\n    undeleteAll or setNorm in IndexReader could leave the reader in a\n    state where close() fails to release the write lock.\n    (Mike McCandless)\n\n31. Remove \"tvp\" from known index file extensions because it is\n    never used. (Nicolas Lalevée via Bernhard Messer)\n    \n32. LUCENE-767: Change how SegmentReader.maxDoc() is computed to not\n    rely on file length check and instead use the SegmentInfo's\n    docCount that's already stored explicitly in the index.  this is a\n    defensive bug fix (ie, there is no known problem seen \"in real\n    life\" due to this, just a possible future problem).  (Chuck\n    Williams via Mike McCandless)\n\nOptimizations\n\n  1. LUCENE-586: TermDocs.skipTo() is now more efficient for\n     multi-segment indexes.  this will improve the performance of many\n     types of queries against a non-optimized index. (Andrew Hudson\n     via Yonik Seeley)\n\n  2. LUCENE-623: RAMDirectory.close now nulls out its reference to all\n     internal \"files\", allowing them to be GCed even if references to the\n     RAMDirectory itself still exist. (Nadav Har'El via Chris Hostetter)\n\n  3. LUCENE-629: Compressed fields are no longer uncompressed and\n     recompressed during segment merges (e.g. during indexing or\n     optimizing), thus improving performance . (Michael Busch via Otis\n     Gospodnetic)\n\n  4. LUCENE-388: Improve indexing performance when maxBufferedDocs is\n     large by keeping a count of buffered documents rather than\n     counting after each document addition.  (Doron Cohen, Paul Smith,\n     Yonik Seeley)\n\n  5. Modified TermScorer.explain to use TermDocs.skipTo() instead of\n     looping through docs. (Grant Ingersoll)\n\n  6. LUCENE-672: New indexing segment merge policy flushes all\n     buffered docs to their own segment and delays a merge until\n     mergeFactor segments of a certain level have been accumulated.\n     this increases indexing performance in the presence of deleted\n     docs or partially full segments as well as enabling future\n     optimizations.\n\n     NOTE: this also fixes an \"under-merging\" bug whereby it is\n     possible to get far too many segments in your index (which will\n     drastically slow down search, risks exhausting file descriptor\n     limit, etc.).  this can happen when the number of buffered docs\n     at close, plus the number of docs in the last non-ram segment is\n     greater than mergeFactor. (Ning Li, Yonik Seeley)\n\n  7. Lazy loaded fields unnecessarily retained an extra copy of loaded\n     String data.  (Yonik Seeley)\n\n  8. LUCENE-443: ConjunctionScorer performance increase.  Speed up\n     any BooleanQuery with more than one mandatory clause.\n     (Abdul Chaudhry, Paul Elschot via Yonik Seeley)\n\n  9. LUCENE-365: DisjunctionSumScorer performance increase of\n     ~30%. Speeds up queries with optional clauses. (Paul Elschot via\n     Yonik Seeley)\n\n 10. LUCENE-695: Optimized BufferedIndexInput.readBytes() for medium\n     size buffers, which will speed up merging and retrieving binary\n     and compressed fields.  (Nadav Har'El via Yonik Seeley)\n\n 11. LUCENE-687: Lazy skipping on proximity file speeds up most\n     queries involving term positions, including phrase queries.\n     (Michael Busch via Yonik Seeley)\n\n 12. LUCENE-714: Replaced 2 cases of manual for-loop array copying\n     with calls to System.arraycopy instead, in DocumentWriter.java.\n     (Nicolas Lalevee via Mike McCandless)\n\n 13. LUCENE-729: Non-recursive skipTo and next implementation of\n     TermDocs for a MultiReader.  The old implementation could\n     recurse up to the number of segments in the index. (Yonik Seeley)\n\n 14. LUCENE-739: Improve segment merging performance by reusing\n     the norm array across different fields and doing bulk writes\n     of norms of segments with no deleted docs.\n    (Michael Busch via Yonik Seeley)\n\n 15. LUCENE-745: Add BooleanQuery.clauses(), allowing direct access\n     to the List of clauses and replaced the internal synchronized Vector\n     with an unsynchronized List. (Yonik Seeley)\n\n 16. LUCENE-750: Remove finalizers from FSIndexOutput and move the\n     FSIndexInput finalizer to the actual file so all clones don't\n     register a new finalizer. (Yonik Seeley)\n\nTest Cases\n\n  1. Added TestTermScorer.java (Grant Ingersoll)\n\n  2. Added TestWindowsMMap.java (Benson Margulies via Mike McCandless)\n\n  3. LUCENE-744 Append the user.name property onto the temporary directory \n     that is created so it doesn't interfere with other users. (Grant Ingersoll)\n\nDocumentation\n\n  1. Added style sheet to xdocs named lucene.css and included in the\n     Anakia VSL descriptor.  (Grant Ingersoll)\n\n  2. Added scoring.xml document into xdocs.  Updated Similarity.java\n     scoring formula.(Grant Ingersoll and Steve Rowe.  Updates from:\n     Michael McCandless, Doron Cohen, Chris Hostetter, Doug Cutting).\n     Issue 664.\n\n  3. Added javadocs for FieldSelectorResult.java. (Grant Ingersoll)\n\n  4. Moved xdocs directory to src/site/src/documentation/content/xdocs per\n     Issue 707.  Site now builds using Forrest, just like the other Lucene\n     siblings.  See http://wiki.apache.org/jakarta-lucene/HowToUpdateTheWebsite\n     for info on updating the website. (Grant Ingersoll with help from Steve Rowe,\n     Chris Hostetter, Doug Cutting, Otis Gospodnetic, Yonik Seeley)\n\n  5. Added in Developer and System Requirements sections under Resources (Grant Ingersoll)\n\n  6. LUCENE-713 Updated the Term Vector section of File Formats to include\n     documentation on how Offset and Position info are stored in the TVF file.\n     (Grant Ingersoll, Samir Abdou)\n\n  7. Added in link to Clover Test Code Coverage Reports under the Develop\n     section in Resources (Grant Ingersoll)\n\n  8. LUCENE-748: Added details for semantics of IndexWriter.close on\n     hitting an Exception.  (Jed Wesley-Smith via Mike McCandless)\n\n  9. Added some text about what is contained in releases.\n     (Eric Haszlakiewicz via Grant Ingersoll)\n\n  10. LUCENE-758: Fix javadoc to clarify that RAMDirectory(Directory)\n      makes a full copy of the starting Directory.  (Mike McCandless)\n\n  11. LUCENE-764: Fix javadocs to detail temporary space requirements\n      for IndexWriter's optimize(), addIndexes(*) and addDocument(...)\n      methods.  (Mike McCandless)\n\nBuild\n\n  1. Added in clover test code coverage per http://issues.apache.org/jira/browse/LUCENE-721\n     To enable clover code coverage, you must have clover.jar in the ANT\n     classpath and specify -Drun.clover=true on the command line.\n     (Michael Busch and Grant Ingersoll)\n\n  2. Added a sysproperty in common-build.xml per Lucene 752 to map java.io.tmpdir to\n     ${build.dir}/test just like the tempDir sysproperty.\n\n  3. LUCENE-757 Added new target named init-dist that does setup for\n     distribution of both binary and source distributions.  Called by package \n     and package-*-src\n\n======================= Release 2.0.0 2006-05-26 =======================\n\nAPI Changes\n\n 1. All deprecated methods and fields have been removed, except\n    DateField, which will still be supported for some time\n    so Lucene can read its date fields from old indexes\n    (Yonik Seeley & Grant Ingersoll)\n\n 2. DisjunctionSumScorer is no longer public.\n    (Paul Elschot via Otis Gospodnetic)\n\n 3. Creating a Field with both an empty name and an empty value\n    now throws an IllegalArgumentException\n    (Daniel Naber)\n\n 4. LUCENE-301: Added new IndexWriter({String,File,Directory},\n    Analyzer) constructors that do not take a boolean \"create\"\n    argument.  These new constructors will create a new index if\n    necessary, else append to the existing one.  (Dan Armbrust via\n    Mike McCandless)\n\nNew features\n\n 1. LUCENE-496: Command line tool for modifying the field norms of an\n    existing index; added to contrib/miscellaneous.  (Chris Hostetter)\n\n 2. LUCENE-577: SweetSpotSimilarity added to contrib/miscellaneous.\n    (Chris Hostetter)\n    \nBug fixes\n\n 1. LUCENE-330: Fix issue of FilteredQuery not working properly within\n    BooleanQuery.  (Paul Elschot via Erik Hatcher)\n\n 2. LUCENE-515: Make ConstantScoreRangeQuery and ConstantScoreQuery work\n    with RemoteSearchable.  (Philippe Laflamme via Yonik Seeley)\n\n 3. Added methods to get/set writeLockTimeout and commitLockTimeout in\n    IndexWriter. These could be set in Lucene 1.4 using a system property.\n    this feature had been removed without adding the corresponding\n    getter/setter methods.  (Daniel Naber)\n\n 4. LUCENE-413: Fixed ArrayIndexOutOfBoundsException exceptions\n    when using SpanQueries. (Paul Elschot via Yonik Seeley)\n\n 5. Implemented FilterIndexReader.getVersion() and isCurrent()\n    (Yonik Seeley)\n\n 6. LUCENE-540: Fixed a bug with IndexWriter.addIndexes(Directory[])\n    that sometimes caused the index order of documents to change.\n    (Yonik Seeley)\n\n 7. LUCENE-526: Fixed a bug in FieldSortedHitQueue that caused\n    subsequent String sorts with different locales to sort identically.\n    (Paul Cowan via Yonik Seeley)\n\n 8. LUCENE-541: Add missing extractTerms() to DisjunctionMaxQuery\n    (Stefan Will via Yonik Seeley)\n\n 9. LUCENE-514: Added getTermArrays() and extractTerms() to\n    MultiPhraseQuery (Eric Jain & Yonik Seeley)\n\n10. LUCENE-512: Fixed ClassCastException in ParallelReader.getTermFreqVectors\n    (frederic via Yonik)\n\n11. LUCENE-352: Fixed bug in SpanNotQuery that manifested as\n    NullPointerException when \"exclude\" query was not a SpanTermQuery.\n    (Chris Hostetter)\n\n12. LUCENE-572: Fixed bug in SpanNotQuery hashCode, was ignoring exclude clause\n    (Chris Hostetter)\n\n13. LUCENE-561: Fixed some ParallelReader bugs. NullPointerException if the reader\n    didn't know about the field yet, reader didn't keep track if it had deletions,\n    and deleteDocument calls could circumvent synchronization on the subreaders.\n    (Chuck Williams via Yonik Seeley)\n\n14. LUCENE-556: Added empty extractTerms() implementation to MatchAllDocsQuery and\n    ConstantScoreQuery in order to allow their use with a MultiSearcher.\n    (Yonik Seeley)\n\n15. LUCENE-546: Removed 2GB file size limitations for RAMDirectory.\n    (Peter Royal, Michael Chan, Yonik Seeley)\n\n16. LUCENE-485: Don't hold commit lock while removing obsolete index\n    files.  (Luc Vanlerberghe via cutting)\n\n\n1.9.1\n\nBug fixes\n\n 1. LUCENE-511: Fix a bug in the BufferedIndexOutput optimization\n    introduced in 1.9-final.  (Shay Banon & Steven Tamm via cutting)\n\n1.9 final\n\nNote that this release is mostly but not 100% source compatible with\nthe previous release of Lucene (1.4.3). In other words, you should\nmake sure your application compiles with this version of Lucene before\nyou replace the old Lucene JAR with the new one.  Many methods have\nbeen deprecated in anticipation of release 2.0, so deprecation\nwarnings are to be expected when upgrading from 1.4.3 to 1.9.\n\nBug fixes\n \n 1. The fix that made IndexWriter.setMaxBufferedDocs(1) work had negative \n    effects on indexing performance and has thus been reverted. The \n    argument for setMaxBufferedDocs(int) must now at least be 2, otherwise\n    an exception is thrown. (Daniel Naber)\n \nOptimizations\n     \n 1. Optimized BufferedIndexOutput.writeBytes() to use\n    System.arraycopy() in more cases, rather than copying byte-by-byte.\n    (Lukas Zapletal via Cutting)\n\n1.9 RC1\n\nRequirements\n\n 1. To compile and use Lucene you now need Java 1.4 or later.\n\nChanges in runtime behavior\n\n 1. FuzzyQuery can no longer throw a TooManyClauses exception. If a\n    FuzzyQuery expands to more than BooleanQuery.maxClauseCount\n    terms only the BooleanQuery.maxClauseCount most similar terms\n    go into the rewritten query and thus the exception is avoided.\n    (Christoph)\n\n 2. Changed system property from \"org.apache.lucene.lockdir\" to\n    \"org.apache.lucene.lockDir\", so that its casing follows the existing\n    pattern used in other Lucene system properties. (Bernhard)\n\n 3. The terms of RangeQueries and FuzzyQueries are now converted to\n    lowercase by default (as it has been the case for PrefixQueries\n    and WildcardQueries before). Use setLowercaseExpandedTerms(false)\n    to disable that behavior but note that this also affects\n    PrefixQueries and WildcardQueries. (Daniel Naber)\n\n 4. Document frequency that is computed when MultiSearcher is used is now\n    computed correctly and \"globally\" across subsearchers and indices, while\n    before it used to be computed locally to each index, which caused\n    ranking across multiple indices not to be equivalent.\n    (Chuck Williams, Wolf Siberski via Otis, bug #31841)\n\n 5. When opening an IndexWriter with create=true, Lucene now only deletes\n    its own files from the index directory (looking at the file name suffixes\n    to decide if a file belongs to Lucene). The old behavior was to delete\n    all files. (Daniel Naber and Bernhard Messer, bug #34695)\n\n 6. The version of an IndexReader, as returned by getCurrentVersion()\n    and getVersion() doesn't start at 0 anymore for new indexes. Instead, it\n    is now initialized by the system time in milliseconds.\n    (Bernhard Messer via Daniel Naber)\n\n 7. Several default values cannot be set via system properties anymore, as\n    this has been considered inappropriate for a library like Lucene. For\n    most properties there are set/get methods available in IndexWriter which\n    you should use instead. this affects the following properties:\n    See IndexWriter for getter/setter methods:\n      org.apache.lucene.writeLockTimeout, org.apache.lucene.commitLockTimeout,\n      org.apache.lucene.minMergeDocs, org.apache.lucene.maxMergeDocs,\n      org.apache.lucene.maxFieldLength, org.apache.lucene.termIndexInterval,\n      org.apache.lucene.mergeFactor,\n    See BooleanQuery for getter/setter methods:\n      org.apache.lucene.maxClauseCount\n    See FSDirectory for getter/setter methods:\n      disableLuceneLocks\n    (Daniel Naber)\n\n 8. Fixed FieldCacheImpl to use user-provided IntParser and FloatParser,\n    instead of using Integer and Float classes for parsing.\n    (Yonik Seeley via Otis Gospodnetic)\n\n 9. Expert level search routines returning TopDocs and TopFieldDocs\n    no longer normalize scores.  this also fixes bugs related to\n    MultiSearchers and score sorting/normalization.\n    (Luc Vanlerberghe via Yonik Seeley, LUCENE-469)\n\nNew features\n\n 1. Added support for stored compressed fields (patch #31149)\n    (Bernhard Messer via Christoph)\n\n 2. Added support for binary stored fields (patch #29370)\n    (Drew Farris and Bernhard Messer via Christoph)\n\n 3. Added support for position and offset information in term vectors\n    (patch #18927). (Grant Ingersoll & Christoph)\n\n 4. A new class DateTools has been added. It allows you to format dates\n    in a readable format adequate for indexing. Unlike the existing\n    DateField class DateTools can cope with dates before 1970 and it\n    forces you to specify the desired date resolution (e.g. month, day,\n    second, ...) which can make RangeQuerys on those fields more efficient.\n    (Daniel Naber)\n\n 5. QueryParser now correctly works with Analyzers that can return more\n    than one token per position. For example, a query \"+fast +car\"\n    would be parsed as \"+fast +(car automobile)\" if the Analyzer\n    returns \"car\" and \"automobile\" at the same position whenever it\n    finds \"car\" (Patch #23307).\n    (Pierrick Brihaye, Daniel Naber)\n\n 6. Permit unbuffered Directory implementations (e.g., using mmap).\n    InputStream is replaced by the new classes IndexInput and\n    BufferedIndexInput.  OutputStream is replaced by the new classes\n    IndexOutput and BufferedIndexOutput.  InputStream and OutputStream\n    are now deprecated and FSDirectory is now subclassable. (cutting)\n\n 7. Add native Directory and TermDocs implementations that work under\n    GCJ.  These require GCC 3.4.0 or later and have only been tested\n    on Linux.  Use 'ant gcj' to build demo applications. (cutting)\n\n 8. Add MMapDirectory, which uses nio to mmap input files.  this is\n    still somewhat slower than FSDirectory.  However it uses less\n    memory per query term, since a new buffer is not allocated per\n    term, which may help applications which use, e.g., wildcard\n    queries.  It may also someday be faster. (cutting & Paul Elschot)\n\n 9. Added javadocs-internal to build.xml - bug #30360\n    (Paul Elschot via Otis)\n\n10. Added RangeFilter, a more generically useful filter than DateFilter.\n    (Chris M Hostetter via Erik)\n\n11. Added NumberTools, a utility class indexing numeric fields.\n    (adapted from code contributed by Matt Quail; committed by Erik)\n\n12. Added public static IndexReader.main(String[] args) method.\n    IndexReader can now be used directly at command line level\n    to list and optionally extract the individual files from an existing\n    compound index file.\n    (adapted from code contributed by Garrett Rooney; committed by Bernhard)\n\n13. Add IndexWriter.setTermIndexInterval() method.  See javadocs.\n    (Doug Cutting)\n\n14. Added LucenePackage, whose static get() method returns java.util.Package,\n    which lets the caller get the Lucene version information specified in\n    the Lucene Jar.\n    (Doug Cutting via Otis)\n\n15. Added Hits.iterator() method and corresponding HitIterator and Hit objects.\n    this provides standard java.util.Iterator iteration over Hits.\n    Each call to the iterator's next() method returns a Hit object.\n    (Jeremy Rayner via Erik)\n\n16. Add ParallelReader, an IndexReader that combines separate indexes\n    over different fields into a single virtual index.  (Doug Cutting)\n\n17. Add IntParser and FloatParser interfaces to FieldCache, so that\n    fields in arbitrarily formats can be cached as ints and floats.\n    (Doug Cutting)\n\n18. Added class org.apache.lucene.index.IndexModifier which combines\n    IndexWriter and IndexReader, so you can add and delete documents without\n    worrying about synchronization/locking issues.\n    (Daniel Naber)\n\n19. Lucene can now be used inside an unsigned applet, as Lucene's access\n    to system properties will not cause a SecurityException anymore.\n    (Jon Schuster via Daniel Naber, bug #34359)\n\n20. Added a new class MatchAllDocsQuery that matches all documents.\n    (John Wang via Daniel Naber, bug #34946)\n\n21. Added ability to omit norms on a per field basis to decrease\n    index size and memory consumption when there are many indexed fields.\n    See Field.setOmitNorms()\n    (Yonik Seeley, LUCENE-448)\n\n22. Added NullFragmenter to contrib/highlighter, which is useful for\n    highlighting entire documents or fields.\n    (Erik Hatcher)\n\n23. Added regular expression queries, RegexQuery and SpanRegexQuery.\n    Note the same term enumeration caveats apply with these queries as\n    apply to WildcardQuery and other term expanding queries.\n    These two new queries are not currently supported via QueryParser.\n    (Erik Hatcher)\n\n24. Added ConstantScoreQuery which wraps a filter and produces a score\n    equal to the query boost for every matching document.\n    (Yonik Seeley, LUCENE-383)\n\n25. Added ConstantScoreRangeQuery which produces a constant score for\n    every document in the range.  One advantage over a normal RangeQuery\n    is that it doesn't expand to a BooleanQuery and thus doesn't have a maximum\n    number of terms the range can cover.  Both endpoints may also be open.\n    (Yonik Seeley, LUCENE-383)\n\n26. Added ability to specify a minimum number of optional clauses that\n    must match in a BooleanQuery.  See BooleanQuery.setMinimumNumberShouldMatch().\n    (Paul Elschot, Chris Hostetter via Yonik Seeley, LUCENE-395)\n\n27. Added DisjunctionMaxQuery which provides the maximum score across its clauses.\n    It's very useful for searching across multiple fields.\n    (Chuck Williams via Yonik Seeley, LUCENE-323)\n\n28. New class ISOLatin1AccentFilter that replaces accented characters in the ISO\n    Latin 1 character set by their unaccented equivalent.\n    (Sven Duzont via Erik Hatcher)\n\n29. New class KeywordAnalyzer. \"Tokenizes\" the entire stream as a single token.\n    this is useful for data like zip codes, ids, and some product names.\n    (Erik Hatcher)\n\n30. Copied LengthFilter from contrib area to core. Removes words that are too\n    long and too short from the stream.\n    (David Spencer via Otis and Daniel)\n\n31. Added getPositionIncrementGap(String fieldName) to Analyzer.  this allows\n    custom analyzers to put gaps between Field instances with the same field\n    name, preventing phrase or span queries crossing these boundaries.  The\n    default implementation issues a gap of 0, allowing the default token\n    position increment of 1 to put the next field's first token into a\n    successive position.\n    (Erik Hatcher, with advice from Yonik)\n\n32. StopFilter can now ignore case when checking for stop words.\n    (Grant Ingersoll via Yonik, LUCENE-248)\n\n33. Add TopDocCollector and TopFieldDocCollector.  These simplify the\n    implementation of hit collectors that collect only the\n    top-scoring or top-sorting hits.\n\nAPI Changes\n\n 1. Several methods and fields have been deprecated. The API documentation\n    contains information about the recommended replacements. It is planned\n    that most of the deprecated methods and fields will be removed in\n    Lucene 2.0. (Daniel Naber)\n\n 2. The Russian and the German analyzers have been moved to contrib/analyzers.\n    Also, the WordlistLoader class has been moved one level up in the\n    hierarchy and is now Lucene.Net.Analysis.WordlistLoader\n    (Daniel Naber)\n\n 3. The API contained methods that declared to throw an IOException\n    but that never did this. These declarations have been removed. If\n    your code tries to catch these exceptions you might need to remove\n    those catch clauses to avoid compile errors. (Daniel Naber)\n\n 4. Add a serializable Parameter Class to standardize parameter enum\n    classes in BooleanClause and Field. (Christoph)\n\n 5. Added rewrite methods to all SpanQuery subclasses that nest other SpanQuerys.\n    this allows custom SpanQuery subclasses that rewrite (for term expansion, for\n    example) to nest within the built-in SpanQuery classes successfully.\n\nBug fixes\n\n 1. The JSP demo page (src/jsp/results.jsp) now properly closes the\n    IndexSearcher it opens. (Daniel Naber)\n\n 2. Fixed a bug in IndexWriter.addIndexes(IndexReader[] readers) that\n    prevented deletion of obsolete segments. (Christoph Goller)\n\n 3. Fix in FieldInfos to avoid the return of an extra blank field in\n    IndexReader.getFieldNames() (Patch #19058). (Mark Harwood via Bernhard)\n\n 4. Some combinations of BooleanQuery and MultiPhraseQuery (formerly\n    PhrasePrefixQuery) could provoke UnsupportedOperationException\n    (bug #33161). (Rhett Sutphin via Daniel Naber)\n\n 5. Small bug in skipTo of ConjunctionScorer that caused NullPointerException\n    if skipTo() was called without prior call to next() fixed. (Christoph)\n\n 6. Disable Similiarty.coord() in the scoring of most automatically\n    generated boolean queries.  The coord() score factor is\n    appropriate when clauses are independently specified by a user,\n    but is usually not appropriate when clauses are generated\n    automatically, e.g., by a fuzzy, wildcard or range query.  Matches\n    on such automatically generated queries are no longer penalized\n    for not matching all terms.  (Doug Cutting, Patch #33472)\n\n 7. Getting a lock file with Lock.obtain(long) was supposed to wait for\n    a given amount of milliseconds, but this didn't work.\n    (John Wang via Daniel Naber, Bug #33799)\n\n 8. Fix FSDirectory.createOutput() to always create new files.\n    Previously, existing files were overwritten, and an index could be\n    corrupted when the old version of a file was longer than the new.\n    Now any existing file is first removed.  (Doug Cutting)\n\n 9. Fix BooleanQuery containing nested SpanTermQuery's, which previously\n    could return an incorrect number of hits.\n    (Reece Wilton via Erik Hatcher, Bug #35157)\n\n10. Fix NullPointerException that could occur with a MultiPhraseQuery\n    inside a BooleanQuery.\n    (Hans Hjelm and Scotty Allen via Daniel Naber, Bug #35626)\n\n11. Fixed SnowballFilter to pass through the position increment from\n    the original token.\n    (Yonik Seeley via Erik Hatcher, LUCENE-437)\n\n12. Added Unicode range of Korean characters to StandardTokenizer,\n    grouping contiguous characters into a token rather than one token\n    per character.  this change also changes the token type to \"<CJ>\"\n    for Chinese and Japanese character tokens (previously it was \"<CJK>\").\n    (Cheolgoo Kang via Otis and Erik, LUCENE-444 and LUCENE-461)\n\n13. FieldsReader now looks at FieldInfo.storeOffsetWithTermVector and\n    FieldInfo.storePositionWithTermVector and creates the Field with\n    correct TermVector parameter.\n    (Frank Steinmann via Bernhard, LUCENE-455)\n\n14. Fixed WildcardQuery to prevent \"cat\" matching \"ca??\".\n    (Xiaozheng Ma via Bernhard, LUCENE-306)\n\n15. Fixed a bug where MultiSearcher and ParallelMultiSearcher could\n    change the sort order when sorting by string for documents without\n    a value for the sort field.\n    (Luc Vanlerberghe via Yonik, LUCENE-453)\n\n16. Fixed a sorting problem with MultiSearchers that can lead to\n    missing or duplicate docs due to equal docs sorting in an arbitrary order.\n    (Yonik Seeley, LUCENE-456)\n\n17. A single hit using the expert level sorted search methods\n    resulted in the score not being normalized.\n    (Yonik Seeley, LUCENE-462)\n\n18. Fixed inefficient memory usage when loading an index into RAMDirectory.\n    (Volodymyr Bychkoviak via Bernhard, LUCENE-475)\n\n19. Corrected term offsets returned by ChineseTokenizer.\n    (Ray Tsang via Erik Hatcher, LUCENE-324)\n\n20. Fixed MultiReader.undeleteAll() to correctly update numDocs.\n    (Robert Kirchgessner via Doug Cutting, LUCENE-479)\n\n21. Race condition in IndexReader.getCurrentVersion() and isCurrent()\n    fixed by acquiring the commit lock.\n    (Luc Vanlerberghe via Yonik Seeley, LUCENE-481)\n\n22. IndexWriter.setMaxBufferedDocs(1) didn't have the expected effect,\n    this has now been fixed. (Daniel Naber)\n\n23. Fixed QueryParser when called with a date in local form like \n    \"[1/16/2000 TO 1/18/2000]\". this query did not include the documents\n    of 1/18/2000, i.e. the last day was not included. (Daniel Naber)\n\n24. Removed sorting constraint that threw an exception if there were\n    not yet any values for the sort field (Yonik Seeley, LUCENE-374)\n\nOptimizations\n     \n 1. Disk usage (peak requirements during indexing and optimization)\n    in case of compound file format has been improved. \n    (Bernhard, Dmitry, and Christoph)\n\n 2. Optimize the performance of certain uses of BooleanScorer,\n    TermScorer and IndexSearcher.  In particular, a BooleanQuery\n    composed of TermQuery, with not all terms required, that returns a\n    TopDocs (e.g., through a Hits with no Sort specified) runs much\n    faster.  (cutting)\n    \n 3. Removed synchronization from reading of term vectors with an\n    IndexReader (Patch #30736). (Bernhard Messer via Christoph)\n\n 4. Optimize term-dictionary lookup to allocate far fewer terms when\n    scanning for the matching term.  this speeds searches involving\n    low-frequency terms, where the cost of dictionary lookup can be\n    significant. (cutting)\n\n 5. Optimize fuzzy queries so the standard fuzzy queries with a prefix \n    of 0 now run 20-50% faster (Patch #31882).\n    (Jonathan Hager via Daniel Naber)\n    \n 6. A Version of BooleanScorer (BooleanScorer2) added that delivers\n    documents in increasing order and implements skipTo. For queries\n    with required or forbidden clauses it may be faster than the old\n    BooleanScorer, for BooleanQueries consisting only of optional\n    clauses it is probably slower. The new BooleanScorer is now the\n    default. (Patch 31785 by Paul Elschot via Christoph)\n\n 7. Use uncached access to norms when merging to reduce RAM usage.\n    (Bug #32847).  (Doug Cutting)\n\n 8. Don't read term index when random-access is not required.  this\n    reduces time to open IndexReaders and they use less memory when\n    random access is not required, e.g., when merging segments.  The\n    term index is now read into memory lazily at the first\n    random-access.  (Doug Cutting)\n\n 9. Optimize IndexWriter.addIndexes(Directory[]) when the number of\n    added indexes is larger than mergeFactor.  Previously this could\n    result in quadratic performance.  Now performance is n log(n).\n    (Doug Cutting)\n\n10. Speed up the creation of TermEnum for indices with multiple\n    segments and deleted documents, and thus speed up PrefixQuery,\n    RangeQuery, WildcardQuery, FuzzyQuery, RangeFilter, DateFilter,\n    and sorting the first time on a field.\n    (Yonik Seeley, LUCENE-454)\n\n11. Optimized and generalized 32 bit floating point to byte\n    (custom 8 bit floating point) conversions.  Increased the speed of\n    Similarity.encodeNorm() anywhere from 10% to 250%, depending on the JVM.\n    (Yonik Seeley, LUCENE-467)\n\nInfrastructure\n\n 1. Lucene's source code repository has converted from CVS to\n    Subversion.  The new repository is at\n    http://svn.apache.org/repos/asf/lucene/java/trunk\n\n 2. Lucene's issue tracker has migrated from Bugzilla to JIRA.\n    Lucene's JIRA is at http://issues.apache.org/jira/browse/LUCENE\n    The old issues are still available at\n    http://issues.apache.org/bugzilla/show_bug.cgi?id=xxxx\n    (use the bug number instead of xxxx)\n\n\n1.4.3\n\n 1. The JSP demo page (src/jsp/results.jsp) now properly escapes error\n    messages which might contain user input (e.g. error messages about \n    query parsing). If you used that page as a starting point for your\n    own code please make sure your code also properly escapes HTML\n    characters from user input in order to avoid so-called cross site\n    scripting attacks. (Daniel Naber)\n  \n  2. QueryParser changes in 1.4.2 broke the QueryParser API. Now the old \n     API is supported again. (Christoph)\n\n\n1.4.2\n\n 1. Fixed bug #31241: Sorting could lead to incorrect results (documents\n    missing, others duplicated) if the sort keys were not unique and there\n    were more than 100 matches. (Daniel Naber)\n\n 2. Memory leak in Sort code (bug #31240) eliminated.\n    (Rafal Krzewski via Christoph and Daniel)\n    \n 3. FuzzyQuery now takes an additional parameter that specifies the\n    minimum similarity that is required for a term to match the query.\n    The QueryParser syntax for this is term~x, where x is a floating \n    point number >= 0 and < 1 (a bigger number means that a higher\n    similarity is required). Furthermore, a prefix can be specified\n    for FuzzyQuerys so that only those terms are considered similar that \n    start with this prefix. this can speed up FuzzyQuery greatly.\n    (Daniel Naber, Christoph Goller)\n    \n 4. PhraseQuery and PhrasePrefixQuery now allow the explicit specification\n    of relative positions. (Christoph Goller)\n    \n 5. QueryParser changes: Fix for ArrayIndexOutOfBoundsExceptions \n    (patch #9110); some unused method parameters removed; The ability\n    to specify a minimum similarity for FuzzyQuery has been added.\n    (Christoph Goller)\n\n 6. IndexSearcher optimization: a new ScoreDoc is no longer allocated\n    for every non-zero-scoring hit.  this makes 'OR' queries that\n    contain common terms substantially faster.  (cutting)\n\n\n1.4.1\n\n 1. Fixed a performance bug in hit sorting code, where values were not\n    correctly cached.  (Aviran via cutting)\n\n 2. Fixed errors in file format documentation. (Daniel Naber)\n\n\n1.4 final\n\n 1. Added \"an\" to the list of stop words in StopAnalyzer, to complement\n    the existing \"a\" there.  Fix for bug 28960\n     (http://issues.apache.org/bugzilla/show_bug.cgi?id=28960). (Otis)\n\n 2. Added new class FieldCache to manage in-memory caches of field term\n    values.  (Tim Jones)\n\n 3. Added overloaded getFieldQuery method to QueryParser which\n    accepts the slop factor specified for the phrase (or the default\n    phrase slop for the QueryParser instance).  this allows overriding\n    methods to replace a PhraseQuery with a SpanNearQuery instead,\n    keeping the proper slop factor. (Erik Hatcher)\n\n 4. Changed the encoding of GermanAnalyzer.java and GermanStemmer.java to\n    UTF-8 and changed the build encoding to UTF-8, to make changed files\n    compile. (Otis Gospodnetic)\n\n 5. Removed synchronization from term lookup under IndexReader methods\n    termFreq(), termDocs() or termPositions() to improve\n    multi-threaded performance.  (cutting)\n\n 6. Fix a bug where obsolete segment files were not deleted on Win32.\n\n\n1.4 RC3\n\n 1. Fixed several search bugs introduced by the skipTo() changes in\n    release 1.4RC1.  The index file format was changed a bit, so\n    collections must be re-indexed to take advantage of the skipTo()\n    optimizations.  (Christoph Goller)\n\n 2. Added new Document methods, removeField() and removeFields().\n    (Christoph Goller)\n\n 3. Fixed inconsistencies with index closing.  Indexes and directories\n    are now only closed automatically by Lucene when Lucene opened\n    them automatically.  (Christoph Goller)\n\n 4. Added new class: FilteredQuery.  (Tim Jones)\n\n 5. Added a new SortField type for custom comparators.  (Tim Jones)\n\n 6. Lock obtain timed out message now displays the full path to the lock\n    file. (Daniel Naber via Erik)\n\n 7. Fixed a bug in SpanNearQuery when ordered. (Paul Elschot via cutting)\n\n 8. Fixed so that FSDirectory's locks still work when the\n    java.io.tmpdir system property is null.  (cutting)\n\n 9. Changed FilteredTermEnum's constructor to take no parameters,\n    as the parameters were ignored anyway (bug #28858)\n\n1.4 RC2\n\n 1. GermanAnalyzer now throws an exception if the stopword file\n    cannot be found (bug #27987). It now uses LowerCaseFilter\n    (bug #18410) (Daniel Naber via Otis, Erik)\n\n 2. Fixed a few bugs in the file format documentation. (cutting)\n\n\n1.4 RC1\n\n 1. Changed the format of the .tis file, so that:\n\n    - it has a format version number, which makes it easier to\n      back-compatibly change file formats in the future.\n\n    - the term count is now stored as a long.  this was the one aspect\n      of the Lucene's file formats which limited index size.\n\n    - a few internal index parameters are now stored in the index, so\n      that they can (in theory) now be changed from index to index,\n      although there is not yet an API to do so.\n\n    These changes are back compatible.  The new code can read old\n    indexes.  But old code will not be able read new indexes. (cutting)\n\n 2. Added an optimized implementation of TermDocs.skipTo().  A skip\n    table is now stored for each term in the .frq file.  this only\n    adds a percent or two to overall index size, but can substantially\n    speedup many searches.  (cutting)\n\n 3. Restructured the Scorer API and all Scorer implementations to take\n    advantage of an optimized TermDocs.skipTo() implementation.  In\n    particular, PhraseQuerys and conjunctive BooleanQuerys are\n    faster when one clause has substantially fewer matches than the\n    others.  (A conjunctive BooleanQuery is a BooleanQuery where all\n    clauses are required.)  (cutting)\n\n 4. Added new class ParallelMultiSearcher.  Combined with\n    RemoteSearchable this makes it easy to implement distributed\n    search systems.  (Jean-Francois Halleux via cutting)\n\n 5. Added support for hit sorting.  Results may now be sorted by any\n    indexed field.  For details see the javadoc for\n    Searcher#search(Query, Sort).  (Tim Jones via Cutting)\n\n 6. Changed FSDirectory to auto-create a full directory tree that it\n    needs by using mkdirs() instead of mkdir().  (Mladen Turk via Otis)\n\n 7. Added a new span-based query API.  this implements, among other\n    things, nested phrases.  See javadocs for details.  (Doug Cutting)\n\n 8. Added new method Query.getSimilarity(Searcher), and changed\n    scorers to use it.  this permits one to subclass a Query class so\n    that it can specify its own Similarity implementation, perhaps\n    one that delegates through that of the Searcher.  (Julien Nioche\n    via Cutting)\n\n 9. Added MultiReader, an IndexReader that combines multiple other\n    IndexReaders.  (Cutting)\n\n10. Added support for term vectors.  See Field#isTermVectorStored().\n    (Grant Ingersoll, Cutting & Dmitry)\n\n11. Fixed the old bug with escaping of special characters in query\n    strings: http://issues.apache.org/bugzilla/show_bug.cgi?id=24665\n    (Jean-Francois Halleux via Otis)\n\n12. Added support for overriding default values for the following,\n    using system properties:\n      - default commit lock timeout\n      - default maxFieldLength\n      - default maxMergeDocs\n      - default mergeFactor\n      - default minMergeDocs\n      - default write lock timeout\n    (Otis)\n\n13. Changed QueryParser.jj to allow '-' and '+' within tokens:\n    http://issues.apache.org/bugzilla/show_bug.cgi?id=27491\n    (Morus Walter via Otis)\n\n14. Changed so that the compound index format is used by default.\n    this makes indexing a bit slower, but vastly reduces the chances\n    of file handle problems.  (Cutting)\n\n\n1.3 final\n\n 1. Added catch of BooleanQuery$TooManyClauses in QueryParser to\n    throw ParseException instead. (Erik Hatcher)\n\n 2. Fixed a NullPointerException in Query.explain(). (Doug Cutting)\n\n 3. Added a new method IndexReader.setNorm(), that permits one to\n    alter the boosting of fields after an index is created.\n\n 4. Distinguish between the final position and length when indexing a\n    field.  The length is now defined as the total number of tokens,\n    instead of the final position, as it was previously.  Length is\n    used for score normalization (Similarity.lengthNorm()) and for\n    controlling memory usage (IndexWriter.maxFieldLength).  In both of\n    these cases, the total number of tokens is a better value to use\n    than the final token position.  Position is used in phrase\n    searching (see PhraseQuery and Token.setPositionIncrement()).\n\n 5. Fix StandardTokenizer's handling of CJK characters (Chinese,\n    Japanese and Korean ideograms).  Previously contiguous sequences\n    were combined in a single token, which is not very useful.  Now\n    each ideogram generates a separate token, which is more useful.\n\n\n1.3 RC3\n\n 1. Added minMergeDocs in IndexWriter.  this can be raised to speed\n    indexing without altering the number of files, but only using more\n    memory.  (Julien Nioche via Otis)\n\n 2. Fix bug #24786, in query rewriting. (bschneeman via Cutting)\n\n 3. Fix bug #16952, in demo HTML parser, skip comments in\n    javascript. (Christoph Goller)\n\n 4. Fix bug #19253, in demo HTML parser, add whitespace as needed to\n    output (Daniel Naber via Christoph Goller)\n\n 5. Fix bug #24301, in demo HTML parser, long titles no longer\n    hang things. (Christoph Goller)\n\n 6. Fix bug #23534, Replace use of file timestamp of segments file\n    with an index version number stored in the segments file.  this\n    resolves problems when running on file systems with low-resolution\n    timestamps, e.g., HFS under MacOS X.  (Christoph Goller)\n\n 7. Fix QueryParser so that TokenMgrError is not thrown, only\n    ParseException.  (Erik Hatcher)\n\n 8. Fix some bugs introduced by change 11 of RC2.  (Christoph Goller)\n\n 9. Fixed a problem compiling TestRussianStem.  (Christoph Goller)\n\n10. Cleaned up some build stuff.  (Erik Hatcher)\n\n\n1.3 RC2\n\n 1. Added getFieldNames(boolean) to IndexReader, SegmentReader, and\n    SegmentsReader. (Julien Nioche via otis)\n\n 2. Changed file locking to place lock files in\n    System.getProperty(\"java.io.tmpdir\"), where all users are\n    permitted to write files.  this way folks can open and correctly\n    lock indexes which are read-only to them.\n\n 3. IndexWriter: added a new method, addDocument(Document, Analyzer),\n    permitting one to easily use different analyzers for different\n    documents in the same index.\n\n 4. Minor enhancements to FuzzyTermEnum.\n    (Christoph Goller via Otis)\n\n 5. PriorityQueue: added insert(Object) method and adjusted IndexSearcher\n    and MultiIndexSearcher to use it.\n    (Christoph Goller via Otis)\n\n 6. Fixed a bug in IndexWriter that returned incorrect docCount().\n    (Christoph Goller via Otis)\n\n 7. Fixed SegmentsReader to eliminate the confusing and slightly different\n    behaviour of TermEnum when dealing with an enumeration of all terms,\n    versus an enumeration starting from a specific term.\n    this patch also fixes incorrect term document frequencies when the same term\n    is present in multiple segments.\n    (Christoph Goller via Otis)\n\n 8. Added CachingWrapperFilter and PerFieldAnalyzerWrapper. (Erik Hatcher)\n\n 9. Added support for the new \"compound file\" index format (Dmitry\n    Serebrennikov)\n\n10. Added Locale setting to QueryParser, for use by date range parsing.\n\n11. Changed IndexReader so that it can be subclassed by classes\n    outside of its package.  Previously it had package-private\n    abstract methods.  Also modified the index merging code so that it\n    can work on an arbitrary IndexReader implementation, and added a\n    new method, IndexWriter.addIndexes(IndexReader[]), to take\n    advantage of this. (cutting)\n\n12. Added a limit to the number of clauses which may be added to a\n    BooleanQuery.  The default limit is 1024 clauses.  this should\n    stop most OutOfMemoryExceptions by prefix, wildcard and fuzzy\n    queries which run amok. (cutting)\n\n13. Add new method: IndexReader.undeleteAll().  this undeletes all\n    deleted documents which still remain in the index. (cutting)\n\n\n1.3 RC1\n\n 1. Fixed PriorityQueue's clear() method.\n    Fix for bug 9454, http://nagoya.apache.org/bugzilla/show_bug.cgi?id=9454\n    (Matthijs Bomhoff via otis)\n\n 2. Changed StandardTokenizer.jj grammar for EMAIL tokens.\n    Fix for bug 9015, http://nagoya.apache.org/bugzilla/show_bug.cgi?id=9015\n    (Dale Anson via otis)\n\n 3. Added the ability to disable lock creation by using disableLuceneLocks\n    system property.  this is useful for read-only media, such as CD-ROMs.\n    (otis)\n\n 4. Added id method to Hits to be able to access the index global id.\n    Required for sorting options.\n    (carlson)\n\n 5. Added support for new range query syntax to QueryParser.jj.\n    (briangoetz)\n\n 6. Added the ability to retrieve HTML documents' META tag values to\n    HTMLParser.jj.\n    (Mark Harwood via otis)\n\n 7. Modified QueryParser to make it possible to programmatically specify the\n    default Boolean operator (OR or AND).\n    (Péter Halácsy via otis)\n\n 8. Made many search methods and classes non-final, per requests.\n    this includes IndexWriter and IndexSearcher, among others.\n    (cutting)\n\n 9. Added class RemoteSearchable, providing support for remote\n    searching via RMI.  The test class RemoteSearchableTest.java\n    provides an example of how this can be used.  (cutting)\n\n 10. Added PhrasePrefixQuery (and supporting MultipleTermPositions).  The\n     test class TestPhrasePrefixQuery provides the usage example.\n     (Anders Nielsen via otis)\n\n 11. Changed the German stemming algorithm to ignore case while\n     stripping. The new algorithm is faster and produces more equal\n     stems from nouns and verbs derived from the same word.\n     (gschwarz)\n\n 12. Added support for boosting the score of documents and fields via\n     the new methods Document.setBoost(float) and Field.setBoost(float).\n\n     Note: this changes the encoding of an indexed value.  Indexes\n     should be re-created from scratch in order for search scores to\n     be correct.  With the new code and an old index, searches will\n     yield very large scores for shorter fields, and very small scores\n     for longer fields.  Once the index is re-created, scores will be\n     as before. (cutting)\n\n 13. Added new method Token.setPositionIncrement().\n\n     this permits, for the purpose of phrase searching, placing\n     multiple terms in a single position.  this is useful with\n     stemmers that produce multiple possible stems for a word.\n\n     this also permits the introduction of gaps between terms, so that\n     terms which are adjacent in a token stream will not be matched by\n     and exact phrase query.  this makes it possible, e.g., to build\n     an analyzer where phrases are not matched over stop words which\n     have been removed.\n\n     Finally, repeating a token with an increment of zero can also be\n     used to boost scores of matches on that token.  (cutting)\n\n 14. Added new Filter class, QueryFilter.  this constrains search\n     results to only match those which also match a provided query.\n     Results are cached, so that searches after the first on the same\n     index using this filter are very fast.\n\n     this could be used, for example, with a RangeQuery on a formatted\n     date field to implement date filtering.  One could re-use a\n     single QueryFilter that matches, e.g., only documents modified\n     within the last week.  The QueryFilter and RangeQuery would only\n     need to be reconstructed once per day. (cutting)\n\n 15. Added a new IndexWriter method, getAnalyzer().  this returns the\n     analyzer used when adding documents to this index. (cutting)\n\n 16. Fixed a bug with IndexReader.lastModified().  Before, document\n     deletion did not update this.  Now it does.  (cutting)\n\n 17. Added Russian Analyzer.\n     (Boris Okner via otis)\n\n 18. Added a public, extensible scoring API.  For details, see the\n     javadoc for org.apache.lucene.search.Similarity.\n\n 19. Fixed return of Hits.id() from float to int. (Terry Steichen via Peter).\n\n 20. Added getFieldNames() to IndexReader and Segment(s)Reader classes.\n     (Peter Mularien via otis)\n\n 21. Added getFields(String) and getValues(String) methods.\n     Contributed by Rasik Pandey on 2002-10-09\n     (Rasik Pandey via otis)\n\n 22. Revised internal search APIs.  Changes include:\n\n       a. Queries are no longer modified during a search.  this makes\n       it possible, e.g., to reuse the same query instance with\n       multiple indexes from multiple threads.\n\n       b. Term-expanding queries (e.g. PrefixQuery, WildcardQuery,\n       etc.)  now work correctly with MultiSearcher, fixing bugs 12619\n       and 12667.\n\n       c. Boosting BooleanQuery's now works, and is supported by the\n       query parser (problem reported by Lee Mallabone).  Thus a query\n       like \"(+foo +bar)^2 +baz\" is now supported and equivalent to\n       \"(+foo^2 +bar^2) +baz\".\n\n       d. New method: Query.rewrite(IndexReader).  this permits a\n       query to re-write itself as an alternate, more primitive query.\n       Most of the term-expanding query classes (PrefixQuery,\n       WildcardQuery, etc.) are now implemented using this method.\n\n       e. New method: Searchable.explain(Query q, int doc).  this\n       returns an Explanation instance that describes how a particular\n       document is scored against a query.  An explanation can be\n       displayed as either plain text, with the toString() method, or\n       as HTML, with the toHtml() method.  Note that computing an\n       explanation is as expensive as executing the query over the\n       entire index.  this is intended to be used in developing\n       Similarity implementations, and, for good performance, should\n       not be displayed with every hit.\n\n       f. Scorer and Weight are public, not package protected.  It now\n       possible for someone to write a Scorer implementation that is\n       not in the org.apache.lucene.search package.  this is still\n       fairly advanced programming, and I don't expect anyone to do\n       this anytime soon, but at least now it is possible.\n\n       g. Added public accessors to the primitive query classes\n       (TermQuery, PhraseQuery and BooleanQuery), permitting access to\n       their terms and clauses.\n\n     Caution: These are extensive changes and they have not yet been\n     tested extensively.  Bug reports are appreciated.\n     (cutting)\n\n 23. Added convenience RAMDirectory constructors taking File and String\n     arguments, for easy FSDirectory to RAMDirectory conversion.\n     (otis)\n\n 24. Added code for manual renaming of files in FSDirectory, since it\n     has been reported that java.io.File's renameTo(File) method sometimes\n     fails on Windows JVMs.\n     (Matt Tucker via otis)\n\n 25. Refactored QueryParser to make it easier for people to extend it.\n     Added the ability to automatically lower-case Wildcard terms in\n     the QueryParser.\n     (Tatu Saloranta via otis)\n\n\n1.2 RC6\n\n 1. Changed QueryParser.jj to have \"?\" be a special character which\n    allowed it to be used as a wildcard term. Updated TestWildcard\n    unit test also. (Ralf Hettesheimer via carlson)\n\n1.2 RC5\n\n 1. Renamed build.properties to default.properties and updated\n    the BUILD.txt document to describe how to override the\n    default.property settings without having to edit the file. this\n    brings the build process closer to Scarab's build process.\n    (jon)\n\n 2. Added MultiFieldQueryParser class. (Kelvin Tan, via otis)\n\n 3. Updated \"powered by\" links. (otis)\n\n 4. Fixed instruction for setting up JavaCC - Bug #7017 (otis)\n\n 5. Added throwing exception if FSDirectory could not create directory\n    - Bug #6914 (Eugene Gluzberg via otis)\n\n 6. Update MultiSearcher, MultiFieldParse, Constants, DateFilter,\n    LowerCaseTokenizer javadoc (otis)\n\n 7. Added fix to avoid NullPointerException in results.jsp\n    (Mark Hayes via otis)\n\n 8. Changed Wildcard search to find 0 or more char instead of 1 or more\n    (Lee Mallobone, via otis)\n\n 9. Fixed error in offset issue in GermanStemFilter - Bug #7412\n    (Rodrigo Reyes, via otis)\n\n 10. Added unit tests for wildcard search and DateFilter (otis)\n\n 11. Allow co-existence of indexed and non-indexed fields with the same name\n     (cutting/casper, via otis)\n\n 12. Add escape character to query parser.\n     (briangoetz)\n\n 13. Applied a patch that ensures that searches that use DateFilter\n     don't throw an exception when no matches are found. (David Smiley, via\n     otis)\n\n 14. Fixed bugs in DateFilter and wildcardquery unit tests. (cutting, otis, carlson)\n\n\n1.2 RC4\n\n 1. Updated contributions section of website.\n    Add XML Document #3 implementation to Document Section.\n    Also added Term Highlighting to Misc Section. (carlson)\n\n 2. Fixed NullPointerException for phrase searches containing\n    unindexed terms, introduced in 1.2RC3.  (cutting)\n\n 3. Changed document deletion code to obtain the index write lock,\n    enforcing the fact that document addition and deletion cannot be\n    performed concurrently.  (cutting)\n\n 4. Various documentation cleanups.  (otis, acoliver)\n\n 5. Updated \"powered by\" links.  (cutting, jon)\n\n 6. Fixed a bug in the GermanStemmer.  (Bernhard Messer, via otis)\n\n 7. Changed Term and Query to implement Serializable.  (scottganyo)\n\n 8. Fixed to never delete indexes added with IndexWriter.addIndexes().\n    (cutting)\n\n 9. Upgraded to JUnit 3.7. (otis)\n\n1.2 RC3\n\n 1. IndexWriter: fixed a bug where adding an optimized index to an\n    empty index failed.  this was encountered using addIndexes to copy\n    a RAMDirectory index to an FSDirectory.\n\n 2. RAMDirectory: fixed a bug where RAMInputStream could not read\n    across more than across a single buffer boundary.\n\n 3. Fix query parser so it accepts queries with unicode characters.\n    (briangoetz)\n\n 4. Fix query parser so that PrefixQuery is used in preference to\n    WildcardQuery when there's only an asterisk at the end of the\n    term.  Previously PrefixQuery would never be used.\n\n 5. Fix tests so they compile; fix ant file so it compiles tests\n    properly.  Added test cases for Analyzers and PriorityQueue.\n\n 6. Updated demos, added Getting Started documentation. (acoliver)\n\n 7. Added 'contributions' section to website & docs. (carlson)\n\n 8. Removed JavaCC from source distribution for copyright reasons.\n    Folks must now download this separately from metamata in order to\n    compile Lucene.  (cutting)\n\n 9. Substantially improved the performance of DateFilter by adding the\n    ability to reuse TermDocs objects.  (cutting)\n\n10. Added IndexReader methods:\n      public static boolean indexExists(String directory);\n      public static boolean indexExists(File directory);\n      public static boolean indexExists(Directory directory);\n      public static boolean isLocked(Directory directory);\n      public static void unlock(Directory directory);\n    (cutting, otis)\n\n11. Fixed bugs in GermanAnalyzer (gschwarz)\n\n\n1.2 RC2, 19 October 2001:\n - added sources to distribution\n - removed broken build scripts and libraries from distribution\n - SegmentsReader: fixed potential race condition\n - FSDirectory: fixed so that getDirectory(xxx,true) correctly\n   erases the directory contents, even when the directory\n   has already been accessed in this JVM.\n - RangeQuery: Fix issue where an inclusive range query would\n   include the nearest term in the index above a non-existant\n   specified upper term.\n - SegmentTermEnum: Fix NullPointerException in clone() method\n   when the Term is null.\n - JDK 1.1 compatibility fix: disabled lock files for JDK 1.1,\n   since they rely on a feature added in JDK 1.2.\n\n1.2 RC1 (first Apache release), 2 October 2001:\n  - packages renamed from com.lucene to org.apache.lucene\n  - license switched from LGPL to Apache\n  - ant-only build -- no more makefiles\n  - addition of lock files--now fully thread & process safe\n  - addition of German stemmer\n  - MultiSearcher now supports low-level search API\n  - added RangeQuery, for term-range searching\n  - Analyzers can choose tokenizer based on field name\n  - misc bug fixes.\n\n1.01b (last Sourceforge release), 2 July 2001\n . a few bug fixes\n . new Query Parser\n . new prefix query (search for \"foo*\" matches \"food\")\n\n1.0, 2000-10-04\n\nthis release fixes a few serious bugs and also includes some\nperformance optimizations, a stemmer, and a few other minor\nenhancements.\n\n0.04 2000-04-19\n\nLucene now includes a grammar-based tokenizer, StandardTokenizer.\n\nThe only tokenizer included in the previous release (LetterTokenizer)\nidentified terms consisting entirely of alphabetic characters.  The\nnew tokenizer uses a regular-expression grammar to identify more\ncomplex classes of terms, including numbers, acronyms, email\naddresses, etc.\n\nStandardTokenizer serves two purposes:\n\n 1. It is a much better, general purpose tokenizer for use by\n    applications as is.\n\n    The easiest way for applications to start using\n    StandardTokenizer is to use StandardAnalyzer.\n\n 2. It provides a good example of grammar-based tokenization.\n\n    If an application has special tokenization requirements, it can\n    implement a custom tokenizer by copying the directory containing\n    the new tokenizer into the application and modifying it\n    accordingly.\n\n0.01, 2000-03-30\n\nFirst open source release.\n\nThe code has been re-organized into a new package and directory\nstructure for this release.  It builds OK, but has not been tested\nbeyond that since the re-organization.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 5.1787109375,
          "content": "﻿# Lucene.NET Contributor's Guide\n\nHave you found a bug or do you have an idea for a cool new enhancement? Contributing code is a great way to give something back to the open-source community. Before you dig right into the code there are a few guidelines that we need contributors to follow so that we can have a chance of keeping on top of things.\n\n## Getting Started\n\n- Read [Open Source Contribution Etiquette](http://tirania.org/blog/archive/2010/Dec-31.html) and [Don't \"Push\" Your Pull Requests](http://www.igvita.com/2011/12/19/dont-push-your-pull-requests/).\n- Make sure you have a [GitHub account](https://github.com/signup/free). NOTE: Although this is a mirror of our Git repository, pull requests are accepted through GitHub.\n- If you are thinking of adding a feature, we would appreciate you opening a discussion on our [developer mailing list](https://cwiki.apache.org/confluence/display/LUCENENET/Mailing+Lists) before you start writing. It could save both you and our team quite a bit of work if the code doesn't have to be rewritten to fit in with our overall objectives.\n- Submit a [new issue on GitHub](https://github.com/apache/lucenenet/issues), assuming one doesn't exist already.\n  - If reporting a bug, clearly describe the issue including steps to reproduce, observed behavior, and expected behavior.\n  - If reporting a bug, provide source code that we can run without any alteration demonstrating the issue. Issues submitted with runnable code will be given a higher priority than those submitted without.\n- If you will be submitting a [pull request](https://github.com/apache/lucenenet/pulls), fork the repository on GitHub.\n  - Create a new branch with a descriptive name (tracking master) and [submit a Pull Request](https://help.github.com/articles/creating-a-pull-request/).\n\n> **NOTE:** In the past, the Lucene.NET project used the [JIRA issue tracker](https://issues.apache.org/jira/projects/LUCENENET/issues), which has now been deprecated. However, we are keeping it active for tracking legacy issues. Please submit any new issues to GitHub.\n  \n## Up For Grabs\n\nThere are several [**Open Issues on GitHub**](https://github.com/apache/lucenenet/labels/up-for-grabs) that are marked `up-for-grabs` that we could use help with.\n\n## Other Ways To Help\n\n* Be a power beta tester. Make it your mission to track down bugs and report them to us on [GitHub](https://github.com/apache/lucenenet/issues).\n* Optimizing code. During porting we have ended up with some code that is less than optimal. We could use a hand getting everything up to speed (pun intended).\n* Helping update the API, or at least just providing feedback on which API changes are affecting the usability. There are several things on our radar, like integrating something like [Lucene.Net.Linq](https://github.com/themotleyfool/Lucene.Net.Linq) directly into our project, [converting the remaining public-facing iterator classes into `IEnumerator<T>`](https://issues.apache.org/jira/projects/LUCENENET/issues/LUCENENET-469?filter=allopenissues) so they can be used with foreach loops, adding extension methods to remove the need for casting, etc.\n* Making demos and tutorials, blogging about Lucene.Net, etc. (and providing feedback on how we can make the API better!). If you write a helpful Lucene.Net post on your blog, be sure to let us know so we can link to it.\n* Helping out with documentation. We are still trying to make the API docs easily navigable (see #206), and there are many files that are not formatted correctly (links not appearing, tables not very readable, etc). Also, we need help getting all of the Java-related documentation converted to use .NET methodologies.\n* Fixing TODOs. There are several TODOs throughout the code that need to be reviewed and action taken, if necessary. Search for `LUCENENET TODO` using the regular expression option in Visual Studio to find them. Do note there are a lot of TODOs left over from Java Lucene that are safe to ignore.\n* Reviewing code. Pick a random section, review line by line, comparing the code against the [original Lucene 4.8.0 code](https://github.com/apache/lucene-solr/tree/releases/lucene-solr/4.8.0/lucene). Many of the bugs have been found this way, as the tests are not showing them. Let us know if you find anything suspicious on the [dev mailing list](https://cwiki.apache.org/confluence/display/LUCENENET/Mailing+Lists) or [submit a pull request](https://help.github.com/en/github/collaborating-with-issues-and-pull-requests/creating-a-pull-request) with a fix.\n* Creating projects to make Lucene.Net easier to use with various .NET frameworks (ASP.NET MVC, WebApi, AspNetCore, WPF, EntityFramework, etc). In general, we would like common tasks as easy as possible to integrate into applications build on these frameworks without everyone having to write the same boilerplate code.\n* Building automation tools to eliminate some of the manual work of managing the project, updating information on various web pages, creating tools to make porting/upgrading more automated, etc.\n\nOr, if none of that interests you, join our [dev mailing list](https://cwiki.apache.org/confluence/display/LUCENENET/Mailing+Lists) and ask!\n\n## Thank You For Your Help!\n\nAgain, thank you very much for your contribution. May the fork be with you!\n"
        },
        {
          "name": "Directory.Build.props",
          "type": "blob",
          "size": 4.994140625,
          "content": "﻿<!--\n\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n\n-->\n<Project>\n  <PropertyGroup>\n    <!-- According to the docs (https://docs.microsoft.com/en-us/cpp/build/reference/common-macros-for-build-commands-and-properties?view=vs-2019), the\n      SolutionDir is only available when running in the IDE, so we patch to ensure it also works when using dotnet.exe -->\n    <SolutionDir Condition=\" '$(SolutionDir)' == '' \">$(MSBuildThisFileDirectory)</SolutionDir>\n  </PropertyGroup>\n\n  <PropertyGroup>\n    <LangVersion>11.0</LangVersion>\n    <GitHubOrganization>apache</GitHubOrganization>\n    <GitHubProject>lucenenet</GitHubProject>\n  </PropertyGroup>\n\n  <!-- IMPORTANT: When these values are changed, the CI counter number should also be reset. -->\n  <PropertyGroup Label=\"Version of Builds\">\n    <!-- IMPORTANT: VersionPrefix must always be the same as the Lucene version this is based on.\n      Never increment it for an individual build - only increment this when an entire version's changes\n      are ported over from Lucene. This is what determines the version of all of the NuGet packages and binaries.\n      For patching a production build, we will add a 4th segment (4.8.0.1) since it would be confusing to increment to\n      4.8.1 if we haven't actually ported over the changes from Lucene 4.8.1. -->\n    <VersionPrefix>4.8.0</VersionPrefix>\n\n    <!-- .NET enforces AssemblyVersion as the \"major\" version (when strong-named),\n      so AssemblyVersion should only be changed for each major version release. -->\n    <AssemblyVersion>4.0.0</AssemblyVersion>\n\n    <!-- .NET 8 SDK has a breaking change where they automatically add the commit hash to InformationalVersion unless it is explicitly disabled. -->\n    <IncludeSourceRevisionInInformationalVersion>false</IncludeSourceRevisionInInformationalVersion>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"Assembly Signing\">\n    <AssemblyOriginatorKeyFile>$(MSBuildThisFileDirectory)Lucene.Net.snk</AssemblyOriginatorKeyFile>\n    <PublicKey>002400000480000094000000060200000024000052534131000400000100010075a07ce602f88ef263c7db8cb342c58ebd49ecdcc210fac874260b0213fb929ac3dcaf4f5b39744b800f99073eca72aebfac5f7284e1d5f2c82012a804a140f06d7d043d83e830cdb606a04da2ad5374cc92c0a49508437802fb4f8fb80a05e59f80afb99f4ccd0dfe44065743543c4b053b669509d29d332cd32a0cb1e97e84</PublicKey>\n    <SignAssembly>true</SignAssembly>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"Assembly Publishing\">\n    <IsPublishable>false</IsPublishable>\n    <AutoGenerateBindingRedirects>true</AutoGenerateBindingRedirects>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"NuGet Package Defaults\">\n    <IsPackable>false</IsPackable>\n    <IncludeSymbols>true</IncludeSymbols>\n    <!-- This is the new symbols format (the only one currently supported at NuGet.org) -->\n    <SymbolPackageFormat>snupkg</SymbolPackageFormat>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"Copyright Info\">\n    <Product>Lucene.Net</Product>\n    <Company>The Apache Software Foundation</Company>\n    <CurrentYear Condition=\" '$(CurrentYear)' == '' \">$([System.DateTime]::UtcNow.Year.ToString())</CurrentYear>\n    <BeginCopyrightYear>2006</BeginCopyrightYear>\n    <CopyrightYearRange>$(BeginCopyrightYear) - $(CurrentYear)</CopyrightYearRange>\n    <CopyrightYearRange Condition=\" '$(BeginCopyrightYear)' == '$(CurrentYear)' \">$(CurrentYear)</CopyrightYearRange>\n    <Copyright>Copyright © $(CopyrightYearRange) $(Company)</Copyright>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"SourceLink Settings: https://github.com/dotnet/sourcelink/blob/main/README.md\">\n    <PublishRepositoryUrl>true</PublishRepositoryUrl>\n    <EmbedUntrackedSources>true</EmbedUntrackedSources>\n  </PropertyGroup>\n\n  <PropertyGroup Condition=\" '$(BUILD_REPOSITORY_PROVIDER)' == 'GitHub' Or '$(BUILD_REPOSITORY_PROVIDER)' == 'TfsGit' \" Label=\"Deterministic builds: https://github.com/clairernovotny/DeterministicBuilds#readme\">\n    <ContinuousIntegrationBuild>true</ContinuousIntegrationBuild>\n  </PropertyGroup>\n\n  <!-- Settings to override the above Version of Builds. These can be used to\n      \"freeze\" the build number for a release, so whether building within\n      an IDE or from the commmand line, the version is always what is\n      in Version.props, if it exists and the PrepareForBuild argument\n      passed into build.ps1 is 'false'. -->\n  <Import Project=\"version.props\" Condition=\"Exists('version.props')\" />\n</Project>\n"
        },
        {
          "name": "Directory.Build.targets",
          "type": "blob",
          "size": 13.8388671875,
          "content": "﻿<!--\n\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n\n-->\n<Project>\n\n  <Import Project=\".build/dependencies.props\" Condition=\"Exists('.build/dependencies.props')\" />\n\n  <PropertyGroup Label=\"Warnings to be Disabled in Solution\">\n    <NoWarn Label=\"Legacy serialization support APIs are obsolete\">$(NoWarn);SYSLIB0051</NoWarn>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"Solution-level Publish to Project-specific Directory\">\n    <PublishDir Condition=\"'$(AlternatePublishRootDirectory)' != ''\">$(AlternatePublishRootDirectory)/$(TargetFramework)/$(MSBuildProjectName)/</PublishDir>\n  </PropertyGroup>\n\n  <!-- Features in .NET 9.x only -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_STREAM_READEXACTLY</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET 8.x and .NET 9.x only -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_UTF8_TOUTF16</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET 6.x, .NET 7.x, .NET 8.x, and .NET 9.x only -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('net6.')) Or $(TargetFramework.StartsWith('net7.')) Or $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_RANDOM_NEXTINT64_NEXTSINGLE</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_SPANFORMATTABLE</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_SUPPORTEDOSPLATFORMATTRIBUTE</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET 5.x, .NET 6.x, .NET 7.x, .NET 8.x, and .NET 9.x only -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('net5.')) Or $(TargetFramework.StartsWith('net6.')) Or $(TargetFramework.StartsWith('net7.')) Or $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_ASPNETCORE_ENDPOINT_CONFIG</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_READONLYSET</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET Core 3.x, .NET 5.x, .NET 6.x, .NET 7.x, .NET 8.x, and .NET 9.x only -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('netcoreapp3.')) Or $(TargetFramework.StartsWith('net5.')) Or $(TargetFramework.StartsWith('net6.')) Or $(TargetFramework.StartsWith('net7.')) Or $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_ARGITERATOR</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_PROCESS_KILL_ENTIREPROCESSTREE</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_STRING_CONCAT_READONLYSPAN</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET Standard, .NET Core, .NET 5.x, .NET 6.x, .NET 7.x, .NET 8.x, and .NET 9.x only (no .NET Framework support) -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('netstandard')) Or $(TargetFramework.StartsWith('netcoreapp')) Or $(TargetFramework.StartsWith('net5.')) Or $(TargetFramework.StartsWith('net6.')) Or $(TargetFramework.StartsWith('net7.')) Or $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);NETSTANDARD</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_CULTUREINFO_CURRENTCULTURE_SETTER</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_ENCODINGPROVIDERS</DefineConstants>\n\n    <DebugType>portable</DebugType>\n  </PropertyGroup>\n\n  <!-- Features in .NET Standard 2.1, .NET 5.x, .NET 6.x, .NET 7.x, .NET 8.x, and .NET 9.x only -->\n  <PropertyGroup Condition=\" '$(TargetFramework)' == 'netstandard2.1' Or $(TargetFramework.StartsWith('netcoreapp3.')) Or $(TargetFramework.StartsWith('net5.')) Or $(TargetFramework.StartsWith('net6.')) Or $(TargetFramework.StartsWith('net7.')) Or $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_ARRAY_FILL</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_CONDITIONALWEAKTABLE_ENUMERATOR</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_CONDITIONALWEAKTABLE_ADDORUPDATE</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_NUMBER_PARSE_READONLYSPAN</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_STREAM_READ_SPAN</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_STRINGBUILDER_APPEND_READONLYSPAN</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_QUEUE_TRYDEQUEUE_TRYPEEK</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET Standard 2.x, .NET Core 2.x, .NET Core 3.x, .NET 5.x, .NET 6.x, .NET 7.x, .NET 8.x, and .NET 9.x -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('netstandard2.')) Or $(TargetFramework.StartsWith('netcoreapp2.')) Or $(TargetFramework.StartsWith('netcoreapp3.')) Or $(TargetFramework.StartsWith('net5.')) Or $(TargetFramework.StartsWith('net6.')) Or $(TargetFramework.StartsWith('net7.')) Or $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_ICONFIGURATIONROOT_PROVIDERS</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET Framework 4.5+, .NET Standard 2.x, .NET Core 2.x, .NET Core 3.x, .NET 5.x, .NET 6.x, .NET 7.x, .NET 8.x, and .NET 9.x  -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('net4')) Or $(TargetFramework.StartsWith('netstandard2.')) Or $(TargetFramework.StartsWith('netcoreapp2.')) Or $(TargetFramework.StartsWith('netcoreapp3.')) Or $(TargetFramework.StartsWith('net5.')) Or $(TargetFramework.StartsWith('net6.')) Or $(TargetFramework.StartsWith('net7.')) Or $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_ASSEMBLY_GETCALLINGASSEMBLY</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_FILESTREAM_LOCK</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_TEXTWRITER_CLOSE</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_THREADPOOL_UNSAFEQUEUEWORKITEM</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_TYPE_GETMETHOD__BINDINGFLAGS_PARAMS</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET Framework 4.5+, .NET Standard 2.x, .NET Core 2.x, .NET Core 3.x, .NET 5.x, .NET 6.x, .NET 7.x, and .NET 8.x (No .NET 9.x support)  -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('net4')) Or $(TargetFramework.StartsWith('netstandard2.')) Or $(TargetFramework.StartsWith('netcoreapp2.')) Or $(TargetFramework.StartsWith('netcoreapp3.')) Or $(TargetFramework.StartsWith('net5.')) Or $(TargetFramework.StartsWith('net6.')) Or $(TargetFramework.StartsWith('net7.')) Or $(TargetFramework.StartsWith('net8.')) \">\n    <DefineConstants>$(DefineConstants);FEATURE_SERIALIZABLE_EXCEPTIONS</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_SERIALIZABLE</DefineConstants>\n  </PropertyGroup>\n\n  <!-- Features in .NET Framework 4.5+ and .NET Standard 2.x only (No .NET Core support) -->\n  <PropertyGroup Condition=\" $(TargetFramework.StartsWith('net4')) Or $(TargetFramework.StartsWith('netstandard2.')) \">\n\n    <!-- NOTE: The API for this exists in .NET Core, but it throws a PlatformNotSupportedException.\n         We simply don't override this to get the same behavior. -->\n    <DefineConstants>$(DefineConstants);FEATURE_TEXTWRITER_INITIALIZELIFETIMESERVICE</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET Framework 4.6.1+ only -->\n  <PropertyGroup Condition=\"'$(TargetFramework)' == 'net461' Or '$(TargetFramework)' == 'net462' Or $(TargetFramework.StartsWith('net47')) Or $(TargetFramework.StartsWith('net48'))\">\n\n    <DefineConstants>$(DefineConstants);FEATURE_ICONFIGURATIONROOT_PROVIDERS</DefineConstants>\n\n  </PropertyGroup>\n\n  <!-- Features in .NET Framework 4.5+ -->\n  <PropertyGroup Condition=\"$(TargetFramework.StartsWith('net4'))\">\n\n    <DefineConstants>$(DefineConstants);NETFRAMEWORK</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_ARGITERATOR</DefineConstants>\n    <!-- Although code access security is available in .NET Standard 2.0+ via platform extensions, we are excluding\n    it due to the fact it is not a primary feature of Lucene.NET and it is supported in .NET Framework -->\n    <DefineConstants>$(DefineConstants);FEATURE_CODE_ACCESS_SECURITY</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_MEMORYMAPPEDFILESECURITY</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_STACKOVERFLOWEXCEPTION__ISCATCHABLE</DefineConstants>\n    <DefineConstants>$(DefineConstants);FEATURE_TEXTWRITER_CREATEOBJREF</DefineConstants>\n\n    <DebugType>full</DebugType>\n  </PropertyGroup>\n\n  <!-- Features in .NET Framework 4.5+ and .NET 8.0+ but not in .NET Standard 2.0 or .NET Standard 2.1 -->\n  <!-- net472 is used to test .NET Standard 2.0, and .NET 6.0 for .NET Standard 2.1, so we treat them like it is not part of this group -->\n  <PropertyGroup Condition=\" ($(TargetFramework.StartsWith('net4')) And '$(TargetFramework)' != 'net472') Or $(TargetFramework.StartsWith('net8.')) Or $(TargetFramework.StartsWith('net9.')) \">\n\n    <DefineConstants>$(DefineConstants);FEATURE_OPENNLP</DefineConstants>\n\n  </PropertyGroup>\n\n\n  <PropertyGroup>\n    <!-- NuGet.org only supports portable debug symbol format:\n         https://docs.microsoft.com/en-us/nuget/create-packages/symbol-packages-snupkg#nugetorg-symbol-package-constraints -->\n    <DebugType Condition=\" '$(PortableDebugTypeOnly)' == 'true' \">portable</DebugType>\n  </PropertyGroup>\n\n  <Target Name=\"AddInternalsVisibleTo\" BeforeTargets=\"BeforeCompile\" Label=\"Adds InternalsVisibleTo Attribute and PublicKey (if supplied)\">\n    <ItemGroup Condition=\"'@(InternalsVisibleTo->Count())' &gt; 0 \">\n      <AssemblyAttribute Include=\"System.Runtime.CompilerServices.InternalsVisibleTo\">\n        <_Parameter1>%(InternalsVisibleTo.Identity)</_Parameter1>\n        <_Parameter1 Condition=\" '$(SignAssembly)' == 'true' And '$(PublicKey)' != '' \">%(InternalsVisibleTo.Identity), PublicKey=$(PublicKey)</_Parameter1>\n      </AssemblyAttribute>\n    </ItemGroup>\n  </Target>\n\n  <Target Name=\"PrintTargetFrameworks\" Label=\"Prints the value for the $(TargetFrameworks) property or 'none' if no frameworks apply. Pass TestProjectsOnly=true to get results only if this is a test project.\">\n    <PropertyGroup>\n      <DoOutputTargetFrameworks Condition=\" '$(TestProjectsOnly)' != 'true' Or ('$(TestProjectsOnly)' == 'true' And '$(IsTestProject)' == 'true')\">true</DoOutputTargetFrameworks>\n      <OutputTargetFrameworks Condition=\" '$(DoOutputTargetFrameworks)' == 'true' \">$(TargetFramework)</OutputTargetFrameworks>\n      <!-- Fallback to TargetFrameworks field if TargetFramework is empty -->\n      <OutputTargetFrameworks Condition=\" '$(DoOutputTargetFrameworks)' == 'true' And '$(OutputTargetFrameworks)' == '' \">$(TargetFrameworks)</OutputTargetFrameworks>\n      <OutputTargetFrameworks Condition=\" '$(OutputTargetFrameworks)' == '' \">none</OutputTargetFrameworks>\n    </PropertyGroup>\n    <Message Text=\"$(OutputTargetFrameworks)\" Importance=\"high\"/>\n  </Target>\n\n\n  <!-- Disable Json Source Generator from being added to projects. This is being done to work around a bug in VS 2022\n    which appeared around VS2022 7.3.6 which causes StackOverflowExceptions when compiling the solution.\n\tsee: https://developercommunity.visualstudio.com/t/VS-2022-1736-Process-is-terminated-due/10173885#T-ND10184855\n\tfor more details.  Once a fix is rolled out for VS2022, this block can be removed -->\n  <Target Name=\"RemoveJsonSourceGenerator\" BeforeTargets=\"CoreCompile\">\n\t<ItemGroup>\n\t\t<AnalyzersByFileName Include=\"@(Analyzer -> '%(FileName)')\">\n\t\t\t<OriginalIdentity>%(Identity)</OriginalIdentity>\n\t\t</AnalyzersByFileName>\n\t\t<AnalyzersToRemoveByFileName Include=\"System.Text.Json.SourceGeneration\" />\n\t</ItemGroup>\n\n\t<ItemGroup>\n\t\t<AnalyzersToRemove Include=\"@(AnalyzersByFileName)\" Condition=\"'@(AnalyzersToRemoveByFileName)' == '@(AnalyzersByFileName)' and '%(Identity)' != ''\" />\n\t\t<Analyzer Remove=\"%(AnalyzersToRemove.OriginalIdentity)\" />\n\t</ItemGroup>\n  </Target>\n\n\n  <!-- Global PackageReferences -->\n  <ItemGroup>\n    <!-- This is to allow the .NET Framework references to be machine-indepenedent so builds can happen without installing prerequisites -->\n    <PackageReference Include=\"Microsoft.NETFramework.ReferenceAssemblies\" Version=\"$(MicrosoftNETFrameworkReferenceAssembliesPackageReferenceVersion)\" PrivateAssets=\"All\" />\n  </ItemGroup>\n\n  <!-- This is for testing only, we use SourceLink from any Azure DevOps git repo -->\n  <ItemGroup Condition=\" '$(BUILD_REPOSITORY_PROVIDER)' == 'TfsGit' \" Label=\"SourceLink Packages (experimental Azure Repos)\">\n    <PackageReference Include=\"Microsoft.SourceLink.AzureRepos.Git\" Version=\"$(MicrosoftSourceLinkAzureReposGitPackageReferenceVersion)\" PrivateAssets=\"All\"/>\n  </ItemGroup>\n\n  <ItemGroup Condition=\" '$(BUILD_REPOSITORY_PROVIDER)' == 'GitHub' \" Label=\"SourceLink Packages (main repo)\">\n    <PackageReference Include=\"Microsoft.SourceLink.GitHub\" Version=\"$(MicrosoftSourceLinkGitHubPackageReferenceVersion)\" PrivateAssets=\"All\"/>\n  </ItemGroup>\n\n  <Import Project=\".build/release.targets\" Condition=\"Exists('.build/release.targets')\" />\n\n</Project>\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 39.3564453125,
          "content": "﻿\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. this License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2024 Apache Lucene.NET\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n========\n\nSome code in src/Lucene.Net/Util/UnicodeUtil.cs was derived from unicode \nconversion examples available at http://www.unicode.org/Public/PROGRAMS/CVTUTF.\n\nHere is the copyright from those sources:\n\n/*\n * Copyright 2001-2004 Unicode, Inc.\n * \n * Disclaimer\n * \n * this source code is provided as is by Unicode, Inc. No claims are\n * made as to fitness for any particular purpose. No warranties of any\n * kind are expressed or implied. The recipient agrees to determine\n * applicability of information provided. If this file has been\n * purchased on magnetic or optical media from Unicode, Inc., the\n * sole remedy for any claim will be exchange of defective media\n * within 90 days of receipt.\n * \n * Limitations on Rights to Redistribute this Code\n * \n * Unicode, Inc. hereby grants the right to freely use the information\n * supplied in this file in the creation of products supporting the\n * Unicode Standard, and to make copies of this file in any form\n * for internal or external distribution as long as this notice\n * remains attached.\n */\n\n========\n\nSome code in src/Lucene.Net/Util/ArrayUtil.cs was derived from Python 2.4.2 \nsources available at http://www.python.org. \n\nFull license is here:\n\n  http://www.python.org/download/releases/2.4.2/license/\n\n========\n\nSome code in src/Lucene.Net/Util/UnicodeUtil.cs was\nderived from Python 3.1.2 sources available at\nhttp://www.python.org. Full license is here:\n\n  http://www.python.org/download/releases/3.1.2/license/\n\n========\n\nSome code in src/Lucene.Net/Util/Automaton was\nderived from Brics automaton sources available at\nwww.brics.dk/automaton/. Here is the copyright from those sources:\n\n/*\n * Copyright (c) 2001-2009 Anders Moeller\n * All rights reserved.\n * \n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions\n * are met:\n * 1. Redistributions of source code must retain the above copyright\n *    notice, this list of conditions and the following disclaimer.\n * 2. Redistributions in binary form must reproduce the above copyright\n *    notice, this list of conditions and the following disclaimer in the\n *    documentation and/or other materials provided with the distribution.\n * 3. The name of the author may not be used to endorse or promote products\n *    derived from this software without specific prior written permission.\n * \n * THIS SOFTWARE IS PROVIDED BY THE AUTHOR ``AS IS'' AND ANY EXPRESS OR\n * IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES\n * OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n * IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT,\n * INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT\n * NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF\n * THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n */\n\n========\n\nThe levenshtein automata tables in src/Lucene.Net/Util/Automaton \nwere automatically generated with the moman/finenight FSA package.\nHere is the copyright for those sources:\n\n# Copyright (c) 2010, Jean-Philippe Barrette-LaPierre, <jpb@rrette.com>\n#\n# Permission is hereby granted, free of charge, to any person\n# obtaining a copy of this software and associated documentation\n# files (the \"Software\"), to deal in the Software without\n# restriction, including without limitation the rights to use,\n# copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the\n# Software is furnished to do so, subject to the following\n# conditions:\n#\n# The above copyright notice and this permission notice shall be\n# included in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND,\n# EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES\n# OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT\n# HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY,\n# WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR\n# OTHER DEALINGS IN THE SOFTWARE.\n\n========\n\nSome code in src/Lucene.Net/Util/UnicodeUtil.cs was\nderived from ICU (http://www.icu-project.org)\nThe full license is available here: \n  http://source.icu-project.org/repos/icu/icu/trunk/license.html\n\n/*\n * Copyright (C) 1999-2010, International Business Machines\n * Corporation and others.  All Rights Reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a copy \n * of this software and associated documentation files (the \"Software\"), to deal\n * in the Software without restriction, including without limitation the rights \n * to use, copy, modify, merge, publish, distribute, and/or sell copies of the \n * Software, and to permit persons to whom the Software is furnished to do so, \n * provided that the above copyright notice(s) and this permission notice appear \n * in all copies of the Software and that both the above copyright notice(s) and\n * this permission notice appear in supporting documentation.\n * \n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR \n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, \n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT OF THIRD PARTY RIGHTS. \n * IN NO EVENT SHALL THE COPYRIGHT HOLDER OR HOLDERS INCLUDED IN THIS NOTICE BE \n * LIABLE FOR ANY CLAIM, OR ANY SPECIAL INDIRECT OR CONSEQUENTIAL DAMAGES, OR \n * ANY DAMAGES WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER \n * IN AN ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT \n * OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n *\n * Except as contained in this notice, the name of a copyright holder shall not \n * be used in advertising or otherwise to promote the sale, use or other \n * dealings in this Software without prior written authorization of the \n * copyright holder.\n */\n\n========\n\nThe following license applies to the Snowball stemmers:\n\nCopyright (c) 2001, Dr Martin Porter\nCopyright (c) 2002, Richard Boulton\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n    * Redistributions of source code must retain the above copyright notice,\n    * this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n    * notice, this list of conditions and the following disclaimer in the\n    * documentation and/or other materials provided with the distribution.\n    * Neither the name of the copyright holders nor the names of its contributors\n    * may be used to endorse or promote products derived from this software\n    * without specific prior written permission.\n\nthis SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF this SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n========\n\nThe following license applies to src/Lucene.Net.Analysis.Common/Analysis/En/KStemmer.cs:\n\nCopyright © 2003,\nCenter for Intelligent Information Retrieval,\nUniversity of Massachusetts, Amherst.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification,\nare permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. The names \"Center for Intelligent Information Retrieval\" and\n\"University of Massachusetts\" must not be used to endorse or promote products\nderived from this software without prior written permission. To obtain\npermission, contact info@ciir.cs.umass.edu.\n\nTHIS SOFTWARE IS PROVIDED BY UNIVERSITY OF MASSACHUSETTS AND OTHER CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\nARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDERS OR CONTRIBUTORS BE\nLIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE\nGOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION)\nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT\nLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY\nOUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGE.\n\n========\n\nThe following license applies to the Morfologik project:\n\nCopyright (c) 2006 Dawid Weiss\nCopyright (c) 2007-2011 Dawid Weiss, Marcin Miłkowski\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n    * Redistributions of source code must retain the above copyright notice, \n    this list of conditions and the following disclaimer.\n    \n    * Redistributions in binary form must reproduce the above copyright notice, \n    this list of conditions and the following disclaimer in the documentation \n    and/or other materials provided with the distribution.\n    \n    * Neither the name of Morfologik nor the names of its contributors \n    may be used to endorse or promote products derived from this software \n    without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND \nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED \nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE \nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES \n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; \nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON \nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT \n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS \nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n---\n\nThe dictionary comes from Morfologik project. Morfologik uses data from \nPolish ispell/myspell dictionary hosted at http://www.sjp.pl/slownik/en/ and \nis licenced on the terms of (inter alia) LGPL and Creative Commons \nShareAlike. The part-of-speech tags were added in Morfologik project and\nare not found in the data from sjp.pl. The tagset is similar to IPI PAN\ntagset.\n\n---\n\nThe following license applies to the Morfeusz project,\nused by org.apache.lucene.analysis.morfologik.\n\nBSD-licensed dictionary of Polish (SGJP)\nhttp://sgjp.pl/morfeusz/\n\nCopyright © 2011 Zygmunt Saloni, Włodzimierz Gruszczyński, \n\t    \t Marcin Woliński, Robert Wołosz\n\nAll rights reserved.\n\nRedistribution and  use in  source and binary  forms, with  or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n1. Redistributions of source code must retain the above copyright\n   notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright\n   notice, this list of conditions and the following disclaimer in the\n   documentation and/or other materials provided with the\n   distribution.\n\nTHIS SOFTWARE IS PROVIDED BY COPYRIGHT HOLDERS “AS IS” AND ANY EXPRESS\nOR  IMPLIED WARRANTIES,  INCLUDING, BUT  NOT LIMITED  TO,  THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED.  IN NO EVENT  SHALL COPYRIGHT  HOLDERS OR  CONTRIBUTORS BE\nLIABLE FOR  ANY DIRECT,  INDIRECT, INCIDENTAL, SPECIAL,  EXEMPLARY, OR\nCONSEQUENTIAL DAMAGES  (INCLUDING, BUT NOT LIMITED  TO, PROCUREMENT OF\nSUBSTITUTE  GOODS OR  SERVICES;  LOSS  OF USE,  DATA,  OR PROFITS;  OR\nBUSINESS INTERRUPTION) HOWEVER CAUSED  AND ON ANY THEORY OF LIABILITY,\nWHETHER IN  CONTRACT, STRICT LIABILITY, OR  TORT (INCLUDING NEGLIGENCE\nOR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\nIF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n========\n\nSome code in src/Lucene.Net/Support/LimitedConcurrencyLevelTaskScheduler.cs \nwas derived from the MSDN web site and falls under the following license:\n\nMICROSOFT LIMITED PUBLIC LICENSE version 1.1\nThis license governs use of code marked as \"sample\" or \"example\" available on this web site \nwithout a license agreement, as provided under the section above titled \n\"NOTICE SPECIFIC TO SOFTWARE AVAILABLE ON THIS WEB SITE.\" If you use such \ncode (the \"software\"), you accept this license. If you do not accept the \nlicense, do not use the software.\n\n1. Definitions\nThe terms \"reproduce,\" \"reproduction,\" \"derivative works,\" and \"distribution\" have the \nsame meaning here as under U.S. copyright law.\nA \"contribution\" is the original software, or any additions or changes to the software.\nA \"contributor\" is any person that distributes its contribution under this license.\n\"Licensed patents\" are a contributor’s patent claims that read directly on its contribution.\n\n2. Grant of Rights\n(A) Copyright Grant - Subject to the terms of this license, including the license conditions \nand limitations in section 3, each contributor grants you a non-exclusive, worldwide, \nroyalty-free copyright license to reproduce its contribution, prepare derivative works \nof its contribution, and distribute its contribution or any derivative works that you create.\n(B) Patent Grant - Subject to the terms of this license, including the license conditions \nand limitations in section 3, each contributor grants you a non-exclusive, worldwide, \nroyalty-free license under its licensed patents to make, have made, use, sell, \noffer for sale, import, and/or otherwise dispose of its contribution in the \nsoftware or derivative works of the contribution in the software.\n\n3. Conditions and Limitations\n(A) No Trademark License- This license does not grant you rights to use any contributors’ \nname, logo, or trademarks.\n(B) If you bring a patent claim against any contributor over patents that you claim are \ninfringed by the software, your patent license from such contributor to the software \nends automatically.\n(C) If you distribute any portion of the software, you must retain all copyright, patent, \ntrademark, and attribution notices that are present in the software.\n(D) If you distribute any portion of the software in source code form, you may do so only \nunder this license by including a complete copy of this license with your distribution. \nIf you distribute any portion of the software in compiled or object code form, you may \nonly do so under a license that complies with this license.\n(E) The software is licensed \"as-is.\" You bear the risk of using it. The contributors \ngive no express warranties, guarantees or conditions. You may have additional consumer \nrights under your local laws which this license cannot change. To the extent permitted \nunder your local laws, the contributors exclude the implied warranties of merchantability, \nfitness for a particular purpose and non-infringement.\n(F) Platform Limitation - The licenses granted in sections 2(A) and 2(B) extend only \nto the software or derivative works that you create that run directly on a Microsoft \nWindows operating system product, Microsoft run-time technology (such as the .NET \nFramework or Silverlight), or Microsoft application platform (such as Microsoft \nOffice or Microsoft Dynamics).\n\n========\n\nSome code in src/Lucene.Net.Analysis.Stempel/Egothor.Stemmer and \nsrc/Lucene.Net.Tests.Analysis.Stempel/Egothor.Stemmer falls\nunder the following license:\n\n                    Egothor Software License version 1.00\n                    Copyright (C) 1997-2004 Leo Galambos.\n                 Copyright (C) 2002-2004 \"Egothor developers\"\n                      on behalf of the Egothor Project.\n                             All rights reserved.\n\n   This  software  is  copyrighted  by  the \"Egothor developers\". If this\n   license applies to a single file or document, the \"Egothor developers\"\n   are the people or entities mentioned as copyright holders in that file\n   or  document.  If  this  license  applies  to the Egothor project as a\n   whole,  the  copyright holders are the people or entities mentioned in\n   the  file CREDITS. This file can be found in the same location as this\n   license in the distribution.\n\n   Redistribution  and  use  in  source and binary forms, with or without\n   modification, are permitted provided that the following conditions are\n   met:\n    1. Redistributions  of  source  code  must retain the above copyright\n       notice, the list of contributors, this list of conditions, and the\n       following disclaimer.\n    2. Redistributions  in binary form must reproduce the above copyright\n       notice, the list of contributors, this list of conditions, and the\n       disclaimer  that  follows  these  conditions  in the documentation\n       and/or other materials provided with the distribution.\n    3. The name \"Egothor\" must not be used to endorse or promote products\n       derived  from  this software without prior written permission. For\n       written permission, please contact Leo.G@seznam.cz\n    4. Products  derived  from this software may not be called \"Egothor\",\n       nor  may  \"Egothor\"  appear  in  their name, without prior written\n       permission from Leo.G@seznam.cz.\n\n   In addition, we request that you include in the end-user documentation\n   provided  with  the  redistribution  and/or  in the software itself an\n   acknowledgement equivalent to the following:\n   \"This product includes software developed by the Egothor Project.\n    http://egothor.sf.net/\"\n\n   WARRANTIES,  INCLUDING,  BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF\n   MERCHANTABILITY  AND  FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED.\n   IN  NO  EVENT  SHALL THE EGOTHOR PROJECT OR ITS CONTRIBUTORS BE LIABLE\n   FOR   ANY   DIRECT,   INDIRECT,  INCIDENTAL,  SPECIAL,  EXEMPLARY,  OR\n   CONSEQUENTIAL  DAMAGES  (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n   SUBSTITUTE  GOODS  OR  SERVICES;  LOSS  OF  USE,  DATA, OR PROFITS; OR\n   BUSINESS  INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY,\n   WHETHER  IN  CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE\n   OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN\n   IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n   This  software  consists  of  voluntary  contributions  made  by  many\n   individuals  on  behalf  of  the  Egothor  Project  and was originally\n   created by Leo Galambos (Leo.G@seznam.cz).\n\n========\n   \nThe code RectangularArrays classes falls under the following license\n\n Copyright © 2007 - 2013 Tangible Software Solutions Inc.\n this class can be used by anyone provided that the copyright notice remains intact.\n\n========\n \nsrc/Lucene.Net.Suggest/Suggest/Jaspell/JaspellTernarySearchTrie.cs\nfalls under the following license\n\n Copyright (c) 2005 Bruno Martins\n All rights reserved.\n \n Redistribution and use in source and binary forms, with or without \n modification, are permitted provided that the following conditions \n are met:\n 1. Redistributions of source code must retain the above copyright \n    notice, this list of conditions and the following disclaimer.\n 2. Redistributions in binary form must reproduce the above copyright\n    notice, this list of conditions and the following disclaimer in the\n    documentation and/or other materials provided with the distribution.\n 3. Neither the name of the organization nor the names of its contributors\n    may be used to endorse or promote products derived from this software\n    without specific prior written permission.\n \n THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE \n IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE\n ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE\n LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR\n CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF\n SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\n INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN\n CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE)\n ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF\n THE POSSIBILITY OF SUCH DAMAGE.\n\n========\n\nThe code in\nsrc/Lucene.Net.Benchmark/Support/Sax\nfalls under the following license:\n\n// http://www.saxproject.org\n// Written by David Megginson\n// NO WARRANTY!  This class is in the public domain.\n\n========\n\nThe code in .build/psake/ falls under the following license:\n\npsake\nCopyright (c) 2012-13 James Kovacs, Damian Hickey and Contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in\nall copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\nTHE SOFTWARE.\n\n========\n\nsrc/Lucene.Net/Support/Compatibility/NullableAttributes.cs\nsrc/Lucene.Net/Support/DateTimeOffsetUtil.cs\nfalls under the following license\n\nThe MIT License (MIT)\n\nCopyright (c) 2019 .NET Foundation\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n========\n\nSome code in src/Lucene.Net/Support/ConcurrentHashSet.cs falls under the following license:\n\nMIT License\n\nCopyright (c) 2019 Bar Arnon\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n\n========\n\nSome code in\nsrc/Lucene.Net.TestFramework/Support/Util/DefaultNamespaceTypeWrapper.cs\nsrc/Lucene.Net.TestFramework/Support/Util/LuceneTestCase.TestFixtureAttribute.cs\nsrc/Lucene.Net.TestFramework/Support/Util/NUnitTestFixtureBuilder.cs\nfalls under the following license:\n\n// Copyright (c) 2021 Charlie Poole, Rob Prouse\n// \n// Permission is hereby granted, free of charge, to any person obtaining a copy\n// of this software and associated documentation files (the \"Software\"), to deal\n// in the Software without restriction, including without limitation the rights\n// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n// copies of the Software, and to permit persons to whom the Software is\n// furnished to do so, subject to the following conditions:\n// \n// The above copyright notice and this permission notice shall be included in\n// all copies or substantial portions of the Software.\n// \n// THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n// THE SOFTWARE.\n\n========\n\nThe code in src/Lucene.Net.Expressions/JS/Javascript.g4 falls under the following license:\n\nCopyrights 2008-2009 Xebic Reasearch BV. All rights reserved..\nOriginal work by Patrick Hulsmeijer.\n\nThis ANTLR 3 LL(*) grammar is based on Ecma-262 3rd edition (JavaScript 1.5, JScript 5.5).\nThe annotations refer to the \"A Grammar Summary\" section (e.g. A.1 Lexical Grammar)\nand the numbers in parenthesis to the paragraph numbers (e.g. (7.8) ).\nThis document is best viewed with ANTLRWorks (www.antlr.org).\n\nSoftware License Agreement (BSD License)\n\nCopyright (c) 2008-2010, Xebic Research B.V.\nAll rights reserved.\n\nRedistribution and use of this software in source and binary forms, with or without modification, are\npermitted provided that the following conditions are met:\n\n    * Redistributions of source code must retain the above\n      copyright notice, this list of conditions and the\n      following disclaimer.\n\n    * Redistributions in binary form must reproduce the above\n      copyright notice, this list of conditions and the\n      following disclaimer in the documentation and/or other\n      materials provided with the distribution.\n\n    * Neither the name of Xebic Research B.V. nor the names of its\n      contributors may be used to endorse or promote products\n      derived from this software without specific prior\n      written permission of Xebic Research B.V.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED\nWARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A\nPARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR\nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS\nINTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR\nTORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF\nADVISED OF THE POSSIBILITY OF SUCH DAMAGE."
        },
        {
          "name": "Lucene.Net.sln",
          "type": "blob",
          "size": 40.4560546875,
          "content": "Microsoft Visual Studio Solution File, Format Version 12.00\n# Visual Studio Version 17\n# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\nVisualStudioVersion = 17.10.35004.147\nMinimumVisualStudioVersion = 15.0.26730.8\nProject(\"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\") = \"dotnet\", \"dotnet\", \"{8CA61D33-3590-4024-A304-7B1F75B50653}\"\n\tProjectSection(SolutionItems) = preProject\n\t\tsrc\\dotnet\\Directory.Build.props = src\\dotnet\\Directory.Build.props\n\tEndProjectSection\nEndProject\nProject(\"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\") = \"tools\", \"tools\", \"{4DF7EACE-2B25-43F6-B558-8520BF20BD76}\"\n\tProjectSection(SolutionItems) = preProject\n\t\tsrc\\dotnet\\tools\\Directory.Build.props = src\\dotnet\\tools\\Directory.Build.props\n\tEndProjectSection\nEndProject\nProject(\"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\") = \"Solution Items\", \"Solution Items\", \"{4DF0A2A1-B9C7-4EE5-BAF0-BEEF53E34220}\"\n\tProjectSection(SolutionItems) = preProject\n\t\t.asf.yaml = .asf.yaml\n\t\t.editorconfig = .editorconfig\n\t\t.rat-excludes = .rat-excludes\n\t\tCHANGES.txt = CHANGES.txt\n\t\tCONTRIBUTING.md = CONTRIBUTING.md\n\t\tDirectory.Build.props = Directory.Build.props\n\t\tDirectory.Build.targets = Directory.Build.targets\n\t\t.config\\dotnet-tools.json = .config\\dotnet-tools.json\n\t\tglobal.json = global.json\n\t\tLICENSE.txt = LICENSE.txt\n\t\tNOTICE.txt = NOTICE.txt\n\t\tNuGet.config = NuGet.config\n\t\tREADME.md = README.md\n\tEndProjectSection\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net\", \"src\\Lucene.Net\\Lucene.Net.csproj\", \"{3A0AA37E-2B7B-4416-B528-DA4E0E6A6706}\"\n\tProjectSection(ProjectDependencies) = postProject\n\t\t{441876AF-F691-408C-85EC-6A934E60F627} = {441876AF-F691-408C-85EC-6A934E60F627}\n\t\t{5CD4D4E8-6132-4384-98FC-6AB1C97E0B80} = {5CD4D4E8-6132-4384-98FC-6AB1C97E0B80}\n\tEndProjectSection\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Analysis.Common\", \"src\\Lucene.Net.Analysis.Common\\Lucene.Net.Analysis.Common.csproj\", \"{3D0366A8-515D-44F0-835F-4118853CFA14}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Analysis.Kuromoji\", \"src\\Lucene.Net.Analysis.Kuromoji\\Lucene.Net.Analysis.Kuromoji.csproj\", \"{2DFBA3AD-BB7D-41A1-9478-F3E1FD1FE886}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Analysis.Phonetic\", \"src\\Lucene.Net.Analysis.Phonetic\\Lucene.Net.Analysis.Phonetic.csproj\", \"{60DA2959-109E-4E3C-AC5E-51C291311302}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Analysis.SmartCn\", \"src\\Lucene.Net.Analysis.SmartCn\\Lucene.Net.Analysis.SmartCn.csproj\", \"{CE1F86F0-ECDD-4218-BF55-4E1738C562F6}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Analysis.Stempel\", \"src\\Lucene.Net.Analysis.Stempel\\Lucene.Net.Analysis.Stempel.csproj\", \"{F8293D73-AB75-4603-BBF6-3F3D093E934E}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Benchmark\", \"src\\Lucene.Net.Benchmark\\Lucene.Net.Benchmark.csproj\", \"{B6E957F3-0042-4CA6-A92B-7F14CCD9BF32}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Classification\", \"src\\Lucene.Net.Classification\\Lucene.Net.Classification.csproj\", \"{63EC0EDA-81E9-4AEB-A09A-F956A9001743}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Codecs\", \"src\\Lucene.Net.Codecs\\Lucene.Net.Codecs.csproj\", \"{8D6952F1-5BC9-4389-A20A-E7BAE40D5597}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Demo\", \"src\\Lucene.Net.Demo\\Lucene.Net.Demo.csproj\", \"{7BB449AA-EB89-404F-B5BC-586197C06B5B}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Expressions\", \"src\\Lucene.Net.Expressions\\Lucene.Net.Expressions.csproj\", \"{EA889582-4ABA-4AFA-A125-D7A37B353B10}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Facet\", \"src\\Lucene.Net.Facet\\Lucene.Net.Facet.csproj\", \"{BFBAB6D5-AE96-4364-A996-D31B0C55D4A6}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Grouping\", \"src\\Lucene.Net.Grouping\\Lucene.Net.Grouping.csproj\", \"{FF862392-B2AF-422D-865E-768E969A9412}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Highlighter\", \"src\\Lucene.Net.Highlighter\\Lucene.Net.Highlighter.csproj\", \"{AAE8E7DE-F98E-40FE-87F0-F1A97FB72E71}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Join\", \"src\\Lucene.Net.Join\\Lucene.Net.Join.csproj\", \"{31562276-B6A8-4A9E-8324-31A2D1CB04AD}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Memory\", \"src\\Lucene.Net.Memory\\Lucene.Net.Memory.csproj\", \"{0E948685-B4BD-4143-B70D-85FE11E6E406}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Misc\", \"src\\Lucene.Net.Misc\\Lucene.Net.Misc.csproj\", \"{56B95CA9-CD38-433E-B0AC-0504EC61A479}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Queries\", \"src\\Lucene.Net.Queries\\Lucene.Net.Queries.csproj\", \"{B91B0E5E-AF3A-4373-934E-91AEE02C5FA1}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.QueryParser\", \"src\\Lucene.Net.QueryParser\\Lucene.Net.QueryParser.csproj\", \"{01CD8DE8-274F-476F-8419-7B3F018C749D}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Sandbox\", \"src\\Lucene.Net.Sandbox\\Lucene.Net.Sandbox.csproj\", \"{46D088A4-5F56-4462-9E84-B482E2E1516F}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Spatial\", \"src\\Lucene.Net.Spatial\\Lucene.Net.Spatial.csproj\", \"{62FFF347-AE79-447A-B77F-F3C33F957342}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Suggest\", \"src\\Lucene.Net.Suggest\\Lucene.Net.Suggest.csproj\", \"{ED5520B3-79D6-46DF-AF02-3BA1E89A4B13}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Analysis.Common\", \"src\\Lucene.Net.Tests.Analysis.Common\\Lucene.Net.Tests.Analysis.Common.csproj\", \"{258BB6A8-A23A-42CE-A3C2-2B577CEC3F5A}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Analysis.Kuromoji\", \"src\\Lucene.Net.Tests.Analysis.Kuromoji\\Lucene.Net.Tests.Analysis.Kuromoji.csproj\", \"{8DA818F6-1E5E-4DCF-B152-A0E0A817A42C}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Analysis.Phonetic\", \"src\\Lucene.Net.Tests.Analysis.Phonetic\\Lucene.Net.Tests.Analysis.Phonetic.csproj\", \"{B586E20A-C956-447C-898F-9394BDDDE203}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Analysis.SmartCn\", \"src\\Lucene.Net.Tests.Analysis.SmartCn\\Lucene.Net.Tests.Analysis.SmartCn.csproj\", \"{824ADF18-5051-4530-88CA-4B39278B136E}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Analysis.Stempel\", \"src\\Lucene.Net.Tests.Analysis.Stempel\\Lucene.Net.Tests.Analysis.Stempel.csproj\", \"{19ED3375-A167-4CB0-8EFA-1C7B071BE10F}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Benchmark\", \"src\\Lucene.Net.Tests.Benchmark\\Lucene.Net.Tests.Benchmark.csproj\", \"{2001B0FC-28D1-44C7-928E-A07DDFF78B9F}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Classification\", \"src\\Lucene.Net.Tests.Classification\\Lucene.Net.Tests.Classification.csproj\", \"{C5E413A1-79CD-4454-8283-E36AF64F316E}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Codecs\", \"src\\Lucene.Net.Tests.Codecs\\Lucene.Net.Tests.Codecs.csproj\", \"{8D40FDA3-B26D-48F1-BCF2-176F71C95A18}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Demo\", \"src\\Lucene.Net.Tests.Demo\\Lucene.Net.Tests.Demo.csproj\", \"{9E94C13A-1557-4FEA-832B-6796923E5D8C}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Expressions\", \"src\\Lucene.Net.Tests.Expressions\\Lucene.Net.Tests.Expressions.csproj\", \"{6E25E694-B3A5-478A-B906-92FCE65F6D79}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Facet\", \"src\\Lucene.Net.Tests.Facet\\Lucene.Net.Tests.Facet.csproj\", \"{CFBBF43D-730F-4361-BDC3-F7F4D1724B2A}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Grouping\", \"src\\Lucene.Net.Tests.Grouping\\Lucene.Net.Tests.Grouping.csproj\", \"{E28BC63B-27B4-4010-A554-96BEC5E025B5}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Highlighter\", \"src\\Lucene.Net.Tests.Highlighter\\Lucene.Net.Tests.Highlighter.csproj\", \"{AE5D4D03-81CF-473B-8977-5B7905D317FF}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Join\", \"src\\Lucene.Net.Tests.Join\\Lucene.Net.Tests.Join.csproj\", \"{58714AB1-C300-4B00-8DE6-081A2F610517}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Memory\", \"src\\Lucene.Net.Tests.Memory\\Lucene.Net.Tests.Memory.csproj\", \"{3BE7B6EA-8DBC-45E2-947C-1CA7E63B5603}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Misc\", \"src\\Lucene.Net.Tests.Misc\\Lucene.Net.Tests.Misc.csproj\", \"{F8DDC5B7-A621-4B67-AB4B-BBE083C05BB8}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Queries\", \"src\\Lucene.Net.Tests.Queries\\Lucene.Net.Tests.Queries.csproj\", \"{AC750DC0-05A3-4F96-8CC5-CFC8FD01D4CF}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.QueryParser\", \"src\\Lucene.Net.Tests.QueryParser\\Lucene.Net.Tests.QueryParser.csproj\", \"{05E0A438-4329-45D4-B63D-C3E0710CFB8E}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Sandbox\", \"src\\Lucene.Net.Tests.Sandbox\\Lucene.Net.Tests.Sandbox.csproj\", \"{B4641989-BB63-4B72-B8CF-751D96D39098}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Spatial\", \"src\\Lucene.Net.Tests.Spatial\\Lucene.Net.Tests.Spatial.csproj\", \"{8CE981D1-E0D0-4A91-A004-325C6607A6E2}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Suggest\", \"src\\Lucene.Net.Tests.Suggest\\Lucene.Net.Tests.Suggest.csproj\", \"{FC05947E-B57D-4458-B937-8AC19E6C85EC}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Replicator\", \"src\\Lucene.Net.Replicator\\Lucene.Net.Replicator.csproj\", \"{41C809F8-85B0-47A1-B3B6-9735DD518846}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Replicator\", \"src\\Lucene.Net.Tests.Replicator\\Lucene.Net.Tests.Replicator.csproj\", \"{10D76AFD-AFE9-4C45-8452-E489FC6362B8}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Replicator.AspNetCore\", \"src\\dotnet\\Lucene.Net.Replicator.AspNetCore\\Lucene.Net.Replicator.AspNetCore.csproj\", \"{EFB2E31A-5917-49D5-A808-FE5061A550B4}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"lucene-cli\", \"src\\dotnet\\tools\\lucene-cli\\lucene-cli.csproj\", \"{119BBACD-D4DB-4E3B-922F-3DA83E0B29E2}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.ICU\", \"src\\dotnet\\Lucene.Net.ICU\\Lucene.Net.ICU.csproj\", \"{CF3A74CA-FEFD-4F41-961B-CC8CF8D96286}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.ICU\", \"src\\dotnet\\Lucene.Net.Tests.ICU\\Lucene.Net.Tests.ICU.csproj\", \"{4B054831-5275-44E2-A4D4-CA0B19BEE19A}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Cli\", \"src\\dotnet\\tools\\Lucene.Net.Tests.Cli\\Lucene.Net.Tests.Cli.csproj\", \"{1F5574FE-19F7-4F10-9B88-76A938621F5B}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests._A-D\", \"src\\Lucene.Net.Tests._A-D\\Lucene.Net.Tests._A-D.csproj\", \"{86FADACC-57EC-4AA3-8BBE-C6ED526EE596}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests._E-I\", \"src\\Lucene.Net.Tests._E-I\\Lucene.Net.Tests._E-I.csproj\", \"{8251BB22-EDA9-4850-A9B7-259C5B171040}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests._I-J\", \"src\\Lucene.Net.Tests._I-J\\Lucene.Net.Tests._I-J.csproj\", \"{1FD12FC1-BE00-4EA3-8377-2043A6D6E6E6}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests._J-S\", \"src\\Lucene.Net.Tests._J-S\\Lucene.Net.Tests._J-S.csproj\", \"{5BE1EBA7-876C-4E9D-A78C-46A0D6F588C9}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests._T-Z\", \"src\\Lucene.Net.Tests._T-Z\\Lucene.Net.Tests._T-Z.csproj\", \"{2C6C2EDE-C4B6-45D3-8E86-CDAE0A629FD6}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.TestFramework\", \"src\\Lucene.Net.TestFramework\\Lucene.Net.TestFramework.csproj\", \"{845F8491-348C-4242-A58A-1979E8338B5D}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.TestFramework\", \"src\\Lucene.Net.Tests.TestFramework\\Lucene.Net.Tests.TestFramework.csproj\", \"{3A761C02-C3B2-4672-92F9-B700C68E7EFA}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.TestFramework.DependencyInjection\", \"src\\Lucene.Net.Tests.TestFramework.DependencyInjection\\Lucene.Net.Tests.TestFramework.DependencyInjection.csproj\", \"{6360B673-1E0E-4249-8F84-F24B281F8FD7}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Analysis.OpenNLP\", \"src\\Lucene.Net.Analysis.OpenNLP\\Lucene.Net.Analysis.OpenNLP.csproj\", \"{CC2CE069-5BBB-429E-8510-7C3FBA8069D5}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Analysis.OpenNLP\", \"src\\Lucene.Net.Tests.Analysis.OpenNLP\\Lucene.Net.Tests.Analysis.OpenNLP.csproj\", \"{88D6D124-711D-4232-AD70-F22AB6AF9EA1}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Analysis.Morfologik\", \"src\\Lucene.Net.Analysis.Morfologik\\Lucene.Net.Analysis.Morfologik.csproj\", \"{17C7E54C-7A95-46A5-9905-90F68D349F3F}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.Analysis.Morfologik\", \"src\\Lucene.Net.Tests.Analysis.Morfologik\\Lucene.Net.Tests.Analysis.Morfologik.csproj\", \"{435F91AD-8BA4-4376-904C-385A165C1AF0}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.CodeAnalysis\", \"src\\dotnet\\Lucene.Net.Tests.CodeAnalysis\\Lucene.Net.Tests.CodeAnalysis.csproj\", \"{158F5D30-8B96-4C49-9009-0B8ACEDF8546}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.CodeAnalysis.CSharp\", \"src\\dotnet\\Lucene.Net.CodeAnalysis.CSharp\\Lucene.Net.CodeAnalysis.CSharp.csproj\", \"{441876AF-F691-408C-85EC-6A934E60F627}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.CodeAnalysis.VisualBasic\", \"src\\dotnet\\Lucene.Net.CodeAnalysis.VisualBasic\\Lucene.Net.CodeAnalysis.VisualBasic.csproj\", \"{5CD4D4E8-6132-4384-98FC-6AB1C97E0B80}\"\nEndProject\nProject(\"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\") = \"Lucene.Net.CodeAnalysis\", \"Lucene.Net.CodeAnalysis\", \"{E5E8C5DC-7048-4818-B884-FB2D037D2EF2}\"\n\tProjectSection(SolutionItems) = preProject\n\t\tsrc\\dotnet\\Lucene.Net.CodeAnalysis\\Version.props = src\\dotnet\\Lucene.Net.CodeAnalysis\\Version.props\n\tEndProjectSection\nEndProject\nProject(\"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\") = \"tools\", \"tools\", \"{4D0ED7D9-ABEE-4890-B06C-477E3A32B9A0}\"\n\tProjectSection(SolutionItems) = preProject\n\t\tsrc\\dotnet\\Lucene.Net.CodeAnalysis\\tools\\install.ps1 = src\\dotnet\\Lucene.Net.CodeAnalysis\\tools\\install.ps1\n\t\tsrc\\dotnet\\Lucene.Net.CodeAnalysis\\tools\\uninstall.ps1 = src\\dotnet\\Lucene.Net.CodeAnalysis\\tools\\uninstall.ps1\n\tEndProjectSection\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"Lucene.Net.Tests.AllProjects\", \"src\\Lucene.Net.Tests.AllProjects\\Lucene.Net.Tests.AllProjects.csproj\", \"{9880B87D-8D14-476B-B093-9C3AA0DA8B24}\"\nEndProject\nProject(\"{2150E333-8FDC-42A3-9474-1A3956D46DE8}\") = \"docs\", \"docs\", \"{42599646-275F-4970-BC60-A3349F6498CC}\"\nEndProject\nProject(\"{9A19103F-16F7-4668-BE54-9A1E7A4F7556}\") = \"LuceneDocsPlugins\", \"src\\docs\\LuceneDocsPlugins\\LuceneDocsPlugins.csproj\", \"{FED4A824-1F32-4948-8D37-2B7610804DB5}\"\nEndProject\nProject(\"{13B669BE-BB05-4DDF-9536-439F39A36129}\") = \"websites\", \"proj\\websites.msbuildproj\", \"{C0448DD3-68D2-485F-B31A-D2806E589FA7}\"\nEndProject\nProject(\"{13B669BE-BB05-4DDF-9536-439F39A36129}\") = \"build\", \"proj\\build.msbuildproj\", \"{5C5253E9-BAF2-493C-B4D4-EE01D2E1769F}\"\nEndProject\nProject(\"{13B669BE-BB05-4DDF-9536-439F39A36129}\") = \"github\", \"proj\\github.msbuildproj\", \"{E71152A0-48CC-4334-981F-F5FBFFA50891}\"\nEndProject\nGlobal\n\tGlobalSection(SolutionConfigurationPlatforms) = preSolution\n\t\tDebug|Any CPU = Debug|Any CPU\n\t\tRelease|Any CPU = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(ProjectConfigurationPlatforms) = postSolution\n\t\t{3A0AA37E-2B7B-4416-B528-DA4E0E6A6706}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{3A0AA37E-2B7B-4416-B528-DA4E0E6A6706}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{3A0AA37E-2B7B-4416-B528-DA4E0E6A6706}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{3A0AA37E-2B7B-4416-B528-DA4E0E6A6706}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{3D0366A8-515D-44F0-835F-4118853CFA14}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{3D0366A8-515D-44F0-835F-4118853CFA14}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{3D0366A8-515D-44F0-835F-4118853CFA14}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{3D0366A8-515D-44F0-835F-4118853CFA14}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{2DFBA3AD-BB7D-41A1-9478-F3E1FD1FE886}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{2DFBA3AD-BB7D-41A1-9478-F3E1FD1FE886}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{2DFBA3AD-BB7D-41A1-9478-F3E1FD1FE886}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{2DFBA3AD-BB7D-41A1-9478-F3E1FD1FE886}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{60DA2959-109E-4E3C-AC5E-51C291311302}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{60DA2959-109E-4E3C-AC5E-51C291311302}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{60DA2959-109E-4E3C-AC5E-51C291311302}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{60DA2959-109E-4E3C-AC5E-51C291311302}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{CE1F86F0-ECDD-4218-BF55-4E1738C562F6}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{CE1F86F0-ECDD-4218-BF55-4E1738C562F6}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{CE1F86F0-ECDD-4218-BF55-4E1738C562F6}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{CE1F86F0-ECDD-4218-BF55-4E1738C562F6}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{F8293D73-AB75-4603-BBF6-3F3D093E934E}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{F8293D73-AB75-4603-BBF6-3F3D093E934E}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{F8293D73-AB75-4603-BBF6-3F3D093E934E}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{F8293D73-AB75-4603-BBF6-3F3D093E934E}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{B6E957F3-0042-4CA6-A92B-7F14CCD9BF32}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{B6E957F3-0042-4CA6-A92B-7F14CCD9BF32}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{B6E957F3-0042-4CA6-A92B-7F14CCD9BF32}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{B6E957F3-0042-4CA6-A92B-7F14CCD9BF32}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{63EC0EDA-81E9-4AEB-A09A-F956A9001743}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{63EC0EDA-81E9-4AEB-A09A-F956A9001743}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{63EC0EDA-81E9-4AEB-A09A-F956A9001743}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{63EC0EDA-81E9-4AEB-A09A-F956A9001743}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{8D6952F1-5BC9-4389-A20A-E7BAE40D5597}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{8D6952F1-5BC9-4389-A20A-E7BAE40D5597}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{8D6952F1-5BC9-4389-A20A-E7BAE40D5597}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{8D6952F1-5BC9-4389-A20A-E7BAE40D5597}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{7BB449AA-EB89-404F-B5BC-586197C06B5B}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{7BB449AA-EB89-404F-B5BC-586197C06B5B}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{7BB449AA-EB89-404F-B5BC-586197C06B5B}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{7BB449AA-EB89-404F-B5BC-586197C06B5B}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{EA889582-4ABA-4AFA-A125-D7A37B353B10}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{EA889582-4ABA-4AFA-A125-D7A37B353B10}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{EA889582-4ABA-4AFA-A125-D7A37B353B10}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{EA889582-4ABA-4AFA-A125-D7A37B353B10}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{BFBAB6D5-AE96-4364-A996-D31B0C55D4A6}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{BFBAB6D5-AE96-4364-A996-D31B0C55D4A6}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{BFBAB6D5-AE96-4364-A996-D31B0C55D4A6}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{BFBAB6D5-AE96-4364-A996-D31B0C55D4A6}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{FF862392-B2AF-422D-865E-768E969A9412}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{FF862392-B2AF-422D-865E-768E969A9412}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{FF862392-B2AF-422D-865E-768E969A9412}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{FF862392-B2AF-422D-865E-768E969A9412}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{AAE8E7DE-F98E-40FE-87F0-F1A97FB72E71}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{AAE8E7DE-F98E-40FE-87F0-F1A97FB72E71}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{AAE8E7DE-F98E-40FE-87F0-F1A97FB72E71}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{AAE8E7DE-F98E-40FE-87F0-F1A97FB72E71}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{31562276-B6A8-4A9E-8324-31A2D1CB04AD}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{31562276-B6A8-4A9E-8324-31A2D1CB04AD}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{31562276-B6A8-4A9E-8324-31A2D1CB04AD}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{31562276-B6A8-4A9E-8324-31A2D1CB04AD}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{0E948685-B4BD-4143-B70D-85FE11E6E406}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{0E948685-B4BD-4143-B70D-85FE11E6E406}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{0E948685-B4BD-4143-B70D-85FE11E6E406}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{0E948685-B4BD-4143-B70D-85FE11E6E406}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{56B95CA9-CD38-433E-B0AC-0504EC61A479}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{56B95CA9-CD38-433E-B0AC-0504EC61A479}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{56B95CA9-CD38-433E-B0AC-0504EC61A479}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{56B95CA9-CD38-433E-B0AC-0504EC61A479}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{B91B0E5E-AF3A-4373-934E-91AEE02C5FA1}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{B91B0E5E-AF3A-4373-934E-91AEE02C5FA1}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{B91B0E5E-AF3A-4373-934E-91AEE02C5FA1}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{B91B0E5E-AF3A-4373-934E-91AEE02C5FA1}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{01CD8DE8-274F-476F-8419-7B3F018C749D}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{01CD8DE8-274F-476F-8419-7B3F018C749D}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{01CD8DE8-274F-476F-8419-7B3F018C749D}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{01CD8DE8-274F-476F-8419-7B3F018C749D}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{46D088A4-5F56-4462-9E84-B482E2E1516F}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{46D088A4-5F56-4462-9E84-B482E2E1516F}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{46D088A4-5F56-4462-9E84-B482E2E1516F}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{46D088A4-5F56-4462-9E84-B482E2E1516F}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{62FFF347-AE79-447A-B77F-F3C33F957342}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{62FFF347-AE79-447A-B77F-F3C33F957342}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{62FFF347-AE79-447A-B77F-F3C33F957342}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{62FFF347-AE79-447A-B77F-F3C33F957342}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{ED5520B3-79D6-46DF-AF02-3BA1E89A4B13}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{ED5520B3-79D6-46DF-AF02-3BA1E89A4B13}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{ED5520B3-79D6-46DF-AF02-3BA1E89A4B13}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{ED5520B3-79D6-46DF-AF02-3BA1E89A4B13}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{258BB6A8-A23A-42CE-A3C2-2B577CEC3F5A}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{258BB6A8-A23A-42CE-A3C2-2B577CEC3F5A}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{258BB6A8-A23A-42CE-A3C2-2B577CEC3F5A}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{258BB6A8-A23A-42CE-A3C2-2B577CEC3F5A}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{8DA818F6-1E5E-4DCF-B152-A0E0A817A42C}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{8DA818F6-1E5E-4DCF-B152-A0E0A817A42C}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{8DA818F6-1E5E-4DCF-B152-A0E0A817A42C}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{8DA818F6-1E5E-4DCF-B152-A0E0A817A42C}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{B586E20A-C956-447C-898F-9394BDDDE203}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{B586E20A-C956-447C-898F-9394BDDDE203}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{B586E20A-C956-447C-898F-9394BDDDE203}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{B586E20A-C956-447C-898F-9394BDDDE203}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{824ADF18-5051-4530-88CA-4B39278B136E}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{824ADF18-5051-4530-88CA-4B39278B136E}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{824ADF18-5051-4530-88CA-4B39278B136E}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{824ADF18-5051-4530-88CA-4B39278B136E}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{19ED3375-A167-4CB0-8EFA-1C7B071BE10F}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{19ED3375-A167-4CB0-8EFA-1C7B071BE10F}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{19ED3375-A167-4CB0-8EFA-1C7B071BE10F}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{19ED3375-A167-4CB0-8EFA-1C7B071BE10F}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{2001B0FC-28D1-44C7-928E-A07DDFF78B9F}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{2001B0FC-28D1-44C7-928E-A07DDFF78B9F}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{2001B0FC-28D1-44C7-928E-A07DDFF78B9F}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{2001B0FC-28D1-44C7-928E-A07DDFF78B9F}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{C5E413A1-79CD-4454-8283-E36AF64F316E}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{C5E413A1-79CD-4454-8283-E36AF64F316E}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{C5E413A1-79CD-4454-8283-E36AF64F316E}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{C5E413A1-79CD-4454-8283-E36AF64F316E}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{8D40FDA3-B26D-48F1-BCF2-176F71C95A18}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{8D40FDA3-B26D-48F1-BCF2-176F71C95A18}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{8D40FDA3-B26D-48F1-BCF2-176F71C95A18}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{8D40FDA3-B26D-48F1-BCF2-176F71C95A18}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{9E94C13A-1557-4FEA-832B-6796923E5D8C}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{9E94C13A-1557-4FEA-832B-6796923E5D8C}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{9E94C13A-1557-4FEA-832B-6796923E5D8C}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{9E94C13A-1557-4FEA-832B-6796923E5D8C}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{6E25E694-B3A5-478A-B906-92FCE65F6D79}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{6E25E694-B3A5-478A-B906-92FCE65F6D79}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{6E25E694-B3A5-478A-B906-92FCE65F6D79}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{6E25E694-B3A5-478A-B906-92FCE65F6D79}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{CFBBF43D-730F-4361-BDC3-F7F4D1724B2A}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{CFBBF43D-730F-4361-BDC3-F7F4D1724B2A}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{CFBBF43D-730F-4361-BDC3-F7F4D1724B2A}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{CFBBF43D-730F-4361-BDC3-F7F4D1724B2A}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{E28BC63B-27B4-4010-A554-96BEC5E025B5}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{E28BC63B-27B4-4010-A554-96BEC5E025B5}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{E28BC63B-27B4-4010-A554-96BEC5E025B5}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{E28BC63B-27B4-4010-A554-96BEC5E025B5}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{AE5D4D03-81CF-473B-8977-5B7905D317FF}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{AE5D4D03-81CF-473B-8977-5B7905D317FF}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{AE5D4D03-81CF-473B-8977-5B7905D317FF}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{AE5D4D03-81CF-473B-8977-5B7905D317FF}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{58714AB1-C300-4B00-8DE6-081A2F610517}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{58714AB1-C300-4B00-8DE6-081A2F610517}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{58714AB1-C300-4B00-8DE6-081A2F610517}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{58714AB1-C300-4B00-8DE6-081A2F610517}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{3BE7B6EA-8DBC-45E2-947C-1CA7E63B5603}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{3BE7B6EA-8DBC-45E2-947C-1CA7E63B5603}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{3BE7B6EA-8DBC-45E2-947C-1CA7E63B5603}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{3BE7B6EA-8DBC-45E2-947C-1CA7E63B5603}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{F8DDC5B7-A621-4B67-AB4B-BBE083C05BB8}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{F8DDC5B7-A621-4B67-AB4B-BBE083C05BB8}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{F8DDC5B7-A621-4B67-AB4B-BBE083C05BB8}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{F8DDC5B7-A621-4B67-AB4B-BBE083C05BB8}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{AC750DC0-05A3-4F96-8CC5-CFC8FD01D4CF}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{AC750DC0-05A3-4F96-8CC5-CFC8FD01D4CF}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{AC750DC0-05A3-4F96-8CC5-CFC8FD01D4CF}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{AC750DC0-05A3-4F96-8CC5-CFC8FD01D4CF}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{05E0A438-4329-45D4-B63D-C3E0710CFB8E}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{05E0A438-4329-45D4-B63D-C3E0710CFB8E}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{05E0A438-4329-45D4-B63D-C3E0710CFB8E}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{05E0A438-4329-45D4-B63D-C3E0710CFB8E}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{B4641989-BB63-4B72-B8CF-751D96D39098}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{B4641989-BB63-4B72-B8CF-751D96D39098}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{B4641989-BB63-4B72-B8CF-751D96D39098}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{B4641989-BB63-4B72-B8CF-751D96D39098}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{8CE981D1-E0D0-4A91-A004-325C6607A6E2}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{8CE981D1-E0D0-4A91-A004-325C6607A6E2}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{8CE981D1-E0D0-4A91-A004-325C6607A6E2}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{8CE981D1-E0D0-4A91-A004-325C6607A6E2}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{FC05947E-B57D-4458-B937-8AC19E6C85EC}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{FC05947E-B57D-4458-B937-8AC19E6C85EC}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{FC05947E-B57D-4458-B937-8AC19E6C85EC}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{FC05947E-B57D-4458-B937-8AC19E6C85EC}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{41C809F8-85B0-47A1-B3B6-9735DD518846}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{41C809F8-85B0-47A1-B3B6-9735DD518846}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{41C809F8-85B0-47A1-B3B6-9735DD518846}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{41C809F8-85B0-47A1-B3B6-9735DD518846}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{10D76AFD-AFE9-4C45-8452-E489FC6362B8}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{10D76AFD-AFE9-4C45-8452-E489FC6362B8}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{10D76AFD-AFE9-4C45-8452-E489FC6362B8}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{10D76AFD-AFE9-4C45-8452-E489FC6362B8}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{EFB2E31A-5917-49D5-A808-FE5061A550B4}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{EFB2E31A-5917-49D5-A808-FE5061A550B4}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{EFB2E31A-5917-49D5-A808-FE5061A550B4}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{EFB2E31A-5917-49D5-A808-FE5061A550B4}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{119BBACD-D4DB-4E3B-922F-3DA83E0B29E2}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{119BBACD-D4DB-4E3B-922F-3DA83E0B29E2}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{119BBACD-D4DB-4E3B-922F-3DA83E0B29E2}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{119BBACD-D4DB-4E3B-922F-3DA83E0B29E2}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{CF3A74CA-FEFD-4F41-961B-CC8CF8D96286}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{CF3A74CA-FEFD-4F41-961B-CC8CF8D96286}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{CF3A74CA-FEFD-4F41-961B-CC8CF8D96286}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{CF3A74CA-FEFD-4F41-961B-CC8CF8D96286}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{4B054831-5275-44E2-A4D4-CA0B19BEE19A}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{4B054831-5275-44E2-A4D4-CA0B19BEE19A}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{4B054831-5275-44E2-A4D4-CA0B19BEE19A}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{4B054831-5275-44E2-A4D4-CA0B19BEE19A}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{1F5574FE-19F7-4F10-9B88-76A938621F5B}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{1F5574FE-19F7-4F10-9B88-76A938621F5B}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{1F5574FE-19F7-4F10-9B88-76A938621F5B}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{1F5574FE-19F7-4F10-9B88-76A938621F5B}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{86FADACC-57EC-4AA3-8BBE-C6ED526EE596}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{86FADACC-57EC-4AA3-8BBE-C6ED526EE596}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{86FADACC-57EC-4AA3-8BBE-C6ED526EE596}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{86FADACC-57EC-4AA3-8BBE-C6ED526EE596}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{8251BB22-EDA9-4850-A9B7-259C5B171040}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{8251BB22-EDA9-4850-A9B7-259C5B171040}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{8251BB22-EDA9-4850-A9B7-259C5B171040}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{8251BB22-EDA9-4850-A9B7-259C5B171040}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{1FD12FC1-BE00-4EA3-8377-2043A6D6E6E6}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{1FD12FC1-BE00-4EA3-8377-2043A6D6E6E6}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{1FD12FC1-BE00-4EA3-8377-2043A6D6E6E6}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{1FD12FC1-BE00-4EA3-8377-2043A6D6E6E6}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{5BE1EBA7-876C-4E9D-A78C-46A0D6F588C9}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{5BE1EBA7-876C-4E9D-A78C-46A0D6F588C9}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{5BE1EBA7-876C-4E9D-A78C-46A0D6F588C9}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{5BE1EBA7-876C-4E9D-A78C-46A0D6F588C9}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{2C6C2EDE-C4B6-45D3-8E86-CDAE0A629FD6}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{2C6C2EDE-C4B6-45D3-8E86-CDAE0A629FD6}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{2C6C2EDE-C4B6-45D3-8E86-CDAE0A629FD6}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{2C6C2EDE-C4B6-45D3-8E86-CDAE0A629FD6}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{845F8491-348C-4242-A58A-1979E8338B5D}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{845F8491-348C-4242-A58A-1979E8338B5D}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{845F8491-348C-4242-A58A-1979E8338B5D}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{845F8491-348C-4242-A58A-1979E8338B5D}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{3A761C02-C3B2-4672-92F9-B700C68E7EFA}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{3A761C02-C3B2-4672-92F9-B700C68E7EFA}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{3A761C02-C3B2-4672-92F9-B700C68E7EFA}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{3A761C02-C3B2-4672-92F9-B700C68E7EFA}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{6360B673-1E0E-4249-8F84-F24B281F8FD7}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{6360B673-1E0E-4249-8F84-F24B281F8FD7}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{6360B673-1E0E-4249-8F84-F24B281F8FD7}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{6360B673-1E0E-4249-8F84-F24B281F8FD7}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{CC2CE069-5BBB-429E-8510-7C3FBA8069D5}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{CC2CE069-5BBB-429E-8510-7C3FBA8069D5}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{CC2CE069-5BBB-429E-8510-7C3FBA8069D5}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{CC2CE069-5BBB-429E-8510-7C3FBA8069D5}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{88D6D124-711D-4232-AD70-F22AB6AF9EA1}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{88D6D124-711D-4232-AD70-F22AB6AF9EA1}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{88D6D124-711D-4232-AD70-F22AB6AF9EA1}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{88D6D124-711D-4232-AD70-F22AB6AF9EA1}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{17C7E54C-7A95-46A5-9905-90F68D349F3F}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{17C7E54C-7A95-46A5-9905-90F68D349F3F}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{17C7E54C-7A95-46A5-9905-90F68D349F3F}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{17C7E54C-7A95-46A5-9905-90F68D349F3F}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{435F91AD-8BA4-4376-904C-385A165C1AF0}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{435F91AD-8BA4-4376-904C-385A165C1AF0}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{435F91AD-8BA4-4376-904C-385A165C1AF0}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{435F91AD-8BA4-4376-904C-385A165C1AF0}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{158F5D30-8B96-4C49-9009-0B8ACEDF8546}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{158F5D30-8B96-4C49-9009-0B8ACEDF8546}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{158F5D30-8B96-4C49-9009-0B8ACEDF8546}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{158F5D30-8B96-4C49-9009-0B8ACEDF8546}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{441876AF-F691-408C-85EC-6A934E60F627}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{441876AF-F691-408C-85EC-6A934E60F627}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{441876AF-F691-408C-85EC-6A934E60F627}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{441876AF-F691-408C-85EC-6A934E60F627}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{5CD4D4E8-6132-4384-98FC-6AB1C97E0B80}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{5CD4D4E8-6132-4384-98FC-6AB1C97E0B80}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{5CD4D4E8-6132-4384-98FC-6AB1C97E0B80}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{5CD4D4E8-6132-4384-98FC-6AB1C97E0B80}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{9880B87D-8D14-476B-B093-9C3AA0DA8B24}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{9880B87D-8D14-476B-B093-9C3AA0DA8B24}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{9880B87D-8D14-476B-B093-9C3AA0DA8B24}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{9880B87D-8D14-476B-B093-9C3AA0DA8B24}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{FED4A824-1F32-4948-8D37-2B7610804DB5}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{FED4A824-1F32-4948-8D37-2B7610804DB5}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{FED4A824-1F32-4948-8D37-2B7610804DB5}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{FED4A824-1F32-4948-8D37-2B7610804DB5}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{C0448DD3-68D2-485F-B31A-D2806E589FA7}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{C0448DD3-68D2-485F-B31A-D2806E589FA7}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{C0448DD3-68D2-485F-B31A-D2806E589FA7}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{C0448DD3-68D2-485F-B31A-D2806E589FA7}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{5C5253E9-BAF2-493C-B4D4-EE01D2E1769F}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{5C5253E9-BAF2-493C-B4D4-EE01D2E1769F}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{5C5253E9-BAF2-493C-B4D4-EE01D2E1769F}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{5C5253E9-BAF2-493C-B4D4-EE01D2E1769F}.Release|Any CPU.Build.0 = Release|Any CPU\n\t\t{E71152A0-48CC-4334-981F-F5FBFFA50891}.Debug|Any CPU.ActiveCfg = Debug|Any CPU\n\t\t{E71152A0-48CC-4334-981F-F5FBFFA50891}.Debug|Any CPU.Build.0 = Debug|Any CPU\n\t\t{E71152A0-48CC-4334-981F-F5FBFFA50891}.Release|Any CPU.ActiveCfg = Release|Any CPU\n\t\t{E71152A0-48CC-4334-981F-F5FBFFA50891}.Release|Any CPU.Build.0 = Release|Any CPU\n\tEndGlobalSection\n\tGlobalSection(SolutionProperties) = preSolution\n\t\tHideSolutionNode = FALSE\n\tEndGlobalSection\n\tGlobalSection(NestedProjects) = preSolution\n\t\t{4DF7EACE-2B25-43F6-B558-8520BF20BD76} = {8CA61D33-3590-4024-A304-7B1F75B50653}\n\t\t{EFB2E31A-5917-49D5-A808-FE5061A550B4} = {8CA61D33-3590-4024-A304-7B1F75B50653}\n\t\t{119BBACD-D4DB-4E3B-922F-3DA83E0B29E2} = {4DF7EACE-2B25-43F6-B558-8520BF20BD76}\n\t\t{CF3A74CA-FEFD-4F41-961B-CC8CF8D96286} = {8CA61D33-3590-4024-A304-7B1F75B50653}\n\t\t{4B054831-5275-44E2-A4D4-CA0B19BEE19A} = {8CA61D33-3590-4024-A304-7B1F75B50653}\n\t\t{1F5574FE-19F7-4F10-9B88-76A938621F5B} = {4DF7EACE-2B25-43F6-B558-8520BF20BD76}\n\t\t{158F5D30-8B96-4C49-9009-0B8ACEDF8546} = {8CA61D33-3590-4024-A304-7B1F75B50653}\n\t\t{441876AF-F691-408C-85EC-6A934E60F627} = {8CA61D33-3590-4024-A304-7B1F75B50653}\n\t\t{5CD4D4E8-6132-4384-98FC-6AB1C97E0B80} = {8CA61D33-3590-4024-A304-7B1F75B50653}\n\t\t{E5E8C5DC-7048-4818-B884-FB2D037D2EF2} = {8CA61D33-3590-4024-A304-7B1F75B50653}\n\t\t{4D0ED7D9-ABEE-4890-B06C-477E3A32B9A0} = {E5E8C5DC-7048-4818-B884-FB2D037D2EF2}\n\t\t{FED4A824-1F32-4948-8D37-2B7610804DB5} = {42599646-275F-4970-BC60-A3349F6498CC}\n\t\t{C0448DD3-68D2-485F-B31A-D2806E589FA7} = {42599646-275F-4970-BC60-A3349F6498CC}\n\tEndGlobalSection\n\tGlobalSection(ExtensibilityGlobals) = postSolution\n\t\tSolutionGuid = {9F2179CC-CFD2-4419-AB74-D72856931F36}\n\tEndGlobalSection\nEndGlobal\n"
        },
        {
          "name": "Lucene.Net.sln.DotSettings",
          "type": "blob",
          "size": 0.677734375,
          "content": "﻿<wpf:ResourceDictionary xml:space=\"preserve\" xmlns:x=\"http://schemas.microsoft.com/winfx/2006/xaml\" xmlns:s=\"clr-namespace:System;assembly=mscorlib\" xmlns:ss=\"urn:shemas-jetbrains-com:settings-storage-xaml\" xmlns:wpf=\"http://schemas.microsoft.com/winfx/2006/xaml/presentation\">\n\t<s:Boolean x:Key=\"/Default/UserDictionary/Words/=Coord/@EntryIndexedValue\">True</s:Boolean>\n\t<s:Boolean x:Key=\"/Default/UserDictionary/Words/=csharpsquid/@EntryIndexedValue\">True</s:Boolean>\n\t<s:Boolean x:Key=\"/Default/UserDictionary/Words/=LUCENENET/@EntryIndexedValue\">True</s:Boolean>\n\t<s:Boolean x:Key=\"/Default/UserDictionary/Words/=testsettings/@EntryIndexedValue\">True</s:Boolean></wpf:ResourceDictionary>"
        },
        {
          "name": "Lucene.Net.snk",
          "type": "blob",
          "size": 0.58203125,
          "content": null
        },
        {
          "name": "MIGRATE.md",
          "type": "blob",
          "size": 0.1015625,
          "content": "The migration guide has moved to: [src/Lucene.Net/migration-guide.md](src/Lucene.Net/migration-guide.md)"
        },
        {
          "name": "NOTICE.txt",
          "type": "blob",
          "size": 0.169921875,
          "content": "﻿Apache Lucene.Net\nCopyright 2006-2024 The Apache Software Foundation\n\nThis product includes software developed at\nThe Apache Software Foundation (http://www.apache.org/).\n"
        },
        {
          "name": "NuGet.config",
          "type": "blob",
          "size": 1.134765625,
          "content": "﻿<?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n<!--\nLicensed to the Apache Software Foundation (ASF) under one\nor more contributor license agreements.  See the NOTICE file\ndistributed with this work for additional information\nregarding copyright ownership.  The ASF licenses this file\nto you under the Apache License, Version 2.0 (the\n\"License\"); you may not use this file except in compliance\nwith the License.  You may obtain a copy of the License at\n\n  http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing,\nsoftware distributed under the License is distributed on an\n\"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\nKIND, either express or implied.  See the License for the\nspecific language governing permissions and limitations\nunder the License.\n-->\n\n<configuration>\n  <packageSources>\n    <add key=\"ICU4N preview feed\" value=\"https://www.myget.org/F/icu4n/api/v3/index.json\" />\n    <add key=\"J2N preview feed\" value=\"https://www.myget.org/F/j2n-preview/api/v3/index.json\" />\n    <add key=\"NuGet official package source\" value=\"https://api.nuget.org/v3/index.json\" />\n  </packageSources>\n</configuration>\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 18.326171875,
          "content": "﻿# Welcome to Apache Lucene.NET \n\n[![Nuget](https://img.shields.io/nuget/dt/Lucene.Net)](https://www.nuget.org/packages/Lucene.Net)\n[![Azure DevOps builds (master)](https://img.shields.io/azure-devops/build/lucene-net/6ba240c9-9598-47e7-a793-0ed8a4ba2f8b/3/master)](https://dev.azure.com/lucene-net/Lucene.NET/_build?definitionId=3&_a=summary)\n[![GitHub](https://img.shields.io/github/license/apache/lucenenet)](https://github.com/apache/lucenenet/blob/master/LICENSE.txt)\n\n## Powerful Full-text search for .NET\n\nApache Lucene.NET is an open-source full-text search library written in C#. It is a port of the popular Java Apache Lucene project.\n\nApache Lucene.NET is a .NET library providing powerful indexing and search features, as well as spellchecking, hit highlighting and advanced analysis/tokenization capabilities.\n\nLucene.NET version 4.8 (still in Beta) runs everywhere .NET runs, including Windows, Unix, MacOS, Android and iOS.\n\nThe Apache Lucene.NET website is at:\n  http://lucenenet.apache.org\n\n## Supported Frameworks\n\n### Lucene.NET 3.0.3\n\n- .NET Framework 4.0\n- .NET Framework 3.5\n\n### Lucene.NET 4.8.0\n\n- [.NET 8.0](https://dotnet.microsoft.com/download/dotnet/8.0)\n- [.NET 6.0](https://dotnet.microsoft.com/download/dotnet/6.0)\n- [.NET Standard 2.1](https://docs.microsoft.com/en-us/dotnet/standard/net-standard)\n- [.NET Standard 2.0](https://docs.microsoft.com/en-us/dotnet/standard/net-standard)\n- .NET Framework 4.6.2\n\n## Status\n\nLatest Release Version: Lucene.NET 3.0.3\n\nWorking toward Lucene.NET 4.8.0 (currently in BETA)\n\n* The beta version is extremely stable\n* Has more than 7800+ passing unit tests\n* Integrates well with .NET 8.0 and .NET 6.0 (as well as other unsupported versions)\n* Supports .NET Standard 2.1 and .NET Standard 2.0 \n* Supports .NET Framework 4.6.2+\n* Some developers already use it in production environments\n\n## Download\n\n### Lucene.NET 3.0.3\n\n##### Core Library\n\n[![NuGet version](https://img.shields.io/nuget/v/Lucene.Net.svg)](https://www.nuget.org/packages/Lucene.Net/3.0.3)\n\n```\nPM> Install-Package Lucene.Net\n```\n\n##### All Packages\n\n- [Lucene.Net](https://www.nuget.org/packages/Lucene.Net/3.0.3) - Core library\n- [Lucene.Net.Contrib](https://www.nuget.org/packages/Lucene.Net.Contrib/3.0.3) - Various user contributed functionality and extras\n- [Lucene.Net.Contrib.Spatial](https://www.nuget.org/packages/Lucene.Net.Contrib.Spatial/3.0.3) - Geospatial Search\n- [Lucene.Net.Contrib.Spatial.NTS](https://www.nuget.org/packages/Lucene.Net.Contrib.Spatial.NTS/3.0.3) - Geospatial search with support for NetTopologySuite.\n\n### Lucene.NET 4.8.0\n\n##### Core Library\n\n[![NuGet version](https://img.shields.io/nuget/vpre/Lucene.Net.svg)](https://www.nuget.org/packages/Lucene.Net/)\n\n```\nPM> Install-Package Lucene.Net -Pre\n```\n\n##### All Packages\n\n<!--- TO BE ADDED WHEN RELEASED \n\n- [Lucene.Net.Analysis.Nori](https://www.nuget.org/packages/Lucene.Net.Analysis.Nori/) - Korean Morphological Analyzer\n\n-->\n\n- [Lucene.Net](https://www.nuget.org/packages/Lucene.Net/) - Core library\n- [Lucene.Net.Analysis.Common](https://www.nuget.org/packages/Lucene.Net.Analysis.Common/) - Analyzers for indexing content in different languages and domains\n- [Lucene.Net.Analysis.Kuromoji](https://www.nuget.org/packages/Lucene.Net.Analysis.Kuromoji/) - Japanese Morphological Analyzer\n- [Lucene.Net.Analysis.Morfologik](https://www.nuget.org/packages/Lucene.Net.Analysis.Morfologik/) - Analyzer for dictionary stemming, built-in Polish dictionary\n- [Lucene.Net.Analysis.OpenNLP](https://www.nuget.org/packages/Lucene.Net.Analysis.OpenNLP/) - OpenNLP Library Integration\n- [Lucene.Net.Analysis.Phonetic](https://www.nuget.org/packages/Lucene.Net.Analysis.Phonetic/) - Analyzer for indexing phonetic signatures (for sounds-alike search)\n- [Lucene.Net.Analysis.SmartCn](https://www.nuget.org/packages/Lucene.Net.Analysis.SmartCn/) - Analyzer for indexing Chinese\n- [Lucene.Net.Analysis.Stempel](https://www.nuget.org/packages/Lucene.Net.Analysis.Stempel/) - Analyzer for indexing Polish\n- [Lucene.Net.Benchmark](https://www.nuget.org/packages/Lucene.Net.Benchmark/) - System for benchmarking Lucene\n- [Lucene.Net.Classification](https://www.nuget.org/packages/Lucene.Net.Classification/) - Classification module for Lucene\n- [Lucene.Net.Codecs](https://www.nuget.org/packages/Lucene.Net.Codecs/) - Lucene codecs and postings formats\n- [Lucene.Net.Expressions](https://www.nuget.org/packages/Lucene.Net.Expressions/) - Dynamically computed values to sort/facet/search on based on a pluggable grammar\n- [Lucene.Net.Facet](https://www.nuget.org/packages/Lucene.Net.Facet/) - Faceted indexing and search capabilities\n- [Lucene.Net.Grouping](https://www.nuget.org/packages/Lucene.Net.Grouping/) - Collectors for grouping search results\n- [Lucene.Net.Highlighter](https://www.nuget.org/packages/Lucene.Net.Highlighter/) - Highlights search keywords in results\n- [Lucene.Net.ICU](https://www.nuget.org/packages/Lucene.Net.ICU/) - Specialized ICU (International Components for Unicode) Analyzers and Highlighters\n- [Lucene.Net.Join](https://www.nuget.org/packages/Lucene.Net.Join/) - Index-time and Query-time joins for normalized content\n- [Lucene.Net.Memory](https://www.nuget.org/packages/Lucene.Net.Memory/) - Single-document in-memory index implementation\n- [Lucene.Net.Misc](https://www.nuget.org/packages/Lucene.Net.Misc/) - Index tools and other miscellaneous code\n- [Lucene.Net.Queries](https://www.nuget.org/packages/Lucene.Net.Queries/) - Filters and Queries that add to core Lucene\n- [Lucene.Net.QueryParser](https://www.nuget.org/packages/Lucene.Net.QueryParser/) - Text to Query parsers and parsing framework\n- [Lucene.Net.Replicator](https://www.nuget.org/packages/Lucene.Net.Replicator/)  Files replication utility\n- [Lucene.Net.Sandbox](https://www.nuget.org/packages/Lucene.Net.Sandbox/) - Various third party contributions and new ideas\n- [Lucene.Net.Spatial](https://www.nuget.org/packages/Lucene.Net.Spatial/) - Geospatial search\n- [Lucene.Net.Suggest](https://www.nuget.org/packages/Lucene.Net.Suggest/) - Auto-suggest and Spell-checking support\n- [Lucene.Net.TestFramework](https://www.nuget.org/packages/Lucene.Net.TestFramework/) - Framework for testing Lucene-based applications\n\n## Documentation\n\nWe have preliminary documentation for Lucene.NET 4.8.0 [on the Lucene.NET Website](https://lucenenet.apache.org/).\n\nThe API is similar to Java [Lucene 4.8.0](https://lucene.apache.org/core/4_8_0/), which you may also find helpful to review.\n\n> NOTE: We are working on fixing issues with the documentation, but could use more help since it is a massive project. See [#206](https://github.com/apache/lucenenet/pull/206).\n\n### Legacy Versions\n\n- [Lucene.Net 3.0.3 API Documentation](http://incubator.apache.org/lucene.net/docs/3.0.3/Index.html)\n- [Lucene.Net 2.9.4 API Documentation](http://incubator.apache.org/lucene.net/docs/2.9.4/Index.html)\n\n## Demos & Tools\n\nThere are several demos implemented as simple console applications that can be copied and pasted into Visual Studio or compiled on the command line in the [Lucene.Net.Demo project](https://github.com/apache/lucenenet/tree/master/src/Lucene.Net.Demo).\n\nThere is also a dotnet command line tool available on NuGet. It contains all of the demos as well as tools maintaining your Lucene.NET index, featuring operations such as splitting, merging, listing segment info, fixing, deleting segments, upgrading, etc. Always be sure to back up your index before running any commands against it!\n\n- [Prerequisite: .NET 8.0 Runtime or Higher](https://dotnet.microsoft.com/en-us/download/dotnet)\n\n```\ndotnet tool install lucene-cli -g --version 4.8.0-beta00015\n```\n\n> NOTE: The version of the CLI you install should match the version of Lucene.NET you use.\n\nOnce installed, you can explore the commands and options that are available by entering the command `lucene`.\n\n[lucene-cli Documentation](https://github.com/apache/lucenenet/blob/master/src/dotnet/tools/lucene-cli/docs/index.md)\n\n## How to Contribute\n\nWe love getting contributions! Read our [Contribution Guide](https://github.com/apache/lucenenet/blob/master/CONTRIBUTING.md) or read on for ways that you can help.\n\n### Join Mailing Lists\n\n[How to Join Mailing Lists](https://cwiki.apache.org/confluence/display/LUCENENET/Mailing+Lists)\n\n### Ask a Question\n\nIf you have a general how-to question or need help from the Lucene.NET community, please subscribe to the `user` mailing list by sending an email to [user-subscribe@lucenenet.apache.org](mailto:user-subscribe@lucenenet.apache.org) and then follow the instructions to verify your email address. Note that you only need to subscribe once.\n\nAfter you have subscribed to the mailing list, email your message to [user@lucenenet.apache.org](mailto:user@lucenenet.apache.org).\n\nAlternatively, you can get help via [StackOverflow](https://stackoverflow.com/questions/tagged/lucene.net)'s active community.\n\nPlease do not submit general how-to questions to GitHub, use GitHub for bug reports and tasks only.\n\n### Report a Bug\n\nTo report a bug, please use the [GitHub issue tracker](https://github.com/apache/lucenenet/issues).\n\n> **NOTE:** In the past, the Lucene.NET project used the [JIRA issue tracker](https://issues.apache.org/jira/projects/LUCENENET/issues), which has now been deprecated. However, we are keeping it active for tracking legacy issues. Please submit any new issues to GitHub.\n\n### Start a Discussion\n\nTo start a development discussion regarding the technical features of Lucene.NET, please email the `dev` mailing list by sending an email to [dev-subscribe@lucenenet.apache.org](mailto:dev-subscribe@lucenenet.apache.org) and then follow the instructions to verify your email address. Note that you only need to subscribe once.\n\nAfter you have subscribed to the mailing list, email your message to [dev@lucenenet.apache.org](mailto:dev@lucenenet.apache.org).\n\n### Submit a Pull Request\n\nBefore you start working on a pull request, please read our [Contributing](https://github.com/apache/lucenenet/blob/master/CONTRIBUTING.md) guide.\n\n## Building and Testing\n\n### Command Line\n\n##### Prerequisites\n\n1. [PowerShell](https://docs.microsoft.com/en-us/powershell/scripting/install/installing-powershell) 5.0 or higher (see [this question](http://stackoverflow.com/questions/1825585/determine-installed-powershell-version) to check your PowerShell version)\n2. [.NET 8.0 SDK or higher](https://dotnet.microsoft.com/download/visual-studio-sdks)\n\n##### Execution\n\n> **NOTE:** If the project is open in Visual Studio, its background restore may interfere with these commands. It is recommended to close all instances of Visual Studio that have `Lucene.Net.sln` open before executing.\n\nTo build the source, clone or download and unzip the repository. For specific releases, download and unzip the `.src.zip` file from the [download page of the specific version](https://lucenenet.apache.org/download/download.html). From the repository or distribution root, execute the **build** command from a command prompt and include the desired options from the build options table below:\n\n###### Windows\n\n```\n> build [options]\n```\n\n###### Linux or macOS\n\n```\n./build [options]\n```\n\n> **NOTE:** The `build` file will need to be given permission to run using the command `chmod u+x build` before the first execution.\n\n##### Build Options\n\nThe following options are case-insensitive. Each option has both a short form indicated by a single `-` and a long-form indicated by `--`. The options that require a value must be followed by a space and then the value, similar to running the [dotnet CLI](https://docs.microsoft.com/en-us/dotnet/core/tools/).\n\n<table>\n    <tr>\n        <th>Short</th>\n        <th>Long</th>\n        <th>Description</th>\n        <th>Example</th>\n    </tr>\n    <tr>\n        <td>&#8209;config</td>\n        <td>&#8209;&#8209;configuration</td>\n        <td>The build configuration (\"Release\" or \"Debug\").</td>\n        <td>build&nbsp;&#8209;&#8209;configuration Debug</td>\n    </tr>\n    <tr>\n        <td>&#8209;mp</td>\n        <td>&#8209;&#8209;maximum-parallel-jobs</td>\n        <td>The maximum number of parallel jobs to run during testing. If not supplied, the default is 8.</td>\n        <td>build&nbsp;&#8209;t&nbsp;&#8209;mp 10</td>\n    </tr>\n    <tr>\n        <td>&#8209;pv</td>\n        <td>&#8209;&#8209;package-version</td>\n        <td>The NuGet package version. If not supplied, will use the version from the Version.proj file.</td>\n        <td>build&nbsp;&#8209;pv 4.8.0&#8209;beta00001</td>\n    </tr>\n    <tr>\n        <td>&#8209;t</td>\n        <td>&#8209;&#8209;test</td>\n        <td>Runs the tests after building. This option does not require a value. Note that testing typically takes around 40 minutes with 8 parallel jobs.</td>\n        <td>build&nbsp;&#8209;t</td>\n    </tr>\n    <tr>\n        <td>&#8209;fv</td>\n        <td>&#8209;&#8209;file-version</td>\n        <td>The assembly file version. If not supplied, defaults to the --package-version value (excluding any pre-release label). The assembly version will be derived from the major version component of the passed in value, excluding the minor, build and revision components.</td>\n        <td>build&nbsp;&#8209;pv 4.8.0&#8209;beta00001&nbsp;&#8209;fv 4.8.0</td>\n    </tr>\n</table>\n\nFor example, the following command creates a Release build with NuGet package version 4.8.0‑ci00015 and file version 4.8.0. The assembly version will be derived from the major version component of the passed in value, excluding the minor, build and revision components (in this case 4.0.0).\n\n###### Windows\n\n```\n> build ‑‑configuration Release ‑pv 4.8.0‑ci00015 ‑fv 4.8.0\n```\n\n###### Linux or macOS\n\n```\n./build ‑‑configuration Release ‑pv 4.8.0‑ci00015 ‑fv 4.8.0\n```\n\nIn the above example, we are using \"ci\" in the package version to indicate this is not a publicly released beta version but rather the output of a continuous integration build from master which occurred after beta00014 but before beta00015 was released.\n\nNuGet packages are output by the build to the `/_artifacts/NuGetPackages/` directory. Test results (if applicable) are output to the `/_artifacts/TestResults/` directory.\n\nYou can setup Visual Studio to read the NuGet packages like any NuGet feed by following these steps:\n\n1. In Visual Studio, right-click the solution in Solution Explorer, and choose \"Manage NuGet Packages for Solution\"\n2. Click the gear icon next to the Package sources dropdown.\n3. Click the `+` icon (for add)\n4. Give the source a name such as `Lucene.Net Local Packages`\n5. Click the `...` button next to the Source field, and choose the `/src/_artifacts/NuGetPackages` folder on your local system.\n6. Click Ok\n\nThen all you need to do is choose the `Lucene.Net Local Packages` feed from the dropdown (in the NuGet Package Manager) and you can search for, install, and update the NuGet packages just as you can with any Internet-based feed.\n\n### Visual Studio\n\n#### Prerequisites\n\n1. Visual Studio 2022 or higher\n2. [.NET 8.0 SDK or higher](https://dotnet.microsoft.com/download/visual-studio-sdks)\n\n#### Execution\n\n1. Open `Lucene.Net.sln` in Visual Studio.\n2. Choose the target framework to test by opening `.build/TestTargetFramework.props` and uncommenting the corresponding `<TargetFramework>` (and commenting all others).\n3. Build a project or the entire solution, and wait for Visual Studio to discover the tests - this may take several minutes.\n4. Run or debug the tests in Test Explorer, optionally using the desired filters.\n\n> **NOTE:** When running tests in Visual Studio, be sure to [set the default processor architecture to 64 bit](https://stackoverflow.com/a/45946727) to avoid running out of virtual memory on some tests.\n\n### Azure DevOps\n\nWe have setup our `azure-pipelines.yml` file with logical defaults so anyone with an Azure DevOps account can build Lucene.NET and run the tests with minimal effort. Even a free Azure DevOps account will work, but tests will run much faster if the account is setup as public, which enables up to 10 parallel jobs to run simultaneously.\n\n#### Prerequisites\n\n1. An [Azure DevOps](https://azure.microsoft.com/en-us/services/devops/) account.\n2. A fork of this repository either on GitHub or Azure DevOps. The rest of these instructions assume a [GitHub fork](https://help.github.com/en/github/getting-started-with-github/fork-a-repo).\n\n#### Execution\n\n##### If you don't already have a pipeline set up:\n\n1. [Create an Azure DevOps organization](https://docs.microsoft.com/en-us/azure/devops/organizations/accounts/create-organization?view=azure-devops). If you already have one that you wish to use, you may skip this step.\n2. [Create an Azure DevOps project](https://docs.microsoft.com/en-us/azure/devops/organizations/projects/create-project?view=azure-devops&tabs=preview-page). We recommend naming the project Lucene.NET. Note that if you are using a free Azure DevOps account, you should choose to make the project public in order to enable 10 parallel jobs. If you make the project private, you will only get 1 parallel job. Also, if disabling features, make sure to leave Pipelines enabled.\n3. Create an Azure DevOps pipeline.\n   - Click on \"Pipelines\" from the left menu.\n   - Click the \"Create Pipeline\" or \"New Pipeline\" button, depending on whether any pipelines already exist.\n   - Select GitHub as the location to find the YAML file.\n   - Select the fork of this repository you created in \"Prerequisites\". Note that if this is a new Azure DevOps account you may need to [setup extra permissions to access your GitHub account](https://docs.microsoft.com/en-us/azure/devops/pipelines/repos/github?view=azure-devops&tabs=yaml).\n   - Next a \"Review your YAML\" page is presented showing the contents of `azure-pipelines.yml`. There is documentation near the top of the file indicating the variables that can be setup to enable additional options, but note that the default configuration will automatically run the build and all of the tests.\n   - Click the \"Run\" button at the top right of the page.\n\n##### If you already have a pipeline set up:\n\n1. Click on \"Pipelines\" from the left menu.\n2. Select the pipeline you wish to run.\n3. Click the \"Queue\" button on the upper right.\n4. (Optional) Select the branch and override any variables in the pipeline for this run.\n5. Click the \"Run\" button.\n\nNote that after the build is complete, the `nuget` artifact contains `.nupkg` files which may be downloaded to your local machine where you can [setup a local folder to act as a NuGet feed](https://docs.microsoft.com/en-us/nuget/hosting-packages/local-feeds).\n\n> It is also possible to add an Azure DevOps feed id to a new variable named `ArtifactFeedID`, but we are getting mixed results due to permission issues.\n"
        },
        {
          "name": "TestTargetFramework.props",
          "type": "blob",
          "size": 4.6103515625,
          "content": "﻿<?xml version=\"1.0\" encoding=\"utf-8\"?>\n<!--\n\n Licensed to the Apache Software Foundation (ASF) under one\n or more contributor license agreements.  See the NOTICE file\n distributed with this work for additional information\n regarding copyright ownership.  The ASF licenses this file\n to you under the Apache License, Version 2.0 (the\n \"License\"); you may not use this file except in compliance\n with the License.  You may obtain a copy of the License at\n\n   http://www.apache.org/licenses/LICENSE-2.0\n\n Unless required by applicable law or agreed to in writing,\n software distributed under the License is distributed on an\n \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n KIND, either express or implied.  See the License for the\n specific language governing permissions and limitations\n under the License.\n\n-->\n\n<Project>\n\n  <PropertyGroup>\n    <!-- Changing this setting will allow testing on all target frameworks within Visual Studio.\n    Note that the main libraries are multi-targeted, so this has no effect on how they are compiled,\n    this setting only affects the test projects. -->\n    <!--<TargetFramework>net472</TargetFramework>-->\n    <!--<TargetFramework>net48</TargetFramework>-->\n    <!--<TargetFramework>net6.0</TargetFramework>-->\n    <!--<TargetFramework>net8.0</TargetFramework>-->\n    <TargetFramework>net9.0</TargetFramework>\n\n    <!-- Allow the build script to pass in the test frameworks to build for.\n      This overrides the above TargetFramework setting.\n      LUCENENET TODO: Due to a parsing bug, we cannot pass a string with a ; to dotnet msbuild, so passing true as a workaround -->\n\n    <!-- Test Client to DLL target works as follows:\n      Test Client       | Target Under Test\n      net9.0            | net8.0\n      net8.0            | net8.0\n      net6.0            | netstandard2.1\n      net48             | net462\n      net472            | netstandard2.0\n    -->\n\n    <TargetFrameworks Condition=\" '$(TestFrameworks)' == 'true' \">net9.0;net8.0;net6.0</TargetFrameworks>\n    <TargetFrameworks Condition=\" '$(TestFrameworks)' == 'true' AND $([MSBuild]::IsOsPlatform('Windows')) \">$(TargetFrameworks);net48;net472</TargetFrameworks>\n    <TargetFramework Condition=\" '$(TargetFrameworks)' != '' \"></TargetFramework>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"Mismatched Target Framework (to override the target framework under test)\">\n    <SetTargetFramework></SetTargetFramework>\n    <SetTargetFramework Condition=\" '$(TargetFramework)' == 'net472' \">TargetFramework=netstandard2.0</SetTargetFramework>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"Assembly Publishing\">\n    <IsPublishable>true</IsPublishable>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"Test Settings\">\n    <IsTestProject>true</IsTestProject>\n  </PropertyGroup>\n\n  <PropertyGroup Label=\"Warnings to be Disabled in Test Projects\">\n\n    <!-- We purposely test on EoL frameworks for testing netstandard2.1, but we want to keep this warning in production code. -->\n    <CheckEolTargetFramework Condition=\" '$(TargetFramework)' == 'net5.0' \">false</CheckEolTargetFramework>\n    <SuppressTfmSupportBuildWarnings Condition=\" '$(TargetFramework)' == 'net5.0' \">true</SuppressTfmSupportBuildWarnings>\n\n    <NoWarn Label=\"Nested types should not be visible\">$(NoWarn);CA1034</NoWarn>\n    <NoWarn Label=\"Use Literals Where Appropriate\">$(NoWarn);CA1802</NoWarn>\n    <NoWarn Label=\"Do not ignore method results\">$(NoWarn);CA1806</NoWarn>\n    <NoWarn Label=\"Add readonly modifier\">$(NoWarn);CA1822</NoWarn>\n    <NoWarn Label=\"Avoid zero-length array allocations\">$(NoWarn);CA1825</NoWarn>\n    <NoWarn Label=\"Do not raise exceptions in exception clauses\">$(NoWarn);CA2219</NoWarn>\n\n    <NoWarn Label=\"Use object initializers\">$(NoWarn);IDE0017</NoWarn>\n    <NoWarn Label=\"Use pattern matching\">$(NoWarn);IDE0019;IDE0020;IDE0038</NoWarn>\n    <NoWarn Label=\"Use collection initializers\">$(NoWarn);IDE0028</NoWarn>\n    <NoWarn Label=\"Use null propagation\">$(NoWarn);IDE0031</NoWarn>\n    <NoWarn Label=\"Add accessibility modifiers\">$(NoWarn);IDE0040</NoWarn>\n    <NoWarn Label=\"Add readonly modifier\">$(NoWarn);IDE0044</NoWarn>\n    <NoWarn Label=\"Use language keywords\">$(NoWarn);IDE0049</NoWarn>\n    <NoWarn Label=\"Remove unused private member\">$(NoWarn);IDE0051</NoWarn>\n    <NoWarn Label=\"Remove unread private member\">$(NoWarn);IDE0052</NoWarn>\n    <NoWarn Label=\"Remove unnecessary value assignment\">$(NoWarn);IDE0059</NoWarn>\n    <NoWarn Label=\"Remove unused parameter\">$(NoWarn);IDE0060</NoWarn>\n    <NoWarn Label=\"Naming rule violation\">$(NoWarn);IDE1006</NoWarn>\n\n    <!-- SonarCloud issues -->\n    <NoWarn Label=\"Add at least one assertion to this test case\">$(NoWarn);S2699</NoWarn>\n  </PropertyGroup>\n\n</Project>\n"
        },
        {
          "name": "azure-pipelines.yml",
          "type": "blob",
          "size": 33.0263671875,
          "content": "﻿# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n\nname: 'vNext$(rev:.r)' # Format for build number (will be overridden)\n\n# DevOps Setup: Define the following pipeline level variables in Azure DevOps build pipeline\n\n# ArtifactFeedID: (Optional - set to your Azure DevOps Artifact (NuGet) feed. If not provided, publish job will be skipped.)\n# BuildConfiguration: (Optional. Defaults to 'Release')\n# BuildPlatform: (Optional. Defaults to 'Any CPU')\n# GenerateDocs: (Optional. Only builds documentation website if set to 'true'.)\n# GenerateWebsite: (Optional. Only builds lucene.net website if set to 'true'.)\n# IsRelease: (Optional. By default the Release job is disabled, setting this to 'true' will enable it)\n# GeneratePackages: (Optional. Defaults to 'true'. Setting to 'false' will cut about 1 minute from the build time by skipping packing the NuGet files and subsequent artiact upload. If ArtifactFeedID is provided or IsRelease is 'true', this setting has no effect.)\n\n# Versioning Variables\n\n# BuildCounterSeed: (Optional - Set in conjunction with VersionSuffix, will cause the build counter to begin at this value. Note that it is set once, to reset is an API call.)\n# PackageVersion: (Optional - This can be used to explicitly set the whole version number to a specific version, i.e. 4.8.0-beta00005. It overrides all other version settings.)\n# PreReleaseCounterPattern: (Optional. Set to '0000000000' in ci pipeline or '00000' in release pipeline. The default is '0000000000'. This setting has no effect if VersionSuffix is ''.)\n# VersionSuffix: (Optional. Defaults to 'ci'. Set to 'beta' or 'rc' or '' in production pipeline.)\n\n# Testing variables\n\n# RunTests: 'true' (Optional - set to 'false' to disable test jobs - useful for debugging. If not provided, tests will be run.)\n# AssertsEnabled: 'true' (Optional - set to 'false' to run tests without asserts, which is less thorough. This can speed up testing and verify the application will run without asserts.)\n# IsNightly: 'false' (Optional - set to 'true' to run additional tests for the nightly build)\n# IsWeekly: 'false' (Optional - set to 'true' to run additional tests for the weekly build)\n# RunSlowTests: 'true' (Optional - set to 'false' to skip slow tests to make testing time shorter)\n# RunAwaitsFixTests: 'true' (Optional - set to 'false' to disable running flakey tests)\n# Codec: 'random' (Optional - set to a specific codec to test the same codec throughout all tests)\n# DocValuesFormat: 'random' (Optional - set to a specific doc values format to test the same codec throughout all tests)\n# PostingsFormat: 'random' (Optional - set to a specific postings format to test the same codec throughout all tests)\n# Directory: 'random' (Optional - set to a specific directory implementation to test the same codec throughout all tests)\n# Verbose: 'false' (Optional - set to true for verbose logging output)\n# Multiplier: '1' (Optional - the number of iterations to multiply applicable tests by)\n# DisplayFullName: 'true' (Optional - set to 'false' to display only the test name instead of the full name with class and method)\n# FailOnTestFixtureOneTimeSetUpError: 'true' (Optional - set to 'false' to allow tests to pass if the test fixture (class) has a OneTimeSetUp failure.)\n\n# RunX86Tests: 'false' (Optional - set to 'true' to enable x86 tests)\n\nvariables:\n- name: BuildCounter\n  value: $[counter(variables['VersionSuffix'],coalesce(variables['BuildCounterSeed'], 1250))]\n- name: DotNetSDKVersion\n  value: '9.0.100'\n- name: DocumentationArtifactName\n  value: 'docs'\n- name: DocumentationArtifactZipFileName\n  value: 'documentation.zip'\n- name: WebsiteArtifactName\n  value: 'website'\n- name: WebsiteArtifactZipFileName\n  value: 'website.zip'\n- name: BinaryArtifactName\n  value: 'testbinaries'\n- name: NuGetArtifactName\n  value: 'nuget'\n- name: DebugArtifactName # For .pdb symbols\n  value: 'debug'\n- name: ReleaseArtifactName\n  value: 'release'\n- name: TestResultsArtifactName\n  value: 'testresults'\n- name: VersionArtifactName\n  value: 'version'\n- name: BuildNumberFileName\n  value: 'buildNumber.txt'\n- name: PackageVersionFileName\n  value: 'packageVersion.txt'\n- name: FileVersionFileName\n  value: 'fileVersion.txt'\n- name: TestSettingsFileName\n  value: 'lucene.testsettings.json'\n- name: BuildDirectory # Where the build scripts and configs are\n  value: '$(System.DefaultWorkingDirectory)/.build'\n- name: PublishDirectory # Test binaries directory\n  value: '$(Build.ArtifactStagingDirectory)/$(BinaryArtifactName)'\n- name: NuGetArtifactDirectory # NuGet binaries directory\n  value: '$(Build.ArtifactStagingDirectory)/$(NuGetArtifactName)'\n\n\nstages:\n- stage: Build_Stage\n  displayName: 'Build Stage:'\n  jobs:\n\n  - job: Build\n    pool:\n      vmImage: 'windows-latest'\n\n    steps:\n\n    - checkout: self # self represents the repo where the initial Pipelines YAML file was found\n      fetchDepth: '1'  # the depth of commits to ask Git to fetch\n\n    - pwsh: |\n        $configuration = if ($env:BUILDCONFIGURATION) { $env:BUILDCONFIGURATION } else { \"Release\" }\n        Write-Host \"##vso[task.setvariable variable=BuildConfiguration;]$configuration\"\n        $platform = if ($env:BUILDPLATFORM) { $env:BUILDPLATFORM } else { \"Any CPU\" }\n        Write-Host \"##vso[task.setvariable variable=BuildPlatform;]$platform\"\n        $isRelease = if ($env:ISRELEASE -eq 'true') { 'true' } else { 'false' }\n        Write-Host \"##vso[task.setvariable variable=IsRelease;]$isRelease\"\n        $isNightly = if ($env:ISNIGHTLY -eq 'true') { 'true' } else { 'false' }\n        Write-Host \"##vso[task.setvariable variable=IsNightly;]$isNightly\"\n        $isWeekly = if ($env:ISWEEKLY -eq 'true') { 'true' } else { 'false' }\n        Write-Host \"##vso[task.setvariable variable=IsWeekly;]$isWeekly\"\n        $runPack = if ($env:ISRELEASE -eq 'true' -or $env:ARTIFACTFEEDID -ne '' -or $env:GENERATEPACKAGES -ne 'false') { 'true' } else { 'false' }\n        Write-Host \"##vso[task.setvariable variable=RunPack;]$runPack\"\n      displayName: 'Setup Default Variable Values'\n\n    - template: '.build/azure-templates/install-dotnet-sdk.yml'\n      parameters:\n        sdkVersion: '$(DotNetSDKVersion)'\n\n    - pwsh: |\n        Import-Module \"$(BuildDirectory)/psake/psake.psm1\"\n        $primaryCommand = if ($env:RUNPACK -ne 'false') { 'Pack' } else { 'Compile' }\n        $parameters = @{}\n        $properties = @{\n            backupFiles='false';\n            nugetPackageDirectory='$(NuGetArtifactDirectory)'\n        }\n        [string[]]$tasks = @($primaryCommand)\n        Invoke-Psake $(BuildDirectory)/runbuild.ps1 -Task $tasks -properties $properties -parameters $parameters\n        exit !($psake.build_success)\n      displayName: 'PSake Build and Pack'\n\n    #- template: '.build/azure-templates/show-all-environment-variables.yml' # Uncomment for debugging\n\n    - pwsh: |\n        $dir = '$(Build.ArtifactStagingDirectory)/$(VersionArtifactName)'\n        if (!(Test-Path $dir)) { New-Item -ItemType Directory -Path \"$dir\" -Force }\n        '$(PackageVersion)' | Out-File -FilePath \"$dir/$(PackageVersionFileName)\" -Force\n        '$(FileVersion)' | Out-File -FilePath \"$dir/$(FileVersionFileName)\" -Force\n        '$(Build.BuildNumber)' | Out-File -FilePath \"$dir/$(BuildNumberFileName)\" -Force\n      displayName: 'Persist Versions to Files'\n\n    - pwsh: |\n        # Generate a lucene.testsettings.json file for use with the test framework\n        $assert = if ($env:AssertsEnabled -ne 'false') { 'true' } else { 'false' }\n        $nightly = if ($env:IsNightly -eq 'true') { 'true' } else { 'false' }\n        $weekly = if ($env:IsWeekly -eq 'true') { 'true' } else { 'false' }\n        $slow = if ($env:RunSlowTests -ne 'false') { 'true' } else { 'false' }\n        $awaitsFix = if ($env:RunAwaitsFixTests -ne 'false') { 'true' } else { 'false' }\n        $codec = if ($env:Codec -eq $null) { 'random' } else { $env:Codec }\n        $docValuesFormat = if ($env:DocValuesFormat -eq $null) { 'random' } else { $env:DocValuesFormat }\n        $postingsFormat = if ($env:PostingsFormat -eq $null) { 'random' } else { $env:PostingsFormat }\n        $directory = if ($env:Directory -eq $null) { 'random' } else { $env:Directory }\n        $verbose = if ($env:Verbose -eq 'true') { 'true' } else { 'false' }\n        $multiplier = if ($env:Multiplier -eq $null) { '1' } else { $env:Multiplier }\n        $failOnTestFixtureOneTimeSetUpError = if ($env.FailOnTestFixtureOneTimeSetUpError -eq 'false') { 'false' } else { 'true' }\n        $fileText = \"{`n`t\" +\n            \"\"\"assert\"\": \"\"$assert\"\",`n`t\" +\n            \"\"\"tests\"\": {`n`t`t\" +\n                \"\"\"nightly\"\": \"\"$nightly\"\",`n`t`t\" +\n                \"\"\"weekly\"\": \"\"$weekly\"\",`n`t`t\" +\n                \"\"\"slow\"\": \"\"$slow\"\",`n`t`t\" +\n                \"\"\"awaitsfix\"\": \"\"$awaitsFix\"\",`n`t`t\" +\n                \"\"\"codec\"\": \"\"$codec\"\",`n`t`t\" +\n                \"\"\"docvaluesformat\"\": \"\"$docValuesFormat\"\",`n`t`t\" +\n                \"\"\"postingsformat\"\": \"\"$postingsFormat\"\",`n`t`t\" +\n                \"\"\"directory\"\": \"\"$directory\"\",`n`t`t\" +\n                \"\"\"verbose\"\": \"\"$verbose\"\",`n`t`t\" +\n                \"\"\"multiplier\"\": \"\"$multiplier\"\",`n`t`t\" +\n                \"\"\"failontestfixtureonetimesetuperror\"\": \"\"$failOnTestFixtureOneTimeSetUpError\"\"`n`t\" +\n            \"}`n\" +\n        \"}\"\n        Out-File -filePath \"$(Build.ArtifactStagingDirectory)/$(TestSettingsFileName)\" -encoding UTF8 -inputObject $fileText\n      displayName: 'Persist Test Settings to lucene.testsettings.json'\n      condition: and(succeeded(), ne(variables['RunTests'], 'false'))\n\n    # If this is a release pipeline, copy the version.props files as a version artifact, which will\n    # be included in the release.\n    - task: CopyFiles@2\n      displayName: 'Copy version.props Files to: /$(VersionArtifactName)'\n      inputs:\n        SourceFolder: '$(System.DefaultWorkingDirectory)'\n        Contents: |\n          version.props\n        TargetFolder: '$(Build.ArtifactStagingDirectory)/$(VersionArtifactName)'\n      condition: and(succeeded(), eq(variables['IsRelease'], 'true'))\n\n    - task: PublishPipelineArtifact@1\n      displayName: 'Publish Artifact: $(VersionArtifactName)'\n      inputs:\n        targetPath: '$(Build.ArtifactStagingDirectory)/$(VersionArtifactName)'\n        artifact: '$(VersionArtifactName)'\n        publishLocation: 'pipeline'\n\n    # Copy the .pdb files as build artifacts, which will\n    # later be used to push to the Azure Artifacts symbol server.\n    - task: CopyFiles@2\n      displayName: 'Copy .pdb Files to: /$(DebugArtifactName)'\n      inputs:\n        SourceFolder: '$(System.DefaultWorkingDirectory)'\n        Contents: '**/bin/$(BuildConfiguration)/**/*.pdb'\n        TargetFolder: '$(Build.ArtifactStagingDirectory)/$(DebugArtifactName)'\n      condition: and(succeeded(), ne(variables['ArtifactFeedID'], ''))\n\n    - task: PublishPipelineArtifact@1\n      displayName: 'Publish Artifact: $(DebugArtifactName)'\n      inputs:\n        targetPath: '$(Build.ArtifactStagingDirectory)/$(DebugArtifactName)'\n        artifact: '$(DebugArtifactName)'\n        publishLocation: 'pipeline'\n      condition: and(succeeded(), ne(variables['ArtifactFeedID'], ''))\n\n    - task: PublishPipelineArtifact@1\n      displayName: 'Publish Artifact: $(NuGetArtifactName)'\n      inputs:\n        targetPath: '$(NuGetArtifactDirectory)'\n        artifact: '$(NuGetArtifactName)'\n        publishLocation: 'pipeline'\n      condition: and(succeeded(), ne(variables['RunPack'], 'false'))\n\n    - pwsh: |\n        Remove-Item -Path \"$(NuGetArtifactDirectory)/*\" -Recurse -Force\n      displayName: 'Delete temp publish location: $(NuGetArtifactDirectory)'\n      condition: and(succeeded(), ne(variables['RunPack'], 'false'))\n\n    - template: '.build/azure-templates/publish-test-binaries.yml'\n      parameters:\n        publishDirectory: $(PublishDirectory)\n        framework: 'net9.0'\n        binaryArtifactName: '$(BinaryArtifactName)'\n        testSettingsFilePath: '$(Build.ArtifactStagingDirectory)/$(TestSettingsFileName)'\n        configuration: '$(BuildConfiguration)'\n        platform: '$(BuildPlatform)'\n\n    - template: '.build/azure-templates/publish-test-binaries.yml'\n      parameters:\n        publishDirectory: $(PublishDirectory)\n        framework: 'net8.0'\n        binaryArtifactName: '$(BinaryArtifactName)'\n        testSettingsFilePath: '$(Build.ArtifactStagingDirectory)/$(TestSettingsFileName)'\n        configuration: '$(BuildConfiguration)'\n        platform: '$(BuildPlatform)'\n\n    - template: '.build/azure-templates/publish-test-binaries.yml'\n      parameters:\n        publishDirectory: $(PublishDirectory)\n        framework: 'net6.0'\n        binaryArtifactName: '$(BinaryArtifactName)'\n        testSettingsFilePath: '$(Build.ArtifactStagingDirectory)/$(TestSettingsFileName)'\n        configuration: '$(BuildConfiguration)'\n        platform: '$(BuildPlatform)'\n\n    - template: '.build/azure-templates/publish-test-binaries.yml'\n      parameters:\n        publishDirectory: $(PublishDirectory)\n        framework: 'net472'\n        binaryArtifactName: '$(BinaryArtifactName)'\n        testSettingsFilePath: '$(Build.ArtifactStagingDirectory)/$(TestSettingsFileName)'\n        configuration: '$(BuildConfiguration)'\n        platform: '$(BuildPlatform)'\n\n    - template: '.build/azure-templates/publish-test-binaries.yml'\n      parameters:\n        publishDirectory: $(PublishDirectory)\n        framework: 'net48'\n        binaryArtifactName: '$(BinaryArtifactName)'\n        testSettingsFilePath: '$(Build.ArtifactStagingDirectory)/$(TestSettingsFileName)'\n        configuration: '$(BuildConfiguration)'\n        platform: '$(BuildPlatform)'\n\n  - job: Docs\n    condition: and(succeeded(), eq(variables['GenerateDocs'], 'true'))\n    pool:\n      vmImage: 'windows-latest'\n\n    steps:\n    - template: '.build/azure-templates/install-dotnet-sdk.yml'\n      parameters:\n        sdkVersion: '$(DotNetSDKVersion)'\n\n    - pwsh: |\n         $(Build.SourcesDirectory)/websites/apidocs/docs.ps1 -LuceneNetVersion $(PackageVersion) -Clean -BaseUrl https://lucenenet.apache.org/docs/\n      errorActionPreference: 'continue'\n      ignoreLASTEXITCODE: 'true'\n      failOnStderr: 'false'\n      displayName: 'Generate Documentation'\n\n    - task: ArchiveFiles@2\n      displayName: 'Zip Documenation Files'\n      inputs:\n        rootFolderOrFile: '$(Build.SourcesDirectory)/websites/apidocs/_site'\n        includeRootFolder: false\n        archiveFile: '$(Build.ArtifactStagingDirectory)/$(DocumentationArtifactName)/$(DocumentationArtifactZipFileName)'\n\n    - task: PublishBuildArtifacts@1\n      displayName: 'Publish Artifact: $(DocumentationArtifactName)'\n      inputs:\n        PathtoPublish: '$(Build.ArtifactStagingDirectory)/$(DocumentationArtifactName)'\n        ArtifactName: '$(DocumentationArtifactName)'\n\n  - job: Website\n    condition: and(succeeded(), eq(variables['GenerateWebsite'], 'true'))\n    pool:\n      vmImage: 'windows-latest'\n\n    steps:\n    - pwsh: |\n         $(Build.SourcesDirectory)/websites/site/site.ps1 0 1\n      errorActionPreference: 'continue'\n      ignoreLASTEXITCODE: 'true'\n      failOnStderr: 'false'\n      displayName: 'Generate Website'\n\n    - task: ArchiveFiles@2\n      displayName: 'Zip Website Files'\n      inputs:\n        rootFolderOrFile: '$(Build.SourcesDirectory)/websites/site/_site'\n        includeRootFolder: false\n        archiveFile: '$(Build.ArtifactStagingDirectory)/$(WebsiteArtifactName)/$(WebsiteArtifactZipFileName)'\n\n    - task: PublishBuildArtifacts@1\n      displayName: 'Publish Artifact: $(WebsiteArtifactName)'\n      inputs:\n        PathtoPublish: '$(Build.ArtifactStagingDirectory)/$(WebsiteArtifactName)'\n        ArtifactName: '$(WebsiteArtifactName)'\n\n\n- stage: Test_Stage\n  displayName: 'Test Stage:'\n  jobs:\n\n  - job: Test_net9_0_x64\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'))\n    strategy:\n      matrix:\n        Windows:\n          osName: 'Windows'\n          imageName: 'windows-latest'\n          maximumParallelJobs: 8\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n        Linux:\n          osName: 'Linux'\n          imageName: 'ubuntu-latest'\n          maximumParallelJobs: 7\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n        macOS:\n          osName: 'macOS'\n          imageName: 'macOS-latest'\n          maximumParallelJobs: 7\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n    displayName: 'Test net9.0,x64 on'\n    pool:\n      vmImage: $(imageName)\n    steps:\n      - template: '.build/azure-templates/run-tests-on-os.yml'\n        parameters:\n          osName: $(osName)\n          framework: 'net9.0'\n          vsTestPlatform: 'x64'\n          testBinariesArtifactName: '$(TestBinariesArtifactName)'\n          nugetArtifactName: '$(NuGetArtifactName)'\n          testResultsArtifactName: '$(TestResultsArtifactName)'\n          maximumParallelJobs: $(maximumParallelJobs)\n          maximumAllowedFailures: $(maximumAllowedFailures)\n          dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net9_0_x86 # Only run Nightly or if explicitly enabled with RunX86Tests\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'), or(eq(variables['IsNightly'], 'true'), eq(variables['RunX86Tests'], 'true')))\n    strategy:\n      matrix:\n        Windows:\n          osName: 'Windows'\n          imageName: 'windows-latest'\n          maximumParallelJobs: 8\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n    displayName: 'Test net9.0,x86 on'\n    pool:\n      vmImage: $(imageName)\n    steps:\n      - template: '.build/azure-templates/run-tests-on-os.yml'\n        parameters:\n          osName: $(osName)\n          framework: 'net9.0'\n          vsTestPlatform: 'x86'\n          testBinariesArtifactName: '$(TestBinariesArtifactName)'\n          nugetArtifactName: '$(NuGetArtifactName)'\n          testResultsArtifactName: '$(TestResultsArtifactName)'\n          maximumParallelJobs: $(maximumParallelJobs)\n          maximumAllowedFailures: $(maximumAllowedFailures)\n          dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net8_0_x64\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'))\n    strategy:\n      matrix:\n        Windows:\n          osName: 'Windows'\n          imageName: 'windows-latest'\n          maximumParallelJobs: 8\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n        Linux:\n          osName: 'Linux'\n          imageName: 'ubuntu-latest'\n          maximumParallelJobs: 7\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n        macOS:\n          osName: 'macOS'\n          imageName: 'macOS-latest'\n          maximumParallelJobs: 7\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n    displayName: 'Test net8.0,x64 on'\n    pool:\n      vmImage: $(imageName)\n    steps:\n    - template: '.build/azure-templates/run-tests-on-os.yml'\n      parameters:\n        osName: $(osName)\n        framework: 'net8.0'\n        vsTestPlatform: 'x64'\n        testBinariesArtifactName: '$(TestBinariesArtifactName)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        testResultsArtifactName: '$(TestResultsArtifactName)'\n        maximumParallelJobs: $(maximumParallelJobs)\n        maximumAllowedFailures: $(maximumAllowedFailures)\n        dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net8_0_x86 # Only run Nightly or if explicitly enabled with RunX86Tests\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'), or(eq(variables['IsNightly'], 'true'), eq(variables['RunX86Tests'], 'true')))\n    strategy:\n      matrix:\n        Windows:\n          osName: 'Windows'\n          imageName: 'windows-latest'\n          maximumParallelJobs: 8\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n    displayName: 'Test net8.0,x86 on'\n    pool:\n      vmImage: $(imageName)\n    steps:\n    - template: '.build/azure-templates/run-tests-on-os.yml'\n      parameters:\n        osName: $(osName)\n        framework: 'net8.0'\n        vsTestPlatform: 'x86'\n        testBinariesArtifactName: '$(TestBinariesArtifactName)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        testResultsArtifactName: '$(TestResultsArtifactName)'\n        maximumParallelJobs: $(maximumParallelJobs)\n        maximumAllowedFailures: $(maximumAllowedFailures)\n        dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net6_0_x64\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'))\n    strategy:\n      matrix:\n        Windows:\n          osName: 'Windows'\n          imageName: 'windows-latest'\n          maximumParallelJobs: 8\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n        Linux:\n          osName: 'Linux'\n          imageName: 'ubuntu-latest'\n          maximumParallelJobs: 7\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n        macOS:\n          osName: 'macOS'\n          imageName: 'macOS-latest'\n          maximumParallelJobs: 7\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n    displayName: 'Test net6.0,x64 on'\n    pool:\n      vmImage: $(imageName)\n    steps:\n    - template: '.build/azure-templates/run-tests-on-os.yml'\n      parameters:\n        osName: $(osName)\n        framework: 'net6.0'\n        vsTestPlatform: 'x64'\n        testBinariesArtifactName: '$(TestBinariesArtifactName)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        testResultsArtifactName: '$(TestResultsArtifactName)'\n        maximumParallelJobs: $(maximumParallelJobs)\n        maximumAllowedFailures: $(maximumAllowedFailures)\n        dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net6_0_x86 # Only run Nightly or if explicitly enabled with RunX86Tests\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'), or(eq(variables['IsNightly'], 'true'), eq(variables['RunX86Tests'], 'true')))\n    strategy:\n      matrix:\n        Windows:\n          osName: 'Windows'\n          imageName: 'windows-latest'\n          maximumParallelJobs: 8\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n    displayName: 'Test net6.0,x86 on'\n    pool:\n      vmImage: $(imageName)\n    steps:\n    - template: '.build/azure-templates/run-tests-on-os.yml'\n      parameters:\n        osName: $(osName)\n        framework: 'net6.0'\n        vsTestPlatform: 'x86'\n        testBinariesArtifactName: '$(TestBinariesArtifactName)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        testResultsArtifactName: '$(TestResultsArtifactName)'\n        maximumParallelJobs: $(maximumParallelJobs)\n        maximumAllowedFailures: $(maximumAllowedFailures)\n        dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net472_x64\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'))\n    strategy:\n      matrix:\n        Windows:\n          osName: 'Windows'\n          imageName: 'windows-latest'\n          maximumParallelJobs: 8\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n    displayName: 'Test net472,x64 on'\n    pool:\n      vmImage: $(imageName)\n    steps:\n    - template: '.build/azure-templates/run-tests-on-os.yml'\n      parameters:\n        osName: $(osName)\n        framework: 'net472'\n        vsTestPlatform: 'x64'\n        testBinariesArtifactName: '$(TestBinariesArtifactName)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        testResultsArtifactName: '$(TestResultsArtifactName)'\n        maximumParallelJobs: $(maximumParallelJobs)\n        maximumAllowedFailures: $(maximumAllowedFailures)\n        dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net472_x86 # Only run Nightly or if explicitly enabled with RunX86Tests\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'), or(eq(variables['IsNightly'], 'true'), eq(variables['RunX86Tests'], 'true')))\n    strategy:\n      matrix:\n        Windows:\n          osName: 'Windows'\n          imageName: 'windows-latest'\n          maximumParallelJobs: 8\n          maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n    displayName: 'Test net472,x86 on'\n    pool:\n      vmImage: $(imageName)\n    steps:\n    - template: '.build/azure-templates/run-tests-on-os.yml'\n      parameters:\n        osName: $(osName)\n        framework: 'net472'\n        vsTestPlatform: 'x86'\n        testBinariesArtifactName: '$(TestBinariesArtifactName)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        testResultsArtifactName: '$(TestResultsArtifactName)'\n        maximumParallelJobs: $(maximumParallelJobs)\n        maximumAllowedFailures: $(maximumAllowedFailures)\n        dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net48_Windows_x64\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'))\n    displayName: 'Test net48,x64 on Windows'\n    pool:\n      vmImage: 'windows-latest'\n    steps:\n    - template: '.build/azure-templates/run-tests-on-os.yml'\n      parameters:\n        osName: 'Windows'\n        framework: 'net48'\n        vsTestPlatform: 'x64'\n        testBinariesArtifactName: '$(TestBinariesArtifactName)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        testResultsArtifactName: '$(TestResultsArtifactName)'\n        maximumParallelJobs: 8\n        maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n        dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n  - job: Test_net48_Windows_x86 # Only run Nightly or if explicitly enabled with RunX86Tests\n    condition: and(succeeded(), ne(variables['RunTests'], 'false'), or(eq(variables['IsNightly'], 'true'), eq(variables['RunX86Tests'], 'true')))\n    displayName: 'Test net48,x86 on Windows'\n    pool:\n      vmImage: 'windows-latest'\n    steps:\n    - template: '.build/azure-templates/run-tests-on-os.yml'\n      parameters:\n        osName: 'Windows'\n        framework: 'net48'\n        vsTestPlatform: 'x86'\n        testBinariesArtifactName: '$(TestBinariesArtifactName)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        testResultsArtifactName: '$(TestResultsArtifactName)'\n        maximumParallelJobs: 8\n        maximumAllowedFailures: 0 # Maximum allowed failures for a successful build\n        dotNetSdkVersion: '$(DotNetSDKVersion)'\n\n\n- stage: Publish_Stage\n  displayName: 'Publish Stage:'\n  jobs:\n\n  # Optional job to push to Azure Artifact feed. Just pass in\n  # the GUID of the artifact feed as ArtifactFeedID to enable.\n  - job: Publish_Azure_Artifacts\n    condition: and(succeeded(), ne(variables['ArtifactFeedID'], ''))\n    pool:\n      vmImage: 'windows-latest'\n\n    steps:\n      # We checkout here because we need to publish the source code along with the symbols for debugging\n    - checkout: self # self represents the repo where the initial Pipelines YAML file was found\n      fetchDepth: '1'  # the depth of commits to ask Git to fetch\n\n    - template: '.build/azure-templates/show-all-environment-variables.yml'\n\n    - task: DownloadPipelineArtifact@0\n      displayName: 'Download Build Artifacts: $(VersionArtifactName)'\n      inputs:\n        artifactName: '$(VersionArtifactName)'\n        targetPath: '$(System.DefaultWorkingDirectory)/$(VersionArtifactName)'\n\n      # For debugging this pipeline\n    #- pwsh: |\n    #    Get-ChildItem -Path $(System.DefaultWorkingDirectory)\n    #    Get-ChildItem -Path '$(VersionArtifactName)'\n\n      # NOTE: We are setting Build.BuildNumber here to the NuGet package version to work around the limitation that\n      # the version cannot be passed to the Index Sources & Publish Symbols task.\n    - pwsh: |\n        $version = Get-Content '$(VersionArtifactName)/$(PackageVersionFileName)' -Raw\n        Write-Host \"##vso[task.setvariable variable=PackageVersion;]$version\"\n        Write-Host \"##vso[build.updatebuildnumber]$version\"\n      displayName: 'Read PackageVersion from File to Build.BuildNumber'\n    - template: '.build/azure-templates/show-all-environment-variables.yml'\n\n    - template: '.build/azure-templates/publish-nuget-packages.yml'\n      parameters:\n        artifactFeedID: '$(ArtifactFeedID)'\n        nugetArtifactName: '$(NuGetArtifactName)'\n        debugArtifactName: '$(DebugArtifactName)'\n\n- stage: Release_Stage\n  displayName: 'Release Stage:'\n  jobs:\n  - job: Release\n    condition: and(succeeded(), eq(variables['IsRelease'], 'true'))\n    displayName: 'Build Release Artifacts for [VOTE]'\n    pool:\n      vmImage: 'windows-latest'\n\n    steps:\n    # We checkout here because we need to publish the source code along with the binaries per Apache's releae policy\n    - checkout: self # self represents the repo where the initial Pipelines YAML file was found\n      fetchDepth: '1'  # the depth of commits to ask Git to fetch\n\n    - template: '.build/azure-templates/show-all-environment-variables.yml'\n\n\n    - task: 'DownloadPipelineArtifact@0'\n      displayName: 'Download Build Artifacts: $(NuGetArtifactName)'\n      inputs:\n        artifactName: '$(NuGetArtifactName)'\n        targetPath: '$(Build.ArtifactStagingDirectory)/$(NuGetArtifactName)'\n\n\n    - task: DownloadPipelineArtifact@0\n      displayName: 'Download Build Artifacts: $(VersionArtifactName)'\n      inputs:\n        artifactName: '$(VersionArtifactName)'\n        targetPath: '$(Build.ArtifactStagingDirectory)/$(VersionArtifactName)'\n\n    - template: '.build/azure-templates/show-all-files.yml' # Uncomment for debugging\n\n      # NOTE: We are setting Build.BuildNumber here to the NuGet package version to work around the limitation that\n      # the version cannot be passed to the Index Sources & Publish Symbols task.\n    - pwsh: |\n        $version = Get-Content '$(Build.ArtifactStagingDirectory)/$(VersionArtifactName)/$(PackageVersionFileName)' -Raw\n        $vcsLabel = 'Lucene.Net_' + $version.Replace('.', '_').Replace('-', '_')\n        Write-Host \"##vso[task.setvariable variable=VCSLabel;]$vcsLabel\"\n        Write-Host \"##vso[task.setvariable variable=PackageVersion;]$version\"\n        Write-Host \"##vso[build.updatebuildnumber]$version\"\n      displayName: 'Build VCS Label and Rehydrate Version Variables'\n\n    - pwsh: |\n        [string[]]$files = @('version.props')\n        foreach ($file in $files) {\n            Copy-Item -Path \"$(Build.ArtifactStagingDirectory)/$(VersionArtifactName)/$file\" -Destination \"$(Build.SourcesDirectory)/$file\" -Force -ErrorAction Continue\n        }\n      displayName: 'Update version.props to build only version $(PackageVersion)'\n\n    - template: '.build/azure-templates/show-all-environment-variables.yml'\n\n    - task: CopyFiles@2\n      displayName: 'Copy Source Code Files to: $(Build.ArtifactStagingDirectory)/srctemp'\n      inputs:\n        SourceFolder: '$(Build.SourcesDirectory)'\n        Contents: |\n         **\n         !.git/**/*\n         !.github/**/*\n         !branding/**/*\n         !_artifacts/**/*\n         !src/**/bin/**/*\n         !src/**/obj/**/*\n         !websites/**/*\n         branding/logo/lucene-net-icon-128x128.png\n        TargetFolder: '$(Build.ArtifactStagingDirectory)/srctemp'\n\n    - task: ArchiveFiles@2\n      displayName: 'Archive Source Code Files'\n      inputs:\n        rootFolderOrFile: '$(Build.ArtifactStagingDirectory)/srctemp'\n        includeRootFolder: false\n        archiveFile: '$(Build.ArtifactStagingDirectory)/$(ReleaseArtifactName)/Apache-Lucene.Net-$(PackageVersion).src.zip'\n\n    - task: CopyFiles@2\n      displayName: 'Copy License/Notice Files to: $(NuGetArtifactName)'\n      inputs:\n        SourceFolder: '$(Build.SourcesDirectory)'\n        Contents: |\n         LICENSE.txt\n         NOTICE.txt\n        TargetFolder: '$(Build.ArtifactStagingDirectory)/$(NuGetArtifactName)'\n\n    - task: ArchiveFiles@2\n      displayName: 'Archive Binary Files'\n      inputs:\n        rootFolderOrFile: '$(Build.ArtifactStagingDirectory)/$(NuGetArtifactName)'\n        includeRootFolder: false\n        archiveFile: '$(Build.ArtifactStagingDirectory)/$(ReleaseArtifactName)/Apache-Lucene.Net-$(PackageVersion).bin.zip'\n\n    - pwsh: |\n        $dir = '$(Build.ArtifactStagingDirectory)/$(ReleaseArtifactName)'\n        if (!(Test-Path $dir)) { New-Item -ItemType Directory -Path \"$dir\" -Force }\n        $nl = [Environment]::NewLine\n        \"TODO: Review: http://www.apache.org/legal/release-policy.html\" + $nl + `\n        \"TODO: Tag Repository\" + $nl + `\n        \"  commit: $(Build.SourceVersion)\" + $nl + `\n        \"  tag: $(VCSLabel)\" + $nl + `\n        \"TODO: Sign release artifacts (see https://www.apache.org/dev/release-signing.html)\" + $nl + `\n        \"TODO: Push release artifacts to dev (https://dist.apache.org/repos/dist/dev/lucenenet/)\" + $nl + `\n        \"TODO: Start release [VOTE] (see https://www.apache.org/foundation/voting.html)\" + $nl | Out-File -FilePath \"$dir/RELEASE-TODO.txt\" -Force\n      displayName: 'Write RELEASE-TODO.txt'\n\n    - task: PublishPipelineArtifact@1\n      displayName: 'Publish Artifact: $(ReleaseArtifactName)'\n      inputs:\n        targetPath: '$(Build.ArtifactStagingDirectory)/$(ReleaseArtifactName)'\n        artifact:  '$(ReleaseArtifactName)'\n        publishLocation: 'pipeline'\n"
        },
        {
          "name": "branding",
          "type": "tree",
          "content": null
        },
        {
          "name": "build",
          "type": "blob",
          "size": 2.27734375,
          "content": "#! /usr/bin/env bash\n# -----------------------------------------------------------------------------------\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"\"License\"\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n# \n# http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"\"AS IS\"\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# -----------------------------------------------------------------------------------\n#\n# This file will build Lucene.Net and create the NuGet packages.\n#\n# Syntax:\n#   build[.bat] [<options>]\n#\n# Available Options:\n#\n#   --file-version <FileVersion>\n#   -fv <FileVersion> - File version number. If not supplied, the file version will be the same\n#                  as PackageVersion (excluding any pre-release tag). This value is not allowed\n#                  if there is a version.props file (which is included in the release distribution).\n#\n#   --package-version <PackageVersion>\n#   -pv <PackageVersion> - Nuget package version. Default is the value defined in Directory.Build.props.\n#                  This value is not allowed if there is a version.props file (which is included in the\n#                  release distribution).\n#\n#   --configuration <Configuration>\n#   -config <Configuration> - MSBuild configuration for the build.\n#\n#   --test\n#   -t - Run the tests.\n#\n#   --maximum-parallel-jobs\n#   -mp - Set the maxumum number of parallel jobs to run during testing. If not supplied, the default is 8.\n#\n#   All options are case insensitive.\n#\n# -----------------------------------------------------------------------------------\nif ! command -v pwsh &> /dev/null\nthen\n    echo \"Powershell Core could not be found. Please install version 3 or higher.\"\n    exit\nfi\npwsh -ExecutionPolicy bypass -Command \"& './build.ps1'\" \"$@\""
        },
        {
          "name": "build.bat",
          "type": "blob",
          "size": 2.3505859375,
          "content": "@echo off\nGOTO endcommentblock\n:: -----------------------------------------------------------------------------------\n::\n::  Licensed to the Apache Software Foundation (ASF) under one or more\n::  contributor license agreements.  See the NOTICE file distributed with\n::  this work for additional information regarding copyright ownership.\n::  The ASF licenses this file to You under the Apache License, Version 2.0\n::  (the \"License\"); you may not use this file except in compliance with\n::  the License.  You may obtain a copy of the License at\n::\n::      http://www.apache.org/licenses/LICENSE-2.0\n::\n::  Unless required by applicable law or agreed to in writing, software\n::  distributed under the License is distributed on an \"AS IS\" BASIS,\n::  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n::  See the License for the specific language governing permissions and\n::  limitations under the License.\n::\n:: -----------------------------------------------------------------------------------\n::\n:: This file will build Lucene.Net and create the NuGet packages.\n::\n:: Syntax:\n::   build[.bat] [<options>]\n::\n:: Available Options:\n::\n::   --file-version <FileVersion>\n::   -fv <FileVersion> - File version number. If not supplied, the file version will be the same\n::                  as PackageVersion (excluding any pre-release tag). This value is not allowed\n::                  if there is a version.props file (which is included in the release distribution).\n::\n::   --package-version <PackageVersion>\n::   -pv <PackageVersion> - Nuget package version. Default is the value defined in Directory.Build.props.\n::                  This value is not allowed if there is a version.props file (which is included in the\n::                  release distribution).\n::\n::   --configuration <Configuration>\n::   -config <Configuration> - MSBuild configuration for the build.\n::\n::   --test\n::   -t - Run the tests.\n::\n::   --maximum-parallel-jobs\n::   -mp - Set the maxumum number of parallel jobs to run during testing. If not supplied, the default is 8.\n::\n::   All options are case insensitive.\n::\n:: -----------------------------------------------------------------------------------\n:endcommentblock\nwhere pwsh >nul 2>nul\nif %ERRORLEVEL% NEQ 0 (echo \"Powershell could not be found. Please install version 3 or higher.\") else (pwsh -ExecutionPolicy bypass -Command \"& '%~dpn0.ps1'\" %*)"
        },
        {
          "name": "build.ps1",
          "type": "blob",
          "size": 4.388671875,
          "content": "﻿# Parses and validates the command arguments and bootstraps the Psake build script with the cleaned values\n\n# -----------------------------------------------------------------------------------\n#\n# Licensed to the Apache Software Foundation (ASF) under one or more\n# contributor license agreements.  See the NOTICE file distributed with\n# this work for additional information regarding copyright ownership.\n# The ASF licenses this file to You under the Apache License, Version 2.0\n# (the \"\"License\"\"); you may not use this file except in compliance with\n# the License.  You may obtain a copy of the License at\n# \n# http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"\"AS IS\"\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n# -----------------------------------------------------------------------------------\n\nfunction Get-NextArg([string[]]$arguments, [int]$i, [string]$argName) {\n    $i++\n    if ($arguments.Length -gt $i -and -not $($arguments[$i]).StartsWith('-')) {\n        return $arguments[$i]\n    } else {\n        throw $(\"'$argName' requires a value to be passed as the next argument.\")\n    }\n}\n\n# Default values, if not supplied as args\n[string]$packageVersion = ''\n[string]$fileVersion = ''\n[string]$configuration = 'Release'\n[bool]$runTests = $false\n[int]$maximumParallelJobs = 8\n\n# If the version.props file exists at the repository root, it is used to \"lock\" the version\n# to the current release (this happens in the official release distribution at\n# https://dist.apache.org/repos/dist/release/lucenenet/). In this case, we will not process\n# any version values that are passed, and the user will get an error. Note we don't do any\n# validation to ensure it has the values we need to produce a build (that part is automated\n# as part of the release).\n[string]$versionPropsFile = \"$PSScriptRoot/version.props\"\n[bool]$versionPropsExists = Test-Path $versionPropsFile\n\n# Analyze the args that were passed and process them\nfor ([int]$i = 0; $i -lt $args.Length; $i++) {\n    $arg = $args[$i]\n    $loweredArg =  \"$arg\".ToLowerInvariant()\n    \n    if ($loweredArg -eq '-t' -or $loweredArg -eq '--test') {\n        $runTests = $true\n    } elseif ($loweredArg -eq '-mp' -or $loweredArg -eq '--maximum-parallel-jobs' -or $loweredArg -eq '--maximumparalleljobs') {\n        [string]$mpjStr = Get-NextArg $args $i $arg\n        [int]$mpjInt = $null\n        if (-not [int]::TryParse($mpjStr, [ref]$mpjInt)) { throw $(\"The '$arg' value must be a 32 bit integer. Got: $mpjStr.\") }\n        $maximumParallelJobs = $mpjInt\n        $i++\n    } elseif ($loweredArg -eq '-pv' -or $loweredArg -eq '--package-version' -or $loweredArg -eq '--packageversion') {\n        if ($versionPropsExists) { throw $(\"'$arg' is not valid when $versionPropsFile exists.\") }\n        $packageVersion = Get-NextArg $args $i $arg\n        $i++\n    } elseif ($loweredArg -eq '-fv' -or $loweredArg -eq '--file-version' -or $loweredArg -eq '--fileversion') {\n        if ($versionPropsExists) { throw $(\"'$arg' is not valid when $versionPropsFile exists.\") }\n        $fileVersion = Get-NextArg $args $i $arg\n        $i++\n    } elseif ($loweredArg -eq '-config' -or $loweredArg -eq '--configuration') {\n        $configuration = Get-NextArg $args $i $arg\n        $i++\n    } else {\n        throw $(\"Unrecognized argument: '$arg'\")\n    }\n}\n\n# Build the call to the Psake script using the captured/default args\n[string[]]$task = 'Default'\nif ($runTests) {\n    $task = 'Default','Test'\n}\n$parameters = @{}\n$properties = @{}\n\n$properties.maximumParallelJobs = $maximumParallelJobs\n\n# If version.props exists, we must not prepare for build or backup, because\n# we assume we are a release distribution.\n$properties.prepareForBuild = -not $versionPropsExists\n$properties.backupFiles = -not $versionPropsExists\n\nif (-not [string]::IsNullOrWhiteSpace($packageVersion)) {\n    $properties.packageVersion=$packageVersion\n}\nif (-not [string]::IsNullOrWhiteSpace($fileVersion)) {\n    $properties.version=$fileVersion\n}\nif (-not [string]::IsNullOrWhiteSpace($configuration)) {\n    $properties.configuration=$configuration\n}\n\nImport-Module \"$PSScriptRoot/.build/psake/psake.psm1\"\nInvoke-Psake \"$PSScriptRoot/.build/runbuild.ps1\" -task $task -properties $properties -parameters $parameters"
        },
        {
          "name": "global.json",
          "type": "blob",
          "size": 0.0947265625,
          "content": "﻿{\n  \"msbuild-sdks\": {\n    \"Microsoft.Build.NoTargets\": \"3.7.56\"\n  },\n  \"sources\": [ \"src\" ]\n}\n"
        },
        {
          "name": "lib",
          "type": "tree",
          "content": null
        },
        {
          "name": "proj",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "websites",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}