{
  "metadata": {
    "timestamp": 1736557310102,
    "page": 234,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "CorentinJ/Real-Time-Voice-Cloning",
      "stars": 53179,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.03,
          "content": "*.ipynb linguist-vendored\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.2,
          "content": "*.pyc\n*.aux\n*.log\n*.out\n*.synctex.gz\n*.suo\n*__pycache__\n*.idea\n*.ipynb_checkpoints\n*.pickle\n*.npy\n*.blg\n*.bbl\n*.bcf\n*.toc\n*.wav\n*.sh\nencoder/saved_models/*\nsynthesizer/saved_models/*\nvocoder/saved_models/*\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.32,
          "content": "MIT License\n\nModified & original work Copyright (c) 2019 Corentin Jemine (https://github.com/CorentinJ)\nOriginal work Copyright (c) 2018 Rayhane Mama (https://github.com/Rayhane-mamah)\nOriginal work Copyright (c) 2019 fatchord (https://github.com/fatchord)\nOriginal work Copyright (c) 2015 braindead (https://github.com/braindead)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.3,
          "content": "# Real-Time Voice Cloning\nThis repository is an implementation of [Transfer Learning from Speaker Verification to\nMultispeaker Text-To-Speech Synthesis](https://arxiv.org/pdf/1806.04558.pdf) (SV2TTS) with a vocoder that works in real-time. This was my [master's thesis](https://matheo.uliege.be/handle/2268.2/6801).\n\nSV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text.\n\n**Video demonstration** (click the picture):\n\n[![Toolbox demo](https://i.imgur.com/8lFUlgz.png)](https://www.youtube.com/watch?v=-O_hYhToKoA)\n\n\n\n### Papers implemented  \n| URL | Designation | Title | Implementation source |\n| --- | ----------- | ----- | --------------------- |\n|[**1806.04558**](https://arxiv.org/pdf/1806.04558.pdf) | **SV2TTS** | **Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis** | This repo |\n|[1802.08435](https://arxiv.org/pdf/1802.08435.pdf) | WaveRNN (vocoder) | Efficient Neural Audio Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN) |\n|[1703.10135](https://arxiv.org/pdf/1703.10135.pdf) | Tacotron (synthesizer) | Tacotron: Towards End-to-End Speech Synthesis | [fatchord/WaveRNN](https://github.com/fatchord/WaveRNN)\n|[1710.10467](https://arxiv.org/pdf/1710.10467.pdf) | GE2E (encoder)| Generalized End-To-End Loss for Speaker Verification | This repo |\n\n## Heads up\nLike everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality:\n- Check out [paperswithcode](https://paperswithcode.com/task/speech-synthesis/) for other repositories and recent research in the field of speech synthesis.\n- Check out [CoquiTTS](https://github.com/coqui-ai/tts) for a repository with a better voice cloning quality and more functionalities.\n- Check out [MetaVoice-1B](https://github.com/metavoiceio/metavoice-src) for a large voice model with high voice quality\n\n## Setup\n\n### 1. Install Requirements\n1. Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory.\n2. Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using `venv`, but this is optional.\n3. Install [ffmpeg](https://ffmpeg.org/download.html#get-packages). This is necessary for reading audio files.\n4. Install [PyTorch](https://pytorch.org/get-started/locally/). Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command.\n5. Install the remaining requirements with `pip install -r requirements.txt`\n\n### 2. (Optional) Download Pretrained Models\nPretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Pretrained-models).\n\n### 3. (Optional) Test Configuration\nBefore you download any dataset, you can begin by testing your configuration with:\n\n`python demo_cli.py`\n\nIf all tests pass, you're good to go.\n\n### 4. (Optional) Download Datasets\nFor playing with the toolbox alone, I only recommend downloading [`LibriSpeech/train-clean-100`](https://www.openslr.org/resources/12/train-clean-100.tar.gz). Extract the contents as `<datasets_root>/LibriSpeech/train-clean-100` where `<datasets_root>` is a directory of your choosing. Other datasets are supported in the toolbox, see [here](https://github.com/CorentinJ/Real-Time-Voice-Cloning/wiki/Training#datasets). You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox.\n\n### 5. Launch the Toolbox\nYou can then try the toolbox:\n\n`python demo_toolbox.py -d <datasets_root>`  \nor  \n`python demo_toolbox.py`  \n\ndepending on whether you downloaded any datasets. If you are running an X-server or if you have the error `Aborted (core dumped)`, see [this issue](https://github.com/CorentinJ/Real-Time-Voice-Cloning/issues/11#issuecomment-504733590).\n"
        },
        {
          "name": "demo_cli.py",
          "type": "blob",
          "size": 9.36,
          "content": "import argparse\nimport os\nfrom pathlib import Path\n\nimport librosa\nimport numpy as np\nimport soundfile as sf\nimport torch\n\nfrom encoder import inference as encoder\nfrom encoder.params_model import model_embedding_size as speaker_embedding_size\nfrom synthesizer.inference import Synthesizer\nfrom utils.argutils import print_args\nfrom utils.default_models import ensure_default_models\nfrom vocoder import inference as vocoder\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\"-e\", \"--enc_model_fpath\", type=Path,\n                        default=\"saved_models/default/encoder.pt\",\n                        help=\"Path to a saved encoder\")\n    parser.add_argument(\"-s\", \"--syn_model_fpath\", type=Path,\n                        default=\"saved_models/default/synthesizer.pt\",\n                        help=\"Path to a saved synthesizer\")\n    parser.add_argument(\"-v\", \"--voc_model_fpath\", type=Path,\n                        default=\"saved_models/default/vocoder.pt\",\n                        help=\"Path to a saved vocoder\")\n    parser.add_argument(\"--cpu\", action=\"store_true\", help=\\\n        \"If True, processing is done on CPU, even when a GPU is available.\")\n    parser.add_argument(\"--no_sound\", action=\"store_true\", help=\\\n        \"If True, audio won't be played.\")\n    parser.add_argument(\"--seed\", type=int, default=None, help=\\\n        \"Optional random number seed value to make toolbox deterministic.\")\n    args = parser.parse_args()\n    arg_dict = vars(args)\n    print_args(args, parser)\n\n    # Hide GPUs from Pytorch to force CPU processing\n    if arg_dict.pop(\"cpu\"):\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\n    print(\"Running a test of your configuration...\\n\")\n\n    if torch.cuda.is_available():\n        device_id = torch.cuda.current_device()\n        gpu_properties = torch.cuda.get_device_properties(device_id)\n        ## Print some environment information (for debugging purposes)\n        print(\"Found %d GPUs available. Using GPU %d (%s) of compute capability %d.%d with \"\n            \"%.1fGb total memory.\\n\" %\n            (torch.cuda.device_count(),\n            device_id,\n            gpu_properties.name,\n            gpu_properties.major,\n            gpu_properties.minor,\n            gpu_properties.total_memory / 1e9))\n    else:\n        print(\"Using CPU for inference.\\n\")\n\n    ## Load the models one by one.\n    print(\"Preparing the encoder, the synthesizer and the vocoder...\")\n    ensure_default_models(Path(\"saved_models\"))\n    encoder.load_model(args.enc_model_fpath)\n    synthesizer = Synthesizer(args.syn_model_fpath)\n    vocoder.load_model(args.voc_model_fpath)\n\n\n    ## Run a test\n    print(\"Testing your configuration with small inputs.\")\n    # Forward an audio waveform of zeroes that lasts 1 second. Notice how we can get the encoder's\n    # sampling rate, which may differ.\n    # If you're unfamiliar with digital audio, know that it is encoded as an array of floats\n    # (or sometimes integers, but mostly floats in this projects) ranging from -1 to 1.\n    # The sampling rate is the number of values (samples) recorded per second, it is set to\n    # 16000 for the encoder. Creating an array of length <sampling_rate> will always correspond\n    # to an audio of 1 second.\n    print(\"\\tTesting the encoder...\")\n    encoder.embed_utterance(np.zeros(encoder.sampling_rate))\n\n    # Create a dummy embedding. You would normally use the embedding that encoder.embed_utterance\n    # returns, but here we're going to make one ourselves just for the sake of showing that it's\n    # possible.\n    embed = np.random.rand(speaker_embedding_size)\n    # Embeddings are L2-normalized (this isn't important here, but if you want to make your own\n    # embeddings it will be).\n    embed /= np.linalg.norm(embed)\n    # The synthesizer can handle multiple inputs with batching. Let's create another embedding to\n    # illustrate that\n    embeds = [embed, np.zeros(speaker_embedding_size)]\n    texts = [\"test 1\", \"test 2\"]\n    print(\"\\tTesting the synthesizer... (loading the model will output a lot of text)\")\n    mels = synthesizer.synthesize_spectrograms(texts, embeds)\n\n    # The vocoder synthesizes one waveform at a time, but it's more efficient for long ones. We\n    # can concatenate the mel spectrograms to a single one.\n    mel = np.concatenate(mels, axis=1)\n    # The vocoder can take a callback function to display the generation. More on that later. For\n    # now we'll simply hide it like this:\n    no_action = lambda *args: None\n    print(\"\\tTesting the vocoder...\")\n    # For the sake of making this test short, we'll pass a short target length. The target length\n    # is the length of the wav segments that are processed in parallel. E.g. for audio sampled\n    # at 16000 Hertz, a target length of 8000 means that the target audio will be cut in chunks of\n    # 0.5 seconds which will all be generated together. The parameters here are absurdly short, and\n    # that has a detrimental effect on the quality of the audio. The default parameters are\n    # recommended in general.\n    vocoder.infer_waveform(mel, target=200, overlap=50, progress_callback=no_action)\n\n    print(\"All test passed! You can now synthesize speech.\\n\\n\")\n\n\n    ## Interactive speech generation\n    print(\"This is a GUI-less example of interface to SV2TTS. The purpose of this script is to \"\n          \"show how you can interface this project easily with your own. See the source code for \"\n          \"an explanation of what is happening.\\n\")\n\n    print(\"Interactive generation loop\")\n    num_generated = 0\n    while True:\n        try:\n            # Get the reference audio filepath\n            message = \"Reference voice: enter an audio filepath of a voice to be cloned (mp3, \" \\\n                      \"wav, m4a, flac, ...):\\n\"\n            in_fpath = Path(input(message).replace(\"\\\"\", \"\").replace(\"\\'\", \"\"))\n\n            ## Computing the embedding\n            # First, we load the wav using the function that the speaker encoder provides. This is\n            # important: there is preprocessing that must be applied.\n\n            # The following two methods are equivalent:\n            # - Directly load from the filepath:\n            preprocessed_wav = encoder.preprocess_wav(in_fpath)\n            # - If the wav is already loaded:\n            original_wav, sampling_rate = librosa.load(str(in_fpath))\n            preprocessed_wav = encoder.preprocess_wav(original_wav, sampling_rate)\n            print(\"Loaded file succesfully\")\n\n            # Then we derive the embedding. There are many functions and parameters that the\n            # speaker encoder interfaces. These are mostly for in-depth research. You will typically\n            # only use this function (with its default parameters):\n            embed = encoder.embed_utterance(preprocessed_wav)\n            print(\"Created the embedding\")\n\n\n            ## Generating the spectrogram\n            text = input(\"Write a sentence (+-20 words) to be synthesized:\\n\")\n\n            # If seed is specified, reset torch seed and force synthesizer reload\n            if args.seed is not None:\n                torch.manual_seed(args.seed)\n                synthesizer = Synthesizer(args.syn_model_fpath)\n\n            # The synthesizer works in batch, so you need to put your data in a list or numpy array\n            texts = [text]\n            embeds = [embed]\n            # If you know what the attention layer alignments are, you can retrieve them here by\n            # passing return_alignments=True\n            specs = synthesizer.synthesize_spectrograms(texts, embeds)\n            spec = specs[0]\n            print(\"Created the mel spectrogram\")\n\n\n            ## Generating the waveform\n            print(\"Synthesizing the waveform:\")\n\n            # If seed is specified, reset torch seed and reload vocoder\n            if args.seed is not None:\n                torch.manual_seed(args.seed)\n                vocoder.load_model(args.voc_model_fpath)\n\n            # Synthesizing the waveform is fairly straightforward. Remember that the longer the\n            # spectrogram, the more time-efficient the vocoder.\n            generated_wav = vocoder.infer_waveform(spec)\n\n\n            ## Post-generation\n            # There's a bug with sounddevice that makes the audio cut one second earlier, so we\n            # pad it.\n            generated_wav = np.pad(generated_wav, (0, synthesizer.sample_rate), mode=\"constant\")\n\n            # Trim excess silences to compensate for gaps in spectrograms (issue #53)\n            generated_wav = encoder.preprocess_wav(generated_wav)\n\n            # Play the audio (non-blocking)\n            if not args.no_sound:\n                import sounddevice as sd\n                try:\n                    sd.stop()\n                    sd.play(generated_wav, synthesizer.sample_rate)\n                except sd.PortAudioError as e:\n                    print(\"\\nCaught exception: %s\" % repr(e))\n                    print(\"Continuing without audio playback. Suppress this message with the \\\"--no_sound\\\" flag.\\n\")\n                except:\n                    raise\n\n            # Save it on the disk\n            filename = \"demo_output_%02d.wav\" % num_generated\n            print(generated_wav.dtype)\n            sf.write(filename, generated_wav.astype(np.float32), synthesizer.sample_rate)\n            num_generated += 1\n            print(\"\\nSaved output as %s\\n\\n\" % filename)\n\n\n        except Exception as e:\n            print(\"Caught exception: %s\" % repr(e))\n            print(\"Restarting\\n\")\n"
        },
        {
          "name": "demo_toolbox.py",
          "type": "blob",
          "size": 1.31,
          "content": "import argparse\nimport os\nfrom pathlib import Path\n\nfrom toolbox import Toolbox\nfrom utils.argutils import print_args\nfrom utils.default_models import ensure_default_models\n\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(\n        description=\"Runs the toolbox.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\"-d\", \"--datasets_root\", type=Path, help= \\\n        \"Path to the directory containing your datasets. See toolbox/__init__.py for a list of \"\n        \"supported datasets.\", default=None)\n    parser.add_argument(\"-m\", \"--models_dir\", type=Path, default=\"saved_models\",\n                        help=\"Directory containing all saved models\")\n    parser.add_argument(\"--cpu\", action=\"store_true\", help=\\\n        \"If True, all inference will be done on CPU\")\n    parser.add_argument(\"--seed\", type=int, default=None, help=\\\n        \"Optional random number seed value to make toolbox deterministic.\")\n    args = parser.parse_args()\n    arg_dict = vars(args)\n    print_args(args, parser)\n\n    # Hide GPUs from Pytorch to force CPU processing\n    if arg_dict.pop(\"cpu\"):\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\n    # Remind the user to download pretrained models if needed\n    ensure_default_models(args.models_dir)\n\n    # Launch the toolbox\n    Toolbox(**arg_dict)\n"
        },
        {
          "name": "encoder",
          "type": "tree",
          "content": null
        },
        {
          "name": "encoder_preprocess.py",
          "type": "blob",
          "size": 3.26,
          "content": "from encoder.preprocess import preprocess_librispeech, preprocess_voxceleb1, preprocess_voxceleb2\nfrom utils.argutils import print_args\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == \"__main__\":\n    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):\n        pass\n\n    parser = argparse.ArgumentParser(\n        description=\"Preprocesses audio files from datasets, encodes them as mel spectrograms and \"\n                    \"writes them to the disk. This will allow you to train the encoder. The \"\n                    \"datasets required are at least one of VoxCeleb1, VoxCeleb2 and LibriSpeech. \"\n                    \"Ideally, you should have all three. You should extract them as they are \"\n                    \"after having downloaded them and put them in a same directory, e.g.:\\n\"\n                    \"-[datasets_root]\\n\"\n                    \"  -LibriSpeech\\n\"\n                    \"    -train-other-500\\n\"\n                    \"  -VoxCeleb1\\n\"\n                    \"    -wav\\n\"\n                    \"    -vox1_meta.csv\\n\"\n                    \"  -VoxCeleb2\\n\"\n                    \"    -dev\",\n        formatter_class=MyFormatter\n    )\n    parser.add_argument(\"datasets_root\", type=Path, help=\\\n        \"Path to the directory containing your LibriSpeech/TTS and VoxCeleb datasets.\")\n    parser.add_argument(\"-o\", \"--out_dir\", type=Path, default=argparse.SUPPRESS, help=\\\n        \"Path to the output directory that will contain the mel spectrograms. If left out, \"\n        \"defaults to <datasets_root>/SV2TTS/encoder/\")\n    parser.add_argument(\"-d\", \"--datasets\", type=str,\n                        default=\"librispeech_other,voxceleb1,voxceleb2\", help=\\\n        \"Comma-separated list of the name of the datasets you want to preprocess. Only the train \"\n        \"set of these datasets will be used. Possible names: librispeech_other, voxceleb1, \"\n        \"voxceleb2.\")\n    parser.add_argument(\"-s\", \"--skip_existing\", action=\"store_true\", help=\\\n        \"Whether to skip existing output files with the same name. Useful if this script was \"\n        \"interrupted.\")\n    parser.add_argument(\"--no_trim\", action=\"store_true\", help=\\\n        \"Preprocess audio without trimming silences (not recommended).\")\n    args = parser.parse_args()\n\n    # Verify webrtcvad is available\n    if not args.no_trim:\n        try:\n            import webrtcvad\n        except:\n            raise ModuleNotFoundError(\"Package 'webrtcvad' not found. This package enables \"\n                \"noise removal and is recommended. Please install and try again. If installation fails, \"\n                \"use --no_trim to disable this error message.\")\n    del args.no_trim\n\n    # Process the arguments\n    args.datasets = args.datasets.split(\",\")\n    if not hasattr(args, \"out_dir\"):\n        args.out_dir = args.datasets_root.joinpath(\"SV2TTS\", \"encoder\")\n    assert args.datasets_root.exists()\n    args.out_dir.mkdir(exist_ok=True, parents=True)\n\n    # Preprocess the datasets\n    print_args(args, parser)\n    preprocess_func = {\n        \"librispeech_other\": preprocess_librispeech,\n        \"voxceleb1\": preprocess_voxceleb1,\n        \"voxceleb2\": preprocess_voxceleb2,\n    }\n    args = vars(args)\n    for dataset in args.pop(\"datasets\"):\n        print(\"Preprocessing %s\" % dataset)\n        preprocess_func[dataset](**args)\n"
        },
        {
          "name": "encoder_train.py",
          "type": "blob",
          "size": 2.34,
          "content": "from utils.argutils import print_args\nfrom encoder.train import train\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Trains the speaker encoder. You must have run encoder_preprocess.py first.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\"run_id\", type=str, help= \\\n        \"Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state \"\n        \"from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved \"\n        \"states and restart from scratch.\")\n    parser.add_argument(\"clean_data_root\", type=Path, help= \\\n        \"Path to the output directory of encoder_preprocess.py. If you left the default \"\n        \"output directory when preprocessing, it should be <datasets_root>/SV2TTS/encoder/.\")\n    parser.add_argument(\"-m\", \"--models_dir\", type=Path, default=\"saved_models\", help=\\\n        \"Path to the root directory that contains all models. A directory <run_name> will be created under this root.\"\n        \"It will contain the saved model weights, as well as backups of those weights and plots generated during \"\n        \"training.\")\n    parser.add_argument(\"-v\", \"--vis_every\", type=int, default=10, help= \\\n        \"Number of steps between updates of the loss and the plots.\")\n    parser.add_argument(\"-u\", \"--umap_every\", type=int, default=100, help= \\\n        \"Number of steps between updates of the umap projection. Set to 0 to never update the \"\n        \"projections.\")\n    parser.add_argument(\"-s\", \"--save_every\", type=int, default=500, help= \\\n        \"Number of steps between updates of the model on the disk. Set to 0 to never save the \"\n        \"model.\")\n    parser.add_argument(\"-b\", \"--backup_every\", type=int, default=7500, help= \\\n        \"Number of steps between backups of the model. Set to 0 to never make backups of the \"\n        \"model.\")\n    parser.add_argument(\"-f\", \"--force_restart\", action=\"store_true\", help= \\\n        \"Do not load any saved model.\")\n    parser.add_argument(\"--visdom_server\", type=str, default=\"http://localhost\")\n    parser.add_argument(\"--no_visdom\", action=\"store_true\", help= \\\n        \"Disable visdom.\")\n    args = parser.parse_args()\n\n    # Run the training\n    print_args(args, parser)\n    train(**vars(args))\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.55,
          "content": "﻿inflect==5.3.0\r\nlibrosa==0.8.1\r\nmatplotlib==3.5.1\r\nnumpy==1.20.3\r\nPillow==8.4.0\r\nPyQt5==5.15.6\r\nscikit-learn==1.0.2\r\nscipy==1.7.3\r\nsounddevice==0.4.3\r\nSoundFile==0.10.3.post1\r\ntqdm==4.62.3\r\numap-learn==0.5.2\r\nUnidecode==1.3.2\r\nurllib3==1.26.7\r\nvisdom==0.1.8.9\r\nwebrtcvad==2.0.10\r\n"
        },
        {
          "name": "samples",
          "type": "tree",
          "content": null
        },
        {
          "name": "synthesizer",
          "type": "tree",
          "content": null
        },
        {
          "name": "synthesizer_preprocess_audio.py",
          "type": "blob",
          "size": 2.32,
          "content": "from synthesizer.preprocess import preprocess_dataset\nfrom synthesizer.hparams import hparams\nfrom utils.argutils import print_args\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Preprocesses audio files from datasets, encodes them as mel spectrograms \"\n                    \"and writes them to  the disk. Audio files are also saved, to be used by the \"\n                    \"vocoder for training.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\"datasets_root\", type=Path, help=\\\n        \"Path to the directory containing your LibriSpeech/TTS datasets.\")\n    parser.add_argument(\"-o\", \"--out_dir\", type=Path, default=argparse.SUPPRESS, help=\\\n        \"Path to the output directory that will contain the mel spectrograms, the audios and the \"\n        \"embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/\")\n    parser.add_argument(\"-n\", \"--n_processes\", type=int, default=4, help=\\\n        \"Number of processes in parallel.\")\n    parser.add_argument(\"-s\", \"--skip_existing\", action=\"store_true\", help=\\\n        \"Whether to overwrite existing files with the same name. Useful if the preprocessing was \"\n        \"interrupted.\")\n    parser.add_argument(\"--hparams\", type=str, default=\"\", help=\\\n        \"Hyperparameter overrides as a comma-separated list of name-value pairs\")\n    parser.add_argument(\"--no_alignments\", action=\"store_true\", help=\\\n        \"Use this option when dataset does not include alignments\\\n        (these are used to split long audio files into sub-utterances.)\")\n    parser.add_argument(\"--datasets_name\", type=str, default=\"LibriSpeech\", help=\\\n        \"Name of the dataset directory to process.\")\n    parser.add_argument(\"--subfolders\", type=str, default=\"train-clean-100,train-clean-360\", help=\\\n        \"Comma-separated list of subfolders to process inside your dataset directory\")\n    args = parser.parse_args()\n\n    # Process the arguments\n    if not hasattr(args, \"out_dir\"):\n        args.out_dir = args.datasets_root.joinpath(\"SV2TTS\", \"synthesizer\")\n\n    # Create directories\n    assert args.datasets_root.exists()\n    args.out_dir.mkdir(exist_ok=True, parents=True)\n\n    # Preprocess the dataset\n    print_args(args, parser)\n    args.hparams = hparams.parse(args.hparams)\n    preprocess_dataset(**vars(args))\n"
        },
        {
          "name": "synthesizer_preprocess_embeds.py",
          "type": "blob",
          "size": 1.15,
          "content": "from synthesizer.preprocess import create_embeddings\nfrom utils.argutils import print_args\nfrom pathlib import Path\nimport argparse\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Creates embeddings for the synthesizer from the LibriSpeech utterances.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n    parser.add_argument(\"synthesizer_root\", type=Path, help=\\\n        \"Path to the synthesizer training data that contains the audios and the train.txt file. \"\n        \"If you let everything as default, it should be <datasets_root>/SV2TTS/synthesizer/.\")\n    parser.add_argument(\"-e\", \"--encoder_model_fpath\", type=Path,\n                        default=\"saved_models/default/encoder.pt\", help=\\\n        \"Path your trained encoder model.\")\n    parser.add_argument(\"-n\", \"--n_processes\", type=int, default=4, help= \\\n        \"Number of parallel processes. An encoder is created for each, so you may need to lower \"\n        \"this value on GPUs with low memory. Set it to 1 if CUDA is unhappy.\")\n    args = parser.parse_args()\n\n    # Preprocess the dataset\n    print_args(args, parser)\n    create_embeddings(**vars(args))\n"
        },
        {
          "name": "synthesizer_train.py",
          "type": "blob",
          "size": 1.71,
          "content": "from pathlib import Path\n\nfrom synthesizer.hparams import hparams\nfrom synthesizer.train import train\nfrom utils.argutils import print_args\nimport argparse\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"run_id\", type=str, help= \\\n        \"Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state \"\n        \"from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved \"\n        \"states and restart from scratch.\")\n    parser.add_argument(\"syn_dir\", type=Path, help= \\\n        \"Path to the synthesizer directory that contains the ground truth mel spectrograms, \"\n        \"the wavs and the embeds.\")\n    parser.add_argument(\"-m\", \"--models_dir\", type=Path, default=\"saved_models\", help=\\\n        \"Path to the output directory that will contain the saved model weights and the logs.\")\n    parser.add_argument(\"-s\", \"--save_every\", type=int, default=1000, help= \\\n        \"Number of steps between updates of the model on the disk. Set to 0 to never save the \"\n        \"model.\")\n    parser.add_argument(\"-b\", \"--backup_every\", type=int, default=25000, help= \\\n        \"Number of steps between backups of the model. Set to 0 to never make backups of the \"\n        \"model.\")\n    parser.add_argument(\"-f\", \"--force_restart\", action=\"store_true\", help= \\\n        \"Do not load any saved model and restart from scratch.\")\n    parser.add_argument(\"--hparams\", default=\"\", help=\\\n        \"Hyperparameter overrides as a comma-separated list of name=value pairs\")\n    args = parser.parse_args()\n    print_args(args, parser)\n\n    args.hparams = hparams.parse(args.hparams)\n\n    # Run the training\n    train(**vars(args))\n"
        },
        {
          "name": "toolbox",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "vocoder",
          "type": "tree",
          "content": null
        },
        {
          "name": "vocoder_preprocess.py",
          "type": "blob",
          "size": 2.15,
          "content": "import argparse\nimport os\nfrom pathlib import Path\n\nfrom synthesizer.hparams import hparams\nfrom synthesizer.synthesize import run_synthesis\nfrom utils.argutils import print_args\n\n\n\nif __name__ == \"__main__\":\n    class MyFormatter(argparse.ArgumentDefaultsHelpFormatter, argparse.RawDescriptionHelpFormatter):\n        pass\n\n    parser = argparse.ArgumentParser(\n        description=\"Creates ground-truth aligned (GTA) spectrograms from the vocoder.\",\n        formatter_class=MyFormatter\n    )\n    parser.add_argument(\"datasets_root\", type=Path, help=\\\n        \"Path to the directory containing your SV2TTS directory. If you specify both --in_dir and \"\n        \"--out_dir, this argument won't be used.\")\n    parser.add_argument(\"-s\", \"--syn_model_fpath\", type=Path,\n                        default=\"saved_models/default/synthesizer.pt\",\n                        help=\"Path to a saved synthesizer\")\n    parser.add_argument(\"-i\", \"--in_dir\", type=Path, default=argparse.SUPPRESS, help= \\\n        \"Path to the synthesizer directory that contains the mel spectrograms, the wavs and the \"\n        \"embeds. Defaults to  <datasets_root>/SV2TTS/synthesizer/.\")\n    parser.add_argument(\"-o\", \"--out_dir\", type=Path, default=argparse.SUPPRESS, help= \\\n        \"Path to the output vocoder directory that will contain the ground truth aligned mel \"\n        \"spectrograms. Defaults to <datasets_root>/SV2TTS/vocoder/.\")\n    parser.add_argument(\"--hparams\", default=\"\", help=\\\n        \"Hyperparameter overrides as a comma-separated list of name=value pairs\")\n    parser.add_argument(\"--cpu\", action=\"store_true\", help=\\\n        \"If True, processing is done on CPU, even when a GPU is available.\")\n    args = parser.parse_args()\n    print_args(args, parser)\n    modified_hp = hparams.parse(args.hparams)\n\n    if not hasattr(args, \"in_dir\"):\n        args.in_dir = args.datasets_root / \"SV2TTS\" / \"synthesizer\"\n    if not hasattr(args, \"out_dir\"):\n        args.out_dir = args.datasets_root / \"SV2TTS\" / \"vocoder\"\n\n    if args.cpu:\n        # Hide GPUs from Pytorch to force CPU processing\n        os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n\n    run_synthesis(args.in_dir, args.out_dir, args.syn_model_fpath, modified_hp)\n"
        },
        {
          "name": "vocoder_train.py",
          "type": "blob",
          "size": 2.76,
          "content": "import argparse\nfrom pathlib import Path\n\nfrom utils.argutils import print_args\nfrom vocoder.train import train\n\n\nif __name__ == \"__main__\":\n    parser = argparse.ArgumentParser(\n        description=\"Trains the vocoder from the synthesizer audios and the GTA synthesized mels, \"\n                    \"or ground truth mels.\",\n        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n    )\n\n    parser.add_argument(\"run_id\", type=str, help= \\\n        \"Name for this model. By default, training outputs will be stored to saved_models/<run_id>/. If a model state \"\n        \"from the same run ID was previously saved, the training will restart from there. Pass -f to overwrite saved \"\n        \"states and restart from scratch.\")\n    parser.add_argument(\"datasets_root\", type=Path, help= \\\n        \"Path to the directory containing your SV2TTS directory. Specifying --syn_dir or --voc_dir \"\n        \"will take priority over this argument.\")\n    parser.add_argument(\"--syn_dir\", type=Path, default=argparse.SUPPRESS, help= \\\n        \"Path to the synthesizer directory that contains the ground truth mel spectrograms, \"\n        \"the wavs and the embeds. Defaults to <datasets_root>/SV2TTS/synthesizer/.\")\n    parser.add_argument(\"--voc_dir\", type=Path, default=argparse.SUPPRESS, help= \\\n        \"Path to the vocoder directory that contains the GTA synthesized mel spectrograms. \"\n        \"Defaults to <datasets_root>/SV2TTS/vocoder/. Unused if --ground_truth is passed.\")\n    parser.add_argument(\"-m\", \"--models_dir\", type=Path, default=\"saved_models\", help=\\\n        \"Path to the directory that will contain the saved model weights, as well as backups \"\n        \"of those weights and wavs generated during training.\")\n    parser.add_argument(\"-g\", \"--ground_truth\", action=\"store_true\", help= \\\n        \"Train on ground truth spectrograms (<datasets_root>/SV2TTS/synthesizer/mels).\")\n    parser.add_argument(\"-s\", \"--save_every\", type=int, default=1000, help= \\\n        \"Number of steps between updates of the model on the disk. Set to 0 to never save the \"\n        \"model.\")\n    parser.add_argument(\"-b\", \"--backup_every\", type=int, default=25000, help= \\\n        \"Number of steps between backups of the model. Set to 0 to never make backups of the \"\n        \"model.\")\n    parser.add_argument(\"-f\", \"--force_restart\", action=\"store_true\", help= \\\n        \"Do not load any saved model and restart from scratch.\")\n    args = parser.parse_args()\n\n    # Process the arguments\n    if not hasattr(args, \"syn_dir\"):\n        args.syn_dir = args.datasets_root / \"SV2TTS\" / \"synthesizer\"\n    if not hasattr(args, \"voc_dir\"):\n        args.voc_dir = args.datasets_root / \"SV2TTS\" / \"vocoder\"\n    del args.datasets_root\n    args.models_dir.mkdir(exist_ok=True)\n\n    # Run the training\n    print_args(args, parser)\n    train(**vars(args))\n"
        }
      ]
    }
  ]
}