{
  "metadata": {
    "timestamp": 1736557529270,
    "page": 574,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYwMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "vllm-project/vllm",
      "stars": 33512,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".buildkite",
          "type": "tree",
          "content": null
        },
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.63,
          "content": "BasedOnStyle: Google\nUseTab: Never\nIndentWidth: 2\nColumnLimit: 80\n\n# Force pointers to the type for C++.\nDerivePointerAlignment: false\nPointerAlignment: Left\n\n# Reordering #include statements can (and currently will) introduce errors\nSortIncludes: false\n\n# Style choices\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nIndentPPDirectives: BeforeHash\n\nIncludeCategories:\n  - Regex:           '^<'\n    Priority:        4\n  - Regex:           '^\"(llvm|llvm-c|clang|clang-c|mlir|mlir-c)/'\n    Priority:        3\n  - Regex:           '^\"(qoda|\\.\\.)/'\n    Priority:        2\n  - Regex:           '.*'\n    Priority:        1\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.34,
          "content": "/.venv\n/build\ndist\nvllm/*.so\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n.mypy_cache\n\n# Distribution / packaging\n.Python\n/build/\ncmake-build-*/\nCMakeUserPresets.json\ndevelop-eggs/\n/dist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 3.51,
          "content": "# version file generated by setuptools-scm\n/vllm/_version.py\n\n# vllm-flash-attn built from source\nvllm/vllm_flash_attn/\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ncmake-build-*/\nCMakeUserPresets.json\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n/.deps/\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\ndocs/source/getting_started/examples/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# generated files\n**/generated/**\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# poetry\n#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.\n#   This is especially recommended for binary packages to ensure reproducibility, and is more\n#   commonly ignored for libraries.\n#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control\n#poetry.lock\n\n# pdm\n#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.\n#pdm.lock\n#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it\n#   in version control.\n#   https://pdm.fming.dev/#use-with-ide\n.pdm.toml\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n# PyCharm\n#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can\n#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore\n#  and can be added to the global gitignore or merged into this file.  For a more nuclear\n#  option (not recommended) you can uncomment the following to ignore the entire idea folder.\n.idea/\n\n# VSCode\n.vscode/\n\n# DS Store\n.DS_Store\n\n# Results\n*.csv\n\n# Python pickle files\n*.pkl\n\n# Sphinx documentation\n_build/\n\n# vim swap files\n*.swo\n*.swp\n\n# hip files generated by PyTorch\n*.hip\n*_hip*\nhip_compat.h\n\n# Benchmark dataset\nbenchmarks/*.json\n\n# Linting\nactionlint\nshellcheck*/\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 0.47,
          "content": "# Read the Docs configuration file\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\nversion: 2\n\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.12\"\n\nsphinx:\n  configuration: docs/source/conf.py\n  fail_on_warning: true\n\n# If using Sphinx, optionally build your docs in additional formats such as PDF\nformats: []\n\n# Optionally declare the Python requirements required to build your docs\npython:\n  install:\n    - requirements: docs/requirements-docs.txt\n"
        },
        {
          "name": ".shellcheckrc",
          "type": "blob",
          "size": 0.48,
          "content": "# rules currently disabled:\n#\n#   SC1091 (info): Not following: <sourced file> was not specified as input (see shellcheck -x)\n#   SC2004 (style): $/${} is unnecessary on arithmetic variables.\n#   SC2129 (style): Consider using { cmd1; cmd2; } >> file instead of individual redirects.\n#   SC2155 (warning): Declare and assign separately to avoid masking return values.\n#   SC2164 (warning): Use 'cd ... || exit' or 'cd ... || return' in case cd fails.\n#\ndisable=SC1091,SC2004,SC2129,SC2155,SC2164\n"
        },
        {
          "name": ".yapfignore",
          "type": "blob",
          "size": 0.01,
          "content": "collect_env.py\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 22.18,
          "content": "cmake_minimum_required(VERSION 3.26)\n\n# When building directly using CMake, make sure you run the install step\n# (it places the .so files in the correct location).\n#\n# Example:\n# mkdir build && cd build\n# cmake -G Ninja -DVLLM_PYTHON_EXECUTABLE=`which python3` -DCMAKE_INSTALL_PREFIX=.. ..\n# cmake --build . --target install\n#\n# If you want to only build one target, make sure to install it manually:\n# cmake --build . --target _C\n# cmake --install . --component _C\nproject(vllm_extensions LANGUAGES CXX)\n\n# CUDA by default, can be overridden by using -DVLLM_TARGET_DEVICE=... (used by setup.py)\nset(VLLM_TARGET_DEVICE \"cuda\" CACHE STRING \"Target device backend for vLLM\")\n\nmessage(STATUS \"Build type: ${CMAKE_BUILD_TYPE}\")\nmessage(STATUS \"Target device: ${VLLM_TARGET_DEVICE}\")\n\ninclude(${CMAKE_CURRENT_LIST_DIR}/cmake/utils.cmake)\n\n# Suppress potential warnings about unused manually-specified variables\nset(ignoreMe \"${VLLM_PYTHON_PATH}\")\n\n# Prevent installation of dependencies (cutlass) by default.\ninstall(CODE \"set(CMAKE_INSTALL_LOCAL_ONLY TRUE)\" ALL_COMPONENTS)\n\n#\n# Supported python versions.  These versions will be searched in order, the\n# first match will be selected.  These should be kept in sync with setup.py.\n#\nset(PYTHON_SUPPORTED_VERSIONS \"3.9\" \"3.10\" \"3.11\" \"3.12\")\n\n# Supported NVIDIA architectures.\nset(CUDA_SUPPORTED_ARCHS \"7.0;7.2;7.5;8.0;8.6;8.7;8.9;9.0\")\n\n# Supported AMD GPU architectures.\nset(HIP_SUPPORTED_ARCHS \"gfx906;gfx908;gfx90a;gfx940;gfx941;gfx942;gfx1030;gfx1100;gfx1101\")\n\n#\n# Supported/expected torch versions for CUDA/ROCm.\n#\n# Currently, having an incorrect pytorch version results in a warning\n# rather than an error.\n#\n# Note: the CUDA torch version is derived from pyproject.toml and various\n# requirements.txt files and should be kept consistent.  The ROCm torch\n# versions are derived from Dockerfile.rocm\n#\nset(TORCH_SUPPORTED_VERSION_CUDA \"2.5.1\")\nset(TORCH_SUPPORTED_VERSION_ROCM \"2.5.1\")\n\n#\n# Try to find python package with an executable that exactly matches\n# `VLLM_PYTHON_EXECUTABLE` and is one of the supported versions.\n#\nif (VLLM_PYTHON_EXECUTABLE)\n  find_python_from_executable(${VLLM_PYTHON_EXECUTABLE} \"${PYTHON_SUPPORTED_VERSIONS}\")\nelse()\n  message(FATAL_ERROR\n    \"Please set VLLM_PYTHON_EXECUTABLE to the path of the desired python version\"\n    \" before running cmake configure.\")\nendif()\n\n#\n# Update cmake's `CMAKE_PREFIX_PATH` with torch location.\n#\nappend_cmake_prefix_path(\"torch\" \"torch.utils.cmake_prefix_path\")\n\n# Ensure the 'nvcc' command is in the PATH\nfind_program(NVCC_EXECUTABLE nvcc)\nif (CUDA_FOUND AND NOT NVCC_EXECUTABLE)\n    message(FATAL_ERROR \"nvcc not found\")\nendif()\n\n#\n# Import torch cmake configuration.\n# Torch also imports CUDA (and partially HIP) languages with some customizations,\n# so there is no need to do this explicitly with check_language/enable_language,\n# etc.\n#\nfind_package(Torch REQUIRED)\n\n#\n# Forward the non-CUDA device extensions to external CMake scripts.\n#\nif (NOT VLLM_TARGET_DEVICE STREQUAL \"cuda\" AND\n    NOT VLLM_TARGET_DEVICE STREQUAL \"rocm\")\n    if (VLLM_TARGET_DEVICE STREQUAL \"cpu\")\n        include(${CMAKE_CURRENT_LIST_DIR}/cmake/cpu_extension.cmake)\n    else()\n        return()\n    endif()\n    return()\nendif()\n\n#\n# Set up GPU language and check the torch version and warn if it isn't\n# what is expected.\n#\nif (NOT HIP_FOUND AND CUDA_FOUND)\n  set(VLLM_GPU_LANG \"CUDA\")\n\n  if (NOT Torch_VERSION VERSION_EQUAL ${TORCH_SUPPORTED_VERSION_CUDA})\n    message(WARNING \"Pytorch version ${TORCH_SUPPORTED_VERSION_CUDA} \"\n      \"expected for CUDA build, saw ${Torch_VERSION} instead.\")\n  endif()\nelseif(HIP_FOUND)\n  set(VLLM_GPU_LANG \"HIP\")\n\n  # Importing torch recognizes and sets up some HIP/ROCm configuration but does\n  # not let cmake recognize .hip files. In order to get cmake to understand the\n  # .hip extension automatically, HIP must be enabled explicitly.\n  enable_language(HIP)\n\n  # ROCm 5.X and 6.X\n  if (ROCM_VERSION_DEV_MAJOR GREATER_EQUAL 5 AND\n      NOT Torch_VERSION VERSION_EQUAL ${TORCH_SUPPORTED_VERSION_ROCM})\n    message(WARNING \"Pytorch version >= ${TORCH_SUPPORTED_VERSION_ROCM} \"\n      \"expected for ROCm build, saw ${Torch_VERSION} instead.\")\n  endif()\nelse()\n  message(FATAL_ERROR \"Can't find CUDA or HIP installation.\")\nendif()\n\n\nif(VLLM_GPU_LANG STREQUAL \"CUDA\")\n  #\n  # For cuda we want to be able to control which architectures we compile for on\n  # a per-file basis in order to cut down on compile time. So here we extract\n  # the set of architectures we want to compile for and remove the from the\n  # CMAKE_CUDA_FLAGS so that they are not applied globally.\n  #\n  clear_cuda_arches(CUDA_ARCH_FLAGS)\n  extract_unique_cuda_archs_ascending(CUDA_ARCHS \"${CUDA_ARCH_FLAGS}\")\n  message(STATUS \"CUDA target architectures: ${CUDA_ARCHS}\")\n  # Filter the target architectures by the supported supported archs\n  # since for some files we will build for all CUDA_ARCHS.\n  cuda_archs_loose_intersection(CUDA_ARCHS\n    \"${CUDA_SUPPORTED_ARCHS}\" \"${CUDA_ARCHS}\")\n  message(STATUS \"CUDA supported target architectures: ${CUDA_ARCHS}\")\nelse()\n  #\n  # For other GPU targets override the GPU architectures detected by cmake/torch\n  # and filter them by the supported versions for the current language.\n  # The final set of arches is stored in `VLLM_GPU_ARCHES`.\n  #\n  override_gpu_arches(VLLM_GPU_ARCHES\n    ${VLLM_GPU_LANG}\n    \"${${VLLM_GPU_LANG}_SUPPORTED_ARCHS}\")\nendif()\n\n#\n# Query torch for additional GPU compilation flags for the given\n# `VLLM_GPU_LANG`.\n# The final set of arches is stored in `VLLM_GPU_FLAGS`.\n#\nget_torch_gpu_compiler_flags(VLLM_GPU_FLAGS ${VLLM_GPU_LANG})\n\n#\n# Set nvcc parallelism.\n#\nif(NVCC_THREADS AND VLLM_GPU_LANG STREQUAL \"CUDA\")\n  list(APPEND VLLM_GPU_FLAGS \"--threads=${NVCC_THREADS}\")\nendif()\n\n\n#\n# Use FetchContent for C++ dependencies that are compiled as part of vLLM's build process.\n# setup.py will override FETCHCONTENT_BASE_DIR to play nicely with sccache.\n# Each dependency that produces build artifacts should override its BINARY_DIR to avoid\n# conflicts between build types. It should instead be set to ${CMAKE_BINARY_DIR}/<dependency>.\n#\ninclude(FetchContent)\nfile(MAKE_DIRECTORY ${FETCHCONTENT_BASE_DIR}) # Ensure the directory exists\nmessage(STATUS \"FetchContent base directory: ${FETCHCONTENT_BASE_DIR}\")\n\n#\n# Define other extension targets\n#\n\n#\n# _C extension\n#\n\nset(VLLM_EXT_SRC\n  \"csrc/cache_kernels.cu\"\n  \"csrc/attention/paged_attention_v1.cu\"\n  \"csrc/attention/paged_attention_v2.cu\"\n  \"csrc/pos_encoding_kernels.cu\"\n  \"csrc/activation_kernels.cu\"\n  \"csrc/layernorm_kernels.cu\"\n  \"csrc/layernorm_quant_kernels.cu\"\n  \"csrc/quantization/gptq/q_gemm.cu\"\n  \"csrc/quantization/compressed_tensors/int8_quant_kernels.cu\"\n  \"csrc/quantization/fp8/common.cu\"\n  \"csrc/quantization/fused_kernels/fused_layernorm_dynamic_per_token_quant.cu\"\n  \"csrc/quantization/gguf/gguf_kernel.cu\"\n  \"csrc/cuda_utils_kernels.cu\"\n  \"csrc/prepare_inputs/advance_step.cu\"\n  \"csrc/torch_bindings.cpp\")\n\nif(VLLM_GPU_LANG STREQUAL \"CUDA\")\n  SET(CUTLASS_ENABLE_HEADERS_ONLY ON CACHE BOOL \"Enable only the header library\")\n\n  # Set CUTLASS_REVISION manually -- its revision detection doesn't work in this case.\n  set(CUTLASS_REVISION \"v3.6.0\" CACHE STRING \"CUTLASS revision to use\")\n\n  # Use the specified CUTLASS source directory for compilation if VLLM_CUTLASS_SRC_DIR is provided\n  if (DEFINED ENV{VLLM_CUTLASS_SRC_DIR})\n    set(VLLM_CUTLASS_SRC_DIR $ENV{VLLM_CUTLASS_SRC_DIR})\n  endif()\n\n  if(VLLM_CUTLASS_SRC_DIR)\n    if(NOT IS_ABSOLUTE VLLM_CUTLASS_SRC_DIR)\n      get_filename_component(VLLM_CUTLASS_SRC_DIR \"${VLLM_CUTLASS_SRC_DIR}\" ABSOLUTE)\n    endif()\n    message(STATUS \"The VLLM_CUTLASS_SRC_DIR is set, using ${VLLM_CUTLASS_SRC_DIR} for compilation\")\n    FetchContent_Declare(cutlass SOURCE_DIR ${VLLM_CUTLASS_SRC_DIR})\n  else()\n    FetchContent_Declare(\n        cutlass\n        GIT_REPOSITORY https://github.com/nvidia/cutlass.git\n        GIT_TAG v3.6.0\n        GIT_PROGRESS TRUE\n\n        # Speed up CUTLASS download by retrieving only the specified GIT_TAG instead of the history.\n        # Important: If GIT_SHALLOW is enabled then GIT_TAG works only with branch names and tags.\n        # So if the GIT_TAG above is updated to a commit hash, GIT_SHALLOW must be set to FALSE\n        GIT_SHALLOW TRUE\n    )\n  endif()\n  FetchContent_MakeAvailable(cutlass)\n\n  list(APPEND VLLM_EXT_SRC\n    \"csrc/mamba/mamba_ssm/selective_scan_fwd.cu\"\n    \"csrc/mamba/causal_conv1d/causal_conv1d.cu\"\n    \"csrc/quantization/aqlm/gemm_kernels.cu\"\n    \"csrc/quantization/awq/gemm_kernels.cu\"\n    \"csrc/custom_all_reduce.cu\"\n    \"csrc/permute_cols.cu\"\n    \"csrc/quantization/cutlass_w8a8/scaled_mm_entry.cu\"\n    \"csrc/sparse/cutlass/sparse_scaled_mm_entry.cu\"\n    \"csrc/sparse/cutlass/sparse_compressor_entry.cu\"\n    \"csrc/cutlass_extensions/common.cpp\")\n\n  set_gencode_flags_for_srcs(\n    SRCS \"${VLLM_EXT_SRC}\"\n    CUDA_ARCHS \"${CUDA_ARCHS}\")\n\n  # Only build Marlin kernels if we are building for at least some compatible archs.\n  # Keep building Marlin for 9.0 as there are some group sizes and shapes that\n  # are not supported by Machete yet.\n  cuda_archs_loose_intersection(MARLIN_ARCHS \"8.0;8.6;8.7;8.9;9.0\" ${CUDA_ARCHS})\n  if (MARLIN_ARCHS)\n    set(MARLIN_SRCS\n       \"csrc/quantization/fp8/fp8_marlin.cu\"\n       \"csrc/quantization/marlin/dense/marlin_cuda_kernel.cu\"\n       \"csrc/quantization/marlin/sparse/marlin_24_cuda_kernel.cu\"\n       \"csrc/quantization/marlin/qqq/marlin_qqq_gemm_kernel.cu\"\n       \"csrc/quantization/gptq_marlin/gptq_marlin.cu\"\n       \"csrc/quantization/gptq_marlin/gptq_marlin_repack.cu\"\n       \"csrc/quantization/gptq_marlin/awq_marlin_repack.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${MARLIN_SRCS}\"\n      CUDA_ARCHS \"${MARLIN_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${MARLIN_SRCS}\")\n    message(STATUS \"Building Marlin kernels for archs: ${MARLIN_ARCHS}\")\n  else()\n    message(STATUS \"Not building Marlin kernels as no compatible archs found\"\n                   \" in CUDA target architectures\")\n  endif()\n\n  # The cutlass_scaled_mm kernels for Hopper (c3x, i.e. CUTLASS 3.x) require\n  # CUDA 12.0 or later (and only work on Hopper, 9.0/9.0a for now).\n  cuda_archs_loose_intersection(SCALED_MM_3X_ARCHS \"9.0;9.0a\" \"${CUDA_ARCHS}\")\n  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_3X_ARCHS)\n    set(SRCS \"csrc/quantization/cutlass_w8a8/scaled_mm_c3x.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${SRCS}\"\n      CUDA_ARCHS \"${SCALED_MM_3X_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n    list(APPEND VLLM_GPU_FLAGS \"-DENABLE_SCALED_MM_C3X=1\")\n    message(STATUS \"Building scaled_mm_c3x for archs: ${SCALED_MM_3X_ARCHS}\")\n  else()\n    if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND SCALED_MM_3X_ARCHS)\n      message(STATUS \"Not building scaled_mm_c3x as CUDA Compiler version is \"\n                     \"not >= 12.0, we recommend upgrading to CUDA 12.0 or \"\n                     \"later if you intend on running FP8 quantized models on \"\n                     \"Hopper.\")\n    else()\n      message(STATUS \"Not building scaled_mm_c3x as no compatible archs found \"\n                     \"in CUDA target architectures\")\n    endif()\n\n    # clear SCALED_MM_3X_ARCHS so the scaled_mm_c2x kernels know we didn't\n    # build any 3x kernels\n    set(SCALED_MM_3X_ARCHS)\n  endif()\n\n  #\n  # For the cutlass_scaled_mm kernels we want to build the c2x (CUTLASS 2.x)\n  # kernels for the remaining archs that are not already built for 3x.\n  cuda_archs_loose_intersection(SCALED_MM_2X_ARCHS\n    \"7.5;8.0;8.6;8.7;8.9;9.0\" \"${CUDA_ARCHS}\")\n  # subtract out the archs that are already built for 3x\n  list(REMOVE_ITEM SCALED_MM_2X_ARCHS ${SCALED_MM_3X_ARCHS})\n  if (SCALED_MM_2X_ARCHS)\n    set(SRCS \"csrc/quantization/cutlass_w8a8/scaled_mm_c2x.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${SRCS}\"\n      CUDA_ARCHS \"${SCALED_MM_2X_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n    list(APPEND VLLM_GPU_FLAGS \"-DENABLE_SCALED_MM_C2X=1\")\n    message(STATUS \"Building scaled_mm_c2x for archs: ${SCALED_MM_2X_ARCHS}\")\n  else()\n    if (SCALED_MM_3X_ARCHS)\n      message(STATUS \"Not building scaled_mm_c2x as all archs are already built\"\n                     \" for and covered by scaled_mm_c3x\")\n    else()\n      message(STATUS \"Not building scaled_mm_c2x as no compatible archs found \"\n                    \"in CUDA target architectures\")\n    endif()\n  endif()\n\n  #\n  # 2:4 Sparse Kernels\n\n  # The 2:4 sparse kernels cutlass_scaled_sparse_mm and cutlass_compressor\n  # require CUDA 12.2 or later (and only work on Hopper, 9.0/9.0a for now).\n  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.2 AND SCALED_MM_3X_ARCHS)\n    set(SRCS \"csrc/sparse/cutlass/sparse_compressor_c3x.cu\"\n             \"csrc/sparse/cutlass/sparse_scaled_mm_c3x.cu\")\n    set_gencode_flags_for_srcs(\n      SRCS \"${SRCS}\"\n      CUDA_ARCHS \"${SCALED_MM_3X_ARCHS}\")\n    list(APPEND VLLM_EXT_SRC \"${SRCS}\")\n    list(APPEND VLLM_GPU_FLAGS \"-DENABLE_SPARSE_SCALED_MM_C3X=1\")\n    message(STATUS \"Building sparse_scaled_mm_c3x for archs: ${SCALED_MM_3X_ARCHS}\")\n  else()\n    if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.2 AND SCALED_MM_3X_ARCHS)\n      message(STATUS \"Not building sparse_scaled_mm_c3x kernels as CUDA Compiler version is \"\n                     \"not >= 12.2, we recommend upgrading to CUDA 12.2 or later \"\n                     \"if you intend on running FP8 sparse quantized models on Hopper.\")\n    else()\n      message(STATUS \"Not building sparse_scaled_mm_c3x as no compatible archs found \"\n                     \"in CUDA target architectures\")\n    endif()\n  endif()\n\n\n  #\n  # Machete kernels\n\n  # The machete kernels only work on hopper and require CUDA 12.0 or later.\n  # Only build Machete kernels if we are building for something compatible with sm90a\n  cuda_archs_loose_intersection(MACHETE_ARCHS \"9.0a\" \"${CUDA_ARCHS}\")\n  if(${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0 AND MACHETE_ARCHS)\n    #\n    # For the Machete kernels we automatically generate sources for various\n    # preselected input type pairs and schedules.\n    # Generate sources:\n    set(MACHETE_GEN_SCRIPT\n      ${CMAKE_CURRENT_SOURCE_DIR}/csrc/quantization/machete/generate.py)\n    file(MD5 ${MACHETE_GEN_SCRIPT} MACHETE_GEN_SCRIPT_HASH)\n\n    message(STATUS \"Machete generation script hash: ${MACHETE_GEN_SCRIPT_HASH}\")\n    message(STATUS \"Last run machete generate script hash: $CACHE{MACHETE_GEN_SCRIPT_HASH}\")\n\n    if (NOT DEFINED CACHE{MACHETE_GEN_SCRIPT_HASH}\n        OR NOT $CACHE{MACHETE_GEN_SCRIPT_HASH} STREQUAL ${MACHETE_GEN_SCRIPT_HASH})\n      execute_process(\n        COMMAND ${CMAKE_COMMAND} -E env\n        PYTHONPATH=${CMAKE_CURRENT_SOURCE_DIR}/csrc/cutlass_extensions/:${CUTLASS_DIR}/python/:${VLLM_PYTHON_PATH}:$PYTHONPATH\n          ${Python_EXECUTABLE} ${MACHETE_GEN_SCRIPT}\n        RESULT_VARIABLE machete_generation_result\n        OUTPUT_VARIABLE machete_generation_output\n        OUTPUT_FILE ${CMAKE_CURRENT_BINARY_DIR}/machete_generation.log\n        ERROR_FILE ${CMAKE_CURRENT_BINARY_DIR}/machete_generation.log\n      )\n\n      if (NOT machete_generation_result EQUAL 0)\n        message(FATAL_ERROR \"Machete generation failed.\"\n                            \" Result: \\\"${machete_generation_result}\\\"\"\n                            \"\\nCheck the log for details: \"\n                            \"${CMAKE_CURRENT_BINARY_DIR}/machete_generation.log\")\n      else()\n        set(MACHETE_GEN_SCRIPT_HASH ${MACHETE_GEN_SCRIPT_HASH}\n            CACHE STRING \"Last run machete generate script hash\" FORCE)\n        message(STATUS \"Machete generation completed successfully.\")\n      endif()\n    else()\n      message(STATUS \"Machete generation script has not changed, skipping generation.\")\n    endif()\n\n    # Add machete generated sources\n    file(GLOB MACHETE_GEN_SOURCES \"csrc/quantization/machete/generated/*.cu\")\n    list(APPEND VLLM_EXT_SRC ${MACHETE_GEN_SOURCES})\n\n    # forward compatible\n    set_gencode_flags_for_srcs(\n      SRCS \"${MACHETE_GEN_SOURCES}\"\n      CUDA_ARCHS \"${MACHETE_ARCHS}\")\n\n    list(APPEND VLLM_EXT_SRC\n      csrc/quantization/machete/machete_pytorch.cu)\n\n    message(STATUS \"Building Machete kernels for archs: ${MACHETE_ARCHS}\")\n  else()\n    if (NOT ${CMAKE_CUDA_COMPILER_VERSION} VERSION_GREATER 12.0\n        AND MACHETE_ARCHS)\n      message(STATUS \"Not building Machete kernels as CUDA Compiler version is \"\n                     \"not >= 12.0, we recommend upgrading to CUDA 12.0 or \"\n                     \"later if you intend on running w4a16 quantized models on \"\n                     \"Hopper.\")\n    else()\n      message(STATUS \"Not building Machete kernels as no compatible archs \"\n                     \"found in CUDA target architectures\")\n    endif()\n  endif()\n# if CUDA endif\nendif()\n\nmessage(STATUS \"Enabling C extension.\")\ndefine_gpu_extension_target(\n  _C\n  DESTINATION vllm\n  LANGUAGE ${VLLM_GPU_LANG}\n  SOURCES ${VLLM_EXT_SRC}\n  COMPILE_FLAGS ${VLLM_GPU_FLAGS}\n  ARCHITECTURES ${VLLM_GPU_ARCHES}\n  INCLUDE_DIRECTORIES ${CUTLASS_INCLUDE_DIR};${CUTLASS_TOOLS_UTIL_INCLUDE_DIR}\n  USE_SABI 3\n  WITH_SOABI)\n\n# If CUTLASS is compiled on NVCC >= 12.5, it by default uses\n# cudaGetDriverEntryPointByVersion as a wrapper to avoid directly calling the\n# driver API. This causes problems when linking with earlier versions of CUDA.\n# Setting this variable sidesteps the issue by calling the driver directly.\ntarget_compile_definitions(_C PRIVATE CUTLASS_ENABLE_DIRECT_CUDA_DRIVER_CALL=1)\n\n#\n# _moe_C extension\n#\n\nset(VLLM_MOE_EXT_SRC\n  \"csrc/moe/torch_bindings.cpp\"\n  \"csrc/moe/moe_align_sum_kernels.cu\"\n  \"csrc/moe/topk_softmax_kernels.cu\")\n\nset_gencode_flags_for_srcs(\n  SRCS \"${VLLM_MOE_EXT_SRC}\"\n  CUDA_ARCHS \"${CUDA_ARCHS}\")\n\nif(VLLM_GPU_LANG STREQUAL \"CUDA\")\n  cuda_archs_loose_intersection(MARLIN_MOE_ARCHS \"8.0;8.6;8.7;8.9;9.0\" \"${CUDA_ARCHS}\")\n  if (MARLIN_MOE_ARCHS)\n    set(MARLIN_MOE_SRC\n        \"csrc/moe/marlin_kernels/marlin_moe_kernel.h\"\n        \"csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.h\"\n        \"csrc/moe/marlin_kernels/marlin_moe_kernel_ku4b8.cu\"\n        \"csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.h\"\n        \"csrc/moe/marlin_kernels/marlin_moe_kernel_ku8b128.cu\"\n        \"csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.h\"\n        \"csrc/moe/marlin_kernels/marlin_moe_kernel_ku4.cu\"\n        \"csrc/moe/marlin_moe_ops.cu\")\n\n    set_gencode_flags_for_srcs(\n      SRCS \"${MARLIN_MOE_SRC}\"\n      CUDA_ARCHS \"${MARLIN_MOE_ARCHS}\")\n\n    list(APPEND VLLM_MOE_EXT_SRC \"${MARLIN_MOE_SRC}\")\n    message(STATUS \"Building Marlin MOE kernels for archs: ${MARLIN_MOE_ARCHS}\")\n  else()\n    message(STATUS \"Not building Marlin MOE kernels as no compatible archs found\"\n                   \" in CUDA target architectures\")\n  endif()\nendif()\n\nmessage(STATUS \"Enabling moe extension.\")\ndefine_gpu_extension_target(\n  _moe_C\n  DESTINATION vllm\n  LANGUAGE ${VLLM_GPU_LANG}\n  SOURCES ${VLLM_MOE_EXT_SRC}\n  COMPILE_FLAGS ${VLLM_GPU_FLAGS}\n  ARCHITECTURES ${VLLM_GPU_ARCHES}\n  USE_SABI 3\n  WITH_SOABI)\n\nif(VLLM_GPU_LANG STREQUAL \"HIP\")\n  #\n  # _rocm_C extension\n  #\n  set(VLLM_ROCM_EXT_SRC\n    \"csrc/rocm/torch_bindings.cpp\"\n    \"csrc/rocm/attention.cu\")\n\n  define_gpu_extension_target(\n    _rocm_C\n    DESTINATION vllm\n    LANGUAGE ${VLLM_GPU_LANG}\n    SOURCES ${VLLM_ROCM_EXT_SRC}\n    COMPILE_FLAGS ${VLLM_GPU_FLAGS}\n    ARCHITECTURES ${VLLM_GPU_ARCHES}\n    USE_SABI 3\n    WITH_SOABI)\nendif()\n\n# vllm-flash-attn currently only supported on CUDA\nif (NOT VLLM_TARGET_DEVICE STREQUAL \"cuda\")\n  return()\nendif ()\n\n# vLLM flash attention requires VLLM_GPU_ARCHES to contain the set of target\n# arches in the CMake syntax (75-real, 89-virtual, etc), since we clear the\n# arches in the CUDA case (and instead set the gencodes on a per file basis)\n# we need to manually set VLLM_GPU_ARCHES here.\nif(VLLM_GPU_LANG STREQUAL \"CUDA\")\n  foreach(_ARCH ${CUDA_ARCHS})\n    string(REPLACE \".\" \"\" _ARCH \"${_ARCH}\")\n    list(APPEND VLLM_GPU_ARCHES \"${_ARCH}-real\")\n  endforeach()\nendif()\n\n#\n# Build vLLM flash attention from source\n#\n# IMPORTANT: This has to be the last thing we do, because vllm-flash-attn uses the same macros/functions as vLLM.\n# Because functions all belong to the global scope, vllm-flash-attn's functions overwrite vLLMs.\n# They should be identical but if they aren't, this is a massive footgun.\n#\n# The vllm-flash-attn install rules are nested under vllm to make sure the library gets installed in the correct place.\n# To only install vllm-flash-attn, use --component vllm_flash_attn_c.\n# If no component is specified, vllm-flash-attn is still installed.\n\n# If VLLM_FLASH_ATTN_SRC_DIR is set, vllm-flash-attn is installed from that directory instead of downloading.\n# This is to enable local development of vllm-flash-attn within vLLM.\n# It can be set as an environment variable or passed as a cmake argument.\n# The environment variable takes precedence.\nif (DEFINED ENV{VLLM_FLASH_ATTN_SRC_DIR})\n  set(VLLM_FLASH_ATTN_SRC_DIR $ENV{VLLM_FLASH_ATTN_SRC_DIR})\nendif()\n\nif(VLLM_FLASH_ATTN_SRC_DIR)\n  FetchContent_Declare(vllm-flash-attn SOURCE_DIR ${VLLM_FLASH_ATTN_SRC_DIR})\nelse()\n  FetchContent_Declare(\n          vllm-flash-attn\n          GIT_REPOSITORY https://github.com/vllm-project/flash-attention.git\n          GIT_TAG 96266b1111111f3d11aabefaf3bacbab6a89d03c\n          GIT_PROGRESS TRUE\n          # Don't share the vllm-flash-attn build between build types\n          BINARY_DIR ${CMAKE_BINARY_DIR}/vllm-flash-attn\n  )\nendif()\n\n# Set the parent build flag so that the vllm-flash-attn library does not redo compile flag and arch initialization.\nset(VLLM_PARENT_BUILD ON)\n\n# Ensure the vllm/vllm_flash_attn directory exists before installation\ninstall(CODE \"file(MAKE_DIRECTORY \\\"\\${CMAKE_INSTALL_PREFIX}/vllm/vllm_flash_attn\\\")\" COMPONENT vllm_flash_attn_c)\n\n# Make sure vllm-flash-attn install rules are nested under vllm/\ninstall(CODE \"set(CMAKE_INSTALL_LOCAL_ONLY FALSE)\" COMPONENT vllm_flash_attn_c)\ninstall(CODE \"set(OLD_CMAKE_INSTALL_PREFIX \\\"\\${CMAKE_INSTALL_PREFIX}\\\")\" COMPONENT vllm_flash_attn_c)\ninstall(CODE \"set(CMAKE_INSTALL_PREFIX \\\"\\${CMAKE_INSTALL_PREFIX}/vllm/\\\")\" COMPONENT vllm_flash_attn_c)\n\n# Fetch the vllm-flash-attn library\nFetchContent_MakeAvailable(vllm-flash-attn)\nmessage(STATUS \"vllm-flash-attn is available at ${vllm-flash-attn_SOURCE_DIR}\")\n\n# Restore the install prefix\ninstall(CODE \"set(CMAKE_INSTALL_PREFIX \\\"\\${OLD_CMAKE_INSTALL_PREFIX}\\\")\" COMPONENT vllm_flash_attn_c)\ninstall(CODE \"set(CMAKE_INSTALL_LOCAL_ONLY TRUE)\" COMPONENT vllm_flash_attn_c)\n\n# Copy over the vllm-flash-attn python files\ninstall(\n        DIRECTORY ${vllm-flash-attn_SOURCE_DIR}/vllm_flash_attn/\n        DESTINATION vllm/vllm_flash_attn\n        COMPONENT vllm_flash_attn_c\n        FILES_MATCHING PATTERN \"*.py\"\n)\n\n# Nothing after vllm-flash-attn, see comment about macros above\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.21,
          "content": "\n# vLLM Code of Conduct\n\n## Our Pledge\n\nWe as members, contributors, and leaders pledge to make participation in our\ncommunity a harassment-free experience for everyone, regardless of age, body\nsize, visible or invisible disability, ethnicity, sex characteristics, gender\nidentity and expression, level of experience, education, socioeconomic status,\nnationality, personal appearance, race, caste, color, religion, or sexual\nidentity and orientation.\n\nWe pledge to act and interact in ways that contribute to an open, welcoming,\ndiverse, inclusive, and healthy community.\n\n## Our Standards\n\nExamples of behavior that contributes to a positive environment for our\ncommunity include:\n\n* Demonstrating empathy and kindness toward other people\n* Being respectful of differing opinions, viewpoints, and experiences\n* Giving and gracefully accepting constructive feedback\n* Accepting responsibility and apologizing to those affected by our mistakes,\n  and learning from the experience\n* Focusing on what is best not just for us as individuals, but for the overall\n  community\n\nExamples of unacceptable behavior include:\n\n* The use of sexualized language or imagery, and sexual attention or advances of\n  any kind\n* Trolling, insulting or derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or email address,\n  without their explicit permission\n* Other conduct which could reasonably be considered inappropriate in a\n  professional setting\n\n## Enforcement Responsibilities\n\nCommunity leaders are responsible for clarifying and enforcing our standards of\nacceptable behavior and will take appropriate and fair corrective action in\nresponse to any behavior that they deem inappropriate, threatening, offensive,\nor harmful.\n\nCommunity leaders have the right and responsibility to remove, edit, or reject\ncomments, commits, code, wiki edits, issues, and other contributions that are\nnot aligned to this Code of Conduct, and will communicate reasons for moderation\ndecisions when appropriate.\n\n## Scope\n\nThis Code of Conduct applies within all community spaces, and also applies when\nan individual is officially representing the community in public spaces.\nExamples of representing our community include using an official email address,\nposting via an official social media account, or acting as an appointed\nrepresentative at an online or offline/IRL event.\n\n## Enforcement\n\nInstances of abusive, harassing, or otherwise unacceptable behavior may be\nreported to the community leaders responsible for enforcement in the #code-of-conduct\nchannel in the [vLLM Discord](https://discord.com/invite/jz7wjKhh6g).\nAll complaints will be reviewed and investigated promptly and fairly.\n\nAll community leaders are obligated to respect the privacy and security of the\nreporter of any incident.\n\n## Enforcement Guidelines\n\nCommunity leaders will follow these Community Impact Guidelines in determining\nthe consequences for any action they deem in violation of this Code of Conduct:\n\n### 1. Correction\n\n**Community Impact**: Use of inappropriate language or other behavior deemed\nunprofessional or unwelcome in the community.\n\n**Consequence**: A private, written warning from community leaders, providing\nclarity around the nature of the violation and an explanation of why the\nbehavior was inappropriate. A public apology may be requested.\n\n### 2. Warning\n\n**Community Impact**: A violation through a single incident or series of\nactions.\n\n**Consequence**: A warning with consequences for continued behavior. No\ninteraction with the people involved, including unsolicited interaction with\nthose enforcing the Code of Conduct, for a specified period of time. This\nincludes avoiding interactions in community spaces as well as external channels\nlike social media. Violating these terms may lead to a temporary or permanent\nban.\n\n### 3. Temporary Ban\n\n**Community Impact**: A serious violation of community standards, including\nsustained inappropriate behavior.\n\n**Consequence**: A temporary ban from any sort of interaction or public\ncommunication with the community for a specified period of time. No public or\nprivate interaction with the people involved, including unsolicited interaction\nwith those enforcing the Code of Conduct, is allowed during this period.\nViolating these terms may lead to a permanent ban.\n\n### 4. Permanent Ban\n\n**Community Impact**: Demonstrating a pattern of violation of community\nstandards, including sustained inappropriate behavior, harassment of an\nindividual, or aggression toward or disparagement of classes of individuals.\n\n**Consequence**: A permanent ban from any sort of public interaction within the\ncommunity.\n\n## Attribution\n\nThis Code of Conduct is adapted from the [Contributor Covenant](https://www.contributor-covenant.org/),\nversion 2.1, available at\n[v2.1](https://www.contributor-covenant.org/version/2/1/code_of_conduct.html).\n\nCommunity Impact Guidelines were inspired by\n[Mozilla's code of conduct enforcement ladder](https://github.com/mozilla/inclusion).\n\nFor answers to common questions about this code of conduct, see the\n[Contributor Covenant FAQ](https://www.contributor-covenant.org/faq). Translations are available at\n[Contributor Covenant translations](https://www.contributor-covenant.org/translations).\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.15,
          "content": "# Contributing to vLLM\n\nYou may find information about contributing to vLLM on [docs.vllm.ai](https://docs.vllm.ai/en/latest/contributing/overview.html).\n"
        },
        {
          "name": "DCO",
          "type": "blob",
          "size": 1.33,
          "content": "Developer Certificate of Origin\nVersion 1.1\n\nCopyright (C) 2004, 2006 The Linux Foundation and its contributors.\n\nEveryone is permitted to copy and distribute verbatim copies of this\nlicense document, but changing it is not allowed.\n\n\nDeveloper's Certificate of Origin 1.1\n\nBy making a contribution to this project, I certify that:\n\n(a) The contribution was created in whole or in part by me and I\n    have the right to submit it under the open source license\n    indicated in the file; or\n\n(b) The contribution is based upon previous work that, to the best\n    of my knowledge, is covered under an appropriate open source\n    license and I have the right under that license to submit that\n    work with modifications, whether created in whole or in part\n    by me, under the same open source license (unless I am\n    permitted to submit under a different license), as indicated\n    in the file; or\n\n(c) The contribution was provided directly to me by some other\n    person who certified (a), (b) or (c) and I have not modified\n    it.\n\n(d) I understand and agree that this project and the contribution\n    are public and that a record of the contribution (including all\n    personal information I submit with it, including my sign-off) is\n    maintained indefinitely and may be redistributed consistent with\n    this project or the open source license(s) involved.\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 11.02,
          "content": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\n# Please update any changes made here to\n# docs/source/contributing/dockerfile/dockerfile.md and\n# docs/source/assets/contributing/dockerfile-stages-dependency.png\n\nARG CUDA_VERSION=12.4.1\n#################### BASE BUILD IMAGE ####################\n# prepare basic build environment\nFROM nvidia/cuda:${CUDA_VERSION}-devel-ubuntu20.04 AS base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nARG TARGETPLATFORM\nENV DEBIAN_FRONTEND=noninteractive\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl sudo \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Upgrade to GCC 10 to avoid https://gcc.gnu.org/bugzilla/show_bug.cgi?id=92519\n# as it was causing spam when compiling the CUTLASS kernels\nRUN apt-get install -y gcc-10 g++-10\nRUN update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-10 110 --slave /usr/bin/g++ g++ /usr/bin/g++-10\nRUN <<EOF\ngcc --version\nEOF\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\nWORKDIR /workspace\n\n# install build and runtime dependencies\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cuda.txt requirements-cuda.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-cuda.txt\n\n# cuda arch list used by torch\n# can be useful for both `dev` and `test`\n# explicitly set the list to avoid issues with torch 2.2\n# see https://github.com/pytorch/pytorch/pull/123243\nARG torch_cuda_arch_list='7.0 7.5 8.0 8.6 8.9 9.0+PTX'\nENV TORCH_CUDA_ARCH_LIST=${torch_cuda_arch_list}\n# Override the arch list for flash-attn to reduce the binary size\nARG vllm_fa_cmake_gpu_arches='80-real;90-real'\nENV VLLM_FA_CMAKE_GPU_ARCHES=${vllm_fa_cmake_gpu_arches}\n#################### BASE BUILD IMAGE ####################\n\n#################### WHEEL BUILD IMAGE ####################\nFROM base AS build\nARG TARGETPLATFORM\n\n# install build dependencies\nCOPY requirements-build.txt requirements-build.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-build.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# max jobs used by Ninja to build extensions\nARG max_jobs=2\nENV MAX_JOBS=${max_jobs}\n# number of threads used by nvcc\nARG nvcc_threads=8\nENV NVCC_THREADS=$nvcc_threads\n\nARG USE_SCCACHE\nARG SCCACHE_BUCKET_NAME=vllm-build-sccache\nARG SCCACHE_REGION_NAME=us-west-2\nARG SCCACHE_S3_NO_CREDENTIALS=0\n# if USE_SCCACHE is set, use sccache to speed up compilation\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    if [ \"$USE_SCCACHE\" = \"1\" ]; then \\\n        echo \"Installing sccache...\" \\\n        && curl -L -o sccache.tar.gz https://github.com/mozilla/sccache/releases/download/v0.8.1/sccache-v0.8.1-x86_64-unknown-linux-musl.tar.gz \\\n        && tar -xzf sccache.tar.gz \\\n        && sudo mv sccache-v0.8.1-x86_64-unknown-linux-musl/sccache /usr/bin/sccache \\\n        && rm -rf sccache.tar.gz sccache-v0.8.1-x86_64-unknown-linux-musl \\\n        && export SCCACHE_BUCKET=${SCCACHE_BUCKET_NAME} \\\n        && export SCCACHE_REGION=${SCCACHE_REGION_NAME} \\\n        && export SCCACHE_S3_NO_CREDENTIALS=${SCCACHE_S3_NO_CREDENTIALS} \\\n        && export SCCACHE_IDLE_TIMEOUT=0 \\\n        && export CMAKE_BUILD_TYPE=Release \\\n        && sccache --show-stats \\\n        && python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38 \\\n        && sccache --show-stats; \\\n    fi\n\nENV CCACHE_DIR=/root/.cache/ccache\nRUN --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git  \\\n    if [ \"$USE_SCCACHE\" != \"1\" ]; then \\\n        python3 setup.py bdist_wheel --dist-dir=dist --py-limited-api=cp38; \\\n    fi\n\n# Check the size of the wheel if RUN_WHEEL_CHECK is true\nCOPY .buildkite/check-wheel-size.py check-wheel-size.py\n# Default max size of the wheel is 250MB\nARG VLLM_MAX_SIZE_MB=250\nENV VLLM_MAX_SIZE_MB=$VLLM_MAX_SIZE_MB\nARG RUN_WHEEL_CHECK=true\nRUN if [ \"$RUN_WHEEL_CHECK\" = \"true\" ]; then \\\n        python3 check-wheel-size.py dist; \\\n    else \\\n        echo \"Skipping wheel size check.\"; \\\n    fi\n#################### EXTENSION Build IMAGE ####################\n\n#################### DEV IMAGE ####################\nFROM base as dev\n\nCOPY requirements-lint.txt requirements-lint.txt\nCOPY requirements-test.txt requirements-test.txt\nCOPY requirements-dev.txt requirements-dev.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n#################### DEV IMAGE ####################\n\n#################### vLLM installation IMAGE ####################\n# image with vLLM installed\nFROM nvidia/cuda:${CUDA_VERSION}-base-ubuntu22.04 AS vllm-base\nARG CUDA_VERSION=12.4.1\nARG PYTHON_VERSION=3.12\nWORKDIR /vllm-workspace\nENV DEBIAN_FRONTEND=noninteractive\nARG TARGETPLATFORM\n\nRUN PYTHON_VERSION_STR=$(echo ${PYTHON_VERSION} | sed 's/\\.//g') && \\\n    echo \"export PYTHON_VERSION_STR=${PYTHON_VERSION_STR}\" >> /etc/environment\n\n# Install Python and other dependencies\nRUN echo 'tzdata tzdata/Areas select America' | debconf-set-selections \\\n    && echo 'tzdata tzdata/Zones/America select Los_Angeles' | debconf-set-selections \\\n    && apt-get update -y \\\n    && apt-get install -y ccache software-properties-common git curl wget sudo vim python3-pip \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && add-apt-repository ppa:deadsnakes/ppa \\\n    && apt-get update -y \\\n    && apt-get install -y python${PYTHON_VERSION} python${PYTHON_VERSION}-dev python${PYTHON_VERSION}-venv libibverbs-dev \\\n    && update-alternatives --install /usr/bin/python3 python3 /usr/bin/python${PYTHON_VERSION} 1 \\\n    && update-alternatives --set python3 /usr/bin/python${PYTHON_VERSION} \\\n    && ln -sf /usr/bin/python${PYTHON_VERSION}-config /usr/bin/python3-config \\\n    && curl -sS https://bootstrap.pypa.io/get-pip.py | python${PYTHON_VERSION} \\\n    && python3 --version && python3 -m pip --version\n\n# Workaround for https://github.com/openai/triton/issues/2507 and\n# https://github.com/pytorch/pytorch/issues/107960 -- hopefully\n# this won't be needed for future versions of this docker image\n# or future versions of triton.\nRUN ldconfig /usr/local/cuda-$(echo $CUDA_VERSION | cut -d. -f1,2)/compat/\n\n# arm64 (GH200) build follows the practice of \"use existing pytorch\" build,\n# we need to install torch and torchvision from the nightly builds first,\n# pytorch will not appear as a vLLM dependency in all of the following steps\n# after this step\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        python3 -m pip install --index-url https://download.pytorch.org/whl/nightly/cu124 \"torch==2.6.0.dev20241210+cu124\" \"torchvision==0.22.0.dev20241215\";  \\\n    fi\n\n# Install vllm wheel first, so that torch etc will be installed.\nRUN --mount=type=bind,from=build,src=/workspace/dist,target=/vllm-workspace/dist \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install dist/*.whl --verbose\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n. /etc/environment && \\\nif [ \"$TARGETPLATFORM\" != \"linux/arm64\" ]; then \\\n    python3 -m pip install https://github.com/flashinfer-ai/flashinfer/releases/download/v0.1.6/flashinfer-0.1.6+cu121torch2.4-cp${PYTHON_VERSION_STR}-cp${PYTHON_VERSION_STR}-linux_x86_64.whl; \\\nfi\nCOPY examples examples\n#################### vLLM installation IMAGE ####################\n\n#################### TEST IMAGE ####################\n# image to run unit testing suite\n# note that this uses vllm installed by `pip`\nFROM vllm-base AS test\n\nADD . /vllm-workspace/\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -r requirements-dev.txt\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -e tests/vllm_test_utils\n\n# enable fast downloads from hf (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install hf_transfer\nENV HF_HUB_ENABLE_HF_TRANSFER 1\n\n# Copy in the v1 package for testing (it isn't distributed yet)\nCOPY vllm/v1 /usr/local/lib/python3.12/dist-packages/vllm/v1\n\n# doc requires source code\n# we hide them inside `test_docs/` , so that this source code\n# will not be imported by other tests\nRUN mkdir test_docs\nRUN mv docs test_docs/\nRUN mv vllm test_docs/\n#################### TEST IMAGE ####################\n\n#################### OPENAI API SERVER ####################\n# base openai image with additional requirements, for any subsequent openai-style images\nFROM vllm-base AS vllm-openai-base\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if [ \"$TARGETPLATFORM\" = \"linux/arm64\" ]; then \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.42.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    else \\\n        pip install accelerate hf_transfer 'modelscope!=1.15.0' 'bitsandbytes>=0.45.0' 'timm==0.9.10' boto3 runai-model-streamer runai-model-streamer[s3]; \\\n    fi\n\nENV VLLM_USAGE_SOURCE production-docker-image\n\n# define sagemaker first, so it is not default from `docker build`\nFROM vllm-openai-base AS vllm-sagemaker\n\nCOPY examples/online_serving/sagemaker-entrypoint.sh .\nRUN chmod +x sagemaker-entrypoint.sh\nENTRYPOINT [\"./sagemaker-entrypoint.sh\"]\n\nFROM vllm-openai-base AS vllm-openai\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n#################### OPENAI API SERVER ####################\n"
        },
        {
          "name": "Dockerfile.arm",
          "type": "blob",
          "size": 2.34,
          "content": "# This vLLM Dockerfile is used to construct an image that can build and run vLLM on ARM CPU platform.\n\nFROM ubuntu:22.04 AS cpu-test-arm\n\nENV CCACHE_DIR=/root/.cache/ccache\n\nENV CMAKE_CXX_COMPILER_LAUNCHER=ccache\n\nRUN --mount=type=cache,target=/var/cache/apt \\\n    apt-get update -y \\\n    && apt-get install -y curl ccache git wget vim numactl gcc-12 g++-12 python3 python3-pip libtcmalloc-minimal4 libnuma-dev \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n\n# tcmalloc provides better memory allocation efficiency, e.g., holding memory in caches to speed up access of commonly-used objects.\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install py-cpuinfo  # Use this to gather CPU info and optimize based on ARM Neoverse cores\n\n# Set LD_PRELOAD for tcmalloc on ARM\nENV LD_PRELOAD=\"/usr/lib/aarch64-linux-gnu/libtcmalloc_minimal.so.4\"\n\nRUN echo 'ulimit -c 0' >> ~/.bashrc\n\nWORKDIR /workspace\n\nARG PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\"\nENV PIP_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,src=requirements-build.txt,target=requirements-build.txt \\\n    pip install --upgrade pip && \\\n    pip install -r requirements-build.txt\n\nFROM cpu-test-arm AS build\n\nWORKDIR /workspace/vllm\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,src=requirements-common.txt,target=requirements-common.txt \\\n    --mount=type=bind,src=requirements-cpu.txt,target=requirements-cpu.txt \\\n    pip install -v -r requirements-cpu.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# Disabling AVX512 specific optimizations for ARM\nARG VLLM_CPU_DISABLE_AVX512=\"true\"\nENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512}\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=bind,source=.git,target=.git \\\n    VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel && \\\n    pip install dist/*.whl && \\\n    rm -rf dist\n\nWORKDIR /workspace/\n\nRUN ln -s /workspace/vllm/tests && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]"
        },
        {
          "name": "Dockerfile.cpu",
          "type": "blob",
          "size": 2.59,
          "content": "# This vLLM Dockerfile is used to construct image that can build and run vLLM on x86 CPU platform.\n\nFROM ubuntu:22.04 AS cpu-test-1\n\nENV CCACHE_DIR=/root/.cache/ccache\n\nENV CMAKE_CXX_COMPILER_LAUNCHER=ccache\n\nRUN --mount=type=cache,target=/var/cache/apt \\\n    apt-get update -y \\\n    && apt-get install -y curl ccache git wget vim numactl gcc-12 g++-12 python3 python3-pip libtcmalloc-minimal4 libnuma-dev \\\n    && apt-get install -y ffmpeg libsm6 libxext6 libgl1 \\\n    && update-alternatives --install /usr/bin/gcc gcc /usr/bin/gcc-12 10 --slave /usr/bin/g++ g++ /usr/bin/g++-12\n\n# https://intel.github.io/intel-extension-for-pytorch/cpu/latest/tutorials/performance_tuning/tuning_guide.html\n# intel-openmp provides additional performance improvement vs. openmp\n# tcmalloc provides better memory allocation efficiency, e.g, holding memory in caches to speed up access of commonly-used objects.\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install intel-openmp==2025.0.1\n\nENV LD_PRELOAD=\"/usr/lib/x86_64-linux-gnu/libtcmalloc_minimal.so.4:/usr/local/lib/libiomp5.so\"\n\nRUN echo 'ulimit -c 0' >> ~/.bashrc\n\nRUN pip install intel_extension_for_pytorch==2.5.0\n\nWORKDIR /workspace\n\nCOPY requirements-build.txt requirements-build.txt\nARG PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\"\nENV PIP_EXTRA_INDEX_URL=${PIP_EXTRA_INDEX_URL}\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install --upgrade pip && \\\n    pip install -r requirements-build.txt\n\nFROM cpu-test-1 AS build\n\nWORKDIR /workspace/vllm\n\nCOPY requirements-common.txt requirements-common.txt\nCOPY requirements-cpu.txt requirements-cpu.txt\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -v -r requirements-cpu.txt\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\n# Support for building with non-AVX512 vLLM: docker build --build-arg VLLM_CPU_DISABLE_AVX512=\"true\" ...\nARG VLLM_CPU_DISABLE_AVX512\nENV VLLM_CPU_DISABLE_AVX512=${VLLM_CPU_DISABLE_AVX512}\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=cache,target=/root/.cache/ccache \\\n    --mount=type=bind,source=.git,target=.git \\\n    VLLM_TARGET_DEVICE=cpu python3 setup.py bdist_wheel && \\\n    pip install dist/*.whl && \\\n    rm -rf dist\n\nWORKDIR /workspace/\n\nRUN ln -s /workspace/vllm/tests && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n\n# install development dependencies (for testing)\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install -e tests/vllm_test_utils\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n"
        },
        {
          "name": "Dockerfile.hpu",
          "type": "blob",
          "size": 0.6,
          "content": "FROM vault.habana.ai/gaudi-docker/1.18.0/ubuntu22.04/habanalabs/pytorch-installer-2.4.0:latest\n\nCOPY ./ /workspace/vllm\n\nWORKDIR /workspace/vllm\n\nRUN pip install -v -r requirements-hpu.txt\n\nENV no_proxy=localhost,127.0.0.1\nENV PT_HPU_ENABLE_LAZY_COLLECTIVES=true\n\nRUN VLLM_TARGET_DEVICE=hpu python3 setup.py install\n\n# install development dependencies (for testing)\nRUN python3 -m pip install -e tests/vllm_test_utils\n\nWORKDIR /workspace/\n\nRUN ln -s /workspace/vllm/tests && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n"
        },
        {
          "name": "Dockerfile.neuron",
          "type": "blob",
          "size": 1.67,
          "content": "# default base image\n# https://gallery.ecr.aws/neuron/pytorch-inference-neuronx\nARG BASE_IMAGE=\"public.ecr.aws/neuron/pytorch-inference-neuronx:2.5.1-neuronx-py310-sdk2.21.0-ubuntu22.04\"\n\nFROM $BASE_IMAGE\n\nRUN echo \"Base image is $BASE_IMAGE\"\n\n# Install some basic utilities\nRUN apt-get update && \\\n    apt-get install -y \\\n        git \\\n        python3 \\\n        python3-pip \\\n        ffmpeg libsm6 libxext6 libgl1\n\n### Mount Point ###\n# When launching the container, mount the code directory to /workspace\nARG APP_MOUNT=/workspace\nVOLUME [ ${APP_MOUNT} ]\nWORKDIR ${APP_MOUNT}/vllm\n\nRUN python3 -m pip install --upgrade pip\nRUN python3 -m pip install --no-cache-dir fastapi ninja tokenizers pandas\nRUN python3 -m pip install sentencepiece transformers==4.45.2 -U\nRUN python3 -m pip install transformers-neuronx --extra-index-url=https://pip.repos.neuron.amazonaws.com -U\nRUN python3 -m pip install neuronx-cc==2.16.345.0 --extra-index-url=https://pip.repos.neuron.amazonaws.com -U\nRUN python3 -m pip install pytest\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\nRUN python3 -m pip install -U \\\n        'cmake>=3.26' ninja packaging 'setuptools-scm>=8' wheel jinja2 \\\n        -r requirements-neuron.txt\n\nENV VLLM_TARGET_DEVICE neuron\nRUN --mount=type=bind,source=.git,target=.git \\\n    pip install --no-build-isolation -v -e .\n\n# install development dependencies (for testing)\nRUN python3 -m pip install -e tests/vllm_test_utils\n\n# overwrite entrypoint to run bash script\nRUN echo \"import subprocess; import sys; subprocess.check_call(sys.argv[1:])\" > /usr/local/bin/dockerd-entrypoint.py\n\nCMD [\"/bin/bash\"]\n"
        },
        {
          "name": "Dockerfile.openvino",
          "type": "blob",
          "size": 0.95,
          "content": "# The vLLM Dockerfile is used to construct vLLM image that can be directly used\n# to run the OpenAI compatible server.\n\nFROM ubuntu:22.04 AS dev\n\nRUN apt-get update -y && \\\n    apt-get install -y \\\n        git python3-pip \\\n        ffmpeg libsm6 libxext6 libgl1\nWORKDIR /workspace\n\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\nRUN python3 -m pip install -U pip\n# install build requirements\nRUN PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\" python3 -m pip install -r /workspace/requirements-build.txt\n# build vLLM with OpenVINO backend\nRUN PIP_EXTRA_INDEX_URL=\"https://download.pytorch.org/whl/cpu\" VLLM_TARGET_DEVICE=\"openvino\" python3 -m pip install /workspace\n\nCOPY examples/ /workspace/examples\nCOPY benchmarks/ /workspace/benchmarks\n\n# install development dependencies (for testing)\nRUN python3 -m pip install -e tests/vllm_test_utils\n\nCMD [\"/bin/bash\"]\n"
        },
        {
          "name": "Dockerfile.ppc64le",
          "type": "blob",
          "size": 1.55,
          "content": "FROM mambaorg/micromamba\nARG MAMBA_DOCKERFILE_ACTIVATE=1\nUSER root\n\nENV PATH=\"/usr/local/cargo/bin:$PATH:/opt/conda/bin/\"\n\nRUN apt-get update -y && apt-get install -y git wget curl vim libnuma-dev libsndfile-dev libprotobuf-dev build-essential ffmpeg libsm6 libxext6 libgl1 libssl-dev \n\n# Some packages in requirements-cpu are installed here\n# IBM provides optimized packages for ppc64le processors in the open-ce project for mamba\n# Currently these may not be available for venv or pip directly\nRUN micromamba install -y -n base -c https://ftp.osuosl.org/pub/open-ce/1.11.0-p10/ -c defaults python=3.10 torchvision-cpu=0.16.2 rust && micromamba clean --all --yes\n\nCOPY ./ /workspace/vllm\n\nWORKDIR /workspace/vllm\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh; fi\n\nRUN --mount=type=cache,target=/root/.cache/pip  \\\n    RUSTFLAGS='-L /opt/conda/lib' pip install -v --prefer-binary --extra-index-url https://repo.fury.io/mgiessing \\\n        'cmake>=3.26' ninja packaging 'setuptools-scm>=8' wheel jinja2 \\\n        torch==2.3.1 \\\n        -r requirements-cpu.txt \\\n        xformers uvloop==0.20.0\n\nRUN --mount=type=bind,source=.git,target=.git \\\n    VLLM_TARGET_DEVICE=cpu python3 setup.py install\n\n# install development dependencies (for testing)\nRUN python3 -m pip install -e tests/vllm_test_utils\n\nWORKDIR /workspace/\n\nRUN ln -s /workspace/vllm/tests && ln -s /workspace/vllm/examples && ln -s /workspace/vllm/benchmarks\n\nENTRYPOINT [\"/opt/conda/bin/python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n"
        },
        {
          "name": "Dockerfile.rocm",
          "type": "blob",
          "size": 5.65,
          "content": "# Default ROCm 6.2 base image\nARG BASE_IMAGE=\"rocm/pytorch:rocm6.2_ubuntu20.04_py3.9_pytorch_release_2.3.0\"\n\n# Default ROCm ARCHes to build vLLM for.\nARG PYTORCH_ROCM_ARCH=\"gfx908;gfx90a;gfx942;gfx1100\"\n\n# Whether to install CK-based flash-attention\n# If 0, will not install flash-attention\nARG BUILD_FA=\"1\"\nARG FA_GFX_ARCHS=\"gfx90a;gfx942\"\nARG FA_BRANCH=\"3cea2fb\"\n\n# Whether to build triton on rocm\nARG BUILD_TRITON=\"1\"\nARG TRITON_BRANCH=\"e192dba\"\n\n### Base image build stage\nFROM $BASE_IMAGE AS base\n\n# Import arg(s) defined before this build stage\nARG PYTORCH_ROCM_ARCH\n\n# Install some basic utilities\nRUN apt-get update && apt-get install python3 python3-pip -y\nRUN apt-get update && apt-get install -y \\\n    curl \\\n    ca-certificates \\\n    sudo \\\n    git \\\n    bzip2 \\\n    libx11-6 \\\n    build-essential \\\n    wget \\\n    unzip \\\n    tmux \\\n    ccache \\\n && rm -rf /var/lib/apt/lists/*\n\n# When launching the container, mount the code directory to /vllm-workspace\nARG APP_MOUNT=/vllm-workspace\nWORKDIR ${APP_MOUNT}\n\nRUN python3 -m pip install --upgrade pip\n# Remove sccache so it doesn't interfere with ccache\n# TODO: implement sccache support across components\nRUN apt-get purge -y sccache; python3 -m pip uninstall -y sccache; rm -f \"$(which sccache)\"\n\n# Install torch == 2.6.0 on ROCm\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    case \"$(ls /opt | grep -Po 'rocm-[0-9]\\.[0-9]')\" in \\\n        *\"rocm-6.2\"*) \\\n            python3 -m pip uninstall -y torch torchvision \\\n            && python3 -m pip install --pre \\\n                torch==2.6.0.dev20241113+rocm6.2 \\\n                'setuptools-scm>=8' \\\n                torchvision==0.20.0.dev20241113+rocm6.2 \\\n                --extra-index-url https://download.pytorch.org/whl/nightly/rocm6.2;; \\\n        *) ;; esac\n\nENV LLVM_SYMBOLIZER_PATH=/opt/rocm/llvm/bin/llvm-symbolizer\nENV PATH=$PATH:/opt/rocm/bin:/libtorch/bin:\nENV LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/opt/rocm/lib/:/libtorch/lib:\nENV CPLUS_INCLUDE_PATH=$CPLUS_INCLUDE_PATH:/libtorch/include:/libtorch/include/torch/csrc/api/include/:/opt/rocm/include/:\n\nENV PYTORCH_ROCM_ARCH=${PYTORCH_ROCM_ARCH}\nENV CCACHE_DIR=/root/.cache/ccache\n\n\n### AMD-SMI build stage\nFROM base AS build_amdsmi\n# Build amdsmi wheel always\nRUN cd /opt/rocm/share/amd_smi \\\n    && python3 -m pip wheel . --wheel-dir=/install\n\n\n### Flash-Attention wheel build stage\nFROM base AS build_fa\nARG BUILD_FA\nARG FA_GFX_ARCHS\nARG FA_BRANCH\n# Build ROCm flash-attention wheel if `BUILD_FA = 1`\nRUN --mount=type=cache,target=${CCACHE_DIR} \\\n    if [ \"$BUILD_FA\" = \"1\" ]; then \\\n        mkdir -p libs \\\n        && cd libs \\\n        && git clone https://github.com/ROCm/flash-attention.git \\\n        && cd flash-attention \\\n        && git checkout \"${FA_BRANCH}\" \\\n        && git submodule update --init \\\n        && GPU_ARCHS=\"${FA_GFX_ARCHS}\" python3 setup.py bdist_wheel --dist-dir=/install; \\\n    # Create an empty directory otherwise as later build stages expect one\n    else mkdir -p /install; \\\n    fi\n\n\n### Triton wheel build stage\nFROM base AS build_triton\nARG BUILD_TRITON\nARG TRITON_BRANCH\n# Build triton wheel if `BUILD_TRITON = 1`\nRUN --mount=type=cache,target=${CCACHE_DIR} \\\n    if [ \"$BUILD_TRITON\" = \"1\" ]; then \\\n    mkdir -p libs \\\n    && cd libs \\\n    && python3 -m pip install ninja cmake wheel pybind11 \\\n    && git clone https://github.com/OpenAI/triton.git \\\n    && cd triton \\\n    && git checkout \"${TRITON_BRANCH}\" \\\n    && cd python \\\n    && python3 setup.py bdist_wheel --dist-dir=/install; \\\n    # Create an empty directory otherwise as later build stages expect one\n    else mkdir -p /install; \\\n    fi\n\n\n### Final vLLM build stage\nFROM base AS final\n# Import the vLLM development directory from the build context\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh ; fi\n\nRUN python3 -m pip install --upgrade pip\n\n# Package upgrades for useful functionality or to avoid dependency issues\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install --upgrade numba scipy huggingface-hub[cli] pytest-shard\n\n\n# Workaround for ray >= 2.10.0\nENV RAY_EXPERIMENTAL_NOSET_ROCR_VISIBLE_DEVICES=1\n# Silences the HF Tokenizers warning\nENV TOKENIZERS_PARALLELISM=false\n\nRUN --mount=type=cache,target=${CCACHE_DIR} \\\n    --mount=type=bind,source=.git,target=.git \\\n    --mount=type=cache,target=/root/.cache/pip \\\n    python3 -m pip install -Ur requirements-rocm.txt \\\n    && python3 setup.py clean --all \\\n    && python3 setup.py develop\n\n# Copy amdsmi wheel into final image\nRUN --mount=type=bind,from=build_amdsmi,src=/install,target=/install \\\n    mkdir -p libs \\\n    && cp /install/*.whl libs \\\n    # Preemptively uninstall to avoid same-version no-installs\n    && python3 -m pip uninstall -y amdsmi;\n\n# Copy triton wheel(s) into final image if they were built\nRUN --mount=type=bind,from=build_triton,src=/install,target=/install \\\n    mkdir -p libs \\\n    && if ls /install/*.whl; then \\\n        cp /install/*.whl libs \\\n        # Preemptively uninstall to avoid same-version no-installs\n        && python3 -m pip uninstall -y triton; fi\n\n# Copy flash-attn wheel(s) into final image if they were built\nRUN --mount=type=bind,from=build_fa,src=/install,target=/install \\\n    mkdir -p libs \\\n    && if ls /install/*.whl; then \\\n        cp /install/*.whl libs \\\n        # Preemptively uninstall to avoid same-version no-installs\n        && python3 -m pip uninstall -y flash-attn; fi\n\n# Install wheels that were built to the final image\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    if ls libs/*.whl; then \\\n    python3 -m pip install libs/*.whl; fi\n\n# install development dependencies (for testing)\nRUN python3 -m pip install -e tests/vllm_test_utils\n\nCMD [\"/bin/bash\"]\n"
        },
        {
          "name": "Dockerfile.tpu",
          "type": "blob",
          "size": 0.78,
          "content": "ARG NIGHTLY_DATE=\"20241017\"\nARG BASE_IMAGE=\"us-central1-docker.pkg.dev/tpu-pytorch-releases/docker/xla:nightly_3.10_tpuvm_$NIGHTLY_DATE\"\n\nFROM $BASE_IMAGE\nWORKDIR /workspace/vllm\n\n# Install some basic utilities\nRUN apt-get update && apt-get install -y \\\n    git \\\n    ffmpeg libsm6 libxext6 libgl1\n\n# Build vLLM.\nCOPY . .\nARG GIT_REPO_CHECK=0\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh; fi\n\nENV VLLM_TARGET_DEVICE=\"tpu\"\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    python3 -m pip install \\\n        -r requirements-tpu.txt\nRUN python3 setup.py develop\n\n# install development dependencies (for testing)\nRUN python3 -m pip install -e tests/vllm_test_utils\n\nCMD [\"/bin/bash\"]\n"
        },
        {
          "name": "Dockerfile.xpu",
          "type": "blob",
          "size": 2.59,
          "content": "FROM intel/oneapi-basekit:2024.2.1-0-devel-ubuntu22.04 AS vllm-base\n\nRUN wget -O- https://apt.repos.intel.com/intel-gpg-keys/GPG-PUB-KEY-INTEL-SW-PRODUCTS.PUB | gpg --dearmor | tee /usr/share/keyrings/intel-oneapi-archive-keyring.gpg > /dev/null && \\\n    echo \"deb [signed-by=/usr/share/keyrings/intel-oneapi-archive-keyring.gpg] https://apt.repos.intel.com/oneapi all main \" | tee /etc/apt/sources.list.d/oneAPI.list && \\\n    chmod 644 /usr/share/keyrings/intel-oneapi-archive-keyring.gpg && \\\n    wget -O- https://repositories.intel.com/graphics/intel-graphics.key | gpg --dearmor | tee /usr/share/keyrings/intel-graphics.gpg > /dev/null && \\\n    echo \"deb [arch=amd64,i386 signed-by=/usr/share/keyrings/intel-graphics.gpg] https://repositories.intel.com/graphics/ubuntu jammy arc\" | tee /etc/apt/sources.list.d/intel.gpu.jammy.list && \\\n    chmod 644 /usr/share/keyrings/intel-graphics.gpg\n\nRUN apt-get update -y && \\\n    apt-get install -y --no-install-recommends --fix-missing \\\n    curl \\\n    ffmpeg \\\n    git \\\n    libsndfile1 \\\n    libsm6 \\\n    libxext6 \\\n    libgl1 \\\n    lsb-release \\\n    numactl \\\n    python3 \\\n    python3-dev \\\n    python3-pip \\\n    # vim \\\n    wget\n\nWORKDIR /workspace/vllm\nCOPY requirements-xpu.txt /workspace/vllm/requirements-xpu.txt\nCOPY requirements-common.txt /workspace/vllm/requirements-common.txt\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install --no-cache-dir \\\n    -r requirements-xpu.txt\n\nRUN git clone https://github.com/intel/pti-gpu && \\\n    cd pti-gpu/sdk && \\\n    git checkout 6c491f07a777ed872c2654ca9942f1d0dde0a082 && \\\n    mkdir build && \\\n    cd build && \\\n    cmake -DCMAKE_BUILD_TYPE=Release -DCMAKE_TOOLCHAIN_FILE=../cmake/toolchains/icpx_toolchain.cmake -DBUILD_TESTING=OFF .. && \\\n    make -j && \\\n    cmake --install . --config Release --prefix \"/usr/local\"\n\nENV LD_LIBRARY_PATH=\"$LD_LIBRARY_PATH:/usr/local/lib/\"\n\nCOPY . .\nARG GIT_REPO_CHECK\nRUN --mount=type=bind,source=.git,target=.git \\\n    if [ \"$GIT_REPO_CHECK\" != 0 ]; then bash tools/check_repo.sh; fi\n\nENV VLLM_TARGET_DEVICE=xpu\n\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    --mount=type=bind,source=.git,target=.git \\\n    python3 setup.py install\n\nCMD [\"/bin/bash\"]\n\nFROM vllm-base AS vllm-openai\n\n# install additional dependencies for openai api server\nRUN --mount=type=cache,target=/root/.cache/pip \\\n    pip install accelerate hf_transfer 'modelscope!=1.15.0'\n\nENV VLLM_USAGE_SOURCE production-docker-image \\\n    TRITON_XPU_PROFILE 1\n# install development dependencies (for testing)\nRUN python3 -m pip install -e tests/vllm_test_utils\nENTRYPOINT [\"python3\", \"-m\", \"vllm.entrypoints.openai.api_server\"]\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.09,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "MANIFEST.in",
          "type": "blob",
          "size": 0.24,
          "content": "include LICENSE\ninclude requirements-common.txt\ninclude requirements-cuda.txt\ninclude requirements-rocm.txt\ninclude requirements-neuron.txt\ninclude requirements-cpu.txt\ninclude CMakeLists.txt\n\nrecursive-include cmake *\nrecursive-include csrc *\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.61,
          "content": "<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-dark.png\">\n    <img alt=\"vLLM\" src=\"https://raw.githubusercontent.com/vllm-project/vllm/main/docs/source/assets/logos/vllm-logo-text-light.png\" width=55%>\n  </picture>\n</p>\n\n<h3 align=\"center\">\nEasy, fast, and cheap LLM serving for everyone\n</h3>\n\n<p align=\"center\">\n| <a href=\"https://docs.vllm.ai\"><b>Documentation</b></a> | <a href=\"https://vllm.ai\"><b>Blog</b></a> | <a href=\"https://arxiv.org/abs/2309.06180\"><b>Paper</b></a> | <a href=\"https://discord.gg/jz7wjKhh6g\"><b>Discord</b></a> | <a href=\"https://x.com/vllm_project\"><b>Twitter/X</b></a> | <a href=\"https://slack.vllm.ai\"><b>Developer Slack</b></a> |\n</p>\n\n---\n\nThe first vLLM meetup in 2025 is happening on January 22nd, Wednesday, with Google Cloud in San Francisco! We will talk about vLLM's performant V1 architecture, Q1 roadmap, Google Cloud's innovation around vLLM: networking, Cloud Run, Vertex, and TPU! [Register Now](https://lu.ma/zep56hui)\n\n---\n\n*Latest News* 🔥\n- [2024/12] vLLM joins [pytorch ecosystem](https://pytorch.org/blog/vllm-joins-pytorch)! Easy, Fast, and Cheap LLM Serving for Everyone!\n- [2024/11] We hosted [the seventh vLLM meetup](https://lu.ma/h0qvrajz) with Snowflake! Please find the meetup slides from vLLM team [here](https://docs.google.com/presentation/d/1e3CxQBV3JsfGp30SwyvS3eM_tW-ghOhJ9PAJGK6KR54/edit?usp=sharing), and Snowflake team [here](https://docs.google.com/presentation/d/1qF3RkDAbOULwz9WK5TOltt2fE9t6uIc_hVNLFAaQX6A/edit?usp=sharing).\n- [2024/10] We have just created a developer slack ([slack.vllm.ai](https://slack.vllm.ai)) focusing on coordinating contributions and discussing features. Please feel free to join us there!\n- [2024/10] Ray Summit 2024 held a special track for vLLM! Please find the opening talk slides from the vLLM team [here](https://docs.google.com/presentation/d/1B_KQxpHBTRa_mDF-tR6i8rWdOU5QoTZNcEg2MKZxEHM/edit?usp=sharing). Learn more from the [talks](https://www.youtube.com/playlist?list=PLzTswPQNepXl6AQwifuwUImLPFRVpksjR) from other vLLM contributors and users!\n- [2024/09] We hosted [the sixth vLLM meetup](https://lu.ma/87q3nvnh) with NVIDIA! Please find the meetup slides [here](https://docs.google.com/presentation/d/1wrLGwytQfaOTd5wCGSPNhoaW3nq0E-9wqyP7ny93xRs/edit?usp=sharing).\n- [2024/07] We hosted [the fifth vLLM meetup](https://lu.ma/lp0gyjqr) with AWS! Please find the meetup slides [here](https://docs.google.com/presentation/d/1RgUD8aCfcHocghoP3zmXzck9vX3RCI9yfUAB2Bbcl4Y/edit?usp=sharing).\n- [2024/07] In partnership with Meta, vLLM officially supports Llama 3.1 with FP8 quantization and pipeline parallelism! Please check out our blog post [here](https://blog.vllm.ai/2024/07/23/llama31.html).\n- [2024/06] We hosted [the fourth vLLM meetup](https://lu.ma/agivllm) with Cloudflare and BentoML! Please find the meetup slides [here](https://docs.google.com/presentation/d/1iJ8o7V2bQEi0BFEljLTwc5G1S10_Rhv3beed5oB0NJ4/edit?usp=sharing).\n- [2024/04] We hosted [the third vLLM meetup](https://robloxandvllmmeetup2024.splashthat.com/) with Roblox! Please find the meetup slides [here](https://docs.google.com/presentation/d/1A--47JAK4BJ39t954HyTkvtfwn0fkqtsL8NGFuslReM/edit?usp=sharing).\n- [2024/01] We hosted [the second vLLM meetup](https://lu.ma/ygxbpzhl) with IBM! Please find the meetup slides [here](https://docs.google.com/presentation/d/12mI2sKABnUw5RBWXDYY-HtHth4iMSNcEoQ10jDQbxgA/edit?usp=sharing).\n- [2023/10] We hosted [the first vLLM meetup](https://lu.ma/first-vllm-meetup) with a16z! Please find the meetup slides [here](https://docs.google.com/presentation/d/1QL-XPFXiFpDBh86DbEegFXBXFXjix4v032GhShbKf3s/edit?usp=sharing).\n- [2023/08] We would like to express our sincere gratitude to [Andreessen Horowitz](https://a16z.com/2023/08/30/supporting-the-open-source-ai-community/) (a16z) for providing a generous grant to support the open-source development and research of vLLM.\n- [2023/06] We officially released vLLM! FastChat-vLLM integration has powered [LMSYS Vicuna and Chatbot Arena](https://chat.lmsys.org) since mid-April. Check out our [blog post](https://vllm.ai).\n\n---\n## About\nvLLM is a fast and easy-to-use library for LLM inference and serving.\n\nvLLM is fast with:\n\n- State-of-the-art serving throughput\n- Efficient management of attention key and value memory with [**PagedAttention**](https://blog.vllm.ai/2023/06/20/vllm.html)\n- Continuous batching of incoming requests\n- Fast model execution with CUDA/HIP graph\n- Quantizations: [GPTQ](https://arxiv.org/abs/2210.17323), [AWQ](https://arxiv.org/abs/2306.00978), INT4, INT8, and FP8.\n- Optimized CUDA kernels, including integration with FlashAttention and FlashInfer.\n- Speculative decoding\n- Chunked prefill\n\n**Performance benchmark**: We include a performance benchmark at the end of [our blog post](https://blog.vllm.ai/2024/09/05/perf-update.html). It compares the performance of vLLM against other LLM serving engines ([TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM), [SGLang](https://github.com/sgl-project/sglang) and [LMDeploy](https://github.com/InternLM/lmdeploy)). The implementation is under [nightly-benchmarks folder](.buildkite/nightly-benchmarks/) and you can [reproduce](https://github.com/vllm-project/vllm/issues/8176) this benchmark using our one-click runnable script.\n\nvLLM is flexible and easy to use with:\n\n- Seamless integration with popular Hugging Face models\n- High-throughput serving with various decoding algorithms, including *parallel sampling*, *beam search*, and more\n- Tensor parallelism and pipeline parallelism support for distributed inference\n- Streaming outputs\n- OpenAI-compatible API server\n- Support NVIDIA GPUs, AMD CPUs and GPUs, Intel CPUs and GPUs, PowerPC CPUs, TPU, and AWS Neuron.\n- Prefix caching support\n- Multi-lora support\n\nvLLM seamlessly supports most popular open-source models on HuggingFace, including:\n- Transformer-like LLMs (e.g., Llama)\n- Mixture-of-Expert LLMs (e.g., Mixtral, Deepseek-V2 and V3)\n- Embedding Models (e.g. E5-Mistral)\n- Multi-modal LLMs (e.g., LLaVA)\n\nFind the full list of supported models [here](https://docs.vllm.ai/en/latest/models/supported_models.html).\n\n## Getting Started\n\nInstall vLLM with `pip` or [from source](https://vllm.readthedocs.io/en/latest/getting_started/installation.html#build-from-source):\n\n```bash\npip install vllm\n```\n\nVisit our [documentation](https://vllm.readthedocs.io/en/latest/) to learn more.\n- [Installation](https://vllm.readthedocs.io/en/latest/getting_started/installation.html)\n- [Quickstart](https://vllm.readthedocs.io/en/latest/getting_started/quickstart.html)\n- [List of Supported Models](https://vllm.readthedocs.io/en/latest/models/supported_models.html)\n\n## Contributing\n\nWe welcome and value any contributions and collaborations.\nPlease check out [CONTRIBUTING.md](./CONTRIBUTING.md) for how to get involved.\n\n## Sponsors\n\nvLLM is a community project. Our compute resources for development and testing are supported by the following organizations. Thank you for your support!\n\n<!-- Note: Please sort them in alphabetical order. -->\n<!-- Note: Please keep these consistent with docs/source/community/sponsors.md -->\nCash Donations:\n- a16z\n- Dropbox\n- Sequoia Capital\n- Skywork AI\n- ZhenFund\n\nCompute Resources:\n- AMD\n- Anyscale\n- AWS\n- Crusoe Cloud\n- Databricks\n- DeepInfra\n- Google Cloud\n- Lambda Lab\n- Nebius\n- Novita AI\n- NVIDIA\n- Replicate\n- Roblox\n- RunPod\n- Trainy\n- UC Berkeley\n- UC San Diego\n\nSlack Sponsor: Anyscale\n\nWe also have an official fundraising venue through [OpenCollective](https://opencollective.com/vllm). We plan to use the fund to support the development, maintenance, and adoption of vLLM.\n\n## Citation\n\nIf you use vLLM for your research, please cite our [paper](https://arxiv.org/abs/2309.06180):\n```bibtex\n@inproceedings{kwon2023efficient,\n  title={Efficient Memory Management for Large Language Model Serving with PagedAttention},\n  author={Woosuk Kwon and Zhuohan Li and Siyuan Zhuang and Ying Sheng and Lianmin Zheng and Cody Hao Yu and Joseph E. Gonzalez and Hao Zhang and Ion Stoica},\n  booktitle={Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles},\n  year={2023}\n}\n```\n\n## Contact Us\n\n* For technical questions and feature requests, please use Github issues or discussions.\n* For discussing with fellow users, please use Discord.\n* For coordinating contributions and development, please use Slack.\n* For security disclosures, please use Github's security advisory feature.\n* For collaborations and partnerships, please contact us at vllm-questions AT lists.berkeley.edu.\n\n## Media Kit\n\n* If you wish to use vLLM's logo, please refer to [our media kit repo](https://github.com/vllm-project/media-kit).\n"
        },
        {
          "name": "SECURITY.md",
          "type": "blob",
          "size": 0.69,
          "content": "# Security Policy\n\n## Reporting a Vulnerability\n\nIf you believe you have found a security vulnerability in vLLM, we encourage you to let us know right away. We will investigate all legitimate reports and do our best to quickly fix the problem.\n\nPlease report security issues privately using [the vulnerability submission form](https://github.com/vllm-project/vllm/security/advisories/new). Reports will then be triaged by the [vulnerability management team](https://docs.vllm.ai/contributing/vulnerability_management/).\n\n---\n\nPlease see [PyTorch's Security Policy](https://github.com/pytorch/pytorch/blob/main/SECURITY.md) for more information and recommendations on how to securely interact with models.\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "collect_env.py",
          "type": "blob",
          "size": 25.6,
          "content": "# ruff: noqa\n# code borrowed from https://github.com/pytorch/pytorch/blob/main/torch/utils/collect_env.py\n\nimport datetime\nimport locale\nimport os\nimport re\nimport subprocess\nimport sys\n# Unlike the rest of the PyTorch this file must be python2 compliant.\n# This script outputs relevant system environment info\n# Run it with `python collect_env.py` or `python -m torch.utils.collect_env`\nfrom collections import namedtuple\n\nfrom vllm.envs import environment_variables\n\ntry:\n    import torch\n    TORCH_AVAILABLE = True\nexcept (ImportError, NameError, AttributeError, OSError):\n    TORCH_AVAILABLE = False\n\n# System Environment Information\nSystemEnv = namedtuple(\n    'SystemEnv',\n    [\n        'torch_version',\n        'is_debug_build',\n        'cuda_compiled_version',\n        'gcc_version',\n        'clang_version',\n        'cmake_version',\n        'os',\n        'libc_version',\n        'python_version',\n        'python_platform',\n        'is_cuda_available',\n        'cuda_runtime_version',\n        'cuda_module_loading',\n        'nvidia_driver_version',\n        'nvidia_gpu_models',\n        'cudnn_version',\n        'pip_version',  # 'pip' or 'pip3'\n        'pip_packages',\n        'conda_packages',\n        'hip_compiled_version',\n        'hip_runtime_version',\n        'miopen_runtime_version',\n        'caching_allocator_config',\n        'is_xnnpack_available',\n        'cpu_info',\n        'rocm_version',  # vllm specific field\n        'neuron_sdk_version',  # vllm specific field\n        'vllm_version',  # vllm specific field\n        'vllm_build_flags',  # vllm specific field\n        'gpu_topo',  # vllm specific field\n        'env_vars',\n    ])\n\nDEFAULT_CONDA_PATTERNS = {\n    \"torch\",\n    \"numpy\",\n    \"cudatoolkit\",\n    \"soumith\",\n    \"mkl\",\n    \"magma\",\n    \"triton\",\n    \"optree\",\n    \"nccl\",\n    \"transformers\",\n    \"zmq\",\n    \"nvidia\",\n    \"pynvml\",\n}\n\nDEFAULT_PIP_PATTERNS = {\n    \"torch\",\n    \"numpy\",\n    \"mypy\",\n    \"flake8\",\n    \"triton\",\n    \"optree\",\n    \"onnx\",\n    \"nccl\",\n    \"transformers\",\n    \"zmq\",\n    \"nvidia\",\n    \"pynvml\",\n}\n\n\ndef run(command):\n    \"\"\"Return (return-code, stdout, stderr).\"\"\"\n    shell = True if type(command) is str else False\n    p = subprocess.Popen(command,\n                         stdout=subprocess.PIPE,\n                         stderr=subprocess.PIPE,\n                         shell=shell)\n    raw_output, raw_err = p.communicate()\n    rc = p.returncode\n    if get_platform() == 'win32':\n        enc = 'oem'\n    else:\n        enc = locale.getpreferredencoding()\n    output = raw_output.decode(enc)\n    err = raw_err.decode(enc)\n    return rc, output.strip(), err.strip()\n\n\ndef run_and_read_all(run_lambda, command):\n    \"\"\"Run command using run_lambda; reads and returns entire output if rc is 0.\"\"\"\n    rc, out, _ = run_lambda(command)\n    if rc != 0:\n        return None\n    return out\n\n\ndef run_and_parse_first_match(run_lambda, command, regex):\n    \"\"\"Run command using run_lambda, returns the first regex match if it exists.\"\"\"\n    rc, out, _ = run_lambda(command)\n    if rc != 0:\n        return None\n    match = re.search(regex, out)\n    if match is None:\n        return None\n    return match.group(1)\n\n\ndef run_and_return_first_line(run_lambda, command):\n    \"\"\"Run command using run_lambda and returns first line if output is not empty.\"\"\"\n    rc, out, _ = run_lambda(command)\n    if rc != 0:\n        return None\n    return out.split('\\n')[0]\n\n\ndef get_conda_packages(run_lambda, patterns=None):\n    if patterns is None:\n        patterns = DEFAULT_CONDA_PATTERNS\n    conda = os.environ.get('CONDA_EXE', 'conda')\n    out = run_and_read_all(run_lambda, \"{} list\".format(conda))\n    if out is None:\n        return out\n\n    return \"\\n\".join(line for line in out.splitlines()\n                     if not line.startswith(\"#\") and any(name in line\n                                                         for name in patterns))\n\n\ndef get_gcc_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, 'gcc --version', r'gcc (.*)')\n\n\ndef get_clang_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, 'clang --version',\n                                     r'clang version (.*)')\n\n\ndef get_cmake_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, 'cmake --version',\n                                     r'cmake (.*)')\n\n\ndef get_nvidia_driver_version(run_lambda):\n    if get_platform() == 'darwin':\n        cmd = 'kextstat | grep -i cuda'\n        return run_and_parse_first_match(run_lambda, cmd,\n                                         r'com[.]nvidia[.]CUDA [(](.*?)[)]')\n    smi = get_nvidia_smi()\n    return run_and_parse_first_match(run_lambda, smi,\n                                     r'Driver Version: (.*?) ')\n\n\ndef get_gpu_info(run_lambda):\n    if get_platform() == 'darwin' or (TORCH_AVAILABLE and hasattr(\n            torch.version, 'hip') and torch.version.hip is not None):\n        if TORCH_AVAILABLE and torch.cuda.is_available():\n            if torch.version.hip is not None:\n                prop = torch.cuda.get_device_properties(0)\n                if hasattr(prop, \"gcnArchName\"):\n                    gcnArch = \" ({})\".format(prop.gcnArchName)\n                else:\n                    gcnArch = \"NoGCNArchNameOnOldPyTorch\"\n            else:\n                gcnArch = \"\"\n            return torch.cuda.get_device_name(None) + gcnArch\n        return None\n    smi = get_nvidia_smi()\n    uuid_regex = re.compile(r' \\(UUID: .+?\\)')\n    rc, out, _ = run_lambda(smi + ' -L')\n    if rc != 0:\n        return None\n    # Anonymize GPUs by removing their UUID\n    return re.sub(uuid_regex, '', out)\n\n\ndef get_running_cuda_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, 'nvcc --version',\n                                     r'release .+ V(.*)')\n\n\ndef get_cudnn_version(run_lambda):\n    \"\"\"Return a list of libcudnn.so; it's hard to tell which one is being used.\"\"\"\n    if get_platform() == 'win32':\n        system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n        cuda_path = os.environ.get('CUDA_PATH', \"%CUDA_PATH%\")\n        where_cmd = os.path.join(system_root, 'System32', 'where')\n        cudnn_cmd = '{} /R \"{}\\\\bin\" cudnn*.dll'.format(where_cmd, cuda_path)\n    elif get_platform() == 'darwin':\n        # CUDA libraries and drivers can be found in /usr/local/cuda/. See\n        # https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html#install\n        # https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installmac\n        # Use CUDNN_LIBRARY when cudnn library is installed elsewhere.\n        cudnn_cmd = 'ls /usr/local/cuda/lib/libcudnn*'\n    else:\n        cudnn_cmd = 'ldconfig -p | grep libcudnn | rev | cut -d\" \" -f1 | rev'\n    rc, out, _ = run_lambda(cudnn_cmd)\n    # find will return 1 if there are permission errors or if not found\n    if len(out) == 0 or (rc != 1 and rc != 0):\n        l = os.environ.get('CUDNN_LIBRARY')\n        if l is not None and os.path.isfile(l):\n            return os.path.realpath(l)\n        return None\n    files_set = set()\n    for fn in out.split('\\n'):\n        fn = os.path.realpath(fn)  # eliminate symbolic links\n        if os.path.isfile(fn):\n            files_set.add(fn)\n    if not files_set:\n        return None\n    # Alphabetize the result because the order is non-deterministic otherwise\n    files = sorted(files_set)\n    if len(files) == 1:\n        return files[0]\n    result = '\\n'.join(files)\n    return 'Probably one of the following:\\n{}'.format(result)\n\n\ndef get_nvidia_smi():\n    # Note: nvidia-smi is currently available only on Windows and Linux\n    smi = 'nvidia-smi'\n    if get_platform() == 'win32':\n        system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n        program_files_root = os.environ.get('PROGRAMFILES',\n                                            'C:\\\\Program Files')\n        legacy_path = os.path.join(program_files_root, 'NVIDIA Corporation',\n                                   'NVSMI', smi)\n        new_path = os.path.join(system_root, 'System32', smi)\n        smis = [new_path, legacy_path]\n        for candidate_smi in smis:\n            if os.path.exists(candidate_smi):\n                smi = '\"{}\"'.format(candidate_smi)\n                break\n    return smi\n\n\ndef get_rocm_version(run_lambda):\n    \"\"\"Returns the ROCm version if available, otherwise 'N/A'.\"\"\"\n    return run_and_parse_first_match(run_lambda, 'hipcc --version',\n                                     r'HIP version: (\\S+)')\n\n\ndef get_neuron_sdk_version(run_lambda):\n    # Adapted from your install script\n    try:\n        result = run_lambda([\"neuron-ls\"])\n        return result if result[0] == 0 else 'N/A'\n    except Exception:\n        return 'N/A'\n\n\ndef get_vllm_version():\n    from vllm import __version__, __version_tuple__\n\n    if __version__ == \"dev\":\n        return \"N/A (dev)\"\n\n    if len(__version_tuple__) == 4: # dev build\n        git_sha = __version_tuple__[-1][1:] # type: ignore\n        return f\"{__version__} (git sha: {git_sha}\"\n\n    return __version__\n\ndef summarize_vllm_build_flags():\n    # This could be a static method if the flags are constant, or dynamic if you need to check environment variables, etc.\n    return 'CUDA Archs: {}; ROCm: {}; Neuron: {}'.format(\n        os.environ.get('TORCH_CUDA_ARCH_LIST', 'Not Set'),\n        'Enabled' if os.environ.get('ROCM_HOME') else 'Disabled',\n        'Enabled' if os.environ.get('NEURON_CORES') else 'Disabled',\n    )\n\n\ndef get_gpu_topo(run_lambda):\n    output = None\n\n    if get_platform() == 'linux':\n        output = run_and_read_all(run_lambda, 'nvidia-smi topo -m')\n        if output is None:\n            output = run_and_read_all(run_lambda, 'rocm-smi --showtopo')\n\n    return output\n\n\n# example outputs of CPU infos\n#  * linux\n#    Architecture:            x86_64\n#      CPU op-mode(s):        32-bit, 64-bit\n#      Address sizes:         46 bits physical, 48 bits virtual\n#      Byte Order:            Little Endian\n#    CPU(s):                  128\n#      On-line CPU(s) list:   0-127\n#    Vendor ID:               GenuineIntel\n#      Model name:            Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz\n#        CPU family:          6\n#        Model:               106\n#        Thread(s) per core:  2\n#        Core(s) per socket:  32\n#        Socket(s):           2\n#        Stepping:            6\n#        BogoMIPS:            5799.78\n#        Flags:               fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr\n#                             sse sse2 ss ht syscall nx pdpe1gb rdtscp lm constant_tsc arch_perfmon rep_good nopl\n#                             xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq monitor ssse3 fma cx16\n#                             pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand\n#                             hypervisor lahf_lm abm 3dnowprefetch invpcid_single ssbd ibrs ibpb stibp ibrs_enhanced\n#                             fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid avx512f avx512dq rdseed adx smap\n#                             avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1\n#                             xsaves wbnoinvd ida arat avx512vbmi pku ospke avx512_vbmi2 gfni vaes vpclmulqdq\n#                             avx512_vnni avx512_bitalg tme avx512_vpopcntdq rdpid md_clear flush_l1d arch_capabilities\n#    Virtualization features:\n#      Hypervisor vendor:     KVM\n#      Virtualization type:   full\n#    Caches (sum of all):\n#      L1d:                   3 MiB (64 instances)\n#      L1i:                   2 MiB (64 instances)\n#      L2:                    80 MiB (64 instances)\n#      L3:                    108 MiB (2 instances)\n#    NUMA:\n#      NUMA node(s):          2\n#      NUMA node0 CPU(s):     0-31,64-95\n#      NUMA node1 CPU(s):     32-63,96-127\n#    Vulnerabilities:\n#      Itlb multihit:         Not affected\n#      L1tf:                  Not affected\n#      Mds:                   Not affected\n#      Meltdown:              Not affected\n#      Mmio stale data:       Vulnerable: Clear CPU buffers attempted, no microcode; SMT Host state unknown\n#      Retbleed:              Not affected\n#      Spec store bypass:     Mitigation; Speculative Store Bypass disabled via prctl and seccomp\n#      Spectre v1:            Mitigation; usercopy/swapgs barriers and __user pointer sanitization\n#      Spectre v2:            Mitigation; Enhanced IBRS, IBPB conditional, RSB filling, PBRSB-eIBRS SW sequence\n#      Srbds:                 Not affected\n#      Tsx async abort:       Not affected\n#  * win32\n#    Architecture=9\n#    CurrentClockSpeed=2900\n#    DeviceID=CPU0\n#    Family=179\n#    L2CacheSize=40960\n#    L2CacheSpeed=\n#    Manufacturer=GenuineIntel\n#    MaxClockSpeed=2900\n#    Name=Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz\n#    ProcessorType=3\n#    Revision=27142\n#\n#    Architecture=9\n#    CurrentClockSpeed=2900\n#    DeviceID=CPU1\n#    Family=179\n#    L2CacheSize=40960\n#    L2CacheSpeed=\n#    Manufacturer=GenuineIntel\n#    MaxClockSpeed=2900\n#    Name=Intel(R) Xeon(R) Platinum 8375C CPU @ 2.90GHz\n#    ProcessorType=3\n#    Revision=27142\n\n\ndef get_cpu_info(run_lambda):\n    rc, out, err = 0, '', ''\n    if get_platform() == 'linux':\n        rc, out, err = run_lambda('lscpu')\n    elif get_platform() == 'win32':\n        rc, out, err = run_lambda(\n            'wmic cpu get Name,Manufacturer,Family,Architecture,ProcessorType,DeviceID, \\\n        CurrentClockSpeed,MaxClockSpeed,L2CacheSize,L2CacheSpeed,Revision /VALUE'\n        )\n    elif get_platform() == 'darwin':\n        rc, out, err = run_lambda(\"sysctl -n machdep.cpu.brand_string\")\n    cpu_info = 'None'\n    if rc == 0:\n        cpu_info = out\n    else:\n        cpu_info = err\n    return cpu_info\n\n\ndef get_platform():\n    if sys.platform.startswith('linux'):\n        return 'linux'\n    elif sys.platform.startswith('win32'):\n        return 'win32'\n    elif sys.platform.startswith('cygwin'):\n        return 'cygwin'\n    elif sys.platform.startswith('darwin'):\n        return 'darwin'\n    else:\n        return sys.platform\n\n\ndef get_mac_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, 'sw_vers -productVersion',\n                                     r'(.*)')\n\n\ndef get_windows_version(run_lambda):\n    system_root = os.environ.get('SYSTEMROOT', 'C:\\\\Windows')\n    wmic_cmd = os.path.join(system_root, 'System32', 'Wbem', 'wmic')\n    findstr_cmd = os.path.join(system_root, 'System32', 'findstr')\n    return run_and_read_all(\n        run_lambda,\n        '{} os get Caption | {} /v Caption'.format(wmic_cmd, findstr_cmd))\n\n\ndef get_lsb_version(run_lambda):\n    return run_and_parse_first_match(run_lambda, 'lsb_release -a',\n                                     r'Description:\\t(.*)')\n\n\ndef check_release_file(run_lambda):\n    return run_and_parse_first_match(run_lambda, 'cat /etc/*-release',\n                                     r'PRETTY_NAME=\"(.*)\"')\n\n\ndef get_os(run_lambda):\n    from platform import machine\n    platform = get_platform()\n\n    if platform == 'win32' or platform == 'cygwin':\n        return get_windows_version(run_lambda)\n\n    if platform == 'darwin':\n        version = get_mac_version(run_lambda)\n        if version is None:\n            return None\n        return 'macOS {} ({})'.format(version, machine())\n\n    if platform == 'linux':\n        # Ubuntu/Debian based\n        desc = get_lsb_version(run_lambda)\n        if desc is not None:\n            return '{} ({})'.format(desc, machine())\n\n        # Try reading /etc/*-release\n        desc = check_release_file(run_lambda)\n        if desc is not None:\n            return '{} ({})'.format(desc, machine())\n\n        return '{} ({})'.format(platform, machine())\n\n    # Unknown platform\n    return platform\n\n\ndef get_python_platform():\n    import platform\n    return platform.platform()\n\n\ndef get_libc_version():\n    import platform\n    if get_platform() != 'linux':\n        return 'N/A'\n    return '-'.join(platform.libc_ver())\n\n\ndef get_pip_packages(run_lambda, patterns=None):\n    \"\"\"Return `pip list` output. Note: will also find conda-installed pytorch and numpy packages.\"\"\"\n    if patterns is None:\n        patterns = DEFAULT_PIP_PATTERNS\n\n    # People generally have `pip` as `pip` or `pip3`\n    # But here it is invoked as `python -mpip`\n    def run_with_pip(pip):\n        out = run_and_read_all(run_lambda, pip + [\"list\", \"--format=freeze\"])\n        return \"\\n\".join(line for line in out.splitlines()\n                         if any(name in line for name in patterns))\n\n    pip_version = 'pip3' if sys.version[0] == '3' else 'pip'\n    out = run_with_pip([sys.executable, '-mpip'])\n\n    return pip_version, out\n\n\ndef get_cachingallocator_config():\n    ca_config = os.environ.get('PYTORCH_CUDA_ALLOC_CONF', '')\n    return ca_config\n\n\ndef get_cuda_module_loading_config():\n    if TORCH_AVAILABLE and torch.cuda.is_available():\n        torch.cuda.init()\n        config = os.environ.get('CUDA_MODULE_LOADING', '')\n        return config\n    else:\n        return \"N/A\"\n\n\ndef is_xnnpack_available():\n    if TORCH_AVAILABLE:\n        import torch.backends.xnnpack\n        return str(\n            torch.backends.xnnpack.enabled)  # type: ignore[attr-defined]\n    else:\n        return \"N/A\"\n\ndef get_env_vars():\n    env_vars = ''\n    secret_terms=('secret', 'token', 'api', 'access', 'password')\n    report_prefix = (\"TORCH\", \"NCCL\", \"PYTORCH\",\n                     \"CUDA\", \"CUBLAS\", \"CUDNN\",\n                     \"OMP_\", \"MKL_\",\n                     \"NVIDIA\")\n    for k, v in os.environ.items():\n        if any(term in k.lower() for term in secret_terms):\n            continue\n        if k in environment_variables:\n            env_vars = env_vars + \"{}={}\".format(k, v) + \"\\n\"\n        if k.startswith(report_prefix):\n            env_vars = env_vars + \"{}={}\".format(k, v) + \"\\n\"\n\n    return env_vars\n\ndef get_env_info():\n    run_lambda = run\n    pip_version, pip_list_output = get_pip_packages(run_lambda)\n\n    if TORCH_AVAILABLE:\n        version_str = torch.__version__\n        debug_mode_str = str(torch.version.debug)\n        cuda_available_str = str(torch.cuda.is_available())\n        cuda_version_str = torch.version.cuda\n        if not hasattr(torch.version,\n                       'hip') or torch.version.hip is None:  # cuda version\n            hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\n        else:  # HIP version\n\n            def get_version_or_na(cfg, prefix):\n                _lst = [s.rsplit(None, 1)[-1] for s in cfg if prefix in s]\n                return _lst[0] if _lst else 'N/A'\n\n            cfg = torch._C._show_config().split('\\n')\n            hip_runtime_version = get_version_or_na(cfg, 'HIP Runtime')\n            miopen_runtime_version = get_version_or_na(cfg, 'MIOpen')\n            cuda_version_str = 'N/A'\n            hip_compiled_version = torch.version.hip\n    else:\n        version_str = debug_mode_str = cuda_available_str = cuda_version_str = 'N/A'\n        hip_compiled_version = hip_runtime_version = miopen_runtime_version = 'N/A'\n\n    sys_version = sys.version.replace(\"\\n\", \" \")\n\n    conda_packages = get_conda_packages(run_lambda)\n\n    rocm_version = get_rocm_version(run_lambda)\n    neuron_sdk_version = get_neuron_sdk_version(run_lambda)\n    vllm_version = get_vllm_version()\n    vllm_build_flags = summarize_vllm_build_flags()\n    gpu_topo = get_gpu_topo(run_lambda)\n\n    return SystemEnv(\n        torch_version=version_str,\n        is_debug_build=debug_mode_str,\n        python_version='{} ({}-bit runtime)'.format(\n            sys_version,\n            sys.maxsize.bit_length() + 1),\n        python_platform=get_python_platform(),\n        is_cuda_available=cuda_available_str,\n        cuda_compiled_version=cuda_version_str,\n        cuda_runtime_version=get_running_cuda_version(run_lambda),\n        cuda_module_loading=get_cuda_module_loading_config(),\n        nvidia_gpu_models=get_gpu_info(run_lambda),\n        nvidia_driver_version=get_nvidia_driver_version(run_lambda),\n        cudnn_version=get_cudnn_version(run_lambda),\n        hip_compiled_version=hip_compiled_version,\n        hip_runtime_version=hip_runtime_version,\n        miopen_runtime_version=miopen_runtime_version,\n        pip_version=pip_version,\n        pip_packages=pip_list_output,\n        conda_packages=conda_packages,\n        os=get_os(run_lambda),\n        libc_version=get_libc_version(),\n        gcc_version=get_gcc_version(run_lambda),\n        clang_version=get_clang_version(run_lambda),\n        cmake_version=get_cmake_version(run_lambda),\n        caching_allocator_config=get_cachingallocator_config(),\n        is_xnnpack_available=is_xnnpack_available(),\n        cpu_info=get_cpu_info(run_lambda),\n        rocm_version=rocm_version,\n        neuron_sdk_version=neuron_sdk_version,\n        vllm_version=vllm_version,\n        vllm_build_flags=vllm_build_flags,\n        gpu_topo=gpu_topo,\n        env_vars=get_env_vars(),\n    )\n\n\nenv_info_fmt = \"\"\"\nPyTorch version: {torch_version}\nIs debug build: {is_debug_build}\nCUDA used to build PyTorch: {cuda_compiled_version}\nROCM used to build PyTorch: {hip_compiled_version}\n\nOS: {os}\nGCC version: {gcc_version}\nClang version: {clang_version}\nCMake version: {cmake_version}\nLibc version: {libc_version}\n\nPython version: {python_version}\nPython platform: {python_platform}\nIs CUDA available: {is_cuda_available}\nCUDA runtime version: {cuda_runtime_version}\nCUDA_MODULE_LOADING set to: {cuda_module_loading}\nGPU models and configuration: {nvidia_gpu_models}\nNvidia driver version: {nvidia_driver_version}\ncuDNN version: {cudnn_version}\nHIP runtime version: {hip_runtime_version}\nMIOpen runtime version: {miopen_runtime_version}\nIs XNNPACK available: {is_xnnpack_available}\n\nCPU:\n{cpu_info}\n\nVersions of relevant libraries:\n{pip_packages}\n{conda_packages}\n\"\"\".strip()\n\n# both the above code and the following code use `strip()` to\n# remove leading/trailing whitespaces, so we need to add a newline\n# in between to separate the two sections\nenv_info_fmt += \"\\n\"\n\nenv_info_fmt += \"\"\"\nROCM Version: {rocm_version}\nNeuron SDK Version: {neuron_sdk_version}\nvLLM Version: {vllm_version}\nvLLM Build Flags:\n{vllm_build_flags}\nGPU Topology:\n{gpu_topo}\n\n{env_vars}\n\"\"\".strip()\n\n\ndef pretty_str(envinfo):\n\n    def replace_nones(dct, replacement='Could not collect'):\n        for key in dct.keys():\n            if dct[key] is not None:\n                continue\n            dct[key] = replacement\n        return dct\n\n    def replace_bools(dct, true='Yes', false='No'):\n        for key in dct.keys():\n            if dct[key] is True:\n                dct[key] = true\n            elif dct[key] is False:\n                dct[key] = false\n        return dct\n\n    def prepend(text, tag='[prepend]'):\n        lines = text.split('\\n')\n        updated_lines = [tag + line for line in lines]\n        return '\\n'.join(updated_lines)\n\n    def replace_if_empty(text, replacement='No relevant packages'):\n        if text is not None and len(text) == 0:\n            return replacement\n        return text\n\n    def maybe_start_on_next_line(string):\n        # If `string` is multiline, prepend a \\n to it.\n        if string is not None and len(string.split('\\n')) > 1:\n            return '\\n{}\\n'.format(string)\n        return string\n\n    mutable_dict = envinfo._asdict()\n\n    # If nvidia_gpu_models is multiline, start on the next line\n    mutable_dict['nvidia_gpu_models'] = \\\n        maybe_start_on_next_line(envinfo.nvidia_gpu_models)\n\n    # If the machine doesn't have CUDA, report some fields as 'No CUDA'\n    dynamic_cuda_fields = [\n        'cuda_runtime_version',\n        'nvidia_gpu_models',\n        'nvidia_driver_version',\n    ]\n    all_cuda_fields = dynamic_cuda_fields + ['cudnn_version']\n    all_dynamic_cuda_fields_missing = all(mutable_dict[field] is None\n                                          for field in dynamic_cuda_fields)\n    if TORCH_AVAILABLE and not torch.cuda.is_available(\n    ) and all_dynamic_cuda_fields_missing:\n        for field in all_cuda_fields:\n            mutable_dict[field] = 'No CUDA'\n        if envinfo.cuda_compiled_version is None:\n            mutable_dict['cuda_compiled_version'] = 'None'\n\n    # Replace True with Yes, False with No\n    mutable_dict = replace_bools(mutable_dict)\n\n    # Replace all None objects with 'Could not collect'\n    mutable_dict = replace_nones(mutable_dict)\n\n    # If either of these are '', replace with 'No relevant packages'\n    mutable_dict['pip_packages'] = replace_if_empty(\n        mutable_dict['pip_packages'])\n    mutable_dict['conda_packages'] = replace_if_empty(\n        mutable_dict['conda_packages'])\n\n    # Tag conda and pip packages with a prefix\n    # If they were previously None, they'll show up as ie '[conda] Could not collect'\n    if mutable_dict['pip_packages']:\n        mutable_dict['pip_packages'] = prepend(\n            mutable_dict['pip_packages'], '[{}] '.format(envinfo.pip_version))\n    if mutable_dict['conda_packages']:\n        mutable_dict['conda_packages'] = prepend(\n            mutable_dict['conda_packages'], '[conda] ')\n    mutable_dict['cpu_info'] = envinfo.cpu_info\n    return env_info_fmt.format(**mutable_dict)\n\n\ndef get_pretty_env_info():\n    return pretty_str(get_env_info())\n\n\ndef main():\n    print(\"Collecting environment information...\")\n    output = get_pretty_env_info()\n    print(output)\n\n    if TORCH_AVAILABLE and hasattr(torch, 'utils') and hasattr(\n            torch.utils, '_crash_handler'):\n        minidump_dir = torch.utils._crash_handler.DEFAULT_MINIDUMP_DIR\n        if sys.platform == \"linux\" and os.path.exists(minidump_dir):\n            dumps = [\n                os.path.join(minidump_dir, dump)\n                for dump in os.listdir(minidump_dir)\n            ]\n            latest = max(dumps, key=os.path.getctime)\n            ctime = os.path.getctime(latest)\n            creation_time = datetime.datetime.fromtimestamp(ctime).strftime(\n                '%Y-%m-%d %H:%M:%S')\n            msg = \"\\n*** Detected a minidump at {} created on {}, \".format(latest, creation_time) + \\\n                  \"if this is related to your bug please include it when you file a report ***\"\n            print(msg, file=sys.stderr)\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "csrc",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "find_cuda_init.py",
          "type": "blob",
          "size": 0.82,
          "content": "import importlib\nimport traceback\nfrom typing import Callable\nfrom unittest.mock import patch\n\n\ndef find_cuda_init(fn: Callable[[], object]) -> None:\n    \"\"\"\n    Helper function to debug CUDA re-initialization errors.\n\n    If `fn` initializes CUDA, prints the stack trace of how this happens.\n    \"\"\"\n    from torch.cuda import _lazy_init\n\n    stack = None\n\n    def wrapper():\n        nonlocal stack\n        stack = traceback.extract_stack()\n        return _lazy_init()\n\n    with patch(\"torch.cuda._lazy_init\", wrapper):\n        fn()\n\n    if stack is not None:\n        print(\"==== CUDA Initialized ====\")\n        print(\"\".join(traceback.format_list(stack)).strip())\n        print(\"==========================\")\n\n\nif __name__ == \"__main__\":\n    find_cuda_init(\n        lambda: importlib.import_module(\"vllm.model_executor.models.llava\"))\n"
        },
        {
          "name": "format.sh",
          "type": "blob",
          "size": 9.79,
          "content": "#!/usr/bin/env bash\n# YAPF formatter, adapted from ray and skypilot.\n#\n# Usage:\n#    # Do work and commit your work.\n\n#    # Format files that differ from origin/main.\n#    bash format.sh\n\n#    # Commit changed files with message 'Run yapf and ruff'\n#\n#\n# YAPF + Clang formatter (if installed). This script formats all changed files from the last mergebase.\n# You are encouraged to run this locally before pushing changes for review.\n\n# Cause the script to exit if a single command fails\nset -eo pipefail\n\n# this stops git rev-parse from failing if we run this from the .git directory\nbuiltin cd \"$(dirname \"${BASH_SOURCE:-$0}\")\"\nROOT=\"$(git rev-parse --show-toplevel)\"\nbuiltin cd \"$ROOT\" || exit 1\n\ncheck_command() {\n    if ! command -v \"$1\" &> /dev/null; then\n        echo \"❓❓$1 is not installed, please run \\`pip install -r requirements-lint.txt\\`\"\n        exit 1\n    fi\n}\n\ncheck_command yapf\ncheck_command ruff\ncheck_command mypy\ncheck_command codespell\ncheck_command isort\ncheck_command clang-format\n\nYAPF_VERSION=$(yapf --version | awk '{print $2}')\nRUFF_VERSION=$(ruff --version | awk '{print $2}')\nMYPY_VERSION=$(mypy --version | awk '{print $2}')\nCODESPELL_VERSION=$(codespell --version)\nISORT_VERSION=$(isort --vn)\nCLANGFORMAT_VERSION=$(clang-format --version | awk '{print $3}')\nSPHINX_LINT_VERSION=$(sphinx-lint --version | awk '{print $2}')\n\n# # params: tool name, tool version, required version\ntool_version_check() {\n    expected=$(grep \"$1\" requirements-lint.txt | cut -d'=' -f3)\n    if [[ \"$2\" != \"$expected\" ]]; then\n        echo \"❓❓Wrong $1 version installed: $expected is required, not $2.\"\n        exit 1\n    fi\n}\n\ntool_version_check \"yapf\" \"$YAPF_VERSION\"\ntool_version_check \"ruff\" \"$RUFF_VERSION\"\ntool_version_check \"mypy\" \"$MYPY_VERSION\"\ntool_version_check \"isort\" \"$ISORT_VERSION\"\ntool_version_check \"codespell\" \"$CODESPELL_VERSION\"\ntool_version_check \"clang-format\" \"$CLANGFORMAT_VERSION\"\ntool_version_check \"sphinx-lint\" \"$SPHINX_LINT_VERSION\"\n\nYAPF_FLAGS=(\n    '--recursive'\n    '--parallel'\n)\n\nYAPF_EXCLUDES=(\n    '--exclude' 'build/**'\n)\n\n# Format specified files\nformat() {\n    yapf --in-place \"${YAPF_FLAGS[@]}\" \"$@\"\n}\n\n# Format files that differ from main branch. Ignores dirs that are not slated\n# for autoformat yet.\nformat_changed() {\n    # The `if` guard ensures that the list of filenames is not empty, which\n    # could cause yapf to receive 0 positional arguments, making it hang\n    # waiting for STDIN.\n    #\n    # `diff-filter=ACM` and $MERGEBASE is to ensure we only format files that\n    # exist on both branches.\n    MERGEBASE=\"$(git merge-base origin/main HEAD)\"\n\n    if ! git diff --diff-filter=ACM --quiet --exit-code \"$MERGEBASE\" -- '*.py' '*.pyi' &>/dev/null; then\n        git diff --name-only --diff-filter=ACM \"$MERGEBASE\" -- '*.py' '*.pyi' | xargs -P 5 \\\n             yapf --in-place \"${YAPF_EXCLUDES[@]}\" \"${YAPF_FLAGS[@]}\"\n    fi\n\n}\n\n# Format all files\nformat_all() {\n    yapf --in-place \"${YAPF_FLAGS[@]}\" \"${YAPF_EXCLUDES[@]}\" .\n}\n\n## This flag formats individual files. --files *must* be the first command line\n## arg to use this option.\nif [[ \"$1\" == '--files' ]]; then\n   format \"${@:2}\"\n   # If `--all` is passed, then any further arguments are ignored and the\n   # entire python directory is formatted.\nelif [[ \"$1\" == '--all' ]]; then\n   format_all\nelse\n   # Format only the files that changed in last commit.\n   format_changed\nfi\necho 'vLLM yapf: Done'\n\n# Run mypy\necho 'vLLM mypy:'\ntools/mypy.sh\necho 'vLLM mypy: Done'\n\n\n# If git diff returns a file that is in the skip list, the file may be checked anyway:\n# https://github.com/codespell-project/codespell/issues/1915\n# Avoiding the \"./\" prefix and using \"/**\" globs for directories appears to solve the problem\nCODESPELL_EXCLUDES=(\n    '--skip' 'tests/prompts/**,./benchmarks/sonnet.txt,*tests/lora/data/**,build/**'\n)\n\n# check spelling of specified files\nspell_check() {\n    codespell \"$@\"\n}\n\nspell_check_all(){\n  codespell --toml pyproject.toml \"${CODESPELL_EXCLUDES[@]}\"\n}\n\n# Spelling check of files that differ from main branch.\nspell_check_changed() {\n    # The `if` guard ensures that the list of filenames is not empty, which\n    # could cause ruff to receive 0 positional arguments, making it hang\n    # waiting for STDIN.\n    #\n    # `diff-filter=ACM` and $MERGEBASE is to ensure we only lint files that\n    # exist on both branches.\n    MERGEBASE=\"$(git merge-base origin/main HEAD)\"\n    if ! git diff --diff-filter=ACM --quiet --exit-code \"$MERGEBASE\" -- '*.py' '*.pyi' &>/dev/null; then\n        git diff --name-only --diff-filter=ACM \"$MERGEBASE\" -- '*.py' '*.pyi' | xargs \\\n            codespell \"${CODESPELL_EXCLUDES[@]}\"\n    fi\n}\n\n# Run Codespell\n## This flag runs spell check of individual files. --files *must* be the first command line\n## arg to use this option.\nif [[ \"$1\" == '--files' ]]; then\n   spell_check \"${@:2}\"\n   # If `--all` is passed, then any further arguments are ignored and the\n   # entire python directory is linted.\nelif [[ \"$1\" == '--all' ]]; then\n   spell_check_all\nelse\n   # Check spelling only of the files that changed in last commit.\n   spell_check_changed\nfi\necho 'vLLM codespell: Done'\n\n\n# Lint specified files\nlint() {\n    ruff check \"$@\"\n}\n\n# Lint files that differ from main branch. Ignores dirs that are not slated\n# for autolint yet.\nlint_changed() {\n    # The `if` guard ensures that the list of filenames is not empty, which\n    # could cause ruff to receive 0 positional arguments, making it hang\n    # waiting for STDIN.\n    #\n    # `diff-filter=ACM` and $MERGEBASE is to ensure we only lint files that\n    # exist on both branches.\n    MERGEBASE=\"$(git merge-base origin/main HEAD)\"\n\n    if ! git diff --diff-filter=ACM --quiet --exit-code \"$MERGEBASE\" -- '*.py' '*.pyi' &>/dev/null; then\n        git diff --name-only --diff-filter=ACM \"$MERGEBASE\" -- '*.py' '*.pyi' | xargs \\\n             ruff check\n    fi\n\n}\n\n# Run Ruff\n### This flag lints individual files. --files *must* be the first command line\n### arg to use this option.\nif [[ \"$1\" == '--files' ]]; then\n   lint \"${@:2}\"\n   # If `--all` is passed, then any further arguments are ignored and the\n   # entire python directory is linted.\nelif [[ \"$1\" == '--all' ]]; then\n   lint vllm tests\nelse\n   # Format only the files that changed in last commit.\n   lint_changed\nfi\necho 'vLLM ruff: Done'\n\n# check spelling of specified files\nisort_check() {\n    isort \"$@\"\n}\n\nisort_check_all(){\n  isort .\n}\n\n# Spelling  check of files that differ from main branch.\nisort_check_changed() {\n    # The `if` guard ensures that the list of filenames is not empty, which\n    # could cause ruff to receive 0 positional arguments, making it hang\n    # waiting for STDIN.\n    #\n    # `diff-filter=ACM` and $MERGEBASE is to ensure we only lint files that\n    # exist on both branches.\n    MERGEBASE=\"$(git merge-base origin/main HEAD)\"\n\n    if ! git diff --diff-filter=ACM --quiet --exit-code \"$MERGEBASE\" -- '*.py' '*.pyi' &>/dev/null; then\n        git diff --name-only --diff-filter=ACM \"$MERGEBASE\" -- '*.py' '*.pyi' | xargs \\\n             isort\n    fi\n}\n\n# Run Isort\n# This flag runs spell check of individual files. --files *must* be the first command line\n# arg to use this option.\nif [[ \"$1\" == '--files' ]]; then\n   isort_check \"${@:2}\"\n   # If `--all` is passed, then any further arguments are ignored and the\n   # entire python directory is linted.\nelif [[ \"$1\" == '--all' ]]; then\n   isort_check_all\nelse\n   # Check spelling only of the files that changed in last commit.\n   isort_check_changed\nfi\necho 'vLLM isort: Done'\n\n# Clang-format section\n# Exclude some files for formatting because they are vendored\n# NOTE: Keep up to date with .github/workflows/clang-format.yml\nCLANG_FORMAT_EXCLUDES=(\n    'csrc/moe/topk_softmax_kernels.cu'\n    'csrc/quantization/gguf/ggml-common.h'\n    'csrc/quantization/gguf/dequantize.cuh'\n    'csrc/quantization/gguf/vecdotq.cuh'\n    'csrc/quantization/gguf/mmq.cuh'\n    'csrc/quantization/gguf/mmvq.cuh'\n)\n\n# Format specified files with clang-format\nclang_format() {\n    clang-format -i \"$@\"\n}\n\n# Format files that differ from main branch with clang-format.\nclang_format_changed() {\n    # The `if` guard ensures that the list of filenames is not empty, which\n    # could cause clang-format to receive 0 positional arguments, making it hang\n    # waiting for STDIN.\n    #\n    # `diff-filter=ACM` and $MERGEBASE is to ensure we only format files that\n    # exist on both branches.\n    MERGEBASE=\"$(git merge-base origin/main HEAD)\"\n\n    # Get the list of changed files, excluding the specified ones\n    changed_files=$(git diff --name-only --diff-filter=ACM \"$MERGEBASE\" -- '*.h' '*.cpp' '*.cu' '*.cuh' | (grep -vFf <(printf \"%s\\n\" \"${CLANG_FORMAT_EXCLUDES[@]}\") || echo -e))\n    if [ -n \"$changed_files\" ]; then\n        echo \"$changed_files\" | xargs -P 5 clang-format -i\n    fi\n}\n\n# Format all files with clang-format\nclang_format_all() {\n    find csrc/ \\( -name '*.h' -o -name '*.cpp' -o -name '*.cu' -o -name '*.cuh' \\) -print \\\n        | grep -vFf <(printf \"%s\\n\" \"${CLANG_FORMAT_EXCLUDES[@]}\") \\\n        | xargs clang-format -i\n}\n\n# Run clang-format\nif [[ \"$1\" == '--files' ]]; then\n   clang_format \"${@:2}\"\nelif [[ \"$1\" == '--all' ]]; then\n   clang_format_all\nelse\n   clang_format_changed\nfi\necho 'vLLM clang-format: Done'\n\necho 'vLLM actionlint:'\ntools/actionlint.sh -color\necho 'vLLM actionlint: Done'\n\necho 'vLLM shellcheck:'\ntools/shellcheck.sh\necho 'vLLM shellcheck: Done'\n\necho 'excalidraw png check:'\ntools/png-lint.sh\necho 'excalidraw png check: Done'\n\nif ! git diff --quiet &>/dev/null; then\n    echo \n    echo \"🔍🔍There are files changed by the format checker or by you that are not added and committed:\"\n    git --no-pager diff --name-only\n    echo \"🔍🔍Format checker passed, but please add, commit and push all the files above to include changes made by the format checker.\"\n\n    exit 1\nelse\n    echo \"✨🎉 Format check passed! Congratulations! 🎉✨\"\nfi\n\necho 'vLLM sphinx-lint:'\ntools/sphinx-lint.sh\necho 'vLLM sphinx-lint: Done'\n"
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 2.46,
          "content": "[build-system]\n# Should be mirrored in requirements-build.txt\nrequires = [\n    \"cmake>=3.26\",\n    \"ninja\",\n    \"packaging\",\n    \"setuptools>=61\",\n    \"setuptools-scm>=8.0\",\n    \"torch == 2.5.1\",\n    \"wheel\",\n    \"jinja2\",\n]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.setuptools_scm]\n# version_file = \"vllm/_version.py\" # currently handled by `setup.py:get_version()`\n\n[tool.ruff]\n# Allow lines to be as long as 80.\nline-length = 80\nexclude = [\n    # External file, leaving license intact\n    \"examples/other/fp8/quantizer/quantize.py\"\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"vllm/version.py\" = [\"F401\"]\n\"vllm/_version.py\" = [\"ALL\"]\n\n[tool.ruff.lint]\nselect = [\n    # pycodestyle\n    \"E\",\n    # Pyflakes\n    \"F\",\n    # pyupgrade\n    \"UP\",\n    # flake8-bugbear\n    \"B\",\n    # flake8-simplify\n    \"SIM\",\n    # isort\n    # \"I\",\n    \"G\",\n]\nignore = [\n    # star imports\n    \"F405\", \"F403\",\n    # lambda expression assignment\n    \"E731\",\n    # Loop control variable not used within loop body\n    \"B007\",\n    # f-string format\n    \"UP032\",\n]\n\n[tool.mypy]\nignore_missing_imports = true\ncheck_untyped_defs = true\nfollow_imports = \"silent\"\n\n# After fixing type errors resulting from follow_imports: \"skip\" -> \"silent\",\n# move the directory here and remove it from tools/mypy.sh\nfiles = [\n    \"vllm/*.py\",\n    \"vllm/adapter_commons\",\n    \"vllm/assets\",\n    \"vllm/entrypoints\",\n    \"vllm/core\",\n    \"vllm/inputs\",\n    \"vllm/logging_utils\",\n    \"vllm/multimodal\",\n    \"vllm/platforms\",\n    \"vllm/transformers_utils\",\n    \"vllm/triton_utils\",\n    \"vllm/usage\",\n]\n# TODO(woosuk): Include the code from Megatron and HuggingFace.\nexclude = [\n    \"vllm/model_executor/parallel_utils/|vllm/model_executor/models/\",\n    # Ignore triton kernels in ops.\n    'vllm/attention/ops/.*\\.py$'\n]\n\n[tool.codespell]\nignore-words-list = \"dout, te, indicies, subtile, ElementE\"\nskip = \"./tests/models/fixtures,./tests/prompts,./benchmarks/sonnet.txt,./tests/lora/data,./build\"\n\n[tool.isort]\nuse_parentheses = true\nskip_gitignore = true\n\n[tool.pytest.ini_options]\nmarkers = [\n    \"skip_global_cleanup\",\n    \"core_model: enable this model test in each PR instead of only nightly\",\n    \"cpu_model: enable this model test in CPU tests\",\n    \"quant_model: run this model test under Quantized category\",\n    \"split: run this test as part of a split\",\n    \"distributed: run this test only in distributed GPU tests\",\n    \"skip_v1: do not run this test with v1\",\n    \"optional: optional tests that are automatically skipped, include --optional to run them\",\n]\n"
        },
        {
          "name": "python_only_dev.py",
          "type": "blob",
          "size": 0.56,
          "content": "msg = \"\"\"Old style python only build (without compilation) is deprecated, please check https://docs.vllm.ai/en/latest/getting_started/installation.html#python-only-build-without-compilation for the new way to do python only build (without compilation).\n\nTL;DR:\n\nVLLM_USE_PRECOMPILED=1 pip install -e .\n\nor\n\nexport VLLM_COMMIT=33f460b17a54acb3b6cc0b03f4a17876cff5eafd # use full commit hash from the main branch\nexport VLLM_PRECOMPILED_WHEEL_LOCATION=https://wheels.vllm.ai/${VLLM_COMMIT}/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\npip install -e .\n\"\"\" # noqa\n\nprint(msg)\n"
        },
        {
          "name": "requirements-build.txt",
          "type": "blob",
          "size": 0.12,
          "content": "# Should be mirrored in pyproject.toml\ncmake>=3.26\nninja\npackaging\nsetuptools>=61\nsetuptools-scm>=8\ntorch==2.5.1\nwheel\njinja2\n"
        },
        {
          "name": "requirements-common.txt",
          "type": "blob",
          "size": 1.69,
          "content": "psutil\nsentencepiece  # Required for LLaMA tokenizer.\nnumpy < 2.0.0\nrequests >= 2.26.0\ntqdm\nblake3\npy-cpuinfo\ntransformers >= 4.45.2  # Required for Llama 3.2 and Qwen2-VL.\ntokenizers >= 0.19.1  # Required for Llama 3.\nprotobuf # Required by LlamaTokenizer.\nfastapi >= 0.107.0, < 0.113.0; python_version < '3.9'\nfastapi >= 0.107.0, != 0.113.*, != 0.114.0; python_version >= '3.9'\naiohttp\nopenai >= 1.52.0 # Ensure modern openai package (ensure types module present and max_completion_tokens field support)\nuvicorn[standard]\npydantic >= 2.9  # Required for fastapi >= 0.113.0\nprometheus_client >= 0.18.0\npillow  # Required for image processing\nprometheus-fastapi-instrumentator >= 7.0.0\ntiktoken >= 0.6.0  # Required for DBRX tokenizer\nlm-format-enforcer >= 0.10.9, < 0.11\noutlines == 0.1.11 # Requires pytorch\nlark == 1.2.2 \nxgrammar >= 0.1.6; platform_machine == \"x86_64\"\ntyping_extensions >= 4.10\nfilelock >= 3.16.1 # need to contain https://github.com/tox-dev/filelock/pull/317\npartial-json-parser # used for parsing partial JSON outputs\npyzmq\nmsgspec\ngguf == 0.10.0\nimportlib_metadata\nmistral_common[opencv] >= 1.5.0\npyyaml\nsix>=1.16.0; python_version > '3.11' # transitive dependency of pandas that needs to be the latest version for python 3.12\nsetuptools>=74.1.1; python_version > '3.11' # Setuptools is used by triton, we need to ensure a modern version is installed for 3.12+ so that it does not try to import distutils, which was removed in 3.12\neinops # Required for Qwen2-VL.\ncompressed-tensors == 0.8.1 # required for compressed-tensors, requires pytorch\ndepyf==0.18.0 # required for profiling and debugging with compilation config\ncloudpickle # allows pickling lambda functions in model_executor/models/registry.py\n"
        },
        {
          "name": "requirements-cpu.txt",
          "type": "blob",
          "size": 0.41,
          "content": "# Common dependencies\n-r requirements-common.txt\n\n# Dependencies for CPUs\ntorch==2.5.1+cpu; platform_machine != \"ppc64le\" and platform_machine != \"aarch64\" and platform_system != \"Darwin\"\ntorch==2.5.1; platform_machine == \"aarch64\" or platform_system == \"Darwin\" \ntorchvision; platform_machine != \"ppc64le\"  # required for the image processor of phi3v, this must be updated alongside torch\ndatasets # for benchmark scripts\n"
        },
        {
          "name": "requirements-cuda.txt",
          "type": "blob",
          "size": 0.45,
          "content": "# Common dependencies\n-r requirements-common.txt\n\n# Dependencies for NVIDIA GPUs\nray[default] >= 2.9\nnvidia-ml-py >= 12.560.30 # for pynvml package\ntorch == 2.5.1\n# These must be updated alongside torch\ntorchvision == 0.20.1 # Required for phi3v processor. See https://github.com/pytorch/vision?tab=readme-ov-file#installation for corresponding version\nxformers == 0.0.28.post3; platform_system == 'Linux' and platform_machine == 'x86_64'  # Requires PyTorch 2.5.1\n"
        },
        {
          "name": "requirements-dev.txt",
          "type": "blob",
          "size": 0.15,
          "content": "-r requirements-lint.txt\n-r requirements-test.txt\n\n# Avoid adding requirements directly to this file.\n# Instead, modify the two files referenced above.\n"
        },
        {
          "name": "requirements-hpu.txt",
          "type": "blob",
          "size": 0.22,
          "content": "# Common dependencies\n-r requirements-common.txt\n\n# Dependencies for HPU code\nray\ntriton\npandas\ntabulate\nsetuptools>=61\nsetuptools-scm>=8\nvllm-hpu-extension @ git+https://github.com/HabanaAI/vllm-hpu-extension.git@4312768\n"
        },
        {
          "name": "requirements-lint.txt",
          "type": "blob",
          "size": 0.21,
          "content": "# formatting\nyapf==0.32.0\ntoml==0.10.2\ntomli==2.0.2\nruff==0.6.5\ncodespell==2.3.0\nisort==5.13.2\nclang-format==18.1.5\nsphinx-lint==1.0.0\n\n# type checking\nmypy==1.11.1\ntypes-PyYAML\ntypes-requests\ntypes-setuptools\n"
        },
        {
          "name": "requirements-neuron.txt",
          "type": "blob",
          "size": 0.15,
          "content": "# Common dependencies\n-r requirements-common.txt\n\n# Dependencies for Neuron devices\ntransformers-neuronx >= 0.13.0\ntorch-neuronx >= 2.5.0\nneuronx-cc\n"
        },
        {
          "name": "requirements-openvino.txt",
          "type": "blob",
          "size": 0.45,
          "content": "# Common dependencies\n-r requirements-common.txt\n\ntorch == 2.5.1 #  should be aligned with \"common\" vLLM torch version\nopenvino >= 2024.4.0 # since 2024.4.0 both CPU and GPU support Paged Attention\n\noptimum @ git+https://github.com/huggingface/optimum.git # latest optimum is used to support latest transformers version\noptimum-intel[nncf] @ git+https://github.com/huggingface/optimum-intel.git # latest optimum-intel is used to support latest transformers version\n"
        },
        {
          "name": "requirements-rocm.txt",
          "type": "blob",
          "size": 0.16,
          "content": "# Common dependencies\n-r requirements-common.txt\n\n# Dependencies for AMD GPUs\nawscli\nboto3\nbotocore\ndatasets\nray >= 2.10.0\npeft\npytest-asyncio\ntensorizer>=2.9.0\n"
        },
        {
          "name": "requirements-test.in",
          "type": "blob",
          "size": 0.74,
          "content": "# testing\npytest\ntensorizer>=2.9.0\npytest-forked\npytest-asyncio\npytest-rerunfailures\npytest-shard\n\n# testing utils\nawscli\ndecord # required for video tests\neinops # required for MPT, qwen-vl and Mamba\nhttpx\nlibrosa # required for audio tests\npeft\npqdm\nray[adag]==2.40.0\nsentence-transformers # required for embedding tests\nsoundfile # required for audio tests\ntimm # required for internvl test\ntorch==2.5.1\ntransformers_stream_generator # required for qwen-vl test\nmatplotlib # required for qwen-vl test\nmistral_common[opencv] >= 1.5.0 # required for pixtral test\ndatamodel_code_generator # required for minicpm3 test\nlm-eval[api]==0.4.4 # required for model evaluation test\n\n# quantization\nbitsandbytes>=0.45.0\nbuildkite-test-collector==0.1.9\n\nnumpy < 2.0.0\n"
        },
        {
          "name": "requirements-test.txt",
          "type": "blob",
          "size": 10.67,
          "content": "#\n# This file is autogenerated by pip-compile with Python 3.12\n# by the following command:\n#\n#    python3.12 -m piptools compile requirements-test.in -o requirements-test.txt\n#\nabsl-py==2.1.0\n    # via rouge-score\naccelerate==1.0.1\n    # via\n    #   lm-eval\n    #   peft\naiohappyeyeballs==2.4.3\n    # via aiohttp\naiohttp==3.10.10\n    # via\n    #   datasets\n    #   fsspec\n    #   lm-eval\naiosignal==1.3.1\n    # via\n    #   aiohttp\n    #   ray\nannotated-types==0.7.0\n    # via pydantic\nanyio==4.6.2.post1\n    # via httpx\nargcomplete==3.5.1\n    # via datamodel-code-generator\nattrs==24.2.0\n    # via\n    #   aiohttp\n    #   jsonlines\n    #   jsonschema\n    #   referencing\naudioread==3.0.1\n    # via librosa\nawscli==1.35.23\n    # via -r requirements-test.in\nbitsandbytes>=0.45.0\n    # via -r requirements-test.in\nblack==24.10.0\n    # via datamodel-code-generator\nboto3==1.35.57\n    # via tensorizer\nbotocore==1.35.57\n    # via\n    #   awscli\n    #   boto3\n    #   s3transfer\nbounded-pool-executor==0.0.3\n    # via pqdm\nbuildkite-test-collector==0.1.9\n    # via -r requirements-test.in\ncertifi==2024.8.30\n    # via\n    #   httpcore\n    #   httpx\n    #   requests\ncffi==1.17.1\n    # via soundfile\nchardet==5.2.0\n    # via mbstrdecoder\ncharset-normalizer==3.4.0\n    # via requests\nclick==8.1.7\n    # via\n    #   black\n    #   nltk\n    #   ray\ncolorama==0.4.6\n    # via\n    #   awscli\n    #   sacrebleu\n    #   tqdm-multiprocess\ncontourpy==1.3.0\n    # via matplotlib\ncupy-cuda12x==13.3.0\n    # via ray\ncycler==0.12.1\n    # via matplotlib\ndatamodel-code-generator==0.26.3\n    # via -r requirements-test.in\ndataproperty==1.0.1\n    # via\n    #   pytablewriter\n    #   tabledata\ndatasets==3.0.2\n    # via\n    #   evaluate\n    #   lm-eval\ndecorator==5.1.1\n    # via librosa\ndecord==0.6.0\n    # via -r requirements-test.in\ndill==0.3.8\n    # via\n    #   datasets\n    #   evaluate\n    #   lm-eval\n    #   multiprocess\ndnspython==2.7.0\n    # via email-validator\ndocutils==0.16\n    # via awscli\neinops==0.8.0\n    # via -r requirements-test.in\nemail-validator==2.2.0\n    # via pydantic\nevaluate==0.4.3\n    # via lm-eval\nfastrlock==0.8.2\n    # via cupy-cuda12x\nfilelock==3.16.1\n    # via\n    #   datasets\n    #   huggingface-hub\n    #   ray\n    #   torch\n    #   transformers\n    #   triton\nfonttools==4.54.1\n    # via matplotlib\nfrozenlist==1.5.0\n    # via\n    #   aiohttp\n    #   aiosignal\n    #   ray\nfsspec[http]==2024.9.0\n    # via\n    #   datasets\n    #   evaluate\n    #   huggingface-hub\n    #   torch\ngenson==1.3.0\n    # via datamodel-code-generator\nh11==0.14.0\n    # via httpcore\nhiredis==3.0.0\n    # via tensorizer\nhttpcore==1.0.6\n    # via httpx\nhttpx==0.27.2\n    # via -r requirements-test.in\nhuggingface-hub==0.26.2\n    # via\n    #   accelerate\n    #   datasets\n    #   evaluate\n    #   peft\n    #   sentence-transformers\n    #   timm\n    #   tokenizers\n    #   transformers\nidna==3.10\n    # via\n    #   anyio\n    #   email-validator\n    #   httpx\n    #   requests\n    #   yarl\ninflect==5.6.2\n    # via datamodel-code-generator\niniconfig==2.0.0\n    # via pytest\nisort==5.13.2\n    # via datamodel-code-generator\njinja2==3.1.4\n    # via\n    #   datamodel-code-generator\n    #   torch\njmespath==1.0.1\n    # via\n    #   boto3\n    #   botocore\njoblib==1.4.2\n    # via\n    #   librosa\n    #   nltk\n    #   scikit-learn\njsonlines==4.0.0\n    # via lm-eval\njsonschema==4.23.0\n    # via\n    #   mistral-common\n    #   ray\njsonschema-specifications==2024.10.1\n    # via jsonschema\nkiwisolver==1.4.7\n    # via matplotlib\nlazy-loader==0.4\n    # via librosa\nlibnacl==2.1.0\n    # via tensorizer\nlibrosa==0.10.2.post1\n    # via -r requirements-test.in\nllvmlite==0.43.0\n    # via numba\nlm-eval[api]==0.4.4\n    # via -r requirements-test.in\nlxml==5.3.0\n    # via sacrebleu\nmarkupsafe==3.0.2\n    # via jinja2\nmatplotlib==3.9.2\n    # via -r requirements-test.in\nmbstrdecoder==1.1.3\n    # via\n    #   dataproperty\n    #   pytablewriter\n    #   typepy\nmistral-common[opencv]==1.5.1\n    # via\n    #   -r requirements-test.in\n    #   mistral-common\nmore-itertools==10.5.0\n    # via lm-eval\nmpmath==1.3.0\n    # via sympy\nmsgpack==1.1.0\n    # via\n    #   librosa\n    #   ray\nmultidict==6.1.0\n    # via\n    #   aiohttp\n    #   yarl\nmultiprocess==0.70.16\n    # via\n    #   datasets\n    #   evaluate\nmypy-extensions==1.0.0\n    # via black\nnetworkx==3.2.1\n    # via torch\nnltk==3.9.1\n    # via rouge-score\nnumba==0.60.0\n    # via librosa\nnumexpr==2.10.1\n    # via lm-eval\nnumpy==1.26.4\n    # via\n    #   -r requirements-test.in\n    #   accelerate\n    #   bitsandbytes\n    #   contourpy\n    #   cupy-cuda12x\n    #   datasets\n    #   decord\n    #   evaluate\n    #   librosa\n    #   matplotlib\n    #   mistral-common\n    #   numba\n    #   numexpr\n    #   opencv-python-headless\n    #   pandas\n    #   peft\n    #   rouge-score\n    #   sacrebleu\n    #   scikit-learn\n    #   scipy\n    #   soxr\n    #   tensorizer\n    #   torchvision\n    #   transformers\nnvidia-cublas-cu12==12.4.5.8\n    # via\n    #   nvidia-cudnn-cu12\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-cuda-cupti-cu12==12.4.127\n    # via torch\nnvidia-cuda-nvrtc-cu12==12.4.127\n    # via torch\nnvidia-cuda-runtime-cu12==12.4.127\n    # via torch\nnvidia-cudnn-cu12==9.1.0.70\n    # via torch\nnvidia-cufft-cu12==11.2.1.3\n    # via torch\nnvidia-curand-cu12==10.3.5.147\n    # via torch\nnvidia-cusolver-cu12==11.6.1.9\n    # via torch\nnvidia-cusparse-cu12==12.3.1.170\n    # via\n    #   nvidia-cusolver-cu12\n    #   torch\nnvidia-nccl-cu12==2.21.5\n    # via torch\nnvidia-nvjitlink-cu12==12.4.127\n    # via\n    #   nvidia-cusolver-cu12\n    #   nvidia-cusparse-cu12\n    #   torch\nnvidia-nvtx-cu12==12.4.127\n    # via torch\nopencv-python-headless==4.10.0.84\n    # via mistral-common\npackaging==24.1\n    # via\n    #   accelerate\n    #   black\n    #   datamodel-code-generator\n    #   datasets\n    #   evaluate\n    #   huggingface-hub\n    #   lazy-loader\n    #   matplotlib\n    #   peft\n    #   pooch\n    #   pytest\n    #   pytest-rerunfailures\n    #   ray\n    #   transformers\n    #   typepy\npandas==2.2.3\n    # via\n    #   datasets\n    #   evaluate\npathspec==0.12.1\n    # via black\npathvalidate==3.2.1\n    # via pytablewriter\npeft==0.13.2\n    # via\n    #   -r requirements-test.in\n    #   lm-eval\npillow==10.4.0\n    # via\n    #   matplotlib\n    #   mistral-common\n    #   sentence-transformers\n    #   torchvision\nplatformdirs==4.3.6\n    # via\n    #   black\n    #   pooch\npluggy==1.5.0\n    # via pytest\npooch==1.8.2\n    # via librosa\nportalocker==2.10.1\n    # via sacrebleu\npqdm==0.2.0\n    # via -r requirements-test.in\npropcache==0.2.0\n    # via yarl\nprotobuf==5.28.3\n    # via\n    #   ray\n    #   tensorizer\npsutil==6.1.0\n    # via\n    #   accelerate\n    #   peft\n    #   tensorizer\npy==1.11.0\n    # via pytest-forked\npyarrow==18.0.0\n    # via datasets\npyasn1==0.6.1\n    # via rsa\npybind11==2.13.6\n    # via lm-eval\npycparser==2.22\n    # via cffi\npydantic[email]==2.9.2\n    # via\n    #   datamodel-code-generator\n    #   mistral-common\npydantic-core==2.23.4\n    # via pydantic\npyparsing==3.2.0\n    # via matplotlib\npytablewriter==1.2.0\n    # via lm-eval\npytest==8.3.3\n    # via\n    #   -r requirements-test.in\n    #   buildkite-test-collector\n    #   pytest-asyncio\n    #   pytest-forked\n    #   pytest-rerunfailures\n    #   pytest-shard\npytest-asyncio==0.24.0\n    # via -r requirements-test.in\npytest-forked==1.6.0\n    # via -r requirements-test.in\npytest-rerunfailures==14.0\n    # via -r requirements-test.in\npytest-shard==0.1.2\n    # via -r requirements-test.in\npython-dateutil==2.9.0.post0\n    # via\n    #   botocore\n    #   matplotlib\n    #   pandas\n    #   typepy\npytz==2024.2\n    # via\n    #   pandas\n    #   typepy\npyyaml==6.0.2\n    # via\n    #   accelerate\n    #   awscli\n    #   datamodel-code-generator\n    #   datasets\n    #   huggingface-hub\n    #   peft\n    #   ray\n    #   timm\n    #   transformers\nray[adag]==2.40.0\n    # via -r requirements-test.in\nredis==5.2.0\n    # via tensorizer\nreferencing==0.35.1\n    # via\n    #   jsonschema\n    #   jsonschema-specifications\nregex==2024.9.11\n    # via\n    #   nltk\n    #   sacrebleu\n    #   tiktoken\n    #   transformers\nrequests==2.32.3\n    # via\n    #   buildkite-test-collector\n    #   datasets\n    #   evaluate\n    #   huggingface-hub\n    #   lm-eval\n    #   mistral-common\n    #   pooch\n    #   ray\n    #   tiktoken\n    #   transformers\nrouge-score==0.1.2\n    # via lm-eval\nrpds-py==0.20.1\n    # via\n    #   jsonschema\n    #   referencing\nrsa==4.7.2\n    # via awscli\ns3transfer==0.10.3\n    # via\n    #   awscli\n    #   boto3\nsacrebleu==2.4.3\n    # via lm-eval\nsafetensors==0.4.5\n    # via\n    #   accelerate\n    #   peft\n    #   timm\n    #   transformers\nscikit-learn==1.5.2\n    # via\n    #   librosa\n    #   lm-eval\n    #   sentence-transformers\nscipy==1.13.1\n    # via\n    #   librosa\n    #   scikit-learn\n    #   sentence-transformers\nsentence-transformers==3.2.1\n    # via -r requirements-test.in\nsentencepiece==0.2.0\n    # via mistral-common\nsix==1.16.0\n    # via\n    #   python-dateutil\n    #   rouge-score\nsniffio==1.3.1\n    # via\n    #   anyio\n    #   httpx\nsoundfile==0.12.1\n    # via\n    #   -r requirements-test.in\n    #   librosa\nsoxr==0.5.0.post1\n    # via librosa\nsqlitedict==2.1.0\n    # via lm-eval\nsympy==1.13.1\n    # via torch\ntabledata==1.3.3\n    # via pytablewriter\ntabulate==0.9.0\n    # via sacrebleu\ntcolorpy==0.1.6\n    # via pytablewriter\ntenacity==9.0.0\n    # via lm-eval\ntensorizer==2.9.0\n    # via -r requirements-test.in\nthreadpoolctl==3.5.0\n    # via scikit-learn\ntiktoken==0.7.0\n    # via\n    #   lm-eval\n    #   mistral-common\ntimm==1.0.11\n    # via -r requirements-test.in\ntokenizers==0.21.0\n    # via transformers\ntorch==2.5.1\n    # via\n    #   -r requirements-test.in\n    #   accelerate\n    #   bitsandbytes\n    #   lm-eval\n    #   peft\n    #   sentence-transformers\n    #   tensorizer\n    #   timm\n    #   torchvision\ntorchvision==0.20.1\n    # via timm\ntqdm==4.66.6\n    # via\n    #   datasets\n    #   evaluate\n    #   huggingface-hub\n    #   lm-eval\n    #   nltk\n    #   peft\n    #   sentence-transformers\n    #   tqdm-multiprocess\n    #   transformers\ntqdm-multiprocess==0.0.11\n    # via lm-eval\ntransformers==4.47.0\n    # via\n    #   lm-eval\n    #   peft\n    #   sentence-transformers\n    #   transformers-stream-generator\ntransformers-stream-generator==0.0.5\n    # via -r requirements-test.in\ntriton==3.1.0\n    # via torch\ntypepy[datetime]==1.3.2\n    # via\n    #   dataproperty\n    #   pytablewriter\n    #   tabledata\ntyping-extensions==4.12.2\n    # via\n    #   huggingface-hub\n    #   librosa\n    #   mistral-common\n    #   pydantic\n    #   pydantic-core\n    #   torch\ntzdata==2024.2\n    # via pandas\nurllib3==1.26.20\n    # via\n    #   botocore\n    #   requests\nword2number==1.1\n    # via lm-eval\nxxhash==3.5.0\n    # via\n    #   datasets\n    #   evaluate\nyarl==1.17.1\n    # via aiohttp\nzstandard==0.23.0\n    # via lm-eval\n\n# The following packages are considered to be unsafe in a requirements file:\n# setuptools\n"
        },
        {
          "name": "requirements-tpu.txt",
          "type": "blob",
          "size": 1.06,
          "content": "# Common dependencies\n-r requirements-common.txt\n\n# Dependencies for TPU\ncmake>=3.26\nninja\npackaging\nsetuptools-scm>=8\nwheel\njinja2\nray[default]\n\n# Install torch_xla\n--pre\n--extra-index-url https://download.pytorch.org/whl/nightly/cpu\n--find-links https://storage.googleapis.com/libtpu-releases/index.html\n--find-links https://storage.googleapis.com/jax-releases/jax_nightly_releases.html\n--find-links https://storage.googleapis.com/jax-releases/jaxlib_nightly_releases.html\ntorch==2.6.0.dev20241126+cpu\ntorchvision==0.20.0.dev20241126+cpu\ntorch_xla[tpu] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.6.0.dev20241126-cp39-cp39-linux_x86_64.whl ; python_version == \"3.9\"\ntorch_xla[tpu] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.6.0.dev20241126-cp310-cp310-linux_x86_64.whl ; python_version == \"3.10\"\ntorch_xla[tpu] @ https://storage.googleapis.com/pytorch-xla-releases/wheels/tpuvm/torch_xla-2.6.0.dev20241126-cp311-cp311-linux_x86_64.whl ; python_version == \"3.11\"\njaxlib==0.4.36.dev20241122\njax==0.4.36.dev20241122\n"
        },
        {
          "name": "requirements-xpu.txt",
          "type": "blob",
          "size": 0.6,
          "content": "# Common dependencies\n-r requirements-common.txt\n\nray >= 2.9\ncmake>=3.26\nninja\npackaging\nsetuptools-scm>=8\nwheel\njinja2\n\ntorch @ https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/ipex_dev/xpu/torch-2.5.0a0%2Bgite84e33f-cp310-cp310-linux_x86_64.whl\nintel-extension-for-pytorch @ https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/ipex_dev/xpu/intel_extension_for_pytorch-2.5.10%2Bgit9d489a8-cp310-cp310-linux_x86_64.whl\noneccl_bind_pt @ https://intel-optimized-pytorch.s3.cn-north-1.amazonaws.com.cn/ipex_dev/xpu/oneccl_bind_pt-2.5.0%2Bxpu-cp310-cp310-linux_x86_64.whl\n\ntriton-xpu == 3.0.0b1\n"
        },
        {
          "name": "setup.py",
          "type": "blob",
          "size": 22.84,
          "content": "import ctypes\nimport importlib.util\nimport logging\nimport os\nimport re\nimport subprocess\nimport sys\nfrom pathlib import Path\nfrom shutil import which\nfrom typing import Dict, List\n\nimport torch\nfrom packaging.version import Version, parse\nfrom setuptools import Extension, find_packages, setup\nfrom setuptools.command.build_ext import build_ext\nfrom setuptools_scm import get_version\nfrom torch.utils.cpp_extension import CUDA_HOME, ROCM_HOME\n\n\ndef load_module_from_path(module_name, path):\n    spec = importlib.util.spec_from_file_location(module_name, path)\n    module = importlib.util.module_from_spec(spec)\n    sys.modules[module_name] = module\n    spec.loader.exec_module(module)\n    return module\n\n\nROOT_DIR = os.path.dirname(__file__)\nlogger = logging.getLogger(__name__)\n\n# cannot import envs directly because it depends on vllm,\n#  which is not installed yet\nenvs = load_module_from_path('envs', os.path.join(ROOT_DIR, 'vllm', 'envs.py'))\n\nVLLM_TARGET_DEVICE = envs.VLLM_TARGET_DEVICE\n\nif sys.platform.startswith(\"darwin\") and VLLM_TARGET_DEVICE != \"cpu\":\n    logger.warning(\n        \"VLLM_TARGET_DEVICE automatically set to `cpu` due to macOS\")\n    VLLM_TARGET_DEVICE = \"cpu\"\nelif not (sys.platform.startswith(\"linux\")\n          or sys.platform.startswith(\"darwin\")):\n    logger.warning(\n        \"vLLM only supports Linux platform (including WSL) and MacOS.\"\n        \"Building on %s, \"\n        \"so vLLM may not be able to run correctly\", sys.platform)\n    VLLM_TARGET_DEVICE = \"empty\"\n\nMAIN_CUDA_VERSION = \"12.1\"\n\n\ndef is_sccache_available() -> bool:\n    return which(\"sccache\") is not None\n\n\ndef is_ccache_available() -> bool:\n    return which(\"ccache\") is not None\n\n\ndef is_ninja_available() -> bool:\n    return which(\"ninja\") is not None\n\n\nclass CMakeExtension(Extension):\n\n    def __init__(self, name: str, cmake_lists_dir: str = '.', **kwa) -> None:\n        super().__init__(name, sources=[], py_limited_api=True, **kwa)\n        self.cmake_lists_dir = os.path.abspath(cmake_lists_dir)\n\n\nclass cmake_build_ext(build_ext):\n    # A dict of extension directories that have been configured.\n    did_config: Dict[str, bool] = {}\n\n    #\n    # Determine number of compilation jobs and optionally nvcc compile threads.\n    #\n    def compute_num_jobs(self):\n        # `num_jobs` is either the value of the MAX_JOBS environment variable\n        # (if defined) or the number of CPUs available.\n        num_jobs = envs.MAX_JOBS\n        if num_jobs is not None:\n            num_jobs = int(num_jobs)\n            logger.info(\"Using MAX_JOBS=%d as the number of jobs.\", num_jobs)\n        else:\n            try:\n                # os.sched_getaffinity() isn't universally available, so fall\n                #  back to os.cpu_count() if we get an error here.\n                num_jobs = len(os.sched_getaffinity(0))\n            except AttributeError:\n                num_jobs = os.cpu_count()\n\n        nvcc_threads = None\n        if _is_cuda() and get_nvcc_cuda_version() >= Version(\"11.2\"):\n            # `nvcc_threads` is either the value of the NVCC_THREADS\n            # environment variable (if defined) or 1.\n            # when it is set, we reduce `num_jobs` to avoid\n            # overloading the system.\n            nvcc_threads = envs.NVCC_THREADS\n            if nvcc_threads is not None:\n                nvcc_threads = int(nvcc_threads)\n                logger.info(\n                    \"Using NVCC_THREADS=%d as the number of nvcc threads.\",\n                    nvcc_threads)\n            else:\n                nvcc_threads = 1\n            num_jobs = max(1, num_jobs // nvcc_threads)\n\n        return num_jobs, nvcc_threads\n\n    #\n    # Perform cmake configuration for a single extension.\n    #\n    def configure(self, ext: CMakeExtension) -> None:\n        # If we've already configured using the CMakeLists.txt for\n        # this extension, exit early.\n        if ext.cmake_lists_dir in cmake_build_ext.did_config:\n            return\n\n        cmake_build_ext.did_config[ext.cmake_lists_dir] = True\n\n        # Select the build type.\n        # Note: optimization level + debug info are set by the build type\n        default_cfg = \"Debug\" if self.debug else \"RelWithDebInfo\"\n        cfg = envs.CMAKE_BUILD_TYPE or default_cfg\n\n        cmake_args = [\n            '-DCMAKE_BUILD_TYPE={}'.format(cfg),\n            '-DVLLM_TARGET_DEVICE={}'.format(VLLM_TARGET_DEVICE),\n        ]\n\n        verbose = envs.VERBOSE\n        if verbose:\n            cmake_args += ['-DCMAKE_VERBOSE_MAKEFILE=ON']\n\n        if is_sccache_available():\n            cmake_args += [\n                '-DCMAKE_C_COMPILER_LAUNCHER=sccache',\n                '-DCMAKE_CXX_COMPILER_LAUNCHER=sccache',\n                '-DCMAKE_CUDA_COMPILER_LAUNCHER=sccache',\n                '-DCMAKE_HIP_COMPILER_LAUNCHER=sccache',\n            ]\n        elif is_ccache_available():\n            cmake_args += [\n                '-DCMAKE_C_COMPILER_LAUNCHER=ccache',\n                '-DCMAKE_CXX_COMPILER_LAUNCHER=ccache',\n                '-DCMAKE_CUDA_COMPILER_LAUNCHER=ccache',\n                '-DCMAKE_HIP_COMPILER_LAUNCHER=ccache',\n            ]\n\n        # Pass the python executable to cmake so it can find an exact\n        # match.\n        cmake_args += ['-DVLLM_PYTHON_EXECUTABLE={}'.format(sys.executable)]\n\n        # Pass the python path to cmake so it can reuse the build dependencies\n        # on subsequent calls to python.\n        cmake_args += ['-DVLLM_PYTHON_PATH={}'.format(\":\".join(sys.path))]\n\n        # Override the base directory for FetchContent downloads to $ROOT/.deps\n        # This allows sharing dependencies between profiles,\n        # and plays more nicely with sccache.\n        # To override this, set the FETCHCONTENT_BASE_DIR environment variable.\n        fc_base_dir = os.path.join(ROOT_DIR, \".deps\")\n        fc_base_dir = os.environ.get(\"FETCHCONTENT_BASE_DIR\", fc_base_dir)\n        cmake_args += ['-DFETCHCONTENT_BASE_DIR={}'.format(fc_base_dir)]\n\n        #\n        # Setup parallelism and build tool\n        #\n        num_jobs, nvcc_threads = self.compute_num_jobs()\n\n        if nvcc_threads:\n            cmake_args += ['-DNVCC_THREADS={}'.format(nvcc_threads)]\n\n        if is_ninja_available():\n            build_tool = ['-G', 'Ninja']\n            cmake_args += [\n                '-DCMAKE_JOB_POOL_COMPILE:STRING=compile',\n                '-DCMAKE_JOB_POOLS:STRING=compile={}'.format(num_jobs),\n            ]\n        else:\n            # Default build tool to whatever cmake picks.\n            build_tool = []\n        subprocess.check_call(\n            ['cmake', ext.cmake_lists_dir, *build_tool, *cmake_args],\n            cwd=self.build_temp)\n\n    def build_extensions(self) -> None:\n        # Ensure that CMake is present and working\n        try:\n            subprocess.check_output(['cmake', '--version'])\n        except OSError as e:\n            raise RuntimeError('Cannot find CMake executable') from e\n\n        # Create build directory if it does not exist.\n        if not os.path.exists(self.build_temp):\n            os.makedirs(self.build_temp)\n\n        targets = []\n\n        def target_name(s: str) -> str:\n            return s.removeprefix(\"vllm.\").removeprefix(\"vllm_flash_attn.\")\n\n        # Build all the extensions\n        for ext in self.extensions:\n            self.configure(ext)\n            targets.append(target_name(ext.name))\n\n        num_jobs, _ = self.compute_num_jobs()\n\n        build_args = [\n            \"--build\",\n            \".\",\n            f\"-j={num_jobs}\",\n            *[f\"--target={name}\" for name in targets],\n        ]\n\n        subprocess.check_call([\"cmake\", *build_args], cwd=self.build_temp)\n\n        # Install the libraries\n        for ext in self.extensions:\n            # Install the extension into the proper location\n            outdir = Path(self.get_ext_fullpath(ext.name)).parent.absolute()\n\n            # Skip if the install directory is the same as the build directory\n            if outdir == self.build_temp:\n                continue\n\n            # CMake appends the extension prefix to the install path,\n            # and outdir already contains that prefix, so we need to remove it.\n            prefix = outdir\n            for i in range(ext.name.count('.')):\n                prefix = prefix.parent\n\n            # prefix here should actually be the same for all components\n            install_args = [\n                \"cmake\", \"--install\", \".\", \"--prefix\", prefix, \"--component\",\n                target_name(ext.name)\n            ]\n            subprocess.check_call(install_args, cwd=self.build_temp)\n\n    def run(self):\n        # First, run the standard build_ext command to compile the extensions\n        super().run()\n\n        # copy vllm/vllm_flash_attn/*.py from self.build_lib to current\n        # directory so that they can be included in the editable build\n        import glob\n        files = glob.glob(\n            os.path.join(self.build_lib, \"vllm\", \"vllm_flash_attn\", \"*.py\"))\n        for file in files:\n            dst_file = os.path.join(\"vllm/vllm_flash_attn\",\n                                    os.path.basename(file))\n            print(f\"Copying {file} to {dst_file}\")\n            self.copy_file(file, dst_file)\n\n\nclass repackage_wheel(build_ext):\n    \"\"\"Extracts libraries and other files from an existing wheel.\"\"\"\n    default_wheel = \"https://wheels.vllm.ai/nightly/vllm-1.0.0.dev-cp38-abi3-manylinux1_x86_64.whl\"\n\n    def run(self) -> None:\n        wheel_location = os.getenv(\"VLLM_PRECOMPILED_WHEEL_LOCATION\",\n                                   self.default_wheel)\n\n        assert _is_cuda(\n        ), \"VLLM_USE_PRECOMPILED is only supported for CUDA builds\"\n\n        import zipfile\n\n        if os.path.isfile(wheel_location):\n            wheel_path = wheel_location\n            print(f\"Using existing wheel={wheel_path}\")\n        else:\n            # Download the wheel from a given URL, assume\n            # the filename is the last part of the URL\n            wheel_filename = wheel_location.split(\"/\")[-1]\n\n            import tempfile\n\n            # create a temporary directory to store the wheel\n            temp_dir = tempfile.mkdtemp(prefix=\"vllm-wheels\")\n            wheel_path = os.path.join(temp_dir, wheel_filename)\n\n            print(f\"Downloading wheel from {wheel_location} to {wheel_path}\")\n\n            from urllib.request import urlretrieve\n\n            try:\n                urlretrieve(wheel_location, filename=wheel_path)\n            except Exception as e:\n                from setuptools.errors import SetupError\n\n                raise SetupError(\n                    f\"Failed to get vLLM wheel from {wheel_location}\") from e\n\n        with zipfile.ZipFile(wheel_path) as wheel:\n            files_to_copy = [\n                \"vllm/_C.abi3.so\",\n                \"vllm/_moe_C.abi3.so\",\n                \"vllm/vllm_flash_attn/vllm_flash_attn_c.abi3.so\",\n                \"vllm/vllm_flash_attn/flash_attn_interface.py\",\n                \"vllm/vllm_flash_attn/__init__.py\",\n                # \"vllm/_version.py\", # not available in nightly wheels yet\n            ]\n            file_members = filter(lambda x: x.filename in files_to_copy,\n                                  wheel.filelist)\n\n            for file in file_members:\n                print(f\"Extracting and including {file.filename} \"\n                      \"from existing wheel\")\n                package_name = os.path.dirname(file.filename).replace(\"/\", \".\")\n                file_name = os.path.basename(file.filename)\n\n                if package_name not in package_data:\n                    package_data[package_name] = []\n\n                wheel.extract(file)\n                if file_name.endswith(\".py\"):\n                    # python files shouldn't be added to package_data\n                    continue\n\n                package_data[package_name].append(file_name)\n\n\ndef _is_hpu() -> bool:\n    is_hpu_available = True\n    try:\n        subprocess.run([\"hl-smi\"], capture_output=True, check=True)\n    except (FileNotFoundError, PermissionError, subprocess.CalledProcessError):\n        if not os.path.exists('/dev/accel/accel0') and not os.path.exists(\n                '/dev/accel/accel_controlD0'):\n            # last resort...\n            try:\n                output = subprocess.check_output(\n                    'lsmod | grep habanalabs | wc -l', shell=True)\n                is_hpu_available = int(output) > 0\n            except (ValueError, FileNotFoundError, PermissionError,\n                    subprocess.CalledProcessError):\n                is_hpu_available = False\n    return is_hpu_available or VLLM_TARGET_DEVICE == \"hpu\"\n\n\ndef _no_device() -> bool:\n    return VLLM_TARGET_DEVICE == \"empty\"\n\n\ndef _is_cuda() -> bool:\n    has_cuda = torch.version.cuda is not None\n    return (VLLM_TARGET_DEVICE == \"cuda\" and has_cuda\n            and not (_is_neuron() or _is_tpu() or _is_hpu()))\n\n\ndef _is_hip() -> bool:\n    return (VLLM_TARGET_DEVICE == \"cuda\"\n            or VLLM_TARGET_DEVICE == \"rocm\") and torch.version.hip is not None\n\n\ndef _is_neuron() -> bool:\n    torch_neuronx_installed = True\n    try:\n        subprocess.run([\"neuron-ls\"], capture_output=True, check=True)\n    except (FileNotFoundError, PermissionError, subprocess.CalledProcessError):\n        torch_neuronx_installed = False\n    return torch_neuronx_installed or VLLM_TARGET_DEVICE == \"neuron\"\n\n\ndef _is_tpu() -> bool:\n    return VLLM_TARGET_DEVICE == \"tpu\"\n\n\ndef _is_cpu() -> bool:\n    return VLLM_TARGET_DEVICE == \"cpu\"\n\n\ndef _is_openvino() -> bool:\n    return VLLM_TARGET_DEVICE == \"openvino\"\n\n\ndef _is_xpu() -> bool:\n    return VLLM_TARGET_DEVICE == \"xpu\"\n\n\ndef _build_custom_ops() -> bool:\n    return _is_cuda() or _is_hip() or _is_cpu()\n\n\ndef get_rocm_version():\n    # Get the Rocm version from the ROCM_HOME/bin/librocm-core.so\n    # see https://github.com/ROCm/rocm-core/blob/d11f5c20d500f729c393680a01fa902ebf92094b/rocm_version.cpp#L21\n    try:\n        librocm_core_file = Path(ROCM_HOME) / \"lib\" / \"librocm-core.so\"\n        if not librocm_core_file.is_file():\n            return None\n        librocm_core = ctypes.CDLL(librocm_core_file)\n        VerErrors = ctypes.c_uint32\n        get_rocm_core_version = librocm_core.getROCmVersion\n        get_rocm_core_version.restype = VerErrors\n        get_rocm_core_version.argtypes = [\n            ctypes.POINTER(ctypes.c_uint32),\n            ctypes.POINTER(ctypes.c_uint32),\n            ctypes.POINTER(ctypes.c_uint32),\n        ]\n        major = ctypes.c_uint32()\n        minor = ctypes.c_uint32()\n        patch = ctypes.c_uint32()\n\n        if (get_rocm_core_version(ctypes.byref(major), ctypes.byref(minor),\n                                  ctypes.byref(patch)) == 0):\n            return \"%d.%d.%d\" % (major.value, minor.value, patch.value)\n        return None\n    except Exception:\n        return None\n\n\ndef get_neuronxcc_version():\n    import sysconfig\n    site_dir = sysconfig.get_paths()[\"purelib\"]\n    version_file = os.path.join(site_dir, \"neuronxcc\", \"version\",\n                                \"__init__.py\")\n\n    # Check if the command was executed successfully\n    with open(version_file) as fp:\n        content = fp.read()\n\n    # Extract the version using a regular expression\n    match = re.search(r\"__version__ = '(\\S+)'\", content)\n    if match:\n        # Return the version string\n        return match.group(1)\n    else:\n        raise RuntimeError(\"Could not find Neuron version in the output\")\n\n\ndef get_nvcc_cuda_version() -> Version:\n    \"\"\"Get the CUDA version from nvcc.\n\n    Adapted from https://github.com/NVIDIA/apex/blob/8b7a1ff183741dd8f9b87e7bafd04cfde99cea28/setup.py\n    \"\"\"\n    assert CUDA_HOME is not None, \"CUDA_HOME is not set\"\n    nvcc_output = subprocess.check_output([CUDA_HOME + \"/bin/nvcc\", \"-V\"],\n                                          universal_newlines=True)\n    output = nvcc_output.split()\n    release_idx = output.index(\"release\") + 1\n    nvcc_cuda_version = parse(output[release_idx].split(\",\")[0])\n    return nvcc_cuda_version\n\n\ndef get_path(*filepath) -> str:\n    return os.path.join(ROOT_DIR, *filepath)\n\n\ndef get_gaudi_sw_version():\n    \"\"\"\n    Returns the driver version.\n    \"\"\"\n    # Enable console printing for `hl-smi` check\n    output = subprocess.run(\"hl-smi\",\n                            shell=True,\n                            text=True,\n                            capture_output=True,\n                            env={\"ENABLE_CONSOLE\": \"true\"})\n    if output.returncode == 0 and output.stdout:\n        return output.stdout.split(\"\\n\")[2].replace(\n            \" \", \"\").split(\":\")[1][:-1].split(\"-\")[0]\n    return \"0.0.0\"  # when hl-smi is not available\n\n\ndef get_vllm_version() -> str:\n    # TODO: Revisit this temporary approach: https://github.com/vllm-project/vllm/issues/9182#issuecomment-2404860236\n    try:\n        version = get_version(\n            write_to=\"vllm/_version.py\",  # TODO: move this to pyproject.toml\n        )\n    except LookupError:\n        version = \"0.0.0\"\n\n    sep = \"+\" if \"+\" not in version else \".\"  # dev versions might contain +\n\n    if _no_device():\n        if envs.VLLM_TARGET_DEVICE == \"empty\":\n            version += f\"{sep}empty\"\n    elif _is_cuda():\n        if envs.VLLM_USE_PRECOMPILED:\n            version += f\"{sep}precompiled\"\n        else:\n            cuda_version = str(get_nvcc_cuda_version())\n            if cuda_version != MAIN_CUDA_VERSION:\n                cuda_version_str = cuda_version.replace(\".\", \"\")[:3]\n                # skip this for source tarball, required for pypi\n                if \"sdist\" not in sys.argv:\n                    version += f\"{sep}cu{cuda_version_str}\"\n    elif _is_hip():\n        # Get the Rocm Version\n        rocm_version = get_rocm_version() or torch.version.hip\n        if rocm_version and rocm_version != MAIN_CUDA_VERSION:\n            version += f\"{sep}rocm{rocm_version.replace('.', '')[:3]}\"\n    elif _is_neuron():\n        # Get the Neuron version\n        neuron_version = str(get_neuronxcc_version())\n        if neuron_version != MAIN_CUDA_VERSION:\n            neuron_version_str = neuron_version.replace(\".\", \"\")[:3]\n            version += f\"{sep}neuron{neuron_version_str}\"\n    elif _is_hpu():\n        # Get the Intel Gaudi Software Suite version\n        gaudi_sw_version = str(get_gaudi_sw_version())\n        if gaudi_sw_version != MAIN_CUDA_VERSION:\n            gaudi_sw_version = gaudi_sw_version.replace(\".\", \"\")[:3]\n            version += f\"{sep}gaudi{gaudi_sw_version}\"\n    elif _is_openvino():\n        version += f\"{sep}openvino\"\n    elif _is_tpu():\n        version += f\"{sep}tpu\"\n    elif _is_cpu():\n        version += f\"{sep}cpu\"\n    elif _is_xpu():\n        version += f\"{sep}xpu\"\n    else:\n        raise RuntimeError(\"Unknown runtime environment\")\n\n    return version\n\n\ndef read_readme() -> str:\n    \"\"\"Read the README file if present.\"\"\"\n    p = get_path(\"README.md\")\n    if os.path.isfile(p):\n        with open(get_path(\"README.md\"), encoding=\"utf-8\") as f:\n            return f.read()\n    else:\n        return \"\"\n\n\ndef get_requirements() -> List[str]:\n    \"\"\"Get Python package dependencies from requirements.txt.\"\"\"\n\n    def _read_requirements(filename: str) -> List[str]:\n        with open(get_path(filename)) as f:\n            requirements = f.read().strip().split(\"\\n\")\n        resolved_requirements = []\n        for line in requirements:\n            if line.startswith(\"-r \"):\n                resolved_requirements += _read_requirements(line.split()[1])\n            elif line.startswith(\"--\"):\n                continue\n            else:\n                resolved_requirements.append(line)\n        return resolved_requirements\n\n    if _no_device():\n        requirements = _read_requirements(\"requirements-cuda.txt\")\n    elif _is_cuda():\n        requirements = _read_requirements(\"requirements-cuda.txt\")\n        cuda_major, cuda_minor = torch.version.cuda.split(\".\")\n        modified_requirements = []\n        for req in requirements:\n            if (\"vllm-flash-attn\" in req\n                    and not (cuda_major == \"12\" and cuda_minor == \"1\")):\n                # vllm-flash-attn is built only for CUDA 12.1.\n                # Skip for other versions.\n                continue\n            modified_requirements.append(req)\n        requirements = modified_requirements\n    elif _is_hip():\n        requirements = _read_requirements(\"requirements-rocm.txt\")\n    elif _is_neuron():\n        requirements = _read_requirements(\"requirements-neuron.txt\")\n    elif _is_hpu():\n        requirements = _read_requirements(\"requirements-hpu.txt\")\n    elif _is_openvino():\n        requirements = _read_requirements(\"requirements-openvino.txt\")\n    elif _is_tpu():\n        requirements = _read_requirements(\"requirements-tpu.txt\")\n    elif _is_cpu():\n        requirements = _read_requirements(\"requirements-cpu.txt\")\n    elif _is_xpu():\n        requirements = _read_requirements(\"requirements-xpu.txt\")\n    else:\n        raise ValueError(\n            \"Unsupported platform, please use CUDA, ROCm, Neuron, HPU, \"\n            \"OpenVINO, or CPU.\")\n    return requirements\n\n\next_modules = []\n\nif _is_cuda() or _is_hip():\n    ext_modules.append(CMakeExtension(name=\"vllm._moe_C\"))\n\nif _is_hip():\n    ext_modules.append(CMakeExtension(name=\"vllm._rocm_C\"))\n\nif _is_cuda():\n    ext_modules.append(\n        CMakeExtension(name=\"vllm.vllm_flash_attn.vllm_flash_attn_c\"))\n\nif _build_custom_ops():\n    ext_modules.append(CMakeExtension(name=\"vllm._C\"))\n\npackage_data = {\n    \"vllm\": [\"py.typed\", \"model_executor/layers/fused_moe/configs/*.json\"]\n}\n\nif _no_device():\n    ext_modules = []\n\nif not ext_modules:\n    cmdclass = {}\nelse:\n    cmdclass = {\n        \"build_ext\":\n        repackage_wheel if envs.VLLM_USE_PRECOMPILED else cmake_build_ext\n    }\n\nsetup(\n    name=\"vllm\",\n    version=get_vllm_version(),\n    author=\"vLLM Team\",\n    license=\"Apache 2.0\",\n    description=(\"A high-throughput and memory-efficient inference and \"\n                 \"serving engine for LLMs\"),\n    long_description=read_readme(),\n    long_description_content_type=\"text/markdown\",\n    url=\"https://github.com/vllm-project/vllm\",\n    project_urls={\n        \"Homepage\": \"https://github.com/vllm-project/vllm\",\n        \"Documentation\": \"https://vllm.readthedocs.io/en/latest/\",\n    },\n    classifiers=[\n        \"Programming Language :: Python :: 3.9\",\n        \"Programming Language :: Python :: 3.10\",\n        \"Programming Language :: Python :: 3.11\",\n        \"Programming Language :: Python :: 3.12\",\n        \"License :: OSI Approved :: Apache Software License\",\n        \"Intended Audience :: Developers\",\n        \"Intended Audience :: Information Technology\",\n        \"Intended Audience :: Science/Research\",\n        \"Topic :: Scientific/Engineering :: Artificial Intelligence\",\n        \"Topic :: Scientific/Engineering :: Information Analysis\",\n    ],\n    packages=find_packages(exclude=(\"benchmarks\", \"csrc\", \"docs\", \"examples\",\n                                    \"tests*\")),\n    python_requires=\">=3.9\",\n    install_requires=get_requirements(),\n    ext_modules=ext_modules,\n    extras_require={\n        \"tensorizer\": [\"tensorizer>=2.9.0\"],\n        \"runai\": [\"runai-model-streamer\", \"runai-model-streamer-s3\", \"boto3\"],\n        \"audio\": [\"librosa\", \"soundfile\"],  # Required for audio processing\n        \"video\": [\"decord\"]  # Required for video processing\n    },\n    cmdclass=cmdclass,\n    package_data=package_data,\n    entry_points={\n        \"console_scripts\": [\n            \"vllm=vllm.scripts:main\",\n        ],\n    },\n)\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "use_existing_torch.py",
          "type": "blob",
          "size": 0.53,
          "content": "import glob\n\nrequires_files = glob.glob('requirements*.txt')\nrequires_files += [\"pyproject.toml\"]\nfor file in requires_files:\n    print(f\">>> cleaning {file}\")\n    with open(file) as f:\n        lines = f.readlines()\n    if \"torch\" in \"\".join(lines).lower():\n        print(\"removed:\")\n        with open(file, 'w') as f:\n            for line in lines:\n                if 'torch' not in line.lower():\n                    f.write(line)\n                else:\n                    print(line.strip())\n    print(f\"<<< done cleaning {file}\")\n    print()\n"
        },
        {
          "name": "vllm",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}