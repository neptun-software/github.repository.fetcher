{
  "metadata": {
    "timestamp": 1736566047266,
    "page": 31,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjQw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "alibaba/MNN",
      "stars": 8914,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 5.6982421875,
          "content": "\n# Created by https://www.gitignore.io/api/python,cmake,xcode,androidstudio\n# Edit at https://www.gitignore.io/?templates=python,cmake,xcode,androidstudio\n\n### AndroidStudio ###\n# Covers files to be ignored for android development using Android Studio.\n\n# Built application files\n*.apk\n*.ap_\n\n# Files for the ART/Dalvik VM\n*.dex\n\n# Java class files\n*.class\n\n# Generated files\nbin/\ngen/\nout/\n\n# Gradle files\n.gradle\n.gradle/\nbuild/\nbuildvisionOs/\n\n# Signing files\n.signing/\n\n# Local configuration file (sdk path, etc)\nlocal.properties\n\n# Proguard folder generated by Eclipse\nproguard/\n\n# Log Files\n*.log\n\n# Android Studio\n/*/build/\n/*/local.properties\n/*/out\n/*/*/build\n/*/*/production\ncaptures/\n.navigation/\n*.ipr\n*~\n*.swp\n\n# Android Patch\ngen-external-apklibs\n\n# External native build folder generated in Android Studio 2.2 and later\n.externalNativeBuild\n\n# NDK\nobj/\n\n# IntelliJ IDEA\n*.iml\n*.iws\n/out/\n\n# User-specific configurations\n.idea/caches/\n.idea/libraries/\n.idea/shelf/\n.idea/workspace.xml\n.idea/tasks.xml\n.idea/.name\n.idea/compiler.xml\n.idea/copyright/profiles_settings.xml\n.idea/encodings.xml\n.idea/misc.xml\n.idea/modules.xml\n.idea/scopes/scope_settings.xml\n.idea/dictionaries\n.idea/vcs.xml\n.idea/jsLibraryMappings.xml\n.idea/datasources.xml\n.idea/dataSources.ids\n.idea/sqlDataSources.xml\n.idea/dynamic.xml\n.idea/uiDesigner.xml\n.idea/assetWizardSettings.xml\n\n# OS-specific files\n.DS_Store\n.DS_Store?\n._*\n.Spotlight-V100\n.Trashes\nehthumbs.db\nThumbs.db\n\n# Legacy Eclipse project files\n.classpath\n.project\n.cproject\n.settings/\n\n# Mobile Tools for Java (J2ME)\n.mtj.tmp/\n\n# Package Files #\n*.war\n*.ear\n\n# virtual machine crash logs (Reference: http://www.java.com/en/download/help/error_hotspot.xml)\nhs_err_pid*\n\n## Plugin-specific files:\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Mongo Explorer plugin\n.idea/mongoSettings.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n### AndroidStudio Patch ###\n\n!/gradle/wrapper/gradle-wrapper.jar\n\n### CMake ###\nCMakeLists.txt.user\nCMakeCache.txt\nCMakeFiles\nCMakeScripts\nTesting\nMakefile\ncmake_install.cmake\ninstall_manifest.txt\ncompile_commands.json\nCTestTestfile.cmake\n\n### Python ###\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[od]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\nlocal_settings.py\ndb.sqlite3\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# celery beat schedule file\ncelerybeat-schedule\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n### Python Patch ###\n.venv/\n\n### Xcode ###\n# Xcode\n#\n# gitignore contributors: remember to update Global/Xcode.gitignore, Objective-C.gitignore & Swift.gitignore\n\n## User settings\nxcuserdata/\n\n## compatibility with Xcode 8 and earlier (ignoring not required starting Xcode 9)\n*.xcscmblueprint\n*.xccheckout\n\n## compatibility with Xcode 3 and earlier (ignoring not required starting Xcode 4)\nDerivedData/\n*.moved-aside\n*.pbxuser\n!default.pbxuser\n*.mode1v3\n!default.mode1v3\n*.mode2v3\n!default.mode2v3\n*.perspectivev3\n!default.perspectivev3\n\n### Xcode Patch ###\n*.xcodeproj/*\n!*.xcodeproj/project.pbxproj\n!*.xcodeproj/xcshareddata/\n!*.xcworkspace/contents.xcworkspacedata\n/*.gcno\n**/xcshareddata/WorkspaceSettings.xcsettings\nPods\n\n# End of https://www.gitignore.io/api/python,cmake,xcode,androidstudio\n\n\n\n### VSCode\n.vscode\n.tags\n\n### Build Result\nbuild32\nbuild64\nbuild.mac/\n\n### Projects\n*.podspec.json\ndemo/android/.idea\ndemo/android/.idea/gradle.xml\ndemo/android/.idea/misc.xml\ndemo/android/.idea/runConfigurations.xml\ndemo/android/.idea/vcs.xml\ndemo/android/.idea/caches/build_file_checksums.ser\ndemo/android/app/libs/\nproject/android/.idea/.name\nproject/android/.idea/gradle.xml\nproject/android/.idea/misc.xml\nproject/android/.idea/modules.xml\nproject/android/.idea/runConfigurations.xml\nproject/android/.idea/vcs.xml\nproject/android/.idea/caches/build_file_checksums.ser\n\n### Temps\n3rd_party/flatbuffers/tmp\n# FIXME(haijing): Xcode pre-build stage breaks compilation of flatbuffers by setting envs that do cmake cross-compilation for iOS\n# schema/current\nschema/private\ntools/converter/source/IR\nbenchmark/benchmark.txt\n\n### Python MNN\npymnn/android/build/\npymnn/android/local.properties\npymnn/android/.idea\npymnn/android/.idea/.name\npymnn/android/.idea/gradle.xml\npymnn/android/.idea/misc.xml\npymnn/android/.idea/modules.xml\npymnn/android/.idea/runConfigurations.xml\npymnn/android/.idea/vcs.xml\npymnn/android/.idea/caches/build_file_checksums.ser\npymnn/src/pybind_private/\n\nbuildios\nbuild*/\ninclude/MNN/VCS.h\nsource/backend/opengl/AllShader.cpp\ninclude/MNN/backend/opengl/shaders/AllShader.h\n.idea\nproject/ios/ios_64\nproject/ios/ios_32\nproject/ios/MNN.framework\n\npymnn_build/\n\n# mnncompress generated\nMNN_compression_pb2.py\n"
        },
        {
          "name": ".readthedocs.yaml",
          "type": "blob",
          "size": 1.00390625,
          "content": "# Read the Docs configuration file for Sphinx projects\n# See https://docs.readthedocs.io/en/stable/config-file/v2.html for details\n\n# Required\nversion: 2\n\n# Set the OS, Python version and other tools you might need\nbuild:\n  os: ubuntu-22.04\n  tools:\n    python: \"3.12\"\n    # You can also specify other tool versions:\n    # nodejs: \"20\"\n    # rust: \"1.70\"\n    # golang: \"1.20\"\n\n# Build documentation in the \"docs/\" directory with Sphinx\nsphinx:\n  configuration: docs/conf.py\n  # You can configure Sphinx to use a different builder, for instance use the dirhtml builder for simpler URLs\n  # builder: \"dirhtml\"\n  # Fail on all warnings to avoid broken references\n  # fail_on_warning: true\n\n# Optionally build your docs in additional formats such as PDF and ePub\n# formats:\n#   - pdf\n#   - epub\n\n# Optional but recommended, declare the Python requirements required\n# to build your documentation\n# See https://docs.readthedocs.io/en/stable/guides/reproducible-builds.html\npython:\n  install:\n    - requirements: docs/requirements.txt\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 5.7548828125,
          "content": "git:\n  depth: 3\n  quiet: true\nmatrix:\n  include:\n    - os: osx\n      language: cpp\n      osx_image: xcode11.2\n      compiler: clang\n      script:\n        - ./ciscripts/macOS/CPU_Metal.sh\n      name: \"macOS11.2 | CPU_Metal\"\n      env:\n        - MNNCITARGET=MACOSCPUMETAL\n    - os: osx\n      language: cpp\n      osx_image: xcode11.2\n      compiler: clang\n      script:\n        - ./ciscripts/macOS/CPU.sh\n      name: \"macOS11.2 | CPU\"\n      env:\n        - MNNCITARGET=MACOSCPU\n    - os: osx\n      language: cpp\n      osx_image: xcode11.2\n      compiler: clang\n      script:\n        - ./ciscripts/iOS/Xcode.sh\n      name: \"iOS | CPU_Metal | Xcode\"\n      env:\n        - MNNCITARGET=IOSCPUMETALXCODE\n    - os: osx\n      language: cpp\n      osx_image: xcode11.2\n      compiler: clang\n      script:\n        - ./ciscripts/iOS/CMake.sh\n      name: \"iOS | CPU_Metal | CMake\"\n      env:\n        - MNNCITARGET=IOSCPUMETALCMAKE\n    - os: linux\n      sudo: required\n      dist: bionic\n      language: cpp\n      install:\n        - sudo apt-get install ant libprotobuf-dev libvulkan-dev libglew-dev freeglut3-dev protobuf-compiler ocl-icd-opencl-dev libglfw3-dev\n      compiler: gcc\n      script:\n        - ./ciscripts/Linux/CL_ThreadPool_Vulkan.sh\n      name: \"Linux | CPU_CL_ThreadPool_Vulkan\"\n      env:\n        - MNNCITARGET=LINUXCLTHREADPOOLVULKAN\n    - os: linux\n      sudo: required\n      dist: bionic\n      language: cpp\n      install:\n        - sudo apt-get install ant libprotobuf-dev libvulkan-dev libglew-dev freeglut3-dev protobuf-compiler ocl-icd-opencl-dev libglfw3-dev\n      compiler: gcc\n      script:\n        - ./ciscripts/Linux/CL_OMP_Vulkan.sh\n      name: \"Linux | CPU_CL_OMP_Vulkan\"\n      env:\n        - MNNCITARGET=LINUXCLOMPVULKAN\n    - os: linux\n      sudo: required\n      dist: trusty\n      language: android\n      compiler: clang\n      android:\n        components:\n          - tools\n          - build-tools\n          - platform-tools\n          - android-21\n        licenses:\n          - 'android-sdk-preview-license-.+'\n          - 'android-sdk-license-.+'\n          - 'google-gdk-license-.+'\n      before_script:\n        - sudo apt-get install ant libprotobuf-dev protobuf-compiler\n        - sudo apt-get remove cmake\n        - echo yes | sdkmanager \"ndk-bundle\"\n        - echo yes | sdkmanager \"cmake;3.10.2.4988404\"\n        - export ANDROID_NDK=$ANDROID_HOME/ndk-bundle\n        - export PATH=/usr/local/android-sdk/cmake/3.10.2.4988404/bin/:$PATH\n      script:\n        - ./ciscripts/Android/32.sh\n      name: \"Android | AArch32_ThreadPool_Vulkan\"\n      env:\n        - MNNCITARGET=ARM32THREADPOOLVULKAN\n    - os: linux\n      sudo: required\n      dist: trusty\n      language: android\n      compiler: clang\n      android:\n        components:\n          - tools\n          - build-tools\n          - platform-tools\n          - android-21\n        licenses:\n          - 'android-sdk-preview-license-.+'\n          - 'android-sdk-license-.+'\n          - 'google-gdk-license-.+'\n      before_script:\n        - sudo apt-get install ant libprotobuf-dev protobuf-compiler\n        - echo yes | sdkmanager \"ndk-bundle\"\n        - echo yes | sdkmanager \"cmake;3.10.2.4988404\"\n        - export ANDROID_NDK=$ANDROID_HOME/ndk-bundle\n        - export PATH=/usr/local/android-sdk/cmake/3.10.2.4988404/bin/:$PATH\n      script:\n        - ./ciscripts/Android/32OMP.sh\n      name: \"Android | AArch32_OMP_Vulkan\"\n      env:\n        - MNNCITARGET=ARM32OMPVULKAN\n    - os: linux\n      sudo: required\n      dist: trusty\n      language: android\n      compiler: clang\n      android:\n        components:\n          - tools\n          - build-tools\n          - platform-tools\n          - android-21\n        licenses:\n          - 'android-sdk-preview-license-.+'\n          - 'android-sdk-license-.+'\n          - 'google-gdk-license-.+'\n      before_script:\n        - sudo apt-get install ant libprotobuf-dev protobuf-compiler\n        - echo yes | sdkmanager \"ndk-bundle\"\n        - echo yes | sdkmanager \"cmake;3.10.2.4988404\"\n        - export ANDROID_NDK=$ANDROID_HOME/ndk-bundle\n        - export PATH=/usr/local/android-sdk/cmake/3.10.2.4988404/bin/:$PATH\n      script:\n        - ./ciscripts/Android/64.sh\n      name: \"Android | AArch64_ThreadPool_Vulkan\"\n      env:\n        - MNNCITARGET=ARM64THREADPOOLVULKAN\n    - os: linux\n      sudo: required\n      dist: trusty\n      language: android\n      compiler: clang\n      android:\n        components:\n          - tools\n          - build-tools\n          - platform-tools\n          - android-21\n        licenses:\n          - 'android-sdk-preview-license-.+'\n          - 'android-sdk-license-.+'\n          - 'google-gdk-license-.+'\n      before_script:\n        - sudo apt-get install ant libprotobuf-dev protobuf-compiler\n        - echo yes | sdkmanager \"ndk-bundle\"\n        - echo yes | sdkmanager \"cmake;3.10.2.4988404\"\n        - export ANDROID_NDK=$ANDROID_HOME/ndk-bundle\n        - export PATH=/usr/local/android-sdk/cmake/3.10.2.4988404/bin/:$PATH\n      script:\n        - ./ciscripts/Android/64OMP.sh\n      name: \"Android | AArch64_OMP_Vulkan\"\n      env:\n        - MNNCITARGET=ARM64OMPVULKAN\n    - os: windows\n      language: cpp\n      install:\n        - PowerShell -Command 'Set-ExecutionPolicy -ExecutionPolicy RemoteSigned'\n        - choco install ninja\n      script:\n        - ciscripts/Windows/X64.bat\n      name: \"Windows | x64 CPU\"\n      env:\n        - MNNCITARGET=WINX64\n        - CXX=cl.exe\n        - CXX_FOR_BUILD=cl.exe\n        - CC=cl.exe\n        - CC_FOR_BUILD=cl.exe\n    - os: windows\n      language: cpp\n      install:\n        - PowerShell -Command 'Set-ExecutionPolicy -ExecutionPolicy RemoteSigned'\n        - choco install ninja\n      script:\n        - ciscripts/Windows/X86.bat\n      name: \"Windows | x86 CPU\"\n      env:\n        - MNNCITARGET=WINX86\n        - CXX=cl.exe\n        - CXX_FOR_BUILD=cl.exe\n        - CC=cl.exe\n        - CC_FOR_BUILD=cl.exe\n"
        },
        {
          "name": "3rd_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 36.052734375,
          "content": "cmake_minimum_required(VERSION 3.6)\n# Versioning stuff\nfile(STRINGS \"${CMAKE_CURRENT_LIST_DIR}/include/MNN/MNNDefine.h\" MNN_DEFINE)\nstring(REGEX MATCH \"MNN_VERSION_MAJOR [0-9]+\" MNN_VERSION_MAJOR_DEFINE ${MNN_DEFINE})\nstring(REGEX MATCH \"[0-9]+\" MNN_VERSION_MAJOR ${MNN_VERSION_MAJOR_DEFINE})\nstring(REGEX MATCH \"MNN_VERSION_MINOR [0-9]+\" MNN_VERSION_MINOR_DEFINE ${MNN_DEFINE})\nstring(REGEX MATCH \"[0-9]+\" MNN_VERSION_MINOR ${MNN_VERSION_MINOR_DEFINE})\nstring(REGEX MATCH \"MNN_VERSION_PATCH [0-9]+\" MNN_VERSION_PATCH_DEFINE ${MNN_DEFINE})\nstring(REGEX MATCH \"[0-9]+\" MNN_VERSION_PATCH ${MNN_VERSION_PATCH_DEFINE})\nset(MNN_VERSION ${MNN_VERSION_MAJOR}.${MNN_VERSION_MINOR}.${MNN_VERSION_PATCH})\n\n# Clear VERSION variables when no VERSION is given to project()\nif(POLICY CMP0048)\n  cmake_policy(SET CMP0048 NEW)\nendif()\n# MSVC runtime library flags are selected by an abstraction.\nif(POLICY CMP0091)\n  cmake_policy(SET CMP0091 NEW)\nendif()\nproject(MNN VERSION ${MNN_VERSION} LANGUAGES C CXX ASM)\n# complier options\nset(CMAKE_C_STANDARD 99)\nset(CMAKE_CXX_STANDARD 11)\nset(CMAKE_MODULE_PATH\n  ${CMAKE_MODULE_PATH}\n  \"${CMAKE_CURRENT_LIST_DIR}/cmake\"\n)\n\nif(WIN32)\n  if(NOT MSVC)\n    set(CMAKE_MSVC_RUNTIME_LIBRARY \"\")\n    set(MSVC_RUNTIME_LIBRARY \"\")\n  endif()\nendif()\n\n# build options\noption(MNN_USE_SYSTEM_LIB \"For opencl and vulkan, use system lib or use dlopen\" OFF)\noption(MNN_BUILD_HARD \"Build -mfloat-abi=hard or not\" OFF)\noption(MNN_BUILD_SHARED_LIBS \"MNN build shared or static lib\" ON)\noption(MNN_WIN_RUNTIME_MT \"MNN use /MT on Windows dll\" OFF)\noption(MNN_FORBID_MULTI_THREAD \"Disable Multi Thread\" OFF)\noption(MNN_OPENMP \"Use OpenMP's thread pool implementation. Does not work on iOS or Mac OS\" OFF)\noption(MNN_USE_THREAD_POOL \"Use MNN's own thread pool implementation\" ON)\noption(MNN_BUILD_TRAIN \"Build MNN's training framework\" OFF)\noption(MNN_BUILD_DEMO \"Build demo/exec or not\" OFF)\noption(MNN_BUILD_TOOLS \"Build tools/cpp or not\" ON)\noption(MNN_BUILD_QUANTOOLS \"Build Quantized Tools or not\" OFF)\noption(MNN_EVALUATION \"Build Evaluation Tools or not\" OFF)\noption(MNN_BUILD_CONVERTER \"Build Converter\" OFF)\noption(MNN_SUPPORT_DEPRECATED_OP \"Enable MNN's tflite quantized op\" OFF)\noption(MNN_DEBUG_MEMORY \"MNN Debug Memory Access\" OFF)\noption(MNN_DEBUG_TENSOR_SIZE \"Enable Tensor Size\" OFF)\noption(MNN_GPU_TRACE \"Enable MNN Gpu Debug\" OFF)\noption(MNN_SUPPORT_RENDER \"Enable MNN Render Ops\" OFF)\noption(MNN_SUPPORT_TRANSFORMER_FUSE \"Enable MNN transformer Fuse Ops\" OFF)\noption(MNN_PORTABLE_BUILD \"Link the static version of third party libraries where possible to improve the portability of built executables\" OFF)\noption(MNN_SEP_BUILD \"Build MNN Backends and expression separately. Only works with MNN_BUILD_SHARED_LIBS=ON\" ON)\noption(NATIVE_LIBRARY_OUTPUT \"Native Library Path\" OFF)\noption(NATIVE_INCLUDE_OUTPUT \"Native Include Path\" OFF)\noption(MNN_AAPL_FMWK \"Build MNN.framework instead of traditional .a/.dylib\" OFF)\noption(MNN_WITH_PLUGIN \"Build with plugin op support.\" OFF)\noption(MNN_BUILD_MINI \"Build MNN-MINI that just supports fixed shape models.\" OFF)\noption(MNN_USE_SSE \"Use SSE optimization for x86 if possiable\" ON)\noption(MNN_BUILD_CODEGEN \"Build with codegen\" OFF)\noption(MNN_ENABLE_COVERAGE \"Build with coverage enable\" OFF)\noption(MNN_BUILD_PROTOBUFFER \"Build with protobuffer in MNN\" ON)\noption(MNN_BUILD_OPENCV \"Build OpenCV api in MNN.\" OFF)\noption(MNN_BUILD_LLM \"Build llm library based MNN.\" OFF)\noption(MNN_BUILD_DIFFUSION \"Build diffusion demo based MNN.\" OFF)\noption(MNN_INTERNAL \"Build with MNN internal features, such as model authentication, metrics logging\" OFF)\noption(MNN_JNI \"Build MNN Jni for java to use\" OFF)\noption(MNN_SUPPORT_BF16 \"Enable MNN's bf16 op\" OFF)\noption(MNN_LOW_MEMORY \"Build MNN support low memory for weight quant model.\" OFF)\noption(MNN_CPU_WEIGHT_DEQUANT_GEMM \"Build MNN CPU weight dequant related gemm kernels.\" OFF)\noption(MNN_BUILD_AUDIO \"Build audio api in MNN.\" OFF)\n\nIF (OHOS AND MNN_INTERNAL)\n  include($ENV{NODE_PATH}/@ali/tcpkg/tcpkg.cmake)\n  export_headers(DIR ${CMAKE_SOURCE_DIR}/include/MNN)\n  IF (MNN_BUILD_OPENCV)\n    export_headers(DIR ${CMAKE_SOURCE_DIR}/tools/cv/include/cv)\n  ENDIF()\nENDIF()\n\nIF (NOT DEFINED MNN_USE_SPARSE_COMPUTE)\n   set(MNN_USE_SPARSE_COMPUTE ON)\nENDIF()\n\nIF(NOT MNN_BUILD_SHARED_LIBS AND MNN_SEP_BUILD)\n  message(WARNING \"Close MNN_SEP_BUILD for static library\")\n  SET(MNN_SEP_BUILD OFF CACHE BOOL \"<docstring>\" FORCE)\nENDIF()\nIF(APPLE AND MNN_AAPL_FMWK AND MNN_SEP_BUILD)\n  message(WARNING \"MNN_SEP_BUILD AND MNN_AAPL_FMWK can't coexist. Turning off MNN_SEP_BUILD\")\n  SET(MNN_SEP_BUILD OFF CACHE BOOL \"<docstring>\" FORCE)\nENDIF()\nIF(WIN32)\n  IF(MNN_SEP_BUILD)\n    message(WARNING \"MNN_SEP_BUILD IS TROUBLESOME ON Windows. Forcing OFF...\")\n    SET(MNN_SEP_BUILD OFF CACHE BOOL \"<docstring>\" FORCE)\n  ENDIF()\n  add_definitions(-D_CRT_SECURE_NO_WARNINGS)\n\n  IF(MSVC)\n    # generate optimized (release) exe and library with pdb debug file, https://stackoverflow.com/a/31264946\n    SET(CMAKE_EXE_LINKER_FLAGS_RELEASE \"${CMAKE_EXE_LINKER_FLAGS_RELEASE} /DEBUG /OPT:REF /OPT:ICF\")\n    SET(CMAKE_SHARED_LINKER_FLAGS_RELEASE \"${CMAKE_SHARED_LINKER_FLAGS_RELEASE} /DEBUG /OPT:REF /OPT:ICF\")\n    SET(CMAKE_C_FLAGS_RELEASE \"${CMAKE_C_FLAGS_RELEASE} /Zi\")\n    SET(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} /Zi\")\n\n    SET(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /wd4267 /wd4018 /wd4251 /wd4996 /wd4244 /wd4146 /wd4129 /wd4305 /wd4275 /wd4101\")\n    SET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4267 /wd4018 /wd4251 /wd4996 /wd4244 /wd4146 /wd4129 /wd4305 /wd4275 /wd4101\")\n  ENDIF()\nENDIF()\n\n# for coverage test\nIF( MNN_ENABLE_COVERAGE)\n    SET(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fprofile-arcs -ftest-coverage\")\n    SET(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fprofile-arcs -ftest-coverage\")\nENDIF()\n\nif ((CMAKE_SYSTEM_NAME STREQUAL \"Darwin\") AND CMAKE_OSX_ARCHITECTURES)\n  set(CMAKE_SYSTEM_PROCESSOR ${CMAKE_OSX_ARCHITECTURES})\nendif()\n\n# do this before protobuf, make sure wincrt config of protobuf and MNN is same\nif(MSVC)\n    # same as protobuf, otherwise config is inconsistent\n    if(CMAKE_VERSION VERSION_GREATER 3.15 OR CMAKE_VERSION VERSION_EQUAL 3.15)\n      set(CMAKE_MSVC_RUNTIME_LIBRARY MultiThreaded$<$<CONFIG:Debug>:Debug>)\n      if(NOT MNN_WIN_RUNTIME_MT)\n        set(CMAKE_MSVC_RUNTIME_LIBRARY ${CMAKE_MSVC_RUNTIME_LIBRARY}DLL)\n      endif()\n    else()\n      foreach(flag_var\n          CMAKE_C_FLAGS CMAKE_C_FLAGS_DEBUG CMAKE_C_FLAGS_RELEASE\n          CMAKE_C_FLAGS_MINSIZEREL CMAKE_C_FLAGS_RELWITHDEBINFO\n          CMAKE_CXX_FLAGS CMAKE_CXX_FLAGS_DEBUG CMAKE_CXX_FLAGS_RELEASE\n          CMAKE_CXX_FLAGS_MINSIZEREL CMAKE_CXX_FLAGS_RELWITHDEBINFO)\n          if (MNN_WIN_RUNTIME_MT)\n              if(${flag_var} MATCHES \"/MD\")\n                  string(REGEX REPLACE \"/MD\" \"/MT\" ${flag_var} \"${${flag_var}}\")\n              endif()\n          else ()\n              if(${flag_var} MATCHES \"/MT\")\n                  string(REGEX REPLACE \"/MT\" \"/MD\" ${flag_var} \"${${flag_var}}\")\n              endif()\n          endif ()\n      endforeach()\n    endif()\n    set(protobuf_BUILD_SHARED_LIBS ${MNN_BUILD_SHARED_LIBS})\nendif()\n\ninclude(${CMAKE_CURRENT_LIST_DIR}/cmake/macros.cmake)\nIF(MNN_BUILD_PROTOBUFFER)\nIF(MNN_BUILD_CONVERTER)\n  IF(MSVC)\n    set(protobuf_BUILD_SHARED_LIBS ${MNN_BUILD_SHARED_LIBS})\n    IF((NOT MNN_BUILD_SHARED_LIBS) AND (NOT MNN_WIN_RUNTIME_MT))\n      message(FATAL_ERROR \"When MNN_BUILD_CONVERTER=ON and MNN_BUILD_SHARED_LIBS=OFF, MNN_WIN_RUNTIME_MT must be ON. Because protobuf not support the config(static /MD)\")\n    ENDIF()\n  ENDIF()\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/3rd_party/protobuf/cmake)\nENDIF()\nENDIF()\n\n# specify source file encoding explicitly, fix cross-platform garbled output issue\n# we need do this after protobuf which set different execution-charset\nIF(MSVC)\n  set(CMAKE_C_FLAGS \"${CMAKE_CXX_FLAGS} /source-charset:utf-8\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /source-charset:utf-8\")\nENDIF()\n\nIF(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\" AND NOT MNN_BUILD_SHARED_LIBS AND NOT (MSVC OR WIN32))\n  SET(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS}\")\n  SET(MNN_SEP_BUILD OFF CACHE BOOL \"<docstring>\" FORCE)\n  IF(MNN_BUILD_CONVERTER)\n    SET(MNN_PORTABLE_BUILD ON CACHE BOOL \"<docstring>\" FORCE)\n  ENDIF()\nENDIF()\n\nif(MNN_FORBID_MULTI_THREAD)\n    add_definitions(-DMNN_FORBIT_MULTI_THREADS)\nendif()\nif(MNN_SUPPORT_DEPRECATED_OP)\n    add_definitions(-DMNN_SUPPORT_DEPRECATED_OP)\nendif()\nif(MNN_SUPPORT_RENDER)\n    add_definitions(-DMNN_SUPPORT_RENDER)\nendif()\nif(MNN_SUPPORT_TRANSFORMER_FUSE)\n    add_definitions(-DMNN_SUPPORT_TRANSFORMER_FUSE)\nendif()\nif(MNN_BUILD_AUDIO)\n    add_definitions(-DMNN_BUILD_AUDIO)\nendif()\n# debug options\nif(MNN_DEBUG_MEMORY)\n    add_definitions(-DMNN_DEBUG_MEMORY)\nendif()\nif(MNN_DEBUG_TENSOR_SIZE)\n    add_definitions(-DMNN_DEBUG_TENSOR_SIZE)\nendif()\nif(MNN_GPU_TRACE)\n    add_definitions(-DMNN_GPU_FORCE_FINISH)\nendif()\n\n# backend options\noption(MNN_METAL \"Enable Metal\" OFF)\noption(MNN_OPENCL \"Enable OpenCL\" OFF)\noption(MNN_OPENGL \"Enable OpenGL\" OFF)\noption(MNN_VULKAN \"Enable Vulkan\" OFF)\noption(MNN_ARM82 \"Enable ARMv8.2's FP16 Compute\" ON)\noption(MNN_KLEIDIAI \"Enable KLEIDIAI\" OFF)\noption(MNN_ONEDNN \"Enable oneDNN\" OFF)\noption(MNN_AVX2 \"Open AVX2 Compile for x86 if possible\" ON)\noption(MNN_AVX512 \"Enable AVX512\" OFF)\noption(MNN_CUDA \"Enable CUDA\" OFF)\noption(MNN_TENSORRT \"Enable TensorRT\" OFF)\noption(MNN_COREML \"Enable CoreML\" OFF)\noption(MNN_NNAPI \"Enable NNAPI\" OFF)\n\noption(MNN_CUDA_PROFILE \"Enable CUDA profile\" OFF)\n\nif (NOT MNN_CUDA OR NOT CMAKE_SYSTEM_NAME MATCHES \"^Linux\")\n  set(MNN_CUDA_PROFILE OFF)\nendif()\n\nif (MNN_USE_THREAD_POOL)\n    message(STATUS \"Use Threadpool, forbid openmp\")\n    set(MNN_OPENMP OFF)\n    add_definitions(-DMNN_USE_THREAD_POOL)\nendif()\n\n# When build Android based on arm32 by MTL, force turn off MNN_ARM82\nif (CMAKE_SYSTEM_NAME MATCHES \"^Android\" AND CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv7\" AND NOT MNN_BUILD_FOR_ANDROID_COMMAND)\n    message(STATUS \"force turn off MNN_ARM82 when build for Android based on arm32 by MTL\")\n    SET(MNN_ARM82 OFF CACHE BOOL \"Enable ARM82\" FORCE)\nendif()\n\n# target options\noption(MNN_BUILD_BENCHMARK \"Build benchmark or not\" OFF)\noption(MNN_BUILD_TEST \"Build tests or not\" OFF)\noption(MNN_BUILD_FOR_ANDROID_COMMAND \"Build from command\" OFF)\noption(MNN_USE_LOGCAT \"Use Logcat intead of print for info\" ON)\nset (MNN_HIDDEN FALSE)\nIF(CMAKE_BUILD_TYPE MATCHES Debug)\nELSE()\n    set(MNN_HIDDEN TRUE)\nENDIF(CMAKE_BUILD_TYPE MATCHES Debug)\n\nmessage(STATUS \">>>>>>>>>>>>>\")\nmessage(STATUS \"MNN BUILD INFO:\")\nmessage(STATUS \"\\tSystem: ${CMAKE_SYSTEM_NAME}\")\nmessage(STATUS \"\\tProcessor: ${CMAKE_SYSTEM_PROCESSOR}\")\nmessage(STATUS \"\\tVersion: ${MNN_VERSION}\")\nmessage(STATUS \"\\tMetal: ${MNN_METAL}\")\nmessage(STATUS \"\\tOpenCL: ${MNN_OPENCL}\")\nmessage(STATUS \"\\tOpenGL: ${MNN_OPENGL}\")\nmessage(STATUS \"\\tVulkan: ${MNN_VULKAN}\")\nmessage(STATUS \"\\tARM82: ${MNN_ARM82}\")\nmessage(STATUS \"\\tKleidiAI: ${MNN_KLEIDIAI}\")\nmessage(STATUS \"\\toneDNN: ${MNN_ONEDNN}\")\nmessage(STATUS \"\\tTensorRT: ${MNN_TENSORRT}\")\nmessage(STATUS \"\\tCoreML: ${MNN_COREML}\")\nmessage(STATUS \"\\tNNAPI: ${MNN_NNAPI}\")\nmessage(STATUS \"\\tCUDA: ${MNN_CUDA}\")\nmessage(STATUS \"\\tOpenMP: ${MNN_OPENMP}\")\nmessage(STATUS \"\\tBF16: ${MNN_SUPPORT_BF16}\")\nmessage(STATUS \"\\tThreadPool: ${MNN_USE_THREAD_POOL}\")\nmessage(STATUS \"\\tHidden: ${MNN_HIDDEN}\")\nmessage(STATUS \"\\tBuild Path: ${CMAKE_CURRENT_BINARY_DIR}\")\nmessage(STATUS \"\\tCUDA PROFILE: ${MNN_CUDA_PROFILE}\")\n\nif(CMAKE_SYSTEM_NAME MATCHES \"^Android\" OR CMAKE_SYSTEM_NAME MATCHES \"^Linux\")\n    add_definitions(-fPIC)\nendif()\n\n# Raspberry Pi 32-bit fix\nif(CMAKE_SYSTEM_NAME MATCHES \"^Linux\" AND CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv7\")\n    add_definitions(-march=armv7-a -mfpu=neon-vfpv4)\nendif()\n\nif(CMAKE_SYSTEM_NAME MATCHES \"^Android\")\n    add_definitions(-DMNN_BUILD_FOR_ANDROID)\n    if(CMAKE_SYSTEM_PROCESSOR MATCHES \"^arm\")\n        add_definitions(-mfloat-abi=softfp -mfpu=neon)\n    endif()\nendif()\noption(MNN_USE_CPP11 \"Enable MNN use c++11\" ON)\nif (NOT MSVC)\n    if(MNN_CUDA AND MNN_SUPPORT_TRANSFORMER_FUSE)\n        set(CMAKE_CXX_STANDARD 17)\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -std=gnu99\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++17\")\n    elseif(MNN_USE_CPP11)\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -std=gnu99\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11\")\n    else()\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -std=gnu99\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++0x\")\n    endif()\nendif()\n\nif(CMAKE_SYSTEM_NAME MATCHES \"^Linux\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D__STRICT_ANSI__\")\n    if(CMAKE_SYSTEM_PROCESSOR MATCHES \"^armv7\")\n        add_definitions(-mfpu=neon)    #please define in project/cross-compile/arm.toolchain.cmake\n    endif()\n    if(MNN_BUILD_HARD)\n        add_definitions(-mfloat-abi=hard)  #better define in project/cross-compile/arm.toolchain.cmake\n    endif()\nendif()\n\nIF(MNN_DEBUG_MEMORY)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fsanitize=address\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=address\")\nendif()\n\nset(MNN_DEPS \"\")\nset(MNN_EXTRA_DEPENDS \"\")\n\nIF(CMAKE_BUILD_TYPE MATCHES Debug)\n    add_definitions(-DMNN_DEBUG -DDEBUG)\n    if(MSVC)\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /DEBUG\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /DEBUG\")\n    else()\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -g\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -g\")\n    endif()\nelse()\n    if (MSVC)\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /O2\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /O2\")\n    else()\n        set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -O3\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -O3\")\n        if(CMAKE_SYSTEM_NAME MATCHES \"^Android\")\n            if(MNN_BUILD_FOR_ANDROID_COMMAND)\n                set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -s\")\n                set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -pie -fPIE -s\")\n                set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -Wl,--gc-sections\")\n            endif()\n        endif()\n    endif()\nENDIF(CMAKE_BUILD_TYPE MATCHES Debug)\nif(OHOS)\n    IF(MNN_USE_LOGCAT)\n        add_definitions(-DMNN_USE_LOGCAT)\n        add_definitions(-Wno-format-security)\n        list(APPEND MNN_EXTRA_DEPENDS libhilog_ndk.z.so)\n    ENDIF()\nendif()\nif(CMAKE_SYSTEM_NAME MATCHES \"^Android\")\n    IF(MNN_USE_LOGCAT)\n        add_definitions(-DMNN_USE_LOGCAT)\n    ENDIF()\n    IF (NOT MNN_BUILD_FOR_ANDROID_COMMAND)\n        set(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${NATIVE_LIBRARY_OUTPUT}/${ANDROID_ABI})\n    ENDIF()\nendif()\n\nif(${CMAKE_SYSTEM_NAME} MATCHES \"^Linux\")\n    if((CMAKE_SYSTEM_PROCESSOR MATCHES \"^arm\") OR (CMAKE_SYSTEM_PROCESSOR MATCHES \"^aarch64\"))\n        set(aarch64_linux_include\n            #/usr/include/c++/4.9\n            #/usr/lib/gcc/x86_64-linux-gnu/4.9\n            #/usr/lib/gcc/x86_64-linux-gnu/4.9/include\n            #/usr/include/x86_64-linux-gnu/c++/4.9\n        )\n        include_directories(${aarch64_linux_include})\n    endif()\nendif()\ninclude_directories(${CMAKE_CURRENT_LIST_DIR}/include/\n                    ${CMAKE_CURRENT_LIST_DIR}/source/\n                    ${CMAKE_CURRENT_LIST_DIR}/express/\n                    ${CMAKE_CURRENT_LIST_DIR}/tools/\n                    ${CMAKE_CURRENT_LIST_DIR}/codegen/\n                    ${CMAKE_CURRENT_LIST_DIR}/schema/current/\n                    ${CMAKE_CURRENT_LIST_DIR}/3rd_party/\n                    ${CMAKE_CURRENT_LIST_DIR}/3rd_party/flatbuffers/include\n                    ${CMAKE_CURRENT_LIST_DIR}/3rd_party/half\n                    ${CMAKE_CURRENT_LIST_DIR}/3rd_party/imageHelper\n                    ${CMAKE_CURRENT_LIST_DIR}/3rd_party/OpenCLHeaders/\n                  )\n\n\nset(MNN_OBJECTS_TO_LINK \"\")\nset(MNN_TARGETS \"\")\n\n# Core\nFILE(GLOB MNN_Core_SRC ${CMAKE_CURRENT_LIST_DIR}/source/core/*)\nadd_library(MNNCore OBJECT ${MNN_Core_SRC})\nlist(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNCore>)\nlist(APPEND MNN_TARGETS MNNCore)\nif(MNN_BUILD_MINI)\n    target_compile_options(MNNCore PRIVATE -DMNN_BUILD_MINI)\nendif()\n\n# CV\nFILE(GLOB MNN_CV_SRC ${CMAKE_CURRENT_LIST_DIR}/source/cv/*)\nadd_library(MNNCV OBJECT ${MNN_CV_SRC})\nlist(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNCV>)\nlist(APPEND MNN_TARGETS MNNCV)\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"(x86_64)|(X86_64)|(x64)|(X64)|(amd64)|(AMD64)|(i686)\")\n    if (APPLE)\n        add_definitions(-fno-stack-check) # Workaround a Xcode 11.X bug\n    endif()\nendif()\n\n# Math\nFILE(GLOB MNN_Math_SRC ${CMAKE_CURRENT_LIST_DIR}/source/math/*)\nadd_library(MNNMath OBJECT ${MNN_Math_SRC})\nlist(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNMath>)\nlist(APPEND MNN_TARGETS MNNMath)\n\n# Transform\nFILE(GLOB_RECURSE MNN_Transform_SRC ${CMAKE_CURRENT_LIST_DIR}/source/shape/* ${CMAKE_CURRENT_LIST_DIR}/source/geometry/*)\nadd_library(MNNTransform OBJECT ${MNN_Transform_SRC})\nIF (NOT MNN_BUILD_MINI)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNTransform>)\nENDIF()\nlist(APPEND MNN_TARGETS MNNTransform)\n\n# Utils\nFILE(GLOB MNN_Utils_SRC ${CMAKE_CURRENT_LIST_DIR}/source/utils/*)\nadd_library(MNNUtils OBJECT ${MNN_Utils_SRC})\nlist(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNUtils>)\nlist(APPEND MNN_TARGETS MNNUtils)\n\ninclude(${CMAKE_CURRENT_LIST_DIR}/source/backend/cpu/CMakeLists.txt)\n\n\nSET(MNN_PUB_HDRS \"\")\nSET(MNN_EXPR_PUB_HDRS \"\")\nset(MNN_EXTRA_HEADERS \"\")\n\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/MNNDefine.h\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/Interpreter.hpp\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/HalideRuntime.h\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/Tensor.hpp\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/ErrorCode.hpp\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/ImageProcess.hpp\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/Matrix.h\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/Rect.h\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/MNNForwardType.h\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/AutoTime.hpp\")\nlist(APPEND MNN_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/MNNSharedContext.h\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/Expr.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/ExprCreator.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/MathOp.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/NeuralNetWorkOp.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/Optimizer.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/Executor.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/Module.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/NeuralNetWorkOp.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/ExecutorScope.hpp\")\nlist(APPEND MNN_EXPR_PUB_HDRS \"${CMAKE_CURRENT_SOURCE_DIR}/include/MNN/expr/Scope.hpp\")\n\n# Add Extra Header\nIF(MNN_BUILD_OPENCV)\n  file(GLOB MNN_CV_HDRS ${CMAKE_CURRENT_SOURCE_DIR}/tools/cv/include/cv/*.hpp PARENT_SCOPE)\n  file(GLOB MNN_CV_IMGHDRS ${CMAKE_CURRENT_SOURCE_DIR}/tools/cv/include/cv/imgproc/*.hpp PARENT_SCOPE)\n  list(APPEND MNN_EXTRA_HEADERS ${MNN_CV_HDRS})\n  list(APPEND MNN_EXTRA_HEADERS ${MNN_CV_IMGHDRS})\nENDIF()\nIF(MNN_BUILD_AUDIO)\n  file(GLOB MNN_AUDIO_HDRS ${CMAKE_CURRENT_SOURCE_DIR}/tools/audio/include/audio/*.hpp PARENT_SCOPE)\n  list(APPEND MNN_EXTRA_HEADERS ${MNN_AUDIO_HDRS})\nENDIF()\nIF(MNN_BUILD_LLM)\n  file(GLOB MNN_LLM_HDRS ${CMAKE_CURRENT_SOURCE_DIR}/transformers/llm/engine/include/llm/*)\n  list(APPEND MNN_EXTRA_HEADERS ${CMAKE_CURRENT_SOURCE_DIR}/transformers/llm/engine/include/llm/llm.hpp)\nENDIF()\n\n\n\n# Add Thread dependency\nfind_package(Threads)\nlist(APPEND MNN_EXTRA_DEPENDS ${CMAKE_THREAD_LIBS_INIT})\nif(WIN32)\n  if(NOT MSVC)\n    set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -fuse-ld=lld-link -lmsvcrt\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -fuse-ld=lld-link -lmsvcrt\")\n  endif()\nendif()\n\nif (NOT APPLE)\n  if(MNN_OPENMP)\n      message(STATUS \"[*] Checking OpenMP\")\n      find_package(OpenMP)\n      # For CMake < 3.9, we need to make the target ourselves\n      if(NOT TARGET OpenMP::OpenMP_CXX)\n          add_library(OpenMP::OpenMP_CXX IMPORTED INTERFACE)\n          set_property(TARGET OpenMP::OpenMP_CXX\n              PROPERTY INTERFACE_COMPILE_OPTIONS ${OpenMP_CXX_FLAGS})\n          # Only works if the same flag is passed to the linker; use CMake 3.9+ otherwise (Intel, AppleClang)\n          set_property(TARGET OpenMP::OpenMP_CXX\n              PROPERTY INTERFACE_LINK_LIBRARIES ${OpenMP_CXX_FLAGS} Threads::Threads)\n      endif()\n      # TODO: Don't pollute global CFLAGS\n      set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\")\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\")\n      set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} ${OpenMP_SHARED_LINKER_FLAGS}\")\n      set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} ${OpenMP_EXE_LINKER_FLAGS}\")\n      if (MSVC)\n          set(OpenMP_C_FLAGS \"/openmp ${OpenMP_C_FLAGS}\")\n          set(OpenMP_CXX_FLAGS \"/openmp ${OpenMP_CXX_FLAGS}\")\n      endif()\n      list(APPEND MNN_EXTRA_DEPENDS OpenMP::OpenMP_CXX)\n    endif()\nendif()\n\nif ((NOT MSVC) AND MNN_HIDDEN)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fvisibility-inlines-hidden -fvisibility=hidden\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fvisibility=hidden\")\n    # Omit frame pointer may cause difficult debug\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fomit-frame-pointer\")\nendif()\nif (NOT MSVC)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-math -fno-rtti -fno-exceptions \")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fstrict-aliasing -ffunction-sections -fdata-sections -ffast-math\")\nelse()\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /fp:precise\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /fp:precise\")\nendif()\n\n# Metal\nlist(APPEND MNN_DEPS MNN)\n\n# Plugin\nif(MNN_WITH_PLUGIN)\n    add_definitions(-DMNN_WITH_PLUGIN)\n    include(${CMAKE_CURRENT_LIST_DIR}/source/plugin/CMakeLists.txt)\nendif()\n\n# Metal\nif(MNN_METAL AND APPLE)\n    target_compile_options(MNNCore PRIVATE -DMNN_METAL_ENABLED=1)\n    include(${CMAKE_CURRENT_LIST_DIR}/source/backend/metal/CMakeLists.txt)\n    list(APPEND MNN_TARGETS MNNMetal)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNMetal>)\nendif()\n\n# CoreML\nIF(MNN_COREML)\n    add_definitions(-DMNN_COREML_ENABLED=1)\n    include(${CMAKE_CURRENT_LIST_DIR}/source/backend/coreml/CMakeLists.txt)\n\n    list(APPEND MNN_TARGETS MNNCoreML)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNCoreML>)\n\n    find_library(COREML CoreML)\n    find_library(FOUNDATION Foundation)\n    find_library(METAL Metal)\n    find_library(VIDEO CoreVideo)\n    list(APPEND MNN_EXTRA_DEPENDS ${COREML})\n    list(APPEND MNN_EXTRA_DEPENDS ${FOUNDATION})\n    list(APPEND MNN_EXTRA_DEPENDS ${METAL})\n    list(APPEND MNN_EXTRA_DEPENDS ${VIDEO})\nENDIF()\n\n# NNAPI\nIF(MNN_NNAPI)\n    add_definitions(-DMNN_NNAPI_ENABLED=1)\n    add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/source/backend/nnapi/)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNN_NNAPI>)\nENDIF()\n\n# Vulkan\nIF(MNN_VULKAN)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/source/backend/vulkan/)\n  IF(MNN_SEP_BUILD)\n    list(APPEND MNN_DEPS MNN_Vulkan)\n  ELSE()\n    list(APPEND MNN_TARGETS MNN_Vulkan)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNN_Vulkan>)\n    list(APPEND MNN_EXTRA_DEPENDS ${MNN_VULKAN_LIBS})\n  ENDIF()\nENDIF()\n\n# oneDNN\nIF(MNN_ONEDNN)\n    target_compile_definitions(MNNCPU PRIVATE \"-DMNN_USE_ONEDNN\")\n    add_dependencies(MNNCPU oneDNN)\n    include(cmake/oneDNN.cmake)\n    set(ONEDNN_DIR ${CMAKE_CURRENT_LIST_DIR}/3rd_party/oneDNN)\n    add_library(ONEDNN_COMMON OBJECT IMPORTED)\n    file(GLOB_RECURSE OBJECT_FILES ${ONEDNN_DIR}/src/common/CMakeFiles/dnnl_common.dir/*.o)\n    set_property(TARGET ONEDNN_COMMON PROPERTY IMPORTED_OBJECTS ${OBJECT_FILES})\n    add_library(ONEDNN_CPU OBJECT IMPORTED)\n    file(GLOB_RECURSE OBJECT_FILES ${ONEDNN_DIR}/src/cpu/CMakeFiles/dnnl_cpu.dir/*.o)\n    set_property(TARGET ONEDNN_CPU PROPERTY IMPORTED_OBJECTS ${OBJECT_FILES})\n    add_library(ONEDNN_CPU_X64 OBJECT IMPORTED)\n    file(GLOB_RECURSE OBJECT_FILES ${ONEDNN_DIR}/src/cpu/x64/CMakeFiles/dnnl_cpu_x64.dir/*.o)\n    set_property(TARGET ONEDNN_CPU_X64 PROPERTY IMPORTED_OBJECTS ${OBJECT_FILES})\n    include_directories(${ONEDNN_DIR}/include)\n    list(APPEND MNN_TARGETS ${ONEDNN_COMMON})\n    list(APPEND MNN_TARGETS ${ONEDNN_CPU})\n    list(APPEND MNN_TARGETS ${ONEDNN_CPU_X64})\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:ONEDNN_COMMON>)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:ONEDNN_CPU>)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:ONEDNN_CPU_X64>)\nENDIF()\n\n# OpenCL\nIF(MNN_OPENCL)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/source/backend/opencl/)\n  IF(MNN_SEP_BUILD)\n    list(APPEND MNN_DEPS MNN_CL)\n  ELSE()\n    add_definitions(-DMNN_OPENCL_ENABLED=1)\n    list(APPEND MNN_TARGETS MNN_CL)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNN_CL>)\n    list(APPEND MNN_EXTRA_DEPENDS ${MNN_OCL_LIBS})\n  ENDIF()\nENDIF()\n\n# OpenGL\nIF(MNN_OPENGL)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/source/backend/opengl/)\n  IF(MNN_SEP_BUILD)\n    list(APPEND MNN_DEPS MNN_GL)\n  ELSE()\n    list(APPEND MNN_TARGETS MNN_GL)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNN_GL>)\n    list(APPEND MNN_EXTRA_DEPENDS GLESv3)\n    list(APPEND MNN_EXTRA_DEPENDS EGL)\n  ENDIF()\nENDIF()\n\n# CUDA\nIF(MNN_CUDA)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/source/backend/cuda/)\n  list(APPEND MNN_TARGETS MNN_CUDA)\n  if (NOT MSVC)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNN_CUDA>)\n  endif()\n  list(APPEND MNN_EXTRA_DEPENDS ${MNN_CUDA_LIBS})\nENDIF()\n\n# Express\nadd_subdirectory(${CMAKE_CURRENT_LIST_DIR}/express/)\nIF(MNN_SEP_BUILD)\n  list(APPEND MNN_DEPS MNN_Express)\nELSE()\n   list(APPEND MNN_TARGETS MNN_Express)\n   list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNN_Express>)\nENDIF()\n\n# Model Internal. Enable MNN internal features such as model authentication and metrics logging.\nif (MNN_INTERNAL AND NOT OHOS) # TODO: support OHOS logging\n    target_compile_options(MNNCore PRIVATE -DMNN_INTERNAL_ENABLED)\n    target_compile_options(MNN_Express PRIVATE -DMNN_INTERNAL_ENABLED)\n    include(${CMAKE_CURRENT_LIST_DIR}/source/internal/logging/CMakeLists.txt)\n    if(CMAKE_SYSTEM_NAME MATCHES \"^Linux\")\n        list(APPEND MNN_EXTRA_DEPENDS \"-lcurl -lssl -lcrypto\")\n    endif()\nendif()\n\n# Train\nIF(MNN_BUILD_TRAIN OR MNN_BUILD_QUANTOOLS)\n  add_subdirectory(tools/train)\n  IF(MNN_SEP_BUILD)\n    list(APPEND MNN_DEPS MNNTrain)\n    list(APPEND MNN_DEPS MNNTrainUtils)\n  ELSE()\n    list(APPEND MNN_TARGETS MNNTrain)\n    list(APPEND MNN_TARGETS MNNTrainUtils)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNTrain>)\n    list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNNTrainUtils>)\n  ENDIF()\nENDIF()\n\n#CodeGen\nIF(MNN_BUILD_CODEGEN)\n    add_definitions(-DMNN_BUILD_CODEGEN)\n    include(${CMAKE_CURRENT_LIST_DIR}/codegen/CMakeLists.txt)\nENDIF()\n\n# NPU\nIF(MNN_NPU)\n    add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/source/backend/hiai/)\n    IF(MNN_SEP_BUILD)\n        list(APPEND MNN_DEPS MNN_NPU)\n    ELSE()\n        list(APPEND MNN_TARGETS MNN_NPU)\n        list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNN_NPU>)\n        list(APPEND MNN_EXTRA_DEPENDS ${CMAKE_CURRENT_LIST_DIR}/source/backend/hiai/3rdParty/${ANDROID_ABI}/libhiai.so)\n        list(APPEND MNN_EXTRA_DEPENDS ${CMAKE_CURRENT_LIST_DIR}/source/backend/hiai/3rdParty/${ANDROID_ABI}/libhiai_ir_build.so)\n        list(APPEND MNN_EXTRA_DEPENDS ${CMAKE_CURRENT_LIST_DIR}/source/backend/hiai/3rdParty/${ANDROID_ABI}/libhiai_ir.so)\n    ENDIF()\nENDIF()\n\n# TensorRT\nIF(MNN_TENSORRT)\n  add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/source/backend/tensorrt/)\n  list(APPEND MNN_TARGETS MNN_TRT)\n  list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:MNN_TRT>)\n  list(APPEND MNN_EXTRA_DEPENDS ${MNN_TRT_LIBS})\nENDIF()\n\nIF(MNN_BUILD_LLM)\n    # add_definitions(-DMNN_BUILD_LLM)\n    include(${CMAKE_CURRENT_LIST_DIR}/transformers/llm/engine/CMakeLists.txt)\n    IF(NOT MNN_SEP_BUILD)\n      list(APPEND MNN_TARGETS llm)\n      list(APPEND MNN_OBJECTS_TO_LINK $<TARGET_OBJECTS:llm>)\n    ENDIF()\nENDIF()\n\nIF(MNN_SEP_BUILD)\n  add_library(MNN SHARED ${CMAKE_CURRENT_LIST_DIR}/cmake/dummy.cpp ${MNN_OBJECTS_TO_LINK} ${MNN_PUB_HDRS} ${MNN_EXPR_PUB_HDRS} ${MNN_EXTRA_HEADERS})\n  target_link_libraries(MNN PUBLIC ${MNN_EXTRA_DEPENDS})\nELSE()\n  IF(MNN_BUILD_SHARED_LIBS)\n    add_library(MNN SHARED ${CMAKE_CURRENT_LIST_DIR}/cmake/dummy.cpp ${MNN_OBJECTS_TO_LINK} ${MNN_PUB_HDRS} ${MNN_EXPR_PUB_HDRS} ${MNN_EXTRA_HEADERS})\n    if (WIN32)\n      foreach(TARGET ${MNN_TARGETS})\n        target_compile_definitions(${TARGET} PRIVATE \"-DBUILDING_MNN_DLL\")\n        target_compile_definitions(${TARGET} INTERFACE \"-DUSING_MNN_DLL\")\n      endforeach()\n      target_compile_definitions(MNN PRIVATE \"-DBUILDING_MNN_DLL\")\n      target_compile_definitions(MNN INTERFACE \"-DUSING_MNN_DLL\")\n    endif()\n  ELSE()\n    add_library(MNN STATIC ${CMAKE_CURRENT_LIST_DIR}/cmake/dummy.cpp ${MNN_OBJECTS_TO_LINK} ${MNN_PUB_HDRS} ${MNN_EXPR_PUB_HDRS} ${MNN_EXTRA_HEADERS})\n  ENDIF()\n  target_link_libraries(MNN PUBLIC ${MNN_EXTRA_DEPENDS})\nENDIF()\nif (MSVC)\n  target_link_options(MNN PRIVATE \"/IGNORE:4049,4217\")\n  if (MNN_CUDA)\n    if (MNN_BUILD_SHARED_LIBS)\n      target_link_options(MNN PRIVATE \"/WHOLEARCHIVE:$<TARGET_FILE:MNN_CUDA>\")\n    else()\n      add_custom_command(\n        TARGET MNN\n        POST_BUILD\n        COMMAND lib.exe ARGS /OUT:$<TARGET_FILE:MNN> $<TARGET_FILE:MNN> $<TARGET_FILE:MNN_CUDA>\n      )\n    endif()\n  endif()\nendif()\nif (MNN_ONEDNN)\n    add_dependencies(MNN ONEDNN_COMMON ONEDNN_CPU ONEDNN_CPU_X64)\nendif()\n\nif(APPLE)\n    IF(MNN_AAPL_FMWK)\n      set_target_properties(MNN PROPERTIES FRAMEWORK TRUE)\n      set_target_properties(MNN PROPERTIES\n          MACOSX_FRAMEWORK_IDENTIFIER com.alibaba.MNN\n          MACOSX_FRAMEWORK_SHORT_VERSION_STRING ${PACKAGE_VERSION}\n          MACOSX_FRAMEWORK_BUNDLE_VERSION ${PACKAGE_VERSION}\n          XCODE_ATTRIBUTE_CODE_SIGN_IDENTITY \"iPhone Developer\"\n      )\n      set_target_properties(MNN PROPERTIES MACOSX_FRAMEWORK_INFO_PLIST ${CMAKE_CURRENT_SOURCE_DIR}/project/ios/MNN/Info.plist)\n    ENDIF()\n    IF(MNN_METAL)\n      find_library(FOUNDATION Foundation REQUIRED)\n      target_link_libraries(MNN PUBLIC ${FOUNDATION})\n      find_library(METAL Metal REQUIRED)\n      target_link_libraries(MNN PUBLIC ${METAL})\n      find_library(GRAPHIC CoreGraphics)\n      target_link_libraries(MNN PUBLIC ${GRAPHIC})\n    ENDIF()\nendif()\nadd_dependencies(MNN MNNCore MNNCV MNNTransform MNNMath MNNCPU)\nadd_subdirectory(${CMAKE_CURRENT_LIST_DIR}/tools/converter)\nIF(WIN32 AND MNN_BUILD_CONVERTER AND MNN_BUILD_SHARED_LIBS)\n# Because of dllimport/dllexport, we merge MNN and MNNConvertDeps together, which depend protobuf\n  target_link_libraries(MNN PUBLIC ${Protobuf_LIBRARIES})\nENDIF()\n# Merge MNN/MNNExpress/MNNOpenCV and other backends into one .lib/.dll on Windows\nadd_subdirectory(${CMAKE_CURRENT_LIST_DIR}/tools/cv)\nIF(MNN_BUILD_OPENCV AND NOT MNN_SEP_BUILD)\n  IF(MSVC)\n    target_compile_definitions(MNNOpenCV PRIVATE \"-DBUILDING_MNN_DLL\" INTERFACE \"-DUSING_MNN_DLL\")\n  ENDIF()\n  target_sources(MNN PRIVATE $<TARGET_OBJECTS:MNNOpenCV>)\nENDIF()\nadd_subdirectory(${CMAKE_CURRENT_LIST_DIR}/tools/audio)\nIF(MNN_BUILD_AUDIO AND NOT MNN_SEP_BUILD)\n  IF(MSVC)\n    target_compile_definitions(MNNAudio PRIVATE \"-DBUILDING_MNN_DLL\" INTERFACE \"-DUSING_MNN_DLL\")\n  ENDIF()\n  message(STATUC \"### build MNNAudio into MNN\")\n  target_sources(MNN PRIVATE $<TARGET_OBJECTS:MNNAudio>)\nENDIF()\n\n\nif(CMAKE_SYSTEM_NAME MATCHES \"^Linux\")\n# Using -pthread, needed by thread-safe implemention of glibc, is better than only using -lpthread\n# https://stackoverflow.com/questions/23250863/difference-between-pthread-and-lpthread-while-compiling\n  target_link_libraries(MNN PUBLIC -pthread dl)\nelseif(CMAKE_SYSTEM_NAME MATCHES \"^Android\")\n  target_link_libraries(MNN PUBLIC log m)\nelse()\nendif()\nif (NOT MNN_BUILD_SHARED_LIBS)\n    if (CMAKE_CXX_COMPILER_ID MATCHES \"GNU\" OR CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n        # Static-link will not replace thread-related weak symbol in glibc with strong symbol\n        # in pthread library, so we need use --whole-archive to pthread\n        # https://stackoverflow.com/questions/35116327/when-g-static-link-pthread-cause-segmentation-fault-why\n        if(CMAKE_SYSTEM_NAME MATCHES \"^Linux\")\n            set(MNN_DEPS -Wl,--whole-archive ${MNN_DEPS} -lpthread -Wl,--no-whole-archive)\n        else()\n          if(APPLE)\n            set(MNN_DEPS -Wl,-force_load ${MNN_DEPS})\n          else()\n            set(MNN_DEPS -Wl,--whole-archive ${MNN_DEPS} -Wl,--no-whole-archive)\n          endif()\n        endif()\n    endif()\nendif()\nlist(APPEND MNN_TARGETS MNN)\nlist(REMOVE_ITEM MNN_TARGETS MNN)\nIF(MNN_BUILD_DEMO)\ninclude(${CMAKE_CURRENT_LIST_DIR}/demo/exec/CMakeLists.txt)\nENDIF()\nIF(MNN_BUILD_DIFFUSION AND MNN_BUILD_OPENCV AND MNN_IMGCODECS)\ninclude(${CMAKE_CURRENT_LIST_DIR}/transformers/diffusion/CMakeLists.txt)\nENDIF()\nIF(MNN_BUILD_TOOLS)\ninclude(${CMAKE_CURRENT_LIST_DIR}/tools/cpp/CMakeLists.txt)\nENDIF()\nIF(MNN_BUILD_TEST)\ninclude(${CMAKE_CURRENT_LIST_DIR}/test/CMakeLists.txt)\nENDIF()\nIF(MNN_BUILD_BENCHMARK)\ninclude(${CMAKE_CURRENT_LIST_DIR}/benchmark/CMakeLists.txt)\nENDIF()\nIF(MNN_BUILD_QUANTOOLS)\ninclude(${CMAKE_CURRENT_LIST_DIR}/tools/quantization/CMakeLists.txt)\nENDIF()\nIF(MNN_EVALUATION)\ninclude(${CMAKE_CURRENT_LIST_DIR}/tools/evaluation/CMakeLists.txt)\nENDIF()\n\n# Install headers\nIF(CMAKE_SYSTEM_NAME MATCHES \"^Android\" AND NOT MNN_BUILD_FOR_ANDROID_COMMAND)\n    IF(NOT NATIVE_INCLUDE_OUTPUT)\n      set(NATIVE_INCLUDE_OUTPUT \".\")\n    ENDIF()\n    set(MNN_INCLUDE_OUTPUT ${NATIVE_INCLUDE_OUTPUT}/MNN)\n    add_custom_command(\n      TARGET MNN\n      POST_BUILD\n      COMMAND ${CMAKE_COMMAND}\n      -E make_directory \"${MNN_INCLUDE_OUTPUT}/\"\n    )\n    add_custom_command(\n      TARGET MNN\n      POST_BUILD\n      COMMAND ${CMAKE_COMMAND}\n      -E make_directory \"${MNN_INCLUDE_OUTPUT}/expr/\"\n    )\n    FOREACH(header ${MNN_PUB_HDRS})\n      add_custom_command(\n        TARGET MNN\n        POST_BUILD\n        COMMAND ${CMAKE_COMMAND}\n        ARGS -E copy ${header} \"${MNN_INCLUDE_OUTPUT}/\"\n      )\n    ENDFOREACH()\n    FOREACH(header ${MNN_EXPR_PUB_HDRS})\n      add_custom_command(\n        TARGET MNN\n        POST_BUILD\n        COMMAND ${CMAKE_COMMAND}\n        ARGS -E copy ${header} \"${MNN_INCLUDE_OUTPUT}/expr/\"\n      )\n    ENDFOREACH()\nELSEIF(NOT APPLE)\n  INSTALL(FILES ${MNN_PUB_HDRS} DESTINATION include/MNN/)\n  INSTALL(FILES ${MNN_EXPR_PUB_HDRS} DESTINATION include/MNN/expr/)\n  install(TARGETS MNN\n      LIBRARY DESTINATION lib\n      ARCHIVE DESTINATION lib\n  )\nELSE()\n  install(TARGETS MNN\n      LIBRARY DESTINATION lib\n      ARCHIVE DESTINATION lib\n      FRAMEWORK DESTINATION /Library/Frameworks/\n  )\n  IF(MNN_BUILD_OPENCV)\n    if (NOT MNN_AAPL_FMWK)\n        INSTALL(FILES ${MNN_CV_HDRS} DESTINATION include/MNN/cv)\n        INSTALL(FILES ${MNN_CV_IMGHDRS} DESTINATION include/MNN/cv/imgproc)\n    endif()\n    FOREACH(HDR ${MNN_CV_HDRS})\n      SET_SOURCE_FILES_PROPERTIES(${HDR} PROPERTIES MACOSX_PACKAGE_LOCATION Headers/cv/ )\n    ENDFOREACH()\n    FOREACH(HDR ${MNN_CV_IMGHDRS})\n      SET_SOURCE_FILES_PROPERTIES(${HDR} PROPERTIES MACOSX_PACKAGE_LOCATION Headers/cv/imgproc )\n    ENDFOREACH()\n  ENDIF()\n  IF(MNN_BUILD_AUDIO)\n    if (NOT MNN_AAPL_FMWK)\n      INSTALL(FILES ${MNN_AUDIO_HDRS} DESTINATION include/MNN/audio)\n    endif()\n    FOREACH(HDR ${MNN_AUDIO_HDRS})\n      SET_SOURCE_FILES_PROPERTIES(${HDR} PROPERTIES MACOSX_PACKAGE_LOCATION Headers/audio/ )\n    ENDFOREACH()\n  ENDIF()\n  IF(MNN_BUILD_LLM)\n    if (NOT MNN_AAPL_FMWK)\n        INSTALL(FILES ${MNN_LLM_HDRS} DESTINATION include/MNN/llm)\n    endif()\n    FOREACH(HDR ${MNN_LLM_HDRS})\n      SET_SOURCE_FILES_PROPERTIES(${HDR} PROPERTIES MACOSX_PACKAGE_LOCATION Headers/llm )\n    ENDFOREACH()\n  ENDIF()\n\n  if (NOT MNN_AAPL_FMWK)\n      INSTALL(FILES ${MNN_PUB_HDRS} DESTINATION include/MNN/)\n      INSTALL(FILES ${MNN_EXPR_PUB_HDRS} DESTINATION include/MNN/expr/)\n  endif()\n  FOREACH(HDR ${MNN_EXPR_PUB_HDRS})\n    SET_SOURCE_FILES_PROPERTIES(${HDR} PROPERTIES MACOSX_PACKAGE_LOCATION Headers/expr/ )\n  ENDFOREACH()\n  FOREACH(HDR ${MNN_PUB_HDRS})\n    SET_SOURCE_FILES_PROPERTIES(${HDR} PROPERTIES MACOSX_PACKAGE_LOCATION Headers/ )\n  ENDFOREACH()\n  IF(MNN_METAL)\n    SET_SOURCE_FILES_PROPERTIES(${CMAKE_CURRENT_BINARY_DIR}/mnn.metallib PROPERTIES MACOSX_PACKAGE_LOCATION Resources/)\n  ENDIF()\nENDIF()\nif (MNN_JNI)\n    add_subdirectory(${CMAKE_CURRENT_LIST_DIR}/source/jni/)\nendif()\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 3.5087890625,
          "content": "# 🚀 Contribute to My Amazing Fork of MNN 🤖\n\nThank you for considering contributing to this fantastic fork of MNN! Your contributions are like rocket fuel for this project, propelling us to new heights. Whether you're smashing bugs, adding awesome features, or turbocharging our documentation, you're a key player in this AI adventure. 🌟\n\n## Table of Contents\n- [How Can You Contribute?](#how-can-you-contribute)\n- [🐞 Reporting Issues](#reporting-issues)\n- [✨ Feature Requests](#feature-requests)\n- [🚀 Pull Requests](#pull-requests)\n- [📝 Coding Guidelines](#coding-guidelines)\n- [📜 License](#license)\n\n## How Can You Contribute?\n\nThere are several ways you can be part of this AI extravaganza:\n\n1. **🐞 Report Issues:** Help us exterminate those pesky bugs by reporting them.\n2. **✨ Feature Requests:** Suggest mind-blowing features and improvements.\n3. **🚀 Pull Requests:** Strap in and contribute code to launch this project to the stars.\n4. **📝 Documentation Improvements:** Create clear and engaging documentation to make our mission crystal clear.\n5. **📣 Spread the Word:** Shout about this project from the mountaintops and share it with fellow space explorers.\n\n## 🐞 Reporting Issues\n\nIf you stumble upon a space-time anomaly or any other glitches, please report them. We're looking for detailed, time-travel-quality issue reports. 🌀 Here's how to do it:\n\n1. Check if the issue has already been reported in the [🔍 Issues](https://github.com/alibaba/MNN/issues) section.\n2. If it's an uncharted territory, hit the \"New Issue\" button and provide a clear and descriptive title, along with details about the anomaly.\n3. Share any relevant information, like star maps, your spacecraft's operating system, and steps to recreate the anomaly.\n\n## ✨ Feature Requests\n\nGot a groundbreaking idea that could revolutionize our AI galaxy? Share it with us! Here's how to request a feature:\n\n1. Check if your concept is already orbiting in the [🔍 Issues](https://github.com/alibaba/MNN/issues) section.\n2. If it's a new feature request, press the \"New Issue\" button and use the \"Feature Request\" template to beam down the details of your vision.\n\n## 🚀 Pull Requests\n\nWe invite you to join us on this cosmic journey. To submit a pull request, follow these stellar steps:\n\n1. 🌌 Fork this repository and create your very own starbase.\n2. Blast off with a `new-branch` branch for your galactic changes.\n3. Make your modifications, ensuring your code adheres to the project's cosmic coding guidelines.\n4. Perform gravity-defying tests to verify that your changes are warp-speed ready.\n5. Document your discoveries and provide clear commit messages.\n6. Transmit your changes to your starbase by pushing to your fork.\n7. Launch a pull request to the `new-branch` branch of this repository, and we'll navigate the rest of the way together.\n\nOur starship will review your pull request, communicate in warp speed, and collaborate to merge it into the main repository.\n\n## 📝 Coding Guidelines\n\nPlease ensure your code adheres to the coding guidelines and coding style used in the original MNN project. Consistency in coding style helps maintain the integrity of our codebase.\n\n## 📜 License\n\nBy joining this interstellar mission, you agree that your contributions will be licensed under the Apache 2.0 LICENSE. Together, we'll explore the AI universe and make it an exciting and accessible journey for all sentient beings in the galaxy. 🌌\n\nThank you for contributing to this amazing fork of MNN! Together, we'll reach the stars and beyond. 🚀🌠\n"
        },
        {
          "name": "MNN.podspec",
          "type": "blob",
          "size": 4.021484375,
          "content": "Pod::Spec.new do |s|\n  s.name         = \"MNN\"\n  s.version      = \"2.2.0\"\n  s.summary      = \"MNN\"\n\n  s.description  = <<-DESC\n                    MNN is a lightweight deep neural network inference framework. It loads models and do inference on devices.\n                   DESC\n\n  s.homepage     = \"https://github.com/alibaba/MNN\"\n  s.license = {\n    :type => 'Apache License, Version 2.0',\n    :text => <<-LICENSE\n                      Copyright © 2018, Alibaba Group Holding Limited\n\n                      Licensed under the Apache License, Version 2.0 (the \"License\");\n                      you may not use this file except in compliance with the License.\n                      You may obtain a copy of the License at\n\n                        http://www.apache.org/licenses/LICENSE-2.0\n\n                      Unless required by applicable law or agreed to in writing, software\n                      distributed under the License is distributed on an \"AS IS\" BASIS,\n                      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n                      See the License for the specific language governing permissions and\n                      limitations under the License.\n    LICENSE\n  }\n\n  s.author       = { \"MNN\" => \"MNN@alibaba-inc.com\" }\n  s.platform     = :ios\n  s.ios.deployment_target = '8.0'\n  s.requires_arc = true\n\n  #s.source =  { :git => \"git@github.com:alibaba/MNN.git\", :branch => 'master' }\n  s.source = {:git => \"/Users/zhang/Development/AliNNPrivate/\",:branch=> 'head'}\n  s.frameworks = 'Metal', 'Accelerate', 'CoreML'\n  s.library = 'c++'\n  s.source_files = \\\n  'include/MNN/*.{h,hpp}',\\\n  'include/MNN/expr/*.{h,hpp}',\\\n  'schema/current/*.{h}',\\\n  '3rd_party/flatbuffers/include/flatbuffers/*.{h}',\\\n  'source/internal/logging/*.{hpp,cpp}',\\\n  'source/internal/logging/ios/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/internal/logging/aliyun-log-c-sdk/src/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/core/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/common/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/utils/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/geometry/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/cv/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/math/**/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/shape/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  #'source/backend/arm82/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  #'source/backend/arm82/asm/**/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/bf16/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/arm/**/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/compute/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/metal/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/backend/coreml/backend/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/backend/coreml/execution/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/backend/coreml/mlmodel/src/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'express/**/*.{hpp,cpp}',\\\n  'tools/cv/include/**/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'tools/cv/source/imgproc/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'tools/cv/source/calib3d/*.{h,c,m,mm,cc,hpp,cpp,metal}'\n\n  s.header_mappings_dir = 'include'\n  s.subspec 'cv' do |sp|\n    sp.source_files = 'tools/cv/include/**/*.hpp'\n    sp.header_mappings_dir = 'tools/cv/include'\n    sp.xcconfig = { 'ALWAYS_SEARCH_USER_PATHS' => 'NO' }\n  end\n\n  s.compiler_flags = '-arch arm64 -march=armv8.2-a+simd+fp16'\n  s.pod_target_xcconfig = {'METAL_LIBRARY_FILE_BASE' => 'mnn', 'HEADER_SEARCH_PATHS' => '\"$(PODS_TARGET_SRCROOT)/include\" \"$(PODS_TARGET_SRCROOT)/3rd_party/flatbuffers/include\" \"$(PODS_TARGET_SRCROOT)/source\" \"$(PODS_TARGET_SRCROOT)/3rd_party/half\" \"$(PODS_TARGET_SRCROOT)/source/backend/coreml/mlmodel/include\" \"$(PODS_TARGET_SRCROOT)/tools/cv/include\"', 'GCC_PREPROCESSOR_DEFINITIONS' => '$(inherited) MNN_CODEGEN_REGISTER=1 MNN_SUPPORT_TFLITE_QUAN=1 MNN_METAL_ENABLED=1 MNN_SUPPORT_BF16=1 MNN_COREML_ENABLED=1 USE_LZ4_FLAG=1 MNN_INTERNAL_ENABLED=1 MNN_USE_SPARSE_COMPUTE=1'}\n  s.user_target_xcconfig = { 'OTHER_LDFLAGS' => '-force_load $(BUILD_DIR)/$(CONFIGURATION)$(EFFECTIVE_PLATFORM_NAME)/MNN/libMNN.a', 'HEADER_SEARCH_PATHS' => '\"$(PODS_TARGET_SRCROOT)/include\"' }\nend\n"
        },
        {
          "name": "MNN.sln",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "MNN_Render.podspec",
          "type": "blob",
          "size": 4.236328125,
          "content": "Pod::Spec.new do |s|\n  s.name         = \"MNN\"\n  s.version      = \"2.2.0\"\n  s.summary      = \"MNN\"\n\n  s.description  = <<-DESC\n                    MNN is a lightweight deep neural network inference framework. It loads models and do inference on devices.\n                   DESC\n\n  s.homepage     = \"https://github.com/alibaba/MNN\"\n  s.license = {\n    :type => 'Apache License, Version 2.0',\n    :text => <<-LICENSE\n                      Copyright © 2018, Alibaba Group Holding Limited\n\n                      Licensed under the Apache License, Version 2.0 (the \"License\");\n                      you may not use this file except in compliance with the License.\n                      You may obtain a copy of the License at\n\n                        http://www.apache.org/licenses/LICENSE-2.0\n\n                      Unless required by applicable law or agreed to in writing, software\n                      distributed under the License is distributed on an \"AS IS\" BASIS,\n                      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n                      See the License for the specific language governing permissions and\n                      limitations under the License.\n    LICENSE\n  }\n\n  s.author       = { \"MNN\" => \"MNN@alibaba-inc.com\" }\n  s.platform     = :ios\n  s.ios.deployment_target = '8.0'\n  s.requires_arc = true\n\n  #s.source =  { :git => \"git@github.com:alibaba/MNN.git\", :branch => 'master' }\n  s.source = {:git => \"/Users/zhang/Development/AliNNPrivate/\",:branch=> 'head'}\n  s.frameworks = 'Metal', 'Accelerate', 'CoreML'\n  s.library = 'c++'\n  s.source_files = \\\n  'include/MNN/*.{h,hpp}',\\\n  'include/MNN/expr/*.{h,hpp}',\\\n  'schema/current/*.{h}',\\\n  '3rd_party/flatbuffers/include/flatbuffers/*.{h}',\\\n  'source/internal/logging/*.{hpp,cpp}',\\\n  'source/internal/logging/ios/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/internal/logging/aliyun-log-c-sdk/src/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/core/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/common/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/utils/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/geometry/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/cv/**/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/math/**/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/shape/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  'source/shape/render/*.{h,c,m,mm,cc,hpp,cpp}',\\\n  #'source/backend/arm82/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  #'source/backend/arm82/asm/**/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/render/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/bf16/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/arm/**/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/cpu/compute/*.{h,c,m,mm,cc,S,hpp,cpp}',\\\n  'source/backend/metal/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/backend/metal/render/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/backend/coreml/backend/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/backend/coreml/execution/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'source/backend/coreml/mlmodel/src/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'express/**/*.{hpp,cpp}',\\\n  'tools/cv/include/**/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'tools/cv/source/imgproc/*.{h,c,m,mm,cc,hpp,cpp,metal}',\\\n  'tools/cv/source/calib3d/*.{h,c,m,mm,cc,hpp,cpp,metal}'\n\n  s.header_mappings_dir = 'include'\n  s.subspec 'cv' do |sp|\n    sp.source_files = 'tools/cv/include/**/*.hpp'\n    sp.header_mappings_dir = 'tools/cv/include'\n    sp.xcconfig = { 'ALWAYS_SEARCH_USER_PATHS' => 'NO' }\n  end\n\n  s.compiler_flags = '-arch arm64 -march=armv8.2-a+simd+fp16'\n  s.pod_target_xcconfig = {'METAL_LIBRARY_FILE_BASE' => 'mnn', 'HEADER_SEARCH_PATHS' => '\"$(PODS_TARGET_SRCROOT)/include\" \"$(PODS_TARGET_SRCROOT)/3rd_party/flatbuffers/include\" \"$(PODS_TARGET_SRCROOT)/source\" \"$(PODS_TARGET_SRCROOT)/3rd_party/half\" \"$(PODS_TARGET_SRCROOT)/source/backend/coreml/mlmodel/include\" \"$(PODS_TARGET_SRCROOT)/tools/cv/include\"', 'GCC_PREPROCESSOR_DEFINITIONS' => '$(inherited) MNN_CODEGEN_REGISTER=1 MNN_SUPPORT_TFLITE_QUAN=1 MNN_METAL_ENABLED=1 MNN_METAL_FULL_PRECISION=1 MNN_SUPPORT_RENDER=1 MNN_SUPPORT_BF16=1 MNN_COREML_ENABLED=1 USE_LZ4_FLAG=1 MNN_INTERNAL_ENABLED=1 MNN_USE_SPARSE_COMPUTE=1'}\n  s.user_target_xcconfig = { 'OTHER_LDFLAGS' => '-force_load $(BUILD_DIR)/$(CONFIGURATION)$(EFFECTIVE_PLATFORM_NAME)/MNN/libMNN.a', 'HEADER_SEARCH_PATHS' => '\"$(PODS_TARGET_SRCROOT)/include\"' }\nend\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 8.8369140625,
          "content": "![MNN](doc/banner.png)\n\n[中文版本](README_CN.md)\n\n[MNN Homepage](http://www.mnn.zone)\n\n## Intro\nMNN is a highly efficient and lightweight deep learning framework. It supports inference and training of deep learning models and has industry-leading performance for inference and training on-device. At present, MNN has been integrated into more than 30 apps of Alibaba Inc, such as Taobao, Tmall, Youku, DingTalk, Xianyu, etc., covering more than 70 usage scenarios such as live broadcast, short video capture, search recommendation, product searching by image, interactive marketing, equity distribution, security risk control. In addition, MNN is also used on embedded devices, such as IoT.\n\n[MNN-LLM](https://github.com/alibaba/MNN/tree/master/transformers/llm) is a large language model runtime solution developed based on the MNN engine. The mission of this project is to deploy LLM models locally on everyone's platforms(Mobile Phone/PC/IOT). It supports popular large language models such as Qianwen, Baichuan, Zhipu, LLAMA, and others. [MNN-LLM User guide](https://mnn-docs.readthedocs.io/en/latest/transformers/llm.html)\n\n[MNN-Diffusion](https://github.com/alibaba/MNN/tree/master/transformers/diffusion) is a stable diffusion model runtime solution developed based on the MNN engine. The mission of this project is to deploy stable diffusion models locally on everyone's platforms. [MNN-Diffusion User guide](https://mnn-docs.readthedocs.io/en/latest/transformers/diffusion.html)\n\n![architecture](doc/architecture.png)\n\nInside Alibaba, [MNN](https://mp.weixin.qq.com/s/5I1ISpx8lQqvCS8tGd6EJw) works as the basic module of the compute container in the [Walle](https://mp.weixin.qq.com/s/qpeCETty0BqqNJV9CMJafA) System, the first end-to-end, general-purpose, and large-scale production system for device-cloud collaborative machine learning, which has been published in the top system conference OSDI’22. The key design principles of MNN and the extensive benchmark testing results (vs. TensorFlow, TensorFlow Lite, PyTorch, PyTorch Mobile, TVM) can be found in the OSDI paper. The scripts and instructions for benchmark testing are put in the path “/benchmark”. If MNN or the design of Walle helps your research or production use, please cite our OSDI paper as follows:\n\n    @inproceedings {proc:osdi22:walle,\n        author = {Chengfei Lv and Chaoyue Niu and Renjie Gu and Xiaotang Jiang and Zhaode Wang and Bin Liu and Ziqi Wu and Qiulin Yao and Congyu Huang and Panos Huang and Tao Huang and Hui Shu and Jinde Song and Bin Zou and Peng Lan and Guohuan Xu and Fei Wu and Shaojie Tang and Fan Wu and Guihai Chen},\n        title = {Walle: An {End-to-End}, {General-Purpose}, and {Large-Scale} Production System for {Device-Cloud} Collaborative Machine Learning},\n        booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},\n        year = {2022},\n        isbn = {978-1-939133-28-1},\n        address = {Carlsbad, CA},\n        pages = {249--265},\n        url = {https://www.usenix.org/conference/osdi22/presentation/lv},\n        publisher = {USENIX Association},\n        month = jul,\n    }\n\n\n## Documentation and Workbench\nMNN's docs are in place in [Read the docs](https://mnn-docs.readthedocs.io/en/latest).\n\nYou can also read docs/README to build docs's html.\n\nMNN Workbench could be downloaded from [MNN's homepage](http://www.mnn.zone), which provides pretrained models, visualized training tools, and one-click deployment of models to devices.\n\n## Key Features\n### Lightweight\n- Optimized for devices, no dependencies, can be easily deployed to mobile devices and a variety of embedded devices.\n- iOS platform: static library size will full option for armv7+arm64 platforms is about 12MB, size increase of linked executables is about 2M.\n- Android platform: core so size is about 800KB (armv7a - c++_shared).\n- Using MNN_BUILD_MINI can reduce package size by about 25%, with a limit of fixed model input size\n- Support FP16 / Int8 quantize, can reduce model size 50%-70%\n\n### Versatility\n- Supports `Tensorflow`, `Caffe`, `ONNX`,`Torchscripts` and supports common neural networks such as `CNN`, `RNN`, `GAN`, `Transformer`.\n- Supports AI model with multi-inputs or multi-outputs, every kind of dimension format, dynamic inputs, controlflow.\n- MNN supports approximate full OPs used for the AI Model. The converter supports 178 `Tensorflow` OPs, 52 `Caffe` OPs, 163 `Torchscripts` OPs, 158 `ONNX` OPs.\n- Supports iOS 8.0+, Android 4.3+, and embedded devices with POSIX interface.\n- Supports hybrid computing on multiple devices. Currently supports CPU and GPU.\n\n\n### High performance\n- Implements core computing with lots of optimized assembly code to make full use of the ARM / x64 CPU.\n- Use Metal / OpenCL / Vulkan to support GPU inference on mobile.\n- Use CUDA and tensorcore to support NVIDIA GPU for better performance\n- Convolution and transposition convolution algorithms are efficient and stable. The Winograd convolution algorithm is widely used to better symmetric convolutions such as 3x3,4x4,5x5,6x6,7x7.\n- Twice speed increase for the new architecture ARM v8.2 with FP16 half-precision calculation support. 2.5 faster to use sdot for ARM v8.2 and VNNI.\n\n### Ease of use\n- Support use MNN's OP to do numerical calculating like numpy.\n- Support lightweight image process module like OpenCV, which is only 100k.\n- Support build model and train it on PC / mobile.\n- MNN Python API helps ML engineers to easily use MNN to infer, train, and process images, without dipping their toes in C++ code.\n\nThe Architecture / Precision MNN supported is shown below:\n\n- S ：Support and work well, deeply optimized, recommend to use\n- A ：Support and work well, can use\n- B ：Support but has bug or not optimized, no recommend to use\n- C ：Not Support\n\n| Architecture / Precision |  | Normal | FP16 | BF16 | Int8 |\n| --- | --- | --- | --- | --- | --- |\n| CPU | Native | B | C | B | B |\n|  | x86/x64-SSE4.1 | A | B | B | A |\n|  | x86/x64-AVX2 | S | B | B | A |\n|  | x86/x64-AVX512 | S | B | B | S |\n|  | ARMv7a | S | S (ARMv8.2) | S | S |\n|  | ARMv8 | S | S (ARMv8.2) | S(ARMv8.6) | S |\n| GPU | OpenCL | A | S | C | C |\n|  | Vulkan | A | A | C | C |\n|  | Metal | A | S | C | C |\n|  | CUDA | A | S | C | C |\n| NPU | CoreML | B | B | C | C |\n|  | HIAI | B | C | C | B |\n|  | NNAPI | B | B | C | C |\n\n\n\n## Tools\n\nBase on MNN (Tensor compute engine), we provided a series of tools for inference, train and general computation.\n\n- MNN-Converter: Convert other models to MNN models for inference, such as Tensorflow(lite), Caffe, ONNX, Torchscripts. And do graph optimization to reduce computation.\n- MNN-Compress: Compress model to reduce size and increase performance / speed\n- MNN-Express: Support model with controlflow, use MNN's OP to do general-purpose computing.\n- MNN-CV: An OpenCV-like library, but based on MNN and then much more lightweight.\n- MNN-Train: Support train MNN model.\n\n## How to Discuss and Get Help From the MNN Community\n\nThe group discussions are predominantly Chinese. But we welcome and will help English speakers.\n\nDingtalk discussion groups:\n\nGroup #1 (Full): 23329087\n\nGroup #2 (Full): 23350225\n\nGroup #3: QR code:\n\n![MNN-3](doc/dingdingmnn3.png)\n\n## Historical Paper\n\nThe preliminary version of MNN, as mobile inference engine and with the focus on manual optimization, has also been published in MLSys 2020. Please cite the paper, if MNN previously helped your research:\n\n\n    @inproceedings{alibaba2020mnn,\n      author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},\n      title = {MNN: A Universal and Efficient Inference Engine},\n      booktitle = {MLSys},\n      year = {2020}\n    }\n\n\n## License\nApache 2.0\n\n## Acknowledgement\nMNN participants: Taobao Technology Department, Search Engineering Team, DAMO Team, Youku and other Alibaba Group employees.\n\nMNN refers to the following projects:\n- [Caffe](https://github.com/BVLC/caffe)\n- [flatbuffer](https://github.com/google/flatbuffers)\n- [gemmlowp](https://github.com/google/gemmlowp)\n- [Google Vulkan demo](http://www.github.com/googlesamples/android-vulkan-tutorials)\n- [Halide](https://github.com/halide/Halide)\n- [Mace](https://github.com/XiaoMi/mace)\n- [ONNX](https://github.com/onnx/onnx)\n- [protobuffer](https://github.com/protocolbuffers/protobuf)\n- [skia](https://github.com/google/skia)\n- [Tensorflow](https://github.com/tensorflow/tensorflow)\n- [ncnn](https://github.com/Tencent/ncnn)\n- [paddle-mobile](https://github.com/PaddlePaddle/paddle-mobile)\n- [stb](https://github.com/nothings/stb)\n- [rapidjson](https://github.com/Tencent/rapidjson)\n- [pybind11](https://github.com/pybind/pybind11)\n- [pytorch](https://github.com/pytorch/pytorch)\n- [bolt](https://github.com/huawei-noah/bolt)\n- [libyuv](https://chromium.googlesource.com/libyuv/libyuv)\n- [libjpeg](https://github.com/libjpeg-turbo/libjpeg-turbo)\n- [opencv](https://github.com/opencv/opencv)\n"
        },
        {
          "name": "README_CN.md",
          "type": "blob",
          "size": 9.08984375,
          "content": "![MNN](doc/banner.png)\n\n[English Version](README.md)\n\n[MNN Homepage](http://www.mnn.zone)\n\n[MNN](https://github.com/alibaba/MNN)是一个轻量级的深度神经网络引擎，支持深度学习的推理与训练。适用于服务器/个人电脑/手机/嵌入式各类设备。目前，MNN已经在阿里巴巴的手机淘宝、手机天猫、优酷等30多个App中使用，覆盖直播、短视频、搜索推荐、商品图像搜索、互动营销、权益发放、安全风控等场景。\n\n[MNN-LLM](https://github.com/alibaba/MNN/tree/master/transformers/llm)是基于MNN引擎开发的大语言模型运行方案，解决大语言模型在本地设备的高效部署问题(手机/个人电脑/嵌入式设备)。支持常见的千问/百川/智谱/LLAMA等大语言模型。使用教程：[MNN-LLM使用教程](https://mnn-docs.readthedocs.io/en/latest/transformers/llm.html)\n\n[MNN-Diffusion](https://github.com/alibaba/MNN/tree/master/transformers/diffusion)是基于MNN引擎开发的Stable Diffusion文生图模型运行方案，解决Stable Diffusion模型在本地设备的高效部署问题。使用教程：[MNN-Diffusion使用教程](https://mnn-docs.readthedocs.io/en/latest/transformers/diffusion.html)\n\n![架构图](doc/architecture.png)\n\n在阿里巴巴中，[MNN](https://mp.weixin.qq.com/s/5I1ISpx8lQqvCS8tGd6EJw)被用作为[Walle](https://mp.weixin.qq.com/s/qpeCETty0BqqNJV9CMJafA)系统中计算容器的基础模块。Walle是首个端到端、通用型、规模化产业应用的端云协同机器学习系统，发表于操作系统顶会OSDI 2022。Walle的论文中解释了MNN的关键设计理念，并提供了MNN相对于其他深度学习框架（TensorFlow, TensorFlow Lite, PyTorch, PyTorch Mobile, TVM）的benchmark测试结果。相关测试脚本和说明文档被放在“/benchmark”目录下。如果MNN或Walle的设计对你的研究或生产有所助益，欢迎引用我们的OSDI论文：\n\n    @inproceedings {proc:osdi22:walle,\n        author = {Chengfei Lv and Chaoyue Niu and Renjie Gu and Xiaotang Jiang and Zhaode Wang and Bin Liu and Ziqi Wu and Qiulin Yao and Congyu Huang and Panos Huang and Tao Huang and Hui Shu and Jinde Song and Bin Zou and Peng Lan and Guohuan Xu and Fei Wu and Shaojie Tang and Fan Wu and Guihai Chen},\n        title = {Walle: An {End-to-End}, {General-Purpose}, and {Large-Scale} Production System for {Device-Cloud} Collaborative Machine Learning},\n        booktitle = {16th USENIX Symposium on Operating Systems Design and Implementation (OSDI 22)},\n        year = {2022},\n        isbn = {978-1-939133-28-1},\n        address = {Carlsbad, CA},\n        pages = {249--265},\n        url = {https://www.usenix.org/conference/osdi22/presentation/lv},\n        publisher = {USENIX Association},\n        month = jul,\n    }\n\n## 文档与工作台\nMNN文档：\n- [最新文档(readthedocs)](https://mnn-docs.readthedocs.io/en/latest/index.html)\n\n- 也可阅读 docs/README ，编译本地文档\n\n\n[MNN官网](http://www.mnn.zone)上还可以下载MNN团队全新力作MNN工作台，涵盖开箱即用模型、可视化训练等工具，更可以一键部署到多端设备。\n\n## 整体特点\n\n### 轻量性 \n\n- 主体功能（模型推理CPU+GPU）无任何依赖，代码精简，可以方便地部署到移动设备和各种嵌入式设备中。 \n   - iOS平台：功能全开的MNN静态库 armv7+arm64大小12MB左右，链接生成可执行文件增加大小2M左右。可裁剪主体功能后静态库大小6.1M ，链接生成可执行文件增加大小 600 KB。\n   - Android平台：主体功能 armv7a - c++_shared 动态库大小800KB左右。\n- 支持采用 Mini 编辑选项进一步降低包大小，大约能在上述库体积基础上进一步降低 25% 左右。\n- 支持模型FP16/Int8压缩与量化，可减少模型50% - 75% 的体积\n\n### 通用性 \n\n- 支持 Tensorflow、Caffe、ONNX、Torchscripts 等主流模型文件格式，支持CNN / RNN / GAN / Transformer 等主流网络结构。\n- 支持多输入多输出，支持任意维度的输入输出，支持动态输入（输入大小可变），支持带控制流的模型\n- 算子丰富，支持 178 个Tensorflow Op、52个 Caffe Op、163个 Torchscipts Op、158 个 ONNX Op（ONNX 基本完整支持）\n- 支持 服务器 / 个人电脑 / 手机 及具有POSIX接口的嵌入式设备，支持使用设备的 CPU / GPU 计算，支持部分设备的 NPU 计算（IOS 11 + CoreML / Huawei + HIAI / Android + NNAPI）\n- 支持 Windows / iOS 8.0+ / Android 4.3+ / Linux  及具有POSIX接口的操作系统\n\n### 高性能\n\n- 对iOS / Android / PC / Server 的CPU架构进行了适配，编写SIMD代码或手写汇编以实现核心运算，充分发挥 CPU的算力，单线程下运行常见CV模型接近设备算力峰值\n- 支持基于 Metal / OpenCL / Vulkan 使用移动端设备上的GPU进行推理\n- 支持基于 CUDA 使用 PC / Server 上的 NVIDIA GPU 实现更快速的推理\n- 广泛运用了 Winograd 卷积算法提升卷积性能，首次在业界工程实践中实现转置卷积的Winograd算法优化与矩阵乘的Strassen算法优化，并取得加速效果\n- 支持低精度计算（ int8 / fp16 / bf16）以提升推理性能。并对 ARMv8.2 和 AVX512架构的相关指令进行了适配，这两种架构下有更好的加速效果\n\n### 易用性\n\n- 支持使用 MNN 的算子进行常用的数值计算，覆盖 numpy 常用功能\n- 提供 MNN CV 模块，支持图像仿射变换与归一化等 MNN_CV 库，支持常用的图像处理（armv7a 架构下小于 100 k ）\n- 支持各平台下的模型训练，尤其是移动端上的模型训练\n- 支持 python 调用\n\nMNN适配的硬件架构与精度详见下表：\n\n- S ：支持，深度优化并已有应用场景，推荐使用\n- A ：支持，有初步优化或已有应用场景，可以使用\n- B ：支持，无优化或在实验状态，不推荐使用\n- C ：不支持\n\n| Architecture / Precision |  | Normal | FP16 | BF16 | Int8 |\n| --- | --- | --- | --- | --- | --- |\n| CPU | Native | B | C | B | B |\n|  | x86/x64-SSE4.1 | A | B | B | A |\n|  | x86/x64-AVX2 | S | B | B | A |\n|  | x86/x64-AVX512 | S | B | B | S |\n|  | ARMv7a | S | S (ARMv8.2) | S | S |\n|  | ARMv8 | S | S (ARMv8.2) | S(ARMv8.6) | S |\n| GPU | OpenCL | A | S | C | C |\n|  | Vulkan | A | A | C | C |\n|  | Metal | A | S | C | C |\n|  | CUDA | A | S | C | C |\n| NPU | CoreML | B | B | C | C |\n|  | HIAI | B | C | C | B |\n|  | NNAPI | B | B | C | C |\n\n\n## 工具\n\n基于MNN (张量计算引擎)，提供了一系列工具，以支持模型推理、训练和通用计算：\n\n\n- MNN-Converter：模型转换工具，由Frontends和Graph Optimize构成。前者负责支持不同的训练框架，MNN当前支持Tensorflow(Lite)、Caffe、ONNX(PyTorch/MXNet的模型可先转为ONNX模型再转到MNN)和Torchscripts；后者通过算子融合、算子替代、布局调整等方式优化图，一般离线运行。\n- MNN-Compress: 模型压缩工具，在一定的精度误差许可下，对MNN模型进行压缩，减少模型体积，提升运行性能。\n- MNN-Express ：支持带控制流的模型运行，支持调用 MNN 的算子进行自定义的计算。\n- MNN-CV ：类似 OpenCV ，但核心计算功能基于 MNN 实现的图像处理算法库\n- MNN-Train ：MNN 训练模块，支持各平台训练\n\n## 社区交流与反馈\n钉钉群组：\n\n- 钉钉群1:23329087 \n- 钉钉群2:23350225\n- 钉钉群3:扫描二维码加入\n\n![MNN-3](doc/dingdingmnn3.png)\n\n\n## 历史论文\n\nMNN初步版本的[论文](https://arxiv.org/pdf/2002.12418.pdf)也曾在MLSys 2020上面发表。该论文主要关注MNN作为移动端机器学习推理引擎的手动算子优化。如果MNN之前对你的研究有所助益，欢迎引用MNN的MLSys论文：\n\n\t@inproceedings{alibaba2020mnn,\n      author = {Jiang, Xiaotang and Wang, Huan and Chen, Yiliu and Wu, Ziqi and Wang, Lichuan and Zou, Bin and Yang, Yafeng and Cui, Zongyang and Cai, Yu and Yu, Tianhang and Lv, Chengfei and Wu, Zhihua},\n      title = {MNN: A Universal and Efficient Inference Engine},\n      booktitle = {MLSys},\n      year = {2020}\n    }\n\n## License\nApache 2.0\n\n## 致谢\nMNN参与人员：淘宝技术部、搜索工程团队、达摩院团队、优酷等集团员工。\n\nMNN参考、借鉴了下列项目：\n\n- [Caffe](https://github.com/BVLC/caffe)\n- [flatbuffer](https://github.com/google/flatbuffers)\n- [gemmlowp](https://github.com/google/gemmlowp)\n- [Google Vulkan demo](http://www.github.com/googlesamples/android-vulkan-tutorials)\n- [Halide](https://github.com/halide/Halide)\n- [Mace](https://github.com/XiaoMi/mace)\n- [ONNX](https://github.com/onnx/onnx)\n- [protobuffer](https://github.com/protocolbuffers/protobuf)\n- [skia](https://github.com/google/skia)\n- [Tensorflow](https://github.com/tensorflow/tensorflow)\n- [ncnn](https://github.com/Tencent/ncnn)\n- [paddle-mobile](https://github.com/PaddlePaddle/paddle-mobile)\n- [stb](https://github.com/nothings/stb)\n- [rapidjson](https://github.com/Tencent/rapidjson)\n- [pybind11](https://github.com/pybind/pybind11)\n- [pytorch](https://github.com/pytorch/pytorch)\n- [bolt](https://github.com/huawei-noah/bolt)\n- [libyuv](https://chromium.googlesource.com/libyuv/libyuv)\n- [libjpeg](https://github.com/libjpeg-turbo/libjpeg-turbo)\n- [opencv](https://github.com/opencv/opencv)\n\n"
        },
        {
          "name": "backupcode",
          "type": "tree",
          "content": null
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "ciscripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "codegen",
          "type": "tree",
          "content": null
        },
        {
          "name": "demo",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker_release.sh",
          "type": "blob",
          "size": 0.212890625,
          "content": "# using docker run release\ndocker start mnn_release\ndocker exec -i -e TEST_ID=$(pwd | awk -F \"/\" '{print $(NF-1)}') mnn_release bash <<'EOF'\ncd ~/yanxing_zhaode/cise/space/$TEST_ID/source && ./release.sh pymnn\nexit\nEOF"
        },
        {
          "name": "docker_run.sh",
          "type": "blob",
          "size": 0.1982421875,
          "content": "# using docker run test\ndocker start mnn_ci\ndocker exec -i -e TEST_ID=$(pwd | awk -F \"/\" '{print $(NF-1)}') mnn_ci bash <<'EOF'\ncd ~/yanxing_zhaode/cise/space/$TEST_ID/source && ./test.sh linux\nexit\nEOF\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "express",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "package_scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "project",
          "type": "tree",
          "content": null
        },
        {
          "name": "pymnn",
          "type": "tree",
          "content": null
        },
        {
          "name": "release.sh",
          "type": "blob",
          "size": 1.15234375,
          "content": "get_version() {\n    version_header=\"./include/MNN/MNNDefine.h\"\n    version_major='x'\n    version_minor='x'\n    version_patch='x'\n\n    # 读取 version_header 文件并提取版本信息\n    while IFS='' read -r line || [[ -n \"$line\" ]]; do\n        if echo \"$line\" | grep -q '#define MNN_VERSION_MAJOR'; then\n            version_major=$(echo \"$line\" | awk '{print $3}')\n        elif echo \"$line\" | grep -q '#define MNN_VERSION_MINOR'; then\n            version_minor=$(echo \"$line\" | awk '{print $3}')\n        elif echo \"$line\" | grep -q '#define MNN_VERSION_PATCH'; then\n            version_patch=$(echo \"$line\" | awk '{print $3}')\n        fi\n    done < \"$version_header\"\n\n    mnn_version=\"$version_major.$version_minor.$version_patch\"\n}\n\n\nmnn() {\n    echo 'build mnn release package.'\n    # TODO\n}\n\npymnn() {\n    echo 'build pymnn release package.'\n    get_version\n    ./package_scripts/linux/build_whl.sh -v $mnn_version -o MNN-CPU/py_whl\n    /opt/python/cp39-cp39/bin/python -m twine upload ./MNN-CPU/py_whl/*\n}\n\ncase \"$1\" in\n    mnn)\n        mnn\n        ;;\n    pymnn)\n        pymnn\n        ;;\n    *)\n        $1\n        echo $\"Usage: $0 {mnn|pymnn}\"\n        exit 2\nesac\nexit $?\n"
        },
        {
          "name": "resource",
          "type": "tree",
          "content": null
        },
        {
          "name": "schema",
          "type": "tree",
          "content": null
        },
        {
          "name": "source",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.bat",
          "type": "blob",
          "size": 0.185546875,
          "content": "if %1 EQU x86 (\n    @call \"%vs_env_setup%/vcvarsamd64_x86.bat\"\n    powershell \"%~dp0test.ps1\" -gpu -x86\n) else (\n    @call \"%vs_env_setup%/vcvars64.bat\"\n    powershell \"%~dp0test.ps1\" -gpu\n)"
        },
        {
          "name": "test.ps1",
          "type": "blob",
          "size": 8.78515625,
          "content": "﻿# Powershell Script must be save as UTF-8 with BOM, otherwise system-wide code page will be used, causing garbled code\n\n# MNN-CPU-GPU\n#  |-- include\n#  |-- lib\n#  |    |-- x64\n#  |    |    |-- (Debug/Release x Dynamic/Static x MD/MT)\n#  |    |\n#  |    |-- x86\n#  |         |-- (Debug/Release x Dynamic/Static x MD/MT)\n#  |\n#  |-- tools (Release + Dynamic + MD)\n#  |    |-- x64\n#  |    |-- x86\n#  |\n#  |-- py_whl\n#  |-- py_bridge\n#       |-- include\n#       |-- wrapper\n#       |-- test (Release + Dynamic + MD)\n#            |-- x64\n#            |-- x86\n#       |-- lib\n#            |-- x64\n#            |    |-- (Debug/Release x Dynamic/Static x MD/MT)\n#            |\n#            |-- x86\n#                 |-- (Debug/Release x Dynamic/Static x MD/MT)\n\nParam(\n    [Switch]$gpu,\n    [Switch]$x86\n)\n\n$basedir = $(Split-Path -Parent $MyInvocation.MyCommand.Path)\n$outdir = \"$basedir/$(If ($gpu) {\"MNN-CPU-GPU\"} Else {\"MNN-CPU\"})\"\n$arch = \"$(If ($x86) {\"x86\"} Else {\"x64\"})\"\nWrite-Output $arch\n\n$test_avx512 = ((!$x86) -and $env:avx512_server -and $env:avx512_password)\nif ($test_avx512) {\n    $remote_home = $(Invoke-Expression 'plink -batch -ssh $env:avx512_server -pw $env:avx512_password powershell \"echo `$HOME\"')\n    $remote_dir = \"${remote_home}\\cise-space\\$(Split-Path -Path $(pushd .. ; pwd ; popd) -Leaf)\"\n}\nfunction sync_remote() {\n    Invoke-Expression 'plink -batch -ssh $env:avx512_server -pw $env:avx512_password powershell \"Remove-Item -Recurse $remote_dir -ErrorAction Ignore ; mkdir $remote_dir\"'\n    Invoke-Expression 'pscp -pw $env:avx512_password -r $outdir/tools ${env:avx512_server}:${remote_dir}'\n    Invoke-Expression 'pscp -pw $env:avx512_password tools/script/modelTest.py ${env:avx512_server}:${remote_dir}'\n}\n\nfunction run_remote([String]$cmd) {\n    $tmpfile = New-TemporaryFile\n    Set-Content -Path $tmpfile -Value \"powershell `\"cd ${remote_dir} ;  $cmd`\"\"\n    $output = $(Invoke-Expression 'plink -batch -ssh $env:avx512_server -pw $env:avx512_password -m $tmpfile')\n    Remove-Item $tmpfile\n    return $output\n}\n\nfunction log($case, $title, $blocked, $failed, $passed, $skipped) {\n    Write-Output \"TEST_NAME_${case}: $title\"\n    Write-Output \"TEST_CASE_AMOUNT_${case}: {`\"blocked`\":$blocked,`\"failed`\":$failed,`\"passed`\":$passed,`\"skipped`\":$skipped}\"\n}\n\nfunction failed() {\n    Write-Output \"TEST_NAME_EXCEPTION: Exception\"\n    Write-Output 'TEST_CASE_AMOUNT_EXCEPTION: {\"blocked\":0,\"failed\":1,\"passed\":0,\"skipped\":0}'\n    exit\n}\n\nfunction build_lib_test() {\n    # build_lib_release.ps1 just build release for speed\n    Invoke-Expression \"./package_scripts/win/build_lib_release.ps1 -path $outdir -cibuild $(If ($gpu) {\"-backends 'opencl,vulkan'\"}) $(If ($x86) {'-x86'})\"\n    $WrongNum = [int]$($LastExitCode -ne 0)\n    log \"WINDOWS_LIB\" \"Windows主库编译测试\" 0 $WrongNum $(1 - $WrongNum) 0\n    if ($WrongNum -ne 0) {\n        Write-Output \"### Windows主库编译测试失败，测试终止\"\n        failed\n    }\n}\n\nfunction build_tool_test() {\n    Invoke-Expression \"./package_scripts/win/build_tools.ps1 -path $outdir/tools/$arch $(If ($gpu) {\"-backends 'opencl,vulkan'\"}) -build_all -dynamic_link\"\n    $WrongNum = $($LastExitCode -ne 0)\n    log \"WINDOWS_LIB\" \"Windows工具编译测试\" 0 $WrongNum $(1 - $WrongNum) 0\n    if ($WrongNum -ne 0) {\n        Write-Output \"### Windows工具编译测试失败，测试终止\"\n        failed\n    }\n}\n\nfunction build_whl_test() {\n    $pyenvs = \"py27,py37,py38,py39\"\n    if ($x86) {\n        $pyenvs = \"py27-win32,py37-win32,py38-win32,py39-win32\"\n    }\n    Invoke-Expression \"./package_scripts/win/build_whl.ps1 -version ci_test -path $outdir/py_whl -pyenvs '$pyenvs' $(If ($x86) {'-x86'})\"\n    $WrongNum = $($LastExitCode -ne 0)\n    log \"WINDOWS_LIB\" \"Windows pymnn wheel编译测试\" 0 $WrongNum $(1 - $WrongNum) 0\n    if ($WrongNum -ne 0) {\n        Write-Output \"### Windows pymnn wheel编译测试失败，测试终止\"\n        failed\n    }\n}\n\nfunction build_bridge_test() {\n    Invoke-Expression \"./package_scripts/win/build_bridge.ps1 -version ci_test -pyc_env py27 -mnn_path $outdir -python_path $HOME/PyBridgeDeps/python -numpy_path $HOME/PyBridgeDeps/numpy -path $outdir/py_bridge -train_api $(If ($x86) {'-x86'})\"\n    $WrongNum = $($LastExitCode -ne 0)\n    log \"WINDOWS_LIB\" \"Windows pymnn bridge编译测试\" 0 $WrongNum $(1 - $WrongNum) 0\n    if ($WrongNum -ne 0) {\n        Write-Output \"### Windows pymnn bridge编译测试失败，测试终止\"\n        failed\n    }\n}\n\nfunction unit_test() {\n    Invoke-Expression \"$outdir/tools/$arch/run_test.out.exe\"\n    if ($LastExitCode -ne 0) {\n        Write-Output \"### CPU后端 单元测试失败，测试终止\"\n        failed\n    }\n    Invoke-Expression \"$outdir/tools/$arch/run_test.out.exe op 0 0 4\"\n    if ($LastExitCode -ne 0) {\n        Write-Output \"### CPU后端 多线程测试失败，测试终止\"\n        failed\n    }\n    if ($test_avx512) {\n        $RemoteExitCode = run_remote \"cd tools/x64 ; ./run_test.out.exe > log.txt ; echo `$LastExitCode\"\n        Write-Output $(run_remote \"Get-Content -Path tools/x64/log.txt\")\n        if ($RemoteExitCode -ne 0) {\n            Write-Output \"### CPU后端(AVX512) 单元测试失败，测试终止\"\n            failed\n        }\n        $RemoteExitCode = run_remote \"cd tools/x64 ; ./run_test.out.exe op 0 0 4 > log.txt ; echo `$LastExitCode\"\n        Write-Output $(run_remote \"Get-Content -Path tools/x64/log.txt\")\n        if ($RemoteExitCode -ne 0) {\n            Write-Output \"### CPU后端(AVX512) 多线程测试失败，测试终止\"\n            failed\n        }\n    }\n    #Invoke-Expression \"$outdir/tools/$arch/run_test.out.exe op 3\"\n    #if ($LastExitCode -ne 0) {\n    #    echo \"### OpenCL后端 单元测试失败，测试终止\"\n    #    failed\n    #}\n}\n\nfunction model_test() {\n    Push-Location $outdir/tools/$arch\n    python $basedir/tools/script/modelTest.py $HOME/AliNNModel 0 0.002\n    if ($LastExitCode -ne 0) {\n        Write-Output \"### CPU后端 模型测试失败，测试终止\"\n        Pop-Location\n        failed\n    }\n    python $basedir/tools/script/modelTest.py $HOME/AliNNModel 0 0.002 0 1\n    if ($LastExitCode -ne 0) {\n        Write-Output \"### CPU后端 静态模型测试失败，测试终止\"\n        Pop-Location\n        failed\n    }\n    if ($test_avx512) {\n        $RemoteExitCode = run_remote \"cd tools/x64 ; python ../../modelTest.py `$HOME/AliNNModel 0 0.002 > log.txt ; echo `$LastExitCode\"\n        Write-Output $(run_remote \"Get-Content -Path tools/x64/log.txt\")\n        if ($RemoteExitCode -ne 0) {\n            Write-Output \"### CPU后端(AVX512) 模型测试失败，测试终止\"\n            Pop-Location\n            failed\n        }\n        $RemoteExitCode = run_remote \"cd tools/x64 ; python ../../modelTest.py `$HOME/AliNNModel 0 0.002 0 1 > log.txt ; echo `$LastExitCode\"\n        Write-Output $(run_remote \"Get-Content -Path tools/x64/log.txt\")\n        if ($RemoteExitCode -ne 0) {\n            Write-Output \"### CPU后端(AVX512) 静态模型测试失败，测试终止\"\n            Pop-Location\n            failed\n        }\n    }\n    #python $basedir/tools/script/modelTest.py $HOME/AliNNModel 3 0.01\n    #if ($LastExitCode -ne 0) {\n    #    echo \"### OpenCL后端 模型测试失败，测试终止\"\n    #    Pop-Location\n    #    failed\n    #}\n    Pop-Location\n}\n\nfunction pymnn_whl_test() {\n    $pyarch = $(If ($x86) {\"win32\"} Else {\"amd64\"})\n    Push-Location pymnn/test\n    $local = \"$(Get-Location)/aone-site-packages\"\n    $pythonpath_backup = ${env:PYTHONPATH}\n    Foreach ($pyenv in @(\"27\", \"37\", \"38\", \"39\")) {\n        Invoke-Expression \"conda activate py$pyenv$(If($x86) {'-win32'})\"\n        Remove-Item -Recurse $local -ErrorAction Ignore\n        pip install --target $local $outdir/py_whl/$(Get-ChildItem -Path $outdir/py_whl -Include \"*$pyenv*$pyarch*\" -Name)\n        do {\n            # unit_test.py need torch, which isn't support on 32bit Windows and py27\n            # https://pytorch.org/docs/stable/notes/windows.html#package-not-found-in-win-32-channel\n            if ($x86 -or ($pyenv -eq \"27\")) {\n                break;\n            }\n            ${env:PYTHONPATH} = $local\n            python unit_test.py\n            ${env:PYTHONPATH} = $pythonpath_backup\n            if ($LastExitCode -ne 0) {\n                Write-Output \"### PYMNN单元测试失败，测试终止\"\n                conda deactivate\n                Pop-Location\n                failed\n            }\n        } while(0);\n        ${env:PYTHONPATH} = \"$local\"\n        python model_test.py $HOME/AliNNModel\n        ${env:PYTHONPATH} = $pythonpath_backup\n        if ($LastExitCode -ne 0) {\n            Write-Output \"### PYMNN模型测试失败，测试终止\"\n            conda deactivate\n            Pop-Location\n            failed\n        }\n        conda deactivate\n    }\n    Pop-Location\n}\n\nbuild_lib_test\n# TODO: open other test\n# build_tool_test\n# build_whl_test\n# build_bridge_test\n\n# if ($test_avx512) {\n#     sync_remote\n# }\n# unit_test\n# model_test\n# pymnn_whl_test"
        },
        {
          "name": "test.sh",
          "type": "blob",
          "size": 26.6416015625,
          "content": "# test script for MNN-Release\n#\n# 0. arg = local: [ test for your local build ]\n#       1. unit-test;\n#       2. model-test;\n#       3. onnx convert test\n#       4. tf convert test\n#       5. tflite convert test\n#       6. torch convert test\n#       7. ptq test\n#       8. pymnn test\n#\n# 1. arg = linux: [ all test on linux with coverage ]\n#       0. static check (if source change)\n#       1. pyc check (if *.py change)\n#       2. build for linux;\n#       3. unit-test;\n#       4. model-test;\n#       5. onnx convert test\n#       6. tf convert test\n#       7. tflite convert test\n#       8. torch convert test\n#       9. ptq test\n#      10. pymnn test (if pymnn change)\n#      11. opencv test (if opencv change)\n#      12. convert-report;\n#\n# 2. arg = android: [ simple test on android ]\n#       1. build Android with static_stl\n#       2. build Android arm64\n#       3. unit-test for Android arm64\n#       4. build Android arm32\n#       5. unit-test for Android arm32\n\n# 0. build for android\nUSER_NAME=`whoami`\nUSER_HOME=\"$(echo -n $(bash -c \"cd ~${USER_NAME} && pwd\"))\"\n\n# detect change\nSOURCE_CHANGE=$(git show --name-only | grep -E \"^source/(internal|backend|core|common|cv|geometry|math|plugin|shape|utils)/.*\\.(cpp|cc|c|hpp)$\" | \\\n                grep -Ev \"aliyun-log-c-sdk|hiai|tensorrt|Backend|FunctionDispatcher|ThreadPool\")\nPYMNN_CHANGE=$(git show --name-only | grep -E \"^pymnn/.*\\.(cpp|cc|c|h|hpp|py)$\")\nPY_CHANGE=$(git show --name-only | grep -E \"^pymnn/pip_package/MNN/.*\\.(py)$\")\nOPENCV_CHANGE=$(git show --name-only | grep -E \"^tools/cv/.*\\.(cpp|cc|c|h|hpp)$\")\n# OPENCL_CHANGE=$(git show --name-only | grep -E \"^source/backend/opencl/.*\\.(cpp|cc|c|h|hpp)$\")\nOPENCL_CHANGE=true\nfailed() {\n    printf \"TEST_NAME_EXCEPTION: Exception\\nTEST_CASE_AMOUNT_EXCEPTION: {\\\"blocked\\\":0,\\\"failed\\\":1,\\\"passed\\\":0,\\\"skipped\\\":0}\\n\"\n    exit 1\n}\n\n#############################################################################################\n#                                                                                           #\n#                                  Linux Test Functions                                     #\n#                                                                                           #\n#############################################################################################\ndoc_check() {\n    echo 'doc_check'\n    # 1. CHECK CMakeLists.txt:\n    cmake_files=$(find tools source demo test benchmark  -name \"CMakeLists.txt\")\n    cmake_files=\"$cmake_files CMakeLists.txt\"\n    macros=''\n    executables=''\n    for cmake_file in $cmake_files\n    do\n        executables=\"$executables $(cat $cmake_file | grep -oE \"add_executable\\((.+) \" | awk '{print $1}' | awk -F \"(\" '{print $2}')\"\n        macros=\"$macros $(cat $cmake_file | grep -oE \"option\\((.+) \" | awk '{print $1}' | awk -F \"(\" '{print $2}')\"\n    done\n    # 1.1 check all macro\n    for macro in $macros\n    do\n        if [ $(grep -c $macro ./docs/compile/cmake.md) -le 0 ]; then\n            echo 'DOC CHECK FAILED:' $macro 'not in ./docs/compile/cmake.md'\n            failed\n        fi\n    done\n    # 1.2 check executable\n    for executable in $executables\n    do\n        if [ $(grep -c $executable ./docs/compile/other.md) -le 0 ]; then\n            echo 'DOC CHECK FAILED:' $executable 'not in ./docs/compile/other.md'\n            failed\n        fi\n    done\n    # 2. CHECK Pymnn API:\n    # 2.1 check cv api\n    cv_apis=$(cat pymnn/src/cv.h | grep -oE \"        .+, \\\".+\\\"\" | awk '{ print $1 }' | awk -F ',' '{ print $1 }')\n    cv_apis=\"$cv_apis $(cat pymnn/pip_package/MNN/cv/__init__.py | grep -oE \"def .+\\(\" | awk '{ print $2 }' | awk -F '(' '{print $1}' | grep -v \"__\")\"\n    for cv_api in $cv_apis\n    do\n        if [ $(grep -c $cv_api ./docs/pymnn/cv.md) -le 0 ]; then\n            echo 'DOC CHECK FAILED:' $cv_api 'not in ./docs/pymnn/cv.md'\n            failed\n        fi\n    done\n    # 2.2 check numpy api\n    # np_apis=$(cat pymnn/pip_package/MNN/numpy/__init__.py | grep -oE \"def .+\\(\" | grep -v \"__\" | awk '{ print $2 }' | awk -F '(' '{print $1}')\n    # for np_api in $np_apis\n    # do\n    #     if [ $(grep -c $np_api ./docs/pymnn/numpy.md) -le 0 ]; then\n    #         echo 'DOC CHECK FAILED:' $np_api 'not in ./docs/pymnn/numpy.md'\n    #         # failed\n    #     fi\n    # done\n    # 2.3 check expr api\n    expr_apis=$(cat pymnn/src/expr.h | grep -oE \"        [a-z_]+, \\\"\" | awk '{ print $1 }' | awk -F ',' '{ print $1 }')\n    for expr_api in $expr_apis\n    do\n        if [ $(grep -c $expr_api ./docs/pymnn/expr.md) -le 0 ]; then\n            echo 'DOC CHECK FAILED:' $expr_api 'not in ./docs/pymnn/expr.md'\n            # failed\n        fi\n    done\n    # 3. CHECK C++ API:\n    # 3.1 check Interpreter\n    # 3.2 check Tensor\n}\n\npy_check() {\n    echo 'py_check'\n    if [ -z \"$PY_CHANGE\" ]; then\n        return\n    fi\n    pushd pymnn\n    ./update_mnn_wrapper_assets.sh -c\n    pyc_check_wrong=$[$? > 0]\n    printf \"TEST_NAME_PYC_CHECK: pyc资源文件校验\\nTEST_CASE_AMOUNT_PYC_CHECK: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" \\\n           $pyc_check_wrong $[1 - $pyc_check_wrong]\n    if [ $pyc_check_wrong -ne 0 ]; then\n        echo '### pyc资源文件校验失败，测试终止！'\n        failed\n    fi\n    popd\n}\n\nstatic_check() {\n    echo 'static_check'\n    if [ -z \"$SOURCE_CHANGE\" ]; then\n        return\n    fi\n    cppcheck --error-exitcode=1 --language=c++ --std=c++14 --addon=tools/script/mnn_rules.py $SOURCE_CHANGE 1> /dev/null\n    static_check_wrong=$[$? > 0]\n    printf \"TEST_NAME_STATIC_CHECK: cppcheck静态分析\\nTEST_CASE_AMOUNT_STATIC_CHECK: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" \\\n           $static_check_wrong $[1 - $static_check_wrong]\n    if [ $static_check_wrong -ne 0 ]; then\n        echo '### cppcheck静态分析失败，测试终止！'\n        failed\n    fi\n}\n\nandroid_static_build() {\n    BASH_FILE=\"$USER_HOME/.zshrc\"\n    if [ -f \"$BASH_FILE\" ]; then\n        source $BASH_FILE\n    fi\n    if [ ! $ANDROID_NDK ] || [ ! -d $ANDROID_NDK ]; then\n        export ANDROID_NDK=\"$USER_HOME/android-ndk-r21\"\n    fi\n    mkdir android_build\n    pushd android_build\n    cmake .. \\\n    -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \\\n    -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DANDROID_ABI=\"arm64-v8a\" \\\n    -DANDROID_STL=c++_static \\\n    -DMNN_INTERNAL=ON \\\n    -DMNN_USE_LOGCAT=false \\\n    -DMNN_BUILD_BENCHMARK=ON \\\n    -DANDROID_NATIVE_API_LEVEL=android-26  \\\n    -DMNN_BUILD_FOR_ANDROID_COMMAND=true \\\n    -DMNN_OPENGL=true \\\n    -DMNN_BUILD_TRAIN=true \\\n    -DMNN_VULKAN=true \\\n    -DMNN_OPENCL=true \\\n    -DMNN_SUPPORT_BF16=true \\\n    -DMNN_OPENCL=true -DMNN_ARM82=true \\\n    -DMNN_SUPPORT_TRANSFORMER_FUSE=ON \\\n    -DNATIVE_LIBRARY_OUTPUT=. -DNATIVE_INCLUDE_OUTPUT=. $1 $2 $3\n    make -j16\n    android_build_wrong=$[$? > 0]\n    printf \"TEST_NAME_ANDROID_STATIC: AndroidStatic编译测试\\nTEST_CASE_AMOUNT_ANDROID_STATIC: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" \\\n           $android_build_wrong $[1 - $android_build_wrong]\n    if [ $android_build_wrong -ne 0 ]; then\n        echo '### AndroidStatic编译失败，测试终止！'\n        failed\n    fi\n    popd\n\n    mkdir android_build_32\n    pushd android_build_32\n    cmake .. \\\n    -DCMAKE_TOOLCHAIN_FILE=$ANDROID_NDK/build/cmake/android.toolchain.cmake \\\n    -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\\n    -DCMAKE_BUILD_TYPE=Release \\\n    -DANDROID_ABI=\"armeabi-v7a\" \\\n    -DANDROID_STL=c++_shared \\\n    -DMNN_USE_LOGCAT=false \\\n    -DMNN_BUILD_BENCHMARK=ON \\\n    -DMNN_INTERNAL=ON \\\n    -DANDROID_NATIVE_API_LEVEL=android-26  \\\n    -DMNN_BUILD_FOR_ANDROID_COMMAND=true \\\n    -DMNN_OPENGL=true \\\n    -DMNN_BUILD_TRAIN=true \\\n    -DMNN_VULKAN=true \\\n    -DMNN_OPENCL=true \\\n    -DMNN_BUILD_MINI=true \\\n    -DMNN_SUPPORT_BF16=true \\\n    -DMNN_ARM82=false \\\n    -DMNN_OPENCL=true \\\n    -DMNN_SUPPORT_TRANSFORMER_FUSE=ON \\\n    -DNATIVE_LIBRARY_OUTPUT=. -DNATIVE_INCLUDE_OUTPUT=.\n    make -j16\n    android_build_wrong=$[$? > 0]\n    printf \"TEST_NAME_ANDROID_32: Android 32-Mini 编译测试\\nTEST_CASE_AMOUNT_ANDROID_32: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" $android_build_wrong $[1 - $android_build_wrong]\n    if [ $android_build_wrong -ne 0 ]; then\n        echo '### Android编译失败，测试终止！'\n        failed\n    fi\n    popd\n}\n\nlinux_build() {\n    if [ $# -gt 0 ]; then\n        COVERAGE=ON\n    else\n        COVERAGE=OFF\n    fi\n\n    mkdir build_non_sse\n    pushd build_non_sse\n    cmake .. -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DMNN_USE_SSE=OFF && make -j16\n\n    linux_build_wrong=$[$? > 0]\n    popd\n\n    mkdir build\n    pushd build\n    # copy libtorch avoid wget, speed up ci build\n    cp ~/libtorch-cxx11-abi-shared-with-deps-1.9.0+cpu.zip .\n    cmake .. \\\n        -DCMAKE_CXX_COMPILER_LAUNCHER=ccache \\\n        -DCMAKE_BUILD_TYPE=Release \\\n        -DMNN_BUILD_TEST=ON \\\n        -DMNN_CUDA=ON \\\n        -DMNN_OPENCL=ON \\\n        -DMNN_BUILD_QUANTOOLS=ON \\\n        -DMNN_BUILD_DEMO=ON \\\n        -DMNN_BUILD_TRAIN=ON \\\n        -DMNN_BUILD_CONVERTER=ON \\\n        -DMNN_BUILD_TORCH=ON \\\n        -DMNN_BUILD_OPENCV=ON \\\n        -DMNN_LOW_MEMORY=ON \\\n        -DMNN_IMGCODECS=ON \\\n        -DMNN_SUPPORT_TRANSFORMER_FUSE=ON \\\n        -DMNN_ENABLE_COVERAGE=$COVERAGE\n    make -j16\n\n    linux_build_wrong+=$[$? > 0]\n    printf \"TEST_NAME_LINUX: Linux编译测试\\nTEST_CASE_AMOUNT_LINUX: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" $linux_build_wrong $[2 - $linux_build_wrong]\n    if [ $linux_build_wrong -ne 0 ]; then\n        echo '### Linux编译失败，测试终止！'\n        failed\n    fi\n\n    # Don't remove this! It turn off MNN_CUDA and MNN_TENSORRT in build, workaround some bug in PTQTest\n    cmake .. -DMNN_CUDA=OFF -DMNN_TENSORRT=OFF && make -j16\n}\n\nunit_test() {\n    ./run_test.out\n    if [ $? -ne 0 ]; then\n        echo '### 单元测试失败，测试终止！'\n        failed\n    fi\n\n    ./run_test.out op 0 0 4\n    if [ $? -ne 0 ]; then\n        echo '### 多线程单元测试失败，测试终止！'\n        failed\n    fi\n    if [ \"$OPENCL_CHANGE\" ]; then\n        ./run_test.out op 3 1 4\n        if [ $? -ne 0 ]; then\n            echo '### OpenCL单元测试失败，测试终止！'\n            failed\n        fi\n    fi\n}\n\nmodel_test() {\n    ../tools/script/modelTest.py ~/AliNNModel 0 0.002\n    if [ $? -ne 0 ]; then\n        echo '### 模型测试失败，测试终止！'\n        failed\n    fi\n\n    ../tools/script/modelTest.py ~/AliNNModel 0 0.002 0 1\n    if [ $? -ne 0 ]; then\n        echo '### 静态模型测试失败，测试终止！'\n        failed\n    fi\n\n    if [ \"$OPENCL_CHANGE\" ]; then\n        ../tools/script/modelTest.py ~/AliNNModel 3 0.002 1\n        if [ $? -ne 0 ]; then\n            echo '### OpenCL模型测试失败，测试终止！'\n            failed\n        fi\n    fi\n}\n\nonnx_convert_test() {\n    ../tools/script/convertOnnxTest.py ~/AliNNModel\n    if [ $? -ne 0 ]; then\n        echo '### ONNXConvert测试失败，测试终止！'\n        failed\n    fi\n}\n\ntf_convert_test() {\n    ../tools/script/convertTfTest.py ~/AliNNModel\n    if [ $? -ne 0 ]; then\n        echo '### TFConvert测试失败，测试终止！'\n        failed\n    fi\n}\n\ntflite_convert_test() {\n    ../tools/script/convertTfliteTest.py ~/AliNNModel\n    if [ $? -ne 0 ]; then\n        echo '### TFLITEConvert测试失败，测试终止！'\n        failed\n    fi\n}\n\ntorch_convert_test() {\n    ../tools/script/convertTorchTest.py ~/AliNNModel\n    if [ $? -ne 0 ]; then\n        echo '### TORCHConvert测试失败，测试终止！'\n        failed\n    fi\n}\n\nptq_test() {\n    ../tools/script/testPTQ.py ~/AliNNModel\n    if [ $? -ne 0 ]; then\n        echo '### PTQ测试失败，测试终止！'\n        failed\n    fi\n}\n\npymnn_test() {\n    if [ -z \"$PYMNN_CHANGE\" ]; then\n        return\n    fi\n    popd\n    pushd pymnn\n    # 1. build pymnn\n    pushd pip_package\n    python3 build_deps.py\n    # uninstall original MNN\n    pip uninstall --yes MNN MNN-Internal\n    python3 setup.py install --version 1.0 --install-lib=/usr/lib/python3/dist-packages\n    pymnn_build_wrong=$[$? > 0]\n    printf \"TEST_NAME_PYMNN_BUILD: PYMNN编译测试\\nTEST_CASE_AMOUNT_PYMNN_BUILD: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" \\\n            $pymnn_build_wrong $[1 - $pymnn_build_wrong]\n    if [ $pymnn_build_wrong -ne 0 ]; then\n        echo '### PYMNN编译失败，测试终止！'\n        failed\n    fi\n    popd\n    # 2. unit test\n    pushd test\n    python3 unit_test.py\n    if [ $? -ne 0 ]; then\n        echo '### PYMNN单元测试失败，测试终止！'\n        failed\n    fi\n    # 3. model test\n    python3 model_test.py ~/AliNNModel\n    if [ $? -ne 0 ]; then\n        echo '### PYMNN模型测试失败，测试终止！'\n        failed\n    fi\n    # 4. train test\n    ./train_test.sh\n    # 5. uninstall pymnn\n    pip uninstall --yes MNN-Internal\n    popd\n    popd\n    pushd build\n}\n\nopencv_test() {\n    if [ -z \"$OPENCV_CHANGE\" ]; then\n        return\n    fi\n    # 1. build opencv-test\n    cmake -DMNN_OPENCV_TEST=ON ..\n    make -j8\n    opencv_build_wrong=$[$? > 0]\n    printf \"TEST_NAME_OPENCV_BUILD: OPENCV编译测试\\nTEST_CASE_AMOUNT_OPENCV_BUILD: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" \\\n            $opencv_build_wrong $[1 - $opencv_build_wrong]\n    if [ $opencv_build_wrong -ne 0 ]; then\n        echo '### OPENCV编译失败，测试终止！'\n        failed\n    fi\n    # 2. run opencv unit test\n    ./opencv_test\n    if [ $? -gt 0 ]; then\n        echo '### OPENCV单元测试失败，测试终止！'\n        failed\n    fi\n}\n\nllm_test() {\n    # 1. build llm with low memory\n    cmake -DMNN_LOW_MEMORY=ON -DMNN_BUILD_LLM=ON -DMNN_SUPPORT_TRANSFORMER_FUSE=ON ..\n    make -j8\n    llm_build_wrong=$[$? > 0]\n    printf \"TEST_NAME_LLM_BUILD: LLM编译测试\\nTEST_CASE_AMOUNT_LLM_BUILD: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" \\\n            $llm_build_wrong $[1 - $llm_build_wrong]\n    if [ $llm_build_wrong -ne 0 ]; then\n        echo '### LLM编译失败，测试终止！'\n        failed\n    fi\n    # 2. run llm model test\n    ./llm_demo ~/AliNNModel/qwen1.5-0.5b-int4/config.json ~/AliNNModel/qwen1.5-0.5b-int4/prompt.txt\n    if [ $? -gt 0 ]; then\n        echo '### LLM模型测试失败，测试终止！'\n        failed\n    fi\n}\n\ncoverage_init() {\n    popd\n    lcov -c -i -d ./ -o init.info\n    pushd build\n}\n\ncoverage_report() {\n    popd\n    cover_report_dir=\"../../../../CoverageReport\"\n    lcov -c -d ./ -o cover.info\n    lcov -a init.info -a cover.info -o total.info\n    lcov --remove total.info \\\n    '*/usr/include/*' '*/usr/lib/*' '*/usr/lib64/*' '*/usr/local/*'  \\\n    '*/3rd_party/*' '*/build/*' '*/schema/*' '*/test/*' '/tmp/*' \\\n    '*/demo/*' '*/tools/cpp/*' '*/tools/train/*' '*/source/backend/cuda/*' \\\n    -o final.info\n    commitId=$(git log | head -n1 | awk '{print $2}')\n    genhtml -o cover_report --legend --title \"MNN Coverage Report [commit SHA1:${commitId}]\" --prefix=`pwd` final.info\n    coverage_wrong=$[$? > 0]\n    printf \"TEST_NAME_COVERAGE: 代码覆盖率(点击\\\"通过\\\"查看报告)\\nTEST_CASE_AMOUNT_COVERAGE: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" $coverage_wrong $[1 - $coverage_wrong]\n    if [ $coverage_wrong -ne 0 ]; then\n        echo '### 代码覆盖率生成失败，测试终止！'\n        failed\n    else\n        hostIp=$(cat .aoneci.yml | grep host -m 1 | awk '{print $2}')\n        testId=$(pwd | awk -F \"/\" '{print $(NF-1)}')\n        mv cover_report $cover_report_dir/$testId\n        echo \"TEST_REPORT_COVERAGE: http://$hostIp/$testId\"\n    fi\n    # clean test dir\n    cd ../.. && rm -rf $testId\n}\n\n#############################################################################################\n#                                                                                           #\n#                                  Android Test Functions                                   #\n#                                                                                           #\n#############################################################################################\nandroid_unit_test() {\n    memory_mode=$2\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out all 0 0 1 $1 $memory_mode\"\n    if [ $? -ne 0 ]; then\n        echo '### Android单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op 0 0 4 multi$1 $memory_mode\"\n    if [ $? -ne 0 ]; then\n        echo '### Android单元测试多线程失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/convolution 0 2 4 fp16multi$1 $memory_mode\"\n    if [ $? -ne 0 ]; then\n        echo '### Android单元测试卷积FP16多线程失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/col2im 0 2 4 fp16col2im$1 $memory_mode\"\n    if [ $? -ne 0 ]; then\n        echo '### Android单元测试FP16-col2im多线程失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/R 0 2 4 fp16roipooling$1 $memory_mode\"\n    if [ $? -ne 0 ]; then\n        echo '### Android单元测试FP16-roipooling多线程失败，测试终止！'\n        failed\n    fi\n    if [ \"$OPENCL_CHANGE\" ]; then\n        adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op 3 1 4 $1 $memory_mode\"\n        if [ $? -ne 0 ]; then\n            echo '### Android单元测试OpenCL失败，测试终止！'\n            failed\n        fi\n    fi\n}\nandroid_model_test() {\n    fail_num=0\n    pass_num=0\n    fail_cl_num=0\n    pass_cl_num=0\n    models=`adb shell ls /data/local/tmp/AliNNModel/OpTestResource/`\n    for model in $models\n    do\n        adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./testModel.out ../AliNNModel/OpTestResource/$model/temp.bin ../AliNNModel/OpTestResource/$model/input_0.txt ../AliNNModel/OpTestResource/$model/output_0.txt 0 0.002\"\n        if [ $? -ne 0 ]; then\n            fail_num=$[$fail_num+1]\n        else\n            pass_num=$[$pass_num+1]\n        fi\n        if [ \"$OPENCL_CHANGE\" ]; then\n            adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./testModel.out ../AliNNModel/OpTestResource/$model/temp.bin ../AliNNModel/OpTestResource/$model/input_0.txt ../AliNNModel/OpTestResource/$model/output_0.txt 3 0.002 1\"\n            if [ $? -ne 0 ]; then\n                fail_cl_num=$[$fail_cl_num+1]\n            else\n                pass_cl_num=$[$pass_cl_num+1]\n            fi\n        fi\n    done\n\n    models=`adb shell ls /data/local/tmp/AliNNModel/TestResource/`\n    for model in $models\n    do\n        if [ $model == 'mobilenetv1quan' ]; then\n            adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./testModel.out ../AliNNModel/TestResource/$model/temp.bin ../AliNNModel/TestResource/$model/input_0.txt ../AliNNModel/TestResource/$model/output.txt 0 0.1\"\n        else\n            adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./testModel.out ../AliNNModel/TestResource/$model/temp.bin ../AliNNModel/TestResource/$model/input_0.txt ../AliNNModel/TestResource/$model/output.txt 0 0.002\"\n        fi\n        if [ $? -ne 0 ]; then\n            fail_num=$[$fail_num+1]\n        else\n            pass_num=$[$pass_num+1]\n        fi\n        if [ \"$OPENCL_CHANGE\" ]; then\n        if [ $model == 'mobilenetv1quan' ]; then\n            adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./testModel.out ../AliNNModel/TestResource/$model/temp.bin ../AliNNModel/TestResource/$model/input_0.txt ../AliNNModel/TestResource/$model/output.txt 3 0.1 1\"\n        else\n            adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./testModel.out ../AliNNModel/TestResource/$model/temp.bin ../AliNNModel/TestResource/$model/input_0.txt ../AliNNModel/TestResource/$model/output.txt 3 0.002 1\"\n        fi\n            if [ $? -ne 0 ]; then\n                fail_cl_num=$[$fail_cl_num+1]\n            else\n                pass_cl_num=$[$pass_cl_num+1]\n            fi\n        fi\n    done\n\n    models=`adb shell ls /data/local/tmp/AliNNModel/TestWithDescribe/`\n    for model in $models\n    do\n        adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./testModelWithDescribe.out ../AliNNModel/TestWithDescribe/$model/temp.bin ../AliNNModel/TestWithDescribe/$model/config.txt 0 0.002\"\n        if [ $? -ne 0 ]; then\n            fail_num=$[$fail_num+1]\n        else\n            pass_num=$[$pass_num+1]\n        fi\n        if [ \"$OPENCL_CHANGE\" ]; then\n            adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./testModelWithDescribe.out ../AliNNModel/TestWithDescribe/$model/temp.bin ../AliNNModel/TestWithDescribe/$model/config.txt 3 0.002 1\"\n            if [ $? -ne 0 ]; then\n                fail_cl_num=$[$fail_cl_num+1]\n            else\n                pass_cl_num=$[$pass_cl_num+1]\n            fi\n        fi\n    done\n    printf \"TEST_NAME_ANDROID_MODEL_TEST_$1: Android_$1模型测试\\nTEST_CASE_AMOUNT_ANDROID_MODEL_TEST_$1: {\\\"blocked\\\":0,\\\"failed\\\":$fail_num,\\\"passed\\\":$pass_num,\\\"skipped\\\":0}\\n\"\n    if [ $fail_num -ne 0 ]; then\n        echo '### Android模型测试失败，测试终止！'\n        failed\n    fi\n    if [ \"$OPENCL_CHANGE\" ]; then\n        printf \"TEST_NAME_ANDROID_MODEL_OPENCL_TEST_$1: Android_$1模型测试\\nTEST_CASE_AMOUNT_ANDROID_MODEL_TEST_$1: {\\\"blocked\\\":0,\\\"failed\\\":$fail_cl_num,\\\"passed\\\":$pass_cl_num,\\\"skipped\\\":0}\\n\"\n        if [ $fail_cl_num -ne 0 ]; then\n            echo '### Android OpenCL后端模型测试失败，测试终止！'\n            failed\n        fi\n    fi\n}\nandroid_unit_test_low_memory_armv8() {\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 1 1 $1 2\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 64位Low Memory,动态量化, precision=1, thread=1 单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 2 1 $1 2\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 64位Low Memory,动态量化, precision=2, thread=1 单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 1 4 $1 2\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 64位Low Memory,动态量化, precision=1, thread=4 单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 2 4 $1 2\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 64位Low Memory,动态量化, precision=2, thread=4 单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 1 1 $1\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 64位Low Memory 权重反量化, precision=1 单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 2 1 $1\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 64位Low Memory 权重反量化, precision=2 单元测试失败，测试终止！'\n        failed\n    fi\n}\n\nandroid_unit_test_low_memory_armv7() {\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 1 1 $1 2\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 32位Low Memory,动态量化, precision=1, thread=1 单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 2 1 $1 2\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 32位Low Memory,动态量化, precision=2, thread=1 单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 1 4 $1 2\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 32位Low Memory,动态量化, precision=1, thread=4 单元测试失败，测试终止！'\n        failed\n    fi\n    adb shell \"cd /data/local/tmp/MNN&&export LD_LIBRARY_PATH=.&&./run_test.out op/lowMemory 0 2 4 $1 2\"\n    if [ $? -ne 0 ]; then\n        echo '### Android 32位Low Memory,动态量化, precision=2, thread=4 单元测试失败，测试终止！'\n        failed\n    fi\n}\n\nandroid_test() {\n    pushd project/android\n    # 1. build Android32\n    mkdir build_32\n    pushd build_32\n    ../build_32.sh -DMNN_BUILD_TRAIN=OFF -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DMNN_OPENCL=true -DMNN_LOW_MEMORY=ON -DMNN_SUPPORT_TRANSFORMER_FUSE=ON -DMNN_ARM82=OFF\n    android32_build_wrong=$[$? > 0]\n    mnn32_size=$(ls -lh libMNN.so | awk '{print $5}')\n    expr32_size=$(ls -lh libMNN_Express.so | awk '{print $5}')\n    printf \"TEST_NAME_ANDROID_32: Android32编译测试(libMNN.so - %s, libMNN_Express.so - %s)\\nTEST_CASE_AMOUNT_ANDROID_32: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" \\\n           $mnn32_size $expr32_size $android32_build_wrong $[1 - $android32_build_wrong]\n    if [ $android32_build_wrong -ne 0 ]; then\n        echo '### Android32编译失败，测试终止！'\n        failed\n    fi\n    ../updateTest.sh\n    android_unit_test 32bit 1\n    android_unit_test_low_memory_armv7 32bit\n    android_model_test 32\n    popd\n\n    # 3. build Android64\n    mkdir build_64\n    pushd build_64\n    ../build_64.sh -DMNN_BUILD_TRAIN=OFF -DCMAKE_CXX_COMPILER_LAUNCHER=ccache -DMNN_ARM82=true -DMNN_OPENCL=true -DMNN_LOW_MEMORY=true -DMNN_SUPPORT_TRANSFORMER_FUSE=ON\n    android64_build_wrong=$[$? > 0]\n    mnn64_size=$(ls -lh libMNN.so | awk '{print $5}')\n    expr64_size=$(ls -lh libMNN_Express.so | awk '{print $5}')\n    printf \"TEST_NAME_ANDROID_64: Android64编译测试(libMNN.so - %s, libMNN_Express.so - %s)\\nTEST_CASE_AMOUNT_ANDROID_64: {\\\"blocked\\\":0,\\\"failed\\\":%d,\\\"passed\\\":%d,\\\"skipped\\\":0}\\n\" \\\n            $mnn64_size $expr64_size $android64_build_wrong $[1 - $android64_build_wrong]\n    if [ $android64_build_wrong -ne 0 ]; then\n        echo '### Android64编译失败，测试终止！'\n        failed\n    fi\n\n    # 4. test Android64\n    ../updateTest.sh\n    android_unit_test 64 0\n    android_unit_test_low_memory_armv8 64\n    android_model_test 64\n    popd\n\n    popd\n}\n\ncase \"$1\" in\n    local)\n        pushd build\n        unit_test\n        model_test\n        onnx_convert_test\n        tf_convert_test\n        tflite_convert_test\n        torch_convert_test\n        ptq_test\n        pymnn_test\n        ;;\n    linux)\n        doc_check\n        static_check\n        py_check\n        linux_build 1\n        coverage_init\n        unit_test\n        model_test\n        onnx_convert_test\n        tf_convert_test\n        tflite_convert_test\n        torch_convert_test\n        ptq_test\n        pymnn_test\n        opencv_test\n        llm_test\n        coverage_report\n        ;;\n    android)\n        android_static_build\n        android_test\n        ;;\n    static)\n        doc_check\n        static_check\n        py_check\n        ;;\n    *)\n        $1\n        echo $\"Usage: $0 {local|linux|android|func}\"\n        exit 2\nesac\nexit $?\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "transformers",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}