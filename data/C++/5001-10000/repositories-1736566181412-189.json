{
  "metadata": {
    "timestamp": 1736566181412,
    "page": 189,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVIDIA/cutlass",
      "stars": 5975,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.052734375,
          "content": "# PyCache files\n__pycache__/\ncutlass_library.egg-info/"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 46.134765625,
          "content": "# NVIDIA CUTLASS Changelog\n## [3.6.0](https://github.com/NVIDIA/cutlass/releases/tag/v3.6.0) (2024-10-03)\n\n- [Hopper structured sparse GEMM](./examples/62_hopper_sparse_gemm/62_hopper_sparse_gemm.cu).\n  + [FP16](./test/unit/gemm/device/sm90_sparse_gemm_f16_f16_f32_tensor_op_f32.cu)\n  + [FP8](./test/unit/gemm/device/sm90_sparse_gemm_f8_f8_f32_tensor_op_f32.cu)\n  + [INT8](./test/unit/gemm/device/sm90_sparse_gemm_s8_s8_s32_tensor_op_s32.cu)\n  + [TF32](./test/unit/gemm/device/sm90_sparse_gemm_tf32_tf32_f32_tensor_op_f32.cu)\n- A refactor to the CUTLASS 3.x convolution `kernel::ConvUniversal` [API](./include/cutlass/conv/kernel/sm90_implicit_gemm_tma_warpspecialized.hpp) to bring it in line with `gemm::GemmUniversal`. Now the 3.x convolution API is no longer considered as a beta API.\n- Improve [mixed input GEMM](./examples/55_hopper_mixed_dtype_gemm/README.md).\n  + Added a [lookup table implementation](./examples/55_hopper_mixed_dtype_gemm/55_hopper_int4_fp8_gemm.cu) for `INT4`x`FP8` scale-only mode.\n  + Added [layout pre-shuffling](./examples/55_hopper_mixed_dtype_gemm/55_hopper_int4_fp8_gemm.cu#L50-55) to optimize memory loading.\n  + Added [interleaved conversion](./examples/55_hopper_mixed_dtype_gemm/55_hopper_int4_bf16_gemm.cu#L50-52) for `{INT4, UINT4, INT8}` x `{FP16, BF16}`.\n  + Other general optimizations.\n- The suffixes of the mixed input kernel schedules have been removed. Use `KernelTmaWarpSpecialized`, `KernelTmaWarpSpecializedPingpong` and `KernelTmaWarpSpecializedCooperative` instead.\n- [EVT nodes for Top-K selection and softmax](./include/cutlass/epilogue/fusion/sm90_visitor_topk_softmax.hpp) and [GEMM example using those](./examples/61_hopper_gemm_with_topk_and_softmax/61_hopper_gemm_with_topk_and_softmax.cu).\n- [Programmatic Dependent Launch](./include/cutlass/arch/grid_dependency_control.h) (PDL) that leverages a new Hopper feature to speedup two back-to-back kernels, and its corresponding [documentations](./media/docs/dependent_kernel_launch.md).\n- [A new debugging tool, synclog](./include/cutlass/arch/synclog.hpp), for dumping out all synchronization events from within a kernel to a file. Please see [synclog documentation](./media/docs/utilities.md#debugging-asynchronous-kernels-with-cutlasss-built-in-synclog-tool) for details.\n- A new TMA-enabled [epilogue](./include/cutlass/epilogue/collective/sm90_epilogue_array_tma_warpspecialized.hpp) for grouped GEMM that brings significant performance improvement, as well as its EVT support.\n- A SIMT-enabled pointer-array [epilogue](./include/cutlass/epilogue/collective/sm70_epilogue_vectorized_array.hpp).\n- A new [Ping-Pong kernel schedule for Grouped GEMM](./include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_pingpong.hpp) and some other optimizations.\n- [A new instantiation strategy for CUTLASS profiler kernels](./python/cutlass_library/sm90_shapes.py) along with [improved documentation for instantiation level in CUTLASS profiler](./media/docs/profiler.md#instantiating-more-kernels-with-hopper).\n- A new hardware support for comparisons and computations of [`cutlass::bfloat16_t`](./include/cutlass/bfloat16.h)\n- Fixed use of isnan on Windows for [`half_t`](./test/unit/core/functional.cu).\n- Various improvements and fixes from the community and CUTLASS team. Thanks to everyone who submitted PRs!\n- Optimal code generation with CUDA toolkit versions 12.6.\n\n## [3.5.1](https://github.com/NVIDIA/cutlass/releases/tag/v3.5.1) (2024-07-25)\n\n- [Minimal SM90 WGMMA + TMA GEMM example in 100 lines of code](./examples/cute/tutorial/wgmma_sm90.cu)\n- [Exposure of L2 `cache_hint`s in TMA copy atoms](./include/cute/arch/copy_sm90_tma.hpp#L48)\n- Exposure of raster order and tile swizzle extent in [CUTLASS library profiler](./media/docs/profiler.md#GEMM), and\n[example 48](./examples/48_hopper_warp_specialized_gemm/48_hopper_warp_specialized_gemm.cu).\n- [TMA store based and EVT supported epilogues](./include/cutlass/epilogue/collective/sm90_epilogue_array_tma_warpspecialized.hpp) for [Hopper pointer array batched kernels](./test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_ptr_array.cu).\n- A new [`GemmSparseUniversal` API for CUTLASS 2.x Ampere kernels](./include/cutlass/gemm/device/gemm_sparse_universal.h) to enable serial and parallel split-k for sparse tensor cores and new tiny tile sizes to better support LLM inferrence:\n  + [FP16 TN](./test/unit/gemm/device/gemm_f16t_f16n_f32t_tensor_op_f32_sparse_sm80.cu#L269-L393) and [NT](./test/unit/gemm/device/gemm_f16n_f16t_f32t_tensor_op_f32_sparse_sm80.cu#L269-L411).\n  + [int8 TN](./test/unit/gemm/device/gemm_s8t_s8n_s32t_tensor_op_s32_sparse_sm80.cu#L264-L452).\n  + [int4 TN](./test/unit/gemm/device/gemm_s4t_s4n_s32t_tensor_op_s32_sparse_sm80.cu#L264-L452).\n  + [FP32 TN](./test/unit/gemm/device/gemm_f32t_f32n_f32t_tensor_op_f32_sparse_sm80.cu#L427-L642) and [NT](./test/unit/gemm/device/gemm_f32n_f32t_f32t_tensor_op_f32_sparse_sm80.cu#L427-L456).\n- [CUDA host adapter](./include/cutlass/cuda_host_adapter.hpp) extensions to support TMA descriptor construction driver APIs.\n- Inclusion of more [Hopper fprop, dgrad, and wgrad convolution kernels in CUTLASS library and profiler](./python/cutlass_library/generator.py).\n- Support for residual add (beta != 0) in convolution kernels.\n- A new convolution [epilogue](./examples/16_ampere_tensorop_conv2dfprop/ampere_tensorop_conv2dfprop.cu#L269) for CUTLASS 2.x to support non-packed NHWC output.\n- A refactor of [include files throughout CUTLASS core directories](./include/cutlass/gemm/collective/collective_mma_decl.hpp) to reduce circular dependencies and [tests to guard against them](./test/self_contained_includes/CMakeLists.txt).\n- [A guide for setting up VSCode to work well with CUTLASS](./media/docs/ide_setup.md) and [expanded code style guide](./media/docs/programming_guidelines.md).\n- Better support for MSVC as a host compiler.\n- Many performance optimizations, improvements, and bug fixes including fixes for FlashAttention-2.\n- Optimal code generation with CUDA toolkit versions 12.4 and 12.5u1.\n\n## [3.5.0](https://github.com/NVIDIA/cutlass/releases/tag/v3.5.0) (2024-04-09)\n\n- Implicit GEMM Convolutions targeting Hopper SM90A via WGMMA + [TMA im2col](./include/cute/atom/copy_traits_sm90_im2col.hpp)\n  + Native implementation in CUTLASS 3.x using CuTe, mirroring the [same design hierarchy as that of GEMMs](./media/docs/gemm_api_3x.md).\n  + Support for 1D, 2D, and 3D convolutions in a [rank-agnostic fashion](./include/cutlass/conv/convnd_problem_shape.hpp).\n  + Support for [Fprop](./test/unit/conv/device_3x/fprop/sm90_conv3d_fprop_implicit_gemm_s8_s8_s32_tensorop_s32.cu), [Dgrad](./test/unit/conv/device_3x/dgrad/sm90_conv2d_dgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu), and [Wgrad](./test/unit/conv/device_3x/wgrad/sm90_conv1d_wgrad_implicit_gemm_f16_f16_f32_tensorop_f16.cu) algorithms\n  + [CUTLASS profiler support](./python/cutlass_library/conv3x_emitter.py) for 2D and 3D convolutions implemented via the 3.x API.\n  + NOTE: this is a beta release. Further updates to CUTLASS will include major performance improvements, feature enablement, and possible breaking changes to the API until 3.7 release. Your feedback is welcome on the design!\n- Support for [Ada (SM89) FP8 tensor cores via the 2.x API](./examples/58_ada_fp8_gemm/ada_fp8_gemm.cu). Requires CUDA 12.4 or newer.\n- [Ampere gather/scatter convolution example](./examples/59_ampere_gather_scatter_conv/README.md) in CuTe and CUTLASS 3.x\n  + Showcasing how custom kernels can be written and optimized using CUTLASS 3.x and CuTe and the general strategy for implementing convolutions as specializations of GETTs.\n  + Implementation of a coarse grained sparse gather/scatter kernel achieving peak performance on Ampere class tensor cores.\n- 32x and 16x tile sizes are added to CUTLASS 2.x to improve the performance of narrow-tall and wide-short matrices.\n  + [Ampere FP16 TN](./test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f32_sm80.cu) and [NT](./test/unit/gemm/device/gemm_f16n_f16t_f16t_tensor_op_f32_sm80.cu#L227-L301), [Ampere INT8 TN](./test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm80.cu#L392-L1342), [Ampere INT4 TN](./test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm80.cu#L372-L934).\n  + [Turing FP16 TN](./test/unit/gemm/device/gemm_f16t_f16n_f16t_tensor_op_f32_sm75.cu#L55-L394), [Turing INT8 TN](./test/unit/gemm/device/gemm_s8t_s8n_s8t_tensor_op_s32_sm75.cu#L166-L537), [Turing INT4 TN](./test/unit/gemm/device/gemm_s4t_s4n_s4t_tensor_op_s32_sm75.cu#L310-L564).\n- Updates to CuTe documentation for [`cute::Tensor<>`](./media/docs/cute/03_tensor.md), [MMA atoms](./media/docs/cute/0t_mma_atom.md), and an overhauled [CuTe GEMM tutorial series](./examples/cute/tutorial).\n- Extensions to CuTe to support [L2 prefetching](./include/cute/algorithm/prefetch.hpp) and [TMA store+reductions](./include/cute/arch/copy_sm90_tma.hpp#L1337).\n- Remove C++11 requirement on a few CUTLASS 2.x API header files. All CUTLASS files now require C++17.\n- Fixes to greatly reduce build warnings.\n- Updates and bugfixes from the community (thanks!)\n\n## [3.4.1](https://github.com/NVIDIA/cutlass/releases/tag/v3.4.1) (2024-02-14)\n\n- Statically available [CUTLASS Version macros](./include/cutlass/version.h) that allow for handling API changes between CUTLASS releases on the users' side.\n- Improvements for Hopper [Group-GEMMs](./examples/57_hopper_grouped_gemm) and [Pointer-Array Batched GEMMs](./examples/56_hopper_ptr_array_batched_gemm).\n- Updates and bugfixes from the community (thanks!).\n\n## [3.4.0](https://github.com/NVIDIA/cutlass/releases/tag/v3.4.0) (2024-01-12)\n* Expanded [Mixed-input Hopper GEMMs](./examples/55_hopper_mixed_dtype_gemm) support covering {16-bit, 8-bit} x {8-bit, 4-bit} input types with fast numerical converters and group scaling factors.\n* Performance improvements to [Mixed-input Hopper GEMMs](./examples/55_hopper_mixed_dtype_gemm)\n* Beta release of [Pointer-Array Batched GEMMs](./examples/56_hopper_ptr_array_batched_gemm) now available on Hopper GPUs utilizing TMA and WGMMA (requires CUDA 12.3 or above).\n* Beta release of [Group-GEMM](./examples/57_hopper_grouped_gemm) utilizing TMA and WGMMA (requires CUDA 12.3 or above).\n* [Ampere Sparse GEMM](./examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm_with_visitor.cu) supports Epilogue Visitor Tree (EVT) now.\n* NamedBarriers usability improvement and list of [ReservedNamedBarriers](./include/cutlass/arch/barrier.h) has been officially released.\n* Improved [CuTe documentation](./media/docs/cute/) including improved clarity and depth of [Quickstart](./media/docs/cute/00_quickstart.md), [CuTe Layout](./media/docs/cute/01_layout.md), and [CuTe Layout Algebra](./media/docs/cute/02_layout_algebra.md). Associated code comments, post-conditions, and details in [CuTe Core Unit Tests](./test/unit/cute/core/) also improved.\n\n## [3.3](https://github.com/NVIDIA/cutlass/releases/tag/v3.3.0) (2023-10-31)\n* [Mixed-input Hopper GEMMs](./examples/55_hopper_mixed_dtype_gemm) support covering 16-bit x 8-bit input operand types.\n* [Mixed-input Ampere GEMMs](https://github.com/NVIDIA/cutlass/pull/1084) with support for canonical layouts (TN). The implementation supports upcast on operandB {fp16, bf16} x {s8, u8}, and upcast on operandA {s8, u8} x {fp16, bf16}.\n* [Copy Async based Hopper GEMMs](./test/unit/gemm/device/sm90_gemm_bf16_bf16_bf16_alignx_tensor_op_f32_warpspecialized_cooperative.cu) - which support lower than 16B aligned input tensors.\n* Kernel schedules and Builder support for mixed precision and Copy Async GEMMs with < 16B aligned input tensors.\n* Profiler support for lower-aligned Hopper GEMMs.\n* Performance Improvements to [Scatter-Gather Hopper Example](./examples/52_hopper_gather_scatter_fusion).\n* Sub-Byte type fixes and improvements.\n* EVT Support for RELU with Aux bitmap tensor store (used in dRELU). See [SM90 EVT fusions](./include/cutlass/epilogue/fusion/sm90_visitor_compute_tma_warpspecialized.hpp) for details.\n* Fusion support for backprop fusions including drelu, dgelu, and dbias.\n* Support for void-C kernels and SM80 mixed-input GEMMs in the CUTLASS Python interface\n\n## [3.2.2](https://github.com/NVIDIA/cutlass/releases/tag/v3.2.2) (2023-10-25)\n* Minor patch for issue/1138\n\n## [3.2.1](https://github.com/NVIDIA/cutlass/releases/tag/v3.2.1) (2023-09-22)\n* Python support SM90 Epilogue Visitor Tree (EVT) on top of the C++ support released in 3.2.0.\n* SM80 EVT support in C++ and Python.\n* Other SM90 epilogue improvements.\n* Splitting CUTLASS library into smaller units based on operation, arch and datatypes. See [1105](https://github.com/NVIDIA/cutlass/discussions/1105) for details.\n* Making `tools/library/scripts` packageable - `tools/library/scripts` is now moving to `python/cutlass_library`. See the Python [README](./python/README.md) for details.\n* SM90 TF32 kernel improvements for all layouts.\n* SM90 rasterization direction support in the CUTLASS profiler.\n* Improvement for CUTLASS profiler build times.\n* Remove Python-C++ bindings.\n\n## [3.2.0](https://github.com/NVIDIA/cutlass/releases/tag/v3.2.0) (2023-08-03)\n\n* New warp-specialized persistent FP8 GEMM kernel [kernel schedules](./include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp) and [mainloops](./include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized_fp8.hpp)  targeting Hopper architecture that achieve great performance with TMA, WGMMA, and threadblock clusters. An example showcasing [Hopper warp-specialized FP8 GEMMs](./examples/54_hopper_fp8_warp_specialized_gemm). FP8 GEMMs come with a fast accumulation mode. When enabled, problem execution might be faster but at the cost of lower accuracy because intermediate results will not periodically be promoted to a higher precision.\n* New [Epilogue Visitor Tree (EVT)](./examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu) support for Hopper TMA epilogues. EVTs allows for user-defined customized epilogue fusion patterns without having to write a new epilogue.\n* [Stream-K](./include/cutlass/gemm/kernel/sm90_tile_scheduler_stream_k.hpp) feature for Hopper. Note that this is only a functional implementation of stream-K, and should not be used for performance comparison. Optimizations are expected in a future release.\n* Improved CTA rasterization and support for CTA swizzling for Hopper kernels using the [Tile Scheduler](./include/cutlass/gemm/kernel/sm90_tile_scheduler.hpp).\n* Improved performance for [warp-specialized TensorFloat-32 (TF32) GEMM kernels](test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32_gmma_rs_cluster_warpspecialized.cu) targeting Hopper TMA.\n* [Hopper GEMM+Permute](./examples/53_hopper_gemm_permute/53_hopper_gemm_permute.cu), an example of fusing tensor reordering (permutation) with GEMM mainloop or epilogue.\n* New CUTLASS 2D Convolution Python interface. New [example](./examples/python/03_basic_conv2d.ipynb) here.\n* Support for Windows (MSVC) builds. Tested with Visual Studio 2019 v16.11.27 on Windows 10.0.\n* Optimal performance using [**CUDA 12.2u1**](https://developer.nvidia.com/cuda-downloads)\n* Updates and bugfixes from the community (thanks!)\n\n## [3.1.0](https://github.com/NVIDIA/cutlass/releases/tag/v3.1.0) (2023-04-14)\n* New CUTLASS Python interface that aims to provide an ease-of-use interface for instantiating, emitting, compiling, and running CUTLASS kernels via Python. More details [here](./python/README.md) and new [examples](./examples/python).\n* New [efficient epilogues](test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative.cu#L783) using TMA for Hopper.\n* Support for [fused epilogues](test/unit/gemm/device/sm90_gemm_f16_f16_f16_tensor_op_f32_cluster_warpspecialized_cooperative_bias_elementwise.cu), such Bias, ReLU and GELU, using the new efficient epilogues.\n* New [warp-specialized TensorFloat-32 (TF32) GEMM kernels](test/unit/gemm/device/sm90_gemm_tf32_tf32_f32_tensor_op_f32_gmma_rs_cluster_warpspecialized.cu) targeting Hopper TMA.\n* New [*warp-specialized persistent cooperative*](./include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_cooperative.hpp) kernel design that allows for larger tile sizes and improves performance on Hopper.\n* An [example](./examples/51_hopper_gett) showcasing GEMM-Like Tensor-Tensor Contraction (GETT) capability on Hopper.\n* Epilogue builders. Similar to mainloop builders (see [example 49](./examples/49_hopper_gemm_with_collective_builder/49_collective_builder.cu)), epilogue builders aim to generate the best-possible epilogue while exposing incremental opt-ins for greater customization.\n* Profiler support for overriding kernel and epilogue builder auto schedules for 3.x API kernels, allowing specific policies to be run in the CUTLASS profiler.\n* Performance optimizations for the [*warp-specialized persistent ping-pong*](./include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized_pingpong.hpp) kernel.\n* Changes to the [GEMM API 3.x](./media/docs/gemm_api_3x.md), involving the host-facing arguments and the underlying `Params` structs.\n* [FMHA Backward Pass](./examples/41_fused_multi_head_attention/fused_multi_head_attention_backward.cu) from Meta xFormers.\n* [Streamk GEMM with Broadcast](./examples/47_ampere_gemm_universal_streamk/ampere_gemm_universal_streamk_broadcast.cu) enables epilogue broadcast with StreamK GEMM.\n* [Batched B2B GEMM](./examples/13_two_tensor_op_fusion) now can run multiple Back-to-Back GEMM with the same problem size in parallel.\n* [Batched Strided GEMV](test/unit/gemm/device/gemv.cu) support both row major and column major input matrix.\n* [Permute + GEMM fusion](./examples/39_gemm_permute) can fuse Permute with following GEMM now.  Before, we only support fusing GEMM with Permute in the epilogue.\n* [Row Broadcast](./include/cutlass/epilogue/threadblock/predicated_tile_iterator_row_broadcast.h) can be fused in the epilogue.\n* The GitHub branch is renamed from `master` to `main` in this release.\n* Optimal performance using [**CUDA 12.1**](https://developer.nvidia.com/cuda-downloads)\n* Updates and bugfixes from the community (thanks!)\n\n## [3.0.0](https://github.com/NVIDIA/cutlass/releases/tag/v3.0.0) (2023-01-23)\n* [CuTe](./media/docs/cute/00_quickstart.md), a [new core library and backend](./include/cute) for CUTLASS 3.0 that defines a single Layout vocabulary type and an associated algebra of layouts for a much more expressive and composable abstraction for tensors, sets of parallel agents, and operations by said agents on tensors.\n* [A new conceptual operation hierarchy](./media/docs/cutlass_3x_design.md) that replaces the architecture-centric hierarchy of CUTLASS 2.x and [documentation for CUTLASS 3.0's GEMM API changes](./media/docs/gemm_api_3x.md).\n* Strict API backwards compatibility that exposes both 2.x and 3.x API kernels through the same [`device::GemmUniversalAdapter`](./include/cutlass/gemm/device/gemm_universal_adapter.h) and [`kernel::GemmUniversal`](./include/cutlass/gemm/kernel/gemm_universal.hpp) types, allowing users to include both APIs in the same translation units. More information can be found in the [3.x backwards compatibility section](./media/docs/cutlass_3x_backwards_compatibility.md).\n* Updates to [Functionality](./media/docs/functionality.md) which directs users on which kernels are supported via CUTLASS-2 and CUTLASS-3.\n* Updates to [Compatibility](./README.md#compatibility) Section regarding supported compilers, operating systems, CUDA Toolkits, Hardware Architectures and [Target Architecture](./README.md#Target-Architecture).\n* New warp-specialized GEMM [kernel schedules](./include/cutlass/gemm/kernel/sm90_gemm_tma_warpspecialized.hpp) and [mainloops](./include/cutlass/gemm/collective/sm90_mma_tma_gmma_ss_warpspecialized.hpp) targeting Hopper architecture that achieve great performance with TMA, WGMMA, and threadblock clusters.\n* Extensions to CUTLASS profiler to support threadblock cluster shapes in library and profiler tile configurations.\n* [CUTLASS library integration](./tools/library/src/gemm_operation_3x.hpp) for 3.x API kernels built through the new `CollectiveBuilder` API, enabling CUTLASS profiler.\n* Support for [Hopper GEMMs](./examples/48_hopper_warp_specialized_gemm) through the new 3.0 API with CuTe-based exposure of the Hopper [Tensor Memory Accelerator](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#data-movement-and-conversion-instructions-cp-async-bulk-tensor) and [WGMMA Tensor Core](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions) features.\n* Set of examples that demonstrate the usage of the new 3.0 API to easily build GEMM kernels targeting Hopper: examples [48](./examples/48_hopper_warp_specialized_gemm), [49](./examples/49_hopper_gemm_schedules_with_collective_builder), and [50](./examples/50_hopper_gemm_with_epilogue_swizzle).\n\n## [2.11.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.11.0) (2022-11-19)\n* [Stream-K](./examples/47_ampere_gemm_universal_streamk), which is a new general way to do split-K.  It can not only improve performance, but can also significantly reduce the number of tile sizes that need to be profiled to find the best one.\n* [Fused multi-head attention Kernel](./examples/41_fused_multi_head_attention).  It has two variants: one uses batched GEMM for the fixed sequence length, and the other one uses group GEMM for the variable sequence length.  Both versions just need one kernel.\n* [Dual GEMM](./examples/45_dual_gemm), which can fuse A x B and A x C into one kernel. Two GEMMs has no producer-consumer dependency.\n* Hopper improves [double precision matrix multiplication](./test/unit/gemm/device/gemm_f64n_f64t_f64t_tensor_op_f64_sm90.cu) by 2x compared to Ampere at iso-clocks. It is supported since CUDA 11.8.\n* [BLAS3](./test/unit/gemm/device/hemm_cf64_cf64_cf64_tensor_op_f64_sm90.cu) functions with Hoppers new double precision matrix multiplication instructions.\n* [ELL Block Sparse GEMM](./examples/43_ell_block_sparse_gemm), which uses an [ELL matrix](https://developer.nvidia.com/blog/accelerating-matrix-multiplication-with-block-sparse-format-and-nvidia-tensor-cores/) to describe the sparsity of A matrix.  B and output matrices are still dense. The block size can be arbitary.\n* Optimized [Group Conv](./examples/42_ampere_tensorop_group_conv) for SingleGroup mode, which requires that the output channel per group is a multiple of Threadblock tile N.\n* [Optimized DepthWise Conv](./examples/46_depthwise_simt_conv2dfprop/depthwise_simt_conv2dfprop.cu).  Two new modes are added\n  * [kOptimized](./test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu) - use direct conv to compute instead of implicit GEMM.\n    *  The restrictions are: 1) input ,output channel and group number should be multiple of (128 / sizeof(input element)). 2) The input filter size should be the same as the template parameter configuration.\n  * [kFixedStrideDilation](./test/unit/conv/device/depthwise_conv2d_fprop_direct_conv_fixed_stride_dilation_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu) - which puts stride and dilation into templates to further improve the performance. In this mode, kernel persistents some inputs into register to squeeze more performance, so large filter/stride/dilation is not recommanded.\n    * The restrictions are: 1) input, output channel and group number should be multiple of (128 / sizeof(input element)). 2) input filter size, stride, dilation should same as the template parameter configuration.\n* [Scripts](./examples/44_multi_gemm_ir_and_codegen) to fuse multiple back-to-back GEMM.  Its implementation was discussed in a GTC'22 Spring [talk](https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41606/).\n* [FP8 data type definition](./include/cutlass/float8.h) and [conversion routines](./include/cutlass/numeric_conversion.h#L1274-2115).\n* Updates and bugfixes from the community (thanks!).  Big shout out to Meta's [xFormers](https://github.com/facebookresearch/xformers).\n\n* **Deprecation announcement:** CUTLASS plans to deprecate the following:\n  * Maxwell and Pascal GPU architectures\n  * Ubuntu 16.04\n  * CUDA 10.2\n\n## [2.10.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.10.0) (2022-08-23)\n* [CUTLASS Python](./examples/40_cutlass_py) now supports GEMM, CONV, Group GEMM for different data types as well as different epilogue flavours.\n* Optimizations for CUTLASS's [Grouped GEMM](./examples/24_gemm_grouped/gemm_grouped.cu) kernel.  Threadblock scheduling part is improved.  Some computation can be moved to the host side if applicable.  [Grouped Syr2k](./examples/38_syr2k_grouped/syr2k_grouped.cu) kernels are added, too.\n* Optimizations for [GEMM+Softmax](./examples/35_gemm_softmax).  All the reduction computation is fused into the previous GEMM.  More template arguments are provided to fine tune the performance.\n* [Grouped GEMM for Multihead Attention](./examples/41_multi_head_attention).  This general group gemm based MHA does not require the sequence length of all GEMMs to be the same which makes it most useful for natural language processing.\n* [GEMM + Layer norm fusion for Ampere](./examples/37_gemm_layernorm_gemm_fusion/) splits the layernorm into two parts and both of them can be fused into the GEMMs before and after separately.  In addition to use square sum to compute variance of layernorm, [Shift-K](https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance#Computing_shifted_data) is provided if square sum raise numerical issues.\n* [GEMM Epilogue Permutation Fusion](./examples/39_gemm_permute) can apply user provided permutation layout mapping in the GEMM epilogue.\n* [Grouped convolution targeting implicit GEMM](test/unit/conv/device/group_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu) introduces the first group convolution implementation to CUTLASS.  It is an Analytical implementation, not an Optimized.  The restrictions are: 1) input and output channel number should be multiple of group number. 2) split-K is not supported.  The implementation has 2 modes:\n  * kSingleGroup: output channel per group is multiple of Threadblock tile N.\n  * kMultipleGroup: Threadblock tile N is multiple of output channel per group.\n* [Depthwise separable convolution](test/unit/conv/device/depthwise_conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_simt_f16_sm60.cu) introduces the first depthwise convolution which is also Analytical for now.  The restrictions are: 1) SIMT only 2) No split-K 3) input channel equals to output channel equals to group number.\n* Standalone [Layernorm](./tools/util/include/cutlass/util/device_layernorm.h) and [Pooling](./tools/util/include/cutlass/util/device_nhwc_pooling.h) kernels.\n* [Back-to-back GEMM/CONV](./examples/13_two_tensor_op_fusion) relaxes the requirement that the first GEMM K dimension needs to be the multiple of Threadblock Tile K dimension.\n* Optimal performance using [**CUDA 11.6u2**](https://developer.nvidia.com/cuda-downloads)\n* Updates and bugfixes from the community (thanks!)\n\n## [2.9.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.9.0) (2022-04-21)\n\n* [First layer Convolution kernels](./test/unit/conv/device/conv2d_fprop_fixed_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu) specialized for small channel counts and reduced alignment\n  * [Few channels](./include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_few_channels.h) specialization for reduced alignment capabilities\n  * [Fixed channels](./include/cutlass/conv/threadblock/conv2d_fprop_activation_tile_access_iterator_fixed_channels.h) further specialized when channel count perfectly matches the access vector size\n  * [Unit tests](./test/unit/conv/device/conv2d_fprop_few_channels_f16nhwc_f16nhwc_f16nhwc_tensor_op_f32_sm80.cu)\n  * [Python-based instance emitter](./python/cutlass_library/generator.py) in the CUTLASS Library and support in the Profiler\n* [BLAS3](https://docs.nvidia.com/cuda/cublas/index.html#cublas-level-3-function-reference) operators accelerated by Tensor Cores\n  * Supported types: f32, cf32, f64, cf64, tf32x3, complex tf32x3\n  * [HERK](./test/unit/gemm/device/her2k_cf32h_cf32n_tensor_op_fast_f32_sm80.cu) with [emitter](./python/cutlass_library/rank_k_operation.py)\n  * [SYRK](./test/unit/gemm/device/syrk_f32n_f32t_tensor_op_fast_f32_sm80.cu) with [emitter](./python/cutlass_library/rank_k_operation.py)\n  * [SYMM](./test/unit/gemm/device/symm_f32n_f32n_tensor_op_fast_f32_ls_sm80.cu) with [emitter](./python/cutlass_library/symm_operation.py)\n  * [TRMM](./test/unit/gemm/device/trmm_f32n_f32t_f32t_tensor_op_fast_f32_ls_sm80.cu) with [emitter](./python/cutlass_library/trmm_operation.py)\n  * [Unit tests](./test/unit/gemm/device/testbed_rank_k_universal.h)\n* [CUTLASS Python](./examples/40_cutlass_py) demonstrating JIT compilation of CUTLASS kernels and a Python-based runtime using [CUDA Python](https://developer.nvidia.com/cuda-python)\n  * [Python-based runtime](./tools/library/scripts/rt.py) interoperable with existing emitters\n* [GEMM + Softmax example](./examples/35_gemm_softmax)\n* [Gather and Scatter Fusion with GEMM](./examples/36_gather_scatter_fusion) can gather inputs and scatters outputs based on indices vectors in the same GEMM kernel.\n  * It can select random rows in a row major matrix.\n  * It can select random columns in a column major matrix.\n* [Back-to-back GEMM/CONV](./examples/13_two_tensor_op_fusion) fully supports buffering the first GEMM/CONV results in the shared memory for the latter one to use.  It can eliminate register spill when the tile size is big.  Additionally, bias vector add is supported in the first GEMM/CONV.\n  * Supported kernels: GEMM and CONV.\n  * Supported types: fp16 and int8.\n  * Supported architectures: Turing and Ampere.\n* [Transposed Convolution](./examples/34_transposed_conv2d) (a.k.a Deconvolution) support which reuses Dgrad implementation.\n* [Utility functions](./tools/util/include/cutlass/util) that can pad NHWC and convert between NCHW and NHWC.\n* [Small alignment implicit gemm](https://github.com/NVIDIA/cutlass/issues/242) support for Fprop/Dgrad/Wgrad so that padding is no longer mandated to use tensor cores in these kernels.\n* Epilogue enhancement:\n  * Eliminate bank conflicts in int8 tensor core kernels.\n  * Half2 usage if epilogue compute type is fp16.\n  * More activation functions: Silu, Hardswish, Leaky Relu.\n  * New elementwise fusion pattern for [residual block](./include/cutlass/epilogue/thread/linear_combination_residual_block.h).\n* [Group GEMM](./examples/24_gemm_grouped) thread block number calculation fix which helps to launch the intended number of threadblocks to fully occupy the GPUs.\n* [Parallel GEMM splitk](https://github.com/NVIDIA/cutlass/pull/277) support in the CUTLASS profiler.\n* Optimal performance using [**CUDA 11.6u2**](https://developer.nvidia.com/cuda-downloads)\n* Updates and bugfixes from the community (thanks!)\n\n\n## [2.8.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.8.0) (2021-11-19)\n\n* **TF32x3:** emulated single-precision using Tensor Cores\n  * 45+ TFLOPs on NVIDIA A100\n  * [GEMM SDK example](./examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm/27_ampere_3xtf32_fast_accurate_tensorop_gemm.cu) (real)\n  * [COMPLEX GEMM SDK example](./examples/29_ampere_3xtf32_fast_accurate_tensorop_complex_gemm/29_3xtf32_complex_gemm.cu) (complex)\n  * [Implicit GEMM Convolution SDK example](./examples/28_ampere_3xtf32_fast_accurate_tensorop_fprop/ampere_3xtf32_fast_accurate_tensorop_fprop.cu)\n* **Mainloop fusion for Convolution:** convolution with fused per-channel scale-bias-relu\n  * [Conv Fprop SDK example](./examples/25_ampere_fprop_mainloop_fusion/ampere_fprop_mainloop_fusion.cu)\n  * [Conv WGrad SDK example](./examples/26_ampere_wgrad_mainloop_fusion/ampere_wgrad_mainloop_fusion.cu)\n  * [cutlass::conv::device::ImplicitGemmConvolutionFusion](./include/cutlass/conv/device/implicit_gemm_convolution_fusion.h)\n* **Grouped GEMM:** similar to batched GEMM with distinct problem size per group\n  * [SDK example](./examples/24_gemm_grouped) with performance comparison with Batched Strided GEMM\n  * [cutlass::gemm::device::GemmGrouped](./include/cutlass/gemm/device/gemm_grouped.h)\n* [Implicit GEMM Convolution fusion](./examples/13_two_tensor_op_fusion/) supports staging 1st convolution's output accumulator in the shared memory on Turing. This allows more flexible warp tile sizes and less regsiter pressue.\n* Optimal performance using [**CUDA 11.5**](https://developer.nvidia.com/cuda-downloads)\n* Updates from the community (thanks!)\n\n* **Deprecation announcement:** CUTLASS plans to deprecate the following:\n  * Maxwell and Pascal GPU architectures\n  * Ubuntu 16.04\n  * CUDA 10.2\n\n## [2.7.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.7.0) (2021-09-24)\n  * Mainloop fusion for GEMM: [summation over A or B](./examples/23_ampere_gemm_operand_reduction_fusion/ampere_gemm_operand_reduction_fusion.cu)\n  * [Strided DGRAD (optimized iterators)](./include/cutlass/conv/kernel/default_conv2d_dgrad.h)\n  * [Half-precision GELU_taylor activation functions](./include/cutlass/epilogue/thread/activation.h#L196)\n    * Use these when accumulation and epilogue compute types are all `cutlass::half_t`\n  * Tuning and bug fixes to [fused GEMM + GEMM example](./examples/13_two_tensor_op_fusion/)\n  * Support for smaller than 128b aligned Convolutions: [see examples](test/unit/conv/device/conv2d_fprop_implicit_gemm_f16nhwc_f16nhwc_f16nhwc_tensor_op_f16_sm80.cu#L272)\n  * Caching of results to accelerate Convolution [unit tests](test/unit/conv/device/cache_testbed_output.h)\n    * Can be enabled or disabled by running `cmake .. -DCUTLASS_TEST_ENABLE_CACHED_RESULTS=OFF`\n  * Corrections and bug fixes reported by the CUTLASS community\n    * Thank you for filing these issues!\n\n## [2.6.1](https://github.com/NVIDIA/cutlass/releases/tag/v2.6.1) (2021-09-03)\n  * Arbitrary padding and striding for CUTLASS Strided DGRAD Convolution operator (Analytic Iterators)\n  * Tuning for GEMMs fused with partial reductions\n  * Corrections and bug fixes reported by the CUTLASS community\n    * Thank you for filing these issues!\n\n## [2.6.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.6.0) (2021-07-22)\n  * Optimal performance when compiled with the [CUDA 11.4 Toolkit](https://developer.nvidia.com/cuda-toolkit)\n    * Adopt the new L2 prefetch feature in [cp.async](./include/cutlass/arch/memory.h) and [global load](./include/cutlass/arch/memory_sm80.h)\n  * Fused operators with GEMM and Convolution\n    * [Fused broadcast in epilogue](test/unit/gemm/device/gemm_with_broadcast_f16n_f16n_f16n_tensorop_f32_sm75.cu)\n    * [Fused partial reduction in epilogue](./test/unit/gemm/device/gemm_with_reduction_f16n_f16n_f16n_tensorop_f32_sm75.cu)\n  * 64b tensor strides and leading dimensions support for GEMMs\n  * Affine rank=2 matrix layouts\n    * Row stride and column stride for matrices using [cutlass::layout::AffineRank2](./include/cutlass/layout/matrix.h)\n    * Support [FP64 tensor core](./examples/18_ampere_fp64_tensorop_affine2_gemm/ampere_fp64_tensorop_affine2_gemm.cu) and SIMT GEMM.\n  * [Batched GEMV](./test/unit/gemm/device/gemv.cu) preview implementation\n  * [New strided Dgrad](test/unit/conv/device/conv2d_strided_dgrad_implicit_gemm_f16nhwc_f16nhwc_f32nhwc_tensor_op_f32_sm80.cu) implementation\n    * Accelerates over previous implementation by cutting down redundant math by 4x\n    * Support using new `Dy` and `w` analytic iterators and existing `cutlass::conv::device::ImplicitGemmConvolution` interface\n  * Quaternion-valued GEMM and Convolution in single- and double-precision (targeting CUDA Cores)\n    * Updates to [quaternion.h](./include/cutlass/quaternion.h) and [functional.h](./include/cutlass/functional.h)\n    * SDK Example for [GEMM](./examples/21_quaternion_gemm/quaternion_gemm.cu) and [Convolution](./examples/22_quaternion_conv/quaternion_conv.cu)\n    * [Unit tests for GEMM](./test/unit/gemm/device/simt_qgemm_nn_sm50.cu) and [Convolution](./test/unit/conv/device/conv2d_fprop_implicit_gemm_qf32nhwc_qf32nhwc_qf32nhwc_simt_f32_sm50.cu)\n  * Many improvements to the epilogue.\n    * Provide an [option](./include/cutlass/epilogue/threadblock/epilogue.h) to not fully unroll the epilogue to reduce the code size and improve the performance when using complicated elementwise operations\n    * Performance improvement for FP16 tensor core kernels\n    * Bug fixes\n  * Enhanced Clang support and the combination of Clang 13 and CUDA 11.4 can build and run kernels from Pascal and Ampere.\n  * Updated minimum CUDA Toolkit requirement to 10.2\n    * [CUDA 11.4 Toolkit](https://developer.nvidia.com/cuda-toolkit) recommended\n  * Corrections and bug fixes reported by the CUTLASS community\n    * Thank you for filing these issues!\n\n## [2.5.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.5.0) (2021-02-26)\n  * Tensor reductions\n    * _m_-to-_n_ reductions of tensors with affine layout\n    * [Specializations](./test/unit/reduction/device/tensor_reduce_contiguous.cu) for reductions including contiguous dimension\n    * [Specializations](./test/unit/reduction/device/tensor_reduce_strided.cu) for reductions excluding contiguous dimension\n    * Custom reduction functors such as `cutlass::logical_and`\n    * Large tensor support, up to 2^63 elements (however, each dimension is limited to an extent of 2^31)\n  * Optimizations for 3-D convolution\n    * [Optimized tile iterators](./include/cutlass/conv/threadblock/conv3d_fprop_activation_tile_access_iterator_optimized.h) using precomputed delta table for 3-D convolution\n    * Full coverage of [forward](test/unit/conv/device/conv3d_fprop_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu) and [backwards](test/unit/conv/device/conv3d_dgrad_implicit_gemm_f16ndhwc_f16ndhwc_f32ndhwc_tensor_op_f32_sm80.cu) passes for 3D convolution\n  * [Fused Convolution+Convolution example](./examples/13_two_tensor_op_fusion/README.md)\n  * Corrections and bug fixes reported by the CUTLASS community\n    * Thank you for filing these issues!\n\n\n## [2.4.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.4.0) (2020-11-19)\n  * Implicit GEMM convolution kernels supporting CUDA and Tensor Cores on NVIDIA GPUs\n    * Operators: forward (Fprop), backward data gradient (Dgrad), and backward weight gradient (Wgrad) convolution\n    * Data type: FP32, complex<FP32>, Tensor Float 32 (TF32), BFloat16 (BF16), Float16, Int4, Int8, Int32\n    * Spatial dimensions: 1-D, 2-D, and 3-D\n    * Layout: NHWC, NCxHWx\n  * Implicit GEMM convolution components:\n    * Global memory iterators supporting Fprop, Dgrad, and Wgrad\n    * `MmaMultistage` for implicit GEMM convolution for NVIDIA Ampere architecture\n    * `MmaPipeline` for implicit GEMM convolution for NVIDIA Volta and Turing architectures\n    * [Documentation](./media/docs/implicit_gemm_convolution.md) describing Implicit GEMM Convolution algorithm and implementation\n\n## [2.3.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.3.0) (2020-09-23)\n * [NVIDIA Ampere Architecture features](https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/)\n   * [Sparse Tensor Core GEMM kernels](test/unit/gemm/device/gemm_f16n_f16n_f32t_tensor_op_f32_sparse_sm80.cu):\n     * Direct access to Sparse Tensor Cores and maximum performance via [`mma.sp.sync`](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma-and-friends)\n   * Fast SGEMM targeting GeForce RTX 30-series CUDA Cores\n * Minor Features:\n   * [Activation functions](./include/cutlass/epilogue/thread/activation.h) such as [GeLU](./include/cutlass/epilogue/thread/linear_combination_gelu.h) and [Sigmoid](./include/cutlass/epilogue/thread/linear_combination_sigmoid.h)\n   * Small [matrix](./include/cutlass/matrix.h) and [quaternion](./include/cutlass/quaternion.h) template classes in device code\n   * [Floating-point constants](./include/cutlass/constants.h)\n * NVIDIA Ampere GPU Architecture examples and documentation:\n   * [Tensor Float 32](./examples/14_ampere_tf32_tensorop_gemm/ampere_tf32_tensorop_gemm.cu) and\n   * [Sparse Tensor Cores](./examples/15_ampere_sparse_tensorop_gemm/ampere_sparse_tensorop_gemm.cu)\n   * Documentation added on CUTLASS [efficient row-major epilogue](./media/docs/gemm_api.md#efficient-epilogue)\n\n## [2.2.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.2.0) (2020-06-08)\n * [NVIDIA Ampere Architecture features](https://devblogs.nvidia.com/nvidia-ampere-architecture-in-depth/)\n   * Fast Tensor Core operations:\n    * Maximum performance via [`mma.sync`](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma-and-friends)\n    * Tensor Float 32, BFloat16, and double-precision data types\n    * Mixed integer data types (int8, int4, bin1)\n   * Asynchronous copy for deep software pipelines via [`cp.async`](https://docs.nvidia.com/cuda/parallel-thread-execution)\n   * Described in [GTC 2020 Webinar (SR 21745)](https://developer.nvidia.com/gtc/2020/video/s21745) (free registration required)\n * Features:\n   * SDK examples showing GEMM fused with bias+relu and fused GEMM+GEMM\n   * Complex-valued GEMMs targeting NVIDIA Ampere Tensor Cores in double-precision and Tensor Float 32\n   * Gaussian complex GEMMs using 3m complex multiply algorithm\n   * Universal GEMM kernel supporting two batch modes and two algorithms for parallel reductions\n * Policy updates:\n   * [CUDA 11 Toolkit](https://developer.nvidia.com/cuda-toolkit) needed to enable NVIDIA Ampere Architecture features\n   * Disabled F16C by default for compatibility - enable on cmake command line with `-DCUTLASS_ENABLE_F16C=ON`\n\n## [2.1.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.1.0) (2020-04-06)\n * BLAS-style host-side API added to [CUTLASS Library](./media/docs/quickstart.md#cutlass-library)\n    * API to launch compiled kernel instances for GEMM and planar complex GEMM\n * Planar Complex GEMM kernels targeting Volta and Turing Tensor Cores\n    * Computes complex matrix products on matrices stored as disjoint real and imaginary parts\n    * [SDK Examples of Planar Complex GEMMs](./examples/10_planar_complex/planar_complex.cu)\n * Minor enhancements and bug fixes\n\n## [2.0.0](https://github.com/NVIDIA/cutlass/releases/tag/v2.0.0) (2019-11-19)\n * Substantially refactored for\n    * Better performance, particularly for native Turing Tensor Cores\n    * Robust and durable templates spanning the design space\n    * Encapsulated functionality embodying modern C++11 programming techniques\n    * Optimized containers and data types for efficient, generic, portable device code\n  * Updates to:\n    * [Quick start guide](./media/docs/quickstart.md)\n    * [Documentation](./README.md#documentation)\n    * [Utilities](./media/docs/utilities.md)\n    * [CUTLASS Profiler](./media/docs/profiler.md)\n * Native Turing Tensor Cores\n    * Efficient GEMM kernels targeting Turing Tensor Cores\n    * Mixed-precision floating point, 8-bit integer, 4-bit integer, and binarized operands\n * Coverage of existing CUTLASS functionality\n    * GEMM kernels targeting CUDA and Tensor Cores in NVIDIA GPUs\n    * Volta Tensor Cores through native mma.sync and through WMMA API\n    * Optimizations such as parallel reductions, threadblock rasterization, and intra-threadblock reductions\n    * Batched GEMM operations\n    * Complex-valued GEMMs\n * **Note: a host compiler supporting C++11 or greater is required.**\n\n# CUTLASS 1.x\n\n## [1.3.2](https://github.com/NVIDIA/cutlass/releases/tag/v1.3.2) (2019-07-09)\n * Performance improvement for Volta Tensor Cores TN and TT layouts.\n\n## [1.3.1](https://github.com/NVIDIA/cutlass/releases/tag/v1.3.1) (2019-04-09)\n * Corrected NVRTC unit tests.\n\n## [1.3.0](https://github.com/NVIDIA/cutlass/releases/tag/v1.3.0) (2019-03-20)\n * Efficient GEMM kernel targeting Volta Tensor Cores via `mma.sync` instruction added in CUDA 10.1.\n\n## [1.2.0](https://github.com/NVIDIA/cutlass/releases/tag/v1.2.0) (2018-10-26)\n * Parallelized reductions across threadblocks (\"Split-K\")\n   * Improved IGEMM performance\n * Batched strided WMMA GEMMs\n\n## [1.1.0](https://github.com/NVIDIA/cutlass/releases/tag/v1.1.0) (2018-09-19)\n  * Turing Features\n    * WMMA GEMM targeting TensorCores - INT8, INT4, 1-bit\n  * Batched Strided GEMM\n  * Threadblock rasterization strategies\n    * Improved performance for adverse problem sizes and data layouts\n  * Extended CUTLASS Core comonents\n    * Tensor views support arbitrary matrix and tensor layouts\n    * Zip iterators for structuring multiple data streams\n  * Enhanced CUTLASS utilities\n    * Reference code for tensor operations in host and device code\n    * Added HostMatrix<> for simplified matrix creation\n  * Examples\n    * Basic GEMM, tensor views, CUTLASS utilities, batched GEMM, WMMA GEMM\n\n## [1.0.1](https://github.com/NVIDIA/cutlass/releases/tag/v1.0.1) (2018-06-11)\n\n  * Intra-threadblock reduction added for small threadblock tile sizes\n    * sgemm_64x128x16, sgemm_128x128x16, sgemm_128x64x16, sgemm_128x32x16, sgemm_64x64x16, sgemm_64x32x16\n    * igemm_32x32x128\n  * GEMM _K_ residue handled during prologue prior to mainloop\n  * Replaced Google Test copy with submodule. Use `git submodule init --recursive --update`\n\n## [1.0.0](https://github.com/NVIDIA/cutlass/commit/2028ebe120aab22bfd0b2baf8902d4c9627eb33f) (2018-05-16)\n\n  * Substantial rewrite to accommodate new architecture\n  * Kernels: SGEMM, DGEMM, IGEMM, HGEMM, WMMA GEMM\n  * Unit and performance tests\n\n## [0.0.1](https://github.com/NVIDIA/cutlass/commit/d08ba8ac46e2fa3f745e070c390182edb56b2e91) (2017-12-04)\n\n  * Initial release\n\n\n## Copyright\n\nCopyright (c) 2017 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\nSPDX-License-Identifier: BSD-3-Clause\n\n```\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 3.1884765625,
          "content": "cff-version: 1.2.0\ntitle: CUTLASS\nmessage: >-\n  If you use this software, please cite using the\n  following metadata.\ntype: software\nauthors:\n  - given-names: Vijay\n    family-names: Thakkar\n    email: vithakkar@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Pradeep\n    family-names: Ramani\n    email: prramani@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Cris\n    family-names: Cecka\n    email: ccecka@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Aniket\n    family-names: Shivam\n    email: ashivam@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Honghao\n    family-names: Lu\n    email: honghaol@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Ethan\n    family-names: Yan\n    email: etyan@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Jack\n    family-names: Kosaian\n    email: jkosaian@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Mark\n    family-names: Hoemmen\n    email: mhoemmen@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Haicheng\n    family-names: Wu\n    email: haichengw@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Andrew\n    family-names: Kerr\n    email: akerr@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Matt\n    family-names: Nicely\n    email: mnicely@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Duane\n    family-names: Merrill\n    email: dumerrill@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Dustyn\n    family-names: Blasig\n    email: dblasig@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Fengqi\n    family-names: Qiao\n    email: fqiao@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Piotr\n    family-names: Majcher\n    email: pmajcher@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Paul\n    family-names: Springer\n    email: pspringer@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Markus\n    family-names: Hohnerbach\n    affiliation: NVIDIA\n    email: mhohnerbach@nvidia.com\n  - given-names: Jin\n    family-names: Wang\n    email: jinw@nvidia.com\n    affiliation: NVIDIA\n  - given-names: Manish\n    family-names: Gupta\n    affiliation: Google\n    email: manigupta@google.com\n\n\nrepository-code: 'https://github.com/NVIDIA/cutlass'\nabstract: >-\n  CUTLASS is a collection of CUDA C++ template\n  abstractions for implementing high-performance\n  matrix-multiplication (GEMM) and related\n  computations at all levels and scales within CUDA.\n  It incorporates strategies for hierarchical\n  decomposition and data movement similar to those\n  used to implement cuBLAS and cuDNN. CUTLASS\n  decomposes these \"moving parts\" into reusable,\n  modular software components abstracted by C++\n  template classes. These thread-wide, warp-wide,\n  block-wide, and device-wide primitives can be\n  specialized and tuned via custom tiling sizes, data\n  types, and other algorithmic policy. The resulting\n  flexibility simplifies their use as building blocks\n  within custom kernels and applications.\nkeywords:\n  - 'cutlass, tensor cores, cuda, cute, nvidia, gpu, linear algebra, matrix computations'\nlicense: BSD-3-Clause\nlicense-url: https://github.com/NVIDIA/cutlass/blob/v3.0.0/LICENSE.txt\nversion: '3.0.0'\ndate-released: '2023-01-23'\nidentifiers:\n  - type: url\n    value: \"https://github.com/NVIDIA/cutlass/tree/v3.0.0\"\n    description: The GitHub release URL of tag 3.0.0\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 40.603515625,
          "content": "# Copyright (c) 2017 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this\n# list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n# this list of conditions and the following disclaimer in the documentation\n# and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\ncmake_minimum_required(VERSION 3.19 FATAL_ERROR)\ncmake_policy(SET CMP0112 NEW)\n\nif(cutlass_LOADED)\n  # If CUTLASS has been previously fetched and loaded, don't do it again.\n  return()\nelse()\n  set(cutlass_LOADED ON)\n  set(CUTLASS_DIR ${CMAKE_CURRENT_SOURCE_DIR} CACHE PATH \"CUTLASS Repository Directory\")\nendif()\n\nmessage(STATUS \"CMake Version: ${CMAKE_VERSION}\")\nset(IMPLICIT_CMAKE_CXX_STANDARD OFF CACHE BOOL \"Do not explicitly specify -std=c++17 if set\")\n\n# To reduce duplicate version locations, parse the version out of the\n# main versions.h file and reuse it here.\n\nfile(READ ${CMAKE_CURRENT_SOURCE_DIR}/include/cutlass/version.h VERSION_FILE_CONTENTS)\nstring(REGEX MATCH \"#define CUTLASS_MAJOR ([0-9]+)\" _CUTLASS_VERSION_MAJOR \"${VERSION_FILE_CONTENTS}\")\nset(_CUTLASS_VERSION_MAJOR ${CMAKE_MATCH_1})\nstring(REGEX MATCH \"#define CUTLASS_MINOR ([0-9]+)\" _CUTLASS_VERSION_MINOR \"${VERSION_FILE_CONTENTS}\")\nset(_CUTLASS_VERSION_MINOR ${CMAKE_MATCH_1})\nstring(REGEX MATCH \"#define CUTLASS_PATCH ([0-9]+)\" _CUTLASS_VERSION_PATCH \"${VERSION_FILE_CONTENTS}\")\nset(_CUTLASS_VERSION_PATCH ${CMAKE_MATCH_1})\n\nmessage(STATUS \"CUTLASS ${_CUTLASS_VERSION_MAJOR}.${_CUTLASS_VERSION_MINOR}.${_CUTLASS_VERSION_PATCH}\")\n\n## CUTLASS PROJECT #############################################################\n\nproject(CUTLASS VERSION ${_CUTLASS_VERSION_MAJOR}.${_CUTLASS_VERSION_MINOR}.${_CUTLASS_VERSION_PATCH} LANGUAGES CXX)\n\n################################################################################\n\nif (CMAKE_CXX_COMPILER_ID MATCHES \"GNU\")\n  set(CUTLASS_GNU_HOST_COMPILE ON CACHE BOOL \"Using GNU tools for host code compilation\")\nendif()\nif (CMAKE_CXX_COMPILER_ID MATCHES \"[Cc]lang\")\n  set(CUTLASS_CLANG_HOST_COMPILE ON CACHE BOOL \"Using Clang tools for host code compilation\")\nendif()\nif (CMAKE_CXX_COMPILER_ID MATCHES \"MSVC\")\n  set(CUTLASS_MSVC_HOST_COMPILE ON CACHE BOOL \"Using MSVC tools for host code compilation\")\nendif()\n\n################################################################################\n\ninclude(${CMAKE_CURRENT_SOURCE_DIR}/CUDA.cmake)\n\nif (CUDA_VERSION VERSION_LESS 11.3)\n  message(WARNING \"CUTLASS ${CUTLASS_VERSION} requires CUDA 11.4 or higher, and strongly recommends CUDA 11.8 or higher.\")\nelseif (CUDA_VERSION VERSION_LESS 11.4)\n  message(WARNING \"CUTLASS ${CUTLASS_VERSION} support for CUDA ${CUDA_VERSION} is deprecated, please use CUDA 11.8 or higher.\")\nendif()\n\nif(CUTLASS_GNU_HOST_COMPILE AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.3)\n  message(FATAL_ERROR \"GCC version must be at least 7.3!\")\nendif()\n\nif (CUTLASS_CLANG_DEVICE_COMPILE AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS 7.0)\n  message(FATAL_ERROR \"Clang 7.0+ required for GPU compilation\")\nendif()\nfind_package(Doxygen QUIET)\n\n################################################################################\n\n#\n# CUTLASS 3.x requires C++17\n#\nset(CMAKE_CXX_STANDARD 17)\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CXX_EXTENSIONS OFF)\n\nset(CMAKE_CUDA_STANDARD 17)\nset(CMAKE_CUDA_STANDARD_REQUIRED ON)\n\nlist(APPEND CUTLASS_CUDA_NVCC_FLAGS --expt-relaxed-constexpr)\n\nif(CMAKE_INSTALL_PREFIX_INITIALIZED_TO_DEFAULT)\n  set(CMAKE_INSTALL_PREFIX install CACHE PATH \"Default installation location.\" FORCE)\nendif()\n\nmessage(STATUS \"Default Install Location: ${CMAKE_INSTALL_PREFIX}\")\n\nset(CUTLASS_TEST_LEVEL \"0\" CACHE STRING \"Level of tests to compile.\")\n# 0 - Sanity, 1 - Release-Quality, 2 - Exhaustive\n\nfind_package(Python3 3.5 COMPONENTS Interpreter REQUIRED)\n\n################################################################################\nset(CUTLASS_ENABLE_HEADERS_ONLY OFF CACHE BOOL \"Enable only the header library\")\n\nif(CUTLASS_ENABLE_HEADERS_ONLY)\n  set(CUTLASS_ENABLE_EXAMPLES_INIT OFF)\n  set(CUTLASS_ENABLE_TOOLS_INIT ON)\n  set(CUTLASS_ENABLE_LIBRARY_INIT OFF)\n  set(CUTLASS_ENABLE_TESTS_INIT OFF)\nelse()\n  set(CUTLASS_ENABLE_EXAMPLES_INIT ON)\n  set(CUTLASS_ENABLE_TOOLS_INIT ON)\n  set(CUTLASS_ENABLE_LIBRARY_INIT ON)\n  if(${CMAKE_PROJECT_NAME} STREQUAL ${PROJECT_NAME})\n    set(CUTLASS_ENABLE_TESTS_INIT ON)\n  else()\n    set(CUTLASS_ENABLE_TESTS_INIT OFF)\n  endif()\nendif()\n\nset(CUTLASS_TEST_UNIT_ENABLE_WARNINGS OFF CACHE BOOL \"Enable warnings on waived unit tests.\")\n\nset(CUTLASS_ENABLE_EXAMPLES ${CUTLASS_ENABLE_EXAMPLES_INIT} CACHE BOOL \"Enable CUTLASS Examples\")\nset(CUTLASS_ENABLE_TOOLS ${CUTLASS_ENABLE_TOOLS_INIT} CACHE BOOL \"Enable CUTLASS Tools\")\nset(CUTLASS_ENABLE_LIBRARY ${CUTLASS_ENABLE_LIBRARY_INIT} CACHE BOOL \"Enable CUTLASS Library\")\nset(CUTLASS_ENABLE_PROFILER ${CUTLASS_ENABLE_LIBRARY} CACHE BOOL \"Enable CUTLASS Profiler\")\nset(CUTLASS_ENABLE_PERFORMANCE ${CUTLASS_ENABLE_PROFILER} CACHE BOOL \"Enable CUTLASS Performance\")\n\nset(CUTLASS_ENABLE_TESTS ${CUTLASS_ENABLE_TESTS_INIT} CACHE BOOL \"Enable CUTLASS Tests\")\nset(CUTLASS_ENABLE_GTEST_UNIT_TESTS ${CUTLASS_ENABLE_TESTS} CACHE BOOL \"Enable CUTLASS GTest-based Unit Tests\")\nset(CUTLASS_USE_SYSTEM_GOOGLETEST OFF CACHE BOOL \"Use system/external installation of GTest\")\nset(CUTLASS_USE_PACKED_TUPLE ON CACHE BOOL \"If ON, make cute::tuple be new standard-layout tuple type; if OFF, use the original cute::tuple implementation that is _not_ standard-layout.\")\nif (CUTLASS_USE_PACKED_TUPLE)\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTE_USE_PACKED_TUPLE=1)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DCUTLASS_USE_PACKED_TUPLE=1\")\n  message(STATUS \"Make cute::tuple be the new standard-layout tuple type\")\nelseif()\n  message(STATUS \"Use the original cute::tuple implementation that is _not_ standard-layout\")\nendif()\n\n################################################################################\n\nset(CUTLASS_NVCC_ARCHS_SUPPORTED \"\")\nif (CUDA_VERSION VERSION_GREATER_EQUAL 11.4)\n  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 70 72 75 80 86 87)\nendif()\nif (CUDA_VERSION VERSION_GREATER_EQUAL 11.8)\n  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 89 90)\nendif()\nif (CUDA_VERSION VERSION_GREATER_EQUAL 12.0)\n  list(APPEND CUTLASS_NVCC_ARCHS_SUPPORTED 90a)\nendif()\nset(CUTLASS_NVCC_ARCHS ${CUTLASS_NVCC_ARCHS_SUPPORTED} CACHE STRING \"The SM architectures requested.\")\nset(CUTLASS_NVCC_ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS} CACHE STRING \"The SM architectures to build code for.\")\n\n# Find unsupported and deprecated compute capabilities\nif (CUTLASS_NVCC_ARCHS_SUPPORTED)\n  set(CUTLASS_NVCC_ARCHS_UNSUPPORTED ${CUTLASS_NVCC_ARCHS})\n  list(REMOVE_ITEM CUTLASS_NVCC_ARCHS_UNSUPPORTED ${CUTLASS_NVCC_ARCHS_SUPPORTED})\n  if (CUTLASS_NVCC_ARCHS_UNSUPPORTED)\n    message(WARNING \"Using unsupported or deprecated compute capabilities ${CUTLASS_NVCC_ARCHS_UNSUPPORTED}. Support may be removed in future versions.\")\n  endif()\nelse()\n  message(WARNING \"No supported compute capabilities for CUDA ${CUDA_VERSION}.\")\nendif()\n\n# Special policy introduced in CMake 3.13\nif (POLICY CMP0076)\n  cmake_policy(SET CMP0076 NEW)\nendif()\n\ninclude(GNUInstallDirs)\n\nlink_directories(${CUDA_TOOLKIT_ROOT_DIR}/lib64/stubs)\nlink_directories(${CUDA_TOOLKIT_ROOT_DIR}/lib64)\n\n###################################################################################################\n#\n# Configure CMake variables\n#\n###################################################################################################\n\nmessage(STATUS \"CUDA Compilation Architectures: ${CUTLASS_NVCC_ARCHS_ENABLED}\")\n\nif (NOT (CMAKE_BUILD_TYPE OR CONFIGURATION_TYPES))\n  # By default we want to build in Release mode to ensure that we're getting best performance.\n  set(CMAKE_BUILD_TYPE Release CACHE STRING \"Choose build level\" FORCE)\n  set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"RelWithDebInfo\" \"Release\")\nendif()\n\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\nif (DEFINED CMAKE_DEBUG_POSTFIX)\n  set(CUTLASS_LIBRARY_DEBUG_POSTFIX_INIT ${CMAKE_DEBUG_POSTFIX})\nelse()\n  set(CUTLASS_LIBRARY_DEBUG_POSTFIX_INIT .debug)\nendif()\nset(CUTLASS_LIBRARY_DEBUG_POSTFIX ${CUTLASS_LIBRARY_DEBUG_POSTFIX_INIT} CACHE STRING \"Default postfix value for debug libraries\")\n\nif(WIN32)\n  # On Windows we link against the shared (DLL) runtime. Change gtest settings to match this.\n  set(gtest_force_shared_crt ON CACHE BOOL \"Use shared (DLL) run-time lib even when Google Test is built as static lib\" FORCE)\nendif()\n\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DCUTLASS_VERSIONS_GENERATED\")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -DCUTLASS_VERSIONS_GENERATED\")\n\nif (WIN32)\n  # Enable more warnings.  Add \"-Xcompiler=/WX\" to enable warnings as errors.\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -Xcompiler=/W3)\n\n  # Disable warning on Unicode characters\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -Xcompiler=/wd4819)\n\n  # Disable excess x86 floating point precision that can lead to results being labeled incorrectly\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -Xcompiler=/fp:strict)\nendif(WIN32)\n\nif (${CUTLASS_NVCC_VERBOSE})\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -v)\nendif()\n\n#\n# CUTLASS NAMESPACE\n#\nset(CUTLASS_NAMESPACE \"cutlass\" CACHE STRING \"Top level namespace of CUTLASS\")\n\nset(CUTLASS_NVCC_EMBED_CUBIN ON CACHE BOOL \"Embed compiled CUDA kernel binaries into executables.\")\nset(CUTLASS_NVCC_EMBED_PTX ON CACHE BOOL \"Embed compiled PTX into executables.\")\nset(CUTLASS_NVCC_KEEP OFF CACHE BOOL \"Keep intermediate files generated by NVCC.\")\nset(CUTLASS_ENABLE_F16C OFF CACHE BOOL \"Enable F16C x86 extensions in host code.\")\n\n################################################################################\n#\n# CUTLASS generator cmake configuration\n#\n\n# Kernel unified filter file\n\nset(KERNEL_FILTER_FILE \"\" CACHE STRING \"KERNEL FILTER FILE FULL PATH\")\n\nif (KERNEL_FILTER_FILE AND NOT CUTLASS_LIBRARY_KERNELS)\n  # If a kernel filter file is specified, we want to generate and then\n  # filter on the entire kernel set, not the default kernel\n  # (sub)set. The user may have overridden CUTLASS_LIBRARY_KERNELS, in which\n  # case the resulting kernel set will be the intersection of the two\n  # options differenced against CUTLASS_LIBRARY_IGNORE_KERNELS.\n  set(CUTLASS_LIBRARY_KERNELS_INIT \"*\")\nelse()\n  set(CUTLASS_LIBRARY_KERNELS_INIT \"\")\nendif()\n\nif (KERNEL_FILTER_FILE)\n  get_filename_component(KERNEL_FILTER_FILE \"${KERNEL_FILTER_FILE}\" ABSOLUTE)\n  set(KERNEL_FILTER_FILE \"${KERNEL_FILTER_FILE}\" CACHE STRING \"KERNEL FILTER FILE FULL PATH\" FORCE)\nendif()\n\nset(SELECTED_KERNEL_LIST \"selected\" CACHE STRING \"Name of the filtered kernel list\")\n\nif(KERNEL_FILTER_FILE)\n  message(STATUS \"Full path of filter file: ${KERNEL_FILTER_FILE}\")\nendif()\n\nset(CUTLASS_LIBRARY_OPERATIONS \"all\" CACHE STRING \"Comma-delimited list of operation name filters. Default '' means all operations are enabled.\")\nset(CUTLASS_LIBRARY_KERNELS ${CUTLASS_LIBRARY_KERNELS_INIT} CACHE STRING \"Comma-delimited list of kernel name filters. If unspecified, only the largest tile size is enabled. If the string 'all' is specified, all kernels are enabled.\")\nset(CUTLASS_LIBRARY_IGNORE_KERNELS \"\" CACHE STRING \"Comma-delimited list of kernels to exclude from build. This option ONLY takes effect if CUTLASS_LIBRARY_KERNELS is set.\")\nset(CUTLASS_LIBRARY_EXCLUDE_KERNELS \"\" CACHE STRING \"Comma-delimited list of kernels to exclude from build. This option always takes effect, whether or not CUTLASS_LIBRARY_KERNELS is set. It also can exclude kernels from the filter file (see KERNEL_FILTER_FILE).\")\nset(CUTLASS_LIBRARY_INSTANTIATION_LEVEL \"\" CACHE STRING \"Instantiation level for SM90 kernels. Set to `max` and make sure CUTLASS_LIBRARY_KERNELS is non-empty to stamp all possible kernel configurations.\")\n\n################################################################################\n\nset(CUTLASS_TEST_ENABLE_CACHED_RESULTS ON CACHE BOOL \"Enable caching and reuse of test results in unit tests\")\n\nset_property(CACHE CUTLASS_TEST_LEVEL PROPERTY STRINGS 0 1 2)\nlist(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTLASS_TEST_LEVEL=${CUTLASS_TEST_LEVEL})\nlist(APPEND CUTLASS_CUDA_CLANG_FLAGS -DCUTLASS_TEST_LEVEL=${CUTLASS_TEST_LEVEL})\n\nif (CUTLASS_TEST_ENABLE_CACHED_RESULTS)\n  message(STATUS \"Enable caching of reference results in conv unit tests\")\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTLASS_TEST_ENABLE_CACHED_RESULTS=1)\nendif()\n\nset(CUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED ON CACHE BOOL \"Enable/Disable rigorous conv problem sizes in conv unit tests\")\n\nif (CUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED)\n  message(STATUS \"Enable rigorous conv problem sizes in conv unit tests\")\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTLASS_CONV_UNIT_TEST_RIGOROUS_SIZE_ENABLED=1)\nendif()\n\n################################################################################\n\n#\n# CUDA 10.1 introduces \"mma\" in PTX performing collective matrix multiply operations.\n#\n\nif (CUDA_VERSION VERSION_LESS 10.1)\n  set(CUTLASS_ENABLE_TENSOR_CORE_MMA_DEFAULT OFF)\nelse()\n  set(CUTLASS_ENABLE_TENSOR_CORE_MMA_DEFAULT ON)\nendif()\n\n# Trace levels for debugging\nset(CUTLASS_DEBUG_TRACE_LEVEL \"0\" CACHE STRING \"Level of debug tracing to perform.\")\nlist(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTLASS_DEBUG_TRACE_LEVEL=${CUTLASS_DEBUG_TRACE_LEVEL})\n\nset(CUTLASS_ENABLE_TENSOR_CORE_MMA ${CUTLASS_ENABLE_TENSOR_CORE_MMA_DEFAULT} CACHE BOOL\n  \"Enable PTX mma instruction for collective matrix multiply operations.\")\n\nset(CUTLASS_ENABLE_SM90_EXTENDED_MMA_SHAPES OFF CACHE BOOL\n  \"Enable an extended set of SM90 WGMMA instruction shapes (may lead to increased compilation times)\")\nif(CUTLASS_ENABLE_SM90_EXTENDED_MMA_SHAPES)\n  message(STATUS \"Enabled extended SM90 WGMMA instruction shapes\")\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTE_SM90_EXTENDED_MMA_SHAPES_ENABLED)\nendif()\n\nset(CUTLASS_SKIP_REDUCTION_INIT OFF CACHE BOOL \"Disable init reduction workspace\")\n\n#\n# NOTE: running with asan and CUDA requires the following environment variable:\n#\n#  ASAN_OPTIONS=protect_shadow_gap=0:replace_intrin=0:detect_leaks=0\n#\n# without the above environment setting, an error like the following may be generated:\n#\n#  *** Error: Could not detect active GPU device ID [out of memory]\n#  ...\n#  ==9149==ERROR: LeakSanitizer: detected memory leaks\n#  ...\n#\nif(ENABLE_ASAN)  # https://github.com/google/sanitizers/wiki/AddressSanitizer\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS --compiler-options=-fsanitize=address --compiler-options=-fno-omit-frame-pointer)\n  string(APPEND CMAKE_EXE_LINKER_FLAGS \" -fsanitize=address\")\nendif()\n\n###################################################################################################\n#\n# Configure CUDA build options\n#\n###################################################################################################\n\nif(CUTLASS_NVCC_EMBED_PTX)\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS --cuda-include-ptx=all)\nendif()\n\nif (CUTLASS_SKIP_REDUCTION_INIT)\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTLASS_SKIP_REDUCTION_INIT=1)\nendif()\n\nif (CUTLASS_ENABLE_TENSOR_CORE_MMA)\n  list(APPEND CUTLASS_CUDA_FLAGS -DCUTLASS_ENABLE_TENSOR_CORE_MMA=1)\nendif()\n\nset(CUTLASS_PROFILER_DISABLE_REFERENCE OFF CACHE BOOL \"Disable compilation of reference kernels in the CUTLASS profiler.\")\nif (CUTLASS_PROFILER_DISABLE_REFERENCE)\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTLASS_PROFILER_DISABLE_REFERENCE=1)\nendif()\n\nif (CUTLASS_ENABLE_GDC_FOR_SM90)\n  message(STATUS \"Grid Dependency Control (GDC) is enabled for SM90 kernels (required for programmatic dependent launches).\")\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -DCUTLASS_ENABLE_GDC_FOR_SM90=1)\nendif()\n\nset(CUTLASS_ENABLE_SYNCLOG OFF CACHE BOOL \"Enable synchronization event logging for race condition debugging. WARNING: This redefines __syncthreads() and __syncwarp() in all downstream code!\")\n\nif (CUTLASS_ENABLE_SYNCLOG)\n  set(CMAKE_CUDA_SEPARABLE_COMPILATION ON)\n  string(APPEND CMAKE_CXX_FLAGS \" -DCUTLASS_ENABLE_SYNCLOG=1\")\n  string(APPEND CMAKE_CUDA_FLAGS \" -DCUTLASS_ENABLE_SYNCLOG=1\")\nendif()\n\n\n\n# Warnings-as-error exceptions and warning suppressions for Clang builds\nif (CUTLASS_CLANG_HOST_COMPILE)\n  \n  set(FLAGS_TO_ADD\n    \"-Wno-error=implicit-int-conversion\"\n    \"-Wno-error=pass-failed\"\n    \"-Wno-error=inconsistent-missing-override\"\n    \"-Wno-sign-conversion\"\n    \"-Wno-unused-parameter\"\n  )\n  \n  foreach(FLAG ${FLAGS_TO_ADD})\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${FLAG}\")\n    list(APPEND CUTLASS_CUDA_NVCC_FLAGS \"${FLAG}\")\n    list(APPEND CUTLASS_CUDA_CLANG_FLAGS \"${FLAG}\")\n  endforeach()\n  \nendif()\n\nif (NOT MSVC AND CUTLASS_NVCC_KEEP)\n  # MSVC flow handles caching already, but for other generators we handle it here.\n  set(CUTLASS_NVCC_KEEP_DIR ${CMAKE_CURRENT_BINARY_DIR}/tmp CACHE PATH \"Location to store NVCC scratch files\")\n  file(MAKE_DIRECTORY ${CUTLASS_NVCC_KEEP_DIR})\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS --keep -v) # --keep-dir may not work with nvcc for some directories.\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -save-temps=${CUTLASS_NVCC_KEEP_DIR})\nendif()\n\nif (CUTLASS_ENABLE_F16C AND NOT CMAKE_CROSSCOMPILING)\n  list(APPEND CUTLASS_CUDA_FLAGS -DCUTLASS_ENABLE_F16C=1)\n  if (CUTLASS_GNU_HOST_COMPILE OR CUTLASS_CLANG_HOST_COMPILE)\n    list(APPEND CUTLASS_CUDA_NVCC_FLAGS -Xcompiler=-mf16c)\n  elseif(CUTLASS_MSVC_HOST_COMPILE)\n    list(APPEND CUTLASS_CUDA_NVCC_FLAGS -Xcompiler=/arch:AVX2)\n  endif()\nendif()\n\nif (CUTLASS_ENABLE_OPENMP_TESTS)\n  find_package(OpenMP)\n  if(OpenMP_CXX_FOUND)\n    list(APPEND CUTLASS_CUDA_NVCC_FLAGS -Xcompiler=${OpenMP_CXX_FLAGS})\n  else()\n    message(WARNING \"CUTLASS_ENABLE_OPENMP_TESTS set but OpenMP not found.\")\n  endif()\nendif()\n\nif(UNIX)\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -Xcompiler=-Wconversion)\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -Xcompiler=-fno-strict-aliasing)\nendif()\n\n# Don't leak lineinfo in release builds\nif (NOT CMAKE_BUILD_TYPE MATCHES \"Release\")\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -gmlt)\n  list(APPEND CUTLASS_CUDA_NVCC_FLAGS -lineinfo)\nendif()\n\nif (CUTLASS_CLANG_DEVICE_COMPILE)\n  if (NOT CUTLASS_CLANG_HOST_COMPILE)\n    message(FATAL_ERROR \"Clang CUDA compilation requires Clang CXX compilation. Currently CMAKE_CXX_COMPILER is ${CMAKE_CXX_COMPILER_ID}\" )\n  endif()\n\n  # There are numerous Clang versions that can work with each CUDA toolkit and the\n  # the checks are not very useful so we are turning them off and using testing to\n  # ensure the various combinations work properly.\n\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS --cuda-path=${CUDA_TOOLKIT_ROOT_DIR})\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -D__NV_NO_HOST_COMPILER_CHECK=1)\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -Wno-unknown-cuda-version)\n\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -mllvm -pragma-unroll-threshold=100000)\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -mllvm -unroll-threshold=5000)\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -Wno-unused-command-line-argument)\n\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -D__CUDACC_VER_MAJOR__=${CUDA_VERSION_MAJOR} -D__CUDACC_VER_MINOR__=${CUDA_VERSION_MINOR})\n\n  # needed for libcublasLt.so in case it's installed in the same location as libcudart.so\n  # dynamic linker can find it if linker sets RPATH (forced by --disable-new-tags)\n  # Otherwise linker uses RUNPATH and that does not propagate to loaded libs.\n  list(APPEND CUTLASS_CUDA_CLANG_FLAGS -Wl,--disable-new-dtags)\n\n  link_libraries(nvidia::cudart)\n  link_libraries(nvidia::cuda_driver)\n  \nendif()\n\n#Report CUDA build flags\nif (CUTLASS_CLANG_DEVICE_COMPILE AND CUTLASS_CUDA_CLANG_FLAGS)\n  set(__FLAG_GROUP Clang)\n  set(__FLAG_LIST CUTLASS_CUDA_CLANG_FLAGS)\nelse(CUTLASS_NVCC_DEVICE_COMPILE AND CUTLASS_CUDA_NVCC_FLAGS)\n  set(__FLAG_GROUP NVCC)\n  set(__FLAG_LIST CUTLASS_CUDA_NVCC_FLAGS)\nendif()\n\nset(__FLAG_DISPLAY_STRING \"\")\nset(__FLAG_DISPLAY_SEPARATOR)\nlist(JOIN ${__FLAG_LIST} \"\\n  \" __FLAG_DISPLAY_STRING)\nmessage(STATUS \"Using the following ${__FLAG_GROUP} flags: \\n  ${__FLAG_DISPLAY_STRING}\")\n\n# Known gcc 8.1-8.3 SFINAE issue (fixed in gcc 8.4), check https://gcc.gnu.org/bugzilla/show_bug.cgi?id=87748\n# Also see https://github.com/NVIDIA/nccl/issues/835 for nvtx3.hpp\nif (CUTLASS_GNU_HOST_COMPILE AND CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 8.1 AND CMAKE_CXX_COMPILER_VERSION VERSION_LESS_EQUAL 8.3)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DNVTX3_USE_CHECKED_OVERLOADS_FOR_GET=0\")\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -DNVTX3_USE_CHECKED_OVERLOADS_FOR_GET=0\")\nendif()\n\n# Support for 128-bit integers if using NVIDIA C++ compiler\nif (${CMAKE_CXX_COMPILER_ID} MATCHES \"PGI\" OR ${CMAKE_CXX_COMPILER_ID} MATCHES \"NVHPC\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Mint128 \")\nendif()\n\n# CMake 3.18 added support for CUDA_ARCHITECTURES target property. We will use this\n# property for CMake 3.18+, so we request the NEW behavior for correct compatibility.\n# https://cmake.org/cmake/help/v3.18/policy/CMP0104.html#policy:CMP0104\ncmake_policy(SET CMP0104 NEW)\n\nif (MSVC)\n\n  # MSVC by default does not apply the correct __cplusplus version as specified by the C++ standard\n  # because MSVC is not a completely compliant implementation. This option forces MSVC to use the\n  # appropriate value given the requested --std option. This fixes a compilation issue mismatch\n  # between GCC/Clang and MSVC.\n  #\n  # error : a constexpr function cannot have a nonliteral return type \"dim3\"\n  #\n  # See https://developercommunity.visualstudio.com/t/msvc-incorrectly-defines-cplusplus/139261\n\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /Zc:__cplusplus\")\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler  /Zc:__cplusplus\")\n\nendif()\n\n# Some tests require this build option in order to link.\nif (MSVC)\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /bigobj\")\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler /bigobj\")\nendif()\n\nfunction(cutlass_apply_cuda_gencode_flags TARGET)\n  set(options)\n  set(oneValueArgs)\n  set(multiValueArgs SM_ARCHS)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  if (__SM_ARCHS)\n    set(ARCHS_ENABLED ${__SM_ARCHS})\n  else()\n    set(ARCHS_ENABLED ${CUTLASS_NVCC_ARCHS_ENABLED})\n  endif()\n\n  set(__CMAKE_CUDA_ARCHS)\n  foreach(ARCH ${ARCHS_ENABLED})\n    set(CODES)\n    if(CUTLASS_NVCC_EMBED_CUBIN)\n      list(APPEND __CMAKE_CUDA_ARCHS ${ARCH}-real)\n    endif()\n    if(CUTLASS_NVCC_EMBED_PTX AND NOT CUTLASS_CLANG_DEVICE_COMPILE)\n      # If we're using clang for device compilation, the ptx is inserted \n      # via another command line option and the `-virtual` flags will cause an error.\n      list(APPEND __CMAKE_CUDA_ARCHS ${ARCH}-virtual)\n    endif()\n    list(JOIN CODES \",\" CODES_STR)\n  endforeach()\n\n  set_property(TARGET ${TARGET} PROPERTY CUDA_ARCHITECTURES ${__CMAKE_CUDA_ARCHS})\n\nendfunction()\n\n# Cache the flags so they are available when the function below is called anywhere globally.\n\nset(__CUTLASS_CUDA_FLAGS ${CUTLASS_CUDA_FLAGS} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_FLAGS_RELEASE ${CUTLASS_CUDA_FLAGS_RELEASE} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_FLAGS_RELWITHDEBINFO ${CUTLASS_CUDA_FLAGS_RELWITHDEBINFO} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_FLAGS_DEBUG ${CUTLASS_CUDA_FLAGS_DEBUG} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_CLANG_FLAGS ${CUTLASS_CUDA_CLANG_FLAGS} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_CLANG_FLAGS_RELEASE ${CUTLASS_CUDA_CLANG_FLAGS_RELEASE} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_CLANG_FLAGS_RELWITHDEBINFO ${CUTLASS_CUDA_CLANG_FLAGS_RELWITHDEBINFO} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_CLANG_FLAGS_DEBUG ${CUTLASS_CUDA_CLANG_FLAGS_DEBUG} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_NVCC_FLAGS ${CUTLASS_CUDA_NVCC_FLAGS} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_NVCC_FLAGS_RELEASE ${CUTLASS_CUDA_NVCC_FLAGS_RELEASE} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_NVCC_FLAGS_RELWITHDEBINFO ${CUTLASS_CUDA_NVCC_FLAGS_RELWITHDEBINFO} CACHE INTERNAL \"\")\nset(__CUTLASS_CUDA_NVCC_FLAGS_DEBUG ${CUTLASS_CUDA_NVCC_FLAGS_DEBUG} CACHE INTERNAL \"\")\n\nfunction(cutlass_apply_standard_compile_options TARGET)\n\n  if(CUTLASS_CLANG_DEVICE_COMPILE)\n    set(CUDA_COMPILE_LANGUAGE CUDA)\n    set(_FLAGS ${__CUTLASS_CUDA_FLAGS} ${__CUTLASS_CUDA_CLANG_FLAGS})\n    set(_FLAGS_RELEASE ${__CUTLASS_CUDA_FLAGS_RELEASE} ${__CUTLASS_CUDA_CLANG_FLAGS_RELEASE})\n    set(_FLAGS_RELWITHDEBINFO ${__CUTLASS_CUDA_FLAGS_RELWITHDEBINFO} ${__CUTLASS_CUDA_CLANG_FLAGS_RELWITHDEBINFO})\n    set(_FLAGS_DEBUG ${__CUTLASS_CUDA_FLAGS_DEBUG} ${__CUTLASS_CUDA_CLANG_FLAGS_DEBUG})\n  else()\n    set(CUDA_COMPILE_LANGUAGE CUDA)\n    set(_FLAGS ${__CUTLASS_CUDA_FLAGS} ${__CUTLASS_CUDA_NVCC_FLAGS})\n    set(_FLAGS_RELEASE ${__CUTLASS_CUDA_FLAGS_RELEASE} ${__CUTLASS_CUDA_NVCC_FLAGS_RELEASE})\n    set(_FLAGS_RELWITHDEBINFO ${__CUTLASS_CUDA_FLAGS_RELWITHDEBINFO} ${__CUTLASS_CUDA_NVCC_FLAGS_RELWITHDEBINFO})\n    set(_FLAGS_DEBUG ${__CUTLASS_CUDA_FLAGS_DEBUG} ${__CUTLASS_CUDA_NVCC_FLAGS_DEBUG})\n  endif()\n\n  target_link_libraries(${TARGET} PRIVATE CUTLASS)\n\n  target_compile_options(\n    ${TARGET}\n    PRIVATE\n    $<$<COMPILE_LANGUAGE:${CUDA_COMPILE_LANGUAGE}>:${_FLAGS}>\n    $<$<COMPILE_LANGUAGE:${CUDA_COMPILE_LANGUAGE}>:$<$<CONFIG:RELEASE>:${_FLAGS_RELEASE}>>\n    $<$<COMPILE_LANGUAGE:${CUDA_COMPILE_LANGUAGE}>:$<$<CONFIG:RELWITHDEBINFO>:${_FLAGS_RELWITHDEBINFO}>>\n    $<$<COMPILE_LANGUAGE:${CUDA_COMPILE_LANGUAGE}>:$<$<CONFIG:DEBUG>:${_FLAGS_DEBUG}>>\n    )\n\nendfunction()\n\n#\n# The following items should eventually be pushed into cutlass/CMakeLists.txt\n#\n\n# GLOB for CUTLASS header files. Should we use a static list instead?\nfile(GLOB_RECURSE CUTLASS_INCLUDE RELATIVE ${CMAKE_CURRENT_SOURCE_DIR} include/cutlass/*.h)\nfile(GLOB_RECURSE CUTLASS_CUTLASS RELATIVE ${CMAKE_CURRENT_SOURCE_DIR}/include include/cutlass/*.h include/cutlass/*.hpp include/cutlass/*.inl)\nfile(GLOB_RECURSE CUTLASS_CUTE RELATIVE ${CMAKE_CURRENT_SOURCE_DIR}/include include/cute/*.h*)\nfile(GLOB_RECURSE CUTLASS_NVRTC RELATIVE ${CMAKE_CURRENT_SOURCE_DIR}/test test/unit/nvrtc/kernel/*.h)\n\n###################################################################################################\n#\n# Define build targets\n#\n###################################################################################################\n\nsource_group(TREE ${CMAKE_CURRENT_SOURCE_DIR}/include REGULAR_EXPRESSION \".*\\.h\")\n\nadd_library(CUTLASS INTERFACE)\nadd_library(nvidia::cutlass::cutlass ALIAS CUTLASS)\nset_target_properties(CUTLASS PROPERTIES EXPORT_NAME cutlass)\n\nset(CUTLASS_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/include CACHE PATH \"CUTLASS Header Library\")\n\nset(CUTLASS_GENERATOR_DIR ${CMAKE_CURRENT_SOURCE_DIR}/tools/library CACHE INTERNAL \"Location of generator scripts\")\n\n# The following utility directory is needed even if the tools build is disabled, so it exists here.\nset(CUTLASS_TOOLS_UTIL_INCLUDE_DIR ${CMAKE_CURRENT_SOURCE_DIR}/tools/util/include CACHE INTERNAL \"\")\n\ninclude_directories(${CUTLASS_INCLUDE_DIR})\n\ntarget_compile_features(CUTLASS INTERFACE cxx_std_11)\n\nif (NOT CUTLASS_NAMESPACE STREQUAL \"cutlass\")\n  target_compile_definitions(CUTLASS INTERFACE CUTLASS_NAMESPACE=${CUTLASS_NAMESPACE})\nendif()\n\nif (NOT DEFINED CUTLASS_REVISION)\n\n  find_package(Git QUIET)\n\n  execute_process(\n    COMMAND ${GIT_EXECUTABLE} rev-parse --short HEAD\n    RESULT_VARIABLE CUTLASS_REVISION_RESULT\n    OUTPUT_VARIABLE CUTLASS_REVISION\n    OUTPUT_STRIP_TRAILING_WHITESPACE\n  )\n\n  if (CUTLASS_REVISION_RESULT)\n    message(STATUS \"CUTLASS Revision: Unable to detect, Git returned code ${CUTLASS_REVISION_RESULT}.\")\n  else()\n    message(STATUS \"CUTLASS Revision: ${CUTLASS_REVISION}\")\n  endif()\n\nendif()\n\nconfigure_file(\n  ${CMAKE_CURRENT_SOURCE_DIR}/cmake/version_extended.h.in\n  ${CMAKE_CURRENT_BINARY_DIR}/include/cutlass/version_extended.h\n  @ONLY)\n\ntarget_include_directories(\n  CUTLASS\n  INTERFACE\n  $<INSTALL_INTERFACE:include>\n  $<BUILD_INTERFACE:${CUTLASS_INCLUDE_DIR}>\n  $<BUILD_INTERFACE:${CMAKE_CURRENT_BINARY_DIR}/include>\n  )\n\n# Mark CTK headers as system to supress warnings from them\ntarget_include_directories(\n  CUTLASS\n  SYSTEM INTERFACE\n  $<BUILD_INTERFACE:${CUDA_TOOLKIT_ROOT_DIR}/include>\n  )\n\ninstall(\n  DIRECTORY\n  ${CUTLASS_INCLUDE_DIR}/\n  ${CMAKE_CURRENT_BINARY_DIR}/include/\n  DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n  )\n\ninstall(\n  TARGETS CUTLASS\n  EXPORT NvidiaCutlass\n  PUBLIC_HEADER DESTINATION ${CMAKE_INSTALL_INCLUDEDIR}\n  )\n\n################################################################################\n\n# Doxygen is available. Generate documentation\nif (DOXYGEN_FOUND)\n    # DOT is available. Enable graph generation in the documentation\n    if (DOXYGEN_DOT_EXECUTABLE)\n        set(CUTLASS_ENABLE_DOXYGEN_DOT ON CACHE BOOL \"Use dot to generate graphs in the doxygen documentation.\")\n    else()\n        set(CUTLASS_ENABLE_DOXYGEN_DOT OFF CACHE BOOL \"Use dot to generate graphs in the doxygen documentation.\" FORCE)\n    endif()\n\n    if (CUTLASS_ENABLE_DOXYGEN_DOT)\n        set(HAVE_DOT \"YES\")\n    else()\n        set(HAVE_DOT \"NO\")\n    endif()\n\n    # Add custom target for Doxygen.\n    add_custom_target(cutlass_docs ${CMAKE_COMMAND} -E env\n        \"DOT_PATH=${DOXYGEN_DOT_EXECUTABLE}\"\n        \"HAVE_DOT=${HAVE_DOT}\"\n        ${DOXYGEN_EXECUTABLE} ${CMAKE_CURRENT_SOURCE_DIR}/Doxyfile\n        WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n        VERBATIM\n    )\nendif()\n\nif(NOT WIN32)\n  # Add common library search paths so executables and libraries can load and run\n  # without LD_LIBRARY_PATH being set.\n  link_libraries(\n    \"-Wl,-rpath,'$ORIGIN'\"\n    \"-Wl,-rpath,'$ORIGIN/../lib64'\"\n    \"-Wl,-rpath,'$ORIGIN/../lib'\"\n    \"-Wl,-rpath,'${CUDA_TOOLKIT_ROOT_DIR}/lib64'\"\n    \"-Wl,-rpath,'${CUDA_TOOLKIT_ROOT_DIR}/lib'\"\n    ${CMAKE_DL_LIBS}\n    )\nendif()\n\n################################################################################\n\ninclude(CTest)\nenable_testing()\n\nif (CUTLASS_ENABLE_GTEST_UNIT_TESTS)\n  if (CUTLASS_USE_SYSTEM_GOOGLETEST)\n    find_package(GTest REQUIRED)\n  else()\n    include(${CMAKE_CURRENT_SOURCE_DIR}/cmake/googletest.cmake)\n  endif()\nendif()\n\nif (NOT TARGET test_all)\n  add_custom_target(test_all)\nendif()\n\nset(CUTLASS_INSTALL_TESTS ON CACHE BOOL \"Install test executables\")\nset(CUTLASS_TEST_EXECUTION_ENVIRONMENT \"\" CACHE BOOL \"Environment in which to invoke unit test executables\")\n\nset(CMAKE_TEST_INSTALL_PREFIX test CACHE STRING \"Test root install location, relative to CMAKE_INSTALL_PREFIX.\")\nset(CUTLASS_TEST_INSTALL_PREFIX ${CMAKE_TEST_INSTALL_PREFIX}/cutlass CACHE STRING \"Test root install location, relative to CMAKE_INSTALL_PREFIX.\")\nset(CUTLASS_TEST_INSTALL_BINDIR ${CUTLASS_TEST_INSTALL_PREFIX}/${CMAKE_INSTALL_BINDIR} CACHE STRING \"Test root install location, relative to CMAKE_INSTALL_PREFIX.\")\nset(CUTLASS_TEST_INSTALL_LIBDIR ${CUTLASS_TEST_INSTALL_PREFIX}/${CMAKE_INSTALL_LIBDIR} CACHE STRING \"Test root install location, relative to CMAKE_INSTALL_PREFIX.\")\n\ninstall(DIRECTORY DESTINATION ${CUTLASS_TEST_INSTALL_PREFIX})\ninstall(DIRECTORY DESTINATION ${CUTLASS_TEST_INSTALL_BINDIR})\ninstall(DIRECTORY DESTINATION ${CUTLASS_TEST_INSTALL_LIBDIR})\ninstall(DIRECTORY DESTINATION ${CUTLASS_TEST_INSTALL_PREFIX}/ctest)\n\n################################################################################\n\nset(CUTLASS_ENABLE_CUBLAS OFF CACHE BOOL \"cuBLAS usage for tests\")\nset(CUTLASS_ENABLE_CUDNN OFF CACHE BOOL \"cuDNN usage for tests\")\n\ninclude(${CMAKE_CURRENT_SOURCE_DIR}/cuBLAS.cmake)\n\nif (CUTLASS_ENABLE_CUBLAS)\n  target_compile_definitions(CUTLASS INTERFACE CUTLASS_ENABLE_CUBLAS=1)\nendif()\n\ninclude(${CMAKE_CURRENT_SOURCE_DIR}/cuDNN.cmake)\n\nif (CUTLASS_ENABLE_CUDNN)\n  target_compile_definitions(CUTLASS INTERFACE CUTLASS_ENABLE_CUDNN=1)\nendif()\n\n################################################################################\n\nset(CUTLASS_DEFAULT_ACTIVE_TEST_SETS \"default\" CACHE STRING \"Default\n  activated test sets. In `make test` mode, this string determines the\n  active set of tests. In `ctest` mode, this value can be overriden\n  with CUTLASS_TEST_SETS environment variable when running the ctest\n  executable.\")\n\nfile(MAKE_DIRECTORY \"${CMAKE_BINARY_DIR}/${CMAKE_INSTALL_BINDIR}\")\nset(CUTLASS_CTEST_TEMPLATE_FILE ${CMAKE_CURRENT_LIST_DIR}/cmake/CTestTestfile.configure.cmake)\nset(CUTLASS_CTEST_GENERATED_FILES \"\" CACHE INTERNAL \"\")\n\nfunction(cutlass_add_executable_tests NAME TARGET)\n#\n# Generates test rules for `make test`, `make test_all`, and `ctest` invoked from either the\n# <CMAKE_BINARY_DIR> or the <CMAKE_INSTALL_PREFIX>/<CUTLASS_TEST_INSTALL_PREFIX> after installation.\n#\n# NAME: The base name for the test. Can be run with `make <NAME>` or `ctest -R 'c<NAME>'`.\n# TARGET: The target corresponding to the executable under test.\n# DISABLE_EXECUTABLE_INSTALL_RULE: An option, if given, that disables creating an install rule for TARGET.\n# DEPENDS: A list of targets or files on which this test is dependent.\n# DEPENDEES: A list of targets which should depend on this test.\n# TEST_COMMAND_OPTIONS: A list of variables (i.e. by reference params) which contain command line arguments\n#   to pass to the test executable. A unique test is generated for each set of\n#   options given. If this option is not used, a single test with no arguments is generated.\n# TEST_COMMAND_OPTIONS_PREFIX: If provided, is added as a prefix to each TEST_COMMAND_OPTIONS value for\n#   generating the full variable name to be referenced.\n# RESULT_CACHE_FILE: A file to be installed alongside the test executable with pre-computed\n#   test results to speed up test runtime.\n# TEST_SETS_SUPPORTED: A list of test set names these tests support.\n#\n\n  set(options DISABLE_EXECUTABLE_INSTALL_RULE DO_NOT_LOWERCASE_TEST_NAME)\n  set(oneValueArgs DISABLE_TESTS RESULT_CACHE_FILE TEST_COMMAND_OPTIONS_PREFIX)\n  set(multiValueArgs DEPENDS DEPENDEES TEST_COMMAND_OPTIONS TEST_SETS_SUPPORTED)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  if (NOT DEFINED __DISABLE_TESTS)\n    set(__DISABLE_TESTS OFF)\n  endif()\n\n  set(TEST_EXE $<TARGET_FILE_NAME:${TARGET}>)\n  set(TEST_EXE_WORKING_DIRECTORY ./${CMAKE_INSTALL_BINDIR})\n\n  if (NOT DEFINED __TEST_SETS_SUPPORTED)\n    set(__TEST_SETS_SUPPORTED ${CUTLASS_DEFAULT_ACTIVE_TEST_SETS})\n  endif()\n\n  set(TEST_SETS_SUPPORTED ${__TEST_SETS_SUPPORTED})\n\n  if (__RESULT_CACHE_FILE)\n\n    add_custom_command(\n      TARGET ${TARGET}\n      POST_BUILD\n      COMMAND ${CMAKE_COMMAND}\n      ARGS -E copy ${__RESULT_CACHE_FILE} \"$<TARGET_FILE_DIR:${TARGET}>\"\n      )\n\n  endif()\n\n  if (NOT __DISABLE_EXECUTABLE_INSTALL_RULE AND CUTLASS_INSTALL_TESTS)\n\n    # file(RELATIVE_PATH CMAKE_CURRENT_BINARY_RELATIVE_DIR ${CMAKE_BINARY_DIR} ${CMAKE_CURRENT_BINARY_DIR})\n\n    install(\n      TARGETS ${TARGET}\n      RUNTIME DESTINATION ${CUTLASS_TEST_INSTALL_BINDIR}\n      )\n\n    if (__RESULT_CACHE_FILE)\n\n     install(\n       FILES ${__RESULT_CACHE_FILE}\n       DESTINATION ${CUTLASS_TEST_INSTALL_BINDIR}/\n       )\n\n    endif()\n\n  endif()\n\n  if (NOT __TEST_COMMAND_OPTIONS)\n    set(__TEST_COMMAND_OPTIONS \" \")\n  endif()\n\n  list(LENGTH __TEST_COMMAND_OPTIONS CMD_COUNT)\n\n  if (CMD_COUNT GREATER 1)\n    add_custom_target(${NAME} DEPENDS ${TARGET} ${__DEPENDS})\n    foreach(DEPENDEE ${__DEPENDEES})\n      add_dependencies(${DEPENDEE} ${NAME})\n    endforeach()\n  endif()\n\n  if (CUTLASS_INSTALL_TESTS)\n\n    set(_INLINE_PER_TEST_CODE)\n\n    file(READ \"${PROJECT_SOURCE_DIR}/cmake/CTestTestfile.test.configure.cmake\" _INLINE_PER_TEST_CODE_TEMPLATE)\n\n  endif()\n\n  set(TEST_GROUP_NAME ${NAME})\n\n  # To run the tests from an install package with tests enabled, we need to generate test files\n  # that don't rely on the current directory structure in build.\n\n  set(TEST_NAME c${NAME})\n  set(TEST_GEN_DIR ${CMAKE_CURRENT_BINARY_DIR}/ctest/${TEST_NAME})\n  file(MAKE_DIRECTORY ${TEST_GEN_DIR})\n\n  set(TEST_EXE_PATH $<TARGET_FILE:${TARGET}>)\n  set(TEST_USE_EXTENDED_FORMAT ON)\n  configure_file(\"${CUTLASS_CTEST_TEMPLATE_FILE}\" \"${TEST_GEN_DIR}/CTestTestfile.${TEST_NAME}.cmake\" @ONLY)\n\n  set(TEST_EXE_PATH $<TARGET_FILE_NAME:${TARGET}>)\n  set(TEST_USE_EXTENDED_FORMAT OFF) # ctest does not support extended add_test format.\n  configure_file(\"${CUTLASS_CTEST_TEMPLATE_FILE}\" \"${TEST_GEN_DIR}/CTestTestfile.${TEST_NAME}.install.cmake.in\" @ONLY)\n\n  foreach(CMD_OPTIONS_VAR IN LISTS __TEST_COMMAND_OPTIONS)\n\n    if (CMD_COUNT GREATER 1)\n      set(TESTCASE_NAME \"${NAME}_${CMD_OPTIONS_VAR}\")\n    else()\n      set(TESTCASE_NAME \"${NAME}\")\n    endif()\n\n    if (NOT __DO_NOT_LOWERCASE_TEST_NAME)\n      string(TOLOWER \"${TESTCASE_NAME}\" TESTCASE_NAME)\n    endif()\n    \n    # The following rigmarole is needed to deal with spaces and possible quotes in\n    # command line arguments. The options are passed \"by reference\" as the actual\n    # variable names holding the real options. We then expand these in a way that\n    # preserves any quotes. Note, they have to be in this order for it to work for\n    # all the use cases below.\n\n    set(TEST_COMMAND_OPTIONS ${${__TEST_COMMAND_OPTIONS_PREFIX}${CMD_OPTIONS_VAR}})\n    list(JOIN TEST_COMMAND_OPTIONS \" \" TEST_COMMAND_OPTIONS)\n    separate_arguments(TEST_COMMAND_OPTIONS)\n\n    add_custom_target(\n      ${TESTCASE_NAME}\n      COMMAND\n      ${CUTLASS_TEST_EXECUTION_ENVIRONMENT} $<TARGET_FILE:${TARGET}> ${TEST_COMMAND_OPTIONS}\n      DEPENDS\n      ${TARGET}\n      )\n\n    if (CMD_COUNT GREATER 1)\n      add_dependencies(${NAME} ${TESTCASE_NAME})\n    endif()\n\n    foreach(DEPENDEE ${__DEPENDEES})\n      add_dependencies(${DEPENDEE} ${TESTCASE_NAME})\n    endforeach()\n\n    set(TESTCASE_NAME c${TESTCASE_NAME})\n    string(CONFIGURE \"${_INLINE_PER_TEST_CODE_TEMPLATE}\" _TEST_CODE @ONLY)\n    file(APPEND \"${TEST_GEN_DIR}/CTestTestfile.${TEST_NAME}.cmake\" \"${_TEST_CODE}\")\n    file(APPEND \"${TEST_GEN_DIR}/CTestTestfile.${TEST_NAME}.install.cmake.in\" \"${_TEST_CODE}\")\n\n  endforeach()\n\n  # The following line imports the tests for immediate run via `make test`.\n\n  include(${TEST_GEN_DIR}/CTestTestfile.${TEST_NAME}.cmake)\n\n  set(CUTLASS_CTEST_GENERATED_FILES ${CUTLASS_CTEST_GENERATED_FILES};ctest/${TEST_NAME}/CTestTestfile.${TEST_NAME}.cmake CACHE INTERNAL \"\")\n\n    if (CUTLASS_INSTALL_TESTS)\n\n    file(GENERATE\n      OUTPUT \"${TEST_GEN_DIR}/CTestTestfile.${TEST_NAME}.install.cmake\"\n      INPUT \"${TEST_GEN_DIR}/CTestTestfile.${TEST_NAME}.install.cmake.in\"\n      )\n\n    install(\n      FILES \"${TEST_GEN_DIR}/CTestTestfile.${TEST_NAME}.install.cmake\"\n      DESTINATION ${CUTLASS_TEST_INSTALL_PREFIX}/ctest/${TEST_NAME}\n      RENAME CTestTestfile.${TEST_NAME}.cmake\n      )\n\n    endif()\n\nendfunction()\n\nif (CUTLASS_ENABLE_TOOLS)\n  add_subdirectory(tools)\n  if (CUTLASS_ENABLE_PROFILER)\n    add_dependencies(test_all test_profiler)\n  endif()\nendif()\n\nif (CUTLASS_ENABLE_EXAMPLES)\n  add_subdirectory(examples)\n  add_dependencies(test_all test_examples)\nendif()\n\nif (CUTLASS_ENABLE_TESTS)\n  add_subdirectory(test)\n  if (CUTLASS_ENABLE_GTEST_UNIT_TESTS)\n  add_dependencies(test_all test_unit)\n  endif()\nendif()\n\nif (CUTLASS_INSTALL_TESTS)\n\n  file(MAKE_DIRECTORY \"${CMAKE_BINARY_DIR}/ctest\")\n\n  file(WRITE \"${CMAKE_BINARY_DIR}/ctest/CTestTestfile.cmake\" \"# Generated File\\n\\n\")\n  file(APPEND \"${CMAKE_BINARY_DIR}/ctest/CTestTestfile.cmake\" \"cmake_policy(SET CMP0057 NEW) # Allow IN_LIST for if()\\n\\n\")\n  file(APPEND \"${CMAKE_BINARY_DIR}/ctest/CTestTestfile.cmake\" \"if (NOT DEFINED ENV{CUTLASS_TEST_SETS})\\n\")\n  file(APPEND \"${CMAKE_BINARY_DIR}/ctest/CTestTestfile.cmake\" \"  set(ENV{CUTLASS_TEST_SETS} ${CUTLASS_DEFAULT_ACTIVE_TEST_SETS})\\n\")\n  file(APPEND \"${CMAKE_BINARY_DIR}/ctest/CTestTestfile.cmake\" \"endif()\\n\\n\")\n\n  foreach(GENERATED_FILE ${CUTLASS_CTEST_GENERATED_FILES})\n    file(APPEND \"${CMAKE_BINARY_DIR}/ctest/CTestTestfile.cmake\" \"include(${GENERATED_FILE})\\n\")\n  endforeach()\n\n  install(\n    FILES \"${CMAKE_BINARY_DIR}/ctest/CTestTestfile.cmake\"\n    DESTINATION \"${CUTLASS_TEST_INSTALL_PREFIX}/\"\n    )\n\nendif()\n\n################################################################################\n\ninclude(CMakePackageConfigHelpers)\n\nwrite_basic_package_version_file(\n  ${CMAKE_CURRENT_BINARY_DIR}/NvidiaCutlassConfigVersion.cmake\n  COMPATIBILITY AnyNewerVersion)\n\nconfigure_file(\n  ${CMAKE_CURRENT_SOURCE_DIR}/cmake/NvidiaCutlassConfig.cmake.in\n  ${CMAKE_CURRENT_BINARY_DIR}/NvidiaCutlassConfig.cmake\n  @ONLY\n  )\n\ninstall(\n  FILES\n    ${CMAKE_CURRENT_BINARY_DIR}/NvidiaCutlassConfig.cmake\n    ${CMAKE_CURRENT_BINARY_DIR}/NvidiaCutlassConfigVersion.cmake\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/NvidiaCutlass/\n  )\n\ninstall(\n  EXPORT NvidiaCutlass\n  NAMESPACE nvidia::cutlass::\n  DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/NvidiaCutlass/\n  FILE NvidiaCutlassTargets.cmake\n  )\n\n################################################################################\n\ninclude(${CMAKE_CURRENT_SOURCE_DIR}/cmake/NvidiaCutlassPackageConfig.cmake)\n\n"
        },
        {
          "name": "CONTRIBUTORS.md",
          "type": "blob",
          "size": 1.587890625,
          "content": "![ALT](./media/images/gemm-hierarchy-with-epilogue-no-labels.png \"CUTLASS\")\n\n[README](./README.md#documentation) > **Contributors**\n\n# CUTLASS Developers and Contributors\n\nThis is the official list of CUTLASS developers and contributors.\n\n## DEVELOPERS\nVijay Thakkar<br />\nPradeep Ramani<br />\nCris Cecka<br />\nAniket Shivam<br />\nJack Kosaian<br />\nMark Hoemmen<br />\nRichard Cai<br />\nHonghao Lu<br />\nEthan Yan<br />\nHaicheng Wu<br />\nAndrew Kerr<br />\nDustyn Blasig<br />\nFengqi Qiao<br />\nDuane Merrill<br />\nYujia Zhai<br />\nRawn Henry<br />\nSergey Klevtsov<br />\nShang Zhang<br />\nPiotr Majcher<br />\nPaul Springer<br />\nMarkus Hohnerbach<br />\nJin Wang<br />\nAditya Atluri<br />\n\n## CuTe\nCris Cecka<br />\nVijay Thakkar<br />\n\n## CUTLASS Product Manager\nMatthew Nicely<br />\n\n## Former CUTLASS Developers\nManish Gupta<br />\nNaila Farooqui<br />\nDavid Tanner<br />\nManikandan Ananth<br />\nZhaodong Chen<br />\nChinmay Talegaonkar<br />\n\n## CONTRIBUTORS\nTimothy Costa<br />\nJulien Demouth<br />\nBrian Fahs<br />\nMichael Garland<br />\nMichael Goldfarb<br />\nMostafa Hagog<br />\nFei Hu<br />\nAlan Kaatz<br />\nTina Li<br />\nTimmy Liu<br />\nWei Liu<br />\nTim Martin<br />\nDuane Merrill<br />\nKevin Siu<br />\nMarkus Tavenrath<br />\nJohn Tran<br />\nVicki Wang<br />\nJunkai Wu<br />\nFung Xie<br />\nAlbert Xu<br />\nYang Xu<br />\nJack Yang<br />\nScott Yokim<br />\nXiuxia Zhang<br />\nNick Zhao<br />\n\n## ACKNOWLEDGEMENTS\n\nGirish Bharambe<br />\nLuke Durant<br />\nCarter Edwards<br />\nOlivier Giroux<br />\nStephen Jones<br />\nRishkul Kulkarni<br />\nBryce Lelbach<br />\nJoel McCormack<br />\nKyrylo Perelygin<br />\nSean Treichler<br />\n"
        },
        {
          "name": "CUDA.cmake",
          "type": "blob",
          "size": 11.1865234375,
          "content": "# Copyright (c) 2017 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this\n# list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n# this list of conditions and the following disclaimer in the documentation\n# and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nif (CUDA_COMPILER MATCHES \"[Cc]lang\")\n  message(WARNING \"CUDA_COMPILER flag is deprecated, set CMAKE_CUDA_COMPILER to desired compiler executable.\")\n  set(__CLANG_DEVICE_COMPILATION_REQUESTED ON)\nelseif(CUDA_COMPILER)\n  message(WARNING \"Deprecated flag CUDA_COMPILER used with unknown argument ${CUDA_COMPILER}, ignoring.\")\nendif()\n\nif (__CLANG_DEVICE_COMPILATION_REQUESTED AND NOT DEFINED CMAKE_CUDA_COMPILER)\n  set(CMAKE_CUDA_COMPILER clang++) # We will let the system find Clang or error out\nendif()\n\nenable_language(CUDA)\nfind_package(CUDAToolkit REQUIRED)\n\nif(NOT CUDA_VERSION)\n  # For backward compatibility with older CMake code.\n  set(CUDA_VERSION ${CUDAToolkit_VERSION})\n  set(CUDA_VERSION_MAJOR ${CUDAToolkit_VERSION_MAJOR})\n  set(CUDA_VERSION_MINOR ${CUDAToolkit_VERSION_MINOR})\nendif()\nif(NOT CUDA_TOOLKIT_ROOT_DIR)\n  # In some scenarios, such as clang device compilation, the toolkit root may not be set, so we \n  # force it here to the nvcc we found via the CUDAToolkit package.\n  get_filename_component(CUDA_TOOLKIT_ROOT_DIR \"${CUDAToolkit_NVCC_EXECUTABLE}/../..\" ABSOLUTE)\nendif()\n\nif (CMAKE_CUDA_COMPILER_ID MATCHES \"(nvcc|[Nn][Vv][Ii][Dd][Ii][Aa])\")\n  set(CUTLASS_NVCC_DEVICE_COMPILE ON CACHE BOOL \"Using nvcc tools for device compilation\")\nelseif (CMAKE_CUDA_COMPILER_ID MATCHES \"[Cc]lang\")\n  set(CUTLASS_CLANG_DEVICE_COMPILE ON CACHE BOOL \"Using Clang tools for device compilation\")\nelse()\n  message(FATAL_ERROR \"Uknown device-side compiler ${CMAKE_CUDA_COMPILER_ID} found. Set CMAKE_CUDA_COMPILER to either nvcc or clang++.\")\nendif()\n\nif (CUTLASS_CLANG_DEVICE_COMPILE AND CMAKE_VERSION VERSION_LESS_EQUAL \"3.30\")\n  message(FATAL_ERROR \"Clang device compilation for CUTLASS requires CMake 3.30 or higher.\")\nendif()\n\nif (CUDA_VERSION VERSION_LESS 9.2)\n  message(FATAL_ERROR \"CUDA 9.2+ required, found ${CUDA_VERSION}.\")\nendif()\n\nfind_library(\n  CUDART_LIBRARY cudart\n  PATHS\n  ${CUDA_TOOLKIT_ROOT_DIR}\n  PATH_SUFFIXES\n  lib/x86_64-linux-gnu\n  lib/x64\n  lib64\n  lib\n  NO_DEFAULT_PATH\n  # We aren't going to search any system paths. We want to find the runtime \n  # in the CUDA toolkit we're building against.\n  )\n\nif(NOT TARGET cudart AND CUDART_LIBRARY)\n\n  message(STATUS \"CUDART: ${CUDART_LIBRARY}\")\n\n  if(WIN32)\n    add_library(cudart STATIC IMPORTED GLOBAL)\n    # Even though we're linking against a .dll, in Windows you statically link against\n    # the .lib file found under lib/x64. The .dll will be loaded at runtime automatically\n    # from the PATH search.\n  else()\n    add_library(cudart SHARED IMPORTED GLOBAL)\n  endif()  \n\n  add_library(nvidia::cudart ALIAS cudart)\n  \n  set_property(\n    TARGET cudart\n    PROPERTY IMPORTED_LOCATION\n    ${CUDART_LIBRARY}\n    )\n\nelseif(TARGET cudart)\n\n  message(STATUS \"CUDART: Already Found\")\n\nelse()\n\n  message(STATUS \"CUDART: Not Found\")\n\nendif()\n\nfind_library(\n  CUDA_DRIVER_LIBRARY cuda\n  PATHS\n  ${CUDA_TOOLKIT_ROOT_DIR}\n  PATH_SUFFIXES\n  lib/x86_64-linux-gnu\n  lib/x64\n  lib64\n  lib\n  lib64/stubs\n  lib/stubs\n  NO_DEFAULT_PATH\n  # We aren't going to search any system paths. We want to find the runtime \n  # in the CUDA toolkit we're building against.\n  )\n\nif(NOT TARGET cuda_driver AND CUDA_DRIVER_LIBRARY)\n\n  message(STATUS \"CUDA Driver: ${CUDA_DRIVER_LIBRARY}\")\n\n  if(WIN32)\n    add_library(cuda_driver STATIC IMPORTED GLOBAL)\n    # Even though we're linking against a .dll, in Windows you statically link against\n    # the .lib file found under lib/x64. The .dll will be loaded at runtime automatically\n    # from the PATH search.\n  else()\n    add_library(cuda_driver SHARED IMPORTED GLOBAL)\n  endif()  \n\n  add_library(nvidia::cuda_driver ALIAS cuda_driver)\n  \n  set_property(\n    TARGET cuda_driver\n    PROPERTY IMPORTED_LOCATION\n    ${CUDA_DRIVER_LIBRARY}\n    )\n\nelseif(TARGET cuda_driver)\n\n  message(STATUS \"CUDA Driver: Already Found\")\n\nelse()\n\n  message(STATUS \"CUDA Driver: Not Found\")\n\nendif()\n\nfind_library(\n  NVRTC_LIBRARY nvrtc\n  PATHS\n  ${CUDA_TOOLKIT_ROOT_DIR}\n  PATH_SUFFIXES\n  lib/x64\n  lib64\n  lib\n  NO_DEFAULT_PATH\n  # We aren't going to search any system paths. We want to find the runtime \n  # in the CUDA toolkit we're building against.\n  )\n\nif(NOT TARGET nvrtc AND NVRTC_LIBRARY)\n\n  message(STATUS \"NVRTC: ${NVRTC_LIBRARY}\")\n\n  if(WIN32)\n    add_library(nvrtc STATIC IMPORTED GLOBAL)\n    # Even though we're linking against a .dll, in Windows you statically link against\n    # the .lib file found under lib/x64. The .dll will be loaded at runtime automatically\n    # from the PATH search.\n  else()\n    add_library(nvrtc SHARED IMPORTED GLOBAL)\n  endif()  \n  \n  add_library(nvidia::nvrtc ALIAS nvrtc)\n  \n  set_property(\n    TARGET nvrtc\n    PROPERTY IMPORTED_LOCATION\n    ${NVRTC_LIBRARY}\n    )\n\nelseif(TARGET nvrtc)\n\n  message(STATUS \"NVRTC: Already Found\")\n\nelse()\n\n  message(STATUS \"NVRTC: Not Found\")\n\nendif()\n\ninclude_directories(SYSTEM ${CUDA_INCLUDE_DIRS})\n# Some platforms (e.g. Visual Studio) don't add the CUDA include directories to the system include\n# paths by default, so we add it explicitly here.\n\nif (MSVC OR CUTLASS_LIBRARY_KERNELS MATCHES \"all\")\n  set(CUTLASS_UNITY_BUILD_ENABLED_INIT ON)\nelse()\n  set(CUTLASS_UNITY_BUILD_ENABLED_INIT OFF)\nendif()\n\nset(CUTLASS_UNITY_BUILD_ENABLED ${CUTLASS_UNITY_BUILD_ENABLED_INIT} CACHE BOOL \"Enable combined source compilation\")\n\nif (MSVC)\n  set(CUTLASS_UNITY_BUILD_BATCH_SIZE_INIT 8)\nelse()\n  set(CUTLASS_UNITY_BUILD_BATCH_SIZE_INIT 16)\nendif()\n\nset(CUTLASS_UNITY_BUILD_BATCH_SIZE ${CUTLASS_UNITY_BUILD_BATCH_SIZE_INIT} CACHE STRING \"Batch size for unified source files\")\n\nfunction(cutlass_unify_source_files TARGET_ARGS_VAR)\n\n  set(options)\n  set(oneValueArgs BATCH_SOURCES BATCH_SIZE)\n  set(multiValueArgs)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  if (NOT DEFINED TARGET_ARGS_VAR)\n    message(FATAL_ERROR \"TARGET_ARGS_VAR parameter is required\")\n  endif()\n\n  if (NOT DEFINED __BATCH_SOURCES)\n    set(__BATCH_SOURCES ON)\n  endif()\n\n  if (__BATCH_SOURCES AND NOT DEFINED __BATCH_SIZE)\n    set(__BATCH_SIZE ${CUTLASS_UNITY_BUILD_BATCH_SIZE})\n  endif()\n\n  if (CUTLASS_UNITY_BUILD_ENABLED AND __BATCH_SOURCES AND __BATCH_SIZE GREATER 1)\n\n    set(CUDA_FILE_ARGS)\n    set(TARGET_SOURCE_ARGS)\n    \n    foreach(ARG ${__UNPARSED_ARGUMENTS})\n      if(${ARG} MATCHES \".*\\.cu$\")\n        list(APPEND CUDA_FILE_ARGS ${ARG})\n      else()\n        list(APPEND TARGET_SOURCE_ARGS ${ARG})\n      endif()\n    endforeach()\n    \n    list(LENGTH CUDA_FILE_ARGS NUM_CUDA_FILE_ARGS)\n    while(NUM_CUDA_FILE_ARGS GREATER 0)\n      list(SUBLIST CUDA_FILE_ARGS 0 ${__BATCH_SIZE} CUDA_FILE_BATCH)\n      string(SHA256 CUDA_FILE_BATCH_HASH \"${CUDA_FILE_BATCH}\")\n      string(SUBSTRING ${CUDA_FILE_BATCH_HASH} 0 12 CUDA_FILE_BATCH_HASH)\n      set(BATCH_FILE ${CMAKE_CURRENT_BINARY_DIR}/${NAME}.unity.${CUDA_FILE_BATCH_HASH}.cu)\n      message(STATUS \"Generating ${BATCH_FILE}\")\n      file(WRITE ${BATCH_FILE} \"// Unity File - Auto Generated!\\n\")\n      foreach(CUDA_FILE ${CUDA_FILE_BATCH})\n        get_filename_component(CUDA_FILE_ABS_PATH ${CUDA_FILE} ABSOLUTE)\n        file(APPEND ${BATCH_FILE} \"#include \\\"${CUDA_FILE_ABS_PATH}\\\"\\n\")\n      endforeach()\n      list(APPEND TARGET_SOURCE_ARGS ${BATCH_FILE})\n      if (NUM_CUDA_FILE_ARGS LESS_EQUAL __BATCH_SIZE)\n        break()\n      endif()\n      list(SUBLIST CUDA_FILE_ARGS ${__BATCH_SIZE} -1 CUDA_FILE_ARGS)\n      list(LENGTH CUDA_FILE_ARGS NUM_CUDA_FILE_ARGS)\n    endwhile()\n\n  else()\n\n    set(TARGET_SOURCE_ARGS ${__UNPARSED_ARGUMENTS})\n\n  endif()\n\n  set(${TARGET_ARGS_VAR} ${TARGET_SOURCE_ARGS} PARENT_SCOPE)\n\nendfunction()\nfunction(cutlass_add_library NAME)\n\n  set(options SKIP_GENCODE_FLAGS)\n  set(oneValueArgs EXPORT_NAME)\n  set(multiValueArgs)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  cutlass_unify_source_files(TARGET_SOURCE_ARGS ${__UNPARSED_ARGUMENTS})\n  \n  add_library(${NAME} ${TARGET_SOURCE_ARGS} \"\")\n\n  cutlass_apply_standard_compile_options(${NAME})\n\n  if (NOT __SKIP_GENCODE_FLAGS)\n    cutlass_apply_cuda_gencode_flags(${NAME})\n  endif()\n\n  target_compile_features(\n   ${NAME}\n   INTERFACE\n   cxx_std_11\n   )\n\n  get_target_property(TARGET_TYPE ${NAME} TYPE)\n\n  if (TARGET_TYPE MATCHES \"SHARED\")\n    set_target_properties(${NAME} PROPERTIES CUDA_RUNTIME_LIBRARY Shared)\n  elseif(TARGET_TYPE MATCHES \"STATIC\")\n    set_target_properties(${NAME} PROPERTIES CUDA_RUNTIME_LIBRARY Static)\n  endif()\n\n  if(__EXPORT_NAME)\n    add_library(nvidia::cutlass::${__EXPORT_NAME} ALIAS ${NAME})\n    set_target_properties(${NAME} PROPERTIES EXPORT_NAME ${__EXPORT_NAME})\n  endif()\n\nendfunction()\n\nfunction(cutlass_add_executable NAME)\n\n  set(options)\n  set(oneValueArgs CUDA_RUNTIME_LIBRARY)\n  set(multiValueArgs)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  if (NOT DEFINED __CUDA_RUNTIME_LIBRARY)\n    set(__CUDA_RUNTIME_LIBRARY Shared)\n  endif()\n\n  set(__CUDA_RUNTIME_LIBRARY_ALLOWED None Shared Static)\n  if (NOT __CUDA_RUNTIME_LIBRARY IN_LIST __CUDA_RUNTIME_LIBRARY_ALLOWED)\n    message(FATAL_ERROR \"CUDA_RUNTIME_LIBRARY value '${__CUDA_RUNTIME_LIBRARY}' is not in allowed list of '${__CUDA_RUNTIME_LIBRARY_ALLOWED}'\")\n  endif()\n\n  cutlass_unify_source_files(TARGET_SOURCE_ARGS ${__UNPARSED_ARGUMENTS})\n\n  add_executable(${NAME} ${TARGET_SOURCE_ARGS})\n\n  cutlass_apply_standard_compile_options(${NAME})\n  cutlass_apply_cuda_gencode_flags(${NAME})\n\n  target_compile_features(\n   ${NAME}\n   INTERFACE\n   cxx_std_11\n   )\n\n  set_target_properties(${NAME} PROPERTIES CUDA_RUNTIME_LIBRARY ${__CUDA_RUNTIME_LIBRARY})\n\nendfunction()\n\nfunction(cutlass_target_sources NAME)\n\n  set(options)\n  set(oneValueArgs)\n  set(multiValueArgs)\n  cmake_parse_arguments(_ \"${options}\" \"${oneValueArgs}\" \"${multiValueArgs}\" ${ARGN})\n\n  cutlass_unify_source_files(TARGET_SOURCE_ARGS ${__UNPARSED_ARGUMENTS})\n  target_sources(${NAME} ${TARGET_SOURCE_ARGS})\n\nendfunction()\n"
        },
        {
          "name": "Doxyfile",
          "type": "blob",
          "size": 97.2177734375,
          "content": "# Doxyfile 1.8.5\n\n# This file describes the settings to be used by the documentation system\n# doxygen (www.doxygen.org) for a project.\n#\n# All text after a double hash (##) is considered a comment and is placed in\n# front of the TAG it is preceding.\n#\n# All text after a single hash (#) is considered a comment and will be ignored.\n# The format is:\n# TAG = value [value, ...]\n# For lists, items can also be appended using:\n# TAG += value [value, ...]\n# Values that contain spaces should be placed between quotes (\\\" \\\").\n\n#---------------------------------------------------------------------------\n# Project related configuration options\n#---------------------------------------------------------------------------\n\n# This tag specifies the encoding used for all characters in the config file\n# that follow. The default is UTF-8 which is also the encoding used for all text\n# before the first occurrence of this tag. Doxygen uses libiconv (or the iconv\n# built into libc) for the transcoding. See http://www.gnu.org/software/libiconv\n# for the list of possible encodings.\n# The default value is: UTF-8.\n\nDOXYFILE_ENCODING      = UTF-8\n\n# The PROJECT_NAME tag is a single word (or a sequence of words surrounded by\n# double-quotes, unless you are using Doxywizard) that should identify the\n# project for which the documentation is generated. This name is used in the\n# title of most generated pages and in a few other places.\n# The default value is: My Project.\n\nPROJECT_NAME           = \"CUTLASS\"\n\n# The PROJECT_NUMBER tag can be used to enter a project or revision number. This\n# could be handy for archiving the generated documentation or if some version\n# control system is used.\n\nPROJECT_NUMBER         =\n\n# Using the PROJECT_BRIEF tag one can provide an optional one line description\n# for a project that appears at the top of each page and should give viewer a\n# quick idea about the purpose of the project. Keep the description short.\n\nPROJECT_BRIEF          = \"CUDA Templates for Linear Algebra Subroutines and Solvers\"\n\n# With the PROJECT_LOGO tag one can specify an logo or icon that is included in\n# the documentation. The maximum height of the logo should not exceed 55 pixels\n# and the maximum width should not exceed 200 pixels. Doxygen will copy the logo\n# to the output directory.\n\nPROJECT_LOGO           = media/images/cutlass-logo-small.png\n\n# The OUTPUT_DIRECTORY tag is used to specify the (relative or absolute) path\n# into which the generated documentation will be written. If a relative path is\n# entered, it will be relative to the location where doxygen was started. If\n# left blank the current directory will be used.\n\nOUTPUT_DIRECTORY       = doxygen\n\n# If the CREATE_SUBDIRS tag is set to YES, then doxygen will create 4096 sub-\n# directories (in 2 levels) under the output directory of each output format and\n# will distribute the generated files over these directories. Enabling this\n# option can be useful when feeding doxygen a huge amount of source files, where\n# putting all generated files in the same directory would otherwise causes\n# performance problems for the file system.\n# The default value is: NO.\n\nCREATE_SUBDIRS         = NO\n\n# The OUTPUT_LANGUAGE tag is used to specify the language in which all\n# documentation generated by doxygen is written. Doxygen will use this\n# information to generate all constant output in the proper language.\n# Possible values are: Afrikaans, Arabic, Brazilian, Catalan, Chinese, Chinese-\n# Traditional, Croatian, Czech, Danish, Dutch, English, Esperanto, Farsi,\n# Finnish, French, German, Greek, Hungarian, Italian, Japanese, Japanese-en,\n# Korean, Korean-en, Latvian, Norwegian, Macedonian, Persian, Polish,\n# Portuguese, Romanian, Russian, Serbian, Slovak, Slovene, Spanish, Swedish,\n# Turkish, Ukrainian and Vietnamese.\n# The default value is: English.\n\nOUTPUT_LANGUAGE        = English\n\n# If the BRIEF_MEMBER_DESC tag is set to YES doxygen will include brief member\n# descriptions after the members that are listed in the file and class\n# documentation (similar to Javadoc). Set to NO to disable this.\n# The default value is: YES.\n\nBRIEF_MEMBER_DESC      = YES\n\n# If the REPEAT_BRIEF tag is set to YES doxygen will prepend the brief\n# description of a member or function before the detailed description\n#\n# Note: If both HIDE_UNDOC_MEMBERS and BRIEF_MEMBER_DESC are set to NO, the\n# brief descriptions will be completely suppressed.\n# The default value is: YES.\n\nREPEAT_BRIEF           = NO\n\n# This tag implements a quasi-intelligent brief description abbreviator that is\n# used to form the text in various listings. Each string in this list, if found\n# as the leading text of the brief description, will be stripped from the text\n# and the result, after processing the whole list, is used as the annotated\n# text. Otherwise, the brief description is used as-is. If left blank, the\n# following values are used ($name is automatically replaced with the name of\n# the entity):The $name class, The $name widget, The $name file, is, provides,\n# specifies, contains, represents, a, an and the.\n\nABBREVIATE_BRIEF       =\n\n# If the ALWAYS_DETAILED_SEC and REPEAT_BRIEF tags are both set to YES then\n# doxygen will generate a detailed section even if there is only a brief\n# description.\n# The default value is: NO.\n\nALWAYS_DETAILED_SEC    = NO\n\n# If the INLINE_INHERITED_MEMB tag is set to YES, doxygen will show all\n# inherited members of a class in the documentation of that class as if those\n# members were ordinary class members. Constructors, destructors and assignment\n# operators of the base classes will not be shown.\n# The default value is: NO.\n\nINLINE_INHERITED_MEMB  = NO\n\n# If the FULL_PATH_NAMES tag is set to YES doxygen will prepend the full path\n# before files name in the file list and in the header files. If set to NO the\n# shortest path that makes the file name unique will be used\n# The default value is: YES.\n\nFULL_PATH_NAMES        = NO\n\n# The STRIP_FROM_PATH tag can be used to strip a user-defined part of the path.\n# Stripping is only done if one of the specified strings matches the left-hand\n# part of the path. The tag can be used to show relative paths in the file list.\n# If left blank the directory from which doxygen is run is used as the path to\n# strip.\n#\n# Note that you can specify absolute paths here, but also relative paths, which\n# will be relative from the directory where doxygen is started.\n# This tag requires that the tag FULL_PATH_NAMES is set to YES.\n\nSTRIP_FROM_PATH        =\n\n# The STRIP_FROM_INC_PATH tag can be used to strip a user-defined part of the\n# path mentioned in the documentation of a class, which tells the reader which\n# header file to include in order to use a class. If left blank only the name of\n# the header file containing the class definition is used. Otherwise one should\n# specify the list of include paths that are normally passed to the compiler\n# using the -I flag.\n\nSTRIP_FROM_INC_PATH    =\n\n# If the SHORT_NAMES tag is set to YES, doxygen will generate much shorter (but\n# less readable) file names. This can be useful is your file systems doesn't\n# support long names like on DOS, Mac, or CD-ROM.\n# The default value is: NO.\n\nSHORT_NAMES            = NO\n\n# If the JAVADOC_AUTOBRIEF tag is set to YES then doxygen will interpret the\n# first line (until the first dot) of a Javadoc-style comment as the brief\n# description. If set to NO, the Javadoc-style will behave just like regular Qt-\n# style comments (thus requiring an explicit @brief command for a brief\n# description.)\n# The default value is: NO.\n\nJAVADOC_AUTOBRIEF      = NO\n\n# If the QT_AUTOBRIEF tag is set to YES then doxygen will interpret the first\n# line (until the first dot) of a Qt-style comment as the brief description. If\n# set to NO, the Qt-style will behave just like regular Qt-style comments (thus\n# requiring an explicit \\brief command for a brief description.)\n# The default value is: NO.\n\nQT_AUTOBRIEF           = NO\n\n# The MULTILINE_CPP_IS_BRIEF tag can be set to YES to make doxygen treat a\n# multi-line C++ special comment block (i.e. a block of //! or /// comments) as\n# a brief description. This used to be the default behavior. The new default is\n# to treat a multi-line C++ comment block as a detailed description. Set this\n# tag to YES if you prefer the old behavior instead.\n#\n# Note that setting this tag to YES also means that rational rose comments are\n# not recognized any more.\n# The default value is: NO.\n\nMULTILINE_CPP_IS_BRIEF = NO\n\n# If the INHERIT_DOCS tag is set to YES then an undocumented member inherits the\n# documentation from any documented member that it re-implements.\n# The default value is: YES.\n\nINHERIT_DOCS           = YES\n\n# If the SEPARATE_MEMBER_PAGES tag is set to YES, then doxygen will produce a\n# new page for each member. If set to NO, the documentation of a member will be\n# part of the file/class/namespace that contains it.\n# The default value is: NO.\n\nSEPARATE_MEMBER_PAGES  = NO\n\n# The TAB_SIZE tag can be used to set the number of spaces in a tab. Doxygen\n# uses this value to replace tabs by spaces in code fragments.\n# Minimum value: 1, maximum value: 16, default value: 4.\n\nTAB_SIZE               = 2\n\n# This tag can be used to specify a number of aliases that act as commands in\n# the documentation. An alias has the form:\n# name=value\n# For example adding\n# \"sideeffect=@par Side Effects:\\n\"\n# will allow you to put the command \\sideeffect (or @sideeffect) in the\n# documentation, which will result in a user-defined paragraph with heading\n# \"Side Effects:\". You can put \\n's in the value part of an alias to insert\n# newlines.\n\n#ALIASES += \"concept{1}=@ingroup \\1\\n@par Implemented concepts:\\n@ref \\1\"\nALIASES += \"concept{1}=@ingroup \\1\"\n\n# This tag can be used to specify a number of word-keyword mappings (TCL only).\n# A mapping has the form \"name=value\". For example adding \"class=itcl::class\"\n# will allow you to use the command class in the itcl::class meaning.\n\nTCL_SUBST              =\n\n# Set the OPTIMIZE_OUTPUT_FOR_C tag to YES if your project consists of C sources\n# only. Doxygen will then generate output that is more tailored for C. For\n# instance, some of the names that are used will be different. The list of all\n# members will be omitted, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_FOR_C  = NO\n\n# Set the OPTIMIZE_OUTPUT_JAVA tag to YES if your project consists of Java or\n# Python sources only. Doxygen will then generate output that is more tailored\n# for that language. For instance, namespaces will be presented as packages,\n# qualified scopes will look different, etc.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_JAVA   = NO\n\n# Set the OPTIMIZE_FOR_FORTRAN tag to YES if your project consists of Fortran\n# sources. Doxygen will then generate output that is tailored for Fortran.\n# The default value is: NO.\n\nOPTIMIZE_FOR_FORTRAN   = NO\n\n# Set the OPTIMIZE_OUTPUT_VHDL tag to YES if your project consists of VHDL\n# sources. Doxygen will then generate output that is tailored for VHDL.\n# The default value is: NO.\n\nOPTIMIZE_OUTPUT_VHDL   = NO\n\n# Doxygen selects the parser to use depending on the extension of the files it\n# parses. With this tag you can assign which parser to use for a given\n# extension. Doxygen has a built-in mapping, but you can override or extend it\n# using this tag. The format is ext=language, where ext is a file extension, and\n# language is one of the parsers supported by doxygen: IDL, Java, Javascript,\n# C#, C, C++, D, PHP, Objective-C, Python, Fortran, VHDL. For instance to make\n# doxygen treat .inc files as Fortran files (default is PHP), and .f files as C\n# (default is Fortran), use: inc=Fortran f=C.\n#\n# Note For files without extension you can use no_extension as a placeholder.\n#\n# Note that for custom extensions you also need to set FILE_PATTERNS otherwise\n# the files are not read by doxygen.\n\nEXTENSION_MAPPING      = cu=C++\n\n# If the MARKDOWN_SUPPORT tag is enabled then doxygen pre-processes all comments\n# according to the Markdown format, which allows for more readable\n# documentation. See http://daringfireball.net/projects/markdown/ for details.\n# The output of markdown processing is further processed by doxygen, so you can\n# mix doxygen, HTML, and XML commands with Markdown formatting. Disable only in\n# case of backward compatibilities issues.\n# The default value is: YES.\n\nMARKDOWN_SUPPORT       = YES\n\n# When enabled doxygen tries to link words that correspond to documented\n# classes, or namespaces to their corresponding documentation. Such a link can\n# be prevented in individual cases by by putting a % sign in front of the word\n# or globally by setting AUTOLINK_SUPPORT to NO.\n# The default value is: YES.\n\nAUTOLINK_SUPPORT       = YES\n\n# If you use STL classes (i.e. std::string, std::vector, etc.) but do not want\n# to include (a tag file for) the STL sources as input, then you should set this\n# tag to YES in order to let doxygen match functions declarations and\n# definitions whose arguments contain STL classes (e.g. func(std::string);\n# versus func(std::string) {}). This also make the inheritance and collaboration\n# diagrams that involve STL classes more complete and accurate.\n# The default value is: NO.\n\nBUILTIN_STL_SUPPORT    = YES\n\n# If you use Microsoft's C++/CLI language, you should set this option to YES to\n# enable parsing support.\n# The default value is: NO.\n\nCPP_CLI_SUPPORT        = NO\n\n# Set the SIP_SUPPORT tag to YES if your project consists of sip (see:\n# http://www.riverbankcomputing.co.uk/software/sip/intro) sources only. Doxygen\n# will parse them like normal C++ but will assume all classes use public instead\n# of private inheritance when no explicit protection keyword is present.\n# The default value is: NO.\n\nSIP_SUPPORT            = NO\n\n# For Microsoft's IDL there are propget and propput attributes to indicate\n# getter and setter methods for a property. Setting this option to YES will make\n# doxygen to replace the get and set methods by a property in the documentation.\n# This will only work if the methods are indeed getting or setting a simple\n# type. If this is not the case, or you want to show the methods anyway, you\n# should set this option to NO.\n# The default value is: YES.\n\nIDL_PROPERTY_SUPPORT   = YES\n\n# If member grouping is used in the documentation and the DISTRIBUTE_GROUP_DOC\n# tag is set to YES, then doxygen will reuse the documentation of the first\n# member in the group (if any) for the other members of the group. By default\n# all members of a group must be documented explicitly.\n# The default value is: NO.\n\nDISTRIBUTE_GROUP_DOC   = NO\n\n# Set the SUBGROUPING tag to YES to allow class member groups of the same type\n# (for instance a group of public functions) to be put as a subgroup of that\n# type (e.g. under the Public Functions section). Set it to NO to prevent\n# subgrouping. Alternatively, this can be done per class using the\n# \\nosubgrouping command.\n# The default value is: YES.\n\nSUBGROUPING            = YES\n\n# When the INLINE_GROUPED_CLASSES tag is set to YES, classes, structs and unions\n# are shown inside the group in which they are included (e.g. using \\ingroup)\n# instead of on a separate page (for HTML and Man pages) or section (for LaTeX\n# and RTF).\n#\n# Note that this feature does not work in combination with\n# SEPARATE_MEMBER_PAGES.\n# The default value is: NO.\n\nINLINE_GROUPED_CLASSES = NO\n\n# When the INLINE_SIMPLE_STRUCTS tag is set to YES, structs, classes, and unions\n# with only public data fields or simple typedef fields will be shown inline in\n# the documentation of the scope in which they are defined (i.e. file,\n# namespace, or group documentation), provided this scope is documented. If set\n# to NO, structs, classes, and unions are shown on a separate page (for HTML and\n# Man pages) or section (for LaTeX and RTF).\n# The default value is: NO.\n\nINLINE_SIMPLE_STRUCTS  = NO\n\n# When TYPEDEF_HIDES_STRUCT tag is enabled, a typedef of a struct, union, or\n# enum is documented as struct, union, or enum with the name of the typedef. So\n# typedef struct TypeS {} TypeT, will appear in the documentation as a struct\n# with name TypeT. When disabled the typedef will appear as a member of a file,\n# namespace, or class. And the struct will be named TypeS. This can typically be\n# useful for C code in case the coding convention dictates that all compound\n# types are typedef'ed and only the typedef is referenced, never the tag name.\n# The default value is: NO.\n\nTYPEDEF_HIDES_STRUCT   = NO\n\n# The size of the symbol lookup cache can be set using LOOKUP_CACHE_SIZE. This\n# cache is used to resolve symbols given their name and scope. Since this can be\n# an expensive process and often the same symbol appears multiple times in the\n# code, doxygen keeps a cache of pre-resolved symbols. If the cache is too small\n# doxygen will become slower. If the cache is too large, memory is wasted. The\n# cache size is given by this formula: 2^(16+LOOKUP_CACHE_SIZE). The valid range\n# is 0..9, the default is 0, corresponding to a cache size of 2^16=65536\n# symbols. At the end of a run doxygen will report the cache usage and suggest\n# the optimal cache size from a speed point of view.\n# Minimum value: 0, maximum value: 9, default value: 0.\n\nLOOKUP_CACHE_SIZE      = 0\n\n#---------------------------------------------------------------------------\n# Build related configuration options\n#---------------------------------------------------------------------------\n\n# If the EXTRACT_ALL tag is set to YES doxygen will assume all entities in\n# documentation are documented, even if no documentation was available. Private\n# class members and static file members will be hidden unless the\n# EXTRACT_PRIVATE respectively EXTRACT_STATIC tags are set to YES.\n# Note: This will also disable the warnings about undocumented members that are\n# normally produced when WARNINGS is set to YES.\n# The default value is: NO.\n\nEXTRACT_ALL            = YES\n\n# If the EXTRACT_PRIVATE tag is set to YES all private members of a class will\n# be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PRIVATE        = NO\n\n# If the EXTRACT_PACKAGE tag is set to YES all members with package or internal\n# scope will be included in the documentation.\n# The default value is: NO.\n\nEXTRACT_PACKAGE        = NO\n\n# If the EXTRACT_STATIC tag is set to YES all static members of a file will be\n# included in the documentation.\n# The default value is: NO.\n\nEXTRACT_STATIC         = NO\n\n# If the EXTRACT_LOCAL_CLASSES tag is set to YES classes (and structs) defined\n# locally in source files will be included in the documentation. If set to NO\n# only classes defined in header files are included. Does not have any effect\n# for Java sources.\n# The default value is: YES.\n\nEXTRACT_LOCAL_CLASSES  = YES\n\n# This flag is only useful for Objective-C code. When set to YES local methods,\n# which are defined in the implementation section but not in the interface are\n# included in the documentation. If set to NO only methods in the interface are\n# included.\n# The default value is: NO.\n\nEXTRACT_LOCAL_METHODS  = NO\n\n# If this flag is set to YES, the members of anonymous namespaces will be\n# extracted and appear in the documentation as a namespace called\n# 'anonymous_namespace{file}', where file will be replaced with the base name of\n# the file that contains the anonymous namespace. By default anonymous namespace\n# are hidden.\n# The default value is: NO.\n\nEXTRACT_ANON_NSPACES   = NO\n\n# If the HIDE_UNDOC_MEMBERS tag is set to YES, doxygen will hide all\n# undocumented members inside documented classes or files. If set to NO these\n# members will be included in the various overviews, but no documentation\n# section is generated. This option has no effect if EXTRACT_ALL is enabled.\n# The default value is: NO.\n\nHIDE_UNDOC_MEMBERS     = NO\n\n# If the HIDE_UNDOC_CLASSES tag is set to YES, doxygen will hide all\n# undocumented classes that are normally visible in the class hierarchy. If set\n# to NO these classes will be included in the various overviews. This option has\n# no effect if EXTRACT_ALL is enabled.\n# The default value is: NO.\n\nHIDE_UNDOC_CLASSES     = NO\n\n# If the HIDE_FRIEND_COMPOUNDS tag is set to YES, doxygen will hide all friend\n# (class|struct|union) declarations. If set to NO these declarations will be\n# included in the documentation.\n# The default value is: NO.\n\nHIDE_FRIEND_COMPOUNDS  = NO\n\n# If the HIDE_IN_BODY_DOCS tag is set to YES, doxygen will hide any\n# documentation blocks found inside the body of a function. If set to NO these\n# blocks will be appended to the function's detailed documentation block.\n# The default value is: NO.\n\nHIDE_IN_BODY_DOCS      = NO\n\n# The INTERNAL_DOCS tag determines if documentation that is typed after a\n# \\internal command is included. If the tag is set to NO then the documentation\n# will be excluded. Set it to YES to include the internal documentation.\n# The default value is: NO.\n\nINTERNAL_DOCS          = NO\n\n# If the CASE_SENSE_NAMES tag is set to NO then doxygen will only generate file\n# names in lower-case letters. If set to YES upper-case letters are also\n# allowed. This is useful if you have classes or files whose names only differ\n# in case and if your file system supports case sensitive file names. Windows\n# and Mac users are advised to set this option to NO.\n# The default value is: system dependent.\n\nCASE_SENSE_NAMES       = YES\n\n# If the HIDE_SCOPE_NAMES tag is set to NO then doxygen will show members with\n# their full class and namespace scopes in the documentation. If set to YES the\n# scope will be hidden.\n# The default value is: NO.\n\nHIDE_SCOPE_NAMES       = NO\n\n# If the SHOW_INCLUDE_FILES tag is set to YES then doxygen will put a list of\n# the files that are included by a file in the documentation of that file.\n# The default value is: YES.\n\nSHOW_INCLUDE_FILES     = YES\n\n# If the FORCE_LOCAL_INCLUDES tag is set to YES then doxygen will list include\n# files with double quotes in the documentation rather than with sharp brackets.\n# The default value is: NO.\n\nFORCE_LOCAL_INCLUDES   = NO\n\n# If the INLINE_INFO tag is set to YES then a tag [inline] is inserted in the\n# documentation for inline members.\n# The default value is: YES.\n\nINLINE_INFO            = YES\n\n# If the SORT_MEMBER_DOCS tag is set to YES then doxygen will sort the\n# (detailed) documentation of file and class members alphabetically by member\n# name. If set to NO the members will appear in declaration order.\n# The default value is: YES.\n\nSORT_MEMBER_DOCS       = YES\n\n# If the SORT_BRIEF_DOCS tag is set to YES then doxygen will sort the brief\n# descriptions of file, namespace and class members alphabetically by member\n# name. If set to NO the members will appear in declaration order.\n# The default value is: NO.\n\nSORT_BRIEF_DOCS        = NO\n\n# If the SORT_MEMBERS_CTORS_1ST tag is set to YES then doxygen will sort the\n# (brief and detailed) documentation of class members so that constructors and\n# destructors are listed first. If set to NO the constructors will appear in the\n# respective orders defined by SORT_BRIEF_DOCS and SORT_MEMBER_DOCS.\n# Note: If SORT_BRIEF_DOCS is set to NO this option is ignored for sorting brief\n# member documentation.\n# Note: If SORT_MEMBER_DOCS is set to NO this option is ignored for sorting\n# detailed member documentation.\n# The default value is: NO.\n\nSORT_MEMBERS_CTORS_1ST = NO\n\n# If the SORT_GROUP_NAMES tag is set to YES then doxygen will sort the hierarchy\n# of group names into alphabetical order. If set to NO the group names will\n# appear in their defined order.\n# The default value is: NO.\n\nSORT_GROUP_NAMES       = NO\n\n# If the SORT_BY_SCOPE_NAME tag is set to YES, the class list will be sorted by\n# fully-qualified names, including namespaces. If set to NO, the class list will\n# be sorted only by class name, not including the namespace part.\n# Note: This option is not very useful if HIDE_SCOPE_NAMES is set to YES.\n# Note: This option applies only to the class list, not to the alphabetical\n# list.\n# The default value is: NO.\n\nSORT_BY_SCOPE_NAME     = NO\n\n# If the STRICT_PROTO_MATCHING option is enabled and doxygen fails to do proper\n# type resolution of all parameters of a function it will reject a match between\n# the prototype and the implementation of a member function even if there is\n# only one candidate or it is obvious which candidate to choose by doing a\n# simple string match. By disabling STRICT_PROTO_MATCHING doxygen will still\n# accept a match between prototype and implementation in such cases.\n# The default value is: NO.\n\nSTRICT_PROTO_MATCHING  = NO\n\n# The GENERATE_TODOLIST tag can be used to enable ( YES) or disable ( NO) the\n# todo list. This list is created by putting \\todo commands in the\n# documentation.\n# The default value is: YES.\n\nGENERATE_TODOLIST      = YES\n\n# The GENERATE_TESTLIST tag can be used to enable ( YES) or disable ( NO) the\n# test list. This list is created by putting \\test commands in the\n# documentation.\n# The default value is: YES.\n\nGENERATE_TESTLIST      = YES\n\n# The GENERATE_BUGLIST tag can be used to enable ( YES) or disable ( NO) the bug\n# list. This list is created by putting \\bug commands in the documentation.\n# The default value is: YES.\n\nGENERATE_BUGLIST       = YES\n\n# The GENERATE_DEPRECATEDLIST tag can be used to enable ( YES) or disable ( NO)\n# the deprecated list. This list is created by putting \\deprecated commands in\n# the documentation.\n# The default value is: YES.\n\nGENERATE_DEPRECATEDLIST= YES\n\n# The ENABLED_SECTIONS tag can be used to enable conditional documentation\n# sections, marked by \\if <section_label> ... \\endif and \\cond <section_label>\n# ... \\endcond blocks.\n\nENABLED_SECTIONS       =\n\n# The MAX_INITIALIZER_LINES tag determines the maximum number of lines that the\n# initial value of a variable or macro / define can have for it to appear in the\n# documentation. If the initializer consists of more lines than specified here\n# it will be hidden. Use a value of 0 to hide initializers completely. The\n# appearance of the value of individual variables and macros / defines can be\n# controlled using \\showinitializer or \\hideinitializer command in the\n# documentation regardless of this setting.\n# Minimum value: 0, maximum value: 10000, default value: 30.\n\nMAX_INITIALIZER_LINES  = 30\n\n# Set the SHOW_USED_FILES tag to NO to disable the list of files generated at\n# the bottom of the documentation of classes and structs. If set to YES the list\n# will mention the files that were used to generate the documentation.\n# The default value is: YES.\n\nSHOW_USED_FILES        = YES\n\n# Set the SHOW_FILES tag to NO to disable the generation of the Files page. This\n# will remove the Files entry from the Quick Index and from the Folder Tree View\n# (if specified).\n# The default value is: YES.\n\nSHOW_FILES             = YES\n\n# Set the SHOW_NAMESPACES tag to NO to disable the generation of the Namespaces\n# page. This will remove the Namespaces entry from the Quick Index and from the\n# Folder Tree View (if specified).\n# The default value is: YES.\n\nSHOW_NAMESPACES        = YES\n\n# The FILE_VERSION_FILTER tag can be used to specify a program or script that\n# doxygen should invoke to get the current version for each file (typically from\n# the version control system). Doxygen will invoke the program by executing (via\n# popen()) the command command input-file, where command is the value of the\n# FILE_VERSION_FILTER tag, and input-file is the name of an input file provided\n# by doxygen. Whatever the program writes to standard output is used as the file\n# version. For an example see the documentation.\n\nFILE_VERSION_FILTER    =\n\n# The LAYOUT_FILE tag can be used to specify a layout file which will be parsed\n# by doxygen. The layout file controls the global structure of the generated\n# output files in an output format independent way. To create the layout file\n# that represents doxygen's defaults, run doxygen with the -l option. You can\n# optionally specify a file name after the option, if omitted DoxygenLayout.xml\n# will be used as the name of the layout file.\n#\n# Note that if you run doxygen from a directory containing a file called\n# DoxygenLayout.xml, doxygen will parse it automatically even if the LAYOUT_FILE\n# tag is left empty.\n\nLAYOUT_FILE            =\n\n# The CITE_BIB_FILES tag can be used to specify one or more bib files containing\n# the reference definitions. This must be a list of .bib files. The .bib\n# extension is automatically appended if omitted. This requires the bibtex tool\n# to be installed. See also http://en.wikipedia.org/wiki/BibTeX for more info.\n# For LaTeX the style of the bibliography can be controlled using\n# LATEX_BIB_STYLE. To use this feature you need bibtex and perl available in the\n# search path. Do not use file names with spaces, bibtex cannot handle them. See\n# also \\cite for info how to create references.\n\nCITE_BIB_FILES         =\n\n#---------------------------------------------------------------------------\n# Configuration options related to warning and progress messages\n#---------------------------------------------------------------------------\n\n# The QUIET tag can be used to turn on/off the messages that are generated to\n# standard output by doxygen. If QUIET is set to YES this implies that the\n# messages are off.\n# The default value is: NO.\n\nQUIET                  = NO\n\n# The WARNINGS tag can be used to turn on/off the warning messages that are\n# generated to standard error ( stderr) by doxygen. If WARNINGS is set to YES\n# this implies that the warnings are on.\n#\n# Tip: Turn warnings on while writing the documentation.\n# The default value is: YES.\n\nWARNINGS               = YES\n\n# If the WARN_IF_UNDOCUMENTED tag is set to YES, then doxygen will generate\n# warnings for undocumented members. If EXTRACT_ALL is set to YES then this flag\n# will automatically be disabled.\n# The default value is: YES.\n\nWARN_IF_UNDOCUMENTED   = YES\n\n# If the WARN_IF_DOC_ERROR tag is set to YES, doxygen will generate warnings for\n# potential errors in the documentation, such as not documenting some parameters\n# in a documented function, or documenting parameters that don't exist or using\n# markup commands wrongly.\n# The default value is: YES.\n\nWARN_IF_DOC_ERROR      = YES\n\n# This WARN_NO_PARAMDOC option can be enabled to get warnings for functions that\n# are documented, but have no documentation for their parameters or return\n# value. If set to NO doxygen will only warn about wrong or incomplete parameter\n# documentation, but not about the absence of documentation.\n# The default value is: NO.\n\nWARN_NO_PARAMDOC       = NO\n\n# The WARN_FORMAT tag determines the format of the warning messages that doxygen\n# can produce. The string should contain the $file, $line, and $text tags, which\n# will be replaced by the file and line number from which the warning originated\n# and the warning text. Optionally the format may contain $version, which will\n# be replaced by the version of the file (if it could be obtained via\n# FILE_VERSION_FILTER)\n# The default value is: $file:$line: $text.\n\nWARN_FORMAT            = \"$file:$line: $text\"\n\n# The WARN_LOGFILE tag can be used to specify a file to which warning and error\n# messages should be written. If left blank the output is written to standard\n# error (stderr).\n\nWARN_LOGFILE           =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the input files\n#---------------------------------------------------------------------------\n\n# The INPUT tag is used to specify the files and/or directories that contain\n# documented source files. You may enter file names like myfile.cpp or\n# directories like /usr/src/myproject. Separate the files or directories with\n# spaces.\n# Note: If this tag is empty the current directory is searched.\n\nINPUT                  = include/cutlass tools/util/include/cutlass/ tools/library/include/cutlass/\n\nINPUT += media/docs/doxygen_mainpage.md\n\n# This tag can be used to specify the character encoding of the source files\n# that doxygen parses. Internally doxygen uses the UTF-8 encoding. Doxygen uses\n# libiconv (or the iconv built into libc) for the transcoding. See the libiconv\n# documentation (see: http://www.gnu.org/software/libiconv) for the list of\n# possible encodings.\n# The default value is: UTF-8.\n\nINPUT_ENCODING         = UTF-8\n\n# If the value of the INPUT tag contains directories, you can use the\n# FILE_PATTERNS tag to specify one or more wildcard patterns (like *.cpp and\n# *.h) to filter out the source-files in the directories. If left blank the\n# following patterns are tested:*.c, *.cc, *.cxx, *.cpp, *.c++, *.java, *.ii,\n# *.ixx, *.ipp, *.i++, *.inl, *.idl, *.ddl, *.odl, *.h, *.hh, *.hxx, *.hpp,\n# *.h++, *.cs, *.d, *.php, *.php4, *.php5, *.phtml, *.inc, *.m, *.markdown,\n# *.md, *.mm, *.dox, *.py, *.f90, *.f, *.for, *.tcl, *.vhd, *.vhdl, *.ucf,\n# *.qsf, *.as and *.js.\n\nFILE_PATTERNS          =\n\n# The RECURSIVE tag can be used to specify whether or not subdirectories should\n# be searched for input files as well.\n# The default value is: NO.\n\nRECURSIVE              = YES\n\n# The EXCLUDE tag can be used to specify files and/or directories that should be\n# excluded from the INPUT source files. This way you can easily exclude a\n# subdirectory from a directory tree whose root is specified with the INPUT tag.\n#\n# Note that relative paths are relative to the directory from which doxygen is\n# run.\n\nEXCLUDE                =\n\n# The EXCLUDE_SYMLINKS tag can be used to select whether or not files or\n# directories that are symbolic links (a Unix file system feature) are excluded\n# from the input.\n# The default value is: NO.\n\nEXCLUDE_SYMLINKS       = NO\n\n# If the value of the INPUT tag contains directories, you can use the\n# EXCLUDE_PATTERNS tag to specify one or more wildcard patterns to exclude\n# certain files from those directories.\n#\n# Note that the wildcards are matched against the file with absolute path, so to\n# exclude all test directories for example use the pattern */test/*\n\nEXCLUDE_PATTERNS       =\n\n# The EXCLUDE_SYMBOLS tag can be used to specify one or more symbol names\n# (namespaces, classes, functions, etc.) that should be excluded from the\n# output. The symbol name can be a fully qualified name, a word, or if the\n# wildcard * is used, a substring. Examples: ANamespace, AClass,\n# AClass::ANamespace, ANamespace::*Test\n#\n# Note that the wildcards are matched against the file with absolute path, so to\n# exclude all test directories use the pattern */test/*\n\nEXCLUDE_SYMBOLS        =\n\n# The EXAMPLE_PATH tag can be used to specify one or more files or directories\n# that contain example code fragments that are included (see the \\include\n# command).\n\nEXAMPLE_PATH           =\n\n# If the value of the EXAMPLE_PATH tag contains directories, you can use the\n# EXAMPLE_PATTERNS tag to specify one or more wildcard pattern (like *.cpp and\n# *.h) to filter out the source-files in the directories. If left blank all\n# files are included.\n\nEXAMPLE_PATTERNS       =\n\n# If the EXAMPLE_RECURSIVE tag is set to YES then subdirectories will be\n# searched for input files to be used with the \\include or \\dontinclude commands\n# irrespective of the value of the RECURSIVE tag.\n# The default value is: NO.\n\nEXAMPLE_RECURSIVE      = NO\n\n# The IMAGE_PATH tag can be used to specify one or more files or directories\n# that contain images that are to be included in the documentation (see the\n# \\image command).\n\nIMAGE_PATH             =\n\n# The INPUT_FILTER tag can be used to specify a program that doxygen should\n# invoke to filter for each input file. Doxygen will invoke the filter program\n# by executing (via popen()) the command:\n#\n# <filter> <input-file>\n#\n# where <filter> is the value of the INPUT_FILTER tag, and <input-file> is the\n# name of an input file. Doxygen will then use the output that the filter\n# program writes to standard output. If FILTER_PATTERNS is specified, this tag\n# will be ignored.\n#\n# Note that the filter must not add or remove lines; it is applied before the\n# code is scanned, but not when the output code is generated. If lines are added\n# or removed, the anchors will not be placed correctly.\n\nINPUT_FILTER           =\n\n# The FILTER_PATTERNS tag can be used to specify filters on a per file pattern\n# basis. Doxygen will compare the file name with each pattern and apply the\n# filter if there is a match. The filters are a list of the form: pattern=filter\n# (like *.cpp=my_cpp_filter). See INPUT_FILTER for further information on how\n# filters are used. If the FILTER_PATTERNS tag is empty or if none of the\n# patterns match the file name, INPUT_FILTER is applied.\n\nFILTER_PATTERNS        =\n\n# If the FILTER_SOURCE_FILES tag is set to YES, the input filter (if set using\n# INPUT_FILTER ) will also be used to filter the input files that are used for\n# producing the source files to browse (i.e. when SOURCE_BROWSER is set to YES).\n# The default value is: NO.\n\nFILTER_SOURCE_FILES    = NO\n\n# The FILTER_SOURCE_PATTERNS tag can be used to specify source filters per file\n# pattern. A pattern will override the setting for FILTER_PATTERN (if any) and\n# it is also possible to disable source filtering for a specific pattern using\n# *.ext= (so without naming a filter).\n# This tag requires that the tag FILTER_SOURCE_FILES is set to YES.\n\nFILTER_SOURCE_PATTERNS =\n\n# If the USE_MDFILE_AS_MAINPAGE tag refers to the name of a markdown file that\n# is part of the input, its contents will be placed on the main page\n# (index.html). This can be useful if you have a project on for instance GitHub\n# and want to reuse the introduction page also for the doxygen output.\n\nUSE_MDFILE_AS_MAINPAGE = media/docs/doxygen_mainpage.md\n\n#---------------------------------------------------------------------------\n# Configuration options related to source browsing\n#---------------------------------------------------------------------------\n\n# If the SOURCE_BROWSER tag is set to YES then a list of source files will be\n# generated. Documented entities will be cross-referenced with these sources.\n#\n# Note: To get rid of all source code in the generated output, make sure that\n# also VERBATIM_HEADERS is set to NO.\n# The default value is: NO.\n\nSOURCE_BROWSER         = NO\n\n# Setting the INLINE_SOURCES tag to YES will include the body of functions,\n# classes and enums directly into the documentation.\n# The default value is: NO.\n\nINLINE_SOURCES         = NO\n\n# Setting the STRIP_CODE_COMMENTS tag to YES will instruct doxygen to hide any\n# special comment blocks from generated source code fragments. Normal C, C++ and\n# Fortran comments will always remain visible.\n# The default value is: YES.\n\nSTRIP_CODE_COMMENTS    = YES\n\n# If the REFERENCED_BY_RELATION tag is set to YES then for each documented\n# function all documented functions referencing it will be listed.\n# The default value is: NO.\n\nREFERENCED_BY_RELATION = NO\n\n# If the REFERENCES_RELATION tag is set to YES then for each documented function\n# all documented entities called/used by that function will be listed.\n# The default value is: NO.\n\nREFERENCES_RELATION    = NO\n\n# If the REFERENCES_LINK_SOURCE tag is set to YES and SOURCE_BROWSER tag is set\n# to YES, then the hyperlinks from functions in REFERENCES_RELATION and\n# REFERENCED_BY_RELATION lists will link to the source code. Otherwise they will\n# link to the documentation.\n# The default value is: YES.\n\nREFERENCES_LINK_SOURCE = YES\n\n# If SOURCE_TOOLTIPS is enabled (the default) then hovering a hyperlink in the\n# source code will show a tooltip with additional information such as prototype,\n# brief description and links to the definition and documentation. Since this\n# will make the HTML file larger and loading of large files a bit slower, you\n# can opt to disable this feature.\n# The default value is: YES.\n# This tag requires that the tag SOURCE_BROWSER is set to YES.\n\nSOURCE_TOOLTIPS        = YES\n\n# If the USE_HTAGS tag is set to YES then the references to source code will\n# point to the HTML generated by the htags(1) tool instead of doxygen built-in\n# source browser. The htags tool is part of GNU's global source tagging system\n# (see http://www.gnu.org/software/global/global.html). You will need version\n# 4.8.6 or higher.\n#\n# To use it do the following:\n# - Install the latest version of global\n# - Enable SOURCE_BROWSER and USE_HTAGS in the config file\n# - Make sure the INPUT points to the root of the source tree\n# - Run doxygen as normal\n#\n# Doxygen will invoke htags (and that will in turn invoke gtags), so these\n# tools must be available from the command line (i.e. in the search path).\n#\n# The result: instead of the source browser generated by doxygen, the links to\n# source code will now point to the output of htags.\n# The default value is: NO.\n# This tag requires that the tag SOURCE_BROWSER is set to YES.\n\nUSE_HTAGS              = NO\n\n# If the VERBATIM_HEADERS tag is set the YES then doxygen will generate a\n# verbatim copy of the header file for each class for which an include is\n# specified. Set to NO to disable this.\n# See also: Section \\class.\n# The default value is: YES.\n\nVERBATIM_HEADERS       = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to the alphabetical class index\n#---------------------------------------------------------------------------\n\n# If the ALPHABETICAL_INDEX tag is set to YES, an alphabetical index of all\n# compounds will be generated. Enable this if the project contains a lot of\n# classes, structs, unions or interfaces.\n# The default value is: YES.\n\nALPHABETICAL_INDEX     = YES\n\n# The COLS_IN_ALPHA_INDEX tag can be used to specify the number of columns in\n# which the alphabetical index list will be split.\n# Minimum value: 1, maximum value: 20, default value: 5.\n# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.\n\nCOLS_IN_ALPHA_INDEX    = 5\n\n# In case all classes in a project start with a common prefix, all classes will\n# be put under the same header in the alphabetical index. The IGNORE_PREFIX tag\n# can be used to specify a prefix (or a list of prefixes) that should be ignored\n# while generating the index headers.\n# This tag requires that the tag ALPHABETICAL_INDEX is set to YES.\n\nIGNORE_PREFIX          =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the HTML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_HTML tag is set to YES doxygen will generate HTML output\n# The default value is: YES.\n\nGENERATE_HTML          = YES\n\n# The HTML_OUTPUT tag is used to specify where the HTML docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: html.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_OUTPUT            = \n\n# The HTML_FILE_EXTENSION tag can be used to specify the file extension for each\n# generated HTML page (for example: .htm, .php, .asp).\n# The default value is: .html.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_FILE_EXTENSION    = .html\n\n# The HTML_HEADER tag can be used to specify a user-defined HTML header file for\n# each generated HTML page. If the tag is left blank doxygen will generate a\n# standard header.\n#\n# To get valid HTML the header file that includes any scripts and style sheets\n# that doxygen needs, which is dependent on the configuration options used (e.g.\n# the setting GENERATE_TREEVIEW). It is highly recommended to start with a\n# default header using\n# doxygen -w html new_header.html new_footer.html new_stylesheet.css\n# YourConfigFile\n# and then modify the file new_header.html. See also section \"Doxygen usage\"\n# for information on how to generate the default header that doxygen normally\n# uses.\n# Note: The header is subject to change so you typically have to regenerate the\n# default header when upgrading to a newer version of doxygen. For a description\n# of the possible markers and block names see the documentation.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_HEADER            =\n\n# The HTML_FOOTER tag can be used to specify a user-defined HTML footer for each\n# generated HTML page. If the tag is left blank doxygen will generate a standard\n# footer. See HTML_HEADER for more information on how to generate a default\n# footer and what special commands can be used inside the footer. See also\n# section \"Doxygen usage\" for information on how to generate the default footer\n# that doxygen normally uses.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_FOOTER            =\n\n# The HTML_STYLESHEET tag can be used to specify a user-defined cascading style\n# sheet that is used by each HTML page. It can be used to fine-tune the look of\n# the HTML output. If left blank doxygen will generate a default style sheet.\n# See also section \"Doxygen usage\" for information on how to generate the style\n# sheet that doxygen normally uses.\n# Note: It is recommended to use HTML_EXTRA_STYLESHEET instead of this tag, as\n# it is more robust and this tag (HTML_STYLESHEET) will in the future become\n# obsolete.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_STYLESHEET        =\n\n# The HTML_EXTRA_STYLESHEET tag can be used to specify an additional user-\n# defined cascading style sheet that is included after the standard style sheets\n# created by doxygen. Using this option one can overrule certain style aspects.\n# This is preferred over using HTML_STYLESHEET since it does not replace the\n# standard style sheet and is therefor more robust against future updates.\n# Doxygen will copy the style sheet file to the output directory. For an example\n# see the documentation.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_EXTRA_STYLESHEET  =\n\n# The HTML_EXTRA_FILES tag can be used to specify one or more extra images or\n# other source files which should be copied to the HTML output directory. Note\n# that these files will be copied to the base HTML output directory. Use the\n# $relpath^ marker in the HTML_HEADER and/or HTML_FOOTER files to load these\n# files. In the HTML_STYLESHEET file, use the file name only. Also note that the\n# files will be copied as-is; there are no commands or markers available.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_EXTRA_FILES       =\n\n# The HTML_COLORSTYLE_HUE tag controls the color of the HTML output. Doxygen\n# will adjust the colors in the stylesheet and background images according to\n# this color. Hue is specified as an angle on a colorwheel, see\n# http://en.wikipedia.org/wiki/Hue for more information. For instance the value\n# 0 represents red, 60 is yellow, 120 is green, 180 is cyan, 240 is blue, 300\n# purple, and 360 is red again.\n# Minimum value: 0, maximum value: 359, default value: 220.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_HUE    = 100\n\n# The HTML_COLORSTYLE_SAT tag controls the purity (or saturation) of the colors\n# in the HTML output. For a value of 0 the output will use grayscales only. A\n# value of 255 will produce the most vivid colors.\n# Minimum value: 0, maximum value: 255, default value: 100.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_SAT    = 50\n\n# The HTML_COLORSTYLE_GAMMA tag controls the gamma correction applied to the\n# luminance component of the colors in the HTML output. Values below 100\n# gradually make the output lighter, whereas values above 100 make the output\n# darker. The value divided by 100 is the actual gamma applied, so 80 represents\n# a gamma of 0.8, The value 220 represents a gamma of 2.2, and 100 does not\n# change the gamma.\n# Minimum value: 40, maximum value: 240, default value: 80.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_COLORSTYLE_GAMMA  = 80\n\n# If the HTML_TIMESTAMP tag is set to YES then the footer of each generated HTML\n# page will contain the date and time when the page was generated. Setting this\n# to NO can help when comparing the output of multiple runs.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_TIMESTAMP         = NO\n\n# If the HTML_DYNAMIC_SECTIONS tag is set to YES then the generated HTML\n# documentation will contain sections that can be hidden and shown after the\n# page has loaded.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_DYNAMIC_SECTIONS  = NO\n\n# With HTML_INDEX_NUM_ENTRIES one can control the preferred number of entries\n# shown in the various tree structured indices initially; the user can expand\n# and collapse entries dynamically later on. Doxygen will expand the tree to\n# such a level that at most the specified number of entries are visible (unless\n# a fully collapsed tree already exceeds this amount). So setting the number of\n# entries 1 will produce a full collapsed tree by default. 0 is a special value\n# representing an infinite number of entries and will result in a full expanded\n# tree by default.\n# Minimum value: 0, maximum value: 9999, default value: 100.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nHTML_INDEX_NUM_ENTRIES = 100\n\n# If the GENERATE_DOCSET tag is set to YES, additional index files will be\n# generated that can be used as input for Apple's Xcode 3 integrated development\n# environment (see: http://developer.apple.com/tools/xcode/), introduced with\n# OSX 10.5 (Leopard). To create a documentation set, doxygen will generate a\n# Makefile in the HTML output directory. Running make will produce the docset in\n# that directory and running make install will install the docset in\n# ~/Library/Developer/Shared/Documentation/DocSets so that Xcode will find it at\n# startup. See http://developer.apple.com/tools/creatingdocsetswithdoxygen.html\n# for more information.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_DOCSET        = NO\n\n# This tag determines the name of the docset feed. A documentation feed provides\n# an umbrella under which multiple documentation sets from a single provider\n# (such as a company or product suite) can be grouped.\n# The default value is: Doxygen generated docs.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_FEEDNAME        = \"Doxygen generated docs\"\n\n# This tag specifies a string that should uniquely identify the documentation\n# set bundle. This should be a reverse domain-name style string, e.g.\n# com.mycompany.MyDocSet. Doxygen will append .docset to the name.\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_BUNDLE_ID       = org.doxygen.Project\n\n# The DOCSET_PUBLISHER_ID tag specifies a string that should uniquely identify\n# the documentation publisher. This should be a reverse domain-name style\n# string, e.g. com.mycompany.MyDocSet.documentation.\n# The default value is: org.doxygen.Publisher.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_PUBLISHER_ID    = org.doxygen.Publisher\n\n# The DOCSET_PUBLISHER_NAME tag identifies the documentation publisher.\n# The default value is: Publisher.\n# This tag requires that the tag GENERATE_DOCSET is set to YES.\n\nDOCSET_PUBLISHER_NAME  = Publisher\n\n# If the GENERATE_HTMLHELP tag is set to YES then doxygen generates three\n# additional HTML index files: index.hhp, index.hhc, and index.hhk. The\n# index.hhp is a project file that can be read by Microsoft's HTML Help Workshop\n# (see: http://www.microsoft.com/en-us/download/details.aspx?id=21138) on\n# Windows.\n#\n# The HTML Help Workshop contains a compiler that can convert all HTML output\n# generated by doxygen into a single compiled HTML file (.chm). Compiled HTML\n# files are now used as the Windows 98 help format, and will replace the old\n# Windows help format (.hlp) on all Windows platforms in the future. Compressed\n# HTML files also contain an index, a table of contents, and you can search for\n# words in the documentation. The HTML workshop also contains a viewer for\n# compressed HTML files.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_HTMLHELP      = NO\n\n# The CHM_FILE tag can be used to specify the file name of the resulting .chm\n# file. You can add a path in front of the file if the result should not be\n# written to the html output directory.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nCHM_FILE               =\n\n# The HHC_LOCATION tag can be used to specify the location (absolute path\n# including file name) of the HTML help compiler ( hhc.exe). If non-empty\n# doxygen will try to run the HTML help compiler on the generated index.hhp.\n# The file has to be specified with full path.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nHHC_LOCATION           =\n\n# The GENERATE_CHI flag controls if a separate .chi index file is generated (\n# YES) or that it should be included in the master .chm file ( NO).\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nGENERATE_CHI           = NO\n\n# The CHM_INDEX_ENCODING is used to encode HtmlHelp index ( hhk), content ( hhc)\n# and project file content.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nCHM_INDEX_ENCODING     =\n\n# The BINARY_TOC flag controls whether a binary table of contents is generated (\n# YES) or a normal table of contents ( NO) in the .chm file.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nBINARY_TOC             = NO\n\n# The TOC_EXPAND flag can be set to YES to add extra items for group members to\n# the table of contents of the HTML help documentation and to the tree view.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTMLHELP is set to YES.\n\nTOC_EXPAND             = NO\n\n# If the GENERATE_QHP tag is set to YES and both QHP_NAMESPACE and\n# QHP_VIRTUAL_FOLDER are set, an additional index file will be generated that\n# can be used as input for Qt's qhelpgenerator to generate a Qt Compressed Help\n# (.qch) of the generated HTML documentation.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_QHP           = NO\n\n# If the QHG_LOCATION tag is specified, the QCH_FILE tag can be used to specify\n# the file name of the resulting .qch file. The path specified is relative to\n# the HTML output folder.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQCH_FILE               =\n\n# The QHP_NAMESPACE tag specifies the namespace to use when generating Qt Help\n# Project output. For more information please see Qt Help Project / Namespace\n# (see: http://qt-project.org/doc/qt-4.8/qthelpproject.html#namespace).\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_NAMESPACE          = org.doxygen.Project\n\n# The QHP_VIRTUAL_FOLDER tag specifies the namespace to use when generating Qt\n# Help Project output. For more information please see Qt Help Project / Virtual\n# Folders (see: http://qt-project.org/doc/qt-4.8/qthelpproject.html#virtual-\n# folders).\n# The default value is: doc.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_VIRTUAL_FOLDER     = doc\n\n# If the QHP_CUST_FILTER_NAME tag is set, it specifies the name of a custom\n# filter to add. For more information please see Qt Help Project / Custom\n# Filters (see: http://qt-project.org/doc/qt-4.8/qthelpproject.html#custom-\n# filters).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_CUST_FILTER_NAME   =\n\n# The QHP_CUST_FILTER_ATTRS tag specifies the list of the attributes of the\n# custom filter to add. For more information please see Qt Help Project / Custom\n# Filters (see: http://qt-project.org/doc/qt-4.8/qthelpproject.html#custom-\n# filters).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_CUST_FILTER_ATTRS  =\n\n# The QHP_SECT_FILTER_ATTRS tag specifies the list of the attributes this\n# project's filter section matches. Qt Help Project / Filter Attributes (see:\n# http://qt-project.org/doc/qt-4.8/qthelpproject.html#filter-attributes).\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHP_SECT_FILTER_ATTRS  =\n\n# The QHG_LOCATION tag can be used to specify the location of Qt's\n# qhelpgenerator. If non-empty doxygen will try to run qhelpgenerator on the\n# generated .qhp file.\n# This tag requires that the tag GENERATE_QHP is set to YES.\n\nQHG_LOCATION           =\n\n# If the GENERATE_ECLIPSEHELP tag is set to YES, additional index files will be\n# generated, together with the HTML files, they form an Eclipse help plugin. To\n# install this plugin and make it available under the help contents menu in\n# Eclipse, the contents of the directory containing the HTML and XML files needs\n# to be copied into the plugins directory of eclipse. The name of the directory\n# within the plugins directory should be the same as the ECLIPSE_DOC_ID value.\n# After copying Eclipse needs to be restarted before the help appears.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_ECLIPSEHELP   = NO\n\n# A unique identifier for the Eclipse help plugin. When installing the plugin\n# the directory name containing the HTML and XML files should also have this\n# name. Each documentation set should have its own identifier.\n# The default value is: org.doxygen.Project.\n# This tag requires that the tag GENERATE_ECLIPSEHELP is set to YES.\n\nECLIPSE_DOC_ID         = org.doxygen.Project\n\n# If you want full control over the layout of the generated HTML pages it might\n# be necessary to disable the index and replace it with your own. The\n# DISABLE_INDEX tag can be used to turn on/off the condensed index (tabs) at top\n# of each HTML page. A value of NO enables the index and the value YES disables\n# it. Since the tabs in the index contain the same information as the navigation\n# tree, you can set this option to YES if you also set GENERATE_TREEVIEW to YES.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nDISABLE_INDEX          = NO\n\n# The GENERATE_TREEVIEW tag is used to specify whether a tree-like index\n# structure should be generated to display hierarchical information. If the tag\n# value is set to YES, a side panel will be generated containing a tree-like\n# index structure (just like the one that is generated for HTML Help). For this\n# to work a browser that supports JavaScript, DHTML, CSS and frames is required\n# (i.e. any modern browser). Windows users are probably better off using the\n# HTML help feature. Via custom stylesheets (see HTML_EXTRA_STYLESHEET) one can\n# further fine-tune the look of the index. As an example, the default style\n# sheet generated by doxygen has an example that shows how to put an image at\n# the root of the tree instead of the PROJECT_NAME. Since the tree basically has\n# the same information as the tab index, you could consider setting\n# DISABLE_INDEX to YES when enabling this option.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nGENERATE_TREEVIEW      = NO\n\n# The ENUM_VALUES_PER_LINE tag can be used to set the number of enum values that\n# doxygen will group on one line in the generated HTML documentation.\n#\n# Note that a value of 0 will completely suppress the enum values from appearing\n# in the overview section.\n# Minimum value: 0, maximum value: 20, default value: 4.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nENUM_VALUES_PER_LINE   = 4\n\n# If the treeview is enabled (see GENERATE_TREEVIEW) then this tag can be used\n# to set the initial width (in pixels) of the frame in which the tree is shown.\n# Minimum value: 0, maximum value: 1500, default value: 250.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nTREEVIEW_WIDTH         = 250\n\n# When the EXT_LINKS_IN_WINDOW option is set to YES doxygen will open links to\n# external symbols imported via tag files in a separate window.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nEXT_LINKS_IN_WINDOW    = NO\n\n# Use this tag to change the font size of LaTeX formulas included as images in\n# the HTML documentation. When you change the font size after a successful\n# doxygen run you need to manually remove any form_*.png images from the HTML\n# output directory to force them to be regenerated.\n# Minimum value: 8, maximum value: 50, default value: 10.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nFORMULA_FONTSIZE       = 10\n\n# Use the FORMULA_TRANPARENT tag to determine whether or not the images\n# generated for formulas are transparent PNGs. Transparent PNGs are not\n# supported properly for IE 6.0, but are supported on all modern browsers.\n#\n# Note that when changing this option you need to delete any form_*.png files in\n# the HTML output directory before the changes have effect.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nFORMULA_TRANSPARENT    = YES\n\n# Enable the USE_MATHJAX option to render LaTeX formulas using MathJax (see\n# http://www.mathjax.org) which uses client side Javascript for the rendering\n# instead of using prerendered bitmaps. Use this if you do not have LaTeX\n# installed or if you want to formulas look prettier in the HTML output. When\n# enabled you may also need to install MathJax separately and configure the path\n# to it using the MATHJAX_RELPATH option.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nUSE_MATHJAX            = YES\n\n# When MathJax is enabled you can set the default output format to be used for\n# the MathJax output. See the MathJax site (see:\n# http://docs.mathjax.org/en/latest/output.html) for more details.\n# Possible values are: HTML-CSS (which is slower, but has the best\n# compatibility), NativeMML (i.e. MathML) and SVG.\n# The default value is: HTML-CSS.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_FORMAT         = HTML-CSS\n\n# When MathJax is enabled you need to specify the location relative to the HTML\n# output directory using the MATHJAX_RELPATH option. The destination directory\n# should contain the MathJax.js script. For instance, if the mathjax directory\n# is located at the same level as the HTML output directory, then\n# MATHJAX_RELPATH should be ../mathjax. The default value points to the MathJax\n# Content Delivery Network so you can quickly see the result without installing\n# MathJax. However, it is strongly recommended to install a local copy of\n# MathJax from http://www.mathjax.org before deployment.\n# The default value is: http://cdn.mathjax.org/mathjax/latest.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_RELPATH        = http://cdn.mathjax.org/mathjax/latest\n\n# The MATHJAX_EXTENSIONS tag can be used to specify one or more MathJax\n# extension names that should be enabled during MathJax rendering. For example\n# MATHJAX_EXTENSIONS = TeX/AMSmath TeX/AMSsymbols\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_EXTENSIONS     =\n\n# The MATHJAX_CODEFILE tag can be used to specify a file with javascript pieces\n# of code that will be used on startup of the MathJax code. See the MathJax site\n# (see: http://docs.mathjax.org/en/latest/output.html) for more details. For an\n# example see the documentation.\n# This tag requires that the tag USE_MATHJAX is set to YES.\n\nMATHJAX_CODEFILE       =\n\n# When the SEARCHENGINE tag is enabled doxygen will generate a search box for\n# the HTML output. The underlying search engine uses javascript and DHTML and\n# should work on any modern browser. Note that when using HTML help\n# (GENERATE_HTMLHELP), Qt help (GENERATE_QHP), or docsets (GENERATE_DOCSET)\n# there is already a search function so this one should typically be disabled.\n# For large projects the javascript based search engine can be slow, then\n# enabling SERVER_BASED_SEARCH may provide a better solution. It is possible to\n# search using the keyboard; to jump to the search box use <access key> + S\n# (what the <access key> is depends on the OS and browser, but it is typically\n# <CTRL>, <ALT>/<option>, or both). Inside the search box use the <cursor down\n# key> to jump into the search results window, the results can be navigated\n# using the <cursor keys>. Press <Enter> to select an item or <escape> to cancel\n# the search. The filter options can be selected when the cursor is inside the\n# search box by pressing <Shift>+<cursor down>. Also here use the <cursor keys>\n# to select a filter and <Enter> or <escape> to activate or cancel the filter\n# option.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_HTML is set to YES.\n\nSEARCHENGINE           = YES\n\n# When the SERVER_BASED_SEARCH tag is enabled the search engine will be\n# implemented using a web server instead of a web client using Javascript. There\n# are two flavours of web server based searching depending on the\n# EXTERNAL_SEARCH setting. When disabled, doxygen will generate a PHP script for\n# searching and an index file used by the script. When EXTERNAL_SEARCH is\n# enabled the indexing and searching needs to be provided by external tools. See\n# the section \"External Indexing and Searching\" for details.\n# The default value is: NO.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSERVER_BASED_SEARCH    = NO\n\n# When EXTERNAL_SEARCH tag is enabled doxygen will no longer generate the PHP\n# script for searching. Instead the search results are written to an XML file\n# which needs to be processed by an external indexer. Doxygen will invoke an\n# external search engine pointed to by the SEARCHENGINE_URL option to obtain the\n# search results.\n#\n# Doxygen ships with an example indexer ( doxyindexer) and search engine\n# (doxysearch.cgi) which are based on the open source search engine library\n# Xapian (see: http://xapian.org/).\n#\n# See the section \"External Indexing and Searching\" for details.\n# The default value is: NO.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTERNAL_SEARCH        = NO\n\n# The SEARCHENGINE_URL should point to a search engine hosted by a web server\n# which will return the search results when EXTERNAL_SEARCH is enabled.\n#\n# Doxygen ships with an example indexer ( doxyindexer) and search engine\n# (doxysearch.cgi) which are based on the open source search engine library\n# Xapian (see: http://xapian.org/). See the section \"External Indexing and\n# Searching\" for details.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSEARCHENGINE_URL       =\n\n# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the unindexed\n# search data is written to a file for indexing by an external tool. With the\n# SEARCHDATA_FILE tag the name of this file can be specified.\n# The default file is: searchdata.xml.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nSEARCHDATA_FILE        = searchdata.xml\n\n# When SERVER_BASED_SEARCH and EXTERNAL_SEARCH are both enabled the\n# EXTERNAL_SEARCH_ID tag can be used as an identifier for the project. This is\n# useful in combination with EXTRA_SEARCH_MAPPINGS to search through multiple\n# projects and redirect the results back to the right project.\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTERNAL_SEARCH_ID     =\n\n# The EXTRA_SEARCH_MAPPINGS tag can be used to enable searching through doxygen\n# projects other than the one defined by this configuration file, but that are\n# all added to the same external search index. Each project needs to have a\n# unique id set via EXTERNAL_SEARCH_ID. The search mapping then maps the id of\n# to a relative location where the documentation can be found. The format is:\n# EXTRA_SEARCH_MAPPINGS = tagname1=loc1 tagname2=loc2 ...\n# This tag requires that the tag SEARCHENGINE is set to YES.\n\nEXTRA_SEARCH_MAPPINGS  =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the LaTeX output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_LATEX tag is set to YES doxygen will generate LaTeX output.\n# The default value is: YES.\n\nGENERATE_LATEX         = NO\n\n# The LATEX_OUTPUT tag is used to specify where the LaTeX docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: latex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_OUTPUT           = latex\n\n# The LATEX_CMD_NAME tag can be used to specify the LaTeX command name to be\n# invoked.\n#\n# Note that when enabling USE_PDFLATEX this option is only used for generating\n# bitmaps for formulas in the HTML output, but not in the Makefile that is\n# written to the output directory.\n# The default file is: latex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_CMD_NAME         = latex\n\n# The MAKEINDEX_CMD_NAME tag can be used to specify the command name to generate\n# index for LaTeX.\n# The default file is: makeindex.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nMAKEINDEX_CMD_NAME     = makeindex\n\n# If the COMPACT_LATEX tag is set to YES doxygen generates more compact LaTeX\n# documents. This may be useful for small projects and may help to save some\n# trees in general.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nCOMPACT_LATEX          = NO\n\n# The PAPER_TYPE tag can be used to set the paper type that is used by the\n# printer.\n# Possible values are: a4 (210 x 297 mm), letter (8.5 x 11 inches), legal (8.5 x\n# 14 inches) and executive (7.25 x 10.5 inches).\n# The default value is: a4.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nPAPER_TYPE             = a4\n\n# The EXTRA_PACKAGES tag can be used to specify one or more LaTeX package names\n# that should be included in the LaTeX output. To get the times font for\n# instance you can specify\n# EXTRA_PACKAGES=times\n# If left blank no extra packages will be included.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nEXTRA_PACKAGES         =\n\n# The LATEX_HEADER tag can be used to specify a personal LaTeX header for the\n# generated LaTeX document. The header should contain everything until the first\n# chapter. If it is left blank doxygen will generate a standard header. See\n# section \"Doxygen usage\" for information on how to let doxygen write the\n# default header to a separate file.\n#\n# Note: Only use a user-defined header if you know what you are doing! The\n# following commands have a special meaning inside the header: $title,\n# $datetime, $date, $doxygenversion, $projectname, $projectnumber. Doxygen will\n# replace them by respectively the title of the page, the current date and time,\n# only the current date, the version number of doxygen, the project name (see\n# PROJECT_NAME), or the project number (see PROJECT_NUMBER).\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_HEADER           =\n\n# The LATEX_FOOTER tag can be used to specify a personal LaTeX footer for the\n# generated LaTeX document. The footer should contain everything after the last\n# chapter. If it is left blank doxygen will generate a standard footer.\n#\n# Note: Only use a user-defined footer if you know what you are doing!\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_FOOTER           =\n\n# The LATEX_EXTRA_FILES tag can be used to specify one or more extra images or\n# other source files which should be copied to the LATEX_OUTPUT output\n# directory. Note that the files will be copied as-is; there are no commands or\n# markers available.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_EXTRA_FILES      =\n\n# If the PDF_HYPERLINKS tag is set to YES, the LaTeX that is generated is\n# prepared for conversion to PDF (using ps2pdf or pdflatex). The PDF file will\n# contain links (just like the HTML output) instead of page references. This\n# makes the output suitable for online browsing using a PDF viewer.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nPDF_HYPERLINKS         = YES\n\n# If the LATEX_PDFLATEX tag is set to YES, doxygen will use pdflatex to generate\n# the PDF file directly from the LaTeX files. Set this option to YES to get a\n# higher quality PDF documentation.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nUSE_PDFLATEX           = YES\n\n# If the LATEX_BATCHMODE tag is set to YES, doxygen will add the \\batchmode\n# command to the generated LaTeX files. This will instruct LaTeX to keep running\n# if errors occur, instead of asking the user for help. This option is also used\n# when generating formulas in HTML.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_BATCHMODE        = NO\n\n# If the LATEX_HIDE_INDICES tag is set to YES then doxygen will not include the\n# index chapters (such as File Index, Compound Index, etc.) in the output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_HIDE_INDICES     = NO\n\n# If the LATEX_SOURCE_CODE tag is set to YES then doxygen will include source\n# code with syntax highlighting in the LaTeX output.\n#\n# Note that which sources are shown also depends on other settings such as\n# SOURCE_BROWSER.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_SOURCE_CODE      = NO\n\n# The LATEX_BIB_STYLE tag can be used to specify the style to use for the\n# bibliography, e.g. plainnat, or ieeetr. See\n# http://en.wikipedia.org/wiki/BibTeX and \\cite for more info.\n# The default value is: plain.\n# This tag requires that the tag GENERATE_LATEX is set to YES.\n\nLATEX_BIB_STYLE        = plain\n\n#---------------------------------------------------------------------------\n# Configuration options related to the RTF output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_RTF tag is set to YES doxygen will generate RTF output. The\n# RTF output is optimized for Word 97 and may not look too pretty with other RTF\n# readers/editors.\n# The default value is: NO.\n\nGENERATE_RTF           = NO\n\n# The RTF_OUTPUT tag is used to specify where the RTF docs will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: rtf.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_OUTPUT             = rtf\n\n# If the COMPACT_RTF tag is set to YES doxygen generates more compact RTF\n# documents. This may be useful for small projects and may help to save some\n# trees in general.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nCOMPACT_RTF            = NO\n\n# If the RTF_HYPERLINKS tag is set to YES, the RTF that is generated will\n# contain hyperlink fields. The RTF file will contain links (just like the HTML\n# output) instead of page references. This makes the output suitable for online\n# browsing using Word or some other Word compatible readers that support those\n# fields.\n#\n# Note: WordPad (write) and others do not support links.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_HYPERLINKS         = NO\n\n# Load stylesheet definitions from file. Syntax is similar to doxygen's config\n# file, i.e. a series of assignments. You only have to provide replacements,\n# missing definitions are set to their default value.\n#\n# See also section \"Doxygen usage\" for information on how to generate the\n# default style sheet that doxygen normally uses.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_STYLESHEET_FILE    =\n\n# Set optional variables used in the generation of an RTF document. Syntax is\n# similar to doxygen's config file. A template extensions file can be generated\n# using doxygen -e rtf extensionFile.\n# This tag requires that the tag GENERATE_RTF is set to YES.\n\nRTF_EXTENSIONS_FILE    =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the man page output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_MAN tag is set to YES doxygen will generate man pages for\n# classes and files.\n# The default value is: NO.\n\nGENERATE_MAN           = NO\n\n# The MAN_OUTPUT tag is used to specify where the man pages will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it. A directory man3 will be created inside the directory specified by\n# MAN_OUTPUT.\n# The default directory is: man.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_OUTPUT             = man\n\n# The MAN_EXTENSION tag determines the extension that is added to the generated\n# man pages. In case the manual section does not start with a number, the number\n# 3 is prepended. The dot (.) at the beginning of the MAN_EXTENSION tag is\n# optional.\n# The default value is: .3.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_EXTENSION          = .3\n\n# If the MAN_LINKS tag is set to YES and doxygen generates man output, then it\n# will generate one additional man file for each entity documented in the real\n# man page(s). These additional files only source the real man page, but without\n# them the man command would be unable to find the correct page.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_MAN is set to YES.\n\nMAN_LINKS              = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the XML output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_XML tag is set to YES doxygen will generate an XML file that\n# captures the structure of the code including all documentation.\n# The default value is: NO.\n\nGENERATE_XML           = NO\n\n# The XML_OUTPUT tag is used to specify where the XML pages will be put. If a\n# relative path is entered the value of OUTPUT_DIRECTORY will be put in front of\n# it.\n# The default directory is: xml.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_OUTPUT             = xml\n\n# The XML_SCHEMA tag can be used to specify a XML schema, which can be used by a\n# validating XML parser to check the syntax of the XML files.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_SCHEMA             =\n\n# The XML_DTD tag can be used to specify a XML DTD, which can be used by a\n# validating XML parser to check the syntax of the XML files.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_DTD                =\n\n# If the XML_PROGRAMLISTING tag is set to YES doxygen will dump the program\n# listings (including syntax highlighting and cross-referencing information) to\n# the XML output. Note that enabling this will significantly increase the size\n# of the XML output.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_XML is set to YES.\n\nXML_PROGRAMLISTING     = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to the DOCBOOK output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_DOCBOOK tag is set to YES doxygen will generate Docbook files\n# that can be used to generate PDF.\n# The default value is: NO.\n\nGENERATE_DOCBOOK       = NO\n\n# The DOCBOOK_OUTPUT tag is used to specify where the Docbook pages will be put.\n# If a relative path is entered the value of OUTPUT_DIRECTORY will be put in\n# front of it.\n# The default directory is: docbook.\n# This tag requires that the tag GENERATE_DOCBOOK is set to YES.\n\nDOCBOOK_OUTPUT         = docbook\n\n#---------------------------------------------------------------------------\n# Configuration options for the AutoGen Definitions output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_AUTOGEN_DEF tag is set to YES doxygen will generate an AutoGen\n# Definitions (see http://autogen.sf.net) file that captures the structure of\n# the code including all documentation. Note that this feature is still\n# experimental and incomplete at the moment.\n# The default value is: NO.\n\nGENERATE_AUTOGEN_DEF   = NO\n\n#---------------------------------------------------------------------------\n# Configuration options related to the Perl module output\n#---------------------------------------------------------------------------\n\n# If the GENERATE_PERLMOD tag is set to YES doxygen will generate a Perl module\n# file that captures the structure of the code including all documentation.\n#\n# Note that this feature is still experimental and incomplete at the moment.\n# The default value is: NO.\n\nGENERATE_PERLMOD       = NO\n\n# If the PERLMOD_LATEX tag is set to YES doxygen will generate the necessary\n# Makefile rules, Perl scripts and LaTeX code to be able to generate PDF and DVI\n# output from the Perl module output.\n# The default value is: NO.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_LATEX          = NO\n\n# If the PERLMOD_PRETTY tag is set to YES the Perl module output will be nicely\n# formatted so it can be parsed by a human reader. This is useful if you want to\n# understand what is going on. On the other hand, if this tag is set to NO the\n# size of the Perl module output will be much smaller and Perl will parse it\n# just the same.\n# The default value is: YES.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_PRETTY         = YES\n\n# The names of the make variables in the generated doxyrules.make file are\n# prefixed with the string contained in PERLMOD_MAKEVAR_PREFIX. This is useful\n# so different doxyrules.make files included by the same Makefile don't\n# overwrite each other's variables.\n# This tag requires that the tag GENERATE_PERLMOD is set to YES.\n\nPERLMOD_MAKEVAR_PREFIX =\n\n#---------------------------------------------------------------------------\n# Configuration options related to the preprocessor\n#---------------------------------------------------------------------------\n\n# If the ENABLE_PREPROCESSING tag is set to YES doxygen will evaluate all\n# C-preprocessor directives found in the sources and include files.\n# The default value is: YES.\n\nENABLE_PREPROCESSING   = YES\n\n# If the MACRO_EXPANSION tag is set to YES doxygen will expand all macro names\n# in the source code. If set to NO only conditional compilation will be\n# performed. Macro expansion can be done in a controlled way by setting\n# EXPAND_ONLY_PREDEF to YES.\n# The default value is: NO.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nMACRO_EXPANSION        = NO\n\n# If the EXPAND_ONLY_PREDEF and MACRO_EXPANSION tags are both set to YES then\n# the macro expansion is limited to the macros specified with the PREDEFINED and\n# EXPAND_AS_DEFINED tags.\n# The default value is: NO.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nEXPAND_ONLY_PREDEF     = NO\n\n# If the SEARCH_INCLUDES tag is set to YES the includes files in the\n# INCLUDE_PATH will be searched if a #include is found.\n# The default value is: YES.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nSEARCH_INCLUDES        = YES\n\n# The INCLUDE_PATH tag can be used to specify one or more directories that\n# contain include files that are not input files but should be processed by the\n# preprocessor.\n# This tag requires that the tag SEARCH_INCLUDES is set to YES.\n\nINCLUDE_PATH           = .\n\n# You can use the INCLUDE_FILE_PATTERNS tag to specify one or more wildcard\n# patterns (like *.h and *.hpp) to filter out the header-files in the\n# directories. If left blank, the patterns specified with FILE_PATTERNS will be\n# used.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nINCLUDE_FILE_PATTERNS  =\n\n# The PREDEFINED tag can be used to specify one or more macro names that are\n# defined before the preprocessor is started (similar to the -D option of e.g.\n# gcc). The argument of the tag is a list of macros of the form: name or\n# name=definition (no spaces). If the definition and the \"=\" are omitted, \"=1\"\n# is assumed. To prevent a macro definition from being undefined via #undef or\n# recursively expanded use the := operator instead of the = operator.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nPREDEFINED             =\n\n# If the MACRO_EXPANSION and EXPAND_ONLY_PREDEF tags are set to YES then this\n# tag can be used to specify a list of macro names that should be expanded. The\n# macro definition that is found in the sources will be used. Use the PREDEFINED\n# tag if you want to use a different macro definition that overrules the\n# definition found in the source code.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nEXPAND_AS_DEFINED      =\n\n# If the SKIP_FUNCTION_MACROS tag is set to YES then doxygen's preprocessor will\n# remove all refrences to function-like macros that are alone on a line, have an\n# all uppercase name, and do not end with a semicolon. Such function macros are\n# typically used for boiler-plate code, and will confuse the parser if not\n# removed.\n# The default value is: YES.\n# This tag requires that the tag ENABLE_PREPROCESSING is set to YES.\n\nSKIP_FUNCTION_MACROS   = YES\n\n#---------------------------------------------------------------------------\n# Configuration options related to external references\n#---------------------------------------------------------------------------\n\n# The TAGFILES tag can be used to specify one or more tag files. For each tag\n# file the location of the external documentation should be added. The format of\n# a tag file without this location is as follows:\n# TAGFILES = file1 file2 ...\n# Adding location for the tag files is done as follows:\n# TAGFILES = file1=loc1 \"file2 = loc2\" ...\n# where loc1 and loc2 can be relative or absolute paths or URLs. See the\n# section \"Linking to external documentation\" for more information about the use\n# of tag files.\n# Note: Each tag file must have an unique name (where the name does NOT include\n# the path). If a tag file is not located in the directory in which doxygen is\n# run, you must also specify the path to the tagfile here.\n\nTAGFILES               =\n\n# When a file name is specified after GENERATE_TAGFILE, doxygen will create a\n# tag file that is based on the input files it reads. See section \"Linking to\n# external documentation\" for more information about the usage of tag files.\n\nGENERATE_TAGFILE       =\n\n# If the ALLEXTERNALS tag is set to YES all external class will be listed in the\n# class index. If set to NO only the inherited external classes will be listed.\n# The default value is: NO.\n\nALLEXTERNALS           = NO\n\n# If the EXTERNAL_GROUPS tag is set to YES all external groups will be listed in\n# the modules index. If set to NO, only the current project's groups will be\n# listed.\n# The default value is: YES.\n\nEXTERNAL_GROUPS        = YES\n\n# If the EXTERNAL_PAGES tag is set to YES all external pages will be listed in\n# the related pages index. If set to NO, only the current project's pages will\n# be listed.\n# The default value is: YES.\n\nEXTERNAL_PAGES         = YES\n\n# The PERL_PATH should be the absolute path and name of the perl script\n# interpreter (i.e. the result of 'which perl').\n# The default file (with absolute path) is: /usr/bin/perl.\n\nPERL_PATH              = /usr/bin/perl\n\n#---------------------------------------------------------------------------\n# Configuration options related to the dot tool\n#---------------------------------------------------------------------------\n\n# If the CLASS_DIAGRAMS tag is set to YES doxygen will generate a class diagram\n# (in HTML and LaTeX) for classes with base or super classes. Setting the tag to\n# NO turns the diagrams off. Note that this option also works with HAVE_DOT\n# disabled, but it is recommended to install and use dot, since it yields more\n# powerful graphs.\n# The default value is: YES.\n\nCLASS_DIAGRAMS         = YES\n\n# You can define message sequence charts within doxygen comments using the \\msc\n# command. Doxygen will then run the mscgen tool (see:\n# http://www.mcternan.me.uk/mscgen/)) to produce the chart and insert it in the\n# documentation. The MSCGEN_PATH tag allows you to specify the directory where\n# the mscgen tool resides. If left empty the tool is assumed to be found in the\n# default search path.\n\nMSCGEN_PATH            =\n\n# If set to YES, the inheritance and collaboration graphs will hide inheritance\n# and usage relations if the target is undocumented or is not a class.\n# The default value is: YES.\n\nHIDE_UNDOC_RELATIONS   = YES\n\n# If you set the HAVE_DOT tag to YES then doxygen will assume the dot tool is\n# available from the path. This tool is part of Graphviz (see:\n# http://www.graphviz.org/), a graph visualization toolkit from AT&T and Lucent\n# Bell Labs. The other options in this section have no effect if this option is\n# set to NO\n# The default value is: NO.\n\nHAVE_DOT               = $(HAVE_DOT)\n\n# The DOT_NUM_THREADS specifies the number of dot invocations doxygen is allowed\n# to run in parallel. When set to 0 doxygen will base this on the number of\n# processors available in the system. You can set it explicitly to a value\n# larger than 0 to get control over the balance between CPU load and processing\n# speed.\n# Minimum value: 0, maximum value: 32, default value: 0.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_NUM_THREADS        = 0\n\n# When you want a differently looking font n the dot files that doxygen\n# generates you can specify the font name using DOT_FONTNAME. You need to make\n# sure dot is able to find the font, which can be done by putting it in a\n# standard location or by setting the DOTFONTPATH environment variable or by\n# setting DOT_FONTPATH to the directory containing the font.\n# The default value is: Helvetica.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTNAME           = Helvetica\n\n# The DOT_FONTSIZE tag can be used to set the size (in points) of the font of\n# dot graphs.\n# Minimum value: 4, maximum value: 24, default value: 10.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTSIZE           = 10\n\n# By default doxygen will tell dot to use the default font as specified with\n# DOT_FONTNAME. If you specify a different font using DOT_FONTNAME you can set\n# the path where dot can find it using this tag.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_FONTPATH           =\n\n# If the CLASS_GRAPH tag is set to YES then doxygen will generate a graph for\n# each documented class showing the direct and indirect inheritance relations.\n# Setting this tag to YES will force the CLASS_DIAGRAMS tag to NO.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCLASS_GRAPH            = YES\n\n# If the COLLABORATION_GRAPH tag is set to YES then doxygen will generate a\n# graph for each documented class showing the direct and indirect implementation\n# dependencies (inheritance, containment, and class references variables) of the\n# class with other documented classes.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCOLLABORATION_GRAPH    = YES\n\n# If the GROUP_GRAPHS tag is set to YES then doxygen will generate a graph for\n# groups, showing the direct groups dependencies.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGROUP_GRAPHS           = YES\n\n# If the UML_LOOK tag is set to YES doxygen will generate inheritance and\n# collaboration diagrams in a style similar to the OMG's Unified Modeling\n# Language.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nUML_LOOK               = NO\n\n# If the UML_LOOK tag is enabled, the fields and methods are shown inside the\n# class node. If there are many fields or methods and many nodes the graph may\n# become too big to be useful. The UML_LIMIT_NUM_FIELDS threshold limits the\n# number of items for each type to make the size more manageable. Set this to 0\n# for no limit. Note that the threshold may be exceeded by 50% before the limit\n# is enforced. So when you set the threshold to 10, up to 15 fields may appear,\n# but if the number exceeds 15, the total amount of fields shown is limited to\n# 10.\n# Minimum value: 0, maximum value: 100, default value: 10.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nUML_LIMIT_NUM_FIELDS   = 10\n\n# If the TEMPLATE_RELATIONS tag is set to YES then the inheritance and\n# collaboration graphs will show the relations between templates and their\n# instances.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nTEMPLATE_RELATIONS     = NO\n\n# If the INCLUDE_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are set to\n# YES then doxygen will generate a graph for each documented file showing the\n# direct and indirect include dependencies of the file with other documented\n# files.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINCLUDE_GRAPH          = YES\n\n# If the INCLUDED_BY_GRAPH, ENABLE_PREPROCESSING and SEARCH_INCLUDES tags are\n# set to YES then doxygen will generate a graph for each documented file showing\n# the direct and indirect include dependencies of the file with other documented\n# files.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINCLUDED_BY_GRAPH      = YES\n\n# If the CALL_GRAPH tag is set to YES then doxygen will generate a call\n# dependency graph for every global function or class method.\n#\n# Note that enabling this option will significantly increase the time of a run.\n# So in most cases it will be better to enable call graphs for selected\n# functions only using the \\callgraph command.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCALL_GRAPH             = NO\n\n# If the CALLER_GRAPH tag is set to YES then doxygen will generate a caller\n# dependency graph for every global function or class method.\n#\n# Note that enabling this option will significantly increase the time of a run.\n# So in most cases it will be better to enable caller graphs for selected\n# functions only using the \\callergraph command.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nCALLER_GRAPH           = NO\n\n# If the GRAPHICAL_HIERARCHY tag is set to YES then doxygen will graphical\n# hierarchy of all classes instead of a textual one.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGRAPHICAL_HIERARCHY    = YES\n\n# If the DIRECTORY_GRAPH tag is set to YES then doxygen will show the\n# dependencies a directory has on other directories in a graphical way. The\n# dependency relations are determined by the #include relations between the\n# files in the directories.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDIRECTORY_GRAPH        = YES\n\n# The DOT_IMAGE_FORMAT tag can be used to set the image format of the images\n# generated by dot.\n# Note: If you choose svg you need to set HTML_FILE_EXTENSION to xhtml in order\n# to make the SVG files visible in IE 9+ (other browsers do not have this\n# requirement).\n# Possible values are: png, jpg, gif and svg.\n# The default value is: png.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_IMAGE_FORMAT       = png\n\n# If DOT_IMAGE_FORMAT is set to svg, then this option can be set to YES to\n# enable generation of interactive SVG images that allow zooming and panning.\n#\n# Note that this requires a modern browser other than Internet Explorer. Tested\n# and working are Firefox, Chrome, Safari, and Opera.\n# Note: For IE 9+ you need to set HTML_FILE_EXTENSION to xhtml in order to make\n# the SVG files visible. Older versions of IE do not have SVG support.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nINTERACTIVE_SVG        = NO\n\n# The DOT_PATH tag can be used to specify the path where the dot tool can be\n# found. If left blank, it is assumed the dot tool can be found in the path.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_PATH               = $(DOT_PATH)\n\n# The DOTFILE_DIRS tag can be used to specify one or more directories that\n# contain dot files that are included in the documentation (see the \\dotfile\n# command).\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOTFILE_DIRS           =\n\n# The MSCFILE_DIRS tag can be used to specify one or more directories that\n# contain msc files that are included in the documentation (see the \\mscfile\n# command).\n\nMSCFILE_DIRS           =\n\n# The DOT_GRAPH_MAX_NODES tag can be used to set the maximum number of nodes\n# that will be shown in the graph. If the number of nodes in a graph becomes\n# larger than this value, doxygen will truncate the graph, which is visualized\n# by representing a node as a red box. Note that doxygen if the number of direct\n# children of the root node in a graph is already larger than\n# DOT_GRAPH_MAX_NODES then the graph will not be shown at all. Also note that\n# the size of a graph can be further restricted by MAX_DOT_GRAPH_DEPTH.\n# Minimum value: 0, maximum value: 10000, default value: 50.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_GRAPH_MAX_NODES    = 50\n\n# The MAX_DOT_GRAPH_DEPTH tag can be used to set the maximum depth of the graphs\n# generated by dot. A depth value of 3 means that only nodes reachable from the\n# root by following a path via at most 3 edges will be shown. Nodes that lay\n# further from the root node will be omitted. Note that setting this option to 1\n# or 2 may greatly reduce the computation time needed for large code bases. Also\n# note that the size of a graph can be further restricted by\n# DOT_GRAPH_MAX_NODES. Using a depth of 0 means no depth restriction.\n# Minimum value: 0, maximum value: 1000, default value: 0.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nMAX_DOT_GRAPH_DEPTH    = 0\n\n# Set the DOT_TRANSPARENT tag to YES to generate images with a transparent\n# background. This is disabled by default, because dot on Windows does not seem\n# to support this out of the box.\n#\n# Warning: Depending on the platform used, enabling this option may lead to\n# badly anti-aliased labels on the edges of a graph (i.e. they become hard to\n# read).\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_TRANSPARENT        = NO\n\n# Set the DOT_MULTI_TARGETS tag to YES allow dot to generate multiple output\n# files in one run (i.e. multiple -o and -T options on the command line). This\n# makes dot run faster, but since only newer versions of dot (>1.8.10) support\n# this, this feature is disabled by default.\n# The default value is: NO.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_MULTI_TARGETS      = NO\n\n# If the GENERATE_LEGEND tag is set to YES doxygen will generate a legend page\n# explaining the meaning of the various boxes and arrows in the dot generated\n# graphs.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nGENERATE_LEGEND        = YES\n\n# If the DOT_CLEANUP tag is set to YES doxygen will remove the intermediate dot\n# files that are used to generate the various graphs.\n# The default value is: YES.\n# This tag requires that the tag HAVE_DOT is set to YES.\n\nDOT_CLEANUP            = YES\n"
        },
        {
          "name": "LICENSE.txt",
          "type": "blob",
          "size": 1.5107421875,
          "content": "Copyright (c) 2017 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\nSPDX-License-Identifier: BSD-3-Clause\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\nlist of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\nthis list of conditions and the following disclaimer in the documentation\nand/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "PUBLICATIONS.md",
          "type": "blob",
          "size": 6.72265625,
          "content": "# Publications Using Cutlass\n\n## 2024\n\n- [\"ShadowKV: KV Cache in Shadows for High-Throughput Long-Context LLM Inference\"](https://arxiv.org/abs/2410.21465). Hanshi Sun, Li-Wen Chang, Wenlei Bao, Size Zheng, Ningxin Zheng, Xin Liu, Harry Dong, Yuejie Chi, Beidi Chen. _arXiv_, October 2024.\n\n- [\"FLUX: Fast Software-based Communication Overlap On GPUs Through Kernel Fusion\"](https://arxiv.org/abs/2406.06858). Li-Wen Chang, Wenlei Bao, Qi Hou, Chengquan Jiang, Ningxin Zheng, Yinmin Zhong, Xuanrun Zhang, Zuquan Song, Chengji Yao, Ziheng Jiang, Haibin Lin, Xin Jin, Xin Liu. _arXiv_, June 2024.\n\n- [\"EVT: Accelerating Deep Learning Training with Epilogue Visitor Tree\"](https://dl.acm.org/doi/10.1145/3620666.3651369). Zhaodong Chen, Andrew Kerr, Richard Cai, Jack Kosaian, Haicheng Wu, Yufei Ding, and Yuan Xie. _Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems_, April 2024.\n\n- [\"Faster Neighborhood Attention: Reducing the O(n^2) Cost of Self Attention at the Threadblock Level\"](https://arxiv.org/abs/2403.04690). Ali Hassani, Wen-Mei Hwu, Humphrey Shi. _arXiv_, March 2024.\n\n## 2023\n\n- [\"A Case Study in CUDA Kernel Fusion: Implementing FlashAttention-2 on NVIDIA Hopper Architecture using the CUTLASS Library\"](https://arxiv.org/abs/2312.11918). Ganesh Bikshandi, Jay Shah. _arXiv_, December 2023.\n\n- [\"Benchmarking GPU Tensor Cores on General Matrix Multiplication Kernels through CUTLASS\"](https://www.mdpi.com/2076-3417/13/24/13022). Xuanteng Huang, Xianwei Zhang, Panfei Yang, Nong Xiao. _Journal of Applied Sciences_, December 2023.\n\n- [\"A Speed Odyssey for Deployable Quantization of LLMs\"](https://arxiv.org/abs/2311.09550). Qingyuan Li, Ran Meng, Yiduo Li, Bo Zhang, Liang Li, Yifan Lu, Xiangxiang Chu, Yerui Sun, Yuchen Xie. _arXiv_, November 2023.\n\n- [\"FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning\"](https://arxiv.org/abs/2307.08691). Tri Dao. _Technical Report_, July 2023.\n\n- [\"MegaBlocks: Efficient Sparse Training with Mixture-of-Experts\"](https://arxiv.org/abs/2211.15841). Trevor Gale, Deepak Narayanan, Cliff Young, Matei Zaharia. _Proceedings of the Sixth Machine Learning and Systems_, May 2023.\n\n- [\"ByteTransformer: A High-Performance Transformer Boosted for Variable-Length Inputs\"](https://arxiv.org/abs/2210.03052). Yujia Zhai, Chengquan Jiang, Leyuan Wang, Xiaoying Jia, Shang Zhang, Zizhong Chen, Xin Liu, Yibo Zhu. _Proceedings of the 37th IEEE International Parallel & Distributed Processing Symposium (Best Paper)_, May 2023.\n\n- [\"A Framework for Fine-Grained Synchronization of Dependent GPU Kernels\"](https://arxiv.org/abs/2305.13450). Abhinav Jangda, Saeed Maleki, Maryam Mehri Dehnavi, Madan Musuvathi, Olli Saarikivi. _Computing Research Repository_, May 2023.\n\n- [\"Graphene: An IR for Optimized Tensor Computations on GPUs\"](https://dl.acm.org/doi/pdf/10.1145/3582016.3582018). Hagedorn, Bastian, Bin Fan, Hanfeng Chen, Cris Cecka, Michael Garland, Vinod Grover. _Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems_, March 2023.\n\n- [\"Mixed Precision Post Training Quantization of Neural Networks with Sensitivity Guided Search\"](https://arxiv.org/abs/2302.01382). Clemens JS Schaefer, Elfie Guo, Caitlin Stanton, Xiaofan Zhang, Tom Jablin, Navid Lambert-Shirzad, Jian Li, Chiachen Chou, Siddharth Joshi, Yu Emma Wang. _arXiv_, Feburary 2023.\n\n- [\"Dynamic N:M Fine-Grained Structured Sparse Attention Mechanism\"](https://dl.acm.org/doi/abs/10.1145/3572848.3577500). Zhaodong Chen, Zheng Qu, Yuying Quan, Liu Liu, Yufei Ding, Yuan Xie. _Proceedings of the 28th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming_, Feburary 2023.\n\n- [\"Stream-K: Work-centric Parallel Decomposition for Dense Matrix-Matrix Multiplication on the GPU\"](https://arxiv.org/abs/2301.03598). Muhammad Osama, Duane Merrill, Cris Cecka, Michael Garland, John D. Owens. _arXiv_, January 2023.\n\n## 2022\n\n- [\"GPU Load Balancing\"](https://arxiv.org/abs/2212.08964). Muhammad Osama. _Doctoral dissertation, University of California, Davis_, December 2022.\n\n- [\"Who Says Elephants Can't Run: Bringing Large Scale MoE Models into Cloud Scale Production\"](https://arxiv.org/abs/2211.10017). Young Jin Kim, Rawn Henry, Raffy Fahim, Hany Hassan Awadalla. _Proceedings of the Third Workshop on Simple and Efficient Natural Language Processing_, December 2022.\n\n- [\"Bolt: Bridging the Gap between Auto-tuners and Hardware-native Performance\"](https://arxiv.org/abs/2110.15238). Jiarong Xing, Leyuan Wang, Shang Zhang, Jack Chen, Ang Chen, Yibo Zhu. _Proceedings of the 5th MLSys Conference_, August 2022.\n\n- [\"Recovering single precision accuracy from Tensor Cores while surpassing the FP32 theoretical peak performance\"](https://arxiv.org/abs/2203.03341). Hiroyuki Ootomo, Rio Yokota. _International Journal of High Performance Computing_, March 2022.\n\n- [\"Breaking the Computation and Communication Abstraction Barrier in Distributed Machine Learning Workloads\"](https://arxiv.org/abs/2105.05720). Abhinav Jangda, Jun Huang, Guodong Liu, Amir Hossein Nodehi Sabet, Saeed Maleki, Youshan Miao, Madanlal Musuvathi, Todd Mytkowicz, Olli Sarikivi. _Proceedings of the 27th ACM International Conference on Architectural Support for Programming Languages and Operating Systems_, February 2022.\n\n## 2021\n\n- [\"Arithmetic-intensity-guided fault tolerance for neural network inference on GPUs\"](https://dl.acm.org/doi/abs/10.1145/3458817.3476184). Jack Kosaian, K. V. Rashmi. _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, November 2021.\n\n- [\"Real-time Neural Radiance Caching for Path Tracing\"](https://dl.acm.org/doi/abs/10.1145/3450626.3459812). Thomas Muller, Fabrice Rousselle, Jan Novak, Alex Keller. _ACM Trans. Graph._, August 2021.\n\n## 2020\n\n- [\"Scalable Knowledge Graph Analytics at 136 Petaflop/s\"](https://www.computer.org/csdl/proceedings-article/sc/2020/999800a061/1oeORDgCM0g). Ramakrishnan Kannan, Piyush Sao, Hao Lu, Drahomira Herrmannova, Vijay Thakkar,  Robert Patton, Richard Vuduc, Thomas Potok. _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, November 2020.\n\n- [\"Accelerating Sparse DNN Models without Hardware-Support via Tile-Wise Sparsity\n\"](https://arxiv.org/abs/2008.13006). Cong Guo, Bo Yang Hsueh, Jingwen Leng, Yuxian Qiu, Yue Guan, Zehuan Wang, Xiaoying Jia, Xipeng Li, Minyi Guo, Yuhao Zhu. _Proceedings of the International Conference for High Performance Computing, Networking, Storage and Analysis_, November 2020.\n\n- [\"Strassen's Algorithm Reloaded on GPUs\"](https://dl.acm.org/doi/10.1145/3372419). Jianyu Huang, Chenhan D. Yu, Robert A. van de Geijn. _ACM Transactions on Mathematical Software_, March 2020.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 28.3701171875,
          "content": "![ALT](./media/images/gemm-hierarchy-with-epilogue-no-labels.png \"Complete CUDA GEMM decomposition\")\n\n# CUTLASS 3.6.0\n\n_CUTLASS 3.6.0 - October 2024_\n\nCUTLASS is a collection of CUDA C++ template abstractions for implementing\nhigh-performance matrix-matrix multiplication (GEMM) and related computations at all levels \nand scales within CUDA. It incorporates strategies for hierarchical decomposition and \ndata movement similar to those used to implement cuBLAS and cuDNN.  CUTLASS decomposes \nthese \"moving parts\" into reusable, modular software components abstracted by C++ template \nclasses.  Primitives for different levels of a conceptual parallelization hierarchy\ncan be specialized and tuned via custom tiling sizes, data types,\nand other algorithmic policy. The resulting flexibility simplifies their use\nas building blocks within custom kernels and applications.\n\nTo support a wide variety of applications, CUTLASS provides extensive support for\nmixed-precision computations, providing specialized data-movement and\nmultiply-accumulate abstractions for half-precision floating\npoint (FP16), BFloat16 (BF16), Tensor Float 32 (TF32),\nsingle-precision floating point (FP32),\n[FP32 emulation via tensor core instruction](./examples/27_ampere_3xtf32_fast_accurate_tensorop_gemm),\ndouble-precision floating\npoint (FP64) types, integer data types (4b and 8b), and binary data types (1b).\nCUTLASS demonstrates warp-synchronous matrix multiply operations\ntargeting the programmable, high-throughput _Tensor Cores_ implemented by\nNVIDIA's Volta, Turing, Ampere, and Hopper architectures.\n\nSee the [Quick Start Guide](./media/docs/quickstart.md) to get started quickly.\n\nSee the [functionality listing](./media/docs/functionality.md) for the list of operations\nsupported at each level of the execution model hierarchy.\n\nCUTLASS 3.0 introduced a new core library, CuTe, to describe and manipulate tensors of threads and data.\nCuTe is a collection of C++ CUDA template abstractions for defining and operating on hierarchically multidimensional layouts of threads and data. CuTe provides `Layout` and `Tensor` objects that compactly package the type, shape, memory space, and layout of data, while performing the complicated indexing for the user. This lets programmers focus on the logical descriptions of their algorithms while CuTe does the mechanical bookkeeping for them. With these tools, we can quickly design, implement, and modify all dense linear algebra operations.\n\nThe core abstractions of CuTe are hierarchically multidimensional layouts which can be composed with data arrays to represent tensors. The representation of layouts is powerful enough to represent nearly everything we need to implement efficient dense linear algebra. Layouts can also be combined and manipulated via functional composition, on which we build a large set of common operations such as tiling and partitioning.\n\nCUTLASS 3.0 and beyond adopts CuTe throughout the GEMM hierarchy in its templates. This greatly simplifies the design\nand improves code composability and readability. More documentation specific to CuTe can be found in its [dedicated documentation directory](./media/docs/cute/00_quickstart.md).\n\nIn addition to GEMMs, CUTLASS implements high-performance convolution via the implicit GEMM algorithm. Implicit GEMM is the formulation of a convolution operation as a GEMM thereby taking advantage of CUTLASS's modular GEMM pipeline. This allows CUTLASS to build convolutions by reusing highly-optimized GEMM components.\n\n\n# What's New in CUTLASS 3.6\n\nCUTLASS 3.6.0 is an update to CUTLASS adding:\n\n- [Hopper structured sparse GEMM](./examples/62_hopper_sparse_gemm/62_hopper_sparse_gemm.cu).\n  + [FP16](./test/unit/gemm/device/sm90_sparse_gemm_f16_f16_f32_tensor_op_f32.cu)\n  + [FP8](./test/unit/gemm/device/sm90_sparse_gemm_f8_f8_f32_tensor_op_f32.cu)\n  + [INT8](./test/unit/gemm/device/sm90_sparse_gemm_s8_s8_s32_tensor_op_s32.cu)\n  + [TF32](./test/unit/gemm/device/sm90_sparse_gemm_tf32_tf32_f32_tensor_op_f32.cu)\n- A refactor to the CUTLASS 3.x convolution `kernel::ConvUniversal` [API](./include/cutlass/conv/kernel/sm90_implicit_gemm_tma_warpspecialized.hpp) to bring it in line with `gemm::GemmUniversal`. Now the 3.x convolution API is no longer considered as a beta API.\n- [An improved mixed input GEMM](./examples/55_hopper_mixed_dtype_gemm/README.md) and a [lookup table implementation](./examples/55_hopper_mixed_dtype_gemm/55_hopper_int4_fp8_gemm.cu) for `INT4`x`FP8` scale-only mode.\n- [EVT nodes for Top-K selection and softmax](./include/cutlass/epilogue/fusion/sm90_visitor_topk_softmax.hpp) and [GEMM example using those](./examples/61_hopper_gemm_with_topk_and_softmax/61_hopper_gemm_with_topk_and_softmax.cu).\n- [Programmatic Dependent Launch](./include/cutlass/arch/grid_dependency_control.h) (PDL) that leverages a new Hopper feature to speedup two back-to-back kernels, and its corresponding [documentations](./media/docs/dependent_kernel_launch.md).\n- [A new debugging tool, synclog](./include/cutlass/arch/synclog.hpp), for dumping out all synchronization events from within a kernel to a file. Please see [synclog documentation](./media/docs/utilities.md#debugging-asynchronous-kernels-with-cutlasss-built-in-synclog-tool) for details.\n- A new TMA-enabled [epilogue](./include/cutlass/epilogue/collective/sm90_epilogue_array_tma_warpspecialized.hpp) for grouped GEMM that brings significant performance improvement, as well as its EVT support.\n- A SIMT-enabled pointer-array [epilogue](./include/cutlass/epilogue/collective/sm70_epilogue_vectorized_array.hpp).\n- A new [Ping-Pong kernel schedule for Grouped GEMM](./include/cutlass/gemm/kernel/sm90_gemm_array_tma_warpspecialized_pingpong.hpp) and some other optimizations.\n- [A new instantiation strategy for CUTLASS profiler kernels](./python/cutlass_library/sm90_shapes.py) along with [improved documentation for instantiation level in CUTLASS profiler](./media/docs/profiler.md#instantiating-more-kernels-with-hopper).\n- A new hardware support for comparisons and computations of [`cutlass::bfloat16_t`](./include/cutlass/bfloat16.h)\n- Fixed use of isnan on Windows for [`half_t`](./test/unit/core/functional.cu).\n\nMinimum requirements:\n\n- Architecture: Volta\n- Compiler: Must support at least C++17\n- CUDA Toolkit version: 11.4\n\nStarting from CUTLASS 3.0, CUTLASS removed support for the following:\n\n- Maxwell and Pascal GPU architectures\n- Ubuntu 16.04\n- CUDA 10.2\n- C++ language versions less than 17.\n\n**See the [CHANGELOG](CHANGELOG.md) for a detailed listing of releases and updates.**\n\n# Performance\n\n<p align=\"center\"><img src=media/images/cutlass-3.5.1-gemm-peak-performance.png></p>\n<p align=\"center\"><img src=media/images/cutlass-3.5.1-gemm-peak-performance-fp8.png></p>\n\nCUTLASS primitives are very efficient.  When used to construct device-wide GEMM kernels,\nthey exhibit peak performance comparable to cuBLAS for scalar GEMM\ncomputations. The above figure shows the continual CUTLASS performance improvements \non an [NVIDIA H100](https://www.nvidia.com/en-us/data-center/h100/) (NVIDIA Hopper architecture) since\nCUTLASS 3.1.\nCUTLASS 3.5.1 was compiled with the [CUDA 12.5u1 Toolkit](https://developer.nvidia.com/cuda-downloads). \nTensor Core operations are implemented using CUDA's \n[mma](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma) and\n[wgmma](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#asynchronous-warpgroup-level-matrix-instructions) instructions.\n\n<p align=\"center\"><img src=media/images/cutlass-2.9-implicit-gemm-performance.png></p>\n\nWhen using CUTLASS building blocks to construct device-wide implicit gemm (Fprop, Dgrad, and Wgrad)\nkernels, CUTLASS performance is also comparable to cuDNN when running Resnet-50 layers on an [NVIDIA A100](https://www.nvidia.com/en-us/data-center/a100/)\nas shown in the above figure.  Tensor Core operations are implemented using CUDA's\n[mma instruction](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#warp-level-matrix-instructions-mma).\n\n# Compatibility\n\nCUTLASS requires a C++17 host compiler and \nperforms best when built with the [**CUDA 12.4 Toolkit**](https://developer.nvidia.com/cuda-downloads).\nIt is also compatible with CUDA 11.4, CUDA 11.5, CUDA 11.6, CUDA 11.7, CUDA 11.8, CUDA 12.0, CUDA 12.1, CUDA 12.2.2, CUDA 12.3.1 and CUDA 12.3.2.\n\n## Operating Systems\nWe have tested the following environments.\n\n|**Operating System** | **Compiler** |\n|-----------------|----------|\n| Ubuntu 18.04 | GCC 7.5.0  |\n| Ubuntu 20.04 | GCC 10.3.0 |\n| Ubuntu 22.04 | GCC 11.2.0 |\n| Ubuntu 22.04 | Clang 10.0.0 |\n| Ubuntu 22.04 | Clang 14.0.6 |\n| Ubuntu 22.04 | Clang 17.0.6 |\n| Windows 10.0 | Visual Studio 2019 v16.11.27 |\n\nNote: GCC 8.5.0 has known regressions regarding fold expressions and overloaded operators. Using GCC 7.5.0 or (preferred) GCC >= 9 is recommended.\n\n## Hardware\nCUTLASS runs successfully on the following NVIDIA GPUs, and it is expected to be efficient on Volta, Turing, Ampere, Ada, and Hopper architecture based NVIDIA GPUs.\n\n|**GPU**|**CUDA Compute Capability**|**Minimum CUDA Toolkit Required by CUTLASS-3**|\n|---|---|---|\n|NVIDIA V100 Tensor Core GPU            |7.0|11.4|\n|NVIDIA TitanV                          |7.0|11.4|\n|NVIDIA GeForce RTX 2080 TI, 2080, 2070 |7.5|11.4|\n|NVIDIA T4                              |7.5|11.4|\n|NVIDIA A100 Tensor Core GPU            |8.0|11.4|\n|NVIDIA A10                             |8.6|11.4|\n|NVIDIA GeForce RTX 3090                |8.6|11.4|\n|NVIDIA GeForce RTX 4090                |8.9|11.8|\n|NVIDIA L40                             |8.9|11.8|\n|NVIDIA H100 Tensor Core GPU            |9.0|11.8|\n\n## Target Architecture\n\nIn general, PTX code generated for one target architecture can be run on future architectures (i.e., it is forward compatible).  However, CUDA 12.0 introduced the concept of \"architecture-accelerated features\" whose PTX does not have forward compatibility guarantees. Several Hopper PTX instructions fall under this category of architecture-accelerated features, and thus require a `sm_90a` target architecture (note the \"a\" appended). For more details on this and other architecture-accelerated instructions, please refer to the [CUDA Documentation](https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#feature-availability).\n\nThe target architecture information is passed on to CUTLASS via the cmake flag `CUTLASS_NVCC_ARCHS`. In order to maximize performance on Hopper GH100, users are required to build CUTLASS with `90a` as the target architecture. If a user accidentally builds a kernel which uses SM90a features (e.g. Hopper Tensor Core Instructions), using the SM90 target (note the lack of \"a\"), with either CUDA Toolkit 12 or 11.8, the kernel is expected to fail with a runtime error.\n\n```\ncmake .. -DCUTLASS_NVCC_ARCHS=\"90a\" \n```\n\nPlease refer to the [functionality documentation](./media/docs/functionality.md) for details on which kernels require which target architectures.\n\n# Documentation\n\nCUTLASS is described in the following documents and the accompanying\n[Doxygen documentation](https://nvidia.github.io/cutlass).\n\n- [Quick Start Guide](./media/docs/quickstart.md) - build and run CUTLASS\n- [Functionality](./media/docs/functionality.md) - summarizes functionality available in CUTLASS\n- [Efficient GEMM in CUDA](./media/docs/efficient_gemm.md) - describes how GEMM kernels may be implemented efficiently in CUDA\n- [CUTLASS 3.x Design](./media/docs/cutlass_3x_design.md) - describes the CUTLASS 3.x design, its benefits, and how CuTe enables us to write much more composable components\n- [GEMM API 3.x](./media/docs/gemm_api_3x.md) - describes the CUTLASS 3.x GEMM model and C++ template concepts\n- [GEMM API 2.x](./media/docs/gemm_api.md) - describes the CUTLASS 2.x GEMM model and C++ template concepts\n- [Implicit GEMM Convolution](./media/docs/implicit_gemm_convolution.md) - describes 2-D and 3-D convolution in CUTLASS\n- [Code Organization](./media/docs/code_organization.md) - describes the organization and contents of the CUTLASS project\n- [Terminology](./media/docs/terminology.md) - describes terms used in the code\n- [Programming Guidelines](./media/docs/programming_guidelines.md) - guidelines for writing efficient modern CUDA C++\n- [Fundamental types](./media/docs/fundamental_types.md) - describes basic C++ classes used in CUTLASS to represent numeric quantities and arrays\n- [Layouts](./media/docs/layout.md) - describes layouts of matrices and tensors in memory\n- [Tile Iterators](./media/docs/tile_iterator_concept.md) - describes C++ concepts for iterating over tiles of matrices in memory\n- [CUTLASS Profiler](./media/docs/profiler.md) - command-line driven profiling application\n- [CUTLASS Utilities](./media/docs/utilities.md) - additional templates used to facilate rapid development\n- [Dependent kernel launch](./media/docs/dependent_kernel_launch.md) - describes a new feature in Hopper which allows overlapping dependent \nkernels in the same stream, and how it is used in CUTLASS.\n\n# Resources\nWe have also described the structure of an efficient GEMM in our talk at the\n[GPU Technology Conference 2018](http://on-demand.gputechconf.com/gtc/2018/presentation/s8854-cutlass-software-primitives-for-dense-linear-algebra-at-all-levels-and-scales-within-cuda.pdf).\n\n - [CUTLASS: Software Primitives for Dense Linear Algebra at All Levels and Scales within CUDA](https://www.nvidia.com/en-us/on-demand/session/gtcsiliconvalley2018-s8854/)\n - [Developing CUDA Kernels to Push Tensor Cores to the Absolute Limit on NVIDIA A100](https://www.nvidia.com/en-us/on-demand/session/gtcsj20-s21745/)\n - [Accelerating Convolution with Tensor Cores in CUTLASS](https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s31883/)\n - [Accelerating Backward Data Gradient by Increasing Tensor Core Utilization in CUTLASS](https://www.nvidia.com/en-us/on-demand/session/gtcspring22-s41996/)\n - [CUTLASS: Python API, Enhancements, and NVIDIA Hopper](https://www.nvidia.com/en-us/on-demand/session/gtcfall22-a41131/)\n\n# Building CUTLASS\n\nCUTLASS is a header-only template library and does not need to be built to be used by other\nprojects. Client applications should target CUTLASS's `include/` directory in their include\npaths.\n\nCUTLASS unit tests, examples, and utilities can be build with CMake.\nThe minimum version of CMake is given in the [Quickstart guide](./media/docs/quickstart.md).\nMake sure the `CUDACXX` environment  variable points to NVCC in the CUDA Toolkit installed\non your system.\n\n```bash\n$ export CUDACXX=${CUDA_INSTALL_PATH}/bin/nvcc\n```\n\nCreate a build directory within the CUTLASS project, then run CMake. By default CUTLASS will build kernels\nfor CUDA architecture versions 5.0, 6.0, 6.1, 7.0, 7.5, 8.0, 8.6, 8.9, and 9.0.\nTo reduce compile time you can specify\nthe architectures to build CUTLASS for by changing the CMake configuration setting\n`CUTLASS_NVCC_ARCHS`.\n\n```bash\n$ mkdir build && cd build\n\n$ cmake .. -DCUTLASS_NVCC_ARCHS=80               # compiles for NVIDIA's Ampere Architecture\n```\n\nFrom the `build/` directory, compile and run the CUTLASS unit tests by building the target `test_unit` with make.\n\nThe unit tests are organized as several binaries mirroring the top-level namespaces of CUTLASS,\nand they may be executed in parallel via make's `-j` command line argument.\n\n```bash\n$ make test_unit -j\n...\n...\n...\n[----------] Global test environment tear-down\n[==========] 946 tests from 57 test cases ran. (10812 ms total)\n[  PASSED  ] 946 tests.\n```\n\nAll tests should pass on supported platforms, though the exact number of tests may vary over time.\n\n\n# Project Structure\n\nCUTLASS is arranged as a header-only library along with Utilities, Tools, Examples, and unit tests. \n[Doxygen documentation](https://nvidia.github.io/cutlass) provides a complete list of files, classes, \nand template concepts defined in the CUTLASS project.\n\nA detailed explanation of the source code organization may be found in the \n[CUTLASS documentation](./media/docs/code_organization.md), but several main components are summarized below.\n\n## CUTLASS Template Library\n\n```\ninclude/                     # client applications should target this directory in their build's include paths\n\n  cutlass/                   # CUDA Templates for Linear Algebra Subroutines and Solvers - headers only\n\n    arch/                    # direct exposure of architecture features (including instruction-level GEMMs)\n\n    conv/                    # code specialized for convolution\n\n    epilogue/                # code specialized for the epilogue of gemm/convolution\n\n    gemm/                    # code specialized for general matrix product computations\n\n    layout/                  # layout definitions for matrices, tensors, and other mathematical objects in memory\n\n    platform/                # CUDA-capable Standard Library components\n\n    reduction/               # bandwidth-limited reduction kernels that do not fit the \"gemm\" model\n\n    thread/                  # simt code that can be performed within a CUDA thread\n    \n    transform/               # code specialized for layout, type, and domain transformations\n\n    *                        # core vocabulary types, containers, and basic numeric operations\n\n  cute/                      # CuTe Layout, layout algebra, MMA/Copy atoms, tiled MMA/Copy\n\n    algorithm/               # Definitions of core operations such as copy, gemm, and operations on cute::tuples\n\n    arch/                    # Bare bones PTX wrapper structs for copy and math instructions\n\n    atom/                    # Meta-information either link to or built from arch/ operators\n\n      mma_atom.hpp           # cute::Mma_Atom and cute::TiledMma\n\n      copy_atom.hpp          # cute::Copy_Atom and cute::TiledCopy\n\n      *sm*.hpp               # Arch specific meta-information for copy and math operations\n\n    *                        # Core library types such as Shape, Stride, Layout, Tensor, and associated operations\n\n```\n\n### CUTLASS SDK Examples\n\n[CUTLASS SDK examples](./examples) apply CUTLASS templates to implement basic computations.\n\n### Tools\n\n```\ntools/\n  library/                   # CUTLASS Instance Library - contains instantiations of all supported CUTLASS templates\n    include/\n      cutlass/\n        library/\n\n  profiler/                  # CUTLASS Profiler         - command-line utility for executing operations in the\n                             #                            CUTLASS Library\n  \n  util/                      # CUTLASS Utilities        - contains numerous helper classes for\n    include/                 #                            manging tensors in device memory, reference\n      cutlass/               #                            implementations for GEMM, random initialization\n        util/                #                            of tensors, and I/O.\n```\n\n### Test\n\nThe `test/unit/` directory consist of unit tests implemented with Google Test that demonstrate\nbasic usage of Core API components and complete tests of the CUTLASS GEMM computations.\n\nInstructions for building and running the Unit tests are described in the [Quickstart guide](./media/docs/quickstart.md).\n\n# Performance Profiling\n\nThe `tools/profiler/` directory contains a command-line utility for launching each of the GEMM kernels.\nIt can be built as follows:\n\n```bash\n$ make cutlass_profiler -j16\n```\n## Building all GEMM and Convolution kernels (_long_ build times)\n\nBy default, only one tile size is instantiated for each data type, math instruction, and layout.\nTo instantiate all, set the following environment variable when running CMake from an empty `build/` directory.\nBeware, this results in *tens of thousands* of kernels and long build times. \nThis would also result in a large binary size and on some platforms linker to fail on building the library.\nTherefore, it's highly recommended to generate only a subset of kernels as demonstrated in the sub-section below.\n```bash\n$ cmake .. -DCUTLASS_NVCC_ARCHS=90a -DCUTLASS_LIBRARY_KERNELS=all\n...\n$ make cutlass_profiler -j16\n```\n\n## Building a subset of GEMM and Convolution kernels (_reduced_ build times)\n\nTo compile strictly one kernel or a small set of kernels, a comma-delimited list of kernel names with \nwildcard characters may be used to reduce the set of kernels. The following examples show building exactly one\nor a subset of kernels for NVIDIA Ampere and Turing architecture:\n\n### Building a subset Tensor Core GEMM kernels\n\nTo compile a subset of Tensor Core GEMM kernels with FP32 accumulation and FP16 input targeting NVIDIA Ampere and Turing architecture, \nuse the below cmake command line:\n```bash\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*gemm_f16_*_nt_align8\n...\n$ make cutlass_profiler -j16\n```\n\nExample command line for profiling a subset of Tensor Core GEMM kernels is as follows:\n```bash\n./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*gemm_f16_*_nt_align8 --m=3456 --n=4096 --k=4096\n\n...\n=============================\n  Problem ID: 1\n\n        Provider: CUTLASS\n   OperationKind: gemm\n       Operation: cutlass_tensorop_s1688gemm_f16_256x128_32x2_nt_align8\n\n          Status: Success\n    Verification: ON\n     Disposition: Passed\n\nreference_device: Passed\n          cuBLAS: Passed\n\n       Arguments: --gemm_kind=universal --m=3456 --n=4096 --k=4096 --A=f16:column --B=f16:row --C=f32:column --alpha=1  \\\n                  --beta=0 --split_k_slices=1 --batch_count=1 --op_class=tensorop --accum=f32 --cta_m=256 --cta_n=128  \\\n                  --cta_k=32 --stages=2 --warps_m=4 --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=8 --min_cc=75  \\\n                  --max_cc=1024\n\n           Bytes: 118489088  bytes\n           FLOPs: 115992428544  flops\n\n         Runtime: 1.55948  ms\n          Memory: 70.7616 GiB/s\n\n            Math: 74378.8 GFLOP/s\n\n\n\n=============================\n...\n```\n\n### Building one CUDA Core GEMM kernel\n\nTo compile one SGEMM kernel targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:\n```bash\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sgemm_128x128_8x2_nn_align1\n...\n$ make cutlass_profiler -j16\n```\n\nExample command line for profiling single SGEMM CUDA kernel is as follows:\n```bash\n$ ./tools/profiler/cutlass_profiler --kernels=sgemm --m=3456 --n=4096 --k=4096\n\n=============================\n  Problem ID: 1\n\n        Provider: CUTLASS\n   OperationKind: gemm\n       Operation: cutlass_simt_sgemm_128x128_8x2_nn_align1\n\n          Status: Success\n    Verification: ON\n     Disposition: Passed\n\n          cuBLAS: Passed\n\n       Arguments: --m=3456 --n=4096 --k=4096 --A=f32:column --B=f32:column --C=f32:column --alpha=1 --beta=0 --split_k_slices=1  \\\n                  --batch_count=1 --op_class=simt --accum=f32 --cta_m=128 --cta_n=128 --cta_k=8 --stages=2 --warps_m=4  \\\n                  --warps_n=2 --warps_k=1 --inst_m=1 --inst_n=1 --inst_k=1 --min_cc=50 --max_cc=1024\n\n           Bytes: 180355072  bytes\n           FLOPs: 115992428544  flops\n\n         Runtime: 6.73655  ms\n          Memory: 24.934 GiB/s\n\n            Math: 17218.4 GFLOP/s\n\n=============================\n```\n\n### Building a subset of Tensor Core Convolution kernels\n\nTo compile a subset of Tensor core convolution kernels implementing forward propagation (fprop) with FP32 accumulation \nand FP16 input targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:\n```bash\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_tensorop_s*fprop_optimized_f16\n...\n$ make cutlass_profiler -j16\n```\n\nExample command line for profiling a subset of Tensor Core convolution kernels is as follows:\n\n```bash\n$ ./tools/profiler/cutlass_profiler --kernels=cutlass_tensorop_s*fprop_optimized_f16 --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n\n...\n=============================\n  Problem ID: 1\n\n        Provider: CUTLASS\n   OperationKind: conv2d\n       Operation: cutlass_tensorop_s16816fprop_optimized_f16_128x128_32x5_nhwc\n\n          Status: Success\n    Verification: ON\n     Disposition: Passed\n\nreference_device: Passed\n\n       Arguments: --conv_kind=fprop --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3 --p=224 --q=224 --pad_h=1 --pad_w=1  \\\n                  --stride_h=1 --stride_w=1 --dilation_h=1 --dilation_w=1 --Activation=f16:nhwc --Filter=f16:nhwc --Output=f32:nhwc  \\\n                  --conv_mode=cross --iterator_algorithm=optimized --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1  \\\n                  --eq_gemm_provider=none --op_class=tensorop --accum=f32 --cta_m=128 --cta_n=128 --cta_k=32 --stages=5  \\\n                  --warps_m=2 --warps_n=2 --warps_k=1 --inst_m=16 --inst_n=8 --inst_k=16 --min_cc=80 --max_cc=1024\n\n           Bytes: 1130659840  bytes\n           FLOPs: 118482796544  flops\n\n         Runtime: 0.711496  ms\n          Memory: 1479.99 GiB/s\n\n            Math: 166526 GFLOP/s\n\n=============================\n...\n```\n\n\n### Building one Convolution CUDA kernel\n\nTo compile and run one CUDA Core convolution kernel implementing forward propagation (fprop) with F32 accumulation \nand FP32 input targeting NVIDIA Ampere and Turing architecture, use the below cmake command line:\n```bash\n$ cmake .. -DCUTLASS_NVCC_ARCHS='75;80' -DCUTLASS_LIBRARY_KERNELS=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc\n...\n$ make cutlass_profiler -j16\n```\n\nExample command line for profiling one CUDA Core convolution kernel:\n\n```bash\n$ ./tools/profiler/cutlass_profiler --kernels=cutlass_simt_sfprop_optimized_128x128_8x2_nhwc --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3\n\n\n=============================\n  Problem ID: 1\n\n        Provider: CUTLASS\n   OperationKind: conv2d\n       Operation: cutlass_simt_sfprop_optimized_128x128_8x2_nhwc\n\n          Status: Success\n    Verification: ON\n     Disposition: Passed\n\nreference_device: Passed\n\n       Arguments: --conv_kind=fprop --n=8 --h=224 --w=224 --c=128 --k=128 --r=3 --s=3 --p=224 --q=224 --pad_h=1 --pad_w=1  \\\n                  --stride_h=1 --stride_w=1 --dilation_h=1 --dilation_w=1 --Activation=f32:nhwc --Filter=f32:nhwc --Output=f32:nhwc  \\\n                  --conv_mode=cross --iterator_algorithm=optimized --alpha=1 --beta=0 --split_k_mode=serial --split_k_slices=1  \\\n                  --eq_gemm_provider=none --op_class=simt --accum=f32 --cta_m=128 --cta_n=128 --cta_k=8 --stages=2 --warps_m=4  \\\n                  --warps_n=2 --warps_k=1 --inst_m=1 --inst_n=1 --inst_k=1 --min_cc=50 --max_cc=1024\n\n           Bytes: 2055798784  bytes\n           FLOPs: 118482796544  flops\n\n         Runtime: 7.34266  ms\n          Memory: 260.752 GiB/s\n\n            Math: 16136.2 GFLOP/s\n\n\n=============================\n\n```\n\n## More Details on Compiling CUTLASS Kernels and CUTLASS Profiler\n- Please follow the links for more CMake examples on selectively compiling CUTLASS kernels:\n  - [GEMM CMake Examples](./media/docs/quickstart.md#gemm-cmake-examples) \n  - [Implicit GEMM convolution CMake Examples](./media/docs/quickstart.md#convolution-cmake-examples)\n- [Further details about the CUTLASS Profiler are described here.](./media/docs/profiler.md)\n\n\n# About\n\nCUTLASS is released by NVIDIA Corporation as Open Source software under the \n[3-clause \"New\" BSD license](LICENSE.txt).\n\n# Contributors\n\nThe official list of CUTLASS developers and contributors is available here: [CONTRIBUTORS](CONTRIBUTORS.md).\n\n# Copyright\n\nCopyright (c) 2017 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\nSPDX-License-Identifier: BSD-3-Clause\n\n```\n  Redistribution and use in source and binary forms, with or without\n  modification, are permitted provided that the following conditions are met:\n\n  1. Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n\n  2. Redistributions in binary form must reproduce the above copyright notice,\n  this list of conditions and the following disclaimer in the documentation\n  and/or other materials provided with the distribution.\n\n  3. Neither the name of the copyright holder nor the names of its\n  contributors may be used to endorse or promote products derived from\n  this software without specific prior written permission.\n\n  THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n  AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n  IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n  DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n  FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n  DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n  SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n  CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n  OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n  OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n```\n"
        },
        {
          "name": "bin2hex.cmake",
          "type": "blob",
          "size": 2.5703125,
          "content": "# Copyright (c) 2019 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this\n# list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n# this list of conditions and the following disclaimer in the documentation\n# and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n# A small utility function which generates a C-header from an input file\nfunction(FILE_TO_C_STRING FILENAME VARIABLE_NAME OUTPUT_STRING ZERO_TERMINATED)\n  FILE(READ \"${FILENAME}\" HEX_INPUT HEX)\n  if (${ZERO_TERMINATED})\n    string(APPEND HEX_INPUT \"00\")\n  endif()\n\n  string(REGEX REPLACE \"(....)\" \"\\\\1\\n\" HEX_OUTPUT ${HEX_INPUT})\n  string(REGEX REPLACE \"([0-9a-f][0-9a-f])\" \"char(0x\\\\1),\" HEX_OUTPUT ${HEX_OUTPUT})\n\n  set(HEX_OUTPUT \"static char const ${VARIABLE_NAME}[] = {\\n  ${HEX_OUTPUT}\\n};\\n\")\n\n  set(${OUTPUT_STRING} \"${HEX_OUTPUT}\" PARENT_SCOPE)\nendfunction()\n\n# message(\"Create header file for ${FILE_IN}\")\n# message(\"Create header file for ${FILE_OUT}\")\nfile_to_c_string(${FILE_IN} ${VARIABLE_NAME} OUTPUT_STRING ZERO_TERMINATED)\n\nset(RESULT \"#pragma once\\n\")\nstring(APPEND RESULT \"namespace cutlass {\\n\")\nstring(APPEND RESULT \"namespace nvrtc {\\n\")\nstring(APPEND RESULT \"${OUTPUT_STRING}\")\nstring(APPEND RESULT \"} // namespace nvrtc\\n\")\nstring(APPEND RESULT \"} // namespace cutlass\\n\")\nfile(WRITE \"${FILE_OUT}\" \"${RESULT}\")\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "cuBLAS.cmake",
          "type": "blob",
          "size": 4.2587890625,
          "content": "# Copyright (c) 2017 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this\n# list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n# this list of conditions and the following disclaimer in the documentation\n# and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nmessage(STATUS \"Configuring cublas ...\")\n\nif((DEFINED CUTLASS_ENABLE_CUBLAS AND NOT CUTLASS_ENABLE_CUBLAS) OR\n   (DEFINED CUBLAS_ENABLED AND NOT CUBLAS_ENABLED))\n  \n  # Don't add cuBLAS if it's defined and false, assume it's not found.\n\n  set(CUBLAS_FOUND OFF)\n  message(STATUS \"cuBLAS Disabled.\")\n\nelseif(NOT TARGET cublas)\n \n  find_path(\n    _CUBLAS_INCLUDE_DIR\n    NAMES cublas_v2.h\n    HINTS\n      ${CUBLAS_INCLUDE_PATH}\n      ENV CUBLAS_INCLUDE_PATH\n      ${CUBLAS_PATH}\n      ENV CUBLAS_PATH\n      ${CUDA_TOOLKIT_ROOT_DIR}\n    PATH_SUFFIXES\n      include\n    )\n\n  find_library(\n    _CUBLAS_LIBRARY\n    NAMES cublas\n    HINTS\n      ${CUBLAS_LIBRARY_PATH}\n      ENV CUBLAS_LIBRARY_PATH\n      ${_CUBLAS_INCLUDE_DIR}/..\n      ${CUBLAS_PATH}\n      ENV CUBLAS_PATH\n      ${CUDA_TOOLKIT_ROOT_DIR}\n    PATH_SUFFIXES\n      lib64\n      lib/x64\n      lib\n    )\n\n  if(_CUBLAS_INCLUDE_DIR AND _CUBLAS_LIBRARY)\n\n    message(STATUS \"cuBLAS: ${_CUBLAS_LIBRARY}\")\n    message(STATUS \"cuBLAS: ${_CUBLAS_INCLUDE_DIR}\")\n    \n    set(CUBLAS_FOUND ON CACHE INTERNAL \"cublas Library Found\")\n    set(CUBLAS_LIBRARY ${_CUBLAS_LIBRARY})\n    set(CUBLAS_INCLUDE_DIR ${_CUBLAS_INCLUDE_DIR})\n\n  else()\n\n    message(STATUS \"cublas not found.\")\n    set(CUBLAS_FOUND OFF CACHE INTERNAL \"cublas Library Found\")\n\n  endif()\n\nendif()\n\nset(CUTLASS_ENABLE_CUBLAS ${CUBLAS_FOUND} CACHE BOOL \"Enable CUTLASS to build with cuBLAS library.\")\n\nif(CUTLASS_ENABLE_CUBLAS AND NOT CUBLAS_FOUND)\n  message(FATAL_ERROR \"CUTLASS_ENABLE_CUBLAS enabled but cuBLAS library could not be found.\")\nendif()\n\nif(CUTLASS_ENABLE_CUBLAS AND NOT TARGET cublas)\n\n  if(WIN32)\n    add_library(cublas STATIC IMPORTED GLOBAL)\n  else()\n    add_library(cublas SHARED IMPORTED GLOBAL)\n  endif()\n\n  add_library(nvidia::cublas ALIAS cublas)\n\n  set_property(\n    TARGET cublas\n    PROPERTY IMPORTED_LOCATION\n    ${CUBLAS_LIBRARY})\n    \n  target_include_directories(\n    cublas\n    INTERFACE\n    $<INSTALL_INTERFACE:include>\n    $<BUILD_INTERFACE:${CUBLAS_INCLUDE_DIR}>)\n\n  find_library(\n    _CUBLASLT_LIBRARY\n    NAMES cublasLt\n    HINTS\n      ${CUBLAS_LIBRARY_PATH}\n      ENV CUBLAS_LIBRARY_PATH\n      ${_CUBLAS_INCLUDE_DIR}/..\n      ${CUBLAS_PATH}\n      ENV CUBLAS_PATH\n      ${CUDA_TOOLKIT_ROOT_DIR}\n    PATH_SUFFIXES\n      lib64\n      lib/x64\n      lib\n    )\n\n  if(_CUBLASLT_LIBRARY AND NOT TARGET cublasLt)\n\n    if(WIN32)\n      add_library(cublasLt STATIC IMPORTED GLOBAL)\n    else()\n      add_library(cublasLt SHARED IMPORTED GLOBAL)\n    endif()\n    \n    set_property(\n      TARGET cublasLt\n      PROPERTY IMPORTED_LOCATION\n      ${_CUBLASLT_LIBRARY})\n  \n    add_library(nvidia::cublasLt ALIAS cublasLt)\n\n    target_link_libraries(cublas INTERFACE cublasLt)\n\n  endif()\n\nendif()\n\nmessage(STATUS \"Configuring cuBLAS ... done.\")\n"
        },
        {
          "name": "cuDNN.cmake",
          "type": "blob",
          "size": 3.548828125,
          "content": "# Copyright (c) 2017 - 2024 NVIDIA CORPORATION & AFFILIATES. All rights reserved.\n# SPDX-License-Identifier: BSD-3-Clause\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are met:\n#\n# 1. Redistributions of source code must retain the above copyright notice, this\n# list of conditions and the following disclaimer.\n#\n# 2. Redistributions in binary form must reproduce the above copyright notice,\n# this list of conditions and the following disclaimer in the documentation\n# and/or other materials provided with the distribution.\n#\n# 3. Neither the name of the copyright holder nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\n# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\n# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\n# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\n# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\n# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\n# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\n# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\n# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nif(DEFINED CUDNN_ENABLED)\n    set(CUTLASS_ENABLE_CUDNN ${CUDNN_ENABLED} CACHE BOOL \"Enable CUTLASS to build with cuDNN library.\")\nendif()\n\nif(DEFINED CUTLASS_ENABLE_CUDNN AND NOT CUTLASS_ENABLE_CUDNN)\n  return()\nendif()\n  \nmessage(STATUS \"Configuring cuDNN ...\")\n\nfind_path(\n    _CUDNN_INCLUDE_DIR cudnn.h\n    PATHS\n    ${CUDA_TOOLKIT_ROOT_DIR}/include\n    $ENV{CUDNN_PATH}/include\n    $ENV{CUDA_PATH}/include\n    ${CUDNN_PATH}/include\n    /usr/include)\n\nfind_library(\n    _CUDNN_LIBRARY cudnn\n    HINTS\n    ${CUDA_TOOLKIT_ROOT_DIR}/lib64\n    ${CUDA_TOOLKIT_ROOT_DIR}/lib/x64\n    ${CUDA_TOOLKIT_ROOT_DIR}/lib\n    $ENV{CUDNN_PATH}/lib64\n    $ENV{CUDNN_PATH}/lib/x64\n    $ENV{CUDNN_PATH}/lib\n    $ENV{CUDA_PATH}/lib64\n    $ENV{CUDA_PATH}/lib/x64\n    $ENV{CUDA_PATH}/lib\n    ${CUDNN_PATH}/lib64\n    ${CUDNN_PATH}/lib/x64\n    ${CUDNN_PATH}/lib\n    /usr/lib/x86_64-linux-gnu\n    /usr/lib)\n\nif(_CUDNN_INCLUDE_DIR AND _CUDNN_LIBRARY)\n\n    message(STATUS \"cuDNN: ${_CUDNN_LIBRARY}\")\n    message(STATUS \"cuDNN: ${_CUDNN_INCLUDE_DIR}\")\n    \n    set(CUDNN_FOUND ON CACHE INTERNAL \"cuDNN Library Found\")\n\nelse()\n\n    message(STATUS \"cuDNN not found.\")\n    set(CUDNN_FOUND OFF CACHE INTERNAL \"cuDNN Library Found\")\n\nendif()\n\nset(CUTLASS_ENABLE_CUDNN ${CUDNN_FOUND} CACHE BOOL \"Enable CUTLASS to build with cuDNN library.\")\n\nif (CUTLASS_ENABLE_CUDNN AND NOT TARGET cudnn)\n\n  set(CUDNN_INCLUDE_DIR ${_CUDNN_INCLUDE_DIR})\n  set(CUDNN_LIBRARY ${_CUDNN_LIBRARY})\n\n  if(WIN32)\n    add_library(cudnn STATIC IMPORTED GLOBAL)\n  else()\n    add_library(cudnn SHARED IMPORTED GLOBAL)\n  endif()\n\n  add_library(nvidia::cudnn ALIAS cudnn)\n\n  set_property(\n    TARGET cudnn\n    PROPERTY IMPORTED_LOCATION\n    ${CUDNN_LIBRARY})\n    \n  target_include_directories(\n    cudnn\n    INTERFACE\n    $<INSTALL_INTERFACE:include>\n    $<BUILD_INTERFACE:${CUDNN_INCLUDE_DIR}>)\n\nendif()\n\nif(CUTLASS_ENABLE_CUDNN AND NOT CUDNN_FOUND)\n  message(FATAL_ERROR \"CUTLASS_ENABLE_CUDNN enabled but cuDNN library could not be found.\")\nendif()\n\nmessage(STATUS \"Configuring cuDNN ... done.\")\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "pyproject.toml",
          "type": "blob",
          "size": 0.6025390625,
          "content": "[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"nvidia-cutlass\"\nversion = \"3.6.0.0\"\ndescription = \"CUTLASS\"\nreadme = \"README.md\"\nrequires-python = \">=3.8\"\nlicense = {text = \"BSD-3-Clause\"}\nclassifiers = [\n    \"Programming Language :: Python :: 3\",\n    \"License :: OSI Approved :: BSD License\",\n    \"Operating System :: OS Independent\",\n]\ndependencies = [\n  \"cuda-python>=11.8.0\",\n  \"networkx\",\n  \"numpy\",\n  \"pydot\",\n  \"scipy\",\n  \"treelib\"\n]\n\n[project.urls]\n\"Homepage\" = \"https://github.com/nvidia/cutlass\"\n\"Bug Tracker\" = \"https://github.com/nvidia/cutlass/issues\"\n"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "setup.cfg",
          "type": "blob",
          "size": 0.712890625,
          "content": "[metadata]\nname = nvidia-cutlass\nversion = 3.4.0.0\n\n[options]\npackages =\n  cutlass\n  cutlass.backend\n  cutlass.backend.evt\n  cutlass.backend.evt.backend\n  cutlass.backend.evt.frontend\n  cutlass.backend.evt.ir\n  cutlass.backend.evt.passes\n  cutlass.backend.utils\n  cutlass.emit\n  cutlass.epilogue\n  cutlass.op\n  cutlass.utils\n  cutlass_library\n  cutlass_library.source\n  pycute\npackage_dir =\n  cutlass=python/cutlass\n  cutlass_library=python/cutlass_library\n  cutlass_library.source=.\n  pycute=python/pycute\ninclude_package_data = True\n\n[options.package_data]\ncutlass_library.source = include/**/*, examples/**/*, tools/**/*\n\n[options.exclude_package_data]\ncutlass_library.source = include/**/*.py, examples/**/*.py, tools/**/*.py\n"
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}