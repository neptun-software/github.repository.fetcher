{
  "metadata": {
    "timestamp": 1736566085953,
    "page": 76,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "dusty-nv/jetson-inference",
      "stars": 8041,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.28125,
          "content": ".cache\r\n.git\r\n__pycache__\r\n**/__pycache__\r\n**.pyc\r\n**.pyo\r\n**.pyd\r\ndata/\r\n!data/networks/models.json\r\npython/training/classification/data/*\r\npython/training/classification/models/*\r\npython/training/detection/ssd/data/*\r\npython/training/detection/ssd/models/*\r\npython/www/recognizer/data/*"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.07421875,
          "content": "root = true\r\n\r\n[*.{c,cpp,cu,h,hpp,inl}]\r\nindent_style = tab\r\nindent_size = 5"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.0869140625,
          "content": "build/\ndata/networks/\ndata/images/test\ndata/images/qa/results\nlogs/\n*.pyc\n*.tar.gz\n*.pem\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.9677734375,
          "content": "[submodule \"utils\"]\n\tpath = utils\n\turl = https://github.com/dusty-nv/jetson-utils\n\tbranch = master\n[submodule \"tools/camera-capture\"]\n\tpath = tools/camera-capture\n\turl = https://github.com/dusty-nv/camera-capture\n\tbranch = master\n[submodule \"python/training/classification\"]\n\tpath = python/training/classification\n\turl = https://github.com/dusty-nv/pytorch-classification\n\tbranch = master\n[submodule \"python/training/detection\"]\n\tpath = python/training/detection\n\turl = https://github.com/dusty-nv/pytorch-detection\n\tbranch = master\n[submodule \"python/training/segmentation\"]\n\tpath = python/training/segmentation\n\turl = https://github.com/dusty-nv/pytorch-segmentation\n\tbranch = master\n[submodule \"plugins/pose\"]\n\tpath = c/plugins/pose\n\turl = https://github.com/dusty-nv/trt_pose\n[submodule \"docker/containers\"]\n\tpath = docker/containers\n\turl = https://github.com/dusty-nv/jetson-containers\n\tbranch = master\n[submodule \"ros\"]\n\tpath = ros\n\turl = https://github.com/dusty-nv/ros_deep_learning\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 9.5869140625,
          "content": "<img src=\"https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-header.jpg\" width=\"100%\">\n\n# Change Log\n\nMajor updates and new features to this project will be listed in this document.\n\n## May 5, 2023\n\n* [WebRTC](docs/aux-streaming.md#webrtc) support and [WebApp Framework](README.md#webapp-frameworks) tutorials:\n   * [WebRTC Server](docs/webrtc-server.md)\n   * [HTML / JavaScript](docs/webrtc-html.md)\n   * [Flask + REST](docs/webrtc-flask.md)\n   * [Plotly Dashboard](docs/webrtc-dash.md)\n   * [Recognizer (Interactive Training)](docs/webrtc-recognizer.md)\n* Support for [TAO detection models](docs/detectnet-tao.md) in `detectNet`\n* Added [`actionNet`](docs/actionnet.md) (action/activity recognition)\n* Added [`backgroundNet`](docs/backgroundnet.md) (foreground/background segmentation/removal)\n* Added [`objectTracker`](c/tracking) - [IoU object tracking](docs/detectnet-tracking.md) for detectNet\n* [Image Tagging and Multi-Label Classification](docs/imagenet-tagging.md) (support for [`topK`](https://github.com/dusty-nv/jetson-inference/blob/b50bf1d5eefed73acda5c963513e0d8c79d18be3/c/imageNet.h#L201) in imageNet)\n* [Temporal smoothing of classification results](https://github.com/dusty-nv/jetson-inference/blob/b50bf1d5eefed73acda5c963513e0d8c79d18be3/c/imageNet.h#L271) in imageNet\n* Automatic model downloader => [`data/networks/models.json`](data/networks/models.json)\n* Build TensorRT timing cache for quick loading of updated models (or models that share layer configurations)\n* Zero-copy interoperability of Python [cudaImage](https://github.com/dusty-nv/jetson-inference/blob/master/docs/aux-image.md#image-capsules-in-python) with other libraries:\n   * [`__cuda_array_interface__`](docs/aux-image.md#cuda-array-interface) (Numba, PyTorch, CuPy, PyCUDA, VPI, and [others](https://numba.readthedocs.io/en/stable/cuda/cuda_array_interface.html#interoperability))\n   * [`__array__`](docs/aux-image.md#accessing-as-a-numpy-array) interface ([Numpy](https://numpy.org/doc/stable/reference/arrays.interface.html)) \n* Train higher-resolution detection models with [`train_ssd.py --resolution=N`](https://github.com/dusty-nv/pytorch-ssd/blob/86155c0c410e0959df0184b24af6a8f59f49fbe5/train_ssd.py#L49)\n* Compute per-class Mean Average Precision (mAP) with [`train_ssd.py --validate-mean-ap`](https://github.com/dusty-nv/pytorch-ssd/blob/86155c0c410e0959df0184b24af6a8f59f49fbe5/train_ssd.py#L98)\n* Tensorboard logging in [`train.py`](https://github.com/dusty-nv/pytorch-classification/blob/819b105087c397c23cd81fd9446b5f0a0213db94/train.py#L95) / [`train_ssd.py`](https://github.com/dusty-nv/pytorch-ssd/blob/86155c0c410e0959df0184b24af6a8f59f49fbe5/train_ssd.py#L114)\n* Added [RTSP server](docs/aux-streaming.md#rtsp) video output\n* Added optional timeout status code to [`videoSource.Capture()`](https://github.com/dusty-nv/jetson-utils/blob/0bcb19b498326eb866a80d7d13388b2e59bc9dfd/video/videoSource.h#L235)\n* Added [`--input-save`](docs/aux-streaming.md#input-options) and [`--output-save`](docs/aux-streaming.md#output-options) for dumping video to disk in addition to the primary I/O stream\n* Added [`ros_deep_learning`](https://github.com/dusty-nv/ros_deep_learning) package as a submodule and to container builds\n* Added x86_64 + dGPU support and WSL2 for [Docker container](docs/aux-docker.md#x86-support)\n* Automated testing with [`test-models.py`](tools/test-models.py) and [`test-cuda.sh`](https://github.com/dusty-nv/jetson-utils/blob/master/python/examples/test-cuda.sh)\n\n\n## April 8, 2022\n\n* Added support for JetPack 5.0 and [Jetson AGX Orin](https://developer.nvidia.com/embedded/jetson-agx-orin-developer-kit)\n* Conditionally use NVIDIA V4L2-based hardware codecs when on JetPack 5.0 and newer\n* Minor bug fixes and improvements\n\n## August 3, 2021\n\n* Added [Pose Estimation with PoseNet](docs/posenet.md) with pre-trained models\n* Added [Mononocular Depth with DepthNet](docs/depthnet.md) with pre-trained models\n* Added support for [`cudaMemcpy()` from Python](docs/aux-image.md#copying-images)\n* Added support for [drawing 2D shapes with CUDA](docs/aux-image.md#drawing-shapes)\n\n## August 31, 2020\n\n* Added initial support for [Running in Docker Containers](docs/aux-docker.md)\n* Changed OpenGL behavior to show window on first frame\n* Minor bug fixes and improvements\n\n## July 15, 2020\n\n> **note:** API changes from this update are intended to be backwards-compatible, so previous code should still run.\n\n* [Re-training SSD-Mobilenet](docs/pytorch-ssd.md) Object Detection tutorial with PyTorch\n* Support for [collection of object detection datasets](docs/pytorch-collect-detection.md) and bounding-box labeling in `camera-capture` tool\n* [`videoSource`](docs/aux-streaming.md#source-code) and [`videoOutput`](docs/aux-streaming.md#source-code) APIs for C++/Python that supports multiple types of video streams:\n   * [MIPI CSI cameras](docs/aux-streaming.md#mipi-csi-cameras)\n   * [V4L2 cameras](docs/aux-streaming.md#v4l2-cameras)\n   * [RTP](docs/aux-streaming.md#rtp) / [RTSP](docs/aux-streaming.md#rtsp) \n   * [Videos](docs/aux-streaming.md#video-files) & [Images](docs/aux-streaming.md#image-files)\n   * [Image sequences](docs/aux-streaming.md#image-files)\n   * [OpenGL windows](docs/aux-streaming.md#output-streams)\n* Unified the `-console` and `-camera` samples to process both images and video streams\n   * [`imagenet.cpp`](examples/imagenet/imagenet.cpp) / [`imagenet.py`](python/examples/imagenet.py)\n   * [`detectnet.cpp`](examples/detectnet/detectnet.cpp) / [`detectnet.py`](python/examples/detectnet.py)\n   * [`segnet.cpp`](examples/segnet/segnet.cpp) / [`segnet.py`](python/examples/segnet.py)\n* Support for `uchar3/uchar4/float3/float4` images (default is now `uchar3` as opposed to `float4`)\n* Replaced opaque Python memory capsule with [`jetson.utils.cudaImage`](docs/aux-image.md#image-capsules-in-python) object\n   * See [Image Capsules in Python](docs/aux-image.md#image-capsules-in-python) for more info\n   * Images are now subscriptable/indexable from Python to directly access the pixel dataset\n   * Numpy ndarray conversion now supports `uchar3/uchar4/float3/float4` formats\n* [`cudaConvertColor()`](https://github.com/dusty-nv/jetson-utils/blob/a587c20ad95d71efd47f9c91e3fbf703ad48644d/cuda/cudaColorspace.h#L31) automated colorspace conversion function (RGB, BGR, YUV, Bayer, grayscale, ect)\n* Python CUDA bindings for `cudaResize()`, `cudaCrop()`, `cudaNormalize()`, `cudaOverlay()`\n   * See [Image Manipulation with CUDA](docs/aux-image.md) and [`cuda-examples.py`](https://github.com/dusty-nv/jetson-utils/blob/master/python/examples/cuda-examples.py) for examples of using these \n* Transitioned to using Python3 by default since Python 2.7 is now past EOL\n* DIGITS tutorial is now marked as deprecated (replaced by PyTorch transfer learning tutorial)\n* Logging can now be controlled/disabled from the command line (e.g. `--log-level=verbose`)\n\nThanks to everyone from the forums and GitHub who helped to test these updates in advance!\n\n## October 3, 2019\n\n* Added new pre-trained FCN-ResNet18 semantic segmentation models:\n\n| Dataset      | Resolution | CLI Argument | Accuracy | Jetson Nano | Jetson Xavier |\n|:------------:|:----------:|--------------|:--------:|:-----------:|:-------------:|\n| [Cityscapes](https://www.cityscapes-dataset.com/) | 512x256 | `fcn-resnet18-cityscapes-512x256` | 83.3% | 48 FPS | 480 FPS |\n| [Cityscapes](https://www.cityscapes-dataset.com/) | 1024x512 | `fcn-resnet18-cityscapes-1024x512` | 87.3% | 12 FPS | 175 FPS |\n| [Cityscapes](https://www.cityscapes-dataset.com/) | 2048x1024 | `fcn-resnet18-cityscapes-2048x1024` | 89.6% | 3 FPS | 47 FPS |\n| [DeepScene](http://deepscene.cs.uni-freiburg.de/) | 576x320 | `fcn-resnet18-deepscene-576x320` | 96.4% | 26 FPS | 360 FPS |\n| [DeepScene](http://deepscene.cs.uni-freiburg.de/) | 864x480 | `fcn-resnet18-deepscene-864x480` | 96.9% | 14 FPS | 190 FPS |\n| [Multi-Human](https://lv-mhp.github.io/) | 512x320 | `fcn-resnet18-mhp-512x320` | 86.5% | 34 FPS | 370 FPS |\n| [Multi-Human](https://lv-mhp.github.io/) | 640x360 | `fcn-resnet18-mhp-512x320` | 87.1% | 23 FPS | 325 FPS |\n| [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) | 320x320 | `fcn-resnet18-voc-320x320` | 85.9% | 45 FPS | 508 FPS |\n| [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) | 512x320 | `fcn-resnet18-voc-512x320` | 88.5% | 34 FPS | 375 FPS |\n| [SUN RGB-D](http://rgbd.cs.princeton.edu/) | 512x400 | `fcn-resnet18-sun-512x400` | 64.3% | 28 FPS | 340 FPS |\n| [SUN RGB-D](http://rgbd.cs.princeton.edu/) | 640x512 | `fcn-resnet18-sun-640x512` | 65.1% | 17 FPS | 224 FPS |\n\n## July 19, 2019\n\n* Python API support for imageNet, detectNet, and camera/display utilities</li>\n* Python examples for processing static images and live camera streaming</li>\n* Support for interacting with numpy ndarrays from CUDA</li>\n* Onboard re-training of ResNet-18 models with PyTorch</li>\n* Example datasets:  800MB Cat/Dog and 1.5GB PlantCLEF</li>\n* Camera-based tool for collecting and labeling custom datasets</li>\n* Text UI tool for selecting/downloading pre-trained models</li>\n* New pre-trained image classification models (on 1000-class ImageNet ILSVRC)\n   * ResNet-18, ResNet-50, ResNet-101, ResNet-152</li>\n   * VGG-16, VGG-19</li>\n   * Inception-v4</li>\n* New pre-trained object detection models (on 90-class MS-COCO)\n   * SSD-Mobilenet-v1</li>\n   * SSD-Mobilenet-v2</li>\n   * SSD-Inception-v2</li>\n* API Reference documentation for C++ and Python</li>\n   * Command line usage info for all examples, run with --help</li>\n   * Output of network profiler times, including pre/post-processing</li>\n   * Improved font rasterization using system TTF fonts</li>\n\n\n##\n<p align=\"center\"><sup>© 2016-2020 NVIDIA | </sup><a href=\"README.md#hello-ai-world\"><sup>Table of Contents</sup></a></p>\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 8.9208984375,
          "content": "\ncmake_minimum_required(VERSION 2.8)\nproject(jetson-inference)\n\n# submodule warning\nmessage(\" \")\nmessage(\"Note:  this project uses git submodules in the source tree.\")\nmessage(\"       if you haven't already, run the following command from\")\nmessage(\"       the project's root directory:\")\nmessage(\" \")\nmessage(\"           git submodule update --init\") \nmessage(\"\\n\")\n\nif( NOT EXISTS \"${PROJECT_SOURCE_DIR}/utils/.git\" )\n\tmessage(\"Note:  required git submodules have not been detected.\")\n\tmessage(\"       first, please run the following command from the\")\n\tmessage(\"       the project's root directory to clone them:\")\n\tmessage(\" \")\n\tmessage(\"          git submodule update --init\")\n\tmessage(\" \")\n\tmessage(FATAL_ERROR \"missing required git submodules, see instructions above\")\nendif()\n\n\n# setup build flags\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++11 -Wno-write-strings -Wno-deprecated-declarations\")\t# -std=c++14 \nset(BUILD_DEPS \"YES\" CACHE BOOL \"If YES, will install dependencies into sandbox.  Automatically reset to NO after dependencies are installed.\")\nset(BUILD_INTERACTIVE \"YES\" CACHE BOOL \"If NO, will download/install the default DNN models without prompting the user, and skip installation of PyTorch.\")\nset(BUILD_EXPERIMENTAL \"NO\" CACHE BOOL \"If YES, will enable support for experimental DNNs, examples, and plugins\")\n\n\n# copy configuration tools to build dir\n#file(COPY \"tools/download-models.sh\" DESTINATION ${PROJECT_BINARY_DIR})\n#file(COPY \"tools/download-models.rc\" DESTINATION ${PROJECT_BINARY_DIR})\nfile(COPY \"tools/install-pytorch.sh\" DESTINATION ${PROJECT_BINARY_DIR})\nfile(COPY \"tools/install-pytorch.rc\" DESTINATION ${PROJECT_BINARY_DIR})\n\n\n# detect distro version\nfind_program(LSB_RELEASE_EXEC lsb_release)\n\nexecute_process(COMMAND \"${LSB_RELEASE_EXEC}\" --short --id OUTPUT_VARIABLE LSB_RELEASE_ID OUTPUT_STRIP_TRAILING_WHITESPACE)\nexecute_process(COMMAND \"${LSB_RELEASE_EXEC}\" --short --release OUTPUT_VARIABLE LSB_RELEASE_VERSION OUTPUT_STRIP_TRAILING_WHITESPACE)\nexecute_process(COMMAND \"${LSB_RELEASE_EXEC}\" --short --codename OUTPUT_VARIABLE LSB_RELEASE_CODENAME OUTPUT_STRIP_TRAILING_WHITESPACE)\n\nmessage(\"-- distro ID:       ${LSB_RELEASE_ID}\")\nmessage(\"-- distro version:  ${LSB_RELEASE_VERSION}\")\nmessage(\"-- distro codename: ${LSB_RELEASE_CODENAME}\")\n\n\n# if this is the first time running cmake, perform pre-build dependency install script (or if the user manually triggers re-building the dependencies)\nif( ${BUILD_DEPS} )\n\tmessage(\"-- Launching pre-build dependency installer script...\")\n\tmessage(\"-- Build interactive:  ${BUILD_INTERACTIVE}\")\n\n\texecute_process(COMMAND sh ../CMakePreBuild.sh ${BUILD_INTERACTIVE}\n\t\t\t\tWORKING_DIRECTORY ${PROJECT_BINARY_DIR}\n\t\t\t\tRESULT_VARIABLE PREBUILD_SCRIPT_RESULT)\n\n\tset(BUILD_DEPS \"NO\" CACHE BOOL \"If YES, will install dependencies into sandbox.  Automatically reset to NO after dependencies are installed.\" FORCE)\n\tmessage(\"-- Finished installing dependencies\")\nendif()\n\n\n# setup CUDA\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \"${CMAKE_CURRENT_SOURCE_DIR}/utils/cuda\" )\nfind_package(CUDA)\nmessage(\"-- CUDA version: ${CUDA_VERSION}\")\n\nset(CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -O3)\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\")\n\tmessage(\"-- CUDA ${CUDA_VERSION} detected (${CMAKE_SYSTEM_PROCESSOR}), enabling SM_53 SM_62\")\n\tset(CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_53,code=sm_53 -gencode arch=compute_62,code=sm_62)\n\n\tif(CUDA_VERSION_MAJOR GREATER 9)\n\t\tmessage(\"-- CUDA ${CUDA_VERSION} detected (${CMAKE_SYSTEM_PROCESSOR}), enabling SM_72\")\n\t\tset(CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_72,code=sm_72)\n\tendif()\n\n\tif(CUDA_VERSION_MAJOR GREATER 10)\n\t\tmessage(\"-- CUDA ${CUDA_VERSION} detected (${CMAKE_SYSTEM_PROCESSOR}), enabling SM_87\")\n\t\tset(CUDA_NVCC_FLAGS ${CUDA_NVCC_FLAGS}; -gencode arch=compute_87,code=sm_87)\n\tendif()\nendif()\n\n# OpenCV used for findHomography() and decomposeHomography()\n# OpenCV version >= 3.0.0 required for decomposeHomography()\nfind_package(OpenCV COMPONENTS core calib3d)\n\nif( NOT OpenCV_FOUND )\n\tmessage(\"-- didn't find OpenCV on system, disabling OpenCV\")\nelse()\n\tmessage(\"-- OpenCV version:  \" ${OpenCV_VERSION})\n\n\tif( ${OpenCV_VERSION_MAJOR} LESS 3 )\n\t\tmessage(\"-- OpenCV version less than 3.0, disabling OpenCV\")\n\telse()\n\t\tmessage(\"-- OpenCV version >= 3.0.0, enabling OpenCV\")\n\t\tset(HAS_OPENCV 1)\n\t\tadd_definitions(-DHAS_OPENCV)\n\tendif()\t\nendif()\n\t\n# check for VPI (TODO: VPI 1.0 support for JetPack 4.x)\nfind_package(VPI 2.0)\n\nif( NOT VPI_FOUND )\n\tmessage(\"-- didn't find VPI on system, disabling VPI\")\nelse()\n\tmessage(\"-- VPI version:  \" ${VPI_VERSION})\n\tset(HAS_VPI 1)\n\tadd_definitions(-DHAS_VPI)\nendif()\n\n# setup project output paths\nset(PROJECT_OUTPUT_DIR  ${PROJECT_BINARY_DIR}/${CMAKE_SYSTEM_PROCESSOR})\nset(PROJECT_INCLUDE_DIR ${PROJECT_OUTPUT_DIR}/include)\n\nfile(MAKE_DIRECTORY ${PROJECT_INCLUDE_DIR})\nfile(MAKE_DIRECTORY ${PROJECT_OUTPUT_DIR}/bin)\n\nmessage(\"-- system arch:  ${CMAKE_SYSTEM_PROCESSOR}\")\nmessage(\"-- output path:  ${PROJECT_OUTPUT_DIR}\")\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${PROJECT_OUTPUT_DIR}/bin)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${PROJECT_OUTPUT_DIR}/lib)\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${PROJECT_OUTPUT_DIR}/lib)\n\n# build C/C++ library\ninclude_directories(${PROJECT_INCLUDE_DIR} ${PROJECT_INCLUDE_DIR}/jetson-inference ${PROJECT_INCLUDE_DIR}/jetson-utils)\ninclude_directories(/usr/include/gstreamer-1.0 /usr/lib/${CMAKE_SYSTEM_PROCESSOR}/gstreamer-1.0/include /usr/include/glib-2.0 /usr/include/libxml2 /usr/lib/${CMAKE_SYSTEM_PROCESSOR}/glib-2.0/include/)\n\nfile(GLOB inferenceSources c/*.cpp c/*.cu c/calibration/*.cpp c/tracking/*.cpp)\nfile(GLOB inferenceIncludes c/*.h c/*.cuh c/calibration/*.h c/tracking/*.h)\n\nif(BUILD_EXPERIMENTAL)\n\tmessage(\"-- BUILD_EXPERIMENTAL enabled\")\n\n\tfile(GLOB experimentalSources c/experimental/*.cpp c/experimental/*.cu)\n\tfile(GLOB experimentalIncludes c/experimental/*.h c/experimental/*.cuh)\n\tfile(GLOB_RECURSE pluginSources c/plugins/*.cpp c/plugins/*.cu)\n\n\tlist(APPEND inferenceSources ${experimentalSources})\n\tlist(APPEND inferenceIncludes ${experimentalIncludes})\nelse()\n\tmessage(\"-- BUILD_EXPERIMENTAL disabled\")\n\tfile(GLOB pluginSources c/plugins/*.cpp c/plugins/*.cu c/plugins/pose/trt_pose/parse/*.cpp)\nendif()\n\nif(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\")\n\tlink_directories(/usr/lib/aarch64-linux-gnu/tegra)\nendif()\n\t\ncuda_add_library(jetson-inference SHARED ${inferenceSources} ${pluginSources})\ntarget_include_directories(jetson-inference PRIVATE ${PROJECT_SOURCE_DIR}/c/plugins/pose/trt_pose/parse)\n\n# transfer all headers to the include directory\nfile(MAKE_DIRECTORY ${PROJECT_INCLUDE_DIR}/jetson-inference)\n\nforeach(include ${inferenceIncludes})\n\tmessage(\"-- Copying ${include}\")\n\tconfigure_file(${include} ${PROJECT_INCLUDE_DIR}/jetson-inference COPYONLY)\nendforeach()\n\n#if(BUILD_EXPERIMENTAL)\n\n#endif()\n\n# create symbolic link for network and image data\nexecute_process( COMMAND \"${CMAKE_COMMAND}\" \"-E\" \"create_symlink\" \"${PROJECT_SOURCE_DIR}/data/networks\" \"${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/networks\" )\nexecute_process( COMMAND \"${CMAKE_COMMAND}\" \"-E\" \"create_symlink\" \"${PROJECT_SOURCE_DIR}/data/images\" \"${CMAKE_RUNTIME_OUTPUT_DIRECTORY}/images\" )\n\n  \n# copy image data (EDIT: these are now symlinked above)\n#file(GLOB imageData ${PROJECT_SOURCE_DIR}/data/images/*)\n\n#foreach(image ${imageData})\n#\tmessage(\"-- Copying ${image}\")\n#\tfile(COPY ${image} DESTINATION ${CMAKE_RUNTIME_OUTPUT_DIRECTORY})\n#\t#configure_file(${include} ${CMAKE_RUNTIME_OUTPUT_DIRECTORY} COPYONLY)\n#endforeach()\n\n\n# build subdirectories\nadd_subdirectory(docs)\nadd_subdirectory(examples)\nadd_subdirectory(tools)\nadd_subdirectory(utils)\nadd_subdirectory(python)\n\n\n# set linker options\ntarget_link_libraries(jetson-inference jetson-utils nvinfer nvinfer_plugin)\n\nif(CUDA_VERSION_MAJOR GREATER 9)\n\ttarget_link_libraries(jetson-inference nvonnxparser)\nendif()\n\nif(CUDA_VERSION VERSION_LESS \"12.6\")\n\ttarget_link_libraries(jetson-inference nvcaffe_parser)\nelse()\n    CUDA_ADD_CUBLAS_TO_TARGET(jetson-inference)\nendif()\n\nif(HAS_OPENCV) \n   message(\"-- linking jetson-inference with OpenCV \" ${OpenCV_VERSION})\n   target_link_libraries(jetson-inference opencv_core opencv_calib3d)\nendif()\n\nif(HAS_VPI) \n   message(\"-- linking jetson-inference with VPI \" ${VPI_VERSION})\n   target_link_libraries(jetson-inference vpi)\nendif()\n\n# install includes\nforeach(include ${inferenceIncludes})\n    install(FILES \"${include}\" DESTINATION include/jetson-inference)\nendforeach()\n\n# install symlink to networks and images\ninstall(CODE \"execute_process( COMMAND ${CMAKE_COMMAND} -E create_symlink ${PROJECT_SOURCE_DIR}/data/networks ${CMAKE_INSTALL_PREFIX}/bin/networks )\" )\ninstall(CODE \"execute_process( COMMAND ${CMAKE_COMMAND} -E create_symlink ${PROJECT_SOURCE_DIR}/data/images ${CMAKE_INSTALL_PREFIX}/bin/images )\" )\n\n# install the shared library\ninstall(TARGETS jetson-inference DESTINATION lib EXPORT jetson-inferenceConfig)\n\n# install the cmake project, for importing\ninstall(EXPORT jetson-inferenceConfig DESTINATION share/jetson-inference/cmake)\n\n# run ldconfig after installing\ninstall(CODE \"execute_process( COMMAND ldconfig )\")\n\n"
        },
        {
          "name": "CMakePreBuild.sh",
          "type": "blob",
          "size": 1.697265625,
          "content": "#!/usr/bin/env bash\n# this script is automatically run from CMakeLists.txt\n\nBUILD_ROOT=$PWD\nBUILD_INTERACTIVE=$1\n\nif [ -f /.dockerenv ]; then\n\tBUILD_CONTAINER=\"YES\"\nelse\n\tBUILD_CONTAINER=\"NO\"\nfi\n\necho \"[Pre-build]  dependency installer script running...\"\necho \"[Pre-build]  build root directory: $BUILD_ROOT\"\necho \"[Pre-build]  build interactive:    $BUILD_INTERACTIVE\"\necho \"[Pre-build]  build container:      $BUILD_CONTAINER\"\necho \" \"\n\n# detect build architecture\nARCH=$(uname -i)\n\n# break on errors\n#set -e\n\n# docker doesn't use sudo\nif [ $BUILD_CONTAINER = \"YES\" ]; then\n\tSUDO=\"\"\nelse\n\tSUDO=\"sudo\"\nfi\n\t\n# install packages\n$SUDO apt-get update\n$SUDO apt-get install -y --no-install-recommends \\\n\t\tdialog \\\n\t\tlibglew-dev \\\n\t\tglew-utils \\\n\t\tgstreamer1.0-libav \\\n\t\tgstreamer1.0-nice \\\n\t\tlibgstreamer1.0-dev \\\n\t\tlibgstrtspserver-1.0-dev \\\n\t\tlibglib2.0-dev \\\n\t\tlibsoup2.4-dev \\\n\t\tlibjson-glib-dev \\\n\t\tpython3-pip \\\n\t\tpython3-packaging \\\n\t\tqtbase5-dev \\\n\t\tavahi-utils\n\nif [ $BUILD_CONTAINER = \"NO\" ]; then\n\t# these are installed in a different step in the Dockerfile\n\t$SUDO apt-get install -y --no-install-recommends \\\n\t\tlibgstreamer-plugins-base1.0-dev \\\n\t\tlibgstreamer-plugins-good1.0-dev \\\n\t\tlibgstreamer-plugins-bad1.0-dev\nfi\n\nif [ $ARCH != \"x86_64\" ]; then\n\t# on x86, these are already installed by conda and installing them again creates conflicts\n\t$SUDO apt-get install -y libpython3-dev python3-numpy\nfi\n\n# install cython for if numpy gets built by later packages\npip3 install --no-cache-dir --verbose --upgrade Cython\n\n# download/install models and PyTorch\nif [ $BUILD_CONTAINER = \"NO\" ]; then\n\t#./download-models.sh $BUILD_INTERACTIVE\n\t./install-pytorch.sh $BUILD_INTERACTIVE\nfi\n\necho \"[Pre-build]  Finished CMakePreBuild script\"\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 4.8173828125,
          "content": "# Copyright (c) 2020, NVIDIA CORPORATION. All rights reserved.\r\n#\r\n# Permission is hereby granted, free of charge, to any person obtaining a\r\n# copy of this software and associated documentation files (the \"Software\"),\r\n# to deal in the Software without restriction, including without limitation\r\n# the rights to use, copy, modify, merge, publish, distribute, sublicense,\r\n# and/or sell copies of the Software, and to permit persons to whom the\r\n# Software is furnished to do so, subject to the following conditions:\r\n#\r\n# The above copyright notice and this permission notice shall be included in\r\n# all copies or substantial portions of the Software.\r\n#\r\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\r\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\r\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\r\n# THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\r\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\r\n# FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\r\n# DEALINGS IN THE SOFTWARE.\r\n#\r\n# Build this Dockerfile by running the following commands:\r\n#\r\n#     $ cd /path/to/your/jetson-inference\r\n#     $ docker/build.sh\r\n#\r\n# Also you should set your docker default-runtime to nvidia:\r\n#     https://github.com/dusty-nv/jetson-containers#docker-default-runtime\r\n#\r\n\r\nARG BASE_IMAGE=nvcr.io/nvidia/l4t-pytorch:r32.4.3-pth1.6-py3\r\nFROM ${BASE_IMAGE}\r\n\r\nENV DEBIAN_FRONTEND=noninteractive\r\nENV SHELL /bin/bash\r\n\r\nWORKDIR /jetson-inference\r\n\r\n  \r\n#\r\n# install development packages\r\n#\r\nRUN add-apt-repository --remove \"deb https://apt.kitware.com/ubuntu/ $(lsb_release --codename --short) main\" && \\\r\n    apt-get update && \\\r\n    apt-get purge -y '*opencv*' || echo \"existing OpenCV installation not found\" && \\\r\n    apt-get install -y --no-install-recommends \\\r\n            cmake \\\r\n\t\t  nano \\\r\n\t\t  mesa-utils \\\r\n\t\t  lsb-release \\\r\n\t\t  gstreamer1.0-tools \\\r\n\t\t  gstreamer1.0-libav \\\r\n\t\t  gstreamer1.0-rtsp \\\r\n\t\t  gstreamer1.0-plugins-good \\\r\n\t\t  gstreamer1.0-plugins-bad \\\r\n\t\t  gstreamer1.0-plugins-ugly \\\r\n\t\t  libgstreamer-plugins-base1.0-dev \\\r\n\t\t  libgstreamer-plugins-good1.0-dev \\\r\n\t\t  libgstreamer-plugins-bad1.0-dev && \\\r\n    if [ `lsb_release --codename --short` != 'bionic' ]; then \\\r\n    apt-get install -y --no-install-recommends \\\r\n\t\t  gstreamer1.0-plugins-rtp; \\\r\n    else echo \"skipping packages unavailable for Ubuntu 18.04\"; fi \\\r\n    && rm -rf /var/lib/apt/lists/* \\\r\n    && apt-get clean\r\n\r\n# make a copy of this cause it gets purged...\r\nRUN mkdir -p /usr/local/include/gstreamer-1.0/gst && \\\r\n    cp -r /usr/include/gstreamer-1.0/gst/webrtc /usr/local/include/gstreamer-1.0/gst && \\\r\n    ls -ll /usr/local/include/ && \\\r\n    ls -ll /usr/local/include/gstreamer-1.0/gst/webrtc\r\n\r\n\r\n# \r\n# install python packages\r\n#\r\nCOPY python/training/detection/ssd/requirements.txt /tmp/pytorch_ssd_requirements.txt\r\nCOPY python/www/flask/requirements.txt /tmp/flask_requirements.txt\r\nCOPY python/www/dash/requirements.txt /tmp/dash_requirements.txt\r\n\r\nRUN pip3 install --no-cache-dir --verbose --upgrade Cython && \\\r\n    pip3 install --no-cache-dir --verbose -r /tmp/pytorch_ssd_requirements.txt && \\\r\n    pip3 install --no-cache-dir --verbose -r /tmp/flask_requirements.txt && \\\r\n    pip3 install --no-cache-dir --verbose -r /tmp/dash_requirements.txt\r\n    \r\n    \r\n# \r\n# install OpenCV (with CUDA)\r\n#\r\nARG OPENCV_URL=https://nvidia.box.com/shared/static/5v89u6g5rb62fpz4lh0rz531ajo2t5ef.gz\r\nARG OPENCV_DEB=OpenCV-4.5.0-aarch64.tar.gz\r\n\r\nCOPY docker/containers/scripts/opencv_install.sh /tmp/opencv_install.sh\r\nRUN cd /tmp && ./opencv_install.sh ${OPENCV_URL} ${OPENCV_DEB}\r\n\r\n  \r\n#\r\n# copy source\r\n#\r\nCOPY c c\r\nCOPY examples examples\r\nCOPY python python\r\nCOPY tools tools\r\nCOPY utils utils\r\nCOPY data/networks/models.json data/networks/models.json\r\n\r\nCOPY CMakeLists.txt CMakeLists.txt\r\nCOPY CMakePreBuild.sh CMakePreBuild.sh\r\n\r\n\r\n#\r\n# build source\r\n#\r\nRUN mkdir docs && \\\r\n    touch docs/CMakeLists.txt && \\\r\n    sed -i 's/nvcaffe_parser/nvparsers/g' CMakeLists.txt && \\\r\n    cp -r /usr/local/include/gstreamer-1.0/gst/webrtc /usr/include/gstreamer-1.0/gst && \\\r\n    ln -s /usr/lib/$(uname -m)-linux-gnu/libgstwebrtc-1.0.so.0 /usr/lib/$(uname -m)-linux-gnu/libgstwebrtc-1.0.so && \\\r\n    mkdir build && \\\r\n    cd build && \\\r\n    cmake ../ && \\\r\n    make -j$(nproc) && \\\r\n    make install && \\\r\n    /bin/bash -O extglob -c \"cd /jetson-inference/build; rm -rf -v !($(uname -m)|download-models.*)\" && \\\r\n    rm -rf /var/lib/apt/lists/* \\\r\n    && apt-get clean\r\n    \r\n# build out-of-tree samples\r\nRUN cd examples/my-recognition && \\\r\n    mkdir build && \\\r\n    cd build && \\\r\n    cmake ../ && \\\r\n    make\r\n\r\n# workaround for \"cannot allocate memory in static TLS block\"\r\nENV LD_PRELOAD=${LD_PRELOAD}:/usr/lib/aarch64-linux-gnu/libgomp.so.1:/lib/aarch64-linux-gnu/libGLdispatch.so.0\r\n"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 1.1201171875,
          "content": "/*\n * Copyright (c) 2017, NVIDIA CORPORATION. All rights reserved.\n *\n * Permission is hereby granted, free of charge, to any person obtaining a\n * copy of this software and associated documentation files (the \"Software\"),\n * to deal in the Software without restriction, including without limitation\n * the rights to use, copy, modify, merge, publish, distribute, sublicense,\n * and/or sell copies of the Software, and to permit persons to whom the\n * Software is furnished to do so, subject to the following conditions:\n *\n * The above copyright notice and this permission notice shall be included in\n * all copies or substantial portions of the Software.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.  IN NO EVENT SHALL\n * THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING\n * FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER\n * DEALINGS IN THE SOFTWARE.\n */\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 26.50390625,
          "content": "<img src=\"https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-header.jpg\" width=\"100%\">\n\n# Deploying Deep Learning\nWelcome to our instructional guide for inference and realtime vision [DNN library](#api-reference) for **[NVIDIA Jetson](https://developer.nvidia.com/embedded-computing)** devices.  This project uses **[TensorRT](https://developer.nvidia.com/tensorrt)** to run optimized networks on GPUs from C++ or Python, and PyTorch for training models.\n\nSupported DNN vision primitives include [`imageNet`](docs/imagenet-console-2.md) for image classification, [`detectNet`](docs/detectnet-console-2.md) for object detection, [`segNet`](docs/segnet-console-2.md) for semantic segmentation, [`poseNet`](docs/posenet.md) for pose estimation, and [`actionNet`](docs/actionnet.md) for action recognition.  Examples are provided for streaming from live camera feeds, making webapps with WebRTC, and support for ROS/ROS2.\n\n<img src=\"https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/deep-vision-primitives.jpg\">\n\nFollow the **[Hello AI World](#hello-ai-world)** tutorial for running inference and transfer learning onboard your Jetson, including collecting your own datasets, training your own models with PyTorch, and deploying them with TensorRT.\n\n### Table of Contents\n\n* [Hello AI World](#hello-ai-world)\n* [Jetson AI Lab](#jetson-ai-lab)\n* [Video Walkthroughs](#video-walkthroughs)\n* [API Reference](#api-reference)\n* [Code Examples](#code-examples)\n* [Pre-Trained Models](#pre-trained-models)\n* [System Requirements](#recommended-system-requirements)\n* [Change Log](CHANGELOG.md)\n\n> &gt; &nbsp; JetPack 6 is now supported on Orin devices ([developer.nvidia.com/jetpack](https://developer.nvidia.com/embedded/jetpack)) <br/>\n> &gt; &nbsp; Check out the Generative AI and LLM tutorials on [Jetson AI Lab](https://www.jetson-ai-lab.com/)! <br/>\n> &gt; &nbsp; See the [Change Log](CHANGELOG.md) for the latest updates and new features. <br/>\n\n## Hello AI World\n\nHello AI World can be run completely onboard your Jetson, including live inferencing with TensorRT and transfer learning with PyTorch.  For installation instructions, see [System Setup](#system-setup).  It's then recommended to start with the [Inference](#inference) section to familiarize yourself with the concepts, before diving into [Training](#training) your own models.\n\n#### System Setup\n\n* [Setting up Jetson with JetPack](docs/jetpack-setup-2.md)\n* [Running the Docker Container](docs/aux-docker.md)\n* [Building the Project from Source](docs/building-repo-2.md)\n\n#### Inference\n\n* [Image Classification](docs/imagenet-console-2.md)\n\t* [Using the ImageNet Program on Jetson](docs/imagenet-console-2.md)\n\t* [Coding Your Own Image Recognition Program (Python)](docs/imagenet-example-python-2.md)\n\t* [Coding Your Own Image Recognition Program (C++)](docs/imagenet-example-2.md)\n\t* [Running the Live Camera Recognition Demo](docs/imagenet-camera-2.md)\n\t* [Multi-Label Classification for Image Tagging](docs/imagenet-tagging.md)\n* [Object Detection](docs/detectnet-console-2.md)\n\t* [Detecting Objects from Images](docs/detectnet-console-2.md#detecting-objects-from-the-command-line)\n\t* [Running the Live Camera Detection Demo](docs/detectnet-camera-2.md)\n\t* [Coding Your Own Object Detection Program](docs/detectnet-example-2.md)\n\t* [Using TAO Detection Models](docs/detectnet-tao.md)\n\t* [Object Tracking on Video](docs/detectnet-tracking.md)\n* [Semantic Segmentation](docs/segnet-console-2.md)\n\t* [Segmenting Images from the Command Line](docs/segnet-console-2.md#segmenting-images-from-the-command-line)\n\t* [Running the Live Camera Segmentation Demo](docs/segnet-camera-2.md)\n* [Pose Estimation](docs/posenet.md)\n* [Action Recognition](docs/actionnet.md)\n* [Background Removal](docs/backgroundnet.md)\n* [Monocular Depth](docs/depthnet.md)\n\n#### Training\n\n* [Transfer Learning with PyTorch](docs/pytorch-transfer-learning.md)\n* Classification/Recognition (ResNet-18)\n\t* [Re-training on the Cat/Dog Dataset](docs/pytorch-cat-dog.md)\n\t* [Re-training on the PlantCLEF Dataset](docs/pytorch-plants.md)\n\t* [Collecting your own Classification Datasets](docs/pytorch-collect.md)\n* Object Detection (SSD-Mobilenet)\n\t* [Re-training SSD-Mobilenet](docs/pytorch-ssd.md)\n\t* [Collecting your own Detection Datasets](docs/pytorch-collect-detection.md)\n\n#### WebApp Frameworks\n\n* [WebRTC Server](docs/webrtc-server.md)\n* [HTML / JavaScript](docs/webrtc-html.md)\n* [Flask + REST](docs/webrtc-flask.md)\n* [Plotly Dashboard](docs/webrtc-dash.md)\n* [Recognizer (Interactive Training)](docs/webrtc-recognizer.md)\n\n#### Appendix\n\n* [Camera Streaming and Multimedia](docs/aux-streaming.md)\n* [Image Manipulation with CUDA](docs/aux-image.md)\n* [DNN Inference Nodes for ROS/ROS2](https://github.com/dusty-nv/ros_deep_learning)\n\n## Jetson AI Lab\n\n<a href=\"https://www.jetson-ai-lab.com\"><img align=\"right\" width=\"200\" height=\"200\" src=\"https://nvidia-ai-iot.github.io/jetson-generative-ai-playground/images/JON_Gen-AI-panels.png\"></a>\n\nThe [**Jetson AI Lab**](https://www.jetson-ai-lab.com) has additional tutorials on LLMs, Vision Transformers (ViT), and Vision Language Models (VLM) that run on Orin (and in some cases Xavier).  Check out some of these:\n\n<a href=\"https://www.jetson-ai-lab.com/tutorial_nanoowl.html\"><img src=\"https://github.com/NVIDIA-AI-IOT/nanoowl/raw/main/assets/jetson_person_2x.gif\"></a>\n> [NanoOWL - Open Vocabulary Object Detection ViT](https://www.jetson-ai-lab.com/tutorial_nanoowl.html) (container: [`nanoowl`](/packages/vit/nanoowl)) \n\n<a href=\"https://youtu.be/X-OXxPiUTuU\"><img width=\"600px\" src=\"https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava.gif\"></a>\n> [Live Llava on Jetson AGX Orin](https://youtu.be/X-OXxPiUTuU) (container: [`local_llm`](/packages/llm/local_llm#live-llava)) \n\n<a href=\"https://youtu.be/dRmAGGuupuE\"><img width=\"600px\" src=\"https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/live_llava_bear.jpg\"></a>\n> [Live Llava 2.0 - VILA + Multimodal NanoDB on Jetson Orin](https://youtu.be/X-OXxPiUTuU) (container: [`local_llm`](/packages/llm/local_llm#live-llava)) \n\n<a href=\"https://youtu.be/ayqKpQNd1Jw\"><img src=\"https://raw.githubusercontent.com/dusty-nv/jetson-containers/docs/docs/images/nanodb_horse.gif\"></a>\n> [Realtime Multimodal VectorDB on NVIDIA Jetson](https://www.youtube.com/watch?v=wzLHAgDxMjQ) (container: [`nanodb`](/packages/vectordb/nanodb))  \n\n## Video Walkthroughs\n\nBelow are screencasts of Hello AI World that were recorded for the [Jetson AI Certification](https://developer.nvidia.com/embedded/learn/jetson-ai-certification-programs) course:\n\n| Description                                                                                                                                                                                                                                                                                                        | Video                                                                                                                                                                                                                                                 |\n|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n| <a href=\"https://www.youtube.com/watch?v=QXIwdsyK7Rw&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=9\" target=\"_blank\">**Hello AI World Setup**</a><br/>Download and run the Hello AI World container on Jetson Nano, test your camera feed, and see how to stream it over the network via RTP.                                     | <a href=\"https://www.youtube.com/watch?v=QXIwdsyK7Rw&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=9\" target=\"_blank\"><img src=https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_setup.jpg width=\"750\"></a>               |\n| <a href=\"https://www.youtube.com/watch?v=QatH8iF0Efk&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=10\" target=\"_blank\">**Image Classification Inference**</a><br/>Code your own Python program for image classification using Jetson Nano and deep learning, then experiment with realtime classification on a live camera stream. | <a href=\"https://www.youtube.com/watch?v=QatH8iF0Efk&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=10\" target=\"_blank\"><img src=https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_imagenet.jpg width=\"750\"></a>           |\n| <a href=\"https://www.youtube.com/watch?v=sN6aT9TpltU&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=11\" target=\"_blank\">**Training Image Classification Models**</a><br/>Learn how to train image classification models with PyTorch onboard Jetson Nano, and collect your own classification datasets to create custom models.     | <a href=\"https://www.youtube.com/watch?v=sN6aT9TpltU&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=11\" target=\"_blank\"><img src=https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_imagenet_training.jpg width=\"750\"></a>  |\n| <a href=\"https://www.youtube.com/watch?v=obt60r8ZeB0&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=12\" target=\"_blank\">**Object Detection Inference**</a><br/>Code your own Python program for object detection using Jetson Nano and deep learning, then experiment with realtime detection on a live camera stream.              | <a href=\"https://www.youtube.com/watch?v=obt60r8ZeB0&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=12\" target=\"_blank\"><img src=https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_detectnet.jpg width=\"750\"></a>          |\n| <a href=\"https://www.youtube.com/watch?v=2XMkPW_sIGg&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=13\" target=\"_blank\">**Training Object Detection Models**</a><br/>Learn how to train object detection models with PyTorch onboard Jetson Nano, and collect your own detection datasets to create custom models.                  | <a href=\"https://www.youtube.com/watch?v=2XMkPW_sIGg&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=13\" target=\"_blank\"><img src=https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_detectnet_training.jpg width=\"750\"></a> |\n| <a href=\"https://www.youtube.com/watch?v=AQhkMLaB_fY&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=14\" target=\"_blank\">**Semantic Segmentation**</a><br/>Experiment with fully-convolutional semantic segmentation networks on Jetson Nano, and run realtime segmentation on a live camera stream.                                 | <a href=\"https://www.youtube.com/watch?v=AQhkMLaB_fY&list=PL5B692fm6--uQRRDTPsJDp4o0xbzkoyf8&index=14\" target=\"_blank\"><img src=https://github.com/dusty-nv/jetson-inference/raw/master/docs/images/thumbnail_segnet.jpg width=\"750\"></a>             |\n\n## API Reference\n\nBelow are links to reference documentation for the [C++](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/index.html) and [Python](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.html) libraries from the repo:\n\n#### jetson-inference\n\n|                    | [C++](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__deepVision.html) | [Python](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.inference.html) |\n|--------------------|--------------|--------------|\n| Image Recognition  | [`imageNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__imageNet.html#classimageNet) | [`imageNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.inference.html#imageNet) |\n| Object Detection   | [`detectNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__detectNet.html#classdetectNet) | [`detectNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.inference.html#detectNet)\n| Segmentation       | [`segNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__segNet.html#classsegNet) | [`segNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.inference.html#segNet) |\n| Pose Estimation    | [`poseNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__poseNet.html#classposeNet) | [`poseNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.inference.html#poseNet) |\n| Action Recognition | [`actionNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__actionNet.html#classactionNet) | [`actionNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.inference.html#actionNet) |\n| Background Removal | [`backgroundNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__backgroundNet.html#classbackgroundNet) | [`actionNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.inference.html#backgroundNet) |\n| Monocular Depth    | [`depthNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__depthNet.html#classdepthNet) | [`depthNet`](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.inference.html#depthNet) |\n\n#### jetson-utils\n\n* [C++](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/group__util.html)\n* [Python](https://rawgit.com/dusty-nv/jetson-inference/master/docs/html/python/jetson.utils.html)\n\nThese libraries are able to be used in external projects by linking to `libjetson-inference` and `libjetson-utils`.\n\n## Code Examples\n\nIntroductory code walkthroughs of using the library are covered during these steps of the Hello AI World tutorial:\n\n* [Coding Your Own Image Recognition Program (Python)](docs/imagenet-example-python-2.md)\n* [Coding Your Own Image Recognition Program (C++)](docs/imagenet-example-2.md)\n\nAdditional C++ and Python samples for running the networks on images and live camera streams can be found here:\n\n|                   | C++              | Python             |\n|-------------------|---------------------|---------------------|\n| &nbsp;&nbsp;&nbsp;Image Recognition  | [`imagenet.cpp`](examples/imagenet/imagenet.cpp) | [`imagenet.py`](python/examples/imagenet.py) |\n| &nbsp;&nbsp;&nbsp;Object Detection   | [`detectnet.cpp`](examples/detectnet/detectnet.cpp) | [`detectnet.py`](python/examples/detectnet.py) |\n| &nbsp;&nbsp;&nbsp;Segmentation       | [`segnet.cpp`](examples/segnet/segnet.cpp) | [`segnet.py`](python/examples/segnet.py) |\n| &nbsp;&nbsp;&nbsp;Pose Estimation    | [`posenet.cpp`](examples/posenet/posenet.cpp) | [`posenet.py`](python/examples/posenet.py) |\n| &nbsp;&nbsp;&nbsp;Action Recognition | [`actionnet.cpp`](examples/actionnet/actionnet.cpp) | [`actionnet.py`](python/examples/actionnet.py) |\n| &nbsp;&nbsp;&nbsp;Background Removal | [`backgroundnet.cpp`](examples/backgroundnet/backgroundnet.cpp) | [`backgroundnet.py`](python/examples/backgroundnet.py) |\n| &nbsp;&nbsp;&nbsp;Monocular Depth    | [`depthnet.cpp`](examples/depthnet/segnet.cpp) | [`depthnet.py`](python/examples/depthnet.py) |\n\n> **note**:  see the [Array Interfaces](docs/aux-image.md#array-interfaces) section for using memory with other Python libraries (like Numpy, PyTorch, ect)\n\nThese examples will automatically be compiled while [Building the Project from Source](docs/building-repo-2.md), and are able to run the pre-trained models listed below in addition to custom models provided by the user.  Launch each example with `--help` for usage info.\n\n## Pre-Trained Models\n\nThe project comes with a number of pre-trained models that are available to use and will be automatically downloaded:\n\n#### Image Recognition\n\n| Network       | CLI argument   | NetworkType enum |\n| --------------|----------------|------------------|\n| AlexNet       | `alexnet`      | `ALEXNET`        |\n| GoogleNet     | `googlenet`    | `GOOGLENET`      |\n| GoogleNet-12  | `googlenet-12` | `GOOGLENET_12`   |\n| ResNet-18     | `resnet-18`    | `RESNET_18`      |\n| ResNet-50     | `resnet-50`    | `RESNET_50`      |\n| ResNet-101    | `resnet-101`   | `RESNET_101`     |\n| ResNet-152    | `resnet-152`   | `RESNET_152`     |\n| VGG-16        | `vgg-16`       | `VGG-16`         |\n| VGG-19        | `vgg-19`       | `VGG-19`         |\n| Inception-v4  | `inception-v4` | `INCEPTION_V4`   |\n\n#### Object Detection\n\n| Model                   | CLI argument       | NetworkType enum   | Object classes       |\n| ------------------------|--------------------|--------------------|----------------------|\n| SSD-Mobilenet-v1        | `ssd-mobilenet-v1` | `SSD_MOBILENET_V1` | 91 ([COCO classes](../data/networks/ssd_coco_labels.txt))     |\n| SSD-Mobilenet-v2        | `ssd-mobilenet-v2` | `SSD_MOBILENET_V2` | 91 ([COCO classes](../data/networks/ssd_coco_labels.txt))     |\n| SSD-Inception-v2        | `ssd-inception-v2` | `SSD_INCEPTION_V2` | 91 ([COCO classes](../data/networks/ssd_coco_labels.txt))     |\n| TAO PeopleNet           | `peoplenet`        | `PEOPLENET`        | person, bag, face    |\n| TAO PeopleNet (pruned)  | `peoplenet-pruned` | `PEOPLENET_PRUNED` | person, bag, face    |\n| TAO DashCamNet          | `dashcamnet`       | `DASHCAMNET`       | person, car, bike, sign |\n| TAO TrafficCamNet       | `trafficcamnet`    | `TRAFFICCAMNET`    | person, car, bike, sign | \n| TAO FaceDetect          | `facedetect`       | `FACEDETECT`       | face                 |\n\n<details>\n<summary>Legacy Detection Models</summary>\n\n| Model                   | CLI argument       | NetworkType enum   | Object classes       |\n| ------------------------|--------------------|--------------------|----------------------|\n| DetectNet-COCO-Dog      | `coco-dog`         | `COCO_DOG`         | dogs                 |\n| DetectNet-COCO-Bottle   | `coco-bottle`      | `COCO_BOTTLE`      | bottles              |\n| DetectNet-COCO-Chair    | `coco-chair`       | `COCO_CHAIR`       | chairs               |\n| DetectNet-COCO-Airplane | `coco-airplane`    | `COCO_AIRPLANE`    | airplanes            |\n| ped-100                 | `pednet`           | `PEDNET`           | pedestrians          |\n| multiped-500            | `multiped`         | `PEDNET_MULTI`     | pedestrians, luggage |\n| facenet-120             | `facenet`          | `FACENET`          | faces                |\n\n</details>\n\n#### Semantic Segmentation\n\n| Dataset      | Resolution | CLI Argument | Accuracy | Jetson Nano | Jetson Xavier |\n|:------------:|:----------:|--------------|:--------:|:-----------:|:-------------:|\n| [Cityscapes](https://www.cityscapes-dataset.com/) | 512x256 | `fcn-resnet18-cityscapes-512x256` | 83.3% | 48 FPS | 480 FPS |\n| [Cityscapes](https://www.cityscapes-dataset.com/) | 1024x512 | `fcn-resnet18-cityscapes-1024x512` | 87.3% | 12 FPS | 175 FPS |\n| [Cityscapes](https://www.cityscapes-dataset.com/) | 2048x1024 | `fcn-resnet18-cityscapes-2048x1024` | 89.6% | 3 FPS | 47 FPS |\n| [DeepScene](http://deepscene.cs.uni-freiburg.de/) | 576x320 | `fcn-resnet18-deepscene-576x320` | 96.4% | 26 FPS | 360 FPS |\n| [DeepScene](http://deepscene.cs.uni-freiburg.de/) | 864x480 | `fcn-resnet18-deepscene-864x480` | 96.9% | 14 FPS | 190 FPS |\n| [Multi-Human](https://lv-mhp.github.io/) | 512x320 | `fcn-resnet18-mhp-512x320` | 86.5% | 34 FPS | 370 FPS |\n| [Multi-Human](https://lv-mhp.github.io/) | 640x360 | `fcn-resnet18-mhp-512x320` | 87.1% | 23 FPS | 325 FPS |\n| [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) | 320x320 | `fcn-resnet18-voc-320x320` | 85.9% | 45 FPS | 508 FPS |\n| [Pascal VOC](http://host.robots.ox.ac.uk/pascal/VOC/) | 512x320 | `fcn-resnet18-voc-512x320` | 88.5% | 34 FPS | 375 FPS |\n| [SUN RGB-D](http://rgbd.cs.princeton.edu/) | 512x400 | `fcn-resnet18-sun-512x400` | 64.3% | 28 FPS | 340 FPS |\n| [SUN RGB-D](http://rgbd.cs.princeton.edu/) | 640x512 | `fcn-resnet18-sun-640x512` | 65.1% | 17 FPS | 224 FPS |\n\n* If the resolution is omitted from the CLI argument, the lowest resolution model is loaded\n* Accuracy indicates the pixel classification accuracy across the model's validation dataset\n* Performance is measured for GPU FP16 mode with JetPack 4.2.1, `nvpmodel 0` (MAX-N)\n\n<details>\n<summary>Legacy Segmentation Models</summary>\n\n| Network                 | CLI Argument                    | NetworkType enum                | Classes |\n| ------------------------|---------------------------------|---------------------------------|---------|\n| Cityscapes (2048x2048)  | `fcn-alexnet-cityscapes-hd`     | `FCN_ALEXNET_CITYSCAPES_HD`     |    21   |\n| Cityscapes (1024x1024)  | `fcn-alexnet-cityscapes-sd`     | `FCN_ALEXNET_CITYSCAPES_SD`     |    21   |\n| Pascal VOC (500x356)    | `fcn-alexnet-pascal-voc`        | `FCN_ALEXNET_PASCAL_VOC`        |    21   |\n| Synthia (CVPR16)        | `fcn-alexnet-synthia-cvpr`      | `FCN_ALEXNET_SYNTHIA_CVPR`      |    14   |\n| Synthia (Summer-HD)     | `fcn-alexnet-synthia-summer-hd` | `FCN_ALEXNET_SYNTHIA_SUMMER_HD` |    14   |\n| Synthia (Summer-SD)     | `fcn-alexnet-synthia-summer-sd` | `FCN_ALEXNET_SYNTHIA_SUMMER_SD` |    14   |\n| Aerial-FPV (1280x720)   | `fcn-alexnet-aerial-fpv-720p`   | `FCN_ALEXNET_AERIAL_FPV_720p`   |     2   |\n\n</details>\n\n#### Pose Estimation\n\n| Model                   | CLI argument       | NetworkType enum   | Keypoints |\n| ------------------------|--------------------|--------------------|-----------|\n| Pose-ResNet18-Body      | `resnet18-body`    | `RESNET18_BODY`    | 18        |\n| Pose-ResNet18-Hand      | `resnet18-hand`    | `RESNET18_HAND`    | 21        |\n| Pose-DenseNet121-Body   | `densenet121-body` | `DENSENET121_BODY` | 18        |\n\n#### Action Recognition\n\n| Model                    | CLI argument | Classes |\n| -------------------------|--------------|---------|\n| Action-ResNet18-Kinetics | `resnet18`   |  1040   |\n| Action-ResNet34-Kinetics | `resnet34`   |  1040   |\n\n## Recommended System Requirements\n\n* Jetson Nano Developer Kit with JetPack 4.2 or newer (Ubuntu 18.04 aarch64).  \n* Jetson Nano 2GB Developer Kit with JetPack 4.4.1 or newer (Ubuntu 18.04 aarch64).\n* Jetson Orin Nano Developer Kit with JetPack 5.0 or newer (Ubuntu 20.04 aarch64).\n* Jetson Xavier NX Developer Kit with JetPack 4.4 or newer (Ubuntu 18.04 aarch64).  \n* Jetson AGX Xavier Developer Kit with JetPack 4.0 or newer (Ubuntu 18.04 aarch64).  \n* Jetson AGX Orin Developer Kit with JetPack 5.0 or newer (Ubuntu 20.04 aarch64).\n* Jetson TX2 Developer Kit with JetPack 3.0 or newer (Ubuntu 16.04 aarch64).  \n* Jetson TX1 Developer Kit with JetPack 2.3 or newer (Ubuntu 16.04 aarch64).  \n\nThe [Transfer Learning with PyTorch](#training) section of the tutorial speaks from the perspective of running PyTorch onboard Jetson for training DNNs, however the same PyTorch code can be used on a PC, server, or cloud instance with an NVIDIA discrete GPU for faster training.\n\n\n## Extra Resources\n\nIn this area, links and resources for deep learning are listed:\n\n* [ros_deep_learning](http://www.github.com/dusty-nv/ros_deep_learning) - TensorRT inference ROS nodes\n* [NVIDIA AI IoT](https://github.com/NVIDIA-AI-IOT) - NVIDIA Jetson GitHub repositories\n* [Jetson eLinux Wiki](https://www.eLinux.org/Jetson) - Jetson eLinux Wiki\n\n\n## Two Days to a Demo (DIGITS)\n\n> **note:** the DIGITS/Caffe tutorial from below is deprecated.  It's recommended to follow the [Transfer Learning with PyTorch](#training) tutorial from Hello AI World.\n \n<details>\n<summary>Expand this section to see original DIGITS tutorial (deprecated)</summary>\n<br/>\nThe DIGITS tutorial includes training DNN's in the cloud or PC, and inference on the Jetson with TensorRT, and can take roughly two days or more depending on system setup, downloading the datasets, and the training speed of your GPU.\n\n* [DIGITS Workflow](docs/digits-workflow.md) \n* [DIGITS System Setup](docs/digits-setup.md)\n* [Setting up Jetson with JetPack](docs/jetpack-setup.md)\n* [Building the Project from Source](docs/building-repo.md)\n* [Classifying Images with ImageNet](docs/imagenet-console.md)\n\t* [Using the Console Program on Jetson](docs/imagenet-console.md#using-the-console-program-on-jetson)\n\t* [Coding Your Own Image Recognition Program](docs/imagenet-example.md)\n\t* [Running the Live Camera Recognition Demo](docs/imagenet-camera.md)\n\t* [Re-Training the Network with DIGITS](docs/imagenet-training.md)\n\t* [Downloading Image Recognition Dataset](docs/imagenet-training.md#downloading-image-recognition-dataset)\n\t* [Customizing the Object Classes](docs/imagenet-training.md#customizing-the-object-classes)\n\t* [Importing Classification Dataset into DIGITS](docs/imagenet-training.md#importing-classification-dataset-into-digits)\n\t* [Creating Image Classification Model with DIGITS](docs/imagenet-training.md#creating-image-classification-model-with-digits)\n\t* [Testing Classification Model in DIGITS](docs/imagenet-training.md#testing-classification-model-in-digits)\n\t* [Downloading Model Snapshot to Jetson](docs/imagenet-snapshot.md)\n\t* [Loading Custom Models on Jetson](docs/imagenet-custom.md)\n* [Locating Objects with DetectNet](docs/detectnet-training.md)\n\t* [Detection Data Formatting in DIGITS](docs/detectnet-training.md#detection-data-formatting-in-digits)\n\t* [Downloading the Detection Dataset](docs/detectnet-training.md#downloading-the-detection-dataset)\n\t* [Importing the Detection Dataset into DIGITS](docs/detectnet-training.md#importing-the-detection-dataset-into-digits)\n\t* [Creating DetectNet Model with DIGITS](docs/detectnet-training.md#creating-detectnet-model-with-digits)\n\t* [Testing DetectNet Model Inference in DIGITS](docs/detectnet-training.md#testing-detectnet-model-inference-in-digits)\n\t* [Downloading the Detection Model to Jetson](docs/detectnet-snapshot.md)\n\t* [DetectNet Patches for TensorRT](docs/detectnet-snapshot.md#detectnet-patches-for-tensorrt)\n\t* [Detecting Objects from the Command Line](docs/detectnet-console.md)\n\t* [Multi-class Object Detection Models](docs/detectnet-console.md#multi-class-object-detection-models)\n\t* [Running the Live Camera Detection Demo on Jetson](docs/detectnet-camera.md)\n* [Semantic Segmentation with SegNet](docs/segnet-dataset.md)\n\t* [Downloading Aerial Drone Dataset](docs/segnet-dataset.md#downloading-aerial-drone-dataset)\n\t* [Importing the Aerial Dataset into DIGITS](docs/segnet-dataset.md#importing-the-aerial-dataset-into-digits)\n\t* [Generating Pretrained FCN-Alexnet](docs/segnet-pretrained.md)\n\t* [Training FCN-Alexnet with DIGITS](docs/segnet-training.md)\n\t* [Testing Inference Model in DIGITS](docs/segnet-training.md#testing-inference-model-in-digits)\n\t* [FCN-Alexnet Patches for TensorRT](docs/segnet-patches.md)\n\t* [Running Segmentation Models on Jetson](docs/segnet-console.md)\n\n</details>\n\n##\n<p align=\"center\"><sup>© 2016-2019 NVIDIA | </sup><a href=\"#deploying-deep-learning\"><sup>Table of Contents</sup></a></p>\n\n"
        },
        {
          "name": "c",
          "type": "tree",
          "content": null
        },
        {
          "name": "data",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "ros",
          "type": "commit",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "utils",
          "type": "commit",
          "content": null
        }
      ]
    }
  ]
}