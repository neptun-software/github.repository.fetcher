{
  "metadata": {
    "timestamp": 1736566179842,
    "page": 187,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "yuesong-feng/30dayMakeCppServer",
      "stars": 5977,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".DS_Store",
          "type": "blob",
          "size": 6.00390625,
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 5.2216796875,
          "content": "# 30天自制C++服务器\n\n>该教程是本人学生时代初学C++的历程，工作后已无精力写完剩下部分，回顾当年的代码有诸多不完美甚至瑕疵，有意愿者可以自由修改、开发、续写该项目。\n>更完美的C语言基础库请移步[wheelib](https://github.com/yuesong-feng/wheelib)\n\n先说结论：不管使用什么语言，一切后台开发的根基，是面向Linux的C/C++服务器开发。\n\n几乎所有高并发服务器都是运行在Linux环境的，笔者之前也用Java、node写过服务器，但最后发现只是学会了一门技术、一门语言，而并不了解底层的基础原理。一个HTTP请求的过程，为什么可以实现高并发，如何控制TCP连接，如何处理好数据传输的逻辑等等，这些只有面向C/C++编程才能深入了解。\n\n本教程模仿《30天自制操作系统》，面向零经验的新手，教你在30天内入门Linux服务器开发。本教程更偏向实践，将会把重点放在如何写代码上，而不会花太多的篇幅讲解背后的计算机基础原理，涉及到的地方会给出相应书籍的具体章节，但这并不代表这些理论知识不重要，事实上理论基础相当重要，没有理论的支撑，构建出一个高性能服务器是无稽之谈。\n\n本教程希望读者：\n\n- 熟悉C/C++语言\n- 熟悉计算机网络基础，如TCP协议、socket原理等\n- 了解基本的操作系统基础概念，如进程、线程、内存资源、系统调用等\n\n学完本教程后，你将会很轻松地看懂muduo源码。\n\nC/C++学习的一个难点在于初学时无法做出实际上的东西，没有反馈，程序都在黑乎乎的命令行里运行，不像web开发，可以随时看到自己学习的成果。本教程的代码都放在code文件夹里，每一天学习后都可以得到一个可以编译运行的服务器，不断迭代开发。\n\n在code文件夹里有每一天的代码文件夹，进入该文件夹，使用`make`命令编译，会生成两个可执行文件，输入命令`./server`就能看到今天的学习成果！然后新建一个Terminal，然后输入`./client`运行客户端，与服务器交互。\n\n[day01-从一个最简单的socket开始](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day01-从一个最简单的socket开始.md)\n\n[day02-不要放过任何一个错误](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day02-不要放过任何一个错误.md)\n\n[day03-高并发还得用epoll](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day03-高并发还得用epoll.md)\n\n[day04-来看看我们的第一个类](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day04-来看看我们的第一个类.md)\n\n[day05-epoll高级用法-Channel登场](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day05-epoll高级用法-Channel登场.md)\n\n[day06-服务器与事件驱动核心类登场](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day06-服务器与事件驱动核心类登场.md)\n\n[day07-为我们的服务器添加一个Acceptor](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day07-为我们的服务器添加一个Acceptor.md)\n\n[day08-一切皆是类，连TCP连接也不例外](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day08-一切皆是类，连TCP连接也不例外.md)\n\n[day09-缓冲区-大作用](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day09-缓冲区-大作用.md)\n\n[day10-加入线程池到服务器](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day10-加入线程池到服务器.md)\n\n[day11-完善线程池，加入一个简单的测试程序](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day11-完善线程池，加入一个简单的测试程序.md)\n\n[day12-将服务器改写为主从Reactor多线程模式](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day12-将服务器改写为主从Reactor多线程模式.md)\n\n[day13-C++工程化、代码分析、性能优化](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day13-C++工程化、代码分析、性能优化.md)\n\n[day14-支持业务逻辑自定义、完善Connection类](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day14-支持业务逻辑自定义、完善Connection类.md)\n\n[day15-macOS支持、完善业务逻辑自定义](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day15-macOS支持、完善业务逻辑自定义.md)\n\n[day16-重构服务器、使用智能指针](https://github.com/yuesong-feng/30dayMakeCppServer/blob/main/day16-重构核心库、使用智能指针.md)\n\n### todo list\n\n定时器\n\n日志系统\n\nHTTP协议支持\n\nwebbench测试\n\n文件下载断点续传\n\n静态资源存储\n\n......\n\n[Wlgls/30daysCppWebServer](https://github.com/Wlgls/30daysCppWebServer)项目尝试续写了后续部分，可供学习参考\n\n## Contribute\n\n能力一般、水平有限，如果发现我的教程有不正确或者值得改进的地方，欢迎提issue或直接PR。\n\n欢迎大家为本项目贡献自己的代码，如果有你觉得更好的代码，请提issue或者直接PR，所有建议都会被考虑。\n\n贡献代码请到[pine](https://github.com/yuesong-feng/pine)项目，这是本教程开发的网络库，也是最新的代码版本。\n"
        },
        {
          "name": "code",
          "type": "tree",
          "content": null
        },
        {
          "name": "day01-从一个最简单的socket开始.md",
          "type": "blob",
          "size": 6.28125,
          "content": "# day01-从一个最简单的socket开始\n\n如果读者之前有计算机网络的基础知识那就更好了，没有也没关系，socket编程非常容易上手。但本教程主要偏向实践，不会详细讲述计算机网络协议、网络编程原理等。想快速入门可以看以下博客，讲解比较清楚、错误较少：\n- [计算机网络基础知识总结](https://www.runoob.com/w3cnote/summary-of-network.html)\n\n要想打好基础，抄近道是不可的，有时间一定要认真学一遍谢希仁的《计算机网络》，要想精通服务器开发，这必不可少。\n\n首先在服务器，我们需要建立一个socket套接字，对外提供一个网络通信接口，在Linux系统中这个套接字竟然仅仅是一个文件描述符，也就是一个`int`类型的值！这个对套接字的所有操作（包括创建）都是最底层的系统调用。\n> 在这里读者务必先了解什么是Linux系统调用和文件描述符，《现代操作系统》第四版第一章有详细的讨论。如果你想抄近道看博客，C语言中文网的这篇文章讲了一部分：[socket是什么？套接字是什么？](http://c.biancheng.net/view/2123.html)\n\n> Unix哲学KISS：keep it simple, stupid。在Linux系统里，一切看上去十分复杂的逻辑功能，都用简单到不可思议的方式实现，甚至有些时候看上去很愚蠢。但仔细推敲，人们将会赞叹Linux的精巧设计，或许这就是大智若愚。\n```cpp\n#include <sys/socket.h>\nint sockfd = socket(AF_INET, SOCK_STREAM, 0);\n```\n- 第一个参数：IP地址类型，AF_INET表示使用IPv4，如果使用IPv6请使用AF_INET6。\n- 第二个参数：数据传输方式，SOCK_STREAM表示流格式、面向连接，多用于TCP。SOCK_DGRAM表示数据报格式、无连接，多用于UDP。\n- 第三个参数：协议，0表示根据前面的两个参数自动推导协议类型。设置为IPPROTO_TCP和IPPTOTO_UDP，分别表示TCP和UDP。\n\n对于客户端，服务器存在的唯一标识是一个IP地址和端口，这时候我们需要将这个套接字绑定到一个IP地址和端口上。首先创建一个sockaddr_in结构体\n```cpp\n#include <arpa/inet.h>  //这个头文件包含了<netinet/in.h>，不用再次包含了\nstruct sockaddr_in serv_addr;\nbzero(&serv_addr, sizeof(serv_addr));\n```\n然后使用`bzero`初始化这个结构体，这个函数在头文件`<string.h>`或`<cstring>`中。这里用到了两条《Effective C++》的准则：\n> 条款04: 确定对象被使用前已先被初始化。如果不清空，使用gdb调试器查看addr内的变量，会是一些随机值，未来可能会导致意想不到的问题。\n\n> 条款01: 视C++为一个语言联邦。把C和C++看作两种语言，写代码时需要清楚地知道自己在写C还是C++。如果在写C，请包含头文件`<string.h>`。如果在写C++，请包含`<cstring>`。\n\n设置地址族、IP地址和端口：\n```cpp\nserv_addr.sin_family = AF_INET;\nserv_addr.sin_addr.s_addr = inet_addr(\"127.0.0.1\");\nserv_addr.sin_port = htons(8888);\n```\n然后将socket地址与文件描述符绑定：\n```cpp\nbind(sockfd, (sockaddr*)&serv_addr, sizeof(serv_addr));\n```\n> 为什么定义的时候使用专用socket地址（sockaddr_in）而绑定的时候要转化为通用socket地址（sockaddr），以及转化IP地址和端口号为网络字节序的`inet_addr`和`htons`等函数及其必要性，在游双《Linux高性能服务器编程》第五章第一节：socket地址API中有详细讨论。\n\n最后我们需要使用`listen`函数监听这个socket端口，这个函数的第二个参数是listen函数的最大监听队列长度，系统建议的最大值`SOMAXCONN`被定义为128。\n```cpp\nlisten(sockfd, SOMAXCONN);\n```\n要接受一个客户端连接，需要使用`accept`函数。对于每一个客户端，我们在接受连接时也需要保存客户端的socket地址信息，于是有以下代码：\n```cpp\nstruct sockaddr_in clnt_addr;\nsocklen_t clnt_addr_len = sizeof(clnt_addr);\nbzero(&clnt_addr, sizeof(clnt_addr));\nint clnt_sockfd = accept(sockfd, (sockaddr*)&clnt_addr, &clnt_addr_len);\nprintf(\"new client fd %d! IP: %s Port: %d\\n\", clnt_sockfd, inet_ntoa(clnt_addr.sin_addr), ntohs(clnt_addr.sin_port));\n```\n要注意和`accept`和`bind`的第三个参数有一点区别，对于`bind`只需要传入serv_addr的大小即可，而`accept`需要写入客户端socket长度，所以需要定义一个类型为`socklen_t`的变量，并传入这个变量的地址。另外，`accept`函数会阻塞当前程序，直到有一个客户端socket被接受后程序才会往下运行。\n\n到现在，客户端已经可以通过IP地址和端口号连接到这个socket端口了，让我们写一个测试客户端连接试试：\n```cpp\nint sockfd = socket(AF_INET, SOCK_STREAM, 0);\nstruct sockaddr_in serv_addr;\nbzero(&serv_addr, sizeof(serv_addr));\nserv_addr.sin_family = AF_INET;\nserv_addr.sin_addr.s_addr = inet_addr(\"127.0.0.1\");\nserv_addr.sin_port = htons(8888);\nconnect(sockfd, (sockaddr*)&serv_addr, sizeof(serv_addr));  \n```\n代码和服务器代码几乎一样：创建一个socket文件描述符，与一个IP地址和端口绑定，最后并不是监听这个端口，而是使用`connect`函数尝试连接这个服务器。\n\n至此，day01的教程已经结束了，进入`code/day01`文件夹，使用make命令编译，将会得到`server`和`client`。输入命令`./server`开始运行，直到`accept`函数，程序阻塞、等待客户端连接。然后在一个新终端输入命令`./client`运行客户端，可以看到服务器接收到了客户端的连接请求，并成功连接。\n```\nnew client fd 3! IP: 127.0.0.1 Port: 53505\n```\n但如果我们先运行客户端、后运行服务器，在客户端一侧无任何区别，却并没有连接服务器成功，因为我们day01的程序没有任何的错误处理。\n\n事实上对于如`socket`,`bind`,`listen`,`accept`,`connect`等函数，通过返回值以及`errno`可以确定程序运行的状态、是否发生错误。在day02的教程中，我们会进一步完善整个服务器，处理所有可能的错误，并实现一个echo服务器（客户端发送给服务器一个字符串，服务器收到后返回相同的内容）。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day01](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day01)"
        },
        {
          "name": "day02-不要放过任何一个错误.md",
          "type": "blob",
          "size": 7.171875,
          "content": "# day02-不要放过任何一个错误\n在上一天，我们写了一个客户端发起socket连接和一个服务器接受socket连接。然而对于`socket`,`bind`,`listen`,`accept`,`connect`等函数，我们都设想程序完美地、没有任何异常地运行，而这显然是不可能的，不管写代码水平多高，就算你是林纳斯，也会在程序里写出bug。\n\n在《Effective C++》中条款08讲到，别让异常逃离析构函数。在这里我拓展一下，我们不应该放过每一个异常，否则在大型项目开发中一定会遇到很难定位的bug！\n> 具体信息可以参考《Effective C++》原书条款08，这里不再赘述。\n\n对于Linux系统调用，常见的错误提示方式是使用返回值和设置errno来说明错误类型。\n> 详细的C++语言异常处理请参考《C++ Primer》第五版第五章第六节\n\n通常来讲，当一个系统调用返回-1，说明有error发生。我们来看看socket编程最常见的错误处理模版：\n```cpp\nint sockfd = socket(AF_INET, SOCK_STREAM, 0);\nif(sockfd == -1)\n{\n    print(\"socket create error\");\n    exit(-1);\n}\n```\n为了处理一个错误，需要至少占用五行代码，这使编程十分繁琐，程序也不好看，异常处理所占篇幅比程序本身都多。\n\n为了方便编码以及代码的可读性，可以封装一个错误处理函数：\n```cpp\nvoid errif(bool condition, const char *errmsg){\n    if(condition){\n        perror(errmsg);\n        exit(EXIT_FAILURE);\n    }\n}\n```\n第一个参数是是否发生错误，如果为真，则表示有错误发生，会调用`<stdio.h>`头文件中的`perror`，这个函数会打印出`errno`的实际意义，还会打印出我们传入的字符串，也就是第函数第二个参数，让我们很方便定位到程序出现错误的地方。然后使用`<stdlib.h>`中的`exit`函数让程序退出并返回一个预定义常量`EXIT_FAILURE`。\n\n在使用的时候:\n```cpp\nint sockfd = socket(AF_INET, SOCK_STREAM, 0);\nerrif(sockfd == -1, \"socket create error\");\n```\n这样我们只需要使用一行进行错误处理，写起来方便简单，也输出了自定义信息，用于定位bug。\n\n对于所有的函数，我们都使用这种方式处理错误：\n```cpp\nerrif(bind(sockfd, (sockaddr*)&serv_addr, sizeof(serv_addr)) == -1, \"socket bind error\");\nerrif(listen(sockfd, SOMAXCONN) == -1, \"socket listen error\");\nint clnt_sockfd = accept(sockfd, (sockaddr*)&clnt_addr, &clnt_addr_len);\nerrif(clnt_sockfd == -1, \"socket accept error\");\nerrif(connect(sockfd, (sockaddr*)&serv_addr, sizeof(serv_addr)) == -1, \"socket connect error\");\n```\n到现在最简单的错误处理函数已经封装好了，但这仅仅用于本教程的开发，在真实的服务器开发中，错误绝不是一个如此简单的话题。\n\n当我们建立一个socket连接后，就可以使用`<unistd.h>`头文件中`read`和`write`来进行网络接口的数据读写操作了。\n> 这两个函数用于TCP连接。如果是UDP，需要使用`sendto`和`recvfrom`，这些函数的详细用法可以参考游双《Linux高性能服务器编程》第五章第八节。\n\n接下来的教程用注释的形式写在代码中，先来看服务器代码：\n```cpp\nwhile (true) {\n    char buf[1024];     //定义缓冲区\n    bzero(&buf, sizeof(buf));       //清空缓冲区\n    ssize_t read_bytes = read(clnt_sockfd, buf, sizeof(buf)); //从客户端socket读到缓冲区，返回已读数据大小\n    if(read_bytes > 0){\n        printf(\"message from client fd %d: %s\\n\", clnt_sockfd, buf);  \n        write(clnt_sockfd, buf, sizeof(buf));           //将相同的数据写回到客户端\n    } else if(read_bytes == 0){             //read返回0，表示EOF\n        printf(\"client fd %d disconnected\\n\", clnt_sockfd);\n        close(clnt_sockfd);\n        break;\n    } else if(read_bytes == -1){        //read返回-1，表示发生错误，按照上文方法进行错误处理\n        close(clnt_sockfd);\n        errif(true, \"socket read error\");\n    }\n}\n```\n客户端代码逻辑是一样的：\n```cpp\nwhile(true){\n    char buf[1024];     //定义缓冲区\n    bzero(&buf, sizeof(buf));       //清空缓冲区\n    scanf(\"%s\", buf);             //从键盘输入要传到服务器的数据\n    ssize_t write_bytes = write(sockfd, buf, sizeof(buf));      //发送缓冲区中的数据到服务器socket，返回已发送数据大小\n    if(write_bytes == -1){          //write返回-1，表示发生错误\n        printf(\"socket already disconnected, can't write any more!\\n\");\n        break;\n    }\n    bzero(&buf, sizeof(buf));       //清空缓冲区 \n    ssize_t read_bytes = read(sockfd, buf, sizeof(buf));    //从服务器socket读到缓冲区，返回已读数据大小\n    if(read_bytes > 0){\n        printf(\"message from server: %s\\n\", buf);\n    }else if(read_bytes == 0){      //read返回0，表示EOF，通常是服务器断开链接，等会儿进行测试\n        printf(\"server socket disconnected!\\n\");\n        break;\n    }else if(read_bytes == -1){     //read返回-1，表示发生错误，按照上文方法进行错误处理\n        close(sockfd);\n        errif(true, \"socket read error\");\n    }\n}\n```\n> 一个小细节，Linux系统的文件描述符理论上是有限的，在使用完一个fd之后，需要使用头文件`<unistd.h>`中的`close`函数关闭。更多内核相关知识可以参考Robert Love《Linux内核设计与实现》的第三版。\n\n至此，day02的主要教程已经结束了，完整源代码请在`code/day02`文件夹，接下来看看今天的学习成果以及测试我们的服务器！\n\n进入`code/day02`文件夹，使用make命令编译，将会得到`server`和`client`。输入命令`./server`开始运行，直到`accept`函数，程序阻塞、等待客户端连接。然后在一个新终端输入命令`./client`运行客户端，可以看到服务器接收到了客户端的连接请求，并成功连接。现在客户端阻塞在`scanf`函数，等待我们键盘输入，我们可以输入一句话，然后回车。在服务器终端，我们可以看到:\n```\nmessage from client fd 4: Hello!\n```\n然后在客户端，也能接受到服务器的消息：\n```\nmessage from server: Hello!\n```\n> 由于是一个`while(true)`循环，客户端可以一直输入，服务器也会一直echo我们的消息。由于`scanf`函数的特性，输入的语句遇到空格时，会当成多行进行处理，我们可以试试。\n\n接下来在客户端使用`control+c`终止程序，可以看到服务器也退出了程序并显示：\n```\nclient fd 4 disconnected\n```\n再次运行两个程序，这次我们使用`control+c`终止掉服务器，再试图从客户端发送信息，可以看到客户端输出：\n```\nserver socket disconnected!\n```\n至此，我们已经完整地开发了一个echo服务器，并且有最基本的错误处理！\n\n但现在，我们的服务器只能处理一个客户端，我们可以试试两个客户端同时连接服务器，看程序将会如何运行。在day03的教程里，我们将会讲解Linux系统高并发的基石--epoll，并编程实现一个可以支持无数客户端同时连接的echo服务器！\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day02](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day02)\n"
        },
        {
          "name": "day03-高并发还得用epoll.md",
          "type": "blob",
          "size": 7.060546875,
          "content": "# day03-高并发还得用epoll\n\n在上一天，我们写了一个简单的echo服务器，但只能同时处理一个客户端的连接。但在这个连接的生命周期中，绝大部分时间都是空闲的，活跃时间（发送数据和接收数据的时间）占比极少，这样独占一个服务器是严重的资源浪费。事实上所有的服务器都是高并发的，可以同时为成千上万个客户端提供服务，这一技术又被称为IO复用。\n> IO复用和多线程有相似之处，但绝不是一个概念。IO复用是针对IO接口，而多线程是针对CPU。\n\nIO复用的基本思想是事件驱动，服务器同时保持多个客户端IO连接，当这个IO上有可读或可写事件发生时，表示这个IO对应的客户端在请求服务器的某项服务，此时服务器响应该服务。在Linux系统中，IO复用使用select, poll和epoll来实现。epoll改进了前两者，更加高效、性能更好，是目前几乎所有高并发服务器的基石。请读者务必先掌握epoll的原理再进行编码开发。\n> select, poll与epoll的详细原理和区别请参考《UNIX网络编程：卷1》第二部分第六章，游双《Linux高性能服务器编程》第九章\n\nepoll主要由三个系统调用组成：\n```cpp\n//int epfd = epoll_create(1024);  //参数表示监听事件的大小，如超过内核会自动调整，已经被舍弃，无实际意义，传入一个大于0的数即可\nint epfd = epoll_create1(0);       //参数是一个flag，一般设为0，详细参考man epoll\n```\n创建一个epoll文件描述符并返回，失败则返回-1。\n\nepoll监听事件的描述符会放在一颗红黑树上，我们将要监听的IO口放入epoll红黑树中，就可以监听该IO上的事件。\n```cpp\nepoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);    //添加事件到epoll\nepoll_ctl(epfd, EPOLL_CTL_MOD, sockfd, &ev);    //修改epoll红黑树上的事件\nepoll_ctl(epfd, EPOLL_CTL_DEL, sockfd, NULL);   //删除事件\n```\n其中sockfd表示我们要添加的IO文件描述符，ev是一个epoll_event结构体，其中的events表示事件，如EPOLLIN等，data是一个用户数据union:\n```cpp\ntypedef union epoll_data {\n  void *ptr;\n  int fd;\n  uint32_t u32;\n  uint64_t u64;\n} epoll_data_t;\nstruct epoll_event {\n  uint32_t events;\t/* Epoll events */\n  epoll_data_t data;\t/* User data variable */\n} __EPOLL_PACKED;\n```\nepoll默认采用LT触发模式，即水平触发，只要fd上有事件，就会一直通知内核。这样可以保证所有事件都得到处理、不容易丢失，但可能发生的大量重复通知也会影响epoll的性能。如使用ET模式，即边缘触法，fd从无事件到有事件的变化会通知内核一次，之后就不会再次通知内核。这种方式十分高效，可以大大提高支持的并发度，但程序逻辑必须一次性很好地处理该fd上的事件，编程比LT更繁琐。注意ET模式必须搭配非阻塞式socket使用。\n> 非阻塞式socket和阻塞式有很大的不同，请参考《UNIX网络编程：卷1》第三部分第16章。\n\n我们可以随时使用`epoll_wait`获取有事件发生的fd：\n```cpp\nint nfds = epoll_wait(epfd, events, maxevents, timeout);\n```\n其中events是一个epoll_event结构体数组，maxevents是可供返回的最大事件大小，一般是events的大小，timeout表示最大等待时间，设置为-1表示一直等待。\n\n接下来将day02的服务器改写成epoll版本，基本思想为：在创建了服务器socket fd后，将这个fd添加到epoll，只要这个fd上发生可读事件，表示有一个新的客户端连接。然后accept这个客户端并将客户端的socket fd添加到epoll，epoll会监听客户端socket fd是否有事件发生，如果发生则处理事件。\n\n接下来的教程在伪代码中：\n```cpp\nint sockfd = socket(...);   //创建服务器socket fd\nbind(sockfd...);\nlisten(sockfd...);\nint epfd = epoll_create1(0);\nstruct epoll_event events[MAX_EVENTS], ev;\nev.events = EPOLLIN;    //在代码中使用了ET模式，且未处理错误，在day12进行了修复，实际上接受连接最好不要用ET模式\nev.data.fd = sockfd;    //该IO口为服务器socket fd\nepoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);    //将服务器socket fd添加到epoll\nwhile(true){    // 不断监听epoll上的事件并处理\n    int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);   //有nfds个fd发生事件\n    for(int i = 0; i < nfds; ++i){  //处理这nfds个事件\n        if(events[i].data.fd == sockfd){    //发生事件的fd是服务器socket fd，表示有新客户端连接\n            int clnt_sockfd = accept(sockfd, (sockaddr*)&clnt_addr, &clnt_addr_len);\n            ev.data.fd = clnt_sockfd;   \n            ev.events = EPOLLIN | EPOLLET;  //对于客户端连接，使用ET模式，可以让epoll更加高效，支持更多并发\n            setnonblocking(clnt_sockfd);    //ET需要搭配非阻塞式socket使用\n            epoll_ctl(epfd, EPOLL_CTL_ADD, clnt_sockfd, &ev);   //将该客户端的socket fd添加到epoll\n        } else if(events[i].events & EPOLLIN){      //发生事件的是客户端，并且是可读事件（EPOLLIN）\n            handleEvent(events[i].data.fd);         //处理该fd上发生的事件\n        }\n    }\n}\n```\n从一个非阻塞式socket fd上读取数据时：\n```cpp\nwhile(true){    //由于使用非阻塞IO，需要不断读取，直到全部读取完毕\n    ssize_t bytes_read = read(events[i].data.fd, buf, sizeof(buf));\n    if(bytes_read > 0){\n      //保存读取到的bytes_read大小的数据\n    } else if(bytes_read == -1 && errno == EINTR){  //客户端正常中断、继续读取\n        continue;\n    } else if(bytes_read == -1 && ((errno == EAGAIN) || (errno == EWOULDBLOCK))){//非阻塞IO，这个条件表示数据全部读取完毕\n        //该fd上数据读取完毕\n        break;\n    } else if(bytes_read == 0){  //EOF事件，一般表示客户端断开连接\n        close(events[i].data.fd);   //关闭socket会自动将文件描述符从epoll树上移除\n        break;\n    } //剩下的bytes_read == -1的情况表示其他错误，这里没有处理\n}\n```\n至此，day03的主要教程已经结束了，完整源代码请在`code/day03`文件夹，接下来看看今天的学习成果以及测试我们的服务器！\n\n进入`code/day03`文件夹，使用make命令编译，将会得到`server`和`client`，输入命令`./server`开始运行服务器。然后在一个新终端输入命令`./client`运行客户端，可以看到服务器接收到了客户端的连接请求，并成功连接。再新开一个或多个终端，运行client，可以看到这些客户端也同时连接到了服务器。此时我们在任意一个client输入一条信息，服务器都显示并发送到该客户端。如使用`control+c`终止掉某个client，服务器回显示这个client已经断开连接，但其他client并不受影响。\n\n至此，我们已经完整地开发了一个echo服务器，并且支持多个客户端同时连接，为他们提供服务！\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day03](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day03)\n"
        },
        {
          "name": "day04-来看看我们的第一个类.md",
          "type": "blob",
          "size": 5.1494140625,
          "content": "# day04-来看看我们的第一个类\n\n在上一天，我们开发了一个支持多个客户端连接的服务器，但到目前为止，虽然我们的程序以`.cpp`结尾，本质上我们写的仍然是C语言程序。虽然C++语言完全兼容C语言并且大部分程序中都是混用，但一个很好的习惯是把C和C++看作两种语言，写代码时需要清楚地知道自己在写C还是C++。\n\n另一点是我们的程序会变得越来越长、越来越庞大，虽然现在才不到100行代码，但把所有逻辑放在一个程序里显然是一种错误的做法，我们需要对程序进行模块化，每一个模块专门处理一个任务，这样可以增加程序的可读性，也可以写出更大庞大、功能更加复杂的程序。不仅如此，还可以很方便地进行代码复用，也就是造轮子。\n\nC++是一门面向对象的语言，最低级的模块化的方式就是构建一个类。举个例子，我们的程序有新建服务器socket、绑定IP地址、监听、接受客户端连接等任务，代码如下：\n```cpp\nint sockfd = socket(AF_INET, SOCK_STREAM, 0);\nerrif(sockfd == -1, \"socket create error\");\n\nstruct sockaddr_in serv_addr;\nbzero(&serv_addr, sizeof(serv_addr));\nserv_addr.sin_family = AF_INET;\nserv_addr.sin_addr.s_addr = inet_addr(\"127.0.0.1\");\nserv_addr.sin_port = htons(8888);\n\nerrif(bind(sockfd, (sockaddr*)&serv_addr, sizeof(serv_addr)) == -1, \"socket bind error\");\n\nerrif(listen(sockfd, SOMAXCONN) == -1, \"socket listen error\");\n\nstruct sockaddr_in clnt_addr;\nbzero(&clnt_addr, sizeof(clnt_addr));\nsocklen_t clnt_addr_len = sizeof(clnt_addr);\n\nint clnt_sockfd = accept(sockfd, (sockaddr*)&clnt_addr, &clnt_addr_len);\nerrif(clnt_sockfd == -1, \"socket accept error\");\n```\n可以看到代码有19行，这已经是使用socket最精简的代码。在服务器开发中，我们或许会建立多个socket口，或许会处理多个客户端连接，但我们并不希望每次都重复编写这么多行代码，我们希望这样使用：\n```cpp\nSocket *serv_sock = new Socket();\nInetAddress *serv_addr = new InetAddress(\"127.0.0.1\", 8888);\nserv_sock->bind(serv_addr);\nserv_sock->listen();   \nInetAddress *clnt_addr = new InetAddress();  \nSocket *clnt_sock = new Socket(serv_sock->accept(clnt_addr));    \n```\n仅仅六行代码就可以实现和之前一样的功能，这样的使用方式忽略了底层的语言细节，不用在程序中考虑错误处理，更简单、更加专注于程序的自然逻辑，大家毫无疑问也肯定希望以这样简单的方式使用socket。\n\n类似的还有epoll，最精简的使用方式为：\n```cpp\nint epfd = epoll_create1(0);\nerrif(epfd == -1, \"epoll create error\");\n\nstruct epoll_event events[MAX_EVENTS], ev;\nbzero(&events, sizeof(events) * MAX_EVENTS);\n\nbzero(&ev, sizeof(ev));\nev.data.fd = sockfd;\nev.events = EPOLLIN | EPOLLET;\n\nepoll_ctl(epfd, EPOLL_CTL_ADD, sockfd, &ev);\n\nwhile(true){\n    int nfds = epoll_wait(epfd, events, MAX_EVENTS, -1);\n    errif(nfds == -1, \"epoll wait error\");\n    for(int i = 0; i < nfds; ++i){\n        // handle event\n    }\n}\n```\n而我们更希望这样来使用：\n```cpp\nEpoll *ep = new Epoll();\nep->addFd(serv_sock->getFd(), EPOLLIN | EPOLLET);\nwhile(true){\n    vector<epoll_event> events = ep->poll();\n    for(int i = 0; i < events.size(); ++i){\n        // handle event\n    }\n}\n```\n同样完全忽略了如错误处理之类的底层细节，大大简化了编程，增加了程序的可读性。\n\n在今天的代码中，程序的功能和昨天一样，仅仅将`Socket`、`InetAddress`和`Epoll`封装成类，这也是面向对象编程的最核心、最基本的思想。现在我们的目录结构为：\n```\nclient.cpp\nEpoll.cpp\nEpoll.h\nInetAddress.cpp\nInetAddress.h\nMakefile\nserver.cpp\nSocket.cpp\nSocket.h\nutil.cpp\nutil.h\n```\n注意在编译程序的使用，需要编译`Socket`、`InetAddress`和`Epoll`类的`.cpp`文件，然后进行链接，因为`.h`文件里只是类的定义，并未实现。\n> C/C++程序编译、链接是一个很复杂的事情，具体原理请参考《深入理解计算机系统（第三版）》第七章。\n\n至此，day04的主要教程已经结束了，完整源代码请在`code/day04`文件夹，服务器的功能和昨天一样。\n\n进入`code/day04`文件夹，使用make命令编译，将会得到`server`和`client`，输入命令`./server`开始运行服务器。然后在一个新终端输入命令`./client`运行客户端，可以看到服务器接收到了客户端的连接请求，并成功连接。再新开一个或多个终端，运行client，可以看到这些客户端也同时连接到了服务器。此时我们在任意一个client输入一条信息，服务器都显示并发送到该客户端。如使用`control+c`终止掉某个client，服务器回显示这个client已经断开连接，但其他client并不受影响。\n\n至此，我们已经完整地开发了一个echo服务器，并且引入面向对象编程的思想，初步封装了`Socket`、`InetAddress`和`Epoll`，大大精简了主程序，隐藏了底层语言实现细节、增加了可读性。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day04](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day04)\n"
        },
        {
          "name": "day05-epoll高级用法-Channel登场.md",
          "type": "blob",
          "size": 5.5498046875,
          "content": "# day05-epoll高级用法-Channel登场\n\n在上一天，我们已经完整地开发了一个echo服务器，并且引入面向对象编程的思想，初步封装了`Socket`、`InetAddress`和`Epoll`，大大精简了主程序，隐藏了底层语言实现细节、增加了可读性。\n\n让我们来回顾一下我们是如何使用`epoll`：将一个文件描述符添加到`epoll`红黑树，当该文件描述符上有事件发生时，拿到它、处理事件，这样我们每次只能拿到一个文件描述符，也就是一个`int`类型的整型值。试想，如果一个服务器同时提供不同的服务，如HTTP、FTP等，那么就算文件描述符上发生的事件都是可读事件，不同的连接类型也将决定不同的处理逻辑，仅仅通过一个文件描述符来区分显然会很麻烦，我们更加希望拿到关于这个文件描述符更多的信息。\n\n在day03介绍`epoll`时，曾讲过`epoll_event`结构体：\n```cpp\ntypedef union epoll_data {\n  void *ptr;\n  int fd;\n  uint32_t u32;\n  uint64_t u64;\n} epoll_data_t;\nstruct epoll_event {\n  uint32_t events;\t/* Epoll events */\n  epoll_data_t data;\t/* User data variable */\n} __EPOLL_PACKED;\n```\n可以看到，epoll中的`data`其实是一个union类型，可以储存一个指针。而通过指针，理论上我们可以指向任何一个地址块的内容，可以是一个类的对象，这样就可以将一个文件描述符封装成一个`Channel`类，一个Channel类自始至终只负责一个文件描述符，对不同的服务、不同的事件类型，都可以在类中进行不同的处理，而不是仅仅拿到一个`int`类型的文件描述符。\n> 这里读者务必先了解C++中的union类型，在《C++ Primer（第五版）》第十九章第六节有详细说明。\n\n`Channel`类的核心成员如下：\n```cpp\nclass Channel{\nprivate:\n    Epoll *ep;\n    int fd;\n    uint32_t events;\n    uint32_t revents;\n    bool inEpoll;\n};\n```\n显然每个文件描述符会被分发到一个`Epoll`类，用一个`ep`指针来指向。类中还有这个`Channel`负责的文件描述符。另外是两个事件变量，`events`表示希望监听这个文件描述符的哪些事件，因为不同事件的处理方式不一样。`revents`表示在`epoll`返回该`Channel`时文件描述符正在发生的事件。`inEpoll`表示当前`Channel`是否已经在`epoll`红黑树中，为了注册`Channel`的时候方便区分使用`EPOLL_CTL_ADD`还是`EPOLL_CTL_MOD`。\n\n接下来以`Channel`的方式使用epoll：\n新建一个`Channel`时，必须说明该`Channel`与哪个`epoll`和`fd`绑定：\n```cpp\nChannel *servChannel = new Channel(ep, serv_sock->getFd());\n```\n这时该`Channel`还没有被添加到epoll红黑树，因为`events`没有被设置，不会监听该`Channel`上的任何事件发生。如果我们希望监听该`Channel`上发生的读事件，需要调用一个`enableReading`函数：\n```cpp\nservChannel->enableReading();\n```\n调用这个函数后，如`Channel`不在epoll红黑树中，则添加，否则直接更新`Channel`、打开允许读事件。`enableReading`函数如下：\n```cpp\nvoid Channel::enableReading(){\n    events = EPOLLIN | EPOLLET;\n    ep->updateChannel(this);\n}\n```\n可以看到该函数做了两件事，将要监听的事件`events`设置为读事件并采用ET模式，然后在ep指针指向的Epoll红黑树中更新该`Channel`，`updateChannel`函数的实现如下：\n```cpp\nvoid Epoll::updateChannel(Channel *channel){\n    int fd = channel->getFd();  //拿到Channel的文件描述符\n    struct epoll_event ev;\n    bzero(&ev, sizeof(ev));\n    ev.data.ptr = channel;\n    ev.events = channel->getEvents();   //拿到Channel希望监听的事件\n    if(!channel->getInEpoll()){\n        errif(epoll_ctl(epfd, EPOLL_CTL_ADD, fd, &ev) == -1, \"epoll add error\");//添加Channel中的fd到epoll\n        channel->setInEpoll();\n    } else{\n        errif(epoll_ctl(epfd, EPOLL_CTL_MOD, fd, &ev) == -1, \"epoll modify error\");//已存在，则修改\n    }\n}\n```\n在使用时，我们可以通过`Epoll`类中的`poll()`函数获取当前有事件发生的`Channel`：\n```cpp\nwhile(true){\n    vector<Channel*> activeChannels = ep->poll();\n    // activeChannels是所有有事件发生的Channel\n}\n```\n注：在今天教程的源代码中，并没有将事件处理改为使用`Channel`回调函数的方式，仍然使用了之前对文件描述符进行处理的方法，这是错误的，将在明天的教程中进行改写。\n\n至此，day05的主要教程已经结束了，完整源代码请在`code/day05`文件夹。服务器的功能和昨天一样，添加了`Channel`类，可以让我们更加方便简单、多样化地处理epoll中发生的事件。同时脱离了底层，将epoll、文件描述符和事件进行了抽象，形成了事件分发的模型，这也是Reactor模式的核心，将在明天的教程进行讲解。\n\n进入`code/day05`文件夹，使用make命令编译，将会得到`server`和`client`，输入命令`./server`开始运行服务器。然后在一个新终端输入命令`./client`运行客户端，可以看到服务器接收到了客户端的连接请求，并成功连接。再新开一个或多个终端，运行client，可以看到这些客户端也同时连接到了服务器。此时我们在任意一个client输入一条信息，服务器都显示并发送到该客户端。如使用`control+c`终止掉某个client，服务器回显示这个client已经断开连接，但其他client并不受影响。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day05](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day05)\n"
        },
        {
          "name": "day06-服务器与事件驱动核心类登场.md",
          "type": "blob",
          "size": 5.244140625,
          "content": "# day06-服务器与事件驱动核心类登场\n\n在上一天，我们为每一个添加到epoll的文件描述符都添加了一个`Channel`，用户可以自由注册各种事件、很方便地根据不同事件类型设置不同回调函数（在当前的源代码中只支持了目前所需的可读事件，将在之后逐渐进行完善）。我们的服务器已经基本成型，但目前从新建socket、接受客户端连接到处理客户端事件，整个程序结构是顺序化、流程化的，我们甚至可以使用一个单一的流程图来表示整个程序。而流程化程序设计的缺点之一是不够抽象，当我们的服务器结构越来越庞大、功能越来越复杂、模块越来越多，这种顺序程序设计的思想显然是不能满足需求的。\n\n对于服务器开发，我们需要用到更抽象的设计模式。从代码中我们可以看到，不管是接受客户端连接还是处理客户端事件，都是围绕epoll来编程，可以说epoll是整个程序的核心，服务器做的事情就是监听epoll上的事件，然后对不同事件类型进行不同的处理。这种以事件为核心的模式又叫事件驱动，事实上几乎所有的现代服务器都是事件驱动的。和传统的请求驱动模型有很大不同，事件的捕获、通信、处理和持久保留是解决方案的核心结构。libevent就是一个著名的C语言事件驱动库。\n\n需要注意的是，事件驱动不是服务器开发的专利。事件驱动是一种设计应用的思想、开发模式，而服务器是根据客户端的不同请求提供不同的服务的一个实体应用，服务器开发可以采用事件驱动模型、也可以不采用。事件驱动模型也可以在服务器之外的其他类型应用中出现，如进程通信、k8s调度、V8引擎、Node.js等。\n\n理解了以上的概念，就能容易理解服务器开发的两种经典模式——Reactor和Proactor模式。详细请参考游双《Linux高性能服务器编程》第八章第四节、陈硕《Linux多线程服务器编程》第六章第六节。\n\n> 如何深刻理解Reactor和Proactor？ - 小林coding的回答 - 知乎\nhttps://www.zhihu.com/question/26943938/answer/1856426252\n\n由于Linux内核系统调用的设计更加符合Reactor模式，所以绝大部分高性能服务器都采用Reactor模式进行开发，我们的服务器也使用这种模式。\n\n接下来我们要将服务器改造成Reactor模式。首先我们将整个服务器抽象成一个`Server`类，这个类中有一个main-Reactor（在这个版本没有sub-Reactor），里面的核心是一个`EventLoop`（libevent中叫做EventBase），这是一个事件循环，我们添加需要监听的事务到这个事件循环内，每次有事件发生时就会通知（在程序中返回给我们`Channel`），然后根据不同的描述符、事件类型进行处理（以回调函数的方式）。\n> 如果你不太清楚这个自然段在讲什么，请先看一看前面提到的两本书的具体章节。\n\nEventLoop类的定义如下：\n```cpp\nclass EventLoop {\nprivate:\n    Epoll *ep;\n    bool quit;\npublic:\n    EventLoop();\n    ~EventLoop();\n    void loop();\n    void updateChannel(Channel*);\n};\n```\n调用`loop()`函数可以开始事件驱动，实际上就是原来的程序中调用`epoll_wait()`函数的死循环：\n```cpp\nvoid EventLoop::loop(){\n    while(!quit){\n    std::vector<Channel*> chs;\n        chs = ep->poll();\n        for(auto it = chs.begin(); it != chs.end(); ++it){\n            (*it)->handleEvent();\n        }\n    }\n}\n```\n现在我们可以以这种方式来启动服务器，和muduo的代码已经很接近了：\n```cpp\nEventLoop *loop = new EventLoop();\nServer *server = new Server(loop);\nloop->loop();\n```\n服务器定义如下：\n```cpp\nclass Server {\nprivate:\n    EventLoop *loop;\npublic:\n    Server(EventLoop*);\n    ~Server();\n    void handleReadEvent(int);\n    void newConnection(Socket *serv_sock);\n};\n```\n这个版本服务器内只有一个`EventLoop`，当其中有可读事件发生时，我们可以拿到该描述符对应的`Channel`。在新建`Channel`时，根据`Channel`描述符的不同分别绑定了两个回调函数，`newConnection()`函数被绑定到服务器socket上，`handlrReadEvent()`被绑定到新接受的客户端socket上。这样如果服务器socket有可读事件，`Channel`里的`handleEvent()`函数实际上会调用`Server`类的`newConnection()`新建连接。如果客户端socket有可读事件，`Channel`里的`handleEvent()`函数实际上会调用`Server`类的`handlrReadEvent()`响应客户端请求。\n\n至此，我们已经抽象出了`EventLoop`和`Channel`，构成了事件驱动模型。这两个类和服务器核心`Server`已经没有任何关系，经过完善后可以被任何程序复用，达到了事件驱动的设计思想，现在我们的服务器也可以看成一个最简易的Reactor模式服务器。\n\n当然，这个Reactor模式并不是一个完整的Reactor模式，如处理事件请求仍然在事件驱动的线程里，这显然违背了Reactor的概念。我们还需要做很多工作，在接下来几天的教程里会进一步完善。\n\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day06](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day06)\n"
        },
        {
          "name": "day07-为我们的服务器添加一个Acceptor.md",
          "type": "blob",
          "size": 4.125,
          "content": "# day07-为我们的服务器添加一个Acceptor\n\n在上一天，我们分离了服务器类和事件驱动类，将服务器逐渐开发成Reactor模式。至此，所有服务器逻辑（目前只有接受新连接和echo客户端发来的数据）都写在`Server`类里。但很显然，`Server`作为一个服务器类，应该更抽象、更通用，我们应该对服务器进行进一步的模块化。\n\n仔细分析可发现，对于每一个事件，不管提供什么样的服务，首先需要做的事都是调用`accept()`函数接受这个TCP连接，然后将socket文件描述符添加到epoll。当这个IO口有事件发生的时候，再对此TCP连接提供相应的服务。\n> 在这里务必先理解TCP的面向连接这一特性，在谢希仁《计算机网络》里有详细的讨论。\n\n因此我们可以分离接受连接这一模块，添加一个`Acceptor`类，这个类有以下几个特点：\n- 类存在于事件驱动`EventLoop`类中，也就是Reactor模式的main-Reactor\n- 类中的socket fd就是服务器监听的socket fd，每一个Acceptor对应一个socket fd\n- 这个类也通过一个独有的`Channel`负责分发到epoll，该Channel的事件处理函数`handleEvent()`会调用Acceptor中的接受连接函数来新建一个TCP连接\n\n根据分析，Acceptor类定义如下：\n```cpp\nclass Acceptor{\nprivate:\n    EventLoop *loop;\n    Socket *sock;\n    InetAddress *addr;\n    Channel *acceptChannel;\npublic:\n    Acceptor(EventLoop *_loop);\n    ~Acceptor();\n    void acceptConnection();\n};\n```\n这样一来，新建连接的逻辑就在`Acceptor`类中。但逻辑上新socket建立后就和之前监听的服务器socket没有任何关系了，TCP连接和`Acceptor`一样，拥有以上提到的三个特点，这两个类之间应该是平行关系。所以新的TCP连接应该由`Server`类来创建并管理生命周期，而不是`Acceptor`。并且将这一部分代码放在`Server`类里也并没有打破服务器的通用性，因为对于所有的服务，都要使用`Acceptor`来建立连接。\n\n为了实现这一设计，我们可以用两种方式：\n1. 使用传统的虚类、虚函数来设计一个接口\n2. C++11的特性：std::function、std::bind、右值引用、std::move等实现函数回调\n\n虚函数使用起来比较繁琐，程序的可读性也不够清晰明朗，而std::function、std::bind等新标准的出现可以完全替代虚函数，所以本教程采用第二种方式。\n> 关于虚函数，在《C++ Primer》第十五章第三节有详细讨论，而C++11后的新标准可以参考欧长坤《现代 C++ 教程》\n\n首先我们需要在Acceptor中定义一个新建连接的回调函数：\n```cpp\nstd::function<void(Socket*)> newConnectionCallback;\n```\n在新建连接时，只需要调用这个回调函数：\n```cpp\nvoid Acceptor::acceptConnection(){\n    newConnectionCallback(sock);\n}\n```\n而这个回调函数本身的实现在`Server`类中：\n```cpp\nvoid Server::newConnection(Socket *serv_sock){\n    // 接受serv_sock上的客户端连接\n}\n```\n> 在今天的代码中，Acceptor的Channel使用了ET模式，事实上使用LT模式更合适，将在之后修复\n\n新建Acceptor时通过std::bind进行绑定:\n```cpp\nacceptor = new Acceptor(loop);\nstd::function<void(Socket*)> cb = std::bind(&Server::newConnection, this, std::placeholders::_1);\nacceptor->setNewConnectionCallback(cb);\n```\n这样一来，尽管我们抽象分离出了`Acceptor`，新建连接的工作任然由`Server`类来完成。\n> 请确保清楚地知道为什么要这么做再进行之后的学习。\n\n至此，今天的教程已经结束了。在今天，我们设计了服务器接受新连接的`Acceptor`类。测试方法和之前一样，使用`make`得到服务器和客户端程序并运行。虽然服务器功能已经好几天没有变化了，但每一天我们都在不断抽象、不断完善，从结构化、流程化的程序设计，到面向对象程序设计，再到面向设计模式的程序设计，逐渐学习服务器开发的思想与精髓。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day07](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day07)\n"
        },
        {
          "name": "day08-一切皆是类，连TCP连接也不例外.md",
          "type": "blob",
          "size": 4.5791015625,
          "content": "# day08-一切皆是类，连TCP连接也不例外\n\n在上一天，我们分离了用于接受连接的`Acceptor`类，并把新建连接的逻辑放在了`Server`类中。在上一天我们还提到了`Acceptor`类最主要的三个特点：\n- 类存在于事件驱动`EventLoop`类中，也就是Reactor模式的main-Reactor\n- 类中的socket fd就是服务器监听的socket fd，每一个Acceptor对应一个socket fd\n- 这个类也通过一个独有的`Channel`负责分发到epoll，该Channel的事件处理函数`handleEvent()`会调用Acceptor中的接受连接函数来新建一个TCP连接\n\n对于TCP协议，三次握手新建连接后，这个连接将会一直存在，直到我们四次挥手断开连接。因此，我们也可以把TCP连接抽象成一个`Connection`类，这个类也有以下几个特点：\n- 类存在于事件驱动`EventLoop`类中，也就是Reactor模式的main-Reactor\n- 类中的socket fd就是客户端的socket fd，每一个Connection对应一个socket fd\n- 每一个类的实例通过一个独有的`Channel`负责分发到epoll，该Channel的事件处理函数`handleEvent()`会调用Connection中的事件处理函数来响应客户端请求\n\n可以看到，`Connection`类和`Acceptor`类是平行关系、十分相似，他们都直接由`Server`管理，由一个`Channel`分发到epoll，通过回调函数处理相应事件。唯一的不同在于，`Acceptor`类的处理事件函数（也就是新建连接功能）被放到了`Server`类中，具体原因在上一天的教程中已经详细说明。而`Connection`类则没有必要这么做，处理事件的逻辑应该由`Connection`类本身来完成。\n\n另外，一个高并发服务器一般只会有一个`Acceptor`用于接受连接（也可以有多个），但可能会同时拥有成千上万个TCP连接，也就是成千上万个`Connection`类的实例，我们需要把这些TCP连接都保存起来。现在我们可以改写服务器核心`Server`类，定义如下：\n```cpp\nclass Server {\nprivate:\n    EventLoop *loop;    //事件循环\n    Acceptor *acceptor; //用于接受TCP连接\n    std::map<int, Connection*> connections; //所有TCP连接\npublic:\n    Server(EventLoop*);\n    ~Server();\n\n    void handleReadEvent(int);  //处理客户端请求\n    void newConnection(Socket *sock);   //新建TCP连接\n    void deleteConnection(Socket *sock);   //断开TCP连接\n};\n```\n在接受连接后，服务器把该TCP连接保存在一个`map`中，键为该连接客户端的socket fd，值为指向该连接的指针。该连接客户端的socket fd通过一个`Channel`类分发到epoll，该`Channel`的事件处理回调函数`handleEvent()`绑定为`Connection`的业务处理函数，这样每当该连接的socket fd上发生事件，就会通过`Channel`调用具体连接类的业务处理函数，伪代码如下：\n```cpp\nvoid Connection::echo(int sockfd){\n    // 回显sockfd发来的数据\n}\nConnection::Connection(EventLoop *_loop, Socket *_sock) : loop(_loop), sock(_sock), channel(nullptr){\n    channel = new Channel(loop, sock->getFd()); //该连接的Channel\n    std::function<void()> cb = std::bind(&Connection::echo, this, sock->getFd()); \n    channel->setCallback(cb); //绑定回调函数\n    channel->enableReading(); //打开读事件监听\n}\n```\n对于断开TCP连接操作，也就是销毁一个`Connection`类的实例。由于`Connection`的生命周期由`Server`进行管理，所以也应该由`Server`来删除连接。如果在`Connection`业务中需要断开连接操作，也应该和之前一样使用回调函数来实现，在`Server`新建每一个连接时绑定删除该连接的回调函数：\n```cpp\nConnection *conn = new Connection(loop, sock);\nstd::function<void(Socket*)> cb = std::bind(&Server::deleteConnection, this, std::placeholders::_1);\nconn->setDeleteConnectionCallback(cb);  // 绑定删除连接的回调函数\n\nvoid Server::deleteConnection(Socket * sock){\n    // 删除连接\n}\n```\n至此，今天的教程已经结束，我们将TCP连接抽象成一个类，服务器模型更加成型。测试方法和之前一样，使用`make`得到服务器和客户端程序并运行。\n\n这个版本是一个比较重要的版本，服务器最核心的几个模块都已经抽象出来，Reactor事件驱动大体成型（除了线程池），各个类的生命周期也大体上合适了，一个完整的单线程服务器设计模式已经编码完成了，读者应该完全理解今天的服务器代码后再继续后面的学习。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day08](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day08)\n"
        },
        {
          "name": "day09-缓冲区-大作用.md",
          "type": "blob",
          "size": 5.9423828125,
          "content": "# day09-缓冲区-大作用\n\n在之前的教程中，一个完整的单线程服务器设计模式已经编码完成了。在进入多线程编程之前，应该完全理解单线程服务器的工作原理，因为多线程更加复杂、更加困难，开发难度远大于之前的单线程模式。不仅如此，读者也应根据自己的理解进行二次开发，完善服务器，比如非阻塞式socket模块就值得细细研究。\n\n今天的教程和之前几天的不同，引入了一个最简单、最基本的的缓冲区，可以看作一个完善、改进服务器的例子，更加偏向于细节而不是架构。除了这一细节，读者也可以按照自己的理解完善服务器。\n\n同时，我们已经封装了socket、epoll等基础组件，这些组件都可以复用。现在我们完全可以使用这个网络库来改写客户端程序，让程序更加简单明了，读者可以自己尝试用这些组件写一个客户端，然后和源代码中的对照。\n\n在没有缓冲区的时候，服务器回送客户端消息的代码如下：\n```cpp\n#define READ_BUFFER 1024\nvoid Connection::echo(int sockfd){\n    char buf[READ_BUFFER];\n    while(true){    //由于使用非阻塞IO，读取客户端buffer，一次读取buf大小数据，直到全部读取完毕\n        bzero(&buf, sizeof(buf));\n        ssize_t bytes_read = read(sockfd, buf, sizeof(buf));\n        if(bytes_read > 0){\n            printf(\"message from client fd %d: %s\\n\", sockfd, buf);\n            write(sockfd, buf, sizeof(buf));   // 发送给客户端\n        } else if(bytes_read == -1 && errno == EINTR){  //客户端正常中断、继续读取\n            printf(\"continue reading\");\n            continue;\n        } else if(bytes_read == -1 && ((errno == EAGAIN) || (errno == EWOULDBLOCK))){//非阻塞IO，这个条件表示数据全部读取完毕\n            printf(\"finish reading once, errno: %d\\n\", errno);\n            break;\n        } else if(bytes_read == 0){  //EOF，客户端断开连接\n            printf(\"EOF, client fd %d disconnected\\n\", sockfd);\n            deleteConnectionCallback(sock);\n            break;\n        }\n    }\n}\n```\n这是非阻塞式socket IO的读取，可以看到使用的读缓冲区大小为1024，每次从TCP缓冲区读取1024大小的数据到读缓冲区，然后发送给客户端。这是最底层C语言的编码，在逻辑上有很多不合适的地方。比如我们不知道客户端信息的真正大小是多少，只能以1024的读缓冲区去读TCP缓冲区（就算TCP缓冲区的数据没有1024，也会把后面的用空值补满）；也不能一次性读取所有客户端数据，再统一发给客户端。\n> 关于TCP缓冲区、socket IO读取的细节，在《UNIX网络编程》卷一中有详细说明，想要精通网络编程几乎是必看的\n\n虽然以上提到的缺点以C语言编程的方式都可以解决，但我们仍然希望以一种更加优美的方式读写socket上的数据，和其他模块一样，脱离底层，让我们使用的时候不用在意太多底层细节。所以封装一个缓冲区是很有必要的，为每一个`Connection`类分配一个读缓冲区和写缓冲区，从客户端读取来的数据都存放在读缓冲区里，这样`Connection`类就不再直接使用`char buf[]`这种最笨的缓冲区来处理读写操作。\n\n缓冲区类的定义如下：\n```cpp\nclass Buffer {\nprivate:\n    std::string buf;\npublic:\n    void append(const char* _str, int _size);\n    ssize_t size();\n    const char* c_str();\n    void clear();\n    ......\n};\n```\n> 这个缓冲区类使用`std::string`来储存数据，也可以使用`std::vector<char>`，有兴趣可以比较一下这两者的性能。\n\n为每一个TCP连接分配一个读缓冲区后，就可以把客户端的信息读取到这个缓冲区内，缓冲区大小就是客户端发送的报文真实大小，代码如下：\n```cpp\nvoid Connection::echo(int sockfd){\n    char buf[1024];     //这个buf大小无所谓\n    while(true){    //由于使用非阻塞IO，读取客户端buffer，一次读取buf大小数据，直到全部读取完毕\n        bzero(&buf, sizeof(buf));\n        ssize_t bytes_read = read(sockfd, buf, sizeof(buf));\n        if(bytes_read > 0){\n            readBuffer->append(buf, bytes_read);\n        } else if(bytes_read == -1 && errno == EINTR){  //客户端正常中断、继续读取\n            printf(\"continue reading\");\n            continue;\n        } else if(bytes_read == -1 && ((errno == EAGAIN) || (errno == EWOULDBLOCK))){//非阻塞IO，这个条件表示数据全部读取完毕\n            printf(\"message from client fd %d: %s\\n\", sockfd, readBuffer->c_str());\n            errif(write(sockfd, readBuffer->c_str(), readBuffer->size()) == -1, \"socket write error\");\n            readBuffer->clear();\n            break;\n        } else if(bytes_read == 0){  //EOF，客户端断开连接\n            printf(\"EOF, client fd %d disconnected\\n\", sockfd);\n            deleteConnectionCallback(sock);\n            break;\n        }\n    }\n}\n```\n在这里依然有一个`char buf[]`缓冲区，用于系统调用`read()`的读取，这个缓冲区大小无所谓，但太大或太小都可能对性能有影响（太小读取次数增多，太大资源浪费、单次读取速度慢），设置为1到设备TCP缓冲区的大小都可以。以上代码会把socket IO上的可读数据全部读取到缓冲区，缓冲区大小就等于客户端发送的数据大小。全部读取完成之后，可以构造一个写缓冲区、填好数据发送给客户端。由于是echo服务器，所以这里使用了相同的缓冲区。\n\n至此，今天的教程已经结束，这个缓冲区只是为了满足当前的服务器功能而构造的一个最简单的`Buffer`类，还需要进一步完善，读者可以按照自己的方式构建缓冲区类，完善其他细节，为后续的多线程服务器做准备。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day09](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day09)\n"
        },
        {
          "name": "day10-加入线程池到服务器.md",
          "type": "blob",
          "size": 5.34375,
          "content": "# day10-加入线程池到服务器\n\n今天是本教程的第十天，在之前，我们已经编码完成了一个完整的单线程服务器，最核心的几个模块都已经抽象出来，Reactor事件驱动大体成型（除了线程池），各个类的生命周期也大体上合适了，读者应该完全理解之前的服务器代码后再开始今天的学习。\n\n观察当前的服务器架构，不难发现我们的Reactor模型少了最关键、最重要的一个模块：线程池。当发现socket fd有事件时，我们应该分发给一个工作线程，由这个工作线程处理fd上面的事件。而当前我们的代码是单线程模式，所有fd上的事件都由主线程（也就是EventLoop线程）处理，这是大错特错的，试想如果每一个事件相应需要1秒时间，那么当1000个事件同时到来，EventLoop线程将会至少花费1000秒来传输数据，还有函数调用等其他开销，服务器将直接宕机。\n\n在之前的教程已经讲过，每一个Reactor只应该负责事件分发而不应该负责事件处理。今天我们将构建一个最简单的线程池，用于事件处理。\n\n线程池有许多种实现方法，最容易想到的一种是每有一个新任务、就开一个新线程执行。这种方式最大的缺点是线程数不固定，试想如果在某一时刻有1000个并发请求，那么就需要开1000个线程，如果CPU只有8核或16核，物理上不能支持这么高的并发，那么线程切换会耗费大量的资源。为了避免服务器负载不稳定，这里采用了固定线程数的方法，即启动固定数量的工作线程，一般是CPU核数（物理支持的最大并发数），然后将任务添加到任务队列，工作线程不断主动取出任务队列的任务执行。\n\n关于线程池，需要特别注意的有两点，一是在多线程环境下任务队列的读写操作都应该考虑互斥锁，二是当任务队列为空时CPU不应该不断轮询耗费CPU资源。为了解决第一点，这里使用`std::mutex`来对任务队列进行加锁解锁。为了解决第二个问题，使用了条件变量`std::condition_variable`。\n> 关于`std::function`、`std::mutex`和`std::condition_variable`基本使用方法本教程不会涉及到，但读者应当先熟知，可以参考欧长坤《现代 C++ 教程》\n\n线程池定义如下：\n```cpp\nclass ThreadPool {\nprivate:\n    std::vector<std::thread> threads;\n    std::queue<std::function<void()>> tasks;\n    std::mutex tasks_mtx;\n    std::condition_variable cv;\n    bool stop;\npublic:\n    ThreadPool(int size = 10);  // 默认size最好设置为std::thread::hardware_concurrency()\n    ~ThreadPool();\n    void add(std::function<void()>);\n};\n```\n当线程池被构造时：\n```cpp\nThreadPool::ThreadPool(int size) : stop(false){\n    for(int i = 0; i < size; ++i){  //  启动size个线程\n        threads.emplace_back(std::thread([this](){  //定义每个线程的工作函数\n            while(true){    \n                std::function<void()> task;\n                {   //在这个{}作用域内对std::mutex加锁，出了作用域会自动解锁，不需要调用unlock()\n                    std::unique_lock<std::mutex> lock(tasks_mtx);\n                    cv.wait(lock, [this](){     //等待条件变量，条件为任务队列不为空或线程池停止\n                        return stop || !tasks.empty();\n                    });\n                    if(stop && tasks.empty()) return;   //任务队列为空并且线程池停止，退出线程\n                    task = tasks.front();   //从任务队列头取出一个任务\n                    tasks.pop();\n                }\n                task();     //执行任务\n            }\n        }));\n    }\n}\n```\n当我们需要添加任务时，只需要将任务添加到任务队列：\n```cpp\nvoid ThreadPool::add(std::function<void()> func){\n    { //在这个{}作用域内对std::mutex加锁，出了作用域会自动解锁，不需要调用unlock()\n        std::unique_lock<std::mutex> lock(tasks_mtx);\n        if(stop)\n            throw std::runtime_error(\"ThreadPool already stop, can't add task any more\");\n        tasks.emplace(func);\n    }\n    cv.notify_one();    //通知一次条件变量\n}\n```\n在线程池析构时，需要注意将已经添加的所有任务执行完，最好不采用外部的暴力kill、而是让每个线程从内部自动退出，具体实现参考源代码。\n\n这样一个最简单的线程池就写好了，在源代码中，当`Channel`类有事件需要处理时，将这个事件处理添加到线程池，主线程`EventLoop`就可以继续进行事件循环，而不在乎某个socket fd上的事件处理。\n\n至此，今天的教程已经结束，一个完整的Reactor模式才正式成型。这个线程池只是为了满足我们的需要构建出的最简单的线程池，存在很多问题。比如，由于任务队列的添加、取出都存在拷贝操作，线程池不会有太好的性能，只能用来学习，正确做法是使用右值移动、完美转发等阻止拷贝。另外线程池只能接受`std::function<void()>`类型的参数，所以函数参数需要事先使用`std::bind()`，并且无法得到返回值。针对这些缺点，将会在明天的教程进行修复。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day10](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day10)\n"
        },
        {
          "name": "day11-完善线程池，加入一个简单的测试程序.md",
          "type": "blob",
          "size": 4.412109375,
          "content": "# day11-完善线程池，加入一个简单的测试程序\n\n在昨天的教程里，我们添加了一个最简单的线程池到服务器，一个完整的Reactor模式正式成型。这个线程池只是为了满足我们的需要构建出的最简单的线程池，存在很多问题。比如，由于任务队列的添加、取出都存在拷贝操作，线程池不会有太好的性能，只能用来学习，正确做法是使用右值移动、完美转发等阻止拷贝。另外线程池只能接受`std::function<void()>`类型的参数，所以函数参数需要事先使用`std::bind()`，并且无法得到返回值。\n\n为了解决以上提到的问题，线程池的构造函数和析构函数都不会有太大变化，唯一需要改变的是将任务添加到任务队列的`add`函数。我们希望使用`add`函数前不需要手动绑定参数，而是直接传递，并且可以得到任务的返回值。新的实现代码如下：\n```cpp\ntemplate<class F, class... Args>\nauto ThreadPool::add(F&& f, Args&&... args) -> std::future<typename std::result_of<F(Args...)>::type> {\n    using return_type = typename std::result_of<F(Args...)>::type;  //返回值类型\n\n    auto task = std::make_shared< std::packaged_task<return_type()> >(  //使用智能指针\n            std::bind(std::forward<F>(f), std::forward<Args>(args)...)  //完美转发参数\n        );  \n        \n    std::future<return_type> res = task->get_future();  // 使用期约\n    {   //队列锁作用域\n        std::unique_lock<std::mutex> lock(tasks_mtx);   //加锁\n\n        if(stop)\n            throw std::runtime_error(\"enqueue on stopped ThreadPool\");\n\n        tasks.emplace([task](){ (*task)(); });  //将任务添加到任务队列\n    }\n    cv.notify_one();    //通知一次条件变量\n    return res;     //返回一个期约\n}\n```\n这里使用了大量C++11之后的新标准，具体使用方法可以参考欧长坤《现代 C++ 教程》。另外这里使用了模版，所以不能放在cpp文件，因为C++编译器不支持模版的分离编译\n> 这是一个复杂的问题，具体细节请参考《深入理解计算机系统》有关编译、链接的章节\n\n此外，我们希望对现在的服务器进行多线程、高并发的测试，所以需要使用网络库写一个简单的多线程高并发测试程序，具体实现请参考源代码，使用方式如下：\n\n```bash\n./test -t 10000 -m 10 (-w 100)\n# 10000个线程，每个线程回显10次，建立连接后等待100秒开始发送消息（可用于测试服务器能同时保持的最大连接数）。不指定w参数，则建立连接后开始马上发送消息。\n```\n注意Makefile文件也已重写，现在使用make只能编译服务器，客户端、测试程序的编译指令请参考Makefile文件，服务器程序编译后可以使用vscode调试。也可以使用gdb调试：\n```bash\ngdb server  #使用gdb调试\nr           #执行\nwhere / bt  #查看调用栈\n```\n今天还发现了之前版本的一个缺点：对于`Acceptor`，接受连接的处理时间较短、报文数据极小，并且一般不会有特别多的新连接在同一时间到达，所以`Acceptor`没有必要采用epoll ET模式，也没有必要用线程池。由于不会成为性能瓶颈，为了简单最好使用阻塞式socket，故今天的源代码中做了以下改变：\n1. Acceptor socket fd（服务器监听socket）使用阻塞式\n2. Acceptor使用LT模式，建立好连接后处理事件fd读写用ET模式\n3. Acceptor建立连接不使用线程池，建立好连接后处理事件用线程池\n\n至此，今天的教程已经结束了。使用测试程序来测试我们的服务器，可以发现并发轻松上万。这种设计架构最容易想到、也最容易实现，但有很多缺点，具体请参考陈硕《Linux多线程服务器编程》第三章，在明天的教程中将使用one loop per thread模式改写。\n\n此外，多线程系统编程是一件极其复杂的事情，比此教程中的设计复杂得多，由于这是入门教程，故不会涉及到太多细节，作者也还没有水平讲好这个问题。但要想成为一名合格的C++程序员，高并发编程是必备技能，还需要年复一年地阅读大量书籍、进行大量实践。\n> 路漫漫其修远兮，吾将上下而求索    ———屈原《离骚》\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day11](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day11)\n"
        },
        {
          "name": "day12-将服务器改写为主从Reactor多线程模式.md",
          "type": "blob",
          "size": 4.765625,
          "content": "# day12-将服务器改写为主从Reactor多线程模式\n\n在上一天的教程，我们实现了一种最容易想到的多线程Reactor模式，即将每一个Channel的任务分配给一个线程执行。这种模式有很多缺点，逻辑上也有不合理的地方。比如当前版本线程池对象被`EventLoop`所持有，这显然是不合理的，线程池显然应该由服务器类来管理，不应该和事件驱动产生任何关系。如果强行将线程池放进`Server`类中，由于`Channel`类只有`EventLoop`对象成员，使用线程池则需要注册回调函数，十分麻烦。\n> 更多比较可以参考陈硕《Linux多线程服务器编程》第三章\n\n今天我们将采用主从Reactor多线程模式，也是大多数高性能服务器采用的模式，即陈硕《Linux多线程服务器编程》书中的one loop per thread模式。\n\n此模式的特点为：\n1. 服务器一般只有一个main Reactor，有很多个sub Reactor。\n2. 服务器管理一个线程池，每一个sub Reactor由一个线程来负责`Connection`上的事件循环，事件执行也在这个线程中完成。\n3. main Reactor只负责`Acceptor`建立新连接，然后将这个连接分配给一个sub Reactor。\n\n此时，服务器有如下成员：\n```cpp\nclass Server {\nprivate:\n    EventLoop *mainReactor;     //只负责接受连接，然后分发给一个subReactor\n    Acceptor *acceptor;                     //连接接受器\n    std::map<int, Connection*> connections; //TCP连接\n    std::vector<EventLoop*> subReactors;    //负责处理事件循环\n    ThreadPool *thpool;     //线程池\n};\n```\n在构造服务器时：\n```cpp\nServer::Server(EventLoop *_loop) : mainReactor(_loop), acceptor(nullptr){ \n    acceptor = new Acceptor(mainReactor);   //Acceptor由且只由mainReactor负责\n    std::function<void(Socket*)> cb = std::bind(&Server::newConnection, this, std::placeholders::_1);\n    acceptor->setNewConnectionCallback(cb);\n\n    int size = std::thread::hardware_concurrency();     //线程数量，也是subReactor数量\n    thpool = new ThreadPool(size);      //新建线程池\n    for(int i = 0; i < size; ++i){\n        subReactors.push_back(new EventLoop());     //每一个线程是一个EventLoop\n    }\n\n    for(int i = 0; i < size; ++i){\n        std::function<void()> sub_loop = std::bind(&EventLoop::loop, subReactors[i]);\n        thpool->add(sub_loop);      //开启所有线程的事件循环\n    }\n}\n```\n在新连接到来时，我们需要将这个连接的socket描述符添加到一个subReactor中：\n```cpp\nint random = sock->getFd() % subReactors.size();    //调度策略：全随机\nConnection *conn = new Connection(subReactors[random], sock);   //分配给一个subReactor\n```\n这里有一个很值得研究的问题：当新连接到来时应该分发给哪个subReactor，这会直接影响服务器效率和性能。这里采用了最简单的hash算法实现全随机调度，即将新连接随机分配给一个subReactor。由于socket fd是一个`int`类型的整数，只需要用fd余subReactor数，即可以实现全随机调度。\n\n这种调度算法适用于每个socket上的任务处理时间基本相同，可以让每个线程均匀负载。但事实上，不同的业务传输的数据极有可能不一样，也可能受到网络条件等因素的影响，极有可能会造成一些subReactor线程十分繁忙，而另一些subReactor线程空空如也。此时需要使用更高级的调度算法，如根据繁忙度分配，或支持动态转移连接到另一个空闲subReactor等，读者可以尝试自己设计一种比较好的调度算法。\n\n至此，今天的教程就结束了。在今天，一个简易服务器的所有核心模块已经开发完成，采用主从Reactor多线程模式。在这个模式中，服务器以事件驱动作为核心，服务器线程只负责mainReactor的新建连接任务，同时维护一个线程池，每一个线程也是一个事件循环，新连接建立后分发给一个subReactor开始事件监听，有事件发生则在当前线程处理。这种模式几乎是目前最先进、最好的服务器设计模式，本教程之后也会一直采用此模式。\n\n虽然架构上已经完全开发完毕了，但现在我们还不算拥有一个完整的网络库，因为网络库的业务是写死的`echo`服务，十分单一，如果要提供其他服务，如HTTP服务、FTP服务等，需要重新开发、重新写代码，这打破了通用性原则。我们希望将服务器业务处理也进一步抽象，实现用户特例化，即在`main`函数新建`Server`的时候，可以自己设计、绑定相应的业务，在之后的教程将会实现这一功能。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day12](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day12)\n"
        },
        {
          "name": "day13-C++工程化、代码分析、性能优化.md",
          "type": "blob",
          "size": 7.3095703125,
          "content": "# day13-C++工程化、代码分析、性能优化\n\n在之前的教程里，我们已经完整开发了一个主从Reactor多线程的服务器的核心架构，接下来的开发重心应该从架构转移到细节。在这之前，将整个项目现代化、工程化是必要的，也是必须的。\n\nC++项目工程化的第一步，一定是使用CMake。目前将所有文件都放在一个文件夹，并且没有分类。随着项目越来越复杂、模块越来越多，开发者需要考虑这座屎山的可读性，如将模块拆分到不同文件夹，将头文件统一放在一起等。对于这样复杂的项目，如果手写复杂的Makefile来编译链接，那么将会相当负责繁琐。我们应当使用CMake来管理我们的项目，CMake的使用非常简单、功能强大，会帮我们自动生成Makefile文件，使项目的编译链接更加容易，程序员可以将更多的精力放在写代码上。\n> C++的编译、链接看似简单，实际上相当繁琐复杂，具体原理请参考《深入理解计算机系统（第三版）》第七章。如果没有CMake，开发一个大型C++项目，一半的时间会用在编译链接上。\n\n我们将核心库放在`src`目录下，使用网络库的测试程序放在`test`目录下，所有的头文件放在`/include`目录下：\n```\nset(PINE_SRC_INCLUDE_DIR ${PROJECT_SOURCE_DIR}/src/include)\nset(PINE_TEST_INCLUDE_DIR ${PROJECT_SOURCE_DIR}/test/include)\ninclude_directories(${PINE_SRC_INCLUDE_DIR} ${PINE_TEST_INCLUDE_DIR})\n```\n实现头文件的`.cpp`文件则按照模块放在`src`目录（这个版本还未拆分模块到不同文件夹）。\n\n`src`目录是网络库，并没有可执行的程序，我们只需要将这个网络库的`.cpp`文件编译链接成多个目标文件，然后链接到一个共享库中：\n```\nfile(GLOB_RECURSE pine_sources ${PROJECT_SOURCE_DIR}/src/*.cpp)\nadd_library(pine_shared SHARED ${pine_sources})\n```\n在编译时，根据不同环境设置编译参数也很方便：\n```\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -Wall -Wextra -std=c++17 -pthread\")\nset(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-unused-parameter -Wno-attributes\") #TODO: remove\nset(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -O0 -ggdb -fsanitize=address -fno-omit-frame-pointer -fno-optimize-sibling-calls\")\nset(CMAKE_EXE_LINKER_FLAGS  \"${CMAKE_EXE_LINKER_FLAGS} -fPIC\")\n```\n使用`test`目录下的`.cpp`文件创建可执行文件的代码：\n```\nforeach (pine_test_source ${PINE_TEST_SOURCES})\n    get_filename_component(pine_test_filename ${pine_test_source} NAME)\n    string(REPLACE \".cpp\" \"\" pine_test_name ${pine_test_filename})\n\n    add_executable(${pine_test_name} EXCLUDE_FROM_ALL ${pine_test_source})\n    add_dependencies(build-tests ${pine_test_name})\n    add_dependencies(check-tests ${pine_test_name})\n\n    target_link_libraries(${pine_test_name} pine_shared)\n\n    set_target_properties(${pine_test_name}\n        PROPERTIES\n        RUNTIME_OUTPUT_DIRECTORY \"${CMAKE_BINARY_DIR}/bin\"\n        COMMAND ${pine_test_name}\n    )\nendforeach(pine_test_source ${PINE_TEST_SOURCES})\n```\n注意我们切换到了更强大更好用的clang编译器（之前是GCC）。\n\n配置好CMake和clang后，还需要做以下三件事：\n1. format：作为一个大型C++项目，可能有许多程序员共同开发，每个人的编码习惯风格都不同，整个项目可能风格杂乱，可读性差，不利于项目维护。所以在写C++代码时应该遵守一些约定，使代码的风格统一。目前比较流行的C++代码风格有google、llvm等，本项目采用google风格。\n2. cpplint：基于google C++编码规范的静态代码分析工具，可以查找代码中错误、违反约定、建议修改的地方。\n3. clang-tidy：clang编译器的代码分析工具，功能十分强大。既可以查找代码中的各种静态错误，还可以提示可能会在运行时发生的问题。不仅如此，还可以通过代码分析给出可以提升程序性能的建议。\n\n这三件事可以保证我们写出风格一致、bug较少、性能较好、遵守google编码规范的项目，是开发大型C++项目必备的利器。\n\n为了很方便地自动一键运行，这三个工具都已经以`python`脚本的格式保存在了`build_support`目录：\n```\nbuild_support\n    - clang_format_exclusions.txt     // 不需要格式化的代码\n    - run_clang_format.py             // format\n    - cpplint.py                      // cpplint\n    - run_clang_tidy_extra.py         // 帮助文件，不直接运行\n    - run_clang_tidy.py               // clang-tidy\n.clang-format                         // format配置\n.clang-tidy                           // clang-tidy配置\n```\n\nformat在CMakeLists.txt中的配置：\n```\n# runs clang format and updates files in place.\nadd_custom_target(format ${PINE_BUILD_SUPPORT_DIR}/run_clang_format.py\n        ${CLANG_FORMAT_BIN}\n        ${PINE_BUILD_SUPPORT_DIR}/clang_format_exclusions.txt\n        --source_dirs\n        ${PINE_FORMAT_DIRS}\n        --fix\n        --quiet\n        )\n```\ncpplint在CMakeLists.txt中的配置：\n```\nadd_custom_target(cpplint echo '${PINE_LINT_FILES}' | xargs -n12 -P8\n        ${CPPLINT_BIN}\n        --verbose=2 --quiet\n        --linelength=120\n        --filter=-legal/copyright,-build/include_subdir,-readability/casting\n        )\n```\nclang-tidy在CMakeLists.txt中的配置：    \n```\nadd_custom_target(clang-tidy\n        ${PINE_BUILD_SUPPORT_DIR}/run_clang_tidy.py # run LLVM's clang-tidy script\n        -clang-tidy-binary ${CLANG_TIDY_BIN}        # using our clang-tidy binary\n        -p ${CMAKE_BINARY_DIR}                      # using cmake's generated compile commands\n        )\n```\n这里省略了文件夹定义等很多信息，完整配置在源代码中。\n\n接下来尝试编译我们的项目，首先创建一个`build`文件夹，防止文件和项目混在一起：\n```\nmkdir build\ncd build\n```\n然后使用CMake生成Makefile：\n```\ncmake ..\n```\n生成Makefile后，使用以下命令进行代码格式化:\n```\nmake format\n```\n然后用cpplint检查代码:\n```\nmake cpplint\n```\n最后使用clang-tidy进行代码分析：\n```\nmake clang-tidy\n```\n将所有的警告都修改好，重新运行这三个命令直到全部通过。然后使用`make`指令即可编译整个网络库，会被保存到`lib`文件夹中，但这里没有可执行文件。如果我们需要编译可执行服务器，需要编译`test`目录下相应的源文件:\n```\nmake server\nmake multiple_client\nmake single_client\n```\n生成的可执行文件在`build/test`目录下，这时使用`./test/server`即可运行服务器。\n\n至此，今天的教程已经结束了。今天我们将整个项目工程化，使用了CMake、format、cpplint、clang-tidy，代码的风格变成了google-style，修复了之前版本的许多bug，应用了这些工具给我们提供的现代C++项目建议，性能也提高了。在今天的版本，所有的类也都被声明为不可拷贝、不可移动。clang-tidy提示的按值传参也被修改为引用传参，减少了大量的复制操作。这些工具建议的修改都大大降低了bug发生的几率、提高了服务器性能，虽然还没有用任何的性能测试工具，服务器的处理速度、吞吐量、并发支持度都明显提高了。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day13](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day13)"
        },
        {
          "name": "day14-支持业务逻辑自定义、完善Connection类.md",
          "type": "blob",
          "size": 6.2490234375,
          "content": "# day14-支持业务逻辑自定义、完善Connection类\n\n回顾之前的教程，可以看到服务器Echo业务的逻辑在`Connection`类中。如果我们需要不同的业务逻辑，如搭建一个HTTP服务器，或是一个FTP服务器，则需要改动`Connection`中的代码，这显然是不合理的。`Connection`类作为网络库的一部分，不应该和业务逻辑产生联系，业务逻辑应该由网络库用户自定义，写在`server.cpp`中。同时，作为一个通用网络库，客户端也可以使用网络库来编写相应的业务逻辑。今天我们需要完善`Connection`类，支持业务逻辑自定义。\n\n首先来看看我们希望如何自定义业务逻辑，这是一个echo服务器的完整代码：\n\n```cpp\nint main() {\n  EventLoop *loop = new EventLoop();\n  Server *server = new Server(loop);\n  server->OnConnect([](Connection *conn) {  // 业务逻辑\n    conn->Read();\n    std::cout << \"Message from client \" << conn->GetSocket()->GetFd() << \": \" << conn->ReadBuffer() << std::endl;\n    if (conn->GetState() == Connection::State::Closed) {\n      conn->Close();\n      return;\n    }\n    conn->SetSendBuffer(conn->ReadBuffer());\n    conn->Write();\n  });\n  loop->Loop(); // 开始事件循环\n  delete server;\n  delete loop;\n  return 0;\n}\n```\n\n这里新建了一个服务器和事件循环，然后以回调函数的方式编写业务逻辑。通过`Server`类的`OnConnection`设置lambda回调函数，回调函数的参数是一个`Connection`指针，代表服务器到客户端的连接，在函数体中可以书写业务逻辑。这个函数最终会绑定到`Connection`类的`on_connect_callback_`，也就是`Channel`类处理的事件（这个版本只考虑了可读事件）。这样每次有事件发生，事件处理实际上都在执行用户在这里写的代码逻辑。\n\n关于`Connection`类的使用，提供了两个函数，分别是`Write()`和`Read()`。`Write()`函数表示将`write_buffer_`里的内容发送到该`Connection`的socket，发送后会清空写缓冲区；而`Read()`函数表示清空`read_buffer_`，然后将TCP缓冲区内的数据读取到读缓冲区。\n\n在业务逻辑中，`conn->Read()`表示从客户端读取数据到读缓冲区。在发送回客户端之前，客户端有可能会关闭连接，所以需要先判断`Connection`的状态是否为`Closed`。然后将写缓冲区设置为和读缓冲区一样的内容`conn->SetSendBuffer(conn->ReadBuffer())`，最后调用`conn->Write()`将写缓冲区的数据发送给客户端。\n\n可以看到，现在`Connection`类只有从socket读写数据的逻辑，与具体业务没有任何关系，业务完全由用户自定义。\n\n在客户端我们也希望使用网络库来写业务逻辑，首先来看看客户端的代码：\n\n```cpp\nint main() {\n  Socket *sock = new Socket();\n  sock->Connect(\"127.0.0.1\", 1234);\n  Connection *conn = new Connection(nullptr, sock);\n  while (true) {\n    conn->GetlineSendBuffer();\n    conn->Write();\n    if (conn->GetState() == Connection::State::Closed) {\n      conn->Close();\n      break;\n    }\n    conn->Read();\n    std::cout << \"Message from server: \" << conn->ReadBuffer() << std::endl;\n  }\n  delete conn;\n  return 0;\n}\n```\n\n注意这里和服务器有很大的不同，之前设计的`Connection`类显然不能满足要求，所以需要完善`Connection`。\n\n首先，这里没有服务器和事件循环，仅仅使用了一个裸的`Connection`类来表示从客户端到服务器的连接。所以此时`Read()`表示从服务器读取到客户端，而`Write()`表示从客户端写入到服务器，和之前服务器的`Conneciont`类方向完全相反。这样`Connection`就可以同时表示Server->Client或者Client->Server的连接，不需要新建一个类来区分，大大提高了通用性和代码复用。\n\n其次，客户端`Connection`没有绑定事件循环，所以将第一个参数设置为`nullptr`表示不使用事件循环，这时将不会有`Channel`类创建来分配到`EventLoop`，表示使用一个裸的`Connection`。因此业务逻辑也不用设置服务器回调函数，而是直接写在客户端代码中。\n\n另外，虽然服务器到客户端（Server->Client）的连接都使用非阻塞式socket IO（为了搭配epoll ET模式），但客户端到服务器（Client->Server）的连接却不一定，很多业务都需要使用阻塞式socket IO，比如我们当前的echo客户端。之前`Connection`类的读写逻辑都是非阻塞式socket IO，在这个版本支持了非阻塞式读写，代码如下：\n\n```cpp\nvoid Connection::Read() {\n  ASSERT(state_ == State::Connected, \"connection state is disconnected!\");\n  read_buffer_->Clear();\n  if (sock_->IsNonBlocking()) {\n    ReadNonBlocking();\n  } else {\n    ReadBlocking();\n  }\n}\nvoid Connection::Write() {\n  ASSERT(state_ == State::Connected, \"connection state is disconnected!\");\n  if (sock_->IsNonBlocking()) {\n    WriteNonBlocking();\n  } else {\n    WriteBlocking();\n  }\n  send_buffer_->Clear();\n}\n```\n\nps.如果连接是从服务器到客户端，所有的读写都应采用非阻塞式IO，阻塞式读写是提供给客户端使用的。\n\n至此，今天的教程已经结束了。教程里只会包含极小一部分内容，大量的工作都在代码里，请务必结合源代码阅读。在今天的教程中，我们完善了`Connection`类，将`Connection`类与业务逻辑完全分离，业务逻辑完全由用户自定义。至此，我们的网络库核心代码已经完全脱离了业务，成为一个真正意义上的网络库。今天我们也将`Connection`通用化，同时支持Server->Client和Client->Server，使其可以在客户端脱离`EventLoop`单独绑定socket使用，读写操作也都支持了阻塞式和非阻塞式两种模式。\n\n到今天，本教程已经进行了一半，我们开发了一个真正意义上的网络库，使用这个网络库，只需要不到20行代码，就可以搭建一个echo服务器、客户端（完整程序在`test`目录）。但这只是一个最简单的玩具型网络库，需要做的工作还很多，在今后的教程里，我们会对这个网络库不断完善、不断提升性能，使其可以在生产环境中使用。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day14](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day14)\n"
        },
        {
          "name": "day15-macOS支持、完善业务逻辑自定义.md",
          "type": "blob",
          "size": 5.5234375,
          "content": "# day15-macOS支持、完善业务逻辑自定义\n\n作为程序员，使用MacBook电脑作为开发机很常见，本质和Linux几乎没有区别。本教程的EventLoop中使用Linux系统支持的epoll，然而macOS里并没有epoll，取而代之的是FreeBSD的kqueue，功能和使用都和epoll很相似。Windows系统使用WSL可以完美编译运行源代码，但MacBook则需要Docker、云服务器、或是虚拟机，很麻烦。在今天，我们将支持使用kqueue作为`EventLoop`类的Poller，使网络库可以在macOS等FreeBSD系统上原生运行。\n\n在网络库已有的类当中，`Socket`和`Epoll`类是最底层的、需要和操作系统打交道，而上一层的`EventLoop`类只是使用`Epoll`提供的接口，而不关心`Epoll`类的底层实现。所以在考虑支持不同的操作系统时，只应该改变最底层的`Epoll`类，而不需要改动上层的`EventLoop`类。至于分发`fd`的`Channel`类，可以自定义epoll和kqueue的读、写、ET模式等事件，在`Channel`类中只需要注册好我们自定义的事件，然后在`Poller`类中将事件注册到epoll或kqueue。\n```cpp\nconst int Channel::READ_EVENT = 1;\nconst int Channel::WRITE_EVENT = 2;\nconst int Channel::ET = 4;\n```\n需要注意`Channel`的用户自定义事件必须是1、2、4、8、16等十进制数，因为在`Poller`中判断、更新事件时需要用到按位与、按位或等操作，这里实际上是将16位二进制数的每一位用作标志位。如果这里理解有困难，可以先学一遍《深入理解计算机系统（第三版）》.\n\n在`Poller`类中使用宏定义的形式判断当前操作系统，从而使用不同的代码:\n```cpp\n#ifdef OS_LINUX\n// linux平台的代码\n#endif\n\n#ifdef OS_MACOS\n// FreeBSD平台的代码\n#endif\n```\n操作系统宏在CMakeLists.txt中定义：\n```\nif (CMAKE_SYSTEM_NAME MATCHES \"Darwin\")\n    message(STATUS \"Platform: macOS\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DOS_MACOS\")\nelseif (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n    message(STATUS \"Platform: Linux\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DOS_LINUX\")\nendif()\n```\n这样就可以在不同的操作系统使用不同的代码。如注册/更新`Channel`，在Linux系统下会编译以下代码：\n```cpp\nvoid Poller::UpdateChannel(Channel *ch) {\n  int sockfd = ch->GetSocket()->GetFd();\n  struct epoll_event ev {};\n  ev.data.ptr = ch;\n  if (ch->GetListenEvents() & Channel::READ_EVENT) {\n    ev.events |= EPOLLIN | EPOLLPRI;\n  }\n  if (ch->GetListenEvents() & Channel::WRITE_EVENT) {\n    ev.events |= EPOLLOUT;\n  }\n  if (ch->GetListenEvents() & Channel::ET) {\n    ev.events |= EPOLLET;\n  }\n  if (!ch->GetExist()) {\n    ErrorIf(epoll_ctl(fd_, EPOLL_CTL_ADD, sockfd, &ev) == -1, \"epoll add error\");\n    ch->SetExist();\n  } else {\n    ErrorIf(epoll_ctl(fd_, EPOLL_CTL_MOD, sockfd, &ev) == -1, \"epoll modify error\");\n  }\n}\n```\n而在macOS系统下会编译以下代码：\n```cpp\nvoid Poller::UpdateChannel(Channel *ch) {\n  struct kevent ev[2];\n  memset(ev, 0, sizeof(*ev) * 2);\n  int n = 0;\n  int fd = ch->GetSocket()->GetFd();\n  int op = EV_ADD;\n  if (ch->GetListenEvents() & ch->ET) {\n    op |= EV_CLEAR;\n  }\n  if (ch->GetListenEvents() & ch->READ_EVENT) {\n    EV_SET(&ev[n++], fd, EVFILT_READ, op, 0, 0, ch);\n  }\n  if (ch->GetListenEvents() & ch->WRITE_EVENT) {\n    EV_SET(&ev[n++], fd, EVFILT_WRITE, op, 0, 0, ch);\n  }\n  int r = kevent(fd_, ev, n, NULL, 0, NULL);\n  ErrorIf(r == -1, \"kqueue add event error\");\n}\n```\n\n在之前的教程中，我们使`Connection`类以`OnConnect`回调函数的方式初步支持了业务逻辑自定义，自定义的业务逻辑是从服务器端可读事件触发后开始进入，所以需要自己处理读取数据的逻辑。这显然不合理，怎样事件触发、读取数据、异常处理等流程应该是网络库提供的基本功能，用户只应当关注怎样处理业务即可，所以业务逻辑的进入点应该是服务器读取完客户端的所有数据之后。这是，客户端传来的请求在`Connection`类的读缓冲区里，我们只需要根据请求来分发、处理业务即可。\n\n通过设置`OnMessage`回调函数来自定义自己的业务逻辑，在服务器完全接收到客户端的数据之后，该函数触发。以下是一个echo服务器的业务逻辑：\n\n```cpp\nserver->OnMessage([](Connection *conn){\n  std::cout << \"Message from client \" << conn->ReadBuffer() << std::endl;\n  if(conn->GetState() == Connection::State::Connected){\n    conn->Send(conn->ReadBuffer());\n  }\n});\n```\n\n在进入该函数前，服务器已经完成了接受客户端数据并保存在读缓冲区里，业务逻辑只需要将读缓冲区里的数据发送回即可，这样的设计更加符合服务器的功能准则与设计准则。\n\n在今天的教程中，我们支持了MacOS、FreeBSD平台。在代码中去掉了`Epoll`类，改为通用的`Poller`，在不同的平台会自适应地编译不同的代码。同时我们将`Channel`类和操作系统脱离开来，通过用户自定义事件来表示监听、发生的事件。现在在Linux和macOS系统中，网络库都可以原生编译运行。但具体功能可能会根据操作系统的不同有细微差异，如在macOS平台下的并发支持度明显没有Linux平台高，在后面的开发中会不断完善。我们也完善了业务逻辑自定义，进一步简化了服务器编程、隐藏了更多细节，使用者只需要完全关注自己核心的业务逻辑。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day15](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day15)"
        },
        {
          "name": "day16-重构核心库、使用智能指针.md",
          "type": "blob",
          "size": 7.0087890625,
          "content": "# day16-重构服务器、使用智能指针\n\n至此，本教程进度已经过半，在前15天的学习和开发中，相信大家对服务器的开发原则、核心模块的组织有了一个初步的了解，也有能力写出一个“乞丐”版本的服务器。但重温之前的代码，相信大家已经遇到过无数bug。包括内存相关的，如内存泄漏、野指针、悬垂引用等，还有网络编程相关的，如无效socket、连接意外终止、TCP缓冲区满等，还有事件相关的，如epoll、kqueue返回其他错误情况，等等。这是由于之前是从零开始构建整个服务器，从C语言风格逐渐到C++风格，从单线程到多线程，从阻塞式IO到非阻塞式IO，从任务驱动到事件驱动。所以从一开始就并没有考虑到良好的编码风格、编程习惯和设计模式，自然就会带来许多问题和程序bug。如果继续这样开发下去，但项目越来越大、结构越来越复杂、模块越来越多，程序细节上与设计上的缺陷迟早会逐渐暴露。\n\n一个优秀的程序员、尤其是面向系统编程的程序员，要时刻铭记以下准则：\n\n> 程序中所有可能的异常与错误终将发生。\n\n所以对程序进行重构是很有必要的，重构可以弥补程序之前的设计、细节缺陷，应用最新的、最先进的编程技术和经验。对服务器开发的学习者来说，重构也可以让我们对整个程序架构有更抽象、更深入的了解。此外，程序的重构越早越好，因为早期重构需要改动的代码量不大。如果程序已经逐渐成为一个大型屎山、甚至已经上线交付，此刻再进行重构将会十分困难、甚至得不偿失。\n\n目前，我们已经掌握了高并发服务器的最小核心架构，所以可以根据这个架构重新设计服务器。在之前的入门学习阶段，我们由面向过程编程、逐渐抽象出类、最终形成整个架构，在重构阶段我们完全可以面向对象、面向系统编程，以一个更抽象、更高层、更大局观的视角来设计整个核心库。\n\n重构的一个重点，就是内存管理。在之前的代码中，所有的内存都用裸指针来管理，在类的构造阶段分配内存、析构阶段释放内存。这种编码便于理解，可以让新手清晰地掌握各资源的生命周期，但绝不适用于大型项目，因为极易产生内存泄漏、悬垂引用、野指针等问题。在muduo库的早期设计，陈硕使用了RAII来管理内存资源，具体细节可以参考《Linux多线程服务器编程》，这个优秀的内存资源管理设计被应用于许多项目和语言（如rust）。从C++11标准后，我们也可以使用智能指针来管理内存，让程序员无需过多考虑内存资源的使用。标准中三种智能指针的使用和区别在这里不再赘述，可以参考《C++ Primer》第12章。\n\n重构的另一个重点，就是避免资源的复制操作，尽量使用移动语义来进行所有权的转移，这对提升程序的性能有十分显著的帮助，这也是为何rust语言性能如此高的原因。\n\n> 由于重构后的代码涉及到大量所有权转移、移动语义、智能指针等，如果读者现在还对栈内存与堆内存的使用十分模糊，请立刻停止阅读并打好基础，继续学习将会事倍功半！\n\n所以在重构后的代码中，类自己所拥有的资源用`std::unique_ptr<>`来管理，这样在类被销毁的时候，将会自动释放堆内存里的相关资源。而对不属于自己、但会使用的资源，采用`std::unique_ptr<> &`或`std::shared_ptr<>`来管理会十分麻烦、不易与阅读并且可能对新手带来一系列问题，所以我们参考Chromium的方式，依旧采用裸指针来管理。通过这样的设计，不管程序发生什么异常，资源在离开作用域的时候都会释放其使用的堆内存空间，避免了内存泄漏等诸多内存问题。\n\n重构的第三个重点就是错误、异常的处理。在目前的程序中，由于是开发阶段，我们尽可能暴露所有的异常情况，并使用assert、exit等方式使程序在发生错误时直接崩溃，但这样会使程序不够健壮。程序中有些错误是不可恢复的，遇见此类错误可以直接退出。但对于大型项目、尤其是线上远行的网络服务器、数据库等不断提供服务的程序来说，可靠性是十分重要的一个因素，所以绝大部分错误都是可恢复的。如创建socket失败可能是文件描述符超过操作系统限制，稍后再次尝试即可。监听socket失败可能是端口被占用，切换端口或提示并等待用户处理即可。打开文件失败可能是文件不存在或没有权限，此时只需创建文件或赋予权限即可。所以在底层的编码上，对于部分错误需要进行可恢复处理，避免一个模块或资源发生的小错误影响整个服务器的运行。\n\n以下是重构后`TcpServer`类的定义：\n```cpp\nclass TcpServer {\n public:\n\n    ......\n\n private:\n  std::unique_ptr<EventLoop> main_reactor_;\n  std::unique_ptr<Acceptor> acceptor_;\n\n  std::unordered_map<int, std::unique_ptr<Connection>> connections_;\n  std::vector<std::unique_ptr<EventLoop>> sub_reactors_;\n\n  std::unique_ptr<ThreadPool> thread_pool_;\n\n  std::function<void(Connection *)> on_connect_;\n  std::function<void(Connection *)> on_recv_;\n};\n\n```\n\n可以看到，`main_reactor_`、`acceptor_`、`connections_`、`sub_reactors_`和`thread_pool_`都是该服务器拥有的资源，在服务器实例被销毁时，这些资源也需要被销毁，所以使用智能指针`std::unique_ptr<>`来管理，一旦该`TcpServer`实例被销毁，不需要手动释放这些资源、程序会自动帮我们释放，避免了内存泄漏。\n\n而对于`Channel`类：\n\n```cpp\nclass Channel {\n public:\n\n    ......\n\n private:\n  int fd_;\n  EventLoop *loop_;\n  short listen_events_;\n  short ready_events_;\n  bool exist_;\n  std::function<void()> read_callback_;\n  std::function<void()> write_callback_;\n};\n\n```\n\n可以看到，该类中有一个成员`loop_`，表示该`Channel`实例所在的事件循环`EventLoop`。而Channel类并不拥有该事件循环资源，仅仅是为了访问而存在的一个指针，所以在该`Channel`被销毁时，也绝不可以释放`loop_`，所以这里使用裸指针来表示仅需访问但不拥有的资源。\n\n至此，今天的教程就结束了，我们对前15天的代码进行了重构，使用智能指针`std::unique_ptr<>`来管理独占资源，避免了内存泄漏、内存资源的浪费，也使各组件的生命周期更加明确。我们还尽可能使用了移动语义进行所有权的转移（如针对`std::function<>`），减少了资源复制带来的开销。同时对一部分代码进行了精简、重写，使其更加符合C++编码规范。同时核心库的api以及命名也发生了改变，更加清晰、易用。\n\n完整源代码：[https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day16](https://github.com/yuesong-feng/30dayMakeCppServer/tree/main/code/day16)\n"
        }
      ]
    }
  ]
}