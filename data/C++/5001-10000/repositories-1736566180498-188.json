{
  "metadata": {
    "timestamp": 1736566180498,
    "page": 188,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE5MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "NVIDIA/FasterTransformer",
      "stars": 5976,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 1.8369140625,
          "content": "Language: Cpp\nAccessModifierOffset: -4\nAlignAfterOpenBracket: Align\nAllowShortEnumsOnASingleLine: false\nAlignConsecutiveAssignments: true\nAlignConsecutiveDeclarations: true\nAlignEscapedNewlines: Right\nAlignOperands: true\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowAllArgumentsOnNextLine: true\nAllowShortBlocksOnASingleLine: Empty\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: Empty\nAllowShortIfStatementsOnASingleLine: Never\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: false\nAlwaysBreakTemplateDeclarations: true\nBinPackArguments: false\nBinPackParameters: false\nBreakBeforeBinaryOperators: NonAssignment\nBreakBeforeBraces: Stroustrup\nBreakBeforeTernaryOperators: false\nBreakConstructorInitializers: AfterColon\nBreakInheritanceList: AfterColon\nBreakStringLiterals: false\nColumnLimit: 120\nCompactNamespaces: false\nConstructorInitializerAllOnOneLineOrOnePerLine: true\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nFixNamespaceComments: true\nIndentCaseLabels: true\nIndentPPDirectives: None\nIndentWidth: 4\nIndentWrappedFunctionNames: false\nKeepEmptyLinesAtTheStartOfBlocks: true\nMaxEmptyLinesToKeep: 1\nNamespaceIndentation: None\nPointerAlignment: Left\nReflowComments: true\nSortIncludes: true\nSortUsingDeclarations: false\nSpaceAfterCStyleCast: false\nSpaceAfterTemplateKeyword: false\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeCtorInitializerColon: false\nSpaceBeforeInheritanceColon: false\nSpaceBeforeParens: ControlStatements\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 2\nSpacesInAngles: false\nSpacesInCStyleCastParentheses: false\nSpacesInContainerLiterals: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard: Cpp11\nTabWidth: 4\nUseTab: Never\n"
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.134765625,
          "content": "docker\n.dockerignore\n.gitlab\n.gitlab-ci.yml\n\n*build*\n./models\n__pycache__\n.vscode\ntranslation\n.cache\n*.npy\n*.pth\n*.o\n**/.ipynb_checkpoints"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.2080078125,
          "content": "[flake8]\nignore = W292\nexclude =\n    *migrations*,\n    # python related\n    *.pyc,\n    .git,\n    __pycache__,\n\nmax-line-length=120\nmax-complexity=12\nformat=pylint\nshow_source = True\nstatistics = True\ncount = True\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1767578125,
          "content": "*~\n*.o\n*build*/\n./models/\n__pycache__/\n.vscode\n.idea\n./translation\n.cache\n*.npy\n*.pth\n!tests/data/**/*.npy\n/models\n/notebooks\n**/.ipynb_checkpoints/\n\n/3rdparty/NeMo/\n/3rdparty/apex/"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.7578125,
          "content": "[submodule \"3rdparty/Megatron-LM\"]\n\tpath = 3rdparty/Megatron-LM\n\turl = https://github.com/NVIDIA/Megatron-LM.git\n\tbranch = v2.6\n[submodule \"examples/tensorflow/bert/tensorflow_bert/bert\"]\n\tpath = examples/tensorflow/bert/tensorflow_bert/bert\n\turl = https://github.com/google-research/bert.git\n[submodule \"examples/pytorch/swin/Swin-Transformer-Quantization/SwinTransformer\"]\n\tpath = examples/pytorch/swin/Swin-Transformer-Quantization/SwinTransformer\n\turl = https://github.com/microsoft/Swin-Transformer\n[submodule \"examples/pytorch/vit/ViT-quantization/ViT-pytorch\"]\n\tpath = examples/pytorch/vit/ViT-quantization/ViT-pytorch\n\turl = https://github.com/jeonsworld/ViT-pytorch\n[submodule \"3rdparty/cutlass\"]\n\tpath = 3rdparty/cutlass\n\turl = https://github.com/NVIDIA/cutlass.git\n"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "3rdparty",
          "type": "tree",
          "content": null
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 18.2998046875,
          "content": "# Copyright (c) 2019-2023, NVIDIA CORPORATION.  All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\ncmake_minimum_required(VERSION 3.8 FATAL_ERROR) # for PyTorch extensions, version should be greater than 3.13\nproject(FasterTransformer LANGUAGES CXX CUDA)\n\nfind_package(CUDA 10.2 REQUIRED)\n\nif(${CUDA_VERSION_MAJOR} VERSION_GREATER_EQUAL \"11\")\n  add_definitions(\"-DENABLE_BF16\")\n  message(\"CUDA_VERSION ${CUDA_VERSION_MAJOR}.${CUDA_VERSION_MINOR} is greater or equal than 11.0, enable -DENABLE_BF16 flag\")\nendif()\n\nif((${CUDA_VERSION_MAJOR} VERSION_GREATER_EQUAL \"11\" AND ${CUDA_VERSION_MINOR} VERSION_GREATER_EQUAL \"8\") OR (${CUDA_VERSION_MAJOR} VERSION_GREATER_EQUAL \"12\"))\n  add_definitions(\"-DENABLE_FP8\")\n  option(ENABLE_FP8 \"ENABLE_FP8\" OFF)\n  if(ENABLE_FP8)\n    message(\"CUDA_VERSION ${CUDA_VERSION_MAJOR}.${CUDA_VERSION_MINOR} is greater or equal than 11.8, enable -DENABLE_FP8 flag\")\n  endif()\nendif()\n\nset(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake/Modules)\nfind_package(CUDNN)\n\noption(BUILD_CUTLASS_MOE \"Builds CUTLASS kernels supporting MoE GEMM\" ON)\nif(BUILD_CUTLASS_MOE)\n  message(STATUS \"Add DBUILD_CUTLASS_MOE, requires CUTLASS. Increases compilation time\")\n  add_definitions(\"-DBUILD_CUTLASS_MOE\")\nendif()\n\noption(BUILD_CUTLASS_MIXED_GEMM \"Builds CUTLASS kernels supporting mixed gemm\" ON)\nif(BUILD_CUTLASS_MIXED_GEMM)\n  message(STATUS \"Add DBUILD_CUTLASS_MIXED_GEMM, requires CUTLASS. Increases compilation time\")\n  add_definitions(\"-DBUILD_CUTLASS_MIXED_GEMM\")\nendif()\n\noption(BUILD_TF \"Build in TensorFlow mode\" OFF)\noption(BUILD_TF2 \"Build in TensorFlow2 mode\" OFF)\noption(BUILD_PYT \"Build in PyTorch TorchScript class mode\" OFF)\noption(BUILD_TRT \"Build projects about TensorRT\" OFF)\noption(GIT_AUTOCLONE_CUTLASS \"Check submodules during build\" ON)\nif(NOT BUILD_MULTI_GPU)\n  option(BUILD_MULTI_GPU \"Build project about multi-GPU\" OFF)\nendif()\nif(NOT USE_TRITONSERVER_DATATYPE)\n  option(USE_TRITONSERVER_DATATYPE \"Build triton backend for triton server\" OFF)\nendif()\n\nfind_package(Git QUIET)\nif(GIT_FOUND AND EXISTS \"${PROJECT_SOURCE_DIR}/.git\")\n  if(GIT_AUTOCLONE_CUTLASS)\n    message(STATUS \"Running submodule update to fetch cutlass\")\n    execute_process(COMMAND ${GIT_EXECUTABLE} submodule update --init 3rdparty/cutlass\n                    WORKING_DIRECTORY ${CMAKE_CURRENT_SOURCE_DIR}\n                    RESULT_VARIABLE GIT_SUBMOD_RESULT)\n    if(NOT GIT_SUBMOD_RESULT EQUAL \"0\")\n      message(FATAL_ERROR \"git submodule update --init 3rdparty/cutlass failed with ${GIT_SUBMOD_RESULT}, please checkout cutlass submodule\")\n    endif()\n  endif()\nendif()\n\nset(CUTLASS_HEADER_DIR ${PROJECT_SOURCE_DIR}/3rdparty/cutlass/include)\nset(CUTLASS_EXTENSIONS_DIR ${PROJECT_SOURCE_DIR}/src/fastertransformer/cutlass_extensions/include)\n\noption(SPARSITY_SUPPORT \"Build project with Ampere sparsity feature support\" OFF)\n\noption(BUILD_FAST_MATH \"Build in fast math mode\" ON)\n\nif(BUILD_MULTI_GPU)\n  message(STATUS \"Add DBUILD_MULTI_GPU, requires MPI and NCCL\")\n  add_definitions(\"-DBUILD_MULTI_GPU\")\n  set(CMAKE_MODULE_PATH ${PROJECT_SOURCE_DIR}/cmake/Modules)\n  find_package(MPI REQUIRED)\n  find_package(NCCL REQUIRED)\n  set(CMAKE_MODULE_PATH \"\") # prevent the bugs for pytorch building\nendif()\n\nif(BUILD_PYT)\n  if(DEFINED ENV{NVIDIA_PYTORCH_VERSION})\n    if($ENV{NVIDIA_PYTORCH_VERSION} VERSION_LESS \"20.03\")\n      message(FATAL_ERROR \"NVIDIA PyTorch image is too old for TorchScript mode.\")\n    endif()\n    if($ENV{NVIDIA_PYTORCH_VERSION} VERSION_EQUAL \"20.03\")\n      add_definitions(-DLEGACY_THS=1)\n    endif()\n  endif()\nendif()\n\nif(USE_TRITONSERVER_DATATYPE)\n  message(\"-- USE_TRITONSERVER_DATATYPE\")\n  add_definitions(\"-DUSE_TRITONSERVER_DATATYPE\")\nendif()\n\nset(CXX_STD \"14\" CACHE STRING \"C++ standard\")\n\nset(CUDA_PATH ${CUDA_TOOLKIT_ROOT_DIR})\n\nset(TF_PATH \"\" CACHE STRING \"TensorFlow path\")\nset(CUSPARSELT_PATH \"\" CACHE STRING \"cuSPARSELt path\")\n\nif((BUILD_TF OR BUILD_TF2) AND NOT TF_PATH)\n  message(FATAL_ERROR \"TF_PATH must be set if BUILD_TF or BUILD_TF2 (=TensorFlow mode) is on.\")\nendif()\n\nlist(APPEND CMAKE_MODULE_PATH ${CUDA_PATH}/lib64)\n\n# profiling\noption(USE_NVTX \"Whether or not to use nvtx\" ON)\nif(USE_NVTX)\n  message(STATUS \"NVTX is enabled.\")\n  add_definitions(\"-DUSE_NVTX\")\nendif()\n\n# setting compiler flags\nset(CMAKE_C_FLAGS    \"${CMAKE_C_FLAGS}\")\nset(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS} -std=c++17\")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -Xcompiler -Wall -ldl\") # -Xptxas -v\n\nset(SM_SETS 52 60 61 70 75 80 86 89 90)\nset(USING_WMMA False)\nset(FIND_SM False)\n\nforeach(SM_NUM IN LISTS SM_SETS)\n  string(FIND \"${SM}\" \"${SM_NUM}\" SM_POS)\n  if(SM_POS GREATER -1)\n    if(FIND_SM STREQUAL False)\n      set(ENV{TORCH_CUDA_ARCH_LIST} \"\")\n    endif()\n    set(FIND_SM True)\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -gencode=arch=compute_${SM_NUM},code=\\\\\\\"sm_${SM_NUM},compute_${SM_NUM}\\\\\\\"\")\n\n    if (SM_NUM STREQUAL 70 OR SM_NUM STREQUAL 75 OR SM_NUM STREQUAL 80 OR SM_NUM STREQUAL 86 OR SM_NUM STREQUAL 89 OR SM_NUM STREQUAL 90)\n      set(USING_WMMA True)\n    endif()\n\n    if(BUILD_PYT)\n      string(SUBSTRING ${SM_NUM} 0 1 SM_MAJOR)\n      string(SUBSTRING ${SM_NUM} 1 1 SM_MINOR)\n      set(ENV{TORCH_CUDA_ARCH_LIST} \"$ENV{TORCH_CUDA_ARCH_LIST}\\;${SM_MAJOR}.${SM_MINOR}\")\n    endif()\n\n    list(APPEND CMAKE_CUDA_ARCHITECTURES ${SM_NUM})\n    message(\"-- Assign GPU architecture (sm=${SM_NUM})\")\n  endif()\nendforeach()\n\nif(USING_WMMA STREQUAL True)\n  set(CMAKE_C_FLAGS    \"${CMAKE_C_FLAGS}    -DWMMA\")\n  set(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS}  -DWMMA\")\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -DWMMA\")\n  message(\"-- Use WMMA\")\nendif()\n\nif(NOT (FIND_SM STREQUAL True))\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS}  \\\n                        -gencode=arch=compute_70,code=\\\\\\\"sm_70,compute_70\\\\\\\" \\\n                        -gencode=arch=compute_75,code=\\\\\\\"sm_75,compute_75\\\\\\\" \\\n                        -gencode=arch=compute_80,code=\\\\\\\"sm_80,compute_80\\\\\\\" \\\n                        -gencode=arch=compute_86,code=\\\\\\\"sm_86,compute_86\\\\\\\" \\\n                        \")\n  #                      -rdc=true\")\n  set(CMAKE_C_FLAGS    \"${CMAKE_C_FLAGS}    -DWMMA\")\n  set(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS}  -DWMMA\")\n  set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} -DWMMA\")\n  if(BUILD_PYT)\n    set(ENV{TORCH_CUDA_ARCH_LIST} \"7.0;7.5;8.0;8.6\")\n  endif()\n  set(CMAKE_CUDA_ARCHITECTURES 70 75 80 86)\n  message(\"-- Assign GPU architecture (sm=70,75,80,86)\")\nendif()\n\nif(BUILD_PYT)\n  set(TORCH_CUDA_ARCH_LIST $ENV{TORCH_CUDA_ARCH_LIST})\nendif()\n\nset(CMAKE_C_FLAGS_DEBUG    \"${CMAKE_C_FLAGS_DEBUG}    -Wall -O0\")\nset(CMAKE_CXX_FLAGS_DEBUG  \"${CMAKE_CXX_FLAGS_DEBUG}  -Wall -O0\")\n# set(CMAKE_CUDA_FLAGS_DEBUG \"${CMAKE_CUDA_FLAGS_DEBUG} -O0 -G -Xcompiler -Wall  --ptxas-options=-v --resource-usage\")\nset(CMAKE_CUDA_FLAGS_DEBUG \"${CMAKE_CUDA_FLAGS_DEBUG} -O0 -G -Xcompiler -Wall -DCUDA_PTX_FP8_F2FP_ENABLED\")\n\nset(CMAKE_CXX_STANDARD \"${CXX_STD}\")\nset(CMAKE_CXX_STANDARD_REQUIRED ON)\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-extended-lambda\")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr\")\nset(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --std=c++${CXX_STD} -DCUDA_PTX_FP8_F2FP_ENABLED\")\n\nset(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -O3\")\n# set(CMAKE_CUDA_FLAGS_RELEASE \"${CMAKE_CUDA_FLAGS_RELEASE} -Xcompiler -O3 --ptxas-options=--verbose\")\nset(CMAKE_CUDA_FLAGS_RELEASE \"${CMAKE_CUDA_FLAGS_RELEASE} -Xcompiler -O3 -DCUDA_PTX_FP8_F2FP_ENABLED\")\nif(BUILD_FAST_MATH)\nset(CMAKE_CUDA_FLAGS_RELEASE \"${CMAKE_CUDA_FLAGS_RELEASE} --use_fast_math\")\nmessage(\"CMAKE_CUDA_FLAGS_RELEASE: ${CMAKE_CUDA_FLAGS_RELEASE}\")\nendif()\n\nset(CMAKE_ARCHIVE_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_LIBRARY_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/lib)\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\nset(COMMON_HEADER_DIRS\n  ${PROJECT_SOURCE_DIR}\n  ${CUDA_PATH}/include\n  ${CUTLASS_HEADER_DIR}\n  ${CUTLASS_EXTENSIONS_DIR}\n  ${PROJECT_SOURCE_DIR}/3rdparty/trt_fp8_fmha/src\n  ${PROJECT_SOURCE_DIR}/3rdparty/trt_fp8_fmha/generated\n)\nmessage(\"-- COMMON_HEADER_DIRS: ${COMMON_HEADER_DIRS}\")\n\nset(COMMON_LIB_DIRS\n  ${CUDA_PATH}/lib64\n)\n\nif (SPARSITY_SUPPORT)\n  list(APPEND COMMON_HEADER_DIRS ${CUSPARSELT_PATH}/include)\n  list(APPEND COMMON_LIB_DIRS ${CUSPARSELT_PATH}/lib64)\n  add_definitions(-DSPARSITY_ENABLED=1)\nendif()\n\nif(BUILD_TF)\n  list(APPEND COMMON_HEADER_DIRS ${TF_PATH}/include)\n  list(APPEND COMMON_LIB_DIRS ${TF_PATH})\n  add_definitions(-D_GLIBCXX_USE_CXX11_ABI=0)\nendif()\n\nif(BUILD_TF2)\n  list(APPEND COMMON_HEADER_DIRS ${TF_PATH}/include)\n  list(APPEND COMMON_LIB_DIRS ${TF_PATH})\n  add_definitions(-D_GLIBCXX_USE_CXX11_ABI=1)\nendif()\n\nset(PYTHON_PATH \"python\" CACHE STRING \"Python path\")\nif(BUILD_PYT)\n  execute_process(COMMAND ${PYTHON_PATH} \"-c\" \"from __future__ import print_function; import torch; print(torch.__version__,end='');\"\n                  RESULT_VARIABLE _PYTHON_SUCCESS\n                  OUTPUT_VARIABLE TORCH_VERSION)\n  if (TORCH_VERSION VERSION_LESS \"1.5.0\")\n      message(FATAL_ERROR \"PyTorch >= 1.5.0 is needed for TorchScript mode.\")\n  endif()\n  execute_process(COMMAND ${PYTHON_PATH} \"-c\" \"from __future__ import print_function; import os; import torch;\nprint(os.path.dirname(torch.__file__),end='');\"\n                  RESULT_VARIABLE _PYTHON_SUCCESS\n                  OUTPUT_VARIABLE TORCH_DIR)\n  if (NOT _PYTHON_SUCCESS MATCHES 0)\n      message(FATAL_ERROR \"Torch config Error.\")\n  endif()\n  list(APPEND CMAKE_PREFIX_PATH ${TORCH_DIR})\n  find_package(Torch REQUIRED)\n  execute_process(COMMAND ${PYTHON_PATH} \"-c\" \"from __future__ import print_function; from distutils import sysconfig;\nprint(sysconfig.get_python_inc());\"\n                  RESULT_VARIABLE _PYTHON_SUCCESS\n                  OUTPUT_VARIABLE PY_INCLUDE_DIR)\n  if (NOT _PYTHON_SUCCESS MATCHES 0)\n      message(FATAL_ERROR \"Python config Error.\")\n  endif()\n  list(APPEND COMMON_HEADER_DIRS ${PY_INCLUDE_DIR})\n  execute_process(COMMAND ${PYTHON_PATH} \"-c\" \"from __future__ import print_function; import torch;\nprint(torch._C._GLIBCXX_USE_CXX11_ABI,end='');\"\n                  RESULT_VARIABLE _PYTHON_SUCCESS\n                  OUTPUT_VARIABLE USE_CXX11_ABI)\n  message(\"-- USE_CXX11_ABI=${USE_CXX11_ABI}\")\n  if (USE_CXX11_ABI)\n    set(CMAKE_CUDA_FLAGS_RELEASE \"${CMAKE_CUDA_FLAGS_RELEASE} -D_GLIBCXX_USE_CXX11_ABI=1\")\n    set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -D_GLIBCXX_USE_CXX11_ABI=1\")\n    set(CMAKE_CUDA_FLAGS_DEBUG \"${CMAKE_CUDA_FLAGS_DEBUG} -D_GLIBCXX_USE_CXX11_ABI=1\")\n    set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -D_GLIBCXX_USE_CXX11_ABI=1\")\n  else()\n    set(CMAKE_CUDA_FLAGS_RELEASE \"${CMAKE_CUDA_FLAGS_RELEASE} -D_GLIBCXX_USE_CXX11_ABI=0\")\n    set(CMAKE_CXX_FLAGS_RELEASE \"${CMAKE_CXX_FLAGS_RELEASE} -D_GLIBCXX_USE_CXX11_ABI=0\")\n    set(CMAKE_CUDA_FLAGS_DEBUG \"${CMAKE_CUDA_FLAGS_DEBUG} -D_GLIBCXX_USE_CXX11_ABI=0\")\n    set(CMAKE_CXX_FLAGS_DEBUG \"${CMAKE_CXX_FLAGS_DEBUG} -D_GLIBCXX_USE_CXX11_ABI=0\")\n  endif()\nendif()\n\nif (BUILD_MULTI_GPU)\n  list(APPEND COMMON_HEADER_DIRS ${MPI_INCLUDE_PATH})\n  list(APPEND COMMON_LIB_DIRS /usr/local/mpi/lib)\nendif()\n\nif(USE_TRITONSERVER_DATATYPE)\n  list(APPEND COMMON_HEADER_DIRS ${PROJECT_SOURCE_DIR}/../repo-core-src/include)\nendif()\n\ninclude_directories(\n  ${COMMON_HEADER_DIRS}\n)\n\nlink_directories(\n  ${COMMON_LIB_DIRS}\n)\n\nadd_subdirectory(3rdparty)\nadd_subdirectory(src)\nadd_subdirectory(examples)\n\nadd_subdirectory(tests)\n\n# # Mesaure the compile time\noption(MEASURE_BUILD_TIME \"Measure the build time of each module\" OFF)\nif (MEASURE_BUILD_TIME)\n  set_property(GLOBAL PROPERTY RULE_LAUNCH_COMPILE \"${CMAKE_COMMAND} -E time\")\n  set_property(GLOBAL PROPERTY RULE_LAUNCH_CUSTOM \"${CMAKE_COMMAND} -E time\")\n  set_property(GLOBAL PROPERTY RULE_LAUNCH_LINK \"${CMAKE_COMMAND} -E time\")\nendif()\n\n########################################\n\nadd_library(transformer-shared SHARED\n  $<TARGET_OBJECTS:BaseBeamSearchLayer>\n  $<TARGET_OBJECTS:BaseSamplingLayer>\n  $<TARGET_OBJECTS:BeamSearchLayer>\n  $<TARGET_OBJECTS:Bert>\n  $<TARGET_OBJECTS:BertLayerWeight>\n  $<TARGET_OBJECTS:BertTritonBackend>\n  $<TARGET_OBJECTS:BertWeight>\n  $<TARGET_OBJECTS:DecoderCrossAttentionLayer>\n  $<TARGET_OBJECTS:DecoderSelfAttentionLayer>\n  $<TARGET_OBJECTS:DynamicDecodeLayer>\n  $<TARGET_OBJECTS:FfnLayer>\n  $<TARGET_OBJECTS:FusedAttentionLayer>\n  $<TARGET_OBJECTS:GptContextAttentionLayer>\n  $<TARGET_OBJECTS:GptJ>\n  $<TARGET_OBJECTS:GptJContextDecoder>\n  $<TARGET_OBJECTS:GptJDecoder>\n  $<TARGET_OBJECTS:GptJDecoderLayerWeight>\n  $<TARGET_OBJECTS:GptJTritonBackend>\n  $<TARGET_OBJECTS:GptJWeight>\n  $<TARGET_OBJECTS:GptNeoX>\n  $<TARGET_OBJECTS:GptNeoXContextDecoder>\n  $<TARGET_OBJECTS:GptNeoXDecoder>\n  $<TARGET_OBJECTS:GptNeoXDecoderLayerWeight>\n  $<TARGET_OBJECTS:GptNeoXTritonBackend>\n  $<TARGET_OBJECTS:GptNeoXWeight>\n  $<TARGET_OBJECTS:LinearAdapterLayer>\n  $<TARGET_OBJECTS:OnlineBeamSearchLayer>\n  $<TARGET_OBJECTS:ParallelGpt>\n  $<TARGET_OBJECTS:ParallelGptContextDecoder>\n  $<TARGET_OBJECTS:ParallelGptDecoder>\n  $<TARGET_OBJECTS:ParallelGptDecoderLayerWeight>\n  $<TARGET_OBJECTS:ParallelGptTritonBackend>\n  $<TARGET_OBJECTS:ParallelGptWeight>\n  $<TARGET_OBJECTS:T5Common>\n  $<TARGET_OBJECTS:T5Decoder>\n  $<TARGET_OBJECTS:T5Decoding>\n  $<TARGET_OBJECTS:T5Encoder>\n  $<TARGET_OBJECTS:T5TritonBackend>\n  $<TARGET_OBJECTS:T5EncoderTritonBackend>\n  $<TARGET_OBJECTS:TensorParallelDecoderCrossAttentionLayer>\n  $<TARGET_OBJECTS:TensorParallelDecoderSelfAttentionLayer>\n  $<TARGET_OBJECTS:TensorParallelGeluFfnLayer>\n  $<TARGET_OBJECTS:TensorParallelSiluFfnLayer>\n  $<TARGET_OBJECTS:TensorParallelGptContextAttentionLayer>\n  $<TARGET_OBJECTS:TensorParallelReluFfnLayer>\n  $<TARGET_OBJECTS:TensorParallelUnfusedAttentionLayer>\n  $<TARGET_OBJECTS:TopKSamplingLayer>\n  $<TARGET_OBJECTS:TopPSamplingLayer>\n  $<TARGET_OBJECTS:TransformerTritonBackend>\n  $<TARGET_OBJECTS:UnfusedAttentionLayer>\n  $<TARGET_OBJECTS:activation_int8_kernels>\n  $<TARGET_OBJECTS:activation_kernels>\n  $<TARGET_OBJECTS:add_bias_transpose_kernels>\n  $<TARGET_OBJECTS:add_residual_kernels>\n  $<TARGET_OBJECTS:ban_bad_words>\n  $<TARGET_OBJECTS:beam_search_penalty_kernels>\n  $<TARGET_OBJECTS:beam_search_topk_kernels>\n  $<TARGET_OBJECTS:bert_preprocess_kernels>\n  $<TARGET_OBJECTS:calibrate_quantize_weight_kernels>\n  $<TARGET_OBJECTS:cublasAlgoMap>\n  $<TARGET_OBJECTS:cublasMMWrapper>\n  $<TARGET_OBJECTS:cuda_driver_wrapper>\n  $<TARGET_OBJECTS:cuda_utils>\n  $<TARGET_OBJECTS:custom_ar_comm>\n  $<TARGET_OBJECTS:custom_ar_kernels>\n  $<TARGET_OBJECTS:cutlass_heuristic>\n  $<TARGET_OBJECTS:cutlass_preprocessors>\n  $<TARGET_OBJECTS:decoder_masked_multihead_attention>\n  $<TARGET_OBJECTS:decoding_kernels>\n  $<TARGET_OBJECTS:fpA_intB_gemm>\n  $<TARGET_OBJECTS:gen_relative_pos_bias>\n  $<TARGET_OBJECTS:gpt_kernels>\n  $<TARGET_OBJECTS:int8_gemm>\n  $<TARGET_OBJECTS:layernorm_int8_kernels>\n  $<TARGET_OBJECTS:layernorm_kernels>\n  $<TARGET_OBJECTS:layout_transformer_int8_kernels>\n  $<TARGET_OBJECTS:logprob_kernels>\n  $<TARGET_OBJECTS:logger>\n  $<TARGET_OBJECTS:longformer_kernels>\n  $<TARGET_OBJECTS:matrix_transpose_kernels>\n  $<TARGET_OBJECTS:matrix_vector_multiplication>\n  $<TARGET_OBJECTS:memory_utils>\n  $<TARGET_OBJECTS:moe_gemm_kernels>\n  $<TARGET_OBJECTS:moe_kernels>\n  $<TARGET_OBJECTS:mpi_utils>\n  $<TARGET_OBJECTS:nccl_utils>\n  $<TARGET_OBJECTS:nvtx_utils>\n  $<TARGET_OBJECTS:online_softmax_beamsearch_kernels>\n  $<TARGET_OBJECTS:quantization_int8_kernels>\n  $<TARGET_OBJECTS:sampling_penalty_kernels>\n  $<TARGET_OBJECTS:sampling_topk_kernels>\n  $<TARGET_OBJECTS:sampling_topp_kernels>\n  $<TARGET_OBJECTS:softmax_int8_kernels>\n  $<TARGET_OBJECTS:stop_criteria>\n  $<TARGET_OBJECTS:tensor>\n  $<TARGET_OBJECTS:transpose_int8_kernels>\n  $<TARGET_OBJECTS:trt_fused_multi_head_attention>\n  $<TARGET_OBJECTS:unfused_attention_kernels>\n  $<TARGET_OBJECTS:word_list>\n)\n\nif (BUILD_MULTI_GPU)\ntarget_link_libraries(transformer-shared PUBLIC\n  -lmpi\n  ${NCCL_LIBRARIES}\n)\nendif()\n\nif(USE_NVTX)\ntarget_link_libraries(transformer-shared PUBLIC\n  -lnvToolsExt\n)\nendif()\n  \nif (ENABLE_FP8)\ntarget_link_libraries(transformer-shared PUBLIC \n  $<TARGET_OBJECTS:BertFP8>\n  $<TARGET_OBJECTS:BertFP8Weight>\n  $<TARGET_OBJECTS:DecoderSelfAttentionFP8Layer>\n  $<TARGET_OBJECTS:FfnFP8Layer>\n  $<TARGET_OBJECTS:GptContextAttentionFP8Layer>\n  $<TARGET_OBJECTS:GptFP8>\n  $<TARGET_OBJECTS:GptFP8ContextDecoder>\n  $<TARGET_OBJECTS:GptFP8Decoder>\n  $<TARGET_OBJECTS:GptFP8DecoderLayerWeight>\n  $<TARGET_OBJECTS:ParallelGptFP8TritonBackend>\n  $<TARGET_OBJECTS:GptFP8Weight>\n  $<TARGET_OBJECTS:SelfAttentionFP8Layer>\n  $<TARGET_OBJECTS:TensorParallelDecoderSelfAttentionFP8Layer>\n  $<TARGET_OBJECTS:TensorParallelGeluFfnFP8Layer>\n  $<TARGET_OBJECTS:TensorParallelGptContextAttentionFP8Layer>\n  $<TARGET_OBJECTS:activation_fp8_kernels>\n  $<TARGET_OBJECTS:cublasFP8MMWrapper>\n  $<TARGET_OBJECTS:cuda_fp8_utils>\n  $<TARGET_OBJECTS:fp8_qgmma_1x1_utils>\n  $<TARGET_OBJECTS:layernorm_fp8_kernels>\n  $<TARGET_OBJECTS:unfused_attention_fp8_kernels>\n)\nendif()\n\nset_target_properties(transformer-shared PROPERTIES POSITION_INDEPENDENT_CODE ON)\nset_target_properties(transformer-shared PROPERTIES CUDA_RESOLVE_DEVICE_SYMBOLS ON)\nset_target_properties(transformer-shared PROPERTIES LINKER_LANGUAGE CXX)\ntarget_link_libraries(transformer-shared PUBLIC -lcudart -lcublas -lcublasLt -lcurand)\n\ninclude(GNUInstallDirs)\nset(INSTALL_CONFIGDIR ${CMAKE_INSTALL_LIBDIR}/cmake/FasterTransformer)\n\ninclude(CMakePackageConfigHelpers)\nconfigure_package_config_file(\n  ${CMAKE_CURRENT_LIST_DIR}/cmake/FasterTransformerConfig.cmake.in\n  ${CMAKE_CURRENT_BINARY_DIR}/FasterTransformerConfig.cmake\n  INSTALL_DESTINATION ${INSTALL_CONFIGDIR}\n)\n\ninstall(\n  FILES\n  ${CMAKE_CURRENT_BINARY_DIR}/FasterTransformerConfig.cmake\n  DESTINATION ${INSTALL_CONFIGDIR}\n)\n\ninstall(\n  TARGETS\n    transformer-shared\n  EXPORT\n    transformer-shared-targets\n  LIBRARY DESTINATION ${CMAKE_INSTALL_PREFIX}/backends/fastertransformer\n  ARCHIVE DESTINATION ${CMAKE_INSTALL_PREFIX}/backends/fastertransformer\n)\n\ninstall(\n  EXPORT\n    transformer-shared-targets\n  FILE\n    FasterTransformerTargets.cmake\n  DESTINATION\n    ${INSTALL_CONFIGDIR}\n)\n\nexport(\n  EXPORT\n    transformer-shared-targets\n  FILE\n    ${CMAKE_CURRENT_BINARY_DIR}/FasterTransformerTargets.cmake\n  NAMESPACE\n    TritonCore::\n)\n\nexport(PACKAGE FasterTransformer)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 2.37109375,
          "content": "\n## FasterTransformer OSS Contribution Rules\n\n#### Issue Tracking\n\n* All enhancement, bugfix, or change requests must begin with the creation of a [FasterTransformer Issue Request](https://github.com/nvidia/FasterTransformer/issues).\n  * The issue request must be reviewed by FasterTransformer engineers and approved prior to code review.\n\n#### Signing Your Work\n\n* We require that all contributors \"sign-off\" on their commits. This certifies that the contribution is your original work, or you have rights to submit it under the same license, or a compatible license.\n\n  * Any contribution which contains commits that are not Signed-Off will not be accepted.\n\n* To sign off on a commit you simply use the `--signoff` (or `-s`) option when committing your changes:\n  ```bash\n  $ git commit -s -m \"Add cool feature.\"\n  ```\n  This will append the following to your commit message:\n  ```\n  Signed-off-by: Your Name <your@email.com>\n  ```\n\n* Full text of the DCO:\n\n  ```\n    Developer Certificate of Origin\n    Version 1.1\n    \n    Copyright (C) 2004, 2006 The Linux Foundation and its contributors.\n    1 Letterman Drive\n    Suite D4700\n    San Francisco, CA, 94129\n    \n    Everyone is permitted to copy and distribute verbatim copies of this license document, but changing it is not allowed.\n  ```\n\n  ```\n    Developer's Certificate of Origin 1.1\n    \n    By making a contribution to this project, I certify that:\n    \n    (a) The contribution was created in whole or in part by me and I have the right to submit it under the open source license indicated in the file; or\n    \n    (b) The contribution is based upon previous work that, to the best of my knowledge, is covered under an appropriate open source license and I have the right under that license to submit that work with modifications, whether created in whole or in part by me, under the same open source license (unless I am permitted to submit under a different license), as indicated in the file; or\n    \n    (c) The contribution was provided directly to me by some other person who certified (a), (b) or (c) and I have not modified it.\n    \n    (d) I understand and agree that this project and the contribution are public and that a record of the contribution (including all personal information I submit with it, including my sign-off) is maintained indefinitely and may be redistributed consistent with this project or the open source license(s) involved.\n  ```"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.091796875,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 22.935546875,
          "content": "**Note: FasterTransformer development has transitioned to [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM/tree/release/0.5.0). All developers are encouraged to leverage TensorRT-LLM to get the latest improvements on LLM Inference. The NVIDIA/FasterTransformer repo will stay up, but will not have further development.**\n\n# FasterTransformer\n\nThis repository provides a script and recipe to run the highly optimized transformer-based encoder and decoder component, and it is tested and maintained by NVIDIA.\n\n## Table Of Contents\n\n- [FasterTransformer](#fastertransformer)\n  - [Table Of Contents](#table-of-contents)\n  - [Model overview](#model-overview)\n    - [Support matrix](#support-matrix)\n  - [Advanced](#advanced)\n    - [Global Environment](#global-environment)\n  - [Performance](#performance)\n    - [BERT base performance](#bert-base-performance)\n      - [BERT base performances of FasterTransformer new features](#bert-base-performances-of-fastertransformer-new-features)\n      - [BERT base performance on TensorFlow](#bert-base-performance-on-tensorflow)\n      - [BERT base performance on PyTorch](#bert-base-performance-on-pytorch)\n    - [Decoding and Decoder performance](#decoding-and-decoder-performance)\n      - [Decoder and Decoding end-to-end translation performance on TensorFlow](#decoder-and-decoding-end-to-end-translation-performance-on-tensorflow)\n      - [Decoder and Decoding end-to-end translation performance on PyTorch](#decoder-and-decoding-end-to-end-translation-performance-on-pytorch)\n    - [GPT performance](#gpt-performance)\n  - [Release notes](#release-notes)\n    - [Changelog](#changelog)\n    - [Known issues](#known-issues)\n\n## Model overview\n\nIn NLP, encoder and decoder are two important components, with the transformer layer becoming a popular architecture for both components. FasterTransformer implements a highly optimized transformer layer for both the encoder and decoder for inference. On Volta, Turing and Ampere GPUs, the computing power of Tensor Cores are used automatically when the precision of the data and weights are FP16.\n\nFasterTransformer is built on top of CUDA, cuBLAS, cuBLASLt and C++. We provide at least one API of the following frameworks: TensorFlow, PyTorch and Triton backend. Users can integrate FasterTransformer into these frameworks directly. For supporting frameworks, we also provide example codes to demonstrate how to use, and show the performance on these frameworks.\n\n### Support matrix\n\n| Models           | Framework      | FP16 | INT8 (after Turing) | Sparsity (after Ampere) | Tensor parallel | Pipeline parallel | FP8 (after Hopper) |\n| ---------------- | -------------- | ---- | ------------------- | ----------------------- | --------------- | ----------------- | ------------------ |\n| BERT             | TensorFlow     | Yes  | Yes                 | -                       | -               | -                 | -                  |\n| BERT             | PyTorch        | Yes  | Yes                 | Yes                     | Yes             | Yes               | -                  |\n| BERT             | Triton backend | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| BERT             | C++            | Yes  | Yes                 | -                       | -               | -                 | Yes                |\n| XLNet            | C++            | Yes  | -                   | -                       | -               | -                 | -                  |\n| Encoder          | TensorFlow     | Yes  | Yes                 | -                       | -               | -                 | -                  |\n| Encoder          | PyTorch        | Yes  | Yes                 | Yes                     | -               | -                 | -                  |\n| Decoder          | TensorFlow     | Yes  | -                   | -                       | -               | -                 | -                  |\n| Decoder          | PyTorch        | Yes  | -                   | -                       | -               | -                 | -                  |\n| Decoding         | TensorFlow     | Yes  | -                   | -                       | -               | -                 | -                  |\n| Decoding         | PyTorch        | Yes  | -                   | -                       | -               | -                 | -                  |\n| GPT              | TensorFlow     | Yes  | -                   | -                       | -               | -                 | -                  |\n| GPT/OPT          | PyTorch        | Yes  | -                   | -                       | Yes             | Yes               | Yes                |\n| GPT/OPT          | Triton backend | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| GPT-MoE          | PyTorch        | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| BLOOM            | PyTorch        | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| BLOOM            | Triton backend | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| GPT-J            | Triton backend | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| Longformer       | PyTorch        | Yes  | -                   | -                       | -               | -                 | -                  |\n| T5/UL2           | PyTorch        | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| T5               | TensorFlow 2   | Yes  | -                   | -                       | -               | -                 | -                  |\n| T5/UL2           | Triton backend | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| T5               | TensorRT       | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| T5-MoE           | PyTorch        | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| Swin Transformer | PyTorch        | Yes  | Yes                 | -                       | -               | -                 | -                  |\n| Swin Transformer | TensorRT       | Yes  | Yes                 | -                       | -               | -                 | -                  |\n| ViT              | PyTorch        | Yes  | Yes                 | -                       | -               | -                 | -                  |\n| ViT              | TensorRT       | Yes  | Yes                 | -                       | -               | -                 | -                  |\n| GPT-NeoX         | PyTorch        | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| GPT-NeoX         | Triton backend | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| BART/mBART       | PyTorch        | Yes  | -                   | -                       | Yes             | Yes               | -                  |\n| WeNet            | C++            | Yes  | -                   | -                       | -               | -                 | -                  |\n| DeBERTa          | TensorFlow 2   | Yes  | -                   | -                       | On-going        | On-going          | -                  |\n| DeBERTa          | PyTorch        | Yes  | -                   | -                       | On-going        | On-going          | -                  |\n\n* Note that the FasterTransformer supports the models above on C++ because all source codes are built on C++.\n\nMore details of specific models are put in `xxx_guide.md` of [`docs/`](docs), where `xxx` means the model name. Some common questions and the respective answers are put in [`docs/QAList.md`](docs/QAList.md). Note that the model of Encoder and BERT are similar and we put the explanation into `bert_guide.md` together.\n\n## Advanced\n\nThe following code lists the directory structure of FasterTransformer:\n\n```\n/src/fastertransformer: source code of FasterTransformer\n    |--/cutlass_extensions: Implementation of cutlass gemm/kernels.\n    |--/kernels: CUDA kernels for different models/layers and operations, like addBiasResiual.\n    |--/layers: Implementation of layer modules, like attention layer, ffn layer.\n    |--/models: Implementation of different models, like BERT, GPT.\n    |--/tensorrt_plugin: encapluate FasterTransformer into TensorRT plugin.\n    |--/tf_op: custom Tensorflow OP implementation\n    |--/th_op: custom PyTorch OP implementation\n    |--/triton_backend: custom triton backend implementation\n    |--/utils: Contains common cuda utils, like cublasMMWrapper, memory_utils\n/examples: C++, tensorflow and pytorch interface examples\n    |--/cpp: C++ interface examples\n    |--/pytorch: PyTorch OP examples\n    |--/tensorflow: TensorFlow OP examples\n    |--/tensorrt: TensorRT examples\n/docs: Documents to explain the details of implementation of different models, and show the benchmark\n/benchmark: Contains the scripts to run the benchmarks of different models\n/tests: Unit tests\n/templates: Documents to explain how to add a new model/example into FasterTransformer repo\n```\n\nNote that many folders contains many sub-folders to split different models. Quantization tools are move to `examples`, like `examples/tensorflow/bert/bert-quantization/` and `examples/pytorch/bert/bert-quantization-sparsity/`.\n\n\n### Global Environment\n\nFasterTransformer provides some convenient environment variables for debuging and testing.\n\n1. `FT_LOG_LEVEL`: This environment controls the log level of debug messae. More details are in `src/fastertransformer/utils/logger.h`. Note that the program will print lots of message when the level is lower than `DEBUG` and the program would become very slow.\n2. `FT_NVTX`: If it is set to be `ON` like `FT_NVTX=ON ./bin/gpt_example`, the program will insert tha tag of nvtx to help profiling the program.\n3. `FT_DEBUG_LEVEL`: If it is set to be `DEBUG`, then the program will run `cudaDeviceSynchronize()` after every kernels. Otherwise, the kernel is executued asynchronously by default. It is helpful to locate the error point during debuging. But this flag affects the performance of program significantly. So, it should be used only for debuging.\n\n## Performance\n\nHardware settings:\n\n* 8xA100-80GBs (with mclk 1593MHz, pclk 1410MHz) with AMD EPYC 7742 64-Core Processor\n* T4 (with mclk 5000MHz, pclk 1590MHz) with Intel(R) Xeon(R) CPU E5-2670 0 @ 2.60GHz\n\nIn order to run the following benchmark, we need to install the unix computing tool \"bc\" by\n\n```bash\napt-get install bc\n```\n\n### BERT base performance\n\nThe FP16 results of TensorFlow were obtained by running the `benchmarks/bert/tf_benchmark.sh`.\n\nThe INT8 results of TensorFlow were obtained by running the `benchmarks/bert/tf_int8_benchmark.sh`.\n\nThe FP16 results of PyTorch were obtained by running the `benchmarks/bert/pyt_benchmark.sh`.\n\nThe INT8 results of PyTorch were obtained by running the `benchmarks/bert/pyt_int8_benchmark.sh`.\n\nMore benchmarks are put in [`docs/bert_guide.md`](docs/bert_guide.md#bert-performance).\n\n#### BERT base performances of FasterTransformer new features\n\nThe following figure compares the performances of different features of FasterTransformer and FasterTransformer under FP16 on T4.\n\nFor large batch size and sequence length, both EFF-FT and FT-INT8-v2 bring about 2x speedup. Using Effective FasterTransformer and int8v2 at the same time can bring about 3.5x speedup compared to FasterTransformer FP16 for large case.\n\n<div align=center><img  width=80% src =\"docs/images/FT_Encoder_T4.png\"/></div>\n\n#### BERT base performance on TensorFlow\n\nThe following figure compares the performances of different features of FasterTransformer and TensorFlow XLA under FP16 on T4.\n\nFor small batch size and sequence length, using FasterTransformer can bring about 3x speedup.\n\nFor large batch size and sequence length, using Effective FasterTransformer with INT8-v2 quantization can bring about 5x speedup.\n\n<div align=center><img  width=80% src =\"docs/images/TF_Encoder_T4.png\"/></div>\n\n#### BERT base performance on PyTorch\n\nThe following figure compares the performances of different features of FasterTransformer and PyTorch TorchScript under FP16 on T4.\n\nFor small batch size and sequence length, using FasterTransformer CustomExt can bring about 4x ~ 6x speedup.\n\nFor large batch size and sequence length, using Effective FasterTransformer with INT8-v2 quantization can bring about 5x speedup.\n\n<div align=center><img  width=80% src =\"docs/images/Py_Encoder_T4.png\"/></div>\n\n### Decoding and Decoder performance\n\nThe results of TensorFlow were obtained by running the `benchmarks/decoding/tf_decoding_beamsearch_benchmark.sh` and `benchmarks/decoding/tf_decoding_sampling_benchmark.sh`\n\nThe results of PyTorch were obtained by running the `benchmarks/decoding/pyt_decoding_beamsearch_benchmark.sh`.\n\nIn the experiments of decoding, we updated the following parameters:\n\n* head_num = 8\n* size_per_head = 64\n* num_layers = 6 for both encoder and decoder\n* vocabulary_size = 32001 for TensorFlow sample codes, 31538 for PyTorch sample codes\n* memory_hidden_dim = 512\n* max sequenc elength = 128\n\nMore benchmarks are put in [`docs/decoder_guide.md`](docs/decoder_guide.md#decoding-performance).\n\n#### Decoder and Decoding end-to-end translation performance on TensorFlow\n\nThe following figure shows the speedup of of FT-Decoder op and FT-Decoding op compared to TensorFlow under FP16 with T4. Here, we use the throughput of translating a test set to prevent the total tokens of each methods may be different. Compared to TensorFlow, FT-Decoder provides 1.5x ~ 3x speedup; while FT-Decoding provides 4x ~ 18x speedup.\n\n<div align=center><img  width=80% src =\"docs/images/TF_Decoder_T4.png\"/></div>\n\n#### Decoder and Decoding end-to-end translation performance on PyTorch\n\nThe following figure shows the speedup of of FT-Decoder op and FT-Decoding op compared to PyTorch under FP16 with T4. Here, we use the throughput of translating a test set to prevent the total tokens of each methods may be different. Compared to PyTorch, FT-Decoder provides 1.2x ~ 3x speedup; while FT-Decoding provides 3.8x ~ 13x speedup.\n\n<div align=center><img  width=80% src =\"docs/images/Py_Decoder_T4.png\"/></div>\n\n### GPT performance\n\nThe following figure compares the performances of Megatron and FasterTransformer under FP16 on A100.\n\nIn the experiments of decoding, we updated the following parameters:\n\n* head_num = 96\n* size_per_head = 128\n* num_layers = 48 for GPT-89B model, 96 for GPT-175B model\n* data_type = FP16\n* vocab_size = 51200\n* top_p = 0.9\n* tensor parallel size = 8\n* input sequence length = 512\n* output sequence length = 32\n\n<div align=center><img  width=80% src =\"docs/images/FT_GPT_A100.png\"/></div>\n\n## Release notes\n\n### Changelog\n\nMay 2023\n- Fix bugs of generation early stopping\n\nJanuary 2023\n- Support GPT MoE\n- Support FP8 for Bert and GPT (**Experimental**)\n- Support DeBERTa on TensorFlow 2 and PyTorch\n\nDec 2022\n- **Release the FasterTransformer 5.2**\n- Support min length penalty\n\nNov 2022\n- Support T5 Tensorflow 2 custom op.\n- Support T5 MoE\n- Support WeNet\n- Support BART & mBART\n- Support SwinV2\n- Initial support for w8a8 int8 mode with GPT (preview)\n- Support fused mha in GPT\n\nOct 2022\n- Support BLOOM\n\nSep 2022\n- Support factual sampling ([link](https://arxiv.org/pdf/2206.04624.pdf)) in gpt\n- Support for IA3 adapting scheme in T5\n\nAug 2022\n- Support returning context tokens embeddings in GPT\n- **Release the FasterTransformer 5.1**\n- Support for interactive generation\n- Support for attention time-limited memory\n- Support mt5 and t5-v1.1\n\nJuly 2022\n- Support UL2 huggingface ckpt. ([link](https://huggingface.co/google/ul2))\n  - Fix bug of T5 under bfloat16.\n- Add ViT INT8 TensorRT Plugin\n- Support batch sampling\n- Support shared context optimization in GPT model\n\nJune 2022\n- Support streaming generation for triton backend.\n- Support OPT.\n- Support multi-node multi-GPU BERT under FP32, FP16 and BF16.\n\nMay 2022\n- Support bfloat16 on most models.\n- Support [prefix-prompt](https://arxiv.org/pdf/2101.00190.pdf) for GPT-J.\n- Support GPT-NeoX.\n  - epsilon value used in layernorm is now a parameter\n  - rotary embedding GPT-NeoX style (only GPT-J was implemented)\n  - load per-GPU layernorm and bias parameters\n  - weight conversion from EleutherAI checkpoint\n\nApril 2022\n- **Release the FasterTransformer 5.0**\n  - Change the default accumulation type of all gemm to FP32.\n  - Support bfloat16 inference in GPT model.\n  - Support Nemo Megatron T5 and Megatron-LM T5 model.\n  - Support ViT.\n\nMarch 2022\n- Support `stop_ids` and `ban_bad_ids` in GPT-J.\n- Support dynamice `start_id` and `end_id` in GPT-J, GPT, T5 and Decoding.\n\nFebruary 2022\n- Support Swin Transformer.\n- Optimize the k/v cache update of beam search by in-direction buffer.\n- Support runtime input for GPT-J, T5 and GPT.\n- Support soft prompt in GPT and GPT-J.\n- Support custom all reduce kernel.\n  - Limitation: \n    1. Only support tensor parallel size = 8 on DGX-A100.\n    2. Only support CUDA with cudaMallocAsync.\n\nDecember 2021\n- Add TensorRT plugin of T5 model.\n- Change some hyper-parameters of GPT model to runtime query.\n- Optimize the memory allocator under C++ code.\n- Fix bug of CUB including when using CUDA 11.5 or newer version.\n\nNovember 2021\n- **Update the FasterTransformer 5.0 beta**\n- Add GPT-3 INT8 weight only qauntization for batch size <= 2.\n- Support multi-node multi-gpu support on T5.\n- Enhance the multi-node multi-gpu supporting in GPT-3.\n\nAugust 2021\n- **Release the FasterTransformer 5.0 beta**\n  - Refactor the repo and codes\n  - And special thanks to NAVER Corp. for contributing a lot to this version, as listed below.\n    - Bugs fix\n      - Fix error that occurs when batch_size is less than max_batch_size for gpt pytorch wrapper.\n      - Fix memory leak that occurs every forward because of reused allocator.\n      - Fix race condition that occurs in repetition penalty kernel.\n    - Enhancement\n      - Add random seed setting.\n      - Fix GEMM buffer overflow on FP16 of GPT.\n      - Change to invalidate finished buffer for every completion.\n      - Introduce stop_before for early stop.\n  - Support Longformer.\n  - Rename `layer_para` to `pipeline_para`.\n  - Optimize the sorting of top p sampling.\n  - Support sparsity for Ampere GPUs on BERT.\n  - Support `size_per_head` 96, 160, 192, 224, 256 for GPT model.\n  - Support multi-node inference for GPT Triton backend.\n\nJune 2021\n- Support XLNet\n\nApril 2021\n- **Release the FasterTransformer 4.0**\n  - Support multi-gpus and multi-nodes inference for GPT model on C++ and PyTorch.\n  - Support single node, multi-gpus inference for GPT model on triton.\n  - Add the int8 fused multi-head attention kernel for bert.\n  - Add the FP16 fused multi-head attention kernel of V100 for bert.\n  - Optimize the kernel of decoder.\n  - Move to independent repo.\n  - Eager mode PyTorch extension is deprecated.\n\nDec 2020\n- **Release the FasterTransformer 3.1**\n  - Optimize the decoding by adding the finisehd mask to prevent useless computing.\n  - Support opennmt encoder.\n  - Remove the TensorRT plugin supporting.\n  - TorchScript custom op is deprecated.\n\nNov 2020\n- Optimize the INT8 inference.\n- Support PyTorch INT8 inference.\n- Provide PyTorch INT8 quantiztion tools.\n- Integrate the fused multi-head attention kernel of TensorRT into FasterTransformer.\n- Add unit test of SQuAD.\n- Update the missed NGC checkpoints.\n\nSep 2020\n- Support GPT2\n- **Release the FasterTransformer 3.0**\n  - Support INT8 quantization of encoder of cpp and TensorFlow op.\n  - Add bert-tf-quantization tool.\n  - Fix the issue that Cmake 15 or Cmake 16 fail to build this project.\n\nAug 2020\n- Fix the bug of trt plugin.\n\nJune 2020\n- **Release the FasterTransformer 2.1**\n  - Add Effective FasterTransformer based on the idea of [Effective Transformer](https://github.com/bytedance/effective_transformer) idea.\n  - Optimize the beam search kernels.\n  - Add PyTorch op supporting\n\nMay 2020\n- Fix the bug that seq_len of encoder must be larger than 3.\n- Add the position_encoding of decoding as the input of FasterTransformer decoding. This is convenient to use different types of position encoding. FasterTransformer does not compute the position encoding value, but only lookup the table.\n- Modifying the method of loading model in `translate_sample.py`.\n\nApril 2020\n- Rename `decoding_opennmt.h` to `decoding_beamsearch.h`\n- Add DiverseSiblingsSearch for decoding.\n- Add sampling into Decoding\n  - The implementation is in the `decoding_sampling.h`\n  - Add top_k sampling, top_p sampling for decoding.\n- Refactor the tensorflow custom op codes.\n  - Merge `bert_transformer_op.h`, `bert_transformer_op.cu.cc` into `bert_transformer_op.cc`\n  - Merge `decoder.h`, `decoder.cu.cc` into `decoder.cc`\n  - Merge `decoding_beamsearch.h`, `decoding_beamsearch.cu.cc` into `decoding_beamsearch.cc`\n- Fix the bugs of finalize function decoding.py.\n- Fix the bug of tf DiverseSiblingSearch.\n- Add BLEU scorer `bleu_score.py` into `utils`. Note that the BLEU score requires python3.\n- Fuse QKV Gemm of encoder and masked_multi_head_attention of decoder.\n- Add dynamic batch size and dynamic sequence length features into all ops.\n\nMarch 2020\n- Add feature in FasterTransformer 2.0\n  - Add `translate_sample.py` to demonstrate how to translate a sentence by restoring the pretrained model of OpenNMT-tf.\n- Fix bugs of Fastertransformer 2.0\n  - Fix the bug of maximum sequence length of decoder cannot be larger than 128.\n  - Fix the bug that decoding does not check finish or not after each step.\n  - Fix the bug of decoder about max_seq_len.\n  - Modify the decoding model structure to fit the OpenNMT-tf decoding model.\n    - Add a layer normalization layer after decoder.\n    - Add a normalization for inputs of decoder\n\nFebruary 2020\n- **Release the FasterTransformer 2.0**\n  - Provide a highly optimized OpenNMT-tf based decoder and decoding, including C++ API and TensorFlow op.\n  - Refine the sample codes of encoder.\n  - Add dynamic batch size feature into encoder op.\n\nJuly 2019\n- **Release the FasterTransformer 1.0**\n  - Provide a highly optimized bert equivalent transformer layer, including C++ API, TensorFlow op and TensorRT plugin.\n\n### Known issues\n\n- Cannot compile on tensorflow 2.10 due to undefined symbol issue.\n- Undefined symbol errors when import the extension\n  - Please `import torch` first. If this has been done, it is due to the incompatible C++ ABI. You may need to check the PyTorch used during compilation and execution are the same, or you need to check how your PyTorch is compiled, or the version of your GCC, etc.\n- Results of TensorFlow and OP would be different in decoding. This problem is caused by the accumulated log probability, and we do not avoid this problem.\n- If encounter some problem in the custom environment, try to use the gcc/g++ 4.8 to build the project of TensorFlow op, especially for TensorFlow 1.14.\n"
        },
        {
          "name": "benchmarks",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "docker",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "templates",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}