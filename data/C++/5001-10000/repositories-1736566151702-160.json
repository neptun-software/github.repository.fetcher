{
  "metadata": {
    "timestamp": 1736566151702,
    "page": 160,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "interpretml/interpret",
      "stars": 6349,
      "defaultBranch": "develop",
      "files": [
        {
          "name": ".gitattributes",
          "type": "blob",
          "size": 0.15625,
          "content": "# Set default behavior to automatically normalize line endings\n* text=auto\n# Set files to LF that need it regardless of OS\nDockerfile  eol=lf\n*.sh        eol=lf"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.27734375,
          "content": "# compile script output directories\n/bld/\n\n# Visual Studio\n.vs/\nenc_temp_folder/\n*.user\nTestResults/\n\n# Visual Studio Code\n.vscode/\n\n# Intellij\n.idea/\n\n# Python\n*.egg-info/\n*.ipynb_checkpoints/\nvenv/\nenv/\n.pytest_cache/\n__pycache__\n\n# General\n*.bak\n*.tmp\n*.csv\n*.db\ndist/\n*.env\n*.whl\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.9033203125,
          "content": "repos:\n  - repo: https://github.com/pre-commit/pre-commit-hooks\n    rev: \"v4.6.0\"\n    hooks:\n      - id: check-added-large-files\n      - id: check-case-conflict\n      - id: check-merge-conflict\n      - id: check-symlinks\n      - id: check-yaml\n      - id: debug-statements\n      - id: end-of-file-fixer\n      - id: mixed-line-ending\n      - id: requirements-txt-fixer\n      - id: trailing-whitespace\n  - repo: https://github.com/astral-sh/ruff-pre-commit\n    rev: \"v0.6.7\"\n    hooks:\n      - id: ruff\n        args: [\"--fix\", \"--show-fixes\"]\n      - id: ruff-format\n  - repo: https://github.com/codespell-project/codespell\n    rev: \"v2.3.0\"\n    hooks:\n      - id: codespell\n        args: [\"-L\", \"sur,nd\"]\n  - repo: local\n    hooks:\n      - id: disallow-caps\n        name: Disallow improper capitalization\n        language: pygrep\n        entry: PyBind|Numpy|Cmake|CCache|Github|PyTest\n        exclude: .pre-commit-config.yaml\n"
        },
        {
          "name": "ACKNOWLEDGEMENTS.md",
          "type": "blob",
          "size": 0.984375,
          "content": "Thank you to everyone here for your impactful contributions!\n\n- Yin Lou\n- Sarah Tan\n- Xuezhou Zhang\n- Johannes Gehrke\n- Marco Tulio Ribeiro\n- Gabriel Alon\n- Justin Colannino\n- Peggy Moloney\n- Giles Hooker\n- Aijun Zhang\n- Zebin Yang\n- Marc Sturm\n- Noemie Elhadad\n- Steven Drucker\n- Rob DeLine\n- Fred Hohman\n- Jina Suh\n- Jenn Wortman Vaughan\n- Hanna Wallach\n- Walter Martin\n- Eduardo de Leon\n- Ilya Mattiach\n- Vincent Xu\n- Mehrnoosh Sameki\n- Sarah Bird\n- Daniel Schnieder\n- Brandon Horn\n- Adam Raczkowski\n- Ester de Nicolas Benito\n- Nick Craswell\n- Lucas Meyer\n- Urszula Chajewska\n- Tanuja Bompada\n- Rogan Carr\n- Tom Finley\n- Muhammed Arrabi\n- Javier Salido\n- Steve Sweetman\n- Chris Jones\n- Joshua Allen\n- Levent Ozgur\n- Steve Lim\n- Scott Hoogerwerf\n- Jacquelyn Krones\n- Sam Sun\n- Ioana Marginas\n- Gregory Ellison\n- Allan Sepillo\n- Holly Stewart\n- Rob McCann\n- Christian Seifert\n- Nader Albussam\n- Jeffrey Schlimmer\n- Keith Haunreiter\n- Dean Carignan\n- Tom Mereckis\n- Ken Johnston\n- Rob Mauceri\n- Eric Horvitz\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 33.6044921875,
          "content": "# Changelog\nAll notable changes to this project will be documented in this file.\n\nThe format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),\nand the versioning is mostly derived from [Semantic Versioning](https://semver.org/spec/v2.0.0.html).\n\n## [v0.6.9] - 2025-01-06\n### Added\n- refitting of the intercept term after fitting the rest of the model to improve the intercept value\n- new options for handling missing values: \"low\", \"high\", \"separate\", and \"gain\"\n- use Fischer (1958) for handling categorical values. This is the same method employed by LightGBM.\n- added new parameters to control overfitting of nominal categoricals: gain\\_scale, min\\_cat\\_samples, cat\\_smooth\n### Changed\n- enable AVX-512 by default\n- modified default EBM parameters: outer\\_bags=16, n\\_jobs=-1\n### Fixed\n- fixed memory leak in the purification function\n\n## [v0.6.8] - 2024-12-09\n### Fixed\n- resolved new scikit-learn requirement for having \\_\\_sklearn\\_tags\\_\\_\n- changed position of ClassifierMixin and RegressorMixin inheritance to satisfy scikit-learn check\n- reliable handling of sparse arrays (previously only sparse matrices worked)\n\n## [v0.6.7] - 2024-11-27\n### Changed\n- minimum python version increased to 3.9\n- minimum numpy version increased to 1.25\n### Fixed\n- removed scipy dependency to resolve Issue #588\n\n## [v0.6.6] - 2024-11-20\n### Changed\n- added predict_with_uncertainty function by @degenfabian in PR #584\n- handle mono-classification in SHAP by @degenfabian in PR #582\n- improvements to tree building in C++\n### Fixed\n- issue that develop/debug options were not being honored in Windows when 1<n_jobs in joblib\n- fix several bugs in C++ from negative hessians or negative gain values caused by floating point noise\n\n## [v0.6.5] - 2024-10-23\n### Changed\n- default EBM parameters changed to improve model performance\n- switch to using exact versions of exp/log instead of the previously used approximate versions\n### Fixed\n- fix issue where very large feature values fail in the UI PR #581 by @degenfabian\n\n## [v0.6.4] - 2024-09-28\n### Added\n- support for regularization parameters reg_alpha, and reg_lambda in EBMs\n- support for the parameter max_delta_step in EBMs\n- improved fitting speed for most of the alternative objectives\n\n## [v0.6.3] - 2024-08-07\n### Added\n- visualizations for the APRL (Automatic Piecewise Linear Regression) package by @mathias-von-ottenbreit\n### Changed\n- early_stopping_tolerance default changed to 1e-5 to reduce EBMs fitting time slightly\n- shuffle initial feature order within each bag and during greedy boosting\n### Fixed\n- fixed numpy 2.0 issue in the Marginal class\n\n## [v0.6.2] - 2024-06-22\n### Added\n- pass optional kwargs to DecisionTreeClassifier in PR #537 by @busFred\n- support for multiclass purification\n- support for higher dimensional purification\n- allow higher levels of purification than would be supported via the tolerance parameter\n### Changed\n- numpy 2.0 support for EBMs\n- update documentation regarding monotonicity in PR #531 by @Krzys25\n- moved purification utility from \"interpret/glassbox/_ebm/_research\" to \"interpret.utils\"\n### Fixed\n- possible fix for issue #543 where merge_ebms was creating unexpected NaN values\n\n## [v0.6.1] - 2024-04-14\n### Fixed\n- added compatibility with numpy 2.0 thanks to @DerWeh in PR #525\n- fixed bug that was preventing SIMD from being used in python\n- removed approximate division in SIMD since the approximation was too inaccurate\n### Changed\n- EBM fitting time reduced\n\n## [v0.6.0] - 2024-03-16\n### Added\n- Documentation on recommended hyperparameters to help users optimize their models.\n- Support for monotone_constraints during model fitting, although post-processed monotonization is still suggested/preferred.\n- The EBMModel class now includes _more_tags for better integration with the scikit-learn API, thanks to contributions from @DerWeh.\n### Changed\n- Default max_rounds parameter increased from 5,000 to 25,000, for improved model accuracy.\n- Numerous code simplifications, additional tests, and enhancements for scikit-learn compatibility, thanks to @DerWeh.\n- The greedy boosting algorithm has been updated to support variable-length greedy sections, offering more flexibility during model training.\n- Full compatibility with Python 3.12.\n- Removal of the DecisionListClassifier from our documentation, as the skope-rules package seems to no longer be actively maintained.\n### Fixed\n- The sweep function now properly returns self, correcting an oversight identified by @alvanli.\n- Default exclude parameter set to None, aligning with scikit-learn's expected defaults, fixed by @DerWeh.\n- A potential bug when converting features from categorical to continuous values has been addressed.\n- Updated to handle the new return format for TreeShap in the SHAP 0.45.0 release.\n### Breaking Changes\n- replaced the greediness \\_\\_init\\_\\_ parameter with greedy_ratio and cyclic_progress parameters for better control of the boosting process\n  (see documentation for notes on greedy_ratio and cyclic_progress)\n- replaced breakpoint_iteration_ with best_iteration_, which now contains the number of boosting steps rather than the number of boosting rounds\n\n## [v0.5.1] - 2024-02-08\n### Added\n- Added new init parameter: interaction_smoothing_rounds\n- Added new init parameter: min_hessian\n- synthetic dataset generator (make_synthetic) for testing GAMs and for documentation\n### Changed\n- default parameters have been modified to improve the accuracy of EBMs\n- changed boosting internals to use LogitBoost to improve accuracy\n- changed interaction detection to use hessians to improve interaction selection\n- enabled smoothing_rounds by default to improve the smoothness of EBMs\n- added the ability to specify interactions via feature names or negative indexing\n- improved the speed of Morris sensitivity and partial dependence\n- python 3.12 support for core EBMs. Some of our optional dependencies do not yet support python 3.12 though\n- made early stopping more consistent and changed the early_stopping_tolerance to be a percentage\n### Fixed\n- avoid displaying a scroll bar by default in jupyter notebook cells\n- removed the dependency on deprecated distutils\n### Breaking Changes\n- changed the internal representation for classifiers that have just 1 class\n\n## [v0.5.0] - 2023-12-13\n### Added\n- added support for AVX-512 in PyPI installations to improve fitting speed\n- introduced an option to disable SIMD optimizations through the debug_mode function in python\n- exposed public utils.link_func and utils.inv_link functions\n### Changed\n- the interpret-core package now installs the dependencies required to build and predict EBMs\n  by default without needing to specify the [required] pip install flag\n- experimental/private support for OVR multiclass EBMs\n- added bagged_intercept_ attribute to store the intercepts for the bagged models\n### Fixed\n- resolved an issue in merge_ebms where the merge would fail if all EBMs in the \n  merge contained features with only one bin (issue #485)\n- resolved multiple future warnings from other packages\n### Breaking Changes\n- changed how monoclassification (degenerate classification with 1 class) is expressed\n- replaced predict_and_contrib function with simpler eval_terms function that returns \n  only the per-term contribution values. If you need both the contributions and predictions use:\n  interpret.utils.inv_link(ebm.eval_terms(X).sum(axis=1) + ebm.intercept_, ebm.link_)\n- separate to_json into to_jsonable (for python objects) and to_json (for files) functions\n- create a new link function string for multiclass that is separate from binary classification\n- for better scikit-learn compliance, removed the decision_function from the ExplainableBoostingRegressor\n\n## [v0.4.4] - 2023-08-26\n### Added\n- added the following model editing functions: copy, remove_terms, remove_features, sweep, scale\n- added experimental support for a JSON exporter function: to_json\n\n## [v0.4.3] - 2023-08-04\n### Changed\n- Training speed improvements due to the use of SIMD on Intel processors. \n  Results may vary, but expect approx 2.75x faster for classification and 1.3x faster for RMSE regression\n- Changed from using 64-bit floats to using 32-bit floats internally. Regression performed on datasets with large \n  targets that sum to greater than 3.4E+38 will overflow.\n### Fixed\n- Fixed an issue with the monotonize function that would occur when monotonizing a feature with missing values\n- Resolved issue where excluding the 1st feature would cause an exception\n\n## [v0.4.2] - 2023-05-31\n### Added\n- support for specifying outer bags\n### Changed\n- exceptions raised in the joblib child processes will be re-raised in the main process rather than be expressed as a TerminatedWorkerError\n- small additional improvements in memory compression\n- small improvements in maximizing the benefit of the privacy budget for Differentially Private EBMs\n### Fixed\n- fixed segfault that was occurring in the Anaconda build\n- fixed a bug that would prevent Differentially Private EBMs from using the exclude parameter\n\n## [v0.4.1] - 2023-05-16\n### Added\n- support for visualizations in streamlit\n### Fixed\n- fixed dangling pointer issue in call to CalcInteractionStrength\n\n## [v0.4.0] - 2023-05-11\n### Added\n- alternative objective functions: poisson_deviance, tweedie_deviance, gamma_deviance, pseudo_huber, rmse_log (log link)\n- greediness __init__ parameter that allows selecting a behavior between cyclic boosting and greedy boosting\n- smoothing_rounds __init__ parameter\n- added type hints to the EBM __init__ parameters and class attributes\n- init_score parameter to allow boosting and prediction on top of a previous model\n- multiclass support in merge_ebms\n- ability to monotonize features using post process model editing\n### Changed\n- default BaseLinear regressor is changed from Lasso to LinearRegression class\n- placed limits on the amount of memory used to find interactions with high cardinality categoricals\n### Fixed\n- validation_size of 0 is now handled by disabling early_stopping and using the final model\n### Breaking Changes\n- replaced the __init__ param \"mains\" with \"exclude\"\n- removed the binning __init__ param as this functionality was already fully supported in feature_types\n- removed the unused zero_val_count attribute and n_samples attribute\n- renamed the noise_scale_ attribute to noise_scale_boosting_ and added noise_scale_binning_ to DPEBMs\n\n## [v0.3.2] - 2023-03-14\n### Fixed\n- fix the issue that the shared library would only work on newer linux versions\n\n## [v0.3.1] - 2023-03-13\n### Added\n- Mac m1 support in conda-forge\n- SPOTGreedy prototype selection (PR #392)\n### Fixed\n- fix visualization when both cloud and non-cloud environments are detected (PR #210)\n- fix ShapTree bug where it was treating classifiers as regressors\n- resolve scikit-learn warnings occurring when models were trained using Pandas DataFrames\n- change the defaults to prefer 'continuous' over 'nominal' when a feature has 1 or 2 unique float64 values\n### Breaking Changes\n- in the blackbox and greybox explainers, change from accepting a predict_fn to \n  accepting either a model or a predict_fn\n- feature type 'categorical' has been renamed to 'nominal' for the remaining \n  feature_type parameters in the package (EBMs were already using 'nominal')\n- removed the unused sampler parameters to the Explainer classes\n\n## [v0.3.0] - 2022-11-16\n### Added\n- Full Complexity EBMs with higher order interactions supported: GA3M, GA4M, GA5M, etc... \n  3-way and higher-level interactions lose exact global interpretability, but retain exact local explanations\n  Higher level interactions need to be explicitly specified. No automatic FAST detection yet\n- Mac m1 support\n- support for ordinals\n- merge_ebms now supports merging models with interactions, including higher-level interactions\n- added classic composition option during Differentially Private binning\n- support for different kinds of feature importances (avg_weight, min_max)\n- exposed interaction detection API (FAST algorithm)\n- API to calculate and show the importances of groups of features and terms.\n### Changed\n- memory efficiency: About 20x less memory is required during fitting\n- predict time speed improvements. About 50x faster for Pandas CategoricalDType, \n  and varying levels of improvements for other data types\n- handling of the differential privacy DPOther bin, and non-DP unknowns has been unified by having a universal unknown bin\n- bin weights have been changed from per-feature to per-term and are now multi-dimensional\n- improved scikit-learn compliance: We now conform to the scikit-learn 1.0 feature names API by using \n  self.feature_names_in_ for the X column names and self.n_features_in_. \n  We use the matching self.feature_types_in_ for feature types, and self.term_names_ for the additive term names.\n### Fixed\n- merge_ebms now distributes bin weights proportionally according to volume when splitting bins\n- DP-EBMs now use sample weights instead of bin counts, which preserves privacy budget\n- improved scikit-learn compliance: The following __init__ attributes are no longer overwritten \n  during calls to fit: self.interactions, self.feature_names, self.feature_types\n- better handling of floating point overflows when calculating gain and validation metrics\n### Breaking Changes\n- EBMUtils.merge_models function has been renamed to merge_ebms\n- renamed binning type 'quantile_humanized' to 'rounded_quantile'\n- feature type 'categorical' has been specialized into separate 'nominal' and 'ordinal' types\n- EBM models have changed public attributes:\n  - ```\n    feature_groups_ -> term_features_\n    global_selector -> n_samples_, unique_val_counts_, and zero_val_counts_\n    domain_size_ -> min_target_, max_target_\n    additive_terms_ -> term_scores_\n    bagged_models_ -> BaseCoreEBM has been depricated and the only useful attribute has been moved \n                      into the main EBM class (bagged_models_.model_ -> bagged_scores_)\n    feature_importances_ -> has been changed into the function term_importances(), which can now also \n                            generate different types of importances\n    preprocessor_ & pair_preprocessor_ -> attributes have been moved into the main EBM model class (details below)\n    ```\n- EBMPreprocessor attributes have been moved to the main EBM model class\n  - ```\n    col_names_ -> feature_names_in_\n    col_types_ -> feature_types_in_\n    col_min_ -> feature_bounds_\n    col_max_ -> feature_bounds_\n    col_bin_edges_ -> bins_\n    col_mapping_ -> bins_\n    hist_counts_ -> histogram_counts_\n    hist_edges_ -> histogram_edges_\n    col_bin_counts_ -> bin_weights_ (and is now a per-term tensor)\n    ```\n\n## [v0.2.7] - 2021-09-23\n### Added\n- Synapse cloud support for visualizations.\n### Fixed\n- All category names in bar charts now visible for inline rendering (used in cloud environments).\n- Joblib preference was previously being overriden. This has been reverted to honor the user's preference.\n- Bug in categorical binning for differentially privatized EBMs has been fixed.\n\n## [v0.2.6] - 2021-07-20\n### Added\n- Differential-privacy augmented EBMs now available as `interpret.privacy.{DPExplainableBoostingClassifier,DPExplainableBoostingRegressor}`.\n- Packages `interpret` and `interpret-core` now distributed via docker.\n### Changed\n- Sampling code including stratification within EBM now performed in native code.\n### Fixed\n- Computer provider with `joblib` can now support multiple engines with serialization support.\n- Labels are now all shown for inline rendering of horizontal bar charts.\n- JS dependencies updated.\n\n## [v0.2.5] - 2021-06-21\n### Added\n- Sample weight support added for EBM.\n- Joint `predict_and_contrib` added to EBM where both predictions and feature contributions are generated in one call.\n- EBM predictions now substantially faster with categorical featured predictions.\n- Preliminary documentation for all of `interpret` now public at https://interpret.ml/docs.\n- Decision trees now work in cloud environments (InlineRenderer support).\n- Packages `interpret` and `interpret-core` now distributed via sdist.\n### Fixed\n- EBM uniform binning bug fixed where empty bins can raise exceptions.\n- Users can no longer include duplicate interaction terms for EBM.\n- CSS adjusted for inline rendering such that it does not interfere with its hosting environment.\n- JS dependencies updated.\n### Experimental\n- Ability to merge multiple EBM models into one. Found in `interpret.glassbox.ebm.utils`.\n\n## [v0.2.4] - 2021-01-19\n### Fixed\n- Bug fix on global EBM plots.\n- Rendering fix for AzureML notebooks.\n### Changed\n- JavaScript dependencies for inline renderers updated.\n\n## [v0.2.3] - 2021-01-13\n\n**Major** upgrades to EBM in this release. Automatic interaction detection is now\nincluded by default. This will increase accuracy substantially in most cases.\nNumerous optimizations to support this, especially around binary classification.\nExpect similar or slightly slower training times due to interactions.\n\n### Fixed\n- Automated interaction detection uses low-resolution binning\n  for both FAST and pairwise training.\n### Changed\n- EBM argument has been reduced from `outer_bags=16` to `outer_bags=8`.\n- EBM now includes interactions by default from `interactions=0` to `interactions=10`.\n- Algorithm `treeinterpreter` is now unstable due to upstream dependencies.\n- Automated interaction detection now operates from two-pass to one-pass.\n- Numeric approximations used in boosting (i.e. approx log / exp).\n- Some arguments have been re-ordered for EBM initialization.\n\n## [v0.2.2] - 2020-10-19\n### Fixed\n- Fixed bug on predicting unknown categories with EBM.\n- Fixed bug on max value being placed in its own bin for EBM pre-processing.\n- Numerous native fixes and optimizations.\n### Added\n- Added `max_interaction_bins` as argument to EBM learners for different sized\n  bins on interactions, separate to mains.\n- New binning method 'quantile_humanized' for EBM.\n### Changed\n- Interactions in EBM now use their own pre-processing, separate to mains.\n- Python 3.5 no longer supported.\n- Switched from Python to native code for binning.\n- Switched from Python to native code for PRNG in EBM.\n\n## [v0.2.1] - 2020-08-07\n### Added\n- Python 3.8 support.\n### Changed\n- Dash based visualizations will always default to listen port 7001 on first attempt;\n  if the first attempt fails it will try a random port between 7002-7999.\n### Experimental (WIP)\n- Further cloud environment support.\n- Improvements for multiclass EBM global graphs.\n\n## [v0.2.0] - 2020-07-21\n### Breaking Changes\n- With warning, EBM classifier adapts internal validation size\n  when there are too few instances relative to number of unique classes.\n  This ensures that there is at least one instance of each class in the validation set.\n- Cloud Jupyter environments now use a CDN to fix major rendering bugs and performance.\n  - CDN currently used is https://unpkg.com\n  - If you want to specify your own CDN, add the following as the top cell\n    ```python\n    from interpret import set_visualize_provider\n    from interpret.provider import InlineProvider\n    from interpret.version import __version__\n\n    # Change this to your custom CDN.\n    JS_URL = \"https://unpkg.com/@interpretml/interpret-inline@{}/dist/interpret-inline.js\".format(__version__)\n    set_visualize_provider(InlineProvider(js_url=JS_URL))\n    ```\n- EBM has changed initialization parameters:\n  - ```\n    schema -> DROPPED\n    n_estimators -> outer_bags\n    holdout_size -> validation_size\n    scoring -> DROPPED\n    holdout_split -> DROPPED\n    main_attr -> mains\n    data_n_episodes -> max_rounds\n    early_stopping_run_length -> early_stopping_rounds\n    feature_step_n_inner_bags -> inner_bags\n    training_step_epsiodes -> DROPPED\n    max_tree_splits -> max_leaves\n    min_cases_for_splits -> DROPPED\n    min_samples_leaf -> ADDED (Minimum number of samples that are in a leaf)\n    binning_strategy -> binning\n    max_n_bins -> max_bins\n    ```\n- EBM has changed public attributes:\n  - ```\n    n_estimators -> outer_bags\n    holdout_size -> validation_size\n    scoring -> DROPPED\n    holdout_split -> DROPPED\n    main_attr -> mains\n    data_n_episodes -> max_rounds\n    early_stopping_run_length -> early_stopping_rounds\n    feature_step_n_inner_bags -> inner_bags\n    training_step_epsiodes -> DROPPED\n    max_tree_splits -> max_leaves\n    min_cases_for_splits -> DROPPED\n    min_samples_leaf -> ADDED (Minimum number of samples that are in a leaf)\n    binning_strategy -> binning\n    max_n_bins -> max_bins\n\n    attribute_sets_ -> feature_groups_\n    attribute_set_models_ -> additive_terms_ (Pairs are now transposed)\n    model_errors_ -> term_standard_deviations_\n\n    main_episode_idxs_ -> breakpoint_iteration_[0]\n    inter_episode_idxs_ -> breakpoint_iteration_[1]\n\n    mean_abs_scores_ -> feature_importances_\n    ```\n### Fixed\n- Internal fixes and refactor for native code.\n- Updated dependencies for JavaScript layer.\n- Fixed rendering bugs and performance issues around cloud Jupyter notebooks.\n- Logging flushing bug fixed.\n- Labels that are shaped as nx1 matrices now automatically transform to vectors for training.\n### Experimental (WIP)\n- Added support for AzureML notebook VM.\n- Added local explanation visualizations for multiclass EBM.\n\n## [v0.1.22] - 2020-04-27\n### Upcoming Breaking Changes\n- EBM initialization arguments and public attributes will change in a near-future release.\n- There is a chance Explanation API will change in a near-future release.\n### Added\n- Docstrings for top-level API including for glassbox and blackbox.\n### Fixed\n- Minor fix for linear models where class wasn't propagating for logistic.\n### Experimental\n- For research use, exposed optional_temp_params for EBM's Python / native layer.\n\n## [v0.1.21] - 2020-04-02\n### Added\n- Module \"glassbox.ebm.research\" now has purification utilities.\n- EBM now exposes \"max_n_bins\" argument for its preprocessing stage.\n### Fixed\n- Fix intercept not showing for local EBM binary classification.\n- Stack trace information exposed for extension system failures.\n- Better handling of sparse to dense conversions for all explainers.\n- Internal fixes for native code.\n- Better NaN / infinity handling within EBM.\n### Changed\n- Binning strategy for EBM now defaulted to 'quantile' instead of 'uniform'.\n\n## [v0.1.20] - 2019-12-11\n### Fixed\n- **Major bug fix** around EBM interactions. If you use interactions, please upgrade immediately.\n  Part of the pairwise selection was not operating as expected and has been corrected.\n- Fix for handling dataframes when no named columns are specified.\n- Various EBM fixes around corner-case datasets.\n### Changed\n- All top-level methods relating to show's backing web server now use visualize provider directly.\n  In theory this shouldn't affect top-level API usage, but please raise an issue in the event of failure.\n- Memory footprint heavily reduced for EBM at around 2-3 times.\n\n## [v0.1.19] - 2019-10-25\n### Changed\n- Changed classification metric exposed between C++/python for EBMs to log loss for future public use.\n- Warnings provided when extensions error on load.\n### Fixed\n- Package joblib added to interpret-core as \"required\" extra.\n- Compiler fixes for Oracle Developer Studio.\n- Removed undefined behavior in EBM for several unlikely scenarios.\n\n## [v0.1.18] - 2019-10-09\n### Added\n- Added \"main_attr\" argument to EBM models. Can now select a subset of features to train main effects on.\n- Added AzureML notebook VM detection for visualizations (switches to inline).\n### Fixed\n- Missing values now correctly throw exceptions on explainers.\n- Major visualization fix for pairwise interaction heatmaps from EBM.\n- Corrected inline visualization height in Notebooks.\n### Changed\n- Various internal C++ fixes.\n- New error messages around EBM if the model isn't fitted before calling explain_*.\n\n## [v0.1.17] - 2019-09-24\n### Fixed\n- Morris sensitivity now works for both predict and predict_proba on scikit models.\n- Removal of debug print statements around blackbox explainers.\n### Changed\n- Dependencies for numpy/scipy/pandas/scikit-learn relaxed to (1.11.1,0.18.1,0.19.2, 0.18.1) respectively.\n- Visualization provider defaults set by environment detection (cloud and local use different providers).\n### Experimental (WIP)\n- Inline visualizations for show(explanation). This allows cloud notebooks, and offline notebook support.\n  Dashboard integration still ongoing.\n\n## [v0.1.16] - 2019-09-17\n### Added\n- Visualize and compute platforms are now refactored and use an extension system. Details on use upcoming in later release.\n- Package interpret is now a meta-package using interpret-core.\n  This enables partial installs via interpret-core for production environments.\n### Fixed\n- Updated SHAP dependency to require dill.\n### Experimental (WIP)\n- Greybox introduced (explainers that only work for specific types of models). Starting with SHAP tree and TreeInterpreter.\n- Extension system now works across all explainer types and providers.\n\n## [v0.1.15] - 2019-08-26\n### Experimental (WIP)\n- Multiclass EBM added. Includes visualization and postprocessing. Currently does not support multiclass pairs.\n\n## [v0.1.14] - 2019-08-20\n### Fixed\n- Fixed occasional browser crash relating to density graphs.\n- Fixed decision trees not displaying in Jupyter notebooks.\n### Changed\n- Dash components no longer pinned. Upgraded to latest.\n- Upgrade from dash-table-experiment to dash-table.\n- Numerous renames within native code.\n### Experimental (WIP)\n- Explanation data methods for PDP, EBM enabled for mli interop.\n\n## [v0.1.13] - 2019-08-14\n### Added\n- EBM has new parameter 'binning_strategy'. Can now support quantile based binning.\n- EBM now gracefully handles many edge cases around data.\n- Selenium support added for visual smoke tests.\n### Fixed\n- Method debug_mode now works in wider environments including WSL.\n- Linear models in last version returned the same graphs no matter the selection. Fixed.\n### Changed\n- Testing requirements now fully separate from default user install.\n- Internal EBM class has many renames associated with native codebase. Attribute has been changed to Feature.\n- Native codebase has many renames. Diff commits from v0.1.12 to v0.1.13 for more details.\n- Dependency gevent lightened to take 1.3.6 or greater. This affects cloud/older Python environments.\n- Installation for interpret package should now be 'pip install -U interpret'.\n- Removal of skope-rules as a required dependency. User now has to install it manually.\n- EBM parameter 'cont_n_bins' renamed to 'max_n_bins'.\n### Experimental (WIP)\n- Extensions validation method is hardened to ensure blackbox specs are closely met.\n- Explanation methods data and visual, require key of form ('mli', key), to access mli interop.\n\n## [v0.1.12] - 2019-08-09\n### Fixed\n- Fixed EBM bug where 2 features with 1 state are included in the dataset.\n- Fixed EBM bug that was causing processing of attributes past an attribute combination with 0 useful attributes to\n fail.\n\n## [v0.1.11] - 2019-08-09\n### Added\n- C++ testing framework added.\n- More granular options for training EBM (not public-facing, added for researchers)\n### Fixed\n- Improved POSIX compliance for build scripts.\n- Failure cases handled better for EBM in both Python/native layer.\n- Fixed a bug around dash relating to dependencies.\n- Removed dead code around web server for visualization.\n### Changed\n- For Python setup.py, requirements.txt now used for holding dependencies.\n- Directory structure changed for whole repository, in preparation for R support.\n- Native code further optimized with compiler flags.\n- Consistent scaling for EBM plots across all features.\n- For explanation's data method, behavior will be non-standard at key equals -1.\n- Testing suite for visual interface added via selenium.\n### Experimental (WIP)\n- Extension system for blackbox explainers added. Enables other packages to register into interpret.\n- Data standardization under way, currently for linear, LIME, SHAP where key equals -1 for data method.\n\n## [v0.1.10] - 2019-07-16\n### Fixed\n- Fix for duplicated logs.\n- EBM now throws exception for multi-class (not supported yet).\n- Added requests as dependency.\n### Changed\n- File requirements.txt renamed to dev-requirements.txt\n- Native libraries' names now start with 'lib_' prefix.\n- Adjusted return type for debug_mode method to provide logging handler.\n- EBM native layer upgraded asserts to use logging.\n- EBM native layer hardened for edge case data.\n- Adjustments to dev dependencies.\n- Method debug_mode defaults log level to INFO.\n\n## [v0.1.9] - 2019-06-14\n### Added\n- Added method debug_mode in develop module.\n- Connected native logging to Python layer.\n- Native libraries can now be in release/debug mode.\n### Fixed\n- Increased system compatibility for C++ code.\n### Changed\n- Debug related methods expose memory info in human readable form.\n- Clean-up of logging levels.\n- Various internal C+ fixes.\n\n## [v0.1.8] - 2019-06-07\n### Fixed\n- Fixed calibration issue with EBM.\n- Method show_link fix for anonymous explanation lists.\n### Changed\n- Method show_link now takes same arguments as show.\n- Better error messages with random port allocation.\n- More testing for various modules.\n- Various internal C+ fixes.\n\n## [v0.1.7] - 2019-06-03\n### Added\n- Added show_link method. Exposes the URL of show(explanation) as a returned string.\n### Fixed\n- Fixed shutdown_show_server, can now be called multiple times without failure.\n### Changed\n- Hardened status_show_server method.\n- Testing added for interactive module.\n- Removal of extra memory allocation in C++ code for EBM.\n- Various internal C++ fixes.\n\n## [v0.1.6] - 2019-05-31\n### Added\n- Integer indexing for preserve method.\n- Public-facing CI build added. Important for pull requests.\n### Changed\n- Visual-related imports are now loaded when visualize is called for explanations.\n\n## [v0.1.5] - 2019-05-30\n### Added\n- Added preserve method. Can now save visuals into notebook/file - does not work with decision trees.\n- Added status_show_server method. Acts as a check for server reachability.\n- Exposed init_show_server method. Can adjust address, base_url, etc.\n- Added print_debug_info method in develop module. Important for troubleshooting/bug-reports.\n### Fixed\n- Various internal C++ fixes.\n- Minor clean up on example notebooks.\n### Changed\n- Additional dependency required: psutil.\n- Test refactoring.\n\n## [v0.1.4] - 2019-05-23\n### Added\n- Root path for show server now has a light monitor page.\n- Logging registration can now print to both standard streams and files.\n### Fixed\n- Error handling for non-existent links fixed for visualization backend.\n- In some circumstances, Python process will hang. Resolved with new threading.\n### Changed\n- Unpinning scipy version, upstream dependencies now compatible with latest.\n- Show server is now run by a thread directly, not via executor pools.\n- Re-enabled notebook/show tests, new threading resolves hangs.\n- Small clean-up of setup.py and Azure pipelines config.\n\n## [v0.1.3] - 2019-05-21\n### Added\n- Model fit can now support lists of lists as instance data.\n- Model fit can now support lists for label data.\n### Fixed\n- Various internal C++ fixes.\n### Changed\n- Removed hypothesis as public test dependency.\n- C++ logging introduced (no public access).\n\n## [v0.1.2] - 2019-05-17\n### Added\n- EBM can now disable early stopping with run length set to -1.\n- EBM tracking of final episodes per base estimator.\n### Fixed\n- Pinning scipy, until upstream dependencies are compatible.\n### Changed\n- Clean-up of EBM logging for training.\n- Temporary disable of notebook/show tests until CI environment is fixed.\n\n## [v0.1.1] - 2019-05-16\n### Added\n- Added server shutdown call for 'show' method.\n### Fixed\n- Axis titles now included in performance explainer.\n- Fixed hang on testing interface.\n\n## [v0.1.0] - 2019-05-14\n### Added\n- Added port number assignments for 'show' method.\n- Includes codebase of v0.0.6.\n### Changed\n- Native code build scripts hardened.\n- Libraries are statically linked where possible.\n- Code now conforms to Python Black and its associated flake8.\n\n[v0.2.7]: https://github.com/interpretml/interpret/releases/tag/v0.2.7\n[v0.2.6]: https://github.com/interpretml/interpret/releases/tag/v0.2.6\n[v0.2.5]: https://github.com/interpretml/interpret/releases/tag/v0.2.5\n[v0.2.4]: https://github.com/interpretml/interpret/releases/tag/v0.2.4\n[v0.2.3]: https://github.com/interpretml/interpret/releases/tag/v0.2.3\n[v0.2.2]: https://github.com/interpretml/interpret/releases/tag/v0.2.2\n[v0.2.1]: https://github.com/interpretml/interpret/releases/tag/v0.2.1\n[v0.2.0]: https://github.com/interpretml/interpret/releases/tag/v0.2.0\n[v0.1.22]: https://github.com/interpretml/interpret/releases/tag/v0.1.22\n[v0.1.21]: https://github.com/interpretml/interpret/releases/tag/v0.1.21\n[v0.1.20]: https://github.com/interpretml/interpret/releases/tag/v0.1.20\n[v0.1.19]: https://github.com/interpretml/interpret/releases/tag/v0.1.19\n[v0.1.18]: https://github.com/interpretml/interpret/releases/tag/v0.1.18\n[v0.1.17]: https://github.com/interpretml/interpret/releases/tag/v0.1.17\n[v0.1.16]: https://github.com/interpretml/interpret/releases/tag/v0.1.16\n[v0.1.15]: https://github.com/interpretml/interpret/releases/tag/v0.1.15\n[v0.1.14]: https://github.com/interpretml/interpret/releases/tag/v0.1.14\n[v0.1.13]: https://github.com/interpretml/interpret/releases/tag/v0.1.13\n[v0.1.12]: https://github.com/interpretml/interpret/releases/tag/v0.1.12\n[v0.1.11]: https://github.com/interpretml/interpret/releases/tag/v0.1.11\n[v0.1.10]: https://github.com/interpretml/interpret/releases/tag/v0.1.10\n[v0.1.9]: https://github.com/interpretml/interpret/releases/tag/v0.1.9\n[v0.1.8]: https://github.com/interpretml/interpret/releases/tag/v0.1.8\n[v0.1.7]: https://github.com/interpretml/interpret/releases/tag/v0.1.7\n[v0.1.6]: https://github.com/interpretml/interpret/releases/tag/v0.1.6\n[v0.1.5]: https://github.com/interpretml/interpret/releases/tag/v0.1.5\n[v0.1.4]: https://github.com/interpretml/interpret/releases/tag/v0.1.4\n[v0.1.3]: https://github.com/interpretml/interpret/releases/tag/v0.1.3\n[v0.1.2]: https://github.com/interpretml/interpret/releases/tag/v0.1.2\n[v0.1.1]: https://github.com/interpretml/interpret/releases/tag/v0.1.1\n[v0.1.0]: https://github.com/interpretml/interpret/releases/tag/v0.1.0\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 0.77734375,
          "content": "# Contributing to InterpretML\n\nThis project welcomes contributions and suggestions.\n\n## Developer certificate of origin\nContributions require you to sign a _developer certificate of origin_ (DCO) declaring that you have the right to, and actually do, grant us the rights to use your contribution. For details, visit https://developercertificate.org/.\n\nWhen you submit a pull request, a DCO-bot will automatically determine whether you need to provide a DCO and decorate the PR appropriately (e.g., label, comment). Simply follow the instructions provided by the bot. You will only need to do this once across all repositories using our DCO.\n\n## Code of conduct\nThis project has adopted the [GitHub community guidelines](https://help.github.com/en/github/site-policy/github-community-guidelines).\n"
        },
        {
          "name": "GOVERNANCE.md",
          "type": "blob",
          "size": 3.1123046875,
          "content": "# Governance Policy\n\nThis document provides the minimum governance policy for Projects in the Organization. Maintainers agree to this policy and to abide by all of the Organization's polices, including the code of conduct, trademark policy, and antitrust policy by adding their name to the Maintainer's file.\n\n## 1.\tRoles.\n\nEach Project may include the following roles. Additional roles may be adopted and documented by the Project.\n\n**1.1.\tMaintainer**. “Maintainers” are responsible for organizing activities around developing, maintaining, and updating  the Project. Maintainers are also responsible for determining consensus. Each Project will designate one or more Maintainer. A Project may add or remove Maintainers with the approval of the current Maintainers (absent the maintainer being removed) or oversight of the Organization's Technical Steering Committee (\"TSC\").\n\n**1.2.\tContributors**. “Contributors” are those that have made Contributions to the Project.\n\n## 2.\tDecisions.\n\n**2.1.\tConsensus-Based Decision Making**. Projects make decisions through consensus of the Maintainers. While explicit agreement of all Maintainers is preferred, it is not required for consensus. Rather, the Maintainers will determine consensus based on their good faith consideration of a number of factors, including the dominant view of the Contributors and nature of support and objections. The Maintainers will document evidence of consensus in accordance with these requirements.\n\n**2.2.\tAppeal Process**. Decisions may be appealed by opening an issue and that appeal will be considered by the maintainers in good faith, who will respond in writing within a reasonable time. If the maintainers deny the appeal, the appeal my be brought before the TSC, who will also respond in writing in a reasonable time.\n\n## 3.\tHow We Work.\n\n**3.1.\tOpenness**. Participation shall be open to all persons who are directly and materially affected by the activity in question. There shall be no undue financial barriers to participation.\n\n**3.2.\tBalance.**  The development process should have a balance of interests. Contributors from diverse interest categories shall be sought with the objective of achieving balance.\n\n**3.3.\tCoordination and Harmonization.** Good faith efforts shall be made to resolve potential conflicts or incompatibility between releases in this Project.\n\n**3.4.\tConsideration of Views and Objections.** Prompt consideration shall be given to the written views and objections of all Contributors.\n\n**3.5.\tWritten procedures.** This governance document and other materials documenting this project's development process shall be available to any interested person.\n\n## 4. No Confidentiality.\n\nInformation disclosed in connection with any Project activity, including but not limited to meetings, Contributions, and submissions, is not confidential, regardless of any markings or statements to the contrary.\n\n## 5. Trademarks.\n\nAny names, trademarks, logos, or goodwill arising out of the Project - however owned - may be only used in accordance with the  Organization's Trademark Policy. Maintainer's obligations under this section survive their affiliation with the Project.\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0595703125,
          "content": "MIT License\n\nCopyright (c) 2023 The InterpretML Contributors\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "MAINTAINERS.md",
          "type": "blob",
          "size": 0.7060546875,
          "content": "# Maintainers\n\nThis document lists the Maintainers of Project InterpretML. Maintainers may be added once approved by consensus of the existing maintainers as described in the Governance document. By adding your name to this list you are agreeing to and to abide by the Project governance documents and to abide by all of the Organization's polices, including the code of conduct, trademark policy, and antitrust policy. If you are participating on behalf another organization (designated below), you represent that you have permission to bind that organization to these policies.\n\n| **NAME** | **Organization** |\n| --- | --- |\n| Paul Koch | Microsoft |\n| Rich Caruana | Microsoft |\n| Harsha Nori |  |\n| Samuel Jenkins |  |\n"
        },
        {
          "name": "R",
          "type": "tree",
          "content": null
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 58.384765625,
          "content": "# InterpretML\n\n<a href=\"https://githubtocolab.com/interpretml/interpret/blob/develop/docs/interpret/python/examples/interpretable-classification.ipynb\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/interpretml/interpret/develop?labpath=docs%2Finterpret%2Fpython%2Fexamples%2Finterpretable-classification.ipynb)\n![License](https://img.shields.io/github/license/interpretml/interpret.svg?style=flat-square)\n![Python Version](https://img.shields.io/pypi/pyversions/interpret.svg?style=flat-square)\n![Package Version](https://img.shields.io/pypi/v/interpret.svg?style=flat-square)\n![Conda](https://img.shields.io/conda/v/conda-forge/interpret)\n![Build Status](https://github.com/interpretml/interpret/actions/workflows/ci.yml/badge.svg?branch=develop)\n[![codecov](https://codecov.io/github/interpretml/interpret/branch/develop/graph/badge.svg?token=aPlXLsPEZD)](https://codecov.io/github/interpretml/interpret)\n![Maintenance](https://img.shields.io/maintenance/yes/2099?style=flat-square)\n<br/>\n> ### In the beginning machines learned in darkness, and data scientists struggled in the void to explain them. \n> ### Let there be light.\n\nInterpretML is an open-source package that incorporates state-of-the-art machine learning interpretability techniques under one roof. With this package, you can train interpretable glassbox models and explain blackbox systems. InterpretML helps you understand your model's global behavior, or understand the reasons behind individual predictions.\n\nInterpretability is essential for:\n- Model debugging - Why did my model make this mistake?\n- Feature Engineering - How can I improve my model?\n- Detecting fairness issues - Does my model discriminate?\n- Human-AI cooperation - How can I understand and trust the model's decisions?\n- Regulatory compliance - Does my model satisfy legal requirements?\n- High-risk applications - Healthcare, finance, judicial, ...\n\n![](https://github.com/interpretml/interpretml.github.io/blob/master/interpret-highlight.gif)\n\n# Installation\n\nPython 3.7+ | Linux, Mac, Windows\n```sh\npip install interpret\n# OR\nconda install -c conda-forge interpret\n```\n\n# Introducing the Explainable Boosting Machine (EBM)\n\nEBM is an interpretable model developed at Microsoft Research<sup>[*](#citations)</sup>. It uses modern machine learning techniques like bagging, gradient boosting, and automatic interaction detection to breathe new life into traditional GAMs (Generalized Additive Models). This makes EBMs as accurate as state-of-the-art techniques like random forests and gradient boosted trees. However, unlike these blackbox models, EBMs produce exact explanations and are editable by domain experts.\n\n| Dataset/AUROC | Domain  | Logistic Regression | Random Forest | XGBoost         | Explainable Boosting Machine |\n|---------------|---------|:-------------------:|:-------------:|:---------------:|:----------------------------:|\n| Adult Income  | Finance | .907±.003           | .903±.002     | .927±.001       | **_.928±.002_**              |\n| Heart Disease | Medical | .895±.030           | .890±.008     | .851±.018       | **_.898±.013_**              |\n| Breast Cancer | Medical | **_.995±.005_**     | .992±.009     | .992±.010       | **_.995±.006_**              |\n| Telecom Churn | Business| .849±.005           | .824±.004     | .828±.010       | **_.852±.006_**              |\n| Credit Fraud  | Security| .979±.002           | .950±.007     | **_.981±.003_** | **_.981±.003_**              |\n\n[*Notebook for reproducing table*](https://nbviewer.jupyter.org/github/interpretml/interpret/blob/develop/docs/benchmarks/ebm-classification-comparison.ipynb)\n\n# Supported Techniques\n\n| Interpretability Technique  | Type               |\n|-----------------------------|--------------------|\n| [Explainable Boosting](https://interpret.ml/docs/ebm.html)        | glassbox model     |\n| [APLR](https://interpret.ml/docs/aplr.html)                       | glassbox model     |\n| [Decision Tree](https://interpret.ml/docs/dt.html)                | glassbox model     |\n| [Decision Rule List](https://interpret.ml/docs/dr.html)           | glassbox model     |\n| [Linear/Logistic Regression](https://interpret.ml/docs/lr.html)   | glassbox model     |\n| [SHAP Kernel Explainer](https://interpret.ml/docs/shap.html)      | blackbox explainer |\n| [LIME](https://interpret.ml/docs/lime.html)                       | blackbox explainer |\n| [Morris Sensitivity Analysis](https://interpret.ml/docs/msa.html) | blackbox explainer |\n| [Partial Dependence](https://interpret.ml/docs/pdp.html)          | blackbox explainer |\n\n# Train a glassbox model\n\nLet's fit an Explainable Boosting Machine\n\n```python\nfrom interpret.glassbox import ExplainableBoostingClassifier\n\nebm = ExplainableBoostingClassifier()\nebm.fit(X_train, y_train)\n\n# or substitute with LogisticRegression, DecisionTreeClassifier, RuleListClassifier, ...\n# EBM supports pandas dataframes, numpy arrays, and handles \"string\" data natively.\n```\n\nUnderstand the model\n```python\nfrom interpret import show\n\nebm_global = ebm.explain_global()\nshow(ebm_global)\n```\n![Global Explanation Image](./docs/readme/ebm-global.png?raw=true)\n\n<br/>\n\nUnderstand individual predictions\n```python\nebm_local = ebm.explain_local(X_test, y_test)\nshow(ebm_local)\n```\n![Local Explanation Image](./docs/readme/ebm-local.png?raw=true)\n\n<br/>\n\nAnd if you have multiple model explanations, compare them\n```python\nshow([logistic_regression_global, decision_tree_global])\n```\n![Dashboard Image](./docs/readme/dashboard.png?raw=true)\n\n<br/>\n\nIf you need to keep your data private, use Differentially Private EBMs (see [DP-EBMs](https://proceedings.mlr.press/v139/nori21a/nori21a.pdf))\n\n```python\nfrom interpret.privacy import DPExplainableBoostingClassifier, DPExplainableBoostingRegressor\n\ndp_ebm = DPExplainableBoostingClassifier(epsilon=1, delta=1e-5) # Specify privacy parameters\ndp_ebm.fit(X_train, y_train)\n\nshow(dp_ebm.explain_global()) # Identical function calls to standard EBMs\n```\n\n<br/>\n<br/>\n\nFor more information, see the [documentation](https://interpret.ml/docs).\n\n<br/>\n\nEBMs include pairwise interactions by default. For 3-way interactions and higher see this notebook: https://interpret.ml/docs/python/examples/custom-interactions.html\n\n<br/>\n\nInterpret EBMs can be fit on datasets with 100 million samples in several hours. For larger workloads consider using distributed EBMs on Azure SynapseML: [classification EBMs](https://learn.microsoft.com/en-us/fabric/data-science/explainable-boosting-machines-classification) and [regression EBMs](https://learn.microsoft.com/en-us/fabric/data-science/explainable-boosting-machines-regression)\n\n<br/>\n<br/>\n\n# Acknowledgements\n\nInterpretML was originally created by (equal contributions): Samuel Jenkins, Harsha Nori, Paul Koch, and Rich Caruana\n\nEBMs are fast derivative of GA2M, invented by: Yin Lou, Rich Caruana, Johannes Gehrke, and Giles Hooker\n\nMany people have supported us along the way. Check out [ACKNOWLEDGEMENTS.md](./ACKNOWLEDGEMENTS.md)!\n\nWe also build on top of many great packages. Please check them out!\n\n[plotly](https://github.com/plotly/plotly.py) |\n[dash](https://github.com/plotly/dash) |\n[scikit-learn](https://github.com/scikit-learn/scikit-learn) |\n[lime](https://github.com/marcotcr/lime) |\n[shap](https://github.com/slundberg/shap) |\n[salib](https://github.com/SALib/SALib) |\n[skope-rules](https://github.com/scikit-learn-contrib/skope-rules) |\n[treeinterpreter](https://github.com/andosa/treeinterpreter) |\n[gevent](https://github.com/gevent/gevent) |\n[joblib](https://github.com/joblib/joblib) |\n[pytest](https://github.com/pytest-dev/pytest) |\n[jupyter](https://github.com/jupyter/notebook)\n\n# <a name=\"citations\">Citations</a>\n\n<details open>\n  <summary><strong>InterpretML</strong></summary>\n  <hr/>\n\n  <details open>\n    <summary>\n      <em>\"InterpretML: A Unified Framework for Machine Learning Interpretability\" (H. Nori, S. Jenkins, P. Koch, and R. Caruana 2019)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{nori2019interpretml,\n  title={InterpretML: A Unified Framework for Machine Learning Interpretability},\n  author={Nori, Harsha and Jenkins, Samuel and Koch, Paul and Caruana, Rich},\n  journal={arXiv preprint arXiv:1909.09223},\n  year={2019}\n}\n    </pre>\n    <a href=\"https://arxiv.org/pdf/1909.09223.pdf\">Paper link</a>\n  </details>\n\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>Explainable Boosting</strong></summary>\n  <hr/>\n\n  <details>\n    <summary>\n      <em>\"Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission\" (R. Caruana, Y. Lou, J. Gehrke, P. Koch, M. Sturm, and N. Elhadad 2015)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{caruana2015intelligible,\n  title={Intelligible models for healthcare: Predicting pneumonia risk and hospital 30-day readmission},\n  author={Caruana, Rich and Lou, Yin and Gehrke, Johannes and Koch, Paul and Sturm, Marc and Elhadad, Noemie},\n  booktitle={Proceedings of the 21th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},\n  pages={1721--1730},\n  year={2015},\n  organization={ACM}\n}\n    </pre>\n    <a href=\"https://www.microsoft.com/en-us/research/wp-content/uploads/2017/06/KDD2015FinalDraftIntelligibleModels4HealthCare_igt143e-caruanaA.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Accurate intelligible models with pairwise interactions\" (Y. Lou, R. Caruana, J. Gehrke, and G. Hooker 2013)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{lou2013accurate,\n  title={Accurate intelligible models with pairwise interactions},\n  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes and Hooker, Giles},\n  booktitle={Proceedings of the 19th ACM SIGKDD international conference on Knowledge discovery and data mining},\n  pages={623--631},\n  year={2013},\n  organization={ACM}\n}\n    </pre>\n    <a href=\"https://www.cs.cornell.edu/~yinlou/papers/lou-kdd13.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Intelligible models for classification and regression\" (Y. Lou, R. Caruana, and J. Gehrke 2012)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{lou2012intelligible,\n  title={Intelligible models for classification and regression},\n  author={Lou, Yin and Caruana, Rich and Gehrke, Johannes},\n  booktitle={Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining},\n  pages={150--158},\n  year={2012},\n  organization={ACM}\n}\n    </pre>\n    <a href=\"https://www.cs.cornell.edu/~yinlou/papers/lou-kdd12.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values\" (Zijie J. Wang, Alex Kale, Harsha Nori, Peter Stella, Mark E. Nunnally, Duen Horng Chau, Mihaela Vorvoreanu, Jennifer Wortman Vaughan, Rich Caruana 2022)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{wang2022interpretability,\n  title={Interpretability, Then What? Editing Machine Learning Models to Reflect Human Knowledge and Values},\n  author={Wang, Zijie J and Kale, Alex and Nori, Harsha and Stella, Peter and Nunnally, Mark E and Chau, Duen Horng and Vorvoreanu, Mihaela and Vaughan, Jennifer Wortman and Caruana, Rich},\n  journal={arXiv preprint arXiv:2206.15465},\n  year={2022}\n}\n    </pre>\n    <a href=\"https://arxiv.org/pdf/2206.15465.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Axiomatic Interpretability for Multiclass Additive Models\" (X. Zhang, S. Tan, P. Koch, Y. Lou, U. Chajewska, and R. Caruana 2019)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{zhang2019axiomatic,\n  title={Axiomatic Interpretability for Multiclass Additive Models},\n  author={Zhang, Xuezhou and Tan, Sarah and Koch, Paul and Lou, Yin and Chajewska, Urszula and Caruana, Rich},\n  booktitle={Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery \\& Data Mining},\n  pages={226--234},\n  year={2019},\n  organization={ACM}\n}\n    </pre>\n    <a href=\"https://arxiv.org/pdf/1810.09092.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Distill-and-compare: auditing black-box models using transparent model distillation\" (S. Tan, R. Caruana, G. Hooker, and Y. Lou 2018)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{tan2018distill,\n  title={Distill-and-compare: auditing black-box models using transparent model distillation},\n  author={Tan, Sarah and Caruana, Rich and Hooker, Giles and Lou, Yin},\n  booktitle={Proceedings of the 2018 AAAI/ACM Conference on AI, Ethics, and Society},\n  pages={303--310},\n  year={2018},\n  organization={ACM}\n}\n    </pre>\n    <a href=\"https://arxiv.org/pdf/1710.06169\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models\" (B. Lengerich, S. Tan, C. Chang, G. Hooker, R. Caruana 2019)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{lengerich2019purifying,\n  title={Purifying Interaction Effects with the Functional ANOVA: An Efficient Algorithm for Recovering Identifiable Additive Models},\n  author={Lengerich, Benjamin and Tan, Sarah and Chang, Chun-Hao and Hooker, Giles and Caruana, Rich},\n  journal={arXiv preprint arXiv:1911.04974},\n  year={2019}\n}\n    </pre>\n    <a href=\"https://arxiv.org/pdf/1911.04974.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning\" (H. Kaur, H. Nori, S. Jenkins, R. Caruana, H. Wallach, J. Wortman Vaughan 2020)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{kaur2020interpreting,\n  title={Interpreting Interpretability: Understanding Data Scientists' Use of Interpretability Tools for Machine Learning},\n  author={Kaur, Harmanpreet and Nori, Harsha and Jenkins, Samuel and Caruana, Rich and Wallach, Hanna and Wortman Vaughan, Jennifer},\n  booktitle={Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems},\n  pages={1--14},\n  year={2020}\n}\n    </pre>\n    <a href=\"https://www.microsoft.com/en-us/research/publication/interpreting-interpretability-understanding-data-scientists-use-of-interpretability-tools-for-machine-learning/\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"How Interpretable and Trustworthy are GAMs?\" (C. Chang, S. Tan, B. Lengerich, A. Goldenberg, R. Caruana 2020)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{chang2020interpretable,\n  title={How Interpretable and Trustworthy are GAMs?},\n  author={Chang, Chun-Hao and Tan, Sarah and Lengerich, Ben and Goldenberg, Anna and Caruana, Rich},\n  journal={arXiv preprint arXiv:2006.06466},\n  year={2020}\n}\n    </pre>\n    <a href=\"https://arxiv.org/pdf/2006.06466.pdf\">Paper link</a>\n  </details>\n\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>Differential Privacy</strong></summary>\n  <hr/>\n\n  <details>\n    <summary>\n      <em>\"Accuracy, Interpretability, and Differential Privacy via Explainable Boosting\" (H. Nori, R. Caruana, Z. Bu, J. Shen, J. Kulkarni 2021)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{pmlr-v139-nori21a,\n  title = \t {Accuracy, Interpretability, and Differential Privacy via Explainable Boosting},\n  author =       {Nori, Harsha and Caruana, Rich and Bu, Zhiqi and Shen, Judy Hanwen and Kulkarni, Janardhan},\n  booktitle = \t {Proceedings of the 38th International Conference on Machine Learning},\n  pages = \t {8227--8237},\n  year = \t {2021},\n  volume = \t {139},\n  series = \t {Proceedings of Machine Learning Research},\n  publisher =    {PMLR}\n}\n    </pre>\n    <a href=\"https://proceedings.mlr.press/v139/nori21a/nori21a.pdf\">Paper link</a>\n  </details>\n\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>LIME</strong></summary>\n  <hr/>\n\n  <details>\n    <summary>\n      <em>\"Why should i trust you?: Explaining the predictions of any classifier\" (M. T. Ribeiro, S. Singh, and C. Guestrin 2016)</em>\n    </summary>\n    <br/>\n    <pre>\n@inproceedings{ribeiro2016should,\n  title={Why should i trust you?: Explaining the predictions of any classifier},\n  author={Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},\n  booktitle={Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining},\n  pages={1135--1144},\n  year={2016},\n  organization={ACM}\n}\n    </pre>\n    <a href=\"https://arxiv.org/pdf/1602.04938.pdf\">Paper link</a>\n  </details>\n\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>SHAP</strong></summary>\n  <hr/>\n\n  <details>\n    <summary>\n      <em>\"A Unified Approach to Interpreting Model Predictions\" (S. M. Lundberg and S.-I. Lee 2017)</em>\n    </summary>\n    <br/>\n    <pre>\n@incollection{NIPS2017_7062,\n title = {A Unified Approach to Interpreting Model Predictions},\n author = {Lundberg, Scott M and Lee, Su-In},\n booktitle = {Advances in Neural Information Processing Systems 30},\n editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},\n pages = {4765--4774},\n year = {2017},\n publisher = {Curran Associates, Inc.},\n url = {https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf}\n}\n    </pre>\n    <a href=\"https://papers.nips.cc/paper/7062-a-unified-approach-to-interpreting-model-predictions.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Consistent individualized feature attribution for tree ensembles\" (Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In 2018)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{lundberg2018consistent,\n  title={Consistent individualized feature attribution for tree ensembles},\n  author={Lundberg, Scott M and Erion, Gabriel G and Lee, Su-In},\n  journal={arXiv preprint arXiv:1802.03888},\n  year={2018}\n}\n    </pre>\n    <a href=\"https://arxiv.org/pdf/1802.03888\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Explainable machine-learning predictions for the prevention of hypoxaemia during surgery\" (S. M. Lundberg et al. 2018)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{lundberg2018explainable,\n  title={Explainable machine-learning predictions for the prevention of hypoxaemia during surgery},\n  author={Lundberg, Scott M and Nair, Bala and Vavilala, Monica S and Horibe, Mayumi and Eisses, Michael J and Adams, Trevor and Liston, David E and Low, Daniel King-Wai and Newman, Shu-Fang and Kim, Jerry and others},\n  journal={Nature Biomedical Engineering},\n  volume={2},\n  number={10},\n  pages={749},\n  year={2018},\n  publisher={Nature Publishing Group}\n}\n    </pre>\n    <a href=\"https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6467492/pdf/nihms-1505578.pdf\">Paper link</a>\n  </details>\n\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>Sensitivity Analysis</strong></summary>\n  <hr/>\n\n  <details>\n    <summary>\n      <em>\"SALib: An open-source Python library for Sensitivity Analysis\" (J. D. Herman and W. Usher 2017)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{herman2017salib,\n  title={SALib: An open-source Python library for Sensitivity Analysis.},\n  author={Herman, Jonathan D and Usher, Will},\n  journal={J. Open Source Software},\n  volume={2},\n  number={9},\n  pages={97},\n  year={2017}\n}\n    </pre>\n    <a href=\"https://www.researchgate.net/profile/Will_Usher/publication/312204236_SALib_An_open-source_Python_library_for_Sensitivity_Analysis/links/5ac732d64585151e80a39547/SALib-An-open-source-Python-library-for-Sensitivity-Analysis.pdf?origin=publication_detail\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Factorial sampling plans for preliminary computational experiments\" (M. D. Morris 1991)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{morris1991factorial,\n  title={},\n  author={Morris, Max D},\n  journal={Technometrics},\n  volume={33},\n  number={2},\n  pages={161--174},\n  year={1991},\n  publisher={Taylor \\& Francis Group}\n}\n    </pre>\n    <a href=\"https://abe.ufl.edu/Faculty/jjones/ABE_5646/2010/Morris.1991%20SA%20paper.pdf\">Paper link</a>\n  </details>\n\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>Partial Dependence</strong></summary>\n  <hr/>\n\n  <details>\n    <summary>\n      <em>\"Greedy function approximation: a gradient boosting machine\" (J. H. Friedman 2001)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{friedman2001greedy,\n  title={Greedy function approximation: a gradient boosting machine},\n  author={Friedman, Jerome H},\n  journal={Annals of statistics},\n  pages={1189--1232},\n  year={2001},\n  publisher={JSTOR}\n}\n    </pre>\n    <a href=\"https://projecteuclid.org/download/pdf_1/euclid.aos/1013203451\">Paper link</a>\n  </details>\n\n  <hr/>\n</details>\n\n<details>\n  <summary><strong>Open Source Software</strong></summary>\n  <hr/>\n\n  <details>\n    <summary>\n      <em>\"Scikit-learn: Machine learning in Python\" (F. Pedregosa et al. 2011)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{pedregosa2011scikit,\n  title={Scikit-learn: Machine learning in Python},\n  author={Pedregosa, Fabian and Varoquaux, Ga{\\\"e}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and others},\n  journal={Journal of machine learning research},\n  volume={12},\n  number={Oct},\n  pages={2825--2830},\n  year={2011}\n}\n    </pre>\n    <a href=\"https://www.jmlr.org/papers/volume12/pedregosa11a/pedregosa11a.pdf\">Paper link</a>\n  </details>\n\n  <details>\n    <summary>\n      <em>\"Collaborative data science\" (Plotly Technologies Inc. 2015)</em>\n    </summary>\n    <br/>\n    <pre>\n@online{plotly, \n  author = {Plotly Technologies Inc.}, \n  title = {Collaborative data science}, \n  publisher = {Plotly Technologies Inc.}, \n  address = {Montreal, QC}, \n  year = {2015}, \n  url = {https://plot.ly}\n}\n    </pre>\n    <a href=\"https://plot.ly\">Link</a>\n  </details>\n  \n  <details>\n    <summary>\n      <em>\"Joblib: running python function as pipeline jobs\" (G. Varoquaux and O. Grisel 2009)</em>\n    </summary>\n    <br/>\n    <pre>\n@article{varoquaux2009joblib,\n  title={Joblib: running python function as pipeline jobs},\n  author={Varoquaux, Ga{\\\"e}l and Grisel, O},\n  journal={packages. python. org/joblib},\n  year={2009}\n}\n    </pre>\n    <a href=\"https://joblib.readthedocs.io/en/latest/\">Link</a>\n  </details>\n  \n  <hr/>\n</details>\n\n# Videos\n\n- [The Science Behind InterpretML: Explainable Boosting Machine](https://www.youtube.com/watch?v=MREiHgHgl0k)\n- [How to Explain Models with InterpretML Deep Dive](https://www.youtube.com/watch?v=WwBeKMQ0-I8)\n- [Black-Box and Glass-Box Explanation in Machine Learning](https://youtu.be/7uzNKY8pEhQ)\n- [Explainable AI explained!  By-design interpretable models with Microsofts InterpretML](https://www.youtube.com/watch?v=qPn9m30ojfc)\n- [Interpreting Machine Learning Models with InterpretML](https://www.youtube.com/watch?v=ERNuFfsknhk)\n- [Machine Learning Model Interpretability using AzureML & InterpretML (Explainable Boosting Machine)](https://www.youtube.com/watch?v=0ocVtXU8o1I)\n- [A Case Study of Using Explainable Boosting Machines](https://uncch.hosted.panopto.com/Panopto/Pages/Embed.aspx?id=063d6839-e8db-40e0-8df4-b0fc012e709b&start=0)\n- [From SHAP to EBM: Explain your Gradient Boosting Models in Python](https://www.youtube.com/watch?v=hnZjw77-1rE)\n\n# External links\n\n- [Machine Learning Interpretability in Banking: Why It Matters and How Explainable Boosting Machines Can Help](https://www.prometeia.com/en/trending-topics-article/machine-learning-interpretability-in-banking-why-it-matters-and-how-explainable-boosting-machines-can-help)\n- [Interpretable Machine Learning – Increase Trust and Eliminate Bias](https://ficonsulting.com/insight-post/interpretable-machine-learning-increase-trust-and-eliminate-bias/)\n- [Enhancing Trust in Credit Risk Models: A Comparative Analysis of EBMs and GBMs](https://2os.medium.com/enhancing-trust-in-credit-risk-models-a-comparative-analysis-of-ebms-and-gbms-25e02810300f)\n- [Explainable AI: unlocking value in FEC operations](https://analytiqal.nl/2024/01/22/fec-value-from-explainable-ai/)\n- [Interpretable or Accurate? Why Not Both?](https://towardsdatascience.com/interpretable-or-accurate-why-not-both-4d9c73512192)\n- [The Explainable Boosting Machine. As accurate as gradient boosting, as interpretable as linear regression.](https://towardsdatascience.com/the-explainable-boosting-machine-f24152509ebb)\n- [Exploring explainable boosting machines](https://leinadj.github.io/2023/04/09/Exploring-Explainable-Boosting-Machines.html)\n- [Performance And Explainability With EBM](https://blog.oakbits.com/ebm-algorithm.html)\n- [InterpretML: Another Way to Explain Your Model](https://towardsdatascience.com/interpretml-another-way-to-explain-your-model-b7faf0a384f8)\n- [A gentle introduction to GA2Ms, a white box model](https://www.fiddler.ai/blog/a-gentle-introduction-to-ga2ms-a-white-box-model)\n- [Model Interpretation with Microsoft’s Interpret ML](https://medium.com/@sand.mayur/model-interpretation-with-microsofts-interpret-ml-85aa0ad697ae)\n- [Explaining Model Pipelines With InterpretML](https://medium.com/@mariusvadeika/explaining-model-pipelines-with-interpretml-a9214f75400b)\n- [Explain Your Model with Microsoft’s InterpretML](https://medium.com/@Dataman.ai/explain-your-model-with-microsofts-interpretml-5daab1d693b4)\n- [On Model Explainability: From LIME, SHAP, to Explainable Boosting](https://everdark.github.io/k9/notebooks/ml/model_explain/model_explain.nb.html)\n- [Dealing with Imbalanced Data (Mortgage loans defaults)](https://mikewlange.github.io/ImbalancedData-/index.html)\n- [The right way to compute your Shapley Values](https://towardsdatascience.com/the-right-way-to-compute-your-shapley-values-cfea30509254)\n- [The Art of Sprezzatura for Machine Learning](https://towardsdatascience.com/the-art-of-sprezzatura-for-machine-learning-e2494c0db727)\n- [Mixing Art into the Science of Model Explainability](https://towardsdatascience.com/mixing-art-into-the-science-of-model-explainability-312b8216fa95)\n- [Automatic Piecewise Linear Regression](https://link.springer.com/article/10.1007/s00180-024-01475-4)\n- [MCTS EDA which makes sense](https://www.kaggle.com/code/ambrosm/mcts-eda-which-makes-sense/notebook)\n- [Explainable Boosting machines for Tabular data](https://www.kaggle.com/code/parulpandey/explainable-boosting-machines-for-tabular-data)\n\n# Papers that use or compare EBMs\n\n- [Challenging the Performance-Interpretability Trade-off: An Evaluation of Interpretable Machine Learning Models](https://arxiv.org/pdf/2409.14429)\n- [GAMFORMER: In-context Learning for Generalized Additive Models](https://arxiv.org/pdf/2410.04560v1)\n- [Glass Box Machine Learning and Corporate Bond Returns](https://download.ssrn.com/2024/12/7/5047456.pdf?response-content-disposition=inline&X-Amz-Security-Token=IQoJb3JpZ2luX2VjEJn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQC4Tahz9gLK1PnaT2OcGVvU95LIXVjM6MGGLiR8muLBHwIgDhBNcW1HFouaDPID%2FgMGtfAOIqCtr0JTXSsBYYGjXZEqvQUIYhAEGgwzMDg0NzUzMDEyNTciDBSJqXVnefWlpTvEvSqaBfTjYUxrqKGMDY1QImthn4LR848sVKt0vRNJSuhmBUct5KZ%2FYHehm4HVsRgxd%2FYezoCGxoo%2Bee1rhSCW7WVwdPrNzvAb34a410A6DkywgnsGsKhvMltoeYudsXrL2SlqY6fP5z8mmzELDBjHhNRbpjaPx%2BmHQiv8PrE6bqrQz%2Fe18Aj9JVsAlUAiJ0s9AiK5kaqJU6yZJJvaFh4AWHd8IsOj0QBU%2BHoTYj5ff2XzM9PtWUQbZccke066NcJEuUSE7fj5OREKYi%2BCXG6zNWu8y4BubfSzjL1pLRDuBB%2Fc6zQNWu%2Bz9sibZRzBvx1mmhTrL2ffhWCeTlCaSj1EsM3VuyNz3d8z2MFTpxn4hBqcR3lk0daL4qllASz3UMp%2FPwteZszzpE9moZwEwJWmR4TrH3KFF7x2bZaQPLvgt%2BqnstHyJXOgaEmXDX0yEyGQyDQh9RpD4n%2FlBTSHsXme0OfjEt5AscRnBqRpsX3ZG9Bx0cK5ibthJ5M%2FwsExm76cF8tPdyOCqozaF%2B9l1sJsWK1h4jHYrbdyZZZtKj786Ed2CAOZ0M%2FzY1hyHHAO47jOlNs9Ju2qLhsdXmcbFRiBfA2IN4UxClTyrAaYA2fQhpFHpRdM5CMk8%2Fe4h8Mt1PVO7K95BihB2P5O%2BCiWWzrlwAVKa5KKipxmRTDTjl2VIKFkwLeCwPHJFpQCL1ZknhGyIiD9hLpS%2BykAXnHyN4U7dN6rtOf8FGqDO0QUA2ZvTaT6DXrUps1Wf2iNqk%2Facl3RyB3nNwRw4igeUx9UysRRFsz2Rs93kZJq17yGiCLP38xF%2FniKj8doporrNbBjkMhBpAXKyqDpQ7JQ%2Bwu8GPAWdPiD%2FumnjvuuoG0UkmGX81izy5zCREKOraIuDzqOMjD6%2BIu7BjqxAYNSOCdUDYw30aHTCSDBoj%2B4LzQk2ZMdumvEyvNKv%2ButPmI1J6LsIwV8s1q7dgqpe2%2FA%2BLX4ZbWaT0OPR3idfz7ADSNZp6Ykfinfj6FSf673Kj%2FOm4etpiZxcMy8MDD65CHXCDy0%2FhMz9JAwmDyD2Dmb%2Fhvta226gohBnpuBA8KCoP946GdYEzyaXGvByrO4BOmvB%2BlRifauewFHndym7seWzQtJ%2B514aguu1JT%2F7XJVUw%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241218T174139Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAUPUUPRWE45GYNSPF%2F20241218%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=2c2b0fd799efafee1598fbc8ba7dd050451f67845f5cfa3b01a5b3ed0db7db1c&abstractId=5047456)\n- [Data Science with LLMs and Interpretable Models](https://arxiv.org/pdf/2402.14474v1.pdf)\n- [DimVis: Interpreting Visual Clusters in Dimensionality Reduction With Explainable Boosting Machine](https://arxiv.org/pdf/2402.06885.pdf)\n- [Distill knowledge of additive tree models into generalized linear models](https://detralytics.com/wp-content/uploads/2023/10/Detra-Note_Additive-tree-ensembles.pdf)\n- [Explainable Boosting Machines with Sparsity - Maintaining Explainability in High-Dimensional Settings](https://arxiv.org/abs/2311.07452)\n- [Cost of Explainability in AI: An Example with Credit Scoring Models](https://link.springer.com/chapter/10.1007/978-3-031-44064-9_26)\n- [Interpretable Machine Learning Leverages Proteomics to Improve Cardiovascular Disease Risk Prediction and Biomarker Identification](https://www.medrxiv.org/content/10.1101/2024.01.12.24301213v1.full.pdf)\n- [Interpretable Additive Tabular Transformer Networks](https://openreview.net/pdf/d2f0db2646418b24bb322fc1f4082fd9e65409c2.pdf)\n- [Signature Informed Sampling for Transcriptomic Data](https://www.biorxiv.org/content/biorxiv/early/2023/10/31/2023.10.26.564263.full.pdf)\n- [Interpretable Survival Analysis for Heart Failure Risk Prediction](https://arxiv.org/pdf/2310.15472.pdf)\n- [LLMs Understand Glass-Box Models, Discover Surprises, and Suggest Repairs](https://arxiv.org/pdf/2308.01157.pdf)\n- [Model Interpretability in Credit Insurance](http://hdl.handle.net/10400.5/27507)\n- [Federated Boosted Decision Trees with Differential Privacy](https://arxiv.org/pdf/2210.02910.pdf)\n- [Differentially private and explainable boosting machine with enhanced utility](https://www.sciencedirect.com/science/article/abs/pii/S0925231224011950?via%3Dihub#preview-section-abstract)\n- [Balancing Explainability and Privacy in Bank Failure Prediction: A Differentially Private Glass-Box Approach](https://ieeexplore.ieee.org/abstract/document/10818483)\n- [GAM(E) CHANGER OR NOT? AN EVALUATION OF INTERPRETABLE MACHINE LEARNING MODELS](https://arxiv.org/pdf/2204.09123.pdf)\n- [GAM Coach: Towards Interactive and User-centered Algorithmic Recourse](https://arxiv.org/pdf/2302.14165.pdf)\n- [Missing Values and Imputation in Healthcare Data: Can Interpretable Machine Learning Help?](https://arxiv.org/pdf/2304.11749v1.pdf)\n- [Practice and Challenges in Building a Universal Search Quality Metric](https://www.researchgate.net/profile/Nuo-Chen-38/publication/370126720_Practice_and_Challenges_in_Building_a_Universal_Search_Quality_Metric/links/6440a0f239aa471a524cb77d/Practice-and-Challenges-in-Building-a-Universal-Search-Quality-Metric.pdf?origin=publication_detail)\n- [Explaining Phishing Attacks: An XAI Approach to Enhance User Awareness and Trust](https://www.researchgate.net/profile/Giuseppe-Desolda/publication/370003878_Explaining_Phishing_Attacks_An_XAI_Approach_to_Enhance_User_Awareness_and_Trust/links/643922a8e881690c4bd50ced/Explaining-Phishing-Attacks-An-XAI-Approach-to-Enhance-User-Awareness-and-Trust.pdf)\n- [Revealing the Galaxy-Halo Connection Through Machine Learning](https://arxiv.org/pdf/2204.10332.pdf)\n- [How the Galaxy–Halo Connection Depends on Large-Scale Environment](https://arxiv.org/pdf/2402.07995.pdf)\n- [Explainable Artificial Intelligence for COVID-19 Diagnosis Through Blood Test Variables](https://link.springer.com/content/pdf/10.1007/s40313-021-00858-y.pdf)\n- [Using Explainable Boosting Machines (EBMs) to Detect Common Flaws in Data](https://link.springer.com/chapter/10.1007/978-3-030-93736-2_40)\n- [Differentially Private Gradient Boosting on Linear Learners for Tabular Data Analysis](https://assets.amazon.science/fa/3a/a62ba73f4bbda1d880b678c39193/differentially-private-gradient-boosting-on-linear-learners-for-tabular-data-analysis.pdf)\n- [Differentially private and explainable boosting machine with enhanced utility](https://www.sciencedirect.com/science/article/abs/pii/S0925231224011950)\n- [Concrete compressive strength prediction using an explainable boosting machine model](https://www.sciencedirect.com/science/article/pii/S2214509523000244/pdfft?md5=171c275b6bcae8897cef03d931e908e2&pid=1-s2.0-S2214509523000244-main.pdf)\n- [Proxy endpoints - bridging clinical trials and real world data](https://pdf.sciencedirectassets.com/272371/1-s2.0-S1532046424X00064/1-s2.0-S1532046424001412/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEIn%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIBYgAN6aOVrDnvQ1932tPndUyJ0Dm1nHdMVLiekPVduQAiAzbYe7W%2Bd6Dj8ee42ZeZnQxJwEjEjuGdiUEPx0a2G43SqyBQgSEAUaDDA1OTAwMzU0Njg2NSIMyMkCUNFeDTCUCppMKo8FiVShykb8phR%2F8aWUGE9gfnE5y7X3Jj1ZA2CVldH13T67s536bdTBhjIMF18rV0YP9iMi6B5aGr%2F286ovIJl332fxZ6iQNBIOPTm8kXQDUqvZbknYldiZqUPs69kuC%2FcKnJd1BWnv2SEZwbRuX94rWnRDPDaSoJx%2FVS6o4qsbFjp9%2BMYZr%2BvJzWHKrXAI4W%2Fh9%2BsIa0yvlac3IMWzAeD23HzDNmF0nqjJ6BSZzmDNW4HRIGBTrTUTO40TzQzhaOY7wyGA0Zv8SpWIULI%2FrY8z8EOX%2FU6OhqgyIMKv%2FSx3rUpMi5CrC1WcpnL97j%2FDAijNi4vMfG1b%2BBQIFRu2EmUky76k4w3FYxkCpYj4n4mk9H%2B%2Bc9C%2BdjKjUiayi%2FisIZUD7ISNhQ9oov0kXI1IVTCGKKQC9jqHOvdiA8YbVuMdEzy1Lkx%2B1kiEo79qvSlpTe2BtWAOm2Iequ01XoaMv%2FQb4ajhWKKSkTafzDAxc58aayP1YH49UzQ68Me7ecdHpx3JUHyYnxJGQ82wRpPkfZJA5wCmOUVI%2FBLuwFJyczG0LpALN5IpIqZz%2B8DvDR0xjRoN49dVwhrTSQ9BesvXbi2LKVm1ptacaaKqyx0PwLjQYKOd%2BPI3zCvRxEiM3IKSNFRLsUTyPNEE4E8pMFNxfyEX59yvTQrHwM62P7hvxHs%2BY6CxUGZTKBQwDAgxttJmiO%2BvjCRbTBXZg1WrQdXCkxntBXb15Mnqxo4lyPzUUkLdLAFK%2BLSwzBIcvSw2qG81Y8qhWmBgBT9vfAoSrjxsILFrB3nnz7u9XNNpRxb5Z9NuNG92%2Fpd%2F%2F5VespMY8Q0iwsNqazZ4M4H8UB34JgtrUEY27WrIsDWzLR%2FAYAxU%2BZHrFzCrsae5BjqyASqDBsNqjEkho%2FbuQDT%2F0vGx%2BgAqrksvVX0GrzNgvqnuPyvw6%2F%2B40ZJP5EA4axfltOYb2tNjd18Ngy2A3cd6J57v1G7wYyuSFIUfHGN5LA8BXK7p0x1mNcwN3pKHtAf260gjpsWMG7anvpK%2F3YupTz498C1lAmurJD%2BLN41lq05wBr403cchE41yzqAKHVKVpNq9s6oGHJmq0KJRvk%2FfjZr8oLhod5gtrwLKvLGqULf50L0%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20241105T092058Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYUKUJCDYI%2F20241105%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=15f8e40964e2c750ae15e43aa8e7f7c76eef6a76b792e41434d14bed42b31432&hash=d4a3e49b29443e5eea9e5a44c0dc11b3f30b21addbe6d6d20d523c68db23cd23&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S1532046424001412&tid=spdf-4fbabbd8-becb-4526-98d3-c7517914e457&sid=8ab2a095350fc74edc4b8765ecd8c0260edcgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f165f0b050207505b0151&rr=8ddbc55a0d60a380&cc=us)\n- [Machine Learning Model Reveals Determinators for Admission to Acute Mental Health Wards From Emergency Department Presentations](https://onlinelibrary.wiley.com/doi/epdf/10.1111/inm.13402)\n- [Towards Cleaner Cities: Estimating Vehicle-Induced PM2.5 with Hybrid EBM-CMA-ES Modeling](https://www.mdpi.com/2305-6304/12/11/827)\n- [Using machine learning to assist decision making in the assessment of mental health patients presenting to emergency departments](https://journals.sagepub.com/doi/full/10.1177/20552076241287364)\n- [Proposing an inherently interpretable machine learning model for shear strength prediction of reinforced concrete beams with stirrups](https://pdf.sciencedirectassets.com/287527/1-s2.0-S2214509523X00035/1-s2.0-S2214509524005011/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjECUaCXVzLWVhc3QtMSJGMEQCIB0r0KsYBZufOjbCVtUtozwn1QKMdLt2tbbfhuJKjWlXAiB5Dfr7p0yyj%2FSfypTLmjPL8WbjGAB3tRACFjyyqQbbfiq8BQiu%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMqBpZ2HmN91c%2BJPqpKpAFZtvqQjCScZa4FN%2FeubsPzOk5c%2B58LliO4Zr%2Bn1pm3vtW4I9I1vA29pkhT5was1N3ccPPIm2jNLwJ%2FHiZej7A2SmFv13Ro3sTvhqG%2F6A9Xx70Nx9jOlDPJUmCypKadKp0FGfuhZQuxeN0b%2F1QUUQZG4RpxC%2FXorRRHmb%2FrXcOWBwu4PmLZAkWmTKpncjDI7oj8eh8yBe6%2FA3JkJ14ZyBgR7JnPzR2ZqMdIhvlKoyMn6EnL1Azq2y3qwEMdzSCvz3wH3sT4pClc2vPs6ruQS4CdT3E7BHrf42Q0VnUXWjuy7gt9iRr0vaWR3tD%2FxyrrEKw7XuMHO9L4rQ4Pfn1dhGZ2J8H5ocwJGSh13U5fY6noyaTNViqvHx1oHNMWL03QpkJxmUxYquBWepcDjxEc32V6eGF7Ecm8Vij3s20wdRNcHqxGFKlUCgph48CKUA79iwSGQCkWQh7bq%2FTtowTbSPud7l8xeG1MvfIVy%2B6yzrjqygvPBQs3qkvdoWUrKXe57bhr2jEkKlSdYyp2TJMD6yoYRdTPyFx5xb0KgIt6KQTPmfbqYXkd3FFz3uc0HmWC5NQz6qP9UzNcBhcK8dXo3Dw042pl0HLO1njFaa%2BBfbT89VUVUIqjrAcmHweIl1v7Eyldzr%2BGBXIlsxPO3gPzyPLF2LTggc6dA%2Bswxmgmkv%2B7n5pU5%2F5sxvEhemb%2Fqu%2B8d47O%2Bn6RH8fL4eLGGL2d0dvFvyE7gEwt%2BaU9HsIN0IHqyH5VmaTF5zaKy%2Fn%2BhkF8yGpe5Hq5yNOUGrfQgfyFn4Kqd%2FTVajxIFzk8DEY%2F%2FFtyGJ%2B8BrHV4P%2FYs8R4XcBzPQtyrTuUC1CGmF01Tc2gnnEo4pVPaIjfBk9B%2BXVMc3Mu4Ywy4L%2BsgY6sgFK3hFIXjIfoVjqrIlBvsGYaFiZB1bVKBVy3DRiBgozzYmIVhipN%2FS%2BPok1oETqvYVvLqEVkGcb5W7nUIK16lFgjwDq6ePuxdqSafgOw5jVQroNsDCPRz8B%2F4fg7kv6gs4R9SX7gCaQ2V7L6NxqJDUUqsCMtIYq05Qx43dGByqLoVEz9USpRBmTLQwpGvOmUaGNNwTsCwmt5gRP8UX3CnkwI%2FydxmhrXLEdaUIFVwJbIor9&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20240604T221639Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY4E2DAHPF%2F20240604%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=eece32da8855b55208baecc0ce041e79aa03be1c292b58c67ce0215de36cbdb4&hash=46dd1da122f4cea242c6444a811fb16dde5cb8465e88552ac3eaeee97b975e9b&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S2214509524005011&tid=spdf-45c1c4d1-dd97-4c0d-a04f-c30843a79e78&sid=1fea53ed2d5cf1443e4a7c4-33f4bf6475e1gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0f155c5f060d565b01055d&rr=88eb49dd2a5f7688&cc=us)\n- [Using explainable machine learning and fitbit data to investigate predictors of adolescent obesity](https://www.nature.com/articles/s41598-024-60811-2)\n- [Interpretable Predictive Value of Including HDL-2b and HDL-3 in an Explainable Boosting Machine Model for Multiclass Classification of Coronary Artery Stenosis Severity in Acute Myocardial Infarction Patients](https://watermark.silverchair.com/ztae100.pdf?token=AQECAHi208BE49Ooan9kkhW_Ercy7Dm3ZL_9Cf3qfKAc485ysgAAA2owggNmBgkqhkiG9w0BBwagggNXMIIDUwIBADCCA0wGCSqGSIb3DQEHATAeBglghkgBZQMEAS4wEQQMnDqoUBnqG9Zyr0dAAgEQgIIDHT2M3owEzTRAV3KZzrOpzyqOYgClio-CQrzB5731fvsEe9ZWO_QfqQAKdaPyyOsEKjacd25hWs-_OvgXCqc36R4yFWu46PFOCApII2s3hbHYI1XEQozWfdyosgaQf_e7_5RIqIfwTEHt19LoYZuaDYjCqq2vmWOMZb6dNI6mz-h3Zd6BgbyYAFgRHiJfU94NU0Crf_AbbTx2jW3HqMBLYPn-ysUiyQYILNmqlKAAlw81ZjBwzusaQFsiJMCxwGyFHks7nwtnUQ8J5PU5Jelp8_fQ8x5_dlZvzvdkI9MR87zUkk4hm2XL0uyfvH92-7VV_2gMe-rU3aJZhbHJu2hENPDh_OmoDe7SOC-5EwPsgIDoDr_dgSgyhBMIbOk_TrSM4oEN6dbtvfLSDXQUWDV4semLuPjqz7WyiQz4PPt1mXuaf12X5xyVsf1Mms4UpGAKLyoCdJ-zDJ9csOPCefIsV2Bzs-KzaD63HWFLJuCU0hWIaK0QOcJATnpQb1PhFiAF6YZ_cCYTxkuAcrQyHS-WCEefNy8hB8PQXhNljtw0J499qdnLcNOM1gAQ3-o21KaTrEFs-DyvZwWmaGn8Zw1bK1CG8yVxWOh6_wjJpGjMMenstzrKFcLbJADs1yf3PuNGZds0g-Qf4NDcgsturcr0V1nLHVRFazWZhUKSeRnLjPzA5i3lVKnmwKjKa_50i0LMSIXNFS-dmvHs-qVUb8FO0_aKZ6egckXkoGG8w3Jox4MhhY2-B28Z0wbJOj8_DojCCtAmAPC0T5emRsuk1rkuRXIoMtFDWN0l7fr7RVkuy1TEd3mpa5UuU7Qo-wu_yqi6ibwLupjGeVN__7SeteoBSh8yFJgYN4BEiYmdkEX7DgKaMC90h5GakNJ7zeAPR9PFnQVRORoof04qMWK4aGod2igso1-qsCup-kVWmPy8zrQKlqxE4OCeqUpKQgZMUUAlFu643iuRnQuLnahXhui45TY8lS56XGCLqkwSG594lMoAXAYZ9tVFM4fAVwQJ3EWkJfHRRCWWGZfLwBPsdUnNEziGg4QIdrKhe-Fu7nLF)\n- [Estimate Deformation Capacity of Non-Ductile RC Shear Walls Using Explainable Boosting Machine](https://arxiv.org/pdf/2301.04652.pdf)\n- [Introducing the Rank-Biased Overlap as Similarity Measure for Feature Importance in Explainable Machine Learning: A Case Study on Parkinson’s Disease](https://www.researchgate.net/publication/362808061_Introducing_the_Rank-Biased_Overlap_as_Similarity_Measure_for_Feature_Importance_in_Explainable_Machine_Learning_A_Case_Study_on_Parkinson's_Disease)\n- [Targeting resources efficiently and justifiably by combining causal machine learning and theory](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9768181/pdf/frai-05-1015604.pdf)\n- [Extractive Text Summarization Using Generalized Additive Models with Interactions for Sentence Selection](https://arxiv.org/pdf/2212.10707.pdf)\n- [Death by Round Numbers: Glass-Box Machine Learning Uncovers Biases in Medical Practice](https://www.medrxiv.org/content/medrxiv/early/2022/11/28/2022.04.30.22274520.full.pdf)\n- [Post-Hoc Interpretation of Transformer Hyperparameters with Explainable Boosting Machines](https://www.cs.jhu.edu/~xzhan138/papers/BLACK2022.pdf)\n- [Interpretable machine learning for predicting pathologic complete response in patients treated with chemoradiation therapy for rectal adenocarcinoma](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9771385/pdf/frai-05-1059033.pdf)\n- [Exploring the Balance between Interpretability and Performance with carefully designed Constrainable Neural Additive Models](https://www.sciencedirect.com/science/article/pii/S1566253523001987)\n- [Estimating Discontinuous Time-Varying Risk Factors and Treatment Benefits for COVID-19 with Interpretable ML](https://arxiv.org/pdf/2211.08991.pdf)\n- [StratoMod: Predicting sequencing and variant calling errors with interpretable machine learning](https://www.biorxiv.org/content/10.1101/2023.01.20.524401v1.full.pdf)\n- [Interpretable machine learning algorithms to predict leaf senescence date of deciduous trees](https://pdf.sciencedirectassets.com/271723/1-s2.0-S0168192323X00112/1-s2.0-S0168192323003143/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOP%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJGMEQCIArPrCug2%2BpvA%2F87dfMYdbINsntWDDgNHeCOn72Yfad3AiBHzR9BvMkRvZrjQZ1DoY1YMkD6VsQw45zqo5ykkClnHSq8BQiL%2F%2F%2F%2F%2F%2F%2F%2F%2F%2F8BEAUaDDA1OTAwMzU0Njg2NSIMdV4IhgM83azwHKyjKpAFFMABPkhGjjH1i3y26weF5LN6ZuxfgcDlklmnpEZDoEntreay08vlEU7%2F3CLeNSqYgaq5txCiVztJDv2TBcxDUt0PP4faNrHUWIQdfDksvDs3EE7VEupaqhVjMNi0%2F%2ByLRw2OzjMPpz7H5sd3i4%2F%2FK2%2FJlpAWHlr4RFJ9BXMMPbLDEqhjIJIl5ZzaeLeeijXKrTtvJ6iYwTic%2FHJ23m7Fdnkh94HKkFTOWeglJzGT7FSc5Wnc7DgExrL7EBLvu9YVusMUf9rFYIU%2BKaVyxIa7WDUN48cWjwdGLjYV9XPy%2FP2lRKjeiiNMYbdknQzJfSzh0HWxx0Aq6zlXdkJUbvSgqFoDC2npaUGXjNupSLNIzcMFWr8lUvUFIBm1ZigETFDZrB4zEJFQVxXV%2Bsztpcs1tMO%2F8LAG3MNI%2BI%2Fp7lT3bj%2F%2BZg6S7d6ROGS96XMS3Am3WffiwNIxueTGrWmRWxS75EQexcJmrQ4ELU%2By3vOXxIvqftT68w6%2BnBryUB5kGE%2B6GljxUFD5y7hZFLM0tfFW9XEZF5PjDbz%2Fx%2Bi0dxEiwvN2mzNpSAWiiy6ZBT31GSRRMtTe9Sm4U%2B8DwSR0fymXmme5fKLGzkySq0xPuFhzN6LyLCoxtbob%2BRyLALNdP8E31enPu%2B1xl5Isg%2FXHINRM29SYzK0u1PlPK78ng%2Bqt4mUlLD7jlzIeBKa1vz%2BU8%2F1ZYvEofc8i6q691PqjYl%2FZK5lFQO1EEremVOv4i2nEYwmGtjtCAk1WFChnamFlEdWyJIerN5pKI4YvsGF%2FwXG8aHuYBg41CfGftl%2FwlJ77dPOQ8QHgp5BZFheyeYwEMijnbz4terE7kVpdvBKOk5lBxtiJILI0ftU%2F4F0k825M%2Ft4w%2FqzIpgY6sgFspzJ6vfwqmIKbmprTCY6NBr4uAZU%2FPUWraWxu3hCydMZTVOjlrab%2Bv5NSdCqWKHvK7Yn89JtE9um3P8Gyev9BFPXT6LykCtjNOulKUQnywvl8ngKdbujNjLAyZb4D0p4dFRFsE2sUTUWNvs%2BVwA%2BYdn4%2BwPkMN5PU0KR78myJ7LyYJGodNLOXcBSV%2FXa396TmeXagW3ihm2U7H%2FvXm1IZmOz%2FflT5y6CEy%2FegChXEVpb6&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230808T111525Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTY64PTFOFS%2F20230808%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=e35040e1985923b74081dbdac33f7250949695d95e631d68a8fe20684b3746bc&hash=59ce65176ba4b931ecc905ef2a0bb80561947d73205e8ad2561d63a95552a4fb&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0168192323003143&tid=spdf-41137a89-2992-4585-8512-4303f8dedb0c&sid=b0b6f2a791aeb640d1897e968c8092375869gxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=10145807525053555255&rr=7f375764f8ea2338&cc=us)\n- [Comparing Explainable Machine Learning Approaches With Traditional Statistical Methods for Evaluating Stroke Risk Models: Retrospective Cohort Study](https://cardio.jmir.org/2023/1/e47736/PDF)\n- [An Explainable AI Approach using Graph Learning to Predict ICU Length of Stay](https://shichangzh.github.io/preprints/LoS_XAI_ISR.pdf)\n- [Cross Feature Selection to Eliminate Spurious Interactions and Single Feature Dominance Explainable Boosting Machines](https://arxiv.org/ftp/arxiv/papers/2307/2307.08485.pdf)\n- [Multi-Objective Optimization of Performance and Interpretability of Tabular Supervised Machine Learning Models](https://arxiv.org/pdf/2307.08175v1.pdf)\n- [An explainable model to support the decision about the therapy protocol for AML](https://arxiv.org/pdf/2307.02631.pdf)\n- [Assessing wind field characteristics along the airport runway glide slope: an explainable boosting machine-assisted wind tunnel study](https://www.nature.com/articles/s41598-023-36495-5)\n- [Trustworthy Academic Risk Prediction with Explainable Boosting Machines](https://link.springer.com/chapter/10.1007/978-3-031-36272-9_38)\n- [Binary ECG Classification Using Explainable Boosting Machines for IoT Edge Devices](https://ieeexplore.ieee.org/document/9970834)\n- [Explainable artificial intelligence toward usable and trustworthy computer-aided diagnosis of multiple sclerosis from Optical Coherence Tomography](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10406231/)\n- [An Interpretable Machine Learning Model with Deep Learning-based Imaging Biomarkers for Diagnosis of Alzheimer’s Disease](https://arxiv.org/pdf/2308.07778.pdf)\n- [Comparing explainable machine learning approaches with traditional statistical methods for evaluating stroke risk models: retrospective cohort study](https://pureadmin.qub.ac.uk/ws/portalfiles/portal/495863198/JMIR_Cardio.pdf)\n- [Explainable Artificial Intelligence for Cotton Yield Prediction With Multisource Data](https://ieeexplore.ieee.org/document/10214067)\n- [Preoperative detection of extraprostatic tumor extension in patients with primary prostate cancer utilizing](https://insightsimaging.springeropen.com/articles/10.1186/s13244-024-01876-5)\n- [Monotone Tree-Based GAMI Models by Adapting XGBoost](https://arxiv.org/ftp/arxiv/papers/2309/2309.02426.pdf)\n- [Neural Graphical Models](https://arxiv.org/pdf/2210.00453.pdf)\n- [FAST: An Optimization Framework for Fast Additive Segmentation in Transparent ML](https://arxiv.org/pdf/2402.12630v1.pdf)\n- [The Quantitative Analysis of Explainable AI for Network Anomaly Detection](https://studenttheses.uu.nl/bitstream/handle/20.500.12932/45996/Thesis_SinievanderBen_6021794.pdf?sequence=1&isAllowed=y)\n- [Enhancing Predictive Battery Maintenance Through the Use of Explainable Boosting Machine](https://link.springer.com/chapter/10.1007/978-3-031-44146-2_6)\n- [Improved Differentially Private Regression via Gradient Boosting](https://arxiv.org/pdf/2303.03451.pdf)\n- [Explainable Artificial Intelligence in Job Recommendation Systems](http://essay.utwente.nl/96974/1/Tran_MA_EEMCS.pdf)\n- [Diagnosis uncertain models for medical risk prediction](https://arxiv.org/pdf/2306.17337.pdf)\n- [Extending Explainable Boosting Machines to Scientific Image Data](https://arxiv.org/pdf/2305.16526.pdf)\n- [Pest Presence Prediction Using Interpretable Machine Learning](https://arxiv.org/pdf/2205.07723.pdf)\n- [Key Thresholds and Relative Contributions of Knee Geometry, Anteroposterior Laxity, and Body Weight as Risk Factors for Noncontact ACL Injury](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC10184233/pdf/10.1177_23259671231163627.pdf)\n- [A clinical prediction model for 10-year risk of self-reported osteoporosis diagnosis in pre- and perimenopausal women](https://pubmed.ncbi.nlm.nih.gov/37273115/)\n- [epitope1D: Accurate Taxonomy-Aware B-Cell Linear Epitope Prediction](https://www.biorxiv.org/content/10.1101/2022.10.17.512613v1.full.pdf)\n- [Explainable Boosting Machines for Slope Failure Spatial Predictive Modeling](https://www.mdpi.com/2072-4292/13/24/4991/htm)\n- [Micromodels for Efficient, Explainable, and Reusable Systems: A Case Study on Mental Health](https://arxiv.org/pdf/2109.13770.pdf)\n- [Identifying main and interaction effects of risk factors to predict intensive care admission in patients hospitalized with COVID-19](https://www.medrxiv.org/content/10.1101/2020.06.30.20143651v1.full.pdf)\n- [Leveraging interpretable machine learning in intensive care](https://link.springer.com/article/10.1007/s10479-024-06226-8#Tab10)\n- [Development of prediction models for one-year brain tumour survival using machine learning: a comparison of accuracy and interpretability](https://www.pure.ed.ac.uk/ws/portalfiles/portal/343114800/1_s2.0_S0169260723001487_main.pdf)\n- [Using Interpretable Machine Learning to Predict Maternal and Fetal Outcomes](https://arxiv.org/pdf/2207.05322.pdf)\n- [Calibrate: Interactive Analysis of Probabilistic Model Output](https://arxiv.org/pdf/2207.13770.pdf)\n- [Neural Additive Models: Interpretable Machine Learning with Neural Nets](https://arxiv.org/pdf/2004.13912.pdf)\n- [TabSRA: An Attention based Self-Explainable Model for Tabular Learning](https://www.esann.org/sites/default/files/proceedings/2023/ES2023-37.pdf)\n- [Evaluating the Efficacy of Instance Incremental vs. Batch Learning in Delayed Label Environments: An Empirical Study on Tabular Data Streaming for Fraud Detection](https://arxiv.org/pdf/2409.10111v1)\n- [Improving Neural Additive Models with Bayesian Principles](https://arxiv.org/pdf/2305.16905.pdf)\n- [NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning](https://arxiv.org/pdf/2106.01613.pdf)\n- [Scalable Interpretability via Polynomials](https://arxiv.org/pdf/2205.14108v1.pdf)\n- [Neural Basis Models for Interpretability](https://arxiv.org/pdf/2205.14120.pdf)\n- [ILMART: Interpretable Ranking with Constrained LambdaMART](https://arxiv.org/pdf/2206.00473.pdf)\n- [Integrating Co-Clustering and Interpretable Machine Learning for the Prediction of Intravenous Immunoglobulin Resistance in Kawasaki Disease](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=9097874)\n- [Distilling Reinforcement Learning Policies for Interpretable Robot Locomotion: Gradient Boosting Machines and Symbolic Regression](https://arxiv.org/pdf/2403.14328)\n- [Proxy Endpoints - Bridging clinical trials and real world data](https://deliverypdf.ssrn.com/delivery.php?ID=100104064008112111075114086019087126028049030043069035029115016108019006060084089121082084037060084007106031067094003062092094027085086025068093071031052079088007024075059029108100000124020112107075035009017105116086086122095064020024067066064103085015070113092118127102118080007103101&EXT=pdf&INDEX=TRUE)\n- [Application of boosted trees to the prognosis prediction of COVID‐19](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC11111612/pdf/HSR2-7-e2104.pdf)\n- [Explainable Gradient Boosting for Corporate Crisis Forecasting in Italian Businesses](https://assets-eu.researchsquare.com/files/rs-4426436/v1_covered_0583163e-fa83-4b34-9a7e-eae573b17bd8.pdf?c=1715832940)\n- [Revisiting differentially private XGBoost: Are random decision trees really better than greedy ones?](https://openreview.net/pdf?id=bCynxWndWY)\n- [Investigating Trust in Human-Machine Learning Collaboration: A Pilot Study on Estimating Public Anxiety from Speech](https://dl.acm.org/doi/pdf/10.1145/3462244.3479926)\n- [pureGAM: Learning an Inherently Pure Additive Model](https://www.microsoft.com/en-us/research/uploads/prod/2022/07/pureGAM-camera-ready.pdf)\n- [GAMI-Net: An Explainable Neural Network based on Generalized Additive Models with Structured Interactions](https://arxiv.org/pdf/2003.07132v1.pdf)\n- [Interpretable Machine Learning based on Functional ANOVA Framework: Algorithms and Comparisons](https://arxiv.org/ftp/arxiv/papers/2305/2305.15670.pdf)\n- [Using Model-Based Trees with Boosting to Fit Low-Order Functional ANOVA Models](https://arxiv.org/ftp/arxiv/papers/2207/2207.06950.pdf)\n- [Interpretable generalized additive neural networks](https://pdf.sciencedirectassets.com/271700/AIP/1-s2.0-S0377221723005027/main.pdf?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEOX%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLWVhc3QtMSJHMEUCIQDYA80LSoQY%2FmGTGsi8cQ2BzHoFU7410ljuYQwqt9ht0gIgBg4NSEN4e5jKUouf04uZCPIMh8NHH22jrY3opOG%2Fa1wqsgUIXhAFGgwwNTkwMDM1NDY4NjUiDFuCZlUMsD25uY5h3CqPBW0KcZyo1I0j19n0O26WHoCoxeimG0I7m02rUpQug4EiDYFVkx%2FRqfC4eL2Y0z7iO%2B95NIQ9UrOd3zWWZZPGpKCgHpU1GA4JwHSKNJDi8G2q%2FGm18%2Fl8B9jN4Lq3klUfU3HcjJh%2B4O1aZTJb3PmqDxKn%2BFQIftfS13xNcyqGnGBlw3yaSp3ZXoV55tKSX6b%2Fp5ZuXWORWiC2JlANxa0exR%2FkBeE75gfILdU8bH2TJ1wozoB0yTZwDAl1%2Bc4exGhVdhZRpvr9W6q%2BTG4tx6qhglAwv1uUQN8Zt1z8GEFHMTrtSv5pNJIpLqqMxp62UeufPMesYyoO5RfKjRS96PxYs1S%2FC5zfz0V63kkFtmSVn4IzVQ%2B9tLq%2FEWQ3BvTs8B0cH%2FOm6W8wn4nGk3HywJiUWvGexXahMqDW9o2pq7CWOSoFCKjjkOyxBXAzP0OX5LeCCgOF11BbhNcDSiIqlWQhqsk0738appUu99Yh12XmMWyu6YXAv1jvgrpaliMRkliAu9by418e6%2FBBA%2B%2BfcDQC3VkEv3NpSQklitMaIT2Y624jhM09ntjdC4IcONNRVE3Q5sHIh6DZsBHrPj9oKqpu9nPKnDBrKoAFdnQ%2BkLQ%2B8JAXyCHwd3YBUXQStlYTpUExESOnFFJ36HGJ%2FbkkFC5Ac9W%2BALq%2FkBYIvtPFNBWIGUSC%2BUgSH0kC%2BJqoyYUNHjfYZ3fxCDwI%2BAugNT3UtXtT%2BrnCKlH3f68ZAyOdkFLiHRQevc2%2FRBXJ5gAqCgZFDUVM%2BVjgB%2BInE458PRMxLuRwFHJarOqZhoDvC68ar2q3YDPyqmyUZxaiVrqn2xlJGdh0lcTVwNourzqlY2l4v87nYm7ncxJO%2FiiBQArtSRTOWmkw6JigpQY6sQEhgDdw23Gwj9rSPFlCHfUzj%2B%2BfdgeX3LZpuPITkl6%2BYwjKw0wXpR4c0Rj0IsCH1EAxJcxSLXhSSHgInlZR41EreEAByudeYNtxD0iAQcR3L4RlqTVuI6V3IIcxNltdg5rDAJwUqsGqhMrZOH0uJqQXvLJwxgfkOkckdjdnrfT%2FmOh%2BNjLCR4KTvwTJIC2YAmNHQco2TLKbC27i118DSoKwrYUvb%2BkCTwD3TMkxIf%2BrW5s%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20230707T133754Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAQ3PHCVTYTYKBXRE7%2F20230707%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=389ecd144af85f4eae42ab9684f9d56696191a9d8d33c44386ee6af520187724&hash=e798cbd4d80d01d56a2a1ea75a3947b027daecaea5f6e6674a1dc2dbea97dab3&host=68042c943591013ac2b2430a89b270f6af2c76d8dfd086a07176afe7c76c2c61&pii=S0377221723005027&tid=spdf-79e45837-627e-4a9f-88f5-7359ecb4ca63&sid=7e54333b754ff04056483e557e54be0269ddgxrqa&type=client&tsoh=d3d3LnNjaWVuY2VkaXJlY3QuY29t&ua=0b1a5101565e5607565a&rr=7e307c188e87e7cb&cc=mx)\n- [A Concept and Argumentation based Interpretable Model in High Risk Domains](https://arxiv.org/pdf/2208.08149.pdf)\n- [Analyzing the Differences between Professional and Amateur Esports through Win Probability](https://dl.acm.org/doi/pdf/10.1145/3485447.3512277)\n- [Explainable machine learning with pairwise interactions for the classification of Parkinson’s disease and SWEDD from clinical and imaging features](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9132761/pdf/11682_2022_Article_688.pdf)\n- [Interpretable Prediction of Goals in Soccer](https://statsbomb.com/wp-content/uploads/2019/10/decroos-interpretability-statsbomb.pdf)\n- [Extending the Tsetlin Machine with Integer-Weighted Clauses for Increased Interpretability](https://arxiv.org/pdf/2005.05131.pdf)\n- [In Pursuit of Interpretable, Fair and Accurate Machine Learning for Criminal Recidivism Prediction](https://arxiv.org/pdf/2005.04176.pdf)\n- [From Shapley Values to Generalized Additive Models and back](https://arxiv.org/pdf/2209.04012.pdf)\n- [Developing A Visual-Interactive Interface for Electronic Health Record Labeling](https://arxiv.org/pdf/2209.12778.pdf)\n- [Development and Validation of an Interpretable 3-day Intensive Care Unit Readmission Prediction Model Using Explainable Boosting Machines](https://www.medrxiv.org/content/10.1101/2021.11.01.21265700v1.full.pdf)\n- [Death by Round Numbers and Sharp Thresholds: How to Avoid Dangerous AI EHR Recommendations](https://www.medrxiv.org/content/10.1101/2022.04.30.22274520v1.full.pdf)\n- [Building a predictive model to identify clinical indicators for COVID-19 using machine learning method](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC9037972/pdf/11517_2022_Article_2568.pdf)\n- [Using Innovative Machine Learning Methods to Screen and Identify Predictors of Congenital Heart Diseases](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8777022/pdf/fcvm-08-797002.pdf)\n- [Explainable Boosting Machine for Predicting Alzheimer’s Disease from MRI Hippocampal Subfields](https://link.springer.com/chapter/10.1007/978-3-030-86993-9_31)\n- [Impact of Accuracy on Model Interpretations](https://arxiv.org/pdf/2011.09903.pdf)\n- [Machine Learning Algorithms for Identifying Dependencies in OT Protocols](https://www.mdpi.com/1996-1073/16/10/4056)\n- [Causal Understanding of Why Users Share Hate Speech on Social Media](https://arxiv.org/pdf/2310.15772.pdf)\n- [Explainable Boosting Machine: A Contemporary Glass-Box Model to Analyze Work Zone-Related Road Traffic Crashes](https://www.mdpi.com/2313-576X/9/4/83)\n- [Efficient and Interpretable Traffic Destination Prediction using Explainable Boosting Machines](https://arxiv.org/pdf/2402.03457.pdf)\n- [Explainable Artificial Intelligence Paves the Way in Precision Diagnostics and Biomarker Discovery for the Subclass of Diabetic Retinopathy in Type 2 Diabetics](https://www.mdpi.com/2218-1989/13/12/1204)\n- [A proposed tree-based explainable artificial intelligence approach for the prediction of angina pectoris](https://www.nature.com/articles/s41598-023-49673-2)\n- [Explainable Boosting Machine: A Contemporary Glass-Box Strategy for the Assessment of Wind Shear Severity in the Runway Vicinity Based on the Doppler Light Detection and Ranging Data](https://www.mdpi.com/2073-4433/15/1/20)\n- [On the Physical Nature of Lya Transmission Spikes in High Redshift Quasar Spectra](https://arxiv.org/pdf/2401.04762.pdf)\n- [GRAND-SLAMIN’ Interpretable Additive Modeling with Structural Constraints](https://openreview.net/pdf?id=F5DYsAc7Rt)\n- [Identification of groundwater potential zones in data-scarce mountainous region using explainable machine learning](https://www.sciencedirect.com/science/article/pii/S0022169423013598)\n- [Explainable Classification Techniques for Quantum Dot Device Measurements](https://arxiv.org/pdf/2402.13699v1.pdf)\n\n# Books that cover EBMs\n\n- [Machine Learning for High-Risk Applications](https://www.oreilly.com/library/view/machine-learning-for/9781098102425/)\n- [Interpretable Machine Learning with Python](https://www.amazon.com/Interpretable-Machine-Learning-Python-hands-dp-180323542X/dp/180323542X/)\n- [Explainable Artificial Intelligence: An Introduction to Interpretable Machine Learning](https://www.amazon.com/Explainable-Artificial-Intelligence_-An-Introduction-to-Interpretable-XAI/dp/3030833550)\n- [Applied Machine Learning Explainability Techniques](https://www.amazon.com/Applied-Machine-Learning-Explainability-Techniques/dp/1803246154)\n- [The eXplainable A.I.: With Python examples](https://www.amazon.com/eXplainable-I-Python-examples-ebook/dp/B0B4F98MN6)\n- [Platform and Model Design for Responsible AI: Design and build resilient, private, fair, and transparent machine learning models](https://www.amazon.com/Platform-Model-Design-Responsible-transparent/dp/1803237074)\n- [Explainable AI Recipes](https://www.amazon.com/Explainable-Recipes-Implement-Explainability-Interpretability-ebook/dp/B0BSF5NBY7)\n- [Ensemble Methods for Machine Learning](https://www.amazon.com/Ensemble-Methods-Machine-Learning-Kunapuli/dp/1617297135)\n\n# External tools\n\n- [EBM to Onnx converter by SoftAtHome](https://github.com/interpretml/ebm2onnx)\n- [EBM to SQL converter - ML 2 SQL](https://github.com/kaspersgit/ml_2_sql)\n- [EBM to PMML converter - SkLearn2PMML](https://github.com/jpmml/sklearn2pmml)\n- [EBM visual editor - GAM Changer](https://github.com/interpretml/gam-changer)\n- [Interpreting Visual Clusters in Dimensionality Reduction - DimVis](https://github.com/parisa-salmanian/DimVis)\n\n# Contact us\n\nThere are multiple ways to get in touch:\n- Email us at interpret@microsoft.com\n- Or, feel free to raise a GitHub issue\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n<br/>\n<br/>\n<br/>\n<br/>\n<br/>\n\n> ### If a tree fell in your random forest, would anyone notice?\n"
        },
        {
          "name": "build.bat",
          "type": "blob",
          "size": 2.1142578125,
          "content": "@ECHO OFF\nSETLOCAL\n\nREM The free version of Visual Studio (Community) is sufficient for compiling InterpretML for Windows.\nREM Visual Studio Community can be downloaded for free here:  https://visualstudio.microsoft.com/vs/\n\nREM If running in a non-Visual Studio window, the required environment variables can be obtained with:\nREM \"C:\\Program Files\\Microsoft Visual Studio\\2022\\Enterprise\\VC\\Auxiliary\\Build\\vcvars64.bat\"\n\nSET root_path=%~dp0\n\nSET bld_default=1\n\nSET release_64=0\nSET debug_64=0\nSET release_32=0\nSET debug_32=0\n\nSET \"extra_analysis= \"\nfor %%x in (%*) do (\n   IF \"%%x\"==\"-release_64\" (\n      SET release_64=1\n      SET bld_default=0\n   )\n   IF \"%%x\"==\"-debug_64\" (\n      SET debug_64=1\n      SET bld_default=0\n   )\n   IF \"%%x\"==\"-release_32\" (\n      SET release_32=1\n      SET bld_default=0\n   )\n   IF \"%%x\"==\"-debug_32\" (\n      SET debug_32=1\n      SET bld_default=0\n   )\n\n   IF \"%%x\"==\"-analysis\" (\n      SET extra_analysis=/p:EnableClangTidyCodeAnalysis=True /p:RunCodeAnalysis=True\n   )\n)\n\nIF %bld_default% EQU 1 (\n   SET release_64=1\n   SET debug_64=1\n   SET release_32=1\n   SET debug_32=1\n)\n\nREM \"IF ERRORLEVEL 1\" checks for error levels 1 OR MORE!\nREM %ERRORLEVEL% seems to be unreliable with MSBuild.exe\nREM https://devblogs.microsoft.com/oldnewthing/20080926-00\n\nIF %release_64% EQU 1 (\n   MSBuild.exe \"%root_path%shared\\libebm\\libebm.vcxproj\" /p:Configuration=Release /p:Platform=x64 %extra_analysis%\n   IF ERRORLEVEL 1 (\n      ECHO MSBuild for Release x64 FAILED\n      EXIT /B 101\n   )\n)\nIF %debug_64% EQU 1 (\n   MSBuild.exe \"%root_path%shared\\libebm\\libebm.vcxproj\" /p:Configuration=Debug /p:Platform=x64\n   IF ERRORLEVEL 1 (\n      ECHO MSBuild for Debug x64 FAILED\n      EXIT /B 102\n   )\n)\nIF %release_32% EQU 1 (\n   MSBuild.exe \"%root_path%shared\\libebm\\libebm.vcxproj\" /p:Configuration=Release /p:Platform=Win32 %extra_analysis%\n   IF ERRORLEVEL 1 (\n      ECHO MSBuild for Release x86 FAILED\n      EXIT /B 103\n   )\n)\nIF %debug_32% EQU 1 (\n   MSBuild.exe \"%root_path%shared\\libebm\\libebm.vcxproj\" /p:Configuration=Debug /p:Platform=Win32\n   IF ERRORLEVEL 1 (\n      ECHO MSBuild for Debug x86 FAILED\n      EXIT /B 104\n   )\n)\n\nEXIT /B 0\n"
        },
        {
          "name": "build.sh",
          "type": "blob",
          "size": 37.7265625,
          "content": "#!/bin/sh\n\n# TODO also build our html resources here, and also in the .bat file for Windows\n\nsanitize() {\n   # use this techinque where single quotes are expanded to '\\'' (end quotes insert single quote, start quote)\n   # but fixed from the version in this thread: \n   # https://stackoverflow.com/questions/15783701/which-characters-need-to-be-escaped-when-using-bash\n   # https://stackoverflow.com/questions/17529220/why-should-eval-be-avoided-in-bash-and-what-should-i-use-instead\n   printf \"%s\" \"$1\" | sed \"s/'/'\\\\\\\\''/g; 1s/^/'/; \\$s/\\$/'/\"\n}\n\nget_file_body() {\n   # https://www.oncrashreboot.com/use-sed-to-split-path-into-filename-extension-and-directory\n   printf \"%s\" \"$1\" | sed 's/\\(.*\\)\\/\\(.*\\)\\.\\(.*\\)$/\\2/'\n}\n\ncheck_install() {\n   l1_tmp_path_unsanitized=\"$1\"\n   l1_package=\"$2\"\n   \n   if [ ! -f \"$l1_tmp_path_unsanitized/$l1_package.chk\" ]; then\n      printf \"%s\\n\" \"Installing $l1_package\"\n\n      if [ \"$g_is_updated\" -eq 0 ]; then \n         sudo apt --yes update\n         l1_ret_code=$?\n         if [ $l1_ret_code -ne 0 ]; then \n            exit $l1_ret_code\n         fi\n\n         g_is_updated=1\n      fi\n\n      sudo apt --yes install \"$l1_package\"\n      l1_ret_code=$?\n      if [ $l1_ret_code -ne 0 ]; then \n         exit $l1_ret_code\n      fi\n         \n      # write out an empty file to signal that this has been installed\n      printf \"\" > \"$l1_tmp_path_unsanitized/$l1_package.chk\"\n      l1_ret_code=$?\n      if [ $l1_ret_code -ne 0 ]; then \n         exit $l1_ret_code\n      fi\n   fi\n}\n\nmake_paths() {\n   l2_obj_path_unsanitized=\"$1\"\n   l2_bin_path_unsanitized=\"$2\"\n\n   [ -d \"$l2_obj_path_unsanitized\" ] || mkdir -p \"$l2_obj_path_unsanitized\"\n   l2_ret_code=$?\n   if [ $l2_ret_code -ne 0 ]; then \n      exit $l2_ret_code\n   fi\n   [ -d \"$l2_bin_path_unsanitized\" ] || mkdir -p \"$l2_bin_path_unsanitized\"\n   l2_ret_code=$?\n   if [ $l2_ret_code -ne 0 ]; then \n      exit $l2_ret_code\n   fi\n}\n\ncompile_file() {\n   l3_compiler=\"$1\"\n   l3_compiler_args_sanitized=\"$2\"\n   l3_file_unsanitized=\"$3\"\n   l3_obj_path_unsanitized=\"$4\"\n   l3_asm=\"$5\"\n\n   l3_file_sanitized=`sanitize \"$l3_file_unsanitized\"`\n   l3_file_body_unsanitized=`get_file_body \"$l3_file_unsanitized\"`\n   l3_object_full_file_unsanitized=\"$l3_obj_path_unsanitized/${l3_file_body_unsanitized}.o\"\n   l3_object_full_file_sanitized=`sanitize \"$l3_object_full_file_unsanitized\"`\n   g_all_object_files_sanitized=\"$g_all_object_files_sanitized $l3_object_full_file_sanitized\"\n   l3_compile_specific=\"$l3_compiler $l3_compiler_args_sanitized -c $l3_file_sanitized -o $l3_object_full_file_sanitized 2>&1\"\n   l3_compile_out=`eval \"$l3_compile_specific\"`\n   l3_ret_code=$?\n   g_compile_out_full=\"$g_compile_out_full$l3_compile_out\"\n   if [ $l3_ret_code -ne 0 ]; then \n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      exit $l3_ret_code\n   fi\n\n   if [ $l3_asm -ne 0 ]; then\n      # - I'd rather do our real compile above with no special parameters because I'm not confident the compiler would\n      #   produce the same output if we included extra debugger info disassembly commands.  It's better to stick with \n      #   the normal program flow for our shared library output.  This rules out: --save-temps=obj\n      #   -Wa,-adhln=myoutput.s can be used in-stream, but we've ruled this out per above.  We can also use it\n      #   to generate the .s file below, but I found that this didn't have much benefit over -S and -fverbose-asm\n      #   We also write out objdump disassembly from the final library output itself which should allow us to \n      #   check that this annotated assembly is the same as what gets finally generated\n      # - https://panthema.net/2013/0124-GCC-Output-Assembler-Code/\n      # - https://stackoverflow.com/questions/137038/how-do-you-get-assembler-output-from-c-c-source-in-gcc\n      # - https://linux.die.net/man/1/as\n\n      # If this fails then ignore the error and we'll just be missing this file.\n      l3_asm_full_file_unsanitized=\"$l3_obj_path_unsanitized/${l3_file_body_unsanitized}.s\"\n      l3_asm_full_file_sanitized=`sanitize \"$l3_asm_full_file_unsanitized\"`\n      l3_compile_specific_asm=\"$l3_compiler $l3_compiler_args_sanitized -fverbose-asm -S $l3_file_sanitized -o $l3_asm_full_file_sanitized 2>&1\"\n      l3_compile_out_asm=`eval \"$l3_compile_specific_asm\"`\n   fi\n}\n\ncompile_directory() {\n   l4_compiler=\"$1\"\n   l4_compiler_args_sanitized=\"$2\"\n   l4_src_path_unsanitized=\"$3\"\n   l4_obj_path_unsanitized=\"$4\"\n   l4_asm=\"$5\"\n\n   # zsh (default shell in macs) terminates if you try to glob expand zero results, so check first\n   find \"$l4_src_path_unsanitized\" -maxdepth 1 -type f -name '*.cpp' 2>/dev/null | grep -q .\n   l4_ret_code=$?\n   if [ $l4_ret_code -eq 0 ]; then \n      # use globs with preceeding directory per: https://dwheeler.com/essays/filenames-in-shell.html\n      for l4_file_unsanitized in \"$l4_src_path_unsanitized\"/*.cpp ; do\n         # glob expansion returns *.cpp when there are no matches, so we need to check for the existance of the file\n         if [ -f \"$l4_file_unsanitized\" ] ; then\n            compile_file \"$l4_compiler\" \"$l4_compiler_args_sanitized\" \"$l4_file_unsanitized\" \"$l4_obj_path_unsanitized\" \"$l4_asm\"\n         fi\n      done\n   fi\n}\n\nlink_file() {\n   l5_linker=\"$1\"\n   l5_linker_args_sanitized=\"$2\"\n   l5_bin_path_unsanitized=\"$3\"\n   l5_bin_file=\"$4\"\n\n   l5_bin_path_sanitized=`sanitize \"$l5_bin_path_unsanitized\"`\n   # the linker wants to have the most dependent .o/.so/.dylib files listed FIRST\n   l5_compile_specific=\"$l5_linker $g_all_object_files_sanitized $l5_linker_args_sanitized -o $l5_bin_path_sanitized/$l5_bin_file 2>&1\"\n   l5_compile_out=`eval \"$l5_compile_specific\"`\n   l5_ret_code=$?\n   g_compile_out_full=\"$g_compile_out_full$l5_compile_out\"\n   if [ $l5_ret_code -ne 0 ]; then \n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      exit $l5_ret_code\n   fi\n}\n\ncopy_bin_files() {\n   l6_bin_path_unsanitized=\"$1\"\n   l6_bin_file=\"$2\"\n   l6_staging_path_unsanitized=\"$3\"\n\n   cp \"$l6_bin_path_unsanitized/$l6_bin_file\" \"$l6_staging_path_unsanitized/\"\n   l6_ret_code=$?\n   if [ $l6_ret_code -ne 0 ]; then \n      exit $l6_ret_code\n   fi\n}\n\ncopy_asm_files() {\n   l7_obj_path_unsanitized=\"$1\"\n   l7_bld_path_unsanitized=\"$2\"\n   l7_bin_file_unsanitized=\"$3\" \n   l7_tag=\"$4\"\n   l7_asm=\"$5\"\n\n   if [ $l7_asm -ne 0 ]; then \n      l7_tagged_path_unsanitized=\"$l7_bld_path_unsanitized/asm/$l7_tag\"\n\n      [ -d \"$l7_tagged_path_unsanitized\" ] || mkdir -p \"$l7_tagged_path_unsanitized\"\n      l7_ret_code=$?\n      if [ $l7_ret_code -ne 0 ]; then \n         exit $l7_ret_code\n      fi\n\n      cp \"$l7_obj_path_unsanitized\"/*.s \"$l7_tagged_path_unsanitized/\"\n      l7_ret_code=$?\n      if [ $l7_ret_code -ne 0 ]; then \n         exit $l7_ret_code\n      fi\n\n      #also generate a disassembly from the final output that we can compare the individual files against\n      l7_bin_file_body_unsanitized=`get_file_body \"$l7_bin_file_unsanitized\"`\n      os_type=`uname`\n      if [ \"$os_type\" = \"Linux\" ]; then\n         # - https://stackoverflow.com/questions/1289881/using-gcc-to-produce-readable-assembly\n         # GNU objdump https://linux.die.net/man/1/objdump\n         objdump --disassemble --private-headers --reloc --dynamic-reloc --section-headers --syms --line-numbers --no-show-raw-insn --source \"$l7_bin_file_unsanitized\" > \"$l7_tagged_path_unsanitized/$l7_bin_file_body_unsanitized.s\"\n      elif [ \"$os_type\" = \"Darwin\" ]; then\n         # objdump on mac is actually llvm-objdump\n         # https://llvm.org/docs/CommandGuide/llvm-objdump.html\n         # otool might be a better choice on mac, but this does what we need in combination with the individual \n         # module assembly, so keep it consistent with linux unless we need something more in the future\n         objdump --disassemble --private-headers --reloc --dynamic-reloc --section-headers --syms --line-numbers --no-show-raw-insn --source --print-imm-hex \"$l7_bin_file_unsanitized\" > \"$l7_tagged_path_unsanitized/$l7_bin_file_body_unsanitized.s\"\n      else\n         exit 1\n      fi\n   fi\n}\n\n\ng_is_updated=0\n\nis_conda=0\n\nrelease_default=1\n\nrelease_64=0\ndebug_64=0\nrelease_32=0\ndebug_32=0\nrelease_arm=0\ndebug_arm=0\n\nis_asm=0\nis_extra_debugging=0\nasan=\"\"\n\nfor arg in \"$@\"; do\n   if [ \"$arg\" = \"-conda\" ]; then\n      is_conda=1\n   fi\n\n   if [ \"$arg\" = \"-release_64\" ]; then\n      release_64=1\n      release_default=0\n   fi\n   if [ \"$arg\" = \"-debug_64\" ]; then\n      debug_64=1\n      release_default=0\n   fi\n   if [ \"$arg\" = \"-release_32\" ]; then\n      release_32=1\n      release_default=0\n   fi\n   if [ \"$arg\" = \"-debug_32\" ]; then\n      debug_32=1\n      release_default=0\n   fi\n   if [ \"$arg\" = \"-release_arm\" ]; then\n      release_arm=1\n      release_default=0\n   fi\n   if [ \"$arg\" = \"-debug_arm\" ]; then\n      debug_arm=1\n      release_default=0\n   fi\n\n   if [ \"$arg\" = \"-asm\" ]; then\n      is_asm=1\n   fi\n\n   if [ \"$arg\" = \"-extra_debugging\" ]; then\n      is_extra_debugging=1\n   fi\n\n   if [ \"$arg\" = \"-asan\" ]; then\n      asan=\"-asan\"\n   fi\ndone\n\n# TODO: this could be improved upon.  There is no perfect solution AFAIK for getting the script directory, and I'm not too sure how the CDPATH thing works\n# Look at BASH_SOURCE[0] as well and possibly select either it or $0\n# The output here needs to not be the empty string for glob substitution below:\nscript_path_initial=`dirname -- \"$0\"`\n# the space after the '= ' character is required\nscript_path_unsanitized=`CDPATH= cd -- \"$script_path_initial\" && pwd -P`\nif [ ! -f \"$script_path_unsanitized/build.sh\" ] ; then\n   # there are all kinds of reasons why we might not have gotten the script path in $0.  It's more of a convention\n   # than a requirement to have either the full path or even the script itself.  There are far more complicated\n   # scripts out there that attempt to use various shell specific workarounds, like BASH_SOURCE[0] to best solve\n   # the problem, but it's possible in theory to be running over an SSL connection without a script on the local\n   # system at all, so getting the directory is a fundamentally unsolved problem.  We can terminate though if\n   # we find ourselves in such a weird condition.  This also happens when the \"source\" command is used.\n   printf \"Could not find script file root directory for building InterpretML.  Exiting.\"\n   exit 1\nfi\n\nroot_path_unsanitized=\"$script_path_unsanitized\"\nbld_path_unsanitized=\"$root_path_unsanitized/bld\"\ntmp_path_unsanitized=\"$bld_path_unsanitized/tmp\"\nstaging_path_unsanitized=\"$bld_path_unsanitized/lib\"\nsrc_path_unsanitized=\"$root_path_unsanitized/shared/libebm\"\nsrc_path_sanitized=`sanitize \"$src_path_unsanitized\"`\n\n\nif [ $is_conda -eq 1 ]; then\n   code_path=\"./shared/libebm\"\n   tmp_path=\"./bld/tmp/mk\"\n\n   os_type=`uname`\n   # TODO: change this to accept libebm_local.so or libebm_local.dylib to allow for weird architectures build using sdists\n   if [ \"$os_type\" = \"Linux\" ]; then\n      final_binary=\"./bld/lib/libebm.so\"\n   elif [ \"$os_type\" = \"Darwin\" ]; then\n      final_binary=\"./bld/lib/libebm.dylib\"\n   else\n      printf \"%s\\n\" \"OS $os_type not recognized.  We support clang/clang++ on macOS and gcc/g++ on Linux\"\n      exit 1\n   fi\n\n   mkdir ./bld\n   mkdir ./bld/lib\n\n   extras=\"-DLIBEBM_EXPORTS -DNDEBUG -I$code_path/inc -I$code_path/unzoned -I$code_path/bridge -I$code_path -I$code_path/compute -I$code_path/compute/objectives -I$code_path/compute/metrics\"\n\n   mkdir ./bld\n   mkdir ./bld/tmp\n   mkdir ./bld/tmp/mk\n   mkdir ./bld/lib\n\n   printf \"Building from environment specified compiler\\n\"\n   printf \"%s\\n\" \"CXX=${CXX}\"\n   printf \"%s\\n\" \"CPPFLAGS=${CPPFLAGS}\"\n   printf \"%s\\n\" \"CXXFLAGS=${CXXFLAGS}\"\n\n   printf \"%s\\n\" \"LDFLAGS=${LDFLAGS}\"\n   printf \"%s\\n\" \"LOADLIBES=${LOADLIBES}\"\n   printf \"%s\\n\" \"LDLIBS=${LDLIBS}\"\n\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/ApplyTermUpdate.cpp\" -o \"$tmp_path/ApplyTermUpdate.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/BoosterCore.cpp\" -o \"$tmp_path/BoosterCore.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/BoosterShell.cpp\" -o \"$tmp_path/BoosterShell.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/CalcInteractionStrength.cpp\" -o \"$tmp_path/CalcInteractionStrength.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/compute_accessors.cpp\" -o \"$tmp_path/compute_accessors.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/ConvertAddBin.cpp\" -o \"$tmp_path/ConvertAddBin.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/CutQuantile.cpp\" -o \"$tmp_path/CutQuantile.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/CutUniform.cpp\" -o \"$tmp_path/CutUniform.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/CutWinsorized.cpp\" -o \"$tmp_path/CutWinsorized.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/dataset_shared.cpp\" -o \"$tmp_path/dataset_shared.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/DataSetBoosting.cpp\" -o \"$tmp_path/DataSetBoosting.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/DataSetInnerBag.cpp\" -o \"$tmp_path/DataSetInnerBag.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/DataSetInteraction.cpp\" -o \"$tmp_path/DataSetInteraction.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/DetermineLinkFunction.cpp\" -o \"$tmp_path/DetermineLinkFunction.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/debug_ebm.cpp\" -o \"$tmp_path/debug_ebm.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/Discretize.cpp\" -o \"$tmp_path/Discretize.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/Term.cpp\" -o \"$tmp_path/Term.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/GenerateTermUpdate.cpp\" -o \"$tmp_path/GenerateTermUpdate.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/InitializeGradientsAndHessians.cpp\" -o \"$tmp_path/InitializeGradientsAndHessians.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/InteractionCore.cpp\" -o \"$tmp_path/InteractionCore.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/InteractionShell.cpp\" -o \"$tmp_path/InteractionShell.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/interpretable_numerics.cpp\" -o \"$tmp_path/interpretable_numerics.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/PartitionOneDimensionalBoosting.cpp\" -o \"$tmp_path/PartitionOneDimensionalBoosting.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/PartitionRandomBoosting.cpp\" -o \"$tmp_path/PartitionRandomBoosting.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/PartitionMultiDimensionalTree.cpp\" -o \"$tmp_path/PartitionMultiDimensionalTree.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/PartitionMultiDimensionalStraight.cpp\" -o \"$tmp_path/PartitionMultiDimensionalStraight.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/Purify.cpp\" -o \"$tmp_path/Purify.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/RandomDeterministic.cpp\" -o \"$tmp_path/RandomDeterministic.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/random.cpp\" -o \"$tmp_path/random.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/sampling.cpp\" -o \"$tmp_path/sampling.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/SubsetInnerBag.cpp\" -o \"$tmp_path/SubsetInnerBag.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/Tensor.cpp\" -o \"$tmp_path/Tensor.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/TensorTotalsBuild.cpp\" -o \"$tmp_path/TensorTotalsBuild.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/TermInnerBag.cpp\" -o \"$tmp_path/TermInnerBag.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/unzoned/logging.cpp\" -o \"$tmp_path/logging.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/unzoned/unzoned.cpp\" -o \"$tmp_path/unzoned.o\"\n   ${CXX} -c ${CPPFLAGS} ${CXXFLAGS} ${extras} \"$code_path/compute/cpu_ebm/cpu_64.cpp\" -o \"$tmp_path/cpu_64.o\"\n\n   ${CXX} ${LDFLAGS} -shared \\\n   \"$tmp_path/ApplyTermUpdate.o\" \\\n   \"$tmp_path/BoosterCore.o\" \\\n   \"$tmp_path/BoosterShell.o\" \\\n   \"$tmp_path/CalcInteractionStrength.o\" \\\n   \"$tmp_path/compute_accessors.o\" \\\n   \"$tmp_path/ConvertAddBin.o\" \\\n   \"$tmp_path/CutQuantile.o\" \\\n   \"$tmp_path/CutUniform.o\" \\\n   \"$tmp_path/CutWinsorized.o\" \\\n   \"$tmp_path/dataset_shared.o\" \\\n   \"$tmp_path/DataSetBoosting.o\" \\\n   \"$tmp_path/DataSetInnerBag.o\" \\\n   \"$tmp_path/DataSetInteraction.o\" \\\n   \"$tmp_path/DetermineLinkFunction.o\" \\\n   \"$tmp_path/debug_ebm.o\" \\\n   \"$tmp_path/Discretize.o\" \\\n   \"$tmp_path/Term.o\" \\\n   \"$tmp_path/GenerateTermUpdate.o\" \\\n   \"$tmp_path/InitializeGradientsAndHessians.o\" \\\n   \"$tmp_path/InteractionCore.o\" \\\n   \"$tmp_path/InteractionShell.o\" \\\n   \"$tmp_path/interpretable_numerics.o\" \\\n   \"$tmp_path/PartitionOneDimensionalBoosting.o\" \\\n   \"$tmp_path/PartitionRandomBoosting.o\" \\\n   \"$tmp_path/PartitionMultiDimensionalTree.o\" \\\n   \"$tmp_path/PartitionMultiDimensionalStraight.o\" \\\n   \"$tmp_path/Purify.o\" \\\n   \"$tmp_path/RandomDeterministic.o\" \\\n   \"$tmp_path/random.o\" \\\n   \"$tmp_path/sampling.o\" \\\n   \"$tmp_path/SubsetInnerBag.o\" \\\n   \"$tmp_path/Tensor.o\" \\\n   \"$tmp_path/TensorTotalsBuild.o\" \\\n   \"$tmp_path/TermInnerBag.o\" \\\n   \"$tmp_path/logging.o\" \\\n   \"$tmp_path/unzoned.o\" \\\n   \"$tmp_path/cpu_64.o\" \\\n   ${LOADLIBES} ${LDLIBS} -o \"$final_binary\"\n\n   exit 0\nfi\n\n\n# a good referenece on writing shared libraries is at: https://akkadia.org/drepper/dsohowto.pdf\n\n# re-enable these warnings when they are better supported by g++ or clang: -Wduplicated-cond -Wduplicated-branches -Wrestrict\nall_args=\"-std=c++11\"\nall_args=\"$all_args -Wall -Wextra\"\nall_args=\"$all_args -Wunused-result\"\nall_args=\"$all_args -Wdouble-promotion\"\nall_args=\"$all_args -Wold-style-cast\"\nall_args=\"$all_args -Wshadow\"\nall_args=\"$all_args -Wformat=2\"\nall_args=\"$all_args -Wno-format-nonliteral\"\nall_args=\"$all_args -Wno-parentheses\"\nall_args=\"$all_args -fvisibility=hidden -fvisibility-inlines-hidden\"\nall_args=\"$all_args -fno-math-errno -fno-trapping-math\"\n# TODO: once we have highly efficient tightly looped code, try no -fpic and see if that makes better code.  The compiler can save a register in this case. See https://akkadia.org/drepper/dsohowto.pdf\n# TODO: check no-plt compiler option\nall_args=\"$all_args -fpic\"\nall_args=\"$all_args -pthread\"\nall_args=\"$all_args -DLIBEBM_EXPORTS\"\n\nif [ $is_extra_debugging -ne 0 ]; then \n   all_args=\"$all_args -g\"\nfi\nif [ -n \"$asan\" ]; then\n   all_args=\"$all_args -fsanitize=address,undefined -fno-sanitize-recover=address,undefined\"\nfi\n\nall_args=\"$all_args -I$src_path_sanitized/inc\"\n\nunzoned_args=\"\"\nunzoned_args=\"$unzoned_args -I$src_path_sanitized/unzoned\"\n\ncompute_args=\"\"\ncompute_args=\"$compute_args -I$src_path_sanitized/unzoned\"\ncompute_args=\"$compute_args -I$src_path_sanitized/bridge\"\ncompute_args=\"$compute_args -I$src_path_sanitized/compute\"\ncompute_args=\"$compute_args -I$src_path_sanitized/compute/objectives\"\ncompute_args=\"$compute_args -I$src_path_sanitized/compute/metrics\"\n\nmain_args=\"\"\nmain_args=\"$main_args -I$src_path_sanitized/unzoned\"\nmain_args=\"$main_args -I$src_path_sanitized/bridge\"\nmain_args=\"$main_args -I$src_path_sanitized\"\n\nlink_args=\"\"\n\nos_type=`uname`\n\nif [ \"$os_type\" = \"Linux\" ]; then\n   cpp_compiler=g++\n\n   # try moving some of these g++ specific warnings into the shared all_args if clang eventually supports them\n   all_args=\"$all_args -Wlogical-op\"\n\n   link_args=\"$link_args -Wl,--version-script=$src_path_sanitized/libebm_exports.txt\"\n   link_args=\"$link_args -Wl,--exclude-libs,ALL\"\n   link_args=\"$link_args -Wl,-z,relro,-z,now\"\n   link_args=\"$link_args -Wl,-O2\"\n   link_args=\"$link_args -Wl,--sort-common\"\n   link_args=\"$link_args -static-libgcc\"\n   link_args=\"$link_args -static-libstdc++\"\n   link_args=\"$link_args -shared\"\n\n   printf \"%s\\n\" \"Creating initial directories\"\n   [ -d \"$staging_path_unsanitized\" ] || mkdir -p \"$staging_path_unsanitized\"\n   ret_code=$?\n   if [ $ret_code -ne 0 ]; then \n      exit $ret_code\n   fi\n\n   if [ $release_default -eq 1 ]; then\n      ########################## Linux release|default\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for Linux release|default\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/gcc/obj/release/linux/default/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/gcc/bin/release/linux/default/libebm\"\n      bin_file=\"libebm.so\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_release_linux_default_build_log.txt\"\n      specific_args=\"$all_args -DNDEBUG -O3 -Wl,--wrap=memcpy -Wl,--wrap=exp -Wl,--wrap=log -Wl,--wrap=log2,--wrap=pow,--wrap=expf,--wrap=logf\"\n   \n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      compile_file \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized\"/special/linux_wrap_functions.cpp \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" \"$is_asm\"\n      link_file \"$cpp_compiler\" \"$link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n      copy_asm_files \"$obj_path_unsanitized\" \"$bld_path_unsanitized\" \"$staging_path_unsanitized/$bin_file\" \"linux_default_release\" \"$is_asm\"\n   fi\n\n   if [ $release_64 -eq 1 ]; then\n      ########################## Linux release|x64\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for Linux release|x64\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/gcc/obj/release/linux/x64/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/gcc/bin/release/linux/x64/libebm\"\n      bin_file=\"libebm_linux_x64.so\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_release_linux_x64_build_log.txt\"\n      specific_args=\"$all_args -march=core2 -m64 -DNDEBUG -O3 -DBRIDGE_AVX2_32 -DBRIDGE_AVX512F_32 -Wl,--wrap=memcpy -Wl,--wrap=exp -Wl,--wrap=log -Wl,--wrap=log2,--wrap=pow,--wrap=expf,--wrap=logf\"\n   \n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      compile_file \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized\"/special/linux_wrap_functions.cpp \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx2 -mfma\" \"$src_path_unsanitized/compute/avx2_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx512f\" \"$src_path_unsanitized/compute/avx512f_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" \"$is_asm\"\n      link_file \"$cpp_compiler\" \"$link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n      copy_asm_files \"$obj_path_unsanitized\" \"$bld_path_unsanitized\" \"$staging_path_unsanitized/$bin_file\" \"linux_64_release\" \"$is_asm\"\n   fi\n\n   if [ $debug_64 -eq 1 ]; then\n      ########################## Linux debug|x64\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for Linux debug|x64\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/gcc/obj/debug/linux/x64/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/gcc/bin/debug/linux/x64/libebm\"\n      bin_file=\"libebm_linux_x64_debug.so\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_debug_linux_x64_build_log.txt\"\n      specific_args=\"$all_args -march=core2 -m64 -O1 -DBRIDGE_AVX2_32 -DBRIDGE_AVX512F_32 -Wl,--wrap=memcpy -Wl,--wrap=exp -Wl,--wrap=log -Wl,--wrap=log2,--wrap=pow,--wrap=expf,--wrap=logf\"\n   \n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      compile_file \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized\"/special/linux_wrap_functions.cpp \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx2 -mfma\" \"$src_path_unsanitized/compute/avx2_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx512f\" \"$src_path_unsanitized/compute/avx512f_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" 0\n      link_file \"$cpp_compiler\" \"$link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n   fi\n\n   if [ $release_32 -eq 1 ]; then\n      ########################## Linux release|x86\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for Linux release|x86\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/gcc/obj/release/linux/x86/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/gcc/bin/release/linux/x86/libebm\"\n      bin_file=\"libebm_linux_x86.so\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_release_linux_x86_build_log.txt\"\n      specific_args=\"$all_args -march=core2 -DBRIDGE_AVX2_32 -DBRIDGE_AVX512F_32 -msse2 -mfpmath=sse -m32 -DNDEBUG -O3\"\n      \n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      check_install \"$tmp_path_unsanitized\" \"g++-multilib\"\n      compile_file \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized\"/special/linux_wrap_functions.cpp \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx2 -mfma\" \"$src_path_unsanitized/compute/avx2_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx512f\" \"$src_path_unsanitized/compute/avx512f_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" 0\n      link_file \"$cpp_compiler\" \"$link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n   fi\n\n   if [ $debug_32 -eq 1 ]; then\n      ########################## Linux debug|x86\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for Linux debug|x86\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/gcc/obj/debug/linux/x86/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/gcc/bin/debug/linux/x86/libebm\"\n      bin_file=\"libebm_linux_x86_debug.so\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_debug_linux_x86_build_log.txt\"\n      specific_args=\"$all_args -march=core2 -DBRIDGE_AVX2_32 -DBRIDGE_AVX512F_32 -msse2 -mfpmath=sse -m32 -O1\"\n      \n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      check_install \"$tmp_path_unsanitized\" \"g++-multilib\"\n      compile_file \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized\"/special/linux_wrap_functions.cpp \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx2 -mfma\" \"$src_path_unsanitized/compute/avx2_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx512f\" \"$src_path_unsanitized/compute/avx512f_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" 0\n      link_file \"$cpp_compiler\" \"$link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n   fi\n\nelif [ \"$os_type\" = \"Darwin\" ]; then\n   # reference on rpath & install_name: https://www.mikeash.com/pyblog/friday-qa-2009-11-06-linking-and-install-names.html\n\n   # try moving some of these clang specific warnings into the shared all_args if g++ eventually supports them\n   cpp_compiler=clang++\n   \n   all_args=\"$all_args -Wnull-dereference\"\n   all_args=\"$all_args -Wgnu-zero-variadic-macro-arguments\"\n\n   link_args=\"$link_args -dynamiclib\"\n\n   printf \"%s\\n\" \"Creating initial directories\"\n   [ -d \"$staging_path_unsanitized\" ] || mkdir -p \"$staging_path_unsanitized\"\n   ret_code=$?\n   if [ $ret_code -ne 0 ]; then \n      exit $ret_code\n   fi\n\n   if [ $release_default -eq 1 ]; then\n      ########################## macOS release|default\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for macOS release|default\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/clang/obj/release/mac/default/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/clang/bin/release/mac/default/libebm\"\n      bin_file=\"libebm.dylib\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_release_mac_default_build_log.txt\"\n      specific_args=\"$all_args -DNDEBUG -O3\"\n   \n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" \"$is_asm\"\n      link_file \"$cpp_compiler\" \"-install_name @rpath/$bin_file $link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n      copy_asm_files \"$obj_path_unsanitized\" \"$bld_path_unsanitized\" \"$staging_path_unsanitized/$bin_file\" \"mac_64_release\" \"$is_asm\"\n   fi\n\n   if [ $release_64 -eq 1 ]; then\n      ########################## macOS release|x64\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for macOS release|x64\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/clang/obj/release/mac/x64/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/clang/bin/release/mac/x64/libebm\"\n      bin_file=\"libebm_mac_x64.dylib\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_release_mac_x64_build_log.txt\"\n      specific_args=\"$all_args -march=core2 -target x86_64-apple-macos10.12 -m64 -DNDEBUG -O3 -DBRIDGE_AVX2_32 -DBRIDGE_AVX512F_32\"\n   \n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx2 -mfma\" \"$src_path_unsanitized/compute/avx2_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx512f\" \"$src_path_unsanitized/compute/avx512f_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" \"$is_asm\"\n      link_file \"$cpp_compiler\" \"-install_name @rpath/$bin_file $link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n      copy_asm_files \"$obj_path_unsanitized\" \"$bld_path_unsanitized\" \"$staging_path_unsanitized/$bin_file\" \"mac_64_release\" \"$is_asm\"\n   fi\n\n   if [ $debug_64 -eq 1 ]; then\n      ########################## macOS debug|x64\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for macOS debug|x64\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/clang/obj/debug/mac/x64/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/clang/bin/debug/mac/x64/libebm\"\n      bin_file=\"libebm_mac_x64_debug.dylib\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_debug_mac_x64_build_log.txt\"\n      specific_args=\"$all_args -march=core2 -target x86_64-apple-macos10.12 -m64 -O1 -DBRIDGE_AVX2_32 -DBRIDGE_AVX512F_32 -fno-optimize-sibling-calls -fno-omit-frame-pointer\"\n\n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx2 -mfma\" \"$src_path_unsanitized/compute/avx2_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args -mavx512f\" \"$src_path_unsanitized/compute/avx512f_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" 0\n      link_file \"$cpp_compiler\" \"-install_name @rpath/$bin_file $link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n   fi\n\n   if [ $release_arm -eq 1 ]; then\n      ########################## macOS release|arm\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for macOS release|arm\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/clang/obj/release/mac/arm/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/clang/bin/release/mac/arm/libebm\"\n      bin_file=\"libebm_mac_arm.dylib\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_release_mac_arm_build_log.txt\"\n      specific_args=\"$all_args -target arm64-apple-macos11 -m64 -DNDEBUG -O3\"\n   \n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" \"$is_asm\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" \"$is_asm\"\n      link_file \"$cpp_compiler\" \"-install_name @rpath/$bin_file $link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n      copy_asm_files \"$obj_path_unsanitized\" \"$bld_path_unsanitized\" \"$staging_path_unsanitized/$bin_file\" \"mac_arm_release\" \"$is_asm\"\n   fi\n\n   if [ $debug_arm -eq 1 ]; then\n      ########################## macOS debug|arm\n\n      printf \"%s\\n\" \"Compiling libebm with $cpp_compiler for macOS debug|arm\"\n      obj_path_unsanitized=\"$tmp_path_unsanitized/clang/obj/debug/mac/arm/libebm\"\n      bin_path_unsanitized=\"$tmp_path_unsanitized/clang/bin/debug/mac/arm/libebm\"\n      bin_file=\"libebm_mac_arm_debug.dylib\"\n      g_log_file_unsanitized=\"$obj_path_unsanitized/libebm_debug_mac_arm_build_log.txt\"\n      specific_args=\"$all_args -target arm64-apple-macos11 -m64 -O1 -fno-optimize-sibling-calls -fno-omit-frame-pointer\"\n\n      g_all_object_files_sanitized=\"\"\n      g_compile_out_full=\"\"\n\n      make_paths \"$obj_path_unsanitized\" \"$bin_path_unsanitized\"\n      compile_directory \"$cpp_compiler\" \"$specific_args $unzoned_args\" \"$src_path_unsanitized/unzoned\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $compute_args\" \"$src_path_unsanitized/compute/cpu_ebm\" \"$obj_path_unsanitized\" 0\n      compile_directory \"$cpp_compiler\" \"$specific_args $main_args\" \"$src_path_unsanitized\" \"$obj_path_unsanitized\" 0\n      link_file \"$cpp_compiler\" \"-install_name @rpath/$bin_file $link_args $specific_args\" \"$bin_path_unsanitized\" \"$bin_file\"\n      printf \"%s\\n\" \"$g_compile_out_full\"\n      printf \"%s\\n\" \"$g_compile_out_full\" > \"$g_log_file_unsanitized\"\n      copy_bin_files \"$bin_path_unsanitized\" \"$bin_file\" \"$staging_path_unsanitized\"\n   fi\n\nelse\n   printf \"%s\\n\" \"OS $os_type not recognized.  We support clang/clang++ on macOS and gcc/g++ on Linux\"\n   exit 1\nfi\n"
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.1708984375,
          "content": "coverage:\n  status:\n    project:\n      default:\n        target: 1%\n        threshold: 99%\n        base: auto\n        paths: []\n        flags: []\n    patch: no\n    changes: no\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "lgtm.yml",
          "type": "blob",
          "size": 0.0439453125,
          "content": "queries:\n- exclude: py/modification-of-locals"
        },
        {
          "name": "python",
          "type": "tree",
          "content": null
        },
        {
          "name": "ruff.toml",
          "type": "blob",
          "size": 1.0244140625,
          "content": "# match oldest supported version\ntarget-version = \"py38\"\n[lint]\nextend-select = [\n  \"B\",        # flake8-bugbear\n  \"I\",        # isort\n  \"ARG\",      # flake8-unused-arguments\n  \"C4\",       # flake8-comprehensions\n  \"EM\",       # flake8-errmsg\n  \"ICN\",      # flake8-import-conventions\n  \"G\",        # flake8-logging-format\n  \"PGH\",      # pygrep-hooks\n  \"PIE\",      # flake8-pie\n  \"PL\",       # pylint\n  \"PT\",       # flake8-pytest-style\n  \"PTH\",      # flake8-use-pathlib\n  \"RET\",      # flake8-return\n  \"RUF\",      # Ruff-specific\n  \"SIM\",      # flake8-simplify\n  \"T20\",      # flake8-print\n  \"UP\",       # pyupgrade\n  \"YTT\",      # flake8-2020\n  \"EXE\",      # flake8-executable\n  \"NPY\",      # NumPy specific rules\n  \"PD\",       # pandas-vet\n  \"FURB\",     # refurb\n  \"PYI\",      # flake8-pyi\n  \"PERF\",     # perflint\n]\nignore = [\n  \"PLR09\",    # Too many <...>\n  \"PLR2004\",  # Magic value used in comparison\n  \"ISC001\",   # Conflicts with formatter\n  \"PTH123\",   # using Path.open instead of open\n]\n\n[lint.per-file-ignores]\n\"tests/**\" = [\"T20\"]\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "shared",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}