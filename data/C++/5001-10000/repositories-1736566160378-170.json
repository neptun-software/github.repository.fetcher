{
  "metadata": {
    "timestamp": 1736566160378,
    "page": 170,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjE3MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "google/snappy",
      "stars": 6231,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".bazelrc",
          "type": "blob",
          "size": 0.125,
          "content": "# googletest requires C++14 or above\nbuild --cxxopt='-std=c++17'\n# Enable Bzlmod for every Bazel command\ncommon --enable_bzlmod\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.091796875,
          "content": "# Editors.\n*.sw*\n.vscode\n.DS_Store\n\n# Build directory.\nbuild/\n/bazel-*\nMODULE.bazel.lock\nout/\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.2236328125,
          "content": "[submodule \"third_party/benchmark\"]\n\tpath = third_party/benchmark\n\turl = https://github.com/google/benchmark.git\n[submodule \"third_party/googletest\"]\n\tpath = third_party/googletest\n\turl = https://github.com/google/googletest.git\n"
        },
        {
          "name": "AUTHORS",
          "type": "blob",
          "size": 0.021484375,
          "content": "opensource@google.com\n"
        },
        {
          "name": "BUILD.bazel",
          "type": "blob",
          "size": 5.59375,
          "content": "# Copyright 2023 Google Inc. All Rights Reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#     * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\npackage(default_visibility = [\"//visibility:public\"])\n\nlicenses([\"notice\"])\n\nSNAPPY_VERSION = (1, 1, 10)\n\nconfig_setting(\n    name = \"windows\",\n    constraint_values = [\"@platforms//os:windows\"],\n)\n\ncc_library(\n    name = \"config\",\n    hdrs = [\"config.h\"],\n    defines = [\"HAVE_CONFIG_H\"],\n)\n\ncc_library(\n    name = \"snappy-stubs-public\",\n    hdrs = [\":snappy-stubs-public.h\"],\n)\n\ncc_library(\n    name = \"snappy-stubs-internal\",\n    srcs = [\"snappy-stubs-internal.cc\"],\n    hdrs = [\"snappy-stubs-internal.h\"],\n    deps = [\n        \":config\",\n        \":snappy-stubs-public\",\n    ],\n)\n\ncc_library(\n    name = \"snappy\",\n    srcs = [\n        \"snappy.cc\",\n        \"snappy-internal.h\",\n        \"snappy-sinksource.cc\",\n    ],\n    hdrs = [\n        \"snappy.h\",\n        \"snappy-sinksource.h\",\n    ],\n    copts = select({\n        \":windows\": [],\n        \"//conditions:default\": [\n            \"-Wno-sign-compare\",\n        ],\n    }),\n    deps = [\n        \":config\",\n        \":snappy-stubs-internal\",\n        \":snappy-stubs-public\",\n    ],\n)\n\ncc_library(\n    name = \"snappy-c\",\n    srcs = [\"snappy-c.cc\"],\n    hdrs = [\"snappy-c.h\"],\n    deps = [\":snappy\"],\n)\n\nfilegroup(\n    name = \"testdata\",\n    srcs = glob([\"testdata/*\"]),\n)\n\ncc_library(\n    name = \"snappy-test\",\n    testonly = True,\n    srcs = [\n        \"snappy-test.cc\",\n        \"snappy_test_data.cc\",\n    ],\n    hdrs = [\n        \"snappy-test.h\",\n        \"snappy_test_data.h\",\n    ],\n    deps = [\":snappy-stubs-internal\"],\n)\n\ncc_test(\n    name = \"snappy_benchmark\",\n    srcs = [\"snappy_benchmark.cc\"],\n    data = [\":testdata\"],\n    deps = [\n        \":snappy\",\n        \":snappy-test\",\n        \"@com_google_benchmark//:benchmark_main\",\n    ],\n)\n\ncc_test(\n    name = \"snappy_unittest\",\n    srcs = [\n        \"snappy_unittest.cc\",\n    ],\n    data = [\":testdata\"],\n    deps = [\n        \":snappy\",\n        \":snappy-test\",\n        \"@com_google_googletest//:gtest_main\",\n    ],\n)\n\n# Generate a config.h similar to what cmake would produce.\ngenrule(\n    name = \"config_h\",\n    outs = [\"config.h\"],\n    cmd = \"\"\"cat <<EOF >$@\n#define HAVE_STDDEF_H 1\n#define HAVE_STDINT_H 1\n#ifdef __has_builtin\n#  if !defined(HAVE_BUILTIN_EXPECT) && __has_builtin(__builtin_expect)\n#    define HAVE_BUILTIN_EXPECT 1\n#  endif\n#  if !defined(HAVE_BUILTIN_CTZ) && __has_builtin(__builtin_ctzll)\n#    define HAVE_BUILTIN_CTZ 1\n#  endif\n#  if !defined(HAVE_BUILTIN_PREFETCH) && __has_builtin(__builtin_prefetech)\n#    define HAVE_BUILTIN_PREFETCH 1\n#  endif\n#elif defined(__GNUC__) && (__GNUC__ > 3 || __GNUC__ == 3 && __GNUC_MINOR__ >= 4)\n#  ifndef HAVE_BUILTIN_EXPECT\n#    define HAVE_BUILTIN_EXPECT 1\n#  endif\n#  ifndef HAVE_BUILTIN_CTZ\n#    define HAVE_BUILTIN_CTZ 1\n#  endif\n#  ifndef HAVE_BUILTIN_PREFETCH\n#    define HAVE_BUILTIN_PREFETCH 1\n#  endif\n#endif\n\n#if defined(_WIN32) && !defined(HAVE_WINDOWS_H)\n#define HAVE_WINDOWS_H 1\n#endif\n\n#ifdef __has_include\n#  if !defined(HAVE_BYTESWAP_H) && __has_include(<byteswap.h>)\n#    define HAVE_BYTESWAP_H 1\n#  endif\n#  if !defined(HAVE_UNISTD_H) && __has_include(<unistd.h>)\n#    define HAVE_UNISTD_H 1\n#  endif\n#  if !defined(HAVE_SYS_ENDIAN_H) && __has_include(<sys/endian.h>)\n#    define HAVE_SYS_ENDIAN_H 1\n#  endif\n#  if !defined(HAVE_SYS_MMAN_H) && __has_include(<sys/mman.h>)\n#    define HAVE_SYS_MMAN_H 1\n#  endif\n#  if !defined(HAVE_SYS_UIO_H) && __has_include(<sys/uio.h>)\n#    define HAVE_SYS_UIO_H 1\n#  endif\n#  if !defined(HAVE_SYS_TIME_H) && __has_include(<sys/time.h>)\n#    define HAVE_SYS_TIME_H 1\n#  endif\n#endif\n\n#ifndef SNAPPY_IS_BIG_ENDIAN\n#  ifdef __s390x__\n#    define SNAPPY_IS_BIG_ENDIAN 1\n#  elif defined(__BYTE_ORDER__) && defined(__ORDER_BIG_ENDIAN__) && __BYTE_ORDER__ == __ORDER_BIG_ENDIAN__\n#    define SNAPPY_IS_BIG_ENDIAN 1\n#  endif\n#endif\nEOF\n\"\"\",\n)\n\ngenrule(\n    name = \"snappy_stubs_public_h\",\n    srcs = [\"snappy-stubs-public.h.in\"],\n    outs = [\"snappy-stubs-public.h\"],\n    # Assume sys/uio.h is available on non-Windows.\n    # Set the version numbers.\n    cmd = (\"\"\"sed -e 's/$${HAVE_SYS_UIO_H_01}/!_WIN32/g' \\\n           -e 's/$${PROJECT_VERSION_MAJOR}/%d/g' \\\n           -e 's/$${PROJECT_VERSION_MINOR}/%d/g' \\\n           -e 's/$${PROJECT_VERSION_PATCH}/%d/g' \\\n    $< >$@\"\"\" % SNAPPY_VERSION),\n)\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 14.5390625,
          "content": "# Copyright 2019 Google Inc. All Rights Reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#     * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\ncmake_minimum_required(VERSION 3.1)\nproject(Snappy VERSION 1.2.1 LANGUAGES C CXX)\n\n# C++ standard can be overridden when this is used as a sub-project.\nif(NOT CMAKE_CXX_STANDARD)\n  # This project requires C++11.\n  set(CMAKE_CXX_STANDARD 11)\n  set(CMAKE_CXX_STANDARD_REQUIRED ON)\n  set(CMAKE_CXX_EXTENSIONS OFF)\nendif(NOT CMAKE_CXX_STANDARD)\n\n# https://github.com/izenecloud/cmake/blob/master/SetCompilerWarningAll.cmake\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n  # Use the highest warning level for Visual Studio.\n  set(CMAKE_CXX_WARNING_LEVEL 4)\n  if(CMAKE_CXX_FLAGS MATCHES \"/W[0-4]\")\n    string(REGEX REPLACE \"/W[0-4]\" \"/W4\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n  else(CMAKE_CXX_FLAGS MATCHES \"/W[0-4]\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /W4\")\n  endif(CMAKE_CXX_FLAGS MATCHES \"/W[0-4]\")\n\n  # Disable C++ exceptions.\n  string(REGEX REPLACE \"/EH[a-z]+\" \"\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /EHs-c-\")\n  add_definitions(-D_HAS_EXCEPTIONS=0)\n\n  # Disable RTTI.\n  string(REGEX REPLACE \"/GR\" \"\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /GR-\")\nelse(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n  # Use -Wall for clang and gcc.\n  if(NOT CMAKE_CXX_FLAGS MATCHES \"-Wall\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wall\")\n  endif(NOT CMAKE_CXX_FLAGS MATCHES \"-Wall\")\n\n  # Use -Wextra for clang and gcc.\n  if(NOT CMAKE_CXX_FLAGS MATCHES \"-Wextra\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wextra\")\n  endif(NOT CMAKE_CXX_FLAGS MATCHES \"-Wextra\")\n\n  # Use -Werror for clang only.\n  if(CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n    if(NOT CMAKE_CXX_FLAGS MATCHES \"-Werror\")\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Werror\")\n    endif(NOT CMAKE_CXX_FLAGS MATCHES \"-Werror\")\n  endif(CMAKE_CXX_COMPILER_ID MATCHES \"Clang\")\n\n  # Disable sign comparison warnings. Matches upcoming Bazel setup.\n  if(NOT CMAKE_CXX_FLAGS MATCHES \"-Wno-sign-compare\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-sign-compare\")\n  endif(NOT CMAKE_CXX_FLAGS MATCHES \"-Wno-sign-compare\")\n\n  # Disable C++ exceptions.\n  string(REGEX REPLACE \"-fexceptions\" \"\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-exceptions\")\n\n  # Disable RTTI.\n  string(REGEX REPLACE \"-frtti\" \"\" CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\n  set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fno-rtti\")\nendif(CMAKE_CXX_COMPILER_ID STREQUAL \"MSVC\")\n\n# BUILD_SHARED_LIBS is a standard CMake variable, but we declare it here to make\n# it prominent in the GUI.\noption(BUILD_SHARED_LIBS \"Build shared libraries(DLLs).\" OFF)\n\noption(SNAPPY_BUILD_TESTS \"Build Snappy's own tests.\" ON)\n\noption(SNAPPY_BUILD_BENCHMARKS \"Build Snappy's benchmarks\" ON)\n\noption(SNAPPY_FUZZING_BUILD \"Build Snappy for fuzzing.\" OFF)\n\noption(SNAPPY_REQUIRE_AVX \"Target processors with AVX support.\" OFF)\n\noption(SNAPPY_REQUIRE_AVX2 \"Target processors with AVX2 support.\" OFF)\n\noption(SNAPPY_INSTALL \"Install Snappy's header and library\" ON)\n\ninclude(TestBigEndian)\ntest_big_endian(SNAPPY_IS_BIG_ENDIAN)\n\ninclude(CheckIncludeFile)\ncheck_include_file(\"sys/mman.h\" HAVE_SYS_MMAN_H)\ncheck_include_file(\"sys/resource.h\" HAVE_SYS_RESOURCE_H)\ncheck_include_file(\"sys/time.h\" HAVE_SYS_TIME_H)\ncheck_include_file(\"sys/uio.h\" HAVE_SYS_UIO_H)\ncheck_include_file(\"unistd.h\" HAVE_UNISTD_H)\ncheck_include_file(\"windows.h\" HAVE_WINDOWS_H)\n\ninclude(CheckLibraryExists)\ncheck_library_exists(z zlibVersion \"\" HAVE_LIBZ)\ncheck_library_exists(lzo2 lzo1x_1_15_compress \"\" HAVE_LIBLZO2)\ncheck_library_exists(lz4 LZ4_compress_default \"\" HAVE_LIBLZ4)\n\ninclude(CheckCXXCompilerFlag)\nCHECK_CXX_COMPILER_FLAG(\"/arch:AVX\" HAVE_VISUAL_STUDIO_ARCH_AVX)\nCHECK_CXX_COMPILER_FLAG(\"/arch:AVX2\" HAVE_VISUAL_STUDIO_ARCH_AVX2)\nCHECK_CXX_COMPILER_FLAG(\"-mavx\" HAVE_CLANG_MAVX)\nCHECK_CXX_COMPILER_FLAG(\"-mbmi2\" HAVE_CLANG_MBMI2)\nif(SNAPPY_REQUIRE_AVX2)\n  if(HAVE_VISUAL_STUDIO_ARCH_AVX2)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:AVX2\")\n  endif(HAVE_VISUAL_STUDIO_ARCH_AVX2)\n  if(HAVE_CLANG_MAVX)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mavx\")\n  endif(HAVE_CLANG_MAVX)\n  if(HAVE_CLANG_MBMI2)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mbmi2\")\n  endif(HAVE_CLANG_MBMI2)\nelseif (SNAPPY_REQUIRE_AVX)\n  if(HAVE_VISUAL_STUDIO_ARCH_AVX)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /arch:AVX\")\n  endif(HAVE_VISUAL_STUDIO_ARCH_AVX)\n  if(HAVE_CLANG_MAVX)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -mavx\")\n  endif(HAVE_CLANG_MAVX)\nendif(SNAPPY_REQUIRE_AVX2)\n\n# Used by googletest.\ncheck_cxx_compiler_flag(-Wno-missing-field-initializers\n                        SNAPPY_HAVE_NO_MISSING_FIELD_INITIALIZERS)\ncheck_cxx_compiler_flag(-Wno-implicit-int-float-conversion\n                        SNAPPY_HAVE_NO_IMPLICIT_INT_FLOAT_CONVERSION)\n\ninclude(CheckCXXSourceCompiles)\ncheck_cxx_source_compiles(\"\nint main() {\n  return __builtin_expect(0, 1);\n}\" HAVE_BUILTIN_EXPECT)\n\ncheck_cxx_source_compiles(\"\nint main() {\n  return __builtin_ctzll(0);\n}\" HAVE_BUILTIN_CTZ)\n\ncheck_cxx_source_compiles(\"\nint main() {\n  __builtin_prefetch(0, 0, 3);\n  return 0;\n}\" HAVE_BUILTIN_PREFETCH)\n\ncheck_cxx_source_compiles(\"\n__attribute__((always_inline)) int zero() { return 0; }\n\nint main() {\n  return zero();\n}\" HAVE_ATTRIBUTE_ALWAYS_INLINE)\n\ncheck_cxx_source_compiles(\"\n#include <tmmintrin.h>\n\nint main() {\n  const __m128i *src = 0;\n  __m128i dest;\n  const __m128i shuffle_mask = _mm_load_si128(src);\n  const __m128i pattern = _mm_shuffle_epi8(_mm_loadl_epi64(src), shuffle_mask);\n  _mm_storeu_si128(&dest, pattern);\n  return 0;\n}\" SNAPPY_HAVE_SSSE3)\n\ncheck_cxx_source_compiles(\"\n#include <immintrin.h>\nint main() {\n  return _mm_crc32_u32(0, 1);\n}\" SNAPPY_HAVE_X86_CRC32)\n\ncheck_cxx_source_compiles(\"\n#include <arm_neon.h>\n#include <arm_acle.h>\nint main() {\n  return __crc32cw(0, 1);\n}\" SNAPPY_HAVE_NEON_CRC32)\n\ncheck_cxx_source_compiles(\"\n#include <immintrin.h>\nint main() {\n  return _bzhi_u32(0, 1);\n}\" SNAPPY_HAVE_BMI2)\n\ncheck_cxx_source_compiles(\"\n#include <arm_neon.h>\n#include <stdint.h>\nint main() {\n  uint8_t val = 3, dup[8];\n  uint8x16_t v1 = vld1q_dup_u8(&val);\n  uint8x16_t v2 = vqtbl1q_u8(v1, v1);\n  vst1q_u8(dup, v1);\n  vst1q_u8(dup, v2);\n  return 0;\n}\" SNAPPY_HAVE_NEON)\n\ninclude(CheckSymbolExists)\ncheck_symbol_exists(\"mmap\" \"sys/mman.h\" HAVE_FUNC_MMAP)\ncheck_symbol_exists(\"sysconf\" \"unistd.h\" HAVE_FUNC_SYSCONF)\n\nconfigure_file(\n  \"cmake/config.h.in\"\n  \"${PROJECT_BINARY_DIR}/config.h\"\n)\n\n# We don't want to define HAVE_ macros in public headers. Instead, we use\n# CMake's variable substitution with 0/1 variables, which will be seen by the\n# preprocessor as constants.\nset(HAVE_SYS_UIO_H_01 ${HAVE_SYS_UIO_H})\nif(NOT HAVE_SYS_UIO_H_01)\n  set(HAVE_SYS_UIO_H_01 0)\nendif(NOT HAVE_SYS_UIO_H_01)\n\nif (SNAPPY_FUZZING_BUILD)\n  if (NOT \"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n    message(WARNING \"Fuzzing builds are only supported with Clang\")\n  endif (NOT \"${CMAKE_CXX_COMPILER_ID}\" MATCHES \"Clang\")\n\n  if(NOT CMAKE_CXX_FLAGS MATCHES \"-fsanitize=address\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=address\")\n  endif(NOT CMAKE_CXX_FLAGS MATCHES \"-fsanitize=address\")\n\n  if(NOT CMAKE_CXX_FLAGS MATCHES \"-fsanitize=fuzzer-no-link\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fsanitize=fuzzer-no-link\")\n  endif(NOT CMAKE_CXX_FLAGS MATCHES \"-fsanitize=fuzzer-no-link\")\nendif (SNAPPY_FUZZING_BUILD)\n\nconfigure_file(\n  \"snappy-stubs-public.h.in\"\n  \"${PROJECT_BINARY_DIR}/snappy-stubs-public.h\")\n\nadd_library(snappy \"\")\ntarget_sources(snappy\n  PRIVATE\n    \"snappy-internal.h\"\n    \"snappy-stubs-internal.h\"\n    \"snappy-c.cc\"\n    \"snappy-sinksource.cc\"\n    \"snappy-stubs-internal.cc\"\n    \"snappy.cc\"\n    \"${PROJECT_BINARY_DIR}/config.h\"\n\n  # Only CMake 3.3+ supports PUBLIC sources in targets exported by \"install\".\n  $<$<VERSION_GREATER:CMAKE_VERSION,3.2>:PUBLIC>\n    $<BUILD_INTERFACE:${PROJECT_SOURCE_DIR}/snappy-c.h>\n    $<INSTALL_INTERFACE:include/snappy-c.h>\n    $<BUILD_INTERFACE:${PROJECT_SOURCE_DIR}/snappy-sinksource.h>\n    $<INSTALL_INTERFACE:include/snappy-sinksource.h>\n    $<BUILD_INTERFACE:${PROJECT_SOURCE_DIR}/snappy.h>\n    $<INSTALL_INTERFACE:include/snappy.h>\n    $<BUILD_INTERFACE:${PROJECT_BINARY_DIR}/snappy-stubs-public.h>\n    $<INSTALL_INTERFACE:include/snappy-stubs-public.h>\n)\ntarget_include_directories(snappy\n  PUBLIC\n    $<BUILD_INTERFACE:${PROJECT_BINARY_DIR}>\n    $<BUILD_INTERFACE:${PROJECT_SOURCE_DIR}>\n    $<INSTALL_INTERFACE:include>\n)\nset_target_properties(snappy\n  PROPERTIES VERSION ${PROJECT_VERSION} SOVERSION ${PROJECT_VERSION_MAJOR})\n\ntarget_compile_definitions(snappy PRIVATE -DHAVE_CONFIG_H)\nif(BUILD_SHARED_LIBS)\n  set_target_properties(snappy PROPERTIES WINDOWS_EXPORT_ALL_SYMBOLS ON)\nendif(BUILD_SHARED_LIBS)\n\nif(SNAPPY_BUILD_TESTS OR SNAPPY_BUILD_BENCHMARKS)\n  add_library(snappy_test_support \"\")\n  target_sources(snappy_test_support\n    PRIVATE\n      \"snappy-test.cc\"\n      \"snappy-test.h\"\n      \"snappy_test_data.cc\"\n      \"snappy_test_data.h\"\n      \"${PROJECT_BINARY_DIR}/config.h\"\n  )\n\n  # Test files include snappy-test.h, HAVE_CONFIG_H must be defined.\n  target_compile_definitions(snappy_test_support PUBLIC -DHAVE_CONFIG_H)\n\n  target_link_libraries(snappy_test_support snappy)\n\n  if(HAVE_LIBZ)\n    target_link_libraries(snappy_test_support z)\n  endif(HAVE_LIBZ)\n  if(HAVE_LIBLZO2)\n    target_link_libraries(snappy_test_support lzo2)\n  endif(HAVE_LIBLZO2)\n  if(HAVE_LIBLZ4)\n    target_link_libraries(snappy_test_support lz4)\n  endif(HAVE_LIBLZ4)\n\n  target_include_directories(snappy_test_support\n    BEFORE PUBLIC\n      \"${PROJECT_SOURCE_DIR}\"\n  )\nendif(SNAPPY_BUILD_TESTS OR SNAPPY_BUILD_BENCHMARKS)\n\nif(SNAPPY_BUILD_TESTS)\n  enable_testing()\n\n  # Prevent overriding the parent project's compiler/linker settings on Windows.\n  set(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)\n  set(install_gtest OFF)\n  set(install_gmock OFF)\n  set(build_gmock ON)\n\n  # This project is tested using GoogleTest.\n  add_subdirectory(\"third_party/googletest\")\n\n  # GoogleTest triggers a missing field initializers warning.\n  if(SNAPPY_HAVE_NO_MISSING_FIELD_INITIALIZERS)\n    set_property(TARGET gtest\n        APPEND PROPERTY COMPILE_OPTIONS -Wno-missing-field-initializers)\n    set_property(TARGET gmock\n        APPEND PROPERTY COMPILE_OPTIONS -Wno-missing-field-initializers)\n  endif(SNAPPY_HAVE_NO_MISSING_FIELD_INITIALIZERS)\n\n  if(SNAPPY_HAVE_NO_IMPLICIT_INT_FLOAT_CONVERSION)\n    set_property(TARGET gtest\n        APPEND PROPERTY COMPILE_OPTIONS -Wno-implicit-int-float-conversion)\n  endif(SNAPPY_HAVE_NO_IMPLICIT_INT_FLOAT_CONVERSION)\n\n  add_executable(snappy_unittest \"\")\n  target_sources(snappy_unittest\n    PRIVATE\n      \"snappy_unittest.cc\"\n  )\n  target_link_libraries(snappy_unittest snappy_test_support gmock_main gtest)\n\n  add_test(\n    NAME snappy_unittest\n    WORKING_DIRECTORY \"${PROJECT_SOURCE_DIR}\"\n    COMMAND \"${PROJECT_BINARY_DIR}/snappy_unittest\")\n\n  add_executable(snappy_test_tool \"\")\n  target_sources(snappy_test_tool\n    PRIVATE\n      \"snappy_test_tool.cc\"\n  )\n  target_link_libraries(snappy_test_tool snappy_test_support)\nendif(SNAPPY_BUILD_TESTS)\n\nif(SNAPPY_BUILD_BENCHMARKS)\n  add_executable(snappy_benchmark \"\")\n  target_sources(snappy_benchmark\n    PRIVATE\n      \"snappy_benchmark.cc\"\n  )\n  target_link_libraries(snappy_benchmark snappy_test_support benchmark_main)\n\n  # This project uses Google benchmark for benchmarking.\n  set(BENCHMARK_ENABLE_TESTING OFF CACHE BOOL \"\" FORCE)\n  set(BENCHMARK_ENABLE_EXCEPTIONS OFF CACHE BOOL \"\" FORCE)\n  add_subdirectory(\"third_party/benchmark\")\nendif(SNAPPY_BUILD_BENCHMARKS)\n\nif(SNAPPY_FUZZING_BUILD)\n  add_executable(snappy_compress_fuzzer \"\")\n  target_sources(snappy_compress_fuzzer\n    PRIVATE \"snappy_compress_fuzzer.cc\"\n  )\n  target_link_libraries(snappy_compress_fuzzer snappy)\n  set_target_properties(snappy_compress_fuzzer\n    PROPERTIES LINK_FLAGS \"-fsanitize=fuzzer\"\n  )\n\n  add_executable(snappy_uncompress_fuzzer \"\")\n  target_sources(snappy_uncompress_fuzzer\n    PRIVATE \"snappy_uncompress_fuzzer.cc\"\n  )\n  target_link_libraries(snappy_uncompress_fuzzer snappy)\n  set_target_properties(snappy_uncompress_fuzzer\n    PROPERTIES LINK_FLAGS \"-fsanitize=fuzzer\"\n  )\nendif(SNAPPY_FUZZING_BUILD)\n\n# Must be included before CMAKE_INSTALL_INCLUDEDIR is used.\ninclude(GNUInstallDirs)\n\nif(SNAPPY_INSTALL)\n  install(TARGETS snappy\n    EXPORT SnappyTargets\n    RUNTIME DESTINATION ${CMAKE_INSTALL_BINDIR}\n    LIBRARY DESTINATION ${CMAKE_INSTALL_LIBDIR}\n    ARCHIVE DESTINATION ${CMAKE_INSTALL_LIBDIR}\n  )\n  install(\n    FILES\n      \"snappy-c.h\"\n      \"snappy-sinksource.h\"\n      \"snappy.h\"\n      \"${PROJECT_BINARY_DIR}/snappy-stubs-public.h\"\n    DESTINATION \"${CMAKE_INSTALL_INCLUDEDIR}\"\n  )\n\n  include(CMakePackageConfigHelpers)\n  configure_package_config_file(\n    \"cmake/${PROJECT_NAME}Config.cmake.in\"\n    \"${PROJECT_BINARY_DIR}/cmake/${PROJECT_NAME}Config.cmake\"\n    INSTALL_DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\"\n  )\n  write_basic_package_version_file(\n    \"${PROJECT_BINARY_DIR}/cmake/${PROJECT_NAME}ConfigVersion.cmake\"\n    COMPATIBILITY SameMajorVersion\n  )\n  install(\n    EXPORT SnappyTargets\n    NAMESPACE Snappy::\n    DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\"\n  )\n  install(\n    FILES\n      \"${PROJECT_BINARY_DIR}/cmake/${PROJECT_NAME}Config.cmake\"\n      \"${PROJECT_BINARY_DIR}/cmake/${PROJECT_NAME}ConfigVersion.cmake\"\n    DESTINATION \"${CMAKE_INSTALL_LIBDIR}/cmake/${PROJECT_NAME}\"\n  )\nendif(SNAPPY_INSTALL)\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.1962890625,
          "content": "# How to Contribute\n\nWe'd love to accept your patches and contributions to this project. There are\njust a few small guidelines you need to follow.\n\n## Contributor License Agreement\n\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to <https://cla.developers.google.com/> to see\nyour current agreements on file or to sign a new one.\n\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\n\n## Code Reviews\n\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\n[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more\ninformation on using pull requests.\n\nSee [the README](README.md#contributing-to-the-snappy-project) for areas\nwhere we are likely to accept external contributions.\n\n## Community Guidelines\n\nThis project follows [Google's Open Source Community\nGuidelines](https://opensource.google/conduct/).\n"
        },
        {
          "name": "COPYING",
          "type": "blob",
          "size": 2.5830078125,
          "content": "Copyright 2011, Google Inc.\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are\nmet:\n\n    * Redistributions of source code must retain the above copyright\nnotice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above\ncopyright notice, this list of conditions and the following disclaimer\nin the documentation and/or other materials provided with the\ndistribution.\n    * Neither the name of Google Inc. nor the names of its\ncontributors may be used to endorse or promote products derived from\nthis software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n\"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\nLIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\nA PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\nOWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\nSPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\nLIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\nDATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\nTHEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n===\n\nSome of the benchmark data in testdata/ is licensed differently:\n\n - fireworks.jpeg is Copyright 2013 Steinar H. Gunderson, and\n   is licensed under the Creative Commons Attribution 3.0 license\n   (CC-BY-3.0). See https://creativecommons.org/licenses/by/3.0/\n   for more information.\n\n - kppkn.gtb is taken from the Gaviota chess tablebase set, and\n   is licensed under the MIT License. See\n   https://sites.google.com/site/gaviotachessengine/Home/endgame-tablebases-1\n   for more information.\n\n - paper-100k.pdf is an excerpt (bytes 92160 to 194560) from the paper\n   “Combinatorial Modeling of Chromatin Features Quantitatively Predicts DNA\n   Replication Timing in _Drosophila_” by Federico Comoglio and Renato Paro,\n   which is licensed under the CC-BY license. See\n   http://www.ploscompbiol.org/static/license for more ifnormation.\n\n - alice29.txt, asyoulik.txt, plrabn12.txt and lcet10.txt are from Project\n   Gutenberg. The first three have expired copyrights and are in the public\n   domain; the latter does not have expired copyright, but is still in the\n   public domain according to the license information\n   (http://www.gutenberg.org/ebooks/53).\n"
        },
        {
          "name": "MODULE.bazel",
          "type": "blob",
          "size": 0.4052734375,
          "content": "module(\n    name = \"snappy\",\n    version = \"1.2.1\",\n    compatibility_level = 1,\n)\n\nbazel_dep(\n    name = \"googletest\",\n    version = \"1.14.0.bcr.1\",\n    dev_dependency = True,\n    repo_name = \"com_google_googletest\",\n)\nbazel_dep(\n    name = \"google_benchmark\",\n    version = \"1.9.0\",\n    dev_dependency = True,\n    repo_name = \"com_google_benchmark\",\n)\n\nbazel_dep(\n    name = \"platforms\",\n    version = \"0.0.9\",\n)\n"
        },
        {
          "name": "NEWS",
          "type": "blob",
          "size": 5.94140625,
          "content": "Snappy v1.1.10, Mar 8th 2023:\n\n  * Performance improvements\n\n  * Compilation fixes for various environments\n\nSnappy v1.1.9, May 4th 2021:\n\n  * Performance improvements.\n\n  * Google Test and Google Benchmark are now bundled in third_party/.\n\nSnappy v1.1.8, January 15th 2020:\n\n  * Small performance improvements.\n\n  * Removed snappy::string alias for std::string.\n\n  * Improved CMake configuration.\n\nSnappy v1.1.7, August 24th 2017:\n\n  * Improved CMake build support for 64-bit Linux distributions.\n\n  * MSVC builds now use MSVC-specific intrinsics that map to clzll.\n\n  * ARM64 (AArch64) builds use the code paths optimized for 64-bit processors.\n\nSnappy v1.1.6, July 12th 2017:\n\nThis is a re-release of v1.1.5 with proper SONAME / SOVERSION values.\n\nSnappy v1.1.5, June 28th 2017:\n\nThis release has broken SONAME / SOVERSION values. Users of snappy as a shared\nlibrary should avoid 1.1.5 and use 1.1.6 instead. SONAME / SOVERSION errors will\nmanifest as the dynamic library loader complaining that it cannot find snappy's\nshared library file (libsnappy.so / libsnappy.dylib), or that the library it\nfound does not have the required version. 1.1.6 has the same code as 1.1.5, but\ncarries build configuration fixes for the issues above.\n\n  * Add CMake build support. The autoconf build support is now deprecated, and\n    will be removed in the next release.\n\n  * Add AppVeyor configuration, for Windows CI coverage.\n\n  * Small performance improvement on little-endian PowerPC.\n\n  * Small performance improvement on LLVM with position-independent executables.\n\n  * Fix a few issues with various build environments.\n\nSnappy v1.1.4, January 25th 2017:\n\n  * Fix a 1% performance regression when snappy is used in PIE executables.\n\n  * Improve compression performance by 5%.\n\n  * Improve decompression performance by 20%.\n\nSnappy v1.1.3, July 6th 2015:\n\nThis is the first release to be done from GitHub, which means that\nsome minor things like the ChangeLog format has changed (git log\nformat instead of svn log).\n\n  * Add support for Uncompress() from a Source to a Sink.\n\n  * Various minor changes to improve MSVC support; in particular,\n    the unit tests now compile and run under MSVC.\n\n\nSnappy v1.1.2, February 28th 2014:\n\nThis is a maintenance release with no changes to the actual library\nsource code.\n\n  * Stop distributing benchmark data files that have unclear\n    or unsuitable licensing.\n\n  * Add support for padding chunks in the framing format.\n\n\nSnappy v1.1.1, October 15th 2013:\n\n  * Add support for uncompressing to iovecs (scatter I/O).\n    The bulk of this patch was contributed by Mohit Aron.\n\n  * Speed up decompression by ~2%; much more so (~13-20%) on\n    a few benchmarks on given compilers and CPUs.\n\n  * Fix a few issues with MSVC compilation.\n\n  * Support truncated test data in the benchmark.\n\n\nSnappy v1.1.0, January 18th 2013:\n\n  * Snappy now uses 64 kB block size instead of 32 kB. On average,\n    this means it compresses about 3% denser (more so for some\n    inputs), at the same or better speeds.\n\n  * libsnappy no longer depends on iostream.\n\n  * Some small performance improvements in compression on x86\n    (0.5–1%).\n\n  * Various portability fixes for ARM-based platforms, for MSVC,\n    and for GNU/Hurd.\n\n\nSnappy v1.0.5, February 24th 2012:\n\n  * More speed improvements. Exactly how big will depend on\n    the architecture:\n\n    - 3–10% faster decompression for the base case (x86-64).\n\n    - ARMv7 and higher can now use unaligned accesses,\n      and will see about 30% faster decompression and\n      20–40% faster compression.\n\n    - 32-bit platforms (ARM and 32-bit x86) will see 2–5%\n      faster compression.\n\n    These are all cumulative (e.g., ARM gets all three speedups).\n\n  * Fixed an issue where the unit test would crash on system\n    with less than 256 MB address space available,\n    e.g. some embedded platforms.\n\n  * Added a framing format description, for use over e.g. HTTP,\n    or for a command-line compressor. We do not have any\n    implementations of this at the current point, but there seems\n    to be enough of a general interest in the topic.\n    Also make the format description slightly clearer.\n\n  * Remove some compile-time warnings in -Wall\n    (mostly signed/unsigned comparisons), for easier embedding\n    into projects that use -Wall -Werror.\n\n\nSnappy v1.0.4, September 15th 2011:\n\n  * Speeded up the decompressor somewhat; typically about 2–8%\n    for Core i7, in 64-bit mode (comparable for Opteron).\n    Somewhat more for some tests, almost no gain for others.\n  \n  * Make Snappy compile on certain platforms it didn't before\n    (Solaris with SunPro C++, HP-UX, AIX).\n\n  * Correct some minor errors in the format description.\n\n\nSnappy v1.0.3, June 2nd 2011:\n\n  * Speeded up the decompressor somewhat; about 3-6% for Core 2,\n    6-13% for Core i7, and 5-12% for Opteron (all in 64-bit mode).\n\n  * Added compressed format documentation. This text is new,\n    but an earlier version from Zeev Tarantov was used as reference.\n\n  * Only link snappy_unittest against -lz and other autodetected\n    libraries, not libsnappy.so (which doesn't need any such dependency).\n\n  * Fixed some display issues in the microbenchmarks, one of which would\n    frequently make the test crash on GNU/Hurd.\n\n\nSnappy v1.0.2, April 29th 2011:\n\n  * Relicense to a BSD-type license.\n\n  * Added C bindings, contributed by Martin Gieseking.\n\n  * More Win32 fixes, in particular for MSVC.\n\n  * Replace geo.protodata with a newer version.\n\n  * Fix timing inaccuracies in the unit test when comparing Snappy\n    to other algorithms.\n\n\nSnappy v1.0.1, March 25th 2011:\n\nThis is a maintenance release, mostly containing minor fixes.\nThere is no new functionality. The most important fixes include:\n\n  * The COPYING file and all licensing headers now correctly state that\n    Snappy is licensed under the Apache 2.0 license.\n\n  * snappy_unittest should now compile natively under Windows,\n    as well as on embedded systems with no mmap().\n\n  * Various autotools nits have been fixed.\n\n\nSnappy v1.0, March 17th 2011:\n\n  * Initial version.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 6.533203125,
          "content": "Snappy, a fast compressor/decompressor.\n\n[![Build Status](https://github.com/google/snappy/actions/workflows/build.yml/badge.svg)](https://github.com/google/snappy/actions/workflows/build.yml)\n\nIntroduction\n============\n\nSnappy is a compression/decompression library. It does not aim for maximum\ncompression, or compatibility with any other compression library; instead,\nit aims for very high speeds and reasonable compression. For instance,\ncompared to the fastest mode of zlib, Snappy is an order of magnitude faster\nfor most inputs, but the resulting compressed files are anywhere from 20% to\n100% bigger. (For more information, see \"Performance\", below.)\n\nSnappy has the following properties:\n\n * Fast: Compression speeds at 250 MB/sec and beyond, with no assembler code.\n   See \"Performance\" below.\n * Stable: Over the last few years, Snappy has compressed and decompressed\n   petabytes of data in Google's production environment. The Snappy bitstream\n   format is stable and will not change between versions.\n * Robust: The Snappy decompressor is designed not to crash in the face of\n   corrupted or malicious input.\n * Free and open source software: Snappy is licensed under a BSD-type license.\n   For more information, see the included COPYING file.\n\nSnappy has previously been called \"Zippy\" in some Google presentations\nand the like.\n\n\nPerformance\n===========\n\nSnappy is intended to be fast. On a single core of a Core i7 processor\nin 64-bit mode, it compresses at about 250 MB/sec or more and decompresses at\nabout 500 MB/sec or more. (These numbers are for the slowest inputs in our\nbenchmark suite; others are much faster.) In our tests, Snappy usually\nis faster than algorithms in the same class (e.g. LZO, LZF, QuickLZ,\netc.) while achieving comparable compression ratios.\n\nTypical compression ratios (based on the benchmark suite) are about 1.5-1.7x\nfor plain text, about 2-4x for HTML, and of course 1.0x for JPEGs, PNGs and\nother already-compressed data. Similar numbers for zlib in its fastest mode\nare 2.6-2.8x, 3-7x and 1.0x, respectively. More sophisticated algorithms are\ncapable of achieving yet higher compression rates, although usually at the\nexpense of speed. Of course, compression ratio will vary significantly with\nthe input.\n\nAlthough Snappy should be fairly portable, it is primarily optimized\nfor 64-bit x86-compatible processors, and may run slower in other environments.\nIn particular:\n\n - Snappy uses 64-bit operations in several places to process more data at\n   once than would otherwise be possible.\n - Snappy assumes unaligned 32 and 64-bit loads and stores are cheap.\n   On some platforms, these must be emulated with single-byte loads\n   and stores, which is much slower.\n - Snappy assumes little-endian throughout, and needs to byte-swap data in\n   several places if running on a big-endian platform.\n\nExperience has shown that even heavily tuned code can be improved.\nPerformance optimizations, whether for 64-bit x86 or other platforms,\nare of course most welcome; see \"Contact\", below.\n\n\nBuilding\n========\n\nYou need the CMake version specified in [CMakeLists.txt](./CMakeLists.txt)\nor later to build:\n\n```bash\ngit submodule update --init\nmkdir build\ncd build && cmake ../ && make\n```\n\nUsage\n=====\n\nNote that Snappy, both the implementation and the main interface,\nis written in C++. However, several third-party bindings to other languages\nare available; see the [home page](docs/README.md) for more information.\nAlso, if you want to use Snappy from C code, you can use the included C\nbindings in snappy-c.h.\n\nTo use Snappy from your own C++ program, include the file \"snappy.h\" from\nyour calling file, and link against the compiled library.\n\nThere are many ways to call Snappy, but the simplest possible is\n\n```c++\nsnappy::Compress(input.data(), input.size(), &output);\n```\n\nand similarly\n\n```c++\nsnappy::Uncompress(input.data(), input.size(), &output);\n```\n\nwhere \"input\" and \"output\" are both instances of std::string.\n\nThere are other interfaces that are more flexible in various ways, including\nsupport for custom (non-array) input sources. See the header file for more\ninformation.\n\n\nTests and benchmarks\n====================\n\nWhen you compile Snappy, the following binaries are compiled in addition to the\nlibrary itself. You do not need them to use the compressor from your own\nlibrary, but they are useful for Snappy development.\n\n* `snappy_benchmark` contains microbenchmarks used to tune compression and\n  decompression performance.\n* `snappy_unittests` contains unit tests, verifying correctness on your machine\n  in various scenarios.\n* `snappy_test_tool` can benchmark Snappy against a few other compression\n  libraries (zlib, LZO, LZF, and QuickLZ), if they were detected at configure\n  time. To benchmark using a given file, give the compression algorithm you want\n  to test Snappy against (e.g. --zlib) and then a list of one or more file names\n  on the command line.\n\nIf you want to change or optimize Snappy, please run the tests and benchmarks to\nverify you have not broken anything.\n\nThe testdata/ directory contains the files used by the microbenchmarks, which\nshould provide a reasonably balanced starting point for benchmarking. (Note that\nbaddata[1-3].snappy are not intended as benchmarks; they are used to verify\ncorrectness in the presence of corrupted data in the unit test.)\n\nContributing to the Snappy Project\n==================================\n\nIn addition to the aims listed at the top of the [README](README.md) Snappy\nexplicitly supports the following:\n\n1. C++11\n2. Clang (gcc and MSVC are best-effort).\n3. Low level optimizations (e.g. assembly or equivalent intrinsics) for:\n     - [x86](https://en.wikipedia.org/wiki/X86)\n     - [x86-64](https://en.wikipedia.org/wiki/X86-64)\n     - ARMv7 (32-bit)\n     - ARMv8 (AArch64)\n4. Supports only the Snappy compression scheme as described in\n  [format_description.txt](format_description.txt).\n5. CMake for building\n\nChanges adding features or dependencies outside of the core area of focus listed\nabove might not be accepted. If in doubt post a message to the\n[Snappy discussion mailing list](https://groups.google.com/g/snappy-compression).\n\nWe are unlikely to accept contributions to the build configuration files, such\nas `CMakeLists.txt`. We are focused on maintaining a build configuration that\nallows us to test that the project works in a few supported configurations\ninside Google. We are not currently interested in supporting other requirements,\nsuch as different operating systems, compilers, or build systems.\n\nContact\n=======\n\nSnappy is distributed through GitHub. For the latest version and other\ninformation, see https://github.com/google/snappy.\n"
        },
        {
          "name": "WORKSPACE",
          "type": "blob",
          "size": 1.4892578125,
          "content": "# Copyright 2023 Google Inc. All Rights Reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n#\n#     * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#     * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n#\n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n"
        },
        {
          "name": "WORKSPACE.bzlmod",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "format_description.txt",
          "type": "blob",
          "size": 4.28515625,
          "content": "Snappy compressed format description\nLast revised: 2011-10-05\n\n\nThis is not a formal specification, but should suffice to explain most\nrelevant parts of how the Snappy format works. It is originally based on\ntext by Zeev Tarantov.\n\nSnappy is a LZ77-type compressor with a fixed, byte-oriented encoding.\nThere is no entropy encoder backend nor framing layer -- the latter is\nassumed to be handled by other parts of the system.\n\nThis document only describes the format, not how the Snappy compressor nor\ndecompressor actually works. The correctness of the decompressor should not\ndepend on implementation details of the compressor, and vice versa.\n\n\n1. Preamble\n\nThe stream starts with the uncompressed length (up to a maximum of 2^32 - 1),\nstored as a little-endian varint. Varints consist of a series of bytes,\nwhere the lower 7 bits are data and the upper bit is set iff there are\nmore bytes to be read. In other words, an uncompressed length of 64 would\nbe stored as 0x40, and an uncompressed length of 2097150 (0x1FFFFE)\nwould be stored as 0xFE 0xFF 0x7F.\n\n\n2. The compressed stream itself\n\nThere are two types of elements in a Snappy stream: Literals and\ncopies (backreferences). There is no restriction on the order of elements,\nexcept that the stream naturally cannot start with a copy. (Having\ntwo literals in a row is never optimal from a compression point of\nview, but nevertheless fully permitted.) Each element starts with a tag byte,\nand the lower two bits of this tag byte signal what type of element will\nfollow:\n\n  00: Literal\n  01: Copy with 1-byte offset\n  10: Copy with 2-byte offset\n  11: Copy with 4-byte offset\n\nThe interpretation of the upper six bits are element-dependent.\n\n\n2.1. Literals (00)\n\nLiterals are uncompressed data stored directly in the byte stream.\nThe literal length is stored differently depending on the length\nof the literal:\n\n - For literals up to and including 60 bytes in length, the upper\n   six bits of the tag byte contain (len-1). The literal follows\n   immediately thereafter in the bytestream.\n - For longer literals, the (len-1) value is stored after the tag byte,\n   little-endian. The upper six bits of the tag byte describe how\n   many bytes are used for the length; 60, 61, 62 or 63 for\n   1-4 bytes, respectively. The literal itself follows after the\n   length.\n\n\n2.2. Copies\n\nCopies are references back into previous decompressed data, telling\nthe decompressor to reuse data it has previously decoded.\nThey encode two values: The _offset_, saying how many bytes back\nfrom the current position to read, and the _length_, how many bytes\nto copy. Offsets of zero can be encoded, but are not legal;\nsimilarly, it is possible to encode backreferences that would\ngo past the end of the block (offset > current decompressed position),\nwhich is also nonsensical and thus not allowed.\n\nAs in most LZ77-based compressors, the length can be larger than the offset,\nyielding a form of run-length encoding (RLE). For instance,\n\"xababab\" could be encoded as\n\n  <literal: \"xab\"> <copy: offset=2 length=4>\n\nNote that since the current Snappy compressor works in 32 kB\nblocks and does not do matching across blocks, it will never produce\na bitstream with offsets larger than about 32768. However, the\ndecompressor should not rely on this, as it may change in the future.\n\nThere are several different kinds of copy elements, depending on\nthe amount of bytes to be copied (length), and how far back the\ndata to be copied is (offset).\n\n\n2.2.1. Copy with 1-byte offset (01)\n\nThese elements can encode lengths between [4..11] bytes and offsets\nbetween [0..2047] bytes. (len-4) occupies three bits and is stored\nin bits [2..4] of the tag byte. The offset occupies 11 bits, of which the\nupper three are stored in the upper three bits ([5..7]) of the tag byte,\nand the lower eight are stored in a byte following the tag byte.\n\n\n2.2.2. Copy with 2-byte offset (10)\n\nThese elements can encode lengths between [1..64] and offsets from\n[0..65535]. (len-1) occupies six bits and is stored in the upper\nsix bits ([2..7]) of the tag byte. The offset is stored as a\nlittle-endian 16-bit integer in the two bytes following the tag byte.\n\n\n2.2.3. Copy with 4-byte offset (11)\n\nThese are like the copies with 2-byte offsets (see previous subsection),\nexcept that the offset is stored as a 32-bit integer instead of a\n16-bit integer (and thus will occupy four bytes).\n"
        },
        {
          "name": "framing_format.txt",
          "type": "blob",
          "size": 4.9208984375,
          "content": "Snappy framing format description\nLast revised: 2013-10-25\n\nThis format decribes a framing format for Snappy, allowing compressing to\nfiles or streams that can then more easily be decompressed without having\nto hold the entire stream in memory. It also provides data checksums to\nhelp verify integrity. It does not provide metadata checksums, so it does\nnot protect against e.g. all forms of truncations.\n\nImplementation of the framing format is optional for Snappy compressors and\ndecompressor; it is not part of the Snappy core specification.\n\n\n1. General structure\n\nThe file consists solely of chunks, lying back-to-back with no padding\nin between. Each chunk consists first a single byte of chunk identifier,\nthen a three-byte little-endian length of the chunk in bytes (from 0 to\n16777215, inclusive), and then the data if any. The four bytes of chunk\nheader is not counted in the data length.\n\nThe different chunk types are listed below. The first chunk must always\nbe the stream identifier chunk (see section 4.1, below). The stream\nends when the file ends -- there is no explicit end-of-file marker.\n\n\n2. File type identification\n\nThe following identifiers for this format are recommended where appropriate.\nHowever, note that none have been registered officially, so this is only to\nbe taken as a guideline. We use \"Snappy framed\" to distinguish between this\nformat and raw Snappy data.\n\n  File extension:         .sz\n  MIME type:              application/x-snappy-framed\n  HTTP Content-Encoding:  x-snappy-framed\n\n\n3. Checksum format\n\nSome chunks have data protected by a checksum (the ones that do will say so\nexplicitly). The checksums are always masked CRC-32Cs.\n\nA description of CRC-32C can be found in RFC 3720, section 12.1, with\nexamples in section B.4.\n\nChecksums are not stored directly, but masked, as checksumming data and\nthen its own checksum can be problematic. The masking is the same as used\nin Apache Hadoop: Rotate the checksum by 15 bits, then add the constant\n0xa282ead8 (using wraparound as normal for unsigned integers). This is\nequivalent to the following C code:\n\n  uint32_t mask_checksum(uint32_t x) {\n    return ((x >> 15) | (x << 17)) + 0xa282ead8;\n  }\n\nNote that the masking is reversible.\n\nThe checksum is always stored as a four bytes long integer, in little-endian.\n\n\n4. Chunk types\n\nThe currently supported chunk types are described below. The list may\nbe extended in the future.\n\n\n4.1. Stream identifier (chunk type 0xff)\n\nThe stream identifier is always the first element in the stream.\nIt is exactly six bytes long and contains \"sNaPpY\" in ASCII. This means that\na valid Snappy framed stream always starts with the bytes\n\n  0xff 0x06 0x00 0x00 0x73 0x4e 0x61 0x50 0x70 0x59\n\nThe stream identifier chunk can come multiple times in the stream besides\nthe first; if such a chunk shows up, it should simply be ignored, assuming\nit has the right length and contents. This allows for easy concatenation of\ncompressed files without the need for re-framing.\n\n\n4.2. Compressed data (chunk type 0x00)\n\nCompressed data chunks contain a normal Snappy compressed bitstream;\nsee the compressed format specification. The compressed data is preceded by\nthe CRC-32C (see section 3) of the _uncompressed_ data.\n\nNote that the data portion of the chunk, i.e., the compressed contents,\ncan be at most 16777211 bytes (2^24 - 1, minus the checksum).\nHowever, we place an additional restriction that the uncompressed data\nin a chunk must be no longer than 65536 bytes. This allows consumers to\neasily use small fixed-size buffers.\n\n\n4.3. Uncompressed data (chunk type 0x01)\n\nUncompressed data chunks allow a compressor to send uncompressed,\nraw data; this is useful if, for instance, uncompressible or\nnear-incompressible data is detected, and faster decompression is desired.\n\nAs in the compressed chunks, the data is preceded by its own masked\nCRC-32C (see section 3).\n\nAn uncompressed data chunk, like compressed data chunks, should contain\nno more than 65536 data bytes, so the maximum legal chunk length with the\nchecksum is 65540.\n\n\n4.4. Padding (chunk type 0xfe)\n\nPadding chunks allow a compressor to increase the size of the data stream\nso that it complies with external demands, e.g. that the total number of\nbytes is a multiple of some value.\n\nAll bytes of the padding chunk, except the chunk byte itself and the length,\nshould be zero, but decompressors must not try to interpret or verify the\npadding data in any way.\n\n\n4.5. Reserved unskippable chunks (chunk types 0x02-0x7f)\n\nThese are reserved for future expansion. A decoder that sees such a chunk\nshould immediately return an error, as it must assume it cannot decode the\nstream correctly.\n\nFuture versions of this specification may define meanings for these chunks.\n\n\n4.6. Reserved skippable chunks (chunk types 0x80-0xfd)\n\nThese are also reserved for future expansion, but unlike the chunks\ndescribed in 4.5, a decoder seeing these must skip them and continue\ndecoding.\n\nFuture versions of this specification may define meanings for these chunks.\n"
        },
        {
          "name": "snappy-c.cc",
          "type": "blob",
          "size": 3.5517578125,
          "content": "// Copyright 2011 Martin Gieseking <martin.gieseking@uos.de>.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#include \"snappy.h\"\n#include \"snappy-c.h\"\n\nextern \"C\" {\n\nsnappy_status snappy_compress(const char* input,\n                              size_t input_length,\n                              char* compressed,\n                              size_t *compressed_length) {\n  if (*compressed_length < snappy_max_compressed_length(input_length)) {\n    return SNAPPY_BUFFER_TOO_SMALL;\n  }\n  snappy::RawCompress(input, input_length, compressed, compressed_length);\n  return SNAPPY_OK;\n}\n\nsnappy_status snappy_uncompress(const char* compressed,\n                                size_t compressed_length,\n                                char* uncompressed,\n                                size_t* uncompressed_length) {\n  size_t real_uncompressed_length;\n  if (!snappy::GetUncompressedLength(compressed,\n                                     compressed_length,\n                                     &real_uncompressed_length)) {\n    return SNAPPY_INVALID_INPUT;\n  }\n  if (*uncompressed_length < real_uncompressed_length) {\n    return SNAPPY_BUFFER_TOO_SMALL;\n  }\n  if (!snappy::RawUncompress(compressed, compressed_length, uncompressed)) {\n    return SNAPPY_INVALID_INPUT;\n  }\n  *uncompressed_length = real_uncompressed_length;\n  return SNAPPY_OK;\n}\n\nsize_t snappy_max_compressed_length(size_t source_length) {\n  return snappy::MaxCompressedLength(source_length);\n}\n\nsnappy_status snappy_uncompressed_length(const char *compressed,\n                                         size_t compressed_length,\n                                         size_t *result) {\n  if (snappy::GetUncompressedLength(compressed,\n                                    compressed_length,\n                                    result)) {\n    return SNAPPY_OK;\n  } else {\n    return SNAPPY_INVALID_INPUT;\n  }\n}\n\nsnappy_status snappy_validate_compressed_buffer(const char *compressed,\n                                                size_t compressed_length) {\n  if (snappy::IsValidCompressedBuffer(compressed, compressed_length)) {\n    return SNAPPY_OK;\n  } else {\n    return SNAPPY_INVALID_INPUT;\n  }\n}\n\n}  // extern \"C\"\n"
        },
        {
          "name": "snappy-c.h",
          "type": "blob",
          "size": 5.287109375,
          "content": "/*\n * Copyright 2011 Martin Gieseking <martin.gieseking@uos.de>.\n *\n * Redistribution and use in source and binary forms, with or without\n * modification, are permitted provided that the following conditions are\n * met:\n *\n *     * Redistributions of source code must retain the above copyright\n * notice, this list of conditions and the following disclaimer.\n *     * Redistributions in binary form must reproduce the above\n * copyright notice, this list of conditions and the following disclaimer\n * in the documentation and/or other materials provided with the\n * distribution.\n *     * Neither the name of Google Inc. nor the names of its\n * contributors may be used to endorse or promote products derived from\n * this software without specific prior written permission.\n *\n * THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n * \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n * LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n * A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n * OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n * SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n * LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n * DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n * THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n * (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n * OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n *\n * Plain C interface (a wrapper around the C++ implementation).\n */\n\n#ifndef THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_C_H_\n#define THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_C_H_\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n#include <stddef.h>\n\n/*\n * Return values; see the documentation for each function to know\n * what each can return.\n */\ntypedef enum {\n  SNAPPY_OK = 0,\n  SNAPPY_INVALID_INPUT = 1,\n  SNAPPY_BUFFER_TOO_SMALL = 2\n} snappy_status;\n\n/*\n * Takes the data stored in \"input[0..input_length-1]\" and stores\n * it in the array pointed to by \"compressed\".\n *\n * <compressed_length> signals the space available in \"compressed\".\n * If it is not at least equal to \"snappy_max_compressed_length(input_length)\",\n * SNAPPY_BUFFER_TOO_SMALL is returned. After successful compression,\n * <compressed_length> contains the true length of the compressed output,\n * and SNAPPY_OK is returned.\n *\n * Example:\n *   size_t output_length = snappy_max_compressed_length(input_length);\n *   char* output = (char*)malloc(output_length);\n *   if (snappy_compress(input, input_length, output, &output_length)\n *       == SNAPPY_OK) {\n *     ... Process(output, output_length) ...\n *   }\n *   free(output);\n */\nsnappy_status snappy_compress(const char* input,\n                              size_t input_length,\n                              char* compressed,\n                              size_t* compressed_length);\n\n/*\n * Given data in \"compressed[0..compressed_length-1]\" generated by\n * calling the snappy_compress routine, this routine stores\n * the uncompressed data to\n *   uncompressed[0..uncompressed_length-1].\n * Returns failure (a value not equal to SNAPPY_OK) if the message\n * is corrupted and could not be decrypted.\n *\n * <uncompressed_length> signals the space available in \"uncompressed\".\n * If it is not at least equal to the value returned by\n * snappy_uncompressed_length for this stream, SNAPPY_BUFFER_TOO_SMALL\n * is returned. After successful decompression, <uncompressed_length>\n * contains the true length of the decompressed output.\n *\n * Example:\n *   size_t output_length;\n *   if (snappy_uncompressed_length(input, input_length, &output_length)\n *       != SNAPPY_OK) {\n *     ... fail ...\n *   }\n *   char* output = (char*)malloc(output_length);\n *   if (snappy_uncompress(input, input_length, output, &output_length)\n *       == SNAPPY_OK) {\n *     ... Process(output, output_length) ...\n *   }\n *   free(output);\n */\nsnappy_status snappy_uncompress(const char* compressed,\n                                size_t compressed_length,\n                                char* uncompressed,\n                                size_t* uncompressed_length);\n\n/*\n * Returns the maximal size of the compressed representation of\n * input data that is \"source_length\" bytes in length.\n */\nsize_t snappy_max_compressed_length(size_t source_length);\n\n/*\n * REQUIRES: \"compressed[]\" was produced by snappy_compress()\n * Returns SNAPPY_OK and stores the length of the uncompressed data in\n * *result normally. Returns SNAPPY_INVALID_INPUT on parsing error.\n * This operation takes O(1) time.\n */\nsnappy_status snappy_uncompressed_length(const char* compressed,\n                                         size_t compressed_length,\n                                         size_t* result);\n\n/*\n * Check if the contents of \"compressed[]\" can be uncompressed successfully.\n * Does not return the uncompressed data; if so, returns SNAPPY_OK,\n * or if not, returns SNAPPY_INVALID_INPUT.\n * Takes time proportional to compressed_length, but is usually at least a\n * factor of four faster than actual decompression.\n */\nsnappy_status snappy_validate_compressed_buffer(const char* compressed,\n                                                size_t compressed_length);\n\n#ifdef __cplusplus\n}  // extern \"C\"\n#endif\n\n#endif  /* THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_C_H_ */\n"
        },
        {
          "name": "snappy-internal.h",
          "type": "blob",
          "size": 16.4619140625,
          "content": "// Copyright 2008 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Internals shared between the Snappy implementation and its unittest.\n\n#ifndef THIRD_PARTY_SNAPPY_SNAPPY_INTERNAL_H_\n#define THIRD_PARTY_SNAPPY_SNAPPY_INTERNAL_H_\n\n#include <utility>\n\n#include \"snappy-stubs-internal.h\"\n\n#if SNAPPY_HAVE_SSSE3\n// Please do not replace with <x86intrin.h> or with headers that assume more\n// advanced SSE versions without checking with all the OWNERS.\n#include <emmintrin.h>\n#include <tmmintrin.h>\n#endif\n\n#if SNAPPY_HAVE_NEON\n#include <arm_neon.h>\n#endif\n\n#if SNAPPY_HAVE_SSSE3 || SNAPPY_HAVE_NEON\n#define SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE 1\n#else\n#define SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE 0\n#endif\n\nnamespace snappy {\nnamespace internal {\n\n#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n#if SNAPPY_HAVE_SSSE3\nusing V128 = __m128i;\n#elif SNAPPY_HAVE_NEON\nusing V128 = uint8x16_t;\n#endif\n\n// Load 128 bits of integer data. `src` must be 16-byte aligned.\ninline V128 V128_Load(const V128* src);\n\n// Load 128 bits of integer data. `src` does not need to be aligned.\ninline V128 V128_LoadU(const V128* src);\n\n// Store 128 bits of integer data. `dst` does not need to be aligned.\ninline void V128_StoreU(V128* dst, V128 val);\n\n// Shuffle packed 8-bit integers using a shuffle mask.\n// Each packed integer in the shuffle mask must be in [0,16).\ninline V128 V128_Shuffle(V128 input, V128 shuffle_mask);\n\n// Constructs V128 with 16 chars |c|.\ninline V128 V128_DupChar(char c);\n\n#if SNAPPY_HAVE_SSSE3\ninline V128 V128_Load(const V128* src) { return _mm_load_si128(src); }\n\ninline V128 V128_LoadU(const V128* src) { return _mm_loadu_si128(src); }\n\ninline void V128_StoreU(V128* dst, V128 val) { _mm_storeu_si128(dst, val); }\n\ninline V128 V128_Shuffle(V128 input, V128 shuffle_mask) {\n  return _mm_shuffle_epi8(input, shuffle_mask);\n}\n\ninline V128 V128_DupChar(char c) { return _mm_set1_epi8(c); }\n\n#elif SNAPPY_HAVE_NEON\ninline V128 V128_Load(const V128* src) {\n  return vld1q_u8(reinterpret_cast<const uint8_t*>(src));\n}\n\ninline V128 V128_LoadU(const V128* src) {\n  return vld1q_u8(reinterpret_cast<const uint8_t*>(src));\n}\n\ninline void V128_StoreU(V128* dst, V128 val) {\n  vst1q_u8(reinterpret_cast<uint8_t*>(dst), val);\n}\n\ninline V128 V128_Shuffle(V128 input, V128 shuffle_mask) {\n  assert(vminvq_u8(shuffle_mask) >= 0 && vmaxvq_u8(shuffle_mask) <= 15);\n  return vqtbl1q_u8(input, shuffle_mask);\n}\n\ninline V128 V128_DupChar(char c) { return vdupq_n_u8(c); }\n#endif\n#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n\n// Working memory performs a single allocation to hold all scratch space\n// required for compression.\nclass WorkingMemory {\n public:\n  explicit WorkingMemory(size_t input_size);\n  ~WorkingMemory();\n\n  // Allocates and clears a hash table using memory in \"*this\",\n  // stores the number of buckets in \"*table_size\" and returns a pointer to\n  // the base of the hash table.\n  uint16_t* GetHashTable(size_t fragment_size, int* table_size) const;\n  char* GetScratchInput() const { return input_; }\n  char* GetScratchOutput() const { return output_; }\n\n private:\n  char* mem_;        // the allocated memory, never nullptr\n  size_t size_;      // the size of the allocated memory, never 0\n  uint16_t* table_;  // the pointer to the hashtable\n  char* input_;      // the pointer to the input scratch buffer\n  char* output_;     // the pointer to the output scratch buffer\n\n  // No copying\n  WorkingMemory(const WorkingMemory&);\n  void operator=(const WorkingMemory&);\n};\n\n// Flat array compression that does not emit the \"uncompressed length\"\n// prefix. Compresses \"input\" string to the \"*op\" buffer.\n//\n// REQUIRES: \"input_length <= kBlockSize\"\n// REQUIRES: \"op\" points to an array of memory that is at least\n// \"MaxCompressedLength(input_length)\" in size.\n// REQUIRES: All elements in \"table[0..table_size-1]\" are initialized to zero.\n// REQUIRES: \"table_size\" is a power of two\n//\n// Returns an \"end\" pointer into \"op\" buffer.\n// \"end - op\" is the compressed size of \"input\".\nchar* CompressFragment(const char* input,\n                       size_t input_length,\n                       char* op,\n                       uint16_t* table,\n                       const int table_size);\n\n// Find the largest n such that\n//\n//   s1[0,n-1] == s2[0,n-1]\n//   and n <= (s2_limit - s2).\n//\n// Return make_pair(n, n < 8).\n// Does not read *s2_limit or beyond.\n// Does not read *(s1 + (s2_limit - s2)) or beyond.\n// Requires that s2_limit >= s2.\n//\n// In addition populate *data with the next 5 bytes from the end of the match.\n// This is only done if 8 bytes are available (s2_limit - s2 >= 8). The point is\n// that on some arch's this can be done faster in this routine than subsequent\n// loading from s2 + n.\n//\n// Separate implementation for 64-bit, little-endian cpus.\n#if !SNAPPY_IS_BIG_ENDIAN && \\\n    (defined(__x86_64__) || defined(_M_X64) || defined(ARCH_PPC) || \\\n     defined(ARCH_ARM))\nstatic inline std::pair<size_t, bool> FindMatchLength(const char* s1,\n                                                      const char* s2,\n                                                      const char* s2_limit,\n                                                      uint64_t* data) {\n  assert(s2_limit >= s2);\n  size_t matched = 0;\n\n  // This block isn't necessary for correctness; we could just start looping\n  // immediately.  As an optimization though, it is useful.  It creates some not\n  // uncommon code paths that determine, without extra effort, whether the match\n  // length is less than 8.  In short, we are hoping to avoid a conditional\n  // branch, and perhaps get better code layout from the C++ compiler.\n  if (SNAPPY_PREDICT_TRUE(s2 <= s2_limit - 16)) {\n    uint64_t a1 = UNALIGNED_LOAD64(s1);\n    uint64_t a2 = UNALIGNED_LOAD64(s2);\n    if (SNAPPY_PREDICT_TRUE(a1 != a2)) {\n      // This code is critical for performance. The reason is that it determines\n      // how much to advance `ip` (s2). This obviously depends on both the loads\n      // from the `candidate` (s1) and `ip`. Furthermore the next `candidate`\n      // depends on the advanced `ip` calculated here through a load, hash and\n      // new candidate hash lookup (a lot of cycles). This makes s1 (ie.\n      // `candidate`) the variable that limits throughput. This is the reason we\n      // go through hoops to have this function update `data` for the next iter.\n      // The straightforward code would use *data, given by\n      //\n      // *data = UNALIGNED_LOAD64(s2 + matched_bytes) (Latency of 5 cycles),\n      //\n      // as input for the hash table lookup to find next candidate. However\n      // this forces the load on the data dependency chain of s1, because\n      // matched_bytes directly depends on s1. However matched_bytes is 0..7, so\n      // we can also calculate *data by\n      //\n      // *data = AlignRight(UNALIGNED_LOAD64(s2), UNALIGNED_LOAD64(s2 + 8),\n      //                    matched_bytes);\n      //\n      // The loads do not depend on s1 anymore and are thus off the bottleneck.\n      // The straightforward implementation on x86_64 would be to use\n      //\n      // shrd rax, rdx, cl  (cl being matched_bytes * 8)\n      //\n      // unfortunately shrd with a variable shift has a 4 cycle latency. So this\n      // only wins 1 cycle. The BMI2 shrx instruction is a 1 cycle variable\n      // shift instruction but can only shift 64 bits. If we focus on just\n      // obtaining the least significant 4 bytes, we can obtain this by\n      //\n      // *data = ConditionalMove(matched_bytes < 4, UNALIGNED_LOAD64(s2),\n      //     UNALIGNED_LOAD64(s2 + 4) >> ((matched_bytes & 3) * 8);\n      //\n      // Writen like above this is not a big win, the conditional move would be\n      // a cmp followed by a cmov (2 cycles) followed by a shift (1 cycle).\n      // However matched_bytes < 4 is equal to\n      // static_cast<uint32_t>(xorval) != 0. Writen that way, the conditional\n      // move (2 cycles) can execute in parallel with FindLSBSetNonZero64\n      // (tzcnt), which takes 3 cycles.\n      uint64_t xorval = a1 ^ a2;\n      int shift = Bits::FindLSBSetNonZero64(xorval);\n      size_t matched_bytes = shift >> 3;\n      uint64_t a3 = UNALIGNED_LOAD64(s2 + 4);\n#ifndef __x86_64__\n      a2 = static_cast<uint32_t>(xorval) == 0 ? a3 : a2;\n#else\n      // Ideally this would just be\n      //\n      // a2 = static_cast<uint32_t>(xorval) == 0 ? a3 : a2;\n      //\n      // However clang correctly infers that the above statement participates on\n      // a critical data dependency chain and thus, unfortunately, refuses to\n      // use a conditional move (it's tuned to cut data dependencies). In this\n      // case there is a longer parallel chain anyway AND this will be fairly\n      // unpredictable.\n      asm(\"testl %k2, %k2\\n\\t\"\n          \"cmovzq %1, %0\\n\\t\"\n          : \"+r\"(a2)\n          : \"r\"(a3), \"r\"(xorval)\n          : \"cc\");\n#endif\n      *data = a2 >> (shift & (3 * 8));\n      return std::pair<size_t, bool>(matched_bytes, true);\n    } else {\n      matched = 8;\n      s2 += 8;\n    }\n  }\n  SNAPPY_PREFETCH(s1 + 64);\n  SNAPPY_PREFETCH(s2 + 64);\n\n  // Find out how long the match is. We loop over the data 64 bits at a\n  // time until we find a 64-bit block that doesn't match; then we find\n  // the first non-matching bit and use that to calculate the total\n  // length of the match.\n  while (SNAPPY_PREDICT_TRUE(s2 <= s2_limit - 16)) {\n    uint64_t a1 = UNALIGNED_LOAD64(s1 + matched);\n    uint64_t a2 = UNALIGNED_LOAD64(s2);\n    if (a1 == a2) {\n      s2 += 8;\n      matched += 8;\n    } else {\n      uint64_t xorval = a1 ^ a2;\n      int shift = Bits::FindLSBSetNonZero64(xorval);\n      size_t matched_bytes = shift >> 3;\n      uint64_t a3 = UNALIGNED_LOAD64(s2 + 4);\n#ifndef __x86_64__\n      a2 = static_cast<uint32_t>(xorval) == 0 ? a3 : a2;\n#else\n      asm(\"testl %k2, %k2\\n\\t\"\n          \"cmovzq %1, %0\\n\\t\"\n          : \"+r\"(a2)\n          : \"r\"(a3), \"r\"(xorval)\n          : \"cc\");\n#endif\n      *data = a2 >> (shift & (3 * 8));\n      matched += matched_bytes;\n      assert(matched >= 8);\n      return std::pair<size_t, bool>(matched, false);\n    }\n  }\n  while (SNAPPY_PREDICT_TRUE(s2 < s2_limit)) {\n    if (s1[matched] == *s2) {\n      ++s2;\n      ++matched;\n    } else {\n      if (s2 <= s2_limit - 8) {\n        *data = UNALIGNED_LOAD64(s2);\n      }\n      return std::pair<size_t, bool>(matched, matched < 8);\n    }\n  }\n  return std::pair<size_t, bool>(matched, matched < 8);\n}\n#else\nstatic inline std::pair<size_t, bool> FindMatchLength(const char* s1,\n                                                      const char* s2,\n                                                      const char* s2_limit,\n                                                      uint64_t* data) {\n  // Implementation based on the x86-64 version, above.\n  assert(s2_limit >= s2);\n  int matched = 0;\n\n  while (s2 <= s2_limit - 4 &&\n         UNALIGNED_LOAD32(s2) == UNALIGNED_LOAD32(s1 + matched)) {\n    s2 += 4;\n    matched += 4;\n  }\n  if (LittleEndian::IsLittleEndian() && s2 <= s2_limit - 4) {\n    uint32_t x = UNALIGNED_LOAD32(s2) ^ UNALIGNED_LOAD32(s1 + matched);\n    int matching_bits = Bits::FindLSBSetNonZero(x);\n    matched += matching_bits >> 3;\n    s2 += matching_bits >> 3;\n  } else {\n    while ((s2 < s2_limit) && (s1[matched] == *s2)) {\n      ++s2;\n      ++matched;\n    }\n  }\n  if (s2 <= s2_limit - 8) *data = LittleEndian::Load64(s2);\n  return std::pair<size_t, bool>(matched, matched < 8);\n}\n#endif\n\nstatic inline size_t FindMatchLengthPlain(const char* s1, const char* s2,\n                                          const char* s2_limit) {\n  // Implementation based on the x86-64 version, above.\n  assert(s2_limit >= s2);\n  int matched = 0;\n\n  while (s2 <= s2_limit - 8 &&\n         UNALIGNED_LOAD64(s2) == UNALIGNED_LOAD64(s1 + matched)) {\n    s2 += 8;\n    matched += 8;\n  }\n  if (LittleEndian::IsLittleEndian() && s2 <= s2_limit - 8) {\n    uint64_t x = UNALIGNED_LOAD64(s2) ^ UNALIGNED_LOAD64(s1 + matched);\n    int matching_bits = Bits::FindLSBSetNonZero64(x);\n    matched += matching_bits >> 3;\n    s2 += matching_bits >> 3;\n  } else {\n    while ((s2 < s2_limit) && (s1[matched] == *s2)) {\n      ++s2;\n      ++matched;\n    }\n  }\n  return matched;\n}\n\n// Lookup tables for decompression code.  Give --snappy_dump_decompression_table\n// to the unit test to recompute char_table.\n\nenum {\n  LITERAL = 0,\n  COPY_1_BYTE_OFFSET = 1,  // 3 bit length + 3 bits of offset in opcode\n  COPY_2_BYTE_OFFSET = 2,\n  COPY_4_BYTE_OFFSET = 3\n};\nstatic const int kMaximumTagLength = 5;  // COPY_4_BYTE_OFFSET plus the actual offset.\n\n// Data stored per entry in lookup table:\n//      Range   Bits-used       Description\n//      ------------------------------------\n//      1..64   0..7            Literal/copy length encoded in opcode byte\n//      0..7    8..10           Copy offset encoded in opcode byte / 256\n//      0..4    11..13          Extra bytes after opcode\n//\n// We use eight bits for the length even though 7 would have sufficed\n// because of efficiency reasons:\n//      (1) Extracting a byte is faster than a bit-field\n//      (2) It properly aligns copy offset so we do not need a <<8\nstatic constexpr uint16_t char_table[256] = {\n    // clang-format off\n  0x0001, 0x0804, 0x1001, 0x2001, 0x0002, 0x0805, 0x1002, 0x2002,\n  0x0003, 0x0806, 0x1003, 0x2003, 0x0004, 0x0807, 0x1004, 0x2004,\n  0x0005, 0x0808, 0x1005, 0x2005, 0x0006, 0x0809, 0x1006, 0x2006,\n  0x0007, 0x080a, 0x1007, 0x2007, 0x0008, 0x080b, 0x1008, 0x2008,\n  0x0009, 0x0904, 0x1009, 0x2009, 0x000a, 0x0905, 0x100a, 0x200a,\n  0x000b, 0x0906, 0x100b, 0x200b, 0x000c, 0x0907, 0x100c, 0x200c,\n  0x000d, 0x0908, 0x100d, 0x200d, 0x000e, 0x0909, 0x100e, 0x200e,\n  0x000f, 0x090a, 0x100f, 0x200f, 0x0010, 0x090b, 0x1010, 0x2010,\n  0x0011, 0x0a04, 0x1011, 0x2011, 0x0012, 0x0a05, 0x1012, 0x2012,\n  0x0013, 0x0a06, 0x1013, 0x2013, 0x0014, 0x0a07, 0x1014, 0x2014,\n  0x0015, 0x0a08, 0x1015, 0x2015, 0x0016, 0x0a09, 0x1016, 0x2016,\n  0x0017, 0x0a0a, 0x1017, 0x2017, 0x0018, 0x0a0b, 0x1018, 0x2018,\n  0x0019, 0x0b04, 0x1019, 0x2019, 0x001a, 0x0b05, 0x101a, 0x201a,\n  0x001b, 0x0b06, 0x101b, 0x201b, 0x001c, 0x0b07, 0x101c, 0x201c,\n  0x001d, 0x0b08, 0x101d, 0x201d, 0x001e, 0x0b09, 0x101e, 0x201e,\n  0x001f, 0x0b0a, 0x101f, 0x201f, 0x0020, 0x0b0b, 0x1020, 0x2020,\n  0x0021, 0x0c04, 0x1021, 0x2021, 0x0022, 0x0c05, 0x1022, 0x2022,\n  0x0023, 0x0c06, 0x1023, 0x2023, 0x0024, 0x0c07, 0x1024, 0x2024,\n  0x0025, 0x0c08, 0x1025, 0x2025, 0x0026, 0x0c09, 0x1026, 0x2026,\n  0x0027, 0x0c0a, 0x1027, 0x2027, 0x0028, 0x0c0b, 0x1028, 0x2028,\n  0x0029, 0x0d04, 0x1029, 0x2029, 0x002a, 0x0d05, 0x102a, 0x202a,\n  0x002b, 0x0d06, 0x102b, 0x202b, 0x002c, 0x0d07, 0x102c, 0x202c,\n  0x002d, 0x0d08, 0x102d, 0x202d, 0x002e, 0x0d09, 0x102e, 0x202e,\n  0x002f, 0x0d0a, 0x102f, 0x202f, 0x0030, 0x0d0b, 0x1030, 0x2030,\n  0x0031, 0x0e04, 0x1031, 0x2031, 0x0032, 0x0e05, 0x1032, 0x2032,\n  0x0033, 0x0e06, 0x1033, 0x2033, 0x0034, 0x0e07, 0x1034, 0x2034,\n  0x0035, 0x0e08, 0x1035, 0x2035, 0x0036, 0x0e09, 0x1036, 0x2036,\n  0x0037, 0x0e0a, 0x1037, 0x2037, 0x0038, 0x0e0b, 0x1038, 0x2038,\n  0x0039, 0x0f04, 0x1039, 0x2039, 0x003a, 0x0f05, 0x103a, 0x203a,\n  0x003b, 0x0f06, 0x103b, 0x203b, 0x003c, 0x0f07, 0x103c, 0x203c,\n  0x0801, 0x0f08, 0x103d, 0x203d, 0x1001, 0x0f09, 0x103e, 0x203e,\n  0x1801, 0x0f0a, 0x103f, 0x203f, 0x2001, 0x0f0b, 0x1040, 0x2040,\n    // clang-format on\n};\n\n}  // end namespace internal\n}  // end namespace snappy\n\n#endif  // THIRD_PARTY_SNAPPY_SNAPPY_INTERNAL_H_\n"
        },
        {
          "name": "snappy-sinksource.cc",
          "type": "blob",
          "size": 3.66015625,
          "content": "// Copyright 2011 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#include <stddef.h>\n#include <cstring>\n\n#include \"snappy-sinksource.h\"\n\nnamespace snappy {\n\nSource::~Source() = default;\n\nSink::~Sink() = default;\n\nchar* Sink::GetAppendBuffer(size_t length, char* scratch) {\n  // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n  (void)length;\n\n  return scratch;\n}\n\nchar* Sink::GetAppendBufferVariable(\n      size_t min_size, size_t desired_size_hint, char* scratch,\n      size_t scratch_size, size_t* allocated_size) {\n  // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n  (void)min_size;\n  (void)desired_size_hint;\n\n  *allocated_size = scratch_size;\n  return scratch;\n}\n\nvoid Sink::AppendAndTakeOwnership(\n    char* bytes, size_t n,\n    void (*deleter)(void*, const char*, size_t),\n    void *deleter_arg) {\n  Append(bytes, n);\n  (*deleter)(deleter_arg, bytes, n);\n}\n\nByteArraySource::~ByteArraySource() = default;\n\nsize_t ByteArraySource::Available() const { return left_; }\n\nconst char* ByteArraySource::Peek(size_t* len) {\n  *len = left_;\n  return ptr_;\n}\n\nvoid ByteArraySource::Skip(size_t n) {\n  left_ -= n;\n  ptr_ += n;\n}\n\nUncheckedByteArraySink::~UncheckedByteArraySink() { }\n\nvoid UncheckedByteArraySink::Append(const char* data, size_t n) {\n  // Do no copying if the caller filled in the result of GetAppendBuffer()\n  if (data != dest_) {\n    std::memcpy(dest_, data, n);\n  }\n  dest_ += n;\n}\n\nchar* UncheckedByteArraySink::GetAppendBuffer(size_t len, char* scratch) {\n  // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n  (void)len;\n  (void)scratch;\n\n  return dest_;\n}\n\nvoid UncheckedByteArraySink::AppendAndTakeOwnership(\n    char* bytes, size_t n,\n    void (*deleter)(void*, const char*, size_t),\n    void *deleter_arg) {\n  if (bytes != dest_) {\n    std::memcpy(dest_, bytes, n);\n    (*deleter)(deleter_arg, bytes, n);\n  }\n  dest_ += n;\n}\n\nchar* UncheckedByteArraySink::GetAppendBufferVariable(\n      size_t min_size, size_t desired_size_hint, char* scratch,\n      size_t scratch_size, size_t* allocated_size) {\n  // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n  (void)min_size;\n  (void)scratch;\n  (void)scratch_size;\n\n  *allocated_size = desired_size_hint;\n  return dest_;\n}\n\n}  // namespace snappy\n"
        },
        {
          "name": "snappy-sinksource.h",
          "type": "blob",
          "size": 7.0947265625,
          "content": "// Copyright 2011 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#ifndef THIRD_PARTY_SNAPPY_SNAPPY_SINKSOURCE_H_\n#define THIRD_PARTY_SNAPPY_SNAPPY_SINKSOURCE_H_\n\n#include <stddef.h>\n\nnamespace snappy {\n\n// A Sink is an interface that consumes a sequence of bytes.\nclass Sink {\n public:\n  Sink() { }\n  virtual ~Sink();\n\n  // Append \"bytes[0,n-1]\" to this.\n  virtual void Append(const char* bytes, size_t n) = 0;\n\n  // Returns a writable buffer of the specified length for appending.\n  // May return a pointer to the caller-owned scratch buffer which\n  // must have at least the indicated length.  The returned buffer is\n  // only valid until the next operation on this Sink.\n  //\n  // After writing at most \"length\" bytes, call Append() with the\n  // pointer returned from this function and the number of bytes\n  // written.  Many Append() implementations will avoid copying\n  // bytes if this function returned an internal buffer.\n  //\n  // If a non-scratch buffer is returned, the caller may only pass a\n  // prefix of it to Append().  That is, it is not correct to pass an\n  // interior pointer of the returned array to Append().\n  //\n  // The default implementation always returns the scratch buffer.\n  virtual char* GetAppendBuffer(size_t length, char* scratch);\n\n  // For higher performance, Sink implementations can provide custom\n  // AppendAndTakeOwnership() and GetAppendBufferVariable() methods.\n  // These methods can reduce the number of copies done during\n  // compression/decompression.\n\n  // Append \"bytes[0,n-1] to the sink. Takes ownership of \"bytes\"\n  // and calls the deleter function as (*deleter)(deleter_arg, bytes, n)\n  // to free the buffer. deleter function must be non NULL.\n  //\n  // The default implementation just calls Append and frees \"bytes\".\n  // Other implementations may avoid a copy while appending the buffer.\n  virtual void AppendAndTakeOwnership(\n      char* bytes, size_t n, void (*deleter)(void*, const char*, size_t),\n      void *deleter_arg);\n\n  // Returns a writable buffer for appending and writes the buffer's capacity to\n  // *allocated_size. Guarantees *allocated_size >= min_size.\n  // May return a pointer to the caller-owned scratch buffer which must have\n  // scratch_size >= min_size.\n  //\n  // The returned buffer is only valid until the next operation\n  // on this ByteSink.\n  //\n  // After writing at most *allocated_size bytes, call Append() with the\n  // pointer returned from this function and the number of bytes written.\n  // Many Append() implementations will avoid copying bytes if this function\n  // returned an internal buffer.\n  //\n  // If the sink implementation allocates or reallocates an internal buffer,\n  // it should use the desired_size_hint if appropriate. If a caller cannot\n  // provide a reasonable guess at the desired capacity, it should set\n  // desired_size_hint = 0.\n  //\n  // If a non-scratch buffer is returned, the caller may only pass\n  // a prefix to it to Append(). That is, it is not correct to pass an\n  // interior pointer to Append().\n  //\n  // The default implementation always returns the scratch buffer.\n  virtual char* GetAppendBufferVariable(\n      size_t min_size, size_t desired_size_hint, char* scratch,\n      size_t scratch_size, size_t* allocated_size);\n\n private:\n  // No copying\n  Sink(const Sink&);\n  void operator=(const Sink&);\n};\n\n// A Source is an interface that yields a sequence of bytes\nclass Source {\n public:\n  Source() { }\n  virtual ~Source();\n\n  // Return the number of bytes left to read from the source\n  virtual size_t Available() const = 0;\n\n  // Peek at the next flat region of the source.  Does not reposition\n  // the source.  The returned region is empty iff Available()==0.\n  //\n  // Returns a pointer to the beginning of the region and store its\n  // length in *len.\n  //\n  // The returned region is valid until the next call to Skip() or\n  // until this object is destroyed, whichever occurs first.\n  //\n  // The returned region may be larger than Available() (for example\n  // if this ByteSource is a view on a substring of a larger source).\n  // The caller is responsible for ensuring that it only reads the\n  // Available() bytes.\n  virtual const char* Peek(size_t* len) = 0;\n\n  // Skip the next n bytes.  Invalidates any buffer returned by\n  // a previous call to Peek().\n  // REQUIRES: Available() >= n\n  virtual void Skip(size_t n) = 0;\n\n private:\n  // No copying\n  Source(const Source&);\n  void operator=(const Source&);\n};\n\n// A Source implementation that yields the contents of a flat array\nclass ByteArraySource : public Source {\n public:\n  ByteArraySource(const char* p, size_t n) : ptr_(p), left_(n) { }\n  ~ByteArraySource() override;\n  size_t Available() const override;\n  const char* Peek(size_t* len) override;\n  void Skip(size_t n) override;\n private:\n  const char* ptr_;\n  size_t left_;\n};\n\n// A Sink implementation that writes to a flat array without any bound checks.\nclass UncheckedByteArraySink : public Sink {\n public:\n  explicit UncheckedByteArraySink(char* dest) : dest_(dest) { }\n  ~UncheckedByteArraySink() override;\n  void Append(const char* data, size_t n) override;\n  char* GetAppendBuffer(size_t len, char* scratch) override;\n  char* GetAppendBufferVariable(\n      size_t min_size, size_t desired_size_hint, char* scratch,\n      size_t scratch_size, size_t* allocated_size) override;\n  void AppendAndTakeOwnership(\n      char* bytes, size_t n, void (*deleter)(void*, const char*, size_t),\n      void *deleter_arg) override;\n\n  // Return the current output pointer so that a caller can see how\n  // many bytes were produced.\n  // Note: this is not a Sink method.\n  char* CurrentDestination() const { return dest_; }\n private:\n  char* dest_;\n};\n\n}  // namespace snappy\n\n#endif  // THIRD_PARTY_SNAPPY_SNAPPY_SINKSOURCE_H_\n"
        },
        {
          "name": "snappy-stubs-internal.cc",
          "type": "blob",
          "size": 1.791015625,
          "content": "// Copyright 2011 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#include <algorithm>\n#include <string>\n\n#include \"snappy-stubs-internal.h\"\n\nnamespace snappy {\n\nvoid Varint::Append32(std::string* s, uint32_t value) {\n  char buf[Varint::kMax32];\n  const char* p = Varint::Encode32(buf, value);\n  s->append(buf, p - buf);\n}\n\n}  // namespace snappy\n"
        },
        {
          "name": "snappy-stubs-internal.h",
          "type": "blob",
          "size": 16.927734375,
          "content": "// Copyright 2011 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Various stubs for the open-source version of Snappy.\n\n#ifndef THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_STUBS_INTERNAL_H_\n#define THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_STUBS_INTERNAL_H_\n\n#if HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n\n#include <stdint.h>\n\n#include <cassert>\n#include <cstdlib>\n#include <cstring>\n#include <limits>\n#include <string>\n\n#if HAVE_SYS_MMAN_H\n#include <sys/mman.h>\n#endif\n\n#if HAVE_UNISTD_H\n#include <unistd.h>\n#endif\n\n#if defined(_MSC_VER)\n#include <intrin.h>\n#endif  // defined(_MSC_VER)\n\n#ifndef __has_feature\n#define __has_feature(x) 0\n#endif\n\n#if __has_feature(memory_sanitizer)\n#include <sanitizer/msan_interface.h>\n#define SNAPPY_ANNOTATE_MEMORY_IS_INITIALIZED(address, size) \\\n    __msan_unpoison((address), (size))\n#else\n#define SNAPPY_ANNOTATE_MEMORY_IS_INITIALIZED(address, size) /* empty */\n#endif  // __has_feature(memory_sanitizer)\n\n#include \"snappy-stubs-public.h\"\n\n// Used to enable 64-bit optimized versions of some routines.\n#if defined(__PPC64__) || defined(__powerpc64__)\n#define ARCH_PPC 1\n#elif defined(__aarch64__) || defined(_M_ARM64)\n#define ARCH_ARM 1\n#endif\n\n// Needed by OS X, among others.\n#ifndef MAP_ANONYMOUS\n#define MAP_ANONYMOUS MAP_ANON\n#endif\n\n// The size of an array, if known at compile-time.\n// Will give unexpected results if used on a pointer.\n// We undefine it first, since some compilers already have a definition.\n#ifdef ARRAYSIZE\n#undef ARRAYSIZE\n#endif\n#define ARRAYSIZE(a) int{sizeof(a) / sizeof(*(a))}\n\n// Static prediction hints.\n#if HAVE_BUILTIN_EXPECT\n#define SNAPPY_PREDICT_FALSE(x) (__builtin_expect(x, 0))\n#define SNAPPY_PREDICT_TRUE(x) (__builtin_expect(!!(x), 1))\n#else\n#define SNAPPY_PREDICT_FALSE(x) x\n#define SNAPPY_PREDICT_TRUE(x) x\n#endif  // HAVE_BUILTIN_EXPECT\n\n// Inlining hints.\n#if HAVE_ATTRIBUTE_ALWAYS_INLINE\n#define SNAPPY_ATTRIBUTE_ALWAYS_INLINE __attribute__((always_inline))\n#else\n#define SNAPPY_ATTRIBUTE_ALWAYS_INLINE\n#endif  // HAVE_ATTRIBUTE_ALWAYS_INLINE\n\n#if HAVE_BUILTIN_PREFETCH\n#define SNAPPY_PREFETCH(ptr) __builtin_prefetch(ptr, 0, 3)\n#else\n#define SNAPPY_PREFETCH(ptr) (void)(ptr)\n#endif\n\n// Stubbed version of ABSL_FLAG.\n//\n// In the open source version, flags can only be changed at compile time.\n#define SNAPPY_FLAG(flag_type, flag_name, default_value, help) \\\n  flag_type FLAGS_ ## flag_name = default_value\n\nnamespace snappy {\n\n// Stubbed version of absl::GetFlag().\ntemplate <typename T>\ninline T GetFlag(T flag) { return flag; }\n\nstatic const uint32_t kuint32max = std::numeric_limits<uint32_t>::max();\nstatic const int64_t kint64max = std::numeric_limits<int64_t>::max();\n\n// Potentially unaligned loads and stores.\n\ninline uint16_t UNALIGNED_LOAD16(const void *p) {\n  // Compiles to a single movzx/ldrh on clang/gcc/msvc.\n  uint16_t v;\n  std::memcpy(&v, p, sizeof(v));\n  return v;\n}\n\ninline uint32_t UNALIGNED_LOAD32(const void *p) {\n  // Compiles to a single mov/ldr on clang/gcc/msvc.\n  uint32_t v;\n  std::memcpy(&v, p, sizeof(v));\n  return v;\n}\n\ninline uint64_t UNALIGNED_LOAD64(const void *p) {\n  // Compiles to a single mov/ldr on clang/gcc/msvc.\n  uint64_t v;\n  std::memcpy(&v, p, sizeof(v));\n  return v;\n}\n\ninline void UNALIGNED_STORE16(void *p, uint16_t v) {\n  // Compiles to a single mov/strh on clang/gcc/msvc.\n  std::memcpy(p, &v, sizeof(v));\n}\n\ninline void UNALIGNED_STORE32(void *p, uint32_t v) {\n  // Compiles to a single mov/str on clang/gcc/msvc.\n  std::memcpy(p, &v, sizeof(v));\n}\n\ninline void UNALIGNED_STORE64(void *p, uint64_t v) {\n  // Compiles to a single mov/str on clang/gcc/msvc.\n  std::memcpy(p, &v, sizeof(v));\n}\n\n// Convert to little-endian storage, opposite of network format.\n// Convert x from host to little endian: x = LittleEndian.FromHost(x);\n// convert x from little endian to host: x = LittleEndian.ToHost(x);\n//\n//  Store values into unaligned memory converting to little endian order:\n//    LittleEndian.Store16(p, x);\n//\n//  Load unaligned values stored in little endian converting to host order:\n//    x = LittleEndian.Load16(p);\nclass LittleEndian {\n public:\n  // Functions to do unaligned loads and stores in little-endian order.\n  static inline uint16_t Load16(const void *ptr) {\n    // Compiles to a single mov/str on recent clang and gcc.\n#if SNAPPY_IS_BIG_ENDIAN\n    const uint8_t* const buffer = reinterpret_cast<const uint8_t*>(ptr);\n    return (static_cast<uint16_t>(buffer[0])) |\n            (static_cast<uint16_t>(buffer[1]) << 8);\n#else\n    // memcpy() turns into a single instruction early in the optimization\n    // pipeline (relatively to a series of byte accesses). So, using memcpy\n    // instead of byte accesses may lead to better decisions in more stages of\n    // the optimization pipeline.\n    uint16_t value;\n    std::memcpy(&value, ptr, 2);\n    return value;\n#endif\n  }\n\n  static inline uint32_t Load32(const void *ptr) {\n    // Compiles to a single mov/str on recent clang and gcc.\n#if SNAPPY_IS_BIG_ENDIAN\n    const uint8_t* const buffer = reinterpret_cast<const uint8_t*>(ptr);\n    return (static_cast<uint32_t>(buffer[0])) |\n            (static_cast<uint32_t>(buffer[1]) << 8) |\n            (static_cast<uint32_t>(buffer[2]) << 16) |\n            (static_cast<uint32_t>(buffer[3]) << 24);\n#else\n    // See Load16() for the rationale of using memcpy().\n    uint32_t value;\n    std::memcpy(&value, ptr, 4);\n    return value;\n#endif\n  }\n\n  static inline uint64_t Load64(const void *ptr) {\n    // Compiles to a single mov/str on recent clang and gcc.\n#if SNAPPY_IS_BIG_ENDIAN\n    const uint8_t* const buffer = reinterpret_cast<const uint8_t*>(ptr);\n    return (static_cast<uint64_t>(buffer[0])) |\n            (static_cast<uint64_t>(buffer[1]) << 8) |\n            (static_cast<uint64_t>(buffer[2]) << 16) |\n            (static_cast<uint64_t>(buffer[3]) << 24) |\n            (static_cast<uint64_t>(buffer[4]) << 32) |\n            (static_cast<uint64_t>(buffer[5]) << 40) |\n            (static_cast<uint64_t>(buffer[6]) << 48) |\n            (static_cast<uint64_t>(buffer[7]) << 56);\n#else\n    // See Load16() for the rationale of using memcpy().\n    uint64_t value;\n    std::memcpy(&value, ptr, 8);\n    return value;\n#endif\n  }\n\n  static inline void Store16(void *dst, uint16_t value) {\n    // Compiles to a single mov/str on recent clang and gcc.\n#if SNAPPY_IS_BIG_ENDIAN\n    uint8_t* const buffer = reinterpret_cast<uint8_t*>(dst);\n    buffer[0] = static_cast<uint8_t>(value);\n    buffer[1] = static_cast<uint8_t>(value >> 8);\n#else\n    // See Load16() for the rationale of using memcpy().\n    std::memcpy(dst, &value, 2);\n#endif\n  }\n\n  static void Store32(void *dst, uint32_t value) {\n    // Compiles to a single mov/str on recent clang and gcc.\n#if SNAPPY_IS_BIG_ENDIAN\n    uint8_t* const buffer = reinterpret_cast<uint8_t*>(dst);\n    buffer[0] = static_cast<uint8_t>(value);\n    buffer[1] = static_cast<uint8_t>(value >> 8);\n    buffer[2] = static_cast<uint8_t>(value >> 16);\n    buffer[3] = static_cast<uint8_t>(value >> 24);\n#else\n    // See Load16() for the rationale of using memcpy().\n    std::memcpy(dst, &value, 4);\n#endif\n  }\n\n  static void Store64(void* dst, uint64_t value) {\n    // Compiles to a single mov/str on recent clang and gcc.\n#if SNAPPY_IS_BIG_ENDIAN\n    uint8_t* const buffer = reinterpret_cast<uint8_t*>(dst);\n    buffer[0] = static_cast<uint8_t>(value);\n    buffer[1] = static_cast<uint8_t>(value >> 8);\n    buffer[2] = static_cast<uint8_t>(value >> 16);\n    buffer[3] = static_cast<uint8_t>(value >> 24);\n    buffer[4] = static_cast<uint8_t>(value >> 32);\n    buffer[5] = static_cast<uint8_t>(value >> 40);\n    buffer[6] = static_cast<uint8_t>(value >> 48);\n    buffer[7] = static_cast<uint8_t>(value >> 56);\n#else\n    // See Load16() for the rationale of using memcpy().\n    std::memcpy(dst, &value, 8);\n#endif\n  }\n\n  static inline constexpr bool IsLittleEndian() {\n#if SNAPPY_IS_BIG_ENDIAN\n    return false;\n#else\n    return true;\n#endif  // SNAPPY_IS_BIG_ENDIAN\n  }\n};\n\n// Some bit-manipulation functions.\nclass Bits {\n public:\n  // Return floor(log2(n)) for positive integer n.\n  static int Log2FloorNonZero(uint32_t n);\n\n  // Return floor(log2(n)) for positive integer n.  Returns -1 iff n == 0.\n  static int Log2Floor(uint32_t n);\n\n  // Return the first set least / most significant bit, 0-indexed.  Returns an\n  // undefined value if n == 0.  FindLSBSetNonZero() is similar to ffs() except\n  // that it's 0-indexed.\n  static int FindLSBSetNonZero(uint32_t n);\n\n  static int FindLSBSetNonZero64(uint64_t n);\n\n private:\n  // No copying\n  Bits(const Bits&);\n  void operator=(const Bits&);\n};\n\n#if HAVE_BUILTIN_CTZ\n\ninline int Bits::Log2FloorNonZero(uint32_t n) {\n  assert(n != 0);\n  // (31 ^ x) is equivalent to (31 - x) for x in [0, 31]. An easy proof\n  // represents subtraction in base 2 and observes that there's no carry.\n  //\n  // GCC and Clang represent __builtin_clz on x86 as 31 ^ _bit_scan_reverse(x).\n  // Using \"31 ^\" here instead of \"31 -\" allows the optimizer to strip the\n  // function body down to _bit_scan_reverse(x).\n  return 31 ^ __builtin_clz(n);\n}\n\ninline int Bits::Log2Floor(uint32_t n) {\n  return (n == 0) ? -1 : Bits::Log2FloorNonZero(n);\n}\n\ninline int Bits::FindLSBSetNonZero(uint32_t n) {\n  assert(n != 0);\n  return __builtin_ctz(n);\n}\n\n#elif defined(_MSC_VER)\n\ninline int Bits::Log2FloorNonZero(uint32_t n) {\n  assert(n != 0);\n  // NOLINTNEXTLINE(runtime/int): The MSVC intrinsic demands unsigned long.\n  unsigned long where;\n  _BitScanReverse(&where, n);\n  return static_cast<int>(where);\n}\n\ninline int Bits::Log2Floor(uint32_t n) {\n  // NOLINTNEXTLINE(runtime/int): The MSVC intrinsic demands unsigned long.\n  unsigned long where;\n  if (_BitScanReverse(&where, n))\n    return static_cast<int>(where);\n  return -1;\n}\n\ninline int Bits::FindLSBSetNonZero(uint32_t n) {\n  assert(n != 0);\n  // NOLINTNEXTLINE(runtime/int): The MSVC intrinsic demands unsigned long.\n  unsigned long where;\n  if (_BitScanForward(&where, n))\n    return static_cast<int>(where);\n  return 32;\n}\n\n#else  // Portable versions.\n\ninline int Bits::Log2FloorNonZero(uint32_t n) {\n  assert(n != 0);\n\n  int log = 0;\n  uint32_t value = n;\n  for (int i = 4; i >= 0; --i) {\n    int shift = (1 << i);\n    uint32_t x = value >> shift;\n    if (x != 0) {\n      value = x;\n      log += shift;\n    }\n  }\n  assert(value == 1);\n  return log;\n}\n\ninline int Bits::Log2Floor(uint32_t n) {\n  return (n == 0) ? -1 : Bits::Log2FloorNonZero(n);\n}\n\ninline int Bits::FindLSBSetNonZero(uint32_t n) {\n  assert(n != 0);\n\n  int rc = 31;\n  for (int i = 4, shift = 1 << 4; i >= 0; --i) {\n    const uint32_t x = n << shift;\n    if (x != 0) {\n      n = x;\n      rc -= shift;\n    }\n    shift >>= 1;\n  }\n  return rc;\n}\n\n#endif  // End portable versions.\n\n#if HAVE_BUILTIN_CTZ\n\ninline int Bits::FindLSBSetNonZero64(uint64_t n) {\n  assert(n != 0);\n  return __builtin_ctzll(n);\n}\n\n#elif defined(_MSC_VER) && (defined(_M_X64) || defined(_M_ARM64))\n// _BitScanForward64() is only available on x64 and ARM64.\n\ninline int Bits::FindLSBSetNonZero64(uint64_t n) {\n  assert(n != 0);\n  // NOLINTNEXTLINE(runtime/int): The MSVC intrinsic demands unsigned long.\n  unsigned long where;\n  if (_BitScanForward64(&where, n))\n    return static_cast<int>(where);\n  return 64;\n}\n\n#else  // Portable version.\n\n// FindLSBSetNonZero64() is defined in terms of FindLSBSetNonZero().\ninline int Bits::FindLSBSetNonZero64(uint64_t n) {\n  assert(n != 0);\n\n  const uint32_t bottombits = static_cast<uint32_t>(n);\n  if (bottombits == 0) {\n    // Bottom bits are zero, so scan the top bits.\n    return 32 + FindLSBSetNonZero(static_cast<uint32_t>(n >> 32));\n  } else {\n    return FindLSBSetNonZero(bottombits);\n  }\n}\n\n#endif  // HAVE_BUILTIN_CTZ\n\n// Variable-length integer encoding.\nclass Varint {\n public:\n  // Maximum lengths of varint encoding of uint32_t.\n  static const int kMax32 = 5;\n\n  // Attempts to parse a varint32 from a prefix of the bytes in [ptr,limit-1].\n  // Never reads a character at or beyond limit.  If a valid/terminated varint32\n  // was found in the range, stores it in *OUTPUT and returns a pointer just\n  // past the last byte of the varint32. Else returns NULL.  On success,\n  // \"result <= limit\".\n  static const char* Parse32WithLimit(const char* ptr, const char* limit,\n                                      uint32_t* OUTPUT);\n\n  // REQUIRES   \"ptr\" points to a buffer of length sufficient to hold \"v\".\n  // EFFECTS    Encodes \"v\" into \"ptr\" and returns a pointer to the\n  //            byte just past the last encoded byte.\n  static char* Encode32(char* ptr, uint32_t v);\n\n  // EFFECTS    Appends the varint representation of \"value\" to \"*s\".\n  static void Append32(std::string* s, uint32_t value);\n};\n\ninline const char* Varint::Parse32WithLimit(const char* p,\n                                            const char* l,\n                                            uint32_t* OUTPUT) {\n  const unsigned char* ptr = reinterpret_cast<const unsigned char*>(p);\n  const unsigned char* limit = reinterpret_cast<const unsigned char*>(l);\n  uint32_t b, result;\n  if (ptr >= limit) return NULL;\n  b = *(ptr++); result = b & 127;          if (b < 128) goto done;\n  if (ptr >= limit) return NULL;\n  b = *(ptr++); result |= (b & 127) <<  7; if (b < 128) goto done;\n  if (ptr >= limit) return NULL;\n  b = *(ptr++); result |= (b & 127) << 14; if (b < 128) goto done;\n  if (ptr >= limit) return NULL;\n  b = *(ptr++); result |= (b & 127) << 21; if (b < 128) goto done;\n  if (ptr >= limit) return NULL;\n  b = *(ptr++); result |= (b & 127) << 28; if (b < 16) goto done;\n  return NULL;       // Value is too long to be a varint32\n done:\n  *OUTPUT = result;\n  return reinterpret_cast<const char*>(ptr);\n}\n\ninline char* Varint::Encode32(char* sptr, uint32_t v) {\n  // Operate on characters as unsigneds\n  uint8_t* ptr = reinterpret_cast<uint8_t*>(sptr);\n  static const uint8_t B = 128;\n  if (v < (1 << 7)) {\n    *(ptr++) = static_cast<uint8_t>(v);\n  } else if (v < (1 << 14)) {\n    *(ptr++) = static_cast<uint8_t>(v | B);\n    *(ptr++) = static_cast<uint8_t>(v >> 7);\n  } else if (v < (1 << 21)) {\n    *(ptr++) = static_cast<uint8_t>(v | B);\n    *(ptr++) = static_cast<uint8_t>((v >> 7) | B);\n    *(ptr++) = static_cast<uint8_t>(v >> 14);\n  } else if (v < (1 << 28)) {\n    *(ptr++) = static_cast<uint8_t>(v | B);\n    *(ptr++) = static_cast<uint8_t>((v >> 7) | B);\n    *(ptr++) = static_cast<uint8_t>((v >> 14) | B);\n    *(ptr++) = static_cast<uint8_t>(v >> 21);\n  } else {\n    *(ptr++) = static_cast<uint8_t>(v | B);\n    *(ptr++) = static_cast<uint8_t>((v>>7) | B);\n    *(ptr++) = static_cast<uint8_t>((v>>14) | B);\n    *(ptr++) = static_cast<uint8_t>((v>>21) | B);\n    *(ptr++) = static_cast<uint8_t>(v >> 28);\n  }\n  return reinterpret_cast<char*>(ptr);\n}\n\n// If you know the internal layout of the std::string in use, you can\n// replace this function with one that resizes the string without\n// filling the new space with zeros (if applicable) --\n// it will be non-portable but faster.\ninline void STLStringResizeUninitialized(std::string* s, size_t new_size) {\n  s->resize(new_size);\n}\n\n// Return a mutable char* pointing to a string's internal buffer,\n// which may not be null-terminated. Writing through this pointer will\n// modify the string.\n//\n// string_as_array(&str)[i] is valid for 0 <= i < str.size() until the\n// next call to a string method that invalidates iterators.\n//\n// As of 2006-04, there is no standard-blessed way of getting a\n// mutable reference to a string's internal buffer. However, issue 530\n// (http://www.open-std.org/JTC1/SC22/WG21/docs/lwg-defects.html#530)\n// proposes this as the method. It will officially be part of the standard\n// for C++0x. This should already work on all current implementations.\ninline char* string_as_array(std::string* str) {\n  return str->empty() ? NULL : &*str->begin();\n}\n\n}  // namespace snappy\n\n#endif  // THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_STUBS_INTERNAL_H_\n"
        },
        {
          "name": "snappy-stubs-public.h.in",
          "type": "blob",
          "size": 2.623046875,
          "content": "// Copyright 2011 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Various type stubs for the open-source version of Snappy.\n//\n// This file cannot include config.h, as it is included from snappy.h,\n// which is a public header. Instead, snappy-stubs-public.h is generated by\n// from snappy-stubs-public.h.in at configure time.\n\n#ifndef THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_STUBS_PUBLIC_H_\n#define THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_STUBS_PUBLIC_H_\n\n#include <cstddef>\n\n#if ${HAVE_SYS_UIO_H_01}  // HAVE_SYS_UIO_H\n#include <sys/uio.h>\n#endif  // HAVE_SYS_UIO_H\n\n#define SNAPPY_MAJOR ${PROJECT_VERSION_MAJOR}\n#define SNAPPY_MINOR ${PROJECT_VERSION_MINOR}\n#define SNAPPY_PATCHLEVEL ${PROJECT_VERSION_PATCH}\n#define SNAPPY_VERSION \\\n    ((SNAPPY_MAJOR << 16) | (SNAPPY_MINOR << 8) | SNAPPY_PATCHLEVEL)\n\nnamespace snappy {\n\n#if !${HAVE_SYS_UIO_H_01}  // !HAVE_SYS_UIO_H\n// Windows does not have an iovec type, yet the concept is universally useful.\n// It is simple to define it ourselves, so we put it inside our own namespace.\nstruct iovec {\n  void* iov_base;\n  size_t iov_len;\n};\n#endif  // !HAVE_SYS_UIO_H\n\n}  // namespace snappy\n\n#endif  // THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_STUBS_PUBLIC_H_\n"
        },
        {
          "name": "snappy-test.cc",
          "type": "blob",
          "size": 15.849609375,
          "content": "// Copyright 2011 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Various stubs for the unit tests for the open-source version of Snappy.\n\n#include \"snappy-test.h\"\n\n#include <algorithm>\n#include <cstdarg>\n#include <cstdio>\n#include <cstdlib>\n#include <iostream>\n#include <string>\n\nnamespace file {\n\nOptionsStub::OptionsStub() = default;\nOptionsStub::~OptionsStub() = default;\n\nconst OptionsStub &Defaults() {\n  static OptionsStub defaults;\n  return defaults;\n}\n\nStatusStub::StatusStub() = default;\nStatusStub::StatusStub(const StatusStub &) = default;\nStatusStub &StatusStub::operator=(const StatusStub &) = default;\nStatusStub::~StatusStub() = default;\n\nbool StatusStub::ok() { return true; }\n\nStatusStub GetContents(const std::string &filename, std::string *output,\n                       const OptionsStub & /* options */) {\n  std::FILE *fp = std::fopen(filename.c_str(), \"rb\");\n  if (fp == nullptr) {\n    std::perror(filename.c_str());\n    std::exit(1);\n  }\n\n  output->clear();\n  while (!std::feof(fp)) {\n    char buffer[4096];\n    size_t bytes_read = std::fread(buffer, 1, sizeof(buffer), fp);\n    if (bytes_read == 0 && std::ferror(fp)) {\n      std::perror(\"fread\");\n      std::exit(1);\n    }\n    output->append(buffer, bytes_read);\n  }\n\n  std::fclose(fp);\n  return StatusStub();\n}\n\nStatusStub SetContents(const std::string &file_name, const std::string &content,\n                       const OptionsStub & /* options */) {\n  std::FILE *fp = std::fopen(file_name.c_str(), \"wb\");\n  if (fp == nullptr) {\n    std::perror(file_name.c_str());\n    std::exit(1);\n  }\n\n  size_t bytes_written = std::fwrite(content.data(), 1, content.size(), fp);\n  if (bytes_written != content.size()) {\n    std::perror(\"fwrite\");\n    std::exit(1);\n  }\n\n  std::fclose(fp);\n  return StatusStub();\n}\n\n}  // namespace file\n\nnamespace snappy {\n\nstd::string ReadTestDataFile(const std::string& base, size_t size_limit) {\n  std::string contents;\n  const char* srcdir = getenv(\"srcdir\");  // This is set by Automake.\n  std::string prefix;\n  if (srcdir) {\n    prefix = std::string(srcdir) + \"/\";\n  }\n  file::GetContents(prefix + \"testdata/\" + base, &contents, file::Defaults()\n      ).ok();\n  if (size_limit > 0) {\n    contents = contents.substr(0, size_limit);\n  }\n  return contents;\n}\n\nstd::string StrFormat(const char* format, ...) {\n  char buffer[4096];\n  std::va_list ap;\n  va_start(ap, format);\n  std::vsnprintf(buffer, sizeof(buffer), format, ap);\n  va_end(ap);\n  return buffer;\n}\n\nLogMessage::~LogMessage() { std::cerr << std::endl; }\n\nLogMessage &LogMessage::operator<<(const std::string &message) {\n  std::cerr << message;\n  return *this;\n}\n\nLogMessage &LogMessage::operator<<(int number) {\n  std::cerr << number;\n  return *this;\n}\n\n#ifdef _MSC_VER\n// ~LogMessageCrash calls std::abort() and therefore never exits. This is by\n// design, so temporarily disable warning C4722.\n#pragma warning(push)\n#pragma warning(disable : 4722)\n#endif\n\nLogMessageCrash::~LogMessageCrash() {\n  std::cerr << std::endl;\n  std::abort();\n}\n\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n#if HAVE_LIBZ\n\nZLib::ZLib()\n    : comp_init_(false),\n      uncomp_init_(false) {\n  Reinit();\n}\n\nZLib::~ZLib() {\n  if (comp_init_)   { deflateEnd(&comp_stream_); }\n  if (uncomp_init_) { inflateEnd(&uncomp_stream_); }\n}\n\nvoid ZLib::Reinit() {\n  compression_level_ = Z_DEFAULT_COMPRESSION;\n  window_bits_ = MAX_WBITS;\n  mem_level_ =  8;  // DEF_MEM_LEVEL\n  if (comp_init_) {\n    deflateEnd(&comp_stream_);\n    comp_init_ = false;\n  }\n  if (uncomp_init_) {\n    inflateEnd(&uncomp_stream_);\n    uncomp_init_ = false;\n  }\n  first_chunk_ = true;\n}\n\nvoid ZLib::Reset() {\n  first_chunk_ = true;\n}\n\n// --------- COMPRESS MODE\n\n// Initialization method to be called if we hit an error while\n// compressing. On hitting an error, call this method before returning\n// the error.\nvoid ZLib::CompressErrorInit() {\n  deflateEnd(&comp_stream_);\n  comp_init_ = false;\n  Reset();\n}\n\nint ZLib::DeflateInit() {\n  return deflateInit2(&comp_stream_,\n                      compression_level_,\n                      Z_DEFLATED,\n                      window_bits_,\n                      mem_level_,\n                      Z_DEFAULT_STRATEGY);\n}\n\nint ZLib::CompressInit(Bytef *dest, uLongf *destLen,\n                       const Bytef *source, uLong *sourceLen) {\n  int err;\n\n  comp_stream_.next_in = (Bytef*)source;\n  comp_stream_.avail_in = (uInt)*sourceLen;\n  if ((uLong)comp_stream_.avail_in != *sourceLen) return Z_BUF_ERROR;\n  comp_stream_.next_out = dest;\n  comp_stream_.avail_out = (uInt)*destLen;\n  if ((uLong)comp_stream_.avail_out != *destLen) return Z_BUF_ERROR;\n\n  if ( !first_chunk_ )   // only need to set up stream the first time through\n    return Z_OK;\n\n  if (comp_init_) {      // we've already initted it\n    err = deflateReset(&comp_stream_);\n    if (err != Z_OK) {\n      LOG(WARNING) << \"ERROR: Can't reset compress object; creating a new one\";\n      deflateEnd(&comp_stream_);\n      comp_init_ = false;\n    }\n  }\n  if (!comp_init_) {     // first use\n    comp_stream_.zalloc = (alloc_func)0;\n    comp_stream_.zfree = (free_func)0;\n    comp_stream_.opaque = (voidpf)0;\n    err = DeflateInit();\n    if (err != Z_OK) return err;\n    comp_init_ = true;\n  }\n  return Z_OK;\n}\n\n// In a perfect world we'd always have the full buffer to compress\n// when the time came, and we could just call Compress().  Alas, we\n// want to do chunked compression on our webserver.  In this\n// application, we compress the header, send it off, then compress the\n// results, send them off, then compress the footer.  Thus we need to\n// use the chunked compression features of zlib.\nint ZLib::CompressAtMostOrAll(Bytef *dest, uLongf *destLen,\n                              const Bytef *source, uLong *sourceLen,\n                              int flush_mode) {   // Z_FULL_FLUSH or Z_FINISH\n  int err;\n\n  if ( (err=CompressInit(dest, destLen, source, sourceLen)) != Z_OK )\n    return err;\n\n  // This is used to figure out how many bytes we wrote *this chunk*\n  int compressed_size = comp_stream_.total_out;\n\n  // Some setup happens only for the first chunk we compress in a run\n  if ( first_chunk_ ) {\n    first_chunk_ = false;\n  }\n\n  // flush_mode is Z_FINISH for all mode, Z_SYNC_FLUSH for incremental\n  // compression.\n  err = deflate(&comp_stream_, flush_mode);\n\n  *sourceLen = comp_stream_.avail_in;\n\n  if ((err == Z_STREAM_END || err == Z_OK)\n      && comp_stream_.avail_in == 0\n      && comp_stream_.avail_out != 0 ) {\n    // we processed everything ok and the output buffer was large enough.\n    ;\n  } else if (err == Z_STREAM_END && comp_stream_.avail_in > 0) {\n    return Z_BUF_ERROR;                            // should never happen\n  } else if (err != Z_OK && err != Z_STREAM_END && err != Z_BUF_ERROR) {\n    // an error happened\n    CompressErrorInit();\n    return err;\n  } else if (comp_stream_.avail_out == 0) {     // not enough space\n    err = Z_BUF_ERROR;\n  }\n\n  assert(err == Z_OK || err == Z_STREAM_END || err == Z_BUF_ERROR);\n  if (err == Z_STREAM_END)\n    err = Z_OK;\n\n  // update the crc and other metadata\n  compressed_size = comp_stream_.total_out - compressed_size;  // delta\n  *destLen = compressed_size;\n\n  return err;\n}\n\nint ZLib::CompressChunkOrAll(Bytef *dest, uLongf *destLen,\n                             const Bytef *source, uLong sourceLen,\n                             int flush_mode) {   // Z_FULL_FLUSH or Z_FINISH\n  const int ret =\n    CompressAtMostOrAll(dest, destLen, source, &sourceLen, flush_mode);\n  if (ret == Z_BUF_ERROR)\n    CompressErrorInit();\n  return ret;\n}\n\n// This routine only initializes the compression stream once.  Thereafter, it\n// just does a deflateReset on the stream, which should be faster.\nint ZLib::Compress(Bytef *dest, uLongf *destLen,\n                   const Bytef *source, uLong sourceLen) {\n  int err;\n  if ( (err=CompressChunkOrAll(dest, destLen, source, sourceLen,\n                               Z_FINISH)) != Z_OK )\n    return err;\n  Reset();         // reset for next call to Compress\n\n  return Z_OK;\n}\n\n\n// --------- UNCOMPRESS MODE\n\nint ZLib::InflateInit() {\n  return inflateInit2(&uncomp_stream_, MAX_WBITS);\n}\n\n// Initialization method to be called if we hit an error while\n// uncompressing. On hitting an error, call this method before\n// returning the error.\nvoid ZLib::UncompressErrorInit() {\n  inflateEnd(&uncomp_stream_);\n  uncomp_init_ = false;\n  Reset();\n}\n\nint ZLib::UncompressInit(Bytef *dest, uLongf *destLen,\n                         const Bytef *source, uLong *sourceLen) {\n  int err;\n\n  uncomp_stream_.next_in = (Bytef*)source;\n  uncomp_stream_.avail_in = (uInt)*sourceLen;\n  // Check for source > 64K on 16-bit machine:\n  if ((uLong)uncomp_stream_.avail_in != *sourceLen) return Z_BUF_ERROR;\n\n  uncomp_stream_.next_out = dest;\n  uncomp_stream_.avail_out = (uInt)*destLen;\n  if ((uLong)uncomp_stream_.avail_out != *destLen) return Z_BUF_ERROR;\n\n  if ( !first_chunk_ )   // only need to set up stream the first time through\n    return Z_OK;\n\n  if (uncomp_init_) {    // we've already initted it\n    err = inflateReset(&uncomp_stream_);\n    if (err != Z_OK) {\n      LOG(WARNING)\n        << \"ERROR: Can't reset uncompress object; creating a new one\";\n      UncompressErrorInit();\n    }\n  }\n  if (!uncomp_init_) {\n    uncomp_stream_.zalloc = (alloc_func)0;\n    uncomp_stream_.zfree = (free_func)0;\n    uncomp_stream_.opaque = (voidpf)0;\n    err = InflateInit();\n    if (err != Z_OK) return err;\n    uncomp_init_ = true;\n  }\n  return Z_OK;\n}\n\n// If you compressed your data a chunk at a time, with CompressChunk,\n// you can uncompress it a chunk at a time with UncompressChunk.\n// Only difference bewteen chunked and unchunked uncompression\n// is the flush mode we use: Z_SYNC_FLUSH (chunked) or Z_FINISH (unchunked).\nint ZLib::UncompressAtMostOrAll(Bytef *dest, uLongf *destLen,\n                                const Bytef *source, uLong *sourceLen,\n                                int flush_mode) {  // Z_SYNC_FLUSH or Z_FINISH\n  int err = Z_OK;\n\n  if ( (err=UncompressInit(dest, destLen, source, sourceLen)) != Z_OK ) {\n    LOG(WARNING) << \"UncompressInit: Error: \" << err << \" SourceLen: \"\n                 << *sourceLen;\n    return err;\n  }\n\n  // This is used to figure out how many output bytes we wrote *this chunk*:\n  const uLong old_total_out = uncomp_stream_.total_out;\n\n  // This is used to figure out how many input bytes we read *this chunk*:\n  const uLong old_total_in = uncomp_stream_.total_in;\n\n  // Some setup happens only for the first chunk we compress in a run\n  if ( first_chunk_ ) {\n    first_chunk_ = false;                          // so we don't do this again\n\n    // For the first chunk *only* (to avoid infinite troubles), we let\n    // there be no actual data to uncompress.  This sometimes triggers\n    // when the input is only the gzip header, say.\n    if ( *sourceLen == 0 ) {\n      *destLen = 0;\n      return Z_OK;\n    }\n  }\n\n  // We'll uncompress as much as we can.  If we end OK great, otherwise\n  // if we get an error that seems to be the gzip footer, we store the\n  // gzip footer and return OK, otherwise we return the error.\n\n  // flush_mode is Z_SYNC_FLUSH for chunked mode, Z_FINISH for all mode.\n  err = inflate(&uncomp_stream_, flush_mode);\n\n  // Figure out how many bytes of the input zlib slurped up:\n  const uLong bytes_read = uncomp_stream_.total_in - old_total_in;\n  CHECK_LE(source + bytes_read, source + *sourceLen);\n  *sourceLen = uncomp_stream_.avail_in;\n\n  if ((err == Z_STREAM_END || err == Z_OK)  // everything went ok\n             && uncomp_stream_.avail_in == 0) {    // and we read it all\n    ;\n  } else if (err == Z_STREAM_END && uncomp_stream_.avail_in > 0) {\n    LOG(WARNING)\n      << \"UncompressChunkOrAll: Received some extra data, bytes total: \"\n      << uncomp_stream_.avail_in << \" bytes: \"\n      << std::string(reinterpret_cast<const char *>(uncomp_stream_.next_in),\n                     std::min(int(uncomp_stream_.avail_in), 20));\n    UncompressErrorInit();\n    return Z_DATA_ERROR;       // what's the extra data for?\n  } else if (err != Z_OK && err != Z_STREAM_END && err != Z_BUF_ERROR) {\n    // an error happened\n    LOG(WARNING) << \"UncompressChunkOrAll: Error: \" << err\n                 << \" avail_out: \" << uncomp_stream_.avail_out;\n    UncompressErrorInit();\n    return err;\n  } else if (uncomp_stream_.avail_out == 0) {\n    err = Z_BUF_ERROR;\n  }\n\n  assert(err == Z_OK || err == Z_BUF_ERROR || err == Z_STREAM_END);\n  if (err == Z_STREAM_END)\n    err = Z_OK;\n\n  *destLen = uncomp_stream_.total_out - old_total_out;  // size for this call\n\n  return err;\n}\n\nint ZLib::UncompressChunkOrAll(Bytef *dest, uLongf *destLen,\n                               const Bytef *source, uLong sourceLen,\n                               int flush_mode) {  // Z_SYNC_FLUSH or Z_FINISH\n  const int ret =\n    UncompressAtMostOrAll(dest, destLen, source, &sourceLen, flush_mode);\n  if (ret == Z_BUF_ERROR)\n    UncompressErrorInit();\n  return ret;\n}\n\nint ZLib::UncompressAtMost(Bytef *dest, uLongf *destLen,\n                          const Bytef *source, uLong *sourceLen) {\n  return UncompressAtMostOrAll(dest, destLen, source, sourceLen, Z_SYNC_FLUSH);\n}\n\n// We make sure we've uncompressed everything, that is, the current\n// uncompress stream is at a compressed-buffer-EOF boundary.  In gzip\n// mode, we also check the gzip footer to make sure we pass the gzip\n// consistency checks.  We RETURN true iff both types of checks pass.\nbool ZLib::UncompressChunkDone() {\n  assert(!first_chunk_ && uncomp_init_);\n  // Make sure we're at the end-of-compressed-data point.  This means\n  // if we call inflate with Z_FINISH we won't consume any input or\n  // write any output\n  Bytef dummyin, dummyout;\n  uLongf dummylen = 0;\n  if ( UncompressChunkOrAll(&dummyout, &dummylen, &dummyin, 0, Z_FINISH)\n       != Z_OK ) {\n    return false;\n  }\n\n  // Make sure that when we exit, we can start a new round of chunks later\n  Reset();\n\n  return true;\n}\n\n// Uncompresses the source buffer into the destination buffer.\n// The destination buffer must be long enough to hold the entire\n// decompressed contents.\n//\n// We only initialize the uncomp_stream once.  Thereafter, we use\n// inflateReset, which should be faster.\n//\n// Returns Z_OK on success, otherwise, it returns a zlib error code.\nint ZLib::Uncompress(Bytef *dest, uLongf *destLen,\n                     const Bytef *source, uLong sourceLen) {\n  int err;\n  if ( (err=UncompressChunkOrAll(dest, destLen, source, sourceLen,\n                                 Z_FINISH)) != Z_OK ) {\n    Reset();                           // let us try to compress again\n    return err;\n  }\n  if ( !UncompressChunkDone() )        // calls Reset()\n    return Z_DATA_ERROR;\n  return Z_OK;  // stream_end is ok\n}\n\n#endif  // HAVE_LIBZ\n\n}  // namespace snappy\n"
        },
        {
          "name": "snappy-test.h",
          "type": "blob",
          "size": 11.2265625,
          "content": "// Copyright 2011 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Various stubs for the unit tests for the open-source version of Snappy.\n\n#ifndef THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_TEST_H_\n#define THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_TEST_H_\n\n#if HAVE_CONFIG_H\n#include \"config.h\"\n#endif\n\n#include \"snappy-stubs-internal.h\"\n\n#if HAVE_SYS_MMAN_H\n#include <sys/mman.h>\n#endif\n\n#if HAVE_SYS_RESOURCE_H\n#include <sys/resource.h>\n#endif\n\n#if HAVE_SYS_TIME_H\n#include <sys/time.h>\n#endif\n\n#if HAVE_WINDOWS_H\n// Needed to be able to use std::max without workarounds in the source code.\n// https://support.microsoft.com/en-us/help/143208/prb-using-stl-in-windows-program-can-cause-min-max-conflicts\n#define NOMINMAX\n#include <windows.h>\n#endif\n\n#define InitGoogle(argv0, argc, argv, remove_flags) ((void)(0))\n\n#if HAVE_LIBZ\n#include \"zlib.h\"\n#endif\n\n#if HAVE_LIBLZO2\n#include \"lzo/lzo1x.h\"\n#endif\n\n#if HAVE_LIBLZ4\n#include \"lz4.h\"\n#endif\n\nnamespace file {\n\n// Stubs the class file::Options.\n//\n// This class should not be instantiated explicitly. It should only be used by\n// passing file::Defaults() to file::GetContents() / file::SetContents().\nclass OptionsStub {\n public:\n  OptionsStub();\n  OptionsStub(const OptionsStub &) = delete;\n  OptionsStub &operator=(const OptionsStub &) = delete;\n  ~OptionsStub();\n};\n\nconst OptionsStub &Defaults();\n\n// Stubs the class absl::Status.\n//\n// This class should not be instantiated explicitly. It should only be used by\n// passing the result of file::GetContents() / file::SetContents() to\n// CHECK_OK().\nclass StatusStub {\n public:\n  StatusStub();\n  StatusStub(const StatusStub &);\n  StatusStub &operator=(const StatusStub &);\n  ~StatusStub();\n\n  bool ok();\n};\n\nStatusStub GetContents(const std::string &file_name, std::string *output,\n                       const OptionsStub & /* options */);\n\nStatusStub SetContents(const std::string &file_name, const std::string &content,\n                       const OptionsStub & /* options */);\n\n}  // namespace file\n\nnamespace snappy {\n\n#define FLAGS_test_random_seed 301\n\nstd::string ReadTestDataFile(const std::string& base, size_t size_limit);\n\n// A std::sprintf() variant that returns a std::string.\n// Not safe for general use due to truncation issues.\nstd::string StrFormat(const char* format, ...);\n\n// A wall-time clock. This stub is not super-accurate, nor resistant to the\n// system time changing.\nclass CycleTimer {\n public:\n  inline CycleTimer() : real_time_us_(0) {}\n  inline ~CycleTimer() = default;\n\n  inline void Start() {\n#ifdef WIN32\n    QueryPerformanceCounter(&start_);\n#else\n    ::gettimeofday(&start_, nullptr);\n#endif\n  }\n\n  inline void Stop() {\n#ifdef WIN32\n    LARGE_INTEGER stop;\n    LARGE_INTEGER frequency;\n    QueryPerformanceCounter(&stop);\n    QueryPerformanceFrequency(&frequency);\n\n    double elapsed = static_cast<double>(stop.QuadPart - start_.QuadPart) /\n        frequency.QuadPart;\n    real_time_us_ += elapsed * 1e6 + 0.5;\n#else\n    struct ::timeval stop;\n    ::gettimeofday(&stop, nullptr);\n\n    real_time_us_ += 1000000 * (stop.tv_sec - start_.tv_sec);\n    real_time_us_ += (stop.tv_usec - start_.tv_usec);\n#endif\n  }\n\n  inline double Get() { return real_time_us_ * 1e-6; }\n\n private:\n  int64_t real_time_us_;\n#ifdef WIN32\n  LARGE_INTEGER start_;\n#else\n  struct ::timeval start_;\n#endif\n};\n\n// Logging.\n\nclass LogMessage {\n public:\n  inline LogMessage() = default;\n  ~LogMessage();\n\n  LogMessage &operator<<(const std::string &message);\n  LogMessage &operator<<(int number);\n};\n\nclass LogMessageCrash : public LogMessage {\n public:\n  inline LogMessageCrash() = default;\n  ~LogMessageCrash();\n};\n\n// This class is used to explicitly ignore values in the conditional\n// logging macros.  This avoids compiler warnings like \"value computed\n// is not used\" and \"statement has no effect\".\n\nclass LogMessageVoidify {\n public:\n  inline LogMessageVoidify() = default;\n  inline ~LogMessageVoidify() = default;\n\n  // This has to be an operator with a precedence lower than << but\n  // higher than ?:\n  inline void operator&(const LogMessage &) {}\n};\n\n// Asserts, both versions activated in debug mode only,\n// and ones that are always active.\n\n#define CRASH_UNLESS(condition)  \\\n  SNAPPY_PREDICT_TRUE(condition) \\\n      ? (void)0                  \\\n      : snappy::LogMessageVoidify() & snappy::LogMessageCrash()\n\n#define LOG(level) LogMessage()\n#define VLOG(level) \\\n  true ? (void)0 : snappy::LogMessageVoidify() & snappy::LogMessage()\n\n#define CHECK(cond) CRASH_UNLESS(cond)\n#define CHECK_LE(a, b) CRASH_UNLESS((a) <= (b))\n#define CHECK_GE(a, b) CRASH_UNLESS((a) >= (b))\n#define CHECK_EQ(a, b) CRASH_UNLESS((a) == (b))\n#define CHECK_NE(a, b) CRASH_UNLESS((a) != (b))\n#define CHECK_LT(a, b) CRASH_UNLESS((a) < (b))\n#define CHECK_GT(a, b) CRASH_UNLESS((a) > (b))\n#define CHECK_OK(cond) (cond).ok()\n\n#if HAVE_LIBZ\n\n// Object-oriented wrapper around zlib.\nclass ZLib {\n public:\n  ZLib();\n  ~ZLib();\n\n  // Wipe a ZLib object to a virgin state.  This differs from Reset()\n  // in that it also breaks any state.\n  void Reinit();\n\n  // Call this to make a zlib buffer as good as new.  Here's the only\n  // case where they differ:\n  //    CompressChunk(a); CompressChunk(b); CompressChunkDone();   vs\n  //    CompressChunk(a); Reset(); CompressChunk(b); CompressChunkDone();\n  // You'll want to use Reset(), then, when you interrupt a compress\n  // (or uncompress) in the middle of a chunk and want to start over.\n  void Reset();\n\n  // According to the zlib manual, when you Compress, the destination\n  // buffer must have size at least src + .1%*src + 12.  This function\n  // helps you calculate that.  Augment this to account for a potential\n  // gzip header and footer, plus a few bytes of slack.\n  static int MinCompressbufSize(int uncompress_size) {\n    return uncompress_size + uncompress_size/1000 + 40;\n  }\n\n  // Compresses the source buffer into the destination buffer.\n  // sourceLen is the byte length of the source buffer.\n  // Upon entry, destLen is the total size of the destination buffer,\n  // which must be of size at least MinCompressbufSize(sourceLen).\n  // Upon exit, destLen is the actual size of the compressed buffer.\n  //\n  // This function can be used to compress a whole file at once if the\n  // input file is mmap'ed.\n  //\n  // Returns Z_OK if success, Z_MEM_ERROR if there was not\n  // enough memory, Z_BUF_ERROR if there was not enough room in the\n  // output buffer. Note that if the output buffer is exactly the same\n  // size as the compressed result, we still return Z_BUF_ERROR.\n  // (check CL#1936076)\n  int Compress(Bytef *dest, uLongf *destLen,\n               const Bytef *source, uLong sourceLen);\n\n  // Uncompresses the source buffer into the destination buffer.\n  // The destination buffer must be long enough to hold the entire\n  // decompressed contents.\n  //\n  // Returns Z_OK on success, otherwise, it returns a zlib error code.\n  int Uncompress(Bytef *dest, uLongf *destLen,\n                 const Bytef *source, uLong sourceLen);\n\n  // Uncompress data one chunk at a time -- ie you can call this\n  // more than once.  To get this to work you need to call per-chunk\n  // and \"done\" routines.\n  //\n  // Returns Z_OK if success, Z_MEM_ERROR if there was not\n  // enough memory, Z_BUF_ERROR if there was not enough room in the\n  // output buffer.\n\n  int UncompressAtMost(Bytef *dest, uLongf *destLen,\n                       const Bytef *source, uLong *sourceLen);\n\n  // Checks gzip footer information, as needed.  Mostly this just\n  // makes sure the checksums match.  Whenever you call this, it\n  // will assume the last 8 bytes from the previous UncompressChunk\n  // call are the footer.  Returns true iff everything looks ok.\n  bool UncompressChunkDone();\n\n private:\n  int InflateInit();       // sets up the zlib inflate structure\n  int DeflateInit();       // sets up the zlib deflate structure\n\n  // These init the zlib data structures for compressing/uncompressing\n  int CompressInit(Bytef *dest, uLongf *destLen,\n                   const Bytef *source, uLong *sourceLen);\n  int UncompressInit(Bytef *dest, uLongf *destLen,\n                     const Bytef *source, uLong *sourceLen);\n  // Initialization method to be called if we hit an error while\n  // uncompressing. On hitting an error, call this method before\n  // returning the error.\n  void UncompressErrorInit();\n\n  // Helper function for Compress\n  int CompressChunkOrAll(Bytef *dest, uLongf *destLen,\n                         const Bytef *source, uLong sourceLen,\n                         int flush_mode);\n  int CompressAtMostOrAll(Bytef *dest, uLongf *destLen,\n                          const Bytef *source, uLong *sourceLen,\n                          int flush_mode);\n\n  // Likewise for UncompressAndUncompressChunk\n  int UncompressChunkOrAll(Bytef *dest, uLongf *destLen,\n                           const Bytef *source, uLong sourceLen,\n                           int flush_mode);\n\n  int UncompressAtMostOrAll(Bytef *dest, uLongf *destLen,\n                            const Bytef *source, uLong *sourceLen,\n                            int flush_mode);\n\n  // Initialization method to be called if we hit an error while\n  // compressing. On hitting an error, call this method before\n  // returning the error.\n  void CompressErrorInit();\n\n  int compression_level_;   // compression level\n  int window_bits_;         // log base 2 of the window size used in compression\n  int mem_level_;           // specifies the amount of memory to be used by\n                            // compressor (1-9)\n  z_stream comp_stream_;    // Zlib stream data structure\n  bool comp_init_;          // True if we have initialized comp_stream_\n  z_stream uncomp_stream_;  // Zlib stream data structure\n  bool uncomp_init_;        // True if we have initialized uncomp_stream_\n\n  // These are used only with chunked compression.\n  bool first_chunk_;       // true if we need to emit headers with this chunk\n};\n\n#endif  // HAVE_LIBZ\n\n}  // namespace snappy\n\n#endif  // THIRD_PARTY_SNAPPY_OPENSOURCE_SNAPPY_TEST_H_\n"
        },
        {
          "name": "snappy.cc",
          "type": "blob",
          "size": 98.1513671875,
          "content": "// Copyright 2005 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#include \"snappy-internal.h\"\n#include \"snappy-sinksource.h\"\n#include \"snappy.h\"\n#if !defined(SNAPPY_HAVE_BMI2)\n// __BMI2__ is defined by GCC and Clang. Visual Studio doesn't target BMI2\n// specifically, but it does define __AVX2__ when AVX2 support is available.\n// Fortunately, AVX2 was introduced in Haswell, just like BMI2.\n//\n// BMI2 is not defined as a subset of AVX2 (unlike SSSE3 and AVX above). So,\n// GCC and Clang can build code with AVX2 enabled but BMI2 disabled, in which\n// case issuing BMI2 instructions results in a compiler error.\n#if defined(__BMI2__) || (defined(_MSC_VER) && defined(__AVX2__))\n#define SNAPPY_HAVE_BMI2 1\n#else\n#define SNAPPY_HAVE_BMI2 0\n#endif\n#endif  // !defined(SNAPPY_HAVE_BMI2)\n\n#if !defined(SNAPPY_HAVE_X86_CRC32)\n#if defined(__SSE4_2__)\n#define SNAPPY_HAVE_X86_CRC32 1\n#else\n#define SNAPPY_HAVE_X86_CRC32 0\n#endif\n#endif  // !defined(SNAPPY_HAVE_X86_CRC32)\n\n#if !defined(SNAPPY_HAVE_NEON_CRC32)\n#if SNAPPY_HAVE_NEON && defined(__ARM_FEATURE_CRC32)\n#define SNAPPY_HAVE_NEON_CRC32 1\n#else\n#define SNAPPY_HAVE_NEON_CRC32 0\n#endif\n#endif  // !defined(SNAPPY_HAVE_NEON_CRC32)\n\n#if SNAPPY_HAVE_BMI2 || SNAPPY_HAVE_X86_CRC32\n// Please do not replace with <x86intrin.h>. or with headers that assume more\n// advanced SSE versions without checking with all the OWNERS.\n#include <immintrin.h>\n#elif SNAPPY_HAVE_NEON_CRC32\n#include <arm_acle.h>\n#endif\n\n#include <algorithm>\n#include <array>\n#include <cstddef>\n#include <cstdint>\n#include <cstdio>\n#include <cstring>\n#include <functional>\n#include <memory>\n#include <string>\n#include <utility>\n#include <vector>\n\nnamespace snappy {\n\nnamespace {\n\n// The amount of slop bytes writers are using for unconditional copies.\nconstexpr int kSlopBytes = 64;\n\nusing internal::char_table;\nusing internal::COPY_1_BYTE_OFFSET;\nusing internal::COPY_2_BYTE_OFFSET;\nusing internal::COPY_4_BYTE_OFFSET;\nusing internal::kMaximumTagLength;\nusing internal::LITERAL;\n#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\nusing internal::V128;\nusing internal::V128_Load;\nusing internal::V128_LoadU;\nusing internal::V128_Shuffle;\nusing internal::V128_StoreU;\nusing internal::V128_DupChar;\n#endif\n\n// We translate the information encoded in a tag through a lookup table to a\n// format that requires fewer instructions to decode. Effectively we store\n// the length minus the tag part of the offset. The lowest significant byte\n// thus stores the length. While total length - offset is given by\n// entry - ExtractOffset(type). The nice thing is that the subtraction\n// immediately sets the flags for the necessary check that offset >= length.\n// This folds the cmp with sub. We engineer the long literals and copy-4 to\n// always fail this check, so their presence doesn't affect the fast path.\n// To prevent literals from triggering the guard against offset < length (offset\n// does not apply to literals) the table is giving them a spurious offset of\n// 256.\ninline constexpr int16_t MakeEntry(int16_t len, int16_t offset) {\n  return len - (offset << 8);\n}\n\ninline constexpr int16_t LengthMinusOffset(int data, int type) {\n  return type == 3   ? 0xFF                    // copy-4 (or type == 3)\n         : type == 2 ? MakeEntry(data + 1, 0)  // copy-2\n         : type == 1 ? MakeEntry((data & 7) + 4, data >> 3)  // copy-1\n         : data < 60 ? MakeEntry(data + 1, 1)  // note spurious offset.\n                     : 0xFF;                   // long literal\n}\n\ninline constexpr int16_t LengthMinusOffset(uint8_t tag) {\n  return LengthMinusOffset(tag >> 2, tag & 3);\n}\n\ntemplate <size_t... Ints>\nstruct index_sequence {};\n\ntemplate <std::size_t N, size_t... Is>\nstruct make_index_sequence : make_index_sequence<N - 1, N - 1, Is...> {};\n\ntemplate <size_t... Is>\nstruct make_index_sequence<0, Is...> : index_sequence<Is...> {};\n\ntemplate <size_t... seq>\nconstexpr std::array<int16_t, 256> MakeTable(index_sequence<seq...>) {\n  return std::array<int16_t, 256>{LengthMinusOffset(seq)...};\n}\n\nalignas(64) const std::array<int16_t, 256> kLengthMinusOffset =\n    MakeTable(make_index_sequence<256>{});\n\n// Given a table of uint16_t whose size is mask / 2 + 1, return a pointer to the\n// relevant entry, if any, for the given bytes.  Any hash function will do,\n// but a good hash function reduces the number of collisions and thus yields\n// better compression for compressible input.\n//\n// REQUIRES: mask is 2 * (table_size - 1), and table_size is a power of two.\ninline uint16_t* TableEntry(uint16_t* table, uint32_t bytes, uint32_t mask) {\n  // Our choice is quicker-and-dirtier than the typical hash function;\n  // empirically, that seems beneficial.  The upper bits of kMagic * bytes are a\n  // higher-quality hash than the lower bits, so when using kMagic * bytes we\n  // also shift right to get a higher-quality end result.  There's no similar\n  // issue with a CRC because all of the output bits of a CRC are equally good\n  // \"hashes.\" So, a CPU instruction for CRC, if available, tends to be a good\n  // choice.\n#if SNAPPY_HAVE_NEON_CRC32\n  // We use mask as the second arg to the CRC function, as it's about to\n  // be used anyway; it'd be equally correct to use 0 or some constant.\n  // Mathematically, _mm_crc32_u32 (or similar) is a function of the\n  // xor of its arguments.\n  const uint32_t hash = __crc32cw(bytes, mask);\n#elif SNAPPY_HAVE_X86_CRC32\n  const uint32_t hash = _mm_crc32_u32(bytes, mask);\n#else\n  constexpr uint32_t kMagic = 0x1e35a7bd;\n  const uint32_t hash = (kMagic * bytes) >> (31 - kMaxHashTableBits);\n#endif\n  return reinterpret_cast<uint16_t*>(reinterpret_cast<uintptr_t>(table) +\n                                     (hash & mask));\n}\n\ninline uint16_t* TableEntry4ByteMatch(uint16_t* table, uint32_t bytes,\n                                      uint32_t mask) {\n  constexpr uint32_t kMagic = 2654435761U;\n  const uint32_t hash = (kMagic * bytes) >> (32 - kMaxHashTableBits);\n  return reinterpret_cast<uint16_t*>(reinterpret_cast<uintptr_t>(table) +\n                                     (hash & mask));\n}\n\ninline uint16_t* TableEntry8ByteMatch(uint16_t* table, uint64_t bytes,\n                                      uint32_t mask) {\n  constexpr uint64_t kMagic = 58295818150454627ULL;\n  const uint32_t hash = (kMagic * bytes) >> (64 - kMaxHashTableBits);\n  return reinterpret_cast<uint16_t*>(reinterpret_cast<uintptr_t>(table) +\n                                     (hash & mask));\n}\n\n}  // namespace\n\nsize_t MaxCompressedLength(size_t source_bytes) {\n  // Compressed data can be defined as:\n  //    compressed := item* literal*\n  //    item       := literal* copy\n  //\n  // The trailing literal sequence has a space blowup of at most 62/60\n  // since a literal of length 60 needs one tag byte + one extra byte\n  // for length information.\n  //\n  // Item blowup is trickier to measure.  Suppose the \"copy\" op copies\n  // 4 bytes of data.  Because of a special check in the encoding code,\n  // we produce a 4-byte copy only if the offset is < 65536.  Therefore\n  // the copy op takes 3 bytes to encode, and this type of item leads\n  // to at most the 62/60 blowup for representing literals.\n  //\n  // Suppose the \"copy\" op copies 5 bytes of data.  If the offset is big\n  // enough, it will take 5 bytes to encode the copy op.  Therefore the\n  // worst case here is a one-byte literal followed by a five-byte copy.\n  // I.e., 6 bytes of input turn into 7 bytes of \"compressed\" data.\n  //\n  // This last factor dominates the blowup, so the final estimate is:\n  return 32 + source_bytes + source_bytes / 6;\n}\n\nnamespace {\n\nvoid UnalignedCopy64(const void* src, void* dst) {\n  char tmp[8];\n  std::memcpy(tmp, src, 8);\n  std::memcpy(dst, tmp, 8);\n}\n\nvoid UnalignedCopy128(const void* src, void* dst) {\n  // std::memcpy() gets vectorized when the appropriate compiler options are\n  // used. For example, x86 compilers targeting SSE2+ will optimize to an SSE2\n  // load and store.\n  char tmp[16];\n  std::memcpy(tmp, src, 16);\n  std::memcpy(dst, tmp, 16);\n}\n\ntemplate <bool use_16bytes_chunk>\ninline void ConditionalUnalignedCopy128(const char* src, char* dst) {\n  if (use_16bytes_chunk) {\n    UnalignedCopy128(src, dst);\n  } else {\n    UnalignedCopy64(src, dst);\n    UnalignedCopy64(src + 8, dst + 8);\n  }\n}\n\n// Copy [src, src+(op_limit-op)) to [op, (op_limit-op)) a byte at a time. Used\n// for handling COPY operations where the input and output regions may overlap.\n// For example, suppose:\n//    src       == \"ab\"\n//    op        == src + 2\n//    op_limit  == op + 20\n// After IncrementalCopySlow(src, op, op_limit), the result will have eleven\n// copies of \"ab\"\n//    ababababababababababab\n// Note that this does not match the semantics of either std::memcpy() or\n// std::memmove().\ninline char* IncrementalCopySlow(const char* src, char* op,\n                                 char* const op_limit) {\n  // TODO: Remove pragma when LLVM is aware this\n  // function is only called in cold regions and when cold regions don't get\n  // vectorized or unrolled.\n#ifdef __clang__\n#pragma clang loop unroll(disable)\n#endif\n  while (op < op_limit) {\n    *op++ = *src++;\n  }\n  return op_limit;\n}\n\n#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n\n// Computes the bytes for shuffle control mask (please read comments on\n// 'pattern_generation_masks' as well) for the given index_offset and\n// pattern_size. For example, when the 'offset' is 6, it will generate a\n// repeating pattern of size 6. So, the first 16 byte indexes will correspond to\n// the pattern-bytes {0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3} and the\n// next 16 byte indexes will correspond to the pattern-bytes {4, 5, 0, 1, 2, 3,\n// 4, 5, 0, 1, 2, 3, 4, 5, 0, 1}. These byte index sequences are generated by\n// calling MakePatternMaskBytes(0, 6, index_sequence<16>()) and\n// MakePatternMaskBytes(16, 6, index_sequence<16>()) respectively.\ntemplate <size_t... indexes>\ninline constexpr std::array<char, sizeof...(indexes)> MakePatternMaskBytes(\n    int index_offset, int pattern_size, index_sequence<indexes...>) {\n  return {static_cast<char>((index_offset + indexes) % pattern_size)...};\n}\n\n// Computes the shuffle control mask bytes array for given pattern-sizes and\n// returns an array.\ntemplate <size_t... pattern_sizes_minus_one>\ninline constexpr std::array<std::array<char, sizeof(V128)>,\n                            sizeof...(pattern_sizes_minus_one)>\nMakePatternMaskBytesTable(int index_offset,\n                          index_sequence<pattern_sizes_minus_one...>) {\n  return {\n      MakePatternMaskBytes(index_offset, pattern_sizes_minus_one + 1,\n                           make_index_sequence</*indexes=*/sizeof(V128)>())...};\n}\n\n// This is an array of shuffle control masks that can be used as the source\n// operand for PSHUFB to permute the contents of the destination XMM register\n// into a repeating byte pattern.\nalignas(16) constexpr std::array<std::array<char, sizeof(V128)>,\n                                 16> pattern_generation_masks =\n    MakePatternMaskBytesTable(\n        /*index_offset=*/0,\n        /*pattern_sizes_minus_one=*/make_index_sequence<16>());\n\n// Similar to 'pattern_generation_masks', this table is used to \"rotate\" the\n// pattern so that we can copy the *next 16 bytes* consistent with the pattern.\n// Basically, pattern_reshuffle_masks is a continuation of\n// pattern_generation_masks. It follows that, pattern_reshuffle_masks is same as\n// pattern_generation_masks for offsets 1, 2, 4, 8 and 16.\nalignas(16) constexpr std::array<std::array<char, sizeof(V128)>,\n                                 16> pattern_reshuffle_masks =\n    MakePatternMaskBytesTable(\n        /*index_offset=*/16,\n        /*pattern_sizes_minus_one=*/make_index_sequence<16>());\n\nSNAPPY_ATTRIBUTE_ALWAYS_INLINE\nstatic inline V128 LoadPattern(const char* src, const size_t pattern_size) {\n  V128 generation_mask = V128_Load(reinterpret_cast<const V128*>(\n      pattern_generation_masks[pattern_size - 1].data()));\n  // Uninitialized bytes are masked out by the shuffle mask.\n  // TODO: remove annotation and macro defs once MSan is fixed.\n  SNAPPY_ANNOTATE_MEMORY_IS_INITIALIZED(src + pattern_size, 16 - pattern_size);\n  return V128_Shuffle(V128_LoadU(reinterpret_cast<const V128*>(src)),\n                      generation_mask);\n}\n\nSNAPPY_ATTRIBUTE_ALWAYS_INLINE\nstatic inline std::pair<V128 /* pattern */, V128 /* reshuffle_mask */>\nLoadPatternAndReshuffleMask(const char* src, const size_t pattern_size) {\n  V128 pattern = LoadPattern(src, pattern_size);\n\n  // This mask will generate the next 16 bytes in-place. Doing so enables us to\n  // write data by at most 4 V128_StoreU.\n  //\n  // For example, suppose pattern is:        abcdefabcdefabcd\n  // Shuffling with this mask will generate: efabcdefabcdefab\n  // Shuffling again will generate:          cdefabcdefabcdef\n  V128 reshuffle_mask = V128_Load(reinterpret_cast<const V128*>(\n      pattern_reshuffle_masks[pattern_size - 1].data()));\n  return {pattern, reshuffle_mask};\n}\n\n#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n\n// Fallback for when we need to copy while extending the pattern, for example\n// copying 10 bytes from 3 positions back abc -> abcabcabcabca.\n//\n// REQUIRES: [dst - offset, dst + 64) is a valid address range.\nSNAPPY_ATTRIBUTE_ALWAYS_INLINE\nstatic inline bool Copy64BytesWithPatternExtension(char* dst, size_t offset) {\n#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n  if (SNAPPY_PREDICT_TRUE(offset <= 16)) {\n    switch (offset) {\n      case 0:\n        return false;\n      case 1: {\n        // TODO: Ideally we should memset, move back once the\n        // codegen issues are fixed.\n        V128 pattern = V128_DupChar(dst[-1]);\n        for (int i = 0; i < 4; i++) {\n          V128_StoreU(reinterpret_cast<V128*>(dst + 16 * i), pattern);\n        }\n        return true;\n      }\n      case 2:\n      case 4:\n      case 8:\n      case 16: {\n        V128 pattern = LoadPattern(dst - offset, offset);\n        for (int i = 0; i < 4; i++) {\n          V128_StoreU(reinterpret_cast<V128*>(dst + 16 * i), pattern);\n        }\n        return true;\n      }\n      default: {\n        auto pattern_and_reshuffle_mask =\n            LoadPatternAndReshuffleMask(dst - offset, offset);\n        V128 pattern = pattern_and_reshuffle_mask.first;\n        V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n        for (int i = 0; i < 4; i++) {\n          V128_StoreU(reinterpret_cast<V128*>(dst + 16 * i), pattern);\n          pattern = V128_Shuffle(pattern, reshuffle_mask);\n        }\n        return true;\n      }\n    }\n  }\n#else\n  if (SNAPPY_PREDICT_TRUE(offset < 16)) {\n    if (SNAPPY_PREDICT_FALSE(offset == 0)) return false;\n    // Extend the pattern to the first 16 bytes.\n    // The simpler formulation of `dst[i - offset]` induces undefined behavior.\n    for (int i = 0; i < 16; i++) dst[i] = (dst - offset)[i];\n    // Find a multiple of pattern >= 16.\n    static std::array<uint8_t, 16> pattern_sizes = []() {\n      std::array<uint8_t, 16> res;\n      for (int i = 1; i < 16; i++) res[i] = (16 / i + 1) * i;\n      return res;\n    }();\n    offset = pattern_sizes[offset];\n    for (int i = 1; i < 4; i++) {\n      std::memcpy(dst + i * 16, dst + i * 16 - offset, 16);\n    }\n    return true;\n  }\n#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n\n  // Very rare.\n  for (int i = 0; i < 4; i++) {\n    std::memcpy(dst + i * 16, dst + i * 16 - offset, 16);\n  }\n  return true;\n}\n\n// Copy [src, src+(op_limit-op)) to [op, op_limit) but faster than\n// IncrementalCopySlow. buf_limit is the address past the end of the writable\n// region of the buffer.\ninline char* IncrementalCopy(const char* src, char* op, char* const op_limit,\n                             char* const buf_limit) {\n#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n  constexpr int big_pattern_size_lower_bound = 16;\n#else\n  constexpr int big_pattern_size_lower_bound = 8;\n#endif\n\n  // Terminology:\n  //\n  // slop = buf_limit - op\n  // pat  = op - src\n  // len  = op_limit - op\n  assert(src < op);\n  assert(op < op_limit);\n  assert(op_limit <= buf_limit);\n  // NOTE: The copy tags use 3 or 6 bits to store the copy length, so len <= 64.\n  assert(op_limit - op <= 64);\n  // NOTE: In practice the compressor always emits len >= 4, so it is ok to\n  // assume that to optimize this function, but this is not guaranteed by the\n  // compression format, so we have to also handle len < 4 in case the input\n  // does not satisfy these conditions.\n\n  size_t pattern_size = op - src;\n  // The cases are split into different branches to allow the branch predictor,\n  // FDO, and static prediction hints to work better. For each input we list the\n  // ratio of invocations that match each condition.\n  //\n  // input        slop < 16   pat < 8  len > 16\n  // ------------------------------------------\n  // html|html4|cp   0%         1.01%    27.73%\n  // urls            0%         0.88%    14.79%\n  // jpg             0%        64.29%     7.14%\n  // pdf             0%         2.56%    58.06%\n  // txt[1-4]        0%         0.23%     0.97%\n  // pb              0%         0.96%    13.88%\n  // bin             0.01%     22.27%    41.17%\n  //\n  // It is very rare that we don't have enough slop for doing block copies. It\n  // is also rare that we need to expand a pattern. Small patterns are common\n  // for incompressible formats and for those we are plenty fast already.\n  // Lengths are normally not greater than 16 but they vary depending on the\n  // input. In general if we always predict len <= 16 it would be an ok\n  // prediction.\n  //\n  // In order to be fast we want a pattern >= 16 bytes (or 8 bytes in non-SSE)\n  // and an unrolled loop copying 1x 16 bytes (or 2x 8 bytes in non-SSE) at a\n  // time.\n\n  // Handle the uncommon case where pattern is less than 16 (or 8 in non-SSE)\n  // bytes.\n  if (pattern_size < big_pattern_size_lower_bound) {\n#if SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n    // Load the first eight bytes into an 128-bit XMM register, then use PSHUFB\n    // to permute the register's contents in-place into a repeating sequence of\n    // the first \"pattern_size\" bytes.\n    // For example, suppose:\n    //    src       == \"abc\"\n    //    op        == op + 3\n    // After V128_Shuffle(), \"pattern\" will have five copies of \"abc\"\n    // followed by one byte of slop: abcabcabcabcabca.\n    //\n    // The non-SSE fallback implementation suffers from store-forwarding stalls\n    // because its loads and stores partly overlap. By expanding the pattern\n    // in-place, we avoid the penalty.\n\n    // Typically, the op_limit is the gating factor so try to simplify the loop\n    // based on that.\n    if (SNAPPY_PREDICT_TRUE(op_limit <= buf_limit - 15)) {\n      auto pattern_and_reshuffle_mask =\n          LoadPatternAndReshuffleMask(src, pattern_size);\n      V128 pattern = pattern_and_reshuffle_mask.first;\n      V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n\n      // There is at least one, and at most four 16-byte blocks. Writing four\n      // conditionals instead of a loop allows FDO to layout the code with\n      // respect to the actual probabilities of each length.\n      // TODO: Replace with loop with trip count hint.\n      V128_StoreU(reinterpret_cast<V128*>(op), pattern);\n\n      if (op + 16 < op_limit) {\n        pattern = V128_Shuffle(pattern, reshuffle_mask);\n        V128_StoreU(reinterpret_cast<V128*>(op + 16), pattern);\n      }\n      if (op + 32 < op_limit) {\n        pattern = V128_Shuffle(pattern, reshuffle_mask);\n        V128_StoreU(reinterpret_cast<V128*>(op + 32), pattern);\n      }\n      if (op + 48 < op_limit) {\n        pattern = V128_Shuffle(pattern, reshuffle_mask);\n        V128_StoreU(reinterpret_cast<V128*>(op + 48), pattern);\n      }\n      return op_limit;\n    }\n    char* const op_end = buf_limit - 15;\n    if (SNAPPY_PREDICT_TRUE(op < op_end)) {\n      auto pattern_and_reshuffle_mask =\n          LoadPatternAndReshuffleMask(src, pattern_size);\n      V128 pattern = pattern_and_reshuffle_mask.first;\n      V128 reshuffle_mask = pattern_and_reshuffle_mask.second;\n\n      // This code path is relatively cold however so we save code size\n      // by avoiding unrolling and vectorizing.\n      //\n      // TODO: Remove pragma when when cold regions don't get\n      // vectorized or unrolled.\n#ifdef __clang__\n#pragma clang loop unroll(disable)\n#endif\n      do {\n        V128_StoreU(reinterpret_cast<V128*>(op), pattern);\n        pattern = V128_Shuffle(pattern, reshuffle_mask);\n        op += 16;\n      } while (SNAPPY_PREDICT_TRUE(op < op_end));\n    }\n    return IncrementalCopySlow(op - pattern_size, op, op_limit);\n#else   // !SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n    // If plenty of buffer space remains, expand the pattern to at least 8\n    // bytes. The way the following loop is written, we need 8 bytes of buffer\n    // space if pattern_size >= 4, 11 bytes if pattern_size is 1 or 3, and 10\n    // bytes if pattern_size is 2.  Precisely encoding that is probably not\n    // worthwhile; instead, invoke the slow path if we cannot write 11 bytes\n    // (because 11 are required in the worst case).\n    if (SNAPPY_PREDICT_TRUE(op <= buf_limit - 11)) {\n      while (pattern_size < 8) {\n        UnalignedCopy64(src, op);\n        op += pattern_size;\n        pattern_size *= 2;\n      }\n      if (SNAPPY_PREDICT_TRUE(op >= op_limit)) return op_limit;\n    } else {\n      return IncrementalCopySlow(src, op, op_limit);\n    }\n#endif  // SNAPPY_HAVE_VECTOR_BYTE_SHUFFLE\n  }\n  assert(pattern_size >= big_pattern_size_lower_bound);\n  constexpr bool use_16bytes_chunk = big_pattern_size_lower_bound == 16;\n\n  // Copy 1x 16 bytes (or 2x 8 bytes in non-SSE) at a time. Because op - src can\n  // be < 16 in non-SSE, a single UnalignedCopy128 might overwrite data in op.\n  // UnalignedCopy64 is safe because expanding the pattern to at least 8 bytes\n  // guarantees that op - src >= 8.\n  //\n  // Typically, the op_limit is the gating factor so try to simplify the loop\n  // based on that.\n  if (SNAPPY_PREDICT_TRUE(op_limit <= buf_limit - 15)) {\n    // There is at least one, and at most four 16-byte blocks. Writing four\n    // conditionals instead of a loop allows FDO to layout the code with respect\n    // to the actual probabilities of each length.\n    // TODO: Replace with loop with trip count hint.\n    ConditionalUnalignedCopy128<use_16bytes_chunk>(src, op);\n    if (op + 16 < op_limit) {\n      ConditionalUnalignedCopy128<use_16bytes_chunk>(src + 16, op + 16);\n    }\n    if (op + 32 < op_limit) {\n      ConditionalUnalignedCopy128<use_16bytes_chunk>(src + 32, op + 32);\n    }\n    if (op + 48 < op_limit) {\n      ConditionalUnalignedCopy128<use_16bytes_chunk>(src + 48, op + 48);\n    }\n    return op_limit;\n  }\n\n  // Fall back to doing as much as we can with the available slop in the\n  // buffer. This code path is relatively cold however so we save code size by\n  // avoiding unrolling and vectorizing.\n  //\n  // TODO: Remove pragma when when cold regions don't get vectorized\n  // or unrolled.\n#ifdef __clang__\n#pragma clang loop unroll(disable)\n#endif\n  for (char* op_end = buf_limit - 16; op < op_end; op += 16, src += 16) {\n    ConditionalUnalignedCopy128<use_16bytes_chunk>(src, op);\n  }\n  if (op >= op_limit) return op_limit;\n\n  // We only take this branch if we didn't have enough slop and we can do a\n  // single 8 byte copy.\n  if (SNAPPY_PREDICT_FALSE(op <= buf_limit - 8)) {\n    UnalignedCopy64(src, op);\n    src += 8;\n    op += 8;\n  }\n  return IncrementalCopySlow(src, op, op_limit);\n}\n\n}  // namespace\n\ntemplate <bool allow_fast_path>\nstatic inline char* EmitLiteral(char* op, const char* literal, int len) {\n  // The vast majority of copies are below 16 bytes, for which a\n  // call to std::memcpy() is overkill. This fast path can sometimes\n  // copy up to 15 bytes too much, but that is okay in the\n  // main loop, since we have a bit to go on for both sides:\n  //\n  //   - The input will always have kInputMarginBytes = 15 extra\n  //     available bytes, as long as we're in the main loop, and\n  //     if not, allow_fast_path = false.\n  //   - The output will always have 32 spare bytes (see\n  //     MaxCompressedLength).\n  assert(len > 0);  // Zero-length literals are disallowed\n  int n = len - 1;\n  if (allow_fast_path && len <= 16) {\n    // Fits in tag byte\n    *op++ = LITERAL | (n << 2);\n\n    UnalignedCopy128(literal, op);\n    return op + len;\n  }\n\n  if (n < 60) {\n    // Fits in tag byte\n    *op++ = LITERAL | (n << 2);\n  } else {\n    int count = (Bits::Log2Floor(n) >> 3) + 1;\n    assert(count >= 1);\n    assert(count <= 4);\n    *op++ = LITERAL | ((59 + count) << 2);\n    // Encode in upcoming bytes.\n    // Write 4 bytes, though we may care about only 1 of them. The output buffer\n    // is guaranteed to have at least 3 more spaces left as 'len >= 61' holds\n    // here and there is a std::memcpy() of size 'len' below.\n    LittleEndian::Store32(op, n);\n    op += count;\n  }\n  // When allow_fast_path is true, we can overwrite up to 16 bytes.\n  if (allow_fast_path) {\n    char* destination = op;\n    const char* source = literal;\n    const char* end = destination + len;\n    do {\n      std::memcpy(destination, source, 16);\n      destination += 16;\n      source += 16;\n    } while (destination < end);\n  } else {\n    std::memcpy(op, literal, len);\n  }\n  return op + len;\n}\n\ntemplate <bool len_less_than_12>\nstatic inline char* EmitCopyAtMost64(char* op, size_t offset, size_t len) {\n  assert(len <= 64);\n  assert(len >= 4);\n  assert(offset < 65536);\n  assert(len_less_than_12 == (len < 12));\n\n  if (len_less_than_12) {\n    uint32_t u = (len << 2) + (offset << 8);\n    uint32_t copy1 = COPY_1_BYTE_OFFSET - (4 << 2) + ((offset >> 3) & 0xe0);\n    uint32_t copy2 = COPY_2_BYTE_OFFSET - (1 << 2);\n    // It turns out that offset < 2048 is a difficult to predict branch.\n    // `perf record` shows this is the highest percentage of branch misses in\n    // benchmarks. This code produces branch free code, the data dependency\n    // chain that bottlenecks the throughput is so long that a few extra\n    // instructions are completely free (IPC << 6 because of data deps).\n    u += offset < 2048 ? copy1 : copy2;\n    LittleEndian::Store32(op, u);\n    op += offset < 2048 ? 2 : 3;\n  } else {\n    // Write 4 bytes, though we only care about 3 of them.  The output buffer\n    // is required to have some slack, so the extra byte won't overrun it.\n    uint32_t u = COPY_2_BYTE_OFFSET + ((len - 1) << 2) + (offset << 8);\n    LittleEndian::Store32(op, u);\n    op += 3;\n  }\n  return op;\n}\n\ntemplate <bool len_less_than_12>\nstatic inline char* EmitCopy(char* op, size_t offset, size_t len) {\n  assert(len_less_than_12 == (len < 12));\n  if (len_less_than_12) {\n    return EmitCopyAtMost64</*len_less_than_12=*/true>(op, offset, len);\n  } else {\n    // A special case for len <= 64 might help, but so far measurements suggest\n    // it's in the noise.\n\n    // Emit 64 byte copies but make sure to keep at least four bytes reserved.\n    while (SNAPPY_PREDICT_FALSE(len >= 68)) {\n      op = EmitCopyAtMost64</*len_less_than_12=*/false>(op, offset, 64);\n      len -= 64;\n    }\n\n    // One or two copies will now finish the job.\n    if (len > 64) {\n      op = EmitCopyAtMost64</*len_less_than_12=*/false>(op, offset, 60);\n      len -= 60;\n    }\n\n    // Emit remainder.\n    if (len < 12) {\n      op = EmitCopyAtMost64</*len_less_than_12=*/true>(op, offset, len);\n    } else {\n      op = EmitCopyAtMost64</*len_less_than_12=*/false>(op, offset, len);\n    }\n    return op;\n  }\n}\n\nbool GetUncompressedLength(const char* start, size_t n, size_t* result) {\n  uint32_t v = 0;\n  const char* limit = start + n;\n  if (Varint::Parse32WithLimit(start, limit, &v) != NULL) {\n    *result = v;\n    return true;\n  } else {\n    return false;\n  }\n}\n\nnamespace {\nuint32_t CalculateTableSize(uint32_t input_size) {\n  static_assert(\n      kMaxHashTableSize >= kMinHashTableSize,\n      \"kMaxHashTableSize should be greater or equal to kMinHashTableSize.\");\n  if (input_size > kMaxHashTableSize) {\n    return kMaxHashTableSize;\n  }\n  if (input_size < kMinHashTableSize) {\n    return kMinHashTableSize;\n  }\n  // This is equivalent to Log2Ceiling(input_size), assuming input_size > 1.\n  // 2 << Log2Floor(x - 1) is equivalent to 1 << (1 + Log2Floor(x - 1)).\n  return 2u << Bits::Log2Floor(input_size - 1);\n}\n}  // namespace\n\nnamespace internal {\nWorkingMemory::WorkingMemory(size_t input_size) {\n  const size_t max_fragment_size = std::min(input_size, kBlockSize);\n  const size_t table_size = CalculateTableSize(max_fragment_size);\n  size_ = table_size * sizeof(*table_) + max_fragment_size +\n          MaxCompressedLength(max_fragment_size);\n  mem_ = std::allocator<char>().allocate(size_);\n  table_ = reinterpret_cast<uint16_t*>(mem_);\n  input_ = mem_ + table_size * sizeof(*table_);\n  output_ = input_ + max_fragment_size;\n}\n\nWorkingMemory::~WorkingMemory() {\n  std::allocator<char>().deallocate(mem_, size_);\n}\n\nuint16_t* WorkingMemory::GetHashTable(size_t fragment_size,\n                                      int* table_size) const {\n  const size_t htsize = CalculateTableSize(fragment_size);\n  memset(table_, 0, htsize * sizeof(*table_));\n  *table_size = htsize;\n  return table_;\n}\n}  // end namespace internal\n\n// Flat array compression that does not emit the \"uncompressed length\"\n// prefix. Compresses \"input\" string to the \"*op\" buffer.\n//\n// REQUIRES: \"input\" is at most \"kBlockSize\" bytes long.\n// REQUIRES: \"op\" points to an array of memory that is at least\n// \"MaxCompressedLength(input.size())\" in size.\n// REQUIRES: All elements in \"table[0..table_size-1]\" are initialized to zero.\n// REQUIRES: \"table_size\" is a power of two\n//\n// Returns an \"end\" pointer into \"op\" buffer.\n// \"end - op\" is the compressed size of \"input\".\nnamespace internal {\nchar* CompressFragment(const char* input, size_t input_size, char* op,\n                       uint16_t* table, const int table_size) {\n  // \"ip\" is the input pointer, and \"op\" is the output pointer.\n  const char* ip = input;\n  assert(input_size <= kBlockSize);\n  assert((table_size & (table_size - 1)) == 0);  // table must be power of two\n  const uint32_t mask = 2 * (table_size - 1);\n  const char* ip_end = input + input_size;\n  const char* base_ip = ip;\n\n  const size_t kInputMarginBytes = 15;\n  if (SNAPPY_PREDICT_TRUE(input_size >= kInputMarginBytes)) {\n    const char* ip_limit = input + input_size - kInputMarginBytes;\n\n    for (uint32_t preload = LittleEndian::Load32(ip + 1);;) {\n      // Bytes in [next_emit, ip) will be emitted as literal bytes.  Or\n      // [next_emit, ip_end) after the main loop.\n      const char* next_emit = ip++;\n      uint64_t data = LittleEndian::Load64(ip);\n      // The body of this loop calls EmitLiteral once and then EmitCopy one or\n      // more times.  (The exception is that when we're close to exhausting\n      // the input we goto emit_remainder.)\n      //\n      // In the first iteration of this loop we're just starting, so\n      // there's nothing to copy, so calling EmitLiteral once is\n      // necessary.  And we only start a new iteration when the\n      // current iteration has determined that a call to EmitLiteral will\n      // precede the next call to EmitCopy (if any).\n      //\n      // Step 1: Scan forward in the input looking for a 4-byte-long match.\n      // If we get close to exhausting the input then goto emit_remainder.\n      //\n      // Heuristic match skipping: If 32 bytes are scanned with no matches\n      // found, start looking only at every other byte. If 32 more bytes are\n      // scanned (or skipped), look at every third byte, etc.. When a match is\n      // found, immediately go back to looking at every byte. This is a small\n      // loss (~5% performance, ~0.1% density) for compressible data due to more\n      // bookkeeping, but for non-compressible data (such as JPEG) it's a huge\n      // win since the compressor quickly \"realizes\" the data is incompressible\n      // and doesn't bother looking for matches everywhere.\n      //\n      // The \"skip\" variable keeps track of how many bytes there are since the\n      // last match; dividing it by 32 (ie. right-shifting by five) gives the\n      // number of bytes to move ahead for each iteration.\n      uint32_t skip = 32;\n\n      const char* candidate;\n      if (ip_limit - ip >= 16) {\n        auto delta = ip - base_ip;\n        for (int j = 0; j < 4; ++j) {\n          for (int k = 0; k < 4; ++k) {\n            int i = 4 * j + k;\n            // These for-loops are meant to be unrolled. So we can freely\n            // special case the first iteration to use the value already\n            // loaded in preload.\n            uint32_t dword = i == 0 ? preload : static_cast<uint32_t>(data);\n            assert(dword == LittleEndian::Load32(ip + i));\n            uint16_t* table_entry = TableEntry(table, dword, mask);\n            candidate = base_ip + *table_entry;\n            assert(candidate >= base_ip);\n            assert(candidate < ip + i);\n            *table_entry = delta + i;\n            if (SNAPPY_PREDICT_FALSE(LittleEndian::Load32(candidate) == dword)) {\n              *op = LITERAL | (i << 2);\n              UnalignedCopy128(next_emit, op + 1);\n              ip += i;\n              op = op + i + 2;\n              goto emit_match;\n            }\n            data >>= 8;\n          }\n          data = LittleEndian::Load64(ip + 4 * j + 4);\n        }\n        ip += 16;\n        skip += 16;\n      }\n      while (true) {\n        assert(static_cast<uint32_t>(data) == LittleEndian::Load32(ip));\n        uint16_t* table_entry = TableEntry(table, data, mask);\n        uint32_t bytes_between_hash_lookups = skip >> 5;\n        skip += bytes_between_hash_lookups;\n        const char* next_ip = ip + bytes_between_hash_lookups;\n        if (SNAPPY_PREDICT_FALSE(next_ip > ip_limit)) {\n          ip = next_emit;\n          goto emit_remainder;\n        }\n        candidate = base_ip + *table_entry;\n        assert(candidate >= base_ip);\n        assert(candidate < ip);\n\n        *table_entry = ip - base_ip;\n        if (SNAPPY_PREDICT_FALSE(static_cast<uint32_t>(data) ==\n                                LittleEndian::Load32(candidate))) {\n          break;\n        }\n        data = LittleEndian::Load32(next_ip);\n        ip = next_ip;\n      }\n\n      // Step 2: A 4-byte match has been found.  We'll later see if more\n      // than 4 bytes match.  But, prior to the match, input\n      // bytes [next_emit, ip) are unmatched.  Emit them as \"literal bytes.\"\n      assert(next_emit + 16 <= ip_end);\n      op = EmitLiteral</*allow_fast_path=*/true>(op, next_emit, ip - next_emit);\n\n      // Step 3: Call EmitCopy, and then see if another EmitCopy could\n      // be our next move.  Repeat until we find no match for the\n      // input immediately after what was consumed by the last EmitCopy call.\n      //\n      // If we exit this loop normally then we need to call EmitLiteral next,\n      // though we don't yet know how big the literal will be.  We handle that\n      // by proceeding to the next iteration of the main loop.  We also can exit\n      // this loop via goto if we get close to exhausting the input.\n    emit_match:\n      do {\n        // We have a 4-byte match at ip, and no need to emit any\n        // \"literal bytes\" prior to ip.\n        const char* base = ip;\n        std::pair<size_t, bool> p =\n            FindMatchLength(candidate + 4, ip + 4, ip_end, &data);\n        size_t matched = 4 + p.first;\n        ip += matched;\n        size_t offset = base - candidate;\n        assert(0 == memcmp(base, candidate, matched));\n        if (p.second) {\n          op = EmitCopy</*len_less_than_12=*/true>(op, offset, matched);\n        } else {\n          op = EmitCopy</*len_less_than_12=*/false>(op, offset, matched);\n        }\n        if (SNAPPY_PREDICT_FALSE(ip >= ip_limit)) {\n          goto emit_remainder;\n        }\n        // Expect 5 bytes to match\n        assert((data & 0xFFFFFFFFFF) ==\n               (LittleEndian::Load64(ip) & 0xFFFFFFFFFF));\n        // We are now looking for a 4-byte match again.  We read\n        // table[Hash(ip, mask)] for that.  To improve compression,\n        // we also update table[Hash(ip - 1, mask)] and table[Hash(ip, mask)].\n        *TableEntry(table, LittleEndian::Load32(ip - 1), mask) =\n            ip - base_ip - 1;\n        uint16_t* table_entry = TableEntry(table, data, mask);\n        candidate = base_ip + *table_entry;\n        *table_entry = ip - base_ip;\n        // Measurements on the benchmarks have shown the following probabilities\n        // for the loop to exit (ie. avg. number of iterations is reciprocal).\n        // BM_Flat/6  txt1    p = 0.3-0.4\n        // BM_Flat/7  txt2    p = 0.35\n        // BM_Flat/8  txt3    p = 0.3-0.4\n        // BM_Flat/9  txt3    p = 0.34-0.4\n        // BM_Flat/10 pb      p = 0.4\n        // BM_Flat/11 gaviota p = 0.1\n        // BM_Flat/12 cp      p = 0.5\n        // BM_Flat/13 c       p = 0.3\n      } while (static_cast<uint32_t>(data) == LittleEndian::Load32(candidate));\n      // Because the least significant 5 bytes matched, we can utilize data\n      // for the next iteration.\n      preload = data >> 8;\n    }\n  }\n\nemit_remainder:\n  // Emit the remaining bytes as a literal\n  if (ip < ip_end) {\n    op = EmitLiteral</*allow_fast_path=*/false>(op, ip, ip_end - ip);\n  }\n\n  return op;\n}\n\nchar* CompressFragmentDoubleHash(const char* input, size_t input_size, char* op,\n                                 uint16_t* table, const int table_size,\n                                 uint16_t* table2, const int table_size2) {\n  (void)table_size2;\n  assert(table_size == table_size2);\n  // \"ip\" is the input pointer, and \"op\" is the output pointer.\n  const char* ip = input;\n  assert(input_size <= kBlockSize);\n  assert((table_size & (table_size - 1)) == 0);  // table must be power of two\n  const uint32_t mask = 2 * (table_size - 1);\n  const char* ip_end = input + input_size;\n  const char* base_ip = ip;\n\n  const size_t kInputMarginBytes = 15;\n  if (SNAPPY_PREDICT_TRUE(input_size >= kInputMarginBytes)) {\n    const char* ip_limit = input + input_size - kInputMarginBytes;\n\n    for (;;) {\n      const char* next_emit = ip++;\n      uint64_t data = LittleEndian::Load64(ip);\n      uint32_t skip = 512;\n\n      const char* candidate;\n      uint32_t candidate_length;\n      while (true) {\n        assert(static_cast<uint32_t>(data) == LittleEndian::Load32(ip));\n        uint16_t* table_entry2 = TableEntry8ByteMatch(table2, data, mask);\n        uint32_t bytes_between_hash_lookups = skip >> 9;\n        skip++;\n        const char* next_ip = ip + bytes_between_hash_lookups;\n        if (SNAPPY_PREDICT_FALSE(next_ip > ip_limit)) {\n          ip = next_emit;\n          goto emit_remainder;\n        }\n        candidate = base_ip + *table_entry2;\n        assert(candidate >= base_ip);\n        assert(candidate < ip);\n\n        *table_entry2 = ip - base_ip;\n        if (SNAPPY_PREDICT_FALSE(static_cast<uint32_t>(data) ==\n                                LittleEndian::Load32(candidate))) {\n          candidate_length =\n              FindMatchLengthPlain(candidate + 4, ip + 4, ip_end) + 4;\n          break;\n        }\n\n        uint16_t* table_entry = TableEntry4ByteMatch(table, data, mask);\n        candidate = base_ip + *table_entry;\n        assert(candidate >= base_ip);\n        assert(candidate < ip);\n\n        *table_entry = ip - base_ip;\n        if (SNAPPY_PREDICT_FALSE(static_cast<uint32_t>(data) ==\n                                LittleEndian::Load32(candidate))) {\n          candidate_length =\n              FindMatchLengthPlain(candidate + 4, ip + 4, ip_end) + 4;\n          table_entry2 =\n              TableEntry8ByteMatch(table2, LittleEndian::Load64(ip + 1), mask);\n          auto candidate2 = base_ip + *table_entry2;\n          size_t candidate_length2 =\n              FindMatchLengthPlain(candidate2, ip + 1, ip_end);\n          if (candidate_length2 > candidate_length) {\n            *table_entry2 = ip - base_ip;\n            candidate = candidate2;\n            candidate_length = candidate_length2;\n            ++ip;\n          }\n          break;\n        }\n        data = LittleEndian::Load64(next_ip);\n        ip = next_ip;\n      }\n      // Backtrack to the point it matches fully.\n      while (ip > next_emit && candidate > base_ip &&\n             *(ip - 1) == *(candidate - 1)) {\n        --ip;\n        --candidate;\n        ++candidate_length;\n      }\n      *TableEntry8ByteMatch(table2, LittleEndian::Load64(ip + 1), mask) =\n          ip - base_ip + 1;\n      *TableEntry8ByteMatch(table2, LittleEndian::Load64(ip + 2), mask) =\n          ip - base_ip + 2;\n      *TableEntry4ByteMatch(table, LittleEndian::Load32(ip + 1), mask) =\n          ip - base_ip + 1;\n      // Step 2: A 4-byte or 8-byte match has been found.\n      // We'll later see if more than 4 bytes match.  But, prior to the match,\n      // input bytes [next_emit, ip) are unmatched.  Emit them as\n      // \"literal bytes.\"\n      assert(next_emit + 16 <= ip_end);\n      if (ip - next_emit > 0) {\n        op = EmitLiteral</*allow_fast_path=*/true>(op, next_emit,\n                                                   ip - next_emit);\n      }\n      // Step 3: Call EmitCopy, and then see if another EmitCopy could\n      // be our next move.  Repeat until we find no match for the\n      // input immediately after what was consumed by the last EmitCopy call.\n      //\n      // If we exit this loop normally then we need to call EmitLiteral next,\n      // though we don't yet know how big the literal will be.  We handle that\n      // by proceeding to the next iteration of the main loop.  We also can exit\n      // this loop via goto if we get close to exhausting the input.\n      do {\n        // We have a 4-byte match at ip, and no need to emit any\n        // \"literal bytes\" prior to ip.\n        const char* base = ip;\n        ip += candidate_length;\n        size_t offset = base - candidate;\n        if (candidate_length < 12) {\n          op =\n              EmitCopy</*len_less_than_12=*/true>(op, offset, candidate_length);\n        } else {\n          op = EmitCopy</*len_less_than_12=*/false>(op, offset,\n                                                    candidate_length);\n        }\n        if (SNAPPY_PREDICT_FALSE(ip >= ip_limit)) {\n          goto emit_remainder;\n        }\n        // We are now looking for a 4-byte match again.  We read\n        // table[Hash(ip, mask)] for that. To improve compression,\n        // we also update several previous table entries.\n        if (ip - base_ip > 7) {\n          *TableEntry8ByteMatch(table2, LittleEndian::Load64(ip - 7), mask) =\n              ip - base_ip - 7;\n          *TableEntry8ByteMatch(table2, LittleEndian::Load64(ip - 4), mask) =\n              ip - base_ip - 4;\n        }\n        *TableEntry8ByteMatch(table2, LittleEndian::Load64(ip - 3), mask) =\n            ip - base_ip - 3;\n        *TableEntry8ByteMatch(table2, LittleEndian::Load64(ip - 2), mask) =\n            ip - base_ip - 2;\n        *TableEntry4ByteMatch(table, LittleEndian::Load32(ip - 2), mask) =\n            ip - base_ip - 2;\n        *TableEntry4ByteMatch(table, LittleEndian::Load32(ip - 1), mask) =\n            ip - base_ip - 1;\n\n        uint16_t* table_entry =\n            TableEntry8ByteMatch(table2, LittleEndian::Load64(ip), mask);\n        candidate = base_ip + *table_entry;\n        *table_entry = ip - base_ip;\n        if (LittleEndian::Load32(ip) == LittleEndian::Load32(candidate)) {\n          candidate_length =\n              FindMatchLengthPlain(candidate + 4, ip + 4, ip_end) + 4;\n          continue;\n        }\n        table_entry =\n            TableEntry4ByteMatch(table, LittleEndian::Load32(ip), mask);\n        candidate = base_ip + *table_entry;\n        *table_entry = ip - base_ip;\n        if (LittleEndian::Load32(ip) == LittleEndian::Load32(candidate)) {\n          candidate_length =\n              FindMatchLengthPlain(candidate + 4, ip + 4, ip_end) + 4;\n          continue;\n        }\n        break;\n      } while (true);\n    }\n  }\n\nemit_remainder:\n  // Emit the remaining bytes as a literal\n  if (ip < ip_end) {\n    op = EmitLiteral</*allow_fast_path=*/false>(op, ip, ip_end - ip);\n  }\n\n  return op;\n}\n}  // end namespace internal\n\nstatic inline void Report(int token, const char *algorithm, size_t\ncompressed_size, size_t uncompressed_size) {\n  // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n  (void)token;\n  (void)algorithm;\n  (void)compressed_size;\n  (void)uncompressed_size;\n}\n\n// Signature of output types needed by decompression code.\n// The decompression code is templatized on a type that obeys this\n// signature so that we do not pay virtual function call overhead in\n// the middle of a tight decompression loop.\n//\n// class DecompressionWriter {\n//  public:\n//   // Called before decompression\n//   void SetExpectedLength(size_t length);\n//\n//   // For performance a writer may choose to donate the cursor variable to the\n//   // decompression function. The decompression will inject it in all its\n//   // function calls to the writer. Keeping the important output cursor as a\n//   // function local stack variable allows the compiler to keep it in\n//   // register, which greatly aids performance by avoiding loads and stores of\n//   // this variable in the fast path loop iterations.\n//   T GetOutputPtr() const;\n//\n//   // At end of decompression the loop donates the ownership of the cursor\n//   // variable back to the writer by calling this function.\n//   void SetOutputPtr(T op);\n//\n//   // Called after decompression\n//   bool CheckLength() const;\n//\n//   // Called repeatedly during decompression\n//   // Each function get a pointer to the op (output pointer), that the writer\n//   // can use and update. Note it's important that these functions get fully\n//   // inlined so that no actual address of the local variable needs to be\n//   // taken.\n//   bool Append(const char* ip, size_t length, T* op);\n//   bool AppendFromSelf(uint32_t offset, size_t length, T* op);\n//\n//   // The rules for how TryFastAppend differs from Append are somewhat\n//   // convoluted:\n//   //\n//   //  - TryFastAppend is allowed to decline (return false) at any\n//   //    time, for any reason -- just \"return false\" would be\n//   //    a perfectly legal implementation of TryFastAppend.\n//   //    The intention is for TryFastAppend to allow a fast path\n//   //    in the common case of a small append.\n//   //  - TryFastAppend is allowed to read up to <available> bytes\n//   //    from the input buffer, whereas Append is allowed to read\n//   //    <length>. However, if it returns true, it must leave\n//   //    at least five (kMaximumTagLength) bytes in the input buffer\n//   //    afterwards, so that there is always enough space to read the\n//   //    next tag without checking for a refill.\n//   //  - TryFastAppend must always return decline (return false)\n//   //    if <length> is 61 or more, as in this case the literal length is not\n//   //    decoded fully. In practice, this should not be a big problem,\n//   //    as it is unlikely that one would implement a fast path accepting\n//   //    this much data.\n//   //\n//   bool TryFastAppend(const char* ip, size_t available, size_t length, T* op);\n// };\n\nstatic inline uint32_t ExtractLowBytes(const uint32_t& v, int n) {\n  assert(n >= 0);\n  assert(n <= 4);\n#if SNAPPY_HAVE_BMI2\n  return _bzhi_u32(v, 8 * n);\n#else\n  // This needs to be wider than uint32_t otherwise `mask << 32` will be\n  // undefined.\n  uint64_t mask = 0xffffffff;\n  return v & ~(mask << (8 * n));\n#endif\n}\n\nstatic inline bool LeftShiftOverflows(uint8_t value, uint32_t shift) {\n  assert(shift < 32);\n  static const uint8_t masks[] = {\n      0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,  //\n      0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,  //\n      0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00,  //\n      0x00, 0x80, 0xc0, 0xe0, 0xf0, 0xf8, 0xfc, 0xfe};\n  return (value & masks[shift]) != 0;\n}\n\ninline bool Copy64BytesWithPatternExtension(ptrdiff_t dst, size_t offset) {\n  // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n  (void)dst;\n  return offset != 0;\n}\n\n// Copies between size bytes and 64 bytes from src to dest.  size cannot exceed\n// 64.  More than size bytes, but never exceeding 64, might be copied if doing\n// so gives better performance.  [src, src + size) must not overlap with\n// [dst, dst + size), but [src, src + 64) may overlap with [dst, dst + 64).\nvoid MemCopy64(char* dst, const void* src, size_t size) {\n  // Always copy this many bytes.  If that's below size then copy the full 64.\n  constexpr int kShortMemCopy = 32;\n\n  assert(size <= 64);\n  assert(std::less_equal<const void*>()(static_cast<const char*>(src) + size,\n                                        dst) ||\n         std::less_equal<const void*>()(dst + size, src));\n\n  // We know that src and dst are at least size bytes apart. However, because we\n  // might copy more than size bytes the copy still might overlap past size.\n  // E.g. if src and dst appear consecutively in memory (src + size >= dst).\n  // TODO: Investigate wider copies on other platforms.\n#if defined(__x86_64__) && defined(__AVX__)\n  assert(kShortMemCopy <= 32);\n  __m256i data = _mm256_lddqu_si256(static_cast<const __m256i *>(src));\n  _mm256_storeu_si256(reinterpret_cast<__m256i *>(dst), data);\n  // Profiling shows that nearly all copies are short.\n  if (SNAPPY_PREDICT_FALSE(size > kShortMemCopy)) {\n    data = _mm256_lddqu_si256(static_cast<const __m256i *>(src) + 1);\n    _mm256_storeu_si256(reinterpret_cast<__m256i *>(dst) + 1, data);\n  }\n#else\n  std::memmove(dst, src, kShortMemCopy);\n  // Profiling shows that nearly all copies are short.\n  if (SNAPPY_PREDICT_FALSE(size > kShortMemCopy)) {\n    std::memmove(dst + kShortMemCopy,\n                 static_cast<const uint8_t*>(src) + kShortMemCopy,\n                 64 - kShortMemCopy);\n  }\n#endif\n}\n\nvoid MemCopy64(ptrdiff_t dst, const void* src, size_t size) {\n  // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n  (void)dst;\n  (void)src;\n  (void)size;\n}\n\nvoid ClearDeferred(const void** deferred_src, size_t* deferred_length,\n                   uint8_t* safe_source) {\n  *deferred_src = safe_source;\n  *deferred_length = 0;\n}\n\nvoid DeferMemCopy(const void** deferred_src, size_t* deferred_length,\n                  const void* src, size_t length) {\n  *deferred_src = src;\n  *deferred_length = length;\n}\n\nSNAPPY_ATTRIBUTE_ALWAYS_INLINE\ninline size_t AdvanceToNextTagARMOptimized(const uint8_t** ip_p, size_t* tag) {\n  const uint8_t*& ip = *ip_p;\n  // This section is crucial for the throughput of the decompression loop.\n  // The latency of an iteration is fundamentally constrained by the\n  // following data chain on ip.\n  // ip -> c = Load(ip) -> delta1 = (c & 3)        -> ip += delta1 or delta2\n  //                       delta2 = ((c >> 2) + 1)    ip++\n  // This is different from X86 optimizations because ARM has conditional add\n  // instruction (csinc) and it removes several register moves.\n  const size_t tag_type = *tag & 3;\n  const bool is_literal = (tag_type == 0);\n  if (is_literal) {\n    size_t next_literal_tag = (*tag >> 2) + 1;\n    *tag = ip[next_literal_tag];\n    ip += next_literal_tag + 1;\n  } else {\n    *tag = ip[tag_type];\n    ip += tag_type + 1;\n  }\n  return tag_type;\n}\n\nSNAPPY_ATTRIBUTE_ALWAYS_INLINE\ninline size_t AdvanceToNextTagX86Optimized(const uint8_t** ip_p, size_t* tag) {\n  const uint8_t*& ip = *ip_p;\n  // This section is crucial for the throughput of the decompression loop.\n  // The latency of an iteration is fundamentally constrained by the\n  // following data chain on ip.\n  // ip -> c = Load(ip) -> ip1 = ip + 1 + (c & 3) -> ip = ip1 or ip2\n  //                       ip2 = ip + 2 + (c >> 2)\n  // This amounts to 8 cycles.\n  // 5 (load) + 1 (c & 3) + 1 (lea ip1, [ip + (c & 3) + 1]) + 1 (cmov)\n  size_t literal_len = *tag >> 2;\n  size_t tag_type = *tag;\n  bool is_literal;\n#if defined(__GCC_ASM_FLAG_OUTPUTS__) && defined(__x86_64__)\n  // TODO clang misses the fact that the (c & 3) already correctly\n  // sets the zero flag.\n  asm(\"and $3, %k[tag_type]\\n\\t\"\n      : [tag_type] \"+r\"(tag_type), \"=@ccz\"(is_literal)\n      :: \"cc\");\n#else\n  tag_type &= 3;\n  is_literal = (tag_type == 0);\n#endif\n  // TODO\n  // This is code is subtle. Loading the values first and then cmov has less\n  // latency then cmov ip and then load. However clang would move the loads\n  // in an optimization phase, volatile prevents this transformation.\n  // Note that we have enough slop bytes (64) that the loads are always valid.\n  size_t tag_literal =\n      static_cast<const volatile uint8_t*>(ip)[1 + literal_len];\n  size_t tag_copy = static_cast<const volatile uint8_t*>(ip)[tag_type];\n  *tag = is_literal ? tag_literal : tag_copy;\n  const uint8_t* ip_copy = ip + 1 + tag_type;\n  const uint8_t* ip_literal = ip + 2 + literal_len;\n  ip = is_literal ? ip_literal : ip_copy;\n#if defined(__GNUC__) && defined(__x86_64__)\n  // TODO Clang is \"optimizing\" zero-extension (a totally free\n  // operation) this means that after the cmov of tag, it emits another movzb\n  // tag, byte(tag). It really matters as it's on the core chain. This dummy\n  // asm, persuades clang to do the zero-extension at the load (it's automatic)\n  // removing the expensive movzb.\n  asm(\"\" ::\"r\"(tag_copy));\n#endif\n  return tag_type;\n}\n\n// Extract the offset for copy-1 and copy-2 returns 0 for literals or copy-4.\ninline uint32_t ExtractOffset(uint32_t val, size_t tag_type) {\n  // For x86 non-static storage works better. For ARM static storage is better.\n  // TODO: Once the array is recognized as a register, improve the\n  // readability for x86.\n#if defined(__x86_64__)\n  constexpr uint64_t kExtractMasksCombined = 0x0000FFFF00FF0000ull;\n  uint16_t result;\n  memcpy(&result,\n         reinterpret_cast<const char*>(&kExtractMasksCombined) + 2 * tag_type,\n         sizeof(result));\n  return val & result;\n#elif defined(__aarch64__)\n  constexpr uint64_t kExtractMasksCombined = 0x0000FFFF00FF0000ull;\n  return val & static_cast<uint32_t>(\n      (kExtractMasksCombined >> (tag_type * 16)) & 0xFFFF);\n#else\n  static constexpr uint32_t kExtractMasks[4] = {0, 0xFF, 0xFFFF, 0};\n  return val & kExtractMasks[tag_type];\n#endif\n};\n\n// Core decompression loop, when there is enough data available.\n// Decompresses the input buffer [ip, ip_limit) into the output buffer\n// [op, op_limit_min_slop). Returning when either we are too close to the end\n// of the input buffer, or we exceed op_limit_min_slop or when a exceptional\n// tag is encountered (literal of length > 60) or a copy-4.\n// Returns {ip, op} at the points it stopped decoding.\n// TODO This function probably does not need to be inlined, as it\n// should decode large chunks at a time. This allows runtime dispatch to\n// implementations based on CPU capability (BMI2 / perhaps 32 / 64 byte memcpy).\ntemplate <typename T>\nstd::pair<const uint8_t*, ptrdiff_t> DecompressBranchless(\n    const uint8_t* ip, const uint8_t* ip_limit, ptrdiff_t op, T op_base,\n    ptrdiff_t op_limit_min_slop) {\n  // If deferred_src is invalid point it here.\n  uint8_t safe_source[64];\n  const void* deferred_src;\n  size_t deferred_length;\n  ClearDeferred(&deferred_src, &deferred_length, safe_source);\n\n  // We unroll the inner loop twice so we need twice the spare room.\n  op_limit_min_slop -= kSlopBytes;\n  if (2 * (kSlopBytes + 1) < ip_limit - ip && op < op_limit_min_slop) {\n    const uint8_t* const ip_limit_min_slop = ip_limit - 2 * kSlopBytes - 1;\n    ip++;\n    // ip points just past the tag and we are touching at maximum kSlopBytes\n    // in an iteration.\n    size_t tag = ip[-1];\n#if defined(__clang__) && defined(__aarch64__)\n    // Workaround for https://bugs.llvm.org/show_bug.cgi?id=51317\n    // when loading 1 byte, clang for aarch64 doesn't realize that it(ldrb)\n    // comes with free zero-extension, so clang generates another\n    // 'and xn, xm, 0xff' before it use that as the offset. This 'and' is\n    // redundant and can be removed by adding this dummy asm, which gives\n    // clang a hint that we're doing the zero-extension at the load.\n    asm(\"\" ::\"r\"(tag));\n#endif\n    do {\n      // The throughput is limited by instructions, unrolling the inner loop\n      // twice reduces the amount of instructions checking limits and also\n      // leads to reduced mov's.\n\n      SNAPPY_PREFETCH(ip + 128);\n      for (int i = 0; i < 2; i++) {\n        const uint8_t* old_ip = ip;\n        assert(tag == ip[-1]);\n        // For literals tag_type = 0, hence we will always obtain 0 from\n        // ExtractLowBytes. For literals offset will thus be kLiteralOffset.\n        ptrdiff_t len_minus_offset = kLengthMinusOffset[tag];\n        uint32_t next;\n#if defined(__aarch64__)\n        size_t tag_type = AdvanceToNextTagARMOptimized(&ip, &tag);\n        // We never need more than 16 bits. Doing a Load16 allows the compiler\n        // to elide the masking operation in ExtractOffset.\n        next = LittleEndian::Load16(old_ip);\n#else\n        size_t tag_type = AdvanceToNextTagX86Optimized(&ip, &tag);\n        next = LittleEndian::Load32(old_ip);\n#endif\n        size_t len = len_minus_offset & 0xFF;\n        ptrdiff_t extracted = ExtractOffset(next, tag_type);\n        ptrdiff_t len_min_offset = len_minus_offset - extracted;\n        if (SNAPPY_PREDICT_FALSE(len_minus_offset > extracted)) {\n          if (SNAPPY_PREDICT_FALSE(len & 0x80)) {\n            // Exceptional case (long literal or copy 4).\n            // Actually doing the copy here is negatively impacting the main\n            // loop due to compiler incorrectly allocating a register for\n            // this fallback. Hence we just break.\n          break_loop:\n            ip = old_ip;\n            goto exit;\n          }\n          // Only copy-1 or copy-2 tags can get here.\n          assert(tag_type == 1 || tag_type == 2);\n          std::ptrdiff_t delta = (op + deferred_length) + len_min_offset - len;\n          // Guard against copies before the buffer start.\n          // Execute any deferred MemCopy since we write to dst here.\n          MemCopy64(op_base + op, deferred_src, deferred_length);\n          op += deferred_length;\n          ClearDeferred(&deferred_src, &deferred_length, safe_source);\n          if (SNAPPY_PREDICT_FALSE(delta < 0 ||\n                                  !Copy64BytesWithPatternExtension(\n                                      op_base + op, len - len_min_offset))) {\n            goto break_loop;\n          }\n          // We aren't deferring this copy so add length right away.\n          op += len;\n          continue;\n        }\n        std::ptrdiff_t delta = (op + deferred_length) + len_min_offset - len;\n        if (SNAPPY_PREDICT_FALSE(delta < 0)) {\n          // Due to the spurious offset in literals have this will trigger\n          // at the start of a block when op is still smaller than 256.\n          if (tag_type != 0) goto break_loop;\n          MemCopy64(op_base + op, deferred_src, deferred_length);\n          op += deferred_length;\n          DeferMemCopy(&deferred_src, &deferred_length, old_ip, len);\n          continue;\n        }\n\n        // For copies we need to copy from op_base + delta, for literals\n        // we need to copy from ip instead of from the stream.\n        const void* from =\n            tag_type ? reinterpret_cast<void*>(op_base + delta) : old_ip;\n        MemCopy64(op_base + op, deferred_src, deferred_length);\n        op += deferred_length;\n        DeferMemCopy(&deferred_src, &deferred_length, from, len);\n      }\n    } while (ip < ip_limit_min_slop &&\n             static_cast<ptrdiff_t>(op + deferred_length) < op_limit_min_slop);\n  exit:\n    ip--;\n    assert(ip <= ip_limit);\n  }\n  // If we deferred a copy then we can perform.  If we are up to date then we\n  // might not have enough slop bytes and could run past the end.\n  if (deferred_length) {\n    MemCopy64(op_base + op, deferred_src, deferred_length);\n    op += deferred_length;\n    ClearDeferred(&deferred_src, &deferred_length, safe_source);\n  }\n  return {ip, op};\n}\n\n// Helper class for decompression\nclass SnappyDecompressor {\n private:\n  Source* reader_;        // Underlying source of bytes to decompress\n  const char* ip_;        // Points to next buffered byte\n  const char* ip_limit_;  // Points just past buffered bytes\n  // If ip < ip_limit_min_maxtaglen_ it's safe to read kMaxTagLength from\n  // buffer.\n  const char* ip_limit_min_maxtaglen_;\n  uint32_t peeked_;                  // Bytes peeked from reader (need to skip)\n  bool eof_;                         // Hit end of input without an error?\n  char scratch_[kMaximumTagLength];  // See RefillTag().\n\n  // Ensure that all of the tag metadata for the next tag is available\n  // in [ip_..ip_limit_-1].  Also ensures that [ip,ip+4] is readable even\n  // if (ip_limit_ - ip_ < 5).\n  //\n  // Returns true on success, false on error or end of input.\n  bool RefillTag();\n\n  void ResetLimit(const char* ip) {\n    ip_limit_min_maxtaglen_ =\n        ip_limit_ - std::min<ptrdiff_t>(ip_limit_ - ip, kMaximumTagLength - 1);\n  }\n\n public:\n  explicit SnappyDecompressor(Source* reader)\n      : reader_(reader), ip_(NULL), ip_limit_(NULL), peeked_(0), eof_(false) {}\n\n  ~SnappyDecompressor() {\n    // Advance past any bytes we peeked at from the reader\n    reader_->Skip(peeked_);\n  }\n\n  // Returns true iff we have hit the end of the input without an error.\n  bool eof() const { return eof_; }\n\n  // Read the uncompressed length stored at the start of the compressed data.\n  // On success, stores the length in *result and returns true.\n  // On failure, returns false.\n  bool ReadUncompressedLength(uint32_t* result) {\n    assert(ip_ == NULL);  // Must not have read anything yet\n    // Length is encoded in 1..5 bytes\n    *result = 0;\n    uint32_t shift = 0;\n    while (true) {\n      if (shift >= 32) return false;\n      size_t n;\n      const char* ip = reader_->Peek(&n);\n      if (n == 0) return false;\n      const unsigned char c = *(reinterpret_cast<const unsigned char*>(ip));\n      reader_->Skip(1);\n      uint32_t val = c & 0x7f;\n      if (LeftShiftOverflows(static_cast<uint8_t>(val), shift)) return false;\n      *result |= val << shift;\n      if (c < 128) {\n        break;\n      }\n      shift += 7;\n    }\n    return true;\n  }\n\n  // Process the next item found in the input.\n  // Returns true if successful, false on error or end of input.\n  template <class Writer>\n#if defined(__GNUC__) && defined(__x86_64__)\n  __attribute__((aligned(32)))\n#endif\n  void\n  DecompressAllTags(Writer* writer) {\n    const char* ip = ip_;\n    ResetLimit(ip);\n    auto op = writer->GetOutputPtr();\n    // We could have put this refill fragment only at the beginning of the loop.\n    // However, duplicating it at the end of each branch gives the compiler more\n    // scope to optimize the <ip_limit_ - ip> expression based on the local\n    // context, which overall increases speed.\n#define MAYBE_REFILL()                                      \\\n  if (SNAPPY_PREDICT_FALSE(ip >= ip_limit_min_maxtaglen_)) { \\\n    ip_ = ip;                                               \\\n    if (SNAPPY_PREDICT_FALSE(!RefillTag())) goto exit;       \\\n    ip = ip_;                                               \\\n    ResetLimit(ip);                                         \\\n  }                                                         \\\n  preload = static_cast<uint8_t>(*ip)\n\n    // At the start of the for loop below the least significant byte of preload\n    // contains the tag.\n    uint32_t preload;\n    MAYBE_REFILL();\n    for (;;) {\n      {\n        ptrdiff_t op_limit_min_slop;\n        auto op_base = writer->GetBase(&op_limit_min_slop);\n        if (op_base) {\n          auto res =\n              DecompressBranchless(reinterpret_cast<const uint8_t*>(ip),\n                                   reinterpret_cast<const uint8_t*>(ip_limit_),\n                                   op - op_base, op_base, op_limit_min_slop);\n          ip = reinterpret_cast<const char*>(res.first);\n          op = op_base + res.second;\n          MAYBE_REFILL();\n        }\n      }\n      const uint8_t c = static_cast<uint8_t>(preload);\n      ip++;\n\n      // Ratio of iterations that have LITERAL vs non-LITERAL for different\n      // inputs.\n      //\n      // input          LITERAL  NON_LITERAL\n      // -----------------------------------\n      // html|html4|cp   23%        77%\n      // urls            36%        64%\n      // jpg             47%        53%\n      // pdf             19%        81%\n      // txt[1-4]        25%        75%\n      // pb              24%        76%\n      // bin             24%        76%\n      if (SNAPPY_PREDICT_FALSE((c & 0x3) == LITERAL)) {\n        size_t literal_length = (c >> 2) + 1u;\n        if (writer->TryFastAppend(ip, ip_limit_ - ip, literal_length, &op)) {\n          assert(literal_length < 61);\n          ip += literal_length;\n          // NOTE: There is no MAYBE_REFILL() here, as TryFastAppend()\n          // will not return true unless there's already at least five spare\n          // bytes in addition to the literal.\n          preload = static_cast<uint8_t>(*ip);\n          continue;\n        }\n        if (SNAPPY_PREDICT_FALSE(literal_length >= 61)) {\n          // Long literal.\n          const size_t literal_length_length = literal_length - 60;\n          literal_length =\n              ExtractLowBytes(LittleEndian::Load32(ip), literal_length_length) +\n              1;\n          ip += literal_length_length;\n        }\n\n        size_t avail = ip_limit_ - ip;\n        while (avail < literal_length) {\n          if (!writer->Append(ip, avail, &op)) goto exit;\n          literal_length -= avail;\n          reader_->Skip(peeked_);\n          size_t n;\n          ip = reader_->Peek(&n);\n          avail = n;\n          peeked_ = avail;\n          if (avail == 0) goto exit;\n          ip_limit_ = ip + avail;\n          ResetLimit(ip);\n        }\n        if (!writer->Append(ip, literal_length, &op)) goto exit;\n        ip += literal_length;\n        MAYBE_REFILL();\n      } else {\n        if (SNAPPY_PREDICT_FALSE((c & 3) == COPY_4_BYTE_OFFSET)) {\n          const size_t copy_offset = LittleEndian::Load32(ip);\n          const size_t length = (c >> 2) + 1;\n          ip += 4;\n\n          if (!writer->AppendFromSelf(copy_offset, length, &op)) goto exit;\n        } else {\n          const ptrdiff_t entry = kLengthMinusOffset[c];\n          preload = LittleEndian::Load32(ip);\n          const uint32_t trailer = ExtractLowBytes(preload, c & 3);\n          const uint32_t length = entry & 0xff;\n          assert(length > 0);\n\n          // copy_offset/256 is encoded in bits 8..10.  By just fetching\n          // those bits, we get copy_offset (since the bit-field starts at\n          // bit 8).\n          const uint32_t copy_offset = trailer - entry + length;\n          if (!writer->AppendFromSelf(copy_offset, length, &op)) goto exit;\n\n          ip += (c & 3);\n          // By using the result of the previous load we reduce the critical\n          // dependency chain of ip to 4 cycles.\n          preload >>= (c & 3) * 8;\n          if (ip < ip_limit_min_maxtaglen_) continue;\n        }\n        MAYBE_REFILL();\n      }\n    }\n#undef MAYBE_REFILL\n  exit:\n    writer->SetOutputPtr(op);\n  }\n};\n\nconstexpr uint32_t CalculateNeeded(uint8_t tag) {\n  return ((tag & 3) == 0 && tag >= (60 * 4))\n             ? (tag >> 2) - 58\n             : (0x05030201 >> ((tag * 8) & 31)) & 0xFF;\n}\n\n#if __cplusplus >= 201402L\nconstexpr bool VerifyCalculateNeeded() {\n  for (int i = 0; i < 1; i++) {\n    if (CalculateNeeded(i) != (char_table[i] >> 11) + 1) return false;\n  }\n  return true;\n}\n\n// Make sure CalculateNeeded is correct by verifying it against the established\n// table encoding the number of added bytes needed.\nstatic_assert(VerifyCalculateNeeded(), \"\");\n#endif  // c++14\n\nbool SnappyDecompressor::RefillTag() {\n  const char* ip = ip_;\n  if (ip == ip_limit_) {\n    // Fetch a new fragment from the reader\n    reader_->Skip(peeked_);  // All peeked bytes are used up\n    size_t n;\n    ip = reader_->Peek(&n);\n    peeked_ = n;\n    eof_ = (n == 0);\n    if (eof_) return false;\n    ip_limit_ = ip + n;\n  }\n\n  // Read the tag character\n  assert(ip < ip_limit_);\n  const unsigned char c = *(reinterpret_cast<const unsigned char*>(ip));\n  // At this point make sure that the data for the next tag is consecutive.\n  // For copy 1 this means the next 2 bytes (tag and 1 byte offset)\n  // For copy 2 the next 3 bytes (tag and 2 byte offset)\n  // For copy 4 the next 5 bytes (tag and 4 byte offset)\n  // For all small literals we only need 1 byte buf for literals 60...63 the\n  // length is encoded in 1...4 extra bytes.\n  const uint32_t needed = CalculateNeeded(c);\n  assert(needed <= sizeof(scratch_));\n\n  // Read more bytes from reader if needed\n  uint32_t nbuf = ip_limit_ - ip;\n  if (nbuf < needed) {\n    // Stitch together bytes from ip and reader to form the word\n    // contents.  We store the needed bytes in \"scratch_\".  They\n    // will be consumed immediately by the caller since we do not\n    // read more than we need.\n    std::memmove(scratch_, ip, nbuf);\n    reader_->Skip(peeked_);  // All peeked bytes are used up\n    peeked_ = 0;\n    while (nbuf < needed) {\n      size_t length;\n      const char* src = reader_->Peek(&length);\n      if (length == 0) return false;\n      uint32_t to_add = std::min<uint32_t>(needed - nbuf, length);\n      std::memcpy(scratch_ + nbuf, src, to_add);\n      nbuf += to_add;\n      reader_->Skip(to_add);\n    }\n    assert(nbuf == needed);\n    ip_ = scratch_;\n    ip_limit_ = scratch_ + needed;\n  } else if (nbuf < kMaximumTagLength) {\n    // Have enough bytes, but move into scratch_ so that we do not\n    // read past end of input\n    std::memmove(scratch_, ip, nbuf);\n    reader_->Skip(peeked_);  // All peeked bytes are used up\n    peeked_ = 0;\n    ip_ = scratch_;\n    ip_limit_ = scratch_ + nbuf;\n  } else {\n    // Pass pointer to buffer returned by reader_.\n    ip_ = ip;\n  }\n  return true;\n}\n\ntemplate <typename Writer>\nstatic bool InternalUncompress(Source* r, Writer* writer) {\n  // Read the uncompressed length from the front of the compressed input\n  SnappyDecompressor decompressor(r);\n  uint32_t uncompressed_len = 0;\n  if (!decompressor.ReadUncompressedLength(&uncompressed_len)) return false;\n\n  return InternalUncompressAllTags(&decompressor, writer, r->Available(),\n                                   uncompressed_len);\n}\n\ntemplate <typename Writer>\nstatic bool InternalUncompressAllTags(SnappyDecompressor* decompressor,\n                                      Writer* writer, uint32_t compressed_len,\n                                      uint32_t uncompressed_len) {\n    int token = 0;\n  Report(token, \"snappy_uncompress\", compressed_len, uncompressed_len);\n\n  writer->SetExpectedLength(uncompressed_len);\n\n  // Process the entire input\n  decompressor->DecompressAllTags(writer);\n  writer->Flush();\n  return (decompressor->eof() && writer->CheckLength());\n}\n\nbool GetUncompressedLength(Source* source, uint32_t* result) {\n  SnappyDecompressor decompressor(source);\n  return decompressor.ReadUncompressedLength(result);\n}\n\nsize_t Compress(Source* reader, Sink* writer) {\n  return Compress(reader, writer, CompressionOptions{});\n}\n\nsize_t Compress(Source* reader, Sink* writer, CompressionOptions options) {\n  assert(options.level == 1 || options.level == 2);\n  int token = 0;\n  size_t written = 0;\n  size_t N = reader->Available();\n  const size_t uncompressed_size = N;\n  char ulength[Varint::kMax32];\n  char* p = Varint::Encode32(ulength, N);\n  writer->Append(ulength, p - ulength);\n  written += (p - ulength);\n\n  internal::WorkingMemory wmem(N);\n\n  while (N > 0) {\n    // Get next block to compress (without copying if possible)\n    size_t fragment_size;\n    const char* fragment = reader->Peek(&fragment_size);\n    assert(fragment_size != 0);  // premature end of input\n    const size_t num_to_read = std::min(N, kBlockSize);\n    size_t bytes_read = fragment_size;\n\n    size_t pending_advance = 0;\n    if (bytes_read >= num_to_read) {\n      // Buffer returned by reader is large enough\n      pending_advance = num_to_read;\n      fragment_size = num_to_read;\n    } else {\n      char* scratch = wmem.GetScratchInput();\n      std::memcpy(scratch, fragment, bytes_read);\n      reader->Skip(bytes_read);\n\n      while (bytes_read < num_to_read) {\n        fragment = reader->Peek(&fragment_size);\n        size_t n = std::min<size_t>(fragment_size, num_to_read - bytes_read);\n        std::memcpy(scratch + bytes_read, fragment, n);\n        bytes_read += n;\n        reader->Skip(n);\n      }\n      assert(bytes_read == num_to_read);\n      fragment = scratch;\n      fragment_size = num_to_read;\n    }\n    assert(fragment_size == num_to_read);\n\n    // Get encoding table for compression\n    int table_size;\n    uint16_t* table = wmem.GetHashTable(num_to_read, &table_size);\n\n    // Compress input_fragment and append to dest\n    int max_output = MaxCompressedLength(num_to_read);\n\n    // Since we encode kBlockSize regions followed by a region\n    // which is <= kBlockSize in length, a previously allocated\n    // scratch_output[] region is big enough for this iteration.\n    // Need a scratch buffer for the output, in case the byte sink doesn't\n    // have room for us directly.\n    char* dest = writer->GetAppendBuffer(max_output, wmem.GetScratchOutput());\n    char* end = nullptr;\n    if (options.level == 1) {\n      end = internal::CompressFragment(fragment, fragment_size, dest, table,\n                                       table_size);\n    } else if (options.level == 2) {\n      end = internal::CompressFragmentDoubleHash(\n          fragment, fragment_size, dest, table, table_size >> 1,\n          table + (table_size >> 1), table_size >> 1);\n    }\n    writer->Append(dest, end - dest);\n    written += (end - dest);\n\n    N -= num_to_read;\n    reader->Skip(pending_advance);\n  }\n\n  Report(token, \"snappy_compress\", written, uncompressed_size);\n  return written;\n}\n\n// -----------------------------------------------------------------------\n// IOVec interfaces\n// -----------------------------------------------------------------------\n\n// A `Source` implementation that yields the contents of an `iovec` array. Note\n// that `total_size` is the total number of bytes to be read from the elements\n// of `iov` (_not_ the total number of elements in `iov`).\nclass SnappyIOVecReader : public Source {\n public:\n  SnappyIOVecReader(const struct iovec* iov, size_t total_size)\n      : curr_iov_(iov),\n        curr_pos_(total_size > 0 ? reinterpret_cast<const char*>(iov->iov_base)\n                                 : nullptr),\n        curr_size_remaining_(total_size > 0 ? iov->iov_len : 0),\n        total_size_remaining_(total_size) {\n    // Skip empty leading `iovec`s.\n    if (total_size > 0 && curr_size_remaining_ == 0) Advance();\n  }\n\n  ~SnappyIOVecReader() override = default;\n\n  size_t Available() const override { return total_size_remaining_; }\n\n  const char* Peek(size_t* len) override {\n    *len = curr_size_remaining_;\n    return curr_pos_;\n  }\n\n  void Skip(size_t n) override {\n    while (n >= curr_size_remaining_ && n > 0) {\n      n -= curr_size_remaining_;\n      Advance();\n    }\n    curr_size_remaining_ -= n;\n    total_size_remaining_ -= n;\n    curr_pos_ += n;\n  }\n\n private:\n  // Advances to the next nonempty `iovec` and updates related variables.\n  void Advance() {\n    do {\n      assert(total_size_remaining_ >= curr_size_remaining_);\n      total_size_remaining_ -= curr_size_remaining_;\n      if (total_size_remaining_ == 0) {\n        curr_pos_ = nullptr;\n        curr_size_remaining_ = 0;\n        return;\n      }\n      ++curr_iov_;\n      curr_pos_ = reinterpret_cast<const char*>(curr_iov_->iov_base);\n      curr_size_remaining_ = curr_iov_->iov_len;\n    } while (curr_size_remaining_ == 0);\n  }\n\n  // The `iovec` currently being read.\n  const struct iovec* curr_iov_;\n  // The location in `curr_iov_` currently being read.\n  const char* curr_pos_;\n  // The amount of unread data in `curr_iov_`.\n  size_t curr_size_remaining_;\n  // The amount of unread data in the entire input array.\n  size_t total_size_remaining_;\n};\n\n// A type that writes to an iovec.\n// Note that this is not a \"ByteSink\", but a type that matches the\n// Writer template argument to SnappyDecompressor::DecompressAllTags().\nclass SnappyIOVecWriter {\n private:\n  // output_iov_end_ is set to iov + count and used to determine when\n  // the end of the iovs is reached.\n  const struct iovec* output_iov_end_;\n\n#if !defined(NDEBUG)\n  const struct iovec* output_iov_;\n#endif  // !defined(NDEBUG)\n\n  // Current iov that is being written into.\n  const struct iovec* curr_iov_;\n\n  // Pointer to current iov's write location.\n  char* curr_iov_output_;\n\n  // Remaining bytes to write into curr_iov_output.\n  size_t curr_iov_remaining_;\n\n  // Total bytes decompressed into output_iov_ so far.\n  size_t total_written_;\n\n  // Maximum number of bytes that will be decompressed into output_iov_.\n  size_t output_limit_;\n\n  static inline char* GetIOVecPointer(const struct iovec* iov, size_t offset) {\n    return reinterpret_cast<char*>(iov->iov_base) + offset;\n  }\n\n public:\n  // Does not take ownership of iov. iov must be valid during the\n  // entire lifetime of the SnappyIOVecWriter.\n  inline SnappyIOVecWriter(const struct iovec* iov, size_t iov_count)\n      : output_iov_end_(iov + iov_count),\n#if !defined(NDEBUG)\n        output_iov_(iov),\n#endif  // !defined(NDEBUG)\n        curr_iov_(iov),\n        curr_iov_output_(iov_count ? reinterpret_cast<char*>(iov->iov_base)\n                                   : nullptr),\n        curr_iov_remaining_(iov_count ? iov->iov_len : 0),\n        total_written_(0),\n        output_limit_(-1) {\n  }\n\n  inline void SetExpectedLength(size_t len) { output_limit_ = len; }\n\n  inline bool CheckLength() const { return total_written_ == output_limit_; }\n\n  inline bool Append(const char* ip, size_t len, char**) {\n    if (total_written_ + len > output_limit_) {\n      return false;\n    }\n\n    return AppendNoCheck(ip, len);\n  }\n\n  char* GetOutputPtr() { return nullptr; }\n  char* GetBase(ptrdiff_t*) { return nullptr; }\n  void SetOutputPtr(char* op) {\n    // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n    (void)op;\n  }\n\n  inline bool AppendNoCheck(const char* ip, size_t len) {\n    while (len > 0) {\n      if (curr_iov_remaining_ == 0) {\n        // This iovec is full. Go to the next one.\n        if (curr_iov_ + 1 >= output_iov_end_) {\n          return false;\n        }\n        ++curr_iov_;\n        curr_iov_output_ = reinterpret_cast<char*>(curr_iov_->iov_base);\n        curr_iov_remaining_ = curr_iov_->iov_len;\n      }\n\n      const size_t to_write = std::min(len, curr_iov_remaining_);\n      std::memcpy(curr_iov_output_, ip, to_write);\n      curr_iov_output_ += to_write;\n      curr_iov_remaining_ -= to_write;\n      total_written_ += to_write;\n      ip += to_write;\n      len -= to_write;\n    }\n\n    return true;\n  }\n\n  inline bool TryFastAppend(const char* ip, size_t available, size_t len,\n                            char**) {\n    const size_t space_left = output_limit_ - total_written_;\n    if (len <= 16 && available >= 16 + kMaximumTagLength && space_left >= 16 &&\n        curr_iov_remaining_ >= 16) {\n      // Fast path, used for the majority (about 95%) of invocations.\n      UnalignedCopy128(ip, curr_iov_output_);\n      curr_iov_output_ += len;\n      curr_iov_remaining_ -= len;\n      total_written_ += len;\n      return true;\n    }\n\n    return false;\n  }\n\n  inline bool AppendFromSelf(size_t offset, size_t len, char**) {\n    // See SnappyArrayWriter::AppendFromSelf for an explanation of\n    // the \"offset - 1u\" trick.\n    if (offset - 1u >= total_written_) {\n      return false;\n    }\n    const size_t space_left = output_limit_ - total_written_;\n    if (len > space_left) {\n      return false;\n    }\n\n    // Locate the iovec from which we need to start the copy.\n    const iovec* from_iov = curr_iov_;\n    size_t from_iov_offset = curr_iov_->iov_len - curr_iov_remaining_;\n    while (offset > 0) {\n      if (from_iov_offset >= offset) {\n        from_iov_offset -= offset;\n        break;\n      }\n\n      offset -= from_iov_offset;\n      --from_iov;\n#if !defined(NDEBUG)\n      assert(from_iov >= output_iov_);\n#endif  // !defined(NDEBUG)\n      from_iov_offset = from_iov->iov_len;\n    }\n\n    // Copy <len> bytes starting from the iovec pointed to by from_iov_index to\n    // the current iovec.\n    while (len > 0) {\n      assert(from_iov <= curr_iov_);\n      if (from_iov != curr_iov_) {\n        const size_t to_copy =\n            std::min(from_iov->iov_len - from_iov_offset, len);\n        AppendNoCheck(GetIOVecPointer(from_iov, from_iov_offset), to_copy);\n        len -= to_copy;\n        if (len > 0) {\n          ++from_iov;\n          from_iov_offset = 0;\n        }\n      } else {\n        size_t to_copy = curr_iov_remaining_;\n        if (to_copy == 0) {\n          // This iovec is full. Go to the next one.\n          if (curr_iov_ + 1 >= output_iov_end_) {\n            return false;\n          }\n          ++curr_iov_;\n          curr_iov_output_ = reinterpret_cast<char*>(curr_iov_->iov_base);\n          curr_iov_remaining_ = curr_iov_->iov_len;\n          continue;\n        }\n        if (to_copy > len) {\n          to_copy = len;\n        }\n        assert(to_copy > 0);\n\n        IncrementalCopy(GetIOVecPointer(from_iov, from_iov_offset),\n                        curr_iov_output_, curr_iov_output_ + to_copy,\n                        curr_iov_output_ + curr_iov_remaining_);\n        curr_iov_output_ += to_copy;\n        curr_iov_remaining_ -= to_copy;\n        from_iov_offset += to_copy;\n        total_written_ += to_copy;\n        len -= to_copy;\n      }\n    }\n\n    return true;\n  }\n\n  inline void Flush() {}\n};\n\nbool RawUncompressToIOVec(const char* compressed, size_t compressed_length,\n                          const struct iovec* iov, size_t iov_cnt) {\n  ByteArraySource reader(compressed, compressed_length);\n  return RawUncompressToIOVec(&reader, iov, iov_cnt);\n}\n\nbool RawUncompressToIOVec(Source* compressed, const struct iovec* iov,\n                          size_t iov_cnt) {\n  SnappyIOVecWriter output(iov, iov_cnt);\n  return InternalUncompress(compressed, &output);\n}\n\n// -----------------------------------------------------------------------\n// Flat array interfaces\n// -----------------------------------------------------------------------\n\n// A type that writes to a flat array.\n// Note that this is not a \"ByteSink\", but a type that matches the\n// Writer template argument to SnappyDecompressor::DecompressAllTags().\nclass SnappyArrayWriter {\n private:\n  char* base_;\n  char* op_;\n  char* op_limit_;\n  // If op < op_limit_min_slop_ then it's safe to unconditionally write\n  // kSlopBytes starting at op.\n  char* op_limit_min_slop_;\n\n public:\n  inline explicit SnappyArrayWriter(char* dst)\n      : base_(dst),\n        op_(dst),\n        op_limit_(dst),\n        op_limit_min_slop_(dst) {}  // Safe default see invariant.\n\n  inline void SetExpectedLength(size_t len) {\n    op_limit_ = op_ + len;\n    // Prevent pointer from being past the buffer.\n    op_limit_min_slop_ = op_limit_ - std::min<size_t>(kSlopBytes - 1, len);\n  }\n\n  inline bool CheckLength() const { return op_ == op_limit_; }\n\n  char* GetOutputPtr() { return op_; }\n  char* GetBase(ptrdiff_t* op_limit_min_slop) {\n    *op_limit_min_slop = op_limit_min_slop_ - base_;\n    return base_;\n  }\n  void SetOutputPtr(char* op) { op_ = op; }\n\n  inline bool Append(const char* ip, size_t len, char** op_p) {\n    char* op = *op_p;\n    const size_t space_left = op_limit_ - op;\n    if (space_left < len) return false;\n    std::memcpy(op, ip, len);\n    *op_p = op + len;\n    return true;\n  }\n\n  inline bool TryFastAppend(const char* ip, size_t available, size_t len,\n                            char** op_p) {\n    char* op = *op_p;\n    const size_t space_left = op_limit_ - op;\n    if (len <= 16 && available >= 16 + kMaximumTagLength && space_left >= 16) {\n      // Fast path, used for the majority (about 95%) of invocations.\n      UnalignedCopy128(ip, op);\n      *op_p = op + len;\n      return true;\n    } else {\n      return false;\n    }\n  }\n\n  SNAPPY_ATTRIBUTE_ALWAYS_INLINE\n  inline bool AppendFromSelf(size_t offset, size_t len, char** op_p) {\n    assert(len > 0);\n    char* const op = *op_p;\n    assert(op >= base_);\n    char* const op_end = op + len;\n\n    // Check if we try to append from before the start of the buffer.\n    if (SNAPPY_PREDICT_FALSE(static_cast<size_t>(op - base_) < offset))\n      return false;\n\n    if (SNAPPY_PREDICT_FALSE((kSlopBytes < 64 && len > kSlopBytes) ||\n                            op >= op_limit_min_slop_ || offset < len)) {\n      if (op_end > op_limit_ || offset == 0) return false;\n      *op_p = IncrementalCopy(op - offset, op, op_end, op_limit_);\n      return true;\n    }\n    std::memmove(op, op - offset, kSlopBytes);\n    *op_p = op_end;\n    return true;\n  }\n  inline size_t Produced() const {\n    assert(op_ >= base_);\n    return op_ - base_;\n  }\n  inline void Flush() {}\n};\n\nbool RawUncompress(const char* compressed, size_t compressed_length,\n                   char* uncompressed) {\n  ByteArraySource reader(compressed, compressed_length);\n  return RawUncompress(&reader, uncompressed);\n}\n\nbool RawUncompress(Source* compressed, char* uncompressed) {\n  SnappyArrayWriter output(uncompressed);\n  return InternalUncompress(compressed, &output);\n}\n\nbool Uncompress(const char* compressed, size_t compressed_length,\n                std::string* uncompressed) {\n  size_t ulength;\n  if (!GetUncompressedLength(compressed, compressed_length, &ulength)) {\n    return false;\n  }\n  // On 32-bit builds: max_size() < kuint32max.  Check for that instead\n  // of crashing (e.g., consider externally specified compressed data).\n  if (ulength > uncompressed->max_size()) {\n    return false;\n  }\n  STLStringResizeUninitialized(uncompressed, ulength);\n  return RawUncompress(compressed, compressed_length,\n                       string_as_array(uncompressed));\n}\n\n// A Writer that drops everything on the floor and just does validation\nclass SnappyDecompressionValidator {\n private:\n  size_t expected_;\n  size_t produced_;\n\n public:\n  inline SnappyDecompressionValidator() : expected_(0), produced_(0) {}\n  inline void SetExpectedLength(size_t len) { expected_ = len; }\n  size_t GetOutputPtr() { return produced_; }\n  size_t GetBase(ptrdiff_t* op_limit_min_slop) {\n    *op_limit_min_slop = std::numeric_limits<ptrdiff_t>::max() - kSlopBytes + 1;\n    return 1;\n  }\n  void SetOutputPtr(size_t op) { produced_ = op; }\n  inline bool CheckLength() const { return expected_ == produced_; }\n  inline bool Append(const char* ip, size_t len, size_t* produced) {\n    // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n    (void)ip;\n\n    *produced += len;\n    return *produced <= expected_;\n  }\n  inline bool TryFastAppend(const char* ip, size_t available, size_t length,\n                            size_t* produced) {\n    // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n    (void)ip;\n    (void)available;\n    (void)length;\n    (void)produced;\n\n    return false;\n  }\n  inline bool AppendFromSelf(size_t offset, size_t len, size_t* produced) {\n    // See SnappyArrayWriter::AppendFromSelf for an explanation of\n    // the \"offset - 1u\" trick.\n    if (*produced <= offset - 1u) return false;\n    *produced += len;\n    return *produced <= expected_;\n  }\n  inline void Flush() {}\n};\n\nbool IsValidCompressedBuffer(const char* compressed, size_t compressed_length) {\n  ByteArraySource reader(compressed, compressed_length);\n  SnappyDecompressionValidator writer;\n  return InternalUncompress(&reader, &writer);\n}\n\nbool IsValidCompressed(Source* compressed) {\n  SnappyDecompressionValidator writer;\n  return InternalUncompress(compressed, &writer);\n}\n\nvoid RawCompress(const char* input, size_t input_length, char* compressed,\n                 size_t* compressed_length) {\n  RawCompress(input, input_length, compressed, compressed_length,\n              CompressionOptions{});\n}\n\nvoid RawCompress(const char* input, size_t input_length, char* compressed,\n                 size_t* compressed_length, CompressionOptions options) {\n  ByteArraySource reader(input, input_length);\n  UncheckedByteArraySink writer(compressed);\n  Compress(&reader, &writer, options);\n\n  // Compute how many bytes were added\n  *compressed_length = (writer.CurrentDestination() - compressed);\n}\n\nvoid RawCompressFromIOVec(const struct iovec* iov, size_t uncompressed_length,\n                          char* compressed, size_t* compressed_length) {\n  RawCompressFromIOVec(iov, uncompressed_length, compressed, compressed_length,\n                       CompressionOptions{});\n}\n\nvoid RawCompressFromIOVec(const struct iovec* iov, size_t uncompressed_length,\n                          char* compressed, size_t* compressed_length,\n                          CompressionOptions options) {\n  SnappyIOVecReader reader(iov, uncompressed_length);\n  UncheckedByteArraySink writer(compressed);\n  Compress(&reader, &writer, options);\n\n  // Compute how many bytes were added.\n  *compressed_length = writer.CurrentDestination() - compressed;\n}\n\nsize_t Compress(const char* input, size_t input_length,\n                std::string* compressed) {\n  return Compress(input, input_length, compressed, CompressionOptions{});\n}\n\nsize_t Compress(const char* input, size_t input_length, std::string* compressed,\n                CompressionOptions options) {\n  // Pre-grow the buffer to the max length of the compressed output\n  STLStringResizeUninitialized(compressed, MaxCompressedLength(input_length));\n\n  size_t compressed_length;\n  RawCompress(input, input_length, string_as_array(compressed),\n              &compressed_length, options);\n  compressed->erase(compressed_length);\n  return compressed_length;\n}\n\nsize_t CompressFromIOVec(const struct iovec* iov, size_t iov_cnt,\n                         std::string* compressed) {\n  return CompressFromIOVec(iov, iov_cnt, compressed, CompressionOptions{});\n}\n\nsize_t CompressFromIOVec(const struct iovec* iov, size_t iov_cnt,\n                         std::string* compressed, CompressionOptions options) {\n  // Compute the number of bytes to be compressed.\n  size_t uncompressed_length = 0;\n  for (size_t i = 0; i < iov_cnt; ++i) {\n    uncompressed_length += iov[i].iov_len;\n  }\n\n  // Pre-grow the buffer to the max length of the compressed output.\n  STLStringResizeUninitialized(compressed, MaxCompressedLength(\n      uncompressed_length));\n\n  size_t compressed_length;\n  RawCompressFromIOVec(iov, uncompressed_length, string_as_array(compressed),\n                       &compressed_length, options);\n  compressed->erase(compressed_length);\n  return compressed_length;\n}\n\n// -----------------------------------------------------------------------\n// Sink interface\n// -----------------------------------------------------------------------\n\n// A type that decompresses into a Sink. The template parameter\n// Allocator must export one method \"char* Allocate(int size);\", which\n// allocates a buffer of \"size\" and appends that to the destination.\ntemplate <typename Allocator>\nclass SnappyScatteredWriter {\n  Allocator allocator_;\n\n  // We need random access into the data generated so far.  Therefore\n  // we keep track of all of the generated data as an array of blocks.\n  // All of the blocks except the last have length kBlockSize.\n  std::vector<char*> blocks_;\n  size_t expected_;\n\n  // Total size of all fully generated blocks so far\n  size_t full_size_;\n\n  // Pointer into current output block\n  char* op_base_;   // Base of output block\n  char* op_ptr_;    // Pointer to next unfilled byte in block\n  char* op_limit_;  // Pointer just past block\n  // If op < op_limit_min_slop_ then it's safe to unconditionally write\n  // kSlopBytes starting at op.\n  char* op_limit_min_slop_;\n\n  inline size_t Size() const { return full_size_ + (op_ptr_ - op_base_); }\n\n  bool SlowAppend(const char* ip, size_t len);\n  bool SlowAppendFromSelf(size_t offset, size_t len);\n\n public:\n  inline explicit SnappyScatteredWriter(const Allocator& allocator)\n      : allocator_(allocator),\n        full_size_(0),\n        op_base_(NULL),\n        op_ptr_(NULL),\n        op_limit_(NULL),\n        op_limit_min_slop_(NULL) {}\n  char* GetOutputPtr() { return op_ptr_; }\n  char* GetBase(ptrdiff_t* op_limit_min_slop) {\n    *op_limit_min_slop = op_limit_min_slop_ - op_base_;\n    return op_base_;\n  }\n  void SetOutputPtr(char* op) { op_ptr_ = op; }\n\n  inline void SetExpectedLength(size_t len) {\n    assert(blocks_.empty());\n    expected_ = len;\n  }\n\n  inline bool CheckLength() const { return Size() == expected_; }\n\n  // Return the number of bytes actually uncompressed so far\n  inline size_t Produced() const { return Size(); }\n\n  inline bool Append(const char* ip, size_t len, char** op_p) {\n    char* op = *op_p;\n    size_t avail = op_limit_ - op;\n    if (len <= avail) {\n      // Fast path\n      std::memcpy(op, ip, len);\n      *op_p = op + len;\n      return true;\n    } else {\n      op_ptr_ = op;\n      bool res = SlowAppend(ip, len);\n      *op_p = op_ptr_;\n      return res;\n    }\n  }\n\n  inline bool TryFastAppend(const char* ip, size_t available, size_t length,\n                            char** op_p) {\n    char* op = *op_p;\n    const int space_left = op_limit_ - op;\n    if (length <= 16 && available >= 16 + kMaximumTagLength &&\n        space_left >= 16) {\n      // Fast path, used for the majority (about 95%) of invocations.\n      UnalignedCopy128(ip, op);\n      *op_p = op + length;\n      return true;\n    } else {\n      return false;\n    }\n  }\n\n  inline bool AppendFromSelf(size_t offset, size_t len, char** op_p) {\n    char* op = *op_p;\n    assert(op >= op_base_);\n    // Check if we try to append from before the start of the buffer.\n    if (SNAPPY_PREDICT_FALSE((kSlopBytes < 64 && len > kSlopBytes) ||\n                            static_cast<size_t>(op - op_base_) < offset ||\n                            op >= op_limit_min_slop_ || offset < len)) {\n      if (offset == 0) return false;\n      if (SNAPPY_PREDICT_FALSE(static_cast<size_t>(op - op_base_) < offset ||\n                              op + len > op_limit_)) {\n        op_ptr_ = op;\n        bool res = SlowAppendFromSelf(offset, len);\n        *op_p = op_ptr_;\n        return res;\n      }\n      *op_p = IncrementalCopy(op - offset, op, op + len, op_limit_);\n      return true;\n    }\n    // Fast path\n    char* const op_end = op + len;\n    std::memmove(op, op - offset, kSlopBytes);\n    *op_p = op_end;\n    return true;\n  }\n\n  // Called at the end of the decompress. We ask the allocator\n  // write all blocks to the sink.\n  inline void Flush() { allocator_.Flush(Produced()); }\n};\n\ntemplate <typename Allocator>\nbool SnappyScatteredWriter<Allocator>::SlowAppend(const char* ip, size_t len) {\n  size_t avail = op_limit_ - op_ptr_;\n  while (len > avail) {\n    // Completely fill this block\n    std::memcpy(op_ptr_, ip, avail);\n    op_ptr_ += avail;\n    assert(op_limit_ - op_ptr_ == 0);\n    full_size_ += (op_ptr_ - op_base_);\n    len -= avail;\n    ip += avail;\n\n    // Bounds check\n    if (full_size_ + len > expected_) return false;\n\n    // Make new block\n    size_t bsize = std::min<size_t>(kBlockSize, expected_ - full_size_);\n    op_base_ = allocator_.Allocate(bsize);\n    op_ptr_ = op_base_;\n    op_limit_ = op_base_ + bsize;\n    op_limit_min_slop_ = op_limit_ - std::min<size_t>(kSlopBytes - 1, bsize);\n\n    blocks_.push_back(op_base_);\n    avail = bsize;\n  }\n\n  std::memcpy(op_ptr_, ip, len);\n  op_ptr_ += len;\n  return true;\n}\n\ntemplate <typename Allocator>\nbool SnappyScatteredWriter<Allocator>::SlowAppendFromSelf(size_t offset,\n                                                         size_t len) {\n  // Overflow check\n  // See SnappyArrayWriter::AppendFromSelf for an explanation of\n  // the \"offset - 1u\" trick.\n  const size_t cur = Size();\n  if (offset - 1u >= cur) return false;\n  if (expected_ - cur < len) return false;\n\n  // Currently we shouldn't ever hit this path because Compress() chops the\n  // input into blocks and does not create cross-block copies. However, it is\n  // nice if we do not rely on that, since we can get better compression if we\n  // allow cross-block copies and thus might want to change the compressor in\n  // the future.\n  // TODO Replace this with a properly optimized path. This is not\n  // triggered right now. But this is so super slow, that it would regress\n  // performance unacceptably if triggered.\n  size_t src = cur - offset;\n  char* op = op_ptr_;\n  while (len-- > 0) {\n    char c = blocks_[src >> kBlockLog][src & (kBlockSize - 1)];\n    if (!Append(&c, 1, &op)) {\n      op_ptr_ = op;\n      return false;\n    }\n    src++;\n  }\n  op_ptr_ = op;\n  return true;\n}\n\nclass SnappySinkAllocator {\n public:\n  explicit SnappySinkAllocator(Sink* dest) : dest_(dest) {}\n\n  char* Allocate(int size) {\n    Datablock block(new char[size], size);\n    blocks_.push_back(block);\n    return block.data;\n  }\n\n  // We flush only at the end, because the writer wants\n  // random access to the blocks and once we hand the\n  // block over to the sink, we can't access it anymore.\n  // Also we don't write more than has been actually written\n  // to the blocks.\n  void Flush(size_t size) {\n    size_t size_written = 0;\n    for (Datablock& block : blocks_) {\n      size_t block_size = std::min<size_t>(block.size, size - size_written);\n      dest_->AppendAndTakeOwnership(block.data, block_size,\n                                    &SnappySinkAllocator::Deleter, NULL);\n      size_written += block_size;\n    }\n    blocks_.clear();\n  }\n\n private:\n  struct Datablock {\n    char* data;\n    size_t size;\n    Datablock(char* p, size_t s) : data(p), size(s) {}\n  };\n\n  static void Deleter(void* arg, const char* bytes, size_t size) {\n    // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n    (void)arg;\n    (void)size;\n\n    delete[] bytes;\n  }\n\n  Sink* dest_;\n  std::vector<Datablock> blocks_;\n\n  // Note: copying this object is allowed\n};\n\nsize_t UncompressAsMuchAsPossible(Source* compressed, Sink* uncompressed) {\n  SnappySinkAllocator allocator(uncompressed);\n  SnappyScatteredWriter<SnappySinkAllocator> writer(allocator);\n  InternalUncompress(compressed, &writer);\n  return writer.Produced();\n}\n\nbool Uncompress(Source* compressed, Sink* uncompressed) {\n  // Read the uncompressed length from the front of the compressed input\n  SnappyDecompressor decompressor(compressed);\n  uint32_t uncompressed_len = 0;\n  if (!decompressor.ReadUncompressedLength(&uncompressed_len)) {\n    return false;\n  }\n\n  char c;\n  size_t allocated_size;\n  char* buf = uncompressed->GetAppendBufferVariable(1, uncompressed_len, &c, 1,\n                                                    &allocated_size);\n\n  const size_t compressed_len = compressed->Available();\n  // If we can get a flat buffer, then use it, otherwise do block by block\n  // uncompression\n  if (allocated_size >= uncompressed_len) {\n    SnappyArrayWriter writer(buf);\n    bool result = InternalUncompressAllTags(&decompressor, &writer,\n                                            compressed_len, uncompressed_len);\n    uncompressed->Append(buf, writer.Produced());\n    return result;\n  } else {\n    SnappySinkAllocator allocator(uncompressed);\n    SnappyScatteredWriter<SnappySinkAllocator> writer(allocator);\n    return InternalUncompressAllTags(&decompressor, &writer, compressed_len,\n                                     uncompressed_len);\n  }\n}\n\n}  // namespace snappy\n"
        },
        {
          "name": "snappy.h",
          "type": "blob",
          "size": 12.2724609375,
          "content": "// Copyright 2005 and onwards Google Inc.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// A light-weight compression algorithm.  It is designed for speed of\n// compression and decompression, rather than for the utmost in space\n// savings.\n//\n// For getting better compression ratios when you are compressing data\n// with long repeated sequences or compressing data that is similar to\n// other data, while still compressing fast, you might look at first\n// using BMDiff and then compressing the output of BMDiff with\n// Snappy.\n\n#ifndef THIRD_PARTY_SNAPPY_SNAPPY_H__\n#define THIRD_PARTY_SNAPPY_SNAPPY_H__\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <string>\n\n#include \"snappy-stubs-public.h\"\n\nnamespace snappy {\n  class Source;\n  class Sink;\n\n  struct CompressionOptions {\n    // Compression level.\n    // Level 1 is the fastest\n    // Level 2 is a little slower but provides better compression. Level 2 is\n    // **EXPERIMENTAL** for the time being. It might happen that we decide to\n    // fall back to level 1 in the future.\n    // Levels 3+ are currently not supported. We plan to support levels up to\n    // 9 in the future.\n    // If you played with other compression algorithms, level 1 is equivalent to\n    // fast mode (level 1) of LZ4, level 2 is equivalent to LZ4's level 2 mode\n    // and compresses somewhere around zstd:-3 and zstd:-2 but generally with\n    // faster decompression speeds than snappy:1 and zstd:-3.\n    int level = DefaultCompressionLevel();\n\n    constexpr CompressionOptions() = default;\n    constexpr CompressionOptions(int compression_level)\n        : level(compression_level) {}\n    static constexpr int MinCompressionLevel() { return 1; }\n    static constexpr int MaxCompressionLevel() { return 2; }\n    static constexpr int DefaultCompressionLevel() { return 1; }\n  };\n\n  // ------------------------------------------------------------------------\n  // Generic compression/decompression routines.\n  // ------------------------------------------------------------------------\n\n  // Compress the bytes read from \"*reader\" and append to \"*writer\". Return the\n  // number of bytes written.\n  // First version is to preserve ABI.\n  size_t Compress(Source* reader, Sink* writer);\n  size_t Compress(Source* reader, Sink* writer,\n                  CompressionOptions options);\n\n  // Find the uncompressed length of the given stream, as given by the header.\n  // Note that the true length could deviate from this; the stream could e.g.\n  // be truncated.\n  //\n  // Also note that this leaves \"*source\" in a state that is unsuitable for\n  // further operations, such as RawUncompress(). You will need to rewind\n  // or recreate the source yourself before attempting any further calls.\n  bool GetUncompressedLength(Source* source, uint32_t* result);\n\n  // ------------------------------------------------------------------------\n  // Higher-level string based routines (should be sufficient for most users)\n  // ------------------------------------------------------------------------\n\n  // Sets \"*compressed\" to the compressed version of \"input[0..input_length-1]\".\n  // Original contents of *compressed are lost.\n  //\n  // REQUIRES: \"input[]\" is not an alias of \"*compressed\".\n  // First version is to preserve ABI.\n  size_t Compress(const char* input, size_t input_length,\n                  std::string* compressed);\n  size_t Compress(const char* input, size_t input_length,\n                  std::string* compressed, CompressionOptions options);\n\n  // Same as `Compress` above but taking an `iovec` array as input. Note that\n  // this function preprocesses the inputs to compute the sum of\n  // `iov[0..iov_cnt-1].iov_len` before reading. To avoid this, use\n  // `RawCompressFromIOVec` below.\n  // First version is to preserve ABI.\n  size_t CompressFromIOVec(const struct iovec* iov, size_t iov_cnt,\n                           std::string* compressed);\n  size_t CompressFromIOVec(const struct iovec* iov, size_t iov_cnt,\n                           std::string* compressed,\n                           CompressionOptions options);\n\n  // Decompresses \"compressed[0..compressed_length-1]\" to \"*uncompressed\".\n  // Original contents of \"*uncompressed\" are lost.\n  //\n  // REQUIRES: \"compressed[]\" is not an alias of \"*uncompressed\".\n  //\n  // returns false if the message is corrupted and could not be decompressed\n  bool Uncompress(const char* compressed, size_t compressed_length,\n                  std::string* uncompressed);\n\n  // Decompresses \"compressed\" to \"*uncompressed\".\n  //\n  // returns false if the message is corrupted and could not be decompressed\n  bool Uncompress(Source* compressed, Sink* uncompressed);\n\n  // This routine uncompresses as much of the \"compressed\" as possible\n  // into sink.  It returns the number of valid bytes added to sink\n  // (extra invalid bytes may have been added due to errors; the caller\n  // should ignore those). The emitted data typically has length\n  // GetUncompressedLength(), but may be shorter if an error is\n  // encountered.\n  size_t UncompressAsMuchAsPossible(Source* compressed, Sink* uncompressed);\n\n  // ------------------------------------------------------------------------\n  // Lower-level character array based routines.  May be useful for\n  // efficiency reasons in certain circumstances.\n  // ------------------------------------------------------------------------\n\n  // REQUIRES: \"compressed\" must point to an area of memory that is at\n  // least \"MaxCompressedLength(input_length)\" bytes in length.\n  //\n  // Takes the data stored in \"input[0..input_length]\" and stores\n  // it in the array pointed to by \"compressed\".\n  //\n  // \"*compressed_length\" is set to the length of the compressed output.\n  //\n  // Example:\n  //    char* output = new char[snappy::MaxCompressedLength(input_length)];\n  //    size_t output_length;\n  //    RawCompress(input, input_length, output, &output_length);\n  //    ... Process(output, output_length) ...\n  //    delete [] output;\n  void RawCompress(const char* input, size_t input_length, char* compressed,\n                   size_t* compressed_length);\n  void RawCompress(const char* input, size_t input_length, char* compressed,\n                   size_t* compressed_length, CompressionOptions options);\n\n  // Same as `RawCompress` above but taking an `iovec` array as input. Note that\n  // `uncompressed_length` is the total number of bytes to be read from the\n  // elements of `iov` (_not_ the number of elements in `iov`).\n  void RawCompressFromIOVec(const struct iovec* iov, size_t uncompressed_length,\n                            char* compressed, size_t* compressed_length);\n  void RawCompressFromIOVec(const struct iovec* iov, size_t uncompressed_length,\n                            char* compressed, size_t* compressed_length,\n                            CompressionOptions options);\n\n  // Given data in \"compressed[0..compressed_length-1]\" generated by\n  // calling the Snappy::Compress routine, this routine\n  // stores the uncompressed data to\n  //    uncompressed[0..GetUncompressedLength(compressed)-1]\n  // returns false if the message is corrupted and could not be decrypted\n  bool RawUncompress(const char* compressed, size_t compressed_length,\n                     char* uncompressed);\n\n  // Given data from the byte source 'compressed' generated by calling\n  // the Snappy::Compress routine, this routine stores the uncompressed\n  // data to\n  //    uncompressed[0..GetUncompressedLength(compressed,compressed_length)-1]\n  // returns false if the message is corrupted and could not be decrypted\n  bool RawUncompress(Source* compressed, char* uncompressed);\n\n  // Given data in \"compressed[0..compressed_length-1]\" generated by\n  // calling the Snappy::Compress routine, this routine\n  // stores the uncompressed data to the iovec \"iov\". The number of physical\n  // buffers in \"iov\" is given by iov_cnt and their cumulative size\n  // must be at least GetUncompressedLength(compressed). The individual buffers\n  // in \"iov\" must not overlap with each other.\n  //\n  // returns false if the message is corrupted and could not be decrypted\n  bool RawUncompressToIOVec(const char* compressed, size_t compressed_length,\n                            const struct iovec* iov, size_t iov_cnt);\n\n  // Given data from the byte source 'compressed' generated by calling\n  // the Snappy::Compress routine, this routine stores the uncompressed\n  // data to the iovec \"iov\". The number of physical\n  // buffers in \"iov\" is given by iov_cnt and their cumulative size\n  // must be at least GetUncompressedLength(compressed). The individual buffers\n  // in \"iov\" must not overlap with each other.\n  //\n  // returns false if the message is corrupted and could not be decrypted\n  bool RawUncompressToIOVec(Source* compressed, const struct iovec* iov,\n                            size_t iov_cnt);\n\n  // Returns the maximal size of the compressed representation of\n  // input data that is \"source_bytes\" bytes in length;\n  size_t MaxCompressedLength(size_t source_bytes);\n\n  // REQUIRES: \"compressed[]\" was produced by RawCompress() or Compress()\n  // Returns true and stores the length of the uncompressed data in\n  // *result normally.  Returns false on parsing error.\n  // This operation takes O(1) time.\n  bool GetUncompressedLength(const char* compressed, size_t compressed_length,\n                             size_t* result);\n\n  // Returns true iff the contents of \"compressed[]\" can be uncompressed\n  // successfully.  Does not return the uncompressed data.  Takes\n  // time proportional to compressed_length, but is usually at least\n  // a factor of four faster than actual decompression.\n  bool IsValidCompressedBuffer(const char* compressed,\n                               size_t compressed_length);\n\n  // Returns true iff the contents of \"compressed\" can be uncompressed\n  // successfully.  Does not return the uncompressed data.  Takes\n  // time proportional to *compressed length, but is usually at least\n  // a factor of four faster than actual decompression.\n  // On success, consumes all of *compressed.  On failure, consumes an\n  // unspecified prefix of *compressed.\n  bool IsValidCompressed(Source* compressed);\n\n  // The size of a compression block. Note that many parts of the compression\n  // code assumes that kBlockSize <= 65536; in particular, the hash table\n  // can only store 16-bit offsets, and EmitCopy() also assumes the offset\n  // is 65535 bytes or less. Note also that if you change this, it will\n  // affect the framing format (see framing_format.txt).\n  //\n  // Note that there might be older data around that is compressed with larger\n  // block sizes, so the decompression code should not rely on the\n  // non-existence of long backreferences.\n  static constexpr int kBlockLog = 16;\n  static constexpr size_t kBlockSize = 1 << kBlockLog;\n\n  static constexpr int kMinHashTableBits = 8;\n  static constexpr size_t kMinHashTableSize = 1 << kMinHashTableBits;\n\n  static constexpr int kMaxHashTableBits = 15;\n  static constexpr size_t kMaxHashTableSize = 1 << kMaxHashTableBits;\n}  // end namespace snappy\n\n#endif  // THIRD_PARTY_SNAPPY_SNAPPY_H__\n"
        },
        {
          "name": "snappy_benchmark.cc",
          "type": "blob",
          "size": 13.6953125,
          "content": "// Copyright 2020 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#include <cstddef>\n#include <cstdint>\n#include <string>\n#include <vector>\n\n#include \"benchmark/benchmark.h\"\n#include \"snappy-internal.h\"\n#include \"snappy-sinksource.h\"\n#include \"snappy-test.h\"\n#include \"snappy.h\"\n#include \"snappy_test_data.h\"\n\nnamespace snappy {\n\nnamespace {\n\nvoid FilesAndLevels(benchmark::internal::Benchmark* benchmark) {\n  for (int i = 0; i < ARRAYSIZE(kTestDataFiles); ++i) {\n    for (int level = snappy::CompressionOptions::MinCompressionLevel();\n         level <= snappy::CompressionOptions::MaxCompressionLevel(); ++level) {\n      benchmark->ArgPair(i, level);\n    }\n  }\n}\n\nvoid BM_UFlat(benchmark::State& state) {\n  // Pick file to process based on state.range(0).\n  int file_index = state.range(0);\n\n  CHECK_GE(file_index, 0);\n  CHECK_LT(file_index, ARRAYSIZE(kTestDataFiles));\n  std::string contents =\n      ReadTestDataFile(kTestDataFiles[file_index].filename,\n                       kTestDataFiles[file_index].size_limit);\n\n  std::string zcontents;\n  snappy::Compress(\n      contents.data(), contents.size(), &zcontents,\n      snappy::CompressionOptions{/*level=*/static_cast<int>(state.range(1))});\n  char* dst = new char[contents.size()];\n\n  for (auto s : state) {\n    CHECK(snappy::RawUncompress(zcontents.data(), zcontents.size(), dst));\n    benchmark::DoNotOptimize(dst);\n  }\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          static_cast<int64_t>(contents.size()));\n  state.SetLabel(kTestDataFiles[file_index].label);\n\n  delete[] dst;\n}\nBENCHMARK(BM_UFlat)->Apply(FilesAndLevels);\n\nstruct SourceFiles {\n  SourceFiles() {\n    for (int i = 0; i < kFiles; i++) {\n      std::string contents = ReadTestDataFile(kTestDataFiles[i].filename,\n                                              kTestDataFiles[i].size_limit);\n      max_size = std::max(max_size, contents.size());\n      sizes[i] = contents.size();\n      snappy::Compress(contents.data(), contents.size(), &zcontents[i]);\n    }\n  }\n  static constexpr int kFiles = ARRAYSIZE(kTestDataFiles);\n  std::string zcontents[kFiles];\n  size_t sizes[kFiles];\n  size_t max_size = 0;\n};\n\nvoid BM_UFlatMedley(benchmark::State& state) {\n  static const SourceFiles* const source = new SourceFiles();\n\n  std::vector<char> dst(source->max_size);\n\n  for (auto s : state) {\n    for (int i = 0; i < SourceFiles::kFiles; i++) {\n      CHECK(snappy::RawUncompress(source->zcontents[i].data(),\n                                  source->zcontents[i].size(), dst.data()));\n      benchmark::DoNotOptimize(dst);\n    }\n  }\n\n  int64_t source_sizes = 0;\n  for (int i = 0; i < SourceFiles::kFiles; i++) {\n    source_sizes += static_cast<int64_t>(source->sizes[i]);\n  }\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          source_sizes);\n}\nBENCHMARK(BM_UFlatMedley);\n\nvoid BM_UValidate(benchmark::State& state) {\n  // Pick file to process based on state.range(0).\n  int file_index = state.range(0);\n\n  CHECK_GE(file_index, 0);\n  CHECK_LT(file_index, ARRAYSIZE(kTestDataFiles));\n  std::string contents =\n      ReadTestDataFile(kTestDataFiles[file_index].filename,\n                       kTestDataFiles[file_index].size_limit);\n\n  std::string zcontents;\n  snappy::Compress(\n      contents.data(), contents.size(), &zcontents,\n      snappy::CompressionOptions{/*level=*/static_cast<int>(state.range(1))});\n\n  for (auto s : state) {\n    CHECK(snappy::IsValidCompressedBuffer(zcontents.data(), zcontents.size()));\n  }\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          static_cast<int64_t>(contents.size()));\n  state.SetLabel(kTestDataFiles[file_index].label);\n}\nBENCHMARK(BM_UValidate)->Apply(FilesAndLevels);\n\nvoid BM_UValidateMedley(benchmark::State& state) {\n  static const SourceFiles* const source = new SourceFiles();\n\n  for (auto s : state) {\n    for (int i = 0; i < SourceFiles::kFiles; i++) {\n      CHECK(snappy::IsValidCompressedBuffer(source->zcontents[i].data(),\n                                            source->zcontents[i].size()));\n    }\n  }\n\n  int64_t source_sizes = 0;\n  for (int i = 0; i < SourceFiles::kFiles; i++) {\n    source_sizes += static_cast<int64_t>(source->sizes[i]);\n  }\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          source_sizes);\n}\nBENCHMARK(BM_UValidateMedley);\n\nvoid BM_UIOVecSource(benchmark::State& state) {\n  // Pick file to process based on state.range(0).\n  int file_index = state.range(0);\n  int level = state.range(1);\n\n  CHECK_GE(file_index, 0);\n  CHECK_LT(file_index, ARRAYSIZE(kTestDataFiles));\n  std::string contents =\n      ReadTestDataFile(kTestDataFiles[file_index].filename,\n                       kTestDataFiles[file_index].size_limit);\n\n  // Create `iovec`s of the `contents`.\n  const int kNumEntries = 10;\n  struct iovec iov[kNumEntries];\n  size_t used_so_far = 0;\n  for (int i = 0; i < kNumEntries; ++i) {\n    iov[i].iov_base = const_cast<char*>(contents.data()) + used_so_far;\n    if (used_so_far == contents.size()) {\n      iov[i].iov_len = 0;\n      continue;\n    }\n    if (i == kNumEntries - 1) {\n      iov[i].iov_len = contents.size() - used_so_far;\n    } else {\n      iov[i].iov_len = contents.size() / kNumEntries;\n    }\n    used_so_far += iov[i].iov_len;\n  }\n\n  char* dst = new char[snappy::MaxCompressedLength(contents.size())];\n  size_t zsize = 0;\n  for (auto s : state) {\n    snappy::RawCompressFromIOVec(iov, contents.size(), dst, &zsize,\n                                 snappy::CompressionOptions{/*level=*/level});\n    benchmark::DoNotOptimize(iov);\n  }\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          static_cast<int64_t>(contents.size()));\n  const double compression_ratio =\n      static_cast<double>(zsize) / std::max<size_t>(1, contents.size());\n  state.SetLabel(StrFormat(\"%s (%.2f %%)\", kTestDataFiles[file_index].label,\n                           100.0 * compression_ratio));\n  VLOG(0) << StrFormat(\"compression for %s: %d -> %d bytes\",\n                       kTestDataFiles[file_index].label, contents.size(),\n                       zsize);\n\n  delete[] dst;\n}\nBENCHMARK(BM_UIOVecSource)->Apply(FilesAndLevels);\n\nvoid BM_UIOVecSink(benchmark::State& state) {\n  // Pick file to process based on state.range(0).\n  int file_index = state.range(0);\n\n  CHECK_GE(file_index, 0);\n  CHECK_LT(file_index, ARRAYSIZE(kTestDataFiles));\n  std::string contents =\n      ReadTestDataFile(kTestDataFiles[file_index].filename,\n                       kTestDataFiles[file_index].size_limit);\n\n  std::string zcontents;\n  snappy::Compress(contents.data(), contents.size(), &zcontents);\n\n  // Uncompress into an iovec containing ten entries.\n  const int kNumEntries = 10;\n  struct iovec iov[kNumEntries];\n  char* dst = new char[contents.size()];\n  size_t used_so_far = 0;\n  for (int i = 0; i < kNumEntries; ++i) {\n    iov[i].iov_base = dst + used_so_far;\n    if (used_so_far == contents.size()) {\n      iov[i].iov_len = 0;\n      continue;\n    }\n\n    if (i == kNumEntries - 1) {\n      iov[i].iov_len = contents.size() - used_so_far;\n    } else {\n      iov[i].iov_len = contents.size() / kNumEntries;\n    }\n    used_so_far += iov[i].iov_len;\n  }\n\n  for (auto s : state) {\n    CHECK(snappy::RawUncompressToIOVec(zcontents.data(), zcontents.size(), iov,\n                                       kNumEntries));\n    benchmark::DoNotOptimize(iov);\n  }\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          static_cast<int64_t>(contents.size()));\n  state.SetLabel(kTestDataFiles[file_index].label);\n\n  delete[] dst;\n}\nBENCHMARK(BM_UIOVecSink)->DenseRange(0, 4);\n\nvoid BM_UFlatSink(benchmark::State& state) {\n  // Pick file to process based on state.range(0).\n  int file_index = state.range(0);\n\n  CHECK_GE(file_index, 0);\n  CHECK_LT(file_index, ARRAYSIZE(kTestDataFiles));\n  std::string contents =\n      ReadTestDataFile(kTestDataFiles[file_index].filename,\n                       kTestDataFiles[file_index].size_limit);\n\n  std::string zcontents;\n  snappy::Compress(\n      contents.data(), contents.size(), &zcontents,\n      snappy::CompressionOptions{/*level=*/static_cast<int>(state.range(1))});\n  char* dst = new char[contents.size()];\n\n  for (auto s : state) {\n    snappy::ByteArraySource source(zcontents.data(), zcontents.size());\n    snappy::UncheckedByteArraySink sink(dst);\n    CHECK(snappy::Uncompress(&source, &sink));\n    benchmark::DoNotOptimize(sink);\n  }\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          static_cast<int64_t>(contents.size()));\n  state.SetLabel(kTestDataFiles[file_index].label);\n\n  std::string s(dst, contents.size());\n  CHECK_EQ(contents, s);\n\n  delete[] dst;\n}\n\nBENCHMARK(BM_UFlatSink)->Apply(FilesAndLevels);\n\nvoid BM_ZFlat(benchmark::State& state) {\n  // Pick file to process based on state.range(0).\n  int file_index = state.range(0);\n  int level = state.range(1);\n\n  CHECK_GE(file_index, 0);\n  CHECK_LT(file_index, ARRAYSIZE(kTestDataFiles));\n  std::string contents =\n      ReadTestDataFile(kTestDataFiles[file_index].filename,\n                       kTestDataFiles[file_index].size_limit);\n  char* dst = new char[snappy::MaxCompressedLength(contents.size())];\n\n  size_t zsize = 0;\n  for (auto s : state) {\n    snappy::RawCompress(contents.data(), contents.size(), dst, &zsize,\n                        snappy::CompressionOptions{/*level=*/level});\n    benchmark::DoNotOptimize(dst);\n  }\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          static_cast<int64_t>(contents.size()));\n  const double compression_ratio =\n      static_cast<double>(zsize) / std::max<size_t>(1, contents.size());\n  state.SetLabel(StrFormat(\"%s (%.2f %%)\", kTestDataFiles[file_index].label,\n                           100.0 * compression_ratio));\n  VLOG(0) << StrFormat(\"compression for %s: %d -> %d bytes\",\n                       kTestDataFiles[file_index].label, contents.size(),\n                       zsize);\n  delete[] dst;\n}\n\nBENCHMARK(BM_ZFlat)->Apply(FilesAndLevels);\n\nvoid BM_ZFlatAll(benchmark::State& state) {\n  const int num_files = ARRAYSIZE(kTestDataFiles);\n  int level = state.range(0);\n\n  std::vector<std::string> contents(num_files);\n  std::vector<char*> dst(num_files);\n\n  int64_t total_contents_size = 0;\n  for (int i = 0; i < num_files; ++i) {\n    contents[i] = ReadTestDataFile(kTestDataFiles[i].filename,\n                                   kTestDataFiles[i].size_limit);\n    dst[i] = new char[snappy::MaxCompressedLength(contents[i].size())];\n    total_contents_size += contents[i].size();\n  }\n\n  size_t zsize = 0;\n  for (auto s : state) {\n    for (int i = 0; i < num_files; ++i) {\n      snappy::RawCompress(contents[i].data(), contents[i].size(), dst[i],\n                          &zsize, snappy::CompressionOptions{/*level=*/level});\n      benchmark::DoNotOptimize(dst);\n    }\n  }\n\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          total_contents_size);\n\n  for (char* dst_item : dst) {\n    delete[] dst_item;\n  }\n  state.SetLabel(StrFormat(\"%d kTestDataFiles\", num_files));\n}\nBENCHMARK(BM_ZFlatAll)->DenseRange(1, 2);\n\nvoid BM_ZFlatIncreasingTableSize(benchmark::State& state) {\n  CHECK_GT(ARRAYSIZE(kTestDataFiles), 0);\n  int level = state.range(0);\n  const std::string base_content = ReadTestDataFile(\n      kTestDataFiles[0].filename, kTestDataFiles[0].size_limit);\n\n  std::vector<std::string> contents;\n  std::vector<char*> dst;\n  int64_t total_contents_size = 0;\n  for (int table_bits = kMinHashTableBits; table_bits <= kMaxHashTableBits;\n       ++table_bits) {\n    std::string content = base_content;\n    content.resize(1 << table_bits);\n    dst.push_back(new char[snappy::MaxCompressedLength(content.size())]);\n    total_contents_size += content.size();\n    contents.push_back(std::move(content));\n  }\n\n  size_t zsize = 0;\n  for (auto s : state) {\n    for (size_t i = 0; i < contents.size(); ++i) {\n      snappy::RawCompress(contents[i].data(), contents[i].size(), dst[i],\n                          &zsize, snappy::CompressionOptions{/*level=*/level});\n      benchmark::DoNotOptimize(dst);\n    }\n  }\n\n  state.SetBytesProcessed(static_cast<int64_t>(state.iterations()) *\n                          total_contents_size);\n\n  for (char* dst_item : dst) {\n    delete[] dst_item;\n  }\n  state.SetLabel(StrFormat(\"%d tables\", contents.size()));\n}\nBENCHMARK(BM_ZFlatIncreasingTableSize)->DenseRange(1, 2);\n\n}  // namespace\n\n}  // namespace snappy\n"
        },
        {
          "name": "snappy_compress_fuzzer.cc",
          "type": "blob",
          "size": 2.79296875,
          "content": "// Copyright 2019 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// libFuzzer harness for fuzzing snappy compression code.\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <cassert>\n#include <string>\n\n#include \"snappy.h\"\n\n// Entry point for LibFuzzer.\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {\n  std::string input(reinterpret_cast<const char*>(data), size);\n  for (int level = snappy::CompressionOptions::MinCompressionLevel();\n       level <= snappy::CompressionOptions::MaxCompressionLevel(); ++level) {\n    std::string compressed;\n    size_t compressed_size =\n        snappy::Compress(input.data(), input.size(), &compressed,\n                         snappy::CompressionOptions{/*level=*/level});\n\n    (void)compressed_size;  // Variable only used in debug builds.\n    assert(compressed_size == compressed.size());\n    assert(compressed.size() <= snappy::MaxCompressedLength(input.size()));\n    assert(\n        snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n\n    std::string uncompressed_after_compress;\n    bool uncompress_succeeded = snappy::Uncompress(\n        compressed.data(), compressed.size(), &uncompressed_after_compress);\n\n    (void)uncompress_succeeded;  // Variable only used in debug builds.\n    assert(uncompress_succeeded);\n    assert(input == uncompressed_after_compress);\n  }\n  return 0;\n}\n"
        },
        {
          "name": "snappy_test_data.cc",
          "type": "blob",
          "size": 2.1767578125,
          "content": "// Copyright 2020 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// Support code for reading test data.\n\n#include \"snappy_test_data.h\"\n\n#include <cstddef>\n#include <cstdlib>\n#include <string>\n\n#include \"snappy-test.h\"\n\nnamespace snappy {\n\nstd::string ReadTestDataFile(const char* base, size_t size_limit) {\n  std::string srcdir;\n  const char* srcdir_env = std::getenv(\"srcdir\");  // This is set by Automake.\n  if (srcdir_env) {\n    srcdir = std::string(srcdir_env) + \"/\";\n  }\n\n  std::string contents;\n  CHECK_OK(file::GetContents(srcdir + \"testdata/\" + base, &contents,\n                             file::Defaults()));\n  if (size_limit > 0) {\n    contents = contents.substr(0, size_limit);\n  }\n  return contents;\n}\n\n}  // namespace snappy\n"
        },
        {
          "name": "snappy_test_data.h",
          "type": "blob",
          "size": 2.453125,
          "content": "// Copyright 2020 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// List of test case files.\n\n#ifndef THIRD_PARTY_SNAPPY_SNAPPY_TEST_DATA_H__\n#define THIRD_PARTY_SNAPPY_SNAPPY_TEST_DATA_H__\n\n#include <cstddef>\n#include <string>\n\nnamespace snappy {\n\nstd::string ReadTestDataFile(const char* base, size_t size_limit);\n\n// TODO: Replace anonymous namespace with inline variable when we can\n//               rely on C++17.\nnamespace {\n\nconstexpr struct {\n  const char* label;\n  const char* filename;\n  size_t size_limit;\n} kTestDataFiles[] = {\n  { \"html\", \"html\", 0 },\n  { \"urls\", \"urls.10K\", 0 },\n  { \"jpg\", \"fireworks.jpeg\", 0 },\n  { \"jpg_200\", \"fireworks.jpeg\", 200 },\n  { \"pdf\", \"paper-100k.pdf\", 0 },\n  { \"html4\", \"html_x_4\", 0 },\n  { \"txt1\", \"alice29.txt\", 0 },\n  { \"txt2\", \"asyoulik.txt\", 0 },\n  { \"txt3\", \"lcet10.txt\", 0 },\n  { \"txt4\", \"plrabn12.txt\", 0 },\n  { \"pb\", \"geo.protodata\", 0 },\n  { \"gaviota\", \"kppkn.gtb\", 0 },\n};\n\n}  // namespace\n\n}  // namespace snappy\n\n#endif  // THIRD_PARTY_SNAPPY_SNAPPY_TEST_DATA_H__\n"
        },
        {
          "name": "snappy_test_tool.cc",
          "type": "blob",
          "size": 15.4091796875,
          "content": "// Copyright 2020 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#include <algorithm>\n#include <cmath>\n#include <cstdint>\n#include <cstdlib>\n#include <random>\n#include <string>\n#include <utility>\n#include <vector>\n\n#include \"snappy-test.h\"\n\n#include \"snappy-internal.h\"\n#include \"snappy-sinksource.h\"\n#include \"snappy.h\"\n#include \"snappy_test_data.h\"\n\nSNAPPY_FLAG(int32_t, start_len, -1,\n            \"Starting prefix size for testing (-1: just full file contents)\");\nSNAPPY_FLAG(int32_t, end_len, -1,\n            \"Starting prefix size for testing (-1: just full file contents)\");\nSNAPPY_FLAG(int32_t, bytes, 10485760,\n            \"How many bytes to compress/uncompress per file for timing\");\n\nSNAPPY_FLAG(bool, zlib, true,\n            \"Run zlib compression (http://www.zlib.net)\");\nSNAPPY_FLAG(bool, lzo, true,\n            \"Run LZO compression (http://www.oberhumer.com/opensource/lzo/)\");\nSNAPPY_FLAG(bool, lz4, true,\n            \"Run LZ4 compression (https://github.com/lz4/lz4)\");\nSNAPPY_FLAG(bool, snappy, true, \"Run snappy compression\");\n\nSNAPPY_FLAG(bool, write_compressed, false,\n            \"Write compressed versions of each file to <file>.comp\");\nSNAPPY_FLAG(bool, write_uncompressed, false,\n            \"Write uncompressed versions of each file to <file>.uncomp\");\n\nnamespace snappy {\n\nnamespace {\n\n#if HAVE_FUNC_MMAP && HAVE_FUNC_SYSCONF\n\n// To test against code that reads beyond its input, this class copies a\n// string to a newly allocated group of pages, the last of which\n// is made unreadable via mprotect. Note that we need to allocate the\n// memory with mmap(), as POSIX allows mprotect() only on memory allocated\n// with mmap(), and some malloc/posix_memalign implementations expect to\n// be able to read previously allocated memory while doing heap allocations.\nclass DataEndingAtUnreadablePage {\n public:\n  explicit DataEndingAtUnreadablePage(const std::string& s) {\n    const size_t page_size = sysconf(_SC_PAGESIZE);\n    const size_t size = s.size();\n    // Round up space for string to a multiple of page_size.\n    size_t space_for_string = (size + page_size - 1) & ~(page_size - 1);\n    alloc_size_ = space_for_string + page_size;\n    mem_ = mmap(NULL, alloc_size_,\n                PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n    CHECK_NE(MAP_FAILED, mem_);\n    protected_page_ = reinterpret_cast<char*>(mem_) + space_for_string;\n    char* dst = protected_page_ - size;\n    std::memcpy(dst, s.data(), size);\n    data_ = dst;\n    size_ = size;\n    // Make guard page unreadable.\n    CHECK_EQ(0, mprotect(protected_page_, page_size, PROT_NONE));\n  }\n\n  ~DataEndingAtUnreadablePage() {\n    const size_t page_size = sysconf(_SC_PAGESIZE);\n    // Undo the mprotect.\n    CHECK_EQ(0, mprotect(protected_page_, page_size, PROT_READ|PROT_WRITE));\n    CHECK_EQ(0, munmap(mem_, alloc_size_));\n  }\n\n  const char* data() const { return data_; }\n  size_t size() const { return size_; }\n\n private:\n  size_t alloc_size_;\n  void* mem_;\n  char* protected_page_;\n  const char* data_;\n  size_t size_;\n};\n\n#else  // HAVE_FUNC_MMAP && HAVE_FUNC_SYSCONF\n\n// Fallback for systems without mmap.\nusing DataEndingAtUnreadablePage = std::string;\n\n#endif\n\nenum CompressorType { ZLIB, LZO, LZ4, SNAPPY };\n\nconst char* names[] = {\"ZLIB\", \"LZO\", \"LZ4\", \"SNAPPY\"};\n\nsize_t MinimumRequiredOutputSpace(size_t input_size, CompressorType comp) {\n  switch (comp) {\n#ifdef ZLIB_VERSION\n    case ZLIB:\n      return ZLib::MinCompressbufSize(input_size);\n#endif  // ZLIB_VERSION\n\n#ifdef LZO_VERSION\n    case LZO:\n      return input_size + input_size/64 + 16 + 3;\n#endif  // LZO_VERSION\n\n#ifdef LZ4_VERSION_NUMBER\n    case LZ4:\n      return LZ4_compressBound(input_size);\n#endif  // LZ4_VERSION_NUMBER\n\n    case SNAPPY:\n      return snappy::MaxCompressedLength(input_size);\n\n    default:\n      LOG(FATAL) << \"Unknown compression type number \" << comp;\n      return 0;\n  }\n}\n\n// Returns true if we successfully compressed, false otherwise.\n//\n// If compressed_is_preallocated is set, do not resize the compressed buffer.\n// This is typically what you want for a benchmark, in order to not spend\n// time in the memory allocator. If you do set this flag, however,\n// \"compressed\" must be preinitialized to at least MinCompressbufSize(comp)\n// number of bytes, and may contain junk bytes at the end after return.\nbool Compress(const char* input, size_t input_size, CompressorType comp,\n              std::string* compressed, bool compressed_is_preallocated) {\n  if (!compressed_is_preallocated) {\n    compressed->resize(MinimumRequiredOutputSpace(input_size, comp));\n  }\n\n  switch (comp) {\n#ifdef ZLIB_VERSION\n    case ZLIB: {\n      ZLib zlib;\n      uLongf destlen = compressed->size();\n      int ret = zlib.Compress(\n          reinterpret_cast<Bytef*>(string_as_array(compressed)),\n          &destlen,\n          reinterpret_cast<const Bytef*>(input),\n          input_size);\n      CHECK_EQ(Z_OK, ret);\n      if (!compressed_is_preallocated) {\n        compressed->resize(destlen);\n      }\n      return true;\n    }\n#endif  // ZLIB_VERSION\n\n#ifdef LZO_VERSION\n    case LZO: {\n      unsigned char* mem = new unsigned char[LZO1X_1_15_MEM_COMPRESS];\n      lzo_uint destlen;\n      int ret = lzo1x_1_15_compress(\n          reinterpret_cast<const uint8_t*>(input),\n          input_size,\n          reinterpret_cast<uint8_t*>(string_as_array(compressed)),\n          &destlen,\n          mem);\n      CHECK_EQ(LZO_E_OK, ret);\n      delete[] mem;\n      if (!compressed_is_preallocated) {\n        compressed->resize(destlen);\n      }\n      break;\n    }\n#endif  // LZO_VERSION\n\n#ifdef LZ4_VERSION_NUMBER\n    case LZ4: {\n      int destlen = compressed->size();\n      destlen = LZ4_compress_default(input, string_as_array(compressed),\n                                     input_size, destlen);\n      CHECK_NE(destlen, 0);\n      if (!compressed_is_preallocated) {\n        compressed->resize(destlen);\n      }\n      break;\n    }\n#endif  // LZ4_VERSION_NUMBER\n\n    case SNAPPY: {\n      size_t destlen;\n      snappy::RawCompress(input, input_size,\n                          string_as_array(compressed),\n                          &destlen);\n      CHECK_LE(destlen, snappy::MaxCompressedLength(input_size));\n      if (!compressed_is_preallocated) {\n        compressed->resize(destlen);\n      }\n      break;\n    }\n\n    default: {\n      return false;     // the asked-for library wasn't compiled in\n    }\n  }\n  return true;\n}\n\nbool Uncompress(const std::string& compressed, CompressorType comp, int size,\n                std::string* output) {\n  // TODO: Switch to [[maybe_unused]] when we can assume C++17.\n  (void)size;\n  switch (comp) {\n#ifdef ZLIB_VERSION\n    case ZLIB: {\n      output->resize(size);\n      ZLib zlib;\n      uLongf destlen = output->size();\n      int ret = zlib.Uncompress(\n          reinterpret_cast<Bytef*>(string_as_array(output)),\n          &destlen,\n          reinterpret_cast<const Bytef*>(compressed.data()),\n          compressed.size());\n      CHECK_EQ(Z_OK, ret);\n      CHECK_EQ(static_cast<uLongf>(size), destlen);\n      break;\n    }\n#endif  // ZLIB_VERSION\n\n#ifdef LZO_VERSION\n    case LZO: {\n      output->resize(size);\n      lzo_uint destlen;\n      int ret = lzo1x_decompress(\n          reinterpret_cast<const uint8_t*>(compressed.data()),\n          compressed.size(),\n          reinterpret_cast<uint8_t*>(string_as_array(output)),\n          &destlen,\n          NULL);\n      CHECK_EQ(LZO_E_OK, ret);\n      CHECK_EQ(static_cast<lzo_uint>(size), destlen);\n      break;\n    }\n#endif  // LZO_VERSION\n\n#ifdef LZ4_VERSION_NUMBER\n    case LZ4: {\n      output->resize(size);\n      int destlen = output->size();\n      destlen = LZ4_decompress_safe(compressed.data(), string_as_array(output),\n                                    compressed.size(), destlen);\n      CHECK_NE(destlen, 0);\n      CHECK_EQ(size, destlen);\n      break;\n    }\n#endif  // LZ4_VERSION_NUMBER\n    case SNAPPY: {\n      snappy::RawUncompress(compressed.data(), compressed.size(),\n                            string_as_array(output));\n      break;\n    }\n\n    default: {\n      return false;     // the asked-for library wasn't compiled in\n    }\n  }\n  return true;\n}\n\nvoid Measure(const char* data, size_t length, CompressorType comp, int repeats,\n             int block_size) {\n  // Run tests a few time and pick median running times\n  static const int kRuns = 5;\n  double ctime[kRuns];\n  double utime[kRuns];\n  int compressed_size = 0;\n\n  {\n    // Chop the input into blocks\n    int num_blocks = (length + block_size - 1) / block_size;\n    std::vector<const char*> input(num_blocks);\n    std::vector<size_t> input_length(num_blocks);\n    std::vector<std::string> compressed(num_blocks);\n    std::vector<std::string> output(num_blocks);\n    for (int b = 0; b < num_blocks; ++b) {\n      int input_start = b * block_size;\n      int input_limit = std::min<int>((b+1)*block_size, length);\n      input[b] = data+input_start;\n      input_length[b] = input_limit-input_start;\n    }\n\n    // Pre-grow the output buffers so we don't measure string append time.\n    for (std::string& compressed_block : compressed) {\n      compressed_block.resize(MinimumRequiredOutputSpace(block_size, comp));\n    }\n\n    // First, try one trial compression to make sure the code is compiled in\n    if (!Compress(input[0], input_length[0], comp, &compressed[0], true)) {\n      LOG(WARNING) << \"Skipping \" << names[comp] << \": \"\n                   << \"library not compiled in\";\n      return;\n    }\n\n    for (int run = 0; run < kRuns; ++run) {\n      CycleTimer ctimer, utimer;\n\n      // Pre-grow the output buffers so we don't measure string append time.\n      for (std::string& compressed_block : compressed) {\n        compressed_block.resize(MinimumRequiredOutputSpace(block_size, comp));\n      }\n\n      ctimer.Start();\n      for (int b = 0; b < num_blocks; ++b) {\n        for (int i = 0; i < repeats; ++i)\n          Compress(input[b], input_length[b], comp, &compressed[b], true);\n      }\n      ctimer.Stop();\n\n      // Compress once more, with resizing, so we don't leave junk\n      // at the end that will confuse the decompressor.\n      for (int b = 0; b < num_blocks; ++b) {\n        Compress(input[b], input_length[b], comp, &compressed[b], false);\n      }\n\n      for (int b = 0; b < num_blocks; ++b) {\n        output[b].resize(input_length[b]);\n      }\n\n      utimer.Start();\n      for (int i = 0; i < repeats; ++i) {\n        for (int b = 0; b < num_blocks; ++b)\n          Uncompress(compressed[b], comp, input_length[b], &output[b]);\n      }\n      utimer.Stop();\n\n      ctime[run] = ctimer.Get();\n      utime[run] = utimer.Get();\n    }\n\n    compressed_size = 0;\n    for (const std::string& compressed_item : compressed) {\n      compressed_size += compressed_item.size();\n    }\n  }\n\n  std::sort(ctime, ctime + kRuns);\n  std::sort(utime, utime + kRuns);\n  const int med = kRuns/2;\n\n  float comp_rate = (length / ctime[med]) * repeats / 1048576.0;\n  float uncomp_rate = (length / utime[med]) * repeats / 1048576.0;\n  std::string x = names[comp];\n  x += \":\";\n  std::string urate = (uncomp_rate >= 0) ? StrFormat(\"%.1f\", uncomp_rate)\n                                         : std::string(\"?\");\n  std::printf(\"%-7s [b %dM] bytes %6d -> %6d %4.1f%%  \"\n              \"comp %5.1f MB/s  uncomp %5s MB/s\\n\",\n              x.c_str(),\n              block_size/(1<<20),\n              static_cast<int>(length), static_cast<uint32_t>(compressed_size),\n              (compressed_size * 100.0) / std::max<int>(1, length),\n              comp_rate,\n              urate.c_str());\n}\n\nvoid CompressFile(const char* fname) {\n  std::string fullinput;\n  CHECK_OK(file::GetContents(fname, &fullinput, file::Defaults()));\n\n  std::string compressed;\n  Compress(fullinput.data(), fullinput.size(), SNAPPY, &compressed, false);\n\n  CHECK_OK(file::SetContents(std::string(fname).append(\".comp\"), compressed,\n                             file::Defaults()));\n}\n\nvoid UncompressFile(const char* fname) {\n  std::string fullinput;\n  CHECK_OK(file::GetContents(fname, &fullinput, file::Defaults()));\n\n  size_t uncompLength;\n  CHECK(snappy::GetUncompressedLength(fullinput.data(), fullinput.size(),\n                                      &uncompLength));\n\n  std::string uncompressed;\n  uncompressed.resize(uncompLength);\n  CHECK(snappy::Uncompress(fullinput.data(), fullinput.size(), &uncompressed));\n\n  CHECK_OK(file::SetContents(std::string(fname).append(\".uncomp\"), uncompressed,\n                             file::Defaults()));\n}\n\nvoid MeasureFile(const char* fname) {\n  std::string fullinput;\n  CHECK_OK(file::GetContents(fname, &fullinput, file::Defaults()));\n  std::printf(\"%-40s :\\n\", fname);\n\n  int start_len = (snappy::GetFlag(FLAGS_start_len) < 0)\n                      ? fullinput.size()\n                      : snappy::GetFlag(FLAGS_start_len);\n  int end_len = fullinput.size();\n  if (snappy::GetFlag(FLAGS_end_len) >= 0) {\n    end_len = std::min<int>(fullinput.size(), snappy::GetFlag(FLAGS_end_len));\n  }\n  for (int len = start_len; len <= end_len; ++len) {\n    const char* const input = fullinput.data();\n    int repeats = (snappy::GetFlag(FLAGS_bytes) + len) / (len + 1);\n    if (snappy::GetFlag(FLAGS_zlib))\n      Measure(input, len, ZLIB, repeats, 1024 << 10);\n    if (snappy::GetFlag(FLAGS_lzo))\n      Measure(input, len, LZO, repeats, 1024 << 10);\n    if (snappy::GetFlag(FLAGS_lz4))\n      Measure(input, len, LZ4, repeats, 1024 << 10);\n    if (snappy::GetFlag(FLAGS_snappy))\n      Measure(input, len, SNAPPY, repeats, 4096 << 10);\n\n    // For block-size based measurements\n    if (0 && snappy::GetFlag(FLAGS_snappy)) {\n      Measure(input, len, SNAPPY, repeats, 8<<10);\n      Measure(input, len, SNAPPY, repeats, 16<<10);\n      Measure(input, len, SNAPPY, repeats, 32<<10);\n      Measure(input, len, SNAPPY, repeats, 64<<10);\n      Measure(input, len, SNAPPY, repeats, 256<<10);\n      Measure(input, len, SNAPPY, repeats, 1024<<10);\n    }\n  }\n}\n\n}  // namespace\n\n}  // namespace snappy\n\nint main(int argc, char** argv) {\n  InitGoogle(argv[0], &argc, &argv, true);\n\n  for (int arg = 1; arg < argc; ++arg) {\n    if (snappy::GetFlag(FLAGS_write_compressed)) {\n      snappy::CompressFile(argv[arg]);\n    } else if (snappy::GetFlag(FLAGS_write_uncompressed)) {\n      snappy::UncompressFile(argv[arg]);\n    } else {\n      snappy::MeasureFile(argv[arg]);\n    }\n  }\n  return 0;\n}\n"
        },
        {
          "name": "snappy_uncompress_fuzzer.cc",
          "type": "blob",
          "size": 2.4169921875,
          "content": "// Copyright 2019 Google Inc. All Rights Reserved.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n//\n// libFuzzer harness for fuzzing snappy's decompression code.\n\n#include <stddef.h>\n#include <stdint.h>\n\n#include <cassert>\n#include <string>\n\n#include \"snappy.h\"\n\n// Entry point for LibFuzzer.\nextern \"C\" int LLVMFuzzerTestOneInput(const uint8_t* data, size_t size) {\n  std::string input(reinterpret_cast<const char*>(data), size);\n\n  // Avoid self-crafted decompression bombs.\n  size_t uncompressed_size;\n  constexpr size_t kMaxUncompressedSize = 1 << 20;\n  bool get_uncompressed_length_succeeded = snappy::GetUncompressedLength(\n      input.data(), input.size(), &uncompressed_size);\n  if (!get_uncompressed_length_succeeded ||\n      (uncompressed_size > kMaxUncompressedSize)) {\n    return 0;\n  }\n\n  std::string uncompressed;\n  // The return value of snappy::Uncompress() is ignored because decompression\n  // will fail on invalid inputs.\n  snappy::Uncompress(input.data(), input.size(), &uncompressed);\n  return 0;\n}\n"
        },
        {
          "name": "snappy_unittest.cc",
          "type": "blob",
          "size": 35.4208984375,
          "content": "// Copyright 2005 and onwards Google Inc.\n//\n// Redistribution and use in source and binary forms, with or without\n// modification, are permitted provided that the following conditions are\n// met:\n//\n//     * Redistributions of source code must retain the above copyright\n// notice, this list of conditions and the following disclaimer.\n//     * Redistributions in binary form must reproduce the above\n// copyright notice, this list of conditions and the following disclaimer\n// in the documentation and/or other materials provided with the\n// distribution.\n//     * Neither the name of Google Inc. nor the names of its\n// contributors may be used to endorse or promote products derived from\n// this software without specific prior written permission.\n//\n// THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n// \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n// LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n// A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n// OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n// SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n// LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n// DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n// THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n// (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n// OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n#include <algorithm>\n#include <cmath>\n#include <cstdlib>\n#include <random>\n#include <string>\n#include <utility>\n#include <vector>\n\n#include \"snappy-test.h\"\n\n#include \"gtest/gtest.h\"\n\n#include \"snappy-internal.h\"\n#include \"snappy-sinksource.h\"\n#include \"snappy.h\"\n#include \"snappy_test_data.h\"\n\nSNAPPY_FLAG(bool, snappy_dump_decompression_table, false,\n            \"If true, we print the decompression table during tests.\");\n\nnamespace snappy {\n\nnamespace {\n\n#if HAVE_FUNC_MMAP && HAVE_FUNC_SYSCONF\n\n// To test against code that reads beyond its input, this class copies a\n// string to a newly allocated group of pages, the last of which\n// is made unreadable via mprotect. Note that we need to allocate the\n// memory with mmap(), as POSIX allows mprotect() only on memory allocated\n// with mmap(), and some malloc/posix_memalign implementations expect to\n// be able to read previously allocated memory while doing heap allocations.\nclass DataEndingAtUnreadablePage {\n public:\n  explicit DataEndingAtUnreadablePage(const std::string& s) {\n    const size_t page_size = sysconf(_SC_PAGESIZE);\n    const size_t size = s.size();\n    // Round up space for string to a multiple of page_size.\n    size_t space_for_string = (size + page_size - 1) & ~(page_size - 1);\n    alloc_size_ = space_for_string + page_size;\n    mem_ = mmap(NULL, alloc_size_,\n                PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0);\n    CHECK_NE(MAP_FAILED, mem_);\n    protected_page_ = reinterpret_cast<char*>(mem_) + space_for_string;\n    char* dst = protected_page_ - size;\n    std::memcpy(dst, s.data(), size);\n    data_ = dst;\n    size_ = size;\n    // Make guard page unreadable.\n    CHECK_EQ(0, mprotect(protected_page_, page_size, PROT_NONE));\n  }\n\n  ~DataEndingAtUnreadablePage() {\n    const size_t page_size = sysconf(_SC_PAGESIZE);\n    // Undo the mprotect.\n    CHECK_EQ(0, mprotect(protected_page_, page_size, PROT_READ|PROT_WRITE));\n    CHECK_EQ(0, munmap(mem_, alloc_size_));\n  }\n\n  const char* data() const { return data_; }\n  size_t size() const { return size_; }\n\n private:\n  size_t alloc_size_;\n  void* mem_;\n  char* protected_page_;\n  const char* data_;\n  size_t size_;\n};\n\n#else  // HAVE_FUNC_MMAP) && HAVE_FUNC_SYSCONF\n\n// Fallback for systems without mmap.\nusing DataEndingAtUnreadablePage = std::string;\n\n#endif\n\nint VerifyString(const std::string& input) {\n  std::string compressed;\n  DataEndingAtUnreadablePage i(input);\n  const size_t written = snappy::Compress(i.data(), i.size(), &compressed);\n  CHECK_EQ(written, compressed.size());\n  CHECK_LE(compressed.size(),\n           snappy::MaxCompressedLength(input.size()));\n  CHECK(snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n\n  std::string uncompressed;\n  DataEndingAtUnreadablePage c(compressed);\n  CHECK(snappy::Uncompress(c.data(), c.size(), &uncompressed));\n  CHECK_EQ(uncompressed, input);\n  return uncompressed.size();\n}\n\nvoid VerifyStringSink(const std::string& input) {\n  std::string compressed;\n  DataEndingAtUnreadablePage i(input);\n  const size_t written = snappy::Compress(i.data(), i.size(), &compressed);\n  CHECK_EQ(written, compressed.size());\n  CHECK_LE(compressed.size(),\n           snappy::MaxCompressedLength(input.size()));\n  CHECK(snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n\n  std::string uncompressed;\n  uncompressed.resize(input.size());\n  snappy::UncheckedByteArraySink sink(string_as_array(&uncompressed));\n  DataEndingAtUnreadablePage c(compressed);\n  snappy::ByteArraySource source(c.data(), c.size());\n  CHECK(snappy::Uncompress(&source, &sink));\n  CHECK_EQ(uncompressed, input);\n}\n\nstruct iovec* GetIOVec(const std::string& input, char*& buf, size_t& num) {\n  std::minstd_rand0 rng(input.size());\n  std::uniform_int_distribution<size_t> uniform_1_to_10(1, 10);\n  num = uniform_1_to_10(rng);\n  if (input.size() < num) {\n    num = input.size();\n  }\n  struct iovec* iov = new iovec[num];\n  size_t used_so_far = 0;\n  std::bernoulli_distribution one_in_five(1.0 / 5);\n  for (size_t i = 0; i < num; ++i) {\n    assert(used_so_far < input.size());\n    iov[i].iov_base = buf + used_so_far;\n    if (i == num - 1) {\n      iov[i].iov_len = input.size() - used_so_far;\n    } else {\n      // Randomly choose to insert a 0 byte entry.\n      if (one_in_five(rng)) {\n        iov[i].iov_len = 0;\n      } else {\n        std::uniform_int_distribution<size_t> uniform_not_used_so_far(\n            0, input.size() - used_so_far - 1);\n        iov[i].iov_len = uniform_not_used_so_far(rng);\n      }\n    }\n    used_so_far += iov[i].iov_len;\n  }\n  return iov;\n}\n\nint VerifyIOVecSource(const std::string& input) {\n  std::string compressed;\n  std::string copy = input;\n  char* buf = const_cast<char*>(copy.data());\n  size_t num = 0;\n  struct iovec* iov = GetIOVec(input, buf, num);\n  const size_t written = snappy::CompressFromIOVec(iov, num, &compressed);\n  CHECK_EQ(written, compressed.size());\n  CHECK_LE(compressed.size(), snappy::MaxCompressedLength(input.size()));\n  CHECK(snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n\n  std::string uncompressed;\n  DataEndingAtUnreadablePage c(compressed);\n  CHECK(snappy::Uncompress(c.data(), c.size(), &uncompressed));\n  CHECK_EQ(uncompressed, input);\n  delete[] iov;\n  return uncompressed.size();\n}\n\nvoid VerifyIOVecSink(const std::string& input) {\n  std::string compressed;\n  DataEndingAtUnreadablePage i(input);\n  const size_t written = snappy::Compress(i.data(), i.size(), &compressed);\n  CHECK_EQ(written, compressed.size());\n  CHECK_LE(compressed.size(), snappy::MaxCompressedLength(input.size()));\n  CHECK(snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n  char* buf = new char[input.size()];\n  size_t num = 0;\n  struct iovec* iov = GetIOVec(input, buf, num);\n  CHECK(snappy::RawUncompressToIOVec(compressed.data(), compressed.size(), iov,\n                                     num));\n  CHECK(!memcmp(buf, input.data(), input.size()));\n  delete[] iov;\n  delete[] buf;\n}\n\n// Test that data compressed by a compressor that does not\n// obey block sizes is uncompressed properly.\nvoid VerifyNonBlockedCompression(const std::string& input) {\n  if (input.length() > snappy::kBlockSize) {\n    // We cannot test larger blocks than the maximum block size, obviously.\n    return;\n  }\n\n  std::string prefix;\n  Varint::Append32(&prefix, input.size());\n\n  // Setup compression table\n  snappy::internal::WorkingMemory wmem(input.size());\n  int table_size;\n  uint16_t* table = wmem.GetHashTable(input.size(), &table_size);\n\n  // Compress entire input in one shot\n  std::string compressed;\n  compressed += prefix;\n  compressed.resize(prefix.size()+snappy::MaxCompressedLength(input.size()));\n  char* dest = string_as_array(&compressed) + prefix.size();\n  char* end = snappy::internal::CompressFragment(input.data(), input.size(),\n                                                dest, table, table_size);\n  compressed.resize(end - compressed.data());\n\n  // Uncompress into std::string\n  std::string uncomp_str;\n  CHECK(snappy::Uncompress(compressed.data(), compressed.size(), &uncomp_str));\n  CHECK_EQ(uncomp_str, input);\n\n  // Uncompress using source/sink\n  std::string uncomp_str2;\n  uncomp_str2.resize(input.size());\n  snappy::UncheckedByteArraySink sink(string_as_array(&uncomp_str2));\n  snappy::ByteArraySource source(compressed.data(), compressed.size());\n  CHECK(snappy::Uncompress(&source, &sink));\n  CHECK_EQ(uncomp_str2, input);\n\n  // Uncompress into iovec\n  {\n    static const int kNumBlocks = 10;\n    struct iovec vec[kNumBlocks];\n    const int block_size = 1 + input.size() / kNumBlocks;\n    std::string iovec_data(block_size * kNumBlocks, 'x');\n    for (int i = 0; i < kNumBlocks; ++i) {\n      vec[i].iov_base = string_as_array(&iovec_data) + i * block_size;\n      vec[i].iov_len = block_size;\n    }\n    CHECK(snappy::RawUncompressToIOVec(compressed.data(), compressed.size(),\n                                       vec, kNumBlocks));\n    CHECK_EQ(std::string(iovec_data.data(), input.size()), input);\n  }\n}\n\n// Expand the input so that it is at least K times as big as block size\nstd::string Expand(const std::string& input) {\n  static const int K = 3;\n  std::string data = input;\n  while (data.size() < K * snappy::kBlockSize) {\n    data += input;\n  }\n  return data;\n}\n\nint Verify(const std::string& input) {\n  VLOG(1) << \"Verifying input of size \" << input.size();\n\n  // Compress using string based routines\n  const int result = VerifyString(input);\n\n  // Compress using `iovec`-based routines.\n  CHECK_EQ(VerifyIOVecSource(input), result);\n\n  // Verify using sink based routines\n  VerifyStringSink(input);\n\n  VerifyNonBlockedCompression(input);\n  VerifyIOVecSink(input);\n  if (!input.empty()) {\n    const std::string expanded = Expand(input);\n    VerifyNonBlockedCompression(expanded);\n    VerifyIOVecSink(input);\n  }\n\n  return result;\n}\n\nbool IsValidCompressedBuffer(const std::string& c) {\n  return snappy::IsValidCompressedBuffer(c.data(), c.size());\n}\nbool Uncompress(const std::string& c, std::string* u) {\n  return snappy::Uncompress(c.data(), c.size(), u);\n}\n\n// This test checks to ensure that snappy doesn't coredump if it gets\n// corrupted data.\nTEST(CorruptedTest, VerifyCorrupted) {\n  std::string source = \"making sure we don't crash with corrupted input\";\n  VLOG(1) << source;\n  std::string dest;\n  std::string uncmp;\n  snappy::Compress(source.data(), source.size(), &dest);\n\n  // Mess around with the data. It's hard to simulate all possible\n  // corruptions; this is just one example ...\n  CHECK_GT(dest.size(), 3);\n  dest[1]--;\n  dest[3]++;\n  // this really ought to fail.\n  CHECK(!IsValidCompressedBuffer(dest));\n  CHECK(!Uncompress(dest, &uncmp));\n\n  // This is testing for a security bug - a buffer that decompresses to 100k\n  // but we lie in the snappy header and only reserve 0 bytes of memory :)\n  source.resize(100000);\n  for (char& source_char : source) {\n    source_char = 'A';\n  }\n  snappy::Compress(source.data(), source.size(), &dest);\n  dest[0] = dest[1] = dest[2] = dest[3] = 0;\n  CHECK(!IsValidCompressedBuffer(dest));\n  CHECK(!Uncompress(dest, &uncmp));\n\n  if (sizeof(void *) == 4) {\n    // Another security check; check a crazy big length can't DoS us with an\n    // over-allocation.\n    // Currently this is done only for 32-bit builds.  On 64-bit builds,\n    // where 3 GB might be an acceptable allocation size, Uncompress()\n    // attempts to decompress, and sometimes causes the test to run out of\n    // memory.\n    dest[0] = dest[1] = dest[2] = dest[3] = '\\xff';\n    // This decodes to a really large size, i.e., about 3 GB.\n    dest[4] = 'k';\n    CHECK(!IsValidCompressedBuffer(dest));\n    CHECK(!Uncompress(dest, &uncmp));\n  } else {\n    LOG(WARNING) << \"Crazy decompression lengths not checked on 64-bit build\";\n  }\n\n  // This decodes to about 2 MB; much smaller, but should still fail.\n  dest[0] = dest[1] = dest[2] = '\\xff';\n  dest[3] = 0x00;\n  CHECK(!IsValidCompressedBuffer(dest));\n  CHECK(!Uncompress(dest, &uncmp));\n\n  // try reading stuff in from a bad file.\n  for (int i = 1; i <= 3; ++i) {\n    std::string data =\n        ReadTestDataFile(StrFormat(\"baddata%d.snappy\", i).c_str(), 0);\n    std::string uncmp;\n    // check that we don't return a crazy length\n    size_t ulen;\n    CHECK(!snappy::GetUncompressedLength(data.data(), data.size(), &ulen)\n          || (ulen < (1<<20)));\n    uint32_t ulen2;\n    snappy::ByteArraySource source(data.data(), data.size());\n    CHECK(!snappy::GetUncompressedLength(&source, &ulen2) ||\n          (ulen2 < (1<<20)));\n    CHECK(!IsValidCompressedBuffer(data));\n    CHECK(!Uncompress(data, &uncmp));\n  }\n}\n\n// Helper routines to construct arbitrary compressed strings.\n// These mirror the compression code in snappy.cc, but are copied\n// here so that we can bypass some limitations in the how snappy.cc\n// invokes these routines.\nvoid AppendLiteral(std::string* dst, const std::string& literal) {\n  if (literal.empty()) return;\n  int n = literal.size() - 1;\n  if (n < 60) {\n    // Fit length in tag byte\n    dst->push_back(0 | (n << 2));\n  } else {\n    // Encode in upcoming bytes\n    char number[4];\n    int count = 0;\n    while (n > 0) {\n      number[count++] = n & 0xff;\n      n >>= 8;\n    }\n    dst->push_back(0 | ((59+count) << 2));\n    *dst += std::string(number, count);\n  }\n  *dst += literal;\n}\n\nvoid AppendCopy(std::string* dst, int offset, int length) {\n  while (length > 0) {\n    // Figure out how much to copy in one shot\n    int to_copy;\n    if (length >= 68) {\n      to_copy = 64;\n    } else if (length > 64) {\n      to_copy = 60;\n    } else {\n      to_copy = length;\n    }\n    length -= to_copy;\n\n    if ((to_copy >= 4) && (to_copy < 12) && (offset < 2048)) {\n      assert(to_copy-4 < 8);            // Must fit in 3 bits\n      dst->push_back(1 | ((to_copy-4) << 2) | ((offset >> 8) << 5));\n      dst->push_back(offset & 0xff);\n    } else if (offset < 65536) {\n      dst->push_back(2 | ((to_copy-1) << 2));\n      dst->push_back(offset & 0xff);\n      dst->push_back(offset >> 8);\n    } else {\n      dst->push_back(3 | ((to_copy-1) << 2));\n      dst->push_back(offset & 0xff);\n      dst->push_back((offset >> 8) & 0xff);\n      dst->push_back((offset >> 16) & 0xff);\n      dst->push_back((offset >> 24) & 0xff);\n    }\n  }\n}\n\nTEST(Snappy, SimpleTests) {\n  Verify(\"\");\n  Verify(\"a\");\n  Verify(\"ab\");\n  Verify(\"abc\");\n\n  Verify(\"aaaaaaa\" + std::string(16, 'b') + std::string(\"aaaaa\") + \"abc\");\n  Verify(\"aaaaaaa\" + std::string(256, 'b') + std::string(\"aaaaa\") + \"abc\");\n  Verify(\"aaaaaaa\" + std::string(2047, 'b') + std::string(\"aaaaa\") + \"abc\");\n  Verify(\"aaaaaaa\" + std::string(65536, 'b') + std::string(\"aaaaa\") + \"abc\");\n  Verify(\"abcaaaaaaa\" + std::string(65536, 'b') + std::string(\"aaaaa\") + \"abc\");\n}\n\n// Regression test for cr/345340892.\nTEST(Snappy, AppendSelfPatternExtensionEdgeCases) {\n  Verify(\"abcabcabcabcabcabcab\");\n  Verify(\"abcabcabcabcabcabcab0123456789ABCDEF\");\n\n  Verify(\"abcabcabcabcabcabcabcabcabcabcabcabc\");\n  Verify(\"abcabcabcabcabcabcabcabcabcabcabcabc0123456789ABCDEF\");\n}\n\n// Regression test for cr/345340892.\nTEST(Snappy, AppendSelfPatternExtensionEdgeCasesExhaustive) {\n  std::mt19937 rng;\n  std::uniform_int_distribution<int> uniform_byte(0, 255);\n  for (int pattern_size = 1; pattern_size <= 18; ++pattern_size) {\n    for (int length = 1; length <= 64; ++length) {\n      for (int extra_bytes_after_pattern : {0, 1, 15, 16, 128}) {\n        const int size = pattern_size + length + extra_bytes_after_pattern;\n        std::string input;\n        input.resize(size);\n        for (int i = 0; i < pattern_size; ++i) {\n          input[i] = 'a' + i;\n        }\n        for (int i = 0; i < length; ++i) {\n          input[pattern_size + i] = input[i];\n        }\n        for (int i = 0; i < extra_bytes_after_pattern; ++i) {\n          input[pattern_size + length + i] =\n              static_cast<char>(uniform_byte(rng));\n        }\n        Verify(input);\n      }\n    }\n  }\n}\n\n// Verify max blowup (lots of four-byte copies)\nTEST(Snappy, MaxBlowup) {\n  std::mt19937 rng;\n  std::uniform_int_distribution<int> uniform_byte(0, 255);\n  std::string input;\n  for (int i = 0; i < 80000; ++i)\n    input.push_back(static_cast<char>(uniform_byte(rng)));\n\n  for (int i = 0; i < 80000; i += 4) {\n    std::string four_bytes(input.end() - i - 4, input.end() - i);\n    input.append(four_bytes);\n  }\n  Verify(input);\n}\n\nTEST(Snappy, RandomData) {\n  std::minstd_rand0 rng(snappy::GetFlag(FLAGS_test_random_seed));\n  std::uniform_int_distribution<int> uniform_0_to_3(0, 3);\n  std::uniform_int_distribution<int> uniform_0_to_8(0, 8);\n  std::uniform_int_distribution<int> uniform_byte(0, 255);\n  std::uniform_int_distribution<size_t> uniform_4k(0, 4095);\n  std::uniform_int_distribution<size_t> uniform_64k(0, 65535);\n  std::bernoulli_distribution one_in_ten(1.0 / 10);\n\n  constexpr int num_ops = 20000;\n  for (int i = 0; i < num_ops; ++i) {\n    if ((i % 1000) == 0) {\n      VLOG(0) << \"Random op \" << i << \" of \" << num_ops;\n    }\n\n    std::string x;\n    size_t len = uniform_4k(rng);\n    if (i < 100) {\n      len = 65536 + uniform_64k(rng);\n    }\n    while (x.size() < len) {\n      int run_len = 1;\n      if (one_in_ten(rng)) {\n        int skewed_bits = uniform_0_to_8(rng);\n        // int is guaranteed to hold at least 16 bits, this uses at most 8 bits.\n        std::uniform_int_distribution<int> skewed_low(0,\n                                                      (1 << skewed_bits) - 1);\n        run_len = skewed_low(rng);\n      }\n      char c = static_cast<char>(uniform_byte(rng));\n      if (i >= 100) {\n        int skewed_bits = uniform_0_to_3(rng);\n        // int is guaranteed to hold at least 16 bits, this uses at most 3 bits.\n        std::uniform_int_distribution<int> skewed_low(0,\n                                                      (1 << skewed_bits) - 1);\n        c = static_cast<char>(skewed_low(rng));\n      }\n      while (run_len-- > 0 && x.size() < len) {\n        x.push_back(c);\n      }\n    }\n\n    Verify(x);\n  }\n}\n\nTEST(Snappy, FourByteOffset) {\n  // The new compressor cannot generate four-byte offsets since\n  // it chops up the input into 32KB pieces.  So we hand-emit the\n  // copy manually.\n\n  // The two fragments that make up the input string.\n  std::string fragment1 = \"012345689abcdefghijklmnopqrstuvwxyz\";\n  std::string fragment2 = \"some other string\";\n\n  // How many times each fragment is emitted.\n  const int n1 = 2;\n  const int n2 = 100000 / fragment2.size();\n  const size_t length = n1 * fragment1.size() + n2 * fragment2.size();\n\n  std::string compressed;\n  Varint::Append32(&compressed, length);\n\n  AppendLiteral(&compressed, fragment1);\n  std::string src = fragment1;\n  for (int i = 0; i < n2; ++i) {\n    AppendLiteral(&compressed, fragment2);\n    src += fragment2;\n  }\n  AppendCopy(&compressed, src.size(), fragment1.size());\n  src += fragment1;\n  CHECK_EQ(length, src.size());\n\n  std::string uncompressed;\n  CHECK(snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n  CHECK(snappy::Uncompress(compressed.data(), compressed.size(),\n                           &uncompressed));\n  CHECK_EQ(uncompressed, src);\n}\n\nTEST(Snappy, IOVecSourceEdgeCases) {\n  // Validate that empty leading, trailing, and in-between iovecs are handled:\n  // [] [] ['a'] [] ['b'] [].\n  std::string data = \"ab\";\n  char* buf = const_cast<char*>(data.data());\n  size_t used_so_far = 0;\n  static const int kLengths[] = {0, 0, 1, 0, 1, 0};\n  struct iovec iov[ARRAYSIZE(kLengths)];\n  for (int i = 0; i < ARRAYSIZE(kLengths); ++i) {\n    iov[i].iov_base = buf + used_so_far;\n    iov[i].iov_len = kLengths[i];\n    used_so_far += kLengths[i];\n  }\n  std::string compressed;\n  snappy::CompressFromIOVec(iov, ARRAYSIZE(kLengths), &compressed);\n  std::string uncompressed;\n  snappy::Uncompress(compressed.data(), compressed.size(), &uncompressed);\n  CHECK_EQ(data, uncompressed);\n}\n\nTEST(Snappy, IOVecSinkEdgeCases) {\n  // Test some tricky edge cases in the iovec output that are not necessarily\n  // exercised by random tests.\n\n  // Our output blocks look like this initially (the last iovec is bigger\n  // than depicted):\n  // [  ] [ ] [    ] [        ] [        ]\n  static const int kLengths[] = { 2, 1, 4, 8, 128 };\n\n  struct iovec iov[ARRAYSIZE(kLengths)];\n  for (int i = 0; i < ARRAYSIZE(kLengths); ++i) {\n    iov[i].iov_base = new char[kLengths[i]];\n    iov[i].iov_len = kLengths[i];\n  }\n\n  std::string compressed;\n  Varint::Append32(&compressed, 22);\n\n  // A literal whose output crosses three blocks.\n  // [ab] [c] [123 ] [        ] [        ]\n  AppendLiteral(&compressed, \"abc123\");\n\n  // A copy whose output crosses two blocks (source and destination\n  // segments marked).\n  // [ab] [c] [1231] [23      ] [        ]\n  //           ^--^   --\n  AppendCopy(&compressed, 3, 3);\n\n  // A copy where the input is, at first, in the block before the output:\n  //\n  // [ab] [c] [1231] [231231  ] [        ]\n  //           ^---     ^---\n  // Then during the copy, the pointers move such that the input and\n  // output pointers are in the same block:\n  //\n  // [ab] [c] [1231] [23123123] [        ]\n  //                  ^-    ^-\n  // And then they move again, so that the output pointer is no longer\n  // in the same block as the input pointer:\n  // [ab] [c] [1231] [23123123] [123     ]\n  //                    ^--      ^--\n  AppendCopy(&compressed, 6, 9);\n\n  // Finally, a copy where the input is from several blocks back,\n  // and it also crosses three blocks:\n  //\n  // [ab] [c] [1231] [23123123] [123b    ]\n  //   ^                            ^\n  // [ab] [c] [1231] [23123123] [123bc   ]\n  //       ^                         ^\n  // [ab] [c] [1231] [23123123] [123bc12 ]\n  //           ^-                     ^-\n  AppendCopy(&compressed, 17, 4);\n\n  CHECK(snappy::RawUncompressToIOVec(\n      compressed.data(), compressed.size(), iov, ARRAYSIZE(iov)));\n  CHECK_EQ(0, memcmp(iov[0].iov_base, \"ab\", 2));\n  CHECK_EQ(0, memcmp(iov[1].iov_base, \"c\", 1));\n  CHECK_EQ(0, memcmp(iov[2].iov_base, \"1231\", 4));\n  CHECK_EQ(0, memcmp(iov[3].iov_base, \"23123123\", 8));\n  CHECK_EQ(0, memcmp(iov[4].iov_base, \"123bc12\", 7));\n\n  for (int i = 0; i < ARRAYSIZE(kLengths); ++i) {\n    delete[] reinterpret_cast<char *>(iov[i].iov_base);\n  }\n}\n\nTEST(Snappy, IOVecLiteralOverflow) {\n  static const int kLengths[] = { 3, 4 };\n\n  struct iovec iov[ARRAYSIZE(kLengths)];\n  for (int i = 0; i < ARRAYSIZE(kLengths); ++i) {\n    iov[i].iov_base = new char[kLengths[i]];\n    iov[i].iov_len = kLengths[i];\n  }\n\n  std::string compressed;\n  Varint::Append32(&compressed, 8);\n\n  AppendLiteral(&compressed, \"12345678\");\n\n  CHECK(!snappy::RawUncompressToIOVec(\n      compressed.data(), compressed.size(), iov, ARRAYSIZE(iov)));\n\n  for (int i = 0; i < ARRAYSIZE(kLengths); ++i) {\n    delete[] reinterpret_cast<char *>(iov[i].iov_base);\n  }\n}\n\nTEST(Snappy, IOVecCopyOverflow) {\n  static const int kLengths[] = { 3, 4 };\n\n  struct iovec iov[ARRAYSIZE(kLengths)];\n  for (int i = 0; i < ARRAYSIZE(kLengths); ++i) {\n    iov[i].iov_base = new char[kLengths[i]];\n    iov[i].iov_len = kLengths[i];\n  }\n\n  std::string compressed;\n  Varint::Append32(&compressed, 8);\n\n  AppendLiteral(&compressed, \"123\");\n  AppendCopy(&compressed, 3, 5);\n\n  CHECK(!snappy::RawUncompressToIOVec(\n      compressed.data(), compressed.size(), iov, ARRAYSIZE(iov)));\n\n  for (int i = 0; i < ARRAYSIZE(kLengths); ++i) {\n    delete[] reinterpret_cast<char *>(iov[i].iov_base);\n  }\n}\n\nbool CheckUncompressedLength(const std::string& compressed, size_t* ulength) {\n  const bool result1 = snappy::GetUncompressedLength(compressed.data(),\n                                                     compressed.size(),\n                                                     ulength);\n\n  snappy::ByteArraySource source(compressed.data(), compressed.size());\n  uint32_t length;\n  const bool result2 = snappy::GetUncompressedLength(&source, &length);\n  CHECK_EQ(result1, result2);\n  return result1;\n}\n\nTEST(SnappyCorruption, TruncatedVarint) {\n  std::string compressed, uncompressed;\n  size_t ulength;\n  compressed.push_back('\\xf0');\n  CHECK(!CheckUncompressedLength(compressed, &ulength));\n  CHECK(!snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n  CHECK(!snappy::Uncompress(compressed.data(), compressed.size(),\n                            &uncompressed));\n}\n\nTEST(SnappyCorruption, UnterminatedVarint) {\n  std::string compressed, uncompressed;\n  size_t ulength;\n  compressed.push_back('\\x80');\n  compressed.push_back('\\x80');\n  compressed.push_back('\\x80');\n  compressed.push_back('\\x80');\n  compressed.push_back('\\x80');\n  compressed.push_back(10);\n  CHECK(!CheckUncompressedLength(compressed, &ulength));\n  CHECK(!snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n  CHECK(!snappy::Uncompress(compressed.data(), compressed.size(),\n                            &uncompressed));\n}\n\nTEST(SnappyCorruption, OverflowingVarint) {\n  std::string compressed, uncompressed;\n  size_t ulength;\n  compressed.push_back('\\xfb');\n  compressed.push_back('\\xff');\n  compressed.push_back('\\xff');\n  compressed.push_back('\\xff');\n  compressed.push_back('\\x7f');\n  CHECK(!CheckUncompressedLength(compressed, &ulength));\n  CHECK(!snappy::IsValidCompressedBuffer(compressed.data(), compressed.size()));\n  CHECK(!snappy::Uncompress(compressed.data(), compressed.size(),\n                            &uncompressed));\n}\n\nTEST(Snappy, ReadPastEndOfBuffer) {\n  // Check that we do not read past end of input\n\n  // Make a compressed string that ends with a single-byte literal\n  std::string compressed;\n  Varint::Append32(&compressed, 1);\n  AppendLiteral(&compressed, \"x\");\n\n  std::string uncompressed;\n  DataEndingAtUnreadablePage c(compressed);\n  CHECK(snappy::Uncompress(c.data(), c.size(), &uncompressed));\n  CHECK_EQ(uncompressed, std::string(\"x\"));\n}\n\n// Check for an infinite loop caused by a copy with offset==0\nTEST(Snappy, ZeroOffsetCopy) {\n  const char* compressed = \"\\x40\\x12\\x00\\x00\";\n  //  \\x40              Length (must be > kMaxIncrementCopyOverflow)\n  //  \\x12\\x00\\x00      Copy with offset==0, length==5\n  char uncompressed[100];\n  EXPECT_FALSE(snappy::RawUncompress(compressed, 4, uncompressed));\n}\n\nTEST(Snappy, ZeroOffsetCopyValidation) {\n  const char* compressed = \"\\x05\\x12\\x00\\x00\";\n  //  \\x05              Length\n  //  \\x12\\x00\\x00      Copy with offset==0, length==5\n  EXPECT_FALSE(snappy::IsValidCompressedBuffer(compressed, 4));\n}\n\nint TestFindMatchLength(const char* s1, const char *s2, unsigned length) {\n  uint64_t data;\n  std::pair<size_t, bool> p =\n      snappy::internal::FindMatchLength(s1, s2, s2 + length, &data);\n  CHECK_EQ(p.first < 8, p.second);\n  return p.first;\n}\n\nTEST(Snappy, FindMatchLength) {\n  // Exercise all different code paths through the function.\n  // 64-bit version:\n\n  // Hit s1_limit in 64-bit loop, hit s1_limit in single-character loop.\n  EXPECT_EQ(6, TestFindMatchLength(\"012345\", \"012345\", 6));\n  EXPECT_EQ(11, TestFindMatchLength(\"01234567abc\", \"01234567abc\", 11));\n\n  // Hit s1_limit in 64-bit loop, find a non-match in single-character loop.\n  EXPECT_EQ(9, TestFindMatchLength(\"01234567abc\", \"01234567axc\", 9));\n\n  // Same, but edge cases.\n  EXPECT_EQ(11, TestFindMatchLength(\"01234567abc!\", \"01234567abc!\", 11));\n  EXPECT_EQ(11, TestFindMatchLength(\"01234567abc!\", \"01234567abc?\", 11));\n\n  // Find non-match at once in first loop.\n  EXPECT_EQ(0, TestFindMatchLength(\"01234567xxxxxxxx\", \"?1234567xxxxxxxx\", 16));\n  EXPECT_EQ(1, TestFindMatchLength(\"01234567xxxxxxxx\", \"0?234567xxxxxxxx\", 16));\n  EXPECT_EQ(4, TestFindMatchLength(\"01234567xxxxxxxx\", \"01237654xxxxxxxx\", 16));\n  EXPECT_EQ(7, TestFindMatchLength(\"01234567xxxxxxxx\", \"0123456?xxxxxxxx\", 16));\n\n  // Find non-match in first loop after one block.\n  EXPECT_EQ(8, TestFindMatchLength(\"abcdefgh01234567xxxxxxxx\",\n                                   \"abcdefgh?1234567xxxxxxxx\", 24));\n  EXPECT_EQ(9, TestFindMatchLength(\"abcdefgh01234567xxxxxxxx\",\n                                   \"abcdefgh0?234567xxxxxxxx\", 24));\n  EXPECT_EQ(12, TestFindMatchLength(\"abcdefgh01234567xxxxxxxx\",\n                                    \"abcdefgh01237654xxxxxxxx\", 24));\n  EXPECT_EQ(15, TestFindMatchLength(\"abcdefgh01234567xxxxxxxx\",\n                                    \"abcdefgh0123456?xxxxxxxx\", 24));\n\n  // 32-bit version:\n\n  // Short matches.\n  EXPECT_EQ(0, TestFindMatchLength(\"01234567\", \"?1234567\", 8));\n  EXPECT_EQ(1, TestFindMatchLength(\"01234567\", \"0?234567\", 8));\n  EXPECT_EQ(2, TestFindMatchLength(\"01234567\", \"01?34567\", 8));\n  EXPECT_EQ(3, TestFindMatchLength(\"01234567\", \"012?4567\", 8));\n  EXPECT_EQ(4, TestFindMatchLength(\"01234567\", \"0123?567\", 8));\n  EXPECT_EQ(5, TestFindMatchLength(\"01234567\", \"01234?67\", 8));\n  EXPECT_EQ(6, TestFindMatchLength(\"01234567\", \"012345?7\", 8));\n  EXPECT_EQ(7, TestFindMatchLength(\"01234567\", \"0123456?\", 8));\n  EXPECT_EQ(7, TestFindMatchLength(\"01234567\", \"0123456?\", 7));\n  EXPECT_EQ(7, TestFindMatchLength(\"01234567!\", \"0123456??\", 7));\n\n  // Hit s1_limit in 32-bit loop, hit s1_limit in single-character loop.\n  EXPECT_EQ(10, TestFindMatchLength(\"xxxxxxabcd\", \"xxxxxxabcd\", 10));\n  EXPECT_EQ(10, TestFindMatchLength(\"xxxxxxabcd?\", \"xxxxxxabcd?\", 10));\n  EXPECT_EQ(13, TestFindMatchLength(\"xxxxxxabcdef\", \"xxxxxxabcdef\", 13));\n\n  // Same, but edge cases.\n  EXPECT_EQ(12, TestFindMatchLength(\"xxxxxx0123abc!\", \"xxxxxx0123abc!\", 12));\n  EXPECT_EQ(12, TestFindMatchLength(\"xxxxxx0123abc!\", \"xxxxxx0123abc?\", 12));\n\n  // Hit s1_limit in 32-bit loop, find a non-match in single-character loop.\n  EXPECT_EQ(11, TestFindMatchLength(\"xxxxxx0123abc\", \"xxxxxx0123axc\", 13));\n\n  // Find non-match at once in first loop.\n  EXPECT_EQ(6, TestFindMatchLength(\"xxxxxx0123xxxxxxxx\",\n                                   \"xxxxxx?123xxxxxxxx\", 18));\n  EXPECT_EQ(7, TestFindMatchLength(\"xxxxxx0123xxxxxxxx\",\n                                   \"xxxxxx0?23xxxxxxxx\", 18));\n  EXPECT_EQ(8, TestFindMatchLength(\"xxxxxx0123xxxxxxxx\",\n                                   \"xxxxxx0132xxxxxxxx\", 18));\n  EXPECT_EQ(9, TestFindMatchLength(\"xxxxxx0123xxxxxxxx\",\n                                   \"xxxxxx012?xxxxxxxx\", 18));\n\n  // Same, but edge cases.\n  EXPECT_EQ(6, TestFindMatchLength(\"xxxxxx0123\", \"xxxxxx?123\", 10));\n  EXPECT_EQ(7, TestFindMatchLength(\"xxxxxx0123\", \"xxxxxx0?23\", 10));\n  EXPECT_EQ(8, TestFindMatchLength(\"xxxxxx0123\", \"xxxxxx0132\", 10));\n  EXPECT_EQ(9, TestFindMatchLength(\"xxxxxx0123\", \"xxxxxx012?\", 10));\n\n  // Find non-match in first loop after one block.\n  EXPECT_EQ(10, TestFindMatchLength(\"xxxxxxabcd0123xx\",\n                                    \"xxxxxxabcd?123xx\", 16));\n  EXPECT_EQ(11, TestFindMatchLength(\"xxxxxxabcd0123xx\",\n                                    \"xxxxxxabcd0?23xx\", 16));\n  EXPECT_EQ(12, TestFindMatchLength(\"xxxxxxabcd0123xx\",\n                                    \"xxxxxxabcd0132xx\", 16));\n  EXPECT_EQ(13, TestFindMatchLength(\"xxxxxxabcd0123xx\",\n                                    \"xxxxxxabcd012?xx\", 16));\n\n  // Same, but edge cases.\n  EXPECT_EQ(10, TestFindMatchLength(\"xxxxxxabcd0123\", \"xxxxxxabcd?123\", 14));\n  EXPECT_EQ(11, TestFindMatchLength(\"xxxxxxabcd0123\", \"xxxxxxabcd0?23\", 14));\n  EXPECT_EQ(12, TestFindMatchLength(\"xxxxxxabcd0123\", \"xxxxxxabcd0132\", 14));\n  EXPECT_EQ(13, TestFindMatchLength(\"xxxxxxabcd0123\", \"xxxxxxabcd012?\", 14));\n}\n\nTEST(Snappy, FindMatchLengthRandom) {\n  constexpr int kNumTrials = 10000;\n  constexpr int kTypicalLength = 10;\n  std::minstd_rand0 rng(snappy::GetFlag(FLAGS_test_random_seed));\n  std::uniform_int_distribution<int> uniform_byte(0, 255);\n  std::bernoulli_distribution one_in_two(1.0 / 2);\n  std::bernoulli_distribution one_in_typical_length(1.0 / kTypicalLength);\n\n  for (int i = 0; i < kNumTrials; ++i) {\n    std::string s, t;\n    char a = static_cast<char>(uniform_byte(rng));\n    char b = static_cast<char>(uniform_byte(rng));\n    while (!one_in_typical_length(rng)) {\n      s.push_back(one_in_two(rng) ? a : b);\n      t.push_back(one_in_two(rng) ? a : b);\n    }\n    DataEndingAtUnreadablePage u(s);\n    DataEndingAtUnreadablePage v(t);\n    size_t matched = TestFindMatchLength(u.data(), v.data(), t.size());\n    if (matched == t.size()) {\n      EXPECT_EQ(s, t);\n    } else {\n      EXPECT_NE(s[matched], t[matched]);\n      for (size_t j = 0; j < matched; ++j) {\n        EXPECT_EQ(s[j], t[j]);\n      }\n    }\n  }\n}\n\nuint16_t MakeEntry(unsigned int extra, unsigned int len,\n                   unsigned int copy_offset) {\n  // Check that all of the fields fit within the allocated space\n  assert(extra       == (extra & 0x7));          // At most 3 bits\n  assert(copy_offset == (copy_offset & 0x7));    // At most 3 bits\n  assert(len         == (len & 0x7f));           // At most 7 bits\n  return len | (copy_offset << 8) | (extra << 11);\n}\n\n// Check that the decompression table is correct, and optionally print out\n// the computed one.\nTEST(Snappy, VerifyCharTable) {\n  using snappy::internal::LITERAL;\n  using snappy::internal::COPY_1_BYTE_OFFSET;\n  using snappy::internal::COPY_2_BYTE_OFFSET;\n  using snappy::internal::COPY_4_BYTE_OFFSET;\n  using snappy::internal::char_table;\n\n  uint16_t dst[256];\n\n  // Place invalid entries in all places to detect missing initialization\n  int assigned = 0;\n  for (int i = 0; i < 256; ++i) {\n    dst[i] = 0xffff;\n  }\n\n  // Small LITERAL entries.  We store (len-1) in the top 6 bits.\n  for (uint8_t len = 1; len <= 60; ++len) {\n    dst[LITERAL | ((len - 1) << 2)] = MakeEntry(0, len, 0);\n    assigned++;\n  }\n\n  // Large LITERAL entries.  We use 60..63 in the high 6 bits to\n  // encode the number of bytes of length info that follow the opcode.\n  for (uint8_t extra_bytes = 1; extra_bytes <= 4; ++extra_bytes) {\n    // We set the length field in the lookup table to 1 because extra\n    // bytes encode len-1.\n    dst[LITERAL | ((extra_bytes + 59) << 2)] = MakeEntry(extra_bytes, 1, 0);\n    assigned++;\n  }\n\n  // COPY_1_BYTE_OFFSET.\n  //\n  // The tag byte in the compressed data stores len-4 in 3 bits, and\n  // offset/256 in 3 bits.  offset%256 is stored in the next byte.\n  //\n  // This format is used for length in range [4..11] and offset in\n  // range [0..2047]\n  for (uint8_t len = 4; len < 12; ++len) {\n    for (uint16_t offset = 0; offset < 2048; offset += 256) {\n      uint8_t offset_high = static_cast<uint8_t>(offset >> 8);\n      dst[COPY_1_BYTE_OFFSET | ((len - 4) << 2) | (offset_high << 5)] =\n          MakeEntry(1, len, offset_high);\n      assigned++;\n    }\n  }\n\n  // COPY_2_BYTE_OFFSET.\n  // Tag contains len-1 in top 6 bits, and offset in next two bytes.\n  for (uint8_t len = 1; len <= 64; ++len) {\n    dst[COPY_2_BYTE_OFFSET | ((len - 1) << 2)] = MakeEntry(2, len, 0);\n    assigned++;\n  }\n\n  // COPY_4_BYTE_OFFSET.\n  // Tag contents len-1 in top 6 bits, and offset in next four bytes.\n  for (uint8_t len = 1; len <= 64; ++len) {\n    dst[COPY_4_BYTE_OFFSET | ((len - 1) << 2)] = MakeEntry(4, len, 0);\n    assigned++;\n  }\n\n  // Check that each entry was initialized exactly once.\n  EXPECT_EQ(256, assigned) << \"Assigned only \" << assigned << \" of 256\";\n  for (int i = 0; i < 256; ++i) {\n    EXPECT_NE(0xffff, dst[i]) << \"Did not assign byte \" << i;\n  }\n\n  if (snappy::GetFlag(FLAGS_snappy_dump_decompression_table)) {\n    std::printf(\"static const uint16_t char_table[256] = {\\n  \");\n    for (int i = 0; i < 256; ++i) {\n      std::printf(\"0x%04x%s\",\n                  dst[i],\n                  ((i == 255) ? \"\\n\" : (((i % 8) == 7) ? \",\\n  \" : \", \")));\n    }\n    std::printf(\"};\\n\");\n  }\n\n  // Check that computed table matched recorded table.\n  for (int i = 0; i < 256; ++i) {\n    EXPECT_EQ(dst[i], char_table[i]) << \"Mismatch in byte \" << i;\n  }\n}\n\nTEST(Snappy, TestBenchmarkFiles) {\n  for (int i = 0; i < ARRAYSIZE(kTestDataFiles); ++i) {\n    Verify(ReadTestDataFile(kTestDataFiles[i].filename,\n                            kTestDataFiles[i].size_limit));\n  }\n}\n\n}  // namespace\n\n}  // namespace snappy\n"
        },
        {
          "name": "testdata",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}