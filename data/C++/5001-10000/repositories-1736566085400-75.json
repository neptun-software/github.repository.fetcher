{
  "metadata": {
    "timestamp": 1736566085400,
    "page": 75,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "SJTU-IPADS/PowerInfer",
      "stars": 8048,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-tidy",
          "type": "blob",
          "size": 0.7333984375,
          "content": "---\nChecks: >\n    bugprone-*,\n    -bugprone-easily-swappable-parameters,\n    -bugprone-implicit-widening-of-multiplication-result,\n    -bugprone-misplaced-widening-cast,\n    -bugprone-narrowing-conversions,\n    readability-*,\n    -readability-avoid-unconditional-preprocessor-if,\n    -readability-function-cognitive-complexity,\n    -readability-identifier-length,\n    -readability-implicit-bool-conversion,\n    -readability-magic-numbers,\n    -readability-uppercase-literal-suffix,\n    clang-analyzer-*,\n    -clang-analyzer-security.insecureAPI.DeprecatedOrUnsafeBufferHandling,\n    performance-*,\n    portability-*,\n    misc-*,\n    -misc-const-correctness,\n    -misc-non-private-member-variables-in-classes,\n    -misc-no-recursion,\nFormatStyle: none\n"
        },
        {
          "name": ".devops",
          "type": "tree",
          "content": null
        },
        {
          "name": ".dockerignore",
          "type": "blob",
          "size": 0.1435546875,
          "content": "*.o\n*.a\n.cache/\n.git/\n.github/\n.gitignore\n.vs/\n.vscode/\n.DS_Store\n\nbuild*/\n\nmodels/*\n\n/main\n/quantize\n\narm_neon.h\ncompile_commands.json\nDockerfile\n"
        },
        {
          "name": ".ecrc",
          "type": "blob",
          "size": 0.044921875,
          "content": "{\n  \"Disable\": {\n    \"IndentSize\": true\n  }\n}\n"
        },
        {
          "name": ".editorconfig",
          "type": "blob",
          "size": 0.3857421875,
          "content": "# https://EditorConfig.org\n\n# Top-most EditorConfig file\nroot = true\n\n# Unix-style newlines with a newline ending every file, utf-8 charset\n[*]\nend_of_line = lf\ninsert_final_newline = true\ntrim_trailing_whitespace = true\ncharset = utf-8\nindent_style = space\nindent_size = 4\n\n[Makefile]\nindent_style = tab\n\n[prompts/*.txt]\ninsert_final_newline = unset\n\n[examples/server/public/*]\nindent_size = 2\n"
        },
        {
          "name": ".flake8",
          "type": "blob",
          "size": 0.0302734375,
          "content": "[flake8]\nmax-line-length = 125\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1259765625,
          "content": "*.o\n*.a\n*.so\n*.gguf\n*.bin\n*.exe\n*.dll\n*.log\n*.gcov\n*.gcno\n*.gcda\n*.dot\n*.bat\n*.metallib\n.DS_Store\n.build/\n.cache/\n.ccls-cache/\n.direnv/\n.envrc\n.swiftpm\n.venv\n.clang-tidy\n.vs/\n.vscode/\n\nlcov-report/\ngcovr-report/\n\nbuild*/\nout/\ntmp/\n\nmodels/*\nmodels-mnt\n\n/Pipfile\n/baby-llama\n/beam-search\n/benchmark-matmult\n/convert-llama2c-to-ggml\n/embd-input-test\n/embedding\n/gguf\n/gguf-llama-simple\n/infill\n/libllama.so\n/llama-bench\n/llava-cli\n/main\n/metal\n/perplexity\n/q8dot\n/quantize\n/quantize-stats\n/result\n/save-load-state\n/server\n/simple\n/batched\n/batched-bench\n/export-lora\n/finetune\n/speculative\n/parallel\n/train-text-from-scratch\n/vdot\n/common/build-info.cpp\narm_neon.h\ncompile_commands.json\nCMakeSettings.json\n\n__pycache__\ndist\n\nzig-out/\nzig-cache/\n\nppl-*.txt\nqnt-*.txt\nperf-*.txt\n\nexamples/jeopardy/results.txt\n\npoetry.lock\npoetry.toml\n\n# Test binaries\ntests/test-grammar-parser\ntests/test-llama-grammar\ntests/test-double-float\ntests/test-grad0\ntests/test-opt\ntests/test-quantize-fns\ntests/test-quantize-perf\ntests/test-sampling\ntests/test-tokenizer-0-llama\ntests/test-tokenizer-0-falcon\ntests/test-tokenizer-1-llama\ntests/test-tokenizer-1-bpe\n\nbuild-info.h\n"
        },
        {
          "name": ".pre-commit-config.yaml",
          "type": "blob",
          "size": 0.388671875,
          "content": "# See https://pre-commit.com for more information\n# See https://pre-commit.com/hooks.html for more hooks\nexclude: prompts/.*.txt\nrepos:\n- repo: https://github.com/pre-commit/pre-commit-hooks\n  rev: v3.2.0\n  hooks:\n  - id: trailing-whitespace\n  - id: end-of-file-fixer\n  - id: check-yaml\n  - id: check-added-large-files\n- repo: https://github.com/PyCQA/flake8\n  rev: 6.0.0\n  hooks:\n  -   id: flake8\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 29.140625,
          "content": "cmake_minimum_required(VERSION 3.13)  # for add_link_options\nproject(\"llama.cpp\" C CXX)\n\nset(CMAKE_EXPORT_COMPILE_COMMANDS ON)\n\nif (NOT XCODE AND NOT MSVC AND NOT CMAKE_BUILD_TYPE)\n    set(CMAKE_BUILD_TYPE Release CACHE STRING \"Build type\" FORCE)\n    set_property(CACHE CMAKE_BUILD_TYPE PROPERTY STRINGS \"Debug\" \"Release\" \"MinSizeRel\" \"RelWithDebInfo\")\nendif()\n\nset(CMAKE_RUNTIME_OUTPUT_DIRECTORY ${CMAKE_BINARY_DIR}/bin)\n\nif (CMAKE_SOURCE_DIR STREQUAL CMAKE_CURRENT_SOURCE_DIR)\n    set(LLAMA_STANDALONE ON)\n\n    # configure project version\n    # TODO\nelse()\n    set(LLAMA_STANDALONE OFF)\nendif()\n\nif (EMSCRIPTEN)\n    set(BUILD_SHARED_LIBS_DEFAULT OFF)\n\n    option(LLAMA_WASM_SINGLE_FILE \"llama: embed WASM inside the generated llama.js\" ON)\nelse()\n    if (MINGW)\n        set(BUILD_SHARED_LIBS_DEFAULT OFF)\n    else()\n        set(BUILD_SHARED_LIBS_DEFAULT ON)\n    endif()\nendif()\n\n\n#\n# Option list\n#\n\nif (APPLE)\n    set(LLAMA_METAL_DEFAULT OFF) # metal has not been supported on Apple Silicon yet\nelse()\n    set(LLAMA_METAL_DEFAULT OFF)\nendif()\n\n# general\noption(LLAMA_STATIC                     \"llama: static link libraries\"                          OFF)\noption(LLAMA_NATIVE                     \"llama: enable -march=native flag\"                      ON)\noption(LLAMA_LTO                        \"llama: enable link time optimization\"                  OFF)\n\n# debug\noption(LLAMA_ALL_WARNINGS               \"llama: enable all compiler warnings\"                   ON)\noption(LLAMA_ALL_WARNINGS_3RD_PARTY     \"llama: enable all compiler warnings in 3rd party libs\" OFF)\noption(LLAMA_GPROF                      \"llama: enable gprof\"                                   OFF)\n\n# sanitizers\noption(LLAMA_SANITIZE_THREAD            \"llama: enable thread sanitizer\"                        OFF)\noption(LLAMA_SANITIZE_ADDRESS           \"llama: enable address sanitizer\"                       OFF)\noption(LLAMA_SANITIZE_UNDEFINED         \"llama: enable undefined sanitizer\"                     OFF)\n\n# instruction set specific\nif (LLAMA_NATIVE)\n    set(INS_ENB OFF)\nelse()\n    set(INS_ENB ON)\nendif()\n\noption(LLAMA_AVX                             \"llama: enable AVX\"                                ${INS_ENB})\noption(LLAMA_AVX2                            \"llama: enable AVX2\"                               ${INS_ENB})\noption(LLAMA_AVX512                          \"llama: enable AVX512\"                             OFF)\noption(LLAMA_AVX512_VBMI                     \"llama: enable AVX512-VBMI\"                        OFF)\noption(LLAMA_AVX512_VNNI                     \"llama: enable AVX512-VNNI\"                        OFF)\noption(LLAMA_FMA                             \"llama: enable FMA\"                                ${INS_ENB})\n# in MSVC F16C is implied with AVX2/AVX512\nif (NOT MSVC)\n    option(LLAMA_F16C                        \"llama: enable F16C\"                               ${INS_ENB})\nendif()\n\n# 3rd party libs\noption(LLAMA_ACCELERATE                      \"llama: enable Accelerate framework\"               ON)\noption(LLAMA_BLAS                            \"llama: use BLAS\"                                  OFF)\nset(LLAMA_BLAS_VENDOR \"Generic\" CACHE STRING \"llama: BLAS library vendor\")\noption(LLAMA_CUBLAS                          \"llama: use CUDA\"                                  OFF)\n#option(LLAMA_CUDA_CUBLAS                     \"llama: use cuBLAS for prompt processing\"          OFF)\noption(LLAMA_CUDA_FORCE_DMMV                 \"llama: use dmmv instead of mmvq CUDA kernels\"     OFF)\noption(LLAMA_CUDA_FORCE_MMQ                  \"llama: use mmq kernels instead of cuBLAS\"         OFF)\nset(LLAMA_CUDA_DMMV_X      \"32\" CACHE STRING \"llama: x stride for dmmv CUDA kernels\")\nset(LLAMA_CUDA_MMV_Y        \"1\" CACHE STRING \"llama: y block size for mmv CUDA kernels\")\noption(LLAMA_CUDA_F16                        \"llama: use 16 bit floats for some calculations\"   OFF)\nset(LLAMA_CUDA_KQUANTS_ITER \"2\" CACHE STRING \"llama: iters./thread per block for Q2_K/Q6_K\")\nset(LLAMA_CUDA_PEER_MAX_BATCH_SIZE \"128\" CACHE STRING\n                                             \"llama: max. batch size for using peer access\")\noption(LLAMA_HIPBLAS                         \"llama: use hipBLAS\"                               OFF)\noption(LLAMA_CLBLAST                         \"llama: use CLBlast\"                               OFF)\noption(LLAMA_METAL                           \"llama: use Metal\"                                 ${LLAMA_METAL_DEFAULT})\noption(LLAMA_METAL_NDEBUG                    \"llama: disable Metal debugging\"                   OFF)\noption(LLAMA_MPI                             \"llama: use MPI\"                                   OFF)\noption(LLAMA_QKK_64                          \"llama: use super-block size of 64 for k-quants\"   OFF)\n\noption(LLAMA_BUILD_TESTS                \"llama: build tests\"    ${LLAMA_STANDALONE})\noption(LLAMA_BUILD_EXAMPLES             \"llama: build examples\" ${LLAMA_STANDALONE})\noption(LLAMA_BUILD_SERVER               \"llama: build server example\"                           ON)\n\n#\n# Compile flags\n#\n\nset(CMAKE_CXX_STANDARD 11)\nset(CMAKE_CXX_STANDARD_REQUIRED true)\nset(CMAKE_C_STANDARD 11)\nset(CMAKE_C_STANDARD_REQUIRED true)\nset(THREADS_PREFER_PTHREAD_FLAG ON)\nfind_package(Threads REQUIRED)\ninclude(CheckCXXCompilerFlag)\n\nif (NOT MSVC)\n    if (LLAMA_SANITIZE_THREAD)\n        add_compile_options(-fsanitize=thread)\n        link_libraries(-fsanitize=thread)\n    endif()\n\n    if (LLAMA_SANITIZE_ADDRESS)\n        add_compile_options(-fsanitize=address -fno-omit-frame-pointer)\n        link_libraries(-fsanitize=address)\n    endif()\n\n    if (LLAMA_SANITIZE_UNDEFINED)\n        add_compile_options(-fsanitize=undefined)\n        link_libraries(-fsanitize=undefined)\n    endif()\nendif()\n\nif (APPLE AND LLAMA_ACCELERATE)\n    find_library(ACCELERATE_FRAMEWORK Accelerate)\n    if (ACCELERATE_FRAMEWORK)\n        message(STATUS \"Accelerate framework found\")\n\n        add_compile_definitions(GGML_USE_ACCELERATE)\n        add_compile_definitions(ACCELERATE_NEW_LAPACK)\n        add_compile_definitions(ACCELERATE_LAPACK_ILP64)\n        set(LLAMA_EXTRA_LIBS ${LLAMA_EXTRA_LIBS} ${ACCELERATE_FRAMEWORK})\n    else()\n        message(WARNING \"Accelerate framework not found\")\n    endif()\nendif()\n\nif (LLAMA_METAL)\n    find_library(FOUNDATION_LIBRARY         Foundation              REQUIRED)\n    find_library(METAL_FRAMEWORK            Metal                   REQUIRED)\n    find_library(METALKIT_FRAMEWORK         MetalKit                REQUIRED)\n\n    message(STATUS \"Metal framework found\")\n    set(GGML_HEADERS_METAL ggml-metal.h)\n    set(GGML_SOURCES_METAL ggml-metal.m)\n\n    add_compile_definitions(GGML_USE_METAL)\n    if (LLAMA_METAL_NDEBUG)\n        add_compile_definitions(GGML_METAL_NDEBUG)\n    endif()\n\n    # get full path to the file\n    #add_compile_definitions(GGML_METAL_DIR_KERNELS=\"${CMAKE_CURRENT_SOURCE_DIR}/\")\n\n    # copy ggml-metal.metal to bin directory\n    configure_file(ggml-metal.metal bin/ggml-metal.metal COPYONLY)\n\n    set(LLAMA_EXTRA_LIBS ${LLAMA_EXTRA_LIBS}\n        ${FOUNDATION_LIBRARY}\n        ${METAL_FRAMEWORK}\n        ${METALKIT_FRAMEWORK}\n        )\nendif()\nif (LLAMA_BLAS)\n    if (LLAMA_STATIC)\n        set(BLA_STATIC ON)\n    endif()\n    if ($(CMAKE_VERSION) VERSION_GREATER_EQUAL 3.22)\n        set(BLA_SIZEOF_INTEGER 8)\n    endif()\n\n    set(BLA_VENDOR ${LLAMA_BLAS_VENDOR})\n    find_package(BLAS)\n\n    if (BLAS_FOUND)\n        message(STATUS \"BLAS found, Libraries: ${BLAS_LIBRARIES}\")\n\n        if (\"${BLAS_INCLUDE_DIRS}\" STREQUAL \"\")\n            # BLAS_INCLUDE_DIRS is missing in FindBLAS.cmake.\n            # see https://gitlab.kitware.com/cmake/cmake/-/issues/20268\n            find_package(PkgConfig REQUIRED)\n            if (${LLAMA_BLAS_VENDOR} MATCHES \"Generic\")\n                pkg_check_modules(DepBLAS REQUIRED blas)\n            elseif (${LLAMA_BLAS_VENDOR} MATCHES \"OpenBLAS\")\n                pkg_check_modules(DepBLAS REQUIRED openblas)\n            elseif (${LLAMA_BLAS_VENDOR} MATCHES \"FLAME\")\n                pkg_check_modules(DepBLAS REQUIRED blis)\n            elseif (${LLAMA_BLAS_VENDOR} MATCHES \"ATLAS\")\n                pkg_check_modules(DepBLAS REQUIRED blas-atlas)\n            elseif (${LLAMA_BLAS_VENDOR} MATCHES \"FlexiBLAS\")\n                pkg_check_modules(DepBLAS REQUIRED flexiblas_api)\n            elseif (${LLAMA_BLAS_VENDOR} MATCHES \"Intel\")\n                # all Intel* libraries share the same include path\n                pkg_check_modules(DepBLAS REQUIRED mkl-sdl)\n            elseif (${LLAMA_BLAS_VENDOR} MATCHES \"NVHPC\")\n                # this doesn't provide pkg-config\n                # suggest to assign BLAS_INCLUDE_DIRS on your own\n                if (\"${NVHPC_VERSION}\" STREQUAL \"\")\n                    message(WARNING \"Better to set NVHPC_VERSION\")\n                else()\n                    set(DepBLAS_FOUND ON)\n                    set(DepBLAS_INCLUDE_DIRS \"/opt/nvidia/hpc_sdk/${CMAKE_SYSTEM_NAME}_${CMAKE_SYSTEM_PROCESSOR}/${NVHPC_VERSION}/math_libs/include\")\n                endif()\n            endif()\n            if (DepBLAS_FOUND)\n                set(BLAS_INCLUDE_DIRS ${DepBLAS_INCLUDE_DIRS})\n            else()\n                message(WARNING \"BLAS_INCLUDE_DIRS neither been provided nor been automatically\"\n                \" detected by pkgconfig, trying to find cblas.h from possible paths...\")\n                find_path(BLAS_INCLUDE_DIRS\n                    NAMES cblas.h\n                    HINTS\n                        /usr/include\n                        /usr/local/include\n                        /usr/include/openblas\n                        /opt/homebrew/opt/openblas/include\n                        /usr/local/opt/openblas/include\n                        /usr/include/x86_64-linux-gnu/openblas/include\n                )\n            endif()\n        endif()\n\n        message(STATUS \"BLAS found, Includes: ${BLAS_INCLUDE_DIRS}\")\n        add_compile_options(${BLAS_LINKER_FLAGS})\n        add_compile_definitions(GGML_USE_OPENBLAS)\n        if (${BLAS_INCLUDE_DIRS} MATCHES \"mkl\" AND (${LLAMA_BLAS_VENDOR} MATCHES \"Generic\" OR ${LLAMA_BLAS_VENDOR} MATCHES \"Intel\"))\n            add_compile_definitions(GGML_BLAS_USE_MKL)\n        endif()\n        set(LLAMA_EXTRA_LIBS ${LLAMA_EXTRA_LIBS} ${BLAS_LIBRARIES})\n        set(LLAMA_EXTRA_INCLUDES ${LLAMA_EXTRA_INCLUDES} ${BLAS_INCLUDE_DIRS})\n\n    else()\n        message(WARNING \"BLAS not found, please refer to \"\n        \"https://cmake.org/cmake/help/latest/module/FindBLAS.html#blas-lapack-vendors\"\n        \" to set correct LLAMA_BLAS_VENDOR\")\n    endif()\nendif()\n\nif (LLAMA_QKK_64)\n    add_compile_definitions(GGML_QKK_64)\nendif()\n\nif (LLAMA_CUBLAS)\n    cmake_minimum_required(VERSION 3.17)\n\n    find_package(CUDAToolkit)\n    if (CUDAToolkit_FOUND)\n        message(STATUS \"cuBLAS found\")\n\n        enable_language(CUDA)\n\n        set(GGML_HEADERS_CUDA ggml-cuda.h)\n        set(GGML_SOURCES_CUDA ggml-cuda.cu)\n\n        add_compile_definitions(GGML_USE_CUBLAS)\n#        if (LLAMA_CUDA_CUBLAS)\n#            add_compile_definitions(GGML_CUDA_CUBLAS)\n#        endif()\n        if (LLAMA_CUDA_FORCE_DMMV)\n            add_compile_definitions(GGML_CUDA_FORCE_DMMV)\n        endif()\n        if (LLAMA_CUDA_FORCE_MMQ)\n            add_compile_definitions(GGML_CUDA_FORCE_MMQ)\n        endif()\n        add_compile_definitions(GGML_CUDA_DMMV_X=${LLAMA_CUDA_DMMV_X})\n        add_compile_definitions(GGML_CUDA_MMV_Y=${LLAMA_CUDA_MMV_Y})\n        if (DEFINED LLAMA_CUDA_DMMV_Y)\n            add_compile_definitions(GGML_CUDA_MMV_Y=${LLAMA_CUDA_DMMV_Y}) # for backwards compatibility\n        endif()\n        if (LLAMA_CUDA_F16 OR LLAMA_CUDA_DMMV_F16)\n            add_compile_definitions(GGML_CUDA_F16)\n        endif()\n        add_compile_definitions(K_QUANTS_PER_ITERATION=${LLAMA_CUDA_KQUANTS_ITER})\n        add_compile_definitions(GGML_CUDA_PEER_MAX_BATCH_SIZE=${LLAMA_CUDA_PEER_MAX_BATCH_SIZE})\n\n        if (LLAMA_STATIC)\n            set(LLAMA_EXTRA_LIBS ${LLAMA_EXTRA_LIBS} CUDA::cudart_static CUDA::cublas_static CUDA::cublasLt_static)\n        else()\n            set(LLAMA_EXTRA_LIBS ${LLAMA_EXTRA_LIBS} CUDA::cudart CUDA::cublas CUDA::cublasLt)\n        endif()\n\n    if (NOT DEFINED CMAKE_CUDA_ARCHITECTURES)\n        # 52 == lowest CUDA 12 standard\n        # 60 == f16 CUDA intrinsics\n        # 61 == integer CUDA intrinsics\n        # 70 == compute capability at which unrolling a loop in mul_mat_q kernels is faster\n        if (LLAMA_CUDA_F16 OR LLAMA_CUDA_DMMV_F16)\n            set(CMAKE_CUDA_ARCHITECTURES \"60;61;70\") # needed for f16 CUDA intrinsics\n        else()\n            set(CMAKE_CUDA_ARCHITECTURES \"52;61;70\") # lowest CUDA 12 standard + lowest for integer intrinsics\n            #set(CMAKE_CUDA_ARCHITECTURES \"\") # use this to compile much faster, but only F16 models work\n        endif()\n    endif()\n    message(STATUS \"Using CUDA architectures: ${CMAKE_CUDA_ARCHITECTURES}\")\n\n    else()\n        message(WARNING \"cuBLAS not found\")\n    endif()\nendif()\n\nif (LLAMA_MPI)\n    cmake_minimum_required(VERSION 3.10)\n    find_package(MPI)\n    if (MPI_C_FOUND)\n        message(STATUS \"MPI found\")\n        set(GGML_HEADERS_MPI ggml-mpi.h)\n        set(GGML_SOURCES_MPI ggml-mpi.c ggml-mpi.h)\n        add_compile_definitions(GGML_USE_MPI)\n        add_compile_definitions(${MPI_C_COMPILE_DEFINITIONS})\n        if (NOT MSVC)\n            add_compile_options(-Wno-cast-qual)\n        endif()\n        set(LLAMA_EXTRA_LIBS     ${LLAMA_EXTRA_LIBS}     ${MPI_C_LIBRARIES})\n        set(LLAMA_EXTRA_INCLUDES ${LLAMA_EXTRA_INCLUDES} ${MPI_C_INCLUDE_DIRS})\n        # Even if you're only using the C header, C++ programs may bring in MPI\n        # C++ functions, so more linkage is needed\n        if (MPI_CXX_FOUND)\n            set(LLAMA_EXTRA_LIBS ${LLAMA_EXTRA_LIBS}     ${MPI_CXX_LIBRARIES})\n        endif()\n    else()\n        message(WARNING \"MPI not found\")\n    endif()\nendif()\n\nif (LLAMA_CLBLAST)\n    find_package(CLBlast)\n    if (CLBlast_FOUND)\n        message(STATUS \"CLBlast found\")\n\n        set(GGML_HEADERS_OPENCL ggml-opencl.h)\n        set(GGML_SOURCES_OPENCL ggml-opencl.cpp)\n\n        add_compile_definitions(GGML_USE_CLBLAST)\n\n        set(LLAMA_EXTRA_LIBS ${LLAMA_EXTRA_LIBS} clblast)\n    else()\n        message(WARNING \"CLBlast not found\")\n    endif()\nendif()\n\nif (LLAMA_HIPBLAS)\n    list(APPEND CMAKE_PREFIX_PATH /opt/rocm)\n    # enable fast atomic operation\n    add_compile_options(-munsafe-fp-atomics)\n\n    if (NOT ${CMAKE_C_COMPILER_ID} MATCHES \"Clang\")\n        message(WARNING \"Only LLVM is supported for HIP, hint: CC=/opt/rocm/llvm/bin/clang\")\n    endif()\n    if (NOT ${CMAKE_CXX_COMPILER_ID} MATCHES \"Clang\")\n        message(WARNING \"Only LLVM is supported for HIP, hint: CXX=/opt/rocm/llvm/bin/clang++\")\n    endif()\n\n    find_package(hip)\n    find_package(hipblas)\n    find_package(rocblas)\n\n    if (${hipblas_FOUND} AND ${hip_FOUND})\n        message(STATUS \"HIP and hipBLAS found\")\n        add_compile_definitions(GGML_USE_HIPBLAS GGML_USE_CUBLAS)\n        add_library(ggml-rocm OBJECT ggml-cuda.cu ggml-cuda.h)\n        if (BUILD_SHARED_LIBS)\n            set_target_properties(ggml-rocm PROPERTIES POSITION_INDEPENDENT_CODE ON)\n        endif()\n        if (LLAMA_CUDA_FORCE_DMMV)\n            target_compile_definitions(ggml-rocm PRIVATE GGML_CUDA_FORCE_DMMV)\n        endif()\n        if (LLAMA_CUDA_FORCE_MMQ)\n            target_compile_definitions(ggml-rocm PRIVATE GGML_CUDA_FORCE_MMQ)\n        endif()\n        target_compile_definitions(ggml-rocm PRIVATE GGML_CUDA_DMMV_X=${LLAMA_CUDA_DMMV_X})\n        target_compile_definitions(ggml-rocm PRIVATE GGML_CUDA_MMV_Y=${LLAMA_CUDA_MMV_Y})\n        target_compile_definitions(ggml-rocm PRIVATE K_QUANTS_PER_ITERATION=${LLAMA_CUDA_KQUANTS_ITER})\n        set_source_files_properties(ggml-cuda.cu PROPERTIES LANGUAGE CXX)\n        target_link_libraries(ggml-rocm PRIVATE hip::device PUBLIC hip::host roc::rocblas roc::hipblas)\n\n        if (LLAMA_STATIC)\n            message(FATAL_ERROR \"Static linking not supported for HIP/ROCm\")\n        endif()\n        set(LLAMA_EXTRA_LIBS ${LLAMA_EXTRA_LIBS} ggml-rocm)\n    else()\n        message(WARNING \"hipBLAS or HIP not found. Try setting CMAKE_PREFIX_PATH=/opt/rocm\")\n    endif()\nendif()\n\nif (LLAMA_ALL_WARNINGS)\n    if (NOT MSVC)\n        set(warning_flags -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function)\n        set(c_flags -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Werror=implicit-function-declaration)\n        set(cxx_flags -Wmissing-declarations -Wmissing-noreturn)\n        set(host_cxx_flags \"\")\n\n        if (CMAKE_C_COMPILER_ID MATCHES \"Clang\")\n            set(warning_flags ${warning_flags} -Wunreachable-code-break -Wunreachable-code-return)\n            set(host_cxx_flags ${host_cxx_flags} -Wmissing-prototypes -Wextra-semi)\n\n            if (\n                (CMAKE_C_COMPILER_ID STREQUAL \"Clang\"      AND CMAKE_C_COMPILER_VERSION VERSION_GREATER_EQUAL 3.8.0) OR\n                (CMAKE_C_COMPILER_ID STREQUAL \"AppleClang\" AND CMAKE_C_COMPILER_VERSION VERSION_GREATER_EQUAL 7.3.0)\n            )\n                set(c_flags ${c_flags} -Wdouble-promotion)\n            endif()\n        elseif (CMAKE_C_COMPILER_ID STREQUAL \"GNU\")\n            set(c_flags ${c_flags} -Wdouble-promotion)\n            set(host_cxx_flags ${host_cxx_flags} -Wno-array-bounds)\n\n            if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 7.1.0)\n                set(host_cxx_flags ${host_cxx_flags} -Wno-format-truncation)\n            endif()\n            if (CMAKE_CXX_COMPILER_VERSION VERSION_GREATER_EQUAL 8.1.0)\n                set(host_cxx_flags ${host_cxx_flags} -Wextra-semi)\n            endif()\n        endif()\n    else()\n        # todo : msvc\n    endif()\n\n    set(c_flags   ${c_flags}   ${warning_flags})\n    set(cxx_flags ${cxx_flags} ${warning_flags})\n    add_compile_options(\"$<$<COMPILE_LANGUAGE:C>:${c_flags}>\"\n                        \"$<$<COMPILE_LANGUAGE:CXX>:${cxx_flags}>\"\n                        \"$<$<COMPILE_LANGUAGE:CXX>:${host_cxx_flags}>\")\n\nendif()\n\nif (NOT MSVC)\n    set(cuda_flags -Wno-pedantic)\nendif()\nset(cuda_flags ${cxx_flags} -use_fast_math ${cuda_flags})\n\nlist(JOIN host_cxx_flags \" \" cuda_host_flags)  # pass host compiler flags as a single argument\nif (NOT cuda_host_flags STREQUAL \"\")\n    set(cuda_flags -forward-unknown-to-host-compiler ${cuda_flags} -Xcompiler ${cuda_host_flags})\nendif()\n\nadd_compile_options(\"$<$<COMPILE_LANGUAGE:CUDA>:${cuda_flags}>\")\n\nif (WIN32)\n    add_compile_definitions(_CRT_SECURE_NO_WARNINGS)\n\n    if (BUILD_SHARED_LIBS)\n        set(CMAKE_WINDOWS_EXPORT_ALL_SYMBOLS ON)\n    endif()\nendif()\n\nif (LLAMA_LTO)\n    include(CheckIPOSupported)\n    check_ipo_supported(RESULT result OUTPUT output)\n    if (result)\n        set(CMAKE_INTERPROCEDURAL_OPTIMIZATION TRUE)\n    else()\n        message(WARNING \"IPO is not supported: ${output}\")\n    endif()\nendif()\n\n# this version of Apple ld64 is buggy\nexecute_process(\n    COMMAND ${CMAKE_C_COMPILER} ${CMAKE_EXE_LINKER_FLAGS} -Wl,-v\n    ERROR_VARIABLE output\n)\nif (output MATCHES \"dyld-1015\\.7\")\n    add_compile_definitions(HAVE_BUGGY_APPLE_LINKER)\nendif()\n\n# Architecture specific\n# TODO: probably these flags need to be tweaked on some architectures\n#       feel free to update the Makefile for your architecture and send a pull request or issue\nmessage(STATUS \"CMAKE_SYSTEM_PROCESSOR: ${CMAKE_SYSTEM_PROCESSOR}\")\nif (MSVC)\n  string(TOLOWER \"${CMAKE_GENERATOR_PLATFORM}\" CMAKE_GENERATOR_PLATFORM_LWR)\n  message(STATUS \"CMAKE_GENERATOR_PLATFORM: ${CMAKE_GENERATOR_PLATFORM}\")\nelse ()\n  set(CMAKE_GENERATOR_PLATFORM_LWR \"\")\nendif ()\n\nif (NOT MSVC)\n    if (LLAMA_STATIC)\n        add_link_options(-static)\n        if (MINGW)\n            add_link_options(-static-libgcc -static-libstdc++)\n        endif()\n    endif()\n    if (LLAMA_GPROF)\n        add_compile_options(-pg)\n    endif()\nendif()\n\nif ((${CMAKE_SYSTEM_PROCESSOR} MATCHES \"arm\") OR (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"aarch64\") OR (\"${CMAKE_GENERATOR_PLATFORM_LWR}\" MATCHES \"arm64\"))\n    message(STATUS \"ARM detected\")\n    if (MSVC)\n        add_compile_definitions(__ARM_NEON)\n        add_compile_definitions(__ARM_FEATURE_FMA)\n        add_compile_definitions(__ARM_FEATURE_DOTPROD)\n        # add_compile_definitions(__ARM_FEATURE_FP16_VECTOR_ARITHMETIC) # MSVC doesn't support vdupq_n_f16, vld1q_f16, vst1q_f16\n        add_compile_definitions(__aarch64__) # MSVC defines _M_ARM64 instead\n    else()\n        check_cxx_compiler_flag(-mfp16-format=ieee COMPILER_SUPPORTS_FP16_FORMAT_I3E)\n        if (NOT \"${COMPILER_SUPPORTS_FP16_FORMAT_I3E}\" STREQUAL \"\")\n            add_compile_options(-mfp16-format=ieee)\n        endif()\n        if (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"armv6\")\n            # Raspberry Pi 1, Zero\n            add_compile_options(-mfpu=neon-fp-armv8 -mno-unaligned-access)\n        endif()\n        if (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"armv7\")\n            # Raspberry Pi 2\n            add_compile_options(-mfpu=neon-fp-armv8 -mno-unaligned-access -funsafe-math-optimizations)\n        endif()\n        if (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"armv8\")\n            # Raspberry Pi 3, 4, Zero 2 (32-bit)\n            add_compile_options(-mno-unaligned-access)\n        endif()\n    endif()\nelseif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"^(x86_64|i686|AMD64)$\" OR \"${CMAKE_GENERATOR_PLATFORM_LWR}\" MATCHES \"^(x86_64|i686|amd64|x64)$\" )\n    message(STATUS \"x86 detected\")\n    if (MSVC)\n        # instruction set detection for MSVC only\n        if (LLAMA_NATIVE)\n            include(cmake/FindSIMD.cmake)\n        endif ()\n        if (LLAMA_AVX512)\n            add_compile_options($<$<COMPILE_LANGUAGE:C>:/arch:AVX512>)\n            add_compile_options($<$<COMPILE_LANGUAGE:CXX>:/arch:AVX512>)\n            # MSVC has no compile-time flags enabling specific\n            # AVX512 extensions, neither it defines the\n            # macros corresponding to the extensions.\n            # Do it manually.\n            if (LLAMA_AVX512_VBMI)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:C>:__AVX512VBMI__>)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:CXX>:__AVX512VBMI__>)\n            endif()\n            if (LLAMA_AVX512_VNNI)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:C>:__AVX512VNNI__>)\n                add_compile_definitions($<$<COMPILE_LANGUAGE:CXX>:__AVX512VNNI__>)\n            endif()\n        elseif (LLAMA_AVX2)\n            add_compile_options($<$<COMPILE_LANGUAGE:C>:/arch:AVX2>)\n            add_compile_options($<$<COMPILE_LANGUAGE:CXX>:/arch:AVX2>)\n        elseif (LLAMA_AVX)\n            add_compile_options($<$<COMPILE_LANGUAGE:C>:/arch:AVX>)\n            add_compile_options($<$<COMPILE_LANGUAGE:CXX>:/arch:AVX>)\n        endif()\n    else()\n        if (LLAMA_NATIVE)\n            add_compile_options(-march=native)\n        endif()\n        if (LLAMA_F16C)\n            add_compile_options(-mf16c)\n        endif()\n        if (LLAMA_FMA)\n            add_compile_options(-mfma)\n        endif()\n        if (LLAMA_AVX)\n            add_compile_options(-mavx)\n        endif()\n        if (LLAMA_AVX2)\n            add_compile_options(-mavx2)\n        endif()\n        if (LLAMA_AVX512)\n            add_compile_options(-mavx512f)\n            add_compile_options(-mavx512bw)\n        endif()\n        if (LLAMA_AVX512_VBMI)\n            add_compile_options(-mavx512vbmi)\n        endif()\n        if (LLAMA_AVX512_VNNI)\n            add_compile_options(-mavx512vnni)\n        endif()\n    endif()\nelseif (${CMAKE_SYSTEM_PROCESSOR} MATCHES \"ppc64\")\n    message(STATUS \"PowerPC detected\")\n    add_compile_options(-mcpu=native -mtune=native)\n    #TODO: Add  targets for Power8/Power9 (Altivec/VSX) and Power10(MMA) and query for big endian systems (ppc64/le/be)\nelse()\n    message(STATUS \"Unknown architecture\")\nendif()\n\n#\n# POSIX conformance\n#\n\n# clock_gettime came in POSIX.1b (1993)\n# CLOCK_MONOTONIC came in POSIX.1-2001 / SUSv3 as optional\n# posix_memalign came in POSIX.1-2001 / SUSv3\n# M_PI is an XSI extension since POSIX.1-2001 / SUSv3, came in XPG1 (1985)\nadd_compile_definitions(_XOPEN_SOURCE=600)\n\n# Somehow in OpenBSD whenever POSIX conformance is specified\n# some string functions rely on locale_t availability,\n# which was introduced in POSIX.1-2008, forcing us to go higher\nif (CMAKE_SYSTEM_NAME MATCHES \"OpenBSD\")\n    remove_definitions(-D_XOPEN_SOURCE=600)\n    add_compile_definitions(_XOPEN_SOURCE=700)\nendif()\n\n# Data types, macros and functions related to controlling CPU affinity and\n# some memory allocation are available on Linux through GNU extensions in libc\nif (CMAKE_SYSTEM_NAME MATCHES \"Linux\")\n    add_compile_definitions(_GNU_SOURCE)\nendif()\n\n# RLIMIT_MEMLOCK came in BSD, is not specified in POSIX.1,\n# and on macOS its availability depends on enabling Darwin extensions\n# similarly on DragonFly, enabling BSD extensions is necessary\nif (\n    CMAKE_SYSTEM_NAME MATCHES \"Darwin\" OR\n    CMAKE_SYSTEM_NAME MATCHES \"iOS\" OR\n    CMAKE_SYSTEM_NAME MATCHES \"tvOS\" OR\n    CMAKE_SYSTEM_NAME MATCHES \"DragonFly\"\n)\n    add_compile_definitions(_DARWIN_C_SOURCE)\nendif()\n\n# alloca is a non-standard interface that is not visible on BSDs when\n# POSIX conformance is specified, but not all of them provide a clean way\n# to enable it in such cases\nif (CMAKE_SYSTEM_NAME MATCHES \"FreeBSD\")\n    add_compile_definitions(__BSD_VISIBLE)\nendif()\nif (CMAKE_SYSTEM_NAME MATCHES \"NetBSD\")\n    add_compile_definitions(_NETBSD_SOURCE)\nendif()\nif (CMAKE_SYSTEM_NAME MATCHES \"OpenBSD\")\n    add_compile_definitions(_BSD_SOURCE)\nendif()\n\n#\n# libraries\n#\n\n# ggml\n\nif (GGML_USE_CPU_HBM)\n    add_definitions(-DGGML_USE_CPU_HBM)\n    find_library(memkind memkind REQUIRED)\nendif()\n\nadd_library(ggml OBJECT\n            ggml.c\n            ggml.h\n            ggml-alloc.c\n            ggml-alloc.h\n            ggml-backend.c\n            ggml-backend.h\n            ggml-quants.c\n            ggml-quants.h\n            ${GGML_SOURCES_CUDA} ${GGML_HEADERS_CUDA}\n            ${GGML_SOURCES_OPENCL} ${GGML_HEADERS_OPENCL}\n            ${GGML_SOURCES_METAL} ${GGML_HEADERS_METAL}\n            ${GGML_SOURCES_MPI} ${GGML_HEADERS_MPI}\n            ${GGML_SOURCES_EXTRA} ${GGML_HEADERS_EXTRA}\n            )\n\ntarget_include_directories(ggml PUBLIC . ${LLAMA_EXTRA_INCLUDES})\ntarget_compile_features(ggml PUBLIC c_std_11) # don't bump\ntarget_link_libraries(ggml PUBLIC Threads::Threads ${LLAMA_EXTRA_LIBS})\nif (GGML_USE_CPU_HBM)\n    target_link_libraries(ggml PUBLIC memkind)\nendif()\n\nadd_library(ggml_static STATIC $<TARGET_OBJECTS:ggml>)\nif (BUILD_SHARED_LIBS)\n    set_target_properties(ggml PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    add_library(ggml_shared SHARED $<TARGET_OBJECTS:ggml>)\n    target_link_libraries(ggml_shared PUBLIC Threads::Threads ${LLAMA_EXTRA_LIBS})\n    install(TARGETS ggml_shared LIBRARY)\nendif()\n\n# llama\n\nadd_library(llama\n            llama.cpp\n            llama.h\n            )\n\ntarget_include_directories(llama PUBLIC .)\ntarget_compile_features(llama PUBLIC cxx_std_11) # don't bump\ntarget_link_libraries(llama PRIVATE\n    ggml\n    ${LLAMA_EXTRA_LIBS}\n    )\n\nif (BUILD_SHARED_LIBS)\n    set_target_properties(llama PROPERTIES POSITION_INDEPENDENT_CODE ON)\n    target_compile_definitions(llama PRIVATE LLAMA_SHARED LLAMA_BUILD)\n    if (LLAMA_METAL)\n        set_target_properties(llama PROPERTIES RESOURCE \"${CMAKE_CURRENT_SOURCE_DIR}/ggml-metal.metal\")\n    endif()\nendif()\n\n\n#\n# install\n#\n\ninclude(GNUInstallDirs)\ninclude(CMakePackageConfigHelpers)\n\nset(LLAMA_INCLUDE_INSTALL_DIR ${CMAKE_INSTALL_INCLUDEDIR}\n    CACHE PATH \"Location of header files\")\nset(LLAMA_LIB_INSTALL_DIR ${CMAKE_INSTALL_LIBDIR}\n    CACHE PATH \"Location of library files\")\nset(LLAMA_BIN_INSTALL_DIR ${CMAKE_INSTALL_BINDIR}\n    CACHE PATH \"Location of binary files\")\nset(LLAMA_BUILD_NUMBER ${BUILD_NUMBER})\nset(LLAMA_BUILD_COMMIT ${BUILD_COMMIT})\nset(LLAMA_INSTALL_VERSION 0.0.${BUILD_NUMBER})\nget_directory_property(LLAMA_TRANSIENT_DEFINES COMPILE_DEFINITIONS)\n\nconfigure_package_config_file(\n        ${CMAKE_CURRENT_SOURCE_DIR}/scripts/LlamaConfig.cmake.in\n        ${CMAKE_CURRENT_BINARY_DIR}/LlamaConfig.cmake\n    INSTALL_DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/Llama\n    PATH_VARS LLAMA_INCLUDE_INSTALL_DIR\n              LLAMA_LIB_INSTALL_DIR\n              LLAMA_BIN_INSTALL_DIR )\n\nwrite_basic_package_version_file(\n        ${CMAKE_CURRENT_BINARY_DIR}/LlamaConfigVersion.cmake\n    VERSION ${LLAMA_INSTALL_VERSION}\n    COMPATIBILITY SameMajorVersion)\n\ninstall(FILES ${CMAKE_CURRENT_BINARY_DIR}/LlamaConfig.cmake\n              ${CMAKE_CURRENT_BINARY_DIR}/LlamaConfigVersion.cmake\n        DESTINATION ${CMAKE_INSTALL_LIBDIR}/cmake/Llama)\n\nset(GGML_PUBLIC_HEADERS \"ggml.h\"\n        \"${GGML_HEADERS_CUDA}\" \"${GGML_HEADERS_OPENCL}\"\n        \"${GGML_HEADERS_METAL}\" \"${GGML_HEADERS_MPI}\" \"${GGML_HEADERS_EXTRA}\")\n\nset_target_properties(ggml PROPERTIES PUBLIC_HEADER \"${GGML_PUBLIC_HEADERS}\")\ninstall(TARGETS ggml PUBLIC_HEADER)\n\nset_target_properties(llama PROPERTIES PUBLIC_HEADER ${CMAKE_CURRENT_SOURCE_DIR}/llama.h)\ninstall(TARGETS llama LIBRARY PUBLIC_HEADER)\n\ninstall(\n    FILES convert.py\n    PERMISSIONS\n        OWNER_READ\n        OWNER_WRITE\n        OWNER_EXECUTE\n        GROUP_READ\n        GROUP_EXECUTE\n        WORLD_READ\n        WORLD_EXECUTE\n    DESTINATION ${CMAKE_INSTALL_BINDIR})\ninstall(\n    FILES convert-hf-to-powerinfer-gguf.py\n    PERMISSIONS\n        OWNER_READ\n        OWNER_WRITE\n        OWNER_EXECUTE\n        GROUP_READ\n        GROUP_EXECUTE\n        WORLD_READ\n        WORLD_EXECUTE\n    DESTINATION ${CMAKE_INSTALL_BINDIR})\nif (LLAMA_METAL)\n    install(\n        FILES ggml-metal.metal\n        PERMISSIONS\n            OWNER_READ\n            OWNER_WRITE\n            GROUP_READ\n            WORLD_READ\n        DESTINATION ${CMAKE_INSTALL_BINDIR})\nendif()\n\n#\n# programs, examples and tests\n#\n\nadd_subdirectory(common)\n\nif (LLAMA_BUILD_TESTS AND NOT CMAKE_JS_VERSION)\n    include(CTest)\n    add_subdirectory(tests)\nendif ()\n\nif (LLAMA_BUILD_EXAMPLES)\n    add_subdirectory(examples)\n    add_subdirectory(pocs)\nendif()\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.076171875,
          "content": "MIT License\n\nCopyright (c) 2023 Georgi Gerganov\nCopyright (c) 2023 SJTU-IPADS\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.\n"
        },
        {
          "name": "Package.swift",
          "type": "blob",
          "size": 1.8701171875,
          "content": "// swift-tools-version:5.5\n\nimport PackageDescription\n\n#if arch(arm) || arch(arm64)\nlet platforms: [SupportedPlatform]? = [\n    .macOS(.v12),\n    .iOS(.v14),\n    .watchOS(.v4),\n    .tvOS(.v14)\n]\nlet exclude: [String] = []\nlet resources: [Resource] = [\n    .process(\"ggml-metal.metal\")\n]\nlet additionalSources: [String] = [\"ggml-metal.m\"]\nlet additionalSettings: [CSetting] = [\n    .unsafeFlags([\"-fno-objc-arc\"]),\n    .define(\"GGML_USE_METAL\")\n]\n#else\nlet platforms: [SupportedPlatform]? = nil\nlet exclude: [String] = [\"ggml-metal.metal\"]\nlet resources: [Resource] = []\nlet additionalSources: [String] = []\nlet additionalSettings: [CSetting] = []\n#endif\n\nlet package = Package(\n    name: \"llama\",\n    platforms: platforms,\n    products: [\n        .library(name: \"llama\", targets: [\"llama\"]),\n    ],\n    targets: [\n        .target(\n            name: \"llama\",\n            path: \".\",\n            exclude: exclude,\n            sources: [\n                \"ggml.c\",\n                \"llama.cpp\",\n                \"ggml-alloc.c\",\n                \"ggml-backend.c\",\n                \"ggml-quants.c\",\n            ] + additionalSources,\n            resources: resources,\n            publicHeadersPath: \"spm-headers\",\n            cSettings: [\n                .unsafeFlags([\"-Wno-shorten-64-to-32\", \"-O3\", \"-DNDEBUG\"]),\n                .define(\"GGML_USE_ACCELERATE\")\n                // NOTE: NEW_LAPACK will required iOS version 16.4+\n                // We should consider add this in the future when we drop support for iOS 14\n                // (ref: ref: https://developer.apple.com/documentation/accelerate/1513264-cblas_sgemm?language=objc)\n                // .define(\"ACCELERATE_NEW_LAPACK\"),\n                // .define(\"ACCELERATE_LAPACK_ILP64\")\n            ] + additionalSettings,\n            linkerSettings: [\n                .linkedFramework(\"Accelerate\")\n            ]\n        )\n    ],\n    cxxLanguageStandard: .cxx11\n)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 20.578125,
          "content": "# PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU\n\n## TL;DR\nPowerInfer is a CPU/GPU LLM inference engine leveraging **activation locality** for your device.\n\n[![License: MIT](https://img.shields.io/badge/license-MIT-blue.svg)](https://opensource.org/licenses/MIT)\n\n[Project Kanban](https://github.com/orgs/SJTU-IPADS/projects/2/views/2)\n\n## Latest News ðŸ”¥\n- [2024/6/11] We are thrilled to introduce [PowerInfer-2](https://arxiv.org/abs/2406.06282), our highly optimized inference framework designed specifically for smartphones. With TurboSparse-Mixtral-47B, it achieves an impressive speed of 11.68 tokens per second, which is up to 22 times faster than other state-of-the-art frameworks.\n- [2024/6/11] We are thrilled to present [Turbo Sparse](https://arxiv.org/abs/2406.05955), our TurboSparse models for fast inference. With just $0.1M, we sparsified the original Mistral and Mixtral model to nearly 90% sparsity while maintaining superior performance! For a Mixtral-level model, our TurboSparse-Mixtral activates only **4B** parameters!\n- [2024/5/20] **Competition Recruitment: CCF-TCArch Customized Computing Challenge 2024**. The CCF TCARCH CCC is a national competition organized by the Technical Committee on Computer Architecture (TCARCH) of the China Computer Federation (CCF). This year's competition aims to optimize the PowerInfer inference engine using the open-source ROCm/HIP. More information about the competition can be found [here](https://ccf-tcarch-ccc.github.io/2024/).\n- [2024/5/17] We now provide support for AMD devices with ROCm.\n- [2024/3/28] We are trilled to present [Bamboo LLM](https://github.com/SJTU-IPADS/Bamboo) that achieves both top-level performance and unparalleled speed with PowerInfer! Experience it with Bamboo-7B [Base](https://huggingface.co/PowerInfer/Bamboo-base-v0.1-gguf) / [DPO](https://huggingface.co/PowerInfer/Bamboo-DPO-v0.1-gguf).\n- [2024/3/14] We supported ProSparse Llama 2 ([7B](https://huggingface.co/SparseLLM/prosparse-llama-2-7b)/[13B](https://huggingface.co/SparseLLM/prosparse-llama-2-13b)), ReLU models with ~90% sparsity, matching original Llama 2's performance (Thanks THUNLP & ModelBest)!\n- [2024/1/11] We supported Windows with GPU inference!\n- [2023/12/24] We released an online [gradio demo](https://powerinfer-gradio.vercel.app/) for Falcon(ReLU)-40B-FP16!\n- [2023/12/19] We officially released PowerInfer!\n\n## Demo ðŸ”¥\n\nhttps://github.com/SJTU-IPADS/PowerInfer/assets/34213478/fe441a42-5fce-448b-a3e5-ea4abb43ba23\n\nPowerInfer v.s. llama.cpp on a single RTX 4090(24G) running Falcon(ReLU)-40B-FP16 with a 11x speedup!\n\n<sub>Both PowerInfer and llama.cpp were running on the same hardware and fully utilized VRAM on RTX 4090.</sub>\n\n> [!NOTE]\n> **Live Demo Onlineâš¡ï¸**\n>\n> Try out our [Gradio server](https://powerinfer-gradio.vercel.app/) hosting Falcon(ReLU)-40B-FP16 on a RTX 4090!\n>\n> <sub>Experimental and without warranties ðŸš§</sub>\n\n## Abstract\n\nWe introduce PowerInfer, a high-speed Large Language Model (LLM) inference engine on a personal computer (PC)\nequipped with a single consumer-grade GPU. The key underlying the design of PowerInfer is exploiting the high **locality**\ninherent in LLM inference, characterized by a power-law distribution in neuron activation.\n\nThis distribution indicates that a small subset of neurons, termed hot neurons, are consistently activated\nacross inputs, while the majority, cold neurons, vary based on specific inputs.\nPowerInfer exploits such an insight to design a GPU-CPU hybrid inference engine:\nhot-activated neurons are preloaded onto the GPU for fast access, while cold-activated neurons are computed\non the CPU, thus significantly reducing GPU memory demands and CPU-GPU data transfers.\nPowerInfer further integrates adaptive predictors and neuron-aware sparse operators,\noptimizing the efficiency of neuron activation and computational sparsity.\n\nEvaluation shows that PowerInfer attains an average token generation rate of 13.20 tokens/s, with a peak of 29.08 tokens/s, across various LLMs (including OPT-175B) on a single NVIDIA RTX 4090 GPU,\nonly 18\\% lower than that achieved by a top-tier server-grade A100 GPU.\nThis significantly outperforms llama.cpp by up to 11.69x while retaining model accuracy.\n\n## Features\nPowerInfer is a high-speed and easy-to-use inference engine for deploying LLMs locally.\n\nPowerInfer is fast with:\n\n- **Locality-centric design**: Utilizes sparse activation and 'hot'/'cold' neuron concept for efficient LLM inference, ensuring high speed with lower resource demands.\n- **Hybrid CPU/GPU Utilization**: Seamlessly integrates memory/computation capabilities of CPU and GPU for a balanced workload and faster processing.\n\nPowerInfer is flexible and easy to use with:\n\n- **Easy Integration**: Compatible with popular [ReLU-sparse models](https://huggingface.co/SparseLLM).\n- **Local Deployment Ease**: Designed and deeply optimized for local deployment on consumer-grade hardware, enabling low-latency LLM inference and serving on a single GPU.\n- **Backward Compatibility**: While distinct from llama.cpp, you can make use of most of `examples/` the same way as llama.cpp such as server and batched generation. PowerInfer also supports inference with llama.cpp's model weights for compatibility purposes, but there will be no performance gain.\n\nYou can use these models with PowerInfer today:\n\n- Falcon-40B\n- Llama2 family\n- ProSparse Llama2 family\n- Bamboo-7B\n\nWe have tested PowerInfer on the following platforms:\n\n- x86-64 CPUs with AVX2 instructions, with or without NVIDIA GPUs, under **Linux**.\n- x86-64 CPUs with AVX2 instructions, with or without NVIDIA GPUs, under **Windows**.\n- Apple M Chips (CPU only) on **macOS**. (As we do not optimize for Mac, the performance improvement is not significant now.)\n\nAnd new features coming soon:\n\n- Metal backend for sparse inference on macOS\n\nPlease kindly refer to our [Project Kanban](https://github.com/orgs/SJTU-IPADS/projects/2/views/2) for our current focus of development.\n\n## Getting Started\n\n- [Installation](#setup-and-installation)\n- [Model Weights](#model-weights)\n- [Inference](#inference)\n\n## Setup and Installation\n\n### Pre-requisites\n\nPowerInfer requires the following dependencies:\n\n- CMake (3.17+)\n- Python (3.8+) and pip (19.3+), for converting model weights and automatic FFN offloading\n\n### Get the Code\n\n```bash\ngit clone https://github.com/SJTU-IPADS/PowerInfer\ncd PowerInfer\npip install -r requirements.txt # install Python helpers' dependencies\n```\n### Build\n\nIn order to build PowerInfer you have two different options. These commands are supposed to be run from the root directory of the project.\n\nUsing `CMake`(3.17+):\n* If you have an NVIDIA GPU:\n```bash\ncmake -S . -B build -DLLAMA_CUBLAS=ON\ncmake --build build --config Release\n```\n* If you have an AMD GPU:\n```bash\n# Replace '1100' to your card architecture name, you can get it by rocminfo\nCC=/opt/rocm/llvm/bin/clang CXX=/opt/rocm/llvm/bin/clang++ cmake -S . -B build -DLLAMA_HIPBLAS=ON -DAMDGPU_TARGETS=gfx1100\ncmake --build build --config Release\n```\n\n* If you have just CPU:\n\n```bash\ncmake -S . -B build\ncmake --build build --config Release\n```\n\n## Model Weights\n\nPowerInfer models are stored in a special format called *PowerInfer GGUF* based on GGUF format, consisting of both LLM weights and predictor weights.\n\n### Download PowerInfer GGUF via Hugging Face\n\nYou can obtain PowerInfer GGUF weights at `*.powerinfer.gguf` as well as profiled model activation statistics for 'hot'-neuron offloading from each Hugging Face repo below.\n\n| Base Model            | PowerInfer GGUF                                                                                               |\n| --------------------- | ------------------------------------------------------------------------------------------------------------- |\n| LLaMA(ReLU)-2-7B      | [PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF](https://huggingface.co/PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF)     |\n| LLaMA(ReLU)-2-13B     | [PowerInfer/ReluLLaMA-13B-PowerInfer-GGUF](https://huggingface.co/PowerInfer/ReluLLaMA-13B-PowerInfer-GGUF)   |\n| Falcon(ReLU)-40B      | [PowerInfer/ReluFalcon-40B-PowerInfer-GGUF](https://huggingface.co/PowerInfer/ReluFalcon-40B-PowerInfer-GGUF) |\n| LLaMA(ReLU)-2-70B     | [PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF](https://huggingface.co/PowerInfer/ReluLLaMA-70B-PowerInfer-GGUF)   |\n| ProSparse-LLaMA-2-7B  | [PowerInfer/ProSparse-LLaMA-2-7B-GGUF](https://huggingface.co/PowerInfer/prosparse-llama-2-7b-gguf)           |\n| ProSparse-LLaMA-2-13B | [PowerInfer/ProSparse-LLaMA-2-13B-GGUF](https://huggingface.co/PowerInfer/prosparse-llama-2-13b-gguf)         |\n| Bamboo-base-7B ðŸŒŸ      | [PowerInfer/Bamboo-base-v0.1-gguf](https://huggingface.co/PowerInfer/Bamboo-base-v0.1-gguf)                   |\n| Bamboo-DPO-7B ðŸŒŸ       | [PowerInfer/Bamboo-DPO-v0.1-gguf](https://huggingface.co/PowerInfer/Bamboo-DPO-v0.1-gguf)                     |\n\nWe recommend using [`huggingface-cli`](https://huggingface.co/docs/huggingface_hub/guides/cli) to download the whole model repo. For example, the following command will download [PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF](https://huggingface.co/PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF) into the `./ReluLLaMA-7B` directory.\n\n```shell\nhuggingface-cli download --resume-download --local-dir ReluLLaMA-7B --local-dir-use-symlinks False PowerInfer/ReluLLaMA-7B-PowerInfer-GGUF\n```\n\nAs such, PowerInfer can automatically make use of the following directory structure for feature-complete model offloading:\n```\n.\nâ”œâ”€â”€ *.powerinfer.gguf (Unquantized PowerInfer model)\nâ”œâ”€â”€ *.q4.powerinfer.gguf (INT4 quantized PowerInfer model, if available)\nâ”œâ”€â”€ activation (Profiled activation statistics for fine-grained FFN offloading)\nâ”‚   â”œâ”€â”€ activation_x.pt (Profiled activation statistics for layer x)\nâ”‚   â””â”€â”€ ...\nâ”œâ”€â”€ *.[q4].powerinfer.gguf.generated.gpuidx (Generated GPU index at runtime for corresponding model)\n```\n\n### Convert from Original Model Weights + Predictor Weights\n\nHugging Face limits single model weight to 50GiB. For unquantized models >= 40B, you can convert PowerInfer GGUF from the original model weights and predictor weights obtained from Hugging Face.\n\n| Base Model            | Original Model                                                                            | Predictor                                                                                                       |\n| --------------------- | ----------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------- |\n| LLaMA(ReLU)-2-7B      | [SparseLLM/ReluLLaMA-7B](https://huggingface.co/SparseLLM/ReluLLaMA-7B)                   | [PowerInfer/ReluLLaMA-7B-Predictor](https://huggingface.co/PowerInfer/ReluLLaMA-7B-Predictor)                   |\n| LLaMA(ReLU)-2-13B     | [SparseLLM/ReluLLaMA-13B](https://huggingface.co/SparseLLM/ReluLLaMA-13B)                 | [PowerInfer/ReluLLaMA-13B-Predictor](https://huggingface.co/PowerInfer/ReluLLaMA-13B-Predictor)                 |\n| Falcon(ReLU)-40B      | [SparseLLM/ReluFalcon-40B](https://huggingface.co/SparseLLM/ReluFalcon-40B)               | [PowerInfer/ReluFalcon-40B-Predictor](https://huggingface.co/PowerInfer/ReluFalcon-40B-Predictor)               |\n| LLaMA(ReLU)-2-70B     | [SparseLLM/ReluLLaMA-70B](https://huggingface.co/SparseLLM/ReluLLaMA-70B)                 | [PowerInfer/ReluLLaMA-70B-Predictor](https://huggingface.co/PowerInfer/ReluLLaMA-70B-Predictor)                 |\n| ProSparse-LLaMA-2-7B  | [SparseLLM/ProSparse-LLaMA-2-7B](https://huggingface.co/SparseLLM/prosparse-llama-2-7b)   | [PowerInfer/ProSparse-LLaMA-2-7B-Predictor](https://huggingface.co/PowerInfer/prosparse-llama-2-7b-predictor)   |\n| ProSparse-LLaMA-2-13B | [SparseLLM/ProSparse-LLaMA-2-13B](https://huggingface.co/SparseLLM/prosparse-llama-2-13b) | [PowerInfer/ProSparse-LLaMA-2-13B-Predictor](https://huggingface.co/PowerInfer/prosparse-llama-2-13b-predictor) |\n| Bamboo-base-7B ðŸŒŸ      | [PowerInfer/Bamboo-base-v0.1](https://huggingface.co/PowerInfer/Bamboo-base-v0_1)         | [PowerInfer/Bamboo-base-v0.1-predictor](https://huggingface.co/PowerInfer/Bamboo-base-v0.1-predictor)           |\n| Bamboo-DPO-7B ðŸŒŸ       | [PowerInfer/Bamboo-DPO-v0.1](https://huggingface.co/PowerInfer/Bamboo-DPO-v0_1)           | [PowerInfer/Bamboo-DPO-v0.1-predictor](https://huggingface.co/PowerInfer/Bamboo-DPO-v0.1-predictor)             |\n\nYou can use the following command to convert the original model weights and predictor weights to PowerInfer GGUF:\n```bash\n# make sure that you have done `pip install -r requirements.txt`\npython convert.py --outfile /PATH/TO/POWERINFER/GGUF/REPO/MODELNAME.powerinfer.gguf /PATH/TO/ORIGINAL/MODEL /PATH/TO/PREDICTOR\n# python convert.py --outfile ./ReluLLaMA-70B-PowerInfer-GGUF/llama-70b-relu.powerinfer.gguf ./SparseLLM/ReluLLaMA-70B ./PowerInfer/ReluLLaMA-70B-Predictor\n```\nFor the same reason, we suggest keeping the same directory structure as PowerInfer GGUF repos after conversion.\n\n<details>\n\n<summary>Convert Original models into dense GGUF models(compatible with llama.cpp)</summary>\n\n```bash\npython convert-dense.py --outfile /PATH/TO/DENSE/GGUF/REPO/MODELNAME.gguf /PATH/TO/ORIGINAL/MODEL\n# python convert-dense.py --outfile ./Bamboo-DPO-v0.1-gguf/bamboo-7b-dpo-v0.1.gguf --outtype f16 ./Bamboo-DPO-v0.1\n```\n\nPlease note that the generated dense GGUF models might not work properly with llama.cpp, as we have altered activation functions (for ReluLLaMA and Prosparse models), or the model architecture (for Bamboo models). The dense GGUF models generated by convert-dense.py can be used for PowerInfer in dense inference mode, but might not work properly with llama.cpp.\n\n</details>\n\n## Inference\n\nFor CPU-only and CPU-GPU hybrid inference with all available VRAM, you can use the following instructions to run PowerInfer:\n```bash\n./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt\n# e.g.: ./build/bin/main -m ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf -n 128 -t 8 -p \"Once upon a time\"\n# For Windows: .\\build\\bin\\Release\\main.exe -m .\\ReluFalcon-40B-PowerInfer-GGUF\\falcon-40b-relu.q4.powerinfer.gguf -n 128 -t 8 -p \"Once upon a time\"\n```\n\nIf you want to limit the VRAM usage of GPU:\n```bash\n./build/bin/main -m /PATH/TO/MODEL -n $output_token_count -t $thread_num -p $prompt --vram-budget $vram_gb\n# e.g.: ./build/bin/main -m ./ReluLLaMA-7B-PowerInfer-GGUF/llama-7b-relu.powerinfer.gguf -n 128 -t 8 -p \"Once upon a time\" --vram-budget 8\n# For Windows: .\\build\\bin\\Release\\main.exe -m .\\ReluLLaMA-7B-PowerInfer-GGUF\\llama-7b-relu.powerinfer.gguf -n 128 -t 8 -p \"Once upon a time\" --vram-budget 8\n```\nUnder CPU-GPU hybrid inference, PowerInfer will automatically offload all dense activation blocks to GPU, then split FFN and offload to GPU if possible.\n\n<details>\n<summary>Dense inference mode (limited support)</summary>\n\nIf you want to run PowerInfer to infer with the dense variants of the PowerInfer model family, you can use similarly as llama.cpp does:\n\n```bash\n./build/bin/main -m /PATH/TO/DENSE/MODEL -n $output_token_count -t $thread_num -p $prompt -ngl $num_gpu_layers\n# e.g.: ./build/bin/main -m ./Bamboo-base-v0.1-gguf/bamboo-7b-v0.1.gguf -n 128 -t 8 -p \"Once upon a time\" -ngl 12\n```\n\nSo is the case for other `examples/` like `server` and `batched_generation`. Please note that the dense inference mode is not a \"compatible mode\" for all models. We have altered activation functions (for ReluLLaMA and Prosparse models) in this mode to match with our model family. \n\n</details>\n\n## Serving, Perplexity Evaluation, and more applications\n\nPowerInfer supports serving and batched generation with the same instructions as llama.cpp. Generally, you can use the same command as llama.cpp, except for `-ngl` argument which has been replaced by `--vram-budget` for PowerInfer. Please refer to the detailed instructions in each `examples/` directory. For example:\n\n- [Serving](./examples/server/README.md)\n- [Perplexity Evaluation](./examples/perplexity/README.md)\n- [Batched Generation](./examples/batched/README.md)\n\n## Quantization\n\nPowerInfer has optimized quantization support for INT4(`Q4_0`) models. You can use the following instructions to quantize PowerInfer GGUF model:\n```bash\n./build/bin/quantize /PATH/TO/MODEL /PATH/TO/OUTPUT/QUANTIZED/MODEL Q4_0\n# e.g.: ./build/bin/quantize ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.powerinfer.gguf ./ReluFalcon-40B-PowerInfer-GGUF/falcon-40b-relu.q4.powerinfer.gguf Q4_0\n# For Windows: .\\build\\bin\\Release\\quantize.exe .\\ReluFalcon-40B-PowerInfer-GGUF\\falcon-40b-relu.powerinfer.gguf .\\ReluFalcon-40B-PowerInfer-GGUF\\falcon-40b-relu.q4.powerinfer.gguf Q4_0\n```\nThen you can use the quantized model for inference with PowerInfer with the same instructions as above.\n\n## More Documentation\n- [Performance troubleshooting](./docs/token_generation_performance_tips.md)\n\n## Evaluation\n\nWe evaluated PowerInfer vs. llama.cpp on a single RTX 4090(24G) with a series of FP16 ReLU models under inputs of length 64, and the results are shown below. PowerInfer achieves up to 11x speedup on Falcon 40B and up to 3x speedup on Llama 2 70B.\n\n![github-eval-4090](https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/d700fa6c-77ba-462f-a2fc-3fd21c898f33)\n<sub>The X axis indicates the output length, and the Y axis represents the speedup compared with llama.cpp. The number above each bar indicates the end-to-end generation speed (total prompting + generation time / total tokens generated, in tokens/s).</sub>\n\nWe also evaluated PowerInfer on a single RTX 2080Ti(11G) with INT4 ReLU models under inputs of length 8, and the results are illustrated in the same way as above. PowerInfer achieves up to 8x speedup on Falcon 40B and up to 3x speedup on Llama 2 70B.\n\n![github-eval-2080ti-q4](https://github.com/SJTU-IPADS/PowerInfer/assets/34213478/0fc1bfc4-aafc-4e82-a865-bec0143aff1a)\n\nPlease refer to our [paper](https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf) for more evaluation details.\n\n## FAQs\n1. What if I encountered `CUDA_ERROR_OUT_OF_MEMORY`?\n   - You can try to run with `--reset-gpu-index` argument to rebuild the GPU index for this model to avoid any stale cache.\n   - Due to our current implementation, model offloading might not be as accurate as expected. You can try with `--vram-budget` with a slightly lower value or `--disable-gpu-index` to disable FFN offloading.\n\n2. Does PowerInfer support mistral, original llama, Qwen, ...?\n   - Now we only support models with ReLU/ReGLU/Squared ReLU activation function. So we do not support these models now. It's worth mentioning that a [paper](https://arxiv.org/pdf/2310.04564.pdf) has demonstrated that using the ReLU/ReGLU activation function has a negligible impact on convergence and performance.\n\n3. Why is there a noticeable downgrade in the performance metrics of our current ReLU model, particularly the 70B model?\n   - In contrast to the typical requirement of around 2T tokens for LLM training, our model's fine-tuning was conducted with only 5B tokens. This insufficient retraining has resulted in the model's inability to regain its original performance. We are actively working on updating to a more capable model, so please stay tuned.\n\n4. What if...\n   - Issues are welcomed! Please feel free to open an issue and attach your running environment and running parameters. We will try our best to help you.\n\n## TODOs\nWe will release the code and data in the following order, please stay tuned!\n\n- [x] Release core code of PowerInfer, supporting Llama-2, Falcon-40B.\n- [x] Support ~~Mistral-7B~~ (Bamboo-7B)\n- [x] Support Windows\n- [ ] Support text-generation-webui\n- [x] Release perplexity evaluation code\n- [ ] Support Metal for Mac\n- [ ] Release code for OPT models\n- [ ] Release predictor training code\n- [x] Support online split for FFN network\n- [ ] Support Multi-GPU\n\n\n## Paper and Citation\nMore technical details can be found in our [paper](https://ipads.se.sjtu.edu.cn/_media/publications/powerinfer-20231219.pdf).\n\nIf you find PowerInfer useful or relevant to your project and research, please kindly cite our paper:\n\n```bibtex\n@misc{song2023powerinfer,\n      title={PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU},\n      author={Yixin Song and Zeyu Mi and Haotong Xie and Haibo Chen},\n      year={2023},\n      eprint={2312.12456},\n      archivePrefix={arXiv},\n      primaryClass={cs.LG}\n}\n```\n\n## Acknowledgement\nWe are thankful for the easily modifiable operator library [ggml](https://github.com/ggerganov/ggml) and execution runtime provided by [llama.cpp](https://github.com/ggerganov/llama.cpp). We also extend our gratitude to [THUNLP](https://nlp.csai.tsinghua.edu.cn/) for their support of ReLU-based sparse models. We also appreciate the research of [Deja Vu](https://proceedings.mlr.press/v202/liu23am.html), which inspires PowerInfer.\n"
        },
        {
          "name": "SHA256SUMS",
          "type": "blob",
          "size": 3.7392578125,
          "content": "700df0d3013b703a806d2ae7f1bfb8e59814e3d06ae78be0c66368a50059f33d  models/7B/consolidated.00.pth\n666a4bb533b303bdaf89e1b6a3b6f93535d868de31d903afdc20983dc526c847  models/7B/ggml-model-f16.bin\nec2f2d1f0dfb73b72a4cbac7fa121abbe04c37ab327125a38248f930c0f09ddf  models/7B/ggml-model-q4_0.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/7B/ggml-model-q4_1.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/7B/ggml-model-q5_0.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/7B/ggml-model-q5_1.bin\n7e89e242ddc0dd6f060b43ca219ce8b3e8f08959a72cb3c0855df8bb04d46265  models/7B/params.json\n745bf4e29a4dd6f411e72976d92b452da1b49168a4f41c951cfcc8051823cf08  models/13B/consolidated.00.pth\nd5ccbcc465c71c0de439a5aeffebe8344c68a519bce70bc7f9f92654ee567085  models/13B/consolidated.01.pth\n2b206e9b21fb1076f11cafc624e2af97c9e48ea09312a0962153acc20d45f808  models/13B/ggml-model-f16.bin\nfad169e6f0f575402cf75945961cb4a8ecd824ba4da6be2af831f320c4348fa5  models/13B/ggml-model-q4_0.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/13B/ggml-model-q4_1.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/13B/ggml-model-q5_0.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/13B/ggml-model-q5_1.bin\n4ab77bec4d4405ccb66a97b282574c89a94417e3c32e5f68f37e2876fc21322f  models/13B/params.json\ne23294a58552d8cdec5b7e8abb87993b97ea6eced4178ff2697c02472539d067  models/30B/consolidated.00.pth\n4e077b7136c7ae2302e954860cf64930458d3076fcde9443f4d0e939e95903ff  models/30B/consolidated.01.pth\n24a87f01028cbd3a12de551dcedb712346c0b5cbdeff1454e0ddf2df9b675378  models/30B/consolidated.02.pth\n1adfcef71420886119544949767f6a56cb6339b4d5fcde755d80fe68b49de93b  models/30B/consolidated.03.pth\n7e1b524061a9f4b27c22a12d6d2a5bf13b8ebbea73e99f218809351ed9cf7d37  models/30B/ggml-model-f16.bin\nd2a441403944819492ec8c2002cc36fa38468149bfb4b7b4c52afc7bd9a7166d  models/30B/ggml-model-q4_0.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/30B/ggml-model-q4_1.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/30B/ggml-model-q5_0.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/30B/ggml-model-q5_1.bin\n2c07118ea98d69dbe7810d88520e30288fa994751b337f8fca02b171955f44cb  models/30B/params.json\n135c563f6b3938114458183afb01adc9a63bef3d8ff7cccc3977e5d3664ecafe  models/65B/consolidated.00.pth\n9a600b37b19d38c7e43809485f70d17d1dc12206c07efa83bc72bb498a568bde  models/65B/consolidated.01.pth\ne7babf7c5606f165a3756f527cb0fedc4f83e67ef1290391e52fb1cce5f26770  models/65B/consolidated.02.pth\n73176ffb426b40482f2aa67ae1217ef79fbbd1fff5482bae5060cdc5a24ab70e  models/65B/consolidated.03.pth\n882e6431d0b08a8bc66261a0d3607da21cbaeafa96a24e7e59777632dbdac225  models/65B/consolidated.04.pth\na287c0dfe49081626567c7fe87f74cce5831f58e459b427b5e05567641f47b78  models/65B/consolidated.05.pth\n72b4eba67a1a3b18cb67a85b70f8f1640caae9b40033ea943fb166bd80a7b36b  models/65B/consolidated.06.pth\nd27f5b0677d7ff129ceacd73fd461c4d06910ad7787cf217b249948c3f3bc638  models/65B/consolidated.07.pth\n60758f2384d74e423dffddfd020ffed9d3bb186ebc54506f9c4a787d0f5367b0  models/65B/ggml-model-f16.bin\ncde053439fa4910ae454407e2717cc46cc2c2b4995c00c93297a2b52e790fa92  models/65B/ggml-model-q4_0.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/65B/ggml-model-q4_1.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/65B/ggml-model-q5_0.bin\nffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffffff  models/65B/ggml-model-q5_1.bin\n999ed1659b469ccc2a941714c0a9656fa571d17c9f7c8c7589817ca90edef51b  models/65B/params.json\n9e556afd44213b6bd1be2b850ebbbd98f5481437a8021afaf58ee7fb1818d347  models/tokenizer.model\n"
        },
        {
          "name": "atomic_windows.h",
          "type": "blob",
          "size": 24.154296875,
          "content": "/*\n * C11 <stdatomic.h> emulation header\n *\n * PLEASE LICENSE, (C) 2022, Michael Clark <michaeljclark@mac.com>\n *\n * All rights to this work are granted for all purposes, with exception of\n * author's implied right of copyright to defend the free use of this work.\n *\n * THE SOFTWARE IS PROVIDED \"AS IS\" AND THE AUTHOR DISCLAIMS ALL WARRANTIES\n * WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n * MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL THE AUTHOR BE LIABLE FOR\n * ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n * WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n * ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT OF\n * OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n */\n#ifdef _WIN32\n\n/*\n * C11 <stdatomic.h> emulation\n *\n * This header can be included from C and uses C11 _Generic selection.\n * This header requires MSVC flags: \"/O2 /TC /std:c11 /volatile:iso\".\n *\n * Note: some primitives may be missing and some primitives may haver\n * stronger ordering than is required thus not produce optimal code,\n * and some primitives may be buggy.\n */\n#include <windows.h>\n#include \"winnt.h\"\n\n#define __concat2(x,y) x ## y\n#define __concat3(x,y,z) x ## y ## z\n\n#if UINTPTR_MAX == 0xFFFFFFFFFFFFFFFFull\n#define __intptr __int64\n#define __ptr i64\n#elif UINTPTR_MAX == 0xFFFFFFFFu\n#define __intptr __int32\n#define __ptr i32\n#else\n#error unable to determine pointer width\n#endif\n\n#define _Atomic volatile\n\n#define ATOMIC_BOOL_LOCK_FREE 1\n#define ATOMIC_CHAR_LOCK_FREE 1\n#define ATOMIC_SHORT_LOCK_FREE 1\n#define ATOMIC_INT_LOCK_FREE 1\n#define ATOMIC_LONG_LOCK_FREE 1\n#define ATOMIC_LLONG_LOCK_FREE 1\n#define ATOMIC_POINTER_LOCK_FREE 1\n\n#define ATOMIC_FLAG_INIT { 0 }\n\n#define __ATOMIC_RELAXED 0\n#define __ATOMIC_CONSUME 1\n#define __ATOMIC_ACQUIRE 2\n#define __ATOMIC_RELEASE 3\n#define __ATOMIC_ACQ_REL 4\n#define __ATOMIC_SEQ_CST 5\n\ntypedef enum memory_order {\n    memory_order_relaxed = __ATOMIC_RELAXED,\n    memory_order_consume = __ATOMIC_CONSUME,\n    memory_order_acquire = __ATOMIC_ACQUIRE,\n    memory_order_release = __ATOMIC_RELEASE,\n    memory_order_acq_rel = __ATOMIC_ACQ_REL,\n    memory_order_seq_cst = __ATOMIC_SEQ_CST\n} memory_order;\n\ntypedef long long llong;\ntypedef unsigned char uchar;\ntypedef unsigned short ushort;\ntypedef unsigned int uint;\ntypedef unsigned long ulong;\ntypedef unsigned long long ullong;\n\ntypedef _Atomic _Bool atomic_bool;\ntypedef _Atomic char atomic_char;\ntypedef _Atomic unsigned char atomic_uchar;\ntypedef _Atomic short atomic_short;\ntypedef _Atomic unsigned short atomic_ushort;\ntypedef _Atomic int atomic_int;\ntypedef _Atomic unsigned int atomic_uint;\ntypedef _Atomic long atomic_long;\ntypedef _Atomic unsigned long atomic_ulong;\ntypedef _Atomic long long atomic_llong;\ntypedef _Atomic unsigned long long atomic_ullong;\ntypedef _Atomic intptr_t atomic_intptr_t;\ntypedef _Atomic uintptr_t atomic_uintptr_t;\ntypedef _Atomic size_t atomic_size_t;\ntypedef _Atomic ptrdiff_t atomic_ptrdiff_t;\ntypedef _Atomic intmax_t atomic_intmax_t;\ntypedef _Atomic uintmax_t atomic_uintmax_t;\ntypedef void* _Atomic atomic_ptr;\n\ntypedef struct atomic_flag { atomic_bool _Value; } atomic_flag;\n\nstatic inline __int8  __msvc_xchg_i8(__int8 volatile* addr, __int8 val)\n{\n    return _InterlockedExchange8(addr, val);\n}\nstatic inline __int16 __msvc_xchg_i16(__int16 volatile* addr, __int16 val)\n{\n    return _InterlockedExchange16(addr, val);\n}\nstatic inline __int32 __msvc_xchg_i32(__int32 volatile* addr, __int32 val)\n{\n    return _InterlockedExchange(addr, val);\n}\nstatic inline __int64 __msvc_xchg_i64(__int64 volatile* addr, __int64 val)\n{\n    return _InterlockedExchange64(addr, val);\n}\n\n#define __msvc_xchg_ptr(ptr) __concat2(__msvc_xchg_,ptr)\n\nstatic inline char __c11_atomic_exchange__atomic_char(atomic_char* obj, char desired)\n{\n    return (char)__msvc_xchg_i8((__int8 volatile*)obj, (__int8)desired);\n}\nstatic inline short __c11_atomic_exchange__atomic_short(atomic_short* obj, short desired)\n{\n    return (short)__msvc_xchg_i16((__int16 volatile*)obj, (__int16)desired);\n}\nstatic inline int __c11_atomic_exchange__atomic_int(atomic_int* obj, int desired)\n{\n    return (int)__msvc_xchg_i32((__int32 volatile*)obj, (__int32)desired);\n}\nstatic inline long __c11_atomic_exchange__atomic_long(atomic_long* obj, long desired)\n{\n    return (int)__msvc_xchg_i32((__int32 volatile*)obj, (__int32)desired);\n}\nstatic inline llong __c11_atomic_exchange__atomic_llong(atomic_llong* obj, llong desired)\n{\n    return (llong)__msvc_xchg_i64((__int64 volatile*)obj, (__int64)desired);\n}\nstatic inline uchar __c11_atomic_exchange__atomic_uchar(atomic_uchar* obj, uchar desired)\n{\n    return (char)__msvc_xchg_i8((__int8 volatile*)obj, (__int8)desired);\n}\nstatic inline ushort __c11_atomic_exchange__atomic_ushort(atomic_ushort* obj, ushort desired)\n{\n    return (short)__msvc_xchg_i16((__int16 volatile*)obj, (__int16)desired);\n}\nstatic inline uint __c11_atomic_exchange__atomic_uint(atomic_uint* obj, uint desired)\n{\n    return (int)__msvc_xchg_i32((__int32 volatile*)obj, (__int32)desired);\n}\nstatic inline ulong __c11_atomic_exchange__atomic_ulong(atomic_ulong* obj, ulong desired)\n{\n    return (int)__msvc_xchg_i32((__int32 volatile*)obj, (__int32)desired);\n}\nstatic inline ullong __c11_atomic_exchange__atomic_ullong(atomic_ullong* obj, ullong desired)\n{\n    return (llong)__msvc_xchg_i64((__int64 volatile*)obj, (__int64)desired);\n}\nstatic inline void* __c11_atomic_exchange__atomic_ptr(atomic_ptr* obj, void* desired)\n{\n    return (void*)__msvc_xchg_ptr(__ptr)((__intptr volatile*)obj, (ptrdiff_t)desired);\n}\n\n#define __c11_atomic_exchange(obj,desired)                \\\n_Generic((obj),                                           \\\natomic_char*: __c11_atomic_exchange__atomic_char,         \\\natomic_uchar*: __c11_atomic_exchange__atomic_uchar,       \\\natomic_short*: __c11_atomic_exchange__atomic_short,       \\\natomic_ushort*: __c11_atomic_exchange__atomic_ushort,     \\\natomic_int*: __c11_atomic_exchange__atomic_int,           \\\natomic_uint*: __c11_atomic_exchange__atomic_uint,         \\\natomic_long*: __c11_atomic_exchange__atomic_long,         \\\natomic_ulong*: __c11_atomic_exchange__atomic_ulong,       \\\natomic_llong*: __c11_atomic_exchange__atomic_llong,       \\\natomic_ullong*: __c11_atomic_exchange__atomic_ullong      \\\n)(obj,desired)\n\n#define atomic_exchange(obj,desired) __c11_atomic_exchange(obj,desired)\n#define atomic_store(obj,desired) __c11_atomic_exchange(obj,desired)\n#define atomic_exchange_explicit(obj,desired,mo) __c11_atomic_exchange(obj,desired)\n#define atomic_store_explicit(obj,desired,mo) __c11_atomic_exchange(obj,desired)\n\nstatic inline __int8  __msvc_cmpxchg_i8(__int8 volatile* addr, __int8 oldval, __int8 newval)\n{\n    return _InterlockedCompareExchange8((__int8 volatile*)addr, newval, oldval);\n}\nstatic inline __int16 __msvc_cmpxchg_i16(__int16 volatile* addr, __int16 oldval, __int16 newval)\n{\n    return _InterlockedCompareExchange16((__int16 volatile*)addr, newval, oldval);\n}\nstatic inline __int32 __msvc_cmpxchg_i32(__int32 volatile* addr, __int32 oldval, __int32 newval)\n{\n    return _InterlockedCompareExchange((__int32 volatile*)addr, newval, oldval);\n}\nstatic inline __int64 __msvc_cmpxchg_i64(__int64 volatile* addr, __int64 oldval, __int64 newval)\n{\n    return _InterlockedCompareExchange64((__int64 volatile*)addr, newval, oldval);\n}\n\n#define __msvc_cmpxchg_ptr(ptr) __concat2(__msvc_cmpxchg_,ptr)\n\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_char(atomic_char* obj, char* expected, char desired)\n{\n    char cmp = *expected, val = __msvc_cmpxchg_i8((__int8 volatile*)obj, (__int8)cmp, (__int8)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_short(atomic_short* obj, short* expected, short desired)\n{\n    short cmp = *expected, val = __msvc_cmpxchg_i16((__int16 volatile*)obj, (__int16)cmp, (__int16)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_int(atomic_int* obj, int* expected, int desired)\n{\n    int cmp = *expected, val = __msvc_cmpxchg_i32((__int32 volatile*)obj, (__int32)cmp, (__int32)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_long(atomic_long* obj, long* expected, long desired)\n{\n    long cmp = *expected, val = __msvc_cmpxchg_i32((__int32 volatile*)obj, (__int32)cmp, (__int32)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_llong(atomic_llong* obj, llong* expected, llong desired)\n{\n    llong cmp = *expected, val = __msvc_cmpxchg_i64((__int64 volatile*)obj, (__int64)cmp, (__int64)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_uchar(atomic_uchar* obj, uchar* expected, uchar desired)\n{\n    uchar cmp = *expected, val = __msvc_cmpxchg_i8((__int8 volatile*)obj, (__int8)cmp, (__int8)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_ushort(atomic_ushort* obj, ushort* expected, ushort desired)\n{\n    ushort cmp = *expected, val = __msvc_cmpxchg_i16((__int16 volatile*)obj, (__int16)cmp, (__int16)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_uint(atomic_uint* obj, uint* expected, uint desired)\n{\n    uint cmp = *expected, val = __msvc_cmpxchg_i32((__int32 volatile*)obj, (__int32)cmp, (__int32)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_ulong(atomic_ulong* obj, ulong* expected, ulong desired)\n{\n    ulong cmp = *expected, val = __msvc_cmpxchg_i32((__int32 volatile*)obj, (__int32)cmp, (__int32)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_ullong(atomic_ullong* obj, ullong* expected, ullong desired)\n{\n    ullong cmp = *expected, val = __msvc_cmpxchg_i64((__int64 volatile*)obj, (__int64)cmp, (__int64)desired); return val == cmp;\n}\nstatic inline _Bool __c11_atomic_compare_exchange_strong__atomic_ptr(atomic_ptr* obj, void** expected, void* desired)\n{\n    ptrdiff_t cmp = *(ptrdiff_t*)expected, val = __msvc_cmpxchg_ptr(__ptr)((__intptr volatile*)obj, (ptrdiff_t)cmp, (ptrdiff_t)desired); return (ptrdiff_t)val == cmp;\n}\n\n\n#define __c11_atomic_compare_exchange_strong(obj,expected,desired)       \\\n_Generic((obj),                                                          \\\natomic_char*: __c11_atomic_compare_exchange_strong__atomic_char,         \\\natomic_uchar*: __c11_atomic_compare_exchange_strong__atomic_uchar,       \\\natomic_short*: __c11_atomic_compare_exchange_strong__atomic_short,       \\\natomic_ushort*: __c11_atomic_compare_exchange_strong__atomic_ushort,     \\\natomic_int*: __c11_atomic_compare_exchange_strong__atomic_int,           \\\natomic_uint*: __c11_atomic_compare_exchange_strong__atomic_uint,         \\\natomic_long*: __c11_atomic_compare_exchange_strong__atomic_long,         \\\natomic_ulong*: __c11_atomic_compare_exchange_strong__atomic_ulong,       \\\natomic_llong*: __c11_atomic_compare_exchange_strong__atomic_llong,       \\\natomic_ullong*: __c11_atomic_compare_exchange_strong__atomic_ullong,     \\\natomic_ptr*: __c11_atomic_compare_exchange_strong__atomic_ptr            \\\n)(obj,expected,desired)\n\n#define atomic_compare_exchange_weak(obj,expected,desired) __c11_atomic_compare_exchange_strong(obj,expected,desired)\n#define atomic_compare_exchange_strong(obj,expected,desired) __c11_atomic_compare_exchange_strong(obj,expected,desired)\n#define atomic_compare_exchange_weak_explicit(obj,expected,desired,smo,fmo) __c11_atomic_compare_exchange_strong(obj,expected,desired)\n#define atomic_compare_exchange_strong_explicit(obj,expected,desired,smo,fmo) __c11_atomic_compare_exchange_strong(obj,expected,desired)\n\n#if !(defined __STDC_VERSION__ && __STDC_VERSION__ > 201710L)\n#define ATOMIC_VAR_INIT(VALUE)\t(VALUE)\n#endif\n\n/*\n * atomic_fetch_add\n */\n\nstatic inline __int8  __msvc_xadd_i8(__int8  volatile* addr, __int8  val)\n{\n    return _InterlockedExchangeAdd8(addr, val);\n}\nstatic inline __int16 __msvc_xadd_i16(__int16 volatile* addr, __int16 val)\n{\n    return _InterlockedExchangeAdd16(addr, val);\n}\nstatic inline __int32 __msvc_xadd_i32(__int32 volatile* addr, __int32 val)\n{\n    return _InterlockedExchangeAdd(addr, val);\n}\nstatic inline __int64 __msvc_xadd_i64(__int64 volatile* addr, __int64 val)\n{\n    return _InterlockedExchangeAdd64(addr, val);\n}\n\n#define __msvc_xadd_ptr(ptr) __concat2(__msvc_xadd_,ptr)\n\nstatic inline char __c11_atomic_fetch_add__atomic_char(atomic_char* obj, char arg)\n{\n    return (char)__msvc_xadd_i8((__int8 volatile*)obj, (__int8)arg);\n}\nstatic inline short __c11_atomic_fetch_add__atomic_short(atomic_short* obj, short arg)\n{\n    return (short)__msvc_xadd_i16((__int16 volatile*)obj, (__int16)arg);\n}\nstatic inline int __c11_atomic_fetch_add__atomic_int(atomic_int* obj, int arg)\n{\n    return (int)__msvc_xadd_i32((__int32 volatile*)obj, (__int32)arg);\n}\nstatic inline long __c11_atomic_fetch_add__atomic_long(atomic_long* obj, long arg)\n{\n    return (long)__msvc_xadd_i32((__int32 volatile*)obj, (__int32)arg);\n}\nstatic inline llong __c11_atomic_fetch_add__atomic_llong(atomic_llong* obj, llong arg)\n{\n    return (llong)__msvc_xadd_i64((__int64 volatile*)obj, (__int64)arg);\n}\nstatic inline uchar __c11_atomic_fetch_add__atomic_uchar(atomic_uchar* obj, uchar arg)\n{\n    return (uchar)__msvc_xadd_i8((__int8 volatile*)obj, (__int8)arg);\n}\nstatic inline ushort __c11_atomic_fetch_add__atomic_ushort(atomic_ushort* obj, ushort arg)\n{\n    return (ushort)__msvc_xadd_i16((__int16 volatile*)obj, (__int16)arg);\n}\nstatic inline uint __c11_atomic_fetch_add__atomic_uint(atomic_uint* obj, uint arg)\n{\n    return (uint)__msvc_xadd_i32((__int32 volatile*)obj, (__int32)arg);\n}\nstatic inline ulong __c11_atomic_fetch_add__atomic_ulong(atomic_ulong* obj, ulong arg)\n{\n    return (ulong)__msvc_xadd_i32((__int32 volatile*)obj, (__int32)arg);\n}\nstatic inline ullong __c11_atomic_fetch_add__atomic_ullong(atomic_ullong* obj, ullong arg)\n{\n    return (ullong)__msvc_xadd_i64((__int64 volatile*)obj, (__int64)arg);\n}\nstatic inline void* __c11_atomic_fetch_add__atomic_ptr(atomic_ptr* obj, void* arg)\n{\n    return (void*)__msvc_xadd_ptr(__ptr)((__intptr volatile*)obj, (__intptr)arg);\n}\n\n#define __c11_atomic_fetch_add(obj,arg)                    \\\n_Generic((obj),                                            \\\natomic_char*: __c11_atomic_fetch_add__atomic_char,         \\\natomic_uchar*: __c11_atomic_fetch_add__atomic_uchar,       \\\natomic_short*: __c11_atomic_fetch_add__atomic_short,       \\\natomic_ushort*: __c11_atomic_fetch_add__atomic_ushort,     \\\natomic_int*: __c11_atomic_fetch_add__atomic_int,           \\\natomic_uint*: __c11_atomic_fetch_add__atomic_uint,         \\\natomic_long*: __c11_atomic_fetch_add__atomic_long,         \\\natomic_ulong*: __c11_atomic_fetch_add__atomic_ulong,       \\\natomic_llong*: __c11_atomic_fetch_add__atomic_llong,       \\\natomic_ullong*: __c11_atomic_fetch_add__atomic_ullong,     \\\natomic_ptr*: __c11_atomic_fetch_add__atomic_ptr            \\\n)(obj,arg)\n\n#define atomic_fetch_add(obj,arg) __c11_atomic_fetch_add(obj,arg)\n#define atomic_fetch_sub(obj,arg) __c11_atomic_fetch_add(obj,-(arg))\n#define atomic_fetch_add_explicit(obj,arg,mo) __c11_atomic_fetch_add(obj,arg)\n#define atomic_fetch_sub_explicit(obj,arg,mo) __c11_atomic_fetch_add(obj,-(arg))\n\n/*\n * atomic_load\n */\n\nstatic inline char __c11_atomic_load__atomic_char(atomic_char* obj)\n{\n    char val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline short __c11_atomic_load__atomic_short(atomic_short* obj)\n{\n    short val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline int __c11_atomic_load__atomic_int(atomic_int* obj)\n{\n    int val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline long __c11_atomic_load__atomic_long(atomic_long* obj)\n{\n    long val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline llong __c11_atomic_load__atomic_llong(atomic_llong* obj)\n{\n    llong val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline uchar __c11_atomic_load__atomic_uchar(atomic_uchar* obj)\n{\n    uchar val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline ushort __c11_atomic_load__atomic_ushort(atomic_ushort* obj)\n{\n    ushort val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline uint __c11_atomic_load__atomic_uint(atomic_uint* obj)\n{\n    uint val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline ulong __c11_atomic_load__atomic_ulong(atomic_ulong* obj)\n{\n    ulong val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline ullong __c11_atomic_load__atomic_ullong(atomic_ullong* obj)\n{\n    ullong val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\nstatic inline void* __c11_atomic_load__atomic_ptr(atomic_ptr* obj)\n{\n    void* val; _ReadBarrier(); val = *obj; _ReadWriteBarrier(); return val;\n}\n\n#define __c11_atomic_load(obj)                        \\\n_Generic((obj),                                       \\\natomic_char*: __c11_atomic_load__atomic_char,         \\\natomic_uchar*: __c11_atomic_load__atomic_uchar,       \\\natomic_short*: __c11_atomic_load__atomic_short,       \\\natomic_ushort*: __c11_atomic_load__atomic_ushort,     \\\natomic_int*: __c11_atomic_load__atomic_int,           \\\natomic_uint*: __c11_atomic_load__atomic_uint,         \\\natomic_long*: __c11_atomic_load__atomic_long,         \\\natomic_ulong*: __c11_atomic_load__atomic_ulong,       \\\natomic_llong*: __c11_atomic_load__atomic_llong,       \\\natomic_ullong*: __c11_atomic_load__atomic_ullong,     \\\natomic_ptr*: __c11_atomic_load__atomic_ptr            \\\n)(obj)\n\n#define atomic_load(obj) __c11_atomic_load(obj)\n#define atomic_load_explicit(obj,mo) __c11_atomic_load(obj)\n\n/*\n * atomic_fetch_{op} template for {and,or,xor} using atomic_compare_exchange\n */\n\n#define __C11_ATOMIC_FETCH_OP_TEMPLATE(prefix,type,op) static inline type           \\\n    __concat3(prefix,atomic_,type)(__concat2(atomic_,type) *obj, type arg) {        \\\n    type oldval, newval;                                                            \\\n    do { oldval = atomic_load(obj); newval = oldval op arg; }                       \\\n    while (!atomic_compare_exchange_strong(obj, &oldval, newval));                  \\\n    return oldval;                                                                  \\\n}\n\n#define __C11_ATOMIC_FETCH_OP_POINTER_TEMPLATE(prefix,op) static inline void*       \\\n    __concat2(prefix,atomic_ptr)(atomic_ptr *obj, void* arg) {                      \\\n    ptrdiff_t oldval, newval;                                                       \\\n    do { oldval = (ptrdiff_t)atomic_load(obj); newval = oldval op (ptrdiff_t)arg; } \\\n    while (!atomic_compare_exchange_strong(obj, (void**)&oldval, (void*)newval));   \\\n    return (void*)oldval;\t\t\t\t\t\t            \\\n}\n\n /*\n  * atomic_fetch_and\n  */\n\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, char, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, short, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, int, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, long, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, llong, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, uchar, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, ushort, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, uint, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, ulong, &)\n__C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_and__, ullong, &)\n__C11_ATOMIC_FETCH_OP_POINTER_TEMPLATE(__c11_atomic_fetch_and__, &)\n\n#define __c11_atomic_fetch_and(obj,arg)                    \\\n_Generic((obj),                                            \\\natomic_char*: __c11_atomic_fetch_and__atomic_char,         \\\natomic_uchar*: __c11_atomic_fetch_and__atomic_uchar,       \\\natomic_short*: __c11_atomic_fetch_and__atomic_short,       \\\natomic_ushort*: __c11_atomic_fetch_and__atomic_ushort,     \\\natomic_int*: __c11_atomic_fetch_and__atomic_int,           \\\natomic_uint*: __c11_atomic_fetch_and__atomic_uint,         \\\natomic_long*: __c11_atomic_fetch_and__atomic_long,         \\\natomic_ulong*: __c11_atomic_fetch_and__atomic_ulong,       \\\natomic_llong*: __c11_atomic_fetch_and__atomic_llong,       \\\natomic_ullong*: __c11_atomic_fetch_and__atomic_ullong,\t   \\\natomic_ptr*: __c11_atomic_fetch_and__atomic_ptr            \\\n)(obj,arg)\n\n#define atomic_fetch_and(obj,arg) __c11_atomic_fetch_and(obj,arg)\n#define atomic_fetch_and_explicit(obj,arg,mo) __c11_atomic_fetch_and(obj,arg)\n\n/*\n * atomic_fetch_or\n */\n\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, char, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, short, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, int, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, long, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, llong, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, uchar, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, ushort, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, uint, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, ulong, | )\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_or__, ullong, | )\n    __C11_ATOMIC_FETCH_OP_POINTER_TEMPLATE(__c11_atomic_fetch_or__, | )\n\n#define __c11_atomic_fetch_or(obj,arg)\t\t\t   \\\n_Generic((obj),                                            \\\natomic_char*: __c11_atomic_fetch_or__atomic_char,          \\\natomic_uchar*: __c11_atomic_fetch_or__atomic_uchar,        \\\natomic_short*: __c11_atomic_fetch_or__atomic_short,        \\\natomic_ushort*: __c11_atomic_fetch_or__atomic_ushort,      \\\natomic_int*: __c11_atomic_fetch_or__atomic_int,            \\\natomic_uint*: __c11_atomic_fetch_or__atomic_uint,          \\\natomic_long*: __c11_atomic_fetch_or__atomic_long,          \\\natomic_ulong*: __c11_atomic_fetch_or__atomic_ulong,\t   \\\natomic_llong*: __c11_atomic_fetch_or__atomic_llong,        \\\natomic_ullong*: __c11_atomic_fetch_or__atomic_ullong,\t   \\\natomic_ptr*: __c11_atomic_fetch_or__atomic_ptr             \\\n)(obj,arg)\n\n#define atomic_fetch_or(obj,arg) __c11_atomic_fetch_or(obj,arg)\n#define atomic_fetch_or_explicit(obj,arg,mo) __c11_atomic_fetch_or(obj,arg)\n\n    /*\n     * atomic_fetch_xor\n     */\n\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, char, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, short, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, int, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, long, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, llong, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, uchar, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, ushort, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, uint, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, ulong, ^)\n    __C11_ATOMIC_FETCH_OP_TEMPLATE(__c11_atomic_fetch_xor__, ullong, ^)\n    __C11_ATOMIC_FETCH_OP_POINTER_TEMPLATE(__c11_atomic_fetch_xor__, ^)\n\n#define __c11_atomic_fetch_xor(obj,arg)                    \\\n_Generic((obj),                                            \\\natomic_char*: __c11_atomic_fetch_xor__atomic_char,         \\\natomic_uchar*: __c11_atomic_fetch_xor__atomic_uchar,       \\\natomic_short*: __c11_atomic_fetch_xor__atomic_short,       \\\natomic_ushort*: __c11_atomic_fetch_xor__atomic_ushort,     \\\natomic_int*: __c11_atomic_fetch_xor__atomic_int,           \\\natomic_uint*: __c11_atomic_fetch_xor__atomic_uint,         \\\natomic_long*: __c11_atomic_fetch_xor__atomic_long,         \\\natomic_ulong*: __c11_atomic_fetch_xor__atomic_ulong,\t   \\\natomic_llong*: __c11_atomic_fetch_xor__atomic_llong,       \\\natomic_ullong*: __c11_atomic_fetch_xor__atomic_ullong,\t   \\\natomic_ptr*: __c11_atomic_fetch_xor__atomic_ptr            \\\n)(obj,arg)\n\n#define atomic_fetch_xor(obj,arg) __c11_atomic_fetch_xor(obj,arg)\n#define atomic_fetch_xor_explicit(obj,arg,mo) __c11_atomic_fetch_xor(obj,arg)\n\n    /*\n     * atomic_flag_test_and_set, atomic_flag_clear\n     */\n\n    static inline _Bool atomic_flag_test_and_set(volatile atomic_flag* obj)\n{\n    char o = 0;\n    return atomic_compare_exchange_strong((atomic_char*)&obj->_Value, &o, 1) ? 0 : 1;\n}\n\nstatic inline void atomic_flag_clear(volatile atomic_flag* obj)\n{\n    atomic_store_explicit((atomic_char*)&obj->_Value, 0, memory_order_release);\n}\n\n#define atomic_flag_test_and_set_explicit(obj,mo) atomic_flag_test_and_set(obj)\n#define atomic_flag_clear_explicit(obj,mo) atomic_flag_clear(obj)\n\n#endif"
        },
        {
          "name": "build.zig",
          "type": "blob",
          "size": 6.1396484375,
          "content": "// Compatible with Zig Version 0.11.0\nconst std = @import(\"std\");\nconst ArrayList = std.ArrayList;\nconst Compile = std.Build.Step.Compile;\nconst ConfigHeader = std.Build.Step.ConfigHeader;\nconst Mode = std.builtin.Mode;\nconst CrossTarget = std.zig.CrossTarget;\n\nconst Maker = struct {\n    builder: *std.build.Builder,\n    target: CrossTarget,\n    optimize: Mode,\n    enable_lto: bool,\n\n    include_dirs: ArrayList([]const u8),\n    cflags: ArrayList([]const u8),\n    cxxflags: ArrayList([]const u8),\n    objs: ArrayList(*Compile),\n\n    fn addInclude(m: *Maker, dir: []const u8) !void {\n        try m.include_dirs.append(dir);\n    }\n    fn addProjectInclude(m: *Maker, path: []const []const u8) !void {\n        try m.addInclude(try m.builder.build_root.join(m.builder.allocator, path));\n    }\n    fn addCFlag(m: *Maker, flag: []const u8) !void {\n        try m.cflags.append(flag);\n    }\n    fn addCxxFlag(m: *Maker, flag: []const u8) !void {\n        try m.cxxflags.append(flag);\n    }\n    fn addFlag(m: *Maker, flag: []const u8) !void {\n        try m.addCFlag(flag);\n        try m.addCxxFlag(flag);\n    }\n\n    fn init(builder: *std.build.Builder) !Maker {\n        const target = builder.standardTargetOptions(.{});\n        const zig_version = @import(\"builtin\").zig_version_string;\n        const commit_hash = try std.ChildProcess.exec(\n            .{ .allocator = builder.allocator, .argv = &.{ \"git\", \"rev-parse\", \"HEAD\" } },\n        );\n        try std.fs.cwd().writeFile(\"common/build-info.cpp\", builder.fmt(\n            \\\\int LLAMA_BUILD_NUMBER = {};\n            \\\\char const *LLAMA_COMMIT = \"{s}\";\n            \\\\char const *LLAMA_COMPILER = \"Zig {s}\";\n            \\\\char const *LLAMA_BUILD_TARGET = \"{s}\";\n            \\\\\n        , .{ 0, commit_hash.stdout[0 .. commit_hash.stdout.len - 1], zig_version, try target.allocDescription(builder.allocator) }));\n        var m = Maker{\n            .builder = builder,\n            .target = target,\n            .optimize = builder.standardOptimizeOption(.{}),\n            .enable_lto = false,\n            .include_dirs = ArrayList([]const u8).init(builder.allocator),\n            .cflags = ArrayList([]const u8).init(builder.allocator),\n            .cxxflags = ArrayList([]const u8).init(builder.allocator),\n            .objs = ArrayList(*Compile).init(builder.allocator),\n        };\n\n        try m.addCFlag(\"-std=c11\");\n        try m.addCxxFlag(\"-std=c++11\");\n        try m.addProjectInclude(&.{});\n        try m.addProjectInclude(&.{\"common\"});\n        return m;\n    }\n\n    fn obj(m: *const Maker, name: []const u8, src: []const u8) *Compile {\n        const o = m.builder.addObject(.{ .name = name, .target = m.target, .optimize = m.optimize });\n        if (o.target.getAbi() != .msvc)\n            o.defineCMacro(\"_GNU_SOURCE\", null);\n\n        if (std.mem.endsWith(u8, src, \".c\")) {\n            o.addCSourceFiles(&.{src}, m.cflags.items);\n            o.linkLibC();\n        } else {\n            o.addCSourceFiles(&.{src}, m.cxxflags.items);\n            if (o.target.getAbi() == .msvc) {\n                o.linkLibC(); // need winsdk + crt\n            } else {\n                // linkLibCpp already add (libc++ + libunwind + libc)\n                o.linkLibCpp();\n            }\n        }\n        for (m.include_dirs.items) |i| o.addIncludePath(.{ .path = i });\n        o.want_lto = m.enable_lto;\n        return o;\n    }\n\n    fn exe(m: *const Maker, name: []const u8, src: []const u8, deps: []const *Compile) *Compile {\n        const e = m.builder.addExecutable(.{ .name = name, .target = m.target, .optimize = m.optimize });\n        e.addCSourceFiles(&.{src}, m.cxxflags.items);\n        for (deps) |d| e.addObject(d);\n        for (m.objs.items) |o| e.addObject(o);\n        for (m.include_dirs.items) |i| e.addIncludePath(.{ .path = i });\n\n        // https://github.com/ziglang/zig/issues/15448\n        if (e.target.getAbi() == .msvc) {\n            e.linkLibC(); // need winsdk + crt\n        } else {\n            // linkLibCpp already add (libc++ + libunwind + libc)\n            e.linkLibCpp();\n        }\n        m.builder.installArtifact(e);\n        e.want_lto = m.enable_lto;\n        return e;\n    }\n};\n\npub fn build(b: *std.build.Builder) !void {\n    var make = try Maker.init(b);\n    make.enable_lto = b.option(bool, \"lto\", \"Enable LTO optimization, (default: false)\") orelse false;\n\n    const ggml = make.obj(\"ggml\", \"ggml.c\");\n    const ggml_alloc = make.obj(\"ggml-alloc\", \"ggml-alloc.c\");\n    const ggml_backend = make.obj(\"ggml-backend\", \"ggml-backend.c\");\n    const ggml_quants = make.obj(\"ggml-quants\", \"ggml-quants.c\");\n    const llama = make.obj(\"llama\", \"llama.cpp\");\n    const buildinfo = make.obj(\"common\", \"common/build-info.cpp\");\n    const common = make.obj(\"common\", \"common/common.cpp\");\n    const console = make.obj(\"console\", \"common/console.cpp\");\n    const sampling = make.obj(\"sampling\", \"common/sampling.cpp\");\n    const grammar_parser = make.obj(\"grammar-parser\", \"common/grammar-parser.cpp\");\n    const train = make.obj(\"train\", \"common/train.cpp\");\n    const clip = make.obj(\"clip\", \"examples/llava/clip.cpp\");\n\n    _ = make.exe(\"main\", \"examples/main/main.cpp\", &.{ ggml, ggml_alloc, ggml_backend, ggml_quants, llama, common, buildinfo, sampling, console, grammar_parser });\n    _ = make.exe(\"quantize\", \"examples/quantize/quantize.cpp\", &.{ ggml, ggml_alloc, ggml_backend, ggml_quants, llama, common, buildinfo });\n    _ = make.exe(\"perplexity\", \"examples/perplexity/perplexity.cpp\", &.{ ggml, ggml_alloc, ggml_backend, ggml_quants, llama, common, buildinfo });\n    _ = make.exe(\"embedding\", \"examples/embedding/embedding.cpp\", &.{ ggml, ggml_alloc, ggml_backend, ggml_quants, llama, common, buildinfo });\n    _ = make.exe(\"finetune\", \"examples/finetune/finetune.cpp\", &.{ ggml, ggml_alloc, ggml_backend, ggml_quants, llama, common, buildinfo, train });\n    _ = make.exe(\"train-text-from-scratch\", \"examples/train-text-from-scratch/train-text-from-scratch.cpp\", &.{ ggml, ggml_alloc, ggml_backend, ggml_quants, llama, common, buildinfo, train });\n\n    const server = make.exe(\"server\", \"examples/server/server.cpp\", &.{ ggml, ggml_alloc, ggml_backend, ggml_quants, llama, common, buildinfo, sampling, grammar_parser, clip });\n    if (server.target.isWindows()) {\n        server.linkSystemLibrary(\"ws2_32\");\n    }\n}\n"
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "codecov.yml",
          "type": "blob",
          "size": 0.205078125,
          "content": "comment: off\n\ncoverage:\n  status:\n    project:\n      default:\n        target: auto\n        threshold: 0\n        base: auto\n    patch:\n      default:\n        target: auto\n        threshold: 0\n        base: auto\n"
        },
        {
          "name": "common",
          "type": "tree",
          "content": null
        },
        {
          "name": "convert-dense.py",
          "type": "blob",
          "size": 49.537109375,
          "content": "# SPDX-License-Identifier: MIT\n# Copyright (c) 2023 Georgi Gerganov\n# Based on code from https://github.com/ggerganov/llama.cpp/tree/6bb4908a17150b49373b5f977685b2e180a04f6f\n\n#!/usr/bin/env python3\nfrom __future__ import annotations\n\nimport argparse\nimport concurrent.futures\nimport enum\nimport faulthandler\nimport functools\nimport itertools\nimport json\nimport math\nimport mmap\nimport pickle\nimport re\nimport signal\nimport struct\nimport sys\nimport time\nimport zipfile\nfrom abc import ABCMeta, abstractmethod\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import IO, TYPE_CHECKING, Any, Callable, Iterable, Literal, TypeVar\n\nimport numpy as np\nfrom sentencepiece import SentencePieceProcessor\n\nimport os\nif 'NO_LOCAL_GGUF' not in os.environ:\n    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\nimport gguf\n\nif TYPE_CHECKING:\n    from typing import TypeAlias\n\nif hasattr(faulthandler, 'register') and hasattr(signal, 'SIGUSR1'):\n    faulthandler.register(signal.SIGUSR1)\n\nNDArray: TypeAlias = 'np.ndarray[Any, Any]'\n\nDEFAULT_CONCURRENCY = 8\n#\n# data types\n#\n\n@dataclass(frozen=True)\nclass DataType:\n    name: str\n    dtype: np.dtype[Any]\n    valid_conversions: list[str]\n\n    def elements_to_bytes(self, n_elements: int) -> int:\n        return n_elements * self.dtype.itemsize\n\n@dataclass(frozen=True)\nclass UnquantizedDataType(DataType):\n    pass\n\nDT_F16  = UnquantizedDataType('F16', dtype = np.dtype(np.float16), valid_conversions = ['F32', 'Q8_0'])\nDT_F32  = UnquantizedDataType('F32', dtype = np.dtype(np.float32), valid_conversions = ['F16', 'Q8_0'])\nDT_I32  = UnquantizedDataType('I32', dtype = np.dtype(np.int16), valid_conversions = [])\nDT_BF16 = UnquantizedDataType('BF16', dtype = np.dtype(np.uint16), valid_conversions = ['F32', 'F16', 'Q8_0'])\n\n@dataclass(frozen=True)\nclass QuantizedDataType(DataType):\n    block_size: int\n    quantized_dtype: np.dtype[Any]\n    ggml_type: gguf.GGMLQuantizationType\n\n    def quantize(self, arr: NDArray) -> NDArray:\n        raise NotImplementedError(f'Quantization for {self.name} not implemented')\n\n    def elements_to_bytes(self, n_elements: int) -> int:\n        assert n_elements % self.block_size == 0, f'Invalid number of elements {n_elements} for {self.name} with block size {self.block_size}'\n        return self.quantized_dtype.itemsize * (n_elements // self.block_size)\n\n@dataclass(frozen=True)\nclass Q8_0QuantizedDataType(QuantizedDataType):\n    # Mini Q8_0 quantization in Python!\n    def quantize(self, arr: NDArray) -> NDArray:\n        assert arr.size % self.block_size == 0 and arr.size != 0, f'Bad array size {arr.size}'\n        assert arr.dtype == np.float32, f'Bad array type {arr.dtype}'\n        n_blocks = arr.size // self.block_size\n        blocks = arr.reshape((n_blocks, self.block_size))\n        # Much faster implementation of block quantization contributed by @Cebtenzzre\n        def quantize_blocks_q8_0(blocks: NDArray) -> Iterable[tuple[Any, Any]]:\n            d = abs(blocks).max(axis = 1) / np.float32(127)\n            with np.errstate(divide = 'ignore'):\n                qs = (blocks / d[:, None]).round()\n            qs[d == 0] = 0\n            yield from zip(d, qs)\n        return np.fromiter(quantize_blocks_q8_0(blocks), count = n_blocks, dtype = self.quantized_dtype)\n\nDT_Q8_0 = Q8_0QuantizedDataType('Q8_0',\n    dtype = np.dtype(np.float32), valid_conversions = [],\n    ggml_type = gguf.GGMLQuantizationType.Q8_0, block_size = 32,\n    quantized_dtype = np.dtype([('d', '<f2'), ('qs', 'i1', (32,))]))\n\n# Quantized types skipped here because they may also map to np.float32\nNUMPY_TYPE_TO_DATA_TYPE: dict[np.dtype[Any], DataType] = {}\nfor dt in (DT_BF16, DT_F16, DT_F32, DT_I32):\n    if dt.dtype in NUMPY_TYPE_TO_DATA_TYPE:\n        raise ValueError(f'Invalid duplicate data type {dt}')\n    NUMPY_TYPE_TO_DATA_TYPE[dt.dtype] = dt\n\nSAFETENSORS_DATA_TYPES: dict[str, DataType] = {\n    'BF16': DT_BF16,\n    'F16': DT_F16,\n    'F32': DT_F32,\n    'I32': DT_I32,\n}\n\n# TODO: match this with `llama_ftype`\n# TODO: rename to LLAMAFileType\n# TODO: move to `gguf.py`\nclass GGMLFileType(enum.IntEnum):\n    AllF32     = 0\n    MostlyF16  = 1  # except 1d tensors\n    MostlyQ8_0 = 7  # except 1d tensors\n\n    def type_for_tensor(self, name: str, tensor: LazyTensor) -> DataType:\n        dt = GGML_FILE_TYPE_TO_DATA_TYPE.get(self)\n        if dt is None:\n            raise ValueError(self)\n        # 1D tensors are always F32.\n        return dt if len(tensor.shape) > 1 else DT_F32\n\nGGML_FILE_TYPE_TO_DATA_TYPE: dict[GGMLFileType, DataType] = {\n    GGMLFileType.AllF32    : DT_F32,\n    GGMLFileType.MostlyF16 : DT_F16,\n    GGMLFileType.MostlyQ8_0: DT_Q8_0,\n}\n\n#\n# hparams loading\n#\n\n@dataclass\nclass Params:\n    n_vocab:    int\n    n_embd:     int\n    n_layer:    int\n    n_ctx:      int\n    n_ff:       int\n    n_head:     int\n    n_head_kv:  int\n    f_norm_eps: float\n\n    arch:       gguf.MODEL_ARCH = gguf.MODEL_ARCH.LLAMA\n\n    rope_scaling_type: gguf.RopeScalingType | None = None\n    f_rope_freq_base: float | None = None\n    f_rope_scale: float | None = None\n    n_orig_ctx: int | None = None\n    rope_finetuned: bool | None = None\n\n    ftype: GGMLFileType | None = None\n\n    # path to the directory containing the model files\n    path_model: Path | None = None\n\n    @staticmethod\n    def guessed(model: LazyModel) -> Params:\n        # try transformer naming first\n        n_vocab, n_embd = model[\"model.embed_tokens.weight\"].shape if \"model.embed_tokens.weight\" in model else model[\"tok_embeddings.weight\"].shape\n\n        # try transformer naming first\n        if \"model.layers.0.self_attn.q_proj.weight\" in model:\n            n_layer=next(i for i in itertools.count() if f\"model.layers.{i}.self_attn.q_proj.weight\" not in model)\n        elif \"model.layers.0.self_attn.W_pack.weight\" in model:   # next: try baichuan naming\n            n_layer=next(i for i in itertools.count() if f\"model.layers.{i}.self_attn.W_pack.weight\" not in model)\n        else:\n            n_layer=next(i for i in itertools.count() if f\"layers.{i}.attention.wq.weight\" not in model)\n\n        if n_layer < 1:\n            raise Exception(\"failed to guess 'n_layer'. This model is unknown or unsupported.\\n\"\n                            \"Suggestion: provide 'config.json' of the model in the same directory containing model files.\")\n\n        n_head = n_embd // 128 # guessed\n        n_mult = 256           # guessed\n\n        # TODO: verify this\n        n_ff = int(2 * (4 * n_embd) / 3)\n        n_ff = n_mult * ((n_ff + n_mult - 1) // n_mult)\n\n        return Params(\n            n_vocab    = n_vocab,\n            n_embd     = n_embd,\n            n_layer    = n_layer,\n            n_ctx      = -1,\n            n_ff       = n_ff,\n            n_head     = n_head,\n            n_head_kv  = n_head,\n            f_norm_eps = 1e-5,\n        )\n\n    @staticmethod\n    def loadHFTransformerJson(model: LazyModel, config_path: Path) -> Params:\n        config = json.load(open(config_path))\n\n        rope_scaling_type = f_rope_scale = n_orig_ctx = rope_finetuned = None\n        rope_scaling = config.get(\"rope_scaling\")\n\n        if rope_scaling is not None and (typ := rope_scaling.get(\"type\")):\n            rope_factor = rope_scaling.get(\"factor\")\n            f_rope_scale = rope_factor\n            if typ == \"linear\":\n                rope_scaling_type = gguf.RopeScalingType.LINEAR\n            elif typ == \"yarn\":\n                rope_scaling_type = gguf.RopeScalingType.YARN\n                n_orig_ctx = rope_scaling['original_max_position_embeddings']\n                rope_finetuned = rope_scaling['finetuned']\n            else:\n                raise NotImplementedError(f'Unknown rope scaling type: {typ}')\n\n        if \"max_sequence_length\" in config:\n            n_ctx = config[\"max_sequence_length\"]\n        elif \"max_position_embeddings\" in config:\n            n_ctx = config[\"max_position_embeddings\"]\n        else:\n            raise Exception(\"failed to guess 'n_ctx'. This model is unknown or unsupported.\\n\"\n                            \"Suggestion: provide 'config.json' of the model in the same directory containing model files.\")\n\n        params = Params(\n            n_vocab           = config[\"vocab_size\"],\n            n_embd            = config[\"hidden_size\"],\n            n_layer           = config[\"num_hidden_layers\"],\n            n_ctx             = n_ctx,\n            n_ff              = config[\"intermediate_size\"],\n            n_head            = (n_head := config[\"num_attention_heads\"]),\n            n_head_kv         = config.get(\"num_key_value_heads\", n_head),\n            f_norm_eps        = config[\"rms_norm_eps\"],\n            f_rope_freq_base  = config.get(\"rope_theta\"),\n            rope_scaling_type = rope_scaling_type,\n            f_rope_scale      = f_rope_scale,\n            n_orig_ctx        = n_orig_ctx,\n            rope_finetuned    = rope_finetuned,\n        )\n\n        if config.get(\"model_type\", None) == \"bamboo\":\n            params.arch = gguf.MODEL_ARCH.BAMBOO\n\n        return params\n\n\n    # LLaMA v2 70B params.json\n    # {\"dim\": 8192, \"multiple_of\": 4096, \"ffn_dim_multiplier\": 1.3, \"n_heads\": 64, \"n_kv_heads\": 8, \"n_layers\": 80, \"norm_eps\": 1e-05, \"vocab_size\": -1}\n    @staticmethod\n    def loadOriginalParamsJson(model: LazyModel, config_path: Path) -> Params:\n        config = json.load(open(config_path))\n\n        # hack to determine LLaMA v1 vs v2 vs CodeLlama\n        if config.get(\"rope_theta\") == 1000000:\n            # CodeLlama\n            n_ctx = 16384\n        elif config[\"norm_eps\"] == 1e-05:\n            # LLaMA v2\n            n_ctx = 4096\n        else:\n            # LLaMA v1\n            n_ctx = 2048\n\n        return Params(\n            n_vocab          = config.get(\"vocab_size\", model[\"tok_embeddings.weight\"].shape[0]),\n            n_embd           = config[\"dim\"],\n            n_layer          = config[\"n_layers\"],\n            n_ctx            = n_ctx,\n            n_ff             = model[\"layers.0.feed_forward.w1.weight\"].shape[0],\n            n_head           = (n_head := config[\"n_heads\"]),\n            n_head_kv        = config.get(\"n_kv_heads\", n_head),\n            f_norm_eps       = config[\"norm_eps\"],\n            f_rope_freq_base = config.get(\"rope_theta\"),\n        )\n\n    @staticmethod\n    def load(model_plus: ModelPlus) -> Params:\n        hf_config_path   = model_plus.paths[0].parent / \"config.json\"\n        orig_config_path = model_plus.paths[0].parent / \"params.json\"\n\n        if hf_config_path.exists():\n            params = Params.loadHFTransformerJson(model_plus.model, hf_config_path)\n        elif orig_config_path.exists():\n            params = Params.loadOriginalParamsJson(model_plus.model, orig_config_path)\n        elif model_plus.format != 'none':\n            params = Params.guessed(model_plus.model)\n        else:\n            raise ValueError('Cannot guess params when model format is none')\n\n        params.path_model = model_plus.paths[0].parent\n\n        return params\n\n\n#\n# vocab\n#\n\nclass BpeVocab:\n    def __init__(self, fname_tokenizer: Path, fname_added_tokens: Path | None) -> None:\n        self.bpe_tokenizer = json.loads(open(str(fname_tokenizer), encoding=\"utf-8\").read())\n        added_tokens: dict[str, int]\n        if fname_added_tokens is not None:\n            # FIXME: Verify that added tokens here _cannot_ overlap with the main vocab.\n            added_tokens = json.load(open(fname_added_tokens, encoding=\"utf-8\"))\n        else:\n            # Fall back to trying to find the added tokens in tokenizer.json\n            tokenizer_json_file = fname_tokenizer.parent / 'tokenizer.json'\n            if not tokenizer_json_file.is_file():\n                added_tokens = {}\n            else:\n                tokenizer_json = json.load(open(tokenizer_json_file, encoding=\"utf-8\"))\n                added_tokens = dict(\n                    (item['content'], item['id'])\n                    for item in tokenizer_json.get('added_tokens', [])\n                    # Added tokens here can be duplicates of the main vocabulary.\n                    if item['content'] not in self.bpe_tokenizer )\n\n        vocab_size: int = len(self.bpe_tokenizer)\n        expected_ids    = list(range(vocab_size, vocab_size + len(added_tokens)))\n        actual_ids      = sorted(added_tokens.values())\n        if expected_ids != actual_ids:\n            expected_end_id = vocab_size + len(actual_ids) - 1\n            raise Exception(f\"Expected the {len(actual_ids)} added token ID(s) to be sequential in the range {vocab_size} - {expected_end_id}; got {actual_ids}\")\n\n        items = sorted(added_tokens.items(), key=lambda text_idx: text_idx[1])\n        self.added_tokens_list    = [text for (text, idx) in items]\n        self.vocab_size_base: int = vocab_size\n        self.vocab_size: int      = self.vocab_size_base + len(self.added_tokens_list)\n        self.fname_tokenizer      = fname_tokenizer\n        self.fname_added_tokens   = fname_added_tokens\n\n    def bpe_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        tokenizer = self.bpe_tokenizer\n        from transformers.models.gpt2 import tokenization_gpt2\n        reverse_vocab = {id: encoded_tok for encoded_tok, id in tokenizer.items()}\n\n        for i, _ in enumerate(tokenizer):\n            yield reverse_vocab[i], 0.0, gguf.TokenType.NORMAL\n\n    def added_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        for text in self.added_tokens_list:\n            score = -1000.0\n            yield text.encode(\"utf-8\"), score, gguf.TokenType.CONTROL\n\n    def all_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        yield from self.bpe_tokens()\n        yield from self.added_tokens()\n\n    def __repr__(self) -> str:\n        return f\"<BpeVocab with {self.vocab_size_base} base tokens and {len(self.added_tokens_list)} added tokens>\"\n\n\nclass SentencePieceVocab:\n    def __init__(self, fname_tokenizer: Path, fname_added_tokens: Path | None) -> None:\n        self.sentencepiece_tokenizer = SentencePieceProcessor(str(fname_tokenizer))\n        added_tokens: dict[str, int]\n        if fname_added_tokens is not None:\n            added_tokens = json.load(open(fname_added_tokens, encoding=\"utf-8\"))\n        else:\n            added_tokens = {}\n\n        vocab_size: int = self.sentencepiece_tokenizer.vocab_size()\n\n        new_tokens       = {id: piece for piece, id in added_tokens.items() if id >= vocab_size}\n        expected_new_ids = list(range(vocab_size, vocab_size + len(new_tokens)))\n        actual_new_ids   = sorted(new_tokens.keys())\n\n        if expected_new_ids != actual_new_ids:\n            raise ValueError(f\"Expected new token IDs {expected_new_ids} to be sequential; got {actual_new_ids}\")\n\n        # Token pieces that were added to the base vocabulary.\n        self.added_tokens_list  = [new_tokens[id] for id in actual_new_ids]\n        self.vocab_size_base    = vocab_size\n        self.vocab_size         = self.vocab_size_base + len(self.added_tokens_list)\n        self.fname_tokenizer    = fname_tokenizer\n        self.fname_added_tokens = fname_added_tokens\n\n    def sentencepiece_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        tokenizer = self.sentencepiece_tokenizer\n        for i in range(tokenizer.vocab_size()):\n            piece = tokenizer.id_to_piece(i)\n            text: bytes = piece.encode(\"utf-8\")\n            score: float = tokenizer.get_score(i)\n\n            toktype = gguf.TokenType.NORMAL\n            if tokenizer.is_unknown(i):\n                toktype = gguf.TokenType.UNKNOWN\n            if tokenizer.is_control(i):\n                toktype = gguf.TokenType.CONTROL\n\n            # NOTE: I think added_tokens are user defined.\n            # ref: https://github.com/google/sentencepiece/blob/master/src/sentencepiece_model.proto\n            # if tokenizer.is_user_defined(i): toktype = gguf.TokenType.USER_DEFINED\n\n            if tokenizer.is_unused(i):\n                toktype = gguf.TokenType.UNUSED\n            if tokenizer.is_byte(i):\n                toktype = gguf.TokenType.BYTE\n\n            yield text, score, toktype\n\n    def added_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        for text in self.added_tokens_list:\n            score = -1000.0\n            yield text.encode(\"utf-8\"), score, gguf.TokenType.USER_DEFINED\n\n    def all_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        yield from self.sentencepiece_tokens()\n        yield from self.added_tokens()\n\n    def __repr__(self) -> str:\n        return f\"<SentencePieceVocab with {self.vocab_size_base} base tokens and {len(self.added_tokens_list)} added tokens>\"\n\nVocab: TypeAlias = 'BpeVocab | SentencePieceVocab'\n\n#\n# data loading\n# TODO: reuse (probably move to gguf.py?)\n#\n\ndef permute(weights: NDArray, n_head: int, n_head_kv: int) -> NDArray:\n    #print( \"permute debug \" + str(weights.shape[0]) + \" x \" + str(weights.shape[1]) + \" nhead \" + str(n_head) + \" nheadkv \" + str(n_kv_head) )\n    if n_head_kv is not None and n_head != n_head_kv:\n        n_head = n_head_kv\n    return (weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n                .swapaxes(1, 2)\n                .reshape(weights.shape))\n\n\nclass Tensor(metaclass=ABCMeta):\n    data_type: DataType\n\n    @abstractmethod\n    def astype(self, data_type: DataType) -> Tensor: ...\n    @abstractmethod\n    def permute(self, n_head: int, n_head_kv: int) -> Tensor: ...\n    @abstractmethod\n    def permute_part(self, n_part: int, n_head: int, n_head_kv: int) -> UnquantizedTensor: ...\n    @abstractmethod\n    def part(self, n_part: int) -> UnquantizedTensor: ...\n    @abstractmethod\n    def to_ggml(self) -> GGMLCompatibleTensor: ...\n\n\ndef bf16_to_fp32(bf16_arr: np.ndarray[Any, np.dtype[np.uint16]]) -> NDArray:\n    assert bf16_arr.dtype == np.uint16, f\"Input array should be of dtype uint16, but got {bf16_arr.dtype}\"\n    fp32_arr = bf16_arr.astype(np.uint32) << 16\n    return fp32_arr.view(np.float32)\n\n\nclass UnquantizedTensor(Tensor):\n    def __init__(self, ndarray: NDArray) -> None:\n        assert isinstance(ndarray, np.ndarray)\n        self.ndarray = ndarray\n        self.data_type = NUMPY_TYPE_TO_DATA_TYPE[ndarray.dtype]\n\n    def astype(self, data_type: DataType) -> Tensor:\n        dtype = data_type.dtype\n        if self.data_type == DT_BF16:\n            self.ndarray = bf16_to_fp32(self.ndarray)\n        return UnquantizedTensor(self.ndarray.astype(dtype))\n\n    def to_ggml(self) -> UnquantizedTensor:\n        return self\n\n    def permute_part(self, n_part: int, n_head: int, n_head_kv: int) -> UnquantizedTensor:\n        r = self.ndarray.shape[0] // 3\n        return UnquantizedTensor(permute(self.ndarray[r * n_part : r * n_part + r, ...], n_head, n_head_kv))\n\n    def part(self, n_part: int) -> UnquantizedTensor:\n        r = self.ndarray.shape[0] // 3\n        return UnquantizedTensor(self.ndarray[r * n_part : r * n_part + r, ...])\n\n    def permute(self, n_head: int, n_head_kv: int) -> UnquantizedTensor:\n        return UnquantizedTensor(permute(self.ndarray, n_head, n_head_kv))\n\n\ndef load_unquantized(lazy_tensor: LazyTensor, expected_dtype: Any = None, convert: bool = False) -> NDArray:\n    tensor = lazy_tensor.load()\n    assert isinstance(tensor, UnquantizedTensor)\n\n    # double-check:\n    actual_shape = list(tensor.ndarray.shape)\n    assert actual_shape == lazy_tensor.shape, (actual_shape, lazy_tensor.shape)\n    if expected_dtype is not None and expected_dtype != tensor.ndarray.dtype:\n        if convert:\n            tensor.ndarray = tensor.ndarray.astype(expected_dtype)\n        else:\n            raise ValueError(f'expected this tensor to have dtype {expected_dtype}, got {tensor.ndarray.dtype}')\n\n    return tensor.ndarray\n\n\nGGMLCompatibleTensor = UnquantizedTensor\n\n\n@dataclass\nclass LazyTensor:\n    _load: Callable[[], Tensor]\n    shape: list[int]\n    data_type: DataType\n    description: str\n\n    def load(self) -> Tensor:\n        ret = self._load()\n        # Should be okay if it maps to the same numpy type?\n        assert ret.data_type == self.data_type or (self.data_type.dtype == ret.data_type.dtype), \\\n                (self.data_type, ret.data_type, self.description)\n        return ret\n\n    def astype(self, data_type: DataType) -> LazyTensor:\n        self.validate_conversion_to(data_type)\n\n        def load() -> Tensor:\n            return self.load().astype(data_type)\n        return LazyTensor(load, self.shape, data_type, f'convert({data_type}) {self.description}')\n\n    def validate_conversion_to(self, data_type: DataType) -> None:\n        if data_type != self.data_type and data_type.name not in self.data_type.valid_conversions:\n            raise ValueError(f'Cannot validate conversion from {self.data_type} to {data_type}.')\n\n\nLazyModel: TypeAlias = 'dict[str, LazyTensor]'\n\n\n@dataclass\nclass ModelPlus:\n    model: LazyModel\n    paths: list[Path]  # Where this was read from.\n    format: Literal['ggml', 'torch', 'safetensors', 'none']\n    vocab: Vocab | None  # For GGML models (which have vocab built in), the vocab.\n\n\ndef merge_sharded(models: list[LazyModel]) -> LazyModel:\n    # Original LLaMA models have each file contain one part of each tensor.\n    # Use a dict instead of a set to preserve order.\n    names = {name: None for model in models for name in model}\n\n    def convert(name: str) -> LazyTensor:\n        lazy_tensors: list[LazyTensor] = [model[name] for model in models]\n        if len(lazy_tensors) == 1:\n            # only one file; don't go through this procedure since there might\n            # be quantized tensors\n            return lazy_tensors[0]\n        if len(lazy_tensors[0].shape) == 1:\n            # the tensor is just duplicated in every file\n            return lazy_tensors[0]\n        if name.startswith('tok_embeddings.') or \\\n           name.endswith('.attention.wo.weight') or \\\n           name.endswith('.feed_forward.w2.weight'):\n            # split by columns\n            axis = 1\n        else:\n            # split by rows\n            axis = 0\n        concatenated_shape = list(lazy_tensors[0].shape)\n        concatenated_shape[axis] = sum(tensor.shape[axis] for tensor in lazy_tensors)\n\n        def load() -> UnquantizedTensor:\n            ndarrays = [load_unquantized(tensor) for tensor in lazy_tensors]\n            concatenated: NDArray = np.concatenate(ndarrays, axis=axis)\n            return UnquantizedTensor(concatenated)\n        description = 'concatenated[[' + '] | ['.join(lt.description for lt in lazy_tensors) + ']]'\n        return LazyTensor(load, concatenated_shape, lazy_tensors[0].data_type, description)\n    return {name: convert(name) for name in names}\n\n\ndef merge_multifile_models(models_plus: list[ModelPlus]) -> ModelPlus:\n    formats = set(mp.format for mp in models_plus)\n    assert len(formats) == 1, \"different formats?\"\n    format = formats.pop()\n    paths = [path for mp in models_plus for path in mp.paths]\n    # Use the first non-None vocab, if any.\n    try:\n        vocab = next(mp.vocab for mp in models_plus if mp.vocab is not None)\n    except StopIteration:\n        vocab = None\n\n    if any(\"model.embed_tokens.weight\" in mp.model for mp in models_plus):\n        # Transformers models put different tensors in different files, but\n        # don't split indivdual tensors between files.\n        model: LazyModel = {}\n        for mp in models_plus:\n            model.update(mp.model)\n    else:\n        model = merge_sharded([mp.model for mp in models_plus])\n\n    return ModelPlus(model, paths, format, vocab)\n\n\ndef permute_lazy(lazy_tensor: LazyTensor, n_head: int, n_head_kv: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().permute(n_head, n_head_kv)\n    return LazyTensor(load, lazy_tensor.shape, lazy_tensor.data_type, f'permute({n_head}, {n_head_kv}) ' + lazy_tensor.description)\n\ndef permute_part_lazy(lazy_tensor: LazyTensor, n_part: int, n_head: int, n_head_kv: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().permute_part(n_part, n_head, n_head_kv)\n    s = lazy_tensor.shape.copy()\n    s[0] = s[0] // 3\n    return LazyTensor(load, s, lazy_tensor.data_type, f'permute({n_head}, {n_head_kv}) ' + lazy_tensor.description)\n\ndef part_lazy(lazy_tensor: LazyTensor, n_part: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().part(n_part)\n    s = lazy_tensor.shape.copy()\n    s[0] = s[0] // 3\n    return LazyTensor(load, s, lazy_tensor.data_type, 'part ' + lazy_tensor.description)\n\n\n# Functionality that simulates `torch.load` but where individual tensors are\n# only loaded into memory on demand, not all at once.\n# PyTorch can't do this natively as of time of writing:\n# - https://github.com/pytorch/pytorch/issues/64327\n# This allows us to de-shard without multiplying RAM usage, and also\n# conveniently drops the PyTorch dependency (though we still need numpy).\n\n\n@dataclass\nclass LazyStorageKind:\n    data_type: DataType\n\n\n@dataclass\nclass LazyStorage:\n    load: Callable[[int, int], NDArray]\n    kind: LazyStorageKind\n    description: str\n\n\nclass LazyUnpickler(pickle.Unpickler):\n    def __init__(self, fp: IO[bytes], data_base_path: str, zip_file: zipfile.ZipFile):\n        super().__init__(fp)\n        self.data_base_path = data_base_path\n        self.zip_file = zip_file\n\n    def persistent_load(self, pid: Any) -> Any:\n        assert pid[0] == 'storage'\n        assert isinstance(pid[1], LazyStorageKind)\n        data_type = pid[1].data_type\n        filename_stem = pid[2]\n        filename = f'{self.data_base_path}/{filename_stem}'\n        info = self.zip_file.getinfo(filename)\n\n        def load(offset: int, elm_count: int) -> NDArray:\n            dtype = data_type.dtype\n            fp = self.zip_file.open(info)\n            fp.seek(offset * dtype.itemsize)\n            size = elm_count * dtype.itemsize\n            data = fp.read(size)\n            assert len(data) == size\n            return np.frombuffer(data, dtype)\n        description = f'storage data_type={data_type} path-in-zip={filename} path={self.zip_file.filename}'\n        return LazyStorage(load=load, kind=pid[1], description=description)\n\n    @staticmethod\n    def lazy_rebuild_tensor_v2(storage: Any, storage_offset: Any, size: Any, stride: Any,\n                               requires_grad: Any, backward_hooks: Any, metadata: Any = None) -> LazyTensor:\n        assert isinstance(storage, LazyStorage)\n\n        def load() -> UnquantizedTensor:\n            elm_count = stride[0] * size[0]\n            return UnquantizedTensor(storage.load(storage_offset, elm_count).reshape(size))\n        description = f'pickled storage_offset={storage_offset} in {storage.description}'\n        return LazyTensor(load, list(size), storage.kind.data_type, description)\n\n    @staticmethod\n    def rebuild_from_type_v2(func, new_type, args, state):\n        return func(*args)\n\n    CLASSES: dict[tuple[str, str], Any] = {\n        # getattr used here as a workaround for mypy not being smart enough to detrmine\n        # the staticmethods have a __func__ attribute.\n        ('torch._tensor', '_rebuild_from_type_v2'): getattr(rebuild_from_type_v2, '__func__'),\n        ('torch._utils', '_rebuild_tensor_v2'): getattr(lazy_rebuild_tensor_v2, '__func__'),\n        ('torch', 'BFloat16Storage'): LazyStorageKind(DT_BF16),\n        ('torch', 'HalfStorage'): LazyStorageKind(DT_F16),\n        ('torch', 'FloatStorage'): LazyStorageKind(DT_F32),\n        ('torch', 'IntStorage'): LazyStorageKind(DT_I32),\n        ('torch', 'Tensor'): LazyTensor,\n    }\n\n    def find_class(self, module: str, name: str) -> Any:\n        if not module.startswith('torch'):\n            return super().find_class(module, name)\n        return self.CLASSES[(module, name)]\n\n\ndef lazy_load_torch_file(outer_fp: IO[bytes], path: Path) -> ModelPlus:\n    zf = zipfile.ZipFile(outer_fp)\n    pickle_paths = [name for name in zf.namelist() if name.endswith('.pkl')]\n    assert len(pickle_paths) == 1, pickle_paths\n    pickle_fp = zf.open(pickle_paths[0], 'r')\n    unpickler = LazyUnpickler(pickle_fp,\n                              data_base_path=pickle_paths[0][:-4],\n                              zip_file=zf)\n    model = unpickler.load()\n    as_dict = dict(model.items())\n    return ModelPlus(model=as_dict, paths=[path], format='torch', vocab=None)\n\n\ndef lazy_load_safetensors_file(fp: IO[bytes], path: Path) -> ModelPlus:\n    header_size, = struct.unpack('<Q', fp.read(8))\n    header: dict[str, dict[str, Any]] = json.loads(fp.read(header_size))\n    # Use mmap for the actual data to avoid race conditions with the file offset.\n    mapped = memoryview(mmap.mmap(fp.fileno(), 0, access=mmap.ACCESS_READ))\n    byte_buf = mapped[8 + header_size:]\n\n    def convert(info: dict[str, Any]) -> LazyTensor:\n        data_type = SAFETENSORS_DATA_TYPES[info['dtype']]\n        numpy_dtype = data_type.dtype\n        shape: list[int] = info['shape']\n        begin, end = info['data_offsets']\n        assert 0 <= begin <= end <= len(byte_buf)\n        assert end - begin == math.prod(shape) * numpy_dtype.itemsize\n        buf = byte_buf[begin:end]\n\n        def load() -> UnquantizedTensor:\n            return UnquantizedTensor(np.frombuffer(buf, dtype=numpy_dtype).reshape(shape))\n        description = f'safetensors begin={begin} end={end} type={data_type} path={path}'\n        return LazyTensor(load, shape, data_type, description)\n    model = {name: convert(info) for (name, info) in header.items() if name != '__metadata__'}\n    return ModelPlus(model=model, paths=[path], format='safetensors', vocab=None)\n\n\ndef must_read(fp: IO[bytes], length: int) -> bytes:\n    ret = fp.read(length)\n    if len(ret) < length:\n        raise Exception(\"unexpectedly reached end of file\")\n    return ret\n\n\n@functools.lru_cache(maxsize=None)\ndef lazy_load_file(path: Path) -> ModelPlus:\n    fp = open(path, 'rb')\n    first8 = fp.read(8)\n    fp.seek(0)\n    if first8[:2] == b'PK':\n        # A zip file, i.e. PyTorch format\n        return lazy_load_torch_file(fp, path)\n    elif struct.unpack('<Q', first8)[0] < 16 * 1024 * 1024:\n        # Probably safetensors\n        return lazy_load_safetensors_file(fp, path)\n    else:\n        raise ValueError(f\"unknown format: {path}\")\n\n\nIn = TypeVar('In')\nOut = TypeVar('Out')\n\ndef bounded_parallel_map(func: Callable[[In], Out], iterable: Iterable[In], concurrency: int, max_workers: int | None = None, use_processpool_executor: bool = False) -> Iterable[Out]:\n    '''Parallel map, but with backpressure.  If the caller doesn't call `next`\n    fast enough, this will stop calling `func` at some point rather than\n    letting results pile up in memory.  Specifically, there is a max of one\n    output value buffered per thread.'''\n    if concurrency < 2:\n        yield from map(func, iterable)\n        # Not reached.\n    iterable = iter(iterable)\n    executor_class: type[ThreadPoolExecutor] | type[ProcessPoolExecutor]\n    if use_processpool_executor:\n        executor_class = ProcessPoolExecutor\n    else:\n        executor_class = ThreadPoolExecutor\n    with executor_class(max_workers = max_workers) as executor:\n        futures: list[concurrent.futures.Future[Out]] = []\n        done = False\n        for _ in range(concurrency):\n            try:\n                futures.append(executor.submit(func, next(iterable)))\n            except StopIteration:\n                done = True\n                break\n\n        while futures:\n            result = futures.pop(0).result()\n            while not done and len(futures) < concurrency:\n                try:\n                    futures.append(executor.submit(func, next(iterable)))\n                except StopIteration:\n                    done = True\n                    break\n            yield result\n\ndef check_vocab_size(params: Params, vocab: Vocab) -> None:\n    if params.n_vocab != vocab.vocab_size:\n        assert isinstance(vocab, BpeVocab) or isinstance(vocab, SentencePieceVocab)\n        if params.n_vocab == vocab.vocab_size_base:\n            print(\"Ignoring added_tokens.json since model matches vocab size without it.\")\n            vocab.added_tokens_list = []\n            vocab.vocab_size = vocab.vocab_size_base\n            return\n        msg = f\"Vocab size mismatch (model has {params.n_vocab}, but {vocab.fname_tokenizer}\"\n        if vocab.fname_added_tokens is not None:\n            msg += f\" combined with {vocab.fname_added_tokens}\"\n        msg += f\" has {vocab.vocab_size}).\"\n        if vocab.vocab_size < params.n_vocab < vocab.vocab_size + 20 and vocab.fname_added_tokens is None:\n            msg += f\"  Most likely you are missing added_tokens.json (should be in {vocab.fname_tokenizer.parent}).\"\n        raise Exception(msg)\n\n\nclass OutputFile:\n    def __init__(self, fname_out: Path, arch: gguf.MODEL_ARCH, endianess:gguf.GGUFEndian=gguf.GGUFEndian.LITTLE) -> None:\n        self.gguf = gguf.GGUFWriter(fname_out, gguf.MODEL_ARCH_NAMES[arch], endianess=endianess)\n\n    def add_meta_arch(self, params: Params) -> None:\n        name = \"LLaMA\"\n\n        # TODO: better logic to determine model name\n        if params.n_ctx == 4096:\n            name = \"LLaMA v2\"\n        elif params.path_model is not None:\n            name = str(params.path_model).split('/')[-1]\n\n        self.gguf.add_name                (name)\n        self.gguf.add_context_length      (params.n_ctx)\n        self.gguf.add_embedding_length    (params.n_embd)\n        self.gguf.add_block_count         (params.n_layer)\n        self.gguf.add_feed_forward_length (params.n_ff)\n        self.gguf.add_rope_dimension_count(params.n_embd // params.n_head)\n        self.gguf.add_head_count          (params.n_head)\n        self.gguf.add_head_count_kv       (params.n_head_kv)\n        self.gguf.add_layer_norm_rms_eps  (params.f_norm_eps)\n\n        if params.f_rope_freq_base is not None:\n            self.gguf.add_rope_freq_base(params.f_rope_freq_base)\n\n        if params.rope_scaling_type:\n            assert params.f_rope_scale is not None\n            self.gguf.add_rope_scaling_type(params.rope_scaling_type)\n            self.gguf.add_rope_scaling_factor(params.f_rope_scale)\n\n        if params.n_orig_ctx is not None:\n            self.gguf.add_rope_scaling_orig_ctx_len(params.n_orig_ctx)\n\n        if params.rope_finetuned is not None:\n            self.gguf.add_rope_scaling_finetuned(params.rope_finetuned)\n\n        if params.ftype is not None:\n            self.gguf.add_file_type(params.ftype)\n\n    def add_meta_vocab(self, vocab: Vocab) -> None:\n        tokens = []\n        scores = []\n        toktypes = []\n        # NOTE: `all_tokens` returns the base vocabulary and added tokens\n        for text, score, toktype in vocab.all_tokens():\n            tokens.append(text)\n            scores.append(score)\n            toktypes.append(toktype)\n\n        if isinstance(vocab, SentencePieceVocab):\n            self.gguf.add_tokenizer_model(\"llama\")\n        elif isinstance(vocab, BpeVocab):\n            self.gguf.add_tokenizer_model(\"gpt2\")\n        else:\n            raise ValueError('Unknown vocab type: Not BpeVocab or SentencePieceVocab')\n        self.gguf.add_token_list(tokens)\n        self.gguf.add_token_scores(scores)\n        self.gguf.add_token_types(toktypes)\n\n    def add_meta_special_vocab(self, svocab: gguf.SpecialVocab) -> None:\n        svocab.add_to_gguf(self.gguf)\n\n    def add_tensor_info(self, name: str, tensor: LazyTensor) -> None:\n        n_elements = int(np.prod(tensor.shape))\n        raw_dtype = getattr(tensor.data_type, 'ggml_type', None)\n        data_type = getattr(tensor.data_type, 'quantized_type', None) or tensor.data_type.dtype\n        data_nbytes = tensor.data_type.elements_to_bytes(n_elements)\n        self.gguf.add_tensor_info(name, tensor.shape, data_type, data_nbytes, raw_dtype = raw_dtype)\n\n    def write_meta(self) -> None:\n        self.gguf.write_header_to_file()\n        self.gguf.write_kv_data_to_file()\n\n    def write_tensor_info(self) -> None:\n        self.gguf.write_ti_data_to_file()\n\n    def close(self) -> None:\n        self.gguf.close()\n\n    @staticmethod\n    def write_vocab_only(fname_out: Path, params: Params, vocab: Vocab, svocab: gguf.SpecialVocab, endianess:gguf.GGUFEndian=gguf.GGUFEndian.LITTLE) -> None:\n        check_vocab_size(params, vocab)\n\n        of = OutputFile(fname_out, params.arch, endianess=endianess)\n\n        # meta data\n        of.add_meta_arch(params)\n        of.add_meta_vocab(vocab)\n        of.add_meta_special_vocab(svocab)\n\n        of.write_meta()\n\n        of.close()\n\n    @staticmethod\n    def do_item(item: tuple[str, LazyTensor]) -> tuple[DataType, NDArray]:\n        name, lazy_tensor = item\n        tensor = lazy_tensor.load().to_ggml()\n        return (lazy_tensor.data_type, tensor.ndarray)\n\n    @staticmethod\n    def maybe_do_quantize(item: tuple[DataType, NDArray]) -> NDArray:\n        dt, arr = item\n        if not isinstance(dt, QuantizedDataType):\n            return arr\n        return dt.quantize(arr)\n\n    @staticmethod\n    def write_all(fname_out: Path, ftype: GGMLFileType, params: Params, model: LazyModel, vocab: Vocab, svocab: gguf.SpecialVocab, concurrency: int = DEFAULT_CONCURRENCY, endianess: gguf.GGUFEndian = gguf.GGUFEndian.LITTLE) -> None:\n        check_vocab_size(params, vocab)\n\n        of = OutputFile(fname_out, params.arch, endianess=endianess)\n\n        # meta data\n        of.add_meta_arch(params)\n        of.add_meta_vocab(vocab)\n        of.add_meta_special_vocab(svocab)\n\n        # tensor info\n        for name, lazy_tensor in model.items():\n            of.add_tensor_info(name, lazy_tensor)\n\n        of.write_meta()\n        of.write_tensor_info()\n\n        # tensor data\n        ndarrays_inner = bounded_parallel_map(OutputFile.do_item, model.items(), concurrency = concurrency)\n        if ftype == GGMLFileType.MostlyQ8_0:\n            ndarrays = bounded_parallel_map(OutputFile.maybe_do_quantize, ndarrays_inner, concurrency = concurrency, max_workers = concurrency, use_processpool_executor = True)\n        else:\n            ndarrays = map(OutputFile.maybe_do_quantize, ndarrays_inner)\n\n        start = time.time()\n        for i, ((name, lazy_tensor), ndarray) in enumerate(zip(model.items(), ndarrays)):\n            elapsed = time.time() - start\n            size = ' x '.join(f\"{dim:6d}\" for dim in lazy_tensor.shape)\n            padi = len(str(len(model)))\n            print(f\"[{i+1:{padi}d}/{len(model)}] Writing tensor {name:38s} | size {size:16} | type {lazy_tensor.data_type.name:4} | T+{int(elapsed):4}\")\n            of.gguf.write_tensor_data(ndarray)\n\n        of.close()\n\ndef pick_output_type(model: LazyModel, output_type_str: str | None) -> GGMLFileType:\n    wq_type = model[gguf.TENSOR_NAMES[gguf.MODEL_TENSOR.ATTN_Q].format(bid=0)+\".weight\"].data_type\n\n    if output_type_str == \"f32\" or (output_type_str is None and wq_type == DT_F32):\n        return GGMLFileType.AllF32\n    if output_type_str == \"f16\" or (output_type_str is None and wq_type in (DT_F16, DT_BF16)):\n        return GGMLFileType.MostlyF16\n    if output_type_str == \"q8_0\":\n        return GGMLFileType.MostlyQ8_0\n\n    name_to_type = {name: lazy_tensor.data_type for (name, lazy_tensor) in model.items()}\n\n    raise Exception(f\"Unexpected combination of types: {name_to_type}\")\n\ndef convert_to_output_type(model: LazyModel, output_type: GGMLFileType) -> LazyModel:\n    return {name: tensor.astype(output_type.type_for_tensor(name, tensor))\n            for (name, tensor) in model.items()}\n\ndef convert_model_names(model: LazyModel, params: Params) -> LazyModel:\n    tmap = gguf.TensorNameMap(params.arch, params.n_layer)\n    should_skip: set[gguf.MODEL_TENSOR] = set(gguf.MODEL_TENSOR_SKIP.get(params.arch, []))\n\n    tmp = model\n\n    # HF models permut or pack some of the tensors, so we need to undo that\n    for i in itertools.count():\n        if f\"model.layers.{i}.self_attn.q_proj.weight\" in model:\n            print(f\"Permuting layer {i}\")\n            tmp[f\"model.layers.{i}.self_attn.q_proj.weight\"] = permute_lazy(model[f\"model.layers.{i}.self_attn.q_proj.weight\"], params.n_head, params.n_head)\n            tmp[f\"model.layers.{i}.self_attn.k_proj.weight\"] = permute_lazy(model[f\"model.layers.{i}.self_attn.k_proj.weight\"], params.n_head, params.n_head_kv)\n           #tmp[f\"model.layers.{i}.self_attn.v_proj.weight\"] =              model[f\"model.layers.{i}.self_attn.v_proj.weight\"]\n        elif f\"model.layers.{i}.self_attn.W_pack.weight\" in model:\n            print(f\"Unpacking and permuting layer {i}\")\n            tmp[f\"model.layers.{i}.self_attn.q_proj.weight\"] = permute_part_lazy(model[f\"model.layers.{i}.self_attn.W_pack.weight\"], 0, params.n_head, params.n_head)\n            tmp[f\"model.layers.{i}.self_attn.k_proj.weight\"] = permute_part_lazy(model[f\"model.layers.{i}.self_attn.W_pack.weight\"], 1, params.n_head, params.n_head_kv)\n            tmp[f\"model.layers.{i}.self_attn.v_proj.weight\"] = part_lazy        (model[f\"model.layers.{i}.self_attn.W_pack.weight\"], 2)\n            del tmp[f\"model.layers.{i}.self_attn.W_pack.weight\"]\n        else:\n            break\n\n    out: LazyModel = {}\n    for name, lazy_tensor in model.items():\n        tensor_type, name_new = tmap.get_type_and_name(name, try_suffixes = (\".weight\", \".bias\")) or (None, None)\n        if name_new is None:\n            raise Exception(f\"Unexpected tensor name: {name}\")\n\n        if tensor_type in should_skip:\n            print(f\"skipping tensor {name_new}\")\n            continue\n\n        print(f\"{name:48s} -> {name_new:40s} | {lazy_tensor.data_type.name:6s} | {lazy_tensor.shape}\")\n        out[name_new] = lazy_tensor\n\n    return out\n\ndef nth_multifile_path(path: Path, n: int) -> Path | None:\n    '''Given any path belonging to a multi-file model (e.g. foo.bin.1), return\n    the nth path in the model.\n    '''\n    # Support the following patterns:\n    patterns: list[tuple[str, str]] = [\n        # - x.00.pth, x.01.pth, etc.\n        (r'\\.[0-9]{2}\\.pth$', f'.{n:02}.pth'),\n        # - x-00001-of-00002.bin, x-00002-of-00002.bin, etc.\n        (r'-[0-9]{5}-of-(.*)$', fr'-{n:05}-of-\\1'),\n        # x.bin, x.bin.1, etc.\n        (r'(\\.[0-9]+)?$', r'\\1' if n == 0 else fr'\\1.{n}')\n    ]\n    for regex, replacement in patterns:\n        if re.search(regex, path.name):\n            new_path = path.with_name(re.sub(regex, replacement, path.name))\n            if new_path.exists():\n                return new_path\n    return None\n\n\ndef find_multifile_paths(path: Path) -> list[Path]:\n    '''Given any path belonging to a multi-file model (e.g. foo.bin.1), return\n    the whole list of paths in the model.\n    '''\n    ret: list[Path] = []\n    for i in itertools.count():\n        nth_path = nth_multifile_path(path, i)\n        if nth_path is None:\n            break\n        ret.append(nth_path)\n    if not ret:\n        # No matches.  This should only happen if the file was named, e.g.,\n        # foo.0, and there was no file named foo.  Oh well, try to process it\n        # as a single file.\n        return [path]\n    return ret\n\n\ndef load_some_model(path: Path) -> ModelPlus:\n    '''Load a model of any supported format.'''\n    # Be extra-friendly and accept either a file or a directory:\n    if path.is_dir():\n        # Check if it's a set of safetensors files first\n        globs = [\"model-00001-of-*.safetensors\", \"model.safetensors\"]\n        files = [file for glob in globs for file in path.glob(glob)]\n        if not files:\n            # Try the PyTorch patterns too, with lower priority\n            globs = [\"consolidated.00.pth\", \"pytorch_model-00001-of-*.bin\", \"*.pt\", \"pytorch_model.bin\"]\n            files = [file for glob in globs for file in path.glob(glob)]\n        if not files:\n            raise Exception(f\"Can't find model in directory {path}\")\n        if len(files) > 1:\n            raise Exception(f\"Found multiple models in {path}, not sure which to pick: {files}\")\n        path = files[0]\n\n    paths = find_multifile_paths(path)\n    models_plus: list[ModelPlus] = []\n    for path in paths:\n        print(f\"Loading model file {path}\")\n        models_plus.append(lazy_load_file(path))\n\n    model_plus = merge_multifile_models(models_plus)\n    return model_plus\n\n\ndef load_vocab(path: Path, vocabtype: str | None) -> Vocab:\n    # Be extra-friendly and accept either a file or a directory.  Also, if it's\n    # a directory, it might be the model directory, and tokenizer.model might\n    # be in the parent of that.\n    if path.is_dir():\n        vocab_file = \"tokenizer.model\"\n        if vocabtype == 'bpe':\n            vocab_file = \"vocab.json\"\n        path2 = path / vocab_file\n        # Use `.parent` instead of /.. to handle the symlink case better.\n        path3 = path.parent / vocab_file\n        if path2.exists():\n            path = path2\n        elif path3.exists():\n            path = path3\n        else:\n            raise FileNotFoundError(\n                f\"Could not find {vocab_file} in {path} or its parent; \"\n                \"if it's in another directory, pass the directory as --vocab-dir\")\n\n    print(f\"Loading vocab file '{path}', type '{vocabtype}'\")\n\n    added_tokens_path = path.parent / \"added_tokens.json\"\n    if vocabtype == \"bpe\":\n        return BpeVocab(path, added_tokens_path if added_tokens_path.exists() else None)\n    elif vocabtype == \"spm\":\n        return SentencePieceVocab(path, added_tokens_path if added_tokens_path.exists() else None)\n    else:\n        raise ValueError(f\"Unsupported vocabulary type {vocabtype}\")\n\n\ndef default_outfile(model_paths: list[Path], file_type: GGMLFileType) -> Path:\n    namestr = {\n        GGMLFileType.AllF32:    \"f32\",\n        GGMLFileType.MostlyF16: \"f16\",\n        GGMLFileType.MostlyQ8_0:\"q8_0\",\n    }[file_type]\n    ret = model_paths[0].parent / f\"ggml-model-{namestr}.gguf\"\n    if ret in model_paths:\n        sys.stderr.write(\n            f\"Error: Default output path ({ret}) would overwrite the input. \"\n            \"Please explicitly specify a path using --outfile.\\n\")\n        sys.exit(1)\n    return ret\n\n\ndef do_dump_model(model_plus: ModelPlus) -> None:\n    print(f\"model_plus.paths = {model_plus.paths!r}\")\n    print(f\"model_plus.format = {model_plus.format!r}\")\n    print(f\"model_plus.vocab = {model_plus.vocab!r}\")\n    for name, lazy_tensor in model_plus.model.items():\n        print(f\"{name}: shape={lazy_tensor.shape} type={lazy_tensor.data_type}; {lazy_tensor.description}\")\n\n\ndef main(args_in: list[str] | None = None) -> None:\n    output_choices = [\"f32\", \"f16\"]\n    if np.uint32(1) == np.uint32(1).newbyteorder(\"<\"):\n        # We currently only support Q8_0 output on little endian systems.\n        output_choices.append(\"q8_0\")\n    parser = argparse.ArgumentParser(description=\"Convert a LLaMa model to a GGML compatible file\")\n    parser.add_argument(\"--dump\",        action=\"store_true\",    help=\"don't convert, just show what's in the model\")\n    parser.add_argument(\"--dump-single\", action=\"store_true\",    help=\"don't convert, just show what's in a single model file\")\n    parser.add_argument(\"--vocab-only\",  action=\"store_true\",    help=\"extract only the vocab\")\n    parser.add_argument(\"--outtype\",     choices=output_choices, help=\"output format - note: q8_0 may be very slow (default: f16 or f32 based on input)\")\n    parser.add_argument(\"--vocab-dir\",   type=Path,              help=\"directory containing tokenizer.model, if separate from model file\")\n    parser.add_argument(\"--outfile\",     type=Path,              help=\"path to write to; default: based on input\")\n    parser.add_argument(\"model\",         type=Path,              help=\"directory containing model file, or model file itself (*.pth, *.pt, *.bin, *.safetensors)\")\n    parser.add_argument(\"--vocabtype\",   choices=[\"spm\", \"bpe\"], help=\"vocab format (default: spm)\", default=\"spm\")\n    parser.add_argument(\"--ctx\",         type=int,               help=\"model training context (default: based on input)\")\n    parser.add_argument(\"--concurrency\", type=int,               help=f\"concurrency used for conversion (default: {DEFAULT_CONCURRENCY})\", default = DEFAULT_CONCURRENCY)\n    parser.add_argument(\"--bigendian\",   action=\"store_true\",    help=\"model is executed on big endian machine\")\n\n    args = parser.parse_args(args_in)\n    \n    if args.dump_single:\n        model_plus = lazy_load_file(args.model)\n        do_dump_model(model_plus)\n        return\n\n    if not args.vocab_only:\n        model_plus = load_some_model(args.model)\n    else:\n        model_plus = ModelPlus(model = {}, paths = [args.model / 'dummy'], format = 'none', vocab = None)\n\n    if args.dump:\n        do_dump_model(model_plus)\n        return\n    endianess = gguf.GGUFEndian.LITTLE\n    if args.bigendian:\n        endianess = gguf.GGUFEndian.BIG\n\n    params = Params.load(model_plus)\n    if params.n_ctx == -1:\n        if args.ctx is None:\n            raise Exception(\"The model doesn't have a context size, and you didn't specify one with --ctx\\n\"\n                            \"Please specify one with --ctx:\\n\"\n                            \" - LLaMA v1: --ctx 2048\\n\"\n                            \" - LLaMA v2: --ctx 4096\\n\")\n        params.n_ctx = args.ctx\n\n    if args.outtype:\n        params.ftype = {\n            \"f32\": GGMLFileType.AllF32,\n            \"f16\": GGMLFileType.MostlyF16,\n            \"q8_0\": GGMLFileType.MostlyQ8_0,\n        }[args.outtype]\n\n    print(f\"params = {params}\")\n\n    vocab: Vocab\n    if args.vocab_only:\n        if not args.outfile:\n            raise ValueError(\"need --outfile if using --vocab-only\")\n        # FIXME: Try to respect vocab_dir somehow?\n        vocab = load_vocab(args.vocab_dir or args.model, args.vocabtype)\n        special_vocab = gguf.SpecialVocab(model_plus.paths[0].parent,\n            load_merges = args.vocabtype == 'bpe',\n            n_vocab = vocab.vocab_size)\n        outfile = args.outfile\n        OutputFile.write_vocab_only(outfile, params, vocab, special_vocab)\n        print(f\"Wrote {outfile}\")\n        return\n\n    if model_plus.vocab is not None and args.vocab_dir is None:\n        vocab = model_plus.vocab\n    else:\n        vocab_dir = args.vocab_dir if args.vocab_dir else model_plus.paths[0].parent\n        vocab = load_vocab(vocab_dir, args.vocabtype)\n    # FIXME: Try to respect vocab_dir somehow?\n    special_vocab = gguf.SpecialVocab(model_plus.paths[0].parent,\n        load_merges = args.vocabtype == 'bpe',\n        n_vocab = vocab.vocab_size)\n\n    model   = model_plus.model\n    model   = convert_model_names(model, params)\n    ftype   = pick_output_type(model, args.outtype)\n    model   = convert_to_output_type(model, ftype)\n    outfile = args.outfile or default_outfile(model_plus.paths, ftype)\n\n    params.ftype = ftype\n    print(f\"Writing {outfile}, format {ftype}\")\n\n    OutputFile.write_all(outfile, ftype, params, model, vocab, special_vocab, concurrency = args.concurrency, endianess=endianess)\n    print(f\"Wrote {outfile}\")\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "convert-hf-to-powerinfer-gguf.py",
          "type": "blob",
          "size": 22.3203125,
          "content": "#!/usr/bin/env python3\n\nfrom __future__ import annotations\nfrom abc import ABC, abstractmethod\n\nimport argparse\nimport contextlib\nimport json\nimport os\nimport re\nimport struct\nimport sys\nfrom enum import IntEnum\nfrom pathlib import Path\nfrom typing import TYPE_CHECKING, Any, ContextManager, Iterator, Optional, cast\n\nimport numpy as np\nimport torch\nimport torch.nn as tnn\n\nfrom dataclasses import dataclass\n\nif TYPE_CHECKING:\n    from torch import Tensor\n\nif \"NO_LOCAL_GGUF\" not in os.environ:\n    sys.path.insert(1, str(Path(__file__).parent / \"gguf-py\"))\nimport gguf\n\n\n###### MODEL DEFINITIONS ######\n\n\nclass SentencePieceTokenTypes(IntEnum):\n    NORMAL = 1\n    UNKNOWN = 2\n    CONTROL = 3\n    USER_DEFINED = 4\n    UNUSED = 5\n    BYTE = 6\n\n\nclass ReluMLP(tnn.Module):\n    def __init__(self, input_dim: int, hidden_dim: int, output_dim: int):\n        super(ReluMLP, self).__init__()\n        self.fc1 = tnn.Linear(input_dim, hidden_dim, bias=False)\n        self.relu = tnn.ReLU()\n        self.fc2 = tnn.Linear(hidden_dim, output_dim, bias=False)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        return x\n\n    @staticmethod\n    def from_file(model_file: Path):\n        model = torch.load(model_file, map_location=\"cpu\")\n        hidden_size, input_size = model.get(\"fc1.weight\").shape\n        output_size, _ = model.get(\"fc2.weight\").shape\n        mlp = ReluMLP(input_size, hidden_size, output_size)\n        mlp.load_state_dict(model)\n        return mlp\n\n\nclass Model(ABC):\n    \"\"\"Base class for model conversion\"\"\"\n\n    def __init__(\n        self,\n        dir_model: Path,\n        dir_mlp_pred: Path,\n        ftype: int,\n        fname_out: Path,\n        is_big_endian: bool,\n    ):\n        self.dir_model = dir_model\n        self.dir_mlp_pred = dir_mlp_pred\n        self.ftype = ftype\n        self.fname_out = fname_out\n        self.is_big_endian = is_big_endian\n        self.endianess = (\n            gguf.GGUFEndian.BIG if is_big_endian else gguf.GGUFEndian.LITTLE\n        )\n        self.is_safetensors = self._is_model_safetensors()\n        self.num_parts = Model.count_model_parts(\n            self.dir_model, \".safetensors\" if self.is_safetensors else \".bin\"\n        )\n        self.part_names = self._get_part_names()\n        self.hparams = Model.load_hparams(self.dir_model)\n        self.model_arch = self._get_model_architecture()\n        self.gguf_writer = gguf.GGUFWriter(\n            fname_out, gguf.MODEL_ARCH_NAMES[self.model_arch], endianess=self.endianess, use_temp_file = False\n        )\n\n    def set_vocab(self):\n        self._set_vocab_gpt2()\n\n    def get_tensors(self) -> Iterator[tuple[str, Tensor]]:\n        for model_layer, part_name in self._get_mlp_part_layer_names():\n            print(f\"gguf: loading mlp part '{part_name}'\")\n            mlp_model = ReluMLP.from_file(self.dir_mlp_pred / part_name)\n            for name, data in mlp_model.state_dict().items():\n                yield f\"blk.{model_layer}.{name}\", data\n\n        for part_name in self.part_names:\n            print(f\"gguf: loading model part '{part_name}'\")\n            ctx: ContextManager[Any]\n            if self.is_safetensors:\n                from safetensors import safe_open\n\n                ctx = cast(\n                    ContextManager[Any],\n                    safe_open(self.dir_model / part_name, framework=\"pt\", device=\"cpu\"),\n                )\n            else:\n                ctx = contextlib.nullcontext(\n                    torch.load(self.dir_model / part_name, map_location=\"cpu\")\n                )\n\n            with ctx as model_part:\n                for name in model_part.keys():\n                    data = (\n                        model_part.get_tensor(name)\n                        if self.is_safetensors\n                        else model_part[name]\n                    )\n                    yield name, data\n\n    @abstractmethod\n    def set_gguf_parameters(self):\n        pass\n        # self.gguf_writer.add_name(self.dir_model.name)\n        # self.gguf_writer.add_block_count(\n        #     self.hparams.get(\n        #         \"n_layers\",\n        #         self.hparams.get(\"num_hidden_layers\", self.hparams.get(\"n_layer\")),\n        #     )\n        # )\n        # if (n_ctx := self.hparams.get(\"max_position_embeddings\")) is not None:\n        #     self.gguf_writer.add_context_length(n_ctx)\n        # if (n_embd := self.hparams.get(\"hidden_size\")) is not None:\n        #     self.gguf_writer.add_embedding_length(n_embd)\n        # if (n_ff := self.hparams.get(\"intermediate_size\")) is not None:\n        #     self.gguf_writer.add_feed_forward_length(n_ff)\n        # if (n_head := self.hparams.get(\"num_attention_head\")) is not None:\n        #     self.gguf_writer.add_head_count(n_head)\n        # self.gguf_writer.add_parallel_residual(\n        #     self.hparams.get(\"use_parallel_residual\", True)\n        # )\n\n    @abstractmethod\n    def write_tensors(self):\n        pass\n\n    def write(self):\n        self.write_tensors()\n        self.gguf_writer.write_header_to_file()\n        self.gguf_writer.write_kv_data_to_file()\n        self.gguf_writer.write_tensors_to_file()\n        self.gguf_writer.close()\n\n    def write_vocab(self):\n        self.gguf_writer.write_header_to_file()\n        self.gguf_writer.write_kv_data_to_file()\n        self.gguf_writer.close()\n\n    @staticmethod\n    def count_model_parts(dir_model: Path, prefix: str) -> int:\n        num_parts = 0\n        for filename in os.listdir(dir_model):\n            if filename.endswith(prefix):\n                num_parts += 1\n\n        return num_parts\n\n    @staticmethod\n    def load_hparams(dir_model):\n        with open(dir_model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n            return json.load(f)\n\n    @staticmethod\n    def from_model_architecture(model_architecture):\n        if model_architecture in (\"FalconForCausalLM\", \"RWForCausalLM\"):\n            return FalconModel\n        if model_architecture == \"LlamaForCausalLM\":\n            return LlamaModel\n\n        raise NotImplementedError(f'Architecture \"{model_architecture}\" not supported!')\n\n    def _is_model_safetensors(self) -> bool:\n        return Model.count_model_parts(self.dir_model, \".safetensors\") > 0\n\n    def _get_mlp_part_layer_names(self):\n        \"\"\"Returns a generator of (index, name) for MLP predictors of each model layer\"\"\"\n        n_mlp_parts = Model.count_model_parts(self.dir_mlp_pred, \".pt\")\n        return ((n, f\"model_{n}.pt\") for n in range(n_mlp_parts))\n\n    def _get_part_names(self):\n        if self.is_safetensors:\n            if self.num_parts == 1:  # there's only one .safetensors file\n                return (\"model.safetensors\",)\n            return (\n                f\"model-{n:05}-of-{self.num_parts:05}.safetensors\"\n                for n in range(1, self.num_parts + 1)\n            )\n\n        if self.num_parts == 1:  # there's only one .bin file\n            return (\"pytorch_model.bin\",)\n        return (\n            f\"pytorch_model-{n:05}-of-{self.num_parts:05}.bin\"\n            for n in range(1, self.num_parts + 1)\n        )\n\n    def _get_model_architecture(self) -> gguf.MODEL_ARCH:\n        arch = self.hparams[\"architectures\"][0]\n        if arch == \"FalconForCausalLM\":\n            return gguf.MODEL_ARCH.FALCON\n        if arch == \"RWForCausalLM\" or arch == \"LlamaForCausalLM\":\n            return gguf.MODEL_ARCH.LLAMA\n\n        raise NotImplementedError(f'Architecture \"{arch}\" not supported!')\n\n    def _translate_tensor_key(\n        self, key: str, try_suffixes=(\".weight\", \".bias\")\n    ) -> Optional[str]:\n        block_count = self.hparams.get(\n            \"n_layers\",\n            self.hparams.get(\"num_hidden_layers\", self.hparams.get(\"n_layer\")),\n        )\n        tensor_map = gguf.get_tensor_name_map(self.model_arch, block_count)\n        arch_tensor_key = tensor_map.get_name(key, try_suffixes=try_suffixes)\n        if arch_tensor_key is not None:\n            return arch_tensor_key\n        # check and handle ReluMLP layers\n        mlp_match = re.match(r\"^blk\\.\\d+\\.fc\\d\\.weight$\", key)\n        if mlp_match:\n            return mlp_match.group(0)\n        return None\n\n    def _set_vocab_gpt2(self):\n        dir_model = self.dir_model\n        hparams = self.hparams\n        tokens: list[bytearray] = []\n        toktypes: list[int] = []\n\n        from transformers import AutoTokenizer  # type: ignore[attr-defined]\n\n        tokenizer = AutoTokenizer.from_pretrained(dir_model)\n        vocab_size = hparams.get(\"vocab_size\", len(tokenizer.vocab))\n        assert max(tokenizer.vocab.values()) < vocab_size\n\n        reverse_vocab = {\n            id_: encoded_tok for encoded_tok, id_ in tokenizer.vocab.items()\n        }\n        added_vocab = tokenizer.get_added_vocab()\n\n        for i in range(vocab_size):\n            if i not in reverse_vocab:\n                pad_token = f\"[PAD{i}]\".encode(\"utf-8\")\n                tokens.append(bytearray(pad_token))\n                toktypes.append(gguf.TokenType.USER_DEFINED)\n            elif reverse_vocab[i] in added_vocab:\n                tokens.append(reverse_vocab[i])\n                if tokenizer.added_tokens_decoder[i].special:\n                    toktypes.append(gguf.TokenType.CONTROL)\n                else:\n                    toktypes.append(gguf.TokenType.USER_DEFINED)\n            else:\n                tokens.append(reverse_vocab[i])\n                toktypes.append(gguf.TokenType.NORMAL)\n\n        self.gguf_writer.add_tokenizer_model(\"gpt2\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(dir_model, load_merges=True)\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n    def _set_vocab_sentencepiece(self):\n        from sentencepiece import SentencePieceProcessor\n\n        tokenizer_path = self.dir_model / \"tokenizer.model\"\n\n        tokens: list[bytes] = []\n        scores: list[float] = []\n        toktypes: list[int] = []\n\n        if not tokenizer_path.is_file():\n            print(f\"Error: Missing {tokenizer_path}\", file=sys.stderr)\n            sys.exit(1)\n\n        tokenizer = SentencePieceProcessor(str(tokenizer_path))\n        vocab_size = self.hparams.get(\"vocab_size\", tokenizer.vocab_size())\n\n        for token_id in range(vocab_size):\n            piece = tokenizer.id_to_piece(token_id)\n            text = piece.encode(\"utf-8\")\n            score = tokenizer.get_score(token_id)\n\n            toktype = SentencePieceTokenTypes.NORMAL\n            if tokenizer.is_unknown(token_id):\n                toktype = SentencePieceTokenTypes.UNKNOWN\n            elif tokenizer.is_control(token_id):\n                toktype = SentencePieceTokenTypes.CONTROL\n            elif tokenizer.is_unused(token_id):\n                toktype = SentencePieceTokenTypes.UNUSED\n            elif tokenizer.is_byte(token_id):\n                toktype = SentencePieceTokenTypes.BYTE\n\n            tokens.append(text)\n            scores.append(score)\n            toktypes.append(toktype)\n\n        added_tokens_file = self.dir_model / \"added_tokens.json\"\n        if added_tokens_file.is_file():\n            with open(added_tokens_file, \"r\", encoding=\"utf-8\") as f:\n                added_tokens_json = json.load(f)\n\n                for key in added_tokens_json:\n                    tokens.append(key.encode(\"utf-8\"))\n                    scores.append(-1000.0)\n                    toktypes.append(SentencePieceTokenTypes.USER_DEFINED)\n\n        self.gguf_writer.add_tokenizer_model(\"llama\")\n        self.gguf_writer.add_token_list(tokens)\n        self.gguf_writer.add_token_scores(scores)\n        self.gguf_writer.add_token_types(toktypes)\n\n        special_vocab = gguf.SpecialVocab(self.dir_model, n_vocab=len(tokens))\n        special_vocab.add_to_gguf(self.gguf_writer)\n\n\nclass LlamaModel(Model):\n    def set_vocab(self):\n        self._set_vocab_sentencepiece()\n\n    def set_gguf_parameters(self, params: PredictorParams):\n        self.gguf_writer.add_name(\"Llama\")\n        self.gguf_writer.add_context_length(2048)  # not in config.json\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(self.hparams[\"num_hidden_layers\"])\n        self.gguf_writer.add_feed_forward_length(self.hparams[\"intermediate_size\"])\n        self.gguf_writer.add_rope_dimension_count(\n            self.hparams[\"hidden_size\"] // self.hparams[\"num_attention_heads\"]\n        )\n        self.gguf_writer.add_head_count(self.hparams[\"num_attention_heads\"])\n        self.gguf_writer.add_head_count_kv(self.hparams[\"num_key_value_heads\"])\n        self.gguf_writer.add_layer_norm_rms_eps(self.hparams[\"rms_norm_eps\"])\n        self.gguf_writer.add_rope_freq_base(self.hparams[\"rope_theta\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n        if params.sparse_threshold is not None:\n            self.gguf_writer.add_sparse_threshold(params.sparse_threshold)\n\n    def write_tensors(self):\n        for name, data_torch in self.get_tensors():\n            # we don't need these\n            if name.endswith(\n                (\n                    \".attention.masked_bias\",\n                    \".attention.bias\",\n                    \".attention.rotary_emb.inv_freq\",\n                )\n            ):\n                continue\n\n            old_dtype = data_torch.dtype\n\n            # convert any unsupported data types to float32\n            if data_torch.dtype not in (torch.float16, torch.float32):\n                data_torch = data_torch.to(torch.float32)\n\n            data = data_torch.squeeze().numpy()\n\n            # map tensor names\n            new_name = self._translate_tensor_key(name)\n            if new_name is None:\n                print(f\"Can not map tensor {name!r}\")\n                sys.exit()\n\n            # We need to transpose the weight matrices for the FFN Down layers to support the\n            # Axpy operation in PowerInfer. So we don't need to transpose them at runtime.\n            if \"ffn_down\" in new_name:\n                new_name = new_name.replace(\"ffn_down\", \"ffn_down_t\")\n                data = data.T\n\n            n_dims = len(data.shape)\n            data_dtype = data.dtype\n\n            # if f32 desired, convert any float16 to float32\n            if self.ftype == 0 and data_dtype == np.float16:\n                data = data.astype(np.float32)\n\n            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\n            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\n                data = data.astype(np.float32)\n\n            # if f16 desired, convert any float32 2-dim weight tensors to float16\n            if (\n                self.ftype == 1\n                and data_dtype == np.float32\n                and name.endswith(\".weight\")\n                and n_dims == 2\n            ):\n                data = data.astype(np.float16)\n\n            print(f\"{new_name}, n_dims = {n_dims}, {old_dtype} --> {data.dtype}\")\n\n            self.gguf_writer.add_tensor(new_name, data)\n\n\nclass FalconModel(Model):\n    def set_gguf_parameters(self, params: PredictorParams):\n        block_count = self.hparams.get(\"num_hidden_layers\")\n        if block_count is None:\n            block_count = self.hparams[\"n_layer\"]  # old name\n\n        n_head = self.hparams.get(\"num_attention_heads\")\n        if n_head is None:\n            n_head = self.hparams[\"n_head\"]  # old name\n\n        n_head_kv = self.hparams.get(\"num_kv_heads\")\n        if n_head_kv is None:\n            n_head_kv = self.hparams.get(\"n_head_kv\", 1)  # old name\n\n        self.gguf_writer.add_name(\"Falcon\")\n        self.gguf_writer.add_context_length(2048)  # not in config.json\n        self.gguf_writer.add_tensor_data_layout(\"jploski\")  # qkv tensor transform\n        self.gguf_writer.add_embedding_length(self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_feed_forward_length(4 * self.hparams[\"hidden_size\"])\n        self.gguf_writer.add_block_count(block_count)\n        self.gguf_writer.add_head_count(n_head)\n        self.gguf_writer.add_head_count_kv(n_head_kv)\n        self.gguf_writer.add_layer_norm_eps(self.hparams[\"layer_norm_epsilon\"])\n        self.gguf_writer.add_file_type(self.ftype)\n\n        if params.sparse_threshold is not None:\n            self.gguf_writer.add_sparse_threshold(params.sparse_threshold)\n\n    def write_tensors(self):\n        n_head = self.hparams.get(\"num_attention_heads\")\n        if n_head is None:\n            n_head = self.hparams[\"n_head\"]  # old name\n\n        n_head_kv = self.hparams.get(\"num_kv_heads\")\n        if n_head_kv is None:\n            n_head_kv = self.hparams.get(\"n_head_kv\", 1)  # old name\n\n        head_dim = self.hparams[\"hidden_size\"] // n_head\n\n        for name, data_torch in self.get_tensors():\n            old_dtype = data_torch.dtype\n\n            # convert any unsupported data types to float32\n            if data_torch.dtype not in (torch.float16, torch.float32):\n                data_torch = data_torch.to(torch.float32)\n\n            # QKV tensor transform\n            # The original query_key_value tensor contains n_head_kv \"kv groups\",\n            # each consisting of n_head/n_head_kv query weights followed by one key\n            # and one value weight (shared by all query heads in the kv group).\n            # This layout makes it a big pain to work with in GGML.\n            # So we rearrange them here,, so that we have n_head query weights\n            # followed by n_head_kv key weights followed by n_head_kv value weights,\n            # in contiguous fashion.\n            # ref: https://github.com/jploski/ggml/blob/falcon40b/examples/falcon/convert-hf-to-ggml.py\n\n            if \"query_key_value\" in name:\n                qkv = data_torch.view(\n                    n_head_kv, n_head // n_head_kv + 2, head_dim, head_dim * n_head\n                )\n                q = qkv[:, :-2].reshape(n_head * head_dim, head_dim * n_head)\n                k = qkv[:, [-2]].reshape(n_head_kv * head_dim, head_dim * n_head)\n                v = qkv[:, [-1]].reshape(n_head_kv * head_dim, head_dim * n_head)\n                data_torch = torch.cat((q, k, v)).reshape_as(data_torch)\n\n            data = data_torch.squeeze().numpy()\n\n            # map tensor names\n            new_name = self._translate_tensor_key(name)\n            if new_name is None:\n                print(f\"Can not map tensor {name!r}\")\n                sys.exit()\n\n            # We need to transpose the weight matrices for the FFN Down layers to support the\n            # Axpy operation in PowerInfer. So we don't need to transpose them at runtime.\n            if \"ffn_down\" in new_name:\n                new_name = new_name.replace(\"ffn_down\", \"ffn_down_t\")\n                data = data.T\n\n            n_dims = len(data.shape)\n            data_dtype = data.dtype\n\n            # if f32 desired, convert any float16 to float32\n            if self.ftype == 0 and data_dtype == np.float16:\n                data = data.astype(np.float32)\n\n            # TODO: Why cant we use these float16 as-is? There should be not reason to store float16 as float32\n            if self.ftype == 1 and data_dtype == np.float16 and n_dims == 1:\n                data = data.astype(np.float32)\n\n            # if f16 desired, convert any float32 2-dim weight tensors to float16\n            if (\n                self.ftype == 1\n                and data_dtype == np.float32\n                and name.endswith(\".weight\")\n                and n_dims == 2\n            ):\n                data = data.astype(np.float16)\n\n            print(f\"{new_name}, n_dims = {n_dims}, {old_dtype} --> {data.dtype}\")\n\n            self.gguf_writer.add_tensor(new_name, data)\n\n\n\n@dataclass\nclass PredictorParams:\n    sparse_threshold: float | None = None\n\n    @staticmethod\n    def loadPredictorJson(config_path: Path) -> PredictorParams:\n        config = json.load(open(config_path))\n        return PredictorParams(\n            sparse_threshold = config.get(\"sparse_threshold\"),\n        )\n\n    @staticmethod\n    def load(model_instance: Model) -> PredictorParams:\n        config_path   = model_instance.dir_mlp_pred  / \"config.json\"\n\n        if config_path.exists():\n            params = PredictorParams.loadPredictorJson(config_path)\n        else:\n            params = PredictorParams()\n\n        return params\n\n###### CONVERSION LOGIC ######\n\n\ndef parse_args() -> argparse.Namespace:\n    parser = argparse.ArgumentParser(\n        description=\"Convert a huggingface model to a GGML compatible file\"\n    )\n    parser.add_argument(\n        \"--vocab-only\",\n        action=\"store_true\",\n        help=\"extract only the vocab\",\n    )\n    parser.add_argument(\n        \"--outfile\",\n        type=Path,\n        help=\"path to write to; default: based on input\",\n    )\n    parser.add_argument(\n        \"--outtype\",\n        type=str,\n        choices=[\"f32\", \"f16\"],\n        default=\"f16\",\n        help=\"output format - use f32 for float32, f16 for float16\",\n    )\n    parser.add_argument(\n        \"--bigendian\",\n        action=\"store_true\",\n        help=\"model is executed on big endian machine\",\n    )\n    parser.add_argument(\n        \"model\",\n        type=Path,\n        help=\"directory containing model file\",\n    )\n    parser.add_argument(\n        \"mlp_predictors\",\n        type=Path,\n        help=\"directory containing MLP predictors for model\",\n    )\n\n    return parser.parse_args()\n\n\nargs = parse_args()\n\ndir_model = args.model\ndir_mlp_pred = args.mlp_predictors\nif not dir_model.is_dir():\n    print(f\"Error: {args.model} is not a directory\", file=sys.stderr)\n    sys.exit(1)\nif not dir_mlp_pred.is_dir():\n    print(f\"Error: {args.mlp_predictors} is not a directory\", file=sys.stderr)\n    sys.exit(1)\n\nftype_map = {\n    \"f32\": gguf.GGMLQuantizationType.F32,\n    \"f16\": gguf.GGMLQuantizationType.F16,\n}\n\nif args.outfile is not None:\n    fname_out = args.outfile\nelse:\n    # output in the same directory as the model by default\n    fname_out = dir_model / f\"ggml-model-{args.outtype}.gguf\"\n\nprint(f\"Loading model: {dir_model.name}\")\n\nhparams = Model.load_hparams(dir_model)\n\nmodel_class = Model.from_model_architecture(hparams[\"architectures\"][0])\nmodel_instance = model_class(\n    dir_model, dir_mlp_pred, ftype_map[args.outtype], fname_out, args.bigendian\n)\n\nprint(\"Set model parameters\")\nparams = PredictorParams.load(model_instance)\nmodel_instance.set_gguf_parameters(params)\n\nprint(\"Set model tokenizer\")\nmodel_instance.set_vocab()\n\nif args.vocab_only:\n    print(f\"Exporting model vocab to '{fname_out}'\")\n    model_instance.write_vocab()\nelse:\n    print(f\"Exporting model to '{fname_out}'\")\n    model_instance.write()\n\n# post-process: write another unique file header to distinguish from the origianl GGUF file\nwith open(fname_out, \"r+b\") as fout:\n    POWERINFER_MAGIC = int.from_bytes(b\"PWRI\", \"little\")\n    fout.write(struct.pack(\"<I\", POWERINFER_MAGIC))\n\nprint(f\"Model successfully exported to '{fname_out}'\")\n"
        },
        {
          "name": "convert.py",
          "type": "blob",
          "size": 53.4775390625,
          "content": "#!/usr/bin/env python3\nfrom __future__ import annotations\n\nimport argparse\nimport concurrent.futures\nimport dataclasses\nimport enum\nimport faulthandler\nimport functools\nimport itertools\nimport json\nimport math\nimport mmap\nimport pickle\nimport re\nimport signal\nimport struct\nimport subprocess\nimport sys\nimport time\nimport zipfile\nfrom abc import ABCMeta, abstractmethod\nfrom concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\nfrom dataclasses import dataclass\nfrom pathlib import Path\nfrom typing import IO, TYPE_CHECKING, Any, Callable, Iterable, Literal, TypeVar\n\nimport numpy as np\nfrom sentencepiece import SentencePieceProcessor\n\nimport os\nif 'NO_LOCAL_GGUF' not in os.environ:\n    sys.path.insert(1, str(Path(__file__).parent / 'gguf-py'))\nimport gguf\n\nif TYPE_CHECKING:\n    from typing import TypeAlias\n\nif hasattr(faulthandler, 'register') and hasattr(signal, 'SIGUSR1'):\n    faulthandler.register(signal.SIGUSR1)\n\nNDArray: TypeAlias = 'np.ndarray[Any, Any]'\n\nDEFAULT_CONCURRENCY = 8\n#\n# data types\n#\n\n@dataclass(frozen=True)\nclass DataType:\n    name: str\n    dtype: np.dtype[Any]\n    valid_conversions: list[str]\n\n    def elements_to_bytes(self, n_elements: int) -> int:\n        return n_elements * self.dtype.itemsize\n\n@dataclass(frozen=True)\nclass UnquantizedDataType(DataType):\n    pass\n\nDT_F16  = UnquantizedDataType('F16', dtype = np.dtype(np.float16), valid_conversions = ['F32', 'Q8_0'])\nDT_F32  = UnquantizedDataType('F32', dtype = np.dtype(np.float32), valid_conversions = ['F16', 'Q8_0'])\nDT_I32  = UnquantizedDataType('I32', dtype = np.dtype(np.int16), valid_conversions = [])\nDT_BF16 = UnquantizedDataType('BF16', dtype = np.dtype(np.uint16), valid_conversions = ['F32', 'F16', 'Q8_0'])\n\n@dataclass(frozen=True)\nclass QuantizedDataType(DataType):\n    block_size: int\n    quantized_dtype: np.dtype[Any]\n    ggml_type: gguf.GGMLQuantizationType\n\n    def quantize(self, arr: NDArray) -> NDArray:\n        raise NotImplementedError(f'Quantization for {self.name} not implemented')\n\n    def elements_to_bytes(self, n_elements: int) -> int:\n        assert n_elements % self.block_size == 0, f'Invalid number of elements {n_elements} for {self.name} with block size {self.block_size}'\n        return self.quantized_dtype.itemsize * (n_elements // self.block_size)\n\n@dataclass(frozen=True)\nclass Q8_0QuantizedDataType(QuantizedDataType):\n    # Mini Q8_0 quantization in Python!\n    def quantize(self, arr: NDArray) -> NDArray:\n        assert arr.size % self.block_size == 0 and arr.size != 0, f'Bad array size {arr.size}'\n        assert arr.dtype == np.float32, f'Bad array type {arr.dtype}'\n        n_blocks = arr.size // self.block_size\n        blocks = arr.reshape((n_blocks, self.block_size))\n        # Much faster implementation of block quantization contributed by @Cebtenzzre\n        def quantize_blocks_q8_0(blocks: NDArray) -> Iterable[tuple[Any, Any]]:\n            d = abs(blocks).max(axis = 1) / np.float32(127)\n            with np.errstate(divide = 'ignore'):\n                qs = (blocks / d[:, None]).round()\n            qs[d == 0] = 0\n            yield from zip(d, qs)\n        return np.fromiter(quantize_blocks_q8_0(blocks), count = n_blocks, dtype = self.quantized_dtype)\n\nDT_Q8_0 = Q8_0QuantizedDataType('Q8_0',\n    dtype = np.dtype(np.float32), valid_conversions = [],\n    ggml_type = gguf.GGMLQuantizationType.Q8_0, block_size = 32,\n    quantized_dtype = np.dtype([('d', '<f2'), ('qs', 'i1', (32,))]))\n\n# Quantized types skipped here because they may also map to np.float32\nNUMPY_TYPE_TO_DATA_TYPE: dict[np.dtype[Any], DataType] = {}\nfor dt in (DT_BF16, DT_F16, DT_F32, DT_I32):\n    if dt.dtype in NUMPY_TYPE_TO_DATA_TYPE:\n        raise ValueError(f'Invalid duplicate data type {dt}')\n    NUMPY_TYPE_TO_DATA_TYPE[dt.dtype] = dt\n\nSAFETENSORS_DATA_TYPES: dict[str, DataType] = {\n    'BF16': DT_BF16,\n    'F16': DT_F16,\n    'F32': DT_F32,\n    'I32': DT_I32,\n}\n\n# TODO: match this with `llama_ftype`\n# TODO: rename to LLAMAFileType\n# TODO: move to `gguf.py`\nclass GGMLFileType(enum.IntEnum):\n    AllF32     = 0\n    MostlyF16  = 1  # except 1d tensors\n    MostlyQ8_0 = 7  # except 1d tensors\n\n    def type_for_tensor(self, name: str, tensor: LazyTensor) -> DataType:\n        dt = GGML_FILE_TYPE_TO_DATA_TYPE.get(self)\n        if dt is None:\n            raise ValueError(self)\n        # 1D tensors are always F32.\n        return dt if len(tensor.shape) > 1 else DT_F32\n\nGGML_FILE_TYPE_TO_DATA_TYPE: dict[GGMLFileType, DataType] = {\n    GGMLFileType.AllF32    : DT_F32,\n    GGMLFileType.MostlyF16 : DT_F16,\n    GGMLFileType.MostlyQ8_0: DT_Q8_0,\n}\n\n#\n# hparams loading\n#\n\n@dataclass\nclass PredictorParams:\n    sparse_threshold: float | None = None\n\n    @staticmethod\n    def loadPredictorJson(model: LazyModel, config_path: Path) -> PredictorParams:\n        config = json.load(open(config_path))\n        return PredictorParams(\n            sparse_threshold = config.get(\"sparse_threshold\"),\n        )\n\n    @staticmethod\n    def load(model_plus: ModelPlus) -> PredictorParams:\n        config_path   = model_plus.paths[0].parent / \"config.json\"\n\n        if config_path.exists():\n            params = PredictorParams.loadPredictorJson(model_plus.model, config_path)\n        else:\n            params = PredictorParams()\n\n        return params\n\n@dataclass\nclass Params:\n    n_vocab:    int\n    n_embd:     int\n    n_layer:    int\n    n_ctx:      int\n    n_ff:       int\n    n_head:     int\n    n_head_kv:  int\n    f_norm_eps: float\n\n    arch:       gguf.MODEL_ARCH = gguf.MODEL_ARCH.LLAMA\n    rope_scaling_type: gguf.RopeScalingType | None = None\n    f_rope_freq_base: float | None = None\n    f_rope_scale: float | None = None\n    n_orig_ctx: int | None = None\n    rope_finetuned: bool | None = None\n\n    ftype: GGMLFileType | None = None\n\n    # path to the directory containing the model files\n    path_model: Path | None = None\n\n    # MLP predictor parameters\n    predictor_params: PredictorParams = dataclasses.field(default_factory=PredictorParams)\n\n    @staticmethod\n    def guessed(model: LazyModel) -> Params:\n        # try transformer naming first\n        n_vocab, n_embd = model[\"model.embed_tokens.weight\"].shape if \"model.embed_tokens.weight\" in model else model[\"tok_embeddings.weight\"].shape\n\n        # try transformer naming first\n        if \"model.layers.0.self_attn.q_proj.weight\" in model:\n            n_layer=next(i for i in itertools.count() if f\"model.layers.{i}.self_attn.q_proj.weight\" not in model)\n        elif \"model.layers.0.self_attn.W_pack.weight\" in model:   # next: try baichuan naming\n            n_layer=next(i for i in itertools.count() if f\"model.layers.{i}.self_attn.W_pack.weight\" not in model)\n        else:\n            n_layer=next(i for i in itertools.count() if f\"layers.{i}.attention.wq.weight\" not in model)\n\n        if n_layer < 1:\n            raise Exception(\"failed to guess 'n_layer'. This model is unknown or unsupported.\\n\"\n                            \"Suggestion: provide 'config.json' of the model in the same directory containing model files.\")\n\n        n_head = n_embd // 128 # guessed\n        n_mult = 256           # guessed\n\n        # TODO: verify this\n        n_ff = int(2 * (4 * n_embd) / 3)\n        n_ff = n_mult * ((n_ff + n_mult - 1) // n_mult)\n\n        return Params(\n            n_vocab    = n_vocab,\n            n_embd     = n_embd,\n            n_layer    = n_layer,\n            n_ctx      = -1,\n            n_ff       = n_ff,\n            n_head     = n_head,\n            n_head_kv  = n_head,\n            f_norm_eps = 1e-5,\n        )\n\n    @staticmethod\n    def loadHFTransformerJson(model: LazyModel, config_path: Path) -> Params:\n        config = json.load(open(config_path))\n\n        rope_scaling_type = f_rope_scale = n_orig_ctx = rope_finetuned = None\n        rope_scaling = config.get(\"rope_scaling\")\n\n        if rope_scaling is not None and (typ := rope_scaling.get(\"type\")):\n            rope_factor = rope_scaling.get(\"factor\")\n            f_rope_scale = rope_factor\n            if typ == \"linear\":\n                rope_scaling_type = gguf.RopeScalingType.LINEAR\n            elif typ == \"yarn\":\n                rope_scaling_type = gguf.RopeScalingType.YARN\n                n_orig_ctx = rope_scaling['original_max_position_embeddings']\n                rope_finetuned = rope_scaling['finetuned']\n            else:\n                raise NotImplementedError(f'Unknown rope scaling type: {typ}')\n\n        if \"max_sequence_length\" in config:\n            n_ctx = config[\"max_sequence_length\"]\n        elif \"max_position_embeddings\" in config:\n            n_ctx = config[\"max_position_embeddings\"]\n        else:\n            raise Exception(\"failed to guess 'n_ctx'. This model is unknown or unsupported.\\n\"\n                            \"Suggestion: provide 'config.json' of the model in the same directory containing model files.\")\n\n        params = Params(\n            n_vocab           = config[\"vocab_size\"],\n            n_embd            = config[\"hidden_size\"],\n            n_layer           = config[\"num_hidden_layers\"],\n            n_ctx             = n_ctx,\n            n_ff              = config[\"intermediate_size\"],\n            n_head            = (n_head := config[\"num_attention_heads\"]),\n            n_head_kv         = config.get(\"num_key_value_heads\", n_head),\n            f_norm_eps        = config[\"rms_norm_eps\"],\n            f_rope_freq_base  = config.get(\"rope_theta\"),\n            rope_scaling_type = rope_scaling_type,\n            f_rope_scale      = f_rope_scale,\n            n_orig_ctx        = n_orig_ctx,\n            rope_finetuned    = rope_finetuned,\n        )\n\n        if config.get(\"model_type\", None) == \"bamboo\":\n            params.arch = gguf.MODEL_ARCH.BAMBOO\n\n        return params\n\n    # LLaMA v2 70B params.json\n    # {\"dim\": 8192, \"multiple_of\": 4096, \"ffn_dim_multiplier\": 1.3, \"n_heads\": 64, \"n_kv_heads\": 8, \"n_layers\": 80, \"norm_eps\": 1e-05, \"vocab_size\": -1}\n    @staticmethod\n    def loadOriginalParamsJson(model: LazyModel, config_path: Path) -> Params:\n        config = json.load(open(config_path))\n\n        # hack to determine LLaMA v1 vs v2 vs CodeLlama\n        if config.get(\"rope_theta\") == 1000000:\n            # CodeLlama\n            n_ctx = 16384\n        elif config[\"norm_eps\"] == 1e-05:\n            # LLaMA v2\n            n_ctx = 4096\n        else:\n            # LLaMA v1\n            n_ctx = 2048\n\n        return Params(\n            n_vocab          = config.get(\"vocab_size\", model[\"tok_embeddings.weight\"].shape[0]),\n            n_embd           = config[\"dim\"],\n            n_layer          = config[\"n_layers\"],\n            n_ctx            = n_ctx,\n            n_ff             = model[\"layers.0.feed_forward.w1.weight\"].shape[0],\n            n_head           = (n_head := config[\"n_heads\"]),\n            n_head_kv        = config.get(\"n_kv_heads\", n_head),\n            f_norm_eps       = config[\"norm_eps\"],\n            f_rope_freq_base = config.get(\"rope_theta\"),\n        )\n\n    @staticmethod\n    def load(model_plus: ModelPlus) -> Params:\n        hf_config_path   = model_plus.paths[0].parent / \"config.json\"\n        orig_config_path = model_plus.paths[0].parent / \"params.json\"\n\n        if hf_config_path.exists():\n            params = Params.loadHFTransformerJson(model_plus.model, hf_config_path)\n        elif orig_config_path.exists():\n            params = Params.loadOriginalParamsJson(model_plus.model, orig_config_path)\n        elif model_plus.format != 'none':\n            params = Params.guessed(model_plus.model)\n        else:\n            raise ValueError('Cannot guess params when model format is none')\n\n        params.path_model = model_plus.paths[0].parent\n\n        return params\n\n\n#\n# vocab\n#\n\nclass BpeVocab:\n    def __init__(self, fname_tokenizer: Path, fname_added_tokens: Path | None) -> None:\n        self.bpe_tokenizer = json.loads(open(str(fname_tokenizer), encoding=\"utf-8\").read())\n        added_tokens: dict[str, int]\n        if fname_added_tokens is not None:\n            # FIXME: Verify that added tokens here _cannot_ overlap with the main vocab.\n            added_tokens = json.load(open(fname_added_tokens, encoding=\"utf-8\"))\n        else:\n            # Fall back to trying to find the added tokens in tokenizer.json\n            tokenizer_json_file = fname_tokenizer.parent / 'tokenizer.json'\n            if not tokenizer_json_file.is_file():\n                added_tokens = {}\n            else:\n                tokenizer_json = json.load(open(tokenizer_json_file, encoding=\"utf-8\"))\n                added_tokens = dict(\n                    (item['content'], item['id'])\n                    for item in tokenizer_json.get('added_tokens', [])\n                    # Added tokens here can be duplicates of the main vocabulary.\n                    if item['content'] not in self.bpe_tokenizer )\n\n        vocab_size: int = len(self.bpe_tokenizer)\n        expected_ids    = list(range(vocab_size, vocab_size + len(added_tokens)))\n        actual_ids      = sorted(added_tokens.values())\n        if expected_ids != actual_ids:\n            expected_end_id = vocab_size + len(actual_ids) - 1\n            raise Exception(f\"Expected the {len(actual_ids)} added token ID(s) to be sequential in the range {vocab_size} - {expected_end_id}; got {actual_ids}\")\n\n        items = sorted(added_tokens.items(), key=lambda text_idx: text_idx[1])\n        self.added_tokens_list    = [text for (text, idx) in items]\n        self.vocab_size_base: int = vocab_size\n        self.vocab_size: int      = self.vocab_size_base + len(self.added_tokens_list)\n        self.fname_tokenizer      = fname_tokenizer\n        self.fname_added_tokens   = fname_added_tokens\n\n    def bpe_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        tokenizer = self.bpe_tokenizer\n        from transformers.models.gpt2 import tokenization_gpt2\n        reverse_vocab = {id: encoded_tok for encoded_tok, id in tokenizer.items()}\n\n        for i, _ in enumerate(tokenizer):\n            yield reverse_vocab[i], 0.0, gguf.TokenType.NORMAL\n\n    def added_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        for text in self.added_tokens_list:\n            score = -1000.0\n            yield text.encode(\"utf-8\"), score, gguf.TokenType.CONTROL\n\n    def all_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        yield from self.bpe_tokens()\n        yield from self.added_tokens()\n\n    def __repr__(self) -> str:\n        return f\"<BpeVocab with {self.vocab_size_base} base tokens and {len(self.added_tokens_list)} added tokens>\"\n\n\nclass SentencePieceVocab:\n    def __init__(self, fname_tokenizer: Path, fname_added_tokens: Path | None) -> None:\n        self.sentencepiece_tokenizer = SentencePieceProcessor(str(fname_tokenizer))\n        added_tokens: dict[str, int]\n        if fname_added_tokens is not None:\n            added_tokens = json.load(open(fname_added_tokens, encoding=\"utf-8\"))\n        else:\n            added_tokens = {}\n\n        vocab_size: int = self.sentencepiece_tokenizer.vocab_size()\n\n        new_tokens       = {id: piece for piece, id in added_tokens.items() if id >= vocab_size}\n        expected_new_ids = list(range(vocab_size, vocab_size + len(new_tokens)))\n        actual_new_ids   = sorted(new_tokens.keys())\n\n        if expected_new_ids != actual_new_ids:\n            raise ValueError(f\"Expected new token IDs {expected_new_ids} to be sequential; got {actual_new_ids}\")\n\n        # Token pieces that were added to the base vocabulary.\n        self.added_tokens_list  = [new_tokens[id] for id in actual_new_ids]\n        self.vocab_size_base    = vocab_size\n        self.vocab_size         = self.vocab_size_base + len(self.added_tokens_list)\n        self.fname_tokenizer    = fname_tokenizer\n        self.fname_added_tokens = fname_added_tokens\n\n    def sentencepiece_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        tokenizer = self.sentencepiece_tokenizer\n        for i in range(tokenizer.vocab_size()):\n            piece = tokenizer.id_to_piece(i)\n            text: bytes = piece.encode(\"utf-8\")\n            score: float = tokenizer.get_score(i)\n\n            toktype = gguf.TokenType.NORMAL\n            if tokenizer.is_unknown(i):\n                toktype = gguf.TokenType.UNKNOWN\n            if tokenizer.is_control(i):\n                toktype = gguf.TokenType.CONTROL\n\n            # NOTE: I think added_tokens are user defined.\n            # ref: https://github.com/google/sentencepiece/blob/master/src/sentencepiece_model.proto\n            # if tokenizer.is_user_defined(i): toktype = gguf.TokenType.USER_DEFINED\n\n            if tokenizer.is_unused(i):\n                toktype = gguf.TokenType.UNUSED\n            if tokenizer.is_byte(i):\n                toktype = gguf.TokenType.BYTE\n\n            yield text, score, toktype\n\n    def added_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        for text in self.added_tokens_list:\n            score = -1000.0\n            yield text.encode(\"utf-8\"), score, gguf.TokenType.USER_DEFINED\n\n    def all_tokens(self) -> Iterable[tuple[bytes, float, gguf.TokenType]]:\n        yield from self.sentencepiece_tokens()\n        yield from self.added_tokens()\n\n    def __repr__(self) -> str:\n        return f\"<SentencePieceVocab with {self.vocab_size_base} base tokens and {len(self.added_tokens_list)} added tokens>\"\n\nVocab: TypeAlias = 'BpeVocab | SentencePieceVocab'\n\n#\n# data loading\n# TODO: reuse (probably move to gguf.py?)\n#\n\ndef permute(weights: NDArray, n_head: int, n_head_kv: int) -> NDArray:\n    #print( \"permute debug \" + str(weights.shape[0]) + \" x \" + str(weights.shape[1]) + \" nhead \" + str(n_head) + \" nheadkv \" + str(n_kv_head) )\n    if n_head_kv is not None and n_head != n_head_kv:\n        n_head = n_head_kv\n    return (weights.reshape(n_head, 2, weights.shape[0] // n_head // 2, *weights.shape[1:])\n                .swapaxes(1, 2)\n                .reshape(weights.shape))\n\n\nclass Tensor(metaclass=ABCMeta):\n    data_type: DataType\n\n    @abstractmethod\n    def astype(self, data_type: DataType) -> Tensor: ...\n    @abstractmethod\n    def permute(self, n_head: int, n_head_kv: int) -> Tensor: ...\n    @abstractmethod\n    def permute_part(self, n_part: int, n_head: int, n_head_kv: int) -> UnquantizedTensor: ...\n    @abstractmethod\n    def part(self, n_part: int) -> UnquantizedTensor: ...\n    @abstractmethod\n    def to_ggml(self) -> GGMLCompatibleTensor: ...\n\n\ndef bf16_to_fp32(bf16_arr: np.ndarray[Any, np.dtype[np.uint16]]) -> NDArray:\n    assert bf16_arr.dtype == np.uint16, f\"Input array should be of dtype uint16, but got {bf16_arr.dtype}\"\n    fp32_arr = bf16_arr.astype(np.uint32) << 16\n    return fp32_arr.view(np.float32)\n\n\nclass UnquantizedTensor(Tensor):\n    def __init__(self, ndarray: NDArray) -> None:\n        assert isinstance(ndarray, np.ndarray)\n        self.ndarray = ndarray\n        self.data_type = NUMPY_TYPE_TO_DATA_TYPE[ndarray.dtype]\n\n    def astype(self, data_type: DataType) -> Tensor:\n        dtype = data_type.dtype\n        if self.data_type == DT_BF16:\n            self.ndarray = bf16_to_fp32(self.ndarray)\n        return UnquantizedTensor(self.ndarray.astype(dtype))\n\n    def to_ggml(self) -> UnquantizedTensor:\n        return self\n\n    def permute_part(self, n_part: int, n_head: int, n_head_kv: int) -> UnquantizedTensor:\n        r = self.ndarray.shape[0] // 3\n        return UnquantizedTensor(permute(self.ndarray[r * n_part : r * n_part + r, ...], n_head, n_head_kv))\n\n    def part(self, n_part: int) -> UnquantizedTensor:\n        r = self.ndarray.shape[0] // 3\n        return UnquantizedTensor(self.ndarray[r * n_part : r * n_part + r, ...])\n\n    def permute(self, n_head: int, n_head_kv: int) -> UnquantizedTensor:\n        return UnquantizedTensor(permute(self.ndarray, n_head, n_head_kv))\n\n\ndef load_unquantized(lazy_tensor: LazyTensor, expected_dtype: Any = None, convert: bool = False) -> NDArray:\n    tensor = lazy_tensor.load()\n    assert isinstance(tensor, UnquantizedTensor)\n\n    # double-check:\n    actual_shape = list(tensor.ndarray.shape)\n    assert actual_shape == lazy_tensor.shape, (actual_shape, lazy_tensor.shape)\n    if expected_dtype is not None and expected_dtype != tensor.ndarray.dtype:\n        if convert:\n            tensor.ndarray = tensor.ndarray.astype(expected_dtype)\n        else:\n            raise ValueError(f'expected this tensor to have dtype {expected_dtype}, got {tensor.ndarray.dtype}')\n\n    return tensor.ndarray\n\n\nGGMLCompatibleTensor = UnquantizedTensor\n\n\n@dataclass\nclass LazyTensor:\n    _load: Callable[[], Tensor]\n    shape: list[int]\n    data_type: DataType\n    description: str\n\n    def load(self) -> Tensor:\n        ret = self._load()\n        # Should be okay if it maps to the same numpy type?\n        assert ret.data_type == self.data_type or (self.data_type.dtype == ret.data_type.dtype), \\\n                (self.data_type, ret.data_type, self.description)\n        return ret\n\n    def astype(self, data_type: DataType) -> LazyTensor:\n        self.validate_conversion_to(data_type)\n\n        def load() -> Tensor:\n            return self.load().astype(data_type)\n        return LazyTensor(load, self.shape, data_type, f'convert({data_type}) {self.description}')\n    \n    def transposed(self) -> LazyTensor:\n        def load() -> Tensor:\n            loaded = self.load()\n            assert isinstance(loaded, UnquantizedTensor), f'Cannot transpose {loaded}'\n            loaded.ndarray = loaded.ndarray.T\n            return loaded\n        return LazyTensor(load, self.shape[::-1], self.data_type, f'transpose {self.description}')\n\n    def validate_conversion_to(self, data_type: DataType) -> None:\n        if data_type != self.data_type and data_type.name not in self.data_type.valid_conversions:\n            raise ValueError(f'Cannot validate conversion from {self.data_type} to {data_type}.')\n\n\nLazyModel: TypeAlias = 'dict[str, LazyTensor]'\n\n\n@dataclass\nclass ModelPlus:\n    model: LazyModel\n    paths: list[Path]  # Where this was read from.\n    format: Literal['ggml', 'torch', 'safetensors', 'none']\n    vocab: Vocab | None  # For GGML models (which have vocab built in), the vocab.\n\n\ndef merge_sharded(models: list[LazyModel]) -> LazyModel:\n    # Original LLaMA models have each file contain one part of each tensor.\n    # Use a dict instead of a set to preserve order.\n    names = {name: None for model in models for name in model}\n\n    def convert(name: str) -> LazyTensor:\n        lazy_tensors: list[LazyTensor] = [model[name] for model in models]\n        if len(lazy_tensors) == 1:\n            # only one file; don't go through this procedure since there might\n            # be quantized tensors\n            return lazy_tensors[0]\n        if len(lazy_tensors[0].shape) == 1:\n            # the tensor is just duplicated in every file\n            return lazy_tensors[0]\n        if name.startswith('tok_embeddings.') or \\\n           name.endswith('.attention.wo.weight') or \\\n           name.endswith('.feed_forward.w2.weight'):\n            # split by columns\n            axis = 1\n        else:\n            # split by rows\n            axis = 0\n        concatenated_shape = list(lazy_tensors[0].shape)\n        concatenated_shape[axis] = sum(tensor.shape[axis] for tensor in lazy_tensors)\n\n        def load() -> UnquantizedTensor:\n            ndarrays = [load_unquantized(tensor) for tensor in lazy_tensors]\n            concatenated: NDArray = np.concatenate(ndarrays, axis=axis)\n            return UnquantizedTensor(concatenated)\n        description = 'concatenated[[' + '] | ['.join(lt.description for lt in lazy_tensors) + ']]'\n        return LazyTensor(load, concatenated_shape, lazy_tensors[0].data_type, description)\n    return {name: convert(name) for name in names}\n\n\ndef merge_multifile_models(models_plus: list[ModelPlus]) -> ModelPlus:\n    formats = set(mp.format for mp in models_plus)\n    # assert len(formats) == 1, \"different formats?\"\n    format = formats.pop()\n    paths = [path for mp in models_plus for path in mp.paths]\n    # Use the first non-None vocab, if any.\n    try:\n        vocab = next(mp.vocab for mp in models_plus if mp.vocab is not None)\n    except StopIteration:\n        vocab = None\n\n    if any(\"model.embed_tokens.weight\" in mp.model for mp in models_plus) or \\\n       any(\"model.layers.0.fc1.weight\" in mp.model for mp in models_plus):\n        # Transformers models put different tensors in different files, but\n        # don't split indivdual tensors between files.\n        model: LazyModel = {}\n        for mp in models_plus:\n            model.update(mp.model)\n    else:\n        model = merge_sharded([mp.model for mp in models_plus])\n\n    return ModelPlus(model, paths, format, vocab)\n\n\ndef permute_lazy(lazy_tensor: LazyTensor, n_head: int, n_head_kv: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().permute(n_head, n_head_kv)\n    return LazyTensor(load, lazy_tensor.shape, lazy_tensor.data_type, f'permute({n_head}, {n_head_kv}) ' + lazy_tensor.description)\n\ndef permute_part_lazy(lazy_tensor: LazyTensor, n_part: int, n_head: int, n_head_kv: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().permute_part(n_part, n_head, n_head_kv)\n    s = lazy_tensor.shape.copy()\n    s[0] = s[0] // 3\n    return LazyTensor(load, s, lazy_tensor.data_type, f'permute({n_head}, {n_head_kv}) ' + lazy_tensor.description)\n\ndef part_lazy(lazy_tensor: LazyTensor, n_part: int) -> LazyTensor:\n    def load() -> Tensor:\n        return lazy_tensor.load().part(n_part)\n    s = lazy_tensor.shape.copy()\n    s[0] = s[0] // 3\n    return LazyTensor(load, s, lazy_tensor.data_type, 'part ' + lazy_tensor.description)\n\n\n# Functionality that simulates `torch.load` but where individual tensors are\n# only loaded into memory on demand, not all at once.\n# PyTorch can't do this natively as of time of writing:\n# - https://github.com/pytorch/pytorch/issues/64327\n# This allows us to de-shard without multiplying RAM usage, and also\n# conveniently drops the PyTorch dependency (though we still need numpy).\n\n\n@dataclass\nclass LazyStorageKind:\n    data_type: DataType\n\n\n@dataclass\nclass LazyStorage:\n    load: Callable[[int, int], NDArray]\n    kind: LazyStorageKind\n    description: str\n\n\nclass LazyUnpickler(pickle.Unpickler):\n    def __init__(self, fp: IO[bytes], data_base_path: str, zip_file: zipfile.ZipFile):\n        super().__init__(fp)\n        self.data_base_path = data_base_path\n        self.zip_file = zip_file\n\n    def persistent_load(self, pid: Any) -> Any:\n        assert pid[0] == 'storage'\n        assert isinstance(pid[1], LazyStorageKind)\n        data_type = pid[1].data_type\n        filename_stem = pid[2]\n        filename = f'{self.data_base_path}/{filename_stem}'\n        info = self.zip_file.getinfo(filename)\n\n        def load(offset: int, elm_count: int) -> NDArray:\n            dtype = data_type.dtype\n            fp = self.zip_file.open(info)\n            fp.seek(offset * dtype.itemsize)\n            size = elm_count * dtype.itemsize\n            data = fp.read(size)\n            assert len(data) == size\n            return np.frombuffer(data, dtype)\n        description = f'storage data_type={data_type} path-in-zip={filename} path={self.zip_file.filename}'\n        return LazyStorage(load=load, kind=pid[1], description=description)\n\n    @staticmethod\n    def lazy_rebuild_tensor_v2(storage: Any, storage_offset: Any, size: Any, stride: Any,\n                               requires_grad: Any, backward_hooks: Any, metadata: Any = None) -> LazyTensor:\n        assert isinstance(storage, LazyStorage)\n\n        def load() -> UnquantizedTensor:\n            elm_count = stride[0] * size[0]\n            return UnquantizedTensor(storage.load(storage_offset, elm_count).reshape(size))\n        description = f'pickled storage_offset={storage_offset} in {storage.description}'\n        return LazyTensor(load, list(size), storage.kind.data_type, description)\n\n    @staticmethod\n    def rebuild_from_type_v2(func, new_type, args, state):\n        return func(*args)\n\n    CLASSES: dict[tuple[str, str], Any] = {\n        # getattr used here as a workaround for mypy not being smart enough to detrmine\n        # the staticmethods have a __func__ attribute.\n        ('torch._tensor', '_rebuild_from_type_v2'): getattr(rebuild_from_type_v2, '__func__'),\n        ('torch._utils', '_rebuild_tensor_v2'): getattr(lazy_rebuild_tensor_v2, '__func__'),\n        ('torch', 'BFloat16Storage'): LazyStorageKind(DT_BF16),\n        ('torch', 'HalfStorage'): LazyStorageKind(DT_F16),\n        ('torch', 'FloatStorage'): LazyStorageKind(DT_F32),\n        ('torch', 'IntStorage'): LazyStorageKind(DT_I32),\n        ('torch', 'Tensor'): LazyTensor,\n    }\n\n    def find_class(self, module: str, name: str) -> Any:\n        if not module.startswith('torch'):\n            return super().find_class(module, name)\n        return self.CLASSES[(module, name)]\n\n\ndef lazy_load_torch_file(outer_fp: IO[bytes], path: Path) -> ModelPlus:\n    zf = zipfile.ZipFile(outer_fp)\n    pickle_paths = [name for name in zf.namelist() if name.endswith('.pkl')]\n    assert len(pickle_paths) == 1, pickle_paths\n    pickle_fp = zf.open(pickle_paths[0], 'r')\n    unpickler = LazyUnpickler(pickle_fp,\n                              data_base_path=pickle_paths[0][:-4],\n                              zip_file=zf)\n    model = unpickler.load()\n    as_dict = dict(model.items())\n    return ModelPlus(model=as_dict, paths=[path], format='torch', vocab=None)\n\n\ndef lazy_load_safetensors_file(fp: IO[bytes], path: Path) -> ModelPlus:\n    header_size, = struct.unpack('<Q', fp.read(8))\n    header: dict[str, dict[str, Any]] = json.loads(fp.read(header_size))\n    # Use mmap for the actual data to avoid race conditions with the file offset.\n    mapped = memoryview(mmap.mmap(fp.fileno(), 0, access=mmap.ACCESS_READ))\n    byte_buf = mapped[8 + header_size:]\n\n    def convert(info: dict[str, Any]) -> LazyTensor:\n        data_type = SAFETENSORS_DATA_TYPES[info['dtype']]\n        numpy_dtype = data_type.dtype\n        shape: list[int] = info['shape']\n        begin, end = info['data_offsets']\n        assert 0 <= begin <= end <= len(byte_buf)\n        assert end - begin == math.prod(shape) * numpy_dtype.itemsize\n        buf = byte_buf[begin:end]\n\n        def load() -> UnquantizedTensor:\n            return UnquantizedTensor(np.frombuffer(buf, dtype=numpy_dtype).reshape(shape))\n        description = f'safetensors begin={begin} end={end} type={data_type} path={path}'\n        return LazyTensor(load, shape, data_type, description)\n    model = {name: convert(info) for (name, info) in header.items() if name != '__metadata__'}\n    return ModelPlus(model=model, paths=[path], format='safetensors', vocab=None)\n\n\ndef must_read(fp: IO[bytes], length: int) -> bytes:\n    ret = fp.read(length)\n    if len(ret) < length:\n        raise Exception(\"unexpectedly reached end of file\")\n    return ret\n\n\n@functools.lru_cache(maxsize=None)\ndef lazy_load_file(path: Path) -> ModelPlus:\n    fp = open(path, 'rb')\n    first8 = fp.read(8)\n    fp.seek(0)\n    if first8[:2] == b'PK':\n        # A zip file, i.e. PyTorch format\n        return lazy_load_torch_file(fp, path)\n    elif struct.unpack('<Q', first8)[0] < 16 * 1024 * 1024:\n        # Probably safetensors\n        return lazy_load_safetensors_file(fp, path)\n    else:\n        raise ValueError(f\"unknown format: {path}\")\n\n\nIn = TypeVar('In')\nOut = TypeVar('Out')\n\ndef bounded_parallel_map(func: Callable[[In], Out], iterable: Iterable[In], concurrency: int, max_workers: int | None = None, use_processpool_executor: bool = False) -> Iterable[Out]:\n    '''Parallel map, but with backpressure.  If the caller doesn't call `next`\n    fast enough, this will stop calling `func` at some point rather than\n    letting results pile up in memory.  Specifically, there is a max of one\n    output value buffered per thread.'''\n    if concurrency < 2:\n        yield from map(func, iterable)\n        # Not reached.\n    iterable = iter(iterable)\n    executor_class: type[ThreadPoolExecutor] | type[ProcessPoolExecutor]\n    if use_processpool_executor:\n        executor_class = ProcessPoolExecutor\n    else:\n        executor_class = ThreadPoolExecutor\n    with executor_class(max_workers = max_workers) as executor:\n        futures: list[concurrent.futures.Future[Out]] = []\n        done = False\n        for _ in range(concurrency):\n            try:\n                futures.append(executor.submit(func, next(iterable)))\n            except StopIteration:\n                done = True\n                break\n\n        while futures:\n            result = futures.pop(0).result()\n            while not done and len(futures) < concurrency:\n                try:\n                    futures.append(executor.submit(func, next(iterable)))\n                except StopIteration:\n                    done = True\n                    break\n            yield result\n\ndef check_vocab_size(params: Params, vocab: Vocab) -> None:\n    if params.n_vocab != vocab.vocab_size:\n        assert isinstance(vocab, BpeVocab) or isinstance(vocab, SentencePieceVocab)\n        if params.n_vocab == vocab.vocab_size_base:\n            print(\"Ignoring added_tokens.json since model matches vocab size without it.\")\n            vocab.added_tokens_list = []\n            vocab.vocab_size = vocab.vocab_size_base\n            return\n        msg = f\"Vocab size mismatch (model has {params.n_vocab}, but {vocab.fname_tokenizer}\"\n        if vocab.fname_added_tokens is not None:\n            msg += f\" combined with {vocab.fname_added_tokens}\"\n        msg += f\" has {vocab.vocab_size}).\"\n        if vocab.vocab_size < params.n_vocab < vocab.vocab_size + 20 and vocab.fname_added_tokens is None:\n            msg += f\"  Most likely you are missing added_tokens.json (should be in {vocab.fname_tokenizer.parent}).\"\n        raise Exception(msg)\n\n\nclass OutputFile:\n    def __init__(self, fname_out: Path, arch: gguf.MODEL_ARCH, endianess:gguf.GGUFEndian=gguf.GGUFEndian.LITTLE) -> None:\n        self.gguf = gguf.GGUFWriter(fname_out, gguf.MODEL_ARCH_NAMES[arch], endianess=endianess)\n\n    def add_meta_arch(self, params: Params) -> None:\n        name = \"LLaMA\"\n\n        # TODO: better logic to determine model name\n        if params.n_ctx == 4096:\n            name = \"LLaMA v2\"\n        elif params.path_model is not None:\n            name = str(params.path_model).split('/')[-1]\n\n        self.gguf.add_name                (name)\n        self.gguf.add_context_length      (params.n_ctx)\n        self.gguf.add_embedding_length    (params.n_embd)\n        self.gguf.add_block_count         (params.n_layer)\n        self.gguf.add_feed_forward_length (params.n_ff)\n        self.gguf.add_rope_dimension_count(params.n_embd // params.n_head)\n        self.gguf.add_head_count          (params.n_head)\n        self.gguf.add_head_count_kv       (params.n_head_kv)\n        self.gguf.add_layer_norm_rms_eps  (params.f_norm_eps)\n\n        if params.f_rope_freq_base is not None:\n            self.gguf.add_rope_freq_base(params.f_rope_freq_base)\n\n        if params.rope_scaling_type:\n            assert params.f_rope_scale is not None\n            self.gguf.add_rope_scaling_type(params.rope_scaling_type)\n            self.gguf.add_rope_scaling_factor(params.f_rope_scale)\n\n        if params.n_orig_ctx is not None:\n            self.gguf.add_rope_scaling_orig_ctx_len(params.n_orig_ctx)\n\n        if params.rope_finetuned is not None:\n            self.gguf.add_rope_scaling_finetuned(params.rope_finetuned)\n\n        if params.ftype is not None:\n            self.gguf.add_file_type(params.ftype)\n\n        if params.predictor_params.sparse_threshold is not None:\n            self.gguf.add_sparse_threshold(params.predictor_params.sparse_threshold)\n\n    def add_meta_vocab(self, vocab: Vocab) -> None:\n        tokens = []\n        scores = []\n        toktypes = []\n        # NOTE: `all_tokens` returns the base vocabulary and added tokens\n        for text, score, toktype in vocab.all_tokens():\n            tokens.append(text)\n            scores.append(score)\n            toktypes.append(toktype)\n\n        if isinstance(vocab, SentencePieceVocab):\n            self.gguf.add_tokenizer_model(\"llama\")\n        elif isinstance(vocab, BpeVocab):\n            self.gguf.add_tokenizer_model(\"gpt2\")\n        else:\n            raise ValueError('Unknown vocab type: Not BpeVocab or SentencePieceVocab')\n        self.gguf.add_token_list(tokens)\n        self.gguf.add_token_scores(scores)\n        self.gguf.add_token_types(toktypes)\n\n    def add_meta_special_vocab(self, svocab: gguf.SpecialVocab) -> None:\n        svocab.add_to_gguf(self.gguf)\n\n    def add_tensor_info(self, name: str, tensor: LazyTensor) -> None:\n        n_elements = int(np.prod(tensor.shape))\n        raw_dtype = getattr(tensor.data_type, 'ggml_type', None)\n        data_type = getattr(tensor.data_type, 'quantized_type', None) or tensor.data_type.dtype\n        data_nbytes = tensor.data_type.elements_to_bytes(n_elements)\n        self.gguf.add_tensor_info(name, tensor.shape, data_type, data_nbytes, raw_dtype = raw_dtype)\n\n    def write_meta(self) -> None:\n        self.gguf.write_header_to_file()\n        self.gguf.write_kv_data_to_file()\n\n    def write_tensor_info(self) -> None:\n        self.gguf.write_ti_data_to_file()\n\n    def close(self) -> None:\n        self.gguf.close()\n\n    @staticmethod\n    def write_vocab_only(fname_out: Path, params: Params, vocab: Vocab, svocab: gguf.SpecialVocab, endianess:gguf.GGUFEndian=gguf.GGUFEndian.LITTLE) -> None:\n        check_vocab_size(params, vocab)\n\n        of = OutputFile(fname_out, params.arch, endianess=endianess)\n\n        # meta data\n        of.add_meta_arch(params)\n        of.add_meta_vocab(vocab)\n        of.add_meta_special_vocab(svocab)\n\n        of.write_meta()\n\n        of.close()\n\n    @staticmethod\n    def do_item(item: tuple[str, LazyTensor]) -> tuple[DataType, NDArray]:\n        name, lazy_tensor = item\n        tensor = lazy_tensor.load().to_ggml()\n        return (lazy_tensor.data_type, tensor.ndarray)\n\n    @staticmethod\n    def maybe_do_quantize(item: tuple[DataType, NDArray]) -> NDArray:\n        dt, arr = item\n        if not isinstance(dt, QuantizedDataType):\n            return arr\n        return dt.quantize(arr)\n\n    @staticmethod\n    def write_all(fname_out: Path, ftype: GGMLFileType, params: Params, model: LazyModel, vocab: Vocab, svocab: gguf.SpecialVocab, concurrency: int = DEFAULT_CONCURRENCY, endianess: gguf.GGUFEndian = gguf.GGUFEndian.LITTLE) -> None:\n        check_vocab_size(params, vocab)\n\n        of = OutputFile(fname_out, params.arch, endianess=endianess)\n\n        # meta data\n        of.add_meta_arch(params)\n        of.add_meta_vocab(vocab)\n        of.add_meta_special_vocab(svocab)\n\n        # tensor info\n        for name, lazy_tensor in model.items():\n            of.add_tensor_info(name, lazy_tensor)\n\n        of.write_meta()\n        of.write_tensor_info()\n\n        # tensor data\n        ndarrays_inner = bounded_parallel_map(OutputFile.do_item, model.items(), concurrency = concurrency)\n        if ftype == GGMLFileType.MostlyQ8_0:\n            ndarrays = bounded_parallel_map(OutputFile.maybe_do_quantize, ndarrays_inner, concurrency = concurrency, max_workers = concurrency, use_processpool_executor = True)\n        else:\n            ndarrays = map(OutputFile.maybe_do_quantize, ndarrays_inner)\n\n        start = time.time()\n        for i, ((name, lazy_tensor), ndarray) in enumerate(zip(model.items(), ndarrays)):\n            elapsed = time.time() - start\n            size = ' x '.join(f\"{dim:6d}\" for dim in lazy_tensor.shape)\n            padi = len(str(len(model)))\n            print(f\"[{i+1:{padi}d}/{len(model)}] Writing tensor {name:38s} | size {size:16} | type {lazy_tensor.data_type.name:4} | T+{int(elapsed):4}\")\n            of.gguf.write_tensor_data(ndarray)\n\n        of.close()\n\ndef pick_output_type(model: LazyModel, output_type_str: str | None) -> GGMLFileType:\n    wq_type = model[gguf.TENSOR_NAMES[gguf.MODEL_TENSOR.ATTN_Q].format(bid=0)+\".weight\"].data_type\n\n    if output_type_str == \"f32\" or (output_type_str is None and wq_type == DT_F32):\n        return GGMLFileType.AllF32\n    if output_type_str == \"f16\" or (output_type_str is None and wq_type in (DT_F16, DT_BF16)):\n        return GGMLFileType.MostlyF16\n    if output_type_str == \"q8_0\":\n        return GGMLFileType.MostlyQ8_0\n\n    name_to_type = {name: lazy_tensor.data_type for (name, lazy_tensor) in model.items()}\n\n    raise Exception(f\"Unexpected combination of types: {name_to_type}\")\n\ndef convert_to_output_type(model: LazyModel, output_type: GGMLFileType) -> LazyModel:\n    return {name: tensor.astype(output_type.type_for_tensor(name, tensor))\n            for (name, tensor) in model.items()}\n\ndef convert_model_names(model: LazyModel, params: Params) -> LazyModel:\n    tmap = gguf.TensorNameMap(params.arch, params.n_layer)\n    should_skip: set[gguf.MODEL_TENSOR] = set(gguf.MODEL_TENSOR_SKIP.get(params.arch, []))\n\n    tmp = model\n\n    # HF models permut or pack some of the tensors, so we need to undo that\n    for i in itertools.count():\n        if f\"model.layers.{i}.self_attn.q_proj.weight\" in model:\n            print(f\"Permuting layer {i}\")\n            tmp[f\"model.layers.{i}.self_attn.q_proj.weight\"] = permute_lazy(model[f\"model.layers.{i}.self_attn.q_proj.weight\"], params.n_head, params.n_head)\n            tmp[f\"model.layers.{i}.self_attn.k_proj.weight\"] = permute_lazy(model[f\"model.layers.{i}.self_attn.k_proj.weight\"], params.n_head, params.n_head_kv)\n           #tmp[f\"model.layers.{i}.self_attn.v_proj.weight\"] =              model[f\"model.layers.{i}.self_attn.v_proj.weight\"]\n        elif f\"model.layers.{i}.self_attn.W_pack.weight\" in model:\n            print(f\"Unpacking and permuting layer {i}\")\n            tmp[f\"model.layers.{i}.self_attn.q_proj.weight\"] = permute_part_lazy(model[f\"model.layers.{i}.self_attn.W_pack.weight\"], 0, params.n_head, params.n_head)\n            tmp[f\"model.layers.{i}.self_attn.k_proj.weight\"] = permute_part_lazy(model[f\"model.layers.{i}.self_attn.W_pack.weight\"], 1, params.n_head, params.n_head_kv)\n            tmp[f\"model.layers.{i}.self_attn.v_proj.weight\"] = part_lazy        (model[f\"model.layers.{i}.self_attn.W_pack.weight\"], 2)\n            del tmp[f\"model.layers.{i}.self_attn.W_pack.weight\"]\n        else:\n            break\n\n    out: LazyModel = {}\n    for name, lazy_tensor in model.items():\n        tensor_type, name_new = tmap.get_type_and_name(name, try_suffixes = (\".weight\", \".bias\")) or (None, None)\n        if name_new is None:\n            raise Exception(f\"Unexpected tensor name: {name}\")\n\n        if tensor_type in should_skip:\n            print(f\"skipping tensor {name_new}\")\n            continue\n\n        print(f\"{name:48s} -> {name_new:40s} | {lazy_tensor.data_type.name:6s} | {lazy_tensor.shape}\")\n        out[name_new] = lazy_tensor\n\n    return out\n\ndef postprocess_transpose(model: LazyModel) -> LazyModel:\n    \"\"\"Transpose ffn_down matrices for Axpy ops.\"\"\"\n    out: LazyModel = {}\n    \n    for name, lazy_tensor in model.items():\n        if name.endswith(\".ffn_down.weight\"):\n            out[name.replace(\"ffn_down\", \"ffn_down_t\")] = lazy_tensor.transposed()\n        else:\n            out[name] = lazy_tensor\n    \n    return out\n\ndef nth_multifile_path(path: Path, n: int) -> Path | None:\n    '''Given any path belonging to a multi-file model (e.g. foo.bin.1), return\n    the nth path in the model.\n    '''\n    # Support the following patterns:\n    patterns: list[tuple[str, str]] = [\n        # - x.00.pth, x.01.pth, etc.\n        (r'\\.[0-9]{2}\\.pth$', f'.{n:02}.pth'),\n        # - x-00001-of-00002.bin, x-00002-of-00002.bin, etc.\n        (r'-[0-9]{5}-of-(.*)$', fr'-{n:05}-of-\\1'),\n        # x.bin, x.bin.1, etc.\n        (r'(\\.[0-9]+)?$', r'\\1' if n == 0 else fr'\\1.{n}'),\n        # x_0.pt, x_1.pt, etc.\n        (r'(_[0-9]+)?\\.pt$', fr'_{n}.pt'),\n    ]\n    for regex, replacement in patterns:\n        if re.search(regex, path.name):\n            new_path = path.with_name(re.sub(regex, replacement, path.name))\n            if new_path.exists():\n                return new_path\n    return None\n\n\ndef find_multifile_paths(path: Path) -> list[Path]:\n    '''Given any path belonging to a multi-file model (e.g. foo.bin.1), return\n    the whole list of paths in the model.\n    '''\n    ret: list[Path] = []\n    for i in itertools.count():\n        nth_path = nth_multifile_path(path, i)\n        if nth_path is None:\n            break\n        ret.append(nth_path)\n    if not ret:\n        # No matches.  This should only happen if the file was named, e.g.,\n        # foo.0, and there was no file named foo.  Oh well, try to process it\n        # as a single file.\n        return [path]\n    return ret\n\n\ndef load_some_model(path: Path) -> ModelPlus:\n    '''Load a model of any supported format.'''\n    # Be extra-friendly and accept either a file or a directory:\n    if path.is_dir():\n        # Check if it's a set of safetensors files first\n        globs = [\"model-00001-of-*.safetensors\", \"model.safetensors\"]\n        files = [file for glob in globs for file in path.glob(glob)]\n        if not files:\n            # Try the PyTorch patterns too, with lower priority\n            globs = [\"consolidated.00.pth\", \"pytorch_model-00001-of-*.bin\", \"*.pt\", \"pytorch_model.bin\"]\n            files = [file for glob in globs for file in path.glob(glob)]\n        if not files:\n            raise Exception(f\"Can't find model in directory {path}\")\n        if len(files) > 1:\n            raise Exception(f\"Found multiple models in {path}, not sure which to pick: {files}\")\n        path = files[0]\n\n    paths = find_multifile_paths(path)\n    models_plus: list[ModelPlus] = []\n    for path in paths:\n        print(f\"Loading model file {path}\")\n        models_plus.append(lazy_load_file(path))\n\n    model_plus = merge_multifile_models(models_plus)\n    return model_plus\n\ndef load_predictor_model(path: Path) -> ModelPlus:\n    '''Load MLP models for sparse FFN inference from directory.'''\n    assert path.is_dir(), f\"MLP model path {path} is not a directory\"\n    \n    first_model_path = path / \"model_0.pt\"\n    assert first_model_path.resolve(), f\"MLP model path {path} does not contain model_0.pt\"\n\n    model_paths = find_multifile_paths(first_model_path)\n    models_plus: list[ModelPlus] = []\n    for model_path in model_paths:\n        # find number in model_path\n        model_layer = int(re.search(r'model_(\\d+).pt', str(model_path)).group(1))\n        print(f\"Loading MLP model file {model_path}\")\n        mlp_model = lazy_load_file(model_path)\n        mlp_model.model = {f\"model.layers.{model_layer}.{name}\": tensor for name, tensor in mlp_model.model.items()}\n        models_plus.append(mlp_model)\n\n    return merge_multifile_models(models_plus)\n\n\ndef load_vocab(path: Path, vocabtype: str | None) -> Vocab:\n    # Be extra-friendly and accept either a file or a directory.  Also, if it's\n    # a directory, it might be the model directory, and tokenizer.model might\n    # be in the parent of that.\n    if path.is_dir():\n        vocab_file = \"tokenizer.model\"\n        if vocabtype == 'bpe':\n            vocab_file = \"vocab.json\"\n        path2 = path / vocab_file\n        # Use `.parent` instead of /.. to handle the symlink case better.\n        path3 = path.parent / vocab_file\n        if path2.exists():\n            path = path2\n        elif path3.exists():\n            path = path3\n        else:\n            raise FileNotFoundError(\n                f\"Could not find {vocab_file} in {path} or its parent; \"\n                \"if it's in another directory, pass the directory as --vocab-dir\")\n\n    print(f\"Loading vocab file '{path}', type '{vocabtype}'\")\n\n    added_tokens_path = path.parent / \"added_tokens.json\"\n    if vocabtype == \"bpe\":\n        return BpeVocab(path, added_tokens_path if added_tokens_path.exists() else None)\n    elif vocabtype == \"spm\":\n        return SentencePieceVocab(path, added_tokens_path if added_tokens_path.exists() else None)\n    else:\n        raise ValueError(f\"Unsupported vocabulary type {vocabtype}\")\n\n\ndef default_outfile(model_paths: list[Path], file_type: GGMLFileType) -> Path:\n    namestr = {\n        GGMLFileType.AllF32:    \"f32\",\n        GGMLFileType.MostlyF16: \"f16\",\n        GGMLFileType.MostlyQ8_0:\"q8_0\",\n    }[file_type]\n    ret = model_paths[0].parent / f\"ggml-model-{namestr}.gguf\"\n    if ret in model_paths:\n        sys.stderr.write(\n            f\"Error: Default output path ({ret}) would overwrite the input. \"\n            \"Please explicitly specify a path using --outfile.\\n\")\n        sys.exit(1)\n    return ret\n\n\ndef do_dump_model(model_plus: ModelPlus) -> None:\n    print(f\"model_plus.paths = {model_plus.paths!r}\")\n    print(f\"model_plus.format = {model_plus.format!r}\")\n    print(f\"model_plus.vocab = {model_plus.vocab!r}\")\n    for name, lazy_tensor in model_plus.model.items():\n        print(f\"{name}: shape={lazy_tensor.shape} type={lazy_tensor.data_type}; {lazy_tensor.description}\")\n\n\ndef main(args_in: list[str] | None = None) -> None:\n    output_choices = [\"f32\", \"f16\"]\n    if np.uint32(1) == np.uint32(1).newbyteorder(\"<\"):\n        # We currently only support Q8_0 output on little endian systems.\n        output_choices.append(\"q8_0\")\n    parser = argparse.ArgumentParser(description=\"Convert a LLaMa model to a GGML compatible file\")\n    parser.add_argument(\"--dump\",        action=\"store_true\",    help=\"don't convert, just show what's in the model\")\n    parser.add_argument(\"--dump-single\", action=\"store_true\",    help=\"don't convert, just show what's in a single model file\")\n    parser.add_argument(\"--vocab-only\",  action=\"store_true\",    help=\"extract only the vocab\")\n    parser.add_argument(\"--outtype\",     choices=output_choices, help=\"output format - note: q8_0 may be very slow (default: f16 or f32 based on input)\", default=\"f16\")\n    parser.add_argument(\"--vocab-dir\",   type=Path,              help=\"directory containing tokenizer.model, if separate from model file\")\n    parser.add_argument(\"--outfile\",     type=Path,              help=\"path to write to; default: based on input\")\n    parser.add_argument(\"--ctx\",         type=int,               help=\"model training context (default: based on input)\")\n    parser.add_argument(\"--concurrency\", type=int,               help=f\"concurrency used for conversion (default: {DEFAULT_CONCURRENCY})\", default = DEFAULT_CONCURRENCY)\n    parser.add_argument(\"--bigendian\",   action=\"store_true\",    help=\"model is executed on big endian machine\")\n    parser.add_argument(\"--vocabtype\",   choices=[\"spm\", \"bpe\"], help=\"vocab format (default: spm)\", default=\"spm\")\n    parser.add_argument(\"model\",         type=Path,              help=\"directory containing model file, or model file itself (*.pth, *.pt, *.bin, *.safetensors)\")\n    parser.add_argument(\"sparse_predictor\",     type=Path,              help=\"predictors for sparse FFN inference\")\n\n    args = parser.parse_args(args_in)\n\n    try:\n        with open(args.model / \"config.json\", \"r\", encoding=\"utf-8\") as f:\n            hf_config = json.load(f)\n        if model_type := hf_config.get(\"model_type\") not in (\"llama\", \"bamboo\"):\n            # invoke another script to convert other models\n            print(f\"Model architecture {model_type} is not supported by this `convert.py`. Trying with `convert-hf-to-powerinfer-gguf.py`...\")\n            script_path = Path(__file__).resolve().parent / \"convert-hf-to-powerinfer-gguf.py\"\n            subprocess.run([\"python3\", str(script_path.absolute())] + sys.argv[1:])\n            return\n    except FileNotFoundError:\n        print(\"Could not find config.json under the original model directory. \", file=sys.stderr)\n        sys.exit(1)\n\n    if args.dump_single:\n        model_plus = lazy_load_file(args.model)\n        do_dump_model(model_plus)\n        return\n\n    if not args.vocab_only:\n        model_plus = load_some_model(args.model)\n        params = Params.load(model_plus)\n        mlp_predictor_plus = load_predictor_model(args.sparse_predictor)\n        params.predictor_params = PredictorParams.load(mlp_predictor_plus)\n        model_plus = merge_multifile_models([model_plus, mlp_predictor_plus])\n    else:\n        model_plus = ModelPlus(model = {}, paths = [args.model / 'dummy'], format = 'none', vocab = None)\n        params = Params.load(model_plus)\n\n    if args.dump:\n        do_dump_model(model_plus)\n        return\n    endianess = gguf.GGUFEndian.LITTLE\n    if args.bigendian:\n        endianess = gguf.GGUFEndian.BIG\n\n    if params.n_ctx == -1:\n        if args.ctx is None:\n            raise Exception(\"The model doesn't have a context size, and you didn't specify one with --ctx\\n\"\n                            \"Please specify one with --ctx:\\n\"\n                            \" - LLaMA v1: --ctx 2048\\n\"\n                            \" - LLaMA v2: --ctx 4096\\n\")\n        params.n_ctx = args.ctx\n\n    if args.outtype:\n        params.ftype = {\n            \"f32\": GGMLFileType.AllF32,\n            \"f16\": GGMLFileType.MostlyF16,\n            \"q8_0\": GGMLFileType.MostlyQ8_0,\n        }[args.outtype]\n\n    print(f\"params = {params}\")\n\n    vocab: Vocab\n    if args.vocab_only:\n        if not args.outfile:\n            raise ValueError(\"need --outfile if using --vocab-only\")\n        # FIXME: Try to respect vocab_dir somehow?\n        vocab = load_vocab(args.vocab_dir or args.model, args.vocabtype)\n        special_vocab = gguf.SpecialVocab(model_plus.paths[0].parent,\n            load_merges = args.vocabtype == 'bpe',\n            n_vocab = vocab.vocab_size)\n        outfile = args.outfile\n        OutputFile.write_vocab_only(outfile, params, vocab, special_vocab)\n        print(f\"Wrote {outfile}\")\n        return\n\n    if model_plus.vocab is not None and args.vocab_dir is None:\n        vocab = model_plus.vocab\n    else:\n        vocab_dir = args.vocab_dir if args.vocab_dir else model_plus.paths[0].parent\n        vocab = load_vocab(vocab_dir, args.vocabtype)\n    # FIXME: Try to respect vocab_dir somehow?\n    special_vocab = gguf.SpecialVocab(model_plus.paths[0].parent,\n        load_merges = args.vocabtype == 'bpe',\n        n_vocab = vocab.vocab_size)\n\n    model   = model_plus.model\n    model   = convert_model_names(model, params)\n    model   = postprocess_transpose(model)\n    ftype   = pick_output_type(model, args.outtype)\n    model   = convert_to_output_type(model, ftype)\n    outfile = args.outfile or default_outfile(model_plus.paths, ftype)\n\n    params.ftype = ftype\n    print(f\"Writing {outfile}, format {ftype}\")\n\n    OutputFile.write_all(outfile, ftype, params, model, vocab, special_vocab, concurrency = args.concurrency, endianess=endianess)\n    print(f\"Wrote {outfile}\")\n\n    # post-process: write another unique file header to distinguish from the origianl GGUF file\n    with open(outfile, \"r+b\") as fout:\n        POWERINFER_MAGIC = int.from_bytes(b\"PWRI\", \"little\")\n        fout.write(struct.pack(\"<I\", POWERINFER_MAGIC))\n\n\nif __name__ == '__main__':\n    main()\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "flake.lock",
          "type": "blob",
          "size": 1.4619140625,
          "content": "{\n  \"nodes\": {\n    \"flake-utils\": {\n      \"inputs\": {\n        \"systems\": \"systems\"\n      },\n      \"locked\": {\n        \"lastModified\": 1694529238,\n        \"narHash\": \"sha256-zsNZZGTGnMOf9YpHKJqMSsa0dXbfmxeoJ7xHlrt+xmY=\",\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"rev\": \"ff7b65b44d01cf9ba6a71320833626af21126384\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"numtide\",\n        \"repo\": \"flake-utils\",\n        \"type\": \"github\"\n      }\n    },\n    \"nixpkgs\": {\n      \"locked\": {\n        \"lastModified\": 1698318101,\n        \"narHash\": \"sha256-gUihHt3yPD7bVqg+k/UVHgngyaJ3DMEBchbymBMvK1E=\",\n        \"owner\": \"NixOS\",\n        \"repo\": \"nixpkgs\",\n        \"rev\": \"63678e9f3d3afecfeafa0acead6239cdb447574c\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"NixOS\",\n        \"ref\": \"nixos-unstable\",\n        \"repo\": \"nixpkgs\",\n        \"type\": \"github\"\n      }\n    },\n    \"root\": {\n      \"inputs\": {\n        \"flake-utils\": \"flake-utils\",\n        \"nixpkgs\": \"nixpkgs\"\n      }\n    },\n    \"systems\": {\n      \"locked\": {\n        \"lastModified\": 1681028828,\n        \"narHash\": \"sha256-Vy1rq5AaRuLzOxct8nz4T6wlgyUR7zLU309k9mBC768=\",\n        \"owner\": \"nix-systems\",\n        \"repo\": \"default\",\n        \"rev\": \"da67096a3b9bf56a91d16901293e51ba5b49a27e\",\n        \"type\": \"github\"\n      },\n      \"original\": {\n        \"owner\": \"nix-systems\",\n        \"repo\": \"default\",\n        \"type\": \"github\"\n      }\n    }\n  },\n  \"root\": \"root\",\n  \"version\": 7\n}\n"
        },
        {
          "name": "flake.nix",
          "type": "blob",
          "size": 5.6259765625,
          "content": "{\n  inputs = {\n    nixpkgs.url = \"github:NixOS/nixpkgs/nixos-unstable\";\n    flake-utils.url = \"github:numtide/flake-utils\";\n  };\n  outputs = { self, nixpkgs, flake-utils }:\n    flake-utils.lib.eachDefaultSystem (system:\n      let\n        name = \"llama.cpp\";\n        src = ./.;\n        meta.mainProgram = \"llama\";\n        inherit (pkgs.stdenv) isAarch32 isAarch64 isDarwin;\n        buildInputs = with pkgs; [ openmpi ];\n        osSpecific = with pkgs; buildInputs ++ (\n          if isAarch64 && isDarwin then\n            with pkgs.darwin.apple_sdk_11_0.frameworks; [\n              Accelerate\n              MetalKit\n            ]\n          else if isAarch32 && isDarwin then\n            with pkgs.darwin.apple_sdk.frameworks; [\n              Accelerate\n              CoreGraphics\n              CoreVideo\n            ]\n          else if isDarwin then\n            with pkgs.darwin.apple_sdk.frameworks; [\n              Accelerate\n              CoreGraphics\n              CoreVideo\n            ]\n          else\n            with pkgs; [ openblas ]\n        );\n        pkgs = import nixpkgs { inherit system; };\n        nativeBuildInputs = with pkgs; [ cmake ninja pkg-config ];\n        cudatoolkit_joined = with pkgs; symlinkJoin {\n          # HACK(Green-Sky): nix currently has issues with cmake findcudatoolkit\n          # see https://github.com/NixOS/nixpkgs/issues/224291\n          # copied from jaxlib\n          name = \"${cudaPackages.cudatoolkit.name}-merged\";\n          paths = [\n            cudaPackages.cudatoolkit.lib\n            cudaPackages.cudatoolkit.out\n          ] ++ lib.optionals (lib.versionOlder cudaPackages.cudatoolkit.version \"11\") [\n            # for some reason some of the required libs are in the targets/x86_64-linux\n            # directory; not sure why but this works around it\n            \"${cudaPackages.cudatoolkit}/targets/${system}\"\n          ];\n        };\n        llama-python =\n          pkgs.python3.withPackages (ps: with ps; [ numpy sentencepiece ]);\n        # TODO(Green-Sky): find a better way to opt-into the heavy ml python runtime\n        llama-python-extra =\n          pkgs.python3.withPackages (ps: with ps; [ numpy sentencepiece torchWithoutCuda transformers ]);\n        postPatch = ''\n          substituteInPlace ./ggml-metal.m \\\n            --replace '[bundle pathForResource:@\"ggml-metal\" ofType:@\"metal\"];' \"@\\\"$out/bin/ggml-metal.metal\\\";\"\n          substituteInPlace ./*.py --replace '/usr/bin/env python' '${llama-python}/bin/python'\n        '';\n        postInstall = ''\n          mv $out/bin/main $out/bin/llama\n          mv $out/bin/server $out/bin/llama-server\n          mkdir -p $out/include\n          cp ${src}/llama.h $out/include/\n        '';\n        cmakeFlags = [ \"-DLLAMA_NATIVE=OFF\" \"-DLLAMA_BUILD_SERVER=ON\" \"-DBUILD_SHARED_LIBS=ON\" \"-DCMAKE_SKIP_BUILD_RPATH=ON\" ];\n      in\n      {\n        packages.default = pkgs.stdenv.mkDerivation {\n          inherit name src meta postPatch nativeBuildInputs postInstall;\n          buildInputs = osSpecific;\n          cmakeFlags = cmakeFlags\n            ++ (if isAarch64 && isDarwin then [\n            \"-DCMAKE_C_FLAGS=-D__ARM_FEATURE_DOTPROD=1\"\n            \"-DLLAMA_METAL=ON\"\n          ] else [\n            \"-DLLAMA_BLAS=ON\"\n            \"-DLLAMA_BLAS_VENDOR=OpenBLAS\"\n          ]);\n        };\n        packages.opencl = pkgs.stdenv.mkDerivation {\n          inherit name src meta postPatch nativeBuildInputs postInstall;\n          buildInputs = with pkgs; buildInputs ++ [ clblast ];\n          cmakeFlags = cmakeFlags ++ [\n            \"-DLLAMA_CLBLAST=ON\"\n          ];\n        };\n        packages.cuda = pkgs.stdenv.mkDerivation {\n          inherit name src meta postPatch nativeBuildInputs postInstall;\n          buildInputs = with pkgs; buildInputs ++ [ cudatoolkit_joined ];\n          cmakeFlags = cmakeFlags ++ [\n            \"-DLLAMA_CUBLAS=ON\"\n          ];\n        };\n        packages.rocm = pkgs.stdenv.mkDerivation {\n          inherit name src meta postPatch nativeBuildInputs postInstall;\n          buildInputs = with pkgs.rocmPackages; buildInputs ++ [ clr hipblas rocblas ];\n          cmakeFlags = cmakeFlags ++ [\n            \"-DLLAMA_HIPBLAS=1\"\n            \"-DCMAKE_C_COMPILER=hipcc\"\n            \"-DCMAKE_CXX_COMPILER=hipcc\"\n            # Build all targets supported by rocBLAS. When updating search for TARGET_LIST_ROCM\n            # in github.com/ROCmSoftwarePlatform/rocBLAS/blob/develop/CMakeLists.txt\n            # and select the line that matches the current nixpkgs version of rocBLAS.\n            \"-DAMDGPU_TARGETS=gfx803;gfx900;gfx906:xnack-;gfx908:xnack-;gfx90a:xnack+;gfx90a:xnack-;gfx940;gfx941;gfx942;gfx1010;gfx1012;gfx1030;gfx1100;gfx1101;gfx1102\"\n          ];\n        };\n        apps.llama-server = {\n          type = \"app\";\n          program = \"${self.packages.${system}.default}/bin/llama-server\";\n        };\n        apps.llama-embedding = {\n          type = \"app\";\n          program = \"${self.packages.${system}.default}/bin/embedding\";\n        };\n        apps.llama = {\n          type = \"app\";\n          program = \"${self.packages.${system}.default}/bin/llama\";\n        };\n        apps.quantize = {\n          type = \"app\";\n          program = \"${self.packages.${system}.default}/bin/quantize\";\n        };\n        apps.train-text-from-scratch = {\n          type = \"app\";\n          program = \"${self.packages.${system}.default}/bin/train-text-from-scratch\";\n        };\n        apps.default = self.apps.${system}.llama;\n        devShells.default = pkgs.mkShell {\n          buildInputs = [ llama-python ];\n          packages = nativeBuildInputs ++ osSpecific;\n        };\n        devShells.extra = pkgs.mkShell {\n          buildInputs = [ llama-python-extra ];\n          packages = nativeBuildInputs ++ osSpecific;\n        };\n      });\n}\n"
        },
        {
          "name": "ggml-alloc.c",
          "type": "blob",
          "size": 26.5615234375,
          "content": "#include \"ggml-alloc.h\"\n#include \"ggml-backend-impl.h\"\n#include \"ggml.h\"\n#include \"ggml-impl.h\"\n#include <assert.h>\n#include <limits.h>\n#include <stdarg.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#define MAX(a, b) ((a) > (b) ? (a) : (b))\n#define MAX_FREE_BLOCKS 256\n\n//#define GGML_ALLOCATOR_DEBUG\n\n//#define AT_PRINTF(...) fprintf(stderr, __VA_ARGS__)\n#define AT_PRINTF(...)\n\n// TODO: GGML_PAD ?\nstatic size_t aligned_offset(const void * buffer, size_t offset, size_t alignment) {\n    assert(alignment && !(alignment & (alignment - 1))); // power of 2\n    size_t align = (alignment - (((uintptr_t)buffer + offset) % alignment)) % alignment;\n    return offset + align;\n}\n\nstruct free_block {\n    void * addr;\n    size_t size;\n};\n\nstruct ggml_tallocr {\n    struct ggml_backend_buffer * buffer;\n    bool buffer_owned;\n    void * base;\n    size_t alignment;\n\n    int n_free_blocks;\n    struct free_block free_blocks[MAX_FREE_BLOCKS];\n\n    size_t max_size;\n\n    bool measure;\n\n#ifdef GGML_ALLOCATOR_DEBUG\n    struct ggml_tensor * allocated_tensors[1024];\n#endif\n};\n\n#ifdef GGML_ALLOCATOR_DEBUG\nstatic void add_allocated_tensor(ggml_tallocr_t alloc, struct ggml_tensor * tensor) {\n    for (int i = 0; i < 1024; i++) {\n        if (alloc->allocated_tensors[i] == NULL) {\n            alloc->allocated_tensors[i] = tensor;\n            return;\n        }\n    }\n    GGML_ASSERT(!\"out of allocated_tensors\");\n}\nstatic void remove_allocated_tensor(ggml_tallocr_t alloc, struct ggml_tensor * tensor) {\n    for (int i = 0; i < 1024; i++) {\n        if (alloc->allocated_tensors[i] == tensor ||\n            (alloc->allocated_tensors[i] != NULL && alloc->allocated_tensors[i]->data == tensor->data)) {\n            alloc->allocated_tensors[i] = NULL;\n            return;\n        }\n    }\n    printf(\"tried to free tensor %s not found\\n\", tensor->name);\n    GGML_ASSERT(!\"tensor not found\");\n}\n#endif\n\n// check if a tensor is allocated by this buffer\nstatic bool ggml_tallocr_is_own(ggml_tallocr_t alloc, const struct ggml_tensor * tensor) {\n    return tensor->buffer == alloc->buffer;\n}\n\nstatic bool ggml_is_view(struct ggml_tensor * t) {\n    return t->view_src != NULL;\n}\n\nvoid ggml_tallocr_alloc(ggml_tallocr_t alloc, struct ggml_tensor * tensor) {\n    GGML_ASSERT(!ggml_is_view(tensor)); // views generally get data pointer from one of their sources\n    GGML_ASSERT(tensor->data == NULL); // avoid allocating tensor which already has memory allocated\n\n    size_t size = ggml_backend_buffer_get_alloc_size(alloc->buffer, tensor);\n    size = aligned_offset(NULL, size, alloc->alignment);\n\n    AT_PRINTF(\"%s: allocating %s (%zu bytes) - \", __func__, tensor->name, size);\n\n    size_t max_avail = 0;\n\n    // find the best fitting free block besides the last block\n    int best_fit_block = -1;\n    size_t best_fit_size = SIZE_MAX;\n    for (int i = 0; i < alloc->n_free_blocks - 1; i++) {\n        struct free_block * block = &alloc->free_blocks[i];\n        max_avail = MAX(max_avail, block->size);\n        if (block->size >= size && block->size <= best_fit_size) {\n            best_fit_block = i;\n            best_fit_size = block->size;\n        }\n    }\n\n    AT_PRINTF(\"block %d\\n\", best_fit_block);\n\n    if (best_fit_block == -1) {\n        // the last block is our last resort\n        struct free_block * block = &alloc->free_blocks[alloc->n_free_blocks - 1];\n        max_avail = MAX(max_avail, block->size);\n        if (block->size >= size) {\n            best_fit_block = alloc->n_free_blocks - 1;\n        } else {\n            fprintf(stderr, \"%s: not enough space in the buffer (needed %zu, largest block available %zu)\\n\",\n                    __func__, size, max_avail);\n            GGML_ASSERT(!\"not enough space in the buffer\");\n            return;\n        }\n    }\n    struct free_block * block = &alloc->free_blocks[best_fit_block];\n    void * addr = block->addr;\n    block->addr = (char*)block->addr + size;\n    block->size -= size;\n    if (block->size == 0) {\n        // remove block if empty\n        alloc->n_free_blocks--;\n        for (int j = best_fit_block; j < alloc->n_free_blocks; j++) {\n            alloc->free_blocks[j] = alloc->free_blocks[j+1];\n        }\n    }\n\n    tensor->data = addr;\n    tensor->buffer = alloc->buffer;\n    if (!alloc->measure) {\n        ggml_backend_buffer_init_tensor(alloc->buffer, tensor);\n    }\n\n#ifdef GGML_ALLOCATOR_DEBUG\n    add_allocated_tensor(alloc, tensor);\n    size_t cur_max = (char*)addr - (char*)alloc->data + size;\n    if (cur_max > alloc->max_size) {\n        printf(\"max_size = %.2f MB: tensors: \", cur_max / 1024.0 / 1024.0);\n        for (int i = 0; i < 1024; i++) {\n            if (alloc->allocated_tensors[i]) {\n                printf(\"%s (%.2f MB) \", alloc->allocated_tensors[i]->name, ggml_nbytes(alloc->allocated_tensors[i]) / 1024.0 / 1024.0);\n            }\n        }\n        printf(\"\\n\");\n    }\n#endif\n\n    alloc->max_size = MAX(alloc->max_size, (char*)addr - (char*)alloc->base + size);\n}\n\n// this is a very naive implementation, but for our case the number of free blocks should be very small\nstatic void ggml_tallocr_free_tensor(ggml_tallocr_t alloc, struct ggml_tensor * tensor) {\n    if (ggml_tallocr_is_own(alloc, tensor) == false) {\n        // the tensor was not allocated in this buffer\n        // this can happen because the graph allocator will try to free weights and other tensors from different buffers\n        // the easiest way to deal with this is just to ignore it\n        // AT_PRINTF(\"ignoring %s (their buffer: %p, our buffer: %p)\\n\", tensor->name, (void *)tensor->buffer, (void *)alloc->buffer);\n        return;\n    }\n\n    void * ptr = tensor->data;\n\n    size_t size = ggml_backend_buffer_get_alloc_size(alloc->buffer, tensor);\n    size = aligned_offset(NULL, size, alloc->alignment);\n    AT_PRINTF(\"%s: freeing %s at %p (%zu bytes) - n_free_blocks = %d\\n\", __func__, tensor->name, ptr, size, alloc->n_free_blocks);\n\n    if (!alloc->measure) {\n        ggml_backend_buffer_free_tensor(alloc->buffer, tensor);\n    }\n\n#ifdef GGML_ALLOCATOR_DEBUG\n    remove_allocated_tensor(alloc, tensor);\n#endif\n\n    // see if we can merge with an existing block\n    for (int i = 0; i < alloc->n_free_blocks; i++) {\n        struct free_block * block = &alloc->free_blocks[i];\n        // check if ptr is at the end of the block\n        if ((char*)block->addr + block->size == ptr) {\n            block->size += size;\n            // check if we can merge with the next block\n            if (i < alloc->n_free_blocks - 1 && (char*)block->addr + block->size == alloc->free_blocks[i+1].addr) {\n                block->size += alloc->free_blocks[i+1].size;\n                alloc->n_free_blocks--;\n                for (int j = i+1; j < alloc->n_free_blocks; j++) {\n                    alloc->free_blocks[j] = alloc->free_blocks[j+1];\n                }\n            }\n            return;\n        }\n        // check if ptr is at the beginning of the block\n        if ((char*)ptr + size == block->addr) {\n            block->addr = ptr;\n            block->size += size;\n            // check if we can merge with the previous block\n            if (i > 0 && (char*)alloc->free_blocks[i-1].addr + alloc->free_blocks[i-1].size == block->addr) {\n                alloc->free_blocks[i-1].size += block->size;\n                alloc->n_free_blocks--;\n                for (int j = i; j < alloc->n_free_blocks; j++) {\n                    alloc->free_blocks[j] = alloc->free_blocks[j+1];\n                }\n            }\n            return;\n        }\n    }\n    // otherwise, add a new block\n    GGML_ASSERT(alloc->n_free_blocks < MAX_FREE_BLOCKS && \"out of free blocks\");\n    // insert the new block in the correct position to keep the array sorted by address (to make merging blocks faster)\n    int insert_pos = 0;\n    while (insert_pos < alloc->n_free_blocks && alloc->free_blocks[insert_pos].addr < ptr) {\n        insert_pos++;\n    }\n    // shift all blocks from insert_pos onward to make room for the new block\n    for (int i = alloc->n_free_blocks; i > insert_pos; i--) {\n        alloc->free_blocks[i] = alloc->free_blocks[i-1];\n    }\n    // insert the new block\n    alloc->free_blocks[insert_pos].addr = ptr;\n    alloc->free_blocks[insert_pos].size = size;\n    alloc->n_free_blocks++;\n}\n\nvoid ggml_tallocr_reset(ggml_tallocr_t alloc) {\n    alloc->n_free_blocks = 1;\n    size_t align_offset = aligned_offset(alloc->base, 0, alloc->alignment);\n    alloc->free_blocks[0].addr = (char *)alloc->base + align_offset;\n\n    if (alloc->measure) {\n        alloc->free_blocks[0].size = SIZE_MAX/2; // restrict maximum size of a measure allocator to half size_t max to avoid overflows\n    } else {\n        alloc->free_blocks[0].size = ggml_backend_buffer_get_size(alloc->buffer) - align_offset;\n    }\n}\n\nggml_tallocr_t ggml_tallocr_new(void * data, size_t size, size_t alignment) {\n    struct ggml_backend_buffer * buffer = ggml_backend_cpu_buffer_from_ptr(NULL, data, size);\n\n    ggml_tallocr_t alloc = (ggml_tallocr_t)malloc(sizeof(struct ggml_tallocr));\n\n    *alloc = (struct ggml_tallocr) {\n        /*.buffer        = */ buffer,\n        /*.buffer_owned  = */ true,\n        /*.base          = */ ggml_backend_buffer_get_base(buffer),\n        /*.alignment     = */ alignment,\n        /*.n_free_blocks = */ 0,\n        /*.free_blocks   = */ {{0}},\n        /*.max_size      = */ 0,\n        /*.measure       = */ false,\n#ifdef GGML_ALLOCATOR_DEBUG\n        /*.allocated_tensors = */ {0},\n#endif\n    };\n\n    ggml_tallocr_reset(alloc);\n\n    return alloc;\n}\n\nggml_tallocr_t ggml_tallocr_new_measure(size_t alignment) {\n    ggml_tallocr_t alloc = ggml_tallocr_new((void *)0x1000, SIZE_MAX/2, alignment);\n    alloc->measure = true;\n\n    return alloc;\n}\n\nggml_tallocr_t ggml_tallocr_new_measure_from_backend(struct ggml_backend * backend) {\n    // create a backend buffer to get the correct tensor allocation sizes\n    ggml_backend_buffer_t buffer = ggml_backend_alloc_buffer(backend, 1);\n\n    // TODO: move alloc initialization to a common ggml_tallocr_new_impl function\n    ggml_tallocr_t alloc = ggml_tallocr_new_from_buffer(buffer);\n    alloc->buffer_owned = true;\n    alloc->measure = true;\n    ggml_tallocr_reset(alloc);\n    return alloc;\n}\n\nggml_tallocr_t ggml_tallocr_new_from_backend(struct ggml_backend * backend, size_t size) {\n    ggml_backend_buffer_t buffer = ggml_backend_alloc_buffer(backend, size);\n    ggml_tallocr_t alloc = ggml_tallocr_new_from_buffer(buffer);\n    alloc->buffer_owned = true;\n    return alloc;\n}\n\nggml_tallocr_t ggml_tallocr_new_from_buffer(struct ggml_backend_buffer * buffer) {\n    ggml_tallocr_t alloc = (ggml_tallocr_t)malloc(sizeof(struct ggml_tallocr));\n\n    *alloc = (struct ggml_tallocr) {\n        /*.buffer        = */ buffer,\n        /*.buffer_owned  = */ false,\n        /*.base          = */ ggml_backend_buffer_get_base(buffer),\n        /*.alignment     = */ ggml_backend_buffer_get_alignment(buffer),\n        /*.n_free_blocks = */ 0,\n        /*.free_blocks   = */ {{0}},\n        /*.max_size      = */ 0,\n        /*.measure       = */ false,\n#ifdef GGML_ALLOCATOR_DEBUG\n        /*.allocated_tensors = */ {0},\n#endif\n    };\n\n    ggml_tallocr_reset(alloc);\n\n    return alloc;\n}\n\nstruct ggml_backend_buffer * ggml_tallocr_get_buffer(ggml_tallocr_t alloc) {\n    return alloc->buffer;\n}\n\nvoid ggml_tallocr_free(ggml_tallocr_t alloc) {\n    if (alloc == NULL) {\n        return;\n    }\n\n    if (alloc->buffer_owned) {\n        ggml_backend_buffer_free(alloc->buffer);\n    }\n    free(alloc);\n}\n\nbool ggml_tallocr_is_measure(ggml_tallocr_t alloc) {\n    return alloc->measure;\n}\n\nsize_t ggml_tallocr_max_size(ggml_tallocr_t alloc) {\n    return alloc->max_size;\n}\n\n// graph allocator\n\nstruct hash_node {\n    int n_children;\n    int n_views;\n};\n\nstruct ggml_gallocr {\n    ggml_tallocr_t talloc;\n    struct ggml_hash_set hash_set;\n    struct hash_node * hash_values;\n    size_t hash_values_size;\n    ggml_tallocr_t * hash_allocs;\n    int * parse_seq;\n    int parse_seq_len;\n};\n\nggml_gallocr_t ggml_gallocr_new(void) {\n    ggml_gallocr_t galloc = (ggml_gallocr_t)malloc(sizeof(struct ggml_gallocr));\n\n    *galloc = (struct ggml_gallocr) {\n        /*.talloc           = */ NULL,\n        /*.hash_set         = */ {0},\n        /*.hash_values      = */ NULL,\n        /*.hash_values_size = */ 0,\n        /*.hash_allocs      = */ NULL,\n        /*.parse_seq        = */ NULL,\n        /*.parse_seq_len    = */ 0,\n    };\n\n    return galloc;\n}\n\nvoid ggml_gallocr_free(ggml_gallocr_t galloc) {\n    if (galloc == NULL) {\n        return;\n    }\n\n    if (galloc->hash_set.keys != NULL) {\n        free(galloc->hash_set.keys);\n    }\n    if (galloc->hash_values != NULL) {\n        free(galloc->hash_values);\n    }\n    if (galloc->hash_allocs != NULL) {\n        free(galloc->hash_allocs);\n    }\n    if (galloc->parse_seq != NULL) {\n        free(galloc->parse_seq);\n    }\n    free(galloc);\n}\n\nvoid ggml_gallocr_set_parse_seq(ggml_gallocr_t galloc, const int * list, int n) {\n    free(galloc->parse_seq);\n    galloc->parse_seq = malloc(sizeof(int) * n);\n\n    for (int i = 0; i < n; i++) {\n        galloc->parse_seq[i] = list[i];\n    }\n    galloc->parse_seq_len = n;\n}\n\nstatic struct hash_node * hash_get(ggml_gallocr_t galloc, struct ggml_tensor * t) {\n    size_t i = ggml_hash_find_or_insert(galloc->hash_set, t);\n    return &galloc->hash_values[i];\n}\n\nstatic bool ggml_are_same_layout(const struct ggml_tensor * a, const struct ggml_tensor * b) {\n    if (a->type != b->type) {\n        return false;\n    }\n    for (int i = 0; i < GGML_MAX_DIMS; i++) {\n        if (a->ne[i] != b->ne[i]) {\n            return false;\n        }\n        if (a->nb[i] != b->nb[i]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nstatic bool ggml_op_can_inplace(enum ggml_op op) {\n    switch (op) {\n        case GGML_OP_SCALE:\n        case GGML_OP_DIAG_MASK_ZERO:\n        case GGML_OP_DIAG_MASK_INF:\n        case GGML_OP_ADD:\n        case GGML_OP_ADD1:\n        case GGML_OP_SUB:\n        case GGML_OP_MUL:\n        case GGML_OP_DIV:\n        case GGML_OP_SQR:\n        case GGML_OP_SQRT:\n        case GGML_OP_LOG:\n        case GGML_OP_UNARY:\n        case GGML_OP_ROPE:\n        case GGML_OP_RMS_NORM:\n        case GGML_OP_SOFT_MAX:\n            return true;\n\n        default:\n            return false;\n    }\n}\n\nstatic ggml_tallocr_t node_tallocr(ggml_gallocr_t galloc, struct ggml_tensor * node) {\n    if (galloc->talloc != NULL) {\n        return galloc->talloc;\n    }\n\n    return galloc->hash_allocs[ggml_hash_find_or_insert(galloc->hash_set, node)];\n}\n\nstatic void init_view(ggml_gallocr_t galloc, struct ggml_tensor * view, bool update_backend) {\n    ggml_tallocr_t alloc = node_tallocr(galloc, view);\n\n    //printf(\"init_view: %s from src %s\\n\", view->name, view->view_src->name);\n    GGML_ASSERT(view->view_src != NULL && view->view_src->data != NULL);\n    if (update_backend) {\n        view->backend = view->view_src->backend;\n    }\n    view->buffer  = view->view_src->buffer;\n    view->data    = (char *)view->view_src->data + view->view_offs;\n\n    // FIXME: the view should be initialized by the owning buffer, but currently this breaks the CUDA backend\n    // due to the ggml_tensor_extra_gpu ring buffer overwriting the KV cache extras\n    assert(ggml_tallocr_is_measure(alloc) || !view->buffer || view->buffer->backend == alloc->buffer->backend);\n\n    if (!alloc->measure) {\n        ggml_backend_buffer_init_tensor(alloc->buffer, view);\n    }\n}\n\nstatic void allocate_node(ggml_gallocr_t galloc, struct ggml_tensor * node) {\n    ggml_tallocr_t alloc = node_tallocr(galloc, node);\n\n    if (node->data == NULL) {\n        if (ggml_is_view(node)) {\n            init_view(galloc, node, true);\n        } else {\n            // see if we can reuse a parent's buffer (inplace)\n            if (ggml_op_can_inplace(node->op)) {\n                for (int i = 0; i < GGML_MAX_SRC; i++) {\n                    struct ggml_tensor * parent = node->src[i];\n                    if (parent == NULL) {\n                        break;\n                    }\n\n                    // if the node's data is external, then we cannot re-use it\n                    if (ggml_tallocr_is_own(alloc, parent) == false) {\n                        AT_PRINTF(\"not reusing parent %s for %s as %p is external\\n\", parent->name, node->name, parent->data);\n                        continue;\n                    }\n\n                    struct hash_node * p_hn = hash_get(galloc, parent);\n                    if (parent->data != NULL && p_hn->n_children == 1 && p_hn->n_views == 0 && ggml_are_same_layout(node, parent)) {\n                        if (ggml_is_view(parent)) {\n                            struct ggml_tensor * view_src = parent->view_src;\n                            struct hash_node * view_src_hn = hash_get(galloc, view_src);\n                            if (view_src_hn->n_views == 1 && view_src_hn->n_children == 0 && view_src->data == parent->data) {\n                                // TODO: the offset of the view parent must be kept to ensure that the op doesn't overwrite\n                                // the parent's data that it will need later (same layout requirement). the problem is that then\n                                // we cannot free the tensor because the original address of the allocation is lost.\n                                // adding a view_src pointer to the tensor would solve this and simplify the code dealing with views\n                                // for now, we only reuse the parent's data if the offset is zero (view_src->data == parent->data)\n                                AT_PRINTF(\"reusing view parent %s (%s) for %s\\n\", parent->name, view_src->name, node->name);\n                                node->view_src = view_src;\n                                view_src_hn->n_views += 1;\n                                init_view(galloc, node, false);\n                                return;\n                            }\n                        } else {\n                            AT_PRINTF(\"reusing parent %s for %s\\n\", parent->name, node->name);\n                            node->view_src = parent;\n                            p_hn->n_views += 1;\n                            init_view(galloc, node, false);\n                            return;\n                        }\n                    }\n                }\n            }\n            ggml_tallocr_alloc(alloc, node);\n        }\n    }\n}\n\nstatic void free_node(ggml_gallocr_t galloc, struct ggml_tensor * node) {\n    ggml_tallocr_t alloc = node_tallocr(galloc, node);\n\n    ggml_tallocr_free_tensor(alloc, node);\n}\n\nstatic void ggml_tallocr_alloc_graph_impl(ggml_gallocr_t galloc, struct ggml_cgraph * gf) {\n    const int * parse_seq     = galloc->parse_seq;\n    int         parse_seq_len = galloc->parse_seq_len;\n\n    // count number of children and views\n    for (int i = 0; i < gf->n_nodes; i++) {\n        struct ggml_tensor * node = gf->nodes[i];\n\n        if (ggml_is_view(node)) {\n            struct ggml_tensor * view_src = node->view_src;\n            hash_get(galloc, view_src)->n_views += 1;\n            if (node->buffer == NULL && node->data != NULL) {\n                // view of a pre-allocated tensor, didn't call init_view() yet\n                init_view(galloc, node, true);\n            }\n        }\n\n        for (int j = 0; j < GGML_MAX_SRC; j++) {\n            struct ggml_tensor * parent = node->src[j];\n            if (parent == NULL) {\n                break;\n            }\n            hash_get(galloc, parent)->n_children += 1;\n            if (ggml_is_view(parent) && parent->buffer == NULL && parent->data != NULL) {\n                init_view(galloc, parent, true);\n            }\n        }\n   }\n\n    // allocate tensors\n    // if we have parse_seq then we allocate nodes following the list, and we only free nodes at barriers\n    int last_barrier_pos = 0;\n    int n_nodes = parse_seq_len ? parse_seq_len : gf->n_nodes;\n\n    for (int ind = 0; ind < n_nodes; ind++) {\n        // allocate a node if there is no parse_seq or this is not a barrier\n        if (parse_seq_len == 0 || parse_seq[ind] != -1) {\n            int i = parse_seq_len ? parse_seq[ind] : ind;\n            struct ggml_tensor * node = gf->nodes[i];\n\n            // allocate parents (leafs)\n            for (int j = 0; j < GGML_MAX_SRC; j++) {\n                struct ggml_tensor * parent = node->src[j];\n                if (parent == NULL) {\n                    break;\n                }\n                allocate_node(galloc, parent);\n            }\n\n            // allocate node\n            allocate_node(galloc, node);\n\n            AT_PRINTF(\"exec: %s (%s) <= \", ggml_op_name(node->op), node->name);\n            for (int j = 0; j < GGML_MAX_SRC; j++) {\n                struct ggml_tensor * parent = node->src[j];\n                if (parent == NULL) {\n                    break;\n                }\n                AT_PRINTF(\"%s\", parent->name);\n                if (j < GGML_MAX_SRC - 1 && node->src[j + 1] != NULL) {\n                    AT_PRINTF(\", \");\n                }\n            }\n            AT_PRINTF(\"\\n\");\n        }\n\n        // update parents\n        // update immediately if there is no parse_seq\n        // update only at barriers if there is parse_seq\n        if ((parse_seq_len == 0) || parse_seq[ind] == -1) {\n            int update_start = parse_seq_len ? last_barrier_pos : ind;\n            int update_end   = parse_seq_len ? ind              : ind + 1;\n            for (int i = update_start; i < update_end; i++) {\n                int node_i = parse_seq_len ? parse_seq[i] : i;\n                struct ggml_tensor * node = gf->nodes[node_i];\n\n                for (int j = 0; j < GGML_MAX_SRC; j++) {\n                    struct ggml_tensor * parent = node->src[j];\n                    if (parent == NULL) {\n                        break;\n                    }\n                    struct hash_node * p_hn = hash_get(galloc, parent);\n                    p_hn->n_children -= 1;\n\n                    //AT_PRINTF(\"parent %s: %d children, %d views\\n\", parent->name, parent->n_children, parent->n_views);\n\n                    if (p_hn->n_children == 0 && p_hn->n_views == 0) {\n                        if (ggml_is_view(parent)) {\n                            struct ggml_tensor * view_src = parent->view_src;\n                            struct hash_node * view_src_hn = hash_get(galloc, view_src);\n                            view_src_hn->n_views -= 1;\n                            AT_PRINTF(\"view_src %s: %d children, %d views\\n\", view_src->name, view_src_hn->n_children, view_src_hn->n_views);\n                            if (view_src_hn->n_views == 0 && view_src_hn->n_children == 0) {\n                                free_node(galloc, view_src);\n                            }\n                        }\n                        else {\n                            free_node(galloc, parent);\n                        }\n                    }\n                }\n            }\n            AT_PRINTF(\"\\n\");\n            if (parse_seq_len) {\n                last_barrier_pos = ind + 1;\n            }\n        }\n    }\n}\n\nsize_t ggml_gallocr_alloc_graph(ggml_gallocr_t galloc, ggml_tallocr_t talloc, struct ggml_cgraph * graph) {\n    size_t hash_size = graph->visited_hash_table.size;\n\n    // check if the hash table is initialized and large enough\n    if (galloc->hash_set.size < hash_size) {\n        if (galloc->hash_set.keys != NULL) {\n            free(galloc->hash_set.keys);\n        }\n        if (galloc->hash_values != NULL) {\n            free(galloc->hash_values);\n        }\n        galloc->hash_set.keys = malloc(sizeof(struct ggml_tensor *) * hash_size);\n        galloc->hash_set.size = hash_size;\n        galloc->hash_values = malloc(sizeof(struct hash_node) * hash_size);\n    }\n\n    // reset hash table\n    memset(galloc->hash_set.keys, 0, sizeof(struct ggml_tensor *) * hash_size);\n    memset(galloc->hash_values,   0, sizeof(struct hash_node) * hash_size);\n\n    galloc->talloc = talloc;\n    ggml_tallocr_alloc_graph_impl(galloc, graph);\n    galloc->talloc = NULL;\n\n    size_t max_size = ggml_tallocr_max_size(talloc);\n\n    return max_size;\n}\n\nvoid ggml_gallocr_alloc_graph_n(ggml_gallocr_t galloc, struct ggml_cgraph * graph, struct ggml_hash_set hash_set, ggml_tallocr_t * hash_node_talloc) {\n    const size_t hash_size = hash_set.size;\n\n    GGML_ASSERT(hash_size >= (size_t)(graph->n_nodes + graph->n_leafs));\n\n    galloc->talloc = NULL;\n\n    // alloc hash_values if needed\n    if (galloc->hash_values == NULL || galloc->hash_values_size < hash_size) {\n        free(galloc->hash_values);\n        galloc->hash_values      = malloc(sizeof(struct hash_node) * hash_size);\n        galloc->hash_values_size = hash_size;\n    }\n\n    // free hash_set.keys if needed\n    if (galloc->hash_set.keys != NULL) {\n        free(galloc->hash_set.keys);\n    }\n    galloc->hash_set = hash_set;\n\n    // reset hash values\n    memset(galloc->hash_values, 0, sizeof(struct hash_node) * hash_size);\n\n    galloc->hash_allocs = hash_node_talloc;\n\n    ggml_tallocr_alloc_graph_impl(galloc, graph);\n\n    // remove unowned resources\n    galloc->hash_set.keys = NULL;\n    galloc->hash_allocs = NULL;\n}\n\n// legacy API wrapper\n\nstruct ggml_allocr {\n    ggml_tallocr_t talloc;\n    ggml_gallocr_t galloc;\n};\n\nstatic ggml_allocr_t ggml_allocr_new_impl(ggml_tallocr_t talloc) {\n    ggml_allocr_t alloc = (ggml_allocr_t)malloc(sizeof(struct ggml_allocr));\n    *alloc = (struct ggml_allocr) {\n        /*.talloc = */ talloc,\n        /*.galloc = */ ggml_gallocr_new(),\n    };\n    return alloc;\n}\n\nggml_allocr_t ggml_allocr_new(void * data, size_t size, size_t alignment) {\n    return ggml_allocr_new_impl(ggml_tallocr_new(data, size, alignment));\n}\n\nggml_allocr_t ggml_allocr_new_measure(size_t alignment) {\n    return ggml_allocr_new_impl(ggml_tallocr_new_measure(alignment));\n}\n\nggml_allocr_t ggml_allocr_new_from_buffer(struct ggml_backend_buffer * buffer) {\n    return ggml_allocr_new_impl(ggml_tallocr_new_from_buffer(buffer));\n}\n\nggml_allocr_t ggml_allocr_new_from_backend(struct ggml_backend * backend, size_t size) {\n    return ggml_allocr_new_impl(ggml_tallocr_new_from_backend(backend, size));\n}\n\nggml_allocr_t ggml_allocr_new_measure_from_backend(struct ggml_backend * backend) {\n    return ggml_allocr_new_impl(ggml_tallocr_new_measure_from_backend(backend));\n}\n\nstruct ggml_backend_buffer * ggml_allocr_get_buffer(ggml_allocr_t alloc) {\n    return ggml_tallocr_get_buffer(alloc->talloc);\n}\n\nvoid ggml_allocr_set_parse_seq(ggml_allocr_t alloc, const int * list, int n) {\n    ggml_gallocr_set_parse_seq(alloc->galloc, list, n);\n}\n\nvoid ggml_allocr_free(ggml_allocr_t alloc) {\n    ggml_gallocr_free(alloc->galloc);\n    ggml_tallocr_free(alloc->talloc);\n    free(alloc);\n}\n\nbool ggml_allocr_is_measure(ggml_allocr_t alloc) {\n    return ggml_tallocr_is_measure(alloc->talloc);\n}\n\nvoid ggml_allocr_reset(ggml_allocr_t alloc) {\n    ggml_tallocr_reset(alloc->talloc);\n}\n\nvoid ggml_allocr_alloc(ggml_allocr_t alloc, struct ggml_tensor * tensor) {\n    ggml_tallocr_alloc(alloc->talloc, tensor);\n}\n\nsize_t ggml_allocr_max_size(ggml_allocr_t alloc) {\n    return ggml_tallocr_max_size(alloc->talloc);\n}\n\nsize_t ggml_allocr_alloc_graph(ggml_allocr_t alloc, struct ggml_cgraph * graph) {\n    return ggml_gallocr_alloc_graph(alloc->galloc, alloc->talloc, graph);\n}\n"
        },
        {
          "name": "ggml-alloc.h",
          "type": "blob",
          "size": 3.3837890625,
          "content": "#pragma once\n\n#include \"ggml.h\"\n\n#ifdef  __cplusplus\nextern \"C\" {\n#endif\n\nstruct ggml_backend;\nstruct ggml_backend_buffer;\n\n//\n// Legacy API\n//\n\ntypedef struct ggml_allocr * ggml_allocr_t;\n\n// initialize allocator for use with CPU backend only\nGGML_API ggml_allocr_t ggml_allocr_new(void * data, size_t size, size_t alignment);\nGGML_API ggml_allocr_t ggml_allocr_new_measure(size_t alignment);\n\n// initialize allocator for use with ggml-backend\nGGML_API ggml_allocr_t ggml_allocr_new_from_buffer(struct ggml_backend_buffer * buffer);\nGGML_API ggml_allocr_t ggml_allocr_new_from_backend(struct ggml_backend * backend, size_t size); // allocates an owned buffer\nGGML_API ggml_allocr_t ggml_allocr_new_measure_from_backend(struct ggml_backend * backend);\n\nGGML_API struct ggml_backend_buffer * ggml_allocr_get_buffer(ggml_allocr_t alloc);\n\n// tell the allocator to parse nodes following the order described in the list\n// you should call this if your graph are optimized to execute out-of-order\nGGML_API void   ggml_allocr_set_parse_seq(ggml_allocr_t alloc, const int * list, int n);\n\nGGML_API void   ggml_allocr_free       (ggml_allocr_t alloc);\nGGML_API bool   ggml_allocr_is_measure (ggml_allocr_t alloc);\nGGML_API void   ggml_allocr_reset      (ggml_allocr_t alloc);\nGGML_API void   ggml_allocr_alloc      (ggml_allocr_t alloc, struct ggml_tensor * tensor);\nGGML_API size_t ggml_allocr_max_size   (ggml_allocr_t alloc);\n\nGGML_API size_t ggml_allocr_alloc_graph(ggml_allocr_t alloc, struct ggml_cgraph * graph);\n\n//\n// ggml-backend v2 API\n//\n\n// Seperate tensor and graph allocator objects\n// This is necessary for multi-backend allocation because the graph allocator needs to use multiple tensor allocators\n// The original API is kept as a wrapper around the new API\n\n// Tensor allocator\ntypedef struct ggml_tallocr * ggml_tallocr_t;\n\nGGML_API ggml_tallocr_t ggml_tallocr_new(void * data, size_t size, size_t alignment);\nGGML_API ggml_tallocr_t ggml_tallocr_new_measure(size_t alignment);\nGGML_API ggml_tallocr_t ggml_tallocr_new_from_buffer(struct ggml_backend_buffer * buffer);\nGGML_API ggml_tallocr_t ggml_tallocr_new_from_backend(struct ggml_backend * backend, size_t size); // allocates an owned buffer\nGGML_API ggml_tallocr_t ggml_tallocr_new_measure_from_backend(struct ggml_backend * backend);\n\nGGML_API struct ggml_backend_buffer * ggml_tallocr_get_buffer(ggml_tallocr_t talloc);\n\nGGML_API void   ggml_tallocr_free       (ggml_tallocr_t talloc);\nGGML_API bool   ggml_tallocr_is_measure (ggml_tallocr_t talloc);\nGGML_API void   ggml_tallocr_reset      (ggml_tallocr_t talloc);\nGGML_API void   ggml_tallocr_alloc      (ggml_tallocr_t talloc, struct ggml_tensor * tensor);\nGGML_API size_t ggml_tallocr_max_size   (ggml_tallocr_t talloc);\n\n\n// Graph allocator\ntypedef struct ggml_gallocr * ggml_gallocr_t;\n\nGGML_API ggml_gallocr_t ggml_gallocr_new(void);\nGGML_API void   ggml_gallocr_free(ggml_gallocr_t galloc);\n\nGGML_API void   ggml_gallocr_set_parse_seq(ggml_gallocr_t galloc, const int * list, int n);\nGGML_API size_t ggml_gallocr_alloc_graph(ggml_gallocr_t galloc, ggml_tallocr_t talloc, struct ggml_cgraph * graph);\n\n// Allocate tensors from the allocators given by the hash table\nGGML_API void   ggml_gallocr_alloc_graph_n(\n                    ggml_gallocr_t galloc,\n                    struct ggml_cgraph * graph,\n                    struct ggml_hash_set hash_set,\n                    ggml_tallocr_t * hash_node_talloc);\n\n#ifdef  __cplusplus\n}\n#endif\n"
        },
        {
          "name": "ggml-backend-impl.h",
          "type": "blob",
          "size": 3.1884765625,
          "content": "#pragma once\n\n// ggml-backend internal header\n\n#include \"ggml-backend.h\"\n\n#ifdef  __cplusplus\nextern \"C\" {\n#endif\n\n    //\n    // Backend buffer\n    //\n\n    typedef void * ggml_backend_buffer_context_t;\n\n    struct ggml_backend_buffer_i {\n        void   (*free_buffer)   (ggml_backend_buffer_t buffer);\n        void * (*get_base)      (ggml_backend_buffer_t buffer); // get base pointer\n        size_t (*get_alloc_size)(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor); // pre-allocation callback\n        void   (*init_tensor)   (ggml_backend_buffer_t buffer, struct ggml_tensor * tensor); // post-allocation callback\n        void   (*free_tensor)   (ggml_backend_buffer_t buffer, struct ggml_tensor * tensor); // pre-free callback\n    };\n\n    struct ggml_backend_buffer {\n        struct ggml_backend_buffer_i iface;\n\n        ggml_backend_t                backend;\n        ggml_backend_buffer_context_t context;\n\n        size_t size;\n    };\n\n    GGML_API ggml_backend_buffer_t ggml_backend_buffer_init(\n            struct ggml_backend                  * backend,\n            struct ggml_backend_buffer_i           iface,\n                   ggml_backend_buffer_context_t   context,\n                   size_t                          size);\n\n    //\n    // Backend\n    //\n\n    typedef void * ggml_backend_context_t;\n\n    struct ggml_backend_i {\n        const char * (*get_name)(ggml_backend_t backend);\n\n        void (*free)(ggml_backend_t backend);\n\n        // buffer allocation\n        ggml_backend_buffer_t (*alloc_buffer)(ggml_backend_t backend, size_t size);\n\n        // get buffer alignment\n        size_t (*get_alignment)(ggml_backend_t backend);\n\n        // tensor data access\n        // these functions can be asynchronous, helper functions are provided for synchronous access that automatically call synchronize\n        void (*set_tensor_async)(ggml_backend_t backend,       struct ggml_tensor * tensor, const void * data, size_t offset, size_t size);\n        void (*get_tensor_async)(ggml_backend_t backend, const struct ggml_tensor * tensor,       void * data, size_t offset, size_t size);\n        void (*synchronize)     (ggml_backend_t backend);\n\n        // (optional) copy tensor between different backends, allow for single-copy tranfers\n        void (*cpy_tensor_from)(ggml_backend_t backend, struct ggml_tensor * src, struct ggml_tensor * dst);\n        void (*cpy_tensor_to)  (ggml_backend_t backend, struct ggml_tensor * src, struct ggml_tensor * dst);\n\n        // compute graph with a plan\n        ggml_backend_graph_plan_t (*graph_plan_create) (ggml_backend_t backend, struct ggml_cgraph * cgraph);\n        void                      (*graph_plan_free)   (ggml_backend_t backend, ggml_backend_graph_plan_t plan);\n        void                      (*graph_plan_compute)(ggml_backend_t backend, ggml_backend_graph_plan_t plan);\n\n        // compute graph without a plan\n        void (*graph_compute)(ggml_backend_t backend, struct ggml_cgraph * cgraph);\n\n        // check if the backend supports an operation\n        bool (*supports_op)(ggml_backend_t backend, const struct ggml_tensor * op);\n    };\n\n    struct ggml_backend {\n        struct ggml_backend_i iface;\n\n        ggml_backend_context_t context;\n    };\n\n#ifdef  __cplusplus\n}\n#endif\n"
        },
        {
          "name": "ggml-backend.c",
          "type": "blob",
          "size": 35.431640625,
          "content": "#include \"ggml-backend-impl.h\"\n#include \"ggml-alloc.h\"\n#include \"ggml-impl.h\"\n\n#include <assert.h>\n#include <limits.h>\n#include <stdarg.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n#define UNUSED GGML_UNUSED\n\n#define MAX(a, b) ((a) > (b) ? (a) : (b))\n\n// backend buffer\n\nggml_backend_buffer_t ggml_backend_buffer_init(\n        struct ggml_backend                  * backend,\n        struct ggml_backend_buffer_i           iface,\n               ggml_backend_buffer_context_t   context,\n               size_t                          size) {\n    ggml_backend_buffer_t buffer = malloc(sizeof(struct ggml_backend_buffer));\n\n    GGML_ASSERT(iface.get_base != NULL);\n\n    (*buffer) = (struct ggml_backend_buffer) {\n        /* .interface = */ iface,\n        /* .backend   = */ backend,\n        /* .context   = */ context,\n        /* .size      = */ size,\n    };\n\n    return buffer;\n}\n\nvoid ggml_backend_buffer_free(ggml_backend_buffer_t buffer) {\n    if (buffer == NULL) {\n        return;\n    }\n\n    if (buffer->iface.free_buffer != NULL) {\n        buffer->iface.free_buffer(buffer);\n    }\n    free(buffer);\n}\n\nsize_t ggml_backend_buffer_get_alignment(ggml_backend_buffer_t buffer) {\n    return ggml_backend_get_alignment(buffer->backend);\n}\n\nsize_t ggml_backend_buffer_get_size(ggml_backend_buffer_t buffer) {\n    return buffer->size;\n}\n\nvoid * ggml_backend_buffer_get_base(ggml_backend_buffer_t buffer) {\n    void * base = buffer->iface.get_base(buffer);\n\n    GGML_ASSERT(base != NULL && \"backend buffer base cannot be NULL\");\n\n    return base;\n}\n\nsize_t ggml_backend_buffer_get_alloc_size(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor) {\n    // get_alloc_size is optional, defaults to ggml_nbytes\n    if (buffer->iface.get_alloc_size) {\n        return buffer->iface.get_alloc_size(buffer, tensor);\n    }\n    return ggml_nbytes(tensor);\n}\n\nvoid ggml_backend_buffer_init_tensor(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor) {\n    // init_tensor is optional\n    if (buffer->iface.init_tensor) {\n        buffer->iface.init_tensor(buffer, tensor);\n    }\n}\n\nvoid ggml_backend_buffer_free_tensor(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor) {\n    // free_tensor is optional\n    if (buffer->iface.free_tensor) {\n        buffer->iface.free_tensor(buffer, tensor);\n    }\n}\n\n// backend\n\nggml_backend_t ggml_get_backend(const struct ggml_tensor * tensor) {\n    return tensor->buffer ? tensor->buffer->backend : NULL;\n}\n\nconst char * ggml_backend_name(ggml_backend_t backend) {\n    if (backend == NULL) {\n        return \"NULL\";\n    }\n    return backend->iface.get_name(backend);\n}\n\nvoid ggml_backend_free(ggml_backend_t backend) {\n    if (backend == NULL) {\n        return;\n    }\n\n    backend->iface.free(backend);\n}\n\nggml_backend_buffer_t ggml_backend_alloc_buffer(ggml_backend_t backend, size_t size) {\n    return backend->iface.alloc_buffer(backend, size);\n}\n\nsize_t ggml_backend_get_alignment(ggml_backend_t backend) {\n    return backend->iface.get_alignment(backend);\n}\n\nvoid ggml_backend_tensor_set_async(struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n    ggml_get_backend(tensor)->iface.set_tensor_async(ggml_get_backend(tensor), tensor, data, offset, size);\n}\n\nvoid ggml_backend_tensor_get_async(const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n    ggml_get_backend(tensor)->iface.get_tensor_async(ggml_get_backend(tensor), tensor, data, offset, size);\n}\n\nvoid ggml_backend_tensor_set(struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n    ggml_backend_t backend = ggml_get_backend(tensor);\n\n    GGML_ASSERT(tensor->data != NULL && \"tensor not allocated\");\n    GGML_ASSERT(backend != NULL && \"tensor backend not set\");\n\n    backend->iface.set_tensor_async(backend, tensor, data, offset, size);\n    backend->iface.synchronize(backend);\n}\n\nvoid ggml_backend_tensor_get(const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n    ggml_backend_t backend = ggml_get_backend(tensor);\n\n    GGML_ASSERT(tensor->data != NULL && \"tensor not allocated\");\n    GGML_ASSERT(backend != NULL && \"tensor backend not set\");\n\n    backend->iface.get_tensor_async(backend, tensor, data, offset, size);\n    backend->iface.synchronize(backend);\n}\n\nvoid ggml_backend_synchronize(ggml_backend_t backend) {\n    backend->iface.synchronize(backend);\n}\n\nggml_backend_graph_plan_t ggml_backend_graph_plan_create(ggml_backend_t backend, struct ggml_cgraph * cgraph) {\n    return backend->iface.graph_plan_create(backend, cgraph);\n}\n\nvoid ggml_backend_graph_plan_free(ggml_backend_t backend, ggml_backend_graph_plan_t plan) {\n    backend->iface.graph_plan_free(backend, plan);\n}\n\nvoid ggml_backend_graph_plan_compute(ggml_backend_t backend, ggml_backend_graph_plan_t plan) {\n    backend->iface.graph_plan_compute(backend, plan);\n}\n\nvoid ggml_backend_graph_compute(ggml_backend_t backend, struct ggml_cgraph * cgraph) {\n    backend->iface.graph_compute(backend, cgraph);\n}\n\nbool ggml_backend_supports_op(ggml_backend_t backend, const struct ggml_tensor * op) {\n    return backend->iface.supports_op(backend, op);\n}\n\n// backend copy\n\nstatic bool ggml_are_same_layout(const struct ggml_tensor * a, const struct ggml_tensor * b) {\n    if (a->type != b->type) {\n        return false;\n    }\n    for (int i = 0; i < GGML_MAX_DIMS; i++) {\n        if (a->ne[i] != b->ne[i]) {\n            return false;\n        }\n        if (a->nb[i] != b->nb[i]) {\n            return false;\n        }\n    }\n    return true;\n}\n\nvoid ggml_backend_tensor_copy(struct ggml_tensor * src, struct ggml_tensor * dst) {\n    //printf(\"src: %s ne: [%d %d %d %d] nb: [%d %d %d %d]\\n\", src->name, (int)src->ne[0], (int)src->ne[1], (int)src->ne[2], (int)src->ne[3], (int)src->nb[0], (int)src->nb[1], (int)src->nb[2], (int)src->nb[3]);\n    //printf(\"dst: %s ne: [%d %d %d %d] nb: [%d %d %d %d]\\n\", dst->name, (int)dst->ne[0], (int)dst->ne[1], (int)dst->ne[2], (int)dst->ne[3], (int)dst->nb[0], (int)dst->nb[1], (int)dst->nb[2], (int)dst->nb[3]);\n    GGML_ASSERT(ggml_are_same_layout(src, dst) && \"cannot copy tensors with different layouts\");\n\n    // fprintf(stderr, \"cpy tensor %s from %s to %s (%lu bytes)\\n\", src->name, ggml_backend_name(src->backend), ggml_backend_name(dst->backend), ggml_nbytes(src));\n\n    if (src == dst) {\n        return;\n    }\n\n    // TODO: allow backends to support copy to/from same backend\n\n    if (ggml_get_backend(dst)->iface.cpy_tensor_from != NULL) {\n        ggml_get_backend(dst)->iface.cpy_tensor_from(ggml_get_backend(dst)->context, src, dst);\n    } else if (ggml_get_backend(src)->iface.cpy_tensor_to != NULL) {\n        ggml_get_backend(src)->iface.cpy_tensor_to(ggml_get_backend(src)->context, src, dst);\n    } else {\n        // shouldn't be hit when copying from/to CPU\n        #ifndef NDEBUG\n        fprintf(stderr, \"ggml_backend_tensor_copy: neither cpy_tensor_from nor cpy_tensor_to are implemented for backends %s and %s, falling back to get/set\\n\", ggml_backend_name(src->buffer->backend), ggml_backend_name(dst->buffer->backend));\n        #endif\n        size_t nbytes = ggml_nbytes(src);\n        void * data = malloc(nbytes);\n        ggml_backend_tensor_get(src, data, 0, nbytes);\n        ggml_backend_tensor_set(dst, data, 0, nbytes);\n        free(data);\n    }\n}\n\n// backend CPU\n\nstruct ggml_backend_cpu_context {\n    int n_threads;\n    void * work_data;\n    size_t work_size;\n};\n\nstatic const char * ggml_backend_cpu_name(ggml_backend_t backend) {\n    return \"CPU\";\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cpu_free(ggml_backend_t backend) {\n    struct ggml_backend_cpu_context * cpu_ctx = (struct ggml_backend_cpu_context *)backend->context;\n    free(cpu_ctx->work_data);\n    free(cpu_ctx);\n    free(backend);\n}\n\nstatic void * ggml_backend_cpu_buffer_get_base(ggml_backend_buffer_t buffer) {\n    return (void *)buffer->context;\n}\n\nstatic void ggml_backend_cpu_buffer_free_buffer(ggml_backend_buffer_t buffer) {\n    free(buffer->context);\n    UNUSED(buffer);\n}\n\nstatic struct ggml_backend_buffer_i cpu_backend_buffer_i = {\n    /* .free_buffer    = */ ggml_backend_cpu_buffer_free_buffer,\n    /* .get_base       = */ ggml_backend_cpu_buffer_get_base,\n    /* .get_alloc_size = */ NULL, // defaults to ggml_nbytes\n    /* .init_tensor    = */ NULL, // no initialization required\n    /* .free_tensor    = */ NULL, // no cleanup required\n};\n\n// for buffers from ptr, free is not called\nstatic struct ggml_backend_buffer_i cpu_backend_buffer_i_from_ptr = {\n    /* .free_buffer    = */ NULL, // ptr is not owned by the buffer, so it does not need to be freed\n    /* .get_base       = */ ggml_backend_cpu_buffer_get_base,\n    /* .get_alloc_size = */ NULL, // defaults to ggml_nbytes\n    /* .init_tensor    = */ NULL,\n    /* .free_tensor    = */ NULL,\n};\n\nstatic const size_t TENSOR_ALIGNMENT = 64; // should be enough for AVX 512\n\nstatic ggml_backend_buffer_t ggml_backend_cpu_alloc_buffer(ggml_backend_t backend, size_t size) {\n    size += TENSOR_ALIGNMENT;   // malloc may return an address that is not aligned\n    void * data = malloc(size); // TODO: maybe use GGML_ALIGNED_MALLOC?\n\n    GGML_ASSERT(data != NULL && \"failed to allocate buffer\");\n\n    return ggml_backend_buffer_init(backend, cpu_backend_buffer_i, data, size);\n}\n\nstatic size_t ggml_backend_cpu_get_alignment(ggml_backend_t backend) {\n    return TENSOR_ALIGNMENT;\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cpu_set_tensor_async(ggml_backend_t backend, struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n    GGML_ASSERT(offset + size <= ggml_nbytes(tensor) && \"tensor write out of bounds\");\n    GGML_ASSERT(tensor->data != NULL && \"tensor not allocated\");\n\n    memcpy((char *)tensor->data + offset, data, size);\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cpu_get_tensor_async(ggml_backend_t backend, const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n    GGML_ASSERT(offset + size <= ggml_nbytes(tensor) && \"tensor read out of bounds\");\n    GGML_ASSERT(tensor->data != NULL && \"tensor not allocated\");\n\n    memcpy(data, (const char *)tensor->data + offset, size);\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cpu_synchronize(ggml_backend_t backend) {\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cpu_cpy_tensor_from(ggml_backend_t backend, struct ggml_tensor * src, struct ggml_tensor * dst) {\n    ggml_backend_tensor_get(src, dst->data, 0, ggml_nbytes(src));\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cpu_cpy_tensor_to(ggml_backend_t backend, struct ggml_tensor * src, struct ggml_tensor * dst) {\n    ggml_backend_tensor_set(dst, src->data, 0, ggml_nbytes(src));\n\n    UNUSED(backend);\n}\n\nstruct ggml_backend_plan_cpu {\n    struct ggml_cplan cplan;\n    struct ggml_cgraph cgraph;\n};\n\nstatic ggml_backend_graph_plan_t ggml_backend_cpu_graph_plan_create(ggml_backend_t backend, struct ggml_cgraph * cgraph) {\n    struct ggml_backend_cpu_context * cpu_ctx = (struct ggml_backend_cpu_context *)backend->context;\n\n    struct ggml_backend_plan_cpu * cpu_plan = malloc(sizeof(struct ggml_backend_plan_cpu));\n\n    cpu_plan->cplan = ggml_graph_plan(cgraph, cpu_ctx->n_threads);\n    cpu_plan->cgraph = *cgraph;\n\n    if (cpu_plan->cplan.work_size > 0) {\n        cpu_plan->cplan.work_data = malloc(cpu_plan->cplan.work_size);\n    }\n\n    return cpu_plan;\n}\n\nstatic void ggml_backend_cpu_graph_plan_free(ggml_backend_t backend, ggml_backend_graph_plan_t plan) {\n    struct ggml_backend_plan_cpu * cpu_plan = (struct ggml_backend_plan_cpu *)plan;\n\n    free(cpu_plan->cplan.work_data);\n    free(cpu_plan);\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cpu_graph_plan_compute(ggml_backend_t backend, ggml_backend_graph_plan_t plan) {\n    struct ggml_backend_plan_cpu * cpu_plan = (struct ggml_backend_plan_cpu *)plan;\n\n    ggml_graph_compute(&cpu_plan->cgraph, &cpu_plan->cplan);\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cpu_graph_compute(ggml_backend_t backend, struct ggml_cgraph * cgraph) {\n    struct ggml_backend_cpu_context * cpu_ctx = (struct ggml_backend_cpu_context *)backend->context;\n\n    struct ggml_cplan cplan = ggml_graph_plan(cgraph, cpu_ctx->n_threads);\n\n    if (cpu_ctx->work_size < cplan.work_size) {\n        // TODO: may be faster to free and use malloc to avoid the copy\n        cpu_ctx->work_data = realloc(cpu_ctx->work_data, cplan.work_size);\n        cpu_ctx->work_size = cplan.work_size;\n    }\n\n    cplan.work_data = cpu_ctx->work_data;\n\n    ggml_graph_compute(cgraph, &cplan);\n}\n\nstatic bool ggml_backend_cpu_supports_op(ggml_backend_t backend, const struct ggml_tensor * op) {\n    return true;\n    UNUSED(backend);\n    UNUSED(op);\n}\n\nstatic struct ggml_backend_i cpu_backend_i = {\n    /* .get_name            = */ ggml_backend_cpu_name,\n    /* .free                = */ ggml_backend_cpu_free,\n    /* .alloc_buffer        = */ ggml_backend_cpu_alloc_buffer,\n    /* .get_alignment       = */ ggml_backend_cpu_get_alignment,\n    /* .set_tensor_async    = */ ggml_backend_cpu_set_tensor_async,\n    /* .get_tensor_async    = */ ggml_backend_cpu_get_tensor_async,\n    /* .synchronize         = */ ggml_backend_cpu_synchronize,\n    /* .cpy_tensor_from     = */ ggml_backend_cpu_cpy_tensor_from,\n    /* .cpy_tensor_to       = */ ggml_backend_cpu_cpy_tensor_to,\n    /* .graph_plan_create   = */ ggml_backend_cpu_graph_plan_create,\n    /* .graph_plan_free     = */ ggml_backend_cpu_graph_plan_free,\n    /* .graph_plan_compute  = */ ggml_backend_cpu_graph_plan_compute,\n    /* .graph_compute       = */ ggml_backend_cpu_graph_compute,\n    /* .supports_op         = */ ggml_backend_cpu_supports_op,\n};\n\nggml_backend_t ggml_backend_cpu_init(void) {\n    struct ggml_backend_cpu_context * ctx = malloc(sizeof(struct ggml_backend_cpu_context));\n\n    ctx->n_threads = GGML_DEFAULT_N_THREADS;\n    ctx->work_data = NULL;\n    ctx->work_size = 0;\n\n    ggml_backend_t cpu_backend = malloc(sizeof(struct ggml_backend));\n\n    *cpu_backend = (struct ggml_backend) {\n        /* .interface = */ cpu_backend_i,\n        /* .context   = */ ctx\n    };\n    return cpu_backend;\n}\n\nbool ggml_backend_is_cpu(ggml_backend_t backend) {\n    return backend->iface.get_name == ggml_backend_cpu_name;\n}\n\nvoid ggml_backend_cpu_set_n_threads(ggml_backend_t backend_cpu, int n_threads) {\n    GGML_ASSERT(ggml_backend_is_cpu(backend_cpu));\n\n    struct ggml_backend_cpu_context * ctx = (struct ggml_backend_cpu_context *)backend_cpu->context;\n    ctx->n_threads = n_threads;\n}\n\nggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(ggml_backend_t backend_cpu, void * ptr, size_t size) {\n    return ggml_backend_buffer_init(backend_cpu, cpu_backend_buffer_i_from_ptr, ptr, size);\n}\n\n// scheduler\n\n#define GGML_MAX_BACKENDS 4\n#define GGML_MAX_SPLITS 256\n#define GGML_MAX_SPLIT_INPUTS 16\n\nstruct ggml_backend_sched_split {\n    ggml_tallocr_t tallocr;\n    int i_start;\n    int i_end;\n    struct ggml_tensor * inputs[GGML_MAX_SPLIT_INPUTS];\n    int n_inputs;\n    struct ggml_cgraph * graph;\n};\n\nstruct ggml_backend_sched {\n    int n_backends;\n    ggml_backend_t backends[GGML_MAX_BACKENDS];\n    ggml_tallocr_t  tallocs[GGML_MAX_BACKENDS];\n\n    ggml_gallocr_t galloc;\n\n    struct ggml_hash_set    hash_set;\n    ggml_tallocr_t *        node_talloc;                     // [hash_set.size]\n    struct ggml_tensor * (* node_copies)[GGML_MAX_BACKENDS]; // [hash_set.size][GGML_MAX_BACKENDS]\n\n    struct ggml_cgraph * graph;\n    struct ggml_backend_sched_split splits[GGML_MAX_SPLITS];\n    int n_splits;\n\n    struct ggml_context * ctx;\n\n    // align context_buffer to GGML_MEM_ALIGN\n    #ifdef _MSC_VER\n    __declspec(align(GGML_MEM_ALIGN))\n    #else\n    __attribute__((aligned(GGML_MEM_ALIGN)))\n    #endif\n    char context_buffer[GGML_MAX_SPLITS*GGML_MAX_SPLIT_INPUTS*sizeof(struct ggml_tensor) + GGML_MAX_SPLITS*sizeof(struct ggml_cgraph)];\n};\n\n#define hash_id(node) ggml_hash_find_or_insert(sched->hash_set, node)\n#define node_allocr(node) sched->node_talloc[hash_id(node)]\n\nstatic bool ggml_is_view_op(enum ggml_op op) {\n    return op == GGML_OP_VIEW || op == GGML_OP_RESHAPE || op == GGML_OP_PERMUTE || op == GGML_OP_TRANSPOSE;\n}\n\n// returns the priority of the backend, lower is better\nstatic int sched_backend_prio(ggml_backend_sched_t sched, ggml_backend_t backend) {\n    for (int i = 0; i < sched->n_backends; i++) {\n        if (sched->backends[i] == backend) {\n            return i;\n        }\n    }\n    return INT_MAX;\n}\n\nstatic int sched_allocr_prio(ggml_backend_sched_t sched, ggml_tallocr_t allocr) {\n    for (int i = 0; i < sched->n_backends; i++) {\n        if (sched->tallocs[i] == allocr) {\n            return i;\n        }\n    }\n    return INT_MAX;\n}\n\n// returns the backend that should be used for the node based on the current locations\nchar causes[GGML_DEFAULT_GRAPH_SIZE*4 + GGML_MAX_SPLITS*GGML_MAX_SPLIT_INPUTS][128]; // debug, remove\nstatic ggml_backend_t sched_backend_from_cur(ggml_backend_sched_t sched, struct ggml_tensor * node) {\n    // if the dst tensor is already allocated in a buffer, we must assume that it is critical to keep it there\n    // ie. kv cache updates\n    // note that this doesn't allow fallback to CPU. need to add output tensors to the splits to copy the data back to the original backend.\n    // dst\n    ggml_backend_t cur_backend = ggml_get_backend(node);\n    if (cur_backend != NULL) {\n        sprintf(causes[hash_id(node)], \"1.dst\");\n        return cur_backend;\n    }\n\n    // view_src\n    if (node->view_src != NULL && ggml_get_backend(node->view_src) != NULL) {\n        sprintf(causes[hash_id(node)], \"1.vsrc\");\n        return ggml_get_backend(node->view_src);\n    }\n\n    // src\n    int cur_prio = INT_MAX;\n    size_t cur_size = 0;\n\n    for (int i = 0; i < GGML_MAX_SRC; i++) {\n        const struct ggml_tensor * src = node->src[i];\n        if (src == NULL) {\n            break;\n        }\n        ggml_backend_t src_backend = ggml_get_backend(src);\n        if (src_backend != NULL) {\n            int src_prio = sched_backend_prio(sched, src_backend);\n            size_t src_size = ggml_nbytes(src);\n            if (src_prio < cur_prio && src_size >= cur_size) {\n                cur_prio = src_prio;\n                cur_size = src_size;\n                cur_backend = src_backend;\n                sprintf(causes[hash_id(node)], \"1.src%d\", i);\n            }\n        }\n    }\n    return cur_backend;\n}\n\nstatic char * fmt_size(size_t size) {\n    static char buffer[128];\n    if (size >= 1024*1024) {\n        sprintf(buffer, \"%zuM\", size/1024/1024);\n    } else {\n        sprintf(buffer, \"%zuK\", size/1024);\n    }\n    return buffer;\n}\n\nstatic void sched_print_assignments(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {\n    int cur_split = 0;\n    for (int i = 0; i < graph->n_nodes; i++) {\n        if (cur_split < sched->n_splits && i == sched->splits[cur_split].i_start) {\n            ggml_backend_t split_backend = ggml_tallocr_get_buffer(sched->splits[cur_split].tallocr)->backend;\n            fprintf(stderr, \"\\n## SPLIT #%d: %s # %d inputs: \", cur_split, ggml_backend_name(split_backend), sched->splits[cur_split].n_inputs);\n            for (int j = 0; j < sched->splits[cur_split].n_inputs; j++) {\n                fprintf(stderr, \"[%s (%5.5s)] \", sched->splits[cur_split].inputs[j]->name, fmt_size(ggml_nbytes(sched->splits[cur_split].inputs[j])));\n            }\n            fprintf(stderr, \"\\n\");\n            cur_split++;\n        }\n        struct ggml_tensor * node = graph->nodes[i];\n        if (ggml_is_view_op(node->op)) {\n            continue;\n        }\n        ggml_tallocr_t node_allocr = node_allocr(node);\n        ggml_backend_t node_backend = node_allocr ? ggml_tallocr_get_buffer(node_allocr)->backend : NULL;\n        fprintf(stderr, \"node #%3d (%10.10s): %20.20s (%4.4s) [%4.4s %8.8s]:\", i, ggml_op_name(node->op), node->name, fmt_size(ggml_nbytes(node)), node_allocr ? ggml_backend_name(node_backend) : \"NULL\", causes[hash_id(node)]);\n        for (int j = 0; j < GGML_MAX_SRC; j++) {\n            struct ggml_tensor * src = node->src[j];\n            if (src == NULL) {\n                break;\n            }\n            ggml_tallocr_t src_allocr = node_allocr(src);\n            ggml_backend_t src_backend = src_allocr ? ggml_tallocr_get_buffer(src_allocr)->backend : NULL;\n            fprintf(stderr, \" %20.20s (%4.4s) [%4.4s %8.8s]\", src->name, fmt_size(ggml_nbytes(src)), src_backend ? ggml_backend_name(src_backend) : \"NULL\", causes[hash_id(src)]);\n        }\n        fprintf(stderr, \"\\n\");\n    }\n}\n\n// creates a copy of the tensor with the same memory layout\nstatic struct ggml_tensor * ggml_dup_tensor_layout(struct ggml_context * ctx, const struct ggml_tensor * tensor) {\n    struct ggml_tensor * dup = ggml_dup_tensor(ctx, tensor);\n    for (int i = 0; i < GGML_MAX_DIMS; i++) {\n        dup->nb[i] = tensor->nb[i];\n    }\n    return dup;\n}\n\n// assigns backends to ops and splits the graph into subgraphs that can be computed on the same backend\n// TODO: merge passes\nstatic void sched_split_graph(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {\n    // reset state\n    size_t hash_size = sched->hash_set.size;\n    memset(sched->hash_set.keys, 0, sizeof(sched->hash_set.keys[0]) * hash_size);\n    memset(sched->node_talloc,   0, sizeof(sched->node_talloc[0])   * hash_size);\n    memset(sched->node_copies,   0, sizeof(sched->node_copies[0])   * hash_size);\n    sched->n_splits = 0;\n\n    struct ggml_init_params params = {\n        /*.mem_size =   */ sizeof(sched->context_buffer),\n        /*.mem_buffer = */ sched->context_buffer,\n        /*.no_alloc =   */ true\n    };\n\n    if (sched->ctx != NULL) {\n        ggml_free(sched->ctx);\n    }\n\n    sched->ctx = ggml_init(params);\n\n    // pass 1: assign backends to ops with allocated inputs\n    for (int i = 0; i < graph->n_leafs; i++) {\n        struct ggml_tensor * leaf = graph->leafs[i];\n        if (node_allocr(leaf) != NULL) {\n            // do not overwrite user assignments\n            continue;\n        }\n        ggml_backend_t leaf_backend = ggml_get_backend(leaf);\n        if (leaf_backend == NULL && leaf->view_src != NULL) {\n            leaf_backend = ggml_get_backend(leaf->view_src);\n        }\n        if (leaf_backend != NULL) {\n            node_allocr(leaf) = ggml_backend_sched_get_tallocr(sched, leaf_backend);\n        }\n    }\n\n    for (int i = 0; i < graph->n_nodes; i++) {\n        struct ggml_tensor * node = graph->nodes[i];\n        if (node_allocr(node) != NULL) {\n            // do not overwrite user assignments\n            continue;\n        }\n        ggml_backend_t node_backend = sched_backend_from_cur(sched, node);\n        if (node_backend != NULL) {\n            node_allocr(node) = ggml_backend_sched_get_tallocr(sched, node_backend);\n        }\n    }\n    //printf(\"PASS 1 ASSIGNMENTS\\n\"); sched_print_assignments(sched, graph);\n\n    // pass 2: assign backends to ops from current assignments\n    // TODO:\n    //  - reuse sched_backend_from_cur\n    for (int i = 0; i < graph->n_nodes; i++) {\n        struct ggml_tensor * node = graph->nodes[i];\n        ggml_tallocr_t node_allocr = node_allocr(node);\n        if (node_allocr == NULL) {\n            int    cur_prio = INT_MAX;\n            size_t cur_size = 0;\n            for (int j = 0; j < GGML_MAX_SRC; j++) {\n                struct ggml_tensor * src = node->src[j];\n                if (src == NULL) {\n                    break;\n                }\n                ggml_tallocr_t src_allocr = node_allocr(src);\n                if (src_allocr != NULL) {\n                    int    src_prio = sched_allocr_prio(sched, src_allocr);\n                    size_t src_size = ggml_nbytes(src);\n                    if (src_prio < cur_prio && src_size >= cur_size) {\n                        cur_prio = src_prio;\n                        cur_size = src_size;\n                        node_allocr = src_allocr;\n                        sprintf(causes[hash_id(node)], \"2.src%d\", j);\n                    }\n                }\n            }\n            if (node_allocr != NULL) {\n                node_allocr(node) = node_allocr;\n            }\n        }\n    }\n    //printf(\"PASS 2 ASSIGNMENTS\\n\"); sched_print_assignments(sched, graph);\n\n    // pass 3: assign backends to remaining src from dst (should only be leafs)\n    for (int i = 0; i < graph->n_nodes; i++) {\n        struct ggml_tensor * node = graph->nodes[i];\n        ggml_tallocr_t node_allocr = node_allocr(node);\n        for (int j = 0; j < GGML_MAX_SRC; j++) {\n            struct ggml_tensor * src = node->src[j];\n            if (src == NULL) {\n                break;\n            }\n            ggml_tallocr_t src_allocr = node_allocr(src);\n            if (src_allocr == NULL) {\n                node_allocr(src) = node_allocr;\n            }\n        }\n    }\n    //printf(\"PASS 3 ASSIGNMENTS\\n\"); sched_print_assignments(sched, graph);\n\n    // pass 4: split graph, find tensors that need to be copied\n    // TODO:\n    //  - when switching from a less preferred backend to a more preferred backend, check if it is possible to move the switch to an earlier point for the same cost\n    // find first backend\n    int cur_split = 0;\n    for (int i = 0; i < graph->n_nodes; i++) {\n        struct ggml_tensor * node = graph->nodes[i];\n        if (node->view_src == NULL) {\n            sched->splits[0].tallocr = node_allocr(node);\n            break;\n        }\n    }\n    sched->splits[0].i_start = 0;\n    sched->splits[0].n_inputs = 0;\n    memset(sched->splits[0].inputs, 0, sizeof(sched->splits[0].inputs)); //HACK\n    ggml_tallocr_t cur_allocr = sched->splits[0].tallocr;\n    size_t cur_backend_id = sched_allocr_prio(sched, cur_allocr);\n    for (int i = 0; i < graph->n_nodes; i++) {\n        struct ggml_tensor * node = graph->nodes[i];\n\n        if (ggml_is_view_op(node->op)) {\n            continue;\n        }\n\n        ggml_tallocr_t node_allocr = node_allocr(node);\n\n        if (node_allocr != cur_allocr) {\n            sched->splits[cur_split].i_end = i;\n            cur_split++;\n            GGML_ASSERT(cur_split < GGML_MAX_SPLITS);\n            sched->splits[cur_split].tallocr = node_allocr;\n            sched->splits[cur_split].i_start = i;\n            sched->splits[cur_split].n_inputs = 0;\n            memset(sched->splits[cur_split].inputs, 0, sizeof(sched->splits[cur_split].inputs)); //HACK\n            cur_allocr = node_allocr;\n            cur_backend_id = sched_allocr_prio(sched, cur_allocr);\n        }\n\n        // find inputs that are not on the same backend\n        for (int j = 0; j < GGML_MAX_SRC; j++) {\n            struct ggml_tensor * src = node->src[j];\n            if (src == NULL) {\n                break;\n            }\n            ggml_tallocr_t src_allocr = node_allocr(src);\n            if (src_allocr != node_allocr) {\n                int n_inputs = sched->splits[cur_split].n_inputs++;\n                GGML_ASSERT(n_inputs < GGML_MAX_SPLIT_INPUTS);\n                sched->splits[cur_split].inputs[n_inputs] = (struct ggml_tensor *)src;\n\n                // create copies\n                size_t id = hash_id(src);\n                if (sched->node_copies[id][cur_backend_id] == NULL) {\n                    struct ggml_tensor * tensor_copy = ggml_dup_tensor_layout(sched->ctx, src);\n                    sched->node_copies[id][cur_backend_id] = tensor_copy;\n                    node_allocr(tensor_copy) = cur_allocr;\n                    ggml_backend_t backend = ggml_tallocr_get_buffer(cur_allocr)->backend;\n                    ggml_format_name(tensor_copy, \"%s#%s\", ggml_backend_name(backend), src->name);\n                }\n                node->src[j] = sched->node_copies[id][cur_backend_id];\n            }\n        }\n    }\n    sched->splits[cur_split].i_end = graph->n_nodes;\n    sched->n_splits = cur_split + 1;\n\n    //fprintf(stderr, \"PASS 4 ASSIGNMENTS\\n\"); sched_print_assignments(sched, graph); fflush(stdout);\n\n#if 1\n    // sanity check: all sources should have the same backend as the node\n    for (int i = 0; i < graph->n_nodes; i++) {\n        struct ggml_tensor * node = graph->nodes[i];\n        ggml_tallocr_t node_allocr = node_allocr(node);\n        if (node_allocr == NULL) {\n            fprintf(stderr, \"!!!!!!! %s has no backend\\n\", node->name);\n        }\n        for (int j = 0; j < GGML_MAX_SRC; j++) {\n            struct ggml_tensor * src = node->src[j];\n            if (src == NULL) {\n                break;\n            }\n            ggml_tallocr_t src_allocr = node_allocr(src);\n            if (src_allocr != node_allocr /* && src_backend != NULL */) { // ignore nulls for now\n                fprintf(stderr, \"!!!! %s has backend %s, src %d (%s) has backend %s\\n\",\n                    node->name, node_allocr ? ggml_backend_name(ggml_tallocr_get_buffer(node_allocr)->backend) : \"NULL\",\n                    j, src->name, src_allocr ? ggml_backend_name(ggml_tallocr_get_buffer(src_allocr)->backend) : \"NULL\");\n            }\n        }\n    }\n#endif\n\n    // create copies of the graph for each split\n    // FIXME: avoid this copy, pass split inputs to ggml_gallocr_alloc_graph_n in some other way\n    struct ggml_cgraph * graph_copy = ggml_new_graph_custom(sched->ctx, graph->n_nodes + sched->n_splits*GGML_MAX_SPLIT_INPUTS, false);\n    for (int i = 0; i < sched->n_splits; i++) {\n        struct ggml_backend_sched_split * split = &sched->splits[i];\n        split->graph = ggml_graph_view(sched->ctx, graph, split->i_start, split->i_end);\n\n        // add inputs to the graph copy so that they are allocated by ggml-alloc at the start of the split\n        for (int j = 0; j < split->n_inputs; j++) {\n            struct ggml_tensor * input = split->inputs[j];\n            struct ggml_tensor * input_cpy = sched->node_copies[hash_id(input)][sched_allocr_prio(sched, split->tallocr)];\n            input_cpy->src[0] = input;\n            graph_copy->nodes[graph_copy->n_nodes++] = input_cpy;\n        }\n\n        for (int j = split->i_start; j < split->i_end; j++) {\n            graph_copy->nodes[graph_copy->n_nodes++] = graph->nodes[j];\n        }\n    }\n    sched->graph = graph_copy;\n}\n\nstatic void sched_alloc_splits(ggml_backend_sched_t sched) {\n    ggml_gallocr_alloc_graph_n(\n        sched->galloc,\n        sched->graph,\n        sched->hash_set,\n        sched->node_talloc);\n}\n\nstatic void sched_compute_splits(ggml_backend_sched_t sched) {\n    uint64_t copy_us[GGML_MAX_BACKENDS] = {0};\n    uint64_t compute_us[GGML_MAX_BACKENDS] = {0};\n\n    struct ggml_backend_sched_split * splits = sched->splits;\n\n    for (int i = 0; i < sched->n_splits; i++) {\n        struct ggml_backend_sched_split * split = &splits[i];\n        ggml_backend_t split_backend = ggml_tallocr_get_buffer(split->tallocr)->backend;\n        int split_backend_id = sched_backend_prio(sched, split_backend);\n\n        // copy the input tensors to the split backend\n        uint64_t copy_start_us = ggml_time_us();\n        for (int j = 0; j < split->n_inputs; j++) {\n            struct ggml_tensor * input_cpy = sched->node_copies[hash_id(split->inputs[j])][sched_backend_prio(sched, split_backend)];\n            if (split->inputs[j]->buffer == NULL) {\n                if (split->inputs[j]->view_src == NULL) {\n                    fprintf(stderr, \"input %s has no buffer and no view_src\\n\", split->inputs[j]->name);\n                    exit(1);\n                }\n                struct ggml_tensor * view = split->inputs[j];\n                view->backend = view->view_src->backend;\n                view->buffer  = view->view_src->buffer;\n                view->data    = (char *)view->view_src->data + view->view_offs;\n                ggml_backend_buffer_init_tensor(ggml_backend_sched_get_buffer(sched, view->buffer->backend), view);\n            }\n            if (input_cpy->buffer == NULL) {\n                fprintf(stderr, \"input_cpy %s has no buffer\\n\", input_cpy->name);\n                exit(1);\n            }\n            GGML_ASSERT(split->inputs[j]->buffer->backend != input_cpy->buffer->backend);\n            GGML_ASSERT(input_cpy->buffer->backend == split_backend);\n            ggml_backend_tensor_copy(split->inputs[j], input_cpy);\n        }\n        // ggml_backend_synchronize(split_backend);\n        int64_t copy_end_us = ggml_time_us();\n        copy_us[split_backend_id] += copy_end_us - copy_start_us;\n\n#if 0\n        char split_filename[GGML_MAX_NAME];\n        snprintf(split_filename, GGML_MAX_NAME, \"split_%i_%s.dot\", i, ggml_backend_name(split_backend));\n        ggml_graph_dump_dot(split->graph, NULL, split_filename);\n#endif\n\n        uint64_t compute_start_us = ggml_time_us();\n        ggml_backend_graph_compute(split_backend, split->graph);\n        // ggml_backend_synchronize(split_backend);\n        uint64_t compute_end_us = ggml_time_us();\n        compute_us[split_backend_id] += compute_end_us - compute_start_us;\n    }\n\n#if 0\n    // per-backend timings\n    fprintf(stderr, \"sched_compute_splits times (%d splits):\\n\", sched->n_splits);\n    for (int i = 0; i < sched->n_backends; i++) {\n        if (copy_us[i] > 0 || compute_us[i] > 0) {\n            fprintf(stderr, \"\\t%5.5s: %lu us copy, %lu us compute\\n\", ggml_backend_name(sched->backends[i]), copy_us[i], compute_us[i]);\n        }\n    }\n#endif\n}\n\nstatic void sched_reset(ggml_backend_sched_t sched) {\n    for (int i = 0; i < sched->n_backends; i++) {\n        ggml_tallocr_reset(sched->tallocs[i]);\n    }\n}\n\nggml_backend_sched_t ggml_backend_sched_new(ggml_backend_t * backends, int n_backends) {\n    GGML_ASSERT(n_backends <= GGML_MAX_BACKENDS);\n\n    struct ggml_backend_sched * sched = malloc(sizeof(struct ggml_backend_sched));\n    memset(sched, 0, sizeof(struct ggml_backend_sched));\n\n    fprintf(stderr, \"ggml_backend_sched size: %lu KB\\n\", sizeof(struct ggml_backend_sched)/1024);\n\n    sched->n_backends = n_backends;\n    for (int i = 0; i < n_backends; i++) {\n        sched->backends[i] = backends[i];\n    }\n\n    sched->galloc = ggml_gallocr_new();\n\n    // init measure allocs for each backend\n    for (int i = 0; i < n_backends; i++) {\n        sched->tallocs[i] = ggml_tallocr_new_measure_from_backend(backends[i]);\n    }\n\n    return sched;\n}\n\nvoid ggml_backend_sched_free(ggml_backend_sched_t sched) {\n    if (sched == NULL) {\n        return;\n    }\n    for (int i = 0; i < sched->n_backends; i++) {\n        ggml_tallocr_free(sched->tallocs[i]);\n    }\n    ggml_gallocr_free(sched->galloc);\n    free(sched->hash_set.keys);\n    free(sched->node_talloc);\n    free(sched->node_copies);\n    free(sched);\n}\n\nvoid ggml_backend_sched_init_measure(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph) {\n    // initialize hash tables\n    size_t hash_size = measure_graph->visited_hash_table.size + GGML_MAX_SPLITS*GGML_MAX_SPLIT_INPUTS;\n    sched->hash_set.size = hash_size;\n    sched->hash_set.keys = malloc(sizeof(sched->hash_set.keys[0]) * hash_size);\n    sched->node_talloc   = malloc(sizeof(sched->node_talloc[0])   * hash_size);\n    sched->node_copies   = malloc(sizeof(sched->node_copies[0])   * hash_size);\n\n    sched_split_graph(sched, measure_graph);\n    sched_alloc_splits(sched);\n\n    // allocate buffers and reset allocators\n    for (int i = 0; i < sched->n_backends; i++) {\n        size_t size = ggml_tallocr_max_size(sched->tallocs[i]);\n        ggml_tallocr_free(sched->tallocs[i]);\n        sched->tallocs[i] = ggml_tallocr_new_from_backend(sched->backends[i], size);\n    }\n\n    sched_reset(sched);\n}\n\nvoid ggml_backend_sched_graph_compute(ggml_backend_sched_t sched, struct ggml_cgraph * graph) {\n    GGML_ASSERT(sched->hash_set.size >= graph->visited_hash_table.size + GGML_MAX_SPLITS*GGML_MAX_SPLIT_INPUTS);\n\n    sched_split_graph(sched, graph);\n    sched_alloc_splits(sched);\n    sched_compute_splits(sched);\n    sched_reset(sched);\n}\n\nggml_tallocr_t ggml_backend_sched_get_tallocr(ggml_backend_sched_t sched, ggml_backend_t backend) {\n    int backend_index = sched_backend_prio(sched, backend);\n    return sched->tallocs[backend_index];\n}\n\nggml_backend_buffer_t ggml_backend_sched_get_buffer(ggml_backend_sched_t sched, ggml_backend_t backend) {\n    int backend_index = sched_backend_prio(sched, backend);\n    return ggml_tallocr_get_buffer(sched->tallocs[backend_index]);\n}\n\nvoid ggml_backend_sched_set_node_backend(ggml_backend_sched_t sched, struct ggml_tensor * node, ggml_backend_t backend) {\n    int backend_index = sched_backend_prio(sched, backend);\n    GGML_ASSERT(backend_index >= 0 && backend_index < sched->n_backends);\n    node_allocr(node) = sched->tallocs[backend_index];\n}\n"
        },
        {
          "name": "ggml-backend.h",
          "type": "blob",
          "size": 5.755859375,
          "content": "#pragma once\n\n#include \"ggml.h\"\n#include \"ggml-alloc.h\"\n\n#ifdef  __cplusplus\nextern \"C\" {\n#endif\n\n    //\n    // Backend buffer\n    //\n\n    struct ggml_backend_buffer;\n    typedef struct ggml_backend_buffer * ggml_backend_buffer_t;\n\n    // backend buffer functions\n    GGML_API void   ggml_backend_buffer_free          (ggml_backend_buffer_t buffer);\n    GGML_API size_t ggml_backend_buffer_get_alignment (ggml_backend_buffer_t buffer);\n    GGML_API void * ggml_backend_buffer_get_base      (ggml_backend_buffer_t buffer);\n    GGML_API size_t ggml_backend_buffer_get_size      (ggml_backend_buffer_t buffer);\n    GGML_API size_t ggml_backend_buffer_get_alloc_size(ggml_backend_buffer_t buffer, struct ggml_tensor * tensor);\n    GGML_API void   ggml_backend_buffer_init_tensor   (ggml_backend_buffer_t buffer, struct ggml_tensor * tensor);\n    GGML_API void   ggml_backend_buffer_free_tensor   (ggml_backend_buffer_t buffer, struct ggml_tensor * tensor);\n\n    //\n    // Backend\n    //\n\n    struct ggml_backend;\n    typedef struct ggml_backend * ggml_backend_t;\n    typedef void * ggml_backend_graph_plan_t;\n\n    GGML_API ggml_backend_t ggml_get_backend(const struct ggml_tensor * tensor);\n\n    GGML_API const char * ggml_backend_name(ggml_backend_t backend);\n    GGML_API void         ggml_backend_free(ggml_backend_t backend);\n\n    GGML_API ggml_backend_buffer_t ggml_backend_alloc_buffer(ggml_backend_t backend, size_t size);\n\n    GGML_API size_t ggml_backend_get_alignment(ggml_backend_t backend);\n\n    GGML_API void ggml_backend_tensor_set_async(      struct ggml_tensor * tensor, const void * data, size_t offset, size_t size);\n    GGML_API void ggml_backend_tensor_get_async(const struct ggml_tensor * tensor,       void * data, size_t offset, size_t size);\n\n    GGML_API void ggml_backend_tensor_set(      struct ggml_tensor * tensor, const void * data, size_t offset, size_t size);\n    GGML_API void ggml_backend_tensor_get(const struct ggml_tensor * tensor,       void * data, size_t offset, size_t size);\n\n    GGML_API void ggml_backend_synchronize(ggml_backend_t backend);\n\n    GGML_API ggml_backend_graph_plan_t ggml_backend_graph_plan_create (ggml_backend_t backend, struct ggml_cgraph * cgraph);\n\n    GGML_API void ggml_backend_graph_plan_free   (ggml_backend_t backend, ggml_backend_graph_plan_t plan);\n    GGML_API void ggml_backend_graph_plan_compute(ggml_backend_t backend, ggml_backend_graph_plan_t plan);\n    GGML_API void ggml_backend_graph_compute     (ggml_backend_t backend, struct ggml_cgraph * cgraph);\n    GGML_API bool ggml_backend_supports_op       (ggml_backend_t backend, const struct ggml_tensor * op);\n\n    // tensor copy between different backends\n    GGML_API void ggml_backend_tensor_copy(struct ggml_tensor * src, struct ggml_tensor * dst);\n\n    //\n    // CPU backend\n    //\n\n    GGML_API ggml_backend_t ggml_backend_cpu_init(void);\n\n    GGML_API bool ggml_backend_is_cpu(ggml_backend_t backend);\n    GGML_API void ggml_backend_cpu_set_n_threads(ggml_backend_t backend_cpu, int n_threads);\n\n    // Create a backend buffer from an existing pointer\n    GGML_API ggml_backend_buffer_t ggml_backend_cpu_buffer_from_ptr(ggml_backend_t backend_cpu, void * ptr, size_t size);\n\n\n    //\n    // Backend scheduler\n    //\n\n    // The backend scheduler allows for multiple backends to be used together\n    // Handles compute buffer allocation, assignment of tensors to backends, and copying of tensors between backends\n    // The backends are selected based on:\n    // - the backend that supports the operation\n    // - the location of the pre-allocated tensors (e.g. the weights)\n    /*\n      Example usage:\n\n        sched = ggml_backend_sched_new({backend_gpu, backend_gpu2, backend_cpu}, num_backends);\n        // sched is initialized with measure allocators and cannot be used until allocated with a measure graph\n\n        // initialize buffers from a measure graph\n        measure_graph = build_graph(sched); // use the allocr to allocate inputs as needed\n\n        // in build_graph:\n        build_graph(...) {\n            // allocating tensors in a specific backend (optional, recommended: pre-allocate inputs in a different buffer)\n            alloc_cpu = ggml_backend_sched_get_allocr(sched, backend_cpu);\n            ggml_allocr_alloc(alloc_cpu, tensor);\n\n            // manually assigning nodes to a backend (optional, shouldn't be needed in most cases)\n            struct ggml_tensor * node = ggml_mul_mat(ctx, ...);\n            ggml_backend_sched_set_node_backend(sched, node, backend_gpu);\n        }\n\n        // allocate backend buffers from measure graph\n        ggml_backend_sched_init_measure(sched, measure_graph);\n\n        // the scheduler is now ready to compute graphs\n\n        // compute\n        graph = build_graph(sched);\n        ggml_backend_sched_graph_compute(sched, graph);\n    */\n\n    struct ggml_backend_sched;\n    typedef struct ggml_backend_sched * ggml_backend_sched_t;\n\n    // Initialize a backend scheduler\n    GGML_API ggml_backend_sched_t ggml_backend_sched_new(ggml_backend_t * backends, int n_backends);\n\n    GGML_API void ggml_backend_sched_free(ggml_backend_sched_t sched);\n\n    // Initialize backend buffers from a measure graph\n    GGML_API void ggml_backend_sched_init_measure(ggml_backend_sched_t sched, struct ggml_cgraph * measure_graph);\n\n    GGML_API ggml_tallocr_t        ggml_backend_sched_get_tallocr(ggml_backend_sched_t sched, ggml_backend_t backend);\n    GGML_API ggml_backend_buffer_t ggml_backend_sched_get_buffer (ggml_backend_sched_t sched, ggml_backend_t backend);\n\n    GGML_API void ggml_backend_sched_set_node_backend(ggml_backend_sched_t sched, struct ggml_tensor * node, ggml_backend_t backend);\n\n    // Allocate a graph on the backend scheduler\n    GGML_API void ggml_backend_sched_graph_compute(\n            ggml_backend_sched_t sched,\n            struct ggml_cgraph * graph);\n\n#ifdef  __cplusplus\n}\n#endif\n"
        },
        {
          "name": "ggml-cuda.cu",
          "type": "blob",
          "size": 358.0927734375,
          "content": "#include <algorithm>\n#include <cstddef>\n#include <cstdint>\n#include <limits>\n#include <stdint.h>\n#include <stdio.h>\n#include <atomic>\n#include <assert.h>\n\n#if defined(GGML_USE_HIPBLAS)\n#include <hip/hip_runtime.h>\n#include <hipblas/hipblas.h>\n#include <hip/hip_fp16.h>\n#ifdef __HIP_PLATFORM_AMD__\n// for rocblas_initialize()\n#include \"rocblas/rocblas.h\"\n#endif // __HIP_PLATFORM_AMD__\n#define CUBLAS_COMPUTE_16F HIPBLAS_R_16F\n#define CUBLAS_COMPUTE_32F HIPBLAS_R_32F\n#define CUBLAS_COMPUTE_32F_FAST_16F HIPBLAS_R_32F\n#define CUBLAS_GEMM_DEFAULT HIPBLAS_GEMM_DEFAULT\n#define CUBLAS_GEMM_DEFAULT_TENSOR_OP HIPBLAS_GEMM_DEFAULT\n#define CUBLAS_OP_N HIPBLAS_OP_N\n#define CUBLAS_OP_T HIPBLAS_OP_T\n#define CUBLAS_STATUS_SUCCESS HIPBLAS_STATUS_SUCCESS\n#define CUBLAS_TF32_TENSOR_OP_MATH 0\n#define CUDA_R_16F  HIPBLAS_R_16F\n#define CUDA_R_32F  HIPBLAS_R_32F\n#define __shfl_xor_sync(mask, var, laneMask, width) __shfl_xor(var, laneMask, width)\n#define cublasCreate hipblasCreate\n#define cublasGemmEx hipblasGemmEx\n#define cublasGemmBatchedEx hipblasGemmBatchedEx\n#define cublasGemmStridedBatchedEx hipblasGemmStridedBatchedEx\n#define cublasHandle_t hipblasHandle_t\n#define cublasSetMathMode(handle, mode) CUBLAS_STATUS_SUCCESS\n#define cublasSetStream hipblasSetStream\n#define cublasSgemm hipblasSgemm\n#define cublasStatus_t hipblasStatus_t\n#define cudaDeviceCanAccessPeer hipDeviceCanAccessPeer\n#define cudaDeviceDisablePeerAccess hipDeviceDisablePeerAccess\n#define cudaDeviceEnablePeerAccess hipDeviceEnablePeerAccess\n#define cudaDeviceProp hipDeviceProp_t\n#define cudaDeviceSynchronize hipDeviceSynchronize\n#define cudaError_t hipError_t\n#define cudaEventCreateWithFlags hipEventCreateWithFlags\n#define cudaEventDisableTiming hipEventDisableTiming\n#define cudaEventRecord hipEventRecord\n#define cudaEvent_t hipEvent_t\n#define cudaEventDestroy hipEventDestroy\n#define cudaFree hipFree\n#define cudaFreeHost hipHostFree\n#define cudaGetDevice hipGetDevice\n#define cudaGetDeviceCount hipGetDeviceCount\n#define cudaGetDeviceProperties hipGetDeviceProperties\n#define cudaGetErrorString hipGetErrorString\n#define cudaGetLastError hipGetLastError\n#define cudaMalloc hipMalloc\n#define cudaMallocHost(ptr, size) hipHostMalloc(ptr, size, hipHostMallocDefault)\n#define cudaMemcpy hipMemcpy\n#define cudaMemcpy2DAsync hipMemcpy2DAsync\n#define cudaMemcpyAsync hipMemcpyAsync\n#define cudaMemcpyDeviceToDevice hipMemcpyDeviceToDevice\n#define cudaMemcpyDeviceToHost hipMemcpyDeviceToHost\n#define cudaMemcpyHostToDevice hipMemcpyHostToDevice\n#define cudaMemcpyKind hipMemcpyKind\n#define cudaMemset hipMemset\n#define cudaMemsetAsync hipMemsetAsync\n#define cudaOccupancyMaxPotentialBlockSize hipOccupancyMaxPotentialBlockSize\n#define cudaSetDevice hipSetDevice\n#define cudaStreamCreateWithFlags hipStreamCreateWithFlags\n#define cudaStreamNonBlocking hipStreamNonBlocking\n#define cudaStreamSynchronize hipStreamSynchronize\n#define cudaStreamWaitEvent(stream, event, flags) hipStreamWaitEvent(stream, event, flags)\n#define cudaStream_t hipStream_t\n#define cudaSuccess hipSuccess\n// fix cuda function not defined for rocm\n#define cudaMemGetInfo hipMemGetInfo\n#define cudaMemcpyToSymbol hipMemcpyToSymbol\n#else\n#include <cuda_runtime.h>\n#include <cublas_v2.h>\n#include <cuda_fp16.h>\n#endif // defined(GGML_USE_HIPBLAS)\n\n#include \"ggml-cuda.h\"\n#include \"ggml.h\"\n#include \"ggml-backend-impl.h\"\n\n#define MIN_CC_DP4A   610 // minimum compute capability for __dp4a, an intrinsic for byte-wise dot products\n#define CC_VOLTA      700\n#define CC_OFFSET_AMD 1000000\n#define CC_RDNA2      (CC_OFFSET_AMD + 1030)\n\n#define GGML_CUDA_MAX_NODES 8192\n\n// \n#define AXPY_BLOCK_Y 1\n#define AXPY_BLOCK_X 512\n#define AXPY_BLOCK_Z 256\n\n// define this if you want to always fallback to MMQ kernels and not use cuBLAS for matrix multiplication\n// on modern hardware, using cuBLAS is recommended as it utilizes F16 tensor cores which are very performant\n// for large computational tasks. the drawback is that this requires some extra amount of VRAM:\n// -  7B quantum model: +100-200 MB\n// - 13B quantum model: +200-400 MB\n//\n//#define GGML_CUDA_FORCE_MMQ\n\n// TODO: improve this to be correct for more hardware\n//       for example, currently fails for GeForce GTX 1660 which is TURING arch (> VOLTA) but does not have tensor cores\n//       probably other such cases, and not sure what happens on AMD hardware\n#if !defined(GGML_CUDA_FORCE_MMQ)\n#define CUDA_USE_TENSOR_CORES\n#endif\n\n// max batch size to use MMQ kernels when tensor cores are available\n#define MMQ_MAX_BATCH_SIZE 32\n\n__constant__ float dev_sparse_threshold;\n\n#if defined(GGML_USE_HIPBLAS)\n#define __CUDA_ARCH__ 1300\n\n#if defined(__gfx1100__) || defined(__gfx1101__) || defined(__gfx1102__) || defined(__gfx1103__) || \\\n    defined(__gfx1150__) || defined(__gfx1151__)\n#define RDNA3\n#endif\n\n#if defined(__gfx1030__) || defined(__gfx1031__) || defined(__gfx1032__) || defined(__gfx1033__) || \\\n    defined(__gfx1034__) || defined(__gfx1035__) || defined(__gfx1036__) || defined(__gfx1037__)\n#define RDNA2\n#endif\n\n#ifndef __has_builtin\n    #define __has_builtin(x) 0\n#endif\n\ntypedef int8_t int8x4_t __attribute__((ext_vector_type(4)));\nstatic __device__ __forceinline__ int __vsubss4(const int a, const int b) {\n    const int8x4_t va = reinterpret_cast<const int8x4_t&>(a);\n    const int8x4_t vb = reinterpret_cast<const int8x4_t&>(b);\n#if __has_builtin(__builtin_elementwise_sub_sat)\n    const int8x4_t c = __builtin_elementwise_sub_sat(va, vb);\n    return reinterpret_cast<const int&>(c);\n#else\n    int8x4_t c;\n    int16_t tmp;\n#pragma unroll\n    for (int i = 0; i < 4; i++) {\n        tmp = va[i] - vb[i];\n        if(tmp > std::numeric_limits<int8_t>::max()) tmp = std::numeric_limits<int8_t>::max();\n        if(tmp < std::numeric_limits<int8_t>::min()) tmp = std::numeric_limits<int8_t>::min();\n        c[i] = tmp;\n    }\n    return reinterpret_cast<int&>(c);\n#endif // __has_builtin(__builtin_elementwise_sub_sat)\n}\n\nstatic __device__ __forceinline__ int __dp4a(const int a, const int b, int c) {\n#if defined(__gfx906__) || defined(__gfx908__) || defined(__gfx90a__) || defined(__gfx1030__)\n    c = __builtin_amdgcn_sdot4(a, b, c, false);\n#elif defined(__gfx1100__)\n    c = __builtin_amdgcn_sudot4( true, a, true, b, c, false);\n#elif defined(__gfx1010__) || defined(__gfx900__)\n    int tmp1;\n    int tmp2;\n    asm(\"\\n \\\n        v_mul_i32_i24 %1, sext(%3), sext(%4) dst_sel:DWORD dst_unused:UNUSED_PAD src0_sel:BYTE_0 src1_sel:BYTE_0 \\n \\\n        v_mul_i32_i24 %2, sext(%3), sext(%4) dst_sel:DWORD dst_unused:UNUSED_PAD src0_sel:BYTE_1 src1_sel:BYTE_1 \\n \\\n        v_add3_u32 %0, %1, %2, %0 \\n \\\n        v_mul_i32_i24 %1, sext(%3), sext(%4) dst_sel:DWORD dst_unused:UNUSED_PAD src0_sel:BYTE_2 src1_sel:BYTE_2 \\n \\\n        v_mul_i32_i24 %2, sext(%3), sext(%4) dst_sel:DWORD dst_unused:UNUSED_PAD src0_sel:BYTE_3 src1_sel:BYTE_3 \\n \\\n        v_add3_u32 %0, %1, %2, %0 \\n \\\n        \"\n        : \"+v\"(c), \"=&v\"(tmp1), \"=&v\"(tmp2)\n        : \"v\"(a), \"v\"(b)\n    );\n#else\n    const int8x4_t va = reinterpret_cast<const int8x4_t&>(a);\n    const int8x4_t vb = reinterpret_cast<const int8x4_t&>(b);\n    c += va[0] * vb[0] + va[1] * vb[1] + va[2] * vb[2] + va[3] * vb[3];\n#endif\n    return c;\n}\n#endif // defined(GGML_USE_HIPBLAS)\n\n#if defined(_MSC_VER)\n#pragma warning(disable: 4244 4267) // possible loss of data\n#endif\n\nstatic_assert(sizeof(half) == sizeof(ggml_fp16_t), \"wrong fp16 size\");\n\n#define CUDA_CHECK(err)                                                                 \\\n    do {                                                                                \\\n        cudaError_t err_ = (err);                                                       \\\n        if (err_ != cudaSuccess) {                                                      \\\n            int id;                                                                     \\\n            cudaGetDevice(&id);                                                         \\\n            fprintf(stderr, \"\\nCUDA error %d at %s:%d: %s\\n\", err_, __FILE__, __LINE__, \\\n                cudaGetErrorString(err_));                                              \\\n            fprintf(stderr, \"current device: %d\\n\", id);                                \\\n            exit(1);                                                                    \\\n        }                                                                               \\\n    } while (0)\n\n#if CUDART_VERSION >= 12000\n#define CUBLAS_CHECK(err)                                                               \\\n    do {                                                                                \\\n        cublasStatus_t err_ = (err);                                                    \\\n        if (err_ != CUBLAS_STATUS_SUCCESS) {                                            \\\n            int id;                                                                     \\\n            cudaGetDevice(&id);                                                         \\\n            fprintf(stderr, \"\\ncuBLAS error %d at %s:%d: %s\\n\",                         \\\n                    err_, __FILE__, __LINE__, cublasGetStatusString(err_));             \\\n            fprintf(stderr, \"current device: %d\\n\", id);                                \\\n            exit(1);                                                                    \\\n        }                                                                               \\\n    } while (0)\n#else\n#define CUBLAS_CHECK(err)                                                               \\\n    do {                                                                                \\\n        cublasStatus_t err_ = (err);                                                    \\\n        if (err_ != CUBLAS_STATUS_SUCCESS) {                                            \\\n            int id;                                                                     \\\n            cudaGetDevice(&id);                                                         \\\n            fprintf(stderr, \"\\ncuBLAS error %d at %s:%d\\n\", err_, __FILE__, __LINE__);  \\\n            fprintf(stderr, \"current device: %d\\n\", id);                                \\\n            exit(1);                                                                    \\\n        }                                                                               \\\n    } while (0)\n#endif // CUDART_VERSION >= 11\n\n#if CUDART_VERSION >= 11100\n#define GGML_CUDA_ASSUME(x) __builtin_assume(x)\n#else\n#define GGML_CUDA_ASSUME(x)\n#endif // CUDART_VERSION >= 11100\n\n#ifdef GGML_CUDA_F16\ntypedef half dfloat; // dequantize float\ntypedef half2 dfloat2;\n#else\ntypedef float dfloat; // dequantize float\ntypedef float2 dfloat2;\n#endif //GGML_CUDA_F16\n\nstatic __device__ __forceinline__ int get_int_from_int8(const int8_t * x8, const int & i32) {\n    const uint16_t * x16 = (uint16_t *) (x8 + sizeof(int) * i32); // assume at least 2 byte alignment\n\n    int x32 = 0;\n    x32 |= x16[0] <<  0;\n    x32 |= x16[1] << 16;\n\n    return x32;\n}\n\nstatic __device__ __forceinline__ int get_int_from_uint8(const uint8_t * x8, const int & i32) {\n    const uint16_t * x16 = (uint16_t *) (x8 + sizeof(int) * i32); // assume at least 2 byte alignment\n\n    int x32 = 0;\n    x32 |= x16[0] <<  0;\n    x32 |= x16[1] << 16;\n\n    return x32;\n}\n\nstatic __device__ __forceinline__ int get_int_from_int8_aligned(const int8_t * x8, const int & i32) {\n    return *((int *) (x8 + sizeof(int) * i32)); // assume at least 4 byte alignment\n}\n\nstatic __device__ __forceinline__ int get_int_from_uint8_aligned(const uint8_t * x8, const int & i32) {\n    return *((int *) (x8 + sizeof(int) * i32)); // assume at least 4 byte alignment\n}\n\ntemplate<typename T>\nusing to_t_cuda_t = void (*)(const void * __restrict__ x, T * __restrict__ y, int k, cudaStream_t stream);\ntypedef to_t_cuda_t<float> to_fp32_cuda_t;\ntypedef to_t_cuda_t<half> to_fp16_cuda_t;\n\ntypedef void (*dequantize_kernel_t)(const void * vx, const int ib, const int iqs, dfloat2 & v);\ntypedef void (*dot_kernel_k_t)(const void * __restrict__ vx, const int ib, const int iqs, const float * __restrict__ y, float & v);\ntypedef void (*cpy_kernel_t)(const char * cx, char * cdst);\ntypedef void (*ggml_cuda_func_t)(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst);\ntypedef void (*ggml_cuda_op_mul_mat_t)(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream);\ntypedef void (*ggml_cuda_op_flatten_t)(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream);\n\n// QK = number of values after dequantization\n// QR = QK / number of values before dequantization\n// QI = number of 32 bit integers before dequantization\n\n#define QK4_0 32\n#define QR4_0 2\n#define QI4_0 (QK4_0 / (4 * QR4_0))\ntypedef struct {\n    half    d;              // delta\n    uint8_t qs[QK4_0 / 2];  // nibbles / quants\n} block_q4_0;\nstatic_assert(sizeof(block_q4_0) == sizeof(ggml_fp16_t) + QK4_0 / 2, \"wrong q4_0 block size/padding\");\n\n#define QK4_1 32\n#define QR4_1 2\n#define QI4_1 (QK4_1 / (4 * QR4_1))\ntypedef struct {\n    half2   dm;             // dm.x = delta, dm.y = min\n    uint8_t qs[QK4_1 / 2];  // nibbles / quants\n} block_q4_1;\nstatic_assert(sizeof(block_q4_1) == sizeof(ggml_fp16_t) * 2 + QK4_1 / 2, \"wrong q4_1 block size/padding\");\n\n#define QK5_0 32\n#define QR5_0 2\n#define QI5_0 (QK5_0 / (4 * QR5_0))\ntypedef struct {\n    half d;                 // delta\n    uint8_t qh[4];          // 5-th bit of quants\n    uint8_t qs[QK5_0 / 2];  // nibbles / quants\n} block_q5_0;\nstatic_assert(sizeof(block_q5_0) == sizeof(ggml_fp16_t) + sizeof(uint32_t) + QK5_0 / 2, \"wrong q5_0 block size/padding\");\n\n#define QK5_1 32\n#define QR5_1 2\n#define QI5_1 (QK5_1 / (4 * QR5_1))\ntypedef struct {\n    half2 dm;               // dm.x = delta, dm.y = min\n    uint8_t qh[4];          // 5-th bit of quants\n    uint8_t qs[QK5_1 / 2];  // nibbles / quants\n} block_q5_1;\nstatic_assert(sizeof(block_q5_1) == 2 * sizeof(ggml_fp16_t) + sizeof(uint32_t) + QK5_1 / 2, \"wrong q5_1 block size/padding\");\n\n#define QK8_0 32\n#define QR8_0 1\n#define QI8_0 (QK8_0 / (4 * QR8_0))\ntypedef struct {\n    half    d;              // delta\n    int8_t  qs[QK8_0];      // quants\n} block_q8_0;\nstatic_assert(sizeof(block_q8_0) == sizeof(ggml_fp16_t) + QK8_0, \"wrong q8_0 block size/padding\");\n\n#define QK8_1 32\n#define QR8_1 1\n#define QI8_1 (QK8_1 / (4 * QR8_1))\ntypedef struct {\n    half2   ds;             // ds.x = delta, ds.y = sum\n    int8_t  qs[QK8_0];      // quants\n} block_q8_1;\nstatic_assert(sizeof(block_q8_1) == 2*sizeof(ggml_fp16_t) + QK8_0, \"wrong q8_1 block size/padding\");\n\ntypedef float (*vec_dot_q_cuda_t)(const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs);\ntypedef void (*allocate_tiles_cuda_t)(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc);\ntypedef void (*load_tiles_cuda_t)(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row);\ntypedef float (*vec_dot_q_mul_mat_cuda_t)(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ms, const int & i, const int & j, const int & k);\n\n//================================= k-quants\n\n#ifdef GGML_QKK_64\n#define QK_K 64\n#define K_SCALE_SIZE 4\n#else\n#define QK_K 256\n#define K_SCALE_SIZE 12\n#endif\n\n#define QR2_K 4\n#define QI2_K (QK_K / (4*QR2_K))\ntypedef struct {\n    uint8_t scales[QK_K/16]; // scales and mins, quantized with 4 bits\n    uint8_t qs[QK_K/4];      // quants\n    half2 dm;                // super-block scale for quantized scales/mins\n} block_q2_K;\nstatic_assert(sizeof(block_q2_K) == 2*sizeof(ggml_fp16_t) + QK_K/16 + QK_K/4, \"wrong q2_K block size/padding\");\n\n#define QR3_K 4\n#define QI3_K (QK_K / (4*QR3_K))\ntypedef struct {\n    uint8_t hmask[QK_K/8];     // quants - high bit\n    uint8_t qs[QK_K/4];        // quants - low 2 bits\n#ifdef GGML_QKK_64\n    uint8_t scales[2]; // scales, quantized with 8 bits\n#else\n    uint8_t scales[K_SCALE_SIZE]; // scales, quantized with 6 bits\n#endif\n    half d;             // super-block scale\n} block_q3_K;\n//static_assert(sizeof(block_q3_K) == sizeof(ggml_fp16_t) + QK_K / 4 + QK_K / 8 + K_SCALE_SIZE, \"wrong q3_K block size/padding\");\n\n#define QR4_K 2\n#define QI4_K (QK_K / (4*QR4_K))\n#ifdef GGML_QKK_64\ntypedef struct {\n    half    dm[2];             // super-block scales/mins\n    uint8_t scales[2];         // 4-bit block scales/mins\n    uint8_t qs[QK_K/2];        // 4--bit quants\n} block_q4_K;\nstatic_assert(sizeof(block_q4_K) == sizeof(half2) + QK_K/2 + 2, \"wrong q4_K block size/padding\");\n#else\ntypedef struct {\n    half2 dm;                  // super-block scale for quantized scales/mins\n    uint8_t scales[3*QK_K/64]; // scales, quantized with 6 bits\n    uint8_t qs[QK_K/2];        // 4--bit quants\n} block_q4_K;\nstatic_assert(sizeof(block_q4_K) == 2*sizeof(ggml_fp16_t) + 3*QK_K/64 + QK_K/2, \"wrong q4_K block size/padding\");\n#endif\n\n#define QR5_K 2\n#define QI5_K (QK_K / (4*QR5_K))\n#ifdef GGML_QKK_64\ntypedef struct {\n    half d;                  // super-block scale\n    int8_t scales[QK_K/16];  // block scales\n    uint8_t qh[QK_K/8];      // quants, high bit\n    uint8_t qs[QK_K/2];      // quants, low 4 bits\n} block_q5_K;\nstatic_assert(sizeof(block_q5_K) == sizeof(ggml_fp16_t) + QK_K/2 + QK_K/8 + QK_K/16, \"wrong q5_K block size/padding\");\n#else\ntypedef struct {\n    half2 dm;                     // super-block scale for quantized scales/mins\n    uint8_t scales[K_SCALE_SIZE]; // scales and mins, quantized with 6 bits\n    uint8_t qh[QK_K/8];           // quants, high bit\n    uint8_t qs[QK_K/2];           // quants, low 4 bits\n} block_q5_K;\nstatic_assert(sizeof(block_q5_K) == 2*sizeof(ggml_fp16_t) + K_SCALE_SIZE + QK_K/2 + QK_K/8, \"wrong q5_K block size/padding\");\n#endif\n\n#define QR6_K 2\n#define QI6_K (QK_K / (4*QR6_K))\ntypedef struct {\n    uint8_t ql[QK_K/2];   // quants, lower 4 bits\n    uint8_t qh[QK_K/4];   // quants, upper 2 bits\n    int8_t  scales[QK_K/16]; // scales\n    half    d;         // delta\n} block_q6_K;\nstatic_assert(sizeof(block_q6_K) == sizeof(ggml_fp16_t) + 13*QK_K/16, \"wrong q6_K block size/padding\");\n\n#define WARP_SIZE 32\n#define MATRIX_ROW_PADDING 512 // last row of quant. matrices is a multiple of this to avoid out-of-bounds memory accesses\n\n#define CUDA_ADD_BLOCK_SIZE 256\n#define CUDA_MUL_BLOCK_SIZE 256\n#define CUDA_GELU_BLOCK_SIZE 256\n#define CUDA_SILU_BLOCK_SIZE 256\n#define CUDA_RELU_BLOCK_SIZE 256\n#define CUDA_SQR_BLOCK_SIZE 256\n#define CUDA_CPY_BLOCK_SIZE 32\n#define CUDA_SCALE_BLOCK_SIZE 256\n#define CUDA_CLAMP_BLOCK_SIZE 256\n#define CUDA_ROPE_BLOCK_SIZE 256\n#define CUDA_ALIBI_BLOCK_SIZE 32\n#define CUDA_DIAG_MASK_INF_BLOCK_SIZE 32\n#define CUDA_QUANTIZE_BLOCK_SIZE 256\n#define CUDA_DEQUANTIZE_BLOCK_SIZE 256\n#define CUDA_GET_ROWS_BLOCK_SIZE 256\n\n// dmmv = dequantize_mul_mat_vec\n#ifndef GGML_CUDA_DMMV_X\n#define GGML_CUDA_DMMV_X 32\n#endif\n#ifndef GGML_CUDA_MMV_Y\n#define GGML_CUDA_MMV_Y 1\n#endif\n\n#ifndef K_QUANTS_PER_ITERATION\n#define K_QUANTS_PER_ITERATION 2\n#else\nstatic_assert(K_QUANTS_PER_ITERATION == 1 || K_QUANTS_PER_ITERATION == 2, \"K_QUANTS_PER_ITERATION must be 1 or 2\");\n#endif\n\n#ifndef GGML_CUDA_PEER_MAX_BATCH_SIZE\n#define GGML_CUDA_PEER_MAX_BATCH_SIZE 128\n#endif // GGML_CUDA_PEER_MAX_BATCH_SIZE\n\n#define MUL_MAT_SRC1_COL_STRIDE 128\n\n#define MAX_STREAMS 8\nstatic cudaStream_t g_cudaStreams[GGML_CUDA_MAX_DEVICES][MAX_STREAMS] = { nullptr };\n\nstruct ggml_tensor_extra_gpu {\n    void * data_device[GGML_CUDA_MAX_DEVICES]; // 1 pointer for each device for split tensors\n    cudaEvent_t events[GGML_CUDA_MAX_DEVICES][MAX_STREAMS]; // events for synchronizing multiple GPUs\n};\n\n// this is faster on Windows\n// probably because the Windows CUDA libraries forget to make this check before invoking the drivers\ninline cudaError_t ggml_cuda_set_device(const int device) {\n    int current_device;\n    CUDA_CHECK(cudaGetDevice(&current_device));\n\n    if (device == current_device) {\n        return cudaSuccess;\n    }\n\n    return cudaSetDevice(device);\n}\n\nstatic int g_device_count = -1;\nstatic int g_main_device = 0;\nstatic int g_compute_capabilities[GGML_CUDA_MAX_DEVICES];\nstatic float g_tensor_split[GGML_CUDA_MAX_DEVICES] = {0};\n\nstatic void * g_scratch_buffer = nullptr;\nstatic size_t g_scratch_size = 0; // disabled by default\nstatic size_t g_scratch_offset = 0;\n\nstatic cublasHandle_t g_cublas_handles[GGML_CUDA_MAX_DEVICES] = {nullptr};\n\nstatic __global__ void add_f32(const float * x, const float * y, float * dst, const int kx, const int ky) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= kx) {\n        return;\n    }\n    dst[i] = x[i] + y[i%ky];\n}\n\nstatic __global__ void add_f32_idx(const float * x, const float * y, float * dst, float* idx, const int kx, const int ky) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= kx) {\n        return;\n    }\n    if (idx[i] <= -0.0f) {\n        dst[i] = 0;\n        return;\n    }\n    dst[i] = x[i] + y[i%ky];\n}\n\nstatic __global__ void add_f16_f32_f16(const half * x, const float * y, half * dst, const int k) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n    dst[i] = __hadd(x[i], __float2half(y[i]));\n}\n\nstatic __global__ void add_f16_f32_f32(const half * x, const float * y, float * dst, const int k) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n    dst[i] = __half2float(x[i]) + y[i];\n}\n\nstatic __global__ void mul_f32(const float * x, const float * y, float * dst, const int kx, const int ky) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= kx) {\n        return;\n    }\n    dst[i] = x[i] * y[i%ky];\n}\n\nstatic __global__ void gelu_f32(const float * x, float * dst, const int k) {\n    const float GELU_COEF_A    = 0.044715f;\n    const float SQRT_2_OVER_PI = 0.79788456080286535587989211986876f;\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n\n    float xi = x[i];\n    dst[i] = 0.5f*xi*(1.0f + tanhf(SQRT_2_OVER_PI*xi*(1.0f + GELU_COEF_A*xi*xi)));\n}\n\nstatic __global__ void silu_f32(const float * x, float * dst, const int k) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n    dst[i] = x[i] / (1.0f + expf(-x[i]));\n}\n\nstatic __global__ void sigmoid_f32(const float * x, float * dst, const int k) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n    dst[i] = 1 / (1.0f + expf(-x[i]));\n}\n\nstatic __global__ void relu_f32(const float * x, float * dst, const int k) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n    dst[i] = fmaxf(x[i], 0);\n}\n\nstatic __global__ void sqr_f32(const float * x, float * dst, const int k) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n    dst[i] = x[i] * x[i];\n}\n\nstatic __device__ __forceinline__ float2 warp_reduce_sum(float2 a) {\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        a.x += __shfl_xor_sync(0xffffffff, a.x, mask, 32);\n        a.y += __shfl_xor_sync(0xffffffff, a.y, mask, 32);\n    }\n    return a;\n}\n\ntemplate <int block_size>\nstatic __global__ void norm_f32(const float * x, float * dst, const int ncols) {\n    const int row = blockIdx.x*blockDim.y + threadIdx.y;\n    const int tid = threadIdx.x;\n\n    const float eps = 1e-5f;\n\n    float2 mean_var = make_float2(0.f, 0.f);\n\n    for (int col = tid; col < ncols; col += block_size) {\n        const float xi = x[row*ncols + col];\n        mean_var.x += xi;\n        mean_var.y += xi * xi;\n    }\n\n    // sum up partial sums\n    mean_var = warp_reduce_sum(mean_var);\n    if (block_size > WARP_SIZE) {\n        __shared__ float2 s_sum[32];\n        int warp_id = threadIdx.x / WARP_SIZE;\n        int lane_id = threadIdx.x % WARP_SIZE;\n        if (lane_id == 0) {\n            s_sum[warp_id] = mean_var;\n        }\n        __syncthreads();\n        mean_var = s_sum[lane_id];\n        mean_var = warp_reduce_sum(mean_var);\n    }\n\n    const float mean = mean_var.x / ncols;\n    const float var = mean_var.y / ncols - mean * mean;\n    const float inv_std = rsqrtf(var + eps);\n\n    for (int col = tid; col < ncols; col += block_size) {\n        dst[row*ncols + col] = (x[row*ncols + col] - mean) * inv_std;\n    }\n}\n\nstatic __device__ __forceinline__ float warp_reduce_sum(float x) {\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        x += __shfl_xor_sync(0xffffffff, x, mask, 32);\n    }\n    return x;\n}\n\ntemplate <int block_size>\nstatic __global__ void rms_norm_f32(const float * x, float * dst, const int ncols, const float eps) {\n    const int row = blockIdx.x*blockDim.y + threadIdx.y;\n    const int tid = threadIdx.x;\n\n    float tmp = 0.0f; // partial sum for thread in warp\n\n    for (int col = tid; col < ncols; col += block_size) {\n        const float xi = x[row*ncols + col];\n        tmp += xi * xi;\n    }\n\n    // sum up partial sums\n    tmp = warp_reduce_sum(tmp);\n    if (block_size > WARP_SIZE) {\n        __shared__ float s_sum[32];\n        int warp_id = threadIdx.x / WARP_SIZE;\n        int lane_id = threadIdx.x % WARP_SIZE;\n        if (lane_id == 0) {\n            s_sum[warp_id] = tmp;\n        }\n        __syncthreads();\n        tmp = s_sum[lane_id];\n        tmp = warp_reduce_sum(tmp);\n    }\n\n    const float mean = tmp / ncols;\n    const float scale = rsqrtf(mean + eps);\n\n    for (int col = tid; col < ncols; col += block_size) {\n        dst[row*ncols + col] = scale * x[row*ncols + col];\n    }\n}\n\nstatic __device__ __forceinline__ void dequantize_q4_0(const void * vx, const int ib, const int iqs, dfloat2 & v){\n    const block_q4_0 * x = (const block_q4_0 *) vx;\n\n    const dfloat d = x[ib].d;\n\n    const int vui = x[ib].qs[iqs];\n\n    v.x = vui & 0xF;\n    v.y = vui >> 4;\n\n#ifdef GGML_CUDA_F16\n    v = __hsub2(v, {8.0f, 8.0f});\n    v = __hmul2(v, {d, d});\n#else\n    v.x = (v.x - 8.0f) * d;\n    v.y = (v.y - 8.0f) * d;\n#endif // GGML_CUDA_F16\n}\n\nstatic __device__ __forceinline__ void dequantize_q4_1(const void * vx, const int ib, const int iqs, dfloat2 & v){\n    const block_q4_1 * x = (const block_q4_1 *) vx;\n\n    const dfloat d = __low2half(x[ib].dm);\n    const dfloat m = __high2half(x[ib].dm);\n\n    const int vui = x[ib].qs[iqs];\n\n    v.x = vui & 0xF;\n    v.y = vui >> 4;\n\n#ifdef GGML_CUDA_F16\n    v = __hmul2(v, {d, d});\n    v = __hadd2(v, {m, m});\n#else\n    v.x = (v.x * d) + m;\n    v.y = (v.y * d) + m;\n#endif // GGML_CUDA_F16\n}\n\nstatic __device__ __forceinline__ void dequantize_q5_0(const void * vx, const int ib, const int iqs, dfloat2 & v){\n    const block_q5_0 * x = (const block_q5_0 *) vx;\n\n    const dfloat d = x[ib].d;\n\n    uint32_t qh;\n    memcpy(&qh, x[ib].qh, sizeof(qh));\n\n    const int xh_0 = ((qh >> (iqs +  0)) << 4) & 0x10;\n    const int xh_1 = ((qh >> (iqs + 12))     ) & 0x10;\n\n    v.x = ((x[ib].qs[iqs] & 0xf) | xh_0);\n    v.y = ((x[ib].qs[iqs] >>  4) | xh_1);\n\n#ifdef GGML_CUDA_F16\n    v = __hsub2(v, {16.0f, 16.0f});\n    v = __hmul2(v, {d, d});\n#else\n    v.x = (v.x - 16.0f) * d;\n    v.y = (v.y - 16.0f) * d;\n#endif // GGML_CUDA_F16\n}\n\nstatic __device__ __forceinline__ void dequantize_q5_1(const void * vx, const int ib, const int iqs, dfloat2 & v){\n    const block_q5_1 * x = (const block_q5_1 *) vx;\n\n    const dfloat d = __low2half(x[ib].dm);\n    const dfloat m = __high2half(x[ib].dm);\n\n    uint32_t qh;\n    memcpy(&qh, x[ib].qh, sizeof(qh));\n\n    const int xh_0 = ((qh >> (iqs +  0)) << 4) & 0x10;\n    const int xh_1 = ((qh >> (iqs + 12))     ) & 0x10;\n\n    v.x = ((x[ib].qs[iqs] & 0xf) | xh_0);\n    v.y = ((x[ib].qs[iqs] >>  4) | xh_1);\n\n#ifdef GGML_CUDA_F16\n    v = __hmul2(v, {d, d});\n    v = __hadd2(v, {m, m});\n#else\n    v.x = (v.x * d) + m;\n    v.y = (v.y * d) + m;\n#endif // GGML_CUDA_F16\n}\n\nstatic __device__ __forceinline__ void dequantize_q8_0(const void * vx, const int ib, const int iqs, dfloat2 & v){\n    const block_q8_0 * x = (const block_q8_0 *) vx;\n\n    const dfloat d = x[ib].d;\n\n    v.x = x[ib].qs[iqs + 0];\n    v.y = x[ib].qs[iqs + 1];\n\n#ifdef GGML_CUDA_F16\n    v = __hmul2(v, {d, d});\n#else\n    v.x *= d;\n    v.y *= d;\n#endif // GGML_CUDA_F16\n}\n\n//================================== k-quants\n\ntemplate<typename dst_t>\nstatic __global__ void dequantize_block_q2_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {\n\n    const int i   = blockIdx.x;\n    const block_q2_K * x = (const block_q2_K *) vx;\n\n    const int tid = threadIdx.x;\n#if QK_K == 256\n    const int n   = tid/32;\n    const int l   = tid - 32*n;\n    const int is  = 8*n + l/16;\n\n    const uint8_t q = x[i].qs[32*n + l];\n    dst_t * y = yy + i*QK_K + 128*n;\n\n    float dall = __low2half(x[i].dm);\n    float dmin = __high2half(x[i].dm);\n    y[l+ 0] = dall * (x[i].scales[is+0] & 0xF) * ((q >> 0) & 3) - dmin * (x[i].scales[is+0] >> 4);\n    y[l+32] = dall * (x[i].scales[is+2] & 0xF) * ((q >> 2) & 3) - dmin * (x[i].scales[is+2] >> 4);\n    y[l+64] = dall * (x[i].scales[is+4] & 0xF) * ((q >> 4) & 3) - dmin * (x[i].scales[is+4] >> 4);\n    y[l+96] = dall * (x[i].scales[is+6] & 0xF) * ((q >> 6) & 3) - dmin * (x[i].scales[is+6] >> 4);\n#else\n    const int is = tid/16;  // 0 or 1\n    const int il = tid%16;  // 0...15\n    const uint8_t q = x[i].qs[il] >> (2*is);\n    dst_t * y = yy + i*QK_K + 16*is + il;\n    float dall = __low2half(x[i].dm);\n    float dmin = __high2half(x[i].dm);\n    y[ 0] = dall * (x[i].scales[is+0] & 0xF) * ((q >> 0) & 3) - dmin * (x[i].scales[is+0] >> 4);\n    y[32] = dall * (x[i].scales[is+2] & 0xF) * ((q >> 4) & 3) - dmin * (x[i].scales[is+2] >> 4);\n#endif\n\n}\n\ntemplate<typename dst_t>\nstatic __global__ void dequantize_block_q3_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {\n\n    const int i = blockIdx.x;\n    const block_q3_K * x = (const block_q3_K *) vx;\n\n#if QK_K == 256\n    const int r = threadIdx.x/4;\n    const int tid = r/2;\n    const int is0 = r%2;\n    const int l0 = 16*is0 + 4*(threadIdx.x%4);\n    const int n = tid / 4;\n    const int j = tid - 4*n;\n\n    uint8_t m = 1 << (4*n + j);\n    int is = 8*n + 2*j + is0;\n    int shift = 2*j;\n\n    int8_t us = is <  4 ? (x[i].scales[is-0] & 0xF) | (((x[i].scales[is+8] >> 0) & 3) << 4) :\n                is <  8 ? (x[i].scales[is-0] & 0xF) | (((x[i].scales[is+4] >> 2) & 3) << 4) :\n                is < 12 ? (x[i].scales[is-8] >>  4) | (((x[i].scales[is+0] >> 4) & 3) << 4) :\n                          (x[i].scales[is-8] >>  4) | (((x[i].scales[is-4] >> 6) & 3) << 4);\n    float d_all = x[i].d;\n    float dl = d_all * (us - 32);\n\n    dst_t * y = yy + i*QK_K + 128*n + 32*j;\n    const uint8_t * q = x[i].qs + 32*n;\n    const uint8_t * hm = x[i].hmask;\n\n    for (int l = l0; l < l0+4; ++l) y[l] = dl * ((int8_t)((q[l] >> shift) & 3) - ((hm[l] & m) ? 0 : 4));\n#else\n    const int tid = threadIdx.x;\n    const int is  = tid/16;  // 0 or 1\n    const int il  = tid%16;  // 0...15\n    const int im  = il/8;    // 0...1\n    const int in  = il%8;    // 0...7\n\n    dst_t * y = yy + i*QK_K + 16*is + il;\n\n    const uint8_t q = x[i].qs[il] >> (2*is);\n    const uint8_t h = x[i].hmask[in] >> (2*is + im);\n    const float   d = (float)x[i].d;\n\n    if (is == 0) {\n        y[ 0] = d * ((x[i].scales[0] & 0xF) - 8) * ((int8_t)((q >> 0) & 3) - ((h >> 0) & 1 ? 0 : 4));\n        y[32] = d * ((x[i].scales[1] & 0xF) - 8) * ((int8_t)((q >> 4) & 3) - ((h >> 4) & 1 ? 0 : 4));\n    } else {\n        y[ 0] = d * ((x[i].scales[0] >>  4) - 8) * ((int8_t)((q >> 0) & 3) - ((h >> 0) & 1 ? 0 : 4));\n        y[32] = d * ((x[i].scales[1] >>  4) - 8) * ((int8_t)((q >> 4) & 3) - ((h >> 4) & 1 ? 0 : 4));\n    }\n#endif\n\n}\n\n#if QK_K == 256\nstatic inline __device__ void get_scale_min_k4(int j, const uint8_t * q, uint8_t & d, uint8_t & m) {\n    if (j < 4) {\n        d = q[j] & 63; m = q[j + 4] & 63;\n    } else {\n        d = (q[j+4] & 0xF) | ((q[j-4] >> 6) << 4);\n        m = (q[j+4] >>  4) | ((q[j-0] >> 6) << 4);\n    }\n}\n#endif\n\ntemplate<typename dst_t>\nstatic __global__ void dequantize_block_q4_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {\n    const block_q4_K * x = (const block_q4_K *) vx;\n\n    const int i = blockIdx.x;\n\n#if QK_K == 256\n    // assume 32 threads\n    const int tid = threadIdx.x;\n    const int il  = tid/8;\n    const int ir  = tid%8;\n    const int is  = 2*il;\n    const int n   = 4;\n\n    dst_t * y = yy + i*QK_K + 64*il + n*ir;\n\n    const float dall = __low2half(x[i].dm);\n    const float dmin = __high2half(x[i].dm);\n\n    const uint8_t * q = x[i].qs + 32*il + n*ir;\n\n    uint8_t sc, m;\n    get_scale_min_k4(is + 0, x[i].scales, sc, m);\n    const float d1 = dall * sc; const float m1 = dmin * m;\n    get_scale_min_k4(is + 1, x[i].scales, sc, m);\n    const float d2 = dall * sc; const float m2 = dmin * m;\n    for (int l = 0; l < n; ++l) {\n        y[l + 0] = d1 * (q[l] & 0xF) - m1;\n        y[l +32] = d2 * (q[l] >>  4) - m2;\n    }\n#else\n    const int tid = threadIdx.x;\n    const uint8_t * q = x[i].qs;\n    dst_t * y = yy + i*QK_K;\n    const float d = (float)x[i].dm[0];\n    const float m = (float)x[i].dm[1];\n    y[tid+ 0] = d * (x[i].scales[0] & 0xF) * (q[tid] & 0xF) - m * (x[i].scales[0] >> 4);\n    y[tid+32] = d * (x[i].scales[1] & 0xF) * (q[tid] >>  4) - m * (x[i].scales[1] >> 4);\n#endif\n}\n\ntemplate<typename dst_t>\nstatic __global__ void dequantize_block_q5_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {\n    const block_q5_K * x = (const block_q5_K *) vx;\n\n    const int i = blockIdx.x;\n\n#if QK_K == 256\n    // assume 64 threads - this is very slightly better than the one below\n    const int tid = threadIdx.x;\n    const int il  = tid/16;   // il is in 0...3\n    const int ir  = tid%16;   // ir is in 0...15\n    const int is  = 2*il;     // is is in 0...6\n\n    dst_t * y = yy + i*QK_K + 64*il + 2*ir;\n\n    const float dall = __low2half(x[i].dm);\n    const float dmin = __high2half(x[i].dm);\n\n    const uint8_t * ql = x[i].qs + 32*il + 2*ir;\n    const uint8_t * qh = x[i].qh + 2*ir;\n\n    uint8_t sc, m;\n    get_scale_min_k4(is + 0, x[i].scales, sc, m);\n    const float d1 = dall * sc; const float m1 = dmin * m;\n    get_scale_min_k4(is + 1, x[i].scales, sc, m);\n    const float d2 = dall * sc; const float m2 = dmin * m;\n\n    uint8_t   hm  = 1 << (2*il);\n    y[ 0] = d1 * ((ql[ 0] & 0xF) + (qh[ 0] & hm ? 16 : 0)) - m1;\n    y[ 1] = d1 * ((ql[ 1] & 0xF) + (qh[ 1] & hm ? 16 : 0)) - m1;\n    hm <<= 1;\n    y[32] = d2 * ((ql[ 0] >>  4) + (qh[ 0] & hm ? 16 : 0)) - m2;\n    y[33] = d2 * ((ql[ 1] >>  4) + (qh[ 1] & hm ? 16 : 0)) - m2;\n#else\n    const int tid = threadIdx.x;\n    const uint8_t q = x[i].qs[tid];\n    const int im = tid/8;  // 0...3\n    const int in = tid%8;  // 0...7\n    const int is = tid/16; // 0 or 1\n    const uint8_t h = x[i].qh[in] >> im;\n    const float d = x[i].d;\n    dst_t * y = yy + i*QK_K + tid;\n    y[ 0] = d * x[i].scales[is+0] * ((q & 0xF) - ((h >> 0) & 1 ? 0 : 16));\n    y[32] = d * x[i].scales[is+2] * ((q >>  4) - ((h >> 4) & 1 ? 0 : 16));\n#endif\n}\n\ntemplate<typename dst_t>\nstatic __global__ void dequantize_block_q6_K(const void * __restrict__ vx, dst_t * __restrict__ yy) {\n    const block_q6_K * x = (const block_q6_K *) vx;\n\n    const int i = blockIdx.x;\n#if QK_K == 256\n\n    // assume 64 threads - this is very slightly better than the one below\n    const int tid = threadIdx.x;\n    const int ip  = tid/32;   // ip is 0 or 1\n    const int il  = tid - 32*ip; // 0...32\n    const int is  = 8*ip + il/16;\n\n    dst_t * y = yy + i*QK_K + 128*ip + il;\n\n    const float d = x[i].d;\n\n    const uint8_t * ql = x[i].ql + 64*ip + il;\n    const uint8_t   qh = x[i].qh[32*ip + il];\n    const int8_t  * sc = x[i].scales + is;\n\n    y[ 0] = d * sc[0] * ((int8_t)((ql[ 0] & 0xF) | (((qh >> 0) & 3) << 4)) - 32);\n    y[32] = d * sc[2] * ((int8_t)((ql[32] & 0xF) | (((qh >> 2) & 3) << 4)) - 32);\n    y[64] = d * sc[4] * ((int8_t)((ql[ 0]  >> 4) | (((qh >> 4) & 3) << 4)) - 32);\n    y[96] = d * sc[6] * ((int8_t)((ql[32]  >> 4) | (((qh >> 6) & 3) << 4)) - 32);\n#else\n\n    // assume 32 threads\n    const int tid = threadIdx.x;\n    const int ip  = tid/16;         // 0 or 1\n    const int il  = tid - 16*ip;    // 0...15\n\n    dst_t * y = yy + i*QK_K + 16*ip + il;\n\n    const float d = x[i].d;\n\n    const uint8_t   ql = x[i].ql[16*ip + il];\n    const uint8_t   qh = x[i].qh[il] >> (2*ip);\n    const int8_t  * sc = x[i].scales;\n\n    y[ 0] = d * sc[ip+0] * ((int8_t)((ql & 0xF) | (((qh >> 0) & 3) << 4)) - 32);\n    y[32] = d * sc[ip+2] * ((int8_t)((ql  >> 4) | (((qh >> 4) & 3) << 4)) - 32);\n#endif\n}\n\nstatic __global__ void dequantize_mul_mat_vec_q2_k(const void * __restrict__ vx, const float * __restrict__ yy, float * __restrict__ dst, const int ncols, int nrows) {\n\n    static_assert(16%K_QUANTS_PER_ITERATION == 0, \"16 must be divisible by K_QUANTS_PER_ITERATION\");\n\n    const int row = blockIdx.x*blockDim.y + threadIdx.y;\n    if (row > nrows) return;\n\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row;\n\n    const block_q2_K * x = (const block_q2_K *)vx + ib0;\n\n    float tmp = 0; // partial sum for thread in warp\n\n#if QK_K == 256\n    const int tid = threadIdx.x/K_QUANTS_PER_ITERATION;  // 0...31 or 0...15\n    const int ix  = threadIdx.x%K_QUANTS_PER_ITERATION;  // 0 or 0,1\n\n    const int step = 16/K_QUANTS_PER_ITERATION;\n\n    const int im = tid/step;                             // 0 or 1. 0 computes 0..., 1 computes 128...\n    const int in = tid - step*im;                        // 0...15 or 0...7\n\n    const int l0 = K_QUANTS_PER_ITERATION*in;            // 0...15 or 0...14 in steps of 2\n    const int q_offset = 32*im + l0;\n    const int s_offset = 8*im;\n    const int y_offset = 128*im + l0;\n\n    uint32_t aux[4];\n    const uint8_t * d = (const uint8_t *)aux;\n    const uint8_t * m = (const uint8_t *)(aux + 2);\n\n    for (int i = ix; i < num_blocks_per_row; i += K_QUANTS_PER_ITERATION) {\n\n        const float   * y = yy + i * QK_K + y_offset;\n        const uint8_t * q = x[i].qs + q_offset;\n\n        const float dall = __low2half(x[i].dm);\n        const float dmin = __high2half(x[i].dm);\n\n        const uint32_t * a = (const uint32_t *)(x[i].scales + s_offset);\n        aux[0] = a[0] & 0x0f0f0f0f;\n        aux[1] = a[1] & 0x0f0f0f0f;\n        aux[2] = (a[0] >> 4) & 0x0f0f0f0f;\n        aux[3] = (a[1] >> 4) & 0x0f0f0f0f;\n\n        float sum1 = 0, sum2 = 0;\n        for (int l = 0; l < K_QUANTS_PER_ITERATION; ++l) {\n            sum1 += y[l+ 0] * d[0] * ((q[l+ 0] >> 0) & 3)\n                  + y[l+32] * d[2] * ((q[l+ 0] >> 2) & 3)\n                  + y[l+64] * d[4] * ((q[l+ 0] >> 4) & 3)\n                  + y[l+96] * d[6] * ((q[l+ 0] >> 6) & 3)\n                  + y[l+16] * d[1] * ((q[l+16] >> 0) & 3)\n                  + y[l+48] * d[3] * ((q[l+16] >> 2) & 3)\n                  + y[l+80] * d[5] * ((q[l+16] >> 4) & 3)\n                  +y[l+112] * d[7] * ((q[l+16] >> 6) & 3);\n            sum2 += y[l+ 0] * m[0] + y[l+32] * m[2] + y[l+64] * m[4] + y[ l+96] * m[6]\n                  + y[l+16] * m[1] + y[l+48] * m[3] + y[l+80] * m[5] + y[l+112] * m[7];\n\n        }\n        tmp += dall * sum1 - dmin * sum2;\n\n    }\n#else\n    const int tid = threadIdx.x/(2*K_QUANTS_PER_ITERATION);  // 0...15 or 0...7\n    const int ix  = threadIdx.x%(2*K_QUANTS_PER_ITERATION);  // 0....1 or 0...3\n    const int offset = tid * K_QUANTS_PER_ITERATION;\n\n    uint32_t uaux[2];\n    const uint8_t * d = (const uint8_t *)uaux;\n\n    for (int i = ix; i < num_blocks_per_row; i += 2*K_QUANTS_PER_ITERATION) {\n\n        const float   * y = yy + i * QK_K + offset;\n        const uint8_t * q = x[i].qs + offset;\n        const uint32_t * s = (const uint32_t *)x[i].scales;\n\n        uaux[0] = s[0] & 0x0f0f0f0f;\n        uaux[1] = (s[0] >> 4) & 0x0f0f0f0f;\n\n        const float2 dall = __half22float2(x[i].dm);\n\n        float sum1 = 0, sum2 = 0;\n        for (int l = 0; l < K_QUANTS_PER_ITERATION; ++l) {\n            const uint8_t ql = q[l];\n            sum1 += y[l+ 0] * d[0] * ((ql >> 0) & 3)\n                  + y[l+16] * d[1] * ((ql >> 2) & 3)\n                  + y[l+32] * d[2] * ((ql >> 4) & 3)\n                  + y[l+48] * d[3] * ((ql >> 6) & 3);\n            sum2 += y[l+0] * d[4] + y[l+16] * d[5] + y[l+32] * d[6] + y[l+48] * d[7];\n        }\n        tmp += dall.x * sum1 - dall.y * sum2;\n    }\n#endif\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (threadIdx.x == 0) {\n        dst[row] = tmp;\n    }\n}\n\nstatic __global__ void dequantize_mul_mat_vec_q3_k(const void * __restrict__ vx, const float * __restrict__ yy, float * __restrict__ dst, const int ncols, int nrows) {\n\n    const int row = blockIdx.x*blockDim.y + threadIdx.y;\n    if (row > nrows) return;\n\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row;\n\n    const block_q3_K * x = (const block_q3_K *)vx + ib0;\n\n    float tmp = 0; // partial sum for thread in warp\n\n#if QK_K == 256\n\n    const uint16_t kmask1 = 0x0303;\n    const uint16_t kmask2 = 0x0f0f;\n\n    const int tid = threadIdx.x/K_QUANTS_PER_ITERATION;  // 0...31 or 0...16\n    const int ix  = threadIdx.x%K_QUANTS_PER_ITERATION;  // 0 or 0,1\n\n    const int n  = K_QUANTS_PER_ITERATION;               // iterations in the inner loop\n    const int step = 16/K_QUANTS_PER_ITERATION;\n    const int im = tid/step;                             // 0 or 1. 0 computes 0..., 1 computes 128...\n    const int in = tid - step*im;                        // 0....15 or 0...7\n\n    const uint8_t m = 1 << (4*im);\n\n    const int l0 = n*in;                                 // 0...15 or 0...14 in steps of 2\n    const int q_offset =  32*im + l0;\n    const int y_offset = 128*im + l0;\n\n    uint16_t utmp[4];\n    const int8_t * s = (const int8_t *)utmp;\n\n    const uint16_t s_shift = 4*im;\n\n    for (int i = ix; i < num_blocks_per_row; i += K_QUANTS_PER_ITERATION) {\n\n        const float   * y  = yy + i * QK_K + y_offset;\n        const uint8_t * q = x[i].qs + q_offset;\n        const uint8_t * h = x[i].hmask + l0;\n\n        const uint16_t * a = (const uint16_t *)x[i].scales;\n        utmp[0] = ((a[0] >> s_shift) & kmask2) | (((a[4] >> (s_shift + 0)) & kmask1) << 4);\n        utmp[1] = ((a[1] >> s_shift) & kmask2) | (((a[5] >> (s_shift + 0)) & kmask1) << 4);\n        utmp[2] = ((a[2] >> s_shift) & kmask2) | (((a[4] >> (s_shift + 2)) & kmask1) << 4);\n        utmp[3] = ((a[3] >> s_shift) & kmask2) | (((a[5] >> (s_shift + 2)) & kmask1) << 4);\n\n        const float d = x[i].d;\n\n        float sum = 0;\n        for (int l = 0; l < n; ++l) {\n            sum += y[l+ 0] * (s[0] - 32) * (((q[l] >> 0) & 3) - (h[l] & (m << 0) ? 0 : 4))\n                 + y[l+32] * (s[2] - 32) * (((q[l] >> 2) & 3) - (h[l] & (m << 1) ? 0 : 4))\n                 + y[l+64] * (s[4] - 32) * (((q[l] >> 4) & 3) - (h[l] & (m << 2) ? 0 : 4))\n                 + y[l+96] * (s[6] - 32) * (((q[l] >> 6) & 3) - (h[l] & (m << 3) ? 0 : 4));\n            sum += y[l+16] * (s[1] - 32) * (((q[l+16] >> 0) & 3) - (h[l+16] & (m << 0) ? 0 : 4))\n                 + y[l+48] * (s[3] - 32) * (((q[l+16] >> 2) & 3) - (h[l+16] & (m << 1) ? 0 : 4))\n                 + y[l+80] * (s[5] - 32) * (((q[l+16] >> 4) & 3) - (h[l+16] & (m << 2) ? 0 : 4))\n                + y[l+112] * (s[7] - 32) * (((q[l+16] >> 6) & 3) - (h[l+16] & (m << 3) ? 0 : 4));\n        }\n        tmp += d * sum;\n\n    }\n#else\n\n    const int tid = threadIdx.x/(2*K_QUANTS_PER_ITERATION);  // 0...15 or 0...7\n    const int ix  = threadIdx.x%(2*K_QUANTS_PER_ITERATION);  // 0....1 or 0...3\n    const int offset = tid * K_QUANTS_PER_ITERATION;         // 0...15 or 0...14\n    const int in = offset/8;                                 // 0 or 1\n    const int im = offset%8;                                 // 0...7\n\n    for (int i = ix; i < num_blocks_per_row; i += 2*K_QUANTS_PER_ITERATION) {\n\n        const float   * y = yy + i * QK_K + offset;\n        const uint8_t * q = x[i].qs + offset;\n        const uint8_t * s = x[i].scales;\n\n        const float dall = (float)x[i].d;\n\n        float sum = 0;\n        for (int l = 0; l < K_QUANTS_PER_ITERATION; ++l) {\n            const uint8_t hl = x[i].hmask[im+l] >> in;\n            const uint8_t ql = q[l];\n            sum += y[l+ 0] * dall * ((s[0] & 0xF) - 8) * ((int8_t)((ql >> 0) & 3) - ((hl >> 0) & 1 ? 0 : 4))\n                 + y[l+16] * dall * ((s[0] >>  4) - 8) * ((int8_t)((ql >> 2) & 3) - ((hl >> 2) & 1 ? 0 : 4))\n                 + y[l+32] * dall * ((s[1] & 0xF) - 8) * ((int8_t)((ql >> 4) & 3) - ((hl >> 4) & 1 ? 0 : 4))\n                 + y[l+48] * dall * ((s[1] >>  4) - 8) * ((int8_t)((ql >> 6) & 3) - ((hl >> 6) & 1 ? 0 : 4));\n        }\n        tmp += sum;\n    }\n#endif\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (threadIdx.x == 0) {\n        dst[row] = tmp;\n    }\n}\n\nstatic __global__ void dequantize_mul_mat_vec_q4_k(const void * __restrict__ vx, const float * __restrict__ yy, float * __restrict__ dst, const int ncols, int nrows) {\n\n    const int row = blockIdx.x*blockDim.y + threadIdx.y;\n    if (row > nrows) return;\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row;\n\n    const block_q4_K * x = (const block_q4_K *)vx + ib0;\n\n#if QK_K == 256\n    const uint16_t kmask1 = 0x3f3f;\n    const uint16_t kmask2 = 0x0f0f;\n    const uint16_t kmask3 = 0xc0c0;\n\n    const int tid = threadIdx.x/K_QUANTS_PER_ITERATION;  // 0...31 or 0...16\n    const int ix  = threadIdx.x%K_QUANTS_PER_ITERATION;  // 0 or 0,1\n\n    const int step = 8/K_QUANTS_PER_ITERATION;           // 8 or 4\n\n    const int il  = tid/step;                            // 0...3\n    const int ir  = tid - step*il;                       // 0...7 or 0...3\n    const int n   = 2 * K_QUANTS_PER_ITERATION;          // 2 or 4\n\n    const int im = il/2;  // 0 or 1. 0 computes 0,32 + 128,160, 1 computes 64,96 + 192,224\n    const int in = il%2;\n\n    const int l0 = n*(2*ir + in);\n    const int q_offset = 32*im + l0;\n    const int y_offset = 64*im + l0;\n\n    uint16_t aux[4];\n    const uint8_t * sc = (const uint8_t *)aux;\n\n#if K_QUANTS_PER_ITERATION == 2\n    uint32_t q32[4];\n    const uint8_t * q4 = (const uint8_t *)q32;\n#else\n    uint16_t q16[4];\n    const uint8_t * q4 = (const uint8_t *)q16;\n#endif\n\n    float tmp = 0; // partial sum for thread in warp\n\n    for (int i = ix; i < num_blocks_per_row; i += K_QUANTS_PER_ITERATION) {\n\n        const float   * y1 = yy + i*QK_K + y_offset;\n        const float   * y2 = y1 + 128;\n\n        const float dall = __low2half(x[i].dm);\n        const float dmin = __high2half(x[i].dm);\n\n        const uint16_t * a = (const uint16_t *)x[i].scales;\n        aux[0] = a[im+0] & kmask1;\n        aux[1] = a[im+2] & kmask1;\n        aux[2] = ((a[im+4] >> 0) & kmask2) | ((a[im+0] & kmask3) >> 2);\n        aux[3] = ((a[im+4] >> 4) & kmask2) | ((a[im+2] & kmask3) >> 2);\n\n#if K_QUANTS_PER_ITERATION == 2\n        const uint32_t * q1 = (const uint32_t *)(x[i].qs + q_offset);\n        const uint32_t * q2 = q1 + 16;\n\n        q32[0] = q1[0] & 0x0f0f0f0f;\n        q32[1] = q1[0] & 0xf0f0f0f0;\n        q32[2] = q2[0] & 0x0f0f0f0f;\n        q32[3] = q2[0] & 0xf0f0f0f0;\n\n        float4 s = {0.f, 0.f, 0.f, 0.f};\n        float smin = 0;\n        for (int l = 0; l < 4; ++l) {\n            s.x += y1[l] * q4[l+0]; s.y += y1[l+32] * q4[l+ 4];\n            s.z += y2[l] * q4[l+8]; s.w += y2[l+32] * q4[l+12];\n            smin += y1[l] * sc[2] + y1[l+32] * sc[3] + y2[l] * sc[6] + y2[l+32] * sc[7];\n        }\n        tmp += dall * (s.x * sc[0] + s.y * sc[1] * 1.f/16.f + s.z * sc[4] + s.w * sc[5] * 1.f/16.f) - dmin * smin;\n#else\n        const uint16_t * q1 = (const uint16_t *)(x[i].qs + q_offset);\n        const uint16_t * q2 = q1 + 32;\n\n        q16[0] = q1[0] & 0x0f0f;\n        q16[1] = q1[0] & 0xf0f0;\n        q16[2] = q2[0] & 0x0f0f;\n        q16[3] = q2[0] & 0xf0f0;\n\n        float4 s = {0.f, 0.f, 0.f, 0.f};\n        float smin = 0;\n        for (int l = 0; l < 2; ++l) {\n            s.x += y1[l] * q4[l+0]; s.y += y1[l+32] * q4[l+2];\n            s.z += y2[l] * q4[l+4]; s.w += y2[l+32] * q4[l+6];\n            smin += y1[l] * sc[2] + y1[l+32] * sc[3] + y2[l] * sc[6] + y2[l+32] * sc[7];\n        }\n        tmp += dall * (s.x * sc[0] + s.y * sc[1] * 1.f/16.f + s.z * sc[4] + s.w * sc[5] * 1.f/16.f) - dmin * smin;\n#endif\n\n    }\n#else\n    const int tid = threadIdx.x/(2*K_QUANTS_PER_ITERATION);  // 0...15\n    const int ix  = threadIdx.x%(2*K_QUANTS_PER_ITERATION);\n\n    const int step = tid * K_QUANTS_PER_ITERATION;\n\n    uint16_t aux16[2];\n    const uint8_t * s = (const uint8_t *)aux16;\n\n    float tmp = 0;\n\n    for (int i = ix; i < num_blocks_per_row; i += 2*K_QUANTS_PER_ITERATION) {\n        const uint8_t * q = x[i].qs + step;\n        const float   * y = yy + i*QK_K + step;\n        const uint16_t * a = (const uint16_t *)x[i].scales;\n        aux16[0] = a[0] & 0x0f0f;\n        aux16[1] = (a[0] >> 4) & 0x0f0f;\n        const float d = (float)x[i].dm[0];\n        const float m = (float)x[i].dm[1];\n        float sum = 0.f;\n        for (int j = 0; j < K_QUANTS_PER_ITERATION; ++j) {\n            sum += y[j+ 0] * (d * s[0] * (q[j+ 0] & 0xF) - m * s[2])\n                 + y[j+16] * (d * s[0] * (q[j+16] & 0xF) - m * s[2])\n                 + y[j+32] * (d * s[1] * (q[j+ 0] >>  4) - m * s[3])\n                 + y[j+48] * (d * s[1] * (q[j+16] >>  4) - m * s[3]);\n        }\n        tmp += sum;\n    }\n\n#endif\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (tid == 0) {\n        dst[row] = tmp;\n    }\n}\n\nstatic __global__ void dequantize_mul_mat_vec_q5_k(const void * __restrict__ vx, const float * __restrict__ yy, float * __restrict__ dst, const int ncols) {\n\n    const int row = blockIdx.x;\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row;\n\n    const block_q5_K * x = (const block_q5_K *)vx + ib0;\n\n    float tmp = 0; // partial sum for thread in warp\n\n#if QK_K == 256\n    const uint16_t kmask1 = 0x3f3f;\n    const uint16_t kmask2 = 0x0f0f;\n    const uint16_t kmask3 = 0xc0c0;\n\n    const int tid = threadIdx.x/2;  // 0...15\n    const int ix  = threadIdx.x%2;\n\n    const int il  = tid/4;     // 0...3\n    const int ir  = tid - 4*il;// 0...3\n    const int n   = 2;\n\n    const int im = il/2;  // 0 or 1. 0 computes 0,32 + 128,160, 1 computes 64,96 + 192,224\n    const int in = il%2;\n\n    const int l0 = n*(2*ir + in);\n    const int q_offset = 32*im + l0;\n    const int y_offset = 64*im + l0;\n\n    const uint8_t hm1  = 1 << (2*im);\n    const uint8_t hm2  = hm1 << 4;\n\n    uint16_t aux[4];\n    const uint8_t * sc = (const uint8_t *)aux;\n\n    uint16_t q16[8];\n    const uint8_t * q4 = (const uint8_t *)q16;\n\n    for (int i = ix; i < num_blocks_per_row; i += 2) {\n\n        const uint8_t * ql1 = x[i].qs + q_offset;\n        const uint8_t * qh  = x[i].qh + l0;\n        const float   * y1  = yy + i*QK_K + y_offset;\n        const float   * y2  = y1 + 128;\n\n        const float dall = __low2half(x[i].dm);\n        const float dmin = __high2half(x[i].dm);\n\n        const uint16_t * a = (const uint16_t *)x[i].scales;\n        aux[0] = a[im+0] & kmask1;\n        aux[1] = a[im+2] & kmask1;\n        aux[2] = ((a[im+4] >> 0) & kmask2) | ((a[im+0] & kmask3) >> 2);\n        aux[3] = ((a[im+4] >> 4) & kmask2) | ((a[im+2] & kmask3) >> 2);\n\n        float4 sum = {0.f, 0.f, 0.f, 0.f};\n        float smin = 0;\n        const uint16_t * q1 = (const uint16_t *)ql1;\n        const uint16_t * q2 = q1 + 32;\n        q16[0] = q1[0] & 0x0f0f;\n        q16[1] = q1[8] & 0x0f0f;\n        q16[2] = (q1[0] >> 4) & 0x0f0f;\n        q16[3] = (q1[8] >> 4) & 0x0f0f;\n        q16[4] = q2[0] & 0x0f0f;\n        q16[5] = q2[8] & 0x0f0f;\n        q16[6] = (q2[0] >> 4) & 0x0f0f;\n        q16[7] = (q2[8] >> 4) & 0x0f0f;\n        for (int l = 0; l < n; ++l) {\n            sum.x += y1[l+ 0] * (q4[l +0] + (qh[l+ 0] & (hm1 << 0) ? 16 : 0))\n                   + y1[l+16] * (q4[l +2] + (qh[l+16] & (hm1 << 0) ? 16 : 0));\n            sum.y += y1[l+32] * (q4[l +4] + (qh[l+ 0] & (hm1 << 1) ? 16 : 0))\n                   + y1[l+48] * (q4[l +6] + (qh[l+16] & (hm1 << 1) ? 16 : 0));\n            sum.z += y2[l+ 0] * (q4[l +8] + (qh[l+ 0] & (hm2 << 0) ? 16 : 0))\n                   + y2[l+16] * (q4[l+10] + (qh[l+16] & (hm2 << 0) ? 16 : 0));\n            sum.w += y2[l+32] * (q4[l+12] + (qh[l+ 0] & (hm2 << 1) ? 16 : 0))\n                   + y2[l+48] * (q4[l+14] + (qh[l+16] & (hm2 << 1) ? 16 : 0));\n            smin += (y1[l] + y1[l+16]) * sc[2] + (y1[l+32] + y1[l+48]) * sc[3]\n                  + (y2[l] + y2[l+16]) * sc[6] + (y2[l+32] + y2[l+48]) * sc[7];\n        }\n        tmp += dall * (sum.x * sc[0] + sum.y * sc[1] + sum.z * sc[4] + sum.w * sc[5]) - dmin * smin;\n    }\n\n#else\n    const int tid = threadIdx.x/(2*K_QUANTS_PER_ITERATION);  // 0...15\n    const int ix  = threadIdx.x%(2*K_QUANTS_PER_ITERATION);\n    const int step = tid * K_QUANTS_PER_ITERATION;\n    const int im = step/8;\n    const int in = step%8;\n\n    for (int i = ix; i < num_blocks_per_row; i += 2*K_QUANTS_PER_ITERATION) {\n        const uint8_t * q = x[i].qs + step;\n        const int8_t  * s = x[i].scales;\n        const float   * y = yy + i*QK_K + step;\n        const float     d = x[i].d;\n        float sum = 0.f;\n        for (int j = 0; j < K_QUANTS_PER_ITERATION; ++j) {\n            const uint8_t h = x[i].qh[in+j] >> im;\n            sum += y[j+ 0] * d * s[0] * ((q[j+ 0] & 0xF) - ((h >> 0) & 1 ? 0 : 16))\n                 + y[j+16] * d * s[1] * ((q[j+16] & 0xF) - ((h >> 2) & 1 ? 0 : 16))\n                 + y[j+32] * d * s[2] * ((q[j+ 0] >>  4) - ((h >> 4) & 1 ? 0 : 16))\n                 + y[j+48] * d * s[3] * ((q[j+16] >>  4) - ((h >> 6) & 1 ? 0 : 16));\n        }\n        tmp += sum;\n    }\n#endif\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (threadIdx.x == 0) {\n        dst[row] = tmp;\n    }\n}\n\nstatic __global__ void dequantize_mul_mat_vec_q6_k(const void * __restrict__ vx, const float * __restrict__ yy, float * __restrict__ dst, const int ncols, int nrows) {\n\n    static_assert(16%K_QUANTS_PER_ITERATION == 0, \"16 must be divisible by K_QUANTS_PER_ITERATION\");\n\n    const int row = blockIdx.x*blockDim.y + threadIdx.y;\n    if (row > nrows) return;\n\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row;\n\n    const block_q6_K * x = (const block_q6_K *)vx + ib0;\n\n#if QK_K == 256\n\n    const int tid = threadIdx.x/K_QUANTS_PER_ITERATION;  // 0...31 or 0...16\n    const int ix  = threadIdx.x%K_QUANTS_PER_ITERATION;  // 0 or 0, 1\n\n    const int step = 16/K_QUANTS_PER_ITERATION;          // 16 or 8\n\n    const int im = tid/step;                             // 0 or 1. 0 computes 0..., 1 computes 128...\n    const int in = tid - step*im;                        // 0...15 or 0...7\n\n#if K_QUANTS_PER_ITERATION == 1\n    const int l0 = K_QUANTS_PER_ITERATION*in;            // 0...15\n    const int is = 0;\n#else\n    const int l0 = 4 * in;                               // 0, 4, 8, ..., 28\n    const int is = in / 4;\n#endif\n    const int ql_offset = 64*im + l0;\n    const int qh_offset = 32*im + l0;\n    const int s_offset  =  8*im + is;\n    const int y_offset = 128*im + l0;\n\n    float tmp = 0; // partial sum for thread in warp\n\n    for (int i = ix; i < num_blocks_per_row; i += K_QUANTS_PER_ITERATION) {\n\n        const float   * y  = yy + i * QK_K + y_offset;\n        const uint8_t * ql = x[i].ql + ql_offset;\n        const uint8_t * qh = x[i].qh + qh_offset;\n        const int8_t  * s  = x[i].scales + s_offset;\n\n        const float d = x[i].d;\n\n#if K_QUANTS_PER_ITERATION == 1\n        float sum = y[ 0] * s[0] * d * ((int8_t)((ql[ 0] & 0xF) | ((qh[ 0] & 0x03) << 4)) - 32)\n                  + y[16] * s[1] * d * ((int8_t)((ql[16] & 0xF) | ((qh[16] & 0x03) << 4)) - 32)\n                  + y[32] * s[2] * d * ((int8_t)((ql[32] & 0xF) | ((qh[ 0] & 0x0c) << 2)) - 32)\n                  + y[48] * s[3] * d * ((int8_t)((ql[48] & 0xF) | ((qh[16] & 0x0c) << 2)) - 32)\n                  + y[64] * s[4] * d * ((int8_t)((ql[ 0]  >> 4) | ((qh[ 0] & 0x30) >> 0)) - 32)\n                  + y[80] * s[5] * d * ((int8_t)((ql[16]  >> 4) | ((qh[16] & 0x30) >> 0)) - 32)\n                  + y[96] * s[6] * d * ((int8_t)((ql[32]  >> 4) | ((qh[ 0] & 0xc0) >> 2)) - 32)\n                  +y[112] * s[7] * d * ((int8_t)((ql[48]  >> 4) | ((qh[16] & 0xc0) >> 2)) - 32);\n        tmp += sum;\n#else\n        float sum = 0;\n        for (int l = 0; l < 4; ++l) {\n            sum += y[l+ 0] * s[0] * d * ((int8_t)((ql[l+ 0] & 0xF) | (((qh[l] >> 0) & 3) << 4)) - 32)\n                 + y[l+32] * s[2] * d * ((int8_t)((ql[l+32] & 0xF) | (((qh[l] >> 2) & 3) << 4)) - 32)\n                 + y[l+64] * s[4] * d * ((int8_t)((ql[l+ 0]  >> 4) | (((qh[l] >> 4) & 3) << 4)) - 32)\n                 + y[l+96] * s[6] * d * ((int8_t)((ql[l+32]  >> 4) | (((qh[l] >> 6) & 3) << 4)) - 32);\n        }\n        tmp += sum;\n#endif\n\n    }\n\n#else\n\n    const int tid = threadIdx.x/(2*K_QUANTS_PER_ITERATION);  // 0...7\n    const int ix  = threadIdx.x%(2*K_QUANTS_PER_ITERATION);  // 0...3\n\n    const int step = tid * K_QUANTS_PER_ITERATION;\n\n    float tmp = 0; // partial sum for thread in warp\n\n    for (int i = ix; i < num_blocks_per_row; i += 2*K_QUANTS_PER_ITERATION) {\n\n        const float   * y  = yy + i * QK_K + step;\n        const uint8_t * ql = x[i].ql + step;\n        const uint8_t * qh = x[i].qh + step;\n        const int8_t  * s  = x[i].scales;\n\n        const float d = x[i+0].d;\n\n        float sum = 0;\n        for (int j = 0; j < K_QUANTS_PER_ITERATION; ++j) {\n            sum += y[j+ 0] * s[0] * d * ((int8_t)((ql[j+ 0] & 0xF) | ((qh[j] & 0x03) << 4)) - 32)\n                 + y[j+16] * s[1] * d * ((int8_t)((ql[j+16] & 0xF) | ((qh[j] & 0x0c) << 2)) - 32)\n                 + y[j+32] * s[2] * d * ((int8_t)((ql[j+ 0] >>  4) | ((qh[j] & 0x30) >> 0)) - 32)\n                 + y[j+48] * s[3] * d * ((int8_t)((ql[j+16] >>  4) | ((qh[j] & 0xc0) >> 2)) - 32);\n        }\n        tmp += sum;\n\n    }\n\n#endif\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (tid == 0) {\n        dst[row] = tmp;\n    }\n}\n\nstatic __device__ void convert_f16(const void * vx, const int ib, const int iqs, dfloat2 & v){\n    const half * x = (const half *) vx;\n\n    // automatic half -> float type cast if dfloat == float\n    v.x = x[ib + iqs + 0];\n    v.y = x[ib + iqs + 1];\n}\n\nstatic __device__ void convert_f32(const void * vx, const int ib, const int iqs, dfloat2 & v){\n    const float * x = (const float *) vx;\n\n    // automatic half -> float type cast if dfloat == float\n    v.x = x[ib + iqs + 0];\n    v.y = x[ib + iqs + 1];\n}\n\nstatic __global__ void quantize_q8_1(const float * __restrict__ x, void * __restrict__ vy, const int kx, const int kx_padded) {\n    const int ix = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (ix >= kx_padded) {\n        return;\n    }\n\n    const int iy = blockDim.y*blockIdx.y + threadIdx.y;\n\n    const int i_padded = iy*kx_padded + ix;\n\n    block_q8_1 * y = (block_q8_1 *) vy;\n\n    const int ib = i_padded / QK8_1; // block index\n    const int iqs = i_padded % QK8_1; // quant index\n\n    const float xi = ix < kx ? x[iy*kx + ix] : 0.0f;\n    float amax = fabsf(xi);\n    float sum = xi;\n\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        amax = fmaxf(amax, __shfl_xor_sync(0xffffffff, amax, mask, 32));\n        sum += __shfl_xor_sync(0xffffffff, sum, mask, 32);\n    }\n\n    const float d = amax / 127;\n    const int8_t q = amax == 0.0f ? 0 : roundf(xi / d);\n\n    y[ib].qs[iqs] = q;\n\n    if (iqs > 0) {\n        return;\n    }\n\n    reinterpret_cast<half&>(y[ib].ds.x) = d;\n    reinterpret_cast<half&>(y[ib].ds.y) = sum;\n}\n\ntemplate<int qk, int qr, dequantize_kernel_t dequantize_kernel, typename dst_t>\nstatic __global__ void k_get_rows(const void * x, const int32_t * y, dst_t * dst, const int ncols) {\n    const int col = (blockIdx.x*blockDim.x + threadIdx.x)*2;\n    const int row = blockDim.y*blockIdx.y + threadIdx.y;\n\n    if (col >= ncols) {\n        return;\n    }\n\n    const int r = y[row];\n\n    // copy x[r*ncols + col] to dst[row*ncols + col]\n    const int xi = r*ncols + col;\n    const int di = row*ncols + col;\n\n    const int ib = xi/qk; // block index\n    const int iqs = (xi%qk)/qr; // quant index\n    const int iybs = di - di%qk; // y block start index\n    const int y_offset = qr == 1 ? 1 : qk/2;\n\n    // dequantize\n    dfloat2 v;\n    dequantize_kernel(x, ib, iqs, v);\n\n    dst[iybs + iqs + 0]        = v.x;\n    dst[iybs + iqs + y_offset] = v.y;\n}\n\ntemplate <int qk, int qr, dequantize_kernel_t dequantize_kernel, typename dst_t>\nstatic __global__ void dequantize_block(const void * __restrict__ vx, dst_t * __restrict__ y, const int k) {\n    const int i = blockDim.x*blockIdx.x + 2*threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n\n    const int ib = i/qk; // block index\n    const int iqs = (i%qk)/qr; // quant index\n    const int iybs = i - i%qk; // y block start index\n    const int y_offset = qr == 1 ? 1 : qk/2;\n\n    // dequantize\n    dfloat2 v;\n    dequantize_kernel(vx, ib, iqs, v);\n\n    y[iybs + iqs + 0]        = v.x;\n    y[iybs + iqs + y_offset] = v.y;\n}\n\n// VDR = vec dot ratio, how many contiguous integers each thread processes when the vec dot kernel is called\n// MMVQ = mul_mat_vec_q, MMQ = mul_mat_q\n\n#define VDR_Q4_0_Q8_1_MMVQ 2\n#define VDR_Q4_0_Q8_1_MMQ  4\n\ntemplate <int vdr> static __device__ __forceinline__ float vec_dot_q4_0_q8_1_impl(\n    const int * v, const int * u, const float & d4, const half2 & ds8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    int sumi = 0;\n\n#pragma unroll\n    for (int i = 0; i < vdr; ++i) {\n        const int vi0 = (v[i] >> 0) & 0x0F0F0F0F;\n        const int vi1 = (v[i] >> 4) & 0x0F0F0F0F;\n\n        // SIMD dot product of quantized values\n        sumi = __dp4a(vi0, u[2*i+0], sumi);\n        sumi = __dp4a(vi1, u[2*i+1], sumi);\n    }\n\n    const float2 ds8f = __half22float2(ds8);\n\n    // second part effectively subtracts 8 from each quant value\n    return d4 * (sumi * ds8f.x - (8*vdr/QI4_0) * ds8f.y);\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q4_1_Q8_1_MMVQ 2\n#define VDR_Q4_1_Q8_1_MMQ  4\n\ntemplate <int vdr> static __device__ __forceinline__ float vec_dot_q4_1_q8_1_impl(\n    const int * v, const int * u, const half2 & dm4, const half2 & ds8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    int sumi = 0;\n\n#pragma unroll\n    for (int i = 0; i < vdr; ++i) {\n        const int vi0 = (v[i] >> 0) & 0x0F0F0F0F;\n        const int vi1 = (v[i] >> 4) & 0x0F0F0F0F;\n\n        // SIMD dot product of quantized values\n        sumi = __dp4a(vi0, u[2*i+0], sumi);\n        sumi = __dp4a(vi1, u[2*i+1], sumi);\n    }\n\n#ifdef GGML_CUDA_F16\n    const float2 tmp = __half22float2(__hmul2(dm4, ds8));\n    const float d4d8 = tmp.x;\n    const float m4s8 = tmp.y;\n#else\n    const float2 dm4f = __half22float2(dm4);\n    const float2 ds8f = __half22float2(ds8);\n    const float d4d8 = dm4f.x * ds8f.x;\n    const float m4s8 = dm4f.y * ds8f.y;\n#endif // GGML_CUDA_F16\n\n    // scale second part of sum by QI8_1/(vdr * QR4_1) to compensate for multiple threads adding it\n    return sumi * d4d8 + m4s8 / (QI8_1 / (vdr * QR4_1));\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q5_0_Q8_1_MMVQ 2\n#define VDR_Q5_0_Q8_1_MMQ  4\n\ntemplate <int vdr> static __device__ __forceinline__ float vec_dot_q5_0_q8_1_impl(\n    const int * vl, const int * vh, const int * u, const float & d5, const half2 & ds8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    int sumi = 0;\n\n#pragma unroll\n    for (int i = 0; i < vdr; ++i) {\n        int vi0 = (vl[i] >>  0) & 0x0F0F0F0F; // lower 4 qs bits, still need qh as 5th bits\n        vi0    |= (vh[i] <<  4) & 0x00000010; // 0 ->  4\n        vi0    |= (vh[i] << 11) & 0x00001000; // 1 -> 12\n        vi0    |= (vh[i] << 18) & 0x00100000; // 2 -> 20\n        vi0    |= (vh[i] << 25) & 0x10000000; // 3 -> 28\n        sumi = __dp4a(vi0, u[2*i+0], sumi); // SIMD dot product of quantized values\n\n        int vi1 = (vl[i] >>  4) & 0x0F0F0F0F; // upper 4 qs bits, still need qh as 5th bits\n        vi1    |= (vh[i] >> 12) & 0x00000010; // 16 ->  4\n        vi1    |= (vh[i] >>  5) & 0x00001000; // 17 -> 12\n        vi1    |= (vh[i] <<  2) & 0x00100000; // 18 -> 20\n        vi1    |= (vh[i] <<  9) & 0x10000000; // 19 -> 28\n        sumi = __dp4a(vi1, u[2*i+1], sumi); // SIMD dot product of quantized values\n    }\n\n    const float2 ds8f = __half22float2(ds8);\n\n    // second part effectively subtracts 16 from each quant value\n    return d5 * (sumi * ds8f.x - (16*vdr/QI5_0) * ds8f.y);\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q5_1_Q8_1_MMVQ 2\n#define VDR_Q5_1_Q8_1_MMQ  4\n\ntemplate <int vdr> static __device__ __forceinline__ float vec_dot_q5_1_q8_1_impl(\n    const int * vl, const int * vh, const int * u, const half2 & dm5, const half2 & ds8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    int sumi = 0;\n\n#pragma unroll\n    for (int i = 0; i < vdr; ++i) {\n        int vi0 = (vl[i] >>  0) & 0x0F0F0F0F; // lower 4 qs bits, still need qh as 5th bits\n        vi0    |= (vh[i] <<  4) & 0x00000010; // 0 ->  4\n        vi0    |= (vh[i] << 11) & 0x00001000; // 1 -> 12\n        vi0    |= (vh[i] << 18) & 0x00100000; // 2 -> 20\n        vi0    |= (vh[i] << 25) & 0x10000000; // 3 -> 28\n        sumi = __dp4a(vi0, u[2*i+0], sumi); // SIMD dot product of quantized values\n\n        int vi1 = (vl[i] >>  4) & 0x0F0F0F0F; // upper 4 qs bits, still need qh as 5th bits\n        vi1    |= (vh[i] >> 12) & 0x00000010; // 16 ->  4\n        vi1    |= (vh[i] >>  5) & 0x00001000; // 17 -> 12\n        vi1    |= (vh[i] <<  2) & 0x00100000; // 18 -> 20\n        vi1    |= (vh[i] <<  9) & 0x10000000; // 19 -> 28\n        sumi = __dp4a(vi1, u[2*i+1], sumi); // SIMD dot product of quantized values\n    }\n\n#ifdef GGML_CUDA_F16\n    const float2 tmp = __half22float2(__hmul2(dm5, ds8));\n    const float d5d8 = tmp.x;\n    const float m5s8 = tmp.y;\n#else\n    const float2 dm5f = __half22float2(dm5);\n    const float2 ds8f = __half22float2(ds8);\n    const float d5d8 = dm5f.x * ds8f.x;\n    const float m5s8 = dm5f.y * ds8f.y;\n#endif // GGML_CUDA_F16\n\n    // scale second part of sum by QI5_1 / vdr to compensate for multiple threads adding it\n    return sumi*d5d8 + m5s8 / (QI5_1 / vdr);\n\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q8_0_Q8_1_MMVQ 2\n#define VDR_Q8_0_Q8_1_MMQ 8\n\ntemplate <int vdr> static __device__ __forceinline__ float vec_dot_q8_0_q8_1_impl(\n    const int * v, const int * u, const float & d8_0, const float & d8_1) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    int sumi = 0;\n\n#pragma unroll\n    for (int i = 0; i < vdr; ++i) {\n        // SIMD dot product of quantized values\n        sumi = __dp4a(v[i], u[i], sumi);\n    }\n\n    return d8_0*d8_1 * sumi;\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\ntemplate <int vdr> static __device__ __forceinline__ float vec_dot_q8_1_q8_1_impl(\n    const int * v, const int * u, const half2 & dm8, const half2 & ds8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    int sumi = 0;\n\n#pragma unroll\n    for (int i = 0; i < vdr; ++i) {\n        // SIMD dot product of quantized values\n        sumi = __dp4a(v[i], u[i], sumi);\n    }\n\n#ifdef GGML_CUDA_F16\n    const float2 tmp = __half22float2(__hmul2(dm8, ds8));\n    const float d8d8 = tmp.x;\n    const float m8s8 = tmp.y;\n#else\n    const float2 dm8f = __half22float2(dm8);\n    const float2 ds8f = __half22float2(ds8);\n    const float d8d8 = dm8f.x * ds8f.x;\n    const float m8s8 = dm8f.y * ds8f.y;\n#endif // GGML_CUDA_F16\n\n    // scale second part of sum by QI8_1/ vdr to compensate for multiple threads adding it\n    return sumi*d8d8 + m8s8 / (QI8_1 / vdr);\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q2_K_Q8_1_MMVQ 1\n#define VDR_Q2_K_Q8_1_MMQ  2\n\n// contiguous v/x values\nstatic __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmvq(\n    const int & v, const int * __restrict__ u, const uint8_t * __restrict__ scales,\n    const half2 & dm2, const float * __restrict__ d8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    float sumf_d = 0.0f;\n    float sumf_m = 0.0f;\n\n#pragma unroll\n    for (int i = 0; i < QR2_K; ++i) {\n        const int sc = scales[2*i];\n\n        const int vi = (v >> (2*i)) & 0x03030303;\n\n        sumf_d += d8[i] * (__dp4a(vi, u[i], 0) * (sc & 0xF)); // SIMD dot product\n\n        // fill int with 4x m\n        int m = sc >> 4;\n        m |= m <<  8;\n        m |= m << 16;\n        sumf_m += d8[i] * __dp4a(m, u[i], 0); // multiply constant q2_K part with sum of q8_1 values\n    }\n\n    const float2 dm2f = __half22float2(dm2);\n\n    return dm2f.x*sumf_d - dm2f.y*sumf_m;\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n// contiguous u/y values\nstatic __device__ __forceinline__ float vec_dot_q2_K_q8_1_impl_mmq(\n    const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ scales,\n    const half2 & dm2, const float & d8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    int sumi_d = 0;\n    int sumi_m = 0;\n\n#pragma unroll\n    for (int i0 = 0; i0 < QI8_1; i0 += QI8_1/2) {\n        int sumi_d_sc = 0;\n\n        const int sc = scales[i0 / (QI8_1/2)];\n\n        // fill int with 4x m\n        int m = sc >> 4;\n        m |= m <<  8;\n        m |= m << 16;\n\n#pragma unroll\n        for (int i = i0; i < i0 + QI8_1/2; ++i) {\n            sumi_d_sc = __dp4a(v[i], u[i], sumi_d_sc); // SIMD dot product\n            sumi_m    = __dp4a(m,    u[i], sumi_m); // multiply sum of q8_1 values with m\n        }\n\n        sumi_d += sumi_d_sc * (sc & 0xF);\n    }\n\n    const float2 dm2f = __half22float2(dm2);\n\n    return d8 * (dm2f.x*sumi_d - dm2f.y*sumi_m);\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q3_K_Q8_1_MMVQ 1\n#define VDR_Q3_K_Q8_1_MMQ  2\n\n// contiguous v/x values\nstatic __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmvq(\n    const int & vl, const int & vh, const int * __restrict__ u, const uint8_t * __restrict__ scales,\n    const int & scale_offset, const float & d3, const float * __restrict__ d8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    float sumf = 0.0f;\n\n#pragma unroll\n    for (int i = 0; i < QR3_K; ++i) {\n        const int isc = scale_offset + 2*i;\n\n        const int isc_low = isc % (QK_K/32);\n        const int sc_shift_low = 4 * (isc / (QK_K/32));\n        const int sc_low  = (scales[isc_low] >> sc_shift_low) & 0xF;\n\n        const int isc_high = isc % (QK_K/64);\n        const int sc_shift_high = 2 * (isc / (QK_K/64));\n        const int sc_high = ((scales[(QK_K/32) + isc_high] >> sc_shift_high) & 3) << 4;\n\n        const int sc = (sc_low | sc_high) - 32;\n\n        const int vil = (vl >> (2*i)) & 0x03030303;\n\n        const int vih = ((vh >> i) << 2) & 0x04040404;\n\n        const int vi = __vsubss4(vil, vih);\n\n        sumf += d8[i] * (__dp4a(vi, u[i], 0) * sc); // SIMD dot product\n    }\n\n    return d3 * sumf;\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n// contiguous u/y values\nstatic __device__ __forceinline__ float vec_dot_q3_K_q8_1_impl_mmq(\n    const int * __restrict__ v, const int * __restrict__ u, const int8_t * __restrict__ scales,\n    const float & d3, const float & d8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    int sumi = 0;\n\n#pragma unroll\n    for (int i0 = 0; i0 < QR3_K*VDR_Q3_K_Q8_1_MMQ; i0 += QI8_1/2) {\n        int sumi_sc = 0;\n\n        for (int i = i0; i < i0 + QI8_1/2; ++i) {\n            sumi_sc = __dp4a(v[i], u[i], sumi_sc); // SIMD dot product\n        }\n\n        sumi += sumi_sc * scales[i0 / (QI8_1/2)];\n    }\n\n    return d3*d8 * sumi;\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q4_K_Q8_1_MMVQ 2\n#define VDR_Q4_K_Q8_1_MMQ  8\n\n// contiguous v/x values\nstatic __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_vmmq(\n    const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,\n    const uint8_t * __restrict__ m, const half2 & dm4, const float * __restrict__ d8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    float sumf_d = 0.0f;\n    float sumf_m = 0.0f;\n\n#pragma unroll\n    for (int i = 0; i < QR4_K; ++i) {\n        const int v0i = (v[0] >> (4*i)) & 0x0F0F0F0F;\n        const int v1i = (v[1] >> (4*i)) & 0x0F0F0F0F;\n\n        const int dot1 = __dp4a(v1i, u[2*i+1], __dp4a(v0i, u[2*i+0], 0)); // SIMD dot product\n        const int dot2 = __dp4a(0x01010101, u[2*i+1], __dp4a(0x01010101, u[2*i+0], 0)); // sum of u\n\n        sumf_d += d8[i] * (dot1 * sc[i]);\n        sumf_m += d8[i] * (dot2 * m[i]);  // multiply constant part of q4_K with sum of q8_1 values\n    }\n\n    const float2 dm4f = __half22float2(dm4);\n\n    return dm4f.x*sumf_d - dm4f.y*sumf_m;\n\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n// contiguous u/y values\nstatic __device__ __forceinline__ float vec_dot_q4_K_q8_1_impl_mmq(\n    const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,\n    const uint8_t * __restrict__ m, const half2 & dm4, const half2 * __restrict__ ds8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    float sumf_d = 0.0f;\n    float sumf_m = 0.0f;\n\n#pragma unroll\n    for (int i = 0; i < QR4_K*VDR_Q4_K_Q8_1_MMQ/QI8_1; ++i) {\n        int sumi_d = 0;\n\n#pragma unroll\n        for (int j = 0; j < QI8_1; ++j) {\n            sumi_d = __dp4a((v[j] >> (4*i)) & 0x0F0F0F0F, u[i*QI8_1 + j], sumi_d); // SIMD dot product\n        }\n\n        const float2 ds8f = __half22float2(ds8[i]);\n\n        sumf_d += ds8f.x * (sc[i] * sumi_d);\n        sumf_m += ds8f.y *   m[i]; // sum of q8_1 block * q4_K min val\n    }\n\n    const float2 dm4f = __half22float2(dm4);\n\n    return dm4f.x*sumf_d - dm4f.y*sumf_m;\n\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q5_K_Q8_1_MMVQ 2\n#define VDR_Q5_K_Q8_1_MMQ  8\n\n// contiguous v/x values\nstatic __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_vmmq(\n    const int * __restrict__ vl, const int * __restrict__ vh, const int * __restrict__ u, const uint8_t * __restrict__ sc,\n    const uint8_t * __restrict__ m, const half2 & dm5, const float * __restrict__ d8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    float sumf_d = 0.0f;\n    float sumf_m = 0.0f;\n\n#pragma unroll\n    for (int i = 0; i < QR5_K; ++i) {\n        const int vl0i = (vl[0] >> (4*i)) & 0x0F0F0F0F;\n        const int vl1i = (vl[1] >> (4*i)) & 0x0F0F0F0F;\n\n        const int vh0i = ((vh[0] >> i) << 4) & 0x10101010;\n        const int vh1i = ((vh[1] >> i) << 4) & 0x10101010;\n\n        const int v0i = vl0i | vh0i;\n        const int v1i = vl1i | vh1i;\n\n        const int dot1 = __dp4a(v0i, u[2*i+0], __dp4a(v1i, u[2*i+1], 0)); // SIMD dot product\n        const int dot2 = __dp4a(0x01010101, u[2*i+0], __dp4a(0x01010101, u[2*i+1], 0)); // sum of u\n\n        sumf_d += d8[i] * (dot1 * sc[i]);\n        sumf_m += d8[i] * (dot2 * m[i]);\n\n    }\n\n    const float2 dm5f = __half22float2(dm5);\n\n    return dm5f.x*sumf_d - dm5f.y*sumf_m;\n\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n// contiguous u/y values\nstatic __device__ __forceinline__ float vec_dot_q5_K_q8_1_impl_mmq(\n    const int * __restrict__ v, const int * __restrict__ u, const uint8_t * __restrict__ sc,\n    const uint8_t * __restrict__ m, const half2 & dm4, const half2 * __restrict__ ds8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    float sumf_d = 0.0f;\n    float sumf_m = 0.0f;\n\n#pragma unroll\n    for (int i = 0; i < QR5_K*VDR_Q5_K_Q8_1_MMQ/QI8_1; ++i) {\n        int sumi_d = 0;\n\n#pragma unroll\n        for (int j = 0; j < QI8_1; ++j) {\n            sumi_d = __dp4a(v[i*QI8_1 + j], u[i*QI8_1 + j], sumi_d); // SIMD dot product\n        }\n\n        const float2 ds8f = __half22float2(ds8[i]);\n\n        sumf_d += ds8f.x * (sc[i] * sumi_d);\n        sumf_m += ds8f.y *   m[i]; // sum of q8_1 block * q4_K min val\n    }\n\n    const float2 dm4f = __half22float2(dm4);\n\n    return dm4f.x*sumf_d - dm4f.y*sumf_m;\n\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n#define VDR_Q6_K_Q8_1_MMVQ 1\n#define VDR_Q6_K_Q8_1_MMQ  8\n\n// contiguous v/x values\nstatic __device__ __forceinline__ float vec_dot_q6_K_q8_1_impl_mmvq(\n    const int & vl, const int & vh, const int * __restrict__ u, const int8_t * __restrict__ scales,\n    const float & d, const float * __restrict__ d8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    float sumf = 0.0f;\n\n#pragma unroll\n    for (int i = 0; i < QR6_K; ++i) {\n        const int sc = scales[4*i];\n\n        const int vil = (vl >> (4*i)) & 0x0F0F0F0F;\n\n        const int vih = ((vh >> (4*i)) << 4) & 0x30303030;\n\n        const int vi = __vsubss4((vil | vih), 0x20202020); // vi = (vil | vih) - 32\n\n        sumf += d8[i] * (__dp4a(vi, u[i], 0) * sc); // SIMD dot product\n    }\n\n    return d*sumf;\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\n// contiguous u/y values\nstatic __device__ __forceinline__ float vec_dot_q6_K_q8_1_impl_mmq(\n    const int * __restrict__ v, const int * __restrict__ u, const int8_t * __restrict__ sc,\n    const float & d6, const float * __restrict__ d8) {\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    float sumf_d = 0.0f;\n\n#pragma unroll\n    for (int i0 = 0; i0 < VDR_Q6_K_Q8_1_MMQ; i0 += 4) {\n        int2 sumi_d = {0, 0}; // 2 q6_K scales per q8_1 scale\n\n#pragma unroll\n        for (int i = i0; i < i0 + 2; ++i) {\n            sumi_d.x = __dp4a(v[2*i+0], u[2*i+0], sumi_d.x); // SIMD dot product\n            sumi_d.x = __dp4a(v[2*i+1], u[2*i+1], sumi_d.x); // SIMD dot product\n\n            sumi_d.y = __dp4a(v[2*i+4], u[2*i+4], sumi_d.y); // SIMD dot product\n            sumi_d.y = __dp4a(v[2*i+5], u[2*i+5], sumi_d.y); // SIMD dot product\n        }\n\n        sumf_d += d8[i0/4] * (sc[i0/2+0]*sumi_d.x + sc[i0/2+1]*sumi_d.y);\n    }\n\n    return d6 * sumf_d;\n\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n}\n\nstatic __device__ __forceinline__ float vec_dot_q4_0_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n    const block_q4_0 * bq4_0 = (const block_q4_0 *) vbq;\n\n    int v[VDR_Q4_0_Q8_1_MMVQ];\n    int u[2*VDR_Q4_0_Q8_1_MMVQ];\n\n#pragma unroll\n    for (int i = 0; i < VDR_Q4_0_Q8_1_MMVQ; ++i) {\n        v[i]     = get_int_from_uint8(bq4_0->qs, iqs + i);\n        u[2*i+0] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);\n        u[2*i+1] = get_int_from_int8_aligned(bq8_1->qs, iqs + i + QI4_0);\n    }\n\n    return vec_dot_q4_0_q8_1_impl<VDR_Q4_0_Q8_1_MMVQ>(v, u, bq4_0->d, bq8_1->ds);\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q4_0(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int  tile_x_qs[mmq_y * (WARP_SIZE)       + mmq_y];\n    __shared__ float tile_x_d[mmq_y * (WARP_SIZE/QI4_0) + mmq_y/QI4_0];\n\n    *x_ql = tile_x_qs;\n    *x_dm = (half2 *) tile_x_d;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_0(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI4_0;\n    const int kqsx = k % QI4_0;\n\n    const block_q4_0 * bx0 = (block_q4_0 *) vx;\n\n    float * x_dmf = (float *) x_dm;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q4_0 * bxi = bx0 + i*blocks_per_row + kbx;\n\n        x_ql[i * (WARP_SIZE + 1) + k] = get_int_from_uint8(bxi->qs, kqsx);\n        // x_dmf[i * (WARP_SIZE/QI4_0) + i / QI4_0 + kbx] = bxi->d;\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI4_0;\n    const int kbxd = k % blocks_per_tile_x_row;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI4_0) {\n        int i = i0 + i_offset * QI4_0 + k / blocks_per_tile_x_row;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q4_0 * bxi = bx0 + i*blocks_per_row + kbxd;\n\n        x_dmf[i * (WARP_SIZE/QI4_0) + i / QI4_0 + kbxd] = bxi->d;\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q4_0_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const int kyqs = k % (QI8_1/2) + QI8_1 * (k / (QI8_1/2));\n    const float * x_dmf = (float *) x_dm;\n\n    int u[2*VDR_Q4_0_Q8_1_MMQ];\n\n#pragma unroll\n    for (int l = 0; l < VDR_Q4_0_Q8_1_MMQ; ++l) {\n        u[2*l+0] = y_qs[j * WARP_SIZE + (kyqs + l)         % WARP_SIZE];\n        u[2*l+1] = y_qs[j * WARP_SIZE + (kyqs + l + QI4_0) % WARP_SIZE];\n    }\n\n    return vec_dot_q4_0_q8_1_impl<VDR_Q4_0_Q8_1_MMQ>\n        (&x_ql[i * (WARP_SIZE + 1) + k], u, x_dmf[i * (WARP_SIZE/QI4_0) + i/QI4_0 + k/QI4_0],\n         y_ds[j * (WARP_SIZE/QI8_1) + (2*k/QI8_1) % (WARP_SIZE/QI8_1)]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q4_1_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n    const block_q4_1 * bq4_1 = (const block_q4_1 *) vbq;\n\n    int v[VDR_Q4_1_Q8_1_MMVQ];\n    int u[2*VDR_Q4_1_Q8_1_MMVQ];\n\n#pragma unroll\n    for (int i = 0; i < VDR_Q4_1_Q8_1_MMVQ; ++i) {\n        v[i]    = get_int_from_uint8_aligned(bq4_1->qs, iqs + i);\n        u[2*i+0] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);\n        u[2*i+1] = get_int_from_int8_aligned(bq8_1->qs, iqs + i + QI4_1);\n    }\n\n    return vec_dot_q4_1_q8_1_impl<VDR_Q4_1_Q8_1_MMVQ>(v, u, bq4_1->dm, bq8_1->ds);\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q4_1(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int   tile_x_qs[mmq_y * (WARP_SIZE) +     + mmq_y];\n    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE/QI4_1) + mmq_y/QI4_1];\n\n    *x_ql = tile_x_qs;\n    *x_dm = tile_x_dm;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_1(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI4_1;\n    const int kqsx = k % QI4_1;\n\n    const block_q4_1 * bx0 = (block_q4_1 *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q4_1 * bxi = bx0 + i*blocks_per_row + kbx;\n\n        x_ql[i * (WARP_SIZE + 1) + k] = get_int_from_uint8_aligned(bxi->qs, kqsx);\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI4_1;\n    const int kbxd = k % blocks_per_tile_x_row;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI4_1) {\n        int i = i0 + i_offset * QI4_1 + k / blocks_per_tile_x_row;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q4_1 * bxi = bx0 + i*blocks_per_row + kbxd;\n\n        x_dm[i * (WARP_SIZE/QI4_1) + i / QI4_1 + kbxd] = bxi->dm;\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q4_1_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const int kyqs = k % (QI8_1/2) + QI8_1 * (k / (QI8_1/2));\n\n    int u[2*VDR_Q4_1_Q8_1_MMQ];\n\n#pragma unroll\n    for (int l = 0; l < VDR_Q4_1_Q8_1_MMQ; ++l) {\n        u[2*l+0] = y_qs[j * WARP_SIZE + (kyqs + l)         % WARP_SIZE];\n        u[2*l+1] = y_qs[j * WARP_SIZE + (kyqs + l + QI4_1) % WARP_SIZE];\n    }\n\n    return vec_dot_q4_1_q8_1_impl<VDR_Q4_1_Q8_1_MMQ>\n        (&x_ql[i * (WARP_SIZE + 1) + k], u, x_dm[i * (WARP_SIZE/QI4_1) + i/QI4_1 + k/QI4_1],\n         y_ds[j * (WARP_SIZE/QI8_1) + (2*k/QI8_1) % (WARP_SIZE/QI8_1)]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q5_0_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n    const block_q5_0 * bq5_0 = (const block_q5_0 *) vbq;\n\n    int vl[VDR_Q5_0_Q8_1_MMVQ];\n    int vh[VDR_Q5_0_Q8_1_MMVQ];\n    int  u[2*VDR_Q5_0_Q8_1_MMVQ];\n\n#pragma unroll\n    for (int i = 0; i < VDR_Q5_0_Q8_1_MMVQ; ++i) {\n        vl[i]    = get_int_from_uint8(bq5_0->qs, iqs + i);\n        vh[i]    = get_int_from_uint8(bq5_0->qh, 0) >> (4 * (iqs + i));\n        u[2*i+0] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);\n        u[2*i+1] = get_int_from_int8_aligned(bq8_1->qs, iqs + i + QI5_0);\n    }\n\n    return vec_dot_q5_0_q8_1_impl<VDR_Q5_0_Q8_1_MMVQ>(vl, vh, u, bq5_0->d, bq8_1->ds);\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q5_0(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int  tile_x_ql[mmq_y * (2*WARP_SIZE)     + mmq_y];\n    __shared__ float tile_x_d[mmq_y * (WARP_SIZE/QI5_0) + mmq_y/QI5_0];\n\n    *x_ql = tile_x_ql;\n    *x_dm = (half2 *) tile_x_d;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q5_0(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI5_0;\n    const int kqsx = k % QI5_0;\n\n    const block_q5_0 * bx0 = (block_q5_0 *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q5_0 * bxi = bx0 + i*blocks_per_row + kbx;\n\n        const int ql = get_int_from_uint8(bxi->qs, kqsx);\n        const int qh = get_int_from_uint8(bxi->qh, 0) >> (4 * (k % QI5_0));\n\n        int qs0 = (ql >>  0)   & 0x0F0F0F0F;\n        qs0    |= (qh <<  4)   & 0x00000010;  // 0 ->  4\n        qs0    |= (qh << 11)   & 0x00001000;  // 1 -> 12\n        qs0    |= (qh << 18)   & 0x00100000;  // 2 -> 20\n        qs0    |= (qh << 25)   & 0x10000000;  // 3 -> 28\n        qs0     = __vsubss4(qs0, 0x10101010); // subtract 16\n\n        x_ql[i * (2*WARP_SIZE + 1) + 2*k+0] = qs0;\n\n        int qs1 = (ql >>  4)   & 0x0F0F0F0F;\n        qs1    |= (qh >> 12)   & 0x00000010;  // 16 ->  4\n        qs1    |= (qh >>  5)   & 0x00001000;  // 17 -> 12\n        qs1    |= (qh <<  2)   & 0x00100000;  // 18 -> 20\n        qs1    |= (qh <<  9)   & 0x10000000;  // 19 -> 28\n        qs1     = __vsubss4(qs1, 0x10101010); // subtract 16\n\n        x_ql[i * (2*WARP_SIZE + 1) + 2*k+1] = qs1;\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI5_0;\n    const int kbxd = k % blocks_per_tile_x_row;\n    float * x_dmf = (float *) x_dm;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI5_0) {\n        int i = i0 + i_offset * QI5_0 + k / blocks_per_tile_x_row;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q5_0 * bxi = bx0 + i*blocks_per_row + kbxd;\n\n        x_dmf[i * (WARP_SIZE/QI5_0) + i / QI5_0 + kbxd] = bxi->d;\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q5_0_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const int kyqs = k % (QI8_1/2) + QI8_1 * (k / (QI8_1/2));\n    const int index_bx = i * (WARP_SIZE/QI5_0) + i/QI5_0 + k/QI5_0;\n    const float * x_dmf = (const float *) x_dm;\n    const float * y_df  = (const float *) y_ds;\n\n    int u[2*VDR_Q5_0_Q8_1_MMQ];\n\n#pragma unroll\n    for (int l = 0; l < VDR_Q5_0_Q8_1_MMQ; ++l) {\n        u[2*l+0] = y_qs[j * WARP_SIZE + (kyqs + l)         % WARP_SIZE];\n        u[2*l+1] = y_qs[j * WARP_SIZE + (kyqs + l + QI5_0) % WARP_SIZE];\n    }\n\n    return vec_dot_q8_0_q8_1_impl<QR5_0*VDR_Q5_0_Q8_1_MMQ>\n        (&x_ql[i * (2*WARP_SIZE + 1) + 2 * k], u, x_dmf[index_bx], y_df[j * (WARP_SIZE/QI8_1) + (2*k/QI8_1) % (WARP_SIZE/QI8_1)]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q5_1_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n    const block_q5_1 * bq5_1 = (const block_q5_1 *) vbq;\n\n    int vl[VDR_Q5_1_Q8_1_MMVQ];\n    int vh[VDR_Q5_1_Q8_1_MMVQ];\n    int  u[2*VDR_Q5_1_Q8_1_MMVQ];\n\n#pragma unroll\n    for (int i = 0; i < VDR_Q5_1_Q8_1_MMVQ; ++i) {\n        vl[i]   = get_int_from_uint8_aligned(bq5_1->qs, iqs + i);\n        vh[i]   = get_int_from_uint8_aligned(bq5_1->qh, 0) >> (4 * (iqs + i));\n        u[2*i+0] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);\n        u[2*i+1] = get_int_from_int8_aligned(bq8_1->qs, iqs + i + QI5_1);\n    }\n\n    return vec_dot_q5_1_q8_1_impl<VDR_Q5_1_Q8_1_MMVQ>(vl, vh, u, bq5_1->dm, bq8_1->ds);\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q5_1(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int   tile_x_ql[mmq_y * (2*WARP_SIZE)     + mmq_y];\n    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE/QI5_1) + mmq_y/QI5_1];\n\n    *x_ql = tile_x_ql;\n    *x_dm = tile_x_dm;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q5_1(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset < nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI5_1;\n    const int kqsx = k % QI5_1;\n\n    const block_q5_1 * bx0 = (block_q5_1 *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q5_1 * bxi = bx0 + i*blocks_per_row + kbx;\n\n        const int ql = get_int_from_uint8_aligned(bxi->qs, kqsx);\n        const int qh = get_int_from_uint8_aligned(bxi->qh, 0) >> (4 * (k % QI5_1));\n\n        int qs0 = (ql >>  0) & 0x0F0F0F0F;\n        qs0    |= (qh <<  4) & 0x00000010; // 0 ->  4\n        qs0    |= (qh << 11) & 0x00001000; // 1 -> 12\n        qs0    |= (qh << 18) & 0x00100000; // 2 -> 20\n        qs0    |= (qh << 25) & 0x10000000; // 3 -> 28\n\n        x_ql[i * (2*WARP_SIZE + 1) + 2*k+0] = qs0;\n\n        int qs1 = (ql >>  4) & 0x0F0F0F0F;\n        qs1    |= (qh >> 12) & 0x00000010; // 16 ->  4\n        qs1    |= (qh >>  5) & 0x00001000; // 17 -> 12\n        qs1    |= (qh <<  2) & 0x00100000; // 18 -> 20\n        qs1    |= (qh <<  9) & 0x10000000; // 19 -> 28\n\n        x_ql[i * (2*WARP_SIZE + 1) + 2*k+1] = qs1;\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI5_1;\n    const int kbxd = k % blocks_per_tile_x_row;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI5_1) {\n        int i = i0 + i_offset * QI5_1 + k / blocks_per_tile_x_row;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q5_1 * bxi = bx0 + i*blocks_per_row + kbxd;\n\n        x_dm[i * (WARP_SIZE/QI5_1) + i / QI5_1 + kbxd] = bxi->dm;\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q5_1_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const int kyqs = k % (QI8_1/2) + QI8_1 * (k / (QI8_1/2));\n    const int index_bx = i * (WARP_SIZE/QI5_1) + + i/QI5_1 + k/QI5_1;\n\n    int u[2*VDR_Q5_1_Q8_1_MMQ];\n\n#pragma unroll\n    for (int l = 0; l < VDR_Q5_1_Q8_1_MMQ; ++l) {\n        u[2*l+0] = y_qs[j * WARP_SIZE + (kyqs + l)         % WARP_SIZE];\n        u[2*l+1] = y_qs[j * WARP_SIZE + (kyqs + l + QI5_1) % WARP_SIZE];\n    }\n\n    return vec_dot_q8_1_q8_1_impl<QR5_1*VDR_Q5_1_Q8_1_MMQ>\n        (&x_ql[i * (2*WARP_SIZE + 1) + 2 * k], u, x_dm[index_bx], y_ds[j * (WARP_SIZE/QI8_1) + (2*k/QI8_1) % (WARP_SIZE/QI8_1)]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q8_0_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n    const block_q8_0 * bq8_0 = (const block_q8_0 *) vbq;\n\n    int v[VDR_Q8_0_Q8_1_MMVQ];\n    int u[VDR_Q8_0_Q8_1_MMVQ];\n\n#pragma unroll\n    for (int i = 0; i < VDR_Q8_0_Q8_1_MMVQ; ++i) {\n        v[i] = get_int_from_int8(bq8_0->qs, iqs + i);\n        u[i] = get_int_from_int8_aligned(bq8_1->qs, iqs + i);\n    }\n\n    return vec_dot_q8_0_q8_1_impl<VDR_Q8_0_Q8_1_MMVQ>(v, u, bq8_0->d, __low2half(bq8_1->ds));\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q8_0(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int  tile_x_qs[mmq_y * (WARP_SIZE)       + mmq_y];\n    __shared__ float tile_x_d[mmq_y * (WARP_SIZE/QI8_0) + mmq_y/QI8_0];\n\n    *x_ql = tile_x_qs;\n    *x_dm = (half2 *) tile_x_d;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q8_0(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI8_0;\n    const int kqsx = k % QI8_0;\n    float * x_dmf = (float *) x_dm;\n\n    const block_q8_0 * bx0 = (block_q8_0 *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q8_0 * bxi = bx0 + i*blocks_per_row + kbx;\n\n        x_ql[i * (WARP_SIZE + 1) + k] = get_int_from_int8(bxi->qs, kqsx);\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI8_0;\n    const int kbxd = k % blocks_per_tile_x_row;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI8_0) {\n        int i = i0 + i_offset * QI8_0 + k / blocks_per_tile_x_row;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q8_0 * bxi = bx0 + i*blocks_per_row + kbxd;\n\n        x_dmf[i * (WARP_SIZE/QI8_0) + i / QI8_0 + kbxd] = bxi->d;\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q8_0_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const float * x_dmf = (const float *) x_dm;\n    const float * y_df  = (const float *) y_ds;\n\n    return vec_dot_q8_0_q8_1_impl<VDR_Q8_0_Q8_1_MMQ>\n        (&x_ql[i * (WARP_SIZE + 1) + k], &y_qs[j * WARP_SIZE + k], x_dmf[i * (WARP_SIZE/QI8_0) + i/QI8_0 + k/QI8_0],\n         y_df[j * (WARP_SIZE/QI8_1) + k/QI8_1]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q2_K_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n    const block_q2_K * bq2_K = (const block_q2_K *) vbq;\n\n    const int bq8_offset = QR2_K * (iqs / QI8_1);\n    const int scale_offset = iqs - iqs % QI8_1 + (iqs % QI8_1) / (QI8_1/2);\n\n    const uint8_t * scales = bq2_K->scales + scale_offset;\n\n    const int v = get_int_from_uint8_aligned(bq2_K->qs, iqs);\n    int    u[QR2_K];\n    float d8[QR2_K];\n\n#pragma unroll\n    for (int i = 0; i < QR2_K; ++ i) {\n        u[i]  = get_int_from_int8_aligned(bq8_1[bq8_offset + i].qs, iqs % QI8_1);\n        d8[i] = __low2half(bq8_1[bq8_offset + i].ds);\n    }\n\n    return vec_dot_q2_K_q8_1_impl_mmvq(v, u, scales, bq2_K->dm, d8);\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q2_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int   tile_x_ql[mmq_y * (WARP_SIZE)       + mmq_y];\n    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE/QI2_K) + mmq_y/QI2_K];\n    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE/4)     + mmq_y/4];\n\n    *x_ql = tile_x_ql;\n    *x_dm = tile_x_dm;\n    *x_sc = tile_x_sc;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q2_K(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI2_K;\n    const int kqsx = k % QI2_K;\n\n    const block_q2_K * bx0 = (block_q2_K *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q2_K * bxi = bx0 + i*blocks_per_row + kbx;\n\n        x_ql[i * (WARP_SIZE + 1) + k] = get_int_from_uint8_aligned(bxi->qs, kqsx);\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI2_K;\n    const int kbxd = k % blocks_per_tile_x_row;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI2_K) {\n        int i = (i0 + i_offset * QI2_K + k / blocks_per_tile_x_row) % mmq_y;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q2_K * bxi = bx0 + i*blocks_per_row + kbxd;\n\n        x_dm[i * (WARP_SIZE/QI2_K) + i / QI2_K + kbxd] = bxi->dm;\n    }\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 4) {\n        int i = i0 + i_offset * 4 + k / (WARP_SIZE/4);\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q2_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE/4)) / (QI2_K/4);\n\n        x_sc[i * (WARP_SIZE/4) + i / 4 + k % (WARP_SIZE/4)] = get_int_from_uint8_aligned(bxi->scales, k % (QI2_K/4));\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q2_K_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const int kbx = k / QI2_K;\n    const int ky  = (k % QI2_K) * QR2_K;\n    const float * y_df = (const float *) y_ds;\n\n    int v[QR2_K*VDR_Q2_K_Q8_1_MMQ];\n\n    const int kqsx = i * (WARP_SIZE + 1) + kbx*QI2_K + (QI2_K/2) * (ky/(2*QI2_K)) + ky % (QI2_K/2);\n    const int shift = 2 * ((ky % (2*QI2_K)) / (QI2_K/2));\n\n#pragma unroll\n    for (int l = 0; l < QR2_K*VDR_Q2_K_Q8_1_MMQ; ++l) {\n        v[l] = (x_ql[kqsx + l] >> shift) & 0x03030303;\n    }\n\n    const uint8_t * scales = ((const uint8_t *) &x_sc[i * (WARP_SIZE/4) + i/4 + kbx*4]) + ky/4;\n\n    const int index_y = j * WARP_SIZE + (QR2_K*k) % WARP_SIZE;\n    return vec_dot_q2_K_q8_1_impl_mmq(v, &y_qs[index_y], scales, x_dm[i * (WARP_SIZE/QI2_K) + i/QI2_K + kbx], y_df[index_y/QI8_1]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q3_K_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n    const block_q3_K * bq3_K = (const block_q3_K *) vbq;\n\n    const int bq8_offset = QR3_K * (iqs / (QI3_K/2));\n    const int scale_offset = iqs - iqs % QI8_1 + (iqs % QI8_1) / (QI8_1/2);\n\n    const float d = bq3_K->d;\n\n    const int vl = get_int_from_uint8(bq3_K->qs, iqs);\n\n    // invert the mask with ~ so that a 0/1 results in 4/0 being subtracted\n    const int vh = ~get_int_from_uint8(bq3_K->hmask, iqs % (QI3_K/2)) >> bq8_offset;\n\n    int    u[QR3_K];\n    float d8[QR3_K];\n\n#pragma unroll\n    for (int i = 0; i < QR3_K; ++i) {\n        u[i]  = get_int_from_int8_aligned(bq8_1[bq8_offset + i].qs, iqs % QI8_1);\n        d8[i] = __low2half(bq8_1[bq8_offset + i].ds);\n    }\n\n    return vec_dot_q3_K_q8_1_impl_mmvq(vl, vh, u, bq3_K->scales, scale_offset, d, d8);\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q3_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int   tile_x_ql[mmq_y * (WARP_SIZE)       + mmq_y];\n    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE/QI3_K) + mmq_y/QI3_K];\n    __shared__ int   tile_x_qh[mmq_y * (WARP_SIZE/2)     + mmq_y/2];\n    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE/4)     + mmq_y/4];\n\n    *x_ql = tile_x_ql;\n    *x_dm = tile_x_dm;\n    *x_qh = tile_x_qh;\n    *x_sc = tile_x_sc;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q3_K(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI3_K;\n    const int kqsx = k % QI3_K;\n\n    const block_q3_K * bx0 = (block_q3_K *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q3_K * bxi = bx0 + i*blocks_per_row + kbx;\n\n        x_ql[i * (WARP_SIZE + 1) + k] = get_int_from_uint8(bxi->qs, kqsx);\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI3_K;\n    const int kbxd = k % blocks_per_tile_x_row;\n    float * x_dmf = (float *) x_dm;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI3_K) {\n        int i = (i0 + i_offset * QI3_K + k / blocks_per_tile_x_row) % mmq_y;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q3_K * bxi = bx0 + i*blocks_per_row + kbxd;\n\n        x_dmf[i * (WARP_SIZE/QI3_K) + i / QI3_K + kbxd] = bxi->d;\n    }\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 2) {\n        int i = i0 + i_offset * 2 + k / (WARP_SIZE/2);\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q3_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE/2)) / (QI3_K/2);\n\n        // invert the mask with ~ so that a 0/1 results in 4/0 being subtracted\n        x_qh[i * (WARP_SIZE/2) + i / 2 + k % (WARP_SIZE/2)] = ~get_int_from_uint8(bxi->hmask, k % (QI3_K/2));\n    }\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 4) {\n        int i = i0 + i_offset * 4 + k / (WARP_SIZE/4);\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q3_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE/4)) / (QI3_K/4);\n\n        const int ksc = k % (QI3_K/4);\n\n        const int ksc_low = ksc % (QI3_K/8);\n        const int shift_low = 4 * (ksc / (QI3_K/8));\n        const int sc_low = (get_int_from_uint8(bxi->scales, ksc_low) >> shift_low) & 0x0F0F0F0F;\n\n        const int ksc_high = QI3_K/8;\n        const int shift_high = 2 * ksc;\n        const int sc_high = ((get_int_from_uint8(bxi->scales, ksc_high) >> shift_high) << 4) & 0x30303030;\n\n        const int sc = __vsubss4(sc_low | sc_high, 0x20202020);\n\n        x_sc[i * (WARP_SIZE/4) + i / 4 + k % (WARP_SIZE/4)] = sc;\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q3_K_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const int kbx  = k / QI3_K;\n    const int ky  = (k % QI3_K) * QR3_K;\n    const float * x_dmf = (const float *) x_dm;\n    const float * y_df  = (const float *) y_ds;\n\n    const int8_t * scales = ((int8_t *) (x_sc + i * (WARP_SIZE/4) + i/4 + kbx*4)) + ky/4;\n\n    int v[QR3_K*VDR_Q3_K_Q8_1_MMQ];\n\n#pragma unroll\n    for (int l = 0; l < QR3_K*VDR_Q3_K_Q8_1_MMQ; ++l) {\n        const int kqsx = i * (WARP_SIZE + 1) + kbx*QI3_K + (QI3_K/2) * (ky/(2*QI3_K)) + ky % (QI3_K/2);\n        const int shift = 2 * ((ky % 32) / 8);\n        const int vll = (x_ql[kqsx + l] >> shift) & 0x03030303;\n\n        const int vh = x_qh[i * (WARP_SIZE/2) + i/2 + kbx * (QI3_K/2) + (ky+l)%8] >> ((ky+l) / 8);\n        const int vlh = (vh << 2) & 0x04040404;\n\n        v[l] = __vsubss4(vll, vlh);\n    }\n\n    const int index_y = j * WARP_SIZE + (k*QR3_K) % WARP_SIZE;\n    return vec_dot_q3_K_q8_1_impl_mmq(v, &y_qs[index_y], scales, x_dmf[i * (WARP_SIZE/QI3_K) + i/QI3_K + kbx], y_df[index_y/QI8_1]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q4_K_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n#ifndef GGML_QKK_64\n    const block_q4_K * bq4_K = (const block_q4_K *) vbq;\n\n    int    v[2];\n    int    u[2*QR4_K];\n    float d8[QR4_K];\n\n    // iqs is in 0,2..30. bq8_offset = iqs/4 -> bq8_offset = 0, 2, 4, 6\n    const int bq8_offset = QR4_K * ((iqs/2) / (QI8_1/2));\n\n    // iqs = 0....3 -> bq8_offset = 0, want q4_offset = 0, 4, 8, 12\n    // iqs = 4....7 -> bq8_offset = 2, want q4_offset = 32, 36, 40, 44\n    // iqs = 8...11 -> bq8_offset = 4, want q4_offset = 64, 68, 72, 76\n    // iqs = 12..15 -> bq8_offset = 6, want q4_offset = 96, 100, 104, 108\n\n    const int * q4 = (const int *)(bq4_K->qs + 16 * bq8_offset + 4 * ((iqs/2)%4));\n    v[0] = q4[0];\n    v[1] = q4[4];\n\n    const uint16_t * scales = (const uint16_t *)bq4_K->scales;\n    uint16_t aux[2];\n    const int j = bq8_offset/2;\n    if (j < 2) {\n        aux[0] = scales[j+0] & 0x3f3f;\n        aux[1] = scales[j+2] & 0x3f3f;\n    } else {\n        aux[0] = ((scales[j+2] >> 0) & 0x0f0f) | ((scales[j-2] & 0xc0c0) >> 2);\n        aux[1] = ((scales[j+2] >> 4) & 0x0f0f) | ((scales[j-0] & 0xc0c0) >> 2);\n    }\n    const uint8_t * sc = (const uint8_t *)aux;\n    const uint8_t * m  = sc + 2;\n\n    for (int i = 0; i < QR4_K; ++i) {\n        const block_q8_1 * bq8i = bq8_1 + bq8_offset + i;\n        d8[i] = __low2half(bq8i->ds);\n\n        const int * q8 = (const int *)bq8i->qs + ((iqs/2)%4);\n        u[2*i+0] = q8[0];\n        u[2*i+1] = q8[4];\n    }\n\n    return vec_dot_q4_K_q8_1_impl_vmmq(v, u, sc, m, bq4_K->dm, d8);\n\n#else\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    const block_q4_K * bq4_K = (const block_q4_K *) vbq;\n\n    float sumf_d = 0.0f;\n    float sumf_m = 0.0f;\n\n    uint16_t aux16[2];\n    const uint8_t * s = (const uint8_t *)aux16;\n\n    const uint16_t * a = (const uint16_t *)bq4_K->scales;\n    aux16[0] = a[0] & 0x0f0f;\n    aux16[1] = (a[0] >> 4) & 0x0f0f;\n\n    const float dall = bq4_K->dm[0];\n    const float dmin = bq4_K->dm[1];\n\n    const float d8_1 = __low2float(bq8_1[0].ds);\n    const float d8_2 = __low2float(bq8_1[1].ds);\n\n    const int ui1 = *((const int *)bq8_1[0].qs + (iqs/2));\n    const int ui2 = *((const int *)bq8_1[0].qs + (iqs/2) + 4);\n    const int ui3 = *((const int *)bq8_1[1].qs + (iqs/2));\n    const int ui4 = *((const int *)bq8_1[1].qs + (iqs/2) + 4);\n\n    const int * q4 = (const int *)bq4_K->qs + (iqs/2);\n    const int v1 = q4[0];\n    const int v2 = q4[4];\n\n    const int dot1 = __dp4a(ui2, v2 & 0x0f0f0f0f, __dp4a(ui1, v1 & 0x0f0f0f0f, 0));\n    const int dot2 = __dp4a(ui4, (v2 >> 4) & 0x0f0f0f0f, __dp4a(ui3, (v1 >> 4) & 0x0f0f0f0f, 0));\n    const int dot3 = __dp4a(0x01010101, ui2, __dp4a(0x01010101, ui1, 0));\n    const int dot4 = __dp4a(0x01010101, ui4, __dp4a(0x01010101, ui3, 0));\n\n    sumf_d += d8_1 * (dot1 * s[0]) + d8_2 * (dot2 * s[1]);\n    sumf_m += d8_1 * (dot3 * s[2]) + d8_2 * (dot4 * s[3]);\n\n    return dall * sumf_d - dmin * sumf_m;\n\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n\n#endif\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q4_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int   tile_x_ql[mmq_y * (WARP_SIZE)       + mmq_y];\n    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE/QI4_K) + mmq_y/QI4_K];\n    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE/8)     + mmq_y/8];\n\n    *x_ql = tile_x_ql;\n    *x_dm = tile_x_dm;\n    *x_sc = tile_x_sc;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q4_K(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI4_K; // == 0 if QK_K == 256\n    const int kqsx = k % QI4_K; // == k if QK_K == 256\n\n    const block_q4_K * bx0 = (block_q4_K *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q4_K * bxi = bx0 + i*blocks_per_row + kbx;\n\n        x_ql[i * (WARP_SIZE + 1) + k] = get_int_from_uint8_aligned(bxi->qs, kqsx);\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI4_K; // == 1 if QK_K == 256\n    const int kbxd = k % blocks_per_tile_x_row;          // == 0 if QK_K == 256\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI4_K) {\n        int i = (i0 + i_offset * QI4_K + k / blocks_per_tile_x_row) % mmq_y;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q4_K * bxi = bx0 + i*blocks_per_row + kbxd;\n\n#if QK_K == 256\n        x_dm[i * (WARP_SIZE/QI4_K) + i / QI4_K + kbxd] = bxi->dm;\n#else\n        x_dm[i * (WARP_SIZE/QI4_K) + i / QI4_K + kbxd] = {bxi->dm[0], bxi->dm[1]};\n#endif\n    }\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 8) {\n        int i = (i0 + i_offset * 8 + k / (WARP_SIZE/8)) % mmq_y;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q4_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE/8)) / (QI4_K/8);\n\n        const int * scales = (int *) bxi->scales;\n\n        const int ksc = k % (WARP_SIZE/8);\n\n        // scale arrangement after the following two lines: sc0,...,sc3, sc4,...,sc7, m0,...,m3, m4,...,m8\n        int scales8 = (scales[(ksc%2) + (ksc!=0)] >> (4 * (ksc & (ksc/2)))) & 0x0F0F0F0F; // lower 4 bits\n        scales8    |= (scales[ksc/2]              >> (2 * (ksc % 2)))       & 0x30303030; // upper 2 bits\n\n        x_sc[i * (WARP_SIZE/8) + i / 8 + ksc] = scales8;\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q4_K_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const uint8_t * sc = ((const uint8_t *) &x_sc[i * (WARP_SIZE/8) + i/8 + k/16]) + 2*((k % 16) / 8);\n\n    const int index_y = j * WARP_SIZE + (QR4_K*k) % WARP_SIZE;\n    return vec_dot_q4_K_q8_1_impl_mmq(&x_ql[i * (WARP_SIZE + 1) + k], &y_qs[index_y], sc, sc+8,\n                                      x_dm[i * (WARP_SIZE/QI4_K) + i/QI4_K], &y_ds[index_y/QI8_1]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q5_K_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n#ifndef GGML_QKK_64\n    const block_q5_K * bq5_K = (const block_q5_K *) vbq;\n\n    int   vl[2];\n    int   vh[2];\n    int    u[2*QR5_K];\n    float d8[QR5_K];\n\n    const int bq8_offset = QR5_K * ((iqs/2) / (QI8_1/2));\n    const int * ql = (const int *)(bq5_K->qs + 16 * bq8_offset + 4 * ((iqs/2)%4));\n    const int * qh = (const int *)(bq5_K->qh + 4 * ((iqs/2)%4));\n\n    vl[0] = ql[0];\n    vl[1] = ql[4];\n\n    vh[0] = qh[0] >> bq8_offset;\n    vh[1] = qh[4] >> bq8_offset;\n\n    const uint16_t * scales = (const uint16_t *)bq5_K->scales;\n    uint16_t aux[2];\n    const int j = bq8_offset/2;\n    if (j < 2) {\n        aux[0] = scales[j+0] & 0x3f3f;\n        aux[1] = scales[j+2] & 0x3f3f;\n    } else {\n        aux[0] = ((scales[j+2] >> 0) & 0x0f0f) | ((scales[j-2] & 0xc0c0) >> 2);\n        aux[1] = ((scales[j+2] >> 4) & 0x0f0f) | ((scales[j-0] & 0xc0c0) >> 2);\n    }\n    const uint8_t * sc = (const uint8_t *)aux;\n    const uint8_t * m  = sc + 2;\n\n#pragma unroll\n    for (int i = 0; i < QR5_K; ++i) {\n        const block_q8_1 * bq8i = bq8_1 + bq8_offset + i;\n        d8[i] = __low2float(bq8i->ds);\n\n        const int * q8 = (const int *)bq8i->qs + ((iqs/2)%4);\n        u[2*i+0] = q8[0];\n        u[2*i+1] = q8[4];\n    }\n\n    return vec_dot_q5_K_q8_1_impl_vmmq(vl, vh, u, sc, m, bq5_K->dm, d8);\n\n#else\n\n#if __CUDA_ARCH__ >= MIN_CC_DP4A // lowest compute capability for integer intrinsics\n    const block_q5_K * bq5_K = (const block_q5_K *) vbq;\n\n    const int8_t * s = bq5_K->scales;\n\n    const float d = bq5_K->d;\n\n    const float d8_1 = __low2half(bq8_1[0].ds);\n    const float d8_2 = __low2half(bq8_1[1].ds);\n\n    const int ui1 = *((const int *)bq8_1[0].qs + (iqs/2));\n    const int ui2 = *((const int *)bq8_1[0].qs + (iqs/2) + 4);\n    const int ui3 = *((const int *)bq8_1[1].qs + (iqs/2));\n    const int ui4 = *((const int *)bq8_1[1].qs + (iqs/2) + 4);\n\n    const int * ql = (const int *)bq5_K->qs + (iqs/2);\n    const int vl1 = ql[0];\n    const int vl2 = ql[4];\n\n    const int step = 4 * (iqs/2); // 0, 4, 8, 12\n    const int im = step/8; // = 0 for iqs = 0, 2, = 1 for iqs = 4, 6\n    const int in = step%8; // 0, 4, 0, 4\n    const int vh = (*((const int *)(bq5_K->qh + in))) >> im;\n\n    const int v1 = (((vh << 4) & 0x10101010) ^ 0x10101010) | ((vl1 >> 0) & 0x0f0f0f0f);\n    const int v2 = (((vh << 2) & 0x10101010) ^ 0x10101010) | ((vl2 >> 0) & 0x0f0f0f0f);\n    const int v3 = (((vh >> 0) & 0x10101010) ^ 0x10101010) | ((vl1 >> 4) & 0x0f0f0f0f);\n    const int v4 = (((vh >> 2) & 0x10101010) ^ 0x10101010) | ((vl2 >> 4) & 0x0f0f0f0f);\n\n    const float sumf_d = d8_1 * (__dp4a(ui1, v1, 0) * s[0] + __dp4a(ui2, v2, 0) * s[1])\n                       + d8_2 * (__dp4a(ui3, v3, 0) * s[2] + __dp4a(ui4, v4, 0) * s[3]);\n\n    return d * sumf_d;\n\n#else\n    assert(false);\n    return 0.0f; // only to satisfy the compiler\n#endif // __CUDA_ARCH__ >= MIN_CC_DP4A\n\n#endif\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q5_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int   tile_x_ql[mmq_y * (2*WARP_SIZE)     + mmq_y];\n    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE/QI5_K) + mmq_y/QI5_K];\n    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE/8)     + mmq_y/8];\n\n    *x_ql = tile_x_ql;\n    *x_dm = tile_x_dm;\n    *x_sc = tile_x_sc;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q5_K(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI5_K; // == 0 if QK_K == 256\n    const int kqsx = k % QI5_K; // == k if QK_K == 256\n\n    const block_q5_K * bx0 = (block_q5_K *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q5_K * bxi = bx0 + i*blocks_per_row + kbx;\n        const int ky = QR5_K*kqsx;\n\n        const int ql = get_int_from_uint8_aligned(bxi->qs, kqsx);\n        const int ql0 = (ql >> 0) & 0x0F0F0F0F;\n        const int ql1 = (ql >> 4) & 0x0F0F0F0F;\n\n        const int qh = get_int_from_uint8_aligned(bxi->qh, kqsx % (QI5_K/4));\n        const int qh0 = ((qh >> (2 * (kqsx / (QI5_K/4)) + 0)) << 4) & 0x10101010;\n        const int qh1 = ((qh >> (2 * (kqsx / (QI5_K/4)) + 1)) << 4) & 0x10101010;\n\n        const int kq0 = ky - ky % (QI5_K/2) + k % (QI5_K/4) + 0;\n        const int kq1 = ky - ky % (QI5_K/2) + k % (QI5_K/4) + (QI5_K/4);\n\n        x_ql[i * (2*WARP_SIZE + 1) + kq0] = ql0 | qh0;\n        x_ql[i * (2*WARP_SIZE + 1) + kq1] = ql1 | qh1;\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI5_K; // == 1 if QK_K == 256\n    const int kbxd = k % blocks_per_tile_x_row;          // == 0 if QK_K == 256\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI5_K) {\n        int i = (i0 + i_offset * QI5_K + k / blocks_per_tile_x_row) % mmq_y;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q5_K * bxi = bx0 + i*blocks_per_row + kbxd;\n\n#if QK_K == 256\n        x_dm[i * (WARP_SIZE/QI5_K) + i / QI5_K + kbxd] = bxi->dm;\n#endif\n    }\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 8) {\n        int i = (i0 + i_offset * 8 + k / (WARP_SIZE/8)) % mmq_y;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q5_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE/8)) / (QI5_K/8);\n\n        const int * scales = (int *) bxi->scales;\n\n        const int ksc = k % (WARP_SIZE/8);\n\n        // scale arrangement after the following two lines: sc0,...,sc3, sc4,...,sc7, m0,...,m3, m4,...,m8\n        int scales8 = (scales[(ksc%2) + (ksc!=0)] >> (4 * (ksc & (ksc/2)))) & 0x0F0F0F0F; // lower 4 bits\n        scales8    |= (scales[ksc/2]              >> (2 * (ksc % 2)))       & 0x30303030; // upper 2 bits\n\n        x_sc[i * (WARP_SIZE/8) + i / 8 + ksc] = scales8;\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q5_K_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const uint8_t * sc = ((const uint8_t *) &x_sc[i * (WARP_SIZE/8) + i/8 + k/16]) + 2 * ((k % 16) / 8);\n\n    const int index_x = i * (QR5_K*WARP_SIZE + 1) +  QR5_K*k;\n    const int index_y = j * WARP_SIZE             + (QR5_K*k) % WARP_SIZE;\n    return vec_dot_q5_K_q8_1_impl_mmq(&x_ql[index_x], &y_qs[index_y], sc, sc+8,\n                                      x_dm[i * (WARP_SIZE/QI5_K) + i/QI5_K], &y_ds[index_y/QI8_1]);\n}\n\nstatic __device__ __forceinline__ float vec_dot_q6_K_q8_1(\n    const void * __restrict__ vbq, const block_q8_1 * __restrict__ bq8_1, const int & iqs) {\n\n    const block_q6_K * bq6_K = (const block_q6_K *) vbq;\n\n    const int bq8_offset = 2 * QR6_K * (iqs / (QI6_K/2)) + (iqs % (QI6_K/2)) / (QI6_K/4);\n    const int scale_offset = (QI6_K/4) * (iqs / (QI6_K/2)) + (iqs % (QI6_K/2)) / (QI6_K/8);\n    const int vh_shift = 2 * ((iqs % (QI6_K/2)) / (QI6_K/4));\n\n    const int vl = get_int_from_uint8(bq6_K->ql, iqs);\n    const int vh = get_int_from_uint8(bq6_K->qh, (QI6_K/4) * (iqs / (QI6_K/2)) + iqs % (QI6_K/4)) >> vh_shift;\n\n    const int8_t * scales = bq6_K->scales + scale_offset;\n\n    int    u[QR6_K];\n    float d8[QR6_K];\n\n#pragma unroll\n    for (int i = 0; i < QR6_K; ++i) {\n        u[i]  = get_int_from_int8_aligned(bq8_1[bq8_offset + 2*i].qs, iqs % QI8_1);\n        d8[i] = __low2half(bq8_1[bq8_offset + 2*i].ds);\n    }\n\n    return vec_dot_q6_K_q8_1_impl_mmvq(vl, vh, u, scales, bq6_K->d, d8);\n}\n\ntemplate <int mmq_y> static __device__ __forceinline__ void allocate_tiles_q6_K(int ** x_ql, half2 ** x_dm, int ** x_qh, int ** x_sc) {\n\n    __shared__ int   tile_x_ql[mmq_y * (2*WARP_SIZE)     + mmq_y];\n    __shared__ half2 tile_x_dm[mmq_y * (WARP_SIZE/QI6_K) + mmq_y/QI6_K];\n    __shared__ int   tile_x_sc[mmq_y * (WARP_SIZE/8)     + mmq_y/8];\n\n    *x_ql = tile_x_ql;\n    *x_dm = tile_x_dm;\n    *x_sc = tile_x_sc;\n}\n\ntemplate <int mmq_y, int nwarps, bool need_check> static __device__ __forceinline__ void load_tiles_q6_K(\n    const void * __restrict__ vx, int * __restrict__ x_ql, half2 * __restrict__ x_dm, int * __restrict__ x_qh,\n    int * __restrict__ x_sc, const int & i_offset, const int & i_max, const int & k, const int & blocks_per_row) {\n\n    GGML_CUDA_ASSUME(i_offset >= 0);\n    GGML_CUDA_ASSUME(i_offset <  nwarps);\n    GGML_CUDA_ASSUME(k >= 0);\n    GGML_CUDA_ASSUME(k <  WARP_SIZE);\n\n    const int kbx  = k / QI6_K; // == 0 if QK_K == 256\n    const int kqsx = k % QI6_K; // == k if QK_K == 256\n\n    const block_q6_K * bx0 = (block_q6_K *) vx;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps) {\n        int i = i0 + i_offset;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q6_K * bxi = bx0 + i*blocks_per_row + kbx;\n        const int ky = QR6_K*kqsx;\n\n        const int ql = get_int_from_uint8(bxi->ql, kqsx);\n        const int ql0 = (ql >> 0) & 0x0F0F0F0F;\n        const int ql1 = (ql >> 4) & 0x0F0F0F0F;\n\n        const int qh = get_int_from_uint8(bxi->qh, (QI6_K/4) * (kqsx / (QI6_K/2)) + kqsx % (QI6_K/4));\n        const int qh0 = ((qh >> (2 * ((kqsx % (QI6_K/2)) / (QI6_K/4)))) << 4) & 0x30303030;\n        const int qh1 =  (qh >> (2 * ((kqsx % (QI6_K/2)) / (QI6_K/4))))       & 0x30303030;\n\n        const int kq0 = ky - ky % QI6_K + k % (QI6_K/2) + 0;\n        const int kq1 = ky - ky % QI6_K + k % (QI6_K/2) + (QI6_K/2);\n\n        x_ql[i * (2*WARP_SIZE + 1) + kq0] = __vsubss4(ql0 | qh0, 0x20202020);\n        x_ql[i * (2*WARP_SIZE + 1) + kq1] = __vsubss4(ql1 | qh1, 0x20202020);\n    }\n\n    const int blocks_per_tile_x_row = WARP_SIZE / QI6_K; // == 1 if QK_K == 256\n    const int kbxd = k % blocks_per_tile_x_row;          // == 0 if QK_K == 256\n    float * x_dmf = (float *) x_dm;\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * QI6_K) {\n        int i = (i0 + i_offset * QI6_K + k / blocks_per_tile_x_row) % mmq_y;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q6_K * bxi = bx0 + i*blocks_per_row + kbxd;\n\n        x_dmf[i * (WARP_SIZE/QI6_K) + i / QI6_K + kbxd] = bxi->d;\n    }\n\n#pragma unroll\n    for (int i0 = 0; i0 < mmq_y; i0 += nwarps * 8) {\n        int i = (i0 + i_offset * 8 + k / (WARP_SIZE/8)) % mmq_y;\n\n        if (need_check) {\n            i = min(i, i_max);\n        }\n\n        const block_q6_K * bxi = bx0 + i*blocks_per_row + (k % (WARP_SIZE/8)) / 4;\n\n        x_sc[i * (WARP_SIZE/8) + i / 8 + k % (WARP_SIZE/8)] = get_int_from_int8(bxi->scales, k % (QI6_K/8));\n    }\n}\n\nstatic __device__ __forceinline__ float vec_dot_q6_K_q8_1_mul_mat(\n    const int * __restrict__ x_ql, const half2 * __restrict__ x_dm, const int * __restrict__ x_qh, const int * __restrict__ x_sc,\n    const int * __restrict__ y_qs, const half2 * __restrict__ y_ds, const int & i, const int & j, const int & k) {\n\n    const float * x_dmf = (const float *) x_dm;\n    const float * y_df  = (const float *) y_ds;\n\n    const int8_t * sc = ((const int8_t *) &x_sc[i * (WARP_SIZE/8) + i/8 + k/8]);\n\n    const int index_x = i * (QR6_K*WARP_SIZE + 1) +  QR6_K*k;\n    const int index_y = j * WARP_SIZE             + (QR6_K*k) % WARP_SIZE;\n    return vec_dot_q6_K_q8_1_impl_mmq(&x_ql[index_x], &y_qs[index_y], sc, x_dmf[i * (WARP_SIZE/QI6_K) + i/QI6_K], &y_df[index_y/QI8_1]);\n}\n\ntemplate <int qk, int qr, int qi, bool need_sum, typename block_q_t, int mmq_x, int mmq_y, int nwarps,\n              allocate_tiles_cuda_t allocate_tiles, load_tiles_cuda_t load_tiles, int vdr, vec_dot_q_mul_mat_cuda_t vec_dot>\nstatic __device__ __forceinline__ void mul_mat_q(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n    const block_q_t  * x = (const block_q_t  *) vx;\n    const block_q8_1 * y = (const block_q8_1 *) vy;\n\n    const int blocks_per_row_x = ncols_x / qk;\n    const int blocks_per_col_y = nrows_y / QK8_1;\n    const int blocks_per_warp = WARP_SIZE / qi;\n\n    const int & ncols_dst = ncols_y;\n\n    const int row_dst_0 = blockIdx.x*mmq_y;\n    const int & row_x_0 = row_dst_0;\n\n    const int col_dst_0 = blockIdx.y*mmq_x;\n    const int & col_y_0 = col_dst_0;\n\n    int   * tile_x_ql = nullptr;\n    half2 * tile_x_dm = nullptr;\n    int   * tile_x_qh = nullptr;\n    int   * tile_x_sc = nullptr;\n\n    allocate_tiles(&tile_x_ql, &tile_x_dm, &tile_x_qh, &tile_x_sc);\n\n    __shared__ int    tile_y_qs[mmq_x * WARP_SIZE];\n    __shared__ half2  tile_y_ds[mmq_x * WARP_SIZE/QI8_1];\n\n    float sum[mmq_y/WARP_SIZE][mmq_x/nwarps] = {0.0f};\n\n    for (int ib0 = 0; ib0 < blocks_per_row_x; ib0 += blocks_per_warp) {\n\n        load_tiles(x + row_x_0*blocks_per_row_x + ib0, tile_x_ql, tile_x_dm, tile_x_qh, tile_x_sc,\n                   threadIdx.y, nrows_x-row_x_0-1, threadIdx.x, blocks_per_row_x);\n\n#pragma unroll\n        for (int ir = 0; ir < qr; ++ir) {\n            const int kqs = ir*WARP_SIZE + threadIdx.x;\n            const int kbxd = kqs / QI8_1;\n\n#pragma unroll\n            for (int i = 0; i < mmq_x; i += nwarps) {\n                const int col_y_eff = min(col_y_0 + threadIdx.y + i, ncols_y-1); // to prevent out-of-bounds memory accesses\n\n                const block_q8_1 * by0 = &y[col_y_eff*blocks_per_col_y + ib0 * (qk/QK8_1) + kbxd];\n\n                const int index_y = (threadIdx.y + i) * WARP_SIZE + kqs % WARP_SIZE;\n                tile_y_qs[index_y] = get_int_from_int8_aligned(by0->qs, threadIdx.x % QI8_1);\n            }\n\n#pragma unroll\n            for (int ids0 = 0; ids0 < mmq_x; ids0 += nwarps * QI8_1) {\n                const int ids = (ids0 + threadIdx.y * QI8_1 + threadIdx.x / (WARP_SIZE/QI8_1)) % mmq_x;\n                const int kby = threadIdx.x % (WARP_SIZE/QI8_1);\n                const int col_y_eff = min(col_y_0 + ids, ncols_y-1);\n\n                // if the sum is not needed it's faster to transform the scale to f32 ahead of time\n                const half2 * dsi_src = &y[col_y_eff*blocks_per_col_y + ib0 * (qk/QK8_1) + ir*(WARP_SIZE/QI8_1) + kby].ds;\n                half2       * dsi_dst = &tile_y_ds[ids * (WARP_SIZE/QI8_1) + kby];\n                if (need_sum) {\n                    *dsi_dst = *dsi_src;\n                } else {\n                    float * dfi_dst = (float *) dsi_dst;\n                    *dfi_dst = __low2half(*dsi_src);\n                }\n            }\n\n            __syncthreads();\n\n// #pragma unroll // unrolling this loop causes too much register pressure\n            for (int k = ir*WARP_SIZE/qr; k < (ir+1)*WARP_SIZE/qr; k += vdr) {\n#pragma unroll\n                for (int j = 0; j < mmq_x; j += nwarps) {\n#pragma unroll\n                    for (int i = 0; i < mmq_y; i += WARP_SIZE) {\n                        sum[i/WARP_SIZE][j/nwarps] += vec_dot(\n                            tile_x_ql, tile_x_dm, tile_x_qh, tile_x_sc, tile_y_qs, tile_y_ds,\n                            threadIdx.x + i, threadIdx.y + j, k);\n                    }\n                }\n            }\n\n            __syncthreads();\n        }\n    }\n\n#pragma unroll\n    for (int j = 0; j < mmq_x; j += nwarps) {\n        const int col_dst = col_dst_0 + j + threadIdx.y;\n\n        if (col_dst >= ncols_dst) {\n            return;\n        }\n\n#pragma unroll\n        for (int i = 0; i < mmq_y; i += WARP_SIZE) {\n            const int row_dst = row_dst_0 + threadIdx.x + i;\n\n            if (row_dst >= nrows_dst) {\n                continue;\n            }\n\n            dst[col_dst*nrows_dst + row_dst] = sum[i/WARP_SIZE][j/nwarps];\n        }\n    }\n}\n\n#define  MMQ_X_Q4_0_RDNA2  64\n#define  MMQ_Y_Q4_0_RDNA2  128\n#define NWARPS_Q4_0_RDNA2  8\n#define  MMQ_X_Q4_0_RDNA1  64\n#define  MMQ_Y_Q4_0_RDNA1  64\n#define NWARPS_Q4_0_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q4_0_AMPERE 4\n#define  MMQ_Y_Q4_0_AMPERE 32\n#define NWARPS_Q4_0_AMPERE 4\n#else\n#define  MMQ_X_Q4_0_AMPERE 64\n#define  MMQ_Y_Q4_0_AMPERE 128\n#define NWARPS_Q4_0_AMPERE 4\n#endif\n#define  MMQ_X_Q4_0_PASCAL 64\n#define  MMQ_Y_Q4_0_PASCAL 64\n#define NWARPS_Q4_0_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q4_0_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#endif // defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n    mul_mat_q4_0(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q4_0_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q4_0_RDNA2;\n    const int nwarps = NWARPS_Q4_0_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q4_0_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q4_0_RDNA1;\n    const int nwarps = NWARPS_Q4_0_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK4_0, QR4_0, QI4_0, true, block_q4_0, mmq_x, mmq_y, nwarps, allocate_tiles_q4_0<mmq_y>,\n        load_tiles_q4_0<mmq_y, nwarps, need_check>, VDR_Q4_0_Q8_1_MMQ, vec_dot_q4_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q4_0_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q4_0_AMPERE;\n    const int nwarps = NWARPS_Q4_0_AMPERE;\n\n    mul_mat_q<QK4_0, QR4_0, QI4_0, true, block_q4_0, mmq_x, mmq_y, nwarps, allocate_tiles_q4_0<mmq_y>,\n        load_tiles_q4_0<mmq_y, nwarps, need_check>, VDR_Q4_0_Q8_1_MMQ, vec_dot_q4_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q4_0_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q4_0_PASCAL;\n    const int nwarps = NWARPS_Q4_0_PASCAL;\n\n    mul_mat_q<QK4_0, QR4_0, QI4_0, true, block_q4_0, mmq_x, mmq_y, nwarps, allocate_tiles_q4_0<mmq_y>,\n        load_tiles_q4_0<mmq_y, nwarps, need_check>, VDR_Q4_0_Q8_1_MMQ, vec_dot_q4_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q4_0_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q4_1_RDNA2  64\n#define  MMQ_Y_Q4_1_RDNA2  128\n#define NWARPS_Q4_1_RDNA2  8\n#define  MMQ_X_Q4_1_RDNA1  64\n#define  MMQ_Y_Q4_1_RDNA1  64\n#define NWARPS_Q4_1_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q4_1_AMPERE 4\n#define  MMQ_Y_Q4_1_AMPERE 32\n#define NWARPS_Q4_1_AMPERE 4\n#else\n#define  MMQ_X_Q4_1_AMPERE 64\n#define  MMQ_Y_Q4_1_AMPERE 128\n#define NWARPS_Q4_1_AMPERE 4\n#endif\n#define  MMQ_X_Q4_1_PASCAL 64\n#define  MMQ_Y_Q4_1_PASCAL 64\n#define NWARPS_Q4_1_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q4_1_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#elif __CUDA_ARCH__ < CC_VOLTA\n    __launch_bounds__(WARP_SIZE*NWARPS_Q4_1_PASCAL, 2)\n#endif // __CUDA_ARCH__ < CC_VOLTA\n    mul_mat_q4_1(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q4_1_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q4_1_RDNA2;\n    const int nwarps = NWARPS_Q4_1_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q4_1_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q4_1_RDNA1;\n    const int nwarps = NWARPS_Q4_1_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK4_1, QR4_1, QI4_1, true, block_q4_1, mmq_x, mmq_y, nwarps, allocate_tiles_q4_1<mmq_y>,\n        load_tiles_q4_1<mmq_y, nwarps, need_check>, VDR_Q4_1_Q8_1_MMQ, vec_dot_q4_1_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q4_1_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q4_1_AMPERE;\n    const int nwarps = NWARPS_Q4_1_AMPERE;\n\n    mul_mat_q<QK4_1, QR4_1, QI4_1, true, block_q4_1, mmq_x, mmq_y, nwarps, allocate_tiles_q4_1<mmq_y>,\n        load_tiles_q4_1<mmq_y, nwarps, need_check>, VDR_Q4_1_Q8_1_MMQ, vec_dot_q4_1_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q4_1_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q4_1_PASCAL;\n    const int nwarps = NWARPS_Q4_1_PASCAL;\n\n    mul_mat_q<QK4_1, QR4_1, QI4_1, true, block_q4_1, mmq_x, mmq_y, nwarps, allocate_tiles_q4_1<mmq_y>,\n        load_tiles_q4_1<mmq_y, nwarps, need_check>, VDR_Q4_1_Q8_1_MMQ, vec_dot_q4_1_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q4_1_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q5_0_RDNA2  64\n#define  MMQ_Y_Q5_0_RDNA2  128\n#define NWARPS_Q5_0_RDNA2  8\n#define  MMQ_X_Q5_0_RDNA1  64\n#define  MMQ_Y_Q5_0_RDNA1  64\n#define NWARPS_Q5_0_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q5_0_AMPERE 4\n#define  MMQ_Y_Q5_0_AMPERE 32\n#define NWARPS_Q5_0_AMPERE 4\n#else\n#define  MMQ_X_Q5_0_AMPERE 128\n#define  MMQ_Y_Q5_0_AMPERE 64\n#define NWARPS_Q5_0_AMPERE 4\n#endif\n#define  MMQ_X_Q5_0_PASCAL 64\n#define  MMQ_Y_Q5_0_PASCAL 64\n#define NWARPS_Q5_0_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q5_0_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#endif // defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n    mul_mat_q5_0(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q5_0_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q5_0_RDNA2;\n    const int nwarps = NWARPS_Q5_0_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q5_0_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q5_0_RDNA1;\n    const int nwarps = NWARPS_Q5_0_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK5_0, QR5_0, QI5_0, false, block_q5_0, mmq_x, mmq_y, nwarps, allocate_tiles_q5_0<mmq_y>,\n        load_tiles_q5_0<mmq_y, nwarps, need_check>, VDR_Q5_0_Q8_1_MMQ, vec_dot_q5_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q5_0_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q5_0_AMPERE;\n    const int nwarps = NWARPS_Q5_0_AMPERE;\n\n    mul_mat_q<QK5_0, QR5_0, QI5_0, false, block_q5_0, mmq_x, mmq_y, nwarps, allocate_tiles_q5_0<mmq_y>,\n        load_tiles_q5_0<mmq_y, nwarps, need_check>, VDR_Q5_0_Q8_1_MMQ, vec_dot_q5_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q5_0_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q5_0_PASCAL;\n    const int nwarps = NWARPS_Q5_0_PASCAL;\n\n    mul_mat_q<QK5_0, QR5_0, QI5_0, false, block_q5_0, mmq_x, mmq_y, nwarps, allocate_tiles_q5_0<mmq_y>,\n        load_tiles_q5_0<mmq_y, nwarps, need_check>, VDR_Q5_0_Q8_1_MMQ, vec_dot_q5_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q5_0_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q5_1_RDNA2  64\n#define  MMQ_Y_Q5_1_RDNA2  128\n#define NWARPS_Q5_1_RDNA2  8\n#define  MMQ_X_Q5_1_RDNA1  64\n#define  MMQ_Y_Q5_1_RDNA1  64\n#define NWARPS_Q5_1_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q5_1_AMPERE 4\n#define  MMQ_Y_Q5_1_AMPERE 32\n#define NWARPS_Q5_1_AMPERE 4\n#else\n#define  MMQ_X_Q5_1_AMPERE 128\n#define  MMQ_Y_Q5_1_AMPERE 64\n#define NWARPS_Q5_1_AMPERE 4\n#endif\n#define  MMQ_X_Q5_1_PASCAL 64\n#define  MMQ_Y_Q5_1_PASCAL 64\n#define NWARPS_Q5_1_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q5_1_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#endif // defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\nmul_mat_q5_1(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q5_1_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q5_1_RDNA2;\n    const int nwarps = NWARPS_Q5_1_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q5_1_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q5_1_RDNA1;\n    const int nwarps = NWARPS_Q5_1_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK5_1, QR5_1, QI5_1, true, block_q5_1, mmq_x, mmq_y, nwarps, allocate_tiles_q5_1<mmq_y>,\n        load_tiles_q5_1<mmq_y, nwarps, need_check>, VDR_Q5_1_Q8_1_MMQ, vec_dot_q5_1_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q5_1_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q5_1_AMPERE;\n    const int nwarps = NWARPS_Q5_1_AMPERE;\n\n    mul_mat_q<QK5_1, QR5_1, QI5_1, true, block_q5_1, mmq_x, mmq_y, nwarps, allocate_tiles_q5_1<mmq_y>,\n        load_tiles_q5_1<mmq_y, nwarps, need_check>, VDR_Q5_1_Q8_1_MMQ, vec_dot_q5_1_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q5_1_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q5_1_PASCAL;\n    const int nwarps = NWARPS_Q5_1_PASCAL;\n\n    mul_mat_q<QK5_1, QR5_1, QI5_1, true, block_q5_1, mmq_x, mmq_y, nwarps, allocate_tiles_q5_1<mmq_y>,\n        load_tiles_q5_1<mmq_y, nwarps, need_check>, VDR_Q5_1_Q8_1_MMQ, vec_dot_q5_1_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q5_1_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q8_0_RDNA2  64\n#define  MMQ_Y_Q8_0_RDNA2  128\n#define NWARPS_Q8_0_RDNA2  8\n#define  MMQ_X_Q8_0_RDNA1  64\n#define  MMQ_Y_Q8_0_RDNA1  64\n#define NWARPS_Q8_0_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q8_0_AMPERE 4\n#define  MMQ_Y_Q8_0_AMPERE 32\n#define NWARPS_Q8_0_AMPERE 4\n#else\n#define  MMQ_X_Q8_0_AMPERE 128\n#define  MMQ_Y_Q8_0_AMPERE 64\n#define NWARPS_Q8_0_AMPERE 4\n#endif\n#define  MMQ_X_Q8_0_PASCAL 64\n#define  MMQ_Y_Q8_0_PASCAL 64\n#define NWARPS_Q8_0_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q8_0_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#endif // defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n    mul_mat_q8_0(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q8_0_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q8_0_RDNA2;\n    const int nwarps = NWARPS_Q8_0_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q8_0_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q8_0_RDNA1;\n    const int nwarps = NWARPS_Q8_0_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK8_0, QR8_0, QI8_0, false, block_q8_0, mmq_x, mmq_y, nwarps, allocate_tiles_q8_0<mmq_y>,\n        load_tiles_q8_0<mmq_y, nwarps, need_check>, VDR_Q8_0_Q8_1_MMQ, vec_dot_q8_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q8_0_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q8_0_AMPERE;\n    const int nwarps = NWARPS_Q8_0_AMPERE;\n\n    mul_mat_q<QK8_0, QR8_0, QI8_0, false, block_q8_0, mmq_x, mmq_y, nwarps, allocate_tiles_q8_0<mmq_y>,\n        load_tiles_q8_0<mmq_y, nwarps, need_check>, VDR_Q8_0_Q8_1_MMQ, vec_dot_q8_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q8_0_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q8_0_PASCAL;\n    const int nwarps = NWARPS_Q8_0_PASCAL;\n\n    mul_mat_q<QK8_0, QR8_0, QI8_0, false, block_q8_0, mmq_x, mmq_y, nwarps, allocate_tiles_q8_0<mmq_y>,\n        load_tiles_q8_0<mmq_y, nwarps, need_check>, VDR_Q8_0_Q8_1_MMQ, vec_dot_q8_0_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q8_0_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q2_K_RDNA2  64\n#define  MMQ_Y_Q2_K_RDNA2  128\n#define NWARPS_Q2_K_RDNA2  8\n#define  MMQ_X_Q2_K_RDNA1  128\n#define  MMQ_Y_Q2_K_RDNA1  32\n#define NWARPS_Q2_K_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q2_K_AMPERE 4\n#define  MMQ_Y_Q2_K_AMPERE 32\n#define NWARPS_Q2_K_AMPERE 4\n#else\n#define  MMQ_X_Q2_K_AMPERE 64\n#define  MMQ_Y_Q2_K_AMPERE 128\n#define NWARPS_Q2_K_AMPERE 4\n#endif\n#define  MMQ_X_Q2_K_PASCAL 64\n#define  MMQ_Y_Q2_K_PASCAL 64\n#define NWARPS_Q2_K_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q2_K_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#endif // defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\nmul_mat_q2_K(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q2_K_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q2_K_RDNA2;\n    const int nwarps = NWARPS_Q2_K_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q2_K_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q2_K_RDNA1;\n    const int nwarps = NWARPS_Q2_K_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK_K, QR2_K, QI2_K, false, block_q2_K, mmq_x, mmq_y, nwarps, allocate_tiles_q2_K<mmq_y>,\n        load_tiles_q2_K<mmq_y, nwarps, need_check>, VDR_Q2_K_Q8_1_MMQ, vec_dot_q2_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q2_K_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q2_K_AMPERE;\n    const int nwarps = NWARPS_Q2_K_AMPERE;\n\n    mul_mat_q<QK_K, QR2_K, QI2_K, false, block_q2_K, mmq_x, mmq_y, nwarps, allocate_tiles_q2_K<mmq_y>,\n        load_tiles_q2_K<mmq_y, nwarps, need_check>, VDR_Q2_K_Q8_1_MMQ, vec_dot_q2_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q2_K_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q2_K_PASCAL;\n    const int nwarps = NWARPS_Q2_K_PASCAL;\n\n    mul_mat_q<QK_K, QR2_K, QI2_K, false, block_q2_K, mmq_x, mmq_y, nwarps, allocate_tiles_q2_K<mmq_y>,\n        load_tiles_q2_K<mmq_y, nwarps, need_check>, VDR_Q2_K_Q8_1_MMQ, vec_dot_q2_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q2_K_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q3_K_RDNA2  128\n#define  MMQ_Y_Q3_K_RDNA2  64\n#define NWARPS_Q3_K_RDNA2  8\n#define  MMQ_X_Q3_K_RDNA1  32\n#define  MMQ_Y_Q3_K_RDNA1  128\n#define NWARPS_Q3_K_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q3_K_AMPERE 4\n#define  MMQ_Y_Q3_K_AMPERE 32\n#define NWARPS_Q3_K_AMPERE 4\n#else\n#define  MMQ_X_Q3_K_AMPERE 128\n#define  MMQ_Y_Q3_K_AMPERE 128\n#define NWARPS_Q3_K_AMPERE 4\n#endif\n#define  MMQ_X_Q3_K_PASCAL 64\n#define  MMQ_Y_Q3_K_PASCAL 64\n#define NWARPS_Q3_K_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q3_K_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#elif __CUDA_ARCH__ < CC_VOLTA\n    __launch_bounds__(WARP_SIZE*NWARPS_Q3_K_PASCAL, 2)\n#endif // __CUDA_ARCH__ < CC_VOLTA\n    mul_mat_q3_K(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q3_K_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q3_K_RDNA2;\n    const int nwarps = NWARPS_Q3_K_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q3_K_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q3_K_RDNA1;\n    const int nwarps = NWARPS_Q3_K_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK_K, QR3_K, QI3_K, false, block_q3_K, mmq_x, mmq_y, nwarps, allocate_tiles_q3_K<mmq_y>,\n        load_tiles_q3_K<mmq_y, nwarps, need_check>, VDR_Q3_K_Q8_1_MMQ, vec_dot_q3_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q3_K_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q3_K_AMPERE;\n    const int nwarps = NWARPS_Q3_K_AMPERE;\n\n    mul_mat_q<QK_K, QR3_K, QI3_K, false, block_q3_K, mmq_x, mmq_y, nwarps, allocate_tiles_q3_K<mmq_y>,\n        load_tiles_q3_K<mmq_y, nwarps, need_check>, VDR_Q3_K_Q8_1_MMQ, vec_dot_q3_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q3_K_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q3_K_PASCAL;\n    const int nwarps = NWARPS_Q3_K_PASCAL;\n\n    mul_mat_q<QK_K, QR3_K, QI3_K, false, block_q3_K, mmq_x, mmq_y, nwarps, allocate_tiles_q3_K<mmq_y>,\n        load_tiles_q3_K<mmq_y, nwarps, need_check>, VDR_Q3_K_Q8_1_MMQ, vec_dot_q3_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q3_K_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q4_K_RDNA2  64\n#define  MMQ_Y_Q4_K_RDNA2  128\n#define NWARPS_Q4_K_RDNA2  8\n#define  MMQ_X_Q4_K_RDNA1  32\n#define  MMQ_Y_Q4_K_RDNA1  64\n#define NWARPS_Q4_K_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q4_K_AMPERE 4\n#define  MMQ_Y_Q4_K_AMPERE 32\n#define NWARPS_Q4_K_AMPERE 4\n#else\n#define  MMQ_X_Q4_K_AMPERE 64\n#define  MMQ_Y_Q4_K_AMPERE 128\n#define NWARPS_Q4_K_AMPERE 4\n#endif\n#define  MMQ_X_Q4_K_PASCAL 64\n#define  MMQ_Y_Q4_K_PASCAL 64\n#define NWARPS_Q4_K_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q4_K_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#elif __CUDA_ARCH__ < CC_VOLTA\n    __launch_bounds__(WARP_SIZE*NWARPS_Q4_K_PASCAL, 2)\n#endif // __CUDA_ARCH__ < CC_VOLTA\n    mul_mat_q4_K(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q4_K_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q4_K_RDNA2;\n    const int nwarps = NWARPS_Q4_K_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q4_K_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q4_K_RDNA1;\n    const int nwarps = NWARPS_Q4_K_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK_K, QR4_K, QI4_K, true, block_q4_K, mmq_x, mmq_y, nwarps, allocate_tiles_q4_K<mmq_y>,\n        load_tiles_q4_K<mmq_y, nwarps, need_check>, VDR_Q4_K_Q8_1_MMQ, vec_dot_q4_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q4_K_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q4_K_AMPERE;\n    const int nwarps = NWARPS_Q4_K_AMPERE;\n\n    mul_mat_q<QK_K, QR4_K, QI4_K, true, block_q4_K, mmq_x, mmq_y, nwarps, allocate_tiles_q4_K<mmq_y>,\n        load_tiles_q4_K<mmq_y, nwarps, need_check>, VDR_Q4_K_Q8_1_MMQ, vec_dot_q4_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q4_K_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q4_K_PASCAL;\n    const int nwarps = NWARPS_Q4_K_PASCAL;\n\n    mul_mat_q<QK_K, QR4_K, QI4_K, true, block_q4_K, mmq_x, mmq_y, nwarps, allocate_tiles_q4_K<mmq_y>,\n        load_tiles_q4_K<mmq_y, nwarps, need_check>, VDR_Q4_K_Q8_1_MMQ, vec_dot_q4_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q4_K_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q5_K_RDNA2  64\n#define  MMQ_Y_Q5_K_RDNA2  128\n#define NWARPS_Q5_K_RDNA2  8\n#define  MMQ_X_Q5_K_RDNA1  32\n#define  MMQ_Y_Q5_K_RDNA1  64\n#define NWARPS_Q5_K_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q5_K_AMPERE 4\n#define  MMQ_Y_Q5_K_AMPERE 32\n#define NWARPS_Q5_K_AMPERE 4\n#else\n#define  MMQ_X_Q5_K_AMPERE 64\n#define  MMQ_Y_Q5_K_AMPERE 128\n#define NWARPS_Q5_K_AMPERE 4\n#endif\n#define  MMQ_X_Q5_K_PASCAL 64\n#define  MMQ_Y_Q5_K_PASCAL 64\n#define NWARPS_Q5_K_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q5_K_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#endif // defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\nmul_mat_q5_K(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q5_K_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q5_K_RDNA2;\n    const int nwarps = NWARPS_Q5_K_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q5_K_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q5_K_RDNA1;\n    const int nwarps = NWARPS_Q5_K_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK_K, QR5_K, QI5_K, true, block_q5_K, mmq_x, mmq_y, nwarps, allocate_tiles_q5_K<mmq_y>,\n        load_tiles_q5_K<mmq_y, nwarps, need_check>, VDR_Q5_K_Q8_1_MMQ, vec_dot_q5_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q5_K_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q5_K_AMPERE;\n    const int nwarps = NWARPS_Q5_K_AMPERE;\n\n    mul_mat_q<QK_K, QR5_K, QI5_K, true, block_q5_K, mmq_x, mmq_y, nwarps, allocate_tiles_q5_K<mmq_y>,\n        load_tiles_q5_K<mmq_y, nwarps, need_check>, VDR_Q5_K_Q8_1_MMQ, vec_dot_q5_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q5_K_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q5_K_PASCAL;\n    const int nwarps = NWARPS_Q5_K_PASCAL;\n\n    mul_mat_q<QK_K, QR5_K, QI5_K, true, block_q5_K, mmq_x, mmq_y, nwarps, allocate_tiles_q5_K<mmq_y>,\n        load_tiles_q5_K<mmq_y, nwarps, need_check>, VDR_Q5_K_Q8_1_MMQ, vec_dot_q5_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q5_K_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\n#define  MMQ_X_Q6_K_RDNA2  64\n#define  MMQ_Y_Q6_K_RDNA2  128\n#define NWARPS_Q6_K_RDNA2  8\n#define  MMQ_X_Q6_K_RDNA1  32\n#define  MMQ_Y_Q6_K_RDNA1  64\n#define NWARPS_Q6_K_RDNA1  8\n#if defined(CUDA_USE_TENSOR_CORES)\n#define  MMQ_X_Q6_K_AMPERE 4\n#define  MMQ_Y_Q6_K_AMPERE 32\n#define NWARPS_Q6_K_AMPERE 4\n#else\n#define  MMQ_X_Q6_K_AMPERE 64\n#define  MMQ_Y_Q6_K_AMPERE 64\n#define NWARPS_Q6_K_AMPERE 4\n#endif\n#define  MMQ_X_Q6_K_PASCAL 64\n#define  MMQ_Y_Q6_K_PASCAL 64\n#define NWARPS_Q6_K_PASCAL 8\n\ntemplate <bool need_check> static __global__ void\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    __launch_bounds__(WARP_SIZE*NWARPS_Q6_K_RDNA2, 2)\n#endif // defined(RDNA3) || defined(RDNA2)\n#elif __CUDA_ARCH__ < CC_VOLTA\n    __launch_bounds__(WARP_SIZE*NWARPS_Q6_K_PASCAL, 2)\n#endif // __CUDA_ARCH__ < CC_VOLTA\n    mul_mat_q6_K(\n    const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int ncols_y, const int nrows_y, const int nrows_dst) {\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n#if defined(RDNA3) || defined(RDNA2)\n    const int mmq_x  =  MMQ_X_Q6_K_RDNA2;\n    const int mmq_y  =  MMQ_Y_Q6_K_RDNA2;\n    const int nwarps = NWARPS_Q6_K_RDNA2;\n#else\n    const int mmq_x  =  MMQ_X_Q6_K_RDNA1;\n    const int mmq_y  =  MMQ_Y_Q6_K_RDNA1;\n    const int nwarps = NWARPS_Q6_K_RDNA1;\n#endif // defined(RDNA3) || defined(RDNA2)\n\n    mul_mat_q<QK_K, QR6_K, QI6_K, false, block_q6_K, mmq_x, mmq_y, nwarps, allocate_tiles_q6_K<mmq_y>,\n        load_tiles_q6_K<mmq_y, nwarps, need_check>, VDR_Q6_K_Q8_1_MMQ, vec_dot_q6_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= CC_VOLTA\n    const int mmq_x  =  MMQ_X_Q6_K_AMPERE;\n    const int mmq_y  =  MMQ_Y_Q6_K_AMPERE;\n    const int nwarps = NWARPS_Q6_K_AMPERE;\n\n    mul_mat_q<QK_K, QR6_K, QI6_K, false, block_q6_K, mmq_x, mmq_y, nwarps, allocate_tiles_q6_K<mmq_y>,\n        load_tiles_q6_K<mmq_y, nwarps, need_check>, VDR_Q6_K_Q8_1_MMQ, vec_dot_q6_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n\n#elif __CUDA_ARCH__ >= MIN_CC_DP4A\n    const int mmq_x  =  MMQ_X_Q6_K_PASCAL;\n    const int mmq_y  =  MMQ_Y_Q6_K_PASCAL;\n    const int nwarps = NWARPS_Q6_K_PASCAL;\n\n    mul_mat_q<QK_K, QR6_K, QI6_K, false, block_q6_K, mmq_x, mmq_y, nwarps, allocate_tiles_q6_K<mmq_y>,\n        load_tiles_q6_K<mmq_y, nwarps, need_check>, VDR_Q6_K_Q8_1_MMQ, vec_dot_q6_K_q8_1_mul_mat>\n        (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n#else\n    (void) vec_dot_q6_K_q8_1_mul_mat;\n    assert(false);\n#endif // __CUDA_ARCH__ >= CC_VOLTA\n}\n\ntemplate <int qk, int qi, typename block_q_t, int vdr, vec_dot_q_cuda_t vec_dot_q_cuda>\nstatic __global__ void mul_mat_vec_q(const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst, const int ncols, const int nrows) {\n    const int row = blockIdx.x*blockDim.y + threadIdx.y;\n\n    if (row >= nrows) {\n        return;\n    }\n\n    const int blocks_per_row = ncols / qk;\n    const int blocks_per_warp = vdr * WARP_SIZE / qi;\n\n// partial sum for each thread\n    float tmp = 0.0f;\n\n    const block_q_t  * x = (const block_q_t  *) vx;\n    const block_q8_1 * y = (const block_q8_1 *) vy;\n\n    for (int i = 0; i < blocks_per_row; i += blocks_per_warp) {\n        const int ibx = row*blocks_per_row + i + threadIdx.x / (qi/vdr); // x block index\n\n        const int iby = (i + threadIdx.x / (qi/vdr)) * (qk/QK8_1); // y block index that aligns with ibx\n\n        const int iqs  = vdr * (threadIdx.x % (qi/vdr)); // x block quant index when casting the quants to int\n\n        tmp += vec_dot_q_cuda(&x[ibx], &y[iby], iqs);\n    }\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (threadIdx.x == 0) {\n        dst[row] = tmp;\n    }\n}\n\ntemplate <int qk, int qr, dequantize_kernel_t dequantize_kernel>\nstatic __global__ void dequantize_mul_mat_vec(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows) {\n    // qk = quantized weights per x block\n    // qr = number of quantized weights per data value in x block\n    const int row = blockIdx.x*blockDim.y + threadIdx.y;\n\n    if (row >= nrows) {\n        return;\n    }\n\n    const int tid = threadIdx.x;\n\n    const int iter_stride = 2*GGML_CUDA_DMMV_X;\n    const int vals_per_iter = iter_stride / WARP_SIZE; // num quantized vals per thread and i iter\n    const int y_offset = qr == 1 ? 1 : qk/2;\n\n// partial sum for each thread\n#ifdef GGML_CUDA_F16\n    half2 tmp = {0.0f, 0.0f}; // two sums for f16 to take advantage of half2 intrinsics\n#else\n    float tmp = 0.0f;\n#endif // GGML_CUDA_F16\n\n    for (int i = 0; i < ncols; i += iter_stride) {\n        const int col = i + vals_per_iter*tid;\n        const int ib = (row*ncols + col)/qk; // x block index\n        const int iqs = (col%qk)/qr; // x quant index\n        const int iybs = col - col%qk; // y block start index\n\n// processing >2 values per i iter is faster for fast GPUs\n#pragma unroll\n        for (int j = 0; j < vals_per_iter; j += 2) {\n            // process 2 vals per j iter\n\n            // dequantize\n            // for qr = 2 the iqs needs to increase by 1 per j iter because 2 weights per data val\n            dfloat2 v;\n            dequantize_kernel(vx, ib, iqs + j/qr, v);\n\n            // matrix multiplication\n            // for qr = 2 the y index needs to increase by 1 per j iter because of y_offset = qk/2\n#ifdef GGML_CUDA_F16\n            tmp += __hmul2(v, {\n                y[iybs + iqs + j/qr + 0],\n                y[iybs + iqs + j/qr + y_offset]\n            });\n#else\n            tmp += v.x * y[iybs + iqs + j/qr + 0];\n            tmp += v.y * y[iybs + iqs + j/qr + y_offset];\n#endif // GGML_CUDA_F16\n        }\n    }\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (tid == 0) {\n#ifdef GGML_CUDA_F16\n        dst[row] = tmp.x + tmp.y;\n#else\n        dst[row] = tmp;\n#endif // GGML_CUDA_F16\n    }\n}\n\n\ntemplate <int qk, int qr, dequantize_kernel_t dequantize_kernel>\nstatic __global__ void dequantize_mul_mat_axpy(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows) {\n    // qk = quantized weights per x block\n    // qr = number of quantized weights per data value in x block\n    const int row = blockIdx.y*blockDim.y + threadIdx.y;\n\n    if (row >= nrows) {\n        return;\n    }\n    const int bid = blockIdx.y;\n    // if (bid == 0) global_lock = 0;\n\n    extern __shared__ float shared_dst[]; // TODO:dynamic\n\n    const int tid = threadIdx.x;\n\n    const int iter_stride = 2*GGML_CUDA_DMMV_X;\n    const int vals_per_iter = iter_stride / WARP_SIZE; // num quantized vals per thread and i iter\n    const int y_offset = qr == 1 ? 1 : qk/2;\n\n// partial sum for each thread\n    float tmp = 0.0f;\n    for (int i = 0; i < ncols; i += GGML_CUDA_DMMV_X) {\n        shared_dst[i+tid] = 0;\n    }\n    __syncthreads();\n\n    for (int i = 0; i < ncols; i += iter_stride) {\n        const int col = i + vals_per_iter*tid;\n        const int ib = (row*ncols + col)/qk; // x block index\n        const int iqs = (col%qk)/qr; // x quant index\n        const int iybs = col - col%qk; // y block start index\n\n// processing >2 values per i iter is faster for fast GPUs\n#pragma unroll\n        for (int j = 0; j < vals_per_iter; j += 2) {\n            // process 2 vals per j iter\n\n            // dequantize\n            // for qr = 2 the iqs needs to increase by 1 per j iter because 2 weights per data val\n            dfloat2 v;\n            dequantize_kernel(vx, ib, iqs + j/qr, v);\n\n            // matrix multiplication\n            // for qr = 2 the y index needs to increase by 1 per j iter because of y_offset = qk/2\n            tmp = v.x * y[row];\n            /* atomicAdd((float *)&dst[iybs + iqs + j/qr + 0], tmp); */\n            shared_dst[iybs + iqs + j/qr + 0] = tmp;\n            tmp = v.y * y[row];\n            /* atomicAdd((float *)&dst[iybs + iqs + j/qr + y_offset], tmp); */\n            shared_dst[iybs + iqs + j/qr + y_offset] = tmp;\n        }\n    }\n    /* __syncthreads(); */\n\n    for (int i = 0; i < ncols; i += GGML_CUDA_DMMV_X) {\n        // dst[i+tid] += shared_dst[i+tid]; \n        atomicAdd(&dst[i+tid], shared_dst[i+tid]);\n    }\n}\n\ntemplate <int qk, int qr, dequantize_kernel_t dequantize_kernel> \nstatic __global__ void dequantize_mul_mat_axpy_sparse_pro(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows, int *lst, float *idx) {\n    const int thread_col = blockIdx.y * blockDim.x + threadIdx.x;\n    const int col = 2 * thread_col;\n    const int tid = threadIdx.x;\n    const int wid = threadIdx.y;\n    const int iqs = (col%qk) / qr; // x quant index\n    const int row_offset = blockIdx.z * AXPY_BLOCK_Z;\n\n    if (col >= ncols) {\n        return;\n    }\n\n    __shared__ dfloat dst_tmp[AXPY_BLOCK_Y][AXPY_BLOCK_X*2];\n    __shared__ dfloat y_tmp[AXPY_BLOCK_Z]; \n\n    dst_tmp[wid][tid] = 0.0;\n    dst_tmp[wid][tid+AXPY_BLOCK_X] = 0.0;\n    \n    if (wid == 0) {\n        if (lst) {\n            for(int i=tid; i<AXPY_BLOCK_Z; i += AXPY_BLOCK_X) {\n                y_tmp[i] = y[lst[i+row_offset]];\n            }\n        } else {\n            for(int i=tid; i<AXPY_BLOCK_Z; i += AXPY_BLOCK_X) {\n                // ((dfloat4*)y_tmp)[i] = *(dfloat4*)(&y[i*4+row_offset]);\n                y_tmp[i] = y[i+row_offset];\n            }\n        }\n    }\n    __syncthreads();\n\n#pragma unroll 8\n    for(int gpu_row = wid; gpu_row < nrows; gpu_row += AXPY_BLOCK_Y) {        \n        if(y_tmp[gpu_row] == 0.0) continue;\n\n        const int ib = ((gpu_row + row_offset)*ncols + col) / qk; // x block index\n        \n        dfloat2 v;\n        dequantize_kernel(vx, ib, iqs, v);\n\n        dst_tmp[wid][tid] += v.x * y_tmp[gpu_row];\n        dst_tmp[wid][tid+AXPY_BLOCK_X] += v.y * y_tmp[gpu_row];\n    }\n\n    for (int offset = AXPY_BLOCK_Y / 2; offset > 0; offset >>= 1) {\n        if (wid < offset) {\n            dst_tmp[wid][tid] += dst_tmp[wid+offset][tid];\n            dst_tmp[wid][tid+AXPY_BLOCK_X] += dst_tmp[wid][tid+AXPY_BLOCK_X];\n        }\n        __syncthreads();\n    }\n\n    if (wid == 0) {\n        const int iybs = col - col%qk; // y block start index\n        const int y_offset = qr == 1 ? 1 : qk/2;\n        atomicAdd(&dst[iybs + iqs], dst_tmp[wid][tid]);\n        atomicAdd(&dst[iybs + iqs + y_offset], dst_tmp[wid][tid+AXPY_BLOCK_X]); \n    }\n}\n\ntemplate <int qk, int qr, dequantize_kernel_t dequantize_kernel>\nstatic __global__ void dequantize_mul_mat_axpy_sparse(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows, int *lst, float *idx) {\n    // qk = quantized weights per x block\n    // qr = number of quantized weights per data value in x block\n    const int gpu_row = blockIdx.y*blockDim.y + threadIdx.y;\n\n    if (gpu_row >= nrows) {\n        return;\n    }\n    int row = lst ? lst[gpu_row] : gpu_row;\n    const int tid = threadIdx.x;\n    short *d = (short *)((char *)vx + ncols * gpu_row * 2);\n\n    if (y[row] == 0)\n        return;\n    if (idx[row] < dev_sparse_threshold) {\n        return;\n    }\n\n    const int bid = blockIdx.y;\n\n    extern __shared__ float shared_dst[]; // TODO:dynamic\n\n    const int iter_stride = 2*GGML_CUDA_DMMV_X;\n    const int vals_per_iter = iter_stride / WARP_SIZE; // num quantized vals per thread and i iter\n    const int y_offset = qr == 1 ? 1 : qk/2;\n\n// partial sum for each thread\n    float tmp = 0.0f;\n    for (int i = 0; i < ncols; i += GGML_CUDA_DMMV_X) {\n        shared_dst[i+tid] = 0;\n    }\n    __syncthreads();\n\n    for (int i = 0; i < ncols; i += iter_stride) {\n        const int col = i + vals_per_iter*tid;\n        const int ib = (gpu_row*ncols + col)/qk; // x block index\n        const int iqs = (col%qk)/qr; // x quant index\n        const int iybs = col - col%qk; // y block start index\n\n// processing >2 values per i iter is faster for fast GPUs\n#pragma unroll\n        for (int j = 0; j < vals_per_iter; j += 2) {\n            // process 2 vals per j iter\n\n            // dequantize\n            // for qr = 2 the iqs needs to increase by 1 per j iter because 2 weights per data val\n            dfloat2 v;\n            dequantize_kernel(vx, ib, iqs + j/qr, v);\n\n            // matrix multiplication\n            // for qr = 2 the y index needs to increase by 1 per j iter because of y_offset = qk/2\n            tmp = v.x * y[row];\n            shared_dst[iybs + iqs + j/qr + 0] = tmp;\n            tmp = v.y * y[row];\n            shared_dst[iybs + iqs + j/qr + y_offset] = tmp;\n            \n        }\n    }\n    __syncthreads();\n\n    for (int i = 0; i < ncols; i += GGML_CUDA_DMMV_X) {\n        atomicAdd(&dst[i+tid], shared_dst[i+tid]);\n    }\n}\n\ntemplate <int qk, int qr, dequantize_kernel_t dequantize_kernel>\nstatic __global__ void dequantize_mul_mat_axpy_sparse_batch(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows, int src1_ne0, int src1_ncols, int *lst, float *idx) {\n    // qk = quantized weights per x block\n    // qr = number of quantized weights per data value in x block\n    const int gpu_row = blockIdx.y*blockDim.y + threadIdx.y;\n\n    if (gpu_row >= nrows) {\n        return;\n    }\n    int row = lst ? lst[gpu_row] : gpu_row;\n    const int bid = blockIdx.y;\n\n    extern __shared__ float shared_dst[]; // TODO:dynamic\n\n    const int tid = threadIdx.x;\n\n    const int iter_stride = 2*GGML_CUDA_DMMV_X;\n    const int vals_per_iter = iter_stride / WARP_SIZE; // num quantized vals per thread and i iter\n    const int y_offset = qr == 1 ? 1 : qk/2;\n    float * loop_idx = idx;\n    dfloat * loop_y = (dfloat *)y;\n    float * loop_dst = dst;\n\n// partial sum for each thread\n    float tmp = 0.0f;\n    for (int i = 0; i < ncols; i += GGML_CUDA_DMMV_X) {\n        shared_dst[i+tid] = 0;\n    }\n    // __syncthreads();\n    for (int col_id = 0; col_id < src1_ncols; col_id++) {\n        __syncthreads();\n        if (loop_idx[row] < dev_sparse_threshold) {\n            loop_dst += ncols;\n            loop_idx += src1_ne0;\n            loop_y += src1_ne0;\n            continue;\n        }\n        \n\n        for (int i = 0; i < ncols; i += iter_stride)\n        {\n            const int col = i + vals_per_iter * tid;\n            const int ib = (gpu_row * ncols + col) / qk; // x block index\n            const int iqs = (col % qk) / qr;         // x quant index\n            const int iybs = col - col % qk;         // y block start index\n\n// processing >2 values per i iter is faster for fast GPUs\n#pragma unroll\n            for (int j = 0; j < vals_per_iter; j += 2)\n            {\n                // process 2 vals per j iter\n\n                // dequantize\n                // for qr = 2 the iqs needs to increase by 1 per j iter because 2 weights per data val\n                dfloat2 v;\n                dequantize_kernel(vx, ib, iqs + j / qr, v);\n\n                // matrix multiplication\n                // for qr = 2 the y index needs to increase by 1 per j iter because of y_offset = qk/2\n                tmp = v.x * loop_y[row];\n                shared_dst[iybs + iqs + j / qr + 0] = tmp;\n                tmp = v.y * loop_y[row];\n                shared_dst[iybs + iqs + j / qr + y_offset] = tmp;\n            }\n        }\n        /* __syncthreads(); */\n\n        for (int i = 0; i < ncols; i += GGML_CUDA_DMMV_X)\n        {\n            atomicAdd(&loop_dst[i + tid], shared_dst[i + tid]);\n            shared_dst[i+tid] = 0;\n        }\n        loop_dst += ncols;\n        loop_idx += src1_ne0;\n        loop_y += src1_ne0;\n    }\n}\n\ntemplate <int qk, int qr, dequantize_kernel_t dequantize_kernel>\nstatic __global__ void dequantize_mul_mat_vec_sparse(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows, int * lst, float * idx) {\n    // qk = quantized weights per x block\n    // qr = number of quantized weights per data value in x block\n    const int gpu_row = blockIdx.y*blockDim.y + threadIdx.y;\n\n    if (gpu_row >= nrows) {\n        return;\n    }\n\n    int row = lst ? lst[gpu_row] : gpu_row;\n    if (idx[row] < dev_sparse_threshold) {\n        return;\n    }\n\n    const int tid = threadIdx.x;\n\n    const int iter_stride = 2*GGML_CUDA_DMMV_X;\n    const int vals_per_iter = iter_stride / WARP_SIZE; // num quantized vals per thread and i iter\n    const int y_offset = qr == 1 ? 1 : qk/2;\n\n// partial sum for each thread\n#ifdef GGML_CUDA_F16\n    half2 tmp = {0.0f, 0.0f}; // two sums for f16 to take advantage of half2 intrinsics\n#else\n    float tmp = 0.0f;\n#endif // GGML_CUDA_F16\n\n    for (int i = 0; i < ncols; i += iter_stride) {\n        const int col = i + vals_per_iter*tid;\n        const int ib = (gpu_row*ncols + col)/qk; // x block index\n        const int iqs = (col%qk)/qr; // x quant index\n        const int iybs = col - col%qk; // y block start index\n\n// processing >2 values per i iter is faster for fast GPUs\n#pragma unroll\n        for (int j = 0; j < vals_per_iter; j += 2) {\n            // process 2 vals per j iter\n\n            // dequantize\n            // for qr = 2 the iqs needs to increase by 1 per j iter because 2 weights per data val\n            dfloat2 v;\n            dequantize_kernel(vx, ib, iqs + j/qr, v);\n\n            // matrix multiplication\n            // for qr = 2 the y index needs to increase by 1 per j iter because of y_offset = qk/2\n#ifdef GGML_CUDA_F16\n            tmp += __hmul2(v, {\n                y[iybs + iqs + j/qr + 0],\n                y[iybs + iqs + j/qr + y_offset]\n            });\n#else\n            tmp += v.x * y[iybs + iqs + j/qr + 0];\n            tmp += v.y * y[iybs + iqs + j/qr + y_offset];\n#endif // GGML_CUDA_F16\n        }\n    }\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (tid == 0) {\n#ifdef GGML_CUDA_F16\n        dst[row] = tmp.x + tmp.y;\n#else\n        dst[row] = tmp;\n#endif // GGML_CUDA_F16\n    }\n}\n\ntemplate <int qk, int qr, dequantize_kernel_t dequantize_kernel>\nstatic __global__ void dequantize_mul_mat_batch_sparse(const void * __restrict__ vx, const dfloat * __restrict__ y, float * __restrict__ dst, const int ncols, const int nrows, int src1_cols, int dst_ne0,int * lst, float * idx) {\n    // qk = quantized weights per x block\n    // qr = number of quantized weights per data value in x block\n    const int gpu_row = blockIdx.y*blockDim.y + threadIdx.y;\n\n    if (gpu_row >= nrows) {\n        return;\n    }\n    int row = lst ? lst[gpu_row] : gpu_row;\n\n\n    const int tid = threadIdx.x;\n\n    const int iter_stride = 2*GGML_CUDA_DMMV_X;\n    const int vals_per_iter = iter_stride / WARP_SIZE; // num quantized vals per thread and i iter\n    const int y_offset = qr == 1 ? 1 : qk/2;\n    float * loop_idx = idx;;\n    dfloat * loop_y = (dfloat *)y;\n    float * loop_dst = dst;\n\n\n\n    float tmp = 0.0f;\n\n    for (int col_id = 0; col_id < src1_cols; col_id++)\n    {\n        __syncthreads();\n        tmp = 0.0f;\n        if (loop_idx[row] < dev_sparse_threshold)\n        {\n            loop_dst += dst_ne0;\n            loop_idx += dst_ne0;\n            loop_y += ncols;\n            continue;\n        }\n\n        for (int i = 0; i < ncols; i += iter_stride)\n        {\n            const int col = i + vals_per_iter * tid;\n            const int ib = (gpu_row * ncols + col) / qk; // x block index\n            const int iqs = (col % qk) / qr;         // x quant index\n            const int iybs = col - col % qk;         // y block start index\n\n// processing >2 values per i iter is faster for fast GPUs\n#pragma unroll\n            for (int j = 0; j < vals_per_iter; j += 2)\n            {\n                // process 2 vals per j iter\n\n                // dequantize\n                // for qr = 2 the iqs needs to increase by 1 per j iter because 2 weights per data val\n                dfloat2 v;\n                dequantize_kernel(vx, ib, iqs + j / qr, v);\n\n                // matrix multiplication\n\n                tmp += v.x * loop_y[iybs + iqs + j / qr + 0];\n                tmp += v.y * loop_y[iybs + iqs + j / qr + y_offset];\n                // #endif\n            }\n        }\n        atomicAdd(&loop_dst[row], tmp);\n        loop_dst += dst_ne0;\n        loop_idx += dst_ne0;\n        loop_y += ncols;\n    }\n}\n\nstatic __global__ void mul_mat_p021_f16_f32(\n    const void * __restrict__ vx, const float * __restrict__ y, float * __restrict__ dst,\n    const int ncols_x, const int nrows_x, const int nchannels_x, const int nchannels_y) {\n\n    const half * x = (const half *) vx;\n\n    const int row_x = blockDim.y*blockIdx.y + threadIdx.y;\n    const int channel = blockDim.z*blockIdx.z + threadIdx.z;\n    const int channel_x = channel / (nchannels_y / nchannels_x);\n\n    const int nrows_y = ncols_x;\n    const int nrows_dst = nrows_x;\n    const int row_dst = row_x;\n\n    float tmp = 0.0f;\n\n    for (int col_x0 = 0; col_x0 < ncols_x; col_x0 += blockDim.x) {\n        const int col_x = col_x0 + threadIdx.x;\n\n        if (col_x >= ncols_x) {\n            break;\n        }\n\n        // x is transposed and permuted\n        const int ix = row_x*nchannels_x*ncols_x + channel_x*ncols_x + col_x;\n        const float xi = __half2float(x[ix]);\n\n        const int row_y = col_x;\n\n\n        // y is not transposed but permuted\n        const int iy = channel*nrows_y + row_y;\n\n        tmp += xi * y[iy];\n    }\n\n    // dst is not transposed and not permuted\n    const int idst = channel*nrows_dst + row_dst;\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (threadIdx.x == 0) {\n        dst[idst] = tmp;\n    }\n}\n\nstatic __global__ void mul_mat_vec_nc_f16_f32( // nc == non-contiguous\n    const void * __restrict__ vx, const float * __restrict__ y, float * __restrict__ dst, const int ncols_x, const int nrows_x,\n    const int row_stride_x, const int channel_stride_x, const int channel_x_divisor) {\n\n    const half * x = (const half *) vx;\n\n    const int row_x     = blockDim.y*blockIdx.y + threadIdx.y;\n    const int channel   = blockDim.z*blockIdx.z + threadIdx.z;\n    const int channel_x = channel / channel_x_divisor;\n\n    const int nrows_y   = ncols_x;\n    const int nrows_dst = nrows_x;\n    const int row_dst   = row_x;\n\n    const int idst = channel*nrows_dst + row_dst;\n\n    float tmp = 0.0f;\n\n    for (int col_x0 = 0; col_x0 < ncols_x; col_x0 += blockDim.x) {\n        const int col_x = col_x0 + threadIdx.x;\n\n        if (col_x >= ncols_x) {\n            break;\n        }\n\n        const int row_y = col_x;\n\n        const int ix = channel_x*channel_stride_x + row_x*row_stride_x + col_x;\n        const int iy = channel*nrows_y + row_y;\n\n        const float xi = __half2float(x[ix]);\n\n        tmp += xi * y[iy];\n    }\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (threadIdx.x == 0) {\n        dst[idst] = tmp;\n    }\n}\n\nstatic __device__ void cpy_1_f32_f32(const char * cxi, char * cdsti) {\n    const float * xi = (const float *) cxi;\n    float * dsti = (float *) cdsti;\n\n    *dsti = *xi;\n}\n\nstatic __device__ void cpy_1_f32_f16(const char * cxi, char * cdsti) {\n    const float * xi = (const float *) cxi;\n    half * dsti = (half *) cdsti;\n\n    *dsti = __float2half(*xi);\n}\n\nstatic __device__ void cpy_1_f16_f16(const char * cxi, char * cdsti) {\n    const half * xi = (const half *) cxi;\n    half * dsti = (half *) cdsti;\n\n    *dsti = *xi;\n}\n\ntemplate <cpy_kernel_t cpy_1>\nstatic __global__ void cpy_f32_f16(const char * cx, char * cdst, const int ne,\n                                   const int ne00, const int ne01, const int nb00, const int nb01, const int nb02,\n                                   const int ne10, const int ne11, const int nb10, const int nb11, const int nb12) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= ne) {\n        return;\n    }\n\n    // determine indices i02/i12, i01/i11, i00/i10 as a function of index i of flattened tensor\n    // then combine those indices with the corresponding byte offsets to get the total offsets\n    const int i02 = i / (ne00*ne01);\n    const int i01 = (i - i02*ne01*ne00) / ne00;\n    const int i00 = i - i02*ne01*ne00 - i01*ne00;\n    const int x_offset = i00*nb00 + i01*nb01 + i02*nb02;\n\n    const int i12 = i / (ne10*ne11);\n    const int i11 = (i - i12*ne10*ne11) / ne10;\n    const int i10 = i - i12*ne10*ne11 - i11*ne10;\n    const int dst_offset = i10*nb10 + i11*nb11 + i12*nb12;\n\n    cpy_1(cx + x_offset, cdst + dst_offset);\n}\n\nstatic __device__ float rope_yarn_ramp(const float low, const float high, const int i0) {\n    const float y = (i0 / 2 - low) / max(0.001f, high - low);\n    return 1.0f - min(1.0f, max(0.0f, y));\n}\n\nstruct rope_corr_dims {\n    float v[4];\n};\n\n// YaRN algorithm based on LlamaYaRNScaledRotaryEmbedding.py from https://github.com/jquesnelle/yarn\n// MIT licensed. Copyright (c) 2023 Jeffrey Quesnelle and Bowen Peng.\nstatic __device__ void rope_yarn(\n    float theta_extrap, float freq_scale, rope_corr_dims corr_dims, int64_t i0, float ext_factor, float mscale,\n    float * cos_theta, float * sin_theta\n) {\n    // Get n-d rotational scaling corrected for extrapolation\n    float theta_interp = freq_scale * theta_extrap;\n    float theta = theta_interp;\n    if (ext_factor != 0.0f) {\n        float ramp_mix = rope_yarn_ramp(corr_dims.v[0], corr_dims.v[1], i0) * ext_factor;\n        theta = theta_interp * (1 - ramp_mix) + theta_extrap * ramp_mix;\n\n        // Get n-d magnitude scaling corrected for interpolation\n        mscale *= 1.0f + 0.1f * logf(1.0f / freq_scale);\n    }\n    *cos_theta = cosf(theta) * mscale;\n    *sin_theta = sinf(theta) * mscale;\n}\n\n// rope == RoPE == rotary positional embedding\ntemplate<typename T, bool has_pos>\nstatic __global__ void rope(\n    const T * x, T * dst, int ncols, const int32_t * pos, float freq_scale, int p_delta_rows, float freq_base,\n    float ext_factor, float attn_factor, rope_corr_dims corr_dims\n) {\n    const int col = 2*(blockDim.y*blockIdx.y + threadIdx.y);\n\n    if (col >= ncols) {\n        return;\n    }\n\n    const int row = blockDim.x*blockIdx.x + threadIdx.x;\n    const int i = row*ncols + col;\n    const int i2 = row/p_delta_rows;\n\n    const int p = has_pos ? pos[i2] : 0;\n    const float theta_base = p*powf(freq_base, -float(col)/ncols);\n\n    float cos_theta, sin_theta;\n    rope_yarn(theta_base, freq_scale, corr_dims, col, ext_factor, attn_factor, &cos_theta, &sin_theta);\n\n    const float x0 = x[i + 0];\n    const float x1 = x[i + 1];\n\n    dst[i + 0] = x0*cos_theta - x1*sin_theta;\n    dst[i + 1] = x0*sin_theta + x1*cos_theta;\n}\n\ntemplate<typename T, bool has_pos>\nstatic __global__ void rope_neox(\n    const T * x, T * dst, int ncols, const int32_t * pos, float freq_scale, int p_delta_rows, float freq_base,\n    float ext_factor, float attn_factor, rope_corr_dims corr_dims\n) {\n    const int col = 2*(blockDim.y*blockIdx.y + threadIdx.y);\n\n    if (col >= ncols) {\n        return;\n    }\n\n    const int row = blockDim.x*blockIdx.x + threadIdx.x;\n    const int i = row*ncols + col/2;\n    const int i2 = row/p_delta_rows;\n\n    // simplified from `(ib * ncols + col) * (-1 / ncols)`, where ib is assumed to be zero\n    const float cur_rot = -float(col)/ncols;\n\n    const int p = has_pos ? pos[i2] : 0;\n    const float theta_base = p*powf(freq_base, cur_rot);\n\n    float cos_theta, sin_theta;\n    rope_yarn(theta_base, freq_scale, corr_dims, cur_rot, ext_factor, attn_factor, &cos_theta, &sin_theta);\n\n    const float x0 = x[i + 0];\n    const float x1 = x[i + ncols/2];\n\n    dst[i + 0]       = x0*cos_theta - x1*sin_theta;\n    dst[i + ncols/2] = x0*sin_theta + x1*cos_theta;\n}\n\nstatic __global__ void rope_glm_f32(\n    const float * x, float * dst, int ncols, const int32_t * pos, float freq_scale, int p_delta_rows, float freq_base,\n    int n_ctx\n) {\n    const int col = blockDim.x*blockIdx.x + threadIdx.x;\n    const int half_n_dims = ncols/4;\n\n    if (col >= half_n_dims) {\n        return;\n    }\n\n    const int row = blockDim.y*blockIdx.y + threadIdx.y;\n    const int i = row*ncols + col;\n    const int i2 = row/p_delta_rows;\n\n    const float col_theta_scale = powf(freq_base, -2.0f*col/ncols);\n     // FIXME: this is likely wrong\n    const int p = pos != nullptr ? pos[i2] : 0;\n\n    const float theta = min(p, n_ctx - 2)*freq_scale*col_theta_scale;\n    const float sin_theta = sinf(theta);\n    const float cos_theta = cosf(theta);\n\n    const float x0 = x[i + 0];\n    const float x1 = x[i + half_n_dims];\n\n    dst[i + 0]           = x0*cos_theta - x1*sin_theta;\n    dst[i + half_n_dims] = x0*sin_theta + x1*cos_theta;\n\n    const float block_theta = ((float)max(p - n_ctx - 2, 0))*col_theta_scale;\n    const float sin_block_theta = sinf(block_theta);\n    const float cos_block_theta = cosf(block_theta);\n\n    const float x2 = x[i + half_n_dims * 2];\n    const float x3 = x[i + half_n_dims * 3];\n\n    dst[i + half_n_dims * 2] = x2*cos_block_theta - x3*sin_block_theta;\n    dst[i + half_n_dims * 3] = x2*sin_block_theta + x3*cos_block_theta;\n}\n\nstatic __global__ void alibi_f32(const float * x, float * dst, const int ncols, const int k_rows,\n                                 const int n_heads_log2_floor, const float m0, const float m1) {\n    const int col = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (col >= ncols) {\n        return;\n    }\n\n    const int row = blockDim.y*blockIdx.y + threadIdx.y;\n    const int i = row*ncols + col;\n\n    const int k = row/k_rows;\n\n    float m_k;\n    if (k < n_heads_log2_floor) {\n        m_k = powf(m0, k + 1);\n    } else {\n        m_k = powf(m1, 2 * (k - n_heads_log2_floor) + 1);\n    }\n\n    dst[i] = col * m_k + x[i];\n}\n\nstatic __global__ void diag_mask_inf_f32(const float * x, float * dst, const int ncols, const int rows_per_channel, const int n_past) {\n    const int col = blockDim.y*blockIdx.y + threadIdx.y;\n    const int row = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (col >= ncols) {\n        return;\n    }\n\n    const int i = row*ncols + col;\n    // dst[i] = col > n_past + row ? -INFINITY : x[i];\n    dst[i] = x[i] - (col > n_past + row % rows_per_channel) * INT_MAX; // equivalent within rounding error but slightly faster on GPU\n}\n\n// the CUDA soft max implementation differs from the CPU implementation\n// instead of doubles floats are used\nstatic __global__ void soft_max_f32(const float * x, float * dst, const int ncols) {\n    const int row = blockDim.x*blockIdx.x + threadIdx.x;\n    const int block_size = blockDim.y;\n    const int tid = threadIdx.y;\n\n    float max_val = -INFINITY;\n\n    for (int col = tid; col < ncols; col += block_size) {\n        const int i = row*ncols + col;\n        max_val = max(max_val, x[i]);\n    }\n\n    // find the max value in the block\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        max_val = max(max_val, __shfl_xor_sync(0xffffffff, max_val, mask, 32));\n    }\n\n    float tmp = 0.f;\n\n    for (int col = tid; col < ncols; col += block_size) {\n        const int i = row*ncols + col;\n        const float val = expf(x[i] - max_val);\n        tmp += val;\n        dst[i] = val;\n    }\n\n    // sum up partial sums\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    const float inv_tmp = 1.f / tmp;\n\n    for (int col = tid; col < ncols; col += block_size) {\n        const int i = row*ncols + col;\n        dst[i] *= inv_tmp;\n    }\n}\n\nstatic __global__ void scale_f32(const float * x, float * dst, const float scale, const int k) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n\n    dst[i] = scale * x[i];\n}\n\nstatic __global__ void clamp_f32(const float * x, float * dst, const float min, const float max, const int k) {\n    const int i = blockDim.x*blockIdx.x + threadIdx.x;\n\n    if (i >= k) {\n        return;\n    }\n\n    dst[i] = x[i] < min ? min : (x[i] > max ? max : x[i]);\n}\n\nstatic  __global__ void im2col_f32_f16(\n        const float * x, half * dst,\n        int ofs0, int ofs1, int IW, int IH, int CHW,\n        int s0, int s1, int p0, int p1, int d0, int d1) {\n    const int iiw = blockIdx.z * s0 + threadIdx.z * d0 - p0;\n    const int iih = blockIdx.y * s1 + threadIdx.y * d1 - p1;\n\n    const int offset_dst =\n        (threadIdx.x * gridDim.y * gridDim.z + blockIdx.y * gridDim.z + blockIdx.z) * CHW +\n        (blockIdx.x * (blockDim.y * blockDim.z) + threadIdx.y * blockDim.z + threadIdx.z);\n\n    if (iih < 0 || iih >= IH || iiw < 0 || iiw >= IW) {\n        dst[offset_dst] = __float2half(0.0f);\n    } else {\n        const int offset_src =  threadIdx.x * ofs0 + blockIdx.x * ofs1;\n        dst[offset_dst] = __float2half(x[offset_src + iih * IW + iiw]);\n    }\n}\n\ntemplate<int qk, int qr, dequantize_kernel_t dq>\nstatic void get_rows_cuda(const void * x, const int32_t * y, float * dst, const int nrows, const int ncols, cudaStream_t stream) {\n    const dim3 block_dims(CUDA_GET_ROWS_BLOCK_SIZE, 1, 1);\n    const int block_num_x = (ncols + 2*CUDA_GET_ROWS_BLOCK_SIZE - 1) / (2*CUDA_GET_ROWS_BLOCK_SIZE);\n    const dim3 block_nums(block_num_x, nrows, 1);\n    k_get_rows<qk, qr, dq><<<block_nums, block_dims, 0, stream>>>(x, y, dst, ncols);\n}\n\nstatic void add_idx_f32_cuda(const float * x, const float * y, float * dst, float * idx, const int kx, const int ky, cudaStream_t stream) {\n    const int num_blocks = (kx + CUDA_ADD_BLOCK_SIZE - 1) / CUDA_ADD_BLOCK_SIZE;\n    add_f32_idx<<<num_blocks, CUDA_ADD_BLOCK_SIZE, 0, stream>>>(x, y, dst, idx, kx, ky);\n}\n\nstatic void add_f32_cuda(const float * x, const float * y, float * dst, const int kx, const int ky, cudaStream_t stream) {\n    const int num_blocks = (kx + CUDA_ADD_BLOCK_SIZE - 1) / CUDA_ADD_BLOCK_SIZE;\n    add_f32<<<num_blocks, CUDA_ADD_BLOCK_SIZE, 0, stream>>>(x, y, dst, kx, ky);\n}\n\nstatic void add_f16_f32_f16_cuda(const half * x, const float * y, half * dst, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_ADD_BLOCK_SIZE - 1) / CUDA_ADD_BLOCK_SIZE;\n    add_f16_f32_f16<<<num_blocks, CUDA_ADD_BLOCK_SIZE, 0, stream>>>(x, y, dst, k);\n}\n\nstatic void add_f16_f32_f32_cuda(const half * x, const float * y, float * dst, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_ADD_BLOCK_SIZE - 1) / CUDA_ADD_BLOCK_SIZE;\n    add_f16_f32_f32<<<num_blocks, CUDA_ADD_BLOCK_SIZE, 0, stream>>>(x, y, dst, k);\n}\n\nstatic void mul_f32_cuda(const float * x, const float * y, float * dst, const int kx, const int ky, cudaStream_t stream) {\n    const int num_blocks = (kx + CUDA_MUL_BLOCK_SIZE - 1) / CUDA_MUL_BLOCK_SIZE;\n    mul_f32<<<num_blocks, CUDA_MUL_BLOCK_SIZE, 0, stream>>>(x, y, dst, kx, ky);\n}\n\nstatic void gelu_f32_cuda(const float * x, float * dst, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_GELU_BLOCK_SIZE - 1) / CUDA_GELU_BLOCK_SIZE;\n    gelu_f32<<<num_blocks, CUDA_GELU_BLOCK_SIZE, 0, stream>>>(x, dst, k);\n}\n\nstatic void silu_f32_cuda(const float * x, float * dst, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_SILU_BLOCK_SIZE - 1) / CUDA_SILU_BLOCK_SIZE;\n    silu_f32<<<num_blocks, CUDA_SILU_BLOCK_SIZE, 0, stream>>>(x, dst, k);\n}\n\nstatic void relu_f32_cuda(const float * x, float * dst, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_RELU_BLOCK_SIZE - 1) / CUDA_RELU_BLOCK_SIZE;\n    relu_f32<<<num_blocks, CUDA_RELU_BLOCK_SIZE, 0, stream>>>(x, dst, k);\n}\n\nstatic void sqr_f32_cuda(const float * x, float * dst, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_SQR_BLOCK_SIZE - 1) / CUDA_SQR_BLOCK_SIZE;\n    sqr_f32<<<num_blocks, CUDA_SQR_BLOCK_SIZE, 0, stream>>>(x, dst, k);\n}\n\nstatic void norm_f32_cuda(const float * x, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % WARP_SIZE == 0);\n    if (ncols < 1024) {\n        const dim3 block_dims(WARP_SIZE, 1, 1);\n        norm_f32<WARP_SIZE><<<nrows, block_dims, 0, stream>>>(x, dst, ncols);\n    } else {\n        const dim3 block_dims(1024, 1, 1);\n        norm_f32<1024><<<nrows, block_dims, 0, stream>>>(x, dst, ncols);\n    }\n}\n\nstatic void rms_norm_f32_cuda(const float * x, float * dst, const int ncols, const int nrows, const float eps, cudaStream_t stream) {\n    GGML_ASSERT(ncols % WARP_SIZE == 0);\n    if (ncols < 1024) {\n        const dim3 block_dims(WARP_SIZE, 1, 1);\n        rms_norm_f32<WARP_SIZE><<<nrows, block_dims, 0, stream>>>(x, dst, ncols, eps);\n    } else {\n        const dim3 block_dims(1024, 1, 1);\n        rms_norm_f32<1024><<<nrows, block_dims, 0, stream>>>(x, dst, ncols, eps);\n    }\n}\n\nstatic void quantize_row_q8_1_cuda(const float * x, void * vy, const int kx, const int ky, const int kx_padded, cudaStream_t stream) {\n    const int block_num_x = (kx_padded + CUDA_QUANTIZE_BLOCK_SIZE - 1) / CUDA_QUANTIZE_BLOCK_SIZE;\n    const dim3 num_blocks(block_num_x, ky, 1);\n    const dim3 block_size(CUDA_DEQUANTIZE_BLOCK_SIZE, 1, 1);\n    quantize_q8_1<<<num_blocks, block_size, 0, stream>>>(x, vy, kx, kx_padded);\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q4_0_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_DEQUANTIZE_BLOCK_SIZE - 1) / CUDA_DEQUANTIZE_BLOCK_SIZE;\n    dequantize_block<QK4_0, QR4_0, dequantize_q4_0><<<num_blocks, CUDA_DEQUANTIZE_BLOCK_SIZE, 0, stream>>>(vx, y, k);\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q4_1_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_DEQUANTIZE_BLOCK_SIZE - 1) / CUDA_DEQUANTIZE_BLOCK_SIZE;\n    dequantize_block<QK4_1, QR4_1, dequantize_q4_1><<<num_blocks, CUDA_DEQUANTIZE_BLOCK_SIZE, 0, stream>>>(vx, y, k);\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q5_0_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_DEQUANTIZE_BLOCK_SIZE - 1) / CUDA_DEQUANTIZE_BLOCK_SIZE;\n    dequantize_block<QK5_0, QR5_0, dequantize_q5_0><<<num_blocks, CUDA_DEQUANTIZE_BLOCK_SIZE, 0, stream>>>(vx, y, k);\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q5_1_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_DEQUANTIZE_BLOCK_SIZE - 1) / CUDA_DEQUANTIZE_BLOCK_SIZE;\n    dequantize_block<QK5_1, QR5_1, dequantize_q5_1><<<num_blocks, CUDA_DEQUANTIZE_BLOCK_SIZE, 0, stream>>>(vx, y, k);\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q8_0_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_DEQUANTIZE_BLOCK_SIZE - 1) / CUDA_DEQUANTIZE_BLOCK_SIZE;\n    dequantize_block<QK8_0, QR8_0, dequantize_q8_0><<<num_blocks, CUDA_DEQUANTIZE_BLOCK_SIZE, 0, stream>>>(vx, y, k);\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q2_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int nb = k / QK_K;\n#if QK_K == 256\n    dequantize_block_q2_K<<<nb, 64, 0, stream>>>(vx, y);\n#else\n    dequantize_block_q2_K<<<nb, 32, 0, stream>>>(vx, y);\n#endif\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q3_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int nb = k / QK_K;\n#if QK_K == 256\n    dequantize_block_q3_K<<<nb, 64, 0, stream>>>(vx, y);\n#else\n    dequantize_block_q3_K<<<nb, 32, 0, stream>>>(vx, y);\n#endif\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q4_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int nb = k / QK_K;\n    dequantize_block_q4_K<<<nb, 32, 0, stream>>>(vx, y);\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q5_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int nb = k / QK_K;\n#if QK_K == 256\n    dequantize_block_q5_K<<<nb, 64, 0, stream>>>(vx, y);\n#else\n    dequantize_block_q5_K<<<nb, 32, 0, stream>>>(vx, y);\n#endif\n}\n\ntemplate<typename dst_t>\nstatic void dequantize_row_q6_K_cuda(const void * vx, dst_t * y, const int k, cudaStream_t stream) {\n    const int nb = k / QK_K;\n#if QK_K == 256\n    dequantize_block_q6_K<<<nb, 64, 0, stream>>>(vx, y);\n#else\n    dequantize_block_q6_K<<<nb, 32, 0, stream>>>(vx, y);\n#endif\n}\n\nstatic void dequantize_mul_mat_vec_q4_0_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    // the number of rows may exceed maximum grid size in the y or z dimensions, use the x dimension instead\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_vec<QK4_0, QR4_0, dequantize_q4_0>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\nstatic void dequantize_mul_mat_vec_q4_0_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_vec_sparse<QK4_0, QR4_0, dequantize_q4_0>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows, lst, idx);\n}\nstatic void dequantize_mul_mat_batch_q4_0_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, int src1_ncols, int dst_ne0, cudaStream_t stream, int *lst, float *idx) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    // printf(\"ncols %d, nrows %d, src1_ncols %d, dst_ne0 %d\\n\", ncols, nrows, src1_ncols, dst_ne0);\n\n    dequantize_mul_mat_batch_sparse<QK4_0, QR4_0, dequantize_q4_0>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows, src1_ncols, dst_ne0, lst, idx);\n \n}\n\nstatic void dequantize_mul_mat_vec_q4_1_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_vec<QK4_1, QR4_1, dequantize_q4_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void dequantize_mul_mat_vec_q5_0_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_vec<QK5_0, QR5_0, dequantize_q5_0>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void dequantize_mul_mat_vec_q5_1_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_vec<QK5_1, QR5_1, dequantize_q5_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void dequantize_mul_mat_vec_q8_0_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_vec<QK8_0, QR8_0, dequantize_q8_0>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void dequantize_mul_mat_vec_q2_K_cuda(const void * vx, const float * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int ny = 2; // very slightly faster than 1 even when K_QUANTS_PER_ITERATION = 2\n    const int block_num_y = (nrows + ny - 1) / ny;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(32, ny, 1);\n    dequantize_mul_mat_vec_q2_k<<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void dequantize_mul_mat_vec_q3_K_cuda(const void * vx, const float * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int ny = 2 / K_QUANTS_PER_ITERATION;\n    const int block_num_y = (nrows + ny - 1) / ny;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(32, ny, 1);\n    dequantize_mul_mat_vec_q3_k<<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void dequantize_mul_mat_vec_q4_K_cuda(const void * vx, const float * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int ny = 2 / K_QUANTS_PER_ITERATION;\n    const int block_num_y = (nrows + ny - 1) / ny;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(32, ny, 1);\n    dequantize_mul_mat_vec_q4_k<<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void dequantize_mul_mat_vec_q5_K_cuda(const void * vx, const float * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const dim3 block_dims(32, 1, 1);\n    dequantize_mul_mat_vec_q5_k<<<nrows, block_dims, 0, stream>>>(vx, y, dst, ncols);\n}\n\nstatic void dequantize_mul_mat_vec_q6_K_cuda(const void * vx, const float * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int ny = 2 / K_QUANTS_PER_ITERATION;\n    const int block_num_y = (nrows + ny - 1) / ny;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(32, ny, 1);\n    dequantize_mul_mat_vec_q6_k<<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q4_0_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK4_0 == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK4_0, QI4_0, block_q4_0, VDR_Q4_0_Q8_1_MMVQ, vec_dot_q4_0_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\ntemplate <int qk, int qi, typename block_q_t, int vdr, vec_dot_q_cuda_t vec_dot_q_cuda>\nstatic __global__ void mul_mat_vec_q(const void * __restrict__ vx, const void * __restrict__ vy, float * __restrict__ dst, const int ncols, const int nrows,\n        const int *lst, const float *idx) {\n    const int gpu_row = blockIdx.x * blockDim.y + threadIdx.y;\n    const int row = lst ? lst[gpu_row] : gpu_row;\n\n    if (idx[row] < dev_sparse_threshold) {\n        if (threadIdx.x == 0) { dst[gpu_row] = 0; } // Assign 0 in case of extra memset\n        return; \n    }\n\n    const int blocks_per_row    = ncols / qk; // 4096 / 32\n    const int blocks_per_warp   = vdr * WARP_SIZE / qi; // 2 * 32 / 4\n\n    // partial sum for each thread\n    float tmp = 0.0f;\n\n    const block_q_t  * x = (const block_q_t  *) vx;\n    const block_q8_1 * y = (const block_q8_1 *) vy;\n\n    for (int i = 0; i < blocks_per_row; i += blocks_per_warp) {\n        const int ibx = gpu_row*blocks_per_row + i + threadIdx.x / (qi/vdr); // x block index\n\n        const int iby = (i + threadIdx.x / (qi/vdr)) * (qk/QK8_1); // y block index that aligns with ibx\n\n        const int iqs  = vdr * (threadIdx.x % (qi/vdr)); // x block quant index when casting the quants to int\n\n        tmp += vec_dot_q_cuda(&x[ibx], &y[iby], iqs);\n    }\n\n    // sum up partial sums and write back result\n#pragma unroll\n    for (int mask = 16; mask > 0; mask >>= 1) {\n        tmp += __shfl_xor_sync(0xffffffff, tmp, mask, 32);\n    }\n\n    if (threadIdx.x == 0) { dst[row] = tmp; } // Output result\n}\n\nstatic void mul_mat_vec_q4_0_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream, const int *lst, const float *idx) {\n    GGML_ASSERT(ncols % QK4_0 == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK4_0, QI4_0, block_q4_0, VDR_Q4_0_Q8_1_MMVQ, vec_dot_q4_0_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows, lst, idx);\n}\n\n\nstatic void mul_mat_vec_q4_1_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK4_1 == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK4_0, QI4_1, block_q4_1, VDR_Q4_1_Q8_1_MMVQ, vec_dot_q4_1_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q5_0_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK5_0 == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK5_0, QI5_0, block_q5_0, VDR_Q5_0_Q8_1_MMVQ, vec_dot_q5_0_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q5_1_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK5_1 == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK5_1, QI5_1, block_q5_1, VDR_Q5_1_Q8_1_MMVQ, vec_dot_q5_1_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q8_0_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK8_0 == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK8_0, QI8_0, block_q8_0, VDR_Q8_0_Q8_1_MMVQ, vec_dot_q8_0_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q2_K_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK_K, QI2_K, block_q2_K, VDR_Q2_K_Q8_1_MMVQ, vec_dot_q2_K_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q3_K_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK_K, QI3_K, block_q3_K, VDR_Q3_K_Q8_1_MMVQ, vec_dot_q3_K_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q4_K_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK_K, QI4_K, block_q4_K, VDR_Q4_K_Q8_1_MMVQ, vec_dot_q4_K_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q5_K_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK_K, QI5_K, block_q5_K, VDR_Q5_K_Q8_1_MMVQ, vec_dot_q5_K_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void mul_mat_vec_q6_K_q8_1_cuda(const void * vx, const void * vy, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % QK_K == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    mul_mat_vec_q<QK_K, QI6_K, block_q6_K, VDR_Q6_K_Q8_1_MMVQ, vec_dot_q6_K_q8_1>\n        <<<block_nums, block_dims, 0, stream>>>(vx, vy, dst, ncols, nrows);\n}\n\nstatic void convert_fp16_to_fp32_cuda(const void * vx, float * y, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_DEQUANTIZE_BLOCK_SIZE - 1) / CUDA_DEQUANTIZE_BLOCK_SIZE;\n    dequantize_block<1, 1, convert_f16><<<num_blocks, CUDA_DEQUANTIZE_BLOCK_SIZE, 0, stream>>>(vx, y, k);\n}\n\nstatic void convert_fp32_to_fp16_cuda(const void * vx, half * y, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_QUANTIZE_BLOCK_SIZE - 1) / CUDA_QUANTIZE_BLOCK_SIZE;\n    dequantize_block<1, 1, convert_f32><<<num_blocks, CUDA_DEQUANTIZE_BLOCK_SIZE, 0, stream>>>(vx, y, k);\n}\n\nstatic void convert_mul_mat_vec_f16_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(block_num_y, 1, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_vec<1, 1, convert_f16>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows);\n}\nstatic void convert_mul_mat_vec_f16_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n\n    dequantize_mul_mat_vec_sparse<1, 1, convert_f16>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows, lst, idx);\n \n}\nstatic void convert_mul_mat_batch_f16_cuda_sparse(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, int src1_ncols, int dst_ne0, cudaStream_t stream, int *lst, float *idx) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n\n    dequantize_mul_mat_batch_sparse<1, 1, convert_f16>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, nrows, src1_ncols, dst_ne0, lst, idx);\n \n}\n\nstatic void dequantize_axpy_vec_q4_0_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_axpy<QK4_0, QR4_0, dequantize_q4_0>\n        <<<block_nums, block_dims, ncols*sizeof(float), stream>>>(vx, y, dst, ncols, nrows);\n}\nstatic void dequantize_axpy_sparse_vec_q4_0_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx)  {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (ncols + AXPY_BLOCK_X*2 - 1) / AXPY_BLOCK_X / 2;\n    const int block_num_z = nrows / AXPY_BLOCK_Z;  \n    const dim3 block_nums(1, block_num_y, block_num_z);\n    const dim3 block_dims(AXPY_BLOCK_X, AXPY_BLOCK_Y, 1);\n    // dequantize_mul_mat_axpy<QK4_0, QR4_0, dequantize_q4_0>\n    //     <<<block_nums, block_dims, ncols*sizeof(float), stream>>>(vx, y, dst, ncols, nrows);\n    // printf(\"launch kernel: (%d, %d)\\n\", block_num_x, block_num_y);\n    dequantize_mul_mat_axpy_sparse_pro<QK4_0, QR4_0, dequantize_q4_0>\n        <<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols, AXPY_BLOCK_Z, lst, idx);\n}\n\nstatic void dequantize_axpy_sparse_batch_q4_0_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, int src1_rows, int src1_ncols, cudaStream_t stream, int *lst, float *idx) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_axpy_sparse_batch<QK4_0, QR4_0, dequantize_q4_0>\n        <<<block_nums, block_dims, ncols*sizeof(float), stream>>>(vx, y, dst, ncols, nrows, src1_rows, src1_ncols, lst, idx);\n}\nstatic void convert_axpy_vec_f16_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_axpy<1, 1, convert_f16>\n        <<<block_nums, block_dims, ncols*sizeof(float), stream>>>(vx, y, dst, ncols, nrows);\n}\n\nstatic void convert_axpy_sparse_vec_f16_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, cudaStream_t stream, int *lst, float *idx) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_axpy_sparse<1, 1, convert_f16>\n        <<<block_nums, block_dims, ncols*sizeof(float), stream>>>(vx, y, dst, ncols, nrows, lst, idx);\n}\nstatic void convert_axpy_sparse_batch_f16_cuda(const void * vx, const dfloat * y, float * dst, const int ncols, const int nrows, int src1_rows, int src1_ncols, cudaStream_t stream, int *lst, float *idx) {\n    GGML_ASSERT(ncols % GGML_CUDA_DMMV_X == 0);\n    const int block_num_y = (nrows + GGML_CUDA_MMV_Y - 1) / GGML_CUDA_MMV_Y;\n    const dim3 block_nums(1, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, GGML_CUDA_MMV_Y, 1);\n    dequantize_mul_mat_axpy_sparse_batch<1, 1, convert_f16>\n        <<<block_nums, block_dims, ncols*sizeof(float), stream>>>(vx, y, dst, ncols, nrows, src1_rows, src1_ncols, lst, idx);\n}\n\nstatic to_fp16_cuda_t ggml_get_to_fp16_cuda(ggml_type type) {\n    switch (type) {\n        case GGML_TYPE_Q4_0:\n            return dequantize_row_q4_0_cuda;\n        case GGML_TYPE_Q4_1:\n            return dequantize_row_q4_1_cuda;\n        case GGML_TYPE_Q5_0:\n            return dequantize_row_q5_0_cuda;\n        case GGML_TYPE_Q5_1:\n            return dequantize_row_q5_1_cuda;\n        case GGML_TYPE_Q8_0:\n            return dequantize_row_q8_0_cuda;\n        case GGML_TYPE_Q2_K:\n            return dequantize_row_q2_K_cuda;\n        case GGML_TYPE_Q3_K:\n            return dequantize_row_q3_K_cuda;\n        case GGML_TYPE_Q4_K:\n            return dequantize_row_q4_K_cuda;\n        case GGML_TYPE_Q5_K:\n            return dequantize_row_q5_K_cuda;\n        case GGML_TYPE_Q6_K:\n            return dequantize_row_q6_K_cuda;\n        case GGML_TYPE_F32:\n            return convert_fp32_to_fp16_cuda;\n        default:\n            return nullptr;\n    }\n}\n\nstatic to_fp32_cuda_t ggml_get_to_fp32_cuda(ggml_type type) {\n    switch (type) {\n        case GGML_TYPE_Q4_0:\n            return dequantize_row_q4_0_cuda;\n        case GGML_TYPE_Q4_1:\n            return dequantize_row_q4_1_cuda;\n        case GGML_TYPE_Q5_0:\n            return dequantize_row_q5_0_cuda;\n        case GGML_TYPE_Q5_1:\n            return dequantize_row_q5_1_cuda;\n        case GGML_TYPE_Q8_0:\n            return dequantize_row_q8_0_cuda;\n        case GGML_TYPE_Q2_K:\n            return dequantize_row_q2_K_cuda;\n        case GGML_TYPE_Q3_K:\n            return dequantize_row_q3_K_cuda;\n        case GGML_TYPE_Q4_K:\n            return dequantize_row_q4_K_cuda;\n        case GGML_TYPE_Q5_K:\n            return dequantize_row_q5_K_cuda;\n        case GGML_TYPE_Q6_K:\n            return dequantize_row_q6_K_cuda;\n        case GGML_TYPE_F16:\n            return convert_fp16_to_fp32_cuda;\n        default:\n            return nullptr;\n    }\n}\n\nstatic void ggml_mul_mat_q4_0_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q4_0_RDNA2;\n        mmq_y  =  MMQ_Y_Q4_0_RDNA2;\n        nwarps = NWARPS_Q4_0_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q4_0_RDNA1;\n        mmq_y  =  MMQ_Y_Q4_0_RDNA1;\n        nwarps = NWARPS_Q4_0_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q4_0_AMPERE;\n        mmq_y  =  MMQ_Y_Q4_0_AMPERE;\n        nwarps = NWARPS_Q4_0_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q4_0_PASCAL;\n        mmq_y  =  MMQ_Y_Q4_0_PASCAL;\n        nwarps = NWARPS_Q4_0_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q4_0<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q4_0<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_q4_1_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q4_1_RDNA2;\n        mmq_y  =  MMQ_Y_Q4_1_RDNA2;\n        nwarps = NWARPS_Q4_1_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q4_1_RDNA1;\n        mmq_y  =  MMQ_Y_Q4_1_RDNA1;\n        nwarps = NWARPS_Q4_1_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q4_1_AMPERE;\n        mmq_y  =  MMQ_Y_Q4_1_AMPERE;\n        nwarps = NWARPS_Q4_1_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q4_1_PASCAL;\n        mmq_y  =  MMQ_Y_Q4_1_PASCAL;\n        nwarps = NWARPS_Q4_1_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q4_1<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q4_1<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_q5_0_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q5_0_RDNA2;\n        mmq_y  =  MMQ_Y_Q5_0_RDNA2;\n        nwarps = NWARPS_Q5_0_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q5_0_RDNA1;\n        mmq_y  =  MMQ_Y_Q5_0_RDNA1;\n        nwarps = NWARPS_Q5_0_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q5_0_AMPERE;\n        mmq_y  =  MMQ_Y_Q5_0_AMPERE;\n        nwarps = NWARPS_Q5_0_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q5_0_PASCAL;\n        mmq_y  =  MMQ_Y_Q5_0_PASCAL;\n        nwarps = NWARPS_Q5_0_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q5_0<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q5_0<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_q5_1_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q5_1_RDNA2;\n        mmq_y  =  MMQ_Y_Q5_1_RDNA2;\n        nwarps = NWARPS_Q5_1_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q5_1_RDNA1;\n        mmq_y  =  MMQ_Y_Q5_1_RDNA1;\n        nwarps = NWARPS_Q5_1_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q5_1_AMPERE;\n        mmq_y  =  MMQ_Y_Q5_1_AMPERE;\n        nwarps = NWARPS_Q5_1_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q5_1_PASCAL;\n        mmq_y  =  MMQ_Y_Q5_1_PASCAL;\n        nwarps = NWARPS_Q5_1_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q5_1<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q5_1<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_q8_0_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q8_0_RDNA2;\n        mmq_y  =  MMQ_Y_Q8_0_RDNA2;\n        nwarps = NWARPS_Q8_0_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q8_0_RDNA1;\n        mmq_y  =  MMQ_Y_Q8_0_RDNA1;\n        nwarps = NWARPS_Q8_0_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q8_0_AMPERE;\n        mmq_y  =  MMQ_Y_Q8_0_AMPERE;\n        nwarps = NWARPS_Q8_0_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q8_0_PASCAL;\n        mmq_y  =  MMQ_Y_Q8_0_PASCAL;\n        nwarps = NWARPS_Q8_0_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q8_0<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q8_0<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_q2_K_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q2_K_RDNA2;\n        mmq_y  =  MMQ_Y_Q2_K_RDNA2;\n        nwarps = NWARPS_Q2_K_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q2_K_RDNA1;\n        mmq_y  =  MMQ_Y_Q2_K_RDNA1;\n        nwarps = NWARPS_Q2_K_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q2_K_AMPERE;\n        mmq_y  =  MMQ_Y_Q2_K_AMPERE;\n        nwarps = NWARPS_Q2_K_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q2_K_PASCAL;\n        mmq_y  =  MMQ_Y_Q2_K_PASCAL;\n        nwarps = NWARPS_Q2_K_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q2_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q2_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_q3_K_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n#if QK_K == 256\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q3_K_RDNA2;\n        mmq_y  =  MMQ_Y_Q3_K_RDNA2;\n        nwarps = NWARPS_Q3_K_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q3_K_RDNA1;\n        mmq_y  =  MMQ_Y_Q3_K_RDNA1;\n        nwarps = NWARPS_Q3_K_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q3_K_AMPERE;\n        mmq_y  =  MMQ_Y_Q3_K_AMPERE;\n        nwarps = NWARPS_Q3_K_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q3_K_PASCAL;\n        mmq_y  =  MMQ_Y_Q3_K_PASCAL;\n        nwarps = NWARPS_Q3_K_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q3_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q3_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n#endif\n}\n\nstatic void ggml_mul_mat_q4_K_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q4_K_RDNA2;\n        mmq_y  =  MMQ_Y_Q4_K_RDNA2;\n        nwarps = NWARPS_Q4_K_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q4_K_RDNA1;\n        mmq_y  =  MMQ_Y_Q4_K_RDNA1;\n        nwarps = NWARPS_Q4_K_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q4_K_AMPERE;\n        mmq_y  =  MMQ_Y_Q4_K_AMPERE;\n        nwarps = NWARPS_Q4_K_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q4_K_PASCAL;\n        mmq_y  =  MMQ_Y_Q4_K_PASCAL;\n        nwarps = NWARPS_Q4_K_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q4_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q4_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_q5_K_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q5_K_RDNA2;\n        mmq_y  =  MMQ_Y_Q5_K_RDNA2;\n        nwarps = NWARPS_Q5_K_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q5_K_RDNA1;\n        mmq_y  =  MMQ_Y_Q5_K_RDNA1;\n        nwarps = NWARPS_Q5_K_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q5_K_AMPERE;\n        mmq_y  =  MMQ_Y_Q5_K_AMPERE;\n        nwarps = NWARPS_Q5_K_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q5_K_PASCAL;\n        mmq_y  =  MMQ_Y_Q5_K_PASCAL;\n        nwarps = NWARPS_Q5_K_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q5_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q5_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_q6_K_q8_1_cuda(\n    const void * vx, const void * vy, float * dst, const int ncols_x, const int nrows_x,\n    const int ncols_y, const int nrows_y, const int nrows_dst, cudaStream_t stream) {\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    const int compute_capability = g_compute_capabilities[id];\n\n    int mmq_x, mmq_y, nwarps;\n    if (compute_capability >= CC_RDNA2) {\n        mmq_x  =  MMQ_X_Q6_K_RDNA2;\n        mmq_y  =  MMQ_Y_Q6_K_RDNA2;\n        nwarps = NWARPS_Q6_K_RDNA2;\n    } else if (compute_capability >= CC_OFFSET_AMD) {\n        mmq_x  =  MMQ_X_Q6_K_RDNA1;\n        mmq_y  =  MMQ_Y_Q6_K_RDNA1;\n        nwarps = NWARPS_Q6_K_RDNA1;\n    } else if (compute_capability >= CC_VOLTA) {\n        mmq_x  =  MMQ_X_Q6_K_AMPERE;\n        mmq_y  =  MMQ_Y_Q6_K_AMPERE;\n        nwarps = NWARPS_Q6_K_AMPERE;\n    } else if (compute_capability >= MIN_CC_DP4A) {\n        mmq_x  =  MMQ_X_Q6_K_PASCAL;\n        mmq_y  =  MMQ_Y_Q6_K_PASCAL;\n        nwarps = NWARPS_Q6_K_PASCAL;\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    const int block_num_x = (nrows_x + mmq_y - 1) / mmq_y;\n    const int block_num_y = (ncols_y + mmq_x - 1) / mmq_x;\n    const dim3 block_nums(block_num_x, block_num_y, 1);\n    const dim3 block_dims(WARP_SIZE, nwarps, 1);\n\n    if (nrows_x % mmq_y == 0) {\n        const bool need_check = false;\n        mul_mat_q6_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    } else {\n        const bool need_check = true;\n        mul_mat_q6_K<need_check><<<block_nums, block_dims, 0, stream>>>\n            (vx, vy, dst, ncols_x, nrows_x, ncols_y, nrows_y, nrows_dst);\n    }\n}\n\nstatic void ggml_mul_mat_p021_f16_f32_cuda(\n    const void * vx, const float * y, float * dst, const int ncols_x, const int nrows_x,\n    const int nchannels_x, const int nchannels_y, cudaStream_t stream) {\n\n    const dim3 block_nums(1, nrows_x, nchannels_y);\n    const dim3 block_dims(WARP_SIZE, 1, 1);\n    mul_mat_p021_f16_f32<<<block_nums, block_dims, 0, stream>>>(vx, y, dst, ncols_x, nrows_x, nchannels_x, nchannels_y);\n}\n\nstatic void ggml_mul_mat_vec_nc_f16_f32_cuda(\n    const void * vx, const float * y, float * dst, const int ncols_x, const int nrows_x, const int row_stride_x,\n    const int nchannels_x, const int nchannels_y, const int channel_stride_x, cudaStream_t stream) {\n\n    const dim3 block_nums(1, nrows_x, nchannels_y);\n    const dim3 block_dims(WARP_SIZE, 1, 1);\n    mul_mat_vec_nc_f16_f32<<<block_nums, block_dims, 0, stream>>>\n        (vx, y, dst, ncols_x, nrows_x, row_stride_x, channel_stride_x, nchannels_y/nchannels_x);\n}\n\nstatic void ggml_cpy_f32_f32_cuda(\n    const char * cx, char * cdst, const int ne,\n    const int ne00, const int ne01, const int nb00, const int nb01, const int nb02,\n    const int ne10, const int ne11, const int nb10, const int nb11, const int nb12, cudaStream_t stream) {\n\n    const int num_blocks = (ne + CUDA_CPY_BLOCK_SIZE - 1) / CUDA_CPY_BLOCK_SIZE;\n    cpy_f32_f16<cpy_1_f32_f32><<<num_blocks, CUDA_CPY_BLOCK_SIZE, 0, stream>>>\n        (cx, cdst, ne, ne00, ne01, nb00, nb01, nb02, ne10, ne11, nb10, nb11, nb12);\n}\n\nstatic void ggml_cpy_f32_f16_cuda(\n    const char * cx, char * cdst, const int ne,\n    const int ne00, const int ne01, const int nb00, const int nb01, const int nb02,\n    const int ne10, const int ne11, const int nb10, const int nb11, const int nb12, cudaStream_t stream) {\n\n    const int num_blocks = (ne + CUDA_CPY_BLOCK_SIZE - 1) / CUDA_CPY_BLOCK_SIZE;\n    cpy_f32_f16<cpy_1_f32_f16><<<num_blocks, CUDA_CPY_BLOCK_SIZE, 0, stream>>>\n        (cx, cdst, ne, ne00, ne01, nb00, nb01, nb02, ne10, ne11, nb10, nb11, nb12);\n}\n\nstatic void ggml_cpy_f16_f16_cuda(\n    const char * cx, char * cdst, const int ne,\n    const int ne00, const int ne01, const int nb00, const int nb01, const int nb02,\n    const int ne10, const int ne11, const int nb10, const int nb11, const int nb12, cudaStream_t stream) {\n\n    const int num_blocks = (ne + CUDA_CPY_BLOCK_SIZE - 1) / CUDA_CPY_BLOCK_SIZE;\n    cpy_f32_f16<cpy_1_f16_f16><<<num_blocks, CUDA_CPY_BLOCK_SIZE, 0, stream>>>\n        (cx, cdst, ne, ne00, ne01, nb00, nb01, nb02, ne10, ne11, nb10, nb11, nb12);\n}\n\nstatic void scale_f32_cuda(const float * x, float * dst, const float scale, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_SCALE_BLOCK_SIZE - 1) / CUDA_SCALE_BLOCK_SIZE;\n    scale_f32<<<num_blocks, CUDA_SCALE_BLOCK_SIZE, 0, stream>>>(x, dst, scale, k);\n}\n\nstatic void clamp_f32_cuda(const float * x, float * dst, const float min, const float max, const int k, cudaStream_t stream) {\n    const int num_blocks = (k + CUDA_CLAMP_BLOCK_SIZE - 1) / CUDA_CLAMP_BLOCK_SIZE;\n    clamp_f32<<<num_blocks, CUDA_CLAMP_BLOCK_SIZE, 0, stream>>>(x, dst, min, max, k);\n}\n\ntemplate<typename T>\nstatic void rope_cuda(\n    const T * x, T * dst, int ncols, int nrows, const int32_t * pos, float freq_scale, int p_delta_rows,\n    float freq_base, float ext_factor, float attn_factor, rope_corr_dims corr_dims, cudaStream_t stream\n) {\n    GGML_ASSERT(ncols % 2 == 0);\n    const dim3 block_dims(1, CUDA_ROPE_BLOCK_SIZE, 1);\n    const int num_blocks_x = (ncols + 2*CUDA_ROPE_BLOCK_SIZE - 1) / (2*CUDA_ROPE_BLOCK_SIZE);\n    const dim3 block_nums(nrows, num_blocks_x, 1);\n    if (pos == nullptr) {\n        rope<T, false><<<block_nums, block_dims, 0, stream>>>(\n            x, dst, ncols, pos, freq_scale, p_delta_rows, freq_base, ext_factor, attn_factor, corr_dims\n        );\n    } else {\n        rope<T, true><<<block_nums, block_dims, 0, stream>>>(\n            x, dst, ncols, pos, freq_scale, p_delta_rows, freq_base, ext_factor, attn_factor, corr_dims\n        );\n    }\n}\n\ntemplate<typename T>\nstatic void rope_neox_cuda(\n    const T * x, T * dst, int ncols, int nrows, const int32_t * pos, float freq_scale, int p_delta_rows,\n    float freq_base, float ext_factor, float attn_factor, rope_corr_dims corr_dims, cudaStream_t stream\n) {\n    GGML_ASSERT(ncols % 2 == 0);\n    const dim3 block_dims(1, CUDA_ROPE_BLOCK_SIZE, 1);\n    const int num_blocks_x = (ncols + 2*CUDA_ROPE_BLOCK_SIZE - 1) / (2*CUDA_ROPE_BLOCK_SIZE);\n    const dim3 block_nums(nrows, num_blocks_x, 1);\n    if (pos == nullptr) {\n        rope_neox<T, false><<<block_nums, block_dims, 0, stream>>>(\n            x, dst, ncols, pos, freq_scale, p_delta_rows, freq_base, ext_factor, attn_factor, corr_dims\n        );\n    } else {\n        rope_neox<T, true><<<block_nums, block_dims, 0, stream>>>(\n            x, dst, ncols, pos, freq_scale, p_delta_rows, freq_base, ext_factor, attn_factor, corr_dims\n        );\n    }\n}\n\nstatic void rope_glm_f32_cuda(\n    const float * x, float * dst, int ncols, int nrows, const int32_t * pos, float freq_scale, int p_delta_rows,\n    float freq_base, int n_ctx, cudaStream_t stream\n) {\n    GGML_ASSERT(ncols % 4 == 0);\n    const dim3 block_dims(CUDA_ROPE_BLOCK_SIZE/4, 1, 1);\n    const int num_blocks_x = (ncols + CUDA_ROPE_BLOCK_SIZE - 1) / CUDA_ROPE_BLOCK_SIZE;\n    const dim3 block_nums(num_blocks_x, nrows, 1);\n    rope_glm_f32<<<block_nums, block_dims, 0, stream>>>(x, dst, ncols, pos, freq_scale, p_delta_rows, freq_base, n_ctx);\n}\n\nstatic void alibi_f32_cuda(const float * x, float * dst, const int ncols, const int nrows,\n                           const int k_rows, const int n_heads_log2_floor, const float m0,\n                           const float m1, cudaStream_t stream) {\n    const dim3 block_dims(CUDA_ALIBI_BLOCK_SIZE, 1, 1);\n    const int num_blocks_x = (ncols + CUDA_ALIBI_BLOCK_SIZE - 1) / (CUDA_ALIBI_BLOCK_SIZE);\n    const dim3 block_nums(num_blocks_x, nrows, 1);\n    alibi_f32<<<block_nums, block_dims, 0, stream>>>(x, dst, ncols, k_rows, n_heads_log2_floor, m0, m1);\n}\n\nstatic void diag_mask_inf_f32_cuda(const float * x, float * dst, const int ncols_x, const int nrows_x, const int rows_per_channel, const int n_past, cudaStream_t stream) {\n    const dim3 block_dims(1, CUDA_DIAG_MASK_INF_BLOCK_SIZE, 1);\n    const int block_num_x = (ncols_x + CUDA_DIAG_MASK_INF_BLOCK_SIZE - 1) / CUDA_DIAG_MASK_INF_BLOCK_SIZE;\n    const dim3 block_nums(nrows_x, block_num_x, 1);\n    diag_mask_inf_f32<<<block_nums, block_dims, 0, stream>>>(x, dst, ncols_x, rows_per_channel, n_past);\n}\n\nstatic void soft_max_f32_cuda(const float * x, float * dst, const int ncols_x, const int nrows_x, cudaStream_t stream) {\n    const dim3 block_dims(1, WARP_SIZE, 1);\n    const dim3 block_nums(nrows_x, 1, 1);\n    soft_max_f32<<<block_nums, block_dims, 0, stream>>>(x, dst, ncols_x);\n}\n\nstatic void im2col_f32_f16_cuda(const float * x, half * dst,\n    int OH, int IW, int IH, int OW, int IC,\n    int KH, int KW, int N,  int ofs0, int ofs1,\n    int s0, int s1, int p0, int p1, int d0, int d1, cudaStream_t stream) {\n    dim3 block_nums(IC, OH, OW);\n    dim3 block_dims(N,  KH, KW);\n    im2col_f32_f16<<<block_nums, block_dims, 0, stream>>>(x, dst, ofs0, ofs1, IW, IH, (IC * KH * KW), s0, s1, p0, p1, d0, d1);\n}\n\n// buffer pool for cuda\n#define MAX_CUDA_BUFFERS 256\n\nstruct scoped_spin_lock {\n    std::atomic_flag& lock;\n    scoped_spin_lock(std::atomic_flag& lock) : lock(lock) {\n        while (lock.test_and_set(std::memory_order_acquire)) {\n            ; // spin\n        }\n    }\n    ~scoped_spin_lock() {\n        lock.clear(std::memory_order_release);\n    }\n    scoped_spin_lock(const scoped_spin_lock&) = delete;\n    scoped_spin_lock& operator=(const scoped_spin_lock&) = delete;\n};\n\nstruct cuda_buffer {\n    void * ptr = nullptr;\n    size_t size = 0;\n};\n\nstatic cuda_buffer g_cuda_buffer_pool[GGML_CUDA_MAX_DEVICES][MAX_CUDA_BUFFERS];\nstatic std::atomic_flag g_cuda_pool_lock = ATOMIC_FLAG_INIT;\n\nstatic void * ggml_cuda_pool_malloc(size_t size, size_t * actual_size) {\n    scoped_spin_lock lock(g_cuda_pool_lock);\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n#ifdef DEBUG_CUDA_MALLOC\n    int nnz = 0;\n    size_t max_size = 0, tot_size = 0;\n#endif\n    size_t best_diff = 1ull << 36;\n    int ibest = -1;\n    for (int i = 0; i < MAX_CUDA_BUFFERS; ++i) {\n        cuda_buffer& b = g_cuda_buffer_pool[id][i];\n        if (b.ptr != nullptr) {\n#ifdef DEBUG_CUDA_MALLOC\n            ++nnz;\n            tot_size += b.size;\n            if (b.size > max_size) max_size = b.size;\n#endif\n            if (b.size >= size) {\n                size_t diff = b.size - size;\n                if (diff < best_diff) {\n                    best_diff = diff;\n                    ibest = i;\n                    if (!best_diff) {\n                        void * ptr = b.ptr;\n                        *actual_size = b.size;\n                        b.ptr = nullptr;\n                        b.size = 0;\n                        return ptr;\n                    }\n                }\n            }\n        }\n    }\n    if (ibest >= 0) {\n        cuda_buffer& b = g_cuda_buffer_pool[id][ibest];\n        void * ptr = b.ptr;\n        *actual_size = b.size;\n        b.ptr = nullptr;\n        b.size = 0;\n        return ptr;\n    }\n#ifdef DEBUG_CUDA_MALLOC\n    fprintf(stderr, \"%s: %d buffers, max_size = %u MB, tot_size = %u MB, requested %u MB\\n\", __func__, nnz,\n            (uint32_t)(max_size/1024/1024), (uint32_t)(tot_size/1024/1024), (uint32_t)(size/1024/1024));\n#endif\n    void * ptr;\n    size_t look_ahead_size = (size_t) (1.05 * size);\n    look_ahead_size = 256 * ((look_ahead_size + 255)/256);\n    CUDA_CHECK(cudaMalloc((void **) &ptr, look_ahead_size));\n    *actual_size = look_ahead_size;\n    return ptr;\n}\n\nstatic void ggml_cuda_pool_free(void * ptr, size_t size) {\n    scoped_spin_lock lock(g_cuda_pool_lock);\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n\n    for (int i = 0; i < MAX_CUDA_BUFFERS; ++i) {\n        cuda_buffer& b = g_cuda_buffer_pool[id][i];\n        if (b.ptr == nullptr) {\n            b.ptr = ptr;\n            b.size = size;\n            return;\n        }\n    }\n    fprintf(stderr, \"WARNING: cuda buffer pool full, increase MAX_CUDA_BUFFERS\\n\");\n    CUDA_CHECK(cudaFree(ptr));\n}\n\nstatic bool g_cublas_loaded = false;\n\nbool ggml_cublas_loaded(void) {\n    return g_cublas_loaded;\n}\n\nvoid ggml_init_cublas() {\n    static bool initialized = false;\n\n    if (!initialized) {\n\n#ifdef __HIP_PLATFORM_AMD__\n        // Workaround for a rocBLAS bug when using multiple graphics cards:\n        // https://github.com/ROCmSoftwarePlatform/rocBLAS/issues/1346\n        rocblas_initialize();\n        CUDA_CHECK(cudaDeviceSynchronize());\n#endif\n\n        if (cudaGetDeviceCount(&g_device_count) != cudaSuccess) {\n            initialized = true;\n            g_cublas_loaded = false;\n            return;\n        }\n\n        GGML_ASSERT(g_device_count <= GGML_CUDA_MAX_DEVICES);\n        int64_t total_vram = 0;\n#if defined(GGML_CUDA_FORCE_MMQ)\n        fprintf(stderr, \"%s: GGML_CUDA_FORCE_MMQ:   yes\\n\", __func__);\n#else\n        fprintf(stderr, \"%s: GGML_CUDA_FORCE_MMQ:   no\\n\", __func__);\n#endif\n#if defined(CUDA_USE_TENSOR_CORES)\n        fprintf(stderr, \"%s: CUDA_USE_TENSOR_CORES: yes\\n\", __func__);\n#else\n        fprintf(stderr, \"%s: CUDA_USE_TENSOR_CORES: no\\n\", __func__);\n#endif\n        fprintf(stderr, \"%s: found %d \" GGML_CUDA_NAME \" devices:\\n\", __func__, g_device_count);\n        for (int id = 0; id < g_device_count; ++id) {\n            cudaDeviceProp prop;\n            CUDA_CHECK(cudaGetDeviceProperties(&prop, id));\n            fprintf(stderr, \"  Device %d: %s, compute capability %d.%d\\n\", id, prop.name, prop.major, prop.minor);\n\n            g_tensor_split[id] = total_vram;\n            total_vram += prop.totalGlobalMem;\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n            g_compute_capabilities[id] = 100*prop.major + 10*prop.minor + CC_OFFSET_AMD;\n#else\n            g_compute_capabilities[id] = 100*prop.major + 10*prop.minor;\n#endif // defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n        }\n        for (int id = 0; id < g_device_count; ++id) {\n            g_tensor_split[id] /= total_vram;\n        }\n\n        for (int id = 0; id < g_device_count; ++id) {\n            CUDA_CHECK(ggml_cuda_set_device(id));\n\n            // create cuda streams\n            for (int is = 0; is < MAX_STREAMS; ++is) {\n                CUDA_CHECK(cudaStreamCreateWithFlags(&g_cudaStreams[id][is], cudaStreamNonBlocking));\n            }\n\n            // create cublas handle\n            CUBLAS_CHECK(cublasCreate(&g_cublas_handles[id]));\n            CUBLAS_CHECK(cublasSetMathMode(g_cublas_handles[id], CUBLAS_TF32_TENSOR_OP_MATH));\n        }\n\n        // configure logging to stdout\n        // CUBLAS_CHECK(cublasLoggerConfigure(1, 1, 0, nullptr));\n\n        initialized = true;\n        g_cublas_loaded = true;\n    }\n}\n\nvoid ggml_cuda_set_tensor_split(const float * tensor_split) {\n    if (tensor_split == nullptr) {\n        return;\n    }\n    bool all_zero = true;\n    for (int i = 0; i < g_device_count; ++i) {\n        if (tensor_split[i] != 0.0f) {\n            all_zero = false;\n            break;\n        }\n    }\n    if (all_zero) {\n        return;\n    }\n    float split_sum = 0.0f;\n    for (int i = 0; i < g_device_count; ++i) {\n        g_tensor_split[i] = split_sum;\n        split_sum += tensor_split[i];\n    }\n    for (int i = 0; i < g_device_count; ++i) {\n        g_tensor_split[i] /= split_sum;\n    }\n}\n\nvoid * ggml_cuda_host_malloc(size_t size) {\n    if (getenv(\"GGML_CUDA_NO_PINNED\") != nullptr) {\n        return nullptr;\n    }\n\n    void * ptr = nullptr;\n    cudaError_t err = cudaMallocHost((void **) &ptr, size);\n    if (err != cudaSuccess) {\n        // The allocation error can be bypassed. A null ptr will assigned out of this function.\n        // This can fixed the OOM error in WSL.\n        cudaGetLastError();\n        fprintf(stderr, \"WARNING: failed to allocate %.2f MB of pinned memory: %s\\n\",\n            size/1024.0/1024.0, cudaGetErrorString(err));\n        return nullptr;\n    }\n\n    return ptr;\n}\n\nvoid ggml_cuda_host_free(void * ptr) {\n    CUDA_CHECK(cudaFreeHost(ptr));\n}\n\nstatic cudaError_t ggml_cuda_cpy_tensor_2d(\n    void * dst, const struct ggml_tensor * src, int64_t i3, int64_t i2, int64_t i1_low, int64_t i1_high, cudaStream_t stream) {\n\n    cudaMemcpyKind kind;\n    char * src_ptr;\n    if (src->backend == GGML_BACKEND_CPU) {\n        kind = cudaMemcpyHostToDevice;\n        src_ptr = (char *) src->data;\n    } else if (src->backend == GGML_BACKEND_GPU || src->backend == GGML_BACKEND_GPU_SPLIT) {\n        GGML_ASSERT(src->backend != GGML_BACKEND_GPU_SPLIT || (i1_low == 0 && i1_high == src->ne[1]));\n        kind = cudaMemcpyDeviceToDevice;\n        ggml_tensor_extra_gpu * extra = (ggml_tensor_extra_gpu *) src->extra;\n        int id;\n        CUDA_CHECK(cudaGetDevice(&id));\n        src_ptr = (char *) extra->data_device[id];\n    } else {\n        GGML_ASSERT(false);\n    }\n    char * dst_ptr = (char *) dst;\n\n    const int64_t ne0 = src->ne[0];\n    const int64_t nb0 = src->nb[0];\n    const int64_t nb1 = src->nb[1];\n    const int64_t nb2 = src->nb[2];\n    const int64_t nb3 = src->nb[3];\n    const enum ggml_type type = src->type;\n    const int64_t ts = ggml_type_size(type);\n    const int64_t bs = ggml_blck_size(type);\n    int64_t i1_diff = i1_high - i1_low;\n\n    const char * x = src_ptr + i1_low*nb1 + i2*nb2 + i3*nb3;\n    if (nb0 == ts && nb1 == ts*ne0/bs) {\n        return cudaMemcpyAsync(dst_ptr, x, i1_diff*nb1, kind, stream);\n    } else if (nb0 == ts) {\n        return cudaMemcpy2DAsync(dst_ptr, ts*ne0/bs, x, nb1, ts*ne0/bs, i1_diff, kind, stream);\n    } else {\n        for (int64_t i1 = 0; i1 < i1_diff; i1++) {\n            const void * rx = (const void *) ((const char *) x + i1*nb1);\n            void * rd = (void *) (dst_ptr + i1*ts*ne0/bs);\n            // pretend the row is a matrix with cols=1\n            cudaError_t r = cudaMemcpy2DAsync(rd, ts/bs, rx, nb0, ts/bs, ne0, kind, stream);\n            if (r != cudaSuccess) return r;\n        }\n        return cudaSuccess;\n    }\n}\n\nstatic void ggml_cuda_op_repeat(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_d, const float * src1_d, float * dst_d, const cudaStream_t & stream) {\n    // guaranteed to be an integer due to the check in ggml_can_repeat\n    const int64_t ne0 = dst->ne[0];\n    const int64_t ne1 = dst->ne[1];\n    const int64_t ne2 = dst->ne[2];\n    const int64_t ne3 = dst->ne[3];\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n    const int64_t ne03 = src0->ne[3];\n\n    const size_t nb0 = dst->nb[0];\n    const size_t nb1 = dst->nb[1];\n    const size_t nb2 = dst->nb[2];\n    const size_t nb3 = dst->nb[3];\n\n    const size_t nb00 = src0->nb[0];\n    const size_t nb01 = src0->nb[1];\n    const size_t nb02 = src0->nb[2];\n    const size_t nb03 = src0->nb[3];\n\n    const int nr0 = (int)(ne0/ne00);\n    const int nr1 = (int)(ne1/ne01);\n    const int nr2 = (int)(ne2/ne02);\n    const int nr3 = (int)(ne3/ne03);\n\n    // TODO: support for transposed / permuted tensors\n    GGML_ASSERT(nb0  == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    // TODO: very inefficient, implement in a kernel, or fewer cudaMemcpyAsync calls for contiguous tensors\n    for                         (int i3 = 0; i3 < nr3;  i3++) {\n        for                     (int k3 = 0; k3 < ne03; k3++) {\n            for                 (int i2 = 0; i2 < nr2;  i2++) {\n                for             (int k2 = 0; k2 < ne02; k2++) {\n                    for         (int i1 = 0; i1 < nr1;  i1++) {\n                        for     (int k1 = 0; k1 < ne01; k1++) {\n                            for (int i0 = 0; i0 < nr0;  i0++) {\n                                CUDA_CHECK(cudaMemcpyAsync(\n                                              (char *)  dst_d + (i3*ne03 + k3)*nb3  + (i2*ne02 + k2)*nb2  + (i1*ne01 + k1)*nb1  + (i0*ne00)*nb0,\n                                        (const char *) src0_d + (          k3)*nb03 + (          k2)*nb02 + (          k1)*nb01,\n                                        ne00*nb0, cudaMemcpyDeviceToDevice, stream));\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n\n    (void) src1;\n    (void) src1_d;\n}\n\nstatic void ggml_cuda_op_get_rows(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_d, const float * src1_d, float * dst_d, const cudaStream_t & stream) {\n\n    GGML_ASSERT(src1->type == GGML_TYPE_I32);\n    GGML_ASSERT(dst->type == GGML_TYPE_F32);\n    GGML_ASSERT(ggml_is_contiguous(src0));\n    GGML_ASSERT(ggml_is_contiguous(src1));\n    GGML_ASSERT(ggml_is_contiguous(dst));\n\n    const int ncols = src0->ne[0];\n    const int nrows = ggml_nelements(src1);\n\n    const int32_t * src1_i32 = (const int32_t *) src1_d;\n\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            get_rows_cuda<1, 1, convert_f16>(src0_d, src1_i32, dst_d, nrows, ncols, stream);\n            break;\n        case GGML_TYPE_F32:\n            get_rows_cuda<1, 1, convert_f32>(src0_d, src1_i32, dst_d, nrows, ncols, stream);\n            break;\n        case GGML_TYPE_Q4_0:\n            get_rows_cuda<QK4_0, QR4_0, dequantize_q4_0>(src0_d, src1_i32, dst_d, nrows, ncols, stream);\n            break;\n        case GGML_TYPE_Q4_1:\n            get_rows_cuda<QK4_1, QR4_1, dequantize_q4_1>(src0_d, src1_i32, dst_d, nrows, ncols, stream);\n            break;\n        case GGML_TYPE_Q5_0:\n            get_rows_cuda<QK5_0, QR5_0, dequantize_q5_0>(src0_d, src1_i32, dst_d, nrows, ncols, stream);\n            break;\n        case GGML_TYPE_Q5_1:\n            get_rows_cuda<QK5_1, QR5_1, dequantize_q5_1>(src0_d, src1_i32, dst_d, nrows, ncols, stream);\n            break;\n        case GGML_TYPE_Q8_0:\n            get_rows_cuda<QK8_0, QR8_0, dequantize_q8_0>(src0_d, src1_i32, dst_d, nrows, ncols, stream);\n            break;\n        default:\n            // TODO: k-quants\n            GGML_ASSERT(false);\n            break;\n    }\n}\n\nstatic cudaError_t ggml_cuda_cpy_tensor_1d(\n    void * dst, const struct ggml_tensor * src, int64_t i1_low, int64_t i1_high, cudaStream_t stream) {\n    cudaMemcpyKind kind;\n    char * src_ptr;\n    if (src->backend == GGML_BACKEND_CPU) {\n        kind = cudaMemcpyHostToDevice;\n        src_ptr = (char *) src->data;\n    } else if (src->backend == GGML_BACKEND_GPU || src->backend == GGML_BACKEND_GPU_SPLIT) {\n        GGML_ASSERT(src->backend != GGML_BACKEND_GPU_SPLIT || (i1_low == 0 && i1_high == src->ne[1]));\n        kind = cudaMemcpyDeviceToDevice;\n        struct ggml_tensor_extra_gpu * extra = (ggml_tensor_extra_gpu *) src->extra;\n        int id;\n        CUDA_CHECK(cudaGetDevice(&id));\n        src_ptr = (char *) extra->data_device[id];\n    } else {\n        GGML_ASSERT(false);\n    }\n\n    char * dst_ptr = (char *) dst;\n\n    const int64_t ne0 = src->ne[0];\n    const int64_t nb0 = src->nb[0];\n    const int64_t blck = ggml_blck_size(src->type);\n\n    const enum ggml_type type = src->type;\n    const int64_t ts = ggml_type_size(type);\n    const int64_t bs = ggml_blck_size(type);\n    int64_t i1_diff = i1_high - i1_low;\n\n    const char * x = src_ptr + i1_low*nb0/blck;\n    return cudaMemcpyAsync(dst_ptr, x, i1_diff*nb0/blck, kind, stream);\n}\n\nvoid ggml_cuda_cpy_1d(struct ggml_tensor * dst, const struct ggml_tensor * src) {\n    ggml_cuda_set_device(g_main_device);\n    const cudaStream_t main_stream = g_cudaStreams[g_main_device][0];\n\n    // TODO: only supports CPU -> GPU as of now\n    GGML_ASSERT(src->backend == GGML_BACKEND_CPU && dst->backend == GGML_BACKEND_GPU);\n    struct ggml_tensor_extra_gpu * dst_extra = (ggml_tensor_extra_gpu *) dst->extra;\n\n    CUDA_CHECK(ggml_cuda_cpy_tensor_1d(dst_extra->data_device[0], src, 0, src->ne[0], main_stream));\n}\n\nvoid ** ggml_cuda_get_data_pp(struct ggml_tensor * tensor) {\n    // only supports one device for now\n    GGML_ASSERT(tensor->backend == GGML_BACKEND_GPU);\n    struct ggml_tensor_extra_gpu * extra = (ggml_tensor_extra_gpu *) tensor->extra;\n    return &extra->data_device[0];\n}\n\ninline void ggml_cuda_op_add(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n\n    if (src0->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32) {\n        ggml_tensor * src2 = dst->src[2];\n        if (src2 == NULL)\n            add_f32_cuda(src0_dd, src1_dd, dst_dd, ggml_nelements(src0), ne10*ne11, main_stream);\n        else {\n            float *idx = (src2->backend == GGML_BACKEND_GPU) ? (float *)((ggml_tensor_extra_gpu *)(src2->extra))->data_device[0] : (float *)src2->data;\n            add_idx_f32_cuda(src0_dd, src1_dd, dst_dd, idx, ggml_nelements(src0), ne10*ne11, main_stream);\n        }\n    } else if (src0->type == GGML_TYPE_F16 && dst->type == GGML_TYPE_F16) {\n        add_f16_f32_f16_cuda((const half *) src0_dd, src1_dd, (half *) dst_dd, ggml_nelements(src0), main_stream);\n    } else if (src0->type == GGML_TYPE_F16 && dst->type == GGML_TYPE_F32) {\n        add_f16_f32_f32_cuda((const half *) src0_dd, src1_dd, dst_dd, ggml_nelements(src0), main_stream);\n    } else {\n        fprintf(stderr, \"src0->type: %d  dst->type: %d\\n\", src0->type, dst->type);\n        GGML_ASSERT(false);\n    }\n\n    (void) src1;\n    (void) dst;\n}\n\ninline void ggml_cuda_op_mul(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n\n    mul_f32_cuda(src0_dd, src1_dd, dst_dd, ggml_nelements(src0), ne10*ne11, main_stream);\n\n    (void) dst;\n}\n\ninline void ggml_cuda_op_gelu(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    gelu_f32_cuda(src0_dd, dst_dd, ggml_nelements(src0), main_stream);\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_silu(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    silu_f32_cuda(src0_dd, dst_dd, ggml_nelements(src0), main_stream);\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_relu(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    relu_f32_cuda(src0_dd, dst_dd, ggml_nelements(src0), main_stream);\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_sqr(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    sqr_f32_cuda(src0_dd, dst_dd, ggml_nelements(src0), main_stream);\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_norm(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t nrows = ggml_nrows(src0);\n\n    norm_f32_cuda(src0_dd, dst_dd, ne00, nrows, main_stream);\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_rms_norm(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t nrows = ggml_nrows(src0);\n\n    float eps;\n    memcpy(&eps, dst->op_params, sizeof(float));\n\n    rms_norm_f32_cuda(src0_dd, dst_dd, ne00, nrows, eps, main_stream);\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_mul_mat_q(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    const int64_t ne00 = src0->ne[0];\n\n    const int64_t ne10 = src1->ne[0];\n    GGML_ASSERT(ne10 % QK8_1 == 0);\n\n    const int64_t ne0 = dst->ne[0];\n\n    const int64_t row_diff = row_high - row_low;\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n\n    // the main device has a larger memory buffer to hold the results from all GPUs\n    // nrows_dst == nrows of the matrix that the dequantize_mul_mat kernel writes into\n    const int64_t nrows_dst = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n\n    switch (src0->type) {\n        case GGML_TYPE_Q4_0:\n            ggml_mul_mat_q4_0_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q4_1:\n            ggml_mul_mat_q4_1_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q5_0:\n            ggml_mul_mat_q5_0_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q5_1:\n            ggml_mul_mat_q5_1_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q8_0:\n            ggml_mul_mat_q8_0_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q2_K:\n            ggml_mul_mat_q2_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q3_K:\n            ggml_mul_mat_q3_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q4_K:\n            ggml_mul_mat_q4_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q5_K:\n            ggml_mul_mat_q5_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        case GGML_TYPE_Q6_K:\n            ggml_mul_mat_q6_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, src1_ncols, src1_padded_row_size, nrows_dst, stream);\n            break;\n        default:\n            GGML_ASSERT(false);\n            break;\n    }\n\n    (void) src1;\n    (void) dst;\n    (void) src1_ddf_i;\n}\n\ninline void * ggml_cuda_get_tensor_data(const ggml_tensor * tensor) {\n    if (tensor->backend == GGML_BACKEND_CPU) {\n        return tensor->data;\n    } else if (tensor->backend == GGML_BACKEND_GPU) {\n        ggml_tensor_extra_gpu * extra = (ggml_tensor_extra_gpu *) tensor->extra;\n        return extra->data_device[0];\n    } else {\n        GGML_ASSERT(false);\n    }\n}\n\ninline void ggml_cuda_op_mul_mat_batch_sparse(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    const int64_t ne00 = src0->ne[0];\n\n    const int64_t ne10 = src1->ne[0];\n    GGML_ASSERT(ne10 % QK8_1 == 0);\n\n    const int64_t ne0 = dst->ne[0];\n\n    const int64_t row_diff = row_high - row_low;\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n\n    // the main device has a larger memory buffer to hold the results from all GPUs\n    // nrows_dst == nrows of the matrix that the dequantize_mul_mat kernel writes into\n    const int64_t nrows_dst = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n\n    float * sparse_idx = static_cast<float *>(ggml_cuda_get_tensor_data(dst->src[2]));\n    int32_t * row_lookup = dst->src[3] != NULL ? static_cast<int32_t *>(ggml_cuda_get_tensor_data(dst->src[3])) : NULL;\n    cudaMemsetAsync(dst_dd_i, 0, ggml_nbytes(dst), stream);\n\n    switch (src0->type) {\n        case GGML_TYPE_Q4_0:\n            dequantize_mul_mat_batch_q4_0_cuda_sparse(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, src1_ncols, dst->ne[0], stream, row_lookup, sparse_idx);\n            break;\n        case GGML_TYPE_F16:\n            convert_mul_mat_batch_f16_cuda_sparse(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, src1_ncols, dst->ne[0], stream, row_lookup, sparse_idx);\n            break;\n        default:\n            GGML_ASSERT(false && \"Unsupported type\");\n            break;\n    }\n\n    (void) src1;\n    (void) dst;\n    (void) src1_ddf_i;\n}\n\nstatic int64_t get_row_rounding(ggml_type type) {\n    int64_t min_compute_capability = INT_MAX;\n    int64_t max_compute_capability = INT_MIN;\n    for (int64_t id = 0; id < g_device_count; ++id) {\n        if (g_tensor_split[id] < (id + 1 < g_device_count ? g_tensor_split[id + 1] : 1.0f)) {\n            if (min_compute_capability > g_compute_capabilities[id]) {\n                min_compute_capability = g_compute_capabilities[id];\n            }\n            if (max_compute_capability < g_compute_capabilities[id]) {\n                max_compute_capability = g_compute_capabilities[id];\n            }\n        }\n    }\n\n#if defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n    switch(type) {\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n            return max_compute_capability >= CC_RDNA2 ? 128 : 64;\n        case GGML_TYPE_F16:\n            return 1;\n        case GGML_TYPE_Q2_K:\n            return max_compute_capability >= CC_RDNA2 ? 128 : 32;\n        case GGML_TYPE_Q3_K:\n            return min_compute_capability < CC_RDNA2 ? 128 : 64;\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n            return max_compute_capability >= CC_RDNA2 ? 128 : 64;\n        default:\n            GGML_ASSERT(false);\n    }\n#else\n    switch(type) {\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n            return max_compute_capability >= CC_VOLTA ? 128 : 64;\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n            return 64;\n        case GGML_TYPE_F16:\n            return 1;\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n            return max_compute_capability >= CC_VOLTA ? 128 : 64;\n        case GGML_TYPE_Q6_K:\n            return 64;\n        default:\n            GGML_ASSERT(false);\n    }\n#endif // defined(GGML_USE_HIPBLAS) && defined(__HIP_PLATFORM_AMD__)\n}\n\n__global__ void copyKernel(float* dst, float* src, int len, int* flag) {\n    int id = blockIdx.x * blockDim.x + threadIdx.x;\n    if (id < len ) {\n        dst[id] = src[flag[id]];\n    }\n}\n\ninline void ggml_cuda_op_mul_mat_vec_q(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t row_diff = row_high - row_low;\n\n    switch (src0->type) {\n        case GGML_TYPE_Q4_0:\n            mul_mat_vec_q4_0_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q4_1:\n            mul_mat_vec_q4_1_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q5_0:\n            mul_mat_vec_q5_0_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q5_1:\n            mul_mat_vec_q5_1_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q8_0:\n            mul_mat_vec_q8_0_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q2_K:\n            mul_mat_vec_q2_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q3_K:\n            mul_mat_vec_q3_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q4_K:\n            mul_mat_vec_q4_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q5_K:\n            mul_mat_vec_q5_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q6_K:\n            mul_mat_vec_q6_K_q8_1_cuda(src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        default:\n            GGML_ASSERT(false);\n            break;\n    }\n\n    (void) src1;\n    (void) dst;\n    (void) src1_ddf_i;\n    (void) src1_ncols;\n    (void) src1_padded_row_size;\n}\n\ninline void ggml_cuda_op_dequantize_mul_mat_vec(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t row_diff = row_high - row_low;\n\n    // on some GPUs it is faster to convert src1 to half and to use half precision intrinsics\n#ifdef GGML_CUDA_F16\n    size_t ash;\n    dfloat * src1_dfloat = nullptr; // dfloat == half\n\n    bool src1_convert_f16 = src0->type == GGML_TYPE_Q4_0 || src0->type == GGML_TYPE_Q4_1 ||\n        src0->type == GGML_TYPE_Q5_0 || src0->type == GGML_TYPE_Q5_1 ||\n        src0->type == GGML_TYPE_Q8_0 || src0->type == GGML_TYPE_F16;\n\n    if (src1_convert_f16) {\n        src1_dfloat = (half *) ggml_cuda_pool_malloc(ne00*sizeof(half), &ash);\n        ggml_cpy_f32_f16_cuda((const char *) src1_ddf_i, (char *) src1_dfloat, ne00,\n                                ne00, 1, sizeof(float), 0, 0,\n                                ne00, 1, sizeof(half),  0, 0, stream);\n    }\n#else\n    const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i; // dfloat == float, no conversion\n#endif // GGML_CUDA_F16\n\n    switch (src0->type) {\n        case GGML_TYPE_Q4_0:\n            dequantize_mul_mat_vec_q4_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q4_1:\n            dequantize_mul_mat_vec_q4_1_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q5_0:\n            dequantize_mul_mat_vec_q5_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q5_1:\n            dequantize_mul_mat_vec_q5_1_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q8_0:\n            dequantize_mul_mat_vec_q8_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q2_K:\n            dequantize_mul_mat_vec_q2_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q3_K:\n            dequantize_mul_mat_vec_q3_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q4_K:\n            dequantize_mul_mat_vec_q4_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q5_K:\n            dequantize_mul_mat_vec_q5_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_Q6_K:\n            dequantize_mul_mat_vec_q6_K_cuda(src0_dd_i, src1_ddf_i, dst_dd_i, ne00, row_diff, stream);\n            break;\n        case GGML_TYPE_F16:\n            convert_mul_mat_vec_f16_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);\n            break;\n        default:\n            GGML_ASSERT(false);\n            break;\n    }\n\n#ifdef GGML_CUDA_F16\n    if (src1_convert_f16) {\n        ggml_cuda_pool_free(src1_dfloat, ash);\n    }\n#endif // GGML_CUDA_F16\n\n    (void) src1;\n    (void) dst;\n    (void) src1_ddq_i;\n    (void) src1_ncols;\n    (void) src1_padded_row_size;\n}\n\ninline void ggml_cuda_op_mul_mat_vec_sparse_q(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne10 = src1->ne[1];\n    const int64_t row_diff = row_high - row_low;\n\n    float * sparse_idx = static_cast<float *>(ggml_cuda_get_tensor_data(dst->src[2]));\n    int32_t * row_lookup = dst->src[3] != NULL ? static_cast<int32_t *>(ggml_cuda_get_tensor_data(dst->src[3])) : NULL;\n\n    // on some GPUs it is faster to convert src1 to half and to use half precision intrinsics\n#ifdef GGML_CUDA_F16\n    size_t ash;\n    dfloat * src1_dfloat = nullptr; // dfloat == half\n\n    bool src1_convert_f16 = src0->type == GGML_TYPE_Q4_0 || src0->type == GGML_TYPE_Q4_1 ||\n        src0->type == GGML_TYPE_Q5_0 || src0->type == GGML_TYPE_Q5_1 ||\n        src0->type == GGML_TYPE_Q8_0 || src0->type == GGML_TYPE_F16;\n\n    if (src1_convert_f16) {\n        src1_dfloat = (half *) ggml_cuda_pool_malloc(ne00*sizeof(half), &ash);\n        ggml_cpy_f32_f16_cuda((const char *) src1_ddf_i, (char *) src1_dfloat, ne00,\n                                ne00, 1, sizeof(float), 0, 0,\n                                ne00, 1, sizeof(half),  0, 0, stream);\n    }\n#else\n    const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i; // dfloat == float, no conversion\n#endif // GGML_CUDA_F16\n\n    cudaMemsetAsync((void *)dst_dd_i, 0, ggml_nbytes(dst), stream);\n    switch (src0->type) {\n        case GGML_TYPE_Q4_0:\n             mul_mat_vec_q4_0_q8_1_cuda(\n                src0_dd_i, src1_ddq_i, dst_dd_i, ne00, row_diff, stream,\n                row_lookup, sparse_idx\n            );\n            break;\n        default:\n            GGML_ASSERT(false && \"Unsupported type\");\n            break;\n    }\n\n    (void) src1;\n    (void) dst;\n    (void) src1_ddf_i;\n    (void) src1_ncols;\n    (void) src1_padded_row_size;\n}\n\ninline void ggml_cuda_op_mul_mat_vec_sparse_dequantized(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne10 = src1->ne[1];\n    const int64_t row_diff = row_high - row_low;\n\n    float * sparse_idx = static_cast<float *>(ggml_cuda_get_tensor_data(dst->src[2]));\n    int32_t * row_lookup = dst->src[3] != NULL ? static_cast<int32_t *>(ggml_cuda_get_tensor_data(dst->src[3])) : NULL;\n\n    // on some GPUs it is faster to convert src1 to half and to use half precision intrinsics\n#ifdef GGML_CUDA_F16\n    size_t ash;\n    dfloat * src1_dfloat = nullptr; // dfloat == half\n\n    bool src1_convert_f16 = src0->type == GGML_TYPE_Q4_0 || src0->type == GGML_TYPE_Q4_1 ||\n        src0->type == GGML_TYPE_Q5_0 || src0->type == GGML_TYPE_Q5_1 ||\n        src0->type == GGML_TYPE_Q8_0 || src0->type == GGML_TYPE_F16;\n\n    if (src1_convert_f16) {\n        src1_dfloat = (half *) ggml_cuda_pool_malloc(ne00*sizeof(half), &ash);\n        ggml_cpy_f32_f16_cuda((const char *) src1_ddf_i, (char *) src1_dfloat, ne00,\n                                ne00, 1, sizeof(float), 0, 0,\n                                ne00, 1, sizeof(half),  0, 0, stream);\n    }\n#else\n    const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i; // dfloat == float, no conversion\n#endif // GGML_CUDA_F16\n\n    cudaMemsetAsync((void *)dst_dd_i, 0, ggml_nbytes(dst), stream);\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            convert_mul_mat_vec_f16_cuda_sparse(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream, row_lookup, sparse_idx);\n            break;\n        default:\n            GGML_ASSERT(false && \"Unsupported type\");\n            break;\n    }\n\n    (void) src1;\n    (void) dst;\n    (void) src1_ddf_i;\n    (void) src1_ncols;\n    (void) src1_padded_row_size;\n}\n\n__global__ void compute_positions(float *idx, int *positions, int size, int *positions_out) {\n    int i = blockIdx.x * blockDim.x + threadIdx.x;\n    if (i >= size) return;\n    if (i < size) {\n        positions[i] = idx[i] > 0.5 ? 1 : 0;\n    }\n    __syncthreads();\n\n    for (int stride = 1; stride < size; stride *= 2) {\n        int temp = 0;\n        if (i >= stride) temp = positions[i - stride];\n        __syncthreads(); //Do we really need this sync? \n        positions[i] += temp;\n        __syncthreads();\n    }\n    if(i == 0)\n        positions_out[0] = positions[size - 1];\n}\n\n#define BLOCK_SIZE 32\n\n// still cublas spec N refers to rows, M refers to cols origin\n// N 4096 M 16384\n__global__ inline void transpose_cont(float *A, float *B, int N, int M, int stride_0, int strideA_1, int strideB_1, float *idx) {\n    int row = blockIdx.x;\n    if (row >= N) return;\n    int copy_iter = (M + BLOCK_SIZE - 1) / BLOCK_SIZE; \n    copy_iter = M;\n    int tid = threadIdx.x;\n    // Loop over the A and B matrices in blocks of BLOCK_SIZE\n    int offset = row * strideB_1;\n    for (int i = tid; i < copy_iter; i+=blockDim.x) {\n        // int load_idx = i * BLOCK_SIZE + tid;\n        int load_idx = i;\n        // Load elements into shared memory\n        if (load_idx < M) {\n            B[offset + load_idx] = A[row + load_idx * strideA_1]; // è€ƒè™‘åˆ°äº†Aæ˜¯è½¬ç½®çŸ©é˜µ\n        } else {\n            B[offset + load_idx] = 0.0f;\n        }\n    }\n}\n__global__ void markRows(float *X, int *marks, int rows) {\n    //TODO :idx need to bucket\n    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n    int predict_idx = idx;\n    if (idx < rows) {\n        marks[idx] = (X[predict_idx] >= 0) ? 1 : 0;\n    }\n    else {\n        marks[idx] = 0;\n        return;\n    }\n\n\n}\n\n__global__ void markRowsPosition(int *input, int *output, int rows, int *cols) {\n    //TODO :idx need to bucket\n    int idx = threadIdx.x + blockDim.x * blockIdx.x;\n    int predict_idx = idx;\n    if (idx >= rows) return;\n\n    int32_t sum = 0;\n    for (size_t i = 0; i <= idx; ++i)\n    {\n        sum += input[i];\n    }\n    output[idx] = sum;\n    if (idx != rows -1) return;\n    else {\n        *cols = sum;\n    }\n\n}\n\n// rows for A\n__global__ void copyRows(float *A, float *B, int *prefixSum, int rows, int cols) {\n    int row = blockIdx.x;\n    if (row >= rows) return;\n    int copy_iter = cols; \n    int tid = threadIdx.x;\n    // Loop over the A and B matrices in blocks of BLOCK_SIZE\n    if (prefixSum[row] == 0 || prefixSum[row] == prefixSum[row - 1]) return;\n    int offset = (prefixSum[row]-1) * cols;\n    int origin_offset = row * cols;\n    for (int i = tid; i < copy_iter; i+= blockDim.x) {\n        int load_idx = i;\n        if (load_idx < cols) {\n            B[offset + load_idx] = A[origin_offset + load_idx]; \n        } else {\n            B[offset + load_idx] = 0.0f;\n        }\n    }\n}\n__global__ void copyColumns(float *A, float *B, int *prefixSum, int rows, int cols, int new_cols) {\n    int row = blockIdx.x;\n    if (row >= rows) return;\n    int copy_iter = cols; \n    int tid = threadIdx.x;\n    // Loop over the A and B matrices in blocks of BLOCK_SIZE\n    int offset = row * new_cols;\n    int origin_offset = row * cols;\n    for (int i = tid; i < copy_iter; i+= blockDim.x) {\n        int load_idx = i;\n        if (prefixSum[i] == 0 || prefixSum[i] == prefixSum[i - 1]) continue;\n        int new_position = prefixSum[i] - 1;\n        // Load elements into shared memory\n        if (load_idx < cols) {\n            B[offset + new_position] = A[origin_offset + load_idx]; \n        } \n    }\n}\n\ninline void ggml_cuda_op_mul_mat_transpose_select_gemm(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    GGML_ASSERT(src0_dd_i != nullptr);\n    GGML_ASSERT(src1_ddf_i != nullptr);\n    GGML_ASSERT(dst_dd_i != nullptr);\n\n    const float alpha = 1.0f;\n    const float beta = 0.0f;\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n\n    const int64_t ne10 = src1->ne[0];\n\n    const int64_t ne0 = dst->ne[0];\n    const int64_t row_diff = row_high - row_low;\n\n    float * src0_ddq_as_f32;\n    size_t src0_as = 0;\n\n    if (src0->type != GGML_TYPE_F32) {\n        const to_fp32_cuda_t to_fp32_cuda = ggml_get_to_fp32_cuda(src0->type);\n        src0_ddq_as_f32 = (float *) ggml_cuda_pool_malloc(row_diff*ne00 * sizeof(float), &src0_as); // NOLINT\n        to_fp32_cuda(src0_dd_i, src0_ddq_as_f32, row_diff*ne00, stream);\n    }\n    const float * src0_ddf_i = src0->type == GGML_TYPE_F32 ? (const float *) src0_dd_i : src0_ddq_as_f32;\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n\n    // the main device has a larger memory buffer to hold the results from all GPUs\n    // ldc == nrows of the matrix that cuBLAS writes into\n    int ldc = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n    ldc = ne0;\n    size_t src0_as_t = 0;\n    float *transpose = (float *) ggml_cuda_pool_malloc(row_diff*ne00 * sizeof(float), &src0_as_t); // NOLINT\n    int blockSize = 32;\n    int numBlocks = ne00;\n    transpose_cont<<< numBlocks, blockSize, 0, stream>>>((float *)src0_ddf_i, transpose, ne00, ne01, 1, ne00, ne01,NULL);\n\n    CUBLAS_CHECK(cublasSetStream(g_cublas_handles[id], stream));\n    CUBLAS_CHECK(\n        cublasSgemm(g_cublas_handles[id], CUBLAS_OP_T, CUBLAS_OP_N,\n                ne00, src1_ncols, ne10,\n                &alpha, transpose, ne01,\n                        src1_ddf_i,  ne10,\n                &beta,  dst_dd_i,   ldc));\n \n\n    if (src0_as > 0) {\n        ggml_cuda_pool_free(src0_ddq_as_f32, src0_as);\n        ggml_cuda_pool_free(transpose, src0_as_t);\n    }\n\n    (void) dst;\n    (void) src1_ddq_i;\n    (void) src1_padded_row_size;\n}\n\n__global__ void matrix_row_select_cont(const float * src, float * dst, const int * lst, const int src1_ncols, const int stride_src, const int stride_dst) {\n    const int tid = threadIdx.x + blockIdx.x * blockDim.x;\n    int col_to_read = lst[tid];\n    for (int i = 0; i < src1_ncols; i++) {\n        dst[stride_dst * i + tid] = src[stride_src * i + col_to_read];\n    }\n}\n\ninline void ggml_cuda_op_mul_mat_transpose_gemm(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    GGML_ASSERT(src0_dd_i != nullptr);\n    GGML_ASSERT(src1_ddf_i != nullptr);\n    GGML_ASSERT(dst_dd_i != nullptr);\n\n    const float alpha = 1.0f;\n    const float beta = 0.0f;\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n\n    const int64_t ne10 = src1->ne[0];\n\n    const int64_t ne0 = dst->ne[0];\n    const int64_t row_diff = row_high - row_low;\n\n    float * src0_ddq_as_f32;\n    size_t src0_as = 0;\n\n    if (src0->type != GGML_TYPE_F32) {\n        const to_fp32_cuda_t to_fp32_cuda = ggml_get_to_fp32_cuda(src0->type);\n        src0_ddq_as_f32 = (float *) ggml_cuda_pool_malloc(row_diff*ne00 * sizeof(float), &src0_as); // NOLINT\n        to_fp32_cuda(src0_dd_i, src0_ddq_as_f32, row_diff*ne00, stream);\n    }\n    const float * src0_ddf_i = src0->type == GGML_TYPE_F32 ? (const float *) src0_dd_i : src0_ddq_as_f32;\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n\n    // the main device has a larger memory buffer to hold the results from all GPUs\n    // ldc == nrows of the matrix that cuBLAS writes into\n    int ldc = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n    ldc = ne0;\n\n    CUBLAS_CHECK(cublasSetStream(g_cublas_handles[id], stream));\n\n    // dst->src[3]->data is gpu_bucket, ne01 is length\n    if (dst->src[3] != NULL) {\n        // compress src1\n        GGML_ASSERT(ne01 % 32 == 0);\n        const int block_nums = ne01 / 32;\n        size_t actual_size;\n        float * src1_cont = (float *)ggml_cuda_pool_malloc(ne01 * src1_ncols * sizeof(float), &actual_size);\n        int * row_lookup = static_cast<int *>(ggml_cuda_get_tensor_data(dst->src[3]));\n        matrix_row_select_cont<<<block_nums, 32, 0, stream>>>(src1_ddf_i, src1_cont, row_lookup, src1_ncols, ne10, ne01);\n\n        CUBLAS_CHECK(\n            cublasSgemm(g_cublas_handles[id], CUBLAS_OP_N, CUBLAS_OP_N,\n                    ne00, src1_ncols, ne01,\n                    &alpha, src0_ddf_i, ne00,\n                    src1_cont,  ne01,\n                    &beta,  dst_dd_i,   ldc));\n\n        ggml_cuda_pool_free(src1_cont, actual_size);\n    } else {\n        // full_gpu\n        CUBLAS_CHECK(\n            cublasSgemm(g_cublas_handles[id], CUBLAS_OP_N, CUBLAS_OP_N,\n                    ne00, src1_ncols, ne10,\n                    &alpha, src0_ddf_i, ne00,\n                            src1_ddf_i,  ne10,\n                    &beta,  dst_dd_i,   ldc));\n    }\n\n    if (src0_as > 0) {\n        ggml_cuda_pool_free(src0_ddq_as_f32, src0_as);\n        // ggml_cuda_pool_free(transpose, src0_as_t);\n    }\n\n    (void) dst;\n    (void) src1_ddq_i;\n    (void) src1_padded_row_size;\n}\n\ninline void ggml_cuda_op_dequantize_axpy(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne11 = src1->ne[1]; // input batch size\n    const int64_t ne10 = src1->ne[0]; // input feature size\n    const int64_t row_diff = row_high - row_low;\n\n    // on some GPUs it is faster to convert src1 to half and to use half precision intrinsics\n#ifdef GGML_CUDA_F16\n    size_t ash;\n    dfloat * src1_dfloat = nullptr; // dfloat == half\n\n    bool src1_convert_f16 = src0->type == GGML_TYPE_Q4_0 || src0->type == GGML_TYPE_Q4_1 ||\n        src0->type == GGML_TYPE_Q5_0 || src0->type == GGML_TYPE_Q5_1 ||\n        src0->type == GGML_TYPE_Q8_0 || src0->type == GGML_TYPE_F16;\n\n    if (src1_convert_f16) {\n        src1_dfloat = (half *) ggml_cuda_pool_malloc(ne00*sizeof(half), &ash);\n        ggml_cpy_f32_f16_cuda((const char *) src1_ddf_i, (char *) src1_dfloat, ne00,\n                                ne00, 1, sizeof(float), 0, 0,\n                                ne00, 1, sizeof(half),  0, 0, stream);\n    }\n#else\n    const dfloat * src1_dfloat = (const dfloat *) src1_ddf_i; // dfloat == float, no conversion\n#endif // GGML_CUDA_F16\n    float * sparse_idx = static_cast<float *>(ggml_cuda_get_tensor_data(dst->src[2]));\n    int32_t * row_lookup = dst->src[3] != NULL ? static_cast<int32_t *>(ggml_cuda_get_tensor_data(dst->src[3])) : NULL;\n    cudaMemsetAsync((void *)dst_dd_i, 0, ggml_nbytes(dst), stream);\n\n    switch (src0->type) {\n        case GGML_TYPE_Q4_0:\n            if (sparse_idx == NULL) {\n                dequantize_axpy_vec_q4_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);\n            } else if (ne11 == 1) {\n                dequantize_axpy_sparse_vec_q4_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream, row_lookup, sparse_idx);\n            } else {\n                dequantize_axpy_sparse_batch_q4_0_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, ne10, src1_ncols, stream, row_lookup, sparse_idx);\n            }\n            break;\n        case GGML_TYPE_F16:\n            if (sparse_idx == NULL) {\n                convert_axpy_vec_f16_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream);\n            } else if (ne11 == 1) {\n                convert_axpy_sparse_vec_f16_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, stream, row_lookup, sparse_idx);\n            } else {\n                convert_axpy_sparse_batch_f16_cuda(src0_dd_i, src1_dfloat, dst_dd_i, ne00, row_diff, ne10, src1_ncols, stream, row_lookup, sparse_idx);\n            }\n            break;\n        default:\n            GGML_ASSERT(false && \"Unsupported type\");\n            break;\n    }\n\n#ifdef GGML_CUDA_F16\n    if (src1_convert_f16) {\n        ggml_cuda_pool_free(src1_dfloat, ash);\n    }\n#endif // GGML_CUDA_F16\n\n    (void) src1;\n    (void) dst;\n    (void) src1_ddq_i;\n    (void) src1_ncols;\n    (void) src1_padded_row_size;\n}\n\ninline void ggml_cuda_op_mul_mat_cublas(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const char * src0_dd_i, const float * src1_ddf_i,\n    const char * src1_ddq_i, float * dst_dd_i, const int64_t row_low, const int64_t row_high, const int64_t src1_ncols,\n    const int64_t src1_padded_row_size, const cudaStream_t & stream) {\n\n    GGML_ASSERT(src0_dd_i  != nullptr);\n    GGML_ASSERT(src1_ddf_i != nullptr);\n    GGML_ASSERT(dst_dd_i   != nullptr);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne10 = src1->ne[0];\n\n    const int64_t ne0 = dst->ne[0];\n\n    const int64_t row_diff = row_high - row_low;\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n\n    // the main device has a larger memory buffer to hold the results from all GPUs\n    // ldc == nrows of the matrix that cuBLAS writes into\n    int ldc = dst->backend == GGML_BACKEND_GPU && id == g_main_device ? ne0 : row_diff;\n\n    const int compute_capability = g_compute_capabilities[id];\n\n    if (compute_capability >= CC_VOLTA && (src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type)) && ggml_is_contiguous(src0) && row_diff == src0->ne[1]) {\n        // convert src0 and src1 to fp16, multiply as fp16, convert dst to fp32\n        half * src0_as_f16 = nullptr;\n        size_t src0_as = 0;\n        if (src0->type != GGML_TYPE_F16) {\n            const to_fp16_cuda_t to_fp16_cuda = ggml_get_to_fp16_cuda(src0->type);\n            GGML_ASSERT(to_fp16_cuda != nullptr);\n            size_t ne = row_diff*ne00;\n            src0_as_f16 = (half *) ggml_cuda_pool_malloc(ne * sizeof(half), &src0_as);\n            to_fp16_cuda(src0_dd_i, src0_as_f16, ne, stream);\n        }\n        const half * src0_ptr = src0->type == GGML_TYPE_F16 ? (const half *) src0_dd_i : src0_as_f16;\n\n        half * src1_as_f16 = nullptr;\n        size_t src1_as = 0;\n        if (src1->type != GGML_TYPE_F16) {\n            const to_fp16_cuda_t to_fp16_cuda = ggml_get_to_fp16_cuda(src1->type);\n            GGML_ASSERT(to_fp16_cuda != nullptr);\n            size_t ne = src1_ncols*ne10;\n            src1_as_f16 = (half *) ggml_cuda_pool_malloc(ne * sizeof(half), &src1_as);\n            to_fp16_cuda(src1_ddf_i, src1_as_f16, ne, stream);\n        }\n        const half * src1_ptr = src1->type == GGML_TYPE_F16 ? (const half *) src1_ddf_i : src1_as_f16;\n        size_t dst_as = 0;\n        half * dst_f16 = (half *) ggml_cuda_pool_malloc(row_diff*src1_ncols * sizeof(half), &dst_as);\n\n        const half alpha_f16 = 1.0f;\n        const half beta_f16 = 0.0f;\n\n        CUBLAS_CHECK(cublasSetStream(g_cublas_handles[id], stream));\n        CUBLAS_CHECK(\n            cublasGemmEx(g_cublas_handles[id], CUBLAS_OP_T, CUBLAS_OP_N,\n                    row_diff, src1_ncols, ne10,\n                    &alpha_f16, src0_ptr, CUDA_R_16F, ne00,\n                                src1_ptr, CUDA_R_16F, ne10,\n                    &beta_f16,   dst_f16, CUDA_R_16F, ldc,\n                    CUBLAS_COMPUTE_16F,\n                    CUBLAS_GEMM_DEFAULT_TENSOR_OP));\n\n        const to_fp32_cuda_t to_fp32_cuda = ggml_get_to_fp32_cuda(GGML_TYPE_F16);\n        to_fp32_cuda(dst_f16, dst_dd_i, row_diff*src1_ncols, stream);\n\n        ggml_cuda_pool_free(dst_f16, dst_as);\n\n        if (src0_as != 0) {\n            ggml_cuda_pool_free(src0_as_f16, src0_as);\n        }\n\n        if (src1_as != 0) {\n            ggml_cuda_pool_free(src1_as_f16, src1_as);\n        }\n    }\n    else {\n        float * src0_ddq_as_f32 = nullptr;\n        size_t src0_as = 0;\n\n        if (src0->type != GGML_TYPE_F32) {\n            const to_fp32_cuda_t to_fp32_cuda = ggml_get_to_fp32_cuda(src0->type);\n            GGML_ASSERT(to_fp32_cuda != nullptr);\n            src0_ddq_as_f32 = (float *) ggml_cuda_pool_malloc(row_diff*ne00 * sizeof(float), &src0_as); // NOLINT\n            to_fp32_cuda(src0_dd_i, src0_ddq_as_f32, row_diff*ne00, stream);\n        }\n        const float * src0_ddf_i = src0->type == GGML_TYPE_F32 ? (const float *) src0_dd_i : src0_ddq_as_f32;\n\n        const float alpha = 1.0f;\n        const float beta = 0.0f;\n\n        CUBLAS_CHECK(cublasSetStream(g_cublas_handles[id], stream));\n        CUBLAS_CHECK(\n            cublasSgemm(g_cublas_handles[id], CUBLAS_OP_T, CUBLAS_OP_N,\n                    row_diff, src1_ncols, ne10,\n                    &alpha, src0_ddf_i, ne00,\n                            src1_ddf_i, ne10,\n                    &beta,  dst_dd_i,   ldc));\n\n        if (src0_as != 0) {\n            ggml_cuda_pool_free(src0_ddq_as_f32, src0_as);\n        }\n    }\n\n    (void) dst;\n    (void) src1_ddq_i;\n    (void) src1_padded_row_size;\n}\n\ninline void ggml_cuda_op_rope(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32 || src0->type == GGML_TYPE_F16);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32 ||  dst->type == GGML_TYPE_F16);\n    GGML_ASSERT(src0->type == dst->type);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne2 = dst->ne[2];\n    const int64_t nrows = ggml_nrows(src0);\n\n    //const int n_past      = ((int32_t *) dst->op_params)[0];\n    const int n_dims      = ((int32_t *) dst->op_params)[1];\n    const int mode        = ((int32_t *) dst->op_params)[2];\n    const int n_ctx       = ((int32_t *) dst->op_params)[3];\n    const int n_orig_ctx  = ((int32_t *) dst->op_params)[4];\n\n    // RoPE alteration for extended context\n    float freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow;\n    memcpy(&freq_base,   (int32_t *) dst->op_params +  5, sizeof(float));\n    memcpy(&freq_scale,  (int32_t *) dst->op_params +  6, sizeof(float));\n    memcpy(&ext_factor,  (int32_t *) dst->op_params +  7, sizeof(float));\n    memcpy(&attn_factor, (int32_t *) dst->op_params +  8, sizeof(float));\n    memcpy(&beta_fast,   (int32_t *) dst->op_params +  9, sizeof(float));\n    memcpy(&beta_slow,   (int32_t *) dst->op_params + 10, sizeof(float));\n\n    const int32_t * pos = nullptr;\n    if ((mode & 1) == 0) {\n        GGML_ASSERT(src1->type == GGML_TYPE_I32);\n        GGML_ASSERT(src1->ne[0] == ne2);\n        pos = (const int32_t *) src1_dd;\n    }\n\n    const bool is_neox = mode & 2;\n    const bool is_glm  = mode & 4;\n\n    rope_corr_dims corr_dims;\n    ggml_rope_yarn_corr_dims(n_dims, n_orig_ctx, freq_base, beta_fast, beta_slow, corr_dims.v);\n\n    // compute\n    if (is_glm) {\n        GGML_ASSERT(false);\n        rope_glm_f32_cuda(src0_dd, dst_dd, ne00, nrows, pos, freq_scale, ne01, freq_base, n_ctx, main_stream);\n    } else if (is_neox) {\n        GGML_ASSERT(ne00 == n_dims && \"ne00 != n_dims is not implemented for CUDA yet\");\n        if (src0->type == GGML_TYPE_F32) {\n            rope_neox_cuda(\n                (const float *)src0_dd, (float *)dst_dd, ne00, nrows, pos, freq_scale, ne01, freq_base, ext_factor,\n                attn_factor, corr_dims, main_stream\n            );\n        } else if (src0->type == GGML_TYPE_F16) {\n            rope_neox_cuda(\n                (const half *)src0_dd, (half *)dst_dd, ne00, nrows, pos, freq_scale, ne01, freq_base, ext_factor,\n                attn_factor, corr_dims, main_stream\n            );\n        } else {\n            GGML_ASSERT(false);\n        }\n    } else {\n        if (src0->type == GGML_TYPE_F32) {\n            rope_cuda(\n                (const float *)src0_dd, (float *)dst_dd, ne00, nrows, pos, freq_scale, ne01, freq_base, ext_factor,\n                attn_factor, corr_dims, main_stream\n            );\n        } else if (src0->type == GGML_TYPE_F16) {\n            rope_cuda(\n                (const half *)src0_dd, (half *)dst_dd, ne00, nrows, pos, freq_scale, ne01, freq_base, ext_factor,\n                attn_factor, corr_dims, main_stream\n            );\n        } else {\n            GGML_ASSERT(false);\n        }\n    }\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_alibi(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n    const int64_t nrows = ggml_nrows(src0);\n\n    //const int n_past = ((int32_t *) dst->op_params)[0];\n    const int n_head = ((int32_t *) dst->op_params)[1];\n    float max_bias;\n    memcpy(&max_bias, (int32_t *) dst->op_params + 2, sizeof(float));\n\n    //GGML_ASSERT(ne01 + n_past == ne00);\n    GGML_ASSERT(n_head == ne02);\n\n    const int n_heads_log2_floor = 1 << (int) floor(log2(n_head));\n\n    const float m0 = powf(2.0f, -(max_bias) / n_heads_log2_floor);\n    const float m1 = powf(2.0f, -(max_bias / 2.0f) / n_heads_log2_floor);\n\n    alibi_f32_cuda(src0_dd, dst_dd, ne00, nrows, ne01, n_heads_log2_floor, m0, m1, main_stream);\n\n    (void) src1;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_im2col(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F16);\n\n    const int32_t s0 = ((const int32_t*)(dst->op_params))[0];\n    const int32_t s1 = ((const int32_t*)(dst->op_params))[1];\n    const int32_t p0 = ((const int32_t*)(dst->op_params))[2];\n    const int32_t p1 = ((const int32_t*)(dst->op_params))[3];\n    const int32_t d0 = ((const int32_t*)(dst->op_params))[4];\n    const int32_t d1 = ((const int32_t*)(dst->op_params))[5];\n\n    const bool is_2D = ((const int32_t*)(dst->op_params))[6] == 1;\n\n    const int64_t N  = src1->ne[is_2D ? 3 : 2];\n    const int64_t IC = src1->ne[is_2D ? 2 : 1];\n    const int64_t IH = is_2D ? src1->ne[1] : 1;\n    const int64_t IW =         src1->ne[0];\n\n    const int64_t KH = is_2D ? src0->ne[1] : 1;\n    const int64_t KW =         src0->ne[0];\n\n    const int64_t OH = is_2D ? dst->ne[2] : 1;\n    const int64_t OW =         dst->ne[1];\n\n    const size_t ofs0 = src1->nb[is_2D ? 3 : 2] / 4; // nb is byte offset, src is type float32\n    const size_t ofs1 = src1->nb[is_2D ? 2 : 1] / 4; // nb is byte offset, src is type float32\n\n    im2col_f32_f16_cuda(src1_dd, (half*) dst_dd,\n        OH, IW, IH, OW, IC, KH, KW, N,\n        ofs0, ofs1, s0, s1, p0, p1, d0, d1, main_stream);\n\n    (void) src0;\n    (void) src0_dd;\n}\n\ninline void ggml_cuda_op_diag_mask_inf(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int nrows0 = ggml_nrows(src0);\n\n    const int n_past = ((int32_t *) dst->op_params)[0];\n\n    diag_mask_inf_f32_cuda(src0_dd, dst_dd, ne00, nrows0, ne01, n_past, main_stream);\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_soft_max(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t nrows = ggml_nrows(src0);\n\n    soft_max_f32_cuda(src0_dd, dst_dd, ne00, nrows, main_stream);\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_scale(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    float scale;\n    // HACK: support for ggml backend interface\n    if (src1->backend == GGML_BACKEND_CPU) {\n        scale = ((float *) src1->data)[0];\n    } else {\n        // TODO: pass pointer to kernel instead of copying to host\n        CUDA_CHECK(cudaMemcpy(&scale, src1->data, sizeof(float), cudaMemcpyDeviceToHost));\n    }\n\n    scale_f32_cuda(src0_dd, dst_dd, scale, ggml_nelements(src0), main_stream);\n    CUDA_CHECK(cudaGetLastError());\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\ninline void ggml_cuda_op_clamp(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst,\n    const float * src0_dd, const float * src1_dd, float * dst_dd, const cudaStream_t & main_stream) {\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    float min;\n    float max;\n    memcpy(&min, dst->op_params, sizeof(float));\n    memcpy(&max, (float *) dst->op_params + 1, sizeof(float));\n\n    clamp_f32_cuda(src0_dd, dst_dd, min, max, ggml_nelements(src0), main_stream);\n    CUDA_CHECK(cudaGetLastError());\n\n    (void) src1;\n    (void) dst;\n    (void) src1_dd;\n}\n\nstatic void ggml_cuda_op_flatten(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, const ggml_cuda_op_flatten_t op) {\n    const int64_t nrows0 = ggml_nrows(src0);\n\n    const bool use_src1 = src1 != nullptr;\n    const int64_t nrows1 = use_src1 ? ggml_nrows(src1) : 1;\n\n    GGML_ASSERT(!use_src1 || src1->backend != GGML_BACKEND_GPU_SPLIT);\n    GGML_ASSERT(              dst->backend != GGML_BACKEND_GPU_SPLIT);\n\n    ggml_tensor_extra_gpu * src0_extra =            (ggml_tensor_extra_gpu *) src0->extra;\n    ggml_tensor_extra_gpu * src1_extra = use_src1 ? (ggml_tensor_extra_gpu *) src1->extra : nullptr;\n    ggml_tensor_extra_gpu * dst_extra  =            (ggml_tensor_extra_gpu *)  dst->extra;\n\n    const bool src0_on_device =             src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT;\n    const bool src1_on_device = use_src1 && src1->backend == GGML_BACKEND_GPU;\n    const bool  dst_on_device =              dst->backend == GGML_BACKEND_GPU;\n\n    const bool src1_stays_on_host = use_src1 && dst->op == GGML_OP_SCALE;\n\n    // dd = data device\n    float * src0_ddf = nullptr;\n    float * src1_ddf = nullptr;\n    float *  dst_ddf = nullptr;\n\n    // as = actual size\n    size_t src0_asf = 0;\n    size_t src1_asf = 0;\n    size_t  dst_asf = 0;\n\n    ggml_cuda_set_device(g_main_device);\n    const cudaStream_t main_stream = g_cudaStreams[g_main_device][0];\n\n    if (src0_on_device) {\n        src0_ddf = (float *) src0_extra->data_device[g_main_device];\n    } else {\n        src0_ddf = (float *) ggml_cuda_pool_malloc(ggml_nbytes(src0), &src0_asf);\n        CUDA_CHECK(ggml_cuda_cpy_tensor_2d(src0_ddf, src0, 0, 0, 0, nrows0, main_stream));\n    }\n\n    if (use_src1 && !src1_stays_on_host) {\n        if (src1_on_device) {\n            src1_ddf = (float *) src1_extra->data_device[g_main_device];\n        } else {\n            src1_ddf = (float *) ggml_cuda_pool_malloc(ggml_nbytes(src1), &src1_asf);\n            CUDA_CHECK(ggml_cuda_cpy_tensor_2d(src1_ddf, src1, 0, 0, 0, nrows1, main_stream));\n        }\n    }\n    if (dst_on_device) {\n        dst_ddf = (float *) dst_extra->data_device[g_main_device];\n    } else {\n        dst_ddf = (float *) ggml_cuda_pool_malloc(ggml_nbytes(dst), &dst_asf);\n    }\n\n    // do the computation\n    op(src0, src1, dst, src0_ddf, src1_ddf, dst_ddf, main_stream);\n    CUDA_CHECK(cudaGetLastError());\n\n    // copy dst to host if necessary\n    if (!dst_on_device) {\n        CUDA_CHECK(cudaMemcpyAsync(dst->data, dst_ddf, ggml_nbytes(dst), cudaMemcpyDeviceToHost, main_stream));\n    }\n\n    if (src0_asf > 0) {\n        ggml_cuda_pool_free(src0_ddf, src0_asf);\n    }\n    if (src1_asf > 0) {\n        ggml_cuda_pool_free(src1_ddf, src1_asf);\n    }\n    if (dst_asf > 0) {\n        ggml_cuda_pool_free(dst_ddf, dst_asf);\n    }\n\n    if (dst->backend == GGML_BACKEND_CPU) {\n        CUDA_CHECK(cudaDeviceSynchronize());\n    }\n}\n\nstatic void ggml_cuda_set_peer_access(const int n_tokens) {\n    static bool peer_access_enabled = false;\n\n    const bool enable_peer_access = n_tokens <= GGML_CUDA_PEER_MAX_BATCH_SIZE;\n\n    if (peer_access_enabled == enable_peer_access) {\n        return;\n    }\n\n#ifdef NDEBUG\n    for (int id = 0; id < g_device_count; ++id) {\n        CUDA_CHECK(ggml_cuda_set_device(id));\n\n        for (int id_other = 0; id_other < g_device_count; ++id_other) {\n            if (id == id_other) {\n                continue;\n            }\n            if (id != g_main_device && id_other != g_main_device) {\n                continue;\n            }\n\n            int can_access_peer;\n            CUDA_CHECK(cudaDeviceCanAccessPeer(&can_access_peer, id, id_other));\n            if (can_access_peer) {\n                if (enable_peer_access) {\n                    CUDA_CHECK(cudaDeviceEnablePeerAccess(id_other, 0));\n                } else {\n                    CUDA_CHECK(cudaDeviceDisablePeerAccess(id_other));\n                }\n            }\n        }\n    }\n#endif // NDEBUG\n\n    peer_access_enabled = enable_peer_access;\n}\n\nstatic void ggml_cuda_op_mul_mat(\n    const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, ggml_cuda_op_mul_mat_t op,\n    const bool convert_src1_to_q8_1) {\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n    const int64_t ne03 = src0->ne[3];\n    const int64_t nrows0 = ggml_nrows(src0);\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n    const int64_t ne12 = src1->ne[2];\n    const int64_t ne13 = src1->ne[3];\n    const int64_t nrows1 = ggml_nrows(src1);\n\n    GGML_ASSERT(ne03 == ne13);\n\n    const int64_t ne0 = dst->ne[0];\n    const int64_t ne1 = dst->ne[1];\n\n    const int nb2 = dst->nb[2];\n    const int nb3 = dst->nb[3];\n\n    ggml_cuda_set_peer_access(ne11);\n\n    GGML_ASSERT(dst->backend != GGML_BACKEND_GPU_SPLIT);\n    GGML_ASSERT(src1->backend != GGML_BACKEND_GPU_SPLIT);\n\n    GGML_ASSERT(ne12 >= ne02 && ne12 % ne02 == 0);\n\n    const int64_t i02_divisor = ne12 / ne02;\n\n    const size_t src0_ts = ggml_type_size(src0->type);\n    const size_t src0_bs = ggml_blck_size(src0->type);\n    const size_t q8_1_ts = sizeof(block_q8_1);\n    const size_t q8_1_bs = QK8_1;\n\n    ggml_tensor_extra_gpu * src0_extra = (ggml_tensor_extra_gpu *) src0->extra;\n    ggml_tensor_extra_gpu * src1_extra = (ggml_tensor_extra_gpu *) src1->extra;\n    ggml_tensor_extra_gpu *  dst_extra = (ggml_tensor_extra_gpu *)  dst->extra;\n\n    const bool src0_on_device = src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT;\n    const bool src0_is_contiguous = ggml_is_contiguous(src0);\n\n    const bool src1_is_contiguous = ggml_is_contiguous(src1);\n    const int64_t src1_padded_col_size = ne10 % MATRIX_ROW_PADDING == 0 ?\n        ne10 : ne10 - ne10 % MATRIX_ROW_PADDING + MATRIX_ROW_PADDING;\n\n    const bool split = src0->backend == GGML_BACKEND_GPU_SPLIT;\n    GGML_ASSERT(!(split && ne02 > 1));\n    GGML_ASSERT(!(split && ne03 > 1));\n    GGML_ASSERT(!(split && ne02 < ne12));\n\n    // dd = data device\n    char  *  src0_dd[GGML_CUDA_MAX_DEVICES] = {nullptr};\n    float * src1_ddf[GGML_CUDA_MAX_DEVICES] = {nullptr}; // float\n    char  * src1_ddq[GGML_CUDA_MAX_DEVICES] = {nullptr}; // q8_1\n    float *   dst_dd[GGML_CUDA_MAX_DEVICES] = {nullptr};\n\n    // as = actual size\n    size_t  src0_as[GGML_CUDA_MAX_DEVICES] = {0};\n    size_t src1_asf[GGML_CUDA_MAX_DEVICES] = {0};\n    size_t src1_asq[GGML_CUDA_MAX_DEVICES] = {0};\n    size_t   dst_as[GGML_CUDA_MAX_DEVICES] = {0};\n\n    int64_t  row_low[GGML_CUDA_MAX_DEVICES];\n    int64_t row_high[GGML_CUDA_MAX_DEVICES];\n\n    int used_devices = 0;\n\n    for (int64_t id = 0; id < g_device_count; ++id) {\n        // by default, use all rows\n        row_low[id]  = 0;\n        row_high[id] = ne01;\n\n        // for multi GPU, get the row boundaries from tensor split\n        // and round to mul_mat_q tile sizes\n        if (split) {\n            const int64_t rounding = get_row_rounding(src0->type);\n\n            if (id != 0) {\n                row_low[id]  = ne01*g_tensor_split[id];\n                row_low[id] -= row_low[id] % rounding;\n            }\n\n            if (id != g_device_count - 1) {\n                row_high[id]  = ne01*g_tensor_split[id + 1];\n                row_high[id] -= row_high[id] % rounding;\n            }\n        }\n    }\n\n    for (int64_t id = 0; id < g_device_count; ++id) {\n        if ((!split && id != g_main_device) || row_low[id] == row_high[id]) {\n            continue;\n        }\n\n        used_devices++;\n\n        const bool src1_on_device = src1->backend == GGML_BACKEND_GPU && id == g_main_device;\n        const bool  dst_on_device =  dst->backend == GGML_BACKEND_GPU && id == g_main_device;\n\n        ggml_cuda_set_device(id);\n        const cudaStream_t stream = g_cudaStreams[id][0];\n\n        if (src0_on_device && src0_is_contiguous) {\n            src0_dd[id] = (char *) src0_extra->data_device[id];\n        } else {\n            const size_t size_src0_ddq = split ? (row_high[id]-row_low[id])*ne00 * src0_ts/src0_bs : ggml_nbytes(src0);\n            src0_dd[id] = (char *) ggml_cuda_pool_malloc(ggml_nbytes(src0), &src0_as[id]);\n        }\n\n        if (src1_on_device && src1_is_contiguous) {\n            src1_ddf[id] = (float *) src1_extra->data_device[id];\n        } else {\n            src1_ddf[id] = (float *) ggml_cuda_pool_malloc(ggml_nbytes(src1), &src1_asf[id]);\n        }\n\n        if (convert_src1_to_q8_1) {\n            src1_ddq[id] = (char *) ggml_cuda_pool_malloc(nrows1*src1_padded_col_size*q8_1_ts/q8_1_bs, &src1_asq[id]);\n\n            if (src1_on_device && src1_is_contiguous) {\n                quantize_row_q8_1_cuda(src1_ddf[id], src1_ddq[id], ne10, nrows1, src1_padded_col_size, stream);\n                CUDA_CHECK(cudaGetLastError());\n            }\n        }\n\n        if (dst_on_device) {\n            dst_dd[id] = (float *) dst_extra->data_device[id];\n        } else {\n            const size_t size_dst_ddf = split ? (row_high[id]-row_low[id])*ne1*sizeof(float) : ggml_nbytes(dst);\n            dst_dd[id] = (float *) ggml_cuda_pool_malloc(size_dst_ddf, &dst_as[id]);\n        }\n    }\n\n    // if multiple devices are used they need to wait for the main device\n    // here an event is recorded that signals that the main device has finished calculating the input data\n    if (split && used_devices > 1) {\n        CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n        CUDA_CHECK(cudaEventRecord(src0_extra->events[g_main_device][0], g_cudaStreams[g_main_device][0]));\n    }\n\n    const int64_t src1_col_stride = split && used_devices > 1 ? MUL_MAT_SRC1_COL_STRIDE : ne11;\n    for (int64_t src1_col_0 = 0; src1_col_0 < ne11; src1_col_0 += src1_col_stride) {\n        const int64_t is = split ? (src1_col_0/src1_col_stride) % MAX_STREAMS : 0;\n        const int64_t src1_ncols = src1_col_0 + src1_col_stride > ne11 ? ne11 - src1_col_0 : src1_col_stride;\n\n        for (int64_t id = 0; id < g_device_count; ++id) {\n            if ((!split && id != g_main_device) || row_low[id] == row_high[id]) {\n                continue;\n            }\n\n            const bool src1_on_device = src1->backend == GGML_BACKEND_GPU && id == g_main_device;\n            const bool  dst_on_device =  dst->backend == GGML_BACKEND_GPU && id == g_main_device;\n            const int64_t row_diff = row_high[id] - row_low[id];\n\n            ggml_cuda_set_device(id);\n            const cudaStream_t stream = g_cudaStreams[id][is];\n\n            // wait for main GPU data if necessary\n            if (split && (id != g_main_device || is != 0)) {\n                CUDA_CHECK(cudaStreamWaitEvent(stream, src0_extra->events[g_main_device][0], 0));\n            }\n\n            for (int64_t i0 = 0; i0 < ne13*ne12; ++i0) {\n                const int64_t i03 = i0 / ne12;\n                const int64_t i02 = i0 % ne12;\n\n                const size_t src1_ddq_i_offset = (i0*ne11 + src1_col_0) * src1_padded_col_size*q8_1_ts/q8_1_bs;\n\n                // for split tensors the data begins at i0 == i0_offset_low\n                char  *  src0_dd_i =  src0_dd[id] + (i0/i02_divisor) * ne01*ne00*src0_ts/src0_bs;\n                float * src1_ddf_i = src1_ddf[id] + (i0*ne11 + src1_col_0) * ne10;\n                char  * src1_ddq_i = src1_ddq[id] +  src1_ddq_i_offset;\n                float *   dst_dd_i =   dst_dd[id] + (i0*ne1  + src1_col_0) * (dst_on_device ? ne0 : row_diff);\n\n                // the main device memory buffer can be on VRAM scratch, with space for all partial results\n                // in that case an offset on dst_ddf_i is needed\n                if (dst->backend == GGML_BACKEND_GPU && id == g_main_device) {\n                    dst_dd_i += row_low[id]; // offset is 0 if no tensor split\n                }\n\n                // copy src0, src1 to device if necessary\n                if (src1->backend == GGML_BACKEND_GPU && src1_is_contiguous) {\n                    if (id != g_main_device) {\n                        if (convert_src1_to_q8_1) {\n                            char * src1_ddq_i_source = src1_ddq[g_main_device] + src1_ddq_i_offset;\n                            CUDA_CHECK(cudaMemcpyAsync(src1_ddq_i, src1_ddq_i_source, src1_ncols*src1_padded_col_size*q8_1_ts/q8_1_bs,\n                                                    cudaMemcpyDeviceToDevice, stream));\n                        } else {\n                            float * src1_ddf_i_source = (float *) src1_extra->data_device[g_main_device];\n                            src1_ddf_i_source += (i0*ne11 + src1_col_0) * ne10;\n                            CUDA_CHECK(cudaMemcpyAsync(src1_ddf_i, src1_ddf_i_source, src1_ncols*ne10*sizeof(float),\n                                                    cudaMemcpyDeviceToDevice, stream));\n                        }\n                    }\n                } else if (src1->backend == GGML_BACKEND_CPU || (src1_on_device && !src1_is_contiguous)) {\n                    CUDA_CHECK(ggml_cuda_cpy_tensor_2d(\n                                   src1_ddf_i, src1, i03, i02, src1_col_0, src1_col_0+src1_ncols, stream));\n                } else {\n                    GGML_ASSERT(false);\n                }\n\n                if (convert_src1_to_q8_1 && (src1->backend == GGML_BACKEND_CPU || !src1_is_contiguous)) {\n                    quantize_row_q8_1_cuda(src1_ddf_i, src1_ddq_i, ne10, src1_ncols, src1_padded_col_size, stream);\n                    CUDA_CHECK(cudaGetLastError());\n                }\n\n                if (src1_col_0 == 0 && (!src0_on_device || !src0_is_contiguous) && i02 % i02_divisor == 0) {\n                    CUDA_CHECK(ggml_cuda_cpy_tensor_2d(src0_dd_i, src0, i03, i02/i02_divisor, row_low[id], row_high[id], stream));\n                }\n\n                // do the computation\n                op(src0, src1, dst, src0_dd_i, src1_ddf_i, src1_ddq_i, dst_dd_i,\n                   row_low[id], row_high[id], src1_ncols, src1_padded_col_size, stream);\n                CUDA_CHECK(cudaGetLastError());\n\n                // copy dst to host or other device if necessary\n                if (!dst_on_device) {\n                    void * dst_off_device;\n                    cudaMemcpyKind kind;\n                    if (dst->backend == GGML_BACKEND_CPU) {\n                        dst_off_device = dst->data;\n                        kind = cudaMemcpyDeviceToHost;\n                    } else if (dst->backend == GGML_BACKEND_GPU) {\n                        dst_off_device = dst_extra->data_device[g_main_device];\n                        kind = cudaMemcpyDeviceToDevice;\n                    } else {\n                        GGML_ASSERT(false);\n                    }\n                    if (split) {\n                        // src0 = weight matrix is saved as a transposed matrix for better memory layout.\n                        // dst is NOT transposed.\n                        // The outputs of matrix matrix multiplications can therefore NOT simply be concatenated for >1 GPU.\n                        // Instead they need to be copied to the correct slice in ne0 = dst row index.\n                        // If dst is a vector with ne0 == 1 then you don't have to do this but it still produces correct results.\n                        float * dhf_dst_i = (float *) ((char *) dst_off_device + i02*nb2 + i03*nb3);\n                        GGML_ASSERT(dst->nb[1] == ne0*sizeof(float));\n                        dhf_dst_i += src1_col_0*ne0 + row_low[id];\n                        CUDA_CHECK(cudaMemcpy2DAsync(dhf_dst_i, ne0*sizeof(float), dst_dd_i, row_diff*sizeof(float),\n                                                    row_diff*sizeof(float), src1_ncols, kind, stream));\n                    } else {\n                        float * dhf_dst_i = (float *) ((char *) dst_off_device + i02*nb2 + i03*nb3);\n                        GGML_ASSERT(dst->nb[1] == ne0*sizeof(float));\n                        dhf_dst_i += src1_col_0*ne0;\n                        CUDA_CHECK(cudaMemcpyAsync(dhf_dst_i, dst_dd_i, src1_ncols*ne0*sizeof(float), kind, stream));\n                    }\n                }\n\n                // add event for the main device to wait on until other device is done\n                if (split && (id != g_main_device || is != 0)) {\n                    CUDA_CHECK(cudaEventRecord(src0_extra->events[id][is], stream));\n                }\n            }\n        }\n    }\n\n    for (int64_t id = 0; id < g_device_count; ++id) {\n        if ((!split && id != g_main_device) || row_low[id] == row_high[id]) {\n            continue;\n        }\n        CUDA_CHECK(ggml_cuda_set_device(id));\n\n        // free buffers again when done\n        if (src0_as[id] > 0) {\n            ggml_cuda_pool_free(src0_dd[id], src0_as[id]);\n        }\n        if (src1_asf[id] > 0) {\n            ggml_cuda_pool_free(src1_ddf[id], src1_asf[id]);\n        }\n        if (src1_asq[id] > 0) {\n            ggml_cuda_pool_free(src1_ddq[id], src1_asq[id]);\n        }\n        if (dst_as[id] > 0) {\n            ggml_cuda_pool_free(dst_dd[id], dst_as[id]);\n        }\n    }\n\n    // main device waits for all other devices to be finished\n    if (split && g_device_count > 1) {\n        int64_t is_max = (ne11 + MUL_MAT_SRC1_COL_STRIDE - 1) / MUL_MAT_SRC1_COL_STRIDE;\n        is_max = is_max <= MAX_STREAMS ? is_max : MAX_STREAMS;\n\n        CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n        for (int64_t id = 0; id < g_device_count; ++id) {\n            if (row_low[id] == row_high[id]) {\n                continue;\n            }\n            for (int64_t is = 0; is < is_max; ++is) {\n                CUDA_CHECK(cudaStreamWaitEvent(g_cudaStreams[g_main_device][0], src0_extra->events[id][is], 0));\n            }\n        }\n    }\n\n    if (dst->backend == GGML_BACKEND_CPU) {\n        CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n        CUDA_CHECK(cudaDeviceSynchronize());\n    }\n}\n\nstatic void ggml_cuda_repeat(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_repeat);\n}\n\nstatic void ggml_cuda_get_rows(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_get_rows);\n}\n\nstatic void ggml_cuda_add(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_add);\n}\n\nstatic void ggml_cuda_mul(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_mul);\n}\n\nstatic void ggml_cuda_gelu(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_gelu);\n}\n\nstatic void ggml_cuda_silu(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_silu);\n}\n\nstatic void ggml_cuda_relu(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_relu);\n}\n\nstatic void ggml_cuda_sqr(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_sqr);\n}\n\nstatic void ggml_cuda_norm(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_norm);\n}\n\nstatic void ggml_cuda_rms_norm(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_rms_norm);\n}\n\nbool ggml_cuda_can_mul_mat(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst) {\n    if (!g_cublas_loaded) return false;\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne01 = src0->ne[1];\n\n    const int64_t ne0 = dst->ne[0];\n    const int64_t ne1 = dst->ne[1];\n\n    // TODO: find the optimal values for these\n    return (src0->type == GGML_TYPE_F32 || src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type)) &&\n            src1->type == GGML_TYPE_F32 &&\n             dst->type == GGML_TYPE_F32 &&\n            (ne0 >= 32 && ne1 >= 32 && ne10 >= 32);\n}\n\nstatic void ggml_cuda_mul_mat_vec_p021(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst){\n    GGML_ASSERT(ggml_is_permuted(src0) && ggml_is_permuted(src1));\n    GGML_ASSERT(src0->backend != GGML_BACKEND_GPU_SPLIT);\n    GGML_ASSERT(src0->nb[0] <= src0->nb[1] && src0->nb[2] <= src0->nb[3]); // 0213 permutation\n    GGML_ASSERT(src1->nb[0] <= src1->nb[1] && src1->nb[2] <= src1->nb[3]); // 0213 permutation\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n\n    const int64_t ne12 = src1->ne[2];\n\n    CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n    cudaStream_t main_stream = g_cudaStreams[g_main_device][0];\n\n    ggml_tensor_extra_gpu * src0_extra = (ggml_tensor_extra_gpu *) src0->extra;\n    void * src0_ddq = src0_extra->data_device[g_main_device];\n\n    ggml_tensor_extra_gpu * src1_extra = (ggml_tensor_extra_gpu *) src1->extra;\n    float * src1_ddf = (float *) src1_extra->data_device[g_main_device];\n\n    ggml_tensor_extra_gpu * dst_extra = (ggml_tensor_extra_gpu *) dst->extra;\n    float * dst_ddf = (float *) dst_extra->data_device[g_main_device];\n\n    ggml_mul_mat_p021_f16_f32_cuda(src0_ddq, src1_ddf, dst_ddf, ne00, ne01, ne02, ne12, main_stream);\n}\n\nstatic void ggml_cuda_mul_mat_vec_nc(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst){\n    GGML_ASSERT(!ggml_is_transposed(src0));\n    GGML_ASSERT(!ggml_is_transposed(src1));\n    GGML_ASSERT(!ggml_is_permuted(src0));\n    GGML_ASSERT(src0->backend != GGML_BACKEND_GPU_SPLIT);\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n\n    const int64_t nb01 = src0->nb[1];\n    const int64_t nb02 = src0->nb[2];\n\n    const int64_t ne12 = src1->ne[2];\n\n    CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n    cudaStream_t main_stream = g_cudaStreams[g_main_device][0];\n\n    ggml_tensor_extra_gpu * src0_extra = (ggml_tensor_extra_gpu *) src0->extra;\n    void * src0_ddq = src0_extra->data_device[g_main_device];\n\n    ggml_tensor_extra_gpu * src1_extra = (ggml_tensor_extra_gpu *) src1->extra;\n    float * src1_ddf = (float *) src1_extra->data_device[g_main_device];\n\n    ggml_tensor_extra_gpu * dst_extra = (ggml_tensor_extra_gpu *) dst->extra;\n    float * dst_ddf = (float *) dst_extra->data_device[g_main_device];\n\n    const int64_t row_stride_x = nb01 / sizeof(half);\n    const int64_t channel_stride_x = nb02 / sizeof(half);\n\n    ggml_mul_mat_vec_nc_f16_f32_cuda(src0_ddq, src1_ddf, dst_ddf, ne00, ne01, row_stride_x, ne02, ne12, channel_stride_x, main_stream);\n}\n\n__global__ void k_compute_batched_ptrs(\n        const half * src0_as_f16, const half * src1_as_f16, half * dst_f16,\n        const void ** ptrs_src, void ** ptrs_dst,\n        int ne12, int ne13,\n        int ne23,\n        int nb02, int nb03,\n        int nb12, int nb13,\n        int nb2, int nb3,\n        int r2, int r3) {\n    int i13 = blockIdx.x * blockDim.x + threadIdx.x;\n    int i12 = blockIdx.y * blockDim.y + threadIdx.y;\n\n    if (i13 >= ne13 || i12 >= ne12) {\n        return;\n    }\n\n    int i03 = i13 / r3;\n    int i02 = i12 / r2;\n\n    ptrs_src[0*ne23 + i12 + i13*ne12] = (const char *) src0_as_f16 + i02*nb02   + i03*nb03;\n    ptrs_src[1*ne23 + i12 + i13*ne12] = (const char *) src1_as_f16 + i12*nb12/2 + i13*nb13/2;\n    ptrs_dst[0*ne23 + i12 + i13*ne12] = (      char *)     dst_f16 + i12* nb2/2 + i13* nb3/2;\n}\n\nstatic void ggml_cuda_mul_mat_mat_batched_cublas(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    GGML_ASSERT(!ggml_is_transposed(src0));\n    GGML_ASSERT(!ggml_is_transposed(src1));\n\n    GGML_ASSERT(src0->backend != GGML_BACKEND_GPU_SPLIT);\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n\n    const int64_t ne00 = src0->ne[0]; GGML_UNUSED(ne00);\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n    const int64_t ne03 = src0->ne[3];\n\n    const int64_t nb01 = src0->nb[1];\n    const int64_t nb02 = src0->nb[2]; GGML_UNUSED(nb02);\n    const int64_t nb03 = src0->nb[3]; GGML_UNUSED(nb03);\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n    const int64_t ne12 = src1->ne[2];\n    const int64_t ne13 = src1->ne[3];\n\n    const int64_t nb11 = src1->nb[1];\n    const int64_t nb12 = src1->nb[2]; GGML_UNUSED(nb12);\n    const int64_t nb13 = src1->nb[3]; GGML_UNUSED(nb13);\n\n    const int64_t ne1 = ggml_nelements(src1);\n    const int64_t ne  = ggml_nelements(dst);\n\n    CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n    cudaStream_t main_stream = g_cudaStreams[g_main_device][0];\n\n    int id;\n    CUDA_CHECK(cudaGetDevice(&id));\n    CUBLAS_CHECK(cublasSetStream(g_cublas_handles[id], main_stream));\n\n    ggml_tensor_extra_gpu * src0_extra = (ggml_tensor_extra_gpu *) src0->extra;\n    void * src0_ddq = src0_extra->data_device[g_main_device];\n    half * src0_as_f16 = (half *) src0_ddq;\n\n    ggml_tensor_extra_gpu * src1_extra = (ggml_tensor_extra_gpu *) src1->extra;\n    float * src1_ddf = (float *) src1_extra->data_device[g_main_device];\n\n    ggml_tensor_extra_gpu * dst_extra = (ggml_tensor_extra_gpu *) dst->extra;\n    float * dst_ddf = (float *) dst_extra->data_device[g_main_device];\n\n    // convert src1 to fp16\n    const to_fp16_cuda_t to_fp16_cuda = ggml_get_to_fp16_cuda(src1->type);\n    GGML_ASSERT(to_fp16_cuda != nullptr);\n\n    size_t src1_as = 0;\n    half * src1_as_f16 = (half *) ggml_cuda_pool_malloc(ne1 * sizeof(half), &src1_as);\n    to_fp16_cuda(src1_ddf, src1_as_f16, ne1, main_stream);\n\n    size_t dst_as = 0;\n    half * dst_f16 = (half *) ggml_cuda_pool_malloc(ne * sizeof(half), &dst_as);\n\n    GGML_ASSERT(ne12 % ne02 == 0);\n    GGML_ASSERT(ne13 % ne03 == 0);\n\n    // broadcast factors\n    const int64_t r2 = ne12/ne02;\n    const int64_t r3 = ne13/ne03;\n\n    const half alpha_f16 = 1.0f;\n    const half beta_f16  = 0.0f;\n\n#if 0\n    // use cublasGemmEx\n    {\n        for (int i13 = 0; i13 < ne13; ++i13) {\n            for (int i12 = 0; i12 < ne12; ++i12) {\n                int i03 = i13 / r3;\n                int i02 = i12 / r2;\n\n                CUBLAS_CHECK(\n                        cublasGemmEx(g_cublas_handles[id], CUBLAS_OP_T, CUBLAS_OP_N,\n                            ne01, ne11, ne10,\n                            &alpha_f16, (const char *) src0_as_f16 + i02*src0->nb[2]   + i03*src0->nb[3]  , CUDA_R_16F, nb01/sizeof(half),\n                                        (const char *) src1_as_f16 + i12*src1->nb[2]/2 + i13*src1->nb[3]/2, CUDA_R_16F, nb11/sizeof(float),\n                            &beta_f16,  (      char *)     dst_f16 + i12* dst->nb[2]/2 + i13* dst->nb[3]/2, CUDA_R_16F, ne01,\n                            CUBLAS_COMPUTE_16F,\n                            CUBLAS_GEMM_DEFAULT_TENSOR_OP));\n            }\n        }\n    }\n#else\n    if (r2 == 1 && r3 == 1 && src0->nb[2]*src0->ne[2] == src0->nb[3] && src1->nb[2]*src1->ne[2] == src1->nb[3]) {\n        // there is no broadcast and src0, src1 are contiguous across dims 2, 3\n        // use cublasGemmStridedBatchedEx\n        CUBLAS_CHECK(\n        cublasGemmStridedBatchedEx(g_cublas_handles[id], CUBLAS_OP_T, CUBLAS_OP_N,\n                ne01, ne11, ne10,\n                &alpha_f16, (const char *) src0_as_f16, CUDA_R_16F, nb01/sizeof(half),  src0->nb[2]/sizeof(half),  // strideA\n                            (const char *) src1_as_f16, CUDA_R_16F, nb11/sizeof(float), src1->nb[2]/sizeof(float), // strideB\n                &beta_f16,  (      char *)     dst_f16, CUDA_R_16F, ne01,                dst->nb[2]/sizeof(float), // strideC\n                ne12*ne13,\n                CUBLAS_COMPUTE_16F,\n                CUBLAS_GEMM_DEFAULT_TENSOR_OP));\n    } else {\n        // use cublasGemmBatchedEx\n        const int ne23 = ne12*ne13;\n\n        const void ** ptrs_src = nullptr;\n              void ** ptrs_dst = nullptr;\n\n        size_t ptrs_src_s = 0;\n        size_t ptrs_dst_s = 0;\n\n        ptrs_src = (const void **) ggml_cuda_pool_malloc(2*ne23*sizeof(void *), &ptrs_src_s);\n        ptrs_dst = (      void **) ggml_cuda_pool_malloc(1*ne23*sizeof(void *), &ptrs_dst_s);\n\n        dim3 block_dims(ne13, ne12);\n        k_compute_batched_ptrs<<<1, block_dims, 0, main_stream>>>(\n                src0_as_f16, src1_as_f16, dst_f16,\n                ptrs_src, ptrs_dst,\n                ne12, ne13,\n                ne23,\n                nb02, nb03,\n                nb12, nb13,\n                dst->nb[2], dst->nb[3],\n                r2, r3);\n        CUDA_CHECK(cudaGetLastError());\n\n        CUBLAS_CHECK(\n        cublasGemmBatchedEx(g_cublas_handles[id], CUBLAS_OP_T, CUBLAS_OP_N,\n                ne01, ne11, ne10,\n                &alpha_f16, (const void **) (ptrs_src + 0*ne23), CUDA_R_16F, nb01/sizeof(half),\n                            (const void **) (ptrs_src + 1*ne23), CUDA_R_16F, nb11/sizeof(float),\n                &beta_f16,  (      void **) (ptrs_dst + 0*ne23), CUDA_R_16F, ne01,\n                ne23,\n                CUBLAS_COMPUTE_16F,\n                CUBLAS_GEMM_DEFAULT_TENSOR_OP));\n\n        if (ptrs_src_s != 0) {\n            ggml_cuda_pool_free(ptrs_src, ptrs_src_s);\n        }\n        if (ptrs_dst_s != 0) {\n            ggml_cuda_pool_free(ptrs_dst, ptrs_dst_s);\n        }\n    }\n#endif\n\n    const to_fp32_cuda_t to_fp32_cuda = ggml_get_to_fp32_cuda(GGML_TYPE_F16);\n    to_fp32_cuda(dst_f16, dst_ddf, ne, main_stream);\n\n    ggml_cuda_pool_free(src1_as_f16, src1_as);\n    ggml_cuda_pool_free(dst_f16, dst_as);\n}\n\nstatic void ggml_cuda_mul_mat(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    const bool all_on_device =\n        (src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT) &&\n        (src1->backend == GGML_BACKEND_GPU) &&\n        ( dst->backend == GGML_BACKEND_GPU);\n\n    const bool split = src0->backend == GGML_BACKEND_GPU_SPLIT;\n\n    int64_t min_compute_capability = INT_MAX;\n    for (int64_t id = 0; id < g_device_count; ++id) {\n        if (min_compute_capability > g_compute_capabilities[id] && g_tensor_split[id] < (id + 1 < g_device_count ? g_tensor_split[id + 1] : 1.0f)) {\n            min_compute_capability = g_compute_capabilities[id];\n        }\n    }\n\n#ifdef CUDA_USE_TENSOR_CORES\n    const bool use_tensor_cores = true;\n#else\n    const bool use_tensor_cores = false;\n#endif\n\n    // debug helpers\n    //printf(\"src0: %8d %8d %8d %8d\\n\", src0->ne[0], src0->ne[1], src0->ne[2], src0->ne[3]);\n    //printf(\"      %8d %8d %8d %8d\\n\", src0->nb[0], src0->nb[1], src0->nb[2], src0->nb[3]);\n    //printf(\"src1: %8d %8d %8d %8d\\n\", src1->ne[0], src1->ne[1], src1->ne[2], src1->ne[3]);\n    //printf(\"      %8d %8d %8d %8d\\n\", src1->nb[0], src1->nb[1], src1->nb[2], src1->nb[3]);\n    //printf(\"src0 is contiguous %d, transposed %d, type = %s, name = %s\\n\", ggml_is_contiguous(src0), ggml_is_transposed(src0), ggml_type_name(src0->type), src0->name);\n    //printf(\"src1 is contiguous %d, transposed %d, type = %s, name = %s\\n\", ggml_is_contiguous(src1), ggml_is_transposed(src1), ggml_type_name(src1->type), src1->name);\n\n    if (!split && all_on_device && !use_tensor_cores && src0->type == GGML_TYPE_F16 && ggml_is_permuted(src0) && ggml_is_permuted(src1) && src1->ne[1] == 1) {\n        // KQ single-batch\n        ggml_cuda_mul_mat_vec_p021(src0, src1, dst);\n    } else if (!split && all_on_device && !use_tensor_cores && src0->type == GGML_TYPE_F16 && !ggml_is_contiguous(src0) && !ggml_is_transposed(src1) && src1->ne[1] == 1) {\n        // KQV single-batch\n        ggml_cuda_mul_mat_vec_nc(src0, src1, dst);\n    } else if (!split && all_on_device && use_tensor_cores && src0->type == GGML_TYPE_F16 && src1->type == GGML_TYPE_F32 && !ggml_is_transposed(src0) && !ggml_is_transposed(src1)) {\n        // KQ + KQV multi-batch\n        ggml_cuda_mul_mat_mat_batched_cublas(src0, src1, dst);\n    } else if (src0->type == GGML_TYPE_F32) {\n        ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_cublas, false);\n    } else if (ggml_is_quantized(src0->type) || src0->type == GGML_TYPE_F16) {\n        if (src1->ne[1] == 1 && src0->ne[0] % GGML_CUDA_DMMV_X == 0) {\n#ifdef GGML_CUDA_FORCE_DMMV\n            const bool use_mul_mat_vec_q = false;\n#else\n            const bool use_mul_mat_vec_q = min_compute_capability >= MIN_CC_DP4A && ggml_is_quantized(src0->type);\n#endif // GGML_CUDA_FORCE_DMMV\n\n            if (use_mul_mat_vec_q) {\n                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_q, true);\n            } else {\n                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_dequantize_mul_mat_vec, false);\n            }\n        } else {\n            bool use_mul_mat_q = min_compute_capability >= MIN_CC_DP4A && ggml_is_quantized(src0->type);\n\n            // when tensor cores are available, use them for large batch size\n            // ref: https://github.com/ggerganov/llama.cpp/pull/3776\n            if (use_tensor_cores && min_compute_capability >= CC_VOLTA && src1->ne[1] > MMQ_MAX_BATCH_SIZE) {\n                use_mul_mat_q = false;\n            }\n\n            if (use_mul_mat_q) {\n                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_q, true);\n            } else {\n                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_cublas, false);\n            }\n        }\n    } else {\n        GGML_ASSERT(false);\n    }\n}\n\nstatic void ggml_cuda_mul_mat_sparse(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    GGML_ASSERT(dst->src[2] != NULL && \"dst->src[2] must be present for sparse matrix multiplication\");\n    if (src1->ne[1] == 1 && src0->ne[0] % GGML_CUDA_DMMV_X == 0) {\n        switch(src0->type) {\n            case GGML_TYPE_F16:\n                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_sparse_dequantized, false);\n                break;\n            case GGML_TYPE_Q4_0:\n                ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_vec_sparse_q, true);\n                break;\n            default:\n                GGML_ASSERT(false && \"unsupported type for sparse matrix multiplication\");\n        }\n    } else {\n        ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_batch_sparse, false);\n    }\n}\n\nvoid ggml_cuda_axpy(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    GGML_ASSERT(dst->src[2] != NULL && \"dst->src[2] must be present for axpy\");\n    bool all_on_device = (src0->backend == GGML_BACKEND_GPU || src0->backend == GGML_BACKEND_GPU_SPLIT) &&\n        src1->backend == GGML_BACKEND_GPU && dst->backend == GGML_BACKEND_GPU;\n    if (src1->ne[1] > 100) {\n        ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_mul_mat_transpose_gemm, false);\n    } else {\n        ggml_cuda_op_mul_mat(src0, src1, dst, ggml_cuda_op_dequantize_axpy, false);\n    }\n}\n\nstatic void ggml_cuda_scale(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_scale);\n}\n\nstatic void ggml_cuda_clamp(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_clamp);\n}\n\nstatic void ggml_cuda_cpy(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    const int64_t ne = ggml_nelements(src0);\n    GGML_ASSERT(ne == ggml_nelements(src1));\n\n    // GGML_ASSERT(src0->backend == GGML_BACKEND_GPU);\n    // GGML_ASSERT(src1->backend == GGML_BACKEND_GPU);\n\n    GGML_ASSERT(ggml_nbytes(src0) <= INT_MAX);\n    GGML_ASSERT(ggml_nbytes(src1) <= INT_MAX);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    GGML_ASSERT(src0->ne[3] == 1);\n\n    const int64_t nb00 = src0->nb[0];\n    const int64_t nb01 = src0->nb[1];\n    const int64_t nb02 = src0->nb[2];\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n    GGML_ASSERT(src1->ne[3] == 1);\n\n    const int64_t nb10 = src1->nb[0];\n    const int64_t nb11 = src1->nb[1];\n    const int64_t nb12 = src1->nb[2];\n\n    CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n    cudaStream_t main_stream = g_cudaStreams[g_main_device][0];\n\n    if (src0->backend == GGML_BACKEND_GPU && src1->backend == GGML_BACKEND_CPU) {\n        int size = ggml_nbytes(src0);\n        const struct ggml_tensor_extra_gpu *src0_extra = (ggml_tensor_extra_gpu *)src0->extra;\n        cudaMemcpyAsync(src1->data, src0_extra->data_device[g_main_device], size, cudaMemcpyDeviceToHost, main_stream);\n        cudaStreamSynchronize(main_stream);\n        return ;\n    }\n    else if (src0->backend == GGML_BACKEND_CPU){\n        GGML_ASSERT(-1);\n    }\n\n    const ggml_tensor_extra_gpu * src0_extra = (ggml_tensor_extra_gpu *) src0->extra;\n    const ggml_tensor_extra_gpu * src1_extra = (ggml_tensor_extra_gpu *) src1->extra;\n\n    char * src0_ddc = (char *) src0_extra->data_device[g_main_device];\n    char * src1_ddc = (char *) src1_extra->data_device[g_main_device];\n\n    if (src0->type == GGML_TYPE_F32 && src1->type == GGML_TYPE_F32) {\n        ggml_cpy_f32_f32_cuda(src0_ddc, src1_ddc, ne, ne00, ne01, nb00, nb01, nb02,\n                              ne10, ne11, nb10, nb11, nb12, main_stream);\n    } else if (src0->type == GGML_TYPE_F32 && src1->type == GGML_TYPE_F16) {\n        ggml_cpy_f32_f16_cuda(src0_ddc, src1_ddc, ne, ne00, ne01, nb00, nb01, nb02,\n                              ne10, ne11, nb10, nb11, nb12, main_stream);\n    } else if (src0->type == GGML_TYPE_F16 && src1->type == GGML_TYPE_F16) {\n        ggml_cpy_f16_f16_cuda(src0_ddc, src1_ddc, ne, ne00, ne01, nb00, nb01, nb02,\n                              ne10, ne11, nb10, nb11, nb12, main_stream);\n    } else {\n        fprintf(stderr, \"%s: unsupported type combination (%s to %s)\\n\", __func__,\n                ggml_type_name(src0->type), ggml_type_name(src1->type));\n        GGML_ASSERT(false);\n    }\n\n    (void) dst;\n}\n\nstatic void ggml_cuda_dup(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_cpy(src0, dst, nullptr);\n    (void) src1;\n}\n\nstatic void ggml_cuda_diag_mask_inf(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_diag_mask_inf);\n}\n\nstatic void ggml_cuda_soft_max(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_soft_max);\n}\n\nstatic void ggml_cuda_rope(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous(src0)); // TODO: this restriction is temporary until non-cont support is implemented\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_rope);\n}\n\nstatic void ggml_cuda_alibi(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_alibi);\n}\n\nstatic void ggml_cuda_im2col(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    ggml_cuda_op_flatten(src0, src1, dst, ggml_cuda_op_im2col);\n}\n\nstatic void ggml_cuda_nop(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    (void) src0;\n    (void) src1;\n    (void) dst;\n}\n\nstatic void ggml_cuda_transform_tensor_impl(void * data, struct ggml_tensor * tensor, bool alloc_only) {\n    const int64_t nrows = ggml_nrows(tensor);\n\n    const int64_t ne0 = tensor->ne[0];\n\n    const size_t nb1 = tensor->nb[1];\n\n    ggml_backend_type backend = tensor->backend;\n    ggml_tensor_extra_gpu * extra = new struct ggml_tensor_extra_gpu;\n    memset(extra, 0, sizeof(*extra));\n\n    for (int64_t id = 0; id < g_device_count; ++id) {\n        if (backend == GGML_BACKEND_GPU && id != g_main_device) {\n            continue;\n        }\n\n        ggml_cuda_set_device(id);\n\n        int64_t row_low, row_high;\n        if (backend == GGML_BACKEND_GPU) {\n            row_low = 0;\n            row_high = nrows;\n        } else if (backend == GGML_BACKEND_GPU_SPLIT) {\n            const int64_t rounding = get_row_rounding(tensor->type);\n\n            row_low = id == 0 ? 0 : nrows*g_tensor_split[id];\n            row_low -= row_low % rounding;\n\n            if (id == g_device_count - 1) {\n                row_high = nrows;\n            } else {\n                row_high = nrows*g_tensor_split[id + 1];\n                row_high -= row_high % rounding;\n            }\n        } else {\n            GGML_ASSERT(false);\n        }\n        if (row_low == row_high) {\n            continue;\n        }\n\n        int64_t nrows_split = row_high - row_low;\n\n        const size_t offset_split = row_low*nb1;\n        size_t size = ggml_nbytes_split(tensor, nrows_split);\n        const size_t original_size = size;\n\n        // pad last row to a multiple of 512 elements to avoid out-of-bounds memory accesses\n        if (ne0 % MATRIX_ROW_PADDING != 0) {\n            size += (MATRIX_ROW_PADDING - ne0 % MATRIX_ROW_PADDING)\n                * ggml_type_size(tensor->type)/ggml_blck_size(tensor->type);\n        }\n\n        char * buf;\n        CUDA_CHECK(cudaMalloc(&buf, size));\n\n        // set padding to 0 to avoid possible NaN values\n        if (size > original_size) {\n            CUDA_CHECK(cudaMemset(buf + original_size, 0, size - original_size));\n        }\n\n        if (!alloc_only) {\n            char * buf_host = (char*)data + offset_split;\n            CUDA_CHECK(cudaMemcpy(buf, buf_host, original_size, cudaMemcpyHostToDevice));\n        }\n\n        extra->data_device[id] = buf;\n\n        if (backend == GGML_BACKEND_GPU_SPLIT) {\n            for (int64_t is = 0; is < MAX_STREAMS; ++is) {\n                CUDA_CHECK(cudaEventCreateWithFlags(&extra->events[id][is], cudaEventDisableTiming));\n            }\n        }\n    }\n\n    tensor->extra = extra;\n}\n\nvoid ggml_cuda_transform_tensor(void * data, struct ggml_tensor * tensor) {\n    return ggml_cuda_transform_tensor_impl(data, tensor, false);\n}\n\nvoid ggml_cuda_alloc_tensor(struct ggml_tensor * tensor) {\n    return ggml_cuda_transform_tensor_impl(nullptr, tensor, true);\n}\n\nvoid ggml_cuda_free_data(struct ggml_tensor * tensor) {\n    if (!tensor || (tensor->backend != GGML_BACKEND_GPU && tensor->backend != GGML_BACKEND_GPU_SPLIT) ) {\n        return;\n    }\n\n    ggml_tensor_extra_gpu * extra = (ggml_tensor_extra_gpu *) tensor->extra;\n\n    for (int64_t id = 0; id < g_device_count; ++id) {\n        if (extra->data_device[id] != nullptr) {\n            CUDA_CHECK(ggml_cuda_set_device(id));\n            CUDA_CHECK(cudaFree(extra->data_device[id]));\n        }\n\n        for (int64_t is = 0; is < MAX_STREAMS; ++is) {\n            if (extra->events[id][is] != nullptr) {\n                CUDA_CHECK(ggml_cuda_set_device(id));\n                CUDA_CHECK(cudaEventDestroy(extra->events[id][is]));\n            }\n        }\n    }\n\n    delete extra;\n}\n\nstatic ggml_tensor_extra_gpu * g_temp_tensor_extras = nullptr;\nstatic size_t g_temp_tensor_extra_index = 0;\n\nstatic ggml_tensor_extra_gpu * ggml_cuda_alloc_temp_tensor_extra() {\n    if (g_temp_tensor_extras == nullptr) {\n        g_temp_tensor_extras = new ggml_tensor_extra_gpu[GGML_CUDA_MAX_NODES];\n    }\n\n    size_t alloc_index = g_temp_tensor_extra_index;\n    g_temp_tensor_extra_index = (g_temp_tensor_extra_index + 1) % GGML_CUDA_MAX_NODES;\n    ggml_tensor_extra_gpu * extra = &g_temp_tensor_extras[alloc_index];\n    memset(extra, 0, sizeof(*extra));\n\n    return extra;\n}\n\nstatic void ggml_cuda_assign_buffers_impl(struct ggml_tensor * tensor, bool scratch, bool force_inplace, bool no_alloc) {\n    if (scratch && g_scratch_size == 0) {\n        return;\n    }\n\n    tensor->backend = GGML_BACKEND_GPU;\n\n    // recursively assign CUDA buffers until a compute tensor is found\n    if (tensor->src[0] != nullptr && tensor->src[0]->backend == GGML_BACKEND_CPU) {\n        const ggml_op src0_op = tensor->src[0]->op;\n        if (src0_op == GGML_OP_RESHAPE || src0_op == GGML_OP_TRANSPOSE || src0_op == GGML_OP_VIEW || src0_op == GGML_OP_PERMUTE) {\n            ggml_cuda_assign_buffers_impl(tensor->src[0], scratch, force_inplace, no_alloc);\n        }\n    }\n    if (tensor->op == GGML_OP_CPY && tensor->src[1]->backend == GGML_BACKEND_CPU) {\n        ggml_cuda_assign_buffers_impl(tensor->src[1], scratch, force_inplace, no_alloc);\n    }\n\n    if (scratch && no_alloc) {\n        return;\n    }\n\n    ggml_tensor_extra_gpu * extra;\n\n    const bool inplace = (tensor->src[0] != nullptr && tensor->src[0]->data == tensor->data) ||\n        tensor->op == GGML_OP_VIEW ||\n        force_inplace;\n    const size_t size = ggml_nbytes(tensor);\n\n    CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n    if (inplace && (tensor->src[0]->backend == GGML_BACKEND_GPU || tensor->src[0]->backend == GGML_BACKEND_GPU_SPLIT)) {\n        ggml_tensor_extra_gpu * src0_extra = (ggml_tensor_extra_gpu * ) tensor->src[0]->extra;\n        char * src0_ddc = (char *) src0_extra->data_device[g_main_device];\n        size_t offset = 0;\n        if (tensor->op == GGML_OP_VIEW) {\n            memcpy(&offset, tensor->op_params, sizeof(size_t));\n        }\n        extra = ggml_cuda_alloc_temp_tensor_extra();\n        extra->data_device[g_main_device] = src0_ddc + offset;\n    } else if (tensor->op == GGML_OP_CPY) {\n        ggml_tensor_extra_gpu * src1_extra = (ggml_tensor_extra_gpu * ) tensor->src[1]->extra;\n        void * src1_ddv = src1_extra->data_device[g_main_device];\n        extra = ggml_cuda_alloc_temp_tensor_extra();\n        extra->data_device[g_main_device] = src1_ddv;\n    } else if (scratch) {\n        GGML_ASSERT(size <= g_scratch_size);\n        if (g_scratch_offset + size > g_scratch_size) {\n            g_scratch_offset = 0;\n        }\n\n        char * data = (char *) g_scratch_buffer;\n        if (data == nullptr) {\n            CUDA_CHECK(cudaMalloc(&data, g_scratch_size));\n            g_scratch_buffer = data;\n        }\n        extra = ggml_cuda_alloc_temp_tensor_extra();\n        extra->data_device[g_main_device] = data + g_scratch_offset;\n\n        g_scratch_offset += size;\n\n        GGML_ASSERT(g_scratch_offset <= g_scratch_size);\n    } else { // allocate new buffers outside of scratch\n        void * data;\n        CUDA_CHECK(cudaMalloc(&data, size));\n        CUDA_CHECK(cudaMemset(data, 0, size));\n        extra = new ggml_tensor_extra_gpu;\n        memset(extra, 0, sizeof(*extra));\n        extra->data_device[g_main_device] = data;\n    }\n\n    tensor->extra = extra;\n}\n\nvoid ggml_cuda_assign_scratch_offset(struct ggml_tensor * tensor, size_t offset) {\n    if (g_scratch_size == 0) {\n        return;\n    }\n    if (g_scratch_buffer == nullptr) {\n        ggml_cuda_set_device(g_main_device);\n        CUDA_CHECK(cudaMalloc(&g_scratch_buffer, g_scratch_size));\n    }\n\n    ggml_tensor_extra_gpu * extra = ggml_cuda_alloc_temp_tensor_extra();\n\n    const bool inplace = (tensor->src[0] != nullptr && tensor->src[0]->data == tensor->data) ||\n        tensor->op == GGML_OP_VIEW;\n\n    if (inplace && (tensor->src[0]->backend == GGML_BACKEND_GPU || tensor->src[0]->backend == GGML_BACKEND_GPU_SPLIT)) {\n        ggml_tensor_extra_gpu * src0_extra = (ggml_tensor_extra_gpu * ) tensor->src[0]->extra;\n        char * src0_ddc = (char *) src0_extra->data_device[g_main_device];\n        size_t view_offset = 0;\n        if (tensor->op == GGML_OP_VIEW) {\n            memcpy(&view_offset, tensor->op_params, sizeof(size_t));\n        }\n        extra->data_device[g_main_device] = src0_ddc + view_offset;\n    } else {\n        extra->data_device[g_main_device] = (char *) g_scratch_buffer + offset;\n    }\n\n    tensor->extra = extra;\n}\n\nvoid ggml_cuda_copy_to_device(struct ggml_tensor * tensor) {\n    GGML_ASSERT(tensor->backend == GGML_BACKEND_GPU);\n    GGML_ASSERT(ggml_is_contiguous(tensor));\n\n    ggml_tensor_extra_gpu * extra = (ggml_tensor_extra_gpu *) tensor->extra;\n    CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n    CUDA_CHECK(cudaMemcpy(extra->data_device[g_main_device], tensor->data, ggml_nbytes(tensor), cudaMemcpyHostToDevice));\n}\n\nvoid ggml_cuda_copy_to_host(struct ggml_tensor * tensor) {\n    GGML_ASSERT(tensor->backend != GGML_BACKEND_CPU && \"cannot copy to host from CPU tensor\");\n    GGML_ASSERT(tensor->backend != GGML_BACKEND_GPU_SPLIT && \"not implemented\");\n\n    ggml_tensor_extra_gpu * extra = (ggml_tensor_extra_gpu *) tensor->extra;\n    CUDA_CHECK(ggml_cuda_set_device(g_main_device));\n\n    // assumes GPU data is contiguous and CPU buffer is allocated\n    CUDA_CHECK(cudaMemcpy(tensor->data, extra->data_device[g_main_device], ggml_nbytes(tensor), cudaMemcpyDeviceToHost));\n}\n\nvoid ggml_cuda_assign_buffers(struct ggml_tensor * tensor) {\n    if (tensor == NULL)\n        return;\n    ggml_cuda_assign_buffers_impl(tensor, true, false, false);\n}\n\nvoid ggml_cuda_assign_buffers_no_alloc(struct ggml_tensor * tensor) {\n    ggml_cuda_assign_buffers_impl(tensor, true, false, true);\n}\n\nvoid ggml_cuda_assign_buffers_no_scratch(struct ggml_tensor * tensor) {\n    ggml_cuda_assign_buffers_impl(tensor, false, false, false);\n}\n\nvoid ggml_cuda_assign_buffers_force_inplace(struct ggml_tensor * tensor) {\n    ggml_cuda_assign_buffers_impl(tensor, false, true, false);\n}\n\nvoid ggml_cuda_set_main_device(const int main_device) {\n    if (main_device >= g_device_count) {\n        fprintf(stderr, \"warning: cannot set main_device=%d because there are only %d devices. Using device %d instead.\\n\",\n                main_device, g_device_count, g_main_device);\n        return;\n    }\n    g_main_device = main_device;\n    if (g_device_count > 1) {\n        cudaDeviceProp prop;\n        CUDA_CHECK(cudaGetDeviceProperties(&prop, g_main_device));\n        fprintf(stderr, \"%s: using device %d (%s) as main device\\n\", __func__, g_main_device, prop.name);\n    }\n}\n\nvoid ggml_cuda_set_scratch_size(const size_t scratch_size) {\n    // this is a hack to not completely break llama.cpp when using multiple models or contexts simultaneously\n    // it still won't always work as expected, but it's better than nothing\n    if (scratch_size > g_scratch_size) {\n        ggml_cuda_free_scratch();\n    }\n    g_scratch_size = std::max(g_scratch_size, scratch_size);\n}\n\nvoid ggml_cuda_free_scratch() {\n    if (g_scratch_buffer == nullptr) {\n        return;\n    }\n\n    CUDA_CHECK(cudaFree(g_scratch_buffer));\n    g_scratch_buffer = nullptr;\n}\n\nbool ggml_cuda_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {\n    if (!g_cublas_loaded) return false;\n\n    ggml_cuda_func_t func;\n    const bool src0_on_device = tensor->src[0] != nullptr && (tensor->src[0]->backend != GGML_BACKEND_CPU);\n    const bool any_on_device = tensor->backend == GGML_BACKEND_GPU || src0_on_device\n        || (tensor->src[1] != nullptr && tensor->src[1]->backend == GGML_BACKEND_GPU);\n\n    // when src0 (weights) is not on device, we compute on CPU with sparsity\n    if (!src0_on_device && (tensor->op == GGML_OP_MUL_MAT_SPARSE || tensor->op == GGML_OP_AXPY)\n        || !any_on_device && tensor->op != GGML_OP_MUL_MAT) {\n        return false;\n    }\n\n    if (tensor->op == GGML_OP_MUL_MAT) {\n        if (tensor->src[0]->ne[3] != tensor->src[1]->ne[3]) {\n#ifndef NDEBUG\n            fprintf(stderr, \"%s: cannot compute %s: src0->ne[3] = %d, src1->ne[3] = %d - fallback to CPU\\n\", __func__, tensor->name, tensor->src[0]->ne[3], tensor->src[1]->ne[3]);\n#endif\n            return false;\n        }\n    }\n\n    switch (tensor->op) {\n        case GGML_OP_REPEAT:\n            func = ggml_cuda_repeat;\n            break;\n        case GGML_OP_GET_ROWS:\n            func = ggml_cuda_get_rows;\n            break;\n        case GGML_OP_DUP:\n            func = ggml_cuda_dup;\n            break;\n        case GGML_OP_ADD:\n            func = ggml_cuda_add;\n            break;\n        case GGML_OP_MUL:\n            func = ggml_cuda_mul;\n            break;\n        case GGML_OP_UNARY:\n            switch (ggml_get_unary_op(tensor)) {\n                case GGML_UNARY_OP_GELU:\n                    func = ggml_cuda_gelu;\n                    break;\n                case GGML_UNARY_OP_SILU:\n                    func = ggml_cuda_silu;\n                    break;\n                case GGML_UNARY_OP_RELU:\n                    func = ggml_cuda_relu;\n                    break;\n                default:\n                    return false;\n            } break;\n        case GGML_OP_NORM:\n            func = ggml_cuda_norm;\n            break;\n        case GGML_OP_RMS_NORM:\n            func = ggml_cuda_rms_norm;\n            break;\n        case GGML_OP_MUL_MAT:\n            if (!any_on_device && !ggml_cuda_can_mul_mat(tensor->src[0], tensor->src[1], tensor)) {\n                return false;\n            }\n            func = ggml_cuda_mul_mat;\n            break;\n        case GGML_OP_MUL_MAT_SPARSE:\n            if (!src0_on_device && !ggml_cuda_can_mul_mat(tensor->src[0], tensor->src[1], tensor)) {\n                return false;\n            }\n            func = ggml_cuda_mul_mat_sparse;\n            break;\n        case GGML_OP_AXPY:\n            func = ggml_cuda_axpy;\n            break;\n        case GGML_OP_SCALE:\n            func = ggml_cuda_scale;\n            break;\n        case GGML_OP_SQR:\n            func = ggml_cuda_sqr;\n            break;\n        case GGML_OP_CLAMP:\n            if (!any_on_device) {\n                return false;\n            }\n            func = ggml_cuda_clamp;\n            break;\n        case GGML_OP_CPY:\n            func = ggml_cuda_cpy;\n            break;\n        case GGML_OP_CONT:\n            func = ggml_cuda_dup;\n            break;\n        case GGML_OP_RESHAPE:\n        case GGML_OP_VIEW:\n        case GGML_OP_PERMUTE:\n        case GGML_OP_TRANSPOSE:\n            func = ggml_cuda_nop;\n            break;\n        case GGML_OP_DIAG_MASK_INF:\n            func = ggml_cuda_diag_mask_inf;\n            break;\n        case GGML_OP_SOFT_MAX:\n            func = ggml_cuda_soft_max;\n            break;\n        case GGML_OP_ROPE:\n            func = ggml_cuda_rope;\n            break;\n        case GGML_OP_ALIBI:\n            func = ggml_cuda_alibi;\n            break;\n        case GGML_OP_IM2COL:\n            func = ggml_cuda_im2col;\n            break;\n        default:\n            return false;\n    }\n\n    if (params->ith != 0) {\n        return true;\n    }\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return true;\n    }\n    func(tensor->src[0], tensor->src[1], tensor);\n\n    // CUDA_CHECK(cudaDeviceSynchronize());\n\n    return true;\n}\n\nint ggml_cuda_get_device_count() {\n    int device_count;\n    CUDA_CHECK(cudaGetDeviceCount(&device_count));\n    return device_count;\n}\n\nsize_t ggml_cuda_get_free_memory(int device) {\n    size_t free, total;\n    CUDA_CHECK(cudaSetDevice(device));\n    CUDA_CHECK(cudaMemGetInfo(&free, &total));\n    return free;\n}\n\nvoid ggml_cuda_get_device_description(int device, char * description, size_t description_size) {\n    cudaDeviceProp prop;\n    CUDA_CHECK(cudaGetDeviceProperties(&prop, device));\n    snprintf(description, description_size, \"%s\", prop.name);\n}\n\n////////////////////////////////////////////////////////////////////////////////\n\n// backend interface\n\n#define UNUSED GGML_UNUSED\n\nstruct ggml_backend_context_cuda {\n};\n\nstatic const char * ggml_backend_cuda_name(ggml_backend_t backend) {\n    return GGML_CUDA_NAME;\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cuda_free(ggml_backend_t backend) {\n    ggml_backend_context_cuda * cuda_ctx = (ggml_backend_context_cuda *)backend->context;\n    delete cuda_ctx;\n    delete backend;\n}\n\nstruct ggml_backend_buffer_context_cuda {\n    void * device;\n\n    ggml_tensor_extra_gpu * temp_tensor_extras = nullptr;\n    size_t temp_tensor_extra_index = 0;\n\n    ~ggml_backend_buffer_context_cuda() {\n        delete[] temp_tensor_extras;\n    }\n\n    ggml_tensor_extra_gpu * ggml_cuda_alloc_temp_tensor_extra() {\n        if (temp_tensor_extras == nullptr) {\n            temp_tensor_extras = new ggml_tensor_extra_gpu[GGML_CUDA_MAX_NODES];\n        }\n\n        size_t alloc_index = temp_tensor_extra_index;\n        temp_tensor_extra_index = (temp_tensor_extra_index + 1) % GGML_CUDA_MAX_NODES;\n        ggml_tensor_extra_gpu * extra = &temp_tensor_extras[alloc_index];\n        memset(extra, 0, sizeof(*extra));\n\n        return extra;\n    }\n};\n\nstatic void ggml_backend_cuda_buffer_free_buffer(ggml_backend_buffer_t buffer) {\n    ggml_backend_buffer_context_cuda * ctx = (ggml_backend_buffer_context_cuda *)buffer->context;\n    CUDA_CHECK(cudaFree(ctx->device));\n    delete ctx;\n}\n\nstatic void * ggml_backend_cuda_buffer_get_base(ggml_backend_buffer_t buffer) {\n    ggml_backend_buffer_context_cuda * ctx = (ggml_backend_buffer_context_cuda *)buffer->context;\n    return ctx->device;\n}\n\nstatic size_t ggml_backend_cuda_buffer_get_alloc_size(ggml_backend_buffer_t buffer, ggml_tensor * tensor) {\n    int64_t row_low = 0;\n    int64_t row_high = ggml_nrows(tensor);\n    int64_t nrows_split = row_high - row_low;\n\n    size_t size = ggml_nbytes_split(tensor, nrows_split);\n\n    int64_t ne0 = tensor->ne[0];\n\n    if (ggml_is_quantized(tensor->type)) {\n        if (ne0 % MATRIX_ROW_PADDING != 0) {\n            size += (MATRIX_ROW_PADDING - ne0 % MATRIX_ROW_PADDING)\n                * ggml_type_size(tensor->type)/ggml_blck_size(tensor->type);\n        }\n    }\n\n    return size;\n\n    UNUSED(buffer);\n}\n\nstatic void ggml_backend_cuda_buffer_init_tensor(ggml_backend_buffer_t buffer, ggml_tensor * tensor) {\n    ggml_backend_buffer_context_cuda * ctx = (ggml_backend_buffer_context_cuda *)buffer->context;\n\n    if (tensor->view_src != NULL && tensor->view_offs == 0) {\n        assert(tensor->view_src->buffer->backend == buffer->backend);\n        tensor->backend = tensor->view_src->backend;\n        tensor->extra = tensor->view_src->extra;\n        return;\n    }\n\n    ggml_tensor_extra_gpu * extra = ctx->ggml_cuda_alloc_temp_tensor_extra();\n\n    extra->data_device[g_main_device] = tensor->data;\n\n    tensor->backend = GGML_BACKEND_GPU;\n    tensor->extra = extra;\n\n    if (ggml_is_quantized(tensor->type)) {\n        // initialize padding to 0 to avoid possible NaN values\n        int64_t row_low = 0;\n        int64_t row_high = ggml_nrows(tensor);\n        int64_t nrows_split = row_high - row_low;\n\n        size_t original_size = ggml_nbytes_split(tensor, nrows_split);\n        size_t padded_size = ggml_backend_cuda_buffer_get_alloc_size(tensor->buffer, tensor);\n\n        if (padded_size > original_size && tensor->view_src == nullptr) {\n            CUDA_CHECK(cudaMemsetAsync((char *)tensor->data + original_size, 0, padded_size - original_size, g_cudaStreams[g_main_device][0]));\n        }\n    }\n\n    UNUSED(buffer);\n}\n\nstatic struct ggml_backend_buffer_i cuda_backend_buffer_interface = {\n    /* .free_buffer    = */ ggml_backend_cuda_buffer_free_buffer,\n    /* .get_base       = */ ggml_backend_cuda_buffer_get_base,\n    /* .get_alloc_size = */ ggml_backend_cuda_buffer_get_alloc_size,\n    /* .init_tensor    = */ ggml_backend_cuda_buffer_init_tensor,\n    /* .free_tensor    = */ NULL,\n};\n\nstatic ggml_backend_buffer_t ggml_backend_cuda_alloc_buffer(ggml_backend_t backend, size_t size) {\n    ggml_cuda_set_device(g_main_device);\n\n    ggml_backend_buffer_context_cuda * ctx = new ggml_backend_buffer_context_cuda;\n\n    size = std::max(size, (size_t)1); // cudaMalloc returns null for size 0\n\n    ggml_cuda_set_device(g_main_device);\n    CUDA_CHECK(cudaMalloc(&ctx->device, size));\n\n    return ggml_backend_buffer_init(backend, cuda_backend_buffer_interface, ctx, size);\n}\n\nstatic size_t ggml_backend_cuda_get_alignment(ggml_backend_t backend) {\n    return 128;\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cuda_set_tensor_async(ggml_backend_t backend, ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n    GGML_ASSERT(offset + size <= ggml_nbytes(tensor) && \"tensor write out of bounds\");\n    GGML_ASSERT(tensor->data != NULL && \"tensor not allocated\");\n    GGML_ASSERT(tensor->backend == GGML_BACKEND_GPU);\n\n    CUDA_CHECK(cudaMemcpyAsync((char *)tensor->data + offset, data, size, cudaMemcpyHostToDevice, g_cudaStreams[g_main_device][0]));\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cuda_get_tensor_async(ggml_backend_t backend, const ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n    GGML_ASSERT(offset + size <= ggml_nbytes(tensor) && \"tensor read out of bounds\");\n    GGML_ASSERT(tensor->data != NULL && \"tensor not allocated\");\n    GGML_ASSERT(tensor->backend == GGML_BACKEND_GPU);\n\n    CUDA_CHECK(cudaMemcpyAsync(data, (const char *)tensor->data + offset, size, cudaMemcpyDeviceToHost, g_cudaStreams[g_main_device][0]));\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_cuda_synchronize(ggml_backend_t backend) {\n    CUDA_CHECK(cudaStreamSynchronize(g_cudaStreams[g_main_device][0]));\n\n    UNUSED(backend);\n}\n\nstatic ggml_backend_graph_plan_t ggml_backend_cuda_graph_plan_create(ggml_backend_t backend, ggml_cgraph * cgraph) {\n    GGML_ASSERT(!\"not implemented\");\n\n    return nullptr;\n\n    UNUSED(backend);\n    UNUSED(cgraph);\n}\n\nstatic void ggml_backend_cuda_graph_plan_free(ggml_backend_t backend, ggml_backend_graph_plan_t plan) {\n    GGML_ASSERT(!\"not implemented\");\n\n    UNUSED(backend);\n    UNUSED(plan);\n}\n\nstatic void ggml_backend_cuda_graph_plan_compute(ggml_backend_t backend, ggml_backend_graph_plan_t plan) {\n    GGML_ASSERT(!\"not implemented\");\n\n    UNUSED(backend);\n    UNUSED(plan);\n}\n\nstatic void ggml_backend_cuda_graph_compute(ggml_backend_t backend, ggml_cgraph * cgraph) {\n    ggml_cuda_set_device(g_main_device);\n\n    ggml_compute_params params = {};\n    params.type = GGML_TASK_COMPUTE;\n    params.ith = 0;\n    for (int i = 0; i < cgraph->n_nodes; i++) {\n        ggml_tensor * node = cgraph->nodes[i];\n\n        if (node->op == GGML_OP_RESHAPE || node->op == GGML_OP_TRANSPOSE || node->op == GGML_OP_VIEW || node->op == GGML_OP_PERMUTE)\n            continue;\n        assert(node->backend == GGML_BACKEND_GPU);\n        for (int j = 0; j < GGML_MAX_SRC; j++) {\n            if (node->src[j] != nullptr) {\n                assert(node->src[j]->backend == GGML_BACKEND_GPU);\n            }\n        }\n\n        bool ok = ggml_cuda_compute_forward(&params, node);\n        if (!ok) {\n            fprintf(stderr, \"%s: error: op not supported %s (%s)\\n\", __func__, node->name, ggml_op_name(node->op));\n        }\n        GGML_ASSERT(ok);\n\n#if 0\n        if (node->type == GGML_TYPE_F32) {\n            cudaDeviceSynchronize();\n            std::vector<float> tmp(ggml_nelements(node), 0.0f);\n            cudaMemcpy(tmp.data(), node->data, ggml_nelements(node)*sizeof(float), cudaMemcpyDeviceToHost);\n            printf(\"\\n%s (%s) (%s %s) (%s %s): \", node->name, ggml_op_name(node->op),\n                ggml_type_name(node->src[0]->type),\n                node->src[1] ? ggml_type_name(node->src[1]->type) : \"none\",\n                node->src[0]->name,\n                node->src[1] ? node->src[1]->name : \"none\");\n            double sum = 0.0;\n            double sq_sum = 0.0;\n            for (int i = 0; i < ggml_nelements(node); i++) {\n                printf(\"%f \", tmp[i]);\n                sum += tmp[i];\n                sq_sum += tmp[i]*tmp[i];\n            }\n            printf(\"\\n\");\n            printf(\"sum: %f, \", sum);\n            printf(\"sq_sum: %f\\n\", sq_sum);\n        }\n#endif\n    }\n\n    UNUSED(backend);\n}\n\nstatic ggml_backend_i cuda_backend_i = {\n    /* .get_name            = */ ggml_backend_cuda_name,\n    /* .free                = */ ggml_backend_cuda_free,\n    /* .alloc_buffer        = */ ggml_backend_cuda_alloc_buffer,\n    /* .get_alignment       = */ ggml_backend_cuda_get_alignment,\n    /* .set_tensor_async    = */ ggml_backend_cuda_set_tensor_async,\n    /* .get_tensor_async    = */ ggml_backend_cuda_get_tensor_async,\n    /* .synchronize         = */ ggml_backend_cuda_synchronize,\n    /* .cpy_tensor_from     = */ nullptr,\n    /* .cpy_tensor_to       = */ nullptr,\n    /* .graph_plan_create   = */ ggml_backend_cuda_graph_plan_create,\n    /* .graph_plan_free     = */ ggml_backend_cuda_graph_plan_free,\n    /* .graph_plan_compute  = */ ggml_backend_cuda_graph_plan_compute,\n    /* .graph_compute       = */ ggml_backend_cuda_graph_compute,\n    /* .supports_op         = */ nullptr,\n};\n\nggml_backend_t ggml_backend_cuda_init() {\n    ggml_init_cublas(); // TODO: remove from ggml.c\n\n    ggml_backend_context_cuda * ctx = new ggml_backend_context_cuda;\n\n    ggml_backend_t cuda_backend = new ggml_backend {\n        /* .interface = */ cuda_backend_i,\n        /* .context   = */ ctx\n    };\n\n    return cuda_backend;\n}\n\nvoid ggml_cuda_set_device_constants(float sparse_pred_threshold) {\n    CUDA_CHECK(cudaMemcpyToSymbol(dev_sparse_threshold, &sparse_pred_threshold, sizeof(float)));\n}\n"
        },
        {
          "name": "ggml-cuda.h",
          "type": "blob",
          "size": 2.5830078125,
          "content": "#pragma once\n\n#include \"ggml.h\"\n#include \"ggml-backend.h\"\n\n#ifdef GGML_USE_HIPBLAS\n#define GGML_CUDA_NAME \"ROCm\"\n#define GGML_CUBLAS_NAME \"hipBLAS\"\n#else\n#define GGML_CUDA_NAME \"CUDA\"\n#define GGML_CUBLAS_NAME \"cuBLAS\"\n#endif\n\n#ifdef  __cplusplus\nextern \"C\" {\n#endif\n\n#define GGML_CUDA_MAX_DEVICES       16\n\n// Always success. To check if CUDA is actually loaded, use `ggml_cublas_loaded`.\nGGML_API void   ggml_init_cublas(void);\n\n// Returns `true` if there are available CUDA devices and cublas loads successfully; otherwise, it returns `false`.\nGGML_API bool   ggml_cublas_loaded(void);\n\nGGML_API void * ggml_cuda_host_malloc(size_t size);\nGGML_API void   ggml_cuda_host_free(void * ptr);\n\nGGML_API bool   ggml_cuda_can_mul_mat(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst);\nGGML_API void   ggml_cuda_set_tensor_split(const float * tensor_split);\nGGML_API void   ggml_cuda_transform_tensor(void * data, struct ggml_tensor * tensor);\nGGML_API void   ggml_cuda_alloc_tensor(struct ggml_tensor * tensor);\nGGML_API void   ggml_cuda_free_data(struct ggml_tensor * tensor);\nGGML_API void   ggml_cuda_cpy_1d(struct ggml_tensor * dst, const struct ggml_tensor * src);\nGGML_API bool   debug_equal(short *a, short *b);\nGGML_API void **ggml_cuda_get_data_pp(struct ggml_tensor * tensor);\n\nGGML_API void   ggml_cuda_assign_buffers(struct ggml_tensor * tensor);\nGGML_API void   ggml_cuda_assign_buffers_no_scratch(struct ggml_tensor * tensor);\nGGML_API void   ggml_cuda_assign_buffers_force_inplace(struct ggml_tensor * tensor);\n\nGGML_API void   ggml_cuda_assign_buffers_no_alloc(struct ggml_tensor * tensor);\nGGML_API void   ggml_cuda_assign_scratch_offset(struct ggml_tensor * tensor, size_t offset);\nGGML_API void   ggml_cuda_copy_to_device(struct ggml_tensor * tensor);\nGGML_API void   ggml_cuda_copy_to_host(struct ggml_tensor * tensor);\n\nGGML_API void   ggml_cuda_set_main_device(int main_device);\nGGML_API void   ggml_cuda_set_mul_mat_q(bool mul_mat_q);\nGGML_API void   ggml_cuda_set_scratch_size(size_t scratch_size);\nGGML_API void   ggml_cuda_free_scratch(void);\nGGML_API bool   ggml_cuda_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor);\n\nGGML_API int    ggml_cuda_get_device_count(void);\nGGML_API void   ggml_cuda_get_device_description(int device, char * description, size_t description_size);\nGGML_API size_t ggml_cuda_get_free_memory(int device);\n\nGGML_API void   ggml_cuda_set_device_constants(float sparse_pred_threshold);\n\n// backend API\nGGML_API ggml_backend_t ggml_backend_cuda_init(void); // TODO: take a list of devices to use\n\n#ifdef  __cplusplus\n}\n#endif\n"
        },
        {
          "name": "ggml-impl.h",
          "type": "blob",
          "size": 7.208984375,
          "content": "#pragma once\n\n#include \"ggml.h\"\n\n// GGML internal header\n\n#include <assert.h>\n#include <stddef.h>\n#include <stdbool.h>\n#include <string.h> // memcpy\n#include <math.h>   // fabsf\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n// static_assert should be a #define, but if it's not,\n// fall back to the _Static_assert C11 keyword.\n// if C99 - static_assert is noop\n// ref: https://stackoverflow.com/a/53923785/4039976\n#ifndef static_assert\n#if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 201100L)\n#define static_assert(cond, msg) _Static_assert(cond, msg)\n#else\n#define static_assert(cond, msg) struct global_scope_noop_trick\n#endif\n#endif\n\n// __FMA__ and __F16C__ are not defined in MSVC, however they are implied with AVX2/AVX512\n#if defined(_MSC_VER) && (defined(__AVX2__) || defined(__AVX512F__))\n#ifndef __FMA__\n#define __FMA__\n#endif\n#ifndef __F16C__\n#define __F16C__\n#endif\n#ifndef __SSE3__\n#define __SSE3__\n#endif\n#endif\n\n// 16-bit float\n// on Arm, we use __fp16\n// on x86, we use uint16_t\n#if defined(__ARM_NEON) && !defined(_MSC_VER)\n\n// if YCM cannot find <arm_neon.h>, make a symbolic link to it, for example:\n//\n//   $ ln -sfn /Library/Developer/CommandLineTools/usr/lib/clang/13.1.6/include/arm_neon.h ./src/\n//\n#include <arm_neon.h>\n\n#define GGML_COMPUTE_FP16_TO_FP32(x) ((float) (x))\n#define GGML_COMPUTE_FP32_TO_FP16(x) (x)\n\n#define GGML_FP16_TO_FP32(x) ((float) (x))\n#define GGML_FP32_TO_FP16(x) (x)\n\n#else\n\n#ifdef __wasm_simd128__\n#include <wasm_simd128.h>\n#else\n#ifdef __POWER9_VECTOR__\n#include <altivec.h>\n#undef bool\n#define bool _Bool\n#else\n#if defined(_MSC_VER) || defined(__MINGW32__)\n#include <intrin.h>\n#else\n#if defined(__AVX__) || defined(__AVX2__) || defined(__AVX512F__) || defined(__SSSE3__) || defined(__SSE3__)\n#if !defined(__riscv)\n#include <immintrin.h>\n#endif\n#endif\n#endif\n#endif\n#endif\n\n#ifdef __riscv_v_intrinsic\n#include <riscv_vector.h>\n#endif\n\n#ifdef __F16C__\n\n#ifdef _MSC_VER\n#define GGML_COMPUTE_FP16_TO_FP32(x) _mm_cvtss_f32(_mm_cvtph_ps(_mm_cvtsi32_si128(x)))\n#define GGML_COMPUTE_FP32_TO_FP16(x) _mm_extract_epi16(_mm_cvtps_ph(_mm_set_ss(x), 0), 0)\n#else\n#define GGML_COMPUTE_FP16_TO_FP32(x) _cvtsh_ss(x)\n#define GGML_COMPUTE_FP32_TO_FP16(x) _cvtss_sh(x, 0)\n#endif\n\n#elif defined(__POWER9_VECTOR__)\n\n#define GGML_COMPUTE_FP16_TO_FP32(x) ggml_compute_fp16_to_fp32(x)\n#define GGML_COMPUTE_FP32_TO_FP16(x) ggml_compute_fp32_to_fp16(x)\n/* the inline asm below is about 12% faster than the lookup method */\n#define GGML_FP16_TO_FP32(x) GGML_COMPUTE_FP16_TO_FP32(x)\n#define GGML_FP32_TO_FP16(x) GGML_COMPUTE_FP32_TO_FP16(x)\n\nstatic inline float ggml_compute_fp16_to_fp32(ggml_fp16_t h) {\n    register float f;\n    register double d;\n    __asm__(\n        \"mtfprd %0,%2\\n\"\n        \"xscvhpdp %0,%0\\n\"\n        \"frsp %1,%0\\n\" :\n        /* temp */ \"=d\"(d),\n        /* out */  \"=f\"(f):\n        /* in */   \"r\"(h));\n    return f;\n}\n\nstatic inline ggml_fp16_t ggml_compute_fp32_to_fp16(float f) {\n    register double d;\n    register ggml_fp16_t r;\n    __asm__( /* xscvdphp can work on double or single precision */\n        \"xscvdphp %0,%2\\n\"\n        \"mffprd %1,%0\\n\" :\n        /* temp */ \"=d\"(d),\n        /* out */  \"=r\"(r):\n        /* in */   \"f\"(f));\n    return r;\n}\n\n#else\n\n// FP16 <-> FP32\n// ref: https://github.com/Maratyszcza/FP16\n\nstatic inline float fp32_from_bits(uint32_t w) {\n    union {\n        uint32_t as_bits;\n        float as_value;\n    } fp32;\n    fp32.as_bits = w;\n    return fp32.as_value;\n}\n\nstatic inline uint32_t fp32_to_bits(float f) {\n    union {\n        float as_value;\n        uint32_t as_bits;\n    } fp32;\n    fp32.as_value = f;\n    return fp32.as_bits;\n}\n\nstatic inline float ggml_compute_fp16_to_fp32(ggml_fp16_t h) {\n    const uint32_t w = (uint32_t) h << 16;\n    const uint32_t sign = w & UINT32_C(0x80000000);\n    const uint32_t two_w = w + w;\n\n    const uint32_t exp_offset = UINT32_C(0xE0) << 23;\n#if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) || defined(__GNUC__) && !defined(__STRICT_ANSI__)\n    const float exp_scale = 0x1.0p-112f;\n#else\n    const float exp_scale = fp32_from_bits(UINT32_C(0x7800000));\n#endif\n    const float normalized_value = fp32_from_bits((two_w >> 4) + exp_offset) * exp_scale;\n\n    const uint32_t magic_mask = UINT32_C(126) << 23;\n    const float magic_bias = 0.5f;\n    const float denormalized_value = fp32_from_bits((two_w >> 17) | magic_mask) - magic_bias;\n\n    const uint32_t denormalized_cutoff = UINT32_C(1) << 27;\n    const uint32_t result = sign |\n        (two_w < denormalized_cutoff ? fp32_to_bits(denormalized_value) : fp32_to_bits(normalized_value));\n    return fp32_from_bits(result);\n}\n\nstatic inline ggml_fp16_t ggml_compute_fp32_to_fp16(float f) {\n#if defined(__STDC_VERSION__) && (__STDC_VERSION__ >= 199901L) || defined(__GNUC__) && !defined(__STRICT_ANSI__)\n    const float scale_to_inf = 0x1.0p+112f;\n    const float scale_to_zero = 0x1.0p-110f;\n#else\n    const float scale_to_inf = fp32_from_bits(UINT32_C(0x77800000));\n    const float scale_to_zero = fp32_from_bits(UINT32_C(0x08800000));\n#endif\n    float base = (fabsf(f) * scale_to_inf) * scale_to_zero;\n\n    const uint32_t w = fp32_to_bits(f);\n    const uint32_t shl1_w = w + w;\n    const uint32_t sign = w & UINT32_C(0x80000000);\n    uint32_t bias = shl1_w & UINT32_C(0xFF000000);\n    if (bias < UINT32_C(0x71000000)) {\n        bias = UINT32_C(0x71000000);\n    }\n\n    base = fp32_from_bits((bias >> 1) + UINT32_C(0x07800000)) + base;\n    const uint32_t bits = fp32_to_bits(base);\n    const uint32_t exp_bits = (bits >> 13) & UINT32_C(0x00007C00);\n    const uint32_t mantissa_bits = bits & UINT32_C(0x00000FFF);\n    const uint32_t nonsign = exp_bits + mantissa_bits;\n    return (sign >> 16) | (shl1_w > UINT32_C(0xFF000000) ? UINT16_C(0x7E00) : nonsign);\n}\n\n#define GGML_COMPUTE_FP16_TO_FP32(x) ggml_compute_fp16_to_fp32(x)\n#define GGML_COMPUTE_FP32_TO_FP16(x) ggml_compute_fp32_to_fp16(x)\n\n#endif // __F16C__\n\n#endif // __ARM_NEON\n\n// precomputed f32 table for f16 (256 KB)\n// defined in ggml.c, initialized in ggml_init()\nextern float ggml_table_f32_f16[1 << 16];\n\n// On ARM NEON, it's quicker to directly convert x -> x instead of calling into ggml_lookup_fp16_to_fp32,\n// so we define GGML_FP16_TO_FP32 and GGML_FP32_TO_FP16 elsewhere for NEON.\n// This is also true for POWER9.\n#if !defined(GGML_FP16_TO_FP32) || !defined(GGML_FP32_TO_FP16)\n\ninline static float ggml_lookup_fp16_to_fp32(ggml_fp16_t f) {\n    uint16_t s;\n    memcpy(&s, &f, sizeof(uint16_t));\n    return ggml_table_f32_f16[s];\n}\n\n#define GGML_FP16_TO_FP32(x) ggml_lookup_fp16_to_fp32(x)\n#define GGML_FP32_TO_FP16(x) GGML_COMPUTE_FP32_TO_FP16(x)\n\n#endif\n\n#define GGML_HASHTABLE_FULL ((size_t)-1)\n#define GGML_HASHTABLE_ALREADY_EXISTS ((size_t)-2)\n\nbool   ggml_hash_contains      (const struct ggml_hash_set hash_set, struct ggml_tensor * key);\n\n// returns GGML_HASHTABLE_FULL if table is full, otherwise the current index of the key or where it should be inserted\nsize_t ggml_hash_find          (const struct ggml_hash_set hash_set, struct ggml_tensor * key);\n\n// returns GGML_HAHSHTABLE_ALREADY_EXISTS if key already exists, index otherwise, asserts if table is full\nsize_t ggml_hash_insert        (      struct ggml_hash_set hash_set, struct ggml_tensor * key);\n\n// return index, asserts if table is full\nsize_t ggml_hash_find_or_insert(      struct ggml_hash_set hash_set, struct ggml_tensor * key);\n\n#ifdef __cplusplus\n}\n#endif\n"
        },
        {
          "name": "ggml-metal.h",
          "type": "blob",
          "size": 3.7841796875,
          "content": "// An interface allowing to compute ggml_cgraph with Metal\n//\n// This is a fully functional interface that extends ggml with GPU support for Apple devices.\n// A similar interface can be created for other GPU backends (e.g. Vulkan, CUDA, OpenCL, etc.)\n//\n// How it works?\n//\n// As long as your program can create and evaluate a ggml_cgraph on the CPU, you can use this\n// interface to evaluate the same graph on the GPU. Instead of using ggml_graph_compute(), you\n// use ggml_metal_graph_compute() (or ggml_vulkan_graph_compute(), etc.)\n//\n// You only need to make sure that all memory buffers that you used during the graph creation\n// are mapped to the device memory with the ggml_metal_add_buffer() function. This mapping is\n// used during the graph evaluation to determine the arguments of the compute kernels.\n//\n// Synchronization between device and host memory (for example for input and output tensors)\n// is done with the ggml_metal_set_tensor() and ggml_metal_get_tensor() functions.\n//\n\n#pragma once\n\n#include \"ggml.h\"\n#include \"ggml-backend.h\"\n\n#include <stddef.h>\n#include <stdbool.h>\n\n// max memory buffers that can be mapped to the device\n#define GGML_METAL_MAX_BUFFERS 64\n#define GGML_METAL_MAX_COMMAND_BUFFERS 32\n\nstruct ggml_tensor;\nstruct ggml_cgraph;\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n//\n// internal API\n// temporary exposed to user-code\n//\n\nstruct ggml_metal_context;\n\nvoid ggml_metal_log_set_callback(ggml_log_callback log_callback, void * user_data);\n\n// number of command buffers to use\nstruct ggml_metal_context * ggml_metal_init(int n_cb);\nvoid ggml_metal_free(struct ggml_metal_context * ctx);\n\nvoid * ggml_metal_host_malloc(size_t n);\nvoid   ggml_metal_host_free  (void * data);\n\n// set the number of command buffers to use\nvoid ggml_metal_set_n_cb(struct ggml_metal_context * ctx, int n_cb);\n\n// creates a mapping between a host memory buffer and a device memory buffer\n// - make sure to map all buffers used in the graph before calling ggml_metal_graph_compute\n// - the mapping is used during computation to determine the arguments of the compute kernels\n// - you don't need to keep the host memory buffer allocated as it is never accessed by Metal\n// - max_size specifies the maximum size of a tensor and is used to create shared views such\n//   that it is guaranteed that the tensor will fit in at least one of the views\n//\nbool ggml_metal_add_buffer(\n        struct ggml_metal_context * ctx,\n                       const char * name,\n                             void * data,\n                           size_t   size,\n                           size_t   max_size);\n\n// set data from host memory into the device\nvoid ggml_metal_set_tensor(struct ggml_metal_context * ctx, struct ggml_tensor * t);\n\n// get data from the device into host memory\nvoid ggml_metal_get_tensor(struct ggml_metal_context * ctx, struct ggml_tensor * t);\n\n// try to find operations that can be run concurrently in the graph\n// you should run it again if the topology of your graph changes\nvoid ggml_metal_graph_find_concurrency(struct ggml_metal_context * ctx, struct ggml_cgraph * gf, bool check_mem);\n\n// if the graph has been optimized for concurrently dispatch, return length of the concur_list if optimized\nint ggml_metal_if_optimized(struct ggml_metal_context * ctx);\n\n// output the concur_list for ggml_alloc\nint * ggml_metal_get_concur_list(struct ggml_metal_context * ctx);\n\n// same as ggml_graph_compute but uses Metal\n// creates gf->n_threads command buffers in parallel\nvoid ggml_metal_graph_compute(struct ggml_metal_context * ctx, struct ggml_cgraph * gf);\n\n//\n// backend API\n// user-code should use only these functions\n//\n\nGGML_API ggml_backend_t ggml_backend_metal_init(void);\n\nGGML_API bool ggml_backend_is_metal(ggml_backend_t backend);\n\nGGML_API void ggml_backend_metal_set_n_cb(ggml_backend_t backend, int n_cb);\n\n#ifdef __cplusplus\n}\n#endif\n\n"
        },
        {
          "name": "ggml-metal.m",
          "type": "blob",
          "size": 88.48046875,
          "content": "#import \"ggml-metal.h\"\n\n#import \"ggml-backend-impl.h\"\n#import \"ggml.h\"\n\n#import <Foundation/Foundation.h>\n\n#import <Metal/Metal.h>\n\n#undef MIN\n#undef MAX\n#define MIN(a, b) ((a) < (b) ? (a) : (b))\n#define MAX(a, b) ((a) > (b) ? (a) : (b))\n\n#ifdef GGML_METAL_NDEBUG\n#define GGML_METAL_LOG_INFO(...)\n#define GGML_METAL_LOG_WARN(...)\n#define GGML_METAL_LOG_ERROR(...)\n#else\n#define GGML_METAL_LOG_INFO(...)  ggml_metal_log(GGML_LOG_LEVEL_INFO, __VA_ARGS__)\n#define GGML_METAL_LOG_WARN(...)  ggml_metal_log(GGML_LOG_LEVEL_WARN, __VA_ARGS__)\n#define GGML_METAL_LOG_ERROR(...) ggml_metal_log(GGML_LOG_LEVEL_ERROR, __VA_ARGS__)\n#endif\n\n#define UNUSED(x) (void)(x)\n\n#define GGML_MAX_CONCUR (2*GGML_DEFAULT_GRAPH_SIZE)\n\nstruct ggml_metal_buffer {\n    const char * name;\n\n    void   * data;\n    size_t   size;\n\n    id<MTLBuffer> metal;\n};\n\nstruct ggml_metal_context {\n    int n_cb;\n\n    id<MTLDevice>       device;\n    id<MTLCommandQueue> queue;\n    id<MTLLibrary>      library;\n\n    id<MTLCommandBuffer>         command_buffers [GGML_METAL_MAX_COMMAND_BUFFERS];\n    id<MTLComputeCommandEncoder> command_encoders[GGML_METAL_MAX_COMMAND_BUFFERS];\n\n    dispatch_queue_t d_queue;\n\n    int n_buffers;\n    struct ggml_metal_buffer buffers[GGML_METAL_MAX_BUFFERS];\n\n    int concur_list[GGML_MAX_CONCUR];\n    int concur_list_len;\n\n    // custom kernels\n#define GGML_METAL_DECL_KERNEL(name) \\\n    id<MTLFunction>             function_##name; \\\n    id<MTLComputePipelineState> pipeline_##name\n\n    GGML_METAL_DECL_KERNEL(add);\n    GGML_METAL_DECL_KERNEL(add_row); // TODO: avoid this extra kernel, instead extend the \"add\" kernel to support broadcast\n    GGML_METAL_DECL_KERNEL(mul);\n    GGML_METAL_DECL_KERNEL(mul_row); // TODO: avoid this extra kernel, instead extend the \"mul\" kernel to support broadcast\n    GGML_METAL_DECL_KERNEL(scale);\n    GGML_METAL_DECL_KERNEL(scale_4);\n    GGML_METAL_DECL_KERNEL(silu);\n    GGML_METAL_DECL_KERNEL(relu);\n    GGML_METAL_DECL_KERNEL(gelu);\n    GGML_METAL_DECL_KERNEL(soft_max);\n    GGML_METAL_DECL_KERNEL(soft_max_4);\n    GGML_METAL_DECL_KERNEL(diag_mask_inf);\n    GGML_METAL_DECL_KERNEL(diag_mask_inf_8);\n    GGML_METAL_DECL_KERNEL(get_rows_f32);\n    GGML_METAL_DECL_KERNEL(get_rows_f16);\n    GGML_METAL_DECL_KERNEL(get_rows_q4_0);\n    GGML_METAL_DECL_KERNEL(get_rows_q4_1);\n    GGML_METAL_DECL_KERNEL(get_rows_q5_0);\n    GGML_METAL_DECL_KERNEL(get_rows_q5_1);\n    GGML_METAL_DECL_KERNEL(get_rows_q8_0);\n    GGML_METAL_DECL_KERNEL(get_rows_q2_K);\n    GGML_METAL_DECL_KERNEL(get_rows_q3_K);\n    GGML_METAL_DECL_KERNEL(get_rows_q4_K);\n    GGML_METAL_DECL_KERNEL(get_rows_q5_K);\n    GGML_METAL_DECL_KERNEL(get_rows_q6_K);\n    GGML_METAL_DECL_KERNEL(rms_norm);\n    GGML_METAL_DECL_KERNEL(norm);\n    GGML_METAL_DECL_KERNEL(mul_mv_f32_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_f16_f16);\n    GGML_METAL_DECL_KERNEL(mul_mv_f16_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_f16_f32_1row);\n    GGML_METAL_DECL_KERNEL(mul_mv_f16_f32_l4);\n    GGML_METAL_DECL_KERNEL(mul_mv_q4_0_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q4_1_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q5_0_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q5_1_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q8_0_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q2_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q3_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q4_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q5_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mv_q6_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_f32_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_f16_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q4_0_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q4_1_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q5_0_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q5_1_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q8_0_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q2_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q3_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q4_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q5_K_f32);\n    GGML_METAL_DECL_KERNEL(mul_mm_q6_K_f32);\n    GGML_METAL_DECL_KERNEL(rope_f32);\n    GGML_METAL_DECL_KERNEL(rope_f16);\n    GGML_METAL_DECL_KERNEL(alibi_f32);\n    GGML_METAL_DECL_KERNEL(im2col_f16);\n    GGML_METAL_DECL_KERNEL(cpy_f32_f16);\n    GGML_METAL_DECL_KERNEL(cpy_f32_f32);\n    GGML_METAL_DECL_KERNEL(cpy_f16_f16);\n    GGML_METAL_DECL_KERNEL(concat);\n    GGML_METAL_DECL_KERNEL(sqr);\n\n#undef GGML_METAL_DECL_KERNEL\n};\n\n// MSL code\n// TODO: move the contents here when ready\n//       for now it is easier to work in a separate file\n//static NSString * const msl_library_source = @\"see metal.metal\";\n\n// Here to assist with NSBundle Path Hack\n@interface GGMLMetalClass : NSObject\n@end\n@implementation GGMLMetalClass\n@end\n\nggml_log_callback ggml_metal_log_callback = NULL;\nvoid * ggml_metal_log_user_data = NULL;\n\nvoid ggml_metal_log_set_callback(ggml_log_callback log_callback, void * user_data) {\n    ggml_metal_log_callback  = log_callback;\n    ggml_metal_log_user_data = user_data;\n}\n\nGGML_ATTRIBUTE_FORMAT(2, 3)\nstatic void ggml_metal_log(enum ggml_log_level level, const char * format, ...){\n    if (ggml_metal_log_callback != NULL) {\n        va_list args;\n        va_start(args, format);\n        char buffer[128];\n        int len = vsnprintf(buffer, 128, format, args);\n        if (len < 128) {\n            ggml_metal_log_callback(level, buffer, ggml_metal_log_user_data);\n        } else {\n            char* buffer2 = malloc(len+1);\n            vsnprintf(buffer2, len+1, format, args);\n            buffer2[len] = 0;\n            ggml_metal_log_callback(level, buffer2, ggml_metal_log_user_data);\n            free(buffer2);\n        }\n        va_end(args);\n    }\n}\n\n\n\nstruct ggml_metal_context * ggml_metal_init(int n_cb) {\n    GGML_METAL_LOG_INFO(\"%s: allocating\\n\", __func__);\n\n    id <MTLDevice> device;\n    NSString * s;\n\n#if TARGET_OS_OSX\n    // Show all the Metal device instances in the system\n    NSArray * devices = MTLCopyAllDevices();\n    for (device in devices) {\n        s = [device name];\n        GGML_METAL_LOG_INFO(\"%s: found device: %s\\n\", __func__, [s UTF8String]);\n    }\n#endif\n\n    // Pick and show default Metal device\n    device = MTLCreateSystemDefaultDevice();\n    s = [device name];\n    GGML_METAL_LOG_INFO(\"%s: picking default device: %s\\n\", __func__, [s UTF8String]);\n\n    // Configure context\n    struct ggml_metal_context * ctx = malloc(sizeof(struct ggml_metal_context));\n    ctx->device = device;\n    ctx->n_cb   = MIN(n_cb, GGML_METAL_MAX_BUFFERS);\n    ctx->queue  = [ctx->device newCommandQueue];\n    ctx->n_buffers = 0;\n    ctx->concur_list_len = 0;\n\n    ctx->d_queue = dispatch_queue_create(\"ggml-metal\", DISPATCH_QUEUE_CONCURRENT);\n\n    // load library\n    {\n        NSBundle * bundle = nil;\n#ifdef SWIFT_PACKAGE\n        bundle = SWIFTPM_MODULE_BUNDLE;\n#else\n        bundle = [NSBundle bundleForClass:[GGMLMetalClass class]];\n#endif\n        NSError * error = nil;\n        NSString * libPath = [bundle pathForResource:@\"default\" ofType:@\"metallib\"];\n        if (libPath != nil) {\n            NSURL * libURL = [NSURL fileURLWithPath:libPath];\n            GGML_METAL_LOG_INFO(\"%s: loading '%s'\\n\", __func__, [libPath UTF8String]);\n            ctx->library = [ctx->device newLibraryWithURL:libURL error:&error];\n        } else {\n            GGML_METAL_LOG_INFO(\"%s: default.metallib not found, loading from source\\n\", __func__);\n\n            NSString * sourcePath;\n            NSString * ggmlMetalPathResources = [[NSProcessInfo processInfo].environment objectForKey:@\"GGML_METAL_PATH_RESOURCES\"];\n            if (ggmlMetalPathResources) {\n                sourcePath = [ggmlMetalPathResources stringByAppendingPathComponent:@\"ggml-metal.metal\"];\n            } else {\n                sourcePath = [bundle pathForResource:@\"ggml-metal\" ofType:@\"metal\"];\n            }\n            if (sourcePath == nil) {\n                GGML_METAL_LOG_WARN(\"%s: error: could not use bundle path to find ggml-metal.metal, falling back to trying cwd\\n\", __func__);\n                sourcePath = @\"ggml-metal.metal\";\n            }\n            GGML_METAL_LOG_INFO(\"%s: loading '%s'\\n\", __func__, [sourcePath UTF8String]);\n            NSString * src = [NSString stringWithContentsOfFile:sourcePath encoding:NSUTF8StringEncoding error:&error];\n            if (error) {\n                GGML_METAL_LOG_ERROR(\"%s: error: %s\\n\", __func__, [[error description] UTF8String]);\n                return NULL;\n            }\n\n            MTLCompileOptions* options = nil;\n#ifdef GGML_QKK_64\n            options = [MTLCompileOptions new];\n            options.preprocessorMacros = @{ @\"QK_K\" : @(64) };\n#endif\n            ctx->library = [ctx->device newLibraryWithSource:src options:options error:&error];\n        }\n\n        if (error) {\n            GGML_METAL_LOG_ERROR(\"%s: error: %s\\n\", __func__, [[error description] UTF8String]);\n            return NULL;\n        }\n    }\n\n    // load kernels\n    {\n        NSError * error = nil;\n\n        /*\n        GGML_METAL_LOG_INFO(\"%s: loaded %-32s %16p | th_max = %4d | th_width = %4d\\n\", __func__, \"kernel_\"#name, (void *) ctx->pipeline_##name, \\\n                (int) ctx->pipeline_##name.maxTotalThreadsPerThreadgroup, \\\n                (int) ctx->pipeline_##name.threadExecutionWidth); \\\n        */\n#define GGML_METAL_ADD_KERNEL(name) \\\n        ctx->function_##name = [ctx->library newFunctionWithName:@\"kernel_\"#name]; \\\n        ctx->pipeline_##name = [ctx->device newComputePipelineStateWithFunction:ctx->function_##name error:&error]; \\\n        if (error) { \\\n            GGML_METAL_LOG_ERROR(\"%s: error: load pipeline error: %s\\n\", __func__, [[error description] UTF8String]); \\\n            return NULL; \\\n        }\n\n        GGML_METAL_ADD_KERNEL(add);\n        GGML_METAL_ADD_KERNEL(add_row);\n        GGML_METAL_ADD_KERNEL(mul);\n        GGML_METAL_ADD_KERNEL(mul_row);\n        GGML_METAL_ADD_KERNEL(scale);\n        GGML_METAL_ADD_KERNEL(scale_4);\n        GGML_METAL_ADD_KERNEL(silu);\n        GGML_METAL_ADD_KERNEL(relu);\n        GGML_METAL_ADD_KERNEL(gelu);\n        GGML_METAL_ADD_KERNEL(soft_max);\n        GGML_METAL_ADD_KERNEL(soft_max_4);\n        GGML_METAL_ADD_KERNEL(diag_mask_inf);\n        GGML_METAL_ADD_KERNEL(diag_mask_inf_8);\n        GGML_METAL_ADD_KERNEL(get_rows_f32);\n        GGML_METAL_ADD_KERNEL(get_rows_f16);\n        GGML_METAL_ADD_KERNEL(get_rows_q4_0);\n        GGML_METAL_ADD_KERNEL(get_rows_q4_1);\n        GGML_METAL_ADD_KERNEL(get_rows_q5_0);\n        GGML_METAL_ADD_KERNEL(get_rows_q5_1);\n        GGML_METAL_ADD_KERNEL(get_rows_q8_0);\n        GGML_METAL_ADD_KERNEL(get_rows_q2_K);\n        GGML_METAL_ADD_KERNEL(get_rows_q3_K);\n        GGML_METAL_ADD_KERNEL(get_rows_q4_K);\n        GGML_METAL_ADD_KERNEL(get_rows_q5_K);\n        GGML_METAL_ADD_KERNEL(get_rows_q6_K);\n        GGML_METAL_ADD_KERNEL(rms_norm);\n        GGML_METAL_ADD_KERNEL(norm);\n        GGML_METAL_ADD_KERNEL(mul_mv_f32_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_f16_f16);\n        GGML_METAL_ADD_KERNEL(mul_mv_f16_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_f16_f32_1row);\n        GGML_METAL_ADD_KERNEL(mul_mv_f16_f32_l4);\n        GGML_METAL_ADD_KERNEL(mul_mv_q4_0_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q4_1_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q5_0_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q5_1_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q8_0_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q2_K_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q3_K_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q4_K_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q5_K_f32);\n        GGML_METAL_ADD_KERNEL(mul_mv_q6_K_f32);\n        if ([ctx->device supportsFamily:MTLGPUFamilyApple7]) {\n            GGML_METAL_ADD_KERNEL(mul_mm_f32_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_f16_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q4_0_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q4_1_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q5_0_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q5_1_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q8_0_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q2_K_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q3_K_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q4_K_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q5_K_f32);\n            GGML_METAL_ADD_KERNEL(mul_mm_q6_K_f32);\n        }\n        GGML_METAL_ADD_KERNEL(rope_f32);\n        GGML_METAL_ADD_KERNEL(rope_f16);\n        GGML_METAL_ADD_KERNEL(alibi_f32);\n        GGML_METAL_ADD_KERNEL(im2col_f16);\n        GGML_METAL_ADD_KERNEL(cpy_f32_f16);\n        GGML_METAL_ADD_KERNEL(cpy_f32_f32);\n        GGML_METAL_ADD_KERNEL(cpy_f16_f16);\n        GGML_METAL_ADD_KERNEL(concat);\n        GGML_METAL_ADD_KERNEL(sqr);\n\n#undef GGML_METAL_ADD_KERNEL\n    }\n\n#if TARGET_OS_OSX\n    // print MTL GPU family:\n    GGML_METAL_LOG_INFO(\"%s: GPU name:   %s\\n\", __func__, [[ctx->device name] UTF8String]);\n\n    // determine max supported GPU family\n    // https://developer.apple.com/metal/Metal-Shading-Language-Specification.pdf\n    // https://developer.apple.com/metal/Metal-Feature-Set-Tables.pdf\n    for (int i = MTLGPUFamilyApple1 + 20; i >= MTLGPUFamilyApple1; --i) {\n        if ([ctx->device supportsFamily:i]) {\n            GGML_METAL_LOG_INFO(\"%s: GPU family: MTLGPUFamilyApple%d (%d)\\n\", __func__, i - (int) MTLGPUFamilyApple1 + 1, i);\n            break;\n        }\n    }\n\n    GGML_METAL_LOG_INFO(\"%s: hasUnifiedMemory              = %s\\n\",       __func__, ctx->device.hasUnifiedMemory ? \"true\" : \"false\");\n    GGML_METAL_LOG_INFO(\"%s: recommendedMaxWorkingSetSize  = %8.2f MB\\n\", __func__, ctx->device.recommendedMaxWorkingSetSize / 1024.0 / 1024.0);\n    if (ctx->device.maxTransferRate != 0) {\n        GGML_METAL_LOG_INFO(\"%s: maxTransferRate               = %8.2f MB/s\\n\", __func__, ctx->device.maxTransferRate / 1024.0 / 1024.0);\n    } else {\n        GGML_METAL_LOG_INFO(\"%s: maxTransferRate               = built-in GPU\\n\", __func__);\n    }\n#endif\n\n    return ctx;\n}\n\nvoid ggml_metal_free(struct ggml_metal_context * ctx) {\n    GGML_METAL_LOG_INFO(\"%s: deallocating\\n\", __func__);\n#define GGML_METAL_DEL_KERNEL(name) \\\n    [ctx->function_##name release]; \\\n    [ctx->pipeline_##name release];\n\n    GGML_METAL_DEL_KERNEL(add);\n    GGML_METAL_DEL_KERNEL(add_row);\n    GGML_METAL_DEL_KERNEL(mul);\n    GGML_METAL_DEL_KERNEL(mul_row);\n    GGML_METAL_DEL_KERNEL(scale);\n    GGML_METAL_DEL_KERNEL(scale_4);\n    GGML_METAL_DEL_KERNEL(silu);\n    GGML_METAL_DEL_KERNEL(relu);\n    GGML_METAL_DEL_KERNEL(gelu);\n    GGML_METAL_DEL_KERNEL(soft_max);\n    GGML_METAL_DEL_KERNEL(soft_max_4);\n    GGML_METAL_DEL_KERNEL(diag_mask_inf);\n    GGML_METAL_DEL_KERNEL(diag_mask_inf_8);\n    GGML_METAL_DEL_KERNEL(get_rows_f32);\n    GGML_METAL_DEL_KERNEL(get_rows_f16);\n    GGML_METAL_DEL_KERNEL(get_rows_q4_0);\n    GGML_METAL_DEL_KERNEL(get_rows_q4_1);\n    GGML_METAL_DEL_KERNEL(get_rows_q5_0);\n    GGML_METAL_DEL_KERNEL(get_rows_q5_1);\n    GGML_METAL_DEL_KERNEL(get_rows_q8_0);\n    GGML_METAL_DEL_KERNEL(get_rows_q2_K);\n    GGML_METAL_DEL_KERNEL(get_rows_q3_K);\n    GGML_METAL_DEL_KERNEL(get_rows_q4_K);\n    GGML_METAL_DEL_KERNEL(get_rows_q5_K);\n    GGML_METAL_DEL_KERNEL(get_rows_q6_K);\n    GGML_METAL_DEL_KERNEL(rms_norm);\n    GGML_METAL_DEL_KERNEL(norm);\n    GGML_METAL_DEL_KERNEL(mul_mv_f32_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_f16_f16);\n    GGML_METAL_DEL_KERNEL(mul_mv_f16_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_f16_f32_1row);\n    GGML_METAL_DEL_KERNEL(mul_mv_f16_f32_l4);\n    GGML_METAL_DEL_KERNEL(mul_mv_q4_0_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q4_1_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q5_0_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q5_1_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q8_0_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q2_K_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q3_K_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q4_K_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q5_K_f32);\n    GGML_METAL_DEL_KERNEL(mul_mv_q6_K_f32);\n    if ([ctx->device supportsFamily:MTLGPUFamilyApple7]) {\n        GGML_METAL_DEL_KERNEL(mul_mm_f32_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_f16_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q4_0_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q4_1_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q5_0_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q5_1_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q8_0_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q2_K_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q3_K_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q4_K_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q5_K_f32);\n        GGML_METAL_DEL_KERNEL(mul_mm_q6_K_f32);\n    }\n    GGML_METAL_DEL_KERNEL(rope_f32);\n    GGML_METAL_DEL_KERNEL(rope_f16);\n    GGML_METAL_DEL_KERNEL(alibi_f32);\n    GGML_METAL_DEL_KERNEL(im2col_f16);\n    GGML_METAL_DEL_KERNEL(cpy_f32_f16);\n    GGML_METAL_DEL_KERNEL(cpy_f32_f32);\n    GGML_METAL_DEL_KERNEL(cpy_f16_f16);\n    GGML_METAL_DEL_KERNEL(concat);\n    GGML_METAL_DEL_KERNEL(sqr);\n\n#undef GGML_METAL_DEL_KERNEL\n\n    for (int i = 0; i < ctx->n_buffers; ++i) {\n        [ctx->buffers[i].metal release];\n    }\n\n    [ctx->library release];\n    [ctx->queue release];\n    [ctx->device release];\n\n    dispatch_release(ctx->d_queue);\n\n    free(ctx);\n}\n\nvoid * ggml_metal_host_malloc(size_t n) {\n    void * data = NULL;\n    const int result = posix_memalign((void **) &data, sysconf(_SC_PAGESIZE), n);\n    if (result != 0) {\n        GGML_METAL_LOG_ERROR(\"%s: error: posix_memalign failed\\n\", __func__);\n        return NULL;\n    }\n\n    return data;\n}\n\nvoid ggml_metal_host_free(void * data) {\n    free(data);\n}\n\nvoid ggml_metal_set_n_cb(struct ggml_metal_context * ctx, int n_cb) {\n    ctx->n_cb = MIN(n_cb, GGML_METAL_MAX_BUFFERS);\n}\n\nint ggml_metal_if_optimized(struct ggml_metal_context * ctx) {\n    return ctx->concur_list_len;\n}\n\nint * ggml_metal_get_concur_list(struct ggml_metal_context * ctx) {\n    return ctx->concur_list;\n}\n\n// finds the Metal buffer that contains the tensor data on the GPU device\n// the assumption is that there is 1-to-1 mapping between the host and device memory buffers, so we can find the\n// Metal buffer based on the host memory pointer\n//\nstatic id<MTLBuffer> ggml_metal_get_buffer(struct ggml_metal_context * ctx, struct ggml_tensor * t, size_t * offs) {\n    //GGML_METAL_LOG_INFO(\"%s: data tensor '%16s', offs_data = %8ld, offs_eval = %8ld, offs_cach = %8ld\\n\", __func__, t->name, offs_data, offs_eval, offs_cach);\n\n    const int64_t tsize = ggml_nbytes(t);\n\n    if (t->buffer && t->buffer->backend && t->buffer->backend->context) {\n        ctx = t->buffer->backend->context;\n    }\n\n    // find the view that contains the tensor fully\n    for (int i = 0; i < ctx->n_buffers; ++i) {\n        const int64_t ioffs = (int64_t) t->data - (int64_t) ctx->buffers[i].data;\n\n        //GGML_METAL_LOG_INFO(\"ioffs = %10ld, tsize = %10ld, sum = %10ld, ctx->buffers[%d].size = %10ld, name = %s\\n\", ioffs, tsize, ioffs + tsize, i, ctx->buffers[i].size, ctx->buffers[i].name);\n        if (ioffs >= 0 && ioffs + tsize <= (int64_t) ctx->buffers[i].size) {\n            *offs = (size_t) ioffs;\n\n            //GGML_METAL_LOG_INFO(\"%s: '%s' tensor '%16s', offs = %8ld\\n\", __func__, ctx->buffers[i].name, t->name, *offs);\n\n            return ctx->buffers[i].metal;\n        }\n    }\n\n    GGML_METAL_LOG_ERROR(\"%s: error: buffer is nil\\n\", __func__);\n\n    return nil;\n}\n\nbool ggml_metal_add_buffer(\n        struct ggml_metal_context * ctx,\n                     const char * name,\n                           void * data,\n                         size_t   size,\n                         size_t   max_size) {\n    if (ctx->n_buffers >= GGML_METAL_MAX_BUFFERS) {\n        GGML_METAL_LOG_ERROR(\"%s: error: too many buffers\\n\", __func__);\n        return false;\n    }\n\n    if (data) {\n        // verify that the buffer does not overlap with any of the existing buffers\n        for (int i = 0; i < ctx->n_buffers; ++i) {\n            const int64_t ioffs = (int64_t) data - (int64_t) ctx->buffers[i].data;\n\n            if (ioffs >= 0 && ioffs < (int64_t) ctx->buffers[i].size) {\n                GGML_METAL_LOG_ERROR(\"%s: error: buffer '%s' overlaps with '%s'\\n\", __func__, name, ctx->buffers[i].name);\n                return false;\n            }\n        }\n\n        const size_t size_page = sysconf(_SC_PAGESIZE);\n\n        size_t size_aligned = size;\n        if ((size_aligned % size_page) != 0) {\n            size_aligned += (size_page - (size_aligned % size_page));\n        }\n\n        // the buffer fits into the max buffer size allowed by the device\n        if (size_aligned <= ctx->device.maxBufferLength) {\n            ctx->buffers[ctx->n_buffers].name = name;\n            ctx->buffers[ctx->n_buffers].data = data;\n            ctx->buffers[ctx->n_buffers].size = size;\n\n            ctx->buffers[ctx->n_buffers].metal = [ctx->device newBufferWithBytesNoCopy:data length:size_aligned options:MTLResourceStorageModeShared deallocator:nil];\n\n            if (ctx->buffers[ctx->n_buffers].metal == nil) {\n                GGML_METAL_LOG_ERROR(\"%s: error: failed to allocate '%-16s' buffer, size = %8.2f MB\\n\", __func__, name, size_aligned / 1024.0 / 1024.0);\n                return false;\n            }\n\n            GGML_METAL_LOG_INFO(\"%s: allocated '%-16s' buffer, size = %8.2f MB\", __func__, name, size_aligned / 1024.0 / 1024.0);\n\n            ++ctx->n_buffers;\n        } else {\n            // this overlap between the views will guarantee that the tensor with the maximum size will fully fit into\n            // one of the views\n            const size_t size_ovlp = ((max_size + size_page - 1) / size_page + 1) * size_page; // round-up 2 pages just in case\n            const size_t size_step = ctx->device.maxBufferLength - size_ovlp;\n            const size_t size_view = ctx->device.maxBufferLength;\n\n            for (size_t i = 0; i < size; i += size_step) {\n                const size_t size_step_aligned = (i + size_view <= size) ? size_view : (size_aligned - i);\n\n                ctx->buffers[ctx->n_buffers].name = name;\n                ctx->buffers[ctx->n_buffers].data = (void *) ((uint8_t *) data + i);\n                ctx->buffers[ctx->n_buffers].size = size_step_aligned;\n\n                ctx->buffers[ctx->n_buffers].metal = [ctx->device newBufferWithBytesNoCopy:(void *) ((uint8_t *) data + i) length:size_step_aligned options:MTLResourceStorageModeShared deallocator:nil];\n\n                if (ctx->buffers[ctx->n_buffers].metal == nil) {\n                    GGML_METAL_LOG_ERROR(\"%s: error: failed to allocate '%-16s' buffer, size = %8.2f MB\\n\", __func__, name, size_step_aligned / 1024.0 / 1024.0);\n                    return false;\n                }\n\n                GGML_METAL_LOG_INFO(\"%s: allocated '%-16s' buffer, size = %8.2f MB, offs = %12ld\", __func__, name, size_step_aligned / 1024.0 / 1024.0, i);\n                if (i + size_step < size) {\n                    GGML_METAL_LOG_INFO(\"\\n\");\n                }\n\n                ++ctx->n_buffers;\n            }\n        }\n\n#if TARGET_OS_OSX\n        GGML_METAL_LOG_INFO(\", (%8.2f / %8.2f)\",\n                ctx->device.currentAllocatedSize / 1024.0 / 1024.0,\n                ctx->device.recommendedMaxWorkingSetSize / 1024.0 / 1024.0);\n\n        if (ctx->device.currentAllocatedSize > ctx->device.recommendedMaxWorkingSetSize) {\n            GGML_METAL_LOG_WARN(\"%s: warning: current allocated size is greater than the recommended max working set size\\n\", __func__);\n        } else {\n            GGML_METAL_LOG_INFO(\"\\n\");\n        }\n#else\n        GGML_METAL_LOG_INFO(\", (%8.2f)\\n\", ctx->device.currentAllocatedSize / 1024.0 / 1024.0);\n#endif\n    }\n\n    return true;\n}\n\nvoid ggml_metal_set_tensor(\n        struct ggml_metal_context * ctx,\n        struct ggml_tensor * t) {\n    size_t offs;\n    id<MTLBuffer> id_dst = ggml_metal_get_buffer(ctx, t, &offs);\n\n    memcpy((void *) ((uint8_t *) id_dst.contents + offs), t->data, ggml_nbytes(t));\n}\n\nvoid ggml_metal_get_tensor(\n        struct ggml_metal_context * ctx,\n        struct ggml_tensor * t) {\n    size_t offs;\n    id<MTLBuffer> id_src = ggml_metal_get_buffer(ctx, t, &offs);\n\n    memcpy(t->data, (void *) ((uint8_t *) id_src.contents + offs), ggml_nbytes(t));\n}\n\nvoid ggml_metal_graph_find_concurrency(\n        struct ggml_metal_context * ctx,\n        struct ggml_cgraph * gf, bool check_mem) {\n    int search_depth = gf->n_nodes; //we only find concurrency in this range to avoid wasting too much time\n    int nodes_unused[GGML_MAX_CONCUR];\n\n    for (int i = 0; i < GGML_MAX_CONCUR; i++) { ctx->concur_list[i] = 0; }\n    for (int i = 0; i < gf->n_nodes;     i++) { nodes_unused[i]     = 1; }\n    ctx->concur_list_len = 0;\n\n    int n_left    = gf->n_nodes;\n    int n_start   = 0; // all nodes before n_start at nodes_unused array have been sorted and store back to ctx->concur_list\n    int level_pos = 0; // at ctx->concur_list, the last layer (level) ends at level_pos\n\n    while (n_left > 0) {\n        // number of nodes at a layer (that can be issued concurrently)\n        int concurrency = 0;\n        for (int i = n_start; i < ((n_start + search_depth > gf->n_nodes) ? gf->n_nodes : n_start + search_depth); i++) {\n            if (nodes_unused[i]) {\n                // if the requirements for gf->nodes[i] are satisfied\n                int exe_flag = 1;\n\n                // scan all srcs\n                for (int src_ind = 0; src_ind < GGML_MAX_SRC; src_ind++) {\n                    struct ggml_tensor * src_cur = gf->nodes[i]->src[src_ind];\n                    if (src_cur) {\n                        // if is leaf nodes it's satisfied.\n                        // TODO: ggml_is_leaf()\n                        if (src_cur->op == GGML_OP_NONE && src_cur->grad == NULL) {\n                            continue;\n                        }\n\n                        // otherwise this src should be the output from previous nodes.\n                        int is_found = 0;\n\n                        // scan 2*search_depth back because we inserted barrier.\n                        //for (int j = ((level_pos - 2*search_depth) < 0 ? 0 : (level_pos - 2*search_depth)); j < level_pos; j++) {\n                        for (int j = MAX(0, level_pos - 2*search_depth); j < level_pos; j++) {\n                            if (ctx->concur_list[j] >= 0 && gf->nodes[ctx->concur_list[j]] == src_cur) {\n                                is_found = 1;\n                                break;\n                            }\n                        }\n                        if (is_found == 0) {\n                            exe_flag = 0;\n                            break;\n                        }\n                    }\n                }\n                if (exe_flag && check_mem) {\n                    // check if nodes[i]'s data will be overwritten by a node before nodes[i].\n                    // if node[5] and node[3] write to the same memory region, then we can't issue node[5] before node[3]\n                    int64_t data_start = (int64_t) gf->nodes[i]->data;\n                    int64_t length     = (int64_t) ggml_nbytes(gf->nodes[i]);\n                    for (int j = n_start; j < i; j++) {\n                        if (nodes_unused[j] && gf->nodes[j]->op != GGML_OP_RESHAPE \\\n                                            && gf->nodes[j]->op != GGML_OP_VIEW \\\n                                            && gf->nodes[j]->op != GGML_OP_TRANSPOSE \\\n                                            && gf->nodes[j]->op != GGML_OP_PERMUTE) {\n                            if (((int64_t)gf->nodes[j]->data) >= data_start + length || \\\n                                ((int64_t)gf->nodes[j]->data) + (int64_t) ggml_nbytes(gf->nodes[j]) <= data_start) {\n                                continue;\n                            }\n\n                            exe_flag = 0;\n                        }\n                    }\n                }\n                if (exe_flag) {\n                    ctx->concur_list[level_pos + concurrency] = i;\n                    nodes_unused[i] = 0;\n                    concurrency++;\n                    ctx->concur_list_len++;\n                }\n            }\n        }\n        n_left -= concurrency;\n        // adding a barrier different layer\n        ctx->concur_list[level_pos + concurrency] = -1;\n        ctx->concur_list_len++;\n        // jump all sorted nodes at nodes_bak\n        while (!nodes_unused[n_start]) {\n            n_start++;\n        }\n        level_pos += concurrency + 1;\n    }\n\n    if (ctx->concur_list_len > GGML_MAX_CONCUR) {\n        GGML_METAL_LOG_WARN(\"%s: too many elements for metal ctx->concur_list!\\n\", __func__);\n    }\n}\n\nvoid ggml_metal_graph_compute(\n        struct ggml_metal_context * ctx,\n               struct ggml_cgraph * gf) {\n    @autoreleasepool {\n\n    // if there is ctx->concur_list, dispatch concurrently\n    // else fallback to serial dispatch\n    MTLComputePassDescriptor * edesc = MTLComputePassDescriptor.computePassDescriptor;\n\n    const bool has_concur = ctx->concur_list_len && ctx->concur_list_len <= GGML_MAX_CONCUR;\n\n    const int n_nodes  = has_concur ? ctx->concur_list_len      : gf->n_nodes;\n    edesc.dispatchType = has_concur ? MTLDispatchTypeConcurrent : MTLDispatchTypeSerial;\n\n    // create multiple command buffers and enqueue them\n    // then, we encode the graph into the command buffers in parallel\n\n    const int n_cb = ctx->n_cb;\n\n    for (int i = 0; i < n_cb; ++i) {\n        ctx->command_buffers[i] = [ctx->queue commandBuffer];\n\n        // enqueue the command buffers in order to specify their execution order\n        [ctx->command_buffers[i] enqueue];\n\n        ctx->command_encoders[i] = [ctx->command_buffers[i] computeCommandEncoderWithDescriptor: edesc];\n    }\n\n    for (int cb_idx = 0; cb_idx < n_cb; ++cb_idx) {\n        const int n_nodes_per_cb = (n_nodes + n_cb - 1) / n_cb;\n\n        dispatch_async(ctx->d_queue, ^{\n            size_t offs_src0 = 0;\n            size_t offs_src1 = 0;\n            size_t offs_dst  = 0;\n\n            id<MTLCommandBuffer> command_buffer  = ctx->command_buffers[cb_idx];\n            id<MTLComputeCommandEncoder> encoder = ctx->command_encoders[cb_idx];\n\n            const int node_start =                                      (cb_idx + 0) * n_nodes_per_cb;\n            const int node_end   = MIN((cb_idx == n_cb - 1) ? n_nodes : (cb_idx + 1) * n_nodes_per_cb, n_nodes);\n\n            for (int ind = node_start; ind < node_end; ++ind) {\n                const int i = has_concur ? ctx->concur_list[ind] : ind;\n\n                if (i == -1) {\n                    [encoder memoryBarrierWithScope:MTLBarrierScopeBuffers];\n                    continue;\n                }\n\n                //GGML_METAL_LOG_INFO(\"%s: encoding node %3d, op = %8s\\n\", __func__, i, ggml_op_name(gf->nodes[i]->op));\n\n                struct ggml_tensor * src0 = gf->nodes[i]->src[0];\n                struct ggml_tensor * src1 = gf->nodes[i]->src[1];\n                struct ggml_tensor * dst  = gf->nodes[i];\n\n                switch (dst->op) {\n                    case GGML_OP_NONE:\n                    case GGML_OP_RESHAPE:\n                    case GGML_OP_VIEW:\n                    case GGML_OP_TRANSPOSE:\n                    case GGML_OP_PERMUTE:\n                        {\n                            // noop -> next node\n                        } continue;\n                    default:\n                        {\n                        } break;\n                }\n\n                const int64_t  ne00 = src0 ? src0->ne[0] : 0;\n                const int64_t  ne01 = src0 ? src0->ne[1] : 0;\n                const int64_t  ne02 = src0 ? src0->ne[2] : 0;\n                const int64_t  ne03 = src0 ? src0->ne[3] : 0;\n\n                const uint64_t nb00 = src0 ? src0->nb[0] : 0;\n                const uint64_t nb01 = src0 ? src0->nb[1] : 0;\n                const uint64_t nb02 = src0 ? src0->nb[2] : 0;\n                const uint64_t nb03 = src0 ? src0->nb[3] : 0;\n\n                const int64_t  ne10 = src1 ? src1->ne[0] : 0;\n                const int64_t  ne11 = src1 ? src1->ne[1] : 0;\n                const int64_t  ne12 = src1 ? src1->ne[2] : 0;\n                const int64_t  ne13 = src1 ? src1->ne[3] : 0; UNUSED(ne13);\n\n                const uint64_t nb10 = src1 ? src1->nb[0] : 0;\n                const uint64_t nb11 = src1 ? src1->nb[1] : 0;\n                const uint64_t nb12 = src1 ? src1->nb[2] : 0;\n                const uint64_t nb13 = src1 ? src1->nb[3] : 0; UNUSED(nb13);\n\n                const int64_t  ne0  = dst ? dst->ne[0] : 0;\n                const int64_t  ne1  = dst ? dst->ne[1] : 0;\n                const int64_t  ne2  = dst ? dst->ne[2] : 0;\n                const int64_t  ne3  = dst ? dst->ne[3] : 0;\n\n                const uint64_t nb0  = dst ? dst->nb[0] : 0;\n                const uint64_t nb1  = dst ? dst->nb[1] : 0;\n                const uint64_t nb2  = dst ? dst->nb[2] : 0;\n                const uint64_t nb3  = dst ? dst->nb[3] : 0;\n\n                const enum ggml_type src0t = src0 ? src0->type : GGML_TYPE_COUNT;\n                const enum ggml_type src1t = src1 ? src1->type : GGML_TYPE_COUNT;\n                const enum ggml_type dstt  = dst  ? dst->type  : GGML_TYPE_COUNT;\n\n                id<MTLBuffer> id_src0 = src0 ? ggml_metal_get_buffer(ctx, src0, &offs_src0) : nil;\n                id<MTLBuffer> id_src1 = src1 ? ggml_metal_get_buffer(ctx, src1, &offs_src1) : nil;\n                id<MTLBuffer> id_dst  = dst  ? ggml_metal_get_buffer(ctx, dst,  &offs_dst)  : nil;\n\n                //GGML_METAL_LOG_INFO(\"%s: op - %s\\n\", __func__, ggml_op_name(dst->op));\n                //if (src0) {\n                //    GGML_METAL_LOG_INFO(\"%s: src0 - %4s [%5lld, %5lld, %5lld], %d, %s\\n\", __func__, ggml_type_name(src0t), ne00, ne01, ne02,\n                //            ggml_is_contiguous(src0), src0->name);\n                //}\n                //if (src1) {\n                //    GGML_METAL_LOG_INFO(\"%s: src1 - %4s [%5lld, %5lld, %5lld], %d, %s\\n\", __func__, ggml_type_name(src1t), ne10, ne11, ne12,\n                //            ggml_is_contiguous(src1), src1->name);\n                //}\n                //if (dst) {\n                //    GGML_METAL_LOG_INFO(\"%s: dst  - %4s [%5lld, %5lld, %5lld], 1, %s\\n\",  __func__, ggml_type_name(dstt),  ne0,  ne1,  ne2,\n                //            dst->name);\n                //}\n\n                switch (dst->op) {\n                    case GGML_OP_CONCAT:\n                        {\n                            const int64_t nb = ne00;\n\n                            [encoder setComputePipelineState:ctx->pipeline_concat];\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_src1 offset:offs_src1 atIndex:1];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:2];\n                            [encoder setBytes:&ne00 length:sizeof(ne00) atIndex:3];\n                            [encoder setBytes:&ne01 length:sizeof(ne01) atIndex:4];\n                            [encoder setBytes:&ne02 length:sizeof(ne02) atIndex:5];\n                            [encoder setBytes:&ne03 length:sizeof(ne03) atIndex:6];\n                            [encoder setBytes:&nb00 length:sizeof(nb00) atIndex:7];\n                            [encoder setBytes:&nb01 length:sizeof(nb01) atIndex:8];\n                            [encoder setBytes:&nb02 length:sizeof(nb02) atIndex:9];\n                            [encoder setBytes:&nb03 length:sizeof(nb03) atIndex:10];\n                            [encoder setBytes:&ne10 length:sizeof(ne10) atIndex:11];\n                            [encoder setBytes:&ne11 length:sizeof(ne11) atIndex:12];\n                            [encoder setBytes:&ne12 length:sizeof(ne12) atIndex:13];\n                            [encoder setBytes:&ne13 length:sizeof(ne13) atIndex:14];\n                            [encoder setBytes:&nb10 length:sizeof(nb10) atIndex:15];\n                            [encoder setBytes:&nb11 length:sizeof(nb11) atIndex:16];\n                            [encoder setBytes:&nb12 length:sizeof(nb12) atIndex:17];\n                            [encoder setBytes:&nb13 length:sizeof(nb13) atIndex:18];\n                            [encoder setBytes:&ne0  length:sizeof(ne0)  atIndex:19];\n                            [encoder setBytes:&ne1  length:sizeof(ne1)  atIndex:20];\n                            [encoder setBytes:&ne2  length:sizeof(ne2)  atIndex:21];\n                            [encoder setBytes:&ne3  length:sizeof(ne3)  atIndex:22];\n                            [encoder setBytes:&nb0  length:sizeof(nb0)  atIndex:23];\n                            [encoder setBytes:&nb1  length:sizeof(nb1)  atIndex:24];\n                            [encoder setBytes:&nb2  length:sizeof(nb2)  atIndex:25];\n                            [encoder setBytes:&nb3  length:sizeof(nb3)  atIndex:26];\n                            [encoder setBytes:&nb   length:sizeof(nb)   atIndex:27];\n\n                            const int nth = MIN(1024, ne0);\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(ne1, ne2, ne3) threadsPerThreadgroup:MTLSizeMake(nth, 1, 1)];\n                        } break;\n                    case GGML_OP_ADD:\n                        {\n                            GGML_ASSERT(ggml_is_contiguous(src0));\n                            GGML_ASSERT(ggml_is_contiguous(src1));\n\n                            bool bcast_row = false;\n\n                            int64_t nb = ne00;\n\n                            if (ggml_nelements(src1) == ne10 && ne00 % 4 == 0) {\n                                // src1 is a row\n                                GGML_ASSERT(ne11 == 1);\n\n                                nb = ne00 / 4;\n                                [encoder setComputePipelineState:ctx->pipeline_add_row];\n\n                                bcast_row = true;\n                            } else {\n                                [encoder setComputePipelineState:ctx->pipeline_add];\n                            }\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_src1 offset:offs_src1 atIndex:1];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:2];\n                            [encoder setBytes:&ne00 length:sizeof(ne00) atIndex:3];\n                            [encoder setBytes:&ne01 length:sizeof(ne01) atIndex:4];\n                            [encoder setBytes:&ne02 length:sizeof(ne02) atIndex:5];\n                            [encoder setBytes:&ne03 length:sizeof(ne03) atIndex:6];\n                            [encoder setBytes:&nb00 length:sizeof(nb00) atIndex:7];\n                            [encoder setBytes:&nb01 length:sizeof(nb01) atIndex:8];\n                            [encoder setBytes:&nb02 length:sizeof(nb02) atIndex:9];\n                            [encoder setBytes:&nb03 length:sizeof(nb03) atIndex:10];\n                            [encoder setBytes:&ne10 length:sizeof(ne10) atIndex:11];\n                            [encoder setBytes:&ne11 length:sizeof(ne11) atIndex:12];\n                            [encoder setBytes:&ne12 length:sizeof(ne12) atIndex:13];\n                            [encoder setBytes:&ne13 length:sizeof(ne13) atIndex:14];\n                            [encoder setBytes:&nb10 length:sizeof(nb10) atIndex:15];\n                            [encoder setBytes:&nb11 length:sizeof(nb11) atIndex:16];\n                            [encoder setBytes:&nb12 length:sizeof(nb12) atIndex:17];\n                            [encoder setBytes:&nb13 length:sizeof(nb13) atIndex:18];\n                            [encoder setBytes:&ne0  length:sizeof(ne0)  atIndex:19];\n                            [encoder setBytes:&ne1  length:sizeof(ne1)  atIndex:20];\n                            [encoder setBytes:&ne2  length:sizeof(ne2)  atIndex:21];\n                            [encoder setBytes:&ne3  length:sizeof(ne3)  atIndex:22];\n                            [encoder setBytes:&nb0  length:sizeof(nb0)  atIndex:23];\n                            [encoder setBytes:&nb1  length:sizeof(nb1)  atIndex:24];\n                            [encoder setBytes:&nb2  length:sizeof(nb2)  atIndex:25];\n                            [encoder setBytes:&nb3  length:sizeof(nb3)  atIndex:26];\n                            [encoder setBytes:&nb   length:sizeof(nb)   atIndex:27];\n\n                            if (bcast_row) {\n                                const int64_t n = ggml_nelements(dst)/4;\n\n                                [encoder dispatchThreadgroups:MTLSizeMake(n, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                            } else {\n                                const int nth = MIN(1024, ne0);\n\n                                [encoder dispatchThreadgroups:MTLSizeMake(ne01, ne02, ne03) threadsPerThreadgroup:MTLSizeMake(nth, 1, 1)];\n                            }\n                        } break;\n                    case GGML_OP_MUL:\n                        {\n                            GGML_ASSERT(ggml_is_contiguous(src0));\n                            GGML_ASSERT(ggml_is_contiguous(src1));\n\n                            // utilize float4\n                            GGML_ASSERT(ne00 % 4 == 0);\n                            const int64_t nb = ne00/4;\n\n                            if (ggml_nelements(src1) == ne10) {\n                                // src1 is a row\n                                GGML_ASSERT(ne11 == 1);\n                                [encoder setComputePipelineState:ctx->pipeline_mul_row];\n                            } else {\n                                [encoder setComputePipelineState:ctx->pipeline_mul];\n                            }\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_src1 offset:offs_src1 atIndex:1];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:2];\n                            [encoder setBytes:&nb     length:sizeof(nb) atIndex:3];\n\n                            const int64_t n = ggml_nelements(dst)/4;\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(n, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                        } break;\n                    case GGML_OP_SCALE:\n                        {\n                            GGML_ASSERT(ggml_is_contiguous(src0));\n\n                            const float scale = *(const float *) src1->data;\n\n                            int64_t n = ggml_nelements(dst);\n\n                            if (n % 4 == 0) {\n                                n /= 4;\n                                [encoder setComputePipelineState:ctx->pipeline_scale_4];\n                            } else {\n                                [encoder setComputePipelineState:ctx->pipeline_scale];\n                            }\n\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:1];\n                            [encoder setBytes:&scale length:sizeof(scale) atIndex:2];\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(n, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                        } break;\n                    case GGML_OP_UNARY:\n                        switch (ggml_get_unary_op(gf->nodes[i])) {\n                            case GGML_UNARY_OP_SILU:\n                                {\n                                    [encoder setComputePipelineState:ctx->pipeline_silu];\n                                    [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                                    [encoder setBuffer:id_dst  offset:offs_dst  atIndex:1];\n\n                                    const int64_t n = ggml_nelements(dst);\n                                    GGML_ASSERT(n % 4 == 0);\n\n                                    [encoder dispatchThreadgroups:MTLSizeMake(n/4, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                                } break;\n                            case GGML_UNARY_OP_RELU:\n                                {\n                                    [encoder setComputePipelineState:ctx->pipeline_relu];\n                                    [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                                    [encoder setBuffer:id_dst  offset:offs_dst  atIndex:1];\n\n                                    const int64_t n = ggml_nelements(dst);\n\n                                    [encoder dispatchThreadgroups:MTLSizeMake(n, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                                } break;\n                            case GGML_UNARY_OP_GELU:\n                                {\n                                    [encoder setComputePipelineState:ctx->pipeline_gelu];\n                                    [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                                    [encoder setBuffer:id_dst  offset:offs_dst  atIndex:1];\n\n                                    const int64_t n = ggml_nelements(dst);\n                                    GGML_ASSERT(n % 4 == 0);\n\n                                    [encoder dispatchThreadgroups:MTLSizeMake(n/4, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                                } break;\n                            default:\n                                {\n                                    GGML_METAL_LOG_WARN(\"%s: node %3d, op = %8s not implemented\\n\", __func__, i, ggml_op_name(dst->op));\n                                    GGML_ASSERT(false);\n                                }\n                        } break;\n                    case GGML_OP_SQR:\n                        {\n                            GGML_ASSERT(ggml_is_contiguous(src0));\n\n                            [encoder setComputePipelineState:ctx->pipeline_sqr];\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst atIndex:1];\n\n                            const int64_t n = ggml_nelements(dst);\n                            [encoder dispatchThreadgroups:MTLSizeMake(n, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                        } break;\n                    case GGML_OP_SOFT_MAX:\n                        {\n                            int nth = 32; // SIMD width\n\n                            if (ne00%4 == 0) {\n                                [encoder setComputePipelineState:ctx->pipeline_soft_max_4];\n                            } else {\n                                do {\n                                    nth *= 2;\n                                } while (nth <= ne00 && nth <= 1024);\n                                nth /= 2;\n                                [encoder setComputePipelineState:ctx->pipeline_soft_max];\n                            }\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:1];\n                            [encoder setBytes:&ne00 length:sizeof(ne00) atIndex:2];\n                            [encoder setBytes:&ne01 length:sizeof(ne01) atIndex:3];\n                            [encoder setBytes:&ne02 length:sizeof(ne02) atIndex:4];\n                            [encoder setThreadgroupMemoryLength:GGML_PAD(nth/32*sizeof(float), 16) atIndex:0];\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(ne01*ne02*ne03, 1, 1) threadsPerThreadgroup:MTLSizeMake(nth, 1, 1)];\n                        } break;\n                    case GGML_OP_DIAG_MASK_INF:\n                        {\n                            const int n_past = ((int32_t *)(dst->op_params))[0];\n\n                            if (ne00%8 == 0) {\n                                [encoder setComputePipelineState:ctx->pipeline_diag_mask_inf_8];\n                            } else {\n                                [encoder setComputePipelineState:ctx->pipeline_diag_mask_inf];\n                            }\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:1];\n                            [encoder setBytes:&ne00   length:sizeof(ne00) atIndex:2];\n                            [encoder setBytes:&ne01   length:sizeof(ne01) atIndex:3];\n                            [encoder setBytes:&n_past length:sizeof(int)  atIndex:4];\n\n                            if (ne00%8 == 0) {\n                                [encoder dispatchThreadgroups:MTLSizeMake(ne00*ne01*ne02/8, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                            }\n                            else {\n                                [encoder dispatchThreadgroups:MTLSizeMake(ne00, ne01, ne02) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                            }\n                        } break;\n                    case GGML_OP_MUL_MAT:\n                        {\n                            GGML_ASSERT(ne00 == ne10);\n                            GGML_ASSERT(ne03 == ne13);\n\n                            const uint gqa = ne12/ne02;\n\n                            // find the break-even point where the matrix-matrix kernel becomes more efficient compared\n                            // to the matrix-vector kernel\n                            int ne11_mm_min = 1;\n\n#if 0\n                            // the numbers below are measured on M2 Ultra for 7B and 13B models\n                            // these numbers do not translate to other devices or model sizes\n                            // TODO: need to find a better approach\n                            if ([ctx->device.name isEqualToString:@\"Apple M2 Ultra\"]) {\n                                switch (src0t) {\n                                    case GGML_TYPE_F16:  ne11_mm_min = 2;  break;\n                                    case GGML_TYPE_Q8_0: ne11_mm_min = 7;  break;\n                                    case GGML_TYPE_Q2_K: ne11_mm_min = 15; break;\n                                    case GGML_TYPE_Q3_K: ne11_mm_min = 7;  break;\n                                    case GGML_TYPE_Q4_0:\n                                    case GGML_TYPE_Q4_1: ne11_mm_min = 15; break;\n                                    case GGML_TYPE_Q4_K: ne11_mm_min = 11; break;\n                                    case GGML_TYPE_Q5_0:                          // not tested yet\n                                    case GGML_TYPE_Q5_1: ne11_mm_min = 13; break; // not tested yet\n                                    case GGML_TYPE_Q5_K: ne11_mm_min = 7;  break;\n                                    case GGML_TYPE_Q6_K: ne11_mm_min = 7;  break;\n                                    default:             ne11_mm_min = 1;  break;\n                                }\n                            }\n#endif\n\n                            // for now the matrix-matrix multiplication kernel only works on A14+/M1+ SoCs\n                            // AMD GPU and older A-chips will reuse matrix-vector multiplication kernel\n                            if ([ctx->device supportsFamily:MTLGPUFamilyApple7] &&\n                                !ggml_is_transposed(src0) &&\n                                !ggml_is_transposed(src1) &&\n                                src1t == GGML_TYPE_F32 &&\n                                ne00 % 32 == 0 && ne00 >= 64 &&\n                                ne11 > ne11_mm_min) {\n                                //printf(\"matrix: ne00 = %6d, ne01 = %6d, ne02 = %6d, ne11 = %6d, ne12 = %6d\\n\", ne00, ne01, ne02, ne11, ne12);\n                                switch (src0->type) {\n                                    case GGML_TYPE_F32:  [encoder setComputePipelineState:ctx->pipeline_mul_mm_f32_f32];  break;\n                                    case GGML_TYPE_F16:  [encoder setComputePipelineState:ctx->pipeline_mul_mm_f16_f32];  break;\n                                    case GGML_TYPE_Q4_0: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q4_0_f32]; break;\n                                    case GGML_TYPE_Q4_1: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q4_1_f32]; break;\n                                    case GGML_TYPE_Q5_0: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q5_0_f32]; break;\n                                    case GGML_TYPE_Q5_1: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q5_1_f32]; break;\n                                    case GGML_TYPE_Q8_0: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q8_0_f32]; break;\n                                    case GGML_TYPE_Q2_K: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q2_K_f32]; break;\n                                    case GGML_TYPE_Q3_K: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q3_K_f32]; break;\n                                    case GGML_TYPE_Q4_K: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q4_K_f32]; break;\n                                    case GGML_TYPE_Q5_K: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q5_K_f32]; break;\n                                    case GGML_TYPE_Q6_K: [encoder setComputePipelineState:ctx->pipeline_mul_mm_q6_K_f32]; break;\n                                    default: GGML_ASSERT(false && \"MUL MAT-MAT not implemented\");\n                                }\n                                [encoder setBuffer:id_src0 offset:offs_src0    atIndex:0];\n                                [encoder setBuffer:id_src1 offset:offs_src1    atIndex:1];\n                                [encoder setBuffer:id_dst  offset:offs_dst     atIndex:2];\n                                [encoder setBytes:&ne00    length:sizeof(ne00) atIndex:3];\n                                [encoder setBytes:&ne02    length:sizeof(ne02) atIndex:4];\n                                [encoder setBytes:&nb01    length:sizeof(nb01) atIndex:5];\n                                [encoder setBytes:&nb02    length:sizeof(nb02) atIndex:6];\n                                [encoder setBytes:&ne12    length:sizeof(ne12) atIndex:7];\n                                [encoder setBytes:&nb10    length:sizeof(nb10) atIndex:8];\n                                [encoder setBytes:&nb11    length:sizeof(nb11) atIndex:9];\n                                [encoder setBytes:&nb12    length:sizeof(nb12) atIndex:10];\n                                [encoder setBytes:&ne0     length:sizeof(ne0)  atIndex:11];\n                                [encoder setBytes:&ne1     length:sizeof(ne1)  atIndex:12];\n                                [encoder setBytes:&gqa     length:sizeof(gqa)  atIndex:13];\n                                [encoder setThreadgroupMemoryLength:8192 atIndex:0];\n                                [encoder dispatchThreadgroups:MTLSizeMake( (ne11 + 31)/32, (ne01 + 63)/64, ne12) threadsPerThreadgroup:MTLSizeMake(128, 1, 1)];\n                            } else {\n                                int nth0 = 32;\n                                int nth1 = 1;\n                                int nrows = 1;\n                                //printf(\"vector: ne00 = %6d, ne01 = %6d, ne02 = %6d, ne11 = %6d, ne12 = %6d\\n\", ne00, ne01, ne02, ne11, ne12);\n\n                                // use custom matrix x vector kernel\n                                switch (src0t) {\n                                    case GGML_TYPE_F32:\n                                        {\n                                            GGML_ASSERT(src1t == GGML_TYPE_F32);\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_f32_f32];\n                                            nrows = 4;\n                                        } break;\n                                    case GGML_TYPE_F16:\n                                        {\n                                            nth0 = 32;\n                                            nth1 = 1;\n                                            if (src1t == GGML_TYPE_F32) {\n                                                if (ne11 * ne12 < 4) {\n                                                    [encoder setComputePipelineState:ctx->pipeline_mul_mv_f16_f32_1row];\n                                                } else if (ne00 >= 128 && ne01 >= 8 && ne00%4 == 0) {\n                                                    [encoder setComputePipelineState:ctx->pipeline_mul_mv_f16_f32_l4];\n                                                    nrows = ne11;\n                                                } else {\n                                                    [encoder setComputePipelineState:ctx->pipeline_mul_mv_f16_f32];\n                                                    nrows = 4;\n                                                }\n                                            } else {\n                                                [encoder setComputePipelineState:ctx->pipeline_mul_mv_f16_f16];\n                                                nrows = 4;\n                                            }\n                                        } break;\n                                    case GGML_TYPE_Q4_0:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 8;\n                                            nth1 = 8;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q4_0_f32];\n                                        } break;\n                                    case GGML_TYPE_Q4_1:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 8;\n                                            nth1 = 8;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q4_1_f32];\n                                        } break;\n                                    case GGML_TYPE_Q5_0:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 8;\n                                            nth1 = 8;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q5_0_f32];\n                                        } break;\n                                    case GGML_TYPE_Q5_1:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 8;\n                                            nth1 = 8;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q5_1_f32];\n                                        } break;\n                                    case GGML_TYPE_Q8_0:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 8;\n                                            nth1 = 8;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q8_0_f32];\n                                        } break;\n                                    case GGML_TYPE_Q2_K:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 2;\n                                            nth1 = 32;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q2_K_f32];\n                                        } break;\n                                    case GGML_TYPE_Q3_K:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 2;\n                                            nth1 = 32;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q3_K_f32];\n                                        } break;\n                                    case GGML_TYPE_Q4_K:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 4; //1;\n                                            nth1 = 8; //32;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q4_K_f32];\n                                        } break;\n                                    case GGML_TYPE_Q5_K:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 2;\n                                            nth1 = 32;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q5_K_f32];\n                                        } break;\n                                    case GGML_TYPE_Q6_K:\n                                        {\n                                            GGML_ASSERT(ne02 == 1);\n                                            GGML_ASSERT(ne12 == 1);\n\n                                            nth0 = 2;\n                                            nth1 = 32;\n                                            [encoder setComputePipelineState:ctx->pipeline_mul_mv_q6_K_f32];\n                                        } break;\n                                    default:\n                                        {\n                                            GGML_METAL_LOG_ERROR(\"Asserting on type %d\\n\", (int)src0t);\n                                            GGML_ASSERT(false && \"not implemented\");\n                                        }\n                                };\n\n                                [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                                [encoder setBuffer:id_src1 offset:offs_src1 atIndex:1];\n                                [encoder setBuffer:id_dst  offset:offs_dst  atIndex:2];\n                                [encoder setBytes:&ne00 length:sizeof(ne00) atIndex:3];\n                                [encoder setBytes:&ne01 length:sizeof(ne01) atIndex:4];\n                                [encoder setBytes:&ne02 length:sizeof(ne02) atIndex:5];\n                                [encoder setBytes:&nb00 length:sizeof(nb00) atIndex:6];\n                                [encoder setBytes:&nb01 length:sizeof(nb01) atIndex:7];\n                                [encoder setBytes:&nb02 length:sizeof(nb02) atIndex:8];\n                                [encoder setBytes:&ne10 length:sizeof(ne10) atIndex:9];\n                                [encoder setBytes:&ne11 length:sizeof(ne11) atIndex:10];\n                                [encoder setBytes:&ne12 length:sizeof(ne12) atIndex:11];\n                                [encoder setBytes:&nb10 length:sizeof(nb10) atIndex:12];\n                                [encoder setBytes:&nb11 length:sizeof(nb11) atIndex:13];\n                                [encoder setBytes:&nb12 length:sizeof(nb12) atIndex:14];\n                                [encoder setBytes:&ne0  length:sizeof(ne0)  atIndex:15];\n                                [encoder setBytes:&ne1  length:sizeof(ne1)  atIndex:16];\n                                [encoder setBytes:&gqa  length:sizeof(gqa)  atIndex:17];\n\n                                if (src0t == GGML_TYPE_Q4_0 || src0t == GGML_TYPE_Q4_1 ||\n                                    src0t == GGML_TYPE_Q5_0 || src0t == GGML_TYPE_Q5_1 || src0t == GGML_TYPE_Q8_0 ||\n                                    src0t == GGML_TYPE_Q2_K) { // || src0t == GGML_TYPE_Q4_K) {\n                                    [encoder dispatchThreadgroups:MTLSizeMake((ne01 + 7)/8, ne11, ne12) threadsPerThreadgroup:MTLSizeMake(nth0, nth1, 1)];\n                                }\n                                else if (src0t == GGML_TYPE_Q4_K) {\n                                    [encoder dispatchThreadgroups:MTLSizeMake((ne01 + 3)/4, ne11, ne12) threadsPerThreadgroup:MTLSizeMake(nth0, nth1, 1)];\n                                }\n                                else if (src0t == GGML_TYPE_Q3_K) {\n#ifdef GGML_QKK_64\n                                    [encoder dispatchThreadgroups:MTLSizeMake((ne01 + 1)/2, ne11, ne12) threadsPerThreadgroup:MTLSizeMake(nth0, nth1, 1)];\n#else\n                                    [encoder dispatchThreadgroups:MTLSizeMake((ne01 + 3)/4, ne11, ne12) threadsPerThreadgroup:MTLSizeMake(nth0, nth1, 1)];\n#endif\n                                }\n                                else if (src0t == GGML_TYPE_Q5_K) {\n                                    [encoder dispatchThreadgroups:MTLSizeMake((ne01 + 3)/4, ne11, ne12) threadsPerThreadgroup:MTLSizeMake(nth0, nth1, 1)];\n                                }\n                                else if (src0t == GGML_TYPE_Q6_K) {\n                                    [encoder dispatchThreadgroups:MTLSizeMake((ne01 + 1)/2, ne11, ne12) threadsPerThreadgroup:MTLSizeMake(nth0, nth1, 1)];\n                                } else {\n                                    int64_t ny = (ne11 + nrows - 1)/nrows;\n                                    [encoder dispatchThreadgroups:MTLSizeMake(ne01, ny, ne12) threadsPerThreadgroup:MTLSizeMake(nth0, nth1, 1)];\n                                }\n                            }\n                        } break;\n                    case GGML_OP_GET_ROWS:\n                        {\n                            switch (src0->type) {\n                                case GGML_TYPE_F32:  [encoder setComputePipelineState:ctx->pipeline_get_rows_f32];  break;\n                                case GGML_TYPE_F16:  [encoder setComputePipelineState:ctx->pipeline_get_rows_f16];  break;\n                                case GGML_TYPE_Q4_0: [encoder setComputePipelineState:ctx->pipeline_get_rows_q4_0]; break;\n                                case GGML_TYPE_Q4_1: [encoder setComputePipelineState:ctx->pipeline_get_rows_q4_1]; break;\n                                case GGML_TYPE_Q5_0: [encoder setComputePipelineState:ctx->pipeline_get_rows_q5_0]; break;\n                                case GGML_TYPE_Q5_1: [encoder setComputePipelineState:ctx->pipeline_get_rows_q5_1]; break;\n                                case GGML_TYPE_Q8_0: [encoder setComputePipelineState:ctx->pipeline_get_rows_q8_0]; break;\n                                case GGML_TYPE_Q2_K: [encoder setComputePipelineState:ctx->pipeline_get_rows_q2_K]; break;\n                                case GGML_TYPE_Q3_K: [encoder setComputePipelineState:ctx->pipeline_get_rows_q3_K]; break;\n                                case GGML_TYPE_Q4_K: [encoder setComputePipelineState:ctx->pipeline_get_rows_q4_K]; break;\n                                case GGML_TYPE_Q5_K: [encoder setComputePipelineState:ctx->pipeline_get_rows_q5_K]; break;\n                                case GGML_TYPE_Q6_K: [encoder setComputePipelineState:ctx->pipeline_get_rows_q6_K]; break;\n                                default: GGML_ASSERT(false && \"not implemented\");\n                            }\n\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_src1 offset:offs_src1 atIndex:1];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:2];\n                            [encoder setBytes:&ne00 length:sizeof( int64_t) atIndex:3];\n                            [encoder setBytes:&nb01 length:sizeof(uint64_t) atIndex:4];\n                            [encoder setBytes:&nb1  length:sizeof(uint64_t) atIndex:5];\n\n                            const int64_t n = ggml_nelements(src1);\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(n, 1, 1) threadsPerThreadgroup:MTLSizeMake(1, 1, 1)];\n                        } break;\n                    case GGML_OP_RMS_NORM:\n                        {\n                            GGML_ASSERT(ne00 % 4 == 0);\n\n                            float eps;\n                            memcpy(&eps, dst->op_params, sizeof(float));\n\n                            const int nth = MIN(512, ne00);\n\n                            [encoder setComputePipelineState:ctx->pipeline_rms_norm];\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:1];\n                            [encoder setBytes:&ne00 length:sizeof( int64_t) atIndex:2];\n                            [encoder setBytes:&nb01 length:sizeof(uint64_t) atIndex:3];\n                            [encoder setBytes:&eps  length:sizeof(   float) atIndex:4];\n                            [encoder setThreadgroupMemoryLength:GGML_PAD(nth/32*sizeof(float), 16) atIndex:0];\n\n                            const int64_t nrows = ggml_nrows(src0);\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(nrows, 1, 1) threadsPerThreadgroup:MTLSizeMake(nth, 1, 1)];\n                        } break;\n                    case GGML_OP_NORM:\n                        {\n                            float eps;\n                            memcpy(&eps, dst->op_params, sizeof(float));\n\n                            const int nth = MIN(256, ne00);\n\n                            [encoder setComputePipelineState:ctx->pipeline_norm];\n                            [encoder setBuffer:id_src0 offset:offs_src0        atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst         atIndex:1];\n                            [encoder setBytes:&ne00    length:sizeof( int64_t) atIndex:2];\n                            [encoder setBytes:&nb01    length:sizeof(uint64_t) atIndex:3];\n                            [encoder setBytes:&eps     length:sizeof(   float) atIndex:4];\n                            [encoder setThreadgroupMemoryLength:GGML_PAD(nth*sizeof(float), 16) atIndex:0];\n\n                            const int64_t nrows = ggml_nrows(src0);\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(nrows, 1, 1) threadsPerThreadgroup:MTLSizeMake(nth, 1, 1)];\n                        } break;\n                    case GGML_OP_ALIBI:\n                        {\n                            GGML_ASSERT((src0t == GGML_TYPE_F32));\n\n                            const int nth = MIN(1024, ne00);\n\n                            //const int n_past = ((int32_t *) dst->op_params)[0];\n                            const int n_head = ((int32_t *) dst->op_params)[1];\n                            float max_bias;\n                            memcpy(&max_bias, (int32_t *) dst->op_params + 2, sizeof(float));\n\n                            const int n_heads_log2_floor = 1 << (int) floor(log2(n_head));\n                            const float m0 = powf(2.0f, -(max_bias) / n_heads_log2_floor);\n                            const float m1 = powf(2.0f, -(max_bias / 2.0f) / n_heads_log2_floor);\n\n                            [encoder setComputePipelineState:ctx->pipeline_alibi_f32];\n                            [encoder setBuffer:id_src0 offset:offs_src0 atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst  atIndex:1];\n                            [encoder setBytes:&ne00 length:sizeof( int64_t) atIndex:2];\n                            [encoder setBytes:&ne01 length:sizeof( int64_t) atIndex:3];\n                            [encoder setBytes:&ne02 length:sizeof( int64_t) atIndex:4];\n                            [encoder setBytes:&ne03 length:sizeof( int64_t) atIndex:5];\n                            [encoder setBytes:&nb00 length:sizeof(uint64_t) atIndex:6];\n                            [encoder setBytes:&nb01 length:sizeof(uint64_t) atIndex:7];\n                            [encoder setBytes:&nb02 length:sizeof(uint64_t) atIndex:8];\n                            [encoder setBytes:&nb03 length:sizeof(uint64_t) atIndex:9];\n                            [encoder setBytes:&ne0  length:sizeof( int64_t) atIndex:10];\n                            [encoder setBytes:&ne1  length:sizeof( int64_t) atIndex:11];\n                            [encoder setBytes:&ne2  length:sizeof( int64_t) atIndex:12];\n                            [encoder setBytes:&ne3  length:sizeof( int64_t) atIndex:13];\n                            [encoder setBytes:&nb0  length:sizeof(uint64_t) atIndex:14];\n                            [encoder setBytes:&nb1  length:sizeof(uint64_t) atIndex:15];\n                            [encoder setBytes:&nb2  length:sizeof(uint64_t) atIndex:16];\n                            [encoder setBytes:&nb3  length:sizeof(uint64_t) atIndex:17];\n                            [encoder setBytes:&m0   length:sizeof(   float) atIndex:18];\n                            [encoder setBytes:&m1   length:sizeof(   float) atIndex:19];\n                            [encoder setBytes:&n_heads_log2_floor   length:sizeof(int) atIndex:20];\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(ne01, ne02, ne03) threadsPerThreadgroup:MTLSizeMake(nth, 1, 1)];\n                        } break;\n                    case GGML_OP_ROPE:\n                        {\n                            GGML_ASSERT(ne10 == ne02);\n\n                            const int nth = MIN(1024, ne00);\n\n                            const int n_past     = ((int32_t *) dst->op_params)[0];\n                            const int n_dims     = ((int32_t *) dst->op_params)[1];\n                            const int mode       = ((int32_t *) dst->op_params)[2];\n                            const int n_orig_ctx = ((int32_t *) dst->op_params)[3];\n\n                            float freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow;\n                            memcpy(&freq_base,   (int32_t *) dst->op_params +  5, sizeof(float));\n                            memcpy(&freq_scale,  (int32_t *) dst->op_params +  6, sizeof(float));\n                            memcpy(&ext_factor,  (int32_t *) dst->op_params +  7, sizeof(float));\n                            memcpy(&attn_factor, (int32_t *) dst->op_params +  8, sizeof(float));\n                            memcpy(&beta_fast,   (int32_t *) dst->op_params +  9, sizeof(float));\n                            memcpy(&beta_slow,   (int32_t *) dst->op_params + 10, sizeof(float));\n\n                            switch (src0->type) {\n                                case GGML_TYPE_F32: [encoder setComputePipelineState:ctx->pipeline_rope_f32]; break;\n                                case GGML_TYPE_F16: [encoder setComputePipelineState:ctx->pipeline_rope_f16]; break;\n                                default: GGML_ASSERT(false);\n                            };\n\n                            [encoder setBuffer:id_src0     offset:offs_src0        atIndex:0];\n                            [encoder setBuffer:id_src1     offset:offs_src1        atIndex:1];\n                            [encoder setBuffer:id_dst      offset:offs_dst         atIndex:2];\n                            [encoder setBytes:&ne00        length:sizeof( int64_t) atIndex:3];\n                            [encoder setBytes:&ne01        length:sizeof( int64_t) atIndex:4];\n                            [encoder setBytes:&ne02        length:sizeof( int64_t) atIndex:5];\n                            [encoder setBytes:&ne03        length:sizeof( int64_t) atIndex:6];\n                            [encoder setBytes:&nb00        length:sizeof(uint64_t) atIndex:7];\n                            [encoder setBytes:&nb01        length:sizeof(uint64_t) atIndex:8];\n                            [encoder setBytes:&nb02        length:sizeof(uint64_t) atIndex:9];\n                            [encoder setBytes:&nb03        length:sizeof(uint64_t) atIndex:10];\n                            [encoder setBytes:&ne0         length:sizeof( int64_t) atIndex:11];\n                            [encoder setBytes:&ne1         length:sizeof( int64_t) atIndex:12];\n                            [encoder setBytes:&ne2         length:sizeof( int64_t) atIndex:13];\n                            [encoder setBytes:&ne3         length:sizeof( int64_t) atIndex:14];\n                            [encoder setBytes:&nb0         length:sizeof(uint64_t) atIndex:15];\n                            [encoder setBytes:&nb1         length:sizeof(uint64_t) atIndex:16];\n                            [encoder setBytes:&nb2         length:sizeof(uint64_t) atIndex:17];\n                            [encoder setBytes:&nb3         length:sizeof(uint64_t) atIndex:18];\n                            [encoder setBytes:&n_past      length:sizeof(     int) atIndex:19];\n                            [encoder setBytes:&n_dims      length:sizeof(     int) atIndex:20];\n                            [encoder setBytes:&mode        length:sizeof(     int) atIndex:21];\n                            [encoder setBytes:&n_orig_ctx  length:sizeof(     int) atIndex:22];\n                            [encoder setBytes:&freq_base   length:sizeof(   float) atIndex:23];\n                            [encoder setBytes:&freq_scale  length:sizeof(   float) atIndex:24];\n                            [encoder setBytes:&ext_factor  length:sizeof(   float) atIndex:25];\n                            [encoder setBytes:&attn_factor length:sizeof(   float) atIndex:26];\n                            [encoder setBytes:&beta_fast   length:sizeof(   float) atIndex:27];\n                            [encoder setBytes:&beta_slow   length:sizeof(   float) atIndex:28];\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(ne01, ne02, ne03) threadsPerThreadgroup:MTLSizeMake(nth, 1, 1)];\n                        } break;\n                    case GGML_OP_IM2COL:\n                        {\n                            GGML_ASSERT(src0->type == GGML_TYPE_F16);\n                            GGML_ASSERT(src1->type == GGML_TYPE_F32);\n                            GGML_ASSERT( dst->type == GGML_TYPE_F16);\n\n                            const int32_t s0 = ((const int32_t *)(dst->op_params))[0];\n                            const int32_t s1 = ((const int32_t *)(dst->op_params))[1];\n                            const int32_t p0 = ((const int32_t *)(dst->op_params))[2];\n                            const int32_t p1 = ((const int32_t *)(dst->op_params))[3];\n                            const int32_t d0 = ((const int32_t *)(dst->op_params))[4];\n                            const int32_t d1 = ((const int32_t *)(dst->op_params))[5];\n                            const bool is_2D = ((const int32_t *)(dst->op_params))[6] == 1;\n\n                            const int32_t N  = src1->ne[is_2D ? 3 : 2];\n                            const int32_t IC = src1->ne[is_2D ? 2 : 1];\n                            const int32_t IH = is_2D ? src1->ne[1] : 1;\n                            const int32_t IW =         src1->ne[0];\n\n                            const int32_t KH = is_2D ? src0->ne[1] : 1;\n                            const int32_t KW =         src0->ne[0];\n\n                            const int32_t OH = is_2D ? dst->ne[2] : 1;\n                            const int32_t OW =         dst->ne[1];\n\n                            const int32_t CHW = IC * KH * KW;\n\n                            const int32_t ofs0 = src1->nb[is_2D ? 3 : 2] / 4;\n                            const int32_t ofs1 = src1->nb[is_2D ? 2 : 1] / 4;\n\n                            switch (src0->type) {\n                                case GGML_TYPE_F32: GGML_ASSERT(false && \"not implemented\"); break;\n                                case GGML_TYPE_F16: [encoder setComputePipelineState:ctx->pipeline_im2col_f16]; break;\n                                default: GGML_ASSERT(false);\n                            };\n\n                            [encoder setBuffer:id_src1 offset:offs_src1        atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst         atIndex:1];\n                            [encoder setBytes:&ofs0    length:sizeof( int32_t) atIndex:2];\n                            [encoder setBytes:&ofs1    length:sizeof( int32_t) atIndex:3];\n                            [encoder setBytes:&IW      length:sizeof( int32_t) atIndex:4];\n                            [encoder setBytes:&IH      length:sizeof( int32_t) atIndex:5];\n                            [encoder setBytes:&CHW     length:sizeof( int32_t) atIndex:6];\n                            [encoder setBytes:&s0      length:sizeof( int32_t) atIndex:7];\n                            [encoder setBytes:&s1      length:sizeof( int32_t) atIndex:8];\n                            [encoder setBytes:&p0      length:sizeof( int32_t) atIndex:9];\n                            [encoder setBytes:&p1      length:sizeof( int32_t) atIndex:10];\n                            [encoder setBytes:&d0      length:sizeof( int32_t) atIndex:11];\n                            [encoder setBytes:&d1      length:sizeof( int32_t) atIndex:12];\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(IC, OH, OW) threadsPerThreadgroup:MTLSizeMake(N, KH, KW)];\n                        } break;\n                    case GGML_OP_DUP:\n                    case GGML_OP_CPY:\n                    case GGML_OP_CONT:\n                        {\n                            const int nth = MIN(1024, ne00);\n\n                            switch (src0t) {\n                                case GGML_TYPE_F32:\n                                    {\n                                        switch (dstt) {\n                                            case GGML_TYPE_F16: [encoder setComputePipelineState:ctx->pipeline_cpy_f32_f16]; break;\n                                            case GGML_TYPE_F32: [encoder setComputePipelineState:ctx->pipeline_cpy_f32_f32]; break;\n                                            default: GGML_ASSERT(false && \"not implemented\");\n                                        };\n                                    } break;\n                                case GGML_TYPE_F16:\n                                    {\n                                        switch (dstt) {\n                                            case GGML_TYPE_F16: [encoder setComputePipelineState:ctx->pipeline_cpy_f16_f16]; break;\n                                            case GGML_TYPE_F32: GGML_ASSERT(false && \"cpy_f16_f32 not implemented\"); break;\n                                            default: GGML_ASSERT(false && \"not implemented\");\n                                        };\n                                    } break;\n                                default: GGML_ASSERT(false && \"not implemented\");\n                            }\n\n                            [encoder setBuffer:id_src0 offset:offs_src0        atIndex:0];\n                            [encoder setBuffer:id_dst  offset:offs_dst         atIndex:1];\n                            [encoder setBytes:&ne00    length:sizeof( int64_t) atIndex:2];\n                            [encoder setBytes:&ne01    length:sizeof( int64_t) atIndex:3];\n                            [encoder setBytes:&ne02    length:sizeof( int64_t) atIndex:4];\n                            [encoder setBytes:&ne03    length:sizeof( int64_t) atIndex:5];\n                            [encoder setBytes:&nb00    length:sizeof(uint64_t) atIndex:6];\n                            [encoder setBytes:&nb01    length:sizeof(uint64_t) atIndex:7];\n                            [encoder setBytes:&nb02    length:sizeof(uint64_t) atIndex:8];\n                            [encoder setBytes:&nb03    length:sizeof(uint64_t) atIndex:9];\n                            [encoder setBytes:&ne0     length:sizeof( int64_t) atIndex:10];\n                            [encoder setBytes:&ne1     length:sizeof( int64_t) atIndex:11];\n                            [encoder setBytes:&ne2     length:sizeof( int64_t) atIndex:12];\n                            [encoder setBytes:&ne3     length:sizeof( int64_t) atIndex:13];\n                            [encoder setBytes:&nb0     length:sizeof(uint64_t) atIndex:14];\n                            [encoder setBytes:&nb1     length:sizeof(uint64_t) atIndex:15];\n                            [encoder setBytes:&nb2     length:sizeof(uint64_t) atIndex:16];\n                            [encoder setBytes:&nb3     length:sizeof(uint64_t) atIndex:17];\n\n                            [encoder dispatchThreadgroups:MTLSizeMake(ne01, ne02, ne03) threadsPerThreadgroup:MTLSizeMake(nth, 1, 1)];\n                        } break;\n                    default:\n                        {\n                            GGML_METAL_LOG_ERROR(\"%s: error: node %3d, op = %8s not implemented\\n\", __func__, i, ggml_op_name(dst->op));\n                            GGML_ASSERT(false);\n                        }\n                }\n            }\n\n            if (encoder != nil) {\n                [encoder endEncoding];\n                encoder = nil;\n            }\n\n            [command_buffer commit];\n        });\n    }\n\n    // wait for all threads to finish\n    dispatch_barrier_sync(ctx->d_queue, ^{});\n\n    // check status of command buffers\n    // needed to detect if the device ran out-of-memory for example (#1881)\n    for (int i = 0; i < n_cb; i++) {\n        [ctx->command_buffers[i] waitUntilCompleted];\n\n        MTLCommandBufferStatus status = (MTLCommandBufferStatus) [ctx->command_buffers[i] status];\n        if (status != MTLCommandBufferStatusCompleted) {\n            GGML_METAL_LOG_INFO(\"%s: command buffer %d failed with status %lu\\n\", __func__, i, status);\n            GGML_ASSERT(false);\n        }\n    }\n\n    }\n}\n\n////////////////////////////////////////////////////////////////////////////////\n\n// backend interface\n\nstatic const char * ggml_backend_metal_name(ggml_backend_t backend) {\n    return \"Metal\";\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_metal_free(ggml_backend_t backend) {\n    struct ggml_metal_context * ctx = (struct ggml_metal_context *)backend->context;\n    ggml_metal_free(ctx);\n    free(backend);\n}\n\nstatic void * ggml_backend_metal_buffer_get_base(ggml_backend_buffer_t buffer) {\n    return (void *)buffer->context;\n}\n\nstatic void ggml_backend_metal_buffer_free_buffer(ggml_backend_buffer_t buffer) {\n    free(buffer->context);\n    UNUSED(buffer);\n}\n\nstatic struct ggml_backend_buffer_i metal_backend_buffer_i = {\n    /* .free_buffer    = */ ggml_backend_metal_buffer_free_buffer,\n    /* .get_base       = */ ggml_backend_metal_buffer_get_base,\n    /* .get_alloc_size = */ NULL, // defaults to ggml_nbytes\n    /* .init_tensor    = */ NULL, // no initialization required\n    /* .free_tensor    = */ NULL, // no cleanup required\n};\n\nstatic ggml_backend_buffer_t ggml_backend_metal_alloc_buffer(ggml_backend_t backend, size_t size) {\n    struct ggml_metal_context * ctx = (struct ggml_metal_context *)backend->context;\n\n    void * data = ggml_metal_host_malloc(size);\n\n    // TODO: set proper name of the buffers\n    ggml_metal_add_buffer(ctx, \"backend\", data, size, 0);\n\n    return ggml_backend_buffer_init(backend, metal_backend_buffer_i, data, size);\n}\n\nstatic size_t ggml_backend_metal_get_alignment(ggml_backend_t backend) {\n    return 32;\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_metal_set_tensor_async(ggml_backend_t backend, struct ggml_tensor * tensor, const void * data, size_t offset, size_t size) {\n    GGML_ASSERT(offset + size <= ggml_nbytes(tensor) && \"tensor write out of bounds\");\n    GGML_ASSERT(tensor->data != NULL && \"tensor not allocated\");\n\n    memcpy((char *)tensor->data + offset, data, size);\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_metal_get_tensor_async(ggml_backend_t backend, const struct ggml_tensor * tensor, void * data, size_t offset, size_t size) {\n    GGML_ASSERT(offset + size <= ggml_nbytes(tensor) && \"tensor read out of bounds\");\n    GGML_ASSERT(tensor->data != NULL && \"tensor not allocated\");\n\n    memcpy(data, (const char *)tensor->data + offset, size);\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_metal_synchronize(ggml_backend_t backend) {\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_metal_cpy_tensor_from(ggml_backend_t backend, struct ggml_tensor * src, struct ggml_tensor * dst) {\n    ggml_backend_tensor_get(src, dst->data, 0, ggml_nbytes(src));\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_metal_cpy_tensor_to(ggml_backend_t backend, struct ggml_tensor * src, struct ggml_tensor * dst) {\n    ggml_backend_tensor_set_async(dst, src->data, 0, ggml_nbytes(src));\n\n    UNUSED(backend);\n}\n\nstatic void ggml_backend_metal_graph_compute(ggml_backend_t backend, struct ggml_cgraph * cgraph) {\n    struct ggml_metal_context * metal_ctx = (struct ggml_metal_context *)backend->context;\n\n    ggml_metal_graph_compute(metal_ctx, cgraph);\n}\n\nstatic bool ggml_backend_metal_supports_op(ggml_backend_t backend, const struct ggml_tensor * op) {\n    return true;\n    UNUSED(backend);\n    UNUSED(op);\n}\n\nstatic struct ggml_backend_i metal_backend_i = {\n    /* .get_name            = */ ggml_backend_metal_name,\n    /* .free                = */ ggml_backend_metal_free,\n    /* .alloc_buffer        = */ ggml_backend_metal_alloc_buffer,\n    /* .get_alignment       = */ ggml_backend_metal_get_alignment,\n    /* .set_tensor_async    = */ ggml_backend_metal_set_tensor_async,\n    /* .get_tensor_async    = */ ggml_backend_metal_get_tensor_async,\n    /* .synchronize         = */ ggml_backend_metal_synchronize,\n    /* .cpy_tensor_from     = */ ggml_backend_metal_cpy_tensor_from,\n    /* .cpy_tensor_to       = */ ggml_backend_metal_cpy_tensor_to,\n    /* .graph_plan_create   = */ NULL, // the metal implementation does not require creating graph plans atm\n    /* .graph_plan_free     = */ NULL,\n    /* .graph_plan_compute  = */ NULL,\n    /* .graph_compute       = */ ggml_backend_metal_graph_compute,\n    /* .supports_op         = */ ggml_backend_metal_supports_op,\n};\n\nggml_backend_t ggml_backend_metal_init(void) {\n    struct ggml_metal_context * ctx = malloc(sizeof(struct ggml_metal_context));\n\n    ctx = ggml_metal_init(GGML_DEFAULT_N_THREADS);\n\n    ggml_backend_t metal_backend = malloc(sizeof(struct ggml_backend));\n\n    *metal_backend = (struct ggml_backend) {\n        /* .interface = */ metal_backend_i,\n        /* .context   = */ ctx,\n    };\n\n    return metal_backend;\n}\n\nbool ggml_backend_is_metal(ggml_backend_t backend) {\n    return backend->iface.get_name == ggml_backend_metal_name;\n}\n\nvoid ggml_backend_metal_set_n_cb(ggml_backend_t backend, int n_cb) {\n    struct ggml_metal_context * ctx = (struct ggml_metal_context *)backend->context;\n\n    ggml_metal_set_n_cb(ctx, n_cb);\n}\n"
        },
        {
          "name": "ggml-metal.metal",
          "type": "blob",
          "size": 104.2451171875,
          "content": "#include <metal_stdlib>\n\nusing namespace metal;\n\n#define MAX(x, y) ((x) > (y) ? (x) : (y))\n\n#define QK4_0 32\n#define QR4_0 2\ntypedef struct {\n    half    d;             // delta\n    uint8_t qs[QK4_0 / 2]; // nibbles / quants\n} block_q4_0;\n\n#define QK4_1 32\ntypedef struct {\n    half d;                 // delta\n    half m;                 // min\n    uint8_t qs[QK4_1 / 2];  // nibbles / quants\n} block_q4_1;\n\n#define QK5_0 32\ntypedef struct {\n    half d;                // delta\n    uint8_t qh[4];         // 5-th bit of quants\n    uint8_t qs[QK5_0 / 2]; // nibbles / quants\n} block_q5_0;\n\n#define QK5_1 32\ntypedef struct {\n    half d;                 // delta\n    half m;                 // min\n    uint8_t qh[4];          // 5-th bit of quants\n    uint8_t qs[QK5_1 / 2];  // nibbles / quants\n} block_q5_1;\n\n#define QK8_0 32\ntypedef struct {\n    half    d;         // delta\n    int8_t  qs[QK8_0]; // quants\n} block_q8_0;\n\n// general-purpose kernel for addition of two tensors\n// pros: works for non-contiguous tensors, supports broadcast across dims 1, 2 and 3\n// cons: not very efficient\nkernel void kernel_add(\n        device const char * src0,\n        device const char * src1,\n        device       char * dst,\n        constant  int64_t & ne00,\n        constant  int64_t & ne01,\n        constant  int64_t & ne02,\n        constant  int64_t & ne03,\n        constant  int64_t & nb00,\n        constant  int64_t & nb01,\n        constant  int64_t & nb02,\n        constant  int64_t & nb03,\n        constant  int64_t & ne10,\n        constant  int64_t & ne11,\n        constant  int64_t & ne12,\n        constant  int64_t & ne13,\n        constant  int64_t & nb10,\n        constant  int64_t & nb11,\n        constant  int64_t & nb12,\n        constant  int64_t & nb13,\n        constant  int64_t & ne0,\n        constant  int64_t & ne1,\n        constant  int64_t & ne2,\n        constant  int64_t & ne3,\n        constant  int64_t & nb0,\n        constant  int64_t & nb1,\n        constant  int64_t & nb2,\n        constant  int64_t & nb3,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint3 tpitg[[thread_position_in_threadgroup]],\n        uint3   ntg[[threads_per_threadgroup]]) {\n    const int64_t i03 = tgpig.z;\n    const int64_t i02 = tgpig.y;\n    const int64_t i01 = tgpig.x;\n\n    const int64_t i13 = i03 % ne13;\n    const int64_t i12 = i02 % ne12;\n    const int64_t i11 = i01 % ne11;\n\n    device const char * src0_ptr = src0 + i03*nb03 + i02*nb02 + i01*nb01 + tpitg.x*nb00;\n    device const char * src1_ptr = src1 + i13*nb13 + i12*nb12 + i11*nb11 + tpitg.x*nb10;\n    device       char * dst_ptr  = dst  + i03*nb3  + i02*nb2  + i01*nb1  + tpitg.x*nb0;\n\n    for (int i0 = tpitg.x; i0 < ne0; i0 += ntg.x) {\n        ((device float *)dst_ptr)[0] = ((device float *)src0_ptr)[0] + ((device float *)src1_ptr)[0];\n\n        src0_ptr += ntg.x*nb00;\n        src1_ptr += ntg.x*nb10;\n        dst_ptr  += ntg.x*nb0;\n    }\n}\n\n// assumption: src1 is a row\n// broadcast src1 into src0\nkernel void kernel_add_row(\n        device const float4 * src0,\n        device const float4 * src1,\n        device       float4 * dst,\n        constant    int64_t & nb [[buffer(27)]],\n        uint tpig[[thread_position_in_grid]]) {\n    dst[tpig] = src0[tpig] + src1[tpig % nb];\n}\n\nkernel void kernel_mul(\n        device const float4 * src0,\n        device const float4 * src1,\n        device       float4 * dst,\n        uint tpig[[thread_position_in_grid]]) {\n    dst[tpig] = src0[tpig] * src1[tpig];\n}\n\n// assumption: src1 is a row\n// broadcast src1 into src0\nkernel void kernel_mul_row(\n        device const float4 * src0,\n        device const float4 * src1,\n        device       float4 * dst,\n        constant    int64_t & nb,\n        uint tpig[[thread_position_in_grid]]) {\n    dst[tpig] = src0[tpig] * src1[tpig % nb];\n}\n\nkernel void kernel_scale(\n        device const float * src0,\n        device       float * dst,\n        constant     float & scale,\n        uint tpig[[thread_position_in_grid]]) {\n    dst[tpig] = src0[tpig] * scale;\n}\n\nkernel void kernel_scale_4(\n        device const float4 * src0,\n        device       float4 * dst,\n        constant     float  & scale,\n        uint tpig[[thread_position_in_grid]]) {\n    dst[tpig] = src0[tpig] * scale;\n}\n\nkernel void kernel_silu(\n        device const float4 * src0,\n        device       float4 * dst,\n        uint tpig[[thread_position_in_grid]]) {\n    device const float4 & x = src0[tpig];\n    dst[tpig] = x / (1.0f + exp(-x));\n}\n\nkernel void kernel_relu(\n        device const float * src0,\n        device       float * dst,\n        uint tpig[[thread_position_in_grid]]) {\n    dst[tpig] = max(0.0f, src0[tpig]);\n}\n\nkernel void kernel_sqr(\n        device const float * src0,\n        device       float * dst,\n        uint tpig[[thread_position_in_grid]]) {\n    dst[tpig] = src0[tpig] * src0[tpig];\n}\n\nconstant float GELU_COEF_A    = 0.044715f;\nconstant float SQRT_2_OVER_PI = 0.79788456080286535587989211986876f;\n\nkernel void kernel_gelu(\n    device const float4 * src0,\n    device       float4 * dst,\n    uint tpig[[thread_position_in_grid]]) {\n    device const float4 & x = src0[tpig];\n\n    // BEWARE !!!\n    // Simply using \"tanh\" instead of \"precise::tanh\" will sometimes results in NaNs!\n    // This was observed with Falcon 7B and 40B models\n    //\n    dst[tpig] = 0.5f*x*(1.0f + precise::tanh(SQRT_2_OVER_PI*x*(1.0f + GELU_COEF_A*x*x)));\n}\n\nkernel void kernel_soft_max(\n        device const float * src0,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        threadgroup float  * buf [[threadgroup(0)]],\n        uint  tgpig[[threadgroup_position_in_grid]],\n        uint  tpitg[[thread_position_in_threadgroup]],\n        uint  sgitg[[simdgroup_index_in_threadgroup]],\n        uint  tiisg[[thread_index_in_simdgroup]],\n        uint    ntg[[threads_per_threadgroup]]) {\n    const int64_t i03 = (tgpig) / (ne02*ne01);\n    const int64_t i02 = (tgpig - i03*ne02*ne01) / ne01;\n    const int64_t i01 = (tgpig - i03*ne02*ne01 - i02*ne01);\n\n    device const float * psrc0 = src0 + i03*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00;\n    device       float * pdst  = dst  + i03*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00;\n\n    // parallel max\n    float lmax = tpitg < ne00 ? psrc0[tpitg] : -INFINITY;\n\n    for (int i00 = tpitg + ntg; i00 < ne00; i00 += ntg) {\n        lmax = MAX(lmax, psrc0[i00]);\n    }\n\n    float max = simd_max(lmax);\n    if (tiisg == 0) {\n        buf[sgitg] = max;\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    // broadcast, simd group number is ntg / 32\n    for (uint i = ntg / 32 / 2; i > 0; i /= 2) {\n       if (tpitg < i) {\n           buf[tpitg] = MAX(buf[tpitg], buf[tpitg + i]);\n       }\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    max = buf[0];\n\n    // parallel sum\n    float lsum = 0.0f;\n    for (int i00 = tpitg; i00 < ne00; i00 += ntg) {\n        const float exp_psrc0 = exp(psrc0[i00] - max);\n        lsum += exp_psrc0;\n        // Remember the result of exp here. exp is expensive, so we really do not\n        // wish to compute it twice.\n        pdst[i00] = exp_psrc0;\n    }\n\n    float sum = simd_sum(lsum);\n    if (tiisg == 0) {\n        buf[sgitg] = sum;\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    // broadcast, simd group number is ntg / 32\n    for (uint i = ntg / 32 / 2; i > 0; i /= 2) {\n       if (tpitg < i) {\n           buf[tpitg] += buf[tpitg + i];\n       }\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    sum = buf[0];\n\n    for (int i00 = tpitg; i00 < ne00; i00 += ntg) {\n        pdst[i00] /= sum;\n    }\n}\n\nkernel void kernel_soft_max_4(\n        device const float * src0,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        threadgroup float  * buf [[threadgroup(0)]],\n        uint  tgpig[[threadgroup_position_in_grid]],\n        uint  tpitg[[thread_position_in_threadgroup]],\n        uint  sgitg[[simdgroup_index_in_threadgroup]],\n        uint  tiisg[[thread_index_in_simdgroup]],\n        uint    ntg[[threads_per_threadgroup]]) {\n    const int64_t i03 = (tgpig) / (ne02*ne01);\n    const int64_t i02 = (tgpig - i03*ne02*ne01) / ne01;\n    const int64_t i01 = (tgpig - i03*ne02*ne01 - i02*ne01);\n\n    device const float4 * psrc4 = (device const float4 *)(src0 + i03*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00);\n    device       float4 * pdst4 = (device       float4 *)(dst  + i03*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00);\n\n    // parallel max\n    float4 lmax4 = tpitg < ne00/4 ? psrc4[tpitg] : -INFINITY;\n\n    for (int i00 = tpitg + ntg; i00 < ne00/4; i00 += ntg) {\n        lmax4 = fmax(lmax4, psrc4[i00]);\n    }\n\n    const float lmax = MAX(MAX(lmax4[0], lmax4[1]), MAX(lmax4[2], lmax4[3]));\n    float max = simd_max(lmax);\n    if (tiisg == 0) {\n        buf[sgitg] = max;\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    // broadcast, simd group number is ntg / 32\n    for (uint i = ntg / 32 / 2; i > 0; i /= 2) {\n       if (tpitg < i) {\n           buf[tpitg] = MAX(buf[tpitg], buf[tpitg + i]);\n       }\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    max = buf[0];\n\n    // parallel sum\n    float4 lsum4 = 0.0f;\n    for (int i00 = tpitg; i00 < ne00/4; i00 += ntg) {\n        const float4 exp_psrc4 = exp(psrc4[i00] - max);\n        lsum4 += exp_psrc4;\n        pdst4[i00] = exp_psrc4;\n    }\n\n    const float lsum = lsum4[0] + lsum4[1] + lsum4[2] + lsum4[3];\n    float sum = simd_sum(lsum);\n    if (tiisg == 0) {\n        buf[sgitg] = sum;\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    // broadcast, simd group number is ntg / 32\n    for (uint i = ntg / 32 / 2; i > 0; i /= 2) {\n       if (tpitg < i) {\n           buf[tpitg] += buf[tpitg + i];\n       }\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    sum = buf[0];\n\n    for (int i00 = tpitg; i00 < ne00/4; i00 += ntg) {\n        pdst4[i00] /= sum;\n    }\n}\n\nkernel void kernel_diag_mask_inf(\n        device const float * src0,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant       int & n_past,\n        uint3 tpig[[thread_position_in_grid]]) {\n    const int64_t i02 = tpig[2];\n    const int64_t i01 = tpig[1];\n    const int64_t i00 = tpig[0];\n\n    if (i00 > n_past + i01) {\n        dst[i02*ne01*ne00 + i01*ne00 + i00] = -INFINITY;\n    } else {\n        dst[i02*ne01*ne00 + i01*ne00 + i00] = src0[i02*ne01*ne00 + i01*ne00 + i00];\n    }\n}\n\nkernel void kernel_diag_mask_inf_8(\n        device const float4 * src0,\n        device       float4 * dst,\n        constant    int64_t & ne00,\n        constant    int64_t & ne01,\n        constant        int & n_past,\n        uint3 tpig[[thread_position_in_grid]]) {\n\n    const int64_t i = 2*tpig[0];\n\n    dst[i+0] = src0[i+0];\n    dst[i+1] = src0[i+1];\n    int64_t i4 = 4*i;\n    const int64_t i02 = i4/(ne00*ne01); i4 -= i02*ne00*ne01;\n    const int64_t i01 = i4/(ne00);      i4 -= i01*ne00;\n    const int64_t i00 = i4;\n    for (int k = 3; k >= 0; --k) {\n        if (i00 + 4 + k <= n_past + i01) {\n            break;\n        }\n        dst[i+1][k] = -INFINITY;\n        if (i00 + k > n_past + i01) {\n            dst[i][k] = -INFINITY;\n        }\n    }\n}\n\nkernel void kernel_norm(\n        device const  void * src0,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant  uint64_t & nb01,\n        constant     float & eps,\n        threadgroup float  * sum [[threadgroup(0)]],\n        uint tgpig[[threadgroup_position_in_grid]],\n        uint tpitg[[thread_position_in_threadgroup]],\n        uint   ntg[[threads_per_threadgroup]]) {\n    device const float * x = (device const float *) ((device const char *) src0 + tgpig*nb01);\n    // MEAN\n    // parallel sum\n    sum[tpitg] = 0.0f;\n    for (int i00 = tpitg; i00 < ne00; i00 += ntg) {\n        sum[tpitg] += x[i00];\n    }\n    // reduce\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint i = ntg/2; i > 0; i /= 2) {\n        if (tpitg < i) {\n            sum[tpitg] += sum[tpitg + i];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n    const float mean  = sum[0] / ne00;\n\n    // recenter and VARIANCE\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    device float * y = dst + tgpig*ne00;\n    sum[tpitg] = 0.0f;\n    for (int i00 = tpitg; i00 < ne00; i00 += ntg) {\n        y[i00] = x[i00] - mean;\n        sum[tpitg] += y[i00] * y[i00];\n    }\n\n    // reduce\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n    for (uint i = ntg/2; i > 0; i /= 2) {\n        if (tpitg < i) {\n            sum[tpitg] += sum[tpitg + i];\n        }\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n    }\n    const float variance = sum[0] / ne00;\n\n    const float scale = 1.0f/sqrt(variance + eps);\n    for (int i00 = tpitg; i00 < ne00; i00 += ntg) {\n        y[i00] = y[i00] * scale;\n    }\n}\n\nkernel void kernel_rms_norm(\n        device const  void * src0,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant  uint64_t & nb01,\n        constant     float & eps,\n        threadgroup float  * sum [[threadgroup(0)]],\n        uint tgpig[[threadgroup_position_in_grid]],\n        uint tpitg[[thread_position_in_threadgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint   ntg[[threads_per_threadgroup]]) {\n    device const float4 * x        = (device const float4 *) ((device const char *) src0 + tgpig*nb01);\n    device const float  * x_scalar = (device const float  *) x;\n\n    float4 sumf = 0;\n    float all_sum = 0;\n\n    // parallel sum\n    for (int i00 = tpitg; i00 < ne00/4; i00 += ntg) {\n        sumf += x[i00] * x[i00];\n    }\n    all_sum = sumf[0] + sumf[1] + sumf[2] + sumf[3];\n    all_sum = simd_sum(all_sum);\n    if (tiisg == 0) {\n        sum[sgitg] = all_sum;\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    // broadcast, simd group number is ntg / 32\n    for (uint i = ntg / 32 / 2; i > 0; i /= 2) {\n       if (tpitg < i) {\n           sum[tpitg] += sum[tpitg + i];\n       }\n    }\n    if (tpitg == 0) {\n        for (int i = 4 * (ne00 / 4); i < ne00; i++) {\n            sum[0] += x_scalar[i];\n        }\n        sum[0] /= ne00;\n    }\n\n    threadgroup_barrier(mem_flags::mem_threadgroup);\n\n    const float mean  = sum[0];\n    const float scale = 1.0f/sqrt(mean + eps);\n\n    device float4 * y = (device float4 *) (dst + tgpig*ne00);\n    device float * y_scalar = (device float *) y;\n    for (int i00 = tpitg; i00 < ne00/4; i00 += ntg) {\n        y[i00] = x[i00] * scale;\n    }\n    if (tpitg == 0) {\n        for (int i00 = 4 * (ne00 / 4); i00 < ne00; i00++) {\n            y_scalar[i00] = x_scalar[i00] * scale;\n        }\n    }\n}\n\n// function for calculate inner product between half a q4_0 block and 16 floats (yl), sumy is SUM(yl[i])\n// il indicates where the q4 quants begin (0 or QK4_0/4)\n// we assume that the yl's have been multiplied with the appropriate scale factor\n// that corresponds to the missing bit shifts (1, 1/16, 1/256, 1/4096)\ninline float block_q_n_dot_y(device const block_q4_0 * qb_curr, float sumy, thread float * yl, int il) {\n    float d = qb_curr->d;\n\n    float2 acc = 0.f;\n\n    device const uint16_t * qs = ((device const uint16_t *)qb_curr + 1 + il/2);\n\n    for (int i = 0; i < 8; i+=2) {\n        acc[0] += yl[i + 0] * (qs[i / 2] & 0x000F)\n                + yl[i + 1] * (qs[i / 2] & 0x0F00);\n        acc[1] += yl[i + 8] * (qs[i / 2] & 0x00F0)\n                + yl[i + 9] * (qs[i / 2] & 0xF000);\n    }\n    return d * (sumy * -8.f + acc[0] + acc[1]);\n}\n\n// function for calculate inner product between half a q4_1 block and 16 floats (yl), sumy is SUM(yl[i])\n// il indicates where the q4 quants begin (0 or QK4_0/4)\n// we assume that the yl's have been multiplied with the appropriate scale factor\n// that corresponds to the missing bit shifts (1, 1/16, 1/256, 1/4096)\ninline float block_q_n_dot_y(device const block_q4_1 * qb_curr, float sumy, thread float * yl, int il) {\n    float d = qb_curr->d;\n    float m = qb_curr->m;\n\n    float2 acc = 0.f;\n\n    device const uint16_t * qs = ((device const uint16_t *)qb_curr + 2 + il/2);\n\n    for (int i = 0; i < 8; i+=2) {\n        acc[0] += yl[i + 0] * (qs[i / 2] & 0x000F)\n                + yl[i + 1] * (qs[i / 2] & 0x0F00);\n        acc[1] += yl[i + 8] * (qs[i / 2] & 0x00F0)\n                + yl[i + 9] * (qs[i / 2] & 0xF000);\n    }\n    return d * (acc[0] + acc[1]) + sumy * m;\n}\n\n// function for calculate inner product between half a q5_0 block and 16 floats (yl), sumy is SUM(yl[i])\n// il indicates where the q5 quants begin (0 or QK5_0/4)\n// we assume that the yl's have been multiplied with the appropriate scale factor\n// that corresponds to the missing bit shifts (1, 1/16, 1/256, 1/4096)\ninline float block_q_n_dot_y(device const block_q5_0 * qb_curr, float sumy, thread float * yl, int il) {\n    float d = qb_curr->d;\n\n    float2 acc = 0.f;\n\n    device const uint16_t * qs =  ((device const uint16_t *)qb_curr + 3 + il/2);\n           const uint32_t   qh = *((device const uint32_t *)qb_curr->qh);\n\n    for (int i = 0; i < 8; i+=2) {\n        acc[0] += yl[i + 0] * ((qs[i / 2] & 0x000F) | ((qh >> (i+0+il        ) << 4 ) & 0x00010))\n                + yl[i + 1] * ((qs[i / 2] & 0x0F00) | ((qh >> (i+1+il        ) << 12) & 0x01000));\n        acc[1] += yl[i + 8] * ((qs[i / 2] & 0x00F0) | ((qh >> (i+0+il+QK5_0/2) << 8 ) & 0x00100))\n                + yl[i + 9] * ((qs[i / 2] & 0xF000) | ((qh >> (i+1+il+QK5_0/2) << 16) & 0x10000));\n    }\n    return d * (sumy * -16.f + acc[0] + acc[1]);\n}\n\n// function for calculate inner product between half a q5_1 block and 16 floats (yl), sumy is SUM(yl[i])\n// il indicates where the q5 quants begin (0 or QK5_1/4)\n// we assume that the yl's have been multiplied with the appropriate scale factor\n// that corresponds to the missing bit shifts (1, 1/16, 1/256, 1/4096)\ninline float block_q_n_dot_y(device const block_q5_1 * qb_curr, float sumy, thread float * yl, int il) {\n    float d = qb_curr->d;\n    float m = qb_curr->m;\n\n    float2 acc = 0.f;\n\n    device const uint16_t * qs =  ((device const uint16_t *)qb_curr + 4 + il/2);\n           const uint32_t   qh = *((device const uint32_t *)qb_curr->qh);\n\n    for (int i = 0; i < 8; i+=2) {\n        acc[0] += yl[i + 0] * ((qs[i / 2] & 0x000F) | ((qh >> (i+0+il        ) << 4 ) & 0x00010))\n                + yl[i + 1] * ((qs[i / 2] & 0x0F00) | ((qh >> (i+1+il        ) << 12) & 0x01000));\n        acc[1] += yl[i + 8] * ((qs[i / 2] & 0x00F0) | ((qh >> (i+0+il+QK5_0/2) << 8 ) & 0x00100))\n                + yl[i + 9] * ((qs[i / 2] & 0xF000) | ((qh >> (i+1+il+QK5_0/2) << 16) & 0x10000));\n    }\n    return d * (acc[0] + acc[1]) + sumy * m;\n}\n\n// putting them in the kernel cause a significant performance penalty\n#define N_DST 4        // each SIMD group works on 4 rows\n#define N_SIMDGROUP 2  // number of SIMD groups in a thread group\n#define N_SIMDWIDTH 32 // assuming SIMD group size is 32\n//Note: This is a template, but strictly speaking it only applies to\n//      quantizations where the block size is 32. It also does not\n//      giard against the number of rows not being divisible by\n//      N_DST, so this is another explicit assumption of the implementation.\ntemplate<typename block_q_type, int nr, int nsg, int nw>\nvoid mul_vec_q_n_f32(device const void * src0, device const float * src1, device float * dst,\n                    int64_t ne00, int64_t ne01, int64_t ne02, int64_t ne10, int64_t ne12, int64_t ne0, int64_t ne1, uint gqa,\n                    uint3 tgpig, uint tiisg, uint sgitg) {\n    const int nb = ne00/QK4_0;\n\n    const int r0 = tgpig.x;\n    const int r1 = tgpig.y;\n    const int im = tgpig.z;\n\n    const int first_row = (r0 * nsg + sgitg) * nr;\n\n    const uint offset0 = first_row * nb + im/gqa*(nb*ne0);\n\n    device const block_q_type * x = (device const block_q_type *) src0 + offset0;\n    device const float        * y = (device const float        *) src1 + r1*ne10 + im*ne00*ne1;\n\n    float yl[16]; // src1 vector cache\n    float sumf[nr] = {0.f};\n\n    const int ix = (tiisg/2);\n    const int il = (tiisg%2)*8;\n\n    device const float * yb = y + ix * QK4_0 + il;\n\n    // each thread in a SIMD group deals with half a block.\n    for (int ib = ix; ib < nb; ib += nw/2) {\n        float sumy = 0;\n        for (int i = 0; i < 8; i += 2) {\n            sumy += yb[i] + yb[i+1];\n            yl[i+0] = yb[i+ 0];\n            yl[i+1] = yb[i+ 1]/256.f;\n\n            sumy += yb[i+16] + yb[i+17];\n            yl[i+8] = yb[i+16]/16.f;\n            yl[i+9] = yb[i+17]/4096.f;\n        }\n\n        for (int row = 0; row < nr; row++) {\n            sumf[row] += block_q_n_dot_y(x+ib+row*nb, sumy, yl, il);\n        }\n\n        yb += QK4_0 * 16;\n    }\n\n    for (int row = 0; row < nr; ++row) {\n        const float tot = simd_sum(sumf[row]);\n        if (tiisg == 0 && first_row + row < ne01) {\n            dst[im*ne0*ne1 + r1*ne0 + first_row + row] = tot;\n        }\n    }\n}\n\nkernel void kernel_mul_mv_q4_0_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint  tiisg[[thread_index_in_simdgroup]],\n        uint  sgitg[[simdgroup_index_in_threadgroup]]) {\n    mul_vec_q_n_f32<block_q4_0, N_DST, N_SIMDGROUP, N_SIMDWIDTH>(src0,src1,dst,ne00,ne01,ne02,ne10,ne12,ne0,ne1,gqa,tgpig,tiisg,sgitg);\n}\n\nkernel void kernel_mul_mv_q4_1_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n     mul_vec_q_n_f32<block_q4_1, N_DST, N_SIMDGROUP, N_SIMDWIDTH>(src0,src1,dst,ne00,ne01,ne02,ne10,ne12,ne0,ne1,gqa,tgpig,tiisg,sgitg);\n}\n\nkernel void kernel_mul_mv_q5_0_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint  tiisg[[thread_index_in_simdgroup]],\n        uint  sgitg[[simdgroup_index_in_threadgroup]]) {\n    mul_vec_q_n_f32<block_q5_0, N_DST, N_SIMDGROUP, N_SIMDWIDTH>(src0,src1,dst,ne00,ne01,ne02,ne10,ne12,ne0,ne1,gqa,tgpig,tiisg,sgitg);\n}\n\nkernel void kernel_mul_mv_q5_1_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint  tiisg[[thread_index_in_simdgroup]],\n        uint  sgitg[[simdgroup_index_in_threadgroup]]) {\n    mul_vec_q_n_f32<block_q5_1, N_DST, N_SIMDGROUP, N_SIMDWIDTH>(src0,src1,dst,ne00,ne01,ne02,ne10,ne12,ne0,ne1,gqa,tgpig,tiisg,sgitg);\n}\n\n\n#define NB_Q8_0 8\n\nkernel void kernel_mul_mv_q8_0_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n    const int nr  = N_DST;\n    const int nsg = N_SIMDGROUP;\n    const int nw  = N_SIMDWIDTH;\n\n    const int nb = ne00/QK8_0;\n    const int r0 = tgpig.x;\n    const int r1 = tgpig.y;\n    const int im = tgpig.z;\n    const int first_row = (r0 * nsg + sgitg) * nr;\n    const uint offset0 = first_row * nb + im/gqa*(nb*ne0);\n    device const block_q8_0 * x = (device const block_q8_0 *) src0 + offset0;\n    device const float      * y = (device const float      *) src1 + r1*ne10 + im*ne00*ne1;\n\n    float yl[NB_Q8_0];\n    float sumf[nr]={0.f};\n\n    const int ix = tiisg/4;\n    const int il = tiisg%4;\n\n    device const float * yb = y + ix * QK8_0 + NB_Q8_0*il;\n\n    // each thread in a SIMD group deals with NB_Q8_0 quants at a time\n    for (int ib = ix; ib < nb; ib += nw/4) {\n        for (int i = 0; i < NB_Q8_0; ++i) {\n            yl[i] = yb[i];\n        }\n\n        for (int row = 0; row < nr; row++) {\n            device const int8_t * qs = x[ib+row*nb].qs + NB_Q8_0*il;\n            float sumq = 0.f;\n            for (int iq = 0; iq < NB_Q8_0; ++iq) {\n                sumq += qs[iq] * yl[iq];\n            }\n            sumf[row] += sumq*x[ib+row*nb].d;\n        }\n\n        yb += NB_Q8_0 * nw;\n    }\n\n    for (int row = 0; row < nr; ++row) {\n        const float tot = simd_sum(sumf[row]);\n        if (tiisg == 0 && first_row + row < ne01) {\n            dst[r1*ne0 + im*ne0*ne1 + first_row + row] = tot;\n        }\n    }\n}\n\n#define N_F32_F32 4\n\nkernel void kernel_mul_mv_f32_f32(\n        device const  char * src0,\n        device const  char * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant   int64_t & ne10,\n        constant   int64_t & ne11,\n        constant   int64_t & ne12,\n        constant  uint64_t & nb10,\n        constant  uint64_t & nb11,\n        constant  uint64_t & nb12,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint  tiisg[[thread_index_in_simdgroup]]) {\n\n    const int64_t r0 = tgpig.x;\n    const int64_t rb = tgpig.y*N_F32_F32;\n    const int64_t im = tgpig.z;\n\n    device const float * x = (device const float *) (src0 + r0*nb01 + im/(ne12/ne02)*nb02);\n\n    if (ne00 < 128) {\n        for (int row = 0; row < N_F32_F32; ++row) {\n            int r1 = rb + row;\n            if (r1 >= ne11) {\n                break;\n            }\n\n            device const float * y = (device const float *) (src1 + r1*nb11 + im*nb12);\n\n            float sumf = 0;\n            for (int i = tiisg; i < ne00; i += 32) {\n                sumf += (float) x[i] * (float) y[i];\n            }\n\n            float all_sum = simd_sum(sumf);\n            if (tiisg == 0) {\n                dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n            }\n        }\n    } else {\n        device const float4 * x4 = (device const float4 *)x;\n        for (int row = 0; row < N_F32_F32; ++row) {\n            int r1 = rb + row;\n            if (r1 >= ne11) {\n                break;\n            }\n\n            device const float  * y  = (device const float  *) (src1 + r1*nb11 + im*nb12);\n            device const float4 * y4 = (device const float4 *) y;\n\n            float sumf = 0;\n            for (int i = tiisg; i < ne00/4; i += 32) {\n                for (int k = 0; k < 4; ++k) sumf += (float) x4[i][k] * y4[i][k];\n            }\n\n            float all_sum = simd_sum(sumf);\n            if (tiisg == 0) {\n                for (int i = 4*(ne00/4); i < ne00; ++i) all_sum += (float) x[i] * y[i];\n                dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n            }\n        }\n    }\n}\n\n#define N_F16_F16 4\n\nkernel void kernel_mul_mv_f16_f16(\n        device const  char * src0,\n        device const  char * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant   int64_t & ne10,\n        constant   int64_t & ne11,\n        constant   int64_t & ne12,\n        constant  uint64_t & nb10,\n        constant  uint64_t & nb11,\n        constant  uint64_t & nb12,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint  tiisg[[thread_index_in_simdgroup]]) {\n\n    const int64_t r0 = tgpig.x;\n    const int64_t rb = tgpig.y*N_F16_F16;\n    const int64_t im = tgpig.z;\n\n    device const half * x = (device const half *) (src0 + r0*nb01 + im/(ne12/ne02)*nb02);\n\n    if (ne00 < 128) {\n        for (int row = 0; row < N_F16_F16; ++row) {\n            int r1 = rb + row;\n            if (r1 >= ne11) {\n                break;\n            }\n\n            device const half * y = (device const half *) (src1 + r1*nb11 + im*nb12);\n\n            float sumf = 0;\n            for (int i = tiisg; i < ne00; i += 32) {\n                sumf += (half) x[i] * (half) y[i];\n            }\n\n            float all_sum = simd_sum(sumf);\n            if (tiisg == 0) {\n                dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n            }\n        }\n    } else {\n        device const half4 * x4 = (device const half4 *)x;\n        for (int row = 0; row < N_F16_F16; ++row) {\n            int r1 = rb + row;\n            if (r1 >= ne11) {\n                break;\n            }\n\n            device const half  * y  = (device const half  *) (src1 + r1*nb11 + im*nb12);\n            device const half4 * y4 = (device const half4 *) y;\n\n            float sumf = 0;\n            for (int i = tiisg; i < ne00/4; i += 32) {\n                for (int k = 0; k < 4; ++k) sumf += (half) x4[i][k] * y4[i][k];\n            }\n\n            float all_sum = simd_sum(sumf);\n            if (tiisg == 0) {\n                for (int i = 4*(ne00/4); i < ne00; ++i) all_sum += (half) x[i] * y[i];\n                dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n            }\n        }\n    }\n}\n\nkernel void kernel_mul_mv_f16_f32_1row(\n        device const  char * src0,\n        device const  char * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant   int64_t & ne10,\n        constant   int64_t & ne11,\n        constant   int64_t & ne12,\n        constant  uint64_t & nb10,\n        constant  uint64_t & nb11,\n        constant  uint64_t & nb12,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint  tiisg[[thread_index_in_simdgroup]]) {\n\n    const int64_t r0 = tgpig.x;\n    const int64_t r1 = tgpig.y;\n    const int64_t im = tgpig.z;\n\n    device const half  * x = (device const half  *) (src0 + r0*nb01 + im/(ne12/ne02)*nb02);\n    device const float * y = (device const float *) (src1 + r1*nb11 + im*nb12);\n\n    float sumf = 0;\n    if (ne00 < 128) {\n        for (int i = tiisg; i < ne00; i += 32) {\n            sumf += (float) x[i] * (float) y[i];\n        }\n        float all_sum = simd_sum(sumf);\n        if (tiisg == 0) {\n            dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n        }\n    } else {\n        device const half4  * x4 = (device const half4  *) x;\n        device const float4 * y4 = (device const float4 *) y;\n        for (int i = tiisg; i < ne00/4; i += 32) {\n            for (int k = 0; k < 4; ++k) sumf += (float)x4[i][k] * y4[i][k];\n        }\n        float all_sum = simd_sum(sumf);\n        if (tiisg == 0) {\n            for (int i = 4*(ne00/4); i < ne00; ++i) all_sum += (float) x[i] * y[i];\n            dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n        }\n    }\n\n}\n\n#define N_F16_F32 4\n\nkernel void kernel_mul_mv_f16_f32(\n        device const  char * src0,\n        device const  char * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant   int64_t & ne10,\n        constant   int64_t & ne11,\n        constant   int64_t & ne12,\n        constant  uint64_t & nb10,\n        constant  uint64_t & nb11,\n        constant  uint64_t & nb12,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]]) {\n\n    const int64_t r0 = tgpig.x;\n    const int64_t rb = tgpig.y*N_F16_F32;\n    const int64_t im = tgpig.z;\n\n    device const half * x = (device const half *) (src0 + r0*nb01 + im/(ne12/ne02)*nb02);\n\n    if (ne00 < 128) {\n        for (int row = 0; row < N_F16_F32; ++row) {\n            int r1 = rb + row;\n            if (r1 >= ne11) {\n                break;\n            }\n\n            device const float * y = (device const float *) (src1 + r1*nb11 + im*nb12);\n\n            float sumf = 0;\n            for (int i = tiisg; i < ne00; i += 32) {\n                sumf += (float) x[i] * (float) y[i];\n            }\n\n            float all_sum = simd_sum(sumf);\n            if (tiisg == 0) {\n                dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n            }\n        }\n    } else {\n        device const half4 * x4 = (device const half4 *)x;\n        for (int row = 0; row < N_F16_F32; ++row) {\n            int r1 = rb + row;\n            if (r1 >= ne11) {\n                break;\n            }\n\n            device const float  * y  = (device const float  *) (src1 + r1*nb11 + im*nb12);\n            device const float4 * y4 = (device const float4 *) y;\n\n            float sumf = 0;\n            for (int i = tiisg; i < ne00/4; i += 32) {\n                for (int k = 0; k < 4; ++k) sumf += (float) x4[i][k] * y4[i][k];\n            }\n\n            float all_sum = simd_sum(sumf);\n            if (tiisg == 0) {\n                for (int i = 4*(ne00/4); i < ne00; ++i) all_sum += (float) x[i] * y[i];\n                dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n            }\n        }\n    }\n}\n\n// Assumes row size (ne00) is a multiple of 4\nkernel void kernel_mul_mv_f16_f32_l4(\n        device const  char * src0,\n        device const  char * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant   int64_t & ne10,\n        constant   int64_t & ne11,\n        constant   int64_t & ne12,\n        constant  uint64_t & nb10,\n        constant  uint64_t & nb11,\n        constant  uint64_t & nb12,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]]) {\n\n    const int nrows = ne11;\n    const int64_t r0 = tgpig.x;\n    const int64_t im = tgpig.z;\n\n    device const half4 * x4 = (device const half4 *) (src0 + r0*nb01 + im/(ne12/ne02)*nb02);\n\n    for (int r1 = 0; r1 < nrows; ++r1) {\n        device const float4 * y4 = (device const float4 *) (src1 + r1*nb11 + im*nb12);\n\n        float sumf = 0;\n        for (int i = tiisg; i < ne00/4; i += 32) {\n            for (int k = 0; k < 4; ++k) sumf += (float) x4[i][k] * y4[i][k];\n        }\n\n        float all_sum = simd_sum(sumf);\n        if (tiisg == 0) {\n            dst[im*ne1*ne0 + r1*ne0 + r0] = all_sum;\n        }\n    }\n}\n\nkernel void kernel_alibi_f32(\n        device const float * src0,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant   int64_t & ne03,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant  uint64_t & nb03,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        constant   int64_t & ne2,\n        constant   int64_t & ne3,\n        constant  uint64_t & nb0,\n        constant  uint64_t & nb1,\n        constant  uint64_t & nb2,\n        constant  uint64_t & nb3,\n        constant     float & m0,\n        constant     float & m1,\n        constant       int & n_heads_log2_floor,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint3 tpitg[[thread_position_in_threadgroup]],\n        uint3   ntg[[threads_per_threadgroup]]) {\n    const int64_t i03 = tgpig[2];\n    const int64_t i02 = tgpig[1];\n    const int64_t i01 = tgpig[0];\n\n    const int64_t n = i03*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00;\n\n    const int64_t i3 = n / (ne2*ne1*ne0);\n    const int64_t i2 = (n - i3*ne2*ne1*ne0) / (ne1*ne0);\n    const int64_t i1 = (n - i3*ne2*ne1*ne0 - i2*ne1*ne0) / ne0;\n    const int64_t i0 = (n - i3*ne2*ne1*ne0 - i2*ne1*ne0 - i1*ne0);\n\n    device float * dst_data = (device float *) ((device char *) dst + i3*nb3 + i2*nb2 + i1*nb1 + i0*nb0);\n    float m_k;\n    if (i2 < n_heads_log2_floor) {\n        m_k = pow(m0, i2 + 1);\n    } else {\n        m_k = pow(m1, 2 * (i2 - n_heads_log2_floor) + 1);\n    }\n    for (int64_t i00 = tpitg.x; i00 < ne00; i00 += ntg.x) {\n        device const float * src = (device float *)((device char *) src0 + i03*nb03 + i02*nb02 + i01*nb01 + i00*nb00);\n        dst_data[i00] = src[0] + m_k * (i00 - ne00 + 1);\n    }\n}\n\nstatic float rope_yarn_ramp(const float low, const float high, const int i0) {\n    const float y = (i0 / 2 - low) / max(0.001f, high - low);\n    return 1.0f - min(1.0f, max(0.0f, y));\n}\n\n// YaRN algorithm based on LlamaYaRNScaledRotaryEmbedding.py from https://github.com/jquesnelle/yarn\n// MIT licensed. Copyright (c) 2023 Jeffrey Quesnelle and Bowen Peng.\nstatic void rope_yarn(\n    float theta_extrap, float freq_scale, float corr_dims[2], int64_t i0, float ext_factor, float mscale,\n    thread float * cos_theta, thread float * sin_theta\n) {\n    // Get n-d rotational scaling corrected for extrapolation\n    float theta_interp = freq_scale * theta_extrap;\n    float theta = theta_interp;\n    if (ext_factor != 0.0f) {\n        float ramp_mix = rope_yarn_ramp(corr_dims[0], corr_dims[1], i0) * ext_factor;\n        theta = theta_interp * (1 - ramp_mix) + theta_extrap * ramp_mix;\n\n        // Get n-d magnitude scaling corrected for interpolation\n        mscale *= 1.0f + 0.1f * log(1.0f / freq_scale);\n    }\n    *cos_theta = cos(theta) * mscale;\n    *sin_theta = sin(theta) * mscale;\n}\n\n// Apparently solving `n_rot = 2pi * x * base^((2 * max_pos_emb) / n_dims)` for x, we get\n// `corr_fac(n_rot) = n_dims * log(max_pos_emb / (n_rot * 2pi)) / (2 * log(base))`\nstatic float rope_yarn_corr_factor(int n_dims, int n_orig_ctx, float n_rot, float base) {\n    return n_dims * log(n_orig_ctx / (n_rot * 2 * M_PI_F)) / (2 * log(base));\n}\n\nstatic void rope_yarn_corr_dims(\n    int n_dims, int n_orig_ctx, float freq_base, float beta_fast, float beta_slow, float dims[2]\n) {\n    // start and end correction dims\n    dims[0] = max(0.0f,         floor(rope_yarn_corr_factor(n_dims, n_orig_ctx, beta_fast, freq_base)));\n    dims[1] = min(n_dims - 1.0f, ceil(rope_yarn_corr_factor(n_dims, n_orig_ctx, beta_slow, freq_base)));\n}\n\ntypedef void (rope_t)(\n        device const    void * src0,\n        device const int32_t * src1,\n        device         float * dst,\n        constant     int64_t & ne00,\n        constant     int64_t & ne01,\n        constant     int64_t & ne02,\n        constant     int64_t & ne03,\n        constant    uint64_t & nb00,\n        constant    uint64_t & nb01,\n        constant    uint64_t & nb02,\n        constant    uint64_t & nb03,\n        constant     int64_t & ne0,\n        constant     int64_t & ne1,\n        constant     int64_t & ne2,\n        constant     int64_t & ne3,\n        constant    uint64_t & nb0,\n        constant    uint64_t & nb1,\n        constant    uint64_t & nb2,\n        constant    uint64_t & nb3,\n        constant         int & n_past,\n        constant         int & n_dims,\n        constant         int & mode,\n        constant         int & n_orig_ctx,\n        constant       float & freq_base,\n        constant       float & freq_scale,\n        constant       float & ext_factor,\n        constant       float & attn_factor,\n        constant       float & beta_fast,\n        constant       float & beta_slow,\n        uint  tiitg[[thread_index_in_threadgroup]],\n        uint3 tptg[[threads_per_threadgroup]],\n        uint3 tgpig[[threadgroup_position_in_grid]]);\n\ntemplate<typename T>\nkernel void kernel_rope(\n        device const    void * src0,\n        device const int32_t * src1,\n        device         float * dst,\n        constant     int64_t & ne00,\n        constant     int64_t & ne01,\n        constant     int64_t & ne02,\n        constant     int64_t & ne03,\n        constant    uint64_t & nb00,\n        constant    uint64_t & nb01,\n        constant    uint64_t & nb02,\n        constant    uint64_t & nb03,\n        constant     int64_t & ne0,\n        constant     int64_t & ne1,\n        constant     int64_t & ne2,\n        constant     int64_t & ne3,\n        constant    uint64_t & nb0,\n        constant    uint64_t & nb1,\n        constant    uint64_t & nb2,\n        constant    uint64_t & nb3,\n        constant         int & n_past,\n        constant         int & n_dims,\n        constant         int & mode,\n        constant         int & n_orig_ctx,\n        constant       float & freq_base,\n        constant       float & freq_scale,\n        constant       float & ext_factor,\n        constant       float & attn_factor,\n        constant       float & beta_fast,\n        constant       float & beta_slow,\n        uint  tiitg[[thread_index_in_threadgroup]],\n        uint3 tptg[[threads_per_threadgroup]],\n        uint3 tgpig[[threadgroup_position_in_grid]]) {\n    const int64_t i3 = tgpig[2];\n    const int64_t i2 = tgpig[1];\n    const int64_t i1 = tgpig[0];\n\n    const bool is_neox = mode & 2;\n\n    float corr_dims[2];\n    rope_yarn_corr_dims(n_dims, n_orig_ctx, freq_base, beta_fast, beta_slow, corr_dims);\n\n    device const int32_t * pos = src1;\n\n    const int64_t p = pos[i2];\n\n    const float theta_0 = (float)p;\n    const float inv_ndims = -1.f/n_dims;\n\n    if (!is_neox) {\n        for (int64_t i0 = 2*tiitg; i0 < ne0; i0 += 2*tptg.x) {\n\n            const float theta = theta_0 * pow(freq_base, inv_ndims*i0);\n            float cos_theta, sin_theta;\n            rope_yarn(theta, freq_scale, corr_dims, i0, ext_factor, attn_factor, &cos_theta, &sin_theta);\n\n            device const T * const src = (device T *)((device char *) src0 + i3*nb03 + i2*nb02 + i1*nb01 + i0*nb00);\n            device       T * dst_data  = (device T *)((device char *)  dst + i3*nb3  + i2*nb2  + i1*nb1  + i0*nb0);\n\n            const T x0 = src[0];\n            const T x1 = src[1];\n\n            dst_data[0] = x0*cos_theta - x1*sin_theta;\n            dst_data[1] = x0*sin_theta + x1*cos_theta;\n        }\n    } else {\n        for (int64_t ib = 0; ib < ne0/n_dims; ++ib) {\n            for (int64_t ic = 2*tiitg; ic < n_dims; ic += 2*tptg.x) {\n\n                // simplified from `(ib * n_dims + ic) * inv_ndims`\n                const float cur_rot = inv_ndims*ic - ib;\n\n                const float theta = theta_0 * pow(freq_base, cur_rot);\n                float cos_theta, sin_theta;\n                rope_yarn(theta, freq_scale, corr_dims, cur_rot, ext_factor, attn_factor, &cos_theta, &sin_theta);\n\n                const int64_t i0 = ib*n_dims + ic/2;\n\n                device const T * const src = (device T *)((device char *) src0 + i3*nb03 + i2*nb02 + i1*nb01 + i0*nb00);\n                device       T * dst_data  = (device T *)((device char *)  dst + i3*nb3  + i2*nb2  + i1*nb1  + i0*nb0);\n\n                const float x0 = src[0];\n                const float x1 = src[n_dims/2];\n\n                dst_data[0]        = x0*cos_theta - x1*sin_theta;\n                dst_data[n_dims/2] = x0*sin_theta + x1*cos_theta;\n            }\n        }\n    }\n}\n\ntemplate [[host_name(\"kernel_rope_f32\")]] kernel rope_t kernel_rope<float>;\ntemplate [[host_name(\"kernel_rope_f16\")]] kernel rope_t kernel_rope<half>;\n\nkernel void kernel_im2col_f16(\n        device const float * x,\n        device       half * dst,\n        constant   int32_t & ofs0,\n        constant   int32_t & ofs1,\n        constant   int32_t & IW,\n        constant   int32_t & IH,\n        constant   int32_t & CHW,\n        constant   int32_t & s0,\n        constant   int32_t & s1,\n        constant   int32_t & p0,\n        constant   int32_t & p1,\n        constant   int32_t & d0,\n        constant   int32_t & d1,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint3  tgpg[[threadgroups_per_grid]],\n        uint3 tpitg[[thread_position_in_threadgroup]],\n        uint3   ntg[[threads_per_threadgroup]]) {\n    const int32_t iiw = tgpig[2] * s0 + tpitg[2] * d0 - p0;\n    const int32_t iih = tgpig[1] * s1 + tpitg[1] * d1 - p1;\n\n    const int32_t offset_dst =\n        (tpitg[0] * tgpg[1] * tgpg[2] + tgpig[1] * tgpg[2] + tgpig[2]) * CHW +\n        (tgpig[0] * (ntg[1] * ntg[2]) + tpitg[1] * ntg[2] + tpitg[2]);\n\n    if (iih < 0 || iih >= IH || iiw < 0 || iiw >= IW) {\n        dst[offset_dst] = 0.0f;\n    } else {\n        const int32_t offset_src = tpitg[0] * ofs0 + tgpig[0] * ofs1;\n        dst[offset_dst] = x[offset_src + iih * IW + iiw];\n    }\n}\n\nkernel void kernel_cpy_f16_f16(\n        device const half * src0,\n        device       half * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant   int64_t & ne03,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant  uint64_t & nb03,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        constant   int64_t & ne2,\n        constant   int64_t & ne3,\n        constant  uint64_t & nb0,\n        constant  uint64_t & nb1,\n        constant  uint64_t & nb2,\n        constant  uint64_t & nb3,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint3 tpitg[[thread_position_in_threadgroup]],\n        uint3   ntg[[threads_per_threadgroup]]) {\n    const int64_t i03 = tgpig[2];\n    const int64_t i02 = tgpig[1];\n    const int64_t i01 = tgpig[0];\n\n    const int64_t n = i03*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00;\n\n    const int64_t i3 = n / (ne2*ne1*ne0);\n    const int64_t i2 = (n - i3*ne2*ne1*ne0) / (ne1*ne0);\n    const int64_t i1 = (n - i3*ne2*ne1*ne0 - i2*ne1*ne0) / ne0;\n    const int64_t i0 = (n - i3*ne2*ne1*ne0 - i2*ne1*ne0 - i1*ne0);\n\n    device half * dst_data = (device half *) ((device char *) dst + i3*nb3 + i2*nb2 + i1*nb1 + i0*nb0);\n\n    for (int64_t i00 = tpitg.x; i00 < ne00; i00 += ntg.x) {\n        device const half * src = (device half *)((device char *) src0 + i03*nb03 + i02*nb02 + i01*nb01 + i00*nb00);\n        dst_data[i00] = src[0];\n    }\n}\n\nkernel void kernel_cpy_f32_f16(\n        device const float * src0,\n        device        half * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant   int64_t & ne03,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant  uint64_t & nb03,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        constant   int64_t & ne2,\n        constant   int64_t & ne3,\n        constant  uint64_t & nb0,\n        constant  uint64_t & nb1,\n        constant  uint64_t & nb2,\n        constant  uint64_t & nb3,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint3 tpitg[[thread_position_in_threadgroup]],\n        uint3   ntg[[threads_per_threadgroup]]) {\n    const int64_t i03 = tgpig[2];\n    const int64_t i02 = tgpig[1];\n    const int64_t i01 = tgpig[0];\n\n    const int64_t n = i03*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00;\n\n    const int64_t i3 = n / (ne2*ne1*ne0);\n    const int64_t i2 = (n - i3*ne2*ne1*ne0) / (ne1*ne0);\n    const int64_t i1 = (n - i3*ne2*ne1*ne0 - i2*ne1*ne0) / ne0;\n    const int64_t i0 = (n - i3*ne2*ne1*ne0 - i2*ne1*ne0 - i1*ne0);\n\n    device half * dst_data = (device half *) ((device char *) dst + i3*nb3 + i2*nb2 + i1*nb1 + i0*nb0);\n\n    for (int64_t i00 = tpitg.x; i00 < ne00; i00 += ntg.x) {\n        device const float * src = (device float *)((device char *) src0 + i03*nb03 + i02*nb02 + i01*nb01 + i00*nb00);\n\n        dst_data[i00] = src[0];\n    }\n}\n\nkernel void kernel_cpy_f32_f32(\n        device const float * src0,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01,\n        constant   int64_t & ne02,\n        constant   int64_t & ne03,\n        constant  uint64_t & nb00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb02,\n        constant  uint64_t & nb03,\n        constant   int64_t & ne0,\n        constant   int64_t & ne1,\n        constant   int64_t & ne2,\n        constant   int64_t & ne3,\n        constant  uint64_t & nb0,\n        constant  uint64_t & nb1,\n        constant  uint64_t & nb2,\n        constant  uint64_t & nb3,\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint3 tpitg[[thread_position_in_threadgroup]],\n        uint3   ntg[[threads_per_threadgroup]]) {\n    const int64_t i03 = tgpig[2];\n    const int64_t i02 = tgpig[1];\n    const int64_t i01 = tgpig[0];\n\n    const int64_t n = i03*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00;\n\n    const int64_t i3 = n / (ne2*ne1*ne0);\n    const int64_t i2 = (n - i3*ne2*ne1*ne0) / (ne1*ne0);\n    const int64_t i1 = (n - i3*ne2*ne1*ne0 - i2*ne1*ne0) / ne0;\n    const int64_t i0 = (n - i3*ne2*ne1*ne0 - i2*ne1*ne0 - i1*ne0);\n\n    device float * dst_data = (device float *) ((device char *) dst + i3*nb3 + i2*nb2 + i1*nb1 + i0*nb0);\n\n    for (int64_t i00 = tpitg.x; i00 < ne00; i00 += ntg.x) {\n        device const float * src = (device float *)((device char *) src0 + i03*nb03 + i02*nb02 + i01*nb01 + i00*nb00);\n\n        dst_data[i00] = src[0];\n    }\n}\n\nkernel void kernel_concat(\n    device const char * src0,\n    device const char * src1,\n    device       char * dst,\n    constant   int64_t & ne00,\n    constant   int64_t & ne01,\n    constant   int64_t & ne02,\n    constant   int64_t & ne03,\n    constant  uint64_t & nb00,\n    constant  uint64_t & nb01,\n    constant  uint64_t & nb02,\n    constant  uint64_t & nb03,\n    constant   int64_t & ne10,\n    constant   int64_t & ne11,\n    constant   int64_t & ne12,\n    constant   int64_t & ne13,\n    constant  uint64_t & nb10,\n    constant  uint64_t & nb11,\n    constant  uint64_t & nb12,\n    constant  uint64_t & nb13,\n    constant   int64_t & ne0,\n    constant   int64_t & ne1,\n    constant   int64_t & ne2,\n    constant   int64_t & ne3,\n    constant  uint64_t & nb0,\n    constant  uint64_t & nb1,\n    constant  uint64_t & nb2,\n    constant  uint64_t & nb3,\n    uint3 tgpig[[threadgroup_position_in_grid]],\n    uint3 tpitg[[thread_position_in_threadgroup]],\n    uint3   ntg[[threads_per_threadgroup]]) {\n\n    const int64_t i03 = tgpig.z;\n    const int64_t i02 = tgpig.y;\n    const int64_t i01 = tgpig.x;\n\n    const int64_t i13 = i03 % ne13;\n    const int64_t i12 = i02 % ne12;\n    const int64_t i11 = i01 % ne11;\n\n    device const char * src0_ptr = src0 + i03 * nb03 + i02 * nb02 + i01 * nb01 + tpitg.x*nb00;\n    device const char * src1_ptr = src1 + i13*nb13 + i12*nb12 + i11*nb11 + tpitg.x*nb10;\n    device       char * dst_ptr  = dst  + i03*nb3  + i02*nb2  + i01*nb1  + tpitg.x*nb0;\n\n    for (int i0 = tpitg.x; i0 < ne0; i0 += ntg.x) {\n        if (i02 < ne02) {\n            ((device float *)dst_ptr)[0] = ((device float *)src0_ptr)[0];\n            src0_ptr += ntg.x*nb00;\n        } else {\n            ((device float *)dst_ptr)[0] = ((device float *)src1_ptr)[0];\n            src1_ptr += ntg.x*nb10;\n        }\n        dst_ptr += ntg.x*nb0;\n    }\n}\n\n//============================================ k-quants ======================================================\n\n#ifndef QK_K\n#define QK_K 256\n#else\nstatic_assert(QK_K == 256 || QK_K == 64, \"QK_K must be 256 or 64\");\n#endif\n\n#if QK_K == 256\n#define K_SCALE_SIZE 12\n#else\n#define K_SCALE_SIZE 4\n#endif\n\ntypedef struct {\n    uint8_t scales[QK_K/16]; // scales and mins, quantized with 4 bits\n    uint8_t qs[QK_K/4];      // quants\n    half d;           // super-block scale for quantized scales\n    half dmin;        // super-block scale for quantized mins\n} block_q2_K;\n// 84 bytes / block\n\ntypedef struct {\n    uint8_t hmask[QK_K/8];     // quants - high bit\n    uint8_t qs[QK_K/4];        // quants - low 2 bits\n#if QK_K == 64\n    uint8_t scales[2];\n#else\n    uint8_t scales[K_SCALE_SIZE]; // scales, quantized with 6 bits\n#endif\n    half d;             // super-block scale\n} block_q3_K;\n\n#if QK_K == 64\ntypedef struct {\n    half    d[2];          // super-block scales/mins\n    uint8_t scales[2];\n    uint8_t qs[QK_K/2];    // 4-bit quants\n} block_q4_K;\n#else\ntypedef struct {\n    half d;             // super-block scale for quantized scales\n    half dmin;          // super-block scale for quantized mins\n    uint8_t scales[K_SCALE_SIZE]; // scales and mins, quantized with 6 bits\n    uint8_t qs[QK_K/2];        // 4--bit quants\n} block_q4_K;\n#endif\n\n#if QK_K == 64\ntypedef struct {\n    half  d;                     // super-block scales/mins\n    int8_t  scales[QK_K/16];     // 8-bit block scales\n    uint8_t qh[QK_K/8];          // quants, high bit\n    uint8_t qs[QK_K/2];          // quants, low 4 bits\n} block_q5_K;\n#else\ntypedef struct {\n    half d;                      // super-block scale for quantized scales\n    half dmin;                   // super-block scale for quantized mins\n    uint8_t scales[3*QK_K/64];   // scales and mins, quantized with 6 bits\n    uint8_t qh[QK_K/8];          // quants, high bit\n    uint8_t qs[QK_K/2];          // quants, low 4 bits\n} block_q5_K;\n// 176 bytes / block\n#endif\n\ntypedef struct {\n    uint8_t ql[QK_K/2];      // quants, lower 4 bits\n    uint8_t qh[QK_K/4];      // quants, upper 2 bits\n    int8_t  scales[QK_K/16]; // scales, quantized with 8 bits\n    half d;                  // super-block scale\n} block_q6_K;\n// 210 bytes / block\n\nstatic inline uchar4 get_scale_min_k4(int j, device const uint8_t * q) {\n    uchar4 r;\n    if (j < 4) {\n        r[0] = q[j+0] & 63;\n        r[2] = q[j+1] & 63;\n        r[1] = q[j+4] & 63;\n        r[3] = q[j+5] & 63;\n    } else {\n        r[0] = (q[j+4] & 0xF) | ((q[j-4] >> 6) << 4);\n        r[2] = (q[j+5] & 0xF) | ((q[j-3] >> 6) << 4);\n        r[1] = (q[j+4] >>  4) | ((q[j-0] >> 6) << 4);\n        r[3] = (q[j+5] >>  4) | ((q[j+1] >> 6) << 4);\n    }\n    return r;\n}\n\n//====================================== dot products =========================\n\nkernel void kernel_mul_mv_q2_K_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n\n    const int nb = ne00/QK_K;\n    const int r0 = tgpig.x;\n    const int r1 = tgpig.y;\n    const int r2 = tgpig.z;\n\n    const int first_row = (r0 * N_SIMDGROUP + sgitg) * N_DST;\n    const int ib_row = first_row * nb;\n    const uint offset0 = r2/gqa*(nb*ne0);\n    device const block_q2_K * x = (device const block_q2_K *) src0 + ib_row + offset0;\n    device const float      * y = (device const float      *) src1 + r1*ne10 + r2*ne00*ne1;\n    float yl[32];\n    float sumf[N_DST]={0.f}, all_sum;\n\n    const int step = sizeof(block_q2_K) * nb;\n\n#if QK_K == 256\n    const int ix = tiisg/8;  // 0...3\n    const int it = tiisg%8;  // 0...7\n    const int im = it/4;     // 0 or 1\n    const int ir = it%4;     // 0...3\n    const int is = (8*ir)/16;// 0 or 1\n\n    device const float * y4 = y + ix * QK_K + 128 * im + 8 * ir;\n\n    for (int ib = ix; ib < nb; ib += 4) {\n\n        float4 sumy = {0.f, 0.f, 0.f, 0.f};\n        for (int i = 0; i < 8; ++i) {\n            yl[i+ 0] = y4[i+ 0]; sumy[0] += yl[i+ 0];\n            yl[i+ 8] = y4[i+32]; sumy[1] += yl[i+ 8];\n            yl[i+16] = y4[i+64]; sumy[2] += yl[i+16];\n            yl[i+24] = y4[i+96]; sumy[3] += yl[i+24];\n        }\n\n        device const uint8_t  * sc = (device const uint8_t  *)x[ib].scales + 8*im + is;\n        device const uint16_t * qs = (device const uint16_t *)x[ib].qs + 16 * im + 4 * ir;\n        device const half     * dh = &x[ib].d;\n\n        for (int row = 0; row < N_DST; row++) {\n\n            float4 acc1 = {0.f, 0.f, 0.f, 0.f};\n            float4 acc2 = {0.f, 0.f, 0.f, 0.f};\n            for (int i = 0; i < 8; i += 2) {\n                acc1[0] += yl[i+ 0] * (qs[i/2] & 0x0003);\n                acc2[0] += yl[i+ 1] * (qs[i/2] & 0x0300);\n                acc1[1] += yl[i+ 8] * (qs[i/2] & 0x000c);\n                acc2[1] += yl[i+ 9] * (qs[i/2] & 0x0c00);\n                acc1[2] += yl[i+16] * (qs[i/2] & 0x0030);\n                acc2[2] += yl[i+17] * (qs[i/2] & 0x3000);\n                acc1[3] += yl[i+24] * (qs[i/2] & 0x00c0);\n                acc2[3] += yl[i+25] * (qs[i/2] & 0xc000);\n            }\n            float dall = dh[0];\n            float dmin = dh[1] * 1.f/16.f;\n            sumf[row] += dall * ((acc1[0] + 1.f/256.f * acc2[0]) * (sc[0] & 0xF) * 1.f/ 1.f +\n                                 (acc1[1] + 1.f/256.f * acc2[1]) * (sc[2] & 0xF) * 1.f/ 4.f +\n                                 (acc1[2] + 1.f/256.f * acc2[2]) * (sc[4] & 0xF) * 1.f/16.f +\n                                 (acc1[3] + 1.f/256.f * acc2[3]) * (sc[6] & 0xF) * 1.f/64.f) -\n                         dmin * (sumy[0] * (sc[0] & 0xF0) + sumy[1] * (sc[2] & 0xF0) + sumy[2] * (sc[4] & 0xF0) + sumy[3] * (sc[6] & 0xF0));\n\n            qs += step/2;\n            sc += step;\n            dh += step/2;\n        }\n\n        y4 += 4 * QK_K;\n    }\n#else\n    const int ix = tiisg/2;  // 0...15\n    const int it = tiisg%2;  // 0...1\n\n    device const float * y4 = y + ix * QK_K + 8 * it;\n\n    for (int ib = ix; ib < nb; ib += 16) {\n\n        float4 sumy = {0.f, 0.f, 0.f, 0.f};\n        for (int i = 0; i < 8; ++i) {\n            yl[i+ 0] = y4[i+ 0]; sumy[0] += yl[i+ 0];\n            yl[i+ 8] = y4[i+16]; sumy[1] += yl[i+ 8];\n            yl[i+16] = y4[i+32]; sumy[2] += yl[i+16];\n            yl[i+24] = y4[i+48]; sumy[3] += yl[i+24];\n        }\n\n        device const uint8_t  * sc = (device const uint8_t  *)x[ib].scales;\n        device const uint16_t * qs = (device const uint16_t *)x[ib].qs + 4 * it;\n        device const half     * dh = &x[ib].d;\n\n        for (int row = 0; row < N_DST; row++) {\n\n            float4 acc1 = {0.f, 0.f, 0.f, 0.f};\n            float4 acc2 = {0.f, 0.f, 0.f, 0.f};\n            for (int i = 0; i < 8; i += 2) {\n                acc1[0] += yl[i+ 0] * (qs[i/2] & 0x0003);\n                acc2[0] += yl[i+ 1] * (qs[i/2] & 0x0300);\n                acc1[1] += yl[i+ 8] * (qs[i/2] & 0x000c);\n                acc2[1] += yl[i+ 9] * (qs[i/2] & 0x0c00);\n                acc1[2] += yl[i+16] * (qs[i/2] & 0x0030);\n                acc2[2] += yl[i+17] * (qs[i/2] & 0x3000);\n                acc1[3] += yl[i+24] * (qs[i/2] & 0x00c0);\n                acc2[3] += yl[i+25] * (qs[i/2] & 0xc000);\n            }\n\n            float dall = dh[0];\n            float dmin = dh[1];\n            sumf[row] += dall * ((acc1[0] + 1.f/256.f * acc2[0]) * (sc[0] & 0xF) * 1.f/ 1.f +\n                                 (acc1[1] + 1.f/256.f * acc2[1]) * (sc[1] & 0xF) * 1.f/ 4.f +\n                                 (acc1[2] + 1.f/256.f * acc2[2]) * (sc[2] & 0xF) * 1.f/16.f +\n                                 (acc1[3] + 1.f/256.f * acc2[3]) * (sc[3] & 0xF) * 1.f/64.f) -\n                         dmin * (sumy[0] * (sc[0] >> 4) + sumy[1] * (sc[1] >> 4) + sumy[2] * (sc[2] >> 4) + sumy[3] * (sc[3] >> 4));\n\n            qs += step/2;\n            sc += step;\n            dh += step/2;\n        }\n\n        y4 += 16 * QK_K;\n    }\n#endif\n\n    for (int row = 0; row < N_DST; ++row) {\n        all_sum = simd_sum(sumf[row]);\n        if (tiisg == 0) {\n            dst[r1*ne0 + r2*ne0*ne1 + first_row + row] = all_sum;\n        }\n    }\n}\n\n#if QK_K == 256\nkernel void kernel_mul_mv_q3_K_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n\n    const int nb = ne00/QK_K;\n\n    const int64_t r0 = tgpig.x;\n    const int64_t r1 = tgpig.y;\n    const int64_t r2 = tgpig.z;\n\n    const int first_row = (r0 * N_SIMDGROUP + sgitg) * 2;\n    const uint offset0 = r2/gqa*(nb*ne0);\n    device const block_q3_K * x = (device const block_q3_K *) src0 + first_row*nb + offset0;\n    device const float     * yy = (device const float      *) src1 + r1*ne10 + r2*ne00*ne1;\n\n    float yl[32];\n\n    //const uint16_t kmask1 = 0x3030;\n    //const uint16_t kmask2 = 0x0f0f;\n\n    const int tid = tiisg/4;\n    const int ix  = tiisg%4;\n    const int ip  = tid/4;          // 0 or 1\n    const int il  = 2*((tid%4)/2);  // 0 or 2\n    const int ir  = tid%2;\n    const int n   = 8;\n    const int l0  = n*ir;\n\n    // One would think that the Metal compiler would figure out that ip and il can only have\n    // 4 possible states, and optimize accordingly. Well, no. It needs help, and we do it\n    // with these two tales.\n    //\n    // Possible masks for the high bit\n    const ushort4 mm[4] = {{0x0001, 0x0100, 0x0002, 0x0200},  // ip = 0, il = 0\n                           {0x0004, 0x0400, 0x0008, 0x0800},  // ip = 0, il = 2\n                           {0x0010, 0x1000, 0x0020, 0x2000},  // ip = 1, il = 0\n                           {0x0040, 0x4000, 0x0080, 0x8000}}; // ip = 1, il = 2\n\n    // Possible masks for the low 2 bits\n    const int4 qm[2] = {{0x0003, 0x0300, 0x000c, 0x0c00}, {0x0030, 0x3000, 0x00c0, 0xc000}};\n\n    const ushort4 hm = mm[2*ip + il/2];\n\n    const int shift = 2*il;\n    const float    v1 = il == 0 ? 4.f : 64.f;\n    const float    v2 = 4.f * v1;\n\n    const uint16_t s_shift1 = 4*ip;\n    const uint16_t s_shift2 = s_shift1 + il;\n\n    const int q_offset = 32*ip + l0;\n    const int y_offset = 128*ip + 32*il + l0;\n\n    const int step = sizeof(block_q3_K) * nb / 2;\n\n    device const float * y1 = yy + ix*QK_K + y_offset;\n\n    uint32_t scales32, aux32;\n    thread uint16_t * scales16 = (thread uint16_t *)&scales32;\n    thread const int8_t * scales = (thread const int8_t *)&scales32;\n\n    float sumf1[2] = {0.f};\n    float sumf2[2] = {0.f};\n    for (int i = ix; i < nb; i += 4) {\n\n        for (int l = 0; l < 8; ++l) {\n            yl[l+ 0] = y1[l+ 0];\n            yl[l+ 8] = y1[l+16];\n            yl[l+16] = y1[l+32];\n            yl[l+24] = y1[l+48];\n        }\n\n        device const uint16_t * q = (device const uint16_t *)(x[i].qs + q_offset);\n        device const uint16_t * h = (device const uint16_t *)(x[i].hmask + l0);\n        device const uint16_t * a = (device const uint16_t *)(x[i].scales);\n        device const half * dh = &x[i].d;\n\n        for (int row = 0; row < 2; ++row) {\n\n            const float d_all = (float)dh[0];\n\n            scales16[0] = a[4];\n            scales16[1] = a[5];\n            aux32 = ((scales32 >> s_shift2) << 4) & 0x30303030;\n            scales16[0] = a[il+0];\n            scales16[1] = a[il+1];\n            scales32 = ((scales32 >> s_shift1) & 0x0f0f0f0f) | aux32;\n\n            float s1 = 0, s2 = 0, s3 = 0, s4 = 0, s5 = 0, s6 = 0;\n            for (int l = 0; l < n; l += 2) {\n                const int32_t qs = q[l/2];\n                s1 += yl[l+0] * (qs & qm[il/2][0]);\n                s2 += yl[l+1] * (qs & qm[il/2][1]);\n                s3 += ((h[l/2] & hm[0]) ? 0.f : yl[l+0]) + ((h[l/2] & hm[1]) ? 0.f : yl[l+1]);\n                s4 += yl[l+16] * (qs & qm[il/2][2]);\n                s5 += yl[l+17] * (qs & qm[il/2][3]);\n                s6 += ((h[l/2] & hm[2]) ? 0.f : yl[l+16]) + ((h[l/2] & hm[3]) ? 0.f : yl[l+17]);\n            }\n            float d1 = d_all * (s1 + 1.f/256.f * s2 - s3*v1);\n            float d2 = d_all * (s4 + 1.f/256.f * s5 - s6*v2);\n            sumf1[row] += d1 * (scales[0] - 32);\n            sumf2[row] += d2 * (scales[2] - 32);\n\n            s1 = s2 = s3 = s4 = s5 = s6 = 0;\n            for (int l = 0; l < n; l += 2) {\n                const int32_t qs = q[l/2+8];\n                s1 += yl[l+8] * (qs & qm[il/2][0]);\n                s2 += yl[l+9] * (qs & qm[il/2][1]);\n                s3 += ((h[l/2+8] & hm[0]) ? 0.f : yl[l+8]) + ((h[l/2+8] & hm[1]) ? 0.f : yl[l+9]);\n                s4 += yl[l+24] * (qs & qm[il/2][2]);\n                s5 += yl[l+25] * (qs & qm[il/2][3]);\n                s6 += ((h[l/2+8] & hm[2]) ? 0.f : yl[l+24]) + ((h[l/2+8] & hm[3]) ? 0.f : yl[l+25]);\n            }\n            d1 = d_all * (s1 + 1.f/256.f * s2 - s3*v1);\n            d2 = d_all * (s4 + 1.f/256.f * s5 - s6*v2);\n            sumf1[row] += d1 * (scales[1] - 32);\n            sumf2[row] += d2 * (scales[3] - 32);\n\n            q  += step;\n            h  += step;\n            a  += step;\n            dh += step;\n\n        }\n\n        y1 += 4 * QK_K;\n\n    }\n\n    for (int row = 0; row < 2; ++row) {\n        const float sumf = (sumf1[row] + 0.25f * sumf2[row]) / (1 << shift);\n        sumf1[row] = simd_sum(sumf);\n    }\n    if (tiisg == 0) {\n        for (int row = 0; row < 2; ++row) {\n            dst[r1*ne0 + r2*ne0*ne1 + first_row + row] = sumf1[row];\n        }\n    }\n}\n#else\nkernel void kernel_mul_mv_q3_K_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n\n    const int nb = ne00/QK_K;\n\n    const int64_t r0 = tgpig.x;\n    const int64_t r1 = tgpig.y;\n    const int64_t r2 = tgpig.z;\n\n    const int row = 2 * r0 + sgitg;\n    const uint offset0 = r2/gqa*(nb*ne0);\n    device const block_q3_K * x = (device const block_q3_K *) src0 + row*nb + offset0;\n    device const float     * yy = (device const float      *) src1 + r1*ne10 + r2*ne00*ne1;\n    const int ix = tiisg/4;\n    const int il = 4 * (tiisg%4);// 0, 4, 8, 12\n    const int im = il/8;         // 0, 0, 1, 1\n    const int in = il%8;         // 0, 4, 0, 4\n\n    float2 sum = {0.f, 0.f};\n\n    for (int i = ix; i < nb; i += 8) {\n\n        const float d_all = (float)(x[i].d);\n\n        device const uint16_t * q = (device const uint16_t *)(x[i].qs + il);\n        device const uint16_t * h = (device const uint16_t *)(x[i].hmask + in);\n        device const uint16_t * s = (device const uint16_t *)(x[i].scales);\n        device const float    * y = yy + i * QK_K + il;\n\n        const float d1 = d_all * ((int32_t)(s[0] & 0x000F) - 8);\n        const float d2 = d_all * ((int32_t)(s[0] & 0x00F0) - 128) * 1.f/64.f;\n        const float d3 = d_all * ((int32_t)(s[0] & 0x0F00) - 2048) * 1.f/4096.f;\n        const float d4 = d_all * ((int32_t)(s[0] & 0xF000) - 32768) * 1.f/262144.f;\n\n        for (int l = 0; l < 4; l += 2) {\n            const uint16_t hm = h[l/2] >> im;\n            sum[0] += y[l+ 0] * d1 * ((int32_t)(q[l/2] & 0x0003) - ((hm & 0x0001) ? 0 :  4))\n                    + y[l+16] * d2 * ((int32_t)(q[l/2] & 0x000c) - ((hm & 0x0004) ? 0 : 16))\n                    + y[l+32] * d3 * ((int32_t)(q[l/2] & 0x0030) - ((hm & 0x0010) ? 0 : 64))\n                    + y[l+48] * d4 * ((int32_t)(q[l/2] & 0x00c0) - ((hm & 0x0040) ? 0 : 256));\n            sum[1] += y[l+ 1] * d1 * ((int32_t)(q[l/2] & 0x0300) - ((hm & 0x0100) ? 0 : 1024))\n                    + y[l+17] * d2 * ((int32_t)(q[l/2] & 0x0c00) - ((hm & 0x0400) ? 0 : 4096))\n                    + y[l+33] * d3 * ((int32_t)(q[l/2] & 0x3000) - ((hm & 0x1000) ? 0 : 16384))\n                    + y[l+49] * d4 * ((int32_t)(q[l/2] & 0xc000) - ((hm & 0x4000) ? 0 : 65536));\n        }\n\n    }\n    const float sumf = sum[0] + sum[1] * 1.f/256.f;\n\n    const float tot = simd_sum(sumf);\n    if (tiisg == 0) {\n        dst[r1*ne0 + r2*ne0*ne1 + row] = tot;\n    }\n\n}\n#endif\n\n#if QK_K == 256\nkernel void kernel_mul_mv_q4_K_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01 [[buffer(4)]],\n        constant   int64_t & ne02 [[buffer(5)]],\n        constant   int64_t & ne10 [[buffer(9)]],\n        constant   int64_t & ne12 [[buffer(11)]],\n        constant   int64_t & ne0  [[buffer(15)]],\n        constant   int64_t & ne1  [[buffer(16)]],\n        constant   uint    & gqa  [[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n\n    const uint16_t kmask1 = 0x3f3f;\n    const uint16_t kmask2 = 0x0f0f;\n    const uint16_t kmask3 = 0xc0c0;\n\n    const int ix = tiisg/8;  // 0...3\n    const int it = tiisg%8;  // 0...7\n    const int im = it/4;     // 0 or 1\n    const int ir = it%4;     // 0...3\n\n    const int nb = ne00/QK_K;\n    const int r0 = tgpig.x;\n    const int r1 = tgpig.y;\n    const int r2 = tgpig.z;\n    //const int first_row = (r0 * N_SIMDGROUP + sgitg) * N_DST;\n    const int first_row = r0 * N_DST;\n    const int ib_row = first_row * nb;\n    const uint offset0 = r2/gqa*(nb*ne0);\n    device const block_q4_K * x = (device const block_q4_K *) src0 + ib_row + offset0;\n    device const float      * y = (device const float      *) src1 + r1*ne10 + r2*ne00*ne1;\n    float yl[16];\n    float yh[16];\n    float sumf[N_DST]={0.f}, all_sum;\n\n    const int step = sizeof(block_q4_K) * nb / 2;\n\n    device const float * y4 = y + ix * QK_K + 64 * im + 8 * ir;\n\n    uint16_t sc16[4];\n    thread const uint8_t * sc8 = (thread const uint8_t *)sc16;\n\n    for (int ib = ix; ib < nb; ib += 4) {\n\n        float4 sumy = {0.f, 0.f, 0.f, 0.f};\n        for (int i = 0; i < 8; ++i) {\n            yl[i+0] = y4[i+  0]; sumy[0] += yl[i+0];\n            yl[i+8] = y4[i+ 32]; sumy[1] += yl[i+8];\n            yh[i+0] = y4[i+128]; sumy[2] += yh[i+0];\n            yh[i+8] = y4[i+160]; sumy[3] += yh[i+8];\n        }\n\n        device const uint16_t * sc = (device const uint16_t *)x[ib].scales + im;\n        device const uint16_t * q1 = (device const uint16_t *)x[ib].qs + 16 * im + 4 * ir;\n        device const half     * dh = &x[ib].d;\n\n        for (int row = 0; row < N_DST; row++) {\n\n            sc16[0] = sc[0] & kmask1;\n            sc16[1] = sc[2] & kmask1;\n            sc16[2] = ((sc[4] >> 0) & kmask2) | ((sc[0] & kmask3) >> 2);\n            sc16[3] = ((sc[4] >> 4) & kmask2) | ((sc[2] & kmask3) >> 2);\n\n            device const uint16_t * q2 = q1 + 32;\n\n            float4 acc1 = {0.f, 0.f, 0.f, 0.f};\n            float4 acc2 = {0.f, 0.f, 0.f, 0.f};\n            for (int i = 0; i < 8; i += 2) {\n                acc1[0] += yl[i+0] * (q1[i/2] & 0x000F);\n                acc1[1] += yl[i+1] * (q1[i/2] & 0x0F00);\n                acc1[2] += yl[i+8] * (q1[i/2] & 0x00F0);\n                acc1[3] += yl[i+9] * (q1[i/2] & 0xF000);\n                acc2[0] += yh[i+0] * (q2[i/2] & 0x000F);\n                acc2[1] += yh[i+1] * (q2[i/2] & 0x0F00);\n                acc2[2] += yh[i+8] * (q2[i/2] & 0x00F0);\n                acc2[3] += yh[i+9] * (q2[i/2] & 0xF000);\n            }\n\n            float dall = dh[0];\n            float dmin = dh[1];\n            sumf[row] += dall * ((acc1[0] + 1.f/256.f * acc1[1]) * sc8[0] +\n                                 (acc1[2] + 1.f/256.f * acc1[3]) * sc8[1] * 1.f/16.f +\n                                 (acc2[0] + 1.f/256.f * acc2[1]) * sc8[4] +\n                                 (acc2[2] + 1.f/256.f * acc2[3]) * sc8[5] * 1.f/16.f) -\n                         dmin * (sumy[0] * sc8[2] + sumy[1] * sc8[3] + sumy[2] * sc8[6] + sumy[3] * sc8[7]);\n\n            q1 += step;\n            sc += step;\n            dh += step;\n        }\n\n        y4 += 4 * QK_K;\n    }\n\n    for (int row = 0; row < N_DST; ++row) {\n        all_sum = simd_sum(sumf[row]);\n        if (tiisg == 0) {\n            dst[r1*ne0 + r2*ne0*ne1 + first_row + row] = all_sum;\n        }\n    }\n}\n#else\nkernel void kernel_mul_mv_q4_K_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n\n    const int ix = tiisg/4;  // 0...7\n    const int it = tiisg%4;  // 0...3\n\n    const int nb = ne00/QK_K;\n    const int r0 = tgpig.x;\n    const int r1 = tgpig.y;\n    const int r2 = tgpig.z;\n    const int first_row = (r0 * N_SIMDGROUP + sgitg) * N_DST;\n    const int ib_row = first_row * nb;\n    const uint offset0 = r2/gqa*(nb*ne0);\n    device const block_q4_K * x = (device const block_q4_K *) src0 + ib_row + offset0;\n    device const float      * y = (device const float      *) src1 + r1*ne10 + r2*ne00*ne1;\n    float yl[8];\n    float yh[8];\n    float sumf[N_DST]={0.f}, all_sum;\n\n    const int step = sizeof(block_q4_K) * nb / 2;\n\n    device const float * y4 = y + ix * QK_K + 8 * it;\n\n    uint16_t sc16[4];\n\n    for (int ib = ix; ib < nb; ib += 8) {\n\n        float2 sumy = {0.f, 0.f};\n        for (int i = 0; i < 8; ++i) {\n            yl[i] = y4[i+ 0]; sumy[0] += yl[i];\n            yh[i] = y4[i+32]; sumy[1] += yh[i];\n        }\n\n        device const uint16_t * sc = (device const uint16_t *)x[ib].scales;\n        device const uint16_t * qs = (device const uint16_t *)x[ib].qs + 4 * it;\n        device const half     * dh = x[ib].d;\n\n        for (int row = 0; row < N_DST; row++) {\n\n            sc16[0] = sc[0] & 0x000f;\n            sc16[1] = sc[0] & 0x0f00;\n            sc16[2] = sc[0] & 0x00f0;\n            sc16[3] = sc[0] & 0xf000;\n\n            float2 acc1 = {0.f, 0.f};\n            float2 acc2 = {0.f, 0.f};\n            for (int i = 0; i < 8; i += 2) {\n                acc1[0] += yl[i+0] * (qs[i/2] & 0x000F);\n                acc1[1] += yl[i+1] * (qs[i/2] & 0x0F00);\n                acc2[0] += yh[i+0] * (qs[i/2] & 0x00F0);\n                acc2[1] += yh[i+1] * (qs[i/2] & 0xF000);\n            }\n\n            float dall = dh[0];\n            float dmin = dh[1];\n            sumf[row] += dall * ((acc1[0] + 1.f/256.f * acc1[1]) * sc16[0] +\n                                 (acc2[0] + 1.f/256.f * acc2[1]) * sc16[1] * 1.f/4096.f) -\n                         dmin * 1.f/16.f * (sumy[0] * sc16[2] + sumy[1] * sc16[3] * 1.f/256.f);\n\n            qs += step;\n            sc += step;\n            dh += step;\n        }\n\n        y4 += 8 * QK_K;\n    }\n\n    for (int row = 0; row < N_DST; ++row) {\n        all_sum = simd_sum(sumf[row]);\n        if (tiisg == 0) {\n            dst[r1*ne0+ r2*ne0*ne1 + first_row + row] = all_sum;\n        }\n    }\n}\n#endif\n\nkernel void kernel_mul_mv_q5_K_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n\n    const int nb = ne00/QK_K;\n\n    const int64_t r0 = tgpig.x;\n    const int64_t r1 = tgpig.y;\n    const int r2 = tgpig.z;\n\n    const int first_row = (r0 * N_SIMDGROUP + sgitg) * 2;\n    const uint offset0 = r2/gqa*(nb*ne0);\n    device const block_q5_K * x = (device const block_q5_K *) src0 + first_row*nb + offset0;\n    device const float     * yy = (device const float      *) src1 + r1*ne10 + r2*ne00*ne1;\n\n    float sumf[2]={0.f};\n\n    const int step = sizeof(block_q5_K) * nb;\n\n#if QK_K == 256\n#\n    float yl[16], yh[16];\n\n    const uint16_t kmask1 = 0x3f3f;\n    const uint16_t kmask2 = 0x0f0f;\n    const uint16_t kmask3 = 0xc0c0;\n\n    const int tid = tiisg/4;\n    const int ix  = tiisg%4;\n    const int im  = tid/4;\n    const int ir  = tid%4;\n    const int n   = 8;\n\n    const int l0 = n*ir;\n    const int q_offset = 32*im + l0;\n    const int y_offset = 64*im + l0;\n\n    const uint8_t hm1 = 1u << (2*im);\n    const uint8_t hm2 = hm1 << 1;\n    const uint8_t hm3 = hm1 << 4;\n    const uint8_t hm4 = hm2 << 4;\n\n    uint16_t sc16[4];\n    thread const uint8_t * sc8 = (thread const uint8_t *)sc16;\n\n    device const float * y1 = yy + ix*QK_K + y_offset;\n\n    for (int i = ix; i < nb; i += 4) {\n\n        device const uint8_t * q1 = x[i].qs + q_offset;\n        device const uint8_t * qh = x[i].qh + l0;\n        device const half * dh = &x[i].d;\n        device const uint16_t * a = (device const uint16_t *)x[i].scales + im;\n\n        device const float * y2 = y1 + 128;\n        float4 sumy = {0.f, 0.f, 0.f, 0.f};\n        for (int l = 0; l < 8; ++l) {\n            yl[l+0] = y1[l+ 0]; sumy[0] += yl[l+0];\n            yl[l+8] = y1[l+32]; sumy[1] += yl[l+8];\n            yh[l+0] = y2[l+ 0]; sumy[2] += yh[l+0];\n            yh[l+8] = y2[l+32]; sumy[3] += yh[l+8];\n        }\n\n        for (int row = 0; row < 2; ++row) {\n\n            device const uint8_t * q2 = q1 + 64;\n\n            sc16[0] = a[0] & kmask1;\n            sc16[1] = a[2] & kmask1;\n            sc16[2] = ((a[4] >> 0) & kmask2) | ((a[0] & kmask3) >> 2);\n            sc16[3] = ((a[4] >> 4) & kmask2) | ((a[2] & kmask3) >> 2);\n\n            float4 acc1 = {0.f};\n            float4 acc2 = {0.f};\n            for (int l = 0; l < n; ++l) {\n                uint8_t h = qh[l];\n                acc1[0] += yl[l+0] * (q1[l] & 0x0F);\n                acc1[1] += yl[l+8] * (q1[l] & 0xF0);\n                acc1[2] += yh[l+0] * (q2[l] & 0x0F);\n                acc1[3] += yh[l+8] * (q2[l] & 0xF0);\n                acc2[0] += h & hm1 ? yl[l+0] : 0.f;\n                acc2[1] += h & hm2 ? yl[l+8] : 0.f;\n                acc2[2] += h & hm3 ? yh[l+0] : 0.f;\n                acc2[3] += h & hm4 ? yh[l+8] : 0.f;\n            }\n            const float dall = dh[0];\n            const float dmin = dh[1];\n            sumf[row] += dall * (sc8[0] * (acc1[0] +  16.f*acc2[0]) +\n                                 sc8[1] * (acc1[1]/16.f + 16.f*acc2[1]) +\n                                 sc8[4] * (acc1[2] +  16.f*acc2[2]) +\n                                 sc8[5] * (acc1[3]/16.f + 16.f*acc2[3])) -\n                         dmin * (sumy[0] * sc8[2] + sumy[1] * sc8[3] + sumy[2] * sc8[6] + sumy[3] * sc8[7]);\n\n            q1 += step;\n            qh += step;\n            dh += step/2;\n            a  += step/2;\n\n        }\n\n        y1 += 4 * QK_K;\n\n    }\n#else\n    float yl[8], yh[8];\n\n    const int il = 4 * (tiisg/8);  // 0, 4, 8, 12\n    const int ix = tiisg%8;\n    const int im = il/8;         // 0, 0, 1, 1\n    const int in = il%8;         // 0, 4, 0, 4\n\n    device const float * y = yy + ix*QK_K + il;\n\n    for (int i = ix; i < nb; i += 8) {\n\n        for (int l = 0; l < 4; ++l) {\n            yl[l+0] = y[l+ 0];\n            yl[l+4] = y[l+16];\n            yh[l+0] = y[l+32];\n            yh[l+4] = y[l+48];\n        }\n\n        device const half * dh = &x[i].d;\n        device const uint8_t * q = x[i].qs + il;\n        device const uint8_t * h = x[i].qh + in;\n        device const int8_t  * s = x[i].scales;\n\n        for (int row = 0; row < 2; ++row) {\n\n            const float d = dh[0];\n\n            float2 acc = {0.f, 0.f};\n            for (int l = 0; l < 4; ++l) {\n                const uint8_t hl = h[l] >> im;\n                acc[0] += yl[l+0] * s[0] * ((int16_t)(q[l+ 0] & 0x0F) - (hl & 0x01 ? 0 : 16))\n                        + yl[l+4] * s[1] * ((int16_t)(q[l+16] & 0x0F) - (hl & 0x04 ? 0 : 16));\n                acc[1] += yh[l+0] * s[2] * ((int16_t)(q[l+ 0] & 0xF0) - (hl & 0x10 ? 0 : 256))\n                        + yh[l+4] * s[3] * ((int16_t)(q[l+16] & 0xF0) - (hl & 0x40 ? 0 : 256));\n            }\n            sumf[row] += d * (acc[0] + 1.f/16.f * acc[1]);\n\n            q += step;\n            h += step;\n            s += step;\n            dh += step/2;\n\n        }\n\n        y += 8 * QK_K;\n    }\n#endif\n\n    for (int row = 0; row < 2; ++row) {\n        const float tot = simd_sum(sumf[row]);\n        if (tiisg == 0) {\n            dst[r1*ne0 + r2*ne0*ne1 + first_row + row] = tot;\n        }\n    }\n\n}\n\nkernel void kernel_mul_mv_q6_K_f32(\n        device const  void * src0,\n        device const float * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant   int64_t & ne01[[buffer(4)]],\n        constant   int64_t & ne02[[buffer(5)]],\n        constant   int64_t & ne10[[buffer(9)]],\n        constant   int64_t & ne12[[buffer(11)]],\n        constant   int64_t & ne0[[buffer(15)]],\n        constant   int64_t & ne1[[buffer(16)]],\n        constant   uint    & gqa[[buffer(17)]],\n        uint3 tgpig[[threadgroup_position_in_grid]],\n        uint tiisg[[thread_index_in_simdgroup]],\n        uint sgitg[[simdgroup_index_in_threadgroup]]) {\n\n    const uint8_t kmask1 = 0x03;\n    const uint8_t kmask2 = 0x0C;\n    const uint8_t kmask3 = 0x30;\n    const uint8_t kmask4 = 0xC0;\n\n    const int nb = ne00/QK_K;\n\n    const int64_t r0 = tgpig.x;\n    const int64_t r1 = tgpig.y;\n    const int r2 = tgpig.z;\n\n    const int row = 2 * r0 + sgitg;\n    const uint offset0 = r2/gqa*(nb*ne0);\n    device const block_q6_K * x = (device const block_q6_K *) src0 + row * nb + offset0;\n    device const float     * yy = (device const float      *) src1 + r1*ne10 + r2*ne00*ne1;\n\n    float sumf = 0;\n\n#if QK_K == 256\n    const int tid  = tiisg/2;\n    const int ix   = tiisg%2;\n    const int ip   = tid/8;         // 0 or 1\n    const int il   = tid%8;\n    const int n    = 4;\n    const int l0   = n*il;\n    const int is   = 8*ip + l0/16;\n\n    const int y_offset = 128*ip + l0;\n    const int q_offset_l = 64*ip + l0;\n    const int q_offset_h = 32*ip + l0;\n\n    for (int i = ix; i < nb; i += 2) {\n\n        device const uint8_t * q1 = x[i].ql + q_offset_l;\n        device const uint8_t * q2 = q1 + 32;\n        device const uint8_t * qh = x[i].qh + q_offset_h;\n        device const int8_t  * sc = x[i].scales + is;\n\n        device const float * y = yy + i * QK_K + y_offset;\n\n        const float dall = x[i].d;\n\n        float4 sums = {0.f, 0.f, 0.f, 0.f};\n        for (int l = 0; l < n; ++l) {\n            sums[0] += y[l+ 0] * ((int8_t)((q1[l] & 0xF) | ((qh[l] & kmask1) << 4)) - 32);\n            sums[1] += y[l+32] * ((int8_t)((q2[l] & 0xF) | ((qh[l] & kmask2) << 2)) - 32);\n            sums[2] += y[l+64] * ((int8_t)((q1[l]  >> 4) | ((qh[l] & kmask3) << 0)) - 32);\n            sums[3] += y[l+96] * ((int8_t)((q2[l]  >> 4) | ((qh[l] & kmask4) >> 2)) - 32);\n        }\n\n        sumf += dall * (sums[0] * sc[0] + sums[1] * sc[2] + sums[2] * sc[4] + sums[3] * sc[6]);\n\n    }\n\n#else\n    const int ix  = tiisg/4;\n    const int il  = 4*(tiisg%4);\n\n    for (int i = ix; i < nb; i += 8) {\n        device const float * y = yy + i * QK_K + il;\n        device const uint8_t * ql = x[i].ql + il;\n        device const uint8_t * qh = x[i].qh + il;\n        device const int8_t  * s  = x[i].scales;\n\n        const float d = x[i].d;\n\n        float4 sums = {0.f, 0.f, 0.f, 0.f};\n        for (int l = 0; l < 4; ++l) {\n            sums[0] += y[l+ 0] * ((int8_t)((ql[l+ 0] & 0xF) | ((qh[l] & kmask1) << 4)) - 32);\n            sums[1] += y[l+16] * ((int8_t)((ql[l+16] & 0xF) | ((qh[l] & kmask2) << 2)) - 32);\n            sums[2] += y[l+32] * ((int8_t)((ql[l+ 0] >>  4) | ((qh[l] & kmask3) >> 0)) - 32);\n            sums[3] += y[l+48] * ((int8_t)((ql[l+16] >>  4) | ((qh[l] & kmask4) >> 2)) - 32);\n        }\n        sumf += d * (sums[0] * s[0] + sums[1] * s[1] + sums[2] * s[2] + sums[3] * s[3]);\n    }\n\n#endif\n\n    const float tot = simd_sum(sumf);\n    if (tiisg == 0) {\n        dst[r1*ne0 + r2*ne0*ne1 + row] = tot;\n    }\n}\n\n//============================= templates and their specializations =============================\n\n// NOTE: this is not dequantizing - we are simply fitting the template\ntemplate <typename type4x4>\nvoid dequantize_f32(device const float4x4 * src, short il, thread type4x4 & reg) {\n    float4x4 temp = *(((device float4x4 *)src));\n    for (int i = 0; i < 16; i++){\n        reg[i/4][i%4] = temp[i/4][i%4];\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_f16(device const half4x4 * src, short il, thread type4x4 & reg) {\n    half4x4 temp = *(((device half4x4 *)src));\n    for (int i = 0; i < 16; i++){\n        reg[i/4][i%4] = temp[i/4][i%4];\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q4_0(device const block_q4_0 *xb, short il, thread type4x4 & reg) {\n    device const uint16_t * qs = ((device const uint16_t *)xb + 1);\n    const float d1 = il ? (xb->d / 16.h) : xb->d;\n    const float d2 = d1 / 256.f;\n    const float md = -8.h * xb->d;\n    const ushort mask0 = il ? 0x00F0 : 0x000F;\n    const ushort mask1 = mask0 << 8;\n\n    for (int i=0;i<8;i++) {\n        reg[i/2][2*(i%2)+0] = d1 * (qs[i] & mask0) + md;\n        reg[i/2][2*(i%2)+1] = d2 * (qs[i] & mask1) + md;\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q4_1(device const block_q4_1 *xb, short il, thread type4x4 & reg) {\n    device const uint16_t * qs = ((device const uint16_t *)xb + 2);\n    const float d1 = il ? (xb->d / 16.h) : xb->d;\n    const float d2 = d1 / 256.f;\n    const float  m = xb->m;\n    const ushort mask0 = il ? 0x00F0 : 0x000F;\n    const ushort mask1 = mask0 << 8;\n\n    for (int i=0;i<8;i++) {\n        reg[i/2][2*(i%2)+0] = ((qs[i] & mask0) * d1) + m;\n        reg[i/2][2*(i%2)+1] = ((qs[i] & mask1) * d2) + m;\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q5_0(device const block_q5_0 *xb, short il, thread type4x4 & reg) {\n    device const uint16_t * qs = ((device const uint16_t *)xb + 3);\n    const float d = xb->d;\n    const float md = -16.h * xb->d;\n    const ushort mask = il ? 0x00F0 : 0x000F;\n\n    const uint32_t qh = *((device const uint32_t *)xb->qh);\n\n    const int x_mv = il ? 4 : 0;\n\n    const int gh_mv = il ? 12 : 0;\n    const int gh_bk = il ?  0 : 4;\n\n    for (int i = 0; i < 8; i++) {\n        // extract the 5-th bits for x0 and x1\n        const uint8_t xh_0 = ((qh >> (gh_mv + 2*i  )) << gh_bk) & 0x10;\n        const uint8_t xh_1 = ((qh >> (gh_mv + 2*i+1)) << gh_bk) & 0x10;\n\n        // combine the 4-bits from qs with the 5th bit\n        const int32_t x0 = ((((qs[i]     ) & mask) >> x_mv) | xh_0);\n        const int32_t x1 = ((((qs[i] >> 8) & mask) >> x_mv) | xh_1);\n\n        reg[i/2][2*(i%2)+0] = d * x0 + md;\n        reg[i/2][2*(i%2)+1] = d * x1 + md;\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q5_1(device const block_q5_1 *xb, short il, thread type4x4 & reg) {\n    device const uint16_t * qs = ((device const uint16_t *)xb + 4);\n    const float d = xb->d;\n    const float m = xb->m;\n    const ushort mask = il ? 0x00F0 : 0x000F;\n\n    const uint32_t qh = *((device const uint32_t *)xb->qh);\n\n    const int x_mv = il ? 4 : 0;\n\n    const int gh_mv = il ? 12 : 0;\n    const int gh_bk = il ?  0 : 4;\n\n    for (int i = 0; i < 8; i++) {\n        // extract the 5-th bits for x0 and x1\n        const uint8_t xh_0 = ((qh >> (gh_mv + 2*i  )) << gh_bk) & 0x10;\n        const uint8_t xh_1 = ((qh >> (gh_mv + 2*i+1)) << gh_bk) & 0x10;\n\n        // combine the 4-bits from qs with the 5th bit\n        const int32_t x0 = ((((qs[i]     ) & mask) >> x_mv) | xh_0);\n        const int32_t x1 = ((((qs[i] >> 8) & mask) >> x_mv) | xh_1);\n\n        reg[i/2][2*(i%2)+0] = d * x0 + m;\n        reg[i/2][2*(i%2)+1] = d * x1 + m;\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q8_0(device const block_q8_0 *xb, short il, thread type4x4 & reg) {\n    device const int8_t * qs = ((device const int8_t *)xb->qs);\n    const half d = xb->d;\n\n    for (int i=0;i<16;i++) {\n        reg[i/4][i%4] = (qs[i + 16*il] * d);\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q2_K(device const block_q2_K *xb, short il, thread type4x4 & reg) {\n    const half d = xb->d;\n    const half min = xb->dmin;\n    device const uint8_t * q = (device const uint8_t *)xb->qs;\n    half dl, ml;\n    uint8_t sc = xb->scales[il];\n\n#if QK_K == 256\n    q = q + 32*(il/8) + 16*(il&1);\n    il = (il/2)%4;\n#endif\n    half  coef = il>1 ? (il>2 ? 1/64.h : 1/16.h) : (il>0 ? 1/4.h : 1.h);\n    uchar mask = il>1 ? (il>2 ? 192    : 48)     : (il>0 ? 12    : 3);\n    dl = d * (sc & 0xF) * coef, ml = min * (sc >> 4);\n    for (int i = 0; i < 16; ++i) {\n        reg[i/4][i%4] = dl * (q[i] & mask) - ml;\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q3_K(device const block_q3_K *xb, short il, thread type4x4 & reg) {\n    const half d_all = xb->d;\n    device const uint8_t * q = (device const uint8_t *)xb->qs;\n    device const uint8_t * h = (device const uint8_t *)xb->hmask;\n    device const int8_t * scales = (device const int8_t *)xb->scales;\n\n#if QK_K == 256\n    q = q + 32 * (il/8) + 16 * (il&1);\n    h = h + 16 * (il&1);\n    uint8_t m = 1 << (il/2);\n    uint16_t kmask1 = (il/4)>1 ? ((il/4)>2 ? 192 : 48) : \\\n                                 ((il/4)>0 ? 12  : 3);\n    uint16_t kmask2 = il/8 ? 0xF0 : 0x0F;\n    uint16_t scale_2 = scales[il%8], scale_1 = scales[8 + il%4];\n    int16_t  dl_int = (il/4)&1 ? (scale_2&kmask2) | ((scale_1&kmask1) << 2)\n                               : (scale_2&kmask2) | ((scale_1&kmask1) << 4);\n    half dl = il<8 ? d_all * (dl_int - 32.h) : d_all * (dl_int / 16.h - 32.h);\n    const half ml = 4.h * dl;\n\n    il = (il/2) & 3;\n    const half    coef = il>1 ? (il>2 ? 1/64.h : 1/16.h) : (il>0 ? 1/4.h : 1.h);\n    const uint8_t mask = il>1 ? (il>2 ? 192    : 48)     : (il>0 ? 12    : 3);\n    dl *= coef;\n\n    for (int i = 0; i < 16; ++i) {\n        reg[i/4][i%4] = dl * (q[i] & mask) - (h[i] & m ? 0 : ml);\n    }\n#else\n    float    kcoef = il&1 ? 1.f/16.f : 1.f;\n    uint16_t kmask = il&1 ? 0xF0     : 0x0F;\n    float    dl = d_all * ((scales[il/2] & kmask) * kcoef - 8);\n    float    coef = il>1 ? (il>2 ? 1/64.h : 1/16.h) : (il>0 ? 1/4.h : 1.h);\n    uint8_t  mask = il>1 ? (il>2 ? 192    : 48)     : (il>0 ? 12    : 3);\n    uint8_t  m = 1<<(il*2);\n    for (int i = 0; i < 16; ++i) {\n        reg[i/4][i%4] = coef * dl * ((q[i] & mask) - ((h[i%8] & (m * (1 + i/8))) ? 0 : 4.f/coef));\n    }\n#endif\n}\n\nstatic inline uchar2 get_scale_min_k4_just2(int j, int k, device const uchar * q) {\n    return j < 4 ? uchar2{uchar(q[j+0+k] & 63), uchar(q[j+4+k] & 63)}\n                 : uchar2{uchar((q[j+4+k] & 0xF) | ((q[j-4+k] & 0xc0) >> 2)), uchar((q[j+4+k] >> 4) | ((q[j-0+k] & 0xc0) >> 2))};\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q4_K(device const block_q4_K *xb, short il, thread type4x4 & reg) {\n    device const uchar * q = xb->qs;\n\n#if QK_K == 256\n    short is = (il/4) * 2;\n    q = q + (il/4) * 32 + 16 * (il&1);\n    il = il & 3;\n    const uchar2 sc = get_scale_min_k4_just2(is, il/2, xb->scales);\n    const half d   = il < 2 ? xb->d : xb->d / 16.h;\n    const half min = xb->dmin;\n    const half dl = d * sc[0];\n    const half ml = min * sc[1];\n#else\n    q = q + 16 * (il&1);\n    device const uint8_t * s = xb->scales;\n    device const half2 * dh = (device const half2 *)xb->d;\n    const float2 d = (float2)dh[0];\n    const float dl = il<2 ? d[0] * (s[0]&0xF) : d[0] * (s[1]&0xF)/16.h;\n    const float ml = il<2 ? d[1] * (s[0]>>4)  : d[1] * (s[1]>>4);\n#endif\n    const ushort mask = il<2 ? 0x0F : 0xF0;\n    for (int i = 0; i < 16; ++i) {\n        reg[i/4][i%4] = dl * (q[i] & mask) - ml;\n    }\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q5_K(device const block_q5_K *xb, short il, thread type4x4 & reg) {\n    device const uint8_t * q  = xb->qs;\n    device const uint8_t * qh = xb->qh;\n\n#if QK_K == 256\n    short is = (il/4) * 2;\n    q  = q + 32 * (il/4) + 16 * (il&1);\n    qh = qh + 16 * (il&1);\n    uint8_t ul = 1 << (il/2);\n    il = il & 3;\n    const uchar2 sc = get_scale_min_k4_just2(is, il/2, xb->scales);\n    const half d = il < 2 ? xb->d : xb->d / 16.h;\n    const half min = xb->dmin;\n    const half dl = d * sc[0];\n    const half ml = min * sc[1];\n\n    const ushort mask = il<2 ? 0x0F : 0xF0;\n    const half qh_val = il<2 ? 16.h : 256.h;\n    for (int i = 0; i < 16; ++i) {\n        reg[i/4][i%4] = dl * ((q[i] & mask) + (qh[i] & ul ? qh_val : 0)) - ml;\n    }\n#else\n    q = q + 16 * (il&1);\n    device const int8_t * s = xb->scales;\n    const float dl = xb->d * s[il];\n    uint8_t m = 1<<(il*2);\n    const float  coef = il<2 ? 1.f  : 1.f/16.f;\n    const ushort mask = il<2 ? 0x0F : 0xF0;\n    for (int i = 0; i < 16; ++i) {\n        reg[i/4][i%4] = coef * dl * ((q[i] & mask) - (qh[i%8] & (m*(1+i/8)) ? 0.f : 16.f/coef));\n    }\n#endif\n}\n\ntemplate <typename type4x4>\nvoid dequantize_q6_K(device const block_q6_K *xb, short il, thread type4x4 & reg) {\n    const half d_all = xb->d;\n    device const uint8_t * ql = (device const uint8_t *)xb->ql;\n    device const uint8_t * qh = (device const uint8_t *)xb->qh;\n    device const int8_t * scales = (device const int8_t *)xb->scales;\n\n#if QK_K == 256\n    ql = ql + 64*(il/8) + 32*((il/2)&1) + 16*(il&1);\n    qh = qh + 32*(il/8) + 16*(il&1);\n    half sc = scales[(il%2) + 2 * ((il/2))];\n    il = (il/2) & 3;\n#else\n    ql = ql + 16 * (il&1);\n    half sc = scales[il];\n#endif\n    const uint16_t  kmask1 = il>1 ? (il>2 ? 192 : 48) : (il>0 ? 12 : 3);\n    const uint16_t  kmask2 = il>1 ? 0xF0              : 0x0F;\n    const half        coef = il>1 ? 1.f/16.h          : 1.h;\n    const half ml = d_all * sc * 32.h;\n    const half dl = d_all * sc * coef;\n    for (int i = 0; i < 16; ++i) {\n        const half q = il&1 ? ((ql[i] & kmask2) | ((qh[i] & kmask1) << 2))\n                            : ((ql[i] & kmask2) | ((qh[i] & kmask1) << 4));\n        reg[i/4][i%4] = dl * q - ml;\n    }\n}\n\ntemplate<typename block_q, short nl, void (*dequantize_func)(device const block_q *, short, thread float4x4 &)>\nkernel void kernel_get_rows(\n        device const  void * src0,\n        device const   int * src1,\n        device       float * dst,\n        constant   int64_t & ne00,\n        constant  uint64_t & nb01,\n        constant  uint64_t & nb1,\n        uint                 tgpig[[threadgroup_position_in_grid]],\n        uint                 tiitg[[thread_index_in_threadgroup]],\n        uint                 tptg[[threads_per_threadgroup]]) {\n    const int i = tgpig;\n    const int r = ((device int32_t *) src1)[i];\n\n    for (int ind = tiitg; ind < ne00/16; ind += tptg) {\n        float4x4 temp;\n        dequantize_func(\n            ((device const block_q *) ((device char *) src0 + r*nb01)) + ind/nl, ind%nl, temp);\n        *(((device float4x4 *) ((device char *) dst + i*nb1)) + ind) = temp;\n    }\n}\n\n#define BLOCK_SIZE_M 64 // 8 simdgroup matrices from matrix A\n#define BLOCK_SIZE_N 32 // 4 simdgroup matrices from matrix B\n#define BLOCK_SIZE_K 32\n#define THREAD_MAT_M 4 // each thread take 4 simdgroup matrices from matrix A\n#define THREAD_MAT_N 2 // each thread take 2 simdgroup matrices from matrix B\n#define THREAD_PER_BLOCK 128\n#define THREAD_PER_ROW 2 // 2 thread for each row in matrix A to load numbers\n#define THREAD_PER_COL 4 // 4 thread for each row in matrix B to load numbers\n#define SG_MAT_SIZE 64 // simdgroup matrix is of shape 8x8\n#define SG_MAT_ROW 8\n\n// each block_q contains 16*nl weights\ntemplate<typename block_q, short nl, void (*dequantize_func)(device const block_q *, short, thread half4x4 &)>\nkernel void kernel_mul_mm(device const  uchar * src0,\n                          device const  uchar * src1,\n                          device        float * dst,\n                          constant    int64_t & ne00,\n                          constant    int64_t & ne02,\n                          constant    int64_t & nb01,\n                          constant    int64_t & nb02,\n                          constant    int64_t & ne12,\n                          constant    int64_t & nb10,\n                          constant    int64_t & nb11,\n                          constant    int64_t & nb12,\n                          constant    int64_t & ne0,\n                          constant    int64_t & ne1,\n                          constant       uint & gqa,\n                          threadgroup   uchar * shared_memory [[threadgroup(0)]],\n                          uint3                 tgpig[[threadgroup_position_in_grid]],\n                          uint                  tiitg[[thread_index_in_threadgroup]],\n                          uint                  sgitg[[simdgroup_index_in_threadgroup]]) {\n\n    threadgroup half  * sa = (threadgroup half  *)(shared_memory);\n    threadgroup float * sb = (threadgroup float *)(shared_memory + 4096);\n\n    const uint r0 = tgpig.y;\n    const uint r1 = tgpig.x;\n    const uint im = tgpig.z;\n\n    // if this block is of 64x32 shape or smaller\n    short n_rows = (ne0 - r0 * BLOCK_SIZE_M < BLOCK_SIZE_M) ? (ne0 - r0 * BLOCK_SIZE_M) : BLOCK_SIZE_M;\n    short n_cols = (ne1 - r1 * BLOCK_SIZE_N < BLOCK_SIZE_N) ? (ne1 - r1 * BLOCK_SIZE_N) : BLOCK_SIZE_N;\n\n    // a thread shouldn't load data outside of the matrix\n    short thread_row = ((short)tiitg/THREAD_PER_ROW) < n_rows ? ((short)tiitg/THREAD_PER_ROW) : n_rows - 1;\n    short thread_col = ((short)tiitg/THREAD_PER_COL) < n_cols ? ((short)tiitg/THREAD_PER_COL) : n_cols - 1;\n\n    simdgroup_half8x8  ma[4];\n    simdgroup_float8x8 mb[2];\n    simdgroup_float8x8 c_res[8];\n    for (int i = 0; i < 8; i++){\n        c_res[i] = make_filled_simdgroup_matrix<float, 8>(0.f);\n    }\n\n    short il = (tiitg % THREAD_PER_ROW);\n\n    uint   offset0 = im/gqa*nb02;\n    ushort offset1 = il/nl;\n\n    device const block_q * x = (device const block_q *)(src0 + (r0 * BLOCK_SIZE_M + thread_row) * nb01 + offset0) + offset1;\n    device const float   * y = (device const float   *)(src1\n        + nb12 * im\n        + nb11 * (r1 * BLOCK_SIZE_N + thread_col)\n        + nb10 * (BLOCK_SIZE_K / THREAD_PER_COL * (tiitg % THREAD_PER_COL)));\n\n    for (int loop_k = 0; loop_k < ne00; loop_k += BLOCK_SIZE_K) {\n        // load data and store to threadgroup memory\n        half4x4 temp_a;\n        dequantize_func(x, il, temp_a);\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n\n        #pragma unroll(16)\n        for (int i = 0; i < 16; i++) {\n            *(sa + SG_MAT_SIZE * ((tiitg / THREAD_PER_ROW / 8) \\\n            +                     (tiitg % THREAD_PER_ROW) * 16 + (i / 8) * 8) \\\n            +                     (tiitg / THREAD_PER_ROW) % 8  + (i & 7) * 8) = temp_a[i/4][i%4];\n        }\n\n        *(threadgroup float2x4 *)(sb + (tiitg % THREAD_PER_COL) * 8 * 32 + 8 * (tiitg / THREAD_PER_COL)) = *((device float2x4 *)y);\n\n        il = (il + 2 < nl) ? il + 2 : il % 2;\n        x  = (il < 2) ? x + (2+nl-1)/nl : x;\n        y += BLOCK_SIZE_K;\n\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n\n        // load matrices from threadgroup memory and conduct outer products\n        threadgroup half  * lsma = (sa + THREAD_MAT_M * SG_MAT_SIZE * (sgitg % 2));\n        threadgroup float * lsmb = (sb + THREAD_MAT_N * SG_MAT_SIZE * (sgitg / 2));\n\n        #pragma unroll(4)\n        for (int ik = 0; ik < BLOCK_SIZE_K / 8; ik++) {\n            #pragma unroll(4)\n            for (int i = 0; i < 4; i++) {\n                simdgroup_load(ma[i],lsma + SG_MAT_SIZE * i);\n            }\n            simdgroup_barrier(mem_flags::mem_none);\n            #pragma unroll(2)\n            for (int i = 0; i < 2; i++) {\n                simdgroup_load(mb[i],lsmb + SG_MAT_SIZE * i);\n            }\n\n            lsma += BLOCK_SIZE_M / SG_MAT_ROW * SG_MAT_SIZE;\n            lsmb += BLOCK_SIZE_N / SG_MAT_ROW * SG_MAT_SIZE;\n\n            #pragma unroll(8)\n            for (int i = 0; i < 8; i++){\n                simdgroup_multiply_accumulate(c_res[i], mb[i/4], ma[i%4], c_res[i]);\n            }\n        }\n    }\n\n    if ((r0 + 1) * BLOCK_SIZE_M <= ne0 && (r1 + 1) * BLOCK_SIZE_N <= ne1) {\n        device float * C = dst + (BLOCK_SIZE_M * r0 + 32 * (sgitg &  1)) \\\n                               + (BLOCK_SIZE_N * r1 + 16 * (sgitg >> 1)) * ne0 + im*ne1*ne0;\n        for (int i = 0; i < 8; i++) {\n            simdgroup_store(c_res[i], C + 8 * (i%4) + 8 * ne0 * (i/4), ne0);\n        }\n    } else {\n        // block is smaller than 64x32, we should avoid writing data outside of the matrix\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n        threadgroup float * temp_str = ((threadgroup float *)shared_memory) \\\n                                      + 32 * (sgitg&1) + (16 * (sgitg>>1)) * BLOCK_SIZE_M;\n        for (int i = 0; i < 8; i++) {\n            simdgroup_store(c_res[i], temp_str + 8 * (i%4) + 8 * BLOCK_SIZE_M * (i/4), BLOCK_SIZE_M);\n        }\n\n        threadgroup_barrier(mem_flags::mem_threadgroup);\n\n        device float * C = dst + (BLOCK_SIZE_M * r0) + (BLOCK_SIZE_N * r1) * ne0 + im*ne1*ne0;\n        if (sgitg == 0) {\n            for (int i = 0; i < n_rows; i++) {\n                for (int j = tiitg; j < n_cols; j += BLOCK_SIZE_N) {\n                    *(C + i + j * ne0) = *(temp_str + i + j * BLOCK_SIZE_M);\n                }\n            }\n        }\n    }\n}\n\n#if QK_K == 256\n#define QK_NL 16\n#else\n#define QK_NL 4\n#endif\n\ntypedef void (get_rows_t)(device const void *, device const int *, device float *, constant int64_t &, \\\n                          constant uint64_t &, constant uint64_t &, uint, uint, uint);\n\ntemplate [[host_name(\"kernel_get_rows_f32\")]]  kernel get_rows_t kernel_get_rows<float4x4,   1, dequantize_f32>;\ntemplate [[host_name(\"kernel_get_rows_f16\")]]  kernel get_rows_t kernel_get_rows<half4x4,    1, dequantize_f16>;\ntemplate [[host_name(\"kernel_get_rows_q4_0\")]] kernel get_rows_t kernel_get_rows<block_q4_0, 2, dequantize_q4_0>;\ntemplate [[host_name(\"kernel_get_rows_q4_1\")]] kernel get_rows_t kernel_get_rows<block_q4_1, 2, dequantize_q4_1>;\ntemplate [[host_name(\"kernel_get_rows_q5_0\")]] kernel get_rows_t kernel_get_rows<block_q5_0, 2, dequantize_q5_0>;\ntemplate [[host_name(\"kernel_get_rows_q5_1\")]] kernel get_rows_t kernel_get_rows<block_q5_1, 2, dequantize_q5_1>;\ntemplate [[host_name(\"kernel_get_rows_q8_0\")]] kernel get_rows_t kernel_get_rows<block_q8_0, 2, dequantize_q8_0>;\ntemplate [[host_name(\"kernel_get_rows_q2_K\")]] kernel get_rows_t kernel_get_rows<block_q2_K, QK_NL, dequantize_q2_K>;\ntemplate [[host_name(\"kernel_get_rows_q3_K\")]] kernel get_rows_t kernel_get_rows<block_q3_K, QK_NL, dequantize_q3_K>;\ntemplate [[host_name(\"kernel_get_rows_q4_K\")]] kernel get_rows_t kernel_get_rows<block_q4_K, QK_NL, dequantize_q4_K>;\ntemplate [[host_name(\"kernel_get_rows_q5_K\")]] kernel get_rows_t kernel_get_rows<block_q5_K, QK_NL, dequantize_q5_K>;\ntemplate [[host_name(\"kernel_get_rows_q6_K\")]] kernel get_rows_t kernel_get_rows<block_q6_K, QK_NL, dequantize_q6_K>;\n\ntypedef void (mat_mm_t)(\n        device const  uchar * src0,\n        device const  uchar * src1,\n        device        float * dst,\n        constant    int64_t & ne00,\n        constant    int64_t & ne02,\n        constant    int64_t & nb01,\n        constant    int64_t & nb02,\n        constant    int64_t & ne12,\n        constant    int64_t & nb10,\n        constant    int64_t & nb11,\n        constant    int64_t & nb12,\n        constant    int64_t & ne0,\n        constant    int64_t & ne1,\n        constant       uint & gqa,\n        threadgroup uchar *, uint3, uint, uint);\n\ntemplate [[host_name(\"kernel_mul_mm_f32_f32\")]]  kernel mat_mm_t kernel_mul_mm<float4x4,   1,     dequantize_f32>;\ntemplate [[host_name(\"kernel_mul_mm_f16_f32\")]]  kernel mat_mm_t kernel_mul_mm<half4x4,    1,     dequantize_f16>;\ntemplate [[host_name(\"kernel_mul_mm_q4_0_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q4_0, 2,     dequantize_q4_0>;\ntemplate [[host_name(\"kernel_mul_mm_q4_1_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q4_1, 2,     dequantize_q4_1>;\ntemplate [[host_name(\"kernel_mul_mm_q5_0_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q5_0, 2,     dequantize_q5_0>;\ntemplate [[host_name(\"kernel_mul_mm_q5_1_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q5_1, 2,     dequantize_q5_1>;\ntemplate [[host_name(\"kernel_mul_mm_q8_0_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q8_0, 2,     dequantize_q8_0>;\ntemplate [[host_name(\"kernel_mul_mm_q2_K_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q2_K, QK_NL, dequantize_q2_K>;\ntemplate [[host_name(\"kernel_mul_mm_q3_K_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q3_K, QK_NL, dequantize_q3_K>;\ntemplate [[host_name(\"kernel_mul_mm_q4_K_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q4_K, QK_NL, dequantize_q4_K>;\ntemplate [[host_name(\"kernel_mul_mm_q5_K_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q5_K, QK_NL, dequantize_q5_K>;\ntemplate [[host_name(\"kernel_mul_mm_q6_K_f32\")]] kernel mat_mm_t kernel_mul_mm<block_q6_K, QK_NL, dequantize_q6_K>;\n"
        },
        {
          "name": "ggml-mpi.c",
          "type": "blob",
          "size": 6.7568359375,
          "content": "#include \"ggml-mpi.h\"\n\n#include \"ggml.h\"\n\n#include <mpi.h>\n\n#include <stdio.h>\n#include <stdlib.h>\n\n#define MIN(a, b) ((a) < (b) ? (a) : (b))\n\n#define UNUSED GGML_UNUSED\n\nstruct ggml_mpi_context {\n    int rank;\n    int size;\n};\n\nvoid ggml_mpi_backend_init(void) {\n    MPI_Init(NULL, NULL);\n}\n\nvoid ggml_mpi_backend_free(void) {\n    MPI_Finalize();\n}\n\nstruct ggml_mpi_context * ggml_mpi_init(void) {\n    struct ggml_mpi_context * ctx = calloc(1, sizeof(struct ggml_mpi_context));\n\n    MPI_Comm_rank(MPI_COMM_WORLD, &ctx->rank);\n    MPI_Comm_size(MPI_COMM_WORLD, &ctx->size);\n\n    return ctx;\n}\n\nvoid ggml_mpi_free(struct ggml_mpi_context * ctx) {\n    free(ctx);\n}\n\nint ggml_mpi_rank(struct ggml_mpi_context * ctx) {\n    return ctx->rank;\n}\n\nvoid ggml_mpi_eval_init(\n        struct ggml_mpi_context * ctx_mpi,\n                            int * n_tokens,\n                            int * n_past,\n                            int * n_threads) {\n    UNUSED(ctx_mpi);\n\n    // synchronize the worker node parameters with the root node\n    MPI_Barrier(MPI_COMM_WORLD);\n\n    MPI_Bcast(n_tokens,  1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(n_past,    1, MPI_INT, 0, MPI_COMM_WORLD);\n    MPI_Bcast(n_threads, 1, MPI_INT, 0, MPI_COMM_WORLD);\n}\n\nstatic int ggml_graph_get_node_idx(struct ggml_cgraph * gf, const char * name) {\n    struct ggml_tensor * t = ggml_graph_get_tensor(gf, name);\n    if (t == NULL) {\n        fprintf(stderr, \"%s: tensor %s not found\\n\", __func__, name);\n        return -1;\n    }\n\n    for (int i = 0; i < gf->n_nodes; i++) {\n        if (gf->nodes[i] == t) {\n            return i;\n        }\n    }\n\n    fprintf(stderr, \"%s: tensor %s not found in graph (should not happen)\\n\", __func__, name);\n    return -1;\n}\n\nstatic void ggml_mpi_tensor_send(struct ggml_tensor * t, int mpi_rank_dst) {\n    MPI_Datatype mpi_type;\n\n    switch (t->type) {\n        case GGML_TYPE_I32: mpi_type = MPI_INT32_T; break;\n        case GGML_TYPE_F32: mpi_type = MPI_FLOAT;   break;\n        default: GGML_ASSERT(false && \"not implemented\");\n    }\n\n    const int retval = MPI_Send(t->data, ggml_nelements(t), mpi_type, mpi_rank_dst, 0, MPI_COMM_WORLD);\n    GGML_ASSERT(retval == MPI_SUCCESS);\n}\n\nstatic void ggml_mpi_tensor_recv(struct ggml_tensor * t, int mpi_rank_src) {\n    MPI_Datatype mpi_type;\n\n    switch (t->type) {\n        case GGML_TYPE_I32: mpi_type = MPI_INT32_T; break;\n        case GGML_TYPE_F32: mpi_type = MPI_FLOAT;   break;\n        default: GGML_ASSERT(false && \"not implemented\");\n    }\n\n    MPI_Status status; UNUSED(status);\n\n    const int retval = MPI_Recv(t->data, ggml_nelements(t), mpi_type, mpi_rank_src, MPI_ANY_TAG, MPI_COMM_WORLD, &status);\n    GGML_ASSERT(retval == MPI_SUCCESS);\n}\n\n// TODO: there are many improvements that can be done to this implementation\nvoid ggml_mpi_graph_compute_pre(\n        struct ggml_mpi_context * ctx_mpi,\n             struct ggml_cgraph * gf,\n                            int   n_layers) {\n    const int mpi_rank = ctx_mpi->rank;\n    const int mpi_size = ctx_mpi->size;\n\n    struct ggml_tensor * inp_tokens = ggml_graph_get_tensor(gf, \"inp_tokens\");\n    if (inp_tokens == NULL) {\n        fprintf(stderr, \"%s: tensor 'inp_tokens' not found\\n\", __func__);\n        return;\n    }\n\n    struct ggml_tensor * inp0 = ggml_graph_get_tensor(gf, \"layer_inp_0\");\n    if (inp0 == NULL) {\n        fprintf(stderr, \"%s: tensor 'inp0' not found\\n\", __func__);\n        return;\n    }\n\n    GGML_ASSERT(inp0 == gf->nodes[0]);\n\n    // distribute the compute graph into slices across the MPI nodes\n    //\n    // the main node (0) processes the last layers + the remainder of the compute graph\n    // and is responsible to pass the input tokens to the first node (1)\n    //\n    // node 1:   [(  0) * n_per_node, (  1) * n_per_node)\n    // node 2:   [(  1) * n_per_node, (  2) * n_per_node)\n    // ...\n    // node n-1: [(n-2) * n_per_node, (n-1) * n_per_node)\n    // node 0:   [(n-1) * n_per_node,            n_nodes)\n    //\n    if (mpi_rank > 0) {\n        if (mpi_rank == 1) {\n            // the first node (1) receives the input tokens from the main node (0)\n            ggml_mpi_tensor_recv(inp_tokens, 0);\n        } else {\n            // recv input data for each node into the \"inp0\" tensor (i.e. the first node in the compute graph)\n            ggml_mpi_tensor_recv(inp0, mpi_rank - 1);\n        }\n    } else if (mpi_size > 1) {\n        // node 0 sends the input tokens to node 1\n        ggml_mpi_tensor_send(inp_tokens, 1);\n\n        // recv the output data from the last node\n        ggml_mpi_tensor_recv(inp0, mpi_size - 1);\n    }\n\n    {\n        const int n_per_node = (n_layers + (mpi_size - 1)) / mpi_size;\n\n        const int mpi_idx = mpi_rank > 0 ? mpi_rank - 1 : mpi_size - 1;\n\n        const int il0 =               (mpi_idx + 0) * n_per_node;\n        const int il1 = MIN(n_layers, (mpi_idx + 1) * n_per_node);\n\n        char name_l0[GGML_MAX_NAME];\n        char name_l1[GGML_MAX_NAME];\n\n        snprintf(name_l0, sizeof(name_l0), \"layer_inp_%d\", il0);\n        snprintf(name_l1, sizeof(name_l1), \"layer_inp_%d\", il1);\n\n        const int idx_l0 =                ggml_graph_get_node_idx(gf, name_l0);\n        const int idx_l1 = mpi_rank > 0 ? ggml_graph_get_node_idx(gf, name_l1) + 1 : gf->n_nodes;\n\n        if (idx_l0 < 0 || idx_l1 < 0) {\n            fprintf(stderr, \"%s: layer input nodes not found\\n\", __func__);\n            return;\n        }\n\n        // attach the input data to all nodes that need it\n        // TODO: not great - should be able to do this without modifying the compute graph (see next TODO below)\n        for (int i = idx_l0; i < idx_l1; i++) {\n            if (gf->nodes[i]->src[0] == gf->nodes[idx_l0]) {\n                gf->nodes[i]->src[0] =  inp0;\n            }\n            if (gf->nodes[i]->src[1] == gf->nodes[idx_l0]) {\n                gf->nodes[i]->src[1] =  inp0;\n            }\n        }\n\n        // TODO: instead of rearranging the nodes, we should be able to execute a subset of the compute graph\n        for (int i = 1; i < idx_l1 - idx_l0; i++) {\n            gf->nodes[i] = gf->nodes[idx_l0 + i];\n            gf->grads[i] = gf->grads[idx_l0 + i];\n        }\n\n        // the first node performs the \"get_rows\" operation, the rest of the nodes get the data from the previous node\n        if (mpi_idx != 0) {\n            gf->nodes[0]->op = GGML_OP_NONE;\n        }\n\n        gf->n_nodes = idx_l1 - idx_l0;\n\n        //fprintf(stderr, \"%s: node %d: processing %d nodes [%d, %d)\\n\", __func__, mpi_rank, gf->n_nodes, il0, il1);\n    }\n}\n\nvoid ggml_mpi_graph_compute_post(\n        struct ggml_mpi_context * ctx_mpi,\n             struct ggml_cgraph * gf,\n                            int   n_layers) {\n    UNUSED(n_layers);\n\n    const int mpi_rank = ctx_mpi->rank;\n    const int mpi_size = ctx_mpi->size;\n\n    // send the output data to the next node\n    if (mpi_rank > 0) {\n        ggml_mpi_tensor_send(gf->nodes[gf->n_nodes - 1], (mpi_rank + 1) % mpi_size);\n    }\n}\n"
        },
        {
          "name": "ggml-mpi.h",
          "type": "blob",
          "size": 0.8896484375,
          "content": "#pragma once\n\nstruct ggml_context;\nstruct ggml_tensor;\nstruct ggml_cgraph;\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\nstruct ggml_mpi_context;\n\nvoid ggml_mpi_backend_init(void);\nvoid ggml_mpi_backend_free(void);\n\nstruct ggml_mpi_context * ggml_mpi_init(void);\nvoid ggml_mpi_free(struct ggml_mpi_context * ctx);\n\nint ggml_mpi_rank(struct ggml_mpi_context * ctx);\n\nvoid ggml_mpi_eval_init(\n        struct ggml_mpi_context * ctx_mpi,\n                            int * n_tokens,\n                            int * n_past,\n                            int * n_threads);\n\nvoid ggml_mpi_graph_compute_pre(\n        struct ggml_mpi_context * ctx_mpi,\n             struct ggml_cgraph * gf,\n                            int   n_layers);\n\nvoid ggml_mpi_graph_compute_post(\n        struct ggml_mpi_context * ctx_mpi,\n             struct ggml_cgraph * gf,\n                            int   n_layers);\n\n#ifdef __cplusplus\n}\n#endif\n"
        },
        {
          "name": "ggml-opencl.cpp",
          "type": "blob",
          "size": 69.328125,
          "content": "#include \"ggml-opencl.h\"\n\n#include <array>\n#include <atomic>\n#include <sstream>\n#include <vector>\n#include <limits>\n\n#define CL_TARGET_OPENCL_VERSION 110\n#include <clblast.h>\n\n#include <stdlib.h>\n#include <stdio.h>\n#include <string.h>\n\n#include \"ggml.h\"\n\n#if defined(_MSC_VER)\n#pragma warning(disable: 4244 4267) // possible loss of data\n#endif\n\n#define CL_DMMV_LOCAL_SIZE 32\n\n#ifndef K_QUANTS_PER_ITERATION\n#define K_QUANTS_PER_ITERATION 1\n#else\nstatic_assert(K_QUANTS_PER_ITERATION == 1 || K_QUANTS_PER_ITERATION == 2, \"K_QUANTS_PER_ITERATION must be 1 or 2\");\n#endif\n\n#define MULTILINE_QUOTE(...) #__VA_ARGS__\nstatic std::string program_source = MULTILINE_QUOTE(\n\ntypedef char int8_t;\ntypedef uchar uint8_t;\ntypedef short int16_t;\ntypedef ushort uint16_t;\ntypedef int int32_t;\ntypedef uint uint32_t;\n\nstruct __attribute__ ((packed)) block_q4_0\n{\n    half d;\n    uint8_t qs[QK4_0 / 2];\n};\n\nstruct __attribute__ ((packed)) block_q4_1\n{\n    half d;\n    half m;\n    uint8_t qs[QK4_1 / 2];\n};\n\nstruct __attribute__ ((packed)) block_q5_0\n{\n    half d;\n    uint32_t qh;\n    uint8_t qs[QK5_0 / 2];\n};\n\nstruct __attribute__ ((packed)) block_q5_1\n{\n    half d;\n    half m;\n    uint32_t qh;\n    uint8_t qs[QK5_1 / 2];\n};\n\nstruct __attribute__ ((packed)) block_q8_0\n{\n    half d;\n    int8_t qs[QK8_0];\n};\n\nstruct __attribute__((packed)) block_q2_K\n{\n    uint8_t scales[16];\n    uint8_t qs[64];\n    half d;\n    half dmin;\n};\n\nstruct __attribute__((packed)) block_q3_K\n{\n    uint8_t hmask[32];\n    uint8_t qs[64];\n    uint8_t scales[12];\n    half d;\n};\n\nstruct __attribute__((packed)) block_q4_K\n{\n    half d;\n    half dmin;\n    uint8_t scales[12];\n    uint8_t qs[128];\n};\n\nstruct __attribute__((packed)) block_q5_K\n{\n    half d;\n    half dmin;\n    uint8_t scales[12];\n    uint8_t qh[32];\n    uint8_t qs[128];\n};\n\nstruct __attribute__((packed)) block_q6_K\n{\n    uint8_t ql[128];\n    uint8_t qh[64];\n    int8_t scales[16];\n    half d;\n};\n\n__kernel void convert_fp16_to_fp32(__global half* x, __global float* y) {\n    const uint i = get_global_id(0);\n\n    y[i] = vload_half(0, &x[i]);\n}\n\nvoid dequantize_q4_0(__global const struct block_q4_0* x, const int ib, const int iqs, float* v0, float* v1) {\n    const float d = vload_half(0, &x[ib].d);\n\n    const uint8_t vui = x[ib].qs[iqs];\n\n    const int8_t vi0 = vui & 0xF;\n    const int8_t vi1 = vui >> 4;\n\n    *v0 = (vi0 - 8)*d;\n    *v1 = (vi1 - 8)*d;\n}\nvoid dequantize_q4_1(__global const struct block_q4_1* x, const int ib, const int iqs, float* v0, float* v1) {\n    const float d = vload_half(0, &x[ib].d);\n    const float m = vload_half(0, &x[ib].m);\n\n    const uint8_t vui = x[ib].qs[iqs];\n\n    const int8_t vi0 = vui & 0xF;\n    const int8_t vi1 = vui >> 4;\n\n    *v0 = vi0*d + m;\n    *v1 = vi1*d + m;\n}\nvoid dequantize_q5_0(__global const struct block_q5_0* x, const int ib, const int iqs, float* v0, float* v1) {\n    const float d = vload_half(0, &x[ib].d);\n\n    uint32_t qh = x[ib].qh;\n\n    const uint8_t xh_0 = ((qh >> (iqs +  0)) << 4) & 0x10;\n    const uint8_t xh_1 = ((qh >> (iqs + 12))     ) & 0x10;\n\n    const int32_t x0 = ((x[ib].qs[iqs] & 0xf) | xh_0) - 16;\n    const int32_t x1 = ((x[ib].qs[iqs] >>  4) | xh_1) - 16;\n\n    *v0 = x0*d;\n    *v1 = x1*d;\n}\nvoid dequantize_q5_1(__global const struct block_q5_1* x, const int ib, const int iqs, float* v0, float* v1) {\n    const float d = vload_half(0, &x[ib].d);\n    const float m = vload_half(0, &x[ib].m);\n\n    uint32_t qh = x[ib].qh;\n\n    const uint8_t xh_0 = ((qh >> (iqs +  0)) << 4) & 0x10;\n    const uint8_t xh_1 = ((qh >> (iqs + 12))     ) & 0x10;\n\n    const int32_t x0 = ((x[ib].qs[iqs] & 0xf) | xh_0);\n    const int32_t x1 = ((x[ib].qs[iqs] >>  4) | xh_1);\n\n    *v0 = x0*d + m;\n    *v1 = x1*d + m;\n}\nvoid dequantize_q8_0(__global const struct block_q8_0* x, const int ib, const int iqs, float* v0, float* v1) {\n    const float d = vload_half(0, &x[ib].d);\n\n    const int8_t vi0 = x[ib].qs[iqs + 0];\n    const int8_t vi1 = x[ib].qs[iqs + 1];\n\n    *v0 = vi0*d;\n    *v1 = vi1*d;\n}\nvoid convert_f16(__global half* x, const int ib, const int iqs, float* v0, float* v1){\n    *v0 = vload_half(0, &x[ib + 0]);\n    *v1 = vload_half(0, &x[ib + 1]);\n}\n);\n\nstatic std::string k_quants_source = MULTILINE_QUOTE(\ninline void get_scale_min_k4(int j, const __global uint8_t *q, uint8_t *d, uint8_t *m)\n{\n    if (j < 4)\n    {\n        *d = q[j] & 63;\n        *m = q[j + 4] & 63;\n    }\n    else\n    {\n        *d = (q[j + 4] & 0xF) | ((q[j - 4] >> 6) << 4);\n        *m = (q[j + 4] >> 4) | ((q[j - 0] >> 6) << 4);\n    }\n}\n\n__kernel void dequantize_block_q2_K(__global const struct block_q2_K *x, __global float *yy)\n{\n    const int i = get_group_id(0) + get_global_offset(0);\n    const int tid = get_local_id(0);\n    const int n = tid / 32;\n    const int l = tid - 32 * n;\n    const int is = 8 * n + l / 16;\n\n    const uint8_t q = x[i].qs[32 * n + l];\n    __global float *y = yy + get_group_id(0) * QK_K + 128 * n;\n\n    const float dall = vload_half(0, &x[i].d);\n    const float dmin = vload_half(0, &x[i].dmin);\n\n    y[l + 0] = dall * (x[i].scales[is + 0] & 0xF) * ((q >> 0) & 3) - dmin * (x[i].scales[is + 0] >> 4);\n    y[l + 32] = dall * (x[i].scales[is + 2] & 0xF) * ((q >> 2) & 3) - dmin * (x[i].scales[is + 2] >> 4);\n    y[l + 64] = dall * (x[i].scales[is + 4] & 0xF) * ((q >> 4) & 3) - dmin * (x[i].scales[is + 4] >> 4);\n    y[l + 96] = dall * (x[i].scales[is + 6] & 0xF) * ((q >> 6) & 3) - dmin * (x[i].scales[is + 6] >> 4);\n}\n\n__kernel void dequantize_block_q3_K(__global const struct block_q3_K *x, __global float *yy)\n{\n    int r = get_local_id(0) / 4;\n    int i = get_group_id(0) + get_global_offset(0);\n    int tid = r / 2;\n    int is0 = r % 2;\n    int l0 = 16 * is0 + 4 * (get_local_id(0) % 4);\n    int n = tid / 4;\n    int j = tid - 4 * n;\n\n    uint8_t m = 1 << (4 * n + j);\n    int is = 8 * n + 2 * j + is0;\n    int shift = 2 * j;\n\n    int8_t us = is < 4 ? (x[i].scales[is - 0] & 0xF) | (((x[i].scales[is + 8] >> 0) & 3) << 4)\n              : is < 8 ? (x[i].scales[is - 0] & 0xF) | (((x[i].scales[is + 4] >> 2) & 3) << 4)\n              : is < 12  ? (x[i].scales[is - 8] >> 4) | (((x[i].scales[is + 0] >> 4) & 3) << 4)\n              : (x[i].scales[is - 8] >> 4) | (((x[i].scales[is - 4] >> 6) & 3) << 4);\n    float d_all = vload_half(0, &x[i].d);\n    float dl = d_all * (us - 32);\n\n    __global float *y = yy + get_group_id(0) * QK_K + 128 * n + 32 * j;\n    const __global uint8_t *q = x[i].qs + 32 * n;\n    const __global uint8_t *hm = x[i].hmask;\n\n    for (int l = l0; l < l0 + 4; ++l)\n        y[l] = dl * ((int8_t)((q[l] >> shift) & 3) - ((hm[l] & m) ? 0 : 4));\n}\n\n__kernel void dequantize_block_q4_K(__global const struct block_q4_K *x, __global float *yy)\n{\n    const int i = get_group_id(0) + get_global_offset(0);\n    const int tid = get_local_id(0);\n    const int il = tid / 8;\n    const int ir = tid % 8;\n    const int is = 2 * il;\n    const int n = 4;\n\n    __global float *y = yy + get_group_id(0) * QK_K + 64 * il + n * ir;\n\n    const float dall = vload_half(0, &x[i].d);\n    const float dmin = vload_half(0, &x[i].dmin);\n\n    __global const uint8_t *q = x[i].qs + 32 * il + n * ir;\n\n    uint8_t sc, m;\n    get_scale_min_k4(is + 0, x[i].scales, &sc, &m);\n    float d1 = dall * sc;\n    float m1 = dmin * m;\n    get_scale_min_k4(is + 1, x[i].scales, &sc, &m);\n    float d2 = dall * sc;\n    float m2 = dmin * m;\n    for (int l = 0; l < n; ++l)\n    {\n        y[l + 0] = d1 * (q[l] & 0xF) - m1;\n        y[l + 32] = d2 * (q[l] >> 4) - m2;\n    }\n}\n\n__kernel void dequantize_block_q5_K(__global const struct block_q5_K *x, __global float *yy)\n{\n    const int i = get_group_id(0) + get_global_offset(0);\n    const int tid = get_local_id(0);\n    const int il = tid / 16;\n    const int ir = tid % 16;\n    const int is = 2 * il;\n\n    __global float *y = yy + get_group_id(0) * QK_K + 64 * il + 2 * ir;\n\n    const float dall = vload_half(0, &x[i].d);\n    const float dmin = vload_half(0, &x[i].dmin);\n\n    __global const uint8_t *ql = x[i].qs + 32 * il + 2 * ir;\n    __global const uint8_t *qh = x[i].qh + 2 * ir;\n\n    uint8_t sc, m;\n    get_scale_min_k4(is + 0, x[i].scales, &sc, &m);\n    const float d1 = dall * sc;\n    const float m1 = dmin * m;\n    get_scale_min_k4(is + 1, x[i].scales, &sc, &m);\n    const float d2 = dall * sc;\n    const float m2 = dmin * m;\n\n    uint8_t hm = 1 << (2 * il);\n    y[0] = d1 * ((ql[0] & 0xF) + (qh[0] & hm ? 16 : 0)) - m1;\n    y[1] = d1 * ((ql[1] & 0xF) + (qh[1] & hm ? 16 : 0)) - m1;\n    hm <<= 1;\n    y[32] = d2 * ((ql[0] >> 4) + (qh[0] & hm ? 16 : 0)) - m2;\n    y[33] = d2 * ((ql[1] >> 4) + (qh[1] & hm ? 16 : 0)) - m2;\n}\n\n__kernel void dequantize_block_q6_K(__global const struct block_q6_K *x, __global float *yy)\n{\n    const int i = get_group_id(0) + get_global_offset(0);\n    const int tid = get_local_id(0);\n    const int ip = tid / 32;\n    const int il = tid - 32 * ip;\n    const int is = 8 * ip + il / 16;\n\n    __global float *y = yy + get_group_id(0) * QK_K + 128 * ip + il;\n\n    const float d = vload_half(0, &x[i].d);\n\n    __global const uint8_t *ql = x[i].ql + 64 * ip + il;\n    const uint8_t qh = x[i].qh[32 * ip + il];\n    __global const int8_t *sc = x[i].scales + is;\n\n    y[0] = d * sc[0] * ((int8_t)((ql[0] & 0xF) | (((qh >> 0) & 3) << 4)) - 32);\n    y[32] = d * sc[2] * ((int8_t)((ql[32] & 0xF) | (((qh >> 2) & 3) << 4)) - 32);\n    y[64] = d * sc[4] * ((int8_t)((ql[0] >> 4) | (((qh >> 4) & 3) << 4)) - 32);\n    y[96] = d * sc[6] * ((int8_t)((ql[32] >> 4) | (((qh >> 6) & 3) << 4)) - 32);\n}\n\n__kernel void dequantize_mul_mat_vec_q2_K(__global const struct block_q2_K * xx, __local float* tmp, __global float* yy, __global float* dst, const int ncols) {\n\n    const int row = get_group_id(0);\n\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row + get_global_offset(0);\n\n    __global const struct block_q2_K * x = xx + ib0;\n\n    const int tid = get_local_id(0)/K_QUANTS_PER_ITERATION;  // 0...31 or 0...15\n    const int ix  = get_local_id(0)%K_QUANTS_PER_ITERATION;  // 0 or 0,1\n\n    const int step = 16/K_QUANTS_PER_ITERATION;\n\n    const int im = tid/step;                             // 0 or 1. 0 computes 0..., 1 computes 128...\n    const int in = tid - step*im;                        // 0...15 or 0...7\n\n    const int l0 = K_QUANTS_PER_ITERATION*in;            // 0...15 or 0...14 in steps of 2\n    const int q_offset = 32*im + l0;\n    const int s_offset = 8*im;\n    const int y_offset = 128*im + l0;\n\n    tmp[16 * ix + tid] = 0;\n\n    uint32_t aux[4];\n    const uint8_t * d = (const uint8_t *)aux;\n    const uint8_t * m = (const uint8_t *)(aux + 2);\n\n    for (int i = ix; i < num_blocks_per_row; i += K_QUANTS_PER_ITERATION) {\n\n        __global const float   * y = yy + i * QK_K + y_offset;\n        __global const uint8_t * q = x[i].qs + q_offset;\n\n        const float dall = vload_half(0, &x[i].d);\n        const float dmin = vload_half(0, &x[i].dmin);\n\n        __global const uint32_t * a = (__global const uint32_t *)(x[i].scales + s_offset);\n        aux[0] = a[0] & 0x0f0f0f0f;\n        aux[1] = a[1] & 0x0f0f0f0f;\n        aux[2] = (a[0] >> 4) & 0x0f0f0f0f;\n        aux[3] = (a[1] >> 4) & 0x0f0f0f0f;\n\n        float sum1 = 0, sum2 = 0;\n        for (int l = 0; l < K_QUANTS_PER_ITERATION; ++l) {\n            sum1 += y[l+ 0] * d[0] * ((q[l+ 0] >> 0) & 3)\n                  + y[l+32] * d[2] * ((q[l+ 0] >> 2) & 3)\n                  + y[l+64] * d[4] * ((q[l+ 0] >> 4) & 3)\n                  + y[l+96] * d[6] * ((q[l+ 0] >> 6) & 3)\n                  + y[l+16] * d[1] * ((q[l+16] >> 0) & 3)\n                  + y[l+48] * d[3] * ((q[l+16] >> 2) & 3)\n                  + y[l+80] * d[5] * ((q[l+16] >> 4) & 3)\n                  +y[l+112] * d[7] * ((q[l+16] >> 6) & 3);\n            sum2 += y[l+ 0] * m[0] + y[l+32] * m[2] + y[l+64] * m[4] + y[ l+96] * m[6]\n                  + y[l+16] * m[1] + y[l+48] * m[3] + y[l+80] * m[5] + y[l+112] * m[7];\n\n        }\n        tmp[16 * ix + tid] += dall * sum1 - dmin * sum2;\n\n    }\n\n    // sum up partial sums and write back result\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for (int s=16; s>0; s>>=1) {\n        if (tid < s) {\n            tmp[tid] += tmp[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n    if (tid == 0) {\n        dst[row] = tmp[0];\n    }\n}\n\n__kernel void dequantize_mul_mat_vec_q3_K(__global const struct block_q3_K * xx, __local float* tmp, __global float* yy, __global float* dst, const int ncols) {\n    const uint16_t kmask1 = 0x0303;\n    const uint16_t kmask2 = 0x0f0f;\n\n    const int row = get_group_id(0);\n\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row + get_global_offset(0);\n\n    __global const struct block_q3_K * x = xx + ib0;\n\n    const int tid = get_local_id(0)/K_QUANTS_PER_ITERATION;  // 0...31 or 0...16\n    const int ix  = get_local_id(0)%K_QUANTS_PER_ITERATION;  // 0 or 0,1\n\n    const int n  = K_QUANTS_PER_ITERATION;               // iterations in the inner loop\n    const int step = 16/K_QUANTS_PER_ITERATION;\n    const int im = tid/step;                             // 0 or 1. 0 computes 0..., 1 computes 128...\n    const int in = tid - step*im;                        // 0....15 or 0...7\n\n    const uint8_t m = 1 << (4*im);\n\n    const int l0 = n*in;                                 // 0...15 or 0...14 in steps of 2\n    const int q_offset =  32*im + l0;\n    const int y_offset = 128*im + l0;\n\n    uint16_t utmp[4];\n    const int8_t * s = (const int8_t *)utmp;\n\n    const uint16_t s_shift = 4*im;\n\n    tmp[16 * ix + tid] = 0;\n\n    for (int i = ix; i < num_blocks_per_row; i += K_QUANTS_PER_ITERATION) {\n\n        __global const float   * y  = yy + i * QK_K + y_offset;\n        __global const uint8_t * q = x[i].qs + q_offset;\n        __global const uint8_t * h = x[i].hmask + l0;\n\n        __global const uint16_t * a = (__global const uint16_t *)x[i].scales;\n        utmp[0] = ((a[0] >> s_shift) & kmask2) | (((a[4] >> (s_shift + 0)) & kmask1) << 4);\n        utmp[1] = ((a[1] >> s_shift) & kmask2) | (((a[5] >> (s_shift + 0)) & kmask1) << 4);\n        utmp[2] = ((a[2] >> s_shift) & kmask2) | (((a[4] >> (s_shift + 2)) & kmask1) << 4);\n        utmp[3] = ((a[3] >> s_shift) & kmask2) | (((a[5] >> (s_shift + 2)) & kmask1) << 4);\n\n        const float d = vload_half(0, &x[i].d);\n\n        float sum = 0;\n        for (int l = 0; l < n; ++l) {\n            sum += y[l+ 0] * (s[0] - 32) * (((q[l] >> 0) & 3) - (h[l] & (m << 0) ? 0 : 4))\n                 + y[l+32] * (s[2] - 32) * (((q[l] >> 2) & 3) - (h[l] & (m << 1) ? 0 : 4))\n                 + y[l+64] * (s[4] - 32) * (((q[l] >> 4) & 3) - (h[l] & (m << 2) ? 0 : 4))\n                 + y[l+96] * (s[6] - 32) * (((q[l] >> 6) & 3) - (h[l] & (m << 3) ? 0 : 4));\n            sum += y[l+16] * (s[1] - 32) * (((q[l+16] >> 0) & 3) - (h[l+16] & (m << 0) ? 0 : 4))\n                 + y[l+48] * (s[3] - 32) * (((q[l+16] >> 2) & 3) - (h[l+16] & (m << 1) ? 0 : 4))\n                 + y[l+80] * (s[5] - 32) * (((q[l+16] >> 4) & 3) - (h[l+16] & (m << 2) ? 0 : 4))\n                + y[l+112] * (s[7] - 32) * (((q[l+16] >> 6) & 3) - (h[l+16] & (m << 3) ? 0 : 4));\n        }\n        tmp[16 * ix + tid] += d * sum;\n\n    }\n\n    // sum up partial sums and write back result\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for (int s=16; s>0; s>>=1) {\n        if (tid < s) {\n            tmp[tid] += tmp[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n    if (tid == 0) {\n        dst[row] = tmp[0];\n    }\n}\n\n__kernel void dequantize_mul_mat_vec_q4_K(__global const struct block_q4_K * xx, __local float* tmp, __global float* yy, __global float* dst, const int ncols) {\n\n    //to rename it later, just to test now\n    const uint16_t kmask1 = 0x3f3f;\n    const uint16_t kmask2 = 0x0f0f;\n    const uint16_t kmask3 = 0xc0c0;\n\n    const int row = get_group_id(0);\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row + get_global_offset(0);\n\n    const int tid = get_local_id(0)/K_QUANTS_PER_ITERATION;  // 0...15\n    const int ix  = get_local_id(0)%K_QUANTS_PER_ITERATION;\n\n    const int step = 8/K_QUANTS_PER_ITERATION;\n\n    const int il  = tid/step;     // 0...3\n    const int ir  = tid - step*il;// 0...3\n    const int n   = 2*K_QUANTS_PER_ITERATION;\n\n    const int im = il/2;  // 0 or 1. 0 computes 0,32 + 128,160, 1 computes 64,96 + 192,224\n    const int in = il%2;\n\n    const int l0 = n*(2*ir + in);\n    const int q_offset = 32*im + l0;\n    const int y_offset = 64*im + l0;\n\n    uint16_t aux[4];\n    const uint8_t * sc = (const uint8_t *)aux;\n\n    __global const struct block_q4_K * x = xx + ib0;\n\n    tmp[16 * ix + tid] = 0;\n\n    for (int i = ix; i < num_blocks_per_row; i += K_QUANTS_PER_ITERATION) {\n\n        __global const uint8_t * q1 = x[i].qs + q_offset;\n        __global const uint8_t * q2 = q1 + 64;\n        __global const float   * y1 = yy + i*QK_K + y_offset;\n        __global const float   * y2 = y1 + 128;\n\n        const float dall = vload_half(0, &x[i].d);\n        const float dmin = vload_half(0, &x[i].dmin);\n\n        __global const uint16_t * a = (__global const uint16_t *)x[i].scales;\n        aux[0] = a[im+0] & kmask1;\n        aux[1] = a[im+2] & kmask1;\n        aux[2] = ((a[im+4] >> 0) & kmask2) | ((a[im+0] & kmask3) >> 2);\n        aux[3] = ((a[im+4] >> 4) & kmask2) | ((a[im+2] & kmask3) >> 2);\n\n        float4 s = (float4)(0.f);\n        float smin = 0;\n        for (int l = 0; l < n; ++l) {\n            s.x += y1[l] * (q1[l] & 0xF); s.y += y1[l+32] * (q1[l] >> 4);\n            s.z += y2[l] * (q2[l] & 0xF); s.w += y2[l+32] * (q2[l] >> 4);\n            smin += y1[l] * sc[2] + y1[l+32] * sc[3] + y2[l] * sc[6] + y2[l+32] * sc[7];\n        }\n        tmp[16 * ix + tid] += dall * (s.x * sc[0] + s.y * sc[1] + s.z * sc[4] + s.w * sc[5]) - dmin * smin;\n\n    }\n\n    // sum up partial sums and write back result\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for (int s=16; s>0; s>>=1) {\n        if (tid < s) {\n            tmp[tid] += tmp[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n    if (tid == 0) {\n        dst[row] = tmp[0];\n    }\n}\n\n__kernel void dequantize_mul_mat_vec_q5_K(__global const struct block_q5_K * xx, __local float* tmp, __global float* yy, __global float* dst, const int ncols) {\n\n    const uint16_t kmask1 = 0x3f3f;\n    const uint16_t kmask2 = 0x0f0f;\n    const uint16_t kmask3 = 0xc0c0;\n\n    const int row = get_group_id(0);\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row + get_global_offset(0);\n\n    const int tid = get_local_id(0)/2;  // 0...15\n    const int ix  = get_local_id(0)%2;\n\n    const int il  = tid/4;     // 0...3\n    const int ir  = tid - 4*il;// 0...3\n    const int n   = 2;\n\n    const int im = il/2;  // 0 or 1. 0 computes 0,32 + 128,160, 1 computes 64,96 + 192,224\n    const int in = il%2;\n\n    const int l0 = n*(2*ir + in);\n    const int q_offset = 32*im + l0;\n    const int y_offset = 64*im + l0;\n\n    const uint8_t hm1  = 1 << (2*im);\n    const uint8_t hm2  = hm1 << 4;\n\n    uint16_t aux[4];\n    const uint8_t * sc = (const uint8_t *)aux;\n\n    __global const struct block_q5_K * x = xx + ib0;\n\n    tmp[16 * ix + tid] = 0;\n\n    for (int i = ix; i < num_blocks_per_row; i += 2) {\n\n        __global const uint8_t * ql1 = x[i].qs + q_offset;\n        __global const uint8_t * ql2 = ql1 + 64;\n        __global const uint8_t * qh  = x[i].qh + l0;\n        __global const float   * y1  = yy + i*QK_K + y_offset;\n        __global const float   * y2  = y1 + 128;\n\n        const float dall = vload_half(0, &x[i].d);\n        const float dmin = vload_half(0, &x[i].dmin);\n\n        __global const uint16_t * a = (__global const uint16_t *)x[i].scales;\n        aux[0] = a[im+0] & kmask1;\n        aux[1] = a[im+2] & kmask1;\n        aux[2] = ((a[im+4] >> 0) & kmask2) | ((a[im+0] & kmask3) >> 2);\n        aux[3] = ((a[im+4] >> 4) & kmask2) | ((a[im+2] & kmask3) >> 2);\n\n        float4 sum = (float4)(0.f);\n        float smin = 0;\n        for (int l = 0; l < n; ++l) {\n            sum.x += y1[l+ 0] * ((ql1[l+ 0] & 0xF) + (qh[l+ 0] & (hm1 << 0) ? 16 : 0))\n                   + y1[l+16] * ((ql1[l+16] & 0xF) + (qh[l+16] & (hm1 << 0) ? 16 : 0));\n            sum.y += y1[l+32] * ((ql1[l+ 0] >>  4) + (qh[l+ 0] & (hm1 << 1) ? 16 : 0))\n                   + y1[l+48] * ((ql1[l+16] >>  4) + (qh[l+16] & (hm1 << 1) ? 16 : 0));\n            sum.z += y2[l+ 0] * ((ql2[l+ 0] & 0xF) + (qh[l+ 0] & (hm2 << 0) ? 16 : 0))\n                   + y2[l+16] * ((ql2[l+16] & 0xF) + (qh[l+16] & (hm2 << 0) ? 16 : 0));\n            sum.w += y2[l+32] * ((ql2[l+ 0] >>  4) + (qh[l+ 0] & (hm2 << 1) ? 16 : 0))\n                   + y2[l+48] * ((ql2[l+16] >>  4) + (qh[l+16] & (hm2 << 1) ? 16 : 0));\n            smin += (y1[l] + y1[l+16]) * sc[2] + (y1[l+32] + y1[l+48]) * sc[3]\n                  + (y2[l] + y2[l+16]) * sc[6] + (y2[l+32] + y2[l+48]) * sc[7];\n        }\n        tmp[16 * ix + tid] += dall * (sum.x * sc[0] + sum.y * sc[1] + sum.z * sc[4] + sum.w * sc[5]) - dmin * smin;\n\n    }\n\n    // sum up partial sums and write back result\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for (int s=16; s>0; s>>=1) {\n        if (tid < s) {\n            tmp[tid] += tmp[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n    if (tid == 0) {\n        dst[row] = tmp[0];\n    }\n}\n\n__kernel void dequantize_mul_mat_vec_q6_K(__global const struct block_q6_K * xx, __local float* tmp, __global const float * yy, __global float * dst, const int ncols) {\n\n    const int row = get_group_id(0);\n\n    const int num_blocks_per_row = ncols / QK_K;\n    const int ib0 = row*num_blocks_per_row + get_global_offset(0);\n\n    __global const struct block_q6_K * x = xx + ib0;\n\n    const int tid = get_local_id(0)/K_QUANTS_PER_ITERATION;  // 0...31 or 0...16\n    const int ix  = get_local_id(0)%K_QUANTS_PER_ITERATION;  // 0 or 0, 1\n\n    const int step = 16/K_QUANTS_PER_ITERATION;          // 16 or 8\n\n    const int im = tid/step;                             // 0 or 1. 0 computes 0..., 1 computes 128...\n    const int in = tid - step*im;                        // 0...15 or 0...7\n\n\\n#if K_QUANTS_PER_ITERATION == 1\\n\n    const int l0 = K_QUANTS_PER_ITERATION*in;            // 0...15\n    const int is = 0;\n\n\\n#else\\n\n\n    const int l0 = 4 * in;                               // 0, 4, 8, ..., 28\n    const int is = in / 4;\n\n\\n#endif\\n\n\n    const int ql_offset = 64*im + l0;\n    const int qh_offset = 32*im + l0;\n    const int s_offset  =  8*im + is;\n    const int y_offset = 128*im + l0;\n\n    tmp[16 * ix + tid] = 0; // partial sum for thread in warp\n\n    for (int i = ix; i < num_blocks_per_row; i += K_QUANTS_PER_ITERATION) {\n\n        __global const float   * y  = yy + i * QK_K + y_offset;\n        __global const uint8_t * ql = x[i].ql + ql_offset;\n        __global const uint8_t * qh = x[i].qh + qh_offset;\n        __global const int8_t  * s  = x[i].scales + s_offset;\n\n        const float d = vload_half(0, &x[i].d);\n\n\\n#if K_QUANTS_PER_ITERATION == 1\\n\n        float sum = y[ 0] * s[0] * d * ((int8_t)((ql[ 0] & 0xF) | ((qh[ 0] & 0x03) << 4)) - 32)\n                  + y[16] * s[1] * d * ((int8_t)((ql[16] & 0xF) | ((qh[16] & 0x03) << 4)) - 32)\n                  + y[32] * s[2] * d * ((int8_t)((ql[32] & 0xF) | ((qh[ 0] & 0x0c) << 2)) - 32)\n                  + y[48] * s[3] * d * ((int8_t)((ql[48] & 0xF) | ((qh[16] & 0x0c) << 2)) - 32)\n                  + y[64] * s[4] * d * ((int8_t)((ql[ 0]  >> 4) | ((qh[ 0] & 0x30) >> 0)) - 32)\n                  + y[80] * s[5] * d * ((int8_t)((ql[16]  >> 4) | ((qh[16] & 0x30) >> 0)) - 32)\n                  + y[96] * s[6] * d * ((int8_t)((ql[32]  >> 4) | ((qh[ 0] & 0xc0) >> 2)) - 32)\n                  +y[112] * s[7] * d * ((int8_t)((ql[48]  >> 4) | ((qh[16] & 0xc0) >> 2)) - 32);\n        tmp[16 * ix + tid] += sum;\n\\n#else\\n\n        float sum = 0;\n        for (int l = 0; l < 4; ++l) {\n            sum += y[l+ 0] * s[0] * d * ((int8_t)((ql[l+ 0] & 0xF) | (((qh[l] >> 0) & 3) << 4)) - 32)\n                 + y[l+32] * s[2] * d * ((int8_t)((ql[l+32] & 0xF) | (((qh[l] >> 2) & 3) << 4)) - 32)\n                 + y[l+64] * s[4] * d * ((int8_t)((ql[l+ 0]  >> 4) | (((qh[l] >> 4) & 3) << 4)) - 32)\n                 + y[l+96] * s[6] * d * ((int8_t)((ql[l+32]  >> 4) | (((qh[l] >> 6) & 3) << 4)) - 32);\n        }\n        tmp[16 * ix + tid] += sum;\n\\n#endif\\n\n\n    }\n\n    // sum up partial sums and write back result\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for (int s=16; s>0; s>>=1) {\n        if (tid < s) {\n            tmp[tid] += tmp[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n    if (tid == 0) {\n        dst[row] = tmp[0];\n    }\n}\n\n);\n\n\nstd::string dequant_template = MULTILINE_QUOTE(\n__kernel void KERNEL_NAME(__global X_TYPE* x, __global float* y) {\n    const int i = get_group_id(0)*get_local_size(0) + get_local_id(0)*2;\n\n    if (i >= get_global_size(0)) {\n        return;\n    }\n\n    const uint qk = QUANT_K;\n    const uint qr = QUANT_R;\n\n    const int ib = i/qk + get_global_offset(0); // block index\n    const int iqs = (i%qk)/qr; // quant index\n    const int iybs = i - i%qk; // y block start index\n    const int y_offset = qr == 1 ? 1 : qk/2;\n\n    // dequantize\n    float v0, v1;\n    DEQUANT_FUNC(x, ib, iqs, &v0, &v1);\n    y[iybs + iqs + 0] = v0;\n    y[iybs + iqs + y_offset] = v1;\n}\n);\n\nstd::string dequant_mul_mat_vec_template = MULTILINE_QUOTE(\n__kernel void KERNEL_NAME(__global X_TYPE* x, __local float* tmp, __global float* y, __global float* dst, const int ncols) {\n    const int local_size = get_local_size(0);\n    const int row = get_group_id(0);\n    const int tid = get_local_id(0);\n\n    const uint qk = QUANT_K;\n    const uint qr = QUANT_R;\n\n    const int col_step = local_size * 2;\n    const int y_offset = qr == 1 ? 1 : qk/2;\n\n    x += get_global_offset(0);\n\n    tmp[tid] = 0;\n\n    for (int col = tid*2; col < ncols; col += col_step) {\n        const int ib = (row*ncols + col)/qk; // block index\n        const int iqs = (col%qk)/qr; // quant index\n        const int iybs = col - col%qk; // y block start index\n\n        // dequantize\n        float v0, v1;\n        DEQUANT_FUNC(x, ib, iqs, &v0, &v1);\n\n        // matrix multiplication\n        tmp[tid] += v0 * y[iybs + iqs + 0];\n        tmp[tid] += v1 * y[iybs + iqs + y_offset];\n    }\n\n    // sum up partial sums and write back result\n    barrier(CLK_LOCAL_MEM_FENCE);\n    for (int s=local_size/2; s>0; s>>=1) {\n        if (tid < s) {\n            tmp[tid] += tmp[tid + s];\n        }\n        barrier(CLK_LOCAL_MEM_FENCE);\n    }\n    if (tid == 0) {\n        dst[row] = tmp[0];\n    }\n}\n);\n\n\nstd::string mul_template = MULTILINE_QUOTE(\n__kernel void KERNEL_NAME(__global TYPE* x, const int x_offset, __global TYPE* y, const int y_offset, __global TYPE* dst, const int dst_offset, const int ky) {\n    const int i = get_group_id(0)*get_local_size(0) + get_local_id(0);\n\n    if (i >= get_global_size(0)) {\n        return;\n    }\n\n    dst[dst_offset + i] = x[x_offset + i] * y[y_offset + i%ky];\n}\n);\n\n#define CL_CHECK(err)                                               \\\n    do {                                                            \\\n        cl_int err_ = (err);                                        \\\n        if (err_ != CL_SUCCESS) {                                   \\\n            fprintf(stderr, \"ggml_opencl: %s error %d at %s:%d\\n\",  \\\n                #err, err_, __FILE__, __LINE__);                    \\\n            exit(1);                                                \\\n        }                                                           \\\n    } while (0)\n\n#define CLBLAST_CHECK(err)                                          \\\n    do {                                                            \\\n        CLBlastStatusCode err_ = (err);                             \\\n        if (err_ != CLBlastSuccess) {                               \\\n            fprintf(stderr, \"ggml_opencl: %s error %d at %s:%d\\n\",  \\\n                #err, err_, __FILE__, __LINE__);                    \\\n            exit(1);                                                \\\n        }                                                           \\\n    } while (0)\n\nstd::array<std::string, 5> dequant_str_keys = {\n    \"KERNEL_NAME\", \"X_TYPE\", \"QUANT_K\", \"QUANT_R\", \"DEQUANT_FUNC\"\n};\n\nstd::array<std::string, 30> dequant_str_values = {\n    \"dequantize_row_q4_0\", \"struct block_q4_0\", \"QK4_0\", \"QR4_0\", \"dequantize_q4_0\",\n    \"dequantize_row_q4_1\", \"struct block_q4_1\", \"QK4_1\", \"QR4_1\", \"dequantize_q4_1\",\n    \"dequantize_row_q5_0\", \"struct block_q5_0\", \"QK5_0\", \"QR5_0\", \"dequantize_q5_0\",\n    \"dequantize_row_q5_1\", \"struct block_q5_1\", \"QK5_1\", \"QR5_1\", \"dequantize_q5_1\",\n    \"dequantize_row_q8_0\", \"struct block_q8_0\", \"QK8_0\", \"QR8_0\", \"dequantize_q8_0\",\n    \"convert_row_f16\", \"half\", \"1\", \"1\", \"convert_f16\"\n};\n\nstd::array<std::string, 30> dequant_mul_mat_vec_str_values = {\n    \"dequantize_mul_mat_vec_q4_0\", \"struct block_q4_0\", \"QK4_0\", \"QR4_0\", \"dequantize_q4_0\",\n    \"dequantize_mul_mat_vec_q4_1\", \"struct block_q4_1\", \"QK4_1\", \"QR4_1\", \"dequantize_q4_1\",\n    \"dequantize_mul_mat_vec_q5_0\", \"struct block_q5_0\", \"QK5_0\", \"QR5_0\", \"dequantize_q5_0\",\n    \"dequantize_mul_mat_vec_q5_1\", \"struct block_q5_1\", \"QK5_1\", \"QR5_1\", \"dequantize_q5_1\",\n    \"dequantize_mul_mat_vec_q8_0\", \"struct block_q8_0\", \"QK8_0\", \"QR8_0\", \"dequantize_q8_0\",\n    \"convert_mul_mat_vec_f16\", \"half\", \"1\", \"1\", \"convert_f16\"\n};\n\nstd::array<std::string, 2> mul_str_keys = {\n    \"KERNEL_NAME\", \"TYPE\"\n};\nstd::array<std::string, 2> mul_str_values = {\n    \"mul_f32\", \"float\"\n};\n\nstatic std::string& replace(std::string& s, const std::string& from, const std::string& to) {\n    size_t pos = 0;\n    while ((pos = s.find(from, pos)) != std::string::npos) {\n         s.replace(pos, from.length(), to);\n         pos += to.length();\n    }\n    return s;\n}\n\nstatic std::string generate_kernels() {\n    std::stringstream src;\n    src << program_source << '\\n';\n    src << k_quants_source << '\\n';\n    for (size_t i = 0; i < dequant_str_values.size(); i += dequant_str_keys.size()) {\n        std::string dequant_kernel = dequant_template;\n        std::string dmmv_kernel = dequant_mul_mat_vec_template;\n        for (size_t j = 0; j < dequant_str_keys.size(); j++) {\n            replace(dequant_kernel, dequant_str_keys[j], dequant_str_values[i + j]);\n            replace(dmmv_kernel, dequant_str_keys[j], dequant_mul_mat_vec_str_values[i + j]);\n        }\n        src << dequant_kernel << '\\n';\n        src << dmmv_kernel << '\\n';\n    }\n    for (size_t i = 0; i < mul_str_values.size(); i += mul_str_keys.size()) {\n        std::string mul_kernel = mul_template;\n        for (size_t j = 0; j < mul_str_keys.size(); j++) {\n            replace(mul_kernel, mul_str_keys[j], mul_str_values[i + j]);\n        }\n        src << mul_kernel << '\\n';\n    }\n\n    return src.str();\n}\n\nstatic cl_platform_id platform;\nstatic cl_device_id device;\nstatic cl_context context;\nstatic cl_command_queue queue;\nstatic cl_program program;\nstatic cl_kernel convert_row_f16_cl;\nstatic cl_kernel dequantize_row_q4_0_cl, dequantize_row_q4_1_cl, dequantize_row_q5_0_cl, dequantize_row_q5_1_cl, dequantize_row_q8_0_cl;\nstatic cl_kernel dequantize_mul_mat_vec_q4_0_cl, dequantize_mul_mat_vec_q4_1_cl, dequantize_mul_mat_vec_q5_0_cl, dequantize_mul_mat_vec_q5_1_cl, dequantize_mul_mat_vec_q8_0_cl, convert_mul_mat_vec_f16_cl;\nstatic cl_kernel dequantize_block_q2_k_cl, dequantize_block_q3_k_cl, dequantize_block_q4_k_cl, dequantize_block_q5_k_cl, dequantize_block_q6_k_cl;\nstatic cl_kernel dequantize_mul_mat_vec_q2_K_cl, dequantize_mul_mat_vec_q3_K_cl, dequantize_mul_mat_vec_q4_K_cl, dequantize_mul_mat_vec_q5_K_cl, dequantize_mul_mat_vec_q6_K_cl;\nstatic cl_kernel mul_f32_cl;\nstatic bool fp16_support;\n\nstatic cl_program build_program_from_source(cl_context ctx, cl_device_id dev, const char* program_buffer) {\n    cl_program p;\n    char *program_log;\n    size_t program_size;\n    size_t log_size;\n    int err;\n\n    program_size = strlen(program_buffer);\n\n    p = clCreateProgramWithSource(ctx, 1, (const char**)&program_buffer, &program_size, &err);\n    if(err < 0) {\n        fprintf(stderr, \"OpenCL error creating program\");\n        exit(1);\n    }\n\n    std::string compile_opts = \"-cl-mad-enable -cl-unsafe-math-optimizations -cl-finite-math-only -cl-fast-relaxed-math \"\n                               \"-DQK4_0=32 -DQR4_0=2 -DQK4_1=32 -DQR4_1=2 -DQK5_0=32 -DQR5_0=2 -DQK5_1=32 -DQR5_1=2 -DQK8_0=32 -DQR8_0=1 \"\n                               \"-DQK_K=256 -DK_QUANTS_PER_ITERATION=\" + std::to_string(K_QUANTS_PER_ITERATION);\n\n    err = clBuildProgram(p, 0, NULL, compile_opts.c_str(), NULL, NULL);\n    if(err < 0) {\n\n        clGetProgramBuildInfo(p, dev, CL_PROGRAM_BUILD_LOG, 0, NULL, &log_size);\n        program_log = (char*) malloc(log_size + 1);\n        program_log[log_size] = '\\0';\n        clGetProgramBuildInfo(p, dev, CL_PROGRAM_BUILD_LOG, log_size + 1, program_log, NULL);\n        fprintf(stderr, \"ggml_opencl: kernel compile error:\\n\\n%s\\n\", program_log);\n        free(program_log);\n        exit(1);\n    }\n\n    return p;\n}\n\nvoid ggml_cl_init(void) {\n    cl_int err;\n\n    struct cl_device;\n    struct cl_platform {\n        cl_platform_id id;\n        unsigned number;\n        char name[128];\n        char vendor[128];\n        struct cl_device * devices;\n        unsigned n_devices;\n        struct cl_device * default_device;\n    };\n\n    struct cl_device {\n        struct cl_platform * platform;\n        cl_device_id id;\n        unsigned number;\n        cl_device_type type;\n        char name[128];\n    };\n\n    enum { NPLAT = 16, NDEV = 16 };\n\n    struct cl_platform platforms[NPLAT];\n    unsigned n_platforms = 0;\n    struct cl_device devices[NDEV];\n    unsigned n_devices = 0;\n    struct cl_device * default_device = NULL;\n\n    platform = NULL;\n    device = NULL;\n\n    cl_platform_id platform_ids[NPLAT];\n    CL_CHECK(clGetPlatformIDs(NPLAT, platform_ids, &n_platforms));\n\n    for (unsigned i = 0; i < n_platforms; i++) {\n        struct cl_platform * p = &platforms[i];\n        p->number = i;\n        p->id = platform_ids[i];\n        CL_CHECK(clGetPlatformInfo(p->id, CL_PLATFORM_NAME, sizeof(p->name), &p->name, NULL));\n        CL_CHECK(clGetPlatformInfo(p->id, CL_PLATFORM_VENDOR, sizeof(p->vendor), &p->vendor, NULL));\n\n        cl_device_id device_ids[NDEV];\n        cl_int clGetDeviceIDsError = clGetDeviceIDs(p->id, CL_DEVICE_TYPE_ALL, NDEV, device_ids, &p->n_devices);\n        if (clGetDeviceIDsError == CL_DEVICE_NOT_FOUND) {\n            p->n_devices = 0;\n        } else {\n            CL_CHECK(clGetDeviceIDsError);\n        }\n        p->devices = p->n_devices > 0 ? &devices[n_devices] : NULL;\n        p->default_device = NULL;\n\n        for (unsigned j = 0; j < p->n_devices; j++) {\n            struct cl_device * d = &devices[n_devices];\n            d->number = n_devices++;\n            d->id = device_ids[j];\n            d->platform = p;\n            CL_CHECK(clGetDeviceInfo(d->id, CL_DEVICE_NAME, sizeof(d->name), &d->name, NULL));\n            CL_CHECK(clGetDeviceInfo(d->id, CL_DEVICE_TYPE, sizeof(d->type), &d->type, NULL));\n\n            if (p->default_device == NULL && d->type == CL_DEVICE_TYPE_GPU) {\n                p->default_device = d;\n            }\n        }\n\n        if (default_device == NULL && p->default_device != NULL) {\n            default_device = p->default_device;\n        }\n    }\n\n    if (n_devices == 0) {\n        fprintf(stderr, \"ggml_opencl: could find any OpenCL devices.\\n\");\n        exit(1);\n    }\n\n    char * user_platform_string = getenv(\"GGML_OPENCL_PLATFORM\");\n    char * user_device_string = getenv(\"GGML_OPENCL_DEVICE\");\n    int user_platform_number = -1;\n    int user_device_number = -1;\n\n    unsigned n;\n    if (user_platform_string != NULL && sscanf(user_platform_string, \" %u\", &n) == 1 && n < n_platforms) {\n        user_platform_number = (int)n;\n    }\n    if (user_device_string != NULL && sscanf(user_device_string, \" %u\", &n) == 1 && n < n_devices) {\n        user_device_number = (int)n;\n    }\n    if (user_platform_number != -1 && user_device_number != -1) {\n        cl_platform* platform = &platforms[user_platform_number];\n        if ((unsigned)user_device_number >= platform->n_devices) {\n            fprintf(stderr, \"ggml_opencl: invalid device number %d\\n\", user_device_number);\n            exit(1);\n        }\n        default_device = &platform->devices[user_device_number];\n    } else {\n\n        struct cl_device * selected_devices = devices;\n        unsigned n_selected_devices = n_devices;\n\n        if (user_platform_number == -1 && user_platform_string != NULL && user_platform_string[0] != 0) {\n            for (unsigned i = 0; i < n_platforms; i++) {\n                struct cl_platform * p = &platforms[i];\n                if (strstr(p->name, user_platform_string) != NULL ||\n                    strstr(p->vendor, user_platform_string) != NULL) {\n                    user_platform_number = (int)i;\n                    break;\n                }\n            }\n            if (user_platform_number == -1) {\n                fprintf(stderr, \"ggml_opencl: no platform matching '%s' was found.\\n\", user_platform_string);\n                exit(1);\n            }\n        }\n        if (user_platform_number != -1) {\n            struct cl_platform * p = &platforms[user_platform_number];\n            selected_devices = p->devices;\n            n_selected_devices = p->n_devices;\n            default_device = p->default_device;\n            if (n_selected_devices == 0) {\n                fprintf(stderr, \"ggml_opencl: selected platform '%s' does not have any devices.\\n\", p->name);\n                exit(1);\n            }\n        }\n\n        if (user_device_number == -1 && user_device_string != NULL && user_device_string[0] != 0) {\n            for (unsigned i = 0; i < n_selected_devices; i++) {\n                struct cl_device * d = &selected_devices[i];\n                if (strstr(d->name, user_device_string) != NULL) {\n                    user_device_number = d->number;\n                    break;\n                }\n            }\n            if (user_device_number == -1) {\n                fprintf(stderr, \"ggml_opencl: no device matching '%s' was found.\\n\", user_device_string);\n                exit(1);\n            }\n        }\n        if (user_device_number != -1) {\n            selected_devices = &devices[user_device_number];\n            n_selected_devices = 1;\n            default_device = &selected_devices[0];\n        }\n\n        GGML_ASSERT(n_selected_devices > 0);\n\n        if (default_device == NULL) {\n            default_device = &selected_devices[0];\n        }\n    }\n\n    fprintf(stderr, \"ggml_opencl: selecting platform: '%s'\\n\", default_device->platform->name);\n    fprintf(stderr, \"ggml_opencl: selecting device: '%s'\\n\", default_device->name);\n    if (default_device->type != CL_DEVICE_TYPE_GPU) {\n        fprintf(stderr, \"ggml_opencl: warning, not a GPU: '%s'.\\n\", default_device->name);\n    }\n\n    platform = default_device->platform->id;\n    device = default_device->id;\n\n    size_t ext_str_size;\n    clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, 0, NULL, &ext_str_size);\n    char *ext_buffer = (char *)alloca(ext_str_size + 1);\n    clGetDeviceInfo(device, CL_DEVICE_EXTENSIONS, ext_str_size, ext_buffer, NULL);\n    ext_buffer[ext_str_size] = '\\0'; // ensure it is null terminated\n    // Check if ext_buffer contains cl_khr_fp16\n    fp16_support = strstr(ext_buffer, \"cl_khr_fp16\") != NULL;\n    fprintf(stderr, \"ggml_opencl: device FP16 support: %s\\n\", fp16_support ? \"true\" : \"false\");\n\n    cl_context_properties properties[] = {\n        (intptr_t)CL_CONTEXT_PLATFORM, (intptr_t)platform, 0\n    };\n\n    CL_CHECK((context = clCreateContext(properties, 1, &device, NULL, NULL, &err), err));\n\n    CL_CHECK((queue = clCreateCommandQueue(context, device, CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE, &err),\n        (err != CL_INVALID_QUEUE_PROPERTIES && err != CL_INVALID_VALUE ? err :\n        (queue = clCreateCommandQueue(context, device, 0, &err), err)\n    )));\n\n    const std::string kernel_src = generate_kernels();\n\n    program = build_program_from_source(context, device, kernel_src.c_str());\n\n    // FP16 to FP32 kernel\n    CL_CHECK((convert_row_f16_cl = clCreateKernel(program, \"convert_row_f16\", &err), err));\n\n    // Dequantize kernels\n    CL_CHECK((dequantize_row_q4_0_cl = clCreateKernel(program, \"dequantize_row_q4_0\", &err), err));\n    CL_CHECK((dequantize_row_q4_1_cl = clCreateKernel(program, \"dequantize_row_q4_1\", &err), err));\n    CL_CHECK((dequantize_row_q5_0_cl = clCreateKernel(program, \"dequantize_row_q5_0\", &err), err));\n    CL_CHECK((dequantize_row_q5_1_cl = clCreateKernel(program, \"dequantize_row_q5_1\", &err), err));\n    CL_CHECK((dequantize_row_q8_0_cl = clCreateKernel(program, \"dequantize_row_q8_0\", &err), err));\n    CL_CHECK((dequantize_row_q8_0_cl = clCreateKernel(program, \"dequantize_row_q8_0\", &err), err));\n    CL_CHECK((dequantize_block_q2_k_cl = clCreateKernel(program, \"dequantize_block_q2_K\", &err), err));\n    CL_CHECK((dequantize_block_q3_k_cl = clCreateKernel(program, \"dequantize_block_q3_K\", &err), err));\n    CL_CHECK((dequantize_block_q4_k_cl = clCreateKernel(program, \"dequantize_block_q4_K\", &err), err));\n    CL_CHECK((dequantize_block_q5_k_cl = clCreateKernel(program, \"dequantize_block_q5_K\", &err), err));\n    CL_CHECK((dequantize_block_q6_k_cl = clCreateKernel(program, \"dequantize_block_q6_K\", &err), err));\n\n    // dequant mul mat kernel\n    CL_CHECK((dequantize_mul_mat_vec_q4_0_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q4_0\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q4_1_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q4_1\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q5_0_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q5_0\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q5_1_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q5_1\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q8_0_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q8_0\", &err), err));\n    CL_CHECK((convert_mul_mat_vec_f16_cl = clCreateKernel(program, \"convert_mul_mat_vec_f16\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q2_K_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q2_K\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q3_K_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q3_K\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q4_K_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q4_K\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q5_K_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q5_K\", &err), err));\n    CL_CHECK((dequantize_mul_mat_vec_q6_K_cl = clCreateKernel(program, \"dequantize_mul_mat_vec_q6_K\", &err), err));\n\n    // mul kernel\n    CL_CHECK((mul_f32_cl = clCreateKernel(program, \"mul_f32\", &err), err));\n}\n\nstatic cl_kernel* ggml_get_to_fp32_cl(ggml_type type) {\n    switch (type) {\n        case GGML_TYPE_Q4_0:\n            return &dequantize_row_q4_0_cl;\n        case GGML_TYPE_Q4_1:\n            return &dequantize_row_q4_1_cl;\n        case GGML_TYPE_Q5_0:\n            return &dequantize_row_q5_0_cl;\n        case GGML_TYPE_Q5_1:\n            return &dequantize_row_q5_1_cl;\n        case GGML_TYPE_Q8_0:\n            return &dequantize_row_q8_0_cl;\n        case GGML_TYPE_Q2_K:\n            return &dequantize_block_q2_k_cl;\n        case GGML_TYPE_Q3_K:\n            return &dequantize_block_q3_k_cl;\n        case GGML_TYPE_Q4_K:\n            return &dequantize_block_q4_k_cl;\n        case GGML_TYPE_Q5_K:\n            return &dequantize_block_q5_k_cl;\n        case GGML_TYPE_Q6_K:\n            return &dequantize_block_q6_k_cl;\n        case GGML_TYPE_F16:\n            return &convert_row_f16_cl;\n        default:\n            return nullptr;\n    }\n}\n\nstatic size_t ggml_cl_global_denom(ggml_type type) {\n    switch (type) {\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n            return 1;\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n            return 4;\n        case GGML_TYPE_Q4_K:\n            return 8;\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n            return 4;\n        case GGML_TYPE_F16:\n        default:\n            return 1;\n    }\n}\n\nstatic size_t ggml_cl_local_size(ggml_type type) {\n    switch (type) {\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n            return 0;\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n            return 64;\n        case GGML_TYPE_Q4_K:\n            return 32;\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n            return 64;\n        case GGML_TYPE_F16:\n        default:\n            return 0;\n    }\n}\n\nstatic cl_kernel* ggml_get_dequantize_mul_mat_vec_cl(ggml_type type) {\n    switch (type) {\n        case GGML_TYPE_Q4_0:\n            return &dequantize_mul_mat_vec_q4_0_cl;\n        case GGML_TYPE_Q4_1:\n            return &dequantize_mul_mat_vec_q4_1_cl;\n        case GGML_TYPE_Q5_0:\n            return &dequantize_mul_mat_vec_q5_0_cl;\n        case GGML_TYPE_Q5_1:\n            return &dequantize_mul_mat_vec_q5_1_cl;\n        case GGML_TYPE_Q8_0:\n            return &dequantize_mul_mat_vec_q8_0_cl;\n        case GGML_TYPE_F16:\n            return &convert_mul_mat_vec_f16_cl;\n        case GGML_TYPE_Q2_K:\n            return &dequantize_mul_mat_vec_q2_K_cl;\n        case GGML_TYPE_Q3_K:\n            return &dequantize_mul_mat_vec_q3_K_cl;\n        case GGML_TYPE_Q4_K:\n            return &dequantize_mul_mat_vec_q4_K_cl;\n        case GGML_TYPE_Q5_K:\n            return &dequantize_mul_mat_vec_q5_K_cl;\n        case GGML_TYPE_Q6_K:\n            return &dequantize_mul_mat_vec_q6_K_cl;\n        default:\n            return nullptr;\n    }\n}\n\n// buffer pool for cl\n#define MAX_CL_BUFFERS 256\n\nstruct scoped_spin_lock {\n    std::atomic_flag& lock;\n    scoped_spin_lock(std::atomic_flag& lock) : lock(lock) {\n        while (lock.test_and_set(std::memory_order_acquire)) {\n            ; // spin\n        }\n    }\n    ~scoped_spin_lock() {\n        lock.clear(std::memory_order_release);\n    }\n    scoped_spin_lock(const scoped_spin_lock&) = delete;\n    scoped_spin_lock& operator=(const scoped_spin_lock&) = delete;\n};\n\nstruct cl_buffer {\n    cl_mem mem;\n    size_t size = 0;\n};\n\nstatic cl_buffer g_cl_buffer_pool[MAX_CL_BUFFERS];\nstatic std::atomic_flag g_cl_pool_lock = ATOMIC_FLAG_INIT;\n\nstatic cl_mem ggml_cl_pool_malloc(size_t size, size_t * actual_size) {\n    scoped_spin_lock lock(g_cl_pool_lock);\n    cl_int err;\n\n    int best_i = -1;\n    size_t best_size = std::numeric_limits<size_t>::max(); //smallest unused buffer that fits our needs\n    int worst_i = -1;\n    size_t worst_size = 0; //largest unused buffer seen so far\n    for (int i = 0; i < MAX_CL_BUFFERS; ++i) {\n        cl_buffer &b = g_cl_buffer_pool[i];\n        if (b.size > 0 && b.size >= size && b.size < best_size)\n        {\n            best_i = i;\n            best_size = b.size;\n        }\n        if (b.size > 0 && b.size > worst_size)\n        {\n            worst_i = i;\n            worst_size = b.size;\n        }\n    }\n    if(best_i!=-1) //found the smallest buffer that fits our needs\n    {\n        cl_buffer& b = g_cl_buffer_pool[best_i];\n        cl_mem mem = b.mem;\n        *actual_size = b.size;\n        b.size = 0;\n        return mem;\n    }\n    if(worst_i!=-1) //no buffer that fits our needs, resize largest one to save memory\n    {\n         cl_buffer& b = g_cl_buffer_pool[worst_i];\n         cl_mem mem = b.mem;\n         b.size = 0;\n         clReleaseMemObject(mem);\n    }\n    cl_mem mem;\n    CL_CHECK((mem = clCreateBuffer(context, CL_MEM_READ_WRITE, size, NULL, &err), err));\n    *actual_size = size;\n    return mem;\n}\n\nstatic void ggml_cl_pool_free(cl_mem mem, size_t size) {\n    scoped_spin_lock lock(g_cl_pool_lock);\n\n    for (int i = 0; i < MAX_CL_BUFFERS; ++i) {\n        cl_buffer& b = g_cl_buffer_pool[i];\n        if (b.size == 0) {\n            b.mem = mem;\n            b.size = size;\n            return;\n        }\n    }\n    fprintf(stderr, \"WARNING: cl buffer pool full, increase MAX_CL_BUFFERS\\n\");\n    clReleaseMemObject(mem);\n}\n\nvoid ggml_cl_free_data(const struct ggml_tensor* tensor) {\n    if (tensor->backend != GGML_BACKEND_GPU) {\n        return;\n    }\n\n    cl_mem mem = (cl_mem)tensor->extra;\n    clReleaseMemObject(mem);\n}\n\nstatic cl_int ggml_cl_h2d_tensor_2d(cl_command_queue queue, cl_mem dst, size_t offset, const struct ggml_tensor * src, uint64_t i3, uint64_t i2, cl_event* ev) {\n    cl_int err;\n    const uint64_t ne0 = src->ne[0];\n    const uint64_t ne1 = src->ne[1];\n    const uint64_t nb0 = src->nb[0];\n    const uint64_t nb1 = src->nb[1];\n    const uint64_t nb2 = src->nb[2];\n    const uint64_t nb3 = src->nb[3];\n    const enum ggml_type type = src->type;\n    const size_t ts = ggml_type_size(type);\n    const size_t bs = ggml_blck_size(type);\n    const uint64_t row_size = ts*ne0/bs;\n\n    const char * x = (const char *) src->data + i2*nb2 + i3*nb3;\n    if (nb0 == ts && nb1 == row_size) {\n        return clEnqueueWriteBuffer(queue, dst, CL_FALSE, offset, ne1*row_size, x, 0, NULL, ev);\n    }\n    if (nb0 == ts) {\n        const size_t buffer_origin[3] = { offset, 0, 0 };\n        const size_t host_origin[3] = { 0, 0, 0 };\n        const size_t region[3] = { row_size, ne1, 1 };\n        return clEnqueueWriteBufferRect(queue, dst, CL_FALSE, buffer_origin, host_origin, region, row_size, 0, nb1, 0, x, 0, NULL, ev);\n    }\n    std::vector<cl_event> events;\n    if (ev && ne1>1) events.reserve(ne1-1);\n    for (uint64_t i1 = 0; i1 < ne1; i1++) {\n        // pretend the row is a matrix with cols=1\n        const size_t buffer_origin[3] = { offset + i1*row_size, 0, 0 };\n        const size_t host_origin[3] = { 0, 0, 0 };\n        const size_t region[3] = { ts, ne0/bs, 1 };\n        // if an event is requested, make the last write wait for all previous writes to complete\n        if (ev && i1) {\n            events.push_back(*ev);\n        }\n        cl_uint nevents = i1 == ne1-1 ? events.size() : 0U;\n        err = clEnqueueWriteBufferRect(queue, dst, CL_FALSE, buffer_origin, host_origin, region, ts, 0, nb0, 0, x + i1*nb1, nevents, nevents ? events.data() : nullptr, ev);\n        if (err != CL_SUCCESS) {\n            for (auto event : events) {\n                clReleaseEvent(event);\n            }\n            return err;\n        }\n    }\n    for (auto event : events) {\n        CL_CHECK(clReleaseEvent(event));\n    }\n    return CL_SUCCESS;\n}\n\nstatic void ggml_cl_mul_f32(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    GGML_ASSERT(src1->backend == GGML_BACKEND_GPU);\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n    const int64_t ne03 = src0->ne[3];\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n    const int64_t ne12 = src1->ne[2];\n    const int64_t ne13 = src1->ne[3];\n    const int nb2  = dst->nb[2];\n    const int nb3  = dst->nb[3];\n    size_t x_size;\n    size_t d_size;\n\n    cl_mem d_X = ggml_cl_pool_malloc(ne00 * ne01 * sizeof(float), &x_size); // src0\n    cl_mem d_Y = (cl_mem) src1->extra; // src1 is already on device, broadcasted.\n    cl_mem d_D = ggml_cl_pool_malloc(ne00 * ne01 * sizeof(float), &d_size); // dst\n\n\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        for (int64_t i02 = 0; i02 < ne02; i02++) {\n            cl_event ev;\n\n            // copy src0 to device\n            CL_CHECK(ggml_cl_h2d_tensor_2d(queue, d_X, 0, src0, i03, i02, &ev));\n\n            const int64_t i13 = i03%ne13;\n            const int64_t i12 = i02%ne12;\n            const int i1 = i13*ne12*ne11 + i12*ne11;\n\n            cl_int x_offset = 0;\n            cl_int y_offset = i1*ne10;\n            cl_int d_offset = 0;\n\n            size_t global = ne00 * ne01;\n            cl_int ky = ne10 * ne11;\n\n            CL_CHECK(clSetKernelArg(mul_f32_cl, 0, sizeof(cl_mem), &d_X));\n            CL_CHECK(clSetKernelArg(mul_f32_cl, 1, sizeof(cl_int), &x_offset));\n            CL_CHECK(clSetKernelArg(mul_f32_cl, 2, sizeof(cl_mem), &d_Y));\n            CL_CHECK(clSetKernelArg(mul_f32_cl, 3, sizeof(cl_int), &y_offset));\n            CL_CHECK(clSetKernelArg(mul_f32_cl, 4, sizeof(cl_mem), &d_D));\n            CL_CHECK(clSetKernelArg(mul_f32_cl, 5, sizeof(cl_int), &d_offset));\n            CL_CHECK(clSetKernelArg(mul_f32_cl, 6, sizeof(cl_int), &ky));\n            CL_CHECK(clEnqueueNDRangeKernel(queue, mul_f32_cl, 1, NULL, &global, NULL, 1, &ev, NULL));\n\n            CL_CHECK(clReleaseEvent(ev));\n            CL_CHECK(clFinish(queue));\n\n            // copy dst to host\n            float * d = (float *) ((char *) dst->data + i02*nb2 + i03*nb3);\n            CL_CHECK(clEnqueueReadBuffer(queue, d_D, true, 0, sizeof(float) * ne00*ne01, d, 0, NULL, NULL));\n        }\n    }\n    ggml_cl_pool_free(d_X, x_size);\n    ggml_cl_pool_free(d_D, d_size);\n}\n\nvoid ggml_cl_mul(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst) {\n    GGML_ASSERT(src0->type == GGML_TYPE_F32 && src1->type == GGML_TYPE_F32 && dst->type == GGML_TYPE_F32);\n    ggml_cl_mul_f32(src0, src1, dst);\n}\n\nstatic void ggml_cl_mul_mat_f32(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n    const int64_t ne03 = src0->ne[3];\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n    const int64_t ne12 = src1->ne[2];\n    const int64_t ne13 = src1->ne[3];\n\n    const int nb2  = dst->nb[2];\n    const int nb3  = dst->nb[3];\n\n    const int64_t r2 = ne12 / ne02;\n    const int64_t r3 = ne13 / ne03;\n\n    const float alpha = 1.0f;\n    const float beta = 0.0f;\n    const int x_ne = ne01 * ne00;\n    const int y_ne = ne11 * ne10;\n    const int d_ne = ne11 * ne01;\n\n    size_t x_size;\n    size_t y_size;\n    size_t d_size;\n    cl_mem d_X;\n    if (src0->backend == GGML_BACKEND_GPU) { // NOLINT\n        d_X = (cl_mem) src0->extra;\n    } else {\n        d_X = ggml_cl_pool_malloc(sizeof(float) * x_ne, &x_size);\n    }\n    cl_mem d_Y = ggml_cl_pool_malloc(sizeof(float) * y_ne, &y_size);\n    cl_mem d_D = ggml_cl_pool_malloc(sizeof(float) * d_ne, &d_size);\n\n    size_t x_offset = 0;\n\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        // TODO: copy src0 here when r3>1\n        for (int64_t i13 = i03 * r3, e13 = i13 + r3; i13 < e13; i13++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                if (src0->backend == GGML_BACKEND_GPU) {\n                    x_offset = (i03 * ne02 + i02) * x_ne;\n                } else {\n                    // copy src0 to device\n                    CL_CHECK(ggml_cl_h2d_tensor_2d(queue, d_X, 0, src0, i03, i02, NULL));\n                }\n\n                for (int64_t i12 = i02 * r2, e12 = i12 + r2; i12 < e12; i12++) {\n                    // copy src1 to device\n                    CL_CHECK(ggml_cl_h2d_tensor_2d(queue, d_Y, 0, src1, i13, i12, NULL));\n\n                    CL_CHECK(clFinish(queue));\n\n                    // compute\n                    cl_event ev_sgemm;\n                    clblast::StatusCode status = clblast::Gemm<cl_float>(clblast::Layout::kColMajor,\n                                                               clblast::Transpose::kYes, clblast::Transpose::kNo,\n                                                               ne01, ne11, ne10,\n                                                               alpha,\n                                                               d_X, x_offset, ne00,\n                                                               d_Y, 0, ne10,\n                                                               beta,\n                                                               d_D, 0, ne01,\n                                                               &queue, &ev_sgemm);\n\n                    if (status != clblast::StatusCode::kSuccess) {\n                        GGML_ASSERT(false);\n                    }\n\n                    // copy dst to host\n                    float * d = (float *) ((char *) dst->data + i12*nb2 + i13*nb3);\n                    CL_CHECK(clEnqueueReadBuffer(queue, d_D, true, 0, sizeof(float) * d_ne, d, 1, &ev_sgemm, NULL));\n                }\n            }\n        }\n    }\n\n    if (src0->backend != GGML_BACKEND_GPU) {\n        ggml_cl_pool_free(d_X, x_size);\n    }\n    ggml_cl_pool_free(d_Y, y_size);\n    ggml_cl_pool_free(d_D, d_size);\n}\n\nstatic void ggml_cl_mul_mat_f16(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst, void * wdata, size_t wsize) {\n    GGML_ASSERT(fp16_support);\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n    const int64_t ne03 = src0->ne[3];\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n    const int64_t ne12 = src1->ne[2];\n    const int64_t ne13 = src1->ne[3];\n\n    const int nb10 = src1->nb[0];\n    const int nb11 = src1->nb[1];\n    const int nb12 = src1->nb[2];\n    const int nb13 = src1->nb[3];\n\n    const int nb2  = dst->nb[2];\n    const int nb3  = dst->nb[3];\n\n    const int64_t r2 = ne12 / ne02;\n    const int64_t r3 = ne13 / ne03;\n\n    const ggml_fp16_t alpha = ggml_fp32_to_fp16(1.0f);\n    const ggml_fp16_t beta = ggml_fp32_to_fp16(0.0f);\n    const int x_ne = ne01 * ne00;\n    const int y_ne = ne11 * ne10;\n    const int d_ne = ne11 * ne01;\n\n    GGML_ASSERT(wsize >= sizeof(ggml_fp16_t) * y_ne);\n    GGML_ASSERT(wsize >= sizeof(ggml_fp16_t) * d_ne);\n    ggml_fp16_t * const tmp = (ggml_fp16_t *) wdata;\n\n    size_t x_size;\n    size_t y_size;\n    size_t d_size;\n    cl_mem d_X;\n    if (src0->backend == GGML_BACKEND_GPU) { // NOLINT\n        d_X = (cl_mem) src0->extra;\n    } else {\n        d_X = ggml_cl_pool_malloc(sizeof(ggml_fp16_t) * x_ne, &x_size);\n    }\n    cl_mem d_Y = ggml_cl_pool_malloc(sizeof(ggml_fp16_t) * y_ne, &y_size);\n    cl_mem d_D = ggml_cl_pool_malloc(sizeof(ggml_fp16_t) * d_ne, &d_size);\n\n    bool src1_cont_rows = nb10 == sizeof(float);\n    bool src1_cont_cols = (size_t)nb11 == ne11*sizeof(float);\n\n    size_t x_offset = 0;\n\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        // TODO: copy src0 here when r3>1\n        for (int64_t i13 = i03 * r3, e13 = i13 + r3; i13 < e13; i13++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                if (src0->backend == GGML_BACKEND_GPU) {\n                    x_offset = (i03 * ne02 + i02) * x_ne;\n                } else {\n                    // copy src0 to device\n                    CL_CHECK(ggml_cl_h2d_tensor_2d(queue, d_X, 0, src0, i03, i02, NULL));\n                }\n\n                for (int64_t i12 = i02 * r2, e12 = i12 + r2; i12 < e12; i12++) {\n                    // convert src1 to fp16\n                    // TODO: use multiple threads\n                    char * src1i = (char *) src1->data + i13*nb13 + i12*nb12;\n                    if (src1_cont_rows) {\n                        if (src1_cont_cols) {\n                            ggml_fp32_to_fp16_row((float *) src1i, tmp, ne10*ne11);\n                        }\n                        else {\n                            for (int64_t i11 = 0; i11 < ne11; i11++) {\n                                ggml_fp32_to_fp16_row((float *) (src1i + i11*nb11), tmp + i11*ne10, ne10);\n                            }\n                        }\n                    }\n                    else {\n                        for (int64_t i11 = 0; i11 < ne11; i11++) {\n                            for (int64_t i10 = 0; i10 < ne10; i10++) {\n                                // very slow due to no inlining\n                                tmp[i11*ne10 + i10] = ggml_fp32_to_fp16(*(float *) (src1i + i11*nb11 + i10*nb10));\n                            }\n                        }\n                    }\n\n                    // copy src1 to device\n                    CL_CHECK(clEnqueueWriteBuffer(queue, d_Y, false, 0, sizeof(ggml_fp16_t) * y_ne, tmp, 0, NULL, NULL));\n\n                    CL_CHECK(clFinish(queue));\n\n                    // compute\n                    cl_event ev_sgemm;\n                    clblast::StatusCode status = clblast::Gemm<cl_half>(clblast::Layout::kColMajor,\n                                                               clblast::Transpose::kYes, clblast::Transpose::kNo,\n                                                               ne01, ne11, ne10,\n                                                               alpha,\n                                                               d_X, x_offset, ne00,\n                                                               d_Y, 0, ne10,\n                                                               beta,\n                                                               d_D, 0, ne01,\n                                                               &queue, &ev_sgemm);\n\n                    if (status != clblast::StatusCode::kSuccess) {\n                        GGML_ASSERT(false);\n                    }\n\n                    // copy dst to host, then convert to float\n                    CL_CHECK(clEnqueueReadBuffer(queue, d_D, true, 0, sizeof(ggml_fp16_t) * d_ne, tmp, 1, &ev_sgemm, NULL));\n\n                    float * d = (float *) ((char *) dst->data + i12*nb2 + i13*nb3);\n\n                    ggml_fp16_to_fp32_row(tmp, d, d_ne);\n                }\n            }\n        }\n    }\n\n    if (src0->backend != GGML_BACKEND_GPU) {\n        ggml_cl_pool_free(d_X, x_size);\n    }\n    ggml_cl_pool_free(d_Y, y_size);\n    ggml_cl_pool_free(d_D, d_size);\n}\n\nstatic void ggml_cl_mul_mat_q_f32(const ggml_tensor * src0, const ggml_tensor * src1, ggml_tensor * dst) {\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n    const int64_t ne02 = src0->ne[2];\n    const int64_t ne03 = src0->ne[3];\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n    const int64_t ne12 = src1->ne[2];\n    const int64_t ne13 = src1->ne[3];\n\n    const int nb2  = dst->nb[2];\n    const int nb3  = dst->nb[3];\n    const ggml_type type = src0->type;\n    const bool mul_mat_vec = ne11 == 1 && ne00%2 == 0;\n\n    const int64_t r2 = ne12 / ne02;\n    const int64_t r3 = ne13 / ne03;\n\n    const float alpha = 1.0f;\n    const float beta = 0.0f;\n    const int x_ne = ne01 * ne00;\n    const int y_ne = ne11 * ne10;\n    const int d_ne = ne11 * ne01;\n    const int x_bps = x_ne / ggml_blck_size(type); // blocks per 2D slice\n    const size_t q_sz = ggml_type_size(type) * x_bps;\n\n    size_t x_size;\n    size_t y_size;\n    size_t d_size;\n    size_t q_size;\n    cl_mem d_X;\n    if (!mul_mat_vec) {\n        d_X = ggml_cl_pool_malloc(sizeof(float) * x_ne, &x_size);\n    }\n    cl_mem d_Y = ggml_cl_pool_malloc(sizeof(float) * y_ne, &y_size);\n    cl_mem d_D = ggml_cl_pool_malloc(sizeof(float) * d_ne, &d_size);\n    cl_mem d_Q;\n    if (src0->backend == GGML_BACKEND_CPU) {\n        d_Q = ggml_cl_pool_malloc(q_sz, &q_size);\n    }\n\n    cl_kernel* to_fp32_cl = ggml_get_to_fp32_cl(type);\n    cl_kernel* dmmv = ggml_get_dequantize_mul_mat_vec_cl(type);\n    GGML_ASSERT(to_fp32_cl != nullptr);\n\n    const size_t global_denom = ggml_cl_global_denom(type);\n    const size_t local = mul_mat_vec ? CL_DMMV_LOCAL_SIZE : ggml_cl_local_size(type);\n\n    size_t ev_idx = 0;\n    std::vector<cl_event> events;\n\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        // TODO: copy and dequantize src0 here when r3>1\n        for (int64_t i13 = i03 * r3, e13 = i13 + r3; i13 < e13; i13++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                // copy src0 to device if necessary\n                if (src0->backend == GGML_BACKEND_CPU) {\n                    events.emplace_back();\n                    CL_CHECK(ggml_cl_h2d_tensor_2d(queue, d_Q, 0, src0, i03, i02, events.data() + ev_idx++));\n                } else if (src0->backend == GGML_BACKEND_GPU) {\n                    d_Q = (cl_mem) src0->extra;\n                } else {\n                    GGML_ASSERT(false);\n                }\n\n                if (!mul_mat_vec) {\n                    // convert src0 to fp32 on device\n                    const size_t global = x_ne / global_denom;\n                    const size_t offset = src0->backend == GGML_BACKEND_GPU ? (i03 * ne02 + i02) * x_bps : 0;\n                    CL_CHECK(clSetKernelArg(*to_fp32_cl, 0, sizeof(cl_mem), &d_Q));\n                    CL_CHECK(clSetKernelArg(*to_fp32_cl, 1, sizeof(cl_mem), &d_X));\n                    CL_CHECK(clEnqueueNDRangeKernel(queue, *to_fp32_cl, 1, &offset, &global, local > 0 ? &local : NULL, events.size(), !events.empty() ? events.data() : NULL, NULL));\n                }\n\n                for (int64_t i12 = i02 * r2, e12 = i12 + r2; i12 < e12; i12++) {\n                    if (mul_mat_vec) { // specialized dequantize_mul_mat_vec kernel\n                        // copy src1 to device\n                        events.emplace_back();\n                        CL_CHECK(ggml_cl_h2d_tensor_2d(queue, d_Y, 0, src1, i13, i12, events.data() + ev_idx++));\n\n                        // compute\n                        const size_t global = ne01 * local;\n                        const size_t offset = src0->backend == GGML_BACKEND_GPU ? (i03 * ne02 + i02) * x_bps : 0;\n                        const cl_int ncols = ne00;\n                        events.emplace_back();\n                        CL_CHECK(clSetKernelArg(*dmmv, 0, sizeof(cl_mem), &d_Q));\n                        CL_CHECK(clSetKernelArg(*dmmv, 1, sizeof(float) * local, NULL));\n                        CL_CHECK(clSetKernelArg(*dmmv, 2, sizeof(cl_mem), &d_Y));\n                        CL_CHECK(clSetKernelArg(*dmmv, 3, sizeof(cl_mem), &d_D));\n                        CL_CHECK(clSetKernelArg(*dmmv, 4, sizeof(cl_int), &ncols));\n                        CL_CHECK(clEnqueueNDRangeKernel(queue, *dmmv, 1, &offset, &global, &local, events.size() - 1, events.data(), events.data() + ev_idx++));\n                    } else { // CLBlast matrix matrix multiplication\n                        // copy src1 to device\n                        CL_CHECK(ggml_cl_h2d_tensor_2d(queue, d_Y, 0, src1, i13, i12, NULL));\n\n                        // wait for conversion\n                        CL_CHECK(clFinish(queue));\n\n                        // compute\n                        events.emplace_back();\n                        clblast::StatusCode status = clblast::Gemm<cl_float>(clblast::Layout::kColMajor,\n                                                                   clblast::Transpose::kYes, clblast::Transpose::kNo,\n                                                                   ne01, ne11, ne10,\n                                                                   alpha,\n                                                                   d_X, 0, ne00,\n                                                                   d_Y, 0, ne10,\n                                                                   beta,\n                                                                   d_D, 0, ne01,\n                                                                   &queue, events.data() + ev_idx++);\n\n                        if (status != clblast::StatusCode::kSuccess) {\n                            GGML_ASSERT(false);\n                        }\n                    }\n\n                    // copy dst to host\n                    float * d = (float *) ((char *) dst->data + i12*nb2 + i13*nb3);\n                    CL_CHECK(clEnqueueReadBuffer(queue, d_D, true, 0, sizeof(float) * d_ne, d, 1, &events[events.size() - 1], NULL));\n                    for (auto *event : events) {\n                        clReleaseEvent(event);\n                    }\n\n                    ev_idx = 0;\n                    events.clear();\n                }\n            }\n        }\n    }\n\n    if (!mul_mat_vec) {\n        ggml_cl_pool_free(d_X, x_size);\n    }\n    ggml_cl_pool_free(d_Y, y_size);\n    ggml_cl_pool_free(d_D, d_size);\n    if (src0->backend == GGML_BACKEND_CPU) {\n        ggml_cl_pool_free(d_Q, q_size);\n    }\n}\n\n\nbool ggml_cl_can_mul_mat(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst) {\n    const int64_t ne10 = src1->ne[0];\n\n    const int64_t ne0 = dst->ne[0];\n    const int64_t ne1 = dst->ne[1];\n\n    // TODO: find the optimal values for these\n    if ((src0->type == GGML_TYPE_F32 || src0->type == GGML_TYPE_F16 || ggml_is_quantized(src0->type)) &&\n        src1->type == GGML_TYPE_F32 &&\n        dst->type == GGML_TYPE_F32 &&\n        ((ne0 >= 32 && ne1 >= 32 && ne10 >= 32) || src0->backend == GGML_BACKEND_GPU)) {\n        return true;\n    }\n\n    return false;\n}\n\nstatic bool ggml_cl_mul_mat_use_f16(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * /* dst */) {\n    // If device doesn't support FP16\n    if (!fp16_support) {\n        return false;\n    }\n\n    size_t src0_sz = ggml_nbytes(src0);\n    size_t src1_sz = ggml_nbytes(src1);\n\n    // mul_mat_q: src0 is converted to fp32 on device\n    size_t mul_mat_q_transfer = src0_sz + src1_sz;\n\n    // mul_mat_f16: src1 is converted to fp16 on cpu\n    size_t mul_mat_f16_transfer = src0_sz + sizeof(ggml_fp16_t) * ggml_nelements(src1);\n\n    // choose the smaller one to transfer to the device\n    // TODO: this is not always the best choice due to the overhead of converting to fp16\n    return mul_mat_f16_transfer < mul_mat_q_transfer;\n}\n\nvoid ggml_cl_mul_mat(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst, void * wdata, size_t wsize) {\n    GGML_ASSERT(ggml_cl_can_mul_mat(src0, src1, dst));\n\n    if (src0->type == GGML_TYPE_F32) {\n        ggml_cl_mul_mat_f32(src0, src1, dst);\n    }\n    else if (src0->type == GGML_TYPE_F16) {\n        if (ggml_cl_mul_mat_use_f16(src0, src1, dst)) {\n            ggml_cl_mul_mat_f16(src0, src1, dst, wdata, wsize);\n        }\n        else {\n            ggml_cl_mul_mat_q_f32(src0, src1, dst);\n        }\n    }\n    else if (ggml_is_quantized(src0->type)) {\n        ggml_cl_mul_mat_q_f32(src0, src1, dst);\n    }\n    else {\n        GGML_ASSERT(false);\n    }\n}\n\nsize_t ggml_cl_mul_mat_get_wsize(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst) {\n    if (src0->type == GGML_TYPE_F16 && ggml_cl_mul_mat_use_f16(src0, src1, dst)) {\n        return sizeof(ggml_fp16_t) * std::max(src1->ne[0] * src1->ne[1], dst->ne[0] * dst->ne[1]);\n    }\n    return 0;\n}\n\nvoid ggml_cl_transform_tensor(void * data, ggml_tensor * tensor) {\n    const int64_t ne0 = tensor->ne[0];\n    const int64_t ne1 = tensor->ne[1];\n    const int64_t ne2 = tensor->ne[2];\n    const int64_t ne3 = tensor->ne[3];\n\n    const ggml_type type = tensor->type;\n    const size_t s_sz = ggml_type_size(type) * (size_t) (ne0 * ne1 / ggml_blck_size(type));\n    const size_t q_sz = s_sz * (size_t) (ne2 * ne3);\n\n    size_t q_size;\n    cl_mem dst = ggml_cl_pool_malloc(q_sz, &q_size);\n\n    tensor->data = data;\n    // copy tensor to device\n    size_t offset = 0;\n    for (int64_t i3 = 0; i3 < ne3; i3++) {\n        for (int64_t i2 = 0; i2 < ne2; i2++) {\n            CL_CHECK(ggml_cl_h2d_tensor_2d(queue, dst, offset, tensor, i3, i2, NULL));\n            offset += s_sz;\n        }\n    }\n\n    CL_CHECK(clFinish(queue));\n\n    tensor->extra = dst;\n    GGML_ASSERT(tensor->backend == GGML_BACKEND_GPU);\n}\n"
        },
        {
          "name": "ggml-opencl.h",
          "type": "blob",
          "size": 0.8251953125,
          "content": "#pragma once\n\n#include \"ggml.h\"\n\n#ifdef  __cplusplus\nextern \"C\" {\n#endif\n\nvoid ggml_cl_init(void);\n\nvoid   ggml_cl_mul(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst);\nbool   ggml_cl_can_mul_mat(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst);\nsize_t ggml_cl_mul_mat_get_wsize(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst);\nvoid   ggml_cl_mul_mat(const struct ggml_tensor * src0, const struct ggml_tensor * src1, struct ggml_tensor * dst, void * wdata, size_t wsize);\n\nvoid * ggml_cl_host_malloc(size_t size);\nvoid   ggml_cl_host_free(void * ptr);\n\nvoid ggml_cl_free_data(const struct ggml_tensor* tensor);\n\nvoid ggml_cl_transform_tensor(void * data, struct ggml_tensor * tensor);\n\n#ifdef  __cplusplus\n}\n#endif\n"
        },
        {
          "name": "ggml-quants.c",
          "type": "blob",
          "size": 287.57421875,
          "content": "#include \"ggml-quants.h\"\n#include \"ggml-impl.h\"\n\n#include <math.h>\n#include <string.h>\n#include <assert.h>\n#include <float.h>\n\n#ifdef __ARM_NEON\n\n// if YCM cannot find <arm_neon.h>, make a symbolic link to it, for example:\n//\n//   $ ln -sfn /Library/Developer/CommandLineTools/usr/lib/clang/13.1.6/include/arm_neon.h ./src/\n//\n#include <arm_neon.h>\n\n#else\n\n#ifdef __wasm_simd128__\n#include <wasm_simd128.h>\n#else\n#ifdef __POWER9_VECTOR__\n#include <altivec.h>\n#undef bool\n#define bool _Bool\n#else\n#if defined(_MSC_VER) || defined(__MINGW32__)\n#include <intrin.h>\n#else\n#if defined(__AVX__) || defined(__AVX2__) || defined(__AVX512F__) || defined(__SSSE3__) || defined(__SSE3__)\n#if !defined(__riscv)\n#include <immintrin.h>\n#endif\n#endif\n#endif\n#endif\n#endif\n#endif\n\n#ifdef __riscv_v_intrinsic\n#include <riscv_vector.h>\n#endif\n\n#undef MIN\n#undef MAX\n\n#define MIN(a, b) ((a) < (b) ? (a) : (b))\n#define MAX(a, b) ((a) > (b) ? (a) : (b))\n\n#define MM256_SET_M128I(a, b) _mm256_insertf128_si256(_mm256_castsi128_si256(b), (a), 1)\n\n#if defined(__AVX__) || defined(__AVX2__) || defined(__AVX512F__) || defined(__SSSE3__)\n// multiply int8_t, add results pairwise twice\nstatic inline __m128i mul_sum_i8_pairs(const __m128i x, const __m128i y) {\n    // Get absolute values of x vectors\n    const __m128i ax = _mm_sign_epi8(x, x);\n    // Sign the values of the y vectors\n    const __m128i sy = _mm_sign_epi8(y, x);\n    // Perform multiplication and create 16-bit values\n    const __m128i dot = _mm_maddubs_epi16(ax, sy);\n    const __m128i ones = _mm_set1_epi16(1);\n    return _mm_madd_epi16(ones, dot);\n}\n\n#if __AVX__ || __AVX2__ || __AVX512F__\n// horizontally add 8 floats\nstatic inline float hsum_float_8(const __m256 x) {\n    __m128 res = _mm256_extractf128_ps(x, 1);\n    res = _mm_add_ps(res, _mm256_castps256_ps128(x));\n    res = _mm_add_ps(res, _mm_movehl_ps(res, res));\n    res = _mm_add_ss(res, _mm_movehdup_ps(res));\n    return _mm_cvtss_f32(res);\n}\n\n// horizontally add 8 int32_t\nstatic inline int hsum_i32_8(const __m256i a) {\n    const __m128i sum128 = _mm_add_epi32(_mm256_castsi256_si128(a), _mm256_extractf128_si256(a, 1));\n    const __m128i hi64 = _mm_unpackhi_epi64(sum128, sum128);\n    const __m128i sum64 = _mm_add_epi32(hi64, sum128);\n    const __m128i hi32  = _mm_shuffle_epi32(sum64, _MM_SHUFFLE(2, 3, 0, 1));\n    return _mm_cvtsi128_si32(_mm_add_epi32(sum64, hi32));\n}\n\n// horizontally add 4 int32_t\nstatic inline int hsum_i32_4(const __m128i a) {\n    const __m128i hi64 = _mm_unpackhi_epi64(a, a);\n    const __m128i sum64 = _mm_add_epi32(hi64, a);\n    const __m128i hi32  = _mm_shuffle_epi32(sum64, _MM_SHUFFLE(2, 3, 0, 1));\n    return _mm_cvtsi128_si32(_mm_add_epi32(sum64, hi32));\n}\n\n#if defined(__AVX2__) || defined(__AVX512F__)\n// spread 32 bits to 32 bytes { 0x00, 0xFF }\nstatic inline __m256i bytes_from_bits_32(const uint8_t * x) {\n    uint32_t x32;\n    memcpy(&x32, x, sizeof(uint32_t));\n    const __m256i shuf_mask = _mm256_set_epi64x(\n            0x0303030303030303, 0x0202020202020202,\n            0x0101010101010101, 0x0000000000000000);\n    __m256i bytes = _mm256_shuffle_epi8(_mm256_set1_epi32(x32), shuf_mask);\n    const __m256i bit_mask = _mm256_set1_epi64x(0x7fbfdfeff7fbfdfe);\n    bytes = _mm256_or_si256(bytes, bit_mask);\n    return _mm256_cmpeq_epi8(bytes, _mm256_set1_epi64x(-1));\n}\n\n// Unpack 32 4-bit fields into 32 bytes\n// The output vector contains 32 bytes, each one in [ 0 .. 15 ] interval\nstatic inline __m256i bytes_from_nibbles_32(const uint8_t * rsi)\n{\n    const __m128i tmp = _mm_loadu_si128((const __m128i *)rsi);\n    const __m256i bytes = MM256_SET_M128I(_mm_srli_epi16(tmp, 4), tmp);\n    const __m256i lowMask = _mm256_set1_epi8( 0xF );\n    return _mm256_and_si256(lowMask, bytes);\n}\n\n// add int16_t pairwise and return as float vector\nstatic inline __m256 sum_i16_pairs_float(const __m256i x) {\n    const __m256i ones = _mm256_set1_epi16(1);\n    const __m256i summed_pairs = _mm256_madd_epi16(ones, x);\n    return _mm256_cvtepi32_ps(summed_pairs);\n}\n\nstatic inline __m256 mul_sum_us8_pairs_float(const __m256i ax, const __m256i sy) {\n#if __AVXVNNI__\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i summed_pairs = _mm256_dpbusd_epi32(zero, ax, sy);\n    return _mm256_cvtepi32_ps(summed_pairs);\n#else\n    // Perform multiplication and create 16-bit values\n    const __m256i dot = _mm256_maddubs_epi16(ax, sy);\n    return sum_i16_pairs_float(dot);\n#endif\n}\n\n// multiply int8_t, add results pairwise twice and return as float vector\nstatic inline __m256 mul_sum_i8_pairs_float(const __m256i x, const __m256i y) {\n#if __AVXVNNIINT8__\n    const __m256i zero = _mm256_setzero_si256();\n    const __m256i summed_pairs = _mm256_dpbssd_epi32(zero, x, y);\n    return _mm256_cvtepi32_ps(summed_pairs);\n#else\n    // Get absolute values of x vectors\n    const __m256i ax = _mm256_sign_epi8(x, x);\n    // Sign the values of the y vectors\n    const __m256i sy = _mm256_sign_epi8(y, x);\n    return mul_sum_us8_pairs_float(ax, sy);\n#endif\n}\n\nstatic inline __m128i packNibbles( __m256i bytes )\n{\n    // Move bits within 16-bit lanes from 0000_abcd_0000_efgh into 0000_0000_abcd_efgh\n#if __AVX512F__\n    const __m256i bytes_srli_4 = _mm256_srli_epi16(bytes, 4);   // 0000_0000_abcd_0000\n    bytes = _mm256_or_si256(bytes, bytes_srli_4);               // 0000_abcd_abcd_efgh\n    return _mm256_cvtepi16_epi8(bytes);                         // abcd_efgh\n#else\n    const __m256i lowByte = _mm256_set1_epi16( 0xFF );\n    __m256i high = _mm256_andnot_si256( lowByte, bytes );\n    __m256i low = _mm256_and_si256( lowByte, bytes );\n    high = _mm256_srli_epi16( high, 4 );\n    bytes = _mm256_or_si256( low, high );\n\n    // Compress uint16_t lanes into bytes\n    __m128i r0 = _mm256_castsi256_si128( bytes );\n    __m128i r1 = _mm256_extracti128_si256( bytes, 1 );\n    return _mm_packus_epi16( r0, r1 );\n#endif\n}\n#elif defined(__AVX__)\n// spread 32 bits to 32 bytes { 0x00, 0xFF }\nstatic inline __m256i bytes_from_bits_32(const uint8_t * x) {\n    uint32_t x32;\n    memcpy(&x32, x, sizeof(uint32_t));\n    const __m128i shuf_maskl = _mm_set_epi64x(0x0101010101010101, 0x0000000000000000);\n    const __m128i shuf_maskh = _mm_set_epi64x(0x0303030303030303, 0x0202020202020202);\n    __m128i bytesl = _mm_shuffle_epi8(_mm_set1_epi32(x32), shuf_maskl);\n    __m128i bytesh = _mm_shuffle_epi8(_mm_set1_epi32(x32), shuf_maskh);\n    const __m128i bit_mask = _mm_set1_epi64x(0x7fbfdfeff7fbfdfe);\n    bytesl = _mm_or_si128(bytesl, bit_mask);\n    bytesh = _mm_or_si128(bytesh, bit_mask);\n    bytesl = _mm_cmpeq_epi8(bytesl, _mm_set1_epi64x(-1));\n    bytesh = _mm_cmpeq_epi8(bytesh, _mm_set1_epi64x(-1));\n    return MM256_SET_M128I(bytesh, bytesl);\n}\n\n// Unpack 32 4-bit fields into 32 bytes\n// The output vector contains 32 bytes, each one in [ 0 .. 15 ] interval\nstatic inline __m256i bytes_from_nibbles_32(const uint8_t * rsi)\n{\n    // Load 16 bytes from memory\n    __m128i tmpl = _mm_loadu_si128((const __m128i *)rsi);\n    __m128i tmph = _mm_srli_epi16(tmpl, 4);\n    const __m128i lowMask = _mm_set1_epi8(0xF);\n    tmpl = _mm_and_si128(lowMask, tmpl);\n    tmph = _mm_and_si128(lowMask, tmph);\n    return MM256_SET_M128I(tmph, tmpl);\n}\n\n// add int16_t pairwise and return as float vector\nstatic inline __m256 sum_i16_pairs_float(const __m128i xh, const __m128i xl) {\n    const __m128i ones = _mm_set1_epi16(1);\n    const __m128i summed_pairsl = _mm_madd_epi16(ones, xl);\n    const __m128i summed_pairsh = _mm_madd_epi16(ones, xh);\n    const __m256i summed_pairs = MM256_SET_M128I(summed_pairsh, summed_pairsl);\n    return _mm256_cvtepi32_ps(summed_pairs);\n}\n\nstatic inline __m256 mul_sum_us8_pairs_float(const __m256i ax, const __m256i sy) {\n    const __m128i axl = _mm256_castsi256_si128(ax);\n    const __m128i axh = _mm256_extractf128_si256(ax, 1);\n    const __m128i syl = _mm256_castsi256_si128(sy);\n    const __m128i syh = _mm256_extractf128_si256(sy, 1);\n    // Perform multiplication and create 16-bit values\n    const __m128i dotl = _mm_maddubs_epi16(axl, syl);\n    const __m128i doth = _mm_maddubs_epi16(axh, syh);\n    return sum_i16_pairs_float(doth, dotl);\n}\n\n// multiply int8_t, add results pairwise twice and return as float vector\nstatic inline __m256 mul_sum_i8_pairs_float(const __m256i x, const __m256i y) {\n    const __m128i xl = _mm256_castsi256_si128(x);\n    const __m128i xh = _mm256_extractf128_si256(x, 1);\n    const __m128i yl = _mm256_castsi256_si128(y);\n    const __m128i yh = _mm256_extractf128_si256(y, 1);\n    // Get absolute values of x vectors\n    const __m128i axl = _mm_sign_epi8(xl, xl);\n    const __m128i axh = _mm_sign_epi8(xh, xh);\n    // Sign the values of the y vectors\n    const __m128i syl = _mm_sign_epi8(yl, xl);\n    const __m128i syh = _mm_sign_epi8(yh, xh);\n    // Perform multiplication and create 16-bit values\n    const __m128i dotl = _mm_maddubs_epi16(axl, syl);\n    const __m128i doth = _mm_maddubs_epi16(axh, syh);\n    return sum_i16_pairs_float(doth, dotl);\n}\n\nstatic inline __m128i packNibbles( __m128i bytes1, __m128i bytes2 )\n{\n    // Move bits within 16-bit lanes from 0000_abcd_0000_efgh into 0000_0000_abcd_efgh\n    const __m128i lowByte = _mm_set1_epi16( 0xFF );\n    __m128i high = _mm_andnot_si128( lowByte, bytes1 );\n    __m128i low = _mm_and_si128( lowByte, bytes1 );\n    high = _mm_srli_epi16( high, 4 );\n    bytes1 = _mm_or_si128( low, high );\n    high = _mm_andnot_si128( lowByte, bytes2 );\n    low = _mm_and_si128( lowByte, bytes2 );\n    high = _mm_srli_epi16( high, 4 );\n    bytes2 = _mm_or_si128( low, high );\n\n    return _mm_packus_epi16( bytes1, bytes2);\n}\n#endif\n#elif defined(__SSSE3__)\n// horizontally add 4x4 floats\nstatic inline float hsum_float_4x4(const __m128 a, const __m128 b, const __m128 c, const __m128 d) {\n    __m128 res_0 =_mm_hadd_ps(a, b);\n    __m128 res_1 =_mm_hadd_ps(c, d);\n    __m128 res =_mm_hadd_ps(res_0, res_1);\n    res =_mm_hadd_ps(res, res);\n    res =_mm_hadd_ps(res, res);\n\n    return _mm_cvtss_f32(res);\n}\n#endif // __AVX__ || __AVX2__ || __AVX512F__\n#endif // defined(__AVX__) || defined(__AVX2__) || defined(__AVX512F__) || defined(__SSSE3__)\n\n#if defined(__ARM_NEON)\n#if !defined(__aarch64__)\n\n// 64-bit compatibility\n\n// vaddvq_s16\n// vpaddq_s16\n// vaddvq_s32\n// vaddvq_f32\n// vmaxvq_f32\n// vcvtnq_s32_f32\n\ninline static int32_t vaddvq_s16(int16x8_t v) {\n    return\n        (int32_t)vgetq_lane_s16(v, 0) + (int32_t)vgetq_lane_s16(v, 1) +\n        (int32_t)vgetq_lane_s16(v, 2) + (int32_t)vgetq_lane_s16(v, 3) +\n        (int32_t)vgetq_lane_s16(v, 4) + (int32_t)vgetq_lane_s16(v, 5) +\n        (int32_t)vgetq_lane_s16(v, 6) + (int32_t)vgetq_lane_s16(v, 7);\n}\n\ninline static int16x8_t vpaddq_s16(int16x8_t a, int16x8_t b) {\n    int16x4_t a0 = vpadd_s16(vget_low_s16(a), vget_high_s16(a));\n    int16x4_t b0 = vpadd_s16(vget_low_s16(b), vget_high_s16(b));\n    return vcombine_s16(a0, b0);\n}\n\ninline static int32_t vaddvq_s32(int32x4_t v) {\n    return vgetq_lane_s32(v, 0) + vgetq_lane_s32(v, 1) + vgetq_lane_s32(v, 2) + vgetq_lane_s32(v, 3);\n}\n\ninline static float vaddvq_f32(float32x4_t v) {\n    return vgetq_lane_f32(v, 0) + vgetq_lane_f32(v, 1) + vgetq_lane_f32(v, 2) + vgetq_lane_f32(v, 3);\n}\n\ninline static float vmaxvq_f32(float32x4_t v) {\n    return\n        MAX(MAX(vgetq_lane_f32(v, 0), vgetq_lane_f32(v, 1)),\n            MAX(vgetq_lane_f32(v, 2), vgetq_lane_f32(v, 3)));\n}\n\ninline static int32x4_t vcvtnq_s32_f32(float32x4_t v) {\n    int32x4_t res;\n\n    res[0] = roundf(vgetq_lane_f32(v, 0));\n    res[1] = roundf(vgetq_lane_f32(v, 1));\n    res[2] = roundf(vgetq_lane_f32(v, 2));\n    res[3] = roundf(vgetq_lane_f32(v, 3));\n\n    return res;\n}\n\n// vld1q_s16_x2\n// vld1q_u8_x2\n// vld1q_u8_x4\n// vld1q_s8_x2\n// vld1q_s8_x4\n// TODO: double-check these work correctly\n\ntypedef struct ggml_int16x8x2_t {\n    int16x8_t val[2];\n} ggml_int16x8x2_t;\n\ninline static ggml_int16x8x2_t ggml_vld1q_s16_x2(const int16_t * ptr) {\n    ggml_int16x8x2_t res;\n\n    res.val[0] = vld1q_s16(ptr + 0);\n    res.val[1] = vld1q_s16(ptr + 8);\n\n    return res;\n}\n\ntypedef struct ggml_uint8x16x2_t {\n    uint8x16_t val[2];\n} ggml_uint8x16x2_t;\n\ninline static ggml_uint8x16x2_t ggml_vld1q_u8_x2(const uint8_t * ptr) {\n    ggml_uint8x16x2_t res;\n\n    res.val[0] = vld1q_u8(ptr + 0);\n    res.val[1] = vld1q_u8(ptr + 16);\n\n    return res;\n}\n\ntypedef struct ggml_uint8x16x4_t {\n    uint8x16_t val[4];\n} ggml_uint8x16x4_t;\n\ninline static ggml_uint8x16x4_t ggml_vld1q_u8_x4(const uint8_t * ptr) {\n    ggml_uint8x16x4_t res;\n\n    res.val[0] = vld1q_u8(ptr + 0);\n    res.val[1] = vld1q_u8(ptr + 16);\n    res.val[2] = vld1q_u8(ptr + 32);\n    res.val[3] = vld1q_u8(ptr + 48);\n\n    return res;\n}\n\ntypedef struct ggml_int8x16x2_t {\n    int8x16_t val[2];\n} ggml_int8x16x2_t;\n\ninline static ggml_int8x16x2_t ggml_vld1q_s8_x2(const int8_t * ptr) {\n    ggml_int8x16x2_t res;\n\n    res.val[0] = vld1q_s8(ptr + 0);\n    res.val[1] = vld1q_s8(ptr + 16);\n\n    return res;\n}\n\ntypedef struct ggml_int8x16x4_t {\n    int8x16_t val[4];\n} ggml_int8x16x4_t;\n\ninline static ggml_int8x16x4_t ggml_vld1q_s8_x4(const int8_t * ptr) {\n    ggml_int8x16x4_t res;\n\n    res.val[0] = vld1q_s8(ptr + 0);\n    res.val[1] = vld1q_s8(ptr + 16);\n    res.val[2] = vld1q_s8(ptr + 32);\n    res.val[3] = vld1q_s8(ptr + 48);\n\n    return res;\n}\n\n#else\n\n#define ggml_int16x8x2_t  int16x8x2_t\n#define ggml_uint8x16x2_t uint8x16x2_t\n#define ggml_uint8x16x4_t uint8x16x4_t\n#define ggml_int8x16x2_t  int8x16x2_t\n#define ggml_int8x16x4_t  int8x16x4_t\n\n#define ggml_vld1q_s16_x2 vld1q_s16_x2\n#define ggml_vld1q_u8_x2  vld1q_u8_x2\n#define ggml_vld1q_u8_x4  vld1q_u8_x4\n#define ggml_vld1q_s8_x2  vld1q_s8_x2\n#define ggml_vld1q_s8_x4  vld1q_s8_x4\n\n#endif\n#endif\n\n#if defined(__ARM_NEON) || defined(__wasm_simd128__)\n#define B1(c,s,n)  0x ## n ## c ,  0x ## n ## s\n#define B2(c,s,n) B1(c,s,n ## c), B1(c,s,n ## s)\n#define B3(c,s,n) B2(c,s,n ## c), B2(c,s,n ## s)\n#define B4(c,s,n) B3(c,s,n ## c), B3(c,s,n ## s)\n#define B5(c,s,n) B4(c,s,n ## c), B4(c,s,n ## s)\n#define B6(c,s,n) B5(c,s,n ## c), B5(c,s,n ## s)\n#define B7(c,s,n) B6(c,s,n ## c), B6(c,s,n ## s)\n#define B8(c,s  ) B7(c,s,     c), B7(c,s,     s)\n\n// precomputed tables for expanding 8bits to 8 bytes:\nstatic const uint64_t table_b2b_0[1 << 8] = { B8(00, 10) }; // ( b) << 4\nstatic const uint64_t table_b2b_1[1 << 8] = { B8(10, 00) }; // (!b) << 4\n#endif\n\n// reference implementation for deterministic creation of model files\nvoid quantize_row_q4_0_reference(const float * restrict x, block_q4_0 * restrict y, int k) {\n    static const int qk = QK4_0;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        float amax = 0.0f; // absolute max\n        float max  = 0.0f;\n\n        for (int j = 0; j < qk; j++) {\n            const float v = x[i*qk + j];\n            if (amax < fabsf(v)) {\n                amax = fabsf(v);\n                max  = v;\n            }\n        }\n\n        const float d  = max / -8;\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = GGML_FP32_TO_FP16(d);\n\n        for (int j = 0; j < qk/2; ++j) {\n            const float x0 = x[i*qk + 0    + j]*id;\n            const float x1 = x[i*qk + qk/2 + j]*id;\n\n            const uint8_t xi0 = MIN(15, (int8_t)(x0 + 8.5f));\n            const uint8_t xi1 = MIN(15, (int8_t)(x1 + 8.5f));\n\n            y[i].qs[j]  = xi0;\n            y[i].qs[j] |= xi1 << 4;\n        }\n    }\n}\n\nvoid quantize_row_q4_0(const float * restrict x, void * restrict y, int k) {\n    quantize_row_q4_0_reference(x, y, k);\n}\n\nvoid quantize_row_q4_1_reference(const float * restrict x, block_q4_1 * restrict y, int k) {\n    const int qk = QK4_1;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        float min = FLT_MAX;\n        float max = -FLT_MAX;\n\n        for (int j = 0; j < qk; j++) {\n            const float v = x[i*qk + j];\n\n            if (v < min) min = v;\n            if (v > max) max = v;\n        }\n\n        const float d  = (max - min) / ((1 << 4) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = GGML_FP32_TO_FP16(d);\n        y[i].m = GGML_FP32_TO_FP16(min);\n\n        for (int j = 0; j < qk/2; ++j) {\n            const float x0 = (x[i*qk + 0    + j] - min)*id;\n            const float x1 = (x[i*qk + qk/2 + j] - min)*id;\n\n            const uint8_t xi0 = MIN(15, (int8_t)(x0 + 0.5f));\n            const uint8_t xi1 = MIN(15, (int8_t)(x1 + 0.5f));\n\n            y[i].qs[j]  = xi0;\n            y[i].qs[j] |= xi1 << 4;\n        }\n    }\n}\n\nvoid quantize_row_q4_1(const float * restrict x, void * restrict y, int k) {\n    quantize_row_q4_1_reference(x, y, k);\n}\n\nvoid quantize_row_q5_0_reference(const float * restrict x, block_q5_0 * restrict y, int k) {\n    static const int qk = QK5_0;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        float amax = 0.0f; // absolute max\n        float max  = 0.0f;\n\n        for (int j = 0; j < qk; j++) {\n            const float v = x[i*qk + j];\n            if (amax < fabsf(v)) {\n                amax = fabsf(v);\n                max  = v;\n            }\n        }\n\n        const float d  = max / -16;\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = GGML_FP32_TO_FP16(d);\n\n        uint32_t qh = 0;\n\n        for (int j = 0; j < qk/2; ++j) {\n            const float x0 = x[i*qk + 0    + j]*id;\n            const float x1 = x[i*qk + qk/2 + j]*id;\n\n            const uint8_t xi0 = MIN(31, (int8_t)(x0 + 16.5f));\n            const uint8_t xi1 = MIN(31, (int8_t)(x1 + 16.5f));\n\n            y[i].qs[j] = (xi0 & 0x0F) | ((xi1 & 0x0F) << 4);\n\n            // get the 5-th bit and store it in qh at the right position\n            qh |= ((xi0 & 0x10u) >> 4) << (j + 0);\n            qh |= ((xi1 & 0x10u) >> 4) << (j + qk/2);\n        }\n\n        memcpy(&y[i].qh, &qh, sizeof(qh));\n    }\n}\n\nvoid quantize_row_q5_0(const float * restrict x, void * restrict y, int k) {\n    quantize_row_q5_0_reference(x, y, k);\n}\n\nvoid quantize_row_q5_1_reference(const float * restrict x, block_q5_1 * restrict y, int k) {\n    const int qk = QK5_1;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        float min = FLT_MAX;\n        float max = -FLT_MAX;\n\n        for (int j = 0; j < qk; j++) {\n            const float v = x[i*qk + j];\n\n            if (v < min) min = v;\n            if (v > max) max = v;\n        }\n\n        const float d  = (max - min) / ((1 << 5) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = GGML_FP32_TO_FP16(d);\n        y[i].m = GGML_FP32_TO_FP16(min);\n\n        uint32_t qh = 0;\n\n        for (int j = 0; j < qk/2; ++j) {\n            const float x0 = (x[i*qk + 0    + j] - min)*id;\n            const float x1 = (x[i*qk + qk/2 + j] - min)*id;\n\n            const uint8_t xi0 = (uint8_t)(x0 + 0.5f);\n            const uint8_t xi1 = (uint8_t)(x1 + 0.5f);\n\n            y[i].qs[j] = (xi0 & 0x0F) | ((xi1 & 0x0F) << 4);\n\n            // get the 5-th bit and store it in qh at the right position\n            qh |= ((xi0 & 0x10u) >> 4) << (j + 0);\n            qh |= ((xi1 & 0x10u) >> 4) << (j + qk/2);\n        }\n\n        memcpy(&y[i].qh, &qh, sizeof(y[i].qh));\n    }\n}\n\nvoid quantize_row_q5_1(const float * restrict x, void * restrict y, int k) {\n    quantize_row_q5_1_reference(x, y, k);\n}\n\n// reference implementation for deterministic creation of model files\nvoid quantize_row_q8_0_reference(const float * restrict x, block_q8_0 * restrict y, int k) {\n    assert(k % QK8_0 == 0);\n    const int nb = k / QK8_0;\n\n    for (int i = 0; i < nb; i++) {\n        float amax = 0.0f; // absolute max\n\n        for (int j = 0; j < QK8_0; j++) {\n            const float v = x[i*QK8_0 + j];\n            amax = MAX(amax, fabsf(v));\n        }\n\n        const float d = amax / ((1 << 7) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = GGML_FP32_TO_FP16(d);\n\n        for (int j = 0; j < QK8_0; ++j) {\n            const float x0 = x[i*QK8_0 + j]*id;\n\n            y[i].qs[j] = roundf(x0);\n        }\n    }\n}\n\nvoid quantize_row_q8_0(const float * restrict x, void * restrict vy, int k) {\n    assert(QK8_0 == 32);\n    assert(k % QK8_0 == 0);\n    const int nb = k / QK8_0;\n\n    block_q8_0 * restrict y = vy;\n\n#if defined(__ARM_NEON)\n    for (int i = 0; i < nb; i++) {\n        float32x4_t srcv [8];\n        float32x4_t asrcv[8];\n        float32x4_t amaxv[8];\n\n        for (int j = 0; j < 8; j++) srcv[j]  = vld1q_f32(x + i*32 + 4*j);\n        for (int j = 0; j < 8; j++) asrcv[j] = vabsq_f32(srcv[j]);\n\n        for (int j = 0; j < 4; j++) amaxv[2*j] = vmaxq_f32(asrcv[2*j], asrcv[2*j+1]);\n        for (int j = 0; j < 2; j++) amaxv[4*j] = vmaxq_f32(amaxv[4*j], amaxv[4*j+2]);\n        for (int j = 0; j < 1; j++) amaxv[8*j] = vmaxq_f32(amaxv[8*j], amaxv[8*j+4]);\n\n        const float amax = vmaxvq_f32(amaxv[0]);\n\n        const float d = amax / ((1 << 7) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = GGML_FP32_TO_FP16(d);\n\n        for (int j = 0; j < 8; j++) {\n            const float32x4_t v  = vmulq_n_f32(srcv[j], id);\n            const int32x4_t   vi = vcvtnq_s32_f32(v);\n\n            y[i].qs[4*j + 0] = vgetq_lane_s32(vi, 0);\n            y[i].qs[4*j + 1] = vgetq_lane_s32(vi, 1);\n            y[i].qs[4*j + 2] = vgetq_lane_s32(vi, 2);\n            y[i].qs[4*j + 3] = vgetq_lane_s32(vi, 3);\n        }\n    }\n#elif defined(__wasm_simd128__)\n    for (int i = 0; i < nb; i++) {\n        v128_t srcv [8];\n        v128_t asrcv[8];\n        v128_t amaxv[8];\n\n        for (int j = 0; j < 8; j++) srcv[j]  = wasm_v128_load(x + i*32 + 4*j);\n        for (int j = 0; j < 8; j++) asrcv[j] = wasm_f32x4_abs(srcv[j]);\n\n        for (int j = 0; j < 4; j++) amaxv[2*j] = wasm_f32x4_max(asrcv[2*j], asrcv[2*j+1]);\n        for (int j = 0; j < 2; j++) amaxv[4*j] = wasm_f32x4_max(amaxv[4*j], amaxv[4*j+2]);\n        for (int j = 0; j < 1; j++) amaxv[8*j] = wasm_f32x4_max(amaxv[8*j], amaxv[8*j+4]);\n\n        const float amax = MAX(MAX(wasm_f32x4_extract_lane(amaxv[0], 0),\n                                   wasm_f32x4_extract_lane(amaxv[0], 1)),\n                               MAX(wasm_f32x4_extract_lane(amaxv[0], 2),\n                                   wasm_f32x4_extract_lane(amaxv[0], 3)));\n\n        const float d = amax / ((1 << 7) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = GGML_FP32_TO_FP16(d);\n\n        for (int j = 0; j < 8; j++) {\n            const v128_t v  = wasm_f32x4_mul(srcv[j], wasm_f32x4_splat(id));\n            const v128_t vi = wasm_i32x4_trunc_sat_f32x4(v);\n\n            y[i].qs[4*j + 0] = wasm_i32x4_extract_lane(vi, 0);\n            y[i].qs[4*j + 1] = wasm_i32x4_extract_lane(vi, 1);\n            y[i].qs[4*j + 2] = wasm_i32x4_extract_lane(vi, 2);\n            y[i].qs[4*j + 3] = wasm_i32x4_extract_lane(vi, 3);\n        }\n    }\n#elif defined(__AVX2__) || defined(__AVX__)\n    for (int i = 0; i < nb; i++) {\n        // Load elements into 4 AVX vectors\n        __m256 v0 = _mm256_loadu_ps( x );\n        __m256 v1 = _mm256_loadu_ps( x + 8 );\n        __m256 v2 = _mm256_loadu_ps( x + 16 );\n        __m256 v3 = _mm256_loadu_ps( x + 24 );\n        x += 32;\n\n        // Compute max(abs(e)) for the block\n        const __m256 signBit = _mm256_set1_ps( -0.0f );\n        __m256 maxAbs = _mm256_andnot_ps( signBit, v0 );\n        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v1 ) );\n        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v2 ) );\n        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v3 ) );\n\n        __m128 max4 = _mm_max_ps( _mm256_extractf128_ps( maxAbs, 1 ), _mm256_castps256_ps128( maxAbs ) );\n        max4 = _mm_max_ps( max4, _mm_movehl_ps( max4, max4 ) );\n        max4 = _mm_max_ss( max4, _mm_movehdup_ps( max4 ) );\n        const float maxScalar = _mm_cvtss_f32( max4 );\n\n        // Quantize these floats\n        const float d = maxScalar / 127.f;\n        y[i].d = GGML_FP32_TO_FP16(d);\n        const float id = ( maxScalar != 0.0f ) ? 127.f / maxScalar : 0.0f;\n        const __m256 mul = _mm256_set1_ps( id );\n\n        // Apply the multiplier\n        v0 = _mm256_mul_ps( v0, mul );\n        v1 = _mm256_mul_ps( v1, mul );\n        v2 = _mm256_mul_ps( v2, mul );\n        v3 = _mm256_mul_ps( v3, mul );\n\n        // Round to nearest integer\n        v0 = _mm256_round_ps( v0, _MM_ROUND_NEAREST );\n        v1 = _mm256_round_ps( v1, _MM_ROUND_NEAREST );\n        v2 = _mm256_round_ps( v2, _MM_ROUND_NEAREST );\n        v3 = _mm256_round_ps( v3, _MM_ROUND_NEAREST );\n\n        // Convert floats to integers\n        __m256i i0 = _mm256_cvtps_epi32( v0 );\n        __m256i i1 = _mm256_cvtps_epi32( v1 );\n        __m256i i2 = _mm256_cvtps_epi32( v2 );\n        __m256i i3 = _mm256_cvtps_epi32( v3 );\n\n#if defined(__AVX2__)\n        // Convert int32 to int16\n        i0 = _mm256_packs_epi32( i0, i1 );\t// 0, 1, 2, 3,  8, 9, 10, 11,  4, 5, 6, 7, 12, 13, 14, 15\n        i2 = _mm256_packs_epi32( i2, i3 );\t// 16, 17, 18, 19,  24, 25, 26, 27,  20, 21, 22, 23, 28, 29, 30, 31\n                                            // Convert int16 to int8\n        i0 = _mm256_packs_epi16( i0, i2 );\t// 0, 1, 2, 3,  8, 9, 10, 11,  16, 17, 18, 19,  24, 25, 26, 27,  4, 5, 6, 7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31\n\n        // We got our precious signed bytes, but the order is now wrong\n        // These AVX2 pack instructions process 16-byte pieces independently\n        // The following instruction is fixing the order\n        const __m256i perm = _mm256_setr_epi32( 0, 4, 1, 5, 2, 6, 3, 7 );\n        i0 = _mm256_permutevar8x32_epi32( i0, perm );\n\n        _mm256_storeu_si256((__m256i *)y[i].qs, i0);\n#else\n        // Since we don't have in AVX some necessary functions,\n        // we split the registers in half and call AVX2 analogs from SSE\n        __m128i ni0 = _mm256_castsi256_si128( i0 );\n        __m128i ni1 = _mm256_extractf128_si256( i0, 1);\n        __m128i ni2 = _mm256_castsi256_si128( i1 );\n        __m128i ni3 = _mm256_extractf128_si256( i1, 1);\n        __m128i ni4 = _mm256_castsi256_si128( i2 );\n        __m128i ni5 = _mm256_extractf128_si256( i2, 1);\n        __m128i ni6 = _mm256_castsi256_si128( i3 );\n        __m128i ni7 = _mm256_extractf128_si256( i3, 1);\n\n        // Convert int32 to int16\n        ni0 = _mm_packs_epi32( ni0, ni1 );\n        ni2 = _mm_packs_epi32( ni2, ni3 );\n        ni4 = _mm_packs_epi32( ni4, ni5 );\n        ni6 = _mm_packs_epi32( ni6, ni7 );\n        // Convert int16 to int8\n        ni0 = _mm_packs_epi16( ni0, ni2 );\n        ni4 = _mm_packs_epi16( ni4, ni6 );\n\n        _mm_storeu_si128((__m128i *)(y[i].qs +  0), ni0);\n        _mm_storeu_si128((__m128i *)(y[i].qs + 16), ni4);\n#endif\n    }\n#elif defined(__riscv_v_intrinsic)\n\n    size_t vl = __riscv_vsetvl_e32m4(QK8_0);\n\n    for (int i = 0; i < nb; i++) {\n        // load elements\n        vfloat32m4_t v_x   = __riscv_vle32_v_f32m4(x+i*QK8_0, vl);\n\n        vfloat32m4_t vfabs = __riscv_vfabs_v_f32m4(v_x, vl);\n        vfloat32m1_t tmp   = __riscv_vfmv_v_f_f32m1(0.0f, vl);\n        vfloat32m1_t vmax  = __riscv_vfredmax_vs_f32m4_f32m1(vfabs, tmp, vl);\n        float amax = __riscv_vfmv_f_s_f32m1_f32(vmax);\n\n        const float d = amax / ((1 << 7) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = GGML_FP32_TO_FP16(d);\n\n        vfloat32m4_t x0 = __riscv_vfmul_vf_f32m4(v_x, id, vl);\n\n        // convert to integer\n        vint16m2_t   vi = __riscv_vfncvt_x_f_w_i16m2(x0, vl);\n        vint8m1_t    vs = __riscv_vncvt_x_x_w_i8m1(vi, vl);\n\n        // store result\n        __riscv_vse8_v_i8m1(y[i].qs , vs, vl);\n    }\n#else\n    GGML_UNUSED(nb);\n    // scalar\n    quantize_row_q8_0_reference(x, y, k);\n#endif\n}\n\n// reference implementation for deterministic creation of model files\nvoid quantize_row_q8_1_reference(const float * restrict x, block_q8_1 * restrict y, int k) {\n    assert(QK8_1 == 32);\n    assert(k % QK8_1 == 0);\n    const int nb = k / QK8_1;\n\n    for (int i = 0; i < nb; i++) {\n        float amax = 0.0f; // absolute max\n\n        for (int j = 0; j < QK8_1; j++) {\n            const float v = x[i*QK8_1 + j];\n            amax = MAX(amax, fabsf(v));\n        }\n\n        const float d = amax / ((1 << 7) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = d;\n\n        int sum = 0;\n\n        for (int j = 0; j < QK8_1/2; ++j) {\n            const float v0 = x[i*QK8_1           + j]*id;\n            const float v1 = x[i*QK8_1 + QK8_1/2 + j]*id;\n\n            y[i].qs[          j] = roundf(v0);\n            y[i].qs[QK8_1/2 + j] = roundf(v1);\n\n            sum += y[i].qs[          j];\n            sum += y[i].qs[QK8_1/2 + j];\n        }\n\n        y[i].s = sum*d;\n    }\n}\n\nvoid quantize_row_q8_1(const float * restrict x, void * restrict vy, int k) {\n    assert(k % QK8_1 == 0);\n    const int nb = k / QK8_1;\n\n    block_q8_1 * restrict y = vy;\n\n#if defined(__ARM_NEON)\n    for (int i = 0; i < nb; i++) {\n        float32x4_t srcv [8];\n        float32x4_t asrcv[8];\n        float32x4_t amaxv[8];\n\n        for (int j = 0; j < 8; j++) srcv[j]  = vld1q_f32(x + i*32 + 4*j);\n        for (int j = 0; j < 8; j++) asrcv[j] = vabsq_f32(srcv[j]);\n\n        for (int j = 0; j < 4; j++) amaxv[2*j] = vmaxq_f32(asrcv[2*j], asrcv[2*j+1]);\n        for (int j = 0; j < 2; j++) amaxv[4*j] = vmaxq_f32(amaxv[4*j], amaxv[4*j+2]);\n        for (int j = 0; j < 1; j++) amaxv[8*j] = vmaxq_f32(amaxv[8*j], amaxv[8*j+4]);\n\n        const float amax = vmaxvq_f32(amaxv[0]);\n\n        const float d = amax / ((1 << 7) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = d;\n\n        int32x4_t accv = vdupq_n_s32(0);\n\n        for (int j = 0; j < 8; j++) {\n            const float32x4_t v  = vmulq_n_f32(srcv[j], id);\n            const int32x4_t   vi = vcvtnq_s32_f32(v);\n\n            y[i].qs[4*j + 0] = vgetq_lane_s32(vi, 0);\n            y[i].qs[4*j + 1] = vgetq_lane_s32(vi, 1);\n            y[i].qs[4*j + 2] = vgetq_lane_s32(vi, 2);\n            y[i].qs[4*j + 3] = vgetq_lane_s32(vi, 3);\n\n            accv = vaddq_s32(accv, vi);\n        }\n\n        y[i].s = d * vaddvq_s32(accv);\n    }\n#elif defined(__wasm_simd128__)\n    for (int i = 0; i < nb; i++) {\n        v128_t srcv [8];\n        v128_t asrcv[8];\n        v128_t amaxv[8];\n\n        for (int j = 0; j < 8; j++) srcv[j]  = wasm_v128_load(x + i*32 + 4*j);\n        for (int j = 0; j < 8; j++) asrcv[j] = wasm_f32x4_abs(srcv[j]);\n\n        for (int j = 0; j < 4; j++) amaxv[2*j] = wasm_f32x4_max(asrcv[2*j], asrcv[2*j+1]);\n        for (int j = 0; j < 2; j++) amaxv[4*j] = wasm_f32x4_max(amaxv[4*j], amaxv[4*j+2]);\n        for (int j = 0; j < 1; j++) amaxv[8*j] = wasm_f32x4_max(amaxv[8*j], amaxv[8*j+4]);\n\n        const float amax = MAX(MAX(wasm_f32x4_extract_lane(amaxv[0], 0),\n                                   wasm_f32x4_extract_lane(amaxv[0], 1)),\n                               MAX(wasm_f32x4_extract_lane(amaxv[0], 2),\n                                   wasm_f32x4_extract_lane(amaxv[0], 3)));\n\n        const float d = amax / ((1 << 7) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = d;\n\n        v128_t accv = wasm_i32x4_splat(0);\n\n        for (int j = 0; j < 8; j++) {\n            const v128_t v  = wasm_f32x4_mul(srcv[j], wasm_f32x4_splat(id));\n            const v128_t vi = wasm_i32x4_trunc_sat_f32x4(v);\n\n            y[i].qs[4*j + 0] = wasm_i32x4_extract_lane(vi, 0);\n            y[i].qs[4*j + 1] = wasm_i32x4_extract_lane(vi, 1);\n            y[i].qs[4*j + 2] = wasm_i32x4_extract_lane(vi, 2);\n            y[i].qs[4*j + 3] = wasm_i32x4_extract_lane(vi, 3);\n\n            accv = wasm_i32x4_add(accv, vi);\n        }\n\n        y[i].s = d * (wasm_i32x4_extract_lane(accv, 0) +\n                      wasm_i32x4_extract_lane(accv, 1) +\n                      wasm_i32x4_extract_lane(accv, 2) +\n                      wasm_i32x4_extract_lane(accv, 3));\n    }\n#elif defined(__AVX2__) || defined(__AVX__)\n    for (int i = 0; i < nb; i++) {\n        // Load elements into 4 AVX vectors\n        __m256 v0 = _mm256_loadu_ps( x );\n        __m256 v1 = _mm256_loadu_ps( x + 8 );\n        __m256 v2 = _mm256_loadu_ps( x + 16 );\n        __m256 v3 = _mm256_loadu_ps( x + 24 );\n        x += 32;\n\n        // Compute max(abs(e)) for the block\n        const __m256 signBit = _mm256_set1_ps( -0.0f );\n        __m256 maxAbs = _mm256_andnot_ps( signBit, v0 );\n        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v1 ) );\n        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v2 ) );\n        maxAbs = _mm256_max_ps( maxAbs, _mm256_andnot_ps( signBit, v3 ) );\n\n        __m128 max4 = _mm_max_ps( _mm256_extractf128_ps( maxAbs, 1 ), _mm256_castps256_ps128( maxAbs ) );\n        max4 = _mm_max_ps( max4, _mm_movehl_ps( max4, max4 ) );\n        max4 = _mm_max_ss( max4, _mm_movehdup_ps( max4 ) );\n        const float maxScalar = _mm_cvtss_f32( max4 );\n\n        // Quantize these floats\n        const float d = maxScalar / 127.f;\n        y[i].d = d;\n        const float id = ( maxScalar != 0.0f ) ? 127.f / maxScalar : 0.0f;\n        const __m256 mul = _mm256_set1_ps( id );\n\n        // Apply the multiplier\n        v0 = _mm256_mul_ps( v0, mul );\n        v1 = _mm256_mul_ps( v1, mul );\n        v2 = _mm256_mul_ps( v2, mul );\n        v3 = _mm256_mul_ps( v3, mul );\n\n        // Round to nearest integer\n        v0 = _mm256_round_ps( v0, _MM_ROUND_NEAREST );\n        v1 = _mm256_round_ps( v1, _MM_ROUND_NEAREST );\n        v2 = _mm256_round_ps( v2, _MM_ROUND_NEAREST );\n        v3 = _mm256_round_ps( v3, _MM_ROUND_NEAREST );\n\n        // Convert floats to integers\n        __m256i i0 = _mm256_cvtps_epi32( v0 );\n        __m256i i1 = _mm256_cvtps_epi32( v1 );\n        __m256i i2 = _mm256_cvtps_epi32( v2 );\n        __m256i i3 = _mm256_cvtps_epi32( v3 );\n\n#if defined(__AVX2__)\n        // Compute the sum of the quants and set y[i].s\n        y[i].s = d * hsum_i32_8(_mm256_add_epi32(_mm256_add_epi32(i0, i1), _mm256_add_epi32(i2, i3)));\n\n        // Convert int32 to int16\n        i0 = _mm256_packs_epi32( i0, i1 );\t// 0, 1, 2, 3,  8, 9, 10, 11,  4, 5, 6, 7, 12, 13, 14, 15\n        i2 = _mm256_packs_epi32( i2, i3 );\t// 16, 17, 18, 19,  24, 25, 26, 27,  20, 21, 22, 23, 28, 29, 30, 31\n                                            // Convert int16 to int8\n        i0 = _mm256_packs_epi16( i0, i2 );\t// 0, 1, 2, 3,  8, 9, 10, 11,  16, 17, 18, 19,  24, 25, 26, 27,  4, 5, 6, 7, 12, 13, 14, 15, 20, 21, 22, 23, 28, 29, 30, 31\n\n        // We got our precious signed bytes, but the order is now wrong\n        // These AVX2 pack instructions process 16-byte pieces independently\n        // The following instruction is fixing the order\n        const __m256i perm = _mm256_setr_epi32( 0, 4, 1, 5, 2, 6, 3, 7 );\n        i0 = _mm256_permutevar8x32_epi32( i0, perm );\n\n        _mm256_storeu_si256((__m256i *)y[i].qs, i0);\n#else\n        // Since we don't have in AVX some necessary functions,\n        // we split the registers in half and call AVX2 analogs from SSE\n        __m128i ni0 = _mm256_castsi256_si128( i0 );\n        __m128i ni1 = _mm256_extractf128_si256( i0, 1);\n        __m128i ni2 = _mm256_castsi256_si128( i1 );\n        __m128i ni3 = _mm256_extractf128_si256( i1, 1);\n        __m128i ni4 = _mm256_castsi256_si128( i2 );\n        __m128i ni5 = _mm256_extractf128_si256( i2, 1);\n        __m128i ni6 = _mm256_castsi256_si128( i3 );\n        __m128i ni7 = _mm256_extractf128_si256( i3, 1);\n\n        // Compute the sum of the quants and set y[i].s\n        const __m128i s0 = _mm_add_epi32(_mm_add_epi32(ni0, ni1), _mm_add_epi32(ni2, ni3));\n        const __m128i s1 = _mm_add_epi32(_mm_add_epi32(ni4, ni5), _mm_add_epi32(ni6, ni7));\n        y[i].s = d * hsum_i32_4(_mm_add_epi32(s0, s1));\n\n        // Convert int32 to int16\n        ni0 = _mm_packs_epi32( ni0, ni1 );\n        ni2 = _mm_packs_epi32( ni2, ni3 );\n        ni4 = _mm_packs_epi32( ni4, ni5 );\n        ni6 = _mm_packs_epi32( ni6, ni7 );\n        // Convert int16 to int8\n        ni0 = _mm_packs_epi16( ni0, ni2 );\n        ni4 = _mm_packs_epi16( ni4, ni6 );\n\n        _mm_storeu_si128((__m128i *)(y[i].qs +  0), ni0);\n        _mm_storeu_si128((__m128i *)(y[i].qs + 16), ni4);\n#endif\n    }\n#elif defined(__riscv_v_intrinsic)\n\n    size_t vl = __riscv_vsetvl_e32m4(QK8_1);\n\n    for (int i = 0; i < nb; i++) {\n        // load elements\n        vfloat32m4_t v_x   = __riscv_vle32_v_f32m4(x+i*QK8_1, vl);\n\n        vfloat32m4_t vfabs = __riscv_vfabs_v_f32m4(v_x, vl);\n        vfloat32m1_t tmp   = __riscv_vfmv_v_f_f32m1(0.0, vl);\n        vfloat32m1_t vmax  = __riscv_vfredmax_vs_f32m4_f32m1(vfabs, tmp, vl);\n        float amax = __riscv_vfmv_f_s_f32m1_f32(vmax);\n\n        const float d  = amax / ((1 << 7) - 1);\n        const float id = d ? 1.0f/d : 0.0f;\n\n        y[i].d = d;\n\n        vfloat32m4_t x0 = __riscv_vfmul_vf_f32m4(v_x, id, vl);\n\n        // convert to integer\n        vint16m2_t   vi = __riscv_vfncvt_x_f_w_i16m2(x0, vl);\n        vint8m1_t    vs = __riscv_vncvt_x_x_w_i8m1(vi, vl);\n\n        // store result\n        __riscv_vse8_v_i8m1(y[i].qs , vs, vl);\n\n        // compute sum for y[i].s\n        vint16m1_t tmp2 = __riscv_vmv_v_x_i16m1(0, vl);\n        vint16m1_t vwrs = __riscv_vwredsum_vs_i8m1_i16m1(vs, tmp2, vl);\n\n        // set y[i].s\n        int sum = __riscv_vmv_x_s_i16m1_i16(vwrs);\n        y[i].s = sum*d;\n    }\n#else\n    GGML_UNUSED(nb);\n    // scalar\n    quantize_row_q8_1_reference(x, y, k);\n#endif\n}\n\nvoid dequantize_row_q4_0(const block_q4_0 * restrict x, float * restrict y, int k) {\n    static const int qk = QK4_0;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n\n        for (int j = 0; j < qk/2; ++j) {\n            const int x0 = (x[i].qs[j] & 0x0F) - 8;\n            const int x1 = (x[i].qs[j] >>   4) - 8;\n\n            y[i*qk + j + 0   ] = x0*d;\n            y[i*qk + j + qk/2] = x1*d;\n        }\n    }\n}\n\nvoid dequantize_row_q4_1(const block_q4_1 * restrict x, float * restrict y, int k) {\n    static const int qk = QK4_1;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n        const float m = GGML_FP16_TO_FP32(x[i].m);\n\n        for (int j = 0; j < qk/2; ++j) {\n            const int x0 = (x[i].qs[j] & 0x0F);\n            const int x1 = (x[i].qs[j] >>   4);\n\n            y[i*qk + j + 0   ] = x0*d + m;\n            y[i*qk + j + qk/2] = x1*d + m;\n        }\n    }\n}\n\nvoid dequantize_row_q5_0(const block_q5_0 * restrict x, float * restrict y, int k) {\n    static const int qk = QK5_0;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n\n        uint32_t qh;\n        memcpy(&qh, x[i].qh, sizeof(qh));\n\n        for (int j = 0; j < qk/2; ++j) {\n            const uint8_t xh_0 = ((qh >> (j +  0)) << 4) & 0x10;\n            const uint8_t xh_1 = ((qh >> (j + 12))     ) & 0x10;\n\n            const int32_t x0 = ((x[i].qs[j] & 0x0F) | xh_0) - 16;\n            const int32_t x1 = ((x[i].qs[j] >>   4) | xh_1) - 16;\n\n            y[i*qk + j + 0   ] = x0*d;\n            y[i*qk + j + qk/2] = x1*d;\n        }\n    }\n}\n\nvoid dequantize_row_q5_1(const block_q5_1 * restrict x, float * restrict y, int k) {\n    static const int qk = QK5_1;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n        const float m = GGML_FP16_TO_FP32(x[i].m);\n\n        uint32_t qh;\n        memcpy(&qh, x[i].qh, sizeof(qh));\n\n        for (int j = 0; j < qk/2; ++j) {\n            const uint8_t xh_0 = ((qh >> (j +  0)) << 4) & 0x10;\n            const uint8_t xh_1 = ((qh >> (j + 12))     ) & 0x10;\n\n            const int x0 = (x[i].qs[j] & 0x0F) | xh_0;\n            const int x1 = (x[i].qs[j] >>   4) | xh_1;\n\n            y[i*qk + j + 0   ] = x0*d + m;\n            y[i*qk + j + qk/2] = x1*d + m;\n        }\n    }\n}\n\nvoid dequantize_row_q8_0(const block_q8_0 * restrict x, float * restrict y, int k) {\n    static const int qk = QK8_0;\n\n    assert(k % qk == 0);\n\n    const int nb = k / qk;\n\n    for (int i = 0; i < nb; i++) {\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n\n        for (int j = 0; j < qk; ++j) {\n            y[i*qk + j] = x[i].qs[j]*d;\n        }\n    }\n}\n\n//\n// 2-6 bit quantization in super-blocks\n//\n\n//\n// ===================== Helper functions\n//\nstatic inline int nearest_int(float fval) {\n    assert(fval <= 4194303.f);\n    float val = fval + 12582912.f;\n    int i; memcpy(&i, &val, sizeof(int));\n    return (i & 0x007fffff) - 0x00400000;\n}\n\nstatic float make_qx_quants(int n, int nmax, const float * restrict x, int8_t * restrict L, int rmse_type) {\n    float max = 0;\n    float amax = 0;\n    for (int i = 0; i < n; ++i) {\n        float ax = fabsf(x[i]);\n        if (ax > amax) { amax = ax; max = x[i]; }\n    }\n    if (amax < 1e-30f) { // all zero\n        for (int i = 0; i < n; ++i) {\n            L[i] = 0;\n        }\n        return 0.f;\n    }\n    float iscale = -nmax / max;\n    if (rmse_type == 0) {\n        for (int i = 0; i < n; ++i) {\n            int l = nearest_int(iscale * x[i]);\n            L[i] = nmax + MAX(-nmax, MIN(nmax-1, l));\n        }\n        return 1/iscale;\n    }\n    bool return_early = false;\n    if (rmse_type < 0) {\n        rmse_type = -rmse_type;\n        return_early = true;\n    }\n    int weight_type = rmse_type%2;\n    float sumlx = 0;\n    float suml2 = 0;\n    for (int i = 0; i < n; ++i) {\n        int l = nearest_int(iscale * x[i]);\n        l = MAX(-nmax, MIN(nmax-1, l));\n        L[i] = l + nmax;\n        float w = weight_type == 1 ? x[i] * x[i] : 1;\n        sumlx += w*x[i]*l;\n        suml2 += w*l*l;\n    }\n    float scale = sumlx/suml2;\n    if (return_early) return suml2 > 0 ? 0.5f*(scale + 1/iscale) : 1/iscale;\n    float best = scale * sumlx;\n    for (int is = -9; is <= 9; ++is) {\n        if (is == 0) {\n            continue;\n        }\n        iscale = -(nmax + 0.1f*is) / max;\n        sumlx = suml2 = 0;\n        for (int i = 0; i < n; ++i) {\n            int l = nearest_int(iscale * x[i]);\n            l = MAX(-nmax, MIN(nmax-1, l));\n            float w = weight_type == 1 ? x[i] * x[i] : 1;\n            sumlx += w*x[i]*l;\n            suml2 += w*l*l;\n        }\n        if (suml2 > 0 && sumlx*sumlx > best*suml2) {\n            for (int i = 0; i < n; ++i) {\n                int l = nearest_int(iscale * x[i]);\n                L[i] = nmax + MAX(-nmax, MIN(nmax-1, l));\n            }\n            scale = sumlx/suml2; best = scale*sumlx;\n        }\n    }\n    return scale;\n}\n\nstatic float make_q3_quants(int n, int nmax, const float * restrict x, int8_t * restrict L, bool do_rmse) {\n    float max = 0;\n    float amax = 0;\n    for (int i = 0; i < n; ++i) {\n        float ax = fabsf(x[i]);\n        if (ax > amax) { amax = ax; max = x[i]; }\n    }\n    if (!amax) { // all zero\n        for (int i = 0; i < n; ++i) { L[i] = 0; }\n        return 0.f;\n    }\n    float iscale = -nmax / max;\n    if (do_rmse) {\n        float sumlx = 0;\n        float suml2 = 0;\n        for (int i = 0; i < n; ++i) {\n            int l = nearest_int(iscale * x[i]);\n            l = MAX(-nmax, MIN(nmax-1, l));\n            L[i] = l;\n            float w = x[i]*x[i];\n            sumlx += w*x[i]*l;\n            suml2 += w*l*l;\n        }\n        for (int itry = 0; itry < 5; ++itry) {\n            int n_changed = 0;\n            for (int i = 0; i < n; ++i) {\n                float w = x[i]*x[i];\n                float slx = sumlx - w*x[i]*L[i];\n                if (slx > 0) {\n                    float sl2 = suml2 - w*L[i]*L[i];\n                    int new_l = nearest_int(x[i] * sl2 / slx);\n                    new_l = MAX(-nmax, MIN(nmax-1, new_l));\n                    if (new_l != L[i]) {\n                        slx += w*x[i]*new_l;\n                        sl2 += w*new_l*new_l;\n                        if (sl2 > 0 && slx*slx*suml2 > sumlx*sumlx*sl2) {\n                            L[i] = new_l; sumlx = slx; suml2 = sl2;\n                            ++n_changed;\n                        }\n                    }\n                }\n            }\n            if (!n_changed) {\n                break;\n            }\n        }\n        for (int i = 0; i < n; ++i) {\n            L[i] += nmax;\n        }\n        return sumlx / suml2;\n    }\n    for (int i = 0; i < n; ++i) {\n        int l = nearest_int(iscale * x[i]);\n        l = MAX(-nmax, MIN(nmax-1, l));\n        L[i] = l + nmax;\n    }\n    return 1/iscale;\n}\n\nstatic float make_qkx1_quants(int n, int nmax, const float * restrict x, uint8_t * restrict L, float * restrict the_min,\n        int ntry, float alpha) {\n    float min = x[0];\n    float max = x[0];\n    for (int i = 1; i < n; ++i) {\n        if (x[i] < min) min = x[i];\n        if (x[i] > max) max = x[i];\n    }\n    if (max == min) {\n        for (int i = 0; i < n; ++i) L[i] = 0;\n        *the_min = 0;\n        return 0.f;\n    }\n    if (min > 0) min = 0;\n    float iscale = nmax/(max - min);\n    float scale = 1/iscale;\n    for (int itry = 0; itry < ntry; ++itry) {\n        float sumlx = 0; int suml2 = 0;\n        bool did_change = false;\n        for (int i = 0; i < n; ++i) {\n            int l = nearest_int(iscale*(x[i] - min));\n            l = MAX(0, MIN(nmax, l));\n            if (l != L[i]) {\n                L[i] = l;\n                did_change = true;\n            }\n            sumlx += (x[i] - min)*l;\n            suml2 += l*l;\n        }\n        scale = sumlx/suml2;\n        float sum = 0;\n        for (int i = 0; i < n; ++i) {\n            sum += x[i] - scale*L[i];\n        }\n        min = alpha*min + (1 - alpha)*sum/n;\n        if (min > 0) min = 0;\n        iscale = 1/scale;\n        if (!did_change) break;\n    }\n    *the_min = -min;\n    return scale;\n}\n\nstatic float make_qkx2_quants(int n, int nmax, const float * restrict x, const float * restrict weights,\n        uint8_t * restrict L, float * restrict the_min, uint8_t * restrict Laux,\n        float rmin, float rdelta, int nstep, bool use_mad) {\n    float min = x[0];\n    float max = x[0];\n    float sum_w = weights[0];\n    float sum_x = sum_w * x[0];\n#ifdef HAVE_BUGGY_APPLE_LINKER\n    // use 'volatile' to prevent unroll and work around a bug in Apple ld64 1015.7\n    for (volatile int i = 1; i < n; ++i) {\n#else\n    for (int i = 1; i < n; ++i) {\n#endif\n        if (x[i] < min) min = x[i];\n        if (x[i] > max) max = x[i];\n        float w = weights[i];\n        sum_w += w;\n        sum_x += w * x[i];\n    }\n    if (min > 0) min = 0;\n    if (max == min) {\n        for (int i = 0; i < n; ++i) L[i] = 0;\n        *the_min = -min;\n        return 0.f;\n    }\n    float iscale = nmax/(max - min);\n    float scale = 1/iscale;\n    float best_mad = 0;\n    for (int i = 0; i < n; ++i) {\n        int l = nearest_int(iscale*(x[i] - min));\n        L[i] = MAX(0, MIN(nmax, l));\n        float diff = scale * L[i] + min - x[i];\n        diff = use_mad ? fabsf(diff) : diff * diff;\n        float w = weights[i];\n        best_mad += w * diff;\n    }\n    if (nstep < 1) {\n        *the_min = -min;\n        return scale;\n    }\n    for (int is = 0; is <= nstep; ++is) {\n        iscale = (rmin + rdelta*is + nmax)/(max - min);\n        float sum_l = 0, sum_l2 = 0, sum_xl = 0;\n        for (int i = 0; i < n; ++i) {\n            int l = nearest_int(iscale*(x[i] - min));\n            l = MAX(0, MIN(nmax, l));\n            Laux[i] = l;\n            float w = weights[i];\n            sum_l += w*l;\n            sum_l2 += w*l*l;\n            sum_xl += w*l*x[i];\n        }\n        float D = sum_w * sum_l2 - sum_l * sum_l;\n        if (D > 0) {\n            float this_scale = (sum_w * sum_xl - sum_x * sum_l)/D;\n            float this_min   = (sum_l2 * sum_x - sum_l * sum_xl)/D;\n            if (this_min > 0) {\n                this_min = 0;\n                this_scale = sum_xl / sum_l2;\n            }\n            float mad = 0;\n            for (int i = 0; i < n; ++i) {\n                float diff = this_scale * Laux[i] + this_min - x[i];\n                diff = use_mad ? fabsf(diff) : diff * diff;\n                float w = weights[i];\n                mad += w * diff;\n            }\n            if (mad < best_mad) {\n                for (int i = 0; i < n; ++i) {\n                    L[i] = Laux[i];\n                }\n                best_mad = mad;\n                scale = this_scale;\n                min = this_min;\n            }\n        }\n    }\n    *the_min = -min;\n    return scale;\n}\n\n#if QK_K == 256\nstatic inline void get_scale_min_k4(int j, const uint8_t * restrict q, uint8_t * restrict d, uint8_t * restrict m) {\n    if (j < 4) {\n        *d = q[j] & 63; *m = q[j + 4] & 63;\n    } else {\n        *d = (q[j+4] & 0xF) | ((q[j-4] >> 6) << 4);\n        *m = (q[j+4] >>  4) | ((q[j-0] >> 6) << 4);\n    }\n}\n#endif\n\n//========================- 2-bit (de)-quantization\n\nvoid quantize_row_q2_K_reference(const float * restrict x, block_q2_K * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    uint8_t L[QK_K];\n    uint8_t Laux[16];\n    float   weights[16];\n    float mins[QK_K/16];\n    float scales[QK_K/16];\n\n    const float q4scale = 15.f;\n\n    for (int i = 0; i < nb; i++) {\n        float max_scale = 0; // as we are deducting the min, scales are always positive\n        float max_min = 0;\n        for (int j = 0; j < QK_K/16; ++j) {\n            for (int l = 0; l < 16; ++l) weights[l] = fabsf(x[16*j + l]);\n            scales[j] = make_qkx2_quants(16, 3, x + 16*j, weights, L + 16*j, &mins[j], Laux, -0.5f, 0.1f, 15, true);\n            float scale = scales[j];\n            if (scale > max_scale) {\n                max_scale = scale;\n            }\n            float min = mins[j];\n            if (min > max_min) {\n                max_min = min;\n            }\n        }\n\n        if (max_scale > 0) {\n            float iscale = q4scale/max_scale;\n            for (int j = 0; j < QK_K/16; ++j) {\n                int l = nearest_int(iscale*scales[j]);\n                y[i].scales[j] = l;\n            }\n            y[i].d = GGML_FP32_TO_FP16(max_scale/q4scale);\n        } else {\n            for (int j = 0; j < QK_K/16; ++j) y[i].scales[j] = 0;\n            y[i].d = GGML_FP32_TO_FP16(0.f);\n        }\n        if (max_min > 0) {\n            float iscale = q4scale/max_min;\n            for (int j = 0; j < QK_K/16; ++j) {\n                int l = nearest_int(iscale*mins[j]);\n                y[i].scales[j] |= (l << 4);\n            }\n            y[i].dmin = GGML_FP32_TO_FP16(max_min/q4scale);\n        } else {\n            y[i].dmin = GGML_FP32_TO_FP16(0.f);\n        }\n        for (int j = 0; j < QK_K/16; ++j) {\n            const float d = GGML_FP16_TO_FP32(y[i].d) * (y[i].scales[j] & 0xF);\n            if (!d) continue;\n            const float dm = GGML_FP16_TO_FP32(y[i].dmin) * (y[i].scales[j] >> 4);\n            for (int ii = 0; ii < 16; ++ii) {\n                int l = nearest_int((x[16*j + ii] + dm)/d);\n                l = MAX(0, MIN(3, l));\n                L[16*j + ii] = l;\n            }\n        }\n\n#if QK_K == 256\n        for (int j = 0; j < QK_K; j += 128) {\n            for (int l = 0; l < 32; ++l) {\n                y[i].qs[j/4 + l] = L[j + l] | (L[j + l + 32] << 2) | (L[j + l + 64] << 4) | (L[j + l + 96] << 6);\n            }\n        }\n#else\n        for (int l = 0; l < 16; ++l) {\n            y[i].qs[l] = L[l] | (L[l + 16] << 2) | (L[l + 32] << 4) | (L[l + 48] << 6);\n        }\n#endif\n\n        x += QK_K;\n\n    }\n}\n\nvoid dequantize_row_q2_K(const block_q2_K * restrict x, float * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    for (int i = 0; i < nb; i++) {\n\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n        const float min = GGML_FP16_TO_FP32(x[i].dmin);\n\n        const uint8_t * q = x[i].qs;\n\n#if QK_K == 256\n        int is = 0;\n        float dl, ml;\n        for (int n = 0; n < QK_K; n += 128) {\n            int shift = 0;\n            for (int j = 0; j < 4; ++j) {\n\n                uint8_t sc = x[i].scales[is++];\n                dl = d * (sc & 0xF); ml = min * (sc >> 4);\n                for (int l = 0; l < 16; ++l) *y++ = dl * ((int8_t)((q[l] >> shift) & 3)) - ml;\n\n                sc = x[i].scales[is++];\n                dl = d * (sc & 0xF); ml = min * (sc >> 4);\n                for (int l = 0; l < 16; ++l) *y++ = dl * ((int8_t)((q[l+16] >> shift) & 3)) - ml;\n\n                shift += 2;\n            }\n            q += 32;\n        }\n#else\n        float dl1 = d * (x[i].scales[0] & 0xF), ml1 = min * (x[i].scales[0] >> 4);\n        float dl2 = d * (x[i].scales[1] & 0xF), ml2 = min * (x[i].scales[1] >> 4);\n        float dl3 = d * (x[i].scales[2] & 0xF), ml3 = min * (x[i].scales[2] >> 4);\n        float dl4 = d * (x[i].scales[3] & 0xF), ml4 = min * (x[i].scales[3] >> 4);\n        for (int l = 0; l < 16; ++l) {\n            y[l+ 0] = dl1 * ((int8_t)((q[l] >> 0) & 3)) - ml1;\n            y[l+16] = dl2 * ((int8_t)((q[l] >> 2) & 3)) - ml2;\n            y[l+32] = dl3 * ((int8_t)((q[l] >> 4) & 3)) - ml3;\n            y[l+48] = dl4 * ((int8_t)((q[l] >> 6) & 3)) - ml4;\n        }\n        y += QK_K;\n#endif\n    }\n}\n\nvoid quantize_row_q2_K(const float * restrict x, void * restrict vy, int k) {\n    quantize_row_q2_K_reference(x, vy, k);\n}\n\nsize_t ggml_quantize_q2_K(const float * restrict src, void * restrict dst, int n, int k, int64_t * restrict hist) {\n    (void)hist; // TODO: collect histograms\n\n    for (int j = 0; j < n; j += k) {\n        block_q2_K * restrict y = (block_q2_K *)dst + j/QK_K;\n        quantize_row_q2_K_reference(src + j, y, k);\n    }\n    return (n/QK_K*sizeof(block_q2_K));\n}\n\n//========================= 3-bit (de)-quantization\n\nvoid quantize_row_q3_K_reference(const float * restrict x, block_q3_K * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    int8_t L[QK_K];\n    float scales[QK_K / 16];\n\n    for (int i = 0; i < nb; i++) {\n\n        float max_scale = 0;\n        float amax = 0;\n        for (int j = 0; j < QK_K/16; ++j) {\n            scales[j] = make_q3_quants(16, 4, x + 16*j, L + 16*j, true);\n            float scale = fabsf(scales[j]);\n            if (scale > amax) {\n                amax = scale; max_scale = scales[j];\n            }\n        }\n\n#if QK_K == 256\n        memset(y[i].scales, 0, 12);\n        if (max_scale) {\n            float iscale = -32.f/max_scale;\n            for (int j = 0; j < QK_K/16; ++j) {\n                int8_t l = nearest_int(iscale*scales[j]);\n                l = MAX(-32, MIN(31, l)) + 32;\n                if (j < 8) {\n                    y[i].scales[j] = l & 0xF;\n                } else {\n                    y[i].scales[j-8] |= ((l & 0xF) << 4);\n                }\n                l >>= 4;\n                y[i].scales[j%4 + 8] |= (l << (2*(j/4)));\n            }\n            y[i].d = GGML_FP32_TO_FP16(1/iscale);\n        } else {\n            y[i].d = GGML_FP32_TO_FP16(0.f);\n        }\n\n        int8_t sc;\n        for (int j = 0; j < QK_K/16; ++j) {\n            sc = j < 8 ? y[i].scales[j] & 0xF : y[i].scales[j-8] >> 4;\n            sc = (sc | (((y[i].scales[8 + j%4] >> (2*(j/4))) & 3) << 4)) - 32;\n            float d = GGML_FP16_TO_FP32(y[i].d) * sc;\n            if (!d) {\n                continue;\n            }\n            for (int ii = 0; ii < 16; ++ii) {\n                int l = nearest_int(x[16*j + ii]/d);\n                l = MAX(-4, MIN(3, l));\n                L[16*j + ii] = l + 4;\n            }\n        }\n#else\n        if (max_scale) {\n            float iscale = -8.f/max_scale;\n            for (int j = 0; j < QK_K/16; j+=2) {\n                int l1 = nearest_int(iscale*scales[j]);\n                l1 = 8 + MAX(-8, MIN(7, l1));\n                int l2 = nearest_int(iscale*scales[j+1]);\n                l2 = 8 + MAX(-8, MIN(7, l2));\n                y[i].scales[j/2] = l1 | (l2 << 4);\n            }\n            y[i].d = GGML_FP32_TO_FP16(1/iscale);\n        } else {\n            for (int j = 0; j < QK_K/16; j+=2) {\n                y[i].scales[j/2] = 0;\n            }\n            y[i].d = GGML_FP32_TO_FP16(0.f);\n        }\n        for (int j = 0; j < QK_K/16; ++j) {\n            int s = j%2 == 0 ? y[i].scales[j/2] & 0xF : y[i].scales[j/2] >> 4;\n            float d = GGML_FP16_TO_FP32(y[i].d) * (s - 8);\n            if (!d) {\n                continue;\n            }\n            for (int ii = 0; ii < 16; ++ii) {\n                int l = nearest_int(x[16*j + ii]/d);\n                l = MAX(-4, MIN(3, l));\n                L[16*j + ii] = l + 4;\n            }\n        }\n#endif\n\n        memset(y[i].hmask, 0, QK_K/8);\n        // We put the high-bit for the 1st 8 quants into bit 0, the next 8 into bit 1, etc.\n        int m = 0;\n        uint8_t hm = 1;\n        for (int j = 0; j < QK_K; ++j) {\n            if (L[j] > 3) {\n                y[i].hmask[m] |= hm;\n                L[j] -= 4;\n            }\n            if (++m == QK_K/8) {\n                m = 0; hm <<= 1;\n            }\n        }\n#if QK_K == 256\n        for (int j = 0; j < QK_K; j += 128) {\n            for (int l = 0; l < 32; ++l) {\n                y[i].qs[j/4 + l] = L[j + l] | (L[j + l + 32] << 2) | (L[j + l + 64] << 4) | (L[j + l + 96] << 6);\n            }\n        }\n#else\n        for (int l = 0; l < 16; ++l) {\n            y[i].qs[l] = L[l] | (L[l + 16] << 2) | (L[l + 32] << 4) | (L[l + 48] << 6);\n        }\n#endif\n\n        x += QK_K;\n    }\n}\n\n#if QK_K == 256\nvoid dequantize_row_q3_K(const block_q3_K * restrict x, float * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    const uint32_t kmask1 = 0x03030303;\n    const uint32_t kmask2 = 0x0f0f0f0f;\n\n    uint32_t aux[4];\n    const int8_t * scales = (const int8_t*)aux;\n\n    for (int i = 0; i < nb; i++) {\n\n        const float d_all = GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q = x[i].qs;\n        const uint8_t * restrict hm = x[i].hmask;\n        uint8_t m = 1;\n\n        memcpy(aux, x[i].scales, 12);\n        uint32_t tmp = aux[2];\n        aux[2] = ((aux[0] >> 4) & kmask2) | (((tmp >> 4) & kmask1) << 4);\n        aux[3] = ((aux[1] >> 4) & kmask2) | (((tmp >> 6) & kmask1) << 4);\n        aux[0] = (aux[0] & kmask2) | (((tmp >> 0) & kmask1) << 4);\n        aux[1] = (aux[1] & kmask2) | (((tmp >> 2) & kmask1) << 4);\n\n        int is = 0;\n        float dl;\n        for (int n = 0; n < QK_K; n += 128) {\n            int shift = 0;\n            for (int j = 0; j < 4; ++j) {\n\n                dl = d_all * (scales[is++] - 32);\n                for (int l = 0; l < 16; ++l) {\n                    *y++ = dl * ((int8_t)((q[l+ 0] >> shift) & 3) - ((hm[l+ 0] & m) ? 0 : 4));\n                }\n\n                dl = d_all * (scales[is++] - 32);\n                for (int l = 0; l < 16; ++l) {\n                    *y++ = dl * ((int8_t)((q[l+16] >> shift) & 3) - ((hm[l+16] & m) ? 0 : 4));\n                }\n\n                shift += 2;\n                m <<= 1;\n            }\n            q += 32;\n        }\n\n    }\n}\n#else\nvoid dequantize_row_q3_K(const block_q3_K * restrict x, float * restrict y, int k) {\n    assert(k % QK_K == 0);\n    assert(QK_K == 64);\n    const int nb = k / QK_K;\n\n    for (int i = 0; i < nb; i++) {\n\n        const float d_all = GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q = x[i].qs;\n        const uint8_t * restrict hm = x[i].hmask;\n\n        const float d1 = d_all * ((x[i].scales[0] & 0xF) - 8);\n        const float d2 = d_all * ((x[i].scales[0] >>  4) - 8);\n        const float d3 = d_all * ((x[i].scales[1] & 0xF) - 8);\n        const float d4 = d_all * ((x[i].scales[1] >>  4) - 8);\n\n        for (int l=0; l<8; ++l) {\n            uint8_t h = hm[l];\n            y[l+ 0] = d1 * ((int8_t)((q[l+0] >> 0) & 3) - ((h & 0x01) ? 0 : 4));\n            y[l+ 8] = d1 * ((int8_t)((q[l+8] >> 0) & 3) - ((h & 0x02) ? 0 : 4));\n            y[l+16] = d2 * ((int8_t)((q[l+0] >> 2) & 3) - ((h & 0x04) ? 0 : 4));\n            y[l+24] = d2 * ((int8_t)((q[l+8] >> 2) & 3) - ((h & 0x08) ? 0 : 4));\n            y[l+32] = d3 * ((int8_t)((q[l+0] >> 4) & 3) - ((h & 0x10) ? 0 : 4));\n            y[l+40] = d3 * ((int8_t)((q[l+8] >> 4) & 3) - ((h & 0x20) ? 0 : 4));\n            y[l+48] = d4 * ((int8_t)((q[l+0] >> 6) & 3) - ((h & 0x40) ? 0 : 4));\n            y[l+56] = d4 * ((int8_t)((q[l+8] >> 6) & 3) - ((h & 0x80) ? 0 : 4));\n        }\n        y += QK_K;\n    }\n}\n#endif\n\nvoid quantize_row_q3_K(const float * restrict x, void * restrict vy, int k) {\n    quantize_row_q3_K_reference(x, vy, k);\n}\n\nsize_t ggml_quantize_q3_K(const float * restrict src, void * restrict dst, int n, int k, int64_t * restrict hist) {\n    (void)hist; // TODO: collect histograms\n\n    for (int j = 0; j < n; j += k) {\n        block_q3_K * restrict y = (block_q3_K *)dst + j/QK_K;\n        quantize_row_q3_K_reference(src + j, y, k);\n    }\n    return (n/QK_K*sizeof(block_q3_K));\n}\n\n// ====================== 4-bit (de)-quantization\n\nvoid quantize_row_q4_K_reference(const float * restrict x, block_q4_K * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    uint8_t L[QK_K];\n    uint8_t Laux[32];\n    float   weights[32];\n    float mins[QK_K/32];\n    float scales[QK_K/32];\n\n    for (int i = 0; i < nb; i++) {\n\n        float max_scale = 0; // as we are deducting the min, scales are always positive\n        float max_min = 0;\n        for (int j = 0; j < QK_K/32; ++j) {\n            //scales[j] = make_qkx1_quants(32, 15, x + 32*j, L + 32*j, &mins[j], 9, 0.5f);\n            float sum_x2 = 0;\n            for (int l = 0; l < 32; ++l) sum_x2 += x[32*j + l] * x[32*j + l];\n            float av_x = sqrtf(sum_x2/32);\n            for (int l = 0; l < 32; ++l) weights[l] = av_x + fabsf(x[32*j + l]);\n            scales[j] = make_qkx2_quants(32, 15, x + 32*j, weights, L + 32*j, &mins[j], Laux, -1.f, 0.1f, 20, false);\n            float scale = scales[j];\n            if (scale > max_scale) {\n                max_scale = scale;\n            }\n            float min = mins[j];\n            if (min > max_min) {\n                max_min = min;\n            }\n        }\n\n#if QK_K == 256\n        float inv_scale = max_scale > 0 ? 63.f/max_scale : 0.f;\n        float inv_min   = max_min   > 0 ? 63.f/max_min   : 0.f;\n        for (int j = 0; j < QK_K/32; ++j) {\n            uint8_t ls = nearest_int(inv_scale*scales[j]);\n            uint8_t lm = nearest_int(inv_min*mins[j]);\n            ls = MIN(63, ls);\n            lm = MIN(63, lm);\n            if (j < 4) {\n                y[i].scales[j] = ls;\n                y[i].scales[j+4] = lm;\n            } else {\n                y[i].scales[j+4] = (ls & 0xF) | ((lm & 0xF) << 4);\n                y[i].scales[j-4] |= ((ls >> 4) << 6);\n                y[i].scales[j-0] |= ((lm >> 4) << 6);\n            }\n        }\n        y[i].d = GGML_FP32_TO_FP16(max_scale/63.f);\n        y[i].dmin = GGML_FP32_TO_FP16(max_min/63.f);\n\n        uint8_t sc, m;\n        for (int j = 0; j < QK_K/32; ++j) {\n            get_scale_min_k4(j, y[i].scales, &sc, &m);\n            const float d = GGML_FP16_TO_FP32(y[i].d) * sc;\n            if (!d) continue;\n            const float dm = GGML_FP16_TO_FP32(y[i].dmin) * m;\n            for (int ii = 0; ii < 32; ++ii) {\n                int l = nearest_int((x[32*j + ii] + dm)/d);\n                l = MAX(0, MIN(15, l));\n                L[32*j + ii] = l;\n            }\n        }\n#else\n        const float s_factor = 15.f;\n        float inv_scale = max_scale > 0 ? s_factor/max_scale : 0.f;\n        float inv_min   = max_min   > 0 ? s_factor/max_min   : 0.f;\n        int d1 = nearest_int(inv_scale*scales[0]);\n        int m1 = nearest_int(inv_min*mins[0]);\n        int d2 = nearest_int(inv_scale*scales[1]);\n        int m2 = nearest_int(inv_min*mins[1]);\n        y[i].scales[0] = d1 | (m1 << 4);\n        y[i].scales[1] = d2 | (m2 << 4);\n        y[i].d[0] = GGML_FP32_TO_FP16(max_scale/s_factor);\n        y[i].d[1] = GGML_FP32_TO_FP16(max_min/s_factor);\n\n        float sumlx = 0;\n        int   suml2 = 0;\n        for (int j = 0; j < QK_K/32; ++j) {\n            const uint8_t sd = y[i].scales[j] & 0xF;\n            const uint8_t sm = y[i].scales[j] >>  4;\n            const float d = GGML_FP16_TO_FP32(y[i].d[0]) * sd;\n            if (!d) continue;\n            const float m = GGML_FP16_TO_FP32(y[i].d[1]) * sm;\n            for (int ii = 0; ii < 32; ++ii) {\n                int l = nearest_int((x[32*j + ii] + m)/d);\n                l = MAX(0, MIN(15, l));\n                L[32*j + ii] = l;\n                sumlx += (x[32*j + ii] + m)*l*sd;\n                suml2 += l*l*sd*sd;\n            }\n        }\n        if (suml2) {\n            y[i].d[0] = GGML_FP32_TO_FP16(sumlx/suml2);\n        }\n#endif\n        uint8_t * q = y[i].qs;\n        for (int j = 0; j < QK_K; j += 64) {\n            for (int l = 0; l < 32; ++l) q[l] = L[j + l] | (L[j + l + 32] << 4);\n            q += 32;\n        }\n\n        x += QK_K;\n\n    }\n}\n\nvoid dequantize_row_q4_K(const block_q4_K * restrict x, float * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    for (int i = 0; i < nb; i++) {\n\n        const uint8_t * q = x[i].qs;\n\n#if QK_K == 256\n\n        const float d   = GGML_FP16_TO_FP32(x[i].d);\n        const float min = GGML_FP16_TO_FP32(x[i].dmin);\n\n        int is = 0;\n        uint8_t sc, m;\n        for (int j = 0; j < QK_K; j += 64) {\n            get_scale_min_k4(is + 0, x[i].scales, &sc, &m);\n            const float d1 = d * sc; const float m1 = min * m;\n            get_scale_min_k4(is + 1, x[i].scales, &sc, &m);\n            const float d2 = d * sc; const float m2 = min * m;\n            for (int l = 0; l < 32; ++l) *y++ = d1 * (q[l] & 0xF) - m1;\n            for (int l = 0; l < 32; ++l) *y++ = d2 * (q[l]  >> 4) - m2;\n            q += 32; is += 2;\n        }\n#else\n        const float dall = GGML_FP16_TO_FP32(x[i].d[0]);\n        const float mall = GGML_FP16_TO_FP32(x[i].d[1]);\n        const float d1 = dall * (x[i].scales[0] & 0xF), m1 = mall * (x[i].scales[0] >> 4);\n        const float d2 = dall * (x[i].scales[1] & 0xF), m2 = mall * (x[i].scales[1] >> 4);\n        for (int l = 0; l < 32; ++l) {\n            y[l+ 0] = d1 * (q[l] & 0xF) - m1;\n            y[l+32] = d2 * (q[l] >>  4) - m2;\n        }\n        y += QK_K;\n#endif\n\n    }\n}\n\nvoid quantize_row_q4_K(const float * restrict x, void * restrict vy, int k) {\n    assert(k % QK_K == 0);\n    block_q4_K * restrict y = vy;\n    quantize_row_q4_K_reference(x, y, k);\n}\n\nsize_t ggml_quantize_q4_K(const float * restrict src, void * restrict dst, int n, int k, int64_t * restrict hist) {\n    assert(k % QK_K == 0);\n    (void)hist; // TODO: collect histograms\n\n    for (int j = 0; j < n; j += k) {\n        block_q4_K * restrict y = (block_q4_K *)dst + j/QK_K;\n        quantize_row_q4_K_reference(src + j, y, k);\n    }\n    return (n/QK_K*sizeof(block_q4_K));\n}\n\n// ====================== 5-bit (de)-quantization\n\nvoid quantize_row_q5_K_reference(const float * restrict x, block_q5_K * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n#if QK_K == 256\n    uint8_t L[QK_K];\n    float mins[QK_K/32];\n    float scales[QK_K/32];\n    float weights[32];\n    uint8_t Laux[32];\n#else\n    int8_t L[QK_K];\n    float scales[QK_K/16];\n#endif\n\n    for (int i = 0; i < nb; i++) {\n\n#if QK_K == 256\n\n        float max_scale = 0; // as we are deducting the min, scales are always positive\n        float max_min = 0;\n        for (int j = 0; j < QK_K/32; ++j) {\n            //scales[j] = make_qkx1_quants(32, 31, x + 32*j, L + 32*j, &mins[j], 9, 0.5f);\n            float sum_x2 = 0;\n            for (int l = 0; l < 32; ++l) sum_x2 += x[32*j + l] * x[32*j + l];\n            float av_x = sqrtf(sum_x2/32);\n            for (int l = 0; l < 32; ++l) weights[l] = av_x + fabsf(x[32*j + l]);\n            scales[j] = make_qkx2_quants(32, 31, x + 32*j, weights, L + 32*j, &mins[j], Laux, -0.5f, 0.1f, 15, false);\n            float scale = scales[j];\n            if (scale > max_scale) {\n                max_scale = scale;\n            }\n            float min = mins[j];\n            if (min > max_min) {\n                max_min = min;\n            }\n        }\n\n        float inv_scale = max_scale > 0 ? 63.f/max_scale : 0.f;\n        float inv_min   = max_min   > 0 ? 63.f/max_min   : 0.f;\n        for (int j = 0; j < QK_K/32; ++j) {\n            uint8_t ls = nearest_int(inv_scale*scales[j]);\n            uint8_t lm = nearest_int(inv_min*mins[j]);\n            ls = MIN(63, ls);\n            lm = MIN(63, lm);\n            if (j < 4) {\n                y[i].scales[j] = ls;\n                y[i].scales[j+4] = lm;\n            } else {\n                y[i].scales[j+4] = (ls & 0xF) | ((lm & 0xF) << 4);\n                y[i].scales[j-4] |= ((ls >> 4) << 6);\n                y[i].scales[j-0] |= ((lm >> 4) << 6);\n            }\n        }\n        y[i].d = GGML_FP32_TO_FP16(max_scale/63.f);\n        y[i].dmin = GGML_FP32_TO_FP16(max_min/63.f);\n\n        uint8_t sc, m;\n        for (int j = 0; j < QK_K/32; ++j) {\n            get_scale_min_k4(j, y[i].scales, &sc, &m);\n            const float d = GGML_FP16_TO_FP32(y[i].d) * sc;\n            if (!d) continue;\n            const float dm = GGML_FP16_TO_FP32(y[i].dmin) * m;\n            for (int ii = 0; ii < 32; ++ii) {\n                int l = nearest_int((x[32*j + ii] + dm)/d);\n                l = MAX(0, MIN(31, l));\n                L[32*j + ii] = l;\n            }\n        }\n\n        uint8_t * restrict qh = y[i].qh;\n        uint8_t * restrict ql = y[i].qs;\n        memset(qh, 0, QK_K/8);\n\n        uint8_t m1 = 1, m2 = 2;\n        for (int n = 0; n < QK_K; n += 64) {\n            for (int j = 0; j < 32; ++j) {\n                int l1 = L[n + j];\n                if (l1 > 15) {\n                    l1 -= 16; qh[j] |= m1;\n                }\n                int l2 = L[n + j + 32];\n                if (l2 > 15) {\n                    l2 -= 16; qh[j] |= m2;\n                }\n                ql[j] = l1 | (l2 << 4);\n            }\n            m1 <<= 2; m2 <<= 2;\n            ql += 32;\n        }\n#else\n        float max_scale = 0, amax = 0;\n        for (int j = 0; j < QK_K/16; ++j) {\n            scales[j] = make_qx_quants(16, 16, x + 16*j, L + 16*j, 1);\n            float abs_scale = fabsf(scales[j]);\n            if (abs_scale > amax) {\n                amax = abs_scale;\n                max_scale = scales[j];\n            }\n        }\n\n        float iscale = -128.f/max_scale;\n        for (int j = 0; j < QK_K/16; ++j) {\n            int l = nearest_int(iscale*scales[j]);\n            y[i].scales[j] = MAX(-128, MIN(127, l));\n        }\n        y[i].d = GGML_FP32_TO_FP16(1/iscale);\n\n        for (int j = 0; j < QK_K/16; ++j) {\n            const float d = GGML_FP16_TO_FP32(y[i].d) * y[i].scales[j];\n            if (!d) continue;\n            for (int ii = 0; ii < 16; ++ii) {\n                int l = nearest_int(x[16*j + ii]/d);\n                l = MAX(-16, MIN(15, l));\n                L[16*j + ii] = l + 16;\n            }\n        }\n\n        uint8_t * restrict qh = y[i].qh;\n        uint8_t * restrict ql = y[i].qs;\n        memset(qh, 0, QK_K/8);\n\n        for (int j = 0; j < 32; ++j) {\n            int jm = j%8;\n            int is = j/8;\n            int l1 = L[j];\n            if (l1 > 15) {\n                l1 -= 16; qh[jm] |= (1 << is);\n            }\n            int l2 = L[j + 32];\n            if (l2 > 15) {\n                l2 -= 16; qh[jm] |= (1 << (4 + is));\n            }\n            ql[j] = l1 | (l2 << 4);\n        }\n#endif\n\n        x += QK_K;\n\n    }\n}\n\nvoid dequantize_row_q5_K(const block_q5_K * restrict x, float * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    for (int i = 0; i < nb; i++) {\n\n        const uint8_t * ql = x[i].qs;\n        const uint8_t * qh = x[i].qh;\n\n#if QK_K == 256\n\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n        const float min = GGML_FP16_TO_FP32(x[i].dmin);\n\n        int is = 0;\n        uint8_t sc, m;\n        uint8_t u1 = 1, u2 = 2;\n        for (int j = 0; j < QK_K; j += 64) {\n            get_scale_min_k4(is + 0, x[i].scales, &sc, &m);\n            const float d1 = d * sc; const float m1 = min * m;\n            get_scale_min_k4(is + 1, x[i].scales, &sc, &m);\n            const float d2 = d * sc; const float m2 = min * m;\n            for (int l = 0; l < 32; ++l) *y++ = d1 * ((ql[l] & 0xF) + (qh[l] & u1 ? 16 : 0)) - m1;\n            for (int l = 0; l < 32; ++l) *y++ = d2 * ((ql[l]  >> 4) + (qh[l] & u2 ? 16 : 0)) - m2;\n            ql += 32; is += 2;\n            u1 <<= 2; u2 <<= 2;\n        }\n#else\n        float d = GGML_FP16_TO_FP32(x[i].d);\n        const int8_t * restrict s = x[i].scales;\n        for (int l = 0; l < 8; ++l) {\n            y[l+ 0] = d * s[0] * ((ql[l+ 0] & 0xF) - (qh[l] & 0x01 ? 0 : 16));\n            y[l+ 8] = d * s[0] * ((ql[l+ 8] & 0xF) - (qh[l] & 0x02 ? 0 : 16));\n            y[l+16] = d * s[1] * ((ql[l+16] & 0xF) - (qh[l] & 0x04 ? 0 : 16));\n            y[l+24] = d * s[1] * ((ql[l+24] & 0xF) - (qh[l] & 0x08 ? 0 : 16));\n            y[l+32] = d * s[2] * ((ql[l+ 0] >>  4) - (qh[l] & 0x10 ? 0 : 16));\n            y[l+40] = d * s[2] * ((ql[l+ 8] >>  4) - (qh[l] & 0x20 ? 0 : 16));\n            y[l+48] = d * s[3] * ((ql[l+16] >>  4) - (qh[l] & 0x40 ? 0 : 16));\n            y[l+56] = d * s[3] * ((ql[l+24] >>  4) - (qh[l] & 0x80 ? 0 : 16));\n        }\n        y += QK_K;\n#endif\n    }\n}\n\nvoid quantize_row_q5_K(const float * restrict x, void * restrict vy, int k) {\n    assert(k % QK_K == 0);\n    block_q5_K * restrict y = vy;\n    quantize_row_q5_K_reference(x, y, k);\n}\n\nsize_t ggml_quantize_q5_K(const float * restrict src, void * restrict dst, int n, int k, int64_t * restrict hist) {\n    assert(k % QK_K == 0);\n    (void)hist; // TODO: collect histograms\n\n    for (int j = 0; j < n; j += k) {\n        block_q5_K * restrict y = (block_q5_K *)dst + j/QK_K;\n        quantize_row_q5_K_reference(src + j, y, k);\n    }\n    return (n/QK_K*sizeof(block_q5_K));\n}\n\n// ====================== 6-bit (de)-quantization\n\nvoid quantize_row_q6_K_reference(const float * restrict x, block_q6_K * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    int8_t L[QK_K];\n    float   scales[QK_K/16];\n\n    for (int i = 0; i < nb; i++) {\n\n        float max_scale = 0;\n        float max_abs_scale = 0;\n\n        for (int ib = 0; ib < QK_K/16; ++ib) {\n\n            const float scale = make_qx_quants(16, 32, x + 16*ib, L + 16*ib, 1);\n            scales[ib] = scale;\n\n            const float abs_scale = fabsf(scale);\n            if (abs_scale > max_abs_scale) {\n                max_abs_scale = abs_scale;\n                max_scale = scale;\n            }\n\n        }\n\n        if (!max_abs_scale) {\n            memset(&y[i], 0, sizeof(block_q6_K));\n            y[i].d = GGML_FP32_TO_FP16(0.f);\n            x += QK_K;\n            continue;\n        }\n\n        float iscale = -128.f/max_scale;\n        y[i].d = GGML_FP32_TO_FP16(1/iscale);\n        for (int ib = 0; ib < QK_K/16; ++ib) {\n            y[i].scales[ib] = MIN(127, nearest_int(iscale*scales[ib]));\n        }\n\n        for (int j = 0; j < QK_K/16; ++j) {\n            float d = GGML_FP16_TO_FP32(y[i].d) * y[i].scales[j];\n            if (!d) {\n                continue;\n            }\n            for (int ii = 0; ii < 16; ++ii) {\n                int l = nearest_int(x[16*j + ii]/d);\n                l = MAX(-32, MIN(31, l));\n                L[16*j + ii] = l + 32;\n            }\n        }\n\n        uint8_t * restrict ql = y[i].ql;\n        uint8_t * restrict qh = y[i].qh;\n#if QK_K == 256\n        for (int j = 0; j < QK_K; j += 128) {\n            for (int l = 0; l < 32; ++l) {\n                const uint8_t q1 = L[j + l +  0] & 0xF;\n                const uint8_t q2 = L[j + l + 32] & 0xF;\n                const uint8_t q3 = L[j + l + 64] & 0xF;\n                const uint8_t q4 = L[j + l + 96] & 0xF;\n                ql[l+ 0] = q1 | (q3 << 4);\n                ql[l+32] = q2 | (q4 << 4);\n                qh[l] = (L[j + l] >> 4) | ((L[j + l + 32] >> 4) << 2) | ((L[j + l + 64] >> 4) << 4) | ((L[j + l + 96] >> 4) << 6);\n            }\n            ql += 64;\n            qh += 32;\n        }\n#else\n        for (int l = 0; l < 32; ++l) {\n            const uint8_t q1 = L[l +  0] & 0xF;\n            const uint8_t q2 = L[l + 32] & 0xF;\n            ql[l] = q1 | (q2 << 4);\n        }\n        for (int l = 0; l < 16; ++l) {\n            qh[l] = (L[l] >> 4) | ((L[l + 16] >> 4) << 2) | ((L[l + 32] >> 4) << 4) | ((L[l + 48] >> 4) << 6);\n        }\n#endif\n\n        x += QK_K;\n\n    }\n}\n\nvoid dequantize_row_q6_K(const block_q6_K * restrict x, float * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    for (int i = 0; i < nb; i++) {\n\n        const float d = GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict ql = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict sc = x[i].scales;\n\n#if QK_K == 256\n        for (int n = 0; n < QK_K; n += 128) {\n            for (int l = 0; l < 32; ++l) {\n                int is = l/16;\n                const int8_t q1 = (int8_t)((ql[l +  0] & 0xF) | (((qh[l] >> 0) & 3) << 4)) - 32;\n                const int8_t q2 = (int8_t)((ql[l + 32] & 0xF) | (((qh[l] >> 2) & 3) << 4)) - 32;\n                const int8_t q3 = (int8_t)((ql[l +  0]  >> 4) | (((qh[l] >> 4) & 3) << 4)) - 32;\n                const int8_t q4 = (int8_t)((ql[l + 32]  >> 4) | (((qh[l] >> 6) & 3) << 4)) - 32;\n                y[l +  0] = d * sc[is + 0] * q1;\n                y[l + 32] = d * sc[is + 2] * q2;\n                y[l + 64] = d * sc[is + 4] * q3;\n                y[l + 96] = d * sc[is + 6] * q4;\n            }\n            y  += 128;\n            ql += 64;\n            qh += 32;\n            sc += 8;\n        }\n#else\n        for (int l = 0; l < 16; ++l) {\n            const int8_t q1 = (int8_t)((ql[l+ 0] & 0xF) | (((qh[l] >> 0) & 3) << 4)) - 32;\n            const int8_t q2 = (int8_t)((ql[l+16] & 0xF) | (((qh[l] >> 2) & 3) << 4)) - 32;\n            const int8_t q3 = (int8_t)((ql[l+ 0]  >> 4) | (((qh[l] >> 4) & 3) << 4)) - 32;\n            const int8_t q4 = (int8_t)((ql[l+16]  >> 4) | (((qh[l] >> 6) & 3) << 4)) - 32;\n            y[l+ 0] = d * sc[0] * q1;\n            y[l+16] = d * sc[1] * q2;\n            y[l+32] = d * sc[2] * q3;\n            y[l+48] = d * sc[3] * q4;\n        }\n        y  += 64;\n#endif\n\n    }\n}\n\nvoid quantize_row_q6_K(const float * restrict x, void * restrict vy, int k) {\n    assert(k % QK_K == 0);\n    block_q6_K * restrict y = vy;\n    quantize_row_q6_K_reference(x, y, k);\n}\n\nsize_t ggml_quantize_q6_K(const float * src, void * dst, int n, int k, int64_t * hist) {\n    assert(k % QK_K == 0);\n    (void)hist; // TODO: collect histograms\n\n    for (int j = 0; j < n; j += k) {\n        block_q6_K * restrict y = (block_q6_K *)dst + j/QK_K;\n        quantize_row_q6_K_reference(src + j, y, k);\n    }\n    return (n/QK_K*sizeof(block_q6_K));\n}\n\n//===================================== Q8_K ==============================================\n\nvoid quantize_row_q8_K_reference(const float * restrict x, block_q8_K * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    for (int i = 0; i < nb; i++) {\n\n        float max = 0;\n        float amax = 0;\n        for (int j = 0; j < QK_K; ++j) {\n            float ax = fabsf(x[j]);\n            if (ax > amax) {\n                amax = ax; max = x[j];\n            }\n        }\n        if (!amax) {\n            y[i].d = 0;\n            memset(y[i].qs, 0, QK_K);\n            x += QK_K;\n            continue;\n        }\n        const float iscale = -128.f/max;\n        for (int j = 0; j < QK_K; ++j) {\n            int v = nearest_int(iscale*x[j]);\n            y[i].qs[j] = MIN(127, v);\n        }\n        for (int j = 0; j < QK_K/16; ++j) {\n            int sum = 0;\n            for (int ii = 0; ii < 16; ++ii) {\n                sum += y[i].qs[j*16 + ii];\n            }\n            y[i].bsums[j] = sum;\n        }\n        y[i].d = 1/iscale;\n        x += QK_K;\n    }\n}\n\nvoid dequantize_row_q8_K(const block_q8_K * restrict x, float * restrict y, int k) {\n    assert(k % QK_K == 0);\n    const int nb = k / QK_K;\n\n    for (int i = 0; i < nb; i++) {\n        for (int j = 0; j < QK_K; ++j) {\n            *y++ = x[i].d * x[i].qs[j];\n        }\n    }\n}\n\nvoid quantize_row_q8_K(const float * restrict x, void * restrict y, int k) {\n    quantize_row_q8_K_reference(x, y, k);\n}\n\n//===================================== Dot ptoducts =================================\n\n//\n// Helper functions\n//\n#if __AVX__ || __AVX2__ || __AVX512F__\n\n// shuffles to pick the required scales in dot products\nstatic inline __m256i get_scale_shuffle_q3k(int i) {\n    static const uint8_t k_shuffle[128] = {\n         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,     2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,\n         4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5,     6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7,\n         8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9,    10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,\n        12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,    14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,\n    };\n    return _mm256_loadu_si256((const __m256i*)k_shuffle + i);\n}\nstatic inline __m256i get_scale_shuffle_k4(int i) {\n    static const uint8_t k_shuffle[256] = {\n         0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1,\n         2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3, 2, 3,\n         4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5, 4, 5,\n         6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7, 6, 7,\n         8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9, 8, 9,\n        10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,10,11,\n        12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,12,13,\n        14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15,14,15\n    };\n    return _mm256_loadu_si256((const __m256i*)k_shuffle + i);\n}\nstatic inline __m128i get_scale_shuffle(int i) {\n    static const uint8_t k_shuffle[128] = {\n         0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n         2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3,\n         4, 4, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 5, 5,\n         6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 7, 7,\n         8, 8, 8, 8, 8, 8, 8, 8, 9, 9, 9, 9, 9, 9, 9, 9,\n        10,10,10,10,10,10,10,10, 11,11,11,11,11,11,11,11,\n        12,12,12,12,12,12,12,12, 13,13,13,13,13,13,13,13,\n        14,14,14,14,14,14,14,14, 15,15,15,15,15,15,15,15\n    };\n    return _mm_loadu_si128((const __m128i*)k_shuffle + i);\n}\n#endif\n\nvoid ggml_axpy_q4_0_q8_0(const int n, const void * restrict vx, const void * restrict vy, const void * restrict vz, int8_t alpha, ggml_fp16_t scale) {\n    const int qk = QK8_0;\n    const int nb = n / qk;\n    assert(n % qk == 0);\n    assert(nb % 2 == 0);\n\n    const block_q4_0 * restrict x = vx;\n#if defined(__AVX2__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n\n    __m256i alpha_v = _mm256_set1_epi16((short)alpha);\n    // Main loop\n    for (int i = 0; i < nb; ++i) {\n        /* Compute combined scale for the block */\n        const __m256 d = _mm256_set1_ps( GGML_FP16_TO_FP32(x[i].d) * GGML_FP16_TO_FP32(scale) );\n        __m256i bx = bytes_from_nibbles_32(x[i].qs);\n\n        // Now we have a vector with bytes in [ 0 .. 15 ] interval. Offset them into [ -8 .. +7 ] interval.\n        const __m256i off = _mm256_set1_epi8( 8 );\n        bx = _mm256_sub_epi8( bx, off );\n        //16ä¸ªæ•°è®¡ç®—\n        __m128i m_a = _mm256_extracti128_si256(bx, 0);\n        __m256i m_x = _mm256_cvtepi8_epi16(m_a); //16 elements\n        m_x = _mm256_mullo_epi16(m_x, alpha_v);\n        __m128i x_0 = _mm256_extracti128_si256(m_x, 0);\n        __m256i x0_32 = _mm256_cvtepi16_epi32(x_0);\n        __m256 fx0 = _mm256_cvtepi32_ps(x0_32);\n        fx0 = _mm256_mul_ps(fx0, d);\n\n\n        __m256 by = _mm256_loadu_ps((const __m256 *)((char *)vy+i*128));\n\n        by = _mm256_add_ps(by, fx0);\n        _mm256_storeu_ps((__m256*)((char*)vz + i*128), by);\n        //second phase\n\n        x_0 = _mm256_extracti128_si256(m_x, 1);\n        x0_32 = _mm256_cvtepi16_epi32(x_0);\n        fx0 = _mm256_cvtepi32_ps(x0_32);\n        fx0 = _mm256_mul_ps(fx0, d);\n        by = _mm256_loadu_ps((const __m256 *)((char*)vy+i*128+32));\n        by = _mm256_add_ps(by, fx0);\n        _mm256_storeu_ps((__m256*)((char*)vz + i*128+32), by);\n\n        //third phase\n        m_a = _mm256_extracti128_si256(bx, 1);\n        m_x = _mm256_cvtepi8_epi16(m_a);\n        m_x = _mm256_mullo_epi16(m_x, alpha_v);\n        x_0 = _mm256_extracti128_si256(m_x, 0);\n        x0_32 = _mm256_cvtepi16_epi32(x_0);\n        fx0 = _mm256_cvtepi32_ps(x0_32);\n        fx0 = _mm256_mul_ps(fx0, d);\n        by = _mm256_loadu_ps((const __m256 *)((char*)vy+i*128+64));\n\n        by = _mm256_add_ps(by, fx0);\n        _mm256_storeu_ps((__m256*)((char*)vz + i*128+64), by);\n\n        //fourth phase\n        x_0 = _mm256_extracti128_si256(m_x, 1);\n        x0_32 = _mm256_cvtepi16_epi32(x_0);\n        fx0 = _mm256_cvtepi32_ps(x0_32);\n        fx0 = _mm256_mul_ps(fx0, d);\n        by = _mm256_loadu_ps((const __m256 *)((char*)vy+i*128+96));\n        by = _mm256_add_ps(by, fx0);\n        _mm256_storeu_ps((__m256*)((char*)vz + i*128+96), by);\n\n    }\n#else\n    float *res = (float *)vz;\n    float scale_fp32 = GGML_FP16_TO_FP32(scale);\n    for (int i = 0; i < nb; i++) {\n        float result_scale = GGML_FP16_TO_FP32(x[i].d) * scale_fp32;\n        int offset = i * QK4_0;\n\n        for (int j = 0; j < qk/2; ++j) {\n            const int v0 = (x[i].qs[j] & 0x0F) - 8;\n            const int v1 = (x[i].qs[j] >>   4) - 8;\n            res[offset + j] = res[offset + j] + ((float)(v0 * (int)alpha) * result_scale);\n            res[offset + j + qk/2] = res[offset + j + qk/2] + ((float)(v1 * (int)alpha) * result_scale);\n        }\n    }\n#endif\n}\n\n\nvoid ggml_vec_dot_q4_0_q8_0(int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    const int qk = QK8_0;\n    const int nb = n / qk;\n\n    assert(n % qk == 0);\n\n    const block_q4_0 * restrict x = vx;\n    const block_q8_0 * restrict y = vy;\n\n#if defined(__ARM_NEON)\n    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n\n    assert(nb % 2 == 0); // TODO: handle odd nb\n\n    for (int i = 0; i < nb; i += 2) {\n        const block_q4_0 * restrict x0 = &x[i + 0];\n        const block_q4_0 * restrict x1 = &x[i + 1];\n        const block_q8_0 * restrict y0 = &y[i + 0];\n        const block_q8_0 * restrict y1 = &y[i + 1];\n\n        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n        const int8x16_t  s8b = vdupq_n_s8(0x8);\n\n        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n\n        // 4-bit -> 8-bit\n        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n\n        // sub 8\n        const int8x16_t v0_0ls = vsubq_s8(v0_0l, s8b);\n        const int8x16_t v0_0hs = vsubq_s8(v0_0h, s8b);\n        const int8x16_t v0_1ls = vsubq_s8(v0_1l, s8b);\n        const int8x16_t v0_1hs = vsubq_s8(v0_1h, s8b);\n\n        // load y\n        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n\n#if defined(__ARM_FEATURE_DOTPROD)\n        // dot product into int32x4_t\n        const int32x4_t p_0 = vdotq_s32(vdotq_s32(vdupq_n_s32(0), v0_0ls, v1_0l), v0_0hs, v1_0h);\n        const int32x4_t p_1 = vdotq_s32(vdotq_s32(vdupq_n_s32(0), v0_1ls, v1_1l), v0_1hs, v1_1h);\n\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(p_0), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(p_1), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n#else\n        const int16x8_t pl0l = vmull_s8(vget_low_s8 (v0_0ls), vget_low_s8 (v1_0l));\n        const int16x8_t pl0h = vmull_s8(vget_high_s8(v0_0ls), vget_high_s8(v1_0l));\n        const int16x8_t ph0l = vmull_s8(vget_low_s8 (v0_0hs), vget_low_s8 (v1_0h));\n        const int16x8_t ph0h = vmull_s8(vget_high_s8(v0_0hs), vget_high_s8(v1_0h));\n\n        const int16x8_t pl1l = vmull_s8(vget_low_s8 (v0_1ls), vget_low_s8 (v1_1l));\n        const int16x8_t pl1h = vmull_s8(vget_high_s8(v0_1ls), vget_high_s8(v1_1l));\n        const int16x8_t ph1l = vmull_s8(vget_low_s8 (v0_1hs), vget_low_s8 (v1_1h));\n        const int16x8_t ph1h = vmull_s8(vget_high_s8(v0_1hs), vget_high_s8(v1_1h));\n\n        const int32x4_t pl0 = vaddq_s32(vpaddlq_s16(pl0l), vpaddlq_s16(pl0h));\n        const int32x4_t ph0 = vaddq_s32(vpaddlq_s16(ph0l), vpaddlq_s16(ph0h));\n        const int32x4_t pl1 = vaddq_s32(vpaddlq_s16(pl1l), vpaddlq_s16(pl1h));\n        const int32x4_t ph1 = vaddq_s32(vpaddlq_s16(ph1l), vpaddlq_s16(ph1h));\n\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(pl0, ph0)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(pl1, ph1)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n#endif\n    }\n\n    *s = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n#elif defined(__AVX2__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n\n    // Main loop\n    for (int i = 0; i < nb; ++i) {\n        /* Compute combined scale for the block */\n        const __m256 d = _mm256_set1_ps( GGML_FP16_TO_FP32(x[i].d) * GGML_FP16_TO_FP32(y[i].d) );\n\n        __m256i bx = bytes_from_nibbles_32(x[i].qs);\n\n        // Now we have a vector with bytes in [ 0 .. 15 ] interval. Offset them into [ -8 .. +7 ] interval.\n        const __m256i off = _mm256_set1_epi8( 8 );\n        bx = _mm256_sub_epi8( bx, off );\n\n        __m256i by = _mm256_loadu_si256((const __m256i *)y[i].qs);\n\n        const __m256 q = mul_sum_i8_pairs_float(bx, by);\n\n        /* Multiply q with scale and accumulate */\n        acc = _mm256_fmadd_ps( d, q, acc );\n    }\n\n    *s = hsum_float_8(acc);\n#elif defined(__AVX__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n\n    // Main loop\n    for (int i = 0; i < nb; ++i) {\n        // Compute combined scale for the block\n        const __m256 d = _mm256_set1_ps( GGML_FP16_TO_FP32(x[i].d) * GGML_FP16_TO_FP32(y[i].d) );\n\n        const __m128i lowMask = _mm_set1_epi8(0xF);\n        const __m128i off = _mm_set1_epi8(8);\n\n        const __m128i tmp = _mm_loadu_si128((const __m128i *)x[i].qs);\n\n        __m128i bx = _mm_and_si128(lowMask, tmp);\n        __m128i by = _mm_loadu_si128((const __m128i *)y[i].qs);\n        bx = _mm_sub_epi8(bx, off);\n        const __m128i i32_0 = mul_sum_i8_pairs(bx, by);\n\n        bx = _mm_and_si128(lowMask, _mm_srli_epi64(tmp, 4));\n        by = _mm_loadu_si128((const __m128i *)(y[i].qs + 16));\n        bx = _mm_sub_epi8(bx, off);\n        const __m128i i32_1 = mul_sum_i8_pairs(bx, by);\n\n        // Convert int32_t to float\n        __m256 p = _mm256_cvtepi32_ps(MM256_SET_M128I(i32_0, i32_1));\n\n        // Apply the scale, and accumulate\n        acc = _mm256_add_ps(_mm256_mul_ps( d, p ), acc);\n    }\n\n    *s = hsum_float_8(acc);\n#elif defined(__SSSE3__)\n    // set constants\n    const __m128i lowMask = _mm_set1_epi8(0xF);\n    const __m128i off = _mm_set1_epi8(8);\n\n    // Initialize accumulator with zeros\n    __m128 acc_0 = _mm_setzero_ps();\n    __m128 acc_1 = _mm_setzero_ps();\n    __m128 acc_2 = _mm_setzero_ps();\n    __m128 acc_3 = _mm_setzero_ps();\n\n    // First round without accumulation\n    {\n        _mm_prefetch(&x[0] + sizeof(block_q4_0), _MM_HINT_T0);\n        _mm_prefetch(&y[0] + sizeof(block_q8_0), _MM_HINT_T0);\n\n        // Compute combined scale for the block 0 and 1\n        const __m128 d_0_1 = _mm_set1_ps( GGML_FP16_TO_FP32(x[0].d) * GGML_FP16_TO_FP32(y[0].d) );\n\n        const __m128i tmp_0_1 = _mm_loadu_si128((const __m128i *)x[0].qs);\n\n        __m128i bx_0 = _mm_and_si128(lowMask, tmp_0_1);\n        __m128i by_0 = _mm_loadu_si128((const __m128i *)y[0].qs);\n        bx_0 = _mm_sub_epi8(bx_0, off);\n        const __m128i i32_0 = mul_sum_i8_pairs(bx_0, by_0);\n\n        __m128i bx_1 = _mm_and_si128(lowMask, _mm_srli_epi64(tmp_0_1, 4));\n        __m128i by_1 = _mm_loadu_si128((const __m128i *)(y[0].qs + 16));\n        bx_1 = _mm_sub_epi8(bx_1, off);\n        const __m128i i32_1 = mul_sum_i8_pairs(bx_1, by_1);\n\n        _mm_prefetch(&x[1] + sizeof(block_q4_0), _MM_HINT_T0);\n        _mm_prefetch(&y[1] + sizeof(block_q8_0), _MM_HINT_T0);\n\n        // Compute combined scale for the block 2 and 3\n        const __m128 d_2_3 = _mm_set1_ps( GGML_FP16_TO_FP32(x[1].d) * GGML_FP16_TO_FP32(y[1].d) );\n\n        const __m128i tmp_2_3 = _mm_loadu_si128((const __m128i *)x[1].qs);\n\n        __m128i bx_2 = _mm_and_si128(lowMask, tmp_2_3);\n        __m128i by_2 = _mm_loadu_si128((const __m128i *)y[1].qs);\n        bx_2 = _mm_sub_epi8(bx_2, off);\n        const __m128i i32_2 = mul_sum_i8_pairs(bx_2, by_2);\n\n        __m128i bx_3 = _mm_and_si128(lowMask, _mm_srli_epi64(tmp_2_3, 4));\n        __m128i by_3 = _mm_loadu_si128((const __m128i *)(y[1].qs + 16));\n        bx_3 = _mm_sub_epi8(bx_3, off);\n        const __m128i i32_3 = mul_sum_i8_pairs(bx_3, by_3);\n\n        // Convert int32_t to float\n        __m128 p0 = _mm_cvtepi32_ps(i32_0);\n        __m128 p1 = _mm_cvtepi32_ps(i32_1);\n        __m128 p2 = _mm_cvtepi32_ps(i32_2);\n        __m128 p3 = _mm_cvtepi32_ps(i32_3);\n\n        // Apply the scale\n        acc_0 = _mm_mul_ps( d_0_1, p0 );\n        acc_1 = _mm_mul_ps( d_0_1, p1 );\n        acc_2 = _mm_mul_ps( d_2_3, p2 );\n        acc_3 = _mm_mul_ps( d_2_3, p3 );\n    }\n\n    assert(nb % 2 == 0); // TODO: handle odd nb\n\n    // Main loop\n    for (int i = 2; i < nb; i+=2) {\n        _mm_prefetch(&x[i] + sizeof(block_q4_0), _MM_HINT_T0);\n        _mm_prefetch(&y[i] + sizeof(block_q8_0), _MM_HINT_T0);\n\n        // Compute combined scale for the block 0 and 1\n        const __m128 d_0_1 = _mm_set1_ps( GGML_FP16_TO_FP32(x[i].d) * GGML_FP16_TO_FP32(y[i].d) );\n\n        const __m128i tmp_0_1 = _mm_loadu_si128((const __m128i *)x[i].qs);\n\n        __m128i bx_0 = _mm_and_si128(lowMask, tmp_0_1);\n        __m128i by_0 = _mm_loadu_si128((const __m128i *)y[i].qs);\n        bx_0 = _mm_sub_epi8(bx_0, off);\n        const __m128i i32_0 = mul_sum_i8_pairs(bx_0, by_0);\n\n        __m128i bx_1 = _mm_and_si128(lowMask, _mm_srli_epi64(tmp_0_1, 4));\n        __m128i by_1 = _mm_loadu_si128((const __m128i *)(y[i].qs + 16));\n        bx_1 = _mm_sub_epi8(bx_1, off);\n        const __m128i i32_1 = mul_sum_i8_pairs(bx_1, by_1);\n\n        _mm_prefetch(&x[i] + 2 * sizeof(block_q4_0), _MM_HINT_T0);\n        _mm_prefetch(&y[i] + 2 * sizeof(block_q8_0), _MM_HINT_T0);\n\n        // Compute combined scale for the block 2 and 3\n        const __m128 d_2_3 = _mm_set1_ps( GGML_FP16_TO_FP32(x[i + 1].d) * GGML_FP16_TO_FP32(y[i + 1].d) );\n\n        const __m128i tmp_2_3 = _mm_loadu_si128((const __m128i *)x[i + 1].qs);\n\n        __m128i bx_2 = _mm_and_si128(lowMask, tmp_2_3);\n        __m128i by_2 = _mm_loadu_si128((const __m128i *)y[i + 1].qs);\n        bx_2 = _mm_sub_epi8(bx_2, off);\n        const __m128i i32_2 = mul_sum_i8_pairs(bx_2, by_2);\n\n        __m128i bx_3 = _mm_and_si128(lowMask, _mm_srli_epi64(tmp_2_3, 4));\n        __m128i by_3 = _mm_loadu_si128((const __m128i *)(y[i + 1].qs + 16));\n        bx_3 = _mm_sub_epi8(bx_3, off);\n        const __m128i i32_3 = mul_sum_i8_pairs(bx_3, by_3);\n\n        // Convert int32_t to float\n        __m128 p0 = _mm_cvtepi32_ps(i32_0);\n        __m128 p1 = _mm_cvtepi32_ps(i32_1);\n        __m128 p2 = _mm_cvtepi32_ps(i32_2);\n        __m128 p3 = _mm_cvtepi32_ps(i32_3);\n\n        // Apply the scale\n        __m128 p0_d = _mm_mul_ps( d_0_1, p0 );\n        __m128 p1_d = _mm_mul_ps( d_0_1, p1 );\n        __m128 p2_d = _mm_mul_ps( d_2_3, p2 );\n        __m128 p3_d = _mm_mul_ps( d_2_3, p3 );\n\n        // Acummulate\n        acc_0 = _mm_add_ps(p0_d, acc_0);\n        acc_1 = _mm_add_ps(p1_d, acc_1);\n        acc_2 = _mm_add_ps(p2_d, acc_2);\n        acc_3 = _mm_add_ps(p3_d, acc_3);\n    }\n\n    *s = hsum_float_4x4(acc_0, acc_1, acc_2, acc_3);\n#elif defined(__riscv_v_intrinsic)\n    float sumf = 0.0;\n\n    size_t vl = __riscv_vsetvl_e8m1(qk/2);\n\n    for (int i = 0; i < nb; i++) {\n        // load elements\n        vuint8mf2_t tx = __riscv_vle8_v_u8mf2(x[i].qs, vl);\n\n        vint8mf2_t y0 = __riscv_vle8_v_i8mf2(y[i].qs, vl);\n        vint8mf2_t y1 = __riscv_vle8_v_i8mf2(y[i].qs+16, vl);\n\n        // mask and store lower part of x, and then upper part\n        vuint8mf2_t x_a = __riscv_vand_vx_u8mf2(tx, 0x0F, vl);\n        vuint8mf2_t x_l = __riscv_vsrl_vx_u8mf2(tx, 0x04, vl);\n\n        vint8mf2_t x_ai = __riscv_vreinterpret_v_u8mf2_i8mf2(x_a);\n        vint8mf2_t x_li = __riscv_vreinterpret_v_u8mf2_i8mf2(x_l);\n\n        // subtract offset\n        vint8mf2_t v0 = __riscv_vsub_vx_i8mf2(x_ai, 8, vl);\n        vint8mf2_t v1 = __riscv_vsub_vx_i8mf2(x_li, 8, vl);\n\n        vint16m1_t vec_mul1 = __riscv_vwmul_vv_i16m1(v0, y0, vl);\n        vint16m1_t vec_mul2 = __riscv_vwmul_vv_i16m1(v1, y1, vl);\n\n        vint32m1_t vec_zero = __riscv_vmv_v_x_i32m1(0, vl);\n\n        vint32m1_t vs1 = __riscv_vwredsum_vs_i16m1_i32m1(vec_mul1, vec_zero, vl);\n        vint32m1_t vs2 = __riscv_vwredsum_vs_i16m1_i32m1(vec_mul2, vs1, vl);\n\n        int sumi = __riscv_vmv_x_s_i32m1_i32(vs2);\n\n        sumf += sumi*GGML_FP16_TO_FP32(x[i].d)*GGML_FP16_TO_FP32(y[i].d);\n    }\n\n    *s = sumf;\n#else\n    // scalar\n    float sumf = 0.0;\n\n    for (int i = 0; i < nb; i++) {\n        int sumi = 0;\n\n        for (int j = 0; j < qk/2; ++j) {\n            const int v0 = (x[i].qs[j] & 0x0F) - 8;\n            const int v1 = (x[i].qs[j] >>   4) - 8;\n\n            sumi += (v0 * y[i].qs[j]) + (v1 * y[i].qs[j + qk/2]);\n        }\n\n        sumf += sumi*GGML_FP16_TO_FP32(x[i].d)*GGML_FP16_TO_FP32(y[i].d);\n    }\n\n    *s = sumf;\n#endif\n}\n\nvoid ggml_vec_dot_q4_1_q8_1(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    const int qk = QK8_1;\n    const int nb = n / qk;\n\n    assert(n % qk == 0);\n\n    const block_q4_1 * restrict x = vx;\n    const block_q8_1 * restrict y = vy;\n\n    // TODO: add WASM SIMD\n#if defined(__ARM_NEON)\n    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n\n    float summs = 0;\n\n    assert(nb % 2 == 0); // TODO: handle odd nb\n\n    for (int i = 0; i < nb; i += 2) {\n        const block_q4_1 * restrict x0 = &x[i + 0];\n        const block_q4_1 * restrict x1 = &x[i + 1];\n        const block_q8_1 * restrict y0 = &y[i + 0];\n        const block_q8_1 * restrict y1 = &y[i + 1];\n\n        summs += GGML_FP16_TO_FP32(x0->m) * y0->s + GGML_FP16_TO_FP32(x1->m) * y1->s;\n\n        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n\n        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n\n        // 4-bit -> 8-bit\n        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n\n        // load y\n        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n\n#if defined(__ARM_FEATURE_DOTPROD)\n        // dot product into int32x4_t\n        const int32x4_t p_0 = vdotq_s32(vdotq_s32(vdupq_n_s32(0), v0_0l, v1_0l), v0_0h, v1_0h);\n        const int32x4_t p_1 = vdotq_s32(vdotq_s32(vdupq_n_s32(0), v0_1l, v1_1l), v0_1h, v1_1h);\n\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(p_0), GGML_FP16_TO_FP32(x0->d)*y0->d);\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(p_1), GGML_FP16_TO_FP32(x1->d)*y1->d);\n#else\n        const int16x8_t pl0l = vmull_s8(vget_low_s8 (v0_0l), vget_low_s8 (v1_0l));\n        const int16x8_t pl0h = vmull_s8(vget_high_s8(v0_0l), vget_high_s8(v1_0l));\n        const int16x8_t ph0l = vmull_s8(vget_low_s8 (v0_0h), vget_low_s8 (v1_0h));\n        const int16x8_t ph0h = vmull_s8(vget_high_s8(v0_0h), vget_high_s8(v1_0h));\n\n        const int16x8_t pl1l = vmull_s8(vget_low_s8 (v0_1l), vget_low_s8 (v1_1l));\n        const int16x8_t pl1h = vmull_s8(vget_high_s8(v0_1l), vget_high_s8(v1_1l));\n        const int16x8_t ph1l = vmull_s8(vget_low_s8 (v0_1h), vget_low_s8 (v1_1h));\n        const int16x8_t ph1h = vmull_s8(vget_high_s8(v0_1h), vget_high_s8(v1_1h));\n\n        const int32x4_t pl0 = vaddq_s32(vpaddlq_s16(pl0l), vpaddlq_s16(pl0h));\n        const int32x4_t ph0 = vaddq_s32(vpaddlq_s16(ph0l), vpaddlq_s16(ph0h));\n        const int32x4_t pl1 = vaddq_s32(vpaddlq_s16(pl1l), vpaddlq_s16(pl1h));\n        const int32x4_t ph1 = vaddq_s32(vpaddlq_s16(ph1l), vpaddlq_s16(ph1h));\n\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(pl0, ph0)), GGML_FP16_TO_FP32(x0->d)*y0->d);\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(pl1, ph1)), GGML_FP16_TO_FP32(x1->d)*y1->d);\n#endif\n    }\n\n    *s = vaddvq_f32(sumv0) + vaddvq_f32(sumv1) + summs;\n#elif defined(__AVX2__) || defined(__AVX__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n\n    float summs = 0;\n\n    // Main loop\n    for (int i = 0; i < nb; ++i) {\n        const float d0 = GGML_FP16_TO_FP32(x[i].d);\n        const float d1 = y[i].d;\n\n        summs += GGML_FP16_TO_FP32(x[i].m) * y[i].s;\n\n        const __m256 d0v = _mm256_set1_ps( d0 );\n        const __m256 d1v = _mm256_set1_ps( d1 );\n\n        // Compute combined scales\n        const __m256 d0d1 = _mm256_mul_ps( d0v, d1v );\n\n        // Load 16 bytes, and unpack 4 bit fields into bytes, making 32 bytes\n        const __m256i bx = bytes_from_nibbles_32(x[i].qs);\n        const __m256i by = _mm256_loadu_si256( (const __m256i *)y[i].qs );\n\n        const __m256 xy = mul_sum_us8_pairs_float(bx, by);\n\n        // Accumulate d0*d1*x*y\n#if defined(__AVX2__)\n        acc = _mm256_fmadd_ps( d0d1, xy, acc );\n#else\n        acc = _mm256_add_ps( _mm256_mul_ps( d0d1, xy ), acc );\n#endif\n    }\n\n    *s = hsum_float_8(acc) + summs;\n#elif defined(__riscv_v_intrinsic)\n    float sumf = 0.0;\n\n    size_t vl = __riscv_vsetvl_e8m1(qk/2);\n\n    for (int i = 0; i < nb; i++) {\n        // load elements\n        vuint8mf2_t tx = __riscv_vle8_v_u8mf2(x[i].qs, vl);\n\n        vint8mf2_t y0 = __riscv_vle8_v_i8mf2(y[i].qs, vl);\n        vint8mf2_t y1 = __riscv_vle8_v_i8mf2(y[i].qs+16, vl);\n\n        // mask and store lower part of x, and then upper part\n        vuint8mf2_t x_a = __riscv_vand_vx_u8mf2(tx, 0x0F, vl);\n        vuint8mf2_t x_l = __riscv_vsrl_vx_u8mf2(tx, 0x04, vl);\n\n        vint8mf2_t v0 = __riscv_vreinterpret_v_u8mf2_i8mf2(x_a);\n        vint8mf2_t v1 = __riscv_vreinterpret_v_u8mf2_i8mf2(x_l);\n\n        vint16m1_t vec_mul1 = __riscv_vwmul_vv_i16m1(v0, y0, vl);\n        vint16m1_t vec_mul2 = __riscv_vwmul_vv_i16m1(v1, y1, vl);\n\n        vint32m1_t vec_zero = __riscv_vmv_v_x_i32m1(0, vl);\n\n        vint32m1_t vs1 = __riscv_vwredsum_vs_i16m1_i32m1(vec_mul1, vec_zero, vl);\n        vint32m1_t vs2 = __riscv_vwredsum_vs_i16m1_i32m1(vec_mul2, vs1, vl);\n\n        int sumi = __riscv_vmv_x_s_i32m1_i32(vs2);\n\n        sumf += (GGML_FP16_TO_FP32(x[i].d)*y[i].d)*sumi + GGML_FP16_TO_FP32(x[i].m)*y[i].s;\n    }\n\n    *s = sumf;\n#else\n    // scalar\n    float sumf = 0.0;\n\n    for (int i = 0; i < nb; i++) {\n        int sumi = 0;\n\n        for (int j = 0; j < qk/2; ++j) {\n            const int v0 = (x[i].qs[j] & 0x0F);\n            const int v1 = (x[i].qs[j] >>   4);\n\n            sumi += (v0 * y[i].qs[j]) + (v1 * y[i].qs[j + qk/2]);\n        }\n\n        sumf += (GGML_FP16_TO_FP32(x[i].d)*y[i].d)*sumi + GGML_FP16_TO_FP32(x[i].m)*y[i].s;\n    }\n\n    *s = sumf;\n#endif\n}\n\nvoid ggml_vec_dot_q5_0_q8_0(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    const int qk = QK8_0;\n    const int nb = n / qk;\n\n    assert(n % qk == 0);\n    assert(qk == QK5_0);\n\n    const block_q5_0 * restrict x = vx;\n    const block_q8_0 * restrict y = vy;\n\n#if defined(__ARM_NEON)\n    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n\n    uint32_t qh0;\n    uint32_t qh1;\n\n    uint64_t tmp0[4];\n    uint64_t tmp1[4];\n\n    assert(nb % 2 == 0); // TODO: handle odd nb\n\n    for (int i = 0; i < nb; i += 2) {\n        const block_q5_0 * restrict x0 = &x[i];\n        const block_q5_0 * restrict x1 = &x[i + 1];\n        const block_q8_0 * restrict y0 = &y[i];\n        const block_q8_0 * restrict y1 = &y[i + 1];\n\n        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n\n        // extract the 5th bit via lookup table ((!b) << 4)\n        memcpy(&qh0, x0->qh, sizeof(qh0));\n        memcpy(&qh1, x1->qh, sizeof(qh1));\n\n        tmp0[0] = table_b2b_1[(qh0 >>  0) & 0xFF];\n        tmp0[1] = table_b2b_1[(qh0 >>  8) & 0xFF];\n        tmp0[2] = table_b2b_1[(qh0 >> 16) & 0xFF];\n        tmp0[3] = table_b2b_1[(qh0 >> 24)       ];\n\n        tmp1[0] = table_b2b_1[(qh1 >>  0) & 0xFF];\n        tmp1[1] = table_b2b_1[(qh1 >>  8) & 0xFF];\n        tmp1[2] = table_b2b_1[(qh1 >> 16) & 0xFF];\n        tmp1[3] = table_b2b_1[(qh1 >> 24)       ];\n\n        const int8x16_t qhl0 = vld1q_s8((const int8_t *)(tmp0 + 0));\n        const int8x16_t qhh0 = vld1q_s8((const int8_t *)(tmp0 + 2));\n        const int8x16_t qhl1 = vld1q_s8((const int8_t *)(tmp1 + 0));\n        const int8x16_t qhh1 = vld1q_s8((const int8_t *)(tmp1 + 2));\n\n        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n\n        // 4-bit -> 8-bit\n        int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n        int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n        int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n        int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n\n        // add high bit and sub 16 (equivalent to sub 0x10 when bit is zero)\n        const int8x16_t v0_0lf = vsubq_s8(v0_0l, qhl0);\n        const int8x16_t v0_0hf = vsubq_s8(v0_0h, qhh0);\n        const int8x16_t v0_1lf = vsubq_s8(v0_1l, qhl1);\n        const int8x16_t v0_1hf = vsubq_s8(v0_1h, qhh1);\n\n        // load y\n        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n\n#if defined(__ARM_FEATURE_DOTPROD)\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n                        vdotq_s32(vdupq_n_s32(0), v0_0lf, v1_0l),\n                        vdotq_s32(vdupq_n_s32(0), v0_0hf, v1_0h))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n                        vdotq_s32(vdupq_n_s32(0), v0_1lf, v1_1l),\n                        vdotq_s32(vdupq_n_s32(0), v0_1hf, v1_1h))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n#else\n        const int16x8_t pl0l = vmull_s8(vget_low_s8 (v0_0lf), vget_low_s8 (v1_0l));\n        const int16x8_t pl0h = vmull_s8(vget_high_s8(v0_0lf), vget_high_s8(v1_0l));\n        const int16x8_t ph0l = vmull_s8(vget_low_s8 (v0_0hf), vget_low_s8 (v1_0h));\n        const int16x8_t ph0h = vmull_s8(vget_high_s8(v0_0hf), vget_high_s8(v1_0h));\n\n        const int16x8_t pl1l = vmull_s8(vget_low_s8 (v0_1lf), vget_low_s8 (v1_1l));\n        const int16x8_t pl1h = vmull_s8(vget_high_s8(v0_1lf), vget_high_s8(v1_1l));\n        const int16x8_t ph1l = vmull_s8(vget_low_s8 (v0_1hf), vget_low_s8 (v1_1h));\n        const int16x8_t ph1h = vmull_s8(vget_high_s8(v0_1hf), vget_high_s8(v1_1h));\n\n        const int32x4_t pl0 = vaddq_s32(vpaddlq_s16(pl0l), vpaddlq_s16(pl0h));\n        const int32x4_t ph0 = vaddq_s32(vpaddlq_s16(ph0l), vpaddlq_s16(ph0h));\n        const int32x4_t pl1 = vaddq_s32(vpaddlq_s16(pl1l), vpaddlq_s16(pl1h));\n        const int32x4_t ph1 = vaddq_s32(vpaddlq_s16(ph1l), vpaddlq_s16(ph1h));\n\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(pl0, ph0)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(pl1, ph1)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n#endif\n    }\n\n    *s = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n#elif defined(__wasm_simd128__)\n    v128_t sumv = wasm_f32x4_splat(0.0f);\n\n    uint32_t qh;\n    uint64_t tmp[4];\n\n    // TODO: check if unrolling this is better\n    for (int i = 0; i < nb; ++i) {\n        const block_q5_0 * restrict x0 = &x[i];\n        const block_q8_0 * restrict y0 = &y[i];\n\n        const v128_t m4b  = wasm_i8x16_splat(0x0F);\n\n        // extract the 5th bit\n        memcpy(&qh, x0->qh, sizeof(qh));\n\n        tmp[0] = table_b2b_1[(qh >>  0) & 0xFF];\n        tmp[1] = table_b2b_1[(qh >>  8) & 0xFF];\n        tmp[2] = table_b2b_1[(qh >> 16) & 0xFF];\n        tmp[3] = table_b2b_1[(qh >> 24)       ];\n\n        const v128_t qhl = wasm_v128_load(tmp + 0);\n        const v128_t qhh = wasm_v128_load(tmp + 2);\n\n        const v128_t v0 = wasm_v128_load(x0->qs);\n\n        // 4-bit -> 8-bit\n        const v128_t v0l = wasm_v128_and (v0, m4b);\n        const v128_t v0h = wasm_u8x16_shr(v0, 4);\n\n        // add high bit and sub 16 (equivalent to sub 0x10 when bit is zero)\n        const v128_t v0lf = wasm_i8x16_sub(v0l, qhl);\n        const v128_t v0hf = wasm_i8x16_sub(v0h, qhh);\n\n        // load y\n        const v128_t v1l = wasm_v128_load(y0->qs);\n        const v128_t v1h = wasm_v128_load(y0->qs + 16);\n\n        // int8x16 -> int16x8\n        const v128_t v0lfl = wasm_i16x8_extend_low_i8x16 (v0lf);\n        const v128_t v0lfh = wasm_i16x8_extend_high_i8x16(v0lf);\n        const v128_t v0hfl = wasm_i16x8_extend_low_i8x16 (v0hf);\n        const v128_t v0hfh = wasm_i16x8_extend_high_i8x16(v0hf);\n\n        const v128_t v1ll = wasm_i16x8_extend_low_i8x16 (v1l);\n        const v128_t v1lh = wasm_i16x8_extend_high_i8x16(v1l);\n        const v128_t v1hl = wasm_i16x8_extend_low_i8x16 (v1h);\n        const v128_t v1hh = wasm_i16x8_extend_high_i8x16(v1h);\n\n        // dot product\n        sumv = wasm_f32x4_add(sumv, wasm_f32x4_mul(wasm_f32x4_convert_i32x4(\n                        wasm_i32x4_add(\n                            wasm_i32x4_add(wasm_i32x4_dot_i16x8(v0lfl, v1ll),\n                                           wasm_i32x4_dot_i16x8(v0lfh, v1lh)),\n                            wasm_i32x4_add(wasm_i32x4_dot_i16x8(v0hfl, v1hl),\n                                           wasm_i32x4_dot_i16x8(v0hfh, v1hh)))),\n                    wasm_f32x4_splat(GGML_FP16_TO_FP32(x0->d) * GGML_FP16_TO_FP32(y0->d))));\n    }\n\n    *s = wasm_f32x4_extract_lane(sumv, 0) + wasm_f32x4_extract_lane(sumv, 1) +\n         wasm_f32x4_extract_lane(sumv, 2) + wasm_f32x4_extract_lane(sumv, 3);\n#elif defined(__AVX2__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n\n    // Main loop\n    for (int i = 0; i < nb; i++) {\n        /* Compute combined scale for the block */\n        const __m256 d = _mm256_set1_ps(GGML_FP16_TO_FP32(x[i].d) * GGML_FP16_TO_FP32(y[i].d));\n\n        __m256i bx = bytes_from_nibbles_32(x[i].qs);\n        __m256i bxhi = bytes_from_bits_32(x[i].qh);\n        bxhi = _mm256_andnot_si256(bxhi, _mm256_set1_epi8((char)0xF0));\n        bx = _mm256_or_si256(bx, bxhi);\n\n        __m256i by = _mm256_loadu_si256((const __m256i *)y[i].qs);\n\n        const __m256 q = mul_sum_i8_pairs_float(bx, by);\n\n        /* Multiply q with scale and accumulate */\n        acc = _mm256_fmadd_ps(d, q, acc);\n    }\n\n    *s = hsum_float_8(acc);\n#elif defined(__AVX__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n    __m128i mask = _mm_set1_epi8((char)0xF0);\n\n    // Main loop\n    for (int i = 0; i < nb; i++) {\n        /* Compute combined scale for the block */\n        const __m256 d = _mm256_set1_ps(GGML_FP16_TO_FP32(x[i].d) * GGML_FP16_TO_FP32(y[i].d));\n\n        __m256i bx = bytes_from_nibbles_32(x[i].qs);\n        const __m256i bxhi = bytes_from_bits_32(x[i].qh);\n        __m128i bxhil = _mm256_castsi256_si128(bxhi);\n        __m128i bxhih = _mm256_extractf128_si256(bxhi, 1);\n        bxhil = _mm_andnot_si128(bxhil, mask);\n        bxhih = _mm_andnot_si128(bxhih, mask);\n        __m128i bxl = _mm256_castsi256_si128(bx);\n        __m128i bxh = _mm256_extractf128_si256(bx, 1);\n        bxl = _mm_or_si128(bxl, bxhil);\n        bxh = _mm_or_si128(bxh, bxhih);\n        bx = MM256_SET_M128I(bxh, bxl);\n\n        const __m256i by = _mm256_loadu_si256((const __m256i *)y[i].qs);\n\n        const __m256 q = mul_sum_i8_pairs_float(bx, by);\n\n        /* Multiply q with scale and accumulate */\n        acc = _mm256_add_ps(_mm256_mul_ps(d, q), acc);\n    }\n\n    *s = hsum_float_8(acc);\n#elif defined(__riscv_v_intrinsic)\n    float sumf = 0.0;\n\n    uint32_t qh;\n\n    size_t vl = __riscv_vsetvl_e8m1(qk/2);\n\n    // These tempory registers are for masking and shift operations\n    vuint32m2_t vt_1 = __riscv_vid_v_u32m2(vl);\n    vuint32m2_t vt_2 = __riscv_vsll_vv_u32m2(__riscv_vmv_v_x_u32m2(1, vl), vt_1, vl);\n\n    vuint32m2_t vt_3 = __riscv_vsll_vx_u32m2(vt_2, 16, vl);\n    vuint32m2_t vt_4 = __riscv_vadd_vx_u32m2(vt_1, 12, vl);\n\n    for (int i = 0; i < nb; i++) {\n        memcpy(&qh, x[i].qh, sizeof(uint32_t));\n\n        // ((qh & (1u << (j + 0 ))) >> (j + 0 )) << 4;\n        vuint32m2_t xha_0 = __riscv_vand_vx_u32m2(vt_2, qh, vl);\n        vuint32m2_t xhr_0 = __riscv_vsrl_vv_u32m2(xha_0, vt_1, vl);\n        vuint32m2_t xhl_0 = __riscv_vsll_vx_u32m2(xhr_0, 4, vl);\n\n        // ((qh & (1u << (j + 16))) >> (j + 12));\n        vuint32m2_t xha_1 = __riscv_vand_vx_u32m2(vt_3, qh, vl);\n        vuint32m2_t xhl_1 = __riscv_vsrl_vv_u32m2(xha_1, vt_4, vl);\n\n        // narrowing\n        vuint16m1_t xhc_0 = __riscv_vncvt_x_x_w_u16m1(xhl_0, vl);\n        vuint8mf2_t xh_0 = __riscv_vncvt_x_x_w_u8mf2(xhc_0, vl);\n\n        vuint16m1_t xhc_1 = __riscv_vncvt_x_x_w_u16m1(xhl_1, vl);\n        vuint8mf2_t xh_1 = __riscv_vncvt_x_x_w_u8mf2(xhc_1, vl);\n\n        // load\n        vuint8mf2_t tx = __riscv_vle8_v_u8mf2(x[i].qs, vl);\n\n        vint8mf2_t y0 = __riscv_vle8_v_i8mf2(y[i].qs, vl);\n        vint8mf2_t y1 = __riscv_vle8_v_i8mf2(y[i].qs+16, vl);\n\n        vuint8mf2_t x_at = __riscv_vand_vx_u8mf2(tx, 0x0F, vl);\n        vuint8mf2_t x_lt = __riscv_vsrl_vx_u8mf2(tx, 0x04, vl);\n\n        vuint8mf2_t x_a = __riscv_vor_vv_u8mf2(x_at, xh_0, vl);\n        vuint8mf2_t x_l = __riscv_vor_vv_u8mf2(x_lt, xh_1, vl);\n\n        vint8mf2_t x_ai = __riscv_vreinterpret_v_u8mf2_i8mf2(x_a);\n        vint8mf2_t x_li = __riscv_vreinterpret_v_u8mf2_i8mf2(x_l);\n\n        vint8mf2_t v0 = __riscv_vsub_vx_i8mf2(x_ai, 16, vl);\n        vint8mf2_t v1 = __riscv_vsub_vx_i8mf2(x_li, 16, vl);\n\n        vint16m1_t vec_mul1 = __riscv_vwmul_vv_i16m1(v0, y0, vl);\n        vint16m1_t vec_mul2 = __riscv_vwmul_vv_i16m1(v1, y1, vl);\n\n        vint32m1_t vec_zero = __riscv_vmv_v_x_i32m1(0, vl);\n\n        vint32m1_t vs1 = __riscv_vwredsum_vs_i16m1_i32m1(vec_mul1, vec_zero, vl);\n        vint32m1_t vs2 = __riscv_vwredsum_vs_i16m1_i32m1(vec_mul2, vs1, vl);\n\n        int sumi = __riscv_vmv_x_s_i32m1_i32(vs2);\n\n        sumf += (GGML_FP16_TO_FP32(x[i].d)*GGML_FP16_TO_FP32(y[i].d)) * sumi;\n    }\n\n    *s = sumf;\n#else\n    // scalar\n    float sumf = 0.0;\n\n    for (int i = 0; i < nb; i++) {\n        uint32_t qh;\n        memcpy(&qh, x[i].qh, sizeof(qh));\n\n        int sumi = 0;\n\n        for (int j = 0; j < qk/2; ++j) {\n            const uint8_t xh_0 = ((qh & (1u << (j + 0 ))) >> (j + 0 )) << 4;\n            const uint8_t xh_1 = ((qh & (1u << (j + 16))) >> (j + 12));\n\n            const int32_t x0 = ((x[i].qs[j] & 0x0F) | xh_0) - 16;\n            const int32_t x1 = ((x[i].qs[j] >>   4) | xh_1) - 16;\n\n            sumi += (x0 * y[i].qs[j]) + (x1 * y[i].qs[j + qk/2]);\n        }\n\n        sumf += (GGML_FP16_TO_FP32(x[i].d)*GGML_FP16_TO_FP32(y[i].d)) * sumi;\n    }\n\n    *s = sumf;\n#endif\n}\n\nvoid ggml_vec_dot_q5_1_q8_1(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    const int qk = QK8_1;\n    const int nb = n / qk;\n\n    assert(n % qk == 0);\n    assert(qk == QK5_1);\n\n    const block_q5_1 * restrict x = vx;\n    const block_q8_1 * restrict y = vy;\n\n#if defined(__ARM_NEON)\n    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n\n    float summs0 = 0.0f;\n    float summs1 = 0.0f;\n\n    uint32_t qh0;\n    uint32_t qh1;\n\n    uint64_t tmp0[4];\n    uint64_t tmp1[4];\n\n    assert(nb % 2 == 0); // TODO: handle odd nb\n\n    for (int i = 0; i < nb; i += 2) {\n        const block_q5_1 * restrict x0 = &x[i];\n        const block_q5_1 * restrict x1 = &x[i + 1];\n        const block_q8_1 * restrict y0 = &y[i];\n        const block_q8_1 * restrict y1 = &y[i + 1];\n\n        const uint8x16_t m4b = vdupq_n_u8(0x0F);\n\n        summs0 += GGML_FP16_TO_FP32(x0->m) * y0->s;\n        summs1 += GGML_FP16_TO_FP32(x1->m) * y1->s;\n\n        // extract the 5th bit via lookup table ((b) << 4)\n        memcpy(&qh0, x0->qh, sizeof(qh0));\n        memcpy(&qh1, x1->qh, sizeof(qh1));\n\n        tmp0[0] = table_b2b_0[(qh0 >>  0) & 0xFF];\n        tmp0[1] = table_b2b_0[(qh0 >>  8) & 0xFF];\n        tmp0[2] = table_b2b_0[(qh0 >> 16) & 0xFF];\n        tmp0[3] = table_b2b_0[(qh0 >> 24)       ];\n\n        tmp1[0] = table_b2b_0[(qh1 >>  0) & 0xFF];\n        tmp1[1] = table_b2b_0[(qh1 >>  8) & 0xFF];\n        tmp1[2] = table_b2b_0[(qh1 >> 16) & 0xFF];\n        tmp1[3] = table_b2b_0[(qh1 >> 24)       ];\n\n        const int8x16_t qhl0 = vld1q_s8((const int8_t *)(tmp0 + 0));\n        const int8x16_t qhh0 = vld1q_s8((const int8_t *)(tmp0 + 2));\n        const int8x16_t qhl1 = vld1q_s8((const int8_t *)(tmp1 + 0));\n        const int8x16_t qhh1 = vld1q_s8((const int8_t *)(tmp1 + 2));\n\n        const uint8x16_t v0_0 = vld1q_u8(x0->qs);\n        const uint8x16_t v0_1 = vld1q_u8(x1->qs);\n\n        // 4-bit -> 8-bit\n        const int8x16_t v0_0l = vreinterpretq_s8_u8(vandq_u8  (v0_0, m4b));\n        const int8x16_t v0_0h = vreinterpretq_s8_u8(vshrq_n_u8(v0_0, 4));\n        const int8x16_t v0_1l = vreinterpretq_s8_u8(vandq_u8  (v0_1, m4b));\n        const int8x16_t v0_1h = vreinterpretq_s8_u8(vshrq_n_u8(v0_1, 4));\n\n        // add high bit\n        const int8x16_t v0_0lf = vorrq_s8(v0_0l, qhl0);\n        const int8x16_t v0_0hf = vorrq_s8(v0_0h, qhh0);\n        const int8x16_t v0_1lf = vorrq_s8(v0_1l, qhl1);\n        const int8x16_t v0_1hf = vorrq_s8(v0_1h, qhh1);\n\n        // load y\n        const int8x16_t v1_0l = vld1q_s8(y0->qs);\n        const int8x16_t v1_0h = vld1q_s8(y0->qs + 16);\n        const int8x16_t v1_1l = vld1q_s8(y1->qs);\n        const int8x16_t v1_1h = vld1q_s8(y1->qs + 16);\n\n#if defined(__ARM_FEATURE_DOTPROD)\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n                        vdotq_s32(vdupq_n_s32(0), v0_0lf, v1_0l),\n                        vdotq_s32(vdupq_n_s32(0), v0_0hf, v1_0h))), GGML_FP16_TO_FP32(x0->d)*y0->d);\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n                        vdotq_s32(vdupq_n_s32(0), v0_1lf, v1_1l),\n                        vdotq_s32(vdupq_n_s32(0), v0_1hf, v1_1h))), GGML_FP16_TO_FP32(x1->d)*y1->d);\n#else\n        const int16x8_t pl0l = vmull_s8(vget_low_s8 (v0_0lf), vget_low_s8 (v1_0l));\n        const int16x8_t pl0h = vmull_s8(vget_high_s8(v0_0lf), vget_high_s8(v1_0l));\n        const int16x8_t ph0l = vmull_s8(vget_low_s8 (v0_0hf), vget_low_s8 (v1_0h));\n        const int16x8_t ph0h = vmull_s8(vget_high_s8(v0_0hf), vget_high_s8(v1_0h));\n\n        const int16x8_t pl1l = vmull_s8(vget_low_s8 (v0_1lf), vget_low_s8 (v1_1l));\n        const int16x8_t pl1h = vmull_s8(vget_high_s8(v0_1lf), vget_high_s8(v1_1l));\n        const int16x8_t ph1l = vmull_s8(vget_low_s8 (v0_1hf), vget_low_s8 (v1_1h));\n        const int16x8_t ph1h = vmull_s8(vget_high_s8(v0_1hf), vget_high_s8(v1_1h));\n\n        const int32x4_t pl0 = vaddq_s32(vpaddlq_s16(pl0l), vpaddlq_s16(pl0h));\n        const int32x4_t ph0 = vaddq_s32(vpaddlq_s16(ph0l), vpaddlq_s16(ph0h));\n        const int32x4_t pl1 = vaddq_s32(vpaddlq_s16(pl1l), vpaddlq_s16(pl1h));\n        const int32x4_t ph1 = vaddq_s32(vpaddlq_s16(ph1l), vpaddlq_s16(ph1h));\n\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(pl0, ph0)), GGML_FP16_TO_FP32(x0->d)*y0->d);\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(pl1, ph1)), GGML_FP16_TO_FP32(x1->d)*y1->d);\n#endif\n    }\n\n    *s = vaddvq_f32(sumv0) + vaddvq_f32(sumv1) + summs0 + summs1;\n#elif defined(__wasm_simd128__)\n    v128_t sumv = wasm_f32x4_splat(0.0f);\n\n    float summs = 0.0f;\n\n    uint32_t qh;\n    uint64_t tmp[4];\n\n    // TODO: check if unrolling this is better\n    for (int i = 0; i < nb; ++i) {\n        const block_q5_1 * restrict x0 = &x[i];\n        const block_q8_1 * restrict y0 = &y[i];\n\n        summs += GGML_FP16_TO_FP32(x0->m) * y0->s;\n\n        const v128_t m4b = wasm_i8x16_splat(0x0F);\n\n        // extract the 5th bit\n        memcpy(&qh, x0->qh, sizeof(qh));\n\n        tmp[0] = table_b2b_0[(qh >>  0) & 0xFF];\n        tmp[1] = table_b2b_0[(qh >>  8) & 0xFF];\n        tmp[2] = table_b2b_0[(qh >> 16) & 0xFF];\n        tmp[3] = table_b2b_0[(qh >> 24)       ];\n\n        const v128_t qhl = wasm_v128_load(tmp + 0);\n        const v128_t qhh = wasm_v128_load(tmp + 2);\n\n        const v128_t v0 = wasm_v128_load(x0->qs);\n\n        // 4-bit -> 8-bit\n        const v128_t v0l = wasm_v128_and (v0, m4b);\n        const v128_t v0h = wasm_u8x16_shr(v0, 4);\n\n        // add high bit\n        const v128_t v0lf = wasm_v128_or(v0l, qhl);\n        const v128_t v0hf = wasm_v128_or(v0h, qhh);\n\n        // load y\n        const v128_t v1l = wasm_v128_load(y0->qs);\n        const v128_t v1h = wasm_v128_load(y0->qs + 16);\n\n        // int8x16 -> int16x8\n        const v128_t v0lfl = wasm_i16x8_extend_low_i8x16 (v0lf);\n        const v128_t v0lfh = wasm_i16x8_extend_high_i8x16(v0lf);\n        const v128_t v0hfl = wasm_i16x8_extend_low_i8x16 (v0hf);\n        const v128_t v0hfh = wasm_i16x8_extend_high_i8x16(v0hf);\n\n        const v128_t v1ll = wasm_i16x8_extend_low_i8x16 (v1l);\n        const v128_t v1lh = wasm_i16x8_extend_high_i8x16(v1l);\n        const v128_t v1hl = wasm_i16x8_extend_low_i8x16 (v1h);\n        const v128_t v1hh = wasm_i16x8_extend_high_i8x16(v1h);\n\n        // dot product\n        sumv = wasm_f32x4_add(sumv,\n                wasm_f32x4_mul(wasm_f32x4_convert_i32x4(wasm_i32x4_add(\n                            wasm_i32x4_add(wasm_i32x4_dot_i16x8(v0lfl, v1ll),\n                                           wasm_i32x4_dot_i16x8(v0lfh, v1lh)),\n                            wasm_i32x4_add(wasm_i32x4_dot_i16x8(v0hfl, v1hl),\n                                           wasm_i32x4_dot_i16x8(v0hfh, v1hh)))),\n                    wasm_f32x4_splat(GGML_FP16_TO_FP32(x0->d) * y0->d)));\n    }\n\n    *s = wasm_f32x4_extract_lane(sumv, 0) + wasm_f32x4_extract_lane(sumv, 1) +\n         wasm_f32x4_extract_lane(sumv, 2) + wasm_f32x4_extract_lane(sumv, 3) + summs;\n#elif defined(__AVX2__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n\n    float summs = 0.0f;\n\n    // Main loop\n    for (int i = 0; i < nb; i++) {\n        const __m256 dx = _mm256_set1_ps(GGML_FP16_TO_FP32(x[i].d));\n\n        summs += GGML_FP16_TO_FP32(x[i].m) * y[i].s;\n\n        __m256i bx = bytes_from_nibbles_32(x[i].qs);\n        __m256i bxhi = bytes_from_bits_32(x[i].qh);\n        bxhi = _mm256_and_si256(bxhi, _mm256_set1_epi8(0x10));\n        bx = _mm256_or_si256(bx, bxhi);\n\n        const __m256 dy = _mm256_set1_ps(y[i].d);\n        const __m256i by = _mm256_loadu_si256((const __m256i *)y[i].qs);\n\n        const __m256 q = mul_sum_us8_pairs_float(bx, by);\n\n        acc = _mm256_fmadd_ps(q, _mm256_mul_ps(dx, dy), acc);\n    }\n\n    *s = hsum_float_8(acc) + summs;\n#elif defined(__AVX__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n    __m128i mask = _mm_set1_epi8(0x10);\n\n    float summs = 0.0f;\n\n    // Main loop\n    for (int i = 0; i < nb; i++) {\n        const __m256 dx = _mm256_set1_ps(GGML_FP16_TO_FP32(x[i].d));\n\n        summs += GGML_FP16_TO_FP32(x[i].m) * y[i].s;\n\n        __m256i bx = bytes_from_nibbles_32(x[i].qs);\n        const __m256i bxhi = bytes_from_bits_32(x[i].qh);\n        __m128i bxhil = _mm256_castsi256_si128(bxhi);\n        __m128i bxhih = _mm256_extractf128_si256(bxhi, 1);\n        bxhil = _mm_and_si128(bxhil, mask);\n        bxhih = _mm_and_si128(bxhih, mask);\n        __m128i bxl = _mm256_castsi256_si128(bx);\n        __m128i bxh = _mm256_extractf128_si256(bx, 1);\n        bxl = _mm_or_si128(bxl, bxhil);\n        bxh = _mm_or_si128(bxh, bxhih);\n        bx = MM256_SET_M128I(bxh, bxl);\n\n        const __m256 dy = _mm256_set1_ps(y[i].d);\n        const __m256i by = _mm256_loadu_si256((const __m256i *)y[i].qs);\n\n        const __m256 q = mul_sum_us8_pairs_float(bx, by);\n\n        acc = _mm256_add_ps(_mm256_mul_ps(q, _mm256_mul_ps(dx, dy)), acc);\n    }\n\n    *s = hsum_float_8(acc) + summs;\n#elif defined(__riscv_v_intrinsic)\n    float sumf = 0.0;\n\n    uint32_t qh;\n\n    size_t vl = __riscv_vsetvl_e8m1(qk/2);\n\n    // temporary registers for shift operations\n    vuint32m2_t vt_1 = __riscv_vid_v_u32m2(vl);\n    vuint32m2_t vt_2 = __riscv_vadd_vx_u32m2(vt_1, 12, vl);\n\n    for (int i = 0; i < nb; i++) {\n        memcpy(&qh, x[i].qh, sizeof(uint32_t));\n\n        // load qh\n        vuint32m2_t vqh = __riscv_vmv_v_x_u32m2(qh, vl);\n\n        // ((qh >> (j +  0)) << 4) & 0x10;\n        vuint32m2_t xhr_0 = __riscv_vsrl_vv_u32m2(vqh, vt_1, vl);\n        vuint32m2_t xhl_0 = __riscv_vsll_vx_u32m2(xhr_0, 4, vl);\n        vuint32m2_t xha_0 = __riscv_vand_vx_u32m2(xhl_0, 0x10, vl);\n\n        // ((qh >> (j + 12))     ) & 0x10;\n        vuint32m2_t xhr_1 = __riscv_vsrl_vv_u32m2(vqh, vt_2, vl);\n        vuint32m2_t xha_1 = __riscv_vand_vx_u32m2(xhr_1, 0x10, vl);\n\n        // narrowing\n        vuint16m1_t xhc_0 = __riscv_vncvt_x_x_w_u16m1(xha_0, vl);\n        vuint8mf2_t xh_0 = __riscv_vncvt_x_x_w_u8mf2(xhc_0, vl);\n\n        vuint16m1_t xhc_1 = __riscv_vncvt_x_x_w_u16m1(xha_1, vl);\n        vuint8mf2_t xh_1 = __riscv_vncvt_x_x_w_u8mf2(xhc_1, vl);\n\n        // load\n        vuint8mf2_t tx = __riscv_vle8_v_u8mf2(x[i].qs, vl);\n\n        vint8mf2_t y0 = __riscv_vle8_v_i8mf2(y[i].qs, vl);\n        vint8mf2_t y1 = __riscv_vle8_v_i8mf2(y[i].qs+16, vl);\n\n        vuint8mf2_t x_at = __riscv_vand_vx_u8mf2(tx, 0x0F, vl);\n        vuint8mf2_t x_lt = __riscv_vsrl_vx_u8mf2(tx, 0x04, vl);\n\n        vuint8mf2_t x_a = __riscv_vor_vv_u8mf2(x_at, xh_0, vl);\n        vuint8mf2_t x_l = __riscv_vor_vv_u8mf2(x_lt, xh_1, vl);\n\n        vint8mf2_t v0 = __riscv_vreinterpret_v_u8mf2_i8mf2(x_a);\n        vint8mf2_t v1 = __riscv_vreinterpret_v_u8mf2_i8mf2(x_l);\n\n        vint16m1_t vec_mul1 = __riscv_vwmul_vv_i16m1(v0, y0, vl);\n        vint16m1_t vec_mul2 = __riscv_vwmul_vv_i16m1(v1, y1, vl);\n\n        vint32m1_t vec_zero = __riscv_vmv_v_x_i32m1(0, vl);\n\n        vint32m1_t vs1 = __riscv_vwredsum_vs_i16m1_i32m1(vec_mul1, vec_zero, vl);\n        vint32m1_t vs2 = __riscv_vwredsum_vs_i16m1_i32m1(vec_mul2, vs1, vl);\n\n        int sumi = __riscv_vmv_x_s_i32m1_i32(vs2);\n\n        sumf += (GGML_FP16_TO_FP32(x[i].d)*y[i].d)*sumi + GGML_FP16_TO_FP32(x[i].m)*y[i].s;\n    }\n\n    *s = sumf;\n#else\n    // scalar\n    float sumf = 0.0;\n\n    for (int i = 0; i < nb; i++) {\n        uint32_t qh;\n        memcpy(&qh, x[i].qh, sizeof(qh));\n\n        int sumi = 0;\n\n        for (int j = 0; j < qk/2; ++j) {\n            const uint8_t xh_0 = ((qh >> (j +  0)) << 4) & 0x10;\n            const uint8_t xh_1 = ((qh >> (j + 12))     ) & 0x10;\n\n            const int32_t x0 = (x[i].qs[j] & 0xF) | xh_0;\n            const int32_t x1 = (x[i].qs[j] >>  4) | xh_1;\n\n            sumi += (x0 * y[i].qs[j]) + (x1 * y[i].qs[j + qk/2]);\n        }\n\n        sumf += (GGML_FP16_TO_FP32(x[i].d)*y[i].d)*sumi + GGML_FP16_TO_FP32(x[i].m)*y[i].s;\n    }\n\n    *s = sumf;\n#endif\n}\n\nvoid ggml_vec_dot_q8_0_q8_0(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    const int qk = QK8_0;\n    const int nb = n / qk;\n\n    assert(n % qk == 0);\n\n    const block_q8_0 * restrict x = vx;\n    const block_q8_0 * restrict y = vy;\n\n#if defined(__ARM_NEON)\n    float32x4_t sumv0 = vdupq_n_f32(0.0f);\n    float32x4_t sumv1 = vdupq_n_f32(0.0f);\n\n    assert(nb % 2 == 0); // TODO: handle odd nb\n\n    for (int i = 0; i < nb; i += 2) {\n        const block_q8_0 * restrict x0 = &x[i + 0];\n        const block_q8_0 * restrict x1 = &x[i + 1];\n        const block_q8_0 * restrict y0 = &y[i + 0];\n        const block_q8_0 * restrict y1 = &y[i + 1];\n\n        const int8x16_t x0_0 = vld1q_s8(x0->qs);\n        const int8x16_t x0_1 = vld1q_s8(x0->qs + 16);\n        const int8x16_t x1_0 = vld1q_s8(x1->qs);\n        const int8x16_t x1_1 = vld1q_s8(x1->qs + 16);\n\n        // load y\n        const int8x16_t y0_0 = vld1q_s8(y0->qs);\n        const int8x16_t y0_1 = vld1q_s8(y0->qs + 16);\n        const int8x16_t y1_0 = vld1q_s8(y1->qs);\n        const int8x16_t y1_1 = vld1q_s8(y1->qs + 16);\n\n#if defined(__ARM_FEATURE_DOTPROD)\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(\n                        vdotq_s32(vdupq_n_s32(0), x0_0, y0_0),\n                        vdotq_s32(vdupq_n_s32(0), x0_1, y0_1))), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(\n                        vdotq_s32(vdupq_n_s32(0), x1_0, y1_0),\n                        vdotq_s32(vdupq_n_s32(0), x1_1, y1_1))), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n\n#else\n        const int16x8_t p0_0 = vmull_s8(vget_low_s8 (x0_0), vget_low_s8 (y0_0));\n        const int16x8_t p0_1 = vmull_s8(vget_high_s8(x0_0), vget_high_s8(y0_0));\n        const int16x8_t p0_2 = vmull_s8(vget_low_s8 (x0_1), vget_low_s8 (y0_1));\n        const int16x8_t p0_3 = vmull_s8(vget_high_s8(x0_1), vget_high_s8(y0_1));\n\n        const int16x8_t p1_0 = vmull_s8(vget_low_s8 (x1_0), vget_low_s8 (y1_0));\n        const int16x8_t p1_1 = vmull_s8(vget_high_s8(x1_0), vget_high_s8(y1_0));\n        const int16x8_t p1_2 = vmull_s8(vget_low_s8 (x1_1), vget_low_s8 (y1_1));\n        const int16x8_t p1_3 = vmull_s8(vget_high_s8(x1_1), vget_high_s8(y1_1));\n\n        const int32x4_t p0 = vaddq_s32(vpaddlq_s16(p0_0), vpaddlq_s16(p0_1));\n        const int32x4_t p1 = vaddq_s32(vpaddlq_s16(p0_2), vpaddlq_s16(p0_3));\n        const int32x4_t p2 = vaddq_s32(vpaddlq_s16(p1_0), vpaddlq_s16(p1_1));\n        const int32x4_t p3 = vaddq_s32(vpaddlq_s16(p1_2), vpaddlq_s16(p1_3));\n\n        sumv0 = vmlaq_n_f32(sumv0, vcvtq_f32_s32(vaddq_s32(p0, p1)), GGML_FP16_TO_FP32(x0->d)*GGML_FP16_TO_FP32(y0->d));\n        sumv1 = vmlaq_n_f32(sumv1, vcvtq_f32_s32(vaddq_s32(p2, p3)), GGML_FP16_TO_FP32(x1->d)*GGML_FP16_TO_FP32(y1->d));\n#endif\n    }\n\n    *s = vaddvq_f32(sumv0) + vaddvq_f32(sumv1);\n#elif defined(__AVX2__) || defined(__AVX__)\n    // Initialize accumulator with zeros\n    __m256 acc = _mm256_setzero_ps();\n\n    // Main loop\n    for (int i = 0; i < nb; ++i) {\n        // Compute combined scale for the block\n        const __m256 d = _mm256_set1_ps(GGML_FP16_TO_FP32(x[i].d) * GGML_FP16_TO_FP32(y[i].d));\n        __m256i bx = _mm256_loadu_si256((const __m256i *)x[i].qs);\n        __m256i by = _mm256_loadu_si256((const __m256i *)y[i].qs);\n\n        const __m256 q = mul_sum_i8_pairs_float(bx, by);\n\n        // Multiply q with scale and accumulate\n#if defined(__AVX2__)\n        acc = _mm256_fmadd_ps( d, q, acc );\n#else\n        acc = _mm256_add_ps( _mm256_mul_ps( d, q ), acc );\n#endif\n    }\n\n    *s = hsum_float_8(acc);\n#elif defined(__riscv_v_intrinsic)\n    float sumf = 0.0;\n    size_t vl = __riscv_vsetvl_e8m1(qk);\n\n    for (int i = 0; i < nb; i++) {\n        // load elements\n        vint8m1_t bx = __riscv_vle8_v_i8m1(x[i].qs, vl);\n        vint8m1_t by = __riscv_vle8_v_i8m1(y[i].qs, vl);\n\n        vint16m2_t vw_mul = __riscv_vwmul_vv_i16m2(bx, by, vl);\n\n        vint32m1_t v_zero = __riscv_vmv_v_x_i32m1(0, vl);\n        vint32m1_t v_sum = __riscv_vwredsum_vs_i16m2_i32m1(vw_mul, v_zero, vl);\n\n        int sumi = __riscv_vmv_x_s_i32m1_i32(v_sum);\n\n        sumf += sumi*(GGML_FP16_TO_FP32(x[i].d)*GGML_FP16_TO_FP32(y[i].d));\n    }\n\n    *s = sumf;\n#else\n    // scalar\n    float sumf = 0.0;\n\n    for (int i = 0; i < nb; i++) {\n        int sumi = 0;\n\n        for (int j = 0; j < qk; j++) {\n            sumi += x[i].qs[j]*y[i].qs[j];\n        }\n\n        sumf += sumi*(GGML_FP16_TO_FP32(x[i].d)*GGML_FP16_TO_FP32(y[i].d));\n    }\n\n    *s = sumf;\n#endif\n}\n\n#if QK_K == 256\nvoid ggml_vec_dot_q2_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n\n    const block_q2_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n#ifdef __ARM_NEON\n\n    const uint8x16_t m3 = vdupq_n_u8(0x3);\n    const uint8x16_t m4 = vdupq_n_u8(0xF);\n#if defined(__ARM_FEATURE_DOTPROD)\n    const int32x4_t  vzero = vdupq_n_s32(0);\n#endif\n\n    ggml_int8x16x2_t q2bytes;\n    uint8_t aux[16];\n\n    float sum = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const uint8_t * restrict q2 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n        const uint8_t * restrict sc = x[i].scales;\n\n        const uint8x16_t mins_and_scales = vld1q_u8(sc);\n        const uint8x16_t scales = vandq_u8(mins_and_scales, m4);\n        vst1q_u8(aux, scales);\n\n        const uint8x16_t mins = vshrq_n_u8(mins_and_scales, 4);\n        const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\n        const ggml_int16x8x2_t mins16 = {vreinterpretq_s16_u16(vmovl_u8(vget_low_u8(mins))), vreinterpretq_s16_u16(vmovl_u8(vget_high_u8(mins)))};\n        const int32x4_t s0 = vaddq_s32(vmull_s16(vget_low_s16 (mins16.val[0]), vget_low_s16 (q8sums.val[0])),\n                                       vmull_s16(vget_high_s16(mins16.val[0]), vget_high_s16(q8sums.val[0])));\n        const int32x4_t s1 = vaddq_s32(vmull_s16(vget_low_s16 (mins16.val[1]), vget_low_s16 (q8sums.val[1])),\n                                       vmull_s16(vget_high_s16(mins16.val[1]), vget_high_s16(q8sums.val[1])));\n        sum += dmin * vaddvq_s32(vaddq_s32(s0, s1));\n\n        int isum = 0;\n        int is = 0;\n\n// We use this macro instead of a function call because for some reason\n// the code runs 2-3% slower, even if the function is declared inline\n#if defined(__ARM_FEATURE_DOTPROD)\n#define MULTIPLY_ACCUM_WITH_SCALE(index)\\\n        isum += vaddvq_s32(vdotq_s32(vzero, q2bytes.val[0], q8bytes.val[0])) * aux[is+(index)];\\\n        isum += vaddvq_s32(vdotq_s32(vzero, q2bytes.val[1], q8bytes.val[1])) * aux[is+1+(index)];\n#else\n#define MULTIPLY_ACCUM_WITH_SCALE(index)\\\n        {\\\n    const int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q2bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\\\n                                   vmull_s8(vget_high_s8(q2bytes.val[0]), vget_high_s8(q8bytes.val[0])));\\\n    const int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q2bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\\\n                                   vmull_s8(vget_high_s8(q2bytes.val[1]), vget_high_s8(q8bytes.val[1])));\\\n    isum += vaddvq_s16(p1) * aux[is+(index)] + vaddvq_s16(p2) * aux[is+1+(index)];\\\n        }\n#endif\n\n#define SHIFT_MULTIPLY_ACCUM_WITH_SCALE(shift, index)\\\n        q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\\\n        q2bytes.val[0] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits.val[0], (shift)), m3));\\\n        q2bytes.val[1] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits.val[1], (shift)), m3));\\\n        MULTIPLY_ACCUM_WITH_SCALE((index));\n\n\n        for (int j = 0; j < QK_K/128; ++j) {\n\n            const ggml_uint8x16x2_t q2bits = ggml_vld1q_u8_x2(q2); q2 += 32;\n\n            ggml_int8x16x2_t q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\n            q2bytes.val[0] = vreinterpretq_s8_u8(vandq_u8(q2bits.val[0], m3));\n            q2bytes.val[1] = vreinterpretq_s8_u8(vandq_u8(q2bits.val[1], m3));\n            MULTIPLY_ACCUM_WITH_SCALE(0);\n\n            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(2, 2);\n\n            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(4, 4);\n\n            SHIFT_MULTIPLY_ACCUM_WITH_SCALE(6, 6);\n\n            is += 8;\n        }\n        sum += d * isum;\n\n    }\n\n    *s = sum;\n\n#elif defined __AVX2__\n\n    const __m256i m3 = _mm256_set1_epi8(3);\n    const __m128i m4 = _mm_set1_epi8(0xF);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const uint8_t * restrict q2 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const __m128i mins_and_scales = _mm_loadu_si128((const __m128i*)x[i].scales);\n        const __m128i scales8 = _mm_and_si128(mins_and_scales, m4);\n        const __m128i mins8 = _mm_and_si128(_mm_srli_epi16(mins_and_scales, 4), m4);\n        const __m256i mins = _mm256_cvtepi8_epi16(mins8);\n        const __m256i prod = _mm256_madd_epi16(mins, _mm256_loadu_si256((const __m256i*)y[i].bsums));\n\n        acc = _mm256_fmadd_ps(_mm256_broadcast_ss(&dmin), _mm256_cvtepi32_ps(prod), acc);\n\n        const __m256i all_scales = _mm256_cvtepi8_epi16(scales8);\n        const __m128i l_scales = _mm256_extracti128_si256(all_scales, 0);\n        const __m128i h_scales = _mm256_extracti128_si256(all_scales, 1);\n        const __m256i scales[2] = {MM256_SET_M128I(l_scales, l_scales), MM256_SET_M128I(h_scales, h_scales)};\n\n        __m256i sumi = _mm256_setzero_si256();\n\n        for (int j = 0; j < QK_K/128; ++j) {\n\n            const __m256i q2bits = _mm256_loadu_si256((const __m256i*)q2); q2 += 32;\n\n            const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_2 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_3 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n\n            const __m256i q2_0 = _mm256_and_si256(q2bits, m3);\n            const __m256i q2_1 = _mm256_and_si256(_mm256_srli_epi16(q2bits, 2), m3);\n            const __m256i q2_2 = _mm256_and_si256(_mm256_srli_epi16(q2bits, 4), m3);\n            const __m256i q2_3 = _mm256_and_si256(_mm256_srli_epi16(q2bits, 6), m3);\n\n            __m256i p0 = _mm256_maddubs_epi16(q2_0, q8_0);\n            __m256i p1 = _mm256_maddubs_epi16(q2_1, q8_1);\n            __m256i p2 = _mm256_maddubs_epi16(q2_2, q8_2);\n            __m256i p3 = _mm256_maddubs_epi16(q2_3, q8_3);\n\n            p0 = _mm256_madd_epi16(_mm256_shuffle_epi8(scales[j], get_scale_shuffle_q3k(0)), p0);\n            p1 = _mm256_madd_epi16(_mm256_shuffle_epi8(scales[j], get_scale_shuffle_q3k(1)), p1);\n            p2 = _mm256_madd_epi16(_mm256_shuffle_epi8(scales[j], get_scale_shuffle_q3k(2)), p2);\n            p3 = _mm256_madd_epi16(_mm256_shuffle_epi8(scales[j], get_scale_shuffle_q3k(3)), p3);\n\n            p0 = _mm256_add_epi32(p0, p1);\n            p2 = _mm256_add_epi32(p2, p3);\n\n            sumi = _mm256_add_epi32(sumi, _mm256_add_epi32(p0, p2));\n        }\n\n        acc = _mm256_fmadd_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(sumi), acc);\n\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __AVX__\n\n    const __m128i m3 = _mm_set1_epi8(0x3);\n    const __m128i m4 = _mm_set1_epi8(0xF);\n    const __m128i m2 = _mm_set1_epi8(0x2);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float dall = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const uint8_t * restrict q2 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        // load mins and scales from block_q2_K.scales[QK_K/16]\n        const __m128i mins_and_scales = _mm_loadu_si128((const __m128i*)x[i].scales);\n        const __m128i scales16 = _mm_and_si128(mins_and_scales, m4);\n        const __m128i mins16 = _mm_and_si128(_mm_srli_epi16(mins_and_scales, 4), m4);\n        const __m128i mins_0 = _mm_cvtepi8_epi16(mins16);\n        const __m128i mins_1 = _mm_cvtepi8_epi16(_mm_unpackhi_epi64(mins16, mins16));\n\n        // summs = y[i].bsums * (x[i].scales >> 4) in 16bits*8*2 to 32bits*4*2\n        const __m128i summs_0 = _mm_madd_epi16(mins_0, _mm_loadu_si128((const __m128i*)&y[i].bsums[0]));\n        const __m128i summs_1 = _mm_madd_epi16(mins_1, _mm_loadu_si128((const __m128i*)&y[i].bsums[8]));\n\n        // sumf += -dmin * summs in 32bits*8\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_broadcast_ss(&dmin), _mm256_cvtepi32_ps(MM256_SET_M128I(summs_1, summs_0))), acc);\n\n        const __m128i scales_0 = _mm_cvtepi8_epi16(scales16);\n        const __m128i scales_1 = _mm_cvtepi8_epi16(_mm_unpackhi_epi64(scales16, scales16));\n        const __m128i scales[2] = { scales_0, scales_1 };\n\n        __m128i sumi_0 = _mm_setzero_si128();\n        __m128i sumi_1 = _mm_setzero_si128();\n\n        for (int j = 0; j < QK_K/128; ++j) {\n\n            // load Q8 quants int8*16*8 from block_q8_K.qs[QK_K]\n            const __m128i q8_0 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_1 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_2 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_3 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_4 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_5 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_6 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_7 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n\n            // load 2bits*16*8 from block_q2_K.qs[QK_K/4]\n            __m128i q2bits = _mm_loadu_si128((const __m128i*)q2); q2 += 16;\n            const __m128i q2_0 = _mm_and_si128(q2bits, m3);\n            const __m128i q2_2 = _mm_and_si128(_mm_srli_epi16(q2bits, 2), m3);\n            const __m128i q2_4 = _mm_and_si128(_mm_srli_epi16(q2bits, 4), m3);\n            const __m128i q2_6 = _mm_and_si128(_mm_srli_epi16(q2bits, 6), m3);\n            q2bits = _mm_loadu_si128((const __m128i*)q2); q2 += 16;\n            const __m128i q2_1 = _mm_and_si128(q2bits, m3);\n            const __m128i q2_3 = _mm_and_si128(_mm_srli_epi16(q2bits, 2), m3);\n            const __m128i q2_5 = _mm_and_si128(_mm_srli_epi16(q2bits, 4), m3);\n            const __m128i q2_7 = _mm_and_si128(_mm_srli_epi16(q2bits, 6), m3);\n\n            // isuml = q8[l] * ((q2[l] >> shift) & 3) in 8bits*16*8 to 16bits*8*8\n            __m128i p0 = _mm_maddubs_epi16(q2_0, q8_0);\n            __m128i p1 = _mm_maddubs_epi16(q2_1, q8_1);\n            __m128i p2 = _mm_maddubs_epi16(q2_2, q8_2);\n            __m128i p3 = _mm_maddubs_epi16(q2_3, q8_3);\n            __m128i p4 = _mm_maddubs_epi16(q2_4, q8_4);\n            __m128i p5 = _mm_maddubs_epi16(q2_5, q8_5);\n            __m128i p6 = _mm_maddubs_epi16(q2_6, q8_6);\n            __m128i p7 = _mm_maddubs_epi16(q2_7, q8_7);\n\n            // isum += (x[i].scales[is++] & 0xF) * isuml in 16bits*8*8 to 32bits*4*8\n            __m128i shuffle = _mm_set1_epi16(0x0100);\n            p0 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p0);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p1 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p1);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p2 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p2);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p3 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p3);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p4 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p4);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p5 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p5);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p6 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p6);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p7 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p7);\n\n            p0 = _mm_add_epi32(p0, p1);\n            p2 = _mm_add_epi32(p2, p3);\n            p4 = _mm_add_epi32(p4, p5);\n            p6 = _mm_add_epi32(p6, p7);\n\n            // isum in 32bits*4*2\n            sumi_0 = _mm_add_epi32(sumi_0, _mm_add_epi32(p0, p2));\n            sumi_1 = _mm_add_epi32(sumi_1, _mm_add_epi32(p4, p6));\n        }\n\n        // sumf += dall * isum - dmin * summs in 32bits\n        __m256i sumi = MM256_SET_M128I(sumi_1, sumi_0);\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_broadcast_ss(&dall), _mm256_cvtepi32_ps(sumi)), acc);\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __riscv_v_intrinsic\n\n    float sumf = 0;\n    uint8_t temp_01[32] = {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n                            1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1};\n\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * q2 = x[i].qs;\n        const  int8_t * q8 = y[i].qs;\n        const uint8_t * sc = x[i].scales;\n\n        const float dall = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        size_t vl = 16;\n\n        vuint8m1_t scales = __riscv_vle8_v_u8m1(sc, vl);\n        vuint8m1_t aux = __riscv_vand_vx_u8m1(scales, 0x0F, vl);\n\n        vint16m1_t q8sums = __riscv_vle16_v_i16m1(y[i].bsums, vl);\n\n        vuint8mf2_t scales_2 = __riscv_vle8_v_u8mf2(sc, vl);\n        vuint8mf2_t mins8 = __riscv_vsrl_vx_u8mf2(scales_2, 0x4, vl);\n        vint16m1_t mins = __riscv_vreinterpret_v_u16m1_i16m1(__riscv_vzext_vf2_u16m1(mins8, vl));\n        vint32m2_t prod = __riscv_vwmul_vv_i32m2(q8sums, mins, vl);\n        vint32m1_t vsums = __riscv_vredsum_vs_i32m2_i32m1(prod, __riscv_vmv_v_x_i32m1(0, 1), vl);\n\n        sumf  += dmin * __riscv_vmv_x_s_i32m1_i32(vsums);\n\n        vl = 32;\n\n        vint32m1_t vzero = __riscv_vmv_v_x_i32m1(0, 1);\n        vuint8m1_t v_b = __riscv_vle8_v_u8m1(temp_01, vl);\n\n        uint8_t is=0;\n        int isum=0;\n\n        for (int j = 0; j < QK_K/128; ++j) {\n            // load Q2\n            vuint8m1_t q2_x = __riscv_vle8_v_u8m1(q2, vl);\n\n            vuint8m1_t q2_0 = __riscv_vand_vx_u8m1(q2_x, 0x03, vl);\n            vuint8m1_t q2_1 = __riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(q2_x, 0x2, vl), 0x03 , vl);\n            vuint8m1_t q2_2 = __riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(q2_x, 0x4, vl), 0x03 , vl);\n            vuint8m1_t q2_3 = __riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(q2_x, 0x6, vl), 0x03 , vl);\n\n            // duplicate scale elements for product\n            vuint8m1_t sc0 = __riscv_vrgather_vv_u8m1(aux, __riscv_vadd_vx_u8m1(v_b, 0+is, vl), vl);\n            vuint8m1_t sc1 = __riscv_vrgather_vv_u8m1(aux, __riscv_vadd_vx_u8m1(v_b, 2+is, vl), vl);\n            vuint8m1_t sc2 = __riscv_vrgather_vv_u8m1(aux, __riscv_vadd_vx_u8m1(v_b, 4+is, vl), vl);\n            vuint8m1_t sc3 = __riscv_vrgather_vv_u8m1(aux, __riscv_vadd_vx_u8m1(v_b, 6+is, vl), vl);\n\n            vint16m2_t p0 = __riscv_vreinterpret_v_u16m2_i16m2(__riscv_vwmulu_vv_u16m2(q2_0, sc0, vl));\n            vint16m2_t p1 = __riscv_vreinterpret_v_u16m2_i16m2(__riscv_vwmulu_vv_u16m2(q2_1, sc1, vl));\n            vint16m2_t p2 = __riscv_vreinterpret_v_u16m2_i16m2(__riscv_vwmulu_vv_u16m2(q2_2, sc2, vl));\n            vint16m2_t p3 = __riscv_vreinterpret_v_u16m2_i16m2(__riscv_vwmulu_vv_u16m2(q2_3, sc3, vl));\n\n            // load Q8\n            vint8m1_t q8_0 = __riscv_vle8_v_i8m1(q8, vl);\n            vint8m1_t q8_1 = __riscv_vle8_v_i8m1(q8+32, vl);\n            vint8m1_t q8_2 = __riscv_vle8_v_i8m1(q8+64, vl);\n            vint8m1_t q8_3 = __riscv_vle8_v_i8m1(q8+96, vl);\n\n            vint32m4_t s0 = __riscv_vwmul_vv_i32m4(p0, __riscv_vwcvt_x_x_v_i16m2(q8_0, vl), vl);\n            vint32m4_t s1 = __riscv_vwmul_vv_i32m4(p1, __riscv_vwcvt_x_x_v_i16m2(q8_1, vl), vl);\n            vint32m4_t s2 = __riscv_vwmul_vv_i32m4(p2, __riscv_vwcvt_x_x_v_i16m2(q8_2, vl), vl);\n            vint32m4_t s3 = __riscv_vwmul_vv_i32m4(p3, __riscv_vwcvt_x_x_v_i16m2(q8_3, vl), vl);\n\n            vint32m1_t isum0 = __riscv_vredsum_vs_i32m4_i32m1(__riscv_vadd_vv_i32m4(s0, s1, vl), vzero, vl);\n            vint32m1_t isum1 = __riscv_vredsum_vs_i32m4_i32m1(__riscv_vadd_vv_i32m4(s2, s3, vl), isum0, vl);\n\n            isum += __riscv_vmv_x_s_i32m1_i32(isum1);\n\n            q2+=32;  q8+=128;  is=8;\n\n        }\n\n        sumf += dall * isum;\n\n    }\n\n    *s = sumf;\n\n#else\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * q2 = x[i].qs;\n        const  int8_t * q8 = y[i].qs;\n        const uint8_t * sc = x[i].scales;\n\n        int summs = 0;\n        for (int j = 0; j < 16; ++j) {\n            summs += y[i].bsums[j] * (sc[j] >> 4);\n        }\n\n        const float dall = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        int isum = 0;\n        int is = 0;\n        int d;\n        for (int k = 0; k < QK_K/128; ++k) {\n            int shift = 0;\n            for (int j = 0; j < 4; ++j) {\n                d = sc[is++] & 0xF;\n                int isuml = 0;\n                for (int l =  0; l < 16; ++l) isuml += q8[l] * ((q2[l] >> shift) & 3);\n                isum += d * isuml;\n                d = sc[is++] & 0xF;\n                isuml = 0;\n                for (int l = 16; l < 32; ++l) isuml += q8[l] * ((q2[l] >> shift) & 3);\n                isum += d * isuml;\n                shift += 2;\n                q8 += 32;\n            }\n            q2 += 32;\n        }\n        sumf += dall * isum - dmin * summs;\n    }\n    *s = sumf;\n#endif\n}\n\n#else\n\nvoid ggml_vec_dot_q2_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n\n    const block_q2_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n#ifdef __ARM_NEON\n\n    const uint8x16_t m3 = vdupq_n_u8(0x3);\n#if defined(__ARM_FEATURE_DOTPROD)\n    const int32x4_t  vzero = vdupq_n_s32(0);\n#endif\n\n    ggml_int8x16x4_t q2bytes;\n\n    uint32_t aux32[2];\n    const uint8_t * scales = (const uint8_t *)aux32;\n\n    float sum = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * (float)x[i].d;\n        const float dmin = -y[i].d * (float)x[i].dmin;\n\n        const uint8_t * restrict q2 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n        const uint32_t * restrict sc = (const uint32_t *)x[i].scales;\n\n        aux32[0] = sc[0] & 0x0f0f0f0f;\n        aux32[1] = (sc[0] >> 4) & 0x0f0f0f0f;\n\n        sum += dmin * (scales[4] * y[i].bsums[0] + scales[5] * y[i].bsums[1] + scales[6] * y[i].bsums[2] + scales[7] * y[i].bsums[3]);\n\n        int isum1 = 0, isum2 = 0;\n\n        const uint8x16_t q2bits = vld1q_u8(q2);\n\n        const ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8);\n\n        q2bytes.val[0] = vreinterpretq_s8_u8(vandq_u8(q2bits, m3));\n        q2bytes.val[1] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits, 2), m3));\n        q2bytes.val[2] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits, 4), m3));\n        q2bytes.val[3] = vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q2bits, 6), m3));\n\n#if defined(__ARM_FEATURE_DOTPROD)\n        isum1 += vaddvq_s32(vdotq_s32(vzero, q2bytes.val[0], q8bytes.val[0])) * scales[0];\n        isum2 += vaddvq_s32(vdotq_s32(vzero, q2bytes.val[1], q8bytes.val[1])) * scales[1];\n        isum1 += vaddvq_s32(vdotq_s32(vzero, q2bytes.val[2], q8bytes.val[2])) * scales[2];\n        isum2 += vaddvq_s32(vdotq_s32(vzero, q2bytes.val[3], q8bytes.val[3])) * scales[3];\n#else\n        const int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q2bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                       vmull_s8(vget_high_s8(q2bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n        const int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q2bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                       vmull_s8(vget_high_s8(q2bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n        isum1 += vaddvq_s16(p1) * scales[0];\n        isum2 += vaddvq_s16(p2) * scales[1];\n\n        const int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q2bytes.val[2]), vget_low_s8 (q8bytes.val[2])),\n                                       vmull_s8(vget_high_s8(q2bytes.val[2]), vget_high_s8(q8bytes.val[2])));\n        const int16x8_t p4 = vaddq_s16(vmull_s8(vget_low_s8 (q2bytes.val[3]), vget_low_s8 (q8bytes.val[3])),\n                                       vmull_s8(vget_high_s8(q2bytes.val[3]), vget_high_s8(q8bytes.val[3])));\n        isum1 += vaddvq_s16(p3) * scales[2];\n        isum2 += vaddvq_s16(p4) * scales[3];\n#endif\n        sum += d * (isum1 + isum2);\n\n    }\n\n    *s = sum;\n\n#elif defined __AVX2__\n\n    const __m256i m3 = _mm256_set1_epi8(3);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    uint32_t ud, um;\n    const uint8_t * restrict db = (const uint8_t *)&ud;\n    const uint8_t * restrict mb = (const uint8_t *)&um;\n\n    float summs = 0;\n\n    // TODO: optimize this\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const uint8_t * restrict q2 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const uint32_t * restrict sc = (const uint32_t *)x[i].scales;\n        ud = (sc[0] >> 0) & 0x0f0f0f0f;\n        um = (sc[0] >> 4) & 0x0f0f0f0f;\n\n        int32_t smin = mb[0] * y[i].bsums[0] + mb[1] * y[i].bsums[1] + mb[2] * y[i].bsums[2] + mb[3] * y[i].bsums[3];\n        summs += dmin * smin;\n\n        const __m128i q2bits = _mm_loadu_si128((const __m128i*)q2);\n        const __m256i q2_0 = _mm256_and_si256(MM256_SET_M128I(_mm_srli_epi16(q2bits, 2), q2bits), m3);\n        const __m256i q2_1 = _mm256_and_si256(MM256_SET_M128I(_mm_srli_epi16(q2bits, 6), _mm_srli_epi16(q2bits, 4)), m3);\n\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        const __m256i p0 = _mm256_maddubs_epi16(q2_0, q8_0);\n        const __m256i p1 = _mm256_maddubs_epi16(q2_1, q8_1);\n\n        const __m256i p_0 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(p0, 0));\n        const __m256i p_1 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(p0, 1));\n        const __m256i p_2 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(p1, 0));\n        const __m256i p_3 = _mm256_cvtepi16_epi32(_mm256_extracti128_si256(p1, 1));\n\n        acc = _mm256_fmadd_ps(_mm256_set1_ps(d * db[0]), _mm256_cvtepi32_ps(p_0), acc);\n        acc = _mm256_fmadd_ps(_mm256_set1_ps(d * db[1]), _mm256_cvtepi32_ps(p_1), acc);\n        acc = _mm256_fmadd_ps(_mm256_set1_ps(d * db[2]), _mm256_cvtepi32_ps(p_2), acc);\n        acc = _mm256_fmadd_ps(_mm256_set1_ps(d * db[3]), _mm256_cvtepi32_ps(p_3), acc);\n    }\n\n    *s = hsum_float_8(acc) + summs;\n\n#elif defined __AVX__\n\n    const __m128i m3 = _mm_set1_epi8(3);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    uint32_t ud, um;\n    const uint8_t * restrict db = (const uint8_t *)&ud;\n    const uint8_t * restrict mb = (const uint8_t *)&um;\n\n    float summs = 0;\n\n    // TODO: optimize this\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const uint8_t * restrict q2 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const uint32_t * restrict sc = (const uint32_t *)x[i].scales;\n        ud = (sc[0] >> 0) & 0x0f0f0f0f;\n        um = (sc[0] >> 4) & 0x0f0f0f0f;\n\n        int32_t smin = mb[0] * y[i].bsums[0] + mb[1] * y[i].bsums[1] + mb[2] * y[i].bsums[2] + mb[3] * y[i].bsums[3];\n        summs += dmin * smin;\n\n        const __m128i q2bits = _mm_loadu_si128((const __m128i*)q2);\n        const __m128i q2_0 = _mm_and_si128(q2bits, m3);\n        const __m128i q2_1 = _mm_and_si128(_mm_srli_epi16(q2bits, 2), m3);\n        const __m128i q2_2 = _mm_and_si128(_mm_srli_epi16(q2bits, 4), m3);\n        const __m128i q2_3 = _mm_and_si128(_mm_srli_epi16(q2bits, 6), m3);\n\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        const __m128i p0 = _mm_maddubs_epi16(q2_0, _mm256_extractf128_si256(q8_0, 0));\n        const __m128i p1 = _mm_maddubs_epi16(q2_1, _mm256_extractf128_si256(q8_0, 1));\n        const __m128i p2 = _mm_maddubs_epi16(q2_2, _mm256_extractf128_si256(q8_1, 0));\n        const __m128i p3 = _mm_maddubs_epi16(q2_3, _mm256_extractf128_si256(q8_1, 1));\n\n        const __m256i p_0 = MM256_SET_M128I(_mm_cvtepi16_epi32(_mm_unpackhi_epi64(p0, p0)), _mm_cvtepi16_epi32(p0));\n        const __m256i p_1 = MM256_SET_M128I(_mm_cvtepi16_epi32(_mm_unpackhi_epi64(p1, p1)), _mm_cvtepi16_epi32(p1));\n        const __m256i p_2 = MM256_SET_M128I(_mm_cvtepi16_epi32(_mm_unpackhi_epi64(p2, p2)), _mm_cvtepi16_epi32(p2));\n        const __m256i p_3 = MM256_SET_M128I(_mm_cvtepi16_epi32(_mm_unpackhi_epi64(p3, p3)), _mm_cvtepi16_epi32(p3));\n\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_set1_ps(d * db[0]), _mm256_cvtepi32_ps(p_0)), acc);\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_set1_ps(d * db[1]), _mm256_cvtepi32_ps(p_1)), acc);\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_set1_ps(d * db[2]), _mm256_cvtepi32_ps(p_2)), acc);\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_set1_ps(d * db[3]), _mm256_cvtepi32_ps(p_3)), acc);\n    }\n\n    *s = hsum_float_8(acc) + summs;\n\n#elif defined __riscv_v_intrinsic\n\n    uint32_t aux32[2];\n    const uint8_t * scales = (const uint8_t *)aux32;\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * (float)x[i].d;\n        const float dmin = -y[i].d * (float)x[i].dmin;\n\n        const uint8_t * restrict q2 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n        const uint32_t * restrict sc = (const uint32_t *)x[i].scales;\n\n        aux32[0] = sc[0] & 0x0f0f0f0f;\n        aux32[1] = (sc[0] >> 4) & 0x0f0f0f0f;\n\n        sumf += dmin * (scales[4] * y[i].bsums[0] + scales[5] * y[i].bsums[1] + scales[6] * y[i].bsums[2] + scales[7] * y[i].bsums[3]);\n\n        int isum1 = 0;\n        int isum2 = 0;\n\n        size_t vl = 16;\n\n        vint16m1_t vzero = __riscv_vmv_v_x_i16m1(0, 1);\n\n        // load Q2\n        vuint8mf2_t q2_x = __riscv_vle8_v_u8mf2(q2, vl);\n\n        vint8mf2_t q2_0 = __riscv_vreinterpret_v_u8mf2_i8mf2(__riscv_vand_vx_u8mf2(q2_x, 0x03, vl));\n        vint8mf2_t q2_1 = __riscv_vreinterpret_v_u8mf2_i8mf2(__riscv_vand_vx_u8mf2(__riscv_vsrl_vx_u8mf2(q2_x, 0x2, vl), 0x03 , vl));\n        vint8mf2_t q2_2 = __riscv_vreinterpret_v_u8mf2_i8mf2(__riscv_vand_vx_u8mf2(__riscv_vsrl_vx_u8mf2(q2_x, 0x4, vl), 0x03 , vl));\n        vint8mf2_t q2_3 = __riscv_vreinterpret_v_u8mf2_i8mf2(__riscv_vand_vx_u8mf2(__riscv_vsrl_vx_u8mf2(q2_x, 0x6, vl), 0x03 , vl));\n\n        // load Q8, and take product with Q2\n        vint16m1_t p0 = __riscv_vwmul_vv_i16m1(q2_0, __riscv_vle8_v_i8mf2(q8, vl), vl);\n        vint16m1_t p1 = __riscv_vwmul_vv_i16m1(q2_1, __riscv_vle8_v_i8mf2(q8+16, vl), vl);\n        vint16m1_t p2 = __riscv_vwmul_vv_i16m1(q2_2, __riscv_vle8_v_i8mf2(q8+32, vl), vl);\n        vint16m1_t p3 = __riscv_vwmul_vv_i16m1(q2_3, __riscv_vle8_v_i8mf2(q8+48, vl), vl);\n\n        vint16m1_t vs_0 = __riscv_vredsum_vs_i16m1_i16m1(p0, vzero, vl);\n        vint16m1_t vs_1 = __riscv_vredsum_vs_i16m1_i16m1(p1, vzero, vl);\n        vint16m1_t vs_2 = __riscv_vredsum_vs_i16m1_i16m1(p2, vzero, vl);\n        vint16m1_t vs_3 = __riscv_vredsum_vs_i16m1_i16m1(p3, vzero, vl);\n\n        isum1 += __riscv_vmv_x_s_i16m1_i16(vs_0) * scales[0];\n        isum2 += __riscv_vmv_x_s_i16m1_i16(vs_1) * scales[1];\n        isum1 += __riscv_vmv_x_s_i16m1_i16(vs_2) * scales[2];\n        isum2 += __riscv_vmv_x_s_i16m1_i16(vs_3) * scales[3];\n\n        sumf += d * (isum1 + isum2);\n\n    }\n\n    *s = sumf;\n\n#else\n\n    float sumf = 0;\n\n    int isum[4];\n\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * q2 = x[i].qs;\n        const  int8_t * q8 = y[i].qs;\n        const uint8_t * sc = x[i].scales;\n\n        int summs = 0;\n        for (int j = 0; j < QK_K/16; ++j) {\n            summs += y[i].bsums[j] * (sc[j] >> 4);\n        }\n\n        const float dall = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        isum[0] = isum[1] = isum[2] = isum[3] = 0;\n        for (int l =  0; l < 16; ++l) {\n            isum[0] += q8[l+ 0] * ((q2[l] >> 0) & 3);\n            isum[1] += q8[l+16] * ((q2[l] >> 2) & 3);\n            isum[2] += q8[l+32] * ((q2[l] >> 4) & 3);\n            isum[3] += q8[l+48] * ((q2[l] >> 6) & 3);\n        }\n        for (int l = 0; l < 4; ++l) {\n            isum[l] *= (sc[l] & 0xF);\n        }\n        sumf += dall * (isum[0] + isum[1] + isum[2] + isum[3]) - dmin * summs;\n    }\n    *s = sumf;\n#endif\n}\n#endif\n\n#if QK_K == 256\nvoid ggml_vec_dot_q3_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    assert(n % QK_K == 0);\n\n    const uint32_t kmask1 = 0x03030303;\n    const uint32_t kmask2 = 0x0f0f0f0f;\n\n    const block_q3_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n#ifdef __ARM_NEON\n\n    uint32_t aux[3];\n    uint32_t utmp[4];\n\n    const uint8x16_t m3b = vdupq_n_u8(0x3);\n#ifdef __ARM_FEATURE_DOTPROD\n    const int32x4_t  vzero = vdupq_n_s32(0);\n#endif\n\n    const uint8x16_t m0 = vdupq_n_u8(1);\n    const uint8x16_t m1 = vshlq_n_u8(m0, 1);\n    const uint8x16_t m2 = vshlq_n_u8(m0, 2);\n    const uint8x16_t m3 = vshlq_n_u8(m0, 3);\n    const int8_t m32 = 32;\n\n    ggml_int8x16x4_t q3bytes;\n\n    float sum = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q3 = x[i].qs;\n        const uint8_t * restrict qh = x[i].hmask;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\n\n        ggml_uint8x16x4_t q3h;\n\n        int32_t isum = 0;\n\n        // Set up scales\n        memcpy(aux, x[i].scales, 12);\n        utmp[3] = ((aux[1] >> 4) & kmask2) | (((aux[2] >> 6) & kmask1) << 4);\n        utmp[2] = ((aux[0] >> 4) & kmask2) | (((aux[2] >> 4) & kmask1) << 4);\n        utmp[1] = (aux[1] & kmask2) | (((aux[2] >> 2) & kmask1) << 4);\n        utmp[0] = (aux[0] & kmask2) | (((aux[2] >> 0) & kmask1) << 4);\n\n        int8_t * scale = (int8_t *)utmp;\n        for (int j = 0; j < 16; ++j) scale[j] -= m32;\n\n        for (int j = 0; j < QK_K/128; ++j) {\n\n            const ggml_uint8x16x2_t q3bits = ggml_vld1q_u8_x2(q3); q3 += 32;\n            const ggml_int8x16x4_t q8bytes_1 = ggml_vld1q_s8_x4(q8); q8 += 64;\n            const ggml_int8x16x4_t q8bytes_2 = ggml_vld1q_s8_x4(q8); q8 += 64;\n\n            q3h.val[0] = vshlq_n_u8(vbicq_u8(m0, qhbits.val[0]), 2);\n            q3h.val[1] = vshlq_n_u8(vbicq_u8(m0, qhbits.val[1]), 2);\n            q3h.val[2] = vshlq_n_u8(vbicq_u8(m1, qhbits.val[0]), 1);\n            q3h.val[3] = vshlq_n_u8(vbicq_u8(m1, qhbits.val[1]), 1);\n\n            q3bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(q3bits.val[0], m3b)), vreinterpretq_s8_u8(q3h.val[0]));\n            q3bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(q3bits.val[1], m3b)), vreinterpretq_s8_u8(q3h.val[1]));\n            q3bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 2), m3b)), vreinterpretq_s8_u8(q3h.val[2]));\n            q3bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 2), m3b)), vreinterpretq_s8_u8(q3h.val[3]));\n\n#if defined(__ARM_FEATURE_DOTPROD)\n            isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[0], q8bytes_1.val[0])) * scale[0];\n            isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[1], q8bytes_1.val[1])) * scale[1];\n            isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[2], q8bytes_1.val[2])) * scale[2];\n            isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[3], q8bytes_1.val[3])) * scale[3];\n#else\n            int16x8_t p0 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[0]), vget_low_s8 (q8bytes_1.val[0])),\n                                     vmull_s8(vget_high_s8(q3bytes.val[0]), vget_high_s8(q8bytes_1.val[0])));\n            int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[1]), vget_low_s8 (q8bytes_1.val[1])),\n                                     vmull_s8(vget_high_s8(q3bytes.val[1]), vget_high_s8(q8bytes_1.val[1])));\n            int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[2]), vget_low_s8 (q8bytes_1.val[2])),\n                                     vmull_s8(vget_high_s8(q3bytes.val[2]), vget_high_s8(q8bytes_1.val[2])));\n            int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[3]), vget_low_s8 (q8bytes_1.val[3])),\n                                     vmull_s8(vget_high_s8(q3bytes.val[3]), vget_high_s8(q8bytes_1.val[3])));\n            isum += vaddvq_s16(p0) * scale[0] + vaddvq_s16(p1) * scale[1] + vaddvq_s16(p2) * scale[2] + vaddvq_s16(p3) * scale[3];\n#endif\n            scale += 4;\n\n            q3h.val[0] = vbicq_u8(m2, qhbits.val[0]);\n            q3h.val[1] = vbicq_u8(m2, qhbits.val[1]);\n            q3h.val[2] = vshrq_n_u8(vbicq_u8(m3, qhbits.val[0]), 1);\n            q3h.val[3] = vshrq_n_u8(vbicq_u8(m3, qhbits.val[1]), 1);\n\n            q3bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 4), m3b)), vreinterpretq_s8_u8(q3h.val[0]));\n            q3bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 4), m3b)), vreinterpretq_s8_u8(q3h.val[1]));\n            q3bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[0], 6), m3b)), vreinterpretq_s8_u8(q3h.val[2]));\n            q3bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(vshrq_n_u8(q3bits.val[1], 6), m3b)), vreinterpretq_s8_u8(q3h.val[3]));\n\n#if defined(__ARM_FEATURE_DOTPROD)\n            isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[0], q8bytes_2.val[0])) * scale[0];\n            isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[1], q8bytes_2.val[1])) * scale[1];\n            isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[2], q8bytes_2.val[2])) * scale[2];\n            isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[3], q8bytes_2.val[3])) * scale[3];\n#else\n            p0 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[0]), vget_low_s8 (q8bytes_2.val[0])),\n                           vmull_s8(vget_high_s8(q3bytes.val[0]), vget_high_s8(q8bytes_2.val[0])));\n            p1 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[1]), vget_low_s8 (q8bytes_2.val[1])),\n                           vmull_s8(vget_high_s8(q3bytes.val[1]), vget_high_s8(q8bytes_2.val[1])));\n            p2 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[2]), vget_low_s8 (q8bytes_2.val[2])),\n                           vmull_s8(vget_high_s8(q3bytes.val[2]), vget_high_s8(q8bytes_2.val[2])));\n            p3 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[3]), vget_low_s8 (q8bytes_2.val[3])),\n                           vmull_s8(vget_high_s8(q3bytes.val[3]), vget_high_s8(q8bytes_2.val[3])));\n            isum += vaddvq_s16(p0) * scale[0] + vaddvq_s16(p1) * scale[1] + vaddvq_s16(p2) * scale[2] + vaddvq_s16(p3) * scale[3];\n#endif\n            scale += 4;\n\n            if (j == 0) {\n                qhbits.val[0] = vshrq_n_u8(qhbits.val[0], 4);\n                qhbits.val[1] = vshrq_n_u8(qhbits.val[1], 4);\n            }\n\n        }\n        sum += d * isum;\n\n    }\n\n    *s = sum;\n\n#elif defined __AVX2__\n\n    const __m256i m3 = _mm256_set1_epi8(3);\n    const __m256i mone = _mm256_set1_epi8(1);\n    const __m128i m32 = _mm_set1_epi8(32);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    uint32_t aux[3];\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q3 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        // Set up scales\n        memcpy(aux, x[i].scales, 12);\n        __m128i scales128 = _mm_set_epi32(\n                ((aux[1] >> 4) & kmask2) | (((aux[2] >> 6) & kmask1) << 4),\n                ((aux[0] >> 4) & kmask2) | (((aux[2] >> 4) & kmask1) << 4),\n                (aux[1] & kmask2) | (((aux[2] >> 2) & kmask1) << 4),\n                (aux[0] & kmask2) | (((aux[2] >> 0) & kmask1) << 4));\n        scales128 = _mm_sub_epi8(scales128, m32);\n        const __m256i all_scales = _mm256_cvtepi8_epi16(scales128);\n        const __m128i l_scales = _mm256_extracti128_si256(all_scales, 0);\n        const __m128i h_scales = _mm256_extracti128_si256(all_scales, 1);\n        const __m256i scales[2] = {MM256_SET_M128I(l_scales, l_scales), MM256_SET_M128I(h_scales, h_scales)};\n\n        // high bit\n        const __m256i hbits = _mm256_loadu_si256((const __m256i*)x[i].hmask);\n\n        // integer accumulator\n        __m256i sumi = _mm256_setzero_si256();\n\n        int bit = 0;\n        int is  = 0;\n\n        for (int j = 0; j < QK_K/128; ++j) {\n            // load low 2 bits\n            const __m256i q3bits = _mm256_loadu_si256((const __m256i*)q3); q3 += 32;\n\n            // prepare low and high bits\n            const __m256i q3l_0 = _mm256_and_si256(q3bits, m3);\n            const __m256i q3h_0 = _mm256_slli_epi16(_mm256_srli_epi16(_mm256_andnot_si256(hbits, _mm256_slli_epi16(mone, bit)), bit), 2);\n            ++bit;\n\n            const __m256i q3l_1 = _mm256_and_si256(_mm256_srli_epi16(q3bits, 2), m3);\n            const __m256i q3h_1 = _mm256_slli_epi16(_mm256_srli_epi16(_mm256_andnot_si256(hbits, _mm256_slli_epi16(mone, bit)), bit), 2);\n            ++bit;\n\n            const __m256i q3l_2 = _mm256_and_si256(_mm256_srli_epi16(q3bits, 4), m3);\n            const __m256i q3h_2 = _mm256_slli_epi16(_mm256_srli_epi16(_mm256_andnot_si256(hbits, _mm256_slli_epi16(mone, bit)), bit), 2);\n            ++bit;\n\n            const __m256i q3l_3 = _mm256_and_si256(_mm256_srli_epi16(q3bits, 6), m3);\n            const __m256i q3h_3 = _mm256_slli_epi16(_mm256_srli_epi16(_mm256_andnot_si256(hbits, _mm256_slli_epi16(mone, bit)), bit), 2);\n            ++bit;\n\n            // load Q8 quants\n            const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_2 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_3 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n\n            // Dot product: we multiply the 2 low bits and 1 high bit part separately, so we can use _mm256_maddubs_epi16,\n            // and then subtract. The high bit part has the 2 already subtracted (and so, it is zero if the high bit was not set,\n            // and 2 if the high bit was set)\n            __m256i q8s_0 = _mm256_maddubs_epi16(q3h_0, q8_0);\n            __m256i q8s_1 = _mm256_maddubs_epi16(q3h_1, q8_1);\n            __m256i q8s_2 = _mm256_maddubs_epi16(q3h_2, q8_2);\n            __m256i q8s_3 = _mm256_maddubs_epi16(q3h_3, q8_3);\n\n            __m256i p16_0 = _mm256_maddubs_epi16(q3l_0, q8_0);\n            __m256i p16_1 = _mm256_maddubs_epi16(q3l_1, q8_1);\n            __m256i p16_2 = _mm256_maddubs_epi16(q3l_2, q8_2);\n            __m256i p16_3 = _mm256_maddubs_epi16(q3l_3, q8_3);\n\n            p16_0 = _mm256_sub_epi16(p16_0, q8s_0);\n            p16_1 = _mm256_sub_epi16(p16_1, q8s_1);\n            p16_2 = _mm256_sub_epi16(p16_2, q8s_2);\n            p16_3 = _mm256_sub_epi16(p16_3, q8s_3);\n\n            // multiply with scales\n            p16_0 = _mm256_madd_epi16(_mm256_shuffle_epi8(scales[j], get_scale_shuffle_q3k(is + 0)), p16_0);\n            p16_1 = _mm256_madd_epi16(_mm256_shuffle_epi8(scales[j], get_scale_shuffle_q3k(is + 1)), p16_1);\n            p16_2 = _mm256_madd_epi16(_mm256_shuffle_epi8(scales[j], get_scale_shuffle_q3k(is + 2)), p16_2);\n            p16_3 = _mm256_madd_epi16(_mm256_shuffle_epi8(scales[j], get_scale_shuffle_q3k(is + 3)), p16_3);\n\n            // accumulate\n            p16_0 = _mm256_add_epi32(p16_0, p16_1);\n            p16_2 = _mm256_add_epi32(p16_2, p16_3);\n            sumi  = _mm256_add_epi32(sumi, _mm256_add_epi32(p16_0, p16_2));\n\n        }\n\n        // multiply with block scale and accumulate\n        acc = _mm256_fmadd_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(sumi), acc);\n\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __AVX__\n\n    const __m128i m3 = _mm_set1_epi8(3);\n    const __m128i mone = _mm_set1_epi8(1);\n    const __m128i m32 = _mm_set1_epi8(32);\n    const __m128i m2 = _mm_set1_epi8(2);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    const uint32_t *aux;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q3 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        // Set up scales\n        aux = (const uint32_t *)x[i].scales;\n        __m128i scales128 = _mm_set_epi32(\n                ((aux[1] >> 4) & kmask2) | (((aux[2] >> 6) & kmask1) << 4),\n                ((aux[0] >> 4) & kmask2) | (((aux[2] >> 4) & kmask1) << 4),\n                (aux[1] & kmask2) | (((aux[2] >> 2) & kmask1) << 4),\n                (aux[0] & kmask2) | (((aux[2] >> 0) & kmask1) << 4));\n        scales128 = _mm_sub_epi8(scales128, m32);\n        const __m128i scales_0 = _mm_cvtepi8_epi16(scales128);\n        const __m128i scales_1 = _mm_cvtepi8_epi16(_mm_unpackhi_epi64(scales128, scales128));\n        const __m128i scales[2] = { scales_0, scales_1 };\n\n        // high bit *128*2 from block_q3_K.hmask[QK_K/8]\n        const __m128i hbits_0 = _mm_loadu_si128((const __m128i*)&x[i].hmask[0]);\n        const __m128i hbits_1 = _mm_loadu_si128((const __m128i*)&x[i].hmask[16]);\n\n        // integer accumulator\n        __m128i sumi_0 = _mm_setzero_si128();\n        __m128i sumi_1 = _mm_setzero_si128();\n\n        for (int j = 0; j < QK_K/128; ++j) {\n            // load low 2 bits *64*2 from block_q3_K.qs[QK_K/4]\n            const __m128i q3bits_0 = _mm_loadu_si128((const __m128i*)q3); q3 += 16;\n            const __m128i q3bits_1 = _mm_loadu_si128((const __m128i*)q3); q3 += 16;\n\n            // prepare low and high bits\n            const int bit = j << 2;\n\n            const __m128i q3l_0 = _mm_and_si128(q3bits_0, m3);\n            const __m128i q3l_1 = _mm_and_si128(q3bits_1, m3);\n            const __m128i q3h_0 = _mm_slli_epi16(_mm_srli_epi16(_mm_andnot_si128(hbits_0, _mm_slli_epi16(mone, bit)), bit), 2);\n            const __m128i q3h_1 = _mm_slli_epi16(_mm_srli_epi16(_mm_andnot_si128(hbits_1, _mm_slli_epi16(mone, bit)), bit), 2);\n\n            const __m128i q3l_2 = _mm_and_si128(_mm_srli_epi16(q3bits_0, 2), m3);\n            const __m128i q3l_3 = _mm_and_si128(_mm_srli_epi16(q3bits_1, 2), m3);\n            const __m128i q3h_2 = _mm_slli_epi16(_mm_srli_epi16(_mm_andnot_si128(hbits_0, _mm_slli_epi16(mone, bit+1)), bit+1), 2);\n            const __m128i q3h_3 = _mm_slli_epi16(_mm_srli_epi16(_mm_andnot_si128(hbits_1, _mm_slli_epi16(mone, bit+1)), bit+1), 2);\n\n            const __m128i q3l_4 = _mm_and_si128(_mm_srli_epi16(q3bits_0, 4), m3);\n            const __m128i q3l_5 = _mm_and_si128(_mm_srli_epi16(q3bits_1, 4), m3);\n            const __m128i q3h_4 = _mm_slli_epi16(_mm_srli_epi16(_mm_andnot_si128(hbits_0, _mm_slli_epi16(mone, bit+2)), bit+2), 2);\n            const __m128i q3h_5 = _mm_slli_epi16(_mm_srli_epi16(_mm_andnot_si128(hbits_1, _mm_slli_epi16(mone, bit+2)), bit+2), 2);\n\n            const __m128i q3l_6 = _mm_and_si128(_mm_srli_epi16(q3bits_0, 6), m3);\n            const __m128i q3l_7 = _mm_and_si128(_mm_srli_epi16(q3bits_1, 6), m3);\n            const __m128i q3h_6 = _mm_slli_epi16(_mm_srli_epi16(_mm_andnot_si128(hbits_0, _mm_slli_epi16(mone, bit+3)), bit+3), 2);\n            const __m128i q3h_7 = _mm_slli_epi16(_mm_srli_epi16(_mm_andnot_si128(hbits_1, _mm_slli_epi16(mone, bit+3)), bit+3), 2);\n\n            // load Q8 quants from block_q8_K.qs[QK_K]\n            const __m128i q8_0 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_1 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_2 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_3 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_4 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_5 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_6 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_7 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n\n            // Dot product: we multiply the 2 low bits and 1 high bit part separately, so we can use _mm256_maddubs_epi16,\n            // and then subtract. The high bit part has the 2 already subtracted (and so, it is zero if the high bit was not set,\n            // and 2 if the high bit was set)\n            __m128i q8s_0 = _mm_maddubs_epi16(q3h_0, q8_0);\n            __m128i q8s_1 = _mm_maddubs_epi16(q3h_1, q8_1);\n            __m128i q8s_2 = _mm_maddubs_epi16(q3h_2, q8_2);\n            __m128i q8s_3 = _mm_maddubs_epi16(q3h_3, q8_3);\n            __m128i q8s_4 = _mm_maddubs_epi16(q3h_4, q8_4);\n            __m128i q8s_5 = _mm_maddubs_epi16(q3h_5, q8_5);\n            __m128i q8s_6 = _mm_maddubs_epi16(q3h_6, q8_6);\n            __m128i q8s_7 = _mm_maddubs_epi16(q3h_7, q8_7);\n\n            __m128i p16_0 = _mm_maddubs_epi16(q3l_0, q8_0);\n            __m128i p16_1 = _mm_maddubs_epi16(q3l_1, q8_1);\n            __m128i p16_2 = _mm_maddubs_epi16(q3l_2, q8_2);\n            __m128i p16_3 = _mm_maddubs_epi16(q3l_3, q8_3);\n            __m128i p16_4 = _mm_maddubs_epi16(q3l_4, q8_4);\n            __m128i p16_5 = _mm_maddubs_epi16(q3l_5, q8_5);\n            __m128i p16_6 = _mm_maddubs_epi16(q3l_6, q8_6);\n            __m128i p16_7 = _mm_maddubs_epi16(q3l_7, q8_7);\n\n            p16_0 = _mm_sub_epi16(p16_0, q8s_0);\n            p16_1 = _mm_sub_epi16(p16_1, q8s_1);\n            p16_2 = _mm_sub_epi16(p16_2, q8s_2);\n            p16_3 = _mm_sub_epi16(p16_3, q8s_3);\n            p16_4 = _mm_sub_epi16(p16_4, q8s_4);\n            p16_5 = _mm_sub_epi16(p16_5, q8s_5);\n            p16_6 = _mm_sub_epi16(p16_6, q8s_6);\n            p16_7 = _mm_sub_epi16(p16_7, q8s_7);\n\n            // multiply with scales\n            __m128i shuffle = _mm_set1_epi16(0x0100);\n            p16_0 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p16_0);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p16_1 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p16_1);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p16_2 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p16_2);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p16_3 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p16_3);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p16_4 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p16_4);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p16_5 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p16_5);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p16_6 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p16_6);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            p16_7 = _mm_madd_epi16(_mm_shuffle_epi8(scales[j], shuffle), p16_7);\n\n            // accumulate\n            p16_0 = _mm_add_epi32(p16_0, p16_1);\n            p16_2 = _mm_add_epi32(p16_2, p16_3);\n            p16_4 = _mm_add_epi32(p16_4, p16_5);\n            p16_6 = _mm_add_epi32(p16_6, p16_7);\n            sumi_0 = _mm_add_epi32(sumi_0, _mm_add_epi32(p16_0, p16_2));\n            sumi_1 = _mm_add_epi32(sumi_1, _mm_add_epi32(p16_4, p16_6));\n\n        }\n\n        // multiply with block scale and accumulate\n        __m256i sumi = MM256_SET_M128I(sumi_1, sumi_0);\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(sumi)), acc);\n\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __riscv_v_intrinsic\n\n    uint32_t aux[3];\n    uint32_t utmp[4];\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * restrict q3 = x[i].qs;\n        const uint8_t * restrict qh = x[i].hmask;\n        const  int8_t * restrict q8 = y[i].qs;\n\n        memcpy(aux, x[i].scales, 12);\n        utmp[3] = ((aux[1] >> 4) & kmask2) | (((aux[2] >> 6) & kmask1) << 4);\n        utmp[2] = ((aux[0] >> 4) & kmask2) | (((aux[2] >> 4) & kmask1) << 4);\n        utmp[1] = (aux[1] & kmask2) | (((aux[2] >> 2) & kmask1) << 4);\n        utmp[0] = (aux[0] & kmask2) | (((aux[2] >> 0) & kmask1) << 4);\n\n        int8_t * scale = (int8_t *)utmp;\n        for (int j = 0; j < 16; ++j) scale[j] -= 32;\n\n\n        size_t vl = 32;\n        uint8_t m =  1;\n\n        vint32m1_t vzero = __riscv_vmv_v_x_i32m1(0, 1);\n        vuint8m1_t vqh = __riscv_vle8_v_u8m1(qh, vl);\n\n        int sum_t = 0;\n\n        for (int j = 0; j < QK_K; j += 128) {\n\n            vl = 32;\n\n            // load Q3\n            vuint8m1_t q3_x = __riscv_vle8_v_u8m1(q3, vl);\n\n            vint8m1_t q3_0 = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vand_vx_u8m1(q3_x, 0x03, vl));\n            vint8m1_t q3_1 = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(q3_x, 0x2, vl), 0x03 , vl));\n            vint8m1_t q3_2 = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(q3_x, 0x4, vl), 0x03 , vl));\n            vint8m1_t q3_3 = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(q3_x, 0x6, vl), 0x03 , vl));\n\n            // compute mask for subtraction\n            vuint8m1_t qh_m0 = __riscv_vand_vx_u8m1(vqh, m, vl);\n            vbool8_t vmask_0 = __riscv_vmseq_vx_u8m1_b8(qh_m0, 0, vl);\n            vint8m1_t q3_m0 = __riscv_vsub_vx_i8m1_m(vmask_0, q3_0, 0x4, vl);\n            m <<= 1;\n\n            vuint8m1_t qh_m1 = __riscv_vand_vx_u8m1(vqh, m, vl);\n            vbool8_t vmask_1 = __riscv_vmseq_vx_u8m1_b8(qh_m1, 0, vl);\n            vint8m1_t q3_m1 = __riscv_vsub_vx_i8m1_m(vmask_1, q3_1, 0x4, vl);\n            m <<= 1;\n\n            vuint8m1_t qh_m2 = __riscv_vand_vx_u8m1(vqh, m, vl);\n            vbool8_t vmask_2 = __riscv_vmseq_vx_u8m1_b8(qh_m2, 0, vl);\n            vint8m1_t q3_m2 = __riscv_vsub_vx_i8m1_m(vmask_2, q3_2, 0x4, vl);\n            m <<= 1;\n\n            vuint8m1_t qh_m3 = __riscv_vand_vx_u8m1(vqh, m, vl);\n            vbool8_t vmask_3 = __riscv_vmseq_vx_u8m1_b8(qh_m3, 0, vl);\n            vint8m1_t q3_m3 = __riscv_vsub_vx_i8m1_m(vmask_3, q3_3, 0x4, vl);\n            m <<= 1;\n\n            // load Q8 and take product with Q3\n            vint16m2_t a0 = __riscv_vwmul_vv_i16m2(q3_m0, __riscv_vle8_v_i8m1(q8, vl), vl);\n            vint16m2_t a1 = __riscv_vwmul_vv_i16m2(q3_m1, __riscv_vle8_v_i8m1(q8+32, vl), vl);\n            vint16m2_t a2 = __riscv_vwmul_vv_i16m2(q3_m2, __riscv_vle8_v_i8m1(q8+64, vl), vl);\n            vint16m2_t a3 = __riscv_vwmul_vv_i16m2(q3_m3, __riscv_vle8_v_i8m1(q8+96, vl), vl);\n\n            vl = 16;\n\n            // retreive lane to multiply with scale\n            vint32m2_t aux0_0 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(a0, 0), (scale[0]), vl);\n            vint32m2_t aux0_1 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(a0, 1), (scale[1]), vl);\n            vint32m2_t aux1_0 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(a1, 0), (scale[2]), vl);\n            vint32m2_t aux1_1 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(a1, 1), (scale[3]), vl);\n            vint32m2_t aux2_0 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(a2, 0), (scale[4]), vl);\n            vint32m2_t aux2_1 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(a2, 1), (scale[5]), vl);\n            vint32m2_t aux3_0 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(a3, 0), (scale[6]), vl);\n            vint32m2_t aux3_1 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(a3, 1), (scale[7]), vl);\n\n            vint32m1_t isum0 = __riscv_vredsum_vs_i32m2_i32m1(__riscv_vadd_vv_i32m2(aux0_0, aux0_1, vl), vzero, vl);\n            vint32m1_t isum1 = __riscv_vredsum_vs_i32m2_i32m1(__riscv_vadd_vv_i32m2(aux1_0, aux1_1, vl), isum0, vl);\n            vint32m1_t isum2 = __riscv_vredsum_vs_i32m2_i32m1(__riscv_vadd_vv_i32m2(aux2_0, aux2_1, vl), isum1, vl);\n            vint32m1_t isum3 = __riscv_vredsum_vs_i32m2_i32m1(__riscv_vadd_vv_i32m2(aux3_0, aux3_1, vl), isum2, vl);\n\n            sum_t +=  __riscv_vmv_x_s_i32m1_i32(isum3);\n\n            q3 += 32;    q8 += 128;   scale += 8;\n\n        }\n\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n\n        sumf += d*sum_t;\n\n    }\n\n    *s = sumf;\n\n#else\n    // scalar version\n    // This function is written like this so the compiler can manage to vectorize most of it\n    // Using -Ofast, GCC and clang manage to produce code that is within a factor of 2 or so from the\n    // manually vectorized version above. Every other version I tried would run at least 4 times slower.\n    // The ideal situation would be if we could just write the code once, and the compiler would\n    // automatically produce the best possible set of machine instructions, instead of us having to manually\n    // write vectorized versions for AVX, ARM_NEON, etc.\n\n    int8_t  aux8[QK_K];\n    int16_t aux16[8];\n    float   sums [8];\n    int32_t aux32[8];\n    memset(sums, 0, 8*sizeof(float));\n\n    uint32_t auxs[4];\n    const int8_t * scales = (const int8_t*)auxs;\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n        const uint8_t * restrict q3 = x[i].qs;\n        const uint8_t * restrict hm = x[i].hmask;\n        const  int8_t * restrict q8 = y[i].qs;\n        memset(aux32, 0, 8*sizeof(int32_t));\n        int8_t * restrict a = aux8;\n        uint8_t m = 1;\n        for (int j = 0; j < QK_K; j += 128) {\n            for (int l = 0; l < 32; ++l) a[l] = q3[l] & 3;\n            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n            a += 32; m <<= 1;\n            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 2) & 3;\n            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n            a += 32; m <<= 1;\n            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 4) & 3;\n            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n            a += 32; m <<= 1;\n            for (int l = 0; l < 32; ++l) a[l] = (q3[l] >> 6) & 3;\n            for (int l = 0; l < 32; ++l) a[l] -= (hm[l] & m ? 0 : 4);\n            a += 32; m <<= 1;\n            q3 += 32;\n        }\n        a = aux8;\n\n        memcpy(auxs, x[i].scales, 12);\n        uint32_t tmp = auxs[2];\n        auxs[2] = ((auxs[0] >> 4) & kmask2) | (((tmp >> 4) & kmask1) << 4);\n        auxs[3] = ((auxs[1] >> 4) & kmask2) | (((tmp >> 6) & kmask1) << 4);\n        auxs[0] = (auxs[0] & kmask2) | (((tmp >> 0) & kmask1) << 4);\n        auxs[1] = (auxs[1] & kmask2) | (((tmp >> 2) & kmask1) << 4);\n        for (int j = 0; j < QK_K/16; ++j) {\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += (scales[j] - 32) * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += (scales[j] - 32) * aux16[l];\n            q8 += 8; a += 8;\n        }\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n        for (int l = 0; l < 8; ++l) sums[l] += d * aux32[l];\n    }\n    for (int l = 0; l < 8; ++l) sumf += sums[l];\n    *s = sumf;\n\n#endif\n\n}\n\n#else\n\nvoid ggml_vec_dot_q3_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    assert(n % QK_K == 0);\n\n    const block_q3_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n#ifdef __ARM_NEON\n\n#ifdef __ARM_FEATURE_DOTPROD\n    const int32x4_t  vzero = vdupq_n_s32(0);\n#endif\n\n    const uint8x16_t m3b = vdupq_n_u8(0x3);\n    const uint8x16_t mh  = vdupq_n_u8(4);\n\n    ggml_int8x16x4_t q3bytes;\n\n    uint16_t aux16[2];\n    int8_t * scales = (int8_t *)aux16;\n\n    float sum = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        ggml_uint8x16x4_t q3h;\n\n        const uint8x8_t  hbits    = vld1_u8(x[i].hmask);\n        const uint8x16_t q3bits   = vld1q_u8(x[i].qs);\n        const ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(y[i].qs);\n\n        const uint16_t a = *(const uint16_t *)x[i].scales;\n        aux16[0] = a & 0x0f0f;\n        aux16[1] = (a >> 4) & 0x0f0f;\n\n        for (int j = 0; j < 4; ++j) scales[j] -= 8;\n\n        int32_t isum = -4*(scales[0] * y[i].bsums[0] + scales[2] * y[i].bsums[1] + scales[1] * y[i].bsums[2] + scales[3] * y[i].bsums[3]);\n\n        const float d = y[i].d * (float)x[i].d;\n\n        const uint8x16_t htmp = vcombine_u8(hbits, vshr_n_u8(hbits, 1));\n        q3h.val[0] = vandq_u8(mh, vshlq_n_u8(htmp, 2));\n        q3h.val[1] = vandq_u8(mh, htmp);\n        q3h.val[2] = vandq_u8(mh, vshrq_n_u8(htmp, 2));\n        q3h.val[3] = vandq_u8(mh, vshrq_n_u8(htmp, 4));\n\n        q3bytes.val[0] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q3bits, m3b),                q3h.val[0]));\n        q3bytes.val[1] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(vshrq_n_u8(q3bits, 2), m3b), q3h.val[1]));\n        q3bytes.val[2] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(vshrq_n_u8(q3bits, 4), m3b), q3h.val[2]));\n        q3bytes.val[3] = vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q3bits, 6),                q3h.val[3]));\n\n#if defined(__ARM_FEATURE_DOTPROD)\n        isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[0], q8bytes.val[0])) * scales[0];\n        isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[1], q8bytes.val[1])) * scales[2];\n        isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[2], q8bytes.val[2])) * scales[1];\n        isum += vaddvq_s32(vdotq_s32(vzero, q3bytes.val[3], q8bytes.val[3])) * scales[3];\n#else\n        const int16x8_t p0 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                       vmull_s8(vget_high_s8(q3bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n        const int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                       vmull_s8(vget_high_s8(q3bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n        const int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[2]), vget_low_s8 (q8bytes.val[2])),\n                                       vmull_s8(vget_high_s8(q3bytes.val[2]), vget_high_s8(q8bytes.val[2])));\n        const int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q3bytes.val[3]), vget_low_s8 (q8bytes.val[3])),\n                                       vmull_s8(vget_high_s8(q3bytes.val[3]), vget_high_s8(q8bytes.val[3])));\n        isum += vaddvq_s16(p0) * scales[0] + vaddvq_s16(p1) * scales[2] + vaddvq_s16(p2) * scales[1] + vaddvq_s16(p3) * scales[3];\n#endif\n\n        sum += d * isum;\n\n    }\n\n    *s = sum;\n\n#elif defined __AVX2__\n\n    const __m256i m3 = _mm256_set1_epi8(3);\n    const __m256i m1 = _mm256_set1_epi8(1);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    uint64_t aux64;\n\n    uint16_t aux16[2];\n    const int8_t * aux8 = (const int8_t *)aux16;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q3 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const uint16_t a = *(const uint16_t *)x[i].scales;\n        aux16[0] = a & 0x0f0f;\n        aux16[1] = (a >> 4) & 0x0f0f;\n\n        const __m256i scale_0 = MM256_SET_M128I(_mm_set1_epi16(aux8[2] - 8), _mm_set1_epi16(aux8[0] - 8));\n        const __m256i scale_1 = MM256_SET_M128I(_mm_set1_epi16(aux8[3] - 8), _mm_set1_epi16(aux8[1] - 8));\n\n        memcpy(&aux64, x[i].hmask, 8);\n\n        const __m128i haux = _mm_set_epi64x(aux64 >> 1, aux64 >> 0);\n        __m256i q3h_0 = MM256_SET_M128I(_mm_srli_epi16(haux, 2), haux);\n        __m256i q3h_1 = _mm256_srli_epi16(q3h_0, 4);\n        q3h_0 = _mm256_slli_epi16(_mm256_andnot_si256(q3h_0, m1), 2);\n        q3h_1 = _mm256_slli_epi16(_mm256_andnot_si256(q3h_1, m1), 2);\n\n        // load low 2 bits\n        const __m128i q3bits = _mm_loadu_si128((const __m128i*)q3);\n\n        // prepare low and high bits\n        const __m256i q3aux  = MM256_SET_M128I(_mm_srli_epi16(q3bits, 2), q3bits);\n        const __m256i q3l_0 = _mm256_and_si256(q3aux, m3);\n        const __m256i q3l_1 = _mm256_and_si256(_mm256_srli_epi16(q3aux, 4), m3);\n\n        // load Q8 quants\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        // Dot product: we multiply the 2 low bits and 1 high bit part separately, so we can use _mm256_maddubs_epi16,\n        // and then subtract. The high bit part has the 2 already subtracted (and so, it is zero if the high bit was not set,\n        // and 2 if the high bit was set)\n        const __m256i q8s_0 = _mm256_maddubs_epi16(q3h_0, q8_0);\n        const __m256i q8s_1 = _mm256_maddubs_epi16(q3h_1, q8_1);\n\n        __m256i p16_0 = _mm256_maddubs_epi16(q3l_0, q8_0);\n        __m256i p16_1 = _mm256_maddubs_epi16(q3l_1, q8_1);\n\n        p16_0 = _mm256_sub_epi16(p16_0, q8s_0);\n        p16_1 = _mm256_sub_epi16(p16_1, q8s_1);\n\n        // multiply with scales\n        p16_0 = _mm256_madd_epi16(scale_0, p16_0);\n        p16_1 = _mm256_madd_epi16(scale_1, p16_1);\n\n        p16_0 = _mm256_add_epi32(p16_0, p16_1);\n\n        // multiply with block scale and accumulate\n        acc = _mm256_fmadd_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(p16_0), acc);\n\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __AVX__\n\n    const __m128i m3 = _mm_set1_epi8(3);\n    const __m128i m1 = _mm_set1_epi8(1);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    uint64_t aux64;\n\n    uint16_t aux16[2];\n    const int8_t * aux8 = (const int8_t *)aux16;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q3 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const uint16_t a = *(const uint16_t *)x[i].scales;\n        aux16[0] = a & 0x0f0f;\n        aux16[1] = (a >> 4) & 0x0f0f;\n\n        const __m128i scale_0 = _mm_set1_epi16(aux8[0] - 8);\n        const __m128i scale_1 = _mm_set1_epi16(aux8[2] - 8);\n        const __m128i scale_2 = _mm_set1_epi16(aux8[1] - 8);\n        const __m128i scale_3 = _mm_set1_epi16(aux8[3] - 8);\n\n        memcpy(&aux64, x[i].hmask, 8);\n\n        __m128i q3h_0 = _mm_set_epi64x(aux64 >> 1, aux64 >> 0);\n        __m128i q3h_1 = _mm_srli_epi16(q3h_0, 2);\n        __m128i q3h_2 = _mm_srli_epi16(q3h_0, 4);\n        __m128i q3h_3 = _mm_srli_epi16(q3h_0, 6);\n        q3h_0 = _mm_slli_epi16(_mm_andnot_si128(q3h_0, m1), 2);\n        q3h_1 = _mm_slli_epi16(_mm_andnot_si128(q3h_1, m1), 2);\n        q3h_2 = _mm_slli_epi16(_mm_andnot_si128(q3h_2, m1), 2);\n        q3h_3 = _mm_slli_epi16(_mm_andnot_si128(q3h_3, m1), 2);\n\n        // load low 2 bits\n        const __m128i q3bits = _mm_loadu_si128((const __m128i*)q3);\n\n        // prepare low and high bits\n        const __m128i q3l_0 = _mm_and_si128(q3bits, m3);\n        const __m128i q3l_1 = _mm_and_si128(_mm_srli_epi16(q3bits, 2), m3);\n        const __m128i q3l_2 = _mm_and_si128(_mm_srli_epi16(q3bits, 4), m3);\n        const __m128i q3l_3 = _mm_and_si128(_mm_srli_epi16(q3bits, 6), m3);\n\n        // load Q8 quants\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        // Dot product: we multiply the 2 low bits and 1 high bit part separately, so we can use _mm_maddubs_epi16,\n        // and then subtract. The high bit part has the 2 already subtracted (and so, it is zero if the high bit was not set,\n        // and 2 if the high bit was set)\n        const __m128i q8s_0 = _mm_maddubs_epi16(q3h_0, _mm256_extractf128_si256(q8_0, 0));\n        const __m128i q8s_1 = _mm_maddubs_epi16(q3h_1, _mm256_extractf128_si256(q8_0, 1));\n        const __m128i q8s_2 = _mm_maddubs_epi16(q3h_2, _mm256_extractf128_si256(q8_1, 0));\n        const __m128i q8s_3 = _mm_maddubs_epi16(q3h_3, _mm256_extractf128_si256(q8_1, 1));\n\n        __m128i p16_0 = _mm_maddubs_epi16(q3l_0, _mm256_extractf128_si256(q8_0, 0));\n        __m128i p16_1 = _mm_maddubs_epi16(q3l_1, _mm256_extractf128_si256(q8_0, 1));\n        __m128i p16_2 = _mm_maddubs_epi16(q3l_2, _mm256_extractf128_si256(q8_1, 0));\n        __m128i p16_3 = _mm_maddubs_epi16(q3l_3, _mm256_extractf128_si256(q8_1, 1));\n\n        p16_0 = _mm_sub_epi16(p16_0, q8s_0);\n        p16_1 = _mm_sub_epi16(p16_1, q8s_1);\n        p16_2 = _mm_sub_epi16(p16_2, q8s_2);\n        p16_3 = _mm_sub_epi16(p16_3, q8s_3);\n\n        // multiply with scales\n        p16_0 = _mm_madd_epi16(scale_0, p16_0);\n        p16_1 = _mm_madd_epi16(scale_1, p16_1);\n        p16_2 = _mm_madd_epi16(scale_2, p16_2);\n        p16_3 = _mm_madd_epi16(scale_3, p16_3);\n\n        p16_0 = _mm_add_epi32(p16_0, p16_2);\n        p16_1 = _mm_add_epi32(p16_1, p16_3);\n        __m256i p16 = MM256_SET_M128I(p16_1, p16_0);\n\n        // multiply with block scale and accumulate\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(p16)), acc);\n\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __riscv_v_intrinsic\n\n    uint16_t aux16[2];\n    int8_t * scales = (int8_t *)aux16;\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * restrict q3 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const uint16_t a = *(const uint16_t *)x[i].scales;\n        aux16[0] = a & 0x0f0f;\n        aux16[1] = (a >> 4) & 0x0f0f;\n\n        for (int j = 0; j < 4; ++j) scales[j] -= 8;\n\n        int32_t isum = -4*(scales[0] * y[i].bsums[0] + scales[2] * y[i].bsums[1] + scales[1] * y[i].bsums[2] + scales[3] * y[i].bsums[3]);\n\n        const float d = y[i].d * (float)x[i].d;\n\n        vint32m1_t vzero = __riscv_vmv_v_x_i32m1(0, 1);\n\n        // load qh\n        vuint8mf4_t qh_x1   = __riscv_vle8_v_u8mf4(x[i].hmask, 8);\n        vuint8mf2_t qh_x2   = __riscv_vlmul_ext_v_u8mf4_u8mf2(__riscv_vsrl_vx_u8mf4(qh_x1, 1, 8));\n\n        size_t vl = 16;\n\n        // extend and combine both qh_x1 and qh_x2\n        vuint8mf2_t qh_x = __riscv_vslideup_vx_u8mf2(__riscv_vlmul_ext_v_u8mf4_u8mf2(qh_x1), qh_x2, vl/2, vl);\n\n        vuint8mf2_t qh_0 = __riscv_vand_vx_u8mf2(__riscv_vsll_vx_u8mf2(qh_x, 0x2, vl), 0x4, vl);\n        vuint8mf2_t qh_1 = __riscv_vand_vx_u8mf2(qh_x, 0x4, vl);\n        vuint8mf2_t qh_2 = __riscv_vand_vx_u8mf2(__riscv_vsrl_vx_u8mf2(qh_x, 0x2, vl), 0x4, vl);\n        vuint8mf2_t qh_3 = __riscv_vand_vx_u8mf2(__riscv_vsrl_vx_u8mf2(qh_x, 0x4, vl), 0x4, vl);\n\n        // load Q3\n        vuint8mf2_t q3_x  = __riscv_vle8_v_u8mf2(q3, vl);\n\n        vuint8mf2_t q3h_0 = __riscv_vor_vv_u8mf2(__riscv_vand_vx_u8mf2(q3_x, 0x3, vl), qh_0, vl);\n        vuint8mf2_t q3h_1 = __riscv_vor_vv_u8mf2(__riscv_vand_vx_u8mf2(__riscv_vsrl_vx_u8mf2(q3_x, 2, vl), 0x3, vl), qh_1, vl);\n        vuint8mf2_t q3h_2 = __riscv_vor_vv_u8mf2(__riscv_vand_vx_u8mf2(__riscv_vsrl_vx_u8mf2(q3_x, 4, vl), 0x3, vl), qh_2, vl);\n        vuint8mf2_t q3h_3 = __riscv_vor_vv_u8mf2(__riscv_vsrl_vx_u8mf2(q3_x, 0x6, vl), qh_3, vl);\n\n        vint8mf2_t q3_0 = __riscv_vreinterpret_v_u8mf2_i8mf2(q3h_0);\n        vint8mf2_t q3_1 = __riscv_vreinterpret_v_u8mf2_i8mf2(q3h_1);\n        vint8mf2_t q3_2 = __riscv_vreinterpret_v_u8mf2_i8mf2(q3h_2);\n        vint8mf2_t q3_3 = __riscv_vreinterpret_v_u8mf2_i8mf2(q3h_3);\n\n        // load Q8 and take product with Q3\n        vint16m1_t p0 = __riscv_vwmul_vv_i16m1(q3_0, __riscv_vle8_v_i8mf2(q8, vl), vl);\n        vint16m1_t p1 = __riscv_vwmul_vv_i16m1(q3_1, __riscv_vle8_v_i8mf2(q8+16, vl), vl);\n        vint16m1_t p2 = __riscv_vwmul_vv_i16m1(q3_2, __riscv_vle8_v_i8mf2(q8+32, vl), vl);\n        vint16m1_t p3 = __riscv_vwmul_vv_i16m1(q3_3, __riscv_vle8_v_i8mf2(q8+48, vl), vl);\n\n        vint32m1_t vs_0 = __riscv_vwredsum_vs_i16m1_i32m1(p0, vzero, vl);\n        vint32m1_t vs_1 = __riscv_vwredsum_vs_i16m1_i32m1(p1, vzero, vl);\n        vint32m1_t vs_2 = __riscv_vwredsum_vs_i16m1_i32m1(p2, vzero, vl);\n        vint32m1_t vs_3 = __riscv_vwredsum_vs_i16m1_i32m1(p3, vzero, vl);\n\n        isum += __riscv_vmv_x_s_i32m1_i32(vs_0) * scales[0];\n        isum += __riscv_vmv_x_s_i32m1_i32(vs_1) * scales[2];\n        isum += __riscv_vmv_x_s_i32m1_i32(vs_2) * scales[1];\n        isum += __riscv_vmv_x_s_i32m1_i32(vs_3) * scales[3];\n\n        sumf += d * isum;\n\n    }\n\n    *s = sumf;\n\n#else\n\n    int8_t  aux8[QK_K];\n    int16_t aux16[8];\n    float   sums [8];\n    int32_t aux32[8];\n    int32_t scales[4];\n    memset(sums, 0, 8*sizeof(float));\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n        const uint8_t * restrict q3 = x[i].qs;\n        const uint8_t * restrict hm = x[i].hmask;\n        const  int8_t * restrict q8 = y[i].qs;\n        int8_t * restrict a = aux8;\n        for (int l = 0; l < 8; ++l) {\n            a[l+ 0] = (int8_t)((q3[l+0] >> 0) & 3) - (hm[l] & 0x01 ? 0 : 4);\n            a[l+ 8] = (int8_t)((q3[l+8] >> 0) & 3) - (hm[l] & 0x02 ? 0 : 4);\n            a[l+16] = (int8_t)((q3[l+0] >> 2) & 3) - (hm[l] & 0x04 ? 0 : 4);\n            a[l+24] = (int8_t)((q3[l+8] >> 2) & 3) - (hm[l] & 0x08 ? 0 : 4);\n            a[l+32] = (int8_t)((q3[l+0] >> 4) & 3) - (hm[l] & 0x10 ? 0 : 4);\n            a[l+40] = (int8_t)((q3[l+8] >> 4) & 3) - (hm[l] & 0x20 ? 0 : 4);\n            a[l+48] = (int8_t)((q3[l+0] >> 6) & 3) - (hm[l] & 0x40 ? 0 : 4);\n            a[l+56] = (int8_t)((q3[l+8] >> 6) & 3) - (hm[l] & 0x80 ? 0 : 4);\n        }\n\n        scales[0] = (x[i].scales[0] & 0xF) - 8;\n        scales[1] = (x[i].scales[0] >>  4) - 8;\n        scales[2] = (x[i].scales[1] & 0xF) - 8;\n        scales[3] = (x[i].scales[1] >>  4) - 8;\n\n        memset(aux32, 0, 8*sizeof(int32_t));\n        for (int j = 0; j < QK_K/16; ++j) {\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] += q8[l] * a[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux32[l] += scales[j] * aux16[l];\n        }\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n        for (int l = 0; l < 8; ++l) sums[l] += d * aux32[l];\n    }\n    for (int l = 0; l < 8; ++l) sumf += sums[l];\n    *s = sumf;\n\n#endif\n\n}\n#endif\n\n#if QK_K == 256\nvoid ggml_vec_dot_q4_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    assert(n % QK_K == 0);\n\n    const block_q4_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n    static const uint32_t kmask1 = 0x3f3f3f3f;\n    static const uint32_t kmask2 = 0x0f0f0f0f;\n    static const uint32_t kmask3 = 0x03030303;\n\n    uint32_t utmp[4];\n\n#ifdef __ARM_NEON\n\n    const uint8x16_t m4b = vdupq_n_u8(0xf);\n#ifdef __ARM_FEATURE_DOTPROD\n    const int32x4_t mzero = vdupq_n_s32(0);\n#endif\n\n    ggml_int8x16x2_t q4bytes;\n    ggml_int8x16x2_t q8bytes;\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const int16x8_t q8sums = vpaddq_s16(vld1q_s16(y[i].bsums), vld1q_s16(y[i].bsums + 8));\n\n        memcpy(utmp, x[i].scales, 12);\n\n        uint32x2_t mins8 = { 0 };\n        mins8 = vset_lane_u32(utmp[1] & kmask1, mins8, 0);\n        mins8 = vset_lane_u32(((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4), mins8, 1);\n\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[0] &= kmask1;\n\n        const int16x8_t mins = vreinterpretq_s16_u16(vmovl_u8(vreinterpret_u8_u32(mins8)));\n        const int32x4_t prod = vaddq_s32(vmull_s16(vget_low_s16 (q8sums), vget_low_s16 (mins)),\n                                         vmull_s16(vget_high_s16(q8sums), vget_high_s16(mins)));\n        sumf -= dmin * vaddvq_s32(prod);\n\n        const uint8_t * scales = (const uint8_t *)utmp;\n\n        const uint8_t * restrict q4 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        int32_t sumi1 = 0;\n        int32_t sumi2 = 0;\n\n        for (int j = 0; j < QK_K/64; ++j) {\n\n            const ggml_uint8x16x2_t q4bits = ggml_vld1q_u8_x2(q4); q4 += 32;\n\n#ifdef __ARM_FEATURE_DOTPROD\n            q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\n            q4bytes.val[0] = vreinterpretq_s8_u8(vandq_u8  (q4bits.val[0], m4b));\n            q4bytes.val[1] = vreinterpretq_s8_u8(vandq_u8  (q4bits.val[1], m4b));\n\n            const int32x4_t p1 = vdotq_s32(vdotq_s32(mzero, q4bytes.val[0], q8bytes.val[0]), q4bytes.val[1], q8bytes.val[1]);\n            sumi1 += vaddvq_s32(p1) * scales[2*j+0];\n\n            q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\n            q4bytes.val[0] = vreinterpretq_s8_u8(vshrq_n_u8(q4bits.val[0], 4));\n            q4bytes.val[1] = vreinterpretq_s8_u8(vshrq_n_u8(q4bits.val[1], 4));\n\n            const int32x4_t p2 = vdotq_s32(vdotq_s32(mzero, q4bytes.val[0], q8bytes.val[0]), q4bytes.val[1], q8bytes.val[1]);\n\n            sumi2 += vaddvq_s32(p2) * scales[2*j+1];\n#else\n            q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\n            q4bytes.val[0] = vreinterpretq_s8_u8(vandq_u8  (q4bits.val[0], m4b));\n            q4bytes.val[1] = vreinterpretq_s8_u8(vandq_u8  (q4bits.val[1], m4b));\n            const int16x8_t p0 = vaddq_s16(vmull_s8(vget_low_s8 (q4bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                           vmull_s8(vget_high_s8(q4bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n            const int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q4bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                           vmull_s8(vget_high_s8(q4bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n            sumi1 += vaddvq_s16(vaddq_s16(p0, p1)) * scales[2*j+0];\n\n            q8bytes = ggml_vld1q_s8_x2(q8); q8 += 32;\n            q4bytes.val[0] = vreinterpretq_s8_u8(vshrq_n_u8(q4bits.val[0], 4));\n            q4bytes.val[1] = vreinterpretq_s8_u8(vshrq_n_u8(q4bits.val[1], 4));\n            const int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q4bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                           vmull_s8(vget_high_s8(q4bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n            const int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q4bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                           vmull_s8(vget_high_s8(q4bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n            sumi2 += vaddvq_s16(vaddq_s16(p2, p3)) * scales[2*j+1];\n\n#endif\n        }\n\n        sumf += d * (sumi1 + sumi2);\n\n    }\n\n    *s = sumf;\n\n#elif defined __AVX2__\n\n    const __m256i m4 = _mm256_set1_epi8(0xF);\n\n    __m256 acc = _mm256_setzero_ps();\n    __m128 acc_m = _mm_setzero_ps();\n\n   for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n\n        const uint8_t * restrict q4 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const __m256i mins_and_scales = _mm256_cvtepu8_epi16(_mm_set_epi32(utmp[3], utmp[2], utmp[1], utmp[0]));\n\n        const __m256i q8sums = _mm256_loadu_si256((const __m256i*)y[i].bsums);\n        const __m128i q8s = _mm_hadd_epi16(_mm256_extracti128_si256(q8sums, 0), _mm256_extracti128_si256(q8sums, 1));\n        const __m128i prod = _mm_madd_epi16(_mm256_extracti128_si256(mins_and_scales, 1), q8s);\n        acc_m = _mm_fmadd_ps(_mm_set1_ps(dmin), _mm_cvtepi32_ps(prod), acc_m);\n\n        const __m128i sc128  = _mm256_extracti128_si256(mins_and_scales, 0);\n        const __m256i scales = MM256_SET_M128I(sc128, sc128);\n\n        __m256i sumi = _mm256_setzero_si256();\n\n        for (int j = 0; j < QK_K/64; ++j) {\n\n            const __m256i scale_l = _mm256_shuffle_epi8(scales, get_scale_shuffle_k4(2*j+0));\n            const __m256i scale_h = _mm256_shuffle_epi8(scales, get_scale_shuffle_k4(2*j+1));\n\n            const __m256i q4bits = _mm256_loadu_si256((const __m256i*)q4); q4 += 32;\n            const __m256i q4l = _mm256_and_si256(q4bits, m4);\n            const __m256i q4h = _mm256_and_si256(_mm256_srli_epi16(q4bits, 4), m4);\n\n            const __m256i q8l = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            __m256i p16l = _mm256_maddubs_epi16(q4l, q8l);\n            p16l = _mm256_madd_epi16(scale_l, p16l);\n\n            const __m256i q8h = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            __m256i p16h = _mm256_maddubs_epi16(q4h, q8h);\n            p16h = _mm256_madd_epi16(scale_h, p16h);\n            const __m256i sumj = _mm256_add_epi32(p16l, p16h);\n\n            sumi = _mm256_add_epi32(sumi, sumj);\n        }\n\n        __m256 vd = _mm256_set1_ps(d);\n        acc = _mm256_fmadd_ps(vd, _mm256_cvtepi32_ps(sumi), acc);\n\n    }\n\n    acc_m = _mm_add_ps(acc_m, _mm_movehl_ps(acc_m, acc_m));\n    acc_m = _mm_add_ss(acc_m, _mm_movehdup_ps(acc_m));\n\n    *s = hsum_float_8(acc) + _mm_cvtss_f32(acc_m);\n\n#elif defined __AVX__\n\n    const __m128i m4 = _mm_set1_epi8(0xF);\n    const __m128i m2 = _mm_set1_epi8(0x2);\n\n    __m256 acc = _mm256_setzero_ps();\n    __m128 acc_m = _mm_setzero_ps();\n\n   for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const uint8_t * restrict q4 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n\n        const __m128i utmps = _mm_set_epi32(utmp[3], utmp[2], utmp[1], utmp[0]);\n        const __m128i scales = _mm_cvtepu8_epi16(utmps);\n        const __m128i mins = _mm_cvtepu8_epi16(_mm_unpackhi_epi64(utmps, utmps));\n\n        const __m128i q8sums_0 = _mm_loadu_si128((const __m128i*)&y[i].bsums[0]);\n        const __m128i q8sums_1 = _mm_loadu_si128((const __m128i*)&y[i].bsums[8]);\n        const __m128i q8s = _mm_hadd_epi16(q8sums_0, q8sums_1);\n        const __m128i prod = _mm_madd_epi16(mins, q8s);\n        acc_m = _mm_add_ps(_mm_mul_ps(_mm_set1_ps(dmin), _mm_cvtepi32_ps(prod)), acc_m);\n\n        __m128i sumi_0 = _mm_setzero_si128();\n        __m128i sumi_1 = _mm_setzero_si128();\n\n        __m128i shuffle = _mm_set1_epi16(0x0100);\n        for (int j = 0; j < QK_K/64; ++j) {\n\n            const __m128i scale_l = _mm_shuffle_epi8(scales, shuffle);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            const __m128i scale_h = _mm_shuffle_epi8(scales, shuffle);\n            shuffle = _mm_add_epi16(shuffle, m2);\n\n            __m128i q4bits = _mm_loadu_si128((const __m128i*)q4); q4 += 16;\n            const __m128i q4l_0 = _mm_and_si128(q4bits, m4);\n            const __m128i q4h_0 = _mm_and_si128(_mm_srli_epi16(q4bits, 4), m4);\n            q4bits = _mm_loadu_si128((const __m128i*)q4); q4 += 16;\n            const __m128i q4l_1 = _mm_and_si128(q4bits, m4);\n            const __m128i q4h_1 = _mm_and_si128(_mm_srli_epi16(q4bits, 4), m4);\n\n            const __m128i q8l_0 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            __m128i p16l = _mm_maddubs_epi16(q4l_0, q8l_0);\n            p16l = _mm_madd_epi16(scale_l, p16l);\n            sumi_0 = _mm_add_epi32(sumi_0, p16l);\n            const __m128i q8l_1 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            p16l = _mm_maddubs_epi16(q4l_1, q8l_1);\n            p16l = _mm_madd_epi16(scale_l, p16l);\n            sumi_1 = _mm_add_epi32(sumi_1, p16l);\n\n            const __m128i q8h_0 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            __m128i p16h = _mm_maddubs_epi16(q4h_0, q8h_0);\n            p16h = _mm_madd_epi16(scale_h, p16h);\n            sumi_0 = _mm_add_epi32(sumi_0, p16h);\n            const __m128i q8h_1 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            p16h = _mm_maddubs_epi16(q4h_1, q8h_1);\n            p16h = _mm_madd_epi16(scale_h, p16h);\n            sumi_1 = _mm_add_epi32(sumi_1, p16h);\n\n        }\n\n        __m256 vd = _mm256_set1_ps(d);\n        __m256i sumi = MM256_SET_M128I(sumi_1, sumi_0);\n        acc = _mm256_add_ps(_mm256_mul_ps(vd, _mm256_cvtepi32_ps(sumi)), acc);\n\n    }\n\n    acc_m = _mm_add_ps(acc_m, _mm_movehl_ps(acc_m, acc_m));\n    acc_m = _mm_add_ss(acc_m, _mm_movehdup_ps(acc_m));\n\n    *s = hsum_float_8(acc) + _mm_cvtss_f32(acc_m);\n\n#elif defined __riscv_v_intrinsic\n\n    const uint8_t * scales = (const uint8_t*)&utmp[0];\n    const uint8_t * mins   = (const uint8_t*)&utmp[2];\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        size_t vl = 8;\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        vint16mf2_t q8sums_0 = __riscv_vlse16_v_i16mf2(y[i].bsums, 4, vl);\n        vint16mf2_t q8sums_1 = __riscv_vlse16_v_i16mf2(y[i].bsums+1, 4, vl);\n        vint16mf2_t q8sums   = __riscv_vadd_vv_i16mf2(q8sums_0, q8sums_1, vl);\n\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n\n        vuint8mf4_t mins8  = __riscv_vle8_v_u8mf4(mins, vl);\n        vint16mf2_t v_mins = __riscv_vreinterpret_v_u16mf2_i16mf2(__riscv_vzext_vf2_u16mf2(mins8, vl));\n        vint32m1_t  prod   = __riscv_vwmul_vv_i32m1(q8sums, v_mins, vl);\n\n        vint32m1_t sumi = __riscv_vredsum_vs_i32m1_i32m1(prod, __riscv_vmv_v_x_i32m1(0, 1), vl);\n        sumf -= dmin * __riscv_vmv_x_s_i32m1_i32(sumi);\n\n        const uint8_t * restrict q4 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        vl = 32;\n\n        int32_t sum_1 = 0;\n        int32_t sum_2 = 0;\n\n        vint16m1_t vzero = __riscv_vmv_v_x_i16m1(0, 1);\n\n        for (int j = 0; j < QK_K/64; ++j) {\n            // load Q4\n            vuint8m1_t q4_x = __riscv_vle8_v_u8m1(q4, vl);\n\n            // load Q8 and multiply it with lower Q4 nibble\n            vint8m1_t  q8_0 = __riscv_vle8_v_i8m1(q8, vl);\n            vint8m1_t  q4_0 = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vand_vx_u8m1(q4_x, 0x0F, vl));\n            vint16m2_t qv_0 = __riscv_vwmul_vv_i16m2(q4_0, q8_0, vl);\n            vint16m1_t vs_0 = __riscv_vredsum_vs_i16m2_i16m1(qv_0, vzero, vl);\n\n            sum_1 += __riscv_vmv_x_s_i16m1_i16(vs_0) * scales[2*j+0];\n\n            // load Q8 and multiply it with upper Q4 nibble\n            vint8m1_t  q8_1 = __riscv_vle8_v_i8m1(q8+32, vl);\n            vint8m1_t  q4_1 = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vsrl_vx_u8m1(q4_x, 0x04, vl));\n            vint16m2_t qv_1 = __riscv_vwmul_vv_i16m2(q4_1, q8_1, vl);\n            vint16m1_t vs_1 = __riscv_vredsum_vs_i16m2_i16m1(qv_1, vzero, vl);\n\n            sum_2 += __riscv_vmv_x_s_i16m1_i16(vs_1) * scales[2*j+1];\n\n            q4 += 32;    q8 += 64;\n\n        }\n\n        sumf += d*(sum_1 + sum_2);\n\n    }\n\n    *s = sumf;\n\n#else\n\n\n    const uint8_t * scales = (const uint8_t*)&utmp[0];\n    const uint8_t * mins   = (const uint8_t*)&utmp[2];\n\n    int8_t  aux8[QK_K];\n    int16_t aux16[8];\n    float   sums [8];\n    int32_t aux32[8];\n    memset(sums, 0, 8*sizeof(float));\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n        const uint8_t * restrict q4 = x[i].qs;\n        const  int8_t * restrict q8 = y[i].qs;\n        memset(aux32, 0, 8*sizeof(int32_t));\n        int8_t * restrict a = aux8;\n        for (int j = 0; j < QK_K/64; ++j) {\n            for (int l = 0; l < 32; ++l) a[l] = (int8_t)(q4[l] & 0xF);\n            a += 32;\n            for (int l = 0; l < 32; ++l) a[l] = (int8_t)(q4[l]  >> 4);\n            a += 32; q4 += 32;\n        }\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n\n        int sumi = 0;\n        for (int j = 0; j < QK_K/16; ++j) sumi += y[i].bsums[j] * mins[j/2];\n        a = aux8;\n        int is = 0;\n        for (int j = 0; j < QK_K/32; ++j) {\n            int32_t scale = scales[is++];\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n        }\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n        for (int l = 0; l < 8; ++l) sums[l] += d * aux32[l];\n        const float dmin = GGML_FP16_TO_FP32(x[i].dmin) * y[i].d;\n        sumf -= dmin * sumi;\n    }\n    for (int l = 0; l < 8; ++l) sumf += sums[l];\n    *s = sumf;\n#endif\n}\n#else\nvoid ggml_vec_dot_q4_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    assert(n % QK_K == 0);\n\n    const block_q4_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n#ifdef __ARM_NEON\n\n    const uint8x16_t m4b = vdupq_n_u8(0xf);\n\n#ifdef __ARM_FEATURE_DOTPROD\n    const int32x4_t mzero = vdupq_n_s32(0);\n#endif\n\n    float sumf = 0;\n\n    ggml_int8x16x2_t q4bytes;\n    ggml_int8x16x4_t q8bytes;\n\n    float sum_mins = 0.f;\n\n    uint16_t aux16[2];\n    const uint8_t * restrict scales = (const uint8_t *)aux16;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * restrict q4 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const uint16_t * restrict a = (const uint16_t *)x[i].scales;\n        aux16[0] = a[0] & 0x0f0f;\n        aux16[1] = (a[0] >> 4) & 0x0f0f;\n\n        const int32_t summi = scales[2] * (y[i].bsums[0] + y[i].bsums[1]) + scales[3] * (y[i].bsums[2] + y[i].bsums[3]);\n        sum_mins += y[i].d * (float)x[i].d[1] * summi;\n\n        const float d = y[i].d * (float)x[i].d[0];\n\n        const ggml_uint8x16x2_t q4bits = ggml_vld1q_u8_x2(q4);\n\n#ifdef __ARM_FEATURE_DOTPROD\n        q8bytes = ggml_vld1q_s8_x4(q8);\n        q4bytes.val[0] = vreinterpretq_s8_u8(vandq_u8  (q4bits.val[0], m4b));\n        q4bytes.val[1] = vreinterpretq_s8_u8(vandq_u8  (q4bits.val[1], m4b));\n\n        const int32x4_t p1 = vdotq_s32(vdotq_s32(mzero, q4bytes.val[0], q8bytes.val[0]), q4bytes.val[1], q8bytes.val[1]);\n        const int32_t sumi1 = vaddvq_s32(p1) * scales[0];\n\n        q4bytes.val[0] = vreinterpretq_s8_u8(vshrq_n_u8(q4bits.val[0], 4));\n        q4bytes.val[1] = vreinterpretq_s8_u8(vshrq_n_u8(q4bits.val[1], 4));\n\n        const int32x4_t p2 = vdotq_s32(vdotq_s32(mzero, q4bytes.val[0], q8bytes.val[2]), q4bytes.val[1], q8bytes.val[3]);\n        const int32_t sumi2 = vaddvq_s32(p2) * scales[1];\n\n#else\n        q8bytes = ggml_vld1q_s8_x4(q8);\n        q4bytes.val[0] = vreinterpretq_s8_u8(vandq_u8  (q4bits.val[0], m4b));\n        q4bytes.val[1] = vreinterpretq_s8_u8(vandq_u8  (q4bits.val[1], m4b));\n        const int16x8_t p0 = vaddq_s16(vmull_s8(vget_low_s8 (q4bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                       vmull_s8(vget_high_s8(q4bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n        const int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q4bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                       vmull_s8(vget_high_s8(q4bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n        int32_t sumi1 = vaddvq_s16(vaddq_s16(p0, p1)) * scales[0];\n\n        q4bytes.val[0] = vreinterpretq_s8_u8(vshrq_n_u8(q4bits.val[0], 4));\n        q4bytes.val[1] = vreinterpretq_s8_u8(vshrq_n_u8(q4bits.val[1], 4));\n        const int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q4bytes.val[0]), vget_low_s8 (q8bytes.val[2])),\n                                       vmull_s8(vget_high_s8(q4bytes.val[0]), vget_high_s8(q8bytes.val[2])));\n        const int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q4bytes.val[1]), vget_low_s8 (q8bytes.val[3])),\n                                       vmull_s8(vget_high_s8(q4bytes.val[1]), vget_high_s8(q8bytes.val[3])));\n        int32_t sumi2 = vaddvq_s16(vaddq_s16(p2, p3)) * scales[1];\n\n#endif\n        sumf += d * (sumi1 + sumi2);\n\n    }\n\n    *s = sumf - sum_mins;\n\n#elif defined __AVX2__\n\n    const __m256i m4 = _mm256_set1_epi8(0xF);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    float summs = 0;\n\n    uint16_t aux16[2];\n    const uint8_t * scales = (const uint8_t *)aux16;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = GGML_FP16_TO_FP32(x[i].d[0]) * y[i].d;\n        const float m = GGML_FP16_TO_FP32(x[i].d[1]) * y[i].d;\n        const __m256 vd = _mm256_set1_ps(d);\n\n        const uint16_t * a = (const uint16_t *)x[i].scales;\n        aux16[0] = a[0] & 0x0f0f;\n        aux16[1] = (a[0] >> 4) & 0x0f0f;\n\n        summs += m * (scales[2] * (y[i].bsums[0] + y[i].bsums[1]) + scales[3] * (y[i].bsums[2] + y[i].bsums[3]));\n\n        const uint8_t * restrict q4 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const __m256i q4bits = _mm256_loadu_si256((const __m256i*)q4);\n        const __m256i q4l = _mm256_and_si256(q4bits, m4);\n        const __m256i q4h = _mm256_and_si256(_mm256_srli_epi16(q4bits, 4), m4);\n\n        const __m256i q8l = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8h = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        const __m256i p16l = _mm256_maddubs_epi16(q4l, q8l);\n        const __m256i p16h = _mm256_maddubs_epi16(q4h, q8h);\n\n        const __m256i p32l = _mm256_madd_epi16(_mm256_set1_epi16(scales[0]), p16l);\n        acc = _mm256_fmadd_ps(vd, _mm256_cvtepi32_ps(p32l), acc);\n\n        const __m256i p32h = _mm256_madd_epi16(_mm256_set1_epi16(scales[1]), p16h);\n        acc = _mm256_fmadd_ps(vd, _mm256_cvtepi32_ps(p32h), acc);\n\n    }\n\n    *s = hsum_float_8(acc) - summs;\n\n#elif defined __AVX__\n\n    const __m128i m4 = _mm_set1_epi8(0xF);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    float summs = 0;\n\n    uint16_t aux16[2];\n    const uint8_t * scales = (const uint8_t *)aux16;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = GGML_FP16_TO_FP32(x[i].d[0]) * y[i].d;\n        const float m = GGML_FP16_TO_FP32(x[i].d[1]) * y[i].d;\n        const __m256 vd = _mm256_set1_ps(d);\n\n        const uint16_t * a = (const uint16_t *)x[i].scales;\n        aux16[0] = a[0] & 0x0f0f;\n        aux16[1] = (a[0] >> 4) & 0x0f0f;\n\n        summs += m * (scales[2] * (y[i].bsums[0] + y[i].bsums[1]) + scales[3] * (y[i].bsums[2] + y[i].bsums[3]));\n\n        const uint8_t * restrict q4 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const __m256i q4bits = _mm256_loadu_si256((const __m256i*)q4);\n        const __m128i q4bits_0 = _mm256_extractf128_si256(q4bits, 0);\n        const __m128i q4bits_1 = _mm256_extractf128_si256(q4bits, 1);\n        const __m128i q4_0 = _mm_and_si128(q4bits_0, m4);\n        const __m128i q4_1 = _mm_and_si128(q4bits_1, m4);\n        const __m128i q4_2 = _mm_and_si128(_mm_srli_epi16(q4bits_0, 4), m4);\n        const __m128i q4_3 = _mm_and_si128(_mm_srli_epi16(q4bits_1, 4), m4);\n\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        const __m128i p16_0 = _mm_maddubs_epi16(q4_0, _mm256_extractf128_si256(q8_0, 0));\n        const __m128i p16_1 = _mm_maddubs_epi16(q4_1, _mm256_extractf128_si256(q8_0, 1));\n        const __m128i p16_2 = _mm_maddubs_epi16(q4_2, _mm256_extractf128_si256(q8_1, 0));\n        const __m128i p16_3 = _mm_maddubs_epi16(q4_3, _mm256_extractf128_si256(q8_1, 1));\n\n        const __m128i p32_0 = _mm_madd_epi16(_mm_set1_epi16(scales[0]), p16_0);\n        const __m128i p32_1 = _mm_madd_epi16(_mm_set1_epi16(scales[0]), p16_1);\n        acc = _mm256_add_ps(_mm256_mul_ps(vd, _mm256_cvtepi32_ps(MM256_SET_M128I(p32_1, p32_0))), acc);\n\n        const __m128i p32_2 = _mm_madd_epi16(_mm_set1_epi16(scales[1]), p16_2);\n        const __m128i p32_3 = _mm_madd_epi16(_mm_set1_epi16(scales[1]), p16_3);\n        acc = _mm256_add_ps(_mm256_mul_ps(vd, _mm256_cvtepi32_ps(MM256_SET_M128I(p32_3, p32_2))), acc);\n\n    }\n\n    *s = hsum_float_8(acc) - summs;\n\n#elif defined __riscv_v_intrinsic\n\n    uint16_t s16[2];\n    const uint8_t * restrict scales = (const uint8_t *)s16;\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * restrict q4 = x[i].qs;\n        const  int8_t * restrict q8 = y[i].qs;\n\n        const uint16_t * restrict b = (const uint16_t *)x[i].scales;\n        s16[0] = b[0] & 0x0f0f;\n        s16[1] = (b[0] >> 4) & 0x0f0f;\n\n        sumf -= y[i].d * GGML_FP16_TO_FP32(x[i].d[1]) * (scales[2] * (y[i].bsums[0] + y[i].bsums[1]) + scales[3] * (y[i].bsums[2] + y[i].bsums[3]));\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d[0]);\n\n        size_t vl = 32;\n\n        vint16m1_t vzero = __riscv_vmv_v_x_i16m1(0, 1);\n\n        // load Q4\n        vuint8m1_t q4_x = __riscv_vle8_v_u8m1(q4, vl);\n\n        // load Q8 and multiply it with lower Q4 nibble\n        vint8m1_t  q4_a = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vand_vx_u8m1(q4_x, 0x0F, vl));\n        vint16m2_t va_0 = __riscv_vwmul_vv_i16m2(q4_a, __riscv_vle8_v_i8m1(q8, vl), vl);\n        vint16m1_t aux1 = __riscv_vredsum_vs_i16m2_i16m1(va_0, vzero, vl);\n\n        sumf += d*scales[0]*__riscv_vmv_x_s_i16m1_i16(aux1);\n\n        // load Q8 and multiply it with upper Q4 nibble\n        vint8m1_t  q4_s = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vsrl_vx_u8m1(q4_x, 0x04, vl));\n        vint16m2_t va_1 = __riscv_vwmul_vv_i16m2(q4_s, __riscv_vle8_v_i8m1(q8+32, vl), vl);\n        vint16m1_t aux2 = __riscv_vredsum_vs_i16m2_i16m1(va_1, vzero, vl);\n\n        sumf += d*scales[1]*__riscv_vmv_x_s_i16m1_i16(aux2);\n\n    }\n\n    *s = sumf;\n\n#else\n\n    uint8_t aux8[QK_K];\n    int16_t aux16[16];\n    float   sums [8];\n    memset(sums, 0, 8*sizeof(float));\n\n    uint16_t s16[2];\n    const uint8_t * restrict scales = (const uint8_t *)s16;\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n        const uint8_t * restrict q4 = x[i].qs;\n        const  int8_t * restrict q8 = y[i].qs;\n        uint8_t * restrict a = aux8;\n        for (int l = 0; l < 32; ++l) a[l+ 0] = q4[l] & 0xF;\n        for (int l = 0; l < 32; ++l) a[l+32] = q4[l]  >> 4;\n\n        const uint16_t * restrict b = (const uint16_t *)x[i].scales;\n        s16[0] = b[0] & 0x0f0f;\n        s16[1] = (b[0] >> 4) & 0x0f0f;\n\n        sumf -= y[i].d * GGML_FP16_TO_FP32(x[i].d[1]) * (scales[2] * (y[i].bsums[0] + y[i].bsums[1]) + scales[3] * (y[i].bsums[2] + y[i].bsums[3]));\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d[0]);\n\n        for (int j = 0; j < QK_K/32; ++j) {\n            for (int l = 0; l < 16; ++l) aux16[l] = q8[l] * a[l];\n            q8 += 16; a += 16;\n            for (int l = 0; l < 16; ++l) aux16[l] += q8[l] * a[l];\n            q8 += 16; a += 16;\n            const float dl = d * scales[j];\n            for (int l = 0; l < 8; ++l) sums[l] += dl * (aux16[l] + aux16[l+8]);\n        }\n    }\n    for (int l = 0; l < 8; ++l) sumf += sums[l];\n    *s = sumf;\n#endif\n}\n#endif\n\n#if QK_K == 256\nvoid ggml_vec_dot_q5_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    assert(n % QK_K == 0);\n\n    const block_q5_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n    static const uint32_t kmask1 = 0x3f3f3f3f;\n    static const uint32_t kmask2 = 0x0f0f0f0f;\n    static const uint32_t kmask3 = 0x03030303;\n\n    uint32_t utmp[4];\n\n\n#ifdef __ARM_NEON\n\n    const uint8x16_t m4b = vdupq_n_u8(0xf);\n    const uint8x16_t mone = vdupq_n_u8(1);\n    const uint8x16_t mtwo = vdupq_n_u8(2);\n#if defined(__ARM_FEATURE_DOTPROD)\n    const int32x4_t mzero = vdupq_n_s32(0);\n#endif\n\n    ggml_int8x16x4_t q5bytes;\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const int16x8_t q8sums = vpaddq_s16(vld1q_s16(y[i].bsums), vld1q_s16(y[i].bsums + 8));\n\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n\n        const uint8x8_t mins8 = vld1_u8((const uint8_t*)utmp + 8);\n        const int16x8_t mins = vreinterpretq_s16_u16(vmovl_u8(mins8));\n        const int32x4_t prod = vaddq_s32(vmull_s16(vget_low_s16 (q8sums), vget_low_s16 (mins)),\n                                         vmull_s16(vget_high_s16(q8sums), vget_high_s16(mins)));\n        int32_t sumi_mins = vaddvq_s32(prod);\n\n        const uint8_t * scales = (const uint8_t *)utmp;\n\n        const uint8_t * restrict q5 = x[i].qs;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh);\n\n        ggml_uint8x16x4_t q5h;\n\n        int32_t sumi = 0;\n\n        for (int j = 0; j < QK_K/64; ++j) {\n\n            const ggml_uint8x16x2_t q5bits = ggml_vld1q_u8_x2(q5); q5 += 32;\n            const ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\n\n            q5h.val[0] = vshlq_n_u8(vandq_u8(mone, qhbits.val[0]), 4);\n            q5h.val[1] = vshlq_n_u8(vandq_u8(mone, qhbits.val[1]), 4);\n            q5h.val[2] = vshlq_n_u8(vandq_u8(mtwo, qhbits.val[0]), 3);\n            q5h.val[3] = vshlq_n_u8(vandq_u8(mtwo, qhbits.val[1]), 3);\n            qhbits.val[0] = vshrq_n_u8(qhbits.val[0], 2);\n            qhbits.val[1] = vshrq_n_u8(qhbits.val[1], 2);\n\n            q5bytes.val[0] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q5bits.val[0], m4b), q5h.val[0]));\n            q5bytes.val[1] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q5bits.val[1], m4b), q5h.val[1]));\n            q5bytes.val[2] = vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q5bits.val[0], 4), q5h.val[2]));\n            q5bytes.val[3] = vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q5bits.val[1], 4), q5h.val[3]));\n\n#if defined(__ARM_FEATURE_DOTPROD)\n\n            sumi += vaddvq_s32(vdotq_s32(vdotq_s32(mzero, q5bytes.val[0], q8bytes.val[0]), q5bytes.val[1], q8bytes.val[1])) * *scales++;\n            sumi += vaddvq_s32(vdotq_s32(vdotq_s32(mzero, q5bytes.val[2], q8bytes.val[2]), q5bytes.val[3], q8bytes.val[3])) * *scales++;\n#else\n\n            const int16x8_t p0 = vaddq_s16(vmull_s8(vget_low_s8 (q5bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                           vmull_s8(vget_high_s8(q5bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n            const int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q5bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                           vmull_s8(vget_high_s8(q5bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n            sumi += vaddvq_s16(vaddq_s16(p0, p1)) * *scales++;\n\n            const int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q5bytes.val[2]), vget_low_s8 (q8bytes.val[2])),\n                                           vmull_s8(vget_high_s8(q5bytes.val[2]), vget_high_s8(q8bytes.val[2])));\n            const int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q5bytes.val[3]), vget_low_s8 (q8bytes.val[3])),\n                                           vmull_s8(vget_high_s8(q5bytes.val[3]), vget_high_s8(q8bytes.val[3])));\n            sumi += vaddvq_s16(vaddq_s16(p2, p3)) * *scales++;\n#endif\n        }\n\n        sumf += d * sumi - dmin * sumi_mins;\n\n    }\n\n    *s = sumf;\n\n#elif defined __AVX2__\n\n    const __m256i m4 = _mm256_set1_epi8(0xF);\n    const __m128i mzero = _mm_setzero_si128();\n    const __m256i mone  = _mm256_set1_epi8(1);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    float summs = 0.f;\n\n   for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * restrict q5 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n#if QK_K == 256\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n#else\n        // TODO\n        const float d = 0, dmin = 0;\n#endif\n\n        const __m256i mins_and_scales = _mm256_cvtepu8_epi16(_mm_set_epi32(utmp[3], utmp[2], utmp[1], utmp[0]));\n\n        const __m256i q8sums = _mm256_loadu_si256((const __m256i*)y[i].bsums);\n        const __m128i q8s = _mm_hadd_epi16(_mm256_extracti128_si256(q8sums, 0), _mm256_extracti128_si256(q8sums, 1));\n        const __m128i prod = _mm_madd_epi16(_mm256_extracti128_si256(mins_and_scales, 1), q8s);\n        const __m128i hsum = _mm_hadd_epi32(_mm_hadd_epi32(prod, mzero), mzero);\n        summs += dmin * _mm_extract_epi32(hsum, 0);\n\n        const __m128i sc128  = _mm256_extracti128_si256(mins_and_scales, 0);\n        const __m256i scales = MM256_SET_M128I(sc128, sc128);\n\n        const __m256i hbits = _mm256_loadu_si256((const __m256i*)x[i].qh);\n        __m256i hmask = mone;\n\n        __m256i sumi = _mm256_setzero_si256();\n\n        int bit = 0;\n\n        for (int j = 0; j < QK_K/64; ++j) {\n\n            const __m256i scale_0 = _mm256_shuffle_epi8(scales, get_scale_shuffle_k4(2*j+0));\n            const __m256i scale_1 = _mm256_shuffle_epi8(scales, get_scale_shuffle_k4(2*j+1));\n\n            const __m256i q5bits = _mm256_loadu_si256((const __m256i*)q5); q5 += 32;\n\n            const __m256i q5l_0 = _mm256_and_si256(q5bits, m4);\n            const __m256i q5h_0 = _mm256_slli_epi16(_mm256_srli_epi16(_mm256_and_si256(hbits, hmask), bit++), 4);\n            const __m256i q5_0  = _mm256_add_epi8(q5l_0, q5h_0);\n            hmask = _mm256_slli_epi16(hmask, 1);\n\n            const __m256i q5l_1 = _mm256_and_si256(_mm256_srli_epi16(q5bits, 4), m4);\n            const __m256i q5h_1 = _mm256_slli_epi16(_mm256_srli_epi16(_mm256_and_si256(hbits, hmask), bit++), 4);\n            const __m256i q5_1  = _mm256_add_epi8(q5l_1, q5h_1);\n            hmask = _mm256_slli_epi16(hmask, 1);\n\n            const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n\n            __m256i p16_0 = _mm256_maddubs_epi16(q5_0, q8_0);\n            __m256i p16_1 = _mm256_maddubs_epi16(q5_1, q8_1);\n\n            p16_0 = _mm256_madd_epi16(scale_0, p16_0);\n            p16_1 = _mm256_madd_epi16(scale_1, p16_1);\n\n            sumi = _mm256_add_epi32(sumi, _mm256_add_epi32(p16_0, p16_1));\n\n        }\n\n        __m256 vd = _mm256_set1_ps(d);\n        acc = _mm256_fmadd_ps(vd, _mm256_cvtepi32_ps(sumi), acc);\n\n    }\n\n    *s = hsum_float_8(acc) + summs;\n\n#elif defined __AVX__\n\n    const __m128i m4 = _mm_set1_epi8(0xF);\n    const __m128i mzero = _mm_setzero_si128();\n    const __m128i mone  = _mm_set1_epi8(1);\n    const __m128i m2 = _mm_set1_epi8(2);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    float summs = 0.f;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const float dmin = -y[i].d * GGML_FP16_TO_FP32(x[i].dmin);\n\n        const uint8_t * restrict q5 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n\n        const __m128i utmps = _mm_set_epi32(utmp[3], utmp[2], utmp[1], utmp[0]);\n        const __m128i scales = _mm_cvtepu8_epi16(utmps);\n        const __m128i mins = _mm_cvtepu8_epi16(_mm_unpackhi_epi64(utmps, utmps));\n\n        const __m128i q8sums_0 = _mm_loadu_si128((const __m128i*)&y[i].bsums[0]);\n        const __m128i q8sums_1 = _mm_loadu_si128((const __m128i*)&y[i].bsums[8]);\n        const __m128i q8s = _mm_hadd_epi16(q8sums_0, q8sums_1);\n        const __m128i prod = _mm_madd_epi16(mins, q8s);\n        const __m128i hsum = _mm_hadd_epi32(_mm_hadd_epi32(prod, mzero), mzero);\n        summs += dmin * _mm_extract_epi32(hsum, 0);\n\n        const __m128i hbits_0 = _mm_loadu_si128((const __m128i*)&x[i].qh[0]);\n        const __m128i hbits_1 = _mm_loadu_si128((const __m128i*)&x[i].qh[16]);\n        __m128i hmask = mone;\n\n        __m128i sumi_0 = _mm_setzero_si128();\n        __m128i sumi_1 = _mm_setzero_si128();\n\n        int bit = 0;\n\n        __m128i shuffle = _mm_set1_epi16(0x0100);\n        for (int j = 0; j < QK_K/64; ++j) {\n\n            const __m128i scale_0 = _mm_shuffle_epi8(scales, shuffle);\n            shuffle = _mm_add_epi16(shuffle, m2);\n            const __m128i scale_1 = _mm_shuffle_epi8(scales, shuffle);\n            shuffle = _mm_add_epi16(shuffle, m2);\n\n            const __m128i q5bits_0 = _mm_loadu_si128((const __m128i*)q5); q5 += 16;\n            const __m128i q5bits_1 = _mm_loadu_si128((const __m128i*)q5); q5 += 16;\n\n            __m128i q5l_0 = _mm_and_si128(q5bits_0, m4);\n            __m128i q5l_1 = _mm_and_si128(q5bits_1, m4);\n            __m128i q5h_0 = _mm_slli_epi16(_mm_srli_epi16(_mm_and_si128(hbits_0, hmask), bit), 4);\n            __m128i q5h_1 = _mm_slli_epi16(_mm_srli_epi16(_mm_and_si128(hbits_1, hmask), bit++), 4);\n            __m128i q5_0  = _mm_add_epi8(q5l_0, q5h_0);\n            __m128i q5_1  = _mm_add_epi8(q5l_1, q5h_1);\n            hmask = _mm_slli_epi16(hmask, 1);\n\n            __m128i q8_0 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            __m128i q8_1 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            __m128i p16_0 = _mm_maddubs_epi16(q5_0, q8_0);\n            __m128i p16_1 = _mm_maddubs_epi16(q5_1, q8_1);\n            p16_0 = _mm_madd_epi16(scale_0, p16_0);\n            p16_1 = _mm_madd_epi16(scale_0, p16_1);\n\n            q5l_0 = _mm_and_si128(_mm_srli_epi16(q5bits_0, 4), m4);\n            q5l_1 = _mm_and_si128(_mm_srli_epi16(q5bits_1, 4), m4);\n            q5h_0 = _mm_slli_epi16(_mm_srli_epi16(_mm_and_si128(hbits_0, hmask), bit), 4);\n            q5h_1 = _mm_slli_epi16(_mm_srli_epi16(_mm_and_si128(hbits_1, hmask), bit++), 4);\n            q5_0  = _mm_add_epi8(q5l_0, q5h_0);\n            q5_1  = _mm_add_epi8(q5l_1, q5h_1);\n            hmask = _mm_slli_epi16(hmask, 1);\n\n            q8_0 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            q8_1 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            __m128i p16_2 = _mm_maddubs_epi16(q5_0, q8_0);\n            __m128i p16_3 = _mm_maddubs_epi16(q5_1, q8_1);\n            p16_2 = _mm_madd_epi16(scale_1, p16_2);\n            p16_3 = _mm_madd_epi16(scale_1, p16_3);\n\n            sumi_0 = _mm_add_epi32(sumi_0, _mm_add_epi32(p16_0, p16_2));\n            sumi_1 = _mm_add_epi32(sumi_1, _mm_add_epi32(p16_1, p16_3));\n\n        }\n\n        __m256 vd = _mm256_set1_ps(d);\n        __m256i sumi = MM256_SET_M128I(sumi_1, sumi_0);\n        acc = _mm256_add_ps(_mm256_mul_ps(vd, _mm256_cvtepi32_ps(sumi)), acc);\n\n    }\n\n    *s = hsum_float_8(acc) + summs;\n\n#elif defined __riscv_v_intrinsic\n\n    const uint8_t * scales = (const uint8_t*)&utmp[0];\n    const uint8_t * mins   = (const uint8_t*)&utmp[2];\n\n    float sumf = 0;\n    float sums = 0.0;\n\n    size_t vl;\n\n    for (int i = 0; i < nb; ++i) {\n\n        vl = 8;\n\n        const uint8_t * restrict q5 = x[i].qs;\n        const uint8_t * restrict hm = x[i].qh;\n        const  int8_t * restrict q8 = y[i].qs;\n\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n        const float dmin = GGML_FP16_TO_FP32(x[i].dmin) * y[i].d;\n\n        vint16mf2_t q8sums_0 = __riscv_vlse16_v_i16mf2(y[i].bsums, 4, vl);\n        vint16mf2_t q8sums_1 = __riscv_vlse16_v_i16mf2(y[i].bsums+1, 4, vl);\n        vint16mf2_t q8sums = __riscv_vadd_vv_i16mf2(q8sums_0, q8sums_1, vl);\n\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n\n        vuint8mf4_t mins8 = __riscv_vle8_v_u8mf4(mins, vl);\n        vint16mf2_t v_mins = __riscv_vreinterpret_v_u16mf2_i16mf2(__riscv_vzext_vf2_u16mf2(mins8, vl));\n        vint32m1_t prod = __riscv_vwmul_vv_i32m1(q8sums, v_mins, vl);\n\n        vint32m1_t sumi = __riscv_vredsum_vs_i32m1_i32m1(prod, __riscv_vmv_v_x_i32m1(0, 1), vl);\n        sumf -= dmin * __riscv_vmv_x_s_i32m1_i32(sumi);\n\n        vl = 32;\n        int32_t aux32 = 0;\n        int is = 0;\n\n        uint8_t m = 1;\n        vint32m1_t vzero = __riscv_vmv_v_x_i32m1(0, 1);\n        vuint8m1_t vqh = __riscv_vle8_v_u8m1(hm, vl);\n\n        for (int j = 0; j < QK_K/64; ++j) {\n            // load Q5 and Q8\n            vuint8m1_t q5_x = __riscv_vle8_v_u8m1(q5, vl);\n            vint8m1_t  q8_y1 = __riscv_vle8_v_i8m1(q8, vl);\n            vint8m1_t  q8_y2 = __riscv_vle8_v_i8m1(q8+32, vl);\n\n            // compute mask for addition\n            vint8m1_t q5_a = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vand_vx_u8m1(q5_x, 0x0F, vl));\n            vuint8m1_t qh_m1 = __riscv_vand_vx_u8m1(vqh, m, vl);\n            vbool8_t vmask_1 = __riscv_vmsne_vx_u8m1_b8(qh_m1, 0, vl);\n            vint8m1_t q5_m1 = __riscv_vadd_vx_i8m1_m(vmask_1, q5_a, 16, vl);\n            m <<= 1;\n\n            vint8m1_t q5_l = __riscv_vreinterpret_v_u8m1_i8m1(__riscv_vsrl_vx_u8m1(q5_x, 0x04, vl));\n            vuint8m1_t qh_m2 = __riscv_vand_vx_u8m1(vqh, m, vl);\n            vbool8_t vmask_2 = __riscv_vmsne_vx_u8m1_b8(qh_m2, 0, vl);\n            vint8m1_t q5_m2 = __riscv_vadd_vx_i8m1_m(vmask_2, q5_l, 16, vl);\n            m <<= 1;\n\n            vint16m2_t v0 = __riscv_vwmul_vv_i16m2(q5_m1, q8_y1, vl);\n            vint16m2_t v1 = __riscv_vwmul_vv_i16m2(q5_m2, q8_y2, vl);\n\n            vint32m4_t vs1 = __riscv_vwmul_vx_i32m4(v0, scales[is++], vl);\n            vint32m4_t vs2 = __riscv_vwmul_vx_i32m4(v1, scales[is++], vl);\n\n            vint32m1_t vacc1 = __riscv_vredsum_vs_i32m4_i32m1(vs1, vzero, vl);\n            vint32m1_t vacc2 = __riscv_vredsum_vs_i32m4_i32m1(vs2, vzero, vl);\n\n            aux32 += __riscv_vmv_x_s_i32m1_i32(vacc1) + __riscv_vmv_x_s_i32m1_i32(vacc2);\n            q5 += 32;    q8 += 64;\n\n        }\n\n        vfloat32m1_t vaux = __riscv_vfmul_vf_f32m1(__riscv_vfmv_v_f_f32m1(aux32, 1), d, 1);\n        sums += __riscv_vfmv_f_s_f32m1_f32(vaux);\n\n    }\n\n    *s = sumf+sums;\n\n#else\n\n    const uint8_t * scales = (const uint8_t*)&utmp[0];\n    const uint8_t * mins   = (const uint8_t*)&utmp[2];\n\n    int8_t  aux8[QK_K];\n    int16_t aux16[8];\n    float   sums [8];\n    int32_t aux32[8];\n    memset(sums, 0, 8*sizeof(float));\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n        const uint8_t * restrict q4 = x[i].qs;\n        const uint8_t * restrict hm = x[i].qh;\n        const  int8_t * restrict q8 = y[i].qs;\n        memset(aux32, 0, 8*sizeof(int32_t));\n        int8_t * restrict a = aux8;\n        uint8_t m = 1;\n        for (int j = 0; j < QK_K/64; ++j) {\n            for (int l = 0; l < 32; ++l) a[l] = (int8_t)(q4[l] & 0xF);\n            for (int l = 0; l < 32; ++l) a[l] += (hm[l] & m ? 16 : 0);\n            a += 32; m <<= 1;\n            for (int l = 0; l < 32; ++l) a[l] = (int8_t)(q4[l]  >> 4);\n            for (int l = 0; l < 32; ++l) a[l] += (hm[l] & m ? 16 : 0);\n            a += 32; m <<= 1;\n            q4 += 32;\n        }\n        memcpy(utmp, x[i].scales, 12);\n        utmp[3] = ((utmp[2] >> 4) & kmask2) | (((utmp[1] >> 6) & kmask3) << 4);\n        const uint32_t uaux = utmp[1] & kmask1;\n        utmp[1] = (utmp[2] & kmask2) | (((utmp[0] >> 6) & kmask3) << 4);\n        utmp[2] = uaux;\n        utmp[0] &= kmask1;\n\n        int sumi = 0;\n        for (int j = 0; j < QK_K/16; ++j) sumi += y[i].bsums[j] * mins[j/2];\n        a = aux8;\n        int is = 0;\n        for (int j = 0; j < QK_K/32; ++j) {\n            int32_t scale = scales[is++];\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n        }\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n        for (int l = 0; l < 8; ++l) sums[l] += d * aux32[l];\n        const float dmin = GGML_FP16_TO_FP32(x[i].dmin) * y[i].d;\n        sumf -= dmin * sumi;\n    }\n    for (int l = 0; l < 8; ++l) sumf += sums[l];\n    *s = sumf;\n#endif\n}\n\n#else\n\nvoid ggml_vec_dot_q5_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    assert(n % QK_K == 0);\n\n    const block_q5_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n#ifdef __ARM_NEON\n\n    const uint8x16_t m4b = vdupq_n_u8(0xf);\n    const uint8x16_t mh = vdupq_n_u8(16);\n#if defined(__ARM_FEATURE_DOTPROD)\n    const int32x4_t mzero = vdupq_n_s32(0);\n#endif\n\n    ggml_int8x16x4_t q5bytes;\n    ggml_uint8x16x4_t q5h;\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * (float)x[i].d;\n        const int8_t * sc = x[i].scales;\n\n        const uint8_t * restrict q5 = x[i].qs;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const uint8x8_t qhbits = vld1_u8(qh);\n\n        const ggml_uint8x16x2_t q5bits = ggml_vld1q_u8_x2(q5);\n        const ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8);\n\n        const uint8x16_t htmp = vcombine_u8(qhbits, vshr_n_u8(qhbits, 1));\n        q5h.val[0] = vbicq_u8(mh, vshlq_n_u8(htmp, 4));\n        q5h.val[1] = vbicq_u8(mh, vshlq_n_u8(htmp, 2));\n        q5h.val[2] = vbicq_u8(mh, htmp);\n        q5h.val[3] = vbicq_u8(mh, vshrq_n_u8(htmp, 2));\n\n        q5bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(q5bits.val[0], m4b)), vreinterpretq_s8_u8(q5h.val[0]));\n        q5bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vandq_u8(q5bits.val[1], m4b)), vreinterpretq_s8_u8(q5h.val[1]));\n        q5bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vshrq_n_u8(q5bits.val[0], 4)), vreinterpretq_s8_u8(q5h.val[2]));\n        q5bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vshrq_n_u8(q5bits.val[1], 4)), vreinterpretq_s8_u8(q5h.val[3]));\n\n#if defined(__ARM_FEATURE_DOTPROD)\n\n        int32_t sumi1 = sc[0] * vaddvq_s32(vdotq_s32(mzero, q5bytes.val[0], q8bytes.val[0]));\n        int32_t sumi2 = sc[1] * vaddvq_s32(vdotq_s32(mzero, q5bytes.val[1], q8bytes.val[1]));\n        int32_t sumi3 = sc[2] * vaddvq_s32(vdotq_s32(mzero, q5bytes.val[2], q8bytes.val[2]));\n        int32_t sumi4 = sc[3] * vaddvq_s32(vdotq_s32(mzero, q5bytes.val[3], q8bytes.val[3]));\n\n        sumf += d * (sumi1 + sumi2 + sumi3 + sumi4);\n\n#else\n\n        const int16x8_t p0 = vaddq_s16(vmull_s8(vget_low_s8 (q5bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                       vmull_s8(vget_high_s8(q5bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n        const int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q5bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                       vmull_s8(vget_high_s8(q5bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n        int32_t sumi = sc[0] * vaddvq_s16(p0) + sc[1] * vaddvq_s16(p1);\n\n        const int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q5bytes.val[2]), vget_low_s8 (q8bytes.val[2])),\n                                       vmull_s8(vget_high_s8(q5bytes.val[2]), vget_high_s8(q8bytes.val[2])));\n        const int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q5bytes.val[3]), vget_low_s8 (q8bytes.val[3])),\n                                       vmull_s8(vget_high_s8(q5bytes.val[3]), vget_high_s8(q8bytes.val[3])));\n        sumi += sc[2] * vaddvq_s16(p2) + sc[3] * vaddvq_s16(p3);\n\n        sumf += d*sumi;\n#endif\n\n    }\n\n    *s = sumf;\n\n#elif defined __AVX2__\n\n    const __m256i m4 = _mm256_set1_epi8(0xF);\n    const __m256i mone  = _mm256_set1_epi8(1);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * restrict q5 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const __m256i q5bits = _mm256_loadu_si256((const __m256i*)q5);\n\n        const __m256i scale_l = MM256_SET_M128I(_mm_set1_epi16(x[i].scales[1]), _mm_set1_epi16(x[i].scales[0]));\n        const __m256i scale_h = MM256_SET_M128I(_mm_set1_epi16(x[i].scales[3]), _mm_set1_epi16(x[i].scales[2]));\n\n        int64_t aux64;\n        memcpy(&aux64, x[i].qh, 8);\n        const __m128i haux128 = _mm_set_epi64x(aux64 >> 1, aux64);\n        const __m256i haux256 = MM256_SET_M128I(_mm_srli_epi16(haux128, 2), haux128);\n\n        const __m256i q5h_0 = _mm256_slli_epi16(_mm256_andnot_si256(haux256, mone), 4);\n        const __m256i q5h_1 = _mm256_slli_epi16(_mm256_andnot_si256(_mm256_srli_epi16(haux256, 4), mone), 4);\n\n        const __m256i q5l_0 = _mm256_and_si256(q5bits, m4);\n        const __m256i q5l_1 = _mm256_and_si256(_mm256_srli_epi16(q5bits, 4), m4);\n\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        const __m256i p16_0 = _mm256_madd_epi16(scale_l, _mm256_maddubs_epi16(q5l_0, q8_0));\n        const __m256i p16_1 = _mm256_madd_epi16(scale_h, _mm256_maddubs_epi16(q5l_1, q8_1));\n        const __m256i s16_0 = _mm256_madd_epi16(scale_l, _mm256_maddubs_epi16(q5h_0, q8_0));\n        const __m256i s16_1 = _mm256_madd_epi16(scale_h, _mm256_maddubs_epi16(q5h_1, q8_1));\n\n        const __m256i dot = _mm256_sub_epi32(_mm256_add_epi32(p16_0, p16_1), _mm256_add_epi32(s16_0, s16_1));\n\n        acc = _mm256_fmadd_ps(_mm256_set1_ps(d), _mm256_cvtepi32_ps(dot), acc);\n\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __AVX__\n\n    const __m128i m4 = _mm_set1_epi8(0xF);\n    const __m128i mone  = _mm_set1_epi8(1);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    for (int i = 0; i < nb; ++i) {\n\n        const uint8_t * restrict q5 = x[i].qs;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const __m256i q5bits = _mm256_loadu_si256((const __m256i*)q5);\n\n        const __m128i scale_0 = _mm_set1_epi16(x[i].scales[0]);\n        const __m128i scale_1 = _mm_set1_epi16(x[i].scales[1]);\n        const __m128i scale_2 = _mm_set1_epi16(x[i].scales[2]);\n        const __m128i scale_3 = _mm_set1_epi16(x[i].scales[3]);\n\n        int64_t aux64;\n        memcpy(&aux64, x[i].qh, 8);\n        const __m128i haux128_0 = _mm_set_epi64x(aux64 >> 1, aux64);\n        const __m128i haux128_1 = _mm_srli_epi16(haux128_0, 2);\n\n        const __m128i q5h_0 = _mm_slli_epi16(_mm_andnot_si128(haux128_0, mone), 4);\n        const __m128i q5h_1 = _mm_slli_epi16(_mm_andnot_si128(haux128_1, mone), 4);\n        const __m128i q5h_2 = _mm_slli_epi16(_mm_andnot_si128(_mm_srli_epi16(haux128_0, 4), mone), 4);\n        const __m128i q5h_3 = _mm_slli_epi16(_mm_andnot_si128(_mm_srli_epi16(haux128_1, 4), mone), 4);\n\n        const __m128i q5l_0 = _mm_and_si128(_mm256_extractf128_si256(q5bits, 0), m4);\n        const __m128i q5l_1 = _mm_and_si128(_mm256_extractf128_si256(q5bits, 1), m4);\n        const __m128i q5l_2 = _mm_and_si128(_mm_srli_epi16(_mm256_extractf128_si256(q5bits, 0), 4), m4);\n        const __m128i q5l_3 = _mm_and_si128(_mm_srli_epi16(_mm256_extractf128_si256(q5bits, 1), 4), m4);\n\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        const __m128i p16_0 = _mm_madd_epi16(scale_0, _mm_maddubs_epi16(q5l_0, _mm256_extractf128_si256(q8_0, 0)));\n        const __m128i p16_1 = _mm_madd_epi16(scale_1, _mm_maddubs_epi16(q5l_1, _mm256_extractf128_si256(q8_0, 1)));\n        const __m128i p16_2 = _mm_madd_epi16(scale_2, _mm_maddubs_epi16(q5l_2, _mm256_extractf128_si256(q8_1, 0)));\n        const __m128i p16_3 = _mm_madd_epi16(scale_3, _mm_maddubs_epi16(q5l_3, _mm256_extractf128_si256(q8_1, 1)));\n        const __m128i s16_0 = _mm_madd_epi16(scale_0, _mm_maddubs_epi16(q5h_0, _mm256_extractf128_si256(q8_0, 0)));\n        const __m128i s16_1 = _mm_madd_epi16(scale_1, _mm_maddubs_epi16(q5h_1, _mm256_extractf128_si256(q8_0, 1)));\n        const __m128i s16_2 = _mm_madd_epi16(scale_2, _mm_maddubs_epi16(q5h_2, _mm256_extractf128_si256(q8_1, 0)));\n        const __m128i s16_3 = _mm_madd_epi16(scale_3, _mm_maddubs_epi16(q5h_3, _mm256_extractf128_si256(q8_1, 1)));\n\n        const __m128i dot_0 = _mm_sub_epi32(_mm_add_epi32(p16_0, p16_2), _mm_add_epi32(s16_0, s16_2));\n        const __m128i dot_1 = _mm_sub_epi32(_mm_add_epi32(p16_1, p16_3), _mm_add_epi32(s16_1, s16_3));\n\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_set1_ps(d), _mm256_cvtepi32_ps(MM256_SET_M128I(dot_1, dot_0))), acc);\n\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __riscv_v_intrinsic\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * (float)x[i].d;\n        const int8_t * sc = x[i].scales;\n\n        const uint8_t * restrict q5 = x[i].qs;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        vint32m1_t vzero = __riscv_vmv_v_x_i32m1(0, 1);\n\n        // load qh\n        vuint8mf4_t qh_x1   = __riscv_vle8_v_u8mf4(qh, 8);\n        vuint8mf2_t qh_x2   = __riscv_vlmul_ext_v_u8mf4_u8mf2(__riscv_vsrl_vx_u8mf4(qh_x1, 1, 8));\n\n        size_t vl = 16;\n\n        // combine both qh_1 and qh_2\n        vuint8mf2_t qh_x = __riscv_vslideup_vx_u8mf2(__riscv_vlmul_ext_v_u8mf4_u8mf2(qh_x1), qh_x2, vl/2, vl);\n\n        vuint8mf2_t qh_h0 = __riscv_vand_vx_u8mf2(__riscv_vnot_v_u8mf2(__riscv_vsll_vx_u8mf2(qh_x, 0x4, vl), vl), 16, vl);\n        vuint8mf2_t qh_h1 = __riscv_vand_vx_u8mf2(__riscv_vnot_v_u8mf2(__riscv_vsll_vx_u8mf2(qh_x, 0x2, vl), vl), 16, vl);\n        vuint8mf2_t qh_h2 = __riscv_vand_vx_u8mf2(__riscv_vnot_v_u8mf2(qh_x, vl), 16, vl);\n        vuint8mf2_t qh_h3 = __riscv_vand_vx_u8mf2(__riscv_vnot_v_u8mf2(__riscv_vsrl_vx_u8mf2(qh_x, 0x4, vl), vl), 16, vl);\n\n        vint8mf2_t qh_0 = __riscv_vreinterpret_v_u8mf2_i8mf2(qh_h0);\n        vint8mf2_t qh_1 = __riscv_vreinterpret_v_u8mf2_i8mf2(qh_h1);\n        vint8mf2_t qh_2 = __riscv_vreinterpret_v_u8mf2_i8mf2(qh_h2);\n        vint8mf2_t qh_3 = __riscv_vreinterpret_v_u8mf2_i8mf2(qh_h3);\n\n        // load q5\n        vuint8mf2_t q5_x1  = __riscv_vle8_v_u8mf2(q5, vl);\n        vuint8mf2_t q5_x2  = __riscv_vle8_v_u8mf2(q5+16, vl);\n\n        vint8mf2_t q5s_0 = __riscv_vreinterpret_v_u8mf2_i8mf2(__riscv_vand_vx_u8mf2(q5_x1, 0xF, vl));\n        vint8mf2_t q5s_1 = __riscv_vreinterpret_v_u8mf2_i8mf2(__riscv_vand_vx_u8mf2(q5_x2, 0xF, vl));\n        vint8mf2_t q5s_2 = __riscv_vreinterpret_v_u8mf2_i8mf2(__riscv_vsrl_vx_u8mf2(q5_x1, 0x4, vl));\n        vint8mf2_t q5s_3 = __riscv_vreinterpret_v_u8mf2_i8mf2(__riscv_vsrl_vx_u8mf2(q5_x2, 0x4, vl));\n\n        vint8mf2_t q5_0 = __riscv_vsub_vv_i8mf2(q5s_0, qh_0, vl);\n        vint8mf2_t q5_1 = __riscv_vsub_vv_i8mf2(q5s_1, qh_1, vl);\n        vint8mf2_t q5_2 = __riscv_vsub_vv_i8mf2(q5s_2, qh_2, vl);\n        vint8mf2_t q5_3 = __riscv_vsub_vv_i8mf2(q5s_3, qh_3, vl);\n\n        // load Q8 and multiply it with Q5\n        vint16m1_t p0 = __riscv_vwmul_vv_i16m1(q5_0, __riscv_vle8_v_i8mf2(q8, vl), vl);\n        vint16m1_t p1 = __riscv_vwmul_vv_i16m1(q5_1, __riscv_vle8_v_i8mf2(q8+16, vl), vl);\n        vint16m1_t p2 = __riscv_vwmul_vv_i16m1(q5_2, __riscv_vle8_v_i8mf2(q8+32, vl), vl);\n        vint16m1_t p3 = __riscv_vwmul_vv_i16m1(q5_3, __riscv_vle8_v_i8mf2(q8+48, vl), vl);\n\n        vint32m1_t vs_0 = __riscv_vwredsum_vs_i16m1_i32m1(p0, vzero, vl);\n        vint32m1_t vs_1 = __riscv_vwredsum_vs_i16m1_i32m1(p1, vzero, vl);\n        vint32m1_t vs_2 = __riscv_vwredsum_vs_i16m1_i32m1(p2, vzero, vl);\n        vint32m1_t vs_3 = __riscv_vwredsum_vs_i16m1_i32m1(p3, vzero, vl);\n\n        int32_t sumi1 = sc[0] * __riscv_vmv_x_s_i32m1_i32(vs_0);\n        int32_t sumi2 = sc[1] * __riscv_vmv_x_s_i32m1_i32(vs_1);\n        int32_t sumi3 = sc[2] * __riscv_vmv_x_s_i32m1_i32(vs_2);\n        int32_t sumi4 = sc[3] * __riscv_vmv_x_s_i32m1_i32(vs_3);\n\n        sumf += d * (sumi1 + sumi2 + sumi3 + sumi4);\n\n    }\n\n    *s = sumf;\n\n#else\n\n    int8_t aux8[QK_K];\n    int16_t aux16[16];\n    float   sums [8];\n    memset(sums, 0, 8*sizeof(float));\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n        const uint8_t * restrict q4 = x[i].qs;\n        const uint8_t * restrict hm = x[i].qh;\n        const  int8_t * restrict q8 = y[i].qs;\n        int8_t * restrict a = aux8;\n        for (int l = 0; l < 32; ++l) {\n            a[l+ 0] = q4[l] & 0xF;\n            a[l+32] = q4[l]  >> 4;\n        }\n        for (int is = 0; is < 8; ++is) {\n            uint8_t m = 1 << is;\n            for (int l = 0; l < 8; ++l) a[8*is + l] -= (hm[l] & m ? 0 : 16);\n        }\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n        const int8_t * restrict sc = x[i].scales;\n\n        for (int j = 0; j < QK_K/16; ++j) {\n            const float dl = d * sc[j];\n            for (int l = 0; l < 16; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l <  8; ++l) sums[l] += dl * (aux16[l] + aux16[8+l]);\n            q8 += 16; a += 16;\n        }\n    }\n    for (int l = 0; l < 8; ++l) sumf += sums[l];\n    *s = sumf;\n#endif\n}\n#endif\n\n\n#if QK_K == 256\nvoid ggml_vec_dot_q6_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    assert(n % QK_K == 0);\n\n    const block_q6_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n#ifdef __ARM_NEON\n\n    float sum = 0;\n\n    const uint8x16_t m4b = vdupq_n_u8(0xF);\n#if defined(__ARM_FEATURE_DOTPROD)\n    const int32x4_t  vzero = vdupq_n_s32(0);\n#endif\n    //const int8x16_t  m32s = vdupq_n_s8(32);\n\n    const uint8x16_t mone = vdupq_n_u8(3);\n\n    ggml_int8x16x4_t q6bytes;\n    ggml_uint8x16x4_t q6h;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d_all = GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q6 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const int8_t * restrict scale = x[i].scales;\n\n        const ggml_int16x8x2_t q8sums = ggml_vld1q_s16_x2(y[i].bsums);\n        const int8x16_t scales = vld1q_s8(scale);\n        const ggml_int16x8x2_t q6scales = {vmovl_s8(vget_low_s8(scales)), vmovl_s8(vget_high_s8(scales))};\n\n        const int32x4_t prod = vaddq_s32(vaddq_s32(vmull_s16(vget_low_s16 (q8sums.val[0]), vget_low_s16 (q6scales.val[0])),\n                                                   vmull_s16(vget_high_s16(q8sums.val[0]), vget_high_s16(q6scales.val[0]))),\n                                         vaddq_s32(vmull_s16(vget_low_s16 (q8sums.val[1]), vget_low_s16 (q6scales.val[1])),\n                                                   vmull_s16(vget_high_s16(q8sums.val[1]), vget_high_s16(q6scales.val[1]))));\n        int32_t isum_mins = vaddvq_s32(prod);\n\n        int32_t isum = 0;\n\n        for (int j = 0; j < QK_K/128; ++j) {\n\n            ggml_uint8x16x2_t qhbits = ggml_vld1q_u8_x2(qh); qh += 32;\n            ggml_uint8x16x4_t q6bits = ggml_vld1q_u8_x4(q6); q6 += 64;\n            ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\n\n            q6h.val[0] = vshlq_n_u8(vandq_u8(mone, qhbits.val[0]), 4);\n            q6h.val[1] = vshlq_n_u8(vandq_u8(mone, qhbits.val[1]), 4);\n            uint8x16_t shifted = vshrq_n_u8(qhbits.val[0], 2);\n            q6h.val[2] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n            shifted = vshrq_n_u8(qhbits.val[1], 2);\n            q6h.val[3] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n\n            //q6bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[0], m4b), q6h.val[0])), m32s);\n            //q6bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[1], m4b), q6h.val[1])), m32s);\n            //q6bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[2], m4b), q6h.val[2])), m32s);\n            //q6bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[3], m4b), q6h.val[3])), m32s);\n            q6bytes.val[0] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[0], m4b), q6h.val[0]));\n            q6bytes.val[1] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[1], m4b), q6h.val[1]));\n            q6bytes.val[2] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[2], m4b), q6h.val[2]));\n            q6bytes.val[3] = vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[3], m4b), q6h.val[3]));\n\n#if defined(__ARM_FEATURE_DOTPROD)\n\n            isum += vaddvq_s32(vdotq_s32(vzero, q6bytes.val[0], q8bytes.val[0])) * scale[0] +\n                    vaddvq_s32(vdotq_s32(vzero, q6bytes.val[1], q8bytes.val[1])) * scale[1] +\n                    vaddvq_s32(vdotq_s32(vzero, q6bytes.val[2], q8bytes.val[2])) * scale[2] +\n                    vaddvq_s32(vdotq_s32(vzero, q6bytes.val[3], q8bytes.val[3])) * scale[3];\n            scale += 4;\n\n#else\n\n            int16x8_t p0 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                     vmull_s8(vget_high_s8(q6bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n            int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                     vmull_s8(vget_high_s8(q6bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n            isum += vaddvq_s16(p0) * scale[0] + vaddvq_s16(p1) * scale[1];\n            scale += 2;\n\n            int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[2]), vget_low_s8 (q8bytes.val[2])),\n                                     vmull_s8(vget_high_s8(q6bytes.val[2]), vget_high_s8(q8bytes.val[2])));\n            int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[3]), vget_low_s8 (q8bytes.val[3])),\n                                     vmull_s8(vget_high_s8(q6bytes.val[3]), vget_high_s8(q8bytes.val[3])));\n            isum += vaddvq_s16(p2) * scale[0] + vaddvq_s16(p3) * scale[1];\n            scale += 2;\n#endif\n\n            q8bytes = ggml_vld1q_s8_x4(q8); q8 += 64;\n\n            shifted = vshrq_n_u8(qhbits.val[0], 4);\n            q6h.val[0] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n            shifted = vshrq_n_u8(qhbits.val[1], 4);\n            q6h.val[1] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n            shifted = vshrq_n_u8(qhbits.val[0], 6);\n            q6h.val[2] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n            shifted = vshrq_n_u8(qhbits.val[1], 6);\n            q6h.val[3] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n\n            //q6bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[0], 4), q6h.val[0])), m32s);\n            //q6bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[1], 4), q6h.val[1])), m32s);\n            //q6bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[2], 4), q6h.val[2])), m32s);\n            //q6bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[3], 4), q6h.val[3])), m32s);\n            q6bytes.val[0] = vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[0], 4), q6h.val[0]));\n            q6bytes.val[1] = vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[1], 4), q6h.val[1]));\n            q6bytes.val[2] = vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[2], 4), q6h.val[2]));\n            q6bytes.val[3] = vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[3], 4), q6h.val[3]));\n\n#if defined(__ARM_FEATURE_DOTPROD)\n\n            isum += vaddvq_s32(vdotq_s32(vzero, q6bytes.val[0], q8bytes.val[0])) * scale[0] +\n                    vaddvq_s32(vdotq_s32(vzero, q6bytes.val[1], q8bytes.val[1])) * scale[1] +\n                    vaddvq_s32(vdotq_s32(vzero, q6bytes.val[2], q8bytes.val[2])) * scale[2] +\n                    vaddvq_s32(vdotq_s32(vzero, q6bytes.val[3], q8bytes.val[3])) * scale[3];\n            scale += 4;\n\n            //for (int l = 0; l < 4; ++l) {\n            //    const int32x4_t p = vdotq_s32(vzero, q6bytes.val[l], q8bytes.val[l]);\n            //    isum += vaddvq_s32(p) * *scale++;\n            //}\n#else\n            p0 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                    vmull_s8(vget_high_s8(q6bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n            p1 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                    vmull_s8(vget_high_s8(q6bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n            isum += vaddvq_s16(p0) * scale[0] + vaddvq_s16(p1) * scale[1];\n            scale += 2;\n\n            p2 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[2]), vget_low_s8 (q8bytes.val[2])),\n                                    vmull_s8(vget_high_s8(q6bytes.val[2]), vget_high_s8(q8bytes.val[2])));\n            p3 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[3]), vget_low_s8 (q8bytes.val[3])),\n                                    vmull_s8(vget_high_s8(q6bytes.val[3]), vget_high_s8(q8bytes.val[3])));\n            isum += vaddvq_s16(p2) * scale[0] + vaddvq_s16(p3) * scale[1];\n            scale += 2;\n#endif\n\n        }\n        //sum += isum * d_all * y[i].d;\n        sum += d_all * y[i].d * (isum - 32 * isum_mins);\n\n    }\n    *s = sum;\n\n#elif defined __AVX2__\n\n    const __m256i m4 = _mm256_set1_epi8(0xF);\n    const __m256i m2 = _mm256_set1_epi8(3);\n    const __m256i m32s = _mm256_set1_epi8(32);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q4 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const __m128i scales = _mm_loadu_si128((const __m128i*)x[i].scales);\n\n        __m256i sumi = _mm256_setzero_si256();\n\n        int is = 0;\n\n        for (int j = 0; j < QK_K/128; ++j) {\n\n            const __m128i scale_0 = _mm_shuffle_epi8(scales, get_scale_shuffle(is + 0));\n            const __m128i scale_1 = _mm_shuffle_epi8(scales, get_scale_shuffle(is + 1));\n            const __m128i scale_2 = _mm_shuffle_epi8(scales, get_scale_shuffle(is + 2));\n            const __m128i scale_3 = _mm_shuffle_epi8(scales, get_scale_shuffle(is + 3));\n            is += 4;\n\n            const __m256i q4bits1 = _mm256_loadu_si256((const __m256i*)q4); q4 += 32;\n            const __m256i q4bits2 = _mm256_loadu_si256((const __m256i*)q4); q4 += 32;\n            const __m256i q4bitsH = _mm256_loadu_si256((const __m256i*)qh); qh += 32;\n\n            const __m256i q4h_0 = _mm256_slli_epi16(_mm256_and_si256(q4bitsH, m2), 4);\n            const __m256i q4h_1 = _mm256_slli_epi16(_mm256_and_si256(_mm256_srli_epi16(q4bitsH, 2), m2), 4);\n            const __m256i q4h_2 = _mm256_slli_epi16(_mm256_and_si256(_mm256_srli_epi16(q4bitsH, 4), m2), 4);\n            const __m256i q4h_3 = _mm256_slli_epi16(_mm256_and_si256(_mm256_srli_epi16(q4bitsH, 6), m2), 4);\n\n            const __m256i q4_0 = _mm256_or_si256(_mm256_and_si256(q4bits1, m4), q4h_0);\n            const __m256i q4_1 = _mm256_or_si256(_mm256_and_si256(q4bits2, m4), q4h_1);\n            const __m256i q4_2 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_epi16(q4bits1, 4), m4), q4h_2);\n            const __m256i q4_3 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_epi16(q4bits2, 4), m4), q4h_3);\n\n            const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_2 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n            const __m256i q8_3 = _mm256_loadu_si256((const __m256i*)q8); q8 += 32;\n\n            __m256i q8s_0 = _mm256_maddubs_epi16(m32s, q8_0);\n            __m256i q8s_1 = _mm256_maddubs_epi16(m32s, q8_1);\n            __m256i q8s_2 = _mm256_maddubs_epi16(m32s, q8_2);\n            __m256i q8s_3 = _mm256_maddubs_epi16(m32s, q8_3);\n\n            __m256i p16_0 = _mm256_maddubs_epi16(q4_0, q8_0);\n            __m256i p16_1 = _mm256_maddubs_epi16(q4_1, q8_1);\n            __m256i p16_2 = _mm256_maddubs_epi16(q4_2, q8_2);\n            __m256i p16_3 = _mm256_maddubs_epi16(q4_3, q8_3);\n\n            p16_0 = _mm256_sub_epi16(p16_0, q8s_0);\n            p16_1 = _mm256_sub_epi16(p16_1, q8s_1);\n            p16_2 = _mm256_sub_epi16(p16_2, q8s_2);\n            p16_3 = _mm256_sub_epi16(p16_3, q8s_3);\n\n            p16_0 = _mm256_madd_epi16(_mm256_cvtepi8_epi16(scale_0), p16_0);\n            p16_1 = _mm256_madd_epi16(_mm256_cvtepi8_epi16(scale_1), p16_1);\n            p16_2 = _mm256_madd_epi16(_mm256_cvtepi8_epi16(scale_2), p16_2);\n            p16_3 = _mm256_madd_epi16(_mm256_cvtepi8_epi16(scale_3), p16_3);\n\n            sumi = _mm256_add_epi32(sumi, _mm256_add_epi32(p16_0, p16_1));\n            sumi = _mm256_add_epi32(sumi, _mm256_add_epi32(p16_2, p16_3));\n\n        }\n\n        acc = _mm256_fmadd_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(sumi), acc);\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __AVX__\n\n    const __m128i m4 = _mm_set1_epi8(0xF);\n    const __m128i m3 = _mm_set1_epi8(3);\n    const __m128i m32s = _mm_set1_epi8(32);\n    const __m128i m2 = _mm_set1_epi8(2);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q4 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const __m128i scales = _mm_loadu_si128((const __m128i*)x[i].scales);\n\n        __m128i sumi_0 = _mm_setzero_si128();\n        __m128i sumi_1 = _mm_setzero_si128();\n\n        __m128i shuffle = _mm_set_epi64x(0x0101010101010101, 0x0000000000000000);\n        for (int j = 0; j < QK_K/128; ++j) {\n\n            const __m128i q4bitsH_0 = _mm_loadu_si128((const __m128i*)qh); qh += 16;\n            const __m128i q4bitsH_1 = _mm_loadu_si128((const __m128i*)qh); qh += 16;\n\n            const __m128i q4h_0 = _mm_slli_epi16(_mm_and_si128(q4bitsH_0, m3), 4);\n            const __m128i q4h_1 = _mm_slli_epi16(_mm_and_si128(q4bitsH_1, m3), 4);\n            const __m128i q4h_2 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH_0, 2), m3), 4);\n            const __m128i q4h_3 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH_1, 2), m3), 4);\n            const __m128i q4h_4 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH_0, 4), m3), 4);\n            const __m128i q4h_5 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH_1, 4), m3), 4);\n            const __m128i q4h_6 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH_0, 6), m3), 4);\n            const __m128i q4h_7 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH_1, 6), m3), 4);\n\n            const __m128i q4bits1_0 = _mm_loadu_si128((const __m128i*)q4); q4 += 16;\n            const __m128i q4bits1_1 = _mm_loadu_si128((const __m128i*)q4); q4 += 16;\n            const __m128i q4bits2_0 = _mm_loadu_si128((const __m128i*)q4); q4 += 16;\n            const __m128i q4bits2_1 = _mm_loadu_si128((const __m128i*)q4); q4 += 16;\n\n            const __m128i q4_0 = _mm_or_si128(_mm_and_si128(q4bits1_0, m4), q4h_0);\n            const __m128i q4_1 = _mm_or_si128(_mm_and_si128(q4bits1_1, m4), q4h_1);\n            const __m128i q4_2 = _mm_or_si128(_mm_and_si128(q4bits2_0, m4), q4h_2);\n            const __m128i q4_3 = _mm_or_si128(_mm_and_si128(q4bits2_1, m4), q4h_3);\n            const __m128i q4_4 = _mm_or_si128(_mm_and_si128(_mm_srli_epi16(q4bits1_0, 4), m4), q4h_4);\n            const __m128i q4_5 = _mm_or_si128(_mm_and_si128(_mm_srli_epi16(q4bits1_1, 4), m4), q4h_5);\n            const __m128i q4_6 = _mm_or_si128(_mm_and_si128(_mm_srli_epi16(q4bits2_0, 4), m4), q4h_6);\n            const __m128i q4_7 = _mm_or_si128(_mm_and_si128(_mm_srli_epi16(q4bits2_1, 4), m4), q4h_7);\n\n            const __m128i q8_0 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_1 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_2 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_3 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_4 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_5 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_6 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n            const __m128i q8_7 = _mm_loadu_si128((const __m128i*)q8); q8 += 16;\n\n            __m128i q8s_0 = _mm_maddubs_epi16(m32s, q8_0);\n            __m128i q8s_1 = _mm_maddubs_epi16(m32s, q8_1);\n            __m128i q8s_2 = _mm_maddubs_epi16(m32s, q8_2);\n            __m128i q8s_3 = _mm_maddubs_epi16(m32s, q8_3);\n            __m128i q8s_4 = _mm_maddubs_epi16(m32s, q8_4);\n            __m128i q8s_5 = _mm_maddubs_epi16(m32s, q8_5);\n            __m128i q8s_6 = _mm_maddubs_epi16(m32s, q8_6);\n            __m128i q8s_7 = _mm_maddubs_epi16(m32s, q8_7);\n\n            __m128i p16_0 = _mm_maddubs_epi16(q4_0, q8_0);\n            __m128i p16_1 = _mm_maddubs_epi16(q4_1, q8_1);\n            __m128i p16_2 = _mm_maddubs_epi16(q4_2, q8_2);\n            __m128i p16_3 = _mm_maddubs_epi16(q4_3, q8_3);\n            __m128i p16_4 = _mm_maddubs_epi16(q4_4, q8_4);\n            __m128i p16_5 = _mm_maddubs_epi16(q4_5, q8_5);\n            __m128i p16_6 = _mm_maddubs_epi16(q4_6, q8_6);\n            __m128i p16_7 = _mm_maddubs_epi16(q4_7, q8_7);\n\n            p16_0 = _mm_sub_epi16(p16_0, q8s_0);\n            p16_1 = _mm_sub_epi16(p16_1, q8s_1);\n            p16_2 = _mm_sub_epi16(p16_2, q8s_2);\n            p16_3 = _mm_sub_epi16(p16_3, q8s_3);\n            p16_4 = _mm_sub_epi16(p16_4, q8s_4);\n            p16_5 = _mm_sub_epi16(p16_5, q8s_5);\n            p16_6 = _mm_sub_epi16(p16_6, q8s_6);\n            p16_7 = _mm_sub_epi16(p16_7, q8s_7);\n\n            const __m128i scale_0 = _mm_shuffle_epi8(scales, shuffle);\n            shuffle = _mm_add_epi8(shuffle, m2);\n            const __m128i scale_1 = _mm_shuffle_epi8(scales, shuffle);\n            shuffle = _mm_add_epi8(shuffle, m2);\n            const __m128i scale_2 = _mm_shuffle_epi8(scales, shuffle);\n            shuffle = _mm_add_epi8(shuffle, m2);\n            const __m128i scale_3 = _mm_shuffle_epi8(scales, shuffle);\n            shuffle = _mm_add_epi8(shuffle, m2);\n\n            p16_0 = _mm_madd_epi16(_mm_cvtepi8_epi16(scale_0), p16_0);\n            p16_1 = _mm_madd_epi16(_mm_cvtepi8_epi16(_mm_unpackhi_epi64(scale_0, scale_0)), p16_1);\n            p16_2 = _mm_madd_epi16(_mm_cvtepi8_epi16(scale_1), p16_2);\n            p16_3 = _mm_madd_epi16(_mm_cvtepi8_epi16(_mm_unpackhi_epi64(scale_1, scale_1)), p16_3);\n            p16_4 = _mm_madd_epi16(_mm_cvtepi8_epi16(scale_2), p16_4);\n            p16_5 = _mm_madd_epi16(_mm_cvtepi8_epi16(_mm_unpackhi_epi64(scale_2, scale_2)), p16_5);\n            p16_6 = _mm_madd_epi16(_mm_cvtepi8_epi16(scale_3), p16_6);\n            p16_7 = _mm_madd_epi16(_mm_cvtepi8_epi16(_mm_unpackhi_epi64(scale_3, scale_3)), p16_7);\n\n            sumi_0 = _mm_add_epi32(sumi_0, _mm_add_epi32(p16_0, p16_2));\n            sumi_1 = _mm_add_epi32(sumi_1, _mm_add_epi32(p16_1, p16_3));\n            sumi_0 = _mm_add_epi32(sumi_0, _mm_add_epi32(p16_4, p16_6));\n            sumi_1 = _mm_add_epi32(sumi_1, _mm_add_epi32(p16_5, p16_7));\n\n        }\n\n        __m256i sumi = MM256_SET_M128I(sumi_1, sumi_0);\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(sumi)), acc);\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __riscv_v_intrinsic\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n\n        const uint8_t * restrict q6 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const  int8_t * restrict q8 = y[i].qs;\n\n        const int8_t * restrict scale = x[i].scales;\n\n        size_t vl;\n\n        vint32m1_t vzero = __riscv_vmv_v_x_i32m1(0, 1);\n\n        int sum_t = 0;\n        int is = 0;\n\n        for (int j = 0; j < QK_K/128; ++j) {\n\n            vl = 32;\n\n            // load qh\n            vuint8m1_t qh_x = __riscv_vle8_v_u8m1(qh, vl);\n\n            // load Q6\n            vuint8m1_t q6_0 = __riscv_vle8_v_u8m1(q6, vl);\n            vuint8m1_t q6_1 = __riscv_vle8_v_u8m1(q6+32, vl);\n\n            vuint8m1_t q6a_0 = __riscv_vand_vx_u8m1(q6_0, 0x0F, vl);\n            vuint8m1_t q6a_1 = __riscv_vand_vx_u8m1(q6_1, 0x0F, vl);\n            vuint8m1_t q6s_0 = __riscv_vsrl_vx_u8m1(q6_0, 0x04, vl);\n            vuint8m1_t q6s_1 = __riscv_vsrl_vx_u8m1(q6_1, 0x04, vl);\n\n            vuint8m1_t qh_0 = __riscv_vand_vx_u8m1(qh_x, 0x03, vl);\n            vuint8m1_t qh_1 = __riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(qh_x, 0x2, vl), 0x03 , vl);\n            vuint8m1_t qh_2 = __riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(qh_x, 0x4, vl), 0x03 , vl);\n            vuint8m1_t qh_3 = __riscv_vand_vx_u8m1(__riscv_vsrl_vx_u8m1(qh_x, 0x6, vl), 0x03 , vl);\n\n            vuint8m1_t qhi_0 = __riscv_vor_vv_u8m1(q6a_0, __riscv_vsll_vx_u8m1(qh_0, 0x04, vl), vl);\n            vuint8m1_t qhi_1 = __riscv_vor_vv_u8m1(q6a_1, __riscv_vsll_vx_u8m1(qh_1, 0x04, vl), vl);\n            vuint8m1_t qhi_2 = __riscv_vor_vv_u8m1(q6s_0, __riscv_vsll_vx_u8m1(qh_2, 0x04, vl), vl);\n            vuint8m1_t qhi_3 = __riscv_vor_vv_u8m1(q6s_1, __riscv_vsll_vx_u8m1(qh_3, 0x04, vl), vl);\n\n            vint8m1_t a_0 = __riscv_vsub_vx_i8m1(__riscv_vreinterpret_v_u8m1_i8m1(qhi_0), 32, vl);\n            vint8m1_t a_1 = __riscv_vsub_vx_i8m1(__riscv_vreinterpret_v_u8m1_i8m1(qhi_1), 32, vl);\n            vint8m1_t a_2 = __riscv_vsub_vx_i8m1(__riscv_vreinterpret_v_u8m1_i8m1(qhi_2), 32, vl);\n            vint8m1_t a_3 = __riscv_vsub_vx_i8m1(__riscv_vreinterpret_v_u8m1_i8m1(qhi_3), 32, vl);\n\n            // load Q8 and take product\n            vint16m2_t va_q_0 = __riscv_vwmul_vv_i16m2(a_0, __riscv_vle8_v_i8m1(q8, vl), vl);\n            vint16m2_t va_q_1 = __riscv_vwmul_vv_i16m2(a_1, __riscv_vle8_v_i8m1(q8+32, vl), vl);\n            vint16m2_t va_q_2 = __riscv_vwmul_vv_i16m2(a_2, __riscv_vle8_v_i8m1(q8+64, vl), vl);\n            vint16m2_t va_q_3 = __riscv_vwmul_vv_i16m2(a_3, __riscv_vle8_v_i8m1(q8+96, vl), vl);\n\n            vl = 16;\n\n            vint32m2_t vaux_0 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(va_q_0, 0), scale[is+0], vl);\n            vint32m2_t vaux_1 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(va_q_0, 1), scale[is+1], vl);\n            vint32m2_t vaux_2 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(va_q_1, 0), scale[is+2], vl);\n            vint32m2_t vaux_3 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(va_q_1, 1), scale[is+3], vl);\n            vint32m2_t vaux_4 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(va_q_2, 0), scale[is+4], vl);\n            vint32m2_t vaux_5 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(va_q_2, 1), scale[is+5], vl);\n            vint32m2_t vaux_6 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(va_q_3, 0), scale[is+6], vl);\n            vint32m2_t vaux_7 = __riscv_vwmul_vx_i32m2(__riscv_vget_v_i16m2_i16m1(va_q_3, 1), scale[is+7], vl);\n\n            vint32m1_t isum0 = __riscv_vredsum_vs_i32m2_i32m1(__riscv_vadd_vv_i32m2(vaux_0, vaux_1, vl), vzero, vl);\n            vint32m1_t isum1 = __riscv_vredsum_vs_i32m2_i32m1(__riscv_vadd_vv_i32m2(vaux_2, vaux_3, vl), isum0, vl);\n            vint32m1_t isum2 = __riscv_vredsum_vs_i32m2_i32m1(__riscv_vadd_vv_i32m2(vaux_4, vaux_5, vl), isum1, vl);\n            vint32m1_t isum3 = __riscv_vredsum_vs_i32m2_i32m1(__riscv_vadd_vv_i32m2(vaux_6, vaux_7, vl), isum2, vl);\n\n            sum_t += __riscv_vmv_x_s_i32m1_i32(isum3);\n\n            q6 += 64;   qh += 32;   q8 += 128;   is=8;\n\n        }\n\n        sumf += d * sum_t;\n\n    }\n\n    *s = sumf;\n\n#else\n\n    int8_t  aux8[QK_K];\n    int16_t aux16[8];\n    float   sums [8];\n    int32_t aux32[8];\n    memset(sums, 0, 8*sizeof(float));\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n        const uint8_t * restrict q4 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const  int8_t * restrict q8 = y[i].qs;\n        memset(aux32, 0, 8*sizeof(int32_t));\n        int8_t * restrict a = aux8;\n        for (int j = 0; j < QK_K; j += 128) {\n            for (int l = 0; l < 32; ++l) {\n                a[l +  0] = (int8_t)((q4[l +  0] & 0xF) | (((qh[l] >> 0) & 3) << 4)) - 32;\n                a[l + 32] = (int8_t)((q4[l + 32] & 0xF) | (((qh[l] >> 2) & 3) << 4)) - 32;\n                a[l + 64] = (int8_t)((q4[l +  0] >>  4) | (((qh[l] >> 4) & 3) << 4)) - 32;\n                a[l + 96] = (int8_t)((q4[l + 32] >>  4) | (((qh[l] >> 6) & 3) << 4)) - 32;\n            }\n            a  += 128;\n            q4 += 64;\n            qh += 32;\n        }\n        a = aux8;\n        int is = 0;\n        for (int j = 0; j < QK_K/16; ++j) {\n            int scale = x[i].scales[is++];\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n        }\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n        for (int l = 0; l < 8; ++l) sums[l] += d * aux32[l];\n    }\n    for (int l = 0; l < 8; ++l) sumf += sums[l];\n    *s = sumf;\n#endif\n}\n\n#else\n\nvoid ggml_vec_dot_q6_K_q8_K(const int n, float * restrict s, const void * restrict vx, const void * restrict vy) {\n    assert(n % QK_K == 0);\n\n    const block_q6_K * restrict x = vx;\n    const block_q8_K * restrict y = vy;\n\n    const int nb = n / QK_K;\n\n#ifdef __ARM_NEON\n\n    float sum = 0;\n\n    const uint8x16_t m4b = vdupq_n_u8(0xF);\n    const int8x16_t  m32s = vdupq_n_s8(32);\n#if defined(__ARM_FEATURE_DOTPROD)\n    const int32x4_t  vzero = vdupq_n_s32(0);\n#endif\n\n    const uint8x16_t mone = vdupq_n_u8(3);\n\n    ggml_int8x16x4_t q6bytes;\n    ggml_uint8x16x4_t q6h;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d_all = (float)x[i].d;\n\n        const uint8_t * restrict q6 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const int8_t * restrict scale = x[i].scales;\n\n        int32_t isum = 0;\n\n        uint8x16_t qhbits = vld1q_u8(qh);\n        ggml_uint8x16x2_t q6bits = ggml_vld1q_u8_x2(q6);\n        ggml_int8x16x4_t q8bytes = ggml_vld1q_s8_x4(q8);\n\n        q6h.val[0] = vshlq_n_u8(vandq_u8(mone, qhbits), 4);\n        uint8x16_t shifted = vshrq_n_u8(qhbits, 2);\n        q6h.val[1] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n        shifted = vshrq_n_u8(qhbits, 4);\n        q6h.val[2] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n        shifted = vshrq_n_u8(qhbits, 6);\n        q6h.val[3] = vshlq_n_u8(vandq_u8(mone, shifted), 4);\n\n        q6bytes.val[0] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[0], m4b), q6h.val[0])), m32s);\n        q6bytes.val[1] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vandq_u8(q6bits.val[1], m4b), q6h.val[1])), m32s);\n        q6bytes.val[2] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[0], 4), q6h.val[2])), m32s);\n        q6bytes.val[3] = vsubq_s8(vreinterpretq_s8_u8(vorrq_u8(vshrq_n_u8(q6bits.val[1], 4), q6h.val[3])), m32s);\n\n#if defined(__ARM_FEATURE_DOTPROD)\n\n        isum += vaddvq_s32(vdotq_s32(vzero, q6bytes.val[0], q8bytes.val[0])) * scale[0] +\n                vaddvq_s32(vdotq_s32(vzero, q6bytes.val[1], q8bytes.val[1])) * scale[1] +\n                vaddvq_s32(vdotq_s32(vzero, q6bytes.val[2], q8bytes.val[2])) * scale[2] +\n                vaddvq_s32(vdotq_s32(vzero, q6bytes.val[3], q8bytes.val[3])) * scale[3];\n#else\n\n        int16x8_t p0 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[0]), vget_low_s8 (q8bytes.val[0])),\n                                 vmull_s8(vget_high_s8(q6bytes.val[0]), vget_high_s8(q8bytes.val[0])));\n        int16x8_t p1 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[1]), vget_low_s8 (q8bytes.val[1])),\n                                 vmull_s8(vget_high_s8(q6bytes.val[1]), vget_high_s8(q8bytes.val[1])));\n        isum += vaddvq_s16(p0) * scale[0] + vaddvq_s16(p1) * scale[1];\n\n        int16x8_t p2 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[2]), vget_low_s8 (q8bytes.val[2])),\n                                 vmull_s8(vget_high_s8(q6bytes.val[2]), vget_high_s8(q8bytes.val[2])));\n        int16x8_t p3 = vaddq_s16(vmull_s8(vget_low_s8 (q6bytes.val[3]), vget_low_s8 (q8bytes.val[3])),\n                                 vmull_s8(vget_high_s8(q6bytes.val[3]), vget_high_s8(q8bytes.val[3])));\n        isum += vaddvq_s16(p2) * scale[2] + vaddvq_s16(p3) * scale[3];\n#endif\n\n        sum += isum * d_all * y[i].d;\n\n    }\n    *s = sum;\n\n#elif defined __AVX2__\n\n    const __m256i m4 = _mm256_set1_epi8(0xF);\n    const __m256i m2 = _mm256_set1_epi8(3);\n    const __m256i m32s = _mm256_set1_epi8(32);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q4 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const __m64 scales_1 = _mm_set1_pi8(x[i].scales[0]);\n        const __m64 scales_2 = _mm_set1_pi8(x[i].scales[1]);\n        const __m64 scales_3 = _mm_set1_pi8(x[i].scales[2]);\n        const __m64 scales_4 = _mm_set1_pi8(x[i].scales[3]);\n\n        __m256i sumi = _mm256_setzero_si256();\n\n        const __m128i scale_0 = _mm_set_epi64(scales_2, scales_1);\n        const __m128i scale_1 = _mm_set_epi64(scales_4, scales_3);\n\n        const __m256i q4bits1 = _mm256_loadu_si256((const __m256i*)q4);\n        const __m128i q4bitsH = _mm_loadu_si128((const __m128i*)qh);\n\n        const __m256i q4h_0 = _mm256_slli_epi16(_mm256_and_si256(MM256_SET_M128I(_mm_srli_epi16(q4bitsH, 2), q4bitsH), m2), 4);\n        const __m256i q4h_1 = _mm256_slli_epi16(_mm256_and_si256(MM256_SET_M128I(_mm_srli_epi16(q4bitsH, 6), _mm_srli_epi16(q4bitsH, 4)), m2), 4);\n\n        const __m256i q4_0 = _mm256_or_si256(_mm256_and_si256(q4bits1, m4), q4h_0);\n        const __m256i q4_1 = _mm256_or_si256(_mm256_and_si256(_mm256_srli_epi16(q4bits1, 4), m4), q4h_1);\n\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        __m256i q8s_0 = _mm256_maddubs_epi16(m32s, q8_0);\n        __m256i q8s_1 = _mm256_maddubs_epi16(m32s, q8_1);\n\n        __m256i p16_0 = _mm256_maddubs_epi16(q4_0, q8_0);\n        __m256i p16_1 = _mm256_maddubs_epi16(q4_1, q8_1);\n\n        p16_0 = _mm256_sub_epi16(p16_0, q8s_0);\n        p16_1 = _mm256_sub_epi16(p16_1, q8s_1);\n\n        p16_0 = _mm256_madd_epi16(_mm256_cvtepi8_epi16(scale_0), p16_0);\n        p16_1 = _mm256_madd_epi16(_mm256_cvtepi8_epi16(scale_1), p16_1);\n\n        sumi = _mm256_add_epi32(sumi, _mm256_add_epi32(p16_0, p16_1));\n\n        acc = _mm256_fmadd_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(sumi), acc);\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __AVX__\n\n    const __m128i m4 = _mm_set1_epi8(0xF);\n    const __m128i m2 = _mm_set1_epi8(3);\n    const __m128i m32s = _mm_set1_epi8(32);\n\n    __m256 acc = _mm256_setzero_ps();\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d = y[i].d * GGML_FP16_TO_FP32(x[i].d);\n\n        const uint8_t * restrict q4 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const __m64 scales_1 = _mm_set1_pi8(x[i].scales[0]);\n        const __m64 scales_2 = _mm_set1_pi8(x[i].scales[1]);\n        const __m64 scales_3 = _mm_set1_pi8(x[i].scales[2]);\n        const __m64 scales_4 = _mm_set1_pi8(x[i].scales[3]);\n\n        __m128i sumi_0 = _mm_setzero_si128();\n        __m128i sumi_1 = _mm_setzero_si128();\n\n        const __m128i scale_0 = _mm_set_epi64(scales_2, scales_1);\n        const __m128i scale_1 = _mm_set_epi64(scales_4, scales_3);\n\n        const __m256i q4bits1 = _mm256_loadu_si256((const __m256i*)q4);\n        const __m128i q4bitsH = _mm_loadu_si128((const __m128i*)qh);\n\n        const __m128i q4h_0 = _mm_slli_epi16(_mm_and_si128(q4bitsH, m2), 4);\n        const __m128i q4h_1 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH, 2), m2), 4);\n        const __m128i q4h_2 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH, 4), m2), 4);\n        const __m128i q4h_3 = _mm_slli_epi16(_mm_and_si128(_mm_srli_epi16(q4bitsH, 6), m2), 4);\n\n        const __m128i q4_0 = _mm_or_si128(_mm_and_si128(_mm256_extractf128_si256(q4bits1, 0), m4), q4h_0);\n        const __m128i q4_1 = _mm_or_si128(_mm_and_si128(_mm256_extractf128_si256(q4bits1, 1), m4), q4h_1);\n        const __m128i q4_2 = _mm_or_si128(_mm_and_si128(_mm_srli_epi16(_mm256_extractf128_si256(q4bits1, 0), 4), m4), q4h_2);\n        const __m128i q4_3 = _mm_or_si128(_mm_and_si128(_mm_srli_epi16(_mm256_extractf128_si256(q4bits1, 1), 4), m4), q4h_3);\n\n        const __m256i q8_0 = _mm256_loadu_si256((const __m256i*)(q8+ 0));\n        const __m256i q8_1 = _mm256_loadu_si256((const __m256i*)(q8+32));\n\n        __m128i q8s_0 = _mm_maddubs_epi16(m32s, _mm256_extractf128_si256(q8_0, 0));\n        __m128i q8s_1 = _mm_maddubs_epi16(m32s, _mm256_extractf128_si256(q8_0, 1));\n        __m128i q8s_2 = _mm_maddubs_epi16(m32s, _mm256_extractf128_si256(q8_1, 0));\n        __m128i q8s_3 = _mm_maddubs_epi16(m32s, _mm256_extractf128_si256(q8_1, 1));\n\n        __m128i p16_0 = _mm_maddubs_epi16(q4_0, _mm256_extractf128_si256(q8_0, 0));\n        __m128i p16_1 = _mm_maddubs_epi16(q4_1, _mm256_extractf128_si256(q8_0, 1));\n        __m128i p16_2 = _mm_maddubs_epi16(q4_2, _mm256_extractf128_si256(q8_1, 0));\n        __m128i p16_3 = _mm_maddubs_epi16(q4_3, _mm256_extractf128_si256(q8_1, 1));\n\n        p16_0 = _mm_sub_epi16(p16_0, q8s_0);\n        p16_1 = _mm_sub_epi16(p16_1, q8s_1);\n        p16_2 = _mm_sub_epi16(p16_2, q8s_2);\n        p16_3 = _mm_sub_epi16(p16_3, q8s_3);\n\n        p16_0 = _mm_madd_epi16(_mm_cvtepi8_epi16(scale_0), p16_0);\n        p16_1 = _mm_madd_epi16(_mm_cvtepi8_epi16(_mm_unpackhi_epi64(scale_0, scale_0)), p16_1);\n        p16_2 = _mm_madd_epi16(_mm_cvtepi8_epi16(scale_1), p16_2);\n        p16_3 = _mm_madd_epi16(_mm_cvtepi8_epi16(_mm_unpackhi_epi64(scale_1, scale_1)), p16_3);\n\n        sumi_0 = _mm_add_epi32(sumi_0, _mm_add_epi32(p16_0, p16_2));\n        sumi_1 = _mm_add_epi32(sumi_1, _mm_add_epi32(p16_1, p16_3));\n\n        acc = _mm256_add_ps(_mm256_mul_ps(_mm256_broadcast_ss(&d), _mm256_cvtepi32_ps(MM256_SET_M128I(sumi_1, sumi_0))), acc);\n    }\n\n    *s = hsum_float_8(acc);\n\n#elif defined __riscv_v_intrinsic\n\n    float sumf = 0;\n\n    for (int i = 0; i < nb; ++i) {\n\n        const float d_all = (float)x[i].d;\n\n        const uint8_t * restrict q6 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const int8_t  * restrict q8 = y[i].qs;\n\n        const int8_t * restrict scale = x[i].scales;\n\n        int32_t isum = 0;\n\n        size_t vl = 16;\n\n        vint32m1_t vzero = __riscv_vmv_v_x_i32m1(0, 1);\n\n        // load Q6\n        vuint8mf2_t q6_0 = __riscv_vle8_v_u8mf2(q6, vl);\n        vuint8mf2_t q6_1 = __riscv_vle8_v_u8mf2(q6+16, vl);\n\n        // load qh\n        vuint8mf2_t qh_x = __riscv_vle8_v_u8mf2(qh, vl);\n\n        vuint8mf2_t qh0 = __riscv_vsll_vx_u8mf2(__riscv_vand_vx_u8mf2(qh_x, 0x3, vl), 0x4, vl);\n        qh_x = __riscv_vsrl_vx_u8mf2(qh_x, 0x2, vl);\n        vuint8mf2_t qh1 = __riscv_vsll_vx_u8mf2(__riscv_vand_vx_u8mf2(qh_x, 0x3, vl), 0x4, vl);\n        qh_x = __riscv_vsrl_vx_u8mf2(qh_x, 0x2, vl);\n        vuint8mf2_t qh2 = __riscv_vsll_vx_u8mf2(__riscv_vand_vx_u8mf2(qh_x, 0x3, vl), 0x4, vl);\n        qh_x = __riscv_vsrl_vx_u8mf2(qh_x, 0x2, vl);\n        vuint8mf2_t qh3 = __riscv_vsll_vx_u8mf2(__riscv_vand_vx_u8mf2(qh_x, 0x3, vl), 0x4, vl);\n\n        vuint8mf2_t q6h_0 = __riscv_vor_vv_u8mf2(__riscv_vand_vx_u8mf2(q6_0, 0xF, vl), qh0, vl);\n        vuint8mf2_t q6h_1 = __riscv_vor_vv_u8mf2(__riscv_vand_vx_u8mf2(q6_1, 0xF, vl), qh1, vl);\n        vuint8mf2_t q6h_2 = __riscv_vor_vv_u8mf2(__riscv_vsrl_vx_u8mf2(q6_0, 0x4, vl), qh2, vl);\n        vuint8mf2_t q6h_3 = __riscv_vor_vv_u8mf2(__riscv_vsrl_vx_u8mf2(q6_1, 0x4, vl), qh3, vl);\n\n        vint8mf2_t q6v_0 = __riscv_vsub_vx_i8mf2(__riscv_vreinterpret_v_u8mf2_i8mf2(q6h_0), 32, vl);\n        vint8mf2_t q6v_1 = __riscv_vsub_vx_i8mf2(__riscv_vreinterpret_v_u8mf2_i8mf2(q6h_1), 32, vl);\n        vint8mf2_t q6v_2 = __riscv_vsub_vx_i8mf2(__riscv_vreinterpret_v_u8mf2_i8mf2(q6h_2), 32, vl);\n        vint8mf2_t q6v_3 = __riscv_vsub_vx_i8mf2(__riscv_vreinterpret_v_u8mf2_i8mf2(q6h_3), 32, vl);\n\n        // load Q8 and take product\n        vint16m1_t p0 = __riscv_vwmul_vv_i16m1(q6v_0, __riscv_vle8_v_i8mf2(q8, vl), vl);\n        vint16m1_t p1 = __riscv_vwmul_vv_i16m1(q6v_1, __riscv_vle8_v_i8mf2(q8+16, vl), vl);\n        vint16m1_t p2 = __riscv_vwmul_vv_i16m1(q6v_2, __riscv_vle8_v_i8mf2(q8+32, vl), vl);\n        vint16m1_t p3 = __riscv_vwmul_vv_i16m1(q6v_3, __riscv_vle8_v_i8mf2(q8+48, vl), vl);\n\n        vint32m1_t vs_0 = __riscv_vwredsum_vs_i16m1_i32m1(p0, vzero, vl);\n        vint32m1_t vs_1 = __riscv_vwredsum_vs_i16m1_i32m1(p1, vzero, vl);\n        vint32m1_t vs_2 = __riscv_vwredsum_vs_i16m1_i32m1(p2, vzero, vl);\n        vint32m1_t vs_3 = __riscv_vwredsum_vs_i16m1_i32m1(p3, vzero, vl);\n\n        isum += __riscv_vmv_x_s_i32m1_i32(vs_0) * scale[0];\n        isum += __riscv_vmv_x_s_i32m1_i32(vs_1) * scale[1];\n        isum += __riscv_vmv_x_s_i32m1_i32(vs_2) * scale[2];\n        isum += __riscv_vmv_x_s_i32m1_i32(vs_3) * scale[3];\n\n        sumf += isum * d_all * y[i].d;\n\n    }\n\n    *s = sumf;\n\n#else\n\n    int8_t  aux8[QK_K];\n    int16_t aux16[8];\n    float   sums [8];\n    int32_t aux32[8];\n    memset(sums, 0, 8*sizeof(float));\n\n    float sumf = 0;\n    for (int i = 0; i < nb; ++i) {\n        const uint8_t * restrict q4 = x[i].ql;\n        const uint8_t * restrict qh = x[i].qh;\n        const  int8_t * restrict q8 = y[i].qs;\n        memset(aux32, 0, 8*sizeof(int32_t));\n        int8_t * restrict a = aux8;\n        for (int l = 0; l < 16; ++l) {\n            a[l+ 0] = (int8_t)((q4[l+ 0] & 0xF) | (((qh[l] >> 0) & 3) << 4)) - 32;\n            a[l+16] = (int8_t)((q4[l+16] & 0xF) | (((qh[l] >> 2) & 3) << 4)) - 32;\n            a[l+32] = (int8_t)((q4[l+ 0] >>  4) | (((qh[l] >> 4) & 3) << 4)) - 32;\n            a[l+48] = (int8_t)((q4[l+16] >>  4) | (((qh[l] >> 6) & 3) << 4)) - 32;\n        }\n        int is = 0;\n        for (int j = 0; j < QK_K/16; ++j) {\n            int scale = x[i].scales[is++];\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n            for (int l = 0; l < 8; ++l) aux16[l] = q8[l] * a[l];\n            for (int l = 0; l < 8; ++l) aux32[l] += scale * aux16[l];\n            q8 += 8; a += 8;\n        }\n        const float d = GGML_FP16_TO_FP32(x[i].d) * y[i].d;\n        for (int l = 0; l < 8; ++l) sums[l] += d * aux32[l];\n    }\n    for (int l = 0; l < 8; ++l) sumf += sums[l];\n    *s = sumf;\n#endif\n}\n\n#endif\n"
        },
        {
          "name": "ggml-quants.h",
          "type": "blob",
          "size": 10.138671875,
          "content": "#pragma once\n\n#include \"ggml-impl.h\"\n\n// GGML internal header\n\n#include <stdint.h>\n#include <stddef.h>\n\n#define QK4_0 32\ntypedef struct {\n    ggml_fp16_t d;          // delta\n    uint8_t qs[QK4_0 / 2];  // nibbles / quants\n} block_q4_0;\nstatic_assert(sizeof(block_q4_0) == sizeof(ggml_fp16_t) + QK4_0 / 2, \"wrong q4_0 block size/padding\");\n\n#define QK4_1 32\ntypedef struct {\n    ggml_fp16_t d;          // delta\n    ggml_fp16_t m;          // min\n    uint8_t qs[QK4_1 / 2];  // nibbles / quants\n} block_q4_1;\nstatic_assert(sizeof(block_q4_1) == 2 * sizeof(ggml_fp16_t) + QK4_1 / 2, \"wrong q4_1 block size/padding\");\n\n#define QK5_0 32\ntypedef struct {\n    ggml_fp16_t d;         // delta\n    uint8_t qh[4];         // 5-th bit of quants\n    uint8_t qs[QK5_0 / 2]; // nibbles / quants\n} block_q5_0;\nstatic_assert(sizeof(block_q5_0) == sizeof(ggml_fp16_t) + sizeof(uint32_t) + QK5_0 / 2, \"wrong q5_0 block size/padding\");\n\n#define QK5_1 32\ntypedef struct {\n    ggml_fp16_t d;         // delta\n    ggml_fp16_t m;         // min\n    uint8_t qh[4];         // 5-th bit of quants\n    uint8_t qs[QK5_1 / 2]; // nibbles / quants\n} block_q5_1;\nstatic_assert(sizeof(block_q5_1) == 2 * sizeof(ggml_fp16_t) + sizeof(uint32_t) + QK5_1 / 2, \"wrong q5_1 block size/padding\");\n\n#define QK8_0 32\ntypedef struct {\n    ggml_fp16_t d;         // delta\n    int8_t  qs[QK8_0];     // quants\n} block_q8_0;\nstatic_assert(sizeof(block_q8_0) == sizeof(ggml_fp16_t) + QK8_0, \"wrong q8_0 block size/padding\");\n\n#define QK8_1 32\ntypedef struct {\n    float d;               // delta\n    float s;               // d * sum(qs[i])\n    int8_t  qs[QK8_1];     // quants\n} block_q8_1;\nstatic_assert(sizeof(block_q8_1) == 2*sizeof(float) + QK8_1, \"wrong q8_1 block size/padding\");\n\n//\n// Super-block quantization structures\n//\n\n// Super-block size\n#ifdef GGML_QKK_64\n#define QK_K 64\n#define K_SCALE_SIZE 4\n#else\n#define QK_K 256\n#define K_SCALE_SIZE 12\n#endif\n\n// 2-bit quantization\n// weight is represented as x = a * q + b\n// 16 blocks of 16 elements each\n// Effectively 2.5625 bits per weight\ntypedef struct {\n    uint8_t scales[QK_K/16]; // scales and mins, quantized with 4 bits\n    uint8_t qs[QK_K/4];      // quants\n    ggml_fp16_t d;           // super-block scale for quantized scales\n    ggml_fp16_t dmin;        // super-block scale for quantized mins\n} block_q2_K;\nstatic_assert(sizeof(block_q2_K) == 2*sizeof(ggml_fp16_t) + QK_K/16 + QK_K/4, \"wrong q2_K block size/padding\");\n\n// 3-bit quantization\n// weight is represented as x = a * q\n// 16 blocks of 16 elements each\n// Effectively 3.4375 bits per weight\n#ifdef GGML_QKK_64\ntypedef struct {\n    uint8_t hmask[QK_K/8];     // quants - high bit\n    uint8_t qs[QK_K/4];        // quants - low 2 bits\n    uint8_t scales[2];\n    ggml_fp16_t d;             // super-block scale\n} block_q3_K;\nstatic_assert(sizeof(block_q3_K) == sizeof(ggml_fp16_t) + QK_K / 4 + QK_K / 8 + 2, \"wrong q3_K block size/padding\");\n#else\ntypedef struct {\n    uint8_t hmask[QK_K/8];     // quants - high bit\n    uint8_t qs[QK_K/4];        // quants - low 2 bits\n    uint8_t scales[12];        // scales, quantized with 6 bits\n    ggml_fp16_t d;             // super-block scale\n} block_q3_K;\nstatic_assert(sizeof(block_q3_K) == sizeof(ggml_fp16_t) + QK_K / 4 + QK_K / 8 + 12, \"wrong q3_K block size/padding\");\n#endif\n\n// 4-bit quantization\n// 8 blocks of 32 elements each\n// weight is represented as x = a * q + b\n// Effectively 4.5 bits per weight\n#ifdef GGML_QKK_64\ntypedef struct {\n    ggml_fp16_t d[2];          // super-block scales/mins\n    uint8_t scales[2];         // 4-bit block scales/mins\n    uint8_t qs[QK_K/2];        // 4--bit quants\n} block_q4_K;\nstatic_assert(sizeof(block_q4_K) == 2*sizeof(ggml_fp16_t) + QK_K/2 + 2, \"wrong q4_K block size/padding\");\n#else\ntypedef struct {\n    ggml_fp16_t d;             // super-block scale for quantized scales\n    ggml_fp16_t dmin;          // super-block scale for quantized mins\n    uint8_t scales[K_SCALE_SIZE]; // scales and mins, quantized with 6 bits\n    uint8_t qs[QK_K/2];        // 4--bit quants\n} block_q4_K;\nstatic_assert(sizeof(block_q4_K) == 2*sizeof(ggml_fp16_t) + K_SCALE_SIZE + QK_K/2, \"wrong q4_K block size/padding\");\n#endif\n\n// 5-bit quantization\n// 8 blocks of 32 elements each\n// weight is represented as x = a * q + b\n// Effectively 5.5 bits per weight\n#ifdef GGML_QKK_64\ntypedef struct {\n    ggml_fp16_t d;               // super-block scale\n    int8_t  scales[QK_K/16];     // 8-bit block scales\n    uint8_t qh[QK_K/8];          // quants, high bit\n    uint8_t qs[QK_K/2];          // quants, low 4 bits\n} block_q5_K;\nstatic_assert(sizeof(block_q5_K) == sizeof(ggml_fp16_t) + QK_K/2 + QK_K/8 + QK_K/16, \"wrong q5_K block size/padding\");\n#else\ntypedef struct {\n    ggml_fp16_t d;               // super-block scale for quantized scales\n    ggml_fp16_t dmin;            // super-block scale for quantized mins\n    uint8_t scales[K_SCALE_SIZE];   // scales and mins, quantized with 6 bits\n    uint8_t qh[QK_K/8];          // quants, high bit\n    uint8_t qs[QK_K/2];          // quants, low 4 bits\n} block_q5_K;\nstatic_assert(sizeof(block_q5_K) == 2*sizeof(ggml_fp16_t) + K_SCALE_SIZE + QK_K/2 + QK_K/8, \"wrong q5_K block size/padding\");\n#endif\n\n// 6-bit quantization\n// weight is represented as x = a * q\n// 16 blocks of 16 elements each\n// Effectively 6.5625 bits per weight\ntypedef struct {\n    uint8_t ql[QK_K/2];      // quants, lower 4 bits\n    uint8_t qh[QK_K/4];      // quants, upper 2 bits\n    int8_t  scales[QK_K/16]; // scales, quantized with 8 bits\n    ggml_fp16_t d;           // super-block scale\n} block_q6_K;\nstatic_assert(sizeof(block_q6_K) == sizeof(ggml_fp16_t) + QK_K / 16 + 3*QK_K/4, \"wrong q6_K block size/padding\");\n\n// This is only used for intermediate quantization and dot products\ntypedef struct {\n    float   d;              // delta\n    int8_t  qs[QK_K];       // quants\n    int16_t bsums[QK_K/16]; // sum of quants in groups of 16\n} block_q8_K;\nstatic_assert(sizeof(block_q8_K) == sizeof(float) + QK_K + QK_K/16*sizeof(int16_t), \"wrong q8_K block size/padding\");\n\n\n// Quantization\nvoid quantize_row_q4_0_reference(const float * restrict x, block_q4_0 * restrict y, int k);\nvoid quantize_row_q4_1_reference(const float * restrict x, block_q4_1 * restrict y, int k);\nvoid quantize_row_q5_0_reference(const float * restrict x, block_q5_0 * restrict y, int k);\nvoid quantize_row_q5_1_reference(const float * restrict x, block_q5_1 * restrict y, int k);\nvoid quantize_row_q8_0_reference(const float * restrict x, block_q8_0 * restrict y, int k);\nvoid quantize_row_q8_1_reference(const float * restrict x, block_q8_1 * restrict y, int k);\n\nvoid quantize_row_q2_K_reference(const float * restrict x, block_q2_K * restrict y, int k);\nvoid quantize_row_q3_K_reference(const float * restrict x, block_q3_K * restrict y, int k);\nvoid quantize_row_q4_K_reference(const float * restrict x, block_q4_K * restrict y, int k);\nvoid quantize_row_q5_K_reference(const float * restrict x, block_q5_K * restrict y, int k);\nvoid quantize_row_q6_K_reference(const float * restrict x, block_q6_K * restrict y, int k);\nvoid quantize_row_q8_K_reference(const float * restrict x, block_q8_K * restrict y, int k);\n\nvoid quantize_row_q4_0(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q4_1(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q5_0(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q5_1(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q8_0(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q8_1(const float * restrict x, void * restrict y, int k);\n\nvoid quantize_row_q2_K(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q3_K(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q4_K(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q5_K(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q6_K(const float * restrict x, void * restrict y, int k);\nvoid quantize_row_q8_K(const float * restrict x, void * restrict y, int k);\n\n// Dequantization\nvoid dequantize_row_q4_0(const block_q4_0 * restrict x, float * restrict y, int k);\nvoid dequantize_row_q4_1(const block_q4_1 * restrict x, float * restrict y, int k);\nvoid dequantize_row_q5_0(const block_q5_0 * restrict x, float * restrict y, int k);\nvoid dequantize_row_q5_1(const block_q5_1 * restrict x, float * restrict y, int k);\nvoid dequantize_row_q8_0(const block_q8_0 * restrict x, float * restrict y, int k);\n//void dequantize_row_q8_1(const block_q8_1 * restrict x, float * restrict y, int k);\n\nvoid dequantize_row_q2_K(const block_q2_K * restrict x, float * restrict y, int k);\nvoid dequantize_row_q3_K(const block_q3_K * restrict x, float * restrict y, int k);\nvoid dequantize_row_q4_K(const block_q4_K * restrict x, float * restrict y, int k);\nvoid dequantize_row_q5_K(const block_q5_K * restrict x, float * restrict y, int k);\nvoid dequantize_row_q6_K(const block_q6_K * restrict x, float * restrict y, int k);\nvoid dequantize_row_q8_K(const block_q8_K * restrict x, float * restrict y, int k);\n\nvoid ggml_axpy_q4_0_q8_0(const int n, const void * restrict vx, const void * restrict vy, const void * restrict vz, int8_t alpha, ggml_fp16_t scale);\n\n// Dot product\nvoid ggml_vec_dot_q4_0_q8_0(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\nvoid ggml_vec_dot_q4_1_q8_1(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\nvoid ggml_vec_dot_q5_0_q8_0(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\nvoid ggml_vec_dot_q5_1_q8_1(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\nvoid ggml_vec_dot_q8_0_q8_0(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\n\nvoid ggml_vec_dot_q2_K_q8_K(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\nvoid ggml_vec_dot_q3_K_q8_K(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\nvoid ggml_vec_dot_q4_K_q8_K(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\nvoid ggml_vec_dot_q5_K_q8_K(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\nvoid ggml_vec_dot_q6_K_q8_K(int n, float * restrict s, const void * restrict vx, const void * restrict vy);\n"
        },
        {
          "name": "ggml.c",
          "type": "blob",
          "size": 663.8251953125,
          "content": "#define _CRT_SECURE_NO_DEPRECATE // Disables ridiculous \"unsafe\" warnigns on Windows\n#define _USE_MATH_DEFINES // For M_PI on MSVC\n\n#include \"ggml-impl.h\"\n#include \"ggml-quants.h\"\n\n#if defined(_MSC_VER) || defined(__MINGW32__)\n#include <malloc.h> // using malloc.h with MSC/MINGW\n#elif !defined(__FreeBSD__) && !defined(__NetBSD__) && !defined(__OpenBSD__)\n#include <alloca.h>\n#endif\n\n#include <assert.h>\n#include <errno.h>\n#include <time.h>\n#include <math.h>\n#include <stdlib.h>\n#include <string.h>\n#include <stdint.h>\n#include <inttypes.h>\n#include <stdio.h>\n#include <float.h>\n#include <limits.h>\n#include <stdarg.h>\n#include <signal.h>\n\n// #define _GNU_SOURCE\n// #include <sched.h>\n\n#ifdef GGML_USE_METAL\n#include <unistd.h>\n#endif\n\n#if defined(_MSC_VER)\n// disable \"possible loss of data\" to avoid hundreds of casts\n// we should just be careful :)\n#pragma warning(disable: 4244 4267)\n\n// disable POSIX deprecation warnigns\n// these functions are never going away, anyway\n#pragma warning(disable: 4996)\n#endif\n\nfloat sparse_pred_threshold = 0.;\n\n#if defined(_WIN32)\n\ntypedef HANDLE pthread_t;\n\ntypedef DWORD thread_ret_t;\nstatic int pthread_create(pthread_t * out, void * unused, thread_ret_t(*func)(void *), void * arg) {\n    (void) unused;\n    HANDLE handle = CreateThread(NULL, 0, (LPTHREAD_START_ROUTINE) func, arg, 0, NULL);\n    if (handle == NULL)\n    {\n        return EAGAIN;\n    }\n\n    *out = handle;\n    return 0;\n}\n\nstatic int pthread_join(pthread_t thread, void * unused) {\n    (void) unused;\n    int ret = (int) WaitForSingleObject(thread, INFINITE);\n    CloseHandle(thread);\n    return ret;\n}\n\nstatic int sched_yield (void) {\n    Sleep (0);\n    return 0;\n}\n#else\n#include <pthread.h>\n#include <stdatomic.h>\n\ntypedef void * thread_ret_t;\n\n#include <sys/types.h>\n#include <sys/stat.h>\n#include <unistd.h>\n\n#endif\n\n#ifdef GGML_USE_CPU_HBM\n#include <hbwmalloc.h>\n#endif\n\n#if defined(__APPLE__)\n#include <TargetConditionals.h>\n#endif\n\n#if (defined(__linux__) || defined(__APPLE__) || defined(__FreeBSD__) || defined(__NetBSD__) || defined(__OpenBSD__)) && \\\n    (!defined(TARGET_OS_TV) && !defined(TARGET_OS_WATCH))\n\n#include <sys/wait.h>\n\nvoid ggml_print_backtrace(void) {\n    /*\n    #include <execinfo.h>\n    #include <dlfcn.h>\n\n    void * trace[100];\n\n    int nptrs = backtrace(trace, sizeof(trace)/sizeof(trace[0]));\n\n    backtrace_symbols_fd(trace, nptrs, STDERR_FILENO);\n    */\n\n    // backtrack_symbols does not show line numbers, use gdb instead\n    char attach[32];\n    snprintf(attach, sizeof(attach), \"attach %d\", getpid());\n    int pid = fork();\n    if (pid == 0) {\n        execlp(\"gdb\", \"gdb\", \"--batch\",\n            \"-ex\", \"set style enabled on\",\n            \"-ex\", attach,\n            \"-ex\", \"bt -frame-info source-and-location\",\n            \"-ex\", \"detach\",\n            \"-ex\", \"quit\",\n            NULL);\n    } else {\n        waitpid(pid, NULL, 0);\n    }\n}\n#else\nvoid ggml_print_backtrace(void) {\n    // platform not supported\n}\n#endif\n\n// #define GGML_PERF\n#define GGML_DEBUG 0\n#define GGML_GELU_FP16\n#define GGML_GELU_QUICK_FP16\n#define GGML_SILU_FP16\n// #define GGML_CROSS_ENTROPY_EXP_FP16\n// #define GGML_FLASH_ATTN_EXP_FP16\n\n#define GGML_SOFT_MAX_UNROLL 4\n#define GGML_VEC_DOT_UNROLL  2\n#define GGML_VEC_MAD_UNROLL  32\n\n//\n// logging\n//\n\n#if (GGML_DEBUG >= 1)\n#define GGML_PRINT_DEBUG(...) printf(__VA_ARGS__)\n#else\n#define GGML_PRINT_DEBUG(...)\n#endif\n\n#if (GGML_DEBUG >= 5)\n#define GGML_PRINT_DEBUG_5(...) printf(__VA_ARGS__)\n#else\n#define GGML_PRINT_DEBUG_5(...)\n#endif\n\n#if (GGML_DEBUG >= 10)\n#define GGML_PRINT_DEBUG_10(...) printf(__VA_ARGS__)\n#else\n#define GGML_PRINT_DEBUG_10(...)\n#endif\n\n#define GGML_PRINT(...) printf(__VA_ARGS__)\n\n//\n// end of logging block\n//\n\n#ifdef GGML_USE_ACCELERATE\n// uncomment to use vDSP for soft max computation\n// note: not sure if it is actually faster\n//#define GGML_SOFT_MAX_ACCELERATE\n#endif\n\n#if defined(_MSC_VER) || defined(__MINGW32__)\n#define GGML_ALIGNED_MALLOC(size) _aligned_malloc(size, GGML_MEM_ALIGN)\n#define GGML_ALIGNED_FREE(ptr)    _aligned_free(ptr)\n#else\ninline static void * ggml_aligned_malloc(size_t size) {\n    if (size == 0) {\n        GGML_PRINT(\"WARNING: Behavior may be unexpected when allocating 0 bytes for ggml_aligned_malloc!\\n\");\n        return NULL;\n    }\n    void * aligned_memory = NULL;\n#ifdef GGML_USE_CPU_HBM\n    int result = hbw_posix_memalign(&aligned_memory, 16, size);\n#elif GGML_USE_METAL\n    int result = posix_memalign(&aligned_memory, sysconf(_SC_PAGESIZE), size);\n#else\n    int result = posix_memalign(&aligned_memory, GGML_MEM_ALIGN, size);\n#endif\n    if (result != 0) {\n        // Handle allocation failure\n        const char *error_desc = \"unknown allocation error\";\n        switch (result) {\n            case EINVAL:\n                error_desc = \"invalid alignment value\";\n                break;\n            case ENOMEM:\n                error_desc = \"insufficient memory\";\n                break;\n        }\n        GGML_PRINT(\"%s: %s (attempted to allocate %6.2f MB)\\n\", __func__, error_desc, size/(1024.0*1024.0));\n        return NULL;\n    }\n    return aligned_memory;\n}\n#define GGML_ALIGNED_MALLOC(size) ggml_aligned_malloc(size)\n#ifdef GGML_USE_CPU_HBM\n#define GGML_ALIGNED_FREE(ptr)    if(NULL != ptr) hbw_free(ptr)\n#else\n#define GGML_ALIGNED_FREE(ptr)    free(ptr)\n#endif\n#endif\n\n#define UNUSED GGML_UNUSED\n#define SWAP(x, y, T) do { T SWAP = x; x = y; y = SWAP; } while (0)\n\n//\n// tensor access macros\n//\n\n#define GGML_TENSOR_UNARY_OP_LOCALS \\\n    GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne) \\\n    GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb) \\\n    GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne) \\\n    GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)\n\n#define GGML_TENSOR_BINARY_OP_LOCALS \\\n    GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne) \\\n    GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb) \\\n    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne) \\\n    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb) \\\n    GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne) \\\n    GGML_TENSOR_LOCALS(size_t,  nb,  dst,  nb)\n\n#if defined(GGML_USE_ACCELERATE)\n#include <Accelerate/Accelerate.h>\n#if defined(GGML_USE_CLBLAST) // allow usage of CLBlast alongside Accelerate functions\n#include \"ggml-opencl.h\"\n#endif\n#elif defined(GGML_USE_OPENBLAS)\n#if defined(GGML_BLAS_USE_MKL)\n#include <mkl.h>\n#else\n#include <cblas.h>\n#endif\n#elif defined(GGML_USE_CUBLAS)\n#include \"ggml-cuda.h\"\n#elif defined(GGML_USE_CLBLAST)\n#include \"ggml-opencl.h\"\n#endif\n#include \"ggml.h\"\n\n// floating point type used to accumulate sums\ntypedef double ggml_float;\n\n#undef MIN\n#undef MAX\n\n#define MIN(a, b) ((a) < (b) ? (a) : (b))\n#define MAX(a, b) ((a) > (b) ? (a) : (b))\n\n//\n// global data\n//\n\n// precomputed gelu table for f16 (128 KB)\nstatic ggml_fp16_t ggml_table_gelu_f16[1 << 16];\n\n// precomputed quick gelu table for f16 (128 KB)\nstatic ggml_fp16_t ggml_table_gelu_quick_f16[1 << 16];\n\n// precomputed silu table for f16 (128 KB)\nstatic ggml_fp16_t ggml_table_silu_f16[1 << 16];\n\n// precomputed exp table for f16 (128 KB)\nstatic ggml_fp16_t ggml_table_exp_f16[1 << 16];\n\n// precomputed f32 table for f16 (256 KB) (ggml-impl.h)\nfloat ggml_table_f32_f16[1 << 16];\n\n// note: do not use these inside ggml.c\n// these are meant to be used via the ggml.h API\nfloat ggml_fp16_to_fp32(ggml_fp16_t x) {\n    return (float) GGML_FP16_TO_FP32(x);\n}\n\nggml_fp16_t ggml_fp32_to_fp16(float x) {\n    return GGML_FP32_TO_FP16(x);\n}\n\nvoid ggml_fp16_to_fp32_row(const ggml_fp16_t * x, float * y, int n) {\n    for (int i = 0; i < n; i++) {\n        y[i] = GGML_FP16_TO_FP32(x[i]);\n    }\n}\n\nvoid ggml_fp32_to_fp16_row(const float * x, ggml_fp16_t * y, int n) {\n    int i = 0;\n#if defined(__F16C__)\n    for (; i + 7 < n; i += 8) {\n        __m256 x_vec = _mm256_loadu_ps(x + i);\n        __m128i y_vec = _mm256_cvtps_ph(x_vec, _MM_FROUND_TO_NEAREST_INT);\n        _mm_storeu_si128((__m128i *)(y + i), y_vec);\n    }\n    for(; i + 3 < n; i += 4) {\n        __m128 x_vec = _mm_loadu_ps(x + i);\n        __m128i y_vec = _mm_cvtps_ph(x_vec, _MM_FROUND_TO_NEAREST_INT);\n        _mm_storel_epi64((__m128i *)(y + i), y_vec);\n    }\n#endif\n    for (; i < n; i++) {\n        y[i] = GGML_FP32_TO_FP16(x[i]);\n    }\n}\n\n//\n// timing\n//\n\n#if defined(_MSC_VER) || defined(__MINGW32__)\nstatic int64_t timer_freq, timer_start;\nvoid ggml_time_init(void) {\n    LARGE_INTEGER t;\n    QueryPerformanceFrequency(&t);\n    timer_freq = t.QuadPart;\n\n    // The multiplication by 1000 or 1000000 below can cause an overflow if timer_freq\n    // and the uptime is high enough.\n    // We subtract the program start time to reduce the likelihood of that happening.\n    QueryPerformanceCounter(&t);\n    timer_start = t.QuadPart;\n}\nint64_t ggml_time_ms(void) {\n    LARGE_INTEGER t;\n    QueryPerformanceCounter(&t);\n    return ((t.QuadPart-timer_start) * 1000) / timer_freq;\n}\nint64_t ggml_time_us(void) {\n    LARGE_INTEGER t;\n    QueryPerformanceCounter(&t);\n    return ((t.QuadPart-timer_start) * 1000000) / timer_freq;\n}\n#else\nvoid ggml_time_init(void) {}\nint64_t ggml_time_ms(void) {\n    struct timespec ts;\n    clock_gettime(CLOCK_MONOTONIC, &ts);\n    return (int64_t)ts.tv_sec*1000 + (int64_t)ts.tv_nsec/1000000;\n}\n\nint64_t ggml_time_us(void) {\n    struct timespec ts;\n    clock_gettime(CLOCK_MONOTONIC, &ts);\n    return (int64_t)ts.tv_sec*1000000 + (int64_t)ts.tv_nsec/1000;\n}\n#endif\n\nint64_t ggml_cycles(void) {\n    return clock();\n}\n\nint64_t ggml_cycles_per_ms(void) {\n    return CLOCKS_PER_SEC/1000;\n}\n\n#ifdef GGML_PERF\n#define ggml_perf_time_ms()       ggml_time_ms()\n#define ggml_perf_time_us()       ggml_time_us()\n#define ggml_perf_cycles()        0\n#define ggml_perf_cycles_per_ms() 0\n#else\n#define ggml_perf_time_ms()       0\n#define ggml_perf_time_us()       0\n#define ggml_perf_cycles()        0\n#define ggml_perf_cycles_per_ms() 0\n#endif\n\n//\n// cache line\n//\n\n#if defined(__cpp_lib_hardware_interference_size)\n#define CACHE_LINE_SIZE hardware_destructive_interference_size\n#else\n#if defined(__POWER9_VECTOR__)\n#define CACHE_LINE_SIZE 128\n#else\n#define CACHE_LINE_SIZE 64\n#endif\n#endif\n\nstatic const size_t CACHE_LINE_SIZE_F32 = CACHE_LINE_SIZE/sizeof(float);\n\nstatic void ggml_vec_dot_f32(const int n, float * restrict s, const float * restrict x, const float * restrict y);\nstatic void ggml_vec_dot_f16(const int n, float * restrict s, ggml_fp16_t * restrict x, ggml_fp16_t * restrict y);\n\nstatic const ggml_type_traits_t type_traits[GGML_TYPE_COUNT] = {\n    [GGML_TYPE_I8] = {\n        .type_name                = \"i8\",\n        .blck_size                = 1,\n        .type_size                = sizeof(int8_t),\n        .is_quantized             = false,\n    },\n    [GGML_TYPE_I16] = {\n        .type_name                = \"i16\",\n        .blck_size                = 1,\n        .type_size                = sizeof(int16_t),\n        .is_quantized             = false,\n    },\n    [GGML_TYPE_I32] = {\n        .type_name                = \"i32\",\n        .blck_size                = 1,\n        .type_size                = sizeof(int32_t),\n        .is_quantized             = false,\n    },\n    [GGML_TYPE_F32] = {\n        .type_name                = \"f32\",\n        .blck_size                = 1,\n        .type_size                = sizeof(float),\n        .is_quantized             = false,\n        .vec_dot                  = (ggml_vec_dot_t) ggml_vec_dot_f32,\n        .vec_dot_type             = GGML_TYPE_F32,\n    },\n    [GGML_TYPE_F16] = {\n        .type_name                = \"f16\",\n        .blck_size                = 1,\n        .type_size                = sizeof(ggml_fp16_t),\n        .is_quantized             = false,\n        .to_float                 = (ggml_to_float_t) ggml_fp16_to_fp32_row,\n        .from_float               = (ggml_from_float_t) ggml_fp32_to_fp16_row,\n        .from_float_reference     = (ggml_from_float_t) ggml_fp32_to_fp16_row,\n        .vec_dot                  = (ggml_vec_dot_t) ggml_vec_dot_f16,\n        .vec_dot_type             = GGML_TYPE_F16,\n    },\n    [GGML_TYPE_Q4_0] = {\n        .type_name                = \"q4_0\",\n        .blck_size                = QK4_0,\n        .type_size                = sizeof(block_q4_0),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q4_0,\n        .from_float               = quantize_row_q4_0,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q4_0_reference,\n        .vec_dot                  = ggml_vec_dot_q4_0_q8_0,\n        .vec_dot_type             = GGML_TYPE_Q8_0,\n    },\n    [GGML_TYPE_Q4_1] = {\n        .type_name                = \"q4_1\",\n        .blck_size                = QK4_1,\n        .type_size                = sizeof(block_q4_1),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q4_1,\n        .from_float               = quantize_row_q4_1,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q4_1_reference,\n        .vec_dot                  = ggml_vec_dot_q4_1_q8_1,\n        .vec_dot_type             = GGML_TYPE_Q8_1,\n    },\n    [4] = { // GGML_TYPE_Q4_2\n        .type_name                = \"DEPRECATED\",\n        .blck_size                = 0,\n        .type_size                = 0,\n        .is_quantized             = false,\n        .to_float                 = NULL,\n        .from_float               = NULL,\n        .from_float_reference     = NULL,\n        .vec_dot                  = NULL,\n        .vec_dot_type             = GGML_TYPE_COUNT,\n    },\n    [5] = { // GGML_TYPE_Q4_3\n        .type_name                = \"DEPRECATED\",\n        .blck_size                = 0,\n        .type_size                = 0,\n        .is_quantized             = false,\n        .to_float                 = NULL,\n        .from_float               = NULL,\n        .from_float_reference     = NULL,\n        .vec_dot                  = NULL,\n        .vec_dot_type             = GGML_TYPE_COUNT,\n    },\n    [GGML_TYPE_Q5_0] = {\n        .type_name                = \"q5_0\",\n        .blck_size                = QK5_0,\n        .type_size                = sizeof(block_q5_0),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q5_0,\n        .from_float               = quantize_row_q5_0,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q5_0_reference,\n        .vec_dot                  = ggml_vec_dot_q5_0_q8_0,\n        .vec_dot_type             = GGML_TYPE_Q8_0,\n    },\n    [GGML_TYPE_Q5_1] = {\n        .type_name                = \"q5_1\",\n        .blck_size                = QK5_1,\n        .type_size                = sizeof(block_q5_1),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q5_1,\n        .from_float               = quantize_row_q5_1,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q5_1_reference,\n        .vec_dot                  = ggml_vec_dot_q5_1_q8_1,\n        .vec_dot_type             = GGML_TYPE_Q8_1,\n    },\n    [GGML_TYPE_Q8_0] = {\n        .type_name                = \"q8_0\",\n        .blck_size                = QK8_0,\n        .type_size                = sizeof(block_q8_0),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q8_0,\n        .from_float               = quantize_row_q8_0,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q8_0_reference,\n        .vec_dot                  = ggml_vec_dot_q8_0_q8_0,\n        .vec_dot_type             = GGML_TYPE_Q8_0,\n    },\n    [GGML_TYPE_Q8_1] = {\n        .type_name                = \"q8_1\",\n        .blck_size                = QK8_1,\n        .type_size                = sizeof(block_q8_1),\n        .is_quantized             = true,\n        .from_float               = quantize_row_q8_1,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q8_1_reference,\n        .vec_dot_type             = GGML_TYPE_Q8_1,\n    },\n    [GGML_TYPE_Q2_K] = {\n        .type_name                = \"q2_K\",\n        .blck_size                = QK_K,\n        .type_size                = sizeof(block_q2_K),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q2_K,\n        .from_float               = quantize_row_q2_K,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q2_K_reference,\n        .vec_dot                  = ggml_vec_dot_q2_K_q8_K,\n        .vec_dot_type             = GGML_TYPE_Q8_K,\n    },\n    [GGML_TYPE_Q3_K] = {\n        .type_name                = \"q3_K\",\n        .blck_size                = QK_K,\n        .type_size                = sizeof(block_q3_K),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q3_K,\n        .from_float               = quantize_row_q3_K,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q3_K_reference,\n        .vec_dot                  = ggml_vec_dot_q3_K_q8_K,\n        .vec_dot_type             = GGML_TYPE_Q8_K,\n    },\n    [GGML_TYPE_Q4_K] = {\n        .type_name                = \"q4_K\",\n        .blck_size                = QK_K,\n        .type_size                = sizeof(block_q4_K),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q4_K,\n        .from_float               = quantize_row_q4_K,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q4_K_reference,\n        .vec_dot                  = ggml_vec_dot_q4_K_q8_K,\n        .vec_dot_type             = GGML_TYPE_Q8_K,\n    },\n    [GGML_TYPE_Q5_K] = {\n        .type_name                = \"q5_K\",\n        .blck_size                = QK_K,\n        .type_size                = sizeof(block_q5_K),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q5_K,\n        .from_float               = quantize_row_q5_K,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q5_K_reference,\n        .vec_dot                  = ggml_vec_dot_q5_K_q8_K,\n        .vec_dot_type             = GGML_TYPE_Q8_K,\n    },\n    [GGML_TYPE_Q6_K] = {\n        .type_name                = \"q6_K\",\n        .blck_size                = QK_K,\n        .type_size                = sizeof(block_q6_K),\n        .is_quantized             = true,\n        .to_float                 = (ggml_to_float_t) dequantize_row_q6_K,\n        .from_float               = quantize_row_q6_K,\n        .from_float_reference     = (ggml_from_float_t) quantize_row_q6_K_reference,\n        .vec_dot                  = ggml_vec_dot_q6_K_q8_K,\n        .vec_dot_type             = GGML_TYPE_Q8_K,\n    },\n    [GGML_TYPE_Q8_K] = {\n        .type_name                = \"q8_K\",\n        .blck_size                = QK_K,\n        .type_size                = sizeof(block_q8_K),\n        .is_quantized             = true,\n        .from_float               = quantize_row_q8_K,\n    }\n};\n\n// For internal test use\nggml_type_traits_t ggml_internal_get_type_traits(enum ggml_type type) {\n    GGML_ASSERT(type < GGML_TYPE_COUNT);\n    return type_traits[type];\n}\n\n//\n// simd mappings\n//\n\n#if defined(__ARM_NEON)\n#if !defined(__aarch64__)\n\n// 64-bit compatibility\n\ninline static float vaddvq_f32(float32x4_t v) {\n    return vgetq_lane_f32(v, 0) + vgetq_lane_f32(v, 1) + vgetq_lane_f32(v, 2) + vgetq_lane_f32(v, 3);\n}\n\n#endif\n#endif\n\n// we define a common set of C macros which map to specific intrinsics based on the current architecture\n// we then implement the fundamental computation operations below using only these macros\n// adding support for new architectures requires to define the corresponding SIMD macros\n//\n// GGML_F32_STEP / GGML_F16_STEP\n//   number of elements to process in a single step\n//\n// GGML_F32_EPR / GGML_F16_EPR\n//   number of elements to fit in a single register\n//\n\n#if defined(__ARM_NEON) && defined(__ARM_FEATURE_FMA)\n\n#define GGML_SIMD\n\n// F32 NEON\n\n#define GGML_F32_STEP 16\n#define GGML_F32_EPR  4\n\n#define GGML_F32x4              float32x4_t\n#define GGML_F32x4_ZERO         vdupq_n_f32(0.0f)\n#define GGML_F32x4_SET1(x)      vdupq_n_f32(x)\n#define GGML_F32x4_LOAD         vld1q_f32\n#define GGML_F32x4_STORE        vst1q_f32\n#define GGML_F32x4_FMA(a, b, c) vfmaq_f32(a, b, c)\n#define GGML_F32x4_ADD          vaddq_f32\n#define GGML_F32x4_MUL          vmulq_f32\n#define GGML_F32x4_REDUCE_ONE(x) vaddvq_f32(x)\n#define GGML_F32x4_REDUCE(res, x)              \\\n{                                              \\\n    int offset = GGML_F32_ARR >> 1;            \\\n    for (int i = 0; i < offset; ++i) {         \\\n        x[i] = vaddq_f32(x[i], x[offset+i]);   \\\n    }                                          \\\n    offset >>= 1;                              \\\n    for (int i = 0; i < offset; ++i) {         \\\n        x[i] = vaddq_f32(x[i], x[offset+i]);   \\\n    }                                          \\\n    offset >>= 1;                              \\\n    for (int i = 0; i < offset; ++i) {         \\\n        x[i] = vaddq_f32(x[i], x[offset+i]);   \\\n    }                                          \\\n    res = GGML_F32x4_REDUCE_ONE(x[0]);         \\\n}\n\n#define GGML_F32_VEC        GGML_F32x4\n#define GGML_F32_VEC_ZERO   GGML_F32x4_ZERO\n#define GGML_F32_VEC_SET1   GGML_F32x4_SET1\n#define GGML_F32_VEC_LOAD   GGML_F32x4_LOAD\n#define GGML_F32_VEC_STORE  GGML_F32x4_STORE\n#define GGML_F32_VEC_FMA    GGML_F32x4_FMA\n#define GGML_F32_VEC_ADD    GGML_F32x4_ADD\n#define GGML_F32_VEC_MUL    GGML_F32x4_MUL\n#define GGML_F32_VEC_REDUCE GGML_F32x4_REDUCE\n\n// F16 NEON\n\n#if defined(__ARM_FEATURE_FP16_VECTOR_ARITHMETIC)\n    #define GGML_F16_STEP 32\n    #define GGML_F16_EPR  8\n\n    #define GGML_F16x8              float16x8_t\n    #define GGML_F16x8_ZERO         vdupq_n_f16(0.0f)\n    #define GGML_F16x8_SET1(x)      vdupq_n_f16(x)\n    #define GGML_F16x8_LOAD         vld1q_f16\n    #define GGML_F16x8_STORE        vst1q_f16\n    #define GGML_F16x8_FMA(a, b, c) vfmaq_f16(a, b, c)\n    #define GGML_F16x8_ADD          vaddq_f16\n    #define GGML_F16x8_MUL          vmulq_f16\n    #define GGML_F16x8_REDUCE(res, x)                             \\\n    do {                                                          \\\n        int offset = GGML_F16_ARR >> 1;                           \\\n        for (int i = 0; i < offset; ++i) {                        \\\n            x[i] = vaddq_f16(x[i], x[offset+i]);                  \\\n        }                                                         \\\n        offset >>= 1;                                             \\\n        for (int i = 0; i < offset; ++i) {                        \\\n            x[i] = vaddq_f16(x[i], x[offset+i]);                  \\\n        }                                                         \\\n        offset >>= 1;                                             \\\n        for (int i = 0; i < offset; ++i) {                        \\\n            x[i] = vaddq_f16(x[i], x[offset+i]);                  \\\n        }                                                         \\\n        const float32x4_t t0 = vcvt_f32_f16(vget_low_f16 (x[0])); \\\n        const float32x4_t t1 = vcvt_f32_f16(vget_high_f16(x[0])); \\\n        res = (ggml_float) vaddvq_f32(vaddq_f32(t0, t1));         \\\n    } while (0)\n\n    #define GGML_F16_VEC                GGML_F16x8\n    #define GGML_F16_VEC_ZERO           GGML_F16x8_ZERO\n    #define GGML_F16_VEC_SET1           GGML_F16x8_SET1\n    #define GGML_F16_VEC_LOAD(p, i)     GGML_F16x8_LOAD(p)\n    #define GGML_F16_VEC_STORE(p, r, i) GGML_F16x8_STORE(p, r[i])\n    #define GGML_F16_VEC_FMA            GGML_F16x8_FMA\n    #define GGML_F16_VEC_ADD            GGML_F16x8_ADD\n    #define GGML_F16_VEC_MUL            GGML_F16x8_MUL\n    #define GGML_F16_VEC_REDUCE         GGML_F16x8_REDUCE\n#else\n    // if FP16 vector arithmetic is not supported, we use FP32 instead\n    // and take advantage of the vcvt_ functions to convert to/from FP16\n\n    #define GGML_F16_STEP 16\n    #define GGML_F16_EPR  4\n\n    #define GGML_F32Cx4              float32x4_t\n    #define GGML_F32Cx4_ZERO         vdupq_n_f32(0.0f)\n    #define GGML_F32Cx4_SET1(x)      vdupq_n_f32(x)\n    #define GGML_F32Cx4_LOAD(x)      vcvt_f32_f16(vld1_f16(x))\n    #define GGML_F32Cx4_STORE(x, y)  vst1_f16(x, vcvt_f16_f32(y))\n    #define GGML_F32Cx4_FMA(a, b, c) vfmaq_f32(a, b, c)\n    #define GGML_F32Cx4_ADD          vaddq_f32\n    #define GGML_F32Cx4_MUL          vmulq_f32\n    #define GGML_F32Cx4_REDUCE       GGML_F32x4_REDUCE\n\n    #define GGML_F16_VEC                GGML_F32Cx4\n    #define GGML_F16_VEC_ZERO           GGML_F32Cx4_ZERO\n    #define GGML_F16_VEC_SET1           GGML_F32Cx4_SET1\n    #define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx4_LOAD(p)\n    #define GGML_F16_VEC_STORE(p, r, i) GGML_F32Cx4_STORE(p, r[i])\n    #define GGML_F16_VEC_FMA            GGML_F32Cx4_FMA\n    #define GGML_F16_VEC_ADD            GGML_F32Cx4_ADD\n    #define GGML_F16_VEC_MUL            GGML_F32Cx4_MUL\n    #define GGML_F16_VEC_REDUCE         GGML_F32Cx4_REDUCE\n#endif\n\n#elif defined(__AVX__)\n\n#define GGML_SIMD\n\n// F32 AVX\n\n#define GGML_F32_STEP 32\n#define GGML_F32_EPR  8\n\n#define GGML_F32x8         __m256\n#define GGML_F32x8_ZERO    _mm256_setzero_ps()\n#define GGML_F32x8_SET1(x) _mm256_set1_ps(x)\n#define GGML_F32x8_LOAD    _mm256_loadu_ps\n#define GGML_F32x8_STORE   _mm256_storeu_ps\n#if defined(__FMA__)\n    #define GGML_F32x8_FMA(a, b, c) _mm256_fmadd_ps(b, c, a)\n#else\n    #define GGML_F32x8_FMA(a, b, c) _mm256_add_ps(_mm256_mul_ps(b, c), a)\n#endif\n#define GGML_F32x8_ADD     _mm256_add_ps\n#define GGML_F32x8_MUL     _mm256_mul_ps\n#define GGML_F32x8_REDUCE(res, x)                                 \\\ndo {                                                              \\\n    int offset = GGML_F32_ARR >> 1;                               \\\n    for (int i = 0; i < offset; ++i) {                            \\\n        x[i] = _mm256_add_ps(x[i], x[offset+i]);                  \\\n    }                                                             \\\n    offset >>= 1;                                                 \\\n    for (int i = 0; i < offset; ++i) {                            \\\n        x[i] = _mm256_add_ps(x[i], x[offset+i]);                  \\\n    }                                                             \\\n    offset >>= 1;                                                 \\\n    for (int i = 0; i < offset; ++i) {                            \\\n        x[i] = _mm256_add_ps(x[i], x[offset+i]);                  \\\n    }                                                             \\\n    const __m128 t0 = _mm_add_ps(_mm256_castps256_ps128(x[0]),    \\\n                                 _mm256_extractf128_ps(x[0], 1)); \\\n    const __m128 t1 = _mm_hadd_ps(t0, t0);                        \\\n    res = _mm_cvtss_f32(_mm_hadd_ps(t1, t1));                     \\\n} while (0)\n// TODO: is this optimal ?\n\n#define GGML_F32_VEC        GGML_F32x8\n#define GGML_F32_VEC_ZERO   GGML_F32x8_ZERO\n#define GGML_F32_VEC_SET1   GGML_F32x8_SET1\n#define GGML_F32_VEC_LOAD   GGML_F32x8_LOAD\n#define GGML_F32_VEC_STORE  GGML_F32x8_STORE\n#define GGML_F32_VEC_FMA    GGML_F32x8_FMA\n#define GGML_F32_VEC_ADD    GGML_F32x8_ADD\n#define GGML_F32_VEC_MUL    GGML_F32x8_MUL\n#define GGML_F32_VEC_REDUCE GGML_F32x8_REDUCE\n\n// F16 AVX\n\n#define GGML_F16_STEP 32\n#define GGML_F16_EPR  8\n\n// F16 arithmetic is not supported by AVX, so we use F32 instead\n\n#define GGML_F32Cx8             __m256\n#define GGML_F32Cx8_ZERO        _mm256_setzero_ps()\n#define GGML_F32Cx8_SET1(x)     _mm256_set1_ps(x)\n\n#if defined(__F16C__)\n// the  _mm256_cvt intrinsics require F16C\n#define GGML_F32Cx8_LOAD(x)     _mm256_cvtph_ps(_mm_loadu_si128((__m128i *)(x)))\n#define GGML_F32Cx8_STORE(x, y) _mm_storeu_si128((__m128i *)(x), _mm256_cvtps_ph(y, 0))\n#else\nstatic inline __m256 __avx_f32cx8_load(ggml_fp16_t *x) {\n    float tmp[8];\n\n    for (int i = 0; i < 8; i++) {\n        tmp[i] = GGML_FP16_TO_FP32(x[i]);\n    }\n\n    return _mm256_loadu_ps(tmp);\n}\nstatic inline void __avx_f32cx8_store(ggml_fp16_t *x, __m256 y) {\n    float arr[8];\n\n    _mm256_storeu_ps(arr, y);\n\n    for (int i = 0; i < 8; i++)\n        x[i] = GGML_FP32_TO_FP16(arr[i]);\n}\n#define GGML_F32Cx8_LOAD(x)     __avx_f32cx8_load(x)\n#define GGML_F32Cx8_STORE(x, y) __avx_f32cx8_store(x, y)\n#endif\n\n#define GGML_F32Cx8_FMA         GGML_F32x8_FMA\n#define GGML_F32Cx8_ADD         _mm256_add_ps\n#define GGML_F32Cx8_MUL         _mm256_mul_ps\n#define GGML_F32Cx8_REDUCE      GGML_F32x8_REDUCE\n\n#define GGML_F16_VEC                GGML_F32Cx8\n#define GGML_F16_VEC_ZERO           GGML_F32Cx8_ZERO\n#define GGML_F16_VEC_SET1           GGML_F32Cx8_SET1\n#define GGML_F16_VEC_LOAD(p, i)     GGML_F32Cx8_LOAD(p)\n#define GGML_F16_VEC_STORE(p, r, i) GGML_F32Cx8_STORE(p, r[i])\n#define GGML_F16_VEC_FMA            GGML_F32Cx8_FMA\n#define GGML_F16_VEC_ADD            GGML_F32Cx8_ADD\n#define GGML_F16_VEC_MUL            GGML_F32Cx8_MUL\n#define GGML_F16_VEC_REDUCE         GGML_F32Cx8_REDUCE\n\n#elif defined(__POWER9_VECTOR__)\n\n#define GGML_SIMD\n\n// F32 POWER9\n\n#define GGML_F32_STEP 32\n#define GGML_F32_EPR  4\n\n#define GGML_F32x4              vector float\n#define GGML_F32x4_ZERO         0.0f\n#define GGML_F32x4_SET1         vec_splats\n#define GGML_F32x4_LOAD(p)      vec_xl(0, p)\n#define GGML_F32x4_STORE(p, r)  vec_xst(r, 0, p)\n#define GGML_F32x4_FMA(a, b, c) vec_madd(b, c, a)\n#define GGML_F32x4_ADD          vec_add\n#define GGML_F32x4_MUL          vec_mul\n#define GGML_F32x4_REDUCE(res, x)              \\\n{                                              \\\n    int offset = GGML_F32_ARR >> 1;            \\\n    for (int i = 0; i < offset; ++i) {         \\\n        x[i] = vec_add(x[i], x[offset+i]);     \\\n    }                                          \\\n    offset >>= 1;                              \\\n    for (int i = 0; i < offset; ++i) {         \\\n        x[i] = vec_add(x[i], x[offset+i]);     \\\n    }                                          \\\n    offset >>= 1;                              \\\n    for (int i = 0; i < offset; ++i) {         \\\n        x[i] = vec_add(x[i], x[offset+i]);     \\\n    }                                          \\\n    res = vec_extract(x[0], 0) +               \\\n          vec_extract(x[0], 1) +               \\\n          vec_extract(x[0], 2) +               \\\n          vec_extract(x[0], 3);                \\\n}\n\n#define GGML_F32_VEC        GGML_F32x4\n#define GGML_F32_VEC_ZERO   GGML_F32x4_ZERO\n#define GGML_F32_VEC_SET1   GGML_F32x4_SET1\n#define GGML_F32_VEC_LOAD   GGML_F32x4_LOAD\n#define GGML_F32_VEC_STORE  GGML_F32x4_STORE\n#define GGML_F32_VEC_FMA    GGML_F32x4_FMA\n#define GGML_F32_VEC_ADD    GGML_F32x4_ADD\n#define GGML_F32_VEC_MUL    GGML_F32x4_MUL\n#define GGML_F32_VEC_REDUCE GGML_F32x4_REDUCE\n\n// F16 POWER9\n#define GGML_F16_STEP       GGML_F32_STEP\n#define GGML_F16_EPR        GGML_F32_EPR\n#define GGML_F16_VEC        GGML_F32x4\n#define GGML_F16_VEC_ZERO   GGML_F32x4_ZERO\n#define GGML_F16_VEC_SET1   GGML_F32x4_SET1\n#define GGML_F16_VEC_FMA    GGML_F32x4_FMA\n#define GGML_F16_VEC_REDUCE GGML_F32x4_REDUCE\n// Use vec_xl, not vec_ld, in case the load address is not aligned.\n#define GGML_F16_VEC_LOAD(p, i) (i & 0x1) ?                   \\\n  vec_extract_fp32_from_shorth(vec_xl(0, p - GGML_F16_EPR)) : \\\n  vec_extract_fp32_from_shortl(vec_xl(0, p))\n#define GGML_ENDIAN_BYTE(i) ((unsigned char *)&(uint16_t){1})[i]\n#define GGML_F16_VEC_STORE(p, r, i)                             \\\n  if (i & 0x1)                                                  \\\n    vec_xst(vec_pack_to_short_fp32(r[i - GGML_ENDIAN_BYTE(1)],  \\\n                                   r[i - GGML_ENDIAN_BYTE(0)]), \\\n            0, p - GGML_F16_EPR)\n\n#elif defined(__wasm_simd128__)\n\n#define GGML_SIMD\n\n// F32 WASM\n\n#define GGML_F32_STEP 16\n#define GGML_F32_EPR  4\n\n#define GGML_F32x4              v128_t\n#define GGML_F32x4_ZERO         wasm_f32x4_splat(0.0f)\n#define GGML_F32x4_SET1(x)      wasm_f32x4_splat(x)\n#define GGML_F32x4_LOAD         wasm_v128_load\n#define GGML_F32x4_STORE        wasm_v128_store\n#define GGML_F32x4_FMA(a, b, c) wasm_f32x4_add(wasm_f32x4_mul(b, c), a)\n#define GGML_F32x4_ADD          wasm_f32x4_add\n#define GGML_F32x4_MUL          wasm_f32x4_mul\n#define GGML_F32x4_REDUCE(res, x)                  \\\n{                                                  \\\n    int offset = GGML_F32_ARR >> 1;                \\\n    for (int i = 0; i < offset; ++i) {             \\\n        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \\\n    }                                              \\\n    offset >>= 1;                                  \\\n    for (int i = 0; i < offset; ++i) {             \\\n        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \\\n    }                                              \\\n    offset >>= 1;                                  \\\n    for (int i = 0; i < offset; ++i) {             \\\n        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \\\n    }                                              \\\n    res = wasm_f32x4_extract_lane(x[0], 0) +       \\\n          wasm_f32x4_extract_lane(x[0], 1) +       \\\n          wasm_f32x4_extract_lane(x[0], 2) +       \\\n          wasm_f32x4_extract_lane(x[0], 3);        \\\n}\n\n#define GGML_F32_VEC        GGML_F32x4\n#define GGML_F32_VEC_ZERO   GGML_F32x4_ZERO\n#define GGML_F32_VEC_SET1   GGML_F32x4_SET1\n#define GGML_F32_VEC_LOAD   GGML_F32x4_LOAD\n#define GGML_F32_VEC_STORE  GGML_F32x4_STORE\n#define GGML_F32_VEC_FMA    GGML_F32x4_FMA\n#define GGML_F32_VEC_ADD    GGML_F32x4_ADD\n#define GGML_F32_VEC_MUL    GGML_F32x4_MUL\n#define GGML_F32_VEC_REDUCE GGML_F32x4_REDUCE\n\n// F16 WASM\n\n#define GGML_F16_STEP 16\n#define GGML_F16_EPR  4\n\ninline static v128_t __wasm_f16x4_load(const ggml_fp16_t * p) {\n    float tmp[4];\n\n    tmp[0] = GGML_FP16_TO_FP32(p[0]);\n    tmp[1] = GGML_FP16_TO_FP32(p[1]);\n    tmp[2] = GGML_FP16_TO_FP32(p[2]);\n    tmp[3] = GGML_FP16_TO_FP32(p[3]);\n\n    return wasm_v128_load(tmp);\n}\n\ninline static void __wasm_f16x4_store(ggml_fp16_t * p, v128_t x) {\n    float tmp[4];\n\n    wasm_v128_store(tmp, x);\n\n    p[0] = GGML_FP32_TO_FP16(tmp[0]);\n    p[1] = GGML_FP32_TO_FP16(tmp[1]);\n    p[2] = GGML_FP32_TO_FP16(tmp[2]);\n    p[3] = GGML_FP32_TO_FP16(tmp[3]);\n}\n\n#define GGML_F16x4             v128_t\n#define GGML_F16x4_ZERO        wasm_f32x4_splat(0.0f)\n#define GGML_F16x4_SET1(x)     wasm_f32x4_splat(x)\n#define GGML_F16x4_LOAD(x)     __wasm_f16x4_load(x)\n#define GGML_F16x4_STORE(x, y) __wasm_f16x4_store(x, y)\n#define GGML_F16x4_FMA         GGML_F32x4_FMA\n#define GGML_F16x4_ADD         wasm_f32x4_add\n#define GGML_F16x4_MUL         wasm_f32x4_mul\n#define GGML_F16x4_REDUCE(res, x)                  \\\n{                                                  \\\n    int offset = GGML_F16_ARR >> 1;                \\\n    for (int i = 0; i < offset; ++i) {             \\\n        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \\\n    }                                              \\\n    offset >>= 1;                                  \\\n    for (int i = 0; i < offset; ++i) {             \\\n        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \\\n    }                                              \\\n    offset >>= 1;                                  \\\n    for (int i = 0; i < offset; ++i) {             \\\n        x[i] = wasm_f32x4_add(x[i], x[offset+i]);  \\\n    }                                              \\\n    res = wasm_f32x4_extract_lane(x[0], 0) +       \\\n          wasm_f32x4_extract_lane(x[0], 1) +       \\\n          wasm_f32x4_extract_lane(x[0], 2) +       \\\n          wasm_f32x4_extract_lane(x[0], 3);        \\\n}\n\n#define GGML_F16_VEC                GGML_F16x4\n#define GGML_F16_VEC_ZERO           GGML_F16x4_ZERO\n#define GGML_F16_VEC_SET1           GGML_F16x4_SET1\n#define GGML_F16_VEC_LOAD(p, i)     GGML_F16x4_LOAD(p)\n#define GGML_F16_VEC_STORE(p, r, i) GGML_F16x4_STORE(p, r[i])\n#define GGML_F16_VEC_FMA            GGML_F16x4_FMA\n#define GGML_F16_VEC_ADD            GGML_F16x4_ADD\n#define GGML_F16_VEC_MUL            GGML_F16x4_MUL\n#define GGML_F16_VEC_REDUCE         GGML_F16x4_REDUCE\n\n#elif defined(__SSE3__)\n\n#define GGML_SIMD\n\n// F32 SSE\n\n#define GGML_F32_STEP 32\n#define GGML_F32_EPR  4\n\n#define GGML_F32x4         __m128\n#define GGML_F32x4_ZERO    _mm_setzero_ps()\n#define GGML_F32x4_SET1(x) _mm_set1_ps(x)\n#define GGML_F32x4_LOAD    _mm_loadu_ps\n#define GGML_F32x4_STORE   _mm_storeu_ps\n#if defined(__FMA__)\n    // TODO: Does this work?\n    #define GGML_F32x4_FMA(a, b, c) _mm_fmadd_ps(b, c, a)\n#else\n    #define GGML_F32x4_FMA(a, b, c) _mm_add_ps(_mm_mul_ps(b, c), a)\n#endif\n#define GGML_F32x4_ADD     _mm_add_ps\n#define GGML_F32x4_MUL     _mm_mul_ps\n#define GGML_F32x4_REDUCE(res, x)                                 \\\n{                                                                 \\\n    int offset = GGML_F32_ARR >> 1;                               \\\n    for (int i = 0; i < offset; ++i) {                            \\\n        x[i] = _mm_add_ps(x[i], x[offset+i]);                     \\\n    }                                                             \\\n    offset >>= 1;                                                 \\\n    for (int i = 0; i < offset; ++i) {                            \\\n        x[i] = _mm_add_ps(x[i], x[offset+i]);                     \\\n    }                                                             \\\n    offset >>= 1;                                                 \\\n    for (int i = 0; i < offset; ++i) {                            \\\n        x[i] = _mm_add_ps(x[i], x[offset+i]);                     \\\n    }                                                             \\\n    const __m128 t0 = _mm_hadd_ps(x[0], x[0]);                    \\\n    res = _mm_cvtss_f32(_mm_hadd_ps(t0, t0));                     \\\n}\n// TODO: is this optimal ?\n\n#define GGML_F32_VEC        GGML_F32x4\n#define GGML_F32_VEC_ZERO   GGML_F32x4_ZERO\n#define GGML_F32_VEC_SET1   GGML_F32x4_SET1\n#define GGML_F32_VEC_LOAD   GGML_F32x4_LOAD\n#define GGML_F32_VEC_STORE  GGML_F32x4_STORE\n#define GGML_F32_VEC_FMA    GGML_F32x4_FMA\n#define GGML_F32_VEC_ADD    GGML_F32x4_ADD\n#define GGML_F32_VEC_MUL    GGML_F32x4_MUL\n#define GGML_F32_VEC_REDUCE GGML_F32x4_REDUCE\n\n// F16 SSE\n\n#define GGML_F16_STEP 32\n#define GGML_F16_EPR  4\n\nstatic inline __m128 __sse_f16x4_load(ggml_fp16_t *x) {\n    float tmp[4];\n\n    tmp[0] = GGML_FP16_TO_FP32(x[0]);\n    tmp[1] = GGML_FP16_TO_FP32(x[1]);\n    tmp[2] = GGML_FP16_TO_FP32(x[2]);\n    tmp[3] = GGML_FP16_TO_FP32(x[3]);\n\n    return _mm_loadu_ps(tmp);\n}\n\nstatic inline void __sse_f16x4_store(ggml_fp16_t *x, __m128 y) {\n    float arr[4];\n\n    _mm_storeu_ps(arr, y);\n\n    x[0] = GGML_FP32_TO_FP16(arr[0]);\n    x[1] = GGML_FP32_TO_FP16(arr[1]);\n    x[2] = GGML_FP32_TO_FP16(arr[2]);\n    x[3] = GGML_FP32_TO_FP16(arr[3]);\n}\n\n#define GGML_F32Cx4             __m128\n#define GGML_F32Cx4_ZERO        _mm_setzero_ps()\n#define GGML_F32Cx4_SET1(x)     _mm_set1_ps(x)\n#define GGML_F32Cx4_LOAD(x)     __sse_f16x4_load(x)\n#define GGML_F32Cx4_STORE(x, y) __sse_f16x4_store(x, y)\n#define GGML_F32Cx4_FMA         GGML_F32x4_FMA\n#define GGML_F32Cx4_ADD         _mm_add_ps\n#define GGML_F32Cx4_MUL         _mm_mul_ps\n#define GGML_F32Cx4_REDUCE      GGML_F32x4_REDUCE\n\n#define GGML_F16_VEC                 GGML_F32Cx4\n#define GGML_F16_VEC_ZERO            GGML_F32Cx4_ZERO\n#define GGML_F16_VEC_SET1            GGML_F32Cx4_SET1\n#define GGML_F16_VEC_LOAD(p, i)      GGML_F32Cx4_LOAD(p)\n#define GGML_F16_VEC_STORE(p, r, i)  GGML_F32Cx4_STORE(p, r[i])\n#define GGML_F16_VEC_FMA             GGML_F32Cx4_FMA\n#define GGML_F16_VEC_ADD             GGML_F32Cx4_ADD\n#define GGML_F16_VEC_MUL             GGML_F32Cx4_MUL\n#define GGML_F16_VEC_REDUCE          GGML_F32Cx4_REDUCE\n\n#endif\n\n// GGML_F32_ARR / GGML_F16_ARR\n//   number of registers to use per step\n#ifdef GGML_SIMD\n#define GGML_F32_ARR (GGML_F32_STEP/GGML_F32_EPR)\n#define GGML_F16_ARR (GGML_F16_STEP/GGML_F16_EPR)\n#endif\n\n//\n// fundamental operations\n//\n\ninline static void ggml_vec_set_i8(const int n, int8_t * x, const int8_t v) { for (int i = 0; i < n; ++i) x[i] = v; }\n\ninline static void ggml_vec_set_i16(const int n, int16_t * x, const int16_t v) { for (int i = 0; i < n; ++i) x[i] = v; }\n\ninline static void ggml_vec_set_i32(const int n, int32_t * x, const int32_t v) { for (int i = 0; i < n; ++i) x[i] = v; }\n\ninline static void ggml_vec_set_f16(const int n, ggml_fp16_t * x, const int32_t v) { for (int i = 0; i < n; ++i) x[i] = v; }\n\ninline static void ggml_vec_add_f32 (const int n, float * z, const float * x, const float * y) { for (int i = 0; i < n; ++i) z[i]  = x[i] + y[i]; }\ninline static void ggml_vec_add1_f32(const int n, float * z, const float * x, const float   v) { for (int i = 0; i < n; ++i) z[i]  = x[i] + v;    }\ninline static void ggml_vec_acc_f32 (const int n, float * y, const float * x)                  { for (int i = 0; i < n; ++i) y[i] += x[i];        }\ninline static void ggml_vec_acc1_f32(const int n, float * y, const float   v)                  { for (int i = 0; i < n; ++i) y[i] += v;           }\ninline static void ggml_vec_sub_f32 (const int n, float * z, const float * x, const float * y) { for (int i = 0; i < n; ++i) z[i]  = x[i] - y[i]; }\ninline static void ggml_vec_set_f32 (const int n, float * x, const float   v)                  { for (int i = 0; i < n; ++i) x[i]  = v;           }\ninline static void ggml_vec_cpy_f32 (const int n, float * y, const float * x)                  { for (int i = 0; i < n; ++i) y[i]  = x[i];        }\ninline static void ggml_vec_neg_f32 (const int n, float * y, const float * x)                  { for (int i = 0; i < n; ++i) y[i]  = -x[i];       }\ninline static void ggml_vec_mul_f32 (const int n, float * z, const float * x, const float * y) { for (int i = 0; i < n; ++i) z[i]  = x[i]*y[i];   }\ninline static void ggml_vec_div_f32 (const int n, float * z, const float * x, const float * y) { for (int i = 0; i < n; ++i) z[i]  = x[i]/y[i];   }\n\nstatic void ggml_vec_dot_f32(const int n, float * restrict s, const float * restrict x, const float * restrict y) {\n#ifdef GGML_SIMD\n    float sumf = 0.0f;\n    const int np = (n & ~(GGML_F32_STEP - 1));\n\n    GGML_F32_VEC sum[GGML_F32_ARR] = { GGML_F32_VEC_ZERO };\n\n    GGML_F32_VEC ax[GGML_F32_ARR];\n    GGML_F32_VEC ay[GGML_F32_ARR];\n\n    for (int i = 0; i < np; i += GGML_F32_STEP) {\n        for (int j = 0; j < GGML_F32_ARR; j++) {\n            ax[j] = GGML_F32_VEC_LOAD(x + i + j*GGML_F32_EPR);\n            ay[j] = GGML_F32_VEC_LOAD(y + i + j*GGML_F32_EPR);\n\n            sum[j] = GGML_F32_VEC_FMA(sum[j], ax[j], ay[j]);\n        }\n    }\n\n    // reduce sum0..sum3 to sum0\n    GGML_F32_VEC_REDUCE(sumf, sum);\n\n    // leftovers\n    for (int i = np; i < n; ++i) {\n        sumf += x[i]*y[i];\n    }\n#else\n    // scalar\n    ggml_float sumf = 0.0;\n    for (int i = 0; i < n; ++i) {\n        sumf += (ggml_float)(x[i]*y[i]);\n    }\n#endif\n\n    *s = sumf;\n}\n\nstatic void ggml_vec_dot_f16(const int n, float * restrict s, ggml_fp16_t * restrict x, ggml_fp16_t * restrict y) {\n    ggml_float sumf = 0.0;\n\n#if defined(GGML_SIMD)\n    const int np = (n & ~(GGML_F16_STEP - 1));\n\n    GGML_F16_VEC sum[GGML_F16_ARR] = { GGML_F16_VEC_ZERO };\n\n    GGML_F16_VEC ax[GGML_F16_ARR];\n    GGML_F16_VEC ay[GGML_F16_ARR];\n\n    for (int i = 0; i < np; i += GGML_F16_STEP) {\n        for (int j = 0; j < GGML_F16_ARR; j++) {\n            ax[j] = GGML_F16_VEC_LOAD(x + i + j*GGML_F16_EPR, j);\n            ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\n\n            sum[j] = GGML_F16_VEC_FMA(sum[j], ax[j], ay[j]);\n        }\n    }\n\n    // reduce sum0..sum3 to sum0\n    GGML_F16_VEC_REDUCE(sumf, sum);\n\n    // leftovers\n    for (int i = np; i < n; ++i) {\n        sumf += (ggml_float)(GGML_FP16_TO_FP32(x[i])*GGML_FP16_TO_FP32(y[i]));\n    }\n#else\n    for (int i = 0; i < n; ++i) {\n        sumf += (ggml_float)(GGML_FP16_TO_FP32(x[i])*GGML_FP16_TO_FP32(y[i]));\n    }\n#endif\n\n    *s = sumf;\n}\n\n// compute GGML_VEC_DOT_UNROLL dot products at once\n// xs - x row stride in bytes\ninline static void ggml_vec_dot_f16_unroll(const int n, const int xs, float * restrict s, void * restrict xv, ggml_fp16_t * restrict y) {\n    ggml_float sumf[GGML_VEC_DOT_UNROLL] = { 0.0 };\n\n    ggml_fp16_t * restrict x[GGML_VEC_DOT_UNROLL];\n\n    for (int i = 0; i < GGML_VEC_DOT_UNROLL; ++i) {\n        x[i] = (ggml_fp16_t *) ((char *) xv + i*xs);\n    }\n\n#if defined(GGML_SIMD)\n    const int np = (n & ~(GGML_F16_STEP - 1));\n\n    GGML_F16_VEC sum[GGML_VEC_DOT_UNROLL][GGML_F16_ARR] = { { GGML_F16_VEC_ZERO } };\n\n    GGML_F16_VEC ax[GGML_F16_ARR];\n    GGML_F16_VEC ay[GGML_F16_ARR];\n\n    for (int i = 0; i < np; i += GGML_F16_STEP) {\n        for (int j = 0; j < GGML_F16_ARR; j++) {\n            ay[j] = GGML_F16_VEC_LOAD(y + i + j*GGML_F16_EPR, j);\n\n            for (int k = 0; k < GGML_VEC_DOT_UNROLL; ++k) {\n                ax[j] = GGML_F16_VEC_LOAD(x[k] + i + j*GGML_F16_EPR, j);\n\n                sum[k][j] = GGML_F16_VEC_FMA(sum[k][j], ax[j], ay[j]);\n            }\n        }\n    }\n\n    // reduce sum0..sum3 to sum0\n    for (int k = 0; k < GGML_VEC_DOT_UNROLL; ++k) {\n        GGML_F16_VEC_REDUCE(sumf[k], sum[k]);\n    }\n\n    // leftovers\n    for (int i = np; i < n; ++i) {\n        for (int j = 0; j < GGML_VEC_DOT_UNROLL; ++j) {\n            sumf[j] += (ggml_float)(GGML_FP16_TO_FP32(x[j][i])*GGML_FP16_TO_FP32(y[i]));\n        }\n    }\n#else\n    for (int i = 0; i < n; ++i) {\n        for (int j = 0; j < GGML_VEC_DOT_UNROLL; ++j) {\n            sumf[j] += (ggml_float)(GGML_FP16_TO_FP32(x[j][i])*GGML_FP16_TO_FP32(y[i]));\n        }\n    }\n#endif\n\n    for (int i = 0; i < GGML_VEC_DOT_UNROLL; ++i) {\n        s[i] = sumf[i];\n    }\n}\n\ninline static void ggml_vec_mad_f32(const int n, float * restrict y, const float * restrict x, const float v) {\n#if defined(GGML_SIMD)\n    const int np = (n & ~(GGML_F32_STEP - 1));\n\n    GGML_F32_VEC vx = GGML_F32_VEC_SET1(v);\n\n    GGML_F32_VEC ax[GGML_F32_ARR];\n    GGML_F32_VEC ay[GGML_F32_ARR];\n\n    for (int i = 0; i < np; i += GGML_F32_STEP) {\n        for (int j = 0; j < GGML_F32_ARR; j++) {\n            ax[j] = GGML_F32_VEC_LOAD(x + i + j*GGML_F32_EPR);\n            ay[j] = GGML_F32_VEC_LOAD(y + i + j*GGML_F32_EPR);\n            ay[j] = GGML_F32_VEC_FMA(ay[j], ax[j], vx);\n\n            GGML_F32_VEC_STORE(y + i + j*GGML_F32_EPR, ay[j]);\n        }\n    }\n\n    // leftovers\n    for (int i = np; i < n; ++i) {\n        y[i] += x[i]*v;\n    }\n#else\n    // scalar\n    for (int i = 0; i < n; ++i) {\n        y[i] += x[i]*v;\n    }\n#endif\n}\n\n// xs and vs are byte strides of x and v\ninline static void ggml_vec_mad_f32_unroll(const int n, const int xs, const int vs, float * restrict y, const float * restrict xv, const float * restrict vv) {\n\n    const float * restrict x[GGML_VEC_MAD_UNROLL];\n    const float * restrict v[GGML_VEC_MAD_UNROLL];\n\n    for (int i = 0; i < GGML_VEC_MAD_UNROLL; ++i) {\n        x[i] = (const float *) ((const char *) xv + i*xs);\n        v[i] = (const float *) ((const char *) vv + i*vs);\n    }\n\n#if defined(GGML_SIMD)\n    const int np = (n & ~(GGML_F32_STEP - 1));\n\n    GGML_F32_VEC vx[GGML_VEC_MAD_UNROLL];\n\n    for (int k = 0; k < GGML_VEC_MAD_UNROLL; ++k) {\n        vx[k] = GGML_F32_VEC_SET1(v[k][0]);\n    }\n\n    GGML_F32_VEC ax[GGML_VEC_MAD_UNROLL][GGML_F32_ARR];\n    GGML_F32_VEC ay[GGML_F32_ARR];\n\n    for (int i = 0; i < np; i += GGML_F32_STEP) {\n        for (int j = 0; j < GGML_F32_ARR; j++) {\n            ay[j] = GGML_F32_VEC_LOAD(y + i + j*GGML_F32_EPR);\n\n            for (int k = 0; k < GGML_VEC_MAD_UNROLL; ++k) {\n                ax[k][j] = GGML_F32_VEC_LOAD(x[k] + i + j*GGML_F32_EPR);\n                ay[j] = GGML_F32_VEC_FMA(ay[j], ax[k][j], vx[k]);\n            }\n\n            GGML_F32_VEC_STORE(y + i + j*GGML_F32_EPR, ay[j]);\n        }\n    }\n\n    // leftovers\n    for (int k = 0; k < GGML_VEC_MAD_UNROLL; ++k) {\n        for (int i = np; i < n; ++i) {\n            y[i] += x[k][i]*v[k][0];\n        }\n    }\n#else\n    // scalar\n    for (int k = 0; k < GGML_VEC_MAD_UNROLL; ++k) {\n        for (int i = 0; i < n; ++i) {\n            y[i] += x[k][i]*v[k][0];\n        }\n    }\n#endif\n}\n\n//inline static void ggml_vec_scale_f32(const int n, float * y, const float   v) { for (int i = 0; i < n; ++i) y[i] *= v;          }\ninline static void ggml_vec_scale_f32(const int n, float * y, const float   v) {\n#if defined(GGML_USE_ACCELERATE)\n    vDSP_vsmul(y, 1, &v, y, 1, n);\n#elif defined(GGML_SIMD)\n    const int np = (n & ~(GGML_F32_STEP - 1));\n\n    GGML_F32_VEC vx = GGML_F32_VEC_SET1(v);\n\n    GGML_F32_VEC ay[GGML_F32_ARR];\n\n    for (int i = 0; i < np; i += GGML_F32_STEP) {\n        for (int j = 0; j < GGML_F32_ARR; j++) {\n            ay[j] = GGML_F32_VEC_LOAD(y + i + j*GGML_F32_EPR);\n            ay[j] = GGML_F32_VEC_MUL(ay[j], vx);\n\n            GGML_F32_VEC_STORE(y + i + j*GGML_F32_EPR, ay[j]);\n        }\n    }\n\n    // leftovers\n    for (int i = np; i < n; ++i) {\n        y[i] *= v;\n    }\n#else\n    // scalar\n    for (int i = 0; i < n; ++i) {\n        y[i] *= v;\n    }\n#endif\n}\n\ninline static void ggml_vec_norm_f32 (const int n, float * s, const float * x) { ggml_vec_dot_f32(n, s, x, x); *s = sqrtf(*s);   }\ninline static void ggml_vec_sqr_f32  (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = x[i]*x[i];   }\ninline static void ggml_vec_sqrt_f32 (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = sqrtf(x[i]); }\ninline static void ggml_vec_log_f32  (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = logf(x[i]);   }\ninline static void ggml_vec_abs_f32  (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = fabsf(x[i]); }\ninline static void ggml_vec_sgn_f32  (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = (x[i] > 0.f) ? 1.f : ((x[i] < 0.f) ? -1.f : 0.f); }\ninline static void ggml_vec_step_f32 (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = (x[i] > 0.f) ? 1.f : 0.f; }\ninline static void ggml_vec_tanh_f32 (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = tanhf(x[i]);  }\ninline static void ggml_vec_elu_f32  (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = (x[i] > 0.f) ? x[i] : expf(x[i])-1; }\ninline static void ggml_vec_relu_f32 (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = (x[i] > 0.f) ? x[i] : 0.f; }\ninline static void ggml_vec_leaky_f32 (const int n, float * y, const float * x) { for (int i = 0; i < n; ++i) y[i] = (x[i] > 0.f) ? x[i] : 0.1f*x[i]; }\n\nstatic const float GELU_COEF_A     = 0.044715f;\nstatic const float GELU_QUICK_COEF = -1.702f;\nstatic const float SQRT_2_OVER_PI  = 0.79788456080286535587989211986876f;\n\ninline static float ggml_gelu_f32(float x) {\n    return 0.5f*x*(1.0f + tanhf(SQRT_2_OVER_PI*x*(1.0f + GELU_COEF_A*x*x)));\n}\n\ninline static void ggml_vec_gelu_f16(const int n, ggml_fp16_t * y, const ggml_fp16_t * x) {\n    const uint16_t * i16 = (const uint16_t *) x;\n    for (int i = 0; i < n; ++i) {\n        y[i] = ggml_table_gelu_f16[i16[i]];\n    }\n}\n\n#ifdef GGML_GELU_FP16\ninline static void ggml_vec_gelu_f32(const int n, float * y, const float * x) {\n    uint16_t t;\n    for (int i = 0; i < n; ++i) {\n        ggml_fp16_t fp16 = GGML_FP32_TO_FP16(x[i]);\n        memcpy(&t, &fp16, sizeof(uint16_t));\n        y[i] = GGML_FP16_TO_FP32(ggml_table_gelu_f16[t]);\n    }\n}\n#else\ninline static void ggml_vec_gelu_f32(const int n, float * y, const float * x) {\n    for (int i = 0; i < n; ++i) {\n        y[i] = ggml_gelu_f32(x[i]);\n    }\n}\n#endif\n\ninline static float ggml_gelu_quick_f32(float x) {\n    return x*(1.0f/(1.0f+expf(GELU_QUICK_COEF*x)));\n}\n\n//inline static void ggml_vec_gelu_quick_f16(const int n, ggml_fp16_t * y, const ggml_fp16_t * x) {\n//    const uint16_t * i16 = (const uint16_t *) x;\n//    for (int i = 0; i < n; ++i) {\n//        y[i] = ggml_table_gelu_quick_f16[i16[i]];\n//    }\n//}\n\n#ifdef GGML_GELU_QUICK_FP16\ninline static void ggml_vec_gelu_quick_f32(const int n, float * y, const float * x) {\n    uint16_t t;\n    for (int i = 0; i < n; ++i) {\n        ggml_fp16_t fp16 = GGML_FP32_TO_FP16(x[i]);\n        memcpy(&t, &fp16, sizeof(uint16_t));\n        y[i] = GGML_FP16_TO_FP32(ggml_table_gelu_quick_f16[t]);\n    }\n}\n#else\ninline static void ggml_vec_gelu_quick_f32(const int n, float * y, const float * x) {\n    for (int i = 0; i < n; ++i) {\n        y[i] = ggml_gelu_quick_f32(x[i]);\n    }\n}\n#endif\n\n// Sigmoid Linear Unit (SiLU) function\ninline static float ggml_silu_f32(float x) {\n    return x/(1.0f + expf(-x));\n}\n\n//inline static void ggml_vec_silu_f16(const int n, ggml_fp16_t * y, const ggml_fp16_t * x) {\n//    const uint16_t * i16 = (const uint16_t *) x;\n//    for (int i = 0; i < n; ++i) {\n//        y[i] = ggml_table_silu_f16[i16[i]];\n//    }\n//}\n\n#ifdef GGML_SILU_FP16\ninline static void ggml_vec_silu_f32(const int n, float * y, const float * x) {\n    uint16_t t;\n    for (int i = 0; i < n; ++i) {\n        ggml_fp16_t fp16 = GGML_FP32_TO_FP16(x[i]);\n        memcpy(&t, &fp16, sizeof(uint16_t));\n        y[i] = GGML_FP16_TO_FP32(ggml_table_silu_f16[t]);\n    }\n}\n#else\ninline static void ggml_vec_silu_f32(const int n, float * y, const float * x) {\n    for (int i = 0; i < n; ++i) {\n        y[i] = ggml_silu_f32(x[i]);\n    }\n}\n#endif\n\ninline static float ggml_silu_backward_f32(float x, float dy) {\n    const float s = 1.0f/(1.0f + expf(-x));\n    return dy*s*(1.0f + x*(1.0f - s));\n}\n\n#ifdef GGML_SILU_FP16\ninline static void ggml_vec_silu_backward_f32(const int n, float * dx, const float * x, const float * dy) {\n    for (int i = 0; i < n; ++i) {\n        // we did not use x[i] to compute forward silu but its f16 equivalent\n        // take derivative at f16 of x[i]:\n        ggml_fp16_t fp16 = GGML_FP32_TO_FP16(x[i]);\n        float usedx = GGML_FP16_TO_FP32(fp16);\n        dx[i] = ggml_silu_backward_f32(usedx, dy[i]);\n    }\n}\n#else\ninline static void ggml_vec_silu_backward_f32(const int n, float * dx, const float * x, const float * dy) {\n    for (int i = 0; i < n; ++i) {\n        dx[i] = ggml_silu_backward_f32(x[i], dy[i]);\n    }\n}\n#endif\n\ninline static void ggml_vec_sum_f32(const int n, float * s, const float * x) {\n#ifndef GGML_USE_ACCELERATE\n    ggml_float sum = 0.0;\n    for (int i = 0; i < n; ++i) {\n        sum += (ggml_float)x[i];\n    }\n    *s = sum;\n#else\n    vDSP_sve(x, 1, s, n);\n#endif\n}\n\ninline static void ggml_vec_sum_f32_ggf(const int n, ggml_float * s, const float * x) {\n    ggml_float sum = 0.0;\n    for (int i = 0; i < n; ++i) {\n        sum += (ggml_float)x[i];\n    }\n    *s = sum;\n}\n\ninline static void ggml_vec_sum_f16_ggf(const int n, float * s, const ggml_fp16_t * x) {\n    float sum = 0.0f;\n    for (int i = 0; i < n; ++i) {\n        sum += GGML_FP16_TO_FP32(x[i]);\n    }\n    *s = sum;\n}\n\ninline static void ggml_vec_sum_i32_ggf(const int n, int64_t * s, const int32_t * x) {\n    int64_t sum = 0;\n    for (int i = 0; i < n; ++i) {\n        sum += (int64_t)x[i];\n    }\n    *s = sum;\n}\n\n\ninline static void ggml_vec_max_f32(const int n, float * s, const float * x) {\n#ifndef GGML_USE_ACCELERATE\n    float max = -INFINITY;\n    for (int i = 0; i < n; ++i) {\n        max = MAX(max, x[i]);\n    }\n    *s = max;\n#else\n    vDSP_maxv(x, 1, s, n);\n#endif\n}\n\ninline static void ggml_vec_norm_inv_f32(const int n, float * s, const float * x) {\n    ggml_vec_norm_f32(n, s, x);\n    *s = 1.f/(*s);\n}\n\ninline static void ggml_vec_argmax_f32(const int n, int * s, const float * x) {\n    float max = -INFINITY;\n    int idx = 0;\n    for (int i = 0; i < n; ++i) {\n        max = MAX(max, x[i]);\n        if (max == x[i]) { idx = i; }\n    }\n    *s = idx;\n}\n\n//\n// data types\n//\n\nstatic const char * GGML_OP_NAME[GGML_OP_COUNT] = {\n    \"NONE\",\n\n    \"DUP\",\n    \"ADD\",\n    \"ADD1\",\n    \"ACC\",\n    \"SUB\",\n    \"MUL\",\n    \"DIV\",\n    \"SQR\",\n    \"SQRT\",\n    \"LOG\",\n    \"SUM\",\n    \"SUM_ROWS\",\n    \"MEAN\",\n    \"ARGMAX\",\n    \"REPEAT\",\n    \"REPEAT_BACK\",\n    \"CONCAT\",\n    \"SILU_BACK\",\n    \"NORM\",\n    \"RMS_NORM\",\n    \"RMS_NORM_BACK\",\n    \"GROUP_NORM\",\n\n    \"MUL_MAT\",\n    \"MUL_MAT_SPARSE\",\n    \"AXPY\",\n    \"OUT_PROD\",\n\n    \"SCALE\",\n    \"SET\",\n    \"CPY\",\n    \"CONT\",\n    \"RESHAPE\",\n    \"VIEW\",\n    \"PERMUTE\",\n    \"TRANSPOSE\",\n    \"GET_ROWS\",\n    \"GET_ROWS_BACK\",\n    \"DIAG\",\n    \"DIAG_MASK_INF\",\n    \"DIAG_MASK_ZERO\",\n    \"SOFT_MAX\",\n    \"SOFT_MAX_BACK\",\n    \"ROPE\",\n    \"ROPE_BACK\",\n    \"ALIBI\",\n    \"CLAMP\",\n    \"CONV_TRANSPOSE_1D\",\n    \"IM2COL\",\n    \"CONV_TRANSPOSE_2D\",\n    \"POOL_1D\",\n    \"POOL_2D\",\n    \"UPSCALE\",\n\n    \"FLASH_ATTN\",\n    \"FLASH_FF\",\n    \"FLASH_ATTN_BACK\",\n    \"WIN_PART\",\n    \"WIN_UNPART\",\n    \"GET_REL_POS\",\n    \"ADD_REL_POS\",\n\n    \"UNARY\",\n\n    \"MAP_UNARY\",\n    \"MAP_BINARY\",\n\n    \"MAP_CUSTOM1_F32\",\n    \"MAP_CUSTOM2_F32\",\n    \"MAP_CUSTOM3_F32\",\n\n    \"MAP_CUSTOM1\",\n    \"MAP_CUSTOM2\",\n    \"MAP_CUSTOM3\",\n\n    \"CROSS_ENTROPY_LOSS\",\n    \"CROSS_ENTROPY_LOSS_BACK\",\n};\n\n// Since we added AXPY\n// static_assert(GGML_OP_COUNT == 68, \"GGML_OP_COUNT != 68\");\n\nstatic const char * GGML_OP_SYMBOL[GGML_OP_COUNT] = {\n    \"none\",\n\n    \"x\",\n    \"x+y\",\n    \"x+y\",\n    \"view(x,nb,offset)+=y->x\",\n    \"x-y\",\n    \"x*y\",\n    \"x/y\",\n    \"x^2\",\n    \"âˆšx\",\n    \"log(x)\",\n    \"Î£x\",\n    \"Î£x_k\",\n    \"Î£x/n\",\n    \"argmax(x)\",\n    \"repeat(x)\",\n    \"repeat_back(x)\",\n    \"concat(x, y)\",\n    \"silu_back(x)\",\n    \"norm(x)\",\n    \"rms_norm(x)\",\n    \"rms_norm_back(x)\",\n    \"group_norm(x)\",\n\n    \"X*Y\",\n    \"X*Y\",\n\n    \"x*v\",\n    \"y-\\\\>view(x)\",\n    \"x-\\\\>y\",\n    \"cont(x)\",\n    \"reshape(x)\",\n    \"view(x)\",\n    \"permute(x)\",\n    \"transpose(x)\",\n    \"get_rows(x)\",\n    \"get_rows_back(x)\",\n    \"diag(x)\",\n    \"diag_mask_inf(x)\",\n    \"diag_mask_zero(x)\",\n    \"soft_max(x)\",\n    \"soft_max_back(x)\",\n    \"rope(x)\",\n    \"rope_back(x)\",\n    \"alibi(x)\",\n    \"clamp(x)\",\n    \"conv_transpose_1d(x)\",\n    \"im2col(x)\",\n    \"conv_transpose_2d(x)\",\n    \"pool_1d(x)\",\n    \"pool_2d(x)\",\n    \"upscale(x)\",\n\n    \"flash_attn(x)\",\n    \"flash_ff(x)\",\n    \"flash_attn_back(x)\",\n    \"win_part(x)\",\n    \"win_unpart(x)\",\n    \"get_rel_pos(x)\",\n    \"add_rel_pos(x)\",\n\n    \"unary(x)\",\n\n    \"f(x)\",\n    \"f(x,y)\",\n\n    \"custom_f32(x)\",\n    \"custom_f32(x,y)\",\n    \"custom_f32(x,y,z)\",\n\n    \"custom(x)\",\n    \"custom(x,y)\",\n    \"custom(x,y,z)\",\n\n    \"cross_entropy_loss(x,y)\",\n    \"cross_entropy_loss_back(x,y)\",\n};\n\n// Since we added AXPY\n// static_assert(GGML_OP_COUNT == 68, \"GGML_OP_COUNT != 68\");\n\nstatic_assert(GGML_OP_POOL_COUNT == 2, \"GGML_OP_POOL_COUNT != 2\");\n\nstatic_assert(sizeof(struct ggml_object)%GGML_MEM_ALIGN == 0, \"ggml_object size must be a multiple of GGML_MEM_ALIGN\");\nstatic_assert(sizeof(struct ggml_tensor)%GGML_MEM_ALIGN == 0, \"ggml_tensor size must be a multiple of GGML_MEM_ALIGN\");\n\n// WARN:\n// Mis-confguration can lead to problem that's hard to reason about:\n// * At best  it crash or talks nosense.\n// * At worst it talks slightly difference but hard to perceive.\n//\n// An op has to enable INIT or FINALIZE when any of it's branch needs that pass.\n// Take care about compile options (e.g., GGML_USE_xxx).\nstatic bool GGML_OP_HAS_INIT    [GGML_OP_COUNT] = { 0 };\nstatic bool GGML_OP_HAS_FINALIZE[GGML_OP_COUNT] = { 0 };\n\nstatic void ggml_setup_op_has_task_pass(void) {\n    {   // INIT\n        bool * p = GGML_OP_HAS_INIT;\n\n        p[GGML_OP_ACC                    ] = true;\n        p[GGML_OP_MUL_MAT                ] = true;\n        p[GGML_OP_MUL_MAT_SPARSE         ] = true;\n        p[GGML_OP_AXPY                   ] = true;\n        p[GGML_OP_OUT_PROD               ] = true;\n        p[GGML_OP_SET                    ] = true;\n        p[GGML_OP_GET_ROWS_BACK          ] = true;\n        p[GGML_OP_DIAG_MASK_INF          ] = true;\n        p[GGML_OP_DIAG_MASK_ZERO         ] = true;\n        p[GGML_OP_CONV_TRANSPOSE_1D      ] = true;\n        p[GGML_OP_CONV_TRANSPOSE_2D      ] = true;\n        p[GGML_OP_FLASH_ATTN_BACK        ] = true;\n        p[GGML_OP_CROSS_ENTROPY_LOSS     ] = true;\n        p[GGML_OP_ADD_REL_POS            ] = true;\n    }\n\n    {   // FINALIZE\n        bool * p = GGML_OP_HAS_FINALIZE;\n\n        p[GGML_OP_CROSS_ENTROPY_LOSS     ] = true;\n    }\n}\n\n//\n// ggml context\n//\n\nstruct ggml_context_container {\n    bool used;\n\n    struct ggml_context context;\n};\n\n//\n// NUMA support\n//\n\n#define GGML_NUMA_MAX_NODES 8\n#define GGML_NUMA_MAX_CPUS 512\n\nstruct ggml_numa_node {\n    uint32_t cpus[GGML_NUMA_MAX_CPUS]; // hardware threads on this node\n    uint32_t n_cpus;\n};\n\nstruct ggml_numa_nodes {\n    struct ggml_numa_node nodes[GGML_NUMA_MAX_NODES];\n    uint32_t n_nodes;\n    uint32_t total_cpus; // hardware threads on system\n};\n\n//\n// ggml state\n//\n\nstruct ggml_state {\n    struct ggml_context_container contexts[GGML_MAX_CONTEXTS];\n    struct ggml_numa_nodes numa;\n};\n\n// global state\nstatic struct ggml_state g_state;\nstatic atomic_int g_state_barrier = 0;\n\n// barrier via spin lock\ninline static void ggml_critical_section_start(void) {\n    int processing = atomic_fetch_add(&g_state_barrier, 1);\n\n    while (processing > 0) {\n        // wait for other threads to finish\n        atomic_fetch_sub(&g_state_barrier, 1);\n        sched_yield(); // TODO: reconsider this\n        processing = atomic_fetch_add(&g_state_barrier, 1);\n    }\n}\n\n// TODO: make this somehow automatically executed\n//       some sort of \"sentry\" mechanism\ninline static void ggml_critical_section_end(void) {\n    atomic_fetch_sub(&g_state_barrier, 1);\n}\n\nvoid ggml_numa_init(void) {\n    if (g_state.numa.n_nodes > 0) {\n        fprintf(stderr, \"ggml_numa_init: NUMA already initialized\\n\");\n\n        return;\n    }\n\n#ifdef __linux__\n    struct stat st;\n    char path[256];\n    int rv;\n\n    // enumerate nodes\n    while (g_state.numa.n_nodes < GGML_NUMA_MAX_NODES) {\n        rv = snprintf(path, sizeof(path), \"/sys/devices/system/node/node%u\", g_state.numa.n_nodes);\n        GGML_ASSERT(rv > 0 && (unsigned)rv < sizeof(path));\n        if (stat(path, &st) != 0) { break; }\n        ++g_state.numa.n_nodes;\n    }\n\n    // enumerate CPUs\n    while (g_state.numa.total_cpus < GGML_NUMA_MAX_CPUS) {\n        rv = snprintf(path, sizeof(path), \"/sys/devices/system/cpu/cpu%u\", g_state.numa.total_cpus);\n        GGML_ASSERT(rv > 0 && (unsigned)rv < sizeof(path));\n        if (stat(path, &st) != 0) { break; }\n        ++g_state.numa.total_cpus;\n    }\n\n    GGML_PRINT_DEBUG(\"found %u numa nodes, %u CPUs\\n\", g_state.numa.n_nodes, g_state.numa.total_cpus);\n\n    if (g_state.numa.n_nodes < 1 || g_state.numa.total_cpus < 1) {\n        g_state.numa.n_nodes = 0;\n        return;\n    }\n\n    for (uint32_t n = 0; n < g_state.numa.n_nodes; ++n) {\n        struct ggml_numa_node * node = &g_state.numa.nodes[n];\n        GGML_PRINT_DEBUG(\"CPUs on node %u:\", n);\n        node->n_cpus = 0;\n        for (uint32_t c = 0; c < g_state.numa.total_cpus; ++c) {\n            rv = snprintf(path, sizeof(path), \"/sys/devices/system/node/node%u/cpu%u\", n, c);\n            GGML_ASSERT(rv > 0 && (unsigned)rv < sizeof(path));\n            if (stat(path, &st) == 0) {\n                node->cpus[node->n_cpus++] = c;\n                GGML_PRINT_DEBUG(\" %u\", c);\n            }\n        }\n        GGML_PRINT_DEBUG(\"\\n\");\n    }\n\n    if (ggml_is_numa()) {\n        FILE *fptr = fopen(\"/proc/sys/kernel/numa_balancing\", \"r\");\n        if (fptr != NULL) {\n            char buf[42];\n            if (fgets(buf, sizeof(buf), fptr) && strncmp(buf, \"0\\n\", sizeof(buf)) != 0) {\n                GGML_PRINT(\"WARNING: /proc/sys/kernel/numa_balancing is enabled, this has been observed to impair performance\\n\");\n            }\n            fclose(fptr);\n        }\n    }\n#else\n    // TODO\n#endif\n}\n\nbool ggml_is_numa(void) {\n    return g_state.numa.n_nodes > 1;\n}\n\n////////////////////////////////////////////////////////////////////////////////\n\nvoid ggml_print_object(const struct ggml_object * obj) {\n    GGML_PRINT(\" - ggml_object: type = %d, offset = %zu, size = %zu, next = %p\\n\",\n            obj->type, obj->offs, obj->size, (const void *) obj->next);\n}\n\nvoid ggml_print_objects(const struct ggml_context * ctx) {\n    struct ggml_object * obj = ctx->objects_begin;\n\n    GGML_PRINT(\"%s: objects in context %p:\\n\", __func__, (const void *) ctx);\n\n    while (obj != NULL) {\n        ggml_print_object(obj);\n        obj = obj->next;\n    }\n\n    GGML_PRINT(\"%s: --- end ---\\n\", __func__);\n}\n\nint64_t ggml_nelements(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return tensor->ne[0]*tensor->ne[1]*tensor->ne[2]*tensor->ne[3];\n}\n\nint64_t ggml_nrows(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return tensor->ne[1]*tensor->ne[2]*tensor->ne[3];\n}\n\nsize_t ggml_nbytes(const struct ggml_tensor * tensor) {\n    size_t nbytes;\n    size_t blck_size = ggml_blck_size(tensor->type);\n    if (blck_size == 1) {\n        nbytes = ggml_type_size(tensor->type);\n        for (int i = 0; i < GGML_MAX_DIMS; ++i) {\n            nbytes += (tensor->ne[i] - 1)*tensor->nb[i];\n        }\n    }\n    else {\n        nbytes = tensor->ne[0]*tensor->nb[0]/blck_size;\n        for (int i = 1; i < GGML_MAX_DIMS; ++i) {\n            nbytes += (tensor->ne[i] - 1)*tensor->nb[i];\n        }\n    }\n\n    return nbytes;\n}\n\nsize_t ggml_nbytes_pad(const struct ggml_tensor * tensor) {\n    return GGML_PAD(ggml_nbytes(tensor), GGML_MEM_ALIGN);\n}\n\nsize_t ggml_nbytes_split(const struct ggml_tensor * tensor, int nrows_split) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return (nrows_split*tensor->ne[0]*ggml_type_size(tensor->type))/ggml_blck_size(tensor->type);\n}\n\nint ggml_blck_size(enum ggml_type type) {\n    return type_traits[type].blck_size;\n}\n\nsize_t ggml_type_size(enum ggml_type type) {\n    return type_traits[type].type_size;\n}\n\nfloat ggml_type_sizef(enum ggml_type type) {\n    return ((float)(type_traits[type].type_size))/type_traits[type].blck_size;\n}\n\nconst char * ggml_type_name(enum ggml_type type) {\n    return type_traits[type].type_name;\n}\n\nbool ggml_is_quantized(enum ggml_type type) {\n    return type_traits[type].is_quantized;\n}\n\nconst char * ggml_op_name(enum ggml_op op) {\n    return GGML_OP_NAME[op];\n}\n\nconst char * ggml_op_symbol(enum ggml_op op) {\n    return GGML_OP_SYMBOL[op];\n}\n\nsize_t ggml_element_size(const struct ggml_tensor * tensor) {\n    return ggml_type_size(tensor->type);\n}\n\nstatic inline bool ggml_is_scalar(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return tensor->ne[0] == 1 && tensor->ne[1] == 1 && tensor->ne[2] == 1 && tensor->ne[3] == 1;\n}\n\nstatic inline bool ggml_is_vector(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return tensor->ne[1] == 1 && tensor->ne[2] == 1 && tensor->ne[3] == 1;\n}\n\nstatic inline bool ggml_is_matrix(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return tensor->ne[2] == 1 && tensor->ne[3] == 1;\n}\n\nstatic inline bool ggml_can_mul_mat(const struct ggml_tensor * t0, const struct ggml_tensor * t1) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return (t0->ne[0]           == t1->ne[0])  &&\n           (t1->ne[2]%t0->ne[2] == 0)          && // verify t0 is broadcastable\n           (t1->ne[3]%t0->ne[3] == 0);\n}\n\nstatic inline bool ggml_can_out_prod(const struct ggml_tensor * t0, const struct ggml_tensor * t1) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return (t0->ne[1] == t1->ne[1])   &&\n           (t1->ne[2]%t0->ne[2] == 0) && // verify t0 is broadcastable\n           (t1->ne[3]%t0->ne[3] == 0);\n}\n\nenum ggml_type ggml_ftype_to_ggml_type(enum ggml_ftype ftype) {\n    enum ggml_type wtype = GGML_TYPE_COUNT;\n\n    switch (ftype) {\n        case GGML_FTYPE_ALL_F32:              wtype = GGML_TYPE_F32;   break;\n        case GGML_FTYPE_MOSTLY_F16:           wtype = GGML_TYPE_F16;   break;\n        case GGML_FTYPE_MOSTLY_Q4_0:          wtype = GGML_TYPE_Q4_0;  break;\n        case GGML_FTYPE_MOSTLY_Q4_1:          wtype = GGML_TYPE_Q4_1;  break;\n        case GGML_FTYPE_MOSTLY_Q5_0:          wtype = GGML_TYPE_Q5_0;  break;\n        case GGML_FTYPE_MOSTLY_Q5_1:          wtype = GGML_TYPE_Q5_1;  break;\n        case GGML_FTYPE_MOSTLY_Q8_0:          wtype = GGML_TYPE_Q8_0;  break;\n        case GGML_FTYPE_MOSTLY_Q2_K:          wtype = GGML_TYPE_Q2_K;  break;\n        case GGML_FTYPE_MOSTLY_Q3_K:          wtype = GGML_TYPE_Q3_K;  break;\n        case GGML_FTYPE_MOSTLY_Q4_K:          wtype = GGML_TYPE_Q4_K;  break;\n        case GGML_FTYPE_MOSTLY_Q5_K:          wtype = GGML_TYPE_Q5_K;  break;\n        case GGML_FTYPE_MOSTLY_Q6_K:          wtype = GGML_TYPE_Q6_K;  break;\n        case GGML_FTYPE_UNKNOWN:              wtype = GGML_TYPE_COUNT; break;\n        case GGML_FTYPE_MOSTLY_Q4_1_SOME_F16: wtype = GGML_TYPE_COUNT; break;\n    }\n\n    GGML_ASSERT(wtype != GGML_TYPE_COUNT);\n\n    return wtype;\n}\n\nsize_t ggml_tensor_overhead(void) {\n    return GGML_OBJECT_SIZE + GGML_TENSOR_SIZE;\n}\n\nbool ggml_is_transposed(const struct ggml_tensor * tensor) {\n    return tensor->nb[0] > tensor->nb[1];\n}\n\nbool ggml_is_contiguous(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return\n        tensor->nb[0] == ggml_type_size(tensor->type) &&\n        tensor->nb[1] == (tensor->nb[0]*tensor->ne[0])/ggml_blck_size(tensor->type) &&\n        tensor->nb[2] == tensor->nb[1]*tensor->ne[1] &&\n        tensor->nb[3] == tensor->nb[2]*tensor->ne[2];\n}\n\nstatic inline bool ggml_is_contiguous_except_dim_1(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return\n        tensor->nb[0] == ggml_type_size(tensor->type) &&\n        tensor->nb[2] == tensor->nb[1]*tensor->ne[1] &&\n        tensor->nb[3] == tensor->nb[2]*tensor->ne[2];\n}\n\nbool ggml_is_permuted(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return tensor->nb[0] > tensor->nb[1] || tensor->nb[1] > tensor->nb[2] || tensor->nb[2] > tensor->nb[3];\n}\n\nstatic inline bool ggml_is_padded_1d(const struct ggml_tensor * tensor) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return\n        tensor->nb[0] == ggml_type_size(tensor->type) &&\n        tensor->nb[2] == tensor->nb[1]*tensor->ne[1] &&\n        tensor->nb[3] == tensor->nb[2]*tensor->ne[2];\n}\n\nbool ggml_are_same_shape(const struct ggml_tensor * t0, const struct ggml_tensor * t1) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return\n        (t0->ne[0] == t1->ne[0] ) &&\n        (t0->ne[1] == t1->ne[1] ) &&\n        (t0->ne[2] == t1->ne[2] ) &&\n        (t0->ne[3] == t1->ne[3] );\n}\n\n// check if t1 can be represented as a repeatition of t0\nstatic inline bool ggml_can_repeat(const struct ggml_tensor * t0, const struct ggml_tensor * t1) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return\n        (t1->ne[0]%t0->ne[0] == 0) &&\n        (t1->ne[1]%t0->ne[1] == 0) &&\n        (t1->ne[2]%t0->ne[2] == 0) &&\n        (t1->ne[3]%t0->ne[3] == 0);\n}\n\nstatic inline bool ggml_can_repeat_rows(const struct ggml_tensor * t0, const struct ggml_tensor * t1) {\n    static_assert(GGML_MAX_DIMS == 4, \"GGML_MAX_DIMS is not 4 - update this function\");\n\n    return (t0->ne[0] == t1->ne[0]) && ggml_can_repeat(t0, t1);\n}\n\nstatic inline int ggml_up32(int n) {\n    return (n + 31) & ~31;\n}\n\n//static inline int ggml_up64(int n) {\n//    return (n + 63) & ~63;\n//}\n\nstatic inline int ggml_up(int n, int m) {\n    // assert m is a power of 2\n    GGML_ASSERT((m & (m - 1)) == 0);\n    return (n + m - 1) & ~(m - 1);\n}\n\n// assert that pointer is aligned to GGML_MEM_ALIGN\n#define ggml_assert_aligned(ptr) \\\n    GGML_ASSERT(((uintptr_t) (ptr))%GGML_MEM_ALIGN == 0)\n\n////////////////////////////////////////////////////////////////////////////////\n\nstruct ggml_context * ggml_init(struct ggml_init_params params) {\n    // make this function thread safe\n    ggml_critical_section_start();\n\n    static bool is_first_call = true;\n\n    if (is_first_call) {\n        // initialize time system (required on Windows)\n        ggml_time_init();\n\n        // initialize GELU, Quick GELU, SILU and EXP F32 tables\n        {\n            const uint64_t t_start = ggml_time_us(); UNUSED(t_start);\n\n            ggml_fp16_t ii;\n            for (int i = 0; i < (1 << 16); ++i) {\n                uint16_t ui = i;\n                memcpy(&ii, &ui, sizeof(ii));\n                const float f = ggml_table_f32_f16[i] = GGML_COMPUTE_FP16_TO_FP32(ii);\n                ggml_table_gelu_f16[i] = GGML_FP32_TO_FP16(ggml_gelu_f32(f));\n                ggml_table_gelu_quick_f16[i] = GGML_FP32_TO_FP16(ggml_gelu_quick_f32(f));\n                ggml_table_silu_f16[i] = GGML_FP32_TO_FP16(ggml_silu_f32(f));\n                ggml_table_exp_f16[i]  = GGML_FP32_TO_FP16(expf(f));\n            }\n\n            const uint64_t t_end = ggml_time_us(); UNUSED(t_end);\n\n            GGML_PRINT_DEBUG(\"%s: GELU, Quick GELU, SILU and EXP tables initialized in %f ms\\n\", __func__, (t_end - t_start)/1000.0f);\n        }\n\n        // initialize g_state\n        {\n            const uint64_t t_start = ggml_time_us(); UNUSED(t_start);\n\n            g_state = (struct ggml_state) {\n                /*.contexts =*/ { { 0 } },\n                /*.numa =*/ {\n                    .n_nodes = 0,\n                    .total_cpus = 0,\n                },\n            };\n\n            for (int i = 0; i < GGML_MAX_CONTEXTS; ++i) {\n                g_state.contexts[i].used = false;\n            }\n\n            const uint64_t t_end = ggml_time_us(); UNUSED(t_end);\n\n            GGML_PRINT_DEBUG(\"%s: g_state initialized in %f ms\\n\", __func__, (t_end - t_start)/1000.0f);\n        }\n\n#if defined(GGML_USE_CUBLAS)\n        ggml_init_cublas();\n#elif defined(GGML_USE_CLBLAST)\n        ggml_cl_init();\n#endif\n\n        ggml_setup_op_has_task_pass();\n\n        is_first_call = false;\n    }\n\n    // find non-used context in g_state\n    struct ggml_context * ctx = NULL;\n\n    for (int i = 0; i < GGML_MAX_CONTEXTS; i++) {\n        if (!g_state.contexts[i].used) {\n            g_state.contexts[i].used = true;\n            ctx = &g_state.contexts[i].context;\n\n            GGML_PRINT_DEBUG(\"%s: found unused context %d\\n\", __func__, i);\n            break;\n        }\n    }\n\n    if (ctx == NULL) {\n        GGML_PRINT_DEBUG(\"%s: no unused context found\\n\", __func__);\n\n        ggml_critical_section_end();\n\n        return NULL;\n    }\n\n    // allow to call ggml_init with 0 size\n    if (params.mem_size == 0) {\n        params.mem_size = GGML_MEM_ALIGN;\n    }\n\n    const size_t mem_size = params.mem_buffer ? params.mem_size : GGML_PAD(params.mem_size, GGML_MEM_ALIGN);\n\n    *ctx = (struct ggml_context) {\n        /*.mem_size           =*/ mem_size,\n        /*.mem_buffer         =*/ params.mem_buffer ? params.mem_buffer : GGML_ALIGNED_MALLOC(mem_size),\n        /*.mem_buffer_owned   =*/ params.mem_buffer ? false : true,\n        /*.no_alloc           =*/ params.no_alloc,\n        /*.no_alloc_save      =*/ params.no_alloc,\n        /*.n_objects          =*/ 0,\n        /*.objects_begin      =*/ NULL,\n        /*.objects_end        =*/ NULL,\n        /*.scratch            =*/ { 0, 0, NULL, },\n        /*.scratch_save       =*/ { 0, 0, NULL, },\n    };\n\n    GGML_ASSERT(ctx->mem_buffer != NULL);\n\n    ggml_assert_aligned(ctx->mem_buffer);\n\n    GGML_PRINT_DEBUG(\"%s: context initialized\\n\", __func__);\n\n    ggml_critical_section_end();\n\n    return ctx;\n}\n\nvoid ggml_free(struct ggml_context * ctx) {\n    // make this function thread safe\n    ggml_critical_section_start();\n\n    bool found = false;\n\n    for (int i = 0; i < GGML_MAX_CONTEXTS; i++) {\n        if (&g_state.contexts[i].context == ctx) {\n            g_state.contexts[i].used = false;\n\n            GGML_PRINT_DEBUG(\"%s: context %d has been freed. memory used = %zu\\n\",\n                    __func__, i, ggml_used_mem(ctx));\n\n            if (ctx->mem_buffer_owned) {\n                GGML_ALIGNED_FREE(ctx->mem_buffer);\n            }\n\n            found = true;\n            break;\n        }\n    }\n\n    if (!found) {\n        GGML_PRINT_DEBUG(\"%s: context not found\\n\", __func__);\n    }\n\n    ggml_critical_section_end();\n}\n\nsize_t ggml_used_mem(const struct ggml_context * ctx) {\n    return ctx->objects_end == NULL ? 0 : ctx->objects_end->offs + ctx->objects_end->size;\n}\n\nsize_t ggml_set_scratch(struct ggml_context * ctx, struct ggml_scratch scratch) {\n    const size_t result = ctx->scratch.data ? ctx->scratch.offs : 0;\n\n    ctx->scratch = scratch;\n\n    return result;\n}\n\nbool ggml_get_no_alloc(struct ggml_context * ctx) {\n    return ctx->no_alloc;\n}\n\nvoid ggml_set_no_alloc(struct ggml_context * ctx, bool no_alloc) {\n    ctx->no_alloc = no_alloc;\n}\n\nvoid * ggml_get_mem_buffer(const struct ggml_context * ctx) {\n    return ctx->mem_buffer;\n}\n\nsize_t ggml_get_mem_size(const struct ggml_context * ctx) {\n    return ctx->mem_size;\n}\n\nsize_t ggml_get_max_tensor_size(const struct ggml_context * ctx) {\n    size_t max_size = 0;\n\n    struct ggml_object * obj = ctx->objects_begin;\n\n    while (obj != NULL) {\n        if (obj->type == GGML_OBJECT_TENSOR) {\n            struct ggml_tensor * tensor = (struct ggml_tensor *) ((char *) ctx->mem_buffer + obj->offs);\n\n            const size_t size = ggml_nbytes(tensor);\n\n            if (max_size < size) {\n                max_size = size;\n            }\n        }\n\n        obj = obj->next;\n    }\n\n    return max_size;\n}\n\n// IMPORTANT:\n// when creating \"opt\" tensors, always save and load the scratch buffer\n// this is an error prone process, but it is necessary to support inplace\n// operators when using scratch buffers\n// TODO: implement a better way\nstatic void ggml_scratch_save(struct ggml_context * ctx) {\n    // this is needed to allow opt tensors to store their data\n    // TODO: again, need to find a better way\n    ctx->no_alloc_save = ctx->no_alloc;\n    ctx->no_alloc      = false;\n\n    ctx->scratch_save = ctx->scratch;\n    ctx->scratch.data = NULL;\n}\n\nstatic void ggml_scratch_load(struct ggml_context * ctx) {\n    ctx->no_alloc = ctx->no_alloc_save;\n\n    ctx->scratch = ctx->scratch_save;\n}\n\n////////////////////////////////////////////////////////////////////////////////\n\nstatic struct ggml_object * ggml_new_object(struct ggml_context * ctx, enum ggml_object_type type, size_t size) {\n    // always insert objects at the end of the context's memory pool\n    struct ggml_object * obj_cur = ctx->objects_end;\n\n    const size_t cur_offs = obj_cur == NULL ? 0 : obj_cur->offs;\n    const size_t cur_size = obj_cur == NULL ? 0 : obj_cur->size;\n    const size_t cur_end  = cur_offs + cur_size;\n\n    // align to GGML_MEM_ALIGN\n    size_t size_needed = GGML_PAD(size, GGML_MEM_ALIGN);\n\n    char * const mem_buffer = ctx->mem_buffer;\n    struct ggml_object * const obj_new = (struct ggml_object *)(mem_buffer + cur_end);\n\n    if (cur_end + size_needed + GGML_OBJECT_SIZE > ctx->mem_size) {\n        GGML_PRINT(\"%s: not enough space in the context's memory pool (needed %zu, available %zu)\\n\",\n                __func__, cur_end + size_needed, ctx->mem_size);\n        assert(false);\n        return NULL;\n    }\n\n    *obj_new = (struct ggml_object) {\n        .offs = cur_end + GGML_OBJECT_SIZE,\n        .size = size_needed,\n        .next = NULL,\n        .type = type,\n    };\n\n    ggml_assert_aligned(mem_buffer + obj_new->offs);\n\n    if (obj_cur != NULL) {\n        obj_cur->next = obj_new;\n    } else {\n        // this is the first object in this context\n        ctx->objects_begin = obj_new;\n    }\n\n    ctx->objects_end = obj_new;\n\n    //printf(\"%s: inserted new object at %zu, size = %zu\\n\", __func__, cur_end, obj_new->size);\n\n    return obj_new;\n}\n\nstatic struct ggml_tensor * ggml_new_tensor_impl(\n        struct ggml_context * ctx,\n        enum   ggml_type      type,\n        int                   n_dims,\n        const int64_t       * ne,\n        struct ggml_tensor  * view_src,\n        size_t                view_offs) {\n\n    assert(n_dims >= 1 && n_dims <= GGML_MAX_DIMS);\n\n    // find the base tensor and absolute offset\n    if (view_src != NULL && view_src->view_src != NULL) {\n        view_offs += view_src->view_offs;\n        view_src   = view_src->view_src;\n    }\n\n    size_t data_size = ggml_type_size(type)*(ne[0]/ggml_blck_size(type));\n    for (int i = 1; i < n_dims; i++) {\n        data_size *= ne[i];\n    }\n\n    GGML_ASSERT(view_src == NULL || data_size + view_offs <= ggml_nbytes(view_src));\n\n    void * data = view_src != NULL ? view_src->data : NULL;\n    if (data != NULL) {\n        data = (char *) data + view_offs;\n    }\n\n    size_t obj_alloc_size = 0;\n\n    if (view_src == NULL && !ctx->no_alloc) {\n        if (ctx->scratch.data != NULL) {\n            // allocate tensor data in the scratch buffer\n            if (ctx->scratch.offs + data_size > ctx->scratch.size) {\n                GGML_PRINT(\"%s: not enough space in the scratch memory pool (needed %zu, available %zu)\\n\",\n                        __func__, ctx->scratch.offs + data_size, ctx->scratch.size);\n                assert(false);\n                return NULL;\n            }\n\n            data = (char * const) ctx->scratch.data + ctx->scratch.offs;\n\n            ctx->scratch.offs += data_size;\n        } else {\n            // allocate tensor data in the context's memory pool\n            obj_alloc_size = data_size;\n        }\n    }\n\n    struct ggml_object * const obj_new = ggml_new_object(ctx, GGML_OBJECT_TENSOR, GGML_TENSOR_SIZE + obj_alloc_size);\n\n    // TODO: for recoverable errors, we would need to free the data allocated from the scratch buffer here\n\n    struct ggml_tensor * const result = (struct ggml_tensor *)((char *)ctx->mem_buffer + obj_new->offs);\n\n    *result = (struct ggml_tensor) {\n        /*.type         =*/ type,\n        /*.backend      =*/ GGML_BACKEND_CPU,\n        /*.buffer       =*/ NULL,\n        /*.n_dims       =*/ n_dims,\n        /*.ne           =*/ { 1, 1, 1, 1 },\n        /*.nb           =*/ { 0, 0, 0, 0 },\n        /*.op           =*/ GGML_OP_NONE,\n        /*.op_params    =*/ { 0 },\n        /*.is_param     =*/ false,\n        /*.grad         =*/ NULL,\n        /*.src          =*/ { NULL },\n        /*.is_finish    =*/ ATOMIC_VAR_INIT(0),\n        /*.perf_runs    =*/ 0,\n        /*.perf_cycles  =*/ 0,\n        /*.perf_time_us =*/ 0,\n        /*.view_src     =*/ view_src,\n        /*.view_offs    =*/ view_offs,\n        /*.data         =*/ obj_alloc_size > 0 ? (void *)(result + 1) : data,\n        /*.name         =*/ { 0 },\n        /*.extra        =*/ NULL,\n        /*.padding      =*/ { 0 },\n    };\n\n    // TODO: this should not be needed as long as we don't rely on aligned SIMD loads\n    //ggml_assert_aligned(result->data);\n\n    for (int i = 0; i < n_dims; i++) {\n        result->ne[i] = ne[i];\n    }\n\n    result->nb[0] = ggml_type_size(type);\n    result->nb[1] = result->nb[0]*(result->ne[0]/ggml_blck_size(type));\n    for (int i = 2; i < GGML_MAX_DIMS; i++) {\n        result->nb[i] = result->nb[i - 1]*result->ne[i - 1];\n    }\n\n    ctx->n_objects++;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_new_tensor(\n        struct ggml_context * ctx,\n        enum   ggml_type      type,\n        int                   n_dims,\n        const int64_t       * ne) {\n    return ggml_new_tensor_impl(ctx, type, n_dims, ne, NULL, 0);\n}\n\nstruct ggml_tensor * ggml_new_tensor_1d(\n        struct ggml_context * ctx,\n        enum   ggml_type      type,\n        int64_t ne0) {\n    return ggml_new_tensor(ctx, type, 1, &ne0);\n}\n\nstruct ggml_tensor * ggml_new_tensor_2d(\n        struct ggml_context * ctx,\n        enum   ggml_type      type,\n        int64_t ne0,\n        int64_t ne1) {\n    const int64_t ne[2] = { ne0, ne1 };\n    return ggml_new_tensor(ctx, type, 2, ne);\n}\n\nstruct ggml_tensor * ggml_new_tensor_3d(\n        struct ggml_context * ctx,\n        enum   ggml_type      type,\n        int64_t ne0,\n        int64_t ne1,\n        int64_t ne2) {\n    const int64_t ne[3] = { ne0, ne1, ne2 };\n    return ggml_new_tensor(ctx, type, 3, ne);\n}\n\nstruct ggml_tensor * ggml_new_tensor_4d(\n        struct ggml_context * ctx,\n        enum   ggml_type type,\n        int64_t ne0,\n        int64_t ne1,\n        int64_t ne2,\n        int64_t ne3) {\n    const int64_t ne[4] = { ne0, ne1, ne2, ne3 };\n    return ggml_new_tensor(ctx, type, 4, ne);\n}\n\nstruct ggml_tensor * ggml_new_i32(struct ggml_context * ctx, int32_t value) {\n    ggml_scratch_save(ctx);\n\n    struct ggml_tensor * result = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, 1);\n\n    ggml_scratch_load(ctx);\n\n    ggml_set_i32(result, value);\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_new_f32(struct ggml_context * ctx, float value) {\n    ggml_scratch_save(ctx);\n\n    struct ggml_tensor * result = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);\n\n    ggml_scratch_load(ctx);\n\n    ggml_set_f32(result, value);\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_dup_tensor(struct ggml_context * ctx, const struct ggml_tensor * src) {\n    return ggml_new_tensor(ctx, src->type, src->n_dims, src->ne);\n}\n\nstatic void ggml_set_op_params(struct ggml_tensor * tensor, const void * params, size_t params_size) {\n    GGML_ASSERT(tensor != NULL); // silence -Warray-bounds warnings\n    assert(params_size <= GGML_MAX_OP_PARAMS);\n    memcpy(tensor->op_params, params, params_size);\n}\n\nstatic int32_t ggml_get_op_params_i32(const struct ggml_tensor * tensor, uint32_t i) {\n    assert(i < GGML_MAX_OP_PARAMS / sizeof(int32_t));\n    return ((const int32_t *)(tensor->op_params))[i];\n}\n\nstatic void ggml_set_op_params_i32(struct ggml_tensor * tensor, uint32_t i, int32_t value) {\n    assert(i < GGML_MAX_OP_PARAMS / sizeof(int32_t));\n    ((int32_t *)(tensor->op_params))[i] = value;\n}\n\nstruct ggml_tensor * ggml_set_zero(struct ggml_tensor * tensor) {\n    memset(tensor->data, 0, ggml_nbytes(tensor));\n    return tensor;\n}\n\nstruct ggml_tensor * ggml_set_i32 (struct ggml_tensor * tensor, int32_t value) {\n    const int n     = ggml_nrows(tensor);\n    const int nc    = tensor->ne[0];\n    const size_t n1 = tensor->nb[1];\n\n    char * const data = tensor->data;\n\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            {\n                assert(tensor->nb[0] == sizeof(int8_t));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_i8(nc, (int8_t *)(data + i*n1), value);\n                }\n            } break;\n        case GGML_TYPE_I16:\n            {\n                assert(tensor->nb[0] == sizeof(int16_t));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_i16(nc, (int16_t *)(data + i*n1), value);\n                }\n            } break;\n        case GGML_TYPE_I32:\n            {\n                assert(tensor->nb[0] == sizeof(int32_t));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_i32(nc, (int32_t *)(data + i*n1), value);\n                }\n            } break;\n        case GGML_TYPE_F16:\n            {\n                assert(tensor->nb[0] == sizeof(ggml_fp16_t));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_f16(nc, (ggml_fp16_t *)(data + i*n1), GGML_FP32_TO_FP16(value));\n                }\n            } break;\n        case GGML_TYPE_F32:\n            {\n                assert(tensor->nb[0] == sizeof(float));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_f32(nc, (float *)(data + i*n1), value);\n                }\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n\n    return tensor;\n}\n\nstruct ggml_tensor * ggml_set_f32(struct ggml_tensor * tensor, float value) {\n    const int n     = ggml_nrows(tensor);\n    const int nc    = tensor->ne[0];\n    const size_t n1 = tensor->nb[1];\n\n    char * const data = tensor->data;\n\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            {\n                assert(tensor->nb[0] == sizeof(int8_t));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_i8(nc, (int8_t *)(data + i*n1), value);\n                }\n            } break;\n        case GGML_TYPE_I16:\n            {\n                assert(tensor->nb[0] == sizeof(int16_t));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_i16(nc, (int16_t *)(data + i*n1), value);\n                }\n            } break;\n        case GGML_TYPE_I32:\n            {\n                assert(tensor->nb[0] == sizeof(int32_t));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_i32(nc, (int32_t *)(data + i*n1), value);\n                }\n            } break;\n        case GGML_TYPE_F16:\n            {\n                assert(tensor->nb[0] == sizeof(ggml_fp16_t));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_f16(nc, (ggml_fp16_t *)(data + i*n1), GGML_FP32_TO_FP16(value));\n                }\n            } break;\n        case GGML_TYPE_F32:\n            {\n                assert(tensor->nb[0] == sizeof(float));\n                for (int i = 0; i < n; i++) {\n                    ggml_vec_set_f32(nc, (float *)(data + i*n1), value);\n                }\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n\n    return tensor;\n}\n\nvoid ggml_unravel_index(const struct ggml_tensor * tensor, int64_t i, int64_t * i0, int64_t * i1, int64_t * i2, int64_t * i3) {\n    const int64_t ne2 = tensor->ne[2];\n    const int64_t ne1 = tensor->ne[1];\n    const int64_t ne0 = tensor->ne[0];\n\n    const int64_t i3_ = (i/(ne2*ne1*ne0));\n    const int64_t i2_ = (i - i3_*ne2*ne1*ne0)/(ne1*ne0);\n    const int64_t i1_ = (i - i3_*ne2*ne1*ne0 - i2_*ne1*ne0)/ne0;\n    const int64_t i0_ = (i - i3_*ne2*ne1*ne0 - i2_*ne1*ne0 - i1_*ne0);\n\n    if (i0) {\n        * i0 = i0_;\n    }\n    if (i1) {\n        * i1 = i1_;\n    }\n    if (i2) {\n        * i2 = i2_;\n    }\n    if (i3) {\n        * i3 = i3_;\n    }\n}\n\nint32_t ggml_get_i32_1d(const struct ggml_tensor * tensor, int i) {\n    if (!ggml_is_contiguous(tensor)) {\n        int64_t id[4] = { 0, 0, 0, 0 };\n        ggml_unravel_index(tensor, i, &id[0], &id[1], &id[2], &id[3]);\n        return ggml_get_i32_nd(tensor, id[0], id[1], id[2], id[3]);\n    }\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int8_t));\n                return ((int8_t *)(tensor->data))[i];\n            }\n        case GGML_TYPE_I16:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int16_t));\n                return ((int16_t *)(tensor->data))[i];\n            }\n        case GGML_TYPE_I32:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int32_t));\n                return ((int32_t *)(tensor->data))[i];\n            }\n        case GGML_TYPE_F16:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(ggml_fp16_t));\n                return GGML_FP16_TO_FP32(((ggml_fp16_t *)(tensor->data))[i]);\n            }\n        case GGML_TYPE_F32:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(float));\n                return ((float *)(tensor->data))[i];\n            }\n        default:\n            {\n                GGML_ASSERT(false);\n            }\n    }\n\n    return 0.0f;\n}\n\nvoid ggml_set_i32_1d(const struct ggml_tensor * tensor, int i, int32_t value) {\n    if (!ggml_is_contiguous(tensor)) {\n        int64_t id[4] = { 0, 0, 0, 0 };\n        ggml_unravel_index(tensor, i, &id[0], &id[1], &id[2], &id[3]);\n        ggml_set_i32_nd(tensor, id[0], id[1], id[2], id[3], value);\n        return;\n    }\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int8_t));\n                ((int8_t *)(tensor->data))[i] = value;\n            } break;\n        case GGML_TYPE_I16:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int16_t));\n                ((int16_t *)(tensor->data))[i] = value;\n            } break;\n        case GGML_TYPE_I32:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int32_t));\n                ((int32_t *)(tensor->data))[i] = value;\n            } break;\n        case GGML_TYPE_F16:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(ggml_fp16_t));\n                ((ggml_fp16_t *)(tensor->data))[i] = GGML_FP32_TO_FP16(value);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(float));\n                ((float *)(tensor->data))[i] = value;\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\nint32_t ggml_get_i32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3) {\n    void * data   = (char *) tensor->data + i0*tensor->nb[0] + i1*tensor->nb[1] + i2*tensor->nb[2] + i3*tensor->nb[3];\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            return ((int8_t *) data)[0];\n        case GGML_TYPE_I16:\n            return ((int16_t *) data)[0];\n        case GGML_TYPE_I32:\n            return ((int32_t *) data)[0];\n        case GGML_TYPE_F16:\n            return GGML_FP16_TO_FP32(((ggml_fp16_t *) data)[0]);\n        case GGML_TYPE_F32:\n            return ((float *) data)[0];\n        default:\n            GGML_ASSERT(false);\n    }\n\n    return 0.0f;\n}\n\nvoid ggml_set_i32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3, int32_t value) {\n    void * data   = (char *) tensor->data + i0*tensor->nb[0] + i1*tensor->nb[1] + i2*tensor->nb[2] + i3*tensor->nb[3];\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            {\n                ((int8_t *)(data))[0] = value;\n            } break;\n        case GGML_TYPE_I16:\n            {\n                ((int16_t *)(data))[0] = value;\n            } break;\n        case GGML_TYPE_I32:\n            {\n                ((int32_t *)(data))[0] = value;\n            } break;\n        case GGML_TYPE_F16:\n            {\n                ((ggml_fp16_t *)(data))[0] = GGML_FP32_TO_FP16(value);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ((float *)(data))[0] = value;\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\nfloat ggml_get_f32_1d(const struct ggml_tensor * tensor, int i) {\n    if (!ggml_is_contiguous(tensor)) {\n        int64_t id[4] = { 0, 0, 0, 0 };\n        ggml_unravel_index(tensor, i, &id[0], &id[1], &id[2], &id[3]);\n        return ggml_get_f32_nd(tensor, id[0], id[1], id[2], id[3]);\n    }\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int8_t));\n                return ((int8_t *)(tensor->data))[i];\n            }\n        case GGML_TYPE_I16:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int16_t));\n                return ((int16_t *)(tensor->data))[i];\n            }\n        case GGML_TYPE_I32:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int32_t));\n                return ((int32_t *)(tensor->data))[i];\n            }\n        case GGML_TYPE_F16:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(ggml_fp16_t));\n                return GGML_FP16_TO_FP32(((ggml_fp16_t *)(tensor->data))[i]);\n            }\n        case GGML_TYPE_F32:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(float));\n                return ((float *)(tensor->data))[i];\n            }\n        default:\n            {\n                GGML_ASSERT(false);\n            }\n    }\n\n    return 0.0f;\n}\n\nvoid ggml_set_f32_1d(const struct ggml_tensor * tensor, int i, float value) {\n    if (!ggml_is_contiguous(tensor)) {\n        int64_t id[4] = { 0, 0, 0, 0 };\n        ggml_unravel_index(tensor, i, &id[0], &id[1], &id[2], &id[3]);\n        ggml_set_f32_nd(tensor, id[0], id[1], id[2], id[3], value);\n        return;\n    }\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int8_t));\n                ((int8_t *)(tensor->data))[i] = value;\n            } break;\n        case GGML_TYPE_I16:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int16_t));\n                ((int16_t *)(tensor->data))[i] = value;\n            } break;\n        case GGML_TYPE_I32:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(int32_t));\n                ((int32_t *)(tensor->data))[i] = value;\n            } break;\n        case GGML_TYPE_F16:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(ggml_fp16_t));\n                ((ggml_fp16_t *)(tensor->data))[i] = GGML_FP32_TO_FP16(value);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                GGML_ASSERT(tensor->nb[0] == sizeof(float));\n                ((float *)(tensor->data))[i] = value;\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\nfloat ggml_get_f32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3) {\n    void * data   = (char *) tensor->data + i0*tensor->nb[0] + i1*tensor->nb[1] + i2*tensor->nb[2] + i3*tensor->nb[3];\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            return ((int8_t *) data)[0];\n        case GGML_TYPE_I16:\n            return ((int16_t *) data)[0];\n        case GGML_TYPE_I32:\n            return ((int32_t *) data)[0];\n        case GGML_TYPE_F16:\n            return GGML_FP16_TO_FP32(((ggml_fp16_t *) data)[0]);\n        case GGML_TYPE_F32:\n            return ((float *) data)[0];\n        default:\n            GGML_ASSERT(false);\n    }\n\n    return 0.0f;\n}\n\nvoid ggml_set_f32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3, float value) {\n    void * data   = (char *) tensor->data + i0*tensor->nb[0] + i1*tensor->nb[1] + i2*tensor->nb[2] + i3*tensor->nb[3];\n    switch (tensor->type) {\n        case GGML_TYPE_I8:\n            {\n                ((int8_t *)(data))[0] = value;\n            } break;\n        case GGML_TYPE_I16:\n            {\n                ((int16_t *)(data))[0] = value;\n            } break;\n        case GGML_TYPE_I32:\n            {\n                ((int32_t *)(data))[0] = value;\n            } break;\n        case GGML_TYPE_F16:\n            {\n                ((ggml_fp16_t *)(data))[0] = GGML_FP32_TO_FP16(value);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ((float *)(data))[0] = value;\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\nvoid * ggml_get_data(const struct ggml_tensor * tensor) {\n    return tensor->data;\n}\n\nfloat * ggml_get_data_f32(const struct ggml_tensor * tensor) {\n    assert(tensor->type == GGML_TYPE_F32);\n    return (float *)(tensor->data);\n}\n\nint32_t * ggml_get_data_i32(const struct ggml_tensor * tensor) {\n    assert(tensor->type == GGML_TYPE_I32);\n    return (int32_t *)(tensor->data);\n}\n\nenum ggml_unary_op ggml_get_unary_op(const struct ggml_tensor * tensor) {\n    GGML_ASSERT(tensor->op == GGML_OP_UNARY);\n    return (enum ggml_unary_op) ggml_get_op_params_i32(tensor, 0);\n}\n\nconst char * ggml_get_name(const struct ggml_tensor * tensor) {\n    if (tensor == NULL) return NULL;\n    return tensor->name;\n}\n\nstruct ggml_tensor * ggml_set_name(struct ggml_tensor * tensor, const char * name) {\n    if (tensor == NULL) return NULL;\n    strncpy(tensor->name, name, sizeof(tensor->name));\n    tensor->name[sizeof(tensor->name) - 1] = '\\0';\n    return tensor;\n}\n\nstruct ggml_tensor * ggml_format_name(struct ggml_tensor * tensor, const char * fmt, ...) {\n    va_list args;\n    va_start(args, fmt);\n    vsnprintf(tensor->name, sizeof(tensor->name), fmt, args);\n    va_end(args);\n    return tensor;\n}\n\nstruct ggml_tensor * ggml_view_tensor(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * src) {\n    struct ggml_tensor * result = ggml_new_tensor_impl(ctx, src->type, src->n_dims, src->ne, src, 0);\n    ggml_format_name(result, \"%s (view)\", src->name);\n\n    for (int i = 0; i < GGML_MAX_DIMS; i++) {\n        result->nb[i] = src->nb[i];\n    }\n    result->op = GGML_OP_VIEW;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_get_first_tensor(struct ggml_context * ctx) {\n    struct ggml_object * obj = ctx->objects_begin;\n\n    char * const mem_buffer = ctx->mem_buffer;\n\n    while (obj != NULL) {\n        if (obj->type == GGML_OBJECT_TENSOR) {\n            return (struct ggml_tensor *)(mem_buffer + obj->offs);\n        }\n\n        obj = obj->next;\n    }\n\n    return NULL;\n}\n\nstruct ggml_tensor * ggml_get_next_tensor(struct ggml_context * ctx, struct ggml_tensor * tensor) {\n    struct ggml_object * obj = (struct ggml_object *) ((char *)tensor - GGML_OBJECT_SIZE);\n    obj = obj->next;\n\n    char * const mem_buffer = ctx->mem_buffer;\n\n    while (obj != NULL) {\n        if (obj->type == GGML_OBJECT_TENSOR) {\n            return (struct ggml_tensor *)(mem_buffer + obj->offs);\n        }\n\n        obj = obj->next;\n    }\n\n    return NULL;\n}\n\nstruct ggml_tensor * ggml_get_tensor(struct ggml_context * ctx, const char * name) {\n    struct ggml_object * obj = ctx->objects_begin;\n\n    char * const mem_buffer = ctx->mem_buffer;\n\n    while (obj != NULL) {\n        if (obj->type == GGML_OBJECT_TENSOR) {\n            struct ggml_tensor * cur = (struct ggml_tensor *)(mem_buffer + obj->offs);\n            if (strcmp(cur->name, name) == 0) {\n                return cur;\n            }\n        }\n\n        obj = obj->next;\n    }\n\n    return NULL;\n}\n\n////////////////////////////////////////////////////////////////////////////////\n\n// ggml_dup\n\nstatic struct ggml_tensor * ggml_dup_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        bool inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_DUP;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_dup(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a) {\n    return ggml_dup_impl(ctx, a, false);\n}\n\nstruct ggml_tensor * ggml_dup_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a) {\n    return ggml_dup_impl(ctx, a, true);\n}\n\n// ggml_add\n\nstatic struct ggml_tensor * ggml_add_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        bool inplace) {\n    // TODO: support less-strict constraint\n    //       GGML_ASSERT(ggml_can_repeat(b, a));\n    GGML_ASSERT(ggml_can_repeat_rows(b, a));\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        // TODO: support backward pass for broadcasting\n        GGML_ASSERT(ggml_are_same_shape(a, b));\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_ADD;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = NULL;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_add(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_add_impl(ctx, a, b, false);\n}\n\nstruct ggml_tensor * ggml_add_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_add_impl(ctx, a, b, true);\n}\n\n// ggml_add_cast\n\nstatic struct ggml_tensor * ggml_add_cast_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        enum   ggml_type     type) {\n    // TODO: support less-strict constraint\n    //       GGML_ASSERT(ggml_can_repeat(b, a));\n    GGML_ASSERT(ggml_can_repeat_rows(b, a));\n    GGML_ASSERT(ggml_is_quantized(a->type) || a->type == GGML_TYPE_F16); // currently only supported for quantized input and f16\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        // TODO: support backward pass for broadcasting\n        GGML_ASSERT(ggml_are_same_shape(a, b));\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor(ctx, type, a->n_dims, a->ne);\n\n    result->op   = GGML_OP_ADD;\n    result->grad = is_node ? ggml_new_tensor(ctx, GGML_TYPE_F32, a->n_dims, a->ne) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstatic struct ggml_tensor * ggml_add_idx_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        struct ggml_tensor * idx,\n        bool inplace) {\n    // TODO: support less-strict constraint\n    //       GGML_ASSERT(ggml_can_repeat(b, a));\n    // GGML_ASSERT(ggml_can_repeat_rows(b, a));\n    // printf(\"in add_idx\\n\");\n    if (a == NULL)\n        return b;\n    if (b == NULL)\n        return a;\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        // TODO: support backward pass for broadcasting\n        GGML_ASSERT(ggml_are_same_shape(a, b));\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_ADD;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = idx;\n\n    return result;\n}\n// add for all gather\nstruct ggml_tensor * ggml_add_idx(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        struct ggml_tensor * idx) {\n    return ggml_add_idx_impl(ctx, a, b, idx, false);\n}\n\nstruct ggml_tensor * ggml_add_cast(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        enum   ggml_type     type) {\n    return ggml_add_cast_impl(ctx, a, b, type);\n}\n\n// ggml_add1\n\nstatic struct ggml_tensor * ggml_add1_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        bool inplace) {\n    GGML_ASSERT(ggml_is_scalar(b));\n    GGML_ASSERT(ggml_is_padded_1d(a));\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_ADD1;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_add1(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_add1_impl(ctx, a, b, false);\n}\n\nstruct ggml_tensor * ggml_add1_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_add1_impl(ctx, a, b, true);\n}\n\n// ggml_acc\n\nstatic struct ggml_tensor * ggml_acc_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        size_t               nb1,\n        size_t               nb2,\n        size_t               nb3,\n        size_t               offset,\n        bool inplace) {\n    GGML_ASSERT(ggml_nelements(b) <= ggml_nelements(a));\n    GGML_ASSERT(ggml_is_contiguous(a));\n    GGML_ASSERT(a->type == GGML_TYPE_F32);\n    GGML_ASSERT(b->type == GGML_TYPE_F32);\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    int32_t params[] = { nb1, nb2, nb3, offset, inplace ? 1 : 0 };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_ACC;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_acc(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        size_t               nb1,\n        size_t               nb2,\n        size_t               nb3,\n        size_t               offset) {\n    return ggml_acc_impl(ctx, a, b, nb1, nb2, nb3, offset, false);\n}\n\nstruct ggml_tensor * ggml_acc_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        size_t               nb1,\n        size_t               nb2,\n        size_t               nb3,\n        size_t               offset) {\n    return ggml_acc_impl(ctx, a, b, nb1, nb2, nb3, offset, true);\n}\n\n// ggml_sub\n\nstatic struct ggml_tensor * ggml_sub_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        bool inplace) {\n    GGML_ASSERT(ggml_are_same_shape(a, b));\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_SUB;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_sub(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_sub_impl(ctx, a, b, false);\n}\n\nstruct ggml_tensor * ggml_sub_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_sub_impl(ctx, a, b, true);\n}\n\n// ggml_mul\n\nstatic struct ggml_tensor * ggml_mul_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        bool inplace) {\n    // TODO: support less-strict constraint\n    //       GGML_ASSERT(ggml_can_repeat(b, a));\n    GGML_ASSERT(ggml_can_repeat_rows(b, a));\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        // TODO: support backward pass for broadcasting\n        GGML_ASSERT(ggml_are_same_shape(a, b));\n        is_node = true;\n    }\n\n    if (inplace) {\n        GGML_ASSERT(!is_node);\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_MUL;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_mul(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_mul_impl(ctx, a, b, false);\n}\n\nstruct ggml_tensor * ggml_mul_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_mul_impl(ctx, a, b, true);\n}\n\n// ggml_div\n\nstatic struct ggml_tensor * ggml_div_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b,\n        bool inplace) {\n    GGML_ASSERT(ggml_are_same_shape(a, b));\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        is_node = true;\n    }\n\n    if (inplace) {\n        GGML_ASSERT(!is_node);\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_DIV;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_div(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_div_impl(ctx, a, b, false);\n}\n\nstruct ggml_tensor * ggml_div_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_div_impl(ctx, a, b, true);\n}\n\n// ggml_sqr\n\nstatic struct ggml_tensor * ggml_sqr_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        bool inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_SQR;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_sqr(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_sqr_impl(ctx, a, false);\n}\n\nstruct ggml_tensor * ggml_sqr_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_sqr_impl(ctx, a, true);\n}\n\n// ggml_sqrt\n\nstatic struct ggml_tensor * ggml_sqrt_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        bool inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_SQRT;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_sqrt(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_sqrt_impl(ctx, a, false);\n}\n\nstruct ggml_tensor * ggml_sqrt_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_sqrt_impl(ctx, a, true);\n}\n\n// ggml_log\n\nstatic struct ggml_tensor * ggml_log_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        bool inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_LOG;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_log(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_log_impl(ctx, a, false);\n}\n\nstruct ggml_tensor * ggml_log_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_log_impl(ctx, a, true);\n}\n\n// ggml_sum\n\nstruct ggml_tensor * ggml_sum(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a) {\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor_1d(ctx, a->type, 1);\n\n    result->op   = GGML_OP_SUM;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_sum_rows\n\nstruct ggml_tensor * ggml_sum_rows(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a) {\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    int64_t ne[4] = {1,1,1,1};\n    for (int i=1; i<a->n_dims; ++i) {\n        ne[i] = a->ne[i];\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor(ctx, a->type, a->n_dims, ne);\n\n    result->op   = GGML_OP_SUM_ROWS;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_mean\n\nstruct ggml_tensor * ggml_mean(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a) {\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement\n        is_node = true;\n    }\n\n    int64_t ne[GGML_MAX_DIMS] = { 1, a->ne[1], a->ne[2], a->ne[3] };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, a->n_dims, ne);\n\n    result->op   = GGML_OP_MEAN;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_argmax\n\nstruct ggml_tensor * ggml_argmax(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a) {\n    GGML_ASSERT(ggml_is_matrix(a));\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false);\n        is_node = true;\n    }\n\n    int64_t ne[GGML_MAX_DIMS] = { a->ne[1], 1, 1, 1 };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_I32, a->n_dims, ne);\n\n    result->op   = GGML_OP_ARGMAX;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_repeat\n\nstruct ggml_tensor * ggml_repeat(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    GGML_ASSERT(ggml_can_repeat(a, b));\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor(ctx, a->type, b->n_dims, b->ne);\n\n    result->op   = GGML_OP_REPEAT;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_repeat_back\n\nstruct ggml_tensor * ggml_repeat_back(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    GGML_ASSERT(ggml_can_repeat(b, a));\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    if (ggml_are_same_shape(a, b) && !is_node) {\n        return a;\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor(ctx, a->type, b->n_dims, b->ne);\n\n    result->op   = GGML_OP_REPEAT_BACK;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_concat\n\nstruct ggml_tensor * ggml_concat(\n    struct ggml_context* ctx,\n    struct ggml_tensor* a,\n    struct ggml_tensor* b) {\n    GGML_ASSERT(a->ne[0] == b->ne[0] && a->ne[1] == b->ne[1] && a->ne[3] == b->ne[3]);\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor_4d(ctx, a->type, a->ne[0], a->ne[1], a->ne[2] + b->ne[2], a->ne[3]);\n\n    result->op = GGML_OP_CONCAT;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_abs\n\nstruct ggml_tensor * ggml_abs(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_ABS);\n}\n\nstruct ggml_tensor * ggml_abs_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_ABS);\n}\n\n// ggml_sgn\n\nstruct ggml_tensor * ggml_sgn(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_SGN);\n}\n\nstruct ggml_tensor * ggml_sgn_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_SGN);\n}\n\n// ggml_neg\n\nstruct ggml_tensor * ggml_neg(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_NEG);\n}\n\nstruct ggml_tensor * ggml_neg_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_NEG);\n}\n\n// ggml_step\n\nstruct ggml_tensor * ggml_step(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_STEP);\n}\n\nstruct ggml_tensor * ggml_step_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_STEP);\n}\n\n// ggml_tanh\n\nstruct ggml_tensor * ggml_tanh(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_TANH);\n}\n\nstruct ggml_tensor * ggml_tanh_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_TANH);\n}\n\n// ggml_elu\n\nstruct ggml_tensor * ggml_elu(\n    struct ggml_context * ctx,\n    struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_ELU);\n}\n\nstruct ggml_tensor * ggml_elu_inplace(\n    struct ggml_context * ctx,\n    struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_ELU);\n}\n\n// ggml_relu\n\nstruct ggml_tensor * ggml_relu(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_RELU);\n}\n\nstruct ggml_tensor * ggml_relu_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_RELU);\n}\n\n// ggml_leaky\n\nstruct ggml_tensor * ggml_leaky(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_LEAKY);\n}\n\n// ggml_gelu\n\nstruct ggml_tensor * ggml_gelu(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_GELU);\n}\n\nstruct ggml_tensor * ggml_gelu_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_GELU);\n}\n\n// ggml_gelu_quick\n\nstruct ggml_tensor * ggml_gelu_quick(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_GELU_QUICK);\n}\n\nstruct ggml_tensor * ggml_gelu_quick_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_GELU_QUICK);\n}\n\n// ggml_silu\n\nstruct ggml_tensor * ggml_silu(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary(ctx, a, GGML_UNARY_OP_SILU);\n}\n\nstruct ggml_tensor * ggml_silu_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_unary_inplace(ctx, a, GGML_UNARY_OP_SILU);\n}\n\n// ggml_silu_back\n\nstruct ggml_tensor * ggml_silu_back(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        // TODO: implement backward\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_SILU_BACK;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_norm\n\nstatic struct ggml_tensor * ggml_norm_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        float eps,\n        bool inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad)) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params(result, &eps, sizeof(eps));\n\n    result->op   = GGML_OP_NORM;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_norm(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        float eps) {\n    return ggml_norm_impl(ctx, a, eps, false);\n}\n\nstruct ggml_tensor * ggml_norm_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        float eps) {\n    return ggml_norm_impl(ctx, a, eps, true);\n}\n\n// ggml_rms_norm\n\nstatic struct ggml_tensor * ggml_rms_norm_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        float eps,\n        bool inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params(result, &eps, sizeof(eps));\n\n    result->op   = GGML_OP_RMS_NORM;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_rms_norm(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        float  eps) {\n    return ggml_rms_norm_impl(ctx, a, eps, false);\n}\n\nstruct ggml_tensor * ggml_rms_norm_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        float eps) {\n    return ggml_rms_norm_impl(ctx, a, eps, true);\n}\n\n// ggml_rms_norm_back\n\nstruct ggml_tensor * ggml_rms_norm_back(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        float  eps) {\n    bool is_node = false;\n\n    if (a->grad) {\n        // TODO: implement backward\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params(result, &eps, sizeof(eps));\n\n    result->op   = GGML_OP_RMS_NORM_BACK;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_group_norm\n\nstatic struct ggml_tensor * ggml_group_norm_impl(\n    struct ggml_context * ctx,\n    struct ggml_tensor * a,\n    int n_groups,\n    bool inplace) {\n\n    bool is_node = false;\n    if (!inplace && (a->grad)) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op = GGML_OP_GROUP_NORM;\n    result->op_params[0] = n_groups;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = NULL; // TODO: maybe store epsilon here?\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_group_norm(\n    struct ggml_context * ctx,\n    struct ggml_tensor * a,\n    int n_groups) {\n    return ggml_group_norm_impl(ctx, a, n_groups, false);\n}\n\nstruct ggml_tensor * ggml_group_norm_inplace(\n    struct ggml_context * ctx,\n    struct ggml_tensor * a,\n    int n_groups) {\n    return ggml_group_norm_impl(ctx, a, n_groups, true);\n}\n\n// ggml_mul_mat\n\nstruct ggml_tensor * ggml_mul_mat(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    GGML_ASSERT(ggml_can_mul_mat(a, b));\n    GGML_ASSERT(!ggml_is_transposed(a));\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[4] = { a->ne[1], b->ne[1], b->ne[2], b->ne[3] };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, MAX(a->n_dims, b->n_dims), ne);\n\n    result->op   = GGML_OP_MUL_MAT;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = NULL;\n    result->src[3] = NULL;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_mul_mat_idx_upscale(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        struct ggml_tensor  * sparse_idx,\n        struct ggml_tensor  * gpu_bucket,\n                      int64_t result_ne0) {\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[4] = { result_ne0, b->ne[1], b->ne[2], b->ne[3] };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, MAX(a->n_dims, b->n_dims), ne);\n\n    result->op   = GGML_OP_MUL_MAT_SPARSE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = sparse_idx;\n    result->src[3] = gpu_bucket;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_mul_mat_idx(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        struct ggml_tensor  * sparse_idx,\n        // Under hybrid inference, this tensor is to indicate which row are offloaded to GPU;\n        // When using full GPU inference, it is NULL.\n        struct ggml_tensor  * gpu_idx) {\n    GGML_ASSERT(!ggml_is_transposed(a));\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[4] = { a->ne[1], b->ne[1], b->ne[2], b->ne[3] };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, MAX(a->n_dims, b->n_dims), ne);\n\n    result->op   = GGML_OP_MUL_MAT_SPARSE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = sparse_idx;\n    result->src[3] = gpu_idx;\n\n    int32_t params[] = { gpu_idx ? 0 : 1 };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_axpy(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        struct ggml_tensor  * sparse_idx,\n        // Under CPU + GPU hybrid inference:\n        // When using GPU, this tensor is gpu_bucket to map the index back to the original tensor;\n        // When using CPU, this tensor is gpu_index to indicate which row are offloaded to GPU.\n        // Under full GPU/GPU inference, it is NULL.\n        struct ggml_tensor  * hybrid_aux) {\n    GGML_ASSERT(a != NULL && b != NULL);\n    GGML_ASSERT(!ggml_is_transposed(a));\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[4] = { a->ne[0], b->ne[1], b->ne[2], b->ne[3] };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, MAX(a->n_dims, b->n_dims), ne);\n\n    result->op   = GGML_OP_AXPY;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = sparse_idx;\n    result->src[3] = hybrid_aux;\n\n    int32_t params[] = { hybrid_aux ? 0 : 1 };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    return result;\n}\n\n// ggml_out_prod\n\nstruct ggml_tensor * ggml_out_prod(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    GGML_ASSERT(ggml_can_out_prod(a, b));\n    GGML_ASSERT(!ggml_is_transposed(a));\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    // a is broadcastable to b for ne[2] and ne[3] -> use b->ne[2] and b->ne[3]\n    const int64_t ne[4] = { a->ne[0], b->ne[0], b->ne[2], b->ne[3] };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, MAX(a->n_dims, b->n_dims), ne);\n\n    result->op   = GGML_OP_OUT_PROD;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_scale\n\nstatic struct ggml_tensor * ggml_scale_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        bool inplace) {\n    GGML_ASSERT(ggml_is_scalar(b));\n    GGML_ASSERT(ggml_is_padded_1d(a));\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_SCALE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_scale(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_scale_impl(ctx, a, b, false);\n}\n\nstruct ggml_tensor * ggml_scale_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_scale_impl(ctx, a, b, true);\n}\n\n// ggml_set\n\nstatic struct ggml_tensor * ggml_set_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        size_t                nb1,\n        size_t                nb2,\n        size_t                nb3,\n        size_t                offset,\n        bool inplace) {\n    GGML_ASSERT(ggml_nelements(a) >= ggml_nelements(b));\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    // make a view of the destination\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    int32_t params[] = { nb1, nb2, nb3, offset, inplace ? 1 : 0 };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_SET;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_set(\n        struct ggml_context * ctx,\n        struct ggml_tensor *  a,\n        struct ggml_tensor *  b,\n        size_t                nb1,\n        size_t                nb2,\n        size_t                nb3,\n        size_t                offset) {\n    return ggml_set_impl(ctx, a, b, nb1, nb2, nb3, offset, false);\n}\n\nstruct ggml_tensor * ggml_set_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor *  a,\n        struct ggml_tensor *  b,\n        size_t                nb1,\n        size_t                nb2,\n        size_t                nb3,\n        size_t                offset) {\n    return ggml_set_impl(ctx, a, b, nb1, nb2, nb3, offset, true);\n}\n\nstruct ggml_tensor * ggml_set_1d(\n        struct ggml_context * ctx,\n        struct ggml_tensor *  a,\n        struct ggml_tensor *  b,\n        size_t                offset) {\n    return ggml_set_impl(ctx, a, b, a->nb[1], a->nb[2], a->nb[3], offset, false);\n}\n\nstruct ggml_tensor * ggml_set_1d_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor *  a,\n        struct ggml_tensor *  b,\n        size_t                offset) {\n    return ggml_set_impl(ctx, a, b, a->nb[1], a->nb[2], a->nb[3], offset, true);\n}\n\nstruct ggml_tensor * ggml_set_2d(\n        struct ggml_context * ctx,\n        struct ggml_tensor *  a,\n        struct ggml_tensor *  b,\n        size_t                nb1,\n        size_t                offset) {\n    return ggml_set_impl(ctx, a, b, nb1, a->nb[2], a->nb[3], offset, false);\n}\n\nstruct ggml_tensor * ggml_set_2d_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor *  a,\n        struct ggml_tensor *  b,\n        size_t                nb1,\n        size_t                offset) {\n    return ggml_set_impl(ctx, a, b, nb1, a->nb[2], a->nb[3], offset, false);\n}\n\n// ggml_cpy\n\nstatic struct ggml_tensor * ggml_cpy_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        bool inplace) {\n    GGML_ASSERT(ggml_nelements(a) == ggml_nelements(b));\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        is_node = true;\n    }\n\n    // make a view of the destination\n    struct ggml_tensor * result = ggml_view_tensor(ctx, b);\n    if (strlen(b->name) > 0) {\n        ggml_format_name(result, \"%s (copy of %s)\", b->name, a->name);\n    } else {\n        ggml_format_name(result, \"%s (copy)\", a->name);\n    }\n\n    result->op   = GGML_OP_CPY;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_cpy(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_cpy_impl(ctx, a, b, false);\n}\n\nstruct ggml_tensor * ggml_cpy_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    return ggml_cpy_impl(ctx, a, b, true);\n}\n\n// ggml_cont\n\nstatic struct ggml_tensor * ggml_cont_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        bool inplace) {\n    bool is_node = false;\n\n    if (!inplace && a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n    ggml_format_name(result, \"%s (cont)\", a->name);\n\n    result->op   = GGML_OP_CONT;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_cont(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a) {\n    return ggml_cont_impl(ctx, a, false);\n}\n\nstruct ggml_tensor * ggml_cont_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a) {\n    return ggml_cont_impl(ctx, a, true);\n}\n\n// make contiguous, with new shape\nGGML_API struct ggml_tensor * ggml_cont_1d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0) {\n    return ggml_cont_4d(ctx, a, ne0, 1, 1, 1);\n}\n\nGGML_API struct ggml_tensor * ggml_cont_2d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1) {\n    return ggml_cont_4d(ctx, a, ne0, ne1, 1, 1);\n}\n\nGGML_API struct ggml_tensor * ggml_cont_3d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1,\n        int64_t               ne2) {\n    return ggml_cont_4d(ctx, a, ne0, ne1, ne2, 1);\n}\n\nstruct ggml_tensor * ggml_cont_4d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1,\n        int64_t               ne2,\n        int64_t               ne3) {\n    GGML_ASSERT(ggml_nelements(a) == (ne0*ne1*ne2*ne3));\n\n    bool is_node = false;\n\n    struct ggml_tensor * result = ggml_new_tensor_4d(ctx, a->type, ne0, ne1, ne2, ne3);\n    ggml_format_name(result, \"%s (cont)\", a->name);\n\n    result->op   = GGML_OP_CONT;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_reshape\n\nstruct ggml_tensor * ggml_reshape(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        struct ggml_tensor * b) {\n    GGML_ASSERT(ggml_is_contiguous(a));\n    // as only the shape of b is relevant, and not its memory layout, b is allowed to be non contiguous.\n    GGML_ASSERT(ggml_nelements(a) == ggml_nelements(b));\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    if (b->grad) {\n        // gradient propagation is not supported\n        //GGML_ASSERT(false);\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor_impl(ctx, a->type, b->n_dims, b->ne, a, 0);\n    ggml_format_name(result, \"%s (reshaped)\", a->name);\n\n    result->op   = GGML_OP_RESHAPE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_reshape_1d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0) {\n    GGML_ASSERT(ggml_is_contiguous(a));\n    GGML_ASSERT(ggml_nelements(a) == ne0);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[1] = { ne0 };\n    struct ggml_tensor * result = ggml_new_tensor_impl(ctx, a->type, 1, ne, a, 0);\n    ggml_format_name(result, \"%s (reshaped)\", a->name);\n\n    result->op   = GGML_OP_RESHAPE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_reshape_2d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1) {\n    GGML_ASSERT(ggml_is_contiguous(a));\n    GGML_ASSERT(ggml_nelements(a) == ne0*ne1);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[2] = { ne0, ne1 };\n    struct ggml_tensor * result = ggml_new_tensor_impl(ctx, a->type, 2, ne, a, 0);\n    ggml_format_name(result, \"%s (reshaped)\", a->name);\n\n    result->op   = GGML_OP_RESHAPE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_reshape_3d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1,\n        int64_t               ne2) {\n    GGML_ASSERT(ggml_is_contiguous(a));\n    GGML_ASSERT(ggml_nelements(a) == ne0*ne1*ne2);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[3] = { ne0, ne1, ne2 };\n    struct ggml_tensor * result = ggml_new_tensor_impl(ctx, a->type, 3, ne, a, 0);\n    ggml_format_name(result, \"%s (reshaped)\", a->name);\n\n    result->op   = GGML_OP_RESHAPE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_reshape_4d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1,\n        int64_t               ne2,\n        int64_t               ne3) {\n    GGML_ASSERT(ggml_is_contiguous(a));\n    GGML_ASSERT(ggml_nelements(a) == ne0*ne1*ne2*ne3);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[4] = { ne0, ne1, ne2, ne3 };\n    struct ggml_tensor * result = ggml_new_tensor_impl(ctx, a->type, 4, ne, a, 0);\n    ggml_format_name(result, \"%s (reshaped)\", a->name);\n\n    result->op   = GGML_OP_RESHAPE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstatic struct ggml_tensor * ggml_view_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   n_dims,\n        const int64_t       * ne,\n        size_t                offset) {\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor_impl(ctx, a->type, n_dims, ne, a, offset);\n    ggml_format_name(result, \"%s (view)\", a->name);\n\n    ggml_set_op_params(result, &offset, sizeof(offset));\n\n    result->op   = GGML_OP_VIEW;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_view_1d\n\nstruct ggml_tensor * ggml_view_1d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        size_t                offset) {\n\n    struct ggml_tensor * result = ggml_view_impl(ctx, a, 1, &ne0, offset);\n\n    return result;\n}\n\n// ggml_view_2d\n\nstruct ggml_tensor * ggml_view_2d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1,\n        size_t                nb1,\n        size_t                offset) {\n\n    const int64_t ne[2] = { ne0, ne1 };\n\n    struct ggml_tensor * result = ggml_view_impl(ctx, a, 2, ne, offset);\n\n    result->nb[1] = nb1;\n    result->nb[2] = result->nb[1]*ne1;\n    result->nb[3] = result->nb[2];\n\n    return result;\n}\n\n// ggml_view_3d\n\nstruct ggml_tensor * ggml_view_3d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1,\n        int64_t               ne2,\n        size_t                nb1,\n        size_t                nb2,\n        size_t                offset) {\n\n    const int64_t ne[3] = { ne0, ne1, ne2 };\n\n    struct ggml_tensor * result = ggml_view_impl(ctx, a, 3, ne, offset);\n\n    result->nb[1] = nb1;\n    result->nb[2] = nb2;\n    result->nb[3] = result->nb[2]*ne2;\n\n    return result;\n}\n\n// ggml_view_4d\n\nstruct ggml_tensor * ggml_view_4d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int64_t               ne0,\n        int64_t               ne1,\n        int64_t               ne2,\n        int64_t               ne3,\n        size_t                nb1,\n        size_t                nb2,\n        size_t                nb3,\n        size_t                offset) {\n\n    const int64_t ne[4] = { ne0, ne1, ne2, ne3 };\n\n    struct ggml_tensor * result = ggml_view_impl(ctx, a, 4, ne, offset);\n\n    result->nb[1] = nb1;\n    result->nb[2] = nb2;\n    result->nb[3] = nb3;\n\n    return result;\n}\n\n// ggml_permute\n\nstruct ggml_tensor * ggml_permute(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   axis0,\n        int                   axis1,\n        int                   axis2,\n        int                   axis3) {\n    GGML_ASSERT(axis0 >= 0 && axis0 < GGML_MAX_DIMS);\n    GGML_ASSERT(axis1 >= 0 && axis1 < GGML_MAX_DIMS);\n    GGML_ASSERT(axis2 >= 0 && axis2 < GGML_MAX_DIMS);\n    GGML_ASSERT(axis3 >= 0 && axis3 < GGML_MAX_DIMS);\n\n    GGML_ASSERT(axis0 != axis1);\n    GGML_ASSERT(axis0 != axis2);\n    GGML_ASSERT(axis0 != axis3);\n    GGML_ASSERT(axis1 != axis2);\n    GGML_ASSERT(axis1 != axis3);\n    GGML_ASSERT(axis2 != axis3);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_view_tensor(ctx, a);\n    ggml_format_name(result, \"%s (permuted)\", a->name);\n\n    int ne[GGML_MAX_DIMS];\n    int nb[GGML_MAX_DIMS];\n\n    ne[axis0] = a->ne[0];\n    ne[axis1] = a->ne[1];\n    ne[axis2] = a->ne[2];\n    ne[axis3] = a->ne[3];\n\n    nb[axis0] = a->nb[0];\n    nb[axis1] = a->nb[1];\n    nb[axis2] = a->nb[2];\n    nb[axis3] = a->nb[3];\n\n    result->ne[0] = ne[0];\n    result->ne[1] = ne[1];\n    result->ne[2] = ne[2];\n    result->ne[3] = ne[3];\n\n    result->nb[0] = nb[0];\n    result->nb[1] = nb[1];\n    result->nb[2] = nb[2];\n    result->nb[3] = nb[3];\n\n    result->op   = GGML_OP_PERMUTE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    int32_t params[] = { axis0, axis1, axis2, axis3 };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    return result;\n}\n\n// ggml_transpose\n\nstruct ggml_tensor * ggml_transpose(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_view_tensor(ctx, a);\n    ggml_format_name(result, \"%s (transposed)\", a->name);\n\n    result->ne[0] = a->ne[1];\n    result->ne[1] = a->ne[0];\n\n    result->nb[0] = a->nb[1];\n    result->nb[1] = a->nb[0];\n\n    result->op   = GGML_OP_TRANSPOSE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_get_rows\n\nstruct ggml_tensor * ggml_get_rows(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    GGML_ASSERT(ggml_is_matrix(a) && ggml_is_vector(b) && b->type == GGML_TYPE_I32);\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    // TODO: implement non F32 return\n    //struct ggml_tensor * result = ggml_new_tensor_2d(ctx, a->type, a->ne[0], b->ne[0]);\n    struct ggml_tensor * result = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, a->ne[0], b->ne[0]);\n\n    result->op   = GGML_OP_GET_ROWS;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_get_rows_back\n\nstruct ggml_tensor * ggml_get_rows_back(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        struct ggml_tensor  * c) {\n    GGML_ASSERT(ggml_is_matrix(a) && ggml_is_vector(b) && b->type == GGML_TYPE_I32);\n    GGML_ASSERT(ggml_is_matrix(c) && (a->ne[0] == c->ne[0]));\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    // TODO: implement non F32 return\n    //struct ggml_tensor * result = ggml_new_tensor_2d(ctx, a->type, a->ne[0], b->ne[0]);\n    struct ggml_tensor * result = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, c->ne[0], c->ne[1]);\n\n    result->op   = GGML_OP_GET_ROWS_BACK;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_diag\n\nstruct ggml_tensor * ggml_diag(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    GGML_ASSERT(a->ne[1] == 1);\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    const int64_t ne[4] = { a->ne[0], a->ne[0], a->ne[2], a->ne[3] };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, a->type, MAX(a->n_dims, 2), ne);\n\n    result->op   = GGML_OP_DIAG;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_diag_mask_inf\n\nstatic struct ggml_tensor * ggml_diag_mask_inf_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   n_past,\n        bool                  inplace) {\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    int32_t params[] = { n_past };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_DIAG_MASK_INF;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_diag_mask_inf(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   n_past) {\n    return ggml_diag_mask_inf_impl(ctx, a, n_past, false);\n}\n\nstruct ggml_tensor * ggml_diag_mask_inf_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   n_past) {\n    return ggml_diag_mask_inf_impl(ctx, a, n_past, true);\n}\n\n// ggml_diag_mask_zero\n\nstatic struct ggml_tensor * ggml_diag_mask_zero_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   n_past,\n        bool                  inplace) {\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    int32_t params[] = { n_past };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_DIAG_MASK_ZERO;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_diag_mask_zero(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   n_past) {\n    return ggml_diag_mask_zero_impl(ctx, a, n_past, false);\n}\n\nstruct ggml_tensor * ggml_diag_mask_zero_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   n_past) {\n    return ggml_diag_mask_zero_impl(ctx, a, n_past, true);\n}\n\n// ggml_soft_max\n\nstatic struct ggml_tensor * ggml_soft_max_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        bool                  inplace) {\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_SOFT_MAX;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_soft_max(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_soft_max_impl(ctx, a, false);\n}\n\nstruct ggml_tensor * ggml_soft_max_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a) {\n    return ggml_soft_max_impl(ctx, a, true);\n}\n\n// ggml_soft_max_back\n\nstatic struct ggml_tensor * ggml_soft_max_back_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        bool                  inplace) {\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true; // TODO : implement backward pass\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_SOFT_MAX_BACK;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_soft_max_back(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_soft_max_back_impl(ctx, a, b, false);\n}\n\nstruct ggml_tensor * ggml_soft_max_back_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_soft_max_back_impl(ctx, a, b, true);\n}\n\n// ggml_rope\n\nstatic struct ggml_tensor * ggml_rope_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   n_dims,\n        int                   mode,\n        int                   n_ctx,\n        int                   n_orig_ctx,\n        float                 freq_base,\n        float                 freq_scale,\n        float                 ext_factor,\n        float                 attn_factor,\n        float                 beta_fast,\n        float                 beta_slow,\n        float                 xpos_base,\n        bool                  xpos_down,\n        bool                  inplace) {\n    GGML_ASSERT(ggml_is_vector(b));\n    GGML_ASSERT(b->type == GGML_TYPE_I32);\n    GGML_ASSERT(a->ne[2] == b->ne[0]);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    int32_t params[13] = { /*n_past*/ 0, n_dims, mode, n_ctx, n_orig_ctx };\n    memcpy(params +  5, &freq_base,    sizeof(float));\n    memcpy(params +  6, &freq_scale,   sizeof(float));\n    memcpy(params +  7, &ext_factor,   sizeof(float));\n    memcpy(params +  8, &attn_factor,  sizeof(float));\n    memcpy(params +  9, &beta_fast,    sizeof(float));\n    memcpy(params + 10, &beta_slow,    sizeof(float));\n    memcpy(params + 11, &xpos_base,    sizeof(float));\n    memcpy(params + 12, &xpos_down,    sizeof(bool));\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_ROPE;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_rope(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   n_dims,\n        int                   mode,\n        int                   n_ctx) {\n    return ggml_rope_impl(\n        ctx, a, b, n_dims, mode, n_ctx, 0, 10000.0f, 1.0f, 0.0f, 1.0f, 0.0f, 0.0f, 0.0f, false, false\n    );\n}\n\nstruct ggml_tensor * ggml_rope_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   n_dims,\n        int                   mode,\n        int                   n_ctx) {\n    return ggml_rope_impl(\n        ctx, a, b, n_dims, mode, n_ctx, 0, 10000.0f, 1.0f, 0.0f, 1.0f, 0.0f, 0.0f, 0.0f, false, true\n    );\n}\n\nstruct ggml_tensor * ggml_rope_custom(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   n_dims,\n        int                   mode,\n        int                   n_ctx,\n        int                   n_orig_ctx,\n        float                 freq_base,\n        float                 freq_scale,\n        float                 ext_factor,\n        float                 attn_factor,\n        float                 beta_fast,\n        float                 beta_slow) {\n    return ggml_rope_impl(\n        ctx, a, b, n_dims, mode, n_ctx, n_orig_ctx, freq_base, freq_scale,\n        ext_factor, attn_factor, beta_fast, beta_slow, 0.0f, false, false\n    );\n}\n\nstruct ggml_tensor * ggml_rope_custom_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   n_dims,\n        int                   mode,\n        int                   n_ctx,\n        int                   n_orig_ctx,\n        float                 freq_base,\n        float                 freq_scale,\n        float                 ext_factor,\n        float                 attn_factor,\n        float                 beta_fast,\n        float                 beta_slow) {\n    return ggml_rope_impl(\n        ctx, a, b, n_dims, mode, n_ctx, n_orig_ctx, freq_base, freq_scale,\n        ext_factor, attn_factor, beta_fast, beta_slow, 0.0f, false, true\n    );\n}\n\nstruct ggml_tensor * ggml_rope_xpos_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   n_dims,\n        float                 base,\n        bool                  down) {\n    return ggml_rope_impl(ctx, a, b, n_dims, 0, 0, 0, 10000.0f, 1.0f, 0.0f, 1.0f, 0.0f, 0.0f, base, down, true);\n}\n\n// ggml_rope_back\n\nstruct ggml_tensor * ggml_rope_back(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   n_dims,\n        int                   mode,\n        int                   n_ctx,\n        int                   n_orig_ctx,\n        float                 freq_base,\n        float                 freq_scale,\n        float                 ext_factor,\n        float                 attn_factor,\n        float                 beta_fast,\n        float                 beta_slow,\n        float                 xpos_base,\n        bool                  xpos_down) {\n    GGML_ASSERT(ggml_is_vector(b));\n    GGML_ASSERT(b->type == GGML_TYPE_I32);\n    GGML_ASSERT(a->ne[2] == b->ne[0]);\n\n    GGML_ASSERT((mode & 4) == 0 && \"ggml_rope_back() for ChatGLM not implemented yet\");\n\n    bool is_node = false;\n\n    if (a->grad) {\n        is_node = false; // TODO: implement backward\n    }\n\n    struct ggml_tensor * result = ggml_dup_tensor(ctx, a);\n\n    int32_t params[13] = { /*n_past*/ 0, n_dims, mode, n_ctx, n_orig_ctx };\n    memcpy(params +  5, &freq_base,    sizeof(float));\n    memcpy(params +  6, &freq_scale,   sizeof(float));\n    memcpy(params +  7, &ext_factor,   sizeof(float));\n    memcpy(params +  8, &attn_factor,  sizeof(float));\n    memcpy(params +  9, &beta_fast,    sizeof(float));\n    memcpy(params + 10, &beta_slow,    sizeof(float));\n    memcpy(params + 11, &xpos_base,    sizeof(float));\n    memcpy(params + 12, &xpos_down,    sizeof(bool));\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_ROPE_BACK;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_alibi\n\nstruct ggml_tensor * ggml_alibi(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   n_past,\n        int                   n_head,\n        float                 bias_max) {\n    GGML_ASSERT(n_past >= 0);\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    // TODO: when implement backward, fix this:\n    //struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n    struct ggml_tensor * result = ggml_view_tensor(ctx, a);\n\n    int32_t op_params[3] = { n_past, n_head };\n    memcpy(op_params + 2, &bias_max, sizeof(float));\n    ggml_set_op_params(result, op_params, sizeof(op_params));\n\n    result->op   = GGML_OP_ALIBI;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_clamp\n\nstruct ggml_tensor * ggml_clamp(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        float                 min,\n        float                 max) {\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    // TODO: when implement backward, fix this:\n    struct ggml_tensor * result = ggml_view_tensor(ctx, a);\n\n    float params[] = { min, max };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_CLAMP;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_conv_1d\n\nstatic int64_t ggml_calc_conv_output_size(int64_t ins, int64_t ks, int s, int p, int d) {\n    return (ins + 2 * p - d * (ks - 1) - 1) / s + 1;\n}\n\nGGML_API struct ggml_tensor * ggml_conv_1d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   s0,\n        int                   p0,\n        int                   d0) {\n    struct ggml_tensor * im2col = ggml_im2col(ctx, a, b, s0, 0, p0, 0, d0, 0, false); // [N, OL, IC * K]\n\n    struct ggml_tensor * result =\n        ggml_mul_mat(ctx,\n                ggml_reshape_2d(ctx, im2col, im2col->ne[0], (im2col->ne[2] * im2col->ne[1])), // [N, OL, IC * K] => [N*OL, IC * K]\n                ggml_reshape_2d(ctx, a, (a->ne[0] * a->ne[1]), a->ne[2]));                    // [OCï¼ŒIC, K] => [OC, IC * K]\n\n    result = ggml_reshape_3d(ctx, result, im2col->ne[1], a->ne[2], im2col->ne[2]); // [N, OC, OL]\n\n    return result;\n}\n\n// ggml_conv_1d_ph\n\nstruct ggml_tensor* ggml_conv_1d_ph(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   s,\n        int                   d) {\n    return ggml_conv_1d(ctx, a, b, s, a->ne[0] / 2, d);\n}\n\n// ggml_conv_transpose_1d\n\nstatic int64_t ggml_calc_conv_transpose_1d_output_size(int64_t ins, int64_t ks, int s, int p, int d) {\n    return (ins - 1) * s - 2 * p + d * (ks - 1) + 1;\n}\n\nGGML_API struct ggml_tensor * ggml_conv_transpose_1d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   s0,\n        int                   p0,\n        int                   d0) {\n    GGML_ASSERT(ggml_is_matrix(b));\n    GGML_ASSERT(a->ne[2] == b->ne[1]);\n    GGML_ASSERT(a->ne[3] == 1);\n\n    GGML_ASSERT(p0 == 0);\n    GGML_ASSERT(d0 == 1);\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    const int64_t ne[4] = {\n        ggml_calc_conv_transpose_1d_output_size(b->ne[0], a->ne[0], s0, 0 /*p0*/, 1 /*d0*/),\n        a->ne[1], b->ne[2], 1,\n    };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, ne);\n\n    int32_t params[] = { s0, p0, d0 };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op = GGML_OP_CONV_TRANSPOSE_1D;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_conv_2d\n\n// im2col: [N, IC, IH, IW] => [N, OH, OW, IC*KH*KW]\n// a: [OCï¼ŒIC, KH, KW]\n// b: [N, IC, IH, IW]\n// result: [N, OH, OW, IC*KH*KW]\nstruct ggml_tensor * ggml_im2col(\n    struct ggml_context * ctx,\n    struct ggml_tensor  * a,\n    struct ggml_tensor  * b,\n    int                  s0,\n    int                  s1,\n    int                  p0,\n    int                  p1,\n    int                  d0,\n    int                  d1,\n    bool                 is_2D) {\n\n    if(is_2D) {\n        GGML_ASSERT(a->ne[2] == b->ne[2]);\n    } else {\n        GGML_ASSERT(a->ne[1] == b->ne[1]);\n    }\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    const int64_t OH = is_2D ? ggml_calc_conv_output_size(b->ne[1], a->ne[1], s1, p1, d1) : 0;\n    const int64_t OW =         ggml_calc_conv_output_size(b->ne[0], a->ne[0], s0, p0, d0);\n\n    const int64_t ne[4] = {\n        is_2D ? (a->ne[2] * a->ne[1] * a->ne[0]) : a->ne[1] * a->ne[0],\n        OW,\n        is_2D ? OH : b->ne[2],\n        is_2D ?      b->ne[3] : 1,\n    };\n\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F16, 4, ne);\n    int32_t params[] = { s0, s1, p0, p1, d0, d1, (is_2D ? 1 : 0) };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op = GGML_OP_IM2COL;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// a: [OCï¼ŒIC, KH, KW]\n// b: [N, IC, IH, IW]\n// result: [N, OC, OH, OW]\nstruct ggml_tensor * ggml_conv_2d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                  s0,\n        int                  s1,\n        int                  p0,\n        int                  p1,\n        int                  d0,\n        int                  d1) {\n    struct ggml_tensor * im2col = ggml_im2col(ctx, a, b, s0, s1, p0, p1, d0, d1, true); // [N, OH, OW, IC * KH * KW]\n\n    struct ggml_tensor * result =\n        ggml_mul_mat(ctx,\n                ggml_reshape_2d(ctx, im2col, im2col->ne[0],  im2col->ne[3] * im2col->ne[2] * im2col->ne[1]), // [N, OH, OW, IC * KH * KW] => [N*OH*OW, IC * KH * KW]\n                ggml_reshape_2d(ctx, a, (a->ne[0] * a->ne[1] * a->ne[2]),  a->ne[3]));                       // [OCï¼ŒIC, KH, KW] => [OC, IC * KH * KW]\n\n    result = ggml_reshape_4d(ctx, result, im2col->ne[1], im2col->ne[2], a->ne[3], im2col->ne[3]); // [N, OC, OH, OW]\n\n    return result;\n}\n\n// ggml_conv_2d_sk_p0\nstruct ggml_tensor * ggml_conv_2d_sk_p0(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_conv_2d(ctx, a, b, a->ne[0], a->ne[1], 0, 0, 1, 1);\n}\n\n// ggml_conv_2d_s1_ph\n\nstruct ggml_tensor * ggml_conv_2d_s1_ph(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b) {\n    return ggml_conv_2d(ctx, a, b, 1, 1, a->ne[0] / 2, a->ne[1] / 2, 1, 1);\n}\n\n// ggml_conv_transpose_2d_p0\n\nstatic int64_t ggml_calc_conv_transpose_output_size(int64_t ins, int64_t ks, int s, int p) {\n    return (ins - 1) * s - 2 * p + ks;\n}\n\nstruct ggml_tensor * ggml_conv_transpose_2d_p0(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b,\n        int                   stride) {\n    GGML_ASSERT(a->ne[3] == b->ne[2]);\n\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    const int64_t ne[4] = {\n        ggml_calc_conv_transpose_output_size(b->ne[0], a->ne[0], stride, 0 /*p0*/),\n        ggml_calc_conv_transpose_output_size(b->ne[1], a->ne[1], stride, 0 /*p1*/),\n        a->ne[2], b->ne[3],\n    };\n\n    struct ggml_tensor* result = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, ne);\n\n    ggml_set_op_params_i32(result, 0, stride);\n\n    result->op = GGML_OP_CONV_TRANSPOSE_2D;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_pool_*\n\nstatic int64_t ggml_calc_pool_output_size(int64_t ins, int ks, int s, float p) {\n    return (ins + 2 * p - ks) / s + 1;\n}\n\n// ggml_pool_1d\n\nstruct ggml_tensor * ggml_pool_1d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        enum ggml_op_pool     op,\n        int                   k0,\n        int                   s0,\n        int                   p0) {\n\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    const int64_t ne[3] = {\n        ggml_calc_pool_output_size(a->ne[0], k0, s0, p0),\n        a->ne[1],\n    };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, 2, ne);\n\n    int32_t params[] = { op, k0, s0, p0 };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op = GGML_OP_POOL_1D;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_pool_2d\n\nstruct ggml_tensor * ggml_pool_2d(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        enum ggml_op_pool     op,\n        int                   k0,\n        int                   k1,\n        int                   s0,\n        int                   s1,\n        float                 p0,\n        float                 p1) {\n\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    const int64_t ne[3] = {\n        ggml_calc_pool_output_size(a->ne[0], k0, s0, p0),\n        ggml_calc_pool_output_size(a->ne[1], k1, s1, p1),\n        a->ne[2],\n    };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, 3, ne);\n\n    int32_t params[] = { op, k0, k1, s0, s1, p0, p1 };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op = GGML_OP_POOL_2D;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_upscale\n\nstatic struct ggml_tensor * ggml_upscale_impl(\n    struct ggml_context * ctx,\n    struct ggml_tensor * a,\n    int scale_factor) {\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor_4d(ctx, a->type,\n            a->ne[0] * scale_factor,\n            a->ne[1] * scale_factor,\n            a->ne[2], a->ne[3]);\n\n    result->op = GGML_OP_UPSCALE;\n    result->op_params[0] = scale_factor;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = NULL;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_upscale(\n    struct ggml_context * ctx,\n    struct ggml_tensor * a,\n    int scale_factor) {\n    return ggml_upscale_impl(ctx, a, scale_factor);\n}\n\n// ggml_flash_attn\n\nstruct ggml_tensor * ggml_flash_attn(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * q,\n        struct ggml_tensor  * k,\n        struct ggml_tensor  * v,\n        bool                  masked) {\n    GGML_ASSERT(ggml_can_mul_mat(k, q));\n    // TODO: check if vT can be multiplied by (k*qT)\n\n    bool is_node = false;\n\n    if (q->grad || k->grad || v->grad) {\n        is_node = true;\n    }\n\n    //struct ggml_tensor * result = ggml_dup_tensor(ctx, q);\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, q->n_dims, q->ne);\n\n    int32_t t = masked ? 1 : 0;\n    ggml_set_op_params(result, &t, sizeof(t));\n\n    result->op   = GGML_OP_FLASH_ATTN;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = q;\n    result->src[1] = k;\n    result->src[2] = v;\n\n    return result;\n}\n\n// ggml_flash_ff\n\nstruct ggml_tensor * ggml_flash_ff(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * b0,\n        struct ggml_tensor  * b1,\n        struct ggml_tensor  * c0,\n        struct ggml_tensor  * c1) {\n    GGML_ASSERT(ggml_can_mul_mat(b0, a));\n    // TODO: more checks\n\n    bool is_node = false;\n\n    if (a->grad || b0->grad || b1->grad || c0->grad || c1->grad) {\n        is_node = true;\n    }\n\n    //struct ggml_tensor * result = ggml_dup_tensor(ctx, a);\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, a->n_dims, a->ne);\n\n    result->op   = GGML_OP_FLASH_FF;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b0;\n    result->src[2] = b1;\n    result->src[3] = c0;\n    result->src[4] = c1;\n\n    return result;\n}\n\n// ggml_flash_attn_back\n\nstruct ggml_tensor * ggml_flash_attn_back(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * q,\n        struct ggml_tensor  * k,\n        struct ggml_tensor  * v,\n        struct ggml_tensor  * d,\n        bool                  masked) {\n    GGML_ASSERT(ggml_can_mul_mat(k, q));\n    // TODO: check if vT can be multiplied by (k*qT)\n\n    // d shape [D,N,ne2,ne3]\n    // q shape [D,N,ne2,ne3]\n    // k shape [D,M,kvne2,ne3]\n    // v shape [M,D,kvne2,ne3]\n\n    const int64_t     D = q->ne[0];\n    const int64_t     N = q->ne[1];\n    const int64_t     M = k->ne[1];\n    const int64_t   ne2 = q->ne[2];\n    const int64_t   ne3 = q->ne[3];\n    const int64_t kvne2 = k->ne[2];\n\n    GGML_ASSERT(k->ne[0] == D);\n    GGML_ASSERT(v->ne[0] == M);\n    GGML_ASSERT(v->ne[1] == D);\n    GGML_ASSERT(d->ne[0] == D);\n    GGML_ASSERT(d->ne[1] == N);\n    GGML_ASSERT(k->ne[2] == kvne2);\n    GGML_ASSERT(k->ne[3] == ne3);\n    GGML_ASSERT(v->ne[2] == kvne2);\n    GGML_ASSERT(v->ne[3] == ne3);\n    GGML_ASSERT(d->ne[2] == ne2);\n    GGML_ASSERT(d->ne[3] == ne3);\n\n    GGML_ASSERT(ne2 % kvne2 == 0);\n\n    bool is_node = false;\n\n    if (q->grad || k->grad || v->grad) {\n        // when using this operation (in backwards pass) these grads are set.\n        // we don't want to create (big) grad of our result, so is_node is false.\n        is_node = false;\n    }\n\n    // store gradients of q, k and v as continuous tensors concatenated in result.\n    // note: v and gradv are actually transposed, i.e. v->ne[0] != D.\n    const int64_t elem_q = ggml_nelements(q);\n    const int64_t elem_k = ggml_nelements(k);\n    const int64_t elem_v = ggml_nelements(v);\n\n    enum ggml_type result_type = GGML_TYPE_F32;\n    GGML_ASSERT(ggml_blck_size(result_type) == 1);\n    const size_t tsize = ggml_type_size(result_type);\n\n    const size_t offs_q = 0;\n    const size_t offs_k = offs_q + GGML_PAD(elem_q * tsize, GGML_MEM_ALIGN);\n    const size_t offs_v = offs_k + GGML_PAD(elem_k * tsize, GGML_MEM_ALIGN);\n    const size_t end    = offs_v + GGML_PAD(elem_v * tsize, GGML_MEM_ALIGN);\n\n    const size_t nelements = (end + tsize - 1)/tsize;\n\n    struct ggml_tensor * result = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, nelements);\n\n    int32_t masked_i = masked ? 1 : 0;\n    ggml_set_op_params(result, &masked_i, sizeof(masked_i));\n\n    result->op   = GGML_OP_FLASH_ATTN_BACK;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = q;\n    result->src[1] = k;\n    result->src[2] = v;\n    result->src[3] = d;\n\n    return result;\n}\n\n// ggml_win_part\n\nstruct ggml_tensor * ggml_win_part(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   w) {\n    GGML_ASSERT(a->ne[3] == 1);\n    GGML_ASSERT(a->type  == GGML_TYPE_F32);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    // padding\n    const int px = (w - a->ne[1]%w)%w;\n    const int py = (w - a->ne[2]%w)%w;\n\n    const int npx = (px + a->ne[1])/w;\n    const int npy = (py + a->ne[2])/w;\n    const int np  = npx*npy;\n\n    const int64_t ne[4] = { a->ne[0], w, w, np, };\n\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, 4, ne);\n\n    int32_t params[] = { npx, npy, w };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_WIN_PART;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_win_unpart\n\nstruct ggml_tensor * ggml_win_unpart(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   w0,\n        int                   h0,\n        int                   w) {\n    GGML_ASSERT(a->type == GGML_TYPE_F32);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    const int64_t ne[4] = { a->ne[0], w0, h0, 1, };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F32, 3, ne);\n\n    int32_t params[] = { w };\n    ggml_set_op_params(result, params, sizeof(params));\n\n    result->op   = GGML_OP_WIN_UNPART;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\n// ggml_get_rel_pos\n\nstruct ggml_tensor * ggml_get_rel_pos(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        int                   qh,\n        int                   kh) {\n    GGML_ASSERT(qh == kh);\n    GGML_ASSERT(2*MAX(qh, kh) - 1 == a->ne[1]);\n\n    bool is_node = false;\n\n    if (a->grad) {\n        GGML_ASSERT(false); // TODO: implement backward\n        is_node = true;\n    }\n\n    const int64_t ne[4] = { a->ne[0], kh, qh, 1, };\n    struct ggml_tensor * result = ggml_new_tensor(ctx, GGML_TYPE_F16, 3, ne);\n\n    result->op   = GGML_OP_GET_REL_POS;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = NULL;\n\n    return result;\n}\n\n// ggml_add_rel_pos\n\nstatic struct ggml_tensor * ggml_add_rel_pos_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * pw,\n        struct ggml_tensor  * ph,\n        bool                  inplace) {\n    GGML_ASSERT(ggml_are_same_shape(pw, ph));\n    GGML_ASSERT(ggml_is_contiguous(a));\n    GGML_ASSERT(ggml_is_contiguous(pw));\n    GGML_ASSERT(ggml_is_contiguous(ph));\n    GGML_ASSERT(ph->type == GGML_TYPE_F32);\n    GGML_ASSERT(pw->type == GGML_TYPE_F32);\n    GGML_ASSERT(pw->ne[3] == a->ne[2]);\n    GGML_ASSERT(pw->ne[0]*pw->ne[0] == a->ne[0]);\n    GGML_ASSERT(pw->ne[1]*pw->ne[2] == a->ne[1]);\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || pw->grad || ph->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n    ggml_set_op_params_i32(result, 0, inplace ? 1 : 0);\n\n    result->op   = GGML_OP_ADD_REL_POS;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = pw;\n    result->src[2] = ph;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_add_rel_pos(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * pw,\n        struct ggml_tensor  * ph) {\n    return ggml_add_rel_pos_impl(ctx, a, pw, ph, false);\n}\n\nstruct ggml_tensor * ggml_add_rel_pos_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        struct ggml_tensor  * pw,\n        struct ggml_tensor  * ph) {\n    return ggml_add_rel_pos_impl(ctx, a, pw, ph, true);\n}\n\n// gmml_unary\n\nstatic struct ggml_tensor * ggml_unary_impl(\n        struct ggml_context * ctx,\n        struct ggml_tensor * a,\n        enum ggml_unary_op op,\n        bool inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params_i32(result, 0, (int32_t) op);\n\n    result->op   = GGML_OP_UNARY;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_unary(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        enum ggml_unary_op op) {\n    return ggml_unary_impl(ctx, a, op, false);\n}\n\nstruct ggml_tensor * ggml_unary_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        enum ggml_unary_op op) {\n    return ggml_unary_impl(ctx, a, op, true);\n}\n\n// ggml_map_unary\n\nstatic struct ggml_tensor * ggml_map_unary_impl_f32(\n        struct ggml_context        * ctx,\n        struct ggml_tensor         * a,\n        const  ggml_unary_op_f32_t fun,\n        bool   inplace) {\n    bool is_node = false;\n\n    if (!inplace && a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params(result, (const void *) &fun, sizeof(fun));\n\n    result->op = GGML_OP_MAP_UNARY;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_map_unary_f32(\n        struct ggml_context        * ctx,\n        struct ggml_tensor         * a,\n        const  ggml_unary_op_f32_t fun) {\n    return ggml_map_unary_impl_f32(ctx, a, fun, false);\n}\n\nstruct ggml_tensor * ggml_map_unary_inplace_f32(\n        struct ggml_context        * ctx,\n        struct ggml_tensor         * a,\n        const  ggml_unary_op_f32_t fun) {\n    return ggml_map_unary_impl_f32(ctx, a, fun, true);\n}\n\n// ggml_map_binary\n\nstatic struct ggml_tensor * ggml_map_binary_impl_f32(\n        struct ggml_context         * ctx,\n        struct ggml_tensor          * a,\n        struct ggml_tensor          * b,\n        const  ggml_binary_op_f32_t fun,\n        bool   inplace) {\n    GGML_ASSERT(ggml_are_same_shape(a, b));\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params(result, (const void *) &fun, sizeof(fun));\n\n    result->op = GGML_OP_MAP_BINARY;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_map_binary_f32(\n        struct ggml_context         * ctx,\n        struct ggml_tensor          * a,\n        struct ggml_tensor          * b,\n        const  ggml_binary_op_f32_t fun) {\n    return ggml_map_binary_impl_f32(ctx, a, b, fun, false);\n}\n\nstruct ggml_tensor * ggml_map_binary_inplace_f32(\n        struct ggml_context         * ctx,\n        struct ggml_tensor          * a,\n        struct ggml_tensor          * b,\n        const  ggml_binary_op_f32_t fun) {\n    return ggml_map_binary_impl_f32(ctx, a, b, fun, true);\n}\n\n// ggml_map_custom1_f32\n\nstatic struct ggml_tensor * ggml_map_custom1_impl_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        const  ggml_custom1_op_f32_t   fun,\n        bool   inplace) {\n    bool is_node = false;\n\n    if (!inplace && a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params(result, (const void *) &fun, sizeof(fun));\n\n    result->op = GGML_OP_MAP_CUSTOM1_F32;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_map_custom1_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        const  ggml_custom1_op_f32_t   fun) {\n    return ggml_map_custom1_impl_f32(ctx, a, fun, false);\n}\n\nstruct ggml_tensor * ggml_map_custom1_inplace_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        const  ggml_custom1_op_f32_t   fun) {\n    return ggml_map_custom1_impl_f32(ctx, a, fun, true);\n}\n\n// ggml_map_custom2_f32\n\nstatic struct ggml_tensor * ggml_map_custom2_impl_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        const  ggml_custom2_op_f32_t   fun,\n        bool   inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params(result, (const void *) &fun, sizeof(fun));\n\n    result->op = GGML_OP_MAP_CUSTOM2_F32;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_map_custom2_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        const  ggml_custom2_op_f32_t   fun) {\n    return ggml_map_custom2_impl_f32(ctx, a, b, fun, false);\n}\n\nstruct ggml_tensor * ggml_map_custom2_inplace_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        const  ggml_custom2_op_f32_t   fun) {\n    return ggml_map_custom2_impl_f32(ctx, a, b, fun, true);\n}\n\n// ggml_map_custom3_f32\n\nstatic struct ggml_tensor * ggml_map_custom3_impl_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        struct ggml_tensor           * c,\n        const  ggml_custom3_op_f32_t   fun,\n        bool   inplace) {\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad || c->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    ggml_set_op_params(result, (const void *) &fun, sizeof(fun));\n\n    result->op = GGML_OP_MAP_CUSTOM3_F32;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = c;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_map_custom3_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        struct ggml_tensor           * c,\n        const  ggml_custom3_op_f32_t   fun) {\n    return ggml_map_custom3_impl_f32(ctx, a, b, c, fun, false);\n}\n\nstruct ggml_tensor * ggml_map_custom3_inplace_f32(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        struct ggml_tensor           * c,\n        const  ggml_custom3_op_f32_t   fun) {\n    return ggml_map_custom3_impl_f32(ctx, a, b, c, fun, true);\n}\n\n// ggml_map_custom1\nstruct ggml_map_custom1_op_params {\n    ggml_custom1_op_t fun;\n    int n_tasks;\n    void * userdata;\n};\n\nstatic struct ggml_tensor * ggml_map_custom1_impl(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        const  ggml_custom1_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata,\n        bool                           inplace) {\n    GGML_ASSERT(n_tasks == GGML_N_TASKS_MAX || n_tasks > 0);\n\n    bool is_node = false;\n\n    if (!inplace && a->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    struct ggml_map_custom1_op_params params = {\n        /*.fun      =*/ fun,\n        /*.n_tasks  =*/ n_tasks,\n        /*.userdata =*/ userdata\n    };\n    ggml_set_op_params(result, (const void *) &params, sizeof(params));\n\n    result->op = GGML_OP_MAP_CUSTOM1;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_map_custom1(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        const  ggml_custom1_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata) {\n    return ggml_map_custom1_impl(ctx, a, fun, n_tasks, userdata, false);\n}\n\nstruct ggml_tensor * ggml_map_custom1_inplace(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        const  ggml_custom1_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata) {\n    return ggml_map_custom1_impl(ctx, a, fun, n_tasks, userdata, true);\n}\n\n// ggml_map_custom2\n\nstruct ggml_map_custom2_op_params {\n    ggml_custom2_op_t fun;\n    int n_tasks;\n    void * userdata;\n};\n\nstatic struct ggml_tensor * ggml_map_custom2_impl(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        const  ggml_custom2_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata,\n        bool                           inplace) {\n    GGML_ASSERT(n_tasks == GGML_N_TASKS_MAX || n_tasks > 0);\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    struct ggml_map_custom2_op_params params = {\n        /*.fun      =*/ fun,\n        /*.n_tasks  =*/ n_tasks,\n        /*.userdata =*/ userdata\n    };\n    ggml_set_op_params(result, (const void *) &params, sizeof(params));\n\n    result->op = GGML_OP_MAP_CUSTOM2;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_map_custom2(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        const  ggml_custom2_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata) {\n    return ggml_map_custom2_impl(ctx, a, b, fun, n_tasks, userdata, false);\n}\n\nstruct ggml_tensor * ggml_map_custom2_inplace(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        const  ggml_custom2_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata) {\n    return ggml_map_custom2_impl(ctx, a, b, fun, n_tasks, userdata, true);\n}\n\n// ggml_map_custom3\n\nstruct ggml_map_custom3_op_params {\n    ggml_custom3_op_t fun;\n    int n_tasks;\n    void * userdata;\n};\n\nstatic struct ggml_tensor * ggml_map_custom3_impl(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        struct ggml_tensor           * c,\n        const  ggml_custom3_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata,\n        bool                           inplace) {\n    GGML_ASSERT(n_tasks == GGML_N_TASKS_MAX || n_tasks > 0);\n\n    bool is_node = false;\n\n    if (!inplace && (a->grad || b->grad || c->grad)) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = inplace ? ggml_view_tensor(ctx, a) : ggml_dup_tensor(ctx, a);\n\n    struct ggml_map_custom3_op_params params = {\n        /*.fun      =*/ fun,\n        /*.n_tasks  =*/ n_tasks,\n        /*.userdata =*/ userdata\n    };\n    ggml_set_op_params(result, (const void *) &params, sizeof(params));\n\n    result->op = GGML_OP_MAP_CUSTOM3;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = c;\n\n    return result;\n}\n\nstruct ggml_tensor * ggml_map_custom3(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        struct ggml_tensor           * c,\n        const  ggml_custom3_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata) {\n    return ggml_map_custom3_impl(ctx, a, b, c, fun, n_tasks, userdata, false);\n}\n\nstruct ggml_tensor * ggml_map_custom3_inplace(\n        struct ggml_context          * ctx,\n        struct ggml_tensor           * a,\n        struct ggml_tensor           * b,\n        struct ggml_tensor           * c,\n        const  ggml_custom3_op_t       fun,\n        int                            n_tasks,\n        void                         * userdata) {\n    return ggml_map_custom3_impl(ctx, a, b, c, fun, n_tasks, userdata, true);\n}\n\n// ggml_cross_entropy_loss\n\nstruct ggml_tensor * ggml_cross_entropy_loss(\n        struct ggml_context         * ctx,\n        struct ggml_tensor          * a,\n        struct ggml_tensor          * b) {\n    GGML_ASSERT(ggml_are_same_shape(a, b));\n    bool is_node = false;\n\n    if (a->grad || b->grad) {\n        is_node = true;\n    }\n\n    struct ggml_tensor * result = ggml_new_tensor_1d(ctx, a->type, 1);\n\n    result->op   = GGML_OP_CROSS_ENTROPY_LOSS;\n    result->grad = is_node ? ggml_dup_tensor(ctx, result) : NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n\n    return result;\n}\n\n// ggml_cross_entropy_loss_back\n\nstruct ggml_tensor * ggml_cross_entropy_loss_back(\n        struct ggml_context         * ctx,\n        struct ggml_tensor          * a,\n        struct ggml_tensor          * b,\n        struct ggml_tensor          * c) {\n    GGML_ASSERT(ggml_are_same_shape(a, b));\n    GGML_ASSERT(ggml_is_scalar(c));\n\n    struct ggml_tensor * result = ggml_dup_tensor(ctx, a);\n\n    result->op   = GGML_OP_CROSS_ENTROPY_LOSS_BACK;\n    result->grad = NULL;\n    result->src[0] = a;\n    result->src[1] = b;\n    result->src[2] = c;\n\n    return result;\n}\n\n////////////////////////////////////////////////////////////////////////////////\n\nvoid ggml_set_param(\n        struct ggml_context * ctx,\n        struct ggml_tensor * tensor) {\n    tensor->is_param = true;\n\n    GGML_ASSERT(tensor->grad == NULL);\n    tensor->grad = ggml_dup_tensor(ctx, tensor);\n    ggml_format_name(tensor->grad, \"%s (grad)\", tensor->name);\n}\n\n// ggml_compute_forward_dup\n\nstatic void ggml_compute_forward_dup_same_cont(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_nelements(dst) == ggml_nelements(src0));\n    GGML_ASSERT(ggml_is_contiguous(dst) && ggml_is_contiguous(src0));\n    GGML_ASSERT(src0->type == dst->type);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const size_t nb00 = src0->nb[0];\n    const size_t nb0 = dst->nb[0];\n\n    const int ith = params->ith; // thread index\n    const int nth = params->nth; // number of threads\n\n    // parallelize by elements\n    const int ne = ggml_nelements(dst);\n    const int dr = (ne + nth - 1) / nth;\n    const int ie0 = dr * ith;\n    const int ie1 = MIN(ie0 + dr, ne);\n\n    if (ie0 < ie1) {\n        memcpy(\n            ((char *)  dst->data + ie0*nb0),\n            ((char *) src0->data + ie0*nb00),\n            (ie1 - ie0) * ggml_type_size(src0->type));\n    }\n\n}\nstatic void ggml_compute_forward_dup_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_nelements(dst) == ggml_nelements(src0));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    const int ith = params->ith; // thread index\n    const int nth = params->nth; // number of threads\n\n    if (ggml_is_contiguous(src0) && ggml_is_contiguous(dst) && src0->type == dst->type) {\n        ggml_compute_forward_dup_same_cont(params, src0, dst);\n        return;\n    }\n\n    // parallelize by rows\n    const int nr = ne01;\n    // number of rows per thread\n    const int dr = (nr + nth - 1) / nth;\n    // row range for this thread\n    const int ir0 = dr * ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    if (src0->type == dst->type &&\n        ne00 == ne0 &&\n        nb00 == ggml_type_size(src0->type) && nb0 == ggml_type_size(dst->type)) {\n        // copy by rows\n        const size_t rs = ne00*nb00;\n        for (int64_t i03 = 0; i03 < ne03; i03++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                for (int64_t i01 = ir0; i01 < ir1; i01++) {\n                    memcpy(\n                        ((char *)  dst->data + i01*nb1  + i02*nb2  + i03*nb3),\n                        ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03),\n                        rs);\n                }\n            }\n        }\n        return;\n    }\n\n    // TODO: add more special-case implementations for tensor shapes/strides that can benefit from memcpy\n\n    if (ggml_is_contiguous(dst)) {\n        if (nb00 == sizeof(ggml_fp16_t)) {\n            if (dst->type == GGML_TYPE_F16) {\n                size_t id = 0;\n                const size_t rs = ne00 * nb00;\n                char * dst_ptr = (char *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += rs * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            const char * src0_ptr = (char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03;\n                            memcpy(dst_ptr + id, src0_ptr, rs);\n                            id += rs;\n                        }\n                        id += rs * (ne01 - ir1);\n                    }\n                }\n            } else if (dst->type == GGML_TYPE_F32) {\n                size_t id = 0;\n                float * dst_ptr = (float *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += ne00 * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            const ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03);\n                            for (int i00 = 0; i00 < ne00; i00++) {\n                                dst_ptr[id] = GGML_FP16_TO_FP32(src0_ptr[i00]);\n                                id++;\n                            }\n                        }\n                        id += ne00 * (ne01 - ir1);\n                    }\n                }\n            } else if (type_traits[dst->type].from_float) {\n                ggml_from_float_t const quantize_row_q = type_traits[dst->type].from_float;\n                float * src0_f32 = (float *) params->wdata + (ne00 + CACHE_LINE_SIZE_F32) * ith;\n\n                size_t id = 0;\n                size_t rs = nb0 * (ne00 / ggml_blck_size(dst->type));\n                char * dst_ptr = (char *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += rs * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            const ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03);\n\n                            for (int i00 = 0; i00 < ne00; i00++) {\n                                src0_f32[i00] = GGML_FP16_TO_FP32(src0_ptr[i00]);\n                            }\n\n                            quantize_row_q(src0_f32, dst_ptr + id, ne00);\n                            id += rs;\n                        }\n                        id += rs * (ne01 - ir1);\n                    }\n                }\n            } else {\n                GGML_ASSERT(false); // TODO: implement\n            }\n        } else {\n            //printf(\"%s: this is not optimal - fix me\\n\", __func__);\n\n            if (dst->type == GGML_TYPE_F32) {\n                size_t id = 0;\n                float * dst_ptr = (float *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += ne00 * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            for (int i00 = 0; i00 < ne00; i00++) {\n                                const ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i00*nb00 + i01*nb01 + i02*nb02 + i03*nb03);\n\n                                dst_ptr[id] = GGML_FP16_TO_FP32(*src0_ptr);\n                                id++;\n                            }\n                        }\n                        id += ne00 * (ne01 - ir1);\n                    }\n                }\n            } else if (dst->type == GGML_TYPE_F16) {\n                size_t id = 0;\n                ggml_fp16_t * dst_ptr = (ggml_fp16_t *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += ne00 * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            for (int i00 = 0; i00 < ne00; i00++) {\n                                const ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i00*nb00 + i01*nb01 + i02*nb02 + i03*nb03);\n\n                                dst_ptr[id] = *src0_ptr;\n                                id++;\n                            }\n                        }\n                        id += ne00 * (ne01 - ir1);\n                    }\n                }\n            } else {\n                GGML_ASSERT(false); // TODO: implement\n            }\n        }\n        return;\n    }\n\n    // dst counters\n    int64_t i10 = 0;\n    int64_t i11 = 0;\n    int64_t i12 = 0;\n    int64_t i13 = 0;\n\n    if (dst->type == GGML_TYPE_F16) {\n        for (int64_t i03 = 0; i03 < ne03; i03++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                i10 += ne00 * ir0;\n                while (i10 >= ne0) {\n                    i10 -= ne0;\n                    if (++i11 == ne1) {\n                        i11 = 0;\n                        if (++i12 == ne2) {\n                            i12 = 0;\n                            if (++i13 == ne3) {\n                                i13 = 0;\n                            }\n                        }\n                    }\n                }\n                for (int64_t i01 = ir0; i01 < ir1; i01++) {\n                    for (int64_t i00 = 0; i00 < ne00; i00++) {\n                        const char * src0_ptr = ((char *) src0->data + i00*nb00 + i01*nb01 + i02*nb02 + i03*nb03);\n                              char * dst_ptr  = ((char *)  dst->data + i10*nb0  + i11*nb1  + i12*nb2  + i13*nb3);\n\n                        memcpy(dst_ptr, src0_ptr, sizeof(ggml_fp16_t));\n\n                        if (++i10 == ne00) {\n                            i10 = 0;\n                            if (++i11 == ne01) {\n                                i11 = 0;\n                                if (++i12 == ne02) {\n                                    i12 = 0;\n                                    if (++i13 == ne03) {\n                                        i13 = 0;\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n                i10 += ne00 * (ne01 - ir1);\n                while (i10 >= ne0) {\n                    i10 -= ne0;\n                    if (++i11 == ne1) {\n                        i11 = 0;\n                        if (++i12 == ne2) {\n                            i12 = 0;\n                            if (++i13 == ne3) {\n                                i13 = 0;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    } else if (dst->type == GGML_TYPE_F32) {\n        for (int64_t i03 = 0; i03 < ne03; i03++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                i10 += ne00 * ir0;\n                while (i10 >= ne0) {\n                    i10 -= ne0;\n                    if (++i11 == ne1) {\n                        i11 = 0;\n                        if (++i12 == ne2) {\n                            i12 = 0;\n                            if (++i13 == ne3) {\n                                i13 = 0;\n                            }\n                        }\n                    }\n                }\n                for (int64_t i01 = ir0; i01 < ir1; i01++) {\n                    for (int64_t i00 = 0; i00 < ne00; i00++) {\n                        const char * src0_ptr = ((char *) src0->data + i00*nb00 + i01*nb01 + i02*nb02 + i03*nb03);\n                              char * dst_ptr  = ((char *)  dst->data + i10*nb0  + i11*nb1  + i12*nb2  + i13*nb3);\n\n                        *(float *) dst_ptr = GGML_FP16_TO_FP32(*(const ggml_fp16_t *) src0_ptr);\n\n                        if (++i10 == ne0) {\n                            i10 = 0;\n                            if (++i11 == ne1) {\n                                i11 = 0;\n                                if (++i12 == ne2) {\n                                    i12 = 0;\n                                    if (++i13 == ne3) {\n                                        i13 = 0;\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n                i10 += ne00 * (ne01 - ir1);\n                while (i10 >= ne0) {\n                    i10 -= ne0;\n                    if (++i11 == ne1) {\n                        i11 = 0;\n                        if (++i12 == ne2) {\n                            i12 = 0;\n                            if (++i13 == ne3) {\n                                i13 = 0;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    } else {\n        GGML_ASSERT(false); // TODO: implement\n    }\n}\n\nstatic void ggml_compute_forward_dup_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_nelements(dst) == ggml_nelements(src0));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    const int ith = params->ith; // thread index\n    const int nth = params->nth; // number of threads\n\n    if (ggml_is_contiguous(src0) && ggml_is_contiguous(dst) && src0->type == dst->type) {\n        ggml_compute_forward_dup_same_cont(params, src0, dst);\n        return;\n    }\n\n    // parallelize by rows\n    const int nr = ne01;\n    // number of rows per thread\n    const int dr = (nr + nth - 1) / nth;\n    // row range for this thread\n    const int ir0 = dr * ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    if (src0->type == dst->type &&\n        ne00 == ne0 &&\n        nb00 == ggml_type_size(src0->type) && nb0 == ggml_type_size(dst->type)) {\n        // copy by rows\n        const size_t rs = ne00*nb00;\n        for (int64_t i03 = 0; i03 < ne03; i03++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                for (int64_t i01 = ir0; i01 < ir1; i01++) {\n                    memcpy(\n                        ((char *)  dst->data + i01*nb1  + i02*nb2  + i03*nb3),\n                        ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03),\n                        rs);\n                }\n            }\n        }\n        return;\n    }\n\n    if (ggml_is_contiguous(dst)) {\n        // TODO: simplify\n        if (nb00 == sizeof(float)) {\n            if (dst->type == GGML_TYPE_F32) {\n                size_t id = 0;\n                const size_t rs = ne00 * nb00;\n                char * dst_ptr = (char *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += rs * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            const char * src0_ptr = (char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03;\n                            memcpy(dst_ptr + id, src0_ptr, rs);\n                            id += rs;\n                        }\n                        id += rs * (ne01 - ir1);\n                    }\n                }\n            } else if (type_traits[dst->type].from_float) {\n                ggml_from_float_t const quantize_row_q = type_traits[dst->type].from_float;\n\n                size_t id = 0;\n                size_t rs = nb0 * (ne00 / ggml_blck_size(dst->type));\n                char * dst_ptr = (char *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += rs * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            const float * src0_ptr = (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03);\n                            quantize_row_q(src0_ptr, dst_ptr + id, ne00);\n                            id += rs;\n                        }\n                        id += rs * (ne01 - ir1);\n                    }\n                }\n            } else {\n                GGML_ASSERT(false); // TODO: implement\n            }\n        } else {\n            //printf(\"%s: this is not optimal - fix me\\n\", __func__);\n\n            if (dst->type == GGML_TYPE_F32) {\n                size_t id = 0;\n                float * dst_ptr = (float *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += ne00 * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            for (int i00 = 0; i00 < ne00; i00++) {\n                                const float * src0_ptr = (float *) ((char *) src0->data + i00*nb00 + i01*nb01 + i02*nb02 + i03*nb03);\n\n                                dst_ptr[id] = *src0_ptr;\n                                id++;\n                            }\n                        }\n                        id += ne00 * (ne01 - ir1);\n                    }\n                }\n            } else if (dst->type == GGML_TYPE_F16) {\n                size_t id = 0;\n                ggml_fp16_t * dst_ptr = (ggml_fp16_t *) dst->data;\n\n                for (int i03 = 0; i03 < ne03; i03++) {\n                    for (int i02 = 0; i02 < ne02; i02++) {\n                        id += ne00 * ir0;\n                        for (int i01 = ir0; i01 < ir1; i01++) {\n                            for (int i00 = 0; i00 < ne00; i00++) {\n                                const float * src0_ptr = (float *) ((char *) src0->data + i00*nb00 + i01*nb01 + i02*nb02 + i03*nb03);\n\n                                dst_ptr[id] = GGML_FP32_TO_FP16(*src0_ptr);\n                                id++;\n                            }\n                        }\n                        id += ne00 * (ne01 - ir1);\n                    }\n                }\n            } else {\n                GGML_ASSERT(false); // TODO: implement\n            }\n        }\n\n        return;\n    }\n\n    // dst counters\n\n    int64_t i10 = 0;\n    int64_t i11 = 0;\n    int64_t i12 = 0;\n    int64_t i13 = 0;\n\n    if (dst->type == GGML_TYPE_F32) {\n        for (int64_t i03 = 0; i03 < ne03; i03++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                i10 += ne00 * ir0;\n                while (i10 >= ne0) {\n                    i10 -= ne0;\n                    if (++i11 == ne1) {\n                        i11 = 0;\n                        if (++i12 == ne2) {\n                            i12 = 0;\n                            if (++i13 == ne3) {\n                                i13 = 0;\n                            }\n                        }\n                    }\n                }\n                for (int64_t i01 = ir0; i01 < ir1; i01++) {\n                    for (int64_t i00 = 0; i00 < ne00; i00++) {\n                        const char * src0_ptr = ((char *) src0->data + i00*nb00 + i01*nb01 + i02*nb02 + i03*nb03);\n                              char * dst_ptr  = ((char *)  dst->data + i10*nb0  + i11*nb1  + i12*nb2  + i13*nb3);\n\n                        memcpy(dst_ptr, src0_ptr, sizeof(float));\n\n                        if (++i10 == ne0) {\n                            i10 = 0;\n                            if (++i11 == ne1) {\n                                i11 = 0;\n                                if (++i12 == ne2) {\n                                    i12 = 0;\n                                    if (++i13 == ne3) {\n                                        i13 = 0;\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n                i10 += ne00 * (ne01 - ir1);\n                while (i10 >= ne0) {\n                    i10 -= ne0;\n                    if (++i11 == ne1) {\n                        i11 = 0;\n                        if (++i12 == ne2) {\n                            i12 = 0;\n                            if (++i13 == ne3) {\n                                i13 = 0;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    } else if (dst->type == GGML_TYPE_F16) {\n        for (int64_t i03 = 0; i03 < ne03; i03++) {\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                i10 += ne00 * ir0;\n                while (i10 >= ne0) {\n                    i10 -= ne0;\n                    if (++i11 == ne1) {\n                        i11 = 0;\n                        if (++i12 == ne2) {\n                            i12 = 0;\n                            if (++i13 == ne3) {\n                                i13 = 0;\n                            }\n                        }\n                    }\n                }\n                for (int64_t i01 = ir0; i01 < ir1; i01++) {\n                    for (int64_t i00 = 0; i00 < ne00; i00++) {\n                        const char * src0_ptr = ((char *) src0->data + i00*nb00 + i01*nb01 + i02*nb02 + i03*nb03);\n                              char * dst_ptr  = ((char *)  dst->data + i10*nb0  + i11*nb1  + i12*nb2  + i13*nb3);\n\n                        *(ggml_fp16_t *) dst_ptr = GGML_FP32_TO_FP16(*(const float *) src0_ptr);\n\n                        if (++i10 == ne0) {\n                            i10 = 0;\n                            if (++i11 == ne1) {\n                                i11 = 0;\n                                if (++i12 == ne2) {\n                                    i12 = 0;\n                                    if (++i13 == ne3) {\n                                        i13 = 0;\n                                    }\n                                }\n                            }\n                        }\n                    }\n                }\n                i10 += ne00 * (ne01 - ir1);\n                while (i10 >= ne0) {\n                    i10 -= ne0;\n                    if (++i11 == ne1) {\n                        i11 = 0;\n                        if (++i12 == ne2) {\n                            i12 = 0;\n                            if (++i13 == ne3) {\n                                i13 = 0;\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    } else {\n        GGML_ASSERT(false); // TODO: implement\n    }\n}\n\nstatic void ggml_compute_forward_dup(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    if (ggml_is_contiguous(src0) && ggml_is_contiguous(dst) && src0->type == dst->type) {\n        ggml_compute_forward_dup_same_cont(params, src0, dst);\n        return;\n    }\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_dup_f16(params, src0, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_dup_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_add\n\nstatic void ggml_compute_forward_add_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_can_repeat_rows(src1, src0) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    GGML_ASSERT( nb0 == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n    struct ggml_tensor *src2 = dst->src[2];\n    float *ft;\n    if (src2 != NULL) \n        ft = src2->data;\n\n    if (nb10 == sizeof(float)) {\n        for (int ir = ir0; ir < ir1; ++ir) {\n            // src1 is broadcastable across src0 and dst in i1, i2, i3\n            const int64_t i03 = ir/(ne02*ne01);\n            const int64_t i02 = (ir - i03*ne02*ne01)/ne01;\n            const int64_t i01 = (ir - i03*ne02*ne01 - i02*ne01);\n\n            const int64_t i13 = i03 % ne13;\n            const int64_t i12 = i02 % ne12;\n            const int64_t i11 = i01 % ne11;\n\n            float * dst_ptr  = (float *) ((char *) dst->data  + i03*nb3  + i02*nb2  + i01*nb1 );\n            float * src0_ptr = (float *) ((char *) src0->data + i03*nb03 + i02*nb02 + i01*nb01);\n            float * src1_ptr = (float *) ((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11);\n\n#ifdef GGML_USE_ACCELERATE\n            vDSP_vadd(src0_ptr, 1, src1_ptr, 1, dst_ptr, 1, ne00);\n#else\n            // ggml_vec_add_f32(ne00, dst_ptr, src0_ptr, src1_ptr);\n            if (src2 == NULL)\n                ggml_vec_add_f32(ne00, dst_ptr, src0_ptr, src1_ptr);\n            else\n            {\n                // printf(\"head %d\\n\", src2->ne[0]);\n                // int k;\n                // scanf(\"%d\", &k);\n                // ggml_vec_add_f32(ne00, dst_ptr, src0_ptr, src1_ptr);\n                int num = src2->ne[0];\n                if (num > 1000) {\n                    for (int i = 0; i < ne00; i++)\n                    {\n                        dst_ptr[i] = ft[i] >= 0.0f ? src0_ptr[i] + src1_ptr[i] : 0;\n                    }\n                }\n                else {\n                    // ggml_set_zero(dst);\n                    for (int i = 0; i < num; i++)\n                    {\n                        int id = i << 7;\n                        /* dst_ptr[i] = ft[id] > 0.4? src0_ptr[i] + src1_ptr[i] : 0; */\n                        if (ft[i] < -7.0f){\n                            for (int j = 0; j < 128; j++)\n                                dst_ptr[id + j] = 0;\n                            // dst_ptr[i]  = 0;\n                            continue;\n                        }\n                        else\n                        {\n                            for (int j = 0; j < 128; j++)\n                                dst_ptr[id+j] = src0_ptr[id+j] + src1_ptr[id+j];\n                        }\n                            // dst_ptr[i] = src0_ptr[i] + src1_ptr[i];\n                    }\n                }\n            }\n#endif\n        }\n    } else {\n        // src1 is not contiguous\n        for (int ir = ir0; ir < ir1; ++ir) {\n            // src1 is broadcastable across src0 and dst in i1, i2, i3\n            const int64_t i03 = ir/(ne02*ne01);\n            const int64_t i02 = (ir - i03*ne02*ne01)/ne01;\n            const int64_t i01 = (ir - i03*ne02*ne01 - i02*ne01);\n\n            const int64_t i13 = i03 % ne13;\n            const int64_t i12 = i02 % ne12;\n            const int64_t i11 = i01 % ne11;\n\n            float * dst_ptr  = (float *) ((char *) dst->data  + i03*nb3  + i02*nb2  + i01*nb1 );\n            float * src0_ptr = (float *) ((char *) src0->data + i03*nb03 + i02*nb02 + i01*nb01);\n\n            for (int i0 = 0; i0 < ne0; i0++) {\n                float * src1_ptr = (float *) ((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11 + i0*nb10);\n\n                dst_ptr[i0] = src0_ptr[i0] + *src1_ptr;\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_add_f16_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n\n    if (dst->type == GGML_TYPE_F32) {\n        GGML_ASSERT( nb0 == sizeof(float));\n    }\n    else {\n        GGML_ASSERT(dst->type  == GGML_TYPE_F16);\n        GGML_ASSERT( nb0 == sizeof(ggml_fp16_t));\n    }\n\n    GGML_ASSERT(nb00 == sizeof(ggml_fp16_t));\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    if (nb10 == sizeof(float)) {\n        if (dst->type == GGML_TYPE_F16) {\n            for (int ir = ir0; ir < ir1; ++ir) {\n                // src0, src1 and dst are same shape => same indices\n                const int i3 = ir/(ne2*ne1);\n                const int i2 = (ir - i3*ne2*ne1)/ne1;\n                const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n                ggml_fp16_t * dst_ptr  = (ggml_fp16_t *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1);\n                ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01);\n                float *       src1_ptr = (float *)       ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11);\n\n                for (int i = 0; i < ne0; i++) {\n                    dst_ptr[i] = GGML_FP32_TO_FP16(GGML_FP16_TO_FP32(src0_ptr[i]) + src1_ptr[i]);\n                }\n            }\n        } else {\n            for (int ir = ir0; ir < ir1; ++ir) {\n                // src0, src1 and dst are same shape => same indices\n                const int i3 = ir/(ne2*ne1);\n                const int i2 = (ir - i3*ne2*ne1)/ne1;\n                const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n                float *       dst_ptr  = (float *)       ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1);\n                ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01);\n                float *       src1_ptr = (float *)       ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11);\n\n                for (int i = 0; i < ne0; i++) {\n                    dst_ptr[i] = GGML_FP16_TO_FP32(src0_ptr[i]) + src1_ptr[i];\n                }\n            }\n        }\n    }\n    else {\n        // src1 is not contiguous\n        GGML_ASSERT(false);\n    }\n}\n\nstatic void ggml_compute_forward_add_f16_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F16);\n    GGML_ASSERT(dst->type  == GGML_TYPE_F16);\n\n    GGML_ASSERT( nb0 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nb00 == sizeof(ggml_fp16_t));\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    if (nb10 == sizeof(ggml_fp16_t)) {\n        for (int ir = ir0; ir < ir1; ++ir) {\n            // src0, src1 and dst are same shape => same indices\n            const int i3 = ir/(ne2*ne1);\n            const int i2 = (ir - i3*ne2*ne1)/ne1;\n            const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n            ggml_fp16_t * dst_ptr  = (ggml_fp16_t *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1);\n            ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01);\n            ggml_fp16_t * src1_ptr = (ggml_fp16_t *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11);\n\n            for (int i = 0; i < ne0; i++) {\n                dst_ptr[i] = GGML_FP32_TO_FP16(GGML_FP16_TO_FP32(src0_ptr[i]) + GGML_FP16_TO_FP32(src1_ptr[i]));\n            }\n        }\n    }\n    else {\n        // src1 is not contiguous\n        GGML_ASSERT(false);\n    }\n}\n\nstatic void ggml_compute_forward_add_q_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const enum ggml_type type = src0->type;\n    const enum ggml_type dtype = dst->type;\n    ggml_to_float_t const dequantize_row_q = type_traits[type].to_float;\n    ggml_from_float_t const quantize_row_q = type_traits[dtype].from_float;\n\n    // we don't support permuted src0 or src1\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    GGML_ASSERT(ggml_is_quantized(src0->type));\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    float * wdata = (float *) params->wdata + (ne00 + CACHE_LINE_SIZE_F32) * ith;\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // src0 indices\n        const int i03 = ir/(ne02*ne01);\n        const int i02 = (ir - i03*ne02*ne01)/ne01;\n        const int i01 = (ir - i03*ne02*ne01 - i02*ne01);\n\n        // src1 and dst are same shape as src0 => same indices\n        const int i13 = i03;\n        const int i12 = i02;\n        const int i11 = i01;\n\n        const int i3 = i03;\n        const int i2 = i02;\n        const int i1 = i01;\n\n        void  * src0_row = (void *) ((char *) src0->data + (i01*nb01 + i02*nb02 + i03*nb03));\n        float * src1_row = (float *)((char *) src1->data + (i11*nb11 + i12*nb12 + i13*nb13));\n        void  * dst_row  = (void *) ((char *)  dst->data + ( i1*nb1  +  i2*nb2  +  i3*nb3));\n\n        assert(ne00 % 32 == 0);\n\n        // unquantize row from src0 to temp buffer\n        dequantize_row_q(src0_row, wdata, ne00);\n        // add src1\n        ggml_vec_acc_f32(ne00, wdata, src1_row);\n        // quantize row to dst\n        if (quantize_row_q != NULL) {\n            quantize_row_q(wdata, dst_row, ne00);\n        } else {\n            memcpy(dst_row, wdata, ne0*nb0);\n        }\n    }\n}\n\nstatic void ggml_compute_forward_add(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_add_f32(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F16:\n            {\n                if (src1->type == GGML_TYPE_F16) {\n                    ggml_compute_forward_add_f16_f16(params, src0, src1, dst);\n                }\n                else if (src1->type == GGML_TYPE_F32) {\n                    ggml_compute_forward_add_f16_f32(params, src0, src1, dst);\n                }\n                else {\n                    GGML_ASSERT(false);\n                }\n            } break;\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n            {\n                ggml_compute_forward_add_q_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_add1\n\nstatic void ggml_compute_forward_add1_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_is_scalar(src1));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    GGML_ASSERT( nb0 == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // src0 and dst are same shape => same indices\n        const int i3 = ir/(ne2*ne1);\n        const int i2 = (ir - i3*ne2*ne1)/ne1;\n        const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n#ifdef GGML_USE_ACCELERATE\n        UNUSED(ggml_vec_add1_f32);\n\n        vDSP_vadd(\n                (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01), 1,\n                (float *) ((char *) src1->data), 0,\n                (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 ), 1,\n                ne0);\n#else\n        ggml_vec_add1_f32(ne0,\n                (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 ),\n                (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01),\n               *(float *) src1->data);\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_add1_f16_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_is_scalar(src1));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // scalar to add\n    const float v = *(float *) src1->data;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n    GGML_ASSERT(dst->type  == GGML_TYPE_F16);\n\n    GGML_ASSERT( nb0 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nb00 == sizeof(ggml_fp16_t));\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // src0 and dst are same shape => same indices\n        const int i3 = ir/(ne2*ne1);\n        const int i2 = (ir - i3*ne2*ne1)/ne1;\n        const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n        ggml_fp16_t * dst_ptr  = (ggml_fp16_t *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 );\n        ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01);\n        for (int i = 0; i < ne0; i++) {\n            dst_ptr[i] = GGML_FP32_TO_FP16(GGML_FP16_TO_FP32(src0_ptr[i]) + v);\n        }\n    }\n}\n\nstatic void ggml_compute_forward_add1_f16_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_is_scalar(src1));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // scalar to add\n    const float v = GGML_FP16_TO_FP32(*(ggml_fp16_t *) src1->data);\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F16);\n    GGML_ASSERT(dst->type  == GGML_TYPE_F16);\n\n    GGML_ASSERT( nb0 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nb00 == sizeof(ggml_fp16_t));\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // src0 and dst are same shape => same indices\n        const int i3 = ir/(ne2*ne1);\n        const int i2 = (ir - i3*ne2*ne1)/ne1;\n        const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n        ggml_fp16_t * dst_ptr  = (ggml_fp16_t *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 );\n        ggml_fp16_t * src0_ptr = (ggml_fp16_t *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01);\n        for (int i = 0; i < ne0; i++) {\n            dst_ptr[i] = GGML_FP32_TO_FP16(GGML_FP16_TO_FP32(src0_ptr[i]) + v);\n        }\n    }\n}\n\nstatic void ggml_compute_forward_add1_q_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_is_scalar(src1));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // scalar to add\n    const float v = *(float *) src1->data;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    const enum ggml_type type = src0->type;\n    ggml_to_float_t const dequantize_row_q = type_traits[type].to_float;\n    ggml_from_float_t const quantize_row_q = type_traits[type].from_float;\n\n    // we don't support permuted src0\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    GGML_ASSERT(ggml_is_quantized(src0->type));\n    GGML_ASSERT(dst->type == src0->type);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    float * wdata = (float *) params->wdata + (ne0 + CACHE_LINE_SIZE_F32) * ith;\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // src0 and dst are same shape => same indices\n        const int i3 = ir/(ne2*ne1);\n        const int i2 = (ir - i3*ne2*ne1)/ne1;\n        const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n        void  * src0_row = (void *) ((char *) src0->data + (i1*nb01 + i2*nb02 + i3*nb03));\n        void  * dst_row  = (void *) ((char *)  dst->data + (i1*nb1  + i2*nb2  + i3*nb0 ));\n\n        assert(ne0 % 32 == 0);\n\n        // unquantize row from src0 to temp buffer\n        dequantize_row_q(src0_row, wdata, ne0);\n        // add src1\n        ggml_vec_acc1_f32(ne0, wdata, v);\n        // quantize row to dst\n        quantize_row_q(wdata, dst_row, ne0);\n    }\n}\n\nstatic void ggml_compute_forward_add1(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_add1_f32(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F16:\n            {\n                if (src1->type == GGML_TYPE_F16) {\n                    ggml_compute_forward_add1_f16_f16(params, src0, src1, dst);\n                }\n                else if (src1->type == GGML_TYPE_F32) {\n                    ggml_compute_forward_add1_f16_f32(params, src0, src1, dst);\n                }\n                else {\n                    GGML_ASSERT(false);\n                }\n            } break;\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n        case GGML_TYPE_Q8_1:\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n            {\n                ggml_compute_forward_add1_q_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_acc\n\nstatic void ggml_compute_forward_acc_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_is_contiguous(dst) && ggml_is_contiguous(src0));\n\n    // view src0 and dst with these strides and data offset inbytes during acc\n    // nb0 is implicitely element_size because src0 and dst are contiguous\n    size_t nb1     = ((int32_t *) dst->op_params)[0];\n    size_t nb2     = ((int32_t *) dst->op_params)[1];\n    size_t nb3     = ((int32_t *) dst->op_params)[2];\n    size_t offset  = ((int32_t *) dst->op_params)[3];\n    bool   inplace = (bool) ((int32_t *) dst->op_params)[4];\n\n    if (!inplace && (params->type == GGML_TASK_INIT)) {\n        // memcpy needs to be synchronized across threads to avoid race conditions.\n        // => do it in INIT phase\n        memcpy(\n            ((char *)  dst->data),\n            ((char *) src0->data),\n            ggml_nbytes(dst));\n    }\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr = ggml_nrows(src1);\n    const int nc = src1->ne[0];\n\n    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb)\n\n    // src0 and dst as viewed during acc\n    const size_t nb0 = ggml_element_size(src0);\n\n    const size_t nb00 = nb0;\n    const size_t nb01 = nb1;\n    const size_t nb02 = nb2;\n    const size_t nb03 = nb3;\n\n    GGML_ASSERT(offset + (ne10 == 0 ? 0 : ne10-1)*nb0  + (ne11 == 0 ? 0 : ne11-1)*nb1  + (ne12 == 0 ? 0 : ne12-1)*nb2  + (ne13 == 0 ? 0 : ne13-1)*nb3  < ggml_nbytes(dst));\n    GGML_ASSERT(offset + (ne10 == 0 ? 0 : ne10-1)*nb00 + (ne11 == 0 ? 0 : ne11-1)*nb01 + (ne12 == 0 ? 0 : ne12-1)*nb02 + (ne13 == 0 ? 0 : ne13-1)*nb03 < ggml_nbytes(src0));\n\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // src0 and dst are viewed with shape of src1 and offset\n        // => same indices\n        const int i3 = ir/(ne12*ne11);\n        const int i2 = (ir - i3*ne12*ne11)/ne11;\n        const int i1 = (ir - i3*ne12*ne11 - i2*ne11);\n\n#ifdef GGML_USE_ACCELERATE\n        vDSP_vadd(\n                (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01 + offset), 1,\n                (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11), 1,\n                (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1  + offset), 1, nc);\n#else\n        ggml_vec_add_f32(nc,\n                (float *) ((char *)  dst->data + i3*nb3  + i2*nb2  + i1*nb1  + offset),\n                (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01 + offset),\n                (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11));\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_acc(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_acc_f32(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F16:\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n        case GGML_TYPE_Q8_1:\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_sub\n\nstatic void ggml_compute_forward_sub_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    GGML_ASSERT( nb0 == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    if (nb10 == sizeof(float)) {\n        for (int ir = 0; ir < nr; ++ir) {\n            // src0, src1 and dst are same shape => same indices\n            const int i3 = ir/(ne2*ne1);\n            const int i2 = (ir - i3*ne2*ne1)/ne1;\n            const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n#ifdef GGML_USE_ACCELERATE\n            vDSP_vsub(\n                    (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11), 1,\n                    (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01), 1,\n                    (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 ), 1,\n                    ne0);\n#else\n            ggml_vec_sub_f32(ne0,\n                    (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 ),\n                    (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01),\n                    (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11));\n#endif\n                // }\n            // }\n        }\n    } else {\n        // src1 is not contiguous\n        for (int ir = 0; ir < nr; ++ir) {\n            // src0, src1 and dst are same shape => same indices\n            const int i3 = ir/(ne2*ne1);\n            const int i2 = (ir - i3*ne2*ne1)/ne1;\n            const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n            float * dst_ptr  = (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 );\n            float * src0_ptr = (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01);\n            for (int i0 = 0; i0 < ne0; i0++) {\n                float * src1_ptr = (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11 + i0*nb10);\n\n                dst_ptr[i0] = src0_ptr[i0] - *src1_ptr;\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_sub(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_sub_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_mul\n\nstatic void ggml_compute_forward_mul_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_can_repeat_rows(src1, src0) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n#ifdef GGML_USE_CLBLAST\n    if (src1->backend == GGML_BACKEND_GPU) {\n        if (ith == 0) {\n            ggml_cl_mul(src0, src1, dst);\n        }\n        return;\n    }\n#endif\n\n    const int64_t nr = ggml_nrows(src0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    GGML_ASSERT( nb0 == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n    GGML_ASSERT(ne00 == ne10);\n\n    if (nb10 == sizeof(float)) {\n        for (int64_t ir = ith; ir < nr; ir += nth) {\n            // src0 and dst are same shape => same indices\n            const int64_t i03 = ir/(ne02*ne01);\n            const int64_t i02 = (ir - i03*ne02*ne01)/ne01;\n            const int64_t i01 = (ir - i03*ne02*ne01 - i02*ne01);\n\n            const int64_t i13 = i03 % ne13;\n            const int64_t i12 = i02 % ne12;\n            const int64_t i11 = i01 % ne11;\n\n            float * dst_ptr  = (float *) ((char *) dst->data  + i03*nb3  + i02*nb2  + i01*nb1 );\n            float * src0_ptr = (float *) ((char *) src0->data + i03*nb03 + i02*nb02 + i01*nb01);\n            float * src1_ptr = (float *) ((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11);\n\n#ifdef GGML_USE_ACCELERATE\n            UNUSED(ggml_vec_mul_f32);\n\n            vDSP_vmul( src0_ptr, 1, src1_ptr, 1, dst_ptr,  1, ne00);\n#else\n            ggml_vec_mul_f32(ne00, dst_ptr, src0_ptr, src1_ptr);\n#endif\n                // }\n            // }\n        }\n    } else {\n        // src1 is not contiguous\n        for (int64_t ir = ith; ir < nr; ir += nth) {\n            // src0 and dst are same shape => same indices\n            // src1 is broadcastable across src0 and dst in i1, i2, i3\n            const int64_t i03 = ir/(ne02*ne01);\n            const int64_t i02 = (ir - i03*ne02*ne01)/ne01;\n            const int64_t i01 = (ir - i03*ne02*ne01 - i02*ne01);\n\n            const int64_t i13 = i03 % ne13;\n            const int64_t i12 = i02 % ne12;\n            const int64_t i11 = i01 % ne11;\n\n            float * dst_ptr  = (float *) ((char *) dst->data  + i03*nb3  + i02*nb2  + i01*nb1 );\n            float * src0_ptr = (float *) ((char *) src0->data + i03*nb03 + i02*nb02 + i01*nb01);\n\n            for (int64_t i0 = 0; i0 < ne00; i0++) {\n                float * src1_ptr = (float *) ((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11 + i0*nb10);\n\n                dst_ptr[i0] = src0_ptr[i0] * (*src1_ptr);\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_mul(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(src1->type == GGML_TYPE_F32 && \"only f32 src1 supported for now\");\n\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_mul_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_div\n\nstatic void ggml_compute_forward_div_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int nr  = ggml_nrows(src0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    GGML_ASSERT( nb0 == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    if (nb10 == sizeof(float)) {\n        for (int ir = 0; ir < nr; ++ir) {\n            // src0, src1 and dst are same shape => same indices\n            const int i3 = ir/(ne2*ne1);\n            const int i2 = (ir - i3*ne2*ne1)/ne1;\n            const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n#ifdef GGML_USE_ACCELERATE\n            UNUSED(ggml_vec_div_f32);\n\n            vDSP_vdiv(\n                    (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11), 1,\n                    (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01), 1,\n                    (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 ), 1,\n                    ne0);\n#else\n            ggml_vec_div_f32(ne0,\n                    (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 ),\n                    (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01),\n                    (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11));\n#endif\n                // }\n            // }\n        }\n    } else {\n        // src1 is not contiguous\n        for (int ir = 0; ir < nr; ++ir) {\n            // src0, src1 and dst are same shape => same indices\n            const int i3 = ir/(ne2*ne1);\n            const int i2 = (ir - i3*ne2*ne1)/ne1;\n            const int i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n            float * dst_ptr  = (float *) ((char *) dst->data  + i3*nb3  + i2*nb2  + i1*nb1 );\n            float * src0_ptr = (float *) ((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01);\n            for (int i0 = 0; i0 < ne0; i0++) {\n                float * src1_ptr = (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11 + i0*nb10);\n\n                dst_ptr[i0] = src0_ptr[i0] / (*src1_ptr);\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_div(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_div_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_sqr\n\nstatic void ggml_compute_forward_sqr_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n     = ggml_nrows(src0);\n    const int nc    = src0->ne[0];\n\n    assert( dst->nb[0] == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_sqr_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_sqr(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_sqr_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_sqrt\n\nstatic void ggml_compute_forward_sqrt_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert( dst->nb[0] == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_sqrt_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_sqrt(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_sqrt_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_log\n\nstatic void ggml_compute_forward_log_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(params->ith == 0);\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    GGML_ASSERT( dst->nb[0] == sizeof(float));\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_log_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_log(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_log_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_sum\n\nstatic void ggml_compute_forward_sum_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_is_scalar(dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    assert(ggml_is_scalar(dst));\n    assert(src0->nb[0] == sizeof(float));\n\n    GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)\n\n    ggml_float sum     = 0;\n    ggml_float row_sum = 0;\n\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        for (int64_t i02 = 0; i02 < ne02; i02++) {\n            for (int64_t i01 = 0; i01 < ne01; i01++) {\n                ggml_vec_sum_f32_ggf(ne00,\n                        &row_sum,\n                        (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03));\n                sum += row_sum;\n            }\n        }\n    }\n    ((float *) dst->data)[0] = sum;\n}\n\nstatic void ggml_compute_forward_sum_f16(\n    const struct ggml_compute_params * params,\n    const struct ggml_tensor * src0,\n          struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_is_scalar(dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    assert(src0->nb[0] == sizeof(ggml_fp16_t));\n\n    GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)\n\n    float sum = 0;\n    float row_sum = 0;\n\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        for (int64_t i02 = 0; i02 < ne02; i02++) {\n            for (int64_t i01 = 0; i01 < ne01; i01++) {\n                ggml_vec_sum_f16_ggf(ne00,\n                    &row_sum,\n                    (ggml_fp16_t *) ((char *) src0->data + i01 * nb01 + i02 * nb02 + i03 * nb03));\n                sum += row_sum;\n            }\n        }\n    }\n    ((ggml_fp16_t *) dst->data)[0] = GGML_FP32_TO_FP16(sum);\n}\n\nstatic void ggml_compute_forward_sum_i32(\n    const struct ggml_compute_params * params,\n    const struct ggml_tensor * src0,\n          struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_is_scalar(dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    assert(src0->nb[0] == sizeof(int32_t));\n\n    GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb0, src0, nb)\n\n    int64_t sum = 0;\n    int64_t row_sum = 0;\n\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        for (int64_t i02 = 0; i02 < ne02; i02++) {\n            for (int64_t i01 = 0; i01 < ne01; i01++) {\n                ggml_vec_sum_i32_ggf(ne00,\n                    &row_sum,\n                    (int32_t *) ((char *) src0->data + i01 * nb01 + i02 * nb02 + i03 * nb03));\n                sum += row_sum;\n            }\n        }\n    }\n    ((int32_t *) dst->data)[0] = sum;\n}\n\n\nstatic void ggml_compute_forward_sum(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_sum_f32(params, src0, dst);\n            } break;\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_sum_f16(params, src0, dst);\n            } break;\n        case GGML_TYPE_I32:\n            {\n                ggml_compute_forward_sum_i32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_sum_rows\n\nstatic void ggml_compute_forward_sum_rows_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n    GGML_ASSERT(dst->nb[0] == sizeof(float));\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    GGML_ASSERT(ne0 == 1);\n    GGML_ASSERT(ne1 == ne01);\n    GGML_ASSERT(ne2 == ne02);\n    GGML_ASSERT(ne3 == ne03);\n\n    for (int64_t i3 = 0; i3 < ne03; i3++) {\n        for (int64_t i2 = 0; i2 < ne02; i2++) {\n            for (int64_t i1 = 0; i1 < ne01; i1++) {\n                float * src_row = (float *) ((char *) src0->data + i1*nb01 + i2*nb02 + i3*nb03);\n                float * dst_row = (float *) ((char *) dst->data  + i1*nb1  + i2*nb2  + i3*nb3);\n                float row_sum = 0;\n                ggml_vec_sum_f32(ne00, &row_sum, src_row);\n                dst_row[0] = row_sum;\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_sum_rows(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_sum_rows_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_mean\n\nstatic void ggml_compute_forward_mean_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    assert(src0->nb[0] == sizeof(float));\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    assert(ne0 == 1);\n    assert(ne1 == ne01);\n    assert(ne2 == ne02);\n    assert(ne3 == ne03);\n\n    UNUSED(ne0);\n    UNUSED(ne1);\n    UNUSED(ne2);\n    UNUSED(ne3);\n\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        for (int64_t i02 = 0; i02 < ne02; i02++) {\n            for (int64_t i01 = 0; i01 < ne01; i01++) {\n                ggml_vec_sum_f32(ne00,\n                        (float *) ((char *)  dst->data + i01*nb1  + i02*nb2  + i03*nb3),\n                        (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03));\n\n                *(float *) ((char *) dst->data + i01*nb1 + i02*nb2 + i03*nb3) /= (float) ne00;\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_mean(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_mean_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_argmax\n\nstatic void ggml_compute_forward_argmax_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    assert(src0->nb[0] == sizeof(float));\n    assert(dst->nb[0] == sizeof(float));\n\n    const int64_t ne00 = src0->ne[0];\n    const int64_t ne01 = src0->ne[1];\n\n    const size_t nb01 = src0->nb[1];\n    const size_t nb0 = dst->nb[0];\n\n    for (int64_t i1 = 0; i1 < ne01; i1++) {\n        float * src = (float *) ((char *) src0->data + i1*nb01);\n        int32_t * dst_ = (int32_t *) ((char *)  dst->data + i1*nb0);\n        int v = 0;\n        ggml_vec_argmax_f32(ne00, &v, src);\n        dst_[0] = v;\n    }\n}\n\nstatic void ggml_compute_forward_argmax(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_argmax_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_repeat\n\nstatic void ggml_compute_forward_repeat_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(params->ith == 0);\n    GGML_ASSERT(ggml_can_repeat(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    // guaranteed to be an integer due to the check in ggml_can_repeat\n    const int nr0 = (int)(ne0/ne00);\n    const int nr1 = (int)(ne1/ne01);\n    const int nr2 = (int)(ne2/ne02);\n    const int nr3 = (int)(ne3/ne03);\n\n    // TODO: support for transposed / permuted tensors\n    GGML_ASSERT(nb0  == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    // TODO: maybe this is not optimal?\n    for                         (int i3 = 0; i3 < nr3;  i3++) {\n        for                     (int k3 = 0; k3 < ne03; k3++) {\n            for                 (int i2 = 0; i2 < nr2;  i2++) {\n                for             (int k2 = 0; k2 < ne02; k2++) {\n                    for         (int i1 = 0; i1 < nr1;  i1++) {\n                        for     (int k1 = 0; k1 < ne01; k1++) {\n                            for (int i0 = 0; i0 < nr0;  i0++) {\n                                ggml_vec_cpy_f32(ne00,\n                                        (float *) ((char *)  dst->data + (i3*ne03 + k3)*nb3  + (i2*ne02 + k2)*nb2  + (i1*ne01 + k1)*nb1  + (i0*ne00)*nb0),\n                                        (float *) ((char *) src0->data + (          k3)*nb03 + (          k2)*nb02 + (          k1)*nb01));\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_repeat_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(params->ith == 0);\n    GGML_ASSERT(ggml_can_repeat(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_TENSOR_UNARY_OP_LOCALS;\n\n    // guaranteed to be an integer due to the check in ggml_can_repeat\n    const int nr0 = (int)(ne0/ne00);\n    const int nr1 = (int)(ne1/ne01);\n    const int nr2 = (int)(ne2/ne02);\n    const int nr3 = (int)(ne3/ne03);\n\n    // TODO: support for transposed / permuted tensors\n    GGML_ASSERT(nb0  == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nb00 == sizeof(ggml_fp16_t));\n\n    // TODO: maybe this is not optimal?\n    for                         (int i3 = 0; i3 < nr3;  i3++) {\n        for                     (int k3 = 0; k3 < ne03; k3++) {\n            for                 (int i2 = 0; i2 < nr2;  i2++) {\n                for             (int k2 = 0; k2 < ne02; k2++) {\n                    for         (int i1 = 0; i1 < nr1;  i1++) {\n                        for     (int k1 = 0; k1 < ne01; k1++) {\n                            for (int i0 = 0; i0 < nr0;  i0++) {\n                                ggml_fp16_t * y = (ggml_fp16_t *) ((char *)  dst->data + (i3*ne03 + k3)*nb3  + (i2*ne02 + k2)*nb2  + (i1*ne01 + k1)*nb1  + (i0*ne00)*nb0);\n                                ggml_fp16_t * x = (ggml_fp16_t *) ((char *) src0->data + (          k3)*nb03 + (          k2)*nb02 + (          k1)*nb01);\n                                // ggml_vec_cpy_f16(ne00, y, x)\n                                for (int i = 0; i < ne00; ++i) {\n                                    y[i]  = x[i];\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_repeat(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_repeat_f16(params, src0, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_repeat_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_repeat_back\n\nstatic void ggml_compute_forward_repeat_back_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(params->ith == 0);\n    GGML_ASSERT(ggml_can_repeat(dst, src0));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    // guaranteed to be an integer due to the check in ggml_can_repeat\n    const int nr0 = (int)(ne00/ne0);\n    const int nr1 = (int)(ne01/ne1);\n    const int nr2 = (int)(ne02/ne2);\n    const int nr3 = (int)(ne03/ne3);\n\n    // TODO: support for transposed / permuted tensors\n    GGML_ASSERT(nb0  == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    if (ggml_is_contiguous(dst)) {\n        ggml_vec_set_f32(ne0*ne1*ne2*ne3, dst->data, 0);\n    } else {\n        for         (int k3 = 0; k3 < ne3; k3++) {\n            for     (int k2 = 0; k2 < ne2; k2++) {\n                for (int k1 = 0; k1 < ne1; k1++) {\n                    ggml_vec_set_f32(ne0,\n                        (float *) ((char *) dst->data + k1*nb1 + k2*nb2 + k3*nb3),\n                        0);\n                }\n            }\n        }\n    }\n\n    // TODO: maybe this is not optimal?\n    for                         (int i3 = 0; i3 < nr3; i3++) {\n        for                     (int k3 = 0; k3 < ne3; k3++) {\n            for                 (int i2 = 0; i2 < nr2; i2++) {\n                for             (int k2 = 0; k2 < ne2; k2++) {\n                    for         (int i1 = 0; i1 < nr1; i1++) {\n                        for     (int k1 = 0; k1 < ne1; k1++) {\n                            for (int i0 = 0; i0 < nr0; i0++) {\n                                ggml_vec_acc_f32(ne0,\n                                        (float *) ((char *)  dst->data + (         k3)*nb3  + (         k2)*nb2  + (         k1)*nb1),\n                                        (float *) ((char *) src0->data + (i3*ne3 + k3)*nb03 + (i2*ne2 + k2)*nb02 + (i1*ne1 + k1)*nb01 + (i0*ne0)*nb00));\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_repeat_back(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_repeat_back_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_concat\n\nstatic void ggml_compute_forward_concat_f32(\n    const struct ggml_compute_params * params,\n    const struct ggml_tensor * src0,\n    const struct ggml_tensor * src1,\n    struct ggml_tensor * dst) {\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    const int ith = params->ith;\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    // TODO: support for transposed / permuted tensors\n    GGML_ASSERT(nb0  == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    for (int i3 = 0; i3 < ne3; i3++) {\n        for (int i2 = ith; i2 < ne2; i2++) {\n            if (i2 < ne02) { // src0\n                for (int i1 = 0; i1 < ne1; i1++) {\n                    for (int i0 = 0; i0 < ne0; i0++) {\n                        const float * x = (float *)((char *) src0->data + i0 * nb00 + i1 * nb01 + i2 * nb02 + i3 * nb03);\n\n                        float * y = (float *)((char *)dst->data + i0 * nb0 + i1 * nb1 + i2 * nb2 + i3 * nb3);\n                        *y = *x;\n                    }\n                }\n            } // src1\n            else {\n                for (int i1 = 0; i1 < ne1; i1++) {\n                    for (int i0 = 0; i0 < ne0; i0++) {\n                        const float * x = (float *)((char *) src1->data + i0 * nb10 + i1 * nb11 + (i2 - ne02) * nb12 + i3 * nb13);\n\n                        float * y = (float *)((char *)dst->data + i0 * nb0 + i1 * nb1 + i2 * nb2 + i3 * nb3);\n                        *y = *x;\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_concat(\n    const struct ggml_compute_params* params,\n    const struct ggml_tensor* src0,\n    const struct ggml_tensor* src1,\n    struct ggml_tensor* dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_concat_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_abs\n\nstatic void ggml_compute_forward_abs_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert(dst->nb[0]  == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_abs_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_abs(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_abs_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_sgn\n\nstatic void ggml_compute_forward_sgn_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert(dst->nb[0]  == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_sgn_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_sgn(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_sgn_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_neg\n\nstatic void ggml_compute_forward_neg_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert(dst->nb[0]  == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_neg_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_neg(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_neg_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_step\n\nstatic void ggml_compute_forward_step_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert(dst->nb[0]  == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_step_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_step(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_step_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_tanh\n\nstatic void ggml_compute_forward_tanh_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert(dst->nb[0]  == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_tanh_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_tanh(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_tanh_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_elu\n\nstatic void ggml_compute_forward_elu_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert(dst->nb[0]  == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_elu_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_elu(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_elu_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_relu\n\nstatic void ggml_compute_forward_relu_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert(dst->nb[0]  == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_relu_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_relu(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_relu_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_gelu\n\nstatic void ggml_compute_forward_gelu_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(src0));\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nrows(src0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        ggml_vec_gelu_f32(nc,\n                (float *) ((char *) dst->data  + i1*( dst->nb[1])),\n                (float *) ((char *) src0->data + i1*(src0->nb[1])));\n\n#ifndef NDEBUG\n        for (int k = 0; k < nc; k++) {\n            const float x = ((float *) ((char *) dst->data + i1*( dst->nb[1])))[k];\n            UNUSED(x);\n            assert(!isnan(x));\n            assert(!isinf(x));\n        }\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_gelu(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_gelu_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_gelu_quick\n\nstatic void ggml_compute_forward_gelu_quick_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(src0));\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nrows(src0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        ggml_vec_gelu_quick_f32(nc,\n                (float *) ((char *) dst->data  + i1*( dst->nb[1])),\n                (float *) ((char *) src0->data + i1*(src0->nb[1])));\n\n#ifndef NDEBUG\n        for (int k = 0; k < nc; k++) {\n            const float x = ((float *) ((char *) dst->data + i1*( dst->nb[1])))[k];\n            UNUSED(x);\n            assert(!isnan(x));\n            assert(!isinf(x));\n        }\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_gelu_quick(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_gelu_quick_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_silu\n\nstatic void ggml_compute_forward_silu_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(src0));\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nrows(src0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        ggml_vec_silu_f32(nc,\n                (float *) ((char *) dst->data  + i1*( dst->nb[1])),\n                (float *) ((char *) src0->data + i1*(src0->nb[1])));\n\n#ifndef NDEBUG\n        for (int k = 0; k < nc; k++) {\n            const float x = ((float *) ((char *) dst->data + i1*(dst->nb[1])))[k];\n            UNUSED(x);\n            assert(!isnan(x));\n            assert(!isinf(x));\n        }\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_silu(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_silu_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_leaky\n\nstatic void ggml_compute_forward_leaky_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert(dst->nb[0]  == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        ggml_vec_leaky_f32(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_leaky(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_leaky_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_silu_back\n\nstatic void ggml_compute_forward_silu_back_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * grad,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(grad));\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(src0));\n    GGML_ASSERT(ggml_is_contiguous_except_dim_1(dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, grad));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nrows(src0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        ggml_vec_silu_backward_f32(nc,\n                (float *) ((char *) dst->data  + i1*( dst->nb[1])),\n                (float *) ((char *) src0->data + i1*(src0->nb[1])),\n                (float *) ((char *) grad->data + i1*(grad->nb[1])));\n\n#ifndef NDEBUG\n        for (int k = 0; k < nc; k++) {\n            const float x = ((float *) ((char *) dst->data + i1*( dst->nb[1])))[k];\n            UNUSED(x);\n            assert(!isnan(x));\n            assert(!isinf(x));\n        }\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_silu_back(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * grad,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_silu_back_f32(params, src0, grad, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_norm\n\nstatic void ggml_compute_forward_norm_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    float eps;\n    memcpy(&eps, dst->op_params, sizeof(float));\n\n    // TODO: optimize\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        for (int64_t i02 = 0; i02 < ne02; i02++) {\n            for (int64_t i01 = ith; i01 < ne01; i01 += nth) {\n                const float * x = (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03);\n\n                ggml_float sum = 0.0;\n                for (int64_t i00 = 0; i00 < ne00; i00++) {\n                    sum += (ggml_float)x[i00];\n                }\n\n                float mean = sum/ne00;\n\n                float * y = (float *) ((char *) dst->data + i01*nb1 + i02*nb2 + i03*nb3);\n\n                ggml_float sum2 = 0.0;\n                for (int64_t i00 = 0; i00 < ne00; i00++) {\n                    float v = x[i00] - mean;\n                    y[i00] = v;\n                    sum2 += (ggml_float)(v*v);\n                }\n\n                float variance = sum2/ne00;\n                const float scale = 1.0f/sqrtf(variance + eps);\n\n                ggml_vec_scale_f32(ne00, y, scale);\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_norm(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_norm_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_group_rms_norm\n\nstatic void ggml_compute_forward_rms_norm_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    float eps;\n    memcpy(&eps, dst->op_params, sizeof(float));\n\n    // TODO: optimize\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        for (int64_t i02 = 0; i02 < ne02; i02++) {\n            for (int64_t i01 = ith; i01 < ne01; i01 += nth) {\n                const float * x = (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03);\n\n                ggml_float sum = 0.0;\n                for (int64_t i00 = 0; i00 < ne00; i00++) {\n                    sum += (ggml_float)(x[i00] * x[i00]);\n                }\n\n                const float mean = sum/ne00;\n\n                float * y = (float *) ((char *) dst->data + i01*nb1 + i02*nb2 + i03*nb3);\n\n                memcpy(y, x, ne00 * sizeof(float));\n                // for (int i00 = 0; i00 < ne00; i00++) {\n                //     y[i00] = x[i00];\n                // }\n\n                const float scale = 1.0f/sqrtf(mean + eps);\n\n                ggml_vec_scale_f32(ne00, y, scale);\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_rms_norm(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_rms_norm_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\nstatic void ggml_compute_forward_rms_norm_back_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst) && ggml_are_same_shape(src0, src1));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    float eps;\n    memcpy(&eps, dst->op_params, sizeof(float));\n\n    // TODO: optimize\n    for (int64_t i03 = 0; i03 < ne03; i03++) {\n        for (int64_t i02 = 0; i02 < ne02; i02++) {\n            for (int64_t i01 = ith; i01 < ne01; i01 += nth) {\n                // src1 is same shape as src0 => same indices\n                const int64_t i11 = i01;\n                const int64_t i12 = i02;\n                const int64_t i13 = i03;\n\n                const float * x = (float *) ((char *) src0->data + i01*nb01 + i02*nb02 + i03*nb03);\n                const float * dz = (float *) ((char *) src1->data + i11*nb11 + i12*nb12 + i13*nb13);\n\n                ggml_float sum_xx  = 0.0;\n                ggml_float sum_xdz = 0.0;\n\n                for (int64_t i00 = 0; i00 < ne00; i00++) {\n                    sum_xx  += (ggml_float)(x[i00] * x[i00]);\n                    sum_xdz += (ggml_float)(x[i00] * dz[i00]);\n                }\n\n                //const float mean     = (float)(sum_xx)/ne00;\n                const float mean_eps = (float)(sum_xx)/ne00 + eps;\n                const float sum_eps  = (float)(sum_xx) + eps*ne00;\n                //const float mean_xdz = (float)(sum_xdz)/ne00;\n                // we could cache rms from forward pass to improve performance.\n                // to do this implement ggml_rms and compose ggml_rms_norm using ggml_rms.\n                //const float rms      = sqrtf(mean_eps);\n                const float rrms     = 1.0f / sqrtf(mean_eps);\n                //const float scale    = -rrms/(ne00 * mean_eps); // -1/(n*rms**3)\n\n                {\n                    // z = rms_norm(x)\n                    //\n                    // rms_norm(src0) =\n                    //     scale(\n                    //         src0,\n                    //         div(\n                    //             1,\n                    //             sqrt(\n                    //                 add(\n                    //                     scale(\n                    //                         sum(\n                    //                             sqr(\n                    //                                 src0)),\n                    //                         (1.0/N)),\n                    //                     eps))));\n\n                    // postorder:\n                    // ## op    args         grad\n                    // 00 param src0         grad[#00]\n                    // 01 const 1\n                    // 02 sqr   (#00)        grad[#02]\n                    // 03 sum   (#02)        grad[#03]\n                    // 04 const 1/N\n                    // 05 scale (#03, #04)   grad[#05]\n                    // 06 const eps\n                    // 07 add   (#05, #06)   grad[#07]\n                    // 08 sqrt  (#07)        grad[#08]\n                    // 09 div   (#01,#08)    grad[#09]\n                    // 10 scale (#00,#09)    grad[#10]\n                    //\n                    // backward pass, given grad[#10]\n                    // #10: scale\n                    // grad[#00] += scale(grad[#10],#09)\n                    // grad[#09] += sum(mul(grad[#10],#00))\n                    // #09: div\n                    // grad[#08] += neg(mul(grad[#09], div(#09,#08)))\n                    // #08: sqrt\n                    // grad[#07] += mul(grad[#08], div(0.5, #08))\n                    // #07: add\n                    // grad[#05] += grad[#07]\n                    // #05: scale\n                    // grad[#03] += scale(grad[#05],#04)\n                    // #03: sum\n                    // grad[#02] += repeat(grad[#03], #02)\n                    // #02:\n                    // grad[#00] += scale(mul(#00, grad[#02]), 2.0)\n                    //\n                    // substitute and simplify:\n                    // grad[#00] = scale(grad(#10), #09) + scale(mul(#00, grad[#02]), 2.0)\n                    // grad[#02] = repeat(grad[#03], #02)\n                    // grad[#02] = repeat(scale(grad[#05],#04), #02)\n                    // grad[#02] = repeat(scale(grad[#07],#04), #02)\n                    // grad[#02] = repeat(scale(mul(grad[#08], div(0.5, #08)),#04), #02)\n                    // grad[#02] = repeat(scale(mul(neg(mul(grad[#09], div(#09,#08))), div(0.5, #08)),#04), #02)\n                    // grad[#02] = repeat(scale(mul(neg(mul(sum(mul(grad[#10],#00)), div(#09,#08))), div(0.5, #08)),#04), #02)\n                    // grad[#02] = repeat(-(sum(mul(grad[#10],#00)) * div(#09,#08) * div(0.5, #08) * (1/N)), #02)\n                    // grad[#02] = repeat(-(sum(mul(grad[#10],#00)) * div(div(#01,#08),#08) * div(0.5, #08) * (1/N)), #02)\n                    // grad[#02] = repeat(-(sum(mul(grad[#10],#00)) * div(1,#08*#08) * div(0.5, #08) * (1/N)), #02)\n                    // grad[#02] = repeat(-(sum(mul(grad[#10],#00)) * div(1,#07) * div(0.5, #08) * (1/N)), #02)\n                    // grad[#00] = scale(grad(#10), #09) + scale(mul(#00, grad[#02]), 2.0)\n                    // grad[#00] = scale(grad(#10), #09) + scale(mul(#00, repeat(-(sum(mul(grad[#10],#00)) * div(1,#07) * div(0.5, #08) * (1/N)), #02)), 2.0)\n                    // grad[#00] = scale(grad(#10), #09) + scale(scale(#00, -(sum(mul(grad[#10],#00)) * div(1,#07) * div(0.5, #08) * (1/N))), 2.0)\n                    // grad[#00] = scale(grad(#10), #09) + scale(#00, -(sum(mul(grad[#10],#00)) * div(1,#07) * div(1,#08) * (1/N)))\n                    // grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(1,#07*#08) * (-1/N))\n                    // grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(1,#07*#08) * (-1/N))\n                    // grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(1,mean_eps*rms) * (-1/N))\n                    // grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(-1,rms*N*mean_eps))\n                    // grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(-1,rms*N*(sum_xx/N+eps)))\n                    // grad[#00] = scale(grad(#10), #09) + scale(#00, sum(mul(grad[#10],#00)) * div(-1,rms*N*sum_xx+rms*N*eps))\n                    // grad[#00] = scale(dz, rrms) + scale(x, sum(mul(dz,x)) * div(-1,rms*N*mean_eps))\n                    // grad[#00] = scale(dz, rrms) + scale(x, sum_xdz * div(-1,rms*N*mean_eps))\n                    // a = b*c + d*e\n                    // a = b*c*f/f + d*e*f/f\n                    // a = (b*c*f + d*e*f)*(1/f)\n                    // a = (b*c*(1/c) + d*e*(1/c))*(1/(1/c))\n                    // a = (b + d*e/c)*c\n                    // b = dz, c = rrms, d = x, e = sum_xdz * div(-1,rms*N*mean_eps)\n                    // a = (dz + x*sum_xdz * div(-1,rms*N*mean_eps)/rrms)*rrms\n                    // a = (dz + x*sum_xdz * div(-1,rms*N*mean_eps)*rms)*rrms\n                    // a = (dz + x*sum_xdz * div(-rms,rms*N*mean_eps))*rrms\n                    // a = (dz + x*sum_xdz * div(-1,N*mean_eps))*rrms\n                    // a = (dz + x*div(-sum_xdz,N*mean_eps))*rrms\n                    // a = (dz + x*div(-mean_xdz,mean_eps))*rrms\n                    // grad[#00] = scale(dz + scale(x, div(-mean_xdz,mean_eps)),rrms)\n                    // grad[#00] = scale(dz + scale(x, -mean_xdz/mean_eps),rrms)\n                    // dx = scale(dz + scale(x, -mean_xdz/mean_eps),rrms)\n                }\n                // dx = scale(dz + scale(x, -mean_xdz/mean_eps),rrms)\n                // post-order:\n                // dx := x\n                // dx := scale(dx,-mean_xdz/mean_eps)\n                // dx := add(dx, dz)\n                // dx := scale(dx, rrms)\n                float * dx = (float *) ((char *) dst->data + i01*nb1 + i02*nb2 + i03*nb3);\n\n                ggml_vec_cpy_f32  (ne00, dx, x);\n                // ggml_vec_scale_f32(ne00, dx, -mean_xdz/mean_eps);\n                ggml_vec_scale_f32(ne00, dx, (float)(-sum_xdz)/sum_eps);\n                ggml_vec_acc_f32  (ne00, dx, dz);\n                ggml_vec_scale_f32(ne00, dx, rrms);\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_rms_norm_back(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_rms_norm_back_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_group_norm\n\nstatic void ggml_compute_forward_group_norm_f32(\n    const struct ggml_compute_params * params,\n    const struct ggml_tensor * src0,\n    struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    const float eps = 1e-6f; // TODO: make this a parameter\n\n    // TODO: optimize\n\n    int n_channels = src0->ne[2];\n    int n_groups = dst->op_params[0];\n    int n_channels_per_group = (n_channels + n_groups - 1) / n_groups;\n    for (int i = ith; i < n_groups; i+=nth) {\n        int start = i * n_channels_per_group;\n        int end = start + n_channels_per_group;\n        if (end > n_channels) {\n            end = n_channels;\n        }\n        int step = end - start;\n\n        for (int64_t i03 = 0; i03 < ne03; i03++) {\n            ggml_float sum = 0.0;\n            for (int64_t i02 = start; i02 < end; i02++) {\n                for (int64_t i01 = 0; i01 < ne01; i01++) {\n                    const float * x = (float *)((char *) src0->data + i01 * nb01 + i02 * nb02 + i03 * nb03);\n\n                    for (int64_t i00 = 0; i00 < ne00; i00++) {\n                        sum += (ggml_float)x[i00];\n                    }\n                }\n            }\n            float mean = sum / (ne00 * ne01 * step);\n            ggml_float sum2 = 0.0;\n\n            for (int64_t i02 = start; i02 < end; i02++) {\n                for (int64_t i01 = 0; i01 < ne01; i01++) {\n                    const float * x = (float *)((char *) src0->data + i01 * nb01 + i02 * nb02 + i03 * nb03);\n\n                    float * y = (float *)((char *) dst->data + i01 * nb1 + i02 * nb2 + i03 * nb3);\n\n                    for (int64_t i00 = 0; i00 < ne00; i00++) {\n                        float v = x[i00] - mean;\n                        y[i00] = v;\n                        sum2 += (ggml_float)(v * v);\n                    }\n                }\n            }\n            float variance = sum2 / (ne00 * ne01 * step);\n            const float scale = 1.0f / sqrtf(variance + eps);\n\n            for (int64_t i02 = start; i02 < end; i02++) {\n                for (int64_t i01 = 0; i01 < ne01; i01++) {\n                    float * y = (float *)((char *) dst->data + i01 * nb1 + i02 * nb2 + i03 * nb3);\n                    ggml_vec_scale_f32(ne00, y, scale);\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_group_norm(\n    const struct ggml_compute_params * params,\n    const struct ggml_tensor * src0,\n    struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_group_norm_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_mul_mat\n\n#if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS)\n// helper function to determine if it is better to use BLAS or not\n// for large matrices, BLAS is faster\nstatic bool ggml_compute_forward_mul_mat_use_blas(\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    //const int64_t ne00 = src0->ne[0];\n    //const int64_t ne01 = src0->ne[1];\n\n    const int64_t ne10 = src1->ne[0];\n\n    const int64_t ne0 = dst->ne[0];\n    const int64_t ne1 = dst->ne[1];\n\n    // TODO: find the optimal values for these\n    if (ggml_is_contiguous(src0) &&\n        ggml_is_contiguous(src1) &&\n        src0->type == GGML_TYPE_F32 &&\n        src1->type == GGML_TYPE_F32 &&\n        (ne0 >= 32 && ne1 >= 32 && ne10 >= 32)) {\n\n        /*printf(\"BLAS: %d %d %d %d %d\\n\", ne0, ne1, ne10, ne00, ne01);*/\n        return true;\n    }\n\n    return false;\n}\n#endif\n\nstatic void ggml_compute_forward_mul_mat(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const enum ggml_type type = src0->type;\n\n    const bool src1_cont = ggml_is_contiguous(src1);\n\n    ggml_vec_dot_t    const vec_dot               = type_traits[type].vec_dot;\n    enum ggml_type    const vec_dot_type          = type_traits[type].vec_dot_type;\n    ggml_from_float_t const from_float_to_vec_dot = type_traits[vec_dot_type].from_float;\n\n    GGML_ASSERT(ne0 == ne01);\n    GGML_ASSERT(ne1 == ne11);\n    GGML_ASSERT(ne2 == ne12);\n    GGML_ASSERT(ne3 == ne13);\n\n    // we don't support permuted src0 or src1\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n    GGML_ASSERT(nb10 == ggml_type_size(src1->type));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    // broadcast factors\n    const int64_t r2 = ne12/ne02;\n    const int64_t r3 = ne13/ne03;\n\n    // nb01 >= nb00 - src0 is not transposed\n    //   compute by src0 rows\n\n#if defined(GGML_USE_CLBLAST)\n    if (ggml_cl_can_mul_mat(src0, src1, dst)) {\n        if (params->ith == 0 && params->type == GGML_TASK_COMPUTE) {\n            ggml_cl_mul_mat(src0, src1, dst, params->wdata, params->wsize);\n        }\n        return;\n    }\n#endif\n\n#if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS)\n    if (ggml_compute_forward_mul_mat_use_blas(src0, src1, dst)) {\n        if (params->ith != 0) {\n            return;\n        }\n\n        if (params->type == GGML_TASK_INIT) {\n            return;\n        }\n\n        if (params->type == GGML_TASK_FINALIZE) {\n            return;\n        }\n\n        for (int64_t i13 = 0; i13 < ne13; i13++) {\n            for (int64_t i12 = 0; i12 < ne12; i12++) {\n                // broadcast src0 into src1 across 2nd,3rd dimension\n                const int64_t i03 = i13/r3;\n                const int64_t i02 = i12/r2;\n\n                const void  * x = (char *)            src0->data + i02*nb02 + i03*nb03;\n                const float * y = (float *) ((char *) src1->data + i12*nb12 + i13*nb13);\n\n                float * d = (float *) ((char *) dst->data + i12*nb2 + i13*nb3);\n\n                if (type != GGML_TYPE_F32) {\n                            float * const wdata    = params->wdata;\n                    ggml_to_float_t const to_float = type_traits[type].to_float;\n\n                    size_t id = 0;\n                    for (int64_t i01 = 0; i01 < ne01; ++i01) {\n                        to_float((const char *) x + i01*nb01, wdata + id, ne00);\n                        id += ne00;\n                    }\n\n                    assert(id*sizeof(float) <= params->wsize);\n                    x = wdata;\n                }\n\n                cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans,\n                        ne11, ne01, ne10,\n                        1.0f,    y, ne10,\n                                 x, ne00,\n                        0.0f,    d, ne01);\n            }\n        }\n\n        //printf(\"CBLAS = %f ms, %d x %d x %d x %d\\n\", (ggml_perf_time_us() - t0)/1000.0, ne0, ne1, ne2, ne3);\n\n        return;\n    }\n#endif\n\n    if (params->type == GGML_TASK_INIT) {\n        if (src1->type != vec_dot_type) {\n            char * wdata = params->wdata;\n            const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n            for (int64_t i13 = 0; i13 < ne13; ++i13) {\n                for (int64_t i12 = 0; i12 < ne12; ++i12) {\n                    for (int64_t i11 = 0; i11 < ne11; ++i11) {\n                        from_float_to_vec_dot((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);\n                        wdata += row_size;\n                    }\n                }\n            }\n        }\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const void * wdata    = (src1->type == vec_dot_type) ? src1->data : params->wdata;\n    const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n    const int64_t nr0 = ne01;           // src0 rows\n    const int64_t nr1 = ne11*ne12*ne13; // src1 rows\n\n    //printf(\"nr0 = %lld, nr1 = %lld\\n\", nr0, nr1);\n\n    // distribute the thread work across the inner or outer loop based on which one is larger\n\n    const int64_t nth0 = nr0 > nr1 ? nth : 1; // parallelize by src0 rows\n    const int64_t nth1 = nr0 > nr1 ? 1 : nth; // parallelize by src1 rows\n\n    const int64_t ith0 = ith % nth0;\n    const int64_t ith1 = ith / nth0;\n\n    const int64_t dr0 = (nr0 + nth0 - 1)/nth0;\n    const int64_t dr1 = (nr1 + nth1 - 1)/nth1;\n\n    const int64_t ir010 = dr0*ith0;\n    const int64_t ir011 = MIN(ir010 + dr0, nr0);\n\n    const int64_t ir110 = dr1*ith1;\n    const int64_t ir111 = MIN(ir110 + dr1, nr1);\n\n    //printf(\"ir010 = %6lld, ir011 = %6lld, ir110 = %6lld, ir111 = %6lld\\n\", ir010, ir011, ir110, ir111);\n\n    // threads with no work simply yield (not sure if it helps)\n    if (ir010 >= ir011 || ir110 >= ir111) {\n        sched_yield();\n        return;\n    }\n\n    assert(ne12 % ne02 == 0);\n    assert(ne13 % ne03 == 0);\n\n    // block-tiling attempt\n    const int64_t blck_0 = 16;\n    const int64_t blck_1 = 16;\n\n    // attempt to reduce false-sharing (does not seem to make a difference)\n    float tmp[16];\n\n    for (int64_t iir1 = ir110; iir1 < ir111; iir1 += blck_1) {\n        for (int64_t iir0 = ir010; iir0 < ir011; iir0 += blck_0) {\n            for (int64_t ir1 = iir1; ir1 < iir1 + blck_1 && ir1 < ir111; ++ir1) {\n                const int64_t i13 = (ir1/(ne12*ne11));\n                const int64_t i12 = (ir1 - i13*ne12*ne11)/ne11;\n                const int64_t i11 = (ir1 - i13*ne12*ne11 - i12*ne11);\n\n                // broadcast src0 into src1\n                const int64_t i03 = i13/r3;\n                const int64_t i02 = i12/r2;\n\n                const int64_t i1 = i11;\n                const int64_t i2 = i12;\n                const int64_t i3 = i13;\n\n                const char * src0_row = (const char *) src0->data + (0 + i02*nb02 + i03*nb03);\n\n                // desc: when src1 is not a contiguous memory block we have to calculate the offset using the strides\n                //       if it is, then we have either copied the data to params->wdata and made it contiguous or we are using\n                //       the original src1 data pointer, so we should index using the indices directly\n                // TODO: this is a bit of a hack, we should probably have a better way to handle this\n                const char * src1_col = (const char *) wdata +\n                    (src1_cont || src1->type != vec_dot_type\n                     ? (i11      + i12*ne11 + i13*ne12*ne11)*row_size\n                     : (i11*nb11 + i12*nb12 + i13*nb13));\n\n                float * dst_col = (float *) ((char *) dst->data + (i1*nb1 + i2*nb2 + i3*nb3));\n\n                //for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir011; ++ir0) {\n                //    vec_dot(ne00, &dst_col[ir0], src0_row + ir0*nb01, src1_col);\n                //}\n\n                for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir011; ++ir0) {\n                    vec_dot(ne00, &tmp[ir0 - iir0], src0_row + ir0*nb01, src1_col);\n                }\n                memcpy(&dst_col[iir0], tmp, (MIN(iir0 + blck_0, ir011) - iir0)*sizeof(float));\n            }\n        }\n    }\n}\n\n// ggml_compute_forward_out_prod\n\nstatic void ggml_compute_forward_out_prod_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    // int64_t t0 = ggml_perf_time_us();\n    // UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    GGML_ASSERT(ne02 == ne12);\n    GGML_ASSERT(ne03 == ne13);\n    GGML_ASSERT(ne2  == ne12);\n    GGML_ASSERT(ne3  == ne13);\n\n    // we don't support permuted src0 or src1\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    // GGML_ASSERT(nb0 <= nb1);\n    // GGML_ASSERT(nb1 <= nb2);\n    // GGML_ASSERT(nb2 <= nb3);\n\n    GGML_ASSERT(ne0 == ne00);\n    GGML_ASSERT(ne1 == ne10);\n    GGML_ASSERT(ne2 == ne02);\n    GGML_ASSERT(ne3 == ne03);\n\n    // nb01 >= nb00 - src0 is not transposed\n    //   compute by src0 rows\n\n    // TODO: #if defined(GGML_USE_CUBLAS) ggml_cuda_out_prod\n    // TODO: #if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS) || defined(GGML_USE_CLBLAST)\n\n    if (params->type == GGML_TASK_INIT) {\n        ggml_vec_set_f32(ne0*ne1*ne2*ne3, dst->data, 0);\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // dst[:,:,:,:] = 0\n    // for i2,i3:\n    //   for i1:\n    //     for i01:\n    //       for i0:\n    //         dst[i0,i1,i2,i3] += src0[i0,i01,i2,i3] * src1[i1,i01,i2,i3]\n\n    // parallelize by last three dimensions\n\n    // total rows in dst\n    const int64_t nr = ne1*ne2*ne3;\n\n    // rows per thread\n    const int64_t dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int64_t ir0 = dr*ith;\n    const int64_t ir1 = MIN(ir0 + dr, nr);\n\n    // block-tiling attempt\n    const int64_t blck_0 = MAX(GGML_VEC_MAD_UNROLL, 32);\n    const int64_t blck_1 = 16;\n\n    for (int64_t bir = ir0; bir < ir1; bir += blck_1) {\n        const int64_t bir1 = MIN(bir + blck_1, ir1);\n        for (int64_t bi01 = 0; bi01 < ne01; bi01 += blck_0) {\n            const int64_t bne01 = MIN(bi01 + blck_0, ne01);\n            for (int64_t ir = bir; ir < bir1; ++ir) {\n                // dst indices\n                const int64_t i3 = ir/(ne2*ne1);\n                const int64_t i2 = (ir - i3*ne2*ne1)/ne1;\n                const int64_t i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n                const int64_t i02 = i2;\n                const int64_t i03 = i3;\n\n                //const int64_t i10 = i1;\n                const int64_t i12 = i2;\n                const int64_t i13 = i3;\n\n#if GGML_VEC_MAD_UNROLL > 2\n                const int64_t bne01_unroll = bne01 - (bne01 % GGML_VEC_MAD_UNROLL);\n                for (int64_t i01 = bi01; i01 < bne01_unroll; i01 += GGML_VEC_MAD_UNROLL) {\n                    const int64_t i11 = i01;\n\n                    float * s0 = (float *) ((char *) src0->data + (          i01*nb01 + i02*nb02 + i03*nb03));\n                    float * s1 = (float *) ((char *) src1->data + (i1*nb10 + i11*nb11 + i12*nb12 + i13*nb13));\n                    float * d  = (float *) ((char *)  dst->data + (          i1*nb1 + i2*nb2 + i3*nb3));\n\n                    ggml_vec_mad_f32_unroll(ne0, nb01, nb11, d, s0, s1);\n                }\n                for (int64_t i01 = bne01_unroll; i01 < bne01; ++i01) {\n                    const int64_t i11 = i01;\n\n                    float * s0 = (float *) ((char *) src0->data + (          i01*nb01 + i02*nb02 + i03*nb03));\n                    float * s1 = (float *) ((char *) src1->data + (i1*nb10 + i11*nb11 + i12*nb12 + i13*nb13));\n                    float * d  = (float *) ((char *)  dst->data + (          i1*nb1 + i2*nb2 + i3*nb3));\n\n                    ggml_vec_mad_f32(ne0, d, s0, *s1);\n                }\n#else\n                for (int64_t i01 = bi01; i01 < bne01; ++i01) {\n                    const int64_t i11 = i01;\n\n                    float * s0 = (float *) ((char *) src0->data + (          i01*nb01 + i02*nb02 + i03*nb03));\n                    float * s1 = (float *) ((char *) src1->data + (i1*nb10 + i11*nb11 + i12*nb12 + i13*nb13));\n                    float * d  = (float *) ((char *)  dst->data + (          i1*nb1 + i2*nb2 + i3*nb3));\n\n                    ggml_vec_mad_f32(ne0, d, s0, *s1);\n                }\n#endif\n            }\n        }\n    }\n\n    //int64_t t1 = ggml_perf_time_us();\n    //static int64_t acc = 0;\n    //acc += t1 - t0;\n    //if (t1 - t0 > 10) {\n    //    printf(\"\\n\");\n    //    printf(\"ne00 = %5d, ne01 = %5d, ne02 = %5d, ne03 = %5d\\n\", ne00, ne01, ne02, ne03);\n    //    printf(\"nb00 = %5d, nb01 = %5d, nb02 = %5d, nb03 = %5d\\n\", nb00, nb01, nb02, nb03);\n    //    printf(\"ne10 = %5d, ne11 = %5d, ne12 = %5d, ne13 = %5d\\n\", ne10, ne11, ne12, ne13);\n    //    printf(\"nb10 = %5d, nb11 = %5d, nb12 = %5d, nb13 = %5d\\n\", nb10, nb11, nb12, nb13);\n\n    //    printf(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX task %d/%d: %d us, acc = %d\\n\", ith, nth, (int) (t1 - t0), (int) acc);\n    //}\n}\n\nstatic void ggml_compute_forward_out_prod_q_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    // int64_t t0 = ggml_perf_time_us();\n    // UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const enum ggml_type type = src0->type;\n    ggml_to_float_t const dequantize_row_q = type_traits[type].to_float;\n\n    GGML_ASSERT(ne02 == ne12);\n    GGML_ASSERT(ne03 == ne13);\n    GGML_ASSERT(ne2  == ne12);\n    GGML_ASSERT(ne3  == ne13);\n\n    // we don't support permuted src0 dim0\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n\n    // dst dim0 cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    // GGML_ASSERT(nb0 <= nb1);\n    // GGML_ASSERT(nb1 <= nb2);\n    // GGML_ASSERT(nb2 <= nb3);\n\n    GGML_ASSERT(ne0 == ne00);\n    GGML_ASSERT(ne1 == ne10);\n    GGML_ASSERT(ne2 == ne02);\n    GGML_ASSERT(ne3 == ne03);\n\n    // nb01 >= nb00 - src0 is not transposed\n    //   compute by src0 rows\n\n    // TODO: #if defined(GGML_USE_CUBLAS) ggml_cuda_out_prod\n    // TODO: #if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS) || defined(GGML_USE_CLBLAST)\n\n    if (params->type == GGML_TASK_INIT) {\n        ggml_vec_set_f32(ne0*ne1*ne2*ne3, dst->data, 0);\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // parallelize by last three dimensions\n\n    // total rows in dst\n    const int64_t nr = ne1*ne2*ne3;\n\n    // rows per thread\n    const int64_t dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int64_t ir0 = dr*ith;\n    const int64_t ir1 = MIN(ir0 + dr, nr);\n\n    // dst[:,:,:,:] = 0\n    // for i2,i3:\n    //   for i1:\n    //     for i01:\n    //       for i0:\n    //         dst[i0,i1,i2,i3] += src0[i0,i01,i2,i3] * src1[i1,i01,i2,i3]\n\n    float * wdata = (float *) params->wdata + (ne0 + CACHE_LINE_SIZE_F32) * ith;\n\n    for (int64_t ir = ir0; ir < ir1; ++ir) {\n        // dst indices\n        const int64_t i3 = ir/(ne2*ne1);\n        const int64_t i2 = (ir - i3*ne2*ne1)/ne1;\n        const int64_t i1 = (ir - i3*ne2*ne1 - i2*ne1);\n\n        const int64_t i02 = i2;\n        const int64_t i03 = i3;\n\n        //const int64_t i10 = i1;\n        const int64_t i12 = i2;\n        const int64_t i13 = i3;\n\n        for (int64_t i01 = 0; i01 < ne01; ++i01) {\n            const int64_t i11 = i01;\n\n            float * s0 = (float *) ((char *) src0->data + (          i01*nb01 + i02*nb02 + i03*nb03));\n            float * s1 = (float *) ((char *) src1->data + (i1*nb10 + i11*nb11 + i12*nb12 + i13*nb13));\n            float * d  = (float *) ((char *)  dst->data + (          i1*nb1 + i2*nb2 + i3*nb3));\n\n            dequantize_row_q(s0, wdata, ne0);\n            ggml_vec_mad_f32(ne0, d, wdata, *s1);\n        }\n    }\n\n    //int64_t t1 = ggml_perf_time_us();\n    //static int64_t acc = 0;\n    //acc += t1 - t0;\n    //if (t1 - t0 > 10) {\n    //    printf(\"\\n\");\n    //    printf(\"ne00 = %5d, ne01 = %5d, ne02 = %5d, ne03 = %5d\\n\", ne00, ne01, ne02, ne03);\n    //    printf(\"nb00 = %5d, nb01 = %5d, nb02 = %5d, nb03 = %5d\\n\", nb00, nb01, nb02, nb03);\n    //    printf(\"ne10 = %5d, ne11 = %5d, ne12 = %5d, ne13 = %5d\\n\", ne10, ne11, ne12, ne13);\n    //    printf(\"nb10 = %5d, nb11 = %5d, nb12 = %5d, nb13 = %5d\\n\", nb10, nb11, nb12, nb13);\n\n    //    printf(\"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX task %d/%d: %d us, acc = %d\\n\", ith, nth, (int) (t1 - t0), (int) acc);\n    //}\n}\n\nstatic void ggml_compute_forward_out_prod(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n            {\n                ggml_compute_forward_out_prod_q_f32(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F16:\n            {\n                GGML_ASSERT(false); // todo\n                // ggml_compute_forward_out_prod_f16_f32(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_out_prod_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_scale\n\nstatic void ggml_compute_forward_scale_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous(src0));\n    GGML_ASSERT(ggml_is_contiguous(dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_is_scalar(src1));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // scale factor\n    const float v = *(float *) src1->data;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nrows(src0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    const size_t nb01 = src0->nb[1];\n\n    const size_t nb1 = dst->nb[1];\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        if (dst->data != src0->data) {\n            // src0 is same shape as dst => same indices\n            memcpy((char *)dst->data + i1*nb1, (char *)src0->data + i1*nb01, nc * sizeof(float));\n        }\n        ggml_vec_scale_f32(nc, (float *) ((char *) dst->data + i1*nb1), v);\n    }\n}\n\nstatic void ggml_compute_forward_scale(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_scale_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_set\n\nstatic void ggml_compute_forward_set_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_is_contiguous(dst) && ggml_is_contiguous(src0));\n\n    // view src0 and dst with these strides and data offset inbytes during set\n    // nb0 is implicitely element_size because src0 and dst are contiguous\n    size_t nb1     = ((int32_t *) dst->op_params)[0];\n    size_t nb2     = ((int32_t *) dst->op_params)[1];\n    size_t nb3     = ((int32_t *) dst->op_params)[2];\n    size_t offset  = ((int32_t *) dst->op_params)[3];\n    bool   inplace = (bool) ((int32_t *) dst->op_params)[4];\n\n    if (!inplace && (params->type == GGML_TASK_INIT)) {\n        // memcpy needs to be synchronized across threads to avoid race conditions.\n        // => do it in INIT phase\n        memcpy(\n            ((char *)  dst->data),\n            ((char *) src0->data),\n            ggml_nbytes(dst));\n    }\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr = ggml_nrows(src1);\n    const int nc = src1->ne[0];\n\n    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb)\n\n    // src0 and dst as viewed during set\n    const size_t nb0 = ggml_element_size(src0);\n\n    const int im0 = (ne10 == 0 ? 0 : ne10-1);\n    const int im1 = (ne11 == 0 ? 0 : ne11-1);\n    const int im2 = (ne12 == 0 ? 0 : ne12-1);\n    const int im3 = (ne13 == 0 ? 0 : ne13-1);\n\n    GGML_ASSERT(offset + im0*nb0  + im1*nb1  + im2*nb2  + im3*nb3  <= ggml_nbytes(dst));\n\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // src0 and dst are viewed with shape of src1 and offset\n        // => same indices\n        const int i3 = ir/(ne12*ne11);\n        const int i2 = (ir - i3*ne12*ne11)/ne11;\n        const int i1 = (ir - i3*ne12*ne11 - i2*ne11);\n\n        ggml_vec_cpy_f32(nc,\n                (float *) ((char *)  dst->data + i3*nb3  + i2*nb2  + i1*nb1  + offset),\n                (float *) ((char *) src1->data + i3*nb13 + i2*nb12 + i1*nb11));\n    }\n}\n\nstatic void ggml_compute_forward_set(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_set_f32(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F16:\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n        case GGML_TYPE_Q8_1:\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_cpy\n\nstatic void ggml_compute_forward_cpy(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    ggml_compute_forward_dup(params, src0, dst);\n}\n\n// ggml_compute_forward_cont\n\nstatic void ggml_compute_forward_cont(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    ggml_compute_forward_dup(params, src0, dst);\n}\n\n// ggml_compute_forward_reshape\n\nstatic void ggml_compute_forward_reshape(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    // NOP\n    UNUSED(params);\n    UNUSED(src0);\n    UNUSED(dst);\n}\n\n// ggml_compute_forward_view\n\nstatic void ggml_compute_forward_view(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0) {\n    // NOP\n    UNUSED(params);\n    UNUSED(src0);\n}\n\n// ggml_compute_forward_permute\n\nstatic void ggml_compute_forward_permute(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0) {\n    // NOP\n    UNUSED(params);\n    UNUSED(src0);\n}\n\n// ggml_compute_forward_transpose\n\nstatic void ggml_compute_forward_transpose(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0) {\n    // NOP\n    UNUSED(params);\n    UNUSED(src0);\n}\n\n// ggml_compute_forward_get_rows\n\nstatic void ggml_compute_forward_get_rows_q(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nelements(src1);\n    const enum ggml_type type = src0->type;\n    ggml_to_float_t const dequantize_row_q = type_traits[type].to_float;\n\n    assert( dst->ne[0] == nc);\n    assert( dst->ne[1] == nr);\n    assert(src0->nb[0] == ggml_type_size(type));\n\n    for (int i = 0; i < nr; ++i) {\n        const int r = ((int32_t *) src1->data)[i];\n\n        dequantize_row_q(\n                (const void *) ((char *) src0->data + r*src0->nb[1]),\n                     (float *) ((char *)  dst->data + i*dst->nb[1]), nc);\n    }\n}\n\nstatic void ggml_compute_forward_get_rows_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nelements(src1);\n\n    assert( dst->ne[0] == nc);\n    assert( dst->ne[1] == nr);\n    assert(src0->nb[0] == sizeof(ggml_fp16_t));\n\n    for (int i = 0; i < nr; ++i) {\n        const int r = ((int32_t *) src1->data)[i];\n\n        for (int j = 0; j < nc; ++j) {\n            ggml_fp16_t v = ((ggml_fp16_t *) ((char *) src0->data + r*src0->nb[1]))[j];\n            ((float *) ((char *)  dst->data + i*dst->nb[1]))[j] = GGML_FP16_TO_FP32(v);\n        }\n    }\n}\n\nstatic void ggml_compute_forward_get_rows_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nelements(src1);\n\n    assert( dst->ne[0] == nc);\n    assert( dst->ne[1] == nr);\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < nr; ++i) {\n        const int r = ((int32_t *) src1->data)[i];\n\n        ggml_vec_cpy_f32(nc,\n                (float *) ((char *)  dst->data + i*dst->nb[1]),\n                (float *) ((char *) src0->data + r*src0->nb[1]));\n    }\n}\n\nstatic void ggml_compute_forward_get_rows(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n        case GGML_TYPE_Q8_1:\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n            {\n                ggml_compute_forward_get_rows_q(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_get_rows_f16(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_get_rows_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n\n    //static bool first = true;\n    //printf(\"ne0 = %d, ne1 = %d, ne2 = %d\\n\", dst->ne[0], dst->ne[1], dst->ne[2]);\n    //if (first) {\n    //    first = false;\n    //} else {\n    //    for (int k = 0; k < dst->ne[1]; ++k) {\n    //        for (int j = 0; j < dst->ne[0]/16; ++j) {\n    //            for (int i = 0; i < 16; ++i) {\n    //                printf(\"%8.4f \", ((float *) dst->data)[k*dst->ne[0] + j*16 + i]);\n    //            }\n    //            printf(\"\\n\");\n    //        }\n    //        printf(\"\\n\");\n    //    }\n    //    printf(\"\\n\");\n    //    exit(0);\n    //}\n}\n\n// ggml_compute_forward_get_rows_back\n\nstatic void ggml_compute_forward_get_rows_back_f32_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    GGML_ASSERT(params->ith == 0);\n    GGML_ASSERT(ggml_is_contiguous(dst));\n\n    // ggml_compute_forward_dup_same_cont(params, opt0, dst);\n\n    if (params->type == GGML_TASK_INIT) {\n        memset(dst->data, 0, ggml_nbytes(dst));\n    }\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nelements(src1);\n\n    GGML_ASSERT( dst->ne[0] == nc);\n    GGML_ASSERT(src0->nb[0] == sizeof(ggml_fp16_t));\n\n    for (int i = 0; i < nr; ++i) {\n        const int r = ((int32_t *) src1->data)[i];\n\n        for (int j = 0; j < nc; ++j) {\n            ggml_fp16_t v = ((ggml_fp16_t *) ((char *) src0->data + i*src0->nb[1]))[j];\n            ((float *) ((char *) dst->data + r*dst->nb[1]))[j] += GGML_FP16_TO_FP32(v);\n        }\n    }\n}\n\nstatic void ggml_compute_forward_get_rows_back_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    GGML_ASSERT(params->ith == 0);\n    GGML_ASSERT(ggml_is_contiguous(dst));\n\n    // ggml_compute_forward_dup_same_cont(params, opt0, dst);\n\n    if (params->type == GGML_TASK_INIT) {\n        memset(dst->data, 0, ggml_nbytes(dst));\n    }\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nelements(src1);\n\n    GGML_ASSERT( dst->ne[0] == nc);\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < nr; ++i) {\n        const int r = ((int32_t *) src1->data)[i];\n\n        ggml_vec_add_f32(nc,\n                (float *) ((char *)  dst->data + r*dst->nb[1]),\n                (float *) ((char *)  dst->data + r*dst->nb[1]),\n                (float *) ((char *) src0->data + i*src0->nb[1]));\n    }\n}\n\nstatic void ggml_compute_forward_get_rows_back(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_get_rows_back_f32_f16(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_get_rows_back_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n\n    //static bool first = true;\n    //printf(\"ne0 = %d, ne1 = %d, ne2 = %d\\n\", dst->ne[0], dst->ne[1], dst->ne[2]);\n    //if (first) {\n    //    first = false;\n    //} else {\n    //    for (int k = 0; k < dst->ne[1]; ++k) {\n    //        for (int j = 0; j < dst->ne[0]/16; ++j) {\n    //            for (int i = 0; i < 16; ++i) {\n    //                printf(\"%8.4f \", ((float *) dst->data)[k*dst->ne[0] + j*16 + i]);\n    //            }\n    //            printf(\"\\n\");\n    //        }\n    //        printf(\"\\n\");\n    //    }\n    //    printf(\"\\n\");\n    //    exit(0);\n    //}\n}\n\n// ggml_compute_forward_diag\n\nstatic void ggml_compute_forward_diag_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // TODO: handle transposed/permuted matrices\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    GGML_ASSERT(ne00 == ne0);\n    GGML_ASSERT(ne00 == ne1);\n    GGML_ASSERT(ne01 == 1);\n    GGML_ASSERT(ne02 == ne2);\n    GGML_ASSERT(ne03 == ne3);\n\n    GGML_ASSERT(nb00 == sizeof(float));\n    GGML_ASSERT(nb0  == sizeof(float));\n\n    for (int i3 = 0; i3 < ne3; i3++) {\n        for (int i2 = 0; i2 < ne2; i2++) {\n            for (int i1 = 0; i1 < ne1; i1++) {\n                float * d = (float *)((char *)  dst->data + i3*nb3  + i2*nb2 + i1*nb1);\n                float * s = (float *)((char *) src0->data + i3*nb03 + i2*nb02);\n                for (int i0 = 0; i0 < i1; i0++) {\n                    d[i0] = 0;\n                }\n                d[i1] = s[i1];\n                for (int i0 = i1+1; i0 < ne0; i0++) {\n                    d[i0] = 0;\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_diag(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_diag_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_diag_mask_inf\n\nstatic void ggml_compute_forward_diag_mask_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst,\n        const float value) {\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int  n_past  = ((int32_t *) dst->op_params)[0];\n    const bool inplace = src0->data == dst->data;\n\n    GGML_ASSERT(n_past >= 0);\n\n    if (!inplace && (params->type == GGML_TASK_INIT)) {\n        // memcpy needs to be synchronized across threads to avoid race conditions.\n        // => do it in INIT phase\n        GGML_ASSERT(ggml_nelements(dst) == ggml_nelements(src0));\n        GGML_ASSERT(ggml_is_contiguous(dst) && ggml_is_contiguous(src0));\n        memcpy(\n            ((char *)  dst->data),\n            ((char *) src0->data),\n            ggml_nbytes(dst));\n    }\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // TODO: handle transposed/permuted matrices\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n    const int nr = src0->ne[1];\n    const int nz = n/nr;\n\n    GGML_ASSERT( dst->nb[0] == sizeof(float));\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    for (int k = 0; k < nz; k++) {\n        for (int j = ith; j < nr; j += nth) {\n            for (int i = n_past; i < nc; i++) {\n                if (i > n_past + j) {\n                    *(float *)((char *) dst->data + k*dst->nb[2] + j*dst->nb[1] + i*dst->nb[0]) = value;\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_diag_mask_inf(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_diag_mask_f32(params, src0, dst, -INFINITY);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\nstatic void ggml_compute_forward_diag_mask_zero(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_diag_mask_f32(params, src0, dst, 0);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_soft_max\n\nstatic void ggml_compute_forward_soft_max_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous(src0));\n    GGML_ASSERT(ggml_is_contiguous(dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // TODO: handle transposed/permuted matrices\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nrows(src0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        float *sp = (float *)((char *) src0->data + i1*src0->nb[1]);\n        float *dp = (float *)((char *)  dst->data +  i1*dst->nb[1]);\n\n#ifndef NDEBUG\n        for (int i = 0; i < nc; ++i) {\n            //printf(\"p[%d] = %f\\n\", i, p[i]);\n            assert(!isnan(sp[i]));\n        }\n#endif\n\n        float max = -INFINITY;\n        ggml_vec_max_f32(nc, &max, sp);\n\n        ggml_float sum = 0.0;\n\n        uint16_t scvt;\n        for (int i = 0; i < nc; i++) {\n            if (sp[i] == -INFINITY) {\n                dp[i] = 0.0f;\n            } else {\n                // const float val = (sp[i] == -INFINITY) ? 0.0 : exp(sp[i] - max);\n                ggml_fp16_t s = GGML_FP32_TO_FP16(sp[i] - max);\n                memcpy(&scvt, &s, sizeof(scvt));\n                const float val = GGML_FP16_TO_FP32(ggml_table_exp_f16[scvt]);\n                sum += (ggml_float)val;\n                dp[i] = val;\n            }\n        }\n\n        assert(sum > 0.0);\n\n        sum = 1.0/sum;\n        ggml_vec_scale_f32(nc, dp, sum);\n\n#ifndef NDEBUG\n        for (int i = 0; i < nc; ++i) {\n            assert(!isnan(dp[i]));\n            assert(!isinf(dp[i]));\n        }\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_soft_max(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_soft_max_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_soft_max_back\n\nstatic void ggml_compute_forward_soft_max_back_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous(src0));\n    GGML_ASSERT(ggml_is_contiguous(src1));\n    GGML_ASSERT(ggml_is_contiguous(dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n    GGML_ASSERT(ggml_are_same_shape(src1, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // TODO: handle transposed/permuted matrices\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nc = src0->ne[0];\n    const int nr = ggml_nrows(src0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        float *dy = (float *)((char *) src0->data + i1*src0->nb[1]);\n        float *y  = (float *)((char *) src1->data + i1*src1->nb[1]);\n        float *dx = (float *)((char *) dst->data  + i1*dst->nb[1]);\n\n#ifndef NDEBUG\n        for (int i = 0; i < nc; ++i) {\n            //printf(\"p[%d] = %f\\n\", i, p[i]);\n            assert(!isnan(dy[i]));\n            assert(!isnan(y[i]));\n        }\n#endif\n        // Jii = yi - yi*yi\n        // Jij = -yi*yj\n        // J = diag(y)-y.T*y\n        // dx = J * dy\n        // dxk = sum_i(Jki * dyi)\n        // dxk = sum_i(-yk*yi * dyi) - (-yk*yk)*dyk + (yk - yk*yk)*dyk\n        // dxk = sum_i(-yk*yi * dyi) + yk*yk*dyk + yk*dyk - yk*yk*dyk\n        // dxk = sum_i(-yk*yi * dyi) + yk*dyk\n        // dxk = -yk * sum_i(yi * dyi) + yk*dyk\n        // dxk = -yk * dot(y, dy) + yk*dyk\n        // dxk = yk * (- dot(y, dy) + dyk)\n        // dxk = yk * (dyk - dot(y, dy))\n        //\n        // post-order:\n        // dot_y_dy := dot(y, dy)\n        // dx := dy\n        // dx := dx - dot_y_dy\n        // dx := dx * y\n\n        // linear runtime, no additional memory\n        float dot_y_dy = 0;\n        ggml_vec_dot_f32 (nc, &dot_y_dy, y, dy);\n        ggml_vec_cpy_f32 (nc, dx, dy);\n        ggml_vec_acc1_f32(nc, dx, -dot_y_dy);\n        ggml_vec_mul_f32 (nc, dx, dx, y);\n\n#ifndef NDEBUG\n        for (int i = 0; i < nc; ++i) {\n            assert(!isnan(dx[i]));\n            assert(!isinf(dx[i]));\n        }\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_soft_max_back(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_soft_max_back_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_alibi\n\nstatic void ggml_compute_forward_alibi_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    //const int n_past = ((int32_t *) dst->op_params)[0];\n    const int n_head = ((int32_t *) dst->op_params)[1];\n    float max_bias;\n    memcpy(&max_bias, (int32_t *) dst->op_params + 2, sizeof(float));\n\n    const int64_t ne0 = src0->ne[0]; // all_seq_len = n_past + ne1\n    const int64_t ne1 = src0->ne[1]; // seq_len_without_past\n    const int64_t ne2 = src0->ne[2]; // n_head -> this is k\n    //const int64_t ne3 = src0->ne[3]; // 1 -> bsz\n\n    const int64_t n  = ggml_nrows(src0);\n    const int64_t ne2_ne3 = n/ne1; // ne2*ne3\n\n    const size_t nb0 = src0->nb[0];\n    const size_t nb1 = src0->nb[1];\n    const size_t nb2 = src0->nb[2];\n    //const int nb3 = src0->nb[3];\n\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(n_head == ne2);\n\n    // add alibi to src0 (KQ_scaled)\n    const int n_heads_log2_floor = 1 << (int) floor(log2(n_head));\n\n    const float m0 = powf(2.0f, -(max_bias) / n_heads_log2_floor);\n    const float m1 = powf(2.0f, -(max_bias / 2.0f) / n_heads_log2_floor);\n\n    for (int64_t i = 0; i < ne0; i++) {\n        for (int64_t j = 0; j < ne1; j++) {\n            for (int64_t k = 0; k < ne2_ne3; k++) {\n                float * const src = (float *)((char *) src0->data + i*nb0 + j*nb1 + k*nb2);\n                float *      pdst = (float *)((char *)  dst->data + i*nb0 + j*nb1 + k*nb2);\n\n                // TODO: k*nb2 or k*nb3\n\n                float m_k;\n\n                if (k < n_heads_log2_floor) {\n                    m_k = powf(m0, k + 1);\n                } else {\n                    m_k = powf(m1, 2 * (k - n_heads_log2_floor) + 1);\n                }\n\n                pdst[0] = i * m_k + src[0];\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_alibi_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    //const int n_past = ((int32_t *) dst->op_params)[0];\n    const int n_head = ((int32_t *) dst->op_params)[1];\n    float max_bias;\n    memcpy(&max_bias, (int32_t *) dst->op_params + 2, sizeof(float));\n\n    const int ne0 = src0->ne[0]; // all_seq_len = n_past + ne1\n    const int ne1 = src0->ne[1]; // seq_len_without_past\n    const int ne2 = src0->ne[2]; // n_head -> this is k\n    //const int ne3 = src0->ne[3]; // 1 -> bsz\n\n    const int n  = ggml_nrows(src0);\n    const int ne2_ne3 = n/ne1; // ne2*ne3\n\n    const int nb0 = src0->nb[0];\n    const int nb1 = src0->nb[1];\n    const int nb2 = src0->nb[2];\n    //const int nb3 = src0->nb[3];\n\n    GGML_ASSERT(nb0 == sizeof(ggml_fp16_t));\n    //GGML_ASSERT(ne1 + n_past == ne0); (void) n_past;\n    GGML_ASSERT(n_head == ne2);\n\n    // add alibi to src0 (KQ_scaled)\n    const int n_heads_log2_floor = 1 << (int) floor(log2(n_head));\n\n    const float m0 = powf(2.0f, -(max_bias) / n_heads_log2_floor);\n    const float m1 = powf(2.0f, -(max_bias / 2.0f) / n_heads_log2_floor);\n\n    for (int i = 0; i < ne0; i++) {\n        for (int j = 0; j < ne1; j++) {\n            for (int k = 0; k < ne2_ne3; k++) {\n                ggml_fp16_t * const src  = (ggml_fp16_t *)((char *) src0->data + i*nb0 + j*nb1 + k*nb2);\n                      float *      pdst  =       (float *)((char *)  dst->data + i*nb0 + j*nb1 + k*nb2);\n\n                // TODO: k*nb2 or k*nb3\n\n                float m_k;\n\n                if (k < n_heads_log2_floor) {\n                    m_k = powf(m0, k + 1);\n                } else {\n                    m_k = powf(m1, 2 * (k - n_heads_log2_floor) + 1);\n                }\n\n                // we return F32\n                pdst[0] = i * m_k + GGML_FP16_TO_FP32(src[0]);\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_alibi(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_alibi_f16(params, src0, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_alibi_f32(params, src0, dst);\n            } break;\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n        case GGML_TYPE_Q8_1:\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n        case GGML_TYPE_Q8_K:\n        case GGML_TYPE_I8:\n        case GGML_TYPE_I16:\n        case GGML_TYPE_I32:\n        case GGML_TYPE_COUNT:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_clamp\n\nstatic void ggml_compute_forward_clamp_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    float min;\n    float max;\n    memcpy(&min, (float *) dst->op_params + 0, sizeof(float));\n    memcpy(&max, (float *) dst->op_params + 1, sizeof(float));\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    const size_t nb00 = src0->nb[0];\n    const size_t nb01 = src0->nb[1];\n\n    const size_t nb0 = dst->nb[0];\n    const size_t nb1 = dst->nb[1];\n\n    GGML_ASSERT( nb0 == sizeof(float));\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    for (int j = ith; j < n; j += nth) {\n        float * dst_ptr  = (float *) ((char *)  dst->data + j*nb1);\n        float * src0_ptr = (float *) ((char *) src0->data + j*nb01);\n\n        for (int i = 0; i < nc; i++) {\n            dst_ptr[i] = MAX(MIN(src0_ptr[i], max), min);\n        }\n    }\n}\n\nstatic void ggml_compute_forward_clamp(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_clamp_f32(params, src0, dst);\n            } break;\n        case GGML_TYPE_F16:\n        case GGML_TYPE_Q4_0:\n        case GGML_TYPE_Q4_1:\n        case GGML_TYPE_Q5_0:\n        case GGML_TYPE_Q5_1:\n        case GGML_TYPE_Q8_0:\n        case GGML_TYPE_Q8_1:\n        case GGML_TYPE_Q2_K:\n        case GGML_TYPE_Q3_K:\n        case GGML_TYPE_Q4_K:\n        case GGML_TYPE_Q5_K:\n        case GGML_TYPE_Q6_K:\n        case GGML_TYPE_Q8_K:\n        case GGML_TYPE_I8:\n        case GGML_TYPE_I16:\n        case GGML_TYPE_I32:\n        case GGML_TYPE_COUNT:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_rope\n\nstatic float rope_yarn_ramp(const float low, const float high, const int i0) {\n    const float y = (i0 / 2 - low) / MAX(0.001f, high - low);\n    return 1 - MIN(1, MAX(0, y));\n}\n\n// YaRN algorithm based on LlamaYaRNScaledRotaryEmbedding.py from https://github.com/jquesnelle/yarn\n// MIT licensed. Copyright (c) 2023 Jeffrey Quesnelle and Bowen Peng.\nstatic void rope_yarn(\n    float theta_extrap, float freq_scale, float corr_dims[2], int64_t i0, float ext_factor, float mscale,\n    float * cos_theta, float * sin_theta\n) {\n    // Get n-d rotational scaling corrected for extrapolation\n    float theta_interp = freq_scale * theta_extrap;\n    float theta = theta_interp;\n    if (ext_factor != 0.0f) {\n        float ramp_mix = rope_yarn_ramp(corr_dims[0], corr_dims[1], i0) * ext_factor;\n        theta = theta_interp * (1 - ramp_mix) + theta_extrap * ramp_mix;\n\n        // Get n-d magnitude scaling corrected for interpolation\n        mscale *= 1.0f + 0.1f * logf(1.0f / freq_scale);\n    }\n    *cos_theta = cosf(theta) * mscale;\n    *sin_theta = sinf(theta) * mscale;\n}\n\n// Apparently solving `n_rot = 2pi * x * base^((2 * max_pos_emb) / n_dims)` for x, we get\n// `corr_dim(n_rot) = n_dims * log(max_pos_emb / (n_rot * 2pi)) / (2 * log(base))`\nstatic float ggml_rope_yarn_corr_dim(int n_dims, int n_orig_ctx, float n_rot, float base) {\n    return n_dims * logf(n_orig_ctx / (n_rot * 2 * (float)M_PI)) / (2 * logf(base));\n}\n\nvoid ggml_rope_yarn_corr_dims(\n    int n_dims, int n_orig_ctx, float freq_base, float beta_fast, float beta_slow, float dims[2]\n) {\n    // start and end correction dims\n    dims[0] = MAX(0,         floorf(ggml_rope_yarn_corr_dim(n_dims, n_orig_ctx, beta_fast, freq_base)));\n    dims[1] = MIN(n_dims - 1, ceilf(ggml_rope_yarn_corr_dim(n_dims, n_orig_ctx, beta_slow, freq_base)));\n}\n\nstatic void ggml_compute_forward_rope_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst,\n        const bool forward) {\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    float freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow;\n\n    // these two only relevant for xPos RoPE:\n    float xpos_base;\n    bool  xpos_down;\n\n    //const int n_past     = ((int32_t *) dst->op_params)[0];\n    const int n_dims     = ((int32_t *) dst->op_params)[1];\n    const int mode       = ((int32_t *) dst->op_params)[2];\n    const int n_ctx      = ((int32_t *) dst->op_params)[3];\n    const int n_orig_ctx = ((int32_t *) dst->op_params)[4];\n\n    memcpy(&freq_base,   (int32_t *) dst->op_params +  5, sizeof(float));\n    memcpy(&freq_scale,  (int32_t *) dst->op_params +  6, sizeof(float));\n    memcpy(&ext_factor,  (int32_t *) dst->op_params +  7, sizeof(float));\n    memcpy(&attn_factor, (int32_t *) dst->op_params +  8, sizeof(float));\n    memcpy(&beta_fast,   (int32_t *) dst->op_params +  9, sizeof(float));\n    memcpy(&beta_slow,   (int32_t *) dst->op_params + 10, sizeof(float));\n    memcpy(&xpos_base,   (int32_t *) dst->op_params + 11, sizeof(float));\n    memcpy(&xpos_down,   (int32_t *) dst->op_params + 12, sizeof(bool));\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    //printf(\"ne0: %d, ne1: %d, ne2: %d, ne3: %d\\n\", ne0, ne1, ne2, ne3);\n    //printf(\"n_past = %d, ne2 = %d\\n\", n_past, ne2);\n\n    GGML_ASSERT(nb00 == sizeof(float));\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr = ggml_nrows(dst);\n\n    GGML_ASSERT(n_dims <= ne0);\n    GGML_ASSERT(n_dims % 2 == 0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    // row index used to determine which thread to use\n    int ir = 0;\n\n    const float theta_scale = powf(freq_base, -2.0f/n_dims);\n    const float inv_ndims = -1.f/n_dims;\n    float corr_dims[2];\n    ggml_rope_yarn_corr_dims(n_dims, n_orig_ctx, freq_base, beta_fast, beta_slow, corr_dims);\n\n    const bool is_neox = mode & 2;\n    const bool is_glm  = mode & 4;\n\n    // backward process uses inverse rotation by cos and sin.\n    // cos and sin build a rotation matrix, where the inverse is the transpose.\n    // this essentially just switches the sign of sin.\n    const float sin_sign = forward ? 1.0f : -1.0f;\n\n    const int32_t * pos = (const int32_t *) src1->data;\n\n    for (int64_t i3 = 0; i3 < ne3; i3++) {\n        for (int64_t i2 = 0; i2 < ne2; i2++) {\n            const int64_t p = pos[i2];\n            for (int64_t i1 = 0; i1 < ne1; i1++) {\n                if (ir++ < ir0) continue;\n                if (ir   > ir1) break;\n\n                float theta_base = (float)p;\n\n                if (is_glm) {\n                    theta_base = MIN(p, n_ctx - 2);\n                    float block_theta = MAX(p - (n_ctx - 2), 0);\n                    for (int64_t i0 = 0; i0 < ne0 / 4; i0++) {\n                        const float cos_theta = cosf(theta_base);\n                        const float sin_theta = sinf(theta_base) * sin_sign;\n                        const float cos_block_theta = cosf(block_theta);\n                        const float sin_block_theta = sinf(block_theta) * sin_sign;\n\n                        theta_base *= theta_scale;\n                        block_theta *= theta_scale;\n\n                        const float * const src = (float *)((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01 + i0*nb00);\n                              float * dst_data  = (float *)((char *)  dst->data +  i3*nb3 + i2*nb2  + i1*nb1  + i0*nb0);\n\n                        const float x0 = src[0];\n                        const float x1 = src[n_dims/2];\n                        const float x2 = src[n_dims];\n                        const float x3 = src[n_dims/2*3];\n\n                        dst_data[0]          = x0*cos_theta - x1*sin_theta;\n                        dst_data[n_dims/2]   = x0*sin_theta + x1*cos_theta;\n                        dst_data[n_dims]     = x2*cos_block_theta - x3*sin_block_theta;\n                        dst_data[n_dims/2*3] = x2*sin_block_theta + x3*cos_block_theta;\n                    }\n                } else if (!is_neox) {\n                    for (int64_t i0 = 0; i0 < ne0; i0 += 2) {\n                        float cos_theta, sin_theta;\n                        rope_yarn(\n                            theta_base, freq_scale, corr_dims, i0, ext_factor, attn_factor, &cos_theta, &sin_theta\n                        );\n                        sin_theta *= sin_sign;\n\n                        // zeta scaling for xPos only:\n                        float zeta = xpos_base != 0.0f ? powf((i0 + 0.4f * ne0) / (1.4f * ne0), p / xpos_base) : 1.0f;\n                        if (xpos_down) zeta = 1.0f / zeta;\n\n                        theta_base *= theta_scale;\n\n                        const float * const src = (float *)((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01 + i0*nb00);\n                              float * dst_data  = (float *)((char *)  dst->data + i3*nb3  + i2*nb2  + i1*nb1  + i0*nb0);\n\n                        const float x0 = src[0];\n                        const float x1 = src[1];\n\n                        dst_data[0] = x0*cos_theta*zeta - x1*sin_theta*zeta;\n                        dst_data[1] = x0*sin_theta*zeta + x1*cos_theta*zeta;\n                    }\n                } else {\n                    // TODO: this might be wrong for ne0 != n_dims - need double check\n                    // ref:  https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py#LL251C1-L294C28\n                    theta_base *= freq_scale;\n                    for (int64_t ib = 0; ib < ne0/n_dims; ++ib) {\n                        for (int64_t ic = 0; ic < n_dims; ic += 2) {\n                            // simplified from `(ib * n_dims + ic) * inv_ndims`\n                            float cur_rot = inv_ndims * ic - ib;\n\n                            float cos_theta, sin_theta;\n                            rope_yarn(\n                                theta_base, freq_scale, corr_dims, cur_rot, ext_factor, attn_factor,\n                                &cos_theta, &sin_theta\n                            );\n                            sin_theta *= sin_sign;\n\n                            theta_base *= theta_scale;\n\n                            const int64_t i0 = ib*n_dims + ic/2;\n\n                            const float * const src = (float *)((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01 + i0*nb00);\n                                  float * dst_data  = (float *)((char *)  dst->data + i3*nb3  + i2*nb2  + i1*nb1  + i0*nb0);\n\n                            const float x0 = src[0];\n                            const float x1 = src[n_dims/2];\n\n                            dst_data[0]        = x0*cos_theta - x1*sin_theta;\n                            dst_data[n_dims/2] = x0*sin_theta + x1*cos_theta;\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_rope_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst,\n        const bool forward) {\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    float freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow;\n\n    //const int n_past     = ((int32_t *) dst->op_params)[0];\n    const int n_dims     = ((int32_t *) dst->op_params)[1];\n    const int mode       = ((int32_t *) dst->op_params)[2];\n    const int n_ctx      = ((int32_t *) dst->op_params)[3];\n    const int n_orig_ctx = ((int32_t *) dst->op_params)[4];\n    memcpy(&freq_base,   (int32_t *) dst->op_params +  5, sizeof(float));\n    memcpy(&freq_scale,  (int32_t *) dst->op_params +  6, sizeof(float));\n    memcpy(&ext_factor,  (int32_t *) dst->op_params +  7, sizeof(float));\n    memcpy(&attn_factor, (int32_t *) dst->op_params +  8, sizeof(float));\n    memcpy(&beta_fast,   (int32_t *) dst->op_params +  9, sizeof(float));\n    memcpy(&beta_slow,   (int32_t *) dst->op_params + 10, sizeof(float));\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    //printf(\"ne0: %d, ne1: %d, ne2: %d, ne3: %d\\n\", ne0, ne1, ne2, ne3);\n    //printf(\"n_past = %d, ne2 = %d\\n\", n_past, ne2);\n\n    GGML_ASSERT(nb0 == sizeof(ggml_fp16_t));\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nr = ggml_nrows(dst);\n\n    GGML_ASSERT(n_dims <= ne0);\n    GGML_ASSERT(n_dims % 2 == 0);\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    // row index used to determine which thread to use\n    int ir = 0;\n\n    const float theta_scale = powf(freq_base, -2.0f/n_dims);\n    const float inv_ndims = -1.f/n_dims;\n    float corr_dims[2];\n    ggml_rope_yarn_corr_dims(n_dims, n_orig_ctx, freq_base, beta_fast, beta_slow, corr_dims);\n\n    const bool is_neox = mode & 2;\n    const bool is_glm  = mode & 4;\n\n    // backward process uses inverse rotation by cos and sin.\n    // cos and sin build a rotation matrix, where the inverse is the transpose.\n    // this essentially just switches the sign of sin.\n    const float sin_sign = forward ? 1.0f : -1.0f;\n\n    const int32_t * pos = (const int32_t *) src1->data;\n\n    for (int64_t i3 = 0; i3 < ne3; i3++) {\n        for (int64_t i2 = 0; i2 < ne2; i2++) {\n            const int64_t p = pos[i2];\n            for (int64_t i1 = 0; i1 < ne1; i1++) {\n                if (ir++ < ir0) continue;\n                if (ir   > ir1) break;\n\n                float theta_base = (float)p;\n\n                if (is_glm) {\n                    theta_base = MIN(p, n_ctx - 2);\n                    float block_theta = MAX(p - (n_ctx - 2), 0);\n                    for (int64_t i0 = 0; i0 < ne0 / 4; i0++) {\n                        const float cos_theta = cosf(theta_base);\n                        const float sin_theta = sinf(theta_base) * sin_sign;\n                        const float cos_block_theta = cosf(block_theta);\n                        const float sin_block_theta = sinf(block_theta) * sin_sign;\n\n                        theta_base *= theta_scale;\n                        block_theta *= theta_scale;\n\n                        const ggml_fp16_t * const src = (ggml_fp16_t *)((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01 + i0*nb00);\n                              ggml_fp16_t * dst_data  = (ggml_fp16_t *)((char *)  dst->data +  i3*nb3 + i2*nb2  + i1*nb1  + i0*nb0);\n\n                        const float x0 = GGML_FP16_TO_FP32(src[0]);\n                        const float x1 = GGML_FP16_TO_FP32(src[n_dims/2]);\n                        const float x2 = GGML_FP16_TO_FP32(src[n_dims]);\n                        const float x3 = GGML_FP16_TO_FP32(src[n_dims/2*3]);\n\n                        dst_data[0]          = GGML_FP32_TO_FP16(x0*cos_theta - x1*sin_theta);\n                        dst_data[n_dims/2]   = GGML_FP32_TO_FP16(x0*sin_theta + x1*cos_theta);\n                        dst_data[n_dims]     = GGML_FP32_TO_FP16(x2*cos_block_theta - x3*sin_block_theta);\n                        dst_data[n_dims/2*3] = GGML_FP32_TO_FP16(x2*sin_block_theta + x3*cos_block_theta);\n                    }\n                } else if (!is_neox) {\n                    for (int64_t i0 = 0; i0 < ne0; i0 += 2) {\n                        float cos_theta, sin_theta;\n                        rope_yarn(\n                            theta_base, freq_scale, corr_dims, i0, ext_factor, attn_factor, &cos_theta, &sin_theta\n                        );\n                        sin_theta *= sin_sign;\n\n                        theta_base *= theta_scale;\n\n                        const ggml_fp16_t * const src = (ggml_fp16_t *)((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01 + i0*nb00);\n                              ggml_fp16_t * dst_data  = (ggml_fp16_t *)((char *)  dst->data + i3*nb3  + i2*nb2  + i1*nb1  + i0*nb0);\n\n                        const float x0 = GGML_FP16_TO_FP32(src[0]);\n                        const float x1 = GGML_FP16_TO_FP32(src[1]);\n\n                        dst_data[0] = GGML_FP32_TO_FP16(x0*cos_theta - x1*sin_theta);\n                        dst_data[1] = GGML_FP32_TO_FP16(x0*sin_theta + x1*cos_theta);\n                    }\n                } else {\n                    // TODO: this might be wrong for ne0 != n_dims - need double check\n                    // ref:  https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt_neox/modeling_gpt_neox.py#LL251C1-L294C28\n                    theta_base *= freq_scale;\n                    for (int64_t ib = 0; ib < ne0/n_dims; ++ib) {\n                        for (int64_t ic = 0; ic < n_dims; ic += 2) {\n                            // simplified from `(ib * n_dims + ic) * inv_ndims`\n                            float cur_rot = inv_ndims * ic - ib;\n\n                            float cos_theta, sin_theta;\n                            rope_yarn(\n                                theta_base, freq_scale, corr_dims, cur_rot, ext_factor, attn_factor,\n                                &cos_theta, &sin_theta\n                            );\n                            sin_theta *= sin_sign;\n\n                            theta_base *= theta_scale;\n\n                            const int64_t i0 = ib*n_dims + ic/2;\n\n                            const ggml_fp16_t * const src = (ggml_fp16_t *)((char *) src0->data + i3*nb03 + i2*nb02 + i1*nb01 + i0*nb00);\n                                  ggml_fp16_t * dst_data  = (ggml_fp16_t *)((char *)  dst->data + i3*nb3  + i2*nb2  + i1*nb1  + i0*nb0);\n\n                            const float x0 = GGML_FP16_TO_FP32(src[0]);\n                            const float x1 = GGML_FP16_TO_FP32(src[n_dims/2]);\n\n                            dst_data[0]        = GGML_FP32_TO_FP16(x0*cos_theta - x1*sin_theta);\n                            dst_data[n_dims/2] = GGML_FP32_TO_FP16(x0*sin_theta + x1*cos_theta);\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_rope(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_rope_f16(params, src0, src1, dst, true);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_rope_f32(params, src0, src1, dst, true);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_rope_back\n\nstatic void ggml_compute_forward_rope_back(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_rope_f16(params, src0, src1, dst, false);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_rope_f32(params, src0, src1, dst, false);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_conv_transpose_1d\n\nstatic void ggml_compute_forward_conv_transpose_1d_f16_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nk = ne00*ne01*ne02;\n\n    GGML_ASSERT(nb00 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    if (params->type == GGML_TASK_INIT) {\n        memset(params->wdata, 0, params->wsize);\n\n        // permute kernel data (src0) from (K x Cout x Cin) to (Cin x K x Cout)\n        {\n            ggml_fp16_t * const wdata = (ggml_fp16_t *) params->wdata + 0;\n\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                for (int64_t i01 = 0; i01 < ne01; i01++) {\n                    const ggml_fp16_t * const src = (ggml_fp16_t *)((char *) src0->data + i02*nb02 + i01*nb01);\n                    ggml_fp16_t * dst_data = wdata + i01*ne00*ne02;\n                    for (int64_t i00 = 0; i00 < ne00; i00++) {\n                        dst_data[i00*ne02 + i02] = src[i00];\n                    }\n                }\n            }\n        }\n\n        // permute source data (src1) from (L x Cin) to (Cin x L)\n        {\n            ggml_fp16_t * const wdata = (ggml_fp16_t *) params->wdata + nk;\n            ggml_fp16_t * dst_data = wdata;\n\n            for (int64_t i11 = 0; i11 < ne11; i11++) {\n                const float * const src = (float *)((char *) src1->data + i11*nb11);\n                for (int64_t i10 = 0; i10 < ne10; i10++) {\n                    dst_data[i10*ne11 + i11] = GGML_FP32_TO_FP16(src[i10]);\n                }\n            }\n        }\n\n        // need to zero dst since we are accumulating into it\n        memset(dst->data, 0, ggml_nbytes(dst));\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int32_t s0 = ((const int32_t*)(dst->op_params))[0];\n\n    // total rows in dst\n    const int nr = ne1;\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    ggml_fp16_t * const wdata     = (ggml_fp16_t *) params->wdata + 0;\n    ggml_fp16_t * const wdata_src = wdata + nk;\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        float * dst_data = (float *)((char *) dst->data + i1*nb1);\n        ggml_fp16_t * wdata_kernel = wdata + i1*ne02*ne00;\n        for (int i10 = 0; i10 < ne10; i10++) {\n            const int i1n = i10*ne11;\n            for (int i00 = 0; i00 < ne00; i00++) {\n                float v = 0;\n                ggml_vec_dot_f16(ne02, &v,\n                        (ggml_fp16_t *)    wdata_src + i1n,\n                        (ggml_fp16_t *) wdata_kernel + i00*ne02);\n                dst_data[i10*s0 + i00] += v;\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_conv_transpose_1d_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    GGML_ASSERT(src0->type == GGML_TYPE_F32);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nk = ne00*ne01*ne02;\n\n    GGML_ASSERT(nb00 == sizeof(float));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    if (params->type == GGML_TASK_INIT) {\n        memset(params->wdata, 0, params->wsize);\n\n        // prepare kernel data (src0) from (K x Cout x Cin) to (Cin x K x Cout)\n        {\n            float * const wdata = (float *) params->wdata + 0;\n\n            for (int64_t i02 = 0; i02 < ne02; i02++) {\n                for (int64_t i01 = 0; i01 < ne01; i01++) {\n                    const float * const src = (float *)((char *) src0->data + i02*nb02 + i01*nb01);\n                    float * dst_data = wdata + i01*ne00*ne02;\n                    for (int64_t i00 = 0; i00 < ne00; i00++) {\n                        dst_data[i00*ne02 + i02] = src[i00];\n                    }\n                }\n            }\n        }\n\n        // prepare source data (src1)\n        {\n            float * const wdata = (float *) params->wdata + nk;\n            float * dst_data = wdata;\n\n            for (int64_t i11 = 0; i11 < ne11; i11++) {\n                const float * const src = (float *)((char *) src1->data + i11*nb11);\n                for (int64_t i10 = 0; i10 < ne10; i10++) {\n                    dst_data[i10*ne11 + i11] = src[i10];\n                }\n            }\n        }\n\n        // need to zero dst since we are accumulating into it\n        memset(dst->data, 0, ggml_nbytes(dst));\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int32_t s0 = ((const int32_t*)(dst->op_params))[0];\n\n    // total rows in dst\n    const int nr = ne1;\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    float * const wdata     = (float *) params->wdata + 0;\n    float * const wdata_src = wdata + nk;\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        float * dst_data = (float *)((char *) dst->data + i1*nb1);\n        float * wdata_kernel = wdata + i1*ne02*ne00;\n        for (int i10 = 0; i10 < ne10; i10++) {\n            const int i1n = i10*ne11;\n            for (int i00 = 0; i00 < ne00; i00++) {\n                float v = 0;\n                ggml_vec_dot_f32(ne02, &v,\n                        wdata_src + i1n,\n                        wdata_kernel + i00*ne02);\n                dst_data[i10*s0 + i00] += v;\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_conv_transpose_1d(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_conv_transpose_1d_f16_f32(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_conv_transpose_1d_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// src0: kernel [OC, IC, KH, KW]\n// src1: image [N, IC, IH, IW]\n// dst:  result [N, OH, OW, IC*KH*KW]\nstatic void ggml_compute_forward_im2col_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F16);\n\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS;\n\n    const int32_t s0 = ((const int32_t *)(dst->op_params))[0];\n    const int32_t s1 = ((const int32_t *)(dst->op_params))[1];\n    const int32_t p0 = ((const int32_t *)(dst->op_params))[2];\n    const int32_t p1 = ((const int32_t *)(dst->op_params))[3];\n    const int32_t d0 = ((const int32_t *)(dst->op_params))[4];\n    const int32_t d1 = ((const int32_t *)(dst->op_params))[5];\n    const bool is_2D = ((const int32_t *)(dst->op_params))[6] == 1;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int64_t N  = is_2D ? ne13 : ne12;\n    const int64_t IC = is_2D ? ne12 : ne11;\n    const int64_t IH = is_2D ? ne11 : 1;\n    const int64_t IW = ne10;\n\n    const int64_t KH = is_2D ? ne01 : 1;\n    const int64_t KW = ne00;\n\n    const int64_t OH = is_2D ? ne2 : 1;\n    const int64_t OW = ne1;\n\n    int ofs0 = is_2D ? nb13 : nb12;\n    int ofs1 = is_2D ? nb12 : nb11;\n\n    GGML_ASSERT(nb00 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    if (params->type == GGML_TASK_INIT) {\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // im2col: [N, IC, IH, IW] => [N, OH, OW, IC*KH*KW]\n    {\n        ggml_fp16_t * const wdata = (ggml_fp16_t *) dst->data;\n\n        for (int64_t in = 0; in < N; in++) {\n            for (int64_t ioh = 0; ioh < OH; ioh++) { // 1\n                for (int64_t iow = 0; iow < OW; iow++) {\n                    for (int64_t iic = ith; iic < IC; iic += nth) {\n\n                        // micro kernel\n                        ggml_fp16_t * dst_data = wdata + (in*OH*OW + ioh*OW + iow)*(IC*KH*KW); // [IC, KH, KW]\n                        const float * const src_data = (float *)((char *) src1->data + in*ofs0 + iic*ofs1); // [IH, IW]\n\n                        for (int64_t ikh = 0; ikh < KH; ikh++) {  // 1\n                            for (int64_t ikw = 0; ikw < KW; ikw++) {\n                                const int64_t iiw = iow*s0 + ikw*d0 - p0;\n                                const int64_t iih = ioh*s1 + ikh*d1 - p1;\n\n                                if (iih < 0 || iih >= IH || iiw < 0 || iiw >= IW) {\n                                    dst_data[iic*(KH*KW) + ikh*KW + ikw] = 0;\n                                } else {\n                                    dst_data[iic*(KH*KW) + ikh*KW + ikw] = GGML_FP32_TO_FP16(src_data[iih*IW + iiw]);\n                                }\n                            }\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_im2col(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_im2col_f16(params, src0, src1, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                GGML_ASSERT(false);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_conv_transpose_2d\n\nstatic void ggml_compute_forward_conv_transpose_2d(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    GGML_ASSERT(src0->type == GGML_TYPE_F16);\n    GGML_ASSERT(src1->type == GGML_TYPE_F32);\n    GGML_ASSERT( dst->type == GGML_TYPE_F32);\n\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int nk = ne00*ne01*ne02*ne03;\n\n    GGML_ASSERT(nb00 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    if (params->type == GGML_TASK_INIT) {\n        memset(params->wdata, 0, params->wsize);\n\n        // permute kernel data (src0) from (Kw x Kh x Cout x Cin) to (Cin x Kw x Kh x Cout)\n        {\n            ggml_fp16_t * const wdata = (ggml_fp16_t *) params->wdata + 0;\n\n            for (int64_t i03 = 0; i03 < ne03; i03++) {\n                for (int64_t i02 = 0; i02 < ne02; i02++) {\n                    const ggml_fp16_t * const src = (ggml_fp16_t *)((char *) src0->data + i03*nb03 + i02*nb02);\n                    ggml_fp16_t * dst_data = wdata + i02*ne01*ne00*ne03;\n                    for (int64_t i01 = 0; i01 < ne01; i01++) {\n                        for (int64_t i00 = 0; i00 < ne00; i00++) {\n                            dst_data[i01*ne00*ne03 + i00*ne03 + i03] = src[i01 * ne00 + i00];\n                        }\n                    }\n                }\n            }\n        }\n\n        // permute source data (src1) from (Sw x Sh x Cin) to (Cin x Sw x Sh)\n        {\n            ggml_fp16_t * const wdata = (ggml_fp16_t *) params->wdata + nk;\n            for (int i12 = 0; i12 < ne12; i12++) {\n                for (int i11 = 0; i11 < ne11; i11++) {\n                    const float * const src = (float *)((char *) src1->data + i12*nb12 + i11*nb11);\n                    ggml_fp16_t * dst_data = wdata + i11*ne10*ne12;\n                    for (int i10 = 0; i10 < ne10; i10++) {\n                        dst_data[i10*ne12 + i12] = GGML_FP32_TO_FP16(src[i10]);\n                    }\n                }\n            }\n        }\n\n        memset(dst->data, 0, ggml_nbytes(dst));\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int32_t stride = ggml_get_op_params_i32(dst, 0);\n\n    // total patches in dst\n    const int np = ne2;\n\n    // patches per thread\n    const int dp = (np + nth - 1)/nth;\n\n    // patch range for this thread\n    const int ip0 = dp*ith;\n    const int ip1 = MIN(ip0 + dp, np);\n\n    ggml_fp16_t * const wdata = (ggml_fp16_t *) params->wdata + 0;\n    ggml_fp16_t * const wdata_src = wdata + nk;\n\n    for (int i2 = ip0; i2 < ip1; i2++) { // Cout\n        float * dst_data = (float *)((char *) dst->data + i2*nb2);\n        ggml_fp16_t * wdata_kernel = wdata + i2*ne01*ne00*ne03;\n        for (int i11 = 0; i11 < ne11; i11++) {\n            for (int i10 = 0; i10 < ne10; i10++) {\n                const int i1n = i11*ne10*ne12 + i10*ne12;\n                for (int i01 = 0; i01 < ne01; i01++) {\n                    for (int i00 = 0; i00 < ne00; i00++) {\n                        float v = 0;\n                        ggml_vec_dot_f16(ne03, &v,\n                                wdata_src + i1n,\n                                wdata_kernel + i01*ne00*ne03 + i00*ne03);\n                        dst_data[(i11*stride + i01)*ne0 + i10*stride + i00] += v;\n                    }\n                }\n            }\n        }\n    }\n}\n\n// ggml_compute_forward_pool_1d_sk_p0\n\nstatic void ggml_compute_forward_pool_1d_sk_p0(\n        const struct ggml_compute_params * params,\n        const enum ggml_op_pool op,\n        const struct ggml_tensor * src,\n        const int k,\n        struct ggml_tensor * dst) {\n    assert(src->type == GGML_TYPE_F32);\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const char * cdata = (const char *)src->data;\n    const char * const data_end = cdata + ggml_nbytes(src);\n    float * drow = (float *)dst->data;\n\n    const int64_t rs = dst->ne[0];\n\n    while (cdata < data_end) {\n        const float * const srow = (const float *)cdata;\n\n        int j = 0;\n\n        for (int64_t i = 0; i < rs; ++i) {\n            switch (op) {\n                case GGML_OP_POOL_AVG:   drow[i] = 0;        break;\n                case GGML_OP_POOL_MAX:   drow[i] = -FLT_MAX; break;\n                case GGML_OP_POOL_COUNT: GGML_ASSERT(false); break;\n            }\n            for (int ki = 0; ki < k; ++ki) {\n                switch (op) {\n                    case GGML_OP_POOL_AVG:                          drow[i] += srow[j]; break;\n                    case GGML_OP_POOL_MAX:   if (srow[j] > drow[i]) drow[i]  = srow[j]; break;\n                    case GGML_OP_POOL_COUNT:                        GGML_ASSERT(false); break;\n                }\n                ++j;\n            }\n            switch (op) {\n                case GGML_OP_POOL_AVG:         drow[i] /= k; break;\n                case GGML_OP_POOL_MAX:                       break;\n                case GGML_OP_POOL_COUNT: GGML_ASSERT(false); break;\n            }\n        }\n\n        cdata += src->nb[1];\n        drow  += rs;\n    }\n}\n\n// ggml_compute_forward_pool_1d\n\nstatic void ggml_compute_forward_pool_1d(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n              struct ggml_tensor * dst) {\n\n    const int32_t * opts = (const int32_t *)dst->op_params;\n    enum ggml_op_pool op = opts[0];\n    const int k0 = opts[1];\n    const int s0 = opts[2];\n    const int p0 = opts[3];\n    GGML_ASSERT(p0 == 0); // padding not supported\n    GGML_ASSERT(k0 == s0); // only s = k supported\n\n    ggml_compute_forward_pool_1d_sk_p0(params, op, src0, k0, dst);\n}\n\n// ggml_compute_forward_pool_2d\n\nstatic void ggml_compute_forward_pool_2d(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src,\n        struct ggml_tensor * dst) {\n    assert(src->type == GGML_TYPE_F32);\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int32_t * opts = (const int32_t *)dst->op_params;\n    enum ggml_op_pool op = opts[0];\n    const int k0 = opts[1];\n    const int k1 = opts[2];\n    const int s0 = opts[3];\n    const int s1 = opts[4];\n    const int p0 = opts[5];\n    const int p1 = opts[6];\n    const char * cdata = (const char*)src->data;\n    const char * const data_end = cdata + ggml_nbytes(src);\n\n    const int64_t px = dst->ne[0];\n    const int64_t py = dst->ne[1];\n    const int64_t pa = px * py;\n\n    float * dplane = (float *)dst->data;\n\n    const int ka = k0 * k1;\n    const int offset0 = -p0;\n    const int offset1 = -p1;\n\n    while (cdata < data_end) {\n        for (int oy = 0; oy < py; ++oy) {\n            float * const drow = dplane + oy * px;\n            for (int ox = 0; ox < px; ++ox) {\n                float * const out =  drow + ox;\n                switch (op) {\n                    case GGML_OP_POOL_AVG:     *out = 0;        break;\n                    case GGML_OP_POOL_MAX:     *out = -FLT_MAX; break;\n                    case GGML_OP_POOL_COUNT: GGML_ASSERT(false); break;\n                }\n\n                const int ix = offset0 + ox * s0;\n                const int iy = offset1 + oy * s1;\n\n                for (int ky = 0; ky < k1; ++ky) {\n                    if (iy + ky < 0 || iy + ky >= src->ne[1]) continue;\n                    const float * const srow = (const float *)(cdata + src->nb[1] * (iy + ky));\n                    for (int kx = 0; kx < k0; ++kx) {\n                        int j = ix + kx;\n                        if (j < 0 || j >= src->ne[0]) continue;\n                        switch (op) {\n                            case GGML_OP_POOL_AVG:                     *out += srow[j]; break;\n                            case GGML_OP_POOL_MAX: if (srow[j] > *out) *out  = srow[j]; break;\n                            case GGML_OP_POOL_COUNT:                GGML_ASSERT(false); break;\n                        }\n                    }\n                }\n                switch (op) {\n                    case GGML_OP_POOL_AVG:           *out /= ka; break;\n                    case GGML_OP_POOL_MAX:                       break;\n                    case GGML_OP_POOL_COUNT: GGML_ASSERT(false); break;\n                }\n            }\n        }\n\n        cdata  += src->nb[2];\n        dplane += pa;\n    }\n}\n\n// ggml_compute_forward_upscale\n\nstatic void ggml_compute_forward_upscale_f32(\n    const struct ggml_compute_params * params,\n    const struct ggml_tensor * src0,\n    struct ggml_tensor * dst) {\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_ASSERT(src0->nb[0] == sizeof(float));\n\n    const int ith = params->ith;\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    const int scale_factor = dst->op_params[0];\n\n    // TODO: optimize\n\n    for (int i03 = 0; i03 < ne03; i03++) {\n        for (int i02 = ith; i02 < ne02; i02++) {\n            for (int m = 0; m < dst->ne[1]; m++) {\n                int i01 = m / scale_factor;\n                for (int n = 0; n < dst->ne[0]; n++) {\n                    int i00 = n / scale_factor;\n\n                    const float * x = (float *)((char *) src0->data + i00 * nb00 +i01 * nb01 + i02 * nb02 + i03 * nb03);\n\n                    float * y = (float *)((char *) dst->data + n * dst->nb[0] + m * dst->nb[1] + i02 * dst->nb[2] + i03 * dst->nb[3]);\n\n                    *y = *x;\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_upscale(\n    const struct ggml_compute_params * params,\n    const struct ggml_tensor * src0,\n    struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_upscale_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_flash_attn\n\nstatic void ggml_compute_forward_flash_attn_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * q,\n        const struct ggml_tensor * k,\n        const struct ggml_tensor * v,\n        const bool masked,\n        struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_LOCALS(int64_t, neq, q,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbq, q,   nb)\n    GGML_TENSOR_LOCALS(int64_t, nek, k,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbk, k,   nb)\n    GGML_TENSOR_LOCALS(int64_t, nev, v,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbv, v,   nb)\n    GGML_TENSOR_LOCALS(int64_t, ne,  dst, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb,  dst, nb)\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int64_t D = neq0;\n    const int64_t N = neq1;\n    const int64_t P = nek1 - N;\n    const int64_t M = P + N;\n\n    const int Mup = ggml_up(M, GGML_SOFT_MAX_UNROLL);\n\n    GGML_ASSERT(ne0 == D);\n    GGML_ASSERT(ne1 == N);\n    GGML_ASSERT(P >= 0);\n\n    GGML_ASSERT(nbq0 == sizeof(float));\n    GGML_ASSERT(nbk0 == sizeof(float));\n    GGML_ASSERT(nbv0 == sizeof(float));\n\n    GGML_ASSERT(neq0 == D);\n    GGML_ASSERT(nek0 == D);\n    GGML_ASSERT(nev1 == D);\n\n    GGML_ASSERT(neq1 == N);\n    GGML_ASSERT(nek1 == N + P);\n    GGML_ASSERT(nev1 == D);\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    if (params->type == GGML_TASK_INIT) {\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // parallelize by q rows using ggml_vec_dot_f32\n\n    // total rows in q\n    const int nr = neq1*neq2*neq3;\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    const float scale = 1.0f/sqrtf(D);\n\n    //printf(\"P=%d N=%d D=%d ir0=%d ir1=%d scale = %f\\n\", P, N, D, ir0, ir1, scale);\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // q indices\n        const int iq3 = ir/(neq2*neq1);\n        const int iq2 = (ir - iq3*neq2*neq1)/neq1;\n        const int iq1 = (ir - iq3*neq2*neq1 - iq2*neq1);\n\n        float * S = (float *) params->wdata + ith*(Mup + CACHE_LINE_SIZE_F32);\n\n        for (int i = M; i < Mup; ++i) {\n            S[i] = -INFINITY;\n        }\n\n        const int64_t masked_begin = masked ? (P + iq1 + 1) : M;\n        for (int64_t ic = 0; ic < masked_begin; ++ic) {\n            // k indices\n            const int ik3 = iq3;\n            const int ik2 = iq2 % nek2;\n            const int ik1 = ic;\n\n            // S indices\n            const int i1 = ik1;\n\n            ggml_vec_dot_f32(neq0,\n                    S + i1,\n                    (float *) ((char *) k->data + (ik1*nbk1 + ik2*nbk2 + ik3*nbk3)),\n                    (float *) ((char *) q->data + (iq1*nbq1 + iq2*nbq2 + iq3*nbq3)));\n        }\n\n        // scale\n        ggml_vec_scale_f32(masked_begin, S, scale);\n\n        for (int64_t i = masked_begin; i < M; i++) {\n            S[i] = -INFINITY;\n        }\n\n        // softmax\n        // exclude known -INF S[..] values from max and loop\n        // dont forget to set their SW values to zero\n        {\n            float max = -INFINITY;\n            ggml_vec_max_f32(masked_begin, &max, S);\n\n            ggml_float sum = 0.0;\n            {\n#ifdef GGML_SOFT_MAX_ACCELERATE\n                max = -max;\n                vDSP_vsadd(S, 1, &max, S, 1, Mup);\n                vvexpf(S, S, &Mup);\n                ggml_vec_sum_f32(Mup, &sum, S);\n#else\n                uint16_t   scvt[GGML_SOFT_MAX_UNROLL]; UNUSED(scvt);\n                ggml_float sump[GGML_SOFT_MAX_UNROLL] = { 0.0 };\n\n                for (int i = 0; i < Mup; i += GGML_SOFT_MAX_UNROLL) {\n                    if (i >= masked_begin) {\n                        break;\n                    }\n                    float * SS = S + i;\n\n                    for (int j = 0; j < GGML_SOFT_MAX_UNROLL; ++j) {\n                        if (i + j >= masked_begin) {\n                            break;\n                        } else if (SS[j] == -INFINITY) {\n                            SS[j] = 0.0f;\n                        } else {\n#ifndef GGML_FLASH_ATTN_EXP_FP16\n                            const float val = expf(SS[j] - max);\n#else\n                            ggml_fp16_t s = GGML_FP32_TO_FP16(SS[j] - max);\n                            memcpy(&scvt[j], &s, sizeof(uint16_t));\n                            const float val = GGML_FP16_TO_FP32(ggml_table_exp_f16[scvt[j]]);\n#endif\n                            sump[j] += (ggml_float)val;\n                            SS[j] = val;\n                        }\n                    }\n                }\n\n                for (int i = 0; i < GGML_SOFT_MAX_UNROLL; i++) {\n                    sum += sump[i];\n                }\n#endif\n            }\n\n            assert(sum > 0.0);\n\n            sum = 1.0/sum;\n            ggml_vec_scale_f32(masked_begin, S, sum);\n\n#ifndef NDEBUG\n            for (int i = 0; i < masked_begin; ++i) {\n                assert(!isnan(S[i]));\n                assert(!isinf(S[i]));\n            }\n#endif\n        }\n\n        for (int64_t ic = 0; ic < nev1; ++ic) {\n            // dst indices\n            const int i1 = iq1;\n            const int i2 = iq2;\n            const int i3 = iq3;\n\n            // v indices\n            const int iv2 = iq2 % nev2;\n            const int iv3 = iq3;\n\n            ggml_vec_dot_f32(masked_begin,\n                    (float *) ((char *) dst->data + (ic*nb0 + i1*nb1  + i2*nb2   + i3*nb3)),\n                    (float *) ((char *) v->data   + (         ic*nbv1 + iv2*nbv2 + iv3*nbv3)),\n                    S);\n        }\n    }\n}\n\nstatic void ggml_compute_forward_flash_attn_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * q,\n        const struct ggml_tensor * k,\n        const struct ggml_tensor * v,\n        const bool masked,\n        struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_LOCALS(int64_t, neq, q,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbq, q,   nb)\n    GGML_TENSOR_LOCALS(int64_t, nek, k,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbk, k,   nb)\n    GGML_TENSOR_LOCALS(int64_t, nev, v,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbv, v,   nb)\n    GGML_TENSOR_LOCALS(int64_t, ne,  dst, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb,  dst, nb)\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int64_t D = neq0;\n    const int64_t N = neq1;\n    const int64_t P = nek1 - N;\n    const int64_t M = P + N;\n\n    const int Mup = ggml_up(M, GGML_SOFT_MAX_UNROLL);\n\n    GGML_ASSERT(ne0 == D);\n    GGML_ASSERT(ne1 == N);\n    GGML_ASSERT(P >= 0);\n\n    GGML_ASSERT(nbq0 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nbk0 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nbv0 == sizeof(ggml_fp16_t));\n\n    GGML_ASSERT(neq0 == D);\n    GGML_ASSERT(nek0 == D);\n    GGML_ASSERT(nev1 == D);\n\n    GGML_ASSERT(neq1 == N);\n    GGML_ASSERT(nek1 == N + P);\n    GGML_ASSERT(nev1 == D);\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    if (params->type == GGML_TASK_INIT) {\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // parallelize by q rows using ggml_vec_dot_f32\n\n    // total rows in q\n    const int nr = neq1*neq2*neq3;\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    const float scale = 1.0f/sqrtf(D);\n\n    //printf(\"P=%d N=%d D=%d ir0=%d ir1=%d scale = %f\\n\", P, N, D, ir0, ir1, scale);\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // q indices\n        const int iq3 = ir/(neq2*neq1);\n        const int iq2 = (ir - iq3*neq2*neq1)/neq1;\n        const int iq1 = (ir - iq3*neq2*neq1 - iq2*neq1);\n\n        float * S = (float *) params->wdata + ith*(2*Mup + CACHE_LINE_SIZE_F32);\n\n        for (int i = M; i < Mup; ++i) {\n            S[i] = -INFINITY;\n        }\n\n        if (GGML_VEC_DOT_UNROLL > 2 || nek1 % GGML_VEC_DOT_UNROLL != 0) {\n            for (int64_t ic = 0; ic < nek1; ++ic) {\n                // k indices\n                const int ik3 = iq3;\n                const int ik2 = iq2 % nek2;\n                const int ik1 = ic;\n\n                // S indices\n                const int i1 = ik1;\n\n                ggml_vec_dot_f16(neq0,\n                        S + i1,\n                        (ggml_fp16_t *) ((char *) k->data + (ik1*nbk1 + ik2*nbk2 + ik3*nbk3)),\n                        (ggml_fp16_t *) ((char *) q->data + (iq1*nbq1 + iq2*nbq2 + iq3*nbq3)));\n            }\n        } else {\n            for (int64_t ic = 0; ic < nek1; ic += GGML_VEC_DOT_UNROLL) {\n                // k indices\n                const int ik3 = iq3;\n                const int ik2 = iq2 % nek2;\n                const int ik1 = ic;\n\n                // S indices\n                const int i1 = ik1;\n\n                ggml_vec_dot_f16_unroll(neq0, nbk1,\n                        S + i1,\n                        ((char *) k->data + (ik1*nbk1 + ik2*nbk2 + ik3*nbk3)),\n                        (ggml_fp16_t *) ((char *) q->data + (iq1*nbq1 + iq2*nbq2 + iq3*nbq3)));\n            }\n        }\n\n        // scale\n        ggml_vec_scale_f32(nek1, S, scale);\n\n        if (masked) {\n            for (int64_t i = P; i < M; i++) {\n                if (i > P + iq1) {\n                    S[i] = -INFINITY;\n                }\n            }\n        }\n\n        // softmax\n        // todo: exclude known -INF S[..] values from max and loop, assuming their results to be zero.\n        // dont forget to set their S values to zero\n        {\n            float max = -INFINITY;\n            ggml_vec_max_f32(M, &max, S);\n\n            ggml_float sum = 0.0;\n            {\n#ifdef GGML_SOFT_MAX_ACCELERATE\n                max = -max;\n                vDSP_vsadd(S, 1, &max, S, 1, Mup);\n                vvexpf(S, S, &Mup);\n                ggml_vec_sum_f32(Mup, &sum, S);\n#else\n                uint16_t   scvt[GGML_SOFT_MAX_UNROLL];\n                ggml_float sump[GGML_SOFT_MAX_UNROLL] = { 0.0 };\n\n                for (int i = 0; i < Mup; i += GGML_SOFT_MAX_UNROLL) {\n                    float * SS = S + i;\n\n                    for (int j = 0; j < GGML_SOFT_MAX_UNROLL; ++j) {\n                        if (SS[j] == -INFINITY) {\n                            SS[j] = 0.0f;\n                        } else {\n                            ggml_fp16_t s = GGML_FP32_TO_FP16(SS[j] - max);\n                            memcpy(&scvt[j], &s, sizeof(uint16_t));\n                            const float val = GGML_FP16_TO_FP32(ggml_table_exp_f16[scvt[j]]);\n                            sump[j] += (ggml_float)val;\n                            SS[j] = val;\n                        }\n                    }\n                }\n\n                for (int i = 0; i < GGML_SOFT_MAX_UNROLL; i++) {\n                    sum += sump[i];\n                }\n#endif\n            }\n\n            assert(sum > 0.0);\n\n            sum = 1.0/sum;\n            ggml_vec_scale_f32(M, S, sum);\n\n#ifndef NDEBUG\n            for (int i = 0; i < M; ++i) {\n                assert(!isnan(S[i]));\n                assert(!isinf(S[i]));\n            }\n#endif\n        }\n\n        ggml_fp16_t * S16 = (ggml_fp16_t *) ((float *) params->wdata + ith*(2*Mup + CACHE_LINE_SIZE_F32) + Mup);\n\n        for (int64_t i = 0; i < M; i++) {\n            S16[i] = GGML_FP32_TO_FP16(S[i]);\n        }\n\n        // todo: exclude known zero S[..] values from dot (reducing nev0 and increasing begin of v and S16).\n        if (GGML_VEC_DOT_UNROLL == 1 || (nev1 % GGML_VEC_DOT_UNROLL != 0)) {\n            for (int64_t ic = 0; ic < nev1; ++ic) {\n                // dst indices\n                const int i1 = iq1;\n                const int i2 = iq2;\n                const int i3 = iq3;\n\n                // v indices\n                const int iv2 = iq2 % nev2;\n                const int iv3 = iq3;\n\n                ggml_vec_dot_f16(nev0,\n                        (float *)       ((char *) dst->data + (ic*nb0 + i1*nb1  + i2*nb2   + i3*nb3)),\n                        (ggml_fp16_t *) ((char *) v->data   + (         ic*nbv1 + iv2*nbv2 + iv3*nbv3)),\n                        S16);\n            }\n        } else {\n            for (int64_t ic = 0; ic < nev1; ic += GGML_VEC_DOT_UNROLL) {\n                // dst indices\n                const int i1 = iq1;\n                const int i2 = iq2;\n                const int i3 = iq3;\n\n                // v indices\n                const int iv2 = iq2 % nev2;\n                const int iv3 = iq3;\n\n                ggml_vec_dot_f16_unroll(nev0, nbv1,\n                        (float *) ((char *) dst->data + (ic*nb0 + i1*nb1  + i2*nb2   + i3*nb3)),\n                        ((char *)             v->data + (         ic*nbv1 + iv2*nbv2 + iv3*nbv3)),\n                        S16);\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_flash_attn(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * q,\n        const struct ggml_tensor * k,\n        const struct ggml_tensor * v,\n        const bool masked,\n        struct ggml_tensor * dst) {\n    switch (q->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_flash_attn_f16(params, q, k, v, masked, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_flash_attn_f32(params, q, k, v, masked, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_flash_ff\n\nstatic void ggml_compute_forward_flash_ff_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * a,  // F16\n        const struct ggml_tensor * b0, // F16 fc_w\n        const struct ggml_tensor * b1, // F32 fc_b\n        const struct ggml_tensor * c0, // F16 proj_w\n        const struct ggml_tensor * c1, // F32 proj_b\n        struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_LOCALS(int64_t, nea,  a,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nba,  a,   nb)\n    GGML_TENSOR_LOCALS(int64_t, neb0, b0,  ne)\n    GGML_TENSOR_LOCALS(size_t,  nbb0, b0,  nb)\n    GGML_TENSOR_LOCALS(int64_t, neb1, b1,  ne)\n    GGML_TENSOR_LOCALS(size_t,  nbb1, b1,  nb)\n    GGML_TENSOR_LOCALS(int64_t, nec0, c0,  ne)\n    GGML_TENSOR_LOCALS(size_t,  nbc0, c0,  nb)\n    GGML_TENSOR_LOCALS(int64_t, nec1, c1,  ne)\n    GGML_TENSOR_LOCALS(size_t,  nbc1, c1,  nb)\n    GGML_TENSOR_LOCALS(int64_t, ne,   dst, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb,   dst, nb)\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int64_t D = nea0;\n    //const int64_t N = nea1;\n    const int64_t M = neb01;\n\n    GGML_ASSERT(ne0 == nea0);\n    GGML_ASSERT(ne1 == nea1);\n    GGML_ASSERT(ne2 == nea2);\n\n    GGML_ASSERT(nba0  == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nbb00 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nbb10 == sizeof(float));\n    GGML_ASSERT(nbc00 == sizeof(ggml_fp16_t));\n    GGML_ASSERT(nbc10 == sizeof(float));\n\n    GGML_ASSERT(neb00 == D);\n    GGML_ASSERT(neb01 == M);\n    GGML_ASSERT(neb10 == M);\n    GGML_ASSERT(neb11 == 1);\n\n    GGML_ASSERT(nec00 == M);\n    GGML_ASSERT(nec01 == D);\n    GGML_ASSERT(nec10 == D);\n    GGML_ASSERT(nec11 == 1);\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    if (params->type == GGML_TASK_INIT) {\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // parallelize by a rows using ggml_vec_dot_f32\n\n    // total rows in a\n    const int nr = nea1*nea2*nea3;\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // a indices\n        const int ia3 = ir/(nea2*nea1);\n        const int ia2 = (ir - ia3*nea2*nea1)/nea1;\n        const int ia1 = (ir - ia3*nea2*nea1 - ia2*nea1);\n\n        float * S = (float *) params->wdata + ith*(2*M + CACHE_LINE_SIZE_F32);\n\n        for (int64_t ic = 0; ic < neb01; ++ic) {\n            // b0 indices\n            const int ib03 = ia3;\n            const int ib02 = ia2;\n            const int ib01 = ic;\n\n            // S indices\n            const int i1 = ib01;\n\n            ggml_vec_dot_f16(nea0,\n                    S + i1,\n                    (ggml_fp16_t *) ((char *) b0->data + (ib01*nbb01 + ib02*nbb02 + ib03*nbb03)),\n                    (ggml_fp16_t *) ((char *)  a->data + ( ia1*nba1  +  ia2*nba2  +  ia3*nba3)));\n        }\n\n        ggml_vec_add_f32(neb01, S, S, (float *) b1->data);\n        //ggml_vec_gelu_f32(neb01, S, S);\n\n        ggml_fp16_t * S16 = (ggml_fp16_t *) ((float *) params->wdata + ith*(2*M + CACHE_LINE_SIZE_F32) + M);\n\n        for (int64_t i = 0; i < M; i++) {\n            S16[i] = GGML_FP32_TO_FP16(S[i]);\n        }\n\n        ggml_vec_gelu_f16(neb01, S16, S16);\n\n        {\n            // dst indices\n            const int i1 = ia1;\n            const int i2 = ia2;\n            const int i3 = ia3;\n\n            for (int64_t ic = 0; ic < nec01; ++ic) {\n\n                ggml_vec_dot_f16(neb01,\n                        (float *)       ((char *) dst->data + (ic*nb0 + i1*nb1   + i2*nb2   + i3*nb3)),\n                        (ggml_fp16_t *) ((char *) c0->data  + (         ic*nbc01 + i2*nbc02 + i3*nbc03)),\n                        S16);\n            }\n\n            ggml_vec_add_f32(nec01,\n                    (float *) ((char *) dst->data + (i1*nb1 + i2*nb2 + i3*nb3)),\n                    (float *) ((char *) dst->data + (i1*nb1 + i2*nb2 + i3*nb3)),\n                    (float *) c1->data);\n        }\n    }\n}\n\nstatic void ggml_compute_forward_flash_ff(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * a,\n        const struct ggml_tensor * b0,\n        const struct ggml_tensor * b1,\n        const struct ggml_tensor * c0,\n        const struct ggml_tensor * c1,\n        struct ggml_tensor * dst) {\n    switch (b0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_flash_ff_f16(params, a, b0, b1, c0, c1, dst);\n            } break;\n        case GGML_TYPE_F32:\n            {\n                GGML_ASSERT(false); // TODO\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_flash_attn_back\n\nstatic void ggml_compute_forward_flash_attn_back_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * q,\n        const struct ggml_tensor * k,\n        const struct ggml_tensor * v,\n        const struct ggml_tensor * d,\n        const bool masked,\n              struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_LOCALS(int64_t, neq, q,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbq, q,   nb)\n    GGML_TENSOR_LOCALS(int64_t, nek, k,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbk, k,   nb)\n    GGML_TENSOR_LOCALS(int64_t, nev, v,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbv, v,   nb)\n    GGML_TENSOR_LOCALS(int64_t, ned, d,   ne)\n    GGML_TENSOR_LOCALS(size_t,  nbd, d,   nb)\n    GGML_TENSOR_LOCALS(int64_t, ne,  dst, ne)\n    GGML_TENSOR_LOCALS(size_t,  nb,  dst, nb)\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const int64_t D = neq0;\n    const int64_t N = neq1;\n    const int64_t P = nek1 - N;\n    const int64_t M = P + N;\n\n    const int Mup  = ggml_up(M, GGML_SOFT_MAX_UNROLL);\n    const int mxDM = MAX(D, Mup);\n\n    // GGML_ASSERT(ne0 == D);\n    // GGML_ASSERT(ne1 == N);\n    GGML_ASSERT(P >= 0);\n\n    GGML_ASSERT(nbq0 == sizeof(float));\n    GGML_ASSERT(nbk0 == sizeof(float));\n    GGML_ASSERT(nbv0 == sizeof(float));\n\n    GGML_ASSERT(neq0 == D);\n    GGML_ASSERT(nek0 == D);\n    GGML_ASSERT(nev1 == D);\n    GGML_ASSERT(ned0 == D);\n\n    GGML_ASSERT(neq1 == N);\n    GGML_ASSERT(nek1 == N + P);\n    GGML_ASSERT(nev1 == D);\n    GGML_ASSERT(ned1 == N);\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    if (params->type == GGML_TASK_INIT) {\n        if (ith == 0) {\n            memset(dst->data, 0, nb0*ne0*ne1*ne2*ne3);\n        }\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int64_t elem_q = ggml_nelements(q);\n    const int64_t elem_k = ggml_nelements(k);\n\n    enum ggml_type result_type = dst->type;\n    GGML_ASSERT(ggml_blck_size(result_type) == 1);\n    const size_t tsize = ggml_type_size(result_type);\n\n    const size_t offs_q = 0;\n    const size_t offs_k = offs_q + GGML_PAD(elem_q * tsize, GGML_MEM_ALIGN);\n    const size_t offs_v = offs_k + GGML_PAD(elem_k * tsize, GGML_MEM_ALIGN);\n\n    void * grad_q = (char *) dst->data;\n    void * grad_k = (char *) dst->data + offs_k;\n    void * grad_v = (char *) dst->data + offs_v;\n\n    const size_t nbgq1 = nb0*neq0;\n    const size_t nbgq2 = nb0*neq0*neq1;\n    const size_t nbgq3 = nb0*neq0*neq1*neq2;\n\n    const size_t nbgk1 = nb0*nek0;\n    const size_t nbgk2 = nb0*nek0*nek1;\n    const size_t nbgk3 = nb0*nek0*nek1*neq2;\n\n    const size_t nbgv1 = nb0*nev0;\n    const size_t nbgv2 = nb0*nev0*nev1;\n    const size_t nbgv3 = nb0*nev0*nev1*neq2;\n\n    // parallelize by k rows using ggml_vec_dot_f32\n\n    // total rows in k\n    const int nr = nek2*nek3;\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    const float scale = 1.0f/sqrtf(D);\n\n    //printf(\"P=%d N=%d D=%d ir0=%d ir1=%d scale = %f\\n\", P, N, D, ir0, ir1, scale);\n\n    // how often k2 (and v2) is repeated in q2\n    int nrep = neq2/nek2;\n\n    for (int ir = ir0; ir < ir1; ++ir) {\n        // q indices\n        const int ik3 = ir/(nek2);\n        const int ik2 = ir - ik3*nek2;\n\n        const int iq3 = ik3;\n        const int id3 = ik3;\n        const int iv3 = ik3;\n        const int iv2 = ik2;\n\n        for (int irep = 0; irep < nrep; ++irep) {\n            const int iq2 = ik2 + irep*nek2;\n            const int id2 = iq2;\n\n            // (ik2 + irep*nek2) % nek2 == ik2\n            for (int iq1 = 0; iq1 < neq1; ++iq1) {\n                const int id1 = iq1;\n\n                // not sure about CACHE_LINE_SIZE_F32..\n                // - maybe it must not be multiplied by 2 and excluded from .. in SM 1*(..) offset?\n                float * S  = (float *) params->wdata + ith*2*(mxDM + CACHE_LINE_SIZE_F32) + 0*(mxDM+CACHE_LINE_SIZE_F32);\n                float * SM = (float *) params->wdata + ith*2*(mxDM + CACHE_LINE_SIZE_F32) + 1*(mxDM+CACHE_LINE_SIZE_F32);\n\n                for (int i = M; i < Mup; ++i) {\n                    S[i] = -INFINITY;\n                }\n\n                const int64_t masked_begin = masked ? (P + iq1 + 1) : M;\n                for (int64_t ic = 0; ic < masked_begin; ++ic) {\n                    // k indices\n                    const int ik1 = ic;\n\n                    // S indices\n                    const int i1 = ik1;\n\n                    ggml_vec_dot_f32(neq0,\n                            S + i1,\n                            (float *) ((char *) k->data + (ik1*nbk1 + ik2*nbk2 + ik3*nbk3)),\n                            (float *) ((char *) q->data + (iq1*nbq1 + iq2*nbq2 + iq3*nbq3)));\n                }\n\n                // scale\n                ggml_vec_scale_f32(masked_begin, S, scale);\n\n                for (int64_t i = masked_begin; i < M; i++) {\n                    S[i] = -INFINITY;\n                }\n\n                // softmax\n                // exclude known -INF S[..] values from max and loop\n                // dont forget to set their SM values to zero\n                {\n                    float max = -INFINITY;\n                    ggml_vec_max_f32(masked_begin, &max, S);\n\n                    ggml_float sum = 0.0;\n                    {\n#ifdef GGML_SOFT_MAX_ACCELERATE\n                        max = -max;\n                        vDSP_vsadd(SM, 1, &max, SM, 1, Mup);\n                        vvexpf(SM, SM, &Mup);\n                        ggml_vec_sum_f32(Mup, &sum, SM);\n#else\n                        uint16_t   scvt[GGML_SOFT_MAX_UNROLL]; UNUSED(scvt);\n                        ggml_float sump[GGML_SOFT_MAX_UNROLL] = { 0.0 };\n\n                        for (int i = 0; i < Mup; i += GGML_SOFT_MAX_UNROLL) {\n                            if (i >= masked_begin) {\n                                break;\n                            }\n                            float * SR =  S + i;\n                            float * SW = SM + i;\n\n                            for (int j = 0; j < GGML_SOFT_MAX_UNROLL; ++j) {\n                                if (i + j >= masked_begin) {\n                                    break;\n                                } else if (SR[j] == -INFINITY) {\n                                    SW[j] = 0.0f;\n                                } else {\n#ifndef GGML_FLASH_ATTN_EXP_FP16\n                                    const float val = expf(SR[j] - max);\n#else\n                                    ggml_fp16_t s = GGML_FP32_TO_FP16(SR[j] - max);\n                                    memcpy(&scvt[j], &s, sizeof(uint16_t));\n                                    const float val = GGML_FP16_TO_FP32(ggml_table_exp_f16[scvt[j]]);\n#endif\n                                    sump[j] += (ggml_float)val;\n                                    SW[j] = val;\n                                }\n                            }\n                        }\n\n                        for (int i = 0; i < GGML_SOFT_MAX_UNROLL; i++) {\n                            sum += sump[i];\n                        }\n#endif\n                    }\n\n                    assert(sum > 0.0);\n\n                    sum = 1.0/sum;\n                    ggml_vec_scale_f32(masked_begin, SM, sum);\n\n                }\n\n                // step-by-step explanation\n                {\n                    // forward-process                    shape      grads from backward process\n                    // parallel_for ik2,ik3:\n                    //  for irep:\n                    //   iq2 = ik2 + irep*nek2\n                    //   k[:D,:M,:,:]                     [D,M,:,:]  grad[k][:D,:M,ik2,ik3]  += grad[kcur]\n                    //   q[:D,:N,:,:]                     [D,N,:,:]  grad[q][:D,iq1,iq2,iq3] += grad[qcur]\n                    //   v[:M,:D,:,:]                     [M,D,:,:]  grad[v][:M,:D,iv2,iv3]  += grad[vcur]\n                    //   for iq1:\n                    //    kcur   = k[:D,:M,ik2,ik3]       [D,M,1,1]  grad[kcur] = grad[S1].T @ qcur\n                    //    qcur   = q[:D,iq1,iq2,iq3]      [D,1,1,1]  grad[qcur] = grad[S1]   @ kcur\n                    //    vcur   = v[:M,:D,iv2,iv3]       [M,D,1,1]  grad[vcur] = grad[S5].T @ S4\n                    //    S0     = -Inf                   [D,1,1,1]\n                    //   ~S1[i]  = dot(kcur[:D,i], qcur)\n                    //    S1     = qcur @ kcur.T          [M,1,1,1]  grad[S1]   = grad[S2] * scale\n                    //    S2     = S1 * scale             [M,1,1,1]  grad[S2]   = diag_mask_zero(grad[S3], P)\n                    //    S3     = diag_mask_inf(S2, P)   [M,1,1,1]  grad[S3]   = S4 * (grad[S4] - dot(S4, grad[S4]))\n                    //    S4     = softmax(S3)            [M,1,1,1]  grad[S4]   = grad[S5] @ vcur\n                    //   ~S5[i]  = dot(vcur[:,i], S4)\n                    //    S5     = S4 @ vcur.T            [D,1,1,1]  grad[S5]   = d[:D,id1,id2,id3]\n                    //   ~dst[i,iq1,iq2,iq3]  = S5[i]              ^\n                    //    dst[:D,iq1,iq2,iq3] = S5                 | grad[dst[:D,iq1,iq2,iq3]] = d[:D,id1,id2,id3]\n                    // dst                               backward-/ grad[dst]                 = d\n                    //\n                    // output gradients with their dependencies:\n                    //\n                    // grad[kcur] = grad[S1].T @ qcur\n                    // grad[S1]   = diag_mask_zero(grad[S3], P) * scale\n                    // grad[S3]   = S4 * (grad[S4] - dot(S4, grad[S4]))\n                    // grad[S4]   = grad[S5] @ vcur\n                    // grad[S4]   = d[:D,id1,id2,id3] @ vcur\n                    // grad[qcur] = grad[S1]   @ kcur\n                    // grad[vcur] = grad[S5].T @ S4\n                    // grad[vcur] = d[:D,id1,id2,id3].T @ S4\n                    //\n                    // in post-order:\n                    //\n                    // S1         = qcur @ kcur.T\n                    // S2         = S1 * scale\n                    // S3         = diag_mask_inf(S2, P)\n                    // S4         = softmax(S3)\n                    // grad[S4]   = d[:D,id1,id2,id3] @ vcur\n                    // grad[S3]   = S4 * (grad[S4] - dot(S4, grad[S4]))\n                    // grad[S1]   = diag_mask_zero(grad[S3], P) * scale\n                    // grad[qcur] = grad[S1]   @ kcur\n                    // grad[kcur] = grad[S1].T @ qcur\n                    // grad[vcur] = d[:D,id1,id2,id3].T @ S4\n                    //\n                    // using less variables (SM=S4):\n                    //\n                    // S             = diag_mask_inf(qcur @ kcur.T * scale, P)\n                    // SM            = softmax(S)\n                    // S             = d[:D,iq1,iq2,iq3] @ vcur\n                    // dot_SM_gradSM = dot(SM, S)\n                    // S             = SM * (S - dot(SM, S))\n                    // S             = diag_mask_zero(S, P) * scale\n                    //\n                    // grad[q][:D,iq1,iq2,iq3] += S   @ kcur\n                    // grad[k][:D,:M,ik2,ik3]  += S.T @ qcur\n                    // grad[v][:M,:D,iv2,iv3]  += d[:D,id1,id2,id3].T @ SM\n                }\n\n                // S = gradSM = d[:D,id1,id2,id3] @ vcur[:,:,iv2,iv3]\n                // S = d[:D,id1,id2,id3] @ vcur[:,:,iv2,iv3]\n                // for ic:\n                //   S[:M] += vcur[:M,ic,iv2,iv3] * d[ic,id1,id2,id3]\n                // exclude known future zero S[..] values from operation\n                ggml_vec_set_f32(masked_begin, S, 0);\n                for (int64_t ic = 0; ic < D; ++ic) {\n                    ggml_vec_mad_f32(masked_begin,\n                            S,\n                             (float *) ((char *) v->data + (          ic*nbv1  + iv2*nbv2 + iv3*nbv3)),\n                            *(float *) ((char *) d->data + (ic*nbd0 + id1*nbd1 + id2*nbd2 + id3*nbd3)));\n                }\n\n                // S = SM * (S - dot(SM, S))\n                float dot_SM_gradSM = 0;\n                ggml_vec_dot_f32 (masked_begin, &dot_SM_gradSM, SM, S);\n                ggml_vec_acc1_f32(M, S, -dot_SM_gradSM);\n                ggml_vec_mul_f32 (masked_begin, S, S, SM);\n\n                // S = diag_mask_zero(S, P) * scale\n                // already done by above ggml_vec_set_f32\n\n                // exclude known zero S[..] values from operation\n                ggml_vec_scale_f32(masked_begin, S, scale);\n\n                // S    shape [M,1]\n                // SM   shape [M,1]\n                // kcur shape [D,M]\n                // qcur shape [D,1]\n                // vcur shape [M,D]\n\n                // grad[q][:D,iq1,iq2,iq3] += S @ kcur\n                // grad[q][:D,iq1,iq2,iq3] += shape[M,1] @ shape[D,M]\n                // for ic:\n                //  grad[q][:D,iq1,iq2,iq3] += S[ic] * kcur[:D,ic,ik2,ik3]\n                // exclude known zero S[..] values from loop\n                for (int64_t ic = 0; ic < masked_begin; ++ic) {\n                    ggml_vec_mad_f32(D,\n                            (float *) ((char *) grad_q  + (iq1*nbgq1 + iq2*nbgq2  + iq3*nbgq3)),\n                            (float *) ((char *) k->data + (ic*nbk1   + ik2*nbk2   + ik3*nbk3)),\n                            S[ic]);\n                }\n\n                // grad[k][:D,:M,iq2,iq3] += S.T @ qcur\n                // for ic:\n                //  grad[k][:D,ic,iq2,iq3] += S.T[0,ic] * qcur[:D,0]\n                //  grad[k][:D,ic,iq2,iq3] += S[ic]     * qcur[:D,0]\n                // exclude known zero S[..] values from loop\n                for (int64_t ic = 0; ic < masked_begin; ++ic) {\n                    ggml_vec_mad_f32(D,\n                            (float *) ((char *) grad_k  + (ic*nbgk1  + ik2*nbgk2  + ik3*nbgk3)),\n                            (float *) ((char *) q->data + (iq1*nbq1  + iq2*nbq2   + iq3*nbq3)),\n                            S[ic]);\n                }\n\n                // grad[v][:M,:D,iv2,iv3] += d[:D,id1,id2,id3].T       @ SM\n                // for ic:\n                //  grad[v][:M,ic,iv2,iv3] += d[:D,id1,id2,id3].T[0,ic] * SM[:M]\n                //  grad[v][:M,ic,iv2,iv3] += d[ic,id1,id2,id3]         * SM[:M]\n                // exclude known zero SM[..] values from mad\n                for (int64_t ic = 0; ic < D; ++ic) {\n                    ggml_vec_mad_f32(masked_begin,\n                            (float *) ((char *) grad_v   + (          ic*nbgv1 + iv2*nbgv2 + iv3*nbgv3)),\n                            SM,\n                            *(float *) ((char *) d->data + (ic*nbd0 + id1*nbd1 + id2*nbd2  + id3*nbd3)));\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_flash_attn_back(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * q,\n        const struct ggml_tensor * k,\n        const struct ggml_tensor * v,\n        const struct ggml_tensor * d,\n        const bool masked,\n        struct ggml_tensor * dst) {\n    switch (q->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_flash_attn_back_f32(params, q, k, v, d, masked, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_win_part\n\nstatic void ggml_compute_forward_win_part_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)\n    GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne)\n\n    const int32_t nep0 = ((const int32_t *)(dst->op_params))[0];\n    const int32_t nep1 = ((const int32_t *)(dst->op_params))[1];\n    const int32_t w    = ((const int32_t *)(dst->op_params))[2];\n\n    assert(ne00 == ne0);\n    assert(ne3  == nep0*nep1);\n\n    // TODO: optimize / multi-thread\n    for (int py = 0; py < nep1; ++py) {\n        for (int px = 0; px < nep0; ++px) {\n            const int64_t i3 = py*nep0 + px;\n            for (int64_t i2 = 0; i2 < ne2; ++i2) {\n                for (int64_t i1 = 0; i1 < ne1; ++i1) {\n                    for (int64_t i0 = 0; i0 < ne0; ++i0) {\n                        const int64_t i02 = py*w + i2;\n                        const int64_t i01 = px*w + i1;\n                        const int64_t i00 = i0;\n\n                        const int64_t i = i3*ne2*ne1*ne0 + i2*ne1*ne0    + i1*ne0   + i0;\n                        const int64_t j =                  i02*ne01*ne00 + i01*ne00 + i00;\n\n                        if (py*w + i2 >= ne02 || px*w + i1 >= ne01) {\n                            ((float *) dst->data)[i] = 0.0f;\n                        } else {\n                            ((float *) dst->data)[i] = ((float *) src0->data)[j];\n                        }\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_win_part(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_win_part_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_win_unpart\n\nstatic void ggml_compute_forward_win_unpart_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    GGML_TENSOR_LOCALS(int64_t, ne0, src0, ne)\n    GGML_TENSOR_LOCALS(int64_t, ne,  dst,  ne)\n\n    const int32_t w = ((const int32_t *)(dst->op_params))[0];\n\n    // padding\n    const int px = (w - ne1%w)%w;\n    //const int py = (w - ne2%w)%w;\n\n    const int npx = (px + ne1)/w;\n    //const int npy = (py + ne2)/w;\n\n    assert(ne0 == ne00);\n\n    // TODO: optimize / multi-thread\n    for (int64_t i2 = 0; i2 < ne2; ++i2) {\n        for (int64_t i1 = 0; i1 < ne1; ++i1) {\n            for (int64_t i0 = 0; i0 < ne0; ++i0) {\n                const int ip2 = i2/w;\n                const int ip1 = i1/w;\n\n                const int64_t i02 = i2%w;\n                const int64_t i01 = i1%w;\n                const int64_t i00 = i0;\n\n                const int64_t i = (ip2*npx + ip1)*ne02*ne01*ne00 + i02*ne01*ne00 + i01*ne00 + i00;\n                const int64_t j =                                  i2*ne1*ne0    + i1*ne0   + i0;\n\n                ((float *) dst->data)[j] = ((float *) src0->data)[i];\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_win_unpart(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_win_unpart_f32(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n//gmml_compute_forward_unary\n\nstatic void ggml_compute_forward_unary(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    const enum ggml_unary_op op = ggml_get_unary_op(dst);\n\n    switch (op) {\n        case GGML_UNARY_OP_ABS:\n            {\n                ggml_compute_forward_abs(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_SGN:\n            {\n                ggml_compute_forward_sgn(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_NEG:\n            {\n                ggml_compute_forward_neg(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_STEP:\n            {\n                ggml_compute_forward_step(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_TANH:\n            {\n                ggml_compute_forward_tanh(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_ELU:\n            {\n                ggml_compute_forward_elu(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_RELU:\n            {\n                ggml_compute_forward_relu(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_GELU:\n            {\n                ggml_compute_forward_gelu(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_GELU_QUICK:\n            {\n                ggml_compute_forward_gelu_quick(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_SILU:\n            {\n                ggml_compute_forward_silu(params, src0, dst);\n            } break;\n        case GGML_UNARY_OP_LEAKY:\n            {\n                ggml_compute_forward_leaky(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_get_rel_pos\n\nstatic void ggml_compute_forward_get_rel_pos_f16(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    // ref: https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/image_encoder.py#L292-L322\n\n    GGML_TENSOR_UNARY_OP_LOCALS\n\n    const int64_t w = ne1;\n\n    ggml_fp16_t * src0_data = (ggml_fp16_t *) src0->data;\n    ggml_fp16_t * dst_data  = (ggml_fp16_t *) dst->data;\n\n    for (int64_t i2 = 0; i2 < ne2; ++i2) {\n        for (int64_t i1 = 0; i1 < ne1; ++i1) {\n            const int64_t pos = (w - i1 - 1) + i2;\n            for (int64_t i0 = 0; i0 < ne0; ++i0) {\n                dst_data[i2*ne1*ne0 + i1*ne0 + i0] = src0_data[pos*ne00 + i0];\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_get_rel_pos(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F16:\n            {\n                ggml_compute_forward_get_rel_pos_f16(params, src0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_add_rel_pos\n\nstatic void ggml_compute_forward_add_rel_pos_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        const struct ggml_tensor * src2,\n        struct ggml_tensor * dst) {\n\n    const bool inplace = (bool) ((int32_t *) dst->op_params)[0];\n    if (!inplace && params->type == GGML_TASK_INIT) {\n        memcpy((char *) dst->data, (char *) src0->data, ggml_nbytes(dst));\n        return;\n    }\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    // ref: https://github.com/facebookresearch/segment-anything/blob/main/segment_anything/modeling/image_encoder.py#L357-L359\n\n    float * src1_data = (float *) src1->data;\n    float * src2_data = (float *) src2->data;\n    float * dst_data  = (float *) dst->data;\n\n    const int64_t ne10 = src1->ne[0];\n    const int64_t ne11 = src1->ne[1];\n    const int64_t ne12 = src1->ne[2];\n    const int64_t ne13 = src1->ne[3];\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    // total patches in dst\n    const int np = ne13;\n\n    // patches per thread\n    const int dp = (np + nth - 1)/nth;\n\n    // patch range for this thread\n    const int ip0 = dp*ith;\n    const int ip1 = MIN(ip0 + dp, np);\n\n    for (int64_t i13 = ip0; i13 < ip1; ++i13) {\n        for (int64_t i12 = 0; i12 < ne12; ++i12) {\n            for (int64_t i11 = 0; i11 < ne11; ++i11) {\n                const int64_t jp1 = i13*ne12*ne11*ne10 + i12*ne11*ne10 + i11*ne10;\n                for (int64_t i10 = 0; i10 < ne10; ++i10) {\n                    const int64_t jp0  = jp1 + i10;\n                    const float src1_e = src1_data[jp0];\n                    const float src2_e = src2_data[jp0];\n\n                    const int64_t jdh = jp0 * ne10;\n                    const int64_t jdw = jdh - (ne10 - 1) * i10;\n\n                    for (int64_t j = 0; j < ne10; ++j) {\n                        dst_data[jdh + j     ] += src2_e;\n                        dst_data[jdw + j*ne10] += src1_e;\n                    }\n                }\n            }\n        }\n    }\n}\n\nstatic void ggml_compute_forward_add_rel_pos(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        const struct ggml_tensor * src2,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_add_rel_pos_f32(params, src0, src1, src2, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_map_unary\n\nstatic void ggml_compute_forward_map_unary_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst,\n        const ggml_unary_op_f32_t fun) {\n    GGML_ASSERT(ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert( dst->nb[0] == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        fun(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_map_unary(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        struct ggml_tensor * dst,\n        const ggml_unary_op_f32_t fun) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_map_unary_f32(params, src0, dst, fun);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_map_binary\n\nstatic void ggml_compute_forward_map_binary_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst,\n        const ggml_binary_op_f32_t fun) {\n    assert(params->ith == 0);\n    assert(ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst));\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const int n  = ggml_nrows(src0);\n    const int nc = src0->ne[0];\n\n    assert( dst->nb[0] == sizeof(float));\n    assert(src0->nb[0] == sizeof(float));\n    assert(src1->nb[0] == sizeof(float));\n\n    for (int i = 0; i < n; i++) {\n        fun(nc,\n                (float *) ((char *) dst->data  + i*( dst->nb[1])),\n                (float *) ((char *) src0->data + i*(src0->nb[1])),\n                (float *) ((char *) src1->data + i*(src1->nb[1])));\n    }\n}\n\nstatic void ggml_compute_forward_map_binary(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst,\n        const ggml_binary_op_f32_t fun) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_map_binary_f32(params, src0, src1, dst, fun);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_map_custom1\n\nstatic void ggml_compute_forward_map_custom1_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * a,\n        struct ggml_tensor * dst,\n        const ggml_custom1_op_f32_t fun) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    fun(dst, a);\n}\n\n// ggml_compute_forward_map_custom2\n\nstatic void ggml_compute_forward_map_custom2_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * a,\n        const struct ggml_tensor * b,\n        struct ggml_tensor * dst,\n        const ggml_custom2_op_f32_t fun) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    fun(dst, a, b);\n}\n\n// ggml_compute_forward_map_custom3\n\nstatic void ggml_compute_forward_map_custom3_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * a,\n        const struct ggml_tensor * b,\n        const struct ggml_tensor * c,\n        struct ggml_tensor * dst,\n        const ggml_custom3_op_f32_t fun) {\n    assert(params->ith == 0);\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    fun(dst, a, b, c);\n}\n\n// ggml_compute_forward_map_custom1\n\nstatic void ggml_compute_forward_map_custom1(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * a,\n              struct ggml_tensor * dst) {\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    struct ggml_map_custom1_op_params * p = (struct ggml_map_custom1_op_params *) dst->op_params;\n\n    p->fun(dst, a, params->ith, params->nth, p->userdata);\n}\n\n// ggml_compute_forward_map_custom2\n\nstatic void ggml_compute_forward_map_custom2(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * a,\n        const struct ggml_tensor * b,\n              struct ggml_tensor * dst) {\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    struct ggml_map_custom2_op_params * p = (struct ggml_map_custom2_op_params *) dst->op_params;\n\n    p->fun(dst, a, b, params->ith, params->nth, p->userdata);\n}\n\n// ggml_compute_forward_map_custom3\n\nstatic void ggml_compute_forward_map_custom3(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * a,\n        const struct ggml_tensor * b,\n        const struct ggml_tensor * c,\n              struct ggml_tensor * dst) {\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    struct ggml_map_custom3_op_params * p = (struct ggml_map_custom3_op_params *) dst->op_params;\n\n    p->fun(dst, a, b, c, params->ith, params->nth, p->userdata);\n}\n\n// ggml_compute_forward_cross_entropy_loss\n\nstatic void ggml_compute_forward_cross_entropy_loss_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous(src0));\n    GGML_ASSERT(ggml_is_contiguous(src1));\n    GGML_ASSERT(ggml_is_scalar(dst));\n    GGML_ASSERT(ggml_are_same_shape(src0, src1));\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    float * sums = (float *) params->wdata;\n\n    // TODO: handle transposed/permuted matrices\n    const int nc = src0->ne[0];\n    const int nr = ggml_nrows(src0);\n\n    GGML_ASSERT(params->wsize >= sizeof(float) * (nth + nth * nc));\n\n    if (params->type == GGML_TASK_INIT) {\n        if (ith == 0) {\n            memset(sums, 0, sizeof(float) * (nth + nth * nc));\n        }\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        if (ith == 0) {\n            float * dp = (float *) dst->data;\n            ggml_vec_sum_f32(nth, dp, sums);\n            dp[0] *= -1.0f / (float) nr;\n        }\n        return;\n    }\n\n    const double eps = 1e-9;\n\n    // rows per thread\n    const int dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int ir0 = dr*ith;\n    const int ir1 = MIN(ir0 + dr, nr);\n\n    for (int i1 = ir0; i1 < ir1; i1++) {\n        float * s0 = (float *)((char *) src0->data + i1*src0->nb[1]);\n        float * s1 = (float *)((char *) src1->data + i1*src1->nb[1]);\n        float * st = ((float *) params->wdata) + nth + ith*nc;\n\n#ifndef NDEBUG\n        for (int i = 0; i < nc; ++i) {\n            //printf(\"p[%d] = %f\\n\", i, p[i]);\n            assert(!isnan(s0[i]));\n            assert(!isnan(s1[i]));\n        }\n#endif\n        // soft_max\n        ggml_float sum = 0.0;\n        {\n            float max = -INFINITY;\n            ggml_vec_max_f32(nc, &max, s0);\n\n            uint16_t scvt; UNUSED(scvt);\n            for (int i = 0; i < nc; i++) {\n                if (s0[i] == -INFINITY) {\n                    st[i] = 0.0f;\n                } else {\n#ifndef GGML_CROSS_ENTROPY_EXP_FP16\n                    const float s = s0[i] - max;\n                    const float val = expf(s);\n#else\n                    ggml_fp16_t s = GGML_FP32_TO_FP16(s0[i] - max);\n                    memcpy(&scvt, &s, sizeof(scvt));\n                    const float val = GGML_FP16_TO_FP32(ggml_table_exp_f16[scvt]);\n#endif\n                    sum += (ggml_float)val;\n                    st[i] = val;\n                }\n            }\n\n            assert(sum > 0.0);\n            // sum = 1.0/sum;\n        }\n        // avoid log(0) by rescaling from [0..1] to [eps..1]\n        sum = (1.0 - eps) / sum;\n        ggml_vec_scale_f32(nc, st, sum);\n        ggml_vec_add1_f32(nc, st, st, eps);\n        ggml_vec_log_f32(nc, st, st);\n        ggml_vec_mul_f32(nc, st, st, s1);\n\n        float st_sum = 0;\n        ggml_vec_sum_f32(nc, &st_sum, st);\n        sums[ith] += st_sum;\n\n#ifndef NDEBUG\n        for (int i = 0; i < nc; ++i) {\n            assert(!isnan(st[i]));\n            assert(!isinf(st[i]));\n        }\n#endif\n    }\n\n}\n\nstatic void ggml_compute_forward_cross_entropy_loss(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_cross_entropy_loss_f32(params, src0, src1, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n// ggml_compute_forward_cross_entropy_loss_back\n\nstatic void ggml_compute_forward_cross_entropy_loss_back_f32(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        const struct ggml_tensor * opt0,\n        struct ggml_tensor * dst) {\n    GGML_ASSERT(ggml_is_contiguous(dst));\n    GGML_ASSERT(ggml_is_contiguous(src0));\n    GGML_ASSERT(ggml_is_contiguous(src1));\n    GGML_ASSERT(ggml_is_contiguous(opt0));\n    GGML_ASSERT(ggml_are_same_shape(src0, src1) && ggml_are_same_shape(src0, dst));\n\n    const int64_t ith = params->ith;\n    const int64_t nth = params->nth;\n\n    if (params->type == GGML_TASK_INIT || params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const double eps = 1e-9;\n\n    // TODO: handle transposed/permuted matrices\n    const int64_t nc = src0->ne[0];\n    const int64_t nr = ggml_nrows(src0);\n\n    // rows per thread\n    const int64_t dr = (nr + nth - 1)/nth;\n\n    // row range for this thread\n    const int64_t ir0 = dr*ith;\n    const int64_t ir1 = MIN(ir0 + dr, nr);\n\n    float * d   = (float *) opt0->data;\n\n    for (int64_t i1 = ir0; i1 < ir1; i1++) {\n        float * ds0 = (float *)((char *) dst->data  + i1*dst->nb[1]);\n        float * s0  = (float *)((char *) src0->data + i1*src0->nb[1]);\n        float * s1  = (float *)((char *) src1->data + i1*src1->nb[1]);\n\n#ifndef NDEBUG\n        for (int i = 0; i < nc; ++i) {\n            //printf(\"p[%d] = %f\\n\", i, p[i]);\n            assert(!isnan(s0[i]));\n            assert(!isnan(s1[i]));\n        }\n#endif\n\n        // soft_max\n        ggml_float sum = 0.0;\n        {\n            float max = -INFINITY;\n            ggml_vec_max_f32(nc, &max, s0);\n\n            uint16_t scvt; UNUSED(scvt);\n            for (int i = 0; i < nc; i++) {\n                if (s0[i] == -INFINITY) {\n                    ds0[i] = 0.0f;\n                } else {\n#ifndef GGML_CROSS_ENTROPY_EXP_FP16\n                    const float s = s0[i] - max;\n                    const float val = expf(s);\n#else\n                    ggml_fp16_t s = GGML_FP32_TO_FP16(s0[i] - max);\n                    memcpy(&scvt, &s, sizeof(scvt));\n                    const float val = GGML_FP16_TO_FP32(ggml_table_exp_f16[scvt]);\n#endif\n                    sum += (ggml_float)val;\n                    ds0[i] = val;\n                }\n            }\n\n            assert(sum > 0.0);\n            sum = (1.0 - eps)/sum;\n        }\n\n        // grad(src0) = (softmax(src0) - src1) * grad(cross_entropy_loss(src0, src1)) / nr\n        ggml_vec_scale_f32(nc, ds0, sum);\n        ggml_vec_add1_f32(nc, ds0, ds0, eps);\n        ggml_vec_sub_f32(nc, ds0, ds0, s1);\n        ggml_vec_scale_f32(nc, ds0, d[0] / (float) nr);\n\n#ifndef NDEBUG\n        for (int i = 0; i < nc; ++i) {\n            assert(!isnan(ds0[i]));\n            assert(!isinf(ds0[i]));\n        }\n#endif\n    }\n}\n\nstatic void ggml_compute_forward_cross_entropy_loss_back(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n        const struct ggml_tensor * opt0,\n        struct ggml_tensor * dst) {\n    switch (src0->type) {\n        case GGML_TYPE_F32:\n            {\n                ggml_compute_forward_cross_entropy_loss_back_f32(params, src0, src1, opt0, dst);\n            } break;\n        default:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n\nstatic void ggml_compute_forward_mul_mat_sparse_head(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const enum ggml_type type = src0->type;\n\n    const bool src1_cont = ggml_is_contiguous(src1);\n\n    ggml_vec_dot_t    const vec_dot               = type_traits[type].vec_dot;\n    enum ggml_type    const vec_dot_type          = type_traits[type].vec_dot_type;\n    ggml_from_float_t const from_float_to_vec_dot = type_traits[vec_dot_type].from_float;\n\n    GGML_ASSERT(ne0 == ne01);\n    GGML_ASSERT(ne1 == ne11);\n    GGML_ASSERT(ne2 == ne12);\n    GGML_ASSERT(ne3 == ne13);\n\n    // we don't support permuted src0 or src1\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    // broadcast factors\n    const int64_t r2 = ne12/ne02;\n    const int64_t r3 = ne13/ne03;\n\n    // nb01 >= nb00 - src0 is not transposed\n    //   compute by src0 rows\n\n\n    if (params->type == GGML_TASK_INIT) {\n        if (src1->type != vec_dot_type) {\n            char * wdata = params->wdata;\n            const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n            for (int64_t i13 = 0; i13 < ne13; ++i13) {\n                for (int64_t i12 = 0; i12 < ne12; ++i12) {\n                    for (int64_t i11 = 0; i11 < ne11; ++i11) {\n                        from_float_to_vec_dot((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);\n                        wdata += row_size;\n                    }\n                }\n            }\n        }\n        ggml_set_zero(dst);\n        atomic_store(params->aic, 0);\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const void * wdata    = (src1->type == vec_dot_type) ? src1->data : params->wdata;\n    const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n    const int64_t nr0 = ne01;           // src0 rows\n    const int64_t nr1 = ne11*ne12*ne13; // src1 rows\n\n    //printf(\"nr0 = %lld, nr1 = %lld\\n\", nr0, nr1);\n\n    // distribute the thread work across the inner or outer loop based on which one is larger\n\n    const int64_t nth0 = nr0 > nr1 ? nth : 1; // parallelize by src0 rows\n    const int64_t nth1 = nr0 > nr1 ? 1 : nth; // parallelize by src1 rows\n\n    const int64_t ith0 = ith % nth0;\n    const int64_t ith1 = ith / nth0;\n\n    const int64_t dr0 = (nr0 + 8*nth0 - 1)/(8*nth0);\n    const int64_t dr1 = (nr1 + nth1 - 1)/nth1;\n\n    int64_t ir010 = dr0*ith0;\n    // const int64_t ir011 = MIN(ir010 + dr0, nr0);\n    // const int64_t ir011 = ir010 + dr0;\n\n    const int64_t ir110 = dr1*ith1;\n    const int64_t ir111 = MIN(ir110 + dr1, nr1);\n\n    //printf(\"ir010 = %6lld, ir011 = %6lld, ir110 = %6lld, ir111 = %6lld\\n\", ir010, ir011, ir110, ir111);\n\n    // threads with no work simply yield (not sure if it helps)\n    // if (ir010 >= ir011 || ir110 >= ir111) {\n    //     sched_yield();\n    //     return;\n    // }\n\n    assert(ne12 % ne02 == 0);\n    assert(ne13 % ne03 == 0);\n\n    // block-tiling attempt\n    // const int64_t blck_0 = 16;\n    const int64_t blck_1 = 16;\n\n    // attempt to reduce false-sharing (does not seem to make a difference)\n    // float tmp[16];\n    float *ffdata = (float *)dst->src[2]->data;\n    // int *gid = (int *)dst->src[3]->data;\n    while(true) {\n        ir010 = atomic_fetch_add(params->aic, dr0);\n        for (int64_t iir1 = ir110; iir1 < ir111; iir1 += blck_1) {\n            // for (int64_t iir0 = ir010; iir0 < ir011; iir0 += blck_0) {\n            // for (int64_t iir0 = ir010; iir0 < ir011;) {\n                for (int64_t ir1 = iir1; ir1 < iir1 + blck_1 && ir1 < ir111; ++ir1) {\n                    const int64_t i13 = (ir1/(ne12*ne11));\n                    const int64_t i12 = (ir1 - i13*ne12*ne11)/ne11;\n                    const int64_t i11 = (ir1 - i13*ne12*ne11 - i12*ne11);\n\n                    // broadcast src0 into src1\n                    const int64_t i03 = i13/r3;\n                    const int64_t i02 = i12/r2;\n\n                    const int64_t i1 = i11;\n                    const int64_t i2 = i12;\n                    const int64_t i3 = i13;\n\n                    const char * src0_row = (const char *) src0->data + (0 + i02*nb02 + i03*nb03);\n\n                    // desc: when src1 is not a contiguous memory block we have to calculate the offset using the strides\n                    //       if it is, then we have either copied the data to params->wdata and made it contiguous or we are using\n                    //       the original src1 data pointer, so we should index using the indices directly\n                    // TODO: this is a bit of a hack, we should probably have a better way to handle this\n                    const char * src1_col = (const char *) wdata +\n                        (src1_cont || src1->type != vec_dot_type\n                        ? (i11      + i12*ne11 + i13*ne12*ne11)*row_size\n                        : (i11*nb11 + i12*nb12 + i13*nb13));\n\n                    float * dst_col = (float *) ((char *) dst->data + (i1*nb1 + i2*nb2 + i3*nb3));\n\n                    //for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir011; ++ir0) {\n                    //    vec_dot(ne00, &dst_col[ir0], src0_row + ir0*nb01, src1_col);\n                    //}\n\n                    // for (int64_t ir0 = iir0; ir0 < iir0 + blck_0 && ir0 < ir011; ++ir0) {\n                    for (int64_t ir0 = ir010; ir0 < ir010+dr0; ++ir0) {\n                        if (ir0 > nr0)\n                            break;\n                        int id = ir0 >> 7;\n                        if (ffdata[id] < -7.0f)\n                        {\n                            dst_col[ir0] = 0;\n                            ir0 += 127;\n                            continue;\n                        }\n                        // vec_dot(ne00, &tmp[ir0 - iir0], src0_row + ir0*nb01, src1_col);\n                        vec_dot(ne00, &dst_col[ir0], src0_row + ir0*nb01, src1_col);\n                    }\n                    // memcpy(&dst_col[iir0], tmp, (MIN(iir0 + blck_0, ir011) - iir0)*sizeof(float));\n                }\n            // }\n        }\n        if (ir010 + dr0 >= nr0) {\n            break;\n        }\n        \n    } \n    \n\n}\n\nstatic void ggml_compute_forward_mul_mat_sparse(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const enum ggml_type type = src0->type;\n\n    const bool src1_cont = ggml_is_contiguous(src1);\n\n    ggml_vec_dot_t    const vec_dot               = type_traits[type].vec_dot;\n    enum ggml_type    const vec_dot_type          = type_traits[type].vec_dot_type;\n    ggml_from_float_t const from_float_to_vec_dot = type_traits[vec_dot_type].from_float;\n\n    const float threshold = sparse_pred_threshold;\n\n    GGML_ASSERT(ne0 == ne01);\n    GGML_ASSERT(ne1 == ne11);\n    GGML_ASSERT(ne2 == ne12);\n    GGML_ASSERT(ne3 == ne13);\n\n    // we don't support permuted src0 or src1\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    // broadcast factors\n    const int64_t r2 = ne12/ne02;\n    const int64_t r3 = ne13/ne03;\n\n    // nb01 >= nb00 - src0 is not transposed\n    //   compute by src0 rows\n\n#if defined(GGML_USE_CLBLAST)\n    if (ggml_cl_can_mul_mat(src0, src1, dst)) {\n        // TODO: handle case when src0 is broadcast-able into src1 across 2nd,3rd dimension\n        //       ref: https://github.com/ggerganov/ggml/pull/224\n        GGML_ASSERT(ne02 == ne12);\n        GGML_ASSERT(ne03 == ne13);\n\n        if (params->ith == 0 && params->type == GGML_TASK_COMPUTE) {\n            ggml_cl_mul_mat(src0, src1, dst, params->wdata, params->wsize);\n        }\n        return;\n    }\n#endif\n\n#if defined(GGML_USE_ACCELERATE) || defined(GGML_USE_OPENBLAS)\n    if (ggml_compute_forward_mul_mat_use_blas(src0, src1, dst)) {\n        if (params->ith != 0) {\n            return;\n        }\n\n        if (params->type == GGML_TASK_INIT) {\n            return;\n        }\n\n        if (params->type == GGML_TASK_FINALIZE) {\n            return;\n        }\n\n        for (int64_t i13 = 0; i13 < ne13; i13++) {\n            for (int64_t i12 = 0; i12 < ne12; i12++) {\n                // broadcast src0 into src1 across 2nd,3rd dimension\n                const int64_t i03 = i13/r3;\n                const int64_t i02 = i12/r2;\n\n                const void  * x = (char *)            src0->data + i02*nb02 + i03*nb03;\n                const float * y = (float *) ((char *) src1->data + i12*nb12 + i13*nb13);\n\n                float * d = (float *) ((char *) dst->data + i12*nb2 + i13*nb3);\n\n                if (type != GGML_TYPE_F32) {\n                            float * const wdata    = params->wdata;\n                    ggml_to_float_t const to_float = type_traits[type].to_float;\n\n                    size_t id = 0;\n                    for (int64_t i01 = 0; i01 < ne01; ++i01) {\n                        to_float((const char *) x + i01*nb01, wdata + id, ne00);\n                        id += ne00;\n                    }\n\n                    assert(id*sizeof(float) <= params->wsize);\n                    x = wdata;\n                }\n\n                cblas_sgemm(CblasRowMajor, CblasNoTrans, CblasTrans,\n                        ne11, ne01, ne10,\n                        1.0f,    y, ne10,\n                                 x, ne00,\n                        0.0f,    d, ne01);\n            }\n        }\n\n        //printf(\"CBLAS = %f ms, %d x %d x %d x %d\\n\", (ggml_perf_time_us() - t0)/1000.0, ne0, ne1, ne2, ne3);\n\n        return;\n    }\n#endif\n\n    if (params->type == GGML_TASK_INIT) {\n        if (src1->type != vec_dot_type) {\n            char * wdata = params->wdata;\n            const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n            for (int64_t i13 = 0; i13 < ne13; ++i13) {\n                for (int64_t i12 = 0; i12 < ne12; ++i12) {\n                    for (int64_t i11 = 0; i11 < ne11; ++i11) {\n                        from_float_to_vec_dot((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);\n                        wdata += row_size;\n                    }\n                }\n            }\n        }\n        atomic_store(params->aic, 0);\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const void * wdata    = (src1->type == vec_dot_type) ? src1->data : params->wdata;\n    const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n    const int64_t nr0 = ne01;           // src0 rows\n    const int64_t nr1 = ne11*ne12*ne13; // src1 rows\n\n\n    // distribute the thread work across the inner or outer loop based on which one is larger\n\n    const int64_t nth0 = nr0 > nr1 ? nth : 1; // parallelize by src0 rows\n    const int64_t nth1 = nr0 > nr1 ? 1 : nth; // parallelize by src1 rows\n\n    const int64_t ith0 = ith % nth0;\n    const int64_t ith1 = ith / nth0;\n\n    const int64_t dr0 = (nr0 + 8*nth0 - 1)/(8*nth0);\n    const int64_t dr1 = (nr1 + nth1 - 1)/nth1;\n    // const int64_t dr0 = (nr0 + nth0 - 1)/(nth0);\n    // const int64_t dr1 = (nr1 + nth1 - 1)/nth1;\n\n    int64_t ir010 = dr0*ith0;\n    int64_t ir011 = MIN(ir010 + dr0, nr0);\n    // const int64_t ir011 = ir010 + dr0;\n\n    const int64_t ir110 = dr1*ith1;\n    const int64_t ir111 = MIN(ir110 + dr1, nr1);\n\n    //printf(\"ir010 = %6lld, ir011 = %6lld, ir110 = %6lld, ir111 = %6lld\\n\", ir010, ir011, ir110, ir111);\n\n    // threads with no work simply yield (not sure if it helps)\n    // if (ir010 >= ir011 || ir110 >= ir111) {\n    //     sched_yield();\n    //     return;\n    // }\n\n    assert(ne12 % ne02 == 0);\n    assert(ne13 % ne03 == 0);\n\n    // block-tiling attempt\n    // const int64_t blck_0 = 16;\n    const int64_t blck_1 = 16;\n    // int total = 0;\n\n    // attempt to reduce false-sharing (does not seem to make a difference)\n    // float tmp[16];\n    float *ffdata = (float *)dst->src[2]->data;\n    int *gid = (int *)dst->src[3]->data;\n    float *predictor_data = (float *)dst->src[2]->data;\n    const size_t predictor_row_size = dst->src[2]->ne[0]*ggml_type_size(GGML_TYPE_F32)/ggml_blck_size(GGML_TYPE_F32);\n\n    while(true) {\n        ir010 = atomic_fetch_add(params->aic, dr0);\n        ir011 = MIN(ir010 + dr0, nr0);\n        for (int64_t ir0 = ir010; ir0 < ir011; ++ir0)\n        {\n            for (int64_t iir1 = ir110; iir1 < ir111; iir1 += blck_1)\n            {\n                    if (ir0 > nr0)\n                        break;\n                // for (int64_t iir0 = ir010; iir0 < ir011; iir0 += blck_0) {\n                // for (int64_t iir0 = ir010; iir0 < ir011;) {\n                for (int64_t ir1 = iir1; ir1 < iir1 + blck_1 && ir1 < ir111; ++ir1)\n                {\n                    const int64_t i13 = (ir1 / (ne12 * ne11));\n                    const int64_t i12 = (ir1 - i13 * ne12 * ne11) / ne11;\n                    const int64_t i11 = (ir1 - i13 * ne12 * ne11 - i12 * ne11);\n\n                    // broadcast src0 into src1\n                    const int64_t i03 = i13 / r3;\n                    const int64_t i02 = i12 / r2;\n\n                    const int64_t i1 = i11;\n                    const int64_t i2 = i12;\n                    const int64_t i3 = i13;\n\n                    const char *src0_row = (const char *)src0->data + (0 + i02 * nb02 + i03 * nb03);\n\n                    // desc: when src1 is not a contiguous memory block we have to calculate the offset using the strides\n                    //       if it is, then we have either copied the data to params->wdata and made it contiguous or we are using\n                    //       the original src1 data pointer, so we should index using the indices directly\n                    // TODO: this is a bit of a hack, we should probably have a better way to handle this\n                    const char *src1_col = (const char *)wdata +\n                                           (src1_cont || src1->type != vec_dot_type\n                                                ? (i11 + i12 * ne11 + i13 * ne12 * ne11) * row_size\n                                                : (i11 * nb11 + i12 * nb12 + i13 * nb13));\n                    ffdata = (float *)((char *)predictor_data + (i11      + i12*ne11 + i13*ne12*ne11)*predictor_row_size);\n                    // printf(\"ith %d row %d ir1 %d %d %d %d %d\\n\", ith, ir0, ir1, src1_col-(char *)wdata, ffdata-predictor_data, predictor_row_size, dst->src[2]->ne[1]);\n\n                    float *dst_col = (float *)((char *)dst->data + (i1 * nb1 + i2 * nb2 + i3 * nb3));\n\n                    // if (ffdata[ir0] <= 0.0f) {\n                    if (gid[ir0] == 1 || ffdata[ir0] < threshold) {\n                        dst_col[ir0] = 0;\n                        continue;\n                    }\n                    vec_dot(ne00, &dst_col[ir0], src0_row + ir0 * nb01, src1_col);\n                }\n                // }\n            }\n        }\n        if (ir010 + dr0 >= nr0) {\n            break;\n        }\n        \n    }\n    // printf(\"total %d\\n\", total);\n\n    // int predictor_cpu = 0;\n    // int predictor = 0;\n    // for (int i = 0; i < 9216 *4 ; i++) {\n    //     if (ffdata[i] > 0.5f && gid[i] == 0)\n    //         predictor_cpu += 1;\n    //     if (ffdata[i] > 0.5f)\n    //         predictor += 1;\n    // }\n    // if (ith == 0)\n    //     printf(\"predictor %d predictor_cpu %d\\n\", predictor, predictor_cpu);\n}\n\n// vz = alpha * vx + vy  \nstatic void ggml_axpy_normal_f16(const int n, const ggml_fp16_t * vx, const ggml_fp16_t * restrict vy, void* restrict vz, ggml_fp16_t alpha) {\n    float *res = (float *)vz;\n    for (int i = 0; i < n; i++) {\n            res[i] = res[i] + (GGML_FP16_TO_FP32(vx[i])*GGML_FP16_TO_FP32(alpha));\n    }\n    (void) vy;\n}\nstatic void ggml_axpy_avx_f16(const int n, const ggml_fp16_t * restrict vx, const ggml_fp16_t * vy, void* vz, ggml_fp16_t alpha) {\n#if defined(__AVX2__) \n    float *result = (float *)vz;\n    float alpha_f32 = GGML_FP16_TO_FP32(alpha);  \n    __m256 scale = _mm256_set1_ps(alpha_f32);  // åˆ›å»ºscaleå‘é‡\n    for (int i = 0; i < n; i += 8) {\n        __m128i vx_low = _mm_loadu_si128((__m128i const*)(&vx[i]));  \n        __m256 vx_f32 = _mm256_cvtph_ps(vx_low);  // è½¬æ¢vxä¸ºfp32\n        __m256 vy_f32 = _mm256_loadu_ps((float const*)(result+ i));  // åŠ è½½vy\n        __m256 res = _mm256_fmadd_ps(vx_f32, scale, vy_f32);  // æ‰§è¡Œå‘é‡åŠ æ³•å’Œä¹˜æ³•æ“ä½œ\n        _mm256_storeu_ps((float*)(&result[i]), res);  // å­˜å‚¨ç»“æžœ\n    }\n#else\n    float *res = (float *)vz;\n    float alpha_convert = GGML_FP16_TO_FP32(alpha);\n    for (int i = 0; i < n; i++) {\n        res[i] = res[i] + (GGML_FP16_TO_FP32(vx[i])*alpha_convert);\n    }\n#endif\n    (void)vy;\n}\n\nstatic void ggml_compute_forward_mul_mat_axpy(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const enum ggml_type type = src0->type;\n\n    // const bool src1_cont = ggml_is_contiguous(src1);\n\n    // ggml_vec_dot_t    const vec_dot               = type_traits[type].vec_dot;\n    enum ggml_type    const vec_dot_type          = type_traits[type].vec_dot_type;\n    ggml_from_float_t const from_float_to_vec_dot = type_traits[vec_dot_type].from_float;\n\n    const float threshold = sparse_pred_threshold;\n\n    // GGML_ASSERT(ne0 == ne01);\n    // GGML_ASSERT(ne1 == ne11);\n    // GGML_ASSERT(ne2 == ne12);\n    // GGML_ASSERT(ne3 == ne13);\n\n    // we don't support permuted src0 or src1\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    // broadcast factors\n    // const int64_t r2 = ne12/ne02;\n    // const int64_t r3 = ne13/ne03;\n\n    // nb01 >= nb00 - src0 is not transposed\n    //   compute by src0 rows\n\n    if (params->type == GGML_TASK_INIT) {\n        ggml_set_zero(dst);\n        if (src1->type != vec_dot_type) {\n            char * wdata = params->wdata;\n            const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n            for (int64_t i13 = 0; i13 < ne13; ++i13) {\n                for (int64_t i12 = 0; i12 < ne12; ++i12) {\n                    for (int64_t i11 = 0; i11 < ne11; ++i11) {\n                        from_float_to_vec_dot((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);\n                        wdata += row_size;\n                    }\n                }\n            }\n        }\n        atomic_store(params->aic, 0);\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    ggml_fp16_t* wdata    = (src1->type == vec_dot_type) ? src1->data : params->wdata;\n    const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n    struct ggml_tensor *src2 = dst->src[2];\n    \n    // parallelize by src0 rows\n    // const int64_t dr = (src2->ne[0] + 8*nth - 1)/(8*nth);\n    const int64_t dr = (ne01 + nth - 1)/(nth);\n    const int nr = ggml_nrows(src0);\n\n    const int64_t ir10 = dr*ith;\n    // const int64_t ir10 = dr*ith;\n    // const int64_t ir11 = MIN(ir10 + dr, src2->ne[0]);\n\n    // src1 rows\n    const int64_t nr1 = ne11*ne12*ne13;\n    float *sparse_idx = src2->data;\n    int idx_row_size = src2->nb[1];\n    int *gpu_idx = dst->src[3] ? (int *)(dst->src[3]->data) : NULL;\n\n#if defined(_MSC_VER)\n    float* vec = (float *)_malloca(ne00 * 4 * sizeof(float));\n#else\n    float vec[ne00*4];\n#endif\n    void *vy = vec;\n    char* src0_row = (char *) src0->data;\n    ggml_fp16_t * src1_ptr = NULL;\n    for (int col_idx = 0; col_idx < nr1; col_idx++) {\n        src1_ptr = (ggml_fp16_t *)((char *)wdata + col_idx * row_size);\n        sparse_idx = (float *)((char *)src2->data + col_idx * idx_row_size);\n        memset(vy, 0, ne00*4);\n        // maybe write a special axpy for batch 1\n        // while(true) {\n            // const int ir0 = atomic_fetch_add(params->aic, dr);\n            for (int64_t ir1 = ir10; ir1 < ir10+dr; ir1++) {\n                if (ir1 >= nr) {\n                    break;\n                }\n\t\t        if (src1_ptr[ir1]==0)\n\t\t\t        continue;\n                if (!gpu_idx || gpu_idx[ir1] == 1) {\n                    continue;\n                }\n                if (sparse_idx[ir1] < threshold)\n                    continue;\n                // ggml_axpy_normal_f16(ne00, src0_row+nb01*ir1, vy, vy, wdata[ir1]);\n                ggml_axpy_avx_f16(ne00, (ggml_fp16_t *)(src0_row+nb01*ir1), (ggml_fp16_t *)vy, vy, src1_ptr[ir1]);\n            }\n        \n        float *res = (float *)((char *)(dst->data) + col_idx * nb1);\n        float *tmp = (float *)vy;\n        int i;\n    \n\n        // è®¡ç®—å‰©ä½™çš„å…ƒç´ ä¸ªæ•°\n        int remainder = ne00 % 8;\n\n#if defined(__AVX2__)\n        // ä½¿ç”¨AVXæŒ‡ä»¤è¿›è¡Œå‘é‡åŒ–è®¡ç®—\n        for (i = 0; i < ne00 - remainder; i += 8) {\n            __m256 res_vec = _mm256_loadu_ps(res + i);  // åŠ è½½resä¸­çš„8ä¸ªæµ®ç‚¹æ•°\n            __m256 tmp_vec = _mm256_loadu_ps(tmp + i);  // åŠ è½½tmpä¸­çš„8ä¸ªæµ®ç‚¹æ•°\n            __m256 result = _mm256_add_ps(res_vec, tmp_vec);  // æ‰§è¡ŒåŠ æ³•è¿ç®—\n            _mm256_storeu_ps(res + i, result);  // å­˜å‚¨ç»“æžœåˆ°resä¸­\n        }\n\n        // å¤„ç†å‰©ä½™çš„å…ƒç´ \n        for (i = ne00 - remainder; i < ne00; i++) {\n            res[i] += tmp[i];\n        }\n#else\n        for (i = 0; i < ne00; i++) {\n            res[i] += tmp[i];\n        }\n#endif\n    }\n#if defined(_MSC_VER)\n    _freea(vec);\n#endif\n}\n\nstatic void ggml_compute_forward_mul_mat_axpy_q4_0(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS;\n\n    const int ith = params->ith;\n    const int nth = params->nth;\n\n    const enum ggml_type type = src0->type;\n\n    // const bool src1_cont = ggml_is_contiguous(src1);\n\n    // ggml_vec_dot_t    const vec_dot               = type_traits[type].vec_dot;\n    enum ggml_type    const vec_dot_type          = type_traits[type].vec_dot_type;\n    ggml_from_float_t const from_float_to_vec_dot = type_traits[vec_dot_type].from_float;\n\n    const float threshold = sparse_pred_threshold;\n\n    // GGML_ASSERT(ne0 == ne01);\n    // GGML_ASSERT(ne1 == ne11);\n    // GGML_ASSERT(ne2 == ne12);\n    // GGML_ASSERT(ne3 == ne13);\n\n    // we don't support permuted src0 or src1\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    // broadcast factors\n    // const int64_t r2 = ne12/ne02;\n    // const int64_t r3 = ne13/ne03;\n\n    // nb01 >= nb00 - src0 is not transposed\n    //   compute by src0 rows\n    if (params->type == GGML_TASK_INIT) {\n        ggml_set_zero(dst);\n        if (src1->type != vec_dot_type) {\n            char * wdata = params->wdata;\n            const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n            for (int64_t i13 = 0; i13 < ne13; ++i13) {\n                for (int64_t i12 = 0; i12 < ne12; ++i12) {\n                    for (int64_t i11 = 0; i11 < ne11; ++i11) {\n                        from_float_to_vec_dot((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);\n                        wdata += row_size;\n                    }\n                }\n            }\n        }\n        atomic_store(params->aic, 0);\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    ggml_fp16_t* wdata    = (src1->type == vec_dot_type) ? src1->data : params->wdata;\n    const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n    struct ggml_tensor *src2 = dst->src[2];\n    \n    // parallelize by src0 rows\n    // const int64_t dr = (src2->ne[0] + 8*nth - 1)/(8*nth);\n    const int64_t dr = (src2->ne[0] + nth - 1)/(nth);\n    const int nr = ggml_nrows(src0);\n\n    const int64_t ir10 = dr*ith;\n    // const int64_t ir11 = MIN(ir10 + dr, src2->ne[0]);\n\n    // src1 rows\n    const int64_t nr1 = ne11*ne12*ne13;\n    float *idx = src2->data;\n    int idx_row_size = src2->nb[1];\n    int *gid = (int *)(dst->src[3]->data);\n    // printf(\"down %d up %d ne00 %d\\n\", ir10, ir11, ne00);\n\n#if defined(_MSC_VER)\n    float* vec = (float *)_malloca(ne00 * 4 * sizeof(float));\n#else\n    float vec[ne00*4];\n#endif\n    void *vy = vec;\n    char* src0_row = (char *) src0->data;\n    for (int col_idx = 0; col_idx < nr1; col_idx++) {\n        // const block_q8_0 * restrict nerual = wdata;\n        const block_q8_0 *restrict nerual = (block_q8_0 *)((char *)wdata + col_idx * row_size);\n        idx = (float *)((char *)src2->data + col_idx * idx_row_size);\n        memset(vy, 0, ne00 * 4);\n        // while(true) {\n        //     const int ir0 = atomic_fetch_add(params->aic, dr);\n        for (int64_t ir1 = ir10; ir1 < ir10 + dr; ir1++)\n        {\n            if (ir1 >= nr)\n                break;\n            if (gid[ir1] == 1)\n                continue;\n            if (idx[ir1] < threshold)\n                continue;\n            int bid = ir1 / QK8_0;\n            int qsid = ir1 % QK8_0;\n            int b = (int)nerual[bid].qs[qsid];\n            if (b == 0)\n                continue;\n            ggml_fp16_t d = nerual[bid].d;\n            ggml_axpy_q4_0_q8_0(ne00, src0_row + nb01 * ir1, vy, vy, b, d);\n        }\n        //     if (ir0 + dr >= nr)\n        //         break;\n        // }\n\n        // float *res = (float *)(dst->data);\n        float *res = (float *)((char *)(dst->data) + col_idx * nb1);\n        float *tmp = (float *)vy;\n        int i;\n\n        // è®¡ç®—å‰©ä½™çš„å…ƒç´ ä¸ªæ•°\n        int remainder = ne00 % 8;\n#if defined(__AVX2__)\n        // ä½¿ç”¨AVXæŒ‡ä»¤è¿›è¡Œå‘é‡åŒ–è®¡ç®—\n        for (i = 0; i < ne00 - remainder; i += 8)\n        {\n            __m256 res_vec = _mm256_loadu_ps(res + i);       // åŠ è½½resä¸­çš„8ä¸ªæµ®ç‚¹æ•°\n            __m256 tmp_vec = _mm256_loadu_ps(tmp + i);       // åŠ è½½tmpä¸­çš„8ä¸ªæµ®ç‚¹æ•°\n            __m256 result = _mm256_add_ps(res_vec, tmp_vec); // æ‰§è¡ŒåŠ æ³•è¿ç®—\n            _mm256_storeu_ps(res + i, result);               // å­˜å‚¨ç»“æžœåˆ°resä¸­\n        }\n\n        // å¤„ç†å‰©ä½™çš„å…ƒç´ \n        for (i = ne00 - remainder; i < ne00; i++)\n        {\n            res[i] += tmp[i];\n        }\n#else\n        for (i = 0; i < ne00; i++) {\n            res[i] += tmp[i];\n        }\n#endif\n    }\n#if defined(_MSC_VER)\n    _freea(vec);\n#endif\n}\natomic_flag g_axpy_head_lock = ATOMIC_FLAG_INIT;\nstatic void ggml_compute_forward_mul_mat_axpy_head(\n        const struct ggml_compute_params * params,\n        const struct ggml_tensor * src0,\n        const struct ggml_tensor * src1,\n              struct ggml_tensor * dst) {\n    int64_t t0 = ggml_perf_time_us();\n    UNUSED(t0);\n\n    GGML_TENSOR_BINARY_OP_LOCALS;\n\n    // const int ith = params->ith;\n    // const int nth = params->nth;\n\n    const enum ggml_type type = src0->type;\n\n    // const bool src1_cont = ggml_is_contiguous(src1);\n\n    // ggml_vec_dot_t    const vec_dot               = type_traits[type].vec_dot;\n    enum ggml_type    const vec_dot_type          = type_traits[type].vec_dot_type;\n    ggml_from_float_t const from_float_to_vec_dot = type_traits[vec_dot_type].from_float;\n\n    // GGML_ASSERT(ne0 == ne01);\n    // GGML_ASSERT(ne1 == ne11);\n    // GGML_ASSERT(ne2 == ne12);\n    // GGML_ASSERT(ne3 == ne13);\n\n    // we don't support permuted src0 or src1\n    GGML_ASSERT(nb00 == ggml_type_size(type));\n    GGML_ASSERT(nb10 == sizeof(float));\n\n    // dst cannot be transposed or permuted\n    GGML_ASSERT(nb0 == sizeof(float));\n    GGML_ASSERT(nb0 <= nb1);\n    GGML_ASSERT(nb1 <= nb2);\n    GGML_ASSERT(nb2 <= nb3);\n\n    // broadcast factors\n    // const int64_t r2 = ne12/ne02;\n    // const int64_t r3 = ne13/ne03;\n\n    // nb01 >= nb00 - src0 is not transposed\n    //   compute by src0 rows\n\n    if (params->type == GGML_TASK_INIT) {\n        ggml_set_zero(dst);\n        if (src1->type != vec_dot_type) {\n            char * wdata = params->wdata;\n            const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n            for (int64_t i13 = 0; i13 < ne13; ++i13) {\n                for (int64_t i12 = 0; i12 < ne12; ++i12) {\n                    for (int64_t i11 = 0; i11 < ne11; ++i11) {\n                        from_float_to_vec_dot((float *)((char *) src1->data + i13*nb13 + i12*nb12 + i11*nb11), (void *) wdata, ne10);\n                        wdata += row_size;\n                    }\n                }\n            }\n        }\n        atomic_store(params->aic, 0);\n\n        return;\n    }\n\n    if (params->type == GGML_TASK_FINALIZE) {\n        return;\n    }\n\n    const ggml_fp16_t* wdata    = (src1->type == vec_dot_type) ? src1->data : params->wdata;\n    // const size_t row_size = ne10*ggml_type_size(vec_dot_type)/ggml_blck_size(vec_dot_type);\n\n    struct ggml_tensor *src2 = dst->src[2];\n    int chunk = ne00 / 32;\n    \n    // parallelize by src0 rows\n    const int64_t dr = (src2->ne[0] + chunk - 1)/chunk;\n    const int nr = ggml_nrows(src0);\n\n    // const int64_t ir10 = dr*ith;\n    // const int64_t ir11 = MIN(ir10 + dr, src2->ne[0]);\n\n    // src1 rows\n    // const int64_t nr1 = ne11*ne12*ne13;\n    // float *idx = src2->data;\n    // int *gid = (int *)(dst->src[3]->data);\n    // printf(\"down %d up %d ne00 %d\\n\", ir10, ir11, ne00);\n\n#if defined(_MSC_VER)\n    float* vec = (float *)_malloca(ne00 * 4 * sizeof(float));\n#else\n    float vec[ne00*4];\n#endif\n    void *vy = vec;\n    memset(vy, 0, ne00*4);\n    char* src0_row = (char *) src0->data;\n    while (true) {\n        const int ir0 = atomic_fetch_add(params->aic, dr);\n        // int id = ir0 >> 7;\n        // if (idx[id] < -15.0f)\n        //     continue;\n        for (int64_t ir1 = ir0; ir1 < ir0+dr; ir1++) {\n            if (ir1 >= nr) break;\n            // ggml_axpy_normal_f16(ne00, src0_row+nb01*ir1, vy, vy, wdata[ir1]);\n            ggml_axpy_avx_f16(ne00, (ggml_fp16_t *)(src0_row+nb01*ir1), (ggml_fp16_t *)vy, vy, wdata[ir1]);\n        }\n        if (ir0 + dr >= nr)\n            break;\n    }\n    \n    // èŽ·å–é”\n    while (atomic_flag_test_and_set(&g_axpy_head_lock)) {\n        // å¦‚æžœé”å·²ç»è¢«å ç”¨ï¼Œåˆ™ç­‰å¾…\n    }\n    float *res = (float *)(dst->data);\n    float *tmp = (float *)vy;\n    int i;\n \n\n    // è®¡ç®—å‰©ä½™çš„å…ƒç´ ä¸ªæ•°\n    int remainder = ne00 % 8;\n\n#if defined(__AVX2__)\n    // ä½¿ç”¨AVXæŒ‡ä»¤è¿›è¡Œå‘é‡åŒ–è®¡ç®—\n    for (i = 0; i < ne00 - remainder; i += 8) {\n        __m256 res_vec = _mm256_loadu_ps(res + i);  // åŠ è½½resä¸­çš„8ä¸ªæµ®ç‚¹æ•°\n        __m256 tmp_vec = _mm256_loadu_ps(tmp + i);  // åŠ è½½tmpä¸­çš„8ä¸ªæµ®ç‚¹æ•°\n        __m256 result = _mm256_add_ps(res_vec, tmp_vec);  // æ‰§è¡ŒåŠ æ³•è¿ç®—\n        _mm256_storeu_ps(res + i, result);  // å­˜å‚¨ç»“æžœåˆ°resä¸­\n    }\n\n    // å¤„ç†å‰©ä½™çš„å…ƒç´ \n    for (i = ne00 - remainder; i < ne00; i++) {\n        res[i] += tmp[i];\n    }\n#else\n    for (i = 0; i < ne00; i++) {\n        res[i] += tmp[i];\n    }\n#endif\n    atomic_flag_clear(&g_axpy_head_lock);\n#if defined(_MSC_VER)\n    _freea(vec);\n#endif\n}\n\n/////////////////////////////////\n\nstatic void ggml_ensure_tensor_data_at_memory(struct ggml_tensor * tensor) {\n#if defined(GGML_USE_CUBLAS)\n    if (tensor->backend == GGML_BACKEND_CPU) {\n        // in this case, the data is already placed in the memory at compute time\n        return;\n    }\n\n    if (tensor->buffer == NULL || tensor->data == NULL) {\n        GGML_ASSERT(false && \"not implemented: tensor has no buffer or data\");\n    }\n\n    fprintf(stderr, \"WARNING: transfering tensor %s to CPU at inference is safe but slow\\n\", ggml_get_name(tensor));\n    ggml_cuda_copy_to_host(tensor);\n#else\n    UNUSED(tensor);\n#endif\n}\n\nstatic void ggml_compute_forward(struct ggml_compute_params * params, struct ggml_tensor * tensor) {\n    GGML_ASSERT(params);\n\n    if (tensor->op == GGML_OP_NONE) {\n        return;\n    }\n\n#ifdef GGML_USE_CUBLAS\n    bool skip_cpu = ggml_cuda_compute_forward(params, tensor);\n    if (skip_cpu) {\n        return;\n    }\n    // Make sure src[0] (weight for binary ops) is on CPU to avoid any weight transfer\n    GGML_ASSERT((tensor->src[0] == NULL || tensor->src[0]->backend == GGML_BACKEND_CPU) && \"weight should be on the CPU to compute on the CPU\");\n#endif // GGML_USE_CUBLAS\n\n    switch (tensor->op) {\n        case GGML_OP_DUP:\n            {\n                ggml_compute_forward_dup(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_ADD:\n            {\n                ggml_compute_forward_add(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_ADD1:\n            {\n                ggml_compute_forward_add1(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_ACC:\n            {\n                ggml_compute_forward_acc(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_SUB:\n            {\n                ggml_compute_forward_sub(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_MUL:\n            {\n                ggml_compute_forward_mul(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_DIV:\n            {\n                ggml_compute_forward_div(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_SQR:\n            {\n                ggml_compute_forward_sqr(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_SQRT:\n            {\n                ggml_compute_forward_sqrt(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_LOG:\n            {\n                ggml_compute_forward_log(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_SUM:\n            {\n                ggml_compute_forward_sum(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_SUM_ROWS:\n            {\n                ggml_compute_forward_sum_rows(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_MEAN:\n            {\n                ggml_compute_forward_mean(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_ARGMAX:\n            {\n                ggml_compute_forward_argmax(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_REPEAT:\n            {\n                ggml_compute_forward_repeat(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_REPEAT_BACK:\n            {\n                ggml_compute_forward_repeat_back(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_CONCAT:\n            {\n                ggml_compute_forward_concat(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_SILU_BACK:\n            {\n                ggml_compute_forward_silu_back(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_NORM:\n            {\n                ggml_compute_forward_norm(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_RMS_NORM:\n            {\n                ggml_compute_forward_rms_norm(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_RMS_NORM_BACK:\n            {\n                ggml_compute_forward_rms_norm_back(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_GROUP_NORM:\n            {\n                ggml_compute_forward_group_norm(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_MUL_MAT:\n            {\n                ggml_compute_forward_mul_mat(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_MUL_MAT_SPARSE:\n            {\n                GGML_ASSERT(tensor->src[2] != NULL && \"sparsity index is required for MUL_MAT_SPARSE\");\n\n                // MUL_MAT_SPARSE is the first operation in the FFN block, and\n                // tensor->src[1] is the activation from the previous layer/attention block and can be at GPU.\n                // tensor->src[2] is the sparsity index and might also be computed at GPU (depending on predictor offloading condition).\n                // we copy them back to CPU in advance to make sure tensor->data is valid.\n                ggml_ensure_tensor_data_at_memory(tensor->src[1]);\n                ggml_ensure_tensor_data_at_memory(tensor->src[2]);\n\n                if (tensor->src[2]->ne[0] > 1000) {\n                    ggml_compute_forward_mul_mat_sparse(params, tensor->src[0], tensor->src[1], tensor);\n                } else {\n                    // if (params->ith == 0)\n                    //     printf(\"name %s num %d\\n\", ggml_get_name(tensor), num);\n                    ggml_compute_forward_mul_mat_sparse_head(params, tensor->src[0], tensor->src[1], tensor);\n                    // ggml_compute_forward_mul_mat(params, tensor->src[0], tensor->src[1], tensor);\n                } \n            } break;\n        case GGML_OP_AXPY:\n            {\n                GGML_ASSERT(tensor->src[2] != NULL && \"sparse index is required for AXPY\");\n                struct ggml_tensor *src3 = tensor->src[3];\n                if (src3 != NULL){\n                    if (tensor->src[0]->type != GGML_TYPE_Q4_0) {\n                        ggml_compute_forward_mul_mat_axpy(params, tensor->src[0], tensor->src[1], tensor);\n                    }\n                    else {\n                        ggml_compute_forward_mul_mat_axpy_q4_0(params, tensor->src[0], tensor->src[1], tensor);\n\n                    }\n                } else {\n                    ggml_compute_forward_mul_mat_axpy_head(params, tensor->src[0], tensor->src[1], tensor);\n                }\n                // ggml_compute_forward_mul_mat_axpy(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_OUT_PROD:\n            {\n                ggml_compute_forward_out_prod(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_SCALE:\n            {\n                ggml_compute_forward_scale(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_SET:\n            {\n                ggml_compute_forward_set(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_CPY:\n            {\n                ggml_compute_forward_cpy(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_CONT:\n            {\n                ggml_compute_forward_cont(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_RESHAPE:\n            {\n                ggml_compute_forward_reshape(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_VIEW:\n            {\n                ggml_compute_forward_view(params, tensor->src[0]);\n            } break;\n        case GGML_OP_PERMUTE:\n            {\n                ggml_compute_forward_permute(params, tensor->src[0]);\n            } break;\n        case GGML_OP_TRANSPOSE:\n            {\n                ggml_compute_forward_transpose(params, tensor->src[0]);\n            } break;\n        case GGML_OP_GET_ROWS:\n            {\n                ggml_compute_forward_get_rows(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_GET_ROWS_BACK:\n            {\n                ggml_compute_forward_get_rows_back(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_DIAG:\n            {\n                ggml_compute_forward_diag(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_DIAG_MASK_INF:\n            {\n                ggml_compute_forward_diag_mask_inf(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_DIAG_MASK_ZERO:\n            {\n                ggml_compute_forward_diag_mask_zero(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_SOFT_MAX:\n            {\n                ggml_compute_forward_soft_max(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_SOFT_MAX_BACK:\n            {\n                ggml_compute_forward_soft_max_back(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_ROPE:\n            {\n                ggml_compute_forward_rope(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_ROPE_BACK:\n            {\n                ggml_compute_forward_rope_back(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_ALIBI:\n            {\n                ggml_compute_forward_alibi(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_CLAMP:\n            {\n                ggml_compute_forward_clamp(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_CONV_TRANSPOSE_1D:\n            {\n                ggml_compute_forward_conv_transpose_1d(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_IM2COL:\n            {\n                ggml_compute_forward_im2col(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_CONV_TRANSPOSE_2D:\n            {\n                ggml_compute_forward_conv_transpose_2d(params, tensor->src[0], tensor->src[1], tensor);\n            } break;\n        case GGML_OP_POOL_1D:\n            {\n                ggml_compute_forward_pool_1d(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_POOL_2D:\n            {\n                ggml_compute_forward_pool_2d(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_UPSCALE:\n            {\n                ggml_compute_forward_upscale(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_FLASH_ATTN:\n            {\n                const int32_t t = ggml_get_op_params_i32(tensor, 0);\n                GGML_ASSERT(t == 0 || t == 1);\n                const bool masked = t != 0;\n                ggml_compute_forward_flash_attn(params, tensor->src[0], tensor->src[1], tensor->src[2], masked, tensor);\n            } break;\n        case GGML_OP_FLASH_FF:\n            {\n                ggml_compute_forward_flash_ff(params, tensor->src[0], tensor->src[1], tensor->src[2], tensor->src[3], tensor->src[4], tensor);\n            } break;\n        case GGML_OP_FLASH_ATTN_BACK:\n            {\n                int32_t t = ggml_get_op_params_i32(tensor, 0);\n                GGML_ASSERT(t == 0 || t == 1);\n                bool masked = t != 0;\n                ggml_compute_forward_flash_attn_back(params, tensor->src[0], tensor->src[1], tensor->src[2], tensor->src[3], masked, tensor);\n            } break;\n        case GGML_OP_WIN_PART:\n            {\n                ggml_compute_forward_win_part(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_WIN_UNPART:\n            {\n                ggml_compute_forward_win_unpart(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_UNARY:\n            {\n                ggml_compute_forward_unary(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_GET_REL_POS:\n            {\n                ggml_compute_forward_get_rel_pos(params, tensor->src[0], tensor);\n            } break;\n        case GGML_OP_ADD_REL_POS:\n            {\n                ggml_compute_forward_add_rel_pos(params, tensor->src[0], tensor->src[1], tensor->src[2], tensor);\n            } break;\n        case GGML_OP_MAP_UNARY:\n            {\n                ggml_unary_op_f32_t fun;\n                memcpy(&fun, tensor->op_params, sizeof(fun));\n                ggml_compute_forward_map_unary(params, tensor->src[0], tensor, fun);\n            }\n            break;\n        case GGML_OP_MAP_BINARY:\n            {\n                ggml_binary_op_f32_t fun;\n                memcpy(&fun, tensor->op_params, sizeof(fun));\n                ggml_compute_forward_map_binary(params, tensor->src[0], tensor->src[1], tensor, fun);\n            }\n            break;\n        case GGML_OP_MAP_CUSTOM1_F32:\n            {\n                ggml_custom1_op_f32_t fun;\n                memcpy(&fun, tensor->op_params, sizeof(fun));\n                ggml_compute_forward_map_custom1_f32(params, tensor->src[0], tensor, fun);\n            }\n            break;\n        case GGML_OP_MAP_CUSTOM2_F32:\n            {\n                ggml_custom2_op_f32_t fun;\n                memcpy(&fun, tensor->op_params, sizeof(fun));\n                ggml_compute_forward_map_custom2_f32(params, tensor->src[0], tensor->src[1], tensor, fun);\n            }\n            break;\n        case GGML_OP_MAP_CUSTOM3_F32:\n            {\n                ggml_custom3_op_f32_t fun;\n                memcpy(&fun, tensor->op_params, sizeof(fun));\n                ggml_compute_forward_map_custom3_f32(params, tensor->src[0], tensor->src[1], tensor->src[2], tensor, fun);\n            }\n            break;\n        case GGML_OP_MAP_CUSTOM1:\n            {\n                ggml_compute_forward_map_custom1(params, tensor->src[0], tensor);\n            }\n            break;\n        case GGML_OP_MAP_CUSTOM2:\n            {\n                ggml_compute_forward_map_custom2(params, tensor->src[0], tensor->src[1], tensor);\n            }\n            break;\n        case GGML_OP_MAP_CUSTOM3:\n            {\n                ggml_compute_forward_map_custom3(params, tensor->src[0], tensor->src[1], tensor->src[2], tensor);\n            }\n            break;\n        case GGML_OP_CROSS_ENTROPY_LOSS:\n            {\n                ggml_compute_forward_cross_entropy_loss(params, tensor->src[0], tensor->src[1], tensor);\n            }\n            break;\n        case GGML_OP_CROSS_ENTROPY_LOSS_BACK:\n            {\n                ggml_compute_forward_cross_entropy_loss_back(params, tensor->src[0], tensor->src[1], tensor->src[2], tensor);\n            }\n            break;\n        case GGML_OP_NONE:\n            {\n                // nop\n            } break;\n        case GGML_OP_COUNT:\n            {\n                GGML_ASSERT(false);\n            } break;\n    }\n}\n\n////////////////////////////////////////////////////////////////////////////////\n\nstatic size_t ggml_hash_size(size_t min_sz) {\n    // next primes after powers of two\n    static const size_t primes[] = {\n        2, 3, 5, 11, 17, 37, 67, 131, 257, 521, 1031,\n        2053, 4099, 8209, 16411, 32771, 65537, 131101,\n        262147, 524309, 1048583, 2097169, 4194319, 8388617,\n        16777259, 33554467, 67108879, 134217757, 268435459,\n        536870923, 1073741827, 2147483659\n    };\n    static const size_t n_primes = sizeof(primes)/sizeof(primes[0]);\n\n    // find the smallest prime that is larger or equal to min_sz\n    size_t l = 0;\n    size_t r = n_primes;\n    while (l < r) {\n        size_t m = (l + r)/2;\n        if (primes[m] < min_sz) {\n            l = m + 1;\n        } else {\n            r = m;\n        }\n    }\n    size_t sz = l < n_primes ? primes[l] : min_sz | 1;\n    return sz;\n}\n\nstatic size_t ggml_hash(const void * p) {\n    return (size_t)p;\n}\n\nsize_t ggml_hash_find(const struct ggml_hash_set hash_set, struct ggml_tensor * key) {\n    size_t h = ggml_hash(key) % hash_set.size;\n\n    // linear probing\n    size_t i = h;\n    while (hash_set.keys[i] != NULL && hash_set.keys[i] != key) {\n        i = (i + 1) % hash_set.size;\n        if (i == h) {\n            // visited all hash table entries -> not found\n            return GGML_HASHTABLE_FULL;\n        }\n    }\n    return i;\n}\n\nbool ggml_hash_contains(struct ggml_hash_set hash_set, struct ggml_tensor * key) {\n    size_t i = ggml_hash_find(hash_set, key);\n    return i != GGML_HASHTABLE_FULL && hash_set.keys[i] == key;\n}\n\nsize_t ggml_hash_insert(struct ggml_hash_set hash_set, struct ggml_tensor * key) {\n    size_t i = ggml_hash_find(hash_set, key);\n\n    GGML_ASSERT(i != GGML_HASHTABLE_FULL);\n\n    if (hash_set.keys[i] == key) {\n        return GGML_HASHTABLE_ALREADY_EXISTS;\n    }\n\n    // insert\n    GGML_ASSERT(hash_set.keys[i] == NULL);\n    hash_set.keys[i] = key;\n    return i;\n}\n\nsize_t ggml_hash_find_or_insert(struct ggml_hash_set hash_set, struct ggml_tensor * key) {\n    size_t i = ggml_hash_find(hash_set, key);\n\n    GGML_ASSERT(i != GGML_HASHTABLE_FULL);\n\n    hash_set.keys[i] = key;\n    return i;\n}\n\nstatic struct ggml_hash_set ggml_hash_set_new(size_t size) {\n    size = ggml_hash_size(size);\n    struct ggml_hash_set result;\n    result.size = size;\n    result.keys = malloc(sizeof(struct ggml_tensor *) * size);\n    memset(result.keys, 0, sizeof(struct ggml_tensor *) * size);\n    return result;\n}\n\nstatic void ggml_hash_set_free(struct ggml_hash_set hash_set) {\n    free(hash_set.keys);\n}\n\nstruct hash_map {\n    struct ggml_hash_set set;\n    struct ggml_tensor ** vals;\n};\n\nstatic struct hash_map * ggml_new_hash_map(size_t size) {\n    struct hash_map * result = malloc(sizeof(struct hash_map));\n    result->set = ggml_hash_set_new(size);\n    result->vals = malloc(sizeof(struct ggml_tensor *) * result->set.size);\n    memset(result->vals, 0, sizeof(struct ggml_tensor *) * result->set.size);\n    return result;\n}\n\nstatic void ggml_hash_map_free(struct hash_map * map) {\n    ggml_hash_set_free(map->set);\n    free(map->vals);\n    free(map);\n}\n\n// gradient checkpointing\n\nstatic struct ggml_tensor * ggml_recompute_graph_node(\n        struct ggml_context * ctx,\n        struct ggml_cgraph  * graph,\n        struct hash_map     * replacements,\n        struct ggml_tensor  * node) {\n\n    if (node == NULL) {\n        return NULL;\n    }\n\n    if (node->is_param) {\n        return node;\n    }\n\n    if (!ggml_hash_contains(graph->visited_hash_table, node)) {\n        return node;\n    }\n\n    int count_children = 0;\n    for (int k = 0; k < GGML_MAX_SRC; ++k) {\n        if (node->src[k]) {\n            ++count_children;\n        }\n    }\n\n    if (count_children == 0) {\n        return node;\n    }\n\n    size_t i = ggml_hash_find(replacements->set, node);\n    GGML_ASSERT(i != GGML_HASHTABLE_FULL); // assert that not full\n    if (replacements->set.keys[i] == node) {\n        return replacements->vals[i];\n    }\n\n    struct ggml_tensor * clone = ggml_new_tensor(ctx, node->type, node->n_dims, node->ne);\n\n    // insert clone into replacements\n    GGML_ASSERT(replacements->set.keys[i] == NULL); // assert that we don't overwrite\n    replacements->set.keys[i] = node;\n    replacements->vals[i] = clone;\n\n    clone->op       = node->op;\n    clone->grad     = node->grad;\n    clone->is_param = node->is_param;\n    clone->extra    = node->extra;\n    for (int k = 0; k < GGML_MAX_DIMS; ++k) {\n        clone->nb[k] = node->nb[k];\n    }\n    for (int k = 0; k < GGML_MAX_SRC; ++k) {\n        clone->src[k] = ggml_recompute_graph_node(ctx, graph, replacements, node->src[k]);\n    }\n    if (node->view_src != NULL) {\n        clone->data = (node->view_src->data == NULL)\n                        ? NULL // view_src not yet allocated\n                        : (char *) node->view_src->data // view_src already allocated\n                                 + node->view_offs;\n        clone->view_src  = node->view_src;\n        clone->view_offs = node->view_offs;\n    }\n\n    GGML_ASSERT(sizeof(node->op_params) == sizeof(int32_t) * (GGML_MAX_OP_PARAMS / sizeof(int32_t)));\n    GGML_ASSERT(sizeof(node->name)      == GGML_MAX_NAME);\n    memcpy(clone->op_params, node->op_params, sizeof(node->op_params));\n    ggml_format_name(clone, \"%s (clone)\", ggml_get_name(node));\n\n    return clone;\n}\n\nvoid ggml_build_backward_gradient_checkpointing(\n        struct ggml_context   * ctx,\n        struct ggml_cgraph    * gf,\n        struct ggml_cgraph    * gb,\n        struct ggml_cgraph    * gb_tmp,\n        struct ggml_tensor  * * checkpoints,\n        int                     n_checkpoints) {\n    ggml_graph_cpy(gf, gb_tmp);\n    ggml_build_backward_expand(ctx, gf, gb_tmp, true);\n\n    if (n_checkpoints <= 0) {\n        ggml_graph_cpy(gb_tmp, gb);\n        return;\n    }\n\n    struct hash_map * replacements = ggml_new_hash_map(gf->n_nodes + gf->n_leafs + n_checkpoints);\n\n    // insert checkpoints in replacements\n    for (int i = 0; i < n_checkpoints; ++i) {\n        size_t k = ggml_hash_find(replacements->set, checkpoints[i]);\n        GGML_ASSERT(k != GGML_HASHTABLE_FULL); // assert that not full\n        GGML_ASSERT(replacements->set.keys[k] == NULL); // assert that we don't overwrite\n        replacements->set.keys[k] = checkpoints[i];\n        replacements->vals[k]     = checkpoints[i];\n    }\n\n    ggml_graph_cpy(gf, gb);\n    // rewrite gb_tmp->nodes[gf->n_nodes:gb_tmp->n_nodes],\n    // replacing references to gb_tmp->nodes[0:gf->n_nodes] ( == gf->nodes[0:gf->n_nodes]),\n    // by recomputing them from checkpoints\n    for (int i = gf->n_nodes; i<gb_tmp->n_nodes; ++i) {\n        struct ggml_tensor * node = gb_tmp->nodes[i];\n        for (int k = 0; k < GGML_MAX_SRC; ++k) {\n            // insert new tensors recomputing src, reusing already made replacements,\n            // remember replacements: remember new tensors with mapping from corresponding gf nodes\n            // recurse for input tensors,\n            // unless (i.e. terminating when) input tensors are replacments (like checkpoints)\n            node->src[k] = ggml_recompute_graph_node(ctx, gf, replacements, node->src[k]);\n        }\n        // insert rewritten backward node with replacements made into resulting backward graph gb\n        ggml_build_forward_expand(gb, node);\n    }\n\n    ggml_hash_map_free(replacements);\n}\n\n// functions to change gradients considering the case that input a might be initial gradient with zero value\n\nstatic struct ggml_tensor * ggml_add_or_set(struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, struct ggml_hash_set zero_table) {\n    if (ggml_hash_contains(zero_table, a)) {\n        return b;\n    } else {\n        return ggml_add_impl(ctx, a, b, false);\n    }\n}\n\nstatic struct ggml_tensor * ggml_acc_or_set(struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, size_t nb1, size_t nb2, size_t nb3, size_t offset, struct ggml_hash_set zero_table) {\n    if (ggml_hash_contains(zero_table, a)) {\n        struct ggml_tensor * a_zero = ggml_scale(ctx, a, ggml_new_f32(ctx, 0));\n        return ggml_acc_impl(ctx, a_zero, b, nb1, nb2, nb3, offset, false);\n    } else {\n        return ggml_acc_impl(ctx, a, b, nb1, nb2, nb3, offset, false);\n    }\n}\n\nstatic struct ggml_tensor * ggml_add1_or_set(struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, struct ggml_hash_set zero_table) {\n    if (ggml_hash_contains(zero_table, a)) {\n        return ggml_repeat(ctx, b, a);\n    } else {\n        return ggml_add1_impl(ctx, a, b, false);\n    }\n}\n\nstatic struct ggml_tensor * ggml_sub_or_set(struct ggml_context * ctx, struct ggml_tensor * a, struct ggml_tensor * b, struct ggml_hash_set zero_table) {\n    if (ggml_hash_contains(zero_table, a)) {\n        return ggml_neg(ctx, b);\n    } else {\n        return ggml_sub_impl(ctx, a, b, false);\n    }\n}\n\nstatic void ggml_compute_backward(struct ggml_context * ctx, struct ggml_tensor * tensor, struct ggml_hash_set zero_table) {\n    struct ggml_tensor * src0 = tensor->src[0];\n    struct ggml_tensor * src1 = tensor->src[1];\n\n    switch (tensor->op) {\n        case GGML_OP_DUP:\n            {\n                if (src0->grad) {\n                    src0->grad = ggml_add_or_set(ctx, src0->grad, tensor->grad, zero_table);\n                }\n            } break;\n        case GGML_OP_ADD:\n            {\n                if (src0->grad) {\n                    src0->grad = ggml_add_or_set(ctx, src0->grad, tensor->grad, zero_table);\n                }\n                if (src1->grad) {\n                    src1->grad = ggml_add_or_set(ctx, src1->grad, tensor->grad, zero_table);\n                }\n            } break;\n        case GGML_OP_ADD1:\n            {\n                if (src0->grad) {\n                    src0->grad = ggml_add_or_set(ctx, src0->grad, tensor->grad, zero_table);\n                }\n                if (src1->grad) {\n                    src1->grad = ggml_add_or_set(ctx,\n                        src1->grad,\n                        ggml_mean(ctx, tensor->grad), // TODO: should probably be sum instead of mean\n                        zero_table);\n                }\n            } break;\n        case GGML_OP_ACC:\n            {\n                if (src0->grad) {\n                    src0->grad = ggml_add_or_set(ctx, src0->grad, tensor->grad, zero_table);\n                }\n                if (src1->grad) {\n                    const size_t nb1     = ((int32_t *) tensor->op_params)[0];\n                    const size_t nb2     = ((int32_t *) tensor->op_params)[1];\n                    const size_t nb3     = ((int32_t *) tensor->op_params)[2];\n                    const size_t offset  = ((int32_t *) tensor->op_params)[3];\n\n                    struct ggml_tensor * tensor_grad_view = ggml_view_4d(ctx,\n                        tensor->grad,\n                        src1->grad->ne[0],\n                        src1->grad->ne[1],\n                        src1->grad->ne[2],\n                        src1->grad->ne[3],\n                        nb1, nb2, nb3, offset);\n\n                    src1->grad =\n                        ggml_add_or_set(ctx,\n                            src1->grad,\n                            ggml_reshape(ctx,\n                                ggml_cont(ctx, tensor_grad_view),\n                                src1->grad),\n                            zero_table);\n                }\n            } break;\n        case GGML_OP_SUB:\n            {\n                if (src0->grad) {\n                    src0->grad = ggml_add_or_set(ctx, src0->grad, tensor->grad, zero_table);\n                }\n                if (src1->grad) {\n                    src1->grad = ggml_sub_or_set(ctx, src1->grad, tensor->grad, zero_table);\n                }\n            } break;\n        case GGML_OP_MUL:\n            {\n                if (src0->grad) {\n                    src0->grad =\n                        ggml_add_or_set(ctx,\n                                src0->grad,\n                                ggml_mul(ctx, src1, tensor->grad),\n                                zero_table);\n                }\n                if (src1->grad) {\n                    src1->grad =\n                        ggml_add_or_set(ctx,\n                                src1->grad,\n                                ggml_mul(ctx, src0, tensor->grad),\n                                zero_table);\n                }\n            } break;\n        case GGML_OP_DIV:\n            {\n                if (src0->grad) {\n                    src0->grad =\n                        ggml_add_or_set(ctx,\n                                src0->grad,\n                                ggml_div(ctx, tensor->grad, src1),\n                                zero_table);\n                }\n                if (src1->grad) {\n                    src1->grad =\n                        ggml_sub_or_set(ctx,\n                                src1->grad,\n                                ggml_mul(ctx,\n                                    tensor->grad,\n                                    ggml_div(ctx, tensor, src1)),\n                                zero_table);\n                }\n            } break;\n        case GGML_OP_SQR:\n            {\n                if (src0->grad) {\n                    src0->grad =\n                        ggml_add_or_set(ctx,\n                                src0->grad,\n                                ggml_scale(ctx,\n                                    ggml_mul(ctx, src0, tensor->grad),\n                                    ggml_new_f32(ctx, 2.0f)),\n                                zero_table);\n                }\n            } break;\n        case GGML_OP_SQRT:\n            {\n                if (src0->grad) {\n                    src0->grad =\n                        ggml_add_or_set(ctx,\n                                src0->grad,\n                                ggml_scale(ctx,\n                                    ggml_div(ctx,\n                                        tensor->grad,\n                                        tensor),\n                                    ggml_new_f32(ctx, 0.5f)),\n                                zero_table);\n                }\n            } break;\n        case GGML_OP_LOG:\n            {\n                if (src0->grad) {\n                    src0->grad =\n                        ggml_add_or_set(ctx,\n                                src0->grad,\n                                ggml_div(ctx,\n                                    tensor->grad,\n                                    src0),\n                                zero_table);\n                }\n            } break;\n        case GGML_OP_SUM:\n            {\n                if (src0->grad) {\n                    src0->grad =\n                        ggml_add1_or_set(ctx,\n                                src0->grad,\n                                tensor->grad,\n                                zero_table);\n                }\n            } break;\n        case GGML_OP_SUM_ROWS:\n            {\n                if (src0->grad) {\n                    src0->grad =\n                        ggml_add_or_set(ctx,\n                                src0->grad,\n                                ggml_repeat(ctx,\n                                    tensor->grad,\n                                    src0->grad),\n                                zero_table);\n                }\n            } break;\n        case GGML_OP_MEAN:\n        case GGML_OP_ARGMAX:\n            {\n                GGML_ASSERT(false); // TODO: implement\n            } break;\n        case GGML_OP_REPEAT:\n            {\n                // necessary for llama\n                if (src0->grad) {\n                    src0->grad = ggml_add_or_set(ctx,\n                            src0->grad,\n                            ggml_repeat_back(ctx, tensor->grad, src0->grad),\n                            zero_table);\n                }\n            } break;\n        case GGML_OP_REPEAT_BACK:\n            {\n                if (src0->grad) {\n                    // TODO: test this\n                    src0->grad = ggml_add_or_set(ctx,\n                            src0->grad,\n                            ggml_repeat(ctx, tensor->grad, src0->grad),\n                            zero_table);\n                }\n            } break;\n        case GGML_OP_CONCAT:\n            {\n                GGML_ASSERT(false); // TODO: implement\n            } break;\n        case GGML_OP_SILU_BACK:\n            {\n                GGML_ASSERT(false); // TODO: not implemented\n            } break;\n        case GGML_OP_NORM:\n            {\n                GGML_ASSERT(false); // TODO: not implemented\n            } break;\n        case GGML_OP_RMS_NORM:\n            {\n                // necessary for llama\n                if (src0->grad) {\n                    float eps;\n                    memcpy(&eps, tensor->op_params, sizeof(float));\n\n                    src0->grad = ggml_add_or_set(ctx,\n                            src0->grad,\n                            ggml_rms_norm_back(ctx, src0, tensor->grad, eps),\n                            zero_table);\n                }\n            } break;\n        case GGML_OP_RMS_NORM_BACK:\n            {\n                GGML_ASSERT(false); // TODO: not implemented\n            } break;\n        case GGML_OP_GROUP_NORM:\n            {\n                GGML_ASSERT(false); // TODO: not implemented\n            } break;\n        case GGML_OP_MUL_MAT:\n        case GGML_OP_MUL_MAT_SPARSE:\n        case GGML_OP_AXPY:\n            {\n                // https://cs231n.github.io/optimization-2/#staged\n                // # forward pass\n                // s0 = np.random.randn(5, 10)\n                // s1 = np.random.randn(10, 3)\n                // t = s0.dot(s1)\n\n                // # now suppose we had the gradient on t from above in the circuit\n                // dt = np.random.randn(*t.shape) # same shape as t\n                // ds0 = dt.dot(s1.T) #.T gives the transpose of the matrix\n                // ds1 = t.T.dot(dt)\n\n                // tensor.shape [m,p,qq,rr]\n                // src0.shape   [n,m,q1,r1]\n                // src1.shape   [n,p,qq,rr]\n\n                // necessary for llama\n                if (src0->grad) {\n                    struct ggml_tensor * s1_tg =\n                        ggml_out_prod(ctx, // [n,m,qq,rr]\n                            src1,          // [n,p,qq,rr]\n                            tensor->grad); // [m,p,qq,rr]\n                    const int64_t qq = s1_tg->ne[2];\n                    const int64_t rr = s1_tg->ne[3];\n                    const int64_t q1 = src0->ne[2];\n                    const int64_t r1 = src0->ne[3];\n                    const bool ne2_broadcasted = qq > q1;\n                    const bool ne3_broadcasted = rr > r1;\n                    if (ne2_broadcasted || ne3_broadcasted) {\n                        // sum broadcast repetitions of s1_tg into shape of src0\n                        s1_tg = ggml_repeat_back(ctx, s1_tg, src0);\n                    }\n                    src0->grad =\n                        ggml_add_or_set(ctx,\n                                src0->grad, // [n,m,q1,r1]\n                                s1_tg,      // [n,m,q1,r1]\n                                zero_table);\n                }\n                if (src1->grad) {\n                    src1->grad =\n                        ggml_add_or_set(ctx,\n                                src1->grad,                            // [n,p,qq,rr]\n                                // ggml_mul_mat(ctx,                   // [n,p,qq,rr]\n                                //     ggml_cont(ctx,                  // [m,n,q1,r1]\n                                //         ggml_transpose(ctx, src0)), // [m,n,q1,r1]\n                                //     tensor->grad),                  // [m,p,qq,rr]\n\n                                // // when src0 is bigger than tensor->grad (this is mostly the case in llama),\n                                // // avoid transpose of src0, rather transpose smaller tensor->grad\n                                // // and then use ggml_out_prod\n                                ggml_out_prod(ctx,                  // [n,p,qq,rr]\n                                    src0,                           // [n,m,q1,r1]\n                                    ggml_transpose(ctx,             // [p,m,qq,rr]\n                                        tensor->grad)),             // [m,p,qq,rr]\n                                zero_table);\n                }\n            } break;\n        case GGML_OP_OUT_PROD:\n            {\n                GGML_ASSERT(false); // TODO: not implemented\n            } break;\n        case GGML_OP_SCALE:\n            {\n                // necessary for llama\n                if (src0->grad) {\n                    src0->grad =\n                        ggml_add_or_set(ctx,\n                            src0->grad,\n                            ggml_scale_impl(ctx, tensor->grad, src1, false),\n                            zero_table);\n                }\n                if (src1->grad) {\n                    src1->grad =\n                        ggml_add_or_set(ctx,\n                            src1->grad,\n                            ggml_sum(ctx, ggml_mul_impl(ctx, tensor->grad, src0, false)),\n                            zero_table);\n                }\n            } break;\n        case GGML_OP_SET:\n            {\n                const size_t nb1     = ((int32_t *) tensor->op_params)[0];\n                const size_t nb2     = ((int32_t *) tensor->op_params)[1];\n                const size_t nb3     = ((int32_t *) tensor->op_params)[2];\n                const size_t offset  = ((int32_t *) tensor->op_params)[3];\n\n                struct ggml_tensor * tensor_grad_view = NULL;\n\n                if (src0->grad || src1->grad) {\n                    GGML_ASSERT(src0->type == tensor->type);\n                    GGML_ASSERT(tensor->grad->type == tensor->type);\n                    GGML_ASSERT(tensor->grad->type == src1->grad->type);\n\n                    tensor_grad_view = ggml_view_4d(ctx,\n                        tensor->grad,\n                        src1->grad->ne[0],\n                        src1->grad->ne[1],\n                        src1->grad->ne[2],\n                        src1->grad->ne[3],\n                        nb1, nb2, nb3, offset);\n                }\n\n                if (src0->grad) {\n                    src0->grad = ggml_add_or_set(ctx,\n                        src0->grad,\n                        ggml_acc_impl(ctx,\n                            tensor->grad,\n                            ggml_neg(ctx, tensor_grad_view),\n                            nb1, nb2, nb3, offset, false),\n                        zero_table);\n                }\n\n                if (src1->grad) {\n                    src1->grad =\n                        ggml_add_or_set(ctx,\n                           "
        },
        {
          "name": "ggml.h",
          "type": "blob",
          "size": 80.1533203125,
          "content": "#pragma once\n\n//\n// GGML Tensor Library\n//\n// This documentation is still a work in progress.\n// If you wish some specific topics to be covered, feel free to drop a comment:\n//\n//   https://github.com/ggerganov/whisper.cpp/issues/40\n//\n// ## Overview\n//\n// This library implements:\n//\n//  - a set of tensor operations\n//  - automatic differentiation\n//  - basic optimization algorithms\n//\n// The aim of this library is to provide a minimalistic approach for various machine learning tasks. This includes,\n// but is not limited to, the following:\n//\n//  - linear regression\n//  - support vector machines\n//  - neural networks\n//\n// The library allows the user to define a certain function using the available tensor operations. This function\n// definition is represented internally via a computation graph. Each tensor operation in the function definition\n// corresponds to a node in the graph. Having the computation graph defined, the user can choose to compute the\n// function's value and/or its gradient with respect to the input variables. Optionally, the function can be optimized\n// using one of the available optimization algorithms.\n//\n// For example, here we define the function: f(x) = a*x^2 + b\n//\n//   {\n//       struct ggml_init_params params = {\n//           .mem_size   = 16*1024*1024,\n//           .mem_buffer = NULL,\n//       };\n//\n//       // memory allocation happens here\n//       struct ggml_context * ctx = ggml_init(params);\n//\n//       struct ggml_tensor * x = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);\n//\n//       ggml_set_param(ctx, x); // x is an input variable\n//\n//       struct ggml_tensor * a  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);\n//       struct ggml_tensor * b  = ggml_new_tensor_1d(ctx, GGML_TYPE_F32, 1);\n//       struct ggml_tensor * x2 = ggml_mul(ctx, x, x);\n//       struct ggml_tensor * f  = ggml_add(ctx, ggml_mul(ctx, a, x2), b);\n//\n//       ...\n//   }\n//\n// Notice that the function definition above does not involve any actual computation. The computation is performed only\n// when the user explicitly requests it. For example, to compute the function's value at x = 2.0:\n//\n//   {\n//       ...\n//\n//       struct ggml_cgraph * gf = ggml_new_graph(ctx);\n//       ggml_build_forward_expand(gf, f);\n//\n//       // set the input variable and parameter values\n//       ggml_set_f32(x, 2.0f);\n//       ggml_set_f32(a, 3.0f);\n//       ggml_set_f32(b, 4.0f);\n//\n//       ggml_graph_compute_with_ctx(ctx, &gf, n_threads);\n//\n//       printf(\"f = %f\\n\", ggml_get_f32_1d(f, 0));\n//\n//       ...\n//   }\n//\n// The actual computation is performed in the ggml_graph_compute() function.\n//\n// The ggml_new_tensor_...() functions create new tensors. They are allocated in the memory buffer provided to the\n// ggml_init() function. You have to be careful not to exceed the memory buffer size. Therefore, you have to know\n// in advance how much memory you need for your computation. Alternatively, you can allocate a large enough memory\n// and after defining the computation graph, call the ggml_used_mem() function to find out how much memory was\n// actually needed.\n//\n// The ggml_set_param() function marks a tensor as an input variable. This is used by the automatic\n// differentiation and optimization algorithms.\n//\n// The described approach allows to define the function graph once and then compute its forward or backward graphs\n// multiple times. All computations will use the same memory buffer allocated in the ggml_init() function. This way\n// the user can avoid the memory allocation overhead at runtime.\n//\n// The library supports multi-dimensional tensors - up to 4 dimensions. The FP16 and FP32 data types are first class\n// citizens, but in theory the library can be extended to support FP8 and integer data types.\n//\n// Each tensor operation produces a new tensor. Initially the library was envisioned to support only the use of unary\n// and binary operations. Most of the available operations fall into one of these two categories. With time, it became\n// clear that the library needs to support more complex operations. The way to support these operations is not clear\n// yet, but a few examples are demonstrated in the following operations:\n//\n//   - ggml_permute()\n//   - ggml_conv_1d_1s()\n//   - ggml_conv_1d_2s()\n//\n// For each tensor operator, the library implements a forward and backward computation function. The forward function\n// computes the output tensor value given the input tensor values. The backward function computes the adjoint of the\n// input tensors given the adjoint of the output tensor. For a detailed explanation of what this means, take a\n// calculus class, or watch the following video:\n//\n//   What is Automatic Differentiation?\n//   https://www.youtube.com/watch?v=wG_nF1awSSY\n//\n//\n// ## Tensor data (struct ggml_tensor)\n//\n// The tensors are stored in memory via the ggml_tensor struct. The structure provides information about the size of\n// the tensor, the data type, and the memory buffer where the tensor data is stored. Additionally, it contains\n// pointers to the \"source\" tensors - i.e. the tensors that were used to compute the current tensor. For example:\n//\n//   {\n//       struct ggml_tensor * c = ggml_add(ctx, a, b);\n//\n//       assert(c->src[0] == a);\n//       assert(c->src[1] == b);\n//   }\n//\n// The multi-dimensional tensors are stored in row-major order. The ggml_tensor struct contains fields for the\n// number of elements in each dimension (\"ne\") as well as the number of bytes (\"nb\", a.k.a. stride). This allows\n// to store tensors that are not contiguous in memory, which is useful for operations such as transposition and\n// permutation. All tensor operations have to take the stride into account and not assume that the tensor is\n// contiguous in memory.\n//\n// The data of the tensor is accessed via the \"data\" pointer. For example:\n//\n//   {\n//       const int nx = 2;\n//       const int ny = 3;\n//\n//       struct ggml_tensor * a = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, nx, ny);\n//\n//       for (int y = 0; y < ny; y++) {\n//           for (int x = 0; x < nx; x++) {\n//               *(float *) ((char *) a->data + y*a->nb[1] + x*a->nb[0]) = x + y;\n//           }\n//       }\n//\n//       ...\n//   }\n//\n// Alternatively, there are helper functions, such as ggml_get_f32_1d() and ggml_set_f32_1d() that can be used.\n//\n// ## The matrix multiplication operator (ggml_mul_mat)\n//\n// TODO\n//\n//\n// ## Multi-threading\n//\n// TODO\n//\n//\n// ## Overview of ggml.c\n//\n// TODO\n//\n//\n// ## SIMD optimizations\n//\n// TODO\n//\n//\n// ## Debugging ggml\n//\n// TODO\n//\n//\n\n#ifdef GGML_SHARED\n#    if defined(_WIN32) && !defined(__MINGW32__)\n#        ifdef GGML_BUILD\n#            define GGML_API __declspec(dllexport)\n#        else\n#            define GGML_API __declspec(dllimport)\n#        endif\n#    else\n#        define GGML_API __attribute__ ((visibility (\"default\")))\n#    endif\n#else\n#    define GGML_API\n#endif\n\n// TODO: support for clang\n#ifdef __GNUC__\n#    define GGML_DEPRECATED(func, hint) func __attribute__((deprecated(hint)))\n#elif defined(_MSC_VER)\n#    define GGML_DEPRECATED(func, hint) __declspec(deprecated(hint)) func\n#else\n#    define GGML_DEPRECATED(func, hint) func\n#endif\n\n#ifndef __GNUC__\n#    define GGML_ATTRIBUTE_FORMAT(...)\n#elif defined(__MINGW32__)\n#    define GGML_ATTRIBUTE_FORMAT(...) __attribute__((format(gnu_printf, __VA_ARGS__)))\n#else\n#    define GGML_ATTRIBUTE_FORMAT(...) __attribute__((format(printf, __VA_ARGS__)))\n#endif\n\n#include <stdint.h>\n#include <stddef.h>\n#include <stdbool.h>\n#ifdef __cplusplus\n  #include <atomic>\n  using std::atomic_int;\n  using std::memory_order;\n  using std::memory_order_acquire;\n#else /* not __cplusplus */\n#if defined(_WIN32)\n#    include \"atomic_windows.h\"\n#else\n#    include <stdatomic.h>\n#endif\n#endif /* __cplusplus */\n\n#define GGML_FILE_MAGIC   0x67676d6c // \"ggml\"\n#define GGML_FILE_VERSION 1\n\n#define GGML_QNT_VERSION        2    // bump this on quantization format changes\n#define GGML_QNT_VERSION_FACTOR 1000 // do not change this\n\n#define GGML_MAX_DIMS           4\n#define GGML_MAX_PARAMS         1024\n#define GGML_MAX_CONTEXTS       64\n#define GGML_MAX_SRC            6\n#define GGML_MAX_NAME           64\n#define GGML_MAX_OP_PARAMS      64\n#define GGML_DEFAULT_N_THREADS  4\n#define GGML_DEFAULT_GRAPH_SIZE 2048\n#if UINTPTR_MAX == 0xFFFFFFFF\n    #define GGML_MEM_ALIGN 4\n#else\n    #define GGML_MEM_ALIGN 16\n#endif\n\n#define GGML_EXIT_SUCCESS 0\n#define GGML_EXIT_ABORTED 1\n\n#define GGUF_MAGIC \"GGUF\"\n#define GGUF_POWERINFER_MAGIC \"PWRI\"\n\n#define GGUF_VERSION 3\n\n#define GGUF_DEFAULT_ALIGNMENT 32\n\n#define GGML_UNUSED(x) (void)(x)\n\n#define GGML_PAD(x, n) (((x) + (n) - 1) & ~((n) - 1))\n\n#define GGML_ASSERT(x) \\\n    do { \\\n        if (!(x)) { \\\n            fprintf(stderr, \"GGML_ASSERT: %s:%d: %s\\n\", __FILE__, __LINE__, #x); \\\n            fflush(stderr); \\\n            fflush(stdout); \\\n            ggml_print_backtrace(); \\\n            exit(1); \\\n        } \\\n    } while (0)\n\n#define GGML_ASSERT_DBG(x, s, ...) \\\n    do { \\\n        if (!(x)) { \\\n            fprintf(stderr, \"GGML_ASSERT: %s:%d: \" s \"\\n\", __FILE__, __LINE__, ##__VA_ARGS__); \\\n            fflush(stderr); \\\n            fflush(stdout); \\\n            ggml_print_backtrace(); \\\n            exit(1); \\\n        } \\\n    } while (0)\n\n#ifndef NDEBUG\n#define GGML_UNREACHABLE() GGML_ASSERT(!\"statement should not be reached\")\n#elif defined(__GNUC__)\n#define GGML_UNREACHABLE() __builtin_unreachable()\n#else\n#define GGML_UNREACHABLE() ((void) 0)\n#endif\n\n// used to copy the number of elements and stride in bytes of tensors into local variables.\n// main purpose is to reduce code duplication and improve readability.\n//\n// example:\n//\n//    GGML_TENSOR_LOCALS(int64_t, ne1, src1, ne);\n//    GGML_TENSOR_LOCALS(size_t,  nb1, src1, nb);\n//\n#define GGML_TENSOR_LOCALS_1(type, prefix, pointer, array) \\\n    const type prefix##0 = (pointer)->array[0]; \\\n    GGML_UNUSED(prefix##0);\n#define GGML_TENSOR_LOCALS_2(type, prefix, pointer, array) \\\n    GGML_TENSOR_LOCALS_1    (type, prefix, pointer, array) \\\n    const type prefix##1 = (pointer)->array[1]; \\\n    GGML_UNUSED(prefix##1);\n#define GGML_TENSOR_LOCALS_3(type, prefix, pointer, array) \\\n    GGML_TENSOR_LOCALS_2    (type, prefix, pointer, array) \\\n    const type prefix##2 = (pointer)->array[2]; \\\n    GGML_UNUSED(prefix##2);\n#define GGML_TENSOR_LOCALS(type, prefix, pointer, array) \\\n    GGML_TENSOR_LOCALS_3  (type, prefix, pointer, array) \\\n    const type prefix##3 = (pointer)->array[3]; \\\n    GGML_UNUSED(prefix##3);\n\n#ifdef  __cplusplus\nextern \"C\" {\n#endif\n\n#if defined(__ARM_NEON) && defined(__CUDACC__)\n    typedef half ggml_fp16_t;\n#elif defined(__ARM_NEON)\n    typedef __fp16 ggml_fp16_t;\n#else\n    typedef uint16_t ggml_fp16_t;\n#endif\n\n    // convert FP16 <-> FP32\n    GGML_API float       ggml_fp16_to_fp32(ggml_fp16_t x);\n    GGML_API ggml_fp16_t ggml_fp32_to_fp16(float x);\n\n    GGML_API void ggml_fp16_to_fp32_row(const ggml_fp16_t * x, float * y, int n);\n    GGML_API void ggml_fp32_to_fp16_row(const float * x, ggml_fp16_t * y, int n);\n\n    struct ggml_object;\n    struct ggml_context;\n\n    enum ggml_type {\n        GGML_TYPE_F32  = 0,\n        GGML_TYPE_F16  = 1,\n        GGML_TYPE_Q4_0 = 2,\n        GGML_TYPE_Q4_1 = 3,\n        // GGML_TYPE_Q4_2 = 4, support has been removed\n        // GGML_TYPE_Q4_3 (5) support has been removed\n        GGML_TYPE_Q5_0 = 6,\n        GGML_TYPE_Q5_1 = 7,\n        GGML_TYPE_Q8_0 = 8,\n        GGML_TYPE_Q8_1 = 9,\n        // k-quantizations\n        GGML_TYPE_Q2_K = 10,\n        GGML_TYPE_Q3_K = 11,\n        GGML_TYPE_Q4_K = 12,\n        GGML_TYPE_Q5_K = 13,\n        GGML_TYPE_Q6_K = 14,\n        GGML_TYPE_Q8_K = 15,\n        GGML_TYPE_I8,\n        GGML_TYPE_I16,\n        GGML_TYPE_I32,\n        GGML_TYPE_COUNT,\n    };\n\n    enum ggml_backend_type {\n        GGML_BACKEND_CPU = 0,\n        GGML_BACKEND_GPU = 10,\n        GGML_BACKEND_GPU_SPLIT = 20,\n    };\n\n    enum ggml_sparse_deriv {\n        GGML_DENSE_INFERENCE = 0,\n        GGML_SPARSE_INFERENCE = 1,\n    };\n\n    // model file types\n    enum ggml_ftype {\n        GGML_FTYPE_UNKNOWN     = -1,\n        GGML_FTYPE_ALL_F32     = 0,\n        GGML_FTYPE_MOSTLY_F16  = 1,  // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q4_0 = 2,  // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q4_1 = 3,  // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4, // tok_embeddings.weight and output.weight are F16\n        GGML_FTYPE_MOSTLY_Q8_0 = 7,  // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q5_0 = 8,  // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q5_1 = 9,  // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q2_K = 10, // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q3_K = 11, // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q4_K = 12, // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q5_K = 13, // except 1d tensors\n        GGML_FTYPE_MOSTLY_Q6_K = 14, // except 1d tensors\n    };\n\n    // available tensor operations:\n    enum ggml_op {\n        GGML_OP_NONE = 0,\n\n        GGML_OP_DUP,\n        GGML_OP_ADD,\n        GGML_OP_ADD1,\n        GGML_OP_ACC,\n        GGML_OP_SUB,\n        GGML_OP_MUL,\n        GGML_OP_DIV,\n        GGML_OP_SQR,\n        GGML_OP_SQRT,\n        GGML_OP_LOG,\n        GGML_OP_SUM,\n        GGML_OP_SUM_ROWS,\n        GGML_OP_MEAN,\n        GGML_OP_ARGMAX,\n        GGML_OP_REPEAT,\n        GGML_OP_REPEAT_BACK,\n        GGML_OP_CONCAT,\n        GGML_OP_SILU_BACK,\n        GGML_OP_NORM, // normalize\n        GGML_OP_RMS_NORM,\n        GGML_OP_RMS_NORM_BACK,\n        GGML_OP_GROUP_NORM,\n\n        GGML_OP_MUL_MAT,\n        GGML_OP_MUL_MAT_SPARSE,\n        GGML_OP_AXPY,\n        GGML_OP_OUT_PROD,\n\n        GGML_OP_SCALE,\n        GGML_OP_SET,\n        GGML_OP_CPY,\n        GGML_OP_CONT,\n        GGML_OP_RESHAPE,\n        GGML_OP_VIEW,\n        GGML_OP_PERMUTE,\n        GGML_OP_TRANSPOSE,\n        GGML_OP_GET_ROWS,\n        GGML_OP_GET_ROWS_BACK,\n        GGML_OP_DIAG,\n        GGML_OP_DIAG_MASK_INF,\n        GGML_OP_DIAG_MASK_ZERO,\n        GGML_OP_SOFT_MAX,\n        GGML_OP_SOFT_MAX_BACK,\n        GGML_OP_ROPE,\n        GGML_OP_ROPE_BACK,\n        GGML_OP_ALIBI,\n        GGML_OP_CLAMP,\n        GGML_OP_CONV_TRANSPOSE_1D,\n        GGML_OP_IM2COL,\n        GGML_OP_CONV_TRANSPOSE_2D,\n        GGML_OP_POOL_1D,\n        GGML_OP_POOL_2D,\n\n        GGML_OP_UPSCALE, // nearest interpolate\n\n        GGML_OP_FLASH_ATTN,\n        GGML_OP_FLASH_FF,\n        GGML_OP_FLASH_ATTN_BACK,\n        GGML_OP_WIN_PART,\n        GGML_OP_WIN_UNPART,\n        GGML_OP_GET_REL_POS,\n        GGML_OP_ADD_REL_POS,\n\n        GGML_OP_UNARY,\n\n        GGML_OP_MAP_UNARY,\n        GGML_OP_MAP_BINARY,\n\n        GGML_OP_MAP_CUSTOM1_F32,\n        GGML_OP_MAP_CUSTOM2_F32,\n        GGML_OP_MAP_CUSTOM3_F32,\n\n        GGML_OP_MAP_CUSTOM1,\n        GGML_OP_MAP_CUSTOM2,\n        GGML_OP_MAP_CUSTOM3,\n\n        GGML_OP_CROSS_ENTROPY_LOSS,\n        GGML_OP_CROSS_ENTROPY_LOSS_BACK,\n\n        GGML_OP_COUNT,\n    };\n\n    enum ggml_unary_op {\n        GGML_UNARY_OP_ABS,\n        GGML_UNARY_OP_SGN,\n        GGML_UNARY_OP_NEG,\n        GGML_UNARY_OP_STEP,\n        GGML_UNARY_OP_TANH,\n        GGML_UNARY_OP_ELU,\n        GGML_UNARY_OP_RELU,\n        GGML_UNARY_OP_GELU,\n        GGML_UNARY_OP_GELU_QUICK,\n        GGML_UNARY_OP_SILU,\n        GGML_UNARY_OP_LEAKY\n    };\n\n    enum ggml_object_type {\n        GGML_OBJECT_TENSOR,\n        GGML_OBJECT_GRAPH,\n        GGML_OBJECT_WORK_BUFFER\n    };\n\n    enum ggml_log_level {\n        GGML_LOG_LEVEL_ERROR = 2,\n        GGML_LOG_LEVEL_WARN = 3,\n        GGML_LOG_LEVEL_INFO = 4\n    };\n\n    // ggml object\n    struct ggml_object {\n        size_t offs;\n        size_t size;\n\n        struct ggml_object * next;\n\n        enum ggml_object_type type;\n\n        char padding[4];\n    };\n\n    static const size_t GGML_OBJECT_SIZE = sizeof(struct ggml_object);\n\n    // n-dimensional tensor\n    struct ggml_tensor {\n        enum ggml_type         type;\n        enum ggml_backend_type backend;\n\n        struct ggml_backend_buffer * buffer;\n\n        int     n_dims;\n        int64_t ne[GGML_MAX_DIMS]; // number of elements\n        size_t  nb[GGML_MAX_DIMS]; // stride in bytes:\n                                   // nb[0] = ggml_type_size(type)\n                                   // nb[1] = nb[0]   * (ne[0] / ggml_blck_size(type)) + padding\n                                   // nb[i] = nb[i-1] * ne[i-1]\n\n        // compute data\n        enum ggml_op op;\n\n        // op params - allocated as int32_t for alignment\n        int32_t op_params[GGML_MAX_OP_PARAMS / sizeof(int32_t)];\n\n        bool is_param;\n\n        struct ggml_tensor * grad;\n        struct ggml_tensor * src[GGML_MAX_SRC];\n\n        // performance\n        atomic_int is_finish;\n        int     perf_runs;\n        int64_t perf_cycles;\n        int64_t perf_time_us;\n\n        struct ggml_tensor * view_src;\n        size_t               view_offs;\n\n        void * data;\n\n        char name[GGML_MAX_NAME];\n\n        void * extra; // extra things e.g. for ggml-cuda.cu\n\n        char padding[12];\n    };\n\n\n    static const int64_t GGML_NE_WILDCARD = -1;\n\n    static const size_t GGML_TENSOR_SIZE = sizeof(struct ggml_tensor);\n\n    // the compute plan that needs to be prepared for ggml_graph_compute()\n    // since https://github.com/ggerganov/ggml/issues/287\n    struct ggml_cplan {\n        size_t    work_size; // size of work buffer, calculated by `ggml_graph_plan()`\n        uint8_t * work_data; // work buffer, to be allocated by caller before calling to `ggml_graph_compute()`\n\n        int n_threads;\n\n        // abort ggml_graph_compute when true\n        bool (*abort_callback)(void * data);\n        void * abort_callback_data;\n    };\n\n    enum ggml_cgraph_eval_order {\n        GGML_CGRAPH_EVAL_ORDER_LEFT_TO_RIGHT = 0,\n        GGML_CGRAPH_EVAL_ORDER_RIGHT_TO_LEFT,\n        GGML_CGRAPH_EVAL_ORDER_COUNT\n    };\n\n    struct ggml_hash_set {\n        size_t size;\n        struct ggml_tensor ** keys;\n    };\n\n    // computation graph\n    struct ggml_cgraph {\n        int size;\n        int n_nodes;\n        int n_leafs;\n\n        struct ggml_tensor ** nodes;\n        struct ggml_tensor ** grads;\n        struct ggml_tensor ** leafs;\n\n        struct ggml_hash_set visited_hash_table;\n\n        enum ggml_cgraph_eval_order order;\n\n        // performance\n        int     perf_runs;\n        int64_t perf_cycles;\n        int64_t perf_time_us;\n    };\n\n    // scratch buffer\n    struct ggml_scratch {\n        size_t offs;\n        size_t size;\n        void * data;\n    };\n\n    struct ggml_context {\n        size_t mem_size;\n        void * mem_buffer;\n        bool   mem_buffer_owned;\n        bool   no_alloc;\n        bool   no_alloc_save; // this is used to save the no_alloc state when using scratch buffers\n\n        int    n_objects;\n\n        struct ggml_object * objects_begin;\n        struct ggml_object * objects_end;\n\n        struct ggml_scratch scratch;\n        struct ggml_scratch scratch_save;\n    };\n\n    struct ggml_init_params {\n        // memory pool\n        size_t mem_size;   // bytes\n        void * mem_buffer; // if NULL, memory will be allocated internally\n        bool   no_alloc;   // don't allocate memory for the tensor data\n    };\n\n\n    // compute types\n\n    // NOTE: the INIT or FINALIZE pass is not scheduled unless explicitly enabled.\n    // This behavior was changed since https://github.com/ggerganov/llama.cpp/pull/1995.\n    enum ggml_task_type {\n        GGML_TASK_INIT = 0,\n        GGML_TASK_COMPUTE,\n        GGML_TASK_FINALIZE,\n    };\n\n    struct ggml_compute_params {\n        enum ggml_task_type type;\n\n        // ith = thread index, nth = number of threads\n        int ith, nth;\n\n        // work buffer for all threads\n        size_t wsize;\n        void * wdata;\n        atomic_int *aic;\n    };\n\n    // misc\n\n    GGML_API void    ggml_time_init(void); // call this once at the beginning of the program\n    GGML_API int64_t ggml_time_ms(void);\n    GGML_API int64_t ggml_time_us(void);\n    GGML_API int64_t ggml_cycles(void);\n    GGML_API int64_t ggml_cycles_per_ms(void);\n\n    GGML_API void    ggml_print_backtrace(void);\n\n    GGML_API void    ggml_numa_init(void); // call once for better performance on NUMA systems\n    GGML_API bool    ggml_is_numa(void); // true if init detected that system has >1 NUMA node\n\n    GGML_API void    ggml_print_object (const struct ggml_object * obj);\n    GGML_API void    ggml_print_objects(const struct ggml_context * ctx);\n\n    GGML_API \n\n    GGML_API int64_t ggml_nelements   (const struct ggml_tensor * tensor);\n    GGML_API int64_t ggml_nrows       (const struct ggml_tensor * tensor);\n    GGML_API size_t  ggml_nbytes      (const struct ggml_tensor * tensor);\n    GGML_API size_t  ggml_nbytes_pad  (const struct ggml_tensor * tensor); // same as ggml_nbytes() but padded to GGML_MEM_ALIGN\n    GGML_API size_t  ggml_nbytes_split(const struct ggml_tensor * tensor, int nrows_split);\n\n    GGML_API int     ggml_blck_size (enum ggml_type type);\n    GGML_API size_t  ggml_type_size (enum ggml_type type); // size in bytes for all elements in a block\n    GGML_API float   ggml_type_sizef(enum ggml_type type); // ggml_type_size()/ggml_blck_size() as float\n\n    GGML_API const char * ggml_type_name(enum ggml_type type);\n    GGML_API const char * ggml_op_name  (enum ggml_op   op);\n    GGML_API const char * ggml_op_symbol(enum ggml_op   op);\n\n    GGML_API size_t  ggml_element_size(const struct ggml_tensor * tensor);\n\n    GGML_API bool    ggml_is_quantized(enum ggml_type type);\n\n    // TODO: temporary until model loading of ggml examples is refactored\n    GGML_API enum ggml_type ggml_ftype_to_ggml_type(enum ggml_ftype ftype);\n\n    GGML_API bool ggml_is_transposed(const struct ggml_tensor * tensor);\n    GGML_API bool ggml_is_contiguous(const struct ggml_tensor * tensor);\n    GGML_API bool ggml_is_permuted  (const struct ggml_tensor * tensor);\n\n    GGML_API bool ggml_are_same_shape(const struct ggml_tensor * t0, const struct ggml_tensor * t1);\n\n    // use this to compute the memory overhead of a tensor\n    GGML_API size_t ggml_tensor_overhead(void);\n\n    // main\n\n    GGML_API struct ggml_context * ggml_init(struct ggml_init_params params);\n    GGML_API void                  ggml_free(struct ggml_context * ctx);\n\n    GGML_API size_t  ggml_used_mem(const struct ggml_context * ctx);\n\n    GGML_API size_t  ggml_set_scratch (struct ggml_context * ctx, struct ggml_scratch scratch);\n    GGML_API bool    ggml_get_no_alloc(struct ggml_context * ctx);\n    GGML_API void    ggml_set_no_alloc(struct ggml_context * ctx, bool no_alloc);\n\n    GGML_API void *  ggml_get_mem_buffer     (const struct ggml_context * ctx);\n    GGML_API size_t  ggml_get_mem_size       (const struct ggml_context * ctx);\n    GGML_API size_t  ggml_get_max_tensor_size(const struct ggml_context * ctx);\n\n    GGML_API struct ggml_tensor * ggml_new_tensor(\n            struct ggml_context * ctx,\n            enum   ggml_type type,\n            int    n_dims,\n            const int64_t *ne);\n\n    GGML_API struct ggml_tensor * ggml_new_tensor_1d(\n            struct ggml_context * ctx,\n            enum   ggml_type type,\n            int64_t ne0);\n\n    GGML_API struct ggml_tensor * ggml_new_tensor_2d(\n            struct ggml_context * ctx,\n            enum   ggml_type type,\n            int64_t ne0,\n            int64_t ne1);\n\n    GGML_API struct ggml_tensor * ggml_new_tensor_3d(\n            struct ggml_context * ctx,\n            enum   ggml_type type,\n            int64_t ne0,\n            int64_t ne1,\n            int64_t ne2);\n\n    GGML_API struct ggml_tensor * ggml_new_tensor_4d(\n            struct ggml_context * ctx,\n            enum   ggml_type type,\n            int64_t ne0,\n            int64_t ne1,\n            int64_t ne2,\n            int64_t ne3);\n\n    GGML_API struct ggml_tensor * ggml_new_i32(struct ggml_context * ctx, int32_t value);\n    GGML_API struct ggml_tensor * ggml_new_f32(struct ggml_context * ctx, float value);\n\n    GGML_API struct ggml_tensor * ggml_dup_tensor (struct ggml_context * ctx, const struct ggml_tensor * src);\n    GGML_API struct ggml_tensor * ggml_view_tensor(struct ggml_context * ctx, struct ggml_tensor * src);\n\n    // Context tensor enumeration and lookup\n    GGML_API struct ggml_tensor * ggml_get_first_tensor(struct ggml_context * ctx);\n    GGML_API struct ggml_tensor * ggml_get_next_tensor (struct ggml_context * ctx, struct ggml_tensor * tensor);\n    GGML_API struct ggml_tensor * ggml_get_tensor(struct ggml_context * ctx, const char * name);\n\n    GGML_API struct ggml_tensor * ggml_set_zero(struct ggml_tensor * tensor);\n    GGML_API struct ggml_tensor * ggml_set_i32 (struct ggml_tensor * tensor, int32_t value);\n    GGML_API struct ggml_tensor * ggml_set_f32 (struct ggml_tensor * tensor, float value);\n\n    // Converts a flat index into coordinates\n    GGML_API void    ggml_unravel_index(const struct ggml_tensor * tensor, int64_t i, int64_t * i0, int64_t * i1, int64_t * i2, int64_t * i3);\n\n    GGML_API int32_t ggml_get_i32_1d(const struct ggml_tensor * tensor, int i);\n    GGML_API void    ggml_set_i32_1d(const struct ggml_tensor * tensor, int i, int32_t value);\n\n    GGML_API int32_t ggml_get_i32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3);\n    GGML_API void    ggml_set_i32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3, int32_t value);\n\n    GGML_API float   ggml_get_f32_1d(const struct ggml_tensor * tensor, int i);\n    GGML_API void    ggml_set_f32_1d(const struct ggml_tensor * tensor, int i, float value);\n\n    GGML_API float   ggml_get_f32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3);\n    GGML_API void    ggml_set_f32_nd(const struct ggml_tensor * tensor, int i0, int i1, int i2, int i3, float value);\n\n    GGML_API void *  ggml_get_data    (const struct ggml_tensor * tensor);\n    GGML_API float * ggml_get_data_f32(const struct ggml_tensor * tensor);\n    GGML_API int32_t * ggml_get_data_i32(const struct ggml_tensor * tensor);\n\n    GGML_API enum ggml_unary_op ggml_get_unary_op(const struct ggml_tensor * tensor);\n\n    GGML_API const char *         ggml_get_name   (const struct ggml_tensor * tensor);\n    GGML_API struct ggml_tensor * ggml_set_name   (      struct ggml_tensor * tensor, const char * name);\n    GGML_ATTRIBUTE_FORMAT(2, 3)\n    GGML_API struct ggml_tensor * ggml_format_name(      struct ggml_tensor * tensor, const char * fmt, ...);\n\n    GGML_API void ggml_set_backend(struct ggml_tensor * tensor, enum ggml_backend_type backend);\n\n\n    //\n    // operations on tensors with backpropagation\n    //\n\n    GGML_API struct ggml_tensor * ggml_dup(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_dup_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_add(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor *ggml_add_idx(\n            struct ggml_context *ctx,\n            struct ggml_tensor *a,\n            struct ggml_tensor *b,\n            struct ggml_tensor *idx);\n\n    GGML_API struct ggml_tensor * ggml_add_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_add_cast(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            enum   ggml_type      type);\n\n    GGML_API struct ggml_tensor * ggml_add1(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_add1_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_acc(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            size_t                nb1,\n            size_t                nb2,\n            size_t                nb3,\n            size_t                offset);\n\n    GGML_API struct ggml_tensor * ggml_acc_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            size_t                nb1,\n            size_t                nb2,\n            size_t                nb3,\n            size_t                offset);\n\n    GGML_API struct ggml_tensor * ggml_sub(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_sub_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_mul(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_mul_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_div(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_div_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_sqr(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_sqr_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_sqrt(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_sqrt_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_log(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_log_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // return scalar\n    GGML_API struct ggml_tensor * ggml_sum(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // sums along rows, with input shape [a,b,c,d] return shape [1,b,c,d]\n    GGML_API struct ggml_tensor * ggml_sum_rows(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // mean along rows\n    GGML_API struct ggml_tensor * ggml_mean(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // argmax along rows\n    GGML_API struct ggml_tensor * ggml_argmax(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // if a is the same shape as b, and a is not parameter, return a\n    // otherwise, return a new tensor: repeat(a) to fit in b\n    GGML_API struct ggml_tensor * ggml_repeat(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // sums repetitions in a into shape of b\n    GGML_API struct ggml_tensor * ggml_repeat_back(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // concat a and b on dim 2\n    // used in stable-diffusion\n    GGML_API struct ggml_tensor * ggml_concat(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_abs(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_abs_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_sgn(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_sgn_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_neg(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_neg_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_step(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_step_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_tanh(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_tanh_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_elu(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_elu_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_relu(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_leaky(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_relu_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // TODO: double-check this computation is correct\n    GGML_API struct ggml_tensor * ggml_gelu(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_gelu_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_gelu_quick(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_gelu_quick_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_silu(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_silu_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // a - x\n    // b - dy\n    GGML_API struct ggml_tensor * ggml_silu_back(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // normalize along rows\n    GGML_API struct ggml_tensor * ggml_norm(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            float                 eps);\n\n    GGML_API struct ggml_tensor * ggml_norm_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            float                 eps);\n\n    GGML_API struct ggml_tensor * ggml_rms_norm(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            float                 eps);\n\n    GGML_API struct ggml_tensor * ggml_rms_norm_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            float                 eps);\n\n    // group normalize along ne0*ne1*n_groups\n    // used in stable-diffusion\n    // TODO: eps is hardcoded to 1e-6 for now\n    GGML_API struct ggml_tensor * ggml_group_norm(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   n_groups);\n\n    GGML_API struct ggml_tensor * ggml_group_norm_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   n_groups);\n\n    // a - x\n    // b - dy\n    GGML_API struct ggml_tensor * ggml_rms_norm_back(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            float                 eps);\n\n    // A: k columns, n rows => [ne03, ne02, n, k]\n    // B: k columns, m rows  (i.e. we transpose it internally) => [ne03 * x, ne02 * y, m, k]\n    // result is n columns, m rows => [ne03 * x, ne02 * y, m, n]\n    GGML_API struct ggml_tensor * ggml_mul_mat(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n    GGML_API struct ggml_tensor *ggml_mul_mat_idx(\n            struct ggml_context *ctx,\n            struct ggml_tensor *a,\n            struct ggml_tensor *b,\n            struct ggml_tensor *sparse_idx,\n            struct ggml_tensor *gpu_idx);\n    GGML_API struct ggml_tensor *ggml_mul_mat_idx_upscale(\n            struct ggml_context *ctx,\n            struct ggml_tensor *a,\n            struct ggml_tensor *b,\n            struct ggml_tensor *sparse_idx,\n            struct ggml_tensor *gpu_bucket,\n                        int64_t result_ne0);\n    GGML_API struct ggml_tensor *ggml_axpy(\n            struct ggml_context *ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            struct ggml_tensor  * sparse_idx,\n            struct ggml_tensor  * hybrid_aux);\n\n    // A: m columns, n rows,\n    // B: p columns, n rows,\n    // result is m columns, p rows\n    GGML_API struct ggml_tensor * ggml_out_prod(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    //\n    // operations on tensors without backpropagation\n    //\n\n    GGML_API struct ggml_tensor * ggml_scale(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_scale_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // b -> view(a,offset,nb1,nb2,3), return modified a\n    GGML_API struct ggml_tensor * ggml_set(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            size_t                nb1,\n            size_t                nb2,\n            size_t                nb3,\n            size_t                offset);\n\n    // b -> view(a,offset,nb1,nb2,3), return view(a)\n    GGML_API struct ggml_tensor * ggml_set_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            size_t                nb1,\n            size_t                nb2,\n            size_t                nb3,\n            size_t                offset);\n\n    GGML_API struct ggml_tensor * ggml_set_1d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            size_t                offset);\n\n    GGML_API struct ggml_tensor * ggml_set_1d_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            size_t                offset);\n\n    // b -> view(a,offset,nb1,nb2,3), return modified a\n    GGML_API struct ggml_tensor * ggml_set_2d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            size_t                nb1,\n            size_t                offset);\n\n    // b -> view(a,offset,nb1,nb2,3), return view(a)\n    GGML_API struct ggml_tensor * ggml_set_2d_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            size_t                nb1,\n            size_t                offset);\n\n    // a -> b, return view(b)\n    GGML_API struct ggml_tensor * ggml_cpy(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // a -> b, in-place, return view(b)\n    GGML_API struct ggml_tensor * ggml_cpy_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // make contiguous\n    GGML_API struct ggml_tensor * ggml_cont(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // make contiguous, in-place\n    GGML_API struct ggml_tensor * ggml_cont_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // make contiguous, with new shape\n    GGML_API struct ggml_tensor * ggml_cont_1d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0);\n\n    GGML_API struct ggml_tensor * ggml_cont_2d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1);\n\n    GGML_API struct ggml_tensor * ggml_cont_3d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1,\n            int64_t               ne2);\n\n    GGML_API struct ggml_tensor * ggml_cont_4d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1,\n            int64_t               ne2,\n            int64_t               ne3);\n\n    // return view(a), b specifies the new shape\n    // TODO: when we start computing gradient, make a copy instead of view\n    GGML_API struct ggml_tensor * ggml_reshape(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // return view(a)\n    // TODO: when we start computing gradient, make a copy instead of view\n    GGML_API struct ggml_tensor * ggml_reshape_1d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0);\n\n    GGML_API struct ggml_tensor * ggml_reshape_2d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1);\n\n    // return view(a)\n    // TODO: when we start computing gradient, make a copy instead of view\n    GGML_API struct ggml_tensor * ggml_reshape_3d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1,\n            int64_t               ne2);\n\n    GGML_API struct ggml_tensor * ggml_reshape_4d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1,\n            int64_t               ne2,\n            int64_t               ne3);\n\n    // offset in bytes\n    GGML_API struct ggml_tensor * ggml_view_1d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            size_t                offset);\n\n    GGML_API struct ggml_tensor * ggml_view_2d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1,\n            size_t                nb1, // row stride in bytes\n            size_t                offset);\n\n    GGML_API struct ggml_tensor * ggml_view_3d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1,\n            int64_t               ne2,\n            size_t                nb1, // row   stride in bytes\n            size_t                nb2, // slice stride in bytes\n            size_t                offset);\n\n    GGML_API struct ggml_tensor * ggml_view_4d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int64_t               ne0,\n            int64_t               ne1,\n            int64_t               ne2,\n            int64_t               ne3,\n            size_t                nb1, // row   stride in bytes\n            size_t                nb2, // slice stride in bytes\n            size_t                nb3,\n            size_t                offset);\n\n    GGML_API struct ggml_tensor * ggml_permute(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   axis0,\n            int                   axis1,\n            int                   axis2,\n            int                   axis3);\n\n    // alias for ggml_permute(ctx, a, 1, 0, 2, 3)\n    GGML_API struct ggml_tensor * ggml_transpose(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_get_rows(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_get_rows_back(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            struct ggml_tensor  * c);\n\n    GGML_API struct ggml_tensor * ggml_diag(\n        struct ggml_context     * ctx,\n        struct ggml_tensor      * a);\n\n    // set elements above the diagonal to -INF\n    GGML_API struct ggml_tensor * ggml_diag_mask_inf(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   n_past);\n\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_diag_mask_inf_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   n_past);\n\n    // set elements above the diagonal to 0\n    GGML_API struct ggml_tensor * ggml_diag_mask_zero(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   n_past);\n\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_diag_mask_zero_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   n_past);\n\n    GGML_API struct ggml_tensor * ggml_soft_max(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_soft_max_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a);\n\n    GGML_API struct ggml_tensor * ggml_soft_max_back(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_soft_max_back_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // rotary position embedding\n    // if mode & 1 == 1, skip n_past elements (DEPRECATED)\n    // if mode & 2 == 1, GPT-NeoX style\n    // if mode & 4 == 1, ChatGLM style\n    //\n    // b is an int32 vector with size a->ne[2], it contains the positions\n    GGML_API struct ggml_tensor * ggml_rope(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   n_dims,\n            int                   mode,\n            int                   n_ctx);\n\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_rope_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   n_dims,\n            int                   mode,\n            int                   n_ctx);\n\n    // custom RoPE\n    GGML_API struct ggml_tensor * ggml_rope_custom(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   n_dims,\n            int                   mode,\n            int                   n_ctx,\n            int                   n_orig_ctx,\n            float                 freq_base,\n            float                 freq_scale,\n            float                 ext_factor,\n            float                 attn_factor,\n            float                 beta_fast,\n            float                 beta_slow);\n\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_rope_custom_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   n_dims,\n            int                   mode,\n            int                   n_ctx,\n            int                   n_orig_ctx,\n            float                 freq_base,\n            float                 freq_scale,\n            float                 ext_factor,\n            float                 attn_factor,\n            float                 beta_fast,\n            float                 beta_slow);\n\n    // compute correction dims for YaRN RoPE scaling\n    void ggml_rope_yarn_corr_dims(\n        int n_dims, int n_orig_ctx, float freq_base, float beta_fast, float beta_slow, float dims[2]);\n\n    // xPos RoPE, in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_rope_xpos_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   n_dims,\n            float                 base,\n            bool                  down);\n\n    // rotary position embedding backward, i.e compute dx from dy\n    // a - dy\n    GGML_API struct ggml_tensor * ggml_rope_back(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   n_dims,\n            int                   mode,\n            int                   n_ctx,\n            int                   n_orig_ctx,\n            float                 freq_base,\n            float                 freq_scale,\n            float                 ext_factor,\n            float                 attn_factor,\n            float                 beta_fast,\n            float                 beta_slow,\n            float                 xpos_base,\n            bool                  xpos_down);\n\n    // alibi position embedding\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_alibi(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   n_past,\n            int                   n_head,\n            float                 bias_max);\n\n    // clamp\n    // in-place, returns view(a)\n    GGML_API struct ggml_tensor * ggml_clamp(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            float                 min,\n            float                 max);\n\n    GGML_API struct ggml_tensor * ggml_im2col(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                  s0,\n            int                  s1,\n            int                  p0,\n            int                  p1,\n            int                  d0,\n            int                  d1,\n            bool                 is_2D);\n\n    GGML_API struct ggml_tensor * ggml_conv_1d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   s0,  // stride\n            int                   p0,  // padding\n            int                   d0); // dilation\n\n    // conv_1d with padding = half\n    // alias for ggml_conv_1d(a, b, s, a->ne[0]/2, d)\n    GGML_API struct ggml_tensor* ggml_conv_1d_ph(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   s,\n            int                   d);\n\n    GGML_API struct ggml_tensor * ggml_conv_transpose_1d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   s0,\n            int                   p0,\n            int                   d0);\n\n    GGML_API struct ggml_tensor * ggml_conv_2d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   s0,\n            int                   s1,\n            int                   p0,\n            int                   p1,\n            int                   d0,\n            int                   d1);\n\n\n    // kernel size is a->ne[0] x a->ne[1]\n    // stride is equal to kernel size\n    // padding is zero\n    // example:\n    // a:     16   16    3  768\n    // b:   1024 1024    3    1\n    // res:   64   64  768    1\n    // used in sam\n    GGML_API struct ggml_tensor * ggml_conv_2d_sk_p0(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    // kernel size is a->ne[0] x a->ne[1]\n    // stride is 1\n    // padding is half\n    // example:\n    // a:      3    3    256  256\n    // b:     64   64    256    1\n    // res:   64   64    256    1\n    // used in sam\n    GGML_API struct ggml_tensor * ggml_conv_2d_s1_ph(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b);\n\n    GGML_API struct ggml_tensor * ggml_conv_transpose_2d_p0(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b,\n            int                   stride);\n\n    enum ggml_op_pool {\n        GGML_OP_POOL_MAX,\n        GGML_OP_POOL_AVG,\n        GGML_OP_POOL_COUNT,\n    };\n\n    GGML_API struct ggml_tensor * ggml_pool_1d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            enum ggml_op_pool     op,\n            int                   k0, // kernel size\n            int                   s0, // stride\n            int                   p0); // padding\n\n    // the result will have 2*p0 padding for the first dimension\n    // and 2*p1 padding for the second dimension\n    GGML_API struct ggml_tensor * ggml_pool_2d(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            enum ggml_op_pool     op,\n            int                   k0,\n            int                   k1,\n            int                   s0,\n            int                   s1,\n            float                 p0,\n            float                 p1);\n\n    // nearest interpolate\n    // used in stable-diffusion\n    GGML_API struct ggml_tensor * ggml_upscale(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   scale_factor);\n\n    GGML_API struct ggml_tensor * ggml_flash_attn(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * q,\n            struct ggml_tensor  * k,\n            struct ggml_tensor  * v,\n            bool                  masked);\n\n    GGML_API struct ggml_tensor * ggml_flash_attn_back(\n           struct ggml_context * ctx,\n           struct ggml_tensor  * q,\n           struct ggml_tensor  * k,\n           struct ggml_tensor  * v,\n           struct ggml_tensor  * d,\n           bool                  masked);\n\n    GGML_API struct ggml_tensor * ggml_flash_ff(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * b0,\n            struct ggml_tensor  * b1,\n            struct ggml_tensor  * c0,\n            struct ggml_tensor  * c1);\n\n    // partition into non-overlapping windows with padding if needed\n    // example:\n    // a:   768   64   64    1\n    // w:    14\n    // res: 768   14   14    25\n    // used in sam\n    GGML_API struct ggml_tensor * ggml_win_part(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   w);\n\n    // reverse of ggml_win_part\n    // used in sam\n    GGML_API struct ggml_tensor * ggml_win_unpart(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   w0,\n            int                   h0,\n            int                   w);\n\n    GGML_API struct ggml_tensor * ggml_unary(\n            struct ggml_context * ctx,\n             struct ggml_tensor * a,\n             enum ggml_unary_op op);\n\n    GGML_API struct ggml_tensor * ggml_unary_inplace(\n        struct ggml_context * ctx,\n        struct ggml_tensor  * a,\n        enum ggml_unary_op op);\n\n    // used in sam\n    GGML_API struct ggml_tensor * ggml_get_rel_pos(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            int                   qh,\n            int                   kh);\n\n    // used in sam\n\n    GGML_API struct ggml_tensor * ggml_add_rel_pos(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * pw,\n            struct ggml_tensor  * ph);\n\n    GGML_API struct ggml_tensor * ggml_add_rel_pos_inplace(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * a,\n            struct ggml_tensor  * pw,\n            struct ggml_tensor  * ph);\n\n    // custom operators\n\n    typedef void (*ggml_unary_op_f32_t) (const int, float *, const float *);\n    typedef void (*ggml_binary_op_f32_t)(const int, float *, const float *, const float *);\n\n    typedef void (*ggml_custom1_op_f32_t)(struct ggml_tensor *, const struct ggml_tensor *);\n    typedef void (*ggml_custom2_op_f32_t)(struct ggml_tensor *, const struct ggml_tensor *, const struct ggml_tensor *);\n    typedef void (*ggml_custom3_op_f32_t)(struct ggml_tensor *, const struct ggml_tensor *, const struct ggml_tensor *, const struct ggml_tensor *);\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_unary_f32(\n            struct ggml_context        * ctx,\n            struct ggml_tensor         * a,\n                   ggml_unary_op_f32_t   fun),\n        \"use ggml_map_custom1 instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_unary_inplace_f32(\n            struct ggml_context        * ctx,\n            struct ggml_tensor         * a,\n                   ggml_unary_op_f32_t   fun),\n        \"use ggml_map_custom1_inplace instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_binary_f32(\n            struct ggml_context         * ctx,\n            struct ggml_tensor          * a,\n            struct ggml_tensor          * b,\n                   ggml_binary_op_f32_t   fun),\n        \"use ggml_map_custom2 instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_binary_inplace_f32(\n            struct ggml_context         * ctx,\n            struct ggml_tensor          * a,\n            struct ggml_tensor          * b,\n                   ggml_binary_op_f32_t   fun),\n        \"use ggml_map_custom2_inplace instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_custom1_f32(\n            struct ggml_context          * ctx,\n            struct ggml_tensor           * a,\n                   ggml_custom1_op_f32_t   fun),\n        \"use ggml_map_custom1 instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_custom1_inplace_f32(\n            struct ggml_context          * ctx,\n            struct ggml_tensor           * a,\n                   ggml_custom1_op_f32_t   fun),\n        \"use ggml_map_custom1_inplace instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_custom2_f32(\n            struct ggml_context          * ctx,\n            struct ggml_tensor           * a,\n            struct ggml_tensor           * b,\n                   ggml_custom2_op_f32_t   fun),\n        \"use ggml_map_custom2 instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_custom2_inplace_f32(\n            struct ggml_context          * ctx,\n            struct ggml_tensor           * a,\n            struct ggml_tensor           * b,\n                   ggml_custom2_op_f32_t   fun),\n        \"use ggml_map_custom2_inplace instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_custom3_f32(\n            struct ggml_context          * ctx,\n            struct ggml_tensor           * a,\n            struct ggml_tensor           * b,\n            struct ggml_tensor           * c,\n                   ggml_custom3_op_f32_t   fun),\n        \"use ggml_map_custom3 instead\");\n\n    GGML_DEPRECATED(GGML_API struct ggml_tensor * ggml_map_custom3_inplace_f32(\n            struct ggml_context          * ctx,\n            struct ggml_tensor           * a,\n            struct ggml_tensor           * b,\n            struct ggml_tensor           * c,\n                   ggml_custom3_op_f32_t   fun),\n        \"use ggml_map_custom3_inplace instead\");\n\n    // custom operators v2\n\n    typedef void (*ggml_custom1_op_t)(struct ggml_tensor * dst , const struct ggml_tensor * a, int ith, int nth, void * userdata);\n    typedef void (*ggml_custom2_op_t)(struct ggml_tensor * dst , const struct ggml_tensor * a, const struct ggml_tensor * b, int ith, int nth, void * userdata);\n    typedef void (*ggml_custom3_op_t)(struct ggml_tensor * dst , const struct ggml_tensor * a, const struct ggml_tensor * b, const struct ggml_tensor * c, int ith, int nth, void * userdata);\n\n    #define GGML_N_TASKS_MAX -1\n\n    GGML_API struct ggml_tensor * ggml_map_custom1(\n            struct ggml_context   * ctx,\n            struct ggml_tensor    * a,\n            ggml_custom1_op_t       fun,\n            int                     n_tasks,\n            void                  * userdata);\n\n    GGML_API struct ggml_tensor * ggml_map_custom1_inplace(\n            struct ggml_context   * ctx,\n            struct ggml_tensor    * a,\n            ggml_custom1_op_t       fun,\n            int                     n_tasks,\n            void                  * userdata);\n\n    GGML_API struct ggml_tensor * ggml_map_custom2(\n            struct ggml_context   * ctx,\n            struct ggml_tensor    * a,\n            struct ggml_tensor    * b,\n            ggml_custom2_op_t       fun,\n            int                     n_tasks,\n            void                  * userdata);\n\n    GGML_API struct ggml_tensor * ggml_map_custom2_inplace(\n            struct ggml_context   * ctx,\n            struct ggml_tensor    * a,\n            struct ggml_tensor    * b,\n            ggml_custom2_op_t       fun,\n            int                     n_tasks,\n            void                  * userdata);\n\n    GGML_API struct ggml_tensor * ggml_map_custom3(\n            struct ggml_context   * ctx,\n            struct ggml_tensor    * a,\n            struct ggml_tensor    * b,\n            struct ggml_tensor    * c,\n            ggml_custom3_op_t       fun,\n            int                     n_tasks,\n            void                  * userdata);\n\n    GGML_API struct ggml_tensor * ggml_map_custom3_inplace(\n            struct ggml_context   * ctx,\n            struct ggml_tensor    * a,\n            struct ggml_tensor    * b,\n            struct ggml_tensor    * c,\n            ggml_custom3_op_t       fun,\n            int                     n_tasks,\n            void                  * userdata);\n\n    // loss function\n\n    GGML_API struct ggml_tensor * ggml_cross_entropy_loss(\n            struct ggml_context         * ctx,\n            struct ggml_tensor          * a,\n            struct ggml_tensor          * b);\n\n    GGML_API struct ggml_tensor * ggml_cross_entropy_loss_back(\n            struct ggml_context         * ctx,\n            struct ggml_tensor          * a,\n            struct ggml_tensor          * b,\n            struct ggml_tensor          * c);\n\n    //\n    // automatic differentiation\n    //\n\n    GGML_API void ggml_set_param(\n            struct ggml_context * ctx,\n            struct ggml_tensor  * tensor);\n\n\n    GGML_API void ggml_build_forward_expand (struct ggml_cgraph * cgraph, struct ggml_tensor * tensor);\n    GGML_API void ggml_build_backward_expand(struct ggml_context * ctx, struct ggml_cgraph * gf, struct ggml_cgraph * gb, bool keep);\n\n    // graph allocation in a context\n    GGML_API struct ggml_cgraph * ggml_new_graph         (struct ggml_context * ctx); // size = GGML_DEFAULT_GRAPH_SIZE, grads = false\n    GGML_API struct ggml_cgraph * ggml_new_graph_custom  (struct ggml_context * ctx, size_t size, bool grads);\n    GGML_API struct ggml_cgraph * ggml_graph_dup         (struct ggml_context * ctx, struct ggml_cgraph * cgraph);\n    GGML_API struct ggml_cgraph * ggml_graph_view        (struct ggml_context * ctx, struct ggml_cgraph * cgraph, int i0, int i1);\n    GGML_API void                 ggml_graph_cpy         (struct ggml_cgraph * src, struct ggml_cgraph * dst);\n    GGML_API void                 ggml_graph_reset       (struct ggml_cgraph * cgraph);  // zero grads\n    GGML_API void                 ggml_graph_clear       (struct ggml_cgraph * cgraph);\n\n    GGML_API size_t ggml_graph_overhead(void);\n    GGML_API size_t ggml_graph_overhead_custom(size_t size, bool grads);\n\n    // ggml_graph_plan() has to be called before ggml_graph_compute()\n    // when plan.work_size > 0, caller must allocate memory for plan.work_data\n    GGML_API struct ggml_cplan ggml_graph_plan   (struct ggml_cgraph * cgraph, int n_threads /*= GGML_DEFAULT_N_THREADS*/);\n    GGML_API int               ggml_graph_compute(struct ggml_cgraph * cgraph, struct ggml_cplan * cplan);\n\n    // same as ggml_graph_compute() but the work data is allocated as a part of the context\n    // note: the drawback of this API is that you must have ensured that the context has enough memory for the work data\n    GGML_API void ggml_graph_compute_with_ctx(struct ggml_context * ctx, struct ggml_cgraph * cgraph, int n_threads);\n\n    GGML_API struct ggml_tensor * ggml_graph_get_tensor(struct ggml_cgraph * cgraph, const char * name);\n\n    GGML_API void                 ggml_graph_export(const struct ggml_cgraph * cgraph, const char * fname);\n    GGML_API struct ggml_cgraph * ggml_graph_import(const char * fname, struct ggml_context ** ctx_data, struct ggml_context ** ctx_eval);\n\n    // print info and performance information for the graph\n    GGML_API void ggml_graph_print(const struct ggml_cgraph * cgraph);\n\n    // dump the graph into a file using the dot format\n    GGML_API void ggml_graph_dump_dot(const struct ggml_cgraph * gb, const struct ggml_cgraph * gf, const char * filename);\n\n    // build gradient checkpointing backward graph gb for gf using provided checkpoints\n    // gb_tmp will contain original backward graph with rewritten backward process nodes,\n    // but without the second forward pass nodes.\n    GGML_API void ggml_build_backward_gradient_checkpointing(\n            struct ggml_context   * ctx,\n            struct ggml_cgraph    * gf,\n            struct ggml_cgraph    * gb,\n            struct ggml_cgraph    * gb_tmp,\n            struct ggml_tensor  * * checkpoints,\n            int                     n_checkpoints);\n    //\n    // optimization\n    //\n\n    // optimization methods\n    enum ggml_opt_type {\n        GGML_OPT_ADAM,\n        GGML_OPT_LBFGS,\n    };\n\n    // linesearch methods\n    enum ggml_linesearch {\n        GGML_LINESEARCH_DEFAULT = 1,\n\n        GGML_LINESEARCH_BACKTRACKING_ARMIJO       = 0,\n        GGML_LINESEARCH_BACKTRACKING_WOLFE        = 1,\n        GGML_LINESEARCH_BACKTRACKING_STRONG_WOLFE = 2,\n    };\n\n    // optimization return values\n    enum ggml_opt_result {\n        GGML_OPT_OK = 0,\n        GGML_OPT_DID_NOT_CONVERGE,\n        GGML_OPT_NO_CONTEXT,\n        GGML_OPT_INVALID_WOLFE,\n        GGML_OPT_FAIL,\n        GGML_OPT_CANCEL,\n\n        GGML_LINESEARCH_FAIL = -128,\n        GGML_LINESEARCH_MINIMUM_STEP,\n        GGML_LINESEARCH_MAXIMUM_STEP,\n        GGML_LINESEARCH_MAXIMUM_ITERATIONS,\n        GGML_LINESEARCH_INVALID_PARAMETERS,\n    };\n\n    typedef void (*ggml_opt_callback)(void * data, int accum_step, float * sched, bool * cancel);\n    typedef void (*ggml_log_callback)(enum ggml_log_level level, const char * text, void * user_data);\n\n    // optimization parameters\n    //\n    //   see ggml.c (ggml_opt_default_params) for default values\n    //\n    struct ggml_opt_params {\n        enum ggml_opt_type type;\n\n        size_t graph_size;\n\n        int n_threads;\n\n        // delta-based convergence test\n        //\n        //   if past == 0 - disabled\n        //   if past > 0:\n        //     stop if |f(x) - f(x_past)| < delta * max(1, |f(x)|)\n        //\n        int past;\n        float delta;\n\n        // maximum number of iterations without improvement\n        //\n        //   if 0 - disabled\n        //   if > 0:\n        //     assume convergence if no cost improvement in this number of iterations\n        //\n        int max_no_improvement;\n\n        bool print_forward_graph;\n        bool print_backward_graph;\n\n        int n_gradient_accumulation;\n\n        // ADAM parameters\n        struct {\n            int n_iter;\n\n            float sched; // schedule multiplier (fixed, decay or warmup)\n            float decay; // weight decay for AdamW, use 0.0f to disable\n            int   decay_min_ndim; // minimum number of tensor dimension to apply weight decay\n            float alpha; // learning rate\n            float beta1;\n            float beta2;\n            float eps;   // epsilon for numerical stability\n            float eps_f; // epsilon for convergence test\n            float eps_g; // epsilon for convergence test\n            float gclip; // gradient clipping\n        } adam;\n\n        // LBFGS parameters\n        struct {\n            int m; // number of corrections to approximate the inv. Hessian\n            int n_iter;\n            int max_linesearch;\n\n            float eps;      // convergence tolerance\n            float ftol;     // line search tolerance\n            float wolfe;\n            float min_step;\n            float max_step;\n\n            enum ggml_linesearch linesearch;\n        } lbfgs;\n    };\n\n    struct ggml_opt_context {\n        struct ggml_context * ctx;\n        struct ggml_opt_params params;\n\n        int iter;\n        int64_t nx; // number of parameter elements\n\n        bool just_initialized;\n\n        float loss_before;\n        float loss_after;\n\n        struct {\n            struct ggml_tensor * g;  // current gradient\n            struct ggml_tensor * m;  // first moment\n            struct ggml_tensor * v;  // second moment\n            struct ggml_tensor * pf; // past function values\n            float fx_best;\n            float fx_prev;\n            int n_no_improvement;\n        } adam;\n\n        struct {\n            struct ggml_tensor * x;    // current parameters\n            struct ggml_tensor * xp;   // previous parameters\n            struct ggml_tensor * g;    // current gradient\n            struct ggml_tensor * gp;   // previous gradient\n            struct ggml_tensor * d;    // search direction\n            struct ggml_tensor * pf;   // past function values\n            struct ggml_tensor * lmal; // the L-BFGS memory alpha\n            struct ggml_tensor * lmys; // the L-BFGS memory ys\n            struct ggml_tensor * lms;  // the L-BFGS memory s\n            struct ggml_tensor * lmy;  // the L-BFGS memory y\n            float fx_best;\n            float step;\n            int j;\n            int k;\n            int end;\n            int n_no_improvement;\n        } lbfgs;\n    };\n\n    GGML_API struct ggml_opt_params ggml_opt_default_params(enum ggml_opt_type type);\n\n    // optimize the function defined by the tensor f\n    GGML_API enum ggml_opt_result ggml_opt(\n            struct ggml_context * ctx,\n            struct ggml_opt_params params,\n            struct ggml_tensor * f);\n\n    // initialize optimizer context\n    GGML_API void ggml_opt_init(\n            struct ggml_context     * ctx,\n            struct ggml_opt_context * opt,\n            struct ggml_opt_params    params,\n            int64_t                   nx);\n\n    // continue optimizing the function defined by the tensor f\n    GGML_API enum ggml_opt_result ggml_opt_resume(\n            struct ggml_context * ctx,\n            struct ggml_opt_context * opt,\n            struct ggml_tensor * f);\n\n    // continue optimizing the function defined by the tensor f\n    GGML_API enum ggml_opt_result ggml_opt_resume_g(\n            struct ggml_context * ctx,\n            struct ggml_opt_context * opt,\n            struct ggml_tensor * f,\n            struct ggml_cgraph * gf,\n            struct ggml_cgraph * gb,\n            ggml_opt_callback callback,\n            void * callback_data);\n\n    //\n    // quantization\n    //\n\n    // TODO: these would probably get removed in favor of the more general ggml_quantize_chunk\n    GGML_API size_t ggml_quantize_q4_0(const float * src, void * dst, int n, int k, int64_t * hist);\n    GGML_API size_t ggml_quantize_q4_1(const float * src, void * dst, int n, int k, int64_t * hist);\n    GGML_API size_t ggml_quantize_q5_0(const float * src, void * dst, int n, int k, int64_t * hist);\n    GGML_API size_t ggml_quantize_q5_1(const float * src, void * dst, int n, int k, int64_t * hist);\n    GGML_API size_t ggml_quantize_q8_0(const float * src, void * dst, int n, int k, int64_t * hist);\n\n    GGML_API size_t ggml_quantize_q2_K(const float * src, void * dst, int n, int k, int64_t * hist);\n    GGML_API size_t ggml_quantize_q3_K(const float * src, void * dst, int n, int k, int64_t * hist);\n    GGML_API size_t ggml_quantize_q4_K(const float * src, void * dst, int n, int k, int64_t * hist);\n    GGML_API size_t ggml_quantize_q5_K(const float * src, void * dst, int n, int k, int64_t * hist);\n    GGML_API size_t ggml_quantize_q6_K(const float * src, void * dst, int n, int k, int64_t * hist);\n\n    GGML_API size_t ggml_quantize_chunk(enum ggml_type type, const float * src, void * dst, int start, int n, int64_t * hist);\n\n    //\n    // gguf\n    //\n\n    enum gguf_type {\n        GGUF_TYPE_UINT8   = 0,\n        GGUF_TYPE_INT8    = 1,\n        GGUF_TYPE_UINT16  = 2,\n        GGUF_TYPE_INT16   = 3,\n        GGUF_TYPE_UINT32  = 4,\n        GGUF_TYPE_INT32   = 5,\n        GGUF_TYPE_FLOAT32 = 6,\n        GGUF_TYPE_BOOL    = 7,\n        GGUF_TYPE_STRING  = 8,\n        GGUF_TYPE_ARRAY   = 9,\n        GGUF_TYPE_UINT64  = 10,\n        GGUF_TYPE_INT64   = 11,\n        GGUF_TYPE_FLOAT64 = 12,\n        GGUF_TYPE_COUNT,       // marks the end of the enum\n    };\n\n    struct gguf_context;\n\n    struct gguf_init_params {\n        bool no_alloc;\n\n        // if not NULL, create a ggml_context and allocate the tensor data in it\n        struct ggml_context ** ctx;\n    };\n\n    GGML_API struct gguf_context * gguf_init_empty(void);\n    GGML_API struct gguf_context * gguf_init_empty_sparse(void);\n    GGML_API struct gguf_context * gguf_init_from_file(const char * fname, struct gguf_init_params params);\n    //GGML_API struct gguf_context * gguf_init_from_buffer(..);\n\n    GGML_API void gguf_free(struct gguf_context * ctx);\n\n    GGML_API const char * gguf_type_name(enum gguf_type type);\n\n    GGML_API int    gguf_get_version    (const struct gguf_context * ctx);\n    GGML_API size_t gguf_get_alignment  (const struct gguf_context * ctx);\n    GGML_API size_t gguf_get_data_offset(const struct gguf_context * ctx);\n    GGML_API void * gguf_get_data       (const struct gguf_context * ctx);\n\n    GGML_API int          gguf_get_n_kv(const struct gguf_context * ctx);\n    GGML_API int          gguf_find_key(const struct gguf_context * ctx, const char * key);\n    GGML_API const char * gguf_get_key (const struct gguf_context * ctx, int key_id);\n\n    GGML_API enum gguf_type gguf_get_kv_type (const struct gguf_context * ctx, int key_id);\n    GGML_API enum gguf_type gguf_get_arr_type(const struct gguf_context * ctx, int key_id);\n\n    // will abort if the wrong type is used for the key\n    GGML_API uint8_t      gguf_get_val_u8  (const struct gguf_context * ctx, int key_id);\n    GGML_API int8_t       gguf_get_val_i8  (const struct gguf_context * ctx, int key_id);\n    GGML_API uint16_t     gguf_get_val_u16 (const struct gguf_context * ctx, int key_id);\n    GGML_API int16_t      gguf_get_val_i16 (const struct gguf_context * ctx, int key_id);\n    GGML_API uint32_t     gguf_get_val_u32 (const struct gguf_context * ctx, int key_id);\n    GGML_API int32_t      gguf_get_val_i32 (const struct gguf_context * ctx, int key_id);\n    GGML_API float        gguf_get_val_f32 (const struct gguf_context * ctx, int key_id);\n    GGML_API uint64_t     gguf_get_val_u64 (const struct gguf_context * ctx, int key_id);\n    GGML_API int64_t      gguf_get_val_i64 (const struct gguf_context * ctx, int key_id);\n    GGML_API double       gguf_get_val_f64 (const struct gguf_context * ctx, int key_id);\n    GGML_API bool         gguf_get_val_bool(const struct gguf_context * ctx, int key_id);\n    GGML_API const char * gguf_get_val_str (const struct gguf_context * ctx, int key_id);\n    GGML_API int          gguf_get_arr_n   (const struct gguf_context * ctx, int key_id);\n    GGML_API const void * gguf_get_arr_data(const struct gguf_context * ctx, int key_id);\n    GGML_API const char * gguf_get_arr_str (const struct gguf_context * ctx, int key_id, int i);\n\n    GGML_API enum ggml_sparse_deriv gguf_get_sparse_deriv(const struct gguf_context * ctx);\n    GGML_API int    gguf_get_n_tensors    (const struct gguf_context * ctx);\n    GGML_API int    gguf_find_tensor      (const struct gguf_context * ctx, const char * name);\n    GGML_API size_t gguf_get_tensor_offset(const struct gguf_context * ctx, int i);\n    GGML_API char * gguf_get_tensor_name  (const struct gguf_context * ctx, int i);\n\n    // overrides existing values or adds a new one\n    GGML_API void gguf_set_val_u8  (struct gguf_context * ctx, const char * key, uint8_t  val);\n    GGML_API void gguf_set_val_i8  (struct gguf_context * ctx, const char * key, int8_t   val);\n    GGML_API void gguf_set_val_u16 (struct gguf_context * ctx, const char * key, uint16_t val);\n    GGML_API void gguf_set_val_i16 (struct gguf_context * ctx, const char * key, int16_t  val);\n    GGML_API void gguf_set_val_u32 (struct gguf_context * ctx, const char * key, uint32_t val);\n    GGML_API void gguf_set_val_i32 (struct gguf_context * ctx, const char * key, int32_t  val);\n    GGML_API void gguf_set_val_f32 (struct gguf_context * ctx, const char * key, float    val);\n    GGML_API void gguf_set_val_u64 (struct gguf_context * ctx, const char * key, uint64_t val);\n    GGML_API void gguf_set_val_i64 (struct gguf_context * ctx, const char * key, int64_t  val);\n    GGML_API void gguf_set_val_f64 (struct gguf_context * ctx, const char * key, double   val);\n    GGML_API void gguf_set_val_bool(struct gguf_context * ctx, const char * key, bool     val);\n    GGML_API void gguf_set_val_str (struct gguf_context * ctx, const char * key, const char * val);\n    GGML_API void gguf_set_arr_data(struct gguf_context * ctx, const char * key, enum gguf_type type, const void * data, int n);\n    GGML_API void gguf_set_arr_str (struct gguf_context * ctx, const char * key, const char ** data, int n);\n\n    // set or add KV pairs from another context\n    GGML_API void gguf_set_kv(struct gguf_context * ctx, struct gguf_context * src);\n\n    // manage tensor info\n    GGML_API void gguf_add_tensor(struct gguf_context * ctx, const struct ggml_tensor * tensor);\n    GGML_API void gguf_set_tensor_type(struct gguf_context * ctx, const char * name, enum ggml_type type);\n    GGML_API void gguf_set_tensor_data(struct gguf_context * ctx, const char * name, const void * data, size_t size);\n\n    // writing gguf files can be done in 2 ways:\n    //\n    // - write the entire gguf_context to a binary file in a single pass:\n    //\n    //   gguf_write_to_file(ctx, fname);\n    //\n    // - first prepare a file with a placeholder for the meta data, write the tensor data, then write the meta data:\n    //\n    //   FILE * f = fopen(fname, \"wb\");\n    //   fseek(f, gguf_get_meta_size(ctx), SEEK_SET);\n    //   fwrite(f, ...);\n    //   void * data = gguf_meta_get_meta_data(ctx);\n    //   fseek(f, 0, SEEK_SET);\n    //   fwrite(f, data, gguf_get_meta_size(ctx));\n    //   free(data);\n    //   fclose(f);\n    //\n\n    // write the entire context to a binary file\n    GGML_API void gguf_write_to_file(const struct gguf_context * ctx, const char * fname, bool only_meta);\n\n    // get the size in bytes of the meta data (header, kv pairs, tensor info) including padding\n    GGML_API size_t gguf_get_meta_size(const struct gguf_context * ctx);\n    GGML_API void   gguf_get_meta_data(const struct gguf_context * ctx, void * data);\n\n    //\n    // system info\n    //\n\n    GGML_API int ggml_cpu_has_avx        (void);\n    GGML_API int ggml_cpu_has_avx2       (void);\n    GGML_API int ggml_cpu_has_avx512     (void);\n    GGML_API int ggml_cpu_has_avx512_vbmi(void);\n    GGML_API int ggml_cpu_has_avx512_vnni(void);\n    GGML_API int ggml_cpu_has_fma        (void);\n    GGML_API int ggml_cpu_has_neon       (void);\n    GGML_API int ggml_cpu_has_arm_fma    (void);\n    GGML_API int ggml_cpu_has_metal      (void);\n    GGML_API int ggml_cpu_has_f16c       (void);\n    GGML_API int ggml_cpu_has_fp16_va    (void);\n    GGML_API int ggml_cpu_has_wasm_simd  (void);\n    GGML_API int ggml_cpu_has_blas       (void);\n    GGML_API int ggml_cpu_has_cublas     (void);\n    GGML_API int ggml_cpu_has_clblast    (void);\n    GGML_API int ggml_cpu_has_gpublas    (void);\n    GGML_API int ggml_cpu_has_sse3       (void);\n    GGML_API int ggml_cpu_has_ssse3      (void);\n    GGML_API int ggml_cpu_has_vsx        (void);\n\n    //\n    // global variables\n    // \n    // TODO: these should be moved to the context\n    extern float sparse_pred_threshold;\n\n    //\n    // Internal types and functions exposed for tests and benchmarks\n    //\n\n#ifdef  __cplusplus\n// restrict not standard in C++\n#define GGML_RESTRICT\n#else\n#define GGML_RESTRICT restrict\n#endif\n    typedef void (*ggml_to_float_t)  (const void  * GGML_RESTRICT x, float * GGML_RESTRICT y, int k);\n    typedef void (*ggml_from_float_t)(const float * GGML_RESTRICT x, void  * GGML_RESTRICT y, int k);\n    typedef void (*ggml_vec_dot_t)   (const int n, float * GGML_RESTRICT s, const void * GGML_RESTRICT x, const void * GGML_RESTRICT y);\n\n    typedef struct {\n        const char      * type_name;\n        int               blck_size;\n        size_t            type_size;\n        bool              is_quantized;\n        ggml_to_float_t   to_float;\n        ggml_from_float_t from_float;\n        ggml_from_float_t from_float_reference;\n        ggml_vec_dot_t    vec_dot;\n        enum ggml_type    vec_dot_type;\n    } ggml_type_traits_t;\n\n    GGML_API ggml_type_traits_t ggml_internal_get_type_traits(enum ggml_type type);\n\n#ifdef  __cplusplus\n}\n#endif\n"
        },
        {
          "name": "gguf-py",
          "type": "tree",
          "content": null
        },
        {
          "name": "grammars",
          "type": "tree",
          "content": null
        },
        {
          "name": "llama.cpp",
          "type": "blob",
          "size": 398.2509765625,
          "content": "#define LLAMA_API_INTERNAL\n#include \"llama.h\"\n\n#include \"unicode.h\"\n\n#include \"ggml.h\"\n\n#include \"ggml-alloc.h\"\n\n#ifdef GGML_USE_CUBLAS\n#  include \"ggml-cuda.h\"\n#elif defined(GGML_USE_CLBLAST)\n#  include \"ggml-opencl.h\"\n#endif\n\n#ifdef GGML_USE_METAL\n#  include \"ggml-metal.h\"\n#endif\n#ifdef GGML_USE_MPI\n#  include \"ggml-mpi.h\"\n#endif\n#ifndef QK_K\n#  ifdef GGML_QKK_64\n#    define QK_K 64\n#  else\n#    define QK_K 256\n#  endif\n#endif\n\n#ifdef __has_include\n    #if __has_include(<unistd.h>)\n        #include <unistd.h>\n        #if defined(_POSIX_MAPPED_FILES)\n            #include <sys/mman.h>\n        #endif\n        #if defined(_POSIX_MEMLOCK_RANGE)\n            #include <sys/resource.h>\n        #endif\n    #endif\n#endif\n\n#if defined(_WIN32)\n    #define WIN32_LEAN_AND_MEAN\n    #ifndef NOMINMAX\n        #define NOMINMAX\n    #endif\n    #include <windows.h>\n    #include <io.h>\n    #include <stdio.h> // for _fseeki64\n    #include <direct.h>\n    #define F_OK 0\n#else\n    #include <libgen.h>\n#endif\n\n#include <algorithm>\n#include <array>\n#include <cassert>\n#include <cinttypes>\n#include <climits>\n#include <cmath>\n#include <cstdarg>\n#include <cstddef>\n#include <cstdint>\n#include <cstdio>\n#include <cstring>\n#include <ctime>\n#include <forward_list>\n#include <fstream>\n#include <functional>\n#include <initializer_list>\n#include <map>\n#include <memory>\n#include <mutex>\n#include <numeric>\n#include <queue>\n#include <random>\n#include <regex>\n#include <set>\n#include <sstream>\n#include <thread>\n#include <unordered_map>\n\n#if defined(_MSC_VER)\n#pragma warning(disable: 4244 4267) // possible loss of data\n#endif\n\n#ifdef __GNUC__\n#ifdef __MINGW32__\n#define LLAMA_ATTRIBUTE_FORMAT(...) __attribute__((format(gnu_printf, __VA_ARGS__)))\n#else\n#define LLAMA_ATTRIBUTE_FORMAT(...) __attribute__((format(printf, __VA_ARGS__)))\n#endif\n#else\n#define LLAMA_ATTRIBUTE_FORMAT(...)\n#endif\n\n#define LLAMA_MAX_NODES 4096\n\n// \n// global variables (should be removed after a better design)\n//\nsize_t vram_budget_bytes = 0;\n\n//\n// logging\n//\n\nLLAMA_ATTRIBUTE_FORMAT(2, 3)\nstatic void llama_log_internal        (ggml_log_level level, const char* format, ...);\nstatic void llama_log_callback_default(ggml_log_level level, const char * text, void * user_data);\n\n#define LLAMA_LOG_INFO(...)  llama_log_internal(GGML_LOG_LEVEL_INFO , __VA_ARGS__)\n#define LLAMA_LOG_WARN(...)  llama_log_internal(GGML_LOG_LEVEL_WARN , __VA_ARGS__)\n#define LLAMA_LOG_ERROR(...) llama_log_internal(GGML_LOG_LEVEL_ERROR, __VA_ARGS__)\n\n//\n// helpers\n//\n\nstatic size_t utf8_len(char src) {\n    const size_t lookup[] = { 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 4 };\n    uint8_t highbits = static_cast<uint8_t>(src) >> 4;\n    return lookup[highbits];\n}\n\nstatic void replace_all(std::string & s, const std::string & search, const std::string & replace) {\n    std::string result;\n    for (size_t pos = 0; ; pos += search.length()) {\n        auto new_pos = s.find(search, pos);\n        if (new_pos == std::string::npos) {\n            result += s.substr(pos, s.size() - pos);\n            break;\n        }\n        result += s.substr(pos, new_pos - pos) + replace;\n        pos = new_pos;\n    }\n    s = std::move(result);\n}\n\nstatic bool is_float_close(float a, float b, float abs_tol) {\n    // Check for non-negative tolerance\n    if (abs_tol < 0.0) {\n        throw std::invalid_argument(\"Tolerance must be non-negative\");\n    }\n\n    // Exact equality check\n    if (a == b) {\n        return true;\n    }\n\n    // Check for infinities\n    if (std::isinf(a) || std::isinf(b)) {\n        return false;\n    }\n\n    // Regular comparison using the provided absolute tolerance\n    return std::fabs(b - a) <= abs_tol;\n}\n\n#ifdef GGML_USE_CPU_HBM\n#include <hbwmalloc.h>\n#endif\n\nstatic void zeros(std::ofstream & file, size_t n) {\n    char zero = 0;\n    for (size_t i = 0; i < n; ++i) {\n        file.write(&zero, 1);\n    }\n}\n\nLLAMA_ATTRIBUTE_FORMAT(1, 2)\nstatic std::string format(const char * fmt, ...) {\n    va_list ap;\n    va_list ap2;\n    va_start(ap, fmt);\n    va_copy(ap2, ap);\n    int size = vsnprintf(NULL, 0, fmt, ap);\n    GGML_ASSERT(size >= 0 && size < INT_MAX); // NOLINT\n    std::vector<char> buf(size + 1);\n    int size2 = vsnprintf(buf.data(), size + 1, fmt, ap2);\n    GGML_ASSERT(size2 == size);\n    va_end(ap2);\n    va_end(ap);\n    return std::string(buf.data(), size);\n}\n\nstatic size_t llama_set_vram_budget(double budget_gb, int gpu_device) {\n#if defined(GGML_USE_CUBLAS)\n    if (!ggml_cublas_loaded()) {\n        throw std::runtime_error(\"CUDA is not loaded\");\n    }\n\n    if (budget_gb < 0) {\n        // if the user didn't specify a budget, use all available memory\n        // and leave 256 MB as a safety margin\n        vram_budget_bytes = ggml_cuda_get_free_memory(gpu_device) - 256 * 1024 * 1024;\n    } else {\n        // otherwise, use the specified budget\n        vram_budget_bytes = (size_t) (budget_gb * 1024 * 1024 * 1024);\n    }\n\n    return vram_budget_bytes;\n#else\n    return 0;\n#endif\n}\n\nstatic bool llama_reduce_vram_budget(size_t budget_bytes) {\n#if not defined(GGML_USE_CUBLAS)\n    throw std::runtime_error(\"CUDA is not enabled\");\n#endif\n\n    if (vram_budget_bytes >= budget_bytes) {\n        vram_budget_bytes -= budget_bytes;\n        return true;\n    }\n    \n    return false;\n}\n\n//\n// gguf constants (sync with gguf.py)\n//\n\nenum llm_arch {\n    LLM_ARCH_LLAMA,\n    LLM_ARCH_FALCON,\n    LLM_ARCH_BAICHUAN,\n    LLM_ARCH_GPT2,\n    LLM_ARCH_GPTJ,\n    LLM_ARCH_GPTNEOX,\n    LLM_ARCH_MPT,\n    LLM_ARCH_STARCODER,\n    LLM_ARCH_PERSIMMON,\n    LLM_ARCH_REFACT,\n    LLM_ARCH_BLOOM,\n    LLM_ARCH_STABLELM,\n    LLM_ARCH_BAMBOO,\n    LLM_ARCH_UNKNOWN,\n};\n\nstatic std::map<llm_arch, std::string> LLM_ARCH_NAMES = {\n    { LLM_ARCH_LLAMA,           \"llama\"     },\n    { LLM_ARCH_FALCON,          \"falcon\"    },\n    { LLM_ARCH_GPT2,            \"gpt2\"      },\n    { LLM_ARCH_GPTJ,            \"gptj\"      },\n    { LLM_ARCH_GPTNEOX,         \"gptneox\"   },\n    { LLM_ARCH_MPT,             \"mpt\"       },\n    { LLM_ARCH_BAICHUAN,        \"baichuan\"  },\n    { LLM_ARCH_STARCODER,       \"starcoder\" },\n    { LLM_ARCH_PERSIMMON,       \"persimmon\" },\n    { LLM_ARCH_REFACT,          \"refact\"    },\n    { LLM_ARCH_BLOOM,           \"bloom\"     },\n    { LLM_ARCH_STABLELM,        \"stablelm\"  },\n    { LLM_ARCH_BAMBOO,          \"bamboo\"    },\n\n    { LLM_ARCH_UNKNOWN,         \"unknown\"   },\n};\n\nenum llm_kv {\n    LLM_KV_GENERAL_ARCHITECTURE,\n    LLM_KV_GENERAL_QUANTIZATION_VERSION,\n    LLM_KV_GENERAL_ALIGNMENT,\n    LLM_KV_GENERAL_NAME,\n    LLM_KV_GENERAL_AUTHOR,\n    LLM_KV_GENERAL_URL,\n    LLM_KV_GENERAL_DESCRIPTION,\n    LLM_KV_GENERAL_LICENSE,\n    LLM_KV_GENERAL_SOURCE_URL,\n    LLM_KV_GENERAL_SOURCE_HF_REPO,\n\n    LLM_KV_CONTEXT_LENGTH,\n    LLM_KV_EMBEDDING_LENGTH,\n    LLM_KV_BLOCK_COUNT,\n    LLM_KV_FEED_FORWARD_LENGTH,\n    LLM_KV_USE_PARALLEL_RESIDUAL,\n    LLM_KV_TENSOR_DATA_LAYOUT,\n\n    LLM_KV_ATTENTION_HEAD_COUNT,\n    LLM_KV_ATTENTION_HEAD_COUNT_KV,\n    LLM_KV_ATTENTION_MAX_ALIBI_BIAS,\n    LLM_KV_ATTENTION_CLAMP_KQV,\n    LLM_KV_ATTENTION_LAYERNORM_EPS,\n    LLM_KV_ATTENTION_LAYERNORM_RMS_EPS,\n\n    LLM_KV_ROPE_DIMENSION_COUNT,\n    LLM_KV_ROPE_FREQ_BASE,\n    LLM_KV_ROPE_SCALE_LINEAR,\n    LLM_KV_ROPE_SCALING_TYPE,\n    LLM_KV_ROPE_SCALING_FACTOR,\n    LLM_KV_ROPE_SCALING_ORIG_CTX_LEN,\n    LLM_KV_ROPE_SCALING_FINETUNED,\n\n    LLM_KV_TOKENIZER_MODEL,\n    LLM_KV_TOKENIZER_LIST,\n    LLM_KV_TOKENIZER_TOKEN_TYPE,\n    LLM_KV_TOKENIZER_SCORES,\n    LLM_KV_TOKENIZER_MERGES,\n    LLM_KV_TOKENIZER_BOS_ID,\n    LLM_KV_TOKENIZER_EOS_ID,\n    LLM_KV_TOKENIZER_UNK_ID,\n    LLM_KV_TOKENIZER_SEP_ID,\n    LLM_KV_TOKENIZER_PAD_ID,\n    LLM_KV_TOKENIZER_HF_JSON,\n    LLM_KV_TOKENIZER_RWKV,\n\n    LLM_KV_SPARSE_THRESHOLD,\n\n    LLM_KV_SPLIT_VRAM_CAPACITY,\n};\n\nstatic std::map<llm_kv, std::string> LLM_KV_NAMES = {\n    { LLM_KV_GENERAL_ARCHITECTURE,          \"general.architecture\"                  },\n    { LLM_KV_GENERAL_QUANTIZATION_VERSION,  \"general.quantization_version\"          },\n    { LLM_KV_GENERAL_ALIGNMENT,             \"general.alignment\"                     },\n    { LLM_KV_GENERAL_NAME,                  \"general.name\"                          },\n    { LLM_KV_GENERAL_AUTHOR,                \"general.author\"                        },\n    { LLM_KV_GENERAL_URL,                   \"general.url\"                           },\n    { LLM_KV_GENERAL_DESCRIPTION,           \"general.description\"                   },\n    { LLM_KV_GENERAL_LICENSE,               \"general.license\"                       },\n    { LLM_KV_GENERAL_SOURCE_URL,            \"general.source.url\"                    },\n    { LLM_KV_GENERAL_SOURCE_HF_REPO,        \"general.source.huggingface.repository\" },\n\n    { LLM_KV_CONTEXT_LENGTH,                \"%s.context_length\"        },\n    { LLM_KV_EMBEDDING_LENGTH,              \"%s.embedding_length\"      },\n    { LLM_KV_BLOCK_COUNT,                   \"%s.block_count\"           },\n    { LLM_KV_FEED_FORWARD_LENGTH,           \"%s.feed_forward_length\"   },\n    { LLM_KV_USE_PARALLEL_RESIDUAL,         \"%s.use_parallel_residual\" },\n    { LLM_KV_TENSOR_DATA_LAYOUT,            \"%s.tensor_data_layout\"    },\n\n    { LLM_KV_ATTENTION_HEAD_COUNT,          \"%s.attention.head_count\"             },\n    { LLM_KV_ATTENTION_HEAD_COUNT_KV,       \"%s.attention.head_count_kv\"          },\n    { LLM_KV_ATTENTION_MAX_ALIBI_BIAS,      \"%s.attention.max_alibi_bias\"         },\n    { LLM_KV_ATTENTION_CLAMP_KQV,           \"%s.attention.clamp_kqv\"              },\n    { LLM_KV_ATTENTION_LAYERNORM_EPS,       \"%s.attention.layer_norm_epsilon\"     },\n    { LLM_KV_ATTENTION_LAYERNORM_RMS_EPS,   \"%s.attention.layer_norm_rms_epsilon\" },\n\n    { LLM_KV_ROPE_DIMENSION_COUNT,          \"%s.rope.dimension_count\"                 },\n    { LLM_KV_ROPE_FREQ_BASE,                \"%s.rope.freq_base\"                       },\n    { LLM_KV_ROPE_SCALE_LINEAR,             \"%s.rope.scale_linear\"                    },\n    { LLM_KV_ROPE_SCALING_TYPE,             \"%s.rope.scaling.type\"                    },\n    { LLM_KV_ROPE_SCALING_FACTOR,           \"%s.rope.scaling.factor\"                  },\n    { LLM_KV_ROPE_SCALING_ORIG_CTX_LEN,     \"%s.rope.scaling.original_context_length\" },\n    { LLM_KV_ROPE_SCALING_FINETUNED,        \"%s.rope.scaling.finetuned\"               },\n\n    { LLM_KV_TOKENIZER_MODEL,               \"tokenizer.ggml.model\"              },\n    { LLM_KV_TOKENIZER_LIST,                \"tokenizer.ggml.tokens\"             },\n    { LLM_KV_TOKENIZER_TOKEN_TYPE,          \"tokenizer.ggml.token_type\"         },\n    { LLM_KV_TOKENIZER_SCORES,              \"tokenizer.ggml.scores\"             },\n    { LLM_KV_TOKENIZER_MERGES,              \"tokenizer.ggml.merges\"             },\n    { LLM_KV_TOKENIZER_BOS_ID,              \"tokenizer.ggml.bos_token_id\"       },\n    { LLM_KV_TOKENIZER_EOS_ID,              \"tokenizer.ggml.eos_token_id\"       },\n    { LLM_KV_TOKENIZER_UNK_ID,              \"tokenizer.ggml.unknown_token_id\"   },\n    { LLM_KV_TOKENIZER_SEP_ID,              \"tokenizer.ggml.seperator_token_id\" },\n    { LLM_KV_TOKENIZER_PAD_ID,              \"tokenizer.ggml.padding_token_id\"   },\n    { LLM_KV_TOKENIZER_HF_JSON,             \"tokenizer.huggingface.json\"        },\n    { LLM_KV_TOKENIZER_RWKV,                \"tokenizer.rwkv.world\"              },\n\n    { LLM_KV_SPARSE_THRESHOLD,              \"powerinfer.sparse_threshold\" },\n\n    { LLM_KV_SPLIT_VRAM_CAPACITY,           \"split.vram_capacity\" },\n};\n\nstruct LLM_KV {\n    LLM_KV(llm_arch arch) : arch(arch) {}\n\n    llm_arch arch;\n\n    std::string operator()(llm_kv kv) const {\n        return ::format(LLM_KV_NAMES[kv].c_str(), LLM_ARCH_NAMES[arch].c_str());\n    }\n};\n\nenum llm_tensor {\n    LLM_TENSOR_TOKEN_EMBD,\n    LLM_TENSOR_TOKEN_EMBD_NORM,\n    LLM_TENSOR_POS_EMBD,\n    LLM_TENSOR_OUTPUT,\n    LLM_TENSOR_OUTPUT_NORM,\n    LLM_TENSOR_ROPE_FREQS,\n    LLM_TENSOR_ATTN_Q,\n    LLM_TENSOR_ATTN_K,\n    LLM_TENSOR_ATTN_V,\n    LLM_TENSOR_ATTN_QKV,\n    LLM_TENSOR_ATTN_OUT,\n    LLM_TENSOR_ATTN_NORM,\n    LLM_TENSOR_ATTN_NORM_2,\n    LLM_TENSOR_ATTN_ROT_EMBD,\n    LLM_TENSOR_FFN_GATE,\n    LLM_TENSOR_FFN_DOWN,\n    LLM_TENSOR_FFN_UP,\n    LLM_TENSOR_FFN_NORM,\n    LLM_TENSOR_ATTN_Q_NORM,\n    LLM_TENSOR_ATTN_K_NORM,\n    LLM_TENSOR_MLP_PRED_FC1,\n    LLM_TENSOR_MLP_PRED_FC2,\n    LLM_TENSOR_FFN_DOWN_T,\n};\n\nstatic std::map<llm_arch, std::map<llm_tensor, std::string>> LLM_TENSOR_NAMES = {\n    {\n        LLM_ARCH_LLAMA,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ROPE_FREQS,      \"rope_freqs\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_Q,          \"blk.%d.attn_q\" },\n            { LLM_TENSOR_ATTN_K,          \"blk.%d.attn_k\" },\n            { LLM_TENSOR_ATTN_V,          \"blk.%d.attn_v\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_ATTN_ROT_EMBD,   \"blk.%d.attn_rot_embd\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_FFN_GATE,        \"blk.%d.ffn_gate\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n            { LLM_TENSOR_FFN_DOWN_T,      \"blk.%d.ffn_down_t\" },\n            { LLM_TENSOR_MLP_PRED_FC1,    \"blk.%d.fc1\" },\n            { LLM_TENSOR_MLP_PRED_FC2,    \"blk.%d.fc2\" },\n        },\n    },\n    {\n        LLM_ARCH_BAICHUAN,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ROPE_FREQS,      \"rope_freqs\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_Q,          \"blk.%d.attn_q\" },\n            { LLM_TENSOR_ATTN_K,          \"blk.%d.attn_k\" },\n            { LLM_TENSOR_ATTN_V,          \"blk.%d.attn_v\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_ATTN_ROT_EMBD,   \"blk.%d.attn_rot_embd\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_FFN_GATE,        \"blk.%d.ffn_gate\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n        },\n    },\n    {\n        LLM_ARCH_FALCON,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_NORM_2,     \"blk.%d.attn_norm_2\" },\n            { LLM_TENSOR_ATTN_QKV,        \"blk.%d.attn_qkv\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n            { LLM_TENSOR_FFN_DOWN_T,      \"blk.%d.ffn_down_t\" },\n            { LLM_TENSOR_MLP_PRED_FC1,    \"blk.%d.fc1\" },\n            { LLM_TENSOR_MLP_PRED_FC2,    \"blk.%d.fc2\" },\n        },\n    },\n    {\n        LLM_ARCH_GPT2,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n        },\n    },\n    {\n        LLM_ARCH_GPTJ,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n        },\n    },\n    {\n        LLM_ARCH_GPTNEOX,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_QKV,        \"blk.%d.attn_qkv\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n        },\n    },\n    {\n        LLM_ARCH_PERSIMMON,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\"},\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\"},\n            { LLM_TENSOR_OUTPUT,          \"output\"},\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\"},\n            { LLM_TENSOR_ATTN_QKV,        \"blk.%d.attn_qkv\"},\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\"},\n            { LLM_TENSOR_ATTN_Q_NORM,     \"blk.%d.attn_q_norm\"},\n            { LLM_TENSOR_ATTN_K_NORM,     \"blk.%d.attn_k_norm\"},\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\"},\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\"},\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\"},\n            { LLM_TENSOR_ATTN_ROT_EMBD,   \"blk.%d.attn_rot_embd\"},\n        },\n    },\n    {\n        LLM_ARCH_MPT,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_ATTN_QKV,        \"blk.%d.attn_qkv\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n        },\n    },\n    {\n        LLM_ARCH_STARCODER,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_POS_EMBD,        \"position_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_QKV,        \"blk.%d.attn_qkv\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n        },\n    },\n    {\n        LLM_ARCH_REFACT,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_Q,          \"blk.%d.attn_q\" },\n            { LLM_TENSOR_ATTN_K,          \"blk.%d.attn_k\" },\n            { LLM_TENSOR_ATTN_V,          \"blk.%d.attn_v\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_FFN_GATE,        \"blk.%d.ffn_gate\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n        },\n    },\n    {\n        LLM_ARCH_BLOOM,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_TOKEN_EMBD_NORM, \"token_embd_norm\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_QKV,        \"blk.%d.attn_qkv\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n        },\n    },\n    {\n        LLM_ARCH_STABLELM,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ROPE_FREQS,      \"rope_freqs\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_Q,          \"blk.%d.attn_q\" },\n            { LLM_TENSOR_ATTN_K,          \"blk.%d.attn_k\" },\n            { LLM_TENSOR_ATTN_V,          \"blk.%d.attn_v\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_FFN_GATE,        \"blk.%d.ffn_gate\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n        },\n    },\n    {\n        LLM_ARCH_BAMBOO,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n            { LLM_TENSOR_OUTPUT_NORM,     \"output_norm\" },\n            { LLM_TENSOR_OUTPUT,          \"output\" },\n            { LLM_TENSOR_ROPE_FREQS,      \"rope_freqs\" },\n            { LLM_TENSOR_ATTN_NORM,       \"blk.%d.attn_norm\" },\n            { LLM_TENSOR_ATTN_Q,          \"blk.%d.attn_q\" },\n            { LLM_TENSOR_ATTN_K,          \"blk.%d.attn_k\" },\n            { LLM_TENSOR_ATTN_V,          \"blk.%d.attn_v\" },\n            { LLM_TENSOR_ATTN_OUT,        \"blk.%d.attn_output\" },\n            { LLM_TENSOR_ATTN_ROT_EMBD,   \"blk.%d.attn_rot_embd\" },\n            { LLM_TENSOR_FFN_NORM,        \"blk.%d.ffn_norm\" },\n            { LLM_TENSOR_FFN_GATE,        \"blk.%d.ffn_gate\" },\n            { LLM_TENSOR_FFN_DOWN,        \"blk.%d.ffn_down\" },\n            { LLM_TENSOR_FFN_UP,          \"blk.%d.ffn_up\" },\n            { LLM_TENSOR_FFN_DOWN_T,      \"blk.%d.ffn_down_t\" },\n            { LLM_TENSOR_MLP_PRED_FC1,    \"blk.%d.fc1\" },\n            { LLM_TENSOR_MLP_PRED_FC2,    \"blk.%d.fc2\" },\n        },\n    },\n    {\n        LLM_ARCH_UNKNOWN,\n        {\n            { LLM_TENSOR_TOKEN_EMBD,      \"token_embd\" },\n        },\n    },\n};\n\nstatic llm_arch llm_arch_from_string(const std::string & name) {\n    for (const auto & kv : LLM_ARCH_NAMES) { // NOLINT\n        if (kv.second == name) {\n            return kv.first;\n        }\n    }\n\n    return LLM_ARCH_UNKNOWN;\n}\n\nenum tensor_offloading_levels {\n    TENSOR_NO_OFFLOAD,\n    TENSOR_OFFLOAD_FFN,\n    TENSOR_OFFLOAD_ATTN,\n    TENSOR_OFFLOAD_MLP_PRED,\n    TENSOR_OFFLOAD_FFN_IO,\n    TENSOR_OFFLOAD_OUTPUT,\n    TENSOR_OFFLOAD_KV_CACHE,\n};\n\ntensor_offloading_levels get_offloading_level(llm_tensor tensor) {\n    switch (tensor) {\n        case LLM_TENSOR_TOKEN_EMBD: case LLM_TENSOR_TOKEN_EMBD_NORM: case LLM_TENSOR_POS_EMBD: \n        case LLM_TENSOR_ROPE_FREQS:\n            return TENSOR_NO_OFFLOAD;\n        case LLM_TENSOR_OUTPUT: case LLM_TENSOR_OUTPUT_NORM:\n            return TENSOR_OFFLOAD_OUTPUT;\n        case LLM_TENSOR_ATTN_Q: case LLM_TENSOR_ATTN_K: case LLM_TENSOR_ATTN_V: \n        case LLM_TENSOR_ATTN_QKV: case LLM_TENSOR_ATTN_OUT: case LLM_TENSOR_ATTN_NORM: \n        case LLM_TENSOR_ATTN_NORM_2: case LLM_TENSOR_ATTN_ROT_EMBD:\n        case LLM_TENSOR_ATTN_Q_NORM: case LLM_TENSOR_ATTN_K_NORM:\n            return TENSOR_OFFLOAD_ATTN;\n        case LLM_TENSOR_FFN_GATE: case LLM_TENSOR_FFN_DOWN: case LLM_TENSOR_FFN_UP:\n        case LLM_TENSOR_FFN_DOWN_T:\n            return TENSOR_OFFLOAD_FFN;\n        case LLM_TENSOR_FFN_NORM:\n            return TENSOR_OFFLOAD_FFN_IO;\n        case LLM_TENSOR_MLP_PRED_FC1: case LLM_TENSOR_MLP_PRED_FC2:\n            return TENSOR_OFFLOAD_MLP_PRED;\n        default:\n            throw std::runtime_error(\"unknown tensor category\");\n    }\n}\n\n\n// helper to handle gguf constants\n// usage:\n//\n//   const auto tn = LLM_TN(LLM_ARCH_LLAMA);\n//\n//   std::string name = tn(LLM_TENSOR_OUTPUT);                     -> \"output\"\n//   std::string name = tn(LLM_TENSOR_TOKEN_EMBD, \"bias\");         -> \"token_embd.bias\"\n//   std::string name = tn(LLM_TENSOR_ATTN_NORM, \"weight\", 3);     -> \"blk.3.attn_norm.weight\"\n//\nstruct LLM_TN {\n    LLM_TN(llm_arch arch) : arch(arch) {}\n\n    llm_arch arch;\n\n    std::pair<std::string, llm_tensor> operator()(llm_tensor tensor) const {\n        return std::make_pair(LLM_TENSOR_NAMES[arch].at(tensor), tensor);\n    }\n\n    std::pair<std::string, llm_tensor> operator()(llm_tensor tensor, const std::string & suffix) const {\n        return std::make_pair(LLM_TENSOR_NAMES[arch].at(tensor) + \".\" + suffix, tensor);\n    }\n\n    std::pair<std::string, llm_tensor> operator()(llm_tensor tensor, int bid) const {\n        return std::make_pair(::format(LLM_TENSOR_NAMES[arch].at(tensor).c_str(), bid), tensor);\n    }\n\n    std::pair<std::string, llm_tensor> operator()(llm_tensor tensor, const std::string & suffix, int bid) const {\n        return std::make_pair(::format(LLM_TENSOR_NAMES[arch].at(tensor).c_str(), bid) + \".\" + suffix, tensor);\n    }\n};\n\n//\n// gguf helpers\n//\n\n#define GGUF_GET_KEY(ctx, dst, func, type, req, key) \\\ndo { \\\n    const std::string skey(key); \\\n    const int kid = gguf_find_key(ctx, skey.c_str()); \\\n    if (kid >= 0) { \\\n        enum gguf_type ktype = gguf_get_kv_type(ctx, kid); \\\n        if (ktype != (type)) { \\\n            throw std::runtime_error(format(\"key %s has wrong type: %s\", skey.c_str(), gguf_type_name(ktype))); \\\n        } \\\n        (dst) = func(ctx, kid); \\\n    } else if (req) { \\\n        throw std::runtime_error(format(\"key not found in model: %s\", skey.c_str())); \\\n    } \\\n} while (0)\n\nstatic std::map<int8_t, std::string> LLAMA_ROPE_SCALING_TYPES = {\n    { LLAMA_ROPE_SCALING_NONE,   \"none\"   },\n    { LLAMA_ROPE_SCALING_LINEAR, \"linear\" },\n    { LLAMA_ROPE_SCALING_YARN,   \"yarn\"   },\n};\n\nstatic int8_t llama_rope_scaling_type_from_string(const std::string & name) {\n    for (const auto & kv : LLAMA_ROPE_SCALING_TYPES) {\n        if (kv.second == name) {\n            return kv.first;\n        }\n    }\n\n    return LLAMA_ROPE_SCALING_UNSPECIFIED;\n}\n\n//\n// ggml helpers\n//\n\nstatic void ggml_graph_compute_helper(std::vector<uint8_t> & buf, ggml_cgraph * graph, int n_threads) {\n    struct ggml_cplan plan = ggml_graph_plan(graph, n_threads);\n\n    if (plan.work_size > 0) {\n        buf.resize(plan.work_size);\n        plan.work_data = buf.data();\n    }\n\n    ggml_graph_compute(graph, &plan);\n}\n\n//\n// llama helpers\n//\n\ninline void * llama_host_malloc(size_t n) {\n#ifdef GGML_USE_CUBLAS\n    if (ggml_cublas_loaded()) {\n        return ggml_cuda_host_malloc(n);\n    } else {\n        return malloc(n);\n    }\n#elif GGML_USE_METAL\n    return ggml_metal_host_malloc(n);\n#elif GGML_USE_CPU_HBM\n    return hbw_malloc(n);\n#else\n    return malloc(n);\n#endif\n}\n\ninline void llama_host_free(void * ptr) {\n#ifdef GGML_USE_CUBLAS\n    if (ggml_cublas_loaded()) {\n        return ggml_cuda_host_free(ptr);\n    } else {\n        return free(ptr);\n    }\n#elif GGML_USE_METAL\n    return ggml_metal_host_free(ptr);\n#elif GGML_USE_CPU_HBM\n    return hbw_free(ptr);\n#else\n    return free(ptr);\n#endif\n}\n\n#if defined(_WIN32)\nstatic std::string llama_format_win_err(DWORD err) {\n    LPSTR buf;\n    size_t size = FormatMessageA(FORMAT_MESSAGE_ALLOCATE_BUFFER | FORMAT_MESSAGE_FROM_SYSTEM | FORMAT_MESSAGE_IGNORE_INSERTS,\n                                 NULL, err, MAKELANGID(LANG_NEUTRAL, SUBLANG_DEFAULT), (LPSTR)&buf, 0, NULL);\n    if (!size) {\n        return \"FormatMessageA failed\";\n    }\n    std::string ret(buf, size);\n    LocalFree(buf);\n    return ret;\n}\n#endif\n\nstruct llama_buffer {\n    void * data = NULL;\n    size_t size = 0;\n\n    // fallback to malloc / free\n    // useful in cases where CUDA can try to allocate PINNED memory\n    bool fallback = false;\n\n    void resize(size_t n) {\n        llama_host_free(data);\n\n        data = llama_host_malloc(n);\n        if (!data) {\n            fallback = true;\n            data = malloc(n);\n        } else {\n            fallback = false;\n        }\n\n        GGML_ASSERT(data);\n        size = n;\n    }\n\n    ~llama_buffer() {\n        if (data) {\n            if (fallback) { // NOLINT\n                free(data);\n            } else {\n                llama_host_free(data);\n            }\n        }\n\n        data = NULL;\n    }\n};\n\nstruct llama_file {\n    // use FILE * so we don't have to re-open the file to mmap\n    FILE * fp;\n    std::string fname;\n    size_t size;\n\n    llama_file(const char * fname, const char * mode): fname(fname) {\n        fp = std::fopen(fname, mode);\n        if (fp == NULL) {\n            throw std::runtime_error(format(\"failed to open %s: %s\", fname, strerror(errno)));\n        }\n        seek(0, SEEK_END);\n        size = tell();\n        seek(0, SEEK_SET);\n    }\n\n    std::string get_basedir() const {\n        const char * model_path = fname.c_str();\n#if defined(_WIN32)\n        size_t found = fname.find_last_of(\"/\\\\\");\n        return fname.substr(0, found);\n#else\n        #include <libgen.h>\n        const char * base_dir = dirname(const_cast<char *>(model_path));\n        return std::string(base_dir);\n#endif\n    }\n\n    size_t tell() const {\n#ifdef _WIN32\n        __int64 ret = _ftelli64(fp);\n#else\n        long ret = std::ftell(fp);\n#endif\n        GGML_ASSERT(ret != -1); // this really shouldn't fail\n        return (size_t) ret;\n    }\n\n    void seek(size_t offset, int whence) const {\n#ifdef _WIN32\n        int ret = _fseeki64(fp, (__int64) offset, whence);\n#else\n        int ret = std::fseek(fp, (long) offset, whence);\n#endif\n        GGML_ASSERT(ret == 0); // same\n    }\n\n    void read_raw(void * ptr, size_t len) const {\n        if (len == 0) {\n            return;\n        }\n        errno = 0;\n        std::size_t ret = std::fread(ptr, len, 1, fp);\n        if (ferror(fp)) {\n            throw std::runtime_error(format(\"read error: %s\", strerror(errno)));\n        }\n        if (ret != 1) {\n            throw std::runtime_error(std::string(\"unexpectedly reached end of file\"));\n        }\n    }\n\n    uint32_t read_u32() const {\n        uint32_t ret;\n        read_raw(&ret, sizeof(ret));\n        return ret;\n    }\n\n    void write_raw(const void * ptr, size_t len) const {\n        if (len == 0) {\n            return;\n        }\n        errno = 0;\n        size_t ret = std::fwrite(ptr, len, 1, fp);\n        if (ret != 1) {\n            throw std::runtime_error(format(\"write error: %s\", strerror(errno)));\n        }\n    }\n\n    void write_u32(std::uint32_t val) const {\n        write_raw(&val, sizeof(val));\n    }\n\n    ~llama_file() {\n        if (fp) {\n            std::fclose(fp);\n        }\n    }\n};\n\nstruct llama_mmap {\n    void * addr;\n    size_t size;\n\n    llama_mmap(const llama_mmap &) = delete;\n\n#ifdef _POSIX_MAPPED_FILES\n    static constexpr bool SUPPORTED = true;\n\n    llama_mmap(struct llama_file * file, size_t prefetch = (size_t) -1 /* -1 = max value */, bool numa = false) {\n        size = file->size;\n        int fd = fileno(file->fp);\n        int flags = MAP_SHARED;\n        // prefetch/readahead impairs performance on NUMA systems\n        if (numa) { prefetch = 0; }\n#ifdef __linux__\n        if (prefetch) { flags |= MAP_POPULATE; }\n#endif\n        addr = mmap(NULL, file->size, PROT_READ, flags, fd, 0);\n        if (addr == MAP_FAILED) {\n            throw std::runtime_error(format(\"mmap failed: %s\", strerror(errno)));\n        }\n\n        if (prefetch > 0) {\n            // Advise the kernel to preload the mapped memory\n            if (posix_madvise(addr, std::min(file->size, prefetch), POSIX_MADV_WILLNEED)) {\n                fprintf(stderr, \"warning: posix_madvise(.., POSIX_MADV_WILLNEED) failed: %s\\n\",\n                        strerror(errno));\n            }\n        }\n        if (numa) {\n            // advise the kernel not to use readahead\n            // (because the next page might not belong on the same node)\n            if (posix_madvise(addr, file->size, POSIX_MADV_RANDOM)) {\n                fprintf(stderr, \"warning: posix_madvise(.., POSIX_MADV_RANDOM) failed: %s\\n\",\n                        strerror(errno));\n            }\n        }\n    }\n\n    ~llama_mmap() {\n        munmap(addr, size);\n    }\n#elif defined(_WIN32)\n    static constexpr bool SUPPORTED = true;\n\n    llama_mmap(struct llama_file * file, bool prefetch = true, bool numa = false) {\n        (void) numa;\n\n        size = file->size;\n\n        HANDLE hFile = (HANDLE) _get_osfhandle(_fileno(file->fp));\n\n        HANDLE hMapping = CreateFileMappingA(hFile, NULL, PAGE_READONLY, 0, 0, NULL);\n        DWORD error = GetLastError();\n\n        if (hMapping == NULL) {\n            throw std::runtime_error(format(\"CreateFileMappingA failed: %s\", llama_format_win_err(error).c_str()));\n        }\n\n        addr = MapViewOfFile(hMapping, FILE_MAP_READ, 0, 0, 0);\n        error = GetLastError();\n        CloseHandle(hMapping);\n\n        if (addr == NULL) {\n            throw std::runtime_error(format(\"MapViewOfFile failed: %s\", llama_format_win_err(error).c_str()));\n        }\n\n        if (prefetch) {\n            // PrefetchVirtualMemory is only present on Windows 8 and above, so we dynamically load it\n            BOOL (WINAPI *pPrefetchVirtualMemory) (HANDLE, ULONG_PTR, PWIN32_MEMORY_RANGE_ENTRY, ULONG);\n            HMODULE hKernel32 = GetModuleHandleW(L\"kernel32.dll\");\n\n            // may fail on pre-Windows 8 systems\n            pPrefetchVirtualMemory = reinterpret_cast<decltype(pPrefetchVirtualMemory)> (GetProcAddress(hKernel32, \"PrefetchVirtualMemory\"));\n\n            if (pPrefetchVirtualMemory) {\n                // advise the kernel to preload the mapped memory\n                WIN32_MEMORY_RANGE_ENTRY range;\n                range.VirtualAddress = addr;\n                range.NumberOfBytes = (SIZE_T)size;\n                if (!pPrefetchVirtualMemory(GetCurrentProcess(), 1, &range, 0)) {\n                    fprintf(stderr, \"warning: PrefetchVirtualMemory failed: %s\\n\",\n                            llama_format_win_err(GetLastError()).c_str());\n                }\n            }\n        }\n    }\n\n    ~llama_mmap() {\n        if (!UnmapViewOfFile(addr)) {\n            fprintf(stderr, \"warning: UnmapViewOfFile failed: %s\\n\",\n                    llama_format_win_err(GetLastError()).c_str());\n        }\n    }\n#else\n    static constexpr bool SUPPORTED = false;\n\n    llama_mmap(struct llama_file * file, bool prefetch = true, bool numa = false) {\n        (void) file;\n        (void) prefetch;\n        (void) numa;\n\n        throw std::runtime_error(std::string(\"mmap not supported\"));\n    }\n#endif\n};\n\n// Represents some region of memory being locked using mlock or VirtualLock;\n// will automatically unlock on destruction.\nstruct llama_mlock {\n    void * addr = NULL;\n    size_t size = 0;\n\n    bool failed_already = false;\n\n    llama_mlock() {}\n    llama_mlock(const llama_mlock &) = delete;\n\n    ~llama_mlock() {\n        if (size) {\n            raw_unlock(addr, size);\n        }\n    }\n\n    void init(void * ptr) {\n        GGML_ASSERT(addr == NULL && size == 0); // NOLINT\n        addr = ptr;\n    }\n\n    void grow_to(size_t target_size) {\n        GGML_ASSERT(addr);\n        if (failed_already) {\n            return;\n        }\n        size_t granularity = lock_granularity();\n        target_size = (target_size + granularity - 1) & ~(granularity - 1);\n        if (target_size > size) {\n            if (raw_lock((uint8_t *) addr + size, target_size - size)) {\n                size = target_size;\n            } else {\n                failed_already = true;\n            }\n        }\n    }\n\n#ifdef _POSIX_MEMLOCK_RANGE\n    static constexpr bool SUPPORTED = true;\n\n    static size_t lock_granularity() {\n        return (size_t) sysconf(_SC_PAGESIZE);\n    }\n\n    #ifdef __APPLE__\n        #define MLOCK_SUGGESTION \\\n            \"Try increasing the sysctl values 'vm.user_wire_limit' and 'vm.global_user_wire_limit' and/or \" \\\n            \"decreasing 'vm.global_no_user_wire_amount'.  Also try increasing RLIMIT_MLOCK (ulimit -l).\\n\"\n    #else\n        #define MLOCK_SUGGESTION \\\n            \"Try increasing RLIMIT_MLOCK ('ulimit -l' as root).\\n\"\n    #endif\n\n    bool raw_lock(const void * addr, size_t size) const {\n        if (!mlock(addr, size)) {\n            return true;\n        }\n\n        char* errmsg = std::strerror(errno);\n        bool suggest = (errno == ENOMEM);\n\n        // Check if the resource limit is fine after all\n        struct rlimit lock_limit;\n        if (suggest && getrlimit(RLIMIT_MEMLOCK, &lock_limit)) {\n            suggest = false;\n        }\n        if (suggest && (lock_limit.rlim_max > lock_limit.rlim_cur + size)) {\n            suggest = false;\n        }\n\n        fprintf(stderr, \"warning: failed to mlock %zu-byte buffer (after previously locking %zu bytes): %s\\n%s\",\n                size, this->size, errmsg, suggest ? MLOCK_SUGGESTION : \"\");\n        return false;\n    }\n\n    #undef MLOCK_SUGGESTION\n\n    static void raw_unlock(void * addr, size_t size) {\n        if (munlock(addr, size)) {\n            fprintf(stderr, \"warning: failed to munlock buffer: %s\\n\", std::strerror(errno));\n        }\n    }\n#elif defined(_WIN32)\n    static constexpr bool SUPPORTED = true;\n\n    static size_t lock_granularity() {\n        SYSTEM_INFO si;\n        GetSystemInfo(&si);\n        return (size_t) si.dwPageSize;\n    }\n\n    bool raw_lock(void * ptr, size_t len) const {\n        for (int tries = 1; ; tries++) {\n            if (VirtualLock(ptr, len)) {\n                return true;\n            }\n            if (tries == 2) {\n                fprintf(stderr, \"warning: failed to VirtualLock %zu-byte buffer (after previously locking %zu bytes): %s\\n\",\n                    len, size, llama_format_win_err(GetLastError()).c_str());\n                return false;\n            }\n\n            // It failed but this was only the first try; increase the working\n            // set size and try again.\n            SIZE_T min_ws_size, max_ws_size;\n            if (!GetProcessWorkingSetSize(GetCurrentProcess(), &min_ws_size, &max_ws_size)) {\n                fprintf(stderr, \"warning: GetProcessWorkingSetSize failed: %s\\n\",\n                        llama_format_win_err(GetLastError()).c_str());\n                return false;\n            }\n            // Per MSDN: \"The maximum number of pages that a process can lock\n            // is equal to the number of pages in its minimum working set minus\n            // a small overhead.\"\n            // Hopefully a megabyte is enough overhead:\n            size_t increment = len + 1048576;\n            // The minimum must be <= the maximum, so we need to increase both:\n            min_ws_size += increment;\n            max_ws_size += increment;\n            if (!SetProcessWorkingSetSize(GetCurrentProcess(), min_ws_size, max_ws_size)) {\n                fprintf(stderr, \"warning: SetProcessWorkingSetSize failed: %s\\n\",\n                        llama_format_win_err(GetLastError()).c_str());\n                return false;\n            }\n        }\n    }\n\n    static void raw_unlock(void * ptr, size_t len) {\n        if (!VirtualUnlock(ptr, len)) {\n            fprintf(stderr, \"warning: failed to VirtualUnlock buffer: %s\\n\",\n                    llama_format_win_err(GetLastError()).c_str());\n        }\n    }\n#else\n    static constexpr bool SUPPORTED = false;\n\n    static size_t lock_granularity() {\n        return (size_t) 65536;\n    }\n\n    bool raw_lock(const void * addr, size_t len) const {\n        fprintf(stderr, \"warning: mlock not supported on this system\\n\");\n        return false;\n    }\n\n    static void raw_unlock(const void * addr, size_t len) {}\n#endif\n};\n\ntypedef void (*offload_func_t)(struct ggml_tensor * tensor);\n\nstatic void ggml_offload_nop(struct ggml_tensor * tensor) {\n    (void) tensor;\n}\n\nstatic std::string llama_token_to_piece(const struct llama_context * ctx, llama_token token) {\n    std::vector<char> result(8, 0);\n    const int n_tokens = llama_token_to_piece(llama_get_model(ctx), token, result.data(), result.size());\n    if (n_tokens < 0) {\n        result.resize(-n_tokens);\n        int check = llama_token_to_piece(llama_get_model(ctx), token, result.data(), result.size());\n        GGML_ASSERT(check == -n_tokens);\n    }\n    else {\n        result.resize(n_tokens);\n    }\n\n    return std::string(result.data(), result.size());\n}\n\n//\n// globals\n//\n\nstruct llama_state {\n    // We save the log callback globally\n    ggml_log_callback log_callback = llama_log_callback_default;\n    void * log_callback_user_data = nullptr;\n};\n\nstatic llama_state g_state;\n\n// available llama models\nenum e_model {\n    MODEL_UNKNOWN,\n    MODEL_1B,\n    MODEL_3B,\n    MODEL_7B,\n    MODEL_8B,\n    MODEL_13B,\n    MODEL_15B,\n    MODEL_30B,\n    MODEL_34B,\n    MODEL_40B,\n    MODEL_65B,\n    MODEL_70B,\n};\n\nstatic const size_t kB = 1024;\nstatic const size_t MB = 1024*kB;\nstatic const size_t GB = 1024*MB;\n\nstruct llama_hparams {\n    bool     vocab_only;\n    uint32_t n_vocab;\n    uint32_t n_ctx_train; // context size the model was trained on\n    uint32_t n_embd;\n    uint32_t n_head;\n    uint32_t n_head_kv;\n    uint32_t n_layer;\n    uint32_t n_rot;\n    uint32_t n_ff;\n\n    float f_norm_eps;\n    float f_norm_rms_eps;\n\n    float    rope_freq_base_train;\n    float    rope_freq_scale_train;\n    uint32_t n_yarn_orig_ctx;\n    int8_t   rope_scaling_type_train : 3;\n    bool     rope_finetuned : 1;\n\n    float f_clamp_kqv;\n    float f_max_alibi_bias;\n    \n    // sparse predictor threshold if sparse inference is enabled\n    float sparse_pred_threshold = (float)atof(getenv(\"LLAMA_SPARSE_PRED_THRESHOLD\") ? getenv(\"LLAMA_SPARSE_PRED_THRESHOLD\") : \"0.0\");\n\n    bool operator!=(const llama_hparams & other) const {\n        if (this->vocab_only  != other.vocab_only)  return true;\n        if (this->n_vocab     != other.n_vocab)     return true;\n        if (this->n_ctx_train != other.n_ctx_train) return true;\n        if (this->n_embd      != other.n_embd)      return true;\n        if (this->n_head      != other.n_head)      return true;\n        if (this->n_head_kv   != other.n_head_kv)   return true;\n        if (this->n_layer     != other.n_layer)     return true;\n        if (this->n_rot       != other.n_rot)       return true;\n        if (this->n_ff        != other.n_ff)        return true;\n        if (this->rope_finetuned  != other.rope_finetuned)  return true;\n        if (this->n_yarn_orig_ctx != other.n_yarn_orig_ctx) return true;\n\n        const float EPSILON = 1e-9;\n\n        if (!is_float_close(this->f_norm_eps,            other.f_norm_eps,            EPSILON)) return true;\n        if (!is_float_close(this->f_norm_rms_eps,        other.f_norm_rms_eps,        EPSILON)) return true;\n        if (!is_float_close(this->rope_freq_base_train,  other.rope_freq_base_train,  EPSILON)) return true;\n        if (!is_float_close(this->rope_freq_scale_train, other.rope_freq_scale_train, EPSILON)) return true;\n\n        return false;\n    }\n\n    uint32_t n_gqa() const {\n        return n_head/n_head_kv;\n    }\n\n    uint32_t n_embd_head() const {\n        return n_embd/n_head;\n    }\n\n    uint32_t n_embd_gqa() const {\n        return n_embd/n_gqa();\n    }\n};\n\nstruct llama_cparams {\n    uint32_t n_ctx;       // context size used during inference\n    uint32_t n_batch;\n    uint32_t n_threads;       // number of threads to use for generation\n    uint32_t n_threads_batch; // number of threads to use for batch processing\n\n    float    rope_freq_base;\n    float    rope_freq_scale;\n\n    uint32_t n_yarn_orig_ctx;\n    // These hyperparameters are not exposed in GGUF, because all\n    // existing YaRN models use the same values for them.\n    float yarn_ext_factor;\n    float yarn_attn_factor;\n    float yarn_beta_fast;\n    float yarn_beta_slow;\n\n    bool mul_mat_q;\n};\n\nstruct llama_layer {\n    // normalization\n    struct ggml_tensor * attn_norm;\n    struct ggml_tensor * attn_norm_b;\n    struct ggml_tensor * attn_norm_2;\n    struct ggml_tensor * attn_norm_2_b;\n    struct ggml_tensor * attn_q_norm;\n    struct ggml_tensor * attn_q_norm_b;\n    struct ggml_tensor * attn_k_norm;\n    struct ggml_tensor * attn_k_norm_b;\n\n    // attention\n    struct ggml_tensor * wq;\n    struct ggml_tensor * wk;\n    struct ggml_tensor * wv;\n    struct ggml_tensor * wo;\n    struct ggml_tensor * wqkv;\n\n    // attention bias\n    struct ggml_tensor * bo;\n    struct ggml_tensor * bqkv;\n\n    // normalization\n    struct ggml_tensor * ffn_norm;\n    struct ggml_tensor * ffn_norm_b;\n\n    // ff\n    struct ggml_tensor * ffn_gate; // w1\n    struct ggml_tensor * ffn_down; // w2\n    struct ggml_tensor * ffn_up;   // w3\n    struct ggml_tensor * ffn_down_t;\n    \n    // ff sliced on gpu\n    struct ggml_tensor * ffn_gate_gpu;\n    struct ggml_tensor * ffn_down_gpu;\n    struct ggml_tensor * ffn_up_gpu;\n\n    // ff bias\n    struct ggml_tensor * ffn_down_b; // b2\n    struct ggml_tensor * ffn_up_b;   // b3\n\n    // mlp predictor weights\n    struct ggml_tensor * mlp_pre_w1;\n    struct ggml_tensor * mlp_pre_w2;\n\n    // ffn split\n    struct ggml_tensor * gpu_idx; // index of ffn neurons on GPU\n    double gpu_offload_ratio; // ratio of ffn split on GPU ([0, 1])\n    struct ggml_tensor * gpu_bucket; // double index from GPU split neuron to original neuron\n};\n\nstruct llama_kv_cell {\n    llama_pos pos   = -1;\n    llama_pos delta = 0;\n\n    std::set<llama_seq_id> seq_id;\n\n    bool has_seq_id(const llama_seq_id & id) const {\n        return seq_id.find(id) != seq_id.end();\n    }\n};\n\n// ring-buffer of cached KV data\nstruct llama_kv_cache {\n    bool has_shift = false;\n\n    // Note: The value of head isn't only used to optimize searching\n    // for a free KV slot. llama_decode_internal also uses it, so it\n    // cannot be freely changed after a slot has been allocated.\n    uint32_t head = 0;\n    uint32_t size = 0;\n\n    // computed before each graph build\n    uint32_t n = 0;\n\n    std::vector<llama_kv_cell> cells;\n\n    struct ggml_tensor * k = NULL;\n    struct ggml_tensor * v = NULL;\n\n    struct ggml_context * ctx = NULL;\n\n    llama_buffer buf;\n\n    ~llama_kv_cache() {\n        if (ctx) {\n            ggml_free(ctx);\n        }\n\n#ifdef GGML_USE_CUBLAS\n        if (ggml_cublas_loaded()) {\n            ggml_cuda_free_data(k);\n            ggml_cuda_free_data(v);\n        }\n#endif\n    }\n};\n\nstruct llama_vocab {\n    using id    = int32_t;\n    using token = std::string;\n    using ttype = llama_token_type;\n\n    struct token_data {\n        token text;\n        float score;\n        ttype type;\n    };\n\n    enum llama_vocab_type type = LLAMA_VOCAB_TYPE_SPM;\n\n    std::unordered_map<token, id> token_to_id;\n    std::vector<token_data>       id_to_token;\n\n    std::unordered_map<token, id> special_tokens_cache;\n\n    std::map<std::pair<std::string, std::string>, int> bpe_ranks;\n\n    // default LLaMA special tokens\n    id special_bos_id = 1;\n    id special_eos_id = 2;\n    id special_unk_id = 0;\n    id special_sep_id = -1;\n    id special_pad_id = -1;\n\n    id linefeed_id       = 13;\n    id special_prefix_id = 32007;\n    id special_middle_id = 32009;\n    id special_suffix_id = 32008;\n    id special_eot_id    = 32010;\n\n    int find_bpe_rank(std::string token_left, std::string token_right) const {\n        GGML_ASSERT(token_left.find(\" \") == std::string::npos);\n        GGML_ASSERT(token_left.find(\"\\n\") == std::string::npos);\n        GGML_ASSERT(token_right.find(\" \") == std::string::npos);\n        GGML_ASSERT(token_right.find(\"\\n\") == std::string::npos);\n\n        auto it = bpe_ranks.find(std::make_pair(token_left, token_right));\n        if (it == bpe_ranks.end()) {\n            return -1;\n        }\n\n        return it->second;\n    }\n};\n\nstruct llama_gpu_split_loader;\nstruct llama_augmentation_model_loader;\n\nstruct llama_model {\n    e_model     type  = MODEL_UNKNOWN;\n    llm_arch    arch  = LLM_ARCH_UNKNOWN;\n    llama_ftype ftype = LLAMA_FTYPE_ALL_F32;\n\n    std::string name = \"n/a\";\n\n    ggml_sparse_deriv sparse_deriv;\n\n    llama_hparams hparams = {};\n    llama_vocab   vocab;\n\n    struct ggml_tensor * tok_embd;\n    struct ggml_tensor * pos_embd;\n    struct ggml_tensor * tok_norm;\n    struct ggml_tensor * tok_norm_b;\n\n    struct ggml_tensor * output_norm;\n    struct ggml_tensor * output_norm_b;\n    struct ggml_tensor * output;\n\n    std::vector<llama_layer> layers;\n\n    int n_gpu_layers;\n\n    // context\n    struct ggml_context * ctx = NULL;\n\n    // the model memory buffer\n    llama_buffer buf;\n\n    // model memory mapped file\n    std::unique_ptr<llama_mmap> mapping;\n\n    // aux model loaders for dynamically loaded/transformed model weights\n    std::unique_ptr<struct llama_gpu_split_loader> mlp_model_loader;\n    std::unique_ptr<struct llama_augmentation_model_loader> aug_model_loader;\n\n    // objects representing data potentially being locked in memory\n    llama_mlock mlock_buf;\n    llama_mlock mlock_mmap;\n\n    // for quantize-stats only\n    std::vector<std::pair<std::string, struct ggml_tensor *>> tensors_by_name;\n\n    int64_t t_load_us = 0;\n    int64_t t_start_us = 0;\n\n    // neuron size of spilt and offloaded FFN\n    size_t ffn_offloaded_bytes = 0;\n\n    ~llama_model() {\n        if (ctx) {\n            ggml_free(ctx);\n        }\n\n#ifdef GGML_USE_CUBLAS\n        if (ggml_cublas_loaded()) {\n            for (size_t i = 0; i < tensors_by_name.size(); ++i) {\n                ggml_cuda_free_data(tensors_by_name[i].second);\n            }\n            ggml_cuda_free_scratch();\n        }\n#endif\n\n#if defined(GGML_USE_CLBLAST)\n        for (size_t i = 0; i < tensors_by_name.size(); ++i) {\n            ggml_cl_free_data(tensors_by_name[i].second);\n        }\n#endif\n    }\n};\n\nstruct llama_context {\n    llama_context(const llama_model & model) : model(model), t_start_us(model.t_start_us), t_load_us(model.t_load_us) {}\n    ~llama_context() {\n#ifdef GGML_USE_METAL\n        if (ctx_metal) {\n            ggml_metal_free(ctx_metal);\n        }\n#endif\n        if (alloc) {\n            ggml_allocr_free(alloc);\n        }\n    }\n\n    llama_cparams cparams;\n\n    const llama_model & model;\n\n    // key + value cache for the self attention\n    struct llama_kv_cache kv_self;\n\n    std::mt19937 rng;\n\n    bool has_evaluated_once = false;\n\n    int64_t t_start_us;\n    int64_t t_load_us;\n    int64_t t_sample_us = 0;\n    int64_t t_p_eval_us = 0;\n    int64_t t_eval_us   = 0;\n\n    int32_t n_sample = 0; // number of tokens sampled\n    int32_t n_p_eval = 0; // number of tokens in eval calls for the prompt (with batch size > 1)\n    int32_t n_eval   = 0; // number of eval calls\n\n    // decode output (2-dimensional array: [n_tokens][n_vocab])\n    std::vector<float> logits;\n    bool logits_all = false;\n\n    // input embedding (1-dimensional array: [n_embd])\n    std::vector<float> embedding;\n\n    // reusable buffer for `struct ggml_graph_plan.work_data`\n    std::vector<uint8_t> work_buffer;\n\n    // memory buffers used to evaluate the model\n    llama_buffer buf_compute;\n\n    llama_buffer buf_alloc;\n    ggml_allocr * alloc = NULL;\n\n#ifdef GGML_USE_METAL\n    ggml_metal_context * ctx_metal = NULL;\n#endif\n\n#ifdef GGML_USE_MPI\n    ggml_mpi_context * ctx_mpi = NULL;\n#endif\n};\n\n//\n// kv cache helpers\n//\nstatic bool llama_kv_cache_init(\n        const struct llama_hparams & hparams,\n             struct llama_kv_cache & cache,\n                         ggml_type   wtype,\n                          uint32_t   n_ctx,\n                               int   n_gpu_layers) {\n    const uint32_t n_embd  = hparams.n_embd_gqa();\n    const uint32_t n_layer = hparams.n_layer;\n\n    const int64_t n_mem      = n_layer*n_ctx;\n    const int64_t n_elements = n_embd*n_mem;\n\n    cache.has_shift = false;\n\n    cache.head = 0;\n    cache.size = n_ctx;\n\n    cache.cells.clear();\n    cache.cells.resize(n_ctx);\n\n    cache.buf.resize(2u*n_elements*ggml_type_size(wtype) + 2u*ggml_tensor_overhead());\n    memset(cache.buf.data, 0, cache.buf.size);\n\n    struct ggml_init_params params;\n    params.mem_size   = cache.buf.size;\n    params.mem_buffer = cache.buf.data;\n    params.no_alloc   = false;\n\n    cache.ctx = ggml_init(params);\n\n    if (!cache.ctx) {\n        LLAMA_LOG_ERROR(\"%s: failed to allocate memory for kv cache\\n\", __func__);\n        return false;\n    }\n\n    cache.k = ggml_new_tensor_1d(cache.ctx, wtype, n_elements);\n    cache.v = ggml_new_tensor_1d(cache.ctx, wtype, n_elements);\n    ggml_set_name(cache.k, \"cache_k\");\n    ggml_set_name(cache.v, \"cache_v\");\n\n    (void) n_gpu_layers;\n\n#ifdef GGML_USE_CUBLAS\n    if (ggml_cublas_loaded()) {\n        size_t vram_kv_cache = 0;\n\n        if (n_gpu_layers > (int)n_layer + 1) {\n            ggml_cuda_assign_buffers_no_scratch(cache.v);\n            LLAMA_LOG_INFO(\"%s: offloading v cache to GPU\\n\", __func__);\n            vram_kv_cache += ggml_nbytes(cache.v);\n        }\n        if (n_gpu_layers > (int)n_layer + 2) {\n            ggml_cuda_assign_buffers_no_scratch(cache.k);\n            LLAMA_LOG_INFO(\"%s: offloading k cache to GPU\\n\", __func__);\n            vram_kv_cache += ggml_nbytes(cache.k);\n        }\n        if (vram_kv_cache > 0) {\n            LLAMA_LOG_INFO(\"%s: VRAM kv self = %.2f MB\\n\", __func__, vram_kv_cache / 1024.0 / 1024.0);\n        }\n    }\n#endif\n\n    return true;\n}\n\n// find an empty slot of size \"n_tokens\" in the cache\n// updates the cache head\n// Note: On success, it's important that cache.head points\n// to the first cell of the slot.\nstatic bool llama_kv_cache_find_slot(\n           struct llama_kv_cache & cache,\n        const struct llama_batch & batch) {\n    const uint32_t n_ctx    = cache.size;\n    const uint32_t n_tokens = batch.n_tokens;\n\n    if (n_tokens > n_ctx) {\n        LLAMA_LOG_ERROR(\"%s: n_tokens=%d > n_ctx=%d\\n\", __func__, n_tokens, n_ctx);\n        return false;\n    }\n\n    uint32_t n_tested = 0;\n\n    while (true) {\n        if (cache.head + n_tokens > n_ctx) {\n            n_tested += n_ctx - cache.head;\n            cache.head = 0;\n            continue;\n        }\n\n        bool found = true;\n        for (uint32_t i = 0; i < n_tokens; i++) {\n            if (cache.cells[cache.head + i].pos >= 0) {\n                found = false;\n                cache.head += i + 1;\n                n_tested   += i + 1;\n                break;\n            }\n        }\n\n        if (found) {\n            break;\n        }\n\n        if (n_tested >= n_ctx) {\n            //LLAMA_LOG_ERROR(\"%s: failed to find a slot for %d tokens\\n\", __func__, n_tokens);\n            return false;\n        }\n    }\n\n    for (uint32_t i = 0; i < n_tokens; i++) {\n        cache.cells[cache.head + i].pos = batch.pos[i];\n\n        for (int32_t j = 0; j < batch.n_seq_id[i]; j++) {\n            cache.cells[cache.head + i].seq_id.insert(batch.seq_id[i][j]);\n        }\n    }\n\n    return true;\n}\n\n// find how many cells are currently in use\nstatic int32_t llama_kv_cache_cell_max(const struct llama_kv_cache & cache) {\n    for (uint32_t i = cache.size - 1; i > 0; --i) {\n        if (cache.cells[i].pos >= 0 && !cache.cells[i].seq_id.empty()) {\n            return i + 1;\n        }\n    }\n\n    return 0;\n}\n\nstatic void llama_kv_cache_clear(struct llama_kv_cache & cache) {\n    for (int32_t i = 0; i < (int32_t) cache.size; ++i) {\n        cache.cells[i].pos = -1;\n        cache.cells[i].seq_id.clear();\n    }\n    cache.head = 0;\n}\n\nstatic void llama_kv_cache_seq_rm(\n        struct llama_kv_cache & cache,\n                 llama_seq_id   seq_id,\n                    llama_pos   p0,\n                    llama_pos   p1) {\n    uint32_t new_head = cache.size;\n\n    if (p0 < 0) p0 = 0;\n    if (p1 < 0) p1 = std::numeric_limits<llama_pos>::max();\n\n    for (uint32_t i = 0; i < cache.size; ++i) {\n        if (cache.cells[i].pos >= p0 && cache.cells[i].pos < p1) {\n            if (seq_id < 0) {\n                cache.cells[i].seq_id.clear();\n            } else if (cache.cells[i].has_seq_id(seq_id)) {\n                cache.cells[i].seq_id.erase(seq_id);\n            } else {\n                continue;\n            }\n            if (cache.cells[i].seq_id.empty()) {\n                cache.cells[i].pos = -1;\n                if (new_head == cache.size) new_head = i;\n            }\n        }\n    }\n\n    // If we freed up a slot, set head to it so searching can start there.\n    if (new_head != cache.size) cache.head = new_head;\n}\n\nstatic void llama_kv_cache_seq_cp(\n        struct llama_kv_cache & cache,\n                 llama_seq_id   seq_id_src,\n                 llama_seq_id   seq_id_dst,\n                    llama_pos   p0,\n                    llama_pos   p1) {\n    if (p0 < 0) p0 = 0;\n    if (p1 < 0) p1 = std::numeric_limits<llama_pos>::max();\n\n    cache.head = 0;\n\n    for (uint32_t i = 0; i < cache.size; ++i) {\n        if (cache.cells[i].has_seq_id(seq_id_src) && cache.cells[i].pos >= p0 && cache.cells[i].pos < p1) {\n            cache.cells[i].seq_id.insert(seq_id_dst);\n        }\n    }\n}\n\nstatic void llama_kv_cache_seq_keep(struct llama_kv_cache & cache, llama_seq_id seq_id) {\n    uint32_t new_head = cache.size;\n\n    for (uint32_t i = 0; i < cache.size; ++i) {\n        if (!cache.cells[i].has_seq_id(seq_id)) {\n            cache.cells[i].pos = -1;\n            cache.cells[i].seq_id.clear();\n            if (new_head == cache.size) new_head = i;\n        } else {\n            cache.cells[i].seq_id.clear();\n            cache.cells[i].seq_id.insert(seq_id);\n        }\n    }\n\n    // If we freed up a slot, set head to it so searching can start there.\n    if (new_head != cache.size) cache.head = new_head;\n}\n\nstatic void llama_kv_cache_seq_shift(\n        struct llama_kv_cache & cache,\n                 llama_seq_id   seq_id,\n                    llama_pos   p0,\n                    llama_pos   p1,\n                    llama_pos   delta) {\n    uint32_t new_head = cache.size;\n\n    if (p0 < 0) p0 = 0;\n    if (p1 < 0) p1 = std::numeric_limits<llama_pos>::max();\n\n    for (uint32_t i = 0; i < cache.size; ++i) {\n        if (cache.cells[i].has_seq_id(seq_id) && cache.cells[i].pos >= p0 && cache.cells[i].pos < p1) {\n            cache.has_shift = true;\n            cache.cells[i].pos   += delta;\n            cache.cells[i].delta += delta;\n\n            if (cache.cells[i].pos < 0) {\n                cache.cells[i].pos = -1;\n                cache.cells[i].seq_id.clear();\n                if (new_head == cache.size) new_head = i;\n            }\n        }\n    }\n\n    // If we freed up a slot, set head to it so searching can start there.\n    // Otherwise we just start the next search from the beginning.\n    cache.head = new_head != cache.size ? new_head : 0;\n}\n\n//\n// model loading and saving\n//\n\nenum llama_fver {\n    GGUF_FILE_VERSION_V1 = 1,\n    GGUF_FILE_VERSION_V2 = 2,\n    GGUF_FILE_VERSION_V3 = 3,\n};\n\nstatic const char * llama_file_version_name(llama_fver version) {\n    switch (version) {\n        case GGUF_FILE_VERSION_V1: return \"GGUF V1 (support until nov 2023)\";\n        case GGUF_FILE_VERSION_V2: return \"GGUF V2\";\n        case GGUF_FILE_VERSION_V3: return \"GGUF V3 (latest)\";\n    }\n\n    return \"unknown\";\n}\n\nstatic std::string llama_format_tensor_shape(const std::vector<int64_t> & ne) {\n    char buf[256];\n    snprintf(buf, sizeof(buf), \"%5\" PRId64, ne.at(0));\n    for (size_t i = 1; i < ne.size(); i++) {\n        snprintf(buf + strlen(buf), sizeof(buf) - strlen(buf), \", %5\" PRId64, ne.at(i));\n    }\n    return buf;\n}\n\nstatic std::string llama_format_tensor_shape(const struct ggml_tensor * t) {\n    char buf[256];\n    snprintf(buf, sizeof(buf), \"%5\" PRId64, t->ne[0]);\n    for (int i = 1; i < GGML_MAX_DIMS; i++) {\n        snprintf(buf + strlen(buf), sizeof(buf) - strlen(buf), \", %5\" PRId64, t->ne[i]);\n    }\n    return buf;\n}\n\nstruct llama_model_loader {\n    int n_kv      = 0;\n    int n_tensors = 0;\n    int n_created = 0;\n\n    ggml_sparse_deriv sparse_deriv;\n\n    int64_t n_elements = 0;\n    size_t  n_bytes    = 0;\n\n    bool use_mmap = false;\n\n    llama_file  file;\n    llama_ftype ftype;\n    llama_fver  fver;\n\n    std::unique_ptr<llama_mmap> mapping;\n\n    struct gguf_context * ctx_gguf = NULL;\n    struct ggml_context * ctx_meta = NULL;\n\n    llama_model_loader(const std::string & fname, bool use_mmap) : file(fname.c_str(), \"rb\") {\n        struct gguf_init_params params = {\n            /*.no_alloc = */ true,\n            /*.ctx      = */ &ctx_meta,\n        };\n\n        ctx_gguf = gguf_init_from_file(fname.c_str(), params);\n        if (!ctx_gguf) {\n            throw std::runtime_error(format(\"%s: failed to load model from %s\\n\", __func__, fname.c_str()));\n        }\n\n        n_kv      = gguf_get_n_kv(ctx_gguf);\n        n_tensors = gguf_get_n_tensors(ctx_gguf);\n        sparse_deriv = gguf_get_sparse_deriv(ctx_gguf);\n        fver = (enum llama_fver ) gguf_get_version(ctx_gguf);\n\n        for (int i = 0; i < n_tensors; i++) {\n            const char * name = gguf_get_tensor_name(ctx_gguf, i);\n            struct ggml_tensor * t = ggml_get_tensor(ctx_meta, name);\n            n_elements += ggml_nelements(t);\n            n_bytes    += ggml_nbytes(t);\n        }\n\n        LLAMA_LOG_INFO(\"%s: loaded meta data with %d key-value pairs and %d tensors from %s (version %s)\\n\",\n                __func__, n_kv, n_tensors, fname.c_str(), llama_file_version_name(fver));\n\n        // determine file type based on the number of tensors for each quantization and print meta data\n        // TODO: make optional\n        {\n            std::map<enum ggml_type, uint32_t> n_type;\n\n            uint32_t n_type_max = 0;\n            enum ggml_type type_max = GGML_TYPE_F32;\n\n            for (int i = 0; i < n_tensors; i++) {\n                const char * name = gguf_get_tensor_name(ctx_gguf, i);\n                struct ggml_tensor * meta = ggml_get_tensor(ctx_meta, name);\n\n                n_type[meta->type]++;\n\n                if (n_type_max < n_type[meta->type]) {\n                    n_type_max = n_type[meta->type];\n                    type_max   = meta->type;\n                }\n\n                LLAMA_LOG_INFO(\"%s: - tensor %4d: %32s %-8s [ %s ]\\n\", __func__, i, name, ggml_type_name(meta->type), llama_format_tensor_shape(meta).c_str());\n            }\n\n            switch (type_max) {\n                case GGML_TYPE_F32:  ftype = LLAMA_FTYPE_ALL_F32;       break;\n                case GGML_TYPE_F16:  ftype = LLAMA_FTYPE_MOSTLY_F16;    break;\n                case GGML_TYPE_Q4_0: ftype = LLAMA_FTYPE_MOSTLY_Q4_0;   break;\n                case GGML_TYPE_Q4_1: ftype = LLAMA_FTYPE_MOSTLY_Q4_1;   break;\n                case GGML_TYPE_Q5_0: ftype = LLAMA_FTYPE_MOSTLY_Q5_0;   break;\n                case GGML_TYPE_Q5_1: ftype = LLAMA_FTYPE_MOSTLY_Q5_1;   break;\n                case GGML_TYPE_Q8_0: ftype = LLAMA_FTYPE_MOSTLY_Q8_0;   break;\n                case GGML_TYPE_Q2_K: ftype = LLAMA_FTYPE_MOSTLY_Q2_K;   break;\n                case GGML_TYPE_Q3_K: ftype = LLAMA_FTYPE_MOSTLY_Q3_K_M; break;\n                case GGML_TYPE_Q4_K: ftype = LLAMA_FTYPE_MOSTLY_Q4_K_M; break;\n                case GGML_TYPE_Q5_K: ftype = LLAMA_FTYPE_MOSTLY_Q5_K_M; break;\n                case GGML_TYPE_Q6_K: ftype = LLAMA_FTYPE_MOSTLY_Q6_K;   break;\n                default:\n                     {\n                         LLAMA_LOG_WARN(\"%s: unknown type %s\\n\", __func__, ggml_type_name(type_max));\n                         ftype = LLAMA_FTYPE_ALL_F32;\n                     } break;\n            }\n\n            // this is a way to mark that we have \"guessed\" the file type\n            ftype = (llama_ftype) (ftype | LLAMA_FTYPE_GUESSED);\n\n            {\n                const int kid = gguf_find_key(ctx_gguf, \"general.file_type\");\n                if (kid >= 0) {\n                    ftype = (llama_ftype) gguf_get_val_u32(ctx_gguf, kid);\n                }\n            }\n\n            for (int i = 0; i < n_kv; i++) {\n                const char * name         = gguf_get_key(ctx_gguf, i);\n                const enum gguf_type type = gguf_get_kv_type(ctx_gguf, i);\n\n                LLAMA_LOG_INFO(\"%s: - kv %3d: %42s %-8s\\n\", __func__, i, name, gguf_type_name(type));\n            }\n\n            // print type counts\n            for (auto & kv : n_type) {\n                if (kv.second == 0) {\n                    continue;\n                }\n\n                LLAMA_LOG_INFO(\"%s: - type %4s: %4d tensors\\n\", __func__, ggml_type_name(kv.first), kv.second);\n            }\n        }\n\n        if (!llama_mmap::SUPPORTED) {\n            LLAMA_LOG_WARN(\"%s: mmap is not supported on this platform\\n\", __func__);\n            use_mmap = false;\n        }\n\n        this->use_mmap = use_mmap;\n    }\n\n    ~llama_model_loader() {\n        if (ctx_gguf) {\n            gguf_free(ctx_gguf);\n        }\n        if (ctx_meta) {\n            ggml_free(ctx_meta);\n        }\n    }\n\n    std::string get_arch_name() const {\n        const auto kv = LLM_KV(LLM_ARCH_UNKNOWN);\n\n        std::string arch_name;\n        GGUF_GET_KEY(ctx_gguf, arch_name, gguf_get_val_str, GGUF_TYPE_STRING, false, kv(LLM_KV_GENERAL_ARCHITECTURE));\n\n        return arch_name;\n    }\n\n    enum llm_arch get_arch() const {\n        const std::string arch_name = get_arch_name();\n\n        return llm_arch_from_string(arch_name);\n    }\n\n    const char * get_tensor_name(int i) const {\n        return gguf_get_tensor_name(ctx_gguf, i);\n    }\n\n    struct ggml_tensor * get_tensor_meta(int i) const {\n        return ggml_get_tensor(ctx_meta, get_tensor_name(i));\n    }\n\n    void calc_sizes(size_t & ctx_size_p, size_t & mmapped_size_p) const {\n        ctx_size_p     = 0;\n        mmapped_size_p = 0;\n\n        for (int i = 0; i < n_tensors; i++) {\n            struct ggml_tensor * meta = get_tensor_meta(i);\n            ctx_size_p += sizeof(struct ggml_tensor) + GGML_OBJECT_SIZE;\n            (use_mmap ? mmapped_size_p : ctx_size_p) += ggml_nbytes_pad(meta);\n        }\n    }\n\n    struct ggml_tensor * create_tensor_for(struct ggml_context * ctx, struct ggml_tensor * meta, ggml_backend_type backend) {\n        if (backend != GGML_BACKEND_CPU) {\n            ggml_set_no_alloc(ctx, true);\n        }\n\n        struct ggml_tensor * tensor = ggml_dup_tensor(ctx, meta);\n        tensor->backend = backend; // TODO: ggml_set_backend\n        ggml_set_name(tensor, ggml_get_name(meta));\n\n        if (backend != GGML_BACKEND_CPU) {\n            ggml_set_no_alloc(ctx, use_mmap);\n        }\n\n        n_created++;\n\n        return tensor;\n    }\n\n    struct ggml_tensor * create_tensor(struct ggml_context * ctx, const std::pair<std::string, llm_tensor> & tn, const std::vector<int64_t> & ne, ggml_backend_type backend) {\n        return create_tensor(ctx, tn.first, ne, backend);\n    }\n\n    struct ggml_tensor * create_tensor(struct ggml_context * ctx, const std::string &name, const std::vector<int64_t> & ne, ggml_backend_type backend) {\n        struct ggml_tensor * cur = ggml_get_tensor(ctx_meta, name.c_str());\n\n        if (cur == NULL) {\n            throw std::runtime_error(format(\"%s: tensor '%s' not found\", __func__, name.c_str()));\n        }\n\n        if (backend == GGML_BACKEND_GPU_SPLIT) {\n            if (ne.size() == 1) {\n                throw std::runtime_error(format(\"%s: 1-dimensional tensor '%s' cannot be split on the GPU\", __func__, name.c_str()));\n            }\n        }\n\n        {\n            bool is_ok = true;\n            for (size_t i = 0; i < ne.size(); ++i) {\n                if (ne[i] != cur->ne[i]) {\n                    // allow for -1 in ne for wildcard dimensions\n                    is_ok = ne[i] == -1;\n                    break;\n                }\n            }\n            if (!is_ok) {\n                throw std::runtime_error(\n                        format(\"%s: tensor '%s' has wrong shape; expected %s, got %s\",\n                            __func__, name.c_str(),\n                            llama_format_tensor_shape(ne).c_str(),\n                            llama_format_tensor_shape(cur).c_str()));\n            }\n        }\n\n        return create_tensor_for(ctx, cur, backend);\n    }\n\n    void done_getting_tensors() const {\n        if (n_created != n_tensors) {\n            throw std::runtime_error(format(\"%s: wrong number of tensors; expected %d, got %d\", __func__, n_tensors, n_created));\n        }\n    }\n\n    size_t file_offset(const char * name) const {\n        const int idx = gguf_find_tensor(ctx_gguf, name);\n\n        if (idx < 0) {\n            throw std::runtime_error(format(\"%s: tensor '%s' not found in the file\", __func__, name));\n        }\n\n        return gguf_get_data_offset(ctx_gguf) + gguf_get_tensor_offset(ctx_gguf, idx);\n    }\n\n    void load_data_for(struct ggml_tensor * cur) const {\n        const size_t offs = file_offset(ggml_get_name(cur));\n\n        if (use_mmap) {\n            cur->data = (uint8_t *) mapping->addr + offs;\n        } else {\n            file.seek(offs, SEEK_SET);\n            file.read_raw(cur->data, ggml_nbytes(cur));\n        }\n    }\n\n    void load_all_data(struct ggml_context * ctx, llama_progress_callback progress_callback, void * progress_callback_user_data, llama_mlock * lmlock) {\n        size_t size_data = 0;\n        size_t size_lock = 0;\n        size_t size_pref = 0; // prefetch\n\n        for (int i = 0; i < gguf_get_n_tensors(ctx_gguf); i++) {\n            struct ggml_tensor * cur = ggml_get_tensor(ctx, gguf_get_tensor_name(ctx_gguf, i));\n            size_data += ggml_nbytes(cur);\n            if (cur->backend == GGML_BACKEND_CPU) {\n                size_pref += ggml_nbytes(cur);\n            }\n        }\n\n        if (use_mmap) {\n            mapping.reset(new llama_mmap(&file, size_pref, ggml_is_numa()));\n            if (lmlock) {\n                lmlock->init(mapping->addr);\n            }\n        }\n\n        size_t done_size = 0;\n        for (int i = 0; i < gguf_get_n_tensors(ctx_gguf); i++) {\n            struct ggml_tensor * cur = ggml_get_tensor(ctx, gguf_get_tensor_name(ctx_gguf, i));\n            GGML_ASSERT(cur); // unused tensors should have been caught by load_data already\n\n            if (progress_callback) {\n                progress_callback((float) done_size / size_data, progress_callback_user_data);\n            }\n\n            // allocate temp buffer if not using mmap\n            if (!use_mmap && cur->data == NULL) {\n                GGML_ASSERT(cur->backend != GGML_BACKEND_CPU);\n                #ifdef GGML_USE_CPU_HBM\n                cur->data = (uint8_t*)hbw_malloc(ggml_nbytes(cur));\n                #else\n                cur->data = (uint8_t*)malloc(ggml_nbytes(cur));\n                #endif\n            }\n\n            load_data_for(cur);\n\n            switch (cur->backend) {\n                case GGML_BACKEND_CPU:\n                    if (use_mmap && lmlock) {\n                        size_lock += ggml_nbytes(cur);\n                        lmlock->grow_to(size_lock);\n                    }\n                    break;\n#ifdef GGML_USE_CUBLAS\n                case GGML_BACKEND_GPU:\n                case GGML_BACKEND_GPU_SPLIT:\n                    // old code:\n                    //ggml_cuda_transform_tensor(lt.data, lt.ggml_tensor);\n\n                    // TODO: test if this works !!\n                    ggml_cuda_transform_tensor(cur->data, cur);\n                    if (!use_mmap) {\n                        free(cur->data);\n                    }\n                    break;\n#elif defined(GGML_USE_CLBLAST)\n                case GGML_BACKEND_GPU:\n                    ggml_cl_transform_tensor(cur->data, cur);\n                    if (!use_mmap) {\n                        free(cur->data);\n                    }\n                    break;\n#endif\n                default:\n                    continue;\n            }\n\n            done_size += ggml_nbytes(cur);\n        }\n    }\n};\n\n//\n// load LLaMA models\n//\n\nstatic std::string llama_model_arch_name(llm_arch arch) {\n    auto it = LLM_ARCH_NAMES.find(arch);\n    if (it == LLM_ARCH_NAMES.end()) {\n        return \"unknown\";\n    }\n    return it->second;\n}\n\nstatic std::string llama_model_ftype_name(llama_ftype ftype) {\n    if (ftype & LLAMA_FTYPE_GUESSED) {\n        return llama_model_ftype_name((enum llama_ftype) (ftype & ~LLAMA_FTYPE_GUESSED)) + \" (guessed)\";\n    }\n\n    switch (ftype) {\n        case LLAMA_FTYPE_ALL_F32:     return \"all F32\";\n        case LLAMA_FTYPE_MOSTLY_F16:  return \"mostly F16\";\n        case LLAMA_FTYPE_MOSTLY_Q4_0: return \"mostly Q4_0\";\n        case LLAMA_FTYPE_MOSTLY_Q4_1: return \"mostly Q4_1\";\n        case LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16:\n                                      return \"mostly Q4_1, some F16\";\n        case LLAMA_FTYPE_MOSTLY_Q5_0: return \"mostly Q5_0\";\n        case LLAMA_FTYPE_MOSTLY_Q5_1: return \"mostly Q5_1\";\n        case LLAMA_FTYPE_MOSTLY_Q8_0: return \"mostly Q8_0\";\n\n        // K-quants\n        case LLAMA_FTYPE_MOSTLY_Q2_K:   return \"mostly Q2_K\";\n        case LLAMA_FTYPE_MOSTLY_Q3_K_S: return \"mostly Q3_K - Small\";\n        case LLAMA_FTYPE_MOSTLY_Q3_K_M: return \"mostly Q3_K - Medium\";\n        case LLAMA_FTYPE_MOSTLY_Q3_K_L: return \"mostly Q3_K - Large\";\n        case LLAMA_FTYPE_MOSTLY_Q4_K_S: return \"mostly Q4_K - Small\";\n        case LLAMA_FTYPE_MOSTLY_Q4_K_M: return \"mostly Q4_K - Medium\";\n        case LLAMA_FTYPE_MOSTLY_Q5_K_S: return \"mostly Q5_K - Small\";\n        case LLAMA_FTYPE_MOSTLY_Q5_K_M: return \"mostly Q5_K - Medium\";\n        case LLAMA_FTYPE_MOSTLY_Q6_K:   return \"mostly Q6_K\";\n\n        default: return \"unknown, may not work\";\n    }\n}\n\nstatic const char * llama_model_type_name(e_model type) {\n    switch (type) {\n        case MODEL_1B:  return \"1B\";\n        case MODEL_3B:  return \"3B\";\n        case MODEL_7B:  return \"7B\";\n        case MODEL_8B:  return \"8B\";\n        case MODEL_13B: return \"13B\";\n        case MODEL_15B: return \"15B\";\n        case MODEL_30B: return \"30B\";\n        case MODEL_34B: return \"34B\";\n        case MODEL_40B: return \"40B\";\n        case MODEL_65B: return \"65B\";\n        case MODEL_70B: return \"70B\";\n        default:        return \"?B\";\n    }\n}\n\nstatic void llm_load_arch(llama_model_loader & ml, llama_model & model) {\n    model.arch = ml.get_arch();\n    if (model.arch == LLM_ARCH_UNKNOWN) {\n        throw std::runtime_error(\"unknown model architecture: '\" + ml.get_arch_name() + \"'\");\n    }\n}\n\nstatic void llm_load_hparams(\n        llama_model_loader & ml,\n        llama_model & model) {\n    struct gguf_context * ctx = ml.ctx_gguf;\n\n    const auto kv = LLM_KV(model.arch);\n\n    auto & hparams = model.hparams;\n\n    // get general kv\n    GGUF_GET_KEY(ctx, model.name, gguf_get_val_str, GGUF_TYPE_STRING, false, kv(LLM_KV_GENERAL_NAME));\n\n    // get hparams kv\n    GGUF_GET_KEY(ctx, hparams.n_vocab,        gguf_get_arr_n,   GGUF_TYPE_ARRAY,  true, kv(LLM_KV_TOKENIZER_LIST));\n    GGUF_GET_KEY(ctx, hparams.n_ctx_train,    gguf_get_val_u32, GGUF_TYPE_UINT32, true, kv(LLM_KV_CONTEXT_LENGTH));\n    GGUF_GET_KEY(ctx, hparams.n_embd,         gguf_get_val_u32, GGUF_TYPE_UINT32, true, kv(LLM_KV_EMBEDDING_LENGTH));\n    GGUF_GET_KEY(ctx, hparams.n_ff,           gguf_get_val_u32, GGUF_TYPE_UINT32, true, kv(LLM_KV_FEED_FORWARD_LENGTH));\n    GGUF_GET_KEY(ctx, hparams.n_head,         gguf_get_val_u32, GGUF_TYPE_UINT32, true, kv(LLM_KV_ATTENTION_HEAD_COUNT));\n    GGUF_GET_KEY(ctx, hparams.n_layer,        gguf_get_val_u32, GGUF_TYPE_UINT32, true, kv(LLM_KV_BLOCK_COUNT));\n\n    // n_head_kv is optional, default to n_head\n    hparams.n_head_kv = hparams.n_head;\n    GGUF_GET_KEY(ctx, hparams.n_head_kv, gguf_get_val_u32, GGUF_TYPE_UINT32, false, kv(LLM_KV_ATTENTION_HEAD_COUNT_KV));\n\n    hparams.rope_finetuned = false;\n    GGUF_GET_KEY(ctx, hparams.rope_finetuned, gguf_get_val_bool, GGUF_TYPE_BOOL, false,\n                 kv(LLM_KV_ROPE_SCALING_FINETUNED));\n\n    hparams.n_yarn_orig_ctx = hparams.n_ctx_train;\n    GGUF_GET_KEY(ctx, hparams.n_yarn_orig_ctx, gguf_get_val_u32, GGUF_TYPE_UINT32, false,\n                 kv(LLM_KV_ROPE_SCALING_ORIG_CTX_LEN));\n\n    // rope_freq_base (optional)\n    hparams.rope_freq_base_train = 10000.0f;\n    GGUF_GET_KEY(ctx, hparams.rope_freq_base_train, gguf_get_val_f32, GGUF_TYPE_FLOAT32, false, kv(LLM_KV_ROPE_FREQ_BASE));\n\n    std::string rope_scaling(\"linear\");\n    GGUF_GET_KEY(ctx, rope_scaling, gguf_get_val_str, GGUF_TYPE_STRING, false, kv(LLM_KV_ROPE_SCALING_TYPE));\n    hparams.rope_scaling_type_train = llama_rope_scaling_type_from_string(rope_scaling);\n    GGML_ASSERT(hparams.rope_scaling_type_train != LLAMA_ROPE_SCALING_UNSPECIFIED);\n\n    // rope_freq_scale (inverse of the kv) is optional\n    float ropescale = 0.0f;\n    GGUF_GET_KEY(ctx, ropescale, gguf_get_val_f32, GGUF_TYPE_FLOAT32, false, kv(LLM_KV_ROPE_SCALING_FACTOR));\n    if (ropescale == 0.0f) { // try the old key name\n        GGUF_GET_KEY(ctx, ropescale, gguf_get_val_f32, GGUF_TYPE_FLOAT32, false, kv(LLM_KV_ROPE_SCALE_LINEAR));\n    }\n    hparams.rope_freq_scale_train = ropescale == 0.0f ? 1.0f : 1.0f/ropescale;\n\n    // sanity check for n_rot (optional)\n    {\n        hparams.n_rot = hparams.n_embd / hparams.n_head;\n\n        GGUF_GET_KEY(ctx, hparams.n_rot, gguf_get_val_u32, GGUF_TYPE_UINT32, false, kv(LLM_KV_ROPE_DIMENSION_COUNT));\n\n        if (model.arch == LLM_ARCH_LLAMA || model.arch == LLM_ARCH_FALCON) {\n            if (hparams.n_rot != hparams.n_embd / hparams.n_head) {\n                throw std::runtime_error(format(\"invalid n_rot: %u, expected %u\", hparams.n_rot, hparams.n_embd / hparams.n_head));\n            }\n        }\n        // gpt-neox n_rot = rotary_pct * (n_embd / n_head)\n        // gpt-j n_rot = rotary_dim\n    }\n\n    if (gguf_get_sparse_deriv(ctx)) {\n        // read sparse threshold override if sparse deriv is enabled\n        GGUF_GET_KEY(ctx, hparams.sparse_pred_threshold, gguf_get_val_f32, GGUF_TYPE_FLOAT32, false, kv(LLM_KV_SPARSE_THRESHOLD));\n        if (getenv(\"LLAMA_SPARSE_PRED_THRESHOLD\"))\n            hparams.sparse_pred_threshold = (float)atof(getenv(\"LLAMA_SPARSE_PRED_THRESHOLD\"));\n    }\n\n    // arch-specific KVs\n    switch (model.arch) {\n        case LLM_ARCH_LLAMA:\n        case LLM_ARCH_BAMBOO:\n            {\n                GGUF_GET_KEY(ctx, hparams.f_norm_rms_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_RMS_EPS));\n\n                switch (hparams.n_layer) {\n                    case 26: model.type = e_model::MODEL_3B; break;\n                    case 32: model.type = e_model::MODEL_7B; break;\n                    case 40: model.type = e_model::MODEL_13B; break;\n                    case 48: model.type = e_model::MODEL_34B; break;\n                    case 60: model.type = e_model::MODEL_30B; break;\n                    case 80: model.type = hparams.n_head == hparams.n_head_kv ? e_model::MODEL_65B : e_model::MODEL_70B; break;\n                    default: model.type = e_model::MODEL_UNKNOWN;\n                }\n            } break;\n        case LLM_ARCH_FALCON:\n            {\n                GGUF_GET_KEY(ctx, hparams.f_norm_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_EPS));\n\n                switch (hparams.n_layer) {\n                    case 32: model.type = e_model::MODEL_7B; break;\n                    case 60: model.type = e_model::MODEL_40B; break;\n                    default: model.type = e_model::MODEL_UNKNOWN;\n                }\n            } break;\n        case LLM_ARCH_BAICHUAN:\n            {\n                GGUF_GET_KEY(ctx, hparams.f_norm_rms_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_RMS_EPS));\n                switch (hparams.n_layer) {\n                    case 32: model.type = e_model::MODEL_7B; break;\n                    case 40: model.type = e_model::MODEL_13B; break;\n                    default: model.type = e_model::MODEL_UNKNOWN;\n                }\n            } break;\n        case LLM_ARCH_STARCODER:\n            {\n                GGUF_GET_KEY(ctx, hparams.f_norm_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_EPS));\n                switch (hparams.n_layer) {\n                    case 24: model.type = e_model::MODEL_1B; break;\n                    case 36: model.type = e_model::MODEL_3B; break;\n                    case 42: model.type = e_model::MODEL_7B; break;\n                    case 40: model.type = e_model::MODEL_15B; break;\n                    default: model.type = e_model::MODEL_UNKNOWN;\n                }\n            } break;\n        case LLM_ARCH_PERSIMMON:\n            {\n                GGUF_GET_KEY(ctx, hparams.f_norm_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_EPS));\n                switch (hparams.n_layer) {\n                    case 36: model.type = e_model::MODEL_8B; break;\n                    default: model.type = e_model::MODEL_UNKNOWN;\n                }\n            } break;\n        case LLM_ARCH_REFACT:\n            {\n                GGUF_GET_KEY(ctx, hparams.f_norm_rms_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_RMS_EPS));\n                switch (hparams.n_layer) {\n                    case 32: model.type = e_model::MODEL_1B; break;\n                    default: model.type = e_model::MODEL_UNKNOWN;\n                }\n            } break;\n        case LLM_ARCH_BLOOM:\n            {\n                GGUF_GET_KEY(ctx, hparams.f_norm_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_EPS));\n\n                switch (hparams.n_layer) {\n                    case 24: model.type = e_model::MODEL_1B; break;\n                    case 30:\n                        switch (hparams.n_embd) {\n                            case 2560: model.type = e_model::MODEL_3B; break;\n                            case 4096: model.type = e_model::MODEL_7B; break;\n                        } break;\n                }\n            } break;\n        case LLM_ARCH_MPT:\n            {\n                hparams.f_clamp_kqv = 0.0f;\n\n                GGUF_GET_KEY(ctx, hparams.f_norm_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_EPS));\n                GGUF_GET_KEY(ctx, hparams.f_clamp_kqv, gguf_get_val_f32, GGUF_TYPE_FLOAT32, false, kv(LLM_KV_ATTENTION_CLAMP_KQV));\n                GGUF_GET_KEY(ctx, hparams.f_max_alibi_bias, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_MAX_ALIBI_BIAS));\n\n                switch (hparams.n_layer) {\n                    case 32: model.type = e_model::MODEL_7B; break;\n                    case 48: model.type = e_model::MODEL_30B; break;\n                    default: model.type = e_model::MODEL_UNKNOWN;\n                }\n            } break;\n        case LLM_ARCH_STABLELM:\n            {\n                GGUF_GET_KEY(ctx, hparams.f_norm_eps, gguf_get_val_f32, GGUF_TYPE_FLOAT32, true, kv(LLM_KV_ATTENTION_LAYERNORM_EPS));\n\n                switch (hparams.n_layer) {\n                    case 32: model.type = e_model::MODEL_3B; break;\n                    default: model.type = e_model::MODEL_UNKNOWN;\n               }\n            } break;\n\n        default: (void)0;\n    }\n\n    model.ftype = ml.ftype;\n}\n\n// TODO: This should probably be in llama.h\nstatic std::vector<llama_vocab::id> llama_tokenize_internal(const llama_vocab & vocab, std::string raw_text, bool bos, bool special = false);\nstatic llama_token llama_byte_to_token(const llama_vocab & vocab, uint8_t ch);\n\nstatic void llm_load_vocab(\n        llama_model_loader & ml,\n        llama_model & model) {\n    auto & vocab = model.vocab;\n\n    struct gguf_context * ctx = ml.ctx_gguf;\n\n    const auto kv = LLM_KV(model.arch);\n\n    const int token_idx = gguf_find_key(ctx, kv(LLM_KV_TOKENIZER_LIST).c_str());\n    if (token_idx == -1) {\n        throw std::runtime_error(\"cannot find tokenizer vocab in model file\\n\");\n    }\n\n    const float * scores = nullptr;\n    const int score_idx = gguf_find_key(ctx, kv(LLM_KV_TOKENIZER_SCORES).c_str());\n    if (score_idx != -1) {\n        scores = (const float * ) gguf_get_arr_data(ctx, score_idx);\n    }\n\n    const int * toktypes = nullptr;\n    const int toktype_idx = gguf_find_key(ctx, kv(LLM_KV_TOKENIZER_TOKEN_TYPE).c_str());\n    if (toktype_idx != -1) {\n        toktypes = (const int * ) gguf_get_arr_data(ctx, toktype_idx);\n    }\n\n    // determine vocab type\n    {\n        std::string tokenizer_name;\n\n        GGUF_GET_KEY(ctx, tokenizer_name, gguf_get_val_str, GGUF_TYPE_STRING, true, kv(LLM_KV_TOKENIZER_MODEL));\n\n        if (tokenizer_name == \"llama\") {\n            vocab.type = LLAMA_VOCAB_TYPE_SPM;\n\n            // default special tokens\n            vocab.special_bos_id = 1;\n            vocab.special_eos_id = 2;\n            vocab.special_unk_id = 0;\n            vocab.special_sep_id = -1;\n            vocab.special_pad_id = -1;\n        } else if (tokenizer_name == \"gpt2\") {\n            vocab.type = LLAMA_VOCAB_TYPE_BPE;\n\n            // read bpe merges and populate bpe ranks\n            const int merges_keyidx = gguf_find_key(ctx, kv(LLM_KV_TOKENIZER_MERGES).c_str());\n            if (merges_keyidx == -1) {\n                throw std::runtime_error(\"cannot find tokenizer merges in model file\\n\");\n            }\n\n            const int n_merges = gguf_get_arr_n(ctx, merges_keyidx);\n\n            for (int i = 0; i < n_merges; i++) {\n                const std::string word = gguf_get_arr_str(ctx, merges_keyidx, i);\n                GGML_ASSERT(codepoints_from_utf8(word).size() > 0);\n\n                std::string first;\n                std::string second;\n\n                const size_t pos = word.find(' ', 1);\n\n                if (pos != std::string::npos) {\n                    first  = word.substr(0, pos);\n                    second = word.substr(pos + 1);\n                }\n\n                vocab.bpe_ranks.emplace(std::make_pair(first, second), i);\n            }\n\n            // default special tokens\n            vocab.special_bos_id = 11;\n            vocab.special_eos_id = 11;\n            vocab.special_unk_id = -1;\n            vocab.special_sep_id = -1;\n            vocab.special_pad_id = -1;\n        } else {\n            LLAMA_LOG_WARN(\"%s: unknown tokenizer: '%s'\", __func__, tokenizer_name.c_str());\n            LLAMA_LOG_WARN(\"%s: using default tokenizer: 'llama'\", __func__);\n\n            vocab.type = LLAMA_VOCAB_TYPE_SPM;\n        }\n    }\n\n    const uint32_t n_vocab = gguf_get_arr_n(ctx, token_idx);\n\n    vocab.id_to_token.resize(n_vocab);\n\n    for (uint32_t i = 0; i < n_vocab; i++) {\n        std::string word = gguf_get_arr_str(ctx, token_idx, i);\n        GGML_ASSERT(codepoints_from_utf8(word).size() > 0);\n\n        vocab.token_to_id[word] = i;\n\n        auto & token_data = vocab.id_to_token[i];\n        token_data.text  = std::move(word);\n        token_data.score = scores ? scores[i] : 0.0f;\n        token_data.type  = toktypes ? (llama_token_type) toktypes[i] : LLAMA_TOKEN_TYPE_NORMAL;\n    }\n    GGML_ASSERT(vocab.id_to_token.size() == vocab.token_to_id.size());\n\n    // determine the newline token: LLaMA \"<0x0A>\" == 10 == '\\n', Falcon 193 == '\\n'\n    if (vocab.type == LLAMA_VOCAB_TYPE_SPM) {\n        vocab.linefeed_id = llama_byte_to_token(vocab, '\\n');\n    } else {\n        const std::vector<int> ids = llama_tokenize_internal(vocab, \"\\u010A\", false);\n        GGML_ASSERT(!ids.empty() && \"model vocab missing newline token\");\n        vocab.linefeed_id = ids[0];\n    }\n\n    // special tokens\n    {\n        const std::vector<std::pair<enum llm_kv, int32_t &>> special_token_types = {\n            { LLM_KV_TOKENIZER_BOS_ID, vocab.special_bos_id },\n            { LLM_KV_TOKENIZER_EOS_ID, vocab.special_eos_id },\n            { LLM_KV_TOKENIZER_UNK_ID, vocab.special_unk_id },\n            { LLM_KV_TOKENIZER_SEP_ID, vocab.special_sep_id },\n            { LLM_KV_TOKENIZER_PAD_ID, vocab.special_pad_id },\n        };\n        for (const auto & it : special_token_types) {\n            const std::string & key = kv(std::get<0>(it));\n            int32_t & id = std::get<1>(it), old_id = id;\n\n            GGUF_GET_KEY(ctx, id, gguf_get_val_u32, GGUF_TYPE_UINT32, false, key);\n            // Must be >= -1 and < vocab size. Since the key is unsigned, -1\n            // can only come from the default value, so there's no point in\n            // validating that.\n            if (size_t(id + 1) > vocab.id_to_token.size()) {\n                LLAMA_LOG_WARN(\"%s: bad special token: '%s' = %d, using default id %d\\n\",\n                    __func__, key.c_str(), id, old_id);\n                id = old_id;\n            }\n        }\n    }\n\n    // build special tokens cache\n    {\n        // TODO: It is unclear (to me) at this point, whether special tokes are guaranteed to be of a deterministic type,\n        //  and will always be correctly labeled in 'added_tokens.json' etc.\n        // The assumption is, since special tokens aren't meant to be exposed to end user, they are designed\n        //  to be unmatchable by the tokenizer, therefore tokens from the vocab, which are unmatchable by the tokenizer\n        //  are special tokens.\n        // From testing, this appears to corelate 1:1 with special tokens.\n        //\n\n        // Counting special tokens and verifying in only one direction\n        //  is sufficient to detect difference in those two sets.\n        //\n        uint32_t special_tokens_count_by_type = 0;\n        uint32_t special_tokens_count_from_verification = 0;\n\n        bool special_tokens_definition_mismatch = false;\n\n        for (const auto & t : vocab.token_to_id) {\n            const auto & token = t.first;\n            const auto & id    = t.second;\n\n            // Count all non-normal tokens in the vocab while iterating\n            if (vocab.id_to_token[id].type != LLAMA_TOKEN_TYPE_NORMAL) {\n                special_tokens_count_by_type++;\n            }\n\n            // Skip single character tokens\n            if (token.length() > 1) {\n                bool is_tokenizable = false;\n\n                // Split token string representation in two, in all possible ways\n                //  and check if both halves can be matched to a valid token\n                for (unsigned i = 1; i < token.length();) {\n                    const auto left  = token.substr(0, i);\n                    const auto right = token.substr(i);\n\n                    // check if we didnt partition in the middle of a utf sequence\n                    auto utf = utf8_len(left.at(left.length() - 1));\n\n                    if (utf == 1) {\n                        if (vocab.token_to_id.find(left)  != vocab.token_to_id.end() &&\n                            vocab.token_to_id.find(right) != vocab.token_to_id.end() ) {\n                            is_tokenizable = true;\n                            break;\n                        }\n                        i++;\n                    } else {\n                        // skip over the rest of multibyte utf sequence\n                        i += utf - 1;\n                    }\n                }\n\n                if (!is_tokenizable) {\n                    // Some tokens are multibyte, but they are utf sequences with equivalent text length of 1\n                    //  it's faster to re-filter them here, since there are way less candidates now\n\n                    // Calculate a total \"utf\" length of a token string representation\n                    size_t utf8_str_len = 0;\n                    for (unsigned i = 0; i < token.length();) {\n                        utf8_str_len++;\n                        i += utf8_len(token.at(i));\n                    }\n\n                    // And skip the ones which are one character\n                    if (utf8_str_len > 1) {\n                        // At this point what we have left are special tokens only\n                        vocab.special_tokens_cache[token] = id;\n\n                        // Count manually found special tokens\n                        special_tokens_count_from_verification++;\n\n                        // If this manually found special token is not marked as such, flag a mismatch\n                        if (vocab.id_to_token[id].type == LLAMA_TOKEN_TYPE_NORMAL) {\n                            special_tokens_definition_mismatch = true;\n                        }\n                    }\n                }\n            }\n        }\n\n        if (special_tokens_definition_mismatch || special_tokens_count_from_verification != special_tokens_count_by_type) {\n            LLAMA_LOG_WARN(\"%s: mismatch in special tokens definition ( %u/%zu vs %u/%zu ).\\n\",\n                __func__,\n                special_tokens_count_from_verification, vocab.id_to_token.size(),\n                special_tokens_count_by_type, vocab.id_to_token.size()\n            );\n        } else {\n            LLAMA_LOG_INFO(\"%s: special tokens definition check successful ( %u/%zu ).\\n\",\n                __func__,\n                special_tokens_count_from_verification, vocab.id_to_token.size()\n            );\n        }\n    }\n}\n\nstatic void llm_load_print_meta(llama_model_loader & ml, llama_model & model) {\n    const auto & hparams = model.hparams;\n    const auto & vocab   = model.vocab;\n\n    const auto rope_scaling_type = LLAMA_ROPE_SCALING_TYPES.at(hparams.rope_scaling_type_train);\n\n    // hparams\n    LLAMA_LOG_INFO(\"%s: format           = %s\\n\",     __func__, llama_file_version_name(ml.fver));\n    LLAMA_LOG_INFO(\"%s: arch             = %s\\n\",     __func__, LLM_ARCH_NAMES.at(model.arch).c_str());\n    LLAMA_LOG_INFO(\"%s: vocab type       = %s\\n\",     __func__, vocab.type == LLAMA_VOCAB_TYPE_SPM ? \"SPM\" : \"BPE\"); // TODO: fix\n    LLAMA_LOG_INFO(\"%s: n_vocab          = %u\\n\",     __func__, hparams.n_vocab);\n    LLAMA_LOG_INFO(\"%s: n_merges         = %u\\n\",     __func__, (int) vocab.bpe_ranks.size());\n    LLAMA_LOG_INFO(\"%s: n_ctx_train      = %u\\n\",     __func__, hparams.n_ctx_train);\n    LLAMA_LOG_INFO(\"%s: n_embd           = %u\\n\",     __func__, hparams.n_embd);\n    LLAMA_LOG_INFO(\"%s: n_head           = %u\\n\",     __func__, hparams.n_head);\n    LLAMA_LOG_INFO(\"%s: n_head_kv        = %u\\n\",     __func__, hparams.n_head_kv);\n    LLAMA_LOG_INFO(\"%s: n_layer          = %u\\n\",     __func__, hparams.n_layer);\n    LLAMA_LOG_INFO(\"%s: n_rot            = %u\\n\",     __func__, hparams.n_rot); // a.k.a. n_embd_head, n_head_dim\n    LLAMA_LOG_INFO(\"%s: n_gqa            = %u\\n\",     __func__, hparams.n_gqa());\n    LLAMA_LOG_INFO(\"%s: f_norm_eps       = %.1e\\n\",   __func__, hparams.f_norm_eps);\n    LLAMA_LOG_INFO(\"%s: f_norm_rms_eps   = %.1e\\n\",   __func__, hparams.f_norm_rms_eps);\n    LLAMA_LOG_INFO(\"%s: f_clamp_kqv      = %.1e\\n\",   __func__, hparams.f_clamp_kqv);\n    LLAMA_LOG_INFO(\"%s: f_max_alibi_bias = %.1e\\n\",   __func__, hparams.f_max_alibi_bias);\n    LLAMA_LOG_INFO(\"%s: n_ff             = %u\\n\",     __func__, hparams.n_ff);\n    LLAMA_LOG_INFO(\"%s: rope scaling     = %s\\n\",     __func__, rope_scaling_type.c_str());\n    LLAMA_LOG_INFO(\"%s: freq_base_train  = %.1f\\n\",   __func__, hparams.rope_freq_base_train);\n    LLAMA_LOG_INFO(\"%s: freq_scale_train = %g\\n\",     __func__, hparams.rope_freq_scale_train);\n    LLAMA_LOG_INFO(\"%s: n_yarn_orig_ctx  = %u\\n\",     __func__, hparams.n_yarn_orig_ctx);\n    LLAMA_LOG_INFO(\"%s: rope_finetuned   = %s\\n\",     __func__, hparams.rope_finetuned ? \"yes\" : \"unknown\");\n    LLAMA_LOG_INFO(\"%s: model type       = %s\\n\",     __func__, llama_model_type_name(model.type));\n    LLAMA_LOG_INFO(\"%s: model ftype      = %s\\n\",     __func__, llama_model_ftype_name(model.ftype).c_str());\n    LLAMA_LOG_INFO(\"%s: model params     = %.2f B\\n\", __func__, ml.n_elements*1e-9);\n    if (ml.n_bytes < GB) {\n        LLAMA_LOG_INFO(\"%s: model size       = %.2f MiB (%.2f BPW) \\n\", __func__, ml.n_bytes/1024.0/1024.0, ml.n_bytes*8.0/ml.n_elements);\n    } else {\n        LLAMA_LOG_INFO(\"%s: model size       = %.2f GiB (%.2f BPW) \\n\", __func__, ml.n_bytes/1024.0/1024.0/1024.0, ml.n_bytes*8.0/ml.n_elements);\n    }\n\n    // general kv\n    LLAMA_LOG_INFO(\"%s: general.name   = %s\\n\",    __func__, model.name.c_str());\n\n    // special tokens\n    if (vocab.special_bos_id != -1) { LLAMA_LOG_INFO( \"%s: BOS token = %d '%s'\\n\", __func__, vocab.special_bos_id, vocab.id_to_token[vocab.special_bos_id].text.c_str() ); }\n    if (vocab.special_eos_id != -1) { LLAMA_LOG_INFO( \"%s: EOS token = %d '%s'\\n\", __func__, vocab.special_eos_id, vocab.id_to_token[vocab.special_eos_id].text.c_str() ); }\n    if (vocab.special_unk_id != -1) { LLAMA_LOG_INFO( \"%s: UNK token = %d '%s'\\n\", __func__, vocab.special_unk_id, vocab.id_to_token[vocab.special_unk_id].text.c_str() ); }\n    if (vocab.special_sep_id != -1) { LLAMA_LOG_INFO( \"%s: SEP token = %d '%s'\\n\", __func__, vocab.special_sep_id, vocab.id_to_token[vocab.special_sep_id].text.c_str() ); }\n    if (vocab.special_pad_id != -1) { LLAMA_LOG_INFO( \"%s: PAD token = %d '%s'\\n\", __func__, vocab.special_pad_id, vocab.id_to_token[vocab.special_pad_id].text.c_str() ); }\n    if (vocab.linefeed_id    != -1) { LLAMA_LOG_INFO( \"%s: LF token  = %d '%s'\\n\", __func__, vocab.linefeed_id,    vocab.id_to_token[vocab.linefeed_id].text.c_str() );    }\n\n    // sparse inference\n    LLAMA_LOG_INFO(\"%s: sparse_pred_threshold = %.2f\\n\", __func__, hparams.sparse_pred_threshold);\n}\n\n\nstatic int64_t sum_gpu_index(struct ggml_tensor * gpu_index) {\n    ggml_context * ctx_aux = ggml_init({\n        /* mem_size */ 1 << 10,\n    });\n\n    GGML_ASSERT(ctx_aux);\n\n    ggml_cgraph * gf = ggml_new_graph_custom(ctx_aux, 1, false);\n    ggml_tensor * sum = ggml_sum(ctx_aux, gpu_index);\n\n    ggml_set_name(sum, \"gpu_index_sum\");\n    ggml_build_forward_expand(gf, sum);\n\n    // TODO: +1 worker for GPU under hybrid inference but no use\n    // ggml_graph_compute_helper(work_buffer, gf, 2);\n    ggml_graph_compute_with_ctx(ctx_aux, gf, 2);\n\n    int32_t sum_val = ggml_get_i32_1d(sum, 0);\n\n    ggml_free(ctx_aux);\n\n    return sum_val;\n}\n\nstruct llama_gpu_split_loader {\n    int n_tensors = 0;\n    size_t n_bytes = 0; // tensor data bytes\n\n    const std::string fname;\n    int fver;\n\n    bool use_mmap = false; // only supports mmap yet\n    std::unique_ptr<llama_mmap> mapping;\n    struct ggml_context * ctx_meta = nullptr;\n\n    llama_model_loader * idx_loader;\n    size_t vram_required = 0;\n\n    llama_gpu_split_loader(const std::string & fname, bool use_mmap) : fname(fname), use_mmap(use_mmap) {\n        GGML_ASSERT(use_mmap);\n\n        idx_loader = new llama_model_loader(fname, use_mmap);\n        GGUF_GET_KEY(idx_loader->ctx_gguf, vram_required, gguf_get_val_u64, GGUF_TYPE_UINT64, true, LLM_KV_NAMES[LLM_KV_SPLIT_VRAM_CAPACITY]);\n        printf(\"loaded gpu_idx, vram_required: %ld\\n\", vram_required);\n\n        n_tensors = idx_loader->n_tensors;\n\n        // allocate memadata/data for mlp tensors\n        // TODO: support allocating buffer for tensor data (when mmap is not used)\n        size_t per_tensor_meta_size = GGML_PAD(sizeof(struct ggml_tensor), GGML_MEM_ALIGN) + GGML_OBJECT_SIZE;\n        size_t tensor_meta_size = n_tensors * per_tensor_meta_size;\n        struct ggml_init_params params = {\n            /*.mem_size   =*/ tensor_meta_size,\n            /*.mem_buffer =*/ nullptr,\n            /*.no_alloc   =*/ true,\n        };\n        ctx_meta = ggml_init(params);\n    }\n\n    bool check_vram_allocable(size_t vram_budget) {\n        return vram_budget >= vram_required;\n    }\n\n    int load_gpu_idx_for_model(llama_model * model) {\n        int n_layers = model->layers.size();\n        // TODO: assert fp is at the end of headers\n        if (n_tensors != n_layers * 2) {\n           LLAMA_LOG_ERROR(\"%s: error: the number of gpu splits does not match the layer of model\\n\", __func__);\n            return 1;\n        }\n        LLAMA_LOG_INFO(\"%s: applying gpu_idx adapter from '%s' - please wait ...\\n\", __func__, fname.c_str());\n        const int64_t t_start_mlp_us = ggml_time_us();\n\n        for (int il = 0; il < n_layers; il++) {\n            llama_layer &model_layer = model->layers[il];\n            ggml_tensor * gpu_idx = idx_loader->get_tensor_meta(il*2);\n            ggml_tensor * gpu_bucket = idx_loader->get_tensor_meta(il*2+1);\n            if (gpu_idx == nullptr || gpu_bucket == nullptr) {\n                LLAMA_LOG_ERROR(\"%s: error: failed to load gpu index or bucket\\n\", __func__);\n                return 1;\n            }\n            model_layer.gpu_idx = idx_loader->create_tensor_for(ctx_meta, gpu_idx, GGML_BACKEND_CPU);\n            model_layer.gpu_bucket = idx_loader->create_tensor_for(ctx_meta, gpu_bucket, GGML_BACKEND_CPU);\n        }\n        llama_progress_callback cb = [](float progress, void *ctx) {\n            LLAMA_LOG_INFO(\".\");\n        };\n        idx_loader->load_all_data(ctx_meta, cb, nullptr, nullptr);\n\n        for (int il = 0; il < n_layers; il++) {\n            llama_layer &model_layer = model->layers[il];\n            ggml_tensor * gpu_idx = model_layer.gpu_idx;\n            ggml_tensor * gpu_bucket = model_layer.gpu_bucket;\n            int64_t gpu_neurons = sum_gpu_index(gpu_idx);\n            model_layer.gpu_offload_ratio = (double)gpu_neurons / gpu_idx->ne[0];\n            if (gpu_neurons == 0 || gpu_neurons == gpu_idx->ne[0]) {\n                // no hybrid inference for this layer, unset gpu_bucket\n                model_layer.gpu_bucket = NULL;\n                // TODO: maybe can also unset gpu_idx\n            } else {\n#if defined(GGML_USE_CUBLAS)\n                ggml_set_backend(gpu_bucket, GGML_BACKEND_GPU);\n                ggml_cuda_transform_tensor(gpu_bucket->data, gpu_bucket);\n#else\n                GGML_ASSERT(false && \"cublas is not enabled\");\n#endif\n            }\n        }\n\n        const int64_t t_mlp_us = ggml_time_us() - t_start_mlp_us;\n        LLAMA_LOG_INFO(\" done (%.2f ms)\\n\", t_mlp_us / 1000.0);\n\n        return 0;\n    }\n};\n\n// to dynamically load/transform llama model weights\nstruct llama_augmentation_model_loader {\n    struct ggml_context * aux_ctx = nullptr;\n\n    llama_augmentation_model_loader(llama_model *model) {\n        // TODO: check precondition - MLP loaded\n\n        // check augmentation fields to load\n        // 1. gpu_idx;\n        // 2. gpu_bucket;\n        // 3. transformed ffn_down;\n        // const int64_t ggml_aux_tensor_size = 4 * (100 * 100 + 5120*40*4 * ggml_tensor_overhead() + (int64_t)13824*5120*40*4);\n        int model_layer = model->layers.size();\n        int ffn_dim = model->layers[0].ffn_up->ne[1];\n        const size_t ggml_aux_tensor_size = 4 * (model_layer*ffn_dim*sizeof(float)*2+ model_layer*ffn_dim*sizeof(float) * ggml_tensor_overhead() );\n\n        struct ggml_init_params params = {\n            /*.mem_size   =*/ ggml_aux_tensor_size,\n            /*.mem_buffer =*/ nullptr,\n            /*.no_alloc   =*/ false,\n        };\n        aux_ctx = ggml_init(params);\n    }\n\n        // allocate and copy selected weights to gpu\n    ggml_tensor * create_striped_mat_to_gpu(struct ggml_tensor *src, struct ggml_tensor * gpu_bucket) {\n#ifdef GGML_USE_CUBLAS\n        if (gpu_bucket == NULL) {\n            // offload the whole tensor to gpu\n            ggml_set_backend(src, GGML_BACKEND_GPU);\n            ggml_cuda_transform_tensor(src->data, src);\n            return src;\n        }\n\n        int64_t row_len = src->ne[0];\n        int64_t gpu_rows = gpu_bucket->ne[0];\n        GGML_ASSERT(0 < gpu_rows && gpu_rows <= src->ne[1]);\n\n        ggml_set_no_alloc(aux_ctx, true);\n        ggml_tensor * gpu_dst = ggml_new_tensor_2d(aux_ctx, src->type, row_len, gpu_rows);\n        ggml_set_backend(gpu_dst, GGML_BACKEND_GPU);\n        ggml_cuda_alloc_tensor(gpu_dst);\n\n        // init two 1d views on host and device\n        ggml_tensor * host_mat_row = ggml_new_tensor_1d(aux_ctx, src->type, row_len);\n        static ggml_tensor * device_mat_row = ggml_dup_tensor(aux_ctx, host_mat_row);\n        ggml_set_backend(device_mat_row, GGML_BACKEND_GPU);\n        ggml_cuda_alloc_tensor(device_mat_row);\n        *ggml_cuda_get_data_pp(device_mat_row) = *ggml_cuda_get_data_pp(gpu_dst);\n\n        // read raw data and copy to device depending on gpu_idx\n        const enum ggml_type type = src->type;\n        const int ne0 = src->ne[0];\n        const size_t row_data_size = ne0*ggml_type_size(type)/ggml_blck_size(type);\n        for (int i = 0; i < gpu_rows; i++) {\n            int32_t host_i = ((int32_t *)gpu_bucket->data)[i];\n            host_mat_row -> data = (char *)(src -> data) + host_i * row_data_size;\n            char ** gpu_data_pp = reinterpret_cast<char **>(ggml_cuda_get_data_pp(device_mat_row));\n            // printf(\"gpu_data_p: %p\\n\", *gpu_data_pp);\n            ggml_cuda_cpy_1d(device_mat_row, host_mat_row);\n            *gpu_data_pp = *gpu_data_pp + row_data_size;\n        }\n        ggml_set_no_alloc(aux_ctx, false);\n\n        return gpu_dst;\n#else\n        return NULL;\n#endif\n    }\n\n    size_t slice_ffn_mat_to_gpu(llama_layer & layer) {\n        std::vector<uint8_t> work_buffer;\n        ggml_tensor * gpu_idx = layer.gpu_idx;\n        ggml_tensor * gpu_bucket = layer.gpu_bucket;\n        size_t offloaded_bytes = 0;\n\n        if (layer.gpu_offload_ratio == 0.) {\n            return 0;\n        }\n\n        GGML_ASSERT((layer.gpu_bucket != NULL) == (layer.gpu_offload_ratio < 1.0));\n\n        if (layer.ffn_gate) {\n            layer.ffn_gate_gpu = create_striped_mat_to_gpu(layer.ffn_gate, gpu_bucket);\n            offloaded_bytes += ggml_nbytes(layer.ffn_gate_gpu);\n        }\n        \n        layer.ffn_up_gpu = create_striped_mat_to_gpu(layer.ffn_up, gpu_bucket);\n        offloaded_bytes += ggml_nbytes(layer.ffn_up_gpu);\n        \n        layer.ffn_down_gpu = create_striped_mat_to_gpu(layer.ffn_down_t, gpu_bucket);\n        offloaded_bytes += ggml_nbytes(layer.ffn_down_gpu);\n\n        return offloaded_bytes;\n    }\n\n    size_t offload_ffn_split(llama_model * model) {\n        LLAMA_LOG_INFO(\"%s: applying augmentation to model - please wait ...\\n\", __func__);\n        const int64_t t_start_aug_us = ggml_time_us();\n        std::vector<uint8_t> work_buffer;\n\n        // Set sparsity threshold via global virables\n        sparse_pred_threshold = model->hparams.sparse_pred_threshold;\n#if defined (GGML_USE_CUBLAS)\n        ggml_cuda_set_device_constants(model->hparams.sparse_pred_threshold);\n#endif\n\n        // load gpu_idx and slice mat to gpu\n        size_t offloaded_bytes = 0;\n        for (llama_layer &model_layer : model -> layers) {\n            // gpu_idx load\n            if (model_layer.gpu_idx == NULL && model_layer.gpu_bucket == NULL) {\n                ggml_tensor * gpu_idx = ggml_new_tensor_1d(aux_ctx, GGML_TYPE_I32, model_layer.mlp_pre_w2 -> ne[1]);\n                ggml_set_zero(gpu_idx);\n                model_layer.gpu_idx = gpu_idx;\n                ggml_tensor * gpu_bucket = ggml_new_tensor_1d(aux_ctx, GGML_TYPE_I32, 0);\n                model_layer.gpu_bucket = gpu_bucket;\n            }\n            offloaded_bytes += slice_ffn_mat_to_gpu(model_layer);\n            LLAMA_LOG_INFO(\".\");\n        }\n\n        LLAMA_LOG_INFO(\" done (%.2f ms)\\n\", (ggml_time_us() - t_start_aug_us) / 1000.0);\n        return offloaded_bytes;\n    }\n};\n\nstruct buffered_tensor_allocator {\n    llama_model_loader &ml;\n    ggml_context *ctx;\n    std::map<tensor_offloading_levels, std::vector<std::tuple<int, llm_tensor, ggml_tensor *>>> alloc_queues;\n    const llama_hparams & hparams;\n    size_t vram_allocated_bytes = 0;\n    int offloaded_layers = 0; // mocks the model's n_gpu_layers\n    bool tensor_offload_complete = false;\n\n    buffered_tensor_allocator(llama_model_loader &ml, ggml_context *ctx, const llama_hparams &hparams) : ml(ml), ctx(ctx), hparams(hparams) {}\n\n    ggml_tensor * buffered_alloc(const std::string & name, const llm_tensor tensor_type, const std::vector<int64_t> & ne, const int i_layer) {\n#if defined(GGML_USE_CUBLAS)\n        tensor_offloading_levels level = get_offloading_level(tensor_type);\n        if (level == TENSOR_NO_OFFLOAD || level == TENSOR_OFFLOAD_FFN) {\n            return ml.create_tensor(ctx, name, ne, GGML_BACKEND_CPU);\n        }\n        // Alloc only metadata for GPU tensors\n        bool no_alloc = ctx->no_alloc;\n        ggml_set_no_alloc(ctx, true);\n        ggml_tensor * meta_tensor = ml.create_tensor(ctx, name, ne, GGML_BACKEND_CPU);\n        ggml_set_no_alloc(ctx, no_alloc);\n        alloc_queues[level].push_back(std::make_tuple(i_layer, tensor_type, meta_tensor));\n        return meta_tensor;\n#else\n        return ml.create_tensor(ctx, name, ne, GGML_BACKEND_CPU);\n#endif\n    }\n\n    bool offload_tensor(ggml_tensor * meta_tensor) {\n        size_t tensor_data_size = ggml_nbytes(meta_tensor);\n        if (!llama_reduce_vram_budget(tensor_data_size)) {\n            return false;\n        }\n        // allocate in VRAM\n        ggml_set_backend(meta_tensor, GGML_BACKEND_GPU);\n        vram_allocated_bytes += tensor_data_size;\n        return true;\n    }\n\n    bool reserve(size_t tensor_bytes) {\n        return llama_reduce_vram_budget(tensor_bytes);\n    }\n\n    // For GPU tensors, we need to allocate them in VRAM as much as possible,\n    // and update the tensor data in-place. If the VRAM budget is exceeded,\n    // we allocate the tensor in CPU memory.\n    // Returns: equivalent of the model's n_gpu_layers\n    int flush() {\n#if defined(GGML_USE_CUBLAS)\n        if (!ggml_cublas_loaded()) {\n            return 0;\n        }\n        \n        // iterate over offloading priorities\n        for (int enum_i = TENSOR_OFFLOAD_ATTN; enum_i <= TENSOR_OFFLOAD_OUTPUT; enum_i ++) {\n            tensor_offloading_levels level = static_cast<tensor_offloading_levels>(enum_i);\n            for (auto tensor_tup : alloc_queues[level]) {\n                const int i_layer = std::get<0>(tensor_tup);\n                const llm_tensor tensor_type = std::get<1>(tensor_tup);\n                ggml_tensor * meta_tensor = std::get<2>(tensor_tup);\n                if (!offload_tensor(meta_tensor)) {\n                    ml.done_getting_tensors();\n                    return offloaded_layers;\n                }\n\n                if (level == TENSOR_OFFLOAD_ATTN && tensor_type == LLM_TENSOR_ATTN_OUT) {\n                    offloaded_layers = i_layer + 1;\n                } else if (level == TENSOR_OFFLOAD_OUTPUT) {\n                    offloaded_layers = hparams.n_layer + 1; // indicate all layers + output are offloaded\n                }\n            }\n        }\n        ml.done_getting_tensors();\n        tensor_offload_complete = true;\n        return offloaded_layers;\n#else // GGML_USE_CUBLAS\n        return 0;\n#endif\n    }\n};\n\nstatic bool load_gpu_split_from_split_file(llama_model & model, std::string split_path, size_t vram_budget) {\n    llama_gpu_split_loader loader(split_path, true);\n    return loader.check_vram_allocable(vram_budget) \n        && loader.load_gpu_idx_for_model(&model) == 0;\n}\n\nstatic bool llm_load_gpu_split_with_budget(llama_model_loader & ml, llama_model & model, size_t vram_allocatable_bytes, bool no_cache) {\n    std::string cached_split_path = ml.file.fname + \".generated.gpuidx\";\n    std::string model_basedir = ml.file.get_basedir();\n\n    // Load GPU split from previously generated cache\n    if (access(cached_split_path.c_str(), F_OK) == 0 && !no_cache) {\n        if (load_gpu_split_from_split_file(model, cached_split_path, vram_allocatable_bytes)) {\n            return true;\n        }\n        LLAMA_LOG_ERROR(\"%s: error: failed to apply previously generated gpu split from '%s'\\n\", __func__, cached_split_path.c_str());\n    }\n\n    // Generate GPU split\n    std::string activation_path = std::string(model_basedir);\n#if defined (_WIN32)\n    activation_path += \"\\\\activation\";\n#else\n    activation_path += \"/activation\";\n#endif\n    if (access(activation_path.c_str(), F_OK) != 0) {\n        LLAMA_LOG_ERROR(\"%s: error: activation files under '%s' not found\\n\", __func__, activation_path.c_str());\n        return false;\n    }\n\n    // Calculate solver parameters\n    ggml_tensor * ffn_up = model.layers[0].ffn_up;\n    ggml_tensor * ffn_gate = model.layers[0].ffn_gate;\n    int slice_size = ffn_up->ne[1] * ggml_type_size(ffn_up->type) / ggml_blck_size(ffn_up->type);\n    // For model arch with FFN gate, the gate is also sliced, otherwise only the up and down matrices are sliced\n    int vram_bytes_per_slice = slice_size * (ffn_gate ? 4.5 : 2); // TODO: why 4.5, not 3?\n    int neuron_cap = floor((double)vram_allocatable_bytes / vram_bytes_per_slice) * 4;\n\n    LLAMA_LOG_INFO(\"invoking powerinfer Python module to generate gpu split for %.2f MiB of VRAM\\n\", vram_allocatable_bytes / 1024.0 / 1024.0);\n\n    std::stringstream command_ss;\n#if defined (_WIN32)\n    command_ss << \"python -m powerinfer\"\n#else\n    command_ss << \"python3 -m powerinfer\"\n#endif\n               << \" --activation \" << activation_path\n               << \" --layer \" << model.hparams.n_layer\n               << \" --neuron \" << ffn_up->ne[1]\n               << \" --capacity \" << neuron_cap\n               << \" --vram-capacity \" << vram_allocatable_bytes\n               << \" --output \" << cached_split_path;\n    if (system(command_ss.str().c_str()) != 0 || access(cached_split_path.c_str(), F_OK) != 0) {\n        LLAMA_LOG_ERROR(\"%s: error: failed to generate gpu split\\n\", __func__);\n        return false;\n    }\n\n    return load_gpu_split_from_split_file(model, cached_split_path, vram_allocatable_bytes);\n}\n\nstatic size_t llm_load_gpu_split(llama_model_loader & ml, llama_model & model, bool no_cache, bool no_offload) {\n#if defined (GGML_USE_CUBLAS)\n    if (!ggml_cublas_loaded()) {\n        throw std::runtime_error(format(\"cannot offload to GPU: \" GGML_CUDA_NAME \" not loaded\"));\n    }\n    if (!no_offload && !llm_load_gpu_split_with_budget(ml, model, vram_budget_bytes, no_cache)) {\n        LLAMA_LOG_ERROR(\"%s: error: failed to generate gpu split, an empty one will be used\\n\", __func__);\n    }\n#endif\n\n    // Apply GPU index and split FFNs to GPU\n    size_t ffn_offloaded_bytes = llama_model_offload_ffn_split(&model);\n    LLAMA_LOG_INFO(\"%s: offloaded %.2f MiB of FFN weights to GPU\\n\", __func__, ffn_offloaded_bytes / 1024.0 / 1024.0);\n\n    return ffn_offloaded_bytes;\n}\n\nstatic void llm_load_sparse_model_tensors(\n        llama_model_loader & ml,\n        llama_model & model,\n        const llama_context_params * cparams,\n        int main_gpu,\n        long int vram_budget_bytes,\n        bool reset_gpu_index,\n        bool disable_ffn_split,\n        bool use_mlock,\n        llama_progress_callback progress_callback,\n        void * progress_callback_user_data) {\n    model.t_start_us = ggml_time_us();\n    auto & ctx     = model.ctx;\n    auto & hparams = model.hparams;\n\n    size_t ctx_size;\n    size_t mmapped_size;\n    ml.calc_sizes(ctx_size, mmapped_size);\n    LLAMA_LOG_INFO(\"%s: ggml ctx size = %7.2f MB\\n\", __func__, ctx_size/1024.0/1024.0);\n\n    // create the ggml context\n    {\n        model.buf.resize(ctx_size);\n        if (use_mlock) {\n            model.mlock_buf.init   (model.buf.data);\n            model.mlock_buf.grow_to(model.buf.size);\n        }\n\n        struct ggml_init_params params = {\n            /*.mem_size   =*/ model.buf.size,\n            /*.mem_buffer =*/ model.buf.data,\n            /*.no_alloc   =*/ ml.use_mmap,\n        };\n\n        model.ctx = ggml_init(params);\n        if (!model.ctx) {\n            throw std::runtime_error(format(\"ggml_init() failed\"));\n        }\n    }\n\n    (void) main_gpu;\n\n    enum ggml_backend_type llama_backend_offload = GGML_BACKEND_CPU;\n    enum ggml_backend_type llama_backend_offload_split = GGML_BACKEND_CPU;\n\n#ifdef GGML_USE_CUBLAS\n    if (ggml_cublas_loaded()) {\n        LLAMA_LOG_INFO(\"%s: using \" GGML_CUDA_NAME \" for GPU acceleration\\n\", __func__);\n        ggml_cuda_set_main_device(main_gpu);\n\n        llama_backend_offload = GGML_BACKEND_GPU;\n        llama_backend_offload_split = GGML_BACKEND_GPU_SPLIT;\n    }\n#elif defined(GGML_USE_CLBLAST)\n        LLAMA_LOG_INFO(\"%s: using OpenCL for GPU acceleration\\n\", __func__);\n        llama_backend_offload = GGML_BACKEND_GPU;\n        llama_backend_offload_split = GGML_BACKEND_GPU;\n#endif\n\n    buffered_tensor_allocator alloc(ml, ctx, hparams);\n    uint32_t current_layer = 0;\n    auto create_tensor = [&alloc, &current_layer] (\n        const std::pair<std::string, llm_tensor> & tn, \n        const std::vector<int64_t> & ne) -> ggml_tensor * {\n        return alloc.buffered_alloc(tn.first, tn.second, ne, current_layer);\n    };\n\n    {\n        const int64_t n_embd     = hparams.n_embd;\n        const int64_t n_embd_gqa = hparams.n_embd_gqa();\n        const int64_t n_layer    = hparams.n_layer;\n        const int64_t n_vocab    = hparams.n_vocab;\n\n        const auto tn = LLM_TN(model.arch);\n        switch (model.arch) {\n            case LLM_ARCH_LLAMA:\n            case LLM_ARCH_REFACT:\n            case LLM_ARCH_BAMBOO:\n                {\n                    model.tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab});\n\n                    // output\n                    {\n                        model.output_norm = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd});\n                        model.output      = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab});\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t &i = current_layer; i < n_layer; ++i) {\n                       auto & layer = model.layers[i];\n\n                        layer.attn_norm = create_tensor(tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd});\n\n                        layer.wq = create_tensor(tn(LLM_TENSOR_ATTN_Q,   \"weight\", i), {n_embd, n_embd});\n                        layer.wk = create_tensor(tn(LLM_TENSOR_ATTN_K,   \"weight\", i), {n_embd, n_embd_gqa});\n                        layer.wv = create_tensor(tn(LLM_TENSOR_ATTN_V,   \"weight\", i), {n_embd, n_embd_gqa});\n                        layer.wo = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd});\n\n                        layer.ffn_norm = create_tensor(tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd});\n\n                        layer.ffn_gate = create_tensor(tn(LLM_TENSOR_FFN_GATE, \"weight\", i), {n_embd,   n_ff});\n                        layer.ffn_down_t = create_tensor(tn(LLM_TENSOR_FFN_DOWN_T, \"weight\", i), {n_embd, n_ff});\n                        layer.mlp_pre_w1 = create_tensor(tn(LLM_TENSOR_MLP_PRED_FC1, \"weight\", i), {n_embd, GGML_NE_WILDCARD});\n                        layer.mlp_pre_w2 = create_tensor(tn(LLM_TENSOR_MLP_PRED_FC2, \"weight\", i), {GGML_NE_WILDCARD, n_ff});\n                        layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff});\n                    }\n                } break;\n            case LLM_ARCH_FALCON:\n                {\n                    model.tok_embd = create_tensor(tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab});\n\n                    // output\n                    {\n                        model.output_norm   = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd});\n                        model.output_norm_b = create_tensor(tn(LLM_TENSOR_OUTPUT_NORM, \"bias\"),   {n_embd});\n                        model.output        = create_tensor(tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab});\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t &i = current_layer; i < n_layer; ++i) {\n                        auto & layer = model.layers[i];\n\n                        layer.attn_norm   = create_tensor(tn(LLM_TENSOR_ATTN_NORM,   \"weight\", i), {n_embd});\n                        layer.attn_norm_b = create_tensor(tn(LLM_TENSOR_ATTN_NORM,   \"bias\", i),   {n_embd});\n\n                        if (gguf_find_tensor(ml.ctx_gguf, tn(LLM_TENSOR_ATTN_NORM_2, \"weight\", i).first.c_str()) >= 0) {\n                            layer.attn_norm_2   = create_tensor(tn(LLM_TENSOR_ATTN_NORM_2, \"weight\", i), {n_embd});\n                            layer.attn_norm_2_b = create_tensor(tn(LLM_TENSOR_ATTN_NORM_2, \"bias\", i),   {n_embd});\n                        }\n\n                        layer.wqkv = create_tensor(tn(LLM_TENSOR_ATTN_QKV, \"weight\", i), {n_embd, n_embd + 2*n_embd_gqa});\n                        layer.wo   = create_tensor(tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd});\n                        layer.ffn_down_t = create_tensor(tn(LLM_TENSOR_FFN_DOWN_T, \"weight\", i), {n_embd, n_ff});\n                        layer.mlp_pre_w1 = create_tensor(tn(LLM_TENSOR_MLP_PRED_FC1, \"weight\", i), {n_embd, GGML_NE_WILDCARD});\n                        layer.mlp_pre_w2 = create_tensor(tn(LLM_TENSOR_MLP_PRED_FC2, \"weight\", i), {GGML_NE_WILDCARD, n_ff});\n                        layer.ffn_up   = create_tensor(tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff});\n                    }\n                } break;\n            default:\n                throw std::runtime_error(\"unknown architecture\");\n        }\n    }\n\n    model.n_gpu_layers = alloc.flush();\n    LLAMA_LOG_INFO(\"%s: offloaded layers from VRAM budget(%ld bytes): %d/%d\\n\", __func__, vram_budget_bytes, model.n_gpu_layers, hparams.n_layer);\n\n    // print memory requirements\n    {\n        // this is the total memory required to run the inference\n        size_t mem_required = ctx_size + mmapped_size;\n\n        LLAMA_LOG_INFO(\"%s: mem required  = %7.2f MB\\n\", __func__, mem_required / 1024.0 / 1024.0);\n\n#if defined(GGML_USE_CUBLAS) || defined(GGML_USE_CLBLAST)\n        LLAMA_LOG_INFO(\"%s: VRAM used: %.2f MB\\n\", __func__, alloc.vram_allocated_bytes / 1024.0 / 1024.0);\n#endif\n    }\n\n    // populate `tensors_by_name`\n    for (int i = 0; i < ml.n_tensors; ++i) {\n        struct ggml_tensor * cur = ggml_get_tensor(ctx, ml.get_tensor_name(i));\n        model.tensors_by_name.emplace_back(ggml_get_name(cur), cur);\n    }\n\n    ml.load_all_data(ctx, progress_callback, progress_callback_user_data, use_mlock ? &model.mlock_mmap : NULL);\n\n    if (progress_callback) {\n        progress_callback(1.0f, progress_callback_user_data);\n    }\n\n    model.mapping = std::move(ml.mapping);\n\n    // Reserve KV cache in VRAM\n    if (cparams != NULL) {\n        llama_reserve_model_kv_cache(&model, cparams);\n    }\n    // Offload FFN segments to GPU if possible\n    model.ffn_offloaded_bytes = llm_load_gpu_split(ml, model, reset_gpu_index, disable_ffn_split || !alloc.tensor_offload_complete);\n\n    // loading time will be recalculate after the first eval, so\n    // we take page faults deferred by mmap() into consideration\n    model.t_load_us = ggml_time_us() - model.t_start_us;\n}\n\nvoid llama_reserve_model_kv_cache(llama_model *model, const llama_context_params *cparams) {\n#if defined(GGML_USE_CUBLAS)\n    if (!ggml_cublas_loaded()) {\n        throw std::runtime_error(format(\"cannot offload to GPU: \" GGML_CUDA_NAME \" not loaded\"));\n    }\n\n    const llama_hparams &hparams = model->hparams;\n    if (model->n_gpu_layers < hparams.n_layer + 1) {\n        // should only reserve kv cache for models with all layers offloaded\n        return;\n    }\n\n    const uint32_t n_embd  = hparams.n_embd_gqa();\n    const uint32_t n_layer = hparams.n_layer;\n\n    const int64_t n_mem      = n_layer*cparams->n_ctx;\n    const int64_t n_elements = n_embd*n_mem;\n\n    const ggml_type wtype = cparams->f16_kv ? GGML_TYPE_F16 : GGML_TYPE_F32;\n    const size_t cache_size = n_elements*ggml_type_size(wtype);\n\n    // reserve for k cache and v cache\n    for (int i = 0; i < 2; i++) {\n        if (!llama_reduce_vram_budget(cache_size)) {\n            return;\n        }\n        model->n_gpu_layers++;\n    }\n#endif\n}\n\nstatic void llm_load_tensors(\n        llama_model_loader & ml,\n        llama_model & model,\n        int n_gpu_layers,\n        int main_gpu,\n        const float * tensor_split,\n        bool use_mlock,\n        llama_progress_callback progress_callback,\n        void * progress_callback_user_data) {\n    model.t_start_us = ggml_time_us();\n\n    auto & ctx     = model.ctx;\n    auto & hparams = model.hparams;\n\n    model.n_gpu_layers = n_gpu_layers;\n\n    size_t ctx_size;\n    size_t mmapped_size;\n\n    ml.calc_sizes(ctx_size, mmapped_size);\n\n    LLAMA_LOG_INFO(\"%s: ggml ctx size = %7.2f MB\\n\", __func__, ctx_size/1024.0/1024.0);\n\n    // create the ggml context\n    {\n        model.buf.resize(ctx_size);\n        if (use_mlock) {\n            model.mlock_buf.init   (model.buf.data);\n            model.mlock_buf.grow_to(model.buf.size);\n        }\n\n        struct ggml_init_params params = {\n            /*.mem_size   =*/ model.buf.size,\n            /*.mem_buffer =*/ model.buf.data,\n            /*.no_alloc   =*/ ml.use_mmap,\n        };\n\n        model.ctx = ggml_init(params);\n        if (!model.ctx) {\n            throw std::runtime_error(format(\"ggml_init() failed\"));\n        }\n    }\n\n    (void) main_gpu;\n\n    enum ggml_backend_type llama_backend_offload = GGML_BACKEND_CPU;\n    enum ggml_backend_type llama_backend_offload_split = GGML_BACKEND_CPU;\n\n#ifdef GGML_USE_CUBLAS\n    if (ggml_cublas_loaded()) {\n        LLAMA_LOG_INFO(\"%s: using \" GGML_CUDA_NAME \" for GPU acceleration\\n\", __func__);\n        ggml_cuda_set_main_device(main_gpu);\n\n        llama_backend_offload = GGML_BACKEND_GPU;\n        llama_backend_offload_split = GGML_BACKEND_GPU_SPLIT;\n    }\n#elif defined(GGML_USE_CLBLAST)\n        LLAMA_LOG_INFO(\"%s: using OpenCL for GPU acceleration\\n\", __func__);\n        llama_backend_offload = GGML_BACKEND_GPU;\n        llama_backend_offload_split = GGML_BACKEND_GPU;\n#endif\n\n    // prepare memory for the weights\n    size_t vram_weights = 0;\n    {\n        const int64_t n_embd     = hparams.n_embd;\n        const int64_t n_embd_gqa = hparams.n_embd_gqa();\n        const int64_t n_layer    = hparams.n_layer;\n        const int64_t n_vocab    = hparams.n_vocab;\n\n        const auto tn = LLM_TN(model.arch);\n        switch (model.arch) {\n            case LLM_ARCH_LLAMA:\n            case LLM_ARCH_REFACT:\n            case LLM_ARCH_BAMBOO:\n                {\n                    model.tok_embd = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, GGML_BACKEND_CPU);\n\n                    // output\n                    {\n                        ggml_backend_type backend_norm;\n                        ggml_backend_type backend_output;\n\n                        if (n_gpu_layers > int(n_layer)) {\n                            // norm is not performance relevant on its own but keeping it in VRAM reduces data copying\n                            // on Windows however this is detrimental unless everything is on the GPU\n#ifndef _WIN32\n                            backend_norm = llama_backend_offload;\n#else\n                            backend_norm = n_gpu_layers <= (int) n_layer + 2 ? GGML_BACKEND_CPU : llama_backend_offload;\n#endif // _WIN32\n\n                            backend_output = llama_backend_offload_split;\n                        } else {\n                            backend_norm   = GGML_BACKEND_CPU;\n                            backend_output = GGML_BACKEND_CPU;\n                        }\n\n                        model.output_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd},          backend_norm);\n                        model.output      = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, backend_output);\n\n                        if (backend_norm == GGML_BACKEND_GPU) {\n                            vram_weights += ggml_nbytes(model.output_norm);\n                        }\n                        if (backend_output == GGML_BACKEND_GPU_SPLIT) {\n                            vram_weights += ggml_nbytes(model.output);\n                        }\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n\n                    const int i_gpu_start = n_layer - n_gpu_layers;\n\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t i = 0; i < n_layer; ++i) {\n                        const ggml_backend_type backend = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload; // NOLINT\n                        const ggml_backend_type backend_split = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload_split; // NOLINT\n\n                        auto & layer = model.layers[i];\n\n                        layer.attn_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, backend);\n\n                        layer.wq = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_Q,   \"weight\", i), {n_embd, n_embd},     backend_split);\n                        layer.wk = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_K,   \"weight\", i), {n_embd, n_embd_gqa}, backend_split);\n                        layer.wv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_V,   \"weight\", i), {n_embd, n_embd_gqa}, backend_split);\n                        layer.wo = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd},     backend_split);\n\n                        layer.ffn_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, backend);\n\n                        layer.ffn_gate = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_GATE, \"weight\", i), {n_embd,   n_ff}, backend_split);\n                        layer.ffn_down = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, backend_split);\n                        layer.ffn_up   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, backend_split);\n\n                        if (backend == GGML_BACKEND_GPU) {\n                            vram_weights +=\n                                ggml_nbytes(layer.attn_norm) + ggml_nbytes(layer.wq)       + ggml_nbytes(layer.wk)       +\n                                ggml_nbytes(layer.wv)        + ggml_nbytes(layer.wo)       + ggml_nbytes(layer.ffn_norm) +\n                                ggml_nbytes(layer.ffn_gate)  + ggml_nbytes(layer.ffn_down) + ggml_nbytes(layer.ffn_up);\n                        }\n                    }\n                } break;\n            case LLM_ARCH_BAICHUAN:\n                {\n                    model.tok_embd = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, GGML_BACKEND_CPU);\n                    {\n                        ggml_backend_type backend_norm;\n                        ggml_backend_type backend_output;\n\n                        if (n_gpu_layers > int(n_layer)) {\n                            // norm is not performance relevant on its own but keeping it in VRAM reduces data copying\n                            // on Windows however this is detrimental unless everything is on the GPU\n#ifndef _WIN32\n                            backend_norm = llama_backend_offload;\n#else\n                            backend_norm = n_gpu_layers <= (int) n_layer + 2 ? GGML_BACKEND_CPU : llama_backend_offload;\n#endif // _WIN32\n\n                            backend_output = llama_backend_offload_split;\n                        } else {\n                            backend_norm   = GGML_BACKEND_CPU;\n                            backend_output = GGML_BACKEND_CPU;\n                        }\n\n                        model.output_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd},          backend_norm);\n                        model.output      = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, backend_output);\n\n                        if (backend_norm == GGML_BACKEND_GPU) {\n                            vram_weights += ggml_nbytes(model.output_norm);\n                        }\n                        if (backend_output == GGML_BACKEND_GPU_SPLIT) {\n                            vram_weights += ggml_nbytes(model.output);\n                        }\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n\n                    const int i_gpu_start = n_layer - n_gpu_layers;\n\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t i = 0; i < n_layer; ++i) {\n                        const ggml_backend_type backend = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload; // NOLINT\n                        const ggml_backend_type backend_split = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload_split; // NOLINT\n\n                        auto & layer = model.layers[i];\n\n                        layer.attn_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, backend);\n\n                        layer.wq = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_Q,   \"weight\", i), {n_embd, n_embd},     backend_split);\n                        layer.wk = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_K,   \"weight\", i), {n_embd, n_embd_gqa}, backend_split);\n                        layer.wv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_V,   \"weight\", i), {n_embd, n_embd_gqa}, backend_split);\n                        layer.wo = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd},     backend_split);\n\n                        layer.ffn_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, backend);\n\n                        layer.ffn_gate = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_GATE, \"weight\", i), {n_embd,   n_ff}, backend_split);\n                        layer.ffn_down = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, backend_split);\n                        layer.ffn_up   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, backend_split);\n\n                        if (backend == GGML_BACKEND_GPU) {\n                            vram_weights +=\n                                ggml_nbytes(layer.attn_norm) + ggml_nbytes(layer.wq)       + ggml_nbytes(layer.wk)       +\n                                ggml_nbytes(layer.wv)        + ggml_nbytes(layer.wo)       + ggml_nbytes(layer.ffn_norm) +\n                                ggml_nbytes(layer.ffn_gate)  + ggml_nbytes(layer.ffn_down) + ggml_nbytes(layer.ffn_up);\n                        }\n                    }\n                } break;\n            case LLM_ARCH_FALCON:\n                {\n                    // TODO: CPU-only for now\n\n                    model.tok_embd = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, GGML_BACKEND_CPU);\n\n                    // output\n                    {\n                        ggml_backend_type backend_norm;\n                        ggml_backend_type backend_output;\n\n                        if (n_gpu_layers > int(n_layer)) {\n                            // norm is not performance relevant on its own but keeping it in VRAM reduces data copying\n                            // on Windows however this is detrimental unless everything is on the GPU\n#ifndef _WIN32\n                            backend_norm = llama_backend_offload;\n#else\n                            backend_norm = n_gpu_layers <= (int) n_layer + 2 ? GGML_BACKEND_CPU : llama_backend_offload;\n#endif // _WIN32\n\n                            backend_output = llama_backend_offload_split;\n                        } else {\n                            backend_norm   = GGML_BACKEND_CPU;\n                            backend_output = GGML_BACKEND_CPU;\n                        }\n\n                        model.output_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd},          backend_norm);\n                        model.output_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"bias\"),   {n_embd},          backend_norm);\n                        model.output        = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, backend_output);\n\n                        if (backend_norm == GGML_BACKEND_GPU) {\n                            vram_weights += ggml_nbytes(model.output_norm);\n                            vram_weights += ggml_nbytes(model.output_norm_b);\n                        }\n                        if (backend_output == GGML_BACKEND_GPU_SPLIT) {\n                            vram_weights += ggml_nbytes(model.output);\n                        }\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n\n                    const int i_gpu_start = n_layer - n_gpu_layers;\n\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t i = 0; i < n_layer; ++i) {\n                        const ggml_backend_type backend       = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload; // NOLINT\n                        const ggml_backend_type backend_split = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload_split; // NOLINT\n\n                        auto & layer = model.layers[i];\n\n                        layer.attn_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM,   \"weight\", i), {n_embd}, backend);\n                        layer.attn_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM,   \"bias\", i),   {n_embd}, backend);\n\n                        if (gguf_find_tensor(ml.ctx_gguf, tn(LLM_TENSOR_ATTN_NORM_2, \"weight\", i).first.c_str()) >= 0) {\n                            layer.attn_norm_2   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM_2, \"weight\", i), {n_embd}, backend);\n                            layer.attn_norm_2_b = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM_2, \"bias\", i),   {n_embd}, backend);\n\n                            if (backend == GGML_BACKEND_GPU) {\n                                vram_weights += ggml_nbytes(layer.attn_norm_2);\n                                vram_weights += ggml_nbytes(layer.attn_norm_2_b);\n                            }\n                        }\n\n                        layer.wqkv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_QKV, \"weight\", i), {n_embd, n_embd + 2*n_embd_gqa}, backend_split);\n                        layer.wo   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd},                backend_split);\n\n                        layer.ffn_down = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, backend_split);\n                        layer.ffn_up   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, backend_split);\n\n                        if (backend == GGML_BACKEND_GPU) {\n                            vram_weights +=\n                                ggml_nbytes(layer.attn_norm) + ggml_nbytes(layer.attn_norm_b) +\n                                ggml_nbytes(layer.wqkv)      + ggml_nbytes(layer.wo)          +\n                                ggml_nbytes(layer.ffn_down)  + ggml_nbytes(layer.ffn_up);\n                        }\n                    }\n                } break;\n            case LLM_ARCH_STARCODER:\n                {\n                    model.tok_embd = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab},             GGML_BACKEND_CPU);\n                    model.pos_embd = ml.create_tensor(ctx, tn(LLM_TENSOR_POS_EMBD, \"weight\"),   {n_embd, hparams.n_ctx_train}, GGML_BACKEND_CPU);\n\n                    // output\n                    {\n                        ggml_backend_type backend_norm;\n                        ggml_backend_type backend_output;\n\n                        if (n_gpu_layers > int(n_layer)) {\n                            // norm is not performance relevant on its own but keeping it in VRAM reduces data copying\n                            // on Windows however this is detrimental unless everything is on the GPU\n#ifndef _WIN32\n                            backend_norm = llama_backend_offload;\n#else\n                            backend_norm = n_gpu_layers <= (int) n_layer + 2 ? GGML_BACKEND_CPU : llama_backend_offload;\n#endif // _WIN32\n\n                            backend_output = llama_backend_offload_split;\n                        } else {\n                            backend_norm   = GGML_BACKEND_CPU;\n                            backend_output = GGML_BACKEND_CPU;\n                        }\n\n                        model.output_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd},          backend_norm);\n                        model.output_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"bias\"),   {n_embd},          backend_norm);\n                        model.output        = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, backend_output);\n\n                        if (backend_norm == GGML_BACKEND_GPU) {\n                            vram_weights += ggml_nbytes(model.output_norm);\n                            vram_weights += ggml_nbytes(model.output_norm_b);\n                        }\n                        if (backend_output == GGML_BACKEND_GPU_SPLIT) {\n                            vram_weights += ggml_nbytes(model.output);\n                        }\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n\n                    const int i_gpu_start = n_layer - n_gpu_layers;\n\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t i = 0; i < n_layer; ++i) {\n                        const ggml_backend_type backend       = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload; // NOLINT\n                        const ggml_backend_type backend_split = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload_split; // NOLINT\n\n                        auto & layer = model.layers[i];\n\n                        layer.attn_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM,   \"weight\", i), {n_embd}, backend);\n                        layer.attn_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM,   \"bias\", i),   {n_embd}, backend);\n\n                        layer.wqkv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_QKV, \"weight\", i), {n_embd, n_embd + 2*n_embd_gqa}, backend_split);\n                        layer.bqkv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_QKV, \"bias\", i),   {n_embd + 2*n_embd_gqa},         backend);\n\n                        layer.wo   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd},   backend_split);\n                        layer.bo   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"bias\", i),   {n_embd},           backend);\n\n                        layer.ffn_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, backend);\n                        layer.ffn_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"bias\", i),   {n_embd}, backend);\n\n                        layer.ffn_down   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {n_ff, n_embd}, backend_split);\n                        layer.ffn_down_b = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"bias\", i),   {n_embd},       backend);\n\n                        layer.ffn_up   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd, n_ff}, backend_split);\n                        layer.ffn_up_b = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"bias\", i),           {n_ff}, backend);\n\n                        if (backend == GGML_BACKEND_GPU) {\n                            vram_weights +=\n                                ggml_nbytes(layer.attn_norm) + ggml_nbytes(layer.attn_norm_b) +\n                                ggml_nbytes(layer.wqkv)      + ggml_nbytes(layer.bqkv)        +\n                                ggml_nbytes(layer.wo)        + ggml_nbytes(layer.bo)          +\n                                ggml_nbytes(layer.ffn_norm)  + ggml_nbytes(layer.ffn_norm_b)  +\n                                ggml_nbytes(layer.ffn_down)  + ggml_nbytes(layer.ffn_down_b)  +\n                                ggml_nbytes(layer.ffn_up)    + ggml_nbytes(layer.ffn_up_b);\n                        }\n                    }\n                } break;\n            case LLM_ARCH_PERSIMMON:\n                {\n                    model.tok_embd = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"),  {n_embd, n_vocab}, GGML_BACKEND_CPU);\n\n                    {\n                        ggml_backend_type backend_norm;\n                        ggml_backend_type backend_output;\n\n                        if (n_gpu_layers > int(n_layer)) {\n#ifdef GGML_USE_CUBLAS\n                            if (n_gpu_layers > int(n_layer + 1)) {\n                                LLAMA_LOG_ERROR(\"%s: CUDA backend missing Persimmon CUDA ops, can offload at most %ld layers. See: https://github.com/ggerganov/llama.cpp/issues/4038\\n\",\n                                    __func__, n_layer + 1);\n                                throw std::runtime_error(\"Persimmon CUDA offload failed\");\n                            }\n#endif\n                            // norm is not performance relevant on its own but keeping it in VRAM reduces data copying\n                            // on Windows however this is detrimental unless everything is on the GPU\n#ifndef _WIN32\n                            backend_norm = llama_backend_offload;\n#else\n                            backend_norm = n_gpu_layers <= (int) n_layer + 2 ? GGML_BACKEND_CPU : llama_backend_offload;\n#endif // _WIN32\n\n                            backend_output = llama_backend_offload_split;\n                        } else {\n                            backend_norm   = GGML_BACKEND_CPU;\n                            backend_output = GGML_BACKEND_CPU;\n                        }\n\n                        model.output_norm    = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd},          backend_norm);\n                        model.output_norm_b  = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"bias\"),   {n_embd},          backend_norm);\n                        model.output         = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, backend_output);\n\n                        if (backend_norm == GGML_BACKEND_GPU) {\n                            vram_weights += ggml_nbytes(model.output_norm);\n                            vram_weights += ggml_nbytes(model.output_norm_b);\n                        }\n                        if (backend_output == GGML_BACKEND_GPU_SPLIT) {\n                            vram_weights += ggml_nbytes(model.output);\n                        }\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n                    const int i_gpu_start = n_layer - n_gpu_layers;\n                    model.layers.resize(n_layer);\n                    for (uint32_t i = 0; i < n_layer; ++i) {\n                        const ggml_backend_type backend = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload;\n                        const ggml_backend_type backend_split = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload_split;\n                        auto & layer = model.layers[i];\n                        layer.attn_norm     = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM,   \"weight\", i), {n_embd}, backend);\n                        layer.attn_norm_b   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM,   \"bias\",   i), {n_embd}, backend);\n                        layer.wqkv          = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_QKV,    \"weight\", i), {n_embd, n_embd + 2*n_embd_gqa}, backend_split);\n                        layer.bqkv          = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_QKV,    \"bias\",   i), {n_embd + 2*n_embd_gqa},         backend);\n                        layer.wo            = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT,    \"weight\", i), {n_embd, n_embd},   backend_split);\n                        layer.bo            = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT,    \"bias\",   i), {n_embd},           backend);\n                        layer.ffn_down      = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN,    \"weight\", i), {n_ff, n_embd}, backend_split);\n                        layer.ffn_down_b    = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN,    \"bias\",   i), {n_embd},       backend);\n                        layer.ffn_up        = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,      \"weight\", i), {n_embd,   n_ff}, backend_split);\n                        layer.ffn_up_b      = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,      \"bias\",   i), {n_ff},           backend);\n                        layer.ffn_norm      = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM,    \"weight\", i), {n_embd}, backend);\n                        layer.ffn_norm_b    = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM,    \"bias\",   i), {n_embd}, backend);\n                        layer.attn_q_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_Q_NORM, \"weight\", i), {64}, backend);\n                        layer.attn_q_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_Q_NORM, \"bias\",   i), {64}, backend);\n                        layer.attn_k_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_K_NORM, \"weight\", i), {64}, backend);\n                        layer.attn_k_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_K_NORM, \"bias\",   i), {64}, backend);\n                    }\n                } break;\n            case LLM_ARCH_BLOOM:\n                {\n                    // TODO: CPU-only for now\n\n                    model.tok_embd   = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD,      \"weight\"), {n_embd, n_vocab}, GGML_BACKEND_CPU);\n                    model.tok_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD_NORM, \"weight\"), {n_embd},          GGML_BACKEND_CPU);\n                    model.tok_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD_NORM, \"bias\"),   {n_embd},          GGML_BACKEND_CPU);\n\n                    // output\n                    {\n                        ggml_backend_type backend_norm;\n                        ggml_backend_type backend_output;\n\n                        if (n_gpu_layers > int(n_layer)) {\n                            // norm is not performance relevant on its own but keeping it in VRAM reduces data copying\n                            // on Windows however this is detrimental unless everything is on the GPU\n#ifndef _WIN32\n                            backend_norm = llama_backend_offload;\n#else\n                            backend_norm = n_gpu_layers <= (int) n_layer + 2 ? GGML_BACKEND_CPU : llama_backend_offload;\n#endif // _WIN32\n\n                            backend_output = llama_backend_offload_split;\n                        } else {\n                            backend_norm   = GGML_BACKEND_CPU;\n                            backend_output = GGML_BACKEND_CPU;\n                        }\n\n                        model.output_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd},          backend_norm);\n                        model.output_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"bias\"),   {n_embd},          backend_norm);\n                        model.output        = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, backend_output);\n\n                        if (backend_norm == GGML_BACKEND_GPU) {\n                            vram_weights += ggml_nbytes(model.output_norm);\n                            vram_weights += ggml_nbytes(model.output_norm_b);\n                        }\n                        if (backend_output == GGML_BACKEND_GPU_SPLIT) {\n                            vram_weights += ggml_nbytes(model.output);\n                        }\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n\n                    const int i_gpu_start = n_layer - n_gpu_layers;\n\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t i = 0; i < n_layer; ++i) {\n                        const ggml_backend_type backend       = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload; // NOLINT\n                        const ggml_backend_type backend_split = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload_split; // NOLINT\n\n                        auto & layer = model.layers[i];\n\n                        layer.attn_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM,   \"weight\", i), {n_embd}, backend);\n                        layer.attn_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM,   \"bias\", i),   {n_embd}, backend);\n\n                        layer.wqkv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_QKV, \"weight\", i), {n_embd, n_embd + 2*n_embd_gqa}, backend_split);\n                        layer.bqkv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_QKV, \"bias\", i),   {n_embd + 2*n_embd_gqa},         backend);\n\n                        layer.wo   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd},                backend_split);\n                        layer.bo   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"bias\", i),   {n_embd},                        backend);\n\n                        layer.ffn_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, backend);\n                        layer.ffn_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"bias\", i),   {n_embd}, backend);\n\n                        layer.ffn_down   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {n_ff, n_embd}, backend_split);\n                        layer.ffn_down_b = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"bias\", i),   {n_embd},       backend);\n\n                        layer.ffn_up   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, backend_split);\n                        layer.ffn_up_b = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"bias\", i),   {n_ff},           backend);\n\n                        if (backend == GGML_BACKEND_GPU) {\n                            vram_weights +=\n                                ggml_nbytes(layer.attn_norm) + ggml_nbytes(layer.attn_norm_b) +\n                                ggml_nbytes(layer.wqkv)      + ggml_nbytes(layer.bqkv)        +\n                                ggml_nbytes(layer.wo)        + ggml_nbytes(layer.bo)          +\n                                ggml_nbytes(layer.ffn_norm)  + ggml_nbytes(layer.ffn_norm_b)  +\n                                ggml_nbytes(layer.ffn_up)    + ggml_nbytes(layer.ffn_up_b)    +\n                                ggml_nbytes(layer.ffn_down)  + ggml_nbytes(layer.ffn_down_b);\n                        }\n                    }\n                } break;\n            case LLM_ARCH_MPT:\n                {\n                    model.tok_embd = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, GGML_BACKEND_CPU);\n\n                    // output\n                    {\n                        ggml_backend_type backend_norm;\n                        ggml_backend_type backend_output;\n\n                        if (n_gpu_layers > int(n_layer)) {\n                            // norm is not performance relevant on its own but keeping it in VRAM reduces data copying\n                            // on Windows however this is detrimental unless everything is on the GPU\n#ifndef _WIN32\n                            backend_norm = llama_backend_offload;\n#else\n                            backend_norm = n_gpu_layers <= (int) n_layer + 2 ? GGML_BACKEND_CPU : llama_backend_offload;\n#endif // _WIN32\n\n                            backend_output = llama_backend_offload_split;\n                        } else {\n                            backend_norm   = GGML_BACKEND_CPU;\n                            backend_output = GGML_BACKEND_CPU;\n                        }\n\n                        model.output_norm   = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd},          backend_norm);\n                        model.output        = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, backend_output);\n\n                        if (backend_norm == GGML_BACKEND_GPU) {\n                            vram_weights += ggml_nbytes(model.output_norm);\n                        }\n                        if (backend_output == GGML_BACKEND_GPU_SPLIT) {\n                            vram_weights += ggml_nbytes(model.output);\n                        }\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n\n                    const int i_gpu_start = n_layer - n_gpu_layers;\n\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t i = 0; i < n_layer; ++i) {\n                        const ggml_backend_type backend = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload; // NOLINT\n                        const ggml_backend_type backend_split = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload_split; // NOLINT\n\n                        auto & layer = model.layers[i];\n\n                        layer.attn_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, backend);\n                        layer.wqkv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_QKV, \"weight\", i), {n_embd, n_embd + 2*n_embd_gqa}, backend_split);\n                        layer.wo   = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd},                backend_split);\n\n                        layer.ffn_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, backend);\n\n                        layer.ffn_down = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, backend_split);\n                        layer.ffn_up   = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, backend_split);\n\n                        if (backend == GGML_BACKEND_GPU) {\n                            vram_weights +=\n                                ggml_nbytes(layer.attn_norm) +\n                                ggml_nbytes(layer.wqkv)      +\n                                ggml_nbytes(layer.wo)        +\n                                ggml_nbytes(layer.ffn_norm)  +\n                                ggml_nbytes(layer.ffn_down)  +\n                                ggml_nbytes(layer.ffn_up);\n                        }\n                    }\n                } break;\n            case LLM_ARCH_STABLELM:\n                {\n                    model.tok_embd = ml.create_tensor(ctx, tn(LLM_TENSOR_TOKEN_EMBD, \"weight\"), {n_embd, n_vocab}, GGML_BACKEND_CPU);\n\n                    // output\n                    {\n                        ggml_backend_type backend_norm;\n                        ggml_backend_type backend_output;\n\n                        if (n_gpu_layers > int(n_layer)) {\n                            // norm is not performance relevant on its own but keeping it in VRAM reduces data copying\n                            // on Windows however this is detrimental unless everything is on the GPU\n#ifndef _WIN32\n                            backend_norm = llama_backend_offload;\n#else\n                            backend_norm = n_gpu_layers <= (int) n_layer + 2 ? GGML_BACKEND_CPU : llama_backend_offload;\n#endif // _WIN32\n\n                            backend_output = llama_backend_offload_split;\n                        } else {\n                            backend_norm   = GGML_BACKEND_CPU;\n                            backend_output = GGML_BACKEND_CPU;\n                        }\n\n                        model.output_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"bias\"), {n_embd},          backend_norm);\n                        model.output_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT_NORM, \"weight\"), {n_embd},          backend_norm);\n                        model.output      = ml.create_tensor(ctx, tn(LLM_TENSOR_OUTPUT,      \"weight\"), {n_embd, n_vocab}, backend_output);\n\n                        if (backend_norm == GGML_BACKEND_GPU) {\n                            vram_weights += ggml_nbytes(model.output_norm);\n                        }\n                        if (backend_output == GGML_BACKEND_GPU_SPLIT) {\n                            vram_weights += ggml_nbytes(model.output);\n                        }\n                    }\n\n                    const uint32_t n_ff = hparams.n_ff;\n\n                    const int i_gpu_start = n_layer - n_gpu_layers;\n\n                    model.layers.resize(n_layer);\n\n                    for (uint32_t i = 0; i < n_layer; ++i) {\n                        /*\n                        llama_model_loader: - tensor    4:         blk.0.attn_output.weight f16      [  2560,  2560,     1,     1 ]\n                        */\n                        const ggml_backend_type backend = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload; // NOLINT\n                        const ggml_backend_type backend_split = int(i) < i_gpu_start ? GGML_BACKEND_CPU : llama_backend_offload_split; // NOLINT\n\n                        auto & layer = model.layers[i];\n\n                        layer.attn_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM, \"weight\", i), {n_embd}, backend);\n                        layer.attn_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_NORM, \"bias\", i), {n_embd}, backend);\n\n                        layer.wq = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_Q,   \"weight\", i), {n_embd, n_embd},     backend_split);\n                        layer.wk = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_K,   \"weight\", i), {n_embd, n_embd_gqa}, backend_split);\n                        layer.wv = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_V,   \"weight\", i), {n_embd, n_embd_gqa}, backend_split);\n                        layer.wo = ml.create_tensor(ctx, tn(LLM_TENSOR_ATTN_OUT, \"weight\", i), {n_embd, n_embd},     backend_split);\n\n                        layer.ffn_norm = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"weight\", i), {n_embd}, backend);\n                        layer.ffn_norm_b = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_NORM, \"bias\", i), {n_embd}, backend);\n\n                        layer.ffn_gate = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_GATE, \"weight\", i), {n_embd,   n_ff}, backend_split);\n                        layer.ffn_down = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_DOWN, \"weight\", i), {  n_ff, n_embd}, backend_split);\n                        layer.ffn_up = ml.create_tensor(ctx, tn(LLM_TENSOR_FFN_UP,   \"weight\", i), {n_embd,   n_ff}, backend_split);\n\n                        if (backend == GGML_BACKEND_GPU) {\n                            vram_weights +=\n                                ggml_nbytes(layer.attn_norm) + ggml_nbytes(layer.wq)       + ggml_nbytes(layer.wk)       +\n                                ggml_nbytes(layer.wv)        + ggml_nbytes(layer.wo)       + ggml_nbytes(layer.ffn_norm) +\n                                ggml_nbytes(layer.ffn_gate)  + ggml_nbytes(layer.ffn_down) + ggml_nbytes(layer.ffn_up);\n                        }\n                    }\n                } break;\n\n            default:\n                throw std::runtime_error(\"unknown architecture\");\n        }\n    }\n\n    ml.done_getting_tensors();\n\n    // print memory requirements\n    {\n        // this is the total memory required to run the inference\n        size_t mem_required =\n            ctx_size +\n            mmapped_size - vram_weights; // weights in VRAM not in memory\n\n        LLAMA_LOG_INFO(\"%s: mem required  = %7.2f MB\\n\", __func__, mem_required / 1024.0 / 1024.0);\n\n#if defined(GGML_USE_CUBLAS) || defined(GGML_USE_CLBLAST)\n        const int n_gpu = std::min(n_gpu_layers, int(hparams.n_layer));\n\n        LLAMA_LOG_INFO(\"%s: offloading %d repeating layers to GPU\\n\", __func__, n_gpu);\n        if (n_gpu_layers > (int) hparams.n_layer) {\n            LLAMA_LOG_INFO(\"%s: offloading non-repeating layers to GPU\\n\", __func__);\n        }\n\n#ifdef GGML_USE_CUBLAS\n        const int max_backend_supported_layers = hparams.n_layer + 3;\n        const int max_offloadable_layers       = hparams.n_layer + 3;\n#elif GGML_USE_CLBLAST\n        const int max_backend_supported_layers = hparams.n_layer + 1;\n        const int max_offloadable_layers       = hparams.n_layer + 1;\n#endif // GGML_USE_CUBLAS\n\n        LLAMA_LOG_INFO(\"%s: offloaded %d/%d layers to GPU\\n\", __func__, std::min(n_gpu_layers, max_offloadable_layers), max_backend_supported_layers);\n        LLAMA_LOG_INFO(\"%s: VRAM used: %.2f MB\\n\", __func__, vram_weights / 1024.0 / 1024.0);\n#else\n        (void) n_gpu_layers;\n#endif // defined(GGML_USE_CUBLAS) || defined(GGML_USE_CLBLAST)\n    }\n\n    // populate `tensors_by_name`\n    for (int i = 0; i < ml.n_tensors; ++i) {\n        struct ggml_tensor * cur = ggml_get_tensor(ctx, ml.get_tensor_name(i));\n        model.tensors_by_name.emplace_back(ggml_get_name(cur), cur);\n    }\n\n    (void) tensor_split;\n#ifdef GGML_USE_CUBLAS\n    {\n        ggml_cuda_set_tensor_split(tensor_split);\n    }\n#endif\n\n    ml.load_all_data(ctx, progress_callback, progress_callback_user_data, use_mlock ? &model.mlock_mmap : NULL);\n\n    if (progress_callback) {\n        progress_callback(1.0f, progress_callback_user_data);\n    }\n\n    model.mapping = std::move(ml.mapping);\n\n    // loading time will be recalculate after the first eval, so\n    // we take page faults deferred by mmap() into consideration\n    model.t_load_us = ggml_time_us() - model.t_start_us;\n}\n\nstatic bool llama_model_load(const std::string & fname, llama_model & model, const llama_model_params & params, const llama_context_params * cparams) {\n    try {\n        llama_model_loader ml(fname, params.use_mmap);\n\n        if (ml.sparse_deriv == GGML_SPARSE_INFERENCE) {\n            LLAMA_LOG_INFO(\"%s: PowerInfer model loaded. Sparse inference will be used.\\n\", __func__);\n        }\n\n        model.hparams.vocab_only = params.vocab_only;\n        model.sparse_deriv = ml.sparse_deriv;\n\n        llm_load_arch   (ml, model);\n        llm_load_hparams(ml, model);\n        llm_load_vocab  (ml, model);\n\n        llm_load_print_meta(ml, model);\n\n        if (model.hparams.n_vocab != model.vocab.id_to_token.size()) {\n            throw std::runtime_error(\"vocab size mismatch\");\n        }\n\n        if (params.vocab_only) {\n            LLAMA_LOG_INFO(\"%s: vocab only - skipping tensors\\n\", __func__);\n            return true;\n        }\n\n        if (llama_use_sparse_inference(&model)) {\n            if (params.n_gpu_layers > 0) {\n                LLAMA_LOG_WARN(\"%s: sparse inference ignores n_gpu_layers, you can use --vram-budget option instead\\n\", __func__);\n                return false;\n            }\n#if defined GGML_USE_CUBLAS\n            llama_set_vram_budget(params.vram_budget_gb, params.main_gpu);\n#endif\n            llm_load_sparse_model_tensors(\n                ml, model, cparams, params.main_gpu, vram_budget_bytes, params.reset_gpu_index, params.disable_gpu_index,\n                params.use_mlock, params.progress_callback, params.progress_callback_user_data\n            );\n        } else {\n            llm_load_tensors(\n                ml, model, params.n_gpu_layers, params.main_gpu, params.tensor_split, params.use_mlock,\n                params.progress_callback, params.progress_callback_user_data\n            );\n        }\n\n    } catch (const std::exception & err) {\n        LLAMA_LOG_ERROR(\"error loading model: %s\\n\", err.what());\n        return false;\n    }\n\n    return true;\n}\n\n//\n// llm_build\n//\n\nusing llm_build_cb = std::function<void(struct ggml_tensor * cur, const char * name, int nl)>;\nusing llm_build_cb_short = std::function<void(struct ggml_tensor * cur, const char * name)>;\n\nenum llm_rope_type {\n    LLM_ROPE,\n    LLM_ROPE_NEOX,\n    LLM_ROPE_GLM,\n};\n\nenum llm_ffn_op_type {\n    LLM_FFN_SILU,\n    LLM_FFN_GELU,\n    LLM_FFN_RELU,\n    LLM_FFN_RELU_SQR,\n};\n\nenum llm_ffn_gate_type {\n    LLM_FFN_SEQ,\n    LLM_FFN_PAR, // ffn_gate is parallel to ffn_up\n    LLM_FFN_SYM, // ffn_gate is parallel to ffn_up and should pass through an activation function\n};\n\nenum llm_norm_type {\n    LLM_NORM,\n    LLM_NORM_RMS,\n};\n\nstatic struct ggml_tensor * llm_build_inp_embd(\n        struct ggml_context * ctx,\n        const llama_hparams & hparams,\n          const llama_batch & batch,\n         struct ggml_tensor * tok_embd,\n         const llm_build_cb & cb) {\n    const int64_t n_embd = hparams.n_embd;\n\n    struct ggml_tensor * inpL;\n\n    if (batch.token) {\n        struct ggml_tensor * inp_tokens = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, batch.n_tokens);\n        cb(inp_tokens, \"inp_tokens\", -1);\n\n        inpL = ggml_get_rows(ctx, tok_embd, inp_tokens);\n    } else {\n#ifdef GGML_USE_MPI\n        GGML_ASSERT(false && \"not implemented\");\n#endif\n\n        inpL = ggml_new_tensor_2d(ctx, GGML_TYPE_F32, n_embd, batch.n_tokens);\n    }\n\n    return inpL;\n}\n\n// Persimmon: n_rot = n_embd_head/2\n// Other:     n_rot = n_embd_head\nstatic void llm_build_k_shift(\n      struct ggml_context * ctx,\n      const llama_hparams & hparams,\n      const llama_cparams & cparams,\n     const llama_kv_cache & kv,\n       struct ggml_cgraph * graph,\n            llm_rope_type   type,\n                  int64_t   n_ctx,\n                  int64_t   n_rot,\n                  float     freq_base,\n                  float     freq_scale,\n       const llm_build_cb & cb) {\n    const int64_t n_layer     = hparams.n_layer;\n    const int64_t n_head_kv   = hparams.n_head_kv;\n    const int64_t n_embd_gqa  = hparams.n_embd_gqa();\n    const int64_t n_embd_head = hparams.n_embd_head();\n    const int32_t n_orig_ctx  = cparams.n_yarn_orig_ctx;\n    const float   ext_factor  = cparams.yarn_ext_factor;\n    const float   attn_factor = cparams.yarn_attn_factor;\n    const float   beta_fast   = cparams.yarn_beta_fast;\n    const float   beta_slow   = cparams.yarn_beta_slow;\n\n    GGML_ASSERT(n_embd_head % n_rot == 0);\n\n    struct ggml_tensor * K_shift = ggml_new_tensor_1d(ctx, GGML_TYPE_I32, n_ctx);\n    cb(K_shift, \"K_shift\", -1);\n\n    int rope_type = 0;\n\n    switch (type) {\n        case LLM_ROPE:      rope_type = 0; break;\n        case LLM_ROPE_NEOX: rope_type = 2; break;\n        case LLM_ROPE_GLM:  rope_type = 4; break;\n    }\n\n    for (int il = 0; il < n_layer; ++il) {\n        struct ggml_tensor * tmp =\n            // we rotate only the first n_rot dimensions\n            ggml_rope_custom_inplace(ctx,\n                    ggml_view_3d(ctx, kv.k,\n                        n_rot, n_head_kv, n_ctx,\n                        ggml_element_size(kv.k)*n_embd_head,\n                        ggml_element_size(kv.k)*n_embd_gqa,\n                        ggml_element_size(kv.k)*n_embd_gqa*n_ctx*il),\n                    K_shift, n_rot, rope_type, 0, n_orig_ctx, freq_base, freq_scale,\n                    ext_factor, attn_factor, beta_fast, beta_slow);\n        cb(tmp, \"K_shifted\", il);\n        ggml_build_forward_expand(graph, tmp);\n    }\n}\n\nstatic std::pair<ggml_tensor*, ggml_tensor*> llm_build_kv_store(\n        struct ggml_context * ctx,\n        const llama_hparams & hparams,\n       const llama_kv_cache & kv,\n         struct ggml_cgraph * graph,\n         struct ggml_tensor * k_cur,\n         struct ggml_tensor * v_cur,\n                    int64_t   n_ctx,\n                    int32_t   n_tokens,\n                    int32_t   kv_head,\n         const llm_build_cb & cb,\n                    int64_t   il) {\n    const int64_t n_embd_gqa = hparams.n_embd_gqa();\n\n    // compute the transposed [n_tokens, n_embd] V matrix\n    struct ggml_tensor * v_cur_t = ggml_transpose(ctx, ggml_reshape_2d(ctx, v_cur, n_embd_gqa, n_tokens));\n    //struct ggml_tensor * v_cur_t = ggml_transpose(ctx, v_cur); // TODO: reshape above is likely not needed\n    cb(v_cur_t, \"v_cur_t\", il);\n\n    struct ggml_tensor * k_cache_view = ggml_view_1d(ctx, kv.k, n_tokens*n_embd_gqa,\n            (ggml_element_size(kv.k)*n_embd_gqa)*(il*n_ctx + kv_head));\n    cb(k_cache_view, \"k_cache_view\", il);\n\n    struct ggml_tensor * v_cache_view = ggml_view_2d(ctx, kv.v, n_tokens, n_embd_gqa,\n            (   n_ctx)*ggml_element_size(kv.v),\n            (il*n_ctx)*ggml_element_size(kv.v)*n_embd_gqa + kv_head*ggml_element_size(kv.v));\n    cb(v_cache_view, \"v_cache_view\", il);\n\n    // important: storing RoPE-ed version of K in the KV cache!\n    ggml_tensor * k_cpy = ggml_cpy(ctx, k_cur,   k_cache_view);\n    ggml_tensor * v_cpy = ggml_cpy(ctx, v_cur_t, v_cache_view);\n    //ggml_build_forward_expand(graph, ggml_cpy(ctx, k_cur,   k_cache_view));\n    //ggml_build_forward_expand(graph, ggml_cpy(ctx, v_cur_t, v_cache_view));\n    \n    return {k_cpy, v_cpy};\n}\n\nstatic struct ggml_tensor * llm_build_norm(\n        struct ggml_context * ctx,\n         struct ggml_tensor * cur,\n        const llama_hparams & hparams,\n         struct ggml_tensor * mw,\n         struct ggml_tensor * mb,\n              llm_norm_type   type,\n         const llm_build_cb & cb,\n                        int   il) {\n    switch (type) {\n        case LLM_NORM:     cur = ggml_norm    (ctx, cur, hparams.f_norm_eps);     break;\n        case LLM_NORM_RMS: cur = ggml_rms_norm(ctx, cur, hparams.f_norm_rms_eps); break;\n    }\n\n    if (mw || mb) {\n        cb(cur, \"norm\", il);\n    }\n\n    if (mw) {\n        cur = ggml_mul(ctx, cur, mw);\n        if (mb) {\n            cb(cur, \"norm_w\", il);\n        }\n    }\n\n    if (mb) {\n        cur = ggml_add(ctx, cur, mb);\n    }\n\n    return cur;\n}\n\nstatic struct ggml_tensor * llm_build_ffn(\n        struct ggml_context * ctx,\n         struct ggml_tensor * cur,\n         struct ggml_tensor * up,\n         struct ggml_tensor * up_b,\n         struct ggml_tensor * gate,\n         struct ggml_tensor * gate_b,\n         struct ggml_tensor * down,\n         struct ggml_tensor * down_b,\n            llm_ffn_op_type   type_op,\n          llm_ffn_gate_type   type_gate,\n         const llm_build_cb & cb,\n                        int   il) {\n    struct ggml_tensor * tmp = ggml_mul_mat(ctx, up, cur);\n    cb(tmp, \"ffn_up\", il);\n\n    if (up_b) {\n        tmp = ggml_add(ctx, tmp, up_b);\n        cb(tmp, \"ffn_up_b\", il);\n    }\n\n    if (gate) {\n        switch (type_gate) {\n            case LLM_FFN_SEQ:\n                {\n                    cur = ggml_mul_mat(ctx, gate, tmp);\n                    cb(cur, \"ffn_gate\", il);\n                } break;\n            case LLM_FFN_PAR:\n            case LLM_FFN_SYM:\n                {\n                    cur = ggml_mul_mat(ctx, gate, cur);\n                    cb(cur, \"ffn_gate\", il);\n                } break;\n        }\n\n        if (gate_b) {\n            cur = ggml_add(ctx, cur, gate_b);\n            cb(cur, \"ffn_gate_b\", il);\n        }\n    } else {\n        cur = tmp;\n    }\n\n    auto act_fn = [&] (ggml_tensor * cur) {\n        switch (type_op) {\n            case LLM_FFN_SILU:\n                {\n                    cur = ggml_silu(ctx, cur);\n                    cb(cur, \"ffn_silu\", il);\n                } break;\n            case LLM_FFN_GELU:\n                {\n                    cur = ggml_gelu(ctx, cur);\n                    cb(cur, \"ffn_gelu\", il);\n                } break;\n            case LLM_FFN_RELU:\n                {\n                    cur = ggml_relu(ctx, cur);\n                    cb(cur, \"ffn_relu\", il);\n                } break;\n            case LLM_FFN_RELU_SQR:\n                {\n                    cur = ggml_relu(ctx, cur);\n                    cb(cur, \"ffn_relu\", il);\n\n                    cur = ggml_sqr(ctx, cur);\n                    cb(cur, \"ffn_sqr(relu)\", il);\n                } break;\n        }\n\n        return cur;\n    };\n\n    cur = act_fn(cur);\n    if (type_gate == LLM_FFN_SYM) {\n        // In this case, the output of up is also activated\n        tmp = act_fn(tmp);\n    }\n\n    if (type_gate == LLM_FFN_PAR || type_gate == LLM_FFN_SYM) {\n        cur = ggml_mul(ctx, cur, tmp);\n        cb(cur, \"ffn_gate_par\", il);\n    }\n\n    cur = ggml_mul_mat(ctx, down, cur);\n\n    if (down_b) {\n        cur = ggml_add(ctx, cur, down_b);\n    }\n\n    return cur;\n}\n\nstatic struct ggml_tensor * llm_build_sparse_mul_mat(\n        struct ggml_context * ctx,\n         struct ggml_tensor * up,\n         struct ggml_tensor * inp,\n         struct ggml_tensor * idx,\n         struct ggml_tensor * up_gpu,\n         struct ggml_tensor * gpu_index,\n         struct ggml_tensor * gpu_bucket,\n   const llm_build_cb_short & cb,\n                 const char * name,\n                         bool full_gpu) {\n    std::string full_name = \"ffn_\" + std::string(name) + \"_sparse\";\n    ggml_tensor * out = nullptr;\n\n#ifdef GGML_USE_HIPBLAS\n// WARNING: THIS IS A HACK! \n// if up_gpu->data is null\n// inference fails when model exceeds 40B on rocm device\n// so we just let up_gpu->data point to itself\n    \n    up_gpu->data = up_gpu;\n\n#endif \n\n#ifdef GGML_USE_CUBLAS\n    // Full offloading fast path\n    if (full_gpu) {\n        GGML_ASSERT(up_gpu && \"full_gpu but no up_gpu\");\n        out = ggml_mul_mat_idx(ctx, up_gpu, inp, idx, NULL);\n        ggml_cuda_assign_buffers_no_alloc(out);\n        cb(out, (full_name).c_str());\n        return out;\n    }\n#endif\n\n    out = ggml_mul_mat_idx(ctx, up, inp, idx, gpu_index);\n    cb(out, full_name.c_str());\n\n#ifdef GGML_USE_CUBLAS\n    if (up_gpu) {\n        ggml_tensor * out_gpu = ggml_mul_mat_idx_upscale(ctx, up_gpu, inp, idx, gpu_bucket, out->ne[0]);\n        ggml_cuda_assign_buffers_no_alloc(out_gpu);\n        cb(out_gpu, (full_name + \"_gpu\").c_str());\n        out = ggml_add(ctx, out, out_gpu);\n        // We don't need to assign buffers here, as the output will be passed into Axpy,\n        // which in this case, is also a hybrid operation.\n        cb(out, (full_name + \"_merged\").c_str());\n    }\n#endif\n\n    return out;\n}\n\nstatic struct ggml_tensor * llm_build_sparse_axpy(\n        struct ggml_context * ctx,\n         struct ggml_tensor * w_t,\n         struct ggml_tensor * x,\n         struct ggml_tensor * sparse_idx,\n         struct ggml_tensor * wt_gpu,\n         struct ggml_tensor * gpu_index,\n         struct ggml_tensor * gpu_bucket,\n   const llm_build_cb_short & cb,\n                 const char * name,\n                         bool full_gpu) {\n    std::string full_name = \"ffn_\" + std::string(name) + \"_sparse\";\n    ggml_tensor * out = nullptr;\n\n#ifdef GGML_USE_HIPBLAS\n// WARNING: THIS IS A HACK! \n// if wt_gpu->data is null\n// inference fails when model exceeds 40B on rocm device\n// so we just let wt_gpu->data point to itself\n    \n    wt_gpu->data = wt_gpu;\n\n#endif \n\n#ifdef GGML_USE_CUBLAS\n    // Full offloading fast path\n    if (full_gpu) {\n        GGML_ASSERT(wt_gpu && \"full_gpu but no wt_gpu\");\n        out = ggml_axpy(ctx, wt_gpu, x, sparse_idx, NULL);\n        ggml_cuda_assign_buffers_no_alloc(out);\n        cb(out, (full_name).c_str());\n        return out;\n    }\n#endif\n\n    // TODO: should pass NULL as hybrid_aux when hybrid_split is false\n    out = ggml_axpy(ctx, w_t, x, sparse_idx, gpu_index);\n    cb(out, full_name.c_str());\n\n#ifdef GGML_USE_CUBLAS\n    if (wt_gpu) {\n        ggml_tensor * out_gpu = ggml_axpy(ctx, wt_gpu, x, sparse_idx, gpu_bucket);\n        cb(out_gpu, (full_name + \"_gpu\").c_str());\n        ggml_cuda_assign_buffers_no_alloc(out_gpu);\n        out = ggml_add(ctx, out, out_gpu);\n        ggml_cuda_assign_buffers_no_alloc(out);\n        cb(out, (full_name + \"_merged\").c_str());\n    }\n#endif\n\n    return out;\n}\n\nstatic struct ggml_tensor * llm_build_ffn_sparse(\n        struct ggml_context * ctx,\n         struct ggml_tensor * cur,\n         struct ggml_tensor * up,\n         struct ggml_tensor * up_b,\n         struct ggml_tensor * gate,\n         struct ggml_tensor * gate_b,\n         struct ggml_tensor * down_t,\n         struct ggml_tensor * down_b,\n         struct ggml_tensor * pre_w1,\n         struct ggml_tensor * pre_w2,\n         struct ggml_tensor * pred_inpl,\n         struct ggml_tensor * gpu_index,\n         struct ggml_tensor * gpu_bucket,\n         struct ggml_tensor * gate_gpu,\n         struct ggml_tensor * down_gpu,\n         struct ggml_tensor * up_gpu,\n            llm_ffn_op_type   type_op,\n          llm_ffn_gate_type   type_gate,\n                     double   gpu_offload_ratio,\n   const llm_build_cb_short & cb_outer) {\n    bool full_gpu = gpu_offload_ratio >= 1.0;\n    ggml_tensor * ffn_input = cur;\n\n    llm_build_cb_short cb = [&cb_outer](struct ggml_tensor * tensor, const char * name) {\n        cb_outer(tensor, name);\n#if defined(GGML_USE_CUBLAS)\n        // Determine offloading based on src[0] (weight for both mul and axpy)\n        bool operates_on_gpu = tensor->src[0]->backend == GGML_BACKEND_GPU;\n        if (operates_on_gpu) {\n            ggml_cuda_assign_buffers_no_alloc(tensor);\n        }\n#endif\n    };\n\n    // prepare sparse idx\n    ggml_tensor * idx = ggml_mul_mat(ctx, pre_w1, pred_inpl);\n    cb(idx, \"mlp_pre_hidden\");\n    idx = ggml_relu(ctx, idx);\n    cb(idx, \"mlp_pre_relu\");\n    idx = ggml_mul_mat(ctx, pre_w2, idx);\n    // If the FFN layer is not fully offloaded, we need to transfer the sparsity index\n    // back to the CPU to avoid synchronization issues.\n    (full_gpu ? cb : cb_outer)(idx, \"mlp_pre_out\");\n\n    auto act_fn = [&](ggml_tensor * tensor, const char * name) {\n        switch (type_op) {\n            case LLM_FFN_RELU:\n                {\n                    tensor = ggml_relu(ctx, tensor);\n                    cb(tensor, name);\n                } break;\n            default:\n                GGML_ASSERT(false && \"unsupported activation function\");\n        }\n        return tensor;\n    };\n\n    // FFN up\n    struct ggml_tensor * up_out = llm_build_sparse_mul_mat(ctx, up, ffn_input, idx, up_gpu, gpu_index, gpu_bucket, cb_outer, \"up\", full_gpu);\n    if (up_b) {\n        up_out = ggml_add(ctx, up_out, up_b);\n        cb(up_out, \"ffn_up_b\");\n    }\n\n    struct ggml_tensor * gate_out = nullptr;\n    if (gate) {\n        ggml_tensor * gate_input = (type_gate == LLM_FFN_PAR || type_gate == LLM_FFN_SYM) ? ffn_input : up_out;\n        gate_out = llm_build_sparse_mul_mat(ctx, gate, gate_input, idx, gate_gpu, gpu_index, gpu_bucket, cb_outer, \"gate\", full_gpu);\n        if (gate_b) {\n            gate_out = ggml_add(ctx, gate_out, gate_b);\n            cb(gate_out, \"ffn_gate_b\");\n        }\n        switch (type_gate) {\n            case LLM_FFN_PAR:\n                {\n                    ggml_tensor * act_gate = act_fn(gate_out, \"ffn_gate_act\");\n                    cur = ggml_mul(ctx, act_gate, up_out);\n                    cb(cur, \"ffn_gate_par\");\n                } break;\n            case LLM_FFN_SYM:\n                {\n                    ggml_tensor * act_gate = act_fn(gate_out, \"ffn_gate_act\");\n                    ggml_tensor * act_up = act_fn(up_out, \"ffn_up_act\");\n                    cur = ggml_mul(ctx, act_gate, act_up);\n                    cb(cur, \"ffn_gate_sym\");\n                } break;\n            case LLM_FFN_SEQ:\n                {\n                    cur = act_fn(gate_out, \"ffn_gate_act\");\n                } break;\n            default: GGML_ASSERT(false && \"unsupported gate type\");\n        }\n    } else {\n        cur = act_fn(up_out, \"ffn_up_act\");\n    }\n\n    cur = llm_build_sparse_axpy(ctx, down_t, cur, idx, down_gpu, gpu_index, gpu_bucket, cb_outer, \"down\", full_gpu);\n\n    if (down_b) {\n        cur = ggml_add(ctx, cur, down_b);\n        cb(cur, \"ffn_down_b\");\n    }\n\n    return cur;\n}\n\n\nstatic ggml_tensor * k_cpy = nullptr;\nstatic ggml_tensor * v_cpy = nullptr;\n\n// if max_alibi_bias > 0 then apply ALiBi\nstatic struct ggml_tensor * llm_build_kqv(\n        struct ggml_context * ctx,\n        const llama_hparams & hparams,\n       const llama_kv_cache & kv,\n         struct ggml_tensor * wo,\n         struct ggml_tensor * wo_b,\n         struct ggml_tensor * q_cur,\n         struct ggml_tensor * kq_scale,\n         struct ggml_tensor * kq_mask,\n                    int64_t   n_ctx,\n                    int32_t   n_tokens,\n                    int32_t   n_kv,\n                    float     max_alibi_bias,\n         const llm_build_cb & cb,\n                    int       il) {\n    const int64_t n_embd      = hparams.n_embd;\n    const int64_t n_head      = hparams.n_head;\n    const int64_t n_head_kv   = hparams.n_head_kv;\n    const int64_t n_embd_head = hparams.n_embd_head();\n    const int64_t n_embd_gqa  = hparams.n_embd_gqa();\n\n    struct ggml_tensor * q = ggml_permute(ctx, q_cur, 0, 2, 1, 3);\n    cb(q, \"q\", il);\n\n    struct ggml_tensor * k =\n        ggml_view_3d(ctx, kv.k,\n                n_embd_head, n_kv, n_head_kv,\n                ggml_element_size(kv.k)*n_embd_gqa,\n                ggml_element_size(kv.k)*n_embd_head,\n                ggml_element_size(kv.k)*n_embd_gqa*n_ctx*il);\n    cb(k, \"k\", il);\n    if (k_cpy != nullptr) {\n        k->src[1] = k_cpy;\n    }\n\n    struct ggml_tensor * kq = ggml_mul_mat(ctx, k, q);\n    cb(kq, \"kq\", il);\n\n    kq = ggml_scale(ctx, kq, kq_scale);\n    cb(kq, \"kq_scaled\", il);\n\n    if (max_alibi_bias > 0.0f) {\n        // TODO: n_head or n_head_kv\n        // TODO: K-shift is likely not working\n        // TODO: change to ggml_add\n        kq = ggml_alibi(ctx, kq, /*n_past*/ 0, n_head, max_alibi_bias);\n        cb(kq, \"kq_scaled_alibi\", il);\n    }\n\n    kq = ggml_add(ctx, kq, kq_mask);\n    cb(kq, \"kq_masked\", il);\n\n    kq = ggml_soft_max(ctx, kq);\n    cb(kq, \"kq_soft_max\", il);\n\n    // split cached v into n_head heads\n    struct ggml_tensor * v =\n        ggml_view_3d(ctx, kv.v,\n                n_kv, n_embd_head, n_head_kv,\n                ggml_element_size(kv.v)*n_ctx,\n                ggml_element_size(kv.v)*n_ctx*n_embd_head,\n                ggml_element_size(kv.v)*n_ctx*n_embd_gqa*il);\n    cb(v, \"v\", il);\n    if (v_cpy != nullptr) {\n        v->src[1] = v_cpy;\n    }\n\n    struct ggml_tensor * kqv = ggml_mul_mat(ctx, v, kq);\n    cb(kqv, \"kqv\", il);\n\n    struct ggml_tensor * kqv_merged = ggml_permute(ctx, kqv, 0, 2, 1, 3);\n    cb(kqv_merged, \"kqv_merged\", il);\n\n    struct ggml_tensor * cur = ggml_cont_2d(ctx, kqv_merged, n_embd, n_tokens);\n    cb(cur, \"kqv_merged_cont\", il);\n\n    cur = ggml_mul_mat(ctx, wo, cur);\n    if (wo_b) {\n        cb(cur, \"kqv_wo\", il);\n    }\n\n    if (wo_b) {\n        cur = ggml_add(ctx, cur, wo_b);\n    }\n\n    return cur;\n}\n\nconst llm_build_cb no_offload_cb = [](struct ggml_tensor * cur, const char * name, int nl) {\n    ggml_set_name(cur, name);\n};\n\nstruct llm_build_context {\n    const llama_model    & model;\n    const llama_hparams  & hparams;\n    const llama_cparams  & cparams;\n    const llama_batch    & batch;\n    const llama_kv_cache & kv_self;\n\n    const int64_t n_embd;\n    const int64_t n_layer;\n    const int64_t n_ctx;       // user-specified context size (can be different from n_ctx_train)\n    const int64_t n_head;\n    const int64_t n_head_kv;\n    const int64_t n_embd_head;\n    const int64_t n_embd_gqa;\n\n    const float freq_base;\n    const float freq_scale;\n    const float ext_factor;\n    const float attn_factor;\n    const float beta_fast;\n    const float beta_slow;\n    const float norm_eps;\n    const float norm_rms_eps;\n\n    const int32_t n_tokens;\n    const int32_t n_kv;     // size of KV cache to consider (n_kv <= n_ctx)\n    const int32_t kv_head;  // index of where we store new KV data in the cache\n    const int32_t n_orig_ctx;\n\n    const bool do_rope_shift;\n\n    llm_build_cb cb;\n\n    llama_buffer & buf_compute;\n\n    struct ggml_context * ctx0 = nullptr;\n\n    // TODO: consider making the entire interface noexcept\n    llm_build_context(\n        llama_context  & lctx,\n    const llama_batch  & batch,\n    const llm_build_cb & cb,\n                  bool   worst_case) :\n        model         (lctx.model),\n        hparams       (model.hparams),\n        cparams       (lctx.cparams),\n        batch         (batch),\n        kv_self       (lctx.kv_self),\n        n_embd        (hparams.n_embd),\n        n_layer       (hparams.n_layer),\n        n_ctx         (cparams.n_ctx),\n        n_head        (hparams.n_head),\n        n_head_kv     (hparams.n_head_kv),\n        n_embd_head   (hparams.n_embd_head()),\n        n_embd_gqa    (hparams.n_embd_gqa()),\n        freq_base     (cparams.rope_freq_base),\n        freq_scale    (cparams.rope_freq_scale),\n        ext_factor    (cparams.yarn_ext_factor),\n        attn_factor   (cparams.yarn_attn_factor),\n        beta_fast     (cparams.yarn_beta_fast),\n        beta_slow     (cparams.yarn_beta_slow),\n        norm_eps      (hparams.f_norm_eps),\n        norm_rms_eps  (hparams.f_norm_rms_eps),\n        n_tokens      (batch.n_tokens),\n        n_kv          (worst_case ? n_ctx            : kv_self.n),\n        kv_head       (worst_case ? n_ctx - n_tokens : kv_self.head),\n        n_orig_ctx    (cparams.n_yarn_orig_ctx),\n        do_rope_shift (worst_case || kv_self.has_shift),\n        cb            (cb),\n        buf_compute   (lctx.buf_compute) {\n            GGML_ASSERT(!!kv_self.ctx);\n\n            // all initializations should be done in init()\n        }\n\n    void init() {\n        struct ggml_init_params params = {\n            /*.mem_size   =*/ buf_compute.size,\n            /*.mem_buffer =*/ buf_compute.data,\n            /*.no_alloc   =*/ true,\n        };\n        \n\n        ctx0 = ggml_init(params);\n    }\n\n    void free() {\n        if (ctx0) {\n            ggml_free(ctx0);\n            ctx0 = nullptr;\n        }\n    }\n\n    struct ggml_cgraph * build_llama_variants() {\n        struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, LLAMA_MAX_NODES, false);\n\n        GGML_ASSERT(n_embd_head == hparams.n_rot);\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"inp_embd\", -1);\n\n        // inp_pos - contains the positions\n        struct ggml_tensor * inp_pos = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens);\n        cb(inp_pos, \"inp_pos\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        // KQ_mask (mask for 1 head, it will be broadcasted to all heads)\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        // shift the entire K-cache if needed\n        if (do_rope_shift) {\n            llm_build_k_shift(ctx0, hparams, cparams, kv_self, gf, LLM_ROPE, n_ctx, n_embd_head, freq_base, freq_scale, cb);\n        }\n\n        for (int il = 0; il < n_layer; ++il) {\n            struct ggml_tensor * inpSA = inpL;\n\n            // norm\n            cur = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm, NULL,\n                    LLM_NORM_RMS, cb, il);\n            cb(cur, \"attn_norm\", il);\n\n            // self-attention\n            {\n                // compute Q and K and RoPE them\n                struct ggml_tensor * Qcur = ggml_mul_mat(ctx0, model.layers[il].wq, cur);\n                cb(Qcur, \"Qcur\", il);\n\n                struct ggml_tensor * Kcur = ggml_mul_mat(ctx0, model.layers[il].wk, cur);\n                cb(Kcur, \"Kcur\", il);\n\n                struct ggml_tensor * Vcur = ggml_mul_mat(ctx0, model.layers[il].wv, cur);\n                cb(Vcur, \"Vcur\", il);\n\n                Qcur = ggml_rope_custom(\n                    ctx0, ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens), inp_pos,\n                    n_embd_head, 0, 0, n_orig_ctx, freq_base, freq_scale,\n                    ext_factor, attn_factor, beta_fast, beta_slow\n                );\n                cb(Qcur, \"Qcur\", il);\n\n                Kcur = ggml_rope_custom(\n                    ctx0, ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens), inp_pos,\n                    n_embd_head, 0, 0, n_orig_ctx, freq_base, freq_scale,\n                    ext_factor, attn_factor, beta_fast, beta_slow\n                );\n                cb(Kcur, \"Kcur\", il);\n\n                std::tie(k_cpy, v_cpy) = llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, NULL,\n                        Qcur, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, -1.0f, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            struct ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n            cb(ffn_inp, \"ffn_inp\", il);\n\n            // feed-forward network\n            {\n                cur = llm_build_norm(ctx0, ffn_inp, hparams,\n                        model.layers[il].ffn_norm, NULL,\n                        LLM_NORM_RMS, cb, il);\n                llm_ffn_gate_type gate_type = model.arch == LLM_ARCH_BAMBOO ? LLM_FFN_SYM : LLM_FFN_PAR;\n\n                if (llama_use_sparse_inference(&model)) {\n                    llm_build_cb_short cbs = [&](ggml_tensor * cur, const char * name) {\n                        std::string name_str = std::string(name) + \"-\" + std::to_string(il);\n                        ggml_set_name(cur, name_str.c_str());\n                    };\n                    // We only offload the ffn input to GPU if all neurons are offloaded\n                    if (model.layers[il].gpu_offload_ratio >= 1.) {\n                        cb(cur, \"ffn_norm\", il);\n                    } else {\n                        cbs(cur, \"ffn_norm\");\n                    }\n                    cur = llm_build_ffn_sparse(ctx0, cur,\n                        model.layers[il].ffn_up,   NULL,\n                        model.layers[il].ffn_gate, NULL,\n                        model.layers[il].ffn_down_t, NULL,\n                        model.layers[il].mlp_pre_w1,\n                        model.layers[il].mlp_pre_w2,\n                        ffn_inp, // as for now, llama's pred use the same input as the ffn\n                        model.layers[il].gpu_idx, \n                        model.layers[il].gpu_bucket, model.layers[il].ffn_gate_gpu, model.layers[il].ffn_down_gpu, model.layers[il].ffn_up_gpu,\n                        LLM_FFN_RELU, gate_type, model.layers[il].gpu_offload_ratio, cbs);\n                } else {\n                    // fallback to dense\n                    cb(cur, \"ffn_norm\", il);\n                    llm_ffn_op_type   act_type = model.arch == LLM_ARCH_BAMBOO ? LLM_FFN_RELU : LLM_FFN_SILU;\n                    cur = llm_build_ffn(ctx0, cur,\n                        model.layers[il].ffn_up,   NULL,\n                        model.layers[il].ffn_gate, NULL,\n                        model.layers[il].ffn_down, NULL,\n                        act_type, gate_type, cb, il);\n                }\n            }\n\n            cur = ggml_add(ctx0, cur, ffn_inp);\n            cb(cur, \"l_out\", il);\n\n            // input for next layer\n            inpL = cur;\n        }\n\n        cur = inpL;\n\n        cur = llm_build_norm(ctx0, cur, hparams,\n                model.output_norm, NULL,\n                LLM_NORM_RMS, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        // lm_head\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n\n    struct ggml_cgraph * build_baichuan() {\n        struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, LLAMA_MAX_NODES, false);\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"inp_embd\", -1);\n\n        // inp_pos - contains the positions\n        struct ggml_tensor * inp_pos = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens);\n        cb(inp_pos, \"inp_pos\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        // KQ_mask (mask for 1 head, it will be broadcasted to all heads)\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        // shift the entire K-cache if needed\n        if (do_rope_shift) {\n            llm_build_k_shift(ctx0, hparams, cparams, kv_self, gf, LLM_ROPE, n_ctx, n_embd_head, freq_base, freq_scale, cb);\n        }\n\n        for (int il = 0; il < n_layer; ++il) {\n            struct ggml_tensor * inpSA = inpL;\n\n            cur = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm, NULL,\n                    LLM_NORM_RMS, cb, il);\n            cb(cur, \"attn_norm\", il);\n\n            // self-attention\n            {\n                struct ggml_tensor * Qcur = ggml_mul_mat(ctx0, model.layers[il].wq, cur);\n                cb(Qcur, \"Qcur\", il);\n\n                struct ggml_tensor * Kcur = ggml_mul_mat(ctx0, model.layers[il].wk, cur);\n                cb(Kcur, \"Kcur\", il);\n\n                struct ggml_tensor * Vcur = ggml_mul_mat(ctx0, model.layers[il].wv, cur);\n                cb(Vcur, \"Vcur\", il);\n\n                switch (model.type) {\n                    case MODEL_7B:\n                        Qcur = ggml_rope_custom(\n                            ctx0, ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head, n_tokens), inp_pos,\n                            n_embd_head, 0, 0, n_orig_ctx, freq_base, freq_scale,\n                            ext_factor, attn_factor, beta_fast, beta_slow\n                        );\n                        Kcur = ggml_rope_custom(\n                            ctx0, ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens), inp_pos,\n                            n_embd_head, 0, 0, n_orig_ctx, freq_base, freq_scale,\n                            ext_factor, attn_factor, beta_fast, beta_slow\n                        );\n                        break;\n                    case MODEL_13B:\n                        Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd/n_head, n_head, n_tokens);\n                        Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd/n_head, n_head, n_tokens);\n                        break;\n                    default:\n                        GGML_ASSERT(false);\n                }\n                cb(Qcur, \"Qcur\", il);\n                cb(Kcur, \"Kcur\", il);\n\n                llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                // apply ALiBi for 13B model\n                const float max_alibi_bias = model.type == MODEL_13B ? 8.0f : -1.0f;\n\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, NULL,\n                        Qcur, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, max_alibi_bias, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            struct ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n            cb(ffn_inp, \"ffn_inp\", il);\n\n            // feed-forward network\n            {\n                cur = llm_build_norm(ctx0, ffn_inp, hparams,\n                        model.layers[il].ffn_norm, NULL,\n                        LLM_NORM_RMS, cb, il);\n                cb(cur, \"ffn_norm\", il);\n\n                cur = llm_build_ffn(ctx0, cur,\n                        model.layers[il].ffn_up,   NULL,\n                        model.layers[il].ffn_gate, NULL,\n                        model.layers[il].ffn_down, NULL,\n                        LLM_FFN_SILU, LLM_FFN_PAR, cb, il);\n                cb(cur, \"ffn_out\", il);\n            }\n\n            cur = ggml_add(ctx0, cur, ffn_inp);\n            cb(cur, \"l_out\", il);\n\n            // input for next layer\n            inpL = cur;\n        }\n\n        cur = inpL;\n\n        cur = llm_build_norm(ctx0, cur, hparams,\n                model.output_norm, NULL,\n                LLM_NORM_RMS, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        // lm_head\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n\n    struct ggml_cgraph * build_falcon() {\n        struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, LLAMA_MAX_NODES, false);\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"inp_embd\", -1);\n\n        // inp_pos - contains the positions\n        struct ggml_tensor * inp_pos = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens);\n        cb(inp_pos, \"inp_pos\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        // KQ_mask (mask for 1 head, it will be broadcasted to all heads)\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        // shift the entire K-cache if needed\n        if (do_rope_shift) {\n            llm_build_k_shift(ctx0, hparams, cparams, kv_self, gf, LLM_ROPE_NEOX, n_ctx, n_embd_head, freq_base, freq_scale, cb);\n        }\n\n        for (int il = 0; il < n_layer; ++il) {\n            struct ggml_tensor * attn_norm;\n\n            attn_norm = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm,\n                    model.layers[il].attn_norm_b,\n                    LLM_NORM, cb, il);\n\n            // self-attention\n            {\n                if (model.layers[il].attn_norm_2) {\n                    // Falcon-40B\n                    cur = llm_build_norm(ctx0, inpL, hparams,\n                            model.layers[il].attn_norm_2,\n                            model.layers[il].attn_norm_2_b,\n                            LLM_NORM, cb, il);\n                    cb(cur, \"attn_norm_2\", il);\n                } else {\n                    cur = attn_norm;\n                }\n\n                cur = ggml_mul_mat(ctx0, model.layers[il].wqkv, cur);\n                cb(cur, \"wqkv\", il);\n\n                struct ggml_tensor * Qcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd,     n_tokens, cur->nb[1], 0*sizeof(float)*(n_embd)));\n                struct ggml_tensor * Kcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd_gqa, n_tokens, cur->nb[1], 1*sizeof(float)*(n_embd)));\n                struct ggml_tensor * Vcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd_gqa, n_tokens, cur->nb[1], 1*sizeof(float)*(n_embd + n_embd_gqa)));\n\n                cb(Qcur, \"Qcur\", il);\n                cb(Kcur, \"Kcur\", il);\n                cb(Vcur, \"Vcur\", il);\n\n                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n                Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n\n                // using mode = 2 for neox mode\n                Qcur = ggml_rope_custom(\n                    ctx0, Qcur, inp_pos, n_embd_head, 2, 0, n_orig_ctx,\n                    freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow\n                );\n                cb(Qcur, \"Qcur\", il);\n\n                Kcur = ggml_rope_custom(\n                    ctx0, Kcur, inp_pos, n_embd_head, 2, 0, n_orig_ctx,\n                    freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow\n                );\n                cb(Kcur, \"Kcur\", il);\n\n                std::tie(k_cpy, v_cpy) = llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, NULL,\n                        Qcur, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, -1.0f, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            struct ggml_tensor * ffn_inp = cur;\n\n            // feed forward\n            if (llama_use_sparse_inference(&model)) {\n                llm_build_cb_short cbs = [&](ggml_tensor * cur, const char * name) {\n                    std::string name_str = std::string(name) + \"-\" + std::to_string(il);\n                    ggml_set_name(cur, name_str.c_str());\n                };\n                // We only offload the ffn input to GPU if all neurons are offloaded\n                if (model.layers[il].gpu_offload_ratio >= 1.) {\n                    cb(cur, \"attn_norm\", il);\n                } else {\n                    cbs(cur, \"attn_norm\");\n                }\n                cur = llm_build_ffn_sparse(ctx0, attn_norm,\n                    model.layers[il].ffn_up,   NULL,\n                    NULL, NULL,\n                    model.layers[il].ffn_down_t, NULL,\n                    model.layers[il].mlp_pre_w1,\n                    model.layers[il].mlp_pre_w2,\n                    inpL, // Falcon uses the layer's input as the pred input\n                    model.layers[il].gpu_idx, \n                    model.layers[il].gpu_bucket, \n                    model.layers[il].ffn_gate_gpu, model.layers[il].ffn_down_gpu, model.layers[il].ffn_up_gpu,\n                    LLM_FFN_RELU, LLM_FFN_SEQ, model.layers[il].gpu_offload_ratio, cbs);\n            } else {\n                cb(attn_norm, \"attn_norm\", il);\n                cur = llm_build_ffn(ctx0, attn_norm, // !! use the attn norm, not the result\n                        model.layers[il].ffn_up,   NULL,\n                        NULL,                      NULL,\n                        model.layers[il].ffn_down_t, NULL,\n                        LLM_FFN_RELU, LLM_FFN_SEQ, cb, il);\n                cb(cur, \"ffn_out\", il);\n            }\n\n            cur = ggml_add(ctx0, cur, ffn_inp);\n            cb(cur, \"l_out\", il);\n\n            cur = ggml_add(ctx0, cur, inpL);\n            cb(cur, \"l_out\", il);\n\n            // input for next layer\n            inpL = cur;\n        }\n\n        cur = inpL;\n\n        // norm\n        cur = llm_build_norm(ctx0, cur, hparams,\n                model.output_norm,\n                model.output_norm_b,\n                LLM_NORM, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n\n\n\n\n\n    struct ggml_cgraph * build_starcoder() {\n        struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, LLAMA_MAX_NODES, false);\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * pos;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"inp_embd\", -1);\n\n        // inp_pos - contains the positions\n        struct ggml_tensor * inp_pos = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens);\n        cb(inp_pos, \"inp_pos\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        // KQ_mask (mask for 1 head, it will be broadcasted to all heads)\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        pos = ggml_get_rows(ctx0, model.pos_embd, inp_pos);\n        cb(pos, \"pos_embd\", -1);\n\n        inpL = ggml_add(ctx0, inpL, pos);\n        cb(inpL, \"inpL\", -1);\n\n        for (int il = 0; il < n_layer; ++il) {\n            cur = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm,\n                    model.layers[il].attn_norm_b,\n                    LLM_NORM, cb, il);\n            cb(cur, \"attn_norm\", il);\n\n            // self-attention\n            {\n                cur = ggml_mul_mat(ctx0, model.layers[il].wqkv, cur);\n                cb(cur, \"wqkv\", il);\n\n                cur = ggml_add(ctx0, cur, model.layers[il].bqkv);\n                cb(cur, \"bqkv\", il);\n\n                struct ggml_tensor * Qcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd,     n_tokens, cur->nb[1], 0*sizeof(float)*(n_embd)));\n                struct ggml_tensor * Kcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd_gqa, n_tokens, cur->nb[1], 1*sizeof(float)*(n_embd)));\n                struct ggml_tensor * Vcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd_gqa, n_tokens, cur->nb[1], 1*sizeof(float)*(n_embd + n_embd_gqa)));\n\n                cb(Qcur, \"Qcur\", il);\n                cb(Kcur, \"Kcur\", il);\n                cb(Vcur, \"Vcur\", il);\n\n                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head, n_tokens);\n\n                llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, model.layers[il].bo,\n                        Qcur, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, -1.0f, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            // add the input\n            struct ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpL);\n            cb(ffn_inp, \"ffn_inp\", il);\n\n            // FF\n            {\n                cur = llm_build_norm(ctx0, ffn_inp, hparams,\n                        model.layers[il].ffn_norm,\n                        model.layers[il].ffn_norm_b,\n                        LLM_NORM, cb, il);\n                cb(cur, \"ffn_norm\", il);\n\n                cur = llm_build_ffn(ctx0, cur,\n                        model.layers[il].ffn_up,   model.layers[il].ffn_up_b,\n                        NULL,                      NULL,\n                        model.layers[il].ffn_down, model.layers[il].ffn_down_b,\n                        LLM_FFN_GELU, LLM_FFN_SEQ, cb, il);\n                cb(cur, \"ffn_out\", il);\n            }\n\n            inpL = ggml_add(ctx0, cur, ffn_inp);\n            cb(inpL, \"l_out\", il);\n        }\n\n        cur = llm_build_norm(ctx0, inpL, hparams,\n                model.output_norm,\n                model.output_norm_b,\n                LLM_NORM, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n\n    struct ggml_cgraph * build_persimmon() {\n        struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, LLAMA_MAX_NODES, false);\n\n        const int64_t n_rot = n_embd_head / 2;\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"imp_embd\", -1);\n\n        struct ggml_tensor * inp_pos = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens);\n        cb(inp_pos, \"inp_pos\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        if (do_rope_shift) {\n            llm_build_k_shift(ctx0, hparams, cparams, kv_self, gf, LLM_ROPE_NEOX, n_ctx, n_embd_head, freq_base, freq_scale, cb);\n        }\n\n        for (int il = 0; il < n_layer; ++il) {\n            struct ggml_tensor * residual = inpL;\n\n            cur = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm,\n                    model.layers[il].attn_norm_b,\n                    LLM_NORM, cb, il);\n            cb(cur, \"attn_norm\", il);\n\n            // self attention\n            {\n                cur = ggml_mul_mat(ctx0, model.layers[il].wqkv, cur);\n                cb(cur, \"wqkv\", il);\n\n                cur = ggml_add(ctx0, cur, model.layers[il].bqkv);\n                cb(cur, \"bqkv\", il);\n\n                // split qkv\n                GGML_ASSERT(n_head_kv == n_head);\n\n                struct ggml_tensor * tmpqkv = ggml_reshape_4d(ctx0, cur, n_embd_head, 3, n_head, n_tokens);\n                cb(tmpqkv, \"tmpqkv\", il);\n\n                struct ggml_tensor * tmpqkv_perm = ggml_cont(ctx0, ggml_permute(ctx0, tmpqkv, 0, 3, 1, 2));\n                cb(tmpqkv_perm, \"tmpqkv\", il);\n\n                struct ggml_tensor * tmpq = ggml_view_3d(\n                        ctx0, tmpqkv_perm, n_embd_head, n_head, n_tokens,\n                        ggml_element_size(tmpqkv_perm) * n_embd_head,\n                        ggml_element_size(tmpqkv_perm) * n_embd_head * n_head,\n                        0\n                        );\n                cb(tmpq, \"tmpq\", il);\n\n                struct ggml_tensor * tmpk = ggml_view_3d(\n                        ctx0, tmpqkv_perm, n_embd_head, n_head, n_tokens,\n                        ggml_element_size(tmpqkv_perm) * n_embd_head,\n                        ggml_element_size(tmpqkv_perm) * n_embd_head * n_head,\n                        ggml_element_size(tmpqkv_perm) * n_embd_head * n_head * n_tokens\n                        );\n                cb(tmpk, \"tmpk\", il);\n\n                // Q/K Layernorm\n                tmpq = llm_build_norm(ctx0, tmpq, hparams,\n                        model.layers[il].attn_q_norm,\n                        model.layers[il].attn_q_norm_b,\n                        LLM_NORM, cb, il);\n                cb(tmpq, \"tmpq\", il);\n\n                tmpk = llm_build_norm(ctx0, tmpk, hparams,\n                        model.layers[il].attn_k_norm,\n                        model.layers[il].attn_k_norm_b,\n                        LLM_NORM, cb, il);\n                cb(tmpk, \"tmpk\", il);\n\n                // RoPE the first n_rot of q/k, pass the other half, and concat.\n                struct ggml_tensor * qrot = ggml_view_3d(\n                        ctx0, tmpq, n_rot, n_head, n_tokens,\n                        ggml_element_size(tmpq) * n_embd_head,\n                        ggml_element_size(tmpq) * n_embd_head * n_head,\n                        0\n                        );\n                cb(qrot, \"qrot\", il);\n\n                struct ggml_tensor * krot = ggml_view_3d(\n                        ctx0, tmpk, n_rot, n_head, n_tokens,\n                        ggml_element_size(tmpk) * n_embd_head,\n                        ggml_element_size(tmpk) * n_embd_head * n_head,\n                        0\n                        );\n                cb(krot, \"krot\", il);\n\n                // get the second half of tmpq, e.g tmpq[n_rot:, :, :]\n                struct ggml_tensor * qpass = ggml_view_3d(\n                        ctx0, tmpq, n_rot, n_head, n_tokens,\n                        ggml_element_size(tmpq) * n_embd_head,\n                        ggml_element_size(tmpq) * n_embd_head * n_head,\n                        ggml_element_size(tmpq) * n_rot\n                        );\n                cb(qpass, \"qpass\", il);\n\n                struct ggml_tensor * kpass = ggml_view_3d(\n                        ctx0, tmpk, n_rot, n_head, n_tokens,\n                        ggml_element_size(tmpk) * n_embd_head,\n                        ggml_element_size(tmpk) * n_embd_head * n_head,\n                        ggml_element_size(tmpk) * n_rot\n                        );\n                cb(kpass, \"kpass\", il);\n\n                struct ggml_tensor * qrotated = ggml_rope_custom(\n                    ctx0, qrot, inp_pos, n_rot, 2, 0, n_orig_ctx,\n                    freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow\n                );\n                cb(qrotated, \"qrotated\", il);\n\n                struct ggml_tensor * krotated = ggml_rope_custom(\n                    ctx0, krot, inp_pos, n_rot, 2, 0, n_orig_ctx,\n                    freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow\n                );\n                cb(krotated, \"krotated\", il);\n\n                // ggml currently only supports concatenation on dim=2\n                // so we need to permute qrot, qpass, concat, then permute back.\n                qrotated = ggml_cont(ctx0, ggml_permute(ctx0, qrotated, 2, 1, 0, 3));\n                cb(qrotated, \"qrotated\", il);\n\n                krotated = ggml_cont(ctx0, ggml_permute(ctx0, krotated, 2, 1, 0, 3));\n                cb(krotated, \"krotated\", il);\n\n                qpass = ggml_cont(ctx0, ggml_permute(ctx0, qpass, 2, 1, 0, 3));\n                cb(qpass, \"qpass\", il);\n\n                kpass = ggml_cont(ctx0, ggml_permute(ctx0, kpass, 2, 1, 0, 3));\n                cb(kpass, \"kpass\", il);\n\n                struct ggml_tensor * Qcur = ggml_concat(ctx0, qrotated, qpass);\n                cb(Qcur, \"Qcur\", il);\n\n                struct ggml_tensor * Kcur = ggml_concat(ctx0, krotated, kpass);\n                cb(Kcur, \"Kcur\", il);\n\n                struct ggml_tensor * Q = ggml_cont(ctx0, ggml_permute(ctx0, Qcur, 2, 1, 0, 3));\n                cb(Q, \"Q\", il);\n\n                Kcur = ggml_cont(ctx0, ggml_permute(ctx0, Kcur, 2, 1, 0, 3));\n                cb(Kcur, \"Kcur\", il);\n\n                struct ggml_tensor * Vcur = ggml_view_3d(\n                        ctx0, tmpqkv_perm, n_embd_head, n_head, n_tokens,\n                        ggml_element_size(tmpqkv_perm) * n_embd_head,\n                        ggml_element_size(tmpqkv_perm) * n_embd_head * n_head,\n                        ggml_element_size(tmpqkv_perm) * n_embd_head * n_head * n_tokens * 2\n                        );\n                cb(Vcur, \"Vcur\", il);\n\n                llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                // TODO: not tested, could be broken\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, model.layers[il].bo,\n                        Q, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, -1.0f, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            struct ggml_tensor * ffn_inp = ggml_add(ctx0, residual, cur);\n            cb(ffn_inp, \"ffn_inp\", il);\n\n            // feed-forward network\n            {\n                cur = llm_build_norm(ctx0, ffn_inp, hparams,\n                        model.layers[il].ffn_norm,\n                        model.layers[il].ffn_norm_b,\n                        LLM_NORM, cb, il);\n                cb(cur, \"ffn_norm\", il);\n\n                cur = llm_build_ffn(ctx0, cur,\n                        model.layers[il].ffn_up,   model.layers[il].ffn_up_b,\n                        NULL,                      NULL,\n                        model.layers[il].ffn_down, model.layers[il].ffn_down_b,\n                        LLM_FFN_RELU_SQR, LLM_FFN_SEQ, cb, il);\n                cb(cur, \"ffn_out\", il);\n            }\n\n            cur = ggml_add(ctx0, cur, ffn_inp);\n            cb(cur, \"l_out\", il);\n\n            inpL = cur;\n        }\n\n        cur = inpL;\n\n        cur = llm_build_norm(ctx0, cur, hparams,\n                model.output_norm,\n                model.output_norm_b,\n                LLM_NORM, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n\n    struct ggml_cgraph * build_refact() {\n        struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, LLAMA_MAX_NODES, false);\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"inp_embd\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        // KQ_mask (mask for 1 head, it will be broadcasted to all heads)\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        for (int il = 0; il < n_layer; ++il) {\n            struct ggml_tensor * inpSA = inpL;\n\n            cur = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm, NULL,\n                    LLM_NORM_RMS, cb, il);\n            cb(cur, \"attn_norm\", il);\n\n            // self-attention\n            {\n                struct ggml_tensor * Qcur = ggml_mul_mat(ctx0, model.layers[il].wq, cur);\n                cb(Qcur, \"Qcur\", il);\n\n                struct ggml_tensor * Kcur = ggml_mul_mat(ctx0, model.layers[il].wk, cur);\n                cb(Kcur, \"Kcur\", il);\n\n                struct ggml_tensor * Vcur = ggml_mul_mat(ctx0, model.layers[il].wv, cur);\n                cb(Vcur, \"Vcur\", il);\n\n                Kcur = ggml_reshape_3d(ctx0, Kcur, n_embd_head, n_head_kv, n_tokens);\n                cb(Kcur, \"Kcur\", il);\n\n                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head,    n_tokens);\n                cb(Qcur, \"Qcur\", il);\n\n                llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, NULL,\n                        Qcur, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, 8.0f, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            struct ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n            cb(ffn_inp, \"ffn_inp\", il);\n\n            // feed-forward network\n            {\n                cur = llm_build_norm(ctx0, ffn_inp, hparams,\n                        model.layers[il].ffn_norm, NULL,\n                        LLM_NORM_RMS, cb, il);\n                cb(cur, \"ffn_norm\", il);\n\n                cur = llm_build_ffn(ctx0, cur,\n                        model.layers[il].ffn_up,   NULL,\n                        model.layers[il].ffn_gate, NULL,\n                        model.layers[il].ffn_down, NULL,\n                        LLM_FFN_SILU, LLM_FFN_PAR, cb, il);\n                cb(cur, \"ffn_out\", il);\n            }\n\n            cur = ggml_add(ctx0, cur, ffn_inp);\n            cb(cur, \"l_out\", il);\n\n            // input for next layer\n            inpL = cur;\n        }\n\n        cur = inpL;\n\n        cur = llm_build_norm(ctx0, cur, hparams,\n                model.output_norm, NULL,\n                LLM_NORM_RMS, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        // lm_head\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n\n    struct ggml_cgraph * build_bloom() {\n        struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, LLAMA_MAX_NODES, false);\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"inp_embd\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        // KQ_mask (mask for 1 head, it will be broadcasted to all heads)\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        inpL = llm_build_norm(ctx0, inpL, hparams,\n                model.tok_norm,\n                model.tok_norm_b,\n                LLM_NORM, cb, -1);\n        cb(inpL, \"inp_norm\", -1);\n\n        for (int il = 0; il < n_layer; ++il) {\n            cur = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm,\n                    model.layers[il].attn_norm_b,\n                    LLM_NORM, cb, il);\n            cb(cur, \"attn_norm\", il);\n\n            // self-attention\n            {\n                cur = ggml_mul_mat(ctx0, model.layers[il].wqkv, cur);\n                cb(cur, \"wqkv\", il);\n\n                cur = ggml_add(ctx0, cur, model.layers[il].bqkv);\n                cb(cur, \"bqkv\", il);\n\n                struct ggml_tensor * Qcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd,     n_tokens, cur->nb[1], 0*sizeof(float)*(n_embd)));\n                struct ggml_tensor * Kcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd_gqa, n_tokens, cur->nb[1], 1*sizeof(float)*(n_embd)));\n                struct ggml_tensor * Vcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd_gqa, n_tokens, cur->nb[1], 1*sizeof(float)*(n_embd + n_embd_gqa)));\n\n                cb(Qcur, \"Qcur\", il);\n                cb(Kcur, \"Kcur\", il);\n                cb(Vcur, \"Vcur\", il);\n\n                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head, n_tokens);\n\n                llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, model.layers[il].bo,\n                        Qcur, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, 8.0f, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            // Add the input\n            struct ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpL);\n            cb(ffn_inp, \"ffn_inp\", il);\n\n            // FF\n            {\n                cur = llm_build_norm(ctx0, ffn_inp, hparams,\n                        model.layers[il].ffn_norm,\n                        model.layers[il].ffn_norm_b,\n                        LLM_NORM, cb, il);\n                cb(cur, \"ffn_norm\", il);\n\n                cur = llm_build_ffn(ctx0, cur,\n                        model.layers[il].ffn_up,   model.layers[il].ffn_up_b,\n                        NULL,                      NULL,\n                        model.layers[il].ffn_down, model.layers[il].ffn_down_b,\n                        LLM_FFN_GELU, LLM_FFN_SEQ, cb, il);\n                cb(cur, \"ffn_out\", il);\n            }\n\n            inpL = ggml_add(ctx0, cur, ffn_inp);\n            cb(inpL, \"l_out\", il);\n        }\n\n        cur = llm_build_norm(ctx0, inpL, hparams,\n                model.output_norm,\n                model.output_norm_b,\n                LLM_NORM, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n\n    struct ggml_cgraph * build_mpt() {\n        struct ggml_cgraph * gf = ggml_new_graph_custom(ctx0, LLAMA_MAX_NODES, false);\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"inp_embd\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        // KQ_mask (mask for 1 head, it will be broadcasted to all heads)\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        for (int il = 0; il < n_layer; ++il) {\n            struct ggml_tensor * attn_norm;\n\n            attn_norm = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm,\n                    NULL,\n                    LLM_NORM, cb, il);\n            cb(attn_norm, \"attn_norm\", il);\n\n            // self-attention\n            {\n                cur = attn_norm;\n\n                cur = ggml_mul_mat(ctx0, model.layers[il].wqkv, cur);\n                cb(cur, \"wqkv\", il);\n\n                if (hparams.f_clamp_kqv > 0.0f) {\n                    cur = ggml_clamp(ctx0, cur, -hparams.f_clamp_kqv, hparams.f_clamp_kqv);\n                    cb(cur, \"wqkv_clamped\", il);\n                }\n\n                struct ggml_tensor * Qcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd,     n_tokens, cur->nb[1], 0*sizeof(float)*(n_embd)));\n                struct ggml_tensor * Kcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd_gqa, n_tokens, cur->nb[1], 1*sizeof(float)*(n_embd)));\n                struct ggml_tensor * Vcur = ggml_cont(ctx0, ggml_view_2d(ctx0, cur, n_embd_gqa, n_tokens, cur->nb[1], 1*sizeof(float)*(n_embd + n_embd_gqa)));\n\n                cb(Qcur, \"Qcur\", il);\n                cb(Kcur, \"Kcur\", il);\n                cb(Vcur, \"Vcur\", il);\n\n                Qcur = ggml_reshape_3d(ctx0, Qcur, n_embd_head, n_head, n_tokens);\n\n                llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, NULL,\n                        Qcur, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, hparams.f_max_alibi_bias, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            // Add the input\n            struct ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpL);\n            cb(ffn_inp, \"ffn_inp\", il);\n\n            // feed forward\n            {\n                cur = llm_build_norm(ctx0, ffn_inp, hparams,\n                        model.layers[il].ffn_norm,\n                        NULL,\n                        LLM_NORM, cb, il);\n                cb(cur, \"ffn_norm\", il);\n\n                cur = llm_build_ffn(ctx0, cur,\n                        model.layers[il].ffn_up,   NULL,\n                        NULL,                      NULL,\n                        model.layers[il].ffn_down, NULL,\n                        LLM_FFN_GELU, LLM_FFN_SEQ, cb, il);\n                cb(cur, \"ffn_out\", il);\n            }\n\n            cur = ggml_add(ctx0, cur, ffn_inp);\n            cb(cur, \"l_out\", il);\n\n            // input for next layer\n            inpL = cur;\n        }\n\n        cur = inpL;\n\n        cur = llm_build_norm(ctx0, cur, hparams,\n                model.output_norm,\n                NULL,\n                LLM_NORM, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n\n    struct ggml_cgraph * build_stablelm() {\n        struct ggml_cgraph * gf = ggml_new_graph(ctx0);\n\n        struct ggml_tensor * cur;\n        struct ggml_tensor * inpL;\n\n        inpL = llm_build_inp_embd(ctx0, hparams, batch, model.tok_embd, cb);\n        cb(inpL, \"inp_embd\", -1);\n\n        // inp_pos - contains the positions\n        struct ggml_tensor * inp_pos = ggml_new_tensor_1d(ctx0, GGML_TYPE_I32, n_tokens);\n        cb(inp_pos, \"inp_pos\", -1);\n\n        // KQ_scale\n        struct ggml_tensor * KQ_scale = ggml_new_tensor_1d(ctx0, GGML_TYPE_F32, 1);\n        cb(KQ_scale, \"KQ_scale\", -1);\n\n        // KQ_mask (mask for 1 head, it will be broadcasted to all heads)\n        struct ggml_tensor * KQ_mask = ggml_new_tensor_3d(ctx0, GGML_TYPE_F32, n_kv, n_tokens, 1);\n        cb(KQ_mask, \"KQ_mask\", -1);\n\n        // shift the entire K-cache if needed\n        if (do_rope_shift) {\n            llm_build_k_shift(ctx0, hparams, cparams, kv_self, gf, LLM_ROPE_NEOX, n_ctx, hparams.n_rot, freq_base, freq_scale, cb);\n        }\n\n        for (int il = 0; il < n_layer; ++il) {\n            struct ggml_tensor * inpSA = inpL;\n\n            // norm\n            cur = llm_build_norm(ctx0, inpL, hparams,\n                    model.layers[il].attn_norm,\n                    model.layers[il].attn_norm_b,\n                    LLM_NORM, cb, il);\n            cb(cur, \"attn_norm\", il);\n\n            // self-attention\n            {\n                // compute Q and K and RoPE them\n                struct ggml_tensor * tmpq = ggml_mul_mat(ctx0, model.layers[il].wq, cur);\n                cb(tmpq, \"tmpq\", il);\n\n                struct ggml_tensor * tmpk = ggml_mul_mat(ctx0, model.layers[il].wk, cur);\n                cb(tmpk, \"tmpk\", il);\n\n                struct ggml_tensor * Vcur = ggml_mul_mat(ctx0, model.layers[il].wv, cur);\n                cb(Vcur, \"Vcur\", il);\n\n                // RoPE the first n_rot of q/k, pass the other half, and concat.\n                struct ggml_tensor * qrot = ggml_cont(ctx0, ggml_view_3d(\n                        ctx0, tmpq, hparams.n_rot, n_head, n_tokens,\n                        ggml_element_size(tmpq) * n_embd_head,\n                        ggml_element_size(tmpq) * n_embd_head * n_head,\n                        0\n                        ));\n                cb(qrot, \"qrot\", il);\n\n                struct ggml_tensor * krot = ggml_cont(ctx0, ggml_view_3d(\n                        ctx0, tmpk, hparams.n_rot, n_head, n_tokens,\n                        ggml_element_size(tmpk) * n_embd_head,\n                        ggml_element_size(tmpk) * n_embd_head * n_head_kv,\n                        0\n                        ));\n                cb(krot, \"krot\", il);\n\n                // get the second half of tmpq, e.g tmpq[n_rot:, :, :]\n                struct ggml_tensor * qpass = ggml_view_3d(\n                        ctx0, tmpq, (n_embd_head - hparams.n_rot), n_head, n_tokens,\n                        ggml_element_size(tmpq) * n_embd_head,\n                        ggml_element_size(tmpq) * n_embd_head * n_head,\n                        ggml_element_size(tmpq) * hparams.n_rot\n                        );\n                cb(qpass, \"qpass\", il);\n\n                struct ggml_tensor * kpass = ggml_view_3d(\n                        ctx0, tmpk, (n_embd_head - hparams.n_rot), n_head_kv, n_tokens,\n                        ggml_element_size(tmpk) * (n_embd_head),\n                        ggml_element_size(tmpk) * (n_embd_head) * n_head_kv,\n                        ggml_element_size(tmpk) * hparams.n_rot\n                        );\n                cb(kpass, \"kpass\", il);\n\n                struct ggml_tensor * qrotated = ggml_rope_custom(\n                    ctx0, qrot, inp_pos, hparams.n_rot, 2, 0, n_orig_ctx,\n                    freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow\n                );\n                cb(qrotated, \"qrotated\", il);\n\n                struct ggml_tensor * krotated = ggml_rope_custom(\n                    ctx0, krot, inp_pos, hparams.n_rot, 2, 0, n_orig_ctx,\n                    freq_base, freq_scale, ext_factor, attn_factor, beta_fast, beta_slow\n                );\n                cb(krotated, \"krotated\", il);\n\n                // ggml currently only supports concatenation on dim=2\n                // so we need to permute qrot, qpass, concat, then permute back.\n                qrotated = ggml_cont(ctx0, ggml_permute(ctx0, qrotated, 2, 1, 0, 3));\n                cb(qrotated, \"qrotated\", il);\n\n                krotated = ggml_cont(ctx0, ggml_permute(ctx0, krotated, 2, 1, 0, 3));\n                cb(krotated, \"krotated\", il);\n\n                qpass = ggml_cont(ctx0, ggml_permute(ctx0, qpass, 2, 1, 0, 3));\n                cb(qpass, \"qpass\", il);\n\n                kpass = ggml_cont(ctx0, ggml_permute(ctx0, kpass, 2, 1, 0, 3));\n                cb(kpass, \"kpass\", il);\n\n                struct ggml_tensor * Qcur = ggml_concat(ctx0, qrotated, qpass);\n                cb(Qcur, \"Qcur\", il);\n\n                struct ggml_tensor * Kcur = ggml_concat(ctx0, krotated, kpass);\n                cb(Kcur, \"Kcur\", il);\n\n                struct ggml_tensor * Q = ggml_cont(ctx0, ggml_permute(ctx0, Qcur, 2, 1, 0, 3));\n                cb(Q, \"Q\", il);\n\n                Kcur = ggml_cont(ctx0, ggml_permute(ctx0, Kcur, 2, 1, 0, 3));\n                cb(Kcur, \"Kcur\", il);\n\n                llm_build_kv_store(ctx0, hparams, kv_self, gf, Kcur, Vcur, n_ctx, n_tokens, kv_head, cb, il);\n\n                cur = llm_build_kqv(ctx0, hparams, kv_self,\n                        model.layers[il].wo, NULL,\n                        Q, KQ_scale, KQ_mask, n_ctx, n_tokens, n_kv, -1.0f, cb, il);\n                cb(cur, \"kqv_out\", il);\n            }\n\n            struct ggml_tensor * ffn_inp = ggml_add(ctx0, cur, inpSA);\n            cb(ffn_inp, \"ffn_inp\", il);\n\n            // feed-forward network\n            {\n                cur = llm_build_norm(ctx0, ffn_inp, hparams,\n                        model.layers[il].ffn_norm,\n                        model.layers[il].ffn_norm_b,\n                        LLM_NORM, cb, il);\n                cb(cur, \"ffn_norm\", il);\n\n                cur = llm_build_ffn(ctx0, cur,\n                        model.layers[il].ffn_up,   NULL,\n                        model.layers[il].ffn_gate, NULL,\n                        model.layers[il].ffn_down, NULL,\n                        LLM_FFN_SILU, LLM_FFN_PAR, cb, il);\n                cb(cur, \"ffn_out\", il);\n            }\n\n            cur = ggml_add(ctx0, cur, ffn_inp);\n            cb(cur, \"l_out\", il);\n\n            // input for next layer\n            inpL = cur;\n        }\n\n        cur = inpL;\n\n        cur = llm_build_norm(ctx0, cur, hparams,\n                model.output_norm,\n                model.output_norm_b,\n                LLM_NORM, cb, -1);\n        cb(cur, \"result_norm\", -1);\n\n        // lm_head\n        cur = ggml_mul_mat(ctx0, model.output, cur);\n        cb(cur, \"result_output\", -1);\n\n        ggml_build_forward_expand(gf, cur);\n\n        return gf;\n    }\n};\n\n//\n// tensor offloading helpers\n//\n// TODO: will be removed with backend v2\n\nenum llm_offload_func_e {\n    OFFLOAD_FUNC_NOP,\n    OFFLOAD_FUNC,\n    OFFLOAD_FUNC_KQ,\n    OFFLOAD_FUNC_V,\n    OFFLOAD_FUNC_NR,\n    OFFLOAD_FUNC_EMB,\n    OFFLOAD_FUNC_OUT,\n};\n\n// TODO: will be removed with backend v2\nstruct llm_offload_trie {\n    struct node {\n        ~node() {\n            for (int i = 0; i < 256; ++i) {\n                if (children[i]) {\n                    delete children[i];\n                }\n            }\n        }\n\n        node * children[256] = { nullptr };\n        llm_offload_func_e func = OFFLOAD_FUNC_NOP;\n    };\n\n    llm_offload_trie() {\n        root = new node;\n    }\n\n    llm_offload_trie(const std::unordered_map<const char *, llm_offload_func_e> & map) {\n        root = new node;\n\n        for (const auto & kv : map) {\n            add(kv.first, kv.second);\n        }\n    }\n\n    ~llm_offload_trie() {\n        delete root;\n    }\n\n    void add(const char * name, llm_offload_func_e func) {\n        node * cur = root;\n\n        for (int i = 0; ; ++i) {\n            const uint8_t c = name[i];\n\n            if (!c) {\n                break;\n            }\n\n            if (!cur->children[c]) {\n                cur->children[c] = new node;\n            }\n\n            cur = cur->children[c];\n        }\n\n        cur->func = func;\n    }\n\n    llm_offload_func_e find(const char * name) const {\n        const node * cur = root;\n\n        for (int i = 0; ; ++i) {\n            const uint8_t c = name[i];\n\n            if (!c) {\n                break;\n            }\n\n            if (!cur->children[c]) {\n                return OFFLOAD_FUNC_NOP;\n            }\n\n            cur = cur->children[c];\n        }\n\n        return cur->func;\n    }\n\n    node * root = nullptr;\n};\n\n// TODO: will be removed with backend v2\nstatic const std::unordered_map<const char *, llm_offload_func_e> k_offload_map = {\n  //{ \"inp_tokens\",                 OFFLOAD_FUNC_NR  }, // TODO: missing K-quants get_rows kernel\n  //{ \"inp_embd\",                   OFFLOAD_FUNC_NR  }, // TODO: missing K-quants get_rows kernel\n    { \"pos_embd\",                   OFFLOAD_FUNC_NR  },\n\n    { \"inp_pos\",                    OFFLOAD_FUNC_KQ  }, // this is often used for KQ ops (e.g. rope)\n    { \"KQ_scale\",                   OFFLOAD_FUNC_KQ  },\n    { \"KQ_mask\",                    OFFLOAD_FUNC_KQ  },\n    { \"K_shift\",                    OFFLOAD_FUNC_KQ  },\n    { \"K_shifted\",                  OFFLOAD_FUNC_KQ  },\n\n    { \"inp_norm\",                   OFFLOAD_FUNC_NR  },\n    { \"inp_norm_w\",                 OFFLOAD_FUNC_NR  },\n    { \"inp_norm_wb\",                OFFLOAD_FUNC_NR  },\n\n    { \"norm\",                       OFFLOAD_FUNC     },\n    { \"norm_w\",                     OFFLOAD_FUNC     },\n    { \"norm_wb\",                    OFFLOAD_FUNC     },\n\n    { \"attn_norm\",                  OFFLOAD_FUNC     },\n    { \"attn_norm_2\",                OFFLOAD_FUNC     },\n\n    { \"wqkv\",                       OFFLOAD_FUNC_KQ  },\n    { \"bqkv\",                       OFFLOAD_FUNC_KQ  },\n    { \"wqkv_clamped\",               OFFLOAD_FUNC_KQ  },\n\n    { \"tmpk\",                       OFFLOAD_FUNC_KQ  },\n    { \"tmpq\",                       OFFLOAD_FUNC_KQ  },\n    { \"tmpv\",                       OFFLOAD_FUNC_V   },\n    { \"Kcur\",                       OFFLOAD_FUNC_KQ  },\n    { \"Qcur\",                       OFFLOAD_FUNC_KQ  },\n    { \"Vcur\",                       OFFLOAD_FUNC_V   },\n\n    { \"krot\",                       OFFLOAD_FUNC_KQ  },\n    { \"qrot\",                       OFFLOAD_FUNC_KQ  },\n    { \"kpass\",                      OFFLOAD_FUNC_KQ  },\n    { \"qpass\",                      OFFLOAD_FUNC_KQ  },\n    { \"krotated\",                   OFFLOAD_FUNC_KQ  },\n    { \"qrotated\",                   OFFLOAD_FUNC_KQ  },\n\n    { \"q\",                          OFFLOAD_FUNC_KQ  },\n    { \"k\",                          OFFLOAD_FUNC_KQ  },\n    { \"kq\",                         OFFLOAD_FUNC_KQ  },\n    { \"kq_scaled\",                  OFFLOAD_FUNC_KQ  },\n    { \"kq_scaled_alibi\",            OFFLOAD_FUNC_KQ  },\n    { \"kq_masked\",                  OFFLOAD_FUNC_KQ  },\n    { \"kq_soft_max\",                OFFLOAD_FUNC_V   },\n    { \"v\",                          OFFLOAD_FUNC_V   },\n    { \"kqv\",                        OFFLOAD_FUNC_V   },\n    { \"kqv_merged\",                 OFFLOAD_FUNC_V   },\n    { \"kqv_merged_cont\",            OFFLOAD_FUNC_V   },\n    { \"kqv_wo\",                     OFFLOAD_FUNC_V   },\n    { \"kqv_out\",                    OFFLOAD_FUNC_V   },\n\n    { \"ffn_inp\",                    OFFLOAD_FUNC     },\n    { \"ffn_norm\",                   OFFLOAD_FUNC     },\n\n    { \"ffn_up\",                     OFFLOAD_FUNC     },\n    { \"ffn_up_b\",                   OFFLOAD_FUNC     },\n    { \"ffn_gate\",                   OFFLOAD_FUNC     },\n    { \"ffn_gate_b\",                 OFFLOAD_FUNC     },\n    { \"ffn_gate_par\",               OFFLOAD_FUNC     },\n    { \"ffn_down\",                   OFFLOAD_FUNC     },\n    { \"ffn_down_b\",                 OFFLOAD_FUNC     },\n    { \"ffn_out\",                    OFFLOAD_FUNC     },\n\n    { \"ffn_silu\",                   OFFLOAD_FUNC     },\n    { \"ffn_gelu\",                   OFFLOAD_FUNC     },\n    { \"ffn_relu\",                   OFFLOAD_FUNC     },\n    { \"ffn_sqr(relu)\",              OFFLOAD_FUNC     },\n\n    { \"l_out\",                      OFFLOAD_FUNC     },\n\n    { \"result_norm\",                OFFLOAD_FUNC_EMB },\n    { \"result_output\",              OFFLOAD_FUNC_OUT },\n};\n\nstatic llm_offload_trie k_offload_func_trie(k_offload_map);\n\nstatic struct ggml_cgraph * llama_build_graph(\n         llama_context & lctx,\n     const llama_batch & batch) {\n    const auto & model = lctx.model;\n\n    // check if we should build the worst-case graph (for memory measurement)\n    const bool worst_case = ggml_allocr_is_measure(lctx.alloc);\n\n    // keep track of the input that has already been allocated\n    bool alloc_inp_tokens   = false;\n    bool alloc_inp_embd     = false;\n    bool alloc_inp_pos      = false;\n    bool alloc_inp_KQ_scale = false;\n    bool alloc_inp_KQ_mask  = false;\n    bool alloc_inp_K_shift  = false;\n\n#ifdef GGML_USE_CUBLAS\n    const bool do_offload = true;\n#else\n    const bool do_offload = true; // TODO: set to false after finishing refactoring\n#endif\n\n    int n_non_view = 0; // number of non-view tensors that have been processed by the callback\n\n    // this callback allows us to apply custom logic to each tensor (e.g. ggml-alloc, offloading, etc.)\n    // TODO: will be removed with backend v2\n\n    // For sparse deriv, we offload layers from the starting layer to the end.\n    // For dense deriv, we offload layers from the end to the starting layer.\n    bool offload_starting_layers = lctx.model.sparse_deriv;\n\n    llm_build_cb cb = [&](struct ggml_tensor * cur, const char * name, int il) {\n        if (il >= 0) {\n            ggml_format_name(cur, \"%s-%d\", name, il);\n        } else {\n            ggml_set_name(cur, name);\n        }\n\n        //\n        // allocate input tensors and set input data\n        //\n        // TODO: will be removed with backend v2\n\n        if (!alloc_inp_tokens && strcmp(name, \"inp_tokens\") == 0) {\n            ggml_allocr_alloc(lctx.alloc, cur);\n\n            if (!ggml_allocr_is_measure(lctx.alloc) && batch.token) {\n                const int64_t n_tokens = cur->ne[0];\n\n                memcpy(cur->data, batch.token, n_tokens*ggml_element_size(cur));\n            }\n\n            alloc_inp_tokens = true;\n        }\n\n        if (!alloc_inp_embd && strcmp(name, \"inp_embd\") == 0) {\n            ggml_allocr_alloc(lctx.alloc, cur);\n\n            if (!ggml_allocr_is_measure(lctx.alloc) && batch.embd) {\n                const int64_t n_embd   = cur->ne[0];\n                const int64_t n_tokens = cur->ne[1];\n\n                memcpy(cur->data, batch.embd, n_tokens*n_embd*ggml_element_size(cur));\n            }\n\n            alloc_inp_embd = true;\n        }\n\n        if (!alloc_inp_pos && strcmp(name, \"inp_pos\") == 0) {\n            ggml_allocr_alloc(lctx.alloc, cur);\n\n            if (!ggml_allocr_is_measure(lctx.alloc) && batch.pos) {\n                const int64_t n_tokens = cur->ne[0];\n\n                int32_t * data = (int32_t *) cur->data;\n\n                for (int i = 0; i < n_tokens; ++i) {\n                    data[i] = batch.pos[i];\n                }\n            }\n\n            alloc_inp_pos = true;\n        }\n\n        if (!alloc_inp_KQ_scale && strcmp(name, \"KQ_scale\") == 0) {\n            ggml_allocr_alloc(lctx.alloc, cur);\n\n            if (!ggml_allocr_is_measure(lctx.alloc)) {\n                const int64_t n_embd_head = model.hparams.n_embd_head();\n                ggml_set_f32(cur, 1.0f/sqrtf(float(n_embd_head)));\n            }\n\n            alloc_inp_KQ_scale = true;\n        }\n\n        if (!alloc_inp_KQ_mask && strcmp(name, \"KQ_mask\") == 0) {\n            ggml_allocr_alloc(lctx.alloc, cur);\n\n            if (!ggml_allocr_is_measure(lctx.alloc)) {\n                const int64_t n_kv     = cur->ne[0];\n                const int64_t n_tokens = cur->ne[1];\n\n                float * data = (float *) cur->data;\n                memset(data, 0, ggml_nbytes(cur));\n\n                for (int h = 0; h < 1; ++h) {\n                    for (int j = 0; j < n_tokens; ++j) {\n                        const llama_pos    pos    = batch.pos[j];\n                        const llama_seq_id seq_id = batch.seq_id[j][0];\n\n                        for (int i = 0; i < n_kv; ++i) {\n                            if (!lctx.kv_self.cells[i].has_seq_id(seq_id) || lctx.kv_self.cells[i].pos > pos) {\n                                data[h*(n_kv*n_tokens) + j*n_kv + i] = -INFINITY;\n                            }\n                        }\n                    }\n                }\n            }\n\n            alloc_inp_KQ_mask = true;\n        }\n\n        if (!alloc_inp_K_shift && strcmp(name, \"K_shift\") == 0) {\n            ggml_allocr_alloc(lctx.alloc, cur);\n\n            if (!ggml_allocr_is_measure(lctx.alloc)) {\n                const int64_t n_ctx = cur->ne[0];\n\n                int32_t * data = (int32_t *) cur->data;\n\n                for (int i = 0; i < n_ctx; ++i) {\n                    data[i] = lctx.kv_self.cells[i].delta;\n                }\n            }\n\n            alloc_inp_K_shift = true;\n        }\n\n        // view tensors are not processed further\n        if (cur->view_src != nullptr) {\n            return;\n        }\n\n        if (cur->op != GGML_OP_NONE) {\n            n_non_view++;\n        }\n\n        //\n        // offload layers\n        //\n        // TODO: will be removed with backend v2\n\n//#define LLAMA_OFFLOAD_DEBUG\n\n        if (!do_offload) {\n            return;\n        }\n\n        const int n_layer = model.hparams.n_layer;\n\n        const int n_gpu_layers = model.n_gpu_layers;\n        const int i_gpu_start  = n_layer - n_gpu_layers;\n        const int i_gpu_end = n_gpu_layers;\n\n        // should we offload the final norm? yes if we are not computing embeddings\n        const bool offload_emb = lctx.embedding.empty();\n\n        static const std::unordered_map<llm_offload_func_e, std::string, std::hash<int>> k_offload_func_name = {\n            { OFFLOAD_FUNC_NOP, \"CPU\" },\n            { OFFLOAD_FUNC_OUT, \"CPU\" },\n#ifdef GGML_USE_CUBLAS\n            { OFFLOAD_FUNC,     \"GPU (CUDA)\" },\n            { OFFLOAD_FUNC_KQ,  \"GPU (CUDA) KQ\" },\n            { OFFLOAD_FUNC_V,   \"GPU (CUDA) V\" },\n            { OFFLOAD_FUNC_NR,  \"GPU (CUDA) NR\" },\n            { OFFLOAD_FUNC_EMB, \"GPU (CUDA) EMB\" },\n#else\n            { OFFLOAD_FUNC,     \"CPU\" },\n            { OFFLOAD_FUNC_KQ,  \"CPU\" },\n            { OFFLOAD_FUNC_V,   \"CPU\" },\n            { OFFLOAD_FUNC_NR,  \"CPU\" },\n            { OFFLOAD_FUNC_EMB, \"CPU\" },\n#endif // GGML_USE_CUBLAS\n        };\n\n        // check the global map for what offload function to use for this tensor\n        llm_offload_func_e func_e = k_offload_func_trie.find(name);\n\n        if (func_e == OFFLOAD_FUNC_NOP) {\n#ifdef LLAMA_OFFLOAD_DEBUG\n            // if a tensor hasn't been offloaded, we warn the user\n            if (worst_case) {\n                LLAMA_LOG_WARN(\"%s: %32s: not offloaded (ref: %s)\\n\", __func__,\n                        cur->name, \"https://github.com/ggerganov/llama.cpp/pull/3837\");\n            }\n#endif\n\n            return;\n        }\n\n        // count the number of layers and respect the provided n_gpu_layers\n        switch (func_e) {\n            case OFFLOAD_FUNC_NOP:\n            case OFFLOAD_FUNC_OUT:\n                break;\n            case OFFLOAD_FUNC:\n                if (n_gpu_layers < n_layer) {\n                    if ((offload_starting_layers && il >= i_gpu_end)\n                        || (!offload_starting_layers && il < i_gpu_start)) {\n                        func_e = OFFLOAD_FUNC_NOP;\n                    }\n                }\n                break;\n            case OFFLOAD_FUNC_NR:\n                if (n_gpu_layers <= n_layer + 0) {\n                    func_e = OFFLOAD_FUNC_NOP;\n                }\n                break;\n            case OFFLOAD_FUNC_V:\n                if (n_gpu_layers <= n_layer + 1) {\n                    func_e = OFFLOAD_FUNC_NOP;\n                }\n                break;\n            case OFFLOAD_FUNC_KQ:\n                if (n_gpu_layers <= n_layer + 2) {\n                    func_e = OFFLOAD_FUNC_NOP;\n                }\n                break;\n            case OFFLOAD_FUNC_EMB:\n                if (!offload_emb || n_gpu_layers < n_layer) {\n                    func_e = OFFLOAD_FUNC_NOP;\n                }\n                break;\n            default: GGML_ASSERT(false);\n        }\n\n        offload_func_t func = ggml_offload_nop;\n\n        // this is needed for compatibility with Metal for example\n#ifdef GGML_USE_CUBLAS\n        static offload_func_t ggml_offload_gpu = ggml_cuda_assign_buffers_no_alloc;\n#else\n        static offload_func_t ggml_offload_gpu = ggml_offload_nop;\n#endif\n\n        switch (func_e) {\n            case OFFLOAD_FUNC_NOP:\n            case OFFLOAD_FUNC_OUT: func = ggml_offload_nop; break;\n            case OFFLOAD_FUNC:\n            case OFFLOAD_FUNC_KQ:\n            case OFFLOAD_FUNC_V:\n            case OFFLOAD_FUNC_NR:\n            case OFFLOAD_FUNC_EMB: func = ggml_offload_gpu; break;\n            default: GGML_ASSERT(false);\n        }\n\n        // apply offload function to the tensor\n        func(cur);\n\n#ifdef LLAMA_OFFLOAD_DEBUG\n        if (worst_case) {\n            LLAMA_LOG_INFO(\"%s: %32s: %s\\n\", __func__, cur->name, k_offload_func_name.at(func_e).c_str());\n        }\n#endif\n    };\n\n    struct ggml_cgraph * result = NULL;\n\n    struct llm_build_context llm(lctx, batch, cb, worst_case);\n\n    llm.init();\n\n    switch (model.arch) {\n        case LLM_ARCH_LLAMA:\n        case LLM_ARCH_BAMBOO:\n            {\n                result = llm.build_llama_variants();\n            } break;\n        case LLM_ARCH_BAICHUAN:\n            {\n                result = llm.build_baichuan();\n            } break;\n        case LLM_ARCH_FALCON:\n            {\n                result = llm.build_falcon();\n            } break;\n        case LLM_ARCH_STARCODER:\n            {\n                result = llm.build_starcoder();\n            } break;\n        case LLM_ARCH_PERSIMMON:\n            {\n                result = llm.build_persimmon();\n            } break;\n        case LLM_ARCH_REFACT:\n            {\n                result = llm.build_refact();\n            } break;\n        case LLM_ARCH_BLOOM:\n            {\n                result = llm.build_bloom();\n            } break;\n        case LLM_ARCH_MPT:\n            {\n                result = llm.build_mpt();\n            } break;\n         case LLM_ARCH_STABLELM:\n            {\n                result = llm.build_stablelm();\n            } break;\n        default:\n            GGML_ASSERT(false);\n    }\n\n    llm.free();\n\n    if (worst_case) {\n        int n_non_view_total = 0;\n\n        for (int i = 0; i < result->n_nodes; ++i) {\n            if (result->nodes[i]->view_src == nullptr) {\n                n_non_view_total++;\n            }\n        }\n\n        LLAMA_LOG_INFO(\"%s: non-view tensors processed: %d/%d\\n\", __func__, n_non_view, n_non_view_total);\n\n        if (n_non_view != n_non_view_total) {\n            LLAMA_LOG_WARN(\"%s: ****************************************************************\\n\", __func__);\n            LLAMA_LOG_WARN(\"%s: not all non-view tensors have been processed with a callback\\n\",     __func__);\n            LLAMA_LOG_WARN(\"%s: this can indicate an inefficiency in the graph implementation\\n\",    __func__);\n            LLAMA_LOG_WARN(\"%s: build with LLAMA_OFFLOAD_DEBUG for more info\\n\",                     __func__);\n            LLAMA_LOG_WARN(\"%s: ref: https://github.com/ggerganov/llama.cpp/pull/3837\\n\",            __func__);\n            LLAMA_LOG_WARN(\"%s: ****************************************************************\\n\", __func__);\n        }\n    }\n\n    return result;\n}\n\n// decode a batch of tokens by evaluating the transformer\n//\n//   - lctx:      llama context\n//   - batch:     batch to evaluate\n//\n// return 0 on success\n// return positive int on warning\n// return negative int on error\n//\nstatic int llama_decode_internal(\n         llama_context & lctx,\n           llama_batch   batch) {\n    const uint32_t n_tokens = batch.n_tokens;\n\n    if (n_tokens == 0) {\n        LLAMA_LOG_ERROR(\"%s: n_tokens == 0\", __func__);\n        return -1;\n    }\n\n    const auto & model   = lctx.model;\n    const auto & hparams = model.hparams;\n    const auto & cparams = lctx.cparams;\n\n    const auto n_batch = cparams.n_batch;\n\n    GGML_ASSERT(n_tokens <= n_batch);\n\n    int n_threads = n_tokens == 1 ? cparams.n_threads : cparams.n_threads_batch;\n    GGML_ASSERT((!batch.token && batch.embd) || (batch.token && !batch.embd)); // NOLINT\n\n    const int64_t t_start_us = ggml_time_us();\n\n#ifdef GGML_USE_MPI\n    // TODO: needs fix after #3228\n    GGML_ASSERT(false && \"not implemented\");\n    //ggml_mpi_eval_init(lctx.ctx_mpi, &n_tokens, &n_past, &n_threads);\n#endif\n\n    GGML_ASSERT(n_threads > 0);\n\n    auto & kv_self = lctx.kv_self;\n\n    GGML_ASSERT(!!kv_self.ctx);\n\n    const int64_t n_embd  = hparams.n_embd;\n    const int64_t n_vocab = hparams.n_vocab;\n\n    // helpers for smoother batch API transistion\n    // after deprecating the llama_eval calls, these will be removed\n    std::vector<llama_pos> pos;\n\n    std::vector<int32_t>                   n_seq_id;\n    std::vector<llama_seq_id *>            seq_id_arr;\n    std::vector<std::vector<llama_seq_id>> seq_id;\n\n    if (batch.pos == nullptr) {\n        pos.resize(n_tokens);\n        for (uint32_t i = 0; i < n_tokens; i++) {\n            pos[i] = batch.all_pos_0 + i*batch.all_pos_1;\n        }\n\n        batch.pos = pos.data();\n    }\n\n    if (batch.seq_id == nullptr) {\n        n_seq_id.resize(n_tokens);\n        seq_id.resize(n_tokens);\n        seq_id_arr.resize(n_tokens);\n        for (uint32_t i = 0; i < n_tokens; i++) {\n            n_seq_id[i] = 1;\n            seq_id[i].resize(1);\n            seq_id[i][0] = batch.all_seq_id;\n            seq_id_arr[i] = seq_id[i].data();\n        }\n\n        batch.n_seq_id = n_seq_id.data();\n        batch.seq_id = seq_id_arr.data();\n    }\n\n    if (!llama_kv_cache_find_slot(kv_self, batch)) {\n        return 1;\n    }\n\n    // a heuristic, to avoid attending the full cache if it is not yet utilized\n    // after enough generations, the benefit from this heuristic disappears\n    // if we start defragmenting the cache, the benefit from this will be more important\n    //kv_self.n = std::max(32, GGML_PAD(llama_kv_cache_cell_max(kv_self), 32));   // TODO: this might be better for CUDA?\n    kv_self.n = std::min((int32_t) cparams.n_ctx, std::max(32, llama_kv_cache_cell_max(kv_self)));\n\n    //printf(\"kv_self.n = %d\\n\", kv_self.n);\n\n    ggml_allocr_reset(lctx.alloc);\n\n    ggml_cgraph * gf = llama_build_graph(lctx, batch);\n\n    ggml_allocr_alloc_graph(lctx.alloc, gf);\n\n    struct ggml_tensor * res        = gf->nodes[gf->n_nodes - 1];\n    struct ggml_tensor * embeddings = gf->nodes[gf->n_nodes - 2];\n\n    GGML_ASSERT(strcmp(res->name,        \"result_output\") == 0);\n    GGML_ASSERT(strcmp(embeddings->name, \"result_norm\")   == 0);\n\n\n#ifdef GGML_USE_CUBLAS\n    for (int i = 0; i < gf->n_leafs; i++) {\n        ggml_tensor * node = gf->leafs[i];\n        if (node->backend == GGML_BACKEND_GPU && node->extra == NULL) {\n            ggml_cuda_assign_scratch_offset(node, (char*)node->data - (char *) lctx.buf_alloc.data);\n            ggml_cuda_copy_to_device(node);\n        }\n    }\n\n    for (int i = 0; i < gf->n_nodes; i++) {\n        ggml_tensor * node = gf->nodes[i];\n        if (node->backend == GGML_BACKEND_GPU && node->extra == NULL) {\n            ggml_cuda_assign_scratch_offset(node, (char*)node->data - (char *) lctx.buf_alloc.data);\n        }\n    }\n\n    // HACK: ggml-alloc may change the tensor backend when reusing a parent, so force output to be on the CPU here if needed\n    if (!lctx.embedding.empty()) {\n        embeddings->backend = GGML_BACKEND_CPU;\n    }\n    res->backend = GGML_BACKEND_CPU;\n#endif\n\n    // LLAMA_LOG_INFO(\"graph build time: %.3f ms (%d nodes, %d leafs)\\n\", (ggml_time_us() - t_start_us)/1000.0, gf->n_nodes, gf->n_leafs);\n\n    // for big prompts, if BLAS is enabled, it is better to use only one thread\n    // otherwise, the threads are spin-lock waiting for the BLAS calls and are degrading the performance\n    // TODO: this is mostly important for Apple Silicon where CBLAS is still performing very well\n    //       we still need some threads to process all non-mul_mat ops, but not too much to avoid interfering\n    //       with the BLAS calls. need a better solution\n    if (n_tokens >= 32 && ggml_cpu_has_blas() && !ggml_cpu_has_gpublas()) {\n        n_threads = std::min(4, n_threads);\n    }\n\n    // If all tensors can be run on the GPU then using more than 1 thread is detrimental.\n    const bool full_offload_supported =\n        model.arch == LLM_ARCH_LLAMA      ||\n        model.arch == LLM_ARCH_BAICHUAN   ||\n        model.arch == LLM_ARCH_FALCON     ||\n        model.arch == LLM_ARCH_REFACT     ||\n        model.arch == LLM_ARCH_MPT        ||\n        model.arch == LLM_ARCH_STARCODER  ||\n        model.arch == LLM_ARCH_STABLELM   ||\n        model.arch == LLM_ARCH_BAMBOO;\n\n    // const bool fully_offloaded = model.n_gpu_layers >= (int) hparams.n_layer + 3;\n    // if (ggml_cpu_has_cublas() && full_offload_supported && fully_offloaded) {\n    //     n_threads = 8;\n    // }\n\n#if GGML_USE_MPI\n    const int64_t n_layer = hparams.n_layer;\n    ggml_mpi_graph_compute_pre(lctx.ctx_mpi, gf, n_layer);\n#endif\n\n#ifdef GGML_USE_METAL\n    if (lctx.ctx_metal) {\n        ggml_metal_set_n_cb     (lctx.ctx_metal, n_threads);\n        ggml_metal_graph_compute(lctx.ctx_metal, gf);\n    } else {\n        ggml_graph_compute_helper(lctx.work_buffer, gf, n_threads);\n    }\n#else\n    ggml_graph_compute_helper(lctx.work_buffer, gf, n_threads);\n#endif\n\n#if GGML_USE_MPI\n    ggml_mpi_graph_compute_post(lctx.ctx_mpi, gf, n_layer);\n#endif\n\n    // update the kv ring buffer\n    {\n        if (kv_self.has_shift) {\n            kv_self.has_shift = false;\n            for (uint32_t i = 0; i < kv_self.size; ++i) {\n                kv_self.cells[i].delta = 0;\n            }\n        }\n\n        kv_self.head += n_tokens;\n\n        // Ensure kv cache head points to a valid index.\n        if (kv_self.head >= kv_self.size) {\n            kv_self.head = 0;\n        }\n    }\n\n#ifdef GGML_PERF\n    // print timing information per ggml operation (for debugging purposes)\n    // requires GGML_PERF to be defined\n    ggml_graph_print(gf);\n#endif\n\n    // plot the computation graph in dot format (for debugging purposes)\n    //if (n_past%100 == 0) {\n    //    ggml_graph_dump_dot(gf, NULL, \"llama.dot\");\n    //}\n\n    // extract logits\n    // TODO: do not compute and extract logits if only embeddings are needed\n    //       need to update the graphs to skip \"result_output\"\n    {\n        auto & logits_out = lctx.logits;\n\n        if (batch.logits) {\n            logits_out.resize(n_vocab * n_tokens);\n            for (uint32_t i = 0; i < n_tokens; i++) {\n                if (batch.logits[i] == 0) {\n                    continue;\n                }\n                memcpy(logits_out.data() + (n_vocab*i), (float *) ggml_get_data(res) + (n_vocab*i), sizeof(float)*n_vocab);\n            }\n        } else if (lctx.logits_all) {\n            logits_out.resize(n_vocab * n_tokens);\n            memcpy(logits_out.data(), (float *) ggml_get_data(res), sizeof(float)*n_vocab*n_tokens);\n        } else {\n            logits_out.resize(n_vocab);\n            memcpy(logits_out.data(), (float *) ggml_get_data(res) + (n_vocab*(n_tokens - 1)), sizeof(float)*n_vocab);\n        }\n    }\n\n    // extract embeddings\n    if (!lctx.embedding.empty()) {\n        auto & embedding_out = lctx.embedding;\n\n        embedding_out.resize(n_embd);\n        memcpy(embedding_out.data(), (float *) ggml_get_data(embeddings) + (n_embd*(n_tokens - 1)), sizeof(float)*n_embd);\n    }\n\n    // measure the performance only for the single-token evals\n    if (n_tokens == 1) {\n        lctx.t_eval_us += ggml_time_us() - t_start_us;\n        lctx.n_eval++;\n    }\n    else if (n_tokens > 1) {\n        lctx.t_p_eval_us += ggml_time_us() - t_start_us;\n        lctx.n_p_eval += n_tokens;\n    }\n\n    // get a more accurate load time, upon first eval\n    // TODO: fix this\n    if (!lctx.has_evaluated_once) {\n        lctx.t_load_us = ggml_time_us() - lctx.t_start_us;\n        lctx.has_evaluated_once = true;\n    }\n\n    return 0;\n}\n\n//\n// tokenizer\n//\n\nstatic enum llama_vocab_type llama_vocab_get_type(const llama_vocab & vocab) {\n    return vocab.type;\n}\n\nstatic bool llama_is_normal_token(const llama_vocab & vocab, llama_token id) {\n    return vocab.id_to_token[id].type == LLAMA_TOKEN_TYPE_NORMAL;\n}\n\nstatic bool llama_is_unknown_token(const llama_vocab & vocab, llama_token id) {\n    return vocab.id_to_token[id].type == LLAMA_TOKEN_TYPE_UNKNOWN;\n}\n\nstatic bool llama_is_control_token(const llama_vocab & vocab, llama_token id) {\n    return vocab.id_to_token[id].type == LLAMA_TOKEN_TYPE_CONTROL;\n}\n\nstatic bool llama_is_byte_token(const llama_vocab & vocab, llama_token id) {\n    return vocab.id_to_token[id].type == LLAMA_TOKEN_TYPE_BYTE;\n}\n\nstatic bool llama_is_user_defined_token(const llama_vocab& vocab, llama_token id) {\n    return vocab.id_to_token[id].type == LLAMA_TOKEN_TYPE_USER_DEFINED;\n}\n\nstatic uint8_t llama_token_to_byte(const llama_vocab& vocab, llama_token id) {\n    GGML_ASSERT(llama_is_byte_token(vocab, id));\n    const auto& token_data = vocab.id_to_token.at(id);\n    switch (llama_vocab_get_type(vocab)) {\n    case LLAMA_VOCAB_TYPE_SPM: {\n        auto buf = token_data.text.substr(3, 2);\n        return strtol(buf.c_str(), NULL, 16);\n    }\n    case LLAMA_VOCAB_TYPE_BPE: {\n        GGML_ASSERT(false);\n        return unicode_to_bytes_bpe(token_data.text);\n    }\n    default:\n        GGML_ASSERT(false);\n    }\n}\n\nstatic llama_token llama_byte_to_token(const llama_vocab & vocab, uint8_t ch) {\n    static const char * hex = \"0123456789ABCDEF\";\n    switch (llama_vocab_get_type(vocab)) {\n    case LLAMA_VOCAB_TYPE_SPM: {\n        const char buf[7] = { '<', '0', 'x', hex[ch >> 4], hex[ch & 15], '>', 0 };\n        return vocab.token_to_id.at(buf);\n    }\n    case LLAMA_VOCAB_TYPE_BPE: {\n        return vocab.token_to_id.at(bytes_to_unicode_bpe(ch));\n    }\n    default:\n        GGML_ASSERT(false);\n    }\n}\n\nstatic void llama_escape_whitespace(std::string & text) {\n    replace_all(text, \" \", \"\\xe2\\x96\\x81\");\n}\n\nstatic void llama_unescape_whitespace(std::string & word) {\n    replace_all(word, \"\\xe2\\x96\\x81\", \" \");\n}\n\nstruct llm_symbol {\n    using index = int;\n    index prev;\n    index next;\n    const char * text;\n    size_t n;\n};\n\nstatic_assert(std::is_trivially_copyable<llm_symbol>::value, \"llm_symbol is not trivially copyable\");\n\n// SPM tokenizer\n// original implementation:\n// https://github.com/ggerganov/llama.cpp/commit/074bea2eb1f1349a0118239c4152914aecaa1be4\n\nstruct llm_bigram_spm {\n    struct comparator {\n        bool operator()(llm_bigram_spm & l, llm_bigram_spm & r) {\n            return (l.score < r.score) || (l.score == r.score && l.left > r.left);\n        }\n    };\n    using queue_storage = std::vector<llm_bigram_spm>;\n    using queue = std::priority_queue<llm_bigram_spm, queue_storage, comparator>;\n    llm_symbol::index left;\n    llm_symbol::index right;\n    float score;\n    size_t size;\n};\n\nstruct llm_tokenizer_spm {\n    llm_tokenizer_spm(const llama_vocab & vocab): vocab(vocab) {}\n\n    void tokenize(const std::string & text, std::vector<llama_vocab::id> & output) {\n        // split string into utf8 chars\n        int index = 0;\n        size_t offs = 0;\n        while (offs < text.size()) {\n            llm_symbol sym;\n            size_t len = utf8_len(text[offs]);\n            sym.text = text.c_str() + offs;\n            sym.n = std::min(len, text.size() - offs);\n            offs += sym.n;\n            sym.prev = index - 1;\n            sym.next = offs == text.size() ? -1 : index + 1;\n            index++;\n            symbols.emplace_back(sym);\n        }\n\n        // seed the work queue with all possible 2-character tokens.\n        for (size_t i = 1; i < symbols.size(); ++i) {\n            try_add_bigram(i - 1, i);\n        }\n\n        // keep substituting the highest frequency pairs for as long as we can.\n        while (!work_queue.empty()) {\n            auto bigram = work_queue.top();\n            work_queue.pop();\n\n            auto & left_sym = symbols[bigram.left];\n            auto & right_sym = symbols[bigram.right];\n\n            // if one of the symbols already got merged, skip it.\n            if (left_sym.n == 0 || right_sym.n == 0 ||\n                left_sym.n + right_sym.n != bigram.size) {\n                continue;\n            }\n\n            // merge the right sym into the left one\n            left_sym.n += right_sym.n;\n            right_sym.n = 0;\n\n            //LLAMA_LOG_INFO(\"left = '%*s' size = %zu\\n\", (int) left_sym.n, left_sym.text, bigram.size);\n\n            // remove the right sym from the chain\n            left_sym.next = right_sym.next;\n            if (right_sym.next >= 0) {\n                symbols[right_sym.next].prev = bigram.left;\n            }\n\n            // find more substitutions\n            try_add_bigram(left_sym.prev, bigram.left);\n            try_add_bigram(bigram.left, left_sym.next);\n        }\n\n        for (int i = 0; i != -1; i = symbols[i].next) {\n            auto & symbol = symbols[i];\n            resegment(symbol, output);\n        }\n    }\n\nprivate:\n    void resegment(llm_symbol & symbol, std::vector<llama_vocab::id> & output) {\n        auto text = std::string(symbol.text, symbol.n);\n        auto token = vocab.token_to_id.find(text);\n\n        // Do we need to support is_unused?\n        if (token != vocab.token_to_id.end()) {\n            output.push_back((*token).second);\n            return;\n        }\n\n        const auto p = rev_merge.find(text);\n\n        if (p == rev_merge.end()) {\n            // output any symbols that did not form tokens as bytes.\n            for (int j = 0; j < (int)symbol.n; ++j) {\n                llama_vocab::id token_id = llama_byte_to_token(vocab, symbol.text[j]);\n                output.push_back(token_id);\n            }\n            return;\n        }\n\n        resegment(symbols[p->second.first],  output);\n        resegment(symbols[p->second.second], output);\n    }\n\n    void try_add_bigram(int left, int right) {\n        if (left == -1 || right == -1) {\n            return;\n        }\n\n        const std::string text = std::string(symbols[left].text, symbols[left].n + symbols[right].n);\n        auto token = vocab.token_to_id.find(text);\n\n        if (token == vocab.token_to_id.end()) {\n            return;\n        }\n\n        if (static_cast<size_t>((*token).second) >= vocab.id_to_token.size()) {\n            return;\n        }\n\n        const auto & tok_data = vocab.id_to_token[(*token).second];\n\n        llm_bigram_spm bigram;\n        bigram.left  = left;\n        bigram.right = right;\n        bigram.score = tok_data.score;\n        bigram.size  = text.size();\n\n        work_queue.push(bigram);\n\n        // Do we need to support is_unused?\n        rev_merge[text] = std::make_pair(left, right);\n    }\n\n    const llama_vocab & vocab;\n\n    std::vector<llm_symbol> symbols;\n    llm_bigram_spm::queue work_queue;\n\n    std::map<std::string, std::pair<int, int>> rev_merge;\n};\n\n// BPE tokenizer\n// adapted from https://github.com/cmp-nct/ggllm.cpp [MIT License]\n// tried to simplify unicode stuff, so most likely does not work 100% correctly!\n\n// TODO: there are a lot of common parts between spm and bpe tokenizers, should be refactored and reused\n\nstruct llm_bigram_bpe {\n    struct comparator {\n        bool operator()(const llm_bigram_bpe & l, const llm_bigram_bpe & r) const {\n            return l.rank > r.rank || (l.rank == r.rank && l.left > r.left);\n        }\n    };\n\n    using queue_storage = std::vector<llm_bigram_bpe>;\n    using queue = std::priority_queue<llm_bigram_bpe, queue_storage, comparator>;\n    llm_symbol::index left;\n    llm_symbol::index right;\n    std::string text;\n    int rank;\n    size_t size;\n};\n\nstruct llm_tokenizer_bpe {\n    llm_tokenizer_bpe(const llama_vocab & vocab): vocab(vocab) {}\n\n    void tokenize(const std::string & text, std::vector<llama_vocab::id> & output) {\n        int final_prev_index = -1;\n        auto word_collection = bpe_gpt2_preprocess(text);\n\n        symbols_final.clear();\n\n        for (auto & word : word_collection) {\n            work_queue = llm_bigram_bpe::queue();\n            symbols.clear();\n\n            int index = 0;\n            size_t offset = 0;\n\n            while (offset < word.size()) {\n                llm_symbol sym;\n                size_t char_len = std::min(word.size() - offset, (size_t) ::utf8_len(word[offset]));\n                sym.text = word.c_str() + offset;\n                sym.n = char_len;\n                offset += sym.n;\n                sym.prev = index - 1;\n                sym.next = offset == word.size() ? -1 : index + 1;\n                index++;\n                symbols.emplace_back(sym);\n            }\n            for (size_t i = 1; i < symbols.size(); ++i) {\n                add_new_bigram(i - 1, i);\n            }\n\n            // build token(s)\n            while (!work_queue.empty()) {\n                auto bigram = work_queue.top();\n                work_queue.pop();\n\n                auto & left_symbol = symbols[bigram.left];\n                auto & right_symbol = symbols[bigram.right];\n\n                if (left_symbol.n == 0 || right_symbol.n == 0) {\n                    continue;\n                }\n                std::string left_token = std::string(left_symbol.text, left_symbol.n);\n                std::string right_token = std::string(right_symbol.text, right_symbol.n);\n                if (left_token + right_token != bigram.text) {\n                    continue;  // Skip this bigram if it's outdated\n                }\n\n                // merge the right sym into the left one\n                left_symbol.n += right_symbol.n;\n                right_symbol.n = 0;\n\n                // remove the right sym from the chain\n                left_symbol.next = right_symbol.next;\n                if (right_symbol.next >= 0) {\n                    symbols[right_symbol.next].prev = bigram.left;\n                }\n\n                add_new_bigram(left_symbol.prev, bigram.left);  // left side of current symbol\n                add_new_bigram(bigram.left, left_symbol.next);  // right side of current symbol\n            }\n\n            // add the fnished tokens to the final list keeping correct order for next and prev\n            for (auto & sym : symbols) {\n                if (sym.n > 0) {\n                    sym.prev = final_prev_index;\n                    sym.next = -1;\n                    if (final_prev_index != -1) {\n                        symbols_final[final_prev_index].next = symbols_final.size();\n                    }\n                    symbols_final.emplace_back(sym);\n                    final_prev_index = symbols_final.size() - 1;\n                }\n            }\n        }\n\n        symbols = symbols_final;\n\n        if (!symbols.empty()) {\n            for (int i = 0; i != -1; i = symbols[i].next) {\n                auto & symbol = symbols[i];\n                if (symbol.n == 0) {\n                    continue;\n                }\n\n                const std::string str = std::string(symbol.text, symbol.n);\n                const auto token = vocab.token_to_id.find(str);\n\n                if (token == vocab.token_to_id.end()) {\n                    for (auto j = str.begin(); j != str.end(); ++j) {\n                        std::string byte_str(1, *j);\n                        auto token_multibyte = vocab.token_to_id.find(byte_str);\n                        if (token_multibyte == vocab.token_to_id.end()) {\n                            throw std::runtime_error(\"ERROR: byte not found in vocab\");\n                        }\n                        output.push_back((*token_multibyte).second);\n                    }\n                } else {\n                    output.push_back((*token).second);\n                }\n            }\n        }\n    }\n\nprivate:\n    void add_new_bigram(int left, int right) {\n        if (left == -1 || right == -1) {\n            return;\n        }\n\n        std::string left_token  = std::string(symbols[left].text,  symbols[left].n);\n        std::string right_token = std::string(symbols[right].text, symbols[right].n);\n\n        int rank_found = -1;\n\n        rank_found = vocab.find_bpe_rank(left_token, right_token);\n\n        if (rank_found < 0) {\n            return;\n        }\n\n        llm_bigram_bpe bigram;\n\n        bigram.left  = left;\n        bigram.right = right;\n        bigram.text  = left_token + right_token;\n        bigram.size  = left_token.size() + right_token.size();\n        bigram.rank  = rank_found;\n\n        work_queue.push(bigram);\n    }\n\n    std::vector<std::string> bpe_gpt2_preprocess(const std::string & text) {\n        std::vector<std::string> bpe_words;\n        std::vector<std::string> bpe_encoded_words;\n\n        std::string token = \"\";\n        // GPT2 system regex:  's|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\n        bool collecting_numeric = false;\n        bool collecting_letter = false;\n        bool collecting_special = false;\n        bool collecting_whitespace_lookahead = false;\n        bool collecting = false;\n\n        std::vector<std::string> text_utf;\n        text_utf.reserve(text.size());\n        bpe_words.reserve(text.size());\n        bpe_encoded_words.reserve(text.size());\n\n        auto cps = codepoints_from_utf8(text);\n        for (size_t i = 0; i < cps.size(); ++i)\n            text_utf.emplace_back(codepoint_to_utf8(cps[i]));\n\n        for (int i = 0; i < (int)text_utf.size(); i++) {\n            const std::string & utf_char = text_utf[i];\n            bool split_condition = false;\n            int bytes_remain = text_utf.size() - i;\n            // forward backward lookups\n            const std::string & utf_char_next = (i + 1 < (int)text_utf.size()) ? text_utf[i + 1] : \"\";\n            const std::string & utf_char_next_next = (i + 2 < (int)text_utf.size()) ? text_utf[i + 2] : \"\";\n\n            // handling contractions\n            if (!split_condition && bytes_remain >= 2) {\n                // 's|'t|'m|'d\n                if (utf_char == \"\\'\" && (utf_char_next == \"s\" || utf_char_next == \"t\" || utf_char_next == \"m\" || utf_char_next == \"d\")) {\n                    split_condition = true;\n                }\n                if (split_condition) {\n                    if (token.size()) {\n                        bpe_words.emplace_back(token); // push previous content as token\n                    }\n                    token = utf_char + utf_char_next;\n                    bpe_words.emplace_back(token);\n                    token = \"\";\n                    i++;\n                    continue;\n                }\n            }\n            if (!split_condition && bytes_remain >= 3) {\n                // 're|'ve|'ll\n                if (utf_char == \"\\'\" && (\n                    (utf_char_next == \"r\" && utf_char_next_next == \"e\") ||\n                    (utf_char_next == \"v\" && utf_char_next_next == \"e\") ||\n                    (utf_char_next == \"l\" && utf_char_next_next == \"l\"))\n                    ) {\n                    split_condition = true;\n                }\n                if (split_condition) {\n                    // current token + next token can be defined\n                    if (token.size()) {\n                        bpe_words.emplace_back(token); // push previous content as token\n                    }\n                    token = utf_char + utf_char_next + utf_char_next_next;\n                    bpe_words.emplace_back(token); // the contraction\n                    token = \"\";\n                    i += 2;\n                    continue;\n                }\n            }\n\n            if (!split_condition && !collecting) {\n                if (codepoint_type(utf_char) == CODEPOINT_TYPE_LETTER || (!token.size() && utf_char == \" \" && codepoint_type(utf_char_next) == CODEPOINT_TYPE_LETTER)) {\n                    collecting_letter = true;\n                    collecting = true;\n                }\n                else if (codepoint_type(utf_char) == CODEPOINT_TYPE_DIGIT || (!token.size() && utf_char == \" \" && codepoint_type(utf_char_next) == CODEPOINT_TYPE_DIGIT)) {\n                    collecting_numeric = true;\n                    collecting = true;\n                }\n                else if (\n                    ((codepoint_type(utf_char) != CODEPOINT_TYPE_LETTER && codepoint_type(utf_char) != CODEPOINT_TYPE_DIGIT) && (codepoint_type(utf_char) != CODEPOINT_TYPE_WHITESPACE)) ||\n                    (!token.size() && utf_char == \" \" && codepoint_type(utf_char_next) != CODEPOINT_TYPE_LETTER && codepoint_type(utf_char_next) != CODEPOINT_TYPE_DIGIT && codepoint_type(utf_char_next) != CODEPOINT_TYPE_WHITESPACE)\n                    ) {\n                    collecting_special = true;\n                    collecting = true;\n                }\n                else if (codepoint_type(utf_char) == CODEPOINT_TYPE_WHITESPACE && codepoint_type(utf_char_next) == CODEPOINT_TYPE_WHITESPACE) {\n                    collecting_whitespace_lookahead = true;\n                    collecting = true;\n                }\n                else if (codepoint_type(utf_char) == CODEPOINT_TYPE_WHITESPACE) {\n                    split_condition = true;\n                }\n            }\n            else if (!split_condition && collecting) {\n                if (collecting_letter && codepoint_type(utf_char) != CODEPOINT_TYPE_LETTER) {\n                    split_condition = true;\n                }\n                else if (collecting_numeric && codepoint_type(utf_char) != CODEPOINT_TYPE_DIGIT) {\n                    split_condition = true;\n                }\n                else if (collecting_special && (codepoint_type(utf_char) == CODEPOINT_TYPE_LETTER || codepoint_type(utf_char) == CODEPOINT_TYPE_DIGIT || codepoint_type(utf_char) == CODEPOINT_TYPE_WHITESPACE)) {\n                    split_condition = true;\n                }\n                else if (collecting_whitespace_lookahead && (codepoint_type(utf_char_next) == CODEPOINT_TYPE_LETTER || codepoint_type(utf_char_next) == CODEPOINT_TYPE_DIGIT)) {\n                    split_condition = true;\n                }\n            }\n\n            if (utf_char_next == \"\") {\n                split_condition = true; // final\n                token += utf_char;\n            }\n\n            if (split_condition) {\n                if (token.size()) {\n                    bpe_words.emplace_back(token);\n                }\n                token = utf_char;\n                collecting = false;\n                collecting_letter = false;\n                collecting_numeric = false;\n                collecting_special = false;\n                collecting_whitespace_lookahead = false;\n            }\n            else {\n                token += utf_char;\n            }\n        }\n\n        for (std::string & word : bpe_words) {\n            std::string encoded_token = \"\";\n            for (char & c : word) {\n                encoded_token += bytes_to_unicode_bpe(c);\n            }\n            bpe_encoded_words.emplace_back(encoded_token);\n        }\n\n        return bpe_encoded_words;\n    }\n\n    const llama_vocab & vocab;\n\n    std::vector<llm_symbol> symbols;\n    std::vector<llm_symbol> symbols_final;\n\n    llm_bigram_bpe::queue work_queue;\n};\n\ntypedef enum FRAGMENT_BUFFER_VARIANT_TYPE{\n    FRAGMENT_BUFFER_VARIANT_TYPE_TOKEN,\n    FRAGMENT_BUFFER_VARIANT_TYPE_RAW_TEXT\n} FRAGMENT_BUFFER_VARIANT_TYPE;\n\nstruct fragment_buffer_variant{\n    fragment_buffer_variant(llama_vocab::id _token)\n    :\n        type(FRAGMENT_BUFFER_VARIANT_TYPE_TOKEN),\n        token(_token),\n        raw_text(_dummy),\n        offset(0),\n        length(0){}\n    fragment_buffer_variant(const std::string & _raw_text, int64_t _offset, int64_t _length)\n    :\n        type(FRAGMENT_BUFFER_VARIANT_TYPE_RAW_TEXT),\n        token((llama_vocab::id)-1),\n        raw_text(_raw_text),\n        offset(_offset),\n        length(_length){\n            GGML_ASSERT( _offset >= 0 );\n            GGML_ASSERT( _length >= 1 );\n            GGML_ASSERT( offset + length <= raw_text.length() );\n        }\n\n    const FRAGMENT_BUFFER_VARIANT_TYPE type;\n    const llama_vocab::id token;\n    const std::string _dummy;\n    const std::string & raw_text;\n    const uint64_t offset;\n    const uint64_t length;\n};\n\n// #define PRETOKENIZERDEBUG\n\nstatic void tokenizer_st_partition(const llama_vocab & vocab, std::forward_list<fragment_buffer_variant> & buffer)\n{\n    // for each special token\n    for (const auto & st: vocab.special_tokens_cache) {\n        const auto & special_token = st.first;\n        const auto & special_id    = st.second;\n\n        // for each text fragment\n        std::forward_list<fragment_buffer_variant>::iterator it = buffer.begin();\n        while (it != buffer.end()) {\n            auto & fragment = (*it);\n\n            // if a fragment is text ( not yet processed )\n            if (fragment.type == FRAGMENT_BUFFER_VARIANT_TYPE_RAW_TEXT) {\n                auto * raw_text = &(fragment.raw_text);\n\n                auto raw_text_base_offset = fragment.offset;\n                auto raw_text_base_length = fragment.length;\n\n                // loop over the text\n                while (true) {\n                    // find the first occurence of a given special token in this fragment\n                    //  passing offset argument only limit the \"search area\" but match coordinates\n                    //  are still relative to the source full raw_text\n                    auto match = raw_text->find(special_token, raw_text_base_offset);\n\n                    // no occurences found, stop processing this fragment for a given special token\n                    if (match == std::string::npos) break;\n\n                    // check if match is within bounds of offset <-> length\n                    if (match + special_token.length() > raw_text_base_offset + raw_text_base_length) break;\n\n#ifdef PRETOKENIZERDEBUG\n                    fprintf(stderr, \"FF: (%ld %ld %ld) '%s'\\n\", raw_text->length(), raw_text_base_offset, raw_text_base_length, raw_text->substr(raw_text_base_offset, raw_text_base_length).c_str());\n#endif\n                    auto source = std::distance(buffer.begin(), it);\n\n                    // if match is further than base offset\n                    //  then we have some text to the left of it\n                    if (match > raw_text_base_offset) {\n                        // left\n                        const int64_t left_reminder_offset = raw_text_base_offset + 0;\n                        const int64_t left_reminder_length = match - raw_text_base_offset;\n                        buffer.emplace_after(it, (*raw_text), left_reminder_offset, left_reminder_length);\n\n#ifdef PRETOKENIZERDEBUG\n                        fprintf(stderr, \"FL: (%ld %ld) '%s'\\n\", left_reminder_offset, left_reminder_length, raw_text->substr(left_reminder_offset, left_reminder_length).c_str());\n#endif\n                        it++;\n                    }\n\n                    // special token\n                    buffer.emplace_after(it, special_id);\n                    it++;\n\n                    // right\n                    if (match + special_token.length() < raw_text_base_offset + raw_text_base_length) {\n                        const int64_t right_reminder_offset = match + special_token.length();\n                        const int64_t right_reminder_length = raw_text_base_length - ((match - raw_text_base_offset) + special_token.length());\n                        buffer.emplace_after(it, (*raw_text), right_reminder_offset, right_reminder_length);\n\n#ifdef PRETOKENIZERDEBUG\n                        fprintf(stderr, \"FR: (%ld %ld) '%s'\\n\", right_reminder_offset, right_reminder_length, raw_text->substr(right_reminder_offset, right_reminder_length).c_str());\n#endif\n\n                        it++;\n\n                        if (source == 0) {\n                            buffer.erase_after(buffer.before_begin());\n                        } else {\n                            buffer.erase_after(std::next(buffer.begin(), (source-1)));\n                        }\n\n                        // repeat for the right side\n                        raw_text_base_offset = right_reminder_offset;\n                        raw_text_base_length = right_reminder_length;\n\n#ifdef PRETOKENIZERDEBUG\n                        fprintf(stderr, \"RR: (%ld %ld) '%s'\\n\", raw_text_base_offset, raw_text_base_length, raw_text->substr(raw_text_base_offset, raw_text_base_length).c_str());\n#endif\n                    } else {\n                        if (source == 0) {\n                            buffer.erase_after(buffer.before_begin());\n                        } else {\n                            buffer.erase_after(std::next(buffer.begin(), (source-1)));\n                        }\n                        break;\n                    }\n                }\n            }\n            it++;\n        }\n    }\n}\n\nstatic std::vector<llama_vocab::id> llama_tokenize_internal(const llama_vocab & vocab, std::string raw_text, bool bos, bool special) {\n    std::vector<llama_vocab::id> output;\n\n    // OG tokenizer behavior:\n    //\n    // tokenizer.encode('', add_bos=True)  returns [1]\n    // tokenizer.encode('', add_bos=False) returns []\n\n    if (bos && vocab.special_bos_id != -1) {\n        output.push_back(vocab.special_bos_id);\n    }\n\n    if (raw_text.empty()) {\n        return output;\n    }\n\n    std::forward_list<fragment_buffer_variant> fragment_buffer;\n    fragment_buffer.emplace_front( raw_text, 0, raw_text.length() );\n\n    if (special) tokenizer_st_partition( vocab, fragment_buffer );\n\n    switch (vocab.type) {\n        case LLAMA_VOCAB_TYPE_SPM:\n            {\n                for (const auto & fragment: fragment_buffer)\n                {\n                    if (fragment.type == FRAGMENT_BUFFER_VARIANT_TYPE_RAW_TEXT)\n                    {\n                        // without adding this leading whitespace, we do not get the same results as the original tokenizer\n\n                        // TODO: It's likely possible to get rid of this string copy entirely\n                        //  by modifying llm_tokenizer_x to operate with string offsets like pre-tokenizer\n                        //  and passing 'add space prefix' as bool argument\n                        //\n                        auto raw_text = (special ? \"\" : \" \") + fragment.raw_text.substr(fragment.offset, fragment.length);\n\n#ifdef PRETOKENIZERDEBUG\n                        fprintf(stderr,\"TT: (%ld %ld %ld) '%s'\\n\", raw_text.length(), fragment.offset, fragment.length, raw_text.c_str());\n#endif\n                        llm_tokenizer_spm tokenizer(vocab);\n                        llama_escape_whitespace(raw_text);\n                        tokenizer.tokenize(raw_text, output);\n                    }\n                    else // if (fragment.type == FRAGMENT_BUFFER_VARIANT_TYPE_TOKEN)\n                    {\n                        output.push_back(fragment.token);\n                    }\n                }\n            } break;\n        case LLAMA_VOCAB_TYPE_BPE:\n            {\n                for (const auto & fragment: fragment_buffer)\n                {\n                    if (fragment.type == FRAGMENT_BUFFER_VARIANT_TYPE_RAW_TEXT)\n                    {\n                        auto raw_text = fragment.raw_text.substr(fragment.offset, fragment.length);\n\n#ifdef PRETOKENIZERDEBUG\n                        fprintf(stderr,\"TT: (%ld %ld %ld) '%s'\\n\", raw_text.length(), fragment.offset, fragment.length, raw_text.c_str());\n#endif\n                        llm_tokenizer_bpe tokenizer(vocab);\n                        tokenizer.tokenize(raw_text, output);\n                    }\n                    else // if (fragment.type == FRAGMENT_BUFFER_VARIANT_TYPE_TOKEN)\n                    {\n                        output.push_back(fragment.token);\n                    }\n                }\n            } break;\n    }\n\n    return output;\n}\n\n//\n// grammar - internal\n//\n\nstruct llama_partial_utf8 {\n    uint32_t value;    // bit value so far (unshifted)\n    int      n_remain; // num bytes remaining; -1 indicates invalid sequence\n};\n\nstruct llama_grammar {\n    const std::vector<std::vector<llama_grammar_element>>   rules;\n    std::vector<std::vector<const llama_grammar_element *>> stacks;\n\n    // buffer for partially generated UTF-8 sequence from accepted tokens\n    llama_partial_utf8                                      partial_utf8;\n};\n\nstruct llama_grammar_candidate {\n    size_t               index;\n    const uint32_t     * code_points;\n    llama_partial_utf8   partial_utf8;\n};\n\n// Decodes a UTF-8 string which may end in an incomplete sequence. Adds a terminating 0 for use as\n// pointer. If an invalid sequence is encountered, returns `llama_partial_utf8.n_remain == -1`.\nstatic std::pair<std::vector<uint32_t>, llama_partial_utf8> decode_utf8(\n        const char         * src,\n        llama_partial_utf8   partial_start) {\n    static const int      lookup[] = { 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 2, 2, 3, 4 };\n    const char          * pos      = src;\n    std::vector<uint32_t> code_points;\n    uint32_t              value    = partial_start.value;\n    int                   n_remain = partial_start.n_remain;\n\n    // continue previous decode, if applicable\n    while (*pos != 0 && n_remain > 0) {\n        uint8_t next_byte = static_cast<uint8_t>(*pos);\n        if ((next_byte >> 6) != 2) {\n            // invalid sequence, abort\n            code_points.push_back(0);\n            return std::make_pair(std::move(code_points), llama_partial_utf8{ 0, -1 });\n        }\n        value = (value << 6) + (next_byte & 0x3F);\n        ++pos;\n        --n_remain;\n    }\n\n    if (partial_start.n_remain > 0 && n_remain == 0) {\n        code_points.push_back(value);\n    }\n\n    // decode any subsequent utf-8 sequences, which may end in an incomplete one\n    while (*pos != 0) {\n        uint8_t  first_byte = static_cast<uint8_t>(*pos);\n        uint8_t  highbits   = first_byte >> 4;\n                 n_remain   = lookup[highbits] - 1;\n\n        if (n_remain < 0) {\n            // invalid sequence, abort\n            code_points.clear();\n            code_points.push_back(0);\n            return std::make_pair(std::move(code_points), llama_partial_utf8{ 0, n_remain });\n        }\n\n        uint8_t  mask       = (1 << (7 - n_remain)) - 1;\n                 value      = first_byte & mask;\n        ++pos;\n        while (*pos != 0 && n_remain > 0) {\n            value = (value << 6) + (static_cast<uint8_t>(*pos) & 0x3F);\n            ++pos;\n            --n_remain;\n        }\n        if (n_remain == 0) {\n            code_points.push_back(value);\n        }\n    }\n    code_points.push_back(0);\n\n    return std::make_pair(std::move(code_points), llama_partial_utf8{ value, n_remain });\n}\n\n// returns true iff pos points to the end of one of the definitions of a rule\nstatic bool llama_grammar_is_end_of_sequence(const llama_grammar_element * pos) {\n    switch (pos->type) {\n        case LLAMA_GRETYPE_END: return true;  // NOLINT\n        case LLAMA_GRETYPE_ALT: return true;  // NOLINT\n        default:                return false;\n    }\n}\n\n// returns true iff chr satisfies the char range at pos (regular or inverse range)\n// asserts that pos is pointing to a char range element\nstatic std::pair<bool, const llama_grammar_element *> llama_grammar_match_char(\n        const llama_grammar_element * pos,\n        const uint32_t                chr) {\n\n    bool found            = false;\n    bool is_positive_char = pos->type == LLAMA_GRETYPE_CHAR;\n\n    GGML_ASSERT(is_positive_char || pos->type == LLAMA_GRETYPE_CHAR_NOT); // NOLINT\n\n    do {\n        if (pos[1].type == LLAMA_GRETYPE_CHAR_RNG_UPPER) {\n            // inclusive range, e.g. [a-z]\n            found = found || (pos->value <= chr && chr <= pos[1].value);\n            pos += 2;\n        } else {\n            // exact char match, e.g. [a] or \"a\"\n            found = found || pos->value == chr;\n            pos += 1;\n        }\n    } while (pos->type == LLAMA_GRETYPE_CHAR_ALT);\n\n    return std::make_pair(found == is_positive_char, pos);\n}\n\n// returns true iff some continuation of the given partial UTF-8 sequence could satisfy the char\n// range at pos (regular or inverse range)\n// asserts that pos is pointing to a char range element\nstatic bool llama_grammar_match_partial_char(\n        const llama_grammar_element * pos,\n        const llama_partial_utf8      partial_utf8) {\n\n    bool is_positive_char = pos->type == LLAMA_GRETYPE_CHAR;\n    GGML_ASSERT(is_positive_char || pos->type == LLAMA_GRETYPE_CHAR_NOT);\n\n    uint32_t partial_value = partial_utf8.value;\n    int      n_remain      = partial_utf8.n_remain;\n\n    // invalid sequence or 7-bit char split across 2 bytes (overlong)\n    if (n_remain < 0 || (n_remain == 1 && partial_value < 2)) {\n        return false;\n    }\n\n    // range of possible code points this partial UTF-8 sequence could complete to\n    uint32_t low  = partial_value << (n_remain * 6);\n    uint32_t high = low | ((1 << (n_remain * 6)) - 1);\n\n    if (low == 0) {\n        if (n_remain == 2) {\n            low = 1 << 11;\n        } else if (n_remain == 3) {\n            low = 1 << 16;\n        }\n    }\n\n    do {\n        if (pos[1].type == LLAMA_GRETYPE_CHAR_RNG_UPPER) {\n            // inclusive range, e.g. [a-z]\n            if (pos->value <= high && low <= pos[1].value) {\n                return is_positive_char;\n            }\n            pos += 2;\n        } else {\n            // exact char match, e.g. [a] or \"a\"\n            if (low <= pos->value && pos->value <= high) {\n                return is_positive_char;\n            }\n            pos += 1;\n        }\n    } while (pos->type == LLAMA_GRETYPE_CHAR_ALT);\n\n    return !is_positive_char;\n}\n\n\n// transforms a grammar pushdown stack into N possible stacks, all ending\n// at a character range (terminal element)\nstatic void llama_grammar_advance_stack(\n        const std::vector<std::vector<llama_grammar_element>>   & rules,\n        const std::vector<const llama_grammar_element *>        & stack,\n        std::vector<std::vector<const llama_grammar_element *>> & new_stacks) {\n\n    if (stack.empty()) {\n        new_stacks.emplace_back(stack);\n        return;\n    }\n\n    const llama_grammar_element * pos = stack.back();\n\n    switch (pos->type) {\n        case LLAMA_GRETYPE_RULE_REF: {\n            const size_t                  rule_id = static_cast<size_t>(pos->value);\n            const llama_grammar_element * subpos  = rules[rule_id].data();\n            do {\n                // init new stack without the top (pos)\n                std::vector<const llama_grammar_element *> new_stack(stack.begin(), stack.end() - 1);\n                if (!llama_grammar_is_end_of_sequence(pos + 1)) {\n                    // if this rule ref is followed by another element, add that to stack\n                    new_stack.push_back(pos + 1);\n                }\n                if (!llama_grammar_is_end_of_sequence(subpos)) {\n                    // if alternate is nonempty, add to stack\n                    new_stack.push_back(subpos);\n                }\n                llama_grammar_advance_stack(rules, new_stack, new_stacks);\n                while (!llama_grammar_is_end_of_sequence(subpos)) {\n                    // scan to end of alternate def\n                    subpos++;\n                }\n                if (subpos->type == LLAMA_GRETYPE_ALT) {\n                    // there's another alternate def of this rule to process\n                    subpos++;\n                } else {\n                    break;\n                }\n            } while (true);\n            break;\n        }\n        case LLAMA_GRETYPE_CHAR:\n        case LLAMA_GRETYPE_CHAR_NOT:\n            new_stacks.emplace_back(stack);\n            break;\n        default:\n            // end of alternate (LLAMA_GRETYPE_END, LLAMA_GRETYPE_ALT) or middle of char range\n            // (LLAMA_GRETYPE_CHAR_ALT, LLAMA_GRETYPE_CHAR_RNG_UPPER); stack should never be left on\n            // those\n            GGML_ASSERT(false);\n    }\n}\n\n// takes a set of possible pushdown stacks on a grammar, which are required to\n// be positioned at a character range (see `llama_grammar_advance_stack`), and\n// produces the N possible stacks if the given char is accepted at those\n// positions\nstatic std::vector<std::vector<const llama_grammar_element *>> llama_grammar_accept(\n        const std::vector<std::vector<llama_grammar_element>>         & rules,\n        const std::vector<std::vector<const llama_grammar_element *>> & stacks,\n        const uint32_t                                                  chr) {\n\n    std::vector<std::vector<const llama_grammar_element *>> new_stacks;\n\n    for (const auto & stack : stacks) {\n        if (stack.empty()) {\n            continue;\n        }\n\n        auto match = llama_grammar_match_char(stack.back(), chr);\n        if (match.first) {\n            const llama_grammar_element * pos = match.second;\n\n            // update top of stack to next element, if any\n            std::vector<const llama_grammar_element *> new_stack(stack.begin(), stack.end() - 1);\n            if (!llama_grammar_is_end_of_sequence(pos)) {\n                new_stack.push_back(pos);\n            }\n            llama_grammar_advance_stack(rules, new_stack, new_stacks);\n        }\n    }\n\n    return new_stacks;\n}\n\nstatic std::vector<llama_grammar_candidate> llama_grammar_reject_candidates(\n        const std::vector<std::vector<llama_grammar_element>>         & rules,\n        const std::vector<std::vector<const llama_grammar_element *>> & stacks,\n        const std::vector<llama_grammar_candidate>                    & candidates);\n\nstatic std::vector<llama_grammar_candidate> llama_grammar_reject_candidates_for_stack(\n        const std::vector<std::vector<llama_grammar_element>> & rules,\n        const std::vector<const llama_grammar_element *>      & stack,\n        const std::vector<llama_grammar_candidate>            & candidates) {\n\n    std::vector<llama_grammar_candidate> rejects;\n\n    if (stack.empty()) {\n        for (const auto & tok : candidates) {\n            if (*tok.code_points != 0 || tok.partial_utf8.n_remain != 0) {\n                rejects.push_back(tok);\n            }\n        }\n        return rejects;\n    }\n\n    const llama_grammar_element * stack_pos = stack.back();\n\n    std::vector<llama_grammar_candidate> next_candidates;\n    for (const auto & tok : candidates) {\n        if (*tok.code_points == 0) {\n            // reached end of full codepoints in token, reject iff it ended in a partial sequence\n            // that cannot satisfy this position in grammar\n            if (tok.partial_utf8.n_remain != 0 &&\n                    !llama_grammar_match_partial_char(stack_pos, tok.partial_utf8)) {\n                rejects.push_back(tok);\n            }\n        } else if (llama_grammar_match_char(stack_pos, *tok.code_points).first) {\n            next_candidates.push_back({ tok.index, tok.code_points + 1, tok.partial_utf8 });\n        } else {\n            rejects.push_back(tok);\n        }\n    }\n\n    const auto * stack_pos_after = llama_grammar_match_char(stack_pos, 0).second;\n\n    // update top of stack to next element, if any\n    std::vector<const llama_grammar_element *> stack_after(stack.begin(), stack.end() - 1);\n    if (!llama_grammar_is_end_of_sequence(stack_pos_after)) {\n        stack_after.push_back(stack_pos_after);\n    }\n    std::vector<std::vector<const llama_grammar_element *>> next_stacks;\n    llama_grammar_advance_stack(rules, stack_after, next_stacks);\n\n    auto next_rejects = llama_grammar_reject_candidates(rules, next_stacks, next_candidates);\n    for (const auto & tok : next_rejects) {\n        rejects.push_back({ tok.index, tok.code_points - 1, tok.partial_utf8 });\n    }\n\n    return rejects;\n}\n\nstatic std::vector<llama_grammar_candidate> llama_grammar_reject_candidates(\n        const std::vector<std::vector<llama_grammar_element>>         & rules,\n        const std::vector<std::vector<const llama_grammar_element *>> & stacks,\n        const std::vector<llama_grammar_candidate>                    & candidates) {\n    GGML_ASSERT(!stacks.empty()); // REVIEW\n\n    if (candidates.empty()) {\n        return std::vector<llama_grammar_candidate>();\n    }\n\n    auto rejects = llama_grammar_reject_candidates_for_stack(rules, stacks.front(), candidates);\n\n    for (size_t i = 1, size = stacks.size(); i < size; ++i) {\n        rejects = llama_grammar_reject_candidates_for_stack(rules, stacks[i], rejects);\n    }\n    return rejects;\n}\n\n//\n// grammar - external\n//\n\nstruct llama_grammar * llama_grammar_init(\n            const llama_grammar_element ** rules,\n                                 size_t    n_rules,\n                                 size_t    start_rule_index) {\n    const llama_grammar_element * pos;\n\n    // copy rule definitions into vectors\n    std::vector<std::vector<llama_grammar_element>> vec_rules(n_rules);\n    for (size_t i = 0; i < n_rules; i++) {\n        for (pos = rules[i]; pos->type != LLAMA_GRETYPE_END; pos++) {\n            vec_rules[i].push_back(*pos);\n        }\n        vec_rules[i].push_back({LLAMA_GRETYPE_END, 0});\n    }\n\n    // loop over alternates of start rule to build initial stacks\n    std::vector<std::vector<const llama_grammar_element *>> stacks;\n    pos = rules[start_rule_index];\n    do {\n        std::vector<const llama_grammar_element *> stack;\n        if (!llama_grammar_is_end_of_sequence(pos)) {\n            // if alternate is nonempty, add to stack\n            stack.push_back(pos);\n        }\n        llama_grammar_advance_stack(vec_rules, stack, stacks);\n        while (!llama_grammar_is_end_of_sequence(pos)) {\n            // scan to end of alternate def\n            pos++;\n        }\n        if (pos->type == LLAMA_GRETYPE_ALT) {\n            // there's another alternate def of this rule to process\n            pos++;\n        } else {\n            break;\n        }\n    } while (true);\n\n    return new llama_grammar{ std::move(vec_rules), std::move(stacks), {} };\n}\n\nvoid llama_grammar_free(struct llama_grammar * grammar) {\n    delete grammar;\n}\n\nstruct llama_grammar * llama_grammar_copy(const struct llama_grammar * grammar) {\n    llama_grammar * result = new llama_grammar{ grammar->rules, grammar->stacks, grammar->partial_utf8 };\n\n    // redirect elements in stacks to point to new rules\n    for (size_t is = 0; is < result->stacks.size(); is++) {\n        for (size_t ie = 0; ie < result->stacks[is].size(); ie++) {\n            for (size_t ir0 = 0; ir0 < grammar->rules.size(); ir0++) {\n                for (size_t ir1 = 0; ir1 < grammar->rules[ir0].size(); ir1++) {\n                    if (grammar->stacks[is][ie] == &grammar->rules[ir0][ir1]) {\n                         result->stacks[is][ie]  =  &result->rules[ir0][ir1];\n                    }\n                }\n            }\n        }\n    }\n\n    return result;\n}\n\n//\n// sampling\n//\n\nvoid llama_set_rng_seed(struct llama_context * ctx, uint32_t seed) {\n    if (seed == LLAMA_DEFAULT_SEED) {\n        seed = time(NULL);\n    }\n    ctx->rng.seed(seed);\n}\n\nvoid llama_sample_softmax(struct llama_context * ctx, llama_token_data_array * candidates) {\n    GGML_ASSERT(candidates->size > 0);\n\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    // Sort the logits in descending order\n    if (!candidates->sorted) {\n        std::sort(candidates->data, candidates->data + candidates->size, [](const llama_token_data & a, const llama_token_data & b) {\n            return a.logit > b.logit;\n        });\n        candidates->sorted = true;\n    }\n\n    float max_l = candidates->data[0].logit;\n    float cum_sum = 0.0f;\n    for (size_t i = 0; i < candidates->size; ++i) {\n        float p = expf(candidates->data[i].logit - max_l);\n        candidates->data[i].p = p;\n        cum_sum += p;\n    }\n    for (size_t i = 0; i < candidates->size; ++i) {\n        candidates->data[i].p /= cum_sum;\n    }\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nvoid llama_sample_top_k(struct llama_context * ctx, llama_token_data_array * candidates, int k, size_t min_keep) {\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    k = std::max(k, (int) min_keep);\n    k = std::min(k, (int) candidates->size);\n\n    // Sort scores in descending order\n    if (!candidates->sorted) {\n        auto comp = [](const llama_token_data & a, const llama_token_data & b) {\n            return a.logit > b.logit;\n        };\n        if (k == (int) candidates->size) {\n            std::sort(candidates->data, candidates->data + candidates->size, comp);\n        } else {\n            std::partial_sort(candidates->data, candidates->data + k, candidates->data + candidates->size, comp);\n        }\n        candidates->sorted = true;\n    }\n    candidates->size = k;\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nvoid llama_sample_top_p(struct llama_context * ctx, llama_token_data_array * candidates, float p, size_t min_keep) {\n    if (p >= 1.0f) {\n        return;\n    }\n\n    llama_sample_softmax(ctx, candidates);\n\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    // Compute the cumulative probabilities\n    float cum_sum = 0.0f;\n    size_t last_idx = candidates->size;\n\n    for (size_t i = 0; i < candidates->size; ++i) {\n        cum_sum += candidates->data[i].p;\n\n        // Check if the running sum is at least p or if we have kept at least min_keep tokens\n        // we set the last index to i+1 to indicate that the current iterate should be included in the set\n        if (cum_sum >= p && i + 1 >= min_keep) {\n            last_idx = i + 1;\n            break;\n        }\n    }\n\n    // Resize the output vector to keep only the top-p tokens\n    candidates->size = last_idx;\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nvoid llama_sample_min_p(struct llama_context * ctx, llama_token_data_array * candidates, float p, size_t min_keep) {\n    if (p <= 0.0f || !candidates->size) {\n        return;\n    }\n\n    llama_sample_softmax(ctx, candidates);\n\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    float scale = candidates->data[0].p; // scale by max prob\n    size_t i = 1; // first token always matches\n\n    for (; i < candidates->size; ++i) {\n        if (candidates->data[i].p < p * scale && i >= min_keep) {\n            break; // prob too small\n        }\n    }\n\n    // Resize the output vector to keep only the matching tokens\n    candidates->size = i;\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nvoid llama_sample_tail_free(struct llama_context * ctx, llama_token_data_array * candidates, float z, size_t min_keep) {\n    if (z >= 1.0f || candidates->size <= 2) {\n        return;\n    }\n\n    llama_sample_softmax(nullptr, candidates);\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    // Compute the first and second derivatives\n    std::vector<float> first_derivatives(candidates->size - 1);\n    std::vector<float> second_derivatives(candidates->size - 2);\n\n    for (size_t i = 0; i < first_derivatives.size(); ++i) {\n        first_derivatives[i] = candidates->data[i].p - candidates->data[i + 1].p;\n    }\n    for (size_t i = 0; i < second_derivatives.size(); ++i) {\n        second_derivatives[i] = first_derivatives[i] - first_derivatives[i + 1];\n    }\n\n    // Calculate absolute value of second derivatives\n    for (size_t i = 0; i < second_derivatives.size(); ++i) {\n        second_derivatives[i] = std::abs(second_derivatives[i]);\n    }\n\n    // Normalize the second derivatives\n    {\n        const float second_derivatives_sum = std::accumulate(second_derivatives.begin(), second_derivatives.end(), 0.0f);\n\n        if (second_derivatives_sum > 1e-6f) {\n            for (float & value : second_derivatives) {\n                value /= second_derivatives_sum;\n            }\n        } else {\n            for (float & value : second_derivatives) {\n                value = 1.0f / second_derivatives.size();\n            }\n        }\n    }\n\n    float cum_sum = 0.0f;\n    size_t last_idx = candidates->size;\n    for (size_t i = 0; i < second_derivatives.size(); ++i) {\n        cum_sum += second_derivatives[i];\n\n        // Check if the running sum is greater than z or if we have kept at least min_keep tokens\n        if (cum_sum > z && i >= min_keep) {\n            last_idx = i;\n            break;\n        }\n    }\n\n    // Resize the output vector to keep only the tokens above the tail location\n    candidates->size = last_idx;\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nvoid llama_sample_typical(struct llama_context * ctx, llama_token_data_array * candidates, float p, size_t min_keep) {\n    // Reference implementation:\n    // https://github.com/huggingface/transformers/compare/main...cimeister:typical-sampling:typical-pr\n    if (p >= 1.0f) {\n        return;\n    }\n\n    // Compute the softmax of logits and calculate entropy\n    llama_sample_softmax(nullptr, candidates);\n\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    float entropy = 0.0f;\n    for (size_t i = 0; i < candidates->size; ++i) {\n        entropy += -candidates->data[i].p * logf(candidates->data[i].p);\n    }\n\n    // Compute the absolute difference between negative log probability and entropy for each candidate\n    std::vector<float> shifted_scores;\n    for (size_t i = 0; i < candidates->size; ++i) {\n        float shifted_score = fabsf(-logf(candidates->data[i].p) - entropy);\n        shifted_scores.push_back(shifted_score);\n    }\n\n    // Sort tokens based on the shifted_scores and their corresponding indices\n    std::vector<size_t> indices(candidates->size);\n    std::iota(indices.begin(), indices.end(), 0);\n\n    std::sort(indices.begin(), indices.end(), [&](size_t a, size_t b) {\n        return shifted_scores[a] < shifted_scores[b];\n    });\n\n    // Compute the cumulative probabilities\n    float cum_sum = 0.0f;\n    size_t last_idx = indices.size();\n\n    for (size_t i = 0; i < indices.size(); ++i) {\n        size_t idx = indices[i];\n        cum_sum += candidates->data[idx].p;\n\n        // Check if the running sum is greater than typical or if we have kept at least min_keep tokens\n        if (cum_sum > p && i >= min_keep - 1) {\n            last_idx = i + 1;\n            break;\n        }\n    }\n\n    // Resize the output vector to keep only the locally typical tokens\n    std::vector<llama_token_data> new_candidates;\n    for (size_t i = 0; i < last_idx; ++i) {\n        size_t idx = indices[i];\n        new_candidates.push_back(candidates->data[idx]);\n    }\n\n    // Replace the data in candidates with the new_candidates data\n    std::copy(new_candidates.begin(), new_candidates.end(), candidates->data);\n    candidates->size = new_candidates.size();\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nvoid llama_sample_temp(struct llama_context * ctx, llama_token_data_array * candidates_p, float temp) {\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    for (size_t i = 0; i < candidates_p->size; ++i) {\n        candidates_p->data[i].logit /= temp;\n    }\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nvoid llama_sample_temperature(struct llama_context * ctx, llama_token_data_array * candidates_p, float temp) {\n    llama_sample_temp(ctx, candidates_p, temp);\n}\n\nvoid llama_sample_repetition_penalties(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n               const llama_token * last_tokens,\n                          size_t   penalty_last_n,\n                           float   penalty_repeat,\n                           float   penalty_freq,\n                           float   penalty_present) {\n    if (penalty_last_n == 0 || (penalty_repeat == 1.0f && penalty_freq == 0.0f && penalty_present == 0.0f)) {\n        return;\n    }\n\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    // Create a frequency map to count occurrences of each token in last_tokens\n    std::unordered_map<llama_token, int> token_count;\n    for (size_t i = 0; i < penalty_last_n; ++i) {\n        token_count[last_tokens[i]]++;\n    }\n\n    // Apply frequency and presence penalties to the candidates\n    for (size_t i = 0; i < candidates->size; ++i) {\n        const auto token_iter = token_count.find(candidates->data[i].id);\n        if (token_iter == token_count.end()) {\n            continue;\n        }\n\n        const int count = token_iter->second;\n\n        // The academic publication that described this technique actually just only divided, but that would cause tokens with negative logits to become more likely, which is obviously wrong.\n        // This is common fix for this problem, which is to multiply by the penalty instead of dividing.\n        if (candidates->data[i].logit <= 0) {\n            candidates->data[i].logit *= penalty_repeat;\n        } else {\n            candidates->data[i].logit /= penalty_repeat;\n        }\n\n        candidates->data[i].logit -= float(count) * penalty_freq + float(count > 0) * penalty_present;\n    }\n\n    candidates->sorted = false;\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nvoid llama_sample_grammar(struct llama_context * ctx, llama_token_data_array * candidates, const struct llama_grammar * grammar) {\n    GGML_ASSERT(ctx);\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    bool allow_eos = false;\n    for (const auto & stack : grammar->stacks) {\n        if (stack.empty()) {\n            allow_eos = true;\n            break;\n        }\n    }\n\n    const llama_token eos = llama_token_eos(&ctx->model);\n\n    std::vector<std::pair<std::vector<uint32_t>, llama_partial_utf8>> candidates_decoded;\n    std::vector<llama_grammar_candidate>                              candidates_grammar;\n\n    for (size_t i = 0; i < candidates->size; ++i) {\n        const llama_token id    = candidates->data[i].id;\n        const std::string piece = llama_token_to_piece(ctx, id);\n        if (id == eos) {\n            if (!allow_eos) {\n                candidates->data[i].logit = -INFINITY;\n            }\n        } else if (piece.empty() || piece[0] == 0) {\n            candidates->data[i].logit = -INFINITY;\n        } else {\n            candidates_decoded.push_back(decode_utf8(piece.c_str(), grammar->partial_utf8));\n            candidates_grammar.push_back({ i, candidates_decoded.back().first.data(), candidates_decoded.back().second });\n        }\n    }\n\n    const auto rejects = llama_grammar_reject_candidates(grammar->rules, grammar->stacks, candidates_grammar);\n    for (const auto & reject : rejects) {\n        candidates->data[reject.index].logit = -INFINITY;\n    }\n\n    ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n}\n\nstatic void llama_log_softmax(float * array, size_t size) {\n    float max_l = *std::max_element(array, array + size);\n    float sum = 0.f;\n    for (size_t i = 0; i < size; ++i) {\n        float p = expf(array[i] - max_l);\n        sum += p;\n        array[i] = p;\n    }\n\n    for (size_t i = 0; i < size; ++i) {\n        array[i] = logf(array[i] / sum);\n    }\n}\n\nvoid llama_sample_classifier_free_guidance(\n          struct llama_context * ctx,\n        llama_token_data_array * candidates,\n          struct llama_context * guidance_ctx,\n                         float   scale) {\n    int64_t t_start_sample_us = ggml_time_us();\n\n    GGML_ASSERT(ctx);\n\n    auto n_vocab = llama_n_vocab(llama_get_model(ctx));\n\n    GGML_ASSERT(n_vocab == (int)candidates->size);\n    GGML_ASSERT(!candidates->sorted);\n\n    std::vector<float> logits_base;\n    logits_base.reserve(candidates->size);\n    for (size_t i = 0; i < candidates->size; ++i) {\n        logits_base.push_back(candidates->data[i].logit);\n    }\n    llama_log_softmax(logits_base.data(), candidates->size);\n\n    float* logits_guidance = llama_get_logits(guidance_ctx);\n    llama_log_softmax(logits_guidance, n_vocab);\n\n    for (int i = 0; i < n_vocab; ++i) {\n        float logit_guidance = logits_guidance[i];\n        float logit_base = logits_base[i];\n        candidates->data[i].logit = scale * (logit_base - logit_guidance) + logit_guidance;\n    }\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n}\n\nllama_token llama_sample_token_mirostat(struct llama_context * ctx, llama_token_data_array * candidates, float tau, float eta, int m, float * mu) {\n    GGML_ASSERT(ctx);\n\n    auto N = float(llama_n_vocab(llama_get_model(ctx)));\n    int64_t t_start_sample_us;\n    t_start_sample_us = ggml_time_us();\n\n    llama_sample_softmax(nullptr, candidates);\n\n    // Estimate s_hat using the most probable m tokens\n    float s_hat = 0.0;\n    float sum_ti_bi = 0.0;\n    float sum_ti_sq = 0.0;\n    for (size_t i = 0; i < size_t(m - 1) && i < candidates->size - 1; ++i) {\n        float t_i = logf(float(i + 2) / float(i + 1));\n        float b_i = logf(candidates->data[i].p / candidates->data[i + 1].p);\n        sum_ti_bi += t_i * b_i;\n        sum_ti_sq += t_i * t_i;\n    }\n    s_hat = sum_ti_bi / sum_ti_sq;\n\n    // Compute k from the estimated s_hat and target surprise value\n    float epsilon_hat = s_hat - 1;\n    float k = powf((epsilon_hat * powf(2, *mu)) / (1 - powf(N, -epsilon_hat)), 1 / s_hat);\n\n    // Sample the next word X using top-k sampling\n    llama_sample_top_k(nullptr, candidates, int(k), 1);\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n    llama_token X = llama_sample_token(ctx, candidates);\n    t_start_sample_us = ggml_time_us();\n\n    // Compute error as the difference between observed surprise and target surprise value\n    size_t X_idx = std::distance(candidates->data, std::find_if(candidates->data, candidates->data + candidates->size, [&](const llama_token_data & candidate) {\n        return candidate.id == X;\n    }));\n    float observed_surprise = -log2f(candidates->data[X_idx].p);\n    float e = observed_surprise - tau;\n\n    // Update mu using the learning rate and error\n    *mu = *mu - eta * e;\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n    return X;\n}\n\nllama_token llama_sample_token_mirostat_v2(struct llama_context * ctx, llama_token_data_array * candidates, float tau, float eta, float * mu) {\n    int64_t t_start_sample_us;\n    t_start_sample_us = ggml_time_us();\n\n    llama_sample_softmax(ctx, candidates);\n\n    // Truncate the words with surprise values greater than mu\n    candidates->size = std::distance(candidates->data, std::find_if(candidates->data, candidates->data + candidates->size, [&](const llama_token_data & candidate) {\n        return -log2f(candidate.p) > *mu;\n    }));\n\n    if (candidates->size == 0) {\n        candidates->size = 1;\n    }\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n\n    // Normalize the probabilities of the remaining words\n    llama_sample_softmax(ctx, candidates);\n\n    // Sample the next word X from the remaining words\n    llama_token X = llama_sample_token(ctx, candidates);\n    t_start_sample_us = ggml_time_us();\n\n    // Compute error as the difference between observed surprise and target surprise value\n    size_t X_idx = std::distance(candidates->data, std::find_if(candidates->data, candidates->data + candidates->size, [&](const llama_token_data & candidate) {\n        return candidate.id == X;\n    }));\n    float observed_surprise = -log2f(candidates->data[X_idx].p);\n    float e = observed_surprise - tau;\n\n    // Update mu using the learning rate and error\n    *mu = *mu - eta * e;\n\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    }\n    return X;\n}\n\nllama_token llama_sample_token_greedy(struct llama_context * ctx, llama_token_data_array * candidates) {\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    // Find max element\n    auto * max_iter = std::max_element(candidates->data, candidates->data + candidates->size, [](const llama_token_data & a, const llama_token_data & b) {\n        return a.logit < b.logit;\n    });\n\n    llama_token result = max_iter->id;\n    if (ctx) {\n        ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n        ctx->n_sample++;\n    }\n    return result;\n}\n\nllama_token llama_sample_token(struct llama_context * ctx, llama_token_data_array * candidates) {\n    GGML_ASSERT(ctx);\n\n    const int64_t t_start_sample_us = ggml_time_us();\n    llama_sample_softmax(nullptr, candidates);\n\n    std::vector<float> probs;\n    probs.reserve(candidates->size);\n    for (size_t i = 0; i < candidates->size; ++i) {\n        probs.push_back(candidates->data[i].p);\n    }\n\n    std::discrete_distribution<> dist(probs.begin(), probs.end());\n    auto & rng = ctx->rng;\n    int idx = dist(rng);\n\n    llama_token result = candidates->data[idx].id;\n\n    ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    ctx->n_sample++;\n    return result;\n}\n\nvoid llama_grammar_accept_token(struct llama_context * ctx, struct llama_grammar * grammar, llama_token token) {\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    if (token == llama_token_eos(&ctx->model)) {\n        for (const auto & stack : grammar->stacks) {\n            if (stack.empty()) {\n                return;\n            }\n        }\n        GGML_ASSERT(false);\n    }\n\n    const std::string piece = llama_token_to_piece(ctx, token);\n\n    // Note terminating 0 in decoded string\n    const auto   decoded     = decode_utf8(piece.c_str(), grammar->partial_utf8);\n    const auto & code_points = decoded.first;\n    for (auto it = code_points.begin(), end = code_points.end() - 1; it != end; ++it) {\n        grammar->stacks = llama_grammar_accept(grammar->rules, grammar->stacks, *it);\n    }\n    grammar->partial_utf8 = decoded.second;\n    GGML_ASSERT(!grammar->stacks.empty());\n\n    ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n}\n\n//\n// Beam search\n//\n\nstruct llama_beam {\n    std::vector<llama_token> tokens;\n    float p;  // Cumulative beam probability (renormalized relative to all beams)\n    bool eob; // Initialize end-of-beam to false. Callback sets this to true.\n    // Sort beams by probability. In case of ties, prefer beams at eob.\n    bool operator<(const llama_beam & rhs) const {\n        return std::make_pair(p, eob) < std::make_pair(rhs.p, rhs.eob);\n    }\n    // Shift off first n tokens and discard them.\n    void shift_tokens(const size_t n) {\n        if (n) {\n            std::copy(tokens.begin() + n, tokens.end(), tokens.begin());\n            tokens.resize(tokens.size() - n);\n        }\n    }\n    llama_beam_view view() const { return {tokens.data(), tokens.size(), p, eob}; }\n};\n\n// A struct for calculating logit-related info.\nstruct llama_logit_info {\n    const float * const logits;\n    const int n_vocab;\n    const float max_l;\n    const float normalizer;\n    struct sum_exp {\n        float max_l;\n        float operator()(float sum, float l) const { return sum + std::exp(l - max_l); }\n    };\n    llama_logit_info(llama_context * ctx)\n      : logits(llama_get_logits(ctx))\n      , n_vocab(llama_n_vocab(llama_get_model(ctx)))\n      , max_l(*std::max_element(logits, logits + n_vocab))\n      , normalizer(1.0f / std::accumulate(logits, logits + n_vocab, 0.0f, sum_exp{max_l}))\n      { }\n    llama_token_data get_token_data(const llama_token token_id) const {\n        constexpr auto p = std::numeric_limits<float>::quiet_NaN();  // never used\n        return {token_id, logits[token_id], p};\n    }\n    // Return top k token_data by logit.\n    std::vector<llama_token_data> top_k(size_t k) {\n        std::vector<llama_token_data> min_heap;  // min-heap by logit\n        const llama_token k_min = std::min(static_cast<llama_token>(k), n_vocab);\n        min_heap.reserve(k_min);\n        for (llama_token token_id = 0 ; token_id < k_min ; ++token_id) {\n            min_heap.push_back(get_token_data(token_id));\n        }\n        auto comp = [](const llama_token_data & a, const llama_token_data & b) { return a.logit > b.logit; };\n        std::make_heap(min_heap.begin(), min_heap.end(), comp);\n        for (llama_token token_id = k_min ; token_id < n_vocab ; ++token_id) {\n            if (min_heap.front().logit < logits[token_id]) {\n                std::pop_heap(min_heap.begin(), min_heap.end(), comp);\n                min_heap.back().id = token_id;\n                min_heap.back().logit = logits[token_id];\n                std::push_heap(min_heap.begin(), min_heap.end(), comp);\n            }\n        }\n        return min_heap;\n    }\n    float probability_from_logit(float logit) const {\n        return normalizer * std::exp(logit - max_l);\n    }\n};\n\nstruct llama_beam_search_data {\n    llama_context * ctx;\n    size_t n_beams;\n    int n_past;\n    int n_predict;\n    std::vector<llama_beam> beams;\n    std::vector<llama_beam> next_beams;\n\n    // Re-calculated on each loop iteration\n    size_t common_prefix_length;\n\n    // Used to communicate to/from callback on beams state.\n    std::vector<llama_beam_view> beam_views;\n\n    llama_beam_search_data(llama_context * ctx, size_t n_beams, int n_past, int n_predict)\n      : ctx(ctx)\n      , n_beams(n_beams)\n      , n_past(n_past)\n      , n_predict(n_predict)\n      , beam_views(n_beams) {\n        beams.reserve(n_beams);\n        next_beams.reserve(n_beams);\n    }\n\n    // Collapse beams to a single beam given by index.\n    void collapse_beams(const size_t beam_idx) {\n        if (0u < beam_idx) {\n            std::swap(beams[0], beams[beam_idx]);\n        }\n        beams.resize(1);\n    }\n\n    // Min-heaps are used to efficiently collect the top-k elements (k=n_beams).\n    // The repetative patterns below reflect the 2 stages of heaps:\n    //  * Gather elements until the vector is full, then call std::make_heap() on it.\n    //  * If the heap is full and a new element is found that should be included, pop the\n    //    least element to the back(), replace it with the new, then push it into the heap.\n    void fill_next_beams_by_top_probabilities(llama_beam & beam) {\n        // Min-heaps use a greater-than comparator.\n        const auto comp = [](const llama_beam & a, const llama_beam & b) { return a.p > b.p; };\n        if (beam.eob) {\n            // beam is at end-of-sentence, so just copy it to next_beams if its probability is high enough.\n            if (next_beams.size() < n_beams) {\n                next_beams.push_back(std::move(beam));\n                if (next_beams.size() == n_beams) {\n                    std::make_heap(next_beams.begin(), next_beams.end(), comp);\n                }\n            } else if (next_beams.front().p < beam.p) {\n                std::pop_heap(next_beams.begin(), next_beams.end(), comp);\n                next_beams.back() = std::move(beam);\n                std::push_heap(next_beams.begin(), next_beams.end(), comp);\n            }\n        } else {\n            // beam is not at end-of-sentence, so branch with next top_k tokens.\n            if (!beam.tokens.empty()) {\n                llama_decode(ctx, llama_batch_get_one(beam.tokens.data(), beam.tokens.size(), n_past, 0));\n            }\n            llama_logit_info logit_info(ctx);\n            std::vector<llama_token_data> next_tokens = logit_info.top_k(n_beams);\n            size_t i=0;\n            if (next_beams.size() < n_beams) {\n                for (; next_beams.size() < n_beams ; ++i) {\n                    llama_beam next_beam = beam;\n                    next_beam.tokens.push_back(next_tokens[i].id);\n                    next_beam.p *= logit_info.probability_from_logit(next_tokens[i].logit);\n                    next_beams.push_back(std::move(next_beam));\n                }\n                std::make_heap(next_beams.begin(), next_beams.end(), comp);\n            } else {\n                for (; next_beams.front().p == 0.0f ; ++i) {\n                    std::pop_heap(next_beams.begin(), next_beams.end(), comp);\n                    next_beams.back() = beam;\n                    next_beams.back().tokens.push_back(next_tokens[i].id);\n                    next_beams.back().p *= logit_info.probability_from_logit(next_tokens[i].logit);\n                    std::push_heap(next_beams.begin(), next_beams.end(), comp);\n                }\n            }\n            for (; i < n_beams ; ++i) {\n                const float next_p = beam.p * logit_info.probability_from_logit(next_tokens[i].logit);\n                if (next_beams.front().p < next_p) {\n                    std::pop_heap(next_beams.begin(), next_beams.end(), comp);\n                    next_beams.back() = beam;\n                    next_beams.back().tokens.push_back(next_tokens[i].id);\n                    next_beams.back().p = next_p;\n                    std::push_heap(next_beams.begin(), next_beams.end(), comp);\n                }\n            }\n        }\n    }\n\n    // Find common_prefix_length based on beams.\n    // Requires beams is not empty.\n    size_t find_common_prefix_length() {\n        size_t common_prefix_length = beams[0].tokens.size();\n        for (size_t i = 1 ; i < beams.size() ; ++i) {\n            common_prefix_length = std::min(common_prefix_length, beams[i].tokens.size());\n            for (size_t j = 0 ; j < common_prefix_length ; ++j) {\n                if (beams[0].tokens[j] != beams[i].tokens[j]) {\n                    common_prefix_length = j;\n                    break;\n                }\n            }\n        }\n        return common_prefix_length;\n    }\n\n    // Construct beams_state to send back to caller via the callback function.\n    // Side effect: set common_prefix_length = find_common_prefix_length();\n    llama_beams_state get_beams_state(const bool last_call) {\n        for (size_t i = 0 ; i < beams.size() ; ++i) {\n            beam_views[i] = beams[i].view();\n        }\n        common_prefix_length = find_common_prefix_length();\n        return {beam_views.data(), beams.size(), common_prefix_length, last_call};\n    }\n\n    // Loop:\n    //  * while i < n_predict, AND\n    //  * any of the beams have not yet reached end-of-beam (eob), AND\n    //  * the highest probability beam(s) (plural in case of ties) are not at end-of-sentence\n    //    (since all other beam probabilities can only decrease)\n    void loop(const llama_beam_search_callback_fn_t callback, void * const callback_data) {\n        beams.push_back({{}, 1.0f, false});  // Start with one empty beam w/ probability = 1.0 and !eob.\n        const auto not_eob = [](const llama_beam & beam) { return !beam.eob; };\n        for (int i = 0 ; i < n_predict && std::any_of(beams.begin(),beams.end(),not_eob) &&\n                       !beams[top_beam_index()].eob ; ++i) {\n            callback(callback_data, get_beams_state(false));  // Sets common_prefix_length\n            update_beams_from_beam_views();   // Update values (p,eob) that callback may have changed.\n            if (common_prefix_length) {\n                llama_decode(ctx, llama_batch_get_one(beams[0].tokens.data(), common_prefix_length, n_past, 0));\n                n_past += common_prefix_length;\n            }\n            // Zero-out next_beam probabilities to place them last in following min-heap.\n            std::for_each(next_beams.begin(), next_beams.end(), [](llama_beam & beam) { beam.p = 0.0f; });\n            for (llama_beam & beam : beams) {\n                beam.shift_tokens(common_prefix_length);\n                fill_next_beams_by_top_probabilities(beam);\n            }\n            // next_beams become the beams of next/final iteration. Swap them to re-use memory.\n            beams.swap(next_beams);\n            renormalize_beam_probabilities(beams);\n        }\n        collapse_beams(top_beam_index());\n        callback(callback_data, get_beams_state(true));\n    }\n\n    // As beams grow, the cumulative probabilities decrease.\n    // Renormalize them to avoid floating point underflow.\n    static void renormalize_beam_probabilities(std::vector<llama_beam> & beams) {\n        const auto sum_p = [](float sum, llama_beam & beam) { return sum + beam.p; };\n        const float inv_sum = 1.0f / std::accumulate(beams.begin(), beams.end(), 0.0f, sum_p);\n        std::for_each(beams.begin(), beams.end(), [=](llama_beam & beam) { beam.p *= inv_sum; });\n    }\n\n    // Assumes beams is non-empty.  Uses llama_beam::operator<() for ordering.\n    size_t top_beam_index() {\n        return std::max_element(beams.begin(), beams.end()) - beams.begin();\n    }\n\n    // Copy (p,eob) for each beam which may have been changed by the callback.\n    void update_beams_from_beam_views() {\n        for (size_t i = 0 ; i < beams.size() ; ++i) {\n            beams[i].p = beam_views[i].p;\n            beams[i].eob = beam_views[i].eob;\n        }\n    }\n};\n\nvoid llama_beam_search(llama_context * ctx,\n                       llama_beam_search_callback_fn_t callback, void * callback_data,\n                       size_t n_beams, int n_past, int n_predict) {\n    assert(ctx);\n    const int64_t t_start_sample_us = ggml_time_us();\n\n    llama_beam_search_data beam_search_data(ctx, n_beams, n_past, n_predict);\n\n    beam_search_data.loop(callback, callback_data);\n\n    ctx->t_sample_us += ggml_time_us() - t_start_sample_us;\n    ctx->n_sample++;\n}\n\n//\n// quantization\n//\n\ntemplate <typename T>\nstruct no_init {\n    T value;\n    no_init() { /* do nothing */ }\n};\n\nstruct quantize_state_internal {\n    const llama_model                 & model;\n    const llama_model_quantize_params * params;\n\n    int n_attention_wv    = 0;\n    int n_feed_forward_w2 = 0;\n    int i_attention_wv    = 0;\n    int i_feed_forward_w2 = 0;\n\n    int n_k_quantized     = 0;\n    int n_fallback        = 0;\n\n    quantize_state_internal(const llama_model & model, const llama_model_quantize_params * params)\n        : model(model)\n        , params(params)\n        {}\n};\n\nstatic void llama_convert_tensor_internal(\n    struct ggml_tensor * tensor, std::vector<no_init<float>> & output, std::vector<std::thread> & workers,\n    const size_t nelements, const int nthread\n) {\n    if (output.size() < nelements) {\n        output.resize(nelements);\n    }\n    float * f32_output = (float *) output.data();\n\n    ggml_type_traits_t qtype;\n    if (ggml_is_quantized(tensor->type)) {\n        qtype = ggml_internal_get_type_traits(tensor->type);\n        if (qtype.to_float == NULL) {\n            throw std::runtime_error(format(\"type %s unsupported for integer quantization: no dequantization available\", ggml_type_name(tensor->type)));\n        }\n    } else if (tensor->type != GGML_TYPE_F16) {\n        throw std::runtime_error(format(\"cannot dequantize/convert tensor type %s\", ggml_type_name(tensor->type)));\n    }\n\n    if (nthread < 2) {\n        if (tensor->type == GGML_TYPE_F16) {\n            ggml_fp16_to_fp32_row((ggml_fp16_t *)tensor->data, f32_output, nelements);\n        } else if (ggml_is_quantized(tensor->type)) {\n            qtype.to_float(tensor->data, f32_output, nelements);\n        } else {\n            GGML_ASSERT(false); // unreachable\n        }\n        return;\n    }\n\n    auto block_size = tensor->type == GGML_TYPE_F16 ? 1 : (size_t)ggml_blck_size(tensor->type);\n    auto block_size_bytes = ggml_type_size(tensor->type);\n\n    GGML_ASSERT(nelements % block_size == 0);\n    auto nblocks = nelements / block_size;\n    auto blocks_per_thread = nblocks / nthread;\n    auto spare_blocks = nblocks - (blocks_per_thread * nthread); // if blocks aren't divisible by thread count\n\n    for (auto tnum = 0, in_buff_offs = 0, out_buff_offs = 0; tnum < nthread; tnum++) {\n        auto thr_blocks = blocks_per_thread + (tnum == nthread - 1 ? spare_blocks : 0); // num blocks for this thread\n        auto thr_elems = thr_blocks * block_size; // number of elements for this thread\n        auto thr_block_bytes = thr_blocks * block_size_bytes; // number of input bytes for this thread\n\n        auto compute = [qtype] (ggml_type typ, uint8_t * inbuf, float * outbuf, int nels) {\n            if (typ == GGML_TYPE_F16) {\n                ggml_fp16_to_fp32_row((ggml_fp16_t *)inbuf, outbuf, nels);\n            } else {\n                qtype.to_float(inbuf, outbuf, nels);\n            }\n        };\n        workers.emplace_back(compute, tensor->type, (uint8_t *) tensor->data + in_buff_offs, f32_output + out_buff_offs, thr_elems);\n        in_buff_offs += thr_block_bytes;\n        out_buff_offs += thr_elems;\n    }\n    for (auto & w : workers) { w.join(); }\n    workers.clear();\n}\n\nstatic ggml_type get_k_quant_type(\n    quantize_state_internal & qs,\n    ggml_type new_type, const ggml_tensor * tensor, llama_ftype ftype\n) {\n    const std::string name = ggml_get_name(tensor);\n    // TODO: avoid hardcoded tensor names - use the TN_* constants\n    const llm_arch arch = qs.model.arch;\n    const auto       tn = LLM_TN(arch);\n\n    auto use_more_bits = [](int i_layer, int num_layers) -> bool {\n        return i_layer < num_layers/8 || i_layer >= 7*num_layers/8 || (i_layer - num_layers/8)%3 == 2;\n    };\n\n    if (name == tn(LLM_TENSOR_OUTPUT, \"weight\").first) {\n        int nx = tensor->ne[0];\n        if (arch == LLM_ARCH_FALCON || nx % QK_K != 0) {\n            new_type = GGML_TYPE_Q8_0;\n        }\n        else if (new_type != GGML_TYPE_Q8_0) {\n            new_type = GGML_TYPE_Q6_K;\n        }\n    } else if (name.find(\"attn_v.weight\") != std::string::npos) {\n        if      (ftype == LLAMA_FTYPE_MOSTLY_Q2_K) new_type = GGML_TYPE_Q3_K;\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q3_K_M) {\n            new_type = qs.i_attention_wv < 2 ? GGML_TYPE_Q5_K : GGML_TYPE_Q4_K;\n        }\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q3_K_L) new_type = GGML_TYPE_Q5_K;\n        else if ((ftype == LLAMA_FTYPE_MOSTLY_Q4_K_M || ftype == LLAMA_FTYPE_MOSTLY_Q5_K_M) &&\n                use_more_bits(qs.i_attention_wv, qs.n_attention_wv)) new_type = GGML_TYPE_Q6_K;\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q4_K_S && qs.i_attention_wv < 4) new_type = GGML_TYPE_Q5_K;\n        else if (QK_K == 64 && (ftype == LLAMA_FTYPE_MOSTLY_Q4_K_S || ftype == LLAMA_FTYPE_MOSTLY_Q3_K_S) &&\n                (qs.i_attention_wv < qs.n_attention_wv/8 || qs.i_attention_wv >= 7*qs.n_attention_wv/8)) new_type = GGML_TYPE_Q6_K;\n        if (qs.model.type == MODEL_70B) {\n            // In the 70B model we have 8 heads sharing the same attn_v weights. As a result, the attn_v.weight tensor is\n            // 8x smaller compared to attn_q.weight. Hence, we can get a nice boost in quantization accuracy with\n            // nearly negligible increase in model size by quantizing this tensor with more bits:\n            if (new_type == GGML_TYPE_Q3_K || new_type == GGML_TYPE_Q4_K) new_type = GGML_TYPE_Q5_K;\n        }\n        ++qs.i_attention_wv;\n    } else if (name.find(\"ffn_down.weight\") != std::string::npos) {\n        if      (ftype == LLAMA_FTYPE_MOSTLY_Q2_K) new_type = GGML_TYPE_Q3_K;\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q3_K_M) {\n            new_type = qs.i_feed_forward_w2 < 2 ? GGML_TYPE_Q5_K\n                     : arch != LLM_ARCH_FALCON || use_more_bits(qs.i_feed_forward_w2, qs.n_feed_forward_w2) ? GGML_TYPE_Q4_K\n                     : GGML_TYPE_Q3_K;\n        }\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q3_K_L) {\n            new_type = arch == LLM_ARCH_FALCON ? GGML_TYPE_Q4_K : GGML_TYPE_Q5_K;\n        }\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q4_K_M) {\n            if (arch == LLM_ARCH_FALCON) {\n                new_type = qs.i_feed_forward_w2 < 2 ? GGML_TYPE_Q6_K :\n                           use_more_bits(qs.i_feed_forward_w2, qs.n_feed_forward_w2) ? GGML_TYPE_Q5_K : GGML_TYPE_Q4_K;\n            } else {\n                if (use_more_bits(qs.i_feed_forward_w2, qs.n_feed_forward_w2)) new_type = GGML_TYPE_Q6_K;\n            }\n        }\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q5_K_M && use_more_bits(qs.i_feed_forward_w2, qs.n_feed_forward_w2)) new_type = GGML_TYPE_Q6_K;\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q4_K_S && arch != LLM_ARCH_FALCON && qs.i_feed_forward_w2 < 4) {\n            new_type = GGML_TYPE_Q5_K;\n        }\n        ++qs.i_feed_forward_w2;\n    } else if (name.find(\"attn_output.weight\") != std::string::npos) {\n        if (arch != LLM_ARCH_FALCON) {\n            if      (ftype == LLAMA_FTYPE_MOSTLY_Q2_K  ) new_type = GGML_TYPE_Q3_K;\n            else if (ftype == LLAMA_FTYPE_MOSTLY_Q3_K_M) new_type = GGML_TYPE_Q4_K;\n            else if (ftype == LLAMA_FTYPE_MOSTLY_Q3_K_L) new_type = GGML_TYPE_Q5_K;\n        } else {\n            if (ftype == LLAMA_FTYPE_MOSTLY_Q3_K_L) new_type = GGML_TYPE_Q4_K;\n        }\n    }\n    else if (name.find(\"attn_qkv.weight\") != std::string::npos) {\n        if (ftype == LLAMA_FTYPE_MOSTLY_Q3_K_M || ftype == LLAMA_FTYPE_MOSTLY_Q3_K_L) new_type = GGML_TYPE_Q4_K;\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q4_K_M) new_type = GGML_TYPE_Q5_K;\n        else if (ftype == LLAMA_FTYPE_MOSTLY_Q5_K_M) new_type = GGML_TYPE_Q6_K;\n    }\n    else if (name.find(\"ffn_gate.weight\") != std::string::npos || name.find(\"ffn_up.weight\") != std::string::npos) {\n        if (ftype == LLAMA_FTYPE_MOSTLY_Q2_K) new_type = GGML_TYPE_Q3_K;\n    }\n    else if (name.find(\"fc1.weight\") != std::string::npos || name.find(\"fc2.weight\") != std::string::npos) {\n        if (ftype == LLAMA_FTYPE_MOSTLY_Q4_0) new_type = GGML_TYPE_Q4_0;\n        else new_type = GGML_TYPE_Q5_0;\n    }\n    // This can be used to reduce the size of the Q5_K_S model.\n    // The associated PPL increase is fully in line with the size reduction\n    //else {\n    //    if (ftype == LLAMA_FTYPE_MOSTLY_Q5_K_S) new_type = GGML_TYPE_Q4_K;\n    //}\n    bool convert_incompatible_tensor = false;\n    if (new_type == GGML_TYPE_Q2_K || new_type == GGML_TYPE_Q3_K || new_type == GGML_TYPE_Q4_K ||\n        new_type == GGML_TYPE_Q5_K || new_type == GGML_TYPE_Q6_K) {\n        int nx = tensor->ne[0];\n        int ny = tensor->ne[1];\n        if (nx % QK_K != 0) {\n            LLAMA_LOG_WARN(\"\\n\\n%s : tensor cols %d x %d are not divisible by %d, required for %s\", __func__, nx, ny, QK_K, ggml_type_name(new_type));\n            convert_incompatible_tensor = true;\n        } else {\n            ++qs.n_k_quantized;\n        }\n    }\n    if (convert_incompatible_tensor) {\n        switch (new_type) {\n            case GGML_TYPE_Q2_K: new_type = GGML_TYPE_Q4_0; break;\n            case GGML_TYPE_Q3_K: new_type = GGML_TYPE_Q4_1; break;\n            case GGML_TYPE_Q4_K: new_type = GGML_TYPE_Q5_0; break;\n            case GGML_TYPE_Q5_K: new_type = GGML_TYPE_Q5_1; break;\n            case GGML_TYPE_Q6_K: new_type = GGML_TYPE_Q8_0; break;\n            default: throw std::runtime_error(\"\\nUnsupported tensor size encountered\\n\");\n        }\n        LLAMA_LOG_WARN(\" - using fallback quantization %s\\n\", ggml_type_name(new_type));\n        ++qs.n_fallback;\n    }\n\n    return new_type;\n}\n\nstatic void llama_model_quantize_internal(const std::string & fname_inp, const std::string & fname_out, const llama_model_quantize_params * params) {\n    ggml_type quantized_type;\n    llama_ftype ftype = params->ftype;\n\n    switch (params->ftype) {\n        case LLAMA_FTYPE_MOSTLY_Q4_0: quantized_type = GGML_TYPE_Q4_0; break;\n        case LLAMA_FTYPE_MOSTLY_Q4_1: quantized_type = GGML_TYPE_Q4_1; break;\n        case LLAMA_FTYPE_MOSTLY_Q5_0: quantized_type = GGML_TYPE_Q5_0; break;\n        case LLAMA_FTYPE_MOSTLY_Q5_1: quantized_type = GGML_TYPE_Q5_1; break;\n        case LLAMA_FTYPE_MOSTLY_Q8_0: quantized_type = GGML_TYPE_Q8_0; break;\n        case LLAMA_FTYPE_MOSTLY_F16:  quantized_type = GGML_TYPE_F16;  break;\n        case LLAMA_FTYPE_ALL_F32:     quantized_type = GGML_TYPE_F32;  break;\n\n        // K-quants\n        case LLAMA_FTYPE_MOSTLY_Q2_K:   quantized_type = GGML_TYPE_Q2_K; break;\n        case LLAMA_FTYPE_MOSTLY_Q3_K_S:\n        case LLAMA_FTYPE_MOSTLY_Q3_K_M:\n        case LLAMA_FTYPE_MOSTLY_Q3_K_L: quantized_type = GGML_TYPE_Q3_K; break;\n        case LLAMA_FTYPE_MOSTLY_Q4_K_S:\n        case LLAMA_FTYPE_MOSTLY_Q4_K_M: quantized_type = GGML_TYPE_Q4_K; break;\n        case LLAMA_FTYPE_MOSTLY_Q5_K_S:\n        case LLAMA_FTYPE_MOSTLY_Q5_K_M: quantized_type = GGML_TYPE_Q5_K; break;\n        case LLAMA_FTYPE_MOSTLY_Q6_K:   quantized_type = GGML_TYPE_Q6_K; break;\n\n        default: throw std::runtime_error(format(\"invalid output file type %d\\n\", ftype));\n    }\n\n    int nthread = params->nthread;\n\n    if (nthread <= 0) {\n        nthread = std::thread::hardware_concurrency();\n    }\n\n    // mmap consistently increases speed Linux, and also increases speed on Windows with\n    // hot cache. It may cause a slowdown on macOS, possibly related to free memory.\n#if defined(__linux__) || defined(_WIN32)\n    constexpr bool use_mmap = true;\n#else\n    constexpr bool use_mmap = false;\n#endif\n\n    llama_model_loader ml(fname_inp, use_mmap);\n    if (ml.use_mmap) {\n        ml.mapping.reset(new llama_mmap(&ml.file, /* prefetch */ 0, ggml_is_numa()));\n    }\n\n    llama_model model;\n    llm_load_arch(ml, model);\n    llm_load_hparams(ml, model);\n\n    struct quantize_state_internal qs(model, params);\n\n    if (params->only_copy) {\n        ftype = model.ftype;\n    }\n\n    const size_t align = GGUF_DEFAULT_ALIGNMENT;\n    struct gguf_context * ctx_out = ml.sparse_deriv == GGML_SPARSE_INFERENCE ? gguf_init_empty_sparse() : gguf_init_empty();\n\n    // copy the KV pairs from the input file\n    gguf_set_kv     (ctx_out, ml.ctx_gguf);\n    gguf_set_val_u32(ctx_out, \"general.quantization_version\", GGML_QNT_VERSION);\n    gguf_set_val_u32(ctx_out, \"general.file_type\", ftype);\n\n    for (int i = 0; i < ml.n_tensors; ++i) {\n        struct ggml_tensor * meta = ml.get_tensor_meta(i);\n\n        const std::string name = ggml_get_name(meta);\n\n        // TODO: avoid hardcoded tensor names - use the TN_* constants\n        if (name.find(\"attn_v.weight\") != std::string::npos || name.find(\"attn_qkv.weight\") != std::string::npos) {\n            ++qs.n_attention_wv;\n        }\n        else if (name.find(\"ffn_down.weight\") != std::string::npos) {\n            ++qs.n_feed_forward_w2;\n        }\n    }\n    if (qs.n_attention_wv != qs.n_feed_forward_w2 || (uint32_t)qs.n_attention_wv != model.hparams.n_layer) {\n        LLAMA_LOG_WARN(\"%s ============ Strange model: n_attention_wv = %d, n_feed_forward_w2 = %d, hparams.n_layer = %d\\n\",\n                __func__, qs.n_attention_wv, qs.n_feed_forward_w2, model.hparams.n_layer);\n    }\n\n    size_t total_size_org = 0;\n    size_t total_size_new = 0;\n    std::vector<int64_t> hist_all(1 << 4, 0);\n\n    std::vector<std::thread> workers;\n    workers.reserve(nthread);\n    std::mutex mutex;\n\n    int idx = 0;\n\n    std::vector<no_init<uint8_t>> read_data;\n    std::vector<no_init<uint8_t>> work;\n    std::vector<no_init<float>> f32_conv_buf;\n\n    // populate the original tensors so we get an initial meta data\n    for (int i = 0; i < ml.n_tensors; ++i) {\n        struct ggml_tensor * meta = ml.get_tensor_meta(i);\n        gguf_add_tensor(ctx_out, meta);\n    }\n\n    std::ofstream fout(fname_out, std::ios::binary);\n    fout.exceptions(std::ofstream::failbit); // fail fast on write errors\n\n    const size_t meta_size = gguf_get_meta_size(ctx_out);\n\n    LLAMA_LOG_INFO(\"%s: meta size = %zu bytes\\n\", __func__, meta_size);\n\n    // placeholder for the meta data\n    ::zeros(fout, meta_size);\n\n    for (int i = 0; i < ml.n_tensors; ++i) {\n        struct ggml_tensor * tensor = ml.get_tensor_meta(i);\n\n        const std::string name = ggml_get_name(tensor);\n\n        if (!ml.use_mmap) {\n            if (read_data.size() < ggml_nbytes(tensor)) {\n                read_data.resize(ggml_nbytes(tensor));\n            }\n            tensor->data = read_data.data();\n        }\n        ml.load_data_for(tensor);\n\n        LLAMA_LOG_INFO(\"[%4d/%4d] %36s - [%s], type = %6s, \",\n               ++idx, ml.n_tensors,\n               ggml_get_name(tensor),\n               llama_format_tensor_shape(tensor).c_str(),\n               ggml_type_name(tensor->type));\n\n        // This used to be a regex, but <regex> has an extreme cost to compile times.\n        bool quantize = name.rfind(\"weight\") == name.size() - 6; // ends with 'weight'?\n\n        // quantize only 2D tensors\n        quantize &= (tensor->n_dims == 2);\n        quantize &= params->quantize_output_tensor || name != \"output.weight\";\n        quantize &= !params->only_copy;\n\n        enum ggml_type new_type;\n        void * new_data;\n        size_t new_size;\n\n        if (quantize) {\n            new_type = quantized_type;\n            if (!params->pure) {\n                new_type = get_k_quant_type(qs, new_type, tensor, ftype);\n            }\n\n            // If we've decided to quantize to the same type the tensor is already\n            // in then there's nothing to do.\n            quantize = tensor->type != new_type;\n        }\n        if (!quantize) {\n            new_type = tensor->type;\n            new_data = tensor->data;\n            new_size = ggml_nbytes(tensor);\n            LLAMA_LOG_INFO(\"size = %8.3f MB\\n\", ggml_nbytes(tensor)/1024.0/1024.0);\n        } else {\n            const size_t nelements = ggml_nelements(tensor);\n\n            float * f32_data;\n\n            if (tensor->type == GGML_TYPE_F32) {\n                f32_data = (float *) tensor->data;\n            } else if (ggml_is_quantized(tensor->type) && !params->allow_requantize) {\n                throw std::runtime_error(format(\"requantizing from type %s is disabled\", ggml_type_name(tensor->type)));\n            } else {\n                llama_convert_tensor_internal(tensor, f32_conv_buf, workers, nelements, nthread);\n                f32_data = (float *) f32_conv_buf.data();\n            }\n\n            LLAMA_LOG_INFO(\"quantizing to %s .. \", ggml_type_name(new_type));\n            fflush(stdout);\n\n            if (work.size() < nelements * 4) {\n                work.resize(nelements * 4); // upper bound on size\n            }\n            new_data = work.data();\n            std::array<int64_t, 1 << 4> hist_cur = {};\n\n            static const int chunk_size = 32 * 512;\n            const int nchunk = (nelements + chunk_size - 1)/chunk_size;\n            const int nthread_use = nthread > 1 ? std::max(1, std::min(nthread, nchunk)) : 1;\n            if (nthread_use < 2) {\n                new_size = ggml_quantize_chunk(new_type, f32_data, new_data, 0, nelements, hist_cur.data());\n            } else {\n                size_t counter = 0;\n                new_size = 0;\n                auto compute = [&mutex, &counter, &hist_cur, &new_size, new_type, f32_data, new_data, nelements]() {\n                    std::array<int64_t, 1 << 4> local_hist = {};\n                    size_t local_size = 0;\n                    while (true) {\n                        std::unique_lock<std::mutex> lock(mutex);\n                        size_t first = counter; counter += chunk_size;\n                        if (first >= nelements) {\n                            if (local_size > 0) {\n                                for (int j=0; j<int(local_hist.size()); ++j) {\n                                    hist_cur[j] += local_hist[j];\n                                }\n                                new_size += local_size;\n                            }\n                            break;\n                        }\n                        lock.unlock();\n                        size_t last = std::min(nelements, first + chunk_size);\n                        local_size += ggml_quantize_chunk(new_type, f32_data, new_data, first, last - first, local_hist.data());\n                    }\n                };\n                for (int it = 0; it < nthread_use - 1; ++it) {\n                    workers.emplace_back(compute);\n                }\n                compute();\n                for (auto & w : workers) { w.join(); }\n                workers.clear();\n            }\n\n            LLAMA_LOG_INFO(\"size = %8.2f MB -> %8.2f MB | hist: \", ggml_nbytes(tensor)/1024.0/1024.0, new_size/1024.0/1024.0);\n            int64_t tot_count = 0;\n            for (size_t i = 0; i < hist_cur.size(); i++) {\n                hist_all[i] += hist_cur[i];\n                tot_count += hist_cur[i];\n            }\n\n            if (tot_count > 0) {\n                for (size_t i = 0; i < hist_cur.size(); i++) {\n                    LLAMA_LOG_INFO(\"%5.3f \", hist_cur[i] / float(nelements));\n                }\n            }\n            LLAMA_LOG_INFO(\"\\n\");\n        }\n        total_size_org += ggml_nbytes(tensor);\n        total_size_new += new_size;\n\n        // update the gguf meta data as we go\n        gguf_set_tensor_type(ctx_out, name.c_str(), new_type);\n        gguf_set_tensor_data(ctx_out, name.c_str(), new_data, new_size);\n\n        // write tensor data + padding\n        fout.write((const char *) new_data, new_size);\n        zeros(fout, GGML_PAD(new_size, align) - new_size);\n    }\n\n    // go back to beginning of file and write the updated meta data\n    {\n        fout.seekp(0);\n        std::vector<uint8_t> data(gguf_get_meta_size(ctx_out));\n        gguf_get_meta_data(ctx_out, data.data());\n        fout.write((const char *) data.data(), data.size());\n    }\n\n    fout.close();\n\n    gguf_free(ctx_out);\n\n    LLAMA_LOG_INFO(\"%s: model size  = %8.2f MB\\n\", __func__, total_size_org/1024.0/1024.0);\n    LLAMA_LOG_INFO(\"%s: quant size  = %8.2f MB\\n\", __func__, total_size_new/1024.0/1024.0);\n\n    // print histogram for all tensors\n    {\n        int64_t sum_all = 0;\n        for (size_t i = 0; i < hist_all.size(); i++) {\n            sum_all += hist_all[i];\n        }\n\n        if (sum_all > 0) {\n            LLAMA_LOG_INFO(\"%s: hist: \", __func__);\n            for (size_t i = 0; i < hist_all.size(); i++) {\n                LLAMA_LOG_INFO(\"%5.3f \", hist_all[i] / float(sum_all));\n            }\n            LLAMA_LOG_INFO(\"\\n\");\n        }\n    }\n\n    if (qs.n_fallback > 0) {\n        LLAMA_LOG_WARN(\"%s: WARNING: %d of %d tensor(s) incompatible with k-quants and required fallback quantization\\n\",\n                __func__, qs.n_fallback, qs.n_k_quantized + qs.n_fallback);\n    }\n}\n\nstatic int llama_apply_lora_from_file_internal(\n    const struct llama_model & model, const char * path_lora, float scale, const char * path_base_model, int n_threads\n) {\n    LLAMA_LOG_INFO(\"%s: applying lora adapter from '%s' - please wait ...\\n\", __func__, path_lora);\n\n    const int64_t t_start_lora_us = ggml_time_us();\n\n    auto fin = std::ifstream(path_lora, std::ios::binary);\n    if (!fin) {\n        LLAMA_LOG_ERROR(\"%s: failed to open '%s'\\n\", __func__, path_lora);\n        return 1;\n    }\n\n    // verify magic and version\n    {\n        uint32_t magic;\n        fin.read((char *) &magic, sizeof(magic));\n        uint32_t format_version;\n        fin.read((char *) &format_version, sizeof(format_version));\n\n        if (format_version != 1) {\n            LLAMA_LOG_ERROR(\"%s: unsupported file version\\n\", __func__ );\n            return 1;\n        }\n    }\n\n    int32_t lora_r;\n    int32_t lora_alpha;\n    fin.read((char *) &lora_r, sizeof(lora_r));\n    fin.read((char *) &lora_alpha, sizeof(lora_alpha));\n    float scaling = scale * (float)lora_alpha / (float)lora_r;\n\n    LLAMA_LOG_INFO(\"%s: r = %d, alpha = %d, scaling = %.2f\\n\", __func__, lora_r, lora_alpha, scaling);\n\n    // create a temporary ggml context to store the lora tensors\n    // todo: calculate size from biggest possible tensor\n    std::vector<uint8_t> lora_buf(1024ull * 1024ull * 1024ull);\n    struct ggml_init_params params;\n    params.mem_size   = lora_buf.size();\n    params.mem_buffer = lora_buf.data();\n    params.no_alloc   = false;\n\n    ggml_context * lora_ctx = ggml_init(params);\n    std::unordered_map<std::string, struct ggml_tensor *> lora_tensors;\n\n    // create a name -> tensor map of the model to accelerate lookups\n    std::unordered_map<std::string, struct ggml_tensor*> model_tensors;\n    for (const auto & kv : model.tensors_by_name) {\n        model_tensors.insert(kv);\n    }\n\n    // load base model\n    std::unique_ptr<llama_model_loader> ml;\n    ggml_context * base_ctx = NULL;\n    std::vector<uint8_t> base_buf;\n    if (path_base_model) {\n        LLAMA_LOG_INFO(\"%s: loading base model from '%s'\\n\", __func__, path_base_model);\n        ml.reset(new llama_model_loader(path_base_model, /*use_mmap*/ true));\n\n        size_t ctx_size;\n        size_t mmapped_size;\n        ml->calc_sizes(ctx_size, mmapped_size);\n        base_buf.resize(ctx_size);\n\n        ggml_init_params base_params;\n        base_params.mem_size   = base_buf.size();\n        base_params.mem_buffer = base_buf.data();\n        base_params.no_alloc   = ml->use_mmap;\n\n        base_ctx = ggml_init(base_params);\n\n        // maybe this should in llama_model_loader\n        if (ml->use_mmap) {\n            ml->mapping.reset(new llama_mmap(&ml->file, /* prefetch */ 0, ggml_is_numa()));\n        }\n    }\n\n    // read tensors and apply\n    bool warned = false;\n    int n_tensors = 0;\n\n    std::vector<uint8_t> work_buffer;\n\n    while (true) {\n        int32_t n_dims;\n        int32_t length;\n        int32_t ftype;\n\n        fin.read(reinterpret_cast<char *>(&n_dims), sizeof(n_dims));\n        fin.read(reinterpret_cast<char *>(&length), sizeof(length));\n        fin.read(reinterpret_cast<char *>(&ftype),  sizeof(ftype));\n        if (fin.eof()) {\n            break;\n        }\n\n        int32_t ne[2] = { 1, 1 };\n        for (int i = 0; i < n_dims; ++i) {\n            fin.read(reinterpret_cast<char *>(&ne[i]), sizeof(ne[i]));\n        }\n\n        std::string name;\n        {\n            char buf[1024];\n            fin.read(buf, length);\n            name = std::string(buf, length);\n        }\n\n        // check for lora suffix and get the type of tensor\n        const std::string lora_suffix = \".lora\";\n        size_t pos = name.rfind(lora_suffix);\n        if (pos == std::string::npos) {\n            LLAMA_LOG_ERROR(\"%s: error: '%s' is not a lora tensor\\n\", __func__, name.c_str());\n            return 1;\n        }\n\n        std::string lora_type = name.substr(pos + lora_suffix.length());\n        std::string base_name = name;\n        base_name.erase(pos);\n        // LLAMA_LOG_INFO(\"%s: %s => %s (lora type %s) \\n\", __func__, name.c_str(),base_name.c_str(), lora_type.c_str());\n\n        if (model_tensors.find(base_name) == model_tensors.end()) {\n            LLAMA_LOG_ERROR(\"%s: unknown tensor '%s' in lora adapter\\n\", __func__, name.data());\n            return 1;\n        }\n\n        // create ggml tensor\n        ggml_type wtype;\n        switch (ftype) {\n            case 0: wtype = GGML_TYPE_F32;  break;\n            case 1: wtype = GGML_TYPE_F16;  break;\n            default:\n                    {\n                        LLAMA_LOG_ERROR(\"%s: invalid tensor data type '%d'\\n\",\n                                __func__, ftype);\n                        return false;\n                    }\n        }\n        ggml_tensor * lora_tensor;\n        if (n_dims == 2) {\n            lora_tensor = ggml_new_tensor_2d(lora_ctx, wtype, ne[0], ne[1]);\n        }\n        else {\n            LLAMA_LOG_ERROR(\"%s: unsupported tensor dimension %d\\n\", __func__, n_dims);\n            return 1;\n        }\n        ggml_set_name(lora_tensor, \"lora_tensor\");\n\n        // load tensor data\n        size_t offset = fin.tellg();\n        size_t tensor_data_size = ggml_nbytes(lora_tensor);\n        offset = (offset + 31) & -32;\n        fin.seekg(offset);\n        fin.read((char*)lora_tensor->data, tensor_data_size);\n\n        lora_tensors[name] = lora_tensor;\n\n        // check if we have both A and B tensors and apply\n        if (lora_tensors.find(base_name + \".loraA\") != lora_tensors.end() &&\n            lora_tensors.find(base_name + \".loraB\") != lora_tensors.end()) {\n\n            ggml_tensor * dest_t = model_tensors[base_name];\n\n            offload_func_t offload_func               = ggml_offload_nop;\n            offload_func_t offload_func_force_inplace = ggml_offload_nop;\n\n#ifdef GGML_USE_CUBLAS\n            if (dest_t->backend == GGML_BACKEND_GPU || dest_t->backend == GGML_BACKEND_GPU_SPLIT) {\n                if (dest_t->type != GGML_TYPE_F16) {\n                    throw std::runtime_error(format(\n                        \"%s: error: the simultaneous use of LoRAs and GPU acceleration is only supported for f16 models. dest_t->type: %d\", __func__, dest_t->type));\n                }\n                offload_func = ggml_cuda_assign_buffers;\n                offload_func_force_inplace = ggml_cuda_assign_buffers_force_inplace;\n            }\n#endif // GGML_USE_CUBLAS\n\n            ggml_tensor * base_t;\n            if (ml) {\n                struct gguf_context * ctx_gguf = ml->ctx_gguf;\n\n                // load from base model\n                if (gguf_find_tensor(ctx_gguf, base_name.c_str()) < 0) {\n                    // TODO: throw\n                    LLAMA_LOG_ERROR(\"%s: error: tensor '%s' not found in base model\\n\", __func__, base_name.c_str());\n                    return 1;\n                }\n\n                // TODO: not tested!! maybe not working!\n                base_t = ml->create_tensor(base_ctx, base_name, { (uint32_t)dest_t->ne[0], (uint32_t)dest_t->ne[1] }, GGML_BACKEND_CPU);\n                ml->load_data_for(base_t);\n            } else {\n                base_t = dest_t;\n            }\n\n            if (ggml_is_quantized(base_t->type)) {\n                if (!warned) {\n                    LLAMA_LOG_WARN(\"%s: warning: using a lora adapter with a quantized model may result in poor quality, \"\n                                   \"use a f16 or f32 base model with --lora-base\\n\", __func__);\n                    warned = true;\n                }\n            }\n\n            ggml_tensor * loraA = lora_tensors[base_name + \".loraA\"];\n            GGML_ASSERT(loraA->type == GGML_TYPE_F32);\n            ggml_set_name(loraA, \"loraA\");\n\n            ggml_tensor * loraB = lora_tensors[base_name + \".loraB\"];\n            GGML_ASSERT(loraB->type == GGML_TYPE_F32);\n            ggml_set_name(loraB, \"loraB\");\n\n            if (base_t->ne[0] != loraA->ne[1] || base_t->ne[1] != loraB->ne[1]) {\n                LLAMA_LOG_ERROR(\"%s: incompatible tensor dimensions (%\" PRId64 \" and %\" PRId64 \");\"\n                                \" are you sure that this adapter is for this model?\\n\", __func__, base_t->ne[0], loraA->ne[1]);\n                return 1;\n            }\n\n            // w = w + BA*s\n            ggml_tensor * BA = ggml_mul_mat(lora_ctx, loraA, loraB);\n            offload_func(BA);\n            ggml_set_name(BA, \"BA\");\n\n            if (scaling != 1.0f) {\n                ggml_tensor * scale_tensor = ggml_new_f32(lora_ctx, scaling);\n                ggml_set_name(scale_tensor, \"scale_tensor\");\n\n                BA = ggml_scale_inplace(lora_ctx, BA, scale_tensor);\n                offload_func(BA);\n                ggml_set_name(BA, \"BA_scaled\");\n            }\n\n            ggml_tensor * r;\n            if (base_t == dest_t) {\n                r = ggml_add_inplace(lora_ctx, dest_t, BA);\n                offload_func_force_inplace(r);\n                ggml_set_name(r, \"r_add_inplace\");\n            }\n            else {\n                r = ggml_add(lora_ctx, base_t, BA);\n                offload_func(r);\n                ggml_set_name(r, \"r_add\");\n\n                r = ggml_cpy(lora_ctx, r, dest_t);\n                offload_func(r);\n                ggml_set_name(r, \"r_cpy\");\n            }\n\n            struct ggml_cgraph * gf = ggml_new_graph(lora_ctx);\n            ggml_build_forward_expand(gf, r);\n\n            ggml_graph_compute_helper(work_buffer, gf, n_threads);\n\n            // we won't need these tensors again, reset the context to save memory\n            ggml_free(lora_ctx);\n            lora_ctx = ggml_init(params);\n            lora_tensors.clear();\n\n            n_tensors++;\n            if (n_tensors % 4 == 0) {\n                LLAMA_LOG_INFO(\".\");\n            }\n        }\n    }\n\n    // TODO: this should be in a destructor, it will leak on failure\n    ggml_free(lora_ctx);\n    if (base_ctx) {\n        ggml_free(base_ctx);\n    }\n\n    const int64_t t_lora_us = ggml_time_us() - t_start_lora_us;\n    LLAMA_LOG_INFO(\" done (%.2f ms)\\n\", t_lora_us / 1000.0);\n\n    return 0;\n}\n\n//\n// interface implementation\n//\nstruct llama_model_params llama_model_default_params() {\n    struct llama_model_params result = {\n        /*.n_gpu_layers                =*/ 0,\n        /*.main_gpu                    =*/ 0,\n        /*.vram_budget_gb              =*/ -1.0,\n        /*.tensor_split                =*/ nullptr,\n        /*.progress_callback           =*/ nullptr,\n        /*.progress_callback_user_data =*/ nullptr,\n        /*.vocab_only                  =*/ false,\n        /*.use_mmap                    =*/ true,\n        /*.use_mlock                   =*/ false,\n    };\n\n#ifdef GGML_USE_METAL\n    result.n_gpu_layers = 1;\n#endif\n\n    return result;\n}\n\nstruct llama_context_params llama_context_default_params() {\n    struct llama_context_params result = {\n        /*.seed                        =*/ LLAMA_DEFAULT_SEED,\n        /*.n_ctx                       =*/ 512,\n        /*.n_batch                     =*/ 512,\n        /*.n_threads                   =*/ GGML_DEFAULT_N_THREADS, // TODO: better default\n        /*.n_threads_batch             =*/ GGML_DEFAULT_N_THREADS,\n        /*.rope_scaling_type           =*/ LLAMA_ROPE_SCALING_UNSPECIFIED,\n        /*.rope_freq_base              =*/ 0.0f,\n        /*.rope_freq_scale             =*/ 0.0f,\n        /*.yarn_ext_factor             =*/ -1.0f,\n        /*.yarn_attn_factor            =*/ 1.0f,\n        /*.yarn_beta_fast              =*/ 32.0f,\n        /*.yarn_beta_slow              =*/ 1.0f,\n        /*.yarn_orig_ctx               =*/ 0,\n        /*.mul_mat_q                   =*/ true,\n        /*.f16_kv                      =*/ true,\n        /*.logits_all                  =*/ false,\n        /*.embedding                   =*/ false,\n    };\n\n    return result;\n}\n\nstruct llama_model_quantize_params llama_model_quantize_default_params() {\n    struct llama_model_quantize_params result = {\n        /*.nthread                     =*/ 0,\n        /*.ftype                       =*/ LLAMA_FTYPE_MOSTLY_Q5_1,\n        /*.allow_requantize            =*/ false,\n        /*.quantize_output_tensor      =*/ true,\n        /*.only_copy                   =*/ false,\n        /*.pure                        =*/ false,\n    };\n\n    return result;\n}\n\nint llama_max_devices(void) {\n    return LLAMA_MAX_DEVICES;\n}\n\nbool llama_mmap_supported(void) {\n    return llama_mmap::SUPPORTED;\n}\n\nbool llama_mlock_supported(void) {\n    return llama_mlock::SUPPORTED;\n}\n\nvoid llama_backend_init(bool numa) {\n    ggml_time_init();\n\n    // needed to initialize f16 tables\n    {\n        struct ggml_init_params params = { 0, NULL, false };\n        struct ggml_context * ctx = ggml_init(params);\n        ggml_free(ctx);\n    }\n\n    if (numa) {\n        ggml_numa_init();\n    }\n\n#ifdef GGML_USE_MPI\n    ggml_mpi_backend_init();\n#endif\n}\n\nvoid llama_backend_free(void) {\n#ifdef GGML_USE_MPI\n    ggml_mpi_backend_free();\n#endif\n}\n\nint64_t llama_time_us(void) {\n    return ggml_time_us();\n}\n\nstruct llama_model * llama_load_model_from_file_with_context(\n    const char * path_model,\n    struct llama_model_params   params,\n    struct llama_context_params * cparams\n) {\n    ggml_time_init();\n\n    llama_model * model = new llama_model;\n\n    unsigned cur_percentage = 0;\n    if (params.progress_callback == NULL) {\n        params.progress_callback_user_data = &cur_percentage;\n        params.progress_callback = [](float progress, void * ctx) {\n            unsigned * cur_percentage_p = (unsigned *) ctx;\n            unsigned percentage = (unsigned) (100 * progress);\n            while (percentage > *cur_percentage_p) {\n                *cur_percentage_p = percentage;\n                LLAMA_LOG_INFO(\".\");\n                if (percentage >= 100) {\n                    LLAMA_LOG_INFO(\"\\n\");\n                }\n            }\n        };\n    }\n\n    if (!llama_model_load(path_model, *model, params, cparams)) {\n        LLAMA_LOG_ERROR(\"%s: failed to load model\\n\", __func__);\n        delete model;\n        return nullptr;\n    }\n\n    return model;\n}\n\nstruct llama_model * llama_load_model_from_file(\n                             const char * path_model,\n              struct llama_model_params   params) {\n    return llama_load_model_from_file_with_context(path_model, params, nullptr);\n}\n\nvoid llama_free_model(struct llama_model * model) {\n    delete model;\n}\n\nstruct llama_context * llama_new_context_with_model(\n                 struct llama_model * model,\n        struct llama_context_params   params) {\n\n    if (!model) {\n        return nullptr;\n    }\n\n    llama_context * ctx = new llama_context(*model);\n\n    const auto & hparams = model->hparams;\n    auto       & cparams = ctx->cparams;\n\n    cparams.n_batch          = params.n_batch;\n    cparams.n_threads        = params.n_threads;\n    cparams.n_threads_batch  = params.n_threads_batch;\n    cparams.yarn_ext_factor  = params.yarn_ext_factor;\n    cparams.yarn_attn_factor = params.yarn_attn_factor;\n    cparams.yarn_beta_fast   = params.yarn_beta_fast;\n    cparams.yarn_beta_slow   = params.yarn_beta_slow;\n    cparams.mul_mat_q        = params.mul_mat_q;\n\n    cparams.n_ctx            = params.n_ctx           == 0    ? hparams.n_ctx_train           : params.n_ctx;\n    cparams.rope_freq_base   = params.rope_freq_base  == 0.0f ? hparams.rope_freq_base_train  : params.rope_freq_base;\n    cparams.rope_freq_scale  = params.rope_freq_scale == 0.0f ? hparams.rope_freq_scale_train : params.rope_freq_scale;\n\n    cparams.n_yarn_orig_ctx  = params.yarn_orig_ctx    != 0 ? params.yarn_orig_ctx    :\n                               hparams.n_yarn_orig_ctx != 0 ? hparams.n_yarn_orig_ctx :\n                                                              hparams.n_ctx_train;\n\n    auto rope_scaling_type = params.rope_scaling_type;\n    if (rope_scaling_type == LLAMA_ROPE_SCALING_UNSPECIFIED) {\n        rope_scaling_type = hparams.rope_scaling_type_train;\n    }\n\n    if (rope_scaling_type == LLAMA_ROPE_SCALING_NONE) {\n        cparams.rope_freq_scale = 1.0f; // never scale if scaling type is none\n    }\n\n    if (cparams.yarn_ext_factor < 0.0f) { // negative indicates 'not set'\n        cparams.yarn_ext_factor = rope_scaling_type == LLAMA_ROPE_SCALING_YARN ? 1.0f : 0.0f;\n    }\n\n    if (params.seed == LLAMA_DEFAULT_SEED) {\n        params.seed = time(NULL);\n    }\n\n    LLAMA_LOG_INFO(\"%s: n_ctx      = %u\\n\",     __func__, cparams.n_ctx);\n    LLAMA_LOG_INFO(\"%s: freq_base  = %.1f\\n\",   __func__, cparams.rope_freq_base);\n    LLAMA_LOG_INFO(\"%s: freq_scale = %g\\n\",     __func__, cparams.rope_freq_scale);\n\n    ctx->rng = std::mt19937(params.seed);\n    ctx->logits_all = params.logits_all;\n\n    ggml_type memory_type = params.f16_kv ? GGML_TYPE_F16 : GGML_TYPE_F32;\n\n    // reserve memory for context buffers\n    if (!hparams.vocab_only) {\n        if (!llama_kv_cache_init(ctx->model.hparams, ctx->kv_self, memory_type, cparams.n_ctx, model->n_gpu_layers)) {\n            LLAMA_LOG_ERROR(\"%s: llama_kv_cache_init() failed for self-attention cache\\n\", __func__);\n            llama_free(ctx);\n            return nullptr;\n        }\n\n        {\n            const size_t memory_size = ggml_nbytes(ctx->kv_self.k) + ggml_nbytes(ctx->kv_self.v);\n            LLAMA_LOG_INFO(\"%s: kv self size  = %7.2f MB\\n\", __func__, memory_size / 1024.0 / 1024.0);\n        }\n\n        // resized during inference\n        if (params.logits_all) {\n            ctx->logits.reserve(cparams.n_ctx*hparams.n_vocab);\n        } else {\n            ctx->logits.reserve(hparams.n_vocab);\n        }\n\n        if (params.embedding){\n            ctx->embedding.resize(hparams.n_embd);\n        }\n\n        {\n            static const size_t tensor_alignment = 32;\n            // the compute buffer is used to store the tensor and graph structs, while the allocator buffer is used for the tensor data\n            ctx->buf_compute.resize(ggml_tensor_overhead()*LLAMA_MAX_NODES + ggml_graph_overhead());\n\n            // create measure allocator\n            ctx->alloc = ggml_allocr_new_measure(tensor_alignment);\n\n            // build worst-case graph\n            int n_tokens = (int)std::min(cparams.n_ctx, cparams.n_batch);\n            int n_past = cparams.n_ctx - n_tokens;\n            llama_token token = llama_token_bos(&ctx->model); // not actually used by llama_build_graph, but required to choose between token and embedding inputs graph\n            ggml_cgraph * gf = llama_build_graph(*ctx, llama_batch_get_one(&token, n_tokens, n_past, 0));\n\n#ifdef GGML_USE_METAL\n            if (model->n_gpu_layers > 0) {\n                ggml_metal_log_set_callback(llama_log_callback_default, NULL);\n\n                ctx->ctx_metal = ggml_metal_init(1);\n                if (!ctx->ctx_metal) {\n                    LLAMA_LOG_ERROR(\"%s: ggml_metal_init() failed\\n\", __func__);\n                    llama_free(ctx);\n                    return NULL;\n                }\n                //ggml_metal_graph_find_concurrency(ctx->ctx_metal, gf, false);\n                //ggml_allocr_set_parse_seq(ctx->alloc, ggml_metal_get_concur_list(ctx->ctx_metal), ggml_metal_if_optimized(ctx->ctx_metal));\n            }\n#endif\n            // measure memory requirements for the graph\n            size_t alloc_size = ggml_allocr_alloc_graph(ctx->alloc, gf) + tensor_alignment;\n\n            LLAMA_LOG_INFO(\"%s: compute buffer total size = %.2f MB\\n\", __func__, (ctx->buf_compute.size + alloc_size) / 1024.0 / 1024.0);\n\n            // recreate allocator with exact memory requirements\n            ggml_allocr_free(ctx->alloc);\n\n            ctx->buf_alloc.resize(alloc_size);\n            ctx->alloc = ggml_allocr_new(ctx->buf_alloc.data, ctx->buf_alloc.size, tensor_alignment);\n#ifdef GGML_USE_METAL\n            if (ctx->ctx_metal) {\n                //ggml_allocr_set_parse_seq(ctx->alloc, ggml_metal_get_concur_list(ctx->ctx_metal), ggml_metal_if_optimized(ctx->ctx_metal));\n            }\n#endif\n#ifdef GGML_USE_CUBLAS\n            ggml_cuda_set_scratch_size(alloc_size);\n            LLAMA_LOG_INFO(\"%s: VRAM scratch buffer: %.2f MB\\n\", __func__, alloc_size / 1024.0 / 1024.0);\n\n            // calculate total VRAM usage\n            auto add_tensor = [](const ggml_tensor * t, size_t & size) {\n                if (t->backend == GGML_BACKEND_GPU || t->backend == GGML_BACKEND_GPU_SPLIT) {\n                    size += ggml_nbytes(t);\n                }\n            };\n            size_t model_vram_size = 0;\n            for (const auto & kv : model->tensors_by_name) {\n                add_tensor(kv.second, model_vram_size);\n            }\n\n            size_t kv_vram_size = 0;\n            add_tensor(ctx->kv_self.k, kv_vram_size);\n            add_tensor(ctx->kv_self.v, kv_vram_size);\n\n            size_t ctx_vram_size = alloc_size + kv_vram_size;\n            size_t total_vram_size = model_vram_size + ctx_vram_size;\n\n            LLAMA_LOG_INFO(\"%s: total VRAM used: %.2f MB (model: %.2f MB, context: %.2f MB)\\n\", __func__,\n                    (total_vram_size + model->ffn_offloaded_bytes) / 1024.0 / 1024.0,\n                    model_vram_size / 1024.0 / 1024.0,\n                    ctx_vram_size / 1024.0 / 1024.0);\n#endif\n        }\n\n#ifdef GGML_USE_METAL\n        if (model->n_gpu_layers > 0) {\n            // this allocates all Metal resources and memory buffers\n\n            void * data_ptr  = NULL;\n            size_t data_size = 0;\n\n            if (ctx->model.mapping) {\n                data_ptr  = ctx->model.mapping->addr;\n                data_size = ctx->model.mapping->size;\n            } else {\n                data_ptr  = ggml_get_mem_buffer(ctx->model.ctx);\n                data_size = ggml_get_mem_size  (ctx->model.ctx);\n            }\n\n            const size_t max_size = ggml_get_max_tensor_size(ctx->model.ctx);\n\n            LLAMA_LOG_INFO(\"%s: max tensor size = %8.2f MB\\n\", __func__, max_size/1024.0/1024.0);\n\n#define LLAMA_METAL_CHECK_BUF(result)                            \\\n            if (!(result)) {                                             \\\n                LLAMA_LOG_ERROR(\"%s: failed to add buffer\\n\", __func__); \\\n                llama_free(ctx);                                         \\\n                return NULL;                                             \\\n            }\n\n            LLAMA_METAL_CHECK_BUF(ggml_metal_add_buffer(ctx->ctx_metal, \"data\",  data_ptr, data_size, max_size));\n            LLAMA_METAL_CHECK_BUF(ggml_metal_add_buffer(ctx->ctx_metal, \"kv\",    ctx->kv_self.buf.data, ctx->kv_self.buf.size, 0));\n            LLAMA_METAL_CHECK_BUF(ggml_metal_add_buffer(ctx->ctx_metal, \"alloc\", ctx->buf_alloc.data, ctx->buf_alloc.size, 0));\n#undef LLAMA_METAL_CHECK_BUF\n        }\n#endif\n    }\n\n#ifdef GGML_USE_MPI\n    ctx->ctx_mpi = ggml_mpi_init();\n\n    if (ggml_mpi_rank(ctx->ctx_mpi) > 0) {\n        // Enter a blocking eval loop with dummy input, letting rank=0 drive the process\n        // TODO: needs fix after #3228\n        GGML_ASSERT(false && \"not implemented\");\n        //const std::vector<llama_token> tmp(ctx->model.hparams.n_ctx, llama_token_bos(ctx));\n        //while (!llama_eval(ctx, tmp.data(), tmp.size(), 0, 0)) {};\n        llama_backend_free();\n        exit(1);\n    }\n#endif\n\n    return ctx;\n}\n\nvoid llama_free(struct llama_context * ctx) {\n    delete ctx;\n}\n\nconst llama_model * llama_get_model(const struct llama_context * ctx) {\n    return &ctx->model;\n}\n\nint llama_n_ctx(const struct llama_context * ctx) {\n    return ctx->cparams.n_ctx;\n}\n\nenum llama_vocab_type llama_vocab_type(const struct llama_model * model) {\n    return model->vocab.type;\n}\n\nbool llama_use_sparse_inference(const struct llama_model * model) {\n    return model->sparse_deriv == GGML_SPARSE_INFERENCE;\n}\n\nint llama_n_vocab(const struct llama_model * model) {\n    return model->vocab.id_to_token.size();\n}\n\nint llama_n_ctx_train(const struct llama_model * model) {\n    return model->hparams.n_ctx_train;\n}\n\nint llama_n_embd(const struct llama_model * model) {\n    return model->hparams.n_embd;\n}\n\nfloat llama_rope_freq_scale_train(const struct llama_model * model) {\n    return model->hparams.rope_freq_scale_train;\n}\n\nint llama_model_desc(const struct llama_model * model, char * buf, size_t buf_size) {\n    return snprintf(buf, buf_size, \"%s %s %s\",\n            llama_model_arch_name(model->arch).c_str(),\n            llama_model_type_name(model->type),\n            llama_model_ftype_name(model->ftype).c_str());\n}\n\nuint64_t llama_model_size(const struct llama_model * model) {\n    uint64_t size = 0;\n    for (const auto & it : model->tensors_by_name) {\n        size += ggml_nbytes(it.second);\n    }\n    return size;\n}\n\nuint64_t llama_model_n_params(const struct llama_model * model) {\n    uint64_t nparams = 0;\n    for (const auto & it : model->tensors_by_name) {\n        nparams += ggml_nelements(it.second);\n    }\n    return nparams;\n}\n\nstruct ggml_tensor * llama_get_model_tensor(struct llama_model * model, const char * name) {\n    return ggml_get_tensor(model->ctx, name);\n}\n\nint llama_model_quantize(\n        const char * fname_inp,\n        const char * fname_out,\n        const llama_model_quantize_params * params) {\n    try {\n        llama_model_quantize_internal(fname_inp, fname_out, params);\n        return 0;\n    } catch (const std::exception & err) {\n        LLAMA_LOG_ERROR(\"%s: failed to quantize: %s\\n\", __func__, err.what());\n        return 1;\n    }\n}\n\nint llama_apply_lora_from_file(struct llama_context * ctx, const char * path_lora, float scale, const char * path_base_model, int n_threads) {\n    try {\n        return llama_apply_lora_from_file_internal(ctx->model, path_lora, scale, path_base_model, n_threads);\n    } catch (const std::exception & err) {\n        LLAMA_LOG_ERROR(\"%s: failed to apply lora adapter: %s\\n\", __func__, err.what());\n        return 1;\n    }\n}\n\nint llama_model_apply_lora_from_file(const struct llama_model * model, const char * path_lora, float scale, const char * path_base_model, int n_threads) {\n    try {\n        return llama_apply_lora_from_file_internal(*model, path_lora, scale, path_base_model, n_threads);\n    } catch (const std::exception & err) {\n        LLAMA_LOG_ERROR(\"%s: failed to apply lora adapter: %s\\n\", __func__, err.what());\n        return 1;\n    }\n}\n\nsize_t llama_model_offload_ffn_split(struct llama_model * model) {\n    llama_augmentation_model_loader * aug_ml = new llama_augmentation_model_loader(model);    \n    size_t offloaded_bytes = aug_ml->offload_ffn_split(model);\n    return offloaded_bytes;\n}\n\nint llama_get_kv_cache_token_count(const struct llama_context * ctx) {\n    return ctx->kv_self.head;\n}\n\nvoid llama_kv_cache_clear(struct llama_context * ctx) {\n    llama_kv_cache_clear(ctx->kv_self);\n}\n\nvoid llama_kv_cache_seq_rm(struct llama_context * ctx, llama_seq_id seq_id, llama_pos p0, llama_pos p1) {\n    llama_kv_cache_seq_rm(ctx->kv_self, seq_id, p0, p1);\n}\n\nvoid llama_kv_cache_seq_cp(struct llama_context * ctx, llama_seq_id seq_id_src, llama_seq_id seq_id_dst, llama_pos p0, llama_pos p1) {\n    if (seq_id_src == seq_id_dst) {\n        return;\n    }\n    llama_kv_cache_seq_cp(ctx->kv_self, seq_id_src, seq_id_dst, p0, p1);\n}\n\nvoid llama_kv_cache_seq_keep(struct llama_context * ctx, llama_seq_id seq_id) {\n    llama_kv_cache_seq_keep(ctx->kv_self, seq_id);\n}\n\nvoid llama_kv_cache_seq_shift(struct llama_context * ctx, llama_seq_id seq_id, llama_pos p0, llama_pos p1, llama_pos delta) {\n    llama_kv_cache_seq_shift(ctx->kv_self, seq_id, p0, p1, delta);\n}\n\n// Returns the *maximum* size of the state\nsize_t llama_get_state_size(const struct llama_context * ctx) {\n    // we don't know size of rng until we actually serialize it. so reserve more than enough memory for its serialized state.\n    // for reference, std::mt19937(1337) serializes to 6701 bytes.\n    const size_t s_rng_size        = sizeof(size_t);\n    const size_t s_rng             = LLAMA_MAX_RNG_STATE;\n    const size_t s_logits_capacity = sizeof(size_t);\n    const size_t s_logits_size     = sizeof(size_t);\n    const size_t s_logits          = ctx->logits.capacity() * sizeof(float);\n    const size_t s_embedding_size  = sizeof(size_t);\n    const size_t s_embedding       = ctx->embedding.size() * sizeof(float);\n    const size_t s_kv_size         = sizeof(size_t);\n    const size_t s_kv_ntok         = sizeof(int);\n    const size_t s_kv              = ctx->kv_self.buf.size;\n\n    const size_t s_total = (\n        + s_rng_size\n        + s_rng\n        + s_logits_capacity\n        + s_logits_size\n        + s_logits\n        + s_embedding_size\n        + s_embedding\n        + s_kv_size\n        + s_kv_ntok\n        + s_kv\n    );\n\n    return s_total;\n}\n\n// llama_context_data\nstruct llama_data_context {\n    virtual void write(const void * src, size_t size) = 0;\n    virtual size_t get_size_written() = 0;\n    virtual ~llama_data_context() = default;\n};\n\nstruct llama_data_buffer_context : llama_data_context {\n    uint8_t * ptr;\n    size_t size_written = 0;\n\n    llama_data_buffer_context(uint8_t * p) : ptr(p) {}\n\n    void write(const void * src, size_t size) override {\n        memcpy(ptr, src, size);\n        ptr += size;\n        size_written += size;\n    }\n\n    size_t get_size_written() override {\n        return size_written;\n    }\n};\n\nstruct llama_data_file_context : llama_data_context {\n    llama_file * file;\n    size_t size_written = 0;\n\n    llama_data_file_context(llama_file * f) : file(f) {}\n\n    void write(const void * src, size_t size) override {\n        file->write_raw(src, size);\n        size_written += size;\n    }\n\n    size_t get_size_written() override {\n        return size_written;\n    }\n};\n\n/** copy state data into either a buffer or file depending on the passed in context\n *\n * file context:\n * llama_file file(\"/path\", \"wb\");\n * llama_data_file_context data_ctx(&file);\n * llama_copy_state_data(ctx, &data_ctx);\n *\n * buffer context:\n * std::vector<uint8_t> buf(max_size, 0);\n * llama_data_buffer_context data_ctx(&buf.data());\n * llama_copy_state_data(ctx, &data_ctx);\n *\n*/\nstatic void llama_copy_state_data_internal(struct llama_context * ctx, llama_data_context * data_ctx) {\n    // copy rng\n    {\n        std::stringstream rng_ss;\n        rng_ss << ctx->rng;\n\n        const size_t rng_size = rng_ss.str().size();\n        char rng_buf[LLAMA_MAX_RNG_STATE];\n\n        memset(&rng_buf[0], 0, LLAMA_MAX_RNG_STATE);\n        memcpy(&rng_buf[0], rng_ss.str().data(), rng_ss.str().size());\n\n        data_ctx->write(&rng_size,   sizeof(rng_size));\n        data_ctx->write(&rng_buf[0], LLAMA_MAX_RNG_STATE);\n    }\n\n    // copy logits\n    {\n        const size_t logits_cap  = ctx->logits.capacity();\n        const size_t logits_size = ctx->logits.size();\n\n        data_ctx->write(&logits_cap,  sizeof(logits_cap));\n        data_ctx->write(&logits_size, sizeof(logits_size));\n\n        if (logits_size) {\n            data_ctx->write(ctx->logits.data(), logits_size * sizeof(float));\n        }\n\n        // If there is a gap between the size and the capacity, write padding\n        size_t padding_size = (logits_cap - logits_size) * sizeof(float);\n        if (padding_size > 0) {\n            std::vector<uint8_t> padding(padding_size, 0); // Create a buffer filled with zeros\n            data_ctx->write(padding.data(), padding_size);\n        }\n    }\n\n    // copy embeddings\n    {\n        const size_t embedding_size = ctx->embedding.size();\n\n        data_ctx->write(&embedding_size, sizeof(embedding_size));\n\n        if (embedding_size) {\n            data_ctx->write(ctx->embedding.data(), embedding_size * sizeof(float));\n        }\n    }\n\n    // copy kv cache\n    {\n        const auto & kv_self = ctx->kv_self;\n        const auto & hparams = ctx->model.hparams;\n        const auto & cparams = ctx->cparams;\n\n        const auto   n_layer = hparams.n_layer;\n        const auto   n_embd  = hparams.n_embd_gqa();\n        const auto   n_ctx   = cparams.n_ctx;\n\n        const size_t   kv_buf_size = kv_self.buf.size;\n        const uint32_t kv_head     = kv_self.head;\n        const uint32_t kv_size     = kv_self.size;\n\n        data_ctx->write(&kv_buf_size, sizeof(kv_buf_size));\n        data_ctx->write(&kv_head,     sizeof(kv_head));\n        data_ctx->write(&kv_size,     sizeof(kv_size));\n\n        if (kv_buf_size) {\n            const size_t elt_size = ggml_element_size(kv_self.k);\n\n            ggml_context * cpy_ctx = ggml_init({ 6*ggml_tensor_overhead() + ggml_graph_overhead(), NULL, /* no_alloc */ true });\n            ggml_cgraph * gf = ggml_new_graph(cpy_ctx);\n\n            ggml_tensor * kout3d = ggml_new_tensor_3d(cpy_ctx, kv_self.k->type, n_embd, kv_head, n_layer);\n            std::vector<uint8_t> kout3d_data(ggml_nbytes(kout3d), 0);\n            kout3d->data = kout3d_data.data();\n\n            ggml_tensor * vout3d = ggml_new_tensor_3d(cpy_ctx, kv_self.v->type, kv_head, n_embd, n_layer);\n            std::vector<uint8_t> vout3d_data(ggml_nbytes(vout3d), 0);\n            vout3d->data = vout3d_data.data();\n\n            ggml_tensor * k3d = ggml_view_3d(cpy_ctx, kv_self.k,\n                n_embd, kv_head, n_layer,\n                elt_size*n_embd, elt_size*n_embd*n_ctx, 0);\n\n            ggml_tensor * v3d = ggml_view_3d(cpy_ctx, kv_self.v,\n                kv_head, n_embd, n_layer,\n                elt_size*n_ctx, elt_size*n_ctx*n_embd, 0);\n\n            ggml_build_forward_expand(gf, ggml_cpy(cpy_ctx, k3d, kout3d));\n            ggml_build_forward_expand(gf, ggml_cpy(cpy_ctx, v3d, vout3d));\n            ggml_graph_compute_helper(ctx->work_buffer, gf, /*n_threads*/ 1);\n\n            ggml_free(cpy_ctx);\n\n            // our data is now in the kout3d_data and vout3d_data buffers\n            // write them to file\n            data_ctx->write(kout3d_data.data(), kout3d_data.size());\n            data_ctx->write(vout3d_data.data(), vout3d_data.size());\n        }\n\n        for (uint32_t i = 0; i < kv_size; ++i) {\n            const auto & cell = kv_self.cells[i];\n\n            const llama_pos pos         = cell.pos;\n            const size_t    seq_id_size = cell.seq_id.size();\n\n            data_ctx->write(&pos,         sizeof(pos));\n            data_ctx->write(&seq_id_size, sizeof(seq_id_size));\n\n            for (auto seq_id : cell.seq_id) {\n                data_ctx->write(&seq_id, sizeof(seq_id));\n            }\n        }\n    }\n}\n\nsize_t llama_copy_state_data(struct llama_context * ctx, uint8_t * dst) {\n    llama_data_buffer_context data_ctx(dst);\n    llama_copy_state_data_internal(ctx, &data_ctx);\n\n    return data_ctx.get_size_written();\n}\n\n// Sets the state reading from the specified source address\nsize_t llama_set_state_data(struct llama_context * ctx, uint8_t * src) {\n    uint8_t * inp = src;\n\n    // set rng\n    {\n        size_t rng_size;\n        char   rng_buf[LLAMA_MAX_RNG_STATE];\n\n        memcpy(&rng_size,   inp, sizeof(rng_size));    inp += sizeof(rng_size);\n        memcpy(&rng_buf[0], inp, LLAMA_MAX_RNG_STATE); inp += LLAMA_MAX_RNG_STATE;\n\n        std::stringstream rng_ss;\n        rng_ss.str(std::string(&rng_buf[0], rng_size));\n        rng_ss >> ctx->rng;\n\n        GGML_ASSERT(!rng_ss.fail());\n    }\n\n    // set logits\n    {\n        size_t logits_cap;\n        size_t logits_size;\n\n        memcpy(&logits_cap,  inp, sizeof(logits_cap));  inp += sizeof(logits_cap);\n        memcpy(&logits_size, inp, sizeof(logits_size)); inp += sizeof(logits_size);\n\n        GGML_ASSERT(ctx->logits.capacity() == logits_cap);\n\n        if (logits_size) {\n            ctx->logits.resize(logits_size);\n            memcpy(ctx->logits.data(), inp, logits_size * sizeof(float));\n        }\n\n        inp += logits_cap * sizeof(float);\n    }\n\n    // set embeddings\n    {\n        size_t embedding_size;\n\n        memcpy(&embedding_size, inp, sizeof(embedding_size)); inp += sizeof(embedding_size);\n\n        GGML_ASSERT(ctx->embedding.capacity() == embedding_size);\n\n        if (embedding_size) {\n            memcpy(ctx->embedding.data(), inp, embedding_size * sizeof(float));\n            inp += embedding_size * sizeof(float);\n        }\n    }\n\n    // set kv cache\n    {\n        const auto & kv_self = ctx->kv_self;\n        const auto & hparams = ctx->model.hparams;\n        const auto & cparams = ctx->cparams;\n\n        const int    n_layer = hparams.n_layer;\n        const int    n_embd  = hparams.n_embd_gqa();\n        const int    n_ctx   = cparams.n_ctx;\n\n        size_t   kv_buf_size;\n        uint32_t kv_head;\n        uint32_t kv_size;\n\n        memcpy(&kv_buf_size, inp, sizeof(kv_buf_size)); inp += sizeof(kv_buf_size);\n        memcpy(&kv_head,     inp, sizeof(kv_head));     inp += sizeof(kv_head);\n        memcpy(&kv_size,     inp, sizeof(kv_size));     inp += sizeof(kv_size);\n\n        if (kv_buf_size) {\n            GGML_ASSERT(kv_self.buf.size == kv_buf_size);\n\n            const size_t elt_size = ggml_element_size(kv_self.k);\n\n            ggml_context * cpy_ctx = ggml_init({ 6*ggml_tensor_overhead() + ggml_graph_overhead(), NULL, /* no_alloc */ true });\n            ggml_cgraph * gf = ggml_new_graph(cpy_ctx);\n\n            ggml_tensor * kin3d = ggml_new_tensor_3d(cpy_ctx, kv_self.k->type, n_embd, kv_head, n_layer);\n            kin3d->data = (void *) inp;\n            inp += ggml_nbytes(kin3d);\n\n            ggml_tensor * vin3d = ggml_new_tensor_3d(cpy_ctx, kv_self.v->type, kv_head, n_embd, n_layer);\n            vin3d->data = (void *) inp;\n            inp += ggml_nbytes(vin3d);\n\n            ggml_tensor * k3d = ggml_view_3d(cpy_ctx, kv_self.k,\n                n_embd, kv_head, n_layer,\n                elt_size*n_embd, elt_size*n_embd*n_ctx, 0);\n\n            ggml_tensor * v3d = ggml_view_3d(cpy_ctx, kv_self.v,\n                kv_head, n_embd, n_layer,\n                elt_size*n_ctx, elt_size*n_ctx*n_embd, 0);\n\n            ggml_build_forward_expand(gf, ggml_cpy(cpy_ctx, kin3d, k3d));\n            ggml_build_forward_expand(gf, ggml_cpy(cpy_ctx, vin3d, v3d));\n            ggml_graph_compute_helper(ctx->work_buffer, gf, /*n_threads*/ 1);\n\n            ggml_free(cpy_ctx);\n        }\n\n        ctx->kv_self.head = kv_head;\n        ctx->kv_self.size = kv_size;\n\n        ctx->kv_self.cells.resize(kv_size);\n\n        for (uint32_t i = 0; i < kv_size; ++i) {\n            llama_pos pos;\n            size_t    seq_id_size;\n\n            memcpy(&pos,         inp, sizeof(pos));         inp += sizeof(pos);\n            memcpy(&seq_id_size, inp, sizeof(seq_id_size)); inp += sizeof(seq_id_size);\n\n            ctx->kv_self.cells[i].pos = pos;\n\n            llama_seq_id seq_id;\n\n            for (size_t j = 0; j < seq_id_size; ++j) {\n                memcpy(&seq_id, inp, sizeof(seq_id)); inp += sizeof(seq_id);\n                ctx->kv_self.cells[i].seq_id.insert(seq_id);\n            }\n        }\n    }\n\n    const size_t nread    = inp - src;\n    const size_t max_size = llama_get_state_size(ctx);\n\n    GGML_ASSERT(nread <= max_size);\n\n    return nread;\n}\n\nstatic bool llama_load_session_file_internal(struct llama_context * ctx, const char * path_session, llama_token * tokens_out, size_t n_token_capacity, size_t * n_token_count_out) {\n    llama_file file(path_session, \"rb\");\n\n    // sanity checks\n    {\n        const uint32_t magic   = file.read_u32();\n        const uint32_t version = file.read_u32();\n\n        if (magic != LLAMA_SESSION_MAGIC || version != LLAMA_SESSION_VERSION) {\n            LLAMA_LOG_ERROR(\"%s : unknown (magic, version) for session file: %08x, %08x\\n\", __func__, magic, version);\n            return false;\n        }\n\n        llama_hparams session_hparams;\n        file.read_raw(&session_hparams, sizeof(llama_hparams));\n\n        if (session_hparams != ctx->model.hparams) {\n            LLAMA_LOG_INFO(\"%s : model hparams didn't match from session file!\\n\", __func__);\n            return false;\n        }\n    }\n\n    // load the prompt\n    {\n        const uint32_t n_token_count = file.read_u32();\n\n        if (n_token_count > n_token_capacity) {\n            LLAMA_LOG_ERROR(\"%s : token count in session file exceeded capacity! %u > %zu\\n\", __func__, n_token_count, n_token_capacity);\n            return false;\n        }\n\n        file.read_raw(tokens_out, sizeof(llama_token) * n_token_count);\n        *n_token_count_out = n_token_count;\n    }\n\n    // restore the context state\n    {\n        const size_t n_state_size_cur = file.size - file.tell();\n        const size_t n_state_size_max = llama_get_state_size(ctx);\n\n        if (n_state_size_cur > n_state_size_max) {\n            LLAMA_LOG_ERROR(\"%s : the state size in session file is too big! max %zu, got %zu\\n\", __func__, n_state_size_max, n_state_size_cur);\n            return false;\n        }\n\n        std::vector<uint8_t> state_data(n_state_size_max);\n        file.read_raw(state_data.data(), n_state_size_cur);\n\n        llama_set_state_data(ctx, state_data.data());\n    }\n\n    return true;\n}\n\nbool llama_load_session_file(struct llama_context * ctx, const char * path_session, llama_token * tokens_out, size_t n_token_capacity, size_t * n_token_count_out) {\n    try {\n        return llama_load_session_file_internal(ctx, path_session, tokens_out, n_token_capacity, n_token_count_out);\n    } catch (const std::exception & err) {\n        LLAMA_LOG_ERROR(\"error loading session file: %s\\n\", err.what());\n        return false;\n    }\n}\n\nbool llama_save_session_file(struct llama_context * ctx, const char * path_session, const llama_token * tokens, size_t n_token_count) {\n    llama_file file(path_session, \"wb\");\n\n    file.write_u32(LLAMA_SESSION_MAGIC);\n    file.write_u32(LLAMA_SESSION_VERSION);\n\n    file.write_raw(&ctx->model.hparams, sizeof(llama_hparams));\n\n    // save the prompt\n    file.write_u32((uint32_t) n_token_count);\n    file.write_raw(tokens, sizeof(llama_token) * n_token_count);\n\n    // save the context state using stream saving\n    llama_data_file_context data_ctx(&file);\n    llama_copy_state_data_internal(ctx, &data_ctx);\n\n    return true;\n}\n\nint llama_eval(\n        struct llama_context * ctx,\n                 llama_token * tokens,\n                     int32_t   n_tokens,\n                         int   n_past) {\n    llama_kv_cache_seq_rm(ctx->kv_self, -1, n_past, -1);\n\n    const int ret = llama_decode_internal(*ctx, llama_batch_get_one(tokens, n_tokens, n_past, 0));\n    if (ret < 0) {\n        LLAMA_LOG_ERROR(\"%s: failed to decode, ret = %d\\n\", __func__, ret);\n    }\n\n    return ret;\n}\n\nint llama_eval_embd(\n            struct llama_context * ctx,\n                           float * embd,\n                         int32_t   n_tokens,\n                             int   n_past) {\n    llama_kv_cache_seq_rm(ctx->kv_self, -1, n_past, -1);\n\n    llama_batch batch = { n_tokens, nullptr, embd, nullptr, nullptr, nullptr, nullptr, n_past, 1, 0, };\n\n    const int ret = llama_decode_internal(*ctx, batch);\n    if (ret < 0) {\n        LLAMA_LOG_ERROR(\"%s: failed to decode, ret = %d\\n\", __func__, ret);\n    }\n\n    return ret;\n}\n\nvoid llama_set_n_threads(struct llama_context * ctx, uint32_t n_threads, uint32_t n_threads_batch) {\n    ctx->cparams.n_threads       = n_threads;\n    ctx->cparams.n_threads_batch = n_threads_batch;\n}\n\nstruct llama_batch llama_batch_get_one(\n             llama_token * tokens,\n                 int32_t   n_tokens,\n               llama_pos   pos_0,\n            llama_seq_id   seq_id) {\n    return {\n        /*n_tokens       =*/ n_tokens,\n        /*tokens         =*/ tokens,\n        /*embd           =*/ nullptr,\n        /*pos            =*/ nullptr,\n        /*n_seq_id       =*/ nullptr,\n        /*seq_id         =*/ nullptr,\n        /*logits         =*/ nullptr,\n        /*all_pos_0      =*/ pos_0,\n        /*all_pos_1      =*/ 1,\n        /*all_seq_id     =*/ seq_id,\n    };\n}\n\nstruct llama_batch llama_batch_init(int32_t n_tokens, int32_t embd, int32_t n_seq_max) {\n    llama_batch batch = { 0, nullptr, nullptr, nullptr, nullptr, nullptr, nullptr, 0, 0, 0, };\n\n    if (embd) {\n        batch.embd = (float *) malloc(sizeof(float) * n_tokens * embd);\n    } else {\n        batch.token = (llama_token *) malloc(sizeof(llama_token) * n_tokens);\n    }\n\n    batch.pos      = (llama_pos *)     malloc(sizeof(llama_pos)      * n_tokens);\n    batch.n_seq_id = (int32_t *)       malloc(sizeof(int32_t)        * n_tokens);\n    batch.seq_id   = (llama_seq_id **) malloc(sizeof(llama_seq_id *) * n_tokens);\n    for (int i = 0; i < n_tokens; ++i) {\n        batch.seq_id[i] = (llama_seq_id *) malloc(sizeof(llama_seq_id) * n_seq_max);\n    }\n    batch.logits   = (int8_t *)        malloc(sizeof(int8_t)         * n_tokens);\n\n    return batch;\n}\n\nvoid llama_batch_free(struct llama_batch batch) {\n    if (batch.token)    free(batch.token);\n    if (batch.embd)     free(batch.embd);\n    if (batch.pos)      free(batch.pos);\n    if (batch.n_seq_id) free(batch.n_seq_id);\n    if (batch.seq_id) {\n        for (int i = 0; i < batch.n_tokens; ++i) {\n            free(batch.seq_id[i]);\n        }\n        free(batch.seq_id);\n    }\n    if (batch.logits)   free(batch.logits);\n}\n\nint llama_decode(\n        struct llama_context * ctx,\n          struct llama_batch   batch) {\n    const int ret = llama_decode_internal(*ctx, batch);\n    if (ret < 0) {\n        LLAMA_LOG_ERROR(\"%s: failed to decode, ret = %d\\n\", __func__, ret);\n    }\n\n    return ret;\n}\n\nfloat * llama_get_logits(struct llama_context * ctx) {\n    return ctx->logits.data();\n}\n\nfloat * llama_get_logits_ith(struct llama_context * ctx, int32_t i) {\n    return ctx->logits.data() + i*ctx->model.hparams.n_vocab;\n}\n\nfloat * llama_get_embeddings(struct llama_context * ctx) {\n    return ctx->embedding.data();\n}\n\nconst char * llama_token_get_text(const struct llama_model * model, llama_token token) {\n    return model->vocab.id_to_token[token].text.c_str();\n}\n\nfloat llama_token_get_score(const struct llama_model * model, llama_token token) {\n    return model->vocab.id_to_token[token].score;\n}\n\nllama_token_type llama_token_get_type(const struct llama_model * model, llama_token token) {\n    return model->vocab.id_to_token[token].type;\n}\n\nllama_token llama_token_bos(const struct llama_model * model) {\n    return model->vocab.special_bos_id;\n}\n\nllama_token llama_token_eos(const struct llama_model * model) {\n    return model->vocab.special_eos_id;\n}\n\nllama_token llama_token_nl(const struct llama_model * model) {\n    return model->vocab.linefeed_id;\n}\n\nllama_token llama_token_prefix(const struct llama_model * model) {\n    return model->vocab.special_prefix_id;\n}\n\nllama_token llama_token_middle(const struct llama_model * model) {\n    return model->vocab.special_middle_id;\n}\n\nllama_token llama_token_suffix(const struct llama_model * model) {\n    return model->vocab.special_suffix_id;\n}\n\nllama_token llama_token_eot(const struct llama_model * model) {\n    return model->vocab.special_eot_id;\n}\n\nint llama_tokenize(\n    const struct llama_model * model,\n                  const char * text,\n                         int   text_len,\n                 llama_token * tokens,\n                         int   n_max_tokens,\n                        bool   add_bos,\n                        bool   special) {\n    auto res = llama_tokenize_internal(model->vocab, std::string(text, text_len), add_bos, special);\n\n    if (n_max_tokens < (int) res.size()) {\n        // LLAMA_LOG_ERROR(\"%s: too many tokens\\n\", __func__);\n        return -((int) res.size());\n    }\n\n    for (size_t i = 0; i < res.size(); i++) {\n        tokens[i] = res[i];\n    }\n\n    return res.size();\n}\n\nstatic std::string llama_decode_text(const std::string & text) {\n    std::string decoded_text;\n    auto unicode_sequences = codepoints_from_utf8(text);\n    for (auto& unicode_sequence : unicode_sequences) {\n        decoded_text += unicode_to_bytes_bpe(codepoint_to_utf8(unicode_sequence));\n    }\n\n    return decoded_text;\n}\n\n// does not write null-terminator to buf\nint llama_token_to_piece(const struct llama_model * model, llama_token token, char * buf, int length) {\n    if (0 <= token && token < llama_n_vocab(model)) {\n        switch (llama_vocab_get_type(model->vocab)) {\n        case LLAMA_VOCAB_TYPE_SPM: {\n            if (llama_is_normal_token(model->vocab, token)) {\n                std::string result = model->vocab.id_to_token[token].text;\n                llama_unescape_whitespace(result);\n                if (length < (int) result.length()) {\n                    return -result.length();\n                }\n                memcpy(buf, result.c_str(), result.length());\n                return result.length();\n            } else if (llama_is_unknown_token(model->vocab, token)) { // NOLINT\n                if (length < 3) {\n                    return -3;\n                }\n                memcpy(buf, \"\\xe2\\x96\\x85\", 3);\n                return 3;\n            } else if (llama_is_control_token(model->vocab, token)) {\n                ;\n            } else if (llama_is_byte_token(model->vocab, token)) {\n                if (length < 1) {\n                    return -1;\n                }\n                buf[0] = llama_token_to_byte(model->vocab, token);\n                return 1;\n            } else {\n                // TODO: for now we accept all unsupported token types,\n                // suppressing them like CONTROL tokens.\n                // GGML_ASSERT(false);\n            }\n            break;\n        }\n        case LLAMA_VOCAB_TYPE_BPE: {\n            if (llama_is_normal_token(model->vocab, token)) {\n                std::string result = model->vocab.id_to_token[token].text;\n                result = llama_decode_text(result);\n                if (length < (int) result.length()) {\n                    return -result.length();\n                }\n                memcpy(buf, result.c_str(), result.length());\n                return result.length();\n            } else if (llama_is_control_token(model->vocab, token)) {\n                ;\n            } else {\n                // TODO: for now we accept all unsupported token types,\n                // suppressing them like CONTROL tokens.\n                // GGML_ASSERT(false);\n            }\n            break;\n        }\n        default:\n            GGML_ASSERT(false);\n        }\n    }\n    return 0;\n}\n\nstruct llama_timings llama_get_timings(struct llama_context * ctx) {\n    struct llama_timings result = {\n        /*.t_start_ms  =*/ 1e-3 * ctx->t_start_us,\n        /*.t_end_ms    =*/ 1.00 * ggml_time_ms(),\n        /*.t_load_ms   =*/ 1e-3 * ctx->t_load_us,\n        /*.t_sample_ms =*/ 1e-3 * ctx->t_sample_us,\n        /*.t_p_eval_ms =*/ 1e-3 * ctx->t_p_eval_us,\n        /*.t_eval_ms   =*/ 1e-3 * ctx->t_eval_us,\n\n        /*.n_sample =*/ std::max(1, ctx->n_sample),\n        /*.n_p_eval =*/ std::max(1, ctx->n_p_eval),\n        /*.n_eval   =*/ std::max(1, ctx->n_eval),\n    };\n\n    return result;\n}\n\nvoid llama_print_timings(struct llama_context * ctx) {\n    const llama_timings timings = llama_get_timings(ctx);\n\n    LLAMA_LOG_INFO(\"\\n\");\n    LLAMA_LOG_INFO(\"%s:        load time = %10.2f ms\\n\", __func__, timings.t_load_ms);\n    LLAMA_LOG_INFO(\"%s:      sample time = %10.2f ms / %5d runs   (%8.2f ms per token, %8.2f tokens per second)\\n\",\n            __func__, timings.t_sample_ms, timings.n_sample, timings.t_sample_ms / timings.n_sample, 1e3 / timings.t_sample_ms * timings.n_sample);\n    LLAMA_LOG_INFO(\"%s: prompt eval time = %10.2f ms / %5d tokens (%8.2f ms per token, %8.2f tokens per second)\\n\",\n            __func__, timings.t_p_eval_ms, timings.n_p_eval, timings.t_p_eval_ms / timings.n_p_eval, 1e3 / timings.t_p_eval_ms * timings.n_p_eval);\n    LLAMA_LOG_INFO(\"%s:        eval time = %10.2f ms / %5d runs   (%8.2f ms per token, %8.2f tokens per second)\\n\",\n            __func__, timings.t_eval_ms, timings.n_eval, timings.t_eval_ms / timings.n_eval, 1e3 / timings.t_eval_ms * timings.n_eval);\n    LLAMA_LOG_INFO(\"%s:       total time = %10.2f ms\\n\", __func__, (timings.t_end_ms - timings.t_start_ms));\n}\n\nvoid llama_reset_timings(struct llama_context * ctx) {\n    ctx->t_start_us = ggml_time_us();\n    ctx->t_sample_us = ctx->n_sample = 0;\n    ctx->t_eval_us   = ctx->n_eval   = 0;\n    ctx->t_p_eval_us = ctx->n_p_eval = 0;\n}\n\nconst char * llama_print_system_info(void) {\n    static std::string s;\n\n    s  = \"\";\n    s += \"AVX = \"         + std::to_string(ggml_cpu_has_avx())         + \" | \";\n    s += \"AVX2 = \"        + std::to_string(ggml_cpu_has_avx2())        + \" | \";\n    s += \"AVX512 = \"      + std::to_string(ggml_cpu_has_avx512())      + \" | \";\n    s += \"AVX512_VBMI = \" + std::to_string(ggml_cpu_has_avx512_vbmi()) + \" | \";\n    s += \"AVX512_VNNI = \" + std::to_string(ggml_cpu_has_avx512_vnni()) + \" | \";\n    s += \"FMA = \"         + std::to_string(ggml_cpu_has_fma())         + \" | \";\n    s += \"NEON = \"        + std::to_string(ggml_cpu_has_neon())        + \" | \";\n    s += \"ARM_FMA = \"     + std::to_string(ggml_cpu_has_arm_fma())     + \" | \";\n    s += \"F16C = \"        + std::to_string(ggml_cpu_has_f16c())        + \" | \";\n    s += \"FP16_VA = \"     + std::to_string(ggml_cpu_has_fp16_va())     + \" | \";\n    s += \"WASM_SIMD = \"   + std::to_string(ggml_cpu_has_wasm_simd())   + \" | \";\n    s += \"BLAS = \"        + std::to_string(ggml_cpu_has_blas())        + \" | \";\n    s += \"SSE3 = \"        + std::to_string(ggml_cpu_has_sse3())        + \" | \";\n    s += \"SSSE3 = \"       + std::to_string(ggml_cpu_has_ssse3())       + \" | \";\n    s += \"VSX = \"         + std::to_string(ggml_cpu_has_vsx())         + \" | \";\n\n    return s.c_str();\n}\n\nvoid llama_dump_timing_info_yaml(FILE * stream, const llama_context * ctx) {\n    fprintf(stream, \"\\n\");\n    fprintf(stream, \"###########\\n\");\n    fprintf(stream, \"# Timings #\\n\");\n    fprintf(stream, \"###########\\n\");\n    fprintf(stream, \"\\n\");\n\n    fprintf(stream, \"mst_eval: %.2f  # ms / token during generation\\n\",\n            1.0e-3 * ctx->t_eval_us / ctx->n_eval);\n    fprintf(stream, \"mst_p_eval: %.2f  # ms / token during prompt processing\\n\",\n            1.0e-3 * ctx->t_p_eval_us / ctx->n_p_eval);\n    fprintf(stream, \"mst_sample: %.2f  # ms / token during sampling\\n\",\n            1.0e-3 * ctx->t_sample_us / ctx->n_sample);\n    fprintf(stream, \"n_eval: %d  # number of tokens generated (excluding the first one)\\n\", ctx->n_eval);\n    fprintf(stream, \"n_p_eval: %d  # number of tokens processed in batches at the beginning\\n\", ctx->n_p_eval);\n    fprintf(stream, \"n_sample: %d  # number of sampled tokens\\n\", ctx->n_sample);\n    fprintf(stream, \"t_eval_us: %\" PRId64 \"  # total microseconds spent generating tokens\\n\", ctx->t_eval_us);\n    fprintf(stream, \"t_load_us: %\" PRId64 \"  # total microseconds spent loading the model\\n\", ctx->t_load_us);\n    fprintf(stream, \"t_p_eval_us: %\" PRId64 \"  # total microseconds spent prompt processing\\n\", ctx->t_p_eval_us);\n    fprintf(stream, \"t_sample_us: %\" PRId64 \"  # total microseconds spent sampling\\n\", ctx->t_sample_us);\n    fprintf(stream, \"ts_eval: %.2f  # tokens / second during generation\\n\",\n            1.0e6 * ctx->n_eval / ctx->t_eval_us);\n    fprintf(stream, \"ts_p_eval: %.2f  # tokens / second during prompt processing\\n\",\n            1.0e6 * ctx->n_p_eval / ctx->t_p_eval_us);\n    fprintf(stream, \"ts_sample: %.2f  # tokens / second during sampling\\n\",\n            1.0e6 * ctx->n_sample / ctx->t_sample_us);\n}\n\n// For internal test use\nconst std::vector<std::pair<std::string, struct ggml_tensor *>> & llama_internal_get_tensor_map(\n    struct llama_context * ctx\n) {\n    return ctx->model.tensors_by_name;\n}\n\nvoid llama_log_set(ggml_log_callback log_callback, void * user_data) {\n    g_state.log_callback = log_callback ? log_callback : llama_log_callback_default;\n    g_state.log_callback_user_data = user_data;\n}\n\nstatic void llama_log_internal_v(ggml_log_level level, const char * format, va_list args) {\n    va_list args_copy;\n    va_copy(args_copy, args);\n    char buffer[128];\n    int len = vsnprintf(buffer, 128, format, args);\n    if (len < 128) {\n        g_state.log_callback(level, buffer, g_state.log_callback_user_data);\n    } else {\n        char* buffer2 = new char[len+1];\n        vsnprintf(buffer2, len+1, format, args_copy);\n        buffer2[len] = 0;\n        g_state.log_callback(level, buffer2, g_state.log_callback_user_data);\n        delete[] buffer2;\n    }\n    va_end(args_copy);\n}\n\nstatic void llama_log_internal(ggml_log_level level, const char * format, ...) {\n    va_list args;\n    va_start(args, format);\n    llama_log_internal_v(level, format, args);\n    va_end(args);\n}\n\nstatic void llama_log_callback_default(ggml_log_level level, const char * text, void * user_data) {\n    (void) level;\n    (void) user_data;\n    fputs(text, stderr);\n    fflush(stderr);\n}\n"
        },
        {
          "name": "llama.h",
          "type": "blob",
          "size": 34.560546875,
          "content": "#ifndef LLAMA_H\n#define LLAMA_H\n\n#include \"ggml.h\"\n#ifdef GGML_USE_CUBLAS\n#include \"ggml-cuda.h\"\n#define LLAMA_MAX_DEVICES GGML_CUDA_MAX_DEVICES\n#else\n#define LLAMA_MAX_DEVICES 1\n#endif // GGML_USE_CUBLAS\n#include <stddef.h>\n#include <stdint.h>\n#include <stdio.h>\n#include <stdbool.h>\n\n#ifdef LLAMA_SHARED\n#    if defined(_WIN32) && !defined(__MINGW32__)\n#        ifdef LLAMA_BUILD\n#            define LLAMA_API __declspec(dllexport)\n#        else\n#            define LLAMA_API __declspec(dllimport)\n#        endif\n#    else\n#        define LLAMA_API __attribute__ ((visibility (\"default\")))\n#    endif\n#else\n#    define LLAMA_API\n#endif\n\n#ifdef __GNUC__\n#    define DEPRECATED(func, hint) func __attribute__((deprecated(hint)))\n#elif defined(_MSC_VER)\n#    define DEPRECATED(func, hint) __declspec(deprecated(hint)) func\n#else\n#    define DEPRECATED(func, hint) func\n#endif\n\n#define LLAMA_DEFAULT_SEED 0xFFFFFFFF\n\n#define LLAMA_MAX_RNG_STATE (64*1024)\n\n#define LLAMA_FILE_MAGIC_GGSN 0x6767736eu // 'ggsn'\n\n#define LLAMA_SESSION_MAGIC   LLAMA_FILE_MAGIC_GGSN\n#define LLAMA_SESSION_VERSION 2\n\n#if defined(GGML_USE_CUBLAS) || defined(GGML_USE_CLBLAST) || defined(GGML_USE_METAL)\n// Defined when llama.cpp is compiled with support for offloading model layers to GPU.\n#define LLAMA_SUPPORTS_GPU_OFFLOAD\n#endif\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n    //\n    // C interface\n    //\n    // TODO: show sample usage\n    //\n\n    struct llama_model;\n    struct llama_context;\n\n    typedef int32_t llama_pos;\n    typedef int32_t llama_token;\n    typedef int32_t llama_seq_id;\n\n    enum llama_vocab_type {\n        LLAMA_VOCAB_TYPE_SPM = 0, // SentencePiece\n        LLAMA_VOCAB_TYPE_BPE = 1, // Byte Pair Encoding\n    };\n\n    enum llama_token_type {\n        LLAMA_TOKEN_TYPE_UNDEFINED    = 0,\n        LLAMA_TOKEN_TYPE_NORMAL       = 1,\n        LLAMA_TOKEN_TYPE_UNKNOWN      = 2,\n        LLAMA_TOKEN_TYPE_CONTROL      = 3,\n        LLAMA_TOKEN_TYPE_USER_DEFINED = 4,\n        LLAMA_TOKEN_TYPE_UNUSED       = 5,\n        LLAMA_TOKEN_TYPE_BYTE         = 6,\n    };\n\n    // model file types\n    enum llama_ftype {\n        LLAMA_FTYPE_ALL_F32              = 0,\n        LLAMA_FTYPE_MOSTLY_F16           = 1,  // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q4_0          = 2,  // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q4_1          = 3,  // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q4_1_SOME_F16 = 4,  // tok_embeddings.weight and output.weight are F16\n        // LLAMA_FTYPE_MOSTLY_Q4_2       = 5,  // support has been removed\n        // LLAMA_FTYPE_MOSTLY_Q4_3       = 6,  // support has been removed\n        LLAMA_FTYPE_MOSTLY_Q8_0          = 7,  // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q5_0          = 8,  // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q5_1          = 9,  // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q2_K          = 10, // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q3_K_S        = 11, // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q3_K_M        = 12, // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q3_K_L        = 13, // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q4_K_S        = 14, // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q4_K_M        = 15, // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q5_K_S        = 16, // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q5_K_M        = 17, // except 1d tensors\n        LLAMA_FTYPE_MOSTLY_Q6_K          = 18, // except 1d tensors\n\n        LLAMA_FTYPE_GUESSED = 1024, // not specified in the model file\n    };\n\n    enum llama_rope_scaling_type {\n        LLAMA_ROPE_SCALING_UNSPECIFIED = -1,\n        LLAMA_ROPE_SCALING_NONE        = 0,\n        LLAMA_ROPE_SCALING_LINEAR      = 1,\n        LLAMA_ROPE_SCALING_YARN        = 2,\n        LLAMA_ROPE_SCALING_MAX_VALUE   = LLAMA_ROPE_SCALING_YARN,\n    };\n\n    typedef struct llama_token_data {\n        llama_token id; // token id\n        float logit;    // log-odds of the token\n        float p;        // probability of the token\n    } llama_token_data;\n\n    typedef struct llama_token_data_array {\n        llama_token_data * data;\n        size_t size;\n        bool sorted;\n    } llama_token_data_array;\n\n    typedef void (*llama_progress_callback)(float progress, void *ctx);\n\n    // Input data for llama_decode\n    // A llama_batch object can contain input about one or many sequences\n    // The provided arrays (i.e. token, embd, pos, etc.) must have size of n_tokens\n    //\n    // - token  : the token ids of the input (used when embd is NULL)\n    // - embd   : token embeddings (i.e. float vector of size n_embd) (used when token is NULL)\n    // - pos    : the positions of the respective token in the sequence\n    // - seq_id : the sequence to which the respective token belongs\n    // - logits : if zero, the logits for the respective token will not be output\n    //\n    typedef struct llama_batch {\n        int32_t n_tokens;\n\n        llama_token  *  token;\n        float        *  embd;\n        llama_pos    *  pos;\n        int32_t      *  n_seq_id;\n        llama_seq_id ** seq_id;\n        int8_t       *  logits;\n\n        // NOTE: helpers for smooth API transition - can be deprecated in the future\n        //       for future-proof code, use the above fields instead and ignore everything below\n        //\n        // pos[i] = all_pos_0 + i*all_pos_1\n        //\n        llama_pos    all_pos_0;  // used if pos == NULL\n        llama_pos    all_pos_1;  // used if pos == NULL\n        llama_seq_id all_seq_id; // used if seq_id == NULL\n    } llama_batch;\n\n    struct llama_model_params {\n        int32_t n_gpu_layers; // number of layers to store in VRAM\n        int32_t main_gpu;     // the GPU that is used for scratch and small tensors\n        float vram_budget_gb; // VRAM budget in GB, -1 for all available VRAM (for a single GPU)\n        const float * tensor_split; // how to split layers across multiple GPUs (size: LLAMA_MAX_DEVICES)\n\n        // called with a progress value between 0 and 1, pass NULL to disable\n        llama_progress_callback progress_callback;\n        // context pointer passed to the progress callback\n        void * progress_callback_user_data;\n\n        // Keep the booleans together to avoid misalignment during copy-by-value.\n        bool vocab_only; // only load the vocabulary, no weights\n        bool use_mmap;   // use mmap if possible\n        bool use_mlock;  // force system to keep model in RAM\n        bool reset_gpu_index; // force reset of the GPU index\n        bool disable_gpu_index; // bypass the GPU index and FFN split\n    };\n\n    struct llama_context_params {\n        uint32_t seed;              // RNG seed, -1 for random\n        uint32_t n_ctx;             // text context, 0 = from model\n        uint32_t n_batch;           // prompt processing maximum batch size\n        uint32_t n_threads;         // number of threads to use for generation\n        uint32_t n_threads_batch;   // number of threads to use for batch processing\n        int8_t   rope_scaling_type; // RoPE scaling type, from `enum llama_rope_scaling_type`\n\n        // ref: https://github.com/ggerganov/llama.cpp/pull/2054\n        float    rope_freq_base;   // RoPE base frequency, 0 = from model\n        float    rope_freq_scale;  // RoPE frequency scaling factor, 0 = from model\n        float    yarn_ext_factor;  // YaRN extrapolation mix factor, NaN = from model\n        float    yarn_attn_factor; // YaRN magnitude scaling factor\n        float    yarn_beta_fast;   // YaRN low correction dim\n        float    yarn_beta_slow;   // YaRN high correction dim\n        uint32_t yarn_orig_ctx;    // YaRN original context size\n\n        // Keep the booleans together to avoid misalignment during copy-by-value.\n        bool mul_mat_q;  // if true, use experimental mul_mat_q kernels (DEPRECATED - always true)\n        bool f16_kv;     // use fp16 for KV cache, fp32 otherwise\n        bool logits_all; // the llama_eval() call computes all logits, not just the last one\n        bool embedding;  // embedding mode only\n    };\n\n    // model quantization parameters\n    typedef struct llama_model_quantize_params {\n        int nthread;                 // number of threads to use for quantizing, if <=0 will use std::thread::hardware_concurrency()\n        enum llama_ftype ftype;      // quantize to this llama_ftype\n        bool allow_requantize;       // allow quantizing non-f32/f16 tensors\n        bool quantize_output_tensor; // quantize output.weight\n        bool only_copy;              // only copy tensors - ftype, allow_requantize and quantize_output_tensor are ignored\n        bool pure;                   // disable k-quant mixtures and quantize all tensors to the same type\n    } llama_model_quantize_params;\n\n    // grammar types\n    struct llama_grammar;\n\n    // grammar element type\n    enum llama_gretype {\n        // end of rule definition\n        LLAMA_GRETYPE_END            = 0,\n\n        // start of alternate definition for rule\n        LLAMA_GRETYPE_ALT            = 1,\n\n        // non-terminal element: reference to rule\n        LLAMA_GRETYPE_RULE_REF       = 2,\n\n        // terminal element: character (code point)\n        LLAMA_GRETYPE_CHAR           = 3,\n\n        // inverse char(s) ([^a], [^a-b] [^abc])\n        LLAMA_GRETYPE_CHAR_NOT       = 4,\n\n        // modifies a preceding LLAMA_GRETYPE_CHAR or LLAMA_GRETYPE_CHAR_ALT to\n        // be an inclusive range ([a-z])\n        LLAMA_GRETYPE_CHAR_RNG_UPPER = 5,\n\n        // modifies a preceding LLAMA_GRETYPE_CHAR or\n        // LLAMA_GRETYPE_CHAR_RNG_UPPER to add an alternate char to match ([ab], [a-zA])\n        LLAMA_GRETYPE_CHAR_ALT       = 6,\n    };\n\n    typedef struct llama_grammar_element {\n        enum llama_gretype type;\n        uint32_t           value; // Unicode code point or rule ID\n    } llama_grammar_element;\n\n    // performance timing information\n    struct llama_timings {\n        double t_start_ms;\n        double t_end_ms;\n        double t_load_ms;\n        double t_sample_ms;\n        double t_p_eval_ms;\n        double t_eval_ms;\n\n        int32_t n_sample;\n        int32_t n_p_eval;\n        int32_t n_eval;\n    };\n\n    // Helpers for getting default parameters\n    LLAMA_API struct llama_model_params llama_model_default_params(void);\n    LLAMA_API struct llama_context_params llama_context_default_params(void);\n    LLAMA_API struct llama_model_quantize_params llama_model_quantize_default_params(void);\n\n    // Initialize the llama + ggml backend\n    // If numa is true, use NUMA optimizations\n    // Call once at the start of the program\n    LLAMA_API void llama_backend_init(bool numa);\n\n    // Call once at the end of the program - currently only used for MPI\n    LLAMA_API void llama_backend_free(void);\n\n    LLAMA_API struct llama_model * llama_load_model_from_file(\n                             const char * path_model,\n            struct llama_model_params     params);\n\n    LLAMA_API struct llama_model * llama_load_model_from_file_with_context(\n                             const char * path_model,\n            struct llama_model_params     params,\n            struct llama_context_params * cparams);\n\n    LLAMA_API void llama_free_model(struct llama_model * model);\n\n    LLAMA_API struct llama_context * llama_new_context_with_model(\n                     struct llama_model * model,\n            struct llama_context_params   params);\n\n    // Frees all allocated memory\n    LLAMA_API void llama_free(struct llama_context * ctx);\n\n    LLAMA_API int64_t llama_time_us(void);\n\n    LLAMA_API int  llama_max_devices    (void);\n    LLAMA_API bool llama_mmap_supported (void);\n    LLAMA_API bool llama_mlock_supported(void);\n\n    LLAMA_API const struct llama_model * llama_get_model(const struct llama_context * ctx);\n\n    LLAMA_API int llama_n_ctx      (const struct llama_context * ctx);\n\n    LLAMA_API enum llama_vocab_type llama_vocab_type(const struct llama_model * model);\n    LLAMA_API bool llama_use_sparse_inference(const struct llama_model * model);\n\n    LLAMA_API int llama_n_vocab    (const struct llama_model * model);\n    LLAMA_API int llama_n_ctx_train(const struct llama_model * model);\n    LLAMA_API int llama_n_embd     (const struct llama_model * model);\n\n    // Get the model's RoPE frequency scaling factor\n    LLAMA_API float llama_rope_freq_scale_train(const struct llama_model * model);\n\n    // Get a string describing the model type\n    LLAMA_API int llama_model_desc(const struct llama_model * model, char * buf, size_t buf_size);\n\n    // Returns the total size of all the tensors in the model in bytes\n    LLAMA_API uint64_t llama_model_size(const struct llama_model * model);\n\n    // Returns the total number of parameters in the model\n    LLAMA_API uint64_t llama_model_n_params(const struct llama_model * model);\n\n    // Get a llama model tensor\n    LLAMA_API struct ggml_tensor * llama_get_model_tensor(struct llama_model * model, const char * name);\n\n    // Returns 0 on success\n    LLAMA_API int llama_model_quantize(\n            const char * fname_inp,\n            const char * fname_out,\n            const llama_model_quantize_params * params);\n\n    // Reserve KV cache in VRAM. This is an optimization to allocate KV cache before \n    // FFN layers being split and offloaded to GPU. \n    LLAMA_API void llama_reserve_model_kv_cache(struct llama_model * model, const struct llama_context_params * cparams);\n\n    // Apply a LoRA adapter to a loaded model\n    // path_base_model is the path to a higher quality model to use as a base for\n    // the layers modified by the adapter. Can be NULL to use the current loaded model.\n    // The model needs to be reloaded before applying a new adapter, otherwise the adapter\n    // will be applied on top of the previous one\n    // Returns 0 on success\n    LLAMA_API DEPRECATED(int llama_apply_lora_from_file(\n            struct llama_context * ctx,\n                      const char * path_lora,\n                           float   scale,\n                      const char * path_base_model,\n                             int   n_threads),\n            \"use llama_model_apply_lora_from_file instead\");\n\n    LLAMA_API int llama_model_apply_lora_from_file(\n            const struct llama_model * model,\n                      const char * path_lora,\n                           float   scale,\n                      const char * path_base_model,\n                             int   n_threads);\n\n    LLAMA_API size_t llama_model_offload_ffn_split(struct llama_model * model);\n\n    //\n    // KV cache\n    //\n\n    // Returns the number of tokens in the KV cache\n    LLAMA_API DEPRECATED(int llama_get_kv_cache_token_count(const struct llama_context * ctx),\n            \"avoid using this, it will be removed in the future, instead - count the tokens in user code\");\n\n    // Clear the KV cache\n    LLAMA_API void llama_kv_cache_clear(\n            struct llama_context * ctx);\n\n    // Removes all tokens that belong to the specified sequence and have positions in [p0, p1)\n    // seq_id < 0 : match any sequence\n    // p0 < 0     : [0,  p1]\n    // p1 < 0     : [p0, inf)\n    LLAMA_API void llama_kv_cache_seq_rm(\n            struct llama_context * ctx,\n                    llama_seq_id   seq_id,\n                       llama_pos   p0,\n                       llama_pos   p1);\n\n    // Copy all tokens that belong to the specified sequence to another sequence\n    // Note that this does not allocate extra KV cache memory - it simply assigns the tokens to the new sequence\n    // p0 < 0 : [0,  p1]\n    // p1 < 0 : [p0, inf)\n    LLAMA_API void llama_kv_cache_seq_cp(\n            struct llama_context * ctx,\n                    llama_seq_id   seq_id_src,\n                    llama_seq_id   seq_id_dst,\n                       llama_pos   p0,\n                       llama_pos   p1);\n\n    // Removes all tokens that do not belong to the specified sequence\n    LLAMA_API void llama_kv_cache_seq_keep(\n            struct llama_context * ctx,\n                    llama_seq_id   seq_id);\n\n    // Adds relative position \"delta\" to all tokens that belong to the specified sequence and have positions in [p0, p1)\n    // If the KV cache is RoPEd, the KV data is updated accordingly\n    // p0 < 0 : [0,  p1]\n    // p1 < 0 : [p0, inf)\n    LLAMA_API void llama_kv_cache_seq_shift(\n            struct llama_context * ctx,\n                    llama_seq_id   seq_id,\n                       llama_pos   p0,\n                       llama_pos   p1,\n                       llama_pos   delta);\n\n    //\n    // State / sessions\n    //\n\n    // Returns the maximum size in bytes of the state (rng, logits, embedding\n    // and kv_cache) - will often be smaller after compacting tokens\n    LLAMA_API size_t llama_get_state_size(const struct llama_context * ctx);\n\n    // Copies the state to the specified destination address.\n    // Destination needs to have allocated enough memory.\n    // Returns the number of bytes copied\n    LLAMA_API size_t llama_copy_state_data(\n            struct llama_context * ctx,\n                         uint8_t * dst);\n\n    // Set the state reading from the specified address\n    // Returns the number of bytes read\n    LLAMA_API size_t llama_set_state_data(\n            struct llama_context * ctx,\n                         uint8_t * src);\n\n    // Save/load session file\n    LLAMA_API bool llama_load_session_file(\n            struct llama_context * ctx,\n                      const char * path_session,\n                     llama_token * tokens_out,\n                          size_t   n_token_capacity,\n                          size_t * n_token_count_out);\n\n    LLAMA_API bool llama_save_session_file(\n            struct llama_context * ctx,\n                      const char * path_session,\n               const llama_token * tokens,\n                          size_t   n_token_count);\n\n    //\n    // Decoding\n    //\n\n    // Run the llama inference to obtain the logits and probabilities for the next token(s).\n    // tokens + n_tokens is the provided batch of new tokens to process\n    // n_past is the number of tokens to use from previous eval calls\n    // Returns 0 on success\n    // DEPRECATED: use llama_decode() instead\n    LLAMA_API DEPRECATED(int llama_eval(\n            struct llama_context * ctx,\n                     llama_token * tokens,\n                         int32_t   n_tokens,\n                             int   n_past),\n            \"use llama_decode() instead\");\n\n    // Same as llama_eval, but use float matrix input directly.\n    // DEPRECATED: use llama_decode() instead\n    LLAMA_API DEPRECATED(int llama_eval_embd(\n            struct llama_context * ctx,\n                           float * embd,\n                         int32_t   n_tokens,\n                             int   n_past),\n            \"use llama_decode() instead\");\n\n    // Return batch for single sequence of tokens starting at pos_0\n    //\n    // NOTE: this is a helper function to facilitate transition to the new batch API - avoid using it\n    //\n    LLAMA_API struct llama_batch llama_batch_get_one(\n                  llama_token * tokens,\n                      int32_t   n_tokens,\n                    llama_pos   pos_0,\n                 llama_seq_id   seq_id);\n\n    // Allocates a batch of tokens on the heap that can hold a maximum of n_tokens\n    // Each token can be assigned up to n_seq_max sequence ids\n    // The batch has to be freed with llama_batch_free()\n    // If embd != 0, llama_batch.embd will be allocated with size of n_tokens * embd * sizeof(float)\n    // Otherwise, llama_batch.token will be allocated to store n_tokens llama_token\n    // The rest of the llama_batch members are allocated with size n_tokens\n    // All members are left uninitialized\n    LLAMA_API struct llama_batch llama_batch_init(\n            int32_t n_tokens,\n            int32_t embd,\n            int32_t n_seq_max);\n\n    // Frees a batch of tokens allocated with llama_batch_init()\n    LLAMA_API void llama_batch_free(struct llama_batch batch);\n\n    // Positive return values does not mean a fatal error, but rather a warning.\n    //   0 - success\n    //   1 - could not find a KV slot for the batch (try reducing the size of the batch or increase the context)\n    // < 0 - error\n    LLAMA_API int llama_decode(\n            struct llama_context * ctx,\n              struct llama_batch   batch);\n\n    // Set the number of threads used for decoding\n    // n_threads is the number of threads used for generation (single token)\n    // n_threads_batch is the number of threads used for prompt and batch processing (multiple tokens)\n    LLAMA_API void llama_set_n_threads(struct llama_context * ctx, uint32_t n_threads, uint32_t n_threads_batch);\n\n    // Token logits obtained from the last call to llama_eval()\n    // The logits for the last token are stored in the last row\n    // Logits for which llama_batch.logits[i] == 0 are undefined\n    // Rows: n_tokens provided with llama_batch\n    // Cols: n_vocab\n    LLAMA_API float * llama_get_logits(struct llama_context * ctx);\n\n    // Logits for the ith token. Equivalent to:\n    // llama_get_logits(ctx) + i*n_vocab\n    LLAMA_API float * llama_get_logits_ith(struct llama_context * ctx, int32_t i);\n\n    // Get the embeddings for the input\n    // shape: [n_embd] (1-dimensional)\n    LLAMA_API float * llama_get_embeddings(struct llama_context * ctx);\n\n    //\n    // Vocab\n    //\n\n    LLAMA_API const char * llama_token_get_text(const struct llama_model * model, llama_token token);\n\n    LLAMA_API float llama_token_get_score(const struct llama_model * model, llama_token token);\n\n    LLAMA_API enum llama_token_type llama_token_get_type(const struct llama_model * model, llama_token token);\n\n    // Special tokens\n    LLAMA_API llama_token llama_token_bos(const struct llama_model * model); // beginning-of-sentence\n    LLAMA_API llama_token llama_token_eos(const struct llama_model * model); // end-of-sentence\n    LLAMA_API llama_token llama_token_nl (const struct llama_model * model); // next-line\n\n    // codellama infill tokens\n    LLAMA_API llama_token llama_token_prefix(const struct llama_model * model); // Beginning of infill prefix\n    LLAMA_API llama_token llama_token_middle(const struct llama_model * model); // Beginning of infill middle\n    LLAMA_API llama_token llama_token_suffix(const struct llama_model * model); // Beginning of infill suffix\n    LLAMA_API llama_token llama_token_eot   (const struct llama_model * model); // End of infill middle\n\n    //\n    // Tokenization\n    //\n\n    /// @details Convert the provided text into tokens.\n    /// @param tokens The tokens pointer must be large enough to hold the resulting tokens.\n    /// @return Returns the number of tokens on success, no more than n_max_tokens\n    /// @return Returns a negative number on failure - the number of tokens that would have been returned\n    /// @param special Allow tokenizing special and/or control tokens which otherwise are not exposed and treated as plaintext.\n    ///                Does not insert a leading space.\n    LLAMA_API int llama_tokenize(\n        const struct llama_model * model,\n                      const char * text,\n                             int   text_len,\n                     llama_token * tokens,\n                             int   n_max_tokens,\n                            bool   add_bos,\n                            bool   special);\n\n    // Token Id -> Piece.\n    // Uses the vocabulary in the provided context.\n    // Does not write null terminator to the buffer.\n    // User code is responsible to remove the leading whitespace of the first non-BOS token when decoding multiple tokens.\n    LLAMA_API int llama_token_to_piece(\n              const struct llama_model * model,\n                           llama_token   token,\n                                  char * buf,\n                                  int    length);\n\n    //\n    // Grammar\n    //\n\n    LLAMA_API struct llama_grammar * llama_grammar_init(\n            const llama_grammar_element ** rules,\n                                 size_t    n_rules,\n                                 size_t    start_rule_index);\n\n    LLAMA_API void llama_grammar_free(struct llama_grammar * grammar);\n\n    LLAMA_API struct llama_grammar * llama_grammar_copy(const struct llama_grammar * grammar);\n\n    //\n    // Sampling functions\n    //\n\n    // Sets the current rng seed.\n    LLAMA_API void llama_set_rng_seed(struct llama_context * ctx, uint32_t seed);\n\n    /// @details Repetition penalty described in CTRL academic paper https://arxiv.org/abs/1909.05858, with negative logit fix.\n    /// @details Frequency and presence penalties described in OpenAI API https://platform.openai.com/docs/api-reference/parameter-details.\n    LLAMA_API void llama_sample_repetition_penalties(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n               const llama_token * last_tokens,\n                          size_t   penalty_last_n,\n                           float   penalty_repeat,\n                           float   penalty_freq,\n                           float   penalty_present);\n\n    /// @details Apply classifier-free guidance to the logits as described in academic paper \"Stay on topic with Classifier-Free Guidance\" https://arxiv.org/abs/2306.17806\n    /// @param candidates A vector of `llama_token_data` containing the candidate tokens, the logits must be directly extracted from the original generation context without being sorted.\n    /// @params guidance_ctx A separate context from the same model. Other than a negative prompt at the beginning, it should have all generated and user input tokens copied from the main context.\n    /// @params scale Guidance strength. 1.0f means no guidance. Higher values mean stronger guidance.\n    LLAMA_API void llama_sample_classifier_free_guidance(\n              struct llama_context * ctx,\n            llama_token_data_array * candidates,\n              struct llama_context * guidance_ctx,\n                             float   scale);\n\n    /// @details Sorts candidate tokens by their logits in descending order and calculate probabilities based on logits.\n    LLAMA_API void llama_sample_softmax(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates);\n\n    /// @details Top-K sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n    LLAMA_API void llama_sample_top_k(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n                             int   k,\n                          size_t   min_keep);\n\n    /// @details Nucleus sampling described in academic paper \"The Curious Case of Neural Text Degeneration\" https://arxiv.org/abs/1904.09751\n    LLAMA_API void llama_sample_top_p(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n                           float   p,\n                          size_t   min_keep);\n\n    /// @details Minimum P sampling as described in https://github.com/ggerganov/llama.cpp/pull/3841\n    LLAMA_API void llama_sample_min_p(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n                           float   p,\n                          size_t   min_keep);\n\n    /// @details Tail Free Sampling described in https://www.trentonbricken.com/Tail-Free-Sampling/.\n    LLAMA_API void llama_sample_tail_free(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n                           float   z,\n                          size_t   min_keep);\n\n    /// @details Locally Typical Sampling implementation described in the paper https://arxiv.org/abs/2202.00666.\n    LLAMA_API void llama_sample_typical(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n                           float   p,\n                          size_t   min_keep);\n\n    LLAMA_API void llama_sample_temp(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n                           float   temp);\n\n    LLAMA_API DEPRECATED(void llama_sample_temperature(\n                struct llama_context * ctx,\n              llama_token_data_array * candidates,\n                               float   temp),\n            \"use llama_sample_temp instead\");\n\n    /// @details Apply constraints from grammar\n    LLAMA_API void llama_sample_grammar(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n      const struct llama_grammar * grammar);\n\n    /// @details Mirostat 1.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.\n    /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.\n    /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.\n    /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.\n    /// @param m The number of tokens considered in the estimation of `s_hat`. This is an arbitrary value that is used to calculate `s_hat`, which in turn helps to calculate the value of `k`. In the paper, they use `m = 100`, but you can experiment with different values to see how it affects the performance of the algorithm.\n    /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.\n    LLAMA_API llama_token llama_sample_token_mirostat(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n                           float   tau,\n                           float   eta,\n                             int   m,\n                           float * mu);\n\n    /// @details Mirostat 2.0 algorithm described in the paper https://arxiv.org/abs/2007.14966. Uses tokens instead of words.\n    /// @param candidates A vector of `llama_token_data` containing the candidate tokens, their probabilities (p), and log-odds (logit) for the current position in the generated text.\n    /// @param tau  The target cross-entropy (or surprise) value you want to achieve for the generated text. A higher value corresponds to more surprising or less predictable text, while a lower value corresponds to less surprising or more predictable text.\n    /// @param eta The learning rate used to update `mu` based on the error between the target and observed surprisal of the sampled word. A larger learning rate will cause `mu` to be updated more quickly, while a smaller learning rate will result in slower updates.\n    /// @param mu Maximum cross-entropy. This value is initialized to be twice the target cross-entropy (`2 * tau`) and is updated in the algorithm based on the error between the target and observed surprisal.\n    LLAMA_API llama_token llama_sample_token_mirostat_v2(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates,\n                           float   tau,\n                           float   eta,\n                           float * mu);\n\n    /// @details Selects the token with the highest probability.\n    ///          Does not compute the token probabilities. Use llama_sample_softmax() instead.\n    LLAMA_API llama_token llama_sample_token_greedy(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates);\n\n    /// @details Randomly selects a token from the candidates based on their probabilities.\n    LLAMA_API llama_token llama_sample_token(\n            struct llama_context * ctx,\n          llama_token_data_array * candidates);\n\n    /// @details Accepts the sampled token into the grammar\n    LLAMA_API void llama_grammar_accept_token(\n            struct llama_context * ctx,\n            struct llama_grammar * grammar,\n                     llama_token   token);\n\n    //\n    // Beam search\n    //\n\n    struct llama_beam_view {\n        const llama_token * tokens;\n\n        size_t n_tokens;\n        float  p;        // Cumulative beam probability (renormalized relative to all beams)\n        bool   eob;      // Callback should set this to true when a beam is at end-of-beam.\n    };\n\n    // Passed to beam_search_callback function.\n    // Whenever 0 < common_prefix_length, this number of tokens should be copied from any of the beams\n    // (e.g. beams[0]) as they will be removed (shifted) from all beams in all subsequent callbacks.\n    // These pointers are valid only during the synchronous callback, so should not be saved.\n    struct llama_beams_state {\n        struct llama_beam_view * beam_views;\n\n        size_t n_beams;               // Number of elements in beam_views[].\n        size_t common_prefix_length;  // Current max length of prefix tokens shared by all beams.\n        bool   last_call;             // True iff this is the last callback invocation.\n    };\n\n    // Type of pointer to the beam_search_callback function.\n    // void* callback_data is any custom data passed to llama_beam_search, that is subsequently\n    // passed back to beam_search_callback. This avoids having to use global variables in the callback.\n    typedef void (*llama_beam_search_callback_fn_t)(void * callback_data, struct llama_beams_state);\n\n    /// @details Deterministically returns entire sentence constructed by a beam search.\n    /// @param ctx Pointer to the llama_context.\n    /// @param callback Invoked for each iteration of the beam_search loop, passing in beams_state.\n    /// @param callback_data A pointer that is simply passed back to callback.\n    /// @param n_beams Number of beams to use.\n    /// @param n_past Number of tokens already evaluated.\n    /// @param n_predict Maximum number of tokens to predict. EOS may occur earlier.\n    LLAMA_API void llama_beam_search(\n                   struct llama_context * ctx,\n        llama_beam_search_callback_fn_t   callback,\n                                   void * callback_data,\n                                 size_t   n_beams,\n                                    int   n_past,\n                                    int   n_predict);\n\n    // Performance information\n    LLAMA_API struct llama_timings llama_get_timings(struct llama_context * ctx);\n\n    LLAMA_API void llama_print_timings(struct llama_context * ctx);\n    LLAMA_API void llama_reset_timings(struct llama_context * ctx);\n\n    // Print system information\n    LLAMA_API const char * llama_print_system_info(void);\n\n    // Set callback for all future logging events.\n    // If this is not called, or NULL is supplied, everything is output on stderr.\n    LLAMA_API void llama_log_set(ggml_log_callback log_callback, void * user_data);\n\n    LLAMA_API void llama_dump_timing_info_yaml(FILE * stream, const struct llama_context * ctx);\n\n#ifdef __cplusplus\n}\n#endif\n\n// Internal API to be implemented by llama.cpp and used by tests/benchmarks only\n#ifdef LLAMA_API_INTERNAL\n\n#include <vector>\n#include <string>\n\nstruct ggml_tensor;\n\nconst std::vector<std::pair<std::string, struct ggml_tensor *>> & llama_internal_get_tensor_map(\n    struct llama_context * ctx\n);\n\n#endif // LLAMA_API_INTERNAL\n\n#endif // LLAMA_H\n"
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "models",
          "type": "tree",
          "content": null
        },
        {
          "name": "mypy.ini",
          "type": "blob",
          "size": 0.1357421875,
          "content": "[mypy]\nstrict = true\nallow_untyped_calls = true\nallow_untyped_defs = true\nallow_incomplete_defs = true\ndisable_error_code = import-untyped\n"
        },
        {
          "name": "pocs",
          "type": "tree",
          "content": null
        },
        {
          "name": "powerinfer-py",
          "type": "tree",
          "content": null
        },
        {
          "name": "prompts",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.0810546875,
          "content": "numpy>=1.24.4\nsentencepiece>=0.1.98\ntransformers>=4.33.2\n./gguf-py\n./powerinfer-py\n"
        },
        {
          "name": "run_with_preset.py",
          "type": "blob",
          "size": 5.1884765625,
          "content": "#!/usr/bin/env python3\n\nimport argparse\nimport os\nimport subprocess\nimport sys\n\nimport yaml\n\nCLI_ARGS_MAIN_PERPLEXITY = [\n    \"batch-size\", \"cfg-negative-prompt\", \"cfg-scale\", \"chunks\", \"color\", \"ctx-size\", \"escape\",\n    \"export\", \"file\", \"frequency-penalty\", \"grammar\", \"grammar-file\", \"hellaswag\",\n    \"hellaswag-tasks\", \"ignore-eos\", \"in-prefix\", \"in-prefix-bos\", \"in-suffix\", \"instruct\",\n    \"interactive\", \"interactive-first\", \"keep\", \"logdir\", \"logit-bias\", \"lora\", \"lora-base\",\n    \"low-vram\", \"main-gpu\", \"memory-f32\", \"mirostat\", \"mirostat-ent\", \"mirostat-lr\", \"mlock\",\n    \"model\", \"multiline-input\", \"n-gpu-layers\", \"n-predict\", \"no-mmap\", \"no-mul-mat-q\",\n    \"np-penalize-nl\", \"numa\", \"ppl-output-type\", \"ppl-stride\", \"presence-penalty\", \"prompt\",\n    \"prompt-cache\", \"prompt-cache-all\", \"prompt-cache-ro\", \"random-prompt\", \"repeat-last-n\",\n    \"repeat-penalty\", \"reverse-prompt\", \"rope-freq-base\", \"rope-freq-scale\", \"rope-scale\", \"seed\",\n    \"simple-io\", \"tensor-split\", \"threads\", \"temp\", \"tfs\", \"top-k\", \"top-p\", \"typical\",\n    \"verbose-prompt\"\n]\n\nCLI_ARGS_LLAMA_BENCH = [\n    \"batch-size\", \"memory-f32\", \"low-vram\", \"model\", \"mul-mat-q\", \"n-gen\", \"n-gpu-layers\",\n    \"n-prompt\", \"output\", \"repetitions\", \"tensor-split\", \"threads\", \"verbose\"\n]\n\nCLI_ARGS_SERVER = [\n    \"alias\", \"batch-size\", \"ctx-size\", \"embedding\", \"host\", \"memory-f32\", \"lora\", \"lora-base\",\n    \"low-vram\", \"main-gpu\", \"mlock\", \"model\", \"n-gpu-layers\", \"n-probs\", \"no-mmap\", \"no-mul-mat-q\",\n    \"numa\", \"path\", \"port\", \"rope-freq-base\", \"timeout\", \"rope-freq-scale\", \"tensor-split\",\n    \"threads\", \"verbose\"\n]\n\ndescription = \"\"\"Run llama.cpp binaries with presets from YAML file(s).\nTo specify which binary should be run, specify the \"binary\" property (main, perplexity, llama-bench, and server are supported).\nTo get a preset file template, run a llama.cpp binary with the \"--logdir\" CLI argument.\n\nFormatting considerations:\n- The YAML property names are the same as the CLI argument names of the corresponding binary.\n- Properties must use the long name of their corresponding llama.cpp CLI arguments.\n- Like the llama.cpp binaries the property names do not differentiate between hyphens and underscores.\n- Flags must be defined as \"<PROPERTY_NAME>: true\" to be effective.\n- To define the logit_bias property, the expected format is \"<TOKEN_ID>: <BIAS>\" in the \"logit_bias\" namespace.\n- To define multiple \"reverse_prompt\" properties simultaneously the expected format is a list of strings.\n- To define a tensor split, pass a list of floats.\n\"\"\"\nusage = \"run_with_preset.py [-h] [yaml_files ...] [--<ARG_NAME> <ARG_VALUE> ...]\"\nepilog = (\"  --<ARG_NAME> specify additional CLI ars to be passed to the binary (override all preset files). \"\n          \"Unknown args will be ignored.\")\n\nparser = argparse.ArgumentParser(\n    description=description, usage=usage, epilog=epilog, formatter_class=argparse.RawTextHelpFormatter)\nparser.add_argument(\"-bin\", \"--binary\", help=\"The binary to run.\")\nparser.add_argument(\"yaml_files\", nargs=\"*\",\n                    help=\"Arbitrary number of YAML files from which to read preset values. \"\n                    \"If two files specify the same values the later one will be used.\")\n\nknown_args, unknown_args = parser.parse_known_args()\n\nif not known_args.yaml_files and not unknown_args:\n    parser.print_help()\n    sys.exit(0)\n\nprops = dict()\n\nfor yaml_file in known_args.yaml_files:\n    with open(yaml_file, \"r\") as f:\n        props.update(yaml.load(f, yaml.SafeLoader))\n\nprops = {prop.replace(\"_\", \"-\"): val for prop, val in props.items()}\n\nbinary = props.pop(\"binary\", \"main\")\nif known_args.binary:\n    binary = known_args.binary\n\nif os.path.exists(f\"./{binary}\"):\n    binary = f\"./{binary}\"\n\nif binary.lower().endswith(\"main\") or binary.lower().endswith(\"perplexity\"):\n    cli_args = CLI_ARGS_MAIN_PERPLEXITY\nelif binary.lower().endswith(\"llama-bench\"):\n    cli_args = CLI_ARGS_LLAMA_BENCH\nelif binary.lower().endswith(\"server\"):\n    cli_args = CLI_ARGS_SERVER\nelse:\n    print(f\"Unknown binary: {binary}\")\n    sys.exit(1)\n\ncommand_list = [binary]\n\nfor cli_arg in cli_args:\n    value = props.pop(cli_arg, None)\n\n    if not value or value == -1:\n        continue\n\n    if cli_arg == \"logit-bias\":\n        for token, bias in value.items():\n            command_list.append(\"--logit-bias\")\n            command_list.append(f\"{token}{bias:+}\")\n        continue\n\n    if cli_arg == \"reverse-prompt\" and not isinstance(value, str):\n        for rp in value:\n            command_list.append(\"--reverse-prompt\")\n            command_list.append(str(rp))\n        continue\n\n    command_list.append(f\"--{cli_arg}\")\n\n    if cli_arg == \"tensor-split\":\n        command_list.append(\",\".join([str(v) for v in value]))\n        continue\n\n    value = str(value)\n\n    if value != \"True\":\n        command_list.append(str(value))\n\nnum_unused = len(props)\nif num_unused > 10:\n    print(f\"The preset file contained a total of {num_unused} unused properties.\")\nelif num_unused > 0:\n    print(\"The preset file contained the following unused properties:\")\n    for prop, value in props.items():\n        print(f\"  {prop}: {value}\")\n\ncommand_list += unknown_args\n\nsp = subprocess.Popen(command_list)\n\nwhile sp.returncode is None:\n    try:\n        sp.wait()\n    except KeyboardInterrupt:\n        pass\n\nsys.exit(sp.returncode)\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "spm-headers",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "unicode.h",
          "type": "blob",
          "size": 45.9814453125,
          "content": "ï»¿#pragma once\n\n#include <cassert>\n#include <stdexcept>\n#include <vector>\n#include <unordered_map>\n\nstatic const std::vector<std::pair<uint32_t, uint32_t>> digit_ranges = {\n{0x30, 0x39}, {0xB2, 0xB3}, {0xB9, 0xB9}, {0x660, 0x669}, {0x6F0, 0x6F9}, {0x7C0, 0x7C9}, {0x966, 0x96F}, {0x9E6, 0x9EF}, {0xA66, 0xA6F}, {0xAE6, 0xAEF}, {0xB66, 0xB6F}, {0xBE6, 0xBEF}, {0xC66, 0xC6F},\n{0xCE6, 0xCEF}, {0xD66, 0xD6F}, {0xDE6, 0xDEF}, {0xE50, 0xE59}, {0xED0, 0xED9}, {0xF20, 0xF29}, {0x1040, 0x1049}, {0x1090, 0x1099}, {0x1369, 0x1371}, {0x17E0, 0x17E9}, {0x1810, 0x1819}, {0x1946, 0x194F},\n{0x19D0, 0x19DA}, {0x1A80, 0x1A89}, {0x1A90, 0x1A99}, {0x1B50, 0x1B59}, {0x1BB0, 0x1BB9}, {0x1C40, 0x1C49}, {0x1C50, 0x1C59}, {0x2070, 0x2070}, {0x2074, 0x2079}, {0x2080, 0x2089}, {0x2460, 0x2468},\n{0x2474, 0x247C}, {0x2488, 0x2490}, {0x24EA, 0x24EA}, {0x24F5, 0x24FD}, {0x24FF, 0x24FF}, {0x2776, 0x277E}, {0x2780, 0x2788}, {0x278A, 0x2792}, {0xA620, 0xA629}, {0xA8D0, 0xA8D9}, {0xA900, 0xA909},\n{0xA9D0, 0xA9D9}, {0xA9F0, 0xA9F9}, {0xAA50, 0xAA59}, {0xABF0, 0xABF9}, {0xFF10, 0xFF19}, {0x104A0, 0x104A9}, {0x10A40, 0x10A43}, {0x10D30, 0x10D39}, {0x10E60, 0x10E68}, {0x11052, 0x1105A},\n{0x11066, 0x1106F}, {0x110F0, 0x110F9}, {0x11136, 0x1113F}, {0x111D0, 0x111D9}, {0x112F0, 0x112F9}, {0x11450, 0x11459}, {0x114D0, 0x114D9}, {0x11650, 0x11659}, {0x116C0, 0x116C9}, {0x11730, 0x11739},\n{0x118E0, 0x118E9}, {0x11950, 0x11959}, {0x11C50, 0x11C59}, {0x11D50, 0x11D59}, {0x11DA0, 0x11DA9}, {0x16A60, 0x16A69}, {0x16B50, 0x16B59}, {0x1D7CE, 0x1D7FF}, {0x1E140, 0x1E149}, {0x1E2F0, 0x1E2F9},\n{0x1E950, 0x1E959}, {0x1F100, 0x1F10A}, {0x1FBF0, 0x1FBF9},\n};\n\nstatic const std::vector<std::pair<uint32_t, uint32_t>> letter_ranges = {\n{0x41, 0x5A}, {0x61, 0x7A}, {0xAA, 0xAA}, {0xB5, 0xB5}, {0xBA, 0xBA}, {0xC0, 0xD6}, {0xD8, 0xF6}, {0xF8, 0x2C1}, {0x2C6, 0x2D1}, {0x2E0, 0x2E4}, {0x2EC, 0x2EC}, {0x2EE, 0x2EE}, {0x370, 0x374},\n{0x376, 0x377}, {0x37A, 0x37D}, {0x37F, 0x37F}, {0x386, 0x386}, {0x388, 0x38A}, {0x38C, 0x38C}, {0x38E, 0x3A1}, {0x3A3, 0x3F5}, {0x3F7, 0x481}, {0x48A, 0x52F}, {0x531, 0x556}, {0x559, 0x559},\n{0x560, 0x588}, {0x5D0, 0x5EA}, {0x5EF, 0x5F2}, {0x620, 0x64A}, {0x66E, 0x66F}, {0x671, 0x6D3}, {0x6D5, 0x6D5}, {0x6E5, 0x6E6}, {0x6EE, 0x6EF}, {0x6FA, 0x6FC}, {0x6FF, 0x6FF}, {0x710, 0x710},\n{0x712, 0x72F}, {0x74D, 0x7A5}, {0x7B1, 0x7B1}, {0x7CA, 0x7EA}, {0x7F4, 0x7F5}, {0x7FA, 0x7FA}, {0x800, 0x815}, {0x81A, 0x81A}, {0x824, 0x824}, {0x828, 0x828}, {0x840, 0x858}, {0x860, 0x86A},\n{0x8A0, 0x8B4}, {0x8B6, 0x8C7}, {0x904, 0x939}, {0x93D, 0x93D}, {0x950, 0x950}, {0x958, 0x961}, {0x971, 0x980}, {0x985, 0x98C}, {0x98F, 0x990}, {0x993, 0x9A8}, {0x9AA, 0x9B0}, {0x9B2, 0x9B2},\n{0x9B6, 0x9B9}, {0x9BD, 0x9BD}, {0x9CE, 0x9CE}, {0x9DC, 0x9DD}, {0x9DF, 0x9E1}, {0x9F0, 0x9F1}, {0x9FC, 0x9FC}, {0xA05, 0xA0A}, {0xA0F, 0xA10}, {0xA13, 0xA28}, {0xA2A, 0xA30}, {0xA32, 0xA33},\n{0xA35, 0xA36}, {0xA38, 0xA39}, {0xA59, 0xA5C}, {0xA5E, 0xA5E}, {0xA72, 0xA74}, {0xA85, 0xA8D}, {0xA8F, 0xA91}, {0xA93, 0xAA8}, {0xAAA, 0xAB0}, {0xAB2, 0xAB3}, {0xAB5, 0xAB9}, {0xABD, 0xABD},\n{0xAD0, 0xAD0}, {0xAE0, 0xAE1}, {0xAF9, 0xAF9}, {0xB05, 0xB0C}, {0xB0F, 0xB10}, {0xB13, 0xB28}, {0xB2A, 0xB30}, {0xB32, 0xB33}, {0xB35, 0xB39}, {0xB3D, 0xB3D}, {0xB5C, 0xB5D}, {0xB5F, 0xB61},\n{0xB71, 0xB71}, {0xB83, 0xB83}, {0xB85, 0xB8A}, {0xB8E, 0xB90}, {0xB92, 0xB95}, {0xB99, 0xB9A}, {0xB9C, 0xB9C}, {0xB9E, 0xB9F}, {0xBA3, 0xBA4}, {0xBA8, 0xBAA}, {0xBAE, 0xBB9}, {0xBD0, 0xBD0},\n{0xC05, 0xC0C}, {0xC0E, 0xC10}, {0xC12, 0xC28}, {0xC2A, 0xC39}, {0xC3D, 0xC3D}, {0xC58, 0xC5A}, {0xC60, 0xC61}, {0xC80, 0xC80}, {0xC85, 0xC8C}, {0xC8E, 0xC90}, {0xC92, 0xCA8}, {0xCAA, 0xCB3},\n{0xCB5, 0xCB9}, {0xCBD, 0xCBD}, {0xCDE, 0xCDE}, {0xCE0, 0xCE1}, {0xCF1, 0xCF2}, {0xD04, 0xD0C}, {0xD0E, 0xD10}, {0xD12, 0xD3A}, {0xD3D, 0xD3D}, {0xD4E, 0xD4E}, {0xD54, 0xD56}, {0xD5F, 0xD61},\n{0xD7A, 0xD7F}, {0xD85, 0xD96}, {0xD9A, 0xDB1}, {0xDB3, 0xDBB}, {0xDBD, 0xDBD}, {0xDC0, 0xDC6}, {0xE01, 0xE30}, {0xE32, 0xE33}, {0xE40, 0xE46}, {0xE81, 0xE82}, {0xE84, 0xE84}, {0xE86, 0xE8A},\n{0xE8C, 0xEA3}, {0xEA5, 0xEA5}, {0xEA7, 0xEB0}, {0xEB2, 0xEB3}, {0xEBD, 0xEBD}, {0xEC0, 0xEC4}, {0xEC6, 0xEC6}, {0xEDC, 0xEDF}, {0xF00, 0xF00}, {0xF40, 0xF47}, {0xF49, 0xF6C}, {0xF88, 0xF8C},\n{0x1000, 0x102A}, {0x103F, 0x103F}, {0x1050, 0x1055}, {0x105A, 0x105D}, {0x1061, 0x1061}, {0x1065, 0x1066}, {0x106E, 0x1070}, {0x1075, 0x1081}, {0x108E, 0x108E}, {0x10A0, 0x10C5}, {0x10C7, 0x10C7},\n{0x10CD, 0x10CD}, {0x10D0, 0x10FA}, {0x10FC, 0x1248}, {0x124A, 0x124D}, {0x1250, 0x1256}, {0x1258, 0x1258}, {0x125A, 0x125D}, {0x1260, 0x1288}, {0x128A, 0x128D}, {0x1290, 0x12B0}, {0x12B2, 0x12B5},\n{0x12B8, 0x12BE}, {0x12C0, 0x12C0}, {0x12C2, 0x12C5}, {0x12C8, 0x12D6}, {0x12D8, 0x1310}, {0x1312, 0x1315}, {0x1318, 0x135A}, {0x1380, 0x138F}, {0x13A0, 0x13F5}, {0x13F8, 0x13FD}, {0x1401, 0x166C},\n{0x166F, 0x167F}, {0x1681, 0x169A}, {0x16A0, 0x16EA}, {0x16F1, 0x16F8}, {0x1700, 0x170C}, {0x170E, 0x1711}, {0x1720, 0x1731}, {0x1740, 0x1751}, {0x1760, 0x176C}, {0x176E, 0x1770}, {0x1780, 0x17B3},\n{0x17D7, 0x17D7}, {0x17DC, 0x17DC}, {0x1820, 0x1878}, {0x1880, 0x1884}, {0x1887, 0x18A8}, {0x18AA, 0x18AA}, {0x18B0, 0x18F5}, {0x1900, 0x191E}, {0x1950, 0x196D}, {0x1970, 0x1974}, {0x1980, 0x19AB},\n{0x19B0, 0x19C9}, {0x1A00, 0x1A16}, {0x1A20, 0x1A54}, {0x1AA7, 0x1AA7}, {0x1B05, 0x1B33}, {0x1B45, 0x1B4B}, {0x1B83, 0x1BA0}, {0x1BAE, 0x1BAF}, {0x1BBA, 0x1BE5}, {0x1C00, 0x1C23}, {0x1C4D, 0x1C4F},\n{0x1C5A, 0x1C7D}, {0x1C80, 0x1C88}, {0x1C90, 0x1CBA}, {0x1CBD, 0x1CBF}, {0x1CE9, 0x1CEC}, {0x1CEE, 0x1CF3}, {0x1CF5, 0x1CF6}, {0x1CFA, 0x1CFA}, {0x1D00, 0x1DBF}, {0x1E00, 0x1F15}, {0x1F18, 0x1F1D},\n{0x1F20, 0x1F45}, {0x1F48, 0x1F4D}, {0x1F50, 0x1F57}, {0x1F59, 0x1F59}, {0x1F5B, 0x1F5B}, {0x1F5D, 0x1F5D}, {0x1F5F, 0x1F7D}, {0x1F80, 0x1FB4}, {0x1FB6, 0x1FBC}, {0x1FBE, 0x1FBE}, {0x1FC2, 0x1FC4},\n{0x1FC6, 0x1FCC}, {0x1FD0, 0x1FD3}, {0x1FD6, 0x1FDB}, {0x1FE0, 0x1FEC}, {0x1FF2, 0x1FF4}, {0x1FF6, 0x1FFC}, {0x2071, 0x2071}, {0x207F, 0x207F}, {0x2090, 0x209C}, {0x2102, 0x2102}, {0x2107, 0x2107},\n{0x210A, 0x2113}, {0x2115, 0x2115}, {0x2119, 0x211D}, {0x2124, 0x2124}, {0x2126, 0x2126}, {0x2128, 0x2128}, {0x212A, 0x212D}, {0x212F, 0x2139}, {0x213C, 0x213F}, {0x2145, 0x2149}, {0x214E, 0x214E},\n{0x2183, 0x2184}, {0x2C00, 0x2C2E}, {0x2C30, 0x2C5E}, {0x2C60, 0x2CE4}, {0x2CEB, 0x2CEE}, {0x2CF2, 0x2CF3}, {0x2D00, 0x2D25}, {0x2D27, 0x2D27}, {0x2D2D, 0x2D2D}, {0x2D30, 0x2D67}, {0x2D6F, 0x2D6F},\n{0x2D80, 0x2D96}, {0x2DA0, 0x2DA6}, {0x2DA8, 0x2DAE}, {0x2DB0, 0x2DB6}, {0x2DB8, 0x2DBE}, {0x2DC0, 0x2DC6}, {0x2DC8, 0x2DCE}, {0x2DD0, 0x2DD6}, {0x2DD8, 0x2DDE}, {0x2E2F, 0x2E2F}, {0x3005, 0x3006},\n{0x3031, 0x3035}, {0x303B, 0x303C}, {0x3041, 0x3096}, {0x309D, 0x309F}, {0x30A1, 0x30FA}, {0x30FC, 0x30FF}, {0x3105, 0x312F}, {0x3131, 0x318E}, {0x31A0, 0x31BF}, {0x31F0, 0x31FF}, {0x3400, 0x4DBF},\n{0x4E00, 0x9FFC}, {0xA000, 0xA48C}, {0xA4D0, 0xA4FD}, {0xA500, 0xA60C}, {0xA610, 0xA61F}, {0xA62A, 0xA62B}, {0xA640, 0xA66E}, {0xA67F, 0xA69D}, {0xA6A0, 0xA6E5}, {0xA717, 0xA71F}, {0xA722, 0xA788},\n{0xA78B, 0xA7BF}, {0xA7C2, 0xA7CA}, {0xA7F5, 0xA801}, {0xA803, 0xA805}, {0xA807, 0xA80A}, {0xA80C, 0xA822}, {0xA840, 0xA873}, {0xA882, 0xA8B3}, {0xA8F2, 0xA8F7}, {0xA8FB, 0xA8FB}, {0xA8FD, 0xA8FE},\n{0xA90A, 0xA925}, {0xA930, 0xA946}, {0xA960, 0xA97C}, {0xA984, 0xA9B2}, {0xA9CF, 0xA9CF}, {0xA9E0, 0xA9E4}, {0xA9E6, 0xA9EF}, {0xA9FA, 0xA9FE}, {0xAA00, 0xAA28}, {0xAA40, 0xAA42}, {0xAA44, 0xAA4B},\n{0xAA60, 0xAA76}, {0xAA7A, 0xAA7A}, {0xAA7E, 0xAAAF}, {0xAAB1, 0xAAB1}, {0xAAB5, 0xAAB6}, {0xAAB9, 0xAABD}, {0xAAC0, 0xAAC0}, {0xAAC2, 0xAAC2}, {0xAADB, 0xAADD}, {0xAAE0, 0xAAEA}, {0xAAF2, 0xAAF4},\n{0xAB01, 0xAB06}, {0xAB09, 0xAB0E}, {0xAB11, 0xAB16}, {0xAB20, 0xAB26}, {0xAB28, 0xAB2E}, {0xAB30, 0xAB5A}, {0xAB5C, 0xAB69}, {0xAB70, 0xABE2}, {0xAC00, 0xD7A3}, {0xD7B0, 0xD7C6}, {0xD7CB, 0xD7FB},\n{0xF900, 0xFA6D}, {0xFA70, 0xFAD9}, {0xFB00, 0xFB06}, {0xFB13, 0xFB17}, {0xFB1D, 0xFB1D}, {0xFB1F, 0xFB28}, {0xFB2A, 0xFB36}, {0xFB38, 0xFB3C}, {0xFB3E, 0xFB3E}, {0xFB40, 0xFB41}, {0xFB43, 0xFB44},\n{0xFB46, 0xFBB1}, {0xFBD3, 0xFD3D}, {0xFD50, 0xFD8F}, {0xFD92, 0xFDC7}, {0xFDF0, 0xFDFB}, {0xFE70, 0xFE74}, {0xFE76, 0xFEFC}, {0xFF21, 0xFF3A}, {0xFF41, 0xFF5A}, {0xFF66, 0xFFBE}, {0xFFC2, 0xFFC7},\n{0xFFCA, 0xFFCF}, {0xFFD2, 0xFFD7}, {0xFFDA, 0xFFDC}, {0x10000, 0x1000B}, {0x1000D, 0x10026}, {0x10028, 0x1003A}, {0x1003C, 0x1003D}, {0x1003F, 0x1004D}, {0x10050, 0x1005D}, {0x10080, 0x100FA},\n{0x10280, 0x1029C}, {0x102A0, 0x102D0}, {0x10300, 0x1031F}, {0x1032D, 0x10340}, {0x10342, 0x10349}, {0x10350, 0x10375}, {0x10380, 0x1039D}, {0x103A0, 0x103C3}, {0x103C8, 0x103CF}, {0x10400, 0x1049D},\n{0x104B0, 0x104D3}, {0x104D8, 0x104FB}, {0x10500, 0x10527}, {0x10530, 0x10563}, {0x10600, 0x10736}, {0x10740, 0x10755}, {0x10760, 0x10767}, {0x10800, 0x10805}, {0x10808, 0x10808}, {0x1080A, 0x10835},\n{0x10837, 0x10838}, {0x1083C, 0x1083C}, {0x1083F, 0x10855}, {0x10860, 0x10876}, {0x10880, 0x1089E}, {0x108E0, 0x108F2}, {0x108F4, 0x108F5}, {0x10900, 0x10915}, {0x10920, 0x10939}, {0x10980, 0x109B7},\n{0x109BE, 0x109BF}, {0x10A00, 0x10A00}, {0x10A10, 0x10A13}, {0x10A15, 0x10A17}, {0x10A19, 0x10A35}, {0x10A60, 0x10A7C}, {0x10A80, 0x10A9C}, {0x10AC0, 0x10AC7}, {0x10AC9, 0x10AE4}, {0x10B00, 0x10B35},\n{0x10B40, 0x10B55}, {0x10B60, 0x10B72}, {0x10B80, 0x10B91}, {0x10C00, 0x10C48}, {0x10C80, 0x10CB2}, {0x10CC0, 0x10CF2}, {0x10D00, 0x10D23}, {0x10E80, 0x10EA9}, {0x10EB0, 0x10EB1}, {0x10F00, 0x10F1C},\n{0x10F27, 0x10F27}, {0x10F30, 0x10F45}, {0x10FB0, 0x10FC4}, {0x10FE0, 0x10FF6}, {0x11003, 0x11037}, {0x11083, 0x110AF}, {0x110D0, 0x110E8}, {0x11103, 0x11126}, {0x11144, 0x11144}, {0x11147, 0x11147},\n{0x11150, 0x11172}, {0x11176, 0x11176}, {0x11183, 0x111B2}, {0x111C1, 0x111C4}, {0x111DA, 0x111DA}, {0x111DC, 0x111DC}, {0x11200, 0x11211}, {0x11213, 0x1122B}, {0x11280, 0x11286}, {0x11288, 0x11288},\n{0x1128A, 0x1128D}, {0x1128F, 0x1129D}, {0x1129F, 0x112A8}, {0x112B0, 0x112DE}, {0x11305, 0x1130C}, {0x1130F, 0x11310}, {0x11313, 0x11328}, {0x1132A, 0x11330}, {0x11332, 0x11333}, {0x11335, 0x11339},\n{0x1133D, 0x1133D}, {0x11350, 0x11350}, {0x1135D, 0x11361}, {0x11400, 0x11434}, {0x11447, 0x1144A}, {0x1145F, 0x11461}, {0x11480, 0x114AF}, {0x114C4, 0x114C5}, {0x114C7, 0x114C7}, {0x11580, 0x115AE},\n{0x115D8, 0x115DB}, {0x11600, 0x1162F}, {0x11644, 0x11644}, {0x11680, 0x116AA}, {0x116B8, 0x116B8}, {0x11700, 0x1171A}, {0x11800, 0x1182B}, {0x118A0, 0x118DF}, {0x118FF, 0x11906}, {0x11909, 0x11909},\n{0x1190C, 0x11913}, {0x11915, 0x11916}, {0x11918, 0x1192F}, {0x1193F, 0x1193F}, {0x11941, 0x11941}, {0x119A0, 0x119A7}, {0x119AA, 0x119D0}, {0x119E1, 0x119E1}, {0x119E3, 0x119E3}, {0x11A00, 0x11A00},\n{0x11A0B, 0x11A32}, {0x11A3A, 0x11A3A}, {0x11A50, 0x11A50}, {0x11A5C, 0x11A89}, {0x11A9D, 0x11A9D}, {0x11AC0, 0x11AF8}, {0x11C00, 0x11C08}, {0x11C0A, 0x11C2E}, {0x11C40, 0x11C40}, {0x11C72, 0x11C8F},\n{0x11D00, 0x11D06}, {0x11D08, 0x11D09}, {0x11D0B, 0x11D30}, {0x11D46, 0x11D46}, {0x11D60, 0x11D65}, {0x11D67, 0x11D68}, {0x11D6A, 0x11D89}, {0x11D98, 0x11D98}, {0x11EE0, 0x11EF2}, {0x11FB0, 0x11FB0},\n{0x12000, 0x12399}, {0x12480, 0x12543}, {0x13000, 0x1342E}, {0x14400, 0x14646}, {0x16800, 0x16A38}, {0x16A40, 0x16A5E}, {0x16AD0, 0x16AED}, {0x16B00, 0x16B2F}, {0x16B40, 0x16B43}, {0x16B63, 0x16B77},\n{0x16B7D, 0x16B8F}, {0x16E40, 0x16E7F}, {0x16F00, 0x16F4A}, {0x16F50, 0x16F50}, {0x16F93, 0x16F9F}, {0x16FE0, 0x16FE1}, {0x16FE3, 0x16FE3}, {0x17000, 0x187F7}, {0x18800, 0x18CD5}, {0x18D00, 0x18D08},\n{0x1B000, 0x1B11E}, {0x1B150, 0x1B152}, {0x1B164, 0x1B167}, {0x1B170, 0x1B2FB}, {0x1BC00, 0x1BC6A}, {0x1BC70, 0x1BC7C}, {0x1BC80, 0x1BC88}, {0x1BC90, 0x1BC99}, {0x1D400, 0x1D454}, {0x1D456, 0x1D49C},\n{0x1D49E, 0x1D49F}, {0x1D4A2, 0x1D4A2}, {0x1D4A5, 0x1D4A6}, {0x1D4A9, 0x1D4AC}, {0x1D4AE, 0x1D4B9}, {0x1D4BB, 0x1D4BB}, {0x1D4BD, 0x1D4C3}, {0x1D4C5, 0x1D505}, {0x1D507, 0x1D50A}, {0x1D50D, 0x1D514},\n{0x1D516, 0x1D51C}, {0x1D51E, 0x1D539}, {0x1D53B, 0x1D53E}, {0x1D540, 0x1D544}, {0x1D546, 0x1D546}, {0x1D54A, 0x1D550}, {0x1D552, 0x1D6A5}, {0x1D6A8, 0x1D6C0}, {0x1D6C2, 0x1D6DA}, {0x1D6DC, 0x1D6FA},\n{0x1D6FC, 0x1D714}, {0x1D716, 0x1D734}, {0x1D736, 0x1D74E}, {0x1D750, 0x1D76E}, {0x1D770, 0x1D788}, {0x1D78A, 0x1D7A8}, {0x1D7AA, 0x1D7C2}, {0x1D7C4, 0x1D7CB}, {0x1E100, 0x1E12C}, {0x1E137, 0x1E13D},\n{0x1E14E, 0x1E14E}, {0x1E2C0, 0x1E2EB}, {0x1E800, 0x1E8C4}, {0x1E900, 0x1E943}, {0x1E94B, 0x1E94B}, {0x1EE00, 0x1EE03}, {0x1EE05, 0x1EE1F}, {0x1EE21, 0x1EE22}, {0x1EE24, 0x1EE24}, {0x1EE27, 0x1EE27},\n{0x1EE29, 0x1EE32}, {0x1EE34, 0x1EE37}, {0x1EE39, 0x1EE39}, {0x1EE3B, 0x1EE3B}, {0x1EE42, 0x1EE42}, {0x1EE47, 0x1EE47}, {0x1EE49, 0x1EE49}, {0x1EE4B, 0x1EE4B}, {0x1EE4D, 0x1EE4F}, {0x1EE51, 0x1EE52},\n{0x1EE54, 0x1EE54}, {0x1EE57, 0x1EE57}, {0x1EE59, 0x1EE59}, {0x1EE5B, 0x1EE5B}, {0x1EE5D, 0x1EE5D}, {0x1EE5F, 0x1EE5F}, {0x1EE61, 0x1EE62}, {0x1EE64, 0x1EE64}, {0x1EE67, 0x1EE6A}, {0x1EE6C, 0x1EE72},\n{0x1EE74, 0x1EE77}, {0x1EE79, 0x1EE7C}, {0x1EE7E, 0x1EE7E}, {0x1EE80, 0x1EE89}, {0x1EE8B, 0x1EE9B}, {0x1EEA1, 0x1EEA3}, {0x1EEA5, 0x1EEA9}, {0x1EEAB, 0x1EEBB}, {0x20000, 0x2A6DD}, {0x2A700, 0x2B734},\n{0x2B740, 0x2B81D}, {0x2B820, 0x2CEA1}, {0x2CEB0, 0x2EBE0}, {0x2F800, 0x2FA1D}, {0x30000, 0x3134A},\n};\n\nstatic const std::vector<std::pair<uint32_t, uint32_t>> whitespace_ranges = {\n{0x9, 0xD}, {0x1C, 0x20}, {0x85, 0x85}, {0xA0, 0xA0}, {0x1680, 0x1680}, {0x2000, 0x200A}, {0x2028, 0x2029}, {0x202F, 0x202F}, {0x205F, 0x205F}, {0x3000, 0x3000},\n};\n\nstatic const std::vector<std::pair<uint32_t, uint32_t>> accent_mark_ranges = {\n{0x300, 0x36F}, {0x483, 0x489}, {0x591, 0x5BD}, {0x5BF, 0x5BF}, {0x5C1, 0x5C2}, {0x5C4, 0x5C5}, {0x5C7, 0x5C7}, {0x610, 0x61A}, {0x64B, 0x65F}, {0x670, 0x670}, {0x6D6, 0x6DC}, {0x6DF, 0x6E4},\n{0x6E7, 0x6E8}, {0x6EA, 0x6ED}, {0x711, 0x711}, {0x730, 0x74A}, {0x7A6, 0x7B0}, {0x7EB, 0x7F3}, {0x7FD, 0x7FD}, {0x816, 0x819}, {0x81B, 0x823}, {0x825, 0x827}, {0x829, 0x82D}, {0x859, 0x85B},\n{0x8D3, 0x8E1}, {0x8E3, 0x903}, {0x93A, 0x93C}, {0x93E, 0x94F}, {0x951, 0x957}, {0x962, 0x963}, {0x981, 0x983}, {0x9BC, 0x9BC}, {0x9BE, 0x9C4}, {0x9C7, 0x9C8}, {0x9CB, 0x9CD}, {0x9D7, 0x9D7},\n{0x9E2, 0x9E3}, {0x9FE, 0x9FE}, {0xA01, 0xA03}, {0xA3C, 0xA3C}, {0xA3E, 0xA42}, {0xA47, 0xA48}, {0xA4B, 0xA4D}, {0xA51, 0xA51}, {0xA70, 0xA71}, {0xA75, 0xA75}, {0xA81, 0xA83}, {0xABC, 0xABC},\n{0xABE, 0xAC5}, {0xAC7, 0xAC9}, {0xACB, 0xACD}, {0xAE2, 0xAE3}, {0xAFA, 0xAFF}, {0xB01, 0xB03}, {0xB3C, 0xB3C}, {0xB3E, 0xB44}, {0xB47, 0xB48}, {0xB4B, 0xB4D}, {0xB55, 0xB57}, {0xB62, 0xB63},\n{0xB82, 0xB82}, {0xBBE, 0xBC2}, {0xBC6, 0xBC8}, {0xBCA, 0xBCD}, {0xBD7, 0xBD7}, {0xC00, 0xC04}, {0xC3E, 0xC44}, {0xC46, 0xC48}, {0xC4A, 0xC4D}, {0xC55, 0xC56}, {0xC62, 0xC63}, {0xC81, 0xC83},\n{0xCBC, 0xCBC}, {0xCBE, 0xCC4}, {0xCC6, 0xCC8}, {0xCCA, 0xCCD}, {0xCD5, 0xCD6}, {0xCE2, 0xCE3}, {0xD00, 0xD03}, {0xD3B, 0xD3C}, {0xD3E, 0xD44}, {0xD46, 0xD48}, {0xD4A, 0xD4D}, {0xD57, 0xD57},\n{0xD62, 0xD63}, {0xD81, 0xD83}, {0xDCA, 0xDCA}, {0xDCF, 0xDD4}, {0xDD6, 0xDD6}, {0xDD8, 0xDDF}, {0xDF2, 0xDF3}, {0xE31, 0xE31}, {0xE34, 0xE3A}, {0xE47, 0xE4E}, {0xEB1, 0xEB1}, {0xEB4, 0xEBC},\n{0xEC8, 0xECD}, {0xF18, 0xF19}, {0xF35, 0xF35}, {0xF37, 0xF37}, {0xF39, 0xF39}, {0xF3E, 0xF3F}, {0xF71, 0xF84}, {0xF86, 0xF87}, {0xF8D, 0xF97}, {0xF99, 0xFBC}, {0xFC6, 0xFC6}, {0x102B, 0x103E},\n{0x1056, 0x1059}, {0x105E, 0x1060}, {0x1062, 0x1064}, {0x1067, 0x106D}, {0x1071, 0x1074}, {0x1082, 0x108D}, {0x108F, 0x108F}, {0x109A, 0x109D}, {0x135D, 0x135F}, {0x1712, 0x1714}, {0x1732, 0x1734},\n{0x1752, 0x1753}, {0x1772, 0x1773}, {0x17B4, 0x17D3}, {0x17DD, 0x17DD}, {0x180B, 0x180D}, {0x1885, 0x1886}, {0x18A9, 0x18A9}, {0x1920, 0x192B}, {0x1930, 0x193B}, {0x1A17, 0x1A1B}, {0x1A55, 0x1A5E},\n{0x1A60, 0x1A7C}, {0x1A7F, 0x1A7F}, {0x1AB0, 0x1AC0}, {0x1B00, 0x1B04}, {0x1B34, 0x1B44}, {0x1B6B, 0x1B73}, {0x1B80, 0x1B82}, {0x1BA1, 0x1BAD}, {0x1BE6, 0x1BF3}, {0x1C24, 0x1C37}, {0x1CD0, 0x1CD2},\n{0x1CD4, 0x1CE8}, {0x1CED, 0x1CED}, {0x1CF4, 0x1CF4}, {0x1CF7, 0x1CF9}, {0x1DC0, 0x1DF9}, {0x1DFB, 0x1DFF}, {0x20D0, 0x20F0}, {0x2CEF, 0x2CF1}, {0x2D7F, 0x2D7F}, {0x2DE0, 0x2DFF}, {0x302A, 0x302F},\n{0x3099, 0x309A}, {0xA66F, 0xA672}, {0xA674, 0xA67D}, {0xA69E, 0xA69F}, {0xA6F0, 0xA6F1}, {0xA802, 0xA802}, {0xA806, 0xA806}, {0xA80B, 0xA80B}, {0xA823, 0xA827}, {0xA82C, 0xA82C}, {0xA880, 0xA881},\n{0xA8B4, 0xA8C5}, {0xA8E0, 0xA8F1}, {0xA8FF, 0xA8FF}, {0xA926, 0xA92D}, {0xA947, 0xA953}, {0xA980, 0xA983}, {0xA9B3, 0xA9C0}, {0xA9E5, 0xA9E5}, {0xAA29, 0xAA36}, {0xAA43, 0xAA43}, {0xAA4C, 0xAA4D},\n{0xAA7B, 0xAA7D}, {0xAAB0, 0xAAB0}, {0xAAB2, 0xAAB4}, {0xAAB7, 0xAAB8}, {0xAABE, 0xAABF}, {0xAAC1, 0xAAC1}, {0xAAEB, 0xAAEF}, {0xAAF5, 0xAAF6}, {0xABE3, 0xABEA}, {0xABEC, 0xABED}, {0xFB1E, 0xFB1E},\n{0xFE00, 0xFE0F}, {0xFE20, 0xFE2F}, {0x101FD, 0x101FD}, {0x102E0, 0x102E0}, {0x10376, 0x1037A}, {0x10A01, 0x10A03}, {0x10A05, 0x10A06}, {0x10A0C, 0x10A0F}, {0x10A38, 0x10A3A}, {0x10A3F, 0x10A3F},\n{0x10AE5, 0x10AE6}, {0x10D24, 0x10D27}, {0x10EAB, 0x10EAC}, {0x10F46, 0x10F50}, {0x11000, 0x11002}, {0x11038, 0x11046}, {0x1107F, 0x11082}, {0x110B0, 0x110BA}, {0x11100, 0x11102}, {0x11127, 0x11134},\n{0x11145, 0x11146}, {0x11173, 0x11173}, {0x11180, 0x11182}, {0x111B3, 0x111C0}, {0x111C9, 0x111CC}, {0x111CE, 0x111CF}, {0x1122C, 0x11237}, {0x1123E, 0x1123E}, {0x112DF, 0x112EA}, {0x11300, 0x11303},\n{0x1133B, 0x1133C}, {0x1133E, 0x11344}, {0x11347, 0x11348}, {0x1134B, 0x1134D}, {0x11357, 0x11357}, {0x11362, 0x11363}, {0x11366, 0x1136C}, {0x11370, 0x11374}, {0x11435, 0x11446}, {0x1145E, 0x1145E},\n{0x114B0, 0x114C3}, {0x115AF, 0x115B5}, {0x115B8, 0x115C0}, {0x115DC, 0x115DD}, {0x11630, 0x11640}, {0x116AB, 0x116B7}, {0x1171D, 0x1172B}, {0x1182C, 0x1183A}, {0x11930, 0x11935}, {0x11937, 0x11938},\n{0x1193B, 0x1193E}, {0x11940, 0x11940}, {0x11942, 0x11943}, {0x119D1, 0x119D7}, {0x119DA, 0x119E0}, {0x119E4, 0x119E4}, {0x11A01, 0x11A0A}, {0x11A33, 0x11A39}, {0x11A3B, 0x11A3E}, {0x11A47, 0x11A47},\n{0x11A51, 0x11A5B}, {0x11A8A, 0x11A99}, {0x11C2F, 0x11C36}, {0x11C38, 0x11C3F}, {0x11C92, 0x11CA7}, {0x11CA9, 0x11CB6}, {0x11D31, 0x11D36}, {0x11D3A, 0x11D3A}, {0x11D3C, 0x11D3D}, {0x11D3F, 0x11D45},\n{0x11D47, 0x11D47}, {0x11D8A, 0x11D8E}, {0x11D90, 0x11D91}, {0x11D93, 0x11D97}, {0x11EF3, 0x11EF6}, {0x16AF0, 0x16AF4}, {0x16B30, 0x16B36}, {0x16F4F, 0x16F4F}, {0x16F51, 0x16F87}, {0x16F8F, 0x16F92},\n{0x16FE4, 0x16FE4}, {0x16FF0, 0x16FF1}, {0x1BC9D, 0x1BC9E}, {0x1D165, 0x1D169}, {0x1D16D, 0x1D172}, {0x1D17B, 0x1D182}, {0x1D185, 0x1D18B}, {0x1D1AA, 0x1D1AD}, {0x1D242, 0x1D244}, {0x1DA00, 0x1DA36},\n{0x1DA3B, 0x1DA6C}, {0x1DA75, 0x1DA75}, {0x1DA84, 0x1DA84}, {0x1DA9B, 0x1DA9F}, {0x1DAA1, 0x1DAAF}, {0x1E000, 0x1E006}, {0x1E008, 0x1E018}, {0x1E01B, 0x1E021}, {0x1E023, 0x1E024}, {0x1E026, 0x1E02A},\n{0x1E130, 0x1E136}, {0x1E2EC, 0x1E2EF}, {0x1E8D0, 0x1E8D6}, {0x1E944, 0x1E94A}, {0xE0100, 0xE01EF},\n};\n\nstatic const std::vector<std::pair<uint32_t, uint32_t>> punctuation_ranges = {\n{0x21, 0x23}, {0x25, 0x2A}, {0x2C, 0x2F}, {0x3A, 0x3B}, {0x3F, 0x40}, {0x5B, 0x5D}, {0x5F, 0x5F}, {0x7B, 0x7B}, {0x7D, 0x7D}, {0xA1, 0xA1}, {0xA7, 0xA7}, {0xAB, 0xAB}, {0xB6, 0xB7}, {0xBB, 0xBB},\n{0xBF, 0xBF}, {0x37E, 0x37E}, {0x387, 0x387}, {0x55A, 0x55F}, {0x589, 0x58A}, {0x5BE, 0x5BE}, {0x5C0, 0x5C0}, {0x5C3, 0x5C3}, {0x5C6, 0x5C6}, {0x5F3, 0x5F4}, {0x609, 0x60A}, {0x60C, 0x60D},\n{0x61B, 0x61B}, {0x61E, 0x61F}, {0x66A, 0x66D}, {0x6D4, 0x6D4}, {0x700, 0x70D}, {0x7F7, 0x7F9}, {0x830, 0x83E}, {0x85E, 0x85E}, {0x964, 0x965}, {0x970, 0x970}, {0x9FD, 0x9FD}, {0xA76, 0xA76},\n{0xAF0, 0xAF0}, {0xC77, 0xC77}, {0xC84, 0xC84}, {0xDF4, 0xDF4}, {0xE4F, 0xE4F}, {0xE5A, 0xE5B}, {0xF04, 0xF12}, {0xF14, 0xF14}, {0xF3A, 0xF3D}, {0xF85, 0xF85}, {0xFD0, 0xFD4}, {0xFD9, 0xFDA},\n{0x104A, 0x104F}, {0x10FB, 0x10FB}, {0x1360, 0x1368}, {0x1400, 0x1400}, {0x166E, 0x166E}, {0x169B, 0x169C}, {0x16EB, 0x16ED}, {0x1735, 0x1736}, {0x17D4, 0x17D6}, {0x17D8, 0x17DA}, {0x1800, 0x180A},\n{0x1944, 0x1945}, {0x1A1E, 0x1A1F}, {0x1AA0, 0x1AA6}, {0x1AA8, 0x1AAD}, {0x1B5A, 0x1B60}, {0x1BFC, 0x1BFF}, {0x1C3B, 0x1C3F}, {0x1C7E, 0x1C7F}, {0x1CC0, 0x1CC7}, {0x1CD3, 0x1CD3}, {0x2010, 0x2027},\n{0x2030, 0x2043}, {0x2045, 0x2051}, {0x2053, 0x205E}, {0x207D, 0x207E}, {0x208D, 0x208E}, {0x2308, 0x230B}, {0x2329, 0x232A}, {0x2768, 0x2775}, {0x27C5, 0x27C6}, {0x27E6, 0x27EF}, {0x2983, 0x2998},\n{0x29D8, 0x29DB}, {0x29FC, 0x29FD}, {0x2CF9, 0x2CFC}, {0x2CFE, 0x2CFF}, {0x2D70, 0x2D70}, {0x2E00, 0x2E2E}, {0x2E30, 0x2E4F}, {0x2E52, 0x2E52}, {0x3001, 0x3003}, {0x3008, 0x3011}, {0x3014, 0x301F},\n{0x3030, 0x3030}, {0x303D, 0x303D}, {0x30A0, 0x30A0}, {0x30FB, 0x30FB}, {0xA4FE, 0xA4FF}, {0xA60D, 0xA60F}, {0xA673, 0xA673}, {0xA67E, 0xA67E}, {0xA6F2, 0xA6F7}, {0xA874, 0xA877}, {0xA8CE, 0xA8CF},\n{0xA8F8, 0xA8FA}, {0xA8FC, 0xA8FC}, {0xA92E, 0xA92F}, {0xA95F, 0xA95F}, {0xA9C1, 0xA9CD}, {0xA9DE, 0xA9DF}, {0xAA5C, 0xAA5F}, {0xAADE, 0xAADF}, {0xAAF0, 0xAAF1}, {0xABEB, 0xABEB}, {0xFD3E, 0xFD3F},\n{0xFE10, 0xFE19}, {0xFE30, 0xFE52}, {0xFE54, 0xFE61}, {0xFE63, 0xFE63}, {0xFE68, 0xFE68}, {0xFE6A, 0xFE6B}, {0xFF01, 0xFF03}, {0xFF05, 0xFF0A}, {0xFF0C, 0xFF0F}, {0xFF1A, 0xFF1B}, {0xFF1F, 0xFF20},\n{0xFF3B, 0xFF3D}, {0xFF3F, 0xFF3F}, {0xFF5B, 0xFF5B}, {0xFF5D, 0xFF5D}, {0xFF5F, 0xFF65}, {0x10100, 0x10102}, {0x1039F, 0x1039F}, {0x103D0, 0x103D0}, {0x1056F, 0x1056F}, {0x10857, 0x10857},\n{0x1091F, 0x1091F}, {0x1093F, 0x1093F}, {0x10A50, 0x10A58}, {0x10A7F, 0x10A7F}, {0x10AF0, 0x10AF6}, {0x10B39, 0x10B3F}, {0x10B99, 0x10B9C}, {0x10EAD, 0x10EAD}, {0x10F55, 0x10F59}, {0x11047, 0x1104D},\n{0x110BB, 0x110BC}, {0x110BE, 0x110C1}, {0x11140, 0x11143}, {0x11174, 0x11175}, {0x111C5, 0x111C8}, {0x111CD, 0x111CD}, {0x111DB, 0x111DB}, {0x111DD, 0x111DF}, {0x11238, 0x1123D}, {0x112A9, 0x112A9},\n{0x1144B, 0x1144F}, {0x1145A, 0x1145B}, {0x1145D, 0x1145D}, {0x114C6, 0x114C6}, {0x115C1, 0x115D7}, {0x11641, 0x11643}, {0x11660, 0x1166C}, {0x1173C, 0x1173E}, {0x1183B, 0x1183B}, {0x11944, 0x11946},\n{0x119E2, 0x119E2}, {0x11A3F, 0x11A46}, {0x11A9A, 0x11A9C}, {0x11A9E, 0x11AA2}, {0x11C41, 0x11C45}, {0x11C70, 0x11C71}, {0x11EF7, 0x11EF8}, {0x11FFF, 0x11FFF}, {0x12470, 0x12474}, {0x16A6E, 0x16A6F},\n{0x16AF5, 0x16AF5}, {0x16B37, 0x16B3B}, {0x16B44, 0x16B44}, {0x16E97, 0x16E9A}, {0x16FE2, 0x16FE2}, {0x1BC9F, 0x1BC9F}, {0x1DA87, 0x1DA8B}, {0x1E95E, 0x1E95F},\n};\n\nstatic const std::vector<std::pair<uint32_t, uint32_t>> symbol_ranges = {\n{0x24, 0x24}, {0x2B, 0x2B}, {0x3C, 0x3E}, {0x5E, 0x5E}, {0x60, 0x60}, {0x7C, 0x7C}, {0x7E, 0x7E}, {0xA2, 0xA6}, {0xA8, 0xA9}, {0xAC, 0xAC}, {0xAE, 0xB1}, {0xB4, 0xB4}, {0xB8, 0xB8}, {0xD7, 0xD7},\n{0xF7, 0xF7}, {0x2C2, 0x2C5}, {0x2D2, 0x2DF}, {0x2E5, 0x2EB}, {0x2ED, 0x2ED}, {0x2EF, 0x2FF}, {0x375, 0x375}, {0x384, 0x385}, {0x3F6, 0x3F6}, {0x482, 0x482}, {0x58D, 0x58F}, {0x606, 0x608},\n{0x60B, 0x60B}, {0x60E, 0x60F}, {0x6DE, 0x6DE}, {0x6E9, 0x6E9}, {0x6FD, 0x6FE}, {0x7F6, 0x7F6}, {0x7FE, 0x7FF}, {0x9F2, 0x9F3}, {0x9FA, 0x9FB}, {0xAF1, 0xAF1}, {0xB70, 0xB70}, {0xBF3, 0xBFA},\n{0xC7F, 0xC7F}, {0xD4F, 0xD4F}, {0xD79, 0xD79}, {0xE3F, 0xE3F}, {0xF01, 0xF03}, {0xF13, 0xF13}, {0xF15, 0xF17}, {0xF1A, 0xF1F}, {0xF34, 0xF34}, {0xF36, 0xF36}, {0xF38, 0xF38}, {0xFBE, 0xFC5},\n{0xFC7, 0xFCC}, {0xFCE, 0xFCF}, {0xFD5, 0xFD8}, {0x109E, 0x109F}, {0x1390, 0x1399}, {0x166D, 0x166D}, {0x17DB, 0x17DB}, {0x1940, 0x1940}, {0x19DE, 0x19FF}, {0x1B61, 0x1B6A}, {0x1B74, 0x1B7C},\n{0x1FBD, 0x1FBD}, {0x1FBF, 0x1FC1}, {0x1FCD, 0x1FCF}, {0x1FDD, 0x1FDF}, {0x1FED, 0x1FEF}, {0x1FFD, 0x1FFE}, {0x2044, 0x2044}, {0x2052, 0x2052}, {0x207A, 0x207C}, {0x208A, 0x208C}, {0x20A0, 0x20BF},\n{0x2100, 0x2101}, {0x2103, 0x2106}, {0x2108, 0x2109}, {0x2114, 0x2114}, {0x2116, 0x2118}, {0x211E, 0x2123}, {0x2125, 0x2125}, {0x2127, 0x2127}, {0x2129, 0x2129}, {0x212E, 0x212E}, {0x213A, 0x213B},\n{0x2140, 0x2144}, {0x214A, 0x214D}, {0x214F, 0x214F}, {0x218A, 0x218B}, {0x2190, 0x2307}, {0x230C, 0x2328}, {0x232B, 0x2426}, {0x2440, 0x244A}, {0x249C, 0x24E9}, {0x2500, 0x2767}, {0x2794, 0x27C4},\n{0x27C7, 0x27E5}, {0x27F0, 0x2982}, {0x2999, 0x29D7}, {0x29DC, 0x29FB}, {0x29FE, 0x2B73}, {0x2B76, 0x2B95}, {0x2B97, 0x2BFF}, {0x2CE5, 0x2CEA}, {0x2E50, 0x2E51}, {0x2E80, 0x2E99}, {0x2E9B, 0x2EF3},\n{0x2F00, 0x2FD5}, {0x2FF0, 0x2FFB}, {0x3004, 0x3004}, {0x3012, 0x3013}, {0x3020, 0x3020}, {0x3036, 0x3037}, {0x303E, 0x303F}, {0x309B, 0x309C}, {0x3190, 0x3191}, {0x3196, 0x319F}, {0x31C0, 0x31E3},\n{0x3200, 0x321E}, {0x322A, 0x3247}, {0x3250, 0x3250}, {0x3260, 0x327F}, {0x328A, 0x32B0}, {0x32C0, 0x33FF}, {0x4DC0, 0x4DFF}, {0xA490, 0xA4C6}, {0xA700, 0xA716}, {0xA720, 0xA721}, {0xA789, 0xA78A},\n{0xA828, 0xA82B}, {0xA836, 0xA839}, {0xAA77, 0xAA79}, {0xAB5B, 0xAB5B}, {0xAB6A, 0xAB6B}, {0xFB29, 0xFB29}, {0xFBB2, 0xFBC1}, {0xFDFC, 0xFDFD}, {0xFE62, 0xFE62}, {0xFE64, 0xFE66}, {0xFE69, 0xFE69},\n{0xFF04, 0xFF04}, {0xFF0B, 0xFF0B}, {0xFF1C, 0xFF1E}, {0xFF3E, 0xFF3E}, {0xFF40, 0xFF40}, {0xFF5C, 0xFF5C}, {0xFF5E, 0xFF5E}, {0xFFE0, 0xFFE6}, {0xFFE8, 0xFFEE}, {0xFFFC, 0xFFFD}, {0x10137, 0x1013F},\n{0x10179, 0x10189}, {0x1018C, 0x1018E}, {0x10190, 0x1019C}, {0x101A0, 0x101A0}, {0x101D0, 0x101FC}, {0x10877, 0x10878}, {0x10AC8, 0x10AC8}, {0x1173F, 0x1173F}, {0x11FD5, 0x11FF1}, {0x16B3C, 0x16B3F},\n{0x16B45, 0x16B45}, {0x1BC9C, 0x1BC9C}, {0x1D000, 0x1D0F5}, {0x1D100, 0x1D126}, {0x1D129, 0x1D164}, {0x1D16A, 0x1D16C}, {0x1D183, 0x1D184}, {0x1D18C, 0x1D1A9}, {0x1D1AE, 0x1D1E8}, {0x1D200, 0x1D241},\n{0x1D245, 0x1D245}, {0x1D300, 0x1D356}, {0x1D6C1, 0x1D6C1}, {0x1D6DB, 0x1D6DB}, {0x1D6FB, 0x1D6FB}, {0x1D715, 0x1D715}, {0x1D735, 0x1D735}, {0x1D74F, 0x1D74F}, {0x1D76F, 0x1D76F}, {0x1D789, 0x1D789},\n{0x1D7A9, 0x1D7A9}, {0x1D7C3, 0x1D7C3}, {0x1D800, 0x1D9FF}, {0x1DA37, 0x1DA3A}, {0x1DA6D, 0x1DA74}, {0x1DA76, 0x1DA83}, {0x1DA85, 0x1DA86}, {0x1E14F, 0x1E14F}, {0x1E2FF, 0x1E2FF}, {0x1ECAC, 0x1ECAC},\n{0x1ECB0, 0x1ECB0}, {0x1ED2E, 0x1ED2E}, {0x1EEF0, 0x1EEF1}, {0x1F000, 0x1F02B}, {0x1F030, 0x1F093}, {0x1F0A0, 0x1F0AE}, {0x1F0B1, 0x1F0BF}, {0x1F0C1, 0x1F0CF}, {0x1F0D1, 0x1F0F5}, {0x1F10D, 0x1F1AD},\n{0x1F1E6, 0x1F202}, {0x1F210, 0x1F23B}, {0x1F240, 0x1F248}, {0x1F250, 0x1F251}, {0x1F260, 0x1F265}, {0x1F300, 0x1F6D7}, {0x1F6E0, 0x1F6EC}, {0x1F6F0, 0x1F6FC}, {0x1F700, 0x1F773}, {0x1F780, 0x1F7D8},\n{0x1F7E0, 0x1F7EB}, {0x1F800, 0x1F80B}, {0x1F810, 0x1F847}, {0x1F850, 0x1F859}, {0x1F860, 0x1F887}, {0x1F890, 0x1F8AD}, {0x1F8B0, 0x1F8B1}, {0x1F900, 0x1F978}, {0x1F97A, 0x1F9CB}, {0x1F9CD, 0x1FA53},\n{0x1FA60, 0x1FA6D}, {0x1FA70, 0x1FA74}, {0x1FA78, 0x1FA7A}, {0x1FA80, 0x1FA86}, {0x1FA90, 0x1FAA8}, {0x1FAB0, 0x1FAB6}, {0x1FAC0, 0x1FAC2}, {0x1FAD0, 0x1FAD6}, {0x1FB00, 0x1FB92}, {0x1FB94, 0x1FBCA},\n};\n\nstatic const std::vector<std::pair<uint32_t, uint32_t>> control_ranges = {\n{0x0, 0x8}, {0xE, 0x1B}, {0x7F, 0x84}, {0x86, 0x9F}, {0xAD, 0xAD}, {0x378, 0x379}, {0x380, 0x383}, {0x38B, 0x38B}, {0x38D, 0x38D}, {0x3A2, 0x3A2}, {0x530, 0x530}, {0x557, 0x558}, {0x58B, 0x58C},\n{0x590, 0x590}, {0x5C8, 0x5CF}, {0x5EB, 0x5EE}, {0x5F5, 0x605}, {0x61C, 0x61D}, {0x6DD, 0x6DD}, {0x70E, 0x70F}, {0x74B, 0x74C}, {0x7B2, 0x7BF}, {0x7FB, 0x7FC}, {0x82E, 0x82F}, {0x83F, 0x83F},\n{0x85C, 0x85D}, {0x85F, 0x85F}, {0x86B, 0x89F}, {0x8B5, 0x8B5}, {0x8C8, 0x8D2}, {0x8E2, 0x8E2}, {0x984, 0x984}, {0x98D, 0x98E}, {0x991, 0x992}, {0x9A9, 0x9A9}, {0x9B1, 0x9B1}, {0x9B3, 0x9B5},\n{0x9BA, 0x9BB}, {0x9C5, 0x9C6}, {0x9C9, 0x9CA}, {0x9CF, 0x9D6}, {0x9D8, 0x9DB}, {0x9DE, 0x9DE}, {0x9E4, 0x9E5}, {0x9FF, 0xA00}, {0xA04, 0xA04}, {0xA0B, 0xA0E}, {0xA11, 0xA12}, {0xA29, 0xA29},\n{0xA31, 0xA31}, {0xA34, 0xA34}, {0xA37, 0xA37}, {0xA3A, 0xA3B}, {0xA3D, 0xA3D}, {0xA43, 0xA46}, {0xA49, 0xA4A}, {0xA4E, 0xA50}, {0xA52, 0xA58}, {0xA5D, 0xA5D}, {0xA5F, 0xA65}, {0xA77, 0xA80},\n{0xA84, 0xA84}, {0xA8E, 0xA8E}, {0xA92, 0xA92}, {0xAA9, 0xAA9}, {0xAB1, 0xAB1}, {0xAB4, 0xAB4}, {0xABA, 0xABB}, {0xAC6, 0xAC6}, {0xACA, 0xACA}, {0xACE, 0xACF}, {0xAD1, 0xADF}, {0xAE4, 0xAE5},\n{0xAF2, 0xAF8}, {0xB00, 0xB00}, {0xB04, 0xB04}, {0xB0D, 0xB0E}, {0xB11, 0xB12}, {0xB29, 0xB29}, {0xB31, 0xB31}, {0xB34, 0xB34}, {0xB3A, 0xB3B}, {0xB45, 0xB46}, {0xB49, 0xB4A}, {0xB4E, 0xB54},\n{0xB58, 0xB5B}, {0xB5E, 0xB5E}, {0xB64, 0xB65}, {0xB78, 0xB81}, {0xB84, 0xB84}, {0xB8B, 0xB8D}, {0xB91, 0xB91}, {0xB96, 0xB98}, {0xB9B, 0xB9B}, {0xB9D, 0xB9D}, {0xBA0, 0xBA2}, {0xBA5, 0xBA7},\n{0xBAB, 0xBAD}, {0xBBA, 0xBBD}, {0xBC3, 0xBC5}, {0xBC9, 0xBC9}, {0xBCE, 0xBCF}, {0xBD1, 0xBD6}, {0xBD8, 0xBE5}, {0xBFB, 0xBFF}, {0xC0D, 0xC0D}, {0xC11, 0xC11}, {0xC29, 0xC29}, {0xC3A, 0xC3C},\n{0xC45, 0xC45}, {0xC49, 0xC49}, {0xC4E, 0xC54}, {0xC57, 0xC57}, {0xC5B, 0xC5F}, {0xC64, 0xC65}, {0xC70, 0xC76}, {0xC8D, 0xC8D}, {0xC91, 0xC91}, {0xCA9, 0xCA9}, {0xCB4, 0xCB4}, {0xCBA, 0xCBB},\n{0xCC5, 0xCC5}, {0xCC9, 0xCC9}, {0xCCE, 0xCD4}, {0xCD7, 0xCDD}, {0xCDF, 0xCDF}, {0xCE4, 0xCE5}, {0xCF0, 0xCF0}, {0xCF3, 0xCFF}, {0xD0D, 0xD0D}, {0xD11, 0xD11}, {0xD45, 0xD45}, {0xD49, 0xD49},\n{0xD50, 0xD53}, {0xD64, 0xD65}, {0xD80, 0xD80}, {0xD84, 0xD84}, {0xD97, 0xD99}, {0xDB2, 0xDB2}, {0xDBC, 0xDBC}, {0xDBE, 0xDBF}, {0xDC7, 0xDC9}, {0xDCB, 0xDCE}, {0xDD5, 0xDD5}, {0xDD7, 0xDD7},\n{0xDE0, 0xDE5}, {0xDF0, 0xDF1}, {0xDF5, 0xE00}, {0xE3B, 0xE3E}, {0xE5C, 0xE80}, {0xE83, 0xE83}, {0xE85, 0xE85}, {0xE8B, 0xE8B}, {0xEA4, 0xEA4}, {0xEA6, 0xEA6}, {0xEBE, 0xEBF}, {0xEC5, 0xEC5},\n{0xEC7, 0xEC7}, {0xECE, 0xECF}, {0xEDA, 0xEDB}, {0xEE0, 0xEFF}, {0xF48, 0xF48}, {0xF6D, 0xF70}, {0xF98, 0xF98}, {0xFBD, 0xFBD}, {0xFCD, 0xFCD}, {0xFDB, 0xFFF}, {0x10C6, 0x10C6}, {0x10C8, 0x10CC},\n{0x10CE, 0x10CF}, {0x1249, 0x1249}, {0x124E, 0x124F}, {0x1257, 0x1257}, {0x1259, 0x1259}, {0x125E, 0x125F}, {0x1289, 0x1289}, {0x128E, 0x128F}, {0x12B1, 0x12B1}, {0x12B6, 0x12B7}, {0x12BF, 0x12BF},\n{0x12C1, 0x12C1}, {0x12C6, 0x12C7}, {0x12D7, 0x12D7}, {0x1311, 0x1311}, {0x1316, 0x1317}, {0x135B, 0x135C}, {0x137D, 0x137F}, {0x139A, 0x139F}, {0x13F6, 0x13F7}, {0x13FE, 0x13FF}, {0x169D, 0x169F},\n{0x16F9, 0x16FF}, {0x170D, 0x170D}, {0x1715, 0x171F}, {0x1737, 0x173F}, {0x1754, 0x175F}, {0x176D, 0x176D}, {0x1771, 0x1771}, {0x1774, 0x177F}, {0x17DE, 0x17DF}, {0x17EA, 0x17EF}, {0x17FA, 0x17FF},\n{0x180E, 0x180F}, {0x181A, 0x181F}, {0x1879, 0x187F}, {0x18AB, 0x18AF}, {0x18F6, 0x18FF}, {0x191F, 0x191F}, {0x192C, 0x192F}, {0x193C, 0x193F}, {0x1941, 0x1943}, {0x196E, 0x196F}, {0x1975, 0x197F},\n{0x19AC, 0x19AF}, {0x19CA, 0x19CF}, {0x19DB, 0x19DD}, {0x1A1C, 0x1A1D}, {0x1A5F, 0x1A5F}, {0x1A7D, 0x1A7E}, {0x1A8A, 0x1A8F}, {0x1A9A, 0x1A9F}, {0x1AAE, 0x1AAF}, {0x1AC1, 0x1AFF}, {0x1B4C, 0x1B4F},\n{0x1B7D, 0x1B7F}, {0x1BF4, 0x1BFB}, {0x1C38, 0x1C3A}, {0x1C4A, 0x1C4C}, {0x1C89, 0x1C8F}, {0x1CBB, 0x1CBC}, {0x1CC8, 0x1CCF}, {0x1CFB, 0x1CFF}, {0x1DFA, 0x1DFA}, {0x1F16, 0x1F17}, {0x1F1E, 0x1F1F},\n{0x1F46, 0x1F47}, {0x1F4E, 0x1F4F}, {0x1F58, 0x1F58}, {0x1F5A, 0x1F5A}, {0x1F5C, 0x1F5C}, {0x1F5E, 0x1F5E}, {0x1F7E, 0x1F7F}, {0x1FB5, 0x1FB5}, {0x1FC5, 0x1FC5}, {0x1FD4, 0x1FD5}, {0x1FDC, 0x1FDC},\n{0x1FF0, 0x1FF1}, {0x1FF5, 0x1FF5}, {0x1FFF, 0x1FFF}, {0x200B, 0x200F}, {0x202A, 0x202E}, {0x2060, 0x206F}, {0x2072, 0x2073}, {0x208F, 0x208F}, {0x209D, 0x209F}, {0x20C0, 0x20CF}, {0x20F1, 0x20FF},\n{0x218C, 0x218F}, {0x2427, 0x243F}, {0x244B, 0x245F}, {0x2B74, 0x2B75}, {0x2B96, 0x2B96}, {0x2C2F, 0x2C2F}, {0x2C5F, 0x2C5F}, {0x2CF4, 0x2CF8}, {0x2D26, 0x2D26}, {0x2D28, 0x2D2C}, {0x2D2E, 0x2D2F},\n{0x2D68, 0x2D6E}, {0x2D71, 0x2D7E}, {0x2D97, 0x2D9F}, {0x2DA7, 0x2DA7}, {0x2DAF, 0x2DAF}, {0x2DB7, 0x2DB7}, {0x2DBF, 0x2DBF}, {0x2DC7, 0x2DC7}, {0x2DCF, 0x2DCF}, {0x2DD7, 0x2DD7}, {0x2DDF, 0x2DDF},\n{0x2E53, 0x2E7F}, {0x2E9A, 0x2E9A}, {0x2EF4, 0x2EFF}, {0x2FD6, 0x2FEF}, {0x2FFC, 0x2FFF}, {0x3040, 0x3040}, {0x3097, 0x3098}, {0x3100, 0x3104}, {0x3130, 0x3130}, {0x318F, 0x318F}, {0x31E4, 0x31EF},\n{0x321F, 0x321F}, {0x9FFD, 0x9FFF}, {0xA48D, 0xA48F}, {0xA4C7, 0xA4CF}, {0xA62C, 0xA63F}, {0xA6F8, 0xA6FF}, {0xA7C0, 0xA7C1}, {0xA7CB, 0xA7F4}, {0xA82D, 0xA82F}, {0xA83A, 0xA83F}, {0xA878, 0xA87F},\n{0xA8C6, 0xA8CD}, {0xA8DA, 0xA8DF}, {0xA954, 0xA95E}, {0xA97D, 0xA97F}, {0xA9CE, 0xA9CE}, {0xA9DA, 0xA9DD}, {0xA9FF, 0xA9FF}, {0xAA37, 0xAA3F}, {0xAA4E, 0xAA4F}, {0xAA5A, 0xAA5B}, {0xAAC3, 0xAADA},\n{0xAAF7, 0xAB00}, {0xAB07, 0xAB08}, {0xAB0F, 0xAB10}, {0xAB17, 0xAB1F}, {0xAB27, 0xAB27}, {0xAB2F, 0xAB2F}, {0xAB6C, 0xAB6F}, {0xABEE, 0xABEF}, {0xABFA, 0xABFF}, {0xD7A4, 0xD7AF}, {0xD7C7, 0xD7CA},\n{0xD7FC, 0xF8FF}, {0xFA6E, 0xFA6F}, {0xFADA, 0xFAFF}, {0xFB07, 0xFB12}, {0xFB18, 0xFB1C}, {0xFB37, 0xFB37}, {0xFB3D, 0xFB3D}, {0xFB3F, 0xFB3F}, {0xFB42, 0xFB42}, {0xFB45, 0xFB45}, {0xFBC2, 0xFBD2},\n{0xFD40, 0xFD4F}, {0xFD90, 0xFD91}, {0xFDC8, 0xFDEF}, {0xFDFE, 0xFDFF}, {0xFE1A, 0xFE1F}, {0xFE53, 0xFE53}, {0xFE67, 0xFE67}, {0xFE6C, 0xFE6F}, {0xFE75, 0xFE75}, {0xFEFD, 0xFF00}, {0xFFBF, 0xFFC1},\n{0xFFC8, 0xFFC9}, {0xFFD0, 0xFFD1}, {0xFFD8, 0xFFD9}, {0xFFDD, 0xFFDF}, {0xFFE7, 0xFFE7}, {0xFFEF, 0xFFFB}, {0xFFFE, 0xFFFF}, {0x1000C, 0x1000C}, {0x10027, 0x10027}, {0x1003B, 0x1003B},\n{0x1003E, 0x1003E}, {0x1004E, 0x1004F}, {0x1005E, 0x1007F}, {0x100FB, 0x100FF}, {0x10103, 0x10106}, {0x10134, 0x10136}, {0x1018F, 0x1018F}, {0x1019D, 0x1019F}, {0x101A1, 0x101CF}, {0x101FE, 0x1027F},\n{0x1029D, 0x1029F}, {0x102D1, 0x102DF}, {0x102FC, 0x102FF}, {0x10324, 0x1032C}, {0x1034B, 0x1034F}, {0x1037B, 0x1037F}, {0x1039E, 0x1039E}, {0x103C4, 0x103C7}, {0x103D6, 0x103FF}, {0x1049E, 0x1049F},\n{0x104AA, 0x104AF}, {0x104D4, 0x104D7}, {0x104FC, 0x104FF}, {0x10528, 0x1052F}, {0x10564, 0x1056E}, {0x10570, 0x105FF}, {0x10737, 0x1073F}, {0x10756, 0x1075F}, {0x10768, 0x107FF}, {0x10806, 0x10807},\n{0x10809, 0x10809}, {0x10836, 0x10836}, {0x10839, 0x1083B}, {0x1083D, 0x1083E}, {0x10856, 0x10856}, {0x1089F, 0x108A6}, {0x108B0, 0x108DF}, {0x108F3, 0x108F3}, {0x108F6, 0x108FA}, {0x1091C, 0x1091E},\n{0x1093A, 0x1093E}, {0x10940, 0x1097F}, {0x109B8, 0x109BB}, {0x109D0, 0x109D1}, {0x10A04, 0x10A04}, {0x10A07, 0x10A0B}, {0x10A14, 0x10A14}, {0x10A18, 0x10A18}, {0x10A36, 0x10A37}, {0x10A3B, 0x10A3E},\n{0x10A49, 0x10A4F}, {0x10A59, 0x10A5F}, {0x10AA0, 0x10ABF}, {0x10AE7, 0x10AEA}, {0x10AF7, 0x10AFF}, {0x10B36, 0x10B38}, {0x10B56, 0x10B57}, {0x10B73, 0x10B77}, {0x10B92, 0x10B98}, {0x10B9D, 0x10BA8},\n{0x10BB0, 0x10BFF}, {0x10C49, 0x10C7F}, {0x10CB3, 0x10CBF}, {0x10CF3, 0x10CF9}, {0x10D28, 0x10D2F}, {0x10D3A, 0x10E5F}, {0x10E7F, 0x10E7F}, {0x10EAA, 0x10EAA}, {0x10EAE, 0x10EAF}, {0x10EB2, 0x10EFF},\n{0x10F28, 0x10F2F}, {0x10F5A, 0x10FAF}, {0x10FCC, 0x10FDF}, {0x10FF7, 0x10FFF}, {0x1104E, 0x11051}, {0x11070, 0x1107E}, {0x110BD, 0x110BD}, {0x110C2, 0x110CF}, {0x110E9, 0x110EF}, {0x110FA, 0x110FF},\n{0x11135, 0x11135}, {0x11148, 0x1114F}, {0x11177, 0x1117F}, {0x111E0, 0x111E0}, {0x111F5, 0x111FF}, {0x11212, 0x11212}, {0x1123F, 0x1127F}, {0x11287, 0x11287}, {0x11289, 0x11289}, {0x1128E, 0x1128E},\n{0x1129E, 0x1129E}, {0x112AA, 0x112AF}, {0x112EB, 0x112EF}, {0x112FA, 0x112FF}, {0x11304, 0x11304}, {0x1130D, 0x1130E}, {0x11311, 0x11312}, {0x11329, 0x11329}, {0x11331, 0x11331}, {0x11334, 0x11334},\n{0x1133A, 0x1133A}, {0x11345, 0x11346}, {0x11349, 0x1134A}, {0x1134E, 0x1134F}, {0x11351, 0x11356}, {0x11358, 0x1135C}, {0x11364, 0x11365}, {0x1136D, 0x1136F}, {0x11375, 0x113FF}, {0x1145C, 0x1145C},\n{0x11462, 0x1147F}, {0x114C8, 0x114CF}, {0x114DA, 0x1157F}, {0x115B6, 0x115B7}, {0x115DE, 0x115FF}, {0x11645, 0x1164F}, {0x1165A, 0x1165F}, {0x1166D, 0x1167F}, {0x116B9, 0x116BF}, {0x116CA, 0x116FF},\n{0x1171B, 0x1171C}, {0x1172C, 0x1172F}, {0x11740, 0x117FF}, {0x1183C, 0x1189F}, {0x118F3, 0x118FE}, {0x11907, 0x11908}, {0x1190A, 0x1190B}, {0x11914, 0x11914}, {0x11917, 0x11917}, {0x11936, 0x11936},\n{0x11939, 0x1193A}, {0x11947, 0x1194F}, {0x1195A, 0x1199F}, {0x119A8, 0x119A9}, {0x119D8, 0x119D9}, {0x119E5, 0x119FF}, {0x11A48, 0x11A4F}, {0x11AA3, 0x11ABF}, {0x11AF9, 0x11BFF}, {0x11C09, 0x11C09},\n{0x11C37, 0x11C37}, {0x11C46, 0x11C4F}, {0x11C6D, 0x11C6F}, {0x11C90, 0x11C91}, {0x11CA8, 0x11CA8}, {0x11CB7, 0x11CFF}, {0x11D07, 0x11D07}, {0x11D0A, 0x11D0A}, {0x11D37, 0x11D39}, {0x11D3B, 0x11D3B},\n{0x11D3E, 0x11D3E}, {0x11D48, 0x11D4F}, {0x11D5A, 0x11D5F}, {0x11D66, 0x11D66}, {0x11D69, 0x11D69}, {0x11D8F, 0x11D8F}, {0x11D92, 0x11D92}, {0x11D99, 0x11D9F}, {0x11DAA, 0x11EDF}, {0x11EF9, 0x11FAF},\n{0x11FB1, 0x11FBF}, {0x11FF2, 0x11FFE}, {0x1239A, 0x123FF}, {0x1246F, 0x1246F}, {0x12475, 0x1247F}, {0x12544, 0x12FFF}, {0x1342F, 0x143FF}, {0x14647, 0x167FF}, {0x16A39, 0x16A3F}, {0x16A5F, 0x16A5F},\n{0x16A6A, 0x16A6D}, {0x16A70, 0x16ACF}, {0x16AEE, 0x16AEF}, {0x16AF6, 0x16AFF}, {0x16B46, 0x16B4F}, {0x16B5A, 0x16B5A}, {0x16B62, 0x16B62}, {0x16B78, 0x16B7C}, {0x16B90, 0x16E3F}, {0x16E9B, 0x16EFF},\n{0x16F4B, 0x16F4E}, {0x16F88, 0x16F8E}, {0x16FA0, 0x16FDF}, {0x16FE5, 0x16FEF}, {0x16FF2, 0x16FFF}, {0x187F8, 0x187FF}, {0x18CD6, 0x18CFF}, {0x18D09, 0x1AFFF}, {0x1B11F, 0x1B14F}, {0x1B153, 0x1B163},\n{0x1B168, 0x1B16F}, {0x1B2FC, 0x1BBFF}, {0x1BC6B, 0x1BC6F}, {0x1BC7D, 0x1BC7F}, {0x1BC89, 0x1BC8F}, {0x1BC9A, 0x1BC9B}, {0x1BCA0, 0x1CFFF}, {0x1D0F6, 0x1D0FF}, {0x1D127, 0x1D128}, {0x1D173, 0x1D17A},\n{0x1D1E9, 0x1D1FF}, {0x1D246, 0x1D2DF}, {0x1D2F4, 0x1D2FF}, {0x1D357, 0x1D35F}, {0x1D379, 0x1D3FF}, {0x1D455, 0x1D455}, {0x1D49D, 0x1D49D}, {0x1D4A0, 0x1D4A1}, {0x1D4A3, 0x1D4A4}, {0x1D4A7, 0x1D4A8},\n{0x1D4AD, 0x1D4AD}, {0x1D4BA, 0x1D4BA}, {0x1D4BC, 0x1D4BC}, {0x1D4C4, 0x1D4C4}, {0x1D506, 0x1D506}, {0x1D50B, 0x1D50C}, {0x1D515, 0x1D515}, {0x1D51D, 0x1D51D}, {0x1D53A, 0x1D53A}, {0x1D53F, 0x1D53F},\n{0x1D545, 0x1D545}, {0x1D547, 0x1D549}, {0x1D551, 0x1D551}, {0x1D6A6, 0x1D6A7}, {0x1D7CC, 0x1D7CD}, {0x1DA8C, 0x1DA9A}, {0x1DAA0, 0x1DAA0}, {0x1DAB0, 0x1DFFF}, {0x1E007, 0x1E007}, {0x1E019, 0x1E01A},\n{0x1E022, 0x1E022}, {0x1E025, 0x1E025}, {0x1E02B, 0x1E0FF}, {0x1E12D, 0x1E12F}, {0x1E13E, 0x1E13F}, {0x1E14A, 0x1E14D}, {0x1E150, 0x1E2BF}, {0x1E2FA, 0x1E2FE}, {0x1E300, 0x1E7FF}, {0x1E8C5, 0x1E8C6},\n{0x1E8D7, 0x1E8FF}, {0x1E94C, 0x1E94F}, {0x1E95A, 0x1E95D}, {0x1E960, 0x1EC70}, {0x1ECB5, 0x1ED00}, {0x1ED3E, 0x1EDFF}, {0x1EE04, 0x1EE04}, {0x1EE20, 0x1EE20}, {0x1EE23, 0x1EE23}, {0x1EE25, 0x1EE26},\n{0x1EE28, 0x1EE28}, {0x1EE33, 0x1EE33}, {0x1EE38, 0x1EE38}, {0x1EE3A, 0x1EE3A}, {0x1EE3C, 0x1EE41}, {0x1EE43, 0x1EE46}, {0x1EE48, 0x1EE48}, {0x1EE4A, 0x1EE4A}, {0x1EE4C, 0x1EE4C}, {0x1EE50, 0x1EE50},\n{0x1EE53, 0x1EE53}, {0x1EE55, 0x1EE56}, {0x1EE58, 0x1EE58}, {0x1EE5A, 0x1EE5A}, {0x1EE5C, 0x1EE5C}, {0x1EE5E, 0x1EE5E}, {0x1EE60, 0x1EE60}, {0x1EE63, 0x1EE63}, {0x1EE65, 0x1EE66}, {0x1EE6B, 0x1EE6B},\n{0x1EE73, 0x1EE73}, {0x1EE78, 0x1EE78}, {0x1EE7D, 0x1EE7D}, {0x1EE7F, 0x1EE7F}, {0x1EE8A, 0x1EE8A}, {0x1EE9C, 0x1EEA0}, {0x1EEA4, 0x1EEA4}, {0x1EEAA, 0x1EEAA}, {0x1EEBC, 0x1EEEF}, {0x1EEF2, 0x1EFFF},\n{0x1F02C, 0x1F02F}, {0x1F094, 0x1F09F}, {0x1F0AF, 0x1F0B0}, {0x1F0C0, 0x1F0C0}, {0x1F0D0, 0x1F0D0}, {0x1F0F6, 0x1F0FF}, {0x1F1AE, 0x1F1E5}, {0x1F203, 0x1F20F}, {0x1F23C, 0x1F23F}, {0x1F249, 0x1F24F},\n{0x1F252, 0x1F25F}, {0x1F266, 0x1F2FF}, {0x1F6D8, 0x1F6DF}, {0x1F6ED, 0x1F6EF}, {0x1F6FD, 0x1F6FF}, {0x1F774, 0x1F77F}, {0x1F7D9, 0x1F7DF}, {0x1F7EC, 0x1F7FF}, {0x1F80C, 0x1F80F}, {0x1F848, 0x1F84F},\n{0x1F85A, 0x1F85F}, {0x1F888, 0x1F88F}, {0x1F8AE, 0x1F8AF}, {0x1F8B2, 0x1F8FF}, {0x1F979, 0x1F979}, {0x1F9CC, 0x1F9CC}, {0x1FA54, 0x1FA5F}, {0x1FA6E, 0x1FA6F}, {0x1FA75, 0x1FA77}, {0x1FA7B, 0x1FA7F},\n{0x1FA87, 0x1FA8F}, {0x1FAA9, 0x1FAAF}, {0x1FAB7, 0x1FABF}, {0x1FAC3, 0x1FACF}, {0x1FAD7, 0x1FAFF}, {0x1FB93, 0x1FB93}, {0x1FBCB, 0x1FBEF}, {0x1FBFA, 0x1FFFF}, {0x2A6DE, 0x2A6FF}, {0x2B735, 0x2B73F},\n{0x2B81E, 0x2B81F}, {0x2CEA2, 0x2CEAF}, {0x2EBE1, 0x2F7FF}, {0x2FA1E, 0x2FFFF}, {0x3134B, 0xE00FF}, {0xE01F0, 0x10FFFF},\n};\n\nstatic std::string codepoint_to_utf8(uint32_t cp) {\n    std::string result;\n    if (/* 0x00 <= cp && */ cp <= 0x7f) {\n        result.push_back(cp);\n    }\n    else if (0x80 <= cp && cp <= 0x7ff) {\n        result.push_back(0xc0 | ((cp >> 6) & 0x1f));\n        result.push_back(0x80 | (cp & 0x3f));\n    }\n    else if (0x800 <= cp && cp <= 0xffff) {\n        result.push_back(0xe0 | ((cp >> 12) & 0x0f));\n        result.push_back(0x80 | ((cp >> 6) & 0x3f));\n        result.push_back(0x80 | (cp & 0x3f));\n    }\n    else if (0x10000 <= cp && cp <= 0x10ffff) {\n        result.push_back(0xf0 | ((cp >> 18) & 0x07));\n        result.push_back(0x80 | ((cp >> 12) & 0x3f));\n        result.push_back(0x80 | ((cp >> 6) & 0x3f));\n        result.push_back(0x80 | (cp & 0x3f));\n    }\n    else {\n        throw std::invalid_argument(\"invalid codepoint\");\n    }\n    return result;\n}\n\nstatic std::string codepoints_to_utf8(const std::vector<uint32_t> & cps) {\n    std::string result;\n    for (size_t i = 0; i < cps.size(); ++i) {\n        result.append(codepoint_to_utf8(cps[i]));\n    }\n    return result;\n}\n\nstatic uint32_t codepoint_from_utf8(const std::string & utf8, size_t & offset) {\n    assert(offset < utf8.size());\n    if (!(utf8[offset + 0] & 0x80)) {\n        auto result = utf8[offset + 0];\n        offset += 1;\n        return result;\n    }\n    else if (!(utf8[offset + 0] & 0x40)) {\n        throw std::invalid_argument(\"invalid character\");\n    }\n    else if (!(utf8[offset + 0] & 0x20)) {\n        if (offset + 1 >= utf8.size() || ! ((utf8[offset + 1] & 0xc0) == 0x80))\n            throw std::invalid_argument(\"invalid character\");\n        auto result = ((utf8[offset + 0] & 0x1f) << 6) | (utf8[offset + 1] & 0x3f);\n        offset += 2;\n        return result;\n    }\n    else if (!(utf8[offset + 0] & 0x10)) {\n        if (offset + 2 >= utf8.size() || ! ((utf8[offset + 1] & 0xc0) == 0x80) || ! ((utf8[offset + 2] & 0xc0) == 0x80))\n            throw std::invalid_argument(\"invalid character\");\n        auto result = ((utf8[offset + 0] & 0x0f) << 12) | ((utf8[offset + 1] & 0x3f) << 6) | (utf8[offset + 2] & 0x3f);\n        offset += 3;\n        return result;\n    }\n    else if (!(utf8[offset + 0] & 0x08)) {\n        if (offset + 3 >= utf8.size() || ! ((utf8[offset + 1] & 0xc0) == 0x80) || ! ((utf8[offset + 2] & 0xc0) == 0x80) || !((utf8[offset + 3] & 0xc0) == 0x80))\n            throw std::invalid_argument(\"invalid character\");\n        auto result = ((utf8[offset + 0] & 0x07) << 18) | ((utf8[offset + 1] & 0x3f) << 12) | ((utf8[offset + 2] & 0x3f) << 6) | (utf8[offset + 3] & 0x3f);\n        offset += 4;\n        return result;\n    }\n    throw std::invalid_argument(\"invalid string\");\n}\n\nstatic std::vector<uint32_t> codepoints_from_utf8(const std::string & utf8) {\n    std::vector<uint32_t> result;\n    size_t offset = 0;\n    while (offset < utf8.size()) {\n        result.push_back(codepoint_from_utf8(utf8, offset));\n    }\n    return result;\n}\n\nstatic std::vector<uint16_t> codepoint_to_utf16(uint32_t cp) {\n    std::vector<uint16_t> result;\n    if (/* 0x0000 <= cp && */ cp <= 0xffff) {\n        result.emplace_back(cp);\n    }\n    else if (0x10000 <= cp && cp <= 0x10ffff) {\n        result.emplace_back(0xd800 | ((cp - 0x10000) >> 10));\n        result.emplace_back(0xdc00 | ((cp - 0x10000) & 0x03ff));\n    }\n    else {\n        throw std::invalid_argument(\"invalid codepoint\");\n    }\n    return result;\n}\n\nstatic std::vector<uint16_t> codepoints_to_utf16(const std::vector<uint32_t> & cps) {\n    std::vector<uint16_t> result;\n    for (size_t i = 0; i < cps.size(); ++i) {\n        auto temp = codepoint_to_utf16(cps[i]);\n        result.insert(result.end(), temp.begin(), temp.end());\n    }\n    return result;\n}\n\nstatic uint32_t codepoint_from_utf16(const std::vector<uint16_t> & utf16, size_t & offset) {\n    assert(offset < utf16.size());\n    if (((utf16[0] >> 10) << 10) != 0xd800) {\n        auto result = utf16[offset + 0];\n        offset += 1;\n        return result;\n    }\n    else {\n        if (offset + 1 >= utf16.size() || !((utf16[1] & 0xdc00) == 0xdc00))\n            throw std::invalid_argument(\"invalid character\");\n        auto result = 0x10000 + (((utf16[0] & 0x03ff) << 10) | (utf16[1] & 0x03ff));\n        offset += 2;\n        return result;\n    }\n    throw std::invalid_argument(\"invalid string\");\n}\n\nstatic std::vector<uint32_t> codepoints_from_utf16(const std::vector<uint16_t> & utf16) {\n    std::vector<uint32_t> result;\n    size_t offset = 0;\n    while (offset < utf16.size())\n        result.push_back(codepoint_from_utf16(utf16, offset));\n    return result;\n}\n\n#define CODEPOINT_TYPE_UNIDENTIFIED 0\n#define CODEPOINT_TYPE_DIGIT 1\n#define CODEPOINT_TYPE_LETTER 2\n#define CODEPOINT_TYPE_WHITESPACE 3\n#define CODEPOINT_TYPE_ACCENT_MARK 4\n#define CODEPOINT_TYPE_PUNCTUATION 5\n#define CODEPOINT_TYPE_SYMBOL 6\n#define CODEPOINT_TYPE_CONTROL 7\n\nstatic std::unordered_map<uint32_t, int> codepoint_type_map() {\n    std::unordered_map<uint32_t, int> codepoint_types;\n    for (auto p : digit_ranges) {\n        for(auto i = p.first; i <= p.second; ++ i)\n            codepoint_types[i] = CODEPOINT_TYPE_DIGIT;\n    }\n    for(auto p : letter_ranges) {\n        for(auto i = p.first; i <= p.second; ++ i)\n            codepoint_types[i] = CODEPOINT_TYPE_LETTER;\n    }\n    for(auto p : whitespace_ranges) {\n        for(auto i = p.first; i <= p.second; ++ i)\n            codepoint_types[i] = CODEPOINT_TYPE_WHITESPACE;\n    }\n    for(auto p : accent_mark_ranges) {\n        for(auto i = p.first; i <= p.second; ++ i)\n            codepoint_types[i] = CODEPOINT_TYPE_ACCENT_MARK;\n    }\n    for(auto p : punctuation_ranges) {\n        for(auto i = p.first; i <= p.second; ++ i)\n            codepoint_types[i] = CODEPOINT_TYPE_PUNCTUATION;\n    }\n    for (auto p : symbol_ranges) {\n        for (auto i = p.first; i <= p.second; ++i)\n            codepoint_types[i] = CODEPOINT_TYPE_SYMBOL;\n    }\n    for(auto p : control_ranges) {\n        for(auto i = p.first; i <= p.second; ++ i)\n            codepoint_types[i] = CODEPOINT_TYPE_CONTROL;\n    }\n    return codepoint_types;\n}\n\nstatic int codepoint_type(uint32_t cp) {\n    static std::unordered_map<uint32_t, int> codepoint_types = codepoint_type_map();\n    return codepoint_types[cp];\n}\n\nstatic int codepoint_type(const std::string & utf8) {\n    if (utf8.length() == 0)\n        return CODEPOINT_TYPE_UNIDENTIFIED;\n    size_t offset = 0;\n    return codepoint_type(codepoint_from_utf8(utf8, offset));\n}\n\nstatic std::unordered_map<uint8_t, std::string> bytes_to_unicode_map_bpe() {\n    std::unordered_map<uint8_t, std::string> map;\n    for (int ch = u'!'; ch <= u'~'; ++ch) {\n        assert(0 <= ch && ch < 256);\n        map[ch] = codepoint_to_utf8(ch);\n    }\n    for (int ch = u'Â¡'; ch <= u'Â¬'; ++ch) {\n        assert(0 <= ch && ch < 256);\n        map[ch] = codepoint_to_utf8(ch);\n    }\n    for (int ch = u'Â®'; ch <= u'Ã¿'; ++ch) {\n        assert(0 <= ch && ch < 256);\n        map[ch] = codepoint_to_utf8(ch);\n    }\n    auto n = 0;\n    for (int ch = 0; ch < 256; ++ch) {\n        if (map.find(ch) == map.end()) {\n            map[ch] = codepoint_to_utf8(256 + n);\n            ++n;\n        }\n    }\n    return map;\n}\n\nstatic std::string bytes_to_unicode_bpe(uint8_t byte) {\n    static std::unordered_map<uint8_t, std::string> map = bytes_to_unicode_map_bpe();\n    return map.at(byte);\n}\n\nstatic std::unordered_map<std::string, uint8_t> unicode_to_bytes_map_bpe() {\n    std::unordered_map<std::string, uint8_t> map;\n    for (int ch = u'!'; ch <= u'~'; ++ch) {\n        assert(0 <= ch && ch < 256);\n        map[codepoint_to_utf8(ch)] = ch;\n    }\n    for (int ch = u'Â¡'; ch <= u'Â¬'; ++ch) {\n        assert(0 <= ch && ch < 256);\n        map[codepoint_to_utf8(ch)] = ch;\n    }\n    for (int ch = u'Â®'; ch <= u'Ã¿'; ++ch) {\n        assert(0 <= ch && ch < 256);\n        map[codepoint_to_utf8(ch)] = ch;\n    }\n    auto n = 0;\n    for (int ch = 0; ch < 256; ++ch) {\n        if (map.find(codepoint_to_utf8(ch)) == map.end()) {\n            map[codepoint_to_utf8(256 + n)] = ch;\n            ++n;\n        }\n    }\n    return map;\n}\n\nstatic uint8_t unicode_to_bytes_bpe(const std::string & utf8) {\n    static std::unordered_map<std::string, uint8_t> map = unicode_to_bytes_map_bpe();\n    return map.at(utf8);\n}\n\n"
        }
      ]
    }
  ]
}