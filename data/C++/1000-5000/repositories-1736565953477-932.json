{
  "metadata": {
    "timestamp": 1736565953477,
    "page": 932,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjk0MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "BlazingDB/blazingsql",
      "stars": 1939,
      "defaultBranch": "branch-21.08",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 4.91796875,
          "content": "---\nLanguage:        Cpp\n# BasedOnStyle:  LLVM\nAccessModifierOffset: -4\nAlignAfterOpenBracket: DontAlign\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignOperands:   true\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: true\nAllowShortFunctionsOnASingleLine: All\nAllowShortIfStatementsOnASingleLine: false\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterDefinitionReturnType: None\nAlwaysBreakAfterReturnType: None\nAlwaysBreakBeforeMultilineStrings: true\nAlwaysBreakTemplateDeclarations: true\nBinPackArguments: false\nBinPackParameters: false\nBraceWrapping:\n  AfterClass:      false\n  AfterControlStatement: false\n  AfterEnum:       false\n  AfterFunction:   false\n  AfterNamespace:  false\n  AfterObjCDeclaration: false\n  AfterStruct:     false\n  AfterUnion:      false\n  BeforeCatch:     false\n  BeforeElse:      false\n  IndentBraces:    false\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Custom\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nColumnLimit:     120\nCommentPragmas:  '^ IWYU pragma:'\nConstructorInitializerAllOnOneLineOrOnePerLine: false\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat:   false\nExperimentalAutoDetectBinPacking: false\nForEachMacros:\n  - foreach\n  - Q_FOREACH\n  - BOOST_FOREACH\nIncludeCategories:\n  - Regex:           '^\"(llvm|llvm-c|clang|clang-c)/'\n    Priority:        2\n  - Regex:           '^(<|\"(gtest|gmock|isl|json)/)'\n    Priority:        3\n  - Regex:           '.*'\n    Priority:        1\nIndentCaseLabels: false\nIndentWidth:     4\nIndentWrappedFunctionNames: false\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 2\nNamespaceIndentation: None\nObjCBlockIndentWidth: 2\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: true\nPenaltyBreakBeforeFirstCallParameter: 19\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 60\nPointerAlignment: Middle\nReflowComments:  true\nSortIncludes:    true\nSpaceAfterCStyleCast: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeParens: Never\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 2\nSpacesInAngles:  false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard:        Cpp11\nTabWidth:        4\nUseTab:          Always\n...\n---\nLanguage: Java\nBasedOnStyle: LLVM\nIndentWidth: 4\nReflowComments: true\nSpacesBeforeTrailingComments: 2\nMaxEmptyLinesToKeep: 1\nReflowComments:  true\nSortIncludes:    true\nAlignAfterOpenBracket: DontAlign\nAlignConsecutiveAssignments: false\nAlignConsecutiveDeclarations: false\nAlignOperands:   true\nAlignTrailingComments: true\nAllowAllParametersOfDeclarationOnNextLine: true\nAllowShortBlocksOnASingleLine: false\nAllowShortCaseLabelsOnASingleLine: false\nAllowShortFunctionsOnASingleLine: All\nAllowShortIfStatementsOnASingleLine: false\nAllowShortLoopsOnASingleLine: false\nAlwaysBreakAfterDefinitionReturnType: All\nAlwaysBreakAfterReturnType: All\nAlwaysBreakBeforeMultilineStrings: false\nAlwaysBreakTemplateDeclarations: false\nBinPackArguments: false\nBinPackParameters: false\nBraceWrapping:\n  AfterClass:      false\n  AfterControlStatement: false\n  AfterEnum:       false\n  AfterFunction:   false\n  AfterNamespace:  false\n  AfterObjCDeclaration: false\n  AfterStruct:     false\n  AfterUnion:      false\n  BeforeCatch:     false\n  BeforeElse:      false\n  IndentBraces:    false\nBreakBeforeBinaryOperators: None\nBreakBeforeBraces: Custom\nBreakBeforeTernaryOperators: true\nBreakConstructorInitializersBeforeComma: false\nColumnLimit:     120\nCommentPragmas:  '^ IWYU pragma:'\nConstructorInitializerAllOnOneLineOrOnePerLine: false\nConstructorInitializerIndentWidth: 4\nContinuationIndentWidth: 4\nCpp11BracedListStyle: true\nDerivePointerAlignment: false\nDisableFormat:   false\nExperimentalAutoDetectBinPacking: false\nIndentCaseLabels: true\nIndentWidth:     4\nIndentWrappedFunctionNames: false\nKeepEmptyLinesAtTheStartOfBlocks: false\nMacroBlockBegin: ''\nMacroBlockEnd:   ''\nMaxEmptyLinesToKeep: 2\nNamespaceIndentation: None\nObjCBlockIndentWidth: 2\nObjCSpaceAfterProperty: false\nObjCSpaceBeforeProtocolList: true\nPenaltyBreakBeforeFirstCallParameter: 19\nPenaltyBreakComment: 300\nPenaltyBreakFirstLessLess: 120\nPenaltyBreakString: 1000\nPenaltyExcessCharacter: 1000000\nPenaltyReturnTypeOnItsOwnLine: 6\nPointerAlignment: Middle\nReflowComments:  true\nSortIncludes:    true\nSpaceAfterCStyleCast: true\nSpaceBeforeAssignmentOperators: true\nSpaceBeforeParens: Never\nSpaceInEmptyParentheses: false\nSpacesBeforeTrailingComments: 2\nSpacesInAngles:  false\nSpacesInContainerLiterals: true\nSpacesInCStyleCastParentheses: false\nSpacesInParentheses: false\nSpacesInSquareBrackets: false\nStandard:        Cpp11\nTabWidth:        4\nUseTab:          Always\n...\n"
        },
        {
          "name": ".clang-tidy",
          "type": "blob",
          "size": 0.1708984375,
          "content": "---\nChecks: >\n  *,\n  -modernize-use-trailing-return-type,\n  -llvmlibc-callee-namespace,\n  -fuchsia-default-arguments-calls,\n  -cppcoreguidelines-pro-bounds-pointer-arithmetic,"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.1708984375,
          "content": "\n.vscode/\n\n.pytest_cache/\n\n.idea/\n\nengine/cmake-build-debug/\ncmake-*\n\nalgebra.log\n\nengine/Doxyfile\n\n# BEGIN gitignore from e2e repo\n\n.idea/\n\n# Compiled class file\n*.class\n# Python bytecode\n*.pyc\n__pycache__/\n\n# Log file\n*.log\n*.xlsx\nrmm_log.txt\n\n# Tmp files\n*.swp\n\n# BlueJ files\n*.ctxt\n\n# Mobile Tools for Java (J2ME)\n.mtj.tmp/\n\n# Package Files #\n*.jar\n*.war\n*.nar\n*.ear\n*.zip\n*.tar.gz\n*.rar\n\n# virtual machine crash logs, see http://www.java.com/en/download/help/error_hotspot.xml\nhs_err_pid*\n\nconfigurationFile.json\n\n# ignore locks\n*.lock\n*.dirlock\n\n*.log.old\n\n# ignore conf files generated by kerberized hdfs docker-compose\n*krb5cc_0*\n*krb5.conf*\n*data-swap*\n\n# ignore dask worker temp files\n*worker-*\n*dask-worker-space\n*.pem\n*.properties\n\n# ignore logs\n*logs*\n\n# END gitignore from e2e repo\n\nthirdparty/aws-cpp\nthirdparty/rapids/\nthirdparty/cudf/\n\n.condarc\n.conda/\ncore\n\n# stuff from our docker utils\n.cache/\n.config/\n.cupy/\n.jitify-cache/\n.nv/\n\n# powerpc\npowerpc/tmp/\npowerpc/blazingsql.tar.gz\npowerpc/developer/requirements.txt\npowerpc/developer/core\npowerpc/developer/blazingsql.tar.gz\n\n# mac junk\n.DS_Store\n._.*\n\n# docs build folders\ndocsrc/build/\ndocsrc/source/doxyfiles/\ndocsrc/source/xml"
        },
        {
          "name": ".nojekyll",
          "type": "blob",
          "size": 0.0009765625,
          "content": "\n"
        },
        {
          "name": "CHANGELOG.md",
          "type": "blob",
          "size": 20.2294921875,
          "content": "# BlazingSQL 21.08.00 (August 12th, 2021)\n\n## Improvements\n- #1571 Update ucx-py versions to 0.21\n- #1554 return ok for filesystems\n- #1572 Setting up default value for max_bytes_chunk_read to 256 MB\n\n## Bug Fixes\n- #1570 Fix build due to changes in rmm device buffer\n- #1574 Fix reading decimal columns from orc file\n- #1576 Fix `CC`/`CXX` variables in CI\n- #1581 Fix latest cudf dependencies\n- #1582 Fix concat suite E2E test for nested calls\n- #1585 Fix for GCS credentials from filepath\n- #1589 Fix decimal support using float64\n- #1590 Fix build issue with thrust package\n- #1595 Fix `spdlog` pinning\n- #1597 Fix `google-cloud-cpp` pinning\n\n# BlazingSQL 21.06.00 (June 10th, 2021)\n\n## New Features\n- #1471 Unbounded partitioned windows\n- #1445 Support for CURRENT_DATE, CURRENT_TIME and CURRENT_TIMESTAMP\n- #1505 Support for right outer join\n- #1523 Support for DURATION type\n- #1552 Support for concurrency in E2E tests\n\n## Improvements\n- #1464 Better Support for unsigned types in C++ side\n- #1511 Folder refactoring related to caches, kernels, execution_graph, BlazingTable\n- #1522 Improve data loading when the algebra contains only BindableScan/Scan and Limit\n- #1524 Enable support for spdlog 1.8.5\n- #1547 Update RAPIDS version references\n- #1539 Support ORDERing by null values\n- #1551 Support for spdlog 1.8.5\n- #1553 multiple columns inside COUNT() statement\n\n## Bug Fixes\n- #1455 Support for IS NOT FALSE condition\n- #1502 Fix IS NOT DISTINCT FROM with joins\n- #1475 Fix wrong results from timestampdiff/add\n- #1528 Fixed build issues due to cudf aggregation API change\n- #1540 Comparing param set to true for e2e\n- #1543 Enables provider unit_tests\n- #1548 Fix orc statistic building\n- #1550 Fix Decimal/Fixed Point issue\n- #1519 Fix for max_bytes_chunk_read param to csv files\n- #1559 Fix `ucx-py` versioning specs\n- #1557 Reading chunks of max bytes for csv files\n\n\n# BlazingSQL 0.19.0 (April 21, 2021)\n\n## New Features\n- #1367 OverlapAccumulator Kernel\n- #1364 Implement the concurrent API (bc.sql with token, bc.status, bc.fetch)\n- #1426 Window Functions without partitioning\n- #1349 Add e2e test for Hive Partitioned Data\n- #1396 Create tables from other RDBMS\n- #1427 Support for CONCAT alias operator\n- #1424 Add get physical plan with explain\n- #1472 Implement predicate pushdown for data providers\n\n## Improvements\n- #1325 Refactored CacheMachine.h and CacheMachine.cpp\n- #1322 Updated and enabled several E2E tests\n- #1333 Fixing build due to cudf update\n- #1344 Removed GPUCacheDataMetadata class\n- #1376 Fixing build due to some strings refactor in cudf, undoing the replace workaround\n- #1430 Updating GCP to >= version\n- #1331 Added flag to enable null e2e testing\n- #1418 Adding support for docker image\n- #1434 Added documentation for C++ and Python in Sphinx\n- #1419 Added concat cache machine timeout\n- #1444 Updating GCP to >= version\n- #1349 Add e2e test for Hive Partitioned Data\n- #1447 Improve getting estimated output num rows\n- #1473 Added Warning to Window Functions\n- #1482 Improve test script for blazingsql-testing-file\n- #1480 Improve dependencies script\n- #1433 Adding ArrowCacheData, refactoring CacheData files\n\n## Bug Fixes\n- #1335 Fixing uninitialized var in orc metadata and handling the parseMetadata exceptions properly\n- #1339 Handling properly the nulls in case conditions with strings\n- #1346 Delete allocated host chunks\n- #1348 Capturing error messages due to exceptions properly\n- #1350 Fixed bug where there are no projects in a bindable table scan\n- #1359 Avoid cuda issues when free pinned memory\n- #1365 Fixed build after sublibs changes on cudf\n- #1369 Updated java path for powerpc build\n- #1371 Fixed e2e settings\n- #1372 Recompute `columns_to_hash` in DistributeAggregationKernel\n- #1375 Fix empty row_group_ids for parquet\n- #1380 Fixed issue with int64 literal values\n- #1379 Remove ProjectRemoveRule\n- #1389 Fix issue when CAST a literal\n- #1387 Skip getting orc metadata for decimal type\n- #1392 Fix substrings with nulls\n- #1398 Fix performance regression\n- #1401 Fix support for minus unary operation\n- #1415 Fixed bug where num_batches was not getting set in BindableTableScan\n- #1413 Fix for null tests 13 and 23 of windowFunctionTest\n- #1416 Fix full join when both tables contains nulls\n- #1423 Fix temporary directory for hive partition test\n- #1351 Fixed 'count distinct' related issues\n- #1425 Fix for new joins API\n- #1400 Fix for Column aliases when exists a Join op\n- #1456 Raising exceptions on Python side for RAL\n- #1466 SQL providers: update README.md\n- #1470 Fix pre compiler flags for sql parsers\n- #1504 Fixing some conflicts in Dockerfile\n\n## Deprecated Features\n- #1394 Disabled support for outer joins with inequalities\n\n# BlazingSQL 0.18.0 (February 24, 2021)\n\n## New Features\n- #1139 Adding centralized task executor for kernels\n- #1200 Implement string REGEXP_REPLACE\n- #1237 Added task memory management\n- #1244 Added memory monitor ability to downgrade task data\n- #1232 Update PartwiseJoin and JoinPartition kernel using the task executor internally\n- #1238 Implements MergeStramKernel executor model\n- #1259 Implements SortAndSamplernel executor model, also avoid setting up num of samples\n- #1271 Added Hive utility for partitioned data\n- #1289 Multiple concurrent query support\n- #1285 Infer PROTOCOL when Dask client is passed\n- #1294 Add config options for logger\n- #1301 Added usage of pinned buffers for communication and fixes various UCX related bugs\n- #1298 Implement progress bar for run query (using tqdm)\n- #1284 Initial support for Windows Function\n- #1303 Add support for INITCAP\n- #1313 getting and using ORC metadata\n- #1347 Fixing issue when reading orc metadata from DATE dtype\n- #1338 Window Function support for LEAD and LAG statements\n- #1362 give useful message when file extension is not recognized\n- #1361 Supporting first_value and last_value for Window Function\n\n\n## Improvements\n- #1293 Added optional acknowledgments to message sending\n- #1236 Moving code from header files to implementation files\n- #1257 Expose the reset max memory usage C++ API to python\n- #1256 Improve Logical project documentation\n- #1262 Stop depending on gtest for runtime\n- #1261 Improve storage plugin output messages\n- #1153 Enable warnings and fixes\n- #1267 Added retrys to comms, fixed deadlocks in executor and order by. Improved logging and error management. Caches have names. Improved Joins\n- #1239 Reducing Memory pressure by moving shuffle data to cpu before transmission\n- #1278 Fix race conditions with UCX\n- #1279 Added cuml to powerpc build scripts\n- #1286 Fixes to initialization and adding unique ids to comms\n- #1255 Kernels are resilient to out of memory errors now and can retry tasks that fail this way\n- #1311 Add queries logger to physical plan\n- #1308 Improve the engine loggers\n- #1314 Added unit tests to verify that OOM error handling works well\n- #1320 Revamping cache logger\n- #1323 Made progress bar update continuously and stay after query is done\n- #1336 Improvements for the cache API\n- #1483 Improve dependencies script\n\n## Bug Fixes\n- #1249 Fix compilation with cuda 11\n- #1253 Fixed distribution so that its evenly distributes based of rowgroups\n- #1204 Reenable json parser\n- #1241 Fixed cython exception handling\n- #1243 Fixed wrong CHAR regex replacing\n- #1275 Fixed issue in StringUtil::findAndReplaceAll when there are several matches\n- #1277 Support FileSystems (GS, S3) when extension of the files are not provided\n- #1300 Fixed issue when creating tables from a local dir relative path\n- #1312 Fix progress bar for jupyterlab\n- #1318 Disabled require acknowledge\n\n# BlazingSQL 0.17.0 (December 10, 2020)\n\n## New Features\n- #1105 Implement to_date/to_timestamp functions\n- #1077 Allow to create tables from compressed files\n- #1126 Add DAYOFWEEK function\n- #981 Added powerPC building script and instructions\n- #912 Added UCX support to how the engine runs\n- #1125 Implement new TCP and UCX comms layer, exposed graph to python\n- #1122 Add ConfigOptionsTest, a test with different config_options values\n- #1110 Adding local logging directory to BlazingContext\n- #1148 Add e2e test for DAYOFWEEK\n- #1130 Infer hive folder partition\n- #1188 Implement upper/lower operators\n- #1193 Implement string REPLACE\n- #1218 Added smiles test set\n- #1201 Implement string TRIM\n- #1216 Add unit test for DAYOFWEEK\n- #1205 Implement string REVERSE\n- #1220 Implement string LEFT and RIGHT\n- #1223 Add support for UNION statement\n- #1250 updated README.md and CHANGELOG and others preparing for 0.17 release\n\n\n## Improvements\n- #878 Adding calcite rule for window functions. (Window functions not supported yet)\n- #1081 Add validation for the kwargs when bc API is called\n- #1082 Validate s3 bucket\n- #1093 Logs configurable to have max size and be rotated\n- #1091 Improves the error message problem when validating any GCP bucket\n- #1102 Add option to read csv files in chunks\n- #1090 Add tests for Uri Data provider for local uri\n- #1119 Add tests for transform json tree and get json plan\n- #1117 Add error logging in DataSourceSequence\n- #1111 output compile json for cppcheck\n- #1132 Refactoring new comms\n- #1078 Bump junit from 4.12 to 4.13.1 in /algebra\n- #1144 update with changes from main\n- #1156 Added scheduler file support for e2e testing framework\n- #1158 Deprecated bc.partition\n- #1154 Recompute the avg_bytes_per_row value\n- #1155 Removing comms subproject and cleaning some related code\n- #1170 Improve gpuCI scripts\n- #1194 Powerpc building scripts\n- #1186 Removing cuda labels to install due cudatoolkit version\n- #1187 Enable MySQL-specific SQL operators in addition to Standard and Oracle\n- #1206 Improved contribution documentation\n- #1224 Added cudaSetDevice to thread initialization so that the cuda context is available to UCX\n- #1229 Change hardcoded version from setup.py\n- #1231 Adding docker support for gpuCI scripts\n- #1248 Jenkins and Docker scripts were improved for building\n\n\n## Bug Fixes\n- #1064 Fixed issue when loading parquet files with local_files=True\n- #1086 Showing an appropriate error to indicate that we don't support opening directories with wildcards\n- #1088 Fixed issue caused by cudf changing from one .so file to multiple\n- #1094 Fixed logging directory setup\n- #1100 Showing an appropriate error for invalid or unsupported expressions on the logical plan\n- #1115 Fixed changes to RMM api using cuda_stream_view instead of cudaStream_t now\n- #1120 Fix missing valid kwargs in create_table\n- #1118 Fixed issue with config_options and adding local_files to valid params\n- #1133 Fixed adressing issue in float columns when parsing parquet metadata\n- #1163 added empty line to trigger build\n- #1108 Remove temp files when an error occurs\n- #1165 E2e tests, distributed mode, again tcp\n- #1171 Don't log timeout in output/input caches\n- #1168 Fix SSL errors for conda\n- #1164 MergeAggr when single node has multiple batches\n- #1191 Fix graph thread pool hang when exception is thrown\n- #1181 Remove unnecesary prints (cluster and logging info)\n- #1185 Create table in distributed mode crash with a InferFolderPartitionMetadata Error\n- #1179 Fix ignore headers when multiple CSV files was provided\n- #1199 Fix non thread-safe access to map containing tag to message_metadata for ucx\n- #1196 Fix column_names (table) always as list of string\n- #1203 Changed code back so that parquet is not read a single rowgroup at a time\n- #1207 Calcite uses literal as int32 if not explicit CAST was provided\n- #1212 Fixed issue when building the thirdpart, cmake version set to 3.18.4\n- #1225 Fixed issue due to change in gather API\n- #1254 Fixing support of nightly and stable on localhost\n- #1258 Fixing gtest version issue\n\n\n# BlazingSQL 0.16.0 (October 22, 2020)\n\n## Improvements\n- #997 Add capacity to set the transport memory\n- #1040 Update conda recipe, remove cxx11 abi from cmake\n- #977 Just one initialize() function at beginning and add logs related to allocation stuff\n- #1046 Make possible to read the system environment variables to set up BlazingContext\n- #998 Update TPCH queries, become implicit joins into implicit joins to avoid random values.\n- #1055 Removing cudf source code dependency as some cudf utilities headers were exposed\n- #1065 Remove thrift from build prodcess as its no longer used\n- #1067 Upload conda packages to both rapidsai and blazingsql conda channels\n\n\n## Bug Fixes\n- #918 Activate validation for GPU_CI tests results.\n- #975 Fixed issue due to cudf orc api change\n- #1017 Fixed issue parsing fixed with string literals\n- #1019 Fix hive string col\n- #1021 removed an rmm include\n- #1020 Fixed build issues with latest rmm 0.16 and columnBasisTest due to deprecated drop_column() function\n- #1029 Fix metadata mistmatch due to parsedMetadata\n- #1016 Removed workaround for parquet read schema\n- #1022 Fix pinned buffer pool\n- #1028 Match dtypes after create_table with multiple files\n- #1030 Avoid read _metadata files\n- #1039 Fixed issues with parsers, in particular ORC parser was misbehaving\n- #1038 Fixed issue with logging dirs in distributed envs\n- #1048 Pinned google cloud version to 1.16\n- #1052 Partial revert of some changes on parquet rowgroups flow with local_files=True\n- #1054 Can set manually BLAZING_CHACHE_DIRECTORY\n- #1053 Fixed issue when loading paths with wildcards\n- #1057 Fixed issue with concat all in concatenating cache\n- #1007 Fix arrow and spdlog compilation issues\n- #1068 Just adds a docs important links and avoid the message about filesystem authority not found\n- #1073 Fixed parseSchemaPython can throw exceptions\n- #1074 Remove lock inside grow() method from PinnedBufferProvider\n- #1071 Fix crash when loading an empty folder\n- #1085 Fixed intra-query memory leak in joins. Fixed by clearing array caches after PartwiseJoin is done\n- #1096 Backport from branch-0.17 with these PRs: #1094, #1086, #1093 and #1091\n- #1099 Fixed issue with config_options\n\n\n# BlazingSQL 0.15.0 (August 31, 2020)\n\n## New Features\n- #835 Added a memory monitor for better memory management and added pull ordered from cache\n- #889 Added Sphinx based code architecture documentation\n- #968 Support PowerPC architecture\n\n## Improvements\n- #777 Update Calcite to the most recent version 1.23\n- #786 Added check for concat String overflow\n- #815 Implemented Unordered pull from cache to help performance\n- #822 remove \"from_cudf\" code and cudf test utilities from engine code\n- #824 Added a test on Calcite to compare the logical plans when the ruleset is updated\n- #802 Support for timestampadd and constant expressions evaluation by Calcite\n- #849 Added check for CUDF_HOME to allow build to use an existing prebuilt cudf source tree\n- #829 Python/Cython check code style\n- #826 Support cross join\n- #866 Added nogil statements for pure C functions in Cython\n- #784 Updated set of TPCH queries on the E2E tests\n- #877 round robing dask workers on single gpu queries\n- #880 reraising query errors in context.py\n- #883 add rand() and running unary operations on literals\n- #894 added exhale to generate doxygen for sphinx docs\n- #887 concatenating cache improvement and replacing PartwiseJoin::load_set with a concatenating cache\n- #885 Added initial set of unit tests for `WaitingQueue` and nullptr checks around spdlog calls\n- #904 Added doxygen comments to CacheMachine.h\n- #901 Added more documentation about memory management\n- #910 updated readme\n- #915 Adding max kernel num threads pool\n- #921 Make AWS and GCS optional\n- #925 Replace random_generator with cudf::sample\n- #900 Added doxygen comments to some kernels and the batch processing\n- #936 Adding extern C for include files\n- #941 Logging level (flush_on) can be configurable\n- #947 Use default client and network interface from Dask\n- #945 Added new separate thresh for concat cache\n- #939 Add unit test for Project kernel\n- #949 Implemented using threadpool for outgoing messages\n- #961 Add list_tables() and describe_table() functions\n- #967 Add bc.get_free_memory() function\n\n## Bug Fixes\n- #774 fixed build issues with latest cudf 0.15 including updating from_cudf\n- #781 Fixed issue with Hive partitions when doing SELECT *\n- #754 Normalize columns before distribution in JoinPartitionKernel\n- #782 fixed issue with hive partitions base folder\n- #791 Fixes issues due to changes in rmm and fixes allocator issues\n- #770 Fix interops operators output types\n- #798 Fix when the algebra plan was provided using one-line as logical plan\n- #799 Fix uri values computacion in runQueryCaller\n- #792 Remove orc temp files when cached on Disk\n- #814 Fix when checking only Limit and Scan Kernels\n- #816 Loading one file at a time (LimitKernel and ScanKernel)\n- #832 updated calcite test reference\n- #834 Fixed small issue with hive and cudf_type_int_to_np_types\n- #839 Fixes literal cast\n- #838 Fixed issue with start and length of substring being different types\n- #823 Fixed issue on logical plans when there is an EXISTS clause\n- #845 Fixed issue with casting string to string\n- #850 Fixed issue with getTableScanInfoCaller\n- #851 Fix row_groups issue in ParquetParser.cpp\n- #847 Fixed issue with some constant expressions not evaluated by calcite\n- #875 Recovered some old unit tests and deleted obsolete unit tests\n- #879 Fixed issue with log directory creation in a distributed environment\n- #890 Fixed issue where we were including testing hpp in our code\n- #891 Fixed issue caused by replacing join load_set with concatenating cache\n- #902 Fixed optimization regression on the select count(*) case\n- #909 Fixed issue caused by using now arrow_io_source\n- #913 Fixed issues caused by cudf adding DECIMAL data type\n- #916 Fix e2e string comparison\n- #927 Fixed random segfault issue in parser\n- #929 Update the GPUManager functions\n- #942 Fix column names on sample function\n- #950 Introducing config param for max orderby samples and fixing oversampling\n- #952 Dummy PR\n- #957 Fixed issues caused by changes to timespamp in cudf\n- #962 Use new rmm API instead of get_device_resource() and set_device_resource() functions\n- #965 Handle exceptions from pool_threads\n- #963 Set log_level when using LOGGING_LEVEL param\n- #973 Fix how we check the existence of the JAVA_HOME environment variable\n\n# BlazingSQL 0.14.0 (June 9, 2020)\n\n- #391 Added the ability to run count distinct queries in a distruted fashion\n- #392 Remove the unnecessary messages on distributed mode\n- #560 Fixed bug where parsing errors would lead to crash\n- #565 made us have same behaviour as cudf for reading csv\n- #612 Print product version: print(blazingsql.__version__) # shows the git hash\n- #638 Refactores and fixes SortAndSample kernels\n- #631 Implemented ability to send config_options to bc.sql function\n- #621 Clean dead code\n- #602 Implements cache flow control feature\n- #625 Implement CAST to TINYINT and SMALLINT\n- #632 Implement CHAR_LENGTH function\n- #635 Handle behavior when the optimized plan contains a LogicalValues\n- #653 Handle exceptions on python side\n- #661 added hive support to parse_batch\n- #662 updated from_cudf code and fixed other issue due to new cudf::list_view\n- #674 Allow to define and use a specific AWS S3 region\n- #677 added guava to pom.xml\n- #679 Support modern compilers (>= g++-7.x)\n- #649 Adding event logging\n- #660 Changed how we handle the partitions of a dask.cudf.DataFrame\n- #697 Update expression parser\n- #659 Improve reading for: SELECT * FROM table LIMIT N\n- #700 Support null column in projection\n- #711 Migrate end to end tests into blazingsql repo\n- #718 Changed all condition variable waits to wait_for\n- #712 fixed how we handle empty tables for estimate for small table join\n- #724 Removed unused BlazingThread creations\n- #725 Added nullptr check to num_rows()\n- #729 Fixed issue with num_rows() and wait_for\n- #728 Add replace_calcite_regex function to the join condition\n- #721 Handling multi-partition output\n- #750 Each table scan now has its own data loader\n- #740 Normalizing types for UNION ALL\n- #744 Fix unit tests\n- #743 Workaround for interops 64 index plan limitation\n- #763 Implemented ability to set the folder for all log files\n- #757 Ensure GPU portability (so we can run on any cloud instance with GPU)\n- #753 Fix for host memory threshold parameter with Dask envs\n- #801 Fix build with new cudf 0.15 and arrow 0.17.1\n- #809 Fix conda build issues\n- #828 Fix gpuci issues and improve tooling to debug gpuci related issues\n- #867 Fix boost dependencie issues\n- #785 Add script for Manual Testing Artifacts.\n- #931 Add script for error messages validation.\n- #932 Import pydrill and pyspark only when its generator or full mode.\n- #1031 adding notebooks into BlazingSQL Tests\n- #1486 Define generic templates for E2E Testing framework.\n- #1542 Cleaning code on E2E Test framework.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 4.0009765625,
          "content": "# Contributing to blazingSQL\n\nContributions to blazingSQL fall into the following three categories.\n\n1. To report a bug, request a new feature, or report a problem with\n    documentation, please file an [issue](https://github.com/blazingdb/blazingsql/issues/new/choose)\n    describing in detail the problem or new feature. The BlazingSQL team evaluates \n    and triages issues, and schedules them for a release. If you believe the \n    issue needs priority attention, please comment on the issue to notify the \n    team.\n2. To propose and implement a new Feature, please file a new feature request \n    [issue](https://github.com/blazingdb/blazingsql/issues/new/choose). Describe the \n    intended feature and discuss the design and implementation with the team and\n    community. Once the team agrees that the plan looks good, go ahead and \n    implement it, using the [code contributions](#code-contributions) guide below.\n3. To implement a feature or bug-fix for an existing outstanding issue, please \n    Follow the [code contributions](#code-contributions) guide below. If you \n    need more context on a particular issue, please ask in a comment.\n\n## Code contributions\n\n1. Follow the guide in our documentation for [Building From Source](https://github.com/BlazingDB/blazingsql#buildinstall-from-source-conda-environment).\n2. Find an issue to work on (that already has not been asigned to someone). The best way is to look for the good first issue or help wanted labels.\n3. Comment on the issue stating that you are going to work on it and assign it to yourself.\n4. When you start working on it, please place the issue on the WIP column in the project board.\n5. All work should be done on your own fork and on a new branch on your fork.\n6. Code! Make sure to update unit tests!\n7. If applicable (i.e.when adding a new SQL function), add new [End-To-End tests](#adding-end-to-end-tests).\n8. When done, [create your pull request](https://github.com/blazingdb/blazingsql/compare).\n9. Verify that CI passes all [status checks](https://help.github.com/articles/about-status-checks/). Fix if needed.\n10. When all the work is done, please place the issue in the _Needs Review_ column of the project board. Wait for other developers to review your code and update code as needed.\n11. Once reviewed and approved, a BlazingSQL developer will merge your pull request.\n\nRemember, if you are unsure about anything, don't hesitate to comment on issues\nand ask for clarifications!\n\n## Adding End to End Tests\n\nDependencies and instructions for how to run the End to End testing framework can be found [here](tests/README.md).\n\nTo add a new End to End test queries, please do the follwing:\n- If the new query is related to an existing script, you can add it **at the end** of the existing script:\n*$CONDA_PREFIX/blazingsql/test/BlazingSQLTest/EndToEndTests/existingScript.py*\n- In case that the new query is part of a non-existent test suite, create a new script and add the new tests in the new script:\n*$CONDA_PREFIX/blazingsql/test/BlazingSQLTest/EndToEndTests/newScript.py*\n- If you added a new script, you have to add this also in `allE2ETest.py`\n- After you add the new queries, you will want to run the testing framework in generator mode, so you will need to following environment variable:\n`export BLAZINGSQL_E2E_EXEC_MODE=\"generator\"`\n- You will also need to have an instance of Apache Drill running. You need to install apache-drill and run it like so:\n`apache-drill-1.17.0/bin/drill-embedded`\n- Enter to $CONDA_PREFIX/blazingsql and execute: \n`./test.sh e2e tests=<new_suiteTest>`\n- New query validation files will be generated in the repo located at: \n`$CONDA_PREFIX/blazingsql-testing-files/`\n- Please create a PR in the `blazingsql-testing-files` repo and reference the PR in the BlazingSQL repo that the new end to end tests correspond to.\n- Once the `blazingsql-testing-files` PR is merged, then you can run the GPU_CI tests in the `blazingsql` PR. \n- Make sure that all the GPU_CI tests are passing.\n\n\n## Attribution\nPortions adopted from https://github.com/rapidsai/cudf/CONTRIBUTING.md\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 2.6533203125,
          "content": "ARG CUDA_VER=\"10.2\"\nARG UBUNTU_VERSION=\"16.04\"\nFROM nvidia/cuda:${CUDA_VER}-runtime-ubuntu${UBUNTU_VERSION}\nLABEL Description=\"blazingdb/blazingsql is the official BlazingDB environment for BlazingSQL on NIVIDA RAPIDS.\" Vendor=\"BlazingSQL\" Version=\"0.4.0\"\n\nARG CUDA_VER=10.2\nARG CONDA_CH=\"-c blazingsql -c rapidsai -c nvidia\"\nARG PYTHON_VERSION=\"3.7\"\nARG RAPIDS_VERSION=\"0.18\"\n\nSHELL [\"/bin/bash\", \"-c\"]\nENV PYTHONDONTWRITEBYTECODE=true\n\nRUN apt-get update -qq && \\\n    apt-get install curl git -yqq --no-install-recommends && \\\n    apt-get clean -y && \\\n    curl -s -o /tmp/miniconda.sh https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh && \\\n    bash /tmp/miniconda.sh -bfp /usr/local/ && \\\n    rm -rf /tmp/miniconda.sh && \\\n    conda create python=${PYTHON_VERSION} -y -n bsql && \\\n    conda install -y -n bsql \\\n    ${CONDA_CH} \\\n    -c conda-forge -c defaults \\\n    cugraph=${RAPIDS_VERSION} cuml=${RAPIDS_VERSION} \\\n    cusignal=${RAPIDS_VERSION} \\\n    cuspatial=${RAPIDS_VERSION} \\\n    cuxfilter clx=${RAPIDS_VERSION} \\\n    python=${PYTHON_VERSION} cudatoolkit=${CUDA_VER} \\\n    blazingsql=${RAPIDS_VERSION} \\\n    jupyterlab \\\n    networkx statsmodels xgboost \\\n    geoviews seaborn matplotlib holoviews colorcet && \\\n    conda clean -afy && \\\n    rm -rf /var/cache/apt /var/lib/apt/lists/* /tmp/miniconda.sh /usr/local/pkgs/* && \\\n    rm -rf /usr/local/envs/bsql/include && \\\n    rm -f /usr/local/envs/bsql/lib/libpython3.*m.so.1.0 && \\\n    find /usr/local/envs/bsql -name '__pycache__' -type d -exec rm -rf '{}' '+' && \\\n    find /usr/local/envs/bsql -follow -type f -name '*.pyc' -delete && \\\n    rm -rf /usr/local/envs/bsql/lib/libasan.so.5.0.0 \\\n    /usr/local/envs/bsql/lib/libtsan.so.0.0.0 \\\n    /usr/local/envs/bsql/lib/liblsan.so.0.0.0 \\\n    /usr/local/envs/bsql/lib/libubsan.so.1.0.0 \\\n    /usr/local/envs/bsql/bin/sqlite3 \\\n    /usr/local/envs/bsql/bin/openssl \\\n    /usr/local/envs/bsql/share/terminfo \\\n    /usr/local/envs/bsql/bin/postgres \\\n    /usr/local/envs/bsql/bin/pg_* \\\n    /usr/local/envs/bsql/man \\\n    /usr/local/envs/bsql/qml \\\n    /usr/local/envs/bsql/qsci \\\n    /usr/local/envs/bsql/mkspecs && \\\n    find /usr/local/envs/bsql/lib/python3.*/site-packages -name 'tests' -type d -exec rm -rf '{}' '+' && \\\n    find /usr/local/envs/bsql/lib/python3.*/site-packages -name '*.pyx' -delete && \\\n    find /usr/local/envs/bsql -name '*.c' -delete && \\\n  git clone --branch=master https://github.com/BlazingDB/Welcome_to_BlazingSQL_Notebooks /blazingsql && \\\n  rm -rf /blazingsql/.git && \\\n  mkdir /.local /.jupyter /.cupy && chmod 777 /.local /.jupyter /.cupy\n\nWORKDIR /blazingsql\nCOPY run_jupyter.sh /blazingsql\n\n# Jupyter\nEXPOSE 8888\nCMD [\"/blazingsql/run_jupyter.sh\"]\n\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0791015625,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright 2018 BlazingDB, Inc.\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 10.802734375,
          "content": "> A lightweight, GPU accelerated, SQL engine built on the [RAPIDS.ai](https://rapids.ai) ecosystem.\n\n<a href='https://app.blazingsql.com/jupyter/user-redirect/lab/workspaces/auto-b/tree/Welcome_to_BlazingSQL_Notebooks/welcome.ipynb'>Get Started on app.blazingsql.com</a>\n\n[Getting Started](#getting-started) | [Documentation](https://docs.blazingdb.com) | [Examples](#examples) | [Contributing](#contributing) | [License](LICENSE) | [Blog](https://blog.blazingdb.com) | [Try Now](https://app.blazingsql.com/jupyter/user-redirect/lab/workspaces/auto-b/tree/Welcome_to_BlazingSQL_Notebooks/welcome.ipynb)\n\nBlazingSQL is a GPU accelerated SQL engine built on top of the RAPIDS ecosystem. RAPIDS is based on the [Apache Arrow](http://arrow.apache.org) columnar memory format, and [cuDF](https://github.com/rapidsai/cudf) is a GPU DataFrame library for loading, joining, aggregating, filtering, and otherwise manipulating data.\n\nBlazingSQL is a SQL interface for cuDF, with various features to support large scale data science workflows and enterprise datasets.\n* **Query Data Stored Externally** - a single line of code can register remote storage solutions, such as Amazon S3.\n* **Simple SQL** - incredibly easy to use, run a SQL query and the results are GPU DataFrames (GDFs).\n* **Interoperable** - GDFs are immediately accessible to any [RAPIDS](htts://github.com/rapidsai) library for data science workloads.\n\nTry our 5-min [Welcome Notebook](https://app.blazingsql.com/jupyter/user-redirect/lab/workspaces/auto-b/tree/Welcome_to_BlazingSQL_Notebooks/welcome.ipynb) to start using BlazingSQL and RAPIDS AI.\n\n# Getting Started\n\nHere's two copy + paste reproducable BlazingSQL snippets, keep scrolling to find [example Notebooks](#examples) below.\n\nCreate and query a table from a `cudf.DataFrame` with progress bar:\n\n```python\nimport cudf\n\ndf = cudf.DataFrame()\n\ndf['key'] = ['a', 'b', 'c', 'd', 'e']\ndf['val'] = [7.6, 2.9, 7.1, 1.6, 2.2]\n\nfrom blazingsql import BlazingContext\nbc = BlazingContext(enable_progress_bar=True)\n\nbc.create_table('game_1', df)\n\nbc.sql('SELECT * FROM game_1 WHERE val > 4') # the query progress will be shown\n```\n\n| | Key | Value |\n| - | -:| ---:|\n| 0 | a | 7.6 |\n| 1 | b | 7.1 |\n\nCreate and query a table from a AWS S3 bucket:\n\n```python\nfrom blazingsql import BlazingContext\nbc = BlazingContext()\n\nbc.s3('blazingsql-colab', bucket_name='blazingsql-colab')\n\nbc.create_table('taxi', 's3://blazingsql-colab/yellow_taxi/taxi_data.parquet')\n\nbc.sql('SELECT passenger_count, trip_distance FROM taxi LIMIT 2')\n```\n\n| | passenger_count | fare_amount |\n| - | -:| ---:|\n| 0 | 1.0 | 1.1 |\n| 1 | 1.0 | 0.7 |\n\n## Examples\n\n| Notebook Title | Description | Try Now |\n| -------------- | ----------- | ------- |\n| Welcome Notebook | An introduction to BlazingSQL Notebooks and the GPU Data Science Ecosystem. | <a href='https://app.blazingsql.com/jupyter/user-redirect/lab/workspaces/auto-b/tree/Welcome_to_BlazingSQL_Notebooks/welcome.ipynb'><img src=\"https://blazingsql.com/launch-notebooks.png\" alt=\"Launch on BlazingSQL Notebooks\" width=\"500\"/></a> |\n| The DataFrame | Learn how to use BlazingSQL and cuDF to create GPU DataFrames with SQL and Pandas-like APIs. | <a href='https://app.blazingsql.com/jupyter/user-redirect/lab/workspaces/auto-b/tree/Welcome_to_BlazingSQL_Notebooks/intro_notebooks/the_dataframe.ipynb'><img src=\"https://blazingsql.com/launch-notebooks.png\" alt=\"Launch on BlazingSQL Notebooks\" width=\"500\"/></a> |\n| Data Visualization | Plug in your favorite Python visualization packages, or use GPU accelerated visualization tools to render millions of rows in a flash. | <a href='https://app.blazingsql.com/jupyter/user-redirect/lab/workspaces/auto-b/tree/Welcome_to_BlazingSQL_Notebooks/intro_notebooks/data_visualization.ipynb'><img src=\"https://blazingsql.com/launch-notebooks.png\" alt=\"Launch on BlazingSQL Notebooks\" width=\"500\"/></a> |\n| Machine Learning | Learn about cuML, mirrored after the Scikit-Learn API, it offers GPU accelerated machine learning on GPU DataFrames. | <a href='https://app.blazingsql.com/jupyter/user-redirect/lab/workspaces/auto-b/tree/Welcome_to_BlazingSQL_Notebooks/intro_notebooks/machine_learning.ipynb'><img src=\"https://blazingsql.com/launch-notebooks.png\" alt=\"Launch on BlazingSQL Notebooks\" width=\"500\"/></a> |\n\n## Documentation\nYou can find our full documentation at [docs.blazingdb.com](https://docs.blazingdb.com/docs).\n\n# Prerequisites\n* [Anaconda or Miniconda](https://docs.conda.io/projects/conda/en/latest/user-guide/install/linux.html) installed\n* OS Support\n  * Ubuntu 16.04/18.04 LTS\n  * CentOS 7\n* GPU Support\n  * Pascal or Better\n  * Compute Capability >= 6.0\n* CUDA Support\n  * 11.0\n  * 11.2\n  * 11.4\n* Python Support\n  * 3.7\n  * 3.8\n# Install Using Conda\nBlazingSQL can be installed with conda ([miniconda](https://conda.io/miniconda.html), or the full [Anaconda distribution](https://www.anaconda.com/download)) from the [blazingsql](https://anaconda.org/blazingsql/) channel:\n\n## Stable Version\n```bash\nconda install -c blazingsql -c rapidsai -c nvidia -c conda-forge -c defaults blazingsql python=$PYTHON_VERSION cudatoolkit=$CUDA_VERSION\n```\nWhere $CUDA_VERSION is 11.0, 11.2 or 11.4  and $PYTHON_VERSION is 3.7 or 3.8\n*For example for CUDA 11.2 and Python 3.8:*\n```bash\nconda install -c blazingsql -c rapidsai -c nvidia -c conda-forge -c defaults blazingsql python=3.8 cudatoolkit=11.2\n```\n\n## Nightly Version\nFor nightly version cuda 11+ are only supported, see https://github.com/rapidsai/cudf#cudagpu-requirements\n```bash\nconda install -c blazingsql-nightly -c rapidsai-nightly -c nvidia -c conda-forge -c defaults blazingsql python=$PYTHON_VERSION  cudatoolkit=$CUDA_VERSION\n```\nWhere $CUDA_VERSION is 11.0, 11.2 or 11.4 and $PYTHON_VERSION is 3.7 or 3.8\n*For example for CUDA 11.2 and Python 3.8:*\n```bash\nconda install -c blazingsql-nightly -c rapidsai-nightly -c nvidia -c conda-forge -c defaults blazingsql python=3.8  cudatoolkit=11.2\n```\n\n# Build/Install from Source (Conda Environment)\nThis is the recommended way of building all of the BlazingSQL components and dependencies from source. It ensures that all the dependencies are available to the build process.\n\n## Stable Version\n\n### Install build dependencies\n```bash\nconda create -n bsql python=$PYTHON_VERSION\nconda activate bsql\n./dependencies.sh 21.08 $CUDA_VERSION\n```\nWhere $CUDA_VERSION is is 11.0, 11.2 or 11.4 and $PYTHON_VERSION is 3.7 or 3.8\n*For example for CUDA 11.2 and Python 3.7:*\n```bash\nconda create -n bsql python=3.7\nconda activate bsql\n./dependencies.sh 21.08 11.2\n```\n\n### Build\nThe build process will checkout the BlazingSQL repository and will build and install into the conda environment.\n\n```bash\ncd $CONDA_PREFIX\ngit clone https://github.com/BlazingDB/blazingsql.git\ncd blazingsql\ngit checkout main\nexport CUDACXX=/usr/local/cuda/bin/nvcc\n./build.sh\n```\nNOTE: You can do `./build.sh -h` to see more build options.\n\n$CONDA_PREFIX now has a folder for the blazingsql repository.\n\n## Nightly Version\n\n### Install build dependencies\nFor nightly version cuda 11+ are only supported, see https://github.com/rapidsai/cudf#cudagpu-requirements\n```bash\nconda create -n bsql python=$PYTHON_VERSION\nconda activate bsql\n./dependencies.sh 21.10 $CUDA_VERSION nightly\n```\nWhere $CUDA_VERSION is 11.0, 11.2 or 11.4 and $PYTHON_VERSION is 3.7 or 3.8\n*For example for CUDA 11.2 and Python 3.8:*\n```bash\nconda create -n bsql python=3.8\nconda activate bsql\n./dependencies.sh 21.10 11.2 nightly\n```\n\n### Build\nThe build process will checkout the BlazingSQL repository and will build and install into the conda environment.\n\n```bash\ncd $CONDA_PREFIX\ngit clone https://github.com/BlazingDB/blazingsql.git\ncd blazingsql\nexport CUDACXX=/usr/local/cuda/bin/nvcc\n./build.sh\n```\nNOTE: You can do `./build.sh -h` to see more build options.\n\nNOTE: You can perform static analysis with cppcheck with the command `cppcheck  --project=compile_commands.json` in any of the cpp project build directories.\n\n$CONDA_PREFIX now has a folder for the blazingsql repository.\n\n#### Storage plugins\nTo build without the storage plugins (AWS S3, Google Cloud Storage) use the next arguments:\n```bash\n# Disable all storage plugins\n./build.sh disable-aws-s3 disable-google-gs\n\n# Disable AWS S3 storage plugin\n./build.sh disable-aws-s3\n\n# Disable Google Cloud Storage plugin\n./build.sh disable-google-gs\n```\nNOTE: By disabling the storage plugins you don't need to install previously AWS SDK C++ or Google Cloud Storage (neither any of its dependencies).\n\n#### SQL providers\nTo build without the SQL providers (MySQL, PostgreSQL, SQLite) use the next arguments:\n```bash\n# Disable all SQL providers\n./build.sh disable-mysql disable-sqlite disable-postgresql\n\n# Disable MySQL provider\n./build.sh disable-mysql\n\n...\n```\nNOTES:\n- By disabling the storage plugins you don't need to install mysql-connector-cpp=8.0.23 libpq=13 sqlite=3 (neither any of its dependencies).\n- Currenlty we support only MySQL. but PostgreSQL and SQLite will be ready for the next version!\n\n# Documentation\nUser guides and public APIs documentation can be found at [here](https://docs.blazingdb.com/docs)\n\nOur internal code architecture can be built using Spinx.\n```bash\nconda install -c conda-forge doxygen\ncd $CONDA_PREFIX\ncd blazingsql/docsrc\npip install -r requirements.txt\nmake doxygen\nmake html\n```\nThe generated documentation can be viewed in a browser at `blazingsql/docsrc/build/html/index.html`\n\n\n# Community\n## Contributing\nHave questions or feedback? Post a [new github issue](https://github.com/blazingdb/blazingsql/issues/new/choose).\n\nPlease see our [guide for contributing to BlazingSQL](CONTRIBUTING.md).\n\n## Contact\nFeel free to join our channel (#blazingsql) in the RAPIDS-GoAi Slack: [![join RAPIDS-GoAi workspace](https://badgen.net/badge/slack/RAPIDS-GoAi/purple?icon=slack)](https://join.slack.com/t/rapids-goai/shared_invite/enQtMjE0Njg5NDQ1MDQxLTJiN2FkNTFkYmQ2YjY1OGI4NTc5Y2NlODQ3ZDdiODEwYmRiNTFhMzNlNTU5ZWJhZjA3NTg4NDZkMThkNTkxMGQ).\n\nYou can also email us at [info@blazingsql.com](info@blazingsql.com) or find out more details on [BlazingSQL.com](https://blazingsql.com).\n\n## License\n[Apache License 2.0](LICENSE)\n\n## RAPIDS AI - Open GPU Data Science\n\nThe RAPIDS suite of open source software libraries aim to enable execution of end-to-end data science and analytics pipelines entirely on GPUs. It relies on NVIDIA CUDA primitives for low-level compute optimization, but exposing that GPU parallelism and high-bandwidth memory speed through user-friendly Python interfaces.\n\n## Apache Arrow on GPU\n\nThe GPU version of [Apache Arrow](https://arrow.apache.org/) is a common API that enables efficient interchange of tabular data between processes running on the GPU. End-to-end computation on the GPU avoids unnecessary copying and converting of data off the GPU, reducing compute time and cost for high-performance analytics common in artificial intelligence workloads. As the name implies, cuDF uses the Apache Arrow columnar data format on the GPU. Currently, a subset of the features in Apache Arrow are supported. \n\n"
        },
        {
          "name": "algebra",
          "type": "tree",
          "content": null
        },
        {
          "name": "build.sh",
          "type": "blob",
          "size": 10.94140625,
          "content": "#!/bin/bash\n\n# Copyright (c) 2019, NVIDIA CORPORATION.\n\n# BlazingSQL build script\n\n# This script is used to build the component(s) in this repo from\n# source, and can be called with various options to customize the\n# build as needed (see the help output for details)\n\n# Abort script on first error\nset -e\n\nNUMARGS=$#\nARGS=$*\n\n# NOTE: ensure all dir changes are relative to the location of this\n# script, and that this script resides in the repo dir!\nREPODIR=$(cd $(dirname $0); pwd)\n\nVALIDARGS=\"clean update thirdparty io libengine engine pyblazing algebra disable-aws-s3 disable-google-gs disable-mysql disable-sqlite disable-postgresql -t -v -g -n -h\"\nHELP=\"$0 [-v] [-g] [-n] [-h] [-t]\n   clean                - remove all existing build artifacts and configuration (start\n                          over) Use 'clean thirdparty' to delete thirdparty folder\n   update               - update cudf conda packages\n   thirdparty           - build the Thirdparty C++ code only\n   io                   - build the IO C++ code only\n   libengine            - build the engine C++ code only\n   engine               - build the engine Python package\n   pyblazing            - build the pyblazing Python package\n   algebra              - build the algebra Python package\n   disable-aws-s3       - flag to disable AWS S3 support for libengine\n   disable-google-gs    - flag to disable Google Cloud Storage support for libengine\n   disable-mysql        - flag to enable MySQL support for libengine\n   disable-sqlite       - flag to enable SQLite support for libengine\n   disable-postgresql   - flag to enable PostgreSQL support for libengine\n   -t                   - skip tests\n   -v                   - verbose build mode\n   -g                   - build for debug\n   -n                   - no install step\n   -h                   - print this text\n   default action (no args) is to build and install all code and packages\n\"\n\nTHIRDPARTY_BUILD_DIR=${REPODIR}/thirdparty/aws-cpp/build\nIO_BUILD_DIR=${REPODIR}/io/build\nLIBENGINE_BUILD_DIR=${REPODIR}/engine/build\nENGINE_BUILD_DIR=${REPODIR}/engine\nPYBLAZING_BUILD_DIR=${REPODIR}/pyblazing\nALGEBRA_BUILD_DIR=${REPODIR}/algebra\nBUILD_DIRS=\"${THIRDPARTY_BUILD_DIR} ${IO_BUILD_DIR} ${LIBENGINE_BUILD_DIR}\"\n\n# Set defaults for vars modified by flags to this script\nVERBOSE=\"\"\nQUIET=\"--quiet\"\nBUILD_TYPE=RelWithDebInfo\nINSTALL_TARGET=install\nTESTS=\"ON\"\n\n# Set defaults for vars that may not have been defined externally\n#  FIXME: if INSTALL_PREFIX is not set, check PREFIX, then check\n#         CONDA_PREFIX, but there is no fallback from there!\nINSTALL_PREFIX=${INSTALL_PREFIX:=${PREFIX:=${CONDA_PREFIX}}}\nPARALLEL_LEVEL=${PARALLEL_LEVEL:=\"4\"}\nexport LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$INSTALL_PREFIX/lib:$INSTALL_PREFIX/lib64\nexport CXXFLAGS=\"-L$INSTALL_PREFIX/lib\"\nexport CFLAGS=$CXXFLAGS\nif [ -z $CUDACXX ]; then\n    export CUDACXX=/usr/local/cuda/bin/nvcc\nfi\n\nfunction hasArg {\n    (( ${NUMARGS} != 0 )) && (echo \" ${ARGS} \" | grep -q \" $1 \")\n}\n\nfunction buildAll {\n    ((${NUMARGS} == 0 )) || !(echo \" ${ARGS} \" | grep -q \" [^-]\\+ \")\n}\n\nif hasArg -h; then\n    echo \"${HELP}\"\n    exit 0\nfi\n\n# Check for valid usage\nif (( ${NUMARGS} != 0 )); then\n    for a in ${ARGS}; do\n    if ! (echo \" ${VALIDARGS} \" | grep -q \" ${a} \"); then\n        echo \"Invalid option: ${a}\"\n        exit 1\n    fi\n    done\nfi\n\n# Get version number\nexport GIT_DESCRIBE_TAG=`git describe --tags`\nexport MINOR_VERSION=`echo $GIT_DESCRIBE_TAG | grep -o -E '([0-9]+\\.[0-9]+)'`\nexport UCX_PY_VERSION=\"0.21\"\n\n# Process flags\nif hasArg -v; then\n    VERBOSE=1\n    QUIET=\"\"\nfi\nif hasArg -g; then\n    BUILD_TYPE=Debug\nfi\nif hasArg -n; then\n    INSTALL_TARGET=\"\"\nfi\nif hasArg -t; then\n    TESTS=\"OFF\"\nfi\n\n# If clean given, run it prior to any other steps\nif hasArg clean; then\n    # If the dirs to clean are mounted dirs in a container, the\n    # contents should be removed but the mounted dirs will remain.\n    # The find removes all contents but leaves the dirs, the rmdir\n    # attempts to remove the dirs but can fail safely.\n    for bd in ${BUILD_DIRS}; do\n    if [ -d ${bd} ]; then\n        find ${bd} -mindepth 1 -delete\n        rmdir ${bd} || true\n    fi\n    done\n\n    if hasArg thirdparty; then\n        rm -rf ${REPODIR}/thirdparty/aws-cpp/\n    fi\n\n    exit 0\nfi\n\n################################################################################\n\nif buildAll || hasArg io || hasArg libengine || hasArg thirdparty || hasArg update; then\n    if hasArg disable-aws-s3; then\n        echo \"AWS S3 thirdparty won't be downloaded\"\n    else\n        if [ ! -d \"${REPODIR}/thirdparty/aws-cpp/\" ]; then\n            cd ${REPODIR}/thirdparty/\n            aws_cpp_version=$(conda list | grep aws-sdk-cpp|tail -n 1|awk '{print $2}')\n            echo \"aws_cpp_version for aws cpp sdk 3rdparty is: $aws_cpp_version\"\n\n            git clone -b $aws_cpp_version --depth=1 https://github.com/aws/aws-sdk-cpp.git ${REPODIR}/thirdparty/aws-cpp/\n            mkdir -p ${THIRDPARTY_BUILD_DIR}\n            cd ${THIRDPARTY_BUILD_DIR}\n            cmake -GNinja \\\n                -DCMAKE_INSTALL_PREFIX=\"${INSTALL_PREFIX}\" \\\n                -DCMAKE_INSTALL_LIBDIR=lib \\\n                -DBUILD_ONLY='s3-encryption' \\\n                -DENABLE_UNITY_BUILD=on \\\n                -DENABLE_TESTING=off \\\n                -DCMAKE_BUILD_TYPE=Release \\\n                ..\n            ninja install\n        else\n            echo \"thirdparty/aws-cpp/ is already installed in ${INSTALL_PREFIX}\"\n        fi\n    fi\nfi\n\n################################################################################\n\nif hasArg update; then\n    conda install --yes -c rapidsai-nightly -c nvidia -c conda-forge -c defaults librmm=$MINOR_VERSION rmm=$MINOR_VERSION libcudf=$MINOR_VERSION cudf=$MINOR_VERSION dask-cudf=$MINOR_VERSION dask-cuda=$MINOR_VERSION ucx-py=$UCX_PY_VERSION ucx-proc=*=gpu\nfi\n\n################################################################################\n\n\n\nif buildAll || hasArg io; then\n    disabled_aws_s3_flag=\"\"\n    if hasArg disable-aws-s3; then\n        disabled_aws_s3_flag=\"-DS3_SUPPORT=OFF\"\n        echo \"AWS S3 support disabled for io\"\n    fi\n\n    disabled_google_gs_flag=\"\"\n    if hasArg disable-google-gs; then\n        disabled_google_gs_flag=\"-DGCS_SUPPORT=OFF\"\n        echo \"Google Cloud Storage support disabled for io\"\n    fi\n\n    mkdir -p ${IO_BUILD_DIR}\n    cd ${IO_BUILD_DIR}\n    cmake -DCMAKE_INSTALL_PREFIX=${INSTALL_PREFIX} \\\n          -DBUILD_TESTING=${TESTS} \\\n          -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \\\n          -DCMAKE_BUILD_TYPE=${BUILD_TYPE} $disabled_aws_s3_flag $disabled_google_gs_flag ..\n\n    if [[ ${TESTS} == \"ON\" ]]; then\n        make -j${PARALLEL_LEVEL} all\n    else\n        make -j${PARALLEL_LEVEL} VERBOSE=${VERBOSE}\n    fi\n\n    if [[ ${INSTALL_TARGET} != \"\" ]]; then\n        make -j${PARALLEL_LEVEL} install VERBOSE=${VERBOSE}\n    fi\nfi\n\nif buildAll || hasArg libengine; then\n    disabled_aws_s3_flag=\"\"\n    if hasArg disable-aws-s3; then\n        disabled_aws_s3_flag=\"-DS3_SUPPORT=OFF\"\n        echo \"AWS S3 support disabled for libengine\"\n    fi\n\n    disabled_google_gs_flag=\"\"\n    if hasArg disable-google-gs; then\n        disabled_google_gs_flag=\"-DGCS_SUPPORT=OFF\"\n        echo \"Google Cloud Storage support disabled for libengine\"\n    fi\n\n    disable_mysql_flag=\"\"\n    if hasArg disable-mysql; then\n        disable_mysql_flag=\"-DMYSQL_SUPPORT=OFF\"\n        echo \"MySQL database support disabled for engine\"\n    fi\n\n    disable_sqlite_flag=\"\"\n    if hasArg disable-sqlite; then\n        disable_sqlite_flag=\"-DSQLITE_SUPPORT=OFF\"\n        echo \"SQLite database support disabled for engine\"\n    fi\n\n    disable_postgresql_flag=\"\"\n    if hasArg disable-postgresql; then\n        disable_postgresql_flag=\"-DPOSTGRESQL_SUPPORT=OFF\"\n        echo \"PostgreSQL database support disabled for engine\"\n    fi\n\n    echo \"Building libengine\"\n    mkdir -p ${LIBENGINE_BUILD_DIR}\n    cd ${LIBENGINE_BUILD_DIR}\n    echo \"cmake -DCMAKE_INSTALL_PREFIX=${INSTALL_PREFIX} -DBUILD_TESTING=${TESTS} -DCMAKE_BUILD_TYPE=${BUILD_TYPE} -DCMAKE_EXE_LINKER_FLAGS=$CXXFLAGS ..\"\n    cmake -DCMAKE_INSTALL_PREFIX=${INSTALL_PREFIX} \\\n          -DBUILD_TESTING=${TESTS} \\\n          -DCMAKE_BUILD_TYPE=${BUILD_TYPE} \\\n          -DCMAKE_EXE_LINKER_FLAGS=\"$CXXFLAGS\" \\\n          -DCMAKE_EXPORT_COMPILE_COMMANDS=ON \\\n          $disabled_aws_s3_flag \\\n          $disabled_google_gs_flag \\\n          $disable_mysql_flag \\\n          $disable_sqlite_flag \\\n          $disable_postgresql_flag ..\n\n    echo \"Building libengine: make step\"\n    if [[ ${TESTS} == \"ON\" ]]; then\n        echo \"make -j${PARALLEL_LEVEL} all\"\n        make -j${PARALLEL_LEVEL} all\n    else\n        echo \"make -j${PARALLEL_LEVEL} blazingsql-engine VERBOSE=${VERBOSE}\"\n        make -j${PARALLEL_LEVEL} blazingsql-engine VERBOSE=${VERBOSE}\n    fi\n\n    if [[ ${INSTALL_TARGET} != \"\" ]]; then\n        echo \"make -j${PARALLEL_LEVEL} install VERBOSE=${VERBOSE}\"\n        make -j${PARALLEL_LEVEL} install VERBOSE=${VERBOSE}\n        cp libblazingsql-engine.so ${INSTALL_PREFIX}/lib/libblazingsql-engine.so\n    fi\nfi\n\n\nif buildAll || hasArg engine; then\n    echo \"Building engine (cython wrapper)\"\n    cd ${ENGINE_BUILD_DIR}\n    rm -f ./bsql_engine/io/io.h\n    rm -f ./bsql_engine/io/io.cpp\n\n    if [[ ${INSTALL_TARGET} != \"\" ]]; then\n        python setup.py build_ext --inplace\n        if [ $? != 0 ]; then\n            exit 1\n        fi\n        python setup.py install --single-version-externally-managed --record=record.txt\n        if [ $? != 0 ]; then\n            exit 1\n        fi\n\n        if [[ $CONDA_BUILD -eq 1 ]]; then\n            cp `pwd`/cio*.so `pwd`/../../_h_env*/lib/python*/site-packages\n            cp -r `pwd`/bsql_engine `pwd`/../../_h_env*/lib/python*/site-packages\n        fi\n    else\n        python setup.py build_ext --inplace --library-dir=${LIBENGINE_BUILD_DIR}\n        if [ $? != 0 ]; then\n            exit 1\n        fi\n    fi\nfi\n\nif buildAll || hasArg pyblazing; then\n    cd ${PYBLAZING_BUILD_DIR}\n    if [[ ${INSTALL_TARGET} != \"\" ]]; then\n        python setup.py build_ext --inplace\n        if [ $? != 0 ]; then\n            exit 1\n        fi\n        python setup.py install --single-version-externally-managed --record=record.txt\n        if [ $? != 0 ]; then\n            exit 1\n        fi\n\n        if [[ $CONDA_BUILD -eq 1 ]]; then\n            cp -r `pwd`/pyblazing `pwd`/../../_h_env*/lib/python*/site-packages\n            cp -r `pwd`/blazingsql `pwd`/../../_h_env*/lib/python*/site-packages\n        fi\n    else\n        python setup.py build_ext --inplace\n        if [ $? != 0 ]; then\n            exit 1\n        fi\n    fi\nfi\n\nif buildAll || hasArg algebra; then\n    cd ${ALGEBRA_BUILD_DIR}\n    if [[ ${TESTS} == \"ON\" ]]; then\n        mvn clean install -f pom.xml -Dmaven.repo.local=$INSTALL_PREFIX/blazing-protocol-mvn/ $QUIET\n    else\n        mvn clean install -Dmaven.test.skip=true -f pom.xml -Dmaven.repo.local=$INSTALL_PREFIX/blazing-protocol-mvn/ $QUIET\n    fi\n\n    if [[ ${INSTALL_TARGET} != \"\" ]]; then\n        cp blazingdb-calcite-application/target/BlazingCalcite.jar $INSTALL_PREFIX/lib/blazingsql-algebra.jar\n        cp blazingdb-calcite-core/target/blazingdb-calcite-core.jar $INSTALL_PREFIX/lib/blazingsql-algebra-core.jar\n    fi\nfi\n"
        },
        {
          "name": "ci",
          "type": "tree",
          "content": null
        },
        {
          "name": "conda-build-docker.sh",
          "type": "blob",
          "size": 8.1669921875,
          "content": "#!/bin/bash\n\n# ================================================================\n# NOTE\n# cpu-build:\n# - here we run conda build inside the docker container\n# - builds the artifacts\n# - but doesn't run any test\n# - it just needs CPU\n# - and it runs on top on gpuci/rapidsai-driver docker image\n# - https://gpuci.gpuopenanalytics.com/job/blazingsql/job/gpuci/job/pyblazing/job/prb/job/pyblazing-cpu-build/\n# ================================================================\n# NOTE\n# gpu-build:\n# - here we install a new conda dev env inside the docker container\n# - builds the artifacts\n# - and run any all the tests: unit tests and e2e\n# - it needs GPU\n# - it runs on top on gpuci/rapidsai docker image\n# - https://gpuci.gpuopenanalytics.com/job/blazingsql/job/gpuci/job/pyblazing/job/prb/job/pyblazing-gpu-build/\n# ================================================================\n# NOTE Examples:\n# Run GPUCI jobs (first the gpu-build and then the cpu-build):\n# ./conda-build-docker.sh cudf_version cuda_version python_version conda_token conda_username custom_label\n#\n# Run only the CPU BUILD job (use this one if you want to debug issues with conda build on gpuci)\n# BLAZING_GPUCI_JOB=cpu-build ./conda-build-docker.sh cudf_version cuda_version python_version conda_token conda_username custom_label\n#\n# Run only the GPU BUILD job (use this one if you want to debug issues with tests on gpuci/gpu build)\n# BLAZING_GPUCI_JOB=gpu-build ./conda-build-docker.sh cudf_version cuda_version python_version\n#\n# Run only the CPU BUILD job and upload the blazingsql package to your conda channel with the label main\n# BLAZING_GPUCI_JOB=cpu-build ./conda-build-docker.sh 0.18 10.2 3.7 conda_token main conda_username\n#\n# Run GPUCI jobs with defaults:\n# ./conda-build-docker.sh\n# ================================================================\n# NOTE Defaults:\n# cudf_version=0.18\n# cuda_version=10.2\n# python_version=3.7\n# conda_token=\"\"\n# conda_username=\"blazingsql-nightly\"\n# custom_label=\"\"\n# ================================================================\n# NOTE Remarks:\n# - In case a job fails then you will go automatically into the docker for debugging\n# - Don't forget to kill all the containers after you finish\n# ================================================================\n\nNUMARGS=$#\nARGS=$*\n\nVALIDARGS=\"-h\"\nHELP=\"# ================================================================\n# NOTE\n# cpu-build:\n# - here we run conda build inside the docker container\n# - builds the artifacts\n# - but doesn't run any test\n# - it just needs CPU\n# - and it runs on top on gpuci/rapidsai-driver docker image\n# - https://gpuci.gpuopenanalytics.com/job/blazingsql/job/gpuci/job/pyblazing/job/prb/job/pyblazing-cpu-build/\n# ================================================================\n# NOTE\n# gpu-build:\n# - here we install a new conda dev env inside the docker container\n# - builds the artifacts\n# - and run any all the tests: unit tests and e2e\n# - it needs GPU\n# - it runs on top on gpuci/rapidsai docker image\n# - https://gpuci.gpuopenanalytics.com/job/blazingsql/job/gpuci/job/pyblazing/job/prb/job/pyblazing-gpu-build/\n# ================================================================\n# NOTE Examples:\n# Run GPUCI jobs (first the gpu-build and then the cpu-build):\n# ./conda-build-docker.sh cudf_version cuda_version python_version conda_token custom_label conda_username\n#\n# Run only the CPU BUILD job (use this one if you want to debug issues with conda build on gpuci)\n# BLAZING_GPUCI_JOB=cpu-build ./conda-build-docker.sh cudf_version cuda_version python_version conda_token custom_label conda_username\n#\n# Run only the GPU BUILD job (use this one if you want to debug issues with tests on gpuci/gpu build)\n# BLAZING_GPUCI_JOB=gpu-build ./conda-build-docker.sh cudf_version cuda_version python_version\n#\n# Run only the CPU BUILD job and upload the blazingsql package to your conda channel with the label main\n# BLAZING_GPUCI_JOB=cpu-build ./conda-build-docker.sh 0.18 10.2 3.7 conda_token main conda_username\n#\n# Run GPUCI jobs with defaults:\n# ./conda-build-docker.sh\n# ================================================================\n# NOTE Defaults:\n# cudf_version=0.18\n# cuda_version=10.2\n# python_version=3.7\n# conda_token=\"\"\n# custom_label=\"\"\n# conda_username=\"blazingsql-nightly\"\n# ================================================================\n# NOTE Remarks:\n# - In case a job fails then you will go automatically into the docker for debugging\n# - Don't forget to kill all the containers after you finish\n# ================================================================\"\n\nfunction hasArg {\n    (( ${NUMARGS} != 0 )) && (echo \" ${ARGS} \" | grep -q \" $1 \")\n}\n\nif hasArg -h; then\n    echo \"${HELP}\"\n    exit 0\nfi\n\n# Logger function for build status output\nfunction logger() {\n  echo -e \"\\n>>>> $@\\n\"\n}\n\nexport WORKSPACE=$PWD\n\nif [ -z $BLAZING_GPUCI_JOB ]; then\n    BLAZING_GPUCI_JOB=\"\"\n    echo \"BLAZING_GPUCI_JOB: $BLAZING_GPUCI_JOB\"\nfi\n\nif [ -z $BLAZING_GPUCI_OS ]; then\n    BLAZING_GPUCI_OS=\"ubuntu16.04\"\n    echo \"BLAZING_GPUCI_OS: $BLAZING_GPUCI_OS\"\nfi\n\nCUDF_VERSION=\"0.18\"\nif [ ! -z $1 ]; then\n    CUDF_VERSION=$1\nfi\necho \"CUDF_VERSION: $CUDF_VERSION\"\n\nCUDA_VERSION=\"10.2\"\nif [ ! -z $2 ]; then\n    CUDA_VERSION=$2\nfi\necho \"CUDA_VERSION: $CUDA_VERSION\"\n\nPYTHON_VERSION=\"3.7\"\nif [ ! -z $3 ]; then\n    PYTHON_VERSION=$3\nfi\necho \"PYTHON_VERSION: $PYTHON_VERSION\"\n\n#USER=$(id -u):$(id -g)\nUSER=\"0:0\"\necho \"USER: $USER\"\n\nif [ \"$BLAZING_GPUCI_JOB\" = \"\" ] || [ \"$BLAZING_GPUCI_JOB\" = \"cpu-build\" ]; then\n    MY_UPLOAD_KEY=\"\"\n    UPLOAD_BLAZING=\"0\"\n    if [ ! -z $4 ]; then\n        MY_UPLOAD_KEY=$4\n        UPLOAD_BLAZING=1\n    fi\n    echo \"MY_UPLOAD_KEY: ${MY_UPLOAD_KEY:(-4)}\"\n    echo \"UPLOAD_BLAZING: $UPLOAD_BLAZING\"\n\n    CONDA_USERNAME=\"blazingsql-nightly\"\n    if [ ! -z $5 ]; then\n        CONDA_USERNAME=$5\n    fi\n    echo \"CONDA_USERNAME: $CONDA_USERNAME\"\n\n    TYPE=\"nightly\"\n    if [ ! -z $6 ]; then\n        TYPE=$6\n    fi\n    echo \"TYPE: $TYPE\"\n\n    CUSTOM_LABEL=\"\"\n    if [ ! -z $7 ]; then\n        CUSTOM_LABEL=$7\n    fi\n    echo \"CUSTOM_LABEL: $CUSTOM_LABEL\"\nfi\n\nif [ \"$BLAZING_GPUCI_JOB\" = \"\" ] || [ \"$BLAZING_GPUCI_JOB\" = \"gpu-build\" ]; then\n    logger \"Cleaning the workspace before start the GPU BUILD job ...\"\n    cd $WORKSPACE\n    ./build.sh clean\n    ./build.sh clean thirdparty\n\n    gpu_build_cmd=\"./ci/gpu/build.sh\"\n    gpu_build_img=gpuci/rapidsai:$CUDF_VERSION-cuda${CUDA_VERSION}-devel-$BLAZING_GPUCI_OS-py$PYTHON_VERSION\n\n    logger \"Updating the docker image for the GPU BUILD job ...\"\n    echo \"docker pull $gpu_build_img\"\n    docker pull $gpu_build_img\n\n    gpu_container=\"blazingsql-gpuci-gpu-build-\"$RANDOM\n\n    logger \"Running the docker container for the GPU BUILD job ...\"\n    GPU_DOCKER=\"docker run --name $gpu_container --rm \\\n        --runtime=nvidia \\\n        -u $USER \\\n        -e CUDA_VER=${CUDA_VERSION} -e PYTHON_VER=$PYTHON_VERSION \\\n        -e WORKSPACE=$WORKSPACE \\\n        -v /etc/passwd:/etc/passwd \\\n        -v ${WORKSPACE}:${WORKSPACE} -w ${WORKSPACE} \\\n        $gpu_build_img \\\n        $gpu_build_cmd\"\n    echo \"GPU_DOCKER: \"$GPU_DOCKER\n    eval $GPU_DOCKER\nfi\n\nif [ \"$BLAZING_GPUCI_JOB\" = \"\" ] || [ \"$BLAZING_GPUCI_JOB\" = \"cpu-build\" ]; then\n    logger \"Cleaning the workspace before start the CPU BUILD job ...\"\n    cd $WORKSPACE\n    ./build.sh clean\n    ./build.sh clean thirdparty\n\n    #cpu_build_cmd=\"bash\"\n    cpu_build_cmd=\"./ci/cpu/build.sh\"\n    cpu_build_img=gpuci/rapidsai-driver:$CUDF_VERSION-cuda${CUDA_VERSION}-devel-$BLAZING_GPUCI_OS-py$PYTHON_VERSION\n\n    logger \"Updating the docker image for the CPU BUILD job ...\"\n    echo \"docker pull $cpu_build_img\"\n    docker pull $cpu_build_img\n\n    cpu_container=\"blazingsql-gpuci-cpu-build-\"$RANDOM\n\n    logger \"Running the docker container for the CPU BUILD job ...\"\n    CPU_DOCKER=\"docker run --name $cpu_container --rm \\\n        -u $USER \\\n        -e CUDA_VER=${CUDA_VERSION} -e PYTHON_VER=$PYTHON_VERSION \\\n        -e CONDA_USERNAME=$CONDA_USERNAME -e MY_UPLOAD_KEY=$MY_UPLOAD_KEY \\\n        -e BUILD_MODE=branch -e TYPE=$TYPE \\\n        -e UPLOAD_BLAZING=$UPLOAD_BLAZING -e CUSTOM_LABEL=$CUSTOM_LABEL \\\n        -e WORKSPACE=$WORKSPACE \\\n        -v /etc/passwd:/etc/passwd \\\n        -v ${WORKSPACE}:${WORKSPACE} -w ${WORKSPACE} \\\n        $cpu_build_img \\\n        $cpu_build_cmd\"\n    echo \"CPU_DOCKER: \"$CPU_DOCKER\n    eval $CPU_DOCKER\nfi\n\n"
        },
        {
          "name": "conda",
          "type": "tree",
          "content": null
        },
        {
          "name": "conda_build.jenkinsfile",
          "type": "blob",
          "size": 1.6748046875,
          "content": "pipeline {\n   \n  agent any\n\n  options {\n    timestamps()\n    timeout(time: 2, unit: 'HOURS')\n  }\n\n  parameters {\n    choice(name: 'TYPE', choices: 'nightly\\nstable', description: 'Nightly or Stable')\n    string(name: 'RAPIDS_BUILD_VERSION', defaultValue: '0.17', description: 'Version of Rapids, example: 0.18')\n    choice(name: 'CUDA_VERSION', choices: '10.1\\n10.2\\n11.0', description: 'Cuda version')\n    choice(name: 'PYTHON_VERSION', choices: '3.7\\n3.8', description: 'Python version')\n\n    string(name: 'CONDA_USERNAME', defaultValue: 'blazingsql-nightly', description: 'The Anaconda account, example: blazingsql, blazingsql-nightly, etc')\n    string(name: 'CONDA_TOKEN', defaultValue: 'conda-token-nightly', description: 'Anaconda account token')\n    booleanParam(name: 'TEST', defaultValue: false, description: 'Run GPU tests')\n  }\n  \n  environment {\n    SLACK_MESSAGE=\" - Job '${env.JOB_NAME}' - Build #${env.BUILD_NUMBER}: ${env.BUILD_URL}\"\n  }\n\n  stages {\n\n    stage(\"Repository\") {\n      steps {\n        checkout scm\n      }\n    }\n\n    stage(\"Test\") {\n      when {\n        expression {\n          return params.TEST ==~ /(?i)(Y|YES|T|TRUE|ON|RUN)/\n        }\n      }\n      steps {\n        sh \"BLAZING_GPUCI_JOB=gpu-build ./conda-build-docker.sh ${params.RAPIDS_BUILD_VERSION} ${params.CUDA_VERSION} ${params.PYTHON_VERSION}\"\n      }\n    }\n\n    stage(\"Build && Upload\") {\n      steps {\n        withCredentials([string(credentialsId: \"${params.CONDA_TOKEN}\", variable: 'TOKEN')]) {\n          sh \"BLAZING_GPUCI_JOB=cpu-build ./conda-build-docker.sh ${params.RAPIDS_BUILD_VERSION} ${params.CUDA_VERSION} ${params.PYTHON_VERSION} $TOKEN ${params.CONDA_USERNAME} ${params.TYPE}\"\n        }\n      }\n    }\n\n  }\n\n}\n\n"
        },
        {
          "name": "dask_rapids.363119",
          "type": "blob",
          "size": 46.09765625,
          "content": "\nCurrently Loaded Modules:\n  1) hsi/5.0.2.p5                     9) cmake/3.17.3\n  2) xalt/1.2.0                      10) boost/1.66.0\n  3) lsf-tools/2.0                   11) cuda/10.1.243\n  4) darshan-runtime/3.1.7           12) zlib/1.2.11\n  5) DefApps                         13) texinfo/6.5\n  6) gcc/7.4.0                       14) openblas/0.3.9-omp\n  7) spectrum-mpi/10.3.1.2-20200121  15) netlib-lapack/3.8.0\n  8) python/3.7.0\n\n \n\ndistributed.scheduler - INFO - -----------------------------------------------\ndistributed.scheduler - INFO - -----------------------------------------------\ndistributed.scheduler - INFO - Clear task state\ndistributed.scheduler - INFO -   Scheduler at:     tcp://10.41.0.45:8786\ndistributed.scheduler - INFO -   dashboard at:           10.41.0.45:8787\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.68:44505'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.68:41089'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.68:37709'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.68:43885'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.68:40553'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.68:41559'\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.41:33213'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.41:41465'\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.41:42719'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.41:34953'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.41:43129'\ndistributed.nanny - INFO -         Start Nanny at: 'tcp://10.41.20.41:46117'\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\n\nlibgomp: Logical CPU number 64 out of range\n\nlibgomp: Invalid value for environment variable OMP_PLACES\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.68:44299\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.68:44299\ndistributed.worker - INFO -          dashboard at:          10.41.20.68:44817\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-mqhb8jd4\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071023-faae132a-a4d6-488b-a317-fbe40eff0b62\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071023f28-ead3dab0-aba7-435e-abce-b21fe25b4ff3\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.68:41509\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.68:41509\ndistributed.worker - INFO -          dashboard at:          10.41.20.68:42265\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-ucqgwa8u\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071031-9954123c-4a04-42ae-b018-7c9ea19a34e5\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071022fd0-ebca4846-6cd5-4d9a-98b1-27630e4c2126\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.68:39207\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.68:39207\ndistributed.worker - INFO -          dashboard at:          10.41.20.68:39995\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-eujvh2lt\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071031-a3f9bff1-083e-4696-9ffc-7c4b9b2f3ed2\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071031048-ce5e9c27-b3f9-405f-bb63-4b58b05d96e1\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.68:37083\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.68:37083\ndistributed.worker - INFO -          dashboard at:          10.41.20.68:34395\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-3deohzh6\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071031-2093376d-bec0-497b-992d-3e223965dcc2\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071022fd0-5c7501b2-09d1-4245-a17e-c8204eb092ef\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.68:42059\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.68:42059\ndistributed.worker - INFO -          dashboard at:          10.41.20.68:33871\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-nuhnksp1\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071024-b4dd9bc8-4712-45e7-8832-5a26a54d903f\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071024ef0-dab59416-e965-48d2-937d-e1b92b9c2f1c\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.68:45689\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.68:45689\ndistributed.worker - INFO -          dashboard at:          10.41.20.68:41041\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-jv9u0mx5\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071022-174c940c-c47d-4ba2-a7a7-8d2b4d6b79c0\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071022f28-1c6b7ca5-8959-41f8-a108-025797695863\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.68:44299', name: tcp://10.41.20.68:44299, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.68:44299\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.68:41509', name: tcp://10.41.20.68:41509, memory: 0, processing: 0>\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.68:41509\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.68:39207', name: tcp://10.41.20.68:39207, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.68:39207\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.68:37083', name: tcp://10.41.20.68:37083, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.68:37083\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.68:42059', name: tcp://10.41.20.68:42059, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.68:42059\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.68:45689', name: tcp://10.41.20.68:45689, memory: 0, processing: 0>\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.68:45689\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.preloading - INFO - Import preload module: dask_cuda.initialize\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.41:44395\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.41:44395\ndistributed.worker - INFO -          dashboard at:          10.41.20.41:42841\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-nr4aatva\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071023-3f035103-d70a-4e40-a04f-4b1779b57cea\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071031048-fd043d96-c8d3-4be1-9515-198a229ed684\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.41:33391\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.41:33391\ndistributed.worker - INFO -          dashboard at:          10.41.20.41:37815\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-k3vlxpxw\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071024f28-03f14fbd-8c43-45d6-a7f8-6e06d1e749f4\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071024-0169b137-fafe-4ebd-8ec1-38866051107a\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.41:44395', name: tcp://10.41.20.41:44395, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.41:44395\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.41:33085\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.41:33085\ndistributed.worker - INFO -          dashboard at:          10.41.20.41:35873\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-rjycxmwp\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071022f28-38493b25-4935-4000-a0ff-4caa49d9e6ec\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071022-4218b65b-c9ff-401b-8020-36e8852c784f\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.41:41479\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.41:41479\ndistributed.worker - INFO -          dashboard at:          10.41.20.41:39613\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-npr8cs0k\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071023-f98a93f4-d244-49a5-b26d-6fd19029fd03\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071023f28-8d253258-0192-4e36-abfc-ef504bd89920\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.41:41085\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.41:41085\ndistributed.worker - INFO -          dashboard at:          10.41.20.41:35073\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-aa6ylioo\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071023fd0-7f7c992e-ea07-4be8-9279-e83cb915e427\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071023-e8cc5b76-9804-423e-b7bc-fdbc21d93932\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -       Start worker at:    tcp://10.41.20.41:34547\ndistributed.worker - INFO -          Listening to:    tcp://10.41.20.41:34547\ndistributed.worker - INFO -          dashboard at:          10.41.20.41:44397\ndistributed.worker - INFO - Waiting to connect to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.worker - INFO -               Threads:                          1\ndistributed.worker - INFO -                Memory:                  330.00 GB\ndistributed.worker - INFO -       Local Directory: /mnt/bb/wmalpica/dask-worker-space/worker-4x4s8m2z\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.CPUAffinity object at 0x200071024-9c6e618a-05de-4cad-9563-2d1e774a7d9c\ndistributed.worker - INFO - Starting Worker plugin <dask_cuda.utils.RMMSetup object at 0x200071024f28-03e477e5-e15c-4b5a-bd5d-a757e44ba40d\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.41:33391', name: tcp://10.41.20.41:33391, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.41:33391\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.41:33085', name: tcp://10.41.20.41:33085, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.41:33085\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.41:41085', name: tcp://10.41.20.41:41085, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.41:41085\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.41:41479', name: tcp://10.41.20.41:41479, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.41:41479\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Register worker <Worker 'tcp://10.41.20.41:34547', name: tcp://10.41.20.41:34547, memory: 0, processing: 0>\ndistributed.scheduler - INFO - Starting worker compute stream, tcp://10.41.20.41:34547\ndistributed.core - INFO - Starting established connection\ndistributed.worker - INFO -         Registered to:      tcp://10.41.0.45:8786\ndistributed.worker - INFO - -------------------------------------------------\ndistributed.core - INFO - Starting established connection\ndistributed.scheduler - INFO - Receive client connection: Client-7628b67a-fdf8-11ea-b7c6-70e28414416b\ndistributed.core - INFO - Starting established connection\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\nlog4j:ERROR setFile(null,false) call failed.\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\tat java.io.FileOutputStream.open0(Native Method)\n\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)log4j:ERROR setFile(null,false) call failed.\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\n\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat java.lang.Class.forNameImpl(Native Method)\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\njava.io.FileNotFoundException: algebra.log (Read-only file system)\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\tat java.io.FileOutputStream.open0(Native Method)\n\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\n\tat java.lang.Class.forNameImpl(Native Method)\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlog4j:ERROR setFile(null,false) call failed.\njava.io.FileNotFoundException: algebra.log (Read-only file system)\n\tat java.io.FileOutputStream.open0(Native Method)\n\tat java.io.FileOutputStream.open(FileOutputStream.java:286)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:226)\n\tat java.io.FileOutputStream.<init>(FileOutputStream.java:144)\n\tat org.apache.log4j.FileAppender.setFile(FileAppender.java:294)\n\tat org.apache.log4j.FileAppender.activateOptions(FileAppender.java:165)\n\tat org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:307)\n\tat org.apache.log4j.xml.DOMConfigurator.parseAppender(DOMConfigurator.java:295)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByName(DOMConfigurator.java:176)\n\tat org.apache.log4j.xml.DOMConfigurator.findAppenderByReference(DOMConfigurator.java:191)\n\tat org.apache.log4j.xml.DOMConfigurator.parseChildrenOfLoggerElement(DOMConfigurator.java:523)\n\tat org.apache.log4j.xml.DOMConfigurator.parseRoot(DOMConfigurator.java:492)\n\tat org.apache.log4j.xml.DOMConfigurator.parse(DOMConfigurator.java:1006)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:872)\n\tat org.apache.log4j.xml.DOMConfigurator.doConfigure(DOMConfigurator.java:778)\n\tat org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:526)\n\tat org.apache.log4j.LogManager.<clinit>(LogManager.java:127)\n\tat org.slf4j.impl.Log4jLoggerFactory.getLogger(Log4jLoggerFactory.java:64)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:358)\n\tat org.slf4j.LoggerFactory.getLogger(LoggerFactory.java:383)\n\tat com.blazingdb.calcite.schema.BlazingSchema.<clinit>(BlazingSchema.java:28)\n\tat java.lang.Class.forNameImpl(Native Method)\n\tat java.lang.Class.forName(Class.java:337)\n\tat org.jpype.manager.TypeManager.lookupByName(Unknown Source)\n\tat org.jpype.manager.TypeManager.findClassByName(Unknown Source)\nlistening: tcp://*:15332\nlistening: tcp://*:26710\nlistening: tcp://*:23968\nlistening: tcp://*:17648\nlistening: tcp://*:26241\nlistening: tcp://*:31553\nlistening: tcp://*:15106\nlistening: tcp://*:20609\nlistening: tcp://*:17594\nlistening: tcp://*:24379\nlistening: tcp://*:11012\nlistening: tcp://*:22502\ndistributed.scheduler - INFO - End scheduler at 'tcp://10.41.0.45:8786'\nCould not read jskill result from pmix server\nTraceback (most recent call last):\n  File \"/gpfs/alpine/gen119/proj-shared/bsql_shared/bsql_env2/bin/dask-scheduler\", line 8, in <module>\n    sys.exit(go())\n  File \"/gpfs/alpine/gen119/proj-shared/bsql_shared/bsql_env2/lib/python3.7/site-packages/distributed/cli/dask_scheduler.py\", line 226, in go\n    main()\n  File \"/gpfs/alpine/gen119/proj-shared/bsql_shared/bsql_env2/lib/python3.7/site-packages/click/core.py\", line 829, in __call__\n    return self.main(*args, **kwargs)\n  File \"/gpfs/alpine/gen119/proj-shared/bsql_shared/bsql_env2/lib/python3.7/site-packages/click/core.py\", line 782, in main\n    rv = self.invoke(ctx)\n  File \"/gpfs/alpine/gen119/proj-shared/bsql_shared/bsql_env2/lib/python3.7/site-packages/click/core.py\", line 1066, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/gpfs/alpine/gen119/proj-shared/bsql_shared/bsql_env2/lib/python3.7/site-packages/click/core.py\", line 610, in invoke\n    return callback(*args, **kwargs)\n  File \"/gpfs/alpine/gen119/proj-shared/bsql_shared/bsql_env2/lib/python3.7/site-packages/distributed/cli/dask_scheduler.py\", line 217, in main\n    loop.run_sync(run)\n  File \"/gpfs/alpine/gen119/proj-shared/bsql_shared/bsql_env2/lib/python3.7/site-packages/tornado/ioloop.py\", line 531, in run_sync\n    raise TimeoutError(\"Operation timed out after %s seconds\" % timeout)\ntornado.util.TimeoutError: Operation timed out after None seconds\n\n------------------------------------------------------------\nSender: LSF System <lsfadmin@batch5>\nSubject: Job 363119: <dask_rapids> in cluster <summit> Exited\n\nJob <dask_rapids> was submitted from host <login4> by user <wmalpica> in cluster <summit> at Wed Sep 23 19:54:02 2020\nJob was executed on host(s) <1*batch5>, in queue <batch>, as user <wmalpica> in cluster <summit> at Wed Sep 23 19:54:28 2020\n                            <42*h26n15>\n                            <42*h28n06>\n</ccs/home/wmalpica> was used as the home directory.\n</ccs/home/wmalpica/blazingsql> was used as the working directory.\nStarted at Wed Sep 23 19:54:28 2020\nTerminated at Wed Sep 23 19:58:22 2020\nResults reported at Wed Sep 23 19:58:22 2020\n\nThe output (if any) is above this job summary.\n\n"
        },
        {
          "name": "dependencies.sh",
          "type": "blob",
          "size": 1.51171875,
          "content": "#!/bin/bash\n#\n# This script install BlazingSQL dependencies based on rapids version\n#\necho \"Usage: ./$0 rapids_version cuda_version nightly\"\n\nexport GREEN='\\033[0;32m'\nexport RED='\\033[0;31m'\nBOLDGREEN=\"\\e[1;${GREEN}\"\nITALICRED=\"\\e[3;${RED}\"\nENDCOLOR=\"\\e[0m\"\n\nRAPIDS_VERSION=\"21.08\"\nUCX_PY_VERSION=\"0.21\"\nCUDA_VERSION=\"11.0\"\nCHANNEL=\"\"\n\nif [ ! -z $1 ]; then\n  RAPIDS_VERSION=$1\nfi\n\nif [ ! -z $2 ]; then\n  CUDA_VERSION=$2\nfi\n\nif [ ! -z $3 ]; then\n  CHANNEL=\"-nightly\"\nfi\n\necho -e \"${GREEN}Installing dependencies${ENDCOLOR}\"\nconda install --yes -c conda-forge spdlog'>=1.8.5,<1.9' google-cloud-cpp'>=1.25,<1.30' ninja mysql-connector-cpp=8.0.23 libpq=13 nlohmann_json=3.9.1\n# NOTE cython must be the same of cudf (for 0.11 and 0.12 cython is >=0.29,<0.30)\nconda install --yes -c conda-forge cmake=3.18 gtest==1.10.0=h0efe328_4 gmock cppzmq cython=0.29 openjdk'>=8.0,<9.0' maven jpype1 netifaces pyhive pytest tqdm ipywidgets boost-cpp=1.72.0\n\n\necho -e \"${GREEN}Install RAPIDS dependencies${ENDCOLOR}\"\nconda install --yes -c rapidsai$CHANNEL -c nvidia -c conda-forge -c defaults dask-cuda=$RAPIDS_VERSION dask-cudf=$RAPIDS_VERSION cudf=$RAPIDS_VERSION \"rapidsai$CHANNEL::librmm=$RAPIDS_VERSION\" ucx-py=$UCX_PY_VERSION ucx-proc=*=gpu cudatoolkit=$CUDA_VERSION\n\necho -e \"${GREEN}Install E2E test dependencies${ENDCOLOR}\"\n\npip install openpyxl pymysql gitpython pynvml gspread oauth2client 'sql-metadata==1.12.0' pyyaml\n\nif [ $? -eq 0 ]; then\n  echo -e \"${GREEN}Installation complete${ENDCOLOR}\"\nelse\n  echo -e \"${RED}Installation failed${ENDCOLOR}\"\nfi\n"
        },
        {
          "name": "docker-build.sh",
          "type": "blob",
          "size": 0.7666015625,
          "content": "#!/bin/bash\n# Usage:   cuda_version   python_version  rapids_version  nightly\n# Example: 10.1|10.2|11.0 3.7|3.8         0.15|0.16       nightly|true\n\nCUDA_VERSION=\"10.1\"\nif [ ! -z $1 ]; then\n  CUDA_VERSION=$1\nfi\n\nPYTHON_VERSION=\"3.7\"\nif [ ! -z $2 ]; then\n  PYTHON_VERSION=$2\nfi\n\nRAPIDS_VERSION=\"0.18\"\nif [ ! -z $3 ]; then\n  RAPIDS_VERSION=$3\nfi\n\nNIGHTLY=\"\"\nif [ ! -z $4 ]; then\n  NIGHTLY=\"-nightly\"\nfi\n\nDOCKER_IMAGE=\"blazingdb/blazingsql$NIGHTLY:cuda$CUDA_VERSION-py$PYTHON_VERSION\"\nCONDA_CH=\"-c blazingsql$NIGHTLY -c rapidsai$NIGHTLY -c nvidia\"\n\nCMD=\"docker build -t ${DOCKER_IMAGE} --build-arg CUDA_VER=${CUDA_VERSION} --build-arg PYTHON_VERSION=${PYTHON_VERSION} --build-arg CONDA_CH=\\\"${CONDA_CH}\\\" --build-arg RAPIDS_VERSION=\\\"${RAPIDS_VERSION}\\\" ./\"\n\necho \"CMD: $CMD\"\neval $CMD\n\n"
        },
        {
          "name": "docs",
          "type": "tree",
          "content": null
        },
        {
          "name": "docsrc",
          "type": "tree",
          "content": null
        },
        {
          "name": "engine",
          "type": "tree",
          "content": null
        },
        {
          "name": "io",
          "type": "tree",
          "content": null
        },
        {
          "name": "new",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "notebooks",
          "type": "tree",
          "content": null
        },
        {
          "name": "powerpc",
          "type": "tree",
          "content": null
        },
        {
          "name": "print_env.sh",
          "type": "blob",
          "size": 1.732421875,
          "content": "#!/usr/bin/env bash\n# Reports relevant environment information useful for diagnosing and\n# debugging cuDF issues.\n# Usage: \n# \"./print_env.sh\" - prints to stdout\n# \"./print_env.sh > env.txt\" - prints to file \"env.txt\"\n\nprint_env() {\necho \"**git***\"\nif [ \"$(git rev-parse --is-inside-work-tree 2>/dev/null)\" == \"true\" ]; then\ngit log --decorate -n 1\necho \"**git submodules***\"\ngit submodule status --recursive\nelse\necho \"Not inside a git repository\"\nfi\necho \n\necho \"***OS Information***\"\ncat /etc/*-release\nuname -a\necho \n\necho \"***GPU Information***\"\nnvidia-smi\necho \n\necho \"***CPU***\"\nlscpu\necho\n\necho \"***CMake***\"\nwhich cmake && cmake --version\necho \n\necho \"***g++***\"\nwhich g++ && g++ --version\necho \n\necho \"***nvcc***\"\nwhich nvcc && nvcc --version\necho \n\necho \"***Python***\"\nwhich python && python -c \"import sys; print('Python {0}.{1}.{2}'.format(sys.version_info[0], sys.version_info[1], sys.version_info[2]))\"\necho\n\necho \"***Environment Variables***\"\n\nprintf '%-32s: %s\\n' PATH $PATH\n\nprintf '%-32s: %s\\n' LD_LIBRARY_PATH $LD_LIBRARY_PATH\n\nprintf '%-32s: %s\\n' NUMBAPRO_NVVM $NUMBAPRO_NVVM\n\nprintf '%-32s: %s\\n' NUMBAPRO_LIBDEVICE $NUMBAPRO_LIBDEVICE\n\nprintf '%-32s: %s\\n' CONDA_PREFIX $CONDA_PREFIX\n\nprintf '%-32s: %s\\n' PYTHON_PATH $PYTHON_PATH\n\necho\n\n\n# Print conda packages if conda exists\nif type \"conda\" &> /dev/null; then\necho '***conda packages***'\nwhich conda && conda list\necho\n# Print pip packages if pip exists\nelif type \"pip\" &> /dev/null; then\necho \"conda not found\"\necho \"***pip packages***\"\nwhich pip && pip list\necho\nelse\necho \"conda not found\"\necho \"pip not found\"\nfi\n}\n\necho \"<details><summary>Click here to see environment details</summary><pre>\"\necho \"     \"\nprint_env | while read -r line; do\n    echo \"     $line\"\ndone\necho \"</pre></details>\"\n"
        },
        {
          "name": "pyblazing",
          "type": "tree",
          "content": null
        },
        {
          "name": "run_2_workers.sh",
          "type": "blob",
          "size": 1.126953125,
          "content": "export DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT=\"100s\"\nexport DASK_DISTRIBUTED__COMM__TIMEOUTS__TCP=\"600s\"\nexport DASK_DISTRIBUTED__COMM__RETRY__DELAY__MIN=\"1s\"\nexport DASK_DISTRIBUTED__COMM__RETRY__DELAY__MAX=\"60s\"\nexport DASK_DISTRIBUTED__SCHEDULER__WORK_STEALING=True\n\nexport DASK_DISTRIBUTED__WORKER__MEMORY__Terminate=\"False\"\nexport DEVICE_MEMORY_LIMIT=\"25GB\"\nexport MAX_SYSTEM_MEMORY=$(free -m | awk '/^Mem:/{print $2}')M\n\nARG_INTERFACE=wlo1\nARG_HOSTNAME=ucx://10.0.0.23:8786\n\nwhile getopts 'i:h:' o; do\n  case \"${o}\" in\n    h)\n      ARG_HOSTNAME=${OPTARG}\n      ;;\n    i)\n      ARG_INTERFACE=${OPTARG}\n      ;;\n  esac\ndone\n\n# Dask-cuda-worker\n\nexport UCX_TLS=tcp,sockcm,cuda_copy,cuda_ipc\nexport UCX_SOCKADDR_TLS_PRIORITY=sockcm\nexport UCX_NET_DEVICES=$ARG_INTERFACE\nexport UCX_MEMTYPE_CACHE=n\n\nUCXPY_NON_BLOCKING_MODE=True \\\nCUDA_VISIBLE_DEVICES=0  \\\nDASK_UCX__CUDA_COPY=True \\\nDASK_UCX__TCP=True \\\nDASK_UCX__NVLINK=False \\\nDASK_UCX__INFINIBAND=False \\\nDASK_UCX__RDMACM=False \\\nDASK_UCX__REUSE_ENDPOINTS=False \\\ndask-cuda-worker $ARG_HOSTNAME \\\n    --interface $ARG_INTERFACE \\\n    --enable-tcp-over-ucx --device-memory-limit \"4GB\" --nthreads=8\n"
        },
        {
          "name": "run_jupyter.sh",
          "type": "blob",
          "size": 0.140625,
          "content": "#!/bin/bash\n\nsource activate bsql\njupyter-lab --notebook=/blazingsql/ --allow-root --ip=0.0.0.0 --port=8888 --no-browser --NotebookApp.token=''\n"
        },
        {
          "name": "run_sched.sh",
          "type": "blob",
          "size": 0.861328125,
          "content": "export DASK_DISTRIBUTED__COMM__TIMEOUTS__CONNECT=\"100s\"\nexport DASK_DISTRIBUTED__COMM__TIMEOUTS__TCP=\"600s\"\nexport DASK_DISTRIBUTED__COMM__RETRY__DELAY__MIN=\"1s\"\nexport DASK_DISTRIBUTED__COMM__RETRY__DELAY__MAX=\"60s\"\nexport DASK_DISTRIBUTED__SCHEDULER__WORK_STEALING=True\n\nARG_INTERFACE=wlo1\n\nexport UCX_TLS=tcp,sockcm,cuda_copy,cuda_ipc\nexport UCX_SOCKADDR_TLS_PRIORITY=sockcm\nexport UCX_NET_DEVICES=$ARG_INTERFACE\nexport UCX_MEMTYPE_CACHE=n\n\nwhile getopts 'i:' o; do\n  case \"${o}\" in\n    i)\n      ARG_INTERFACE=${OPTARG}\n      ;;\n  esac\ndone\n\n#Scheduler\nUCXPY_NON_BLOCKING_MODE=True \\\nDASK_UCX__CUDA_COPY=True \\\nDASK_UCX__TCP=True \\\nDASK_UCX__NVLINK=False \\\nDASK_UCX__INFINIBAND=False \\\nDASK_UCX__RDMACM=False \\\nDASK_UCX__REUSE_ENDPOINTS=False \\\n# DASK_RMM__POOL_SIZE=1GB \\\ndask-scheduler \\\n    --scheduler-file dask-scheduler.json \\\n    --protocol ucx --interface $ARG_INTERFACE\n"
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "test.sh",
          "type": "blob",
          "size": 0.201171875,
          "content": "CUDA_VISIBLE_DEVICES=0  \\\nDASK_UCX__CUDA_COPY=True \\\nDASK_UCX__TCP=True \\\nDASK_UCX__NVLINK=False \\\nDASK_UCX__INFINIBAND=False \\\nDASK_UCX__RDMACM=False \\\nDASK_UCX__REUSE_ENDPOINTS=True \\\nci/gpu/test.sh \"$@\"\n"
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "thirdparty",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}