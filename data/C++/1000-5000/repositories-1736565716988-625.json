{
  "metadata": {
    "timestamp": 1736565716988,
    "page": 625,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjYzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Anttwo/SuGaR",
      "stars": 2438,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 2.154296875,
          "content": "*.pt\n*.pth\noutput*\n*.slurm\n*.pyc\n*.ply\n*.obj\nsugar_tests.ipynb\nsugar_sh_tests.ipynb\ndev_render_blender_scene.py\ntest_environment.yml\nblender/dev_mode.py\n\n# To remove\nfrosting*\nextract_shell.py\nextract_frosting_textured_mesh.py\ntrain_frosting_refined.py\ntrain_frosting.py\nrun_frosting_viewer.py\nrender_blender_scene_frosting.py\nsugar_render_blender_package.ipynb\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\npip-wheel-metadata/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n.python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\nlearnableearthparser/fast_sampler/_sampler.c"
        },
        {
          "name": "LICENSE.md",
          "type": "blob",
          "size": 4.1748046875,
          "content": "Gaussian-Splatting License  \n===========================  \n\n**Inria** and **the Max Planck Institut for Informatik (MPII)** hold all the ownership rights on the *Software* named **gaussian-splatting**.  \nThe *Software* is in the process of being registered with the Agence pour la Protection des  \nProgrammes (APP).  \n\nThe *Software* is still being developed by the *Licensor*.  \n\n*Licensor*'s goal is to allow the research community to use, test and evaluate  \nthe *Software*.  \n\n## 1.  Definitions  \n\n*Licensee* means any person or entity that uses the *Software* and distributes  \nits *Work*.  \n\n*Licensor* means the owners of the *Software*, i.e Inria and MPII  \n\n*Software* means the original work of authorship made available under this  \nLicense ie gaussian-splatting.  \n\n*Work* means the *Software* and any additions to or derivative works of the  \n*Software* that are made available under this License.  \n\n\n## 2.  Purpose  \nThis license is intended to define the rights granted to the *Licensee* by  \nLicensors under the *Software*.  \n\n## 3.  Rights granted  \n\nFor the above reasons Licensors have decided to distribute the *Software*.  \nLicensors grant non-exclusive rights to use the *Software* for research purposes  \nto research users (both academic and industrial), free of charge, without right  \nto sublicense.. The *Software* may be used \"non-commercially\", i.e., for research  \nand/or evaluation purposes only.  \n\nSubject to the terms and conditions of this License, you are granted a  \nnon-exclusive, royalty-free, license to reproduce, prepare derivative works of,  \npublicly display, publicly perform and distribute its *Work* and any resulting  \nderivative works in any form.  \n\n## 4.  Limitations  \n\n**4.1 Redistribution.** You may reproduce or distribute the *Work* only if (a) you do  \nso under this License, (b) you include a complete copy of this License with  \nyour distribution, and (c) you retain without modification any copyright,  \npatent, trademark, or attribution notices that are present in the *Work*.  \n\n**4.2 Derivative Works.** You may specify that additional or different terms apply  \nto the use, reproduction, and distribution of your derivative works of the *Work*  \n(\"Your Terms\") only if (a) Your Terms provide that the use limitation in  \nSection 2 applies to your derivative works, and (b) you identify the specific  \nderivative works that are subject to Your Terms. Notwithstanding Your Terms,  \nthis License (including the redistribution requirements in Section 3.1) will  \ncontinue to apply to the *Work* itself.  \n\n**4.3** Any other use without of prior consent of Licensors is prohibited. Research  \nusers explicitly acknowledge having received from Licensors all information  \nallowing to appreciate the adequacy between of the *Software* and their needs and  \nto undertake all necessary precautions for its execution and use.  \n\n**4.4** The *Software* is provided both as a compiled library file and as source  \ncode. In case of using the *Software* for a publication or other results obtained  \nthrough the use of the *Software*, users are strongly encouraged to cite the  \ncorresponding publications as explained in the documentation of the *Software*.  \n\n## 5.  Disclaimer  \n\nTHE USER CANNOT USE, EXPLOIT OR DISTRIBUTE THE *SOFTWARE* FOR COMMERCIAL PURPOSES  \nWITHOUT PRIOR AND EXPLICIT CONSENT OF LICENSORS. YOU MUST CONTACT INRIA FOR ANY  \nUNAUTHORIZED USE: stip-sophia.transfert@inria.fr . ANY SUCH ACTION WILL  \nCONSTITUTE A FORGERY. THIS *SOFTWARE* IS PROVIDED \"AS IS\" WITHOUT ANY WARRANTIES  \nOF ANY NATURE AND ANY EXPRESS OR IMPLIED WARRANTIES, WITH REGARDS TO COMMERCIAL  \nUSE, PROFESSIONNAL USE, LEGAL OR NOT, OR OTHER, OR COMMERCIALISATION OR  \nADAPTATION. UNLESS EXPLICITLY PROVIDED BY LAW, IN NO EVENT, SHALL INRIA OR THE  \nAUTHOR BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR  \nCONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE  \nGOODS OR SERVICES, LOSS OF USE, DATA, OR PROFITS OR BUSINESS INTERRUPTION)  \nHOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT  \nLIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING FROM, OUT OF OR  \nIN CONNECTION WITH THE *SOFTWARE* OR THE USE OR OTHER DEALINGS IN THE *SOFTWARE*.  \n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 33.4130859375,
          "content": "<div align=\"center\">\n\n# SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering\n\n<font size=\"4\">\nCVPR 2024\n</font>\n<br>\n\n<font size=\"4\">\n<a href=\"https://anttwo.github.io/\" style=\"font-size:100%;\">Antoine Gu√©don</a>&emsp;\n<a href=\"https://vincentlepetit.github.io/\" style=\"font-size:100%;\">Vincent Lepetit</a>&emsp;\n</font>\n<br>\n\n<font size=\"4\">\nLIGM, Ecole des Ponts, Univ Gustave Eiffel, CNRS\n</font>\n\n| <a href=\"https://anttwo.github.io/sugar/\">Webpage</a> | <a href=\"https://arxiv.org/abs/2311.12775\">arXiv</a> | <a href=\"https://github.com/Anttwo/sugar_frosting_blender_addon/\">Blender add-on</a> | <a href=\"https://www.youtube.com/watch?v=MAkFyWfiBQo\">Presentation video</a> | <a href=\"https://www.youtube.com/watch?v=YbjE0wnw67I\">Viewer video</a> |\n\n<img src=\"./media/examples/walk.gif\" alt=\"walk.gif\" width=\"350\"/><img src=\"./media/examples/attack.gif\" alt=\"attack.gif\" width=\"350\"/> <br>\n<b>Our method extracts meshes from 3D Gaussian Splatting reconstructions and builds hybrid representations <br>that enable easy composition and animation in Gaussian Splatting scenes by manipulating the mesh.</b>\n</div>\n\n## Abstract\n\n_We propose a method to allow precise and extremely fast mesh extraction from <a href=\"https://repo-sam.inria.fr/fungraph/3d-gaussian-splatting/\">3D Gaussian Splatting (SIGGRAPH 2023)</a>.\nGaussian Splatting has recently become very popular as it yields realistic rendering while being significantly faster to train than NeRFs. It is however challenging to extract a mesh from the millions of tiny 3D Gaussians as these Gaussians tend to be unorganized after optimization and no method has been proposed so far.\nOur first key contribution is a regularization term that encourages the 3D Gaussians to align well with the surface of the scene.\nWe then introduce a method that exploits this alignment to sample points on the real surface of the scene and extract a mesh from the Gaussians using Poisson reconstruction, which is fast, scalable, and preserves details, in contrast to the Marching Cubes algorithm usually applied to extract meshes from Neural SDFs.\nFinally, we introduce an optional refinement strategy that binds Gaussians to the surface of the mesh, and jointly optimizes these Gaussians and the mesh through Gaussian splatting rendering. This enables easy editing, sculpting, rigging, animating, or relighting of the Gaussians using traditional softwares (Blender, Unity, Unreal Engine, etc.) by manipulating the mesh instead of the Gaussians themselves.\nRetrieving such an editable mesh for realistic rendering is done within minutes with our method, compared to hours with the state-of-the-art method on neural SDFs, while providing a better rendering quality in terms of PSNR, SSIM and LPIPS._\n\n<div align=\"center\">\n<b>Hybrid representation (Mesh + Gaussians on the surface)</b><br>\n<img src=\"./media/overview/garden_hybrid.gif\" alt=\"garden_hybrid.gif\" width=\"250\"/>\n<img src=\"./media/overview/kitchen_hybrid.gif\" alt=\"kitchen_hybrid.gif\" width=\"250\"/>\n<img src=\"./media/overview/counter_hybrid.gif\" alt=\"counter_hybrid.gif\" width=\"250\"/><br>\n<img src=\"./media/overview/playroom_hybrid.gif\" alt=\"playroom_hybrid.gif\" width=\"323\"/>\n<img src=\"./media/overview/qant03_hybrid.gif\" alt=\"qant03_hybrid.gif\" width=\"323\"/>\n<img src=\"./media/overview/dukemon_hybrid.gif\" alt=\"_hybrid.gif\" width=\"102\"/><br>\n<b>Underlying mesh without texture</b><br>\n<img src=\"./media/overview/garden_notex.gif\" alt=\"garden_notex.gif\" width=\"250\"/>\n<img src=\"./media/overview/kitchen_notex.gif\" alt=\"kitchen_notex.gif\" width=\"250\"/>\n<img src=\"./media/overview/counter_notex.gif\" alt=\"counter_notex.gif\" width=\"250\"/><br>\n<img src=\"./media/overview/playroom_notex.gif\" alt=\"playroom_notex.gif\" width=\"323\"/>\n<img src=\"./media/overview/qant03_notex.gif\" alt=\"qant03_notex.gif\" width=\"323\"/>\n<img src=\"./media/overview/dukemon_notex.gif\" alt=\"dukemon_notex.gif\" width=\"102\"/><br>\n</div>\n\n\n## BibTeX\n\n```\n@article{guedon2023sugar,\n  title={SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction and High-Quality Mesh Rendering},\n  author={Gu{\\'e}don, Antoine and Lepetit, Vincent},\n  journal={CVPR},\n  year={2024}\n}\n```\n\n## Updates and To-do list\n\n<details>\n<summary><span style=\"font-weight: bold;\">Updates</span></summary>\n<ul>\n  <li><b>[09/18/2024]</b> Improved the quality of the extracted meshes with the new `dn_consistency` regularization method, and added compatibility with the new Blender add-on for composition and animation. </li>\n  <li><b>[01/09/2024]</b> Added a dedicated, real-time viewer to let users visualize and navigate in the reconstructed scenes (hybrid representation, textured mesh and wireframe mesh).</li>\n  <li><b>[12/20/2023]</b> Added a short notebook showing how to render images with the hybrid representation using the Gaussian Splatting rasterizer.</li>\n  <li><b>[12/18/2023]</b> Code release.</li>\n</ul>\n</details><br>\n\n<details>\n<summary><span style=\"font-weight: bold;\">To-do list</span></summary>\n<ul>\n  <li><b>Viewer:</b> Add option to load the postprocessed mesh.</li>\n  <li><b>Mesh extraction:</b> Add the possibility to edit the extent of the background bounding box.</li>\n  <li><b>Tips&Tricks:</b> Add to the README.md file (and the webpage) some tips and tricks for using SuGaR on your own data and obtain better reconstructions (see the tips provided by user kitmallet).</li>\n  <li><b>Improvement:</b> Add an <code>if</code> block to <code>sugar_extractors/coarse_mesh.py</code> to skip foreground mesh reconstruction and avoid triggering an error if no surface point is detected inside the foreground bounding box. This can be useful for users that want to reconstruct \"<i>background scenes</i>\". </li>\n  <li><b>Using precomputed masks with SuGaR:</b> Add a mask functionality to the SuGaR optimization, to allow the user to mask out some pixels in the training images (like white backgrounds in synthetic datasets).\n  </li>\n  <li><b>Using SuGaR with Windows:</b> Adapt the code to make it compatible with Windows. Due to path-writing conventions, the current code is not compatible with Windows. \n  </li>\n  <li><b>Synthetic datasets:</b> Add the possibility to use the NeRF synthetic dataset (which has a different format than COLMAP scenes)\n  </li>\n  <li><b>Composition and animation:</b> Finish to clean the code for composition and animation, and add it to the <code>sugar_scene/sugar_compositor.py</code> script.\n  </li>\n  <li><b>Composition and animation:</b> Make a tutorial on how to use the scripts in the <code>blender</code> directory and the <code>sugar_scene/sugar_compositor.py</code> class to import composition and animation data into PyTorch and apply it to the SuGaR hybrid representation.\n  </li>\n  <!-- <li><b>Improvement:</b> Implement a simple method to avoid artifacts when reconstructing thin objects with poor coverage/visibility in the training images.</li>\n  </li> -->\n</ul>\n</details>\n\n## Overview\n\nAs we explain in the paper, SuGaR optimization starts with first optimizing a 3D Gaussian Splatting model for 7k iterations with no additional regularization term. Consequently, the current implementation contains a version of the original <a href=\"https://github.com/graphdeco-inria/gaussian-splatting\">3D Gaussian Splatting code</a>, and we built our model as a wrapper of a vanilla 3D Gaussian Splatting model.\nPlease note that, even though this wrapper implementation is convenient for many reasons, it may not be the most optimal one for memory usage.\n\nThe full SuGaR pipeline consists of 4 main steps, and an optional one:\n1. **Short vanilla 3DGS optimization**: optimizing a vanilla 3D Gaussian Splatting model for 7k iterations, in order to let Gaussians position themselves in the scene.\n2. **SuGaR optimization**: optimizing Gaussians alignment with the surface of the scene.\n3. **Mesh extraction**: extracting a mesh from the optimized Gaussians.\n4. **SuGaR refinement**: refining the Gaussians and the mesh together to build a hybrid Mesh+Gaussians representation.\n5. **Textured mesh extraction (Optional)**: extracting a traditional textured mesh from the refined SuGaR model as a tool for visualization, composition and animation in Blender using our <a href=\"https://github.com/Anttwo/sugar_frosting_blender_addon/\">Blender add-on</a>.\n\nWe provide a dedicated script for each of these steps, as well as a script `train_full_pipeline.py` that runs the entire pipeline. We explain how to use this script in the next sections. <br>\n\n<div align=\"center\"><br>\n<img src=\"./media/blender/blender_edit.png\" alt=\"blender_edit.png\" height=\"200\"/>\n<img src=\"./media/examples/attack.gif\" alt=\"attack.gif\" height=\"200\"/>\n<br><b>You can visualize, edit, combine or animate the reconstructed textured meshes in Blender <i>(left)</i> <br>and render the result with SuGaR <i>(right)</i> thanks to our <a href=\"https://github.com/Anttwo/sugar_frosting_blender_addon/\">Blender add-on</a>.</b><br>\n</div><br>\n\nPlease note that the final step, _Textured mesh extraction_, is optional but is enabled by default in the `train_full_pipeline.py` script. Indeed, it is very convenient to have a traditional textured mesh for visualization, composition and animation using traditional softwares such as <a href=\"https://github.com/Anttwo/sugar_frosting_blender_addon/\">Blender</a>. If you installed Nvdiffrast as described below, this step should only take a few seconds anyway.\n\n<div align=\"center\">\n<b>Hybrid representation (Mesh + Gaussians on the surface)</b><br>\n<img src=\"./media/overview/garden_hybrid.png\" alt=\"garden_hybrid.gif\" height=\"135\"/>\n<img src=\"./media/overview/kitchen_hybrid.png\" alt=\"kitchen_hybrid.gif\" height=\"135\"/>\n<img src=\"./media/overview/qant03_hybrid.png\" alt=\"qant03_hybrid.gif\" height=\"135\"/>\n<img src=\"./media/overview/dukemon_hybrid.png\" alt=\"_hybrid.gif\" height=\"135\"/><br>\n<b>Underlying mesh with a traditional colored UV texture</b><br>\n<img src=\"./media/overview/garden_texture.png\" alt=\"garden_notex.gif\" height=\"135\"/>\n<img src=\"./media/overview/kitchen_texture.png\" alt=\"kitchen_notex.gif\" height=\"135\"/>\n<img src=\"./media/overview/qant03_texture.png\" alt=\"qant03_notex.gif\" height=\"135\"/>\n<img src=\"./media/overview/dukemon_texture.png\" alt=\"dukemon_notex.gif\" height=\"135\"/><br>\n</div><br>\n\nBelow is another example of a scene showing a robot with a black and specular material. The following images display the hybrid representation (Mesh + Gaussians on the surface), the mesh with a traditional colored UV texture, and a depth map of the mesh:\n<div align=\"center\">\n<b>Hybrid representation - Textured mesh - Depth map of the mesh</b><br>\n<img src=\"./media/examples/alpha_hybrid.png\" alt=\"alpha_hybrid.png\" height=\"400\"/>\n<img src=\"./media/examples/alpha_texture.png\" alt=\"alpha_texture.gif\" height=\"400\"/>\n<img src=\"./media/examples/alpha_depth.png\" alt=\"alpha_depth.gif\" height=\"400\"/>\n</div>\n\n## Installation\n\n<details>\n<summary><span style=\"font-weight: bold;\">Click here to see content.</span></summary>\n\n### 0. Requirements\n\nThe software requirements are the following:\n- Conda (recommended for easy setup)\n- C++ Compiler for PyTorch extensions\n- CUDA toolkit 11.8 for PyTorch extensions\n- C++ Compiler and CUDA SDK must be compatible\n\nPlease refer to the original <a href=\"https://github.com/graphdeco-inria/gaussian-splatting\">3D Gaussian Splatting repository</a> for more details about requirements.\n\n### 1. Clone the repository\n\nStart by cloning this repository:\n\n```shell\n# HTTPS\ngit clone https://github.com/Anttwo/SuGaR.git --recursive\n```\n\nor\n\n```shell\n# SSH\ngit clone git@github.com:Anttwo/SuGaR.git --recursive\n```\n\n### 2. Creating the Conda environment\n\nTo create and activate the Conda environment with all the required packages, go inside the `SuGaR/` directory and run the following command:\n\n```shell\npython install.py\nconda activate sugar\n```\n\nThis script will automatically create a Conda environment named `sugar` and install all the required packages. It will also automatically install the <a href=\"https://github.com/graphdeco-inria/gaussian-splatting\">3D Gaussian Splatting</a> rasterizer as well as the <a href=\"https://nvlabs.github.io/nvdiffrast/\">Nvdiffrast</a> library for faster mesh rasterization.\n\nIf you encounter any issues with the installation, you can try to follow the detailed instructions below to install the required packages manually.\n\n<details>\n<summary><span style=\"font-weight: bold;\">\nDetailed instructions for manual installation\n</span></summary>\n\n#### a) Install the required Python packages\nTo install the required Python packages and activate the environment, go inside the `SuGaR/` directory and run the following commands:\n\n```shell\nconda env create -f environment.yml\nconda activate sugar\n```\n\nIf this command fails to create a working environment, you can try to install the required packages manually by running the following commands:\n```shell\nconda create --name sugar -y python=3.9\nconda activate sugar\nconda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.8 -c pytorch -c nvidia\nconda install -c fvcore -c iopath -c conda-forge fvcore iopath\nconda install pytorch3d==0.7.4 -c pytorch3d\nconda install -c plotly plotly\nconda install -c conda-forge rich\nconda install -c conda-forge plyfile==0.8.1\nconda install -c conda-forge jupyterlab\nconda install -c conda-forge nodejs\nconda install -c conda-forge ipywidgets\npip install open3d\npip install --upgrade PyMCubes\n```\n\n#### b) Install the Gaussian Splatting rasterizer\n\nRun the following commands inside the `SuGaR` directory to install the additional Python submodules required for Gaussian Splatting:\n\n```shell\ncd gaussian_splatting/submodules/diff-gaussian-rasterization/\npip install -e .\ncd ../simple-knn/\npip install -e .\ncd ../../../\n```\nPlease refer to the <a href=\"https://github.com/graphdeco-inria/gaussian-splatting\">3D Gaussian Splatting repository</a> for more details.\n\n#### c) (Optional) Install Nvdiffrast for faster Mesh Rasterization\n\nInstalling Nvdiffrast is optional but will greatly speed up the textured mesh extraction step, from a few minutes to less than 10 seconds.\n\n```shell\ngit clone https://github.com/NVlabs/nvdiffrast\ncd nvdiffrast\npip install .\ncd ../\n```\n\n</details>\n\n</details>\n\n\n## Quick Start\n\n<details>\n<summary><span style=\"font-weight: bold;\">Click here to see content.</span></summary>\n\n### Training from scratch\n\nYou can run the following single script to optimize a full SuGaR model from scratch using a COLMAP dataset:\n\n```shell\npython train_full_pipeline.py -s <path to COLMAP dataset> -r <\"dn_consistency\", \"density\" or \"sdf\"> --high_poly True --export_obj True\n```\n\nYou can choose the regularization method with the `-r` argument, which can be `\"dn_consistency\"`, `\"density\"` or `\"sdf\"`. We recommend using the newer `\"dn_consistency\"` regularization for best quality meshes, but the results presented in the paper were obtained with the `\"density\"` regularization for object-centered scenes and `\"sdf\"` for scenes with a challenging background, such as the Mip-NeRF 360 dataset.\n\nYou can also replace the `--high_poly True` argument with `--low_poly True` to extract a mesh with 200k vertices instead of 1M, and 6 Gaussians per triangle instead of 1.\n\nMoreover, you can add `--refinement_time \"short\"`, `\"medium\"` or `\"long\"` to set the time spent on the refinement step. The default is `\"long\"` (15k iterations), but `\"short\"` (2k iterations) can be enough to produce a good-looking hybrid representation.\n\nFinally, you can choose to export a traditional textured mesh with the `--export_obj` argument. This step is optional but is enabled by default in the `train_full_pipeline.py` script, as the mesh is required for using the <a href=\"https://github.com/Anttwo/sugar_frosting_blender_addon/\">Blender add-on</a> and editing, combining or animating scenes in Blender.\n\nResults are saved in the `output/` directory.\n\n<details>\n<summary><span style=\"font-weight: bold;\">Please click here to see the most important arguments for the `train_full_pipeline.py` script.</span></summary>\n\n| Parameter | Type | Description |\n| :-------: | :--: | :---------: |\n| `--scene_path` / `-s`   | `str` | Path to the source directory containing a COLMAP dataset.|\n| `--gs_output_dir` | `str` | Path to the checkpoint directory of a vanilla 3D Gaussian Splatting model. If no path is provided, the script will start from scratch and first optimize a vanilla 3DGS model. |\n| `--regularization_type` / `-r` | `str` | Type of regularization to use for aligning Gaussians. Can be `\"dn_consistency\"`, `\"density\"` or `\"sdf\"`. We recommend using the newer `\"dn_consistency\"` regularization for best quality meshes. |\n| `--eval` | `bool` | If True, performs an evaluation split of the training images. Default is `True`. |\n| `--low_poly` | `bool` | If True, uses the standard config for a low poly mesh, with `200_000` vertices and `6` Gaussians per triangle. |\n| `--high_poly` | `bool` | If True, uses the standard config for a high poly mesh, with `1_000_000` vertices and `1` Gaussian per triangle. |\n| `--refinement_time` | `str` | Default configs for time to spend on refinement. Can be `\"short\"` (2k iterations), `\"medium\"` (7k iterations) or `\"long\"` (15k iterations). |\n| `--export_ply` | `bool` | If True, export a `.ply` file with the refined 3D Gaussians at the end of the training. This file can be large (+/- 500MB), but is needed for using 3DGS viewers. Default is `True`. |\n| `--export_obj` / `-t` | `bool` | If True, will optimize and export a traditional textured mesh as an `.obj` file from the refined SuGaR model, after refinement. Computing a traditional color UV texture should just take a few seconds with Nvdiffrast. Default is `True`. |\n| `--square_size` | `int` | Size of the square allocated to each pair of triangles in the UV texture. Increase for higher texture resolution. Please decrease if you encounter memory issues. Default is `8`. |\n|`--white_background` | `bool` | If True, the background of the images will be set to white. Default is `False`. |\n\n\n</details>\n<br>\n\nAs we explain in the paper, this script extracts a mesh in 30 minutes on average on a single GPU. After mesh extraction, the refinement time only takes a few minutes when using `--refinement_time \"short\"`, but can be much longer when using `--refinement_time \"long\"`. A short refinement time is enough to produce a good-looking hybrid representation in most cases.\n\nPlease note that the optimization time may vary depending on the complexity of the scene and the GPU used. Moreover, the current implementation splits the optimization into modular scripts that can be run separately so it reloads the data at each part, which is not optimal and takes several minutes.\n\nPlease see the `train_full_pipeline.py` for more details on all the command line arguments.\n\n### Training from a vanilla Gaussian Splatting model\n\nIf you have already trained a <a href=\"https://github.com/graphdeco-inria/gaussian-splatting\">vanilla Gaussian Splatting model</a> for a scene (we recommend training it for 7k iterations), you can use the `--gs_output_dir` argument to provide the path to the output directory of the vanilla Gaussian Splatting model. This will skip the first part of the optimization and directly load the Gaussians from the vanilla model:\n\n```shell\npython train_full_pipeline.py -s <path to COLMAP dataset> -r <\"dn_consistency\", \"density\" or \"sdf\"> --high_poly True --export_obj True --gs_output_dir <path to the Gaussian Splatting output directory>\n```\n\n</details>\n\n\n## Visualize a SuGaR model in real-time\n\n<details>\n<summary><span style=\"font-weight: bold;\">Click here to see content.</span></summary><br>\n\nAfter optimizing a SuGaR model, you can visualize the model in real-time using the 3D Gaussian Splatting viewer of your choice. <br>\nIndeed, after optimization, we automatically export a `.ply` file in the `./output/refined_ply/` directory, containing the refined 3D Gaussians of SuGaR's hybrid representation and compatible with any 3DGS viewer.\nFor instance, you can use the viewer provided in the original implementation of <a href=\"https://github.com/graphdeco-inria/gaussian-splatting\">3D Gaussian Splatting</a>, or the awesome <a href=\"https://github.com/playcanvas/supersplat\">SuperSplat viewer</a>. <br>\nAn online, <a href=\"https://playcanvas.com/supersplat/editor\">in-browser version of SuperSplat</a> is also available.\n\nWe also propose a dedicated real-time viewer that allows you to visualize not only the refined 3D Gaussians but also the textured mesh and the wireframe mesh. Please see the instructions below to install and use this viewer.\n\n<details>\n<summary><span style=\"font-weight: bold;\">Please click here to see the instructions for installing and using our real-time viewer.</span></summary>\n\nPlease find <a href=\"https://www.youtube.com/watch?v=YbjE0wnw67I\">here</a> a short video illustrating how to use the viewer.\n\n### 1. Installation\n\n*The viewer is currently built for Linux and Mac OS. It is not compatible with Windows. For Windows users, we recommend to use WSL2 (Windows Subsystem for Linux), as it is very easy to install and use. Please refer to the <a href=\"https://docs.microsoft.com/en-us/windows/wsl/install-win10\">official documentation</a> for more details.<br> We thank <a href=\"https://github.com/mkkellogg/GaussianSplats3D\">Mark Kellogg for his awesome 3D Gaussian Splatting implementation for Three.js</a>, which we used for building this viewer.*\n\nPlease start by installing the latest versions of Node.js (such as 21.x) and npm.\nA simple way to do this is to run the following commands (using aptitude):\n\n```shell\ncurl -fsSL https://deb.nodesource.com/setup_21.x | sudo -E bash -\nsudo apt-get install -y nodejs\nsudo apt-get install aptitude\nsudo aptitude install -y npm\n```\n\nThen, go inside the `./sugar_viewer/` directory and run the following commands:\n\n```shell\nnpm install\ncd ..\n```\n\n### 2. Usage\n\nFirst, make sure you have exported a `.ply` file and an `.obj` file using the `train.py` script. The `.ply` file contains the refined 3D Gaussians, and the `.obj` file contains the textured mesh. These files are exported by default when running the `train.py` script, so if you ran the code with default values for `--export_ply` and `--export_obj`, you should be good to go.\n\nThe ply file should be located in `./output/refined_ply/<your scene name>/`. Then, just run the following command in the root directory to start the viewer:\n\n```shell\npython run_viewer.py -p <path to the .ply file>\n```\nPlease make sure your `.ply` file is located in the right folder, and use a relative path starting with `./output/refined_ply`.\nThis command will redirect you to a local URL. Click on the link to open the viewer in your browser. Click the icons on the top right to switch between the different representations (hybrid representation, textured mesh, wireframe mesh). Use the mouse to rotate the scene, and the mouse wheel to zoom in and out. \n\n</details>\n\n<div align=\"center\" >\n<img src=\"./media/examples/viewer_example.png\" alt=\"viewer_example.png\" width=\"800\"/>\n</div><br>\n\nWe also recommend using our <a href=\"https://github.com/Anttwo/sugar_frosting_blender_addon/\">Blender add-on</a> to create animations and video clips of SuGaR representations. \nMore specifically, the add-on allows you to import SuGaR meshes and visualize, edit, combine or animate them in Blender.\nFinally, you can render the result using the 3DGS rasterizer, which provides high-quality and realistic rendering of SuGaR's hybrid representation.\n\n</details>\n\n\n## Rendering, composition and animation with Blender\n\n<details>\n<summary><span style=\"font-weight: bold;\">Click here to see content.</span></summary><br>\n\nThe `view_sugar_results.ipynb` notebook and the `metrics.py` script provides examples of how to load a refined SuGaR model for rendering a scene.\n\nWe also provide a <a href=\"https://github.com/Anttwo/sugar_frosting_blender_addon/\">Blender add-on</a> for editing, combining, and animating SuGaR meshes within Blender and render the result using SuGaR's hybrid representations. Meshes are located in the `./output/refined_mesh/` directory.\n\nPlease refer to the Blender add-on repository for more details on how to use the add-on and create a rendering package for SuGaR.\nAfter preparing the rendering package with Blender, which should be a `.JSON` file located in the `./output/blender/package/` directory, you can render the scene using the `render_blender_scene.py` script:\n\n```shell\npython render_blender_scene.py -p <Path to the rendering package>\n```\n\nThe rendered images will be saved in the `./output/blender/renders/` directory.<br>\nFeel free to adjust the arguments of the script to adjust the rendering quality if you observe artifacts in the images.\nSpecifically, you can switch to `--adaptation_method simple` or reduce `deformation_threshold` to mitigate artifacts in the rendering.\nPlease refer to the script for more details on the command line arguments.\n\n</details>\n\n\n## Evaluation\n\n<details>\n<summary><span style=\"font-weight: bold;\">Click here to see content.</span></summary><br>\n\nTo evaluate the quality of the reconstructions, we provide a script `metrics.py` that computes the PSNR, SSIM and LPIPS metrics on test images. Start by optimizing SuGaR models for the desired scenes and a regularization method (`\"dn_consistency\"`, `\"density\"` or `\"sdf\"`), then create a `.json` config file containing the paths to the scenes in the following format: `{source_images_dir_path: vanilla_gaussian_splatting_checkpoint_path}`.\n\nFinally, run the script as follows:\n\n```shell\npython metrics.py --scene_config <Path to the .json file> -r <\"sdf\" or \"density\"> \n```\n\nResults are saved in a `.json` file in the `output/metrics/` directory. \nPlease refer to the script for more details on the command line arguments.\n\n</details>\n\n## Tips for using SuGaR on your own data and obtain better reconstructions\n\n<details>\n<summary><span style=\"font-weight: bold;\">Click here to see content.</span></summary><br>\n\n<details>\n<summary><span style=\"font-weight: bold;\">1. Capture images or videos that cover the entire surface of the scene</span></summary><br>\n\nUsing a smartphone or a camera, capture images or a video that cover the entire surface of the 3D scene you want to reconstruct. The easiest way to do this is to move around the scene while recording a video. Try to move slowly and smoothly in order to avoid motion blur. For consistent reconstruction and easier camera pose estimation with COLMAP, maintaining a uniform focal length and a constant exposure time is also important. We recommend to disable auto-focus on your smartphone to ensure that the focal length remains constant.\n\nFor better reconstructions, try to cover objects from several and different angles, especially for thin and detailed parts of the scene. \nIndeed, SuGaR is able to reconstruct very thin and detailed objects, but some artifacts may appear if these thin objects are not covered enough and are visible only from one side in the training images.\n\nTo convert a video to images, you can install `ffmpeg` and run the following command:\n```shell\nffmpeg -i <Path to the video file> -qscale:v 1 -qmin 1 -vf fps=<FPS> %04d.jpg\n```\nwhere `<FPS>` is the desired sampling rate of the video images. An FPS value of 1 corresponds to sampling one image per second. We recommend to adjust the sampling rate to the length of the video, so that the number of sampled images is between 100 and 300.\n\n</details>\n\n<details>\n<summary><span style=\"font-weight: bold;\">2. Estimate camera poses with COLMAP</span></summary><br>\n\nPlease first install a recent version of COLMAP (ideally CUDA-powered) and make sure to put the images you want to use in a directory `<location>/input`. Then, run the script `gaussian_splatting/convert.py` from the original Gaussian splatting implementation to compute the camera poses from the images using COLMAP. Please refer to the original <a href=\"https://github.com/graphdeco-inria/gaussian-splatting\">3D Gaussian Splatting repository</a> for more details.\n\n```shell\npython gaussian_splatting/convert.py -s <location>\n```\n\nSometimes COLMAP fails to reconstruct all images into the same model and hence produces multiple sub-models. The smaller sub-models generally contain only a few images. However, by default, the script `convert.py` will apply Image Undistortion only on the first sub-model, which may contain only a few images.\n\nIf this is the case, a simple solution is to keep only the largest sub-model and discard the others. To do this, open the source directory containing your input images, then open the sub-directory `<Source_directory>/distorted/sparse/`. You should see several sub-directories named `0/`, `1/`, etc., each containing a sub-model. Remove all sub-directories except the one containing the largest files, and rename it to `0/`. Then, run the script `convert.py` one more time but skip the matching process:\n\n```shell\npython gaussian_splatting/convert.py -s <location> --skip_matching\n```\n\n_Note: If the sub-models have common registered images, they could be merged into a single model as post-processing step using COLMAP; However, merging sub-models requires to run another global bundle adjustment after the merge, which can be time consuming._\n</details>\n\n\n<details>\n<summary><span style=\"font-weight: bold;\">3. DN-Consistency, Density or SDF? Choose a regularization method that fits your scene</span></summary><br>\n\n**We recommend using the newer DN-Consistency regularization for best quality meshes**.\n\nHowever, the results presented in the paper were obtained with the Density regularization for object-centered scenes and the SDF regularization for scenes with a challenging background, such as the Mip-NeRF 360 dataset.\n\nAs we explain in the paper, the density regularization is the simplest one and works well with objects centered in the scene. The SDF provides a stronger regularization, especially in background regions. \nAs a consequence, the SDF regularization produces higher metrics on standard datasets. \nHowever, for reconstructing an object centered in the scene with images taken from all around the object, the simpler density regularization generally produces a better mesh than SDF.\n\nThe DN-Consistency regularization is a new regularization method that (a) enforces the Gaussians to align with the surface of the scene with the density regularization, while also (b) enforcing the consistency between the gradient of the depth and the normal maps, all rendered using the 3D Gaussians. <br>\nAs described in the paper, the normal of a Gaussian is defined as the shortest axis of the covariance matrix of the Gaussian. <br> \nThis regularization method provides the best quality meshes.\n\n</details>\n\n<details>\n<summary><span style=\"font-weight: bold;\">4. I have holes in my mesh, what can I do?</span></summary><br>\n\nIf you have holes in your mesh, this means the cleaning step of the Poisson mesh is too aggressive for your scene. You can reduce the treshold `vertices_density_quantile` used for cleaning by modifying line 43 of `sugar_extractors/coarse_mesh.py`. For example, you can change this line from\n```python\n  vertices_density_quantile = 0.1\n```\nto\n```python\n  vertices_density_quantile = 0.\n```\n\n</details>\n\n<details>\n<summary><span style=\"font-weight: bold;\">5. I have messy ellipsoidal bumps on the surface of my mesh, what can I do?</span></summary><br>\n\nDepending on your scene, the default hyperparameters used for Poisson reconstruction may be too fine compared to the size of the Gaussians. Gaussian could then become visible on the mesh, which results in messy ellipsoidal bumps on the surface of the mesh.\nThis could happen if the camera trajectory is very close to a simple foreground object, for example.<br>\nTo fix this, you can reduce the depth of Poisson reconstruction `poisson_depth` by modifying line 42 of `sugar_extractors/coarse_mesh.py`. <br>\nFor example, you can change line 42 from\n```python\n  poisson_depth = 10\n```\nto\n```python\n  poisson_depth = 7\n```\nYou may also try `poisson_depth = 6`, or `poisson_depth = 8` if the result is not satisfying.\n\n</details>\n\n\n<details>\n<summary><span style=\"font-weight: bold;\">6. (Optional) Adapt the scale and the bounding box of the scene</span></summary><br>\n\nAs it is explained in the original <a href=\"https://github.com/graphdeco-inria/gaussian-splatting\">3D Gaussian Splatting repository</a>, the method is expected to reconstruct a scene with reasonable scale. For reconstructing much larger datasets, like a city district, the original authors recommend to lower the learning rates of the positions and scaling factors of the Gaussians. The more extensive the scene, the lower these values should be.\n\nConcerning SuGaR, such learning rates should also be lowered when reconstructing a very large scene. Moreover, as we explain in the supplementary material of the paper, for extracting a mesh from the Gaussians with an optimal repartition of vertices, we apply two Poisson reconstructions in practice: one on _foreground_ Gaussians, and one on _background_ Gaussians. The foreground Gaussians are defined as the Gaussians located inside a predefined bounding box, and the background Gaussians are defined as the Gaussians located outside this bounding box. \n\nBy default, this bounding box is computed as the bounding box of all camera centers. This general approach is coherent with how the original 3D Gaussian Splatting scales the learning rates. We used this default bounding box for all the reconstructions shown in the paper and the presentation video.\n\nHowever, this bounding box might not be optimal in very specific cases, especially when the user wants to reconstruct with high details a very specific object located somewhere in the scene, or if the scene is very large, or if the camera centers are very far from the scene.\nThe user is free to provide a custom bounding box to the `train.py` script, using the parameters `--bboxmin` and `--bboxmax`. Please note that the bounding box must be provided as strings, formatted as `\"(x,y,z)\"`, where `x`, `y` and `z` are the coordinates of the min and max points of the bounding box.\n\n</details>\n\n</details>"
        },
        {
          "name": "blender",
          "type": "tree",
          "content": null
        },
        {
          "name": "configs",
          "type": "tree",
          "content": null
        },
        {
          "name": "environment.yml",
          "type": "blob",
          "size": 7.3818359375,
          "content": "name: sugar\nchannels:\n  - pytorch3d\n  - plotly\n  - iopath\n  - pytorch\n  - nvidia\n  - conda-forge\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=main\n  - _openmp_mutex=5.1=1_gnu\n  - anyio=4.1.0=pyhd8ed1ab_0\n  - argon2-cffi=21.1.0=py39h3811e60_2\n  - arrow=1.3.0=pyhd8ed1ab_0\n  - asttokens=2.4.1=pyhd8ed1ab_0\n  - async-lru=2.0.4=pyhd8ed1ab_0\n  - attrs=23.1.0=pyh71513ae_1\n  - babel=2.14.0=pyhd8ed1ab_0\n  - beautifulsoup4=4.12.2=pyha770c72_0\n  - blas=1.0=mkl\n  - bleach=6.1.0=pyhd8ed1ab_0\n  - brotli-python=1.0.9=py39h6a678d5_7\n  - bzip2=1.0.8=h7b6447c_0\n  - ca-certificates=2023.11.17=hbcca054_0\n  - cached-property=1.5.2=hd8ed1ab_1\n  - cached_property=1.5.2=pyha770c72_1\n  - certifi=2023.11.17=pyhd8ed1ab_0\n  - cffi=1.16.0=py39h5eee18b_0\n  - charset-normalizer=2.0.4=pyhd3eb1b0_0\n  - colorama=0.4.6=pyhd8ed1ab_0\n  - cryptography=41.0.7=py39hdda0065_0\n  - cuda-cudart=11.8.89=0\n  - cuda-cupti=11.8.87=0\n  - cuda-libraries=11.8.0=0\n  - cuda-nvrtc=11.8.89=0\n  - cuda-nvtx=11.8.86=0\n  - cuda-runtime=11.8.0=0\n  - decorator=5.1.1=pyhd8ed1ab_0\n  - defusedxml=0.7.1=pyhd8ed1ab_0\n  - entrypoints=0.4=pyhd8ed1ab_0\n  - exceptiongroup=1.2.0=pyhd8ed1ab_0\n  - executing=2.0.1=pyhd8ed1ab_0\n  - ffmpeg=4.3=hf484d3e_0\n  - filelock=3.13.1=py39h06a4308_0\n  - fqdn=1.5.1=pyhd8ed1ab_0\n  - freetype=2.12.1=h4a9f257_0\n  - fvcore=0.1.5.post20221221=pyhd8ed1ab_0\n  - giflib=5.2.1=h5eee18b_3\n  - gmp=6.2.1=h295c915_3\n  - gmpy2=2.1.2=py39heeb90bb_0\n  - gnutls=3.6.15=he1e5248_0\n  - idna=3.4=py39h06a4308_0\n  - importlib-metadata=7.0.0=pyha770c72_0\n  - importlib_metadata=7.0.0=hd8ed1ab_0\n  - importlib_resources=6.1.1=pyhd8ed1ab_0\n  - intel-openmp=2023.1.0=hdb19cb5_46306\n  - iopath=0.1.9=py39\n  - ipykernel=5.5.5=py39hef51801_0\n  - ipython=8.18.1=pyh707e725_3\n  - ipython_genutils=0.2.0=py_1\n  - ipywidgets=8.1.1=pyhd8ed1ab_0\n  - isoduration=20.11.0=pyhd8ed1ab_0\n  - jedi=0.19.1=pyhd8ed1ab_0\n  - jinja2=3.1.2=py39h06a4308_0\n  - jpeg=9e=h5eee18b_1\n  - json5=0.9.14=pyhd8ed1ab_0\n  - jsonpointer=2.4=py39hf3d152e_3\n  - jsonschema=4.20.0=pyhd8ed1ab_0\n  - jsonschema-specifications=2023.11.2=pyhd8ed1ab_0\n  - jsonschema-with-format-nongpl=4.20.0=pyhd8ed1ab_0\n  - jupyter-lsp=2.2.1=pyhd8ed1ab_0\n  - jupyter_client=8.6.0=pyhd8ed1ab_0\n  - jupyter_core=5.5.0=py39hf3d152e_0\n  - jupyter_events=0.9.0=pyhd8ed1ab_0\n  - jupyter_server=2.12.1=pyhd8ed1ab_0\n  - jupyter_server_terminals=0.5.0=pyhd8ed1ab_0\n  - jupyterlab=4.0.9=pyhd8ed1ab_0\n  - jupyterlab_pygments=0.3.0=pyhd8ed1ab_0\n  - jupyterlab_server=2.25.2=pyhd8ed1ab_0\n  - jupyterlab_widgets=3.0.9=pyhd8ed1ab_0\n  - lame=3.100=h7b6447c_0\n  - lcms2=2.12=h3be6417_0\n  - ld_impl_linux-64=2.38=h1181459_1\n  - lerc=3.0=h295c915_0\n  - libcublas=11.11.3.6=0\n  - libcufft=10.9.0.58=0\n  - libcufile=1.8.1.2=0\n  - libcurand=10.3.4.101=0\n  - libcusolver=11.4.1.48=0\n  - libcusparse=11.7.5.86=0\n  - libdeflate=1.17=h5eee18b_1\n  - libffi=3.4.4=h6a678d5_0\n  - libgcc=7.2.0=h69d50b8_2\n  - libgcc-ng=11.2.0=h1234567_1\n  - libgomp=11.2.0=h1234567_1\n  - libiconv=1.16=h7f8727e_2\n  - libidn2=2.3.4=h5eee18b_0\n  - libnpp=11.8.0.86=0\n  - libnvjpeg=11.9.0.86=0\n  - libpng=1.6.39=h5eee18b_0\n  - libsodium=1.0.18=h36c2ea0_1\n  - libstdcxx-ng=11.2.0=h1234567_1\n  - libtasn1=4.19.0=h5eee18b_0\n  - libtiff=4.5.1=h6a678d5_0\n  - libunistring=0.9.10=h27cfd23_0\n  - libwebp=1.3.2=h11a3e52_0\n  - libwebp-base=1.3.2=h5eee18b_0\n  - lz4-c=1.9.4=h6a678d5_0\n  - markdown-it-py=3.0.0=pyhd8ed1ab_0\n  - markupsafe=2.1.1=py39h7f8727e_0\n  - matplotlib-inline=0.1.6=pyhd8ed1ab_0\n  - mdurl=0.1.0=pyhd8ed1ab_0\n  - mistune=3.0.2=pyhd8ed1ab_0\n  - mkl=2023.1.0=h213fc3f_46344\n  - mkl-service=2.4.0=py39h5eee18b_1\n  - mkl_fft=1.3.8=py39h5eee18b_0\n  - mkl_random=1.2.4=py39hdb19cb5_0\n  - mpc=1.1.0=h10f8cd9_1\n  - mpfr=4.0.2=hb69a4c5_1\n  - mpmath=1.3.0=py39h06a4308_0\n  - nbclient=0.8.0=pyhd8ed1ab_0\n  - nbconvert-core=7.12.0=pyhd8ed1ab_0\n  - ncurses=6.4=h6a678d5_0\n  - nettle=3.7.3=hbbd107a_1\n  - networkx=3.1=py39h06a4308_0\n  - nodejs=6.13.1=0\n  - notebook-shim=0.2.3=pyhd8ed1ab_0\n  - numpy=1.26.2=py39h5f9d8c6_0\n  - numpy-base=1.26.2=py39hb5e798b_0\n  - openh264=2.1.1=h4ff587b_0\n  - openjpeg=2.4.0=h3ad879b_0\n  - openssl=3.0.12=h7f8727e_0\n  - overrides=7.4.0=pyhd8ed1ab_0\n  - packaging=23.2=pyhd8ed1ab_0\n  - pandocfilters=1.5.0=pyhd8ed1ab_0\n  - parso=0.8.3=pyhd8ed1ab_0\n  - pickleshare=0.7.5=py_1003\n  - pillow=10.0.1=py39ha6cbd5a_0\n  - pip=23.3.1=py39h06a4308_0\n  - pkgutil-resolve-name=1.3.10=pyhd8ed1ab_1\n  - platformdirs=4.1.0=pyhd8ed1ab_0\n  - plotly=5.18.0=py_0\n  - plyfile=0.8.1=pyhd8ed1ab_0\n  - portalocker=2.8.2=py39hf3d152e_1\n  - prometheus_client=0.19.0=pyhd8ed1ab_0\n  - ptyprocess=0.7.0=pyhd3deb0d_0\n  - pure_eval=0.2.2=pyhd8ed1ab_0\n  - pycparser=2.21=pyhd3eb1b0_0\n  - pygments=2.17.2=pyhd8ed1ab_0\n  - pyopenssl=23.2.0=py39h06a4308_0\n  - pysocks=1.7.1=py39h06a4308_0\n  - python=3.9.18=h955ad1f_0\n  - python-dateutil=2.8.2=pyhd8ed1ab_0\n  - python-fastjsonschema=2.19.0=pyhd8ed1ab_0\n  - python-json-logger=2.0.7=pyhd8ed1ab_0\n  - python_abi=3.9=2_cp39\n  - pytorch=2.0.1=py3.9_cuda11.8_cudnn8.7.0_0\n  - pytorch-cuda=11.8=h7e8668a_5\n  - pytorch-mutex=1.0=cuda\n  - pytorch3d=0.7.4=py39_cu118_pyt201\n  - pytz=2023.3.post1=pyhd8ed1ab_0\n  - pyyaml=6.0=py39hb9d737c_4\n  - pyzmq=25.1.0=py39h6a678d5_0\n  - readline=8.2=h5eee18b_0\n  - referencing=0.32.0=pyhd8ed1ab_0\n  - requests=2.31.0=py39h06a4308_0\n  - rfc3339-validator=0.1.4=pyhd8ed1ab_0\n  - rfc3986-validator=0.1.1=pyh9f0ad1d_0\n  - rich=13.7.0=pyhd8ed1ab_0\n  - send2trash=1.8.2=pyh41d4057_0\n  - setuptools=68.2.2=py39h06a4308_0\n  - six=1.16.0=pyh6c4a22f_0\n  - sniffio=1.3.0=pyhd8ed1ab_0\n  - soupsieve=2.5=pyhd8ed1ab_1\n  - sqlite=3.41.2=h5eee18b_0\n  - stack_data=0.6.2=pyhd8ed1ab_0\n  - sympy=1.12=py39h06a4308_0\n  - tabulate=0.9.0=pyhd8ed1ab_1\n  - tbb=2021.8.0=hdb19cb5_0\n  - tenacity=8.2.2=py39h06a4308_0\n  - termcolor=2.3.0=pyhd8ed1ab_0\n  - terminado=0.18.0=pyh0d859eb_0\n  - tinycss2=1.2.1=pyhd8ed1ab_0\n  - tk=8.6.12=h1ccaba5_0\n  - tomli=2.0.1=pyhd8ed1ab_0\n  - torchaudio=2.0.2=py39_cu118\n  - torchtriton=2.0.0=py39\n  - torchvision=0.15.2=py39_cu118\n  - tornado=6.3.3=py39h5eee18b_0\n  - tqdm=4.66.1=pyhd8ed1ab_0\n  - traitlets=5.14.0=pyhd8ed1ab_0\n  - types-python-dateutil=2.8.19.14=pyhd8ed1ab_0\n  - typing_extensions=4.7.1=py39h06a4308_0\n  - typing_utils=0.1.0=pyhd8ed1ab_0\n  - uri-template=1.3.0=pyhd8ed1ab_0\n  - urllib3=1.26.18=py39h06a4308_0\n  - wcwidth=0.2.12=pyhd8ed1ab_0\n  - webcolors=1.13=pyhd8ed1ab_0\n  - webencodings=0.5.1=pyhd8ed1ab_2\n  - websocket-client=1.7.0=pyhd8ed1ab_0\n  - wheel=0.41.2=py39h06a4308_0\n  - widgetsnbextension=4.0.9=pyhd8ed1ab_0\n  - xz=5.4.5=h5eee18b_0\n  - yacs=0.1.8=pyhd8ed1ab_0\n  - yaml=0.2.5=h7f98852_2\n  - zeromq=4.3.4=h2531618_0\n  - zipp=3.17.0=pyhd8ed1ab_0\n  - zlib=1.2.13=h5eee18b_0\n  - zstd=1.5.5=hc292b87_0\n  - pip:\n      - addict==2.4.0\n      - ansi2html==1.9.1\n      - blinker==1.7.0\n      - click==8.1.7\n      - comm==0.2.0\n      - configargparse==1.7\n      - contourpy==1.2.0\n      - cycler==0.12.1\n      - dash==2.14.2\n      - dash-core-components==2.0.0\n      - dash-html-components==2.0.0\n      - dash-table==5.0.0\n      - flask==3.0.0\n      - fonttools==4.46.0\n      - itsdangerous==2.1.2\n      - joblib==1.3.2\n      - kiwisolver==1.4.5\n      - matplotlib==3.8.2\n      - nbformat==5.7.0\n      - nest-asyncio==1.5.8\n      - open3d==0.17.0\n      - pandas==2.1.4\n      - pexpect==4.9.0\n      - prompt-toolkit==3.0.43\n      - pymcubes==0.1.4\n      - pyparsing==3.1.1\n      - pyquaternion==0.9.9\n      - retrying==1.3.4\n      - rpds-py==0.15.2\n      - scikit-learn==1.3.2\n      - scipy==1.11.4\n      - stack-data==0.6.3\n      - threadpoolctl==3.2.0\n      - tzdata==2023.3\n      - werkzeug==3.0.1\n"
        },
        {
          "name": "extract_mesh.py",
          "type": "blob",
          "size": 2.6923828125,
          "content": "import argparse\nfrom sugar_utils.general_utils import str2bool\nfrom sugar_extractors.coarse_mesh import extract_mesh_from_coarse_sugar\n\nif __name__ == \"__main__\":\n    # Parser\n    parser = argparse.ArgumentParser(description='Script to extract a mesh from a coarse SuGaR scene.')\n    parser.add_argument('-s', '--scene_path',\n                        type=str, \n                        help='path to the scene data to use.')\n    parser.add_argument('-c', '--checkpoint_path', \n                        type=str, \n                        help='path to the vanilla 3D Gaussian Splatting Checkpoint to load.')\n    parser.add_argument('-i', '--iteration_to_load', \n                        type=int, default=7000, \n                        help='iteration to load.')\n    \n    parser.add_argument('-m', '--coarse_model_path', type=str, default=None, help='')\n    \n    parser.add_argument('-l', '--surface_level', type=float, default=None, \n                        help='Surface level to extract the mesh at. If None, will extract levels 0.1, 0.3 and 0.5')\n    parser.add_argument('-d', '--decimation_target', type=int, default=None, \n                        help='Target number of vertices to decimate the mesh to. If None, will decimate to 200_000 and 1_000_000.')\n    parser.add_argument('--project_mesh_on_surface_points', type=str2bool, default=True, \n                        help='If True, project the mesh on the surface points for better details.')\n    \n    parser.add_argument('-o', '--mesh_output_dir',\n                        type=str, default=None, \n                        help='path to the output directory.')\n    \n    parser.add_argument('-b', '--bboxmin', type=str, default=None, help='Min coordinates to use for foreground.')\n    parser.add_argument('-B', '--bboxmax', type=str, default=None, help='Max coordinates to use for foreground.')\n    parser.add_argument('--center_bbox', type=str2bool, default=True, help='If True, center the bounding box. Default is True.')\n    \n    parser.add_argument('--gpu', type=int, default=0, help='Index of GPU device to use.')\n    \n    parser.add_argument('--eval', type=str2bool, default=True, help='Use eval split.')\n    parser.add_argument('--use_centers_to_extract_mesh', type=str2bool, default=False, \n                        help='If True, just use centers of the gaussians to extract mesh.')\n    parser.add_argument('--use_marching_cubes', type=str2bool, default=False, \n                        help='If True, use marching cubes to extract mesh.')\n    parser.add_argument('--use_vanilla_3dgs', type=str2bool, default=False, \n                        help='If True, use vanilla 3DGS to extract mesh.')\n    \n    args = parser.parse_args()\n    \n    # Call function\n    extract_mesh_from_coarse_sugar(args)\n    "
        },
        {
          "name": "extract_refined_mesh_with_texture.py",
          "type": "blob",
          "size": 2.58984375,
          "content": "import argparse\nfrom sugar_utils.general_utils import str2bool\nfrom sugar_extractors.refined_mesh import extract_mesh_and_texture_from_refined_sugar\n\nif __name__ == \"__main__\":\n    # Parser\n    parser = argparse.ArgumentParser(description='Script to extract a mesh and texture from a refined SuGaR model.')\n    parser.add_argument('-s', '--scene_path',\n                        type=str, \n                        help='(Required) path to the scene data to use.')  # --OK\n    parser.add_argument('-i', '--iteration_to_load', \n                        type=int, default=7000, \n                        help='iteration to load.')  # --OK\n    parser.add_argument('-c', '--checkpoint_path', \n                        type=str, \n                        help='(Required) path to the vanilla 3D Gaussian Splatting Checkpoint to load.')  # --OK\n    parser.add_argument('-m', '--refined_model_path',\n                        type=str, \n                        help='(Required) Path to the refine model checkpoint.')  # --OK\n    parser.add_argument('-o', '--mesh_output_dir',\n                        type=str, \n                        default=None, \n                        help='path to the output directory.')  # --OK\n    parser.add_argument('-n', '--n_gaussians_per_surface_triangle',\n                        default=None, type=int, help='Number of gaussians per surface triangle.')  # --OK\n    parser.add_argument('--square_size',\n                        default=None, type=int, help='Size of the square to use for the texture.')  # --OK\n    \n    parser.add_argument('--eval', type=str2bool, default=True, help='Use eval split.')\n    parser.add_argument('-g', '--gpu', type=int, default=0, help='Index of GPU to use.')\n    \n    # Optional postprocessing\n    parser.add_argument('--postprocess_mesh', type=str2bool, default=False, \n                        help='If True, postprocess the mesh by removing border triangles with low-density. '\n                        'This step takes a few minutes and is not needed in general, as it can also be risky. '\n                        'However, it increases the quality of the mesh in some cases, especially when an object is visible only from one side.')  # --OK\n    parser.add_argument('--postprocess_density_threshold', type=float, default=0.1,\n                        help='Threshold to use for postprocessing the mesh.')  # --OK\n    parser.add_argument('--postprocess_iterations', type=int, default=5,\n                        help='Number of iterations to use for postprocessing the mesh.')  # --OK\n    \n    args = parser.parse_args()\n    \n    # Call function\n    extract_mesh_and_texture_from_refined_sugar(args)\n    "
        },
        {
          "name": "gaussian_splatting",
          "type": "tree",
          "content": null
        },
        {
          "name": "gsplat_wrapper",
          "type": "tree",
          "content": null
        },
        {
          "name": "install.py",
          "type": "blob",
          "size": 1.5517578125,
          "content": "import os\nimport argparse\n\nif __name__ == '__main__':\n    parser = argparse.ArgumentParser(description='Setup the environment')\n    \n    parser.add_argument('--no_nvdiffrast', action='store_true', help='Skip installation of Nvdiffrast')\n    args = parser.parse_args()\n    \n    # Create a new conda environment\n    print(\"[INFO] Creating the conda environment for SuGaR...\")\n    os.system(\"conda env create -f environment.yml\")\n    print(\"[INFO] Conda environment created.\")\n    \n    # Install 3D Gaussian Splatting rasterizer\n    print(\"[INFO] Installing the 3D Gaussian Splatting rasterizer...\")\n    os.chdir(\"gaussian_splatting/submodules/diff-gaussian-rasterization/\")\n    os.system(\"conda run -n sugar pip install -e .\")\n    print(\"[INFO] 3D Gaussian Splatting rasterizer installed.\")\n    \n    # Install simple-knn\n    print(\"[INFO] Installing simple-knn...\")\n    os.chdir(\"../simple-knn/\")\n    os.system(\"conda run -n sugar pip install -e .\")\n    print(\"[INFO] simple-knn installed.\")\n    os.chdir(\"../../../\")\n    \n    # Install Nvdiffrast\n    if args.no_nvdiffrast:\n        print(\"[INFO] Skipping installation of Nvdiffrast.\")\n    else:\n        print(\"[INFO] Installing Nvdiffrast...\")\n        os.system(\"git clone https://github.com/NVlabs/nvdiffrast\")\n        os.chdir(\"nvdiffrast\")\n        os.system(\"conda run -n sugar pip install .\")\n        print(\"[INFO] Nvdiffrast installed.\")\n        print(\"[INFO] Please note that Nvdiffrast will take a few seconds or minutes to build the first time it is used.\")\n        os.chdir(\"../\")\n\n    print(\"[INFO] SuGaR installation complete.\")\n"
        },
        {
          "name": "media",
          "type": "tree",
          "content": null
        },
        {
          "name": "metrics.py",
          "type": "blob",
          "size": 23.5546875,
          "content": "import argparse\nimport os\nimport json\nimport open3d as o3d\nimport torch\nfrom pytorch3d.io import load_objs_as_meshes\nfrom gaussian_splatting.utils.loss_utils import ssim\nfrom gaussian_splatting.utils.image_utils import psnr\nfrom gaussian_splatting.lpipsPyTorch import lpips\nfrom sugar_scene.gs_model import GaussianSplattingWrapper\nfrom sugar_scene.sugar_model import SuGaR\nfrom sugar_utils.spherical_harmonics import SH2RGB\nfrom sugar_utils.general_utils import str2bool\n\nfrom rich.console import Console\nCONSOLE = Console(width=120)\n\nos.makedirs('./lpipsPyTorch/weights/', exist_ok=True)\ntorch.hub.set_dir('./lpipsPyTorch/weights/')\n\nn_skip_images_for_eval_split = 8\n\n\nif __name__ == \"__main__\":\n    \n    # Parser\n    parser = argparse.ArgumentParser(description='Script to extract a mesh from a coarse SuGaR scene.')\n    \n    # Config file for scenes to evaluate\n    parser.add_argument('--scene_config', type=str, \n                        help='(Required) Path to the JSON file containing the scenes to evaluate. '\n                        'The JSON file should be a dictionary with the following structure: '\n                        '{source_images_dir_path: vanilla_gaussian_splatting_checkpoint_path}')\n    \n    # Coarse model parameters\n    parser.add_argument('-i', '--iteration_to_load', type=int, default=7000, \n                        help='iteration to load.')\n    parser.add_argument('-e', '--estimation_factor', type=float, default=0.2, \n                        help='Estimation factor to load for coarse model.')\n    parser.add_argument('-r', '--regularization_type', type=str, \n                        help='(Required) Type of regularization to evaluate for coarse SuGaR. Can be \"sdf\" or \"density\".')\n    \n    # Mesh extraction parameters\n    parser.add_argument('-l', '--surface_level', type=float, default=0.3, \n                        help='Surface level used for mesh extraction in SuGaR models to evaluate. Default is 0.3')\n    parser.add_argument('-v', '--n_vertices_in_mesh', type=int, default=1_000_000, \n                        help='Number of vertices in SuGaR meshes to evaluate. Default is 1_000_000')\n    \n    # Refined model parameters\n    parser.add_argument('-g', '--gaussians_per_triangle', type=int, default=1, \n                        help='Number of gaussians per triangle used in the refined SuGaR models to evaluate. Default is 1')\n    parser.add_argument('-f', '--refinement_iterations', type=int, default=15_000, \n                        help='Number of refinement iterations used in the refined SuGaR models to evaluate. Default is 15_000')\n    \n    # Device\n    parser.add_argument('--gpu', type=int, default=0, \n                        help='Index of GPU to use.')\n    \n    # (Optional) Default configurations\n    parser.add_argument('--low_poly', type=str2bool, default=False, \n                        help='Evaluating standard config for a low poly mesh, with 200k vertices and 6 Gaussians per triangle.')\n    parser.add_argument('--high_poly', type=str2bool, default=False,\n                        help='Evaluating standard config for a high poly mesh, with 1M vertices and 1 Gaussians per triangle.')\n    parser.add_argument('--refinement_time', type=str, default=None, \n                        help=\"Default configs for time to spend on refinement. Can be 'short', 'medium' or 'long'.\")\n    \n    # (Optional) Additional evaluation parameters\n    parser.add_argument('--evaluate_vanilla', type=str2bool, default=False, \n                        help='If True, will also evaluate vanilla 3DGS, in addition to SuGaR.')\n    parser.add_argument('--use_uv_texture', type=str2bool, default=False, \n                        help='If True, will use the extracted UV texture for the mesh instead of Gaussian Splatting rendering.')\n    parser.add_argument('--use_diffuse_color_only', type=str2bool, default=False, \n                        help='If True, will use only the diffuse component in Gaussian Splatting rendering.')\n    parser.add_argument('--use_poisson_center', type=str2bool, default=False, \n                        help='If True, will evaluate Poisson center as the extraction method.')\n    parser.add_argument('--use_marching_cubes', type=str2bool, default=False, \n                        help='If True, will evaluate Marching Cubes as the extraction method.')\n    \n    args = parser.parse_args()\n    # Mesh resolution\n    if args.low_poly:\n        args.n_vertices_in_mesh = 200_000\n        args.gaussians_per_triangle = 6\n        print('Evaluating low poly config.')\n    if args.high_poly:\n        args.n_vertices_in_mesh = 1_000_000\n        args.gaussians_per_triangle = 1\n        print('Evaluating high poly config.')\n    # Refinement time\n    if args.refinement_time == 'short':\n        args.refinement_iterations = 2_000\n        print('Evaluating short refinement time.')\n    if args.refinement_time == 'medium':\n        args.refinement_iterations = 7_000\n        print('Evaluating medium refinement time.')\n    if args.refinement_time == 'long':\n        args.refinement_iterations = 15_000\n        print('Evaluating long refinement time.')\n            \n    # --- Scenes dict ---\n    with open(args.scene_config, 'r') as f:\n        gs_checkpoints_eval = json.load(f)\n    \n    # --- Coarse model parameters ---\n    coarse_iteration_to_load = args.iteration_to_load\n    coarse_estimation_factor = args.estimation_factor\n    estim_method = args.regularization_type\n    coarse_normal_factor = 0.2\n    \n    # --- Mesh extraction parameters ---\n    surface_levels = [args.surface_level]    \n    decimation_targets = [args.n_vertices_in_mesh]\n        \n    # --- Refined model parameters ---\n    surface_mesh_normal_consistency_factor = 0.1\n    n_gaussians_per_surface_triangle_map = {args.n_vertices_in_mesh: args.gaussians_per_triangle,}\n    refinement_iterations_list = [args.refinement_iterations]\n        \n    # --- Evaluation parameters ---\n    evaluate_vanilla = args.evaluate_vanilla\n    use_uv_texture = args.use_uv_texture\n    use_diffuse_color_only = args.use_diffuse_color_only\n    use_poisson_center = args.use_poisson_center\n    use_marching_cubes = args.use_marching_cubes\n            \n    CONSOLE.print('==================================================')\n    CONSOLE.print(\"Starting evaluation with the following parameters:\")\n    CONSOLE.print(f\"Coarse iteration to load: {coarse_iteration_to_load}\")\n    CONSOLE.print(f\"Coarse estimation factor: {coarse_estimation_factor}\")\n    CONSOLE.print(f\"Coarse normal factor: {coarse_normal_factor}\")\n    CONSOLE.print(f\"Estimation method: {estim_method}\")\n    CONSOLE.print(f\"Surface levels: {surface_levels}\")\n    CONSOLE.print(f\"Decimation targets: {decimation_targets}\")\n    CONSOLE.print(f\"Surface mesh normal consistency factor: {surface_mesh_normal_consistency_factor}\")\n    CONSOLE.print(f\"Number of Gaussians per surface triangle: {n_gaussians_per_surface_triangle_map}\")\n    CONSOLE.print(f\"Refinement iterations: {refinement_iterations_list}\")\n    CONSOLE.print(f\"GS checkpoints for evaluation: {gs_checkpoints_eval}\")\n    CONSOLE.print(f\"Evaluate vanilla: {evaluate_vanilla}\")\n    CONSOLE.print(f\"Use UV texture: {use_uv_texture}\")\n    CONSOLE.print(f\"Use diffuse color only: {use_diffuse_color_only}\")\n    CONSOLE.print(f\"Use Poisson center: {use_poisson_center}\")\n    CONSOLE.print(f\"Use Marching Cubes: {use_marching_cubes}\")\n    CONSOLE.print('==================================================')\n    \n    # Set the GPU\n    torch.cuda.set_device(args.gpu)\n    \n    # ==========================\n\n    result_file_dir = './output/metrics/'\n    os.makedirs(result_file_dir, exist_ok=True)\n    results = {}\n    \n    for source_path in gs_checkpoints_eval.keys():\n        scene_name = source_path.split('/')[-1]\n        CONSOLE.print(f\"\\n===== Processing scene {scene_name}... =====\")\n        scene_results = {}\n        \n        # Loading vanilla 3DGS models\n        gs_checkpoint_path = gs_checkpoints_eval[source_path]\n        \n        CONSOLE.print(\"Source path:\", source_path)\n        CONSOLE.print(\"Gaussian splatting checkpoint path:\", gs_checkpoint_path)    \n        CONSOLE.print(f\"\\nLoading Vanilla 3DGS model config {gs_checkpoint_path}...\")\n        \n        nerfmodel_30k = GaussianSplattingWrapper(\n            source_path=source_path,\n            output_path=gs_checkpoint_path,\n            iteration_to_load=30_000,\n            load_gt_images=True,\n            eval_split=True,\n            eval_split_interval=n_skip_images_for_eval_split,\n            )\n\n        nerfmodel_7k = GaussianSplattingWrapper(\n            source_path=source_path,\n            output_path=gs_checkpoint_path,\n            iteration_to_load=7000,\n            load_gt_images=False,\n            eval_split=True,\n            eval_split_interval=n_skip_images_for_eval_split,\n            )\n        \n        if use_diffuse_color_only:\n            sh_deg_to_use = 0\n        else:\n            sh_deg_to_use = nerfmodel_30k.gaussians.active_sh_degree\n\n        CONSOLE.print(\"Vanilla 3DGS Loaded.\")\n        CONSOLE.print(\"Number of test cameras:\", len(nerfmodel_30k.test_cameras))\n        CONSOLE.print(\"Using SH degree:\", sh_deg_to_use)\n        \n        compute_lpips = True\n        cam_indices = [cam_idx for cam_idx in range(len(nerfmodel_30k.test_cameras))]\n        \n        # Evaluating Vanilla 3DGS\n        if evaluate_vanilla:\n            CONSOLE.print(\"\\n--- Starting Evaluation of Vanilla 3DGS... ---\")\n\n            gs_7k_ssims = []\n            gs_7k_psnrs = []\n            gs_7k_lpipss = []\n            \n            gs_30k_ssims = []\n            gs_30k_psnrs = []\n            gs_30k_lpipss = []\n            \n            with torch.no_grad():    \n                for cam_idx in cam_indices:\n                    # GT image\n                    gt_img = nerfmodel_30k.get_test_gt_image(cam_idx).permute(2, 0, 1).unsqueeze(0)\n                    \n                    # Vanilla 3DGS image (30K)\n                    gs_30k_img = nerfmodel_30k.render_image(\n                        nerf_cameras=nerfmodel_30k.test_cameras,\n                        camera_indices=cam_idx).clamp(min=0, max=1).permute(2, 0, 1).unsqueeze(0)\n                    \n                    # Vanilla 3DGS image (7K)\n                    gs_7k_img = nerfmodel_7k.render_image(\n                        nerf_cameras=nerfmodel_30k.test_cameras,\n                        camera_indices=cam_idx).clamp(min=0, max=1).permute(2, 0, 1).unsqueeze(0)\n                    \n                    gs_30k_ssims.append(ssim(gs_30k_img, gt_img))\n                    gs_30k_psnrs.append(psnr(gs_30k_img, gt_img))\n                    gs_30k_lpipss.append(lpips(gs_30k_img, gt_img, net_type='vgg'))\n                    \n                    gs_7k_ssims.append(ssim(gs_7k_img, gt_img))\n                    gs_7k_psnrs.append(psnr(gs_7k_img, gt_img))\n                    gs_7k_lpipss.append(lpips(gs_7k_img, gt_img, net_type='vgg'))    \n                    \n            CONSOLE.print(\"Evaluation of Vanilla 3DGS finished.\")\n            scene_results['3dgs_7k'] = {}\n            scene_results['3dgs_7k']['ssim'] = torch.tensor(gs_7k_ssims).mean().item()\n            scene_results['3dgs_7k']['psnr'] = torch.tensor(gs_7k_psnrs).mean().item()\n            scene_results['3dgs_7k']['lpips'] = torch.tensor(gs_7k_lpipss).mean().item()\n            \n            scene_results['3dgs_30k'] = {}\n            scene_results['3dgs_30k']['ssim'] = torch.tensor(gs_30k_ssims).mean().item()\n            scene_results['3dgs_30k']['psnr'] = torch.tensor(gs_30k_psnrs).mean().item()\n            scene_results['3dgs_30k']['lpips'] = torch.tensor(gs_30k_lpipss).mean().item()\n            \n            CONSOLE.print(f\"\\nVanilla 3DGS results (7K iterations):\")\n            CONSOLE.print(\"SSIM:\", torch.tensor(gs_7k_ssims).mean())\n            CONSOLE.print(\"PSNR:\", torch.tensor(gs_7k_psnrs).mean())\n            CONSOLE.print(\"LPIPS:\", torch.tensor(gs_7k_lpipss).mean())\n            \n            CONSOLE.print(f\"\\bVanilla 3DGS results (30K iterations):\")\n            CONSOLE.print(\"SSIM:\", torch.tensor(gs_30k_ssims).mean())\n            CONSOLE.print(\"PSNR:\", torch.tensor(gs_30k_psnrs).mean())\n            CONSOLE.print(\"LPIPS:\", torch.tensor(gs_30k_lpipss).mean())\n        \n        # Evaluating SuGaR models\n        with torch.no_grad():\n            CONSOLE.print(\"\\n--- Starting Evaluation of SuGaR... ---\")\n            for surface_level in surface_levels:\n                for decimation_target in decimation_targets:\n                    for refinement_iterations in refinement_iterations_list:\n                        n_gaussians_per_surface_triangle = n_gaussians_per_surface_triangle_map[decimation_target]\n                        \n                        if use_uv_texture:\n                            CONSOLE.print(\"Using UV texture for rendering.\")\n                            coarse_estimation_factor_str = str(coarse_estimation_factor).replace('.', '')\n                            surface_level_str = str(surface_level).replace('.', '')\n                            textured_mesh_path = os.path.join('./output/refined_mesh/', scene_name, \n                                        f'sugarfine_3Dgs{coarse_iteration_to_load}_{estim_method}estim{coarse_estimation_factor_str}_sdfnorm02_level{surface_level_str}_decim{decimation_target}_normalconsistency01_gaussperface{n_gaussians_per_surface_triangle}.obj')\n                            CONSOLE.print(f'Loading textured mesh: {textured_mesh_path}')\n                            \n                            textured_mesh = load_objs_as_meshes([textured_mesh_path]).to(nerfmodel_30k.device)\n                            CONSOLE.print(f\"Loaded textured mesh with {len(textured_mesh.verts_list()[0])} vertices and {len(textured_mesh.faces_list()[0])} faces.\")\n                            \n                            faces_per_pixel = 1\n                            max_faces_per_bin = 50_000\n\n                            from pytorch3d.renderer import (\n                                AmbientLights,\n                                RasterizationSettings, \n                                MeshRenderer, \n                                MeshRasterizer,  \n                                SoftPhongShader,\n                                )\n                            from pytorch3d.renderer.blending import BlendParams\n\n                            mesh_raster_settings = RasterizationSettings(\n                                image_size=(nerfmodel_30k.image_height, nerfmodel_30k.image_width),\n                                blur_radius=0.0, \n                                faces_per_pixel=faces_per_pixel,\n                                # max_faces_per_bin=max_faces_per_bin\n                            )\n                            renderer = MeshRenderer(\n                                rasterizer=MeshRasterizer(\n                                    cameras=nerfmodel_30k.test_cameras.p3d_cameras[0], \n                                    raster_settings=mesh_raster_settings,\n                                    ),\n                                shader=SoftPhongShader(\n                                    device=nerfmodel_30k.device, \n                                    cameras=nerfmodel_30k.test_cameras.p3d_cameras[0],\n                                    lights=AmbientLights(device=nerfmodel_30k.device),\n                                    blend_params=BlendParams(background_color=(0.0, 0.0, 0.0)),\n                                )\n                            )\n                            \n                        else:\n                            CONSOLE.print(\"Using surface Gaussian Splatting for rendering.\")\n                            sugar_checkpoint_path = f'sugarcoarse_3Dgs{coarse_iteration_to_load}_{estim_method}estimXX_sdfnormYY/'\n                            sugar_checkpoint_path = sugar_checkpoint_path.replace(\n                                'XX', str(coarse_estimation_factor).replace('.', '')\n                                ).replace(\n                                    'YY', str(coarse_normal_factor).replace('.', '')\n                                    )\n                            \n                            # Loading mesh\n                            CONSOLE.print(f\"\\nProcessing Surface level: {surface_level}, Decimation target: {decimation_target}, Refinement iterations: {refinement_iterations}...\")\n                            \n                            if use_marching_cubes:\n                                CONSOLE.print(\"Using Marching Cubes for mesh extraction.\")\n                                sugar_mesh_path = 'sugarmesh_' + sugar_checkpoint_path.split('/')[-2].replace('sugarcoarse_', '') + 'marchingcubes_levelZZ_decimAA.ply'\n                            elif use_poisson_center:\n                                CONSOLE.print(\"Using Poisson Center for mesh extraction.\")\n                                sugar_mesh_path = 'sugarmesh_' + sugar_checkpoint_path.split('/')[-2].replace('sugarcoarse_', '') + '_poissoncenters_decimAA.ply'\n                            else:                        \n                                sugar_mesh_path = 'sugarmesh_' + sugar_checkpoint_path.split('/')[-2].replace('sugarcoarse_', '') + '_levelZZ_decimAA.ply'\n                            sugar_mesh_path = sugar_mesh_path.replace(\n                                'ZZ', str(surface_level).replace('.', '')\n                                ).replace(\n                                    'AA', str(decimation_target).replace('.', '')\n                                    )\n                            mesh_save_dir = os.path.join('./output/coarse_mesh/', scene_name)\n                            sugar_mesh_path = os.path.join(mesh_save_dir, sugar_mesh_path)\n                            CONSOLE.print(f'Loading mesh to bind to: {sugar_mesh_path}')\n                            o3d_mesh = o3d.io.read_triangle_mesh(sugar_mesh_path)\n                            \n                            # Loading refined SuGaR model\n                            mesh_name = sugar_mesh_path.split(\"/\")[-1].split(\".\")[0]\n                            refined_sugar_path = 'sugarfine_' + mesh_name.replace('sugarmesh_', '') + '_normalconsistencyXX_gaussperfaceYY/'\n                            refined_sugar_path = os.path.join(os.path.join('./output/refined/', scene_name), refined_sugar_path)\n                            refined_sugar_path = refined_sugar_path.replace(\n                                'XX', str(surface_mesh_normal_consistency_factor).replace('.', '')\n                                ).replace(\n                                'YY', str(n_gaussians_per_surface_triangle).replace('.', '')\n                                )\n                            refined_sugar_path = os.path.join(refined_sugar_path, f'{refinement_iterations}.pt')\n                            CONSOLE.print(f\"Loading SuGaR model config {refined_sugar_path}...\")\n                            checkpoint = torch.load(refined_sugar_path, map_location=nerfmodel_30k.device)\n                            refined_sugar = SuGaR(\n                                nerfmodel=nerfmodel_30k,\n                                points=checkpoint['state_dict']['_points'],\n                                colors=SH2RGB(checkpoint['state_dict']['_sh_coordinates_dc'][:, 0, :]),\n                                initialize=False,\n                                sh_levels=nerfmodel_30k.gaussians.active_sh_degree+1,\n                                keep_track_of_knn=False,\n                                knn_to_track=0,\n                                beta_mode='average',\n                                surface_mesh_to_bind=o3d_mesh,\n                                n_gaussians_per_surface_triangle=n_gaussians_per_surface_triangle,\n                                )\n                            refined_sugar.load_state_dict(checkpoint['state_dict'])\n                            refined_sugar.eval()\n                        \n                        # Evaluating SuGaR\n                        with torch.no_grad():                \n                            sugar_ssims = []\n                            sugar_psnrs = []\n                            sugar_lpipss = []\n                            \n                            for cam_idx in cam_indices:\n                                # GT image\n                                gt_img = nerfmodel_30k.get_test_gt_image(cam_idx).permute(2, 0, 1).unsqueeze(0)\n                                \n                                # SUGAR image\n                                if use_uv_texture:\n                                    p3d_cameras = nerfmodel_30k.test_cameras.p3d_cameras[cam_idx]\n                                    sugar_img = renderer(textured_mesh, cameras=p3d_cameras)[0, ..., :3].clamp(min=0, max=1).permute(2, 0, 1).unsqueeze(0).contiguous()\n                                else:\n                                    sugar_img = refined_sugar.render_image_gaussian_rasterizer(\n                                        nerf_cameras=nerfmodel_30k.test_cameras,\n                                        camera_indices=cam_idx,\n                                        verbose=False,\n                                        bg_color=None,\n                                        sh_deg=sh_deg_to_use,\n                                        compute_color_in_rasterizer=True,#compute_color_in_rasterizer,\n                                    ).clamp(min=0, max=1).permute(2, 0, 1).unsqueeze(0)\n                                \n                                sugar_ssims.append(ssim(sugar_img, gt_img))\n                                sugar_psnrs.append(psnr(sugar_img, gt_img))\n                                sugar_lpipss.append(lpips(sugar_img, gt_img, net_type='vgg'))\n                        \n                        CONSOLE.print(f\"Evaluation of SuGaR finished, with surface level {surface_level} and decimation target {decimation_target} and refinement iterations {refinement_iterations}.\")\n                        str_surface_level = str(surface_level).replace('.', '')\n                        scene_results[f'sugar_{str_surface_level}_{decimation_target}_{refinement_iterations}'] = {}\n                        scene_results[f'sugar_{str_surface_level}_{decimation_target}_{refinement_iterations}']['ssim'] = torch.tensor(sugar_ssims).mean().item()\n                        scene_results[f'sugar_{str_surface_level}_{decimation_target}_{refinement_iterations}']['psnr'] = torch.tensor(sugar_psnrs).mean().item()\n                        scene_results[f'sugar_{str_surface_level}_{decimation_target}_{refinement_iterations}']['lpips'] = torch.tensor(sugar_lpipss).mean().item()\n                        \n                        CONSOLE.print(f\"SuGaR results:\")\n                        CONSOLE.print(\"SSIM:\", torch.tensor(sugar_ssims).mean())\n                        CONSOLE.print(\"PSNR:\", torch.tensor(sugar_psnrs).mean())\n                        CONSOLE.print(\"LPIPS:\", torch.tensor(sugar_lpipss).mean())\n        \n        # Saves results to JSON file                \n        results[scene_name] = scene_results\n        estim_factor_str = str(coarse_estimation_factor).replace('.', '')\n        normal_factor_str = str(coarse_normal_factor).replace('.', '')\n        if use_uv_texture:\n            result_file_name = f'results_{estim_method}{estim_factor_str}_normal{normal_factor_str}_uvtexture.json'\n        elif use_diffuse_color_only:\n            result_file_name = f'results_{estim_method}{estim_factor_str}_normal{normal_factor_str}_diffuseonly.json'\n        elif use_poisson_center:\n            result_file_name = f'results_{estim_method}{estim_factor_str}_normal{normal_factor_str}_poissoncenter.json'\n        elif use_marching_cubes:\n            result_file_name = f'results_{estim_method}{estim_factor_str}_normal{normal_factor_str}_marchingcubes.json'\n        else:\n            result_file_name = f'results_{estim_method}{estim_factor_str}_normal{normal_factor_str}.json'\n        result_file_name = os.path.join(result_file_dir, result_file_name)\n\n        CONSOLE.print(f\"Saving results to {result_file_name}...\")\n        with open(result_file_name, 'w') as f:\n            json.dump(results, f, indent=4)"
        },
        {
          "name": "render_blender_scene.py",
          "type": "blob",
          "size": 5.0205078125,
          "content": "import argparse\nimport os\nimport torch\nimport numpy as np\nfrom PIL import Image\nfrom sugar_utils.general_utils import str2bool\nfrom blender.sugar_utils import (\n    load_blender_package, \n    load_sugar_models_from_blender_package, \n    load_cameras_from_blender_package, \n    build_composite_scene,\n    render_composited_image,\n) \nfrom rich.console import Console\n\nif __name__ == \"__main__\":\n    print_every_n_frames = 5\n    \n    # ----- Parser -----\n    parser = argparse.ArgumentParser(description='Script to render SuGaR meshes edited or animated with Blender.')\n    \n    parser.add_argument('-p', '--package_path',\n                        type=str, \n                        help='(Required) path to the Blender data package to use for rendering.')\n    \n    parser.add_argument('-o', '--output_path',\n                        type=str, \n                        default=None,\n                        help='Path to the output folder where to save the rendered images. \\\n                        If None, images will be saved in ./output/blender/renders/{package_name}.')\n       \n    parser.add_argument('--sh_degree', type=int, default=3, help='SH degree to use.')\n    parser.add_argument('--gpu', type=int, default=0, help='Index of GPU device to use.')\n    parser.add_argument('--white_background', type=str2bool, default=False, help='Use a white background instead of black.')\n    \n    parser.add_argument('--deformation_threshold', type=float, default=2., \n                        help='Threshold for the deformation of the mesh. A face is considered too much deformed if its size increases by a ratio greater than this threshold.')\n    \n    parser.add_argument('--export_frame_as_ply', type=int, default=0, \n                        help='Export the Frosting representation of the scene at the specified frame as a PLY file. '\n                        'If 0, no PLY file will be exported and all frames will be rendered.')\n    \n    CONSOLE = Console(width=120)\n    \n    args = parser.parse_args()\n    scene_name = os.path.splitext(os.path.basename(args.package_path))[0]\n    output_path = args.output_path if args.output_path else f\"./output/blender/renders/{scene_name}\"\n    sh_degree = args.sh_degree\n    deformation_threshold = args.deformation_threshold\n    frame_to_export_as_ply = args.export_frame_as_ply - 1\n    \n    # ----- Setup -----\n    torch.cuda.set_device(args.gpu)\n    device = torch.device(torch.cuda.current_device())\n    CONSOLE.print(\"[INFO] Using device: \", device)\n    CONSOLE.print(\"[INFO] Images will be saved in: \", output_path)\n    \n    # ----- Load Blender package -----\n    CONSOLE.print(\"\\nLoading Blender package...\")\n    package = load_blender_package(args.package_path, device)\n    CONSOLE.print(\"Blender package loaded.\")\n    \n    # ----- Load SuGaR models -----\n    CONSOLE.print(\"Loading SuGaR models...\")\n    sugar_models, scene_paths = load_sugar_models_from_blender_package(package, device)\n    \n    # ----- Build composite scene -----\n    CONSOLE.print(\"\\nBuilding composite scene...\")\n    sugar_comp = build_composite_scene(sugar_models, scene_paths, package)\n    \n    # ----- Build cameras -----\n    CONSOLE.print(\"Loading cameras...\")\n    render_cameras = load_cameras_from_blender_package(package, device=device)\n    \n    # ----- Render and saving images -----\n    CONSOLE.print(\"\\nLoading successful. Rendering and saving images...\")\n    n_frames = len(package['camera']['lens'])\n    os.makedirs(output_path, exist_ok=True)\n    \n    sugar_comp.eval()\n    sugar_comp.adapt_to_cameras(render_cameras)\n    \n    with torch.no_grad():\n        if frame_to_export_as_ply == -1:\n            for i_frame in range(n_frames):\n                rgb_render = render_composited_image(\n                    package=package,\n                    sugar=sugar_comp, \n                    render_cameras=render_cameras, \n                    i_frame=i_frame,\n                    sh_degree=sh_degree,\n                    deformation_threshold=deformation_threshold,\n                )\n            \n                # Save image\n                save_path = os.path.join(output_path, f\"{i_frame+1:04d}.png\")\n                img = Image.fromarray((rgb_render.cpu().numpy() * 255).astype(np.uint8))\n                img.save(save_path)\n                \n                # Info\n                if i_frame % print_every_n_frames == 0:\n                    print(f\"Saved frame {i_frame} to {save_path}\")\n                    \n                torch.cuda.empty_cache()\n        else:\n            # Export PLY file\n            ply_save_path = os.path.join(output_path, f\"{frame_to_export_as_ply+1:04d}.ply\")\n            render_composited_image(\n                package=package,\n                sugar=sugar_comp, \n                render_cameras=render_cameras, \n                i_frame=frame_to_export_as_ply,\n                sh_degree=sh_degree,\n                deformation_threshold=deformation_threshold,\n                return_GS_model=True,\n            ).save_ply(ply_save_path)\n            CONSOLE.print(f\"Exported PLY file of frame {frame_to_export_as_ply+1} to {ply_save_path}\")\n            \nCONSOLE.print(\"Rendering completed.\")\n"
        },
        {
          "name": "run_viewer.py",
          "type": "blob",
          "size": 1.701171875,
          "content": "import sys\r\nimport os\r\nimport json\r\nimport argparse\r\n\r\nviewer_path = './sugar_viewer'\r\njson_path = os.path.join(viewer_path, 'src', 'scene_to_load.json')\r\n\r\nif __name__ == \"__main__\":\r\n    # ----- Parser -----\r\n    parser = argparse.ArgumentParser(description='Script to run the SuGaR viewer.')\r\n    parser.add_argument('-p', '--ply_path',\r\n                        type=str, \r\n                        default=None,\r\n                        help='(Required) path to the refined SuGaR PLY file.')\r\n    \r\n    args = parser.parse_args()\r\n    ply_path = args.ply_path\r\n    \r\n    # ----- Path checks -----\r\n    if ply_path is None:\r\n        raise ValueError('Please provide a path to the refined SuGaR PLY file.')\r\n    if not os.path.exists(ply_path) or not ply_path.endswith('.ply'):\r\n        raise ValueError('Could not find the refined SuGaR PLY file.')\r\n    \r\n    obj_path = ply_path.replace('.ply', '.obj').replace('refined_ply', 'refined_mesh')\r\n    png_path = obj_path.replace('.obj', '.png')\r\n    \r\n    if not os.path.exists(obj_path):\r\n        raise ValueError('Could not find the corresponding refined SuGaR OBJ file.')\r\n    else:\r\n        print('\\nFound a matching refined SuGaR OBJ file.')\r\n    \r\n    if not os.path.exists(png_path):\r\n        raise ValueError('Could not find the corresponding refined SuGaR texture PNG file.')\r\n    else:\r\n        print('Found a matching refined SuGaR texture PNG file.\\n')\r\n        \r\n    # ----- Write the paths to a json file -----\r\n    with open(json_path, 'w') as f:\r\n        json.dump({\r\n            'ply_path': ply_path, \r\n            'obj_path': obj_path, \r\n            'png_path': png_path}, f)\r\n    \r\n    # ----- Run the viewer -----\r\n    os.system(f'npm run --prefix {viewer_path} dev')\r\n    "
        },
        {
          "name": "sugar_extractors",
          "type": "tree",
          "content": null
        },
        {
          "name": "sugar_scene",
          "type": "tree",
          "content": null
        },
        {
          "name": "sugar_trainers",
          "type": "tree",
          "content": null
        },
        {
          "name": "sugar_utils",
          "type": "tree",
          "content": null
        },
        {
          "name": "sugar_viewer",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 9.787109375,
          "content": "import argparse\nfrom sugar_utils.general_utils import str2bool\nfrom sugar_trainers.coarse_density import coarse_training_with_density_regularization\nfrom sugar_trainers.coarse_sdf import coarse_training_with_sdf_regularization\nfrom sugar_trainers.coarse_density_and_dn_consistency import coarse_training_with_density_regularization_and_dn_consistency\nfrom sugar_extractors.coarse_mesh import extract_mesh_from_coarse_sugar\nfrom sugar_trainers.refine import refined_training\nfrom sugar_extractors.refined_mesh import extract_mesh_and_texture_from_refined_sugar\n\n\nclass AttrDict(dict):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.__dict__ = self\n\n\nif __name__ == \"__main__\":\n    # ----- Parser -----\n    parser = argparse.ArgumentParser(description='Script to optimize a full SuGaR model.')\n    \n    # Data and vanilla 3DGS checkpoint\n    parser.add_argument('-s', '--scene_path',\n                        type=str, \n                        help='(Required) path to the scene data to use.')  \n    parser.add_argument('-c', '--checkpoint_path',\n                        type=str, \n                        help='(Required) path to the vanilla 3D Gaussian Splatting Checkpoint to load.')\n    parser.add_argument('-i', '--iteration_to_load', \n                        type=int, default=7000, \n                        help='iteration to load.')\n    \n    # Regularization for coarse SuGaR\n    parser.add_argument('-r', '--regularization_type', type=str,\n                        help='(Required) Type of regularization to use for coarse SuGaR. Can be \"sdf\", \"density\" or \"dn_consistency\". ' \n                        'We recommend using \"dn_consistency\" for the best mesh quality.')\n    \n    # Extract mesh\n    parser.add_argument('-l', '--surface_level', type=float, default=0.3, \n                        help='Surface level to extract the mesh at. Default is 0.3')\n    parser.add_argument('-v', '--n_vertices_in_mesh', type=int, default=1_000_000, \n                        help='Number of vertices in the extracted mesh.')\n    parser.add_argument('--project_mesh_on_surface_points', type=str2bool, default=True, \n                        help='If True, project the mesh on the surface points for better details.')\n    parser.add_argument('-b', '--bboxmin', type=str, default=None, \n                        help='Min coordinates to use for foreground.')  \n    parser.add_argument('-B', '--bboxmax', type=str, default=None, \n                        help='Max coordinates to use for foreground.')\n    parser.add_argument('--center_bbox', type=str2bool, default=True, \n                        help='If True, center the bbox. Default is False.')\n    \n    # Parameters for refined SuGaR\n    parser.add_argument('-g', '--gaussians_per_triangle', type=int, default=1, \n                        help='Number of gaussians per triangle.')\n    parser.add_argument('-f', '--refinement_iterations', type=int, default=15_000, \n                        help='Number of refinement iterations.')\n    \n    # (Optional) Parameters for textured mesh extraction\n    parser.add_argument('-t', '--export_uv_textured_mesh', type=str2bool, default=True, \n                        help='If True, will export a textured mesh as an .obj file from the refined SuGaR model. '\n                        'Computing a traditional colored UV texture should take less than 10 minutes.')\n    parser.add_argument('--square_size',\n                        default=8, type=int, help='Size of the square to use for the UV texture.')\n    parser.add_argument('--postprocess_mesh', type=str2bool, default=False, \n                        help='If True, postprocess the mesh by removing border triangles with low-density. '\n                        'This step takes a few minutes and is not needed in general, as it can also be risky. '\n                        'However, it increases the quality of the mesh in some cases, especially when an object is visible only from one side.')\n    parser.add_argument('--postprocess_density_threshold', type=float, default=0.1,\n                        help='Threshold to use for postprocessing the mesh.')\n    parser.add_argument('--postprocess_iterations', type=int, default=5,\n                        help='Number of iterations to use for postprocessing the mesh.')\n    \n    # (Optional) PLY file export\n    parser.add_argument('--export_ply', type=str2bool, default=True,\n                        help='If True, export a ply file with the refined 3D Gaussians at the end of the training. '\n                        'This file can be large (+/- 500MB), but is needed for using the dedicated viewer. Default is True.')\n    \n    # (Optional) Default configurations\n    parser.add_argument('--low_poly', type=str2bool, default=False, \n                        help='Use standard config for a low poly mesh, with 200k vertices and 6 Gaussians per triangle.')\n    parser.add_argument('--high_poly', type=str2bool, default=False,\n                        help='Use standard config for a high poly mesh, with 1M vertices and 1 Gaussians per triangle.')\n    parser.add_argument('--refinement_time', type=str, default=None, \n                        help=\"Default configs for time to spend on refinement. Can be 'short', 'medium' or 'long'.\")\n      \n    # Evaluation split\n    parser.add_argument('--eval', type=str2bool, default=True, help='Use eval split.')\n\n    # GPU\n    parser.add_argument('--gpu', type=int, default=0, help='Index of GPU device to use.')\n    parser.add_argument('--white_background', type=str2bool, default=False, help='Use a white background instead of black.')\n\n    # Parse arguments\n    args = parser.parse_args()\n    if args.low_poly:\n        args.n_vertices_in_mesh = 200_000\n        args.gaussians_per_triangle = 6\n        print('Using low poly config.')\n    if args.high_poly:\n        args.n_vertices_in_mesh = 1_000_000\n        args.gaussians_per_triangle = 1\n        print('Using high poly config.')\n    if args.refinement_time == 'short':\n        args.refinement_iterations = 2_000\n        print('Using short refinement time.')\n    if args.refinement_time == 'medium':\n        args.refinement_iterations = 7_000\n        print('Using medium refinement time.')\n    if args.refinement_time == 'long':\n        args.refinement_iterations = 15_000\n        print('Using long refinement time.')\n    if args.export_uv_textured_mesh:\n        print('Will export a UV-textured mesh as an .obj file.')\n    if args.export_ply:\n        print('Will export a ply file with the refined 3D Gaussians at the end of the training.')\n    \n    # ----- Optimize coarse SuGaR -----\n    coarse_args = AttrDict({\n        'checkpoint_path': args.checkpoint_path,\n        'scene_path': args.scene_path,\n        'iteration_to_load': args.iteration_to_load,\n        'output_dir': None,\n        'eval': args.eval,\n        'estimation_factor': 0.2,\n        'normal_factor': 0.2,\n        'gpu': args.gpu,\n        'white_background': args.white_background,\n    })\n    if args.regularization_type == 'sdf':\n        coarse_sugar_path = coarse_training_with_sdf_regularization(coarse_args)\n    elif args.regularization_type == 'density':\n        coarse_sugar_path = coarse_training_with_density_regularization(coarse_args)\n    elif args.regularization_type == 'dn_consistency':\n        coarse_sugar_path = coarse_training_with_density_regularization_and_dn_consistency(coarse_args)\n    else:\n        raise ValueError(f'Unknown regularization type: {args.regularization_type}')\n    \n    \n    # ----- Extract mesh from coarse SuGaR -----\n    coarse_mesh_args = AttrDict({\n        'scene_path': args.scene_path,\n        'checkpoint_path': args.checkpoint_path,\n        'iteration_to_load': args.iteration_to_load,\n        'coarse_model_path': coarse_sugar_path,\n        'surface_level': args.surface_level,\n        'decimation_target': args.n_vertices_in_mesh,\n        'project_mesh_on_surface_points': args.project_mesh_on_surface_points,\n        'mesh_output_dir': None,\n        'bboxmin': args.bboxmin,\n        'bboxmax': args.bboxmax,\n        'center_bbox': args.center_bbox,\n        'gpu': args.gpu,\n        'eval': args.eval,\n        'use_centers_to_extract_mesh': False,\n        'use_marching_cubes': False,\n        'use_vanilla_3dgs': False,\n    })\n    coarse_mesh_path = extract_mesh_from_coarse_sugar(coarse_mesh_args)[0]\n    \n    \n    # ----- Refine SuGaR -----\n    refined_args = AttrDict({\n        'scene_path': args.scene_path,\n        'checkpoint_path': args.checkpoint_path,\n        'mesh_path': coarse_mesh_path,      \n        'output_dir': None,\n        'iteration_to_load': args.iteration_to_load,\n        'normal_consistency_factor': 0.1,    \n        'gaussians_per_triangle': args.gaussians_per_triangle,        \n        'n_vertices_in_fg': args.n_vertices_in_mesh,\n        'refinement_iterations': args.refinement_iterations,\n        'bboxmin': args.bboxmin,\n        'bboxmax': args.bboxmax,\n        'export_ply': args.export_ply,\n        'eval': args.eval,\n        'gpu': args.gpu,\n        'white_background': args.white_background,\n    })\n    refined_sugar_path = refined_training(refined_args)\n    \n    \n    # ----- Extract mesh and texture from refined SuGaR -----\n    if args.export_uv_textured_mesh:\n        refined_mesh_args = AttrDict({\n            'scene_path': args.scene_path,\n            'iteration_to_load': args.iteration_to_load,\n            'checkpoint_path': args.checkpoint_path,\n            'refined_model_path': refined_sugar_path,\n            'mesh_output_dir': None,\n            'n_gaussians_per_surface_triangle': args.gaussians_per_triangle,\n            'square_size': args.square_size,\n            'eval': args.eval,\n            'gpu': args.gpu,\n            'postprocess_mesh': args.postprocess_mesh,\n            'postprocess_density_threshold': args.postprocess_density_threshold,\n            'postprocess_iterations': args.postprocess_iterations,\n        })\n        refined_mesh_path = extract_mesh_and_texture_from_refined_sugar(refined_mesh_args)\n        "
        },
        {
          "name": "train_coarse_density.py",
          "type": "blob",
          "size": 1.650390625,
          "content": "import argparse\nfrom sugar_utils.general_utils import str2bool\nfrom sugar_trainers.coarse_density import coarse_training_with_density_regularization\n\n\nif __name__ == \"__main__\":\n    # Parser\n    parser = argparse.ArgumentParser(description='Script to optimize a coarse SuGaR model, i.e. a 3D Gaussian Splatting model with surface regularization losses in density space.')\n    parser.add_argument('-c', '--checkpoint_path', \n                        type=str, \n                        help='path to the vanilla 3D Gaussian Splatting Checkpoint to load.')\n    parser.add_argument('-s', '--scene_path',\n                        type=str, \n                        help='path to the scene data to use.')\n    parser.add_argument('-o', '--output_dir',\n                        type=str, default=None, \n                        help='path to the output directory.')\n    parser.add_argument('-i', '--iteration_to_load', \n                        type=int, default=7000, \n                        help='iteration to load.')\n    \n    parser.add_argument('--eval', type=str2bool, default=True, help='Use eval split.')\n    parser.add_argument('--white_background', type=str2bool, default=False, help='Use a white background instead of black.')\n    \n    parser.add_argument('-e', '--estimation_factor', type=float, default=0.2, help='factor to multiply the estimation loss by.')\n    parser.add_argument('-n', '--normal_factor', type=float, default=0.2, help='factor to multiply the normal loss by.')\n    \n    parser.add_argument('--gpu', type=int, default=0, help='Index of GPU device to use.')\n\n    args = parser.parse_args()\n    \n    # Call function\n    coarse_training_with_density_regularization(args)\n    "
        },
        {
          "name": "train_coarse_sdf.py",
          "type": "blob",
          "size": 1.634765625,
          "content": "import argparse\nfrom sugar_utils.general_utils import str2bool\nfrom sugar_trainers.coarse_sdf import coarse_training_with_sdf_regularization\n\n\nif __name__ == \"__main__\":\n    # Parser\n    parser = argparse.ArgumentParser(description='Script to optimize a coarse SuGaR model, i.e. a 3D Gaussian Splatting model with surface regularization losses in SDF space.')\n    parser.add_argument('-c', '--checkpoint_path', \n                        type=str, \n                        help='path to the vanilla 3D Gaussian Splatting Checkpoint to load.')\n    parser.add_argument('-s', '--scene_path',\n                        type=str, \n                        help='path to the scene data to use.')\n    parser.add_argument('-o', '--output_dir',\n                        type=str, default=None, \n                        help='path to the output directory.')\n    parser.add_argument('-i', '--iteration_to_load', \n                        type=int, default=7000, \n                        help='iteration to load.')\n    \n    parser.add_argument('--eval', type=str2bool, default=True, help='Use eval split.')\n    parser.add_argument('--white_background', type=str2bool, default=False, help='Use a white background instead of black.')\n    \n    parser.add_argument('-e', '--estimation_factor', type=float, default=0.2, help='factor to multiply the estimation loss by.')\n    parser.add_argument('-n', '--normal_factor', type=float, default=0.2, help='factor to multiply the normal loss by.')\n    \n    parser.add_argument('--gpu', type=int, default=0, help='Index of GPU device to use.')\n\n    args = parser.parse_args()\n    \n    # Call function\n    coarse_training_with_sdf_regularization(args)\n    "
        },
        {
          "name": "train_full_pipeline.py",
          "type": "blob",
          "size": 8.2255859375,
          "content": "import os\nimport argparse\nfrom sugar_utils.general_utils import str2bool\n\n\nclass AttrDict(dict):\n        def __init__(self, *args, **kwargs):\n            super().__init__(*args, **kwargs)\n            self.__dict__ = self\n\n\nif __name__ == \"__main__\":\n    # ----- Parser -----\n    parser = argparse.ArgumentParser(description='Script to optimize a full SuGaR model.')\n    \n    # Data and vanilla 3DGS checkpoint\n    parser.add_argument('-s', '--scene_path',\n                        type=str, \n                        help='(Required) path to the scene data to use.')  \n    \n    # Vanilla 3DGS optimization at beginning\n    parser.add_argument('--gs_output_dir', type=str, default=None,\n                        help='(Optional) If None, will automatically train a vanilla Gaussian Splatting model at the beginning of the training. '\n                        'Else, skips the vanilla Gaussian Splatting optimization and use the checkpoint in the provided directory.')\n    \n    # Regularization for coarse SuGaR\n    parser.add_argument('-r', '--regularization_type', type=str,\n                        help='(Required) Type of regularization to use for coarse SuGaR. Can be \"sdf\", \"density\" or \"dn_consistency\". ' \n                        'We recommend using \"dn_consistency\" for the best mesh quality.')\n    \n    # Extract mesh\n    parser.add_argument('-l', '--surface_level', type=float, default=0.3, \n                        help='Surface level to extract the mesh at. Default is 0.3')\n    parser.add_argument('-v', '--n_vertices_in_mesh', type=int, default=1_000_000, \n                        help='Number of vertices in the extracted mesh.')\n    parser.add_argument('--project_mesh_on_surface_points', type=str2bool, default=True, \n                        help='If True, project the mesh on the surface points for better details.')\n    parser.add_argument('-b', '--bboxmin', type=str, default=None, \n                        help='Min coordinates to use for foreground.')  \n    parser.add_argument('-B', '--bboxmax', type=str, default=None, \n                        help='Max coordinates to use for foreground.')\n    parser.add_argument('--center_bbox', type=str2bool, default=True, \n                        help='If True, center the bbox. Default is False.')\n    \n    # Parameters for refined SuGaR\n    parser.add_argument('-g', '--gaussians_per_triangle', type=int, default=1, \n                        help='Number of gaussians per triangle.')\n    parser.add_argument('-f', '--refinement_iterations', type=int, default=15_000, \n                        help='Number of refinement iterations.')\n    \n    # (Optional) Parameters for textured mesh extraction\n    parser.add_argument('-t', '--export_obj', type=str2bool, default=True, \n                        help='If True, will export a textured mesh as an .obj file from the refined SuGaR model. '\n                        'Computing a traditional colored UV texture should take less than 10 minutes.')\n    parser.add_argument('--square_size',\n                        default=8, type=int, help='Size of the square to use for the UV texture.')\n    parser.add_argument('--postprocess_mesh', type=str2bool, default=False, \n                        help='If True, postprocess the mesh by removing border triangles with low-density. '\n                        'This step takes a few minutes and is not needed in general, as it can also be risky. '\n                        'However, it increases the quality of the mesh in some cases, especially when an object is visible only from one side.')\n    parser.add_argument('--postprocess_density_threshold', type=float, default=0.1,\n                        help='Threshold to use for postprocessing the mesh.')\n    parser.add_argument('--postprocess_iterations', type=int, default=5,\n                        help='Number of iterations to use for postprocessing the mesh.')\n    \n    # (Optional) PLY file export\n    parser.add_argument('--export_ply', type=str2bool, default=True,\n                        help='If True, export a ply file with the refined 3D Gaussians at the end of the training. '\n                        'This file can be large (+/- 500MB), but is needed for using the dedicated viewer. Default is True.')\n    \n    # (Optional) Default configurations\n    parser.add_argument('--low_poly', type=str2bool, default=False, \n                        help='Use standard config for a low poly mesh, with 200k vertices and 6 Gaussians per triangle.')\n    parser.add_argument('--high_poly', type=str2bool, default=False,\n                        help='Use standard config for a high poly mesh, with 1M vertices and 1 Gaussians per triangle.')\n    parser.add_argument('--refinement_time', type=str, default=None, \n                        help=\"Default configs for time to spend on refinement. Can be 'short', 'medium' or 'long'.\")\n      \n    # Evaluation split\n    parser.add_argument('--eval', type=str2bool, default=True, help='Use eval split.')\n\n    # GPU\n    parser.add_argument('--gpu', type=int, default=0, help='Index of GPU device to use.')\n    parser.add_argument('--white_background', type=str2bool, default=False, help='Use a white background instead of black.')\n\n    # Parse arguments\n    args = parser.parse_args()\n    if args.low_poly:\n        args.n_vertices_in_mesh = 200_000\n        args.gaussians_per_triangle = 6\n        print('Using low poly config.')\n    if args.high_poly:\n        args.n_vertices_in_mesh = 1_000_000\n        args.gaussians_per_triangle = 1\n        print('Using high poly config.')\n    if args.refinement_time == 'short':\n        args.refinement_iterations = 2_000\n        print('Using short refinement time.')\n    if args.refinement_time == 'medium':\n        args.refinement_iterations = 7_000\n        print('Using medium refinement time.')\n    if args.refinement_time == 'long':\n        args.refinement_iterations = 15_000\n        print('Using long refinement time.')\n    if args.export_obj:\n        print('Will export a UV-textured mesh as an .obj file.')\n    if args.export_ply:\n        print('Will export a ply file with the refined 3D Gaussians at the end of the training.')\n        \n    # Output directory for the vanilla 3DGS checkpoint\n    if args.gs_output_dir is None:\n        sep = os.path.sep\n        if len(args.scene_path.split(sep)[-1]) > 0:\n            gs_checkpoint_dir = os.path.join(\"output\", \"vanilla_gs\", args.scene_path.split(sep)[-1])\n        else:\n            gs_checkpoint_dir = os.path.join(\"output\", \"vanilla_gs\", args.scene_path.split(sep)[-2])\n        gs_checkpoint_dir = gs_checkpoint_dir + sep\n\n        # Trains a 3DGS scene for 7k iterations\n        white_background_str = '-w ' if args.white_background else ''\n        os.system(\n            f\"CUDA_VISIBLE_DEVICES={args.gpu} python ./gaussian_splatting/train.py \\\n                -s {args.scene_path} \\\n                -m {gs_checkpoint_dir} \\\n                {white_background_str}\\\n                --iterations 7_000\"\n        )\n    else:\n        print(\"A vanilla 3DGS checkpoint was provided. Skipping the vanilla 3DGS optimization.\")\n        gs_checkpoint_dir = args.gs_output_dir\n        if gs_checkpoint_dir[-1] != os.path.sep:\n            gs_checkpoint_dir += os.path.sep\n    \n    # Runs the train.py python script with the given arguments\n    os.system(\n        f\"python train.py \\\n            -s {args.scene_path} \\\n            -c {gs_checkpoint_dir} \\\n            -i 7_000 \\\n            -r {args.regularization_type} \\\n            -l {args.surface_level} \\\n            -v {args.n_vertices_in_mesh} \\\n            --project_mesh_on_surface_points {args.project_mesh_on_surface_points} \\\n            -g {args.gaussians_per_triangle} \\\n            -f {args.refinement_iterations} \\\n            --bboxmin {args.bboxmin} \\\n            --bboxmax {args.bboxmax} \\\n            --center_bbox {args.center_bbox} \\\n            -t {args.export_obj} \\\n            --square_size {args.square_size} \\\n            --postprocess_mesh {args.postprocess_mesh} \\\n            --postprocess_density_threshold {args.postprocess_density_threshold} \\\n            --postprocess_iterations {args.postprocess_iterations} \\\n            --export_ply {args.export_ply} \\\n            --low_poly {args.low_poly} \\\n            --high_poly {args.high_poly} \\\n            --refinement_time {args.refinement_time} \\\n            --eval {args.eval} \\\n            --gpu {args.gpu} \\\n            --white_background {args.white_background}\"\n    )"
        },
        {
          "name": "train_refined.py",
          "type": "blob",
          "size": 2.552734375,
          "content": "import argparse\nfrom sugar_utils.general_utils import str2bool\nfrom sugar_trainers.refine import refined_training\n\nif __name__ == \"__main__\":\n    # Parser\n    parser = argparse.ArgumentParser(description='Script to refine a SuGaR model.')\n    parser.add_argument('-s', '--scene_path',\n                        type=str, \n                        help='path to the scene data to use.')  \n    parser.add_argument('-c', '--checkpoint_path', \n                        type=str, \n                        help='path to the vanilla 3D Gaussian Splatting Checkpoint to load.')  \n    parser.add_argument('-m', '--mesh_path', \n                        type=str, \n                        help='Path to the extracted mesh file to use for refinement.')  \n    parser.add_argument('-o', '--output_dir',\n                        type=str, default=None, \n                        help='path to the output directory.')  \n    parser.add_argument('-i', '--iteration_to_load', \n                        type=int, default=7000, \n                        help='iteration to load.')  \n    \n    parser.add_argument('-n', '--normal_consistency_factor', type=float, default=0.1, \n                        help='Factor to multiply the normal consistency loss by.')  \n    parser.add_argument('-g', '--gaussians_per_triangle', type=int, default=1, \n                        help='Number of gaussians per triangle.')  \n    parser.add_argument('-v', '--n_vertices_in_fg', type=int, default=1_000_000, \n                        help='Number of vertices in the foreground (Mesh resolution). Used for computing learning rates.')  \n    parser.add_argument('-f', '--refinement_iterations', type=int, default=15_000, \n                        help='Number of refinement iterations.')\n    \n    parser.add_argument('-b', '--bboxmin', type=str, default=None, \n                        help='Min coordinates to use for foreground.')  \n    parser.add_argument('-B', '--bboxmax', type=str, default=None, \n                        help='Max coordinates to use for foreground.')  \n    \n    parser.add_argument('--eval', type=str2bool, default=True, help='Use eval split.')\n    parser.add_argument('--white_background', type=str2bool, default=False, help='Use a white background instead of black.')\n    \n    parser.add_argument('--gpu', type=int, default=0, help='Index of GPU device to use.')\n    \n    parser.add_argument('--export_ply', type=str2bool, default=True, \n                        help='If True, export a ply files with the refined 3D Gaussians at the end of the training.')\n\n    args = parser.parse_args()\n    \n    # Call function\n    refined_training(args)\n    "
        },
        {
          "name": "view_sugar_results.ipynb",
          "type": "blob",
          "size": 11.3662109375,
          "content": "{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"%%HTML\\n\",\n    \"<style>\\n\",\n    \"    body {\\n\",\n    \"        --vscode-font-family: \\\"Roboto Thin\\\"\\n\",\n    \"    }\\n\",\n    \"</style>\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"<center>\\n\",\n    \"<h1> SuGaR: Surface-Aligned Gaussian Splatting for Efficient 3D Mesh Reconstruction \\n\",\n    \"    <br>and High-Quality Mesh Rendering</h1>\\n\",\n    \"Antoine Gu√©don and Vincent Lepetit\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import os\\n\",\n    \"import torch\\n\",\n    \"import numpy as np\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import open3d as o3d\\n\",\n    \"from pytorch3d.io import load_objs_as_meshes\\n\",\n    \"from pytorch3d.renderer import (\\n\",\n    \"    AmbientLights,\\n\",\n    \"    RasterizationSettings, \\n\",\n    \"    MeshRenderer, \\n\",\n    \"    MeshRasterizer,  \\n\",\n    \"    SoftPhongShader,\\n\",\n    \"    )\\n\",\n    \"from pytorch3d.renderer.blending import BlendParams\\n\",\n    \"from sugar_scene.gs_model import GaussianSplattingWrapper\\n\",\n    \"from sugar_scene.sugar_model import SuGaR, load_refined_model\\n\",\n    \"from sugar_utils.spherical_harmonics import SH2RGB\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"numGPU = 0\\n\",\n    \"torch.cuda.set_device(numGPU)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Load data and vanilla Gaussian Splatting\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# ========== Loading parameters ==========\\n\",\n    \"use_eval_split = False\\n\",\n    \"n_skip_images_for_eval_split = 8\\n\",\n    \"\\n\",\n    \"iteration_to_load = 7000\\n\",\n    \"# iteration_to_load = 30_000\\n\",\n    \"\\n\",\n    \"load_gt_images = False\\n\",\n    \"use_custom_bbox = False\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Choose a data directory (the directory that contains the images subdirectory).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Example\\n\",\n    \"source_path = './data/nerfstudio/qant03/'\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Choose a corresponding vanilla Gaussian Splatting checkpoint directory.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Example\\n\",\n    \"gs_checkpoint_path = './gaussian_splatting/output/7bdae844-6/'\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Load data and 3DGS checkpoint.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# ====================Load NeRF model and training data====================\\n\",\n    \"\\n\",\n    \"# Load Gaussian Splatting checkpoint \\n\",\n    \"print(f\\\"\\\\nLoading config {gs_checkpoint_path}...\\\")\\n\",\n    \"if use_eval_split:\\n\",\n    \"    print(\\\"Performing train/eval split...\\\")\\n\",\n    \"nerfmodel = GaussianSplattingWrapper(\\n\",\n    \"    source_path=source_path,\\n\",\n    \"    output_path=gs_checkpoint_path,\\n\",\n    \"    iteration_to_load=iteration_to_load,\\n\",\n    \"    load_gt_images=load_gt_images,\\n\",\n    \"    eval_split=use_eval_split,\\n\",\n    \"    eval_split_interval=n_skip_images_for_eval_split,\\n\",\n    \"    )\\n\",\n    \"\\n\",\n    \"print(f'{len(nerfmodel.training_cameras)} training images detected.')\\n\",\n    \"print(f'The model has been trained for {iteration_to_load} steps.')\\n\",\n    \"print(len(nerfmodel.gaussians._xyz) / 1e6, \\\"M gaussians detected.\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Render with a refined SuGaR model\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Choose a corresponding refined SuGaR checkpoint directory (located in `refined/<your scene>`)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Example\\n\",\n    \"refined_sugar_folder = \\\"./output/refined/qant03/sugarfine_3Dgs7000_densityestim02_sdfnorm02_level03_decim1000000_normalconsistency01_gaussperface1/\\\"\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Choose a refinement iteration to load.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"refined_iteration_to_load = 15_000\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Load the refined SuGaR checkpoint.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"refined_sugar_path = os.path.join(refined_sugar_folder, f\\\"{refined_iteration_to_load}.pt\\\")\\n\",\n    \"print(f\\\"\\\\nLoading config {refined_sugar_path}...\\\")\\n\",\n    \"\\n\",\n    \"refined_sugar = load_refined_model(refined_sugar_path, nerfmodel)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Render an image.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"onsamerow = True\\n\",\n    \"also_render_vanilla_3dgs = False\\n\",\n    \"\\n\",\n    \"# -----Camera to render-----\\n\",\n    \"cameras_to_use = nerfmodel.training_cameras\\n\",\n    \"# cameras_to_use = nerfmodel.test_cameras\\n\",\n    \"\\n\",\n    \"cam_idx = np.random.randint(0, len(cameras_to_use.gs_cameras))\\n\",\n    \"# --------------------------\\n\",\n    \"\\n\",\n    \"refined_sugar.eval()\\n\",\n    \"refined_sugar.adapt_to_cameras(cameras_to_use)\\n\",\n    \"\\n\",\n    \"print(f\\\"Rendering image with index {cam_idx}.\\\")\\n\",\n    \"print(\\\"Image name:\\\", cameras_to_use.gs_cameras[cam_idx].image_name)\\n\",\n    \"\\n\",\n    \"verbose = False\\n\",\n    \"normalize_img = False\\n\",\n    \"\\n\",\n    \"if load_gt_images:\\n\",\n    \"    gt_rgb = nerfmodel.get_gt_image(cam_idx)\\n\",\n    \"    i_sugar = 2\\n\",\n    \"else:\\n\",\n    \"    i_sugar = 1\\n\",\n    \"\\n\",\n    \"with torch.no_grad():\\n\",\n    \"    if also_render_vanilla_3dgs:\\n\",\n    \"        gs_image = nerfmodel.render_image(\\n\",\n    \"            nerf_cameras=cameras_to_use,\\n\",\n    \"            camera_indices=cam_idx).clamp(min=0, max=1)\\n\",\n    \"    \\n\",\n    \"    sugar_image = refined_sugar.render_image_gaussian_rasterizer(\\n\",\n    \"        nerf_cameras=cameras_to_use, \\n\",\n    \"        camera_indices=cam_idx,\\n\",\n    \"        # bg_color=1. * torch.Tensor([1.0, 1.0, 1.0]).to(rc_fine.device),\\n\",\n    \"        sh_deg=nerfmodel.gaussians.active_sh_degree,\\n\",\n    \"        compute_color_in_rasterizer=True,\\n\",\n    \"    ).nan_to_num().clamp(min=0, max=1)\\n\",\n    \"\\n\",\n    \"# Change this to adjust the size of the plot\\n\",\n    \"plot_ratio = 2. # 0.7, 1.5, 5\\n\",\n    \"\\n\",\n    \"if also_render_vanilla_3dgs:\\n\",\n    \"    plt.figure(figsize=(10 * plot_ratio, 10 * plot_ratio))\\n\",\n    \"    plt.axis(\\\"off\\\")\\n\",\n    \"    plt.title(\\\"Vanilla 3DGS render\\\")\\n\",\n    \"    plt.imshow(gs_image.cpu().numpy())\\n\",\n    \"    plt.show()\\n\",\n    \"plt.figure(figsize=(10 * plot_ratio, 10 * plot_ratio))\\n\",\n    \"plt.axis(\\\"off\\\")\\n\",\n    \"plt.title(\\\"Refined SuGaR render\\\")\\n\",\n    \"plt.imshow(sugar_image.cpu().numpy())\\n\",\n    \"plt.show()\\n\",\n    \"torch.cuda.empty_cache()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Render with a traditional color texture for SuGaR mesh\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Select the path to the textured mesh (i.e. the obj file in `refined_mesh/<your scene>`).<br>\\n\",\n    \"If None, the path to the mesh will be automatically computed from the checkpoint path.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"refined_mesh_path = None\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"Load mesh.\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"if refined_mesh_path is None:\\n\",\n    \"    post_processed = False\\n\",\n    \"\\n\",\n    \"    if post_processed:\\n\",\n    \"        post_processed_str = '_postprocessed'\\n\",\n    \"    else:\\n\",\n    \"        post_processed_str = ''\\n\",\n    \"\\n\",\n    \"    scene_name = refined_sugar_path.split('/')[-3]\\n\",\n    \"    refined_mesh_dir = './output/refined_mesh'\\n\",\n    \"    refined_mesh_path = os.path.join(\\n\",\n    \"        refined_mesh_dir, scene_name,\\n\",\n    \"        refined_sugar_path.split('/')[-2].split('.')[0] + '.obj'\\n\",\n    \"    )\\n\",\n    \"    \\n\",\n    \"print(f\\\"Loading refined mesh from {refined_mesh_path}, this could take a minute...\\\")\\n\",\n    \"textured_mesh = load_objs_as_meshes([refined_mesh_path]).to(nerfmodel.device)\\n\",\n    \"print(f\\\"Loaded textured mesh with {len(textured_mesh.verts_list()[0])} vertices and {len(textured_mesh.faces_list()[0])} faces.\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# -----Camera index to render-----\\n\",\n    \"cam_idx = np.random.randint(0, len(cameras_to_use))\\n\",\n    \"# --------------------------\\n\",\n    \"\\n\",\n    \"faces_per_pixel = 1\\n\",\n    \"max_faces_per_bin = 50_000\\n\",\n    \"\\n\",\n    \"mesh_raster_settings = RasterizationSettings(\\n\",\n    \"    image_size=(refined_sugar.image_height, refined_sugar.image_width),\\n\",\n    \"    blur_radius=0.0, \\n\",\n    \"    faces_per_pixel=faces_per_pixel,\\n\",\n    \"    # max_faces_per_bin=max_faces_per_bin\\n\",\n    \")\\n\",\n    \"lights = AmbientLights(device=nerfmodel.device)\\n\",\n    \"rasterizer = MeshRasterizer(\\n\",\n    \"        cameras=cameras_to_use.p3d_cameras[cam_idx], \\n\",\n    \"        raster_settings=mesh_raster_settings,\\n\",\n    \"    )\\n\",\n    \"renderer = MeshRenderer(\\n\",\n    \"    rasterizer=rasterizer,\\n\",\n    \"    shader=SoftPhongShader(\\n\",\n    \"        device=refined_sugar.device, \\n\",\n    \"        cameras=cameras_to_use.p3d_cameras[cam_idx],\\n\",\n    \"        lights=lights,\\n\",\n    \"        # blend_params=BlendParams(background_color=(0.0, 0.0, 0.0)),\\n\",\n    \"        blend_params=BlendParams(background_color=(1.0, 1.0, 1.0)),\\n\",\n    \"    )\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"with torch.no_grad():    \\n\",\n    \"    print(\\\"Rendering image\\\", cam_idx)\\n\",\n    \"    print(\\\"Image ID:\\\", cameras_to_use.gs_cameras[cam_idx].image_name)\\n\",\n    \"    \\n\",\n    \"    p3d_cameras = cameras_to_use.p3d_cameras[cam_idx]\\n\",\n    \"    rgb_img = renderer(textured_mesh, cameras=p3d_cameras)[0, ..., :3]\\n\",\n    \"    \\n\",\n    \"# Change this to adjust the size of the plot\\n\",\n    \"plot_ratio = 2.\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(10 * plot_ratio, 10 * plot_ratio))\\n\",\n    \"plt.axis(\\\"off\\\")\\n\",\n    \"plt.title(\\\"Refined SuGaR mesh with a traditional color UV texture\\\")\\n\",\n    \"plt.imshow(rgb_img.cpu().numpy())\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": []\n  }\n ],\n \"metadata\": {\n  \"kernelspec\": {\n   \"display_name\": \"sugar\",\n   \"language\": \"python\",\n   \"name\": \"python3\"\n  },\n  \"language_info\": {\n   \"codemirror_mode\": {\n    \"name\": \"ipython\",\n    \"version\": 3\n   },\n   \"file_extension\": \".py\",\n   \"mimetype\": \"text/x-python\",\n   \"name\": \"python\",\n   \"nbconvert_exporter\": \"python\",\n   \"pygments_lexer\": \"ipython3\",\n   \"version\": \"3.9.18\"\n  },\n  \"orig_nbformat\": 4\n },\n \"nbformat\": 4,\n \"nbformat_minor\": 2\n}\n"
        }
      ]
    }
  ]
}