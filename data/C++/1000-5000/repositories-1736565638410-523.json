{
  "metadata": {
    "timestamp": 1736565638410,
    "page": 523,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjUzMA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "shouxieai/tensorRT_Pro",
      "stars": 2657,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 1.4052734375,
          "content": "#\ntutorial/2.0CenterNet_from_torch_trt/0_to_1_python_to_cuda/cpp_cuda_centernet/src/tensorRT\ntutorial/2.0CenterNet_from_torch_trt/0_to_1_python_to_cuda/cpp_cuda_centernet/objs\ntutorial/2.0CenterNet_from_torch_trt/0_to_1_python_to_cuda/cpp_cuda_centernet/workspace/pro\n\n# compressed files\n*.tar.gz\n*.zip\n\n# temp tensor and data\n*.tensor\n*.data\n\n\n# Prerequisites\n*.d\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n/objs\n\n*.trtmodel\n*.onnx\n/workspace/pro\n/build\n/workspace/*.avi\n/workspace/.ipynb_checkpoints\n/workspace/*_result\n/workspace/face/library_draw\n/workspace/face/result\n/workspace/face/library/laq.jpg\n__pycache__\n/tools/process_so.sh\n/tools/proc2.sh\n/python/pytrt.egg-info\n/python/dist\n/python/build\n/workspace/formtest.ipynb\n/workspace/meta.json\n/.vs\n*.pyd\n*.zip\n*.pdb\n*.ilk\n*.lib\n*.exp\n\n/lean/cuda10.1\n/lean/cudnn8.2.2.26\n/lean/opencv3.4.6\n/lean/protobuf3.11.4\n/lean/TensorRT-8.0.1.6\n\n__pycache__\n\n!/workspace/wget.exe\n/workspace/*.mp4\n/workspace/single_inference\n/workspace/exp/tracker.final.mp4\n/workspace/perf.result.log\n/simple_yolo/workspace/pro\n/simple_yolo/objs\n/workspace/*.json\n/workspace/*.png\n/restful_server/workspace/pro\n/restful_server/objs\n/restful_server/workspace/*.trtmodel\n/restful_server/workspace/*.onnx"
        },
        {
          "name": ".vscode",
          "type": "tree",
          "content": null
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 5.763671875,
          "content": "cmake_minimum_required(VERSION 2.6)\nproject(pro)\n\noption(CUDA_USE_STATIC_CUDA_RUNTIME OFF)\nset(CMAKE_CXX_STANDARD 11)\nset(CMAKE_BUILD_TYPE Debug)\nset(EXECUTABLE_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/workspace)\n\n# 如果要支持python则设置python路径\nset(HAS_PYTHON ON)\nset(PythonRoot \"/datav/software/anaconda3\")\nset(PythonName \"python3.9\")\n\n# 如果你是不同显卡，请设置为显卡对应的号码参考这里：https://developer.nvidia.com/zh-cn/cuda-gpus#compute\n#set(CUDA_GEN_CODE \"-gencode=arch=compute_75,code=sm_75\")\n\n# 如果你的opencv找不到，可以自己指定目录\nset(OpenCV_DIR   \"/datav/lean/opencv-4.2.0/lib/cmake/opencv4/\")\n\nset(CUDA_TOOLKIT_ROOT_DIR     \"/datav/lean/cuda-11.2\")\nset(CUDNN_DIR    \"/datav/lean/cudnn8.2.4.15-cuda11.4\")\nset(TENSORRT_DIR \"/datav/lean/TensorRT-8.2.3.0-cuda11.4-cudnn8.2\")\n\n# set(CUDA_TOOLKIT_ROOT_DIR     \"/data/sxai/lean/cuda-10.2\")\n# set(CUDNN_DIR    \"/data/sxai/lean/cudnn7.6.5.32-cuda10.2\")\n# set(TENSORRT_DIR \"/data/sxai/lean/TensorRT-7.0.0.11\")\n\n# set(CUDA_TOOLKIT_ROOT_DIR  \"/data/sxai/lean/cuda-11.1\")\n# set(CUDNN_DIR    \"/data/sxai/lean/cudnn8.2.2.26\")\n# set(TENSORRT_DIR \"/data/sxai/lean/TensorRT-7.2.1.6\")\n\n# 因为protobuf，需要用特定版本，所以这里指定路径\nset(PROTOBUF_DIR \"/datav/lean/protobuf3.11.4\")\n\n\nfind_package(CUDA REQUIRED)\nfind_package(OpenCV)\n\ninclude_directories(\n    ${PROJECT_SOURCE_DIR}/src\n    ${PROJECT_SOURCE_DIR}/src/application\n    ${PROJECT_SOURCE_DIR}/src/tensorRT\n    ${PROJECT_SOURCE_DIR}/src/tensorRT/common\n    ${OpenCV_INCLUDE_DIRS}\n    ${CUDA_TOOLKIT_ROOT_DIR}/include\n    ${PROTOBUF_DIR}/include\n    ${TENSORRT_DIR}/include\n    ${CUDNN_DIR}/include\n)\n\n# 切记，protobuf的lib目录一定要比tensorRT目录前面，因为tensorRTlib下带有protobuf的so文件\n# 这可能带来错误\nlink_directories(\n    ${PROTOBUF_DIR}/lib\n    ${TENSORRT_DIR}/lib\n    ${CUDA_TOOLKIT_ROOT_DIR}/lib64\n    ${CUDNN_DIR}/lib\n)\n\nif(\"${HAS_PYTHON}\" STREQUAL \"ON\")\n    message(\"Usage Python ${PythonRoot}\")\n    include_directories(${PythonRoot}/include/${PythonName})\n    link_directories(${PythonRoot}/lib)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DHAS_PYTHON\")\nendif()\n\nset(CMAKE_CXX_FLAGS  \"${CMAKE_CXX_FLAGS} -std=c++11 -Wall -O0 -Wfatal-errors -pthread -w -g\")\nset(CUDA_NVCC_FLAGS \"${CUDA_NVCC_FLAGS} -std=c++11 -O0 -Xcompiler -fPIC -g -w ${CUDA_GEN_CODE}\")\nfile(GLOB_RECURSE cpp_srcs ${PROJECT_SOURCE_DIR}/src/*.cpp)\nfile(GLOB_RECURSE cuda_srcs ${PROJECT_SOURCE_DIR}/src/*.cu)\ncuda_add_library(plugin_list SHARED ${cuda_srcs})\ntarget_link_libraries(plugin_list nvinfer nvinfer_plugin)\ntarget_link_libraries(plugin_list cuda cublas cudart cudnn)\ntarget_link_libraries(plugin_list protobuf pthread)\ntarget_link_libraries(plugin_list ${OpenCV_LIBS})\n\nadd_executable(pro ${cpp_srcs})\n\n# 如果提示插件找不到，请使用dlopen(xxx.so, NOW)的方式手动加载可以解决插件找不到问题\ntarget_link_libraries(pro nvinfer nvinfer_plugin)\ntarget_link_libraries(pro cuda cublas cudart cudnn)\ntarget_link_libraries(pro protobuf pthread plugin_list)\ntarget_link_libraries(pro ${OpenCV_LIBS})\n\nif(\"${HAS_PYTHON}\" STREQUAL \"ON\")\n    set(LIBRARY_OUTPUT_PATH ${PROJECT_SOURCE_DIR}/example-python/pytrt)\n    add_library(pytrtc SHARED ${cpp_srcs})\n    target_link_libraries(pytrtc nvinfer nvinfer_plugin)\n    target_link_libraries(pytrtc cuda cublas cudart cudnn)\n    target_link_libraries(pytrtc protobuf pthread plugin_list)\n    target_link_libraries(pytrtc ${OpenCV_LIBS})\n    target_link_libraries(pytrtc \"${PythonName}\")\n    target_link_libraries(pro \"${PythonName}\")\nendif()\n\nadd_custom_target(\n    yolo\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro yolo\n)\n\nadd_custom_target(\n    yolo_gpuptr\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro yolo_gpuptr\n)\n\nadd_custom_target(\n    yolo_fast\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro yolo_fast\n)\n\nadd_custom_target(\n    centernet\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro centernet\n)\n\nadd_custom_target(\n    alphapose \n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro alphapose\n)\n\nadd_custom_target(\n    retinaface\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro retinaface\n)\n\nadd_custom_target(\n    dbface\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro dbface\n)\n\nadd_custom_target(\n    arcface \n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro arcface\n)\n\nadd_custom_target(\n    bert \n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro bert\n)\n\nadd_custom_target(\n    fall\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro fall_recognize\n)\n\nadd_custom_target(\n    scrfd\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro scrfd\n)\n\nadd_custom_target(\n    lesson\n    DEPENDS pro\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/workspace\n    COMMAND ./pro lesson\n)\n\nadd_custom_target(\n    pyscrfd\n    DEPENDS pytrtc\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/example-python\n    COMMAND python test_scrfd.py\n)\n\nadd_custom_target(\n    pyinstall\n    DEPENDS pytrtc\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/example-python\n    COMMAND python setup.py install\n)\n\nadd_custom_target(\n    pytorch\n    DEPENDS pytrtc\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/example-python\n    COMMAND python test_torch.py\n)\n\nadd_custom_target(\n    pyyolov5\n    DEPENDS pytrtc\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/example-python\n    COMMAND python test_yolov5.py\n)\n\nadd_custom_target(\n    pycenternet\n    DEPENDS pytrtc\n    WORKING_DIRECTORY ${PROJECT_SOURCE_DIR}/example-python\n    COMMAND python test_centernet.py\n)"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 1.0419921875,
          "content": "MIT License\n\nCopyright (c) 2022 TensorRTPro\n\nPermission is hereby granted, free of charge, to any person obtaining a copy\nof this software and associated documentation files (the \"Software\"), to deal\nin the Software without restriction, including without limitation the rights\nto use, copy, modify, merge, publish, distribute, sublicense, and/or sell\ncopies of the Software, and to permit persons to whom the Software is\nfurnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE."
        },
        {
          "name": "Makefile",
          "type": "blob",
          "size": 6.1328125,
          "content": "cc        := g++\nnvcc      = ${lean_cuda}/bin/nvcc\n\nlean_protobuf  := /datav/lean/protobuf3.11.4\nlean_tensor_rt := /datav/lean/TensorRT-8.2.3.0-cuda11.4-cudnn8.2\nlean_cudnn     := /datav/lean/cudnn8.2.4.15-cuda11.4\nlean_opencv    := /datav/lean/opencv-4.2.0\nlean_cuda      := /datav/lean/cuda-11.2\nuse_python     := true\npython_root    := /datav/software/anaconda3\n\n# python_root指向的lib目录下有个libpython3.9.so，因此这里写python3.9\n# 对于有些版本，so名字是libpython3.7m.so，你需要填写python3.7m\n# /datav/software/anaconda3/lib/libpython3.9.so\npython_name    := python3.9\n\n# 如果是其他显卡，请修改-gencode=arch=compute_75,code=sm_75为对应显卡的能力\n# 显卡对应的号码参考这里：https://developer.nvidia.com/zh-cn/cuda-gpus#compute\ncuda_arch := # -gencode=arch=compute_75,code=sm_75\n\ncpp_srcs  := $(shell find src -name \"*.cpp\")\ncpp_objs  := $(cpp_srcs:.cpp=.cpp.o)\ncpp_objs  := $(cpp_objs:src/%=objs/%)\ncpp_mk    := $(cpp_objs:.cpp.o=.cpp.mk)\n\ncu_srcs  := $(shell find src -name \"*.cu\")\ncu_objs  := $(cu_srcs:.cu=.cu.o)\ncu_objs  := $(cu_objs:src/%=objs/%)\ncu_mk    := $(cu_objs:.cu.o=.cu.mk)\n\ninclude_paths := src        \\\n\t\t\tsrc/application \\\n\t\t\tsrc/tensorRT\t\\\n\t\t\tsrc/tensorRT/common  \\\n\t\t\t$(lean_protobuf)/include \\\n\t\t\t$(lean_opencv)/include/opencv4 \\\n\t\t\t$(lean_tensor_rt)/include \\\n\t\t\t$(lean_cuda)/include  \\\n\t\t\t$(lean_cudnn)/include \n\nlibrary_paths := $(lean_protobuf)/lib \\\n\t\t\t$(lean_opencv)/lib    \\\n\t\t\t$(lean_tensor_rt)/lib \\\n\t\t\t$(lean_cuda)/lib64  \\\n\t\t\t$(lean_cudnn)/lib\n\nlink_librarys := opencv_core opencv_imgproc opencv_videoio opencv_imgcodecs \\\n\t\t\tnvinfer nvinfer_plugin \\\n\t\t\tcuda cublas cudart cudnn \\\n\t\t\tstdc++ protobuf dl\n\n\n# HAS_PYTHON表示是否编译python支持\nsupport_define    := \n\nifeq ($(use_python), true) \ninclude_paths  += $(python_root)/include/$(python_name)\nlibrary_paths  += $(python_root)/lib\nlink_librarys  += $(python_name)\nsupport_define += -DHAS_PYTHON\nendif\n\nempty         :=\nexport_path   := $(subst $(empty) $(empty),:,$(library_paths))\n\nrun_paths     := $(foreach item,$(library_paths),-Wl,-rpath=$(item))\ninclude_paths := $(foreach item,$(include_paths),-I$(item))\nlibrary_paths := $(foreach item,$(library_paths),-L$(item))\nlink_librarys := $(foreach item,$(link_librarys),-l$(item))\n\ncpp_compile_flags := -std=c++11 -g -w -O0 -fPIC -pthread -fopenmp $(support_define)\ncu_compile_flags  := -std=c++11 -g -w -O0 -Xcompiler \"$(cpp_compile_flags)\" $(cuda_arch) $(support_define)\nlink_flags        := -pthread -fopenmp -Wl,-rpath='$$ORIGIN'\n\ncpp_compile_flags += $(include_paths)\ncu_compile_flags  += $(include_paths)\nlink_flags        += $(library_paths) $(link_librarys) $(run_paths)\n\nifneq ($(MAKECMDGOALS), clean)\n-include $(cpp_mk) $(cu_mk)\nendif\n\npro    : workspace/pro\npytrtc : example-python/pytrt/libpytrtc.so\nexpath : library_path.txt\n\nlibrary_path.txt : \n\t@echo LD_LIBRARY_PATH=$(export_path):\"$$\"LD_LIBRARY_PATH > $@\n\nworkspace/pro : $(cpp_objs) $(cu_objs)\n\t@echo Link $@\n\t@mkdir -p $(dir $@)\n\t@$(cc) $^ -o $@ $(link_flags)\n\nexample-python/pytrt/libpytrtc.so : $(cpp_objs) $(cu_objs)\n\t@echo Link $@\n\t@mkdir -p $(dir $@)\n\t@$(cc) -shared $^ -o $@ $(link_flags)\n\nobjs/%.cpp.o : src/%.cpp\n\t@echo Compile CXX $<\n\t@mkdir -p $(dir $@)\n\t@$(cc) -c $< -o $@ $(cpp_compile_flags)\n\nobjs/%.cu.o : src/%.cu\n\t@echo Compile CUDA $<\n\t@mkdir -p $(dir $@)\n\t@$(nvcc) -c $< -o $@ $(cu_compile_flags)\n\nobjs/%.cpp.mk : src/%.cpp\n\t@echo Compile depends CXX $<\n\t@mkdir -p $(dir $@)\n\t@$(cc) -M $< -MF $@ -MT $(@:.cpp.mk=.cpp.o) $(cpp_compile_flags)\n\t\nobjs/%.cu.mk : src/%.cu\n\t@echo Compile depends CUDA $<\n\t@mkdir -p $(dir $@)\n\t@$(nvcc) -M $< -MF $@ -MT $(@:.cu.mk=.cu.o) $(cu_compile_flags)\n\nyolo : workspace/pro\n\t@cd workspace && ./pro yolo\n\nyolo_gpuptr : workspace/pro\n\t@cd workspace && ./pro yolo_gpuptr\n\ndyolo : workspace/pro\n\t@cd workspace && ./pro dyolo\n\ndunet : workspace/pro\n\t@cd workspace && ./pro dunet\n\ndmae : workspace/pro\n\t@cd workspace && ./pro dmae\n\ndclassifier : workspace/pro\n\t@cd workspace && ./pro dclassifier\n\nyolo_fast : workspace/pro\n\t@cd workspace && ./pro yolo_fast\n\nbert : workspace/pro\n\t@cd workspace && ./pro bert\n\nalphapose : workspace/pro\n\t@cd workspace && ./pro alphapose\n\nfall : workspace/pro\n\t@cd workspace && ./pro fall_recognize\n\nretinaface : workspace/pro\n\t@cd workspace && ./pro retinaface\n\narcface    : workspace/pro\n\t@cd workspace && ./pro arcface\n\ntest_warpaffine    : workspace/pro\n\t@cd workspace && ./pro test_warpaffine\n\ntest_yolo_map    : workspace/pro\n\t@cd workspace && ./pro test_yolo_map\n\narcface_video    : workspace/pro\n\t@cd workspace && ./pro arcface_video\n\narcface_tracker    : workspace/pro\n\t@cd workspace && ./pro arcface_tracker\n\ntest_all : workspace/pro\n\t@cd workspace && ./pro test_all\n\nscrfd : workspace/pro\n\t@cd workspace && ./pro scrfd\n\ncenternet : workspace/pro\n\t@cd workspace && ./pro centernet\n\ndbface : workspace/pro\n\t@cd workspace && ./pro dbface\n\nhigh_perf : workspace/pro\n\t@cd workspace && ./pro high_perf\n\nlesson : workspace/pro\n\t@cd workspace && ./pro lesson\n\nplugin : workspace/pro\n\t@cd workspace && ./pro plugin\n\npytorch : pytrtc\n\t@cd example-python && python test_torch.py\n\npyscrfd : pytrtc\n\t@cd example-python && python test_scrfd.py\n\npyretinaface : pytrtc\n\t@cd example-python && python test_retinaface.py\n\npycenternet : pytrtc\n\t@cd example-python && python test_centernet.py\n\npyyolov5 : pytrtc\n\t@cd example-python && python test_yolov5.py\n\npyyolov7 : pytrtc\n\t@cd example-python && python test_yolov7.py\n\npyyolox : pytrtc\n\t@cd example-python && python test_yolox.py\n\npyarcface : pytrtc\n\t@cd example-python && python test_arcface.py\n\npyinstall : pytrtc\n\t@cd example-python && python setup.py install\n\nclean :\n\t@rm -rf objs workspace/pro example-python/pytrt/libpytrtc.so example-python/build example-python/dist example-python/pytrt.egg-info example-python/pytrt/__pycache__\n\t@rm -rf workspace/single_inference\n\t@rm -rf workspace/scrfd_result workspace/retinaface_result\n\t@rm -rf workspace/YoloV5_result workspace/YoloX_result\n\t@rm -rf workspace/face/library_draw workspace/face/result\n\t@rm -rf build\n\t@rm -rf example-python/pytrt/libplugin_list.so\n\t@rm -rf library_path.txt\n\n.PHONY : clean yolo alphapose fall debug\n\n# 导出符号，使得运行时能够链接上\nexport LD_LIBRARY_PATH:=$(export_path):$(LD_LIBRARY_PATH)"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 31.990234375,
          "content": "*Read this in other languages: [English](README.md), [简体中文](tutorial/README.zh-cn.md).*\n\n## News: \n- 🔥 A simple implementation is released: https://github.com/shouxieai/infer\n- 🔥 Add yolov7 support .\n- 🔥 Released python solution for hardware decoding with tensorRT integration\n- 🔥 Docker Image has been released：https://hub.docker.com/r/hopef/tensorrt-pro\n- ⚡tensorRT_Pro_comments_version(co-contributing version) is also provided for a better learning experience. Repo: https://github.com/Guanbin-Huang/tensorRT_Pro_comments\n- 🔥 [Simple yolov5/yolox implemention is released. Simple and easy to use.](example-simple_yolo)\n- 🔥 yolov5-1.0-6.0/master are supported.\n- Tutorial notebooks download:\n  - [WarpAffine.lesson.tar.gz](http://zifuture.com:1000/fs/25.shared/warpaffine.lesson.tar.gz)\n  - [Offset.tar.gz](http://zifuture.com:1000/fs/25.shared/offset.tar.gz)\n- Tutorial for exporting CenterNet from pytorch to tensorRT is released. \n\n## Tutorial Video\n\n- <b>blibli</b> : https://www.bilibili.com/video/BV1Xw411f7FW (Now only in Chinese. English is comming)\n- <b>slides</b> : http://zifuture.com:1556/fs/sxai/tensorRT.pptx (Now only in Chinese. English is comming)\n- <b>tutorial folder</b>: a good intro for beginner to get a general idea of our framework.(Chinese/English)\n\n## An Out-of-the-Box TensorRT-based Framework for High Performance Inference with C++/Python Support\n\n- C++ Interface: 3 lines of code is all you need to run a YoloX\n\n  ```C++\n  // create inference engine on gpu-0\n  //auto engine = Yolo::create_infer(\"yolov5m.fp32.trtmodel\", Yolo::Type::V5, 0);\n  auto engine = Yolo::create_infer(\"yolox_m.fp32.trtmodel\", Yolo::Type::X, 0);\n  \n  // load image\n  auto image = cv::imread(\"1.jpg\");\n  \n  // do inference and get the result\n  auto box = engine->commit(image).get();  // return vector<Box>\n  ```\n\n- Python Interface:\n  ```python\n  import pytrt\n  \n  model     = models.resnet18(True).eval().to(device)\n  trt_model = tp.from_torch(model, input)\n  trt_out   = trt_model(input)\n  ```\n  \n  - simple yolo for python\n  ```python\n  import os\n  import cv2\n  import numpy as np\n  import pytrt as tp\n\n  engine_file = \"yolov5s.fp32.trtmodel\"\n  if not os.path.exists(engine_file):\n      tp.compile_onnx_to_file(1, tp.onnx_hub(\"yolov5s\"), engine_file)\n\n  yolo   = tp.Yolo(engine_file, type=tp.YoloType.V5)\n  image  = cv2.imread(\"car.jpg\")\n  bboxes = yolo.commit(image).get()\n  print(f\"{len(bboxes)} objects\")\n\n  for box in bboxes:\n      left, top, right, bottom = map(int, [box.left, box.top, box.right, box.bottom])\n      cv2.rectangle(image, (left, top), (right, bottom), tp.random_color(box.class_label), 5)\n\n  saveto = \"yolov5.car.jpg\"\n  print(f\"Save to {saveto}\")\n\n  cv2.imwrite(saveto, image)\n  cv2.imshow(\"result\", image)\n  cv2.waitKey()\n  ```\n\n## INTRO\n\n1. High level interface for C++/Python.\n2. Simplify the implementation of custom plugin. And serialization and deserialization have been encapsulated for easier usage.\n3. Simplify the compile of fp32, fp16 and int8 for facilitating the deployment with C++/Python in server or embeded device.\n4. Models ready for use also with examples are RetinaFace, Scrfd, YoloV5, YoloX, Arcface, AlphaPose, CenterNet and DeepSORT(C++)\n\n## YoloX and YoloV5-series Model Test Report\n\n<details>\n<summary>app_yolo.cpp speed testing</summary>\n  \n1. Resolution (YoloV5P5, YoloX) = (640x640),  (YoloV5P6) = (1280x1280)\n2. max batch size = 16\n3. preprocessing + inference + postprocessing\n4. cuda10.2, cudnn8.2.2.26, TensorRT-8.0.1.6\n5. RTX2080Ti\n6. num of testing: take the average on the results of 100 times but excluding the first time for warmup \n7. Testing log: [workspace/perf.result.std.log (workspace/perf.result.std.log)\n8. code for testing: [src/application/app_yolo.cpp](src/application/app_yolo.cpp)\n9. images for testing: 6 images in workspace/inference \n    - with resolution 810x1080，500x806，1024x684，550x676，1280x720，800x533 respetively\n10. Testing method: load 6 images. Then do the inference on the 6 images, which will be repeated for 100 times. Note that each image should be preprocessed and postprocessed.\n\n---\n\n| Model    | Resolution | Type      | Precision | Elapsed Time | FPS    |\n| -------- | ---------- | --------- | --------- | ------------ | ------ |\n| yolox_x  | 640x640    | YoloX     | FP32      | 21.879       | 45.71  |\n| yolox_l  | 640x640    | YoloX     | FP32      | 12.308       | 81.25  |\n| yolox_m  | 640x640    | YoloX     | FP32      | 6.862        | 145.72 |\n| yolox_s  | 640x640    | YoloX     | FP32      | 3.088        | 323.81 |\n| yolox_x  | 640x640    | YoloX     | FP16      | 6.763        | 147.86 |\n| yolox_l  | 640x640    | YoloX     | FP16      | 3.933        | 254.25 |\n| yolox_m  | 640x640    | YoloX     | FP16      | 2.515        | 397.55 |\n| yolox_s  | 640x640    | YoloX     | FP16      | 1.362        | 734.48 |\n| yolox_x  | 640x640    | YoloX     | INT8      | 4.070        | 245.68 |\n| yolox_l  | 640x640    | YoloX     | INT8      | 2.444        | 409.21 |\n| yolox_m  | 640x640    | YoloX     | INT8      | 1.730        | 577.98 |\n| yolox_s  | 640x640    | YoloX     | INT8      | 1.060        | 943.15 |\n| yolov5x6 | 1280x1280  | YoloV5_P6 | FP32      | 68.022       | 14.70  |\n| yolov5l6 | 1280x1280  | YoloV5_P6 | FP32      | 37.931       | 26.36  |\n| yolov5m6 | 1280x1280  | YoloV5_P6 | FP32      | 20.127       | 49.69  |\n| yolov5s6 | 1280x1280  | YoloV5_P6 | FP32      | 8.715        | 114.75 |\n| yolov5x  | 640x640    | YoloV5_P5 | FP32      | 18.480       | 54.11  |\n| yolov5l  | 640x640    | YoloV5_P5 | FP32      | 10.110       | 98.91  |\n| yolov5m  | 640x640    | YoloV5_P5 | FP32      | 5.639        | 177.33 |\n| yolov5s  | 640x640    | YoloV5_P5 | FP32      | 2.578        | 387.92 |\n| yolov5x6 | 1280x1280  | YoloV5_P6 | FP16      | 20.877       | 47.90  |\n| yolov5l6 | 1280x1280  | YoloV5_P6 | FP16      | 10.960       | 91.24  |\n| yolov5m6 | 1280x1280  | YoloV5_P6 | FP16      | 7.236        | 138.20 |\n| yolov5s6 | 1280x1280  | YoloV5_P6 | FP16      | 3.851        | 259.68 |\n| yolov5x  | 640x640    | YoloV5_P5 | FP16      | 5.933        | 168.55 |\n| yolov5l  | 640x640    | YoloV5_P5 | FP16      | 3.450        | 289.86 |\n| yolov5m  | 640x640    | YoloV5_P5 | FP16      | 2.184        | 457.90 |\n| yolov5s  | 640x640    | YoloV5_P5 | FP16      | 1.307        | 765.10 |\n| yolov5x6 | 1280x1280  | YoloV5_P6 | INT8      | 12.207       | 81.92  |\n| yolov5l6 | 1280x1280  | YoloV5_P6 | INT8      | 7.221        | 138.49 |\n| yolov5m6 | 1280x1280  | YoloV5_P6 | INT8      | 5.248        | 190.55 |\n| yolov5s6 | 1280x1280  | YoloV5_P6 | INT8      | 3.149        | 317.54 |\n| yolov5x  | 640x640    | YoloV5_P5 | INT8      | 3.704        | 269.97 |\n| yolov5l  | 640x640    | YoloV5_P5 | INT8      | 2.255        | 443.53 |\n| yolov5m  | 640x640    | YoloV5_P5 | INT8      | 1.674        | 597.40 |\n| yolov5s  | 640x640    | YoloV5_P5 | INT8      | 1.143        | 874.91 |\n</details>\n\n<details>\n<summary>app_yolo_fast.cpp speed testing. Never stop desiring for being faster</summary>\n  \n- <b>Highlight:</b>   0.5 ms faster without any loss in precision compared with the above. Specifically, we remove the Focus and some transpose nodes etc, and implement them in CUDA kenerl function. But the rest remains the same.\n- <b>Test log:</b>   [workspace/perf.result.std.log](workspace/perf.result.std.log)\n- <b>Code for testing:</b>   [src/application/app_yolo_fast.cpp](src/application/app_yolo_fast.cpp)\n- <b>Tips:</b>   you can do the modification while refering to the downloaded onnx. Any questions are welcomed through any kinds of contact.\n- <b>Conclusion:</b>   the main idea of this work is to optimize the pre-and-post processing. If you go for yolox, yolov5 small version, the optimization might help you.\n\n|Model|Resolution|Type|Precision|Elapsed Time|FPS|\n|---|---|---|---|---|---|\n|yolox_x_fast|640x640|YoloX|FP32|21.598 |46.30 |\n|yolox_l_fast|640x640|YoloX|FP32|12.199 |81.97 |\n|yolox_m_fast|640x640|YoloX|FP32|6.819 |146.65 |\n|yolox_s_fast|640x640|YoloX|FP32|2.979 |335.73 |\n|yolox_x_fast|640x640|YoloX|FP16|6.764 |147.84 |\n|yolox_l_fast|640x640|YoloX|FP16|3.866 |258.64 |\n|yolox_m_fast|640x640|YoloX|FP16|2.386 |419.16 |\n|yolox_s_fast|640x640|YoloX|FP16|1.259 |794.36 |\n|yolox_x_fast|640x640|YoloX|INT8|3.918 |255.26 |\n|yolox_l_fast|640x640|YoloX|INT8|2.292 |436.38 |\n|yolox_m_fast|640x640|YoloX|INT8|1.589 |629.49 |\n|yolox_s_fast|640x640|YoloX|INT8|0.954 |1048.47 |\n|yolov5x6_fast|1280x1280|YoloV5_P6|FP32|67.075 |14.91 |\n|yolov5l6_fast|1280x1280|YoloV5_P6|FP32|37.491 |26.67 |\n|yolov5m6_fast|1280x1280|YoloV5_P6|FP32|19.422 |51.49 |\n|yolov5s6_fast|1280x1280|YoloV5_P6|FP32|7.900 |126.57 |\n|yolov5x_fast|640x640|YoloV5_P5|FP32|18.554 |53.90 |\n|yolov5l_fast|640x640|YoloV5_P5|FP32|10.060 |99.41 |\n|yolov5m_fast|640x640|YoloV5_P5|FP32|5.500 |181.82 |\n|yolov5s_fast|640x640|YoloV5_P5|FP32|2.342 |427.07 |\n|yolov5x6_fast|1280x1280|YoloV5_P6|FP16|20.538 |48.69 |\n|yolov5l6_fast|1280x1280|YoloV5_P6|FP16|10.404 |96.12 |\n|yolov5m6_fast|1280x1280|YoloV5_P6|FP16|6.577 |152.06 |\n|yolov5s6_fast|1280x1280|YoloV5_P6|FP16|3.087 |323.99 |\n|yolov5x_fast|640x640|YoloV5_P5|FP16|5.919 |168.95 |\n|yolov5l_fast|640x640|YoloV5_P5|FP16|3.348 |298.69 |\n|yolov5m_fast|640x640|YoloV5_P5|FP16|2.015 |496.34 |\n|yolov5s_fast|640x640|YoloV5_P5|FP16|1.087 |919.63 |\n|yolov5x6_fast|1280x1280|YoloV5_P6|INT8|11.236 |89.00 |\n|yolov5l6_fast|1280x1280|YoloV5_P6|INT8|6.235 |160.38 |\n|yolov5m6_fast|1280x1280|YoloV5_P6|INT8|4.311 |231.97 |\n|yolov5s6_fast|1280x1280|YoloV5_P6|INT8|2.139 |467.45 |\n|yolov5x_fast|640x640|YoloV5_P5|INT8|3.456 |289.37 |\n|yolov5l_fast|640x640|YoloV5_P5|INT8|2.019 |495.41 |\n|yolov5m_fast|640x640|YoloV5_P5|INT8|1.425 |701.71 |\n|yolov5s_fast|640x640|YoloV5_P5|INT8|0.844 |1185.47 |\n  \n</details>\n\n## Setup and Configuration\n<details>\n<summary>Linux</summary>\n  \n  \n1. VSCode (highly recommended!)\n2. Configure your path for cudnn, cuda, tensorRT8.0 and protobuf.\n3. Configure the compute capability matched with your nvidia graphics card in Makefile/CMakeLists.txt\n    - e.g.  `-gencode=arch=compute_75,code=sm_75`. If you are using 3080Ti, that should be `gencode=arch=compute_86,code=sm_86`\n    - reference for the table for GPU Compute Capability:\n  https://developer.nvidia.com/cuda-gpus#compute\n4. Configure your library path in .vscode/c_cpp_properties.json\n5. CUDA version: CUDA10.2\n6. CUDNN version: cudnn8.2.2.26. Note that dev(.h file) and runtime(.so file) should be downloaded.\n7. tensorRT version：tensorRT-8.0.1.6-cuda10.2\n8. protobuf version（for onnx parser）：protobufv3.11.4\n    - if other version, refer to the ........\n    - link for download: https://github.com/protocolbuffers/protobuf/tree/v3.11.4\n    - download, compile and replace the path in Makefile/CMakeLists.txt with new path to protobuf3.11.4\n  - CMake:\n    - `mkdir build && cd build`\n    - `cmake ..`\n    - `make yolo -j8`\n  - Makefile:\n    - `make yolo -j8`\n  \n</details>\n\n<details>\n<summary>Linux: Compile for Python</summary>\n\n- compile and install\n    - Makefile：\n        - set `use_python := true` in Makefile\n    - CMakeLists.txt:\n        - `set(HAS_PYTHON ON)` in CMakeLists.txt\n    - Type in `make pyinstall -j8`\n    - Complied files are in `python/pytrt/libpytrtc.so`\n\n</details>\n  \n<details>\n<summary>Windows</summary>\n\n  \n1. Please check the [lean/README.md](lean/README.md) for the detailed dependency\n2. In TensorRT.vcxproj, replace the `<Import Project=\"$(VCTargetsPath)\\BuildCustomizations\\CUDA 10.0.props\" />` with your own CUDA path\n3. In TensorRT.vcxproj, replace the `<Import Project=\"$(VCTargetsPath)\\BuildCustomizations\\CUDA 10.0.targets\" />` with your own CUDA path\n4. In TensorRT.vcxproj, replace the `<CodeGeneration>compute_61,sm_61</CodeGeneration>` with your compute capability.\n    - refer to the table in https://developer.nvidia.com/cuda-gpus#compute\n  \n5. Configure your dependency or download it to the foler /lean. Configure VC++ dir (include dir and refence)\n\n6. Configure your env, debug->environment\n7. Compile and run the example, where 3 options are available.\n\n</details>\n\n<details>\n<summary>Windows: Compile for Python</summary>\n\n  \n1. Compile pytrtc.pyd. Choose python in visual studio to compile\n2. Copy dll and execute 'python/copy_dll_to_pytrt.bat'\n3. Execute the example in python dir by 'python test_yolov5.py'\n  - if installation is needed, switch to target env(e.g. your conda env) then 'python setup.py install', which has to be followed by step 1 and step 2.\n  - the compiled files are in `python/pytrt/libpytrtc.pyd`\n\n</details>\n  \n  \n<details>\n<summary>Other Protobuf Version</summary>\n  \n- in onnx/make_pb.sh, replace the path `protoc=/data/sxai/lean/protobuf3.11.4/bin/protoc` in protoc with the protoc of your own version\n\n```bash\n#cd the path in terminal to /onnx\ncd onnx\n\n#execuete the command to make pb files\nbash make_pb.sh\n```\n  \n- CMake:\n    - replace the `set(PROTOBUF_DIR \"/data/sxai/lean/protobuf3.11.4\")` in CMakeLists.txt with the same path of your protoc.\n\n```bash\nmkdir build && cd build\ncmake ..\nmake yolo -j64\n```\n- Makefile:\n    - replace the path `lean_protobuf  := /data/sxai/lean/protobuf3.11.4` in Makefile with the same path of protoc\n\n```bash\nmake yolo -j64\n```\n\n</details>\n  \n\n<details>\n<summary>TensorRT 7.x support</summary>\n\n- The default is tensorRT8.x\n1. Replace onnx_parser_for_7.x/onnx_parser to src/tensorRT/onnx_parser\n    - `bash onnx_parser/use_tensorrt_7.x.sh`\n2. Configure Makefile/CMakeLists.txt path to TensorRT7.x\n3. Execute `make yolo -j64`\n\n</details>\n\n\n<details>\n<summary>TensorRT 8.x support</summary>\n\n- The default is tensorRT8.x\n1. Replace onnx_parser_for_8.x/onnx_parser to src/tensorRT/onnx_parser\n    - `bash onnx_parser/use_tensorrt_8.x.sh`\n2. Configure Makefile/CMakeLists.txt path to TensorRT8.x\n3. Execute `make yolo -j64`\n\n</details>\n  \n  \n## Guide for Different Tasks/Model Support\n<details>\n<summary>YoloV5 Support</summary>\n  \n- if pytorch >= 1.7, and the model is 5.0+, the model is suppored by the framework \n- if pytorch < 1.7 or yolov5(2.0, 3.0 or 4.0), minor modification should be done in opset.\n- if you want to achieve the inference with lower pytorch, dynamic batchsize and other advanced setting, please check our [blog](http://zifuture.com:8090) (now in Chinese) and scan the QRcode via Wechat to join us.\n\n\n1. Download yolov5\n\n```bash\ngit clone git@github.com:ultralytics/yolov5.git\n```\n\n2. Modify the code for dynamic batchsize\n```python\n# line 55 forward function in yolov5/models/yolo.py \n# bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n# x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n# modified into:\n\nbs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\nbs = -1\nny = int(ny)\nnx = int(nx)\nx[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n# line 70 in yolov5/models/yolo.py\n#  z.append(y.view(bs, -1, self.no))\n# modified into：\nz.append(y.view(bs, self.na * ny * nx, self.no))\n\n############# for yolov5-6.0 #####################\n# line 65 in yolov5/models/yolo.py\n# if self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:\n#    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n# modified into:\nif self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:\n    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n\n# disconnect for pytorch trace\nanchor_grid = (self.anchors[i].clone() * self.stride[i]).view(1, -1, 1, 1, 2)\n\n# line 70 in yolov5/models/yolo.py\n# y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n# modified into:\ny[..., 2:4] = (y[..., 2:4] * 2) ** 2 * anchor_grid  # wh\n\n# line 73 in yolov5/models/yolo.py\n# wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n# modified into:\nwh = (y[..., 2:4] * 2) ** 2 * anchor_grid  # wh\n############# for yolov5-6.0 #####################\n\n\n# line 52 in yolov5/export.py\n# torch.onnx.export(dynamic_axes={'images': {0: 'batch', 2: 'height', 3: 'width'},  # shape(1,3,640,640)\n#                                'output': {0: 'batch', 1: 'anchors'}  # shape(1,25200,85)  修改为\n# modified into:\ntorch.onnx.export(dynamic_axes={'images': {0: 'batch'},  # shape(1,3,640,640)\n                                'output': {0: 'batch'}  # shape(1,25200,85) \n```\n3. Export to onnx model\n```bash\ncd yolov5\npython export.py --weights=yolov5s.pt --dynamic --include=onnx --opset=11\n```\n4. Copy the model and execute it\n```bash\ncp yolov5/yolov5s.onnx tensorRT_cpp/workspace/\ncd tensorRT_cpp\nmake yolo -j32\n```\n\n</details>\n\n\n<details>\n<summary>YoloV7 Support</summary>\n1. Download yolov7 and pth\n\n```bash\n# from cdn\n# or wget https://github.com/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\n\nwget https://cdn.githubjs.cf/WongKinYiu/yolov7/releases/download/v0.1/yolov7.pt\ngit clone git@github.com:WongKinYiu/yolov7.git\n```\n\n2. Modify the code for dynamic batchsize\n```python\n# line 45 forward function in yolov7/models/yolo.py \n# bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n# x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n# modified into:\n\nbs, _, ny, nx = map(int, x[i].shape)  # x(bs,255,20,20) to x(bs,3,20,20,85)\nbs = -1\nx[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n# line 52 in yolov7/models/yolo.py\n# y = x[i].sigmoid()\n# y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\n# y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n# z.append(y.view(bs, -1, self.no))\n# modified into：\ny = x[i].sigmoid()\nxy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\nwh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i].view(1, -1, 1, 1, 2)  # wh\nclassif = y[..., 4:]\ny = torch.cat([xy, wh, classif], dim=-1)\nz.append(y.view(bs, self.na * ny * nx, self.no))\n\n# line 57 in yolov7/models/yolo.py\n# return x if self.training else (torch.cat(z, 1), x)\n# modified into:\nreturn x if self.training else torch.cat(z, 1)\n\n\n# line 52 in yolov7/models/export.py\n# output_names=['classes', 'boxes'] if y is None else ['output'],\n# dynamic_axes={'images': {0: 'batch', 2: 'height', 3: 'width'},  # size(1,3,640,640)\n#               'output': {0: 'batch', 2: 'y', 3: 'x'}} if opt.dynamic else None)\n# modified into:\noutput_names=['classes', 'boxes'] if y is None else ['output'],\ndynamic_axes={'images': {0: 'batch'},  # size(1,3,640,640)\n              'output': {0: 'batch'}} if opt.dynamic else None)\n\n```\n3. Export to onnx model\n```bash\ncd yolov7\npython models/export.py --dynamic --grid --weight=yolov7.pt\n```\n4. Copy the model and execute it\n```bash\ncp yolov7/yolov7.onnx tensorRT_cpp/workspace/\ncd tensorRT_cpp\nmake yolo -j32\n```\n\n</details>\n\n\n<details>\n<summary>YoloX Support</summary>\n  \n- download from: https://github.com/Megvii-BaseDetection/YOLOX\n- If you don't want to export onnx by yourself, just make run in the repo of Megavii\n\n1. Download YoloX\n```bash\ngit clone git@github.com:Megvii-BaseDetection/YOLOX.git\ncd YOLOX\n```\n\n2. Modify the code\nThe modification ensures a successful int8 compilation and inference, otherwise `Missing scale and zero-point for tensor (Unnamed Layer* 686)` will be raised.\n  \n```Python\n# line 206 forward fuction in yolox/models/yolo_head.py. Replace the commented code with the uncommented code\n# self.hw = [x.shape[-2:] for x in outputs] \nself.hw = [list(map(int, x.shape[-2:])) for x in outputs]\n\n\n# line 208 forward function in yolox/models/yolo_head.py. Replace the commented code with the uncommented code\n# [batch, n_anchors_all, 85]\n# outputs = torch.cat(\n#     [x.flatten(start_dim=2) for x in outputs], dim=2\n# ).permute(0, 2, 1)\nproc_view = lambda x: x.view(-1, int(x.size(1)), int(x.size(2) * x.size(3)))\noutputs = torch.cat(\n    [proc_view(x) for x in outputs], dim=2\n).permute(0, 2, 1)\n\n\n# line 253 decode_output function in yolox/models/yolo_head.py Replace the commented code with the uncommented code\n#outputs[..., :2] = (outputs[..., :2] + grids) * strides\n#outputs[..., 2:4] = torch.exp(outputs[..., 2:4]) * strides\n#return outputs\nxy = (outputs[..., :2] + grids) * strides\nwh = torch.exp(outputs[..., 2:4]) * strides\nreturn torch.cat((xy, wh, outputs[..., 4:]), dim=-1)\n\n# line 77 in tools/export_onnx.py\nmodel.head.decode_in_inference = True\n```\n\n \n3. Export to onnx\n```bash\n\n# download model\nwget https://github.com/Megvii-BaseDetection/YOLOX/releases/download/0.1.1rc0/yolox_m.pth\n\n# export\nexport PYTHONPATH=$PYTHONPATH:.\npython tools/export_onnx.py -c yolox_m.pth -f exps/default/yolox_m.py --output-name=yolox_m.onnx --dynamic --no-onnxsim\n```\n\n4. Execute the command\n```bash\ncp YOLOX/yolox_m.onnx tensorRT_cpp/workspace/\ncd tensorRT_cpp\nmake yolo -j32\n```\n\n</details>\n\n\n<details>\n<summary>YoloV3 Support</summary>\n  \n- if pytorch >= 1.7, and the model is 5.0+, the model is suppored by the framework \n- if pytorch < 1.7 or yolov3, minor modification should be done in opset.\n- if you want to achieve the inference with lower pytorch, dynamic batchsize and other advanced setting, please check our [blog](http://zifuture.com:8090) (now in Chinese) and scan the QRcode via Wechat to join us.\n\n\n1. Download yolov3\n\n```bash\ngit clone git@github.com:ultralytics/yolov3.git\n```\n\n2. Modify the code for dynamic batchsize\n```python\n# line 55 forward function in yolov3/models/yolo.py \n# bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n# x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n# modified into:\n\nbs, _, ny, nx = map(int, x[i].shape)  # x(bs,255,20,20) to x(bs,3,20,20,85)\nbs = -1\nx[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n\n\n# line 70 in yolov3/models/yolo.py\n#  z.append(y.view(bs, -1, self.no))\n# modified into：\nz.append(y.view(bs, self.na * ny * nx, self.no))\n\n# line 62 in yolov3/models/yolo.py\n# if self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:\n#    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\n# modified into:\nif self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:\n    self.grid[i], self.anchor_grid[i] = self._make_grid(nx, ny, i)\nanchor_grid = (self.anchors[i].clone() * self.stride[i]).view(1, -1, 1, 1, 2)\n\n# line 70 in yolov3/models/yolo.py\n# y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n# modified into:\ny[..., 2:4] = (y[..., 2:4] * 2) ** 2 * anchor_grid  # wh\n\n# line 73 in yolov3/models/yolo.py\n# wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n# modified into:\nwh = (y[..., 2:4] * 2) ** 2 * anchor_grid  # wh\n\n\n# line 52 in yolov3/export.py\n# torch.onnx.export(dynamic_axes={'images': {0: 'batch', 2: 'height', 3: 'width'},  # shape(1,3,640,640)\n#                                'output': {0: 'batch', 1: 'anchors'}  # shape(1,25200,85) \n# modified into:\ntorch.onnx.export(dynamic_axes={'images': {0: 'batch'},  # shape(1,3,640,640)\n                                'output': {0: 'batch'}  # shape(1,25200,85) \n```\n3. Export to onnx model\n```bash\ncd yolov3\npython export.py --weights=yolov3.pt --dynamic --include=onnx --opset=11\n```\n4. Copy the model and execute it\n```bash\ncp yolov3/yolov3.onnx tensorRT_cpp/workspace/\ncd tensorRT_cpp\n\n# change src/application/app_yolo.cpp: main\n# test(Yolo::Type::V3, TRT::Mode::FP32, \"yolov3\");\n\nmake yolo -j32\n```\n\n</details>\n\n\n<details>\n<summary>UNet Support</summary>\n  \n- reference to : https://github.com/shouxieai/unet-pytorch\n\n```\nmake dunet -j32\n```\n\n</details>\n\n\n<details>\n<summary>Retinaface Support</summary>\n\n- https://github.com/biubug6/Pytorch_Retinaface\n\n1. Download Pytorch_Retinaface Repo\n\n```bash\ngit clone git@github.com:biubug6/Pytorch_Retinaface.git\ncd Pytorch_Retinaface\n```\n\n2. Download model from the Training of README.md in https://github.com/biubug6/Pytorch_Retinaface#training .Then unzip it to the /weights . Here, we use mobilenet0.25_Final.pth\n\n3. Modify the code\n\n```python\n# line 24 in models/retinaface.py\n# return out.view(out.shape[0], -1, 2) is modified into \nreturn out.view(-1, int(out.size(1) * out.size(2) * 2), 2)\n\n# line 35 in models/retinaface.py\n# return out.view(out.shape[0], -1, 4) is modified into\nreturn out.view(-1, int(out.size(1) * out.size(2) * 2), 4)\n\n# line 46 in models/retinaface.py\n# return out.view(out.shape[0], -1, 10) is modified into\nreturn out.view(-1, int(out.size(1) * out.size(2) * 2), 10)\n\n# The following modification ensures the output of resize node is based on scale rather than shape such that dynamic batch can be achieved.\n# line 89 in models/net.py\n# up3 = F.interpolate(output3, size=[output2.size(2), output2.size(3)], mode=\"nearest\") is modified into\nup3 = F.interpolate(output3, scale_factor=2, mode=\"nearest\")\n\n# line 93 in models/net.py\n# up2 = F.interpolate(output2, size=[output1.size(2), output1.size(3)], mode=\"nearest\") is modified into\nup2 = F.interpolate(output2, scale_factor=2, mode=\"nearest\")\n\n# The following code removes softmax (bug sometimes happens). At the same time, concatenate the output to simplify the decoding.\n# line 123 in models/retinaface.py\n# if self.phase == 'train':\n#     output = (bbox_regressions, classifications, ldm_regressions)\n# else:\n#     output = (bbox_regressions, F.softmax(classifications, dim=-1), ldm_regressions)\n# return output\n# the above is modified into:\noutput = (bbox_regressions, classifications, ldm_regressions)\nreturn torch.cat(output, dim=-1)\n\n# set 'opset_version=11' to ensure a successful export\n# torch_out = torch.onnx._export(net, inputs, output_onnx, export_params=True, verbose=False,\n#     input_names=input_names, output_names=output_names)\n# is modified into:\ntorch_out = torch.onnx._export(net, inputs, output_onnx, export_params=True, verbose=False, opset_version=11,\n    input_names=input_names, output_names=output_names)\n\n\n\n\n```\n4. Export to onnx\n```bash\npython convert_to_onnx.py\n```\n\n5. Execute\n```bash\ncp FaceDetector.onnx ../tensorRT_cpp/workspace/mb_retinaface.onnx\ncd ../tensorRT_cpp\nmake retinaface -j64\n```\n\n</details>\n\n\n<details>\n<summary>DBFace Support</summary>\n\n- https://github.com/dlunion/DBFace\n\n```bash\nmake dbface -j64\n```\n\n</details>\n\n<details>\n<summary>Scrfd Support</summary>\n\n- https://github.com/deepinsight/insightface/tree/master/detection/scrfd\n- The know-how about exporting to onnx is comming. Before it is released, come and join us to disucss. \n\n</details>\n\n\n\n<details>\n<summary>Arcface Support</summary>\n\n- https://github.com/deepinsight/insightface/tree/master/recognition/arcface_torch\n```C++\nauto arcface = Arcface::create_infer(\"arcface_iresnet50.fp32.trtmodel\", 0);\nauto feature = arcface->commit(make_tuple(face, landmarks)).get();\ncout << feature << endl;  // 1x512\n```\n- In the example of Face Recognition, `workspace/face/library` is the set of faces registered.\n- `workspace/face/recognize` is the set of face to be recognized.\n- the result is saved in `workspace/face/result`和`workspace/face/library_draw`\n\n</details>\n  \n<details>\n<summary>CenterNet Support</summary>\n  \ncheck the great details in tutorial/2.0\n</details>\n\n\n<details>\n<summary>Bert Support(Chinese Classification)</summary>\n\n- https://github.com/649453932/Bert-Chinese-Text-Classification-Pytorch\n- `make bert -j6`  \n\n</details>\n\n\n## the INTRO to Interface\n\n<details>\n<summary>Python Interface：Get onnx and trtmodel from pytorch model more easily</summary>\n\n- Just one line of code to export onnx and trtmodel. And save them for usage in the future.\n```python\nimport pytrt\n\nmodel = models.resnet18(True).eval()\npytrt.from_torch(\n    model, \n    dummy_input, \n    max_batch_size=16, \n    onnx_save_file=\"test.onnx\", \n    engine_save_file=\"engine.trtmodel\"\n)\n```\n\n</details>\n\n<details>\n<summary>Python Interface：TensorRT Inference</summary>\n\n- YoloX TensorRT Inference\n```python\nimport pytrt\n\nyolo   = tp.Yolo(engine_file, type=tp.YoloType.X)   # engine_file is the trtmodel file\nimage  = cv2.imread(\"inference/car.jpg\")\nbboxes = yolo.commit(image).get()\n```\n\n- Seamless Inference from Pytorch to TensorRT\n```python\nimport pytrt\n\nmodel     = models.resnet18(True).eval().to(device) # pt model\ntrt_model = tp.from_torch(model, input)\ntrt_out   = trt_model(input)\n```\n\n</details>\n\n\n<details>\n<summary>C++ Interface：YoloX Inference</summary>\n\n```C++\n\n// create infer engine on gpu 0\nauto engine = Yolo::create_infer(\"yolox_m.fp32.trtmodel\"， Yolo::Type::X, 0);\n\n// load image\nauto image = cv::imread(\"1.jpg\");\n\n// do inference and get the result\nauto box = engine->commit(image).get();\n```\n\n</details>\n\n\n<details>\n<summary>C++ Interface：Compile Model in FP32/FP16</summary>\n\n```cpp\nTRT::compile(\n  TRT::Mode::FP32,   // compile model in fp32\n  3,                          // max batch size\n  \"plugin.onnx\",              // onnx file\n  \"plugin.fp32.trtmodel\",     // save path\n  {}                         //  redefine the shape of input when needed\n);\n```\n- For fp32 compilation, all you need is offering onnx file whose input shape is allowed to be redefined.\n</details>\n\n\n<details>\n<summary>C++ Interface：Compile in int8</summary>\n\n- The in8 inference performs slightly worse than fp32 in precision(about -5% drop down), but stunningly faster. In the framework, we offer int8 inference\n\n```cpp\n// define int8 calibration function to read data and handle it to tenor.\nauto int8process = [](int current, int count, vector<string>& images, shared_ptr<TRT::Tensor>& tensor){\n    for(int i = 0; i < images.size(); ++i){\n    // int8 compilation requires calibration. We read image data and set_norm_mat. Then the data will be transfered into the tensor.\n        auto image = cv::imread(images[i]);\n        cv::resize(image, image, cv::Size(640, 640));\n        float mean[] = {0, 0, 0};\n        float std[]  = {1, 1, 1};\n        tensor->set_norm_mat(i, image, mean, std);\n    }\n};\n\n\n// Specify TRT::Mode as INT8\nauto model_file = \"yolov5m.int8.trtmodel\";\nTRT::compile(\n  TRT::Mode::INT8,            // INT8\n  3,                          // max batch size\n  \"yolov5m.onnx\",             // onnx\n  model_file,                 // saved filename\n  {},                         // redefine the input shape\n  int8process,                // the recall function for calibration\n  \".\",                        // the dir where the image data is used for calibration\n  \"\"                          // the dir where the data generated from calibration is saved(a.k.a where to load the calibration data.)\n);\n```\n- We integrate into only one int8process function to save otherwise a lot of issues that might happen in tensorRT official implementation. \n\n</details>\n\n\n<details>\n<summary>C++ Interface：Inference</summary>\n\n- We introduce class Tensor for easier inference and data transfer between host to device. So that as a user, the details wouldn't be annoying.\n\n- class Engine is another facilitator.\n\n```cpp\n// load model and get a shared_ptr. get nullptr if fail to load.\nauto engine = TRT::load_infer(\"yolov5m.fp32.trtmodel\");\n\n// print model info\nengine->print();\n\n// load image\nauto image = imread(\"demo.jpg\");\n\n// get the model input and output node, which can be accessed by name or index\nauto input = engine->input(0);   // or auto input = engine->input(\"images\");\nauto output = engine->output(0); // or auto output = engine->output(\"output\");\n\n// put the image into input tensor by calling set_norm_mat()\nfloat mean[] = {0, 0, 0};\nfloat std[]  = {1, 1, 1};\ninput->set_norm_mat(i, image, mean, std);\n\n// do the inference. Here sync(true) or async(false) is optional\nengine->forward(); // engine->forward(true or false)\n\n// get the outut_ptr, which can used to access the output\nfloat* output_ptr = output->cpu<float>();\n```\n\n</details>\n\n\n<details>\n<summary>C++ Interface：Plugin</summary>\n\n- You only need to define kernel function and inference process. The details of code(e.g the serialization, deserialization and injection of plugin etc) are under the hood.\n- Easy to implement a new plugin in FP32 and FP16. Refer to HSwish.cu for details.\n```cpp\ntemplate<>\n__global__ void HSwishKernel(float* input, float* output, int edge) {\n\n    KernelPositionBlock;\n    float x = input[position];\n    float a = x + 3;\n    a = a < 0 ? 0 : (a >= 6 ? 6 : a);\n    output[position] = x * a / 6;\n}\n\nint HSwish::enqueue(const std::vector<GTensor>& inputs, std::vector<GTensor>& outputs, const std::vector<GTensor>& weights, void* workspace, cudaStream_t stream) {\n\n    int count = inputs[0].count();\n    auto grid = CUDATools::grid_dims(count);\n    auto block = CUDATools::block_dims(count);\n    HSwishKernel <<<grid, block, 0, stream >>> (inputs[0].ptr<float>(), outputs[0].ptr<float>(), count);\n    return 0;\n}\n\n\nRegisterPlugin(HSwish);\n```\n\n</details>\n\n\n## About Us\n- Our blog：http://www.zifuture.com/                        (Now only in Chinese. English is comming)\n- Our video channel： https://space.bilibili.com/1413433465 (Now only in Chinese. English is comming)\n\n\n\n\n\n\n\n\n\n\n"
        },
        {
          "name": "example-hard-decode-pybind",
          "type": "tree",
          "content": null
        },
        {
          "name": "example-python",
          "type": "tree",
          "content": null
        },
        {
          "name": "example-restful_server",
          "type": "tree",
          "content": null
        },
        {
          "name": "example-simple_yolo",
          "type": "tree",
          "content": null
        },
        {
          "name": "lean",
          "type": "tree",
          "content": null
        },
        {
          "name": "onnx",
          "type": "tree",
          "content": null
        },
        {
          "name": "onnx_parser",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorial",
          "type": "tree",
          "content": null
        },
        {
          "name": "windows",
          "type": "tree",
          "content": null
        },
        {
          "name": "workspace",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}