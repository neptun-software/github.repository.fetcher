{
  "metadata": {
    "timestamp": 1736565826404,
    "page": 760,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjc2MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "mhx/dwarfs",
      "stars": 2196,
      "defaultBranch": "main",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 0.427734375,
          "content": "ï»¿---\nAlwaysBreakTemplateDeclarations: 'true'\nAccessModifierOffset: -1\nBreakConstructorInitializersBeforeComma: 'true'\nColumnLimit: '80'\nConstructorInitializerAllOnOneLineOrOnePerLine: 'false'\nConstructorInitializerIndentWidth: '4'\nCpp11BracedListStyle: 'true'\nIndentWidth: '2'\nLanguage: Cpp\nMaxEmptyLinesToKeep: '1'\nNamespaceIndentation: None\nPenaltyReturnTypeOnItsOwnLine: '0'\nPointerAlignment: Left\nStandard: Cpp11\nUseTab: Never\n\n...\n"
        },
        {
          "name": ".codecov.yml",
          "type": "blob",
          "size": 0.01953125,
          "content": "ignore:\n  - \"fsst/\"\n"
        },
        {
          "name": ".docker",
          "type": "tree",
          "content": null
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.1396484375,
          "content": ".DS_Store\n/@*\n/*.*fs\n/tmp/\n/build*/\n/man/*.1\n/man/*.1.html\n*.log\n/.gdb_history\n\n*~\n.*.swp\n\n# VS build environment\n.vs/\nout/\nCMakeSettings.json\n"
        },
        {
          "name": ".gitmodules",
          "type": "blob",
          "size": 0.1552734375,
          "content": "[submodule \"folly\"]\n\tpath = folly\n\turl = https://github.com/facebook/folly\n[submodule \"fbthrift\"]\n\tpath = fbthrift\n\turl = https://github.com/facebook/fbthrift\n"
        },
        {
          "name": ".maintainer-scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.232421875,
          "content": "language: cpp\n\nos: linux\n\ndist: jammy\n\naddons:\n  apt:\n    packages:\n      - cmake\n      - make\n      - bison\n      - flex\n      - ronn\n      - tar\n      - diffutils\n      - fuse3\n      - pkg-config\n      - binutils-dev\n      - libarchive-dev\n      - libbenchmark-dev\n      - libboost-context-dev\n      - libboost-filesystem-dev\n      - libboost-program-options-dev\n      - libboost-python-dev\n      - libboost-regex-dev\n      - libboost-system-dev\n      - libboost-thread-dev\n      - libbrotli-dev\n      - libevent-dev\n      - libjemalloc-dev\n      - libdouble-conversion-dev\n      - libiberty-dev\n      - liblz4-dev\n      - liblzma-dev\n      - libssl-dev\n      - libunwind-dev\n      - libdwarf-dev\n      - libelf-dev\n      - libfmt-dev\n      - libfuse3-dev\n      - libgoogle-glog-dev\n\ncache: ccache\n\ngit:\n  depth: false\n\nbranches:\n  only:\n  - main\n  - wip\n\ncompiler:\n  - gcc\n\nbefore_script:\n  - mkdir build\n  - cd build\n  - ccache --set-config=compiler_check=content\n  - ccache --set-config=max_size=1G\n  - ccache --set-config=compression=true\n  - ccache --set-config=compression_level=3\n  - ccache -p\n  - ccache -s\n  - cmake -DWITH_TESTS=1 -DWITH_BENCHMARKS=1 -DWITH_LEGACY_FUSE=1 -DWITH_MAN_PAGES=0 ..\n\nscript:\n  - make -j$(nproc)\n  - make test\n  - ccache -s\n"
        },
        {
          "name": "CHANGES.md",
          "type": "blob",
          "size": 47.619140625,
          "content": "# Change Log\n\n## Version 0.10.2 - 2024-12-02\n\n- (fix) Gracefully handle localized error message on Windows.\n  These error messages can contain characters from a Windows\n  (non-UTF-8) code page, which could cause a fatal error in\n  `fmt::print` in the logging code. Call sites that log such\n  error messages now try to convert these from the code page\n  to UTF-8 or, if that fails, simply replace all characters\n  that are invalid from a UTF-8 point-of-view. Partial fix for\n  github #241.\n\n- (fix) Handle invalid wide chars in file names on Windows. For\n  some reason, Windows allows invalid UTF-16 characters in file\n  names. Try to handle these gracefully when converting to UTF-8.\n  Partial fix for github #241.\n\n- (fix) Workaround for new boost versions which have a `process`\n  component.\n\n- (fix) Workaround for a deprecated boost header.\n\n- (fix) Support for upcoming Boost 1.87.0. `io_service` was\n  deprecated and replaced by `io_context` in 1.66.0. The upcoming\n  Boost 1.87.0 will remove the deprecated API. (Thanks to Michael\n  Cho for the fix.)\n\n- (fix) Disable extended output algorithms (`shake(128|256)`).\n\n- (fix) Install libraries to `CMAKE_INSTALL_LIBDIR`. Fixes github\n  #240.\n\n- (fix) mode/uid/gid checks were expecting 16-bit types.\n\n- (fix) stricter metadata checks and improved error messages.\n\n- (fix) Various fixes for `filesystem_extractor` to prevent memory\n  leaks, correctly handle errors during extraction, and prevent\n  creation of invalid archive outputs due to padding.\n\n- (fix) Various minor fixes: non-virtual dtors, missing includes,\n  `std::move` vs. `std::forward`, unused code removal.\n\n- (test) More test cases for stricter metadata checks. Also enable\n  the strict checks in in unit tests by default.\n\n- (docs) Fix typos in man pages.\n\n## Version 0.10.1 - 2024-08-17\n\n- (fix) Allow building `utils_test` against a non-compatible,\n  system-installed version of gtest. This is a common issue\n  when trying to integrate dwarfs into a package manager, as\n  these generally disallow fetching external dependencies at\n  build time.\n\n- (fix) `dwarfsck` was always reporting a block size of 1 byte\n  rather than the actual block size of the image.\n\n- (fix) `DWARFS_HAVE_LIBBROTLI` was not set correctly in the\n  config file, causing build errors if the library was built\n  without `brotli`.\n\n- (fix) Several small fixes for building with Homebrew.\n\n## Version 0.10.0 - 2024-08-14\n\n- (fix) Fixed a race condition identified by ThreadSanitizer\n  in the root node name processing.\n\n- (fix) The terminal abstraction code did not check any errors\n  when trying to determine the terminal width, leading to a\n  random terminal width value. This caused the manual page\n  tests to occasionally crash.\n\n- (feat) Two sets of universal binaries and binary tarballs are\n  provided for Linux platforms: one without any debug symbols, the\n  other with minimal debug symbols and support for stack traces.\n  For the universal binary, only the version without debug symbols\n  will be UPX-compressed, as the stack trace functionality doesn't\n  work with a compressed binary.\n\n- (feat) Symbolic links to the universal binary may now be\n  suffixed with a version (i.e. any part of the name starting\n  with `-` and followed by a digit will be ignored, e.g. the\n  symlink could be `mkdwarfs-0.10` and it would be treated\n  as `mkdwarfs`).\n\n- (feat) Introduced support for extended attributes on Windows,\n  including a new utility for cross-platform xattr manipulation\n  (`pxattr`, for portable xattr).\n\n- (feat) Enhanced file system API, adding error-code based and\n  exception-safe versions for `getattr`, `access`, and similar\n  functions.\n\n- (feat) Filter rules now consistently use Unix path separators,\n  even for the root path component. Addresses a comment in github\n  discussion #228.\n\n- (refactor) Extensive refactoring to improve code modularity,\n  maintainability and to provide proper libraries. The library\n  code has been moved to different namespaces to make it easier\n  to understand the role of different components (e.g. `reader`,\n  `writer`, `extractor`).\n\n- (refactor) Replaced all `folly` library dependencies in the public\n  DwarFS library interface with alternatives from libraries like\n  e.g. `boost` or `nlohmann::json` which are more broadly available.\n  `folly` and `fbthrift` are still used as implementation details,\n  but no longer leak into the public library interfaces.\n\n- (refactor) A much smaller subset of `folly` is now used in DwarFS\n  and only the necessary components are built, significantly\n  reducing the number of compilation units when building DwarFS.\n\n- (build) It is now possible to do modular builds in addition to\n  the default monolithic build, i.e. you can build and install\n  just the DwarFS libraries and later build/install the tools\n  (`mkdwarfs`, ...) and/or the FUSE driver against these libraries.\n  This is particularly useful for packaging (e.g. in Homebrew,\n  which has removed all FUSE support from the core formulae).\n\n- (build) Shared library builds are now explicitly supported.\n  This fixes issues such as github #184.\n\n- (build) The source tarball now contains all auto-generated code,\n  e.g. manual pages or generated thrift code. This reduces the\n  number of build-time dependencies (e.g. `ronn` or `mistletoe`\n  are no longer required) and significantly reduces the build\n  steps (it is no longer necessary to build the thrift compiler).\n  The build is now roughly twice as fast as in the 0.9.x releases.\n\n- (build) The `parallel-hashmap`, `xxHash` and `zstd` submodules\n  have been removed from the git repo and are no longer added to\n  the source tarball. Both `xxHash` and `zstd` are now widely\n  available. If a suitable version of `parallel-hashmap` is found\n  on the system, this will be used, otherwise it will be fetched\n  during the build. Being a header-only library and only used\n  internally, there's no need for it to be installed.\n\n- (build) A *lot* of GCC warnings have been fixed and upstreamed\n  to `folly` / `fbthrift`.\n\n- (test) Fixed some flaky tests, e.g. unmounting the FUSE driver\n  on macOS, or the manpage test that used to crash occasionally.\n\n## Version 0.9.10 - 2024-05-30\n\n- (fix) When cloning LZMA compressor objects, the LZMA filter options\n  of the cloned instance would still point to an options object in the\n  original instance. This could lead to LZMA errors when initializing\n  a new compressor. Fixes github #224.\n\n- (fix) Fetch range-v3 if no suitable version is found. Fixes github #221.\n\n- (fix) Filter rules did not work correctly when input is root dir\n\n- (fix) `duf` reports odd sizes due to using `bsize` instead of `frsize`\n\n## Version 0.9.9 - 2024-04-30\n\n- (fix) A bug introduced by an optimization to skip hashing of large\n  files if they already differ in the first 4 KiB could, under rare\n  circumstances, lead to an unexpected \"inode has no file\" exception\n  after the scanning phase. This bug did not cause any file system\n  inconsistency issues; `mkdwarfs` either crashes with the exception,\n  or its output will be correct. Fixes github #217.\n\n- (feat) Add sequential access detector and block prefetching to the\n  block cache. This improves sequential read throughput roughly by a\n  factor of two. Can be configured / disabled using `-o seq_detector`.\n\n- (feat) Add tracing support in FUSE driver and `dwarfsextract`, which\n  allows simple performance analysis using chrome://tracing. Traces can\n  be enabled using `-o perfmon_trace` and `--perfmon-trace`.\n\n- (feat) Add performance monitoring and tracing support for the block\n  cache.\n\n- (perf) Significantly improve speed of `dwarfsck --checksum`.\n\n## Version 0.9.8 - 2024-04-14\n\n- (fix) Build custom version of libcrypto to link with the release\n  binaries in order for them to run properly on FIPS-enabled setups.\n  Fixes github #210.\n\n- (fix) When mounting a DwarFS image on macOS and viewing the volume\n  in Finder, only the directories were shown, but no files. The root\n  cause was that a non-existent extended attribute is reported via a\n  different error code in macOS (`ENOATTR`) compared to Linux (`ENODATA`)\n  and the wrong error code was returned for certain Finder-related\n  attributes. Fixes github #211.\n\n- (fix) macOS builds using jemalloc were crashing when calling\n  `mallctl(\"version\", ...)`. The root cause of the crash is still\n  unclear, but as a workaround, the jemalloc version is compiled\n  in from a preprocessor constant rather than using `mallctl`.\n\n## Version 0.9.7 - 2024-04-10\n\n- (fix) Handle root uid correctly in access() implementation.\n  Fixes github #204.\n\n- (feature) Show and track library dependencies. Dependencies will\n  be displayed in the command line help; they will also be tracked\n  in the history metadata of a DwarFS image. See also github #207.\n\n- (doc) Describe nilsimsa ordering algorithm more accurately.\n\n- (perf) Reorder branches to improve ricepp speed with real world\n  data.\n\n- (perf) Some tweaks to improve segmenter speed.\n\n## Version 0.9.6 - 2024-02-24\n\n- (fix) Add workaround for new glog release breaking folly build.\n  Fixes github #201.\n\n- (perf) Improve `ricepp` decoding speed by about 25% on x86 and arm\n  and up to 100% on Windows. Also improve encoding speed on Windows\n  by 25%. No more need for special Clang build.\n\n## Version 0.9.5 - 2024-02-13\n\n- (fix) Windows path handling was wrong and didn't work properly for\n  e.g. network shares. This is hopefully fixed for all tools now.\n\n## Version 0.9.4 - 2024-02-12\n\n- (fix) Prevent installation of ricepp headers/libs. Fixes github #195.\n\n- (fix) Don't fetch googletest in ricepp build if the targets are\n  already available. Fixes github #194.\n\n- (feature) Added `blocksize` option to the FUSE driver, which allows\n  the `st_blksize` value to be configured for the mounted file system.\n  Increasing this value can improve throughput for large files.\n\n- (feature) Added experimental `readahead` option to the FUSE driver.\n  This can potentially increase throughput when performing sequential\n  reads.\n\n## Version 0.9.3 - 2024-02-11\n\n- (fix) v0.8.0 removed the implementation of the `null` decompressor\n  under the assumption that it was no longer used; it was, however,\n  still used when recompressing an image with `null`-compressed blocks.\n  The change to remove the implementation was reverted and a new test\n  case was added. Fixes github #193.\n\n- (perf) Some more `ricepp` compression speed improvements. Also, the\n  universal binaries for `x86_64` now automatically choose a `ricepp`\n  version based on CPU capabilities.\n\n## Version 0.9.2 - 2024-02-09\n\n- (fix) v0.9.0 introduced an optimization where large files of equal\n  size were only fully hashed for deduplication if the first 4K of their\n  contents also produced the same hash. This introduced a bug causing\n  an exception to be thrown when processing large hard-linked files.\n  The root cause was that the data structure intended to be used for\n  exactly this case was just never populated, and the fix was adding\n  a single line to fill the data structure. The test cases didn't cover\n  large hard-linked files, so this slipped through into the release.\n  A new test case has been added as well.\n\n- (fix) On Windows, when using Power Shell, the error message dialog\n  for a missing WinFsp DLL was not shown when running `dwarfs.exe`.\n  The workaround is to use the same delayed loading mechanism that's\n  already used for the universal binary and show the error in the\n  terminal. See also the discussion on github #192.\n\n- (feature) Added a `--list` option to `dwarfsck`. This lists all files\n  in the files system image. When used with `--verbose`, the list also\n  shows permissions, size, uid/git and symbolic link information.\n  Fixes github #192.\n\n- (feature) Added a `--checksum` option to `dwarfsck`. This produces\n  output similar to the `*sum` programs from coreutils and can be used\n  to check the contents of a DwarFS image against local files.\n\n## Version 0.9.1 - 2024-02-06\n\n- (fix) Invalid UTF-8 characters in file paths would crash `mkdwarfs`\n  if these paths were displayed in the progress output. A possible\n  workaround was to disable progress output. This fix replaces any\n  invalid characters before displaying them. Fixes github #191.\n\n- (fix) The `CMakeLists.txt` would bail out as soon as it discovered\n  `--as-needed` in the linker flags. However, `--as-needed` is only\n  a problem when combined with `BUILD_SHARED_LIBS=ON`. The check has\n  been changed to only trigger if both conditions are met.\n\n- (perf) Minor speed improvements in `ricepp` compression.\n\n## Version 0.9.0 - 2024-02-05\n\n- (feature) Experimental macOS support. Fixes github #132.\n\n- (feature) New ricepp compression algorithm for raw images as well\n  as a categorizer for the FITS image format. This is quite limited\n  at the moment, as only two-dimensional, 16-bit integer FITS is\n  supported. However, this covers the majority of astro camera images,\n  which is the primary use case at the moment. This can likely be\n  extended to other raw image formats in the future.\n\n## Version 0.8.0 - 2024-01-22\n\n- (fix) Allow version override for nixpkgs. Fixes github #155.\n\n- (fix) Resize progress bar when terminal size changes. Fixes github #159.\n\n- (fix) Add Extended Attributes section to README. Fixes github #160.\n\n- (fix) Support 32-bit uid/gid/mode. Also support more than 65536\n  uids/gids/modes in a filesystem image. Fixes gh #173.\n\n- (fix) Add workaround for broken `utf8cpp` release. Fixes github #182.\n\n- (fix) Don't call `check_section()` in filesystem ctor, as it renders\n  the section index useless. Also add regression test to ensure this\n  won't be accidentally reintroduced. Fixes github #183.\n\n- (fix) Ensure timely exit in progress dtor. This could occasionally\n  block command line tools for a few seconds before exiting.\n\n- (fix) `--set-owner` and `--set-group` did not work properly with\n  non-zero ids. There were two distinct issues: (1) when building a\n  DwarFS image with `--set-owner` and/or `--set-group`, the single\n  uid/gid was stored in place of the index and the respective lookup\n  vectors were left empty and (2) when reading such a DwarFS image,\n  the uid/gid was always set to zero. The issue with (1) is not only\n  that it's a special case, but it also wastes metadata space by\n  repeatedly storing a potentially wide integer value.\n  This fix addresses both issues. The uid/gid information is now\n  stored more efficiently and, when reading an image using the old\n  representation, the correct uid/gid will be reported.\n  Unit tests were added to ensure both old and new formats are\n  read correctly.\n\n- (fix) `mkdwarfs` is now much better at handling inaccessible or\n  vanishing files. In particular on Windows, where a successful\n  `access()` call doesn't necessarily mean it'll be possible to open\n  a file, this will make it possible to create a DwarFS file system\n  from hierarchies containing inaccessible files. On other platforms,\n  this means `mkdwarfs` can now handle files that are vanishing while\n  the file system is being built.\n\n- (fix) `mkdwarfs` progress updates are now \"atomic\", i.e. one update\n  is always written with a single system call. This didn't make much\n  of a difference on Linux, but the notoriously slow Windows terminal,\n  along with somewhat interesting thread scheduling, would sometimes\n  make the updates look like a typewriter in slow-motion.\n\n- (fix) `utf8_truncate()` didn't handle zero-width characters properly.\n  This could cause issues when truncating certain UTF8 strings.\n\n- (fix) A race condition in `simple` progress mode was fixed.\n\n- (fix) A race condition in `filesystem_writer` was fixed.\n\n- (fix) The `--no-create-timestamp` option in `mkdwarfs` was always\n  enabled and thus useless.\n\n- (fix) Common options (like `--log-level`) were inconsistent between\n  tools.\n\n- (fix) Progress was incorrect when `mkdwarfs` was copying sections\n  with `--recompress`.\n\n- (fix) Treat NTFS junctions like directories.\n\n- (fix) Fix canonical path on Windows when accessing mounted DwarFS image.\n\n- (fix) Fix slow sorting in `file_scanner` due to path comparison.\n\n- (fix) On Windows, don't crash with an assertion if the input path for\n  `mkdwarfs` is not found.\n\n- (remove) Python scripting support has been completely removed.\n\n- (feature) Categorizer framework. Initially supported categorizers are\n  `pcmaudio` (detect audio data & metadata and provide context for FLAC\n  compressor) and `incompressible` (detects \"incompressible\" data).\n  Enabled using the `--categorize` option.\n\n- (feature) Multiple segmenters can now run in parallel and write to\n  the same filesystem image in a fully deterministic way. Currently,\n  a segmenter instance will be used per category/subcategory. This can\n  makes segmenting multi-threaded in cases where there are multiple\n  categories. The number of segmenter worker threads can be configured\n  using `--num-segmenter-workers`.\n\n- (feature) The segmenter now supports different \"granularities\". The\n  granularity is determined by the categorizer. For example, when\n  segmenting the audio data in a 16-bit stereo PCM file, the granularity\n  is 4 (bytes). This ensures that the segmenter will only produce chunks\n  that start/end on a sample boundary.\n\n- (feature) The segmenter now also features simple \"repeating sequence\n  detection\". Under certain conditions, these sequences could cause the\n  segmenter to slow down dramatically. See github #161 for details.\n\n- (feature) FLAC compression. This can only be used along with the\n  `pcmaudio` categorizer. Due to the way data is spread across different\n  blocks, both FLAC compression and decompression can likely make use\n  of multiple CPU cores for large audio files, meaning that loading a\n  `.wav` file from a DwarFS image using FLAC compression will likely\n  be much faster than loading the same data from a single FLAC file.\n\n- (feature) Completely new similarity ordering implementation that\n  supports multi-threaded and fully deterministic nilsimsa ordering.\n  Also, nilsimsa options are now ever so slightly more user friendly.\n\n- (feature) The `--recompress` feature of `mkdwarfs` has been largely\n  rewritten. It now ensures the input filesystem is checked before an\n  attempt is made to recompress it. Decompression is now using multiple\n  threads. Also, recompression can be applied only to a subset of\n  categories and compression options can be selected per category.\n\n- (feature) `mkdwarfs` now stores a history block in the output image\n  by default. The history block contains information about the version\n  of `mkdwarfs`, all command line arguments, and a time stamp. A new\n  history entry will be added whenever the image is altered (i.e. by\n  using `--recompress`). The history can be displayed using `dwarfsck`.\n  History timestamps can be disabled using `--no-history-timestamps`\n  for bit-identical images. History creation can also be completely\n  disabled using `--no-history`.\n\n- (feature) All tools now come with built-in manual pages. This is\n  valuable especially on Windows, which doesn't have `man` at all,\n  or for the universal binaries, which are usually not installed\n  alongside the manual pages. Running each tool with `--man` will\n  show the manual page for the tool, using the configured pager.\n  On Windows, if `less.exe` is in the PATH, it'll also be used as\n  a pager.\n\n- (feature) New `verbose` logging level (between `info` and `debug`).\n\n- (feature) Logging now properly supports multi-line strings.\n\n- (feature) Show compression library versions as part of the `--help`\n  output. For `dwarfsextract`, also show `libarchive` version.\n\n- (feature) `--set-time` now supports time strings in different formats\n  (e.g. `20240101T0530`).\n\n- (feature) `mkdwarfs` can now write the filesystem image to `stdout`,\n  making it possible to directly stream the output image to e.g. `netcat`.\n\n- (feature) Progress display for `mkdwarfs` has been completely\n  overhauled. Different components (e.g. hashing, categorization,\n  segmenting, ...) can now display their own progress in addition\n  to a \"global\" progress.\n\n- (feature) `mkdwarfs` now supports ordering by \"reverse path\" with\n  `--order=revpath`. This is like `path` ordering, but with the path\n  components reversed (i.e. `foo/bar/baz.xyz` will be ordered as if\n  it were `baz.xyz/bar/foo`).\n\n- (feature) It is now possible to configure larger bloom filters in\n  `mkdwarfs`.\n\n- (feature) The `mkdwarfs` segmenter can now be fully disabled using\n  `-W 0`.\n\n- (feature) `mkdwarfs` now adds \"feature sets\" to the filesystem\n  metadata. These can be used to introduce now features without\n  necessarily breaking compatibility with older tools. As long as\n  a filesystem image doesn't actively use the new features, it can\n  still be read by old tools. Addresses github #158.\n\n- (feature) `dwarfsck` has a new `--quiet` option that will only\n  report errors.\n\n- (feature) `dwarfsck` with `--print-header` will exit with a special\n  exit code (2) if the image has no header. In all other cases, the\n  exit code will be 0 (no error) or 1 (error).\n\n- (feature) The `--json` option of `dwarfsck` now outputs filesystem\n  information in JSON format.\n\n- (feature) `dwarfsck` has a new `--no-check` option that skips\n  checking all block hashes. This is useful for quickly accessing\n  filesystem information.\n\n- (feature) The FUSE driver exposes a new `dwarfs.inodeinfo` xattr\n  on Linux that contains a JSON object with information about the\n  inode, e.g. a list of chunks and associated categories.\n\n- (feature) Don't enable `readlink` in the FUSE driver if filesystem\n  has no symlinks. This is mainly useful for Windows where symlink\n  support increases the number of `getattr` calls issued by `WinFsp`.\n\n- (feature) As an experimental feature, CPU affinity for each worker\n  group can be configured via the `DWARFS_WORKER_GROUP_AFFINITY`\n  environment variable. This works for all tools, but is really only\n  useful if you have different types of cores (e.g. performance and\n  efficiency cores) and would like to e.g. always run the segmenter\n  on a performance core.\n\n- (doc) Add mkdwarfs sequence diagram.\n\n- (doc) Document known issues with WinFsp.\n\n- (doc) Update README with extended attributes information.\n\n- (doc) Add script to check if all options are documented in manpage.\n\n- (build) Factor out repetitive thrift library code in CMakeLists.txt.\n\n- (build) Use FetchContent for both `fmt` and `googletest`.\n\n- (build) Use `mold` for linking when available.\n\n- (build) The CI workflow now uploads coverage information to codecov.io\n  with every commit.\n\n- (test) A *ton* of tests were added (from 4 kLOC to more than 10 kLOC)\n  and, unsurprisingly, a number of bugs were found in the process.\n\n- (test) Introduced I/O abstraction layer for all `*_main()` functions.\n  This allows testing of almost all tool functionality without the need\n  to start the tool as a subprocess. It also allows to inject errors more\n  easily, and change properties such as the terminal size.\n\n- (other) The universal binaries are now compressed with a different `upx`\n  compression level, making them slightly bigger, but decompress much\n  faster.\n\n## Version 0.7.5 - 2024-01-16\n\n- (fix) Fix crash in the FUSE driver on Windows when tools like Notepad++\n  try to access a file like a directory (presumably because this works in\n  cases where the file is an archive). This is a Windows-only issue because\n  the Linux FUSE driver uses the inode-based API, whereas the Windows driver\n  uses the string-based API. While parsing a path in the string-based API,\n  there was no check whether a path component was a directory before trying\n  to descend further.\n\n## Version 0.7.4 - 2023-12-28\n\n- (fix) Fix regression that broke section index optimization introduced\n  in v0.7.3. Fixes github #183.\n\n- (fix) Add workaround for broken utf8cpp release. Fixes github #182.\n\n## Version 0.7.3 - 2023-12-05\n\n- (feature) Support forward-compatibility. Fixes github #158.\n\n## Version 0.7.2 - 2023-07-24\n\n- (fix) Fix locale fallback if user-default locale cannot be set.\n  Fixes github #156.\n\n## Version 0.7.1 - 2023-07-20\n\n- (fix) Fix potential division by zero crash in speedometer.\n\n- (other) New tool header.\n\n- (other) Source code cleanups.\n\n- (other) Updated static build procedure (see README).\n\n## Version 0.7.0 - 2023-07-11\n\n- (fix) FUSE/WinFsp driver now handles Unicode characters in the\n  file system image name (the file system itself would already\n  work properly with Unicode file names).\n\n- (fix) Fixed heap-use-after-free when using a file system image\n  built with brotli compression. This was caught last minute by\n  ASAN.\n\n- (fix) Catch errors from locale-setting at startup. These errors\n  will only be reported now, but will no longer cause the program\n  to abort.\n\n- (feature) `mkdwarfs` command-line options have been reorganized\n  into groups to make them easier to find and to make the default\n  help message less intimidating. The full help can now be accessed\n  using `-H` or `--long-help`.\n\n- (feature) Symbolic links to the universal binary now also work\n  as aliases on Windows.\n\n- (test) Test universal binary in both `--tool` and symlink modes.\n\n- (other) CI pipeline tweaks & fixes.\n\n## Version 0.7.0-RC6 - 2023-07-09\n\n- (feature) Support delayed loading of WinFsp DLL for universal\n  binary. This makes the `mkdwarfs`, `dwarfsck` and `dwarfsextract`\n  tools of the universal binary usable without the WinFsp DLL.\n\n- (perf) Optimized the offset cache to improve random read\n  latency as well as sequential read latency. This gave a\n  [100x higher throughput](https://github.com/mhx/dwarfs/issues/142)\n  for a case where DwarFS was used to compress raw file system\n  images. Fixes github #142.\n\n- (fix) Fix building with `make` instead of `ninja`. Also fix\n  builing in `Debug` mode. Fixes github #146.\n\n- (fix) Fix `ninja clean`.\n\n- (fix) Fix symlink creation for `mount.dwarfs`/`mount.dwarfs2`.\n\n- (other) Added [CI pipeline](https://github.com/mhx/dwarfs/actions).\n\n- (other) Don't write versioning files to source tree.\n\n## Version 0.7.0-RC5 - 2023-07-04\n\n- (feature) Windows support. All tools can now be built and run\n  on Windows, including the FUSE driver, which makes use of\n  [WinFsp](https://github.com/winfsp/winfsp).\n\n- (feature) Build a \"universal\" binary that combines `mkdwarfs`,\n  `dwarfsck`, `dwarfsextract` and `dwarfs` in a single binary.\n  This binary can be used either through symbolic links with\n  the proper names of the tool, or by passing `--tool=<name>`\n  as the first argument on the command line.\n\n- (feature) Bypass the block cache for uncompressed blocks. This\n  saves copying block data to memory unnecessarily and allows us\n  to keep all uncompressed blocks accessible directly through the\n  memory mapping. Partially addresses github #139.\n\n- (feature) Show throughput in the scanning and segmenting\n  phases in `mkdwarfs`.\n\n- (feature) Show how much of a file has been consumed in the\n  segmenting phase. Useful primarily for large files.\n\n- (feature) `dwarfs` and `dwarfsextract` now have options to\n  enable performance monitoring. This can give insight into the\n  latency of various file system operations.\n\n- (feature) Added inode offset cache, which improves `read()`\n  latency for very fragmented files.\n\n- (fix) Use `folly::hardware_concurrency()`. Fixes github #130.\n\n- (fix) Handle `ARCHIVE_FAILED` status from libarchive, which\n  could be triggered by trying to write long path names to old\n  archive formats.\n\n- (fix) Properly handle unicode path truncation.\n\n- (doc) Update file system format documentation to cover headers\n  and section indices.\n\n- (test) Lots of new tools tests.\n\n- (test) Remove dependency on `tar` and `diff` binaries.\n\n- (other) Switch to C++20.\n\n## Version 0.7.0-RC4 - 2022-12-24\n\n- (feature) Add `--compress-niceness` option to `mkdwarfs`.\n\n## Version 0.7.0-RC3 - 2022-11-20\n\n- (fix) Fix heap-use-after-free in dwarfsextract.\n\n- (fix) Fix dwarfs benchmark binary.\n\n- (feature) Add `--stdout-progress` option to `dwarfsextract`.\n  Fixes github #117.\n\n- (test) Reduce amount of test data to speed up compiles and avoid\n  timeouts on travis.\n\n## Version 0.7.0-RC2 - 2022-11-17\n\n- (fix) Fix linking against compression libs. Fixes github #112.\n\n- (fix) Default FUSE driver debuglevel to `warn` in background\n  mode. Fixes github #113.\n\n- (feature) Add `--chmod` option. Fixes github #7.\n\n- (feature) Add unreadable files as empty files. Fixes github #40.\n\n- (doc) Document how to produce bit-identical images\n\n- (doc) Update internal operation section of mkdwarfs manpage\n\n- (doc) Add more documentation details for `--file-hash` option\n\n- (test) Test image reproducibility for path and similarity ordering\n\n## Version 0.7.0-RC1 - 2022-11-08\n\n- (fix) Fixed `extract_block.py`, which was incorrectly using `printf`\n  instead of `print`.\n\n- (fix) Support LZ4 compression levels above 9.\n\n- (feature) Added `--filter` option to support simple (rsync-like)\n  filter rules. This was driven by a discussion on github #6.\n\n- (feature) Added `--input-list` option to support reading a list\n  of input files from a file or stdin. At least partially fixes\n  github #6.\n\n- (feature) The compression code has been made more modular. This\n  should make it much easier to add support for more compression\n  algorithms in the future.\n\n- (feature) Added support for Brotli compression. This is generally much\n  slower at compression than ZSTD or LZMA, but faster than LZMA, while\n  offering a compression ratio better than ZSTD. Fixes github #76.\n\n- (feature) Added support for choosing the file hashing algorithm using\n  the `--file-hash` option. This allows you to pick a secure hash\n  instead of the default XXH3. Also fixes github #92.\n\n- (feature) Improved de-duplication algorithm to only hash files with\n  the same size. File hashing is delayed until at least one more file\n  with the same size is discovered. This happens automatically and\n  should improve scanning speed, especially on slow file systems.\n\n- (feature) Added `--max-similarity-size` option to prevent similarity\n  hashing of huge files. This saves scanning time, especially on slow\n  file systems, while it shouldn't affect compression ratio too much.\n\n- (feature) Honour user locale when formatting numbers.\n\n- (feature) Added `--num-scanner-workers` option.\n\n- (feature) Added support for extracting corrupted file systems with\n  `dwarfsextract`. This is enabled using the `--continue-on-error`\n  and, if really needed, `--disable-integrity-check` options. Fixes\n  github #51.\n\n- (test) Added unit tests for progress class.\n\n- (other) Lots of internal cleanups.\n\n## Version 0.6.2 - 2022-10-24\n\n- (fix) Fix github #91: image creation reproducibility.\n  Add `--no-create-timestamp` option, produce deterministic\n  inode numbers and fix `fsst` bug that causes symbol tables\n  to be non-deterministic. Images built while omitting create\n  timestamps will now be bit-identical.\n\n- (fix) Fix github #93: only overwrite existing output file\n  when `--force` option given on command line.\n\n- (fix) Fix github #104: extracting large files was causing\n  `dwarfsextract` to OOM. This was fixed by extracting large\n  files in chunks rather than all at once.\n\n- (fix) Fix github #105: handle `strrchr()` return `NULL`.\n\n- (fix) Fix out-of-bounds access (PR #106).\n\n- (fix) Fix swapped-out cached block detection (PR #107).\n\n- (fix) Fix data race in cached block that was triggered by\n  statistics collection and could cause the process to crash.\n\n- (fix) Fix heap-use-after-free when writing section index.\n\n## Version 0.6.1 - 2022-06-11\n\n- (fix) Fix binary installation\n\n## Version 0.6.0 - 2022-06-11\n\n- (fix) Fix and simplify static builds as much as possible.\n  Document how to set up a static build environment. This\n  also fixes github #75 and github #54. Huge shoutout to\n  Maxim Samsonov for implementing most of this!\n\n- (fix) Fix github #71: driver hangs when unmounting\n\n- (fix) Fix github #67: dwarfs I/O hangs if call to to\n  `fuse_reply_iov` fails\n\n- (fix) Fix github #86: block size bits config issues\n\n- (fix) Various build fixes.\n\n- (feature) Add support for cache tidying, which releases\n  cache memory when the mounted file system is unused.\n\n- (feature) Section index support for speeding up mount times\n  (fixes github #48).\n\n## Version 0.5.6 - 2021-07-03\n\n- (fix) Build fixes for gcc-11\n\n- (fix) Use `REALPATH` in `version.cmake` to fix building in\n  symbolically linked repositories (fixes github #47).\n\n## Version 0.5.5 - 2021-05-03\n\n- (feature) If a filesystem block cannot be compressed to less\n  than the uncompressed size, it will be stored uncompressed.\n  This feature actually fixes the bug described below.\n\n- (fix) When building a filesystem from high entropy input data\n  (e.g. already compressed files), and when using LZMA compression\n  with block sizes >= 25, the LZMA algorithm could be unable to\n  pack a block into the worst-case allocated size. This behaviour\n  was not expected and crashed `mkdwarfs`, and seems to me like a\n  bug in LZMA's `lzma_stream_buffer_bound()` function. The issue\n  has been fixed by not compressing blocks at all if the compressed\n  size matches or exceeds the uncompressed size. This fixes part of\n  github #45.\n\n- (fix) Filesystems created such that after segmenting the total\n  data size was a multiple of the block size (i.e. the last block\n  was completely filled) had the last block written to the image\n  twice. Such a filesystem image is perfectly usable, but the\n  repeated block uses space unnecessarily. This is highly unlikely\n  to happen with real data.\n\n- (fix) Filesystems created with `-P shared_files`, but no shared\n  files in the source tree, were created correctly, but could not\n  be loaded. This has been fixed and the filesystems can now be\n  loaded correctly.\n\n- (test) Add tests for binaries and FUSE driver.\n\n- (other) Minor code cleanups.\n\n## Version 0.5.4 - 2021-04-11\n\n- (fix) FUSE driver hangs when accessing files and the driver is\n  *not* started in foreground or debug mode. This bug is present\n  in both the 0.5.2 and 0.5.3 releases. Fixes github #44.\n\n## Version 0.5.3 - 2021-04-11\n\n- (fix) Add `PREFER_SYSTEM_GTEST` for distributions (like Gentoo)\n  that have a `gtest` package.\n\n- (fix) Make sure the source tarball can be built inside a git repo.\n  The version file generation code would attempt to pull information\n  from any outside git repository without checking if it's actually\n  the DwarFS repo.\n\n## Version 0.5.2 - 2021-04-07\n\n- (fix) Make FUSE driver exit with non-zero exit code if filesystem\n  cannot be mounted. Fixes github #41.\n\n## Version 0.5.1 - 2021-04-06\n\n- (fix) `fsst` library was built with `-march=native`, which caused\n  the static binaries not to work on non-AVX platforms. The `fsst`\n  library is now being built with no extra flags.\n\n## Version 0.5.0 - 2021-04-05\n\n- (fix) Disable multiversioning on non-x86 platforms, which broke\n  the ARM build.\n\n- (fix) Due to a bug in the bloom filter code, only half of each\n  64-bit block in the bloom filter was utilized, which reduced the\n  efficiency of the filter. The bug was spotted thanks to `ubsan`.\n  With the fixed filter being twice as effective, the default size\n  of the bloom filter has now been halved.\n\n- (fix) When exporting metadata using `--export-metadata`, `dwarfsck`\n  was not truncating the output file, which could lead to a corrupt\n  metadata export.\n\n- (perf) Scanning has been significantly optimized and is now up to\n  three times faster on average.\n\n- (perf) Digest computation has been parallelized in both `mkdwarfs`\n  and `dwarfsck` giving better performance on multi-core systems.\n\n- (perf) A set of micro-benchmarks has been added to evaluate the\n  performance of different filesystem operations. This can be\n  build by enabling the `-DWITH_BENCHMARKS=1` cmake option.\n\n- (perf) Zstd contexts are now reused during compression, which\n  seems to give some minor speedup.\n\n- (feature) New metadata format (v2.3). This includes a number of\n  changes:\n\n  - Correct hardlink preservation. With older metadata formats,\n    all duplicate files would appear hardlinked. The new format\n    preserves hardlinked files exactly as present in the input\n    data, and performs additional deduplication at a lower level.\n\n  - The new format offers a lot of customization for additional\n    packing of metadata. You can use these to trade off metadata size,\n    mounting speed, etc. Especially for filesystems with millions of\n    files, the metadata size can be reduced significantly.\n\n  - In particular, filename and symlink data can be stored in a\n    [format](https://github.com/cwida/fsst) that reduces the size\n    by roughly a factor of two, but still allows for random access,\n    so the compressed data can be mapped into memory and decompressed\n    on the fly.\n\n- (feature) DwarFS now directly supports images using a custom\n  header. The header can be completely arbitrary. `mkdwarfs` can\n  write, replace or remove such headers, and all other tools can\n  either skip to a specified offset, or determine this offset\n  automatically. This fixes github #38.\n\n- (feature) `dwarfsck` has been improved to perform extensive\n  metadata checks. Also, checksumming is now done in a thread pool,\n  which significantly speeds up `dwarfsck` for large file systems.\n\n- (feature) `dwarfsck` now shows a detailed breakdown of metadata\n  memory usage, which can be used to optimize metadata packing\n  options.\n\n- (feature) Added `ENABLE_COVERAGE` cmake option.\n\n- (test) Compatibility testing with older filesystem versions has\n  been improved.\n\n- (test) A new test suite has been added to check detection of\n  corrupted DwarFS images.\n\n- (doc) Added some high level internals documentation for `mkdwarfs`.\n\n- (doc) Documented the filesystem and metadata formats.\n\n- (other) Lots of internal cleanups.\n\n## Version 0.4.1 - 2021-03-13\n\n- (fix) Linking against libarchive was fixed so that it also\n  works for shared library builds. (fixes github #36)\n\n- (fix) `mkdwarfs` didn't catch certain exceptions correctly,\n  which would cause a stack trace instead of a simple error\n  message. This has been fixed.\n\n- (fix) The statically linked executables were unable to handle\n  any exceptions at all due to duplicate stack unwinding code.\n  This has (hopefully) been fixed now.\n\n- (perf) GCC builds have traditionally been much slower than\n  Clang builds, though it was unclear why that was the case.\n  It turns out the reason is simply that CMake defaults to\n  `-O3` optimization, which is known to cause performance\n  regressions in some cases. The build has been changed to\n  *always* build with `-O2` when doing an optimized GCC build.\n  The Clang build is unaffected. (fixes github #14)\n\n- (perf) The segmenting code now uses a bloom filter to discard\n  unsuccessful matches as early and quickly as possible. While\n  this only gives a minor speedup when using a single lookback\n  block, as you increase the number of lookback blocks speed is\n  barely affected whereas before it would slow down significantly.\n  The bloom filter size (relative to the number of values) can be\n  tuned by using `--bloom-filter-size`, though increasing it any\n  further from the default is likely not going to make a difference.\n\n- (perf) Nilsimsa similarity computation has been improved to\n  make use of different instruction sets depending on the CPU\n  architecture, speeding up the process of ordering files by\n  similarity by almost a factor of 2.\n\n- (doc) Added comparison with `lrzip`, `zpaq`. Updated `wimlib`\n  comparison.\n\n## Version 0.4.0 - 2021-03-06\n\n- (feature) New `dwarfsextract` tool that allows extracting a file\n  system image. It also allows conversion of the file system image\n  directly into a standard archive format (e.g. `tar` or `cpio`).\n  Extracting a DwarFS image can be significantly faster than\n  extracting a equivalent compressed archive.\n\n- (feature) The segmenting algorithm has been completely rewritten\n  and is now much cleaner, uses much less memory, is significantly\n  faster and detects a lot more duplicate segments. At the same time\n  it's easier to configure (just a single window size instead of a\n  list).\n\n- (feature) There's a new option `--max-lookback-blocks` that\n  allows duplicate segments to be detected across multiple blocks,\n  which can result in significantly better compression when using\n  small file system blocks.\n\n- (compat) The `--blockhash-window-sizes` and\n  `--blockhash-increment-shift` options were replaced by\n  `--window-size` and `--window-step`, respectively. The new\n  `--window-size` option takes only a single window size instead\n  of a list.\n\n- (fix) The rewrite of the segmenting algorithm was triggered by\n  a \"bug\" (github #35) that caused excessive memory consumption\n  in `mkdwarfs`. It wasn't really a bug, though, more like a bad\n  algorithm that used memory proportional to the file size. This\n  issue has now been fully solved.\n\n- (fix) Scanning of large files would excessively grow `mkdwarfs`\n  RSS. The memory would have sooner or later be reclaimed by the\n  kernel, but the code now actively releases the memory while\n  scanning.\n\n- (perf) `mkdwarfs` speed has been significantly improved. The\n  47 GiB worth of Perl installations can now be turned into a\n  DwarFS image in less then 6 minutes, about 30% faster than\n  with the 0.3.1 release. Using `lzma` compression, it actually\n  takes less than 4 minutes now, almost twice as fast as 0.3.1.\n\n- (perf) At the same time, compression ratio also significantly\n  improved, mostly due to the new segmenting algorithm. With the\n  0.3.1 release, using the default configuration, the 47 GiB of\n  Perl installations compressed down to 471.6 MiB. With the 0.4.0\n  release, this has dropped to 426.5 MiB, a 10% improvement.\n  Using `lzma` compression (`-l9`), the size of the resulting\n  image went from 319.5 MiB to 300.9 MiB, about 5% better. More\n  importantly, though, the uncompressed file system size dropped\n  from about 7 GiB to 4 GiB thanks to improved segmenting, which\n  means *less* blocks need to be decompressed on average when\n  using the file system.\n\n- (build) The project can now be built to use the system installed\n  `zstd` and `xxHash` libraries. (fixes github #34)\n\n- (build) The project can now be built without the legacy FUSE\n  driver. (fixes github #32)\n\n- (other) Several small code cleanups.\n\n## Version 0.3.1 - 2021-01-07\n\n- (fix) Fix linking of Python libraries\n\n- (fix) Fix missing brace in version generator code\n\n- (fix) Ensure the code builds fine without libdwarf\n\n- (fix) Silence a warning and remove an unused definition\n\n## Version 0.3.0 - 2020-12-30\n\n- (fix) File system images created with versions 0.2.2 and before\n  did store symlinks incorrectly. While this was fixed in 0.2.3,\n  old images could still not be read correctly. This has now been\n  fixed and symlinks on all 0.2.x images will work correctly when\n  using the 0.3.0+ FUSE driver.\n\n- (fix) There was no error if the output file could not be written,\n  `mkdwarfs` would just fail silently. This has now been fixed.\n\n- (fix) When corrupted compressed blocks in either format (LZ4,\n  ZSTD, LZMA) were detected, the FUSE driver would actually show\n  the file contents as all zero bytes instead of signaling an I/O\n  error. This has been fixes and verified for all formats.\n\n- (fix) Better (hopefully) auto-detection of terminal settings to\n  avoid using features like unicode or color when terminals don't\n  support them. Fixes github #20.\n\n- (fix) A number of checks has been added to make sure that corrupt\n  file system images will not crash the binaries. In order for this\n  to be most efficient, old images should be rewritten in the new\n  format using:\n\n  ```\n  mkdwarfs -i old.dwarfs -o new.dwarfs --recompress none\n  ```\n\n- (compat) The metadata format has changed and new file system\n  images can no longer be read by old FUSE drivers.\n\n- (perf) Lots of tweaks and optimizations have resulted in an even\n  better compression ratio while at the same time taking less time\n  to build file system images. On the 48 GiB Perl dataset, for\n  example, the compression improved from 555.7 MiB in 15m12s with\n  0.2.3 to 471.6 MiB in 13m59s with 0.3.0.\n\n- (perf) Replace the cyclic hash function with the one used by rsync.\n  The rsync hash produces similar results, but it's faster.\n\n- (perf) `mkdwarfs` will now make use of hard link and inode data\n  to avoid scanning the same inode multiple times.\n\n- (perf) Segmenting performance has been improved by re-using data\n  structures and thus avoiding extra memory allocations.\n\n- (perf) All binaries now use `jemalloc` by default, which uses\n  significantly less memory than glibc or tcmalloc, especially\n  in the FUSE driver.\n\n- (feature) New file system image format adds integrity checking\n  as well as features for easier recovery in case of corruption.\n  While currently there is no way to recover a corrupt file system,\n  it is important to have the data in place sooner rather than later.\n\n- (feature) New Python scripting support completely replaces Lua\n  scripting. The new interface offers a lot more options and should\n  be much easier to use.\n\n- (feature) New `nilsimsa` similarity algorithm. This has become\n  the default, as it's significantly better on my test data than\n  the \"simple\" `similarity` algorithm.\n\n- (feature) New option `--keep-all-times` to keep atime and ctime\n  in addition to just keeping mtime.\n\n- (feature) New option `--time-resolution` that allows to configure\n  the resolution with which time stamp are stored.\n\n- (feature) Device, FIFO and socket inodes can now be stored in\n  DwarFS file system images. This has to be enabled with the new\n  `--with-devices` and `--with-specials` options.\n\n- (feature) The FUSE driver can now optionally expose correct\n  hard link counts.\n\n- (feature) `mkdwarfs` now has an option `--remove-empty-dirs` to\n  remove empty directories.\n\n- (feature) The FUSE driver has 4 new options to control caching.\n  `no_cache_image` will explictly try to release compressed\n  blocks from the file system image back to the kernel after\n  reading. `cache_image` will keep them in the cache.\n  `no_cache_files` will cause decompressed files not to be cached\n  by the kernel. `cache_files` will cause them to be cached. The\n  defaults are `no_cache_image` and `cache_files`.\n\n- (feature) The FUSE driver now has a `readonly` option that will\n  prevent any entries in the mounted file system to show up as\n  writeable. This is *not* the default, because it interferes with\n  setting up overlays.\n\n- (feature) `dwarfsck` can now dump metadata as JSON blob.\n\n- (feature) `dwarfsck` can now also export raw metadata as JSON.\n  The difference to the `--json` option is that this JSON export\n  could be used to fully reconstruct the metadata for a DwarFS\n  image.\n\n- (feature) More detailed logging and better error handling.\n\n- (test) Added backwards compatibility tests.\n\n- (build) Added `zstd` as a submodule.\n\n- (build) There is now a binary package with statically linked\n  binaries available.\n\n- (doc) More accurate list of dependencies.\n\n- (doc) Document how to add `/etc/fstab` entry for DwarFS image.\n\n- (doc) Comparison with wimlib.\n\n- (doc) Comparison with Cromfs.\n\n- (doc) Comparison with EROFS.\n\n- (doc) Updated benchmarks.\n\n## Version 0.2.4 - 2020-12-13\n\n- Fix `--set-owner` and `--set-group` options, which caused an\n  exception to be thrown at the end of creating a file system.\n  (fixes github #24)\n\n## Version 0.2.3 - 2020-12-01\n\n- Fix link handling. There were two bugs introduced with the\n  new metadata format, one in file system creation and another\n  in the fuse driver. You will have to re-create a file system\n  created with dwarfs < 0.2.3 if it contained links. If you\n  can absolutely not re-create the file system and the data\n  is precious, let me know, there's actually a way to recover\n  the missing data. EDIT: There will be a fix available in the\n  0.3.0 release, so you don't have to rebuild old file systems.\n\n## Version 0.2.2 - 2020-11-30\n\n- Remove read-only masking as it prevents writable overlays\n\n- Throw an error in `mkdwarfs` if unrecognized command line\n  arguments are encountered (github #5)\n\n- Various build fixes (github #2. #3)\n\n- More documentation\n\n## Version 0.2.1 - 2020-11-29\n\n- Replace --no-owner and --no-time with more flexible --set-owner,\n  --set-group and --set-time options\n\n- Update man pages\n\n## Version 0.2.0 - 2020-11-29\n\n- Complete rewrite of the file system metadata storage using\n  fbthrift's `frozen` library\n\n## Version 0.1.1 - 2020-11-23\n\n- Test and fix Debian Buster and Ubuntu Focal builds\n\n- Migrate from `folly::StringPiece` to `std::string_view`\n\n- Documentation updates, list Debian/Ubuntu dependencies\n\n## Version 0.1.0 - 2020-11-22\n\n- Initial release\n"
        },
        {
          "name": "CITATION.cff",
          "type": "blob",
          "size": 0.5068359375,
          "content": "cff-version: 1.2.0\ntitle: >-\n  DwarFS: A fast high compression read-only file system for\n  Linux, Windows and macOS\nmessage: >-\n  If you use this software, please cite it using the\n  metadata from this file.\ntype: software\nauthors:\n  - orcid: 'https://orcid.org/0009-0004-7086-9358'\n    given-names: Marcus\n    family-names: Holland-Moritz\n    email: github@mhxnet.de\nrepository-code: 'https://github.com/mhx/dwarfs'\nkeywords:\n  - filesystem\n  - compression\n  - deduplication\n  - C++\n  - FUSE\nlicense: GPL-3.0-or-later\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 35.123046875,
          "content": "#\n# Copyright (c) Marcus Holland-Moritz\n#\n# This file is part of dwarfs.\n#\n# dwarfs is free software: you can redistribute it and/or modify it under the\n# terms of the GNU General Public License as published by the Free Software\n# Foundation, either version 3 of the License, or (at your option) any later\n# version.\n#\n# dwarfs is distributed in the hope that it will be useful, but WITHOUT ANY\n# WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR\n# A PARTICULAR PURPOSE.  See the GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License along with\n# dwarfs.  If not, see <https://www.gnu.org/licenses/>.\n#\n\ncmake_minimum_required(VERSION 3.28.0)\n\n# Enable CMAKE_MSVC_RUNTIME_LIBRARY\ncmake_policy(SET CMP0091 NEW)\n\nproject(dwarfs)\n\ninclude(ExternalProject)\ninclude(CheckCXXSourceCompiles)\n\noption(WITH_LIBDWARFS \"build with libdwarfs\" ON)\noption(WITH_TOOLS \"build with tools\" ON)\noption(WITH_FUSE_DRIVER \"build with FUSE driver\" ON)\noption(WITH_TESTS \"build with tests\" OFF)\noption(WITH_BENCHMARKS \"build with benchmarks\" OFF)\noption(WITH_FUZZ \"build with fuzzing binaries\" OFF)\noption(WITH_MAN_OPTION \"build with --man option\" ON)\noption(ENABLE_PERFMON \"enable performance monitor in all tools\" ON)\noption(TRY_ENABLE_FLAC \"build with FLAC support\" ON)\noption(ENABLE_RICEPP \"build with RICEPP compression support\" ON)\noption(WITH_UNIVERSAL_BINARY \"build with universal binary\" OFF)\noption(WITH_PXATTR \"build with pxattr binary\" OFF)\noption(WITH_EXAMPLE \"build with example binary\" OFF)\nif(NOT (APPLE OR WIN32))\n  option(ENABLE_STACKTRACE \"build with symbolizer support\" ON)\nendif()\nif(APPLE)\n  option(USE_HOMEBREW_LIBARCHIVE \"use libarchive from homebrew\" ON)\nendif()\nif(NOT WIN32)\n  option(WITH_LEGACY_FUSE \"build fuse2 driver even if we have fuse3\" OFF)\n  option(WITH_MAN_PAGES \"build man pages using ronn\" ON)\n  option(ENABLE_ASAN \"enable address sanitizer\" OFF)\n  option(ENABLE_TSAN \"enable thread sanitizer\" OFF)\n  option(ENABLE_UBSAN \"enable undefined behaviour sanitizer\" OFF)\n  option(ENABLE_COVERAGE \"enable code coverage\" OFF)\n  if(APPLE)\n    # This doesn't work reliably on macOS at the moment\n    set(USE_JEMALLOC OFF)\n  else()\n    option(USE_JEMALLOC \"build with jemalloc\" ON)\n  endif()\n  option(PREFER_SYSTEM_GTEST \"use system gtest if available\" OFF)\n  option(DISABLE_CCACHE \"disable ccache\" OFF)\n  option(DISABLE_MOLD \"disable mold\" OFF)\n  option(STATIC_BUILD_DO_NOT_USE \"try static build (experimental)\" OFF)\nendif()\n\nif(STATIC_BUILD_DO_NOT_USE AND NOT(WITH_LIBDWARFS AND WITH_TOOLS AND WITH_FUSE_DRIVER))\n  message(FATAL_ERROR \"STATIC_BUILD_DO_NOT_USE requires WITH_LIBDWARFS, WITH_TOOLS and WITH_FUSE_DRIVER\")\nendif()\n\n# Libraries that we can fetch on demand if necessary\n#\n# All of these libraries are header-only and not strictly required once\n# libdwarfs is built. Only the range-v3 library is used to implement\n# string splitting, but there's plenty of alternatives for that.\n#\n# We only use these libraries for building the binary targets, we won't\n# install them.\n\nset(LIBFMT_REQUIRED_VERSION 10.0)\nset(LIBFMT_PREFERRED_VERSION 11.0.2)\n\nset(GOOGLETEST_REQUIRED_VERSION 1.13.0)\nset(GOOGLETEST_PREFERRED_VERSION 1.15.2)\n\nset(RANGE_V3_REQUIRED_VERSION 0.12.0)\nset(RANGE_V3_PREFERRED_VERSION 0.12.0)\n\nset(PARALLEL_HASHMAP_REQUIRED_VERSION 1.3.8)\nset(PARALLEL_HASHMAP_PREFERRED_VERSION 1.3.12)\n\nset(BOOST_REQUIRED_VERSION 1.67.0)\nset(LIBCRYPTO_REQUIRED_VERSION 3.0.0)\nset(LIBLZ4_REQUIRED_VERSION 1.9.3)\nset(LIBLZMA_REQUIRED_VERSION 5.2.5)\nset(LIBBROTLI_REQUIRED_VERSION 1.0.9)\nset(LIBARCHIVE_REQUIRED_VERSION 3.6.0)\nset(LIBMAGIC_REQUIRED_VERSION 5.38)\nset(ZSTD_REQUIRED_VERSION 1.4.8)\nset(XXHASH_REQUIRED_VERSION 0.8.1)\nset(FLAC_REQUIRED_VERSION 1.4.2)\nset(JEMALLOC_REQUIRED_VERSION 5.2.1)\n\nif(DEFINED ENV{DWARFS_LOCAL_REPO_PATH})\n  set(LIBFMT_GIT_REPO $ENV{DWARFS_LOCAL_REPO_PATH}/fmt)\n  set(GOOGLETEST_GIT_REPO $ENV{DWARFS_LOCAL_REPO_PATH}/googletest)\n  set(RANGE_V3_GIT_REPO $ENV{DWARFS_LOCAL_REPO_PATH}/range-v3)\n  set(PARALLEL_HASHMAP_GIT_REPO $ENV{DWARFS_LOCAL_REPO_PATH}/parallel-hashmap)\nelse()\n  set(LIBFMT_GIT_REPO https://github.com/fmtlib/fmt.git)\n  set(GOOGLETEST_GIT_REPO https://github.com/google/googletest.git)\n  set(RANGE_V3_GIT_REPO https://github.com/ericniebler/range-v3.git)\n  set(PARALLEL_HASHMAP_GIT_REPO https://github.com/greg7mdp/parallel-hashmap.git)\nendif()\n\nif(APPLE)\n  set(HOMEBREW_PREFIX_PATH \"/opt/homebrew\")\n  set(TRY_RUN_INCLUDE_DIRECTORIES ${HOMEBREW_PREFIX_PATH}/include)\nendif()\n\ninclude(${CMAKE_SOURCE_DIR}/cmake/compile.cmake)\ninclude(${CMAKE_SOURCE_DIR}/cmake/version.cmake)\n\nif(NOT PRJ_VERSION_FULL)\n  message(FATAL_ERROR \"PRJ_VERSION_FULL is not set\")\nendif()\n\nset(CMAKE_MODULE_PATH ${CMAKE_MODULE_PATH} \"${CMAKE_SOURCE_DIR}/cmake/modules\")\n\nfind_package(PkgConfig)\n\nif(STATIC_BUILD_DO_NOT_USE)\n  # Strangely this is needed when linking statically against FLAC++\n  #add_compile_options(-fno-omit-frame-pointer)\n\n  if(STATIC_BUILD_EXTRA_PREFIX)\n    include_directories(BEFORE ${STATIC_BUILD_EXTRA_PREFIX}/include)\n    list(PREPEND CMAKE_PREFIX_PATH ${STATIC_BUILD_EXTRA_PREFIX})\n    set(PKG_CONFIG_USE_CMAKE_PREFIX_PATH ON)\n  endif()\n\n  set(CMAKE_FIND_LIBRARY_SUFFIXES\n      \".a\"\n      CACHE STRING \"please look for static libs\")\n  set(Boost_USE_STATIC_LIBS\n      ON\n      CACHE BOOL \"only static boost libs\")\n  set(BOOST_LINK_STATIC\n      \"ON\"\n      CACHE STRING \"yes, really\")\n  set(USE_STATIC_DEPS_ON_UNIX\n      ON\n      CACHE BOOL \"yes\")\n  set(GFLAGS_SHARED\n      OFF\n      CACHE BOOL \"static\")\n\n  set(CMAKE_FIND_LIBRARY_SUFFIXES .a)\n  list(APPEND PKG_CONFIG_EXECUTABLE \"--static\")\nendif()\n\nif(WITH_MAN_PAGES)\n  include(\"${CMAKE_SOURCE_DIR}/cmake/manpage.cmake\")\n  if(WITH_LIBDWARFS)\n    list(APPEND DWARFS_MANPAGES dwarfs-format.5)\n  endif()\n  if(WITH_TOOLS)\n    list(APPEND DWARFS_MANPAGES mkdwarfs.1 dwarfsck.1 dwarfsextract.1)\n  endif()\n  if(WITH_FUSE_DRIVER)\n    list(APPEND DWARFS_MANPAGES dwarfs.1)\n  endif()\n  foreach(man ${DWARFS_MANPAGES})\n    add_manpage(${man})\n  endforeach()\nendif()\n\ninclude(FetchContent)\n\nif(WITH_TESTS)\n  include(${CMAKE_SOURCE_DIR}/cmake/need_gtest.cmake)\nendif()\n\ninclude(${CMAKE_SOURCE_DIR}/cmake/need_fmt.cmake)\ninclude(${CMAKE_SOURCE_DIR}/cmake/need_range_v3.cmake)\n\nif(WITH_LIBDWARFS)\n  include(${CMAKE_SOURCE_DIR}/cmake/need_phmap.cmake)\n\n  find_package(Boost ${BOOST_REQUIRED_VERSION} REQUIRED CONFIG\n               COMPONENTS chrono iostreams program_options\n               OPTIONAL_COMPONENTS process)\n\n  if(APPLE)\n    if(USE_HOMEBREW_LIBARCHIVE)\n      find_program(HOMEBREW_EXE brew)\n      execute_process(\n        COMMAND ${HOMEBREW_EXE} --prefix libarchive\n        OUTPUT_VARIABLE LIBARCHIVE_PREFIX\n        OUTPUT_STRIP_TRAILING_WHITESPACE)\n      list(PREPEND CMAKE_PREFIX_PATH ${LIBARCHIVE_PREFIX})\n    endif()\n  endif()\n\n  pkg_check_modules(LIBCRYPTO REQUIRED IMPORTED_TARGET libcrypto>=${LIBCRYPTO_REQUIRED_VERSION})\n  pkg_check_modules(LIBARCHIVE REQUIRED IMPORTED_TARGET libarchive>=${LIBARCHIVE_REQUIRED_VERSION})\n  pkg_check_modules(XXHASH REQUIRED IMPORTED_TARGET libxxhash>=${XXHASH_REQUIRED_VERSION})\n  pkg_check_modules(ZSTD REQUIRED IMPORTED_TARGET libzstd>=${ZSTD_REQUIRED_VERSION})\n  pkg_check_modules(LIBLZ4 IMPORTED_TARGET liblz4>=${LIBLZ4_REQUIRED_VERSION})\n  pkg_check_modules(LIBLZMA IMPORTED_TARGET liblzma>=${LIBLZMA_REQUIRED_VERSION})\n  pkg_check_modules(LIBBROTLIDEC IMPORTED_TARGET libbrotlidec>=${LIBBROTLI_REQUIRED_VERSION})\n  pkg_check_modules(LIBBROTLIENC IMPORTED_TARGET libbrotlienc>=${LIBBROTLI_REQUIRED_VERSION})\n  # pkg_check_modules(LIBMAGIC IMPORTED_TARGET libmagic>=${LIBMAGIC_REQUIRED_VERSION})\n  if(TRY_ENABLE_FLAC)\n    pkg_check_modules(FLAC IMPORTED_TARGET flac++>=${FLAC_REQUIRED_VERSION})\n  endif()\n\n  if(USE_JEMALLOC)\n    pkg_check_modules(JEMALLOC IMPORTED_TARGET jemalloc>=${JEMALLOC_REQUIRED_VERSION})\n  endif()\n\n  include(${CMAKE_SOURCE_DIR}/cmake/thrift_library.cmake)\n\n  include(${CMAKE_SOURCE_DIR}/cmake/folly.cmake)\n  include(${CMAKE_SOURCE_DIR}/cmake/thrift.cmake)\n\n  if(ENABLE_RICEPP)\n    # TODO: support FetchContent\n    add_subdirectory(ricepp)\n  endif()\n\n  include(${CMAKE_SOURCE_DIR}/cmake/libdwarfs.cmake)\n\n  set(DWARFS_HAVE_LIBZSTD ON)\n  set(DWARFS_USE_JEMALLOC ${USE_JEMALLOC})\n  set(DWARFS_HAVE_RICEPP ${ENABLE_RICEPP})\n  set(DWARFS_HAVE_LIBMAGIC ${LIBMAGIC_FOUND})\n  set(DWARFS_HAVE_LIBLZ4 ${LIBLZ4_FOUND})\n  set(DWARFS_HAVE_LIBLZMA ${LIBLZMA_FOUND})\n  if(${LIBBROTLIDEC_FOUND} AND ${LIBBROTLIENC_FOUND})\n    set(DWARFS_HAVE_LIBBROTLI ON)\n  else()\n    set(DWARFS_HAVE_LIBBROTLI OFF)\n  endif()\n  set(DWARFS_HAVE_FLAC ${FLAC_FOUND})\n  set(DWARFS_BUILTIN_MANPAGE ${WITH_MAN_OPTION})\n  set(DWARFS_PERFMON_ENABLED ${ENABLE_PERFMON})\n  set(DWARFS_STACKTRACE_ENABLED ${ENABLE_STACKTRACE})\n\n  configure_file(cmake/config.h.in include/dwarfs/config.h @ONLY)\nelse()\n  find_package(dwarfs ${PRJ_VERSION_MAJOR}.${PRJ_VERSION_MINOR}.${PRJ_VERSION_PATCH} EXACT REQUIRED CONFIG)\n  add_library(dwarfs_common ALIAS dwarfs::dwarfs_common)\n  add_library(dwarfs_reader ALIAS dwarfs::dwarfs_reader)\n  add_library(dwarfs_writer ALIAS dwarfs::dwarfs_writer)\n  add_library(dwarfs_rewrite ALIAS dwarfs::dwarfs_rewrite)\n  add_library(dwarfs_extractor ALIAS dwarfs::dwarfs_extractor)\nendif()\n\ninclude(${CMAKE_SOURCE_DIR}/cmake/libdwarfs_tool.cmake)\n\nif(WITH_TOOLS)\n  foreach(tgt mkdwarfs dwarfsck dwarfsextract)\n    add_library(${tgt}_main OBJECT tools/src/${tgt}_main.cpp)\n    add_executable(${tgt} tools/src/${tgt}.cpp)\n\n    target_link_libraries(${tgt} PRIVATE ${tgt}_main)\n\n    list(APPEND MAIN_TARGETS ${tgt}_main)\n    list(APPEND BINARY_TARGETS ${tgt})\n\n    install(TARGETS ${tgt} RUNTIME DESTINATION bin)\n  endforeach()\n\n  target_link_libraries(mkdwarfs_main PRIVATE dwarfs_reader dwarfs_writer dwarfs_rewrite)\n  target_link_libraries(dwarfsck_main PRIVATE dwarfs_reader)\n  target_link_libraries(dwarfsextract_main PRIVATE dwarfs_extractor)\n\n  if(WITH_UNIVERSAL_BINARY)\n    add_executable(dwarfsuniversal tools/src/universal.cpp)\n    list(APPEND BINARY_TARGETS dwarfsuniversal)\n\n    target_link_libraries(dwarfsuniversal PRIVATE\n            mkdwarfs_main dwarfsck_main dwarfsextract_main)\n    set_target_properties(dwarfsuniversal PROPERTIES\n            RUNTIME_OUTPUT_DIRECTORY universal\n            OUTPUT_NAME dwarfs-universal)\n  endif()\nendif()\n\nif(WITH_PXATTR)\n  add_executable(pxattr tools/src/pxattr.cpp)\n  list(APPEND BINARY_TARGETS pxattr)\n  install(TARGETS pxattr RUNTIME DESTINATION bin)\nendif()\n\nif(WITH_EXAMPLE)\n  add_executable(example example/example.cpp)\n  target_link_libraries(example PRIVATE dwarfs_reader dwarfs_extractor)\n  list(APPEND BINARY_TARGETS example)\nendif()\n\nif(WITH_FUSE_DRIVER)\n  include(${CMAKE_SOURCE_DIR}/cmake/need_fuse.cmake)\n\n  if(FUSE3_FOUND OR WINFSP OR APPLE)\n    add_library(dwarfs_main OBJECT tools/src/dwarfs_main.cpp)\n    target_compile_definitions(dwarfs_main PRIVATE _FILE_OFFSET_BITS=64)\n    add_executable(dwarfs-bin tools/src/dwarfs.cpp)\n    target_link_libraries(dwarfs-bin PRIVATE dwarfs_main)\n    set_target_properties(dwarfs-bin PROPERTIES OUTPUT_NAME dwarfs)\n    if(WINFSP)\n      target_compile_definitions(dwarfs_main PRIVATE FUSE_USE_VERSION=32\n                                                     DWARFS_FUSE_LOWLEVEL=0)\n      target_include_directories(dwarfs_main SYSTEM PRIVATE \"${WINFSP_PATH}/inc\")\n      target_link_libraries(dwarfs_main PRIVATE ${WINFSP})\n      target_link_libraries(dwarfs-bin PRIVATE delayimp.lib)\n      target_link_options(dwarfs-bin PRIVATE /DELAYLOAD:winfsp-x64.dll)\n      if(WITH_UNIVERSAL_BINARY)\n        target_link_libraries(dwarfsuniversal PRIVATE delayimp.lib)\n        target_link_options(dwarfsuniversal PRIVATE /DELAYLOAD:winfsp-x64.dll)\n      endif()\n    elseif(APPLE)\n      target_compile_definitions(dwarfs_main PRIVATE FUSE_USE_VERSION=29)\n      target_link_libraries(dwarfs_main PRIVATE PkgConfig::FUSE)\n    else()\n      target_compile_definitions(dwarfs_main PRIVATE FUSE_USE_VERSION=35)\n      target_link_libraries(dwarfs_main PRIVATE PkgConfig::FUSE3)\n    endif()\n    if(WITH_UNIVERSAL_BINARY)\n      target_link_libraries(dwarfsuniversal PRIVATE dwarfs_main)\n    endif()\n    if(WINFSP)\n      install(TARGETS dwarfs-bin RUNTIME DESTINATION bin)\n    else()\n      add_custom_command(OUTPUT  mount.dwarfs\n                         COMMAND ${CMAKE_COMMAND} -E create_symlink dwarfs mount.dwarfs\n                         DEPENDS dwarfs-bin)\n      list(APPEND SYMLINKS mount.dwarfs)\n      install(FILES ${CMAKE_CURRENT_BINARY_DIR}/mount.dwarfs DESTINATION sbin)\n      install(TARGETS dwarfs-bin RUNTIME DESTINATION sbin)\n    endif()\n    list(APPEND BINARY_TARGETS dwarfs-bin)\n    list(APPEND MAIN_TARGETS dwarfs_main)\n    target_link_libraries(dwarfs_main PRIVATE dwarfs_reader)\n  endif()\n\n  if(FUSE_FOUND AND (NOT APPLE) AND (WITH_LEGACY_FUSE OR NOT FUSE3_FOUND))\n    add_library(dwarfs2_main tools/src/dwarfs_main.cpp)\n    target_compile_definitions(dwarfs2_main PRIVATE _FILE_OFFSET_BITS=64\n                                                    FUSE_USE_VERSION=29)\n    target_link_libraries(dwarfs2_main PRIVATE PkgConfig::FUSE)\n    add_executable(dwarfs2-bin tools/src/dwarfs.cpp)\n    target_link_libraries(dwarfs2-bin PRIVATE dwarfs2_main)\n    if(WITH_UNIVERSAL_BINARY AND (NOT FUSE3_FOUND))\n      target_link_libraries(dwarfsuniversal PRIVATE dwarfs2_main)\n    endif()\n    set_target_properties(dwarfs2-bin PROPERTIES OUTPUT_NAME dwarfs2)\n    add_custom_command(OUTPUT  mount.dwarfs2\n                       COMMAND ${CMAKE_COMMAND} -E create_symlink dwarfs2 mount.dwarfs2\n                       DEPENDS dwarfs2-bin)\n    list(APPEND SYMLINKS mount.dwarfs2)\n    install(TARGETS dwarfs2-bin RUNTIME DESTINATION sbin)\n    install(FILES ${CMAKE_CURRENT_BINARY_DIR}/mount.dwarfs2 DESTINATION sbin)\n    list(APPEND BINARY_TARGETS dwarfs2-bin)\n    list(APPEND MAIN_TARGETS dwarfs2_main)\n    target_link_libraries(dwarfs2_main PRIVATE dwarfs_reader)\n  endif()\nendif()\n\nif(WITH_MAN_OPTION)\n  if(DWARFS_GIT_BUILD)\n    include(${CMAKE_SOURCE_DIR}/cmake/render_manpage.cmake)\n    file(MAKE_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/tools/src\")\n    set(DWARFS_MANPAGE_SOURCE_DIR ${CMAKE_CURRENT_BINARY_DIR})\n  else()\n    set(DWARFS_MANPAGE_SOURCE_DIR ${CMAKE_CURRENT_SOURCE_DIR})\n  endif()\n\n  foreach(man mkdwarfs dwarfs dwarfsck dwarfsextract)\n    if(DWARFS_GIT_BUILD)\n      file(MAKE_DIRECTORY \"${CMAKE_CURRENT_BINARY_DIR}/tools/src\")\n      add_manpage_source(doc/${man}.md NAME ${man}\n        OUTPUT ${CMAKE_CURRENT_BINARY_DIR}/tools/src/${man}_manpage.cpp)\n    endif()\n\n    if(TARGET ${man}_main)\n      target_sources(${man}_main PRIVATE ${DWARFS_MANPAGE_SOURCE_DIR}/tools/src/${man}_manpage.cpp)\n    endif()\n  endforeach()\n\n  if(TARGET dwarfs2_main)\n    target_sources(dwarfs2_main PRIVATE ${DWARFS_MANPAGE_SOURCE_DIR}/tools/src/dwarfs_manpage.cpp)\n  endif()\nendif()\n\nadd_custom_target(symlinks ALL DEPENDS ${SYMLINKS})\n\nif(WITH_TESTS OR WITH_BENCHMARKS OR WITH_FUZZ)\n  add_library(dwarfs_test_helpers OBJECT\n    test/test_helpers.cpp\n    test/test_iolayer.cpp\n    test/loremipsum.cpp\n    test/test_dirtree.cpp\n    test/filter_test_data.cpp\n  )\n  if(WITH_BENCHMARKS)\n    target_sources(dwarfs_test_helpers PRIVATE test/test_strings.cpp)\n  endif()\n  target_link_libraries(dwarfs_test_helpers PUBLIC dwarfs_common dwarfs_writer dwarfs_tool)\n  set_property(TARGET dwarfs_test_helpers PROPERTY CXX_STANDARD 20)\nendif()\n\nif(WITH_TESTS)\n  if(WITH_LIBDWARFS)\n    list(APPEND DWARFS_TESTS\n      badfs_test\n      block_merger_test\n      checksum_test\n      chmod_transformer_test\n      compat_test\n      dwarfs_test\n      entry_test\n      error_test\n      file_access_test\n      filesystem_test\n      filesystem_writer_test\n      fits_categorizer_test\n      fragment_category_test\n      global_metadata_test\n      incompressible_categorizer_test\n      integral_value_parser_test\n      lazy_value_test\n      metadata_requirements_test\n      options_test\n      pcm_sample_transformer_test\n      pcmaudio_categorizer_test\n      speedometer_test\n      terminal_test\n      utils_test\n      file_utils_test\n      worker_group_test\n    )\n\n    if(FLAC_FOUND)\n      list(APPEND DWARFS_TESTS\n        flac_compressor_test\n      )\n    endif()\n  endif()\n\n  if(WITH_TOOLS)\n    list(APPEND DWARFS_TESTS\n      block_cache_test\n      tool_main_test\n    )\n  endif()\n\n  if(WITH_TOOLS OR WITH_FUSE_DRIVER)\n    if(NOT WITH_TOOLS)\n      find_program(MKDWARFS_EXE mkdwarfs mkdwarfs.exe)\n      find_program(DWARFSCK_EXE dwarfsck dwarfsck.exe)\n      find_program(DWARFSEXTRACT_EXE dwarfsextract dwarfsextract.exe)\n    endif()\n    if(WITH_TOOLS OR (MKDWARFS_EXE AND DWARFSCK_EXE AND DWARFSEXTRACT_EXE))\n      list(APPEND DWARFS_TESTS\n        tools_test\n      )\n    endif()\n  endif()\n\n  if((WITH_TOOLS OR WITH_FUSE_DRIVER) AND WITH_MAN_OPTION)\n    list(APPEND DWARFS_TESTS manpage_test)\n  endif()\n\n  if(WITH_LIBDWARFS AND ENABLE_RICEPP)\n    list(APPEND DWARFS_TESTS ricepp_compressor_test)\n  endif()\n\n  foreach (test ${DWARFS_TESTS})\n    add_executable(${test} test/${test}.cpp)\n    target_link_libraries(\n      ${test} PRIVATE dwarfs_test_helpers gmock gtest gtest_main\n    )\n\n    if(NOT PREFER_SYSTEM_GTEST)\n      ### This is a wild hack. At least on macOS, gtest and basically everything\n      ### Homebrew is installed in /usr/local, and /usr/local/include can end up\n      ### in the compiler's include path *before* the include path of our local\n      ### gtest/gmock. The following code tries to ensure that the gtest/gmock\n      ### include paths are searched first.\n      get_target_property(gmock_include_dirs gmock INTERFACE_INCLUDE_DIRECTORIES)\n      get_target_property(gtest_include_dirs gtest INTERFACE_INCLUDE_DIRECTORIES)\n      target_include_directories(${test} PRIVATE BEFORE ${gmock_include_dirs} ${gtest_include_dirs})\n    endif()\n\n    target_compile_definitions(${test}\n      PRIVATE TEST_DATA_DIR=\\\"${CMAKE_SOURCE_DIR}/test\\\"\n              TOOLS_BIN_DIR=\\\"${CMAKE_CURRENT_BINARY_DIR}\\\")\n    list(APPEND TEST_TARGETS ${test})\n  endforeach()\n\n  if(TARGET tool_main_test)\n    target_link_libraries(tool_main_test PRIVATE mkdwarfs_main dwarfsck_main dwarfsextract_main)\n  endif()\n\n  if(TARGET manpage_test)\n    if(WITH_TOOLS)\n      target_compile_definitions(manpage_test PRIVATE DWARFS_WITH_TOOLS)\n      target_link_libraries(manpage_test PRIVATE mkdwarfs_main dwarfsck_main dwarfsextract_main)\n    endif()\n    if(WITH_FUSE_DRIVER)\n      target_compile_definitions(manpage_test PRIVATE DWARFS_WITH_FUSE_DRIVER)\n      target_link_libraries(manpage_test PRIVATE dwarfs_main)\n    endif()\n  endif()\n\n  if(TARGET tools_test)\n    target_compile_definitions(tools_test PRIVATE\n      $<$<AND:$<BOOL:${WITH_UNIVERSAL_BINARY}>,$<BOOL:${WITH_TOOLS}>>:DWARFS_HAVE_UNIVERSAL_BINARY>\n      $<$<BOOL:${WITH_TOOLS}>:DWARFS_WITH_TOOLS>\n      $<$<BOOL:${WITH_FUSE_DRIVER}>:DWARFS_WITH_FUSE_DRIVER>\n      $<$<BOOL:${MKDWARFS_EXE}>:MKDWARFS_BINARY=\\\"${MKDWARFS_EXE}\\\">\n      $<$<BOOL:${DWARFSCK_EXE}>:DWARFSCK_BINARY=\\\"${DWARFSCK_EXE}\\\">\n      $<$<BOOL:${DWARFSEXTRACT_EXE}>:DWARFSEXTRACT_BINARY=\\\"${DWARFSEXTRACT_EXE}\\\">\n    )\n  endif()\n\n  if(TARGET block_cache_test)\n    target_link_libraries(block_cache_test PRIVATE mkdwarfs_main)\n  endif()\n\n  foreach(tgt fits_categorizer_test\n              incompressible_categorizer_test\n              pcmaudio_categorizer_test\n              tool_main_test)\n    if(TARGET ${tgt})\n      target_link_libraries(${tgt} PRIVATE dwarfs_writer)\n    endif()\n  endforeach()\n\n  foreach(tgt ${TEST_TARGETS})\n    gtest_discover_tests(${tgt}\n      DISCOVERY_TIMEOUT 120\n      PROPERTIES ENVIRONMENT \"LLVM_PROFILE_FILE=${CMAKE_BINARY_DIR}/profile/%32m.profraw\"\n    )\n  endforeach()\nendif()\n\nif(WITH_LIBDWARFS AND WITH_BENCHMARKS)\n  find_package(benchmark 1.8)\n  if(benchmark_FOUND)\n    add_executable(dwarfs_benchmark test/dwarfs_benchmark.cpp)\n    target_link_libraries(dwarfs_benchmark PRIVATE dwarfs_test_helpers benchmark::benchmark)\n    target_link_libraries(dwarfs_benchmark PRIVATE dwarfs_reader dwarfs_writer)\n    list(APPEND BENCHMARK_TARGETS dwarfs_benchmark)\n\n    add_executable(multiversioning_benchmark test/multiversioning_benchmark.cpp)\n    target_link_libraries(multiversioning_benchmark PRIVATE benchmark::benchmark)\n    target_link_libraries(multiversioning_benchmark PRIVATE dwarfs_writer)\n    list(APPEND BENCHMARK_TARGETS multiversioning_benchmark)\n\n    add_executable(converter_benchmark test/converter_benchmark.cpp)\n    target_link_libraries(converter_benchmark PRIVATE dwarfs_test_helpers benchmark::benchmark)\n    list(APPEND BENCHMARK_TARGETS converter_benchmark)\n  endif()\n\n  # TODO: migrate to benchmark?\n  add_executable(segmenter_benchmark test/segmenter_benchmark.cpp)\n  target_link_libraries(segmenter_benchmark PRIVATE dwarfs_follybenchmark_lite dwarfs_test_helpers)\n  target_link_libraries(segmenter_benchmark PRIVATE dwarfs_writer)\n  list(APPEND BENCHMARK_TARGETS segmenter_benchmark)\n\n  list(APPEND BINARY_TARGETS ${BENCHMARK_TARGETS})\nendif()\n\nif(WITH_LIBDWARFS AND WITH_FUZZ)\n  add_executable(fuzz_categorizers test/fuzz_categorizers.cpp)\n  target_link_libraries(fuzz_categorizers PRIVATE dwarfs_writer)\n  list(APPEND BINARY_TARGETS fuzz_categorizers)\n\n  add_executable(fuzz_mkdwarfs test/fuzz_mkdwarfs.cpp)\n  target_link_libraries(fuzz_mkdwarfs PRIVATE mkdwarfs_main dwarfs_test_helpers)\n  list(APPEND BINARY_TARGETS fuzz_mkdwarfs)\nendif()\n\nforeach(tgt ${LIBDWARFS_TARGETS})\n  target_include_directories(${tgt} PRIVATE\n    $<BUILD_INTERFACE:$<TARGET_PROPERTY:phmap,INTERFACE_INCLUDE_DIRECTORIES>>\n  )\nendforeach()\n\nforeach(tgt ${BINARY_TARGETS} ${TEST_TARGETS} ${MAIN_TARGETS})\n  target_include_directories(${tgt} PRIVATE $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/tools/include>)\nendforeach()\n\nforeach(tgt ${LIBDWARFS_TARGETS} ${LIBDWARFS_OBJECT_TARGETS} dwarfs_test_helpers\n            ${BINARY_TARGETS} ${TEST_TARGETS} ${MAIN_TARGETS})\n  if(NOT TARGET ${tgt})\n    continue()\n  endif()\n\n  set_target_properties(${tgt} PROPERTIES EXPORT_COMPILE_COMMANDS ON)\n\n  target_link_libraries(${tgt} PUBLIC Boost::boost)\n\n  # TODO: need to get USE_JEMALLOC and others from imported libdwarfs\n  if(USE_JEMALLOC)\n    target_link_libraries(${tgt} PRIVATE PkgConfig::JEMALLOC)\n  endif(USE_JEMALLOC)\n\n  target_include_directories(${tgt} PUBLIC\n    $<BUILD_INTERFACE:${CMAKE_CURRENT_SOURCE_DIR}/include>\n    $<INSTALL_INTERFACE:include>\n  )\n\n  target_include_directories(${tgt} PUBLIC\n    $<BUILD_INTERFACE:$<TARGET_PROPERTY:range-v3::range-v3,INTERFACE_INCLUDE_DIRECTORIES>>\n  )\n\n  if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\" OR\n     \"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n    target_compile_options(${tgt} PRIVATE -Wall -Wextra -pedantic)\n  endif()\n\n  if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n    target_compile_options(${tgt} PRIVATE -Wno-stringop-overflow)\n  endif()\n\n  set_property(TARGET ${tgt} PROPERTY CXX_STANDARD 20)\n  set_property(TARGET ${tgt} PROPERTY CXX_STANDARD_REQUIRED ON)\n  set_property(TARGET ${tgt} PROPERTY CXX_EXTENSIONS OFF)\n\n  if(ENABLE_COVERAGE)\n    if(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"GNU\")\n      target_compile_options(${tgt} PRIVATE --coverage -fno-omit-frame-pointer)\n      target_link_options(${tgt} PRIVATE --coverage)\n    elseif(\"${CMAKE_CXX_COMPILER_ID}\" STREQUAL \"Clang\")\n      target_compile_options(\n        ${tgt} PRIVATE -fprofile-instr-generate -fcoverage-mapping\n                       -fno-omit-frame-pointer)\n      target_link_options(${tgt} PRIVATE -fprofile-instr-generate\n                          -fcoverage-mapping)\n    endif()\n    target_compile_definitions(${tgt} PRIVATE DWARFS_COVERAGE_ENABLED=1)\n  endif()\n\n  if(DWARFS_FMT_LIB)\n    target_link_libraries(${tgt} PRIVATE ${DWARFS_FMT_LIB})\n  endif()\n\n  if(WIN32)\n    target_link_libraries(${tgt} PRIVATE ntdll.lib dbghelp.lib)\n  endif()\nendforeach()\n\nif(CMAKE_BUILD_TYPE STREQUAL Release)\n  # not sure why exactly, copied from fsst/CMakeLists.txt\n  set_source_files_properties(fsst/fsst_avx512.cpp PROPERTIES COMPILE_FLAGS -O1)\nendif()\n\nforeach(tgt dwarfs_test_helpers dwarfs_follybenchmark_lite\n            ${LIBDWARFS_TARGETS} ${LIBDWARFS_OBJECT_TARGETS}\n            ${BINARY_TARGETS} ${TEST_TARGETS} ${MAIN_TARGETS})\n  if(TARGET ${tgt})\n    if(ENABLE_ASAN)\n      target_compile_options(${tgt} PRIVATE -fsanitize=address\n                                            -fno-omit-frame-pointer)\n      target_link_options(${tgt} PRIVATE -fsanitize=address)\n    endif()\n\n    if(ENABLE_TSAN)\n      target_compile_options(${tgt} PRIVATE -fsanitize=thread\n                                            -fno-omit-frame-pointer)\n      target_link_options(${tgt} PRIVATE -fsanitize=thread)\n    endif()\n\n    if(ENABLE_UBSAN)\n      target_compile_options(${tgt} PRIVATE -fsanitize=undefined\n                                            -fno-omit-frame-pointer)\n      target_link_options(${tgt} PRIVATE -fsanitize=undefined)\n    endif()\n  endif()\nendforeach()\n\nforeach(tgt ${MAIN_TARGETS} ${BINARY_TARGETS} ${TEST_TARGETS})\n  target_link_libraries(${tgt} PRIVATE dwarfs_tool)\nendforeach()\n\nforeach(tgt ${TEST_TARGETS})\n  target_link_libraries(${tgt} PRIVATE dwarfs_writer dwarfs_rewrite dwarfs_extractor)\nendforeach()\n\nif(STATIC_BUILD_DO_NOT_USE)\n  # ...................................................................\n  # Each library name given to the NAMES option is first considered as a library\n  # file name and then considered with platform-specific prefixes (e.g. lib) and\n  # suffixes (e.g. .so).\n  # ...................................................................\n\n  function(IMPORT_STATIC_LIB TARGET NAME)\n    find_library(_TMP_LIB_LOC_${TARGET} ${NAME} NO_CACHE REQUIRED)\n    add_library(${TARGET} STATIC IMPORTED)\n    set_target_properties(${TARGET} PROPERTIES IMPORTED_LOCATION\n                                               ${_TMP_LIB_LOC_${TARGET}})\n  endfunction()\n\n  import_static_lib(static_libglog \"libglog.a\")\n  import_static_lib(static_libdoubleconv \"libdouble-conversion.a\")\n  import_static_lib(static_libgflags \"libgflags.a\")\n  import_static_lib(static_libevent \"libevent.a\")\n  import_static_lib(static_libacl \"libacl.a\")\n  import_static_lib(static_libxml2 \"libxml2.a\")\n  import_static_lib(static_libcrypto \"libcrypto.a\")\n  import_static_lib(static_libz \"libz.a\")\n  import_static_lib(static_libpthread \"libpthread.a\")\n  import_static_lib(static_libdl \"libdl.a\")\n  import_static_lib(static_libm \"libm.a\")\n  import_static_lib(static_librt \"librt.a\")\n  import_static_lib(static_libssl \"libssl.a\")\n  import_static_lib(static_libunwind \"libunwind.a\")\n  import_static_lib(static_libarchive \"libarchive.a\")\n  import_static_lib(static_libmagic \"libmagic.a\")\n  import_static_lib(static_libflac \"libFLAC++.a\")\n\n  set_target_properties(static_libunwind PROPERTIES INTERFACE_LINK_LIBRARIES\n                                                    \"${LIBLZMA_LIBRARIES};static_libz\")\n  set_target_properties(static_libglog PROPERTIES INTERFACE_LINK_LIBRARIES\n                                                  static_libgflags)\n  set_target_properties(static_librt PROPERTIES INTERFACE_LINK_LIBRARIES\n                                                static_libgflags)\n  set_target_properties(static_libmagic PROPERTIES INTERFACE_LINK_LIBRARIES\n                                                static_libz)\n\n  foreach(tgt ${BINARY_TARGETS} ${TEST_TARGETS})\n    # ...................................................................\n    # -static-libgcc above and gcc_eh below is all together an ugly trick to\n    # enforce static linking\n    # ...................................................................\n    target_link_libraries(\n      ${tgt}\n      PRIVATE\n      static_libdoubleconv\n      static_libglog\n      static_libgflags\n      static_libarchive\n      static_libevent\n      static_libacl\n      static_libssl\n      static_libcrypto\n      static_libpthread\n      static_libdl\n      static_libz\n      static_libm\n      static_librt\n      gcc_eh\n      static_libunwind)\n  endforeach()\nendif(STATIC_BUILD_DO_NOT_USE)\n\nforeach(tgt ${TEST_TARGETS})\n  list(APPEND REALCLEAN_FILES \"${tgt}[1]_include.cmake\")\nendforeach()\n\nforeach(tgt ${BINARY_TARGETS} ${TEST_TARGETS})\n  list(APPEND REALCLEAN_FILES $<TARGET_FILE:${tgt}>.manifest)\n  if(WIN32)\n    list(APPEND REALCLEAN_FILES ${tgt}.ilk ${tgt}.pdb)\n  endif()\n  if(STATIC_BUILD_DO_NOT_USE)\n    target_link_options(${tgt} PRIVATE -static -static-libgcc)\n  endif()\nendforeach()\n\nadd_custom_target(\n  realclean\n  COMMAND ${CMAKE_MAKE_PROGRAM} clean\n  COMMAND ${CMAKE_COMMAND} -E rm -rf\n      cmake_install.cmake install_manifest.txt\n      dwarfs_install.cmake package_version.cmake\n      CPackConfig.cmake CPackSourceConfig.cmake _CPack_Packages\n      CTestTestfile.cmake Testing\n      fbthrift folly zstd ricepp tools\n      include src thrift universal bin lib man1 man5\n      vcpkg-manifest-install.log\n      Makefile compile_commands.json\n      artifacts.env source-artifacts.env\n      default.profraw profile\n      dwarfs-config-version.cmake\n      dwarfs-config.cmake\n      dwarfs.ilk dwarfs.pdb\n      .ninja_deps build.ninja\n      CMakeCache.txt\n      ${REALCLEAN_FILES}\n)\n\nadd_custom_target(\n  distclean\n  COMMAND ${CMAKE_MAKE_PROGRAM} realclean\n  COMMAND ${CMAKE_COMMAND} -E rm -rf _deps\n  COMMAND ${CMAKE_COMMAND} -E rm -rf CMakeFiles\n)\n\nfile(GLOB_RECURSE ALL_SOURCES LIST_DIRECTORIES false\n        ${CMAKE_CURRENT_SOURCE_DIR}/ricepp/*.h\n        ${CMAKE_CURRENT_SOURCE_DIR}/ricepp/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/include/*.h\n        ${CMAKE_CURRENT_SOURCE_DIR}/src/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/tools/include/*.h\n        ${CMAKE_CURRENT_SOURCE_DIR}/tools/src/*.cpp\n        ${CMAKE_CURRENT_SOURCE_DIR}/test/*.h\n        ${CMAKE_CURRENT_SOURCE_DIR}/test/*.cpp)\n\nadd_custom_target(\n  format\n  COMMAND clang-format -i ${ALL_SOURCES})\n\nif(STATIC_BUILD_DO_NOT_USE OR APPLE)\n  if(CMAKE_BUILD_TYPE STREQUAL \"Release\")\n    foreach(tgt ${BINARY_TARGETS})\n      list(APPEND FILES_TO_STRIP $<TARGET_FILE:${tgt}>)\n    endforeach()\n    if(APPLE)\n      add_custom_target(strip COMMAND strip ${FILES_TO_STRIP})\n    else()\n      add_custom_target(strip COMMAND strip $<IF:$<BOOL:${ENABLE_STACKTRACE}>,--strip-debug,--strip-all> ${FILES_TO_STRIP})\n    endif()\n  endif()\nendif()\n\nset(DWARFS_ARTIFACT_ID \"${PRJ_VERSION_FULL}-${CMAKE_SYSTEM_NAME}-${CMAKE_SYSTEM_PROCESSOR}\")\nif(CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n  set(DWARFS_ARTIFACT_ID \"${DWARFS_ARTIFACT_ID}-gcc\")\nelseif(CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n  set(DWARFS_ARTIFACT_ID \"${DWARFS_ARTIFACT_ID}-clang\")\nendif()\nif(DWARFS_OPTIMIZE)\n  set(DWARFS_ARTIFACT_ID \"${DWARFS_ARTIFACT_ID}-O${DWARFS_OPTIMIZE}\")\nendif()\nif(CMAKE_BUILD_TYPE STREQUAL \"RelWithDebInfo\")\n  set(DWARFS_ARTIFACT_ID \"${DWARFS_ARTIFACT_ID}-reldbg\")\nelseif(CMAKE_BUILD_TYPE STREQUAL \"Debug\")\n  set(DWARFS_ARTIFACT_ID \"${DWARFS_ARTIFACT_ID}-debug\")\nendif()\nif(ENABLE_STACKTRACE)\n  set(DWARFS_ARTIFACT_ID \"${DWARFS_ARTIFACT_ID}-stacktrace\")\nendif()\n\nif(STATIC_BUILD_DO_NOT_USE OR WIN32)\n  if(WITH_UNIVERSAL_BINARY)\n    set(UNIVERSAL_OUT \"dwarfs-universal-${DWARFS_ARTIFACT_ID}${CMAKE_EXECUTABLE_SUFFIX}\")\n\n    if(ENABLE_STACKTRACE)\n      message(WARNING \"UPX compression is disabled with ENABLE_STACKTRACE\")\n\n      add_custom_command(\n        OUTPUT ${UNIVERSAL_OUT}\n        COMMAND ${CMAKE_COMMAND} -E copy $<TARGET_FILE:dwarfsuniversal> ${UNIVERSAL_OUT}\n        WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}\n      )\n    else()\n      find_program(UPX_EXE upx upx.exe PATHS \"c:/bin\" DOC \"ultimate packer for executables\" REQUIRED)\n\n      # upx -9 is a good compromise between compression ratio and speed\n      # also, anything above --best increases the startup time of the compressed\n      # executable significantly\n      add_custom_command(\n        OUTPUT ${UNIVERSAL_OUT}\n        COMMAND ${UPX_EXE} -9 -o ${UNIVERSAL_OUT} $<TARGET_FILE:dwarfsuniversal>\n        WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}\n      )\n    endif()\n\n    add_custom_target(universal_upx DEPENDS ${UNIVERSAL_OUT})\n  endif()\nendif()\n\nif(DEFINED ENV{GITHUB_REF_TYPE})\n  message(STATUS \"GITHUB_REF_TYPE: $ENV{GITHUB_REF_TYPE}\")\n  message(STATUS \"GITHUB_REF_NAME: $ENV{GITHUB_REF_NAME}\")\n  message(STATUS \"GITHUB_RUN_ID: $ENV{GITHUB_RUN_ID}\")\n  message(STATUS \"GITHUB_RUN_NUMBER: $ENV{GITHUB_RUN_NUMBER}\")\n  message(STATUS \"GITHUB_RUN_ATTEMPT: $ENV{GITHUB_RUN_ATTEMPT}\")\n\n  if(\"$ENV{GITHUB_REF_TYPE}\" STREQUAL \"tag\")\n    set(ARTIFACTS_SUBDIR \"releases/$ENV{GITHUB_REF_NAME}@${PRJ_GIT_REV}\")\n  else()\n    set(ARTIFACTS_SUBDIR \"builds/$ENV{GITHUB_RUN_NUMBER}.$ENV{GITHUB_RUN_ATTEMPT}-${PRJ_VERSION_FULL}\")\n  endif()\n\n  set(ARTIFACTS_FULL_PATH \"${DWARFS_ARTIFACTS_DIR}/${ARTIFACTS_SUBDIR}\")\n\n  if(WIN32)\n    set(PACKAGE_EXT \".7z\")\n  else()\n    set(PACKAGE_EXT \".tar.zst\")\n  endif()\n\n  set(SOURCE_TARBALL \"${CMAKE_PROJECT_NAME}-${PRJ_VERSION_FULL}${PACKAGE_EXT}\")\n\n  add_custom_target(copy_source_artifacts\n    COMMAND ${CMAKE_COMMAND} -E make_directory ${ARTIFACTS_FULL_PATH}\n    COMMAND ${CMAKE_COMMAND} -E copy ${CMAKE_CURRENT_BINARY_DIR}/${SOURCE_TARBALL} ${ARTIFACTS_FULL_PATH}\n    COMMAND ${CMAKE_COMMAND} -E make_directory ${DWARFS_ARTIFACTS_DIR}/cache\n    COMMAND ${CMAKE_COMMAND} -E create_symlink ../${ARTIFACTS_SUBDIR}/${SOURCE_TARBALL}\n                ${DWARFS_ARTIFACTS_DIR}/cache/dwarfs-source-$ENV{GITHUB_RUN_NUMBER}${PACKAGE_EXT}\n  )\n\n  if(STATIC_BUILD_DO_NOT_USE OR WIN32)\n    file(WRITE \"${CMAKE_CURRENT_BINARY_DIR}/artifacts.env\"\n               \"binary_tarball=${CMAKE_PROJECT_NAME}-${DWARFS_ARTIFACT_ID}${PACKAGE_EXT}\\n\"\n               \"universal_binary=${UNIVERSAL_OUT}\\n\")\n\n    add_custom_command(\n      OUTPUT _copy_artifacts\n      COMMAND ${CMAKE_COMMAND} -E make_directory ${ARTIFACTS_FULL_PATH}\n      COMMAND ${CMAKE_COMMAND} -E copy ${CMAKE_CURRENT_BINARY_DIR}/${CMAKE_PROJECT_NAME}-${DWARFS_ARTIFACT_ID}${PACKAGE_EXT} ${ARTIFACTS_FULL_PATH}\n      COMMAND ${CMAKE_COMMAND} -E copy ${CMAKE_CURRENT_BINARY_DIR}/${UNIVERSAL_OUT} ${ARTIFACTS_FULL_PATH}\n    )\n\n    if(WITH_BENCHMARKS)\n      foreach(tgt ${BENCHMARK_TARGETS})\n        add_custom_command(\n          OUTPUT _copy_artifacts APPEND\n          COMMAND ${CMAKE_COMMAND} -E copy\n              ${CMAKE_CURRENT_BINARY_DIR}/${tgt}${CMAKE_EXECUTABLE_SUFFIX}\n              ${ARTIFACTS_FULL_PATH}/${tgt}-${DWARFS_ARTIFACT_ID}${CMAKE_EXECUTABLE_SUFFIX}\n        )\n      endforeach()\n\n      if (ENABLE_RICEPP)\n        add_custom_command(\n          OUTPUT _copy_artifacts APPEND\n          COMMAND ${CMAKE_COMMAND} -E copy\n              ${CMAKE_CURRENT_BINARY_DIR}/ricepp/ricepp_benchmark_fits${CMAKE_EXECUTABLE_SUFFIX}\n              ${ARTIFACTS_FULL_PATH}/ricepp_benchmark_fits-${DWARFS_ARTIFACT_ID}${CMAKE_EXECUTABLE_SUFFIX}\n        )\n      endif()\n    endif()\n\n    add_custom_target(copy_artifacts DEPENDS _copy_artifacts)\n    set_source_files_properties(_copy_artifacts PROPERTIES SYMBOLIC ON)\n  endif()\nendif()\n\nconfigure_file(\"${PROJECT_SOURCE_DIR}/cmake/dwarfs_install.cmake.in\" dwarfs_install.cmake @ONLY)\nset(CPACK_INSTALL_SCRIPT \"${CMAKE_CURRENT_BINARY_DIR}/dwarfs_install.cmake\")\n\nif(WIN32)\n  # set(CPACK_GENERATOR \"NSIS;ZIP;7Z\")\n  set(CPACK_GENERATOR \"7Z\")\nelse()\n  # use TZST and later re-pack as TXZ\n  set(CPACK_GENERATOR \"TZST\")\nendif()\nset(CPACK_SOURCE_GENERATOR \"${CPACK_GENERATOR}\")\nset(CPACK_PACKAGE_VERSION_MAJOR \"${PRJ_VERSION_MAJOR}\")\nset(CPACK_PACKAGE_VERSION_MINOR \"${PRJ_VERSION_MINOR}\")\nset(CPACK_PACKAGE_VERSION_PATCH \"${PRJ_VERSION_PATCH}\")\nset(CPACK_SOURCE_PACKAGE_FILE_NAME \"${CMAKE_PROJECT_NAME}-${PRJ_VERSION_FULL}\")\nset(CPACK_PACKAGE_FILE_NAME \"${CMAKE_PROJECT_NAME}-${DWARFS_ARTIFACT_ID}\")\nset(CPACK_PACKAGE_DESCRIPTION_SUMMARY \"dwarfs - A high compression read-only file system\")\nset(CPACK_PACKAGE_VENDOR \"Marcus Holland-Moritz <github@mhxnet.de>\")\nset(CPACK_PACKAGE_DESCRIPTION_FILE \"${CMAKE_CURRENT_SOURCE_DIR}/README.md\")\nset(CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_CURRENT_SOURCE_DIR}/LICENSE\")\nlist(APPEND CPACK_SOURCE_IGNORE_FILES\n  \"\\\\.git/\"\n  \"${CMAKE_SOURCE_DIR}/build.*\"\n  \"${CMAKE_SOURCE_DIR}/@\"\n  \"/\\\\.\"\n  \".*~$\"\n  \"${CMAKE_SOURCE_DIR}/doc/.*\\\\.png$\"\n  \"${CMAKE_SOURCE_DIR}/doc/.*\\\\.gif$\"\n)\nset(CPACK_VERBATIM_VARIABLES YES)\n\ninclude(CPack)\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 34.3251953125,
          "content": "                    GNU GENERAL PUBLIC LICENSE\n                       Version 3, 29 June 2007\n\n Copyright (C) 2007 Free Software Foundation, Inc. <https://fsf.org/>\n Everyone is permitted to copy and distribute verbatim copies\n of this license document, but changing it is not allowed.\n\n                            Preamble\n\n  The GNU General Public License is a free, copyleft license for\nsoftware and other kinds of works.\n\n  The licenses for most software and other practical works are designed\nto take away your freedom to share and change the works.  By contrast,\nthe GNU General Public License is intended to guarantee your freedom to\nshare and change all versions of a program--to make sure it remains free\nsoftware for all its users.  We, the Free Software Foundation, use the\nGNU General Public License for most of our software; it applies also to\nany other work released this way by its authors.  You can apply it to\nyour programs, too.\n\n  When we speak of free software, we are referring to freedom, not\nprice.  Our General Public Licenses are designed to make sure that you\nhave the freedom to distribute copies of free software (and charge for\nthem if you wish), that you receive source code or can get it if you\nwant it, that you can change the software or use pieces of it in new\nfree programs, and that you know you can do these things.\n\n  To protect your rights, we need to prevent others from denying you\nthese rights or asking you to surrender the rights.  Therefore, you have\ncertain responsibilities if you distribute copies of the software, or if\nyou modify it: responsibilities to respect the freedom of others.\n\n  For example, if you distribute copies of such a program, whether\ngratis or for a fee, you must pass on to the recipients the same\nfreedoms that you received.  You must make sure that they, too, receive\nor can get the source code.  And you must show them these terms so they\nknow their rights.\n\n  Developers that use the GNU GPL protect your rights with two steps:\n(1) assert copyright on the software, and (2) offer you this License\ngiving you legal permission to copy, distribute and/or modify it.\n\n  For the developers' and authors' protection, the GPL clearly explains\nthat there is no warranty for this free software.  For both users' and\nauthors' sake, the GPL requires that modified versions be marked as\nchanged, so that their problems will not be attributed erroneously to\nauthors of previous versions.\n\n  Some devices are designed to deny users access to install or run\nmodified versions of the software inside them, although the manufacturer\ncan do so.  This is fundamentally incompatible with the aim of\nprotecting users' freedom to change the software.  The systematic\npattern of such abuse occurs in the area of products for individuals to\nuse, which is precisely where it is most unacceptable.  Therefore, we\nhave designed this version of the GPL to prohibit the practice for those\nproducts.  If such problems arise substantially in other domains, we\nstand ready to extend this provision to those domains in future versions\nof the GPL, as needed to protect the freedom of users.\n\n  Finally, every program is threatened constantly by software patents.\nStates should not allow patents to restrict development and use of\nsoftware on general-purpose computers, but in those that do, we wish to\navoid the special danger that patents applied to a free program could\nmake it effectively proprietary.  To prevent this, the GPL assures that\npatents cannot be used to render the program non-free.\n\n  The precise terms and conditions for copying, distribution and\nmodification follow.\n\n                       TERMS AND CONDITIONS\n\n  0. Definitions.\n\n  \"This License\" refers to version 3 of the GNU General Public License.\n\n  \"Copyright\" also means copyright-like laws that apply to other kinds of\nworks, such as semiconductor masks.\n\n  \"The Program\" refers to any copyrightable work licensed under this\nLicense.  Each licensee is addressed as \"you\".  \"Licensees\" and\n\"recipients\" may be individuals or organizations.\n\n  To \"modify\" a work means to copy from or adapt all or part of the work\nin a fashion requiring copyright permission, other than the making of an\nexact copy.  The resulting work is called a \"modified version\" of the\nearlier work or a work \"based on\" the earlier work.\n\n  A \"covered work\" means either the unmodified Program or a work based\non the Program.\n\n  To \"propagate\" a work means to do anything with it that, without\npermission, would make you directly or secondarily liable for\ninfringement under applicable copyright law, except executing it on a\ncomputer or modifying a private copy.  Propagation includes copying,\ndistribution (with or without modification), making available to the\npublic, and in some countries other activities as well.\n\n  To \"convey\" a work means any kind of propagation that enables other\nparties to make or receive copies.  Mere interaction with a user through\na computer network, with no transfer of a copy, is not conveying.\n\n  An interactive user interface displays \"Appropriate Legal Notices\"\nto the extent that it includes a convenient and prominently visible\nfeature that (1) displays an appropriate copyright notice, and (2)\ntells the user that there is no warranty for the work (except to the\nextent that warranties are provided), that licensees may convey the\nwork under this License, and how to view a copy of this License.  If\nthe interface presents a list of user commands or options, such as a\nmenu, a prominent item in the list meets this criterion.\n\n  1. Source Code.\n\n  The \"source code\" for a work means the preferred form of the work\nfor making modifications to it.  \"Object code\" means any non-source\nform of a work.\n\n  A \"Standard Interface\" means an interface that either is an official\nstandard defined by a recognized standards body, or, in the case of\ninterfaces specified for a particular programming language, one that\nis widely used among developers working in that language.\n\n  The \"System Libraries\" of an executable work include anything, other\nthan the work as a whole, that (a) is included in the normal form of\npackaging a Major Component, but which is not part of that Major\nComponent, and (b) serves only to enable use of the work with that\nMajor Component, or to implement a Standard Interface for which an\nimplementation is available to the public in source code form.  A\n\"Major Component\", in this context, means a major essential component\n(kernel, window system, and so on) of the specific operating system\n(if any) on which the executable work runs, or a compiler used to\nproduce the work, or an object code interpreter used to run it.\n\n  The \"Corresponding Source\" for a work in object code form means all\nthe source code needed to generate, install, and (for an executable\nwork) run the object code and to modify the work, including scripts to\ncontrol those activities.  However, it does not include the work's\nSystem Libraries, or general-purpose tools or generally available free\nprograms which are used unmodified in performing those activities but\nwhich are not part of the work.  For example, Corresponding Source\nincludes interface definition files associated with source files for\nthe work, and the source code for shared libraries and dynamically\nlinked subprograms that the work is specifically designed to require,\nsuch as by intimate data communication or control flow between those\nsubprograms and other parts of the work.\n\n  The Corresponding Source need not include anything that users\ncan regenerate automatically from other parts of the Corresponding\nSource.\n\n  The Corresponding Source for a work in source code form is that\nsame work.\n\n  2. Basic Permissions.\n\n  All rights granted under this License are granted for the term of\ncopyright on the Program, and are irrevocable provided the stated\nconditions are met.  This License explicitly affirms your unlimited\npermission to run the unmodified Program.  The output from running a\ncovered work is covered by this License only if the output, given its\ncontent, constitutes a covered work.  This License acknowledges your\nrights of fair use or other equivalent, as provided by copyright law.\n\n  You may make, run and propagate covered works that you do not\nconvey, without conditions so long as your license otherwise remains\nin force.  You may convey covered works to others for the sole purpose\nof having them make modifications exclusively for you, or provide you\nwith facilities for running those works, provided that you comply with\nthe terms of this License in conveying all material for which you do\nnot control copyright.  Those thus making or running the covered works\nfor you must do so exclusively on your behalf, under your direction\nand control, on terms that prohibit them from making any copies of\nyour copyrighted material outside their relationship with you.\n\n  Conveying under any other circumstances is permitted solely under\nthe conditions stated below.  Sublicensing is not allowed; section 10\nmakes it unnecessary.\n\n  3. Protecting Users' Legal Rights From Anti-Circumvention Law.\n\n  No covered work shall be deemed part of an effective technological\nmeasure under any applicable law fulfilling obligations under article\n11 of the WIPO copyright treaty adopted on 20 December 1996, or\nsimilar laws prohibiting or restricting circumvention of such\nmeasures.\n\n  When you convey a covered work, you waive any legal power to forbid\ncircumvention of technological measures to the extent such circumvention\nis effected by exercising rights under this License with respect to\nthe covered work, and you disclaim any intention to limit operation or\nmodification of the work as a means of enforcing, against the work's\nusers, your or third parties' legal rights to forbid circumvention of\ntechnological measures.\n\n  4. Conveying Verbatim Copies.\n\n  You may convey verbatim copies of the Program's source code as you\nreceive it, in any medium, provided that you conspicuously and\nappropriately publish on each copy an appropriate copyright notice;\nkeep intact all notices stating that this License and any\nnon-permissive terms added in accord with section 7 apply to the code;\nkeep intact all notices of the absence of any warranty; and give all\nrecipients a copy of this License along with the Program.\n\n  You may charge any price or no price for each copy that you convey,\nand you may offer support or warranty protection for a fee.\n\n  5. Conveying Modified Source Versions.\n\n  You may convey a work based on the Program, or the modifications to\nproduce it from the Program, in the form of source code under the\nterms of section 4, provided that you also meet all of these conditions:\n\n    a) The work must carry prominent notices stating that you modified\n    it, and giving a relevant date.\n\n    b) The work must carry prominent notices stating that it is\n    released under this License and any conditions added under section\n    7.  This requirement modifies the requirement in section 4 to\n    \"keep intact all notices\".\n\n    c) You must license the entire work, as a whole, under this\n    License to anyone who comes into possession of a copy.  This\n    License will therefore apply, along with any applicable section 7\n    additional terms, to the whole of the work, and all its parts,\n    regardless of how they are packaged.  This License gives no\n    permission to license the work in any other way, but it does not\n    invalidate such permission if you have separately received it.\n\n    d) If the work has interactive user interfaces, each must display\n    Appropriate Legal Notices; however, if the Program has interactive\n    interfaces that do not display Appropriate Legal Notices, your\n    work need not make them do so.\n\n  A compilation of a covered work with other separate and independent\nworks, which are not by their nature extensions of the covered work,\nand which are not combined with it such as to form a larger program,\nin or on a volume of a storage or distribution medium, is called an\n\"aggregate\" if the compilation and its resulting copyright are not\nused to limit the access or legal rights of the compilation's users\nbeyond what the individual works permit.  Inclusion of a covered work\nin an aggregate does not cause this License to apply to the other\nparts of the aggregate.\n\n  6. Conveying Non-Source Forms.\n\n  You may convey a covered work in object code form under the terms\nof sections 4 and 5, provided that you also convey the\nmachine-readable Corresponding Source under the terms of this License,\nin one of these ways:\n\n    a) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by the\n    Corresponding Source fixed on a durable physical medium\n    customarily used for software interchange.\n\n    b) Convey the object code in, or embodied in, a physical product\n    (including a physical distribution medium), accompanied by a\n    written offer, valid for at least three years and valid for as\n    long as you offer spare parts or customer support for that product\n    model, to give anyone who possesses the object code either (1) a\n    copy of the Corresponding Source for all the software in the\n    product that is covered by this License, on a durable physical\n    medium customarily used for software interchange, for a price no\n    more than your reasonable cost of physically performing this\n    conveying of source, or (2) access to copy the\n    Corresponding Source from a network server at no charge.\n\n    c) Convey individual copies of the object code with a copy of the\n    written offer to provide the Corresponding Source.  This\n    alternative is allowed only occasionally and noncommercially, and\n    only if you received the object code with such an offer, in accord\n    with subsection 6b.\n\n    d) Convey the object code by offering access from a designated\n    place (gratis or for a charge), and offer equivalent access to the\n    Corresponding Source in the same way through the same place at no\n    further charge.  You need not require recipients to copy the\n    Corresponding Source along with the object code.  If the place to\n    copy the object code is a network server, the Corresponding Source\n    may be on a different server (operated by you or a third party)\n    that supports equivalent copying facilities, provided you maintain\n    clear directions next to the object code saying where to find the\n    Corresponding Source.  Regardless of what server hosts the\n    Corresponding Source, you remain obligated to ensure that it is\n    available for as long as needed to satisfy these requirements.\n\n    e) Convey the object code using peer-to-peer transmission, provided\n    you inform other peers where the object code and Corresponding\n    Source of the work are being offered to the general public at no\n    charge under subsection 6d.\n\n  A separable portion of the object code, whose source code is excluded\nfrom the Corresponding Source as a System Library, need not be\nincluded in conveying the object code work.\n\n  A \"User Product\" is either (1) a \"consumer product\", which means any\ntangible personal property which is normally used for personal, family,\nor household purposes, or (2) anything designed or sold for incorporation\ninto a dwelling.  In determining whether a product is a consumer product,\ndoubtful cases shall be resolved in favor of coverage.  For a particular\nproduct received by a particular user, \"normally used\" refers to a\ntypical or common use of that class of product, regardless of the status\nof the particular user or of the way in which the particular user\nactually uses, or expects or is expected to use, the product.  A product\nis a consumer product regardless of whether the product has substantial\ncommercial, industrial or non-consumer uses, unless such uses represent\nthe only significant mode of use of the product.\n\n  \"Installation Information\" for a User Product means any methods,\nprocedures, authorization keys, or other information required to install\nand execute modified versions of a covered work in that User Product from\na modified version of its Corresponding Source.  The information must\nsuffice to ensure that the continued functioning of the modified object\ncode is in no case prevented or interfered with solely because\nmodification has been made.\n\n  If you convey an object code work under this section in, or with, or\nspecifically for use in, a User Product, and the conveying occurs as\npart of a transaction in which the right of possession and use of the\nUser Product is transferred to the recipient in perpetuity or for a\nfixed term (regardless of how the transaction is characterized), the\nCorresponding Source conveyed under this section must be accompanied\nby the Installation Information.  But this requirement does not apply\nif neither you nor any third party retains the ability to install\nmodified object code on the User Product (for example, the work has\nbeen installed in ROM).\n\n  The requirement to provide Installation Information does not include a\nrequirement to continue to provide support service, warranty, or updates\nfor a work that has been modified or installed by the recipient, or for\nthe User Product in which it has been modified or installed.  Access to a\nnetwork may be denied when the modification itself materially and\nadversely affects the operation of the network or violates the rules and\nprotocols for communication across the network.\n\n  Corresponding Source conveyed, and Installation Information provided,\nin accord with this section must be in a format that is publicly\ndocumented (and with an implementation available to the public in\nsource code form), and must require no special password or key for\nunpacking, reading or copying.\n\n  7. Additional Terms.\n\n  \"Additional permissions\" are terms that supplement the terms of this\nLicense by making exceptions from one or more of its conditions.\nAdditional permissions that are applicable to the entire Program shall\nbe treated as though they were included in this License, to the extent\nthat they are valid under applicable law.  If additional permissions\napply only to part of the Program, that part may be used separately\nunder those permissions, but the entire Program remains governed by\nthis License without regard to the additional permissions.\n\n  When you convey a copy of a covered work, you may at your option\nremove any additional permissions from that copy, or from any part of\nit.  (Additional permissions may be written to require their own\nremoval in certain cases when you modify the work.)  You may place\nadditional permissions on material, added by you to a covered work,\nfor which you have or can give appropriate copyright permission.\n\n  Notwithstanding any other provision of this License, for material you\nadd to a covered work, you may (if authorized by the copyright holders of\nthat material) supplement the terms of this License with terms:\n\n    a) Disclaiming warranty or limiting liability differently from the\n    terms of sections 15 and 16 of this License; or\n\n    b) Requiring preservation of specified reasonable legal notices or\n    author attributions in that material or in the Appropriate Legal\n    Notices displayed by works containing it; or\n\n    c) Prohibiting misrepresentation of the origin of that material, or\n    requiring that modified versions of such material be marked in\n    reasonable ways as different from the original version; or\n\n    d) Limiting the use for publicity purposes of names of licensors or\n    authors of the material; or\n\n    e) Declining to grant rights under trademark law for use of some\n    trade names, trademarks, or service marks; or\n\n    f) Requiring indemnification of licensors and authors of that\n    material by anyone who conveys the material (or modified versions of\n    it) with contractual assumptions of liability to the recipient, for\n    any liability that these contractual assumptions directly impose on\n    those licensors and authors.\n\n  All other non-permissive additional terms are considered \"further\nrestrictions\" within the meaning of section 10.  If the Program as you\nreceived it, or any part of it, contains a notice stating that it is\ngoverned by this License along with a term that is a further\nrestriction, you may remove that term.  If a license document contains\na further restriction but permits relicensing or conveying under this\nLicense, you may add to a covered work material governed by the terms\nof that license document, provided that the further restriction does\nnot survive such relicensing or conveying.\n\n  If you add terms to a covered work in accord with this section, you\nmust place, in the relevant source files, a statement of the\nadditional terms that apply to those files, or a notice indicating\nwhere to find the applicable terms.\n\n  Additional terms, permissive or non-permissive, may be stated in the\nform of a separately written license, or stated as exceptions;\nthe above requirements apply either way.\n\n  8. Termination.\n\n  You may not propagate or modify a covered work except as expressly\nprovided under this License.  Any attempt otherwise to propagate or\nmodify it is void, and will automatically terminate your rights under\nthis License (including any patent licenses granted under the third\nparagraph of section 11).\n\n  However, if you cease all violation of this License, then your\nlicense from a particular copyright holder is reinstated (a)\nprovisionally, unless and until the copyright holder explicitly and\nfinally terminates your license, and (b) permanently, if the copyright\nholder fails to notify you of the violation by some reasonable means\nprior to 60 days after the cessation.\n\n  Moreover, your license from a particular copyright holder is\nreinstated permanently if the copyright holder notifies you of the\nviolation by some reasonable means, this is the first time you have\nreceived notice of violation of this License (for any work) from that\ncopyright holder, and you cure the violation prior to 30 days after\nyour receipt of the notice.\n\n  Termination of your rights under this section does not terminate the\nlicenses of parties who have received copies or rights from you under\nthis License.  If your rights have been terminated and not permanently\nreinstated, you do not qualify to receive new licenses for the same\nmaterial under section 10.\n\n  9. Acceptance Not Required for Having Copies.\n\n  You are not required to accept this License in order to receive or\nrun a copy of the Program.  Ancillary propagation of a covered work\noccurring solely as a consequence of using peer-to-peer transmission\nto receive a copy likewise does not require acceptance.  However,\nnothing other than this License grants you permission to propagate or\nmodify any covered work.  These actions infringe copyright if you do\nnot accept this License.  Therefore, by modifying or propagating a\ncovered work, you indicate your acceptance of this License to do so.\n\n  10. Automatic Licensing of Downstream Recipients.\n\n  Each time you convey a covered work, the recipient automatically\nreceives a license from the original licensors, to run, modify and\npropagate that work, subject to this License.  You are not responsible\nfor enforcing compliance by third parties with this License.\n\n  An \"entity transaction\" is a transaction transferring control of an\norganization, or substantially all assets of one, or subdividing an\norganization, or merging organizations.  If propagation of a covered\nwork results from an entity transaction, each party to that\ntransaction who receives a copy of the work also receives whatever\nlicenses to the work the party's predecessor in interest had or could\ngive under the previous paragraph, plus a right to possession of the\nCorresponding Source of the work from the predecessor in interest, if\nthe predecessor has it or can get it with reasonable efforts.\n\n  You may not impose any further restrictions on the exercise of the\nrights granted or affirmed under this License.  For example, you may\nnot impose a license fee, royalty, or other charge for exercise of\nrights granted under this License, and you may not initiate litigation\n(including a cross-claim or counterclaim in a lawsuit) alleging that\nany patent claim is infringed by making, using, selling, offering for\nsale, or importing the Program or any portion of it.\n\n  11. Patents.\n\n  A \"contributor\" is a copyright holder who authorizes use under this\nLicense of the Program or a work on which the Program is based.  The\nwork thus licensed is called the contributor's \"contributor version\".\n\n  A contributor's \"essential patent claims\" are all patent claims\nowned or controlled by the contributor, whether already acquired or\nhereafter acquired, that would be infringed by some manner, permitted\nby this License, of making, using, or selling its contributor version,\nbut do not include claims that would be infringed only as a\nconsequence of further modification of the contributor version.  For\npurposes of this definition, \"control\" includes the right to grant\npatent sublicenses in a manner consistent with the requirements of\nthis License.\n\n  Each contributor grants you a non-exclusive, worldwide, royalty-free\npatent license under the contributor's essential patent claims, to\nmake, use, sell, offer for sale, import and otherwise run, modify and\npropagate the contents of its contributor version.\n\n  In the following three paragraphs, a \"patent license\" is any express\nagreement or commitment, however denominated, not to enforce a patent\n(such as an express permission to practice a patent or covenant not to\nsue for patent infringement).  To \"grant\" such a patent license to a\nparty means to make such an agreement or commitment not to enforce a\npatent against the party.\n\n  If you convey a covered work, knowingly relying on a patent license,\nand the Corresponding Source of the work is not available for anyone\nto copy, free of charge and under the terms of this License, through a\npublicly available network server or other readily accessible means,\nthen you must either (1) cause the Corresponding Source to be so\navailable, or (2) arrange to deprive yourself of the benefit of the\npatent license for this particular work, or (3) arrange, in a manner\nconsistent with the requirements of this License, to extend the patent\nlicense to downstream recipients.  \"Knowingly relying\" means you have\nactual knowledge that, but for the patent license, your conveying the\ncovered work in a country, or your recipient's use of the covered work\nin a country, would infringe one or more identifiable patents in that\ncountry that you have reason to believe are valid.\n\n  If, pursuant to or in connection with a single transaction or\narrangement, you convey, or propagate by procuring conveyance of, a\ncovered work, and grant a patent license to some of the parties\nreceiving the covered work authorizing them to use, propagate, modify\nor convey a specific copy of the covered work, then the patent license\nyou grant is automatically extended to all recipients of the covered\nwork and works based on it.\n\n  A patent license is \"discriminatory\" if it does not include within\nthe scope of its coverage, prohibits the exercise of, or is\nconditioned on the non-exercise of one or more of the rights that are\nspecifically granted under this License.  You may not convey a covered\nwork if you are a party to an arrangement with a third party that is\nin the business of distributing software, under which you make payment\nto the third party based on the extent of your activity of conveying\nthe work, and under which the third party grants, to any of the\nparties who would receive the covered work from you, a discriminatory\npatent license (a) in connection with copies of the covered work\nconveyed by you (or copies made from those copies), or (b) primarily\nfor and in connection with specific products or compilations that\ncontain the covered work, unless you entered into that arrangement,\nor that patent license was granted, prior to 28 March 2007.\n\n  Nothing in this License shall be construed as excluding or limiting\nany implied license or other defenses to infringement that may\notherwise be available to you under applicable patent law.\n\n  12. No Surrender of Others' Freedom.\n\n  If conditions are imposed on you (whether by court order, agreement or\notherwise) that contradict the conditions of this License, they do not\nexcuse you from the conditions of this License.  If you cannot convey a\ncovered work so as to satisfy simultaneously your obligations under this\nLicense and any other pertinent obligations, then as a consequence you may\nnot convey it at all.  For example, if you agree to terms that obligate you\nto collect a royalty for further conveying from those to whom you convey\nthe Program, the only way you could satisfy both those terms and this\nLicense would be to refrain entirely from conveying the Program.\n\n  13. Use with the GNU Affero General Public License.\n\n  Notwithstanding any other provision of this License, you have\npermission to link or combine any covered work with a work licensed\nunder version 3 of the GNU Affero General Public License into a single\ncombined work, and to convey the resulting work.  The terms of this\nLicense will continue to apply to the part which is the covered work,\nbut the special requirements of the GNU Affero General Public License,\nsection 13, concerning interaction through a network will apply to the\ncombination as such.\n\n  14. Revised Versions of this License.\n\n  The Free Software Foundation may publish revised and/or new versions of\nthe GNU General Public License from time to time.  Such new versions will\nbe similar in spirit to the present version, but may differ in detail to\naddress new problems or concerns.\n\n  Each version is given a distinguishing version number.  If the\nProgram specifies that a certain numbered version of the GNU General\nPublic License \"or any later version\" applies to it, you have the\noption of following the terms and conditions either of that numbered\nversion or of any later version published by the Free Software\nFoundation.  If the Program does not specify a version number of the\nGNU General Public License, you may choose any version ever published\nby the Free Software Foundation.\n\n  If the Program specifies that a proxy can decide which future\nversions of the GNU General Public License can be used, that proxy's\npublic statement of acceptance of a version permanently authorizes you\nto choose that version for the Program.\n\n  Later license versions may give you additional or different\npermissions.  However, no additional obligations are imposed on any\nauthor or copyright holder as a result of your choosing to follow a\nlater version.\n\n  15. Disclaimer of Warranty.\n\n  THERE IS NO WARRANTY FOR THE PROGRAM, TO THE EXTENT PERMITTED BY\nAPPLICABLE LAW.  EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT\nHOLDERS AND/OR OTHER PARTIES PROVIDE THE PROGRAM \"AS IS\" WITHOUT WARRANTY\nOF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO,\nTHE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR\nPURPOSE.  THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE PROGRAM\nIS WITH YOU.  SHOULD THE PROGRAM PROVE DEFECTIVE, YOU ASSUME THE COST OF\nALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n  16. Limitation of Liability.\n\n  IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING\nWILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MODIFIES AND/OR CONVEYS\nTHE PROGRAM AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY\nGENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE\nUSE OR INABILITY TO USE THE PROGRAM (INCLUDING BUT NOT LIMITED TO LOSS OF\nDATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD\nPARTIES OR A FAILURE OF THE PROGRAM TO OPERATE WITH ANY OTHER PROGRAMS),\nEVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF\nSUCH DAMAGES.\n\n  17. Interpretation of Sections 15 and 16.\n\n  If the disclaimer of warranty and limitation of liability provided\nabove cannot be given local legal effect according to their terms,\nreviewing courts shall apply local law that most closely approximates\nan absolute waiver of all civil liability in connection with the\nProgram, unless a warranty or assumption of liability accompanies a\ncopy of the Program in return for a fee.\n\n                     END OF TERMS AND CONDITIONS\n\n            How to Apply These Terms to Your New Programs\n\n  If you develop a new program, and you want it to be of the greatest\npossible use to the public, the best way to achieve this is to make it\nfree software which everyone can redistribute and change under these terms.\n\n  To do so, attach the following notices to the program.  It is safest\nto attach them to the start of each source file to most effectively\nstate the exclusion of warranty; and each file should have at least\nthe \"copyright\" line and a pointer to where the full notice is found.\n\n    <one line to give the program's name and a brief idea of what it does.>\n    Copyright (C) <year>  <name of author>\n\n    This program is free software: you can redistribute it and/or modify\n    it under the terms of the GNU General Public License as published by\n    the Free Software Foundation, either version 3 of the License, or\n    (at your option) any later version.\n\n    This program is distributed in the hope that it will be useful,\n    but WITHOUT ANY WARRANTY; without even the implied warranty of\n    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n    GNU General Public License for more details.\n\n    You should have received a copy of the GNU General Public License\n    along with this program.  If not, see <https://www.gnu.org/licenses/>.\n\nAlso add information on how to contact you by electronic and paper mail.\n\n  If the program does terminal interaction, make it output a short\nnotice like this when it starts in an interactive mode:\n\n    <program>  Copyright (C) <year>  <name of author>\n    This program comes with ABSOLUTELY NO WARRANTY; for details type `show w'.\n    This is free software, and you are welcome to redistribute it\n    under certain conditions; type `show c' for details.\n\nThe hypothetical commands `show w' and `show c' should show the appropriate\nparts of the General Public License.  Of course, your program's commands\nmight be different; for a GUI interface, you would use an \"about box\".\n\n  You should also get your employer (if you work as a programmer) or school,\nif any, to sign a \"copyright disclaimer\" for the program, if necessary.\nFor more information on this, and how to apply and follow the GNU GPL, see\n<https://www.gnu.org/licenses/>.\n\n  The GNU General Public License does not permit incorporating your program\ninto proprietary programs.  If your program is a subroutine library, you\nmay consider it more useful to permit linking proprietary applications with\nthe library.  If this is what you want to do, use the GNU Lesser General\nPublic License instead of this License.  But first, please read\n<https://www.gnu.org/licenses/why-not-lgpl.html>.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 91.4609375,
          "content": "[![Latest Release](https://img.shields.io/github/release/mhx/dwarfs?label=Latest%20Release)](https://github.com/mhx/dwarfs/releases/latest)\n[![Total Downloads](https://img.shields.io/github/downloads/mhx/dwarfs/total.svg?&color=E95420&label=Total%20Downloads)](https://github.com/mhx/dwarfs/releases)\n[![Homebrew Downloads](https://img.shields.io/homebrew/installs/dm/dwarfs?label=Homebrew)](https://formulae.brew.sh/formula/dwarfs)\n[![DwarFS CI Build](https://github.com/mhx/dwarfs/actions/workflows/build.yml/badge.svg)](https://github.com/mhx/dwarfs/actions/workflows/build.yml)\n[![Codacy Badge](https://app.codacy.com/project/badge/Grade/53489f77755248c999e380500267e889)](https://app.codacy.com/gh/mhx/dwarfs/dashboard)\n[![codecov](https://codecov.io/github/mhx/dwarfs/graph/badge.svg?token=BKR4A3XKA9)](https://codecov.io/github/mhx/dwarfs)\n[![OpenSSF Best Practices](https://www.bestpractices.dev/projects/8663/badge)](https://www.bestpractices.dev/projects/8663)\n\n# DwarFS\n\nThe **D**eduplicating **W**arp-speed **A**dvanced **R**ead-only **F**ile **S**ystem.\n\nA fast high compression read-only file system for Linux and Windows.\n\n## Table of contents\n\n- [Overview](#overview)\n- [History](#history)\n- [Building and Installing](#building-and-installing)\n  - [Note to Package Maintainers](#note-to-package-maintainers)\n  - [Prebuilt Binaries](#prebuilt-binaries)\n  - [Universal Binaries](#universal-binaries)\n  - [Dependencies](#dependencies)\n  - [Building](#building)\n  - [Installing](#installing)\n  - [Static Builds](#static-builds)\n- [Usage](#usage)\n- [Using the Libraries](#using-the-libraries)\n- [Windows Support](#windows-support)\n  - [Building on Windows](#building-on-windows)\n- [macOS Support](#macos-support)\n  - [Building on macOS](#building-on-macos)\n- [Use Cases](#use-cases)\n  - [Astrophotography](#astrophotography)\n- [Dealing with Bit Rot](#dealing-with-bit-rot)\n- [Extended Attributes](#extended-attributes)\n- [Comparison](#comparison)\n  - [With SquashFS](#with-squashfs)\n  - [With SquashFS &amp; xz](#with-squashfs--xz)\n  - [With lrzip](#with-lrzip)\n  - [With zpaq](#with-zpaq)\n  - [With zpaqfranz](#with-zpaqfranz)\n  - [With wimlib](#with-wimlib)\n  - [With Cromfs](#with-cromfs)\n  - [With EROFS](#with-erofs)\n  - [With fuse-archive](#with-fuse-archive)\n- [Performance Monitoring](#performance-monitoring)\n- [Other Obscure Features](#other-obscure-features)\n- [Stargazers over Time](#stargazers-over-time)\n\n## Overview\n\n![Windows Screen Capture](doc/windows.gif?raw=true \"DwarFS Windows\")\n\n![Linux Screen Capture](doc/screenshot.gif?raw=true \"DwarFS Linux\")\n\nDwarFS is a read-only file system with a focus on achieving **very\nhigh compression ratios** in particular for very redundant data.\n\nThis probably doesn't sound very exciting, because if it's redundant,\nit *should* compress well. However, I found that other read-only,\ncompressed file systems don't do a very good job at making use of\nthis redundancy. See [here](#comparison) for a comparison with other\ncompressed file systems.\n\nDwarFS also **doesn't compromise on speed** and for my use cases I've\nfound it to be on par with or perform better than SquashFS. For my\nprimary use case, **DwarFS compression is an order of magnitude better\nthan SquashFS compression**, it's **6 times faster to build the file\nsystem**, it's typically faster to access files on DwarFS and it uses\nless CPU resources.\n\nTo give you an idea of what DwarFS is capable of, here's a quick comparison\nof DwarFS and SquashFS on a set of video files with a total size of 39 GiB.\nThe twist is that each unique video file has two sibling files with a\ndifferent set of audio streams (this is\n[an actual use case](https://github.com/mhx/dwarfs/discussions/63)).\nSo there's redundancy in both the video and audio data, but as the streams\nare interleaved and identical blocks are typically very far apart, it's\nchallenging to make use of that redundancy for compression. SquashFS\nessentially fails to compress the source data at all, whereas DwarFS is\nable to reduce the size by almost a factor of 3, which is close to the\ntheoretical maximum:\n\n```\n$ du -hs dwarfs-video-test\n39G     dwarfs-video-test\n$ ls -lh dwarfs-video-test.*fs\n-rw-r--r-- 1 mhx users 14G Jul  2 13:01 dwarfs-video-test.dwarfs\n-rw-r--r-- 1 mhx users 39G Jul 12 09:41 dwarfs-video-test.squashfs\n```\n\nFurthermore, when mounting the SquashFS image and performing a random-read\nthroughput test using [fio](https://github.com/axboe/fio/)-3.34, both\n`squashfuse` and `squashfuse_ll` top out at around 230 MiB/s:\n\n```\n$ fio --readonly --rw=randread --name=randread --bs=64k --direct=1 \\\n      --opendir=mnt --numjobs=4 --ioengine=libaio --iodepth=32 \\\n      --group_reporting --runtime=60 --time_based\n[...]\n   READ: bw=230MiB/s (241MB/s), 230MiB/s-230MiB/s (241MB/s-241MB/s), io=13.5GiB (14.5GB), run=60004-60004msec\n```\n\nIn comparison, DwarFS manages to sustain **random read rates of 20 GiB/s**:\n\n```\n  READ: bw=20.2GiB/s (21.7GB/s), 20.2GiB/s-20.2GiB/s (21.7GB/s-21.7GB/s), io=1212GiB (1301GB), run=60001-60001msec\n```\n\nDistinct features of DwarFS are:\n\n- Clustering of files by similarity using a similarity hash function.\n  This makes it easier to exploit the redundancy across file boundaries.\n\n- Segmentation analysis across file system blocks in order to reduce\n  the size of the uncompressed file system. This saves memory when\n  using the compressed file system and thus potentially allows for\n  higher cache hit rates as more data can be kept in the cache.\n\n- [Categorization framework](doc/mkdwarfs.md#categorizers) to categorize\n  files or even fragments of files and then process individual categories\n  differently. For example, this allows you to not waste time trying to\n  compress incompressible files or to compress PCM audio data using FLAC\n  compression.\n\n- Highly multi-threaded implementation. Both the\n  [file system creation tool](doc/mkdwarfs.md) as well as the\n  [FUSE driver](doc/dwarfs.md) are able to make good use of the\n  many cores of your system.\n\n## History\n\nI started working on DwarFS in 2013 and my main use case and major\nmotivation was that I had several hundred different versions of Perl\nthat were taking up something around 30 gigabytes of disk space, and\nI was unwilling to spend more than 10% of my hard drive keeping them\naround for when I happened to need them.\n\nUp until then, I had been using [Cromfs](https://bisqwit.iki.fi/source/cromfs.html)\nfor squeezing them into a manageable size. However, I was getting more\nand more annoyed by the time it took to build the filesystem image\nand, to make things worse, more often than not it was crashing after\nabout an hour or so.\n\nI had obviously also looked into [SquashFS](https://en.wikipedia.org/wiki/SquashFS),\nbut never got anywhere close to the compression rates of Cromfs.\n\nThis alone wouldn't have been enough to get me into writing DwarFS,\nbut at around the same time, I was pretty obsessed with the recent\ndevelopments and features of newer C++ standards and really wanted\na C++ hobby project to work on. Also, I've wanted to do something\nwith [FUSE](https://en.wikipedia.org/wiki/Filesystem_in_Userspace)\nfor quite some time. Last but not least, I had been thinking about\nthe problem of compressed file systems for a bit and had some ideas\nthat I definitely wanted to try.\n\nThe majority of the code was written in 2013, then I did a couple\nof cleanups, bugfixes and refactors every once in a while, but I\nnever really got it to a state where I would feel happy releasing\nit. It was too awkward to build with its dependency on Facebook's\n(quite awesome) [folly](https://github.com/facebook/folly) library\nand it didn't have any documentation.\n\nDigging out the project again this year, things didn't look as grim\nas they used to. Folly now builds with CMake and so I just pulled\nit in as a submodule. Most other dependencies can be satisfied\nfrom packages that should be widely available. And I've written\nsome rudimentary docs as well.\n\n## Building and Installing\n\n### Note to Package Maintainers\n\nDwarFS should usually build fine with minimal changes out of the box.\nIf it doesn't, please file a issue. I've set up\n[CI jobs](https://github.com/mhx/dwarfs/actions/workflows/build.yml)\nusing Docker images for Ubuntu ([22.04](https://github.com/mhx/dwarfs/blob/main/.docker/Dockerfile.ubuntu-2204)\nand [24.04](https://github.com/mhx/dwarfs/blob/main/.docker/Dockerfile.ubuntu)),\n[Fedora Rawhide](https://github.com/mhx/dwarfs/blob/main/.docker/Dockerfile.fedora)\nand [Arch](https://github.com/mhx/dwarfs/blob/main/.docker/Dockerfile.arch)\nthat can help with determining an up-to-date set of dependencies.\nNote that building from the release tarball requires less dependencies\nthan building from the git repository, notably the `ronn` tool as well\nas Python and the `mistletoe` Python module are not required when\nbuilding from the release tarball.\n\nThere are some things to be aware of:\n\n- There's a tendency to try and unbundle the [folly](https://github.com/facebook/folly/)\n  and [fbthrift](https://github.com/facebook/fbthrift) libraries that\n  are included as submodules and are built along with DwarFS.\n  While I agree with the sentiment, it's unfortunately a bad idea.\n  Besides the fact that folly does not make any claims about ABI\n  stability (i.e. you can't just dynamically link a binary built\n  against one version of folly against another version), it's not\n  even possible to safely link against a folly library built with\n  different compile options. Even subtle differences, such as the\n  C++ standard version, can cause run-time errors.\n  See [this issue](https://github.com/facebook/folly/pull/1949)\n  for details. Currently, it is not even possible to use external\n  versions of folly/fbthrift as DwarFS is building minimal subsets of\n  both libraries; these are bundled in the `dwarfs_common` library\n  and they are strictly used internally, i.e. none of the folly or\n  fbthrift headers are required to build against DwarFS' libraries.\n\n- Similar issues can arise when using a system-installed version\n  of GoogleTest. GoogleTest itself recommends that it is being\n  downloaded as part of the build. However, you can use the system\n  installed version by passing `-DPREFER_SYSTEM_GTEST=ON` to the\n  `cmake` call. Use at your own risk.\n\n- For other bundled libraries (namely `fmt`, `parallel-hashmap`,\n  `range-v3`), the system installed version is used as long as it\n  meets the minimum required version. Otherwise, the preferred\n  version is fetched during the build.\n\n### Prebuilt Binaries\n\n[Each release](https://github.com/mhx/dwarfs/releases) has pre-built,\nstatically linked binaries for `Linux-x86_64`, `Linux-aarch64` and\n`Windows-AMD64` available for download. These *should* run without\nany dependencies and can be useful especially on older distributions\nwhere you can't easily build the tools from source.\n\n### Universal Binaries\n\nIn addition to the binary tarballs, there's a **universal binary**\navailable for each architecture. These universal binaries contain\n*all* tools (`mkdwarfs`, `dwarfsck`, `dwarfsextract` and the `dwarfs`\nFUSE driver) in a single executable. These executables are compressed\nusing [upx](https://github.com/upx/upx), so they are much smaller than\nthe individual tools combined. However, it also means the binaries need\nto be decompressed each time they are run, which can have a significant\noverhead. If that is an issue, you can either stick to the \"classic\"\nindividual binaries or you can decompress the universal binary, e.g.:\n\n```\nupx -d dwarfs-universal-0.7.0-Linux-aarch64\n```\n\nThe universal binaries can be run through symbolic links named after\nthe proper tool. e.g.:\n\n```\n$ ln -s dwarfs-universal-0.7.0-Linux-aarch64 mkdwarfs\n$ ./mkdwarfs --help\n```\n\nThis also works on Windows if the file system supports symbolic links:\n\n```\n> mklink mkdwarfs.exe dwarfs-universal-0.7.0-Windows-AMD64.exe\n> .\\mkdwarfs.exe --help\n```\n\nAlternatively, you can select the tool by passing `--tool=<name>` as\nthe first argument on the command line:\n\n```\n> .\\dwarfs-universal-0.7.0-Windows-AMD64.exe --tool=mkdwarfs --help\n```\n\nNote that just like the `dwarfs.exe` Windows binary, the universal\nWindows binary depends on the `winfsp-x64.dll` from the\n[WinFsp](https://github.com/winfsp/winfsp) project. However, for the\nuniversal binary, the DLL is loaded lazily, so you can still use all\nother tools without the DLL.\nSee the [Windows Support](#windows-support) section for more details.\n\n### Dependencies\n\nDwarFS uses [CMake](https://cmake.org/) as a build tool.\n\nIt uses both [Boost](https://www.boost.org/) and\n[Folly](https://github.com/facebook/folly), though the latter is\nincluded as a submodule since very few distributions actually\noffer packages for it. Folly itself has a number of dependencies,\nso please check [here](https://github.com/facebook/folly#dependencies)\nfor an up-to-date list.\n\nIt also uses [Facebook Thrift](https://github.com/facebook/fbthrift),\nin particular the `frozen` library, for storing metadata in a highly\nspace-efficient, memory-mappable and well defined format. It's also\nincluded as a submodule, and we only build the compiler and a very\nreduced library that contains just enough for DwarFS to work.\n\nOther than that, DwarFS really only depends on FUSE3 and on a set\nof compression libraries that Folly already depends on (namely\n[lz4](https://github.com/lz4/lz4), [zstd](https://github.com/facebook/zstd)\nand [liblzma](https://github.com/kobolabs/liblzma)).\n\nThe dependency on [googletest](https://github.com/google/googletest)\nwill be automatically resolved if you build with tests.\n\nA good starting point for apt-based systems is probably:\n\n```\n$ apt install \\\n    gcc \\\n    g++ \\\n    clang \\\n    git \\\n    ccache \\\n    ninja-build \\\n    cmake \\\n    make \\\n    bison \\\n    flex \\\n    fuse3 \\\n    pkg-config \\\n    binutils-dev \\\n    libacl1-dev \\\n    libarchive-dev \\\n    libbenchmark-dev \\\n    libboost-chrono-dev \\\n    libboost-context-dev \\\n    libboost-filesystem-dev \\\n    libboost-iostreams-dev \\\n    libboost-program-options-dev \\\n    libboost-regex-dev \\\n    libboost-system-dev \\\n    libboost-thread-dev \\\n    libbrotli-dev \\\n    libevent-dev \\\n    libhowardhinnant-date-dev \\\n    libjemalloc-dev \\\n    libdouble-conversion-dev \\\n    libiberty-dev \\\n    liblz4-dev \\\n    liblzma-dev \\\n    libzstd-dev \\\n    libxxhash-dev \\\n    libmagic-dev \\\n    libparallel-hashmap-dev \\\n    librange-v3-dev \\\n    libssl-dev \\\n    libunwind-dev \\\n    libdwarf-dev \\\n    libelf-dev \\\n    libfmt-dev \\\n    libfuse3-dev \\\n    libgoogle-glog-dev \\\n    libutfcpp-dev \\\n    libflac++-dev \\\n    nlohmann-json3-dev\n```\n\nNote that when building with `gcc`, the optimization level will be\nset to `-O2` instead of the CMake default of `-O3` for release\nbuilds. At least with versions up to `gcc-10`, the `-O3` build is\n[up to 70% slower](https://github.com/mhx/dwarfs/issues/14) than a\nbuild with `-O2`.\n\n### Building\n\nFirst, unpack the release archive:\n\n```\n$ tar xvf dwarfs-x.y.z.tar.xz\n$ cd dwarfs-x.y.z\n```\n\nAlternatively, you can also clone the git repository, but be aware\nthat this has more dependencies and the build will likely take longer\nbecause the release archive ships with most of the auto-generated\nfiles that will have to be generated when building from the repository:\n\n```\n$ git clone --recurse-submodules https://github.com/mhx/dwarfs\n$ cd dwarfs\n```\n\nOnce all dependencies have been installed, you can build DwarFS\nusing:\n\n```\n$ mkdir build\n$ cd build\n$ cmake .. -GNinja -DWITH_TESTS=ON\n$ ninja\n```\n\nYou can then run tests with:\n\n```\n$ ctest -j\n```\n\nAll binaries use [jemalloc](https://github.com/jemalloc/jemalloc)\nas a memory allocator by default, as it is typically uses much less\nsystem memory compared to the `glibc` or `tcmalloc` allocators.\nTo disable the use of `jemalloc`, pass `-DUSE_JEMALLOC=0` on the\n`cmake` command line.\n\nIt is also possible to build/install the DwarFS libraries, tools,\nand FUSE driver independently. This is mostly interesting when\npackaging DwarFS. Note that the tools and FUSE driver require the\nlibraries to be either built or already installed. To build just\nthe libraries, use:\n\n```\n$ cmake .. -GNinja -DWITH_TESTS=ON -DWITH_LIBDWARFS=ON -DWITH_TOOLS=OFF -DWITH_FUSE_DRIVER=OFF\n```\n\nOnce the libraries are tested and installed, you can build the\ntools (i.e. `mkdwarfs`, `dwarfsck`, `dwarfsextract`) using:\n\n```\n$ cmake .. -GNinja -DWITH_TESTS=ON -DWITH_LIBDWARFS=OFF -DWITH_TOOLS=ON -DWITH_FUSE_DRIVER=OFF\n```\n\nTo build the FUSE driver, use:\n\n```\n$ cmake .. -GNinja -DWITH_TESTS=ON -DWITH_LIBDWARFS=OFF -DWITH_TOOLS=OFF -DWITH_FUSE_DRIVER=ON\n```\n\n### Installing\n\nInstalling is as easy as:\n\n```\n$ sudo ninja install\n```\n\nThough you don't have to install the tools to play with them.\n\n### Static Builds\n\nAttempting to build statically linked binaries is highly discouraged\nand not officially supported. That being said, here's how to set up\nan environment where you *might* be able to build static binaries.\n\nThis has been tested with `ubuntu-22.04-live-server-amd64.iso`. First,\ninstall all the packages listed as dependencies above. Also install:\n\n```\n$ apt install ccache ninja libacl1-dev\n```\n\n`ccache` and `ninja` are optional, but help with a speedy compile.\n\nDepending on your distribution, you'll need to build and install static\nversions of some libraries, e.g. `libarchive` and `libmagic` for Ubuntu:\n\n```\n$ wget https://github.com/libarchive/libarchive/releases/download/v3.6.2/libarchive-3.6.2.tar.xz\n$ tar xf libarchive-3.6.2.tar.xz && cd libarchive-3.6.2\n$ ./configure --prefix=/opt/static-libs --without-iconv --without-xml2 --without-expat\n$ make && sudo make install\n```\n\n```\n$ wget ftp://ftp.astron.com/pub/file/file-5.44.tar.gz\n$ tar xf file-5.44.tar.gz && cd file-5.44\n$ ./configure --prefix=/opt/static-libs --enable-static=yes --enable-shared=no\n$ make && make install\n```\n\nThat's it! Now you can try building static binaries for DwarFS:\n\n```\n$ git clone --recurse-submodules https://github.com/mhx/dwarfs\n$ cd dwarfs && mkdir build && cd build\n$ cmake .. -GNinja -DWITH_TESTS=ON -DSTATIC_BUILD_DO_NOT_USE=ON \\\n           -DSTATIC_BUILD_EXTRA_PREFIX=/opt/static-libs\n$ ninja\n$ ninja test\n```\n\n## Usage\n\nPlease check out the manual pages for [mkdwarfs](doc/mkdwarfs.md),\n[dwarfs](doc/dwarfs.md), [dwarfsck](doc/dwarfsck.md) and\n[dwarfsextract](doc/dwarfsextract.md). You can also access the manual\npages using the `--man` option to each binary, e.g.:\n\n```\n$ mkdwarfs --man\n```\n\nThe [dwarfs](doc/dwarfs.md) manual page also shows an example for setting\nup DwarFS with [overlayfs](https://www.kernel.org/doc/Documentation/filesystems/overlayfs.txt)\nin order to create a writable file system mount on top a read-only\nDwarFS image.\n\nA description of the DwarFS filesystem format can be found in\n[dwarfs-format](doc/dwarfs-format.md).\n\nA high-level overview of the internal operation of `mkdwarfs` is shown\nin [this sequence diagram](doc/mkdwarfs-sequence.svg).\n\n## Using the Libraries\n\nUsing the DwarFS libraries should be pretty straightforward if you're\nusing [CMake](https://cmake.org/) to build your project. For a quick\nstart, have a look at the [example code](example/example.cpp) that uses\nthe libraries to print information about a DwarFS image (like `dwarfsck`)\nor extract it (like `dwarfsextract`).\n\nThere are five individual libraries:\n\n- `dwarfs_common` contains the common code required by all the other\n  libraries. The interfaces are defined in [`dwarfs/`](include/dwarfs).\n\n- `dwarfs_reader` contains all code required to read data from a\n  DwarFS image. The interfaces are defined in [`dwarfs/reader/`](include/dwarfs/reader).\n\n- `dwarfs_extractor` contains the ccode required to extract a DwarFS\n  image using [`libarchive`](https://libarchive.org/). The interfaces\n  are defined in [`dwarfs/utility/filesystem_extractor.h`](include/dwarfs/utility/filesystem_extractor.h).\n\n- `dwarfs_writer` contains the code required to create DwarFS images.\n  The interfaces are defined in [`dwarfs/writer/`](include/dwarfs/writer).\n\n- `dwarfs_rewrite` contains the code to re-write DwarFS images. The\n  interfaces are defined in [`dwarfs/utility/rewrite_filesystem.h`](include/dwarfs/utility/rewrite_filesystem.h).\n\nThe headers in `internal` subfolders are only accessible at build\ntime and won't be installed. The same goes for the `tool` subfolder.\n\nThe reader and extractor APIs should be fairly stable. The writer\nAPIs are likely going to change. Note, however, that there are no\nguarantees on API stability before this project reaches version 1.0.0.\n\n## Windows Support\n\nSupport for the Windows operating system is currently experimental.\nHaving worked pretty much exclusively in a Unix world for the past two\ndecades, my experience with Windows development is rather limited and\nI'd expect there to definitely be bugs and rough edges in the Windows\ncode.\n\nThe Windows version of the DwarFS filesystem driver relies on the awesome\n[WinFsp](https://github.com/winfsp/winfsp) project and its `winfsp-x64.dll`\nmust be discoverable by the `dwarfs.exe` driver.\n\nThe different tools should behave pretty much the same whether you're\nusing them on Linux or Windows. The file system images can be copied\nbetween Linux and Windows and images created on one OS should work fine\non the other.\n\nThere are a few things worth pointing out, though:\n\n- DwarFS supports both hardlinks and symlinks on Windows, just as it\n  does on Linux. However, creating hardlinks and symlinks seems to\n  require admin privileges on Windows, so if you want to e.g. extract\n  a DwarFS image that contains links of some sort, you might run into\n  errors if you don't have the right privileges.\n\n- Due to a [problem](https://github.com/winfsp/winfsp/issues/454) in\n  WinFsp, symlinks cannot currently point outside of the mounted file\n  system.  Furthermore, due to another\n  [problem](https://github.com/winfsp/winfsp/issues/530) in WinFsp,\n  symlinks with a drive letter will appear with a mangled target path.\n\n- The DwarFS driver on Windows correctly reports hardlink counts via\n  its API, but currently these counts are not correctly propagated\n  to the Windows file system layer. This is presumably due to a\n  [problem](https://github.com/winfsp/winfsp/issues/511) in WinFsp.\n\n- When mounting a DwarFS image on Windows, the mount point must not\n  exist. This is different from Linux, where the mount point must\n  actually exist. Also, it's possible to mount a DwarFS image as a\n  drive letter, e.g.\n\n    dwarfs.exe image.dwarfs Z:\n\n- Filter rules for `mkdwarfs` always require Unix path separators,\n  regardless of whether it's running on Windows or Linux.\n\n### Building on Windows\n\nBuilding on Windows is not too complicated thanks to [vcpkg](https://vcpkg.io/).\nYou'll need to install:\n\n- [Visual Studio and the MSVC C/C++ compiler](https://visualstudio.microsoft.com/vs/features/cplusplus/)\n\n- [Git](https://git-scm.com/download/win)\n\n- [CMake](https://cmake.org/download/)\n\n- [Ninja](https://github.com/ninja-build/ninja/releases)\n\n- [WinFsp](https://github.com/winfsp/winfsp/releases)\n\n`WinFsp` is expected to be installed in `C:\\Program Files (x86)\\WinFsp`;\nif it's not, you'll need to set `WINFSP_PATH` when running CMake via\n`cmake/win.bat`.\n\nNow you need to clone `vcpkg` and `dwarfs`:\n\n```\n> cd %HOMEPATH%\n> mkdir git\n> cd git\n> git clone https://github.com/Microsoft/vcpkg.git\n> git clone https://github.com/mhx/dwarfs\n```\n\nThen, bootstrap `vcpkg`:\n\n```\n> .\\vcpkg\\bootstrap-vcpkg.bat\n```\n\nAnd build DwarFS:\n\n```\n> cd dwarfs\n> mkdir build\n> cd build\n> ..\\cmake\\win.bat\n> ninja\n```\n\nOnce that's done, you should be able to run the tests.\nSet `CTEST_PARALLEL_LEVEL` according to the number of CPU cores in\nyour machine.\n\n```\n> set CTEST_PARALLEL_LEVEL=10\n> ninja test\n```\n\n## macOS Support\n\nThe DwarFS libraries and tools (`mkdwarfs`, `dwarfsck`, `dwarfsextract`)\nare now available from [Homebrew](https://brew.sh/):\n\n```\n$ brew install dwarfs\n$ brew test dwarfs\n```\n\nThe macOS version of the DwarFS filesystem driver relies on the awesome\n[macFUSE](https://osxfuse.github.io/) project. Until a formula has been\nadded, you will have to build the DwarFS FUSE driver manually.\n\n### Building on macOS\n\nBuilding on macOS should be relatively straightforward:\n\n- Install [Homebrew](https://brew.sh/)\n\n- Use Homebrew to install the necessary dependencies:\n\n```\n$ brew install cmake ninja macfuse brotli howard-hinnant-date double-conversion \\\n               fmt glog libarchive libevent flac openssl nlohmann-json pkg-config \\\n               range-v3 utf8cpp xxhash boost zstd\n```\n\n- When installing macFUSE for the first time, you'll need to explicitly\n  allow the software in *System Preferences* / *Privacy & Security*. It's\n  quite likely that you'll have to reboot after this.\n\n- Download a release tarball from the [releases page](https://github.com/mhx/dwarfs/releases)\n  and extract it:\n\n```\n$ wget https://github.com/mhx/dwarfs/releases/download/v0.10.0/dwarfs-0.10.0.tar.xz\n$ tar xf dwarfs-0.10.0.tar.xz\n```\n\n- Build DwarFS and run its tests:\n\n```\n$ cmake --fresh -B dwarfs-build -S dwarfs-0.10.0 -GNinja -DWITH_TESTS=ON\n$ cmake --build dwarfs-build\n$ ctest --test-dir dwarfs-build -j\n```\n\n- If you don't need the FUSE driver, you can omit `macfuse` from the `brew install`\n  and use the following instead of the first `cmake` command above:\n\n```\n$ cmake --fresh -B dwarfs-build -S dwarfs-0.10.0 -GNinja -DWITH_TESTS=ON -DWITH_FUSE_DRIVER=OFF\n```\n\n- To *only* build the FUSE driver, you can use this instead:\n\n```\n$ cmake --fresh -B dwarfs-build -S dwarfs-0.10.0 -GNinja -DWITH_TESTS=ON -DWITH_LIBDWARFS=OFF -DWITH_TOOLS=OFF\n```\n\n- Install DwarFS:\n\n```\n$ sudo cmake --install dwarfs-build\n```\n\nThat's it!\n\n## Use Cases\n\n### Astrophotography\n\nAstrophotography can generate huge amounts of raw image data. During a\nsingle night, it's not unlikely to end up with a few dozens of gigabytes\nof data. With most dedicated astrophotography cameras, this data ends up\nin the form of FITS images. These are usually uncompressed, don't compress\nvery well with standard compression algorithms, and while there are certain\ncompressed FITS formats, these aren't widely supported.\n\nOne of the compression formats (simply called \"Rice\") compresses reasonably\nwell and is really fast. However, its implementation for compressed FITS\nhas a few drawbacks. The most severe drawbacks are that compression isn't\nquite as good as it could be for color sensors and sensors with a less than\n16 bits of resolution.\n\nDwarFS supports the `ricepp` (Rice++) compression, which builds on the basic\nidea of Rice compression, but makes a few enhancements: it compresses color\nand low bit depth images significantly better and always searches for the\noptimum solution during compression instead of relying on a heuristic.\n\nLet's look at an example using 129 images (darks, flats and lights) taken\nwith an ASI1600MM camera. Each image is 32 MiB, so a total of 4 GiB of data.\nCompressing these with the standard `fpack` tool takes about 16.6 seconds\nand yields a total output size of 2.2 GiB:\n\n```\n$ time fpack */*.fit */*/*.fit\n\nuser\t14.992\nsystem\t1.592\ntotal\t16.616\n\n$ find . -name '*.fz' -print0 | xargs -0 cat | wc -c\n2369943360\n```\n\nHowever, this leaves you with `*.fz` files that not every application can\nactually read.\n\nUsing DwarFS, here's what we get:\n\n```\n$ mkdwarfs -i ASI1600 -o asi1600-20.dwarfs -S 20 --categorize\nI 08:47:47.459077 scanning \"ASI1600\"\nI 08:47:47.491492 assigning directory and link inodes...\nI 08:47:47.491560 waiting for background scanners...\nI 08:47:47.675241 scanning CPU time: 1.051s\nI 08:47:47.675271 finalizing file inodes...\nI 08:47:47.675330 saved 0 B / 3.941 GiB in 0/258 duplicate files\nI 08:47:47.675360 assigning device inodes...\nI 08:47:47.675371 assigning pipe/socket inodes...\nI 08:47:47.675381 building metadata...\nI 08:47:47.675393 building blocks...\nI 08:47:47.675398 saving names and symlinks...\nI 08:47:47.675514 updating name and link indices...\nI 08:47:47.675796 waiting for segmenting/blockifying to finish...\nI 08:47:50.274285 total ordering CPU time: 616.3us\nI 08:47:50.274329 total segmenting CPU time: 1.132s\nI 08:47:50.279476 saving chunks...\nI 08:47:50.279622 saving directories...\nI 08:47:50.279674 saving shared files table...\nI 08:47:50.280745 saving names table... [1.047ms]\nI 08:47:50.280768 saving symlinks table... [743ns]\nI 08:47:50.282031 waiting for compression to finish...\nI 08:47:50.823924 compressed 3.941 GiB to 1.201 GiB (ratio=0.304825)\nI 08:47:50.824280 compression CPU time: 17.92s\nI 08:47:50.824316 filesystem created without errors [3.366s]\nâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯\nwaiting for block compression to finish\n5 dirs, 0/0 soft/hard links, 258/258 files, 0 other\noriginal size: 3.941 GiB, hashed: 315.4 KiB (18 files, 0 B/s)\nscanned: 3.941 GiB (258 files, 117.1 GiB/s), categorizing: 0 B/s\nsaved by deduplication: 0 B (0 files), saved by segmenting: 0 B\nfilesystem: 3.941 GiB in 4037 blocks (4550 chunks, 516/516 fragments, 258 inodes)\ncompressed filesystem: 4037 blocks/1.201 GiB written\n```\n\nIn less than 3.4 seconds, it compresses the data down to 1.2 GiB, almost\nhalf the size of the `fpack` output.\n\nIn addition to saving a lot of disk space, this can also be useful when your\ndata is stored on a NAS. Here's a comparison of the same set of data accessed\nover a 1 Gb/s network connection, first using the uncompressed raw data:\n\n```\nfind /mnt/ASI1600 -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress\n4229012160 bytes (4.2 GB, 3.9 GiB) copied, 36.0455 s, 117 MB/s\n```\n\nAnd next using a DwarFS image on the same share:\n\n```\n$ dwarfs /mnt/asi1600-20.dwarfs mnt\n\n$ find mnt -name '*.fit' -print0 | xargs -0 -P4 -n1 cat | dd of=/dev/null status=progress\n4229012160 bytes (4.2 GB, 3.9 GiB) copied, 14.3681 s, 294 MB/s\n```\n\nThat's roughly 2.5 times faster. You can very likely see similar results\nwith slow external hard drives.\n\n## Dealing with Bit Rot\n\nCurrently, DwarFS has no built-in ability to add recovery information to a\nfile system image. However, for archival purposes, it's a good idea to have\nsuch recovery information in order to be able to repair a damaged image.\n\nThis is fortunately relatively straightforward using something like\n[par2cmdline](https://github.com/Parchive/par2cmdline):\n\n```\n$ par2create -n1 asi1600-20.dwarfs\n```\n\nThis will create two additional files that you can place alongside the image\n(or on a different storage), as you'll only need them if DwarFS has detected\nan issue with the file system image. If there's an issue, you can run\n\n```\n$ par2repair asi1600-20.dwarfs\n```\n\nwhich will very likely be able to recover the image if less than 5% (that's\nthe default used by `par2create`) of the image are damaged.\n\n## Extended Attributes\n\n### Preserving Extended Attributes in DwarFS Images\n\nExtended attributes are not currently supported. Any extended attributes\nstored in the source file system will not currently be preserved when\nbuilding a DwarFS image using `mkdwarfs`.\n\n### Extended Attributes exposed by the FUSE Driver\n\nThat being said, the root inode of a mounted DwarFS image currently exposes\none or two extended attributes on Linux:\n\n```\n$ attr -l mnt\nAttribute \"dwarfs.driver.pid\" has a 4 byte value for mnt\nAttribute \"dwarfs.driver.perfmon\" has a 4849 byte value for mnt\n```\n\nThe `dwarfs.driver.pid` attribute simply contains the PID of the DwarFS\nFUSE driver. The `dwarfs.driver.perfmon` attribute contains the current\nresults of the [performance monitor](#performance-monitoring).\n\nFurthermore, each regular file exposes an attribute `dwarfs.inodeinfo`\nwith information about the underlying inode:\n\n```\n$ attr -l \"05 Disappear.caf\"\nAttribute \"dwarfs.inodeinfo\" has a 448 byte value for 05 Disappear.caf\n```\n\nThe attribute contains a JSON object with information about the\nunderlying inode:\n\n```\n$ attr -qg dwarfs.inodeinfo \"05 Disappear.caf\"\n{\n  \"chunks\": [\n    {\n      \"block\": 2,\n      \"category\": \"pcmaudio/metadata\",\n      \"offset\": 270976,\n      \"size\": 4096\n    },\n    {\n      \"block\": 414,\n      \"category\": \"pcmaudio/waveform\",\n      \"offset\": 37594368,\n      \"size\": 29514492\n    },\n    {\n      \"block\": 419,\n      \"category\": \"pcmaudio/waveform\",\n      \"offset\": 0,\n      \"size\": 29385468\n    }\n  ],\n  \"gid\": 100,\n  \"mode\": 33188,\n  \"modestring\": \"----rw-r--r--\",\n  \"uid\": 1000\n}\n```\n\nThis is useful, for example, to check how a particular file is spread\nacross multiple blocks or which categories have been assigned to the\nfile.\n\n## Comparison\n\nThe SquashFS, `xz`, `lrzip`, `zpaq` and `wimlib` tests were all done on\nan 8 core Intel(R) Xeon(R) E-2286M CPU @ 2.40GHz with 64 GiB of RAM.\n\nThe Cromfs tests were done with an older version of DwarFS\non a 6 core Intel(R) Xeon(R) CPU D-1528 @ 1.90GHz with 64 GiB of RAM.\n\nThe EROFS tests were done using DwarFS v0.9.8 and EROFS v1.7.1 on an\nIntel(R) Core(TM) i9-13900K with 64 GiB of RAM.\n\nThe systems were mostly idle during all of the tests.\n\n### With SquashFS\n\nThe source directory contained **1139 different Perl installations**\nfrom 284 distinct releases, a total of 47.65 GiB of data in 1,927,501\nfiles and 330,733 directories. The source directory was freshly\nunpacked from a tar archive to an XFS partition on a 970 EVO Plus 2TB\nNVME drive, so most of its contents were likely cached.\n\nI'm using the same compression type and compression level for\nSquashFS that is the default setting for DwarFS:\n\n```\n$ time mksquashfs install perl-install.squashfs -comp zstd -Xcompression-level 22\nParallel mksquashfs: Using 16 processors\nCreating 4.0 filesystem on perl-install-zstd.squashfs, block size 131072.\n[=========================================================/] 2107401/2107401 100%\n\nExportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072\n        compressed data, compressed metadata, compressed fragments,\n        compressed xattrs, compressed ids\n        duplicates are removed\nFilesystem size 4637597.63 Kbytes (4528.90 Mbytes)\n        9.29% of uncompressed filesystem size (49922299.04 Kbytes)\nInode table size 19100802 bytes (18653.13 Kbytes)\n        26.06% of uncompressed inode table size (73307702 bytes)\nDirectory table size 19128340 bytes (18680.02 Kbytes)\n        46.28% of uncompressed directory table size (41335540 bytes)\nNumber of duplicate files found 1780387\nNumber of inodes 2255794\nNumber of files 1925061\nNumber of fragments 28713\nNumber of symbolic links  0\nNumber of device nodes 0\nNumber of fifo nodes 0\nNumber of socket nodes 0\nNumber of directories 330733\nNumber of ids (unique uids + gids) 2\nNumber of uids 1\n        mhx (1000)\nNumber of gids 1\n        users (100)\n\nreal    32m54.713s\nuser    501m46.382s\nsys     0m58.528s\n```\n\nFor DwarFS, I'm sticking to the defaults:\n\n```\n$ time mkdwarfs -i install -o perl-install.dwarfs\nI 11:33:33.310931 scanning install\nI 11:33:39.026712 waiting for background scanners...\nI 11:33:50.681305 assigning directory and link inodes...\nI 11:33:50.888441 finding duplicate files...\nI 11:34:01.120800 saved 28.2 GiB / 47.65 GiB in 1782826/1927501 duplicate files\nI 11:34:01.122608 waiting for inode scanners...\nI 11:34:12.839065 assigning device inodes...\nI 11:34:12.875520 assigning pipe/socket inodes...\nI 11:34:12.910431 building metadata...\nI 11:34:12.910524 building blocks...\nI 11:34:12.910594 saving names and links...\nI 11:34:12.910691 bloom filter size: 32 KiB\nI 11:34:12.910760 ordering 144675 inodes using nilsimsa similarity...\nI 11:34:12.915555 nilsimsa: depth=20000 (1000), limit=255\nI 11:34:13.052525 updating name and link indices...\nI 11:34:13.276233 pre-sorted index (660176 name, 366179 path lookups) [360.6ms]\nI 11:35:44.039375 144675 inodes ordered [91.13s]\nI 11:35:44.041427 waiting for segmenting/blockifying to finish...\nI 11:37:38.823902 bloom filter reject rate: 96.017% (TPR=0.244%, lookups=4740563665)\nI 11:37:38.823963 segmentation matches: good=454708, bad=6819, total=464247\nI 11:37:38.824005 segmentation collisions: L1=0.008%, L2=0.000% [2233254 hashes]\nI 11:37:38.824038 saving chunks...\nI 11:37:38.860939 saving directories...\nI 11:37:41.318747 waiting for compression to finish...\nI 11:38:56.046809 compressed 47.65 GiB to 430.9 MiB (ratio=0.00883101)\nI 11:38:56.304922 filesystem created without errors [323s]\nâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯\nwaiting for block compression to finish\n330733 dirs, 0/2440 soft/hard links, 1927501/1927501 files, 0 other\noriginal size: 47.65 GiB, dedupe: 28.2 GiB (1782826 files), segment: 15.19 GiB\nfilesystem: 4.261 GiB in 273 blocks (319178 chunks, 144675/144675 inodes)\ncompressed filesystem: 273 blocks/430.9 MiB written [depth: 20000]\nââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ100% |\n\nreal    5m23.030s\nuser    78m7.554s\nsys     1m47.968s\n```\n\nSo in this comparison, `mkdwarfs` is **more than 6 times faster** than `mksquashfs`,\nboth in terms of CPU time and wall clock time.\n\n```\n$ ll perl-install.*fs\n-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs\n-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs\n```\n\nIn terms of compression ratio, the **DwarFS file system is more than 10 times\nsmaller than the SquashFS file system**. With DwarFS, the content has been\n**compressed down to less than 0.9% (!) of its original size**. This compression\nratio only considers the data stored in the individual files, not the actual\ndisk space used. On the original XFS file system, according to `du`, the\nsource folder uses 52 GiB, so **the DwarFS image actually only uses 0.8% of\nthe original space**.\n\nHere's another comparison using `lzma` compression instead of `zstd`:\n\n```\n$ time mksquashfs install perl-install-lzma.squashfs -comp lzma\n\nreal    13m42.825s\nuser    205m40.851s\nsys     3m29.088s\n```\n\n```\n$ time mkdwarfs -i install -o perl-install-lzma.dwarfs -l9\n\nreal    3m43.937s\nuser    49m45.295s\nsys     1m44.550s\n```\n\n```\n$ ll perl-install-lzma.*fs\n-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-lzma.dwarfs\n-rw-r--r-- 1 mhx users 3838406656 Mar  3 20:50 perl-install-lzma.squashfs\n```\n\nIt's immediately obvious that the runs are significantly faster and the\nresulting images are significantly smaller. Still, `mkdwarfs` is about\n**4 times faster** and produces and image that's **12 times smaller** than\nthe SquashFS image. The DwarFS image is only 0.6% of the original file size.\n\nSo, why not use `lzma` instead of `zstd` by default? The reason is that `lzma`\nis about an order of magnitude slower to decompress than `zstd`. If you're\nonly accessing data on your compressed filesystem occasionally, this might\nnot be a big deal, but if you use it extensively, `zstd` will result in\nbetter performance.\n\nThe comparisons above are not completely fair. `mksquashfs` by default\nuses a block size of 128KiB, whereas `mkdwarfs` uses 16MiB blocks by default,\nor even 64MiB blocks with `-l9`. When using identical block sizes for both\nfile systems, the difference, quite expectedly, becomes a lot less dramatic:\n\n```\n$ time mksquashfs install perl-install-lzma-1M.squashfs -comp lzma -b 1M\n\nreal    15m43.319s\nuser    139m24.533s\nsys     0m45.132s\n```\n\n```\n$ time mkdwarfs -i install -o perl-install-lzma-1M.dwarfs -l9 -S20 -B3\n\nreal    4m25.973s\nuser    52m15.100s\nsys     7m41.889s\n```\n\n```\n$ ll perl-install*.*fs\n-rw-r--r-- 1 mhx users  935953866 Mar 13 12:12 perl-install-lzma-1M.dwarfs\n-rw-r--r-- 1 mhx users 3407474688 Mar  3 21:54 perl-install-lzma-1M.squashfs\n```\n\nEven this is *still* not entirely fair, as it uses a feature (`-B3`) that allows\nDwarFS to reference file chunks from up to two previous filesystem blocks.\n\nBut the point is that this is really where SquashFS tops out, as it doesn't\nsupport larger block sizes or back-referencing. And as you'll see below, the\nlarger blocks that DwarFS is using by default don't necessarily negatively\nimpact performance.\n\nDwarFS also features an option to recompress an existing file system with\na different compression algorithm. This can be useful as it allows relatively\nfast experimentation with different algorithms and options without requiring\na full rebuild of the file system. For example, recompressing the above file\nsystem with the best possible compression (`-l 9`):\n\n```\n$ time mkdwarfs --recompress -i perl-install.dwarfs -o perl-lzma-re.dwarfs -l9\nI 20:28:03.246534 filesystem rewrittenwithout errors [148.3s]\nâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯\nfilesystem: 4.261 GiB in 273 blocks (0 chunks, 0 inodes)\ncompressed filesystem: 273/273 blocks/372.7 MiB written\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ100% \\\n\nreal    2m28.279s\nuser    37m8.825s\nsys     0m43.256s\n```\n\n```\n$ ll perl-*.dwarfs\n-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs\n-rw-r--r-- 1 mhx users 390845518 Mar  4 20:28 perl-lzma-re.dwarfs\n-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-lzma.dwarfs\n```\n\nNote that while the recompressed filesystem is smaller than the original image,\nit is still a lot bigger than the filesystem we previously build with `-l9`.\nThe reason is that the recompressed image still uses the same block size, and\nthe block size cannot be changed by recompressing.\n\nIn terms of how fast the file system is when using it, a quick test\nI've done is to freshly mount the filesystem created above and run\neach of the 1139 `perl` executables to print their version.\n\n```\n$ hyperfine -c \"umount mnt\" -p \"umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1\" -P procs 5 20 -D 5 \"ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P{procs} sh -c '\\$0 -v >/dev/null'\"\nBenchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P5 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):      1.810 s Â±  0.013 s    [User: 1.847 s, System: 0.623 s]\n  Range (min â¦ max):    1.788 s â¦  1.825 s    10 runs\n\nBenchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P10 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):      1.333 s Â±  0.009 s    [User: 1.993 s, System: 0.656 s]\n  Range (min â¦ max):    1.321 s â¦  1.354 s    10 runs\n\nBenchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P15 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):      1.181 s Â±  0.018 s    [User: 2.086 s, System: 0.712 s]\n  Range (min â¦ max):    1.165 s â¦  1.214 s    10 runs\n\nBenchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):      1.149 s Â±  0.015 s    [User: 2.128 s, System: 0.781 s]\n  Range (min â¦ max):    1.136 s â¦  1.186 s    10 runs\n```\n\nThese timings are for *initial* runs on a freshly mounted file system,\nrunning 5, 10, 15 and 20 processes in parallel. 1.1 seconds means that\nit takes only about 1 millisecond per Perl binary.\n\nFollowing are timings for *subsequent* runs, both on DwarFS (at `mnt`)\nand the original XFS (at `install`). DwarFS is around 15% slower here:\n\n```\n$ hyperfine -P procs 10 20 -D 10 -w1 \"ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P{procs} sh -c '\\$0 -v >/dev/null'\" \"ls -1 install/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P{procs} sh -c '\\$0 -v >/dev/null'\"\nBenchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P10 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):     347.0 ms Â±   7.2 ms    [User: 1.755 s, System: 0.452 s]\n  Range (min â¦ max):   341.3 ms â¦ 365.2 ms    10 runs\n\nBenchmark #2: ls -1 install/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P10 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):     302.5 ms Â±   3.3 ms    [User: 1.656 s, System: 0.377 s]\n  Range (min â¦ max):   297.1 ms â¦ 308.7 ms    10 runs\n\nBenchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):     342.2 ms Â±   4.1 ms    [User: 1.766 s, System: 0.451 s]\n  Range (min â¦ max):   336.0 ms â¦ 349.7 ms    10 runs\n\nBenchmark #4: ls -1 install/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):     302.0 ms Â±   3.0 ms    [User: 1.659 s, System: 0.374 s]\n  Range (min â¦ max):   297.0 ms â¦ 305.4 ms    10 runs\n\nSummary\n  'ls -1 install/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '$0 -v >/dev/null'' ran\n    1.00 Â± 0.01 times faster than 'ls -1 install/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P10 sh -c '$0 -v >/dev/null''\n    1.13 Â± 0.02 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '$0 -v >/dev/null''\n    1.15 Â± 0.03 times faster than 'ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P10 sh -c '$0 -v >/dev/null''\n```\n\nUsing the lzma-compressed file system, the metrics for *initial* runs look\nconsiderably worse (about an order of magnitude):\n\n```\n$ hyperfine -c \"umount mnt\" -p \"umount mnt; dwarfs perl-install-lzma.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1\" -P procs 5 20 -D 5 \"ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P{procs} sh -c '\\$0 -v >/dev/null'\"\nBenchmark #1: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P5 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):     10.660 s Â±  0.057 s    [User: 1.952 s, System: 0.729 s]\n  Range (min â¦ max):   10.615 s â¦ 10.811 s    10 runs\n\nBenchmark #2: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P10 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):      9.092 s Â±  0.021 s    [User: 1.979 s, System: 0.680 s]\n  Range (min â¦ max):    9.059 s â¦  9.126 s    10 runs\n\nBenchmark #3: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P15 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):      9.012 s Â±  0.188 s    [User: 2.077 s, System: 0.702 s]\n  Range (min â¦ max):    8.839 s â¦  9.277 s    10 runs\n\nBenchmark #4: ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '$0 -v >/dev/null'\n  Time (mean Â± Ï):      9.004 s Â±  0.298 s    [User: 2.134 s, System: 0.736 s]\n  Range (min â¦ max):    8.611 s â¦  9.555 s    10 runs\n```\n\nSo you might want to consider using `zstd` instead of `lzma` if you'd\nlike to optimize for file system performance. It's also the default\ncompression used by `mkdwarfs`.\n\nNow here's a comparison with the SquashFS filesystem:\n\n```\n$ hyperfine -c 'sudo umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs-zstd \"ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '\\$0 -v >/dev/null'\" -p 'sudo umount mnt; sudo mount -t squashfs perl-install.squashfs mnt; sleep 1' -n squashfs-zstd \"ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '\\$0 -v >/dev/null'\"\nBenchmark #1: dwarfs-zstd\n  Time (mean Â± Ï):      1.151 s Â±  0.015 s    [User: 2.147 s, System: 0.769 s]\n  Range (min â¦ max):    1.118 s â¦  1.174 s    10 runs\n\nBenchmark #2: squashfs-zstd\n  Time (mean Â± Ï):      6.733 s Â±  0.007 s    [User: 3.188 s, System: 17.015 s]\n  Range (min â¦ max):    6.721 s â¦  6.743 s    10 runs\n\nSummary\n  'dwarfs-zstd' ran\n    5.85 Â± 0.08 times faster than 'squashfs-zstd'\n```\n\nSo, DwarFS is almost six times faster than SquashFS. But what's more,\nSquashFS also uses significantly more CPU power. However, the numbers\nshown above for DwarFS obviously don't include the time spent in the\n`dwarfs` process, so I repeated the test outside of hyperfine:\n\n```\n$ time dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4 -f\n\nreal    0m4.569s\nuser    0m2.154s\nsys     0m1.846s\n```\n\nSo, in total, DwarFS was using 5.7 seconds of CPU time, whereas\nSquashFS was using 20.2 seconds, almost four times as much. Ignore\nthe 'real' time, this is only how long it took me to unmount the\nfile system again after mounting it.\n\nAnother real-life test was to build and test a Perl module with 624\ndifferent Perl versions in the compressed file system. The module I've\nused, [Tie::Hash::Indexed](https://github.com/mhx/Tie-Hash-Indexed),\nhas an XS component that requires a C compiler to build. So this really\naccesses a lot of different stuff in the file system:\n\n- The `perl` executables and its shared libraries\n\n- The Perl modules used for writing the Makefile\n\n- Perl's C header files used for building the module\n\n- More Perl modules used for running the tests\n\nI wrote a little script to be able to run multiple builds in parallel:\n\n```bash\n#!/bin/bash\nset -eu\nperl=$1\ndir=$(echo \"$perl\" | cut -d/ --output-delimiter=- -f5,6)\nrsync -a Tie-Hash-Indexed/ $dir/\ncd $dir\n$1 Makefile.PL >/dev/null 2>&1\nmake test >/dev/null 2>&1\ncd ..\nrm -rf $dir\necho $perl\n```\n\nThe following command will run up to 16 builds in parallel on the 8 core\nXeon CPU, including debug, optimized and threaded versions of all Perl\nreleases between 5.10.0 and 5.33.3, a total of 624 `perl` installations:\n\n```\n$ time ls -1 /tmp/perl/install/*/perl-5.??.?/bin/perl5* | sort -t / -k 8 | xargs -d $'\\n' -P 16 -n 1 ./build.sh\n```\n\nTests were done with a cleanly mounted file system to make sure the caches\nwere empty. `ccache` was primed to make sure all compiler runs could be\nsatisfied from the cache. With SquashFS, the timing was:\n\n```\nreal    0m52.385s\nuser    8m10.333s\nsys     4m10.056s\n```\n\nAnd with DwarFS:\n\n```\nreal    0m50.469s\nuser    9m22.597s\nsys     1m18.469s\n```\n\nSo, frankly, not much of a difference, with DwarFS being just a bit faster.\nThe `dwarfs` process itself used:\n\n```\nreal    0m56.686s\nuser    0m18.857s\nsys     0m21.058s\n```\n\nSo again, DwarFS used less raw CPU power overall, but in terms of wallclock\ntime, the difference is really marginal.\n\n### With SquashFS & xz\n\nThis test uses slightly less pathological input data: the root filesystem of\na recent Raspberry Pi OS release. This file system also contains device inodes,\nso in order to preserve those, we pass `--with-devices` to `mkdwarfs`:\n\n```\n$ time sudo mkdwarfs -i raspbian -o raspbian.dwarfs --with-devices\nI 21:30:29.812562 scanning raspbian\nI 21:30:29.908984 waiting for background scanners...\nI 21:30:30.217446 assigning directory and link inodes...\nI 21:30:30.221941 finding duplicate files...\nI 21:30:30.288099 saved 31.05 MiB / 1007 MiB in 1617/34582 duplicate files\nI 21:30:30.288143 waiting for inode scanners...\nI 21:30:31.393710 assigning device inodes...\nI 21:30:31.394481 assigning pipe/socket inodes...\nI 21:30:31.395196 building metadata...\nI 21:30:31.395230 building blocks...\nI 21:30:31.395291 saving names and links...\nI 21:30:31.395374 ordering 32965 inodes using nilsimsa similarity...\nI 21:30:31.396254 nilsimsa: depth=20000 (1000), limit=255\nI 21:30:31.407967 pre-sorted index (46431 name, 2206 path lookups) [11.66ms]\nI 21:30:31.410089 updating name and link indices...\nI 21:30:38.178505 32965 inodes ordered [6.783s]\nI 21:30:38.179417 waiting for segmenting/blockifying to finish...\nI 21:31:06.248304 saving chunks...\nI 21:31:06.251998 saving directories...\nI 21:31:06.402559 waiting for compression to finish...\nI 21:31:16.425563 compressed 1007 MiB to 287 MiB (ratio=0.285036)\nI 21:31:16.464772 filesystem created without errors [46.65s]\nâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯\nwaiting for block compression to finish\n4435 dirs, 5908/0 soft/hard links, 34582/34582 files, 7 other\noriginal size: 1007 MiB, dedupe: 31.05 MiB (1617 files), segment: 47.23 MiB\nfilesystem: 928.4 MiB in 59 blocks (38890 chunks, 32965/32965 inodes)\ncompressed filesystem: 59 blocks/287 MiB written [depth: 20000]\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ100% |\n\nreal    0m46.711s\nuser    10m39.038s\nsys     0m8.123s\n```\n\nAgain, SquashFS uses the same compression options:\n\n```\n$ time sudo mksquashfs raspbian raspbian.squashfs -comp zstd -Xcompression-level 22\nParallel mksquashfs: Using 16 processors\nCreating 4.0 filesystem on raspbian.squashfs, block size 131072.\n[===============================================================\\] 39232/39232 100%\n\nExportable Squashfs 4.0 filesystem, zstd compressed, data block size 131072\n        compressed data, compressed metadata, compressed fragments,\n        compressed xattrs, compressed ids\n        duplicates are removed\nFilesystem size 371934.50 Kbytes (363.22 Mbytes)\n        35.98% of uncompressed filesystem size (1033650.60 Kbytes)\nInode table size 399913 bytes (390.54 Kbytes)\n        26.53% of uncompressed inode table size (1507581 bytes)\nDirectory table size 408749 bytes (399.17 Kbytes)\n        42.31% of uncompressed directory table size (966174 bytes)\nNumber of duplicate files found 1618\nNumber of inodes 44932\nNumber of files 34582\nNumber of fragments 3290\nNumber of symbolic links  5908\nNumber of device nodes 7\nNumber of fifo nodes 0\nNumber of socket nodes 0\nNumber of directories 4435\nNumber of ids (unique uids + gids) 18\nNumber of uids 5\n        root (0)\n        mhx (1000)\n        unknown (103)\n        shutdown (6)\n        unknown (106)\nNumber of gids 15\n        root (0)\n        unknown (109)\n        unknown (42)\n        unknown (1000)\n        users (100)\n        unknown (43)\n        tty (5)\n        unknown (108)\n        unknown (111)\n        unknown (110)\n        unknown (50)\n        mail (12)\n        nobody (65534)\n        adm (4)\n        mem (8)\n\nreal    0m50.124s\nuser    9m41.708s\nsys     0m1.727s\n```\n\nThe difference in speed is almost negligible. SquashFS is just a bit\nslower here. In terms of compression, the difference also isn't huge:\n\n```\n$ ls -lh raspbian.* *.xz\n-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz\n-rw-r--r-- 1 root root  287M Mar  4 21:31 raspbian.dwarfs\n-rw-r--r-- 1 root root  364M Mar  4 21:33 raspbian.squashfs\n```\n\nInterestingly, `xz` actually can't compress the whole original image\nbetter than DwarFS.\n\nWe can even again try to increase the DwarFS compression level:\n\n```\n$ time sudo mkdwarfs -i raspbian -o raspbian-9.dwarfs --with-devices -l9\n\nreal    0m54.161s\nuser    8m40.109s\nsys     0m7.101s\n```\n\nNow that actually gets the DwarFS image size well below that of the\n`xz` archive:\n\n```\n$ ls -lh raspbian-9.dwarfs *.xz\n-rw-r--r-- 1 root root  244M Mar  4 21:36 raspbian-9.dwarfs\n-rw-r--r-- 1 mhx  users 297M Mar  4 21:32 2020-08-20-raspios-buster-armhf-lite.img.xz\n```\n\nEven if you actually build a tarball and compress that (instead of\ncompressing the EXT4 file system itself), `xz` isn't quite able to\nmatch the DwarFS image size:\n\n```\n$ time sudo tar cf - raspbian | xz -9 -vT 0 >raspbian.tar.xz\n  100 %     246.9 MiB / 1,037.2 MiB = 0.238    13 MiB/s       1:18\n\nreal    1m18.226s\nuser    6m35.381s\nsys     0m2.205s\n```\n\n```\n$ ls -lh raspbian.tar.xz\n-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz\n```\n\nDwarFS also comes with the [dwarfsextract](doc/dwarfsextract.md) tool\nthat allows extraction of a filesystem image without the FUSE driver.\nSo here's a comparison of the extraction speed:\n\n```\n$ time sudo tar xf raspbian.tar.xz -C out1\n\nreal    0m12.846s\nuser    0m12.313s\nsys     0m1.616s\n```\n\n```\n$ time sudo dwarfsextract -i raspbian-9.dwarfs -o out2\n\nreal    0m3.825s\nuser    0m13.234s\nsys     0m1.382s\n```\n\nSo, `dwarfsextract` is almost 4 times faster thanks to using multiple\nworker threads for decompression. It's writing about 300 MiB/s in this\nexample.\n\nAnother nice feature of `dwarfsextract` is that it allows you to directly\noutput data in an archive format, so you could create a tarball from\nyour image without extracting the files to disk:\n\n```\n$ dwarfsextract -i raspbian-9.dwarfs -f ustar | xz -9 -T0 >raspbian2.tar.xz\n```\n\nThis has the interesting side-effect that the resulting tarball will\nlikely be smaller than the one built straight from the directory:\n\n```\n$ ls -lh raspbian*.tar.xz\n-rw-r--r-- 1 mhx users 247M Mar  4 21:40 raspbian.tar.xz\n-rw-r--r-- 1 mhx users 240M Mar  4 23:52 raspbian2.tar.xz\n```\n\nThat's because `dwarfsextract` writes files in inode-order, and by\ndefault inodes are ordered by similarity for the best possible\ncompression.\n\n### With lrzip\n\n[lrzip](https://github.com/ckolivas/lrzip) is a compression utility\ntargeted especially at compressing large files. From its description,\nit looks like it does something very similar to DwarFS, i.e. it looks\nfor duplicate segments before passing the de-duplicated data on to\nan `lzma` compressor.\n\nWhen I first read about `lrzip`, I was pretty certain it would easily\nbeat DwarFS. So let's take a look. `lrzip` operates on a single file,\nso it's necessary to first build a tarball:\n\n```\n$ time tar cf perl-install.tar install\n\nreal    2m9.568s\nuser    0m3.757s\nsys     0m26.623s\n```\n\nNow we can run `lrzip`:\n\n```\n$ time lrzip -vL9 -o perl-install.tar.lrzip perl-install.tar\nThe following options are in effect for this COMPRESSION.\nThreading is ENABLED. Number of CPUs detected: 16\nDetected 67106172928 bytes ram\nCompression level 9\nNice Value: 19\nShow Progress\nVerbose\nOutput Filename Specified: perl-install.tar.lrzip\nTemporary Directory set as: ./\nCompression mode is: LZMA. LZO Compressibility testing enabled\nHeuristically Computed Compression Window: 426 = 42600MB\nFile size: 52615639040\nWill take 2 passes\nBeginning rzip pre-processing phase\nBeginning rzip pre-processing phase\nperl-install.tar - Compression Ratio: 100.378. Average Compression Speed: 14.536MB/s.\nTotal time: 00:57:32.47\n\nreal    57m32.472s\nuser    81m44.104s\nsys     4m50.221s\n```\n\nThat definitely took a while. This is about an order of magnitude\nslower than `mkdwarfs` and it barely makes use of the 8 cores.\n\n```\n$ ll -h perl-install.tar.lrzip\n-rw-r--r-- 1 mhx users 500M Mar  6 21:16 perl-install.tar.lrzip\n```\n\nThis is a surprisingly disappointing result. The archive is 65% larger\nthan a DwarFS image at `-l9` that takes less than 4 minutes to build.\nAlso, you can't just access the files in the `.lrzip` without fully\nunpacking the archive first.\n\nThat being said, it *is* better than just using `xz` on the tarball:\n\n```\n$ time xz -T0 -v9 -c perl-install.tar >perl-install.tar.xz\nperl-install.tar (1/1)\n  100 %      4,317.0 MiB / 49.0 GiB = 0.086    24 MiB/s      34:55\n\nreal    34m55.450s\nuser    543m50.810s\nsys     0m26.533s\n```\n\n```\n$ ll perl-install.tar.xz -h\n-rw-r--r-- 1 mhx users 4.3G Mar  6 22:59 perl-install.tar.xz\n```\n\n### With zpaq\n\n[zpaq](http://mattmahoney.net/dc/zpaq.html) is a journaling backup\nutility and archiver. Again, it appears to share some of the ideas in\nDwarFS, like segmentation analysis, but it also adds some features on\ntop that make it useful for incremental backups. However, it's also\nnot usable as a file system, so data needs to be extracted before it\ncan be used.\n\nAnyway, how does it fare in terms of speed and compression performance?\n\n```\n$ time zpaq a perl-install.zpaq install -m5\n```\n\nAfter a few million lines of output that (I think) cannot be turned off:\n\n```\n2258234 +added, 0 -removed.\n\n0.000000 + (51161.953159 -> 8932.000297 -> 490.227707) = 490.227707 MB\n2828.082 seconds (all OK)\n\nreal    47m8.104s\nuser    714m44.286s\nsys     3m6.751s\n```\n\nSo, it's an order of magnitude slower than `mkdwarfs` and uses 14 times\nas much CPU resources as `mkdwarfs -l9`. The resulting archive it pretty\nclose in size to the default configuration DwarFS image, but it's more\nthan 50% bigger than the image produced by `mkdwarfs -l9`.\n\n```\n$ ll perl-install*.*\n-rw-r--r-- 1 mhx users 490227707 Mar  7 01:38 perl-install.zpaq\n-rw-r--r-- 1 mhx users 315482627 Mar  3 21:23 perl-install-l9.dwarfs\n-rw-r--r-- 1 mhx users 447230618 Mar  3 20:28 perl-install.dwarfs\n```\n\nWhat's *really* surprising is how slow it is to extract the `zpaq`\narchive again:\n\n```\n$ time zpaq x perl-install.zpaq\n2798.097 seconds (all OK)\n\nreal    46m38.117s\nuser    711m18.734s\nsys     3m47.876s\n```\n\nThat's 700 times slower than extracting the DwarFS image.\n\n### With zpaqfranz\n\n[zpaqfranz](https://github.com/fcorbelli/zpaqfranz) is a derivative of zpaq.\nMuch to my delight, it doesn't generate millions of lines of output.\nIt claims to be multi-threaded and de-duplicating, so definitely worth\ntaking a look. Like zpaq, it supports incremental backups.\n\nWe'll use a different input to compare zpaqfranz and DwarFS: The source code\nof 670 different releases of the \"wine\" emulator. That's 73 gigabytes of data\nin total, spread across slightly more than 3 million files. It's obviously\nhighly redundant and should thus be a good data set to compare the tools.\nFor reference, a `.tar.xz` of the directory is still 7 GiB in size and a\nSquashFS image of the data gets down to around 1.6 GiB. An \"optimized\"\n`.tar.xz`, where the input files were ordered by similarity, compresses down\nto 399 MiB, almost 20 times better than without ordering.\n\nNow it's time to try zpaqfranz. The input data is stored on a fast SSD and a\nlarge fraction of it is already in the file system cache from previous runs,\nso disk I/O is not a bottleneck.\n\n```\n$ time ./zpaqfranz a winesrc.zpaq winesrc\nzpaqfranz v58.8k-JIT-L(2023-08-05)\nCreating winesrc.zpaq at offset 0 + 0\nAdd 2024-01-11 07:25:22 3.117.413     69.632.090.852 (  64.85 GB) 16T (362.904 dirs)\n3.480.317 +added, 0 -removed.\n\n0 + (69.632.090.852 -> 8.347.553.798 -> 617.600.892) = 617.600.892 @ 58.38 MB/s\n\n1137.441 seconds (000:18:57) (all OK)\n\nreal    18m58.632s\nuser    11m51.052s\nsys     1m3.389s\n```\n\nThat is considerably faster than the original zpaq, and uses about 60 times\nless CPU resources. The output file is 589 MiB, so slightly larger than both\nthe \"optimized\" `.tar.gz` and the zpaq output.\n\nHow does `mkdwarfs` do?\n\n```\n$ time mkdwarfs -i winesrc -o winesrc.dwarfs -l9\n[...]\nI 07:55:20.546636 compressed 64.85 GiB to 93.2 MiB (ratio=0.00140344)\nI 07:55:20.826699 compression CPU time: 6.726m\nI 07:55:20.827338 filesystem created without errors [2.283m]\n[...]\n\nreal    2m17.100s\nuser    9m53.633s\nsys     2m29.236s\n```\n\nIt uses pretty much the same amount of CPU resources, but finishes more than\n8 times faster. The DwarFS output file is more than 6 times smaller.\n\nYou can actually squeeze a bit more redundancy out of the original data by\ntweaking the similarity ordering and switching from lzma to brotli compression,\nalbeit at a somewhat slower compression speed:\n\n```\nmkdwarfs -i winesrc -o winesrc.dwarfs -l9 -C brotli:quality=11:lgwin=26 --order=nilsimsa:max-cluster-size=200k\n[...]\nI 08:21:01.138075 compressed 64.85 GiB to 73.52 MiB (ratio=0.00110716)\nI 08:21:01.485737 compression CPU time: 36.58m\nI 08:21:01.486313 filesystem created without errors [5.501m]\n[...]\nreal    5m30.178s\nuser    40m59.193s\nsys     2m36.234s\n```\n\nThat's almost a 1000x reduction in size.\n\nLet's also look at decompression speed:\n\n```\n$ time zpaqfranz x winesrc.zpaq\nzpaqfranz v58.8k-JIT-L(2023-08-05)\n/home/mhx/winesrc.zpaq:\n1 versions, 3.480.317 files, 617.600.892 bytes (588.99 MB)\nExtract 69.632.090.852 bytes (64.85 GB) in 3.117.413 files (362.904 folders) / 16 T\n        99.18% 00:00:00  (  64.32 GB)=>(  64.85 GB)  548.83 MB/sec\n\n125.636 seconds (000:02:05) (all OK)\n\nreal    2m6.968s\nuser    1m36.177s\nsys     1m10.980s\n```\n\n```\n$ time dwarfsextract -i winesrc.dwarfs\n\nreal    1m49.182s\nuser    0m34.667s\nsys     1m28.733s\n```\n\nDecompression time is pretty much in the same ballpark, with just slightly\nshorter times for the DwarFS image.\n\n### With wimlib\n\n[wimlib](https://wimlib.net/) is a really interesting project that is\na lot more mature than DwarFS. While DwarFS at its core has a library\ncomponent that could potentially be ported to other operating systems,\nwimlib already is available on many platforms. It also seems to have\nquite a rich set of features, so it's definitely worth taking a look at.\n\nI first tried `wimcapture` on the perl dataset:\n\n```\n$ time wimcapture --unix-data --solid --solid-chunk-size=16M install perl-install.wim\nScanning \"install\"\n47 GiB scanned (1927501 files, 330733 directories)\nUsing LZMS compression with 16 threads\nArchiving file data: 19 GiB of 19 GiB (100%) done\n\nreal    15m23.310s\nuser    174m29.274s\nsys     0m42.921s\n```\n\n```\n$ ll perl-install.*\n-rw-r--r-- 1 mhx users  447230618 Mar  3 20:28 perl-install.dwarfs\n-rw-r--r-- 1 mhx users  315482627 Mar  3 21:23 perl-install-l9.dwarfs\n-rw-r--r-- 1 mhx users 4748902400 Mar  3 20:10 perl-install.squashfs\n-rw-r--r-- 1 mhx users 1016981520 Mar  6 21:12 perl-install.wim\n```\n\nSo, wimlib is definitely much better than squashfs, in terms of both\ncompression ratio and speed. DwarFS is however about 3 times faster to\ncreate the file system and the DwarFS file system less than half the size.\nWhen switching to LZMA compression, the DwarFS file system is more than\n3 times smaller (wimlib uses LZMS compression by default).\n\nWhat's a bit surprising is that mounting a *wim* file takes quite a bit\nof time:\n\n```\n$ time wimmount perl-install.wim mnt\n[WARNING] Mounting a WIM file containing solid-compressed data; file access may be slow.\n\nreal    0m2.038s\nuser    0m1.764s\nsys     0m0.242s\n```\n\nMounting the DwarFS image takes almost no time in comparison:\n\n```\n$ time git/github/dwarfs/build-clang-11/dwarfs perl-install-default.dwarfs mnt\nI 00:23:39.238182 dwarfs (v0.4.0, fuse version 35)\n\nreal    0m0.003s\nuser    0m0.003s\nsys     0m0.000s\n```\n\nThat's just because it immediately forks into background by default and\ninitializes the file system in the background. However, even when\nrunning it in the foreground, initializing the file system takes only\nabout 60 milliseconds:\n\n```\n$ dwarfs perl-install.dwarfs mnt -f\nI 00:25:03.186005 dwarfs (v0.4.0, fuse version 35)\nI 00:25:03.248061 file system initialized [60.95ms]\n```\n\nIf you actually build the DwarFS file system with uncompressed metadata,\nmounting is basically instantaneous:\n\n```\n$ dwarfs perl-install-meta.dwarfs mnt -f\nI 00:27:52.667026 dwarfs (v0.4.0, fuse version 35)\nI 00:27:52.671066 file system initialized [2.879ms]\n```\n\nI've tried running the benchmark where all 1139 `perl` executables\nprint their version with the wimlib image, but after about 10 minutes,\nit still hadn't finished the first run (with the DwarFS image, one run\ntook slightly more than 2 seconds). I then tried the following instead:\n\n```\n$ ls -1 /tmp/perl/install/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P1 sh -c 'time $0 -v >/dev/null' 2>&1 | grep ^real\nreal    0m0.802s\nreal    0m0.652s\nreal    0m1.677s\nreal    0m1.973s\nreal    0m1.435s\nreal    0m1.879s\nreal    0m2.003s\nreal    0m1.695s\nreal    0m2.343s\nreal    0m1.899s\nreal    0m1.809s\nreal    0m1.790s\nreal    0m2.115s\n```\n\nJudging from that, it would have probably taken about half an hour\nfor a single run, which makes at least the `--solid` wim image pretty\nmuch unusable for actually working with the file system.\n\nThe `--solid` option was suggested to me because it resembles the way\nthat DwarFS actually organizes data internally. However, judging by the\nwarning when mounting a solid image, it's probably not ideal when using\nthe image as a mounted file system. So I tried again without `--solid`:\n\n```\n$ time wimcapture --unix-data install perl-install-nonsolid.wim\nScanning \"install\"\n47 GiB scanned (1927501 files, 330733 directories)\nUsing LZX compression with 16 threads\nArchiving file data: 19 GiB of 19 GiB (100%) done\n\nreal    8m39.034s\nuser    64m58.575s\nsys     0m32.003s\n```\n\nThis is still more than 3 minutes slower than `mkdwarfs`. However, it\nyields an image that's almost 10 times the size of the DwarFS image\nand comparable in size to the SquashFS image:\n\n```\n$ ll perl-install-nonsolid.wim -h\n-rw-r--r-- 1 mhx users 4.6G Mar  6 23:24 perl-install-nonsolid.wim\n```\n\nThis *still* takes surprisingly long to mount:\n\n```\n$ time wimmount perl-install-nonsolid.wim mnt\n\nreal    0m1.603s\nuser    0m1.327s\nsys     0m0.275s\n```\n\nHowever, it's really usable as a file system, even though it's about\n4-5 times slower than the DwarFS image:\n\n```\n$ hyperfine -c 'umount mnt' -p 'umount mnt; dwarfs perl-install.dwarfs mnt -o cachesize=1g -o workers=4; sleep 1' -n dwarfs \"ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '\\$0 -v >/dev/null'\" -p 'umount mnt; wimmount perl-install-nonsolid.wim mnt; sleep 1' -n wimlib \"ls -1 mnt/*/*/bin/perl5* | xargs -d $'\\n' -n1 -P20 sh -c '\\$0 -v >/dev/null'\"\nBenchmark #1: dwarfs\n  Time (mean Â± Ï):      1.149 s Â±  0.019 s    [User: 2.147 s, System: 0.739 s]\n  Range (min â¦ max):    1.122 s â¦  1.187 s    10 runs\n\nBenchmark #2: wimlib\n  Time (mean Â± Ï):      7.542 s Â±  0.069 s    [User: 2.787 s, System: 0.694 s]\n  Range (min â¦ max):    7.490 s â¦  7.732 s    10 runs\n\nSummary\n  'dwarfs' ran\n    6.56 Â± 0.12 times faster than 'wimlib'\n```\n\n### With Cromfs\n\nI used [Cromfs](https://bisqwit.iki.fi/source/cromfs.html) in the past\nfor compressed file systems and remember that it did a pretty good job\nin terms of compression ratio. But it was never fast. However, I didn't\nquite remember just *how* slow it was until I tried to set up a test.\n\nHere's a run on the Perl dataset, with the block size set to 16 MiB to\nmatch the default of DwarFS, and with additional options suggested to\nspeed up compression:\n\n```\n$ time mkcromfs -f 16777216 -qq -e -r100000 install perl-install.cromfs\nWriting perl-install.cromfs...\nmkcromfs: Automatically enabling --24bitblocknums because it seems possible for this filesystem.\nRoot pseudo file is 108 bytes\nInotab spans 0x7f3a18259000..0x7f3a1bfffb9c\nRoot inode spans 0x7f3a205d2948..0x7f3a205d294c\nBeginning task for Files and directories: Finding identical blocks\n2163608 reuse opportunities found. 561362 unique blocks. Block table will be 79.4% smaller than without the index search.\nBeginning task for Files and directories: Blockifying\nBlockifying:  0.04% (140017/2724970) idx(siz=80423,del=0) rawin(20.97 MB)rawout(20.97 MB)diff(1956 bytes)\nTermination signalled, cleaning up temporaries\n\nreal    29m9.634s\nuser    201m37.816s\nsys     2m15.005s\n```\n\nSo, it processed 21 MiB out of 48 GiB in half an hour, using almost\ntwice as much CPU resources as DwarFS for the *whole* file system.\nAt this point I decided it's likely not worth waiting (presumably)\nanother month (!) for `mkcromfs` to finish. I double checked that\nI didn't accidentally build a debugging version, `mkcromfs` was\ndefinitely built with `-O3`.\n\nI then tried once more with a smaller version of the Perl dataset.\nThis only has 20 versions (instead of 1139) of Perl, and obviously\na lot less redundancy:\n\n```\n$ time mkcromfs -f 16777216 -qq -e -r100000 install-small perl-install.cromfs\nWriting perl-install.cromfs...\nmkcromfs: Automatically enabling --16bitblocknums because it seems possible for this filesystem.\nRoot pseudo file is 108 bytes\nInotab spans 0x7f00e0774000..0x7f00e08410a8\nRoot inode spans 0x7f00b40048f8..0x7f00b40048fc\nBeginning task for Files and directories: Finding identical blocks\n25362 reuse opportunities found. 9815 unique blocks. Block table will be 72.1% smaller than without the index search.\nBeginning task for Files and directories: Blockifying\nCompressing raw rootdir inode (28 bytes)z=982370,del=2) rawin(641.56 MB)rawout(252.72 MB)diff(388.84 MB)\n compressed into 35 bytes\nINOTAB pseudo file is 839.85 kB\nInotab inode spans 0x7f00bc036ed8..0x7f00bc036ef4\nBeginning task for INOTAB: Finding identical blocks\n0 reuse opportunities found. 13 unique blocks. Block table will be 0.0% smaller than without the index search.\nBeginning task for INOTAB: Blockifying\nmkcromfs: Automatically enabling --packedblocks because it is possible for this filesystem.\nCompressing raw inotab inode (52 bytes)\n compressed into 58 bytes\nCompressing 9828 block records (4 bytes each, total 39312 bytes)\n compressed into 15890 bytes\nCompressing and writing 16 fblocks...\n\n16 fblocks were written: 35.31 MB = 13.90 % of 254.01 MB\nFilesystem size: 35.33 MB = 5.50 % of original 642.22 MB\nEnd\n\nreal    27m38.833s\nuser    277m36.208s\nsys     11m36.945s\n```\n\nAnd repeating the same task with `mkdwarfs`:\n\n```\n$ time mkdwarfs -i install-small -o perl-install-small.dwarfs\n21:13:38.131724 scanning install-small\n21:13:38.320139 waiting for background scanners...\n21:13:38.727024 assigning directory and link inodes...\n21:13:38.731807 finding duplicate files...\n21:13:38.832524 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files\n21:13:38.832598 waiting for inode scanners...\n21:13:39.619963 assigning device inodes...\n21:13:39.620855 assigning pipe/socket inodes...\n21:13:39.621356 building metadata...\n21:13:39.621453 building blocks...\n21:13:39.621472 saving names and links...\n21:13:39.621655 ordering 3559 inodes using nilsimsa similarity...\n21:13:39.622031 nilsimsa: depth=20000, limit=255\n21:13:39.629206 updating name and link indices...\n21:13:39.630142 pre-sorted index (3360 name, 2127 path lookups) [8.014ms]\n21:13:39.752051 3559 inodes ordered [130.3ms]\n21:13:39.752101 waiting for segmenting/blockifying to finish...\n21:13:53.250951 saving chunks...\n21:13:53.251581 saving directories...\n21:13:53.303862 waiting for compression to finish...\n21:14:11.073273 compressed 611.8 MiB to 24.01 MiB (ratio=0.0392411)\n21:14:11.091099 filesystem created without errors [32.96s]\nâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯\nwaiting for block compression to finish\n3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other\noriginal size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 121.5 MiB\nfilesystem: 222.5 MiB in 14 blocks (7177 chunks, 3559/3559 inodes)\ncompressed filesystem: 14 blocks/24.01 MiB written\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ100% \\\n\nreal    0m33.007s\nuser    3m43.324s\nsys     0m4.015s\n```\n\nSo, `mkdwarfs` is about 50 times faster than `mkcromfs` and uses 75 times\nless CPU resources. At the same time, the DwarFS file system is 30% smaller:\n\n```\n$ ls -l perl-install-small.*fs\n-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs\n-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs\n```\n\nI noticed that the `blockifying` step that took ages for the full dataset\nwith `mkcromfs` ran substantially faster (in terms of MiB/second) on the\nsmaller dataset, which makes me wonder if there's some quadratic complexity\nbehaviour that's slowing down `mkcromfs`.\n\nIn order to be completely fair, I also ran `mkdwarfs` with `-l 9` to enable\nLZMA compression (which is what `mkcromfs` uses by default):\n\n```\n$ time mkdwarfs -i install-small -o perl-install-small-l9.dwarfs -l 9\n21:16:21.874975 scanning install-small\n21:16:22.092201 waiting for background scanners...\n21:16:22.489470 assigning directory and link inodes...\n21:16:22.495216 finding duplicate files...\n21:16:22.611221 saved 267.8 MiB / 611.8 MiB in 22842/26401 duplicate files\n21:16:22.611314 waiting for inode scanners...\n21:16:23.394332 assigning device inodes...\n21:16:23.395184 assigning pipe/socket inodes...\n21:16:23.395616 building metadata...\n21:16:23.395676 building blocks...\n21:16:23.395685 saving names and links...\n21:16:23.395830 ordering 3559 inodes using nilsimsa similarity...\n21:16:23.396097 nilsimsa: depth=50000, limit=255\n21:16:23.401042 updating name and link indices...\n21:16:23.403127 pre-sorted index (3360 name, 2127 path lookups) [6.936ms]\n21:16:23.524914 3559 inodes ordered [129ms]\n21:16:23.525006 waiting for segmenting/blockifying to finish...\n21:16:33.865023 saving chunks...\n21:16:33.865883 saving directories...\n21:16:33.900140 waiting for compression to finish...\n21:17:10.505779 compressed 611.8 MiB to 17.44 MiB (ratio=0.0284969)\n21:17:10.526171 filesystem created without errors [48.65s]\nâ¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯â¯\nwaiting for block compression to finish\n3334 dirs, 0/0 soft/hard links, 26401/26401 files, 0 other\noriginal size: 611.8 MiB, dedupe: 267.8 MiB (22842 files), segment: 122.2 MiB\nfilesystem: 221.8 MiB in 4 blocks (7304 chunks, 3559/3559 inodes)\ncompressed filesystem: 4 blocks/17.44 MiB written\nâââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââââ100% /\n\nreal    0m48.683s\nuser    2m24.905s\nsys     0m3.292s\n```\n\n```\n$ ls -l perl-install-small*.*fs\n-rw-r--r-- 1 mhx users 18282075 Dec 10 21:17 perl-install-small-l9.dwarfs\n-rw-r--r-- 1 mhx users 35328512 Dec  8 14:25 perl-install-small.cromfs\n-rw-r--r-- 1 mhx users 25175016 Dec 10 21:14 perl-install-small.dwarfs\n```\n\nIt takes about 15 seconds longer to build the DwarFS file system with LZMA\ncompression (this is still 35 times faster than Cromfs), but reduces the\nsize even further to make it almost half the size of the Cromfs file system.\n\nI would have added some benchmarks with the Cromfs FUSE driver, but sadly\nit crashed right upon trying to list the directory after mounting.\n\n### With EROFS\n\n[EROFS](https://github.com/erofs/erofs-utils) is a read-only compressed\nfile system that has been added to the Linux kernel recently.\nIts goals are different from those of DwarFS, though. It is designed to\nbe lightweight (which DwarFS is definitely not) and to run on constrained\nhardware like embedded devices or smartphones. It is not designed to provide\nmaximum compression. It currently supports LZ4 and LZMA compression.\n\nRunning it on the full Perl dataset using options given in the README for\n\"well-compressed images\":\n\n```\n$ time mkfs.erofs -C1048576 -Eztailpacking,fragments,all-fragments,dedupe -zlzma,9 perl-install-lzma9.erofs perl-install\nmkfs.erofs 1.7.1-gd93a18c9\n<W> erofs: It may take a longer time since MicroLZMA is still single-threaded for now.\nBuild completed.\n------\nFilesystem UUID: 538ce164-5f9d-4a6a-9808-5915f17ced30\nFilesystem total blocks: 599854 (of 4096-byte blocks)\nFilesystem total inodes: 2255795\nFilesystem total metadata blocks: 74253\nFilesystem total deduplicated bytes (of source files): 29625028195\n\nuser\t2:35:08.03\nsystem\t1:12.65\ntotal\t2:39:25.35\n\n$ ll -h perl-install-lzma9.erofs\n-rw-r--r-- 1 mhx mhx 2.3G Apr 15 16:23 perl-install-lzma9.erofs\n```\n\nThat's definitely slower than SquashFS, but also significantly smaller.\n\nFor a fair comparison, let's use the same 1 MiB block size with DwarFS,\nbut also tweak the options for best compression:\n\n```\n$ time mkdwarfs -i perl-install -o perl-install-1M.dwarfs -l9 -S20 -B64 --order=nilsimsa:max-cluster-size=150000\n[...]\n330733 dirs, 0/2440 soft/hard links, 1927501/1927501 files, 0 other\noriginal size: 47.49 GiB, hashed: 43.47 GiB (1920025 files, 1.451 GiB/s)\nscanned: 19.45 GiB (144675 files, 159.3 MiB/s), categorizing: 0 B/s\nsaved by deduplication: 28.03 GiB (1780386 files), saved by segmenting: 15.4 GiB\nfilesystem: 4.053 GiB in 4151 blocks (937069 chunks, 144674/144674 fragments, 144675 inodes)\ncompressed filesystem: 4151 blocks/806.2 MiB written\n[...]\nuser\t24:27.47\nsystem\t4:20.74\ntotal\t3:26.79\n```\n\nThat's significantly smaller and, almost more importantly, 46 times\nfaster than `mkfs.erofs`.\n\nActually using the file system images, here's how DwarFS performs:\n\n```\n$ dwarfs perl-install-1M.dwarfs mnt -oworkers=8\n$ find mnt -type f -print0 | xargs -0 -P16 -n64 cat | dd of=/dev/null bs=1M status=progress\n50392172594 bytes (50 GB, 47 GiB) copied, 19 s, 2.7 GB/s\n0+1662649 records in\n0+1662649 records out\n51161953159 bytes (51 GB, 48 GiB) copied, 19.4813 s, 2.6 GB/s\n```\n\nReading every single file from 16 parallel processes took less than\n20 seconds. The FUSE driver consumed 143 seconds of CPU time.\n\nHere's the same for EROFS:\n\n```\n$ erofsfuse perl-install-lzma9.erofs mnt\n$ find mnt -type f -print0 | xargs -0 -P16 -n64 cat | dd of=/dev/null bs=1M status=progress\n2594306810 bytes (2.6 GB, 2.4 GiB) copied, 300 s, 8.6 MB/s^C\n0+133296 records in\n0+133296 records out\n2595212832 bytes (2.6 GB, 2.4 GiB) copied, 300.336 s, 8.6 MB/s\n```\n\nNote that I've stopped this after 5 minutes. The DwarFS FUSE driver\ndelivered about 300 times faster throughput compared to EROFS. The\nEROFS FUSE driver consumed 50 minutes (!) of CPU time for only about\n5% of the data, i.e. more than 400 times the CPU time consumed by\nthe DwarFS FUSE driver.\n\nI've tried two more EROFS configurations on the same set of data.\nThe first one uses more or less just the defaults:\n\n```\n$ time mkfs.erofs -zlz4hc,12 perl-install-lz4hc.erofs perl-install\nmkfs.erofs 1.7.1-gd93a18c9\nBuild completed.\n------\nFilesystem UUID: b75142ed-6cf3-46a4-84f3-12693f7759a0\nFilesystem total blocks: 5847130 (of 4096-byte blocks)\nFilesystem total inodes: 2255794\nFilesystem total metadata blocks: 419699\nFilesystem total deduplicated bytes (of source files): 0\n\nuser\t3:38:23.36\nsystem\t1:10.84\ntotal\t3:41:37.33\n```\n\nThe second one additionally enables the `-Ededupe` option:\n\n```\n$ time mkfs.erofs -zlz4hc,12 -Ededupe perl-install-lz4hc-dedupe.erofs perl-install\nmkfs.erofs 1.7.1-gd93a18c9\nBuild completed.\n------\nFilesystem UUID: 0ccf581e-ad3b-4d08-8b10-5b7e15f8e3cd\nFilesystem total blocks: 1510091 (of 4096-byte blocks)\nFilesystem total inodes: 2255794\nFilesystem total metadata blocks: 435599\nFilesystem total deduplicated bytes (of source files): 19220717568\n\nuser\t4:19:57.61\nsystem\t1:21.62\ntotal\t4:23:55.85\n```\n\nI don't know why these are even slower than the first, seemingly more\ncomplex, set of options. As was to be expected, the resulting images\nwere significantly bigger:\n\n```\n$ ll -h perl-install*.erofs\n-rw-r--r-- 1 mhx mhx 5.8G Apr 16 02:46 perl-install-lz4hc-dedupe.erofs\n-rw-r--r-- 1 mhx mhx  23G Apr 15 22:34 perl-install-lz4hc.erofs\n-rw-r--r-- 1 mhx mhx 2.3G Apr 15 16:23 perl-install-lzma9.erofs\n```\n\nThe good news is that these perform *much* better and even outperform\nDwarFS, albeit by a small margin:\n\n```\n$ erofsfuse perl-install-lz4hc.erofs mnt\n$ find mnt -type f -print0 | xargs -0 -P16 -n64 cat | dd of=/dev/null bs=1M status=progress\n49920168315 bytes (50 GB, 46 GiB) copied, 16 s, 3.1 GB/s\n0+1493031 records in\n0+1493031 records out\n51161953159 bytes (51 GB, 48 GiB) copied, 16.4329 s, 3.1 GB/s\n```\n\nThe deduplicated version is even a tiny bit faster:\n\n```\n$ erofsfuse perl-install-lz4hc-dedupe.erofs mnt\nfind mnt -type f -print0 | xargs -0 -P16 -n64 cat | dd of=/dev/null bs=1M status=progress\n50808037121 bytes (51 GB, 47 GiB) copied, 16 s, 3.2 GB/s\n0+1499949 records in\n0+1499949 records out\n51161953159 bytes (51 GB, 48 GiB) copied, 16.1184 s, 3.2 GB/s\n```\n\nThe EROFS kernel driver wasn't any faster than the FUSE driver.\n\nThe FUSE driver used about 27 seconds of CPU time in both cases,\nsubstantially less than before and 5 times less than DwarFS.\n\nDwarFS can get close to the throughput of EROFS by using `zstd` instead\nof `lzma` compression:\n\n```\n$ dwarfs perl-install-1M-zstd.dwarfs mnt -oworkers=8\nfind mnt -type f -print0 | xargs -0 -P16 -n64 cat | dd of=/dev/null bs=1M status=progress\n49224202357 bytes (49 GB, 46 GiB) copied, 16 s, 3.1 GB/s\n0+1529018 records in\n0+1529018 records out\n51161953159 bytes (51 GB, 48 GiB) copied, 16.6716 s, 3.1 GB/s\n```\n\n### With fuse-archive\n\nI came across [fuse-archive](https://github.com/google/fuse-archive)\nwhile looking for FUSE drivers to mount archives and it seems to be\nthe most versatile of the alternatives (and the one that actually\ncompiles out of the box).\n\nAn interesting test case straight from fuse-archive's README is in\nthe [Performance](https://github.com/google/fuse-archive#performance)\nsection: an archive with a single huge file full of zeroes. Let's\nmake the example a bit more extreme and use a 1 GiB file instead of\njust 256 MiB:\n\n```\n$ mkdir zerotest\n$ truncate --size=1G zerotest/zeroes\n```\n\nNow, we build several different archives and a DwarFS image:\n\n```\n$ time mkdwarfs -i zerotest -o zerotest.dwarfs -W16 --log-level=warn --progress=none\n\nreal    0m7.604s\nuser    0m7.521s\nsys     0m0.083s\n\n$ time zip -9 zerotest.zip zerotest/zeroes\n  adding: zerotest/zeroes (deflated 100%)\n\nreal    0m4.923s\nuser    0m4.840s\nsys     0m0.080s\n\n$ time 7z a -bb0 -bd zerotest.7z zerotest/zeroes\n\n7-Zip [64] 16.02 : Copyright (c) 1999-2016 Igor Pavlov : 2016-05-21\np7zip Version 16.02 (locale=en_US.UTF-8,Utf16=on,HugeFiles=on,64 bits,16 CPUs Intel(R) Xeon(R) E-2286M  CPU @ 2.40GHz (906ED),ASM,AES-NI)\n\nScanning the drive:\n1 file, 1073741824 bytes (1024 MiB)\n\nCreating archive: zerotest.7z\n\nItems to compress: 1\n\nFiles read from disk: 1\nArchive size: 157819 bytes (155 KiB)\nEverything is Ok\n\nreal    0m5.535s\nuser    0m48.281s\nsys     0m1.116s\n\n$ time tar --zstd -cf zerotest.tar.zstd zerotest/zeroes\n\nreal    0m0.449s\nuser    0m0.510s\nsys     0m0.610s\n```\n\nTurns out that `tar --zstd` is easily winning the compression speed\ntest. Looking at the file sizes did actually blow my mind just a bit:\n\n```\n$ ll zerotest.* --sort=size\n-rw-r--r-- 1 mhx users 1042231 Jul  1 15:24 zerotest.zip\n-rw-r--r-- 1 mhx users  157819 Jul  1 15:26 zerotest.7z\n-rw-r--r-- 1 mhx users   33762 Jul  1 15:28 zerotest.tar.zstd\n-rw-r--r-- 1 mhx users     848 Jul  1 15:23 zerotest.dwarfs\n```\n\nI definitely didn't expect the DwarFS image to be *that* small.\nDropping the section index would actually save another 100 bytes.\nSo, if you want to archive lots of zeroes, DwarFS is your friend.\n\nAnyway, let's look at how fast and efficiently the zeroes can\nbe read from the different archives. First, the `zip` archive:\n\n```\n$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress\n1020117504 bytes (1.0 GB, 973 MiB) copied, 2 s, 510 MB/s\n2097152+0 records in\n2097152+0 records out\n1073741824 bytes (1.1 GB, 1.0 GiB) copied, 2.10309 s, 511 MB/s\n\nreal    0m2.104s\nuser    0m0.264s\nsys     0m0.486s\n```\n\nCPU time used by the FUSE driver was 1.8 seconds and mount time\nwas in the milliseconds.\n\nNow, the `7z` archive:\n\n```\n $ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress\n594759168 bytes (595 MB, 567 MiB) copied, 1 s, 595 MB/s\n2097152+0 records in\n2097152+0 records out\n1073741824 bytes (1.1 GB, 1.0 GiB) copied, 1.76904 s, 607 MB/s\n\nreal    0m1.772s\nuser    0m0.229s\nsys     0m0.572s\n```\n\nCPU time used by the FUSE driver was 2.9 seconds and mount time\nwas just over 1.0 seconds.\n\nNow, the `.tar.zstd` archive:\n\n```\n$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress\n2097152+0 records in\n2097152+0 records out\n1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.799409 s, 1.3 GB/s\n\nreal    0m0.801s\nuser    0m0.262s\nsys     0m0.537s\n```\n\nCPU time used by the FUSE driver was 0.53 seconds and mount time\nwas 0.13 seconds.\n\nLast but not least, let's look at DwarFS:\n\n```\n$ time dd if=mnt/zeroes of=/dev/null status=progress\n2097152+0 records in\n2097152+0 records out\n1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.753 s, 1.4 GB/s\n\nreal    0m0.757s\nuser    0m0.220s\nsys     0m0.534s\n```\n\nCPU time used by the FUSE driver was 0.17 seconds and mount time\nwas less than a millisecond.\n\nIf we increase the block size for the `dd` command, we can get\neven higher throughput. For fuse-archive with the `.tar.zstd`:\n\n```\n$ time dd if=mnt/zerotest/zeroes of=/dev/null status=progress bs=16384\n65536+0 records in\n65536+0 records out\n1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.318682 s, 3.4 GB/s\n\nreal    0m0.323s\nuser    0m0.005s\nsys     0m0.154s\n```\n\nAnd for DwarFS:\n\n```\n$ time dd if=mnt/zeroes of=/dev/null status=progress bs=16384\n65536+0 records in\n65536+0 records out\n1073741824 bytes (1.1 GB, 1.0 GiB) copied, 0.172226 s, 6.2 GB/s\n\nreal    0m0.176s\nuser    0m0.020s\nsys     0m0.141s\n```\n\nThis is all nice, but what about a more real-life use case?\nLet's take the 1.82.0 boost release archives:\n\n```\n$ ll --sort=size boost_1_82_0.*\n-rw-r--r-- 1 mhx users 208188085 Apr 10 14:25 boost_1_82_0.zip\n-rw-r--r-- 1 mhx users 142580547 Apr 10 14:23 boost_1_82_0.tar.gz\n-rw-r--r-- 1 mhx users 121325129 Apr 10 14:23 boost_1_82_0.tar.bz2\n-rw-r--r-- 1 mhx users 105901369 Jun 28 12:47 boost_1_82_0.dwarfs\n-rw-r--r-- 1 mhx users 103710551 Apr 10 14:25 boost_1_82_0.7z\n```\n\nHere are the timings for mounting each archive and then using\n`tar` to build another archive from the mountpoint and just counting\nthe number of bytes in that archive, e.g.:\n\n```\n$ time tar cf - mnt | wc -c\n803614720\n\nreal    0m4.602s\nuser    0m0.156s\nsys     0m1.123s\n```\n\nHere are the results in terms of wallclock time and FUSE driver\nCPU time:\n\n| Archive    | Mount Time | `tar` Wallclock Time | FUSE Driver CPU Time |\n| ---------- | ---------: | -------------------: | -------------------: |\n| `.zip`     |     0.458s |               5.073s |               4.418s |\n| `.tar.gz`  |     1.391s |               3.483s |               3.943s |\n| `.tar.bz2` |    15.663s |              17.942s |              32.040s |\n| `.7z`      |     0.321s |              32.554s |              31.625s |\n| `.dwarfs`  |     0.013s |               2.974s |               1.984s |\n\nDwarFS easily wins all categories while still compressing the data\nalmost as well as `7z`.\n\nWhat about accessing files more randomly?\n\n```\n$ find mnt -type f -print0 | xargs -0 -P32 -n32 cat | dd of=/dev/null status=progress\n```\n\nIt turns out that fuse-archive grinds to a halt in this case, so I had\nto run the test on a subset (the `boost` subdirectory) of the data.\nThe `.tar.bz2` and `.7z` archives were so slow to read that I stopped\nthem after a few minutes.\n\n| Archive    | Throughput | Wallclock Time | FUSE Driver CPU Time |\n| ---------- | ---------: | -------------: | -------------------: |\n| `.zip`     |   1.8 MB/s |        83.245s |              83.669s |\n| `.tar.gz`  |   1.2 MB/s |       121.377s |             122.711s |\n| `.tar.bz2` |   0.2 MB/s |              - |                    - |\n| `.7z`      |   0.3 MB/s |              - |                    - |\n| `.dwarfs`  | 598.0 MB/s |         0.249s |               1.099s |\n\n\n## Performance Monitoring\n\nBoth the FUSE driver and `dwarfsextract` by default have support for\nsimple performance monitoring. You can build binaries without this\nfeature (`-DENABLE_PERFMON=OFF`), but impact should be negligible even\nif performance monitoring is enabled at run-time.\n\nTo enable the performance monitor, you pass a list of components for which\nyou want to collect latency metrics, e.g.:\n\n```\n$ dwarfs test.dwarfs mnt -f -operfmon=fuse\n```\n\nWhen the driver exits, you will see output like this:\n\n```\n[fuse.op_read]\n      samples: 45145\n      overall: 3.214s\n  avg latency: 71.2us\n  p50 latency: 131.1us\n  p90 latency: 131.1us\n  p99 latency: 262.1us\n\n[fuse.op_readdir]\n      samples: 2\n      overall: 51.31ms\n  avg latency: 25.65ms\n  p50 latency: 32.77us\n  p90 latency: 67.11ms\n  p99 latency: 67.11ms\n\n[fuse.op_lookup]\n      samples: 16\n      overall: 19.98ms\n  avg latency: 1.249ms\n  p50 latency: 2.097ms\n  p90 latency: 4.194ms\n  p99 latency: 4.194ms\n\n[fuse.op_init]\n      samples: 1\n      overall: 199.4us\n  avg latency: 199.4us\n  p50 latency: 262.1us\n  p90 latency: 262.1us\n  p99 latency: 262.1us\n\n[fuse.op_open]\n      samples: 16\n      overall: 122.2us\n  avg latency: 7.641us\n  p50 latency: 4.096us\n  p90 latency: 32.77us\n  p99 latency: 32.77us\n\n[fuse.op_getattr]\n      samples: 1\n      overall: 5.786us\n  avg latency: 5.786us\n  p50 latency: 8.192us\n  p90 latency: 8.192us\n  p99 latency: 8.192us\n```\n\nThe metrics should be self-explanatory. However, note that the\npercentile metrics are logarithmically quantized in order to use\nas little resources as possible. As a result, you will only see\nvalues that look an awful lot like powers of two.\n\nCurrently, the supported components are `fuse` for the FUSE\noperations, `filesystem_v2` for the DwarFS file system component\nand `inode_reader_v2` for the component that handles all `read()`\nsystem calls.\n\nThe FUSE driver also exposes the performance monitor metrics via\nan [extended attribute](#extended-attributes).\n\n\n## Other Obscure Features\n\n### Setting Worker Thread CPU Affinity\n\nThis only works on Linux and usually only makes sense if you have CPUs\nwith different types of cores (e.g. \"performance\" vs \"efficiency\" cores)\nand are *really* trying to squeeze the last ounce of speed out of DwarFS.\n\nBy setting the environment variable `DWARFS_WORKER_GROUP_AFFINITY`, you\ncan set the CPU affinity of different worker thread groups, e.g.:\n\n```\nexport DWARFS_WORKER_GROUP_AFFINITY=blockify=3:compress=6,7\n```\n\nThis will set the affinity of the `blockify` worker group to CPU 3 and\nthe affinity of the `compress` worker group to CPUs 6 and 7.\n\nYou can use this feature for all tools that use one or more worker thread\ngroups. For example, the FUSE driver `dwarfs` and `dwarfsextract` use a\nworker group `blkcache` that the block cache (i.e. block decompression and\nlookup) runs on. `mkdwarfs` uses a whole array of different worker groups,\nnamely `compress` for compression, `scanner` for scanning, `ordering` for\ninput ordering, and `blockify` for segmenting. `blockify` is what you would\ntypically want to run on your \"performance\" cores.\n\n## Stargazers over Time\n\n[![Stargazers over Time](https://starchart.cc/mhx/dwarfs.svg?variant=adaptive)](https://starchart.cc/mhx/dwarfs)\n"
        },
        {
          "name": "TODO",
          "type": "blob",
          "size": 10.9990234375,
          "content": "- Add support for logging to file (with different level?)\n\n- Add support for libarchive filters in dwarfsextract.\n\n- When hashing, start by only hashing the first, say, 4KiB,\n  and only if the hashes are identical, hash the whole file\n\n- Add support for compressing uncompressed image formats,\n  currently primarily FITS, using different formats (e.g.\n  JPEG2K, Rice, Hcompress).\n\n- Fragment counts don't necessarily match up in presence\n  of errors.\n\n- When opening a file again, check that its timestamp,\n  size and potentially checksum did not change from\n  when we first saw it.\n\n- Option to log to a file instead of stderr?\n\n- Use thrift definitions for all options to make them\n  easily printable/storable?\n\n- Use Elias-Fano for delta-encoded lists in metadata?\n\n- Implement rewriting properly; keep order of blocks etc;\n  ability to remove history?; ability to re-pack metadata?;\n  ability to change other metadata properties (e.g. stuff\n  like --set-owner, --set-group, --chmod, --no-create-ts,\n  --set-time could all be done on existing metadata, but\n  obviously wouldn't be undo-able)\n\n- Packaging of libs added via FetchContent\n\n- Re-assemble global bloom filter rather than merging?\n- Use smaller bloom filters for individual blocks?\n- Use bigger (non-resettable?) global bloom filter?\n\n- file discovery progress?\n\n- show defaults for categorized options\n\n- take a look at CPU measurements, those for nilsimsa\n  ordering are probably wrong\n\n- segmenter tests with different granularities, block sizes,\n  any other options\n\n- Bloom filters can be wasteful if lookback gets really long.\n  Maybe we can use smaller bloom filters for individual blocks\n  and one or two larger \"global\" bloom filters? It's going to\n  be impossible to rebuild those from the smaller filters,\n  though.\n\n- Compress long repetitions of the same byte more efficiently.\n  Currently, segmentation finds an overlap after about one\n  window size. This goes on and on repeatedly. So we end up\n  with a *lot* of chunks pointing to the same segment. The\n  smaller the window size, the larger the number of chunks.\n  It's definitely a trade off, as storing large segments of\n  repeating bytes is wasteful when mounting the image.\n\n  Intriguing idea: pre-compute 256 (or just 2, for 0x00 and 0xFF)\n  hash values for window_size bytes to detect long sequences of\n  identical bytes.\n\n  OTHER intriguing idea: let a categorizer (could even be the\n  incompressible categorizer, but also \"sparse file\" categorizer\n  or something like that) detect these repetitions up front so\n  the segmenter doesn't have to do it (and it can be optional).\n  Then, we can customize the segmenter to run *extremely* fast\n  in this case.\n\n\n- Wiki with use cases\n  - Perl releases\n  - Videos with shared streams\n  - Backups of audio files\n  - Compression of filesystem images for forensic purposes\n\n- Mounting lots of images with shared cache?\n\n- different scenarios for categorized files / chunks:\n\n  - Video files\n    - just store without compression, but perform segmentation, nothing special\n    - keep in lookback buffer for longer, as it doesn't cost memory\n    - needs parallelized segmenter (see below)\n\n  - PCM audio (see also github #95)\n    - segment first in case of e.g. different trims or other types of overlap\n    - split into chunks (for parallel decompression)\n    - compress each chunk as flac\n    - headers to be saved separately\n    - need to store original size and other information\n\n    This is actually quite easy:\n\n    - Identify PCM audio files (libmagic?)\n    - Nilsimsa similarity works surprisingly well\n    - We can potentially switch to larger window size for segmentation and use\n      larger lookback\n    - Run segmentation as usual\n    - Compress each block using FLAC (hopefully we can configure how much header data\n      and/or seek points etc. gets stored) or maybe even WAVPACK is we don't need perf\n    - I *think* this can be done even with the current metadata format without any\n      additions\n    - The features needed should be largely orthogonal to the features needed for the\n      scenarios below\n\n  - Executables, Shared Libs, ...\n    - run filter over blocks that contain a certain kind of binary data before\n      compression\n    - a single binary may contain machine code for different architectures,\n      so we may have to store different parts of the binary in different blocks\n    - actually quite similar to audio files above, except for the additional\n      filter used during compression/decompression\n\n  - JPG\n    - just store a recompressed version\n    - need to store original size\n    - no need for segmentation except for exact \n\n  - PDF\n    - decompress contents\n    - then segment\n    - then compress along with other PDFs (or documents in general)\n\n  - Other compressed format (gz, xz, ...)\n    - decompress\n    - segment\n    - compress\n    - essentially like PDF\n    - maybe only do this for small files? (option for size limit?)\n\n  - It should be possible to treat individual chunks differently, e.g.\n    WAV-header should be stored independently from contents; at some\n    point, we might look deeper into tar files and compress individual\n    contents differently.\n\n- in the metadata, we need to know:\n\n  - the fact that a stored inode is \"special\" (can be reflected in a single bit)\n  - the type of \"specialness\"\n  - the original file size\n\n\n- multi-threaded pre-matcher (for -Bn with n > 0)\n  - pre-compute matches/cyclic hashes for completed blocks; these don't\n    change and so we can do this with very little synchronization\n  - there are two possible strategies:\n    - split the input stream into chunks and then process each chunk in\n      a separate thread, checking all n blocks\n    - process the input stream in each thread and then only checking a\n      subset of past blocks (this seems more wasteful, but each thread\n      would only operate on a few instead of all bloom filters, which\n      could be better from a cache locality pov)\n\n- similarity size limit to avoid similarity computation for huge files\n- store files without similarity hash first, sorted descending by size\n\n\n- use streaming interface for zstd decompressor\n- json metadata recovery\n- handle sparse files?\n- try to be more resilient to modifications of the input while creating fs\n\n- dwarfsck:\n  - show which entries a block references\n  - show partial metadata dumps at lower detail levels\n\n- make dwarfsck more usable\n- cleanup TODOs\n\n- folly: dynamic should support string_view\n- frozen: ViewBase.getPosition() should be const\n\n- docs, moar tests\n\n- extended attributes:\n  - number of blocks\n  - number of chunks\n  - number of times opened?\n\n- per-file \"hotness\" (how often was a file opened); dump to file upon umount\n\n- --unpack option\n\n- readahead?\n\n- window-increment-shift seems silly to configure?\n\n- identify blocks that contain mostly binary data and adjust compressor?\n\n- metadata stripping (i.e. re-write metadata without owner/time info)\n\n- metadata repacking (e.g. just recompress/decompress the metadata block)\n\n\n/*\n\nscanner:\nbhw= -   388.3s  13.07  GiB\nbhw= 8   812.9s   7.559 GiB\nbhw= 9   693.1s   7.565 GiB\nbhw=10   651.8s   7.617 GiB\nbhw=11   618.7s   7.313 GiB\nbhw=12   603.6s   7.625 GiB\nbhw=13   591.2s   7.858 GiB\nbhw=14   574.1s   8.306 GiB\nbhw=15   553.8s   8.869 GiB\nbhw=16   541.9s   9.529 GiB\n\n\nlz4:\n                          <----  1m29.535s / 9m31.212s\n\nlz4hc:\n 1 -  20.94s - 2546 MiB\n 2 -  21.67s - 2441 MiB\n 3 -  24.19s - 2377 MiB\n 4 -  27.29s - 2337 MiB\n 5 -  31.49s - 2311 MiB\n 6 -  36.39s - 2294 MiB\n 7 -  42.04s - 2284 MiB\n 8 -  48.67s - 2277 MiB\n 9 -  56.94s - 2273 MiB  <----  1m27.979s / 9m20.637s\n10 -  68.03s - 2271 MiB\n11 -  79.54s - 2269 MiB\n12 -  94.84s - 2268 MiB\n\nzstd:\n 1 -  11.42s - 1667 MiB\n 2 -  12.95s - 1591 MiB  <----  2m8.351s / 15m25.752s\n 3 -  22.03s - 1454 MiB\n 4 -  25.64s - 1398 MiB\n 5 -  32.34s - 1383 MiB\n 6 -  41.45s - 1118 MiB  <----  2m4.258s / 14m28.627s\n 7 -  46.26s - 1104 MiB\n 8 -  53.34s - 1077 MiB\n 9 -  59.99s - 1066 MiB\n10 -  63.3s  - 1066 MiB\n11 -  66.97s -  956 MiB  <----  2m3.496s / 14m17.862s\n12 -  79.89s -  953 MiB\n13 -  89.8s  -  943 MiB\n14 - 118.1s  -  941 MiB\n15 - 230s    -  951 MiB\n16 - 247.4s  -  863 MiB  <----  2m11.202s / 14m57.245s\n17 - 294.5s  -  854 MiB\n18 - 634s    -  806 MiB\n19 - 762.5s  -  780 MiB\n20 - 776.8s  -  718 MiB  <----  2m16.448s / 15m43.923s\n21 - 990.4s  -  716 MiB\n22 - 984.3s  -  715 MiB  <----  2m18.133s / 15m55.263s\n\nlzma:\nlevel=6:dict_size=21  921.9s  - 838.8 MiB  <----  5m11.219s / 37m36.002s\n\n*/\n\nPerl:\n542 versions of perl\nfound/scanned: 152809/152809 dirs, 0/0 links, 1325098/1325098 files\noriginal size: 32.03 GiB, saved: 19.01 GiB by deduplication (1133032 duplicate files), 5.835 GiB by segmenting\nfilesystem size: 7.183 GiB in 460 blocks (499389 chunks, 192066/192066 inodes), 460 blocks/662.3 MiB written\n\n                                                                                   bench\n                                                                         build   real  user\n-----------------------------------------------------------------------------------------------------\n-rw-r--r-- 1 mhx users  14G Jul 27 23:11 perl-install-0.dwarfs            8:05   0:38  0:45\n-rw-r--r-- 1 mhx users 4.8G Jul 27 23:18 perl-install-1.dwarfs            6:34   0:14  1:24\n-rw-r--r-- 1 mhx users 3.8G Jul 27 23:26 perl-install-2.dwarfs            7:31   0:17  1:11\n-rw-r--r-- 1 mhx users 3.2G Jul 27 23:36 perl-install-3.dwarfs           10:11   0:11  0:59\n-rw-r--r-- 1 mhx users 1.8G Jul 27 23:47 perl-install-4.dwarfs           11:05   0:14  1:24\n-rw-r--r-- 1 mhx users 1.2G Jul 27 23:59 perl-install-5.dwarfs           11:53   0:13  1:15\n-rw-r--r-- 1 mhx users 901M Jul 28 00:16 perl-install-6.dwarfs           17:42   0:14  1:25\n-rw-r--r-- 1 mhx users 704M Jul 28 00:37 perl-install-7.dwarfs           20:52   0:20  2:14\n-rw-r--r-- 1 mhx users 663M Jul 28 04:04 perl-install-8.dwarfs           24:13   0:50  6:02\n-rw-r--r-- 1 mhx users 615M Jul 28 02:50 perl-install-9.dwarfs           34:40   0:51  5:50\n\n-rw-r--r-- 1 mhx users 3.6G Jul 28 09:13 perl-install-defaults.squashfs  17:20\n-rw-r--r-- 1 mhx users 2.4G Jul 28 10:42 perl-install-opt.squashfs       71:49\n\n\n\nsoak:\n\n-7  (cache=1g)\n\nPassed with 542 of 542 combinations.\n\nreal    75m21.191s\nuser    68m3.903s\nsys     6m21.020s\n\n-9  (cache=1g)\n\nPassed with 542 of 542 combinations.\n\nreal    118m48.371s\nuser    107m35.685s\nsys     7m16.438s\n\nsquashfs-opt\n\nreal    81m36.957s\nuser    62m37.369s\nsys     20m52.367s\n\n\n-1  (cache=2g)\nmhx@gimli ~ $ time find tmp/mount/ -type f | xargs -n 1 -P 32 -d $'\\n' -I {} dd of=/dev/null if={} bs=64K status=none\n\nreal    2m19.927s\nuser    0m16.813s\nsys     2m4.293s\n\n-7  (cache=2g)\nmhx@gimli ~ $ time find tmp/mount/ -type f | xargs -n 1 -P 32 -d $'\\n' -I {} dd of=/dev/null if={} bs=64K status=none\n\nreal    2m24.346s\nuser    0m17.007s\nsys     1m59.823s\n\nsquash-default\nmhx@gimli ~ $ time find tmp/mount/ -type f | xargs -n 1 -P 32 -d $'\\n' -I {} dd of=/dev/null if={} bs=64K status=none\n\nreal    8m41.594s\nuser    1m25.346s\nsys     19m12.036s\n\nsquash-opt\nmhx@gimli ~ $ time find tmp/mount/ -type f | xargs -n 1 -P 32 -d $'\\n' -I {} dd of=/dev/null if={} bs=64K status=none\n\nreal    141m41.092s\nuser    1m12.650s\nsys     59m18.194s\n\n"
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "example",
          "type": "tree",
          "content": null
        },
        {
          "name": "fbthrift",
          "type": "commit",
          "content": null
        },
        {
          "name": "folly",
          "type": "commit",
          "content": null
        },
        {
          "name": "fsst",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "ricepp",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "src",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "thrift",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tsan.supp",
          "type": "blob",
          "size": 0.064453125,
          "content": "race:^std::__future_base::_Result<*>::_M_destroy()\nrace:libunwind\n"
        },
        {
          "name": "vcpkg.json",
          "type": "blob",
          "size": 0.5712890625,
          "content": "{\r\n  \"dependencies\": [\r\n    \"benchmark\",\r\n    \"boost-asio\",\r\n    \"boost-chrono\",\r\n    \"boost-context\",\r\n    \"boost-convert\",\r\n    \"boost-crc\",\r\n    \"boost-filesystem\",\r\n    \"boost-iostreams\",\r\n    \"boost-multi-index\",\r\n    \"boost-process\",\r\n    \"boost-program-options\",\r\n    \"boost-thread\",\r\n    \"boost-uuid\",\r\n    \"boost-variant\",\r\n    \"brotli\",\r\n    \"date\",\r\n    \"double-conversion\",\r\n    \"fmt\",\r\n    \"glog\",\r\n    \"libarchive\",\r\n    \"libevent\",\r\n    \"libflac\",\r\n    \"nlohmann-json\",\r\n    \"openssl\",\r\n    \"pkgconf\",\r\n    \"range-v3\",\r\n    \"utfcpp\",\r\n    \"xxhash\",\r\n    \"zstd\"\r\n  ]\r\n}\r\n"
        }
      ]
    }
  ]
}