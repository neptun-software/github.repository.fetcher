{
  "metadata": {
    "timestamp": 1736565405331,
    "page": 246,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI1MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "tensorflow/minigo",
      "stars": 3478,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".bazelrc",
          "type": "blob",
          "size": 0.4462890625,
          "content": "build --define=tf=1\ntest -c dbg\n\n# Some of the Bazel rules used to precompile TensorFlow don't respect Bazel's\n# \"manual\" tag. The following hack prevents Bazel from compiling TensorFlow\n# from source when executing a command such as: bazel test cc/...\ntest //cc/... -- -//cc/tensorflow/...\n\n# These .bazelrc files are generated by the cc/configure_tensorflow.sh script.\ntry-import %workspace%/tf_configure.bazelrc\ntry-import %workspace%/tensorflow.bazelrc\n"
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.4609375,
          "content": "lib\nlib64\nbin\ndata\n*__pycache__\npip-selfcheck.json\n*.pyc\nsgf\npyvenv.cfg\n.DS_store\nlogs/\nsaved_models/\nwgo/*\noneoffs/wgo/*\nratings.db\ncbt_ratings.db\n.rsync_log\n\n# Vim temp files\n*.swp\n*.swo\n*~\n\n*.wtf-trace\n.mypy_cache\n\n# Ignore any staging directory. We use this directory for docker-file creation.\nstaging/\n\nbazel-*\ncc/tensorflow/bin\ncc/tensorflow/include\ncc/tensorflow/lib\ntensorflow.bazelrc\ntf_configure.bazelrc\n\nminigui/static/*.js.map\n\ncluster/cgos/cgosGtp*\n\n.bazelrc\n"
        },
        {
          "name": ".pylintrc",
          "type": "blob",
          "size": 16.580078125,
          "content": "[MASTER]\n\n# A comma-separated list of package or module names from where C extensions may\n# be loaded. Extensions are loading into the active Python interpreter and may\n# run arbitrary code\nextension-pkg-whitelist=\n\n# Add files or directories to the blacklist. They should be base names, not\n# paths.\nignore=\n      dual_net.py,\n      features.py,\n      go.py,\n      gtp_cmd_handlers.py,\n      gtp_engine.py,\n      inference_worker.py,\n      mcts.py,\n      preprocessing.py,\n      sgf_wrapper.py,\n      strategies.py,\n      symmetries.py,\n\n# Add files or directories matching the regex patterns to the blacklist. The\n# regex matches against base names, not paths.\nignore-patterns=oneoffs.*\n\n# Python code to execute, usually for sys.path manipulation such as\n# pygtk.require().\n#init-hook=\n\n# Use multiple processes to speed up Pylint.\njobs=1\n\n# List of plugins (as comma separated values of python modules names) to load,\n# usually to register additional checkers.\nload-plugins=\n\n# Pickle collected data for later comparisons.\npersistent=yes\n\n# Specify a configuration file.\n#rcfile=\n\n# When enabled, pylint would attempt to guess common misconfiguration and emit\n# user-friendly hints instead of false-positive error messages\nsuggestion-mode=yes\n\n# Allow loading of arbitrary C extensions. Extensions are imported into the\n# active Python interpreter and may run arbitrary code.\nunsafe-load-any-extension=no\n\n\n[MESSAGES CONTROL]\n\n# Only show warnings with the listed confidence levels. Leave empty to show\n# all. Valid levels: HIGH, INFERENCE, INFERENCE_FAILURE, UNDEFINED\nconfidence=\n\n# Disable the message, report, category or checker with the given id(s). You\n# can either give multiple identifiers separated by comma (,) or put this\n# option multiple times (only on the command line, not in the configuration\n# file where it should appear only once).You can also use \"--disable=all\" to\n# disable everything first and then reenable specific checks. For example, if\n# you want to run only the similarities checker, you can use \"--disable=all\n# --enable=similarities\". If you want to run only the classes checker, but have\n# no Warning level messages displayed, use\"--disable=all --enable=classes\n# --disable=W\"\n\ndisable=bare-except,\n        import-error,\n        import-outside-toplevel,\n        invalid-name,\n        too-few-methods,\n        too-few-public-methods,\n        too-many-arguments,\n        too-many-locals,\n        subprocess-run-check,\n        # print-statement,\n        # parameter-unpacking,\n        # unpacking-in-except,\n        # old-raise-syntax,\n        # backtick,\n        # long-suffix,\n        # old-ne-operator,\n        # old-octal-literal,\n        # import-star-module-level,\n        # non-ascii-bytes-literal,\n        # raw-checker-failed,\n        # bad-inline-option,\n        # locally-disabled,\n        # locally-enabled,\n        # file-ignored,\n        # suppressed-message,\n        # useless-suppression,\n        # deprecated-pragma,\n        # apply-builtin,\n        # basestring-builtin,\n        # buffer-builtin,\n        # cmp-builtin,\n        # coerce-builtin,\n        # execfile-builtin,\n        # file-builtin,\n        # long-builtin,\n        # raw_input-builtin,\n        # reduce-builtin,\n        # standarderror-builtin,\n        # unicode-builtin,\n        # xrange-builtin,\n        # coerce-method,\n        # delslice-method,\n        # getslice-method,\n        # setslice-method,\n        # no-absolute-import,\n        # old-division,\n        # dict-iter-method,\n        # dict-view-method,\n        # next-method-called,\n        # metaclass-assignment,\n        # indexing-exception,\n        # raising-string,\n        # reload-builtin,\n        # oct-method,\n        # hex-method,\n        # nonzero-method,\n        # cmp-method,\n        # input-builtin,\n        # round-builtin,\n        # intern-builtin,\n        # unichr-builtin,\n        # map-builtin-not-iterating,\n        # zip-builtin-not-iterating,\n        # range-builtin-not-iterating,\n        # filter-builtin-not-iterating,\n        # using-cmp-argument,\n        # eq-without-hash,\n        # div-method,\n        # idiv-method,\n        # rdiv-method,\n        # exception-message-attribute,\n        # invalid-str-codec,\n        # sys-max-int,\n        # bad-python3-import,\n        # deprecated-string-function,\n        # deprecated-str-translate-call,\n        # deprecated-itertools-function,\n        # deprecated-types-field,\n        # next-method-defined,\n        # dict-items-not-iterating,\n        # dict-keys-not-iterating,\n        # dict-values-not-iterating\n\n# Enable the message, report, category or checker with the given id(s). You can\n# either give multiple identifier separated by comma (,) or put this option\n# multiple time (only on the command line, not in the configuration file where\n# it should appear only once). See also the \"--disable\" option for examples.\nenable=c-extension-no-member\n\n\n[REPORTS]\n\n# Python expression which should return a note less than 10 (10 is the highest\n# note). You have access to the variables errors warning, statement which\n# respectively contain the number of errors / warnings messages and the total\n# number of statements analyzed. This is used by the global evaluation report\n# (RP0004).\nevaluation=10.0 - ((float(5 * error + warning + refactor + convention) / statement) * 10)\n\n# Template used to display messages. This is a python new-style format string\n# used to format the message information. See doc for all details\n#msg-template=\n\n# Set the output format. Available formats are text, parseable, colorized, json\n# and msvs (visual studio).You can also give a reporter class, eg\n# mypackage.mymodule.MyReporterClass.\noutput-format=text\n\n# Tells whether to display a full report or only the messages\nreports=no\n\n# Activate the evaluation score.\nscore=yes\n\n\n[REFACTORING]\n\n# Maximum number of nested blocks for function / method body\nmax-nested-blocks=5\n\n# Complete name of functions that never returns. When checking for\n# inconsistent-return-statements if a never returning function is called then\n# it will be considered as an explicit return statement and no message will be\n# printed.\nnever-returning-functions=optparse.Values,sys.exit\n\n\n[LOGGING]\n\n# Logging modules to check that the string format arguments are in logging\n# function parameter format\nlogging-modules=logging\n\n\n[SPELLING]\n\n# Limits count of emitted suggestions for spelling mistakes\nmax-spelling-suggestions=4\n\n# Spelling dictionary name. Available dictionaries: none. To make it working\n# install python-enchant package.\nspelling-dict=\n\n# List of comma separated words that should not be checked.\nspelling-ignore-words=\n\n# A path to a file that contains private dictionary; one word per line.\nspelling-private-dict-file=\n\n# Tells whether to store unknown words to indicated private dictionary in\n# --spelling-private-dict-file option instead of raising a message.\nspelling-store-unknown-words=no\n\n\n[MISCELLANEOUS]\n\n# List of note tags to take in consideration, separated by a comma.\nnotes=FIXME,\n      XXX,\n      TODO\n\n\n[TYPECHECK]\n\n# List of decorators that produce context managers, such as\n# contextlib.contextmanager. Add to this list to register other decorators that\n# produce valid context managers.\ncontextmanager-decorators=contextlib.contextmanager\n\n# List of members which are set dynamically and missed by pylint inference\n# system, and so shouldn't trigger E1101 when accessed. Python regular\n# expressions are accepted.\ngenerated-members=\n\n# Tells whether missing members accessed in mixin class should be ignored. A\n# mixin class is detected if its name ends with \"mixin\" (case insensitive).\nignore-mixin-members=yes\n\n# This flag controls whether pylint should warn about no-member and similar\n# checks whenever an opaque object is returned when inferring. The inference\n# can return multiple potential results while evaluating a Python object, but\n# some branches might not be evaluated, which results in partial inference. In\n# that case, it might be useful to still emit no-member and other checks for\n# the rest of the inferred objects.\nignore-on-opaque-inference=yes\n\n# List of class names for which member attributes should not be checked (useful\n# for classes with dynamically set attributes). This supports the use of\n# qualified names.\nignored-classes=optparse.Values,thread._local,_thread._local\n\n# List of module names for which member attributes should not be checked\n# (useful for modules/projects where namespaces are manipulated during runtime\n# and thus existing member attributes cannot be deduced by static analysis. It\n# supports qualified module names, as well as Unix pattern matching.\nignored-modules=\n\n# Show a hint with possible names when a member name was not found. The aspect\n# of finding the hint is based on edit distance.\nmissing-member-hint=yes\n\n# The minimum edit distance a name should have in order to be considered a\n# similar match for a missing member name.\nmissing-member-hint-distance=1\n\n# The total number of similar names that should be taken in consideration when\n# showing a hint for a missing member.\nmissing-member-max-choices=1\n\n\n[VARIABLES]\n\n# List of additional names supposed to be defined in builtins. Remember that\n# you should avoid to define new builtins when possible.\nadditional-builtins=\n\n# Tells whether unused global variables should be treated as a violation.\nallow-global-unused-variables=yes\n\n# List of strings which can identify a callback function by name. A callback\n# name must start or end with one of those strings.\ncallbacks=cb_,\n          _cb\n\n# A regular expression matching the name of dummy variables (i.e. expectedly\n# not used).\ndummy-variables-rgx=_+$|(_[a-zA-Z0-9_]*[a-zA-Z0-9]+?$)|dummy|^ignored_|^unused_\n\n# Argument names that match this expression will be ignored. Default to name\n# with leading underscore\nignored-argument-names=_.*|^ignored_|^unused_\n\n# Tells whether we should check for unused import in __init__ files.\ninit-import=no\n\n# List of qualified module names which can have objects that can redefine\n# builtins.\nredefining-builtins-modules=six.moves,past.builtins,future.builtins\n\n\n[FORMAT]\n\n# Expected format of line ending, e.g. empty (any line ending), LF or CRLF.\nexpected-line-ending-format=\n\n# Regexp for a line that is allowed to be longer than the limit.\nignore-long-lines=^\\s*(# )?<?https?://\\S+>?$\n\n# Number of spaces of indent required inside a hanging  or continued line.\nindent-after-paren=4\n\n# String used as indentation unit. This is usually \"    \" (4 spaces) or \"\\t\" (1\n# tab).\nindent-string='    '\n\n# Maximum number of characters on a single line.\nmax-line-length=100\n\n# Maximum number of lines in a module\nmax-module-lines=1000\n\n# List of optional constructs for which whitespace checking is disabled. `dict-\n# separator` is used to allow tabulation in dicts, etc.: {1  : 1,\\n222: 2}.\n# `trailing-comma` allows a space between comma and closing bracket: (a, ).\n# `empty-line` allows space-only lines.\nno-space-check=trailing-comma,\n               dict-separator\n\n# Allow the body of a class to be on the same line as the declaration if body\n# contains single statement.\nsingle-line-class-stmt=no\n\n# Allow the body of an if to be on the same line as the test if there is no\n# else.\nsingle-line-if-stmt=no\n\n\n[SIMILARITIES]\n\n# Ignore comments when computing similarities.\nignore-comments=yes\n\n# Ignore docstrings when computing similarities.\nignore-docstrings=yes\n\n# Ignore imports when computing similarities.\nignore-imports=no\n\n# Minimum lines number of a similarity.\nmin-similarity-lines=4\n\n\n[BASIC]\n\n# Naming style matching correct argument names\nargument-naming-style=snake_case\n\n# Regular expression matching correct argument names. Overrides argument-\n# naming-style\n#argument-rgx=\n\n# Naming style matching correct attribute names\nattr-naming-style=snake_case\n\n# Regular expression matching correct attribute names. Overrides attr-naming-\n# style\n#attr-rgx=\n\n# Bad variable names which should always be refused, separated by a comma\nbad-names=foo,\n          bar,\n          baz,\n          toto,\n          tutu,\n          tata\n\n# Naming style matching correct class attribute names\nclass-attribute-naming-style=any\n\n# Regular expression matching correct class attribute names. Overrides class-\n# attribute-naming-style\n#class-attribute-rgx=\n\n# Naming style matching correct class names\nclass-naming-style=PascalCase\n\n# Regular expression matching correct class names. Overrides class-naming-style\n#class-rgx=\n\n# Naming style matching correct constant names\nconst-naming-style=UPPER_CASE\n\n# Regular expression matching correct constant names. Overrides const-naming-\n# style\n#const-rgx=\n\n# Minimum line length for functions/classes that require docstrings, shorter\n# ones are exempt.\ndocstring-min-length=-1\n\n# Naming style matching correct function names\nfunction-naming-style=snake_case\n\n# Regular expression matching correct function names. Overrides function-\n# naming-style\n#function-rgx=\n\n# Good variable names which should always be accepted, separated by a comma\ngood-names=i,\n           j,\n           k,\n           x,\n           y,\n           ex,\n           Run,\n           _\n\n# Include a hint for the correct naming format with invalid-name\ninclude-naming-hint=no\n\n# Naming style matching correct inline iteration names\ninlinevar-naming-style=any\n\n# Regular expression matching correct inline iteration names. Overrides\n# inlinevar-naming-style\n#inlinevar-rgx=\n\n# Naming style matching correct method names\nmethod-naming-style=snake_case\n\n# Regular expression matching correct method names. Overrides method-naming-\n# style\n#method-rgx=\n\n# Naming style matching correct module names\nmodule-naming-style=snake_case\n\n# Regular expression matching correct module names. Overrides module-naming-\n# style\n#module-rgx=\n\n# Colon-delimited sets of names that determine each other's naming style when\n# the name regexes allow several styles.\nname-group=\n\n# Regular expression which should only match function or class names that do\n# not require a docstring.\nno-docstring-rgx=^_\n\n# List of decorators that produce properties, such as abc.abstractproperty. Add\n# to this list to register other decorators that produce valid properties.\nproperty-classes=abc.abstractproperty\n\n# Naming style matching correct variable names\nvariable-naming-style=snake_case\n\n# Regular expression matching correct variable names. Overrides variable-\n# naming-style\n#variable-rgx=\n\n\n[IMPORTS]\n\n# Allow wildcard imports from modules that define __all__.\nallow-wildcard-with-all=no\n\n# Analyse import fallback blocks. This can be used to support both Python 2 and\n# 3 compatible code, which means that the block might have code that exists\n# only in one or another interpreter, leading to false positives when analysed.\nanalyse-fallback-blocks=no\n\n# Deprecated modules which should not be used, separated by a comma\ndeprecated-modules=optparse,tkinter.tix\n\n# Create a graph of external dependencies in the given file (report RP0402 must\n# not be disabled)\next-import-graph=\n\n# Create a graph of every (i.e. internal and external) dependencies in the\n# given file (report RP0402 must not be disabled)\nimport-graph=\n\n# Create a graph of internal dependencies in the given file (report RP0402 must\n# not be disabled)\nint-import-graph=\n\n# Force import order to recognize a module as part of the standard\n# compatibility libraries.\nknown-standard-library=\n\n# Force import order to recognize a module as part of a third party library.\nknown-third-party=enchant\n\n\n[CLASSES]\n\n# List of method names used to declare (i.e. assign) instance attributes.\ndefining-attr-methods=__init__,\n                      __new__,\n                      setUp\n\n# List of member names, which should be excluded from the protected access\n# warning.\nexclude-protected=_asdict,\n                  _fields,\n                  _replace,\n                  _source,\n                  _make\n\n# List of valid names for the first argument in a class method.\nvalid-classmethod-first-arg=cls\n\n# List of valid names for the first argument in a metaclass class method.\nvalid-metaclass-classmethod-first-arg=mcs\n\n\n[DESIGN]\n\n# Maximum number of arguments for function / method\nmax-args=5\n\n# Maximum number of attributes for a class (see R0902).\nmax-attributes=7\n\n# Maximum number of boolean expressions in a if statement\nmax-bool-expr=5\n\n# Maximum number of branch for function / method body\nmax-branches=12\n\n# Maximum number of locals for function / method body\nmax-locals=15\n\n# Maximum number of parents for a class (see R0901).\nmax-parents=7\n\n# Maximum number of public methods for a class (see R0904).\nmax-public-methods=20\n\n# Maximum number of return / yield for function / method body\nmax-returns=6\n\n# Maximum number of statements in function / method body\nmax-statements=50\n\n# Minimum number of public methods for a class (see R0903).\nmin-public-methods=2\n\n\n[EXCEPTIONS]\n\n# Exceptions that will emit a warning when being caught. Defaults to\n# \"Exception\"\novergeneral-exceptions=Exception\n"
        },
        {
          "name": "CODE_OF_CONDUCT.md",
          "type": "blob",
          "size": 5.185546875,
          "content": "# TensorFlow Code of Conduct\n\nIn the interest of fostering an open and welcoming environment, we as contributors and maintainers pledge to making participation in our project and our community a harassment-free experience for everyone, regardless of age, body size, disability, ethnicity, gender identity and expression, level of experience, nationality, personal appearance, race, religion, or sexual identity and orientation.\n\n\n## Our Standards\n\nExamples of behavior that contributes to creating a positive environment include:\n\n* Using welcoming and inclusive language\n* Being respectful of differing viewpoints and experiences\n* Gracefully accepting constructive criticism\n* Focusing on what is best for the community\n* Showing empathy towards other community members\n\nExamples of unacceptable behavior by participants include:\n\n* The use of sexualized language or imagery and unwelcome sexual attention or advances\n* Trolling, insulting/derogatory comments, and personal or political attacks\n* Public or private harassment\n* Publishing others' private information, such as a physical or electronic address, without explicit permission\n* Conduct which could reasonably be considered inappropriate for the forum in which it occurs. \n\nAll TensorFlow forums and spaces are meant for professional interactions, and any behavior which could reasonably be considered inappropriate in a professional setting is unacceptable.\n\n\n## Our Responsibilities\n\nProject maintainers are responsible for clarifying the standards of acceptable behavior and are expected to take appropriate and fair corrective action in response to any instances of unacceptable behavior.\n\nProject maintainers have the right and responsibility to remove, edit, or reject comments, commits, code, wiki edits, issues, and other contributions that are not aligned to this Code of Conduct, or to ban temporarily or permanently any contributor for other behaviors that they deem inappropriate, threatening, offensive, or harmful.\n\n\n## Scope\n\nThis Code of Conduct applies to all content on tensorflow.org, TensorFlow’s GitHub organization, or any other official TensorFlow web presence allowing for community interactions, as well as at all official TensorFlow events, whether offline or online.\n\nThe Code of Conduct also applies within project spaces and in public spaces whenever an individual is representing TensorFlow or its community. Examples of representing a project or community include using an official project e-mail address, posting via an official social media account, or acting as an appointed or de facto representative at an online or offline event. \n\n\n## Conflict Resolution\n\nConflicts in an open source project can take many forms, from someone having a bad day and using harsh and hurtful language in the issue queue, to more serious instances such as sexist/racist statements or threats of violence, and everything in between.\n\nIf the behavior is threatening or harassing, or for other reasons requires immediate escalation, please see below.\n\nHowever, for the vast majority of issues, we aim to empower individuals to first resolve conflicts themselves, asking for help when needed, and only after that fails to escalate further. This approach gives people more control over the outcome of their dispute. \n\nIf you are experiencing or witnessing conflict, we ask you to use the following escalation strategy to address the conflict:\n\n1. Address the perceived conflict directly with those involved, preferably in a real-time medium. \n2. If this fails, get a third party (e.g. a mutual friend, and/or someone with background on the issue, but not involved in conflict) to intercede.\n3. If you are still unable to resolve the conflict, and you believe it rises to harassment or another code of conduct violation, report it.\n\n\n## Reporting Violations\n\nViolations of the Code of Conduct can be reported to TensorFlow’s Project Stewards, Edd Wilder-James (ewj@google.com) and Sarah Novotny (sarahnovotny@google.com). The Project Steward will determine whether the Code of Conduct was violated, and will issue an appropriate sanction, possibly including a written warning or expulsion from the project, project sponsored spaces, or project forums. We ask that you make a good-faith effort to resolve your conflict via the conflict resolution policy before submitting a report.\n\nViolations of the Code of Conduct can occur in any setting, even those unrelated to the project. We will only consider complaints about conduct that has occurred within one year of the report.\n\n\n## Enforcement\n\nIf the Project Stewards receive a report alleging a violation of the Code of Conduct, the Project Stewards will notify the accused of the report, and provide them an opportunity to discuss the report before a sanction is issued. The Project Stewards will do their utmost to keep the reporter anonymous. If the act is ongoing (such as someone engaging in harassment), or involves a threat to anyone's safety (e.g. threats of violence), the Project Stewards may issue sanctions without notice.\n\n\n## Attribution\n\nThis Code of Conduct is adapted from the Contributor Covenant, version 1.4, available at https://contributor-covenant.org/version/1/4, and includes some aspects of the Geek Feminism Code of Conduct and the Drupal Code of Conduct.\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.302734375,
          "content": "# How to Contribute\n\nWe'd love to accept your patches and contributions to this project. There are\njust a few small guidelines you need to follow.\n\n## Contributor License Agreement\n\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to <https://cla.developers.google.com/> to see\nyour current agreements on file or to sign a new one.\n\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\n\n## Code reviews\n\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Consult\n[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more\ninformation on using pull requests.\n\n**Some Minigo-specific Expectations**:\n\n*   We format all our python code with\n    [autopep8](https://pypi.python.org/pypi/autopep8). If you use Vim, check\n    out [Vim-CodeFmt](https://github.com/google/vim-codefmt).\n*   We use [pylint](https://www.pylint.org/) to check the syntax of our code.\n    Not all of our code yet passes pylint, but we're working on it!\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 10.517578125,
          "content": "                                 Apache License\n                           Version 2.0, January 2004\n                        http://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   Copyright 2017 Brian Lee, Andrew Jackson\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       http://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 16.666015625,
          "content": "Minigo: A minimalist Go engine modeled after AlphaGo Zero, built on MuGo\n==================================================\n\nThis is an implementation of a neural-network based Go AI, using TensorFlow.\nWhile inspired by DeepMind's AlphaGo algorithm, this project is not\na DeepMind project nor is it affiliated with the official AlphaGo project.\n\n### This is NOT an official version of AlphaGo ###\n\nRepeat, *this is not the official AlphaGo program by DeepMind*.  This is an\nindependent effort by Go enthusiasts to replicate the results of the AlphaGo\nZero paper (\"Mastering the Game of Go without Human Knowledge,\" *Nature*), with\nsome resources generously made available by Google.\n\nMinigo is based off of Brian Lee's \"[MuGo](https://github.com/brilee/MuGo)\"\n-- a pure Python implementation of the first AlphaGo paper\n[\"Mastering the Game of Go with Deep Neural Networks and\nTree Search\"](https://www.nature.com/articles/nature16961) published in\n*Nature*. This implementation adds features and architecture changes present in\nthe more recent AlphaGo Zero paper, [\"Mastering the Game of Go without Human\nKnowledge\"](https://www.nature.com/articles/nature24270). More recently, this\narchitecture was extended for Chess and Shogi in [\"Mastering Chess and Shogi by\nSelf-Play with a General Reinforcement Learning\nAlgorithm\"](https://arxiv.org/abs/1712.01815).  These papers will often be\nabridged in Minigo documentation as *AG* (for AlphaGo), *AGZ* (for AlphaGo\nZero), and *AZ* (for AlphaZero) respectively.\n\n\nGoals of the Project\n==================================================\n\n1. Provide a clear set of learning examples using Tensorflow, Kubernetes, and\n   Google Cloud Platform for establishing Reinforcement Learning pipelines on\n   various hardware accelerators.\n\n2. Reproduce the methods of the original DeepMind AlphaGo papers as faithfully\n   as possible, through an open-source implementation and open-source pipeline\n   tools.\n\n3. Provide our data, results, and discoveries in the open to benefit the Go,\n   machine learning, and Kubernetes communities.\n\nAn explicit non-goal of the project is to produce a competitive Go program that\nestablishes itself as the top Go AI. Instead, we strive for a readable,\nunderstandable implementation that can benefit the community, even if that\nmeans our implementation is not as fast or efficient as possible.\n\nWhile this product might produce such a strong model, we hope to focus on the\nprocess.  Remember, getting there is half the fun. :)\n\nWe hope this project is an accessible way for interested developers to have\naccess to a strong Go model with an easy-to-understand platform of python code\navailable for extension, adaptation, etc.\n\nIf you'd like to read about our experiences training models, see [RESULTS.md](RESULTS.md).\n\nTo see our guidelines for contributing, see [CONTRIBUTING.md](CONTRIBUTING.md).\n\nGetting Started\n===============\n\nThis project assumes you have the following:\n\n- virtualenv / virtualenvwrapper\n- Python 3.5+\n- [Docker](https://docs.docker.com/install/)\n- [Cloud SDK](https://cloud.google.com/sdk/downloads)\n\nThe [Hitchhiker's guide to\npython](http://docs.python-guide.org/en/latest/dev/virtualenvs/) has a good\nintro to python development and virtualenv usage. The instructions after this\npoint haven't been tested in environments that are not using virtualenv.\n\n```shell\npip3 install virtualenv\npip3 install virtualenvwrapper\n```\n\nInstall Bazel\n------------------\n\n```shell\nBAZEL_VERSION=0.24.1\nwget https://github.com/bazelbuild/bazel/releases/download/${BAZEL_VERSION}/bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh\nchmod 755 bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh\nsudo ./bazel-${BAZEL_VERSION}-installer-linux-x86_64.sh\n```\n\nInstall TensorFlow\n------------------\nFirst set up and enter your virtualenv and then the shared requirements:\n\n```\npip3 install -r requirements.txt\n```\n\nThen, you'll need to choose to install the GPU or CPU tensorflow requirements:\n\n- GPU: `pip3 install \"tensorflow-gpu==1.15.0\"`.\n  - *Note*: You must install [CUDA 10.0](https://developer.nvidia.com/cuda-10.0-download-archive). for Tensorflow\n    1.13.0+.\n- CPU: `pip3 install \"tensorflow==1.15.0\"`.\n\nSetting up the Environment\n--------------------------\n\nYou may want to use a cloud project for resources. If so set:\n\n```shell\nPROJECT=foo-project\n```\n\nThen, running\n\n```shell\nsource cluster/common.sh\n```\n\nwill set up other environment variables defaults.\n\nRunning unit tests\n------------------\n```\n./test.sh\n```\n\nTo run individual modules\n\n```\nBOARD_SIZE=9 python3 tests/run_tests.py test_go\nBOARD_SIZE=19 python3 tests/run_tests.py test_mcts\n```\n\nAutomated Tests\n----------------\n\n[Test Dashboard](https://k8s-testgrid.appspot.com/sig-big-data#tf-minigo-presubmit)\n\nTo automatically test PRs, Minigo uses\n[Prow](https://github.com/kubernetes/test-infra/tree/master/prow), which is a\ntest framework created by the Kubernetes team for testing changes in a hermetic\nenvironment. We use prow for running unit tests, linting our code, and\nlaunching our test Minigo Kubernetes clusters.\n\nYou can see the status of our automated tests by looking at the Prow and\nTestgrid UIs:\n\n- Testgrid (Test Results Dashboard): https://k8s-testgrid.appspot.com/sig-big-data\n- Prow (Test-runner dashboard): https://prow.k8s.io/?repo=tensorflow%2Fminigo\n\nBasics\n======\n\nAll commands are compatible with either Google Cloud Storage as a remote file\nsystem, or your local file system. The examples here use GCS, but local file\npaths will work just as well.\n\nTo use GCS, set the `BUCKET_NAME` variable and authenticate via `gcloud login`.\nOtherwise, all commands fetching files from GCS will hang.\n\nFor instance, this would set a bucket, authenticate, and then look for the most\nrecent model.\n\n```shell\n# When you first start we recommend using our minigo-pub bucket.\n# Later you can setup your own bucket and store data there.\nexport BUCKET_NAME=minigo-pub/v9-19x19\ngcloud auth application-default login\ngsutil ls gs://$BUCKET_NAME/models | tail -4\n```\n\nWhich might look like:\n\n```\ngs://$BUCKET_NAME/models/000737-fury.data-00000-of-00001\ngs://$BUCKET_NAME/models/000737-fury.index\ngs://$BUCKET_NAME/models/000737-fury.meta\ngs://$BUCKET_NAME/models/000737-fury.pb\n```\n\nThese four files comprise the model. Commands that take a model as an\nargument usually need the path to the model basename, e.g.\n`gs://$BUCKET_NAME/models/000737-fury`\n\nYou'll need to copy them to your local disk.  This fragment copies the files\nassociated with `$MODEL_NAME` to the directory specified by `MINIGO_MODELS`:\n\n```shell\nMODEL_NAME=000737-fury\nMINIGO_MODELS=$HOME/minigo-models\nmkdir -p $MINIGO_MODELS/models\ngsutil ls gs://$BUCKET_NAME/models/$MODEL_NAME.* | \\\n       gsutil cp -I $MINIGO_MODELS/models\n```\n\nSelfplay\n--------\nTo watch Minigo play a game, you need to specify a model. Here's an example\nto play using the latest model in your bucket\n\n```shell\npython3 selfplay.py \\\n  --verbose=2 \\\n  --num_readouts=400 \\\n  --load_file=$MINIGO_MODELS/models/$MODEL_NAME\n```\n\nwhere `READOUTS` is how many searches to make per move.  Timing information and\nstatistics will be printed at each move.  Setting verbosity to 3 or\nhigher will print a board at each move.\n\nPlaying Against Minigo\n----------------------\n\nMinigo uses the\n[GTP Protocol](http://www.lysator.liu.se/~gunnar/gtp/gtp2-spec-draft2/gtp2-spec.html),\nand you can use any gtp-compliant program with it.\n\n```shell\n# Latest model should look like: /path/to/models/000123-something\nLATEST_MODEL=$(ls -d $MINIGO_MODELS/* | tail -1 | cut -f 1 -d '.')\npython3 gtp.py --load_file=$LATEST_MODEL --num_readouts=$READOUTS --verbose=3\n```\n\nAfter some loading messages, it will display `GTP engine ready`, at which point\nit can receive commands.  GTP cheatsheet:\n\n```\ngenmove [color]             # Asks the engine to generate a move for a side\nplay [color] [coordinate]   # Tells the engine that a move should be played for `color` at `coordinate`\nshowboard                   # Asks the engine to print the board.\n```\n\nOne way to play via GTP is to use gogui-display (which implements a UI that\nspeaks GTP.) You can download the gogui set of tools at\n[http://gogui.sourceforge.net/](http://gogui.sourceforge.net/). See also\n[documentation on interesting ways to use\nGTP](http://gogui.sourceforge.net/doc/reference-twogtp.html).\n\n```shell\ngogui-twogtp -black 'python3 gtp.py --load_file=$LATEST_MODEL' -white 'gogui-display' -size 19 -komi 7.5 -verbose -auto\n```\n\nAnother way to play via GTP is to watch it play against GnuGo, while\nspectating the games:\n\n```shell\nBLACK=\"gnugo --mode gtp\"\nWHITE=\"python3 gtp.py --load_file=$LATEST_MODEL\"\nTWOGTP=\"gogui-twogtp -black \\\"$BLACK\\\" -white \\\"$WHITE\\\" -games 10 \\\n  -size 19 -alternate -sgffile gnugo\"\ngogui -size 19 -program \"$TWOGTP\" -computer-both -auto\n```\n\nTraining Minigo\n======================\n\nOverview\n--------\n\nThe following sequence of commands will allow you to do one iteration of\nreinforcement learning on 9x9. These are the basic commands used to produce the\nmodels and games referenced above.\n\nThe commands are\n\n - bootstrap: initializes a random model\n - selfplay: plays games with the latest model, producing data used for training\n - train: trains a new model with the selfplay results from the most recent N\n   generations.\n\nTraining works via tf.Estimator; a working directory manages checkpoints and\ntraining logs, and the latest checkpoint is periodically exported to GCS, where\nit gets picked up by selfplay workers.\n\nConfiguration for things like \"where do debug SGFs get written\", \"where does\ntraining data get written\", \"where do the latest models get published\" are\nmanaged by the helper scripts in the rl\\_loop directory. Those helper scripts\nexecute the same commands as demonstrated below. Configuration for things like\n\"what size network is being used?\" or \"how many readouts during selfplay\" can\nbe passed in as flags. The mask\\_flags.py utility helps ensure all parts of the\npipeline are using the same network configuration.\n\nAll local paths in the examples can be replaced with `gs://` GCS paths, and the\nKubernetes-orchestrated version of the reinforcement learning loop uses GCS.\n\nBootstrap\n---------\n\nThis command initializes your working directory for the trainer and a random\nmodel. This random model is also exported to `--model-save-path` so that\nselfplay can immediately start playing with this random model.\n\nIf these directories don't exist, bootstrap will create them for you.\n\n```shell\nexport MODEL_NAME=000000-bootstrap\npython3 bootstrap.py \\\n  --work_dir=estimator_working_dir \\\n  --export_path=outputs/models/$MODEL_NAME\n```\n\nSelf-play\n---------\n\nThis command starts self-playing, outputting its raw game data as tf.Examples\nas well as in SGF form in the directories.\n\n\n```shell\npython3 selfplay.py \\\n  --load_file=outputs/models/$MODEL_NAME \\\n  --num_readouts 10 \\\n  --verbose 3 \\\n  --selfplay_dir=outputs/data/selfplay \\\n  --holdout_dir=outputs/data/holdout \\\n  --sgf_dir=outputs/sgf\n```\n\nTraining\n--------\n\nThis command takes a directory of tf.Example files from selfplay and trains a\nnew model, starting from the latest model weights in the `estimator_working_dir`\nparameter.\n\nRun the training job:\n\n```shell\npython3 train.py \\\n  outputs/data/selfplay/* \\\n  --work_dir=estimator_working_dir \\\n  --export_path=outputs/models/000001-first_generation\n```\n\nAt the end of training, the latest checkpoint will be exported to.\nAdditionally, you can follow along with the training progress with TensorBoard.\nIf you point TensorBoard at the estimator working directory, it will find the\ntraining log files and display them.\n\n```shell\ntensorboard --logdir=estimator_working_dir\n```\n\nValidation\n----------\n\nIt can be useful to set aside some games to use as a 'validation set' for\ntracking the model overfitting.  One way to do this is with the `validate`\ncommand.\n\n### Validating on holdout data\n\nBy default, Minigo will hold out 5% of selfplay games for validation. This can\nbe changed by adjusting the `holdout_pct` flag on the `selfplay` command.\n\nWith this setup, `rl_loop/train_and_validate.py` will validate on the same\nwindow of games that were used to train, writing TensorBoard logs to the\nestimator working directory.\n\n### Validating on a different set of data\n\nThis might be useful if you have some known set of 'good data' to test your\nnetwork against, e.g., a set of pro games.\nAssuming you've got a set of .sgfs with the proper komi & boardsizes, you'll\nwant to preprocess them into the .tfrecord files, by running something similar\nto\n\n```python\nimport preprocessing\nfilenames = [generate a list of filenames here]\nfor f in filenames:\n    try:\n        preprocessing.make_dataset_from_sgf(f, f.replace(\".sgf\", \".tfrecord.zz\"))\n    except:\n        print(f)\n```\n\nOnce you've collected all the files in a directory, producing validation is as\neasy as\n\n```shell\npython3 validate.py \\\n  validation_files/ \\\n  --work_dir=estimator_working_dir \\\n  --validation_name=pro_dataset\n```\n\nThe validate.py will glob all the .tfrecord.zz files under the\ndirectories given as positional arguments and compute the validation error\nfor the positions from those files.\n\n\nRetraining a model\n======================\n\nThe training data for most of Minigo's models up to v13 is publicly available in\nthe `minigo-pub` Cloud storage bucket, e.g.:\n\n```shell\ngsutil ls gs://minigo-pub/v13-19x19/data/golden_chunks/\n```\n\nFor models v14 and onwards, we started using Cloud BigTable and are still\nworking on making that data public.\n\nHere's how to retrain your own model from this source data using a Cloud TPU:\n\n```shell\n# I wrote these notes using our existing TPU-enabled project, so they're missing\n# a few preliminary steps, like setting up a Cloud account, creating a project,\n# etc. New users will also need to enable Cloud TPU on their project using the\n# TPUs panel.\n\n###############################################################################\n\n# Note that you will be billed for any storage you use and also while you have\n# VMs running. Remember to shut down your VMs when you're not using them!\n\n# To use a Cloud TPU on GCE, you need to create a special TPU-enabled VM using\n# the `ctpu` tool. First, set up some environment variables:\n#   GCE_PROJECT=<your project name>\n#   GCE_VM_NAME=<your VM's name>\n#   GCE_ZONE<the zone in which you want to bring uo your VM, e.g. us-central1-f>\n\n# In this example, we will use the following values:\nGCE_PROJECT=example-project\nGCE_VM_NAME=minigo-etpu-test\nGCE_ZONE=us-central1-f\n\n# Create the Cloud TPU enabled VM.\nctpu up \\\n  --project=\"${GCE_PROJECT}\" \\\n  --zone=\"${GCE_ZONE}\" \\\n  --name=\"${GCE_VM_NAME}\" \\\n  --tf-version=1.13\n\n# This will take a few minutes and you should see output similar to the\n# following:\n#   ctpu will use the following configuration values:\n#         Name:                 minigo-etpu-test\n#         Zone:                 us-central1-f\n#         GCP Project:          example-project\n#         TensorFlow Version:   1.13\n#  OK to create your Cloud TPU resources with the above configuration? [Yn]: y\n#  2019/04/09 10:50:04 Creating GCE VM minigo-etpu-test (this may take a minute)...\n#  2019/04/09 10:50:04 Creating TPU minigo-etpu-test (this may take a few minutes)...\n#  2019/04/09 10:50:11 GCE operation still running...\n#  2019/04/09 10:50:12 TPU operation still running...\n\n# Once the Cloud TPU is created, `ctpu` will have SSHed you into the machine.\n\n# Remember to set the same environment variables on your VM.\nGCE_PROJECT=example-project\nGCE_VM_NAME=minigo-etpu-test\nGCE_ZONE=us-central1-f\n\n# Clone the Minigo Github repository:\ngit clone --depth 1 https://github.com/tensorflow/minigo\ncd minigo\n\n# Install virtualenv.\npip3 install virtualenv virtualenvwrapper\n\n# Create a virtual environment\nvirtualenv -p /usr/bin/python3 --system-site-packages \"${HOME}/.venvs/minigo\"\n\n# Activate the virtual environment.\nsource \"${HOME}/.venvs/minigo/bin/activate\"\n\n# Install Minigo dependencies (TensorFlow for Cloud TPU is already installed as\n# part of the VM image).\npip install -r requirements.txt\n\n# When training on a Cloud TPU, the training work directory must be on Google Cloud Storage.\n# You'll need to choose your own globally unique bucket name.\n# The bucket location should be close to your VM.\nGCS_BUCKET_NAME=minigo_test_bucket\nGCE_BUCKET_LOCATION=us-central1\ngsutil mb -p \"${GCE_PROJECT}\" -l \"${GCE_BUCKET_LOCATION}\" \"gs://${GCS_BUCKET_NAME}\"\n\n# Run the training script and note the location of the training work_dir\n# it reports, e.g.\n#    Writing to gs://minigo_test_bucket/train/2019-04-25-18\n./oneoffs/train.sh \"${GCS_BUCKET_NAME}\"\n\n# Launch tensorboard, pointing it at the work_dir reported by the train.sh script.\ntensorboard --logdir=gs://minigo_test_bucket/train/2019-04-25-18\n\n# After a few minutes, TensorBoard should start updating.\n# Interesting graphs to look at are value_cost_normalized, policy_cost and policy_entropy.\n```\n\nRunning Minigo on a Kubernetes Cluster\n==============================\n\nSee more at [cluster/README.md](https://github.com/tensorflow/minigo/tree/master/cluster/README.md)\n"
        },
        {
          "name": "RESULTS.md",
          "type": "blob",
          "size": 37.822265625,
          "content": "# Results\n\n## First run, 9x9, November 2017.\n\n - Ran on a cluster of ~1k cores, no GPUs, 400 readouts/move, 64 games played in\n   parallel per worker.  Trained on a single P100 machine.\n   - No tree re-use\n   - No 'resign disabled' holdout\n   - No vectorized updates\n   - Dir noise probably parameterized wrong\n   - `c_puct` a guess at 2.5\n   - \"Stretch\" for moves after the opening\n   - No evaluation of new networks\n   - [Init Q to 0]( http://computer-go.org/pipermail/computer-go/2017-December/010555.html)\n   - Hardcoded not to pass early on (before move 20 i think).\n - Ran for two weeks\n - Evaluated by playing a variety of kyu & dan players, judged to be SDK\n\nThis first run was largely intended to confirm that the network and MCTS code\nworked and that the algorithm as described would improve.  As such, the CPU-only\nversion on the smaller board was a modest success.  Even without a number of\nenhancements described in the paper (re-use, etc), play was observed to improve\nrapidly towards reasonable human play.\n\nWe didn't see any 'ladder weakness', probably due to the smaller board size.\n\nWhat's really shocking is how effectively this worked even with many parts of\nthe code that were absolutely wrong.  In particular, the batch normalization\ncalls (`tf.layers.batch_normalization`) were entirely wrong, and the lack of\ncorrect batch normalization almost certainly slowed down progress by an order of\nmagnitude.\n\nAs a result, it's hard to say how effective it was at proving things were\ncorrect.  The results were good enough to assume that everything would work fine\nwhen scaled up to 19x19, but obviously that wasn't true.  As a result, it was\nhard to experiment on 9x9s -- If it worked on 9x9s when broken, who was to say\nthat \"working on 9s\" was useful or predictive?\n\nRun was stopped as a result of internal resource allocation discussions.\n\n## Second run, 'somebot', 19x19, Dec'17-Jan'18\n\nThis second run took place over about 4 weeks, on a 20 block network with 128\nfilters.  With 4 weeks to look at it and adjust things, we didn't end up keeping\nthe code constant the whole time through, and instead we were constantly\nmonkeying with the code.\n\nThis ran on a Google Container Engine cluster of pre-release preemptible GPUs;\n(TODO: link) Hopefully the product was improved by our bug reports :)\n\n### Adjustments\n\nGenerations 0-165 played about 12,500 games each, and each model/checkpoint\nwas trained on about 1M positions (compare to DM's 2M positions in AGZ paper).\n\nAfter transitioning to a larger board size, and a cluster with GPUs, we started\na new run.  Very quickly, we realized that the batch normalization was incorrect\non the layers of the value head.  After around 500k games, we realized the\npolicy head should also have had its batchnorm fixed.  This coincided with a\n*really* sharp improvement in policy error.\n\nAround model 95, we realized the [initial\nQ-value](http://computer-go.org/pipermail/computer-go/2017-December/010555.html)\nled to weird ping-ponging behavior inside search, and changed to init to the\naverage of the parent.\n\nAround model 148, we realized we had the learning rate decay time off by an\norder of magnitude...oops.  We also started experimenting with different values\nfor `c_puct`, but didn't really see a difference for our range of values between\n0.8 and 2.0.\n\nAround model 165, we changed how many games we tried to play per model, from\n12,500 games to 7,000 games, and also increased how many positions each\nmodel was trained on, from 1M to 2M.  This was largely due to how we had\noriginally read the AGZ paper, where they say they had played 25,000 games with\nthe most recent model.  However, with 5M games and 700 checkpoints, that works\nout to about 7,000 games/checkpoint.  As we were doing more of an 'AZ' approach,\nwith no evaluation of the new models, we figured this was more accurate.\n\nAround model 230, we disabled resignation in 5% of games and checked our\nresign-threshold false positive rate.  We'd been pretty accurate, having tuned\nit down to about .9 (i.e., odds of winning at 5%) based on our analysis of true\nnegatives.  (i.e., what was the distribution of the closest a player got to\nresigning who ended up winning)\n\nAround model 250, we started experimenting with tree reuse to gather data about\nits effect on the Dirichlet noise, and if it was affecting our position\ndiversity.\n\nWe had also been stretching the probabilities pi by raising to 8th power\nbefore training on them, as a compromise between training on the one-hot\nvector versus the original MCTS visit distribution.\n\nThere's still ambiguity about whether or not a one-hot training target for the\npolicy net is good or bad, but we're leaning towards leaving pi as the softmax\nof visits for training, while using argmax as the method for actually choosing\nthe move for play (after the fuseki).\n\n### Observations\n\nWe put the models on KGS and CGOS as 'somebot' starting around version 160.  The\nharness running these models is very primitive: they are single-threaded python,\nrunning nodes through the accelerator with a batch-size of ONE... very bad.  The\nCGOS harness only manages to run around 300-500 searches per move (5s/move).\n\nHowever, with that said, we were able to watch it improve and reach a level\nwhere it would consistently lose to the top bots (Zen, AQ, Leela 0.11), and\nconsistently beat the basic bots, GnuGo, michi, the Aya placeholder bots, etc.\nWe found CGOS not particularly useful because of this large gap. Additionally,\nthe large population of LeelaZero variants all had similar playstyles, further\ndistorting the rating system.\n\nSometime between v180 and v230, our CGOS results seemed to level off.  The\nladder weakness meant our performance on CGOS was split: either our opponents\ncould ladder us correctly, or they couldn't :)  As such, it was hard to use CGOS\nas an objective measurement.  Similarly, while we were able to notch some wins\nagainst KGS dan players, it was hard to tell if we were still making progress.\nThis led to our continued twiddling of settings and general sense of\nfrustration.  The ladder weakness continued strong all the way to the very end,\naround model v280, when our prerelease cluster expired.\n\nOn the whole, we did feel pretty good about having answered some of the\nambiguities left by the papers:  What was a good value for `c_puct`?  Was\ntree-reuse for evaluation only, or also for selfplay?  What was up with\ninitializing a child's Q to zero?  How about whether or not to one-hot the\npolicy target pi?\n\nWorth observing:  For nearly all of the generations, 3-3 was its ONLY preferred\nopening move.  This drove a lot of the questions around whether or not we had\nadequate move diversity, sufficient noise mixing, etc.  It is unclear whether\nthe early errors (e.g., initial 50-100 generations with broken batch-norm) have\nleft us in a local minima with 3-3s that D-noise & etc. are not able to\novercome.\n\n\n## Third run, Minigo, model 250-..., Jan 20th-Feb 1st (ish)\n\nAs our fiddling about after model 250 didn't seem to get us anywhere, and due to\nour reconsidering about whether or not to one-hot the policy network target\n('pi'), we rolled back to model 250 to continue training.\n\nWe also completely replaced our custom class for handling training examples\n(helpfully called `Dataset`, and predating tf.data.Dataset) with a proper\nimplementation using tfExamples and living natively on GCS.  This greatly\nsimplified the training pipeline, with a few hiccups at the beginning where the\ntraining data was not adequately diversified.\n\nThe remaining differences between this run and the described paper:\n\n1. No \"virtual losses\"\n  - Virtual losses provide a way to parallelize MCTS and choose more nodes for\n    parallel evaluation by the accelerator.  Instead of this, we parallelize by\n    playing multiple games per-worker.  The overall throughput is the same, in\n    terms of games-per-hour, but we could increase the speed at which games are\n    finished and thus the overall recency of the training data if we implemented\n    virtual losses.\n  - A side effect of virtual losses is that the MCTS phase is not always\n    evaluating the optimal leaf, but is evaluating instead its top-8 or top-16\n    or whatever the batch size is.  This has a consequence of broadening the\n    overall search -- is this broadening important to the discovery of new moves\n    by the policy network, and thus the improvement of the network overall?  Who\n    knows? :)\n2. Ambiguity around the training target - should we be trying to predict the\n  MCTS visit distribution pi, or the one-hot representation of the final move\n  picked? What about in the early game when softpick didn't necessarily pick the\n  move with the most visits?\n3. Too few filters (128 vs 256).\n\n\nResults: After a few days a few troublesome signs appeared.\n  - Value error dropped far lower than the 0.6 steady-state we had expected from\n    the paper. (0.6 equates to ~85% prediction accuracy).  This suggested our\n    new dataset code was not shuffling adequately, leading to value overfitting.\n  - This idea was supported when, upon further digging, it appeared that the\n    network would judge transformations (rotation/reflection) of the *same\n    board* to have wildly different value estimates.\n\nThese could also have been exacerbated by our computed value for the learning\nrate decay schedule being incorrect.\n\nThis run was halted after discovering a pretty major bug in our huge Dataset\nrewrite that left it training towards what was effectively noise.  Read the gory\ndetails\n[here](https://github.com/tensorflow/minigo/commit/9c6e013293d415e90b92097327dfaca94a81a6da).\n\nOur [milestones](https://github.com/tensorflow/minigo/milestone/1?closed=1) to\nhit before running a new 19x19 pipeline:\n  - add random transformations to the training pipeline\n  - More completely shuffle our training data.\n  - Figure out an equivalent learning rate decay schedule to the one described\n    in the paper.\n\n\n### Fourth run, 9x9s.  Feb 7-March\n\nSince our 19x19 run seemed irrepairable, we decided to work on instrumentation\nto better understand when things were going off the rails while it would still\nbe possible to make adjustments.\n\nThese [improvements](https://github.com/tensorflow/minigo/milestone/2?closed=1)\nincluded:\n - Logging the [magnitude of the\n   updates](https://github.com/tensorflow/minigo/commit/360e056f218833938d845b454b4e24158034b58a)\n   that training would make to our weights.\n - [Setting aside a\n   set of holdout positions](https://github.com/tensorflow/minigo/commit/f941f5ac72d860f1f583392cbeb69d0694373824)\n   to use to detect overfitting.\n - Improvements to setting up clusters and setting up automated testing/CI.\n\n\nThe results were very promising, reaching pro strength after about a week.\n\nOur models and training data can be found in [the GCS bucket\nhere](https://console.cloud.google.com/storage/browser/minigo-pub/v3-9x9).\n\n\n### V5, 19x19s, March-April\n\nWith our newly improved cluster tools, better monitoring, etc, we were pretty\noptimistic about our next attempt at 19x19.  Our evaluation matches -- pitting\ndifferent models against each other to get a good ratings curve -- were\nautomated during the v5 run, and a lot of new data analysis was made available\nduring the run on the 'unofficial data site'\n['cloudygo'](http://www.cloudygo.com)\n\nUnfortunately, progress stalled shortly after cutting the learning rate, and\nseemed to never recover.  Our three most useful indicators were our value net's\ntrain error and validation error, our value net error on a set of professional\ngames (aka \"[figure 3](http://cloudygo.com/v7-19x19/figure-three)\"), and our\nselfplay rating as measured by our evaluation matches.\n\nFor these measures, shortly after our learning rate cut, performance improved\ndramatically before reverting almost completely, with the value net eventually\nbecoming overfit to be worse than random chance on the holdout data.\n\nVarious explanations for how the training data could be qualitatively different\nthan the holdout data to result in overfitting were advanced:  Inadequate\n[shuffling of examples](http://www.moderndescartes.com/essays/shuffle_viz/), a\ntoo aggressive learning rate, not enough 'new games' played by new models before\ntraining new models, etc.\n\nAfter it seemed clear that it was not going to recover, the last half of the run\n(after 315 or so) was largely treated as a free time to twiddle knobs and\nobserve their effects to try and narrow down these effects.\n\nDuring v5, we wrote a number of new features, including:\n\n - A port of the engine to C++ for better performance.\n - A new UI (in the minigui/ directory)\n - Automated evaluation on a separate cluster\n - Separate tfexample pipeline guaranteeing complete shuffling and better\n   throughput (albeit with some hacks that meant that not every game was sampled\n   from equally).\n - General improvements to stackdriver monitoring, like average Q value.\n - Logging MCTS tree summaries in SGFs to inspect tree search behavior.\n\n\n\n#### v5 changelog:\n\n1. 3/4 of the way through 125, change squash from 0.95 => 0.98, change temp\ncutoff to move 30 (from 31, because odd number = bias against black)\n\n1. 192 -- learning rate erroneously cut to 0.001.  207 returned to 0.01.\n(206 trained with 0.001, 207 @0.01)  (this was eventually reverted and moved off\nas v6)\n\n1. 231 -- moved #readouts to 900 from 800.\n\n1. 295-6 -- move shuffle buffer to 1M from 200k\n\n1. 347 -- changed filter amount to 0.02.  (first present for 347)\n\n1. 348 reverted shuffle buffer change\n\n1. 352 change filter to 0.03 and move shuffle buffer back to 200k\n\n1. during 354 (for 355) change steps per model to 1M from 2M, shuffle\nbuffer down to 100k\n\n1. 23.2M steps -- change l2_strength to 0.0002 (from 0.0001)\n\n1. 360ish -- entered experimental mode; freely adjusted learning rate up and\ndown, adjusted batch size, etc, to see if valnet divergence could be fixed.\n\n\n### v7a, first week of May\n\nWe began our next run by rolling back to v5-173-golden-horse, chosen because of\nits relatively high performance and it seemed to be before the v5 run flattened\nout.\n\nThe major changes were:\n\n - using the c++ selfplay worker\n - better training data marshalling/shuffling\n - larger training batch size (16 => 256)\n - lower learning rate (1e-2 => 1e-3) (note this tracks the original aborted LR\n   cut at model #192 in the short-lived 'v6')\n - higher numbers of games per model ( >15k, usually ending up with ~20k)\n\nv7's first premature run had a couple problems:\n\n - We weren't writing our holdout data for 174-179ish, resulting in some weird\n   tensorboard artifacts as our holdout set became composed of models farther\n   and farther from the training data\n - We also weren't writing SGFs, making analysis hard.\n - But we did have our 'figure three', so we were able to make sure we were\n   moving in the right direction :)\n\nBut it did make very strong progress on selfplay!  And then stopped...why?\n\nAfter finding a [potentially bad engine\nbug](https://github.com/tensorflow/minigo/pull/234) we decided to pause training\nto get to the bottom of it.  It ended up affecting < 0.01% of games, but we had\nalready seen progress stall and retrace similar to v5, so we decided to rollback\nand test a new hypothesis:  That our amount of calibration games (5%)\nwas too small.  Compared to other efforts (AGZ paper says 10%, LZ stepped from 100% => 20%,\nand LCZ was still at 100% games played without resignations), Minigo is\ndefinitely the odd one out.\n\nResign disable rate is a very sensitive parameter; it has several effects:\n\n- Resigning saves compute cycles\n- Resigning removes mostly-decided late-game positions from the dataset, helping\n  prevent overfitting to a large set of very similar looking, very biased data.\n- We need some calibration games; otherwise we forget how to play endgame.\n- Calibration games also prevent our bot from going into a self-perpetuating\n  loop of \"resign early\" -> used as training data, resulting in the bot being\n  even more pessimistic and resigning earlier.\n\nThis, combined with our better selfplay performance, made it a relatively\npainless decision to decide to roll back the few days progress, while changing\ncalibration fraction from 5% -> 20%.\n\n### v7, May 16-July-17\n\nRolled back to model v5-173; continued with the flags & features added in v7a,\nbut with holdout data being written from the start ;)  And also with the\ncalibration rate increased to 20%\n\nWe expect to see it go sideways as it adapts to the new proportion of\ncalibration games, then show a similar improvement to the v7a improvement.\nAfter that, if it flattens off again we'll lower the learning rate further.\n\nAfter our learning rate cuts, the ratings appeared to taper off.  A brief bit of\n\"cyclic learning rate\" didn't seem to jog it out of its local minima so we\ncalled a halt after ~530 models\n\nv7 also marked the beginning of an auto-pair for the evaluation matches that\nwould attempt to pair the models about whom our rating algorithm had the\ngreatest uncertainty.  The best way to increase the confidence of the rating is\nto have it play another model with a very close rank, so I wrote it to pick the\nmodel farthest away in time from among the closest rated.  These made for\ninteresting games, as the models of about the same strength from very different\npoints in training would take very different approaches.\n\nOur limiting factor continues to be our rate of selfplay.\n\n### v9, TPUS #1, July 19 - August 1\n\nBy mid July we were able to get a Cloud TPU implementation working for selfplay\nand TPUs were enabled in GKE, allowing us to attempt running with about 600 TPU\nv2's.  Also, we were able to run our training job on a TPU as well, meaning that\nour training batch size could increase from 256 => 2048, which meant we could\nfollow the learning rate schedule published in paper #2.\n\nUnfortunately, TPU code required that we change around where we did the rotation\nof the training examples, moving it from the host to inside the TF graph.\nUnfortunately, I didn't check the default value of the flag and rotating\ntraining examples was turned OFF until around model 115 or so.\n\nStill, this was a hugely successful run:  We were able to run a full 700k steps\n-- the length described in Paper 2 and 3 -- in only two weeks.  14M selfplay\ngames were played; more than the 5M in paper2 and less than the 22M in paper 3.\n\nHere too we saw our top models show up after our first rate cut.  But after the\nsecond rate cut, performance tapered off and the run was stopped.\n\nThe top model had a 100% winrate vs our friendly professionals who tried to play\nit, which was good to see.  Unfortunately, the model was not able to beat the\nbest Leela Zero model (from before they mixed in ELF games), so there was\nclearly something still amiss.  It was unclear what factors caused the model to\nstop improving.  Perhaps not having rotation enabled from the beginning\nessentially 'clipped its wings', or perhaps the higher percentage of calibration\ngames (20%) caused it to see too many useless game positions, or\nperhaps we had waited too long to cut the learning rate -- this was the AZ\napproach instead of the AGZ approach after all, so perhaps using the AGZ LR\nschedule was not appropriate.\n\nIn any event, since we could do a full run in two weeks, we could now more\neasily forumulate hypotheses and test them.\n\n\n### v10, TPUS #2, August 28 - Sept 14\n\nOur first set of hypothesis were not ambitious.  Mostly we wanted to prove that\nour solution was 'stable' and that we could improve a few of the small things we\nknew were wrong with v9, but mostly to leave it the same.  The major differences\nwere:\n\n1. Setting the fraction of calibration games back to 10% (from 20%)\n1. Keeping rotation in training on the whole time\n1. Playing slightly more games per model\n1. Cutting the learning rate the first time as soon as we saw `l2_cost` bottom\n   out and start rising, and letting it run at the second rate (0.001) for\n   longer.\n1. \"Slow window\", [described here](https://medium.com/oracledevs/lessons-from-alpha-zero-part-6-hyperparameter-tuning-b1cfcbe4ca9a),\n   a way to quickly drop the initial games from out of the `window` positions\n   are drawn from.  Basically, the first games are true noise, so the quicker\n   you can get rid of them, the better.\n\nWe were very excited about this run.  We had very close fidelity to the\nalgorithm described in the paper, we had the hyperparameters for batch size and\nlearning rate as close as we could make them.  Surely this would be the one...\n:)\n\nOverall, we played 22M games in 865 models over about two weeks.  The best v10\nmodels were able to pretty convincingly beat the best v9 models, which was\ngreat.  But with all the things we changed, which was responsible?\n\nIn retrospect, probably the two most useful changes were leaving the learning\nrate at the `medium` setting for longer, giving it a longer time to meander\nthrough the loss landscape, and setting the calibration rate back to 10%.\n(Having rotation on in training the whole time obviously doesn't hurt, but it\nprobably doesn't affect the overall skill ceiling)\n\nThe theory on having the resign-disable games correctly set was based off of\nsome of the [excellent results by Seth on the sensitivity of the value network\nto false-positive resigns](https://github.com/tensorflow/minigo/issues/483).\nHis analysis was to basically randomly flip the result of x% of training\nexamples and measure the effects on value confidence and value error.  The\nobservation was that having 5% of the data fouled was sufficient to dramatically\naffect value error.\n\nWith this amount as a good proxy for 'the amount of data needed to impact\ntraining significantly', we could then think about how the percent of\ncalibration games might be affecting us.  With 20% of the\ngames being played to scoring, sometimes requiring 2x or 3x as many moves, it\nmeant that the moves sampled from these games were essentially 'useless' i.e.,\nfrom a point beyond which the outcome was no longer in doubt.  Since we were\nstill sampling uniformly by game (i.e., picking 4 moves from the most recent\n500k games), this meant that for v9, exactly 20% of the training examples were\nfrom these games.  Reducing that to 10% probably gave us an edge in establishing\na better value net.\n\nIt seemed like there was no real difference from playing more games per model,\nand 'slow window' doesn't seem to dramatically affect how long it takes to get\nout of the early learning phase.  (Basically, policy improves while value\ncost drops to 1.0, implying pure randomness, until eventually the net starts to\nfigure out what 'causes' wins and value error drops below 1.0)\n\nv10 showed that we could reach about the same level as v9, and that we'd plateau\nafter a certain point.  Also, we noted that the variance in strength between\nsuccessive networks was really high!  It seemed to expand as training continued,\nsuggesting that we were in some very reliable minima.  Was this a function of\nthe higher batch size, the higher number of games per model, or both, or\nneither?\n\nQualitatively, v10's Go was interesting for a couple reasons.  We saw the\nfamiliar 'flower' joseki that was the zero-bot signature, but we also saw the\nknight's move approach to the 4-4 point come into favor.  However, by the end of\nthe run, the bot was playing 3-4 points as black, leading to very different\npatterns than those seen by AGZ or AG-Master.\n\nSo while we (again) created a professional level go AI, we were still below the\nlevel of other open-source bots (ELF and LZ), despite the fact that the\nAlphaZero approach was supposed to be equal or slightly superior.  Why?  What\nwere we missing?\n\n### v11, Q=0, Sept 14- Sept 17\n\nSince we were rapidly running out of ways we were differing from the paper, we\nthought we'd test a pretty radical hypothesis, called \"Q=0\".  The short version\nis that, per the paper, a leaf with no visits should have the value initialized\nto '0'.  Say a board position had an average Q-value of 0.2, suggesting that it\nslightly favored black.  Suppose further that, after one move, the expected\nresult of the board was not dramatically different, i.e., it would evaluate to\nnear 0.2 for most reasonable moves.  If it were white to play, picking the leaf\nwith the value closest to white's goal (-1) would mean search would prefer to\nchoose any unvisited leaf (i.e., leaves still with their initialized value of\n0).  This would result in search visiting all legal moves before returning to\nthe 'reasonable' variations.  The net result would mean a huge spread of\nexplored moves for the losing side, and a very diluted search.\n\nConversely, if it were black to play, search would prefer to only pick visited\nleaves, as unvisited leaves would have a value of 0, while any reasonable first\nmove would have a value presumably close to 0.2, which is closer to black's goal\n(+1).  The result here is that search would basically only explore a single move\nfor the winning side.\n\nThe results would be policy examples that are very flat or diffuse for the side\nthat loses more, and very sharp for the side that wins more.\n\nWe had run all of our previous versions assuming that instead of Q=0, we should\ninitialize leaves to the Q of their parent, aka 'Q=parent'.  This seems\nreasonable: both sides would look for moves that represented improvements from a\ngiven position, and the UCB algorithm could widen or narrow the search as\nneeded.\n\nBut what if we were wrong?  What if exhaustively checking all moves for the\nlosing side and essentially one-hot focusing on the moves for the winning side\nwas an intentional choice?  It's worth pointing out that this was one of the\nfirst points of contention found by computer-go researchers between the release\nof the second paper and before the release of the third.  You can see some\ninscrutable replies and a lot of head scratching on the [venerable computer-go\nmailing list here](http://computer-go.org/pipermail/computer-go/2017-December/010550.html).\nEven with a direct reply from a main author, opinions were still divided as to\nwhat it meant.\n\nSo, we set Q=0 and ran V11.  Our expectation was that it would fail quickly in\nobvious ways and we could put this speculation to bed.  We were half-right...\n\nAfter a few hundred models, where strength did not increase as quickly as with\nQ=parent, the resign-rate false-positive began to swing wildly out of control.\nIt seemed that as a side began to win more than 50% of its games, search began\nto greatly favor its policy, and the losing side was basically always left with\na handicap of having a massive number of its moves 'diluted.'  Continuing from\nour previous example... black would be able to spread out its reads quicker than\nwhite would be able to focus on a single useful line, essentially leaving one\nside at a disadvantage.\n\nCompounding the problem, our pipeline architecture meant that the\n'false-positive' resign rate appeared as a lagging indicator: a new model would\nbe released, the TPU workers would all start playing with it immediately, and\nthe 90% of games with resign enabled would finish before the 10% of holdout\ngames that could be measured to set the threshold... So we were right that this\nfailed quickly.\n\nWhere we were wrong was that this would put the theory to bed.  Arguing against\nQ=0 was the fact that it was clearly unstable, has problems with the AZ method,\nand intuitively seemed to massively disadvantage one side.  Arguing for it was\nthe fact that it was actually what was published, that there was a plausible\nexplanation for the imbalance, and that the pipeline could be improved or\ntweaked to only play the resign-holdout games first.  But most interestingly was\nthat, despite the very different behavior of selfplay and the different dynamics\nshowing on tensorboard, the performance of the models on the professional player\nvalidation set was not awful!  It still improved, and it improved fairly well --\nalthough slower than it improved with Q=parent.\n\nSo perhaps with a better pipeline and tighter controls on the resign threshold,\nthis would ultimately lead to better exploration and a better model, but in the\nmeantime it looked far more unstable.  We stopped the run after 3 days.\n\n### v12, vlosses 2\n\nWith Q=0 checked off our list, the next question mark for us was \"virtual\nlosses\".  In brief, virtual losses provide a way for GPUs to more efficiently do\ntree search by selecting multiple leaves.  Simply running search repeatedly will\nalmost always result in the same leaf being chosen, so to fill a 'batch' of\nleaves, chosen leaves are given a 'virtual loss', encouraging search to pick a\ndifferent variation to explore the next time down the tree.  Once the batch is\nfull, the GPU can efficiently evaluate them all, the 'virtual losses' are\nreverted and the real results from the value network are propagated up the tree.\nThe number of simultaneous nodes would be the batch size, and we set it via the\n'vlosses' parameter.  Our previous runs all used vlosses=8.  We had\nonly one sentence from the second paper to guide us: \"We use virtual loss to\nensure each thread evaluates different nodes\" (from Methods, \"Search Algorithm\",\n*Backup*)\n\nIn particular, the cost of running a neural network is greatly amortized by the\nhighly parallel nature of GPUs and TPUs.  E.g., the time taken to evaluate one\nnode might be nearly the same as the cost of evaluating two nodes, as the\noverhead of transferring data to/from the GPU/TPU is incurred regardless of how\nmuch data is sent.\n\nFor TPUs, which really shine with high batch sizes, virtual losses seemed like\na necessity.  But how would they affect reinforcement learning?  On the plus\nside, virtual losses acted almost like increased noise, encouraging exploration\nof moves that the network might not have chosen in favor of a single main line.\nOn the minus side, MCTS' mathematically ideal batch size was 1: Evaluating many\nsub-optimal moves might distort the averages, particularly in the case of long,\nunbranching sequences with a sharp horizon effect, i.e., ladders.  Both v9 and\nv10, while reaching professional strength, would still struggle with ladders.\n\nIn a ladder sequence, only one node in the MCTS tree would read the ladder,\nwhile the rest of the batch would need to pick non-ladder moves.  It's\nconceivable that the averaging done by MCTS would essentially encourage the\nmodels to avoid sharp, narrow paths with high expected value in favor of flatter\npaths with a less dramatic outcome.  We figured we'd test this by setting batch\nsize to 2, doubling the number of games played at once (to achieve roughly the\nsame throughput), and run again.\n\nEarly results are not dramatically stronger.  Qualitatively, we have not seen\nthe model explore the knight's move approach to the 4-4 hardly at all, instead\nimmediately converging to the 3-4 openings as black followed by an immediate\ninvasion of the 3-3 point.\n\nHaving learned from our previous runs, we postponed our final rate cut until\nstep 800k or so, and with it we also increased our window size to 1M (assuming\nthat the lower rate of change between models would lead to a smaller diversity\nof selfplay game examples).\n\nOne other minor tweak was to stop training on moves after move #400 after we had\nreached a reasonable strength.  The \"logic\" here is similar to v10's explanation\nre: the 10% calibration games.  if the calibration games run, on average,\n2x or 3x longer, then the examples taken from them are almost always past the\npoint when the outcome is well understood.  So forcibly drawing the training\nexamples from the first 400 moves should mostly rid us of those pointless\nexamples without dramatically distorting the ratio of opening/middle/endgame\nexamples seen by the network.\n\nThis run ended up not really having fixed anything. On our cross-evaluation\nmatches, v12 ended up slightly stronger than the other runs, but it was well\nwithin the natural variability of the other runs. V12 still had ladder issues\nand it still could not beat LeelaZero or ELF, even with increased playouts.\n\nIn retrospect, the fact that v12 had improved slightly in strength suggested\nthat adding spurious reads to the game tree would diminish MCTS's ability to\ndiscern the best move.\n\n\n### v13, pro game bootstrap\n\nOut of curiosity, we wanted to see what happened if we bootstrapped a model\nfrom a pro game dataset. The expected thing happened: the model started from\na much stronger starting place, and the distribution of opening moves played\nwas very reminiscent of human pro play. We eventually saw the model discard\nsome common human opening patterns and revert back to Zero-ish play (e.g.\nlots of 3-3 invasions). Overall, the run went well, but the strength never\npeaked to the same level as v10 or v12.\n\n### v14, Bigtable integration with init-to-loss switch\n\nFor v14, the biggest change was replacing the in-memory TFExample shuffler with\na BigTable shuffler.\n\nA minor change - we started logging the average depth of search.\n\nPreviously, we'd write out tiny TFRecords containing ~200 TFExamples each, and\nanother Python process would rsync the records to one machine, read them into\nmemory, and maintain a large moving window of positions in memory that would be\nperiodically be spit out as training chunks. This shuffler had all sorts of\nhacks in it:\n\n  - we'd entirely ignore any games with < 30 moves\n  - at the end of v12, we tried ignoring moves > 400 to remove post-endgame\n    noise\n  - we'd draw a fixed number (4) of moves from each game (which oversampled\n    short games). This hacky fixed-number sampling was because exactly 10% of\n    our games were calibration games, but those calibration games were much\n    longer than the other games, meaning that an even sampling by move would\n    oversample the calibration games. A fixed-number sampling would at least\n    guarantee a 10% representation of calibration games.\n\nIn terms of bottlenecks, our sampler took a big chunk of time to run, but it\nwas about the same as training time, so it was not technically a bottleneck\nyet. But we were still training on a single TPU (8 cores) rather than a pod\n(128 cores), so if we were to scale to more TPUs for selfplay, and upgrade our\ntrainer to use a pod to compensate, then this shuffler would quickly become the\nbottleneck.\n\nFor v14, our selfplay workers would instead directly upload each TFExample\nto BigTable, indexed by game number and move number. A set of parallel readers\nwould then use BigTable's built-in samplers to read over a specified window of\ngames. In order to ensure a 10% ratio of calibration games to resign-enabled\ngames, we used the BigTable input readers to enforce a 90:10 ratio.\n\nThe many benefits of BT replacement included:\n\n  - we could directly run a query on BigTable to compute the appropriate resign\n  threshold. Previously, this had been done by manually inspecting a\n  Stackdriver graph and editing a flagfile on GCS. This meant that the latency\n  of resign threshold updates dropped from hours to minutes. Also, because\n  the resign threshold had been set manually, we tended to pick a number on the\n  high side, since it would be in force for a while.\n  - Time between checkpoints dropped from 30 minutes to 20 minutes. This\n  was due to the trainer not having to wait for the shuffler at all. We actually\n  ended up adding waiting logic to the timer to ensure enough games had been\n  completed before starting training again. This was probably an improvement on\n  our previous logic, which output a golden chunk as soon as the trainer\n  completed training a new generation.\n  - The ratio of sampled positions from calibration games and normal games was\n  precisely set at 90:10, instead of implicitly depending on the ratio of game\n  lengths for calibration games / normal games. (The AGZ paper reported setting\n  aside 10% of games, which could lead to anywhere from 10~20% of moves being\n  from calibration games.\n\nThe run was uneventful and was a bit better than previous runs; it escaped\nrandom play much more quickly, probably because the up-to-date resign thresholds\nmeant that our training data was appropriately pruned of 'easy' positions.\nSo all in all, the rewrite didn't create any new pipeline issues and improved\nmany aspects.\n\nAround model 280, we [discovered on LCZero forums](http://talkchess.com/forum3/viewtopic.php?f=2&t=69175&p=781765&sid=c57776201e233b1be14bf56f71f5e54e#p781765)\nthat AlphaGoZero had used\ninit-to-loss for Q values. We switched to init-to-loss from init-to-parent and\nsaw our network gain dramatically in strength. Soon, our top model thoroughly\nexceeded our previous best model, and beat LZ's v157, the last model from before\nLZ started mixing in game data from the much stronger ELF.\n\n### v15, clean run with init to loss\n\nThis run beat v14 pretty early on, producing a model that was stronger than\nv14's best at model 330. v15 ended up slightly stronger than v14 but was\notherwise similar.\n\nQualitatively, we noticed that V14/V15's models were much sharper,\nmeaning that they had more concentrated policy network output and the value\nnetwork output would have stronger opinions on whether it was winning or losing.\nSimultaneously, the calibrated resign threshold for init-to-loss ended up at\n0.5-0.7 compared to 0.8-0.95 in previous runs. So v14/v15's MCTS was operating\nin the sweet spot of [-0.5, 0.5] more frequently. (The MCTS algorithm tends to\ngo blind once Q gets close to 0.9, because there's no more room at the top.)\n\nThe init-to-loss configuration also led to much deeper reads, as the tree search\nbehavior would dive deeply into one variation before going onto the next.\nOriginally, we'd labelled this behavior as pathological and thought it went\nagainst the spirit of MCTS, which was why init-to-loss wasn't seriously\nconsidered. But this read behavior meant that v14 was our first model to be\nable to consistently read ladders. One funny side effect was that\nit was actually not clear whether v14 was actually stronger than v12, or whether\nv14 was just bullying v12 around by reading ladders that v12 couldn't read, and\nwinning easy rating points. But since we had achieved success against external\nreference points, we concluded that v14/v15 were in fact really stronger than\nv12.\n\nDuring v15's run, we realized that our evaluation cluster infrastructure was in\nneed of some upgrades, primarily for two reasons:\n\n1. We wanted to include external reference points, so that we could get a more\nstable evaluation.\n1. We wanted to be able to run each model with its preferred configuration (init\nto parent for v10/v12, and init to loss for v14/v15.) This would seem easy - run\neach side as an independent subprocess. However, because of GPU memory / TPU\ncontention issues, this was tricky to do.\n"
        },
        {
          "name": "WORKSPACE",
          "type": "blob",
          "size": 2.068359375,
          "content": "load(\"@bazel_tools//tools/build_defs/repo:http.bzl\", \"http_archive\", \"http_file\")\n\n# These must be kept up to date with the rules from tensorflow/WORKSPACE.\n# vvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvvv\nhttp_archive(\n    name = \"io_bazel_rules_closure\",\n    sha256 = \"5b00383d08dd71f28503736db0500b6fb4dda47489ff5fc6bed42557c07c6ba9\",\n    strip_prefix = \"rules_closure-308b05b2419edb5c8ee0471b67a40403df940149\",\n    urls = [\n        \"https://storage.googleapis.com/mirror.tensorflow.org/github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",\n        \"https://github.com/bazelbuild/rules_closure/archive/308b05b2419edb5c8ee0471b67a40403df940149.tar.gz\",  # 2019-06-13\n    ],\n)\n\nhttp_archive(\n    name = \"bazel_skylib\",\n    sha256 = \"2ef429f5d7ce7111263289644d233707dba35e39696377ebab8b0bc701f7818e\",\n    urls = [\"https://github.com/bazelbuild/bazel-skylib/releases/download/0.8.0/bazel-skylib.0.8.0.tar.gz\"],\n)\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n# These must be kept up to date with the rules from tensorflow/WORKSPACE.\n\n# This should also be kept up to date with the version used by Tensorflow.\nhttp_file(\n    name = \"com_github_nlohmann_json_single_header\",\n    sha256 = \"63da6d1f22b2a7bb9e4ff7d6b255cf691a161ff49532dcc45d398a53e295835f\",\n    urls = [\n        \"https://github.com/nlohmann/json/releases/download/v3.4.0/json.hpp\",\n    ],\n)\n\nhttp_archive(\n    name = \"org_tensorflow\",\n    sha256 = \"76abfd5045d1474500754566edd54ce4c386a1fbccf22a3a91d6832c6b7e90ad\",\n    strip_prefix = \"tensorflow-1.15.0\",\n    urls = [\"https://github.com/tensorflow/tensorflow/archive/v1.15.0.zip\"],\n)\n\nhttp_archive(\n    name = \"wtf\",\n    build_file = \"//cc:wtf.BUILD\",\n    sha256 = \"1837833cd159060f8bd6f6dd87edf854ed3135d07a6937b7e14b0efe70580d74\",\n    strip_prefix = \"tracing-framework-fb639271fa3d56ed1372a792d74d257d4e0c235c\",\n    urls = [\"https://github.com/google/tracing-framework/archive/fb639271fa3d56ed1372a792d74d257d4e0c235c.zip\"],\n)\n\nload(\"@org_tensorflow//tensorflow:workspace.bzl\", \"tf_workspace\")\n\ntf_workspace()\n"
        },
        {
          "name": "__init__.py",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "batch_exporter.py",
          "type": "blob",
          "size": 6.283203125,
          "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Copy Minigo training sets from table to GCS..\n\"\"\"\n\n\nimport bisect\nimport math\nimport multiprocessing\nimport os\nimport tensorflow as tf\nfrom absl import flags\nfrom absl import app\nfrom tqdm import tqdm\nimport bigtable_input\nimport utils\n\n\nflags.DEFINE_bool('dry_run', False,\n                  'If true, generate and print the windows, rather than export.')\n\nflags.DEFINE_integer('starting_game', None,\n                     'Export beginning with the window that follows this regular game')\n\nflags.DEFINE_integer('training_games', 500000,\n                     'Number of games to include in training window')\n\nflags.DEFINE_integer('training_moves', 2**21,\n                     'Number of moves to select from training window')\n\nflags.DEFINE_float('training_fresh', 0.05,\n                   'Fraction of fresh games in each new training window')\n\nflags.DEFINE_integer('training_thin', 4,\n                     'Factor by which to thin out training sets (keep 1:N)')\n\nflags.DEFINE_integer('batch_size', 1024,\n                     'How many TFRecords to pull through tf.Session at a time')\n\nflags.DEFINE_string('output_prefix', None,\n                    'Name of output file to receive TFRecords')\nflags.mark_flag_as_required('output_prefix')\n\nflags.DEFINE_integer('concurrency', 4,\n                     'Number of parallel subprocesses')\n\nflags.DEFINE_integer('max_trainings', None,\n                     'Process no more than this many training brackets')\n\nFLAGS = flags.FLAGS\n\n\ndef training_series(cursor_r, cursor_c, mix, increment_fraction=0.05):\n    \"\"\"Given two end-cursors and a mix of games, produce a series of bounds.\n    \"\"\"\n    stride_r = math.ceil(mix.games_r * increment_fraction)\n    intervals = math.ceil(cursor_r / stride_r)\n    # Now determine which increment will divide cursor_c into the same\n    # number of intervals\n    stride_c = math.ceil(cursor_c / intervals)\n    print('stride_c was {}, now {}'.format(mix.games_c * increment_fraction, stride_c))\n    print('stride_r: {}  stride_c: {}'.format(stride_r, stride_c))\n    for b_r, b_c in zip(range(0, cursor_r, stride_r), range(0, cursor_c, stride_c)):\n        last_r, last_c = b_r + stride_r, b_c + stride_c\n        yield (b_r, last_r, b_c, last_c)\n    yield last_r, cursor_r, last_c, cursor_c\n\n\ndef _export_training_set(args):\n    spec, start_r, start_c, mix, batch_size, output_url = args\n    gq_r = bigtable_input.GameQueue(spec.project, spec.instance, spec.table)\n    gq_c = bigtable_input.GameQueue(spec.project, spec.instance, spec.table + '-nr')\n    total_moves = mix.moves_r + mix.moves_c\n\n    with tf.Session() as sess:\n        ds = bigtable_input.get_unparsed_moves_from_games(gq_r, gq_c,\n                                                          start_r, start_c,\n                                                          mix)\n        ds = ds.batch(batch_size)\n        iterator = ds.make_initializable_iterator()\n        sess.run(iterator.initializer)\n        get_next = iterator.get_next()\n        writes = 0\n        print('Writing to', output_url)\n        with tf.io.TFRecordWriter(\n                output_url,\n                options=tf.io.TFRecordCompressionType.ZLIB) as wr:\n            log_filename = '/tmp/{}_{}.log'.format(start_r, start_c)\n            with open(log_filename, 'w') as progress_file:\n                with tqdm(desc='Records', unit_scale=2, total=total_moves,\n                          file=progress_file) as pbar:\n                    while True:\n                        try:\n                            batch = sess.run(get_next)\n                            pbar.update(len(batch))\n                            for b in batch:\n                                wr.write(b)\n                            writes += 1\n                            if (writes % 10000) == 0:\n                                wr.flush()\n                        except tf.errors.OutOfRangeError:\n                            break\n            os.unlink(log_filename)\n\n\ndef main(argv):\n    \"\"\"Main program.\n    \"\"\"\n    del argv  # Unused\n    total_games = FLAGS.training_games\n    total_moves = FLAGS.training_moves\n    fresh = FLAGS.training_fresh\n    thin = FLAGS.training_thin\n    batch_size = FLAGS.batch_size\n    output_prefix = FLAGS.output_prefix\n\n    spec = bigtable_input.BigtableSpec(\n        FLAGS.cbt_project,\n        FLAGS.cbt_instance,\n        FLAGS.cbt_table)\n    gq_r = bigtable_input.GameQueue(spec.project, spec.instance, spec.table)\n    gq_c = bigtable_input.GameQueue(spec.project, spec.instance, spec.table + '-nr')\n\n    mix = bigtable_input.mix_by_decile(total_games, total_moves, 9)\n    bounds = list(training_series(gq_r.latest_game_number,\n                                  gq_c.latest_game_number,\n                                  mix,\n                                  fresh))\n    trainings = [(spec, start_r, start_c,\n                  mix, batch_size,\n                  '{}{:0>10}_{:0>10}.tfrecord.zz'.format(output_prefix, start_r, start_c))\n                 for start_r, finish_r, start_c, finish_c\n                 in bounds]\n\n    if FLAGS.starting_game:\n        game = FLAGS.starting_game\n        starts = [t[1] for t in trainings]\n        where = bisect.bisect_left(starts, game)\n        trainings = trainings[where:]\n\n    if FLAGS.max_trainings:\n        trainings = trainings[:FLAGS.max_trainings]\n\n    trainings = trainings[::thin]\n\n    if FLAGS.dry_run:\n        for t in trainings:\n            print(t)\n        raise SystemExit\n\n    concurrency = min(FLAGS.concurrency, multiprocessing.cpu_count() * 2)\n    with tqdm(desc='Training Sets', unit_scale=2, total=len(trainings)) as pbar:\n        for b in utils.iter_chunks(concurrency, trainings):\n            with multiprocessing.Pool(processes=concurrency) as pool:\n                pool.map(_export_training_set, b)\n                pbar.update(len(b))\n\n\nif __name__ == '__main__':\n    app.run(main)\n"
        },
        {
          "name": "bigtable_input.py",
          "type": "blob",
          "size": 28.5615234375,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Read Minigo game examples from a Bigtable.\n\"\"\"\n\nimport bisect\nimport collections\nimport datetime\nimport math\nimport multiprocessing\nimport operator\nimport re\nimport struct\nimport time\nimport numpy as np\n\nfrom tqdm import tqdm\nfrom absl import flags\nfrom google.cloud import bigtable\nfrom google.cloud.bigtable import row_filters as bigtable_row_filters\nfrom google.cloud.bigtable import column_family as bigtable_column_family\nimport tensorflow as tf\nfrom tensorflow.contrib import cloud as contrib_cloud\n\nimport utils\n\n\nflags.DEFINE_string('cbt_project', None,\n                    'The project used to connect to the cloud bigtable ')\n\n# cbt_instance:  identifier of Cloud Bigtable instance in cbt_project.\nflags.DEFINE_string('cbt_instance', None,\n                    'The identifier of the cloud bigtable instance in cbt_project')\n\n# cbt_table:  identifier of Cloud Bigtable table in cbt_instance.\n# The cbt_table is expected to be accompanied by one with an \"-nr\"\n# suffix, for \"no-resign\".\nflags.DEFINE_string('cbt_table', None,\n                    'The table within the cloud bigtable instance to use')\n\nFLAGS = flags.FLAGS\n\n\n# Constants\n\nROW_PREFIX = 'g_{:0>10}_'\nROWCOUNT_PREFIX = 'ct_{:0>10}_'\n\n# Model tabels (models, models_for_eval) row key\nMODEL_PREFIX = \"m_{run}_{num:0>10}\"\n# Name of model\nMODEL_NAME = b'model'\n\n# Maximum number of concurrent processes to use when issuing requests against\n# Bigtable.  Value taken from default in the load-testing tool described here:\n#\n# https://github.com/googleapis/google-cloud-go/blob/master/bigtable/cmd/loadtest/loadtest.go\n\nMAX_BT_CONCURRENCY = 100\n\n# Column family and qualifier constants.\n\n# Column Families\n\nMETADATA = 'metadata'\nTFEXAMPLE = 'tfexample'\n\n# Column Qualifiers\n\n# Note that in CBT, families are strings and qualifiers are bytes.\n\nTABLE_STATE = b'table_state'\nWAIT_CELL = b'wait_for_game_number'\nGAME_COUNTER = b'game_counter'\nMOVE_COUNT = b'move_count'\n\n# Patterns\n\n_game_row_key = re.compile(r'g_(\\d+)_m_(\\d+)')\n_game_from_counter = re.compile(r'ct_(\\d+)_')\n\n\n# The string information needed to construct a client of a Bigtable table.\nBigtableSpec = collections.namedtuple(\n    'BigtableSpec',\n    ['project', 'instance', 'table'])\n\n\n# Information needed to create a mix of two Game queues.\n# r = resign/regular; c = calibration (no-resign)\nGameMix = collections.namedtuple(\n    'GameMix',\n    ['games_r', 'moves_r',\n     'games_c', 'moves_c',\n     'selection'])\n\n\ndef cbt_intvalue(value):\n    \"\"\"Decode a big-endian uint64.\n\n    Cloud Bigtable stores integers as big-endian uint64,\n    and performs this translation when integers are being\n    set.  But when being read, the values need to be\n    decoded.\n    \"\"\"\n    return int(struct.unpack('>q', value)[0])\n\n\ndef make_single_array(ds, batch_size=8*1024):\n    \"\"\"Create a single numpy array from a dataset.\n\n    The dataset must have only one dimension, that is,\n    the length of its `output_shapes` and `output_types`\n    is 1, and its output shape must be `[]`, that is,\n    every tensor in the dataset must be a scalar.\n\n    Args:\n      ds:  a TF Dataset.\n      batch_size:  how many elements to read per pass\n\n    Returns:\n      a single numpy array.\n    \"\"\"\n    if isinstance(ds.output_types, tuple) or isinstance(ds.output_shapes, tuple):\n        raise ValueError('Dataset must have a single type and shape')\n    nshapes = len(ds.output_shapes)\n    if nshapes > 0:\n        raise ValueError('Dataset must be comprised of scalars (TensorShape=[])')\n    batches = []\n    with tf.Session() as sess:\n        ds = ds.batch(batch_size)\n        iterator = ds.make_initializable_iterator()\n        sess.run(iterator.initializer)\n        get_next = iterator.get_next()\n        with tqdm(desc='Elements', unit_scale=1) as pbar:\n            try:\n                while True:\n                    batches.append(sess.run(get_next))\n                    pbar.update(len(batches[-1]))\n            except tf.errors.OutOfRangeError:\n                pass\n    if batches:\n        return np.concatenate(batches)\n    return np.array([], dtype=ds.output_types.as_numpy_dtype)\n\n\ndef _histogram_move_keys_by_game(sess, ds, batch_size=8*1024):\n    \"\"\"Given dataset of key names, return histogram of moves/game.\n\n    Move counts are written by the game players, so\n    this is mostly useful for repair or backfill.\n\n    Args:\n      sess:  TF session\n      ds:  TF dataset containing game move keys.\n      batch_size:  performance tuning parameter\n    \"\"\"\n    ds = ds.batch(batch_size)\n    # Turns 'g_0000001234_m_133' into 'g_0000001234'\n    ds = ds.map(lambda x: tf.strings.substr(x, 0, 12))\n    iterator = ds.make_initializable_iterator()\n    sess.run(iterator.initializer)\n    get_next = iterator.get_next()\n    h = collections.Counter()\n    try:\n        while True:\n            h.update(sess.run(get_next))\n    except tf.errors.OutOfRangeError:\n        pass\n    # NOTE:  Cannot be truly sure the count is right till the end.\n    return h\n\n\ndef _game_keys_as_array(ds):\n    \"\"\"Turn keys of a Bigtable dataset into an array.\n\n    Take g_GGG_m_MMM and create GGG.MMM numbers.\n\n    Valuable when visualizing the distribution of a given dataset in\n    the game keyspace.\n    \"\"\"\n    ds = ds.map(lambda row_key, cell: row_key)\n    # want 'g_0000001234_m_133' is '0000001234.133' and so forth\n    ds = ds.map(lambda x:\n                tf.strings.to_number(tf.strings.substr(x, 2, 10) +\n                                     '.' +\n                                     tf.strings.substr(x, 15, 3),\n                                     out_type=tf.float64))\n    return make_single_array(ds)\n\n\ndef _delete_rows(args):\n    \"\"\"Delete the given row keys from the given Bigtable.\n\n    The args are (BigtableSpec, row_keys), but are passed\n    as a single argument in order to work with\n    multiprocessing.Pool.map.  This is also the reason why this is a\n    top-level function instead of a method.\n\n    \"\"\"\n    btspec, row_keys = args\n    bt_table = bigtable.Client(btspec.project).instance(\n        btspec.instance).table(btspec.table)\n    rows = [bt_table.row(k) for k in row_keys]\n    for r in rows:\n        r.delete()\n    bt_table.mutate_rows(rows)\n    return row_keys\n\n\nclass GameQueue:\n    \"\"\"Queue of games stored in a Cloud Bigtable.\n\n    The state of the table is stored in the `table_state`\n    row, which includes the columns `metadata:game_counter`.\n    \"\"\"\n\n    def __init__(self, project_name, instance_name, table_name):\n        \"\"\"Constructor.\n\n        Args:\n          project_name:  string name of GCP project having table.\n          instance_name:  string name of CBT instance in project.\n          table_name:  string name of CBT table in instance.\n        \"\"\"\n        self.btspec = BigtableSpec(project_name, instance_name, table_name)\n        self.bt_table = bigtable.Client(\n            self.btspec.project, admin=True).instance(\n                self.btspec.instance).table(self.btspec.table)\n        self.tf_table = contrib_cloud.BigtableClient(\n            self.btspec.project,\n            self.btspec.instance).table(self.btspec.table)\n\n    def create(self):\n        \"\"\"Create the table underlying the queue.\n\n        Create the 'metadata' and 'tfexample' column families\n        and their properties.\n        \"\"\"\n        if self.bt_table.exists():\n            utils.dbg('Table already exists')\n            return\n\n        max_versions_rule = bigtable_column_family.MaxVersionsGCRule(1)\n        self.bt_table.create(column_families={\n            METADATA: max_versions_rule,\n            TFEXAMPLE: max_versions_rule})\n\n    @property\n    def latest_game_number(self):\n        \"\"\"Return the number of the next game to be written.\"\"\"\n        table_state = self.bt_table.read_row(\n            TABLE_STATE,\n            filter_=bigtable_row_filters.ColumnRangeFilter(\n                METADATA, GAME_COUNTER, GAME_COUNTER))\n        if table_state is None:\n            return 0\n        return cbt_intvalue(table_state.cell_value(METADATA, GAME_COUNTER))\n\n    @latest_game_number.setter\n    def latest_game_number(self, latest):\n        table_state = self.bt_table.row(TABLE_STATE)\n        table_state.set_cell(METADATA, GAME_COUNTER, int(latest))\n        table_state.commit()\n\n    def games_by_time(self, start_game, end_game):\n        \"\"\"Given a range of games, return the games sorted by time.\n\n        Returns [(time, game_number), ...]\n\n        The time will be a `datetime.datetime` and the game\n        number is the integer used as the basis of the row ID.\n\n        Note that when a cluster of self-play nodes are writing\n        concurrently, the game numbers may be out of order.\n        \"\"\"\n        move_count = b'move_count'\n        rows = self.bt_table.read_rows(\n            ROWCOUNT_PREFIX.format(start_game),\n            ROWCOUNT_PREFIX.format(end_game),\n            filter_=bigtable_row_filters.ColumnRangeFilter(\n                METADATA, move_count, move_count))\n\n        def parse(r):\n            rk = str(r.row_key, 'utf-8')\n            game = _game_from_counter.match(rk).groups()[0]\n            return (r.cells[METADATA][move_count][0].timestamp, game)\n        return sorted([parse(r) for r in rows], key=operator.itemgetter(0))\n\n    def delete_row_range(self, format_str, start_game, end_game):\n        \"\"\"Delete rows related to the given game range.\n\n        Args:\n          format_str:  a string to `.format()` by the game numbers\n            in order to create the row prefixes.\n          start_game:  the starting game number of the deletion.\n          end_game:  the ending game number of the deletion.\n        \"\"\"\n        row_keys = make_single_array(\n            self.tf_table.keys_by_range_dataset(\n                format_str.format(start_game),\n                format_str.format(end_game)))\n        row_keys = list(row_keys)\n        if not row_keys:\n            utils.dbg('No rows left for games %d..%d' % (\n                start_game, end_game))\n            return\n        utils.dbg('Deleting %d rows:  %s..%s' % (\n            len(row_keys), row_keys[0], row_keys[-1]))\n\n        # Reverse the keys so that the queue is left in a more\n        # sensible end state if you change your mind (say, due to a\n        # mistake in the timestamp) and abort the process: there will\n        # be a bit trimmed from the end, rather than a bit\n        # trimmed out of the middle.\n        row_keys.reverse()\n        total_keys = len(row_keys)\n        utils.dbg('Deleting total of %d keys' % total_keys)\n        concurrency = min(MAX_BT_CONCURRENCY,\n                          multiprocessing.cpu_count() * 2)\n        with multiprocessing.Pool(processes=concurrency) as pool:\n            batches = []\n            with tqdm(desc='Keys', unit_scale=2, total=total_keys) as pbar:\n                for b in utils.iter_chunks(bigtable.row.MAX_MUTATIONS,\n                                           row_keys):\n                    pbar.update(len(b))\n                    batches.append((self.btspec, b))\n                    if len(batches) >= concurrency:\n                        pool.map(_delete_rows, batches)\n                        batches = []\n                pool.map(_delete_rows, batches)\n                batches = []\n\n    def trim_games_since(self, t, max_games=500000):\n        \"\"\"Trim off the games since the given time.\n\n        Search back no more than max_games for this time point, locate\n        the game there, and remove all games since that game,\n        resetting the latest game counter.\n\n        If `t` is a `datetime.timedelta`, then the target time will be\n        found by subtracting that delta from the time of the last\n        game.  Otherwise, it will be the target time.\n        \"\"\"\n        latest = self.latest_game_number\n        earliest = int(latest - max_games)\n        gbt = self.games_by_time(earliest, latest)\n        if not gbt:\n            utils.dbg('No games between %d and %d' % (earliest, latest))\n            return\n        most_recent = gbt[-1]\n        if isinstance(t, datetime.timedelta):\n            target = most_recent[0] - t\n        else:\n            target = t\n        i = bisect.bisect_right(gbt, (target,))\n        if i >= len(gbt):\n            utils.dbg('Last game is already at %s' % gbt[-1][0])\n            return\n        when, which = gbt[i]\n        utils.dbg('Most recent:  %s  %s' % most_recent)\n        utils.dbg('     Target:  %s  %s' % (when, which))\n        which = int(which)\n        self.delete_row_range(ROW_PREFIX, which, latest)\n        self.delete_row_range(ROWCOUNT_PREFIX, which, latest)\n        self.latest_game_number = which\n\n    def bleakest_moves(self, start_game, end_game):\n        \"\"\"Given a range of games, return the bleakest moves.\n\n        Returns a list of (game, move, q) sorted by q.\n        \"\"\"\n        bleak = b'bleakest_q'\n        rows = self.bt_table.read_rows(\n            ROW_PREFIX.format(start_game),\n            ROW_PREFIX.format(end_game),\n            filter_=bigtable_row_filters.ColumnRangeFilter(\n                METADATA, bleak, bleak))\n\n        def parse(r):\n            rk = str(r.row_key, 'utf-8')\n            g, m = _game_row_key.match(rk).groups()\n            q = r.cell_value(METADATA, bleak)\n            return int(g), int(m), float(q)\n        return sorted([parse(r) for r in rows], key=operator.itemgetter(2))\n\n    def require_fresh_games(self, number_fresh):\n        \"\"\"Require a given number of fresh games to be played.\n\n        Args:\n          number_fresh:  integer, number of new fresh games needed\n\n        Increments the cell `table_state=metadata:wait_for_game_number`\n        by the given number of games.  This will cause\n        `self.wait_for_fresh_games()` to block until the game\n        counter has reached this number.\n        \"\"\"\n        latest = self.latest_game_number\n        table_state = self.bt_table.row(TABLE_STATE)\n        table_state.set_cell(METADATA, WAIT_CELL, int(latest + number_fresh))\n        table_state.commit()\n        print(\"== Setting wait cell to \", int(latest + number_fresh), flush=True)\n\n    def wait_for_fresh_games(self, poll_interval=15.0):\n        \"\"\"Block caller until required new games have been played.\n\n        Args:\n          poll_interval:  number of seconds to wait between checks\n\n        If the cell `table_state=metadata:wait_for_game_number` exists,\n        then block the caller, checking every `poll_interval` seconds,\n        until `table_state=metadata:game_counter is at least the value\n        in that cell.\n        \"\"\"\n        wait_until_game = self.read_wait_cell()\n        if not wait_until_game:\n            return\n        latest_game = self.latest_game_number\n        last_latest = latest_game\n        while latest_game < wait_until_game:\n            utils.dbg('Latest game {} not yet at required game {} '\n                      '(+{}, {:0.3f} games/sec)'.format(\n                          latest_game,\n                          wait_until_game,\n                          latest_game - last_latest,\n                          (latest_game - last_latest) / poll_interval\n                      ))\n            time.sleep(poll_interval)\n            last_latest = latest_game\n            latest_game = self.latest_game_number\n\n    def read_wait_cell(self):\n        \"\"\"Read the value of the cell holding the 'wait' value,\n\n        Returns the int value of whatever it has, or None if the cell doesn't\n        exist.\n        \"\"\"\n\n        table_state = self.bt_table.read_row(\n            TABLE_STATE,\n            filter_=bigtable_row_filters.ColumnRangeFilter(\n                METADATA, WAIT_CELL, WAIT_CELL))\n        if table_state is None:\n            utils.dbg('No waiting for new games needed; '\n                      'wait_for_game_number column not in table_state')\n            return None\n        value = table_state.cell_value(METADATA, WAIT_CELL)\n        if not value:\n            utils.dbg('No waiting for new games needed; '\n                      'no value in wait_for_game_number cell '\n                      'in table_state')\n            return None\n        return cbt_intvalue(value)\n\n    def count_moves_in_game_range(self, game_begin, game_end):\n        \"\"\"Count the total moves in a game range.\n\n        Args:\n          game_begin:  integer, starting game\n          game_end:  integer, ending game\n\n        Uses the `ct_` keyspace for rapid move summary.\n        \"\"\"\n        rows = self.bt_table.read_rows(\n            ROWCOUNT_PREFIX.format(game_begin),\n            ROWCOUNT_PREFIX.format(game_end),\n            filter_=bigtable_row_filters.ColumnRangeFilter(\n                METADATA, MOVE_COUNT, MOVE_COUNT))\n        return sum([int(r.cell_value(METADATA, MOVE_COUNT)) for r in rows])\n\n    def moves_from_games(self, start_game, end_game, moves, shuffle,\n                         column_family, column):\n        \"\"\"Dataset of samples and/or shuffled moves from game range.\n\n        Args:\n          n:  an integer indicating how many past games should be sourced.\n          moves:  an integer indicating how many moves should be sampled\n            from those N games.\n          column_family:  name of the column family containing move examples.\n          column:  name of the column containing move examples.\n          shuffle:  if True, shuffle the selected move examples.\n\n        Returns:\n          A dataset containing no more than `moves` examples, sampled\n            randomly from the last `n` games in the table.\n        \"\"\"\n        start_row = ROW_PREFIX.format(start_game)\n        end_row = ROW_PREFIX.format(end_game)\n        # NOTE:  Choose a probability high enough to guarantee at least the\n        # required number of moves, by using a slightly lower estimate\n        # of the total moves, then trimming the result.\n        total_moves = self.count_moves_in_game_range(start_game, end_game)\n        probability = moves / (total_moves * 0.99)\n        utils.dbg('Row range: %s - %s; total moves: %d; probability %.3f; moves %d' % (\n            start_row, end_row, total_moves, probability, moves))\n        ds = self.tf_table.parallel_scan_range(start_row, end_row,\n                                               probability=probability,\n                                               columns=[(column_family, column)])\n        if shuffle:\n            utils.dbg('Doing a complete shuffle of %d moves' % moves)\n            ds = ds.shuffle(moves)\n        ds = ds.take(moves)\n        return ds\n\n    def moves_from_last_n_games(self, n, moves, shuffle,\n                                column_family, column):\n        \"\"\"Randomly choose a given number of moves from the last n games.\n\n        Args:\n          n:  number of games at the end of this GameQueue to source.\n          moves:  number of moves to be sampled from `n` games.\n          shuffle:  if True, shuffle the selected moves.\n          column_family:  name of the column family containing move examples.\n          column:  name of the column containing move examples.\n\n        Returns:\n          a dataset containing the selected moves.\n        \"\"\"\n        self.wait_for_fresh_games()\n        latest_game = self.latest_game_number\n        utils.dbg('Latest game in %s: %s' % (self.btspec.table, latest_game))\n        if latest_game == 0:\n            raise ValueError('Cannot find a latest game in the table')\n\n        start = int(max(0, latest_game - n))\n        ds = self.moves_from_games(start, latest_game, moves, shuffle,\n                                   column_family, column)\n        return ds\n\n    def _write_move_counts(self, sess, h):\n        \"\"\"Add move counts from the given histogram to the table.\n\n        Used to update the move counts in an existing table.  Should\n        not be needed except for backfill or repair.\n\n        Args:\n          sess:  TF session to use for doing a Bigtable write.\n          tf_table:  TF Cloud Bigtable to use for writing.\n          h:  a dictionary keyed by game row prefix (\"g_0023561\") whose values\n             are the move counts for each game.\n        \"\"\"\n        def gen():\n            for k, v in h.items():\n                # The keys in the histogram may be of type 'bytes'\n                k = str(k, 'utf-8')\n                vs = str(v)\n                yield (k.replace('g_', 'ct_') + '_%d' % v, vs)\n                yield (k + '_m_000', vs)\n        mc = tf.data.Dataset.from_generator(gen, (tf.string, tf.string))\n        wr_op = self.tf_table.write(mc,\n                                    column_families=[METADATA],\n                                    columns=[MOVE_COUNT])\n        sess.run(wr_op)\n\n    def update_move_counts(self, start_game, end_game, interval=1000):\n        \"\"\"Used to update the move_count cell for older games.\n\n        Should not be needed except for backfill or repair.\n\n        move_count cells will be updated in both g_<game_id>_m_000 rows\n        and ct_<game_id>_<move_count> rows.\n        \"\"\"\n        for g in range(start_game, end_game, interval):\n            with tf.Session() as sess:\n                start_row = ROW_PREFIX.format(g)\n                end_row = ROW_PREFIX.format(g + interval)\n                print('Range:', start_row, end_row)\n                start_time = time.time()\n                ds = self.tf_table.keys_by_range_dataset(start_row, end_row)\n                h = _histogram_move_keys_by_game(sess, ds)\n                self._write_move_counts(sess, h)\n                end_time = time.time()\n                elapsed = end_time - start_time\n                print('  games/sec:', len(h)/elapsed)\n\n\ndef set_fresh_watermark(game_queue, count_from, window_size,\n                        fresh_fraction=0.05, minimum_fresh=20000):\n    \"\"\"Sets the metadata cell used to block until some quantity of games have been played.\n\n    This sets the 'freshness mark' on the `game_queue`, used to block training\n    until enough new games have been played.  The number of fresh games required\n    is the larger of:\n       - The fraction of the total window size\n       - The `minimum_fresh` parameter\n    The number of games required can be indexed from the 'count_from' parameter.\n    Args:\n      game_queue: A GameQueue object, on whose backing table will be modified.\n      count_from: the index of the game to compute the increment from\n      window_size:  an integer indicating how many past games are considered\n      fresh_fraction: a float in (0,1] indicating the fraction of games to wait for\n      minimum_fresh:  an integer indicating the lower bound on the number of new\n      games.\n    \"\"\"\n    already_played = game_queue.latest_game_number - count_from\n    print(\"== already_played: \", already_played, flush=True)\n    if window_size > count_from:  # How to handle the case when the window is not yet 'full'\n        game_queue.require_fresh_games(int(minimum_fresh * .9))\n    else:\n        num_to_play = max(0, math.ceil(window_size * .9 * fresh_fraction) - already_played)\n        print(\"== Num to play: \", num_to_play, flush=True)\n        game_queue.require_fresh_games(num_to_play)\n\n\ndef mix_by_decile(games, moves, deciles=9):\n    \"\"\"Compute a mix of regular and calibration games by decile.\n\n    deciles should be an integer between 0 and 10 inclusive.\n    \"\"\"\n    assert 0 <= deciles <= 10\n    # The prefixes and suffixes below have the following meanings:\n    #   ct_: count\n    #   fr_: fraction\n    #    _r: resign (ordinary)\n    #   _nr: no-resign\n    ct_total = 10\n    lesser = ct_total - math.floor(deciles)\n    greater = ct_total - lesser\n    ct_r, ct_nr = greater, lesser\n    fr_r = ct_r / ct_total\n    fr_nr = ct_nr / ct_total\n    games_r = math.ceil(games * fr_r)\n    moves_r = math.ceil(moves * fr_r)\n    games_c = math.floor(games * fr_nr)\n    moves_c = math.floor(moves * fr_nr)\n    selection = np.array([0] * ct_r + [1] * ct_nr, dtype=np.int64)\n    return GameMix(games_r, moves_r,\n                   games_c, moves_c,\n                   selection)\n\n\ndef get_unparsed_moves_from_last_n_games(games, games_nr, n,\n                                         moves=2**21,\n                                         shuffle=True,\n                                         column_family=TFEXAMPLE,\n                                         column='example',\n                                         values_only=True):\n    \"\"\"Get a dataset of serialized TFExamples from the last N games.\n\n    Args:\n      games, games_nr: GameQueues of the regular selfplay and calibration\n        (aka 'no resign') games to sample from.\n      n:  an integer indicating how many past games should be sourced.\n      moves:  an integer indicating how many moves should be sampled\n        from those N games.\n      column_family:  name of the column family containing move examples.\n      column:  name of the column containing move examples.\n      shuffle:  if True, shuffle the selected move examples.\n      values_only: if True, return only column values, no row keys.\n\n    Returns:\n      A dataset containing no more than `moves` examples, sampled\n        randomly from the last `n` games in the table.\n    \"\"\"\n    mix = mix_by_decile(n, moves, 9)\n    resign = games.moves_from_last_n_games(\n        mix.games_r,\n        mix.moves_r,\n        shuffle,\n        column_family, column)\n    no_resign = games_nr.moves_from_last_n_games(\n        mix.games_c,\n        mix.moves_c,\n        shuffle,\n        column_family, column)\n    choice = tf.data.Dataset.from_tensor_slices(mix.selection).repeat().take(moves)\n    ds = tf.data.experimental.choose_from_datasets([resign, no_resign], choice)\n    if shuffle:\n        ds = ds.shuffle(len(mix.selection) * 2)\n    if values_only:\n        ds = ds.map(lambda row_name, s: s)\n    return ds\n\n\ndef get_unparsed_moves_from_games(games_r, games_c,\n                                  start_r, start_c,\n                                  mix,\n                                  shuffle=True,\n                                  column_family=TFEXAMPLE,\n                                  column='example',\n                                  values_only=True):\n    \"\"\"Get a dataset of serialized TFExamples from a given start point.\n\n    Args:\n      games_r, games_c: GameQueues of the regular selfplay and calibration\n        (aka 'no resign') games to sample from.\n      start_r: an integer indicating the game number to start at in games_r.\n      start_c: an integer indicating the game number to start at in games_c.\n      mix: the result of mix_by_decile()\n      shuffle:  if True, shuffle the selected move examples.\n      column_family:  name of the column family containing move examples.\n      column:  name of the column containing move examples.\n      values_only: if True, return only column values, no row keys.\n\n    Returns:\n      A dataset containing no more than the moves implied by `mix`,\n        sampled randomly from the game ranges implied.\n    \"\"\"\n    resign = games_r.moves_from_games(\n        start_r, start_r + mix.games_r, mix.moves_r, shuffle, column_family, column)\n    calibrated = games_c.moves_from_games(\n        start_c, start_c + mix.games_c, mix.moves_c, shuffle, column_family, column)\n    moves = mix.moves_r + mix.moves_c\n    choice = tf.data.Dataset.from_tensor_slices(mix.selection).repeat().take(moves)\n    ds = tf.data.experimental.choose_from_datasets([resign, calibrated], choice)\n    if shuffle:\n        ds = ds.shuffle(len(mix.selection) * 2)\n    if values_only:\n        ds = ds.map(lambda row_name, s: s)\n    return ds\n\n\ndef count_elements_in_dataset(ds, batch_size=1*1024, parallel_batch=8):\n    \"\"\"Count and return all the elements in the given dataset.\n\n    Debugging function.  The elements in a dataset cannot be counted\n    without enumerating all of them.  By counting in batch and in\n    parallel, this method allows rapid traversal of the dataset.\n\n    Args:\n      ds:  The dataset whose elements should be counted.\n      batch_size:  the number of elements to count a a time.\n      parallel_batch:  how many batches to count in parallel.\n\n    Returns:\n      The number of elements in the dataset.\n    \"\"\"\n    with tf.Session() as sess:\n        dsc = ds.apply(tf.data.experimental.enumerate_dataset())\n        dsc = dsc.apply(tf.data.experimental.map_and_batch(\n            lambda c, v: c, batch_size, num_parallel_batches=parallel_batch))\n        iterator = dsc.make_initializable_iterator()\n        sess.run(iterator.initializer)\n        get_next = iterator.get_next()\n        counted = 0\n        try:\n            while True:\n                # The numbers in the tensors are 0-based indicies,\n                # so add 1 to get the number counted.\n                counted = sess.run(tf.reduce_max(get_next)) + 1\n                utils.dbg('Counted so far: %d' % counted)\n        except tf.errors.OutOfRangeError:\n            pass\n        utils.dbg('Counted total: %d' % counted)\n        return counted\n"
        },
        {
          "name": "bigtable_output.py",
          "type": "blob",
          "size": 1.69140625,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helpers to write data to Bigtable.\n\"\"\"\n\nimport sgf_wrapper\n\ndef process_game(path):\n    \"\"\"\n    Get CBT metadata from a SGF file.\n\n    Calling function should probably overwrite 'tool'.\n    \"\"\"\n    with open(path) as f:\n        sgf_contents = f.read()\n\n    root_node = sgf_wrapper.get_sgf_root_node(sgf_contents)\n    assert root_node.properties['FF'] == ['4'], (\"Bad game record\", path)\n\n    result = root_node.properties['RE'][0]\n    assert result.lower()[0] in 'bw', result\n    assert result.lower()[1] == '+', result\n    black_won = result.lower()[0] == 'b'\n\n    length = 0\n    node = root_node.next\n    while node:\n        props = node.properties\n        length += 1 if props.get('B') or props.get('W') else 0\n        node = node.next\n\n    return {\n        \"black\": root_node.properties['PB'][0],\n        \"white\": root_node.properties['PW'][0],\n        # All values are strings, \"1\" for true and \"0\" for false here\n        \"black_won\": '1' if black_won else '0',\n        \"white_won\": '0' if black_won else '1',\n        \"result\": result,\n        \"length\": str(length),\n        \"sgf\": path,\n        \"tag\": \"\",\n        \"tool\": \"bigtable_output\",\n    }\n"
        },
        {
          "name": "bootstrap.py",
          "type": "blob",
          "size": 1.3779296875,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Bootstrap network weights.\n\nUsage:\nbootstrap.py \\\n    --work_dir=/tmp/estimator_working_dir \\\n    --export_path=/tmp/published_models_dir\n\"\"\"\n\nimport os\n\nfrom absl import app, flags\nimport dual_net\nimport utils\n\nflags.DEFINE_string('export_path', None,\n                    'Where to export the model after training.')\n\nflags.DEFINE_bool('create_bootstrap', True,\n                  'Whether to create a bootstrap model before exporting')\n\nflags.declare_key_flag('work_dir')\n\nFLAGS = flags.FLAGS\n\n\ndef main(unused_argv):\n    \"\"\"Bootstrap random weights.\"\"\"\n    utils.ensure_dir_exists(os.path.dirname(FLAGS.export_path))\n    if FLAGS.create_bootstrap:\n        dual_net.bootstrap()\n    dual_net.export_model(FLAGS.export_path)\n\n\nif __name__ == '__main__':\n    flags.mark_flags_as_required(['work_dir', 'export_path'])\n    app.run(main)\n"
        },
        {
          "name": "cc",
          "type": "tree",
          "content": null
        },
        {
          "name": "cloud_logging.py",
          "type": "blob",
          "size": 1.5732421875,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Helpers to configure cloud logging.\"\"\"\n\nimport logging\nimport contextlib\nimport io\nimport sys\nimport os\n\nimport google.cloud.logging as glog\n\nLOGGING_PROJECT = os.environ.get('LOGGING_PROJECT', '')\n\n\ndef configure(project=LOGGING_PROJECT):\n    \"\"\"Configures cloud logging\n\n    This is called for all main calls. If a $LOGGING_PROJECT is environment\n    variable configured, then STDERR and STDOUT are redirected to cloud\n    logging.\n    \"\"\"\n    if not project:\n        sys.stderr.write('!! Error: The $LOGGING_PROJECT enviroment '\n                         'variable is required in order to set up cloud logging. '\n                         'Cloud logging is disabled.\\n')\n        return\n\n    try:\n        # if this fails, redirect stderr to /dev/null so no startup spam.\n        with contextlib.redirect_stderr(io.StringIO()):\n            client = glog.Client(project)\n            client.setup_logging(logging.INFO)\n    except:\n        logging.basicConfig(level=logging.INFO)\n        sys.stderr.write('!! Cloud logging disabled\\n')\n"
        },
        {
          "name": "cluster",
          "type": "tree",
          "content": null
        },
        {
          "name": "coords.py",
          "type": "blob",
          "size": 3.2509765625,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Logic for dealing with coordinates.\n\nThis introduces some helpers and terminology that are used throughout Minigo.\n\nMinigo Coordinate: This is a tuple of the form (row, column) that is indexed\n    starting out at (0, 0) from the upper-left.\nFlattened Coordinate: this is a number ranging from 0 - N^2 (so N^2+1\n    possible values). The extra value N^2 is used to mark a 'pass' move.\nSGF Coordinate: Coordinate used for SGF serialization format. Coordinates use\n    two-letter pairs having the form (column, row) indexed from the upper-left\n    where 0, 0 = 'aa'.\nGTP Coordinate: Human-readable coordinate string indexed from bottom left, with\n    the first character a capital letter for the column and the second a number\n    from 1-19 for the row. Note that GTP chooses to skip the letter 'I' due to\n    its similarity with 'l' (lowercase 'L').\nPYGTP Coordinate: Tuple coordinate indexed starting at 1,1 from bottom-left\n    in the format (column, row)\n\nSo, for a 19x19,\n\nCoord Type      upper_left      upper_right     pass\n-------------------------------------------------------\nminigo coord    (0, 0)          (0, 18)         None\nflat            0               18              361\nSGF             'aa'            'sa'            ''\nGTP             'A19'           'T19'           'pass'\n\"\"\"\n\nimport go\n\n# We provide more than 19 entries here in case of boards larger than 19 x 19.\n_SGF_COLUMNS = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ'\n_GTP_COLUMNS = 'ABCDEFGHJKLMNOPQRSTUVWXYZ'\n\n\ndef from_flat(flat):\n    \"\"\"Converts from a flattened coordinate to a Minigo coordinate.\"\"\"\n    if flat == go.N * go.N:\n        return None\n    return divmod(flat, go.N)\n\n\ndef to_flat(coord):\n    \"\"\"Converts from a Minigo coordinate to a flattened coordinate.\"\"\"\n    if coord is None:\n        return go.N * go.N\n    return go.N * coord[0] + coord[1]\n\n\ndef from_sgf(sgfc):\n    \"\"\"Converts from an SGF coordinate to a Minigo coordinate.\"\"\"\n    if sgfc is None or sgfc == '' or (go.N <= 19 and sgfc == 'tt'):\n        return None\n    return _SGF_COLUMNS.index(sgfc[1]), _SGF_COLUMNS.index(sgfc[0])\n\n\ndef to_sgf(coord):\n    \"\"\"Converts from a Minigo coordinate to an SGF coordinate.\"\"\"\n    if coord is None:\n        return ''\n    return _SGF_COLUMNS[coord[1]] + _SGF_COLUMNS[coord[0]]\n\n\ndef from_gtp(gtpc):\n    \"\"\"Converts from a GTP coordinate to a Minigo coordinate.\"\"\"\n    gtpc = gtpc.upper()\n    if gtpc == 'PASS':\n        return None\n    col = _GTP_COLUMNS.index(gtpc[0])\n    row_from_bottom = int(gtpc[1:])\n    return go.N - row_from_bottom, col\n\n\ndef to_gtp(coord):\n    \"\"\"Converts from a Minigo coordinate to a GTP coordinate.\"\"\"\n    if coord is None:\n        return 'pass'\n    y, x = coord\n    return '{}{}'.format(_GTP_COLUMNS[x], go.N - y)\n"
        },
        {
          "name": "dual_net.py",
          "type": "blob",
          "size": 28.9658203125,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nThe policy and value networks share a majority of their architecture.\nThis helps the intermediate layers extract concepts that are relevant to both\nmove prediction and score estimation.\n\"\"\"\n\nfrom absl import flags\nimport functools\nimport json\nimport logging\nimport os.path\nimport struct\nimport tempfile\nimport time\nimport numpy as np\nimport random\n\nimport tensorflow as tf\nfrom tensorflow.contrib import cluster_resolver as contrib_cluster_resolver\nfrom tensorflow.contrib import quantize as contrib_quantize\nfrom tensorflow.contrib import summary as contrib_summary\nfrom tensorflow.contrib import tpu as contrib_tpu\nfrom tensorflow.contrib.tpu.python.tpu import tpu_config as contrib_tpu_python_tpu_tpu_config\nfrom tensorflow.contrib.tpu.python.tpu import tpu_estimator as contrib_tpu_python_tpu_tpu_estimator\nfrom tensorflow.contrib.tpu.python.tpu import tpu_optimizer as contrib_tpu_python_tpu_tpu_optimizer\n\nimport features as features_lib\nimport go\nimport symmetries\nimport minigo_model\n\n\nflags.DEFINE_integer('train_batch_size', 256,\n                     'Batch size to use for train/eval evaluation. For GPU '\n                     'this is batch size as expected. If \\\"use_tpu\\\" is set,'\n                     'final batch size will be = train_batch_size * num_tpu_cores')\n\nflags.DEFINE_integer('conv_width', 256 if go.N == 19 else 32,\n                     'The width of each conv layer in the shared trunk.')\n\nflags.DEFINE_integer('policy_conv_width', 2,\n                     'The width of the policy conv layer.')\n\nflags.DEFINE_integer('value_conv_width', 1,\n                     'The width of the value conv layer.')\n\nflags.DEFINE_integer('fc_width', 256 if go.N == 19 else 64,\n                     'The width of the fully connected layer in value head.')\n\nflags.DEFINE_integer('trunk_layers', go.N,\n                     'The number of resnet layers in the shared trunk.')\n\nflags.DEFINE_multi_integer('lr_boundaries', [400000, 600000],\n                           'The number of steps at which the learning rate will decay')\n\nflags.DEFINE_multi_float('lr_rates', [0.01, 0.001, 0.0001],\n                         'The different learning rates')\n\nflags.DEFINE_integer('training_seed', 0,\n                     'Random seed to use for training and validation')\n\nflags.register_multi_flags_validator(\n    ['lr_boundaries', 'lr_rates'],\n    lambda flags: len(flags['lr_boundaries']) == len(flags['lr_rates']) - 1,\n    'Number of learning rates must be exactly one greater than the number of boundaries')\n\nflags.DEFINE_float('l2_strength', 1e-4,\n                   'The L2 regularization parameter applied to weights.')\n\nflags.DEFINE_float('value_cost_weight', 1.0,\n                   'Scalar for value_cost, AGZ paper suggests 1/100 for '\n                   'supervised learning')\n\nflags.DEFINE_float('sgd_momentum', 0.9,\n                   'Momentum parameter for learning rate.')\n\nflags.DEFINE_string('work_dir', None,\n                    'The Estimator working directory. Used to dump: '\n                    'checkpoints, tensorboard logs, etc..')\n\nflags.DEFINE_bool('use_tpu', False, 'Whether to use TPU for training.')\n\nflags.DEFINE_string(\n    'tpu_name', None,\n    'The Cloud TPU to use for training. This should be either the name used'\n    'when creating the Cloud TPU, or a grpc://ip.address.of.tpu:8470 url.')\n\nflags.DEFINE_integer(\n    'num_tpu_cores', default=8,\n    help=('Number of TPU cores. For a single TPU device, this is 8 because each'\n          ' TPU has 4 chips each with 2 cores.'))\n\nflags.DEFINE_string('gpu_device_list', None,\n                    'Comma-separated list of GPU device IDs to use.')\n\nflags.DEFINE_bool('quantize', False,\n                  'Whether create a quantized model. When loading a model for '\n                  'inference, this must match how the model was trained.')\n\nflags.DEFINE_integer('quant_delay', 700 * 1024,\n                     'Number of training steps after which weights and '\n                     'activations are quantized.')\n\nflags.DEFINE_integer(\n    'iterations_per_loop', 128,\n    help=('Number of steps to run on TPU before outfeeding metrics to the CPU.'\n          ' If the number of iterations in the loop would exceed the number of'\n          ' train steps, the loop will exit before reaching'\n          ' --iterations_per_loop. The larger this value is, the higher the'\n          ' utilization on the TPU.'))\n\nflags.DEFINE_integer(\n    'summary_steps', default=256,\n    help='Number of steps between logging summary scalars.')\n\nflags.DEFINE_integer(\n    'keep_checkpoint_max', default=5, help='Number of checkpoints to keep.')\n\nflags.DEFINE_bool(\n    'use_random_symmetry', True,\n    help='If true random symmetries be used when doing inference.')\n\nflags.DEFINE_bool(\n    'use_SE', False,\n    help='Use Squeeze and Excitation.')\n\nflags.DEFINE_bool(\n    'use_SE_bias', False,\n    help='Use Squeeze and Excitation with bias.')\n\nflags.DEFINE_integer(\n    'SE_ratio', 2,\n    help='Squeeze and Excitation ratio.')\n\nflags.DEFINE_bool(\n    'use_swish', False,\n    help=('Use Swish activation function inplace of ReLu. '\n          'https://arxiv.org/pdf/1710.05941.pdf'))\n\nflags.DEFINE_bool(\n    'bool_features', False,\n    help='Use bool input features instead of float')\n\nflags.DEFINE_string(\n    'input_features', 'agz',\n    help='Type of input features: \"agz\" or \"mlperf07\"')\n\nflags.DEFINE_string(\n    'input_layout', 'nhwc',\n    help='Layout of input features: \"nhwc\" or \"nchw\"')\n\n\n# TODO(seth): Verify if this is still required.\nflags.register_multi_flags_validator(\n    ['use_tpu', 'iterations_per_loop', 'summary_steps'],\n    lambda flags: (not flags['use_tpu'] or\n                   flags['summary_steps'] % flags['iterations_per_loop'] == 0),\n    'If use_tpu, summary_steps must be a multiple of iterations_per_loop')\n\nFLAGS = flags.FLAGS\n\n\nclass DualNetwork():\n    def __init__(self, save_file):\n        self.save_file = save_file\n        self.inference_input = None\n        self.inference_output = None\n        config = tf.ConfigProto()\n        config.gpu_options.allow_growth = True\n        if FLAGS.gpu_device_list is not None:\n            config.gpu_options.visible_device_list = FLAGS.gpu_device_list\n        self.sess = tf.Session(graph=tf.Graph(), config=config)\n        self.initialize_graph()\n\n    def initialize_graph(self):\n        with self.sess.graph.as_default():\n            features, labels = get_inference_input()\n            params = FLAGS.flag_values_dict()\n            logging.info('TPU inference is supported on C++ only. '\n                         'DualNetwork will ignore use_tpu=True')\n            params['use_tpu'] = False\n            estimator_spec = model_fn(features, labels,\n                                      tf.estimator.ModeKeys.PREDICT,\n                                      params=params)\n            self.inference_input = features\n            self.inference_output = estimator_spec.predictions\n            if self.save_file is not None:\n                self.initialize_weights(self.save_file)\n            else:\n                self.sess.run(tf.global_variables_initializer())\n\n    def initialize_weights(self, save_file):\n        \"\"\"Initialize the weights from the given save_file.\n        Assumes that the graph has been constructed, and the\n        save_file contains weights that match the graph. Used\n        to set the weights to a different version of the player\n        without redifining the entire graph.\"\"\"\n        tf.train.Saver().restore(self.sess, save_file)\n\n    def run(self, position):\n        probs, values = self.run_many([position])\n        return probs[0], values[0]\n\n    def run_many(self, positions):\n        f = get_features()\n        processed = [features_lib.extract_features(p, f) for p in positions]\n        if FLAGS.use_random_symmetry:\n            syms_used, processed = symmetries.randomize_symmetries_feat(\n                processed)\n        outputs = self.sess.run(self.inference_output,\n                                feed_dict={self.inference_input: processed})\n        probabilities, value = outputs['policy_output'], outputs['value_output']\n        if FLAGS.use_random_symmetry:\n            probabilities = symmetries.invert_symmetries_pi(\n                syms_used, probabilities)\n        return probabilities, value\n\n\ndef get_features_planes():\n    if FLAGS.input_features == 'agz':\n        return features_lib.AGZ_FEATURES_PLANES\n    elif FLAGS.input_features == 'mlperf07':\n        return features_lib.MLPERF07_FEATURES_PLANES\n    else:\n        raise ValueError('unrecognized input features \"%s\"' %\n                         FLAGS.input_features)\n\n\ndef get_features():\n    if FLAGS.input_features == 'agz':\n        return features_lib.AGZ_FEATURES\n    elif FLAGS.input_features == 'mlperf07':\n        return features_lib.MLPERF07_FEATURES\n    else:\n        raise ValueError('unrecognized input features \"%s\"' %\n                         FLAGS.input_features)\n\n\ndef get_inference_input():\n    \"\"\"Set up placeholders for input features/labels.\n\n    Returns the feature, output tensors that get passed into model_fn.\"\"\"\n    feature_type = tf.bool if FLAGS.bool_features else tf.float32\n    if FLAGS.input_layout == 'nhwc':\n        feature_shape = [None, go.N, go.N, get_features_planes()]\n    elif FLAGS.input_layout == 'nchw':\n        feature_shape = [None, get_features_planes(), go.N, go.N]\n    else:\n        raise ValueError('invalid input_layout \"%s\"' % FLAGS.input_layout)\n    return (tf.placeholder(feature_type, feature_shape, name='pos_tensor'),\n            {'pi_tensor': tf.placeholder(tf.float32, [None, go.N * go.N + 1]),\n             'value_tensor': tf.placeholder(tf.float32, [None])})\n\n\ndef model_fn(features, labels, mode, params):\n    \"\"\"\n    Create the model for estimator api\n\n    Args:\n        features: if input_layout == 'nhwc', a tensor with shape:\n                [BATCH_SIZE, go.N, go.N, get_features_planes()]\n            else, a tensor with shape:\n                [BATCH_SIZE, get_features_planes(), go.N, go.N]\n        labels: dict from string to tensor with shape\n            'pi_tensor': [BATCH_SIZE, go.N * go.N + 1]\n            'value_tensor': [BATCH_SIZE]\n        mode: a tf.estimator.ModeKeys (batchnorm params update for TRAIN only)\n        params: A dictionary (Typically derived from the FLAGS object.)\n    Returns: tf.estimator.EstimatorSpec with props\n        mode: same as mode arg\n        predictions: dict of tensors\n            'policy': [BATCH_SIZE, go.N * go.N + 1]\n            'value': [BATCH_SIZE]\n        loss: a single value tensor\n        train_op: train op\n        eval_metric_ops\n    return dict of tensors\n        logits: [BATCH_SIZE, go.N * go.N + 1]\n    \"\"\"\n\n    policy_output, value_output, logits = model_inference_fn(\n        features, mode == tf.estimator.ModeKeys.TRAIN, params)\n\n    # train ops\n    policy_cost = tf.reduce_mean(\n        tf.nn.softmax_cross_entropy_with_logits_v2(\n            logits=logits, labels=tf.stop_gradient(labels['pi_tensor'])))\n\n    value_cost = params['value_cost_weight'] * tf.reduce_mean(\n        tf.square(value_output - labels['value_tensor']))\n\n    reg_vars = [v for v in tf.trainable_variables()\n                if 'bias' not in v.name and 'beta' not in v.name]\n    l2_cost = params['l2_strength'] * \\\n        tf.add_n([tf.nn.l2_loss(v) for v in reg_vars])\n\n    combined_cost = policy_cost + value_cost + l2_cost\n\n    global_step = tf.train.get_or_create_global_step()\n    learning_rate = tf.train.piecewise_constant(\n        global_step, params['lr_boundaries'], params['lr_rates'])\n    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n\n    # Insert quantization ops if requested\n    if params['quantize']:\n        if mode == tf.estimator.ModeKeys.TRAIN:\n            contrib_quantize.create_training_graph(\n                quant_delay=params['quant_delay'])\n        else:\n            contrib_quantize.create_eval_graph()\n\n    optimizer = tf.train.MomentumOptimizer(\n        learning_rate, params['sgd_momentum'])\n    if params['use_tpu']:\n        optimizer = contrib_tpu_python_tpu_tpu_optimizer.CrossShardOptimizer(\n            optimizer)\n    with tf.control_dependencies(update_ops):\n        train_op = optimizer.minimize(combined_cost, global_step=global_step)\n\n    # Computations to be executed on CPU, outside of the main TPU queues.\n    def eval_metrics_host_call_fn(policy_output, value_output, pi_tensor,\n                                  value_tensor, policy_cost, value_cost,\n                                  l2_cost, combined_cost, step,\n                                  est_mode=tf.estimator.ModeKeys.TRAIN):\n        policy_entropy = -tf.reduce_mean(tf.reduce_sum(\n            policy_output * tf.log(policy_output), axis=1))\n        # pi_tensor is one_hot when generated from sgfs (for supervised learning)\n        # and soft-max when using self-play records. argmax normalizes the two.\n        policy_target_top_1 = tf.argmax(pi_tensor, axis=1)\n\n        policy_output_in_top1 = tf.to_float(\n            tf.nn.in_top_k(policy_output, policy_target_top_1, k=1))\n        policy_output_in_top3 = tf.to_float(\n            tf.nn.in_top_k(policy_output, policy_target_top_1, k=3))\n\n        policy_top_1_confidence = tf.reduce_max(policy_output, axis=1)\n        policy_target_top_1_confidence = tf.boolean_mask(\n            policy_output,\n            tf.one_hot(policy_target_top_1, tf.shape(policy_output)[1]))\n\n        value_cost_normalized = value_cost / params['value_cost_weight']\n        avg_value_observed = tf.reduce_mean(value_tensor)\n\n        with tf.variable_scope('metrics'):\n            metric_ops = {\n                'policy_cost': tf.metrics.mean(policy_cost),\n                'value_cost': tf.metrics.mean(value_cost),\n                'value_cost_normalized': tf.metrics.mean(value_cost_normalized),\n                'l2_cost': tf.metrics.mean(l2_cost),\n                'policy_entropy': tf.metrics.mean(policy_entropy),\n                'combined_cost': tf.metrics.mean(combined_cost),\n                'avg_value_observed': tf.metrics.mean(avg_value_observed),\n                'policy_accuracy_top_1': tf.metrics.mean(policy_output_in_top1),\n                'policy_accuracy_top_3': tf.metrics.mean(policy_output_in_top3),\n                'policy_top_1_confidence': tf.metrics.mean(policy_top_1_confidence),\n                'policy_target_top_1_confidence': tf.metrics.mean(\n                    policy_target_top_1_confidence),\n                'value_confidence': tf.metrics.mean(tf.abs(value_output)),\n            }\n\n        if est_mode == tf.estimator.ModeKeys.EVAL:\n            return metric_ops\n\n        # NOTE: global_step is rounded to a multiple of FLAGS.summary_steps.\n        eval_step = tf.reduce_min(step)\n\n        # Create summary ops so that they show up in SUMMARIES collection\n        # That way, they get logged automatically during training\n        summary_writer = contrib_summary.create_file_writer(FLAGS.work_dir)\n        with summary_writer.as_default(), \\\n                contrib_summary.record_summaries_every_n_global_steps(\n                    params['summary_steps'], eval_step):\n            for metric_name, metric_op in metric_ops.items():\n                contrib_summary.scalar(\n                    metric_name, metric_op[1], step=eval_step)\n\n        # Reset metrics occasionally so that they are mean of recent batches.\n        reset_op = tf.variables_initializer(tf.local_variables('metrics'))\n        cond_reset_op = tf.cond(\n            tf.equal(eval_step % params['summary_steps'], tf.to_int64(1)),\n            lambda: reset_op,\n            lambda: tf.no_op())\n\n        return contrib_summary.all_summary_ops() + [cond_reset_op]\n\n    metric_args = [\n        policy_output,\n        value_output,\n        labels['pi_tensor'],\n        labels['value_tensor'],\n        tf.reshape(policy_cost, [1]),\n        tf.reshape(value_cost, [1]),\n        tf.reshape(l2_cost, [1]),\n        tf.reshape(combined_cost, [1]),\n        tf.reshape(global_step, [1]),\n    ]\n\n    predictions = {\n        'policy_output': policy_output,\n        'value_output': value_output,\n    }\n\n    eval_metrics_only_fn = functools.partial(\n        eval_metrics_host_call_fn, est_mode=tf.estimator.ModeKeys.EVAL)\n    host_call_fn = functools.partial(\n        eval_metrics_host_call_fn, est_mode=tf.estimator.ModeKeys.TRAIN)\n\n    tpu_estimator_spec = contrib_tpu_python_tpu_tpu_estimator.TPUEstimatorSpec(\n        mode=mode,\n        predictions=predictions,\n        loss=combined_cost,\n        train_op=train_op,\n        eval_metrics=(eval_metrics_only_fn, metric_args),\n        host_call=(host_call_fn, metric_args)\n    )\n    if params['use_tpu']:\n        return tpu_estimator_spec\n    else:\n        return tpu_estimator_spec.as_estimator_spec()\n\n\ndef model_inference_fn(features, training, params):\n    \"\"\"Builds just the inference part of the model graph.\n\n    Args:\n        features: input features tensor.\n        training: True if the model is training.\n        params: A dictionary\n\n    Returns:\n        (policy_output, value_output, logits) tuple of tensors.\n    \"\"\"\n\n    if FLAGS.bool_features:\n        features = tf.dtypes.cast(features, dtype=tf.float32)\n\n    if FLAGS.input_layout == 'nhwc':\n        bn_axis = -1\n        data_format = 'channels_last'\n    else:\n        bn_axis = 1\n        data_format = 'channels_first'\n\n    mg_batchn = functools.partial(\n        tf.layers.batch_normalization,\n        axis=bn_axis,\n        momentum=.95,\n        epsilon=1e-5,\n        center=True,\n        scale=True,\n        fused=True,\n        training=training)\n\n    mg_conv2d = functools.partial(\n        tf.layers.conv2d,\n        filters=params['conv_width'],\n        kernel_size=3,\n        padding='same',\n        use_bias=False,\n        data_format=data_format)\n\n    mg_global_avgpool2d = functools.partial(\n        tf.layers.average_pooling2d,\n        pool_size=go.N,\n        strides=1,\n        padding='valid',\n        data_format=data_format)\n\n    def mg_activation(inputs):\n        if FLAGS.use_swish:\n            return tf.nn.swish(inputs)\n\n        return tf.nn.relu(inputs)\n\n    def residual_inner(inputs):\n        conv_layer1 = mg_batchn(mg_conv2d(inputs))\n        initial_output = mg_activation(conv_layer1)\n        conv_layer2 = mg_batchn(mg_conv2d(initial_output))\n        return conv_layer2\n\n    def mg_res_layer(inputs):\n        residual = residual_inner(inputs)\n        output = mg_activation(inputs + residual)\n        return output\n\n    def mg_squeeze_excitation_layer(inputs):\n        # Hu, J., Shen, L., & Sun, G. (2018). Squeeze-and-Excitation Networks.\n        # 2018 IEEE/CVF Conference on Computer Vision, 7132-7141.\n        # arXiv:1709.01507 [cs.CV]\n\n        channels = params['conv_width']\n        ratio = FLAGS.SE_ratio\n        assert channels % ratio == 0\n\n        residual = residual_inner(inputs)\n        pool = mg_global_avgpool2d(residual)\n        fc1 = tf.layers.dense(pool, units=channels // ratio)\n        squeeze = mg_activation(fc1)\n\n        if FLAGS.use_SE_bias:\n            fc2 = tf.layers.dense(squeeze, units=2*channels)\n            # Channels_last so axis = 3 = -1\n            gamma, bias = tf.split(fc2, 2, axis=3)\n        else:\n            gamma = tf.layers.dense(squeeze, units=channels)\n            bias = 0\n\n        sig = tf.nn.sigmoid(gamma)\n        # Explicitly signal the broadcast.\n        scale = tf.reshape(sig, [-1, 1, 1, channels])\n\n        excitation = tf.multiply(scale, residual) + bias\n        return mg_activation(inputs + excitation)\n\n    initial_block = mg_activation(mg_batchn(mg_conv2d(features)))\n\n    # the shared stack\n    shared_output = initial_block\n    for _ in range(params['trunk_layers']):\n        if FLAGS.use_SE or FLAGS.use_SE_bias:\n            shared_output = mg_squeeze_excitation_layer(shared_output)\n        else:\n            shared_output = mg_res_layer(shared_output)\n\n    # Policy head\n    policy_conv = mg_conv2d(\n        shared_output, filters=params['policy_conv_width'], kernel_size=1)\n    policy_conv = mg_activation(\n        mg_batchn(policy_conv, center=False, scale=False))\n    logits = tf.layers.dense(\n        tf.reshape(\n            policy_conv, [-1, params['policy_conv_width'] * go.N * go.N]),\n        go.N * go.N + 1)\n\n    policy_output = tf.nn.softmax(logits, name='policy_output')\n\n    # Value head\n    value_conv = mg_conv2d(\n        shared_output, filters=params['value_conv_width'], kernel_size=1)\n    value_conv = mg_activation(\n        mg_batchn(value_conv, center=False, scale=False))\n\n    value_fc_hidden = mg_activation(tf.layers.dense(\n        tf.reshape(value_conv, [-1, params['value_conv_width'] * go.N * go.N]),\n        params['fc_width']))\n    value_output = tf.nn.tanh(\n        tf.reshape(tf.layers.dense(value_fc_hidden, 1), [-1]),\n        name='value_output')\n\n    return policy_output, value_output, logits\n\n\ndef tpu_model_inference_fn(features):\n    \"\"\"Builds the model graph suitable for running on TPU.\n\n    It does two things:\n     1) Mark all weights as constant, which improves TPU inference performance\n        because it prevents the weights being transferred to the TPU every call\n        to Session.run().\n     2) Adds constant to the graph with a unique value and marks it as a\n        dependency on the rest of the model. This works around a TensorFlow bug\n        that prevents multiple models being run on a single TPU.\n\n    Returns:\n        (policy_output, value_output, logits) tuple of tensors.\n    \"\"\"\n    def custom_getter(getter, name, *args, **kwargs):\n        with tf.control_dependencies(None):\n            return tf.guarantee_const(\n                getter(name, *args, **kwargs), name=name + '/GuaranteeConst')\n    with tf.variable_scope('', custom_getter=custom_getter):\n        # TODO(tommadams): remove the tf.control_dependencies context manager\n        # when a fixed version of TensorFlow is released.\n        t = int(time.time())\n        epoch_time = tf.constant(t, name='epoch_time_%d' % t)\n        with tf.control_dependencies([epoch_time]):\n            if FLAGS.input_layout == 'nhwc':\n                feature_shape = [-1, go.N, go.N, get_features_planes()]\n            else:\n                feature_shape = [-1, get_features_planes(), go.N, go.N]\n            features = tf.reshape(features, feature_shape)\n            return model_inference_fn(features, False, FLAGS.flag_values_dict())\n\n\ndef maybe_set_seed():\n    if FLAGS.training_seed != 0:\n        random.seed(FLAGS.training_seed)\n        tf.set_random_seed(FLAGS.training_seed)\n        np.random.seed(FLAGS.training_seed)\n\n\ndef get_estimator():\n    if FLAGS.use_tpu:\n        return _get_tpu_estimator()\n    else:\n        return _get_nontpu_estimator()\n\n\ndef _get_nontpu_estimator():\n    session_config = tf.ConfigProto()\n    session_config.gpu_options.allow_growth = True\n    run_config = tf.estimator.RunConfig(\n        save_summary_steps=FLAGS.summary_steps,\n        keep_checkpoint_max=FLAGS.keep_checkpoint_max,\n        session_config=session_config)\n    return tf.estimator.Estimator(\n        model_fn,\n        model_dir=FLAGS.work_dir,\n        config=run_config,\n        params=FLAGS.flag_values_dict())\n\n\ndef _get_tpu_estimator():\n    tpu_cluster_resolver = contrib_cluster_resolver.TPUClusterResolver(\n        FLAGS.tpu_name, zone=None, project=None)\n    tpu_grpc_url = tpu_cluster_resolver.get_master()\n\n    run_config = contrib_tpu_python_tpu_tpu_config.RunConfig(\n        master=tpu_grpc_url,\n        evaluation_master=tpu_grpc_url,\n        model_dir=FLAGS.work_dir,\n        save_checkpoints_steps=max(1000, FLAGS.iterations_per_loop),\n        save_summary_steps=FLAGS.summary_steps,\n        keep_checkpoint_max=FLAGS.keep_checkpoint_max,\n        session_config=tf.ConfigProto(\n            allow_soft_placement=True, log_device_placement=True),\n        tpu_config=contrib_tpu_python_tpu_tpu_config.TPUConfig(\n            iterations_per_loop=FLAGS.iterations_per_loop,\n            num_shards=FLAGS.num_tpu_cores,\n            per_host_input_for_training=contrib_tpu_python_tpu_tpu_config.InputPipelineConfig.PER_HOST_V2))\n\n    return contrib_tpu_python_tpu_tpu_estimator.TPUEstimator(\n        use_tpu=FLAGS.use_tpu,\n        model_fn=model_fn,\n        config=run_config,\n        train_batch_size=FLAGS.train_batch_size * FLAGS.num_tpu_cores,\n        eval_batch_size=FLAGS.train_batch_size * FLAGS.num_tpu_cores,\n        params=FLAGS.flag_values_dict())\n\n\ndef bootstrap():\n    \"\"\"Initialize a tf.Estimator run with random initial weights.\"\"\"\n    # a bit hacky - forge an initial checkpoint with the name that subsequent\n    # Estimator runs will expect to find.\n    #\n    # Estimator will do this automatically when you call train(), but calling\n    # train() requires data, and I didn't feel like creating training data in\n    # order to run the full train pipeline for 1 step.\n    maybe_set_seed()\n    initial_checkpoint_name = 'model.ckpt-1'\n    save_file = os.path.join(FLAGS.work_dir, initial_checkpoint_name)\n    sess = tf.Session(graph=tf.Graph())\n    with sess.graph.as_default():\n        features, labels = get_inference_input()\n        model_fn(features, labels, tf.estimator.ModeKeys.PREDICT,\n                 params=FLAGS.flag_values_dict())\n        sess.run(tf.global_variables_initializer())\n        tf.train.Saver().save(sess, save_file)\n\n\ndef export_model(model_path):\n    \"\"\"Take the latest checkpoint and copy it to model_path.\n\n    Assumes that all relevant model files are prefixed by the same name.\n    (For example, foo.index, foo.meta and foo.data-00000-of-00001).\n\n    Args:\n        model_path: The path (can be a gs:// path) to export model\n    \"\"\"\n    estimator = tf.estimator.Estimator(model_fn, model_dir=FLAGS.work_dir,\n                                       params=FLAGS.flag_values_dict())\n    latest_checkpoint = estimator.latest_checkpoint()\n    all_checkpoint_files = tf.gfile.Glob(latest_checkpoint + '*')\n    for filename in all_checkpoint_files:\n        suffix = filename.partition(latest_checkpoint)[2]\n        destination_path = model_path + suffix\n        print('Copying {} to {}'.format(filename, destination_path))\n        tf.gfile.Copy(filename, destination_path)\n\n\ndef freeze_graph(model_path, use_trt=False, trt_max_batch_size=8,\n                 trt_precision='fp32'):\n    output_names = ['policy_output', 'value_output']\n\n    n = DualNetwork(model_path)\n    out_graph = tf.graph_util.convert_variables_to_constants(\n        n.sess, n.sess.graph.as_graph_def(), output_names)\n\n    if use_trt:\n        import tensorflow.contrib.tensorrt as trt\n        out_graph = trt.create_inference_graph(\n            input_graph_def=out_graph,\n            outputs=output_names,\n            max_batch_size=trt_max_batch_size,\n            max_workspace_size_bytes=1 << 29,\n            precision_mode=trt_precision)\n\n    metadata = make_model_metadata({\n        'engine': 'tf',\n        'use_trt': bool(use_trt),\n    })\n\n    minigo_model.write_graph_def(out_graph, metadata, model_path + '.minigo')\n\n\ndef freeze_graph_tpu(model_path):\n    \"\"\"Custom freeze_graph implementation for Cloud TPU.\"\"\"\n\n    assert model_path\n    assert FLAGS.tpu_name\n    if FLAGS.tpu_name.startswith('grpc://'):\n        tpu_grpc_url = FLAGS.tpu_name\n    else:\n        tpu_cluster_resolver = contrib_cluster_resolver.TPUClusterResolver(\n            FLAGS.tpu_name, zone=None, project=None)\n        tpu_grpc_url = tpu_cluster_resolver.get_master()\n    sess = tf.Session(tpu_grpc_url)\n\n    output_names = []\n    with sess.graph.as_default():\n        # Replicate the inference function for each TPU core.\n        replicated_features = []\n        feature_type = tf.bool if FLAGS.bool_features else tf.float32\n        for i in range(FLAGS.num_tpu_cores):\n            name = 'pos_tensor_%d' % i\n            features = tf.placeholder(\n                feature_type, [None], name=name)\n            replicated_features.append((features,))\n        outputs = contrib_tpu.replicate(\n            tpu_model_inference_fn, replicated_features)\n\n        # The replicate op assigns names like output_0_shard_0 to the output\n        # names. Give them human readable names.\n        for i, (policy_output, value_output, _) in enumerate(outputs):\n            policy_name = 'policy_output_%d' % i\n            value_name = 'value_output_%d' % i\n            output_names.extend([policy_name, value_name])\n            tf.identity(policy_output, policy_name)\n            tf.identity(value_output, value_name)\n\n        tf.train.Saver().restore(sess, model_path)\n\n    out_graph = tf.graph_util.convert_variables_to_constants(\n        sess, sess.graph.as_graph_def(), output_names)\n\n    metadata = make_model_metadata({\n        'engine': 'tpu',\n        'num_replicas': FLAGS.num_tpu_cores,\n    })\n\n    minigo_model.write_graph_def(out_graph, metadata, model_path + '.minigo')\n\n\ndef make_model_metadata(metadata):\n    for f in ['conv_width', 'fc_width', 'trunk_layers', 'use_SE', 'use_SE_bias',\n              'use_swish', 'input_features', 'input_layout']:\n        metadata[f] = getattr(FLAGS, f)\n    metadata['input_type'] = 'bool' if FLAGS.bool_features else 'float'\n    metadata['board_size'] = go.N\n    return metadata\n"
        },
        {
          "name": "dual_net_edge_tpu.py",
          "type": "blob",
          "size": 3,
          "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"This file contains an implementation of dual_net.py for Google's EdgeTPU.\nIt can only be used for inference and requires a specially quantized and\ncompiled model file.\n\nFor more information see https://coral.withgoogle.com\n\"\"\"\n\nimport numpy as np\nimport features as features_lib\nimport go\nfrom edgetpu.basic.basic_engine import BasicEngine  # pylint: disable=import-error\n\n\ndef extract_agz_features(position):\n    return features_lib.extract_features(position, features_lib.AGZ_FEATURES)\n\n\nclass DualNetworkEdgeTpu():\n    \"\"\"DualNetwork implementation for Google's EdgeTPU.\"\"\"\n\n    def __init__(self, save_file):\n        self.engine = BasicEngine(save_file)\n        self.board_size = go.N\n        self.output_policy_size = self.board_size**2 + 1\n\n        input_tensor_shape = self.engine.get_input_tensor_shape()\n        expected_input_shape = [1, self.board_size, self.board_size, 17]\n        if not np.array_equal(input_tensor_shape, expected_input_shape):\n            raise RuntimeError(\n                'Invalid input tensor shape {}. Expected: {}'.format(\n                    input_tensor_shape, expected_input_shape))\n        output_tensors_sizes = self.engine.get_all_output_tensors_sizes()\n        expected_output_tensor_sizes = [self.output_policy_size, 1]\n        if not np.array_equal(output_tensors_sizes,\n                              expected_output_tensor_sizes):\n            raise RuntimeError(\n                'Invalid output tensor sizes {}. Expected: {}'.format(\n                    output_tensors_sizes, expected_output_tensor_sizes))\n\n    def run(self, position):\n        \"\"\"Runs inference on a single position.\"\"\"\n        probs, values = self.run_many([position])\n        return probs[0], values[0]\n\n    def run_many(self, positions):\n        \"\"\"Runs inference on a list of position.\"\"\"\n        processed = map(extract_agz_features, positions)\n        probabilities = []\n        values = []\n        for state in processed:\n            assert state.shape == (self.board_size, self.board_size,\n                                   17), str(state.shape)\n            result = self.engine.RunInference(state.flatten())\n            # If needed you can get the raw inference time from the result object.\n            # inference_time = result[0] # ms\n            policy_output = result[1][0:self.output_policy_size]\n            value_output = result[1][-1]\n            probabilities.append(policy_output)\n            values.append(value_output)\n        return probabilities, values\n"
        },
        {
          "name": "evaluate.py",
          "type": "blob",
          "size": 4.44921875,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Evalation plays games between two neural nets.\"\"\"\n\nimport os\nimport time\nfrom absl import app, flags\nfrom tensorflow import gfile\n\nimport dual_net\nfrom strategies import MCTSPlayer\nimport sgf_wrapper\nimport utils\n\nflags.DEFINE_string('eval_sgf_dir', None, 'Where to write evaluation results.')\n\nflags.DEFINE_integer('num_evaluation_games', 16, 'How many games to play')\n\n# From strategies.py\nflags.declare_key_flag('num_readouts')\nflags.declare_key_flag('verbose')\n\nFLAGS = flags.FLAGS\n\n\ndef play_match(black_model, white_model, games, sgf_dir):\n    \"\"\"Plays matches between two neural nets.\n\n    Args:\n        black_model: Path to the model for black player\n        white_model: Path to the model for white player\n    \"\"\"\n    with utils.logged_timer(\"Loading weights\"):\n        black_net = dual_net.DualNetwork(black_model)\n        white_net = dual_net.DualNetwork(white_model)\n\n    readouts = FLAGS.num_readouts\n\n    black = MCTSPlayer(black_net, two_player_mode=True)\n    white = MCTSPlayer(white_net, two_player_mode=True)\n\n    black_name = os.path.basename(black_net.save_file)\n    white_name = os.path.basename(white_net.save_file)\n\n    for i in range(games):\n        num_move = 0  # The move number of the current game\n\n        for player in [black, white]:\n            player.initialize_game()\n            first_node = player.root.select_leaf()\n            prob, val = player.network.run(first_node.position)\n            first_node.incorporate_results(prob, val, first_node)\n\n        while True:\n            start = time.time()\n            active = white if num_move % 2 else black\n            inactive = black if num_move % 2 else white\n\n            current_readouts = active.root.N\n            while active.root.N < current_readouts + readouts:\n                active.tree_search()\n\n            # print some stats on the search\n            if FLAGS.verbose >= 3:\n                print(active.root.position)\n\n            # First, check the roots for hopeless games.\n            if active.should_resign():  # Force resign\n                active.set_result(-1 *\n                                  active.root.position.to_play, was_resign=True)\n                inactive.set_result(\n                    active.root.position.to_play, was_resign=True)\n\n            if active.is_done():\n                fname = \"{:d}-{:s}-vs-{:s}-{:d}.sgf\".format(int(time.time()),\n                                                            white_name, black_name, i)\n                active.set_result(active.root.position.result(), was_resign=False)\n                with gfile.GFile(os.path.join(sgf_dir, fname), 'w') as _file:\n                    sgfstr = sgf_wrapper.make_sgf(active.position.recent,\n                                                  active.result_string, black_name=black_name,\n                                                  white_name=white_name)\n                    _file.write(sgfstr)\n                print(\"Finished game\", i, active.result_string)\n                break\n\n            move = active.pick_move()\n            active.play_move(move)\n            inactive.play_move(move)\n\n            dur = time.time() - start\n            num_move += 1\n\n            if (FLAGS.verbose > 1) or (FLAGS.verbose == 1 and num_move % 10 == 9):\n                timeper = (dur / readouts) * 100.0\n                print(active.root.position)\n                print(\"%d: %d readouts, %.3f s/100. (%.2f sec)\" % (num_move,\n                                                                   readouts,\n                                                                   timeper,\n                                                                   dur))\n\n\ndef main(argv):\n    \"\"\"Play matches between two neural nets.\"\"\"\n    _, black_model, white_model = argv\n    utils.ensure_dir_exists(FLAGS.eval_sgf_dir)\n    play_match(black_model, white_model, FLAGS.num_evaluation_games, FLAGS.eval_sgf_dir)\n\n\nif __name__ == '__main__':\n    flags.mark_flag_as_required('eval_sgf_dir')\n    app.run(main)\n"
        },
        {
          "name": "features.py",
          "type": "blob",
          "size": 6.154296875,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nFeatures used by AlphaGo Zero, in approximate order of importance.\nFeature                 # Notes\nStone History           16 The stones of each color during the last 8 moves.\nOnes                    1  Constant plane of 1s\nAll features with 8 planes are 1-hot encoded, with plane i marked with 1\nonly if the feature was equal to i. Any features >= 8 would be marked as 8.\n\nThis file includes the features from the first paper as DEFAULT_FEATURES\nand the features from AGZ as AGZ_FEATURES.\n\"\"\"\n\nimport numpy as np\nimport go\nimport utils\n\n# Resolution/truncation limit for one-hot features\nP = 8\n\n\ndef make_onehot(feature, planes):\n    onehot_features = np.zeros(feature.shape + (planes,), dtype=np.uint8)\n    capped = np.minimum(feature, planes)\n    onehot_index_offsets = np.arange(0, utils.product(\n        onehot_features.shape), planes) + capped.ravel()\n    # A 0 is encoded as [0,0,0,0], not [1,0,0,0], so we'll\n    # filter out any offsets that are a multiple of $planes\n    # A 1 is encoded as [1,0,0,0], not [0,1,0,0], so subtract 1 from offsets\n    nonzero_elements = (capped != 0).ravel()\n    nonzero_index_offsets = onehot_index_offsets[nonzero_elements] - 1\n    onehot_features.ravel()[nonzero_index_offsets] = 1\n    return onehot_features\n\n\ndef planes(num_planes):\n    def deco(f):\n        f.planes = num_planes\n        return f\n    return deco\n\n\n# TODO(tommadams): add a generic stone_features for all N <= 8\n@planes(16)\ndef stone_features(position):\n    # a bit easier to calculate it with axis 0 being the 16 board states,\n    # and then roll axis 0 to the end.\n    features = np.zeros([16, go.N, go.N], dtype=np.uint8)\n\n    num_deltas_avail = position.board_deltas.shape[0]\n    cumulative_deltas = np.cumsum(position.board_deltas, axis=0)\n    last_eight = np.tile(position.board, [8, 1, 1])\n    # apply deltas to compute previous board states\n    last_eight[1:num_deltas_avail + 1] -= cumulative_deltas\n    # if no more deltas are available, just repeat oldest board.\n    last_eight[num_deltas_avail +\n               1:] = last_eight[num_deltas_avail].reshape(1, go.N, go.N)\n\n    features[::2] = last_eight == position.to_play\n    features[1::2] = last_eight == -position.to_play\n    return np.rollaxis(features, 0, 3)\n\n\n# TODO(tommadams): add a generic stone_features for all N <= 8\n@planes(8)\ndef stone_features_4(position):\n    # a bit easier to calculate it with axis 0 being the 16 board states,\n    # and then roll axis 0 to the end.\n    features = np.zeros([8, go.N, go.N], dtype=np.uint8)\n\n    num_deltas_avail = position.board_deltas.shape[0]\n    cumulative_deltas = np.cumsum(position.board_deltas, axis=0)\n    last = np.tile(position.board, [4, 1, 1])\n    # apply deltas to compute previous board states\n    last[1:num_deltas_avail + 1] -= cumulative_deltas\n    # if no more deltas are available, just repeat oldest board.\n    last[num_deltas_avail + 1:] = last[num_deltas_avail].reshape(1, go.N, go.N)\n\n    features[::2] = last == position.to_play\n    features[1::2] = last == -position.to_play\n    return np.rollaxis(features, 0, 3)\n\n\n@planes(1)\ndef color_to_play_feature(position):\n    if position.to_play == go.BLACK:\n        return np.ones([go.N, go.N, 1], dtype=np.uint8)\n    else:\n        return np.zeros([go.N, go.N, 1], dtype=np.uint8)\n\n\n@planes(3)\ndef stone_color_feature(position):\n    board = position.board\n    features = np.zeros([go.N, go.N, 3], dtype=np.uint8)\n    if position.to_play == go.BLACK:\n        features[board == go.BLACK, 0] = 1\n        features[board == go.WHITE, 1] = 1\n    else:\n        features[board == go.WHITE, 0] = 1\n        features[board == go.BLACK, 1] = 1\n\n    features[board == go.EMPTY, 2] = 1\n    return features\n\n\n@planes(1)\ndef ones_feature(position):\n    return np.ones([go.N, go.N, 1], dtype=np.uint8)\n\n\n@planes(P)\ndef recent_move_feature(position):\n    onehot_features = np.zeros([go.N, go.N, P], dtype=np.uint8)\n    for i, player_move in enumerate(reversed(position.recent[-P:])):\n        _, move = player_move  # unpack the info from position.recent\n        if move is not None:\n            onehot_features[move[0], move[1], i] = 1\n    return onehot_features\n\n\n@planes(P)\ndef liberty_feature(position):\n    return make_onehot(position.get_liberties(), P)\n\n\n@planes(3)\ndef few_liberties_feature(position):\n    feature = position.get_liberties()\n    onehot_features = np.zeros(feature.shape + (3,), dtype=np.uint8)\n    onehot_index_offsets = np.arange(0, utils.product(\n        onehot_features.shape), 3) + feature.ravel()\n    nonzero_elements = ((feature != 0) & (feature <= 3)).ravel()\n    nonzero_index_offsets = onehot_index_offsets[nonzero_elements] - 1\n    onehot_features.ravel()[nonzero_index_offsets] = 1\n    return onehot_features\n\n\n@planes(1)\ndef would_capture_feature(position):\n    features = np.zeros([go.N, go.N, 1], dtype=np.uint8)\n    for g in position.lib_tracker.groups.values():\n        if g.color == position.to_play:\n            continue\n        if len(g.liberties) == 1:\n            lib = next(iter(g.liberties))\n            features[lib + (0,)] = 1\n    return features\n\n\nDEFAULT_FEATURES = [\n    stone_color_feature,\n    ones_feature,\n    liberty_feature,\n    recent_move_feature,\n    would_capture_feature,\n]\n\nDEFAULT_FEATURES_PLANES = sum(f.planes for f in DEFAULT_FEATURES)\n\nAGZ_FEATURES = [\n    stone_features,\n    color_to_play_feature\n]\n\nAGZ_FEATURES_PLANES = sum(f.planes for f in AGZ_FEATURES)\n\nMLPERF07_FEATURES = [\n    stone_features_4,\n    color_to_play_feature,\n    few_liberties_feature,\n    would_capture_feature,\n]\n\nMLPERF07_FEATURES_PLANES = sum(f.planes for f in MLPERF07_FEATURES)\n\n\ndef extract_features(position, features):\n    return np.concatenate([feature(position) for feature in features], axis=2)\n"
        },
        {
          "name": "freeze_graph.py",
          "type": "blob",
          "size": 1.61328125,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Freeze a model to a GraphDef proto.\"\"\"\n\nfrom absl import app, flags\n\nimport dual_net\n\nflags.DEFINE_string('model_path', None, 'Path to model to freeze')\n\nflags.mark_flag_as_required('model_path')\n\nflags.DEFINE_boolean(\n    'use_trt', False, 'True to write a GraphDef that uses the TRT runtime')\nflags.DEFINE_integer('trt_max_batch_size', None,\n                     'Maximum TRT batch size')\nflags.DEFINE_string('trt_precision', 'fp32',\n                    'Precision for TRT runtime: fp16, fp32 or int8')\nflags.register_multi_flags_validator(\n    ['use_trt', 'trt_max_batch_size'],\n    lambda flags: not flags['use_trt'] or flags['trt_max_batch_size'],\n    'trt_max_batch_size must be set if use_trt is true')\n\nFLAGS = flags.FLAGS\n\n\ndef main(unused_argv):\n    \"\"\"Freeze a model to a GraphDef proto.\"\"\"\n    if FLAGS.use_tpu:\n        dual_net.freeze_graph_tpu(FLAGS.model_path)\n    else:\n        dual_net.freeze_graph(FLAGS.model_path, FLAGS.use_trt,\n                              FLAGS.trt_max_batch_size, FLAGS.trt_precision)\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n"
        },
        {
          "name": "go.py",
          "type": "blob",
          "size": 19.23828125,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nA board is a NxN numpy array.\nA Coordinate is a tuple index into the board.\nA Move is a (Coordinate c | None).\nA PlayerMove is a (Color, Move) tuple\n\n(0, 0) is considered to be the upper left corner of the board, and (18, 0) is the lower left.\n\"\"\"\nfrom collections import namedtuple\nimport copy\nimport itertools\nimport numpy as np\nimport os\n\nimport coords\n\nN = int(os.environ.get('BOARD_SIZE', 19))\n\n# Represent a board as a numpy array, with 0 empty, 1 is black, -1 is white.\n# This means that swapping colors is as simple as multiplying array by -1.\nWHITE, EMPTY, BLACK, FILL, KO, UNKNOWN = range(-1, 5)\n\n# Represents \"group not found\" in the LibertyTracker object\nMISSING_GROUP_ID = -1\n\nALL_COORDS = [(i, j) for i in range(N) for j in range(N)]\nEMPTY_BOARD = np.zeros([N, N], dtype=np.int8)\n\n\ndef _check_bounds(c):\n    return 0 <= c[0] < N and 0 <= c[1] < N\n\n\nNEIGHBORS = {(x, y): list(filter(_check_bounds, [\n    (x+1, y), (x-1, y), (x, y+1), (x, y-1)])) for x, y in ALL_COORDS}\nDIAGONALS = {(x, y): list(filter(_check_bounds, [\n    (x+1, y+1), (x+1, y-1), (x-1, y+1), (x-1, y-1)])) for x, y in ALL_COORDS}\n\n\nclass IllegalMove(Exception):\n    pass\n\n\nclass PlayerMove(namedtuple('PlayerMove', ['color', 'move'])):\n    pass\n\n\nclass PositionWithContext(namedtuple('SgfPosition', ['position', 'next_move', 'result'])):\n    pass\n\n\ndef place_stones(board, color, stones):\n    for s in stones:\n        board[s] = color\n\n\ndef replay_position(position, result):\n    \"\"\"\n    Wrapper for a go.Position which replays its history.\n    Assumes an empty start position! (i.e. no handicap, and history must be exhaustive.)\n\n    Result must be passed in, since a resign cannot be inferred from position\n    history alone.\n\n    for position_w_context in replay_position(position):\n        print(position_w_context.position)\n    \"\"\"\n    assert position.n == len(position.recent), \"Position history is incomplete\"\n    pos = Position(komi=position.komi)\n    for player_move in position.recent:\n        color, next_move = player_move\n        yield PositionWithContext(pos, next_move, result)\n        pos = pos.play_move(next_move, color=color)\n\n\ndef find_reached(board, c):\n    color = board[c]\n    chain = set([c])\n    reached = set()\n    frontier = [c]\n    while frontier:\n        current = frontier.pop()\n        chain.add(current)\n        for n in NEIGHBORS[current]:\n            if board[n] == color and n not in chain:\n                frontier.append(n)\n            elif board[n] != color:\n                reached.add(n)\n    return chain, reached\n\n\ndef is_koish(board, c):\n    'Check if c is surrounded on all sides by 1 color, and return that color'\n    if board[c] != EMPTY:\n        return None\n    neighbors = {board[n] for n in NEIGHBORS[c]}\n    if len(neighbors) == 1 and EMPTY not in neighbors:\n        return list(neighbors)[0]\n    else:\n        return None\n\n\ndef is_eyeish(board, c):\n    'Check if c is an eye, for the purpose of restricting MC rollouts.'\n    # pass is fine.\n    if c is None:\n        return\n    color = is_koish(board, c)\n    if color is None:\n        return None\n    diagonal_faults = 0\n    diagonals = DIAGONALS[c]\n    if len(diagonals) < 4:\n        diagonal_faults += 1\n    for d in diagonals:\n        if not board[d] in (color, EMPTY):\n            diagonal_faults += 1\n    if diagonal_faults > 1:\n        return None\n    else:\n        return color\n\n\nclass Group(namedtuple('Group', ['id', 'stones', 'liberties', 'color'])):\n    \"\"\"\n    stones: a frozenset of Coordinates belonging to this group\n    liberties: a frozenset of Coordinates that are empty and adjacent to this group.\n    color: color of this group\n    \"\"\"\n\n    def __eq__(self, other):\n        return self.stones == other.stones and self.liberties == other.liberties and self.color == other.color\n\n\nclass LibertyTracker():\n    @staticmethod\n    def from_board(board):\n        board = np.copy(board)\n        curr_group_id = 0\n        lib_tracker = LibertyTracker()\n        for color in (WHITE, BLACK):\n            while color in board:\n                curr_group_id += 1\n                found_color = np.where(board == color)\n                coord = found_color[0][0], found_color[1][0]\n                chain, reached = find_reached(board, coord)\n                liberties = frozenset(r for r in reached if board[r] == EMPTY)\n                new_group = Group(curr_group_id, frozenset(\n                    chain), liberties, color)\n                lib_tracker.groups[curr_group_id] = new_group\n                for s in chain:\n                    lib_tracker.group_index[s] = curr_group_id\n                place_stones(board, FILL, chain)\n\n        lib_tracker.max_group_id = curr_group_id\n\n        liberty_counts = np.zeros([N, N], dtype=np.uint8)\n        for group in lib_tracker.groups.values():\n            num_libs = len(group.liberties)\n            for s in group.stones:\n                liberty_counts[s] = num_libs\n        lib_tracker.liberty_cache = liberty_counts\n\n        return lib_tracker\n\n    def __init__(self, group_index=None, groups=None, liberty_cache=None, max_group_id=1):\n        # group_index: a NxN numpy array of group_ids. -1 means no group\n        # groups: a dict of group_id to groups\n        # liberty_cache: a NxN numpy array of liberty counts\n        self.group_index = group_index if group_index is not None else - \\\n            np.ones([N, N], dtype=np.int32)\n        self.groups = groups or {}\n        self.liberty_cache = liberty_cache if liberty_cache is not None else np.zeros([\n                                                                                      N, N], dtype=np.uint8)\n        self.max_group_id = max_group_id\n\n    def __deepcopy__(self, memodict={}):\n        new_group_index = np.copy(self.group_index)\n        new_lib_cache = np.copy(self.liberty_cache)\n        # shallow copy\n        new_groups = copy.copy(self.groups)\n        return LibertyTracker(new_group_index, new_groups, liberty_cache=new_lib_cache, max_group_id=self.max_group_id)\n\n    def add_stone(self, color, c):\n        assert self.group_index[c] == MISSING_GROUP_ID\n        captured_stones = set()\n        opponent_neighboring_group_ids = set()\n        friendly_neighboring_group_ids = set()\n        empty_neighbors = set()\n\n        for n in NEIGHBORS[c]:\n            neighbor_group_id = self.group_index[n]\n            if neighbor_group_id != MISSING_GROUP_ID:\n                neighbor_group = self.groups[neighbor_group_id]\n                if neighbor_group.color == color:\n                    friendly_neighboring_group_ids.add(neighbor_group_id)\n                else:\n                    opponent_neighboring_group_ids.add(neighbor_group_id)\n            else:\n                empty_neighbors.add(n)\n\n        new_group = self._merge_from_played(\n            color, c, empty_neighbors, friendly_neighboring_group_ids)\n\n        # new_group becomes stale as _update_liberties and\n        # _handle_captures are called; must refetch with self.groups[new_group.id]\n        for group_id in opponent_neighboring_group_ids:\n            neighbor_group = self.groups[group_id]\n            if len(neighbor_group.liberties) == 1:\n                captured = self._capture_group(group_id)\n                captured_stones.update(captured)\n            else:\n                self._update_liberties(group_id, remove={c})\n\n        self._handle_captures(captured_stones)\n\n        # suicide is illegal\n        if len(self.groups[new_group.id].liberties) == 0:\n            raise IllegalMove(\"Move at {} would commit suicide!\\n\".format(c))\n\n        return captured_stones\n\n    def _merge_from_played(self, color, played, libs, other_group_ids):\n        stones = {played}\n        liberties = set(libs)\n        for group_id in other_group_ids:\n            other = self.groups.pop(group_id)\n            stones.update(other.stones)\n            liberties.update(other.liberties)\n\n        if other_group_ids:\n            liberties.remove(played)\n        assert stones.isdisjoint(liberties)\n        self.max_group_id += 1\n        result = Group(\n            self.max_group_id,\n            frozenset(stones),\n            frozenset(liberties),\n            color)\n        self.groups[result.id] = result\n\n        for s in result.stones:\n            self.group_index[s] = result.id\n            self.liberty_cache[s] = len(result.liberties)\n\n        return result\n\n    def _capture_group(self, group_id):\n        dead_group = self.groups.pop(group_id)\n        for s in dead_group.stones:\n            self.group_index[s] = MISSING_GROUP_ID\n            self.liberty_cache[s] = 0\n        return dead_group.stones\n\n    def _update_liberties(self, group_id, add=set(), remove=set()):\n        group = self.groups[group_id]\n        new_libs = (group.liberties | add) - remove\n        self.groups[group_id] = Group(\n            group_id, group.stones, new_libs, group.color)\n\n        new_lib_count = len(new_libs)\n        for s in self.groups[group_id].stones:\n            self.liberty_cache[s] = new_lib_count\n\n    def _handle_captures(self, captured_stones):\n        for s in captured_stones:\n            for n in NEIGHBORS[s]:\n                group_id = self.group_index[n]\n                if group_id != MISSING_GROUP_ID:\n                    self._update_liberties(group_id, add={s})\n\n\nclass Position():\n    def __init__(self, board=None, n=0, komi=7.5, caps=(0, 0),\n                 lib_tracker=None, ko=None, recent=tuple(),\n                 board_deltas=None, to_play=BLACK):\n        \"\"\"\n        board: a numpy array\n        n: an int representing moves played so far\n        komi: a float, representing points given to the second player.\n        caps: a (int, int) tuple of captures for B, W.\n        lib_tracker: a LibertyTracker object\n        ko: a Move\n        recent: a tuple of PlayerMoves, such that recent[-1] is the last move.\n        board_deltas: a np.array of shape (n, go.N, go.N) representing changes\n            made to the board at each move (played move and captures).\n            Should satisfy next_pos.board - next_pos.board_deltas[0] == pos.board\n        to_play: BLACK or WHITE\n        \"\"\"\n        assert type(recent) is tuple\n        self.board = board if board is not None else np.copy(EMPTY_BOARD)\n        # With a full history, self.n == len(self.recent) == num moves played\n        self.n = n\n        self.komi = komi\n        self.caps = caps\n        self.lib_tracker = lib_tracker or LibertyTracker.from_board(self.board)\n        self.ko = ko\n        self.recent = recent\n        self.board_deltas = board_deltas if board_deltas is not None else np.zeros([\n                                                                                   0, N, N], dtype=np.int8)\n        self.to_play = to_play\n\n    def __deepcopy__(self, memodict={}):\n        new_board = np.copy(self.board)\n        new_lib_tracker = copy.deepcopy(self.lib_tracker)\n        return Position(new_board, self.n, self.komi, self.caps, new_lib_tracker, self.ko, self.recent, self.board_deltas, self.to_play)\n\n    def __str__(self, colors=True):\n        if colors:\n            pretty_print_map = {\n                WHITE: '\\x1b[0;31;47mO',\n                EMPTY: '\\x1b[0;31;43m.',\n                BLACK: '\\x1b[0;31;40mX',\n                FILL: '#',\n                KO: '*',\n            }\n        else:\n            pretty_print_map = {\n                WHITE: 'O',\n                EMPTY: '.',\n                BLACK: 'X',\n                FILL: '#',\n                KO: '*',\n            }\n        board = np.copy(self.board)\n        captures = self.caps\n        if self.ko is not None:\n            place_stones(board, KO, [self.ko])\n        raw_board_contents = []\n        for i in range(N):\n            row = [' ']\n            for j in range(N):\n                appended = '<' if (self.recent and (i, j) ==\n                                   self.recent[-1].move) else ' '\n                row.append(pretty_print_map[board[i, j]] + appended)\n                if colors:\n                    row.append('\\x1b[0m')\n\n            raw_board_contents.append(''.join(row))\n\n        row_labels = ['%2d' % i for i in range(N, 0, -1)]\n        annotated_board_contents = [''.join(r) for r in zip(\n            row_labels, raw_board_contents, row_labels)]\n        header_footer_rows = [\n            '   ' + ' '.join('ABCDEFGHJKLMNOPQRST'[:N]) + '   ']\n        annotated_board = '\\n'.join(itertools.chain(\n            header_footer_rows, annotated_board_contents, header_footer_rows))\n        details = \"\\nMove: {}. Captures X: {} O: {}\\n\".format(\n            self.n, *captures)\n        return annotated_board + details\n\n    def is_move_suicidal(self, move):\n        potential_libs = set()\n        for n in NEIGHBORS[move]:\n            neighbor_group_id = self.lib_tracker.group_index[n]\n            if neighbor_group_id == MISSING_GROUP_ID:\n                # at least one liberty after playing here, so not a suicide\n                return False\n            neighbor_group = self.lib_tracker.groups[neighbor_group_id]\n            if neighbor_group.color == self.to_play:\n                potential_libs |= neighbor_group.liberties\n            elif len(neighbor_group.liberties) == 1:\n                # would capture an opponent group if they only had one lib.\n                return False\n        # it's possible to suicide by connecting several friendly groups\n        # each of which had one liberty.\n        potential_libs -= set([move])\n        return not potential_libs\n\n    def is_move_legal(self, move):\n        'Checks that a move is on an empty space, not on ko, and not suicide'\n        if move is None:\n            return True\n        if self.board[move] != EMPTY:\n            return False\n        if move == self.ko:\n            return False\n        if self.is_move_suicidal(move):\n            return False\n\n        return True\n\n    def all_legal_moves(self):\n        'Returns a np.array of size go.N**2 + 1, with 1 = legal, 0 = illegal'\n        # by default, every move is legal\n        legal_moves = np.ones([N, N], dtype=np.int8)\n        # ...unless there is already a stone there\n        legal_moves[self.board != EMPTY] = 0\n        # calculate which spots have 4 stones next to them\n        # padding is because the edge always counts as a lost liberty.\n        adjacent = np.ones([N + 2, N + 2], dtype=np.int8)\n        adjacent[1:-1, 1:-1] = np.abs(self.board)\n        num_adjacent_stones = (adjacent[:-2, 1:-1] + adjacent[1:-1, :-2] +\n                               adjacent[2:, 1:-1] + adjacent[1:-1, 2:])\n        # Surrounded spots are those that are empty and have 4 adjacent stones.\n        surrounded_spots = np.multiply(\n            (self.board == EMPTY),\n            (num_adjacent_stones == 4))\n        # Such spots are possibly illegal, unless they are capturing something.\n        # Iterate over and manually check each spot.\n        for coord in np.transpose(np.nonzero(surrounded_spots)):\n            if self.is_move_suicidal(tuple(coord)):\n                legal_moves[tuple(coord)] = 0\n\n        # ...and retaking ko is always illegal\n        if self.ko is not None:\n            legal_moves[self.ko] = 0\n\n        # and pass is always legal\n        return np.concatenate([legal_moves.ravel(), [1]])\n\n    def pass_move(self, mutate=False):\n        pos = self if mutate else copy.deepcopy(self)\n        pos.n += 1\n        pos.recent += (PlayerMove(pos.to_play, None),)\n        pos.board_deltas = np.concatenate((\n            np.zeros([1, N, N], dtype=np.int8),\n            pos.board_deltas[:6]))\n        pos.to_play *= -1\n        pos.ko = None\n        return pos\n\n    def flip_playerturn(self, mutate=False):\n        pos = self if mutate else copy.deepcopy(self)\n        pos.ko = None\n        pos.to_play *= -1\n        return pos\n\n    def get_liberties(self):\n        return self.lib_tracker.liberty_cache\n\n    def play_move(self, c, color=None, mutate=False):\n        # Obeys CGOS Rules of Play. In short:\n        # No suicides\n        # Chinese/area scoring\n        # Positional superko (this is very crudely approximate at the moment.)\n        if color is None:\n            color = self.to_play\n\n        pos = self if mutate else copy.deepcopy(self)\n\n        if c is None:\n            pos = pos.pass_move(mutate=mutate)\n            return pos\n\n        if not self.is_move_legal(c):\n            raise IllegalMove(\"{} move at {} is illegal: \\n{}\".format(\n                \"Black\" if self.to_play == BLACK else \"White\",\n                coords.to_gtp(c), self))\n\n        potential_ko = is_koish(self.board, c)\n\n        place_stones(pos.board, color, [c])\n        captured_stones = pos.lib_tracker.add_stone(color, c)\n        place_stones(pos.board, EMPTY, captured_stones)\n\n        opp_color = color * -1\n\n        new_board_delta = np.zeros([N, N], dtype=np.int8)\n        new_board_delta[c] = color\n        place_stones(new_board_delta, color, captured_stones)\n\n        if len(captured_stones) == 1 and potential_ko == opp_color:\n            new_ko = list(captured_stones)[0]\n        else:\n            new_ko = None\n\n        if pos.to_play == BLACK:\n            new_caps = (pos.caps[0] + len(captured_stones), pos.caps[1])\n        else:\n            new_caps = (pos.caps[0], pos.caps[1] + len(captured_stones))\n\n        pos.n += 1\n        pos.caps = new_caps\n        pos.ko = new_ko\n        pos.recent += (PlayerMove(color, c),)\n\n        # keep a rolling history of last 7 deltas - that's all we'll need to\n        # extract the last 8 board states.\n        pos.board_deltas = np.concatenate((\n            new_board_delta.reshape(1, N, N),\n            pos.board_deltas[:6]))\n        pos.to_play *= -1\n        return pos\n\n    def is_game_over(self):\n        return (len(self.recent) >= 2 and\n                self.recent[-1].move is None and\n                self.recent[-2].move is None)\n\n    def score(self):\n        'Return score from B perspective. If W is winning, score is negative.'\n        working_board = np.copy(self.board)\n        while EMPTY in working_board:\n            unassigned_spaces = np.where(working_board == EMPTY)\n            c = unassigned_spaces[0][0], unassigned_spaces[1][0]\n            territory, borders = find_reached(working_board, c)\n            border_colors = set(working_board[b] for b in borders)\n            X_border = BLACK in border_colors\n            O_border = WHITE in border_colors\n            if X_border and not O_border:\n                territory_color = BLACK\n            elif O_border and not X_border:\n                territory_color = WHITE\n            else:\n                territory_color = UNKNOWN  # dame, or seki\n            place_stones(working_board, territory_color, territory)\n\n        return np.count_nonzero(working_board == BLACK) - np.count_nonzero(working_board == WHITE) - self.komi\n\n    def result(self):\n        score = self.score()\n        if score > 0:\n            return 1\n        elif score < 0:\n            return -1\n        else:\n            return 0\n\n    def result_string(self):\n        score = self.score()\n        if score > 0:\n            return 'B+' + '%.1f' % score\n        elif score < 0:\n            return 'W+' + '%.1f' % abs(score)\n        else:\n            return 'DRAW'\n"
        },
        {
          "name": "gtp.py",
          "type": "blob",
          "size": 2.9521484375,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"GTP-compliant entry point for Minigo.\"\"\"\n\nimport os\nimport sys\n\nfrom absl import app, flags\n\nfrom gtp_cmd_handlers import (\n    BasicCmdHandler, KgsCmdHandler, GoGuiCmdHandler, MiniguiBasicCmdHandler, RegressionsCmdHandler)\nimport gtp_engine\nfrom strategies import MCTSPlayer, CGOSPlayer\nfrom utils import dbg\n\n\nflags.DEFINE_bool('cgos_mode', False, 'Whether to use CGOS settings.')\n\nflags.DEFINE_bool('kgs_mode', False, 'Whether to use KGS courtesy-pass.')\n\nflags.DEFINE_bool('minigui_mode', False, 'Whether to add minigui logging.')\n\nflags.DEFINE_string('load_file', None, 'Path to model save files.')\n\n\n# See mcts.py, strategies.py for other configurations around gameplay\n\nFLAGS = flags.FLAGS\n\n\ndef make_gtp_instance(load_file, cgos_mode=False, kgs_mode=False,\n                      minigui_mode=False):\n    \"\"\"Takes a path to model files and set up a GTP engine instance.\"\"\"\n    # Here so we dont try load EdgeTPU python library unless we need to\n    if load_file.endswith(\".tflite\"):\n        from dual_net_edge_tpu import DualNetworkEdgeTpu\n        n = DualNetworkEdgeTpu(load_file)\n    else:\n        from dual_net import DualNetwork\n        n = DualNetwork(load_file)\n\n    if cgos_mode:\n        player = CGOSPlayer(network=n, seconds_per_move=5, timed_match=True,\n                            two_player_mode=True)\n    else:\n        player = MCTSPlayer(network=n, two_player_mode=True)\n\n    name = \"Minigo-\" + os.path.basename(load_file)\n    version = \"0.2\"\n\n    engine = gtp_engine.Engine()\n    engine.add_cmd_handler(\n        gtp_engine.EngineCmdHandler(engine, name, version))\n\n    if kgs_mode:\n        engine.add_cmd_handler(KgsCmdHandler(player))\n    engine.add_cmd_handler(RegressionsCmdHandler(player))\n    engine.add_cmd_handler(GoGuiCmdHandler(player))\n    if minigui_mode:\n        engine.add_cmd_handler(MiniguiBasicCmdHandler(player, courtesy_pass=kgs_mode))\n    else:\n        engine.add_cmd_handler(BasicCmdHandler(player, courtesy_pass=kgs_mode))\n\n    return engine\n\n\ndef main(argv):\n    \"\"\"Run Minigo in GTP mode.\"\"\"\n    del argv\n    engine = make_gtp_instance(FLAGS.load_file,\n                               cgos_mode=FLAGS.cgos_mode,\n                               kgs_mode=FLAGS.kgs_mode,\n                               minigui_mode=FLAGS.minigui_mode)\n    dbg(\"GTP engine ready\\n\")\n    for msg in sys.stdin:\n        if not engine.handle_msg(msg.strip()):\n            break\n\n\nif __name__ == '__main__':\n    app.run(main)\n"
        },
        {
          "name": "gtp_cmd_handlers.py",
          "type": "blob",
          "size": 12.859375,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nfrom datetime import datetime\nimport itertools\nimport json\nimport time\nimport sgf_wrapper\nimport go\nimport coords\nfrom utils import dbg\n\n\ndef translate_gtp_color(gtp_color):\n    if gtp_color.lower() in [\"b\", \"black\"]:\n        return go.BLACK\n    if gtp_color.lower() in [\"w\", \"white\"]:\n        return go.WHITE\n    raise ValueError(\"invalid color {}\".format(gtp_color))\n\n\nclass BasicCmdHandler(object):\n    \"\"\"GTP command handler for basic play commands.\"\"\"\n\n    def __init__(self, player, courtesy_pass=False):\n        self._komi = 6.5\n        self._player = player\n        self._player.initialize_game()\n        self._courtesy_pass = courtesy_pass\n\n    def cmd_boardsize(self, n: int):\n        if n != go.N:\n            raise ValueError(\"unsupported board size: {}\".format(n))\n\n    def cmd_clear_board(self):\n        position = self._player.get_position()\n        if (self._player.get_result_string() and\n                position and len(position.recent) > 1):\n            try:\n                sgf = self._player.to_sgf()\n                with open(datetime.now().strftime(\"%Y-%m-%d-%H:%M.sgf\"), 'w') as f:\n                    f.write(sgf)\n            except NotImplementedError:\n                pass\n            except:\n                dbg(\"Error saving sgf\")\n        self._player.initialize_game(go.Position(komi=self._komi))\n\n    def cmd_komi(self, komi: float):\n        self._komi = komi\n        self._player.get_position().komi = komi\n\n    def cmd_play(self, arg0: str, arg1=None):\n        if arg1 is None:\n            move = arg0\n        else:\n            # let's assume this never happens for now.\n            # self._accomodate_out_of_turn(translate_gtp_color(arg0))\n            move = arg1\n        return self._player.play_move(coords.from_gtp(move))\n\n    def cmd_genmove(self, color=None):\n        if color is not None:\n            self._accomodate_out_of_turn(color)\n\n        if self._courtesy_pass:\n            # If courtesy pass is True and the previous move was a pass, we'll\n            # pass too, regardless of score or our opinion on the game.\n            position = self._player.get_position()\n            if position.recent and position.recent[-1].move is None:\n                return \"pass\"\n\n        move = self._player.suggest_move(self._player.get_position())\n        if self._player.should_resign():\n            self._player.set_result(-1 * self._player.get_position().to_play,\n                                    was_resign=True)\n            return \"resign\"\n\n        self._player.play_move(move)\n        if self._player.get_root().is_done():\n            self._player.set_result(self._player.get_position().result(),\n                                    was_resign=False)\n        return coords.to_gtp(move)\n\n    def cmd_undo(self):\n        raise NotImplementedError()\n\n    def cmd_showboard(self):\n        dbg('\\n\\n' + str(self._player.get_position()) + '\\n\\n')\n        return True\n\n    def cmd_final_score(self):\n        return self._player.get_result_string()\n\n    def _accomodate_out_of_turn(self, color: str):\n        position = self._player.get_position()\n        if translate_gtp_color(color) != position.to_play:\n            position.flip_playerturn(mutate=True)\n\n\nclass KgsCmdHandler(object):\n    def __init__(self, player):\n        self._player = player\n\n    def cmd_time_left(self, color: str, time: int, stones: int):\n        pass\n\n    def cmd_kgs_chat(self, msg_type: str, sender: str, text: str):\n        if not hasattr(self._player, 'get_root'):\n            return \"I have nothing interesting to say.\"\n\n        root = self._player.get_root()\n        default_response = \"Supported commands are 'winrate', 'nextplay', 'fortune', and 'help'.\"\n        if root is None or root.position.n == 0:\n            return \"I'm not playing right now.  \" + default_response\n\n        if 'winrate' in text.lower():\n            wr = (abs(root.Q) + 1.0) / 2.0\n            color = \"Black\" if root.Q > 0 else \"White\"\n            return \"{} {:.2f}%\".format(color, wr * 100.0)\n        elif 'nextplay' in text.lower():\n            return \"I'm thinking... \" + root.most_visited_path()\n        elif 'fortune' in text.lower():\n            return \"You're feeling lucky!\"\n        elif 'help' in text.lower():\n            return \"I can't help much with go -- try ladders!  Otherwise: \" + default_response\n        else:\n            return default_response\n\n\nclass RegressionsCmdHandler(object):\n    def __init__(self, player):\n        self._player = player\n\n    def cmd_loadsgf(self, filename: str, movenum=0):\n        try:\n            with open(filename, 'r') as f:\n                contents = f.read()\n        except:\n            raise ValueError(\"Unreadable file: \" + filename)\n\n        # Clear the board before replaying sgf\n        # TODO: should this use the sgfs komi?\n        self._player.initialize_game(go.Position())\n\n        # This is kinda bad, because replay_sgf is already calling\n        # 'play move' on its internal position objects, but we really\n        # want to advance the engine along with us rather than try to\n        # push in some finished Position object.\n        for idx, p in enumerate(sgf_wrapper.replay_sgf(contents)):\n            dbg(\"playing #\", idx, p.next_move)\n            self._player.play_move(p.next_move)\n            if movenum and idx == movenum:\n                break\n\n\nclass GoGuiCmdHandler(object):\n    \"\"\"GTP extensions of 'analysis commands' for gogui.\"\"\"\n\n    def __init__(self, player):\n        self._player = player\n\n    def cmd_gogui_analyze_commands(self):\n        return \"\\n\".join([\"var/Most Read Variation/nextplay\",\n                          \"var/Think a spell/spin\",\n                          \"var/Final score/final_score\",\n                          \"pspairs/Visit Heatmap/visit_heatmap\",\n                          \"pspairs/Q Heatmap/q_heatmap\"])\n\n    def cmd_nextplay(self):\n        return self._player.get_root().mvp_gg()\n\n    def cmd_visit_heatmap(self):\n        root = self._player.get_root()\n        sort_order = list(range(go.N * go.N + 1))\n        sort_order.sort(key=lambda i: root.child_N[i], reverse=True)\n        return self._heatmap(sort_order, root, 'child_N')\n\n    def cmd_spin(self):\n        for i in range(50):\n            for j in range(100):\n                self._player.tree_search()\n            moves = self.cmd_nextplay().lower()\n            moves = moves.split()\n            root = self._player.get_root()\n            colors = \"bw\" if root.position.to_play is go.BLACK else \"wb\"\n            moves_cols = \" \".join(['{} {}'.format(*z)\n                                   for z in zip(itertools.cycle(colors), moves)])\n            dbg(\"gogui-gfx: TEXT\", \"{:.3f} after {}\".format(root.Q, root.N))\n            dbg(\"gogui-gfx: VAR\", moves_cols)\n        return self.cmd_nextplay()\n\n    def _heatmap(self, sort_order, node, prop):\n        return \"\\n\".join([\"{!s:6} {}\".format(\n            coords.to_gtp(coords.from_flat(key)),\n            node.__dict__.get(prop)[key])\n            for key in sort_order if node.child_N[key] > 0][: 20])\n\n\nclass MiniguiBasicCmdHandler(BasicCmdHandler):\n    def __init__(self, player, courtesy_pass=False):\n        super().__init__(player, courtesy_pass)\n\n        # Wrap the game's tree_search method, which allows us to stream the\n        # search state back over stderr if requested.\n        self._tree_search = self._player.tree_search\n        self._player.tree_search = self._tree_search_wrapper\n\n        self._last_report_time = None\n        self._report_search_interval = 0.0\n\n    def cmd_echo(self, *args):\n        return \" \".join(args)\n\n    def cmd_info(self):\n        return (\"num_readouts: %d report_search_interval: %.1f n: %d \"\n                \"resign_threshold: %f\" % (\n                    self._player.get_num_readouts(),\n                    self._report_search_interval * 1000, go.N,\n                    self._player.resign_threshold))\n\n    def cmd_readouts(self, readouts: int):\n        readouts = max(8, readouts)\n        self._player.set_num_readouts(readouts)\n        return readouts\n\n    def cmd_report_search_interval(self, interval_ms: float):\n        self._report_search_interval = interval_ms / 1000.0\n\n    def cmd_clear_board(self):\n        super().cmd_clear_board()\n        self._minigui_report_position()\n\n    def cmd_play(self, arg0: str, arg1=None):\n        super().cmd_play(arg0, arg1)\n        root = self._player.get_root()\n        if root.is_done():\n            self._player.set_result(\n                root.position.result(), was_resign=False)\n\n        self._minigui_report_position()\n\n    def cmd_genmove(self, color=None):\n        start = time.time()\n        result = super().cmd_genmove(color)\n        duration = time.time() - start\n\n        root = self._player.get_root()\n        if result != \"resign\":\n            dbg(\"\")\n            dbg(root.position.__str__(colors=False))\n            dbg(\"%d readouts, %.3f s/100. (%.2f sec)\" % (\n                self._player.get_num_readouts(),\n                duration / self._player.get_num_readouts() * 100.0, duration))\n            dbg(\"\")\n            if root.is_done():\n                self._player.set_result(\n                    root.position.result(), was_resign=False)\n\n        self._minigui_report_position()\n\n        return result\n\n    def _tree_search_wrapper(self, parallel_readouts=None):\n        leaves = self._tree_search(parallel_readouts)\n        if self._report_search_interval:\n            now = time.time()\n            if (self._last_report_time is None or\n                    now - self._last_report_time > self._report_search_interval):\n                self._minigui_report_search_status(leaves)\n                self._last_report_time = now\n        return leaves\n\n    def _minigui_report_position(self):\n        root = self._player.get_root()\n        position = root.position\n\n        board = []\n        for row in range(go.N):\n            for col in range(go.N):\n                stone = position.board[row, col]\n                if stone == go.BLACK:\n                    board.append(\"X\")\n                elif stone == go.WHITE:\n                    board.append(\"O\")\n                else:\n                    board.append(\".\")\n\n        msg = {\n            \"id\": hex(id(root)),\n            \"toPlay\": \"B\" if position.to_play == 1 else \"W\",\n            \"moveNum\": position.n,\n            \"stones\": \"\".join(board),\n            \"gameOver\": position.is_game_over(),\n            \"caps\": position.caps,\n        }\n        if root.parent and root.parent.parent:\n            msg[\"parentId\"] = hex(id(root.parent))\n            msg[\"q\"] = float(root.parent.Q)\n        if position.recent:\n            msg[\"move\"] = coords.to_gtp(position.recent[-1].move)\n        dbg(\"mg-position:%s\" % json.dumps(msg, sort_keys=True))\n\n    def _minigui_report_search_status(self, leaves):\n        \"\"\"Prints the current MCTS search status to stderr.\n\n        Reports the current search path, root node's child_Q, root node's\n        child_N, the most visited path in a format that can be parsed by\n        one of the STDERR_HANDLERS in minigui.ts.\n\n        Args:\n          leaves: list of leaf MCTSNodes returned by tree_search().\n         \"\"\"\n\n        root = self._player.get_root()\n\n        msg = {\n            \"id\": hex(id(root)),\n            \"n\": int(root.N),\n            \"q\": float(root.Q),\n        }\n\n        msg[\"childQ\"] = [int(round(q * 1000)) for q in root.child_Q]\n        msg[\"childN\"] = [int(n) for n in root.child_N]\n\n        ranked_children = root.rank_children()\n        variations = {}\n        for i in ranked_children[:15]:\n            if root.child_N[i] == 0 or i not in root.children:\n                break\n            c = coords.to_gtp(coords.from_flat(i))\n            child = root.children[i]\n            nodes = child.most_visited_path_nodes()\n            moves = [coords.to_gtp(coords.from_flat(m.fmove)) for m in nodes]\n            variations[c] = {\n                \"n\": int(root.child_N[i]),\n                \"q\": float(root.child_Q[i]),\n                \"moves\": [c] + moves,\n            }\n\n        if leaves:\n            path = []\n            leaf = leaves[0]\n            while leaf != root:\n                path.append(leaf.fmove)\n                leaf = leaf.parent\n            if path:\n                path.reverse()\n                variations[\"live\"] = {\n                    \"n\": int(root.child_N[path[0]]),\n                    \"q\": float(root.child_Q[path[0]]),\n                    \"moves\": [coords.to_gtp(coords.from_flat(m)) for m in path]\n                }\n\n        if variations:\n            msg[\"variations\"] = variations\n\n        dbg(\"mg-update:%s\" % json.dumps(msg, sort_keys=True))\n"
        },
        {
          "name": "gtp_engine.py",
          "type": "blob",
          "size": 6.2802734375,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Version of the GTP specification used:\n#   https://www.lysator.liu.se/~gunnar/gtp/gtp2-spec-draft2/gtp2-spec.html\n\nimport inspect\nimport re\nimport sys\nimport traceback\n\n\n# List of canonical error messages as required by 6.3 in the specification.\n_CANONICAL_ERRORS = {\n    \"boardsize\": \"unacceptable size\",\n    \"play\": \"illegal move\",\n    \"undo\": \"cannot undo\",\n    \"final_score\": \"cannot score\",\n    \"loadsgf\": \"cannot load file\",\n}\n\n_GTP_CMD_DONE = \"__GTP_CMD_DONE__\"\n\n\ndef _preprocess(msg):\n    # From the spec (3.1 Preprocessing):\n    # 1. Remove all occurences of CR and other control characters except for\n    #    HT and LF.\n    # 2. For each line with a hash sign (#), remove all text following and\n    #    including this character.\n    # 3. Convert all occurences of HT to SPACE.\n    # 4. Discard any empty or white-space only lines.\n\n    # Control characters are defined in section 2.2 as dec 0-31 (oct 0-037)\n    # and 127 (oct 177). We want to remove them all except \\t (oct 011) and\n    # \\n (oct 12).\n    msg = re.sub(\"r[\\000-\\010\\013-\\037\\177]\", \"\", msg)\n    msg = msg.split(\"#\", 1)[0]\n    msg = msg.replace(\"\\t\", \" \")\n    return msg\n\n\ndef _parse(msg):\n    msg = _preprocess(msg).strip()\n    if not msg:\n        return None, None, None\n    parts = [x for x in msg.split(\" \") if x]\n    if len(parts) > 1 and parts[0].isdigit():\n        msg_id = parts[0]\n        parts = parts[1:]\n    else:\n        msg_id = None\n    return msg_id, parts[0], parts[1:]\n\n\ndef _print_msg(result, msg_id, msg):\n    msg_id = \" {}\".format(msg_id) if msg_id else \"\"\n    if isinstance(msg, bool):\n        msg = \"true\" if msg else \"false\"\n    msg = \" {}\".format(msg) if msg else \"\"\n    print(\"{}{}{}\\n\".format(result, msg_id, msg), flush=True)\n\n\ndef _print_error(msg_id, msg):\n    print(_GTP_CMD_DONE, file=sys.stderr)\n    _print_msg(\"?\", msg_id, msg)\n\n\ndef _print_success(msg_id, msg):\n    print(_GTP_CMD_DONE, file=sys.stderr)\n    _print_msg(\"=\", msg_id, msg)\n\n\ndef _handler_name(fn):\n    return \"{}.{}\".format(fn.__self__.__class__.__name__, fn.__name__)\n\n\ndef _convert_args(handler, args):\n    \"\"\"Convert a list of command arguments to types specified by the handler.\n\n    Args:\n      handler: a command handler function.\n      args: the list of string arguments to pass to handler.\n\n    Returns:\n      A new list containing `args` that have been converted to the expected type\n      for `handler`. For each function parameter of `handler` that has either an\n      explicit type annotation or a non-None default value, the corresponding\n      element in `args` is converted to that type.\n    \"\"\"\n\n    args = list(args)\n    params = inspect.signature(handler).parameters\n    for i, (arg, name) in enumerate(zip(args, params)):\n        default = params[name].default\n        annotation = params[name].annotation\n\n        if annotation != inspect.Parameter.empty:\n            if isinstance(annotation, type) and annotation != str:\n                # The parameter is annotated with a type that isn't str: convert\n                # the arg to that type.\n                args[i] = annotation(arg)\n        elif default != inspect.Parameter.empty:\n            if default is not None and not isinstance(default, str):\n                # The parameter has a default value that isn't None or a str:\n                # convert the arg to the default value's type.\n                args[i] = type(default)(arg)\n\n    return args\n\n\nclass Engine(object):\n    \"\"\"A simple GTP engine.\n\n    The engine by itself doesn't do anything: clients must register command\n    handler objects using `add_cmd_handler`.\n    \"\"\"\n\n    def __init__(self):\n        self.cmds = {}\n\n    def add_cmd_handler(self, handler_obj):\n        \"\"\"Registers a new command handler object.\n\n        All methods on `handler_obj` whose name starts with \"cmd_\" are\n        registered as a GTP command. For example, the method cmd_genmove will\n        be invoked when the engine receives a genmove command.\n\n        Args:\n          handler_obj: the handler object to register.\n        \"\"\"\n        for field in dir(handler_obj):\n            if field.startswith(\"cmd_\"):\n                cmd = field[4:]\n                fn = getattr(handler_obj, field)\n                if cmd in self.cmds:\n                    print('Replacing {} with {}'.format(\n                        _handler_name(self.cmds[cmd]), _handler_name(fn)),\n                        file=sys.stderr)\n                self.cmds[cmd] = fn\n\n    def handle_msg(self, msg):\n        msg_id, cmd, args = _parse(_preprocess(msg))\n        if not cmd:\n            # Ignore empty lines.\n            return True\n\n        if cmd == \"quit\":\n            _print_success(msg_id, \"\")\n            return False\n\n        sanitized_cmd = cmd.replace(\"-\", \"_\")\n        if sanitized_cmd not in self.cmds:\n            _print_error(msg_id, \"unknown command\")\n            return True\n\n        try:\n            handler = self.cmds[sanitized_cmd]\n            args = _convert_args(handler, args)\n            _print_success(msg_id, handler(*args))\n        except Exception as e:\n            traceback.print_exc(file=sys.stderr)\n            if cmd in _CANONICAL_ERRORS:\n                _print_error(msg_id, _CANONICAL_ERRORS[cmd])\n            else:\n                _print_error(msg_id, \" \".join(map(str, e.args)))\n\n        return True\n\n\nclass EngineCmdHandler(object):\n    \"\"\"Command handlers for basic engine stuff.\"\"\"\n\n    def __init__(self, engine, name, version):\n        self._engine = engine\n        self._name = name\n        self._version = version\n\n    def cmd_protocol_version(self):\n        return 2\n\n    def cmd_name(self):\n        return self._name\n\n    def cmd_version(self):\n        return self._version\n\n    def cmd_known_command(self, cmd):\n        return cmd in self._engine.cmds\n\n    def cmd_list_commands(self):\n        return \"\\n\".join(sorted(self._engine.cmds.keys()))\n"
        },
        {
          "name": "mask_flags.py",
          "type": "blob",
          "size": 4.677734375,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Filters flagfile to only pass in flags that are defined.\n\nHaving one big flagfile is great for seeing all the configuration at a glance.\nHowever, absl.flags will throw an error if you pass an undefined flag.\n\nTo solve this problem, we filter the global flagfile by running\n    python3 some_module.py --helpfull\nto generate a list of all flags that some_module.py accepts. Then, we pass in\nonly those flags that are accepted by some_module.py and run as a subprocess\n\nUsage example:\n    import mask_flags\n    mask_flags.run(['python3', 'train.py', '--custom_flag', '--flagfile=flags'])\n    # will be transformed into\n    subprocess.run(['python3', 'train.py', '--custom_flag',\n                    '--train_only_flag=...', '--more_train_only=...''])\n\nCommand line usage example:\n    python3 -m mask_flags train.py --custom_flag --flagfile=flags\n\"\"\"\n\nimport re\nimport subprocess\nimport sys\nimport time\nfrom absl import flags\n\n# Matches both\n#   --some_flag: Flag description\n#   --[no]bool_flag: Flag description\nFLAG_HELP_RE_PY = re.compile(r'--((\\[no\\])?)([\\w_-]+):')\nFLAG_HELP_RE_CC = re.compile(r'-((\\[no\\])?)([\\w_-]+) \\(')\nFLAG_RE = re.compile(r'--[\\w_-]+')\n\n\ndef extract_valid_flags(subprocess_cmd):\n    \"\"\"Extracts the valid flags from a command by running it with --helpfull.\n    Args:\n        subprocess_cmd: List[str], what would be passed into subprocess.call()\n            i.e. ['python', 'train.py', '--flagfile=flags']\n\n    Returns:\n        ['--foo=blah', '--more_flags']\n    \"\"\"\n\n    help_cmd = subprocess_cmd + ['--helpfull']\n    help_output = subprocess.run(help_cmd, stdout=subprocess.PIPE).stdout\n    help_output = help_output.decode('ascii')\n    if 'python' in subprocess_cmd[0]:\n        valid_flags = parse_helpfull_output(help_output)\n    else:\n        valid_flags = parse_helpfull_output(help_output, regex=FLAG_HELP_RE_CC)\n    return valid_flags\n\n\ndef parse_helpfull_output(help_output, regex=FLAG_HELP_RE_PY):\n    \"\"\"Parses the output of --helpfull.\n\n    Args:\n        help_output: str, the full output of --helpfull.\n\n    Returns:\n        A set of flags that are valid flags.\n    \"\"\"\n    valid_flags = set()\n    for _, no_prefix, flag_name in regex.findall(help_output):\n        valid_flags.add('--' + flag_name)\n        if no_prefix:\n            valid_flags.add('--no' + flag_name)\n    return valid_flags\n\n\ndef filter_flags(parsed_flags, valid_flags):\n    \"\"\"Return the subset of `parsed_flags` that are found in the list `valid_flags`\"\"\"\n    def valid_argv(argv):\n        \"\"\"Figures out if a flag parsed from the flagfile matches a flag in\n        the command about to be run.\"\"\"\n        flagname_match = FLAG_RE.match(argv)\n        if not flagname_match:\n            return True\n        flagname = flagname_match.group()\n        return flagname in valid_flags\n    return list(filter(valid_argv, parsed_flags))\n\n\ndef prepare_subprocess_cmd(subprocess_cmd):\n    \"\"\"Prepares a subprocess command by running --helpfull and masking flags.\n\n    Args:\n        subprocess_cmd: List[str], what would be passed into subprocess.call()\n            i.e. ['python', 'train.py', '--flagfile=flags']\n\n    Returns:\n        ['python', 'train.py', '--train_flag=blah', '--more_flags']\n    \"\"\"\n    valid_flags = extract_valid_flags(subprocess_cmd)\n    parsed_flags = flags.FlagValues().read_flags_from_files(subprocess_cmd[1:])\n    filtered_flags = filter_flags(parsed_flags, valid_flags)\n    return [subprocess_cmd[0]] + filtered_flags\n\n\ndef run(cmd):\n    \"\"\"Prepare and run a subprocess cmd, returning a CompletedProcess.\"\"\"\n    print(\"Preparing the following cmd:\")\n    cmd = prepare_subprocess_cmd(cmd)\n    print(\"Running the following cmd:\")\n    print('\\n'.join(cmd))\n    return subprocess.run(cmd, stdout=sys.stdout, stderr=sys.stderr)\n\n\ndef checked_run(cmd):\n    \"\"\"Prepare and run a subprocess cmd, checking for successful completion.\"\"\"\n    completed_process = run(cmd)\n    if completed_process.returncode > 0:\n        print(\"Command failed!  Hanging around in case someone needs a \"\n              \"docker connection. (Ctrl-C to quit now)\")\n        time.sleep(300)\n        raise RuntimeError\n    return completed_process\n\n\nif __name__ == '__main__':\n    sys.argv.pop(0)\n    checked_run(sys.argv)\n"
        },
        {
          "name": "mcts.py",
          "type": "blob",
          "size": 12.037109375,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Monte Carlo Tree Search implementation.\n\nAll terminology here (Q, U, N, p_UCT) uses the same notation as in the\nAlphaGo (AG) paper.\n\"\"\"\n\nimport collections\nimport math\n\nfrom absl import flags\nimport numpy as np\n\nimport coords\nimport go\n\n# 722 moves for 19x19, 162 for 9x9\nflags.DEFINE_integer('max_game_length', int(go.N ** 2 * 2),\n                     'Move number at which game is forcibly terminated')\n\nflags.DEFINE_float('c_puct_base', 19652,\n                   'Exploration constants balancing priors vs. value net output.')\n\nflags.DEFINE_float('c_puct_init', 1.25,\n                   'Exploration constants balancing priors vs. value net output.')\n\nflags.DEFINE_float('dirichlet_noise_alpha', 0.03 * 361 / (go.N ** 2),\n                   'Concentrated-ness of the noise being injected into priors.')\nflags.register_validator('dirichlet_noise_alpha', lambda x: 0 <= x < 1)\n\nflags.DEFINE_float('dirichlet_noise_weight', 0.25,\n                   'How much to weight the priors vs. dirichlet noise when mixing')\nflags.register_validator('dirichlet_noise_weight', lambda x: 0 <= x < 1)\n\nFLAGS = flags.FLAGS\n\n\nclass DummyNode(object):\n    \"\"\"A fake node of a MCTS search tree.\n\n    This node is intended to be a placeholder for the root node, which would\n    otherwise have no parent node. If all nodes have parents, code becomes\n    simpler.\"\"\"\n\n    def __init__(self):\n        self.parent = None\n        self.child_N = collections.defaultdict(float)\n        self.child_W = collections.defaultdict(float)\n\n\nclass MCTSNode(object):\n    \"\"\"A node of a MCTS search tree.\n\n    A node knows how to compute the action scores of all of its children,\n    so that a decision can be made about which move to explore next. Upon\n    selecting a move, the children dictionary is updated with a new node.\n\n    position: A go.Position instance\n    fmove: A move (coordinate) that led to this position, a flattened coord\n            (raw number between 0-N^2, with None a pass)\n    parent: A parent MCTSNode.\n    \"\"\"\n\n    def __init__(self, position, fmove=None, parent=None):\n        if parent is None:\n            parent = DummyNode()\n        self.parent = parent\n        self.fmove = fmove  # move that led to this position, as flattened coords\n        self.position = position\n        self.is_expanded = False\n        self.losses_applied = 0  # number of virtual losses on this node\n        # using child_() allows vectorized computation of action score.\n        self.illegal_moves = 1 - self.position.all_legal_moves()\n        self.child_N = np.zeros([go.N * go.N + 1], dtype=np.float32)\n        self.child_W = np.zeros([go.N * go.N + 1], dtype=np.float32)\n        # save a copy of the original prior before it gets mutated by d-noise.\n        self.original_prior = np.zeros([go.N * go.N + 1], dtype=np.float32)\n        self.child_prior = np.zeros([go.N * go.N + 1], dtype=np.float32)\n        self.children = {}  # map of flattened moves to resulting MCTSNode\n\n    def __repr__(self):\n        return \"<MCTSNode move=%s, N=%s, to_play=%s>\" % (\n            self.position.recent[-1:], self.N, self.position.to_play)\n\n    @property\n    def child_action_score(self):\n        return (self.child_Q * self.position.to_play +\n                self.child_U - 1000 * self.illegal_moves)\n\n    @property\n    def child_Q(self):\n        return self.child_W / (1 + self.child_N)\n\n    @property\n    def child_U(self):\n        return ((2.0 * (math.log(\n                (1.0 + self.N + FLAGS.c_puct_base) / FLAGS.c_puct_base)\n                       + FLAGS.c_puct_init)) * math.sqrt(max(1, self.N - 1)) *\n                self.child_prior / (1 + self.child_N))\n\n    @property\n    def Q(self):\n        return self.W / (1 + self.N)\n\n    @property\n    def N(self):\n        return self.parent.child_N[self.fmove]\n\n    @N.setter\n    def N(self, value):\n        self.parent.child_N[self.fmove] = value\n\n    @property\n    def W(self):\n        return self.parent.child_W[self.fmove]\n\n    @W.setter\n    def W(self, value):\n        self.parent.child_W[self.fmove] = value\n\n    @property\n    def Q_perspective(self):\n        \"\"\"Return value of position, from perspective of player to play.\"\"\"\n        return self.Q * self.position.to_play\n\n    def select_leaf(self):\n        current = self\n        pass_move = go.N * go.N\n        while True:\n            # if a node has never been evaluated, we have no basis to select a child.\n            if not current.is_expanded:\n                break\n            # HACK: if last move was a pass, always investigate double-pass first\n            # to avoid situations where we auto-lose by passing too early.\n            if (current.position.recent and\n                current.position.recent[-1].move is None and\n                    current.child_N[pass_move] == 0):\n                current = current.maybe_add_child(pass_move)\n                continue\n\n            best_move = np.argmax(current.child_action_score)\n            current = current.maybe_add_child(best_move)\n        return current\n\n    def maybe_add_child(self, fcoord):\n        \"\"\"Adds child node for fcoord if it doesn't already exist, and returns it.\"\"\"\n        if fcoord not in self.children:\n            new_position = self.position.play_move(\n                coords.from_flat(fcoord))\n            self.children[fcoord] = MCTSNode(\n                new_position, fmove=fcoord, parent=self)\n        return self.children[fcoord]\n\n    def add_virtual_loss(self, up_to):\n        \"\"\"Propagate a virtual loss up to the root node.\n\n        Args:\n            up_to: The node to propagate until. (Keep track of this! You'll\n                need it to reverse the virtual loss later.)\n        \"\"\"\n        self.losses_applied += 1\n        # This is a \"win\" for the current node; hence a loss for its parent node\n        # who will be deciding whether to investigate this node again.\n        loss = self.position.to_play\n        self.W += loss\n        if self.parent is None or self is up_to:\n            return\n        self.parent.add_virtual_loss(up_to)\n\n    def revert_virtual_loss(self, up_to):\n        self.losses_applied -= 1\n        revert = -1 * self.position.to_play\n        self.W += revert\n        if self.parent is None or self is up_to:\n            return\n        self.parent.revert_virtual_loss(up_to)\n\n    def incorporate_results(self, move_probabilities, value, up_to):\n        assert move_probabilities.shape == (go.N * go.N + 1,)\n        # A finished game should not be going through this code path - should\n        # directly call backup_value() on the result of the game.\n        assert not self.position.is_game_over()\n\n        # If a node was picked multiple times (despite vlosses), we shouldn't\n        # expand it more than once.\n        if self.is_expanded:\n            return\n        self.is_expanded = True\n\n        # Zero out illegal moves.\n        move_probs = move_probabilities * (1 - self.illegal_moves)\n        scale = sum(move_probs)\n        if scale > 0:\n            # Re-normalize move_probabilities.\n            move_probs *= 1 / scale\n\n        self.original_prior = self.child_prior = move_probs\n        # initialize child Q as current node's value, to prevent dynamics where\n        # if B is winning, then B will only ever explore 1 move, because the Q\n        # estimation will be so much larger than the 0 of the other moves.\n        #\n        # Conversely, if W is winning, then B will explore all 362 moves before\n        # continuing to explore the most favorable move. This is a waste of search.\n        #\n        # The value seeded here acts as a prior, and gets averaged into Q calculations.\n        self.child_W = np.ones([go.N * go.N + 1], dtype=np.float32) * value\n        self.backup_value(value, up_to=up_to)\n\n    def backup_value(self, value, up_to):\n        \"\"\"Propagates a value estimation up to the root node.\n\n        Args:\n            value: the value to be propagated (1 = black wins, -1 = white wins)\n            up_to: the node to propagate until.\n        \"\"\"\n        self.N += 1\n        self.W += value\n        if self.parent is None or self is up_to:\n            return\n        self.parent.backup_value(value, up_to)\n\n    def is_done(self):\n        \"\"\"True if the last two moves were Pass or if the position is at a move\n        greater than the max depth.\"\"\"\n        return self.position.is_game_over() or self.position.n >= FLAGS.max_game_length\n\n    def inject_noise(self):\n        epsilon = 1e-5\n        legal_moves = (1 - self.illegal_moves) + epsilon\n        a = legal_moves * ([FLAGS.dirichlet_noise_alpha] * (go.N * go.N + 1))\n        dirichlet = np.random.dirichlet(a)\n        self.child_prior = (self.child_prior * (1 - FLAGS.dirichlet_noise_weight) +\n                            dirichlet * FLAGS.dirichlet_noise_weight)\n\n    def children_as_pi(self, squash=False):\n        \"\"\"Returns the child visit counts as a probability distribution, pi\n        If squash is true, exponentiate the probabilities by a temperature\n        slightly larger than unity to encourage diversity in early play and\n        hopefully to move away from 3-3s\n        \"\"\"\n        probs = self.child_N\n        if squash:\n            probs = probs ** .98\n        sum_probs = np.sum(probs)\n        if sum_probs == 0:\n            return probs\n        return probs / np.sum(probs)\n\n    def best_child(self):\n        # Sort by child_N tie break with action score.\n        return np.argmax(self.child_N + self.child_action_score / 10000)\n\n    def most_visited_path_nodes(self):\n        node = self\n        output = []\n        while node.children:\n            node = node.children.get(node.best_child())\n            assert node is not None\n            output.append(node)\n        return output\n\n    def most_visited_path(self):\n        output = []\n        node = self\n        for node in self.most_visited_path_nodes():\n            output.append(\"%s (%d) ==> \" % (\n                coords.to_gtp(coords.from_flat(node.fmove)), node.N))\n\n        output.append(\"Q: {:.5f}\\n\".format(node.Q))\n        return ''.join(output)\n\n    def mvp_gg(self):\n        \"\"\"Returns most visited path in go-gui VAR format e.g. 'b r3 w c17...\"\"\"\n        output = []\n        for node in self.most_visited_path_nodes():\n            if max(node.child_N) <= 1:\n                break\n            output.append(coords.to_gtp(coords.from_flat(node.fmove)))\n        return ' '.join(output)\n\n    def rank_children(self):\n        ranked_children = list(range(go.N * go.N + 1))\n        ranked_children.sort(key=lambda i: (\n            self.child_N[i], self.child_action_score[i]), reverse=True)\n        return ranked_children\n\n    def describe(self):\n        ranked_children = self.rank_children()\n        soft_n = self.child_N / max(1, sum(self.child_N))\n        prior = self.child_prior\n        p_delta = soft_n - prior\n        p_rel = np.divide(p_delta, prior, out=np.zeros_like(\n            p_delta), where=prior != 0)\n        # Dump out some statistics\n        output = []\n        output.append(\"{q:.4f}\\n\".format(q=self.Q))\n        output.append(self.most_visited_path())\n        output.append(\n            \"move : action    Q     U     P   P-Dir    N  soft-N  p-delta  p-rel\")\n        for i in ranked_children[:15]:\n            if self.child_N[i] == 0:\n                break\n            output.append(\"\\n{!s:4} : {: .3f} {: .3f} {:.3f} {:.3f} {:.3f} {:5d} {:.4f} {: .5f} {: .2f}\".format(\n                coords.to_gtp(coords.from_flat(i)),\n                self.child_action_score[i],\n                self.child_Q[i],\n                self.child_U[i],\n                self.child_prior[i],\n                self.original_prior[i],\n                int(self.child_N[i]),\n                soft_n[i],\n                p_delta[i],\n                p_rel[i]))\n        return ''.join(output)\n"
        },
        {
          "name": "minigo_model.py",
          "type": "blob",
          "size": 4.130859375,
          "content": "# Copyright 2019 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nLibrary for writing Minigo model files.\n\"\"\"\n\nimport json\nimport struct\nimport tensorflow as tf\n\n\nMAGIC = '<minigo>'\nMAGIC_SIZE = len(MAGIC)\nHEADER_SIZE = 24\n\n\ndef _is_supported_metadata_type(value):\n    for t in [int, str, float, bool]:\n        if isinstance(value, t):\n            return True\n    return False\n\n\ndef write_graph_def(graph_def, metadata, dst_path):\n    \"\"\"Writes a TensorFlow GraphDef & metadata in Minigo format.\n\n    Args:\n      model_bytes: the serialized model.\n      metadata: a dictionary of metadata to write to file.\n      dst_path: destination path to write to.\n    \"\"\"\n    write_model_bytes(graph_def.SerializeToString(), metadata, dst_path)\n\n\ndef write_model_bytes(model_bytes, metadata, dst_path):\n    \"\"\"Writes a serialized model & metadata in Minigo format.\n\n    Args:\n      model_bytes: the serialized model.\n      metadata: a dictionary of metadata to write to file.\n      dst_path: destination path to write to.\n    \"\"\"\n\n    for key, value in metadata.items():\n        assert isinstance(key, str), '%s is not a string' % key\n        assert _is_supported_metadata_type(value), '%s: unsupported type %s' % (\n            key, type(value).__name__)\n\n    metadata_bytes = json.dumps(metadata, sort_keys=True,\n                                separators=(',', ':')).encode()\n\n    # If the destination path is on GCS, write there directly since GCS files\n    # are immutable and a partially written file cannot be observed.\n    # Otherwise, write to a temp file and rename. The temp file is written to\n    # the same filesystem as dst_path on the assumption that the rename will be\n    # atomic.\n    if dst_path.startswith('gs://'):\n        write_path = dst_path\n    else:\n        write_path = dst_path + '.tmp'\n\n    # File header:\n    #   char[8]: '<minigo>'\n    #   uint64: version\n    #   uint64: file size\n    #   uint64: metadata size\n    version = 1\n    header_size = 32\n    metadata_size = len(metadata_bytes)\n    model_size = len(model_bytes)\n    file_size = header_size + metadata_size + model_size\n    with tf.io.gfile.GFile(write_path, 'wb') as f:\n        f.write(MAGIC)\n        f.write(struct.pack('<QQQ', version, file_size, metadata_size))\n        f.write(metadata_bytes)\n        f.write(model_bytes)\n\n    if write_path != dst_path:\n        tf.io.gfile.rename(write_path, dst_path, overwrite=True)\n\n\ndef read_model(path):\n    \"\"\"Reads a serialized model & metadata in Minigo format.\n\n    Args:\n      path: the model path.\n\n    Returns:\n      A (metadata, model_bytes) pair of the model's metadata as a dictionary\n      and the serialized model as bytes.\n    \"\"\"\n\n    with tf.io.gfile.GFile(path, 'rb') as f:\n        magic = f.read(MAGIC_SIZE).decode('utf-8')\n        if magic != MAGIC:\n            raise RuntimeError(\n                'expected magic string %s, got %s' % (MAGIC, magic))\n\n        version, file_size, metadata_size = struct.unpack(\n            '<QQQ', f.read(HEADER_SIZE))\n        if version != 1:\n            raise RuntimeError('expected version == 1, got %d' % version)\n\n        metadata_bytes = f.read(metadata_size).decode('utf-8')\n        if len(metadata_bytes) != metadata_size:\n            raise RuntimeError('expected %dB of metadata, read only %dB' % (\n                metadata_size, len(metadata_bytes)))\n\n        metadata = json.loads(metadata_bytes)\n        model_bytes = f.read()\n        model_size = len(model_bytes)\n\n        bytes_read = MAGIC_SIZE + HEADER_SIZE + model_size + metadata_size\n        if bytes_read != file_size:\n            raise RuntimeError('expected %dB, read only %dB' %\n                               (file_size, bytes_read))\n\n    return metadata, model_bytes\n"
        },
        {
          "name": "minigui",
          "type": "tree",
          "content": null
        },
        {
          "name": "ml_perf",
          "type": "tree",
          "content": null
        },
        {
          "name": "notes.txt",
          "type": "blob",
          "size": 2.8359375,
          "content": "\n== v5 notes\n\n1. 3/4 of the way through v125, change squash from 0.95 => 0.98, change temp cutoff\nto move 30 (from 31, because odd number = bias against black)\n\n1. v192 -- learning rate erroneously cut to 0.001.  v207 returned to 0.01.\n(v206 trained with 0.001, v207 @0.01)\n\n1. v231 -- moved #readouts to 900 from 800.\n\n1. v 295-6 -- move shuffle buffer to 1M from 200k\n\n1. v347 -- changed filter amount to 0.02.  (first present for v347)\n\n1. v348 reverted #4\n\n1. v352 change filter to 0.03 and move shuffle buffer back to 200k\n\n1. during v354 (for 355) change steps per generation to 1M from 2M, shuffle\nbuffer down to 100k\n\n1. 23.2M steps -- change l2_strength to 0.0002 (from 0.0001)\n\n1. v360ish -- entered experimental mode; freely adjusted learning rate up and\ndown, adjusted batch size, etc, to see if valnet divergence could be fixed.\n\n\n== v7a\n\nRolled back to v5 model 173 and continued from there; first v7 model was\n174.  Moved minimum games before starting training\nfrom 5k up to 10k.\n\n1. for 215, adjusted validation set to be last 30 models instead of last 50.\nIt's hard coded and non-adaptive but it's an easy fix to make on a sunday :)\n\n1. Easy change on a sunday = famous last words.  Also at 215, also discovered\nhaven't been writing holdout games for models 174-onwards.  Oops.  Should make\nfor an interesting spike on tensorboard's validation error now as the\nvalidation set will be entirely games from 215.  This new cc image ends up\ncausing disk pressure on the nodes leading to all the jobs stopping.  Sigh.\n\n1. 217 onwards -- resign disabled amount is 10%, not 5%.\n\nROLLBACK\n\n== v7\n\nResign-disabled percent moved from 5% => 20%\nmodel 190: move resign threshold 0.89 => .85\nmodel 206: rotation in training on. (from 5/20 00:20 PST)\nmodel 230ish: move resign threshold 0.85 => .90\nmodel 242: cut learning rate from 1e-3 to 1e-4 (step 11.47M)\nmodel 302: resign threshold 0.89=>0.91\nmodel 330: learning rate cut from 1e-4 to 3e-5\nmodel 375: learning rate cut from 3e-5 to 1e-5\nmodel 380: learning rate increased from 1e-5 to 1e-4\nmodel 389: learning rate increased from 1e-4 to 3e-4\nmodel 409: learning rate decreased from 3e-4 to 3e-5\nmodel 433: batch norm momentum decreased from 0.997 to 0.95\nmodel 450: learning rate increased from 3e-5 to 1e-4\nmodel 456: learning rate decreased from 1e-4 to 3e-5\nmodel 476: dropped-in 20x256 model trained via running start.  Learning rate to\ne3-4\nmodel 506: learning rate to 1e-5\nmodel 508: learning rate to 4e-5\n\n\n== v12\n\nchunk 567 and onwards: no moves after move #400\n857 onwards use 750k window size + pick3\nLR cut in half at v12-792\n\n\n\"if you take the system as a combination of \"the topology\" with \"a motion\nacross the topology\"\nfor a large learning rate it makes sense that would be unstable\nfor a small learning rate, it makes sense that you would find a stable minima\nand get stuck.\n\nsuggests making data window larger as the LR drops.\n\n"
        },
        {
          "name": "oneoffs",
          "type": "tree",
          "content": null
        },
        {
          "name": "player_interface.py",
          "type": "blob",
          "size": 3.654296875,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Go player interfaces.\"\"\"\n\nfrom abc import ABC, abstractmethod\n\n\nclass PlayerInterface(ABC):\n    \"\"\"Interface for a basic Go player.\"\"\"\n\n    @abstractmethod\n    def get_position(self):\n        \"\"\"Get the current position.\n\n        Returns:\n          A go.Position instance.\n        \"\"\"\n\n    @abstractmethod\n    def get_result_string(self):\n        \"\"\"Get the result as a string.\n\n        Returns:\n          The result as a string, e.g. B+R, W+1.5.\n        \"\"\"\n\n    @abstractmethod\n    def initialize_game(self, position=None):\n        \"\"\"Initializes a new game.\n\n        Args:\n          position: the board position to copy for the initial game state. If\n                    None, an empty board state is used for the initial position.\n        \"\"\"\n\n    @abstractmethod\n    def suggest_move(self, position):\n        \"\"\"Suggests a move to play from the given position.\n\n        Args:\n          position: the current board position.\n\n        Returns:\n          The players's best guess as the best move to play.\n        \"\"\"\n\n    @abstractmethod\n    def play_move(self, c):\n        \"\"\"Play the given move.\n\n        Args:\n          c: the move to play as a Minigo coordinate (see coords.py).\n\n        Returns:\n          True if the move was successfully played.\n          False if the requested move is illegal.\n        \"\"\"\n\n    @abstractmethod\n    def should_resign(self):\n        \"\"\"Should the current player resign?\n\n        Returns:\n          True if the player thinks the current player is doing so badly they\n          had better just give up.\n        \"\"\"\n\n    @abstractmethod\n    def to_sgf(self, use_comments=True):\n        \"\"\"Format the game history as SGF.\n\n        Args:\n          use_comments: True to add debug info as a comment to the move nodes.\n\n        Returns:\n          A formatted SGF string\n        \"\"\"\n\n    @abstractmethod\n    def set_result(self, winner, was_resign):\n        \"\"\"Sets the game result.\n\n        Args:\n          winner: +1 for a black win, -1 a white win.\n          was_resign: True if the win was by resignation.\n        \"\"\"\n\n\nclass MCTSPlayerInterface(PlayerInterface):\n    \"\"\"Interface for a MCTS-based Go player.\"\"\"\n\n    @abstractmethod\n    def get_root(self):\n        \"\"\"Get the current root node.\n\n        Returns:\n          The current MCTSNode root of the search tree.\n        \"\"\"\n\n    @abstractmethod\n    def tree_search(self, parallel_readouts=None):\n        \"\"\"Performs one tree search step.\n\n        Each tree search step may potentially expand multiple leaves of the\n        game tree, depending on parallel_readouts.\n\n        Args:\n          parallel_readouts: number of leaves to expand in parallel. If None,\n          the number of parallel readouts, the player is free to choose a\n          sensible default.\n\n        Returns:\n          A list of the newly expanded leaves.\n        \"\"\"\n\n    @abstractmethod\n    def get_num_readouts(self):\n        \"\"\"Get the number of readouts.\n\n        Returns:\n          The number of readouts.\n        \"\"\"\n\n    @abstractmethod\n    def set_num_readouts(self, readouts):\n        \"\"\"Set the number of readouts.\n\n        Args:\n          readouts: the number of readouts.\n        \"\"\"\n"
        },
        {
          "name": "preprocessing.py",
          "type": "blob",
          "size": 10.115234375,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Utilities to create, read, write tf.Examples.\"\"\"\nimport functools\nimport random\n\nimport bigtable_input\nimport coords\nimport dual_net\nimport features as features_lib\nimport go\nimport sgf_wrapper\nimport symmetries\n\nimport numpy as np\nimport tensorflow as tf\n\nTF_RECORD_CONFIG = tf.python_io.TFRecordOptions(\n    tf.python_io.TFRecordCompressionType.ZLIB)\n\n\ndef _one_hot(index):\n    onehot = np.zeros([go.N * go.N + 1], dtype=np.float32)\n    onehot[index] = 1\n    return onehot\n\n\ndef make_tf_example(features, pi, value):\n    \"\"\"\n    Args:\n        features: [N, N, FEATURE_DIM] nparray of uint8\n        pi: [N * N + 1] nparray of float32\n        value: float\n    \"\"\"\n    return tf.train.Example(features=tf.train.Features(feature={\n        'x': tf.train.Feature(\n            bytes_list=tf.train.BytesList(\n                value=[features.tostring()])),\n        'pi': tf.train.Feature(\n            bytes_list=tf.train.BytesList(\n                value=[pi.tostring()])),\n        'outcome': tf.train.Feature(\n            float_list=tf.train.FloatList(\n                value=[value]))}))\n\n\ndef write_tf_examples(filename, tf_examples, serialize=True):\n    \"\"\"\n    Args:\n        filename: Where to write tf.records\n        tf_examples: An iterable of tf.Example\n        serialize: whether to serialize the examples.\n    \"\"\"\n    with tf.python_io.TFRecordWriter(\n            filename, options=TF_RECORD_CONFIG) as writer:\n        for ex in tf_examples:\n            if serialize:\n                writer.write(ex.SerializeToString())\n            else:\n                writer.write(ex)\n\n\ndef batch_parse_tf_example(batch_size, layout, example_batch):\n    \"\"\"\n    Args:\n        batch_size: batch size\n        layout: 'nchw' or 'nhwc'\n        example_batch: a batch of tf.Example\n    Returns:\n        A tuple (feature_tensor, dict of output tensors)\n    \"\"\"\n    planes = dual_net.get_features_planes()\n\n    features = {\n        'x': tf.FixedLenFeature([], tf.string),\n        'pi': tf.FixedLenFeature([], tf.string),\n        'outcome': tf.FixedLenFeature([], tf.float32),\n    }\n    parsed = tf.parse_example(example_batch, features)\n    x = tf.decode_raw(parsed['x'], tf.uint8)\n    x = tf.cast(x, tf.float32)\n\n    if layout == 'nhwc':\n        shape = [batch_size, go.N, go.N, planes]\n    else:\n        shape = [batch_size, planes, go.N, go.N]\n    x = tf.reshape(x, shape)\n\n    pi = tf.decode_raw(parsed['pi'], tf.float32)\n    pi = tf.reshape(pi, [batch_size, go.N * go.N + 1])\n    outcome = parsed['outcome']\n    outcome.set_shape([batch_size])\n    return x, {'pi_tensor': pi, 'value_tensor': outcome}\n\n\ndef read_tf_records(batch_size, tf_records, num_repeats=1,\n                    shuffle_records=True, shuffle_examples=True,\n                    shuffle_buffer_size=None, interleave=True,\n                    filter_amount=1.0):\n    \"\"\"\n    Args:\n        batch_size: batch size to return\n        tf_records: a list of tf_record filenames\n        num_repeats: how many times the data should be read (default: One)\n        shuffle_records: whether to shuffle the order of files read\n        shuffle_examples: whether to shuffle the tf.Examples\n        shuffle_buffer_size: how big of a buffer to fill before shuffling.\n        interleave: iwhether to interleave examples from multiple tf_records\n        filter_amount: what fraction of records to keep\n    Returns:\n        a tf dataset of batched tensors\n    \"\"\"\n    if shuffle_examples and not shuffle_buffer_size:\n        raise ValueError(\"Must set shuffle buffer size if shuffling examples\")\n\n    tf_records = list(tf_records)\n    if shuffle_records:\n        random.shuffle(tf_records)\n    record_list = tf.data.Dataset.from_tensor_slices(tf_records)\n\n    # compression_type here must agree with write_tf_examples\n    map_func = functools.partial(\n        tf.data.TFRecordDataset,\n        buffer_size=8 * 1024 * 1024,\n        compression_type='ZLIB')\n\n    if interleave:\n        # cycle_length = how many tfrecord files are read in parallel\n        # The idea is to shuffle both the order of the files being read,\n        # and the examples being read from the files.\n        dataset = record_list.apply(tf.data.experimental.parallel_interleave(\n            map_func, cycle_length=64, sloppy=True))\n    else:\n        dataset = record_list.flat_map(map_func)\n\n    if filter_amount < 1.0:\n        dataset = dataset.filter(\n            lambda _: tf.random_uniform([]) < filter_amount)\n\n    dataset = dataset.repeat(num_repeats)\n    if shuffle_examples:\n        dataset = dataset.shuffle(buffer_size=shuffle_buffer_size)\n\n    dataset = dataset.batch(batch_size)\n    return dataset\n\n\ndef _random_rotation(feature_layout, x_tensor, outcome_tensor):\n    pi_tensor = outcome_tensor['pi_tensor']\n    if feature_layout == 'nhwc':\n        x_rot_tensor, pi_rot_tensor=symmetries.rotate_train_nhwc(\n            x_tensor, pi_tensor)\n    else:\n        x_rot_tensor, pi_rot_tensor=symmetries.rotate_train_nchw(\n            x_tensor, pi_tensor)\n\n    outcome_tensor['pi_tensor'] = pi_rot_tensor\n    return x_rot_tensor, outcome_tensor\n\n\ndef get_input_tensors(batch_size, feature_layout, tf_records, num_repeats=1,\n                      shuffle_records=True, shuffle_examples=True,\n                      shuffle_buffer_size=None,\n                      filter_amount=0.05, random_rotation=True):\n    \"\"\"Read tf.Records and prepare them for ingestion by dual_net.\n\n    See `read_tf_records` for parameter documentation.\n\n    Returns a dict of tensors (see return value of batch_parse_tf_example)\n    \"\"\"\n    print(\"Reading tf_records from {} inputs\".format(len(tf_records)))\n    dataset = read_tf_records(\n        batch_size,\n        tf_records,\n        num_repeats=num_repeats,\n        shuffle_records=shuffle_records,\n        shuffle_examples=shuffle_examples,\n        shuffle_buffer_size=shuffle_buffer_size,\n        filter_amount=filter_amount,\n        interleave=False)\n    dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))\n    dataset = dataset.map(\n        functools.partial(batch_parse_tf_example, batch_size, feature_layout))\n    if random_rotation:\n        # Unbatch the dataset so we can rotate it\n        dataset = dataset.apply(tf.data.experimental.unbatch())\n        dataset = dataset.apply(tf.data.experimental.map_and_batch(\n            functools.partial(_random_rotation, feature_layout),\n            batch_size))\n\n    return dataset.make_one_shot_iterator().get_next()\n\n\ndef get_tpu_input_tensors(batch_size, feature_layout, tf_records, num_repeats=1,\n                          shuffle_records=True, shuffle_examples=True,\n                          shuffle_buffer_size=None,\n                          filter_amount=0.05, random_rotation=True):\n    # TPUs trains on sequential golden chunks to simplify preprocessing and\n    # reproducibility.\n    assert len(tf_records) < 101, \"Use example_buffer to build a golden_chunk\"\n\n    dataset = read_tf_records(\n        batch_size,\n        tf_records,\n        num_repeats=num_repeats,\n        shuffle_records=shuffle_records,\n        shuffle_examples=shuffle_examples,\n        shuffle_buffer_size=shuffle_buffer_size,\n        filter_amount=filter_amount,\n        interleave=False)\n    dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))\n    dataset = dataset.map(\n        functools.partial(batch_parse_tf_example, batch_size, feature_layout))\n\n    # TODO(sethtroisi@): Unify\n    if random_rotation:\n        # Unbatch the dataset so we can rotate it\n        dataset = dataset.apply(tf.data.experimental.unbatch())\n        dataset = dataset.apply(tf.data.experimental.map_and_batch(\n            functools.partial(_random_rotation, feature_layout),\n            batch_size, drop_remainder=True))\n\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\n\n\ndef get_tpu_bt_input_tensors(games, games_nr, batch_size, feature_layout,\n                             num_repeats=1,\n                             number_of_games=500e3,\n                             fresh_fraction=0.05,\n                             random_rotation=True):\n    dataset = bigtable_input.get_unparsed_moves_from_last_n_games(\n        games, games_nr, number_of_games)\n    dataset = dataset.repeat(num_repeats)\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.filter(lambda t: tf.equal(tf.shape(t)[0], batch_size))\n    dataset = dataset.map(\n        functools.partial(batch_parse_tf_example, batch_size, feature_layout))\n    if random_rotation:\n        # Unbatch the dataset so we can rotate it\n        dataset = dataset.apply(tf.data.experimental.unbatch())\n        dataset = dataset.apply(tf.data.experimental.map_and_batch(\n            functools.partial(_random_rotation, feature_layout),\n            batch_size, drop_remainder=True))\n\n    dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n    return dataset\n\n\ndef make_dataset_from_selfplay(data_extracts):\n    \"\"\"\n    Returns an iterable of tf.Examples.\n    Args:\n        data_extracts: An iterable of (position, pi, result) tuples\n    \"\"\"\n    f = dual_net.get_features()\n    tf_examples = (make_tf_example(features_lib.extract_features(pos, f),\n                                   pi, result)\n                   for pos, pi, result in data_extracts)\n    return tf_examples\n\n\ndef make_dataset_from_sgf(sgf_filename, tf_record):\n    pwcs = sgf_wrapper.replay_sgf_file(sgf_filename)\n    tf_examples = map(_make_tf_example_from_pwc, pwcs)\n    write_tf_examples(tf_record, tf_examples)\n\n\ndef _make_tf_example_from_pwc(position_w_context):\n    f = dual_net.get_features()\n    features = features_lib.extract_features(position_w_context.position, f)\n    pi = _one_hot(coords.to_flat(position_w_context.next_move))\n    value = position_w_context.result\n    return make_tf_example(features, pi, value)\n"
        },
        {
          "name": "ratings",
          "type": "tree",
          "content": null
        },
        {
          "name": "requirements-analysis.txt",
          "type": "blob",
          "size": 0.0234375,
          "content": "matplotlib\npandas\nchoix\n"
        },
        {
          "name": "requirements-colab.txt",
          "type": "blob",
          "size": 0.2265625,
          "content": "# Copy of requirements.txt with packages in colab commented.\n\n#absl-py\nautopep8>=1.3\nfire\ngoogle.cloud.logging\ngoogle.cloud.bigtable\n#grpcio-tools\n#keras\n#numpy>=1.14.0\n#protobuf\npylint\nsgf==0.5\n#six\n#tqdm>=4.17\n\n#oauth2client==4.1\n"
        },
        {
          "name": "requirements.txt",
          "type": "blob",
          "size": 0.2099609375,
          "content": "absl-py\nautopep8>=1.3\nchoix>=0.3.3\nfire\ngoogle.cloud.logging\ngoogle.cloud.bigtable\ngrpcio-tools\nkeras\nnumpy>=1.14.0\nprotobuf>=3.8.0\npylint\nsgf==0.5\nsix\ntqdm>=4.17\npyasn1>=0.4.1\nsetuptools>=34.0.0\n\noauth2client==4.1\n"
        },
        {
          "name": "rl_loop",
          "type": "tree",
          "content": null
        },
        {
          "name": "selfplay.py",
          "type": "blob",
          "size": 5.7275390625,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Python implementation of selfplay worker.\n\nThis worker is used to set up many parallel selfplay instances.\"\"\"\n\n\nimport random\nimport os\nimport socket\nimport time\n\nfrom absl import app, flags\nfrom tensorflow import gfile\n\nimport coords\nimport dual_net\nimport preprocessing\nfrom strategies import MCTSPlayer\nimport utils\n\nflags.DEFINE_string('load_file', None, 'Path to model save files.')\nflags.DEFINE_string('selfplay_dir', None, 'Where to write game data.')\nflags.DEFINE_string('holdout_dir', None, 'Where to write held-out game data.')\nflags.DEFINE_string('sgf_dir', None, 'Where to write human-readable SGFs.')\nflags.DEFINE_float('holdout_pct', 0.05, 'What percent of games to hold out.')\nflags.DEFINE_float('resign_disable_pct', 0.05,\n                   'What percent of games to disable resign for.')\n\n# From strategies.py\nflags.declare_key_flag('verbose')\nflags.declare_key_flag('num_readouts')\n\n\nFLAGS = flags.FLAGS\n\n\ndef play(network):\n    \"\"\"Plays out a self-play match, returning a MCTSPlayer object containing:\n        - the final position\n        - the n x 362 tensor of floats representing the mcts search probabilities\n        - the n-ary tensor of floats representing the original value-net estimate\n          where n is the number of moves in the game\n    \"\"\"\n    readouts = FLAGS.num_readouts  # defined in strategies.py\n    # Disable resign in 5% of games\n    if random.random() < FLAGS.resign_disable_pct:\n        resign_threshold = -1.0\n    else:\n        resign_threshold = None\n\n    player = MCTSPlayer(network, resign_threshold=resign_threshold)\n\n    player.initialize_game()\n\n    # Must run this once at the start to expand the root node.\n    first_node = player.root.select_leaf()\n    prob, val = network.run(first_node.position)\n    first_node.incorporate_results(prob, val, first_node)\n\n    while True:\n        start = time.time()\n        player.root.inject_noise()\n        current_readouts = player.root.N\n        # we want to do \"X additional readouts\", rather than \"up to X readouts\".\n        while player.root.N < current_readouts + readouts:\n            player.tree_search()\n\n        if FLAGS.verbose >= 3:\n            print(player.root.position)\n            print(player.root.describe())\n\n        if player.should_resign():\n            player.set_result(-1 * player.root.position.to_play,\n                              was_resign=True)\n            break\n        move = player.pick_move()\n        player.play_move(move)\n        if player.root.is_done():\n            player.set_result(player.root.position.result(), was_resign=False)\n            break\n\n        if (FLAGS.verbose >= 2) or (FLAGS.verbose >= 1 and player.root.position.n % 10 == 9):\n            print(\"Q: {:.5f}\".format(player.root.Q))\n            dur = time.time() - start\n            print(\"%d: %d readouts, %.3f s/100. (%.2f sec)\" % (\n                player.root.position.n, readouts, dur / readouts * 100.0, dur), flush=True)\n        if FLAGS.verbose >= 3:\n            print(\"Played >>\",\n                  coords.to_gtp(coords.from_flat(player.root.fmove)))\n\n    if FLAGS.verbose >= 2:\n        utils.dbg(\"%s: %.3f\" % (player.result_string, player.root.Q))\n        utils.dbg(player.root.position, player.root.position.score())\n\n    return player\n\n\ndef run_game(load_file, selfplay_dir=None, holdout_dir=None,\n             sgf_dir=None, holdout_pct=0.05):\n    \"\"\"Takes a played game and record results and game data.\"\"\"\n    if sgf_dir is not None:\n        minimal_sgf_dir = os.path.join(sgf_dir, 'clean')\n        full_sgf_dir = os.path.join(sgf_dir, 'full')\n        utils.ensure_dir_exists(minimal_sgf_dir)\n        utils.ensure_dir_exists(full_sgf_dir)\n    if selfplay_dir is not None:\n        utils.ensure_dir_exists(selfplay_dir)\n        utils.ensure_dir_exists(holdout_dir)\n\n    with utils.logged_timer(\"Loading weights from %s ... \" % load_file):\n        network = dual_net.DualNetwork(load_file)\n\n    with utils.logged_timer(\"Playing game\"):\n        player = play(network)\n\n    output_name = '{}-{}'.format(int(time.time()), socket.gethostname())\n    game_data = player.extract_data()\n    if sgf_dir is not None:\n        with gfile.GFile(os.path.join(minimal_sgf_dir, '{}.sgf'.format(output_name)), 'w') as f:\n            f.write(player.to_sgf(use_comments=False))\n        with gfile.GFile(os.path.join(full_sgf_dir, '{}.sgf'.format(output_name)), 'w') as f:\n            f.write(player.to_sgf())\n\n    tf_examples = preprocessing.make_dataset_from_selfplay(game_data)\n\n    if selfplay_dir is not None:\n        # Hold out 5% of games for validation.\n        if random.random() < holdout_pct:\n            fname = os.path.join(holdout_dir,\n                                 \"{}.tfrecord.zz\".format(output_name))\n        else:\n            fname = os.path.join(selfplay_dir,\n                                 \"{}.tfrecord.zz\".format(output_name))\n\n        preprocessing.write_tf_examples(fname, tf_examples)\n\n\ndef main(argv):\n    \"\"\"Entry point for running one selfplay game.\"\"\"\n    del argv  # Unused\n    flags.mark_flag_as_required('load_file')\n\n    run_game(\n        load_file=FLAGS.load_file,\n        selfplay_dir=FLAGS.selfplay_dir,\n        holdout_dir=FLAGS.holdout_dir,\n        holdout_pct=FLAGS.holdout_pct,\n        sgf_dir=FLAGS.sgf_dir)\n\n\nif __name__ == '__main__':\n    app.run(main)\n"
        },
        {
          "name": "sgf",
          "type": "tree",
          "content": null
        },
        {
          "name": "sgf_wrapper.py",
          "type": "blob",
          "size": 5.8359375,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"\nCode to extract a series of positions + their next moves from an SGF.\n\nMost of the complexity here is dealing with two features of SGF:\n- Stones can be added via \"play move\" or \"add move\", the latter being used\n  to configure L+D puzzles, but also for initial handicap placement.\n- Plays don't necessarily alternate colors; they can be repeated B or W moves\n  This feature is used to handle free handicap placement.\n\"\"\"\nimport numpy as np\nimport itertools\n\nimport coords\nimport go\nfrom go import Position, PositionWithContext\nimport utils\nimport sgf\n\nSGF_TEMPLATE = '''(;GM[1]FF[4]CA[UTF-8]AP[Minigo_sgfgenerator]RU[{ruleset}]\nSZ[{boardsize}]KM[{komi}]PW[{white_name}]PB[{black_name}]RE[{result}]\n{game_moves})'''\n\nPROGRAM_IDENTIFIER = \"Minigo\"\n\n\ndef translate_sgf_move(player_move, comment):\n    if player_move.color not in (go.BLACK, go.WHITE):\n        raise ValueError(\"Can't translate color %s to sgf\" % player_move.color)\n    c = coords.to_sgf(player_move.move)\n    color = 'B' if player_move.color == go.BLACK else 'W'\n    if comment is not None:\n        comment = comment.replace(']', r'\\]')\n        comment_node = \"C[{}]\".format(comment)\n    else:\n        comment_node = \"\"\n    return \";{color}[{coords}]{comment_node}\".format(\n        color=color, coords=c, comment_node=comment_node)\n\n\ndef make_sgf(\n    move_history,\n    result_string,\n    ruleset=\"Chinese\",\n    komi=7.5,\n    white_name=PROGRAM_IDENTIFIER,\n    black_name=PROGRAM_IDENTIFIER,\n    comments=[]\n):\n    \"\"\"Turn a game into SGF.\n\n    Doesn't handle handicap games or positions with incomplete history.\n\n    Args:\n        move_history: iterable of PlayerMoves\n        result_string: \"B+R\", \"W+0.5\", etc.\n        comments: iterable of string/None. Will be zipped with move_history.\n    \"\"\"\n    boardsize = go.N\n    game_moves = ''.join(translate_sgf_move(*z)\n                         for z in itertools.zip_longest(move_history, comments))\n    result = result_string\n    return SGF_TEMPLATE.format(**locals())\n\n\ndef sgf_prop(value_list):\n    'Converts raw sgf library output to sensible value'\n    if value_list is None:\n        return None\n    if len(value_list) == 1:\n        return value_list[0]\n    else:\n        return value_list\n\n\ndef sgf_prop_get(props, key, default):\n    return sgf_prop(props.get(key, default))\n\n\ndef handle_node(pos, node):\n    'A node can either add B+W stones, play as B, or play as W.'\n    props = node.properties\n    black_stones_added = [coords.from_sgf(\n        c) for c in props.get('AB', [])]\n    white_stones_added = [coords.from_sgf(\n        c) for c in props.get('AW', [])]\n    if black_stones_added or white_stones_added:\n        return add_stones(pos, black_stones_added, white_stones_added)\n    # If B/W props are not present, then there is no move. But if it is present and equal to the empty string, then the move was a pass.\n    elif 'B' in props:\n        black_move = coords.from_sgf(props.get('B', [''])[0])\n        return pos.play_move(black_move, color=go.BLACK)\n    elif 'W' in props:\n        white_move = coords.from_sgf(props.get('W', [''])[0])\n        return pos.play_move(white_move, color=go.WHITE)\n    else:\n        return pos\n\n\ndef add_stones(pos, black_stones_added, white_stones_added):\n    working_board = np.copy(pos.board)\n    go.place_stones(working_board, go.BLACK, black_stones_added)\n    go.place_stones(working_board, go.WHITE, white_stones_added)\n    new_position = Position(board=working_board, n=pos.n, komi=pos.komi,\n                            caps=pos.caps, ko=pos.ko, recent=pos.recent, to_play=pos.to_play)\n    return new_position\n\n\ndef get_next_move(node):\n    props = node.next.properties\n    if 'W' in props:\n        return coords.from_sgf(props['W'][0])\n    else:\n        return coords.from_sgf(props['B'][0])\n\n\ndef maybe_correct_next(pos, next_node):\n    if (('B' in next_node.properties and not pos.to_play == go.BLACK) or\n            ('W' in next_node.properties and not pos.to_play == go.WHITE)):\n        pos.flip_playerturn(mutate=True)\n\n\ndef get_sgf_root_node(sgf_contents):\n    collection = sgf.parse(sgf_contents)\n    game = collection.children[0]\n    return game.root\n\n\ndef replay_sgf(sgf_contents):\n    \"\"\"Wrapper for sgf files, returning go.PositionWithContext instances.\n\n    It does NOT return the very final position, as there is no follow up.\n    To get the final position, call pwc.position.play_move(pwc.next_move)\n    on the last PositionWithContext returned.\n\n    Example usage:\n    with open(filename) as f:\n        for position_w_context in replay_sgf(f.read()):\n            print(position_w_context.position)\n    \"\"\"\n    root_node = get_sgf_root_node(sgf_contents)\n    props = root_node.properties\n    assert int(sgf_prop(props.get('GM', ['1']))) == 1, \"Not a Go SGF!\"\n\n    komi = 0\n    if props.get('KM') is not None:\n        komi = float(sgf_prop(props.get('KM')))\n    result = utils.parse_game_result(sgf_prop(props.get('RE', '')))\n\n    pos = Position(komi=komi)\n    current_node = root_node\n    while pos is not None and current_node.next is not None:\n        pos = handle_node(pos, current_node)\n        maybe_correct_next(pos, current_node.next)\n        next_move = get_next_move(current_node)\n        yield PositionWithContext(pos, next_move, result)\n        current_node = current_node.next\n\n\ndef replay_sgf_file(sgf_file):\n    with open(sgf_file) as f:\n        for pwc in replay_sgf(f.read()):\n            yield pwc\n"
        },
        {
          "name": "strategies.py",
          "type": "blob",
          "size": 10.697265625,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport os\nimport random\nimport time\n\nfrom absl import flags\n\nimport coords\nimport go\nimport mcts\nimport sgf_wrapper\nfrom utils import dbg\nfrom player_interface import MCTSPlayerInterface\n\n\nflags.DEFINE_integer('softpick_move_cutoff', (go.N * go.N // 12) // 2 * 2,\n                     'The move number (<) up to which moves are softpicked from MCTS visits.')\n# Ensure that both white and black have an equal number of softpicked moves.\nflags.register_validator('softpick_move_cutoff', lambda x: x % 2 == 0)\n\nflags.DEFINE_float('resign_threshold', -0.9,\n                   'The post-search Q evaluation at which resign should happen.'\n                   'A threshold of -1 implies resign is disabled.')\nflags.register_validator('resign_threshold', lambda x: -1 <= x < 0)\n\nflags.DEFINE_integer('num_readouts', 800 if go.N == 19 else 200,\n                     'Number of searches to add to the MCTS search tree before playing a move.')\nflags.register_validator('num_readouts', lambda x: x > 0)\n\nflags.DEFINE_integer('parallel_readouts', 8,\n                     'Number of searches to execute in parallel. This is also the batch size'\n                     'for neural network evaluation.')\n\n# this should be called \"verbosity\" but flag name conflicts with absl.logging.\n# Should fix this by overhauling this logging system with appropriate logging.info/debug.\nflags.DEFINE_integer('verbose', 1, 'How much debug info to print.')\n\nFLAGS = flags.FLAGS\n\n\ndef time_recommendation(move_num, seconds_per_move=5, time_limit=15 * 60,\n                        decay_factor=0.98):\n    \"\"\"Given the current move number and the 'desired' seconds per move, return\n    how much time should actually be used. This is intended specifically for\n    CGOS time controls, which has an absolute 15-minute time limit.\n\n    The strategy is to spend the maximum possible moves using seconds_per_move,\n    and then switch to an exponentially decaying time usage, calibrated so that\n    we have enough time for an infinite number of moves.\"\"\"\n\n    # Divide by two since you only play half the moves in a game.\n    player_move_num = move_num / 2\n\n    # Sum of geometric series maxes out at endgame_time seconds.\n    endgame_time = seconds_per_move / (1 - decay_factor)\n\n    if endgame_time > time_limit:\n        # There is so little main time that we're already in 'endgame' mode.\n        base_time = time_limit * (1 - decay_factor)\n        core_moves = 0\n    else:\n        # Leave over endgame_time seconds for the end, and play at\n        # seconds_per_move for as long as possible.\n        base_time = seconds_per_move\n        core_moves = (time_limit - endgame_time) / seconds_per_move\n\n    return base_time * decay_factor ** max(player_move_num - core_moves, 0)\n\n\nclass MCTSPlayer(MCTSPlayerInterface):\n    def __init__(self, network, seconds_per_move=5, num_readouts=0,\n                 resign_threshold=None, two_player_mode=False,\n                 timed_match=False):\n        self.network = network\n        self.seconds_per_move = seconds_per_move\n        self.num_readouts = num_readouts or FLAGS.num_readouts\n        self.verbosity = FLAGS.verbose\n        self.two_player_mode = two_player_mode\n        if two_player_mode:\n            self.temp_threshold = -1\n        else:\n            self.temp_threshold = FLAGS.softpick_move_cutoff\n\n        self.initialize_game()\n        self.root = None\n        self.resign_threshold = resign_threshold or FLAGS.resign_threshold\n        self.timed_match = timed_match\n        assert (self.timed_match and self.seconds_per_move >\n                0) or self.num_readouts > 0\n        super().__init__()\n\n    def get_position(self):\n        return self.root.position if self.root else None\n\n    def get_root(self):\n        return self.root\n\n    def get_result_string(self):\n        return self.result_string\n\n    def initialize_game(self, position=None):\n        if position is None:\n            position = go.Position()\n        self.root = mcts.MCTSNode(position)\n        self.result = 0\n        self.result_string = None\n        self.comments = []\n        self.searches_pi = []\n\n    def suggest_move(self, position):\n        \"\"\"Used for playing a single game.\n\n        For parallel play, use initialize_move, select_leaf,\n        incorporate_results, and pick_move\n        \"\"\"\n        start = time.time()\n\n        if self.timed_match:\n            while time.time() - start < self.seconds_per_move:\n                self.tree_search()\n        else:\n            current_readouts = self.root.N\n            while self.root.N < current_readouts + self.num_readouts:\n                self.tree_search()\n            if self.verbosity > 0:\n                dbg(\"%d: Searched %d times in %.2f seconds\\n\\n\" % (\n                    position.n, self.num_readouts, time.time() - start))\n\n        # print some stats on moves considered.\n        if self.verbosity > 2:\n            dbg(self.root.describe())\n            dbg('\\n\\n')\n        if self.verbosity > 3:\n            dbg(self.root.position)\n\n        return self.pick_move()\n\n    def play_move(self, c):\n        \"\"\"Notable side effects:\n          - finalizes the probability distribution according to\n          this roots visit counts into the class' running tally, `searches_pi`\n          - Makes the node associated with this move the root, for future\n            `inject_noise` calls.\n        \"\"\"\n        if not self.two_player_mode:\n            self.searches_pi.append(self.root.children_as_pi(\n                self.root.position.n < self.temp_threshold))\n        self.comments.append(self.root.describe())\n        try:\n            self.root = self.root.maybe_add_child(coords.to_flat(c))\n        except go.IllegalMove:\n            dbg(\"Illegal move\")\n            if not self.two_player_mode:\n                self.searches_pi.pop()\n            self.comments.pop()\n            raise\n\n        self.position = self.root.position  # for showboard\n        del self.root.parent.children\n        return True  # GTP requires positive result.\n\n    def pick_move(self):\n        \"\"\"Picks a move to play, based on MCTS readout statistics.\n\n        Highest N is most robust indicator. In the early stage of the game, pick\n        a move weighted by visit count; later on, pick the absolute max.\"\"\"\n        if self.root.position.n >= self.temp_threshold:\n            fcoord = self.root.best_child()\n        else:\n            cdf = self.root.children_as_pi(squash=True).cumsum()\n            cdf /= cdf[-2]  # Prevents passing via softpick.\n            selection = random.random()\n            fcoord = cdf.searchsorted(selection)\n            assert self.root.child_N[fcoord] != 0\n        return coords.from_flat(fcoord)\n\n    def tree_search(self, parallel_readouts=None):\n        if parallel_readouts is None:\n            parallel_readouts = min(FLAGS.parallel_readouts, self.num_readouts)\n        leaves = []\n        failsafe = 0\n        while len(leaves) < parallel_readouts and failsafe < parallel_readouts * 2:\n            failsafe += 1\n            leaf = self.root.select_leaf()\n            if self.verbosity >= 4:\n                dbg(self.show_path_to_root(leaf))\n            # if game is over, override the value estimate with the true score\n            if leaf.is_done():\n                value = 1 if leaf.position.score() > 0 else -1\n                leaf.backup_value(value, up_to=self.root)\n                continue\n            leaf.add_virtual_loss(up_to=self.root)\n            leaves.append(leaf)\n        if leaves:\n            move_probs, values = self.network.run_many(\n                [leaf.position for leaf in leaves])\n            for leaf, move_prob, value in zip(leaves, move_probs, values):\n                leaf.revert_virtual_loss(up_to=self.root)\n                leaf.incorporate_results(move_prob, value, up_to=self.root)\n        return leaves\n\n    def show_path_to_root(self, node):\n        pos = node.position\n        diff = node.position.n - self.root.position.n\n        if len(pos.recent) == 0:\n            return\n\n        def fmt(move):\n            return \"{}-{}\".format('b' if move.color == go.BLACK else 'w',\n                                  coords.to_gtp(move.move))\n\n        path = \" \".join(fmt(move) for move in pos.recent[-diff:])\n        if node.position.n >= FLAGS.max_game_length:\n            path += \" (depth cutoff reached) %0.1f\" % node.position.score()\n        elif node.position.is_game_over():\n            path += \" (game over) %0.1f\" % node.position.score()\n        return path\n\n    def is_done(self):\n        return self.result != 0 or self.root.is_done()\n\n    def should_resign(self):\n        \"\"\"Returns true if the player resigned. No further moves should be played\"\"\"\n        return self.root.Q_perspective < self.resign_threshold\n\n    def set_result(self, winner, was_resign):\n        self.result = winner\n        if was_resign:\n            string = \"B+R\" if winner == go.BLACK else \"W+R\"\n        else:\n            string = self.root.position.result_string()\n        self.result_string = string\n\n    def to_sgf(self, use_comments=True):\n        assert self.result_string is not None\n        pos = self.root.position\n        if use_comments:\n            comments = self.comments or ['No comments.']\n            comments[0] = (\"Resign Threshold: %0.3f\\n\" %\n                           self.resign_threshold) + comments[0]\n        else:\n            comments = []\n        return sgf_wrapper.make_sgf(pos.recent, self.result_string,\n                                    white_name=os.path.basename(\n                                        self.network.save_file) or \"Unknown\",\n                                    black_name=os.path.basename(\n                                        self.network.save_file) or \"Unknown\",\n                                    comments=comments)\n\n    def extract_data(self):\n        assert len(self.searches_pi) == self.root.position.n\n        assert self.result != 0\n        for pwc, pi in zip(go.replay_position(self.root.position, self.result),\n                           self.searches_pi):\n            yield pwc.position, pi, pwc.result\n\n    def get_num_readouts(self):\n        return self.num_readouts\n\n    def set_num_readouts(self, readouts):\n        self.num_readouts = readouts\n\n\nclass CGOSPlayer(MCTSPlayer):\n    def suggest_move(self, position):\n        self.seconds_per_move = time_recommendation(position.n)\n        return super().suggest_move(position)\n"
        },
        {
          "name": "symmetries.py",
          "type": "blob",
          "size": 4.9736328125,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\nimport functools\nimport random\n\nimport go\n\nimport numpy as np\nimport tensorflow as tf\n\n\"\"\"\nAllowable symmetries:\nidentity [12][34]\nrot90 [24][13]\nrot180 [43][21]\nrot270 [31][42]\nflip [13][24]\nfliprot90 [34][12]\nfliprot180 [42][31]\nfliprot270 [21][43]\n\"\"\"\nINVERSES = {\n    'identity': 'identity',\n    'rot90': 'rot270',\n    'rot180': 'rot180',\n    'rot270': 'rot90',\n    'flip': 'flip',\n    'fliprot90': 'fliprot90',\n    'fliprot180': 'fliprot180',\n    'fliprot270': 'fliprot270',\n}\n\nIMPLS = {\n    'identity': lambda x: x,\n    'rot90': np.rot90,\n    'rot180': functools.partial(np.rot90, k=2),\n    'rot270': functools.partial(np.rot90, k=3),\n    'flip': lambda x: np.rot90(np.fliplr(x)),\n    'fliprot90': np.flipud,\n    'fliprot180': lambda x: np.rot90(np.flipud(x)),\n    'fliprot270': np.fliplr,\n}\n\nassert set(IMPLS.keys()) == set(INVERSES.keys())\n\n# A symmetry is just a string describing the transformation.\nSYMMETRIES = list(INVERSES.keys())\n\n\ndef invert_symmetry(s):\n    return INVERSES[s]\n\n\ndef apply_symmetry_feat(sym, features):\n    return IMPLS[sym](features)\n\n\ndef apply_symmetry_pi(s, pi):\n    pi = np.copy(pi)\n    # rotate all moves except for the pass move at end\n    pi[:-1] = IMPLS[s](pi[:-1].reshape([go.N, go.N])).ravel()\n    return pi\n\n\ndef randomize_symmetries_feat(features):\n    symmetries_used = [random.choice(SYMMETRIES) for _ in features]\n    return symmetries_used, [apply_symmetry_feat(s, f)\n                             for s, f in zip(symmetries_used, features)]\n\n\ndef invert_symmetries_pi(symmetries, pis):\n    return [apply_symmetry_pi(invert_symmetry(s), pi)\n            for s, pi in zip(symmetries, pis)]\n\n\ndef rotate_train_nhwc(x, pi):\n    sym = tf.random_uniform(\n        [],\n        minval=0,\n        maxval=len(SYMMETRIES),\n        dtype=tf.int32,\n        seed=123)\n\n    def rotate(tensor):\n        # flipLeftRight\n        tensor = tf.where(\n            tf.bitwise.bitwise_and(sym, 1) > 0,\n            tf.reverse(tensor, axis=[0]),\n            tensor)\n        # flipUpDown\n        tensor = tf.where(\n            tf.bitwise.bitwise_and(sym, 2) > 0,\n            tf.reverse(tensor, axis=[1]),\n            tensor)\n        # flipDiagonal\n        tensor = tf.where(\n            tf.bitwise.bitwise_and(sym, 4) > 0,\n            tf.transpose(tensor, perm=[1, 0, 2]),\n            tensor)\n        return tensor\n\n    # TODO(tommadams): use tf.ensure_shape instead of tf.assert_equal.\n    squares = go.N * go.N\n    assert_shape_pi = tf.assert_equal(pi.shape.as_list(), [squares + 1])\n\n    x_shape = x.shape.as_list()\n    assert_shape_x = tf.assert_equal(x_shape, [go.N, go.N, x_shape[2]])\n\n    pi_move = tf.slice(pi, [0], [squares], name=\"slice_moves\")\n    pi_pass = tf.slice(pi, [squares], [1], name=\"slice_pass\")\n    # Add a final dim so that x and pi have same shape: [N,N,num_features].\n    pi_n_by_n = tf.reshape(pi_move, [go.N, go.N, 1])\n\n    with tf.control_dependencies([assert_shape_x, assert_shape_pi]):\n        pi_rot = tf.concat(\n            [tf.reshape(rotate(pi_n_by_n), [squares]), pi_pass],\n            axis=0)\n\n    return rotate(x), pi_rot\n\n\ndef rotate_train_nchw(x, pi):\n    sym = tf.random_uniform(\n        [],\n        minval=0,\n        maxval=len(SYMMETRIES),\n        dtype=tf.int32,\n        seed=123)\n\n    def rotate(tensor):\n        # flipLeftRight\n        tensor = tf.where(\n            tf.bitwise.bitwise_and(sym, 1) > 0,\n            tf.reverse(tensor, axis=[1]),\n            tensor)\n        # flipUpDown\n        tensor = tf.where(\n            tf.bitwise.bitwise_and(sym, 2) > 0,\n            tf.reverse(tensor, axis=[2]),\n            tensor)\n        # flipDiagonal\n        tensor = tf.where(\n            tf.bitwise.bitwise_and(sym, 4) > 0,\n            tf.transpose(tensor, perm=[0, 2, 1]),\n            tensor)\n        return tensor\n\n    # TODO(tommadams): use tf.ensure_shape instead of tf.assert_equal.\n    squares = go.N * go.N\n    assert_shape_pi = tf.assert_equal(pi.shape.as_list(), [squares + 1])\n\n    x_shape = x.shape.as_list()\n    assert_shape_x = tf.assert_equal(x_shape, [x_shape[0], go.N, go.N])\n\n    pi_move = tf.slice(pi, [0], [squares], name=\"slice_moves\")\n    pi_pass = tf.slice(pi, [squares], [1], name=\"slice_pass\")\n    # Add a dim so that x and pi have same shape: [num_features,N,N].\n    pi_n_by_n = tf.reshape(pi_move, [1, go.N, go.N])\n\n    with tf.control_dependencies([assert_shape_x, assert_shape_pi]):\n        pi_rot = tf.concat(\n            [tf.reshape(rotate(pi_n_by_n), [squares]), pi_pass],\n            axis=0)\n\n    return rotate(x), pi_rot\n"
        },
        {
          "name": "test.sh",
          "type": "blob",
          "size": 1.5126953125,
          "content": "#!/bin/bash\n#\n# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n# Simple shell script to lint files and run the tests. Could be helpful for\n# users, but largely for automation.\n#\n# NOTE! If this file changes/moves, please change\n# https://github.com/kubernetes/test-infra/blob/master/config/jobs/tensorflow/minigo/minigo.yaml\n\n# Ensure we're running from this directory to ensure PYTHONPATH is set\n# correctly.\ncd \"$(dirname \"$0\")\"\n\nlint_fail=0\npython3 -m pylint *.py || {\n  lint_fail=1\n  echo >&2 \"--------------------------------------\"\n  echo >&2 \"Py linting did not pass successfully!\"\n}\n\nPYTHONPATH= BOARD_SIZE=9 python3 tests/run_tests.py || {\n  echo >&2 \"--------------------------------------\"\n  echo >&2 \"The tests did not pass successfully!\"\n  exit 1\n}\n\nBOARD_SIZE=9 python3 rl_loop/local_integration_test.py || {\n  echo >&2 \"--------------------------------------\"\n  echo >&2 \"Integration test did not pass successfully!\"\n  exit 1\n}\n\nif [ \"${lint_fail}\" -eq \"1\" ]; then\n  exit 1\nfi\n\necho >&2 \"All tests passed!\"\n"
        },
        {
          "name": "testing",
          "type": "tree",
          "content": null
        },
        {
          "name": "tests",
          "type": "tree",
          "content": null
        },
        {
          "name": "train.py",
          "type": "blob",
          "size": 10.166015625,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Train a network.\n\nUsage:\n  BOARD_SIZE=19 python train.py tfrecord1 tfrecord2 tfrecord3\n\"\"\"\n\nimport logging\nimport math\n\nfrom absl import app, flags\nimport numpy as np\nimport tensorflow as tf\n\nimport bigtable_input\nimport dual_net\nimport preprocessing\nimport utils\n\n# See www.moderndescartes.com/essays/shuffle_viz for discussion on sizing\nflags.DEFINE_integer('shuffle_buffer_size', 2000,\n                     'Size of buffer used to shuffle train examples.')\n\nflags.DEFINE_boolean('shuffle_examples', True,\n                     'Whether to shuffle training examples.')\n\nflags.DEFINE_integer('steps_to_train', None,\n                     'Number of training steps to take. If not set, iterates '\n                     'once over training data.')\n\nflags.DEFINE_integer('num_examples', None,\n                     'Total number of input examples. This is only used if '\n                     'steps_to_train is not set. Requires that filter_amount '\n                     'is 1.0.')\n\nflags.DEFINE_integer('window_size', 500000,\n                     'Number of games to include in the window')\n\nflags.DEFINE_float('filter_amount', 1.0,\n                   'Fraction of positions to filter from golden chunks,'\n                   'default, 1.0 (no filter)')\n\nflags.DEFINE_string('export_path', None,\n                    'Where to export the model after training.')\n\nflags.DEFINE_bool('use_bt', False,\n                  'Whether to use Bigtable as input.  '\n                  '(Only supported with --use_tpu, currently.)')\n\nflags.DEFINE_bool('freeze', False,\n                  'Whether to freeze the graph at the end of training.')\n\nflags.DEFINE_boolean(\n    'use_trt', False, 'True to write a GraphDef that uses the TRT runtime')\nflags.DEFINE_integer('trt_max_batch_size', None,\n                     'Maximum TRT batch size')\nflags.DEFINE_string('trt_precision', 'fp32',\n                    'Precision for TRT runtime: fp16, fp32 or int8')\nflags.register_multi_flags_validator(\n    ['use_trt', 'trt_max_batch_size'],\n    lambda flags: not flags['use_trt'] or flags['trt_max_batch_size'],\n    'trt_max_batch_size must be set if use_trt is true')\n\n\nflags.register_multi_flags_validator(\n    ['use_bt', 'use_tpu'],\n    lambda flags: flags['use_tpu'] if flags['use_bt'] else True,\n    '`use_bt` flag only valid with `use_tpu` as well')\n\n@flags.multi_flags_validator(\n    ['num_examples', 'steps_to_train', 'filter_amount'],\n    '`num_examples` requires `steps_to_train==0` and `filter_amount==1.0`')\ndef _example_flags_validator(flags_dict):\n    if not flags_dict['num_examples']:\n        return True\n    return not flags_dict['steps_to_train'] and flags_dict['filter_amount'] == 1.0\n\n@flags.multi_flags_validator(\n    ['use_bt', 'cbt_project', 'cbt_instance', 'cbt_table'],\n    message='Cloud Bigtable configuration flags not correct')\ndef _bt_checker(flags_dict):\n    if not flags_dict['use_bt']:\n        return True\n    return (flags_dict['cbt_project']\n            and flags_dict['cbt_instance']\n            and flags_dict['cbt_table'])\n\n\n# From dual_net.py\nflags.declare_key_flag('work_dir')\nflags.declare_key_flag('train_batch_size')\nflags.declare_key_flag('num_tpu_cores')\nflags.declare_key_flag('use_tpu')\n\nFLAGS = flags.FLAGS\n\n\nclass EchoStepCounterHook(tf.train.StepCounterHook):\n    \"\"\"A hook that logs steps per second.\"\"\"\n\n    def _log_and_record(self, elapsed_steps, elapsed_time, global_step):\n        s_per_sec = elapsed_steps / elapsed_time\n        logging.info(\"{}: {:.3f} steps per second\".format(global_step, s_per_sec))\n        super()._log_and_record(elapsed_steps, elapsed_time, global_step)\n\n\ndef compute_update_ratio(weight_tensors, before_weights, after_weights):\n    \"\"\"Compute the ratio of gradient norm to weight norm.\"\"\"\n    deltas = [after - before for after,\n              before in zip(after_weights, before_weights)]\n    delta_norms = [np.linalg.norm(d.ravel()) for d in deltas]\n    weight_norms = [np.linalg.norm(w.ravel()) for w in before_weights]\n    ratios = [d / w for d, w in zip(delta_norms, weight_norms)]\n    all_summaries = [\n        tf.Summary.Value(tag='update_ratios/' +\n                         tensor.name, simple_value=ratio)\n        for tensor, ratio in zip(weight_tensors, ratios)]\n    return tf.Summary(value=all_summaries)\n\n\nclass UpdateRatioSessionHook(tf.train.SessionRunHook):\n    \"\"\"A hook that computes ||grad|| / ||weights|| (using frobenius norm).\"\"\"\n\n    def __init__(self, output_dir, every_n_steps=1000):\n        self.output_dir = output_dir\n        self.every_n_steps = every_n_steps\n        self.before_weights = None\n        self.file_writer = None\n        self.weight_tensors = None\n        self.global_step = None\n\n    def begin(self):\n        \"\"\"Called once before using the session\"\"\"\n        # These calls only works because the SessionRunHook api guarantees this\n        # will get called within a graph context containing our model graph.\n\n        self.file_writer = tf.summary.FileWriterCache.get(self.output_dir)\n        self.weight_tensors = tf.trainable_variables()\n        self.global_step = tf.train.get_or_create_global_step()\n\n    def before_run(self, run_context):\n        \"\"\"Called before each call to run().\"\"\"\n        global_step = run_context.session.run(self.global_step)\n        if global_step % self.every_n_steps == 0:\n            self.before_weights = run_context.session.run(self.weight_tensors)\n\n    def after_run(self, run_context, unused_run_values):\n        \"\"\"Called after each call to run().\"\"\"\n        global_step = run_context.session.run(self.global_step)\n        if self.before_weights is not None:\n            after_weights = run_context.session.run(self.weight_tensors)\n            weight_update_summaries = compute_update_ratio(\n                self.weight_tensors, self.before_weights, after_weights)\n            self.file_writer.add_summary(\n                weight_update_summaries, global_step)\n            self.before_weights = None\n\n\ndef train(*tf_records: \"Records to train on\"):\n    \"\"\"Train on examples.\"\"\"\n    tf.logging.set_verbosity(tf.logging.INFO)\n    estimator = dual_net.get_estimator()\n\n    effective_batch_size = FLAGS.train_batch_size\n    if FLAGS.use_tpu:\n        effective_batch_size *= FLAGS.num_tpu_cores\n\n    if FLAGS.use_tpu:\n        if FLAGS.use_bt:\n            def _input_fn(params):\n                games = bigtable_input.GameQueue(\n                    FLAGS.cbt_project, FLAGS.cbt_instance, FLAGS.cbt_table)\n                games_nr = bigtable_input.GameQueue(\n                    FLAGS.cbt_project, FLAGS.cbt_instance, FLAGS.cbt_table + '-nr')\n                return preprocessing.get_tpu_bt_input_tensors(\n                    games,\n                    games_nr,\n                    params['batch_size'],\n                    params['input_layout'],\n                    number_of_games=FLAGS.window_size,\n                    random_rotation=True)\n        else:\n            def _input_fn(params):\n                return preprocessing.get_tpu_input_tensors(\n                    params['batch_size'],\n                    params['input_layout'],\n                    tf_records,\n                    filter_amount=FLAGS.filter_amount,\n                    shuffle_examples=FLAGS.shuffle_examples,\n                    shuffle_buffer_size=FLAGS.shuffle_buffer_size,\n                    random_rotation=True)\n        # Hooks are broken with TPUestimator at the moment.\n        hooks = []\n    else:\n        def _input_fn():\n            return preprocessing.get_input_tensors(\n                FLAGS.train_batch_size,\n                FLAGS.input_layout,\n                tf_records,\n                filter_amount=FLAGS.filter_amount,\n                shuffle_examples=FLAGS.shuffle_examples,\n                shuffle_buffer_size=FLAGS.shuffle_buffer_size,\n                random_rotation=True)\n\n        hooks = [UpdateRatioSessionHook(FLAGS.work_dir),\n                 EchoStepCounterHook(output_dir=FLAGS.work_dir)]\n\n    steps = FLAGS.steps_to_train\n    if not steps and FLAGS.num_examples:\n        batch_size = FLAGS.train_batch_size\n        if FLAGS.use_tpu:\n            batch_size *= FLAGS.num_tpu_cores\n        steps = math.floor(FLAGS.num_examples / batch_size)\n\n    logging.info(\"Training, steps = %s, batch = %s -> %s examples\",\n                 steps or '?', effective_batch_size,\n                 (steps * effective_batch_size) if steps else '?')\n\n    if FLAGS.use_bt:\n        games = bigtable_input.GameQueue(\n            FLAGS.cbt_project, FLAGS.cbt_instance, FLAGS.cbt_table)\n        if not games.read_wait_cell():\n            games.require_fresh_games(20000)\n        latest_game = games.latest_game_number\n        index_from = max(latest_game, games.read_wait_cell())\n        print(\"== Last game before training:\", latest_game, flush=True)\n        print(\"== Wait cell:\", games.read_wait_cell(), flush=True)\n\n    try:\n        estimator.train(_input_fn, steps=steps, hooks=hooks)\n        if FLAGS.use_bt:\n            bigtable_input.set_fresh_watermark(games, index_from,\n                                               FLAGS.window_size)\n    except:\n        if FLAGS.use_bt:\n            games.require_fresh_games(0)\n        raise\n\n\ndef main(argv):\n    \"\"\"Train on examples and export the updated model weights.\"\"\"\n    tf_records = argv[1:]\n    logging.info(\"Training on %s records: %s to %s\",\n                 len(tf_records), tf_records[0], tf_records[-1])\n    with utils.logged_timer(\"Training\"):\n        train(*tf_records)\n    if FLAGS.export_path:\n        dual_net.export_model(FLAGS.export_path)\n    if FLAGS.freeze:\n        if FLAGS.use_tpu:\n            dual_net.freeze_graph_tpu(FLAGS.export_path)\n        else:\n            dual_net.freeze_graph(FLAGS.export_path, FLAGS.use_trt,\n                                  FLAGS.trt_max_batch_size, FLAGS.trt_precision)\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n"
        },
        {
          "name": "tsconfig.json",
          "type": "blob",
          "size": 0.3994140625,
          "content": "{\n    \"compilerOptions\": {\n        \"target\": \"ESNEXT\",\n        \"module\": \"system\",\n        \"noImplicitAny\": true,\n        \"noImplicitThis\": true,\n        \"strictNullChecks\": true,\n        \"removeComments\": true,\n        \"preserveConstEnums\": true,\n        \"outDir\": \"minigui/static/\",\n        \"sourceMap\": true,\n\t\"module\": \"amd\"\n    },\n    \"include\": [\n        \"minigui/*.ts\"\n    ],\n    \"exclude\": [\n    ]\n}\n\n"
        },
        {
          "name": "utils.py",
          "type": "blob",
          "size": 2.3388671875,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\"\"\"Miscellaneous utilities\"\"\"\n\nfrom contextlib import contextmanager\nimport functools\nimport itertools\nimport logging\nimport operator\nimport os\nimport re\nimport sys\nimport time\n\n\ndef dbg(*objects, file=sys.stderr, flush=True, **kwargs):\n    \"Helper function to print to stderr and flush\"\n    print(*objects, file=file, flush=flush, **kwargs)\n\n\ndef ensure_dir_exists(directory):\n    \"Creates local directories if they don't exist.\"\n    if directory.startswith('gs://'):\n        return\n    if not os.path.exists(directory):\n        dbg(\"Making dir {}\".format(directory))\n    os.makedirs(directory, exist_ok=True)\n\n\ndef parse_game_result(result):\n    \"Parse an SGF result string into value target.\"\n    if re.match(r'[bB]\\+', result):\n        return 1\n    if re.match(r'[wW]\\+', result):\n        return -1\n    return 0\n\n\ndef product(iterable):\n    \"Like sum(), but with multiplication.\"\n    return functools.reduce(operator.mul, iterable)\n\n\ndef _take_n(num_things, iterable):\n    return list(itertools.islice(iterable, num_things))\n\n\ndef iter_chunks(chunk_size, iterator):\n    \"Yield from an iterator in chunks of chunk_size.\"\n    iterator = iter(iterator)\n    while True:\n        next_chunk = _take_n(chunk_size, iterator)\n        # If len(iterable) % chunk_size == 0, don't return an empty chunk.\n        if next_chunk:\n            yield next_chunk\n        else:\n            break\n\n\n@contextmanager\ndef timer(message):\n    \"Context manager for timing snippets of code.\"\n    tick = time.time()\n    yield\n    tock = time.time()\n    print(\"%s: %.3f seconds\" % (message, (tock - tick)))\n\n\n@contextmanager\ndef logged_timer(message):\n    \"Context manager for timing snippets of code. Echos to logging module.\"\n    tick = time.time()\n    yield\n    tock = time.time()\n    logging.info(\"%s: %.3f seconds\", message, (tock - tick))\n"
        },
        {
          "name": "validate.py",
          "type": "blob",
          "size": 3.013671875,
          "content": "# Copyright 2018 Google LLC\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\"\"\"Validate a network.\n\nUsage:\n    python validate.py tfrecord_dir/ tfrecord_dir2/\n\"\"\"\nimport os\n\nfrom absl import app, flags\n\nimport dual_net\nimport preprocessing\nimport utils\n\nflags.DEFINE_integer('examples_to_validate', 50 * 2048,\n                     'Number of examples to run validation on.')\n\nflags.DEFINE_string('validate_name', 'holdout',\n                    'Name of validation set (i.e. holdout or human).')\n\nflags.DEFINE_bool('expand_validation_dirs', True,\n                  'Whether to expand the input paths by globbing. If false, '\n                  'directly read and validate on the given files.')\n\n# From dual_net.py\nflags.declare_key_flag('work_dir')\nflags.declare_key_flag('use_tpu')\nflags.declare_key_flag('num_tpu_cores')\n\nFLAGS = flags.FLAGS\n\n\ndef validate(*tf_records):\n    \"\"\"Validate a model's performance on a set of holdout data.\"\"\"\n    if FLAGS.use_tpu:\n        def _input_fn(params):\n            return preprocessing.get_tpu_input_tensors(\n                params['train_batch_size'], params['input_layout'], tf_records,\n                filter_amount=1.0)\n    else:\n        def _input_fn():\n            return preprocessing.get_input_tensors(\n                FLAGS.train_batch_size, FLAGS.input_layout, tf_records,\n                filter_amount=1.0, shuffle_examples=False)\n\n    steps = FLAGS.examples_to_validate // FLAGS.train_batch_size\n    if FLAGS.use_tpu:\n        steps //= FLAGS.num_tpu_cores\n\n    estimator = dual_net.get_estimator()\n    with utils.logged_timer(\"Validating\"):\n        estimator.evaluate(_input_fn, steps=steps, name=FLAGS.validate_name)\n\n\ndef main(argv):\n    \"\"\"Validate a model's performance on a set of holdout data.\"\"\"\n    _, *validation_paths = argv\n    if FLAGS.expand_validation_dirs:\n        tf_records = []\n        with utils.logged_timer(\"Building lists of holdout files\"):\n            dirs = validation_paths\n            while dirs:\n                d = dirs.pop()\n                for path, newdirs, files in os.walk(d):\n                    tf_records.extend(os.path.join(path, f) for f in files if f.endswith('.zz'))\n                    dirs.extend(os.path.join(path, d) for d in newdirs)\n\n    else:\n        tf_records = validation_paths\n\n    if not tf_records:\n        print(\"Validation paths:\", validation_paths)\n        print([\"{}:\\n\\t{}\".format(p, os.listdir(p)) for p in validation_paths])\n        raise RuntimeError(\"Did not find any holdout files for validating!\")\n    validate(*tf_records)\n\n\nif __name__ == \"__main__\":\n    app.run(main)\n"
        }
      ]
    }
  ]
}