{
  "metadata": {
    "timestamp": 1736565267425,
    "page": 75,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjgw",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "Tencent/TNN",
      "stars": 4442,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".clang-format",
          "type": "blob",
          "size": 3.900390625,
          "content": "---\n# 语言: None Cpp Java ObjC Protp\nLanguage:      Cpp\n#LLVM Google\nBasedOnStyle:  Google\n# 语言: None Cpp Java ObjC Protp\n# 访问说明符的偏移(public private)\nAccessModifierOffset: -4\n# 括号之后,水平对齐参数: Align DontAlign AlwaysBreak\nAlignAfterOpenBracket: Align\n# 连续的宏\n# AlignConsecutiveMacros: true\n# 连续的赋值时,对齐所有的等号\nAlignConsecutiveAssignments: true\n# 左对齐换行(使用反斜杠换行)的反斜杠\nAlignEscapedNewlines: Right\n# # 左对齐换行(使用反斜杠换行)的反斜杠\n# AlignEscapedNewlinesLeft: true\n# 水平对齐二元和三元表达式的操作数\nAlignOperands: true\n# 允许函数声明的所有参数在放在下一行\nAllowAllParametersOfDeclarationOnNextLine: false\n# AllowAllArgumentsOnNextLine: false\n# 允许短的块放在同一行\nAllowShortBlocksOnASingleLine : false\n# 允许短的case标签放在同一行\nAllowShortCaseLabelsOnASingleLine: false\n# 允许短的函数放在同一行: None, InlineOnly(定义在类中), Empty(空函数), Inline(定义在类中，空函数), All\nAllowShortFunctionsOnASingleLine: Empty\n# 是否允许短if单行 If true, if (a) return; 可以放到同一行\nAllowShortIfStatementsOnASingleLine: false\n# 允许短的循环保持在同一行\nAllowShortLoopsOnASingleLine: false\n# 总是在定义返回类型后换行(deprecated)\nAlwaysBreakAfterDefinitionReturnType: None\n# 每行字符的限制，0表示没有限制\nColumnLimit: 120\n# 描述具有特殊意义的注释的正则表达式，它不应该被分割为多行或以其它方式改变\nCommentPragmas: '^ IWYU pragma:'\n#指针的*的挨着哪边\nPointerAlignment: Right\n#缩进宽度\nIndentWidth: 4\n# OC block后面的缩进\nObjCBlockIndentWidth: 4\n#tab键盘的宽度\nTabWidth: 4\nStandard:        Cpp11\nUseTab:          Never\nCompactNamespaces: false\n# 命名空间的偏移\nNamespaceIndentation: Inner\n# 命名空间的末尾注释\nFixNamespaceComments: true\n# IndentPPDirectives: BeforeHash\n---\n# 语言: None Cpp Java ObjC Protp\nLanguage:      ObjC\n#LLVM Google\nBasedOnStyle:  LLVM\n# 访问说明符的偏移(public private)\nAccessModifierOffset: -4\n# 括号之后,水平对齐参数: Align DontAlign AlwaysBreak\nAlignAfterOpenBracket: Align\n# 连续的宏\n# AlignConsecutiveMacros: true\n# 连续的赋值时,对齐所有的等号\nAlignConsecutiveAssignments: true\n# 左对齐换行(使用反斜杠换行)的反斜杠\nAlignEscapedNewlines: Right\n# # 左对齐换行(使用反斜杠换行)的反斜杠\n# AlignEscapedNewlinesLeft: true\n# 水平对齐二元和三元表达式的操作数\nAlignOperands: true\n# 允许函数声明的所有参数在放在下一行\nAllowAllParametersOfDeclarationOnNextLine: false\n# AllowAllArgumentsOnNextLine: false\n# 允许短的块放在同一行\nAllowShortBlocksOnASingleLine : false\n# 允许短的case标签放在同一行\nAllowShortCaseLabelsOnASingleLine: false\n# 允许短的函数放在同一行: None, InlineOnly(定义在类中), Empty(空函数), Inline(定义在类中，空函数), All\nAllowShortFunctionsOnASingleLine: Empty\n# 是否允许短if单行 If true, if (a) return; 可以放到同一行\nAllowShortIfStatementsOnASingleLine: false\n# 允许短的循环保持在同一行\nAllowShortLoopsOnASingleLine: false\n# 总是在定义返回类型后换行(deprecated)\nAlwaysBreakAfterDefinitionReturnType: None\n# 每行字符的限制，0表示没有限制\nColumnLimit: 120\n# 描述具有特殊意义的注释的正则表达式，它不应该被分割为多行或以其它方式改变\nCommentPragmas: '^ IWYU pragma:'\n#指针的*的挨着哪边\nPointerAlignment: Right\n#缩进宽度\nIndentWidth: 4\n# OC block后面的缩进\nObjCBlockIndentWidth: 4\n#tab键盘的宽度\nTabWidth: 4\nStandard:        Cpp11\nUseTab:          Never\nCompactNamespaces: false\n# 命名空间的偏移\nNamespaceIndentation: Inner\n# 命名空间的末尾注释\nFixNamespaceComments: true\n# IndentPPDirectives: BeforeHash\n---\nLanguage: Proto\n#.proto文件不格式化\nDisableFormat: true\n...\n"
        },
        {
          "name": ".github",
          "type": "tree",
          "content": null
        },
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 8.1455078125,
          "content": ".DS_Store\n.vscode\nbuild\nbuild32\nbuild64\nrelease\ntags\n.idea/\ntools/onnx2tnn/onnx-converter/3rdparty/\nGPATH\nGRTAGS\nGTAGS\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/Global/JetBrains.gitignore\n\n# Covers JetBrains IDEs: IntelliJ, RubyMine, PhpStorm, AppCode, PyCharm, CLion, Android Studio, WebStorm and Rider\n# Reference: https://intellij-support.jetbrains.com/hc/en-us/articles/206544839\n\n# User-specific stuff\n.idea/**/workspace.xml\n.idea/**/tasks.xml\n.idea/**/usage.statistics.xml\n.idea/**/dictionaries\n.idea/**/shelf\n\n# Generated files\n.idea/**/contentModel.xml\n\n# Sensitive or high-churn files\n.idea/**/dataSources/\n.idea/**/dataSources.ids\n.idea/**/dataSources.local.xml\n.idea/**/sqlDataSources.xml\n.idea/**/dynamic.xml\n.idea/**/uiDesigner.xml\n.idea/**/dbnavigator.xml\n\n# Gradle\n.idea/**/gradle.xml\n.idea/**/libraries\n\n# Gradle and Maven with auto-import\n# When using Gradle or Maven with auto-import, you should exclude module files,\n# since they will be recreated, and may cause churn.  Uncomment if using\n# auto-import.\n# .idea/artifacts\n# .idea/compiler.xml\n# .idea/jarRepositories.xml\n# .idea/modules.xml\n# .idea/*.iml\n# .idea/modules\n# *.iml\n# *.ipr\n\n# CMake\ncmake-build-*/\n\n# Mongo Explorer plugin\n.idea/**/mongoSettings.xml\n\n# File-based project format\n*.iws\n\n# IntelliJ\nout/\n\n# mpeltonen/sbt-idea plugin\n.idea_modules/\n\n# JIRA plugin\natlassian-ide-plugin.xml\n\n# Cursive Clojure plugin\n.idea/replstate.xml\n\n# Crashlytics plugin (for Android Studio and IntelliJ)\ncom_crashlytics_export_strings.xml\ncrashlytics.properties\ncrashlytics-build.properties\nfabric.properties\n\n# Editor-based Rest Client\n.idea/httpRequests\n\n# Android studio 3.1+ serialized cache file\n.idea/caches/build_file_checksums.ser\n\n\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/Global/Emacs.gitignore\n\n# -*- mode: gitignore; -*-\n*~\n\\#*\\#\n/.emacs.desktop\n/.emacs.desktop.lock\n*.elc\nauto-save-list\ntramp\n.\\#*\n\n# Org-mode\n.org-id-locations\n*_archive\n\n# flymake-mode\n*_flymake.*\n\n# eshell files\n/eshell/history\n/eshell/lastdir\n\n# elpa packages\n/elpa/\n\n# reftex files\n*.rel\n\n# AUCTeX auto folder\n/auto/\n\n# cask packages\n.cask/\ndist/\n\n# Flycheck\nflycheck_*.el\n\n# server auth directory\n/server/\n\n# projectiles files\n.projectile\n\n# directory configuration\n.dir-locals.el\n\n# network security\n/network-security.data\n\n\n\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/Global/Vim.gitignore\n\n# Swap\n[._]*.s[a-v][a-z]\n!*.svg  # comment out if you don't need vector files\n[._]*.sw[a-p]\n[._]s[a-rt-v][a-z]\n[._]ss[a-gi-z]\n[._]sw[a-p]\n\n# Session\nSession.vim\nSessionx.vim\n\n# Temporary\n.netrwhist\n*~\n# Auto-generated tag files\ntags\n# Persistent undo\n[._]*.un~\n\n\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/Global/Linux.gitignore\n\n*~\n\n# temporary files which can be created if a process still has a handle open of a deleted file\n.fuse_hidden*\n\n# KDE directory preferences\n.directory\n\n# Linux trash folder which might appear on any partition or disk\n.Trash-*\n\n# .nfs files are created when an open file is removed but is still being accessed\n.nfs*\n\n\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/Global/macOS.gitignore\n\n# General\n.DS_Store\n.AppleDouble\n.LSOverride\n\n# Icon must end with two \\r\nIcon\n\n\n# Thumbnails\n._*\n\n# Files that might appear in the root of a volume\n.DocumentRevisions-V100\n.fseventsd\n.Spotlight-V100\n.TemporaryItems\n.Trashes\n.VolumeIcon.icns\n.com.apple.timemachine.donotpresent\n\n# Directories potentially created on remote AFP share\n.AppleDB\n.AppleDesktop\nNetwork Trash Folder\nTemporary Items\n.apdisk\n\n\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/CMake.gitignore\n\nCMakeLists.txt.user\nCMakeCache.txt\nCMakeFiles\nCMakeScripts\nTesting\nMakefile\ncmake_install.cmake\ninstall_manifest.txt\ncompile_commands.json\nCTestTestfile.cmake\n_deps\n\n\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/C++.gitignore\n\n# Prerequisites\n*.d\n\n# Compiled Object files\n*.slo\n*.lo\n*.o\n*.obj\n\n# Precompiled Headers\n*.gch\n*.pch\n\n# Compiled Dynamic libraries\n*.so\n*.dylib\n*.dll\n\n# Fortran module files\n*.mod\n*.smod\n\n# Compiled Static libraries\n*.lai\n*.la\n*.a\n*.lib\n\n# Executables\n*.exe\n*.out\n*.app\n\n\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/Python.gitignore\n\n# Byte-compiled / optimized / DLL files\n__pycache__/\n*.py[cod]\n*$py.class\n\n# C extensions\n*.so\n\n# Distribution / packaging\n.Python\nbuild/\ndevelop-eggs/\ndist/\ndownloads/\neggs/\n.eggs/\nlib/\nlib64/\nparts/\nsdist/\nvar/\nwheels/\nshare/python-wheels/\n*.egg-info/\n.installed.cfg\n*.egg\nMANIFEST\n\n# PyInstaller\n#  Usually these files are written by a python script from a template\n#  before PyInstaller builds the exe, so as to inject date/other infos into it.\n*.manifest\n*.spec\n\n# Installer logs\npip-log.txt\npip-delete-this-directory.txt\n\n# Unit test / coverage reports\nhtmlcov/\n.tox/\n.nox/\n.coverage\n.coverage.*\n.cache\nnosetests.xml\ncoverage.xml\n*.cover\n*.py,cover\n.hypothesis/\n.pytest_cache/\ncover/\n\n# Translations\n*.mo\n*.pot\n\n# Django stuff:\n*.log\nlocal_settings.py\ndb.sqlite3\ndb.sqlite3-journal\n\n# Flask stuff:\ninstance/\n.webassets-cache\n\n# Scrapy stuff:\n.scrapy\n\n# Sphinx documentation\ndocs/_build/\n\n# PyBuilder\n.pybuilder/\ntarget/\n\n# Jupyter Notebook\n.ipynb_checkpoints\n\n# IPython\nprofile_default/\nipython_config.py\n\n# pyenv\n#   For a library or package, you might want to ignore these files since the code is\n#   intended to run in multiple environments; otherwise, check them in:\n# .python-version\n\n# pipenv\n#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.\n#   However, in case of collaboration, if having platform-specific dependencies or dependencies\n#   having no cross-platform support, pipenv may install dependencies that don't work, or not\n#   install all needed dependencies.\n#Pipfile.lock\n\n# PEP 582; used by e.g. github.com/David-OConnor/pyflow\n__pypackages__/\n\n# Celery stuff\ncelerybeat-schedule\ncelerybeat.pid\n\n# SageMath parsed files\n*.sage.py\n\n# Environments\n.env\n.venv\nenv/\nvenv/\nENV/\nenv.bak/\nvenv.bak/\n\n# Spyder project settings\n.spyderproject\n.spyproject\n\n# Rope project settings\n.ropeproject\n\n# mkdocs documentation\n/site\n\n# mypy\n.mypy_cache/\n.dmypy.json\ndmypy.json\n\n# Pyre type checker\n.pyre/\n\n# pytype static type analyzer\n.pytype/\n\n# Cython debug symbols\ncython_debug/\n\n\n### https://raw.github.com/github/gitignore/eee21bf0c397cddc39ff1c94615d135e0ad36f8c/Android.gitignore\n\n# Built application files\n*.aar\n*.ap_\n*.aab\n\n# Files for the ART/Dalvik VM\n*.dex\n\n# Java class files\n*.class\n\n# Generated files\nbin/\ngen/\nout/\n#  Uncomment the following line in case you need and you don't have the release build type files in your app\n# release/\n\n# Gradle files\n.gradle/\nbuild/\n\n# Local configuration file (sdk path, etc)\nlocal.properties\n\n# Proguard folder generated by Eclipse\nproguard/\n\n# Log Files\n*.log\n\n# Android Studio Navigation editor temp files\n.navigation/\n\n# Android Studio captures folder\ncaptures/\n\n# IntelliJ\n*.iml\n.idea/workspace.xml\n.idea/tasks.xml\n.idea/gradle.xml\n.idea/assetWizardSettings.xml\n.idea/dictionaries\n.idea/libraries\n# Android Studio 3 in .gitignore file.\n.idea/caches\n.idea/modules.xml\n# Comment next line if keeping position of elements in Navigation Editor is relevant for you\n.idea/navEditor.xml\n\n# Keystore files\n# Uncomment the following lines if you do not want to check your keystore files in.\n#*.jks\n#*.keystore\n\n# External native build folder generated in Android Studio 2.2 and later\n.externalNativeBuild\n.cxx/\n\n# Google Services (e.g. APIs or Firebase)\n# google-services.json\n\n# Freeline\nfreeline.py\nfreeline/\nfreeline_project_description.json\n\n# fastlane\nfastlane/report.xml\nfastlane/Preview.html\nfastlane/screenshots\nfastlane/test_output\nfastlane/readme.md\n\n# Version control\nvcs.xml\n\n# lint\nlint/intermediates/\nlint/generated/\nlint/outputs/\nlint/tmp/\n# lint/reports/\n\nsource/tnn/network/tensorrt/thirdparty/TensorRT*\nmodel/\n# opencl generated code\nopencl_program.cc\n\n# opencl generated code\nopencl_program.cc\nplatforms/mac/tnn.xcodeproj/project.xcworkspace/xcuserdata/darrenyao.xcuserdatad/UserInterfaceState.xcuserstate\nplatforms/mac/tnn.xcodeproj/xcuserdata/darrenyao.xcuserdatad/xcschemes/xcschememanagement.plist\n"
        },
        {
          "name": ".travis.yml",
          "type": "blob",
          "size": 1.859375,
          "content": "sudo: false\n\ngit:\n  depth: 3\n  quiet: true\n  \naddons:\n  apt:\n    package:\n      - lcov\n\nmatrix:\n  include:\n    - name: \"Linux | Arm64 | build\"\n      os: linux\n      arch: arm64\n      before_install:\n        - ./scripts/.ci/preflight.sh arm || travis_terminate 0\n      script:\n        - ./scripts/build_aarch64_linux.sh\n\n    - name: \"Linux | Arm32 | build\"\n      os: linux\n      arch: arm64\n      before_install:\n        - ./scripts/.ci/preflight.sh arm || travis_terminate 0\n      before_script:\n        - sudo dpkg --add-architecture armhf\n        - sudo apt-get update\n        - sudo apt-get -y install crossbuild-essential-armhf libc6:armhf libstdc++-5-dev:armhf linux-libc-dev:armhf\n      script:\n        - ./scripts/build_armhf_linux.sh\n\n    - name: \"Linux | Arm64 | test\"\n      os: linux\n      compiler: clang\n      arch: arm64\n      before_install:\n        - ./scripts/.ci/preflight.sh arm || travis_terminate 0\n      script:\n        - travis_wait 40 ./scripts/build_test.sh\n\n    - name: \"Windows | x64 | build\"\n      os: windows\n      language: cpp\n      before_install:\n        - ./scripts/.ci/preflight.sh x86 || travis_terminate 0\n      install:\n        - PowerShell -Command 'Set-ExecutionPolicy -ExecutionPolicy RemoteSigned'\n        - choco install ninja\n      script:\n        - scripts/build_msvc_native.bat x64 ci\n      env:\n        - CXX=cl.exe\n        - CXX_FOR_BUILD=cl.exe\n        - CC=cl.exe\n        - CC_FOR_BUILD=cl.exe\n\n    - name: \"Windows | x86 | build\"\n      os: windows\n      language: cpp\n      before_install:\n        - ./scripts/.ci/preflight.sh x86 || travis_terminate 0\n      install:\n        - PowerShell -Command 'Set-ExecutionPolicy -ExecutionPolicy RemoteSigned'\n        - choco install ninja\n      script:\n        - scripts/build_msvc_native.bat x86 ci\n      env:\n        - CXX=cl.exe\n        - CXX_FOR_BUILD=cl.exe\n        - CC=cl.exe\n        - CC_FOR_BUILD=cl.exe\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 16.7255859375,
          "content": "cmake_minimum_required(VERSION 3.1)\n\n# https://cmake.org/cmake/help/latest/policy/CMP0068.html\nif(POLICY CMP0068)\n    cmake_policy(SET CMP0068 NEW)\nendif()\n\n# https://cmake.org/cmake/help/latest/policy/CMP0077.html\nif (POLICY CMP0077)\n    cmake_policy(SET CMP0077 NEW)\nendif ()\n\nproject(TNN)\n\nENABLE_LANGUAGE(ASM)\n\nset(TNN_MAJOR_VERSION 0)\nset(TNN_MINOR_VERSION 3)\nset(TNN_PATCH_VERSION 0)\nset(TNN_BUILD_VERSION 0)\nset(TNN_VERSION \"${TNN_MAJOR_VERSION}.${TNN_MINOR_VERSION}.${TNN_PATCH_VERSION}.${TNN_BUILD_VERSION}\")\n\noption(TNN_CPU_ENABLE \"Enable Cpu\" ON)\noption(TNN_X86_ENABLE  \"Enable X86\" OFF)\noption(TNN_ARM_ENABLE \"Enable Arm\" OFF)\noption(TNN_ARM82_ENABLE \"Enable Arm82\" OFF)\noption(TNN_METAL_ENABLE \"Enable Metal\" OFF)\noption(TNN_OPENCL_ENABLE \"Enable OpenCL\" OFF)\noption(TNN_CUDA_ENABLE \"Enable CUDA\" OFF)\noption(TNN_DSP_ENABLE \"Enable DSP\" OFF)\noption(TNN_ATLAS_ENABLE \"Enable Atlas\" OFF)\noption(TNN_TENSORRT_ENABLE \"Enable TensorRT\" OFF)\noption(TNN_OPENVINO_ENABLE  \"Enable OPENVINO\" OFF)\noption(TNN_APPLE_NPU_ENABLE \"Enable NPU\" OFF)\noption(TNN_HUAWEI_NPU_ENABLE \"Enable NPU\" OFF)\noption(TNN_RK_NPU_ENABLE \"Enable RKNPU\" OFF)\noption(TNN_JETSON_NANO_ENABLE \"Enable Jetson Nano\" OFF)\noption(TNN_SYMBOL_HIDE \"Enable Hide Symbol Visibility\" ON)\noption(TNN_OPENMP_ENABLE \"Enable OpenMP\" OFF)\noption(TNN_BUILD_SHARED \"Build Shared Library\" ON)\noption(TNN_OPENVINO_BUILD_SHARED \"Build Shared Openvino Library\" OFF)\noption(TNN_TEST_ENABLE \"Enable Test\" OFF)\noption(TNN_UNIT_TEST_ENABLE \"Enable Test\" OFF)\noption(TNN_PROFILER_ENABLE \"Enable Profiler\" OFF)\noption(TNN_QUANTIZATION_ENABLE \"Enable Quantization\" OFF)\noption(TNN_EVALUATION_ENABLE \"Enable Evaluation\" OFF)\noption(TNN_MODEL_CHECK_ENABLE \"Enable Model Check\" OFF)\noption(TNN_BENCHMARK_MODE \"Enable Benchmark\" OFF)\noption(TNN_UNIT_TEST_BENCHMARK \"Enable Benchmark Layer\" OFF)\noption(TNN_CONVERTER_ENABLE \"Enable Model Converter\" OFF)\noption(TNN_ONNX2TNN_ENABLE \"Enable ONNX2TNN Converter\" OFF)\noption(TNN_TNN2MEM_ENABLE \"Enable tnn2mem\" OFF)\noption(TNN_BUILD_BENCHMARK_TEST_LIB_ENABLE \"Enable Build Benchmark Test Lib\" OFF)\noption(TNN_GLIBCXX_USE_CXX11_ABI_ENABLE \"Enable Use CXX11 ABI\" ON)\noption(TNN_METAL_FLOAT32 \"Enable Metal Float32\" OFF)\noption(TNN_COREML_FLOAT32 \"Enable Float32 CoreML Model\" ON)\noption(TNN_DYNAMIC_RANGE_QUANTIZATION_ENABLE \"Enable Dynamic Range Quantization\" OFF)\n\nset(TNN_USE_GFLAGS OFF)\n\nmessage(${CMAKE_SOURCE_DIR})\nmessage(${CMAKE_CURRENT_SOURCE_DIR})\n\ninclude(cmake/macros.cmake)\n\nif (SYSTEM.Windows)\n    add_definitions(-DBUILDING_DLL)\nendif()\n\n# if((SYSTEM.iOS OR SYSTEM.Darwin) AND (TNN_METAL_ENABLE OR TNN_APPLE_NPU_ENABLE))\n#    ENABLE_LANGUAGE(OBJCXX)\n# endif()\n\nif(TNN_PROFILER_ENABLE)\n    add_definitions(-DTNN_PROFILE)\n    set(TNN_SYMBOL_HIDE OFF)\nendif()\n\nif(TNN_BENCHMARK_MODE)\n    add_definitions(-DGENERATE_RESOURCE)\nendif()\n\nif(MSVC)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /wd4003 /wd4819 /wd4244 /wd4018 /utf-8\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /wd4003 /wd4819 /wd4244 /wd4018 /utf-8\")\nendif()\n\n# ignore loop-vectorize warning\nif(SYSTEM.Windows)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS}\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\nelse()\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-pass-failed\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-pass-failed\")\nendif()\n\n# ignore deprecated warning\nif(SYSTEM.Windows)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS}\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS}\")\nelse()\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -Wno-deprecated-declarations -Wno-ignored-attributes\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -Wno-deprecated-declarations -Wno-ignored-attributes\")\nendif()\n\nif(DEBUG)\n    set(TNN_SYMBOL_HIDE OFF)\n    add_definitions(-DDEBUG)\n    if (NOT CMAKE_BUILD_TYPE OR CMAKE_BUILD_TYPE STREQUAL \"\")\n        set(CMAKE_BUILD_TYPE \"Debug\" CACHE STRING \"set build type to debug\" FORCE)\n    endif()\nelse()\n    if (NOT CMAKE_BUILD_TYPE OR CMAKE_BUILD_TYPE STREQUAL \"\")\n        set(CMAKE_BUILD_TYPE \"Release\" CACHE STRING \"set build type to release\" FORCE)\n    endif()\n    if(BUILD_FOR_ANDROID_COMMAND)\n        set(CMAKE_SHARED_LINKER_FLAGS \"${CMAKE_SHARED_LINKER_FLAGS} -s -Wl,--gc-sections\")\n    endif()\nendif()\n\nif(TNN_TEST_ENABLE)\n    set(TNN_METAL_FLOAT32 ON)\nelse()\n    set(TNN_UNIT_TEST_ENABLE OFF)\nendif()\n\nif(TNN_UNIT_TEST_ENABLE)\n    enable_testing()\n    set(TNN_CPU_ENABLE ON)\n    set(TNN_SYMBOL_HIDE OFF)\n    add_definitions(-DGENERATE_RESOURCE)\nendif()\n\nif(TNN_CONVERTER_ENABLE)\n    set(TNN_ONNX2TNN_ENABLE ON)\nendif()\n\nif(TNN_CONVERTER_ENABLE OR TNN_ONNX2TNN_ENABLE)\n    set(TNN_SYMBOL_HIDE OFF)\n    add_definitions(-DTNN_CONVERTER_RUNTIME)\nendif()\n\nif(TNN_QUANTIZATION_ENABLE OR TNN_MODEL_CHECK_ENABLE)\n    set(TNN_SYMBOL_HIDE OFF)\n    add_definitions(-DFORWARD_CALLBACK_ENABLE)\nendif()\n\nif (TNN_DYNAMIC_RANGE_QUANTIZATION_ENABLE)\n    set(TNN_SYMBOL_HIDE OFF)\nendif()\n\nif(TNN_QUANTIZATION_ENABLE OR TNN_UNIT_TEST_ENABLE)\n    add_definitions(-DGET_INTERP_ENABLE)\nendif()\n\nif(TNN_MODEL_CHECK_ENABLE)\n    set(TNN_METAL_FLOAT32 ON)\nendif()\n\nif(TNN_ARM82_ENABLE)\n    add_definitions(-DTNN_ARM82=1)\nendif()\n\n# only used to simulate arm82 computation in the unit test\noption(TNN_ARM82_SIMU \"Enable arm82 simulation\" OFF)\nif(TNN_ARM82_SIMU)\n    add_definitions(-DTNN_ARM82_SIMU)\nendif()\n\nif(TNN_COREML_FLOAT32)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DTNN_COREML_FULL_PRECISION=1\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DTNN_COREML_FULL_PRECISION=1\")\nelse()\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DTNN_COREML_FULL_PRECISION=0\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DTNN_COREML_FULL_PRECISION=0\")\nendif()\n\nif(TNN_METAL_FLOAT32)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DTNN_METAL_FULL_PRECISION=1\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DTNN_METAL_FULL_PRECISION=1\")\n\n    if(TNN_PROFILER_ENABLE OR TNN_MODEL_CHECK_ENABLE)\n      set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -DTNN_METAL_BENCHMARK=1 -DTNN_METAL_DEBUG=1\")\n      set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -DTNN_METAL_BENCHMARK=1 -DTNN_METAL_DEBUG=1\")\n    endif()\nendif()\n\nif(TNN_OPENMP_ENABLE)\n    FIND_PACKAGE(OpenMP REQUIRED)\n    if(OPENMP_FOUND)\n        if(MSVC)\n            set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} /openmp\")\n            set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} /openmp\")\n        else()\n            set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} ${OpenMP_C_FLAGS}\")\n            set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} ${OpenMP_CXX_FLAGS}\")\n            include_directories(${OpenMP_C_INCLUDE_DIRS} ${OpenMP_CXX_INCLUDE_DIRS})\n            if(${ANDROID_NDK_MAJOR})\n                if(${ANDROID_NDK_MAJOR} GREATER 20)\n\n                else()\n                    link_libraries(${OpenMP_C_LIBRARIES} ${OpenMP_CXX_LIBRARIES})\n                endif()\n            else()\n                link_libraries(${OpenMP_C_LIBRARIES} ${OpenMP_CXX_LIBRARIES})\n            endif()\n        endif()\n    else()\n        error(\"OpenMP Not Found.\")\n    endif()\nendif()\n\n\nif(UNIX)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -pthread\")\n    if(TNN_GLIBCXX_USE_CXX11_ABI_ENABLE)\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=1\")\n    else()\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_CXX11_ABI=0\")\n    endif()\nendif()\n\nset(CMAKE_CXX_STANDARD 11)\nset(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\nif(TNN_METAL_ENABLE OR TNN_APPLE_NPU_ENABLE)\n    if(TNN_MODEL_CHECK_ENABLE OR TNN_UNIT_TEST_ENABLE)\n        add_definitions(-DTNN_COREML_TEST=1)\n    endif()\n\n    #compile the file according to file type\n    #add_compile_options(-x objective-c++)\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fobjc-arc -Wno-shorten-64-to-32\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fobjc-arc -Wno-shorten-64-to-32 -Wno-null-character\")\n    set(CMAKE_OBJCXX_FLAGS \"${CMAKE_OBJCXX_FLAGS} -x objective-c++ -fobjc-arc -Wno-shorten-64-to-32 -Wno-null-character\")\nendif()\n\nif(TNN_TNN2MEM_ENABLE)\n    add_subdirectory(tools/tnn2mem)\nendif()\n\nif(TNN_JETSON_NANO_ENABLE)\n    set(TNN_CUDA_ENABLE ON)\n    set(TNN_OPENMP_ENABLE ON)\n    set(TNN_TENSORRT_ENABLE ON)\nendif()\n\nmessage(STATUS \">>>>>>>>>>>>>\")\nmessage(STATUS \"TNN BUILD INFO:\")\nmessage(STATUS \"\\tSystem: ${CMAKE_SYSTEM_NAME}\")\nmessage(STATUS \"\\tProcessor: ${CMAKE_SYSTEM_PROCESSOR}\")\nmessage(STATUS \"\\tCpu:\\t${TNN_CPU_ENABLE}\")\nmessage(STATUS \"\\tX86:\\t${TNN_X86_ENABLE}\")\nmessage(STATUS \"\\tArm:\\t${TNN_ARM_ENABLE}\")\nmessage(STATUS \"\\tArm82:\\t${TNN_ARM82_ENABLE}\")\nmessage(STATUS \"\\tMetal:\\t${TNN_METAL_ENABLE}\")\nmessage(STATUS \"\\tOpenCL:\\t${TNN_OPENCL_ENABLE}\")\nmessage(STATUS \"\\tCUDA:\\t${TNN_CUDA_ENABLE}\")\nmessage(STATUS \"\\tDSP:\\t${TNN_DSP_ENABLE}\")\nmessage(STATUS \"\\tAtlas:\\t${TNN_ATLAS_ENABLE}\")\nmessage(STATUS \"\\tTensorRT:\\t${TNN_TENSORRT_ENABLE}\")\nmessage(STATUS \"\\tAppleNPU:\\t${TNN_APPLE_NPU_ENABLE}\")\nmessage(STATUS \"\\tHuaweiNPU:\\t${TNN_HUAWEI_NPU_ENABLE}\")\nmessage(STATUS \"\\tRKNPU:\\t${TNN_RK_NPU_ENABLE}\")\nmessage(STATUS \"\\tJetson Nano:\\t${TNN_JETSON_NANO_ENABLE}\")\nmessage(STATUS \"\\tOpenVINO:\\t${TNN_OPENVINO_ENABLE}\")\nmessage(STATUS \"\\tOpenMP:\\t${TNN_OPENMP_ENABLE}\")\nmessage(STATUS \"\\tTEST:\\t${TNN_TEST_ENABLE}\")\nmessage(STATUS \"\\t--Unit Test:\\t${TNN_UNIT_TEST_ENABLE}\")\nmessage(STATUS \"\\tQuantization:\\t${TNN_QUANTIZATION_ENABLE}\")\nmessage(STATUS \"\\tModelCheck:\\t${TNN_MODEL_CHECK_ENABLE}\")\nmessage(STATUS \"\\tDEBUG:\\t${DEBUG}\")\nmessage(STATUS \"\\tPROFILE:\\t${TNN_PROFILER_ENABLE}\")\nmessage(STATUS \"\\tBENCHMARK:\\t${TNN_BENCHMARK_MODE}\")\nmessage(STATUS \"\\tBENCHMARK Layer:\\t${TNN_UNIT_TEST_BENCHMARK}\")\nmessage(STATUS \"\\tModel Converter:\\t${TNN_CONVERTER_ENABLE}\")\nmessage(STATUS \"\\tONNX2TNN Converter:\\t${TNN_ONNX2TNN_ENABLE}\")\nmessage(STATUS \"\\tTNN2MEM:\\t${TNN_TNN2MEM_ENABLE}\")\nmessage(STATUS \"\\tBENCHMARK Test Lib:\\t${TNN_BUILD_BENCHMARK_TEST_LIB_ENABLE}\")\nmessage(STATUS \"\\tDynamic Range Quantization:\\t${TNN_DYNAMIC_RANGE_QUANTIZATION_ENABLE}\")\nmessage(STATUS \"\\tSHARING_MEM_WITH_OPENGL:\\t${SHARING_MEM_WITH_OPENGL}\")\n\ninclude_directories(include)\ninclude_directories(source)\n\nfile(GLOB_RECURSE SRC \"source/tnn/core/*.h\"\n                      \"source/tnn/core/*.cc\"\n                      \"source/tnn/layer/*.h\"\n                      \"source/tnn/layer/*.cc\"\n                      \"source/tnn/utils/*.h\"\n                      \"source/tnn/utils/*.cc\"\n                      \"source/tnn/interpreter/*.h\"\n                      \"source/tnn/interpreter/*.cc\"\n                      \"source/tnn/optimizer/*.h\"\n                      \"source/tnn/optimizer/*.cc\"\n                      \"source/tnn/extern_wrapper/*.h\"\n                      \"source/tnn/extern_wrapper/*.cc\"\n                      \"source/tnn/memory_manager/*.h\"\n                      \"source/tnn/memory_manager/*.cc\")\n\nstring(LENGTH ${CMAKE_SOURCE_DIR} CMAKE_SOURCE_DIR_LEN)\nadd_definitions(-D_SOURCE_DIR_LEN=${CMAKE_SOURCE_DIR_LEN})\n\nif(TNN_SYMBOL_HIDE AND UNIX)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fvisibility=hidden -fvisibility-inlines-hidden\")\nendif()\n\nif(SYSTEM.Linux AND CMAKE_SYSTEM_PROCESSOR MATCHES \"arm\" AND ANDROID_API_LEVAL)\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -D_GLIBCXX_USE_C99_MATH_TR1\")\n    add_definitions(-D__ANDROID_API__=${ANDROID_API_LEVAL})\n    add_definitions( -mfloat-abi=softfp )\nendif()\n\nif(SYSTEM.Windows AND TNN_DYNAMIC_RANGE_QUANTIZATION_ENABLE)\n    set(TNN_BUILD_SHARED OFF)\nendif()\n\nif(TNN_X86_ENABLE)\n    # compile with avx2 by default\n    option(TNN_X86_AVX2_ENABLE  \"Enable X86 AVX2\" ON)\n    add_subdirectory(source/tnn/device/x86)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNX86>\")\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNX86ACC>\")\nendif()\n\nif(TNN_CPU_ENABLE)\n    add_subdirectory(source/tnn/device/cpu)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNCpu>\")\nendif()\n\nif(TNN_ARM_ENABLE)\n    if(CMAKE_SYSTEM_PROCESSOR MATCHES \"aarch64\" OR CMAKE_SYSTEM_PROCESSOR STREQUAL \"arm64\")\n\n    elseif(CMAKE_SYSTEM_PROCESSOR MATCHES \"arm\")\n        add_definitions( -mfpu=neon )\n    endif()\n    add_subdirectory(source/tnn/device/arm)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNArm>\")\n    if(TNN_ARM82_ENABLE)\n        set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNArm82>\")\n    endif()\nendif()\n\nif(TNN_OPENVINO_ENABLE)\n    add_subdirectory(source/tnn/network/openvino)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNOpenVINO>\")\nendif()\n\nif(TNN_OPENCL_ENABLE)\n    include(FindPythonInterp REQUIRED)\n    if (NOT PYTHON_EXECUTABLE)\n        message (FATAL_ERROR \"No Python installation found! It is required by OpenCL codegen.\")\n    endif ()\n\n    if (TNN_OPENCL_PREFER_GPU_TYPE)\n        message(\"TNN_OPENCL_PREFER_GPU_TYPE: \" ${TNN_OPENCL_PREFER_GPU_TYPE})\n        add_definitions(-DTNN_OPENCL_PREFER_GPU_TYPE=${TNN_OPENCL_PREFER_GPU_TYPE})\n    endif()\n\n    if(SHARING_MEM_WITH_OPENGL)\n        add_definitions(-DSHARING_MEM_WITH_OPENGL)\n        add_definitions(-DCL_HPP_TARGET_OPENCL_VERSION=120)\n    endif()\n    add_subdirectory(source/tnn/device/opencl)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNOpenCL>\")\nendif()\n\nif(TNN_METAL_ENABLE)\n    add_subdirectory(source/tnn/device/metal)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNMetal>\")\nendif()\n\nif(TNN_APPLE_NPU_ENABLE)\n    add_subdirectory(source/tnn/network/coreml)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNAppleNPU>\")\nendif()\n\nif(TNN_CUDA_ENABLE)\n    set(CMAKE_CUDA_FLAGS \"${CMAKE_CUDA_FLAGS} --expt-relaxed-constexpr\")\n    add_subdirectory(source/tnn/device/cuda)\n    if(TNN_TENSORRT_ENABLE)\n        add_subdirectory(source/tnn/network/tensorrt)\n        set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNTensorRT>\")\n    endif()\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNCuda>\")\nendif()\n\nif(TNN_HUAWEI_NPU_ENABLE)\n    if(ANDROID_ABI STREQUAL \"armeabi-v7a\")\n        link_directories(\n                third_party/huawei_npu/hiai_ddk_latest/armeabi-v7a/\n        )\n    else()\n        link_directories(\n                third_party/huawei_npu/hiai_ddk_latest/arm64-v8a/\n        )\n    endif()\n    add_subdirectory(source/tnn/device/huawei_npu)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNNPU>\")\nendif()\n\nif(TNN_RK_NPU_ENABLE)\n    if(CMAKE_SIZEOF_VOID_P EQUAL 8)\n        link_directories(\n                ./third_party/rknpu/rknpu_ddk/lib64/\n        )\n    else()\n        link_directories(\n                ./third_party/rknpu/rknpu_ddk/lib/\n        )\n    endif()\n    add_subdirectory(source/tnn/device/rknpu)\n    set(TARGET_OBJECTS ${TARGET_OBJECTS} \"$<TARGET_OBJECTS:TNNRKNPU>\")\nendif()\n\nif(TNN_BUILD_SHARED)\n    add_library(TNN SHARED ${SRC} ${TARGET_OBJECTS})\n    set_target_properties(TNN PROPERTIES VERSION ${TNN_VERSION} SOVERSION ${TNN_MAJOR_VERSION})\n    if(SHARING_MEM_WITH_OPENGL)\n        if(SYSTEM.Windows)\n            target_link_libraries(TNN opengl32)\n        else()\n        target_link_libraries(TNN -lEGL -lGLESv2)\n        endif()\n    endif()\nelse()\n    add_library(TNN STATIC ${SRC} ${TARGET_OBJECTS})\n    set_target_properties(TNN PROPERTIES VERSION ${TNN_VERSION})\n    if(SHARING_MEM_WITH_OPENGL)\n        if(SYSTEM.Windows)\n            target_link_libraries(TNN opengl32)\n        else()\n        target_link_libraries(TNN -lEGL -lGLESv2)\n        endif()\n    endif()\nendif()\ntarget_include_directories(TNN PUBLIC ${CMAKE_CURRENT_SOURCE_DIR}/include)\n\nif(TNN_QUANTIZATION_ENABLE)\n    add_subdirectory(tools/quantization)\nendif()\n\nif(SYSTEM.Linux)\n    if(TNN_JETSON_NANO_ENABLE)\n         include(platforms/jetson/CMakeLists.txt)\n    else()\n         include(platforms/linux/CMakeLists.txt)\n    endif()\nelseif(SYSTEM.Android)\n    include(platforms/android/CMakeLists.txt)\nelseif(SYSTEM.iOS)\n    include(platforms/ios/CMakeLists.txt)\nelseif(SYSTEM.Darwin)\n    include(platforms/mac/CMakeLists.txt)\nelseif(SYSTEM.Windows)\n    include(platforms/windows/CMakeLists.txt)\nendif()\n\nif (TNN_TEST_ENABLE OR TNN_CONVERTER_ENABLE OR TNN_MODEL_CHECK_ENABLE OR TNN_DYNAMIC_RANGE_QUANTIZATION_ENABLE)\n    set(TNN_USE_GFLAGS ON)\nendif ()\n\nif (TNN_USE_GFLAGS)\n    add_subdirectory(third_party/gflags)\n    get_target_property(GFLAGS_INCLUDE_DIRS gflags INTERFACE_INCLUDE_DIRECTORIES)\n    include_directories(BEFORE \"${GFLAGS_INCLUDE_DIRS}\")\nendif ()\n\nif(TNN_MODEL_CHECK_ENABLE)\n    add_subdirectory(tools/model_check)\nendif()\n\nif(TNN_TEST_ENABLE)\n    add_subdirectory(test)\nendif()\n\nif(TNN_CONVERTER_ENABLE)\n    add_subdirectory(third_party/flatbuffers)\n    add_subdirectory(tools/converter)\nendif()\n\nif(TNN_ONNX2TNN_ENABLE)\n    add_subdirectory(tools/onnx2tnn/onnx-converter)\nendif()\n\nif(TNN_COVERAGE)\n    if (CMAKE_CXX_COMPILER_ID STREQUAL \"Clang\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fprofile-instr-generate -fcoverage-mapping\")\n    elseif (CMAKE_CXX_COMPILER_ID STREQUAL \"GNU\")\n        set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -coverage -fprofile-arcs -ftest-coverage\")\n        set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -coverage -lgcov\")\n    endif()\nendif()\n\nif(TNN_EVALUATION_ENABLE)\n    add_subdirectory(tools/evaluation)\nendif()\n\nif(TNN_DYNAMIC_RANGE_QUANTIZATION_ENABLE)\n    add_subdirectory(tools/dynamic_range_quantization)\nendif()\n\nif (MSVC)\n    target_compile_options(TNN PUBLIC \"/Zc:__cplusplus\")\n    target_compile_features(TNN PUBLIC cxx_std_11)\nendif()\n"
        },
        {
          "name": "Dockerfile",
          "type": "blob",
          "size": 1.1826171875,
          "content": "FROM ubuntu:18.04\n\nENV LANG C.UTF-8\nENV LANGUAGE C.UTF-8\nENV LC_ALL C.UTF-8\n\nRUN sed -i s@/archive.ubuntu.com/@/mirrors.tencent.com/@g /etc/apt/sources.list\nRUN sed -i s@/security.ubuntu.com/@/mirrors.tencent.com/@g /etc/apt/sources.list\n\nRUN apt-get clean && apt-get update -y && apt-get -y install --no-install-recommends apt-utils\n\nRUN apt-get -y install git cmake make\n\nRUN apt-get -y install gcc g++\n\nRUN apt-get -y install protobuf-compiler libprotobuf-dev\n\nRUN apt-get -y install python3 python3-dev python3-pip\n\n#RUN mkdir -p  /root/.pip && echo \"[global]\\n index-url = https://mirrors.tencent.com/pypi/simple/\" >> /root/.pip/pip.conf\n\nRUN python3 -m pip install --upgrade pip && pip3 install -U onnx==1.6.0 onnxruntime numpy onnx-simplifier setuptools protobuf\n\n\nRUN pip3 install tensorflow==1.15.0 tf2onnx\n\nENV TNN_ROOT=/opt/TNN\nENV TOOLS_ROOT=$TNN_ROOT/tools\n# COPY ./onnx2tnn $TOOLS_ROOT/onnx2tnn\n# COPY ./caffe2onnx $TOOLS_ROOT/caffe2onnx\n# COPY ./convert2tnn $TOOLS_ROOT/convert2tnn\nCOPY . $TNN_ROOT/\n#RUN cd $TOOLS_ROOT/onnx2tnn/onnx-converter && ./build.sh\nRUN cd $TOOLS_ROOT/convert2tnn && bash ./build.sh\n\n\nRUN python3 $TOOLS_ROOT/convert2tnn/converter.py -h\n\nWORKDIR $TOOLS_ROOT/convert2tnn/\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.869140625,
          "content": "﻿Tencent is pleased to support the open source community by making TNN available.  \r\n\r\nCopyright (C) 2020 THL A29 Limited, a Tencent company.  All rights reserved.\r\n\r\nTNN is licensed under the BSD 3-Clause License, except for the third-party components listed below. \r\n \r\nTerms of the BSD 3-Clause License:\r\n--------------------------------------------------------------------\r\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\r\n\r\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\r\n\r\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\r\n\r\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\r\n\r\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n\r\n\r\nThe below software in this distribution may have been modified by THL A29 Limited (“Tencent Modifications”). All Tencent Modifications are Copyright (C) 2020 THL A29 Limited.\r\n----------------------------------------------------------------------------------------\r\n1. lexer from Pytorch:\r\nCopyright (c) 2016-     Facebook, Inc            (Adam Paszke)\r\nCopyright (c) 2014-     Facebook, Inc            (Soumith Chintala)\r\nCopyright (c) 2011-2014 Idiap Research Institute (Ronan Collobert)\r\nCopyright (c) 2012-2014 Deepmind Technologies    (Koray Kavukcuoglu)\r\nCopyright (c) 2011-2012 NEC Laboratories America (Koray Kavukcuoglu)\r\nCopyright (c) 2011-2013 NYU                      (Clement Farabet)\r\nCopyright (c) 2006-2010 NEC Laboratories America (Ronan Collobert, Leon Bottou, Iain Melvin, Jason Weston)\r\nCopyright (c) 2006      Idiap Research Institute (Samy Bengio)\r\nCopyright (c) 2001-2004 Idiap Research Institute (Ronan Collobert, Samy Bengio, Johnny Mariethoz)\r\nAll rights reserved.\r\n\r\n\r\nOther dependencies and licenses:\r\n\r\nOpen Source Software Licensed under the BSD 3-Clause License:\r\n----------------------------------------------------------------------------------------\r\n1. GoogleTest\r\nCopyright 2008, Google Inc.\r\n\r\n2. Gflags\r\nCopyright (c) 2006, Google Inc.\r\n\r\n\r\nTerms of the BSD 3-Clause License:\r\n--------------------------------------------------------------------\r\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\r\n\r\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\r\n\r\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and/or other materials provided with the distribution.\r\n\r\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\r\n\r\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\r\n\r\n\r\nOpen Source Software Licensed under the MIT License:\r\n----------------------------------------------------------------------------------------\r\n1. OpenCL-Headers\r\nCopyright (c) 2008-2015 The Khronos Group Inc.\r\n\r\n2. stb_image\r\nCopyright (c) 2017 Sean Barrett\r\n\r\n\r\nTerms of the  MIT License:\r\n--------------------------------------------------------------------\r\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\r\n　　\r\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\r\n\r\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\r\n\r\n\r\n\r\n                                \r\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 17.896484375,
          "content": "[中文版本](README_CH.md)\n<div align=left ><img src=\"https://github.com/darrenyao87/tnn-models/raw/master/TNN.png\"/>\n\n## Introduction\n\nTNN: A high-performance, lightweight neural network inference framework open sourced by Tencent Youtu Lab. It also has many outstanding advantages such as cross-platform, high performance, model compression, and code tailoring. The TNN framework further strengthens the support and performance optimization of mobile devices on the basis of the original Rapidnet and ncnn frameworks. At the same time, it refers to the high performance and good scalability characteristics of the industry's mainstream open source frameworks, and expands the support for X86 and NV GPUs. On the mobile phone, TNN has been used by many applications such as mobile QQ, weishi, and Pitu. As a basic acceleration framework for Tencent Cloud AI, TNN has provided acceleration support for the implementation of many businesses. Everyone is welcome to participate in the collaborative construction to promote the further improvement of the TNN inference framework.\n\n## Effect Example\n\nFace Detection(blazeface)   |  Face Alignment<br>(from Tencent Youtu Lab)  |   Hair Segmentation<br>(from Tencent Guangying Lab)\n:-------------------------: | :------: | :------:\n[![face_detection](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/face_detection.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/blazeface) <br> model link: [tflite](https://github.com/google/mediapipe/blob/master/mediapipe/models/face_detection_front.tflite) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/blazeface) | [![youtu_face_alignment](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/face_alignment.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/youtu_face_alignment) <br> model link: [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/youtu_face_alignment) | [![hair_segmentation](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/hair_seg_red.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/hair_segmentation) <br> model link: [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/hair_segmentation)\n\nPose Estimation<br>(from Tencent Guangliu)   |   Pose Estimation<br>(blazepose)    |   Chinese OCR\n:--------------------------: | :------: | :------: \n[![skeleton](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/skeleton_guangliu.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/skeleton) <br> model link: [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/skeleton) | [![blazepose](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/skeleton_blazepose.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/blazepose) <br> model link: [tflite](https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_full.tflite) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/blazepose) | [![chinese-ocr](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/chinese-ocr.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/chinese-ocr) <br> model link: [onnx](https://github.com/DayBreak-u/chineseocr_lite/tree/onnx/models) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/chinese-ocr)\n\nObject Detection(yolov5s)    |   Object Detection(MobilenetV2-SSD)    |  Reading Comprehension\n:-------------------------: |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:| :------: \n[![yolov5](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/object-detection.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/yolov5) <br> model link: [onnx](https://github.com/ultralytics/yolov5/blob/master/export.py) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/yolov5) | [![mobilenetv2_ssd](tutorial/mobilenet_v2_ssd/imgs/mobilenetv2_ssd_tf_fix_box.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/mobilenet_v2-ssd) <br> model link: [tensorflow](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/mobilenet_v2-ssd) |  [![bertsquad10](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/bert_squad.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/bertsquad10) <br> model link: [onnx](https://github.com/onnx/models/blob/master/text/machine_comprehension/bert-squad/model/bertsquad-10.onnx) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/bertsquad10)\n\n<small>Chinese OCR demo is the TNN implementation of [chineseocr_lite](https://github.com/DayBreak-u/chineseocr_lite) project. It is lightweight and supports tilted, rotated and vertical text recognition.</small>\n\nThe support for each demo is shown in the following table. You can click the ✅ and find the entrance code for each demo.\n\ndemo                                                                                      |   ARM    |  OpenCL  |   Metal  | Huawei NPU | Apple NPU |    X86   |    CUDA   \n:---------------------------------------------------------------------------------------: | :------: | :------: | :------: | :------: | :------: | :------: |:------:\n[Face Detection (blazeface)](https://github.com/Tencent/TNN/blob/master/examples/base/blazeface_detector.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamBlazeFaceDetector) | [✅ ](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNBlazeFaceDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamBlazeFaceDetector) | [✅ ](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNBlazeFaceDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNFaceDetector/TNNFaceDetector.cc) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNFaceDetector/TNNFaceDetector.cc)\n[Object Detection (yolov5s)](https://github.com/Tencent/TNN/blob/master/examples/base/object_detector_yolo.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamObjectDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNYoloObjectDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamObjectDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNYoloObjectDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNObjectDetector/TNNObjectDetector.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNObjectDetector/TNNObjectDetector.cc)\n[Face Alignment](https://github.com/Tencent/TNN/blob/master/examples/base/face_detect_aligner.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamBlazeFaceAlign) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNFaceDetectAlignerViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamBlazeFaceAlign) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNFaceDetectAlignerViewModel.mm) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNFaceAligner/TNNFaceAligner.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNFaceAligner/TNNFaceAligner.cc)\n[Hair Segmentation](https://github.com/Tencent/TNN/blob/master/examples/base/hair_segmentation.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamHairSegmentation) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNHairSegmentationViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamHairSegmentation) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNHairSegmentationViewModel.mm)| [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNHairSegmentation/TNNHairSegmentation.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNHairSegmentation/TNNHairSegmentation.cc) \n[Pose Estimation<br>(from Tencent Guangliu)](https://github.com/Tencent/TNN/blob/master/examples/base/skeleton_detector.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamSkeletonDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNSkeletonDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamSkeletonDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNSkeletonDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNSkeletonDetector/TNNSkeletonDetector.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNSkeletonDetector/TNNSkeletonDetector.cc)\n[Pose Estimation (blazepose)](https://github.com/Tencent/TNN/blob/master/examples/base/pose_detect_landmark.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamPoseDetectLandmark) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNPoseDetectLandmarkViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamPoseDetectLandmark) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNPoseDetectLandmarkViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNBlazePose/TNNBlazePose.cc) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNBlazePose/TNNBlazePose.cc) \n[Chinese OCR](https://github.com/Tencent/TNN/blob/master/examples/base/ocr_driver.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamOCRDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNOCRViewModel.mm) | | | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNOcrDetector/TNNOcrDetector.cc) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNOcrDetector/TNNOcrDetector.cc)\n[Reading Comprehension](https://github.com/Tencent/TNN/blob/master/examples/base/bert_tokenizer.cc)  | | | | | | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/BertReadingComprehension/BertReadingComprehension.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/BertReadingComprehension/BertReadingComprehension.cc)\n\n## Quick Start\n\nIt is very simple to use TNN. If you have a trained model, the model can be deployed on the target platform through three steps.\n1. Convert the trained model into a TNN model. We provide a wealth of tools to help you complete this step, whether you are using Tensorflow, Pytorch, or Caffe, you can easily complete the conversion.\nDetailed hands-on tutorials can be found here [How to Create a TNN Model](doc/en/user/convert_en.md).\n\n2. When you have finished converting the model, the second step is to compile the TNN engine of the target platform. You can choose among different acceleration solutions such as ARM/OpenCL/Metal/NPU/X86/CUDA according to the hardware support.\n   For these platforms, TNN provides convenient one-click scripts to compile. For detailed steps, please refer to [How to Compile TNN](doc/en/user/compile_en.md).\n\n3. The final step is to use the compiled TNN engine for inference. You can make program calls to TNN inside your application. We provide a rich and detailed demo as a reference to help you complete.\n    * [Run an iOS Demo](doc/en/user/demo_en.md#i-introduction-to-ios-demo)\n    * [Run an Android Demo](doc/en/user/demo_en.md#ii-introduction-to-android-demo)\n    * [Run an Linux/Windows Demo](doc/en/user/demo_en.md#iii-introduction-to-linuxmacwindowsarmlinuxcudalinux-demo)\n\n## Technical Solutions\n\nAt present, TNN has been launched in various major businesses, and its following characteristics have been widely praised.\n\n* Computation optimization\n    * The backend operators are primely optimized to make the best use of computing power in different architectures, regarding instruction issue, throughput, delay, cache bandwidth, cache delay, registers, etc..\n    * The TNN performance on mainstream hardware platforms (CPU: ARMv7, ARMv8， X86, GPU: Mali, Adreno, Apple， NV GPU， NPU) has been greatly tuned and improved.\n    * The convolution function is implemented by various algorithms such as Winograd, Tile-GEMM, Direct Conv, etc., to ensure efficiency under different parameters and sizes.\n    * Op fusion: TNN can do offline analysis of network graph, fuse multiple simple operations and reduce overhead such as redundant memory access and kernel startup cost.\n\n* Low precision computation acceleration\n    * TNN supports INT8/FP16 mode, reduces model size & memory consumption, and utilizes specific hardware low-precision instructions to accelerate calculations.\n    * TNN supports INT8 WINOGRAD algorithm, (input 6bit), further reduces the model calculation complexity without sacrificing the accuracy.\n    * TNN supports mixed-precision data in one model, speeding up the model's calculation speed while preserving its accuracy.\n\n* Memory optimization\n    * Efficient \"memory pool\" implementation: Based on a full network DAG analysis, the implementation reuses memory between non-dependent nodes which reduces memory cost by 90%.\n    * Cross-model memory reduces: This supports external real-time design for network memory so that multiple models can share mutual memory.\n\n* The performance of mainstream models on TNN: [benchmark data](doc/benchmark_data.md)\n\n* TNN architecture diagram：\n\n   <div><img src=\"https://github.com/darrenyao87/tnn-models/raw/master/doc/en/imgs/tnn_architect.jpg\"/>\n\n* TNN supports TensorFlow, Pytorch, MxNet, Caffe, and other training frameworks through ONNX, leveraging the continuous improvement of the ONNX open-source society.\n  Currently, TNN supports 100+ ONNX operators, consisting of most of the mainstream CNN, NLP operators needed.\n* TNN runs on mainstream operating systems (Android, iOS, embedded Linux, Windows, Linux), and is compatible with ARM CPU,X86 GPU, NPU hardware platform.\n* TNN is constructed through Modular Design, which abstracts and isolates components such as model analysis, graph construction, graph optimization, low-level hardware adaptation, and high-performance kernel.\n   It uses \"Factory Mode\" to register and build devices, that tries to minimize the cost of supporting more hardware and acceleration solutions.\n* The size of the mobile dynamic library is only around 400KB, and it provides basic image conversion operations, which are light-weight and convenient. TNN uses unified models and interfaces across platforms and can switch easily by configuring just one single parameter.\n\n## Learn About TNN Abilities\n* [Operator Support](doc/en/user/support_en.md)\n* [Model Support](doc/en/user/support_en.md)\n* [Device Support](doc/en/user/support_en.md)\n* [Profiling](doc/en/development/profiling_en.md)\n\n## Manual\n* [Compile TNN](doc/en/user/compile_en.md)\n* [Tools]()\n    * [Create a TNN Model](doc/en/user/convert_en.md)\n    * [Model Quantization](doc/en/user/quantization_en.md)\n    * [Model Visualization Netron](https://lutzroeder.github.io/netron/)\n    * [Performance Analysis](doc/en/development/profiling_en.md)\n    * [Model Alignment](doc/en/development/model_check_en.md)\n* [Tutorial]()\n    * [TNN model conversion and deployment for SSD](tutorial/mobilenet_v2_ssd/doc/ssd_conversion_and_deployment_en.md)\n\n## API Document\n* [API call](doc/en/user/api_en.md)\n\n## Contribute to TNN\n* [Development Basics](doc/en/development/contributing_en.md)\n* [Detailed Architecture](doc/en/development/architecture_en.md)\n* [Add a New Operator](doc/en/development/add_op_en.md)\n* [Unit Test](doc/en/development/unit_test_en.md)\n\n## Roadmap\n* [Road map](doc/cn/user/roadmap.md)\n\n## Acknowledgement\nTNN referenced the following projects：\n\n* [ncnn](https://github.com/Tencent/ncnn)\n* [mace](https://github.com/XiaoMi/mace.git)\n* [MNN](https://github.com/alibaba/MNN)\n* [caffe-onnx](https://github.com/htshinichi/caffe-onnx)\n* [tensorflow-onnx](https://github.com/onnx/tensorflow-onnx)\n* [onnx](https://github.com/onnx/onnx)\n* [onnxruntime](https://github.com/microsoft/onnxruntime)\n* [openvino](https://github.com/openvinotoolkit/openvino)\n* [xbyak](https://github.com/herumi/xbyak)\n* [TensorRT](https://developer.nvidia.com/tensorrt)\n\n## License\n* [BSD 3 Clause](LICENSE)\n\n## FAQ\n* [FAQ](doc/en/faq_en.md)\n\n## Join Us\n\n* Everyone is welcome to participate to build the best inference framework in the industry.\n\n* Technical Discussion QQ Group: 704900079 Answer: TNN\n\n* Scan the QR code to join the TNN discussion group：\n<div align=left ><img src=\"TNN-QQ.png\"/>\n"
        },
        {
          "name": "README_CH.md",
          "type": "blob",
          "size": 16.5947265625,
          "content": "[English Version](README.md)\n<div align=left ><img src=\"https://github.com/darrenyao87/tnn-models/raw/master/TNN.png\"/>\n\n## 简介\n\nTNN：由腾讯优图实验室开源的高性能、轻量级神经网络推理框架，同时拥有跨平台、高性能、模型压缩、代码裁剪等众多突出优势。TNN框架在原有Rapidnet、ncnn框架的基础上进一步加强了移动端设备的支持以及性能优化，同时借鉴了业界主流开源框架高性能和良好拓展性的特性，拓展了对于后台X86, NV GPU的支持。手机端 TNN已经在手机QQ、微视、P图等众多应用中落地，服务端TNN作为腾讯云AI基础加速框架已为众多业务落地提供加速支持。欢迎大家参与协同共建，促进TNN推理框架进一步完善。\n\n\n## 效果示例\n\n人脸检测(blazeface)   |  人脸配准(腾讯优图)  |   头发分割(腾讯光影)\n:-------------------------: | :------: | :------:\n[![face_detection](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/face_detection.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/blazeface) <br> 模型链接: [tflite](https://github.com/google/mediapipe/blob/master/mediapipe/models/face_detection_front.tflite) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/blazeface) | [![youtu_face_alignment](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/face_alignment.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/youtu_face_alignment) <br> 模型链接: [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/youtu_face_alignment) | [![hair_segmentation](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/hair_seg_red.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/hair_segmentation) <br> 模型链接: [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/hair_segmentation)\n\n姿势估计(腾讯光流)   |   姿势估计(blazepose)    |  中文字符识别 \n:--------------------------: | :------: | :------: \n[![skeleton](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/skeleton_guangliu.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/skeleton) <br> 模型链接: [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/skeleton) | [![blazepose](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/skeleton_blazepose.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/blazepose) <br> 模型链接: [tflite](https://github.com/google/mediapipe/blob/master/mediapipe/modules/pose_landmark/pose_landmark_full.tflite) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/blazepose) | [![chinese-ocr](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/chinese-ocr.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/chinese-ocr) <br> 模型链接: [onnx](https://github.com/DayBreak-u/chineseocr_lite/tree/onnx/models) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/chinese-ocr)\n\n物体检测(yolov5s)    |    物体检测(MobilenetV2-SSD)    |  阅读理解\n:-------------------------: |:--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------:| :------: \n[![yolov5](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/object-detection.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/yolov5) <br> 模型链接: [onnx](https://github.com/ultralytics/yolov5/blob/master/export.py) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/yolov5) | [![mobilenetv2_ssd](tutorial/mobilenet_v2_ssd/imgs/mobilenetv2_ssd_tf_fix_box.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/mobilenet_v2-ssd) <br> 模型链接: [tensorflow](http://download.tensorflow.org/models/object_detection/ssdlite_mobilenet_v2_coco_2018_05_09.tar.gz) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/mobilenet_v2-ssd) |  [![bertsquad10](https://raw.githubusercontent.com/darrenyao87/tnn-models/master/doc/demo/bert_squad.gif)](https://github.com/darrenyao87/tnn-models/tree/master/model/bertsquad10) <br> 模型链接: [onnx](https://github.com/onnx/models/blob/master/text/machine_comprehension/bert-squad/model/bertsquad-10.onnx) [tnn](https://github.com/darrenyao87/tnn-models/tree/master/model/bertsquad10)\n\n<small>中文字符识别demo是[chineseocr_lite](https://github.com/DayBreak-u/chineseocr_lite)的TNN实现，是一个超轻量级的中文ocr，支持倾斜、旋转和竖排文字识别。</small>\n\n各个平台对demo的支持情况如下表所示，单击✅标记，便可以跳转至对应demo的入口代码。\n\ndemo                                                                                      |   ARM    |  OpenCL  |   Metal  | Huawei NPU | Apple NPU |    X86   |    CUDA   \n:---------------------------------------------------------------------------------------: | :------: | :------: | :------: | :------: | :------: | :------: | :------: |\n[人脸检测](https://github.com/Tencent/TNN/blob/master/examples/base/blazeface_detector.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamBlazeFaceDetector) | [✅ ](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNBlazeFaceDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamBlazeFaceDetector) | [✅ ](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNBlazeFaceDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNFaceDetector/TNNFaceDetector.cc) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNFaceDetector/TNNFaceDetector.cc)\n[物体检测](https://github.com/Tencent/TNN/blob/master/examples/base/object_detector_yolo.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamObjectDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNYoloObjectDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamObjectDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNYoloObjectDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNObjectDetector/TNNObjectDetector.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNObjectDetector/TNNObjectDetector.cc)\n[人脸配准](https://github.com/Tencent/TNN/blob/master/examples/base/face_detect_aligner.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamBlazeFaceAlign) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNFaceDetectAlignerViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamBlazeFaceAlign) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNFaceDetectAlignerViewModel.mm) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNFaceAligner/TNNFaceAligner.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNFaceAligner/TNNFaceAligner.cc)\n[头发分割](https://github.com/Tencent/TNN/blob/master/examples/base/hair_segmentation.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamHairSegmentation) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNHairSegmentationViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamHairSegmentation) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNHairSegmentationViewModel.mm)| [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNHairSegmentation/TNNHairSegmentation.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNHairSegmentation/TNNHairSegmentation.cc) \n[姿势估计(腾讯光流)](https://github.com/Tencent/TNN/blob/master/examples/base/skeleton_detector.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamSkeletonDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNSkeletonDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamSkeletonDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNSkeletonDetectorViewModel.mm) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNSkeletonDetector/TNNSkeletonDetector.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/TNNSkeletonDetector/TNNSkeletonDetector.cc)\n[姿势估计(blazepose)](https://github.com/Tencent/TNN/blob/master/examples/base/pose_detect_landmark.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamPoseDetectLandmark) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNPoseDetectLandmarkViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamPoseDetectLandmark) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNPoseDetectLandmarkViewModel.mm) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNBlazePose/TNNBlazePose.cc) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNBlazePose/TNNBlazePose.cc) \n[中文字符识别](https://github.com/Tencent/TNN/blob/master/examples/base/ocr_driver.cc)   | ✅ | [✅](https://github.com/Tencent/TNN/tree/master/examples/android/demo/src/main/java/com/tencent/tnn/demo/StreamOCRDetector) | [✅](https://github.com/Tencent/TNN/blob/master/examples/ios/TNNExamples/TNNCameraPreviewController/TNNViewModel/TNNOCRViewModel.mm) | | | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNOcrDetector/TNNOcrDetector.cc) | [✅](https://github.com/Tencent/TNN/tree/master/examples/linux/src/TNNOcrDetector/TNNOcrDetector.cc)\n[阅读理解](https://github.com/Tencent/TNN/blob/master/examples/base/bert_tokenizer.cc)  | | | | | | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/BertReadingComprehension/BertReadingComprehension.cc) | [✅](https://github.com/Tencent/TNN/blob/master/examples/linux/src/BertReadingComprehension/BertReadingComprehension.cc)\n\n## 快速开始\n\n使用 TNN 非常简单，如果你有一个已经训练好的模型, 那么一般而言通过以下三个步骤就能完成模型在目标平台上的部署。\n1. 第一步是把训练好的模型转换成TNN的模型，为此我们提供了丰富的工具来帮助你完成这一步，无论你使用的是 TensorFlow、PyTorch、或者 Caffe，都可以轻松完成转换。\n详细的手把手教程可以参见这里[如何转换模型](doc/cn/user/convert.md)。\n\n2. 当你完成了模型的转换，第二步就是编译目标平台的 TNN 引擎了，你可以根据自己的目标平台的硬件支持情况，选择 CPU/ARM/OpenCL/Metal/NPU/X86/CUDA 等加速方案。\n   对于这些平台，TNN 都提供了一键编译的脚本，使用非常方便。详细步骤可以参考这里[如何编译TNN](doc/cn/user/compile.md)。\n\n3. 最后一步就是使用编译好的 TNN 引擎进行推理，你可以在自己的应用程序中嵌入对 TNN 的调用，这方面我们提供了丰富而详实的 demo 来帮助你完成。\n    *  [从0开始跑通一个iOS Demo](doc/cn/user/demo.md)\n    *  [从0开始跑通一个Android Demo](doc/cn/user/demo.md)\n    *  [从0开始跑通一个Windows/Linux Demo](doc/cn/user/demo.md)\n\n## 技术方案\n\n目前TNN具有的以下特性获得了广泛的好评。\n\n* 计算优化\n    * 针对不同架构在硬件指令发射、吞吐、延迟、缓存带宽、缓存延迟、寄存器数量等特点，深度优化底层算子，极致利用硬件算力\n    * 主流硬件平台(CPU: ARMv7， ARMv8，X86 GPU: Mali， Adreno， Apple， NV GPU) 深度调优\n    * CNN 核心卷积运算通过 Winograd，Tile-GEMM， Direct Conv 等多种算法实现，保证不同参数、计算尺度下高效计算\n    * Op 融合：离线分析网络计算图，多个小 Op（计算量小、功能较简单）融合运算，减少反复内存读取、kernel 启动等开销\n\n* 低精度优化\n    * 支持 INT8， FP16 低精度计算，减少模型大小、内存消耗，同时利用硬件低精度计算指令加速计算\n    * 支持 INT8 Winograd 算法，(输入6bit)， 在精度满足要求的情况下，进一步降低模型计算复杂度\n    * 支持单模型多种精度混合计算，加速计算同时保证模型精度\n\n* 内存优化\n    * 高效”内存池”实现：通过 DAG 网络计算图分析，实现无计算依赖的节点间复用内存，降低 90% 内存资源消耗\n    * 跨模型内存复用：支持外部实时指定用于网络内存，实现“多个模型，单份内存”。\n\n* 主流模型实测性能：[评测数据](doc/benchmark_data.md)\n\n* TNN架构图：\n\n   <div><img src=\"https://github.com/darrenyao87/tnn-models/raw/master/doc/cn/imgs/tnn_architect.jpg\"/>\n\n* 通过 ONNX 支持 TensorFlow， PyTorch， MXNet， Caffe 等多种训练框架，充分利用和融入不断完善的 ONNX 开源生态。当前支持 ONNX 算子100+，覆盖主流CNN, NLP网络。\n* 支持主流安卓、iOS、Embedded Linux 操作系统, Windows, Linux，支持 ARM CPU, x86, Mali GPU, Adreno GPU, NV GPU, 达芬奇NPU，RK NPU。\n* 模块化设计，将模型解析、计算图构建、优化、底层硬件适配、高性能 kernel 实现各部分抽象隔离，通过 Factory Mode 注册、构建设备，方便接入更多的底层硬件、加速方案。\n* 移动端动态库尺寸仅约 400KB，并提供基础图像变换操作，调用简单便捷。跨平台模型统一、调用接口统一，通过单个配置参数快速切换。\n\n## 能力展示\n* [支持的算子](doc/cn/user/support.md)\n* [支持的网络](doc/cn/user/support.md)\n* [支持的架构](doc/cn/user/support.md)\n* [Benchmark性能测试方法](doc/cn/development/profiling.md)\n\n## 使用手册\n* [从源码编译](doc/cn/user/compile.md)\n* [工具集]()\n    * [模型转换](doc/cn/user/convert.md)\n    * [模型量化](doc/cn/user/quantization.md)\n    * [模型可视化Netron](https://lutzroeder.github.io/netron/)\n    * [性能分析工具](doc/cn/development/profiling.md)\n    * [模型对齐工具](doc/cn/development/model_check.md)\n* [教程]()\n    * [基于 SSD 的 TNN 模型转换和部署](tutorial/mobilenet_v2_ssd/doc/ssd_conversion_and_deployment.md)\n\n## API文档\n* [API调用](doc/cn/user/api.md)\n\n## 贡献者须知\n* [开发基础须知](doc/cn/development/contributing.md)\n* [架构详解](doc/cn/development/architecture.md)\n* [新增OP](doc/cn/development/add_op.md)\n* [单元测试](doc/cn/development/unit_test.md)\n\n## Roadmap\n* [Road map](doc/cn/user/roadmap.md)\n\n## 致谢\nTNN参考和借鉴了下列项目：\n\n* [ncnn](https://github.com/Tencent/ncnn)\n* [mace](https://github.com/XiaoMi/mace.git)\n* [MNN](https://github.com/alibaba/MNN)\n* [caffe-onnx](https://github.com/htshinichi/caffe-onnx)\n* [tensorflow-onnx](https://github.com/onnx/tensorflow-onnx)\n* [onnx](https://github.com/onnx/onnx)\n* [onnxruntime](https://github.com/microsoft/onnxruntime)\n* [openvino](https://github.com/openvinotoolkit/openvino) \n* [xbyak](https://github.com/herumi/xbyak)\n* [TensorRT](https://developer.nvidia.com/zh-cn/tensorrt)\n\n## License\n\n* [BSD 3 Clause](LICENSE)\n\n## FAQ\n* [FAQ 常见问题](doc/cn/faq.md)\n\n## 加入我们\n\n* 欢迎大家参与，协同共建，打造业界最好的高性能推理框架。\n\n* 技术交流 QQ 群： 704900079 答案：TNN\n\n* QQ 群二维码：\n<div align=left ><img src=\"TNN-QQ.png\"/>\n"
        },
        {
          "name": "RELEASE.md",
          "type": "blob",
          "size": 0,
          "content": ""
        },
        {
          "name": "TNN-QQ-1.png",
          "type": "blob",
          "size": 45.24609375,
          "content": null
        },
        {
          "name": "TNN-QQ.png",
          "type": "blob",
          "size": 83.5283203125,
          "content": null
        },
        {
          "name": "TNN.png",
          "type": "blob",
          "size": 135.734375,
          "content": null
        },
        {
          "name": "_config.yml",
          "type": "blob",
          "size": 0.025390625,
          "content": "theme: jekyll-theme-cayman"
        },
        {
          "name": "benchmark",
          "type": "tree",
          "content": null
        },
        {
          "name": "cmake",
          "type": "tree",
          "content": null
        },
        {
          "name": "doc",
          "type": "tree",
          "content": null
        },
        {
          "name": "examples",
          "type": "tree",
          "content": null
        },
        {
          "name": "include",
          "type": "tree",
          "content": null
        },
        {
          "name": "model",
          "type": "tree",
          "content": null
        },
        {
          "name": "platforms",
          "type": "tree",
          "content": null
        },
        {
          "name": "scripts",
          "type": "tree",
          "content": null
        },
        {
          "name": "source",
          "type": "tree",
          "content": null
        },
        {
          "name": "test",
          "type": "tree",
          "content": null
        },
        {
          "name": "third_party",
          "type": "tree",
          "content": null
        },
        {
          "name": "tools",
          "type": "tree",
          "content": null
        },
        {
          "name": "tutorial",
          "type": "tree",
          "content": null
        }
      ]
    }
  ]
}