{
  "metadata": {
    "timestamp": 1736565428539,
    "page": 274,
    "hasNextPage": true,
    "endCursor": "Y3Vyc29yOjI4MA==",
    "completionStatus": "IN_PROGRESS"
  },
  "repositories": [
    {
      "nameWithOwner": "google/robotstxt",
      "stars": 3397,
      "defaultBranch": "master",
      "files": [
        {
          "name": ".gitignore",
          "type": "blob",
          "size": 0.009765625,
          "content": "c-build/*\n"
        },
        {
          "name": "BUILD",
          "type": "blob",
          "size": 1.1767578125,
          "content": "load(\"@rules_cc//cc:defs.bzl\", \"cc_binary\", \"cc_library\", \"cc_test\")\n\npackage(default_visibility = [\"//visibility:public\"])\n\nlicenses([\"notice\"])\n\nexports_files([\"LICENSE\"])\n\ncc_library(\n    name = \"robots\",\n    srcs = [\n        \"robots.cc\",\n    ],\n    hdrs = [\n        \"robots.h\",\n    ],\n    deps = [\n        \"@abseil-cpp//absl/base:core_headers\",\n        \"@abseil-cpp//absl/container:fixed_array\",\n        \"@abseil-cpp//absl/strings\",\n    ],\n)\n\ncc_library(\n    name = \"reporting_robots\",\n    srcs = [\"reporting_robots.cc\"],\n    hdrs = [\"reporting_robots.h\"],\n    deps = [\n        \":robots\",\n        \"@abseil-cpp//absl/container:btree\",\n        \"@abseil-cpp//absl/strings\",\n    ],\n)\n\ncc_test(\n    name = \"robots_test\",\n    srcs = [\"robots_test.cc\"],\n    deps = [\n        \":robots\",\n        \"@abseil-cpp//absl/strings\",\n        \"@googletest//:gtest_main\",\n    ],\n)\n\ncc_test(\n    name = \"reporting_robots_test\",\n    srcs = [\"reporting_robots_test.cc\"],\n    deps = [\n        \":reporting_robots\",\n        \":robots\",\n        \"@abseil-cpp//absl/strings\",\n        \"@googletest//:gtest_main\",\n    ],\n)\n\ncc_binary(\n    name = \"robots_main\",\n    srcs = [\"robots_main.cc\"],\n    deps = [\n        \":robots\",\n    ],\n)\n"
        },
        {
          "name": "CMakeLists.txt",
          "type": "blob",
          "size": 5.1865234375,
          "content": "PROJECT(robots)\nCMAKE_MINIMUM_REQUIRED(VERSION 3.0)\n\nSET(CMAKE_CXX_STANDARD 14)\nSET(CMAKE_POSITION_INDEPENDENT_CODE ON)\n\nSET(VERSION \"0.0.0\")\n\nSTRING(REGEX MATCHALL \"([0-9]+)\" VERSION_DIGITS \"${VERSION}\")\n\nLIST(GET VERSION_DIGITS 0 CPACK_PACKAGE_VERSION_MAJOR)\nLIST(GET VERSION_DIGITS 1 CPACK_PACKAGE_VERSION_MINOR)\nLIST(GET VERSION_DIGITS 2 CPACK_PACKAGE_VERSION_PATCH)\n\nSET(CPACK_PACKAGE_NAME \"robots\")\nSET(CPACK_PACKAGE_DESCRIPTION_SUMMARY \"Google's robots.txt parser and matcher C++ library\")\nSET(CPACK_PACKAGE_VENDOR \"Google Inc.\")\nSET(CPACK_PACAKGE_DESCRIPTION_FILE \"${CMAKE_SOURCE_DIR}/README.md\")\nSET(CPACK_RESOURCE_FILE_LICENSE \"${CMAKE_SOURCE_DIR}/LICENSE\")\n\nSET(CPACK_PACKAGE_INSTALL_DIRECTORY \"${CPACK_PACKAGE_DESCRIPTION_SUMMARY} ${CPACK_PACKAGE_VERSION_MAJOR}.${CPACK_PACKAGE_VERSION_MINOR}.${CPACK_PACKAGE_VERSION_PATCH}\")\nSET(CPACK_SOURCE_PACKAGE_FILE_NAME \"${CPACK_PACKAGE_NAME}-${CPACK_PACKAGE_VERSION_MAJOR}.${CPACK_PACKAGE_VERSION_MINOR}.${CPACK_PACKAGE_VERSION_PATCH}\")\n\nSET(base_with_ver \"robots-[0-9]+\\\\\\\\.[0-9]+\\\\\\\\.[0-9]+\")\nSET(CPACK_SOURCE_IGNORE_FILES\n    \"/_CPack_Packages/\"\n    \"/CMakeFiles/\"\n    \"/.deps/\"\n    \"^${base_with_ver}(-Source|-Linux)?/\"\n    \"${base_with_ver}.tar\\\\\\\\.(gz|bz2|Z|lzma|xz)$\"\n    \"\\\\\\\\.o$\"\n    \"~$\"\n    \"/\\\\\\\\.svn/\"\n    \"/CMakeCache\\\\\\\\.txt$\"\n    \"/CTestTestfile\\\\\\\\.cmake$\"\n    \"/cmake_install\\\\\\\\.cmake$\"\n    \"/CPackConfig\\\\\\\\.cmake$\"\n    \"/CPackSourceConfig\\\\\\\\.cmake$\"\n    \"/tags$\"\n    \"^config\\\\\\\\.h$\"\n    \"/install_manifest\\\\\\\\.txt$\"\n    \"/Testing/\"\n    \"ids-whitelist\\\\\\\\.txt\"\n    \"/_Inline/\"\n    \"/(B|build|BUILD)/\"\n    \"/autom4te.cache/\"\n)\n\n############ build options ##############\n\nOPTION(ROBOTS_BUILD_STATIC \"If ON, robots will build also the static library\" ON)\nOPTION(ROBOTS_BUILD_TESTS \"If ON, robots will build test targets\" OFF)\nOPTION(ROBOTS_INSTALL \"If ON, enable the installation of the targets\" ON)\n\n############ helper libs ############\n\nINCLUDE(CPack)\nINCLUDE(CheckCCompilerFlag)\nINCLUDE(ExternalProject)\n\n############ dependencies ##############\n\nCONFIGURE_FILE(CMakeLists.txt.in libs/CMakeLists.txt)\nEXECUTE_PROCESS(COMMAND ${CMAKE_COMMAND} -G \"${CMAKE_GENERATOR}\" . RESULT_VARIABLE result WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/libs)\n\nIF(result)\n    MESSAGE(FATAL_ERROR \"Failed to download dependencies: ${result}\")\nENDIF()\n\nEXECUTE_PROCESS(COMMAND ${CMAKE_COMMAND} --build . RESULT_VARIABLE result WORKING_DIRECTORY ${CMAKE_CURRENT_BINARY_DIR}/libs)\n\nIF(result)\n    MESSAGE(FATAL_ERROR \"Failed to download dependencies: ${result}\")\nENDIF()\n\n# abseil-cpp\nIF(MSVC)\n    # /wd4005  macro-redefinition\n    # /wd4068  unknown pragma\n    # /wd4244  conversion from 'type1' to 'type2'\n    # /wd4267  conversion from 'size_t' to 'type2'\n    # /wd4800  force value to bool 'true' or 'false' (performance warning)\n    ADD_COMPILE_OPTIONS(/wd4005 /wd4068 /wd4244 /wd4267 /wd4800)\n    ADD_DEFINITIONS(/DNOMINMAX /DWIN32_LEAN_AND_MEAN=1 /D_CRT_SECURE_NO_WARNINGS)\nENDIF(MSVC)\n\nADD_SUBDIRECTORY(${CMAKE_CURRENT_BINARY_DIR}/libs/abseil-cpp-src\n                 ${CMAKE_CURRENT_BINARY_DIR}/libs/abseil-cpp-build\n                 EXCLUDE_FROM_ALL)\n\nIF(ROBOTS_BUILD_TESTS)\n    INCLUDE(CTest)\n\n    # googletest\n    ADD_SUBDIRECTORY(${CMAKE_CURRENT_BINARY_DIR}/libs/gtest-src\n                     ${CMAKE_CURRENT_BINARY_DIR}/libs/gtest-build\n                     EXCLUDE_FROM_ALL)\n\n    SET(INSTALL_GTEST 0)\n    SET(gtest_force_shared_crt ON CACHE BOOL \"\" FORCE)\n\n    IF(CMAKE_VERSION VERSION_LESS 2.8.11)\n        INCLUDE_DIRECTORIES(${gtest_SOURCE_DIR}/include)\n    ENDIF()\nENDIF(ROBOTS_BUILD_TESTS)\n\n########### compiler flags ##############\n\n\nSET(COMPILER_FLAGS_TO_CHECK\n    \"-Wall\" \"-Werror=implicit-function-declaration\"\n)\n\nIF(CPU_ARCH)\n    LIST(APPEND COMPILER_FLAGS_TO_CHECK \"-march=${CPU_ARCH}\")\nENDIF(CPU_ARCH)\n\n########### project files ###############\n\nINCLUDE_DIRECTORIES(.)\n\n######### targets ###########\n\nSET(LIBROBOTS_LIBS)\n\nSET(robots_SRCS ./robots.cc)\nSET(robots_LIBS absl::base absl::strings)\n\nADD_LIBRARY(robots SHARED ${robots_SRCS})\nTARGET_LINK_LIBRARIES(robots ${robots_LIBS})\nLIST(APPEND LIBROBOTS_LIBS \"robots\")\n\nIF(ROBOTS_BUILD_STATIC)\n    ADD_LIBRARY(robots-static STATIC ${robots_SRCS})\n    TARGET_LINK_LIBRARIES(robots-static ${robots_LIBS})\n\n    LIST(APPEND LIBROBOTS_LIBS \"robots-static\")\n\n    SET_TARGET_PROPERTIES(robots-static PROPERTIES OUTPUT_NAME \"robots\")\n\n    SET_TARGET_PROPERTIES(${LIBROBOTS_LIBS} PROPERTIES CLEAN_DIRECT_OUTPUT 1)\nENDIF(ROBOTS_BUILD_STATIC)\n\nIF(WIN_32)\n    SET_TARGET_PROPERTIES(robots PROPERTIES DEFINE_SYMBOL DLL_EXPORT)\nENDIF(WIN_32)\n\nADD_EXECUTABLE(robots-main ./robots_main.cc)\nTARGET_LINK_LIBRARIES(robots-main ${LIBROBOTS_LIBS})\nSET_TARGET_PROPERTIES(robots-main PROPERTIES OUTPUT_NAME \"robots\")\n\n############ installation ############\n\nIF(ROBOTS_INSTALL)\n    INSTALL(TARGETS ${LIBROBOTS_LIBS}\n        LIBRARY DESTINATION lib\n        ARCHIVE DESTINATION lib\n    )\n\n    INSTALL(FILES ${CMAKE_SOURCE_DIR}/robots.h DESTINATION include)\n\n    INSTALL(TARGETS robots-main DESTINATION bin)\nENDIF(ROBOTS_INSTALL)\n\n############ tests ##############\n\nIF(ROBOTS_BUILD_TESTS)\n    ENABLE_TESTING()\n\n    ADD_EXECUTABLE(robots-test ./robots_test.cc)\n    TARGET_LINK_LIBRARIES(robots-test ${LIBROBOTS_LIBS} gtest_main)\n    ADD_TEST(NAME robots-test COMMAND robots-test)\nENDIF(ROBOTS_BUILD_TESTS)\n\n"
        },
        {
          "name": "CMakeLists.txt.in",
          "type": "blob",
          "size": 0.7744140625,
          "content": "\nPROJECT(dependency-downloader NONE)\nCMAKE_MINIMUM_REQUIRED(VERSION 3.0)\n\nINCLUDE(ExternalProject)\n\nExternalProject_Add(abseilcpp\n    GIT_REPOSITORY https://github.com/abseil/abseil-cpp.git\n    GIT_TAG master\n    GIT_PROGRESS 1\n    SOURCE_DIR \"${CMAKE_CURRENT_BINARY_DIR}/libs/abseil-cpp-src\"\n    BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}/libs/abseil-cpp-build\"\n    CONFIGURE_COMMAND \"\"\n    BUILD_COMMAND \"\"\n    INSTALL_COMMAND \"\"\n    TEST_COMMAND \"\"\n)\n\nExternalProject_Add(googletest\n    GIT_REPOSITORY https://github.com/google/googletest.git\n    GIT_TAG main\n    GIT_PROGRESS 1\n    SOURCE_DIR \"${CMAKE_CURRENT_BINARY_DIR}/libs/gtest-src\"\n    BINARY_DIR \"${CMAKE_CURRENT_BINARY_DIR}/libs/gtest-build\"\n    CONFIGURE_COMMAND \"\"\n    BUILD_COMMAND \"\"\n    INSTALL_COMMAND \"\"\n    TEST_COMMAND \"\"\n)\n\n"
        },
        {
          "name": "CONTRIBUTING.md",
          "type": "blob",
          "size": 1.2021484375,
          "content": "# How to Contribute\n\nWe'd love to accept your patches and contributions to this project. There are\njust a few small guidelines you need to follow.\n\n## Contributor License Agreement\n\nContributions to this project must be accompanied by a Contributor License\nAgreement. You (or your employer) retain the copyright to your contribution;\nthis simply gives us permission to use and redistribute your contributions as\npart of the project. Head over to <https://cla.developers.google.com/> to see\nyour current agreements on file or to sign a new one.\n\nYou generally only need to submit a CLA once, so if you've already submitted one\n(even if it was for a different project), you probably don't need to do it\nagain.\n\n## Code reviews\n\nAll submissions, including submissions by project members, require review. We\nuse GitHub pull requests for this purpose. Keep in mind that this library is\nused in Google's production systems, so we need to be very strict about what\nchanges we accept. Consult\n[GitHub Help](https://help.github.com/articles/about-pull-requests/) for more\ninformation on using pull requests.\n\n## Community Guidelines\n\nThis project follows [Google's Open Source Community\nGuidelines](https://opensource.google.com/conduct/).\n"
        },
        {
          "name": "LICENSE",
          "type": "blob",
          "size": 11.0947265625,
          "content": "\n                                 Apache License\n                           Version 2.0, January 2004\n                        https://www.apache.org/licenses/\n\n   TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n   1. Definitions.\n\n      \"License\" shall mean the terms and conditions for use, reproduction,\n      and distribution as defined by Sections 1 through 9 of this document.\n\n      \"Licensor\" shall mean the copyright owner or entity authorized by\n      the copyright owner that is granting the License.\n\n      \"Legal Entity\" shall mean the union of the acting entity and all\n      other entities that control, are controlled by, or are under common\n      control with that entity. For the purposes of this definition,\n      \"control\" means (i) the power, direct or indirect, to cause the\n      direction or management of such entity, whether by contract or\n      otherwise, or (ii) ownership of fifty percent (50%) or more of the\n      outstanding shares, or (iii) beneficial ownership of such entity.\n\n      \"You\" (or \"Your\") shall mean an individual or Legal Entity\n      exercising permissions granted by this License.\n\n      \"Source\" form shall mean the preferred form for making modifications,\n      including but not limited to software source code, documentation\n      source, and configuration files.\n\n      \"Object\" form shall mean any form resulting from mechanical\n      transformation or translation of a Source form, including but\n      not limited to compiled object code, generated documentation,\n      and conversions to other media types.\n\n      \"Work\" shall mean the work of authorship, whether in Source or\n      Object form, made available under the License, as indicated by a\n      copyright notice that is included in or attached to the work\n      (an example is provided in the Appendix below).\n\n      \"Derivative Works\" shall mean any work, whether in Source or Object\n      form, that is based on (or derived from) the Work and for which the\n      editorial revisions, annotations, elaborations, or other modifications\n      represent, as a whole, an original work of authorship. For the purposes\n      of this License, Derivative Works shall not include works that remain\n      separable from, or merely link (or bind by name) to the interfaces of,\n      the Work and Derivative Works thereof.\n\n      \"Contribution\" shall mean any work of authorship, including\n      the original version of the Work and any modifications or additions\n      to that Work or Derivative Works thereof, that is intentionally\n      submitted to Licensor for inclusion in the Work by the copyright owner\n      or by an individual or Legal Entity authorized to submit on behalf of\n      the copyright owner. For the purposes of this definition, \"submitted\"\n      means any form of electronic, verbal, or written communication sent\n      to the Licensor or its representatives, including but not limited to\n      communication on electronic mailing lists, source code control systems,\n      and issue tracking systems that are managed by, or on behalf of, the\n      Licensor for the purpose of discussing and improving the Work, but\n      excluding communication that is conspicuously marked or otherwise\n      designated in writing by the copyright owner as \"Not a Contribution.\"\n\n      \"Contributor\" shall mean Licensor and any individual or Legal Entity\n      on behalf of whom a Contribution has been received by Licensor and\n      subsequently incorporated within the Work.\n\n   2. Grant of Copyright License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      copyright license to reproduce, prepare Derivative Works of,\n      publicly display, publicly perform, sublicense, and distribute the\n      Work and such Derivative Works in Source or Object form.\n\n   3. Grant of Patent License. Subject to the terms and conditions of\n      this License, each Contributor hereby grants to You a perpetual,\n      worldwide, non-exclusive, no-charge, royalty-free, irrevocable\n      (except as stated in this section) patent license to make, have made,\n      use, offer to sell, sell, import, and otherwise transfer the Work,\n      where such license applies only to those patent claims licensable\n      by such Contributor that are necessarily infringed by their\n      Contribution(s) alone or by combination of their Contribution(s)\n      with the Work to which such Contribution(s) was submitted. If You\n      institute patent litigation against any entity (including a\n      cross-claim or counterclaim in a lawsuit) alleging that the Work\n      or a Contribution incorporated within the Work constitutes direct\n      or contributory patent infringement, then any patent licenses\n      granted to You under this License for that Work shall terminate\n      as of the date such litigation is filed.\n\n   4. Redistribution. You may reproduce and distribute copies of the\n      Work or Derivative Works thereof in any medium, with or without\n      modifications, and in Source or Object form, provided that You\n      meet the following conditions:\n\n      (a) You must give any other recipients of the Work or\n          Derivative Works a copy of this License; and\n\n      (b) You must cause any modified files to carry prominent notices\n          stating that You changed the files; and\n\n      (c) You must retain, in the Source form of any Derivative Works\n          that You distribute, all copyright, patent, trademark, and\n          attribution notices from the Source form of the Work,\n          excluding those notices that do not pertain to any part of\n          the Derivative Works; and\n\n      (d) If the Work includes a \"NOTICE\" text file as part of its\n          distribution, then any Derivative Works that You distribute must\n          include a readable copy of the attribution notices contained\n          within such NOTICE file, excluding those notices that do not\n          pertain to any part of the Derivative Works, in at least one\n          of the following places: within a NOTICE text file distributed\n          as part of the Derivative Works; within the Source form or\n          documentation, if provided along with the Derivative Works; or,\n          within a display generated by the Derivative Works, if and\n          wherever such third-party notices normally appear. The contents\n          of the NOTICE file are for informational purposes only and\n          do not modify the License. You may add Your own attribution\n          notices within Derivative Works that You distribute, alongside\n          or as an addendum to the NOTICE text from the Work, provided\n          that such additional attribution notices cannot be construed\n          as modifying the License.\n\n      You may add Your own copyright statement to Your modifications and\n      may provide additional or different license terms and conditions\n      for use, reproduction, or distribution of Your modifications, or\n      for any such Derivative Works as a whole, provided Your use,\n      reproduction, and distribution of the Work otherwise complies with\n      the conditions stated in this License.\n\n   5. Submission of Contributions. Unless You explicitly state otherwise,\n      any Contribution intentionally submitted for inclusion in the Work\n      by You to the Licensor shall be under the terms and conditions of\n      this License, without any additional terms or conditions.\n      Notwithstanding the above, nothing herein shall supersede or modify\n      the terms of any separate license agreement you may have executed\n      with Licensor regarding such Contributions.\n\n   6. Trademarks. This License does not grant permission to use the trade\n      names, trademarks, service marks, or product names of the Licensor,\n      except as required for reasonable and customary use in describing the\n      origin of the Work and reproducing the content of the NOTICE file.\n\n   7. Disclaimer of Warranty. Unless required by applicable law or\n      agreed to in writing, Licensor provides the Work (and each\n      Contributor provides its Contributions) on an \"AS IS\" BASIS,\n      WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or\n      implied, including, without limitation, any warranties or conditions\n      of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A\n      PARTICULAR PURPOSE. You are solely responsible for determining the\n      appropriateness of using or redistributing the Work and assume any\n      risks associated with Your exercise of permissions under this License.\n\n   8. Limitation of Liability. In no event and under no legal theory,\n      whether in tort (including negligence), contract, or otherwise,\n      unless required by applicable law (such as deliberate and grossly\n      negligent acts) or agreed to in writing, shall any Contributor be\n      liable to You for damages, including any direct, indirect, special,\n      incidental, or consequential damages of any character arising as a\n      result of this License or out of the use or inability to use the\n      Work (including but not limited to damages for loss of goodwill,\n      work stoppage, computer failure or malfunction, or any and all\n      other commercial damages or losses), even if such Contributor\n      has been advised of the possibility of such damages.\n\n   9. Accepting Warranty or Additional Liability. While redistributing\n      the Work or Derivative Works thereof, You may choose to offer,\n      and charge a fee for, acceptance of support, warranty, indemnity,\n      or other liability obligations and/or rights consistent with this\n      License. However, in accepting such obligations, You may act only\n      on Your own behalf and on Your sole responsibility, not on behalf\n      of any other Contributor, and only if You agree to indemnify,\n      defend, and hold each Contributor harmless for any liability\n      incurred by, or claims asserted against, such Contributor by reason\n      of your accepting any such warranty or additional liability.\n\n   END OF TERMS AND CONDITIONS\n\n   APPENDIX: How to apply the Apache License to your work.\n\n      To apply the Apache License to your work, attach the following\n      boilerplate notice, with the fields enclosed by brackets \"[]\"\n      replaced with your own identifying information. (Don't include\n      the brackets!)  The text should be enclosed in the appropriate\n      comment syntax for the file format. We also recommend that a\n      file or class name and description of purpose be included on the\n      same \"printed page\" as the copyright notice for easier\n      identification within third-party archives.\n\n   Copyright [yyyy] [name of copyright owner]\n\n   Licensed under the Apache License, Version 2.0 (the \"License\");\n   you may not use this file except in compliance with the License.\n   You may obtain a copy of the License at\n\n       https://www.apache.org/licenses/LICENSE-2.0\n\n   Unless required by applicable law or agreed to in writing, software\n   distributed under the License is distributed on an \"AS IS\" BASIS,\n   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n   See the License for the specific language governing permissions and\n   limitations under the License.\n\n"
        },
        {
          "name": "MODULE.bazel",
          "type": "blob",
          "size": 0.3505859375,
          "content": "module(\n    name = \"com_google_robotstxt\",\n    version = \"head\",\n)\n\n# -- bazel_dep definitions -- #\nbazel_dep(\n    name = \"bazel_skylib\",\n    version = \"1.5.0\",\n)\n\nbazel_dep(\n    name = \"abseil-cpp\",\n    version = \"20240116.2\",\n)\n\nbazel_dep(\n    name = \"googletest\",\n    version = \"1.14.0.bcr.1\",\n)\n\nbazel_dep(\n    name = \"rules_cc\",\n    version = \"0.0.9\",\n)\n"
        },
        {
          "name": "README.md",
          "type": "blob",
          "size": 4.9775390625,
          "content": "\n# Google Robots.txt Parser and Matcher Library\n\nThe repository contains Google's robots.txt parser and matcher as a C++ library\n(compliant to C++14).\n\n## About the library\n\nThe Robots Exclusion Protocol (REP) is a standard that enables website owners to\ncontrol which URLs may be accessed by automated clients (i.e. crawlers) through\na simple text file with a specific syntax. It's one of the basic building blocks\nof the internet as we know it and what allows search engines to operate.\n\nBecause the REP was only a de-facto standard for the past 25 years, different\nimplementers implement parsing of robots.txt slightly differently, leading to\nconfusion. This project aims to fix that by releasing the parser that Google\nuses.\n\nThe library is slightly modified (i.e. some internal headers and equivalent\nsymbols) production code used by Googlebot, Google's crawler, to determine which\nURLs it may access based on rules provided by webmasters in robots.txt files.\nThe library is released open-source to help developers build tools that better\nreflect Google's robots.txt parsing and matching.\n\nFor webmasters, we included a small binary in the project that allows testing a\nsingle URL and user-agent against a robots.txt.\n\n## Building the library\n\n### Quickstart\n\nWe included with the library a small binary to test a local robots.txt against a\nuser-agent and URL. Running the included binary requires:\n\n*   A compatible platform (e.g. Windows, macOS, Linux, etc.). Most platforms are\n    fully supported.\n*   A compatible C++ compiler supporting at least C++14. Most major compilers\n    are supported.\n*   [Git](https://git-scm.com/) for interacting with the source code repository.\n    To install Git, consult the\n    [Set Up Git](https://help.github.com/articles/set-up-git/) guide on\n    [GitHub](https://github.com/).\n*   Although you are free to use your own build system, most of the\n    documentation within this guide will assume you are using\n    [Bazel](https://bazel.build/). To download and install Bazel (and any of its\n    dependencies), consult the\n    [Bazel Installation Guide](https://docs.bazel.build/versions/master/install.html)\n\n#### Building with Bazel\n\n[Bazel](https://bazel.build/) is the official build system for the library,\nwhich is supported on most major platforms (Linux, Windows, MacOS, for example)\nand compilers.\n\nTo build and run the binary:\n\n```bash\n$ git clone https://github.com/google/robotstxt.git robotstxt\nCloning into 'robotstxt'...\n...\n$ cd robotstxt/\nbazel-robots$ bazel test :robots_test\n...\n/:robots_test                                                      PASSED in 0.1s\n\nExecuted 1 out of 1 test: 1 test passes.\n...\nbazel-robots$ bazel build :robots_main\n...\nTarget //:robots_main up-to-date:\n  bazel-bin/robots_main\n...\nbazel-robots$ bazel run robots_main -- ~/local/path/to/robots.txt YourBot https://example.com/url\n  user-agent 'YourBot' with url 'https://example.com/url' allowed: YES\n```\n\n#### Building with CMake\n\n[CMake](https://cmake.org) is the community-supported build system for the\nlibrary.\n\nTo build the library using CMake, just follow the steps below:\n\n```bash\n$ git clone https://github.com/google/robotstxt.git robotstxt\nCloning into 'robotstxt'...\n...\n$ cd robotstxt/\n...\n$ mkdir c-build && cd c-build\n...\n$ cmake .. -DROBOTS_BUILD_TESTS=ON\n...\n$ make\n...\n$ make test\nRunning tests...\nTest project robotstxt/c-build\n    Start 1: robots-test\n1/1 Test #1: robots-test ......................   Passed    0.02 sec\n\n100% tests passed, 0 tests failed out of 1\n\nTotal Test time (real) =   0.02 sec\n...\n$ robots ~/local/path/to/robots.txt YourBot https://example.com/url\n  user-agent 'YourBot' with url 'https://example.com/url' allowed: YES\n```\n\n## Notes\n\nParsing of robots.txt files themselves is done exactly as in the production\nversion of Googlebot, including how percent codes and unicode characters in\npatterns are handled. The user must ensure however that the URI passed to the\nAllowedByRobots and OneAgentAllowedByRobots functions, or to the URI parameter\nof the robots tool, follows the format specified by RFC3986, since this library\nwill not perform full normalization of those URI parameters. Only if the URI is\nin this format, the matching will be done according to the REP specification.\n\nAlso note that the library, and the included binary, do not handle\nimplementation logic that a crawler might apply outside of parsing and matching,\nfor example: `Googlebot-Image` respecting the rules specified for `User-agent:\nGooglebot` if not explicitly defined in the robots.txt file being tested.\n\n## License\n\nThe robots.txt parser and matcher C++ library is licensed under the terms of the\nApache license. See LICENSE for more information.\n\n## Links\n\nTo learn more about this project:\n\n*   check out the\n    [Robots Exclusion Protocol standard](https://www.rfc-editor.org/rfc/rfc9309.html),\n*   how\n    [Google Handles robots.txt](https://developers.google.com/search/reference/robots_txt),\n*   or for a high level overview, the\n    [robots.txt page on Wikipedia](https://en.wikipedia.org/wiki/Robots_exclusion_standard).\n"
        },
        {
          "name": "protocol-draft",
          "type": "tree",
          "content": null
        },
        {
          "name": "reporting_robots.cc",
          "type": "blob",
          "size": 2.9697265625,
          "content": "#include \"reporting_robots.h\"\n\n#include <algorithm>\n#include <string>\n#include <vector>\n\n#include \"absl/strings/ascii.h\"\n#include \"absl/strings/string_view.h\"\n\nnamespace googlebot {\n// The kUnsupportedTags tags are popular tags in robots.txt files, but Google\n// doesn't use them for anything. Other search engines may, however, so we\n// parse them out so users of the library can highlight them for their own\n// users if they so wish.\n// These are different from the \"unknown\" tags, since we know that these may\n// have some use cases; to the best of our knowledge other tags we find, don't.\n// (for example, \"unicorn\" from \"unicorn: /value\")\nstatic const std::vector<std::string> kUnsupportedTags = {\n    \"clean-param\", \"crawl-delay\", \"host\", \"noarchive\", \"noindex\", \"nofollow\"};\n\nvoid RobotsParsingReporter::Digest(int line_num,\n                                   RobotsParsedLine::RobotsTagName parsed_tag) {\n  if (line_num > last_line_seen_) {\n    last_line_seen_ = line_num;\n  }\n  if (parsed_tag != RobotsParsedLine::kUnknown &&\n      parsed_tag != RobotsParsedLine::kUnused) {\n    ++valid_directives_;\n  }\n\n  RobotsParsedLine& line = robots_parse_results_[line_num];\n  line.line_num = line_num;\n  line.tag_name = parsed_tag;\n}\n\nvoid RobotsParsingReporter::ReportLineMetadata(int line_num,\n                                               const LineMetadata& metadata) {\n  if (line_num > last_line_seen_) {\n    last_line_seen_ = line_num;\n  }\n  RobotsParsedLine& line = robots_parse_results_[line_num];\n  line.line_num = line_num;\n  line.is_typo = metadata.is_acceptable_typo;\n  line.metadata = metadata;\n}\n\nvoid RobotsParsingReporter::HandleRobotsStart() {\n  last_line_seen_ = 0;\n  valid_directives_ = 0;\n  unused_directives_ = 0;\n}\nvoid RobotsParsingReporter::HandleRobotsEnd() {}\nvoid RobotsParsingReporter::HandleUserAgent(int line_num,\n                                            absl::string_view line_value) {\n  Digest(line_num, RobotsParsedLine::kUserAgent);\n}\nvoid RobotsParsingReporter::HandleAllow(int line_num,\n                                        absl::string_view line_value) {\n  Digest(line_num, RobotsParsedLine::kAllow);\n}\nvoid RobotsParsingReporter::HandleDisallow(int line_num,\n                                           absl::string_view line_value) {\n  Digest(line_num, RobotsParsedLine::kDisallow);\n}\nvoid RobotsParsingReporter::HandleSitemap(int line_num,\n                                          absl::string_view line_value) {\n  Digest(line_num, RobotsParsedLine::kSitemap);\n}\nvoid RobotsParsingReporter::HandleUnknownAction(int line_num,\n                                                absl::string_view action,\n                                                absl::string_view line_value) {\n  RobotsParsedLine::RobotsTagName rtn =\n      std::count(kUnsupportedTags.begin(), kUnsupportedTags.end(),\n                 absl::AsciiStrToLower(action)) > 0\n          ? RobotsParsedLine::kUnused\n          : RobotsParsedLine::kUnknown;\n  unused_directives_++;\n  Digest(line_num, rtn);\n}\n\n}  // namespace googlebot\n"
        },
        {
          "name": "reporting_robots.h",
          "type": "blob",
          "size": 2.466796875,
          "content": "#ifndef THIRD_PARTY_ROBOTSTXT_REPORTING_ROBOTS_H_\n#define THIRD_PARTY_ROBOTSTXT_REPORTING_ROBOTS_H_\n\n#include <vector>\n\n#include \"absl/container/btree_map.h\"\n#include \"absl/strings/string_view.h\"\n#include \"robots.h\"\n\nnamespace googlebot {\nstruct RobotsParsedLine {\n  enum RobotsTagName {\n    // Identifier for skipped lines. A line may be skipped because it's\n    // unparseable, or because it contains no recognizable key. Note that\n    // comment lines are also skipped, they're no-op for parsing. For example:\n    //   random characters\n    //   unicorn: <value>\n    //   # comment line\n    // Same thing for empty lines.\n    kUnknown = 0,\n    kUserAgent = 1,\n    kAllow = 2,\n    kDisallow = 3,\n    kSitemap = 4,\n    // Identifier for parseable lines whose key is recognized, but unused.\n    // See kUnsupportedTags in reporting_robots.cc for a list of recognized,\n    // but unused keys. For example:\n    //   noindex: <value>\n    //   noarchive: <value>\n    kUnused = 5,\n  };\n\n  int line_num = 0;\n  RobotsTagName tag_name = kUnknown;\n  bool is_typo = false;\n  RobotsParseHandler::LineMetadata metadata;\n};\n\nclass RobotsParsingReporter : public googlebot::RobotsParseHandler {\n public:\n  void HandleRobotsStart() override;\n  void HandleRobotsEnd() override;\n  void HandleUserAgent(int line_num, absl::string_view line_value) override;\n  void HandleAllow(int line_num, absl::string_view line_value) override;\n  void HandleDisallow(int line_num, absl::string_view line_value) override;\n  void HandleSitemap(int line_num, absl::string_view line_value) override;\n  void HandleUnknownAction(int line_num, absl::string_view action,\n                           absl::string_view line_value) override;\n  void ReportLineMetadata(int line_num, const LineMetadata& metadata) override;\n\n  int last_line_seen() const { return last_line_seen_; }\n  int valid_directives() const { return valid_directives_; }\n  int unused_directives() const { return unused_directives_; }\n  std::vector<RobotsParsedLine> parse_results() const {\n    std::vector<RobotsParsedLine> vec;\n    for (const auto& entry : robots_parse_results_) {\n      vec.push_back(entry.second);\n    }\n    return vec;\n  }\n\n private:\n  void Digest(int line_num, RobotsParsedLine::RobotsTagName parsed_tag);\n\n  // Indexed and sorted by line number.\n  absl::btree_map<int, RobotsParsedLine> robots_parse_results_;\n  int last_line_seen_ = 0;\n  int valid_directives_ = 0;\n  int unused_directives_ = 0;\n};\n}  // namespace googlebot\n#endif  // THIRD_PARTY_ROBOTSTXT_REPORTING_ROBOTS_H_\n"
        },
        {
          "name": "reporting_robots_test.cc",
          "type": "blob",
          "size": 17.0087890625,
          "content": "#include \"reporting_robots.h\"\n\n#include <ostream>\n#include <string>\n#include <vector>\n\n#include \"gtest/gtest.h\"\n#include \"absl/strings/str_cat.h\"\n#include \"absl/strings/str_split.h\"\n#include \"absl/strings/string_view.h\"\n#include \"robots.h\"\n\nusing ::googlebot::RobotsParsedLine;\nusing ::googlebot::RobotsParseHandler;\nusing ::googlebot::RobotsParsingReporter;\n\nnamespace {\n// Allows debugging the contents of the LineMetadata struct.\nstd::string LineMetadataToString(const RobotsParseHandler::LineMetadata& line) {\n  // clang-format off\n  return absl::StrCat(\n      \"{ is_empty: \", line.is_empty,\n      \" has_directive: \", line.has_directive,\n      \" has_comment: \", line.has_comment,\n      \" is_comment: \", line.is_comment,\n      \" is_acceptable_typo: \", line.is_acceptable_typo,\n      \" is_line_too_long: \", line.is_line_too_long,\n      \" is_missing_colon_separator: \", line.is_missing_colon_separator, \" }\");\n  // clang-format on\n}\n\nstd::string TagNameToString(RobotsParsedLine::RobotsTagName tag_name) {\n  switch (tag_name) {\n    case RobotsParsedLine::RobotsTagName::kUnknown:\n      return \"Unknown\";\n    case RobotsParsedLine::RobotsTagName::kUserAgent:\n      return \"UserAgent\";\n    case RobotsParsedLine::RobotsTagName::kAllow:\n      return \"Allow\";\n    case RobotsParsedLine::RobotsTagName::kDisallow:\n      return \"Disallow\";\n    case RobotsParsedLine::RobotsTagName::kSitemap:\n      return \"Sitemap\";\n    case RobotsParsedLine::RobotsTagName::kUnused:\n      return \"Unused\";\n  }\n}\n\n// Allows debugging the contents of the RobotsParsedLine struct.\nstd::string RobotsParsedLineToString(const RobotsParsedLine& line) {\n  return absl::StrCat(\"{\\n lin_num:\", line.line_num,\n                      \"\\n tag_name: \", TagNameToString(line.tag_name),\n                      \"\\n is_typo: \", line.is_typo,\n                      \"\\n metadata: \", LineMetadataToString(line.metadata),\n                      \"\\n}\");\n}\n\nvoid expectLineToParseTo(const std::vector<absl::string_view>& lines,\n                         const std::vector<RobotsParsedLine>& parse_results,\n                         const RobotsParsedLine& expected_result) {\n  int line_num = expected_result.line_num;\n  EXPECT_EQ(parse_results[line_num - 1], expected_result)\n      << \"For line \" << line_num << \": '\" << lines[line_num - 1] << \"'\";\n}\n}  // namespace\n\nnamespace googlebot {\n// This allows us to get a debug content of the object in the test. Without\n// this, it would say something like this when a test fails:\n//  Expected equality of these values:\n//   parse_results[line_num - 1]\n//    Which is: 16-byte object <01-00 00-00 01-00 00-00 00-00 00-00 01-00 00-00>\n//   expected_result\n//    Which is: 16-byte object <01-00 00-00 01-00 00-00 00-00 00-00 00-25 00-00>\nstd::ostream& operator<<(std::ostream& o, const RobotsParsedLine& line) {\n  o << RobotsParsedLineToString(line);\n  return o;\n}\n\n// These 2 `operator==` are needed for `EXPECT_EQ` to work.\nbool operator==(const RobotsParseHandler::LineMetadata& lhs,\n                const RobotsParseHandler::LineMetadata& rhs) {\n  return lhs.is_empty == rhs.is_empty &&\n         lhs.has_directive == rhs.has_directive &&\n         lhs.has_comment == rhs.has_comment &&\n         lhs.is_comment == rhs.is_comment &&\n         lhs.is_acceptable_typo == rhs.is_acceptable_typo &&\n         lhs.is_line_too_long == rhs.is_line_too_long &&\n         lhs.is_missing_colon_separator == rhs.is_missing_colon_separator;\n}\n\nbool operator==(const RobotsParsedLine& lhs, const RobotsParsedLine& rhs) {\n  return lhs.line_num == rhs.line_num && lhs.tag_name == rhs.tag_name &&\n         lhs.is_typo == rhs.is_typo && lhs.metadata == rhs.metadata;\n}\n}  // namespace googlebot\n\nTEST(RobotsUnittest, LinesNumbersAreCountedCorrectly) {\n  RobotsParsingReporter report;\n  static const char kSimpleFile[] =\n      \"User-Agent: foo\\n\"                     // 1\n      \"Allow: /some/path\\n\"                   // 2\n      \"User-Agent bar # no\\n\"                 // 3\n      \"absolutely random line\\n\"              // 4\n      \"#so comment, much wow\\n\"               // 5\n      \"\\n\"                                    // 6\n      \"unicorns: /extinct\\n\"                  // 7\n      \"noarchive: /some\\n\"                    // 8\n      \"Disallow: /\\n\"                         // 9\n      \"Error #and comment\\n\"                  // 10\n      \"useragent: baz\\n\"                      // 11\n      \"disallaw: /some\\n\"                     // 12\n      \"site-map: https://e/s.xml #comment\\n\"  // 13\n      \"sitemap: https://e/t.xml\\n\"            // 14\n      \"Noarchive: /someCapital\\n\";            // 15\n                                              // 16 (from \\n)\n  googlebot::ParseRobotsTxt(kSimpleFile, &report);\n  EXPECT_EQ(8, report.valid_directives());\n  EXPECT_EQ(16, report.last_line_seen());\n  EXPECT_EQ(report.parse_results().size(), report.last_line_seen());\n  std::vector<absl::string_view> lines = absl::StrSplit(kSimpleFile, '\\n');\n\n  // For line \"User-Agent: foo\\n\"         // 1\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 1,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUserAgent,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_missing_colon_separator = false,\n                       }});\n  // For line \"Allow: /some/path\\n\"       // 2\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 2,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kAllow,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                       }});\n  // For line \"User-Agent bar # no\\n\"    // 3\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 3,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUserAgent,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = true,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_missing_colon_separator = true,\n                       }});\n  // For line \"absolutely random line\\n\"  // 4\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 4,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUnknown,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = false,\n                           .is_missing_colon_separator = false,\n                       }});\n  // For line \"#so comment, much wow\\n\"   // 5\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 5,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUnknown,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = true,\n                           .is_comment = true,\n                           .has_directive = false,\n                       }});\n  // For line \"\\n\"                        // 6\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 6,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUnknown,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = true,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = false,\n                       }});\n  // For line \"unicorns: /extinct\\n\"      // 7\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 7,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUnknown,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                       }});\n  // For line \"noarchive: /some\\n\"        // 8\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 8,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUnused,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                       }});\n  // For line \"Disallow: /\\n\"             // 9\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 9,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kDisallow,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                       }});\n  // For line \"Error #and comment\\n\";     // 10\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 10,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUnknown,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = true,\n                           .is_comment = false,\n                           .has_directive = false,\n                           .is_missing_colon_separator = false,\n                       }});\n  // For line \"useragent: baz\\n\";         // 11\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 11,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUserAgent,\n                       .is_typo = true,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_acceptable_typo = true,\n                       }});\n  // For line \"disallaw: /some\\n\"         // 12\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 12,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kDisallow,\n                       .is_typo = true,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_acceptable_typo = true,\n                       }});\n  // For line \"site-map: https://e/s.xml #comment\\n\"  // 13;\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 13,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kSitemap,\n                       .is_typo = true,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = true,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_acceptable_typo = true,\n                       }});\n  // For line \"sitemap: https://e/t.xml\\n\"  // 14;\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 14,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kSitemap,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_acceptable_typo = false,\n                       }});\n  // For line \"Noarchive: /someCapital\\n\"        // 15\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 15,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUnused,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                       }});\n  // For line 16 (which is empty and comes from the last \\n)\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 16,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUnknown,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = true,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = false,\n                       }});\n\n  static const char kDosFile[] =\n      \"User-Agent: foo\\r\\n\"\n      \"Allow: /some/path\\r\\n\"\n      \"User-Agent: bar\\r\\n\"\n      \"\\r\\n\"\n      \"\\r\\n\"\n      \"Disallow: /\\r\\n\";\n  googlebot::ParseRobotsTxt(kDosFile, &report);\n  EXPECT_EQ(4, report.valid_directives());\n  EXPECT_EQ(7, report.last_line_seen());\n\n  static const char kMacFile[] =\n      \"User-Agent: foo\\r\"\n      \"Allow: /some/path\\r\"\n      \"User-Agent: bar\\r\"\n      \"\\r\"\n      \"\\r\"\n      \"Disallow: /\\r\";\n  googlebot::ParseRobotsTxt(kMacFile, &report);\n  EXPECT_EQ(4, report.valid_directives());\n  EXPECT_EQ(7, report.last_line_seen());\n}\n\nTEST(RobotsUnittest, LinesTooLongReportedCorrectly) {\n  RobotsParsingReporter report;\n  const int kMaxLineLen = 2084 * 8;\n  std::string allow = \"allow: /\\n\";\n  std::string disallow = \"disallow: \";\n  std::string robotstxt = \"user-agent: foo\\n\";\n  std::string longline = \"/x/\";\n  while (longline.size() < kMaxLineLen) {\n    absl::StrAppend(&longline, \"a\");\n  }\n  absl::StrAppend(&robotstxt, disallow, longline, \"\\n\", allow);\n\n  googlebot::ParseRobotsTxt(robotstxt, &report);\n  EXPECT_EQ(3, report.valid_directives());\n  EXPECT_EQ(4, report.last_line_seen());\n  EXPECT_EQ(report.parse_results().size(), report.last_line_seen());\n  std::vector<absl::string_view> lines = absl::StrSplit(robotstxt, '\\n');\n\n  // For line \"user-agent: foo\\n\"       // 1\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 1,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kUserAgent,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_line_too_long = false,\n                       }});\n  // For line \"disallow: /x/a[...]a\\n\"  // 2\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 2,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kDisallow,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_line_too_long = true,\n                       }});\n  // For line \"allow: /\\n\"              // 3\n  expectLineToParseTo(\n      lines, report.parse_results(),\n      RobotsParsedLine{.line_num = 3,\n                       .tag_name = RobotsParsedLine::RobotsTagName::kAllow,\n                       .is_typo = false,\n                       .metadata = RobotsParseHandler::LineMetadata{\n                           .is_empty = false,\n                           .has_comment = false,\n                           .is_comment = false,\n                           .has_directive = true,\n                           .is_line_too_long = false,\n                       }});\n}\n"
        },
        {
          "name": "robots.cc",
          "type": "blob",
          "size": 25.8486328125,
          "content": "// Copyright 1999 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// -----------------------------------------------------------------------------\n// File: robots.cc\n// -----------------------------------------------------------------------------\n//\n// Implements expired internet draft\n//   http://www.robotstxt.org/norobots-rfc.txt\n// with Google-specific optimizations detailed at\n//   https://developers.google.com/search/reference/robots_txt\n\n#include \"robots.h\"\n\n#include <stdlib.h>\n\n#include <cassert>\n#include <cctype>\n#include <cstddef>\n#include <cstring>\n#include <string>\n#include <vector>\n\n#include \"absl/base/macros.h\"\n#include \"absl/container/fixed_array.h\"\n#include \"absl/strings/ascii.h\"\n#include \"absl/strings/match.h\"\n#include \"absl/strings/numbers.h\"\n#include \"absl/strings/string_view.h\"\n\n// Allow for typos such as DISALOW in robots.txt.\nstatic bool kAllowFrequentTypos = true;\n\nnamespace googlebot {\n\n// A RobotsMatchStrategy defines a strategy for matching individual lines in a\n// robots.txt file. Each Match* method should return a match priority, which is\n// interpreted as:\n//\n// match priority < 0:\n//    No match.\n//\n// match priority == 0:\n//    Match, but treat it as if matched an empty pattern.\n//\n// match priority > 0:\n//    Match.\nclass RobotsMatchStrategy {\n public:\n  virtual ~RobotsMatchStrategy() {}\n\n  virtual int MatchAllow(absl::string_view path,\n                         absl::string_view pattern) = 0;\n  virtual int MatchDisallow(absl::string_view path,\n                            absl::string_view pattern) = 0;\n\n protected:\n  // Implements robots.txt pattern matching.\n  static bool Matches(absl::string_view path, absl::string_view pattern);\n};\n\n// Returns true if URI path matches the specified pattern. Pattern is anchored\n// at the beginning of path. '$' is special only at the end of pattern.\n//\n// Since 'path' and 'pattern' are both externally determined (by the webmaster),\n// we make sure to have acceptable worst-case performance.\n/* static */ bool RobotsMatchStrategy::Matches(\n    absl::string_view path, absl::string_view pattern) {\n  const size_t pathlen = path.length();\n  absl::FixedArray<size_t> pos(pathlen + 1);\n  int numpos;\n\n  // The pos[] array holds a sorted list of indexes of 'path', with length\n  // 'numpos'.  At the start and end of each iteration of the main loop below,\n  // the pos[] array will hold a list of the prefixes of the 'path' which can\n  // match the current prefix of 'pattern'. If this list is ever empty,\n  // return false. If we reach the end of 'pattern' with at least one element\n  // in pos[], return true.\n\n  pos[0] = 0;\n  numpos = 1;\n\n  for (auto pat = pattern.begin(); pat != pattern.end(); ++pat) {\n    if (*pat == '$' && pat + 1 == pattern.end()) {\n      return (pos[numpos - 1] == pathlen);\n    }\n    if (*pat == '*') {\n      numpos = pathlen - pos[0] + 1;\n      for (int i = 1; i < numpos; i++) {\n        pos[i] = pos[i-1] + 1;\n      }\n    } else {\n      // Includes '$' when not at end of pattern.\n      int newnumpos = 0;\n      for (int i = 0; i < numpos; i++) {\n        if (pos[i] < pathlen && path[pos[i]] == *pat) {\n          pos[newnumpos++] = pos[i] + 1;\n        }\n      }\n      numpos = newnumpos;\n      if (numpos == 0) return false;\n    }\n  }\n\n  return true;\n}\n\nstatic const char* kHexDigits = \"0123456789ABCDEF\";\n\n// GetPathParamsQuery is not in anonymous namespace to allow testing.\n//\n// Extracts path (with params) and query part from URL. Removes scheme,\n// authority, and fragment. Result always starts with \"/\".\n// Returns \"/\" if the url doesn't have a path or is not valid.\nstd::string GetPathParamsQuery(const std::string& url) {\n  std::string path;\n\n  // Initial two slashes are ignored.\n  size_t search_start = 0;\n  if (url.size() >= 2 && url[0] == '/' && url[1] == '/') search_start = 2;\n\n  size_t early_path = url.find_first_of(\"/?;\", search_start);\n  size_t protocol_end = url.find(\"://\", search_start);\n  if (early_path < protocol_end) {\n    // If path, param or query starts before ://, :// doesn't indicate protocol.\n    protocol_end = std::string::npos;\n  }\n  if (protocol_end == std::string::npos) {\n    protocol_end = search_start;\n  } else {\n    protocol_end += 3;\n  }\n\n  size_t path_start = url.find_first_of(\"/?;\", protocol_end);\n  if (path_start != std::string::npos) {\n    size_t hash_pos = url.find('#', search_start);\n    if (hash_pos < path_start) return \"/\";\n    size_t path_end = (hash_pos == std::string::npos) ? url.size() : hash_pos;\n    if (url[path_start] != '/') {\n      // Prepend a slash if the result would start e.g. with '?'.\n      return \"/\" + url.substr(path_start, path_end - path_start);\n    }\n    return url.substr(path_start, path_end - path_start);\n  }\n\n  return \"/\";\n}\n\n// MaybeEscapePattern is not in anonymous namespace to allow testing.\n//\n// Canonicalize the allowed/disallowed paths. For example:\n//     /SanJosSellers ==> /Sanjos%C3%A9Sellers\n//     %aa ==> %AA\n// When the function returns, (*dst) either points to src, or is newly\n// allocated.\n// Returns true if dst was newly allocated.\nbool MaybeEscapePattern(const char* src, char** dst) {\n  int num_to_escape = 0;\n  bool need_capitalize = false;\n\n  // First, scan the buffer to see if changes are needed. Most don't.\n  for (int i = 0; src[i] != 0; i++) {\n    // (a) % escape sequence.\n    if (src[i] == '%' &&\n        absl::ascii_isxdigit(src[i+1]) && absl::ascii_isxdigit(src[i+2])) {\n      if (absl::ascii_islower(src[i+1]) || absl::ascii_islower(src[i+2])) {\n        need_capitalize = true;\n      }\n      i += 2;\n    // (b) needs escaping.\n    } else if (src[i] & 0x80) {\n      num_to_escape++;\n    }\n    // (c) Already escaped and escape-characters normalized (eg. %2f -> %2F).\n  }\n  // Return if no changes needed.\n  if (!num_to_escape && !need_capitalize) {\n    (*dst) = const_cast<char*>(src);\n    return false;\n  }\n  (*dst) = new char[num_to_escape * 2 + strlen(src) + 1];\n  int j = 0;\n  for (int i = 0; src[i] != 0; i++) {\n    // (a) Normalize %-escaped sequence (eg. %2f -> %2F).\n    if (src[i] == '%' &&\n        absl::ascii_isxdigit(src[i+1]) && absl::ascii_isxdigit(src[i+2])) {\n      (*dst)[j++] = src[i++];\n      (*dst)[j++] = absl::ascii_toupper(src[i++]);\n      (*dst)[j++] = absl::ascii_toupper(src[i]);\n      // (b) %-escape octets whose highest bit is set. These are outside the\n      // ASCII range.\n    } else if (src[i] & 0x80) {\n      (*dst)[j++] = '%';\n      (*dst)[j++] = kHexDigits[(src[i] >> 4) & 0xf];\n      (*dst)[j++] = kHexDigits[src[i] & 0xf];\n    // (c) Normal character, no modification needed.\n    } else {\n      (*dst)[j++] = src[i];\n    }\n  }\n  (*dst)[j] = 0;\n  return true;\n}\n\n// Internal helper classes and functions.\nnamespace {\n\n// A robots.txt has lines of key/value pairs. A ParsedRobotsKey represents\n// a key. This class can parse a text-representation (including common typos)\n// and represent them as an enumeration which allows for faster processing\n// afterwards.\n// For unparsable keys, the original string representation is kept.\nclass ParsedRobotsKey {\n public:\n  enum KeyType {\n    // Generic highlevel fields.\n    USER_AGENT,\n    SITEMAP,\n\n    // Fields within a user-agent.\n    ALLOW,\n    DISALLOW,\n\n    // Unrecognized field; kept as-is. High number so that additions to the\n    // enumeration above does not change the serialization.\n    UNKNOWN = 128\n  };\n\n  ParsedRobotsKey() : type_(UNKNOWN) {}\n\n  // Disallow copying and assignment.\n  ParsedRobotsKey(const ParsedRobotsKey&) = delete;\n  ParsedRobotsKey& operator=(const ParsedRobotsKey&) = delete;\n\n  // Parse given key text and report in the is_acceptable_typo output parameter\n  // whether the key is one of the accepted typo-variants of a supported key.\n  // Does not copy the text, so the text_key must stay valid for the object's\n  // life-time or the next Parse() call.\n  void Parse(absl::string_view key, bool* is_acceptable_typo);\n\n  // Returns the type of key.\n  KeyType type() const { return type_; }\n\n  // If this is an unknown key, get the text.\n  absl::string_view GetUnknownText() const;\n\n private:\n  // The KeyIs* methods return whether a passed in key is one of the a supported\n  // keys, and report in the is_acceptable_typo output parameter whether the key\n  // is one of the accepted typo-variants of a supported key.\n  static bool KeyIsUserAgent(absl::string_view key, bool* is_acceptable_typo);\n  static bool KeyIsAllow(absl::string_view key, bool* is_acceptable_typo);\n  static bool KeyIsDisallow(absl::string_view key, bool* is_acceptable_typo);\n  static bool KeyIsSitemap(absl::string_view key, bool* is_acceptable_typo);\n\n  KeyType type_;\n  absl::string_view key_text_;\n};\n\nvoid EmitKeyValueToHandler(int line, const ParsedRobotsKey& key,\n                           absl::string_view value,\n                           RobotsParseHandler* handler) {\n  typedef ParsedRobotsKey Key;\n  switch (key.type()) {\n    case Key::USER_AGENT:     handler->HandleUserAgent(line, value); break;\n    case Key::ALLOW:          handler->HandleAllow(line, value); break;\n    case Key::DISALLOW:       handler->HandleDisallow(line, value); break;\n    case Key::SITEMAP:        handler->HandleSitemap(line, value); break;\n    case Key::UNKNOWN:\n      handler->HandleUnknownAction(line, key.GetUnknownText(), value);\n      break;\n      // No default case Key:: to have the compiler warn about new values.\n  }\n}\n\nclass RobotsTxtParser {\n public:\n  typedef ParsedRobotsKey Key;\n\n  RobotsTxtParser(absl::string_view robots_body,\n                  RobotsParseHandler* handler)\n      : robots_body_(robots_body), handler_(handler) {\n  }\n\n  void Parse();\n\n private:\n  // Note that `key` and `value` are only set when `metadata->has_directive\n  // == true`.\n  static void GetKeyAndValueFrom(char** key, char** value, char* line,\n                                 RobotsParseHandler::LineMetadata* metadata);\n  static void StripWhitespaceSlowly(char ** s);\n\n  void ParseAndEmitLine(int current_line, char* line,\n                        bool* line_too_long_strict);\n  bool NeedEscapeValueForKey(const Key& key);\n\n  absl::string_view robots_body_;\n  RobotsParseHandler* const handler_;\n};\n\nbool RobotsTxtParser::NeedEscapeValueForKey(const Key& key) {\n  switch (key.type()) {\n    case RobotsTxtParser::Key::USER_AGENT:\n    case RobotsTxtParser::Key::SITEMAP:\n      return false;\n    default:\n      return true;\n  }\n}\n\n// Removes leading and trailing whitespace from null-terminated string s.\n/* static */ void RobotsTxtParser::StripWhitespaceSlowly(char ** s) {\n  absl::string_view stripped = absl::StripAsciiWhitespace(*s);\n  *s = const_cast<char*>(stripped.data());\n  (*s)[stripped.size()] = '\\0';\n}\n\nvoid RobotsTxtParser::GetKeyAndValueFrom(\n    char** key, char** value, char* line,\n    RobotsParseHandler::LineMetadata* metadata) {\n  // Remove comments from the current robots.txt line.\n  char* const comment = strchr(line, '#');\n  if (nullptr != comment) {\n    metadata->has_comment = true;\n    *comment = '\\0';\n  }\n  StripWhitespaceSlowly(&line);\n  // If the line became empty after removing the comment, return.\n  if (strlen(line) == 0) {\n    if (metadata->has_comment) {\n      metadata->is_comment = true;\n    } else {\n      metadata->is_empty = true;\n    }\n    return;\n  }\n\n  // Rules must match the following pattern:\n  //   <key>[ \\t]*:[ \\t]*<value>\n  char* sep = strchr(line, ':');\n  if (nullptr == sep) {\n    // Google-specific optimization: some people forget the colon, so we need to\n    // accept whitespace in its stead.\n    static const char * const kWhite = \" \\t\";\n    sep = strpbrk(line, kWhite);\n    if (nullptr != sep) {\n      const char* const val = sep + strspn(sep, kWhite);\n      assert(*val);  // since we dropped trailing whitespace above.\n      if (nullptr != strpbrk(val, kWhite)) {\n        // We only accept whitespace as a separator if there are exactly two\n        // sequences of non-whitespace characters.  If we get here, there were\n        // more than 2 such sequences since we stripped trailing whitespace\n        // above.\n        return;\n      }\n      metadata->is_missing_colon_separator = true;\n    }\n  }\n  if (nullptr == sep) {\n    return;                     // Couldn't find a separator.\n  }\n\n  *key = line;                        // Key starts at beginning of line.\n  *sep = '\\0';                        // And stops at the separator.\n  StripWhitespaceSlowly(key);         // Get rid of any trailing whitespace.\n\n  if (strlen(*key) > 0) {\n    *value = 1 + sep;                 // Value starts after the separator.\n    StripWhitespaceSlowly(value);     // Get rid of any leading whitespace.\n    metadata->has_directive = true;\n    return;\n  }\n}\n\nvoid RobotsTxtParser::ParseAndEmitLine(int current_line, char* line,\n                                       bool* line_too_long_strict) {\n  char* string_key;\n  char* value;\n  RobotsParseHandler::LineMetadata line_metadata;\n  // Note that `string_key` and `value` are only set when\n  // `line_metadata->has_directive == true`.\n  GetKeyAndValueFrom(&string_key, &value, line, &line_metadata);\n  line_metadata.is_line_too_long = *line_too_long_strict;\n  if (!line_metadata.has_directive) {\n    handler_->ReportLineMetadata(current_line, line_metadata);\n    return;\n  }\n  Key key;\n  key.Parse(string_key, &line_metadata.is_acceptable_typo);\n  if (NeedEscapeValueForKey(key)) {\n    char* escaped_value = nullptr;\n    const bool is_escaped = MaybeEscapePattern(value, &escaped_value);\n    EmitKeyValueToHandler(current_line, key, escaped_value, handler_);\n    if (is_escaped) delete[] escaped_value;\n  } else {\n    EmitKeyValueToHandler(current_line, key, value, handler_);\n  }\n  // Finish adding metadata to the line.\n  handler_->ReportLineMetadata(current_line, line_metadata);\n}\n\nvoid RobotsTxtParser::Parse() {\n  // UTF-8 byte order marks.\n  static const unsigned char utf_bom[3] = {0xEF, 0xBB, 0xBF};\n\n  // Certain browsers limit the URL length to 2083 bytes. In a robots.txt, it's\n  // fairly safe to assume any valid line isn't going to be more than many times\n  // that max url length of 2KB. We want some padding for\n  // UTF-8 encoding/nulls/etc. but a much smaller bound would be okay as well.\n  // If so, we can ignore the chars on a line past that.\n  const int kBrowserMaxLineLen = 2083;\n  const int kMaxLineLen = kBrowserMaxLineLen * 8;\n  // Allocate a buffer used to process the current line.\n  char* const line_buffer = new char[kMaxLineLen];\n  // last_line_pos is the last writeable pos within the line array\n  // (only a final '\\0' may go here).\n  const char* const line_buffer_end = line_buffer + kMaxLineLen - 1;\n  bool line_too_long_strict = false;\n  char* line_pos = line_buffer;\n  int line_num = 0;\n  size_t bom_pos = 0;\n  bool last_was_carriage_return = false;\n  handler_->HandleRobotsStart();\n\n      {\n        for (const unsigned char ch : robots_body_) {\n      ABSL_ASSERT(line_pos <= line_buffer_end);\n      // Google-specific optimization: UTF-8 byte order marks should never\n      // appear in a robots.txt file, but they do nevertheless. Skipping\n      // possible BOM-prefix in the first bytes of the input.\n      if (bom_pos < sizeof(utf_bom) && ch == utf_bom[bom_pos++]) {\n        continue;\n      }\n      bom_pos = sizeof(utf_bom);\n      if (ch != 0x0A && ch != 0x0D) {  // Non-line-ending char case.\n        // Put in next spot on current line, as long as there's room.\n        if (line_pos < line_buffer_end) {\n          *(line_pos++) = ch;\n        } else {\n          line_too_long_strict = true;\n        }\n      } else {                         // Line-ending character char case.\n        *line_pos = '\\0';\n        // Only emit an empty line if this was not due to the second character\n        // of the DOS line-ending \\r\\n .\n        const bool is_CRLF_continuation =\n            (line_pos == line_buffer) && last_was_carriage_return && ch == 0x0A;\n        if (!is_CRLF_continuation) {\n          ParseAndEmitLine(++line_num, line_buffer, &line_too_long_strict);\n          line_too_long_strict = false;\n        }\n        line_pos = line_buffer;\n        last_was_carriage_return = (ch == 0x0D);\n      }\n    }\n  }\n  *line_pos = '\\0';\n  ParseAndEmitLine(++line_num, line_buffer, &line_too_long_strict);\n  handler_->HandleRobotsEnd();\n  delete [] line_buffer;\n}\n\n// Implements the default robots.txt matching strategy. The maximum number of\n// characters matched by a pattern is returned as its match priority.\nclass LongestMatchRobotsMatchStrategy : public RobotsMatchStrategy {\n public:\n  LongestMatchRobotsMatchStrategy() { }\n\n  // Disallow copying and assignment.\n  LongestMatchRobotsMatchStrategy(const LongestMatchRobotsMatchStrategy&) =\n      delete;\n  LongestMatchRobotsMatchStrategy& operator=(\n      const LongestMatchRobotsMatchStrategy&) = delete;\n\n  int MatchAllow(absl::string_view path, absl::string_view pattern) override;\n  int MatchDisallow(absl::string_view path, absl::string_view pattern) override;\n};\n}  // end anonymous namespace\n\nvoid ParseRobotsTxt(absl::string_view robots_body,\n                    RobotsParseHandler* parse_callback) {\n  RobotsTxtParser parser(robots_body, parse_callback);\n  parser.Parse();\n}\n\nRobotsMatcher::RobotsMatcher()\n    : seen_global_agent_(false),\n      seen_specific_agent_(false),\n      ever_seen_specific_agent_(false),\n      seen_separator_(false),\n      path_(nullptr),\n      user_agents_(nullptr) {\n  match_strategy_ = new LongestMatchRobotsMatchStrategy();\n}\n\nRobotsMatcher::~RobotsMatcher() {\n  delete match_strategy_;\n}\n\nbool RobotsMatcher::ever_seen_specific_agent() const {\n  return ever_seen_specific_agent_;\n}\n\nvoid RobotsMatcher::InitUserAgentsAndPath(\n    const std::vector<std::string>* user_agents, const char* path) {\n  // The RobotsParser object doesn't own path_ or user_agents_, so overwriting\n  // these pointers doesn't cause a memory leak.\n  path_ = path;\n  ABSL_ASSERT('/' == *path_);\n  user_agents_ = user_agents;\n}\n\nbool RobotsMatcher::AllowedByRobots(absl::string_view robots_body,\n                                    const std::vector<std::string>* user_agents,\n                                    const std::string& url) {\n  // The url is not normalized (escaped, percent encoded) here because the user\n  // is asked to provide it in escaped form already.\n  std::string path = GetPathParamsQuery(url);\n  InitUserAgentsAndPath(user_agents, path.c_str());\n  ParseRobotsTxt(robots_body, this);\n  return !disallow();\n}\n\nbool RobotsMatcher::OneAgentAllowedByRobots(absl::string_view robots_txt,\n                                            const std::string& user_agent,\n                                            const std::string& url) {\n  std::vector<std::string> v;\n  v.push_back(user_agent);\n  return AllowedByRobots(robots_txt, &v, url);\n}\n\nbool RobotsMatcher::disallow() const {\n  if (allow_.specific.priority() > 0 || disallow_.specific.priority() > 0) {\n    return (disallow_.specific.priority() > allow_.specific.priority());\n  }\n\n  if (ever_seen_specific_agent_) {\n    // Matching group for user-agent but either without disallow or empty one,\n    // i.e. priority == 0.\n    return false;\n  }\n\n  if (disallow_.global.priority() > 0 || allow_.global.priority() > 0) {\n    return disallow_.global.priority() > allow_.global.priority();\n  }\n  return false;\n}\n\nbool RobotsMatcher::disallow_ignore_global() const {\n  if (allow_.specific.priority() > 0 || disallow_.specific.priority() > 0) {\n    return disallow_.specific.priority() > allow_.specific.priority();\n  }\n  return false;\n}\n\nint RobotsMatcher::matching_line() const {\n  if (ever_seen_specific_agent_) {\n    return Match::HigherPriorityMatch(disallow_.specific, allow_.specific)\n        .line();\n  }\n  return Match::HigherPriorityMatch(disallow_.global, allow_.global).line();\n}\n\nvoid RobotsMatcher::HandleRobotsStart() {\n  // This is a new robots.txt file, so we need to reset all the instance member\n  // variables. We do it in the same order the instance member variables are\n  // declared, so it's easier to keep track of which ones we have (or maybe\n  // haven't!) done.\n  allow_.Clear();\n  disallow_.Clear();\n\n  seen_global_agent_ = false;\n  seen_specific_agent_ = false;\n  ever_seen_specific_agent_ = false;\n  seen_separator_ = false;\n}\n\n/*static*/ absl::string_view RobotsMatcher::ExtractUserAgent(\n    absl::string_view user_agent) {\n  // Allowed characters in user-agent are [a-zA-Z_-].\n  const char* end = user_agent.data();\n  while (absl::ascii_isalpha(*end) || *end == '-' || *end == '_') {\n    ++end;\n  }\n  return user_agent.substr(0, end - user_agent.data());\n}\n\n/*static*/ bool RobotsMatcher::IsValidUserAgentToObey(\n    absl::string_view user_agent) {\n  return user_agent.length() > 0 && ExtractUserAgent(user_agent) == user_agent;\n}\n\nvoid RobotsMatcher::HandleUserAgent(int line_num,\n                                    absl::string_view user_agent) {\n  if (seen_separator_) {\n    seen_specific_agent_ = seen_global_agent_ = seen_separator_ = false;\n  }\n\n  // Google-specific optimization: a '*' followed by space and more characters\n  // in a user-agent record is still regarded a global rule.\n  if (user_agent.length() >= 1 && user_agent[0] == '*' &&\n      (user_agent.length() == 1 || isspace(user_agent[1]))) {\n    seen_global_agent_ = true;\n  } else {\n    user_agent = ExtractUserAgent(user_agent);\n    for (const auto& agent : *user_agents_) {\n      if (absl::EqualsIgnoreCase(user_agent, agent)) {\n        ever_seen_specific_agent_ = seen_specific_agent_ = true;\n        break;\n      }\n    }\n  }\n}\n\nvoid RobotsMatcher::HandleAllow(int line_num, absl::string_view value) {\n  if (!seen_any_agent()) return;\n  seen_separator_ = true;\n  const int priority = match_strategy_->MatchAllow(path_, value);\n  if (priority >= 0) {\n    if (seen_specific_agent_) {\n      if (allow_.specific.priority() < priority) {\n        allow_.specific.Set(priority, line_num);\n      }\n    } else {\n      assert(seen_global_agent_);\n      if (allow_.global.priority() < priority) {\n        allow_.global.Set(priority, line_num);\n      }\n    }\n  } else {\n    // Google-specific optimization: 'index.htm' and 'index.html' are normalized\n    // to '/'.\n    const size_t slash_pos = value.find_last_of('/');\n\n    if (slash_pos != absl::string_view::npos &&\n        absl::StartsWith(absl::ClippedSubstr(value, slash_pos),\n                            \"/index.htm\")) {\n      const int len = slash_pos + 1;\n      absl::FixedArray<char> newpattern(len + 1);\n      strncpy(newpattern.data(), value.data(), len);\n      newpattern[len] = '$';\n      HandleAllow(line_num,\n                  absl::string_view(newpattern.data(), newpattern.size()));\n    }\n  }\n}\n\nvoid RobotsMatcher::HandleDisallow(int line_num, absl::string_view value) {\n  if (!seen_any_agent()) return;\n  seen_separator_ = true;\n  const int priority = match_strategy_->MatchDisallow(path_, value);\n  if (priority >= 0) {\n    if (seen_specific_agent_) {\n      if (disallow_.specific.priority() < priority) {\n        disallow_.specific.Set(priority, line_num);\n      }\n    } else {\n      assert(seen_global_agent_);\n      if (disallow_.global.priority() < priority) {\n        disallow_.global.Set(priority, line_num);\n      }\n    }\n  }\n}\n\nint LongestMatchRobotsMatchStrategy::MatchAllow(absl::string_view path,\n                                                absl::string_view pattern) {\n  return Matches(path, pattern) ? pattern.length() : -1;\n}\n\nint LongestMatchRobotsMatchStrategy::MatchDisallow(absl::string_view path,\n                                                   absl::string_view pattern) {\n  return Matches(path, pattern) ? pattern.length() : -1;\n}\n\nvoid RobotsMatcher::HandleSitemap(int line_num, absl::string_view value) {}\n\nvoid RobotsMatcher::HandleUnknownAction(int line_num, absl::string_view action,\n                                        absl::string_view value) {}\n\nvoid ParsedRobotsKey::Parse(absl::string_view key, bool* is_acceptable_typo) {\n  key_text_ = absl::string_view();\n  if (KeyIsUserAgent(key, is_acceptable_typo)) {\n    type_ = USER_AGENT;\n  } else if (KeyIsAllow(key, is_acceptable_typo)) {\n    type_ = ALLOW;\n  } else if (KeyIsDisallow(key, is_acceptable_typo)) {\n    type_ = DISALLOW;\n  } else if (KeyIsSitemap(key, is_acceptable_typo)) {\n    type_ = SITEMAP;\n  } else {\n    type_ = UNKNOWN;\n    key_text_ = key;\n  }\n}\n\nabsl::string_view ParsedRobotsKey::GetUnknownText() const {\n  ABSL_ASSERT(type_ == UNKNOWN && !key_text_.empty());\n  return key_text_;\n}\n\nbool ParsedRobotsKey::KeyIsUserAgent(absl::string_view key,\n                                     bool* is_acceptable_typo) {\n  *is_acceptable_typo =\n      (kAllowFrequentTypos && (absl::StartsWithIgnoreCase(key, \"useragent\") ||\n                               absl::StartsWithIgnoreCase(key, \"user agent\")));\n  return (absl::StartsWithIgnoreCase(key, \"user-agent\") || *is_acceptable_typo);\n}\n\nbool ParsedRobotsKey::KeyIsAllow(absl::string_view key,\n                                 bool* is_acceptable_typo) {\n  // We don't support typos for the \"allow\" key.\n  *is_acceptable_typo = false;\n  return absl::StartsWithIgnoreCase(key, \"allow\");\n}\n\nbool ParsedRobotsKey::KeyIsDisallow(absl::string_view key,\n                                    bool* is_acceptable_typo) {\n  *is_acceptable_typo =\n      (kAllowFrequentTypos && ((absl::StartsWithIgnoreCase(key, \"dissallow\")) ||\n                               (absl::StartsWithIgnoreCase(key, \"dissalow\")) ||\n                               (absl::StartsWithIgnoreCase(key, \"disalow\")) ||\n                               (absl::StartsWithIgnoreCase(key, \"diasllow\")) ||\n                               (absl::StartsWithIgnoreCase(key, \"disallaw\"))));\n  return (absl::StartsWithIgnoreCase(key, \"disallow\") || *is_acceptable_typo);\n}\n\nbool ParsedRobotsKey::KeyIsSitemap(absl::string_view key,\n                                   bool* is_acceptable_typo) {\n  *is_acceptable_typo =\n      (kAllowFrequentTypos && (absl::StartsWithIgnoreCase(key, \"site-map\")));\n  return absl::StartsWithIgnoreCase(key, \"sitemap\") || *is_acceptable_typo;\n}\n\n}  // namespace googlebot\n"
        },
        {
          "name": "robots.h",
          "type": "blob",
          "size": 10.3876953125,
          "content": "// Copyright 1999 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// -----------------------------------------------------------------------------\n// File: robots.h\n// -----------------------------------------------------------------------------\n//\n// This file implements the standard defined by the Robots Exclusion Protocol\n// (REP) internet draft (I-D).\n//   https://www.rfc-editor.org/rfc/rfc9309.html\n//\n// Google doesn't follow the standard strictly, because there are a lot of\n// non-conforming robots.txt files out there, and we err on the side of\n// disallowing when this seems intended.\n//\n// An more user-friendly description of how Google handles robots.txt can be\n// found at:\n//   https://developers.google.com/search/docs/crawling-indexing/robots/robots_txt\n//\n// This library provides a low-level parser for robots.txt (ParseRobotsTxt()),\n// and a matcher for URLs against a robots.txt (class RobotsMatcher).\n\n#ifndef THIRD_PARTY_ROBOTSTXT_ROBOTS_H__\n#define THIRD_PARTY_ROBOTSTXT_ROBOTS_H__\n\n#include <string>\n#include <vector>\n\n#include \"absl/strings/string_view.h\"\n\nnamespace googlebot {\n// Handler for directives found in robots.txt. These callbacks are called by\n// ParseRobotsTxt() in the sequence they have been found in the file.\nclass RobotsParseHandler {\n public:\n  RobotsParseHandler() {}\n  virtual ~RobotsParseHandler() {}\n\n  // Disallow copying and assignment.\n  RobotsParseHandler(const RobotsParseHandler&) = delete;\n  RobotsParseHandler& operator=(const RobotsParseHandler&) = delete;\n\n  virtual void HandleRobotsStart() = 0;\n  virtual void HandleRobotsEnd() = 0;\n\n  virtual void HandleUserAgent(int line_num, absl::string_view value) = 0;\n  virtual void HandleAllow(int line_num, absl::string_view value) = 0;\n  virtual void HandleDisallow(int line_num, absl::string_view value) = 0;\n\n  virtual void HandleSitemap(int line_num, absl::string_view value) = 0;\n\n  // Any other unrecognized name/value pairs.\n  virtual void HandleUnknownAction(int line_num, absl::string_view action,\n                                   absl::string_view value) = 0;\n\n  struct LineMetadata {\n    // Indicates if the line is totally empty.\n    bool is_empty = false;\n    // Indicates if the line has a comment (may have content before it).\n    bool has_comment = false;\n    // Indicates if the whole line is a comment.\n    bool is_comment = false;\n    // Indicates that the line has a valid robots.txt directive and one of the\n    // `Handle*` methods will be called.\n    bool has_directive = false;\n    // Indicates that the found directive is one of the acceptable typo variants\n    // of the directive. See the key functions in ParsedRobotsKey for accepted\n    // typos.\n    bool is_acceptable_typo = false;\n    // Indicates that the line is too long, specifically over 2083 * 8 bytes.\n    bool is_line_too_long = false;\n    // Indicates that the key-value pair is missing the colon separator.\n    bool is_missing_colon_separator = false;\n  };\n\n  virtual void ReportLineMetadata(int line_num, const LineMetadata& metadata) {}\n};\n\n// Parses body of a robots.txt and emits parse callbacks. This will accept\n// typical typos found in robots.txt, such as 'disalow'.\n//\n// Note, this function will accept all kind of input but will skip\n// everything that does not look like a robots directive.\nvoid ParseRobotsTxt(absl::string_view robots_body,\n                    RobotsParseHandler* parse_callback);\n\n// RobotsMatcher - matches robots.txt against URLs.\n//\n// The Matcher uses a default match strategy for Allow/Disallow patterns which\n// is the official way of Google crawler to match robots.txt. It is also\n// possible to provide a custom match strategy.\n//\n// The entry point for the user is to call one of the *AllowedByRobots()\n// methods that return directly if a URL is being allowed according to the\n// robots.txt and the crawl agent.\n// The RobotsMatcher can be re-used for URLs/robots.txt but is not thread-safe.\nclass RobotsMatchStrategy;\nclass RobotsMatcher : protected RobotsParseHandler {\n public:\n  // Create a RobotsMatcher with the default matching strategy. The default\n  // matching strategy is longest-match as opposed to the former internet draft\n  // that provisioned first-match strategy. Analysis shows that longest-match,\n  // while more restrictive for crawlers, is what webmasters assume when writing\n  // directives. For example, in case of conflicting matches (both Allow and\n  // Disallow), the longest match is the one the user wants. For example, in\n  // case of a robots.txt file that has the following rules\n  //   Allow: /\n  //   Disallow: /cgi-bin\n  // it's pretty obvious what the webmaster wants: they want to allow crawl of\n  // every URI except /cgi-bin. However, according to the expired internet\n  // standard, crawlers should be allowed to crawl everything with such a rule.\n  RobotsMatcher();\n\n  ~RobotsMatcher() override;\n\n  // Disallow copying and assignment.\n  RobotsMatcher(const RobotsMatcher&) = delete;\n  RobotsMatcher& operator=(const RobotsMatcher&) = delete;\n\n  // Verifies that the given user agent is valid to be matched against\n  // robots.txt. Valid user agent strings only contain the characters\n  // [a-zA-Z_-].\n  static bool IsValidUserAgentToObey(absl::string_view user_agent);\n\n  // Returns true iff 'url' is allowed to be fetched by any member of the\n  // \"user_agents\" vector. 'url' must be %-encoded according to RFC3986.\n  bool AllowedByRobots(absl::string_view robots_body,\n                       const std::vector<std::string>* user_agents,\n                       const std::string& url);\n\n  // Do robots check for 'url' when there is only one user agent. 'url' must\n  // be %-encoded according to RFC3986.\n  bool OneAgentAllowedByRobots(absl::string_view robots_txt,\n                               const std::string& user_agent,\n                               const std::string& url);\n\n  // Returns true if we are disallowed from crawling a matching URI.\n  bool disallow() const;\n\n  // Returns true if we are disallowed from crawling a matching URI. Ignores any\n  // rules specified for the default user agent, and bases its results only on\n  // the specified user agents.\n  bool disallow_ignore_global() const;\n\n  // Returns true iff, when AllowedByRobots() was called, the robots file\n  // referred explicitly to one of the specified user agents.\n  bool ever_seen_specific_agent() const;\n\n  // Returns the line that matched or 0 if none matched.\n  int matching_line() const;\n\n protected:\n  // Parse callbacks.\n  // Protected because used in unittest. Never override RobotsMatcher, implement\n  // googlebot::RobotsParseHandler instead.\n  void HandleRobotsStart() override;\n  void HandleRobotsEnd() override {}\n\n  void HandleUserAgent(int line_num, absl::string_view user_agent) override;\n  void HandleAllow(int line_num, absl::string_view value) override;\n  void HandleDisallow(int line_num, absl::string_view value) override;\n\n  void HandleSitemap(int line_num, absl::string_view value) override;\n  void HandleUnknownAction(int line_num, absl::string_view action,\n                           absl::string_view value) override;\n\n protected:\n  // Extract the matchable part of a user agent string, essentially stopping at\n  // the first invalid character.\n  // Example: 'Googlebot/2.1' becomes 'Googlebot'\n  static absl::string_view ExtractUserAgent(absl::string_view user_agent);\n\n  // Initialize next path and user-agents to check. Path must contain only the\n  // path, params, and query (if any) of the url and must start with a '/'.\n  void InitUserAgentsAndPath(const std::vector<std::string>* user_agents,\n                             const char* path);\n\n  // Returns true if any user-agent was seen.\n  bool seen_any_agent() const {\n    return seen_global_agent_ || seen_specific_agent_;\n  }\n\n  // Instead of just maintaining a Boolean indicating whether a given line has\n  // matched, we maintain a count of the maximum number of characters matched by\n  // that pattern.\n  //\n  // This structure stores the information associated with a match (e.g. when a\n  // Disallow is matched) as priority of the match and line matching.\n  //\n  // The priority is initialized with a negative value to make sure that a match\n  // of priority 0 is higher priority than no match at all.\n  class Match {\n   private:\n    static const int kNoMatchPriority = -1;\n\n   public:\n    Match(int priority, int line) : priority_(priority), line_(line) {}\n    Match() : priority_(kNoMatchPriority), line_(0) {}\n\n    void Set(int priority, int line) {\n      priority_ = priority;\n      line_ = line;\n    }\n\n    void Clear() { Set(kNoMatchPriority, 0); }\n\n    int line() const { return line_; }\n    int priority() const { return priority_; }\n\n    static const Match& HigherPriorityMatch(const Match& a, const Match& b) {\n      if (a.priority() > b.priority()) {\n        return a;\n      } else {\n        return b;\n      }\n    }\n\n   private:\n    int priority_;\n    int line_;\n  };\n\n  // For each of the directives within user-agents, we keep global and specific\n  // match scores.\n  struct MatchHierarchy {\n    Match global;    // Match for '*'\n    Match specific;  // Match for queried agent.\n    void Clear() {\n      global.Clear();\n      specific.Clear();\n    }\n  };\n  MatchHierarchy allow_;     // Characters of 'url' matching Allow.\n  MatchHierarchy disallow_;  // Characters of 'url' matching Disallow.\n\n  bool seen_global_agent_;         // True if processing global agent rules.\n  bool seen_specific_agent_;       // True if processing our specific agent.\n  bool ever_seen_specific_agent_;  // True if we ever saw a block for our agent.\n  bool seen_separator_;            // True if saw any key: value pair.\n\n  // The path we want to pattern match. Not owned and only a valid pointer\n  // during the lifetime of *AllowedByRobots calls.\n  const char* path_;\n  // The User-Agents we are interested in. Not owned and only a valid\n  // pointer during the lifetime of *AllowedByRobots calls.\n  const std::vector<std::string>* user_agents_;\n\n  RobotsMatchStrategy* match_strategy_;\n};\n\n}  // namespace googlebot\n#endif  // THIRD_PARTY_ROBOTSTXT_ROBOTS_H__\n"
        },
        {
          "name": "robots_main.cc",
          "type": "blob",
          "size": 4.0283203125,
          "content": "// Copyright 2019 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// -----------------------------------------------------------------------------\n// File: robots_main.cc\n// -----------------------------------------------------------------------------\n//\n// Simple binary to assess whether a URL is accessible to a user-agent according\n// to records found in a local robots.txt file, based on Google's robots.txt\n// parsing and matching algorithms.\n// Usage:\n//     robots_main <local_path_to_robotstxt> <user_agent> <url>\n// Arguments:\n// local_path_to_robotstxt: local path to a file containing robots.txt records.\n//   For example: /home/users/username/robots.txt\n// user_agent: a token to be matched against records in the robots.txt.\n//   For example: Googlebot\n// url: a url to be matched against records in the robots.txt. The URL must be\n// %-encoded according to RFC3986.\n//   For example: https://example.com/accessible/url.html\n// Output: Prints a sentence with verdict about whether 'user_agent' is allowed\n// to access 'url' based on records in 'local_path_to_robotstxt'.\n// Return code:\n//   0 when the url is ALLOWED for the user_agent.\n//   1 when the url is DISALLOWED for the user_agent.\n//   2 when --help is requested or if there is something invalid in the flags\n//   passed.\n//\n#include <fstream>\n#include <iostream>\n\n#include \"robots.h\"\n\nbool LoadFile(const std::string& filename, std::string* result) {\n  std::ifstream file(filename, std::ios::in | std::ios::binary | std::ios::ate);\n  if (file.is_open()) {\n    size_t size = file.tellg();\n    std::vector<char> buffer(size);\n    file.seekg(0, std::ios::beg);\n    file.read(buffer.data(), size);\n    file.close();\n    if (!file) return false;  // file reading error (failbit or badbit).\n    result->assign(buffer.begin(), buffer.end());\n    return true;\n  }\n  return false;\n}\n\nvoid ShowHelp(int argc, char** argv) {\n  std::cerr << \"Shows whether the given user_agent and URI combination\"\n            << \" is allowed or disallowed by the given robots.txt file. \"\n            << std::endl\n            << std::endl;\n  std::cerr << \"Usage: \" << std::endl\n            << \"  \" << argv[0] << \" <robots.txt filename> <user_agent> <URI>\"\n            << std::endl\n            << std::endl;\n  std::cerr << \"The URI must be %-encoded according to RFC3986.\" << std::endl\n            << std::endl;\n  std::cerr << \"Example: \" << std::endl\n            << \"  \" << argv[0] << \" robots.txt FooBot http://example.com/foo\"\n            << std::endl;\n}\n\nint main(int argc, char** argv) {\n  std::string filename = argc >= 2 ? argv[1] : \"\";\n  if (filename == \"-h\" || filename == \"-help\" || filename == \"--help\") {\n    ShowHelp(argc, argv);\n    return 2;\n  }\n  if (argc != 4) {\n    std::cerr << \"Invalid amount of arguments. Showing help.\" << std::endl\n              << std::endl;\n    ShowHelp(argc, argv);\n    return 2;\n  }\n  std::string robots_content;\n  if (!(LoadFile(filename, &robots_content))) {\n    std::cerr << \"failed to read file \\\"\" << filename << \"\\\"\" << std::endl;\n    return 2;\n  }\n\n  std::string user_agent = argv[2];\n  std::vector<std::string> user_agents(1, user_agent);\n  googlebot::RobotsMatcher matcher;\n  std::string url = argv[3];\n  bool allowed = matcher.AllowedByRobots(robots_content, &user_agents, url);\n\n  std::cout << \"user-agent '\" << user_agent << \"' with URI '\" << argv[3]\n            << \"': \" << (allowed ? \"ALLOWED\" : \"DISALLOWED\") << std::endl;\n  if (robots_content.empty()) {\n    std::cout << \"notice: robots file is empty so all user-agents are allowed\"\n              << std::endl;\n  }\n\n  return allowed ? 0 : 1;\n}\n"
        },
        {
          "name": "robots_test.cc",
          "type": "blob",
          "size": 37.00390625,
          "content": "// Copyright 2019 Google LLC\n//\n// Licensed under the Apache License, Version 2.0 (the \"License\");\n// you may not use this file except in compliance with the License.\n// You may obtain a copy of the License at\n//\n//     https://www.apache.org/licenses/LICENSE-2.0\n//\n// Unless required by applicable law or agreed to in writing, software\n// distributed under the License is distributed on an \"AS IS\" BASIS,\n// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n// See the License for the specific language governing permissions and\n// limitations under the License.\n//\n// This file tests the robots.txt parsing and matching code found in robots.cc\n// against the current Robots Exclusion Protocol (REP) RFC.\n// https://www.rfc-editor.org/rfc/rfc9309.html\n#include \"robots.h\"\n\n#include <string>\n\n#include \"gtest/gtest.h\"\n#include \"absl/strings/str_cat.h\"\n#include \"absl/strings/string_view.h\"\n\nnamespace {\n\nusing ::googlebot::RobotsMatcher;\n\nbool IsUserAgentAllowed(const absl::string_view robotstxt,\n                        const std::string& useragent, const std::string& url) {\n  RobotsMatcher matcher;\n  return matcher.OneAgentAllowedByRobots(robotstxt, useragent, url);\n}\n\n// Google-specific: system test.\nTEST(RobotsUnittest, GoogleOnly_SystemTest) {\n  const absl::string_view robotstxt =\n      \"user-agent: FooBot\\n\"\n      \"disallow: /\\n\";\n  // Empty robots.txt: everything allowed.\n  EXPECT_TRUE(IsUserAgentAllowed(\"\", \"FooBot\", \"\"));\n\n  // Empty user-agent to be matched: everything allowed.\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"\", \"\"));\n\n  // Empty url: implicitly disallowed, see method comment for GetPathParamsQuery\n  // in robots.cc.\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"\"));\n\n  // All params empty: same as robots.txt empty, everything allowed.\n  EXPECT_TRUE(IsUserAgentAllowed(\"\", \"\", \"\"));\n}\n// Rules are colon separated name-value pairs. The following names are\n// provisioned:\n//     user-agent: <value>\n//     allow: <value>\n//     disallow: <value>\n// See REP RFC section \"Protocol Definition\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.1\n//\n// Google specific: webmasters sometimes miss the colon separator, but it's\n// obvious what they mean by \"disallow /\", so we assume the colon if it's\n// missing.\nTEST(RobotsUnittest, ID_LineSyntax_Line) {\n  const absl::string_view robotstxt_correct =\n      \"user-agent: FooBot\\n\"\n      \"disallow: /\\n\";\n  const absl::string_view robotstxt_incorrect =\n      \"foo: FooBot\\n\"\n      \"bar: /\\n\";\n  const absl::string_view robotstxt_incorrect_accepted =\n      \"user-agent FooBot\\n\"\n      \"disallow /\\n\";\n  const std::string url = \"http://foo.bar/x/y\";\n\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_correct, \"FooBot\", url));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_incorrect, \"FooBot\", url));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_incorrect_accepted, \"FooBot\", url));\n}\n\n// A group is one or more user-agent line followed by rules, and terminated\n// by a another user-agent line. Rules for same user-agents are combined\n// opaquely into one group. Rules outside groups are ignored.\n// See REP RFC section \"Protocol Definition\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.1\nTEST(RobotsUnittest, ID_LineSyntax_Groups) {\n  const absl::string_view robotstxt =\n      \"allow: /foo/bar/\\n\"\n      \"\\n\"\n      \"user-agent: FooBot\\n\"\n      \"disallow: /\\n\"\n      \"allow: /x/\\n\"\n      \"user-agent: BarBot\\n\"\n      \"disallow: /\\n\"\n      \"allow: /y/\\n\"\n      \"\\n\"\n      \"\\n\"\n      \"allow: /w/\\n\"\n      \"user-agent: BazBot\\n\"\n      \"\\n\"\n      \"user-agent: FooBot\\n\"\n      \"allow: /z/\\n\"\n      \"disallow: /\\n\";\n\n  const std::string url_w = \"http://foo.bar/w/a\";\n  const std::string url_x = \"http://foo.bar/x/b\";\n  const std::string url_y = \"http://foo.bar/y/c\";\n  const std::string url_z = \"http://foo.bar/z/d\";\n  const std::string url_foo = \"http://foo.bar/foo/bar/\";\n\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url_x));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url_z));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", url_y));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"BarBot\", url_y));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"BarBot\", url_w));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"BarBot\", url_z));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"BazBot\", url_z));\n\n  // Lines with rules outside groups are ignored.\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", url_foo));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"BarBot\", url_foo));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"BazBot\", url_foo));\n}\n\n// Group must not be closed by rules not explicitly defined in the REP RFC.\n// See REP RFC section \"Protocol Definition\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.1\nTEST(RobotsUnittest, ID_LineSyntax_Groups_OtherRules) {\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: BarBot\\n\"\n        \"Sitemap: https://foo.bar/sitemap\\n\"\n        \"User-agent: *\\n\"\n        \"Disallow: /\\n\";\n    std::string url = \"http://foo.bar/\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"BarBot\", url));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: FooBot\\n\"\n        \"Invalid-Unknown-Line: unknown\\n\"\n        \"User-agent: *\\n\"\n        \"Disallow: /\\n\";\n    std::string url = \"http://foo.bar/\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"BarBot\", url));\n  }\n}\n\n// REP lines are case insensitive. See REP RFC section \"Protocol Definition\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.1\nTEST(RobotsUnittest, ID_REPLineNamesCaseInsensitive) {\n  const absl::string_view robotstxt_upper =\n      \"USER-AGENT: FooBot\\n\"\n      \"ALLOW: /x/\\n\"\n      \"DISALLOW: /\\n\";\n  const absl::string_view robotstxt_lower =\n      \"user-agent: FooBot\\n\"\n      \"allow: /x/\\n\"\n      \"disallow: /\\n\";\n  const absl::string_view robotstxt_camel =\n      \"uSeR-aGeNt: FooBot\\n\"\n      \"AlLoW: /x/\\n\"\n      \"dIsAlLoW: /\\n\";\n  const std::string url_allowed = \"http://foo.bar/x/y\";\n  const std::string url_disallowed = \"http://foo.bar/a/b\";\n\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_upper, \"FooBot\", url_allowed));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_lower, \"FooBot\", url_allowed));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_camel, \"FooBot\", url_allowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_upper, \"FooBot\", url_disallowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_lower, \"FooBot\", url_disallowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_camel, \"FooBot\", url_disallowed));\n}\n\n// A user-agent line is expected to contain only [a-zA-Z_-] characters and must\n// not be empty. See REP RFC section \"The user-agent line\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.1\nTEST(RobotsUnittest, ID_VerifyValidUserAgentsToObey) {\n  EXPECT_TRUE(RobotsMatcher::IsValidUserAgentToObey(\"Foobot\"));\n  EXPECT_TRUE(RobotsMatcher::IsValidUserAgentToObey(\"Foobot-Bar\"));\n  EXPECT_TRUE(RobotsMatcher::IsValidUserAgentToObey(\"Foo_Bar\"));\n\n  EXPECT_FALSE(RobotsMatcher::IsValidUserAgentToObey(absl::string_view()));\n  EXPECT_FALSE(RobotsMatcher::IsValidUserAgentToObey(\"\"));\n  EXPECT_FALSE(RobotsMatcher::IsValidUserAgentToObey(\"\"));\n\n  EXPECT_FALSE(RobotsMatcher::IsValidUserAgentToObey(\"Foobot*\"));\n  EXPECT_FALSE(RobotsMatcher::IsValidUserAgentToObey(\" Foobot \"));\n  EXPECT_FALSE(RobotsMatcher::IsValidUserAgentToObey(\"Foobot/2.1\"));\n\n  EXPECT_FALSE(RobotsMatcher::IsValidUserAgentToObey(\"Foobot Bar\"));\n}\n\n// User-agent line values are case insensitive. See REP RFC section \"The\n// user-agent line\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.1\nTEST(RobotsUnittest, ID_UserAgentValueCaseInsensitive) {\n  const absl::string_view robotstxt_upper =\n      \"User-Agent: FOO BAR\\n\"\n      \"Allow: /x/\\n\"\n      \"Disallow: /\\n\";\n  const absl::string_view robotstxt_lower =\n      \"User-Agent: foo bar\\n\"\n      \"Allow: /x/\\n\"\n      \"Disallow: /\\n\";\n  const absl::string_view robotstxt_camel =\n      \"User-Agent: FoO bAr\\n\"\n      \"Allow: /x/\\n\"\n      \"Disallow: /\\n\";\n  const std::string url_allowed = \"http://foo.bar/x/y\";\n  const std::string url_disallowed = \"http://foo.bar/a/b\";\n\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_upper, \"Foo\", url_allowed));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_lower, \"Foo\", url_allowed));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_camel, \"Foo\", url_allowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_upper, \"Foo\", url_disallowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_lower, \"Foo\", url_disallowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_camel, \"Foo\", url_disallowed));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_upper, \"foo\", url_allowed));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_lower, \"foo\", url_allowed));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_camel, \"foo\", url_allowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_upper, \"foo\", url_disallowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_lower, \"foo\", url_disallowed));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_camel, \"foo\", url_disallowed));\n}\n\n// Google specific: accept user-agent value up to the first space. Space is not\n// allowed in user-agent values, but that doesn't stop webmasters from using\n// them. This is more restrictive than the RFC, since in case of the bad value\n// \"Googlebot Images\" we'd still obey the rules with \"Googlebot\".\n// Extends REP RFC section \"The user-agent line\"\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.1\nTEST(RobotsUnittest, GoogleOnly_AcceptUserAgentUpToFirstSpace) {\n  EXPECT_FALSE(RobotsMatcher::IsValidUserAgentToObey(\"Foobot Bar\"));\n  const absl::string_view robotstxt =\n      \"User-Agent: *\\n\"\n      \"Disallow: /\\n\"\n      \"User-Agent: Foo Bar\\n\"\n      \"Allow: /x/\\n\"\n      \"Disallow: /\\n\";\n  const std::string url = \"http://foo.bar/x/y\";\n\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"Foo\", url));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"Foo Bar\", url));\n}\n\n// If no group matches the user-agent, crawlers must obey the first group with a\n// user-agent line with a \"*\" value, if present. If no group satisfies either\n// condition, or no groups are present at all, no rules apply.\n// See REP RFC section \"The user-agent line\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.1\nTEST(RobotsUnittest, ID_GlobalGroups_Secondary) {\n  const absl::string_view robotstxt_empty = \"\";\n  const absl::string_view robotstxt_global =\n      \"user-agent: *\\n\"\n      \"allow: /\\n\"\n      \"user-agent: FooBot\\n\"\n      \"disallow: /\\n\";\n  const absl::string_view robotstxt_only_specific =\n      \"user-agent: FooBot\\n\"\n      \"allow: /\\n\"\n      \"user-agent: BarBot\\n\"\n      \"disallow: /\\n\"\n      \"user-agent: BazBot\\n\"\n      \"disallow: /\\n\";\n  const std::string url = \"http://foo.bar/x/y\";\n\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_empty, \"FooBot\", url));\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_global, \"FooBot\", url));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_global, \"BarBot\", url));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_only_specific, \"QuxBot\", url));\n}\n\n// Matching rules against URIs is case sensitive.\n// See REP RFC section \"The Allow and Disallow lines\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.2\nTEST(RobotsUnittest, ID_AllowDisallow_Value_CaseSensitive) {\n  const absl::string_view robotstxt_lowercase_url =\n      \"user-agent: FooBot\\n\"\n      \"disallow: /x/\\n\";\n  const absl::string_view robotstxt_uppercase_url =\n      \"user-agent: FooBot\\n\"\n      \"disallow: /X/\\n\";\n  const std::string url = \"http://foo.bar/x/y\";\n\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt_lowercase_url, \"FooBot\", url));\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt_uppercase_url, \"FooBot\", url));\n}\n\n// The most specific match found MUST be used. The most specific match is the\n// match that has the most octets. In case of multiple rules with the same\n// length, the least strict rule must be used.\n// See REP RFC section \"The Allow and Disallow lines\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.2\nTEST(RobotsUnittest, ID_LongestMatch) {\n  const std::string url = \"http://foo.bar/x/page.html\";\n  {\n    const absl::string_view robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /x/page.html\\n\"\n        \"allow: /x/\\n\";\n\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"allow: /x/page.html\\n\"\n        \"disallow: /x/\\n\";\n\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/x/\"));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: \\n\"\n        \"allow: \\n\";\n    // In case of equivalent disallow and allow patterns for the same\n    // user-agent, allow is used.\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /\\n\"\n        \"allow: /\\n\";\n    // In case of equivalent disallow and allow patterns for the same\n    // user-agent, allow is used.\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n  }\n  {\n    std::string url_a = \"http://foo.bar/x\";\n    std::string url_b = \"http://foo.bar/x/\";\n    const absl::string_view robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /x\\n\"\n        \"allow: /x/\\n\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", url_a));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url_b));\n  }\n\n  {\n    const absl::string_view robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /x/page.html\\n\"\n        \"allow: /x/page.html\\n\";\n    // In case of equivalent disallow and allow patterns for the same\n    // user-agent, allow is used.\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"allow: /page\\n\"\n        \"disallow: /*.html\\n\";\n    // Longest match wins.\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/page.html\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/page\"));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"allow: /x/page.\\n\"\n        \"disallow: /*.html\\n\";\n    // Longest match wins.\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/x/y.html\"));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: *\\n\"\n        \"Disallow: /x/\\n\"\n        \"User-agent: FooBot\\n\"\n        \"Disallow: /y/\\n\";\n    // Most specific group for FooBot allows implicitly /x/page.\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/x/page\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/y/page\"));\n  }\n}\n\n// Octets in the URI and robots.txt paths outside the range of the US-ASCII\n// coded character set, and those in the reserved range defined by RFC3986,\n// MUST be percent-encoded as defined by RFC3986 prior to comparison.\n// See REP RFC section \"The Allow and Disallow lines\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.2\n//\n// NOTE: It's up to the caller to percent encode a URL before passing it to the\n// parser. Percent encoding URIs in the rules is unnecessary.\nTEST(RobotsUnittest, ID_Encoding) {\n  // /foo/bar?baz=http://foo.bar stays unencoded.\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: FooBot\\n\"\n        \"Disallow: /\\n\"\n        \"Allow: /foo/bar?qux=taz&baz=http://foo.bar?tar&par\\n\";\n    EXPECT_TRUE(IsUserAgentAllowed(\n        robotstxt, \"FooBot\",\n        \"http://foo.bar/foo/bar?qux=taz&baz=http://foo.bar?tar&par\"));\n  }\n\n  // 3 byte character: /foo/bar/ -> /foo/bar/%E3%83%84\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: FooBot\\n\"\n        \"Disallow: /\\n\"\n        \"Allow: /foo/bar/\\n\";\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/foo/bar/%E3%83%84\"));\n    // The parser encodes the 3-byte character, but the URL is not %-encoded.\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar/\"));\n  }\n  // Percent encoded 3 byte character: /foo/bar/%E3%83%84 -> /foo/bar/%E3%83%84\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: FooBot\\n\"\n        \"Disallow: /\\n\"\n        \"Allow: /foo/bar/%E3%83%84\\n\";\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/foo/bar/%E3%83%84\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar/\"));\n  }\n  // Percent encoded unreserved US-ASCII: /foo/bar/%62%61%7A -> NULL\n  // This is illegal according to RFC3986 and while it may work here due to\n  // simple string matching, it should not be relied on.\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: FooBot\\n\"\n        \"Disallow: /\\n\"\n        \"Allow: /foo/bar/%62%61%7A\\n\";\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar/baz\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/foo/bar/%62%61%7A\"));\n  }\n}\n\n// The REP RFC defines the following characters that have special meaning in\n// robots.txt:\n// # - inline comment.\n// $ - end of pattern.\n// * - any number of characters.\n// See REP RFC section \"Special Characters\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.3\nTEST(RobotsUnittest, ID_SpecialCharacters) {\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: FooBot\\n\"\n        \"Disallow: /foo/bar/quz\\n\"\n        \"Allow: /foo/*/qux\\n\";\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar/quz\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/quz\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo//quz\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bax/quz\"));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: FooBot\\n\"\n        \"Disallow: /foo/bar$\\n\"\n        \"Allow: /foo/bar/qux\\n\";\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar/qux\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar/\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar/baz\"));\n  }\n  {\n    const absl::string_view robotstxt =\n        \"User-agent: FooBot\\n\"\n        \"# Disallow: /\\n\"\n        \"Disallow: /foo/quz#qux\\n\"\n        \"Allow: /\\n\";\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/bar\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/foo/quz\"));\n  }\n}\n\n// Google-specific: \"index.html\" (and only that) at the end of a pattern is\n// equivalent to \"/\".\nTEST(RobotsUnittest, GoogleOnly_IndexHTMLisDirectory) {\n  const absl::string_view robotstxt =\n      \"User-Agent: *\\n\"\n      \"Allow: /allowed-slash/index.html\\n\"\n      \"Disallow: /\\n\";\n  // If index.html is allowed, we interpret this as / being allowed too.\n  EXPECT_TRUE(\n      IsUserAgentAllowed(robotstxt, \"foobot\", \"http://foo.com/allowed-slash/\"));\n  // Does not exatly match.\n  EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"foobot\",\n                                  \"http://foo.com/allowed-slash/index.htm\"));\n  // Exact match.\n  EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"foobot\",\n                                 \"http://foo.com/allowed-slash/index.html\"));\n  EXPECT_FALSE(\n      IsUserAgentAllowed(robotstxt, \"foobot\", \"http://foo.com/anyother-url\"));\n}\n\n// Google-specific: long lines are ignored after 8 * 2083 bytes. See comment in\n// RobotsTxtParser::Parse().\nTEST(RobotsUnittest, GoogleOnly_LineTooLong) {\n  size_t kEOLLen = std::string(\"\\n\").length();\n  int kMaxLineLen = 2083 * 8;\n  std::string allow = \"allow: \";\n  std::string disallow = \"disallow: \";\n\n  // Disallow rule pattern matches the URL after being cut off at kMaxLineLen.\n  {\n    std::string robotstxt = \"user-agent: FooBot\\n\";\n    std::string longline = \"/x/\";\n    size_t max_length =\n        kMaxLineLen - longline.length() - disallow.length() + kEOLLen;\n    while (longline.size() < max_length) {\n      absl::StrAppend(&longline, \"a\");\n    }\n    absl::StrAppend(&robotstxt, disallow, longline, \"/qux\\n\");\n\n    // Matches nothing, so URL is allowed.\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fux\"));\n    // Matches cut off disallow rule.\n    EXPECT_FALSE(IsUserAgentAllowed(\n        robotstxt, \"FooBot\", absl::StrCat(\"http://foo.bar\", longline, \"/fux\")));\n  }\n\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /\\n\";\n    std::string longline_a = \"/x/\";\n    std::string longline_b = \"/x/\";\n    size_t max_length =\n        kMaxLineLen - longline_a.length() - allow.length() + kEOLLen;\n    while (longline_a.size() < max_length) {\n      absl::StrAppend(&longline_a, \"a\");\n      absl::StrAppend(&longline_b, \"b\");\n    }\n    absl::StrAppend(&robotstxt, allow, longline_a, \"/qux\\n\");\n    absl::StrAppend(&robotstxt, allow, longline_b, \"/qux\\n\");\n\n    // URL matches the disallow rule.\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/\"));\n    // Matches the allow rule exactly.\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\",\n                           absl::StrCat(\"http://foo.bar\", longline_a, \"/qux\")));\n    // Matches cut off allow rule.\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\",\n                           absl::StrCat(\"http://foo.bar\", longline_b, \"/fux\")));\n  }\n}\n\nTEST(RobotsUnittest, GoogleOnly_DocumentationChecks) {\n  // Test documentation from\n  // https://developers.google.com/search/reference/robots_txt\n  // Section \"URL matching based on path values\".\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /\\n\"\n        \"allow: /fish\\n\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/bar\"));\n\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish.html\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/fish/salmon.html\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fishheads\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/fishheads/yummy.html\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/fish.html?id=anything\"));\n\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/Fish.asp\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/catfish\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/?id=fish\"));\n  }\n  // \"/fish*\" equals \"/fish\"\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /\\n\"\n        \"allow: /fish*\\n\";\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/bar\"));\n\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish.html\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/fish/salmon.html\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fishheads\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/fishheads/yummy.html\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/fish.html?id=anything\"));\n\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/Fish.bar\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/catfish\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/?id=fish\"));\n  }\n  // \"/fish/\" does not equal \"/fish\"\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /\\n\"\n        \"allow: /fish/\\n\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/bar\"));\n\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish/\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish/salmon\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish/?salmon\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/fish/salmon.html\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/fish/?id=anything\"));\n\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish.html\"));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/Fish/Salmon.html\"));\n  }\n  // \"/*.php\"\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /\\n\"\n        \"allow: /*.php\\n\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/bar\"));\n\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/filename.php\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/folder/filename.php\"));\n    EXPECT_TRUE(IsUserAgentAllowed(\n        robotstxt, \"FooBot\", \"http://foo.bar/folder/filename.php?parameters\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar//folder/any.php.file.html\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/filename.php/\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/index?f=filename.php/\"));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/php/\"));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/index?php\"));\n\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/windows.PHP\"));\n  }\n  // \"/*.php$\"\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /\\n\"\n        \"allow: /*.php$\\n\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/bar\"));\n\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/filename.php\"));\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                   \"http://foo.bar/folder/filename.php\"));\n\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/filename.php?parameters\"));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/filename.php/\"));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/filename.php5\"));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/php/\"));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/filename?php\"));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\",\n                                    \"http://foo.bar/aaaphpaaa\"));\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar//windows.PHP\"));\n  }\n  // \"/fish*.php\"\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"disallow: /\\n\"\n        \"allow: /fish*.php\\n\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/bar\"));\n\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/fish.php\"));\n    EXPECT_TRUE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\",\n                           \"http://foo.bar/fishheads/catfish.php?parameters\"));\n\n    EXPECT_FALSE(\n        IsUserAgentAllowed(robotstxt, \"FooBot\", \"http://foo.bar/Fish.PHP\"));\n  }\n  // Section \"Order of precedence for group-member records\".\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"allow: /p\\n\"\n        \"disallow: /\\n\";\n    std::string url = \"http://example.com/page\";\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n  }\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"allow: /folder\\n\"\n        \"disallow: /folder\\n\";\n    std::string url = \"http://example.com/folder/page\";\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n  }\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"allow: /page\\n\"\n        \"disallow: /*.htm\\n\";\n    std::string url = \"http://example.com/page.htm\";\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n  }\n  {\n    std::string robotstxt =\n        \"user-agent: FooBot\\n\"\n        \"allow: /$\\n\"\n        \"disallow: /\\n\";\n    std::string url = \"http://example.com/\";\n    std::string url_page = \"http://example.com/page.html\";\n    EXPECT_TRUE(IsUserAgentAllowed(robotstxt, \"FooBot\", url));\n    EXPECT_FALSE(IsUserAgentAllowed(robotstxt, \"FooBot\", url_page));\n  }\n}\n\nclass RobotsStatsReporter : public googlebot::RobotsParseHandler {\n public:\n  void HandleRobotsStart() override {\n    last_line_seen_ = 0;\n    valid_directives_ = 0;\n    unknown_directives_ = 0;\n    sitemap_.clear();\n  }\n  void HandleRobotsEnd() override {}\n\n  void HandleUserAgent(int line_num, absl::string_view value) override {\n    Digest(line_num);\n  }\n  void HandleAllow(int line_num, absl::string_view value) override {\n    Digest(line_num);\n  }\n  void HandleDisallow(int line_num, absl::string_view value) override {\n    Digest(line_num);\n  }\n\n  void HandleSitemap(int line_num, absl::string_view value) override {\n    Digest(line_num);\n    sitemap_.append(value.data(), value.length());\n  }\n\n  // Any other unrecognized name/v pairs.\n  void HandleUnknownAction(int line_num, absl::string_view action,\n                           absl::string_view value) override {\n    last_line_seen_ = line_num;\n    unknown_directives_++;\n  }\n\n  int last_line_seen() const { return last_line_seen_; }\n\n  // All directives found, including unknown.\n  int valid_directives() const { return valid_directives_; }\n\n  // Number of unknown directives.\n  int unknown_directives() const { return unknown_directives_; }\n\n  // Parsed sitemap line.\n  std::string sitemap() const { return sitemap_; }\n\n private:\n  void Digest(int line_num) {\n    ASSERT_GE(line_num, last_line_seen_);\n    last_line_seen_ = line_num;\n    valid_directives_++;\n  }\n\n  int last_line_seen_ = 0;\n  int valid_directives_ = 0;\n  int unknown_directives_ = 0;\n  std::string sitemap_;\n};\n\n// Different kinds of line endings are all supported: %x0D / %x0A / %x0D.0A\nTEST(RobotsUnittest, ID_LinesNumbersAreCountedCorrectly) {\n  RobotsStatsReporter report;\n  static const char kUnixFile[] =\n      \"User-Agent: foo\\n\"\n      \"Allow: /some/path\\n\"\n      \"User-Agent: bar\\n\"\n      \"\\n\"\n      \"\\n\"\n      \"Disallow: /\\n\";\n  googlebot::ParseRobotsTxt(kUnixFile, &report);\n  EXPECT_EQ(4, report.valid_directives());\n  EXPECT_EQ(6, report.last_line_seen());\n\n  static const char kDosFile[] =\n      \"User-Agent: foo\\r\\n\"\n      \"Allow: /some/path\\r\\n\"\n      \"User-Agent: bar\\r\\n\"\n      \"\\r\\n\"\n      \"\\r\\n\"\n      \"Disallow: /\\r\\n\";\n  googlebot::ParseRobotsTxt(kDosFile, &report);\n  EXPECT_EQ(4, report.valid_directives());\n  EXPECT_EQ(6, report.last_line_seen());\n\n  static const char kMacFile[] =\n      \"User-Agent: foo\\r\"\n      \"Allow: /some/path\\r\"\n      \"User-Agent: bar\\r\"\n      \"\\r\"\n      \"\\r\"\n      \"Disallow: /\\r\";\n  googlebot::ParseRobotsTxt(kMacFile, &report);\n  EXPECT_EQ(4, report.valid_directives());\n  EXPECT_EQ(6, report.last_line_seen());\n\n  static const char kNoFinalNewline[] =\n      \"User-Agent: foo\\n\"\n      \"Allow: /some/path\\n\"\n      \"User-Agent: bar\\n\"\n      \"\\n\"\n      \"\\n\"\n      \"Disallow: /\";\n  googlebot::ParseRobotsTxt(kNoFinalNewline, &report);\n  EXPECT_EQ(4, report.valid_directives());\n  EXPECT_EQ(6, report.last_line_seen());\n\n  static const char kMixedFile[] =\n      \"User-Agent: foo\\n\"\n      \"Allow: /some/path\\r\\n\"\n      \"User-Agent: bar\\n\"\n      \"\\r\\n\"\n      \"\\n\"\n      \"Disallow: /\";\n  googlebot::ParseRobotsTxt(kMixedFile, &report);\n  EXPECT_EQ(4, report.valid_directives());\n  EXPECT_EQ(6, report.last_line_seen());\n}\n\n// BOM characters are unparseable and thus skipped. The rules following the line\n// are used.\nTEST(RobotsUnittest, ID_UTF8ByteOrderMarkIsSkipped) {\n  RobotsStatsReporter report;\n  static const char kUtf8FileFullBOM[] =\n      \"\\xEF\\xBB\\xBF\"\n      \"User-Agent: foo\\n\"\n      \"Allow: /AnyValue\\n\";\n  googlebot::ParseRobotsTxt(kUtf8FileFullBOM, &report);\n  EXPECT_EQ(2, report.valid_directives());\n  EXPECT_EQ(0, report.unknown_directives());\n\n  // We allow as well partial ByteOrderMarks.\n  static const char kUtf8FilePartial2BOM[] =\n      \"\\xEF\\xBB\"\n      \"User-Agent: foo\\n\"\n      \"Allow: /AnyValue\\n\";\n  googlebot::ParseRobotsTxt(kUtf8FilePartial2BOM, &report);\n  EXPECT_EQ(2, report.valid_directives());\n  EXPECT_EQ(0, report.unknown_directives());\n\n  static const char kUtf8FilePartial1BOM[] =\n      \"\\xEF\"\n      \"User-Agent: foo\\n\"\n      \"Allow: /AnyValue\\n\";\n  googlebot::ParseRobotsTxt(kUtf8FilePartial1BOM, &report);\n  EXPECT_EQ(2, report.valid_directives());\n  EXPECT_EQ(0, report.unknown_directives());\n\n  // If the BOM is not the right sequence, the first line looks like garbage\n  // that is skipped (we essentially see \"\\x11\\xBFUser-Agent\").\n  static const char kUtf8FileBrokenBOM[] =\n      \"\\xEF\\x11\\xBF\"\n      \"User-Agent: foo\\n\"\n      \"Allow: /AnyValue\\n\";\n  googlebot::ParseRobotsTxt(kUtf8FileBrokenBOM, &report);\n  EXPECT_EQ(1, report.valid_directives());\n  EXPECT_EQ(1, report.unknown_directives());  // We get one broken line.\n\n  // Some other messed up file: BOMs only valid in the beginning of the file.\n  static const char kUtf8BOMSomewhereInMiddleOfFile[] =\n      \"User-Agent: foo\\n\"\n      \"\\xEF\\xBB\\xBF\"\n      \"Allow: /AnyValue\\n\";\n  googlebot::ParseRobotsTxt(kUtf8BOMSomewhereInMiddleOfFile, &report);\n  EXPECT_EQ(1, report.valid_directives());\n  EXPECT_EQ(1, report.unknown_directives());\n}\n\n// Google specific: the RFC allows any line that crawlers might need, such as\n// sitemaps, which Google supports.\n// See REP RFC section \"Other records\".\n// https://www.rfc-editor.org/rfc/rfc9309.html#section-2.2.4\nTEST(RobotsUnittest, ID_NonStandardLineExample_Sitemap) {\n  RobotsStatsReporter report;\n  {\n    std::string sitemap_loc = \"http://foo.bar/sitemap.xml\";\n    std::string robotstxt =\n        \"User-Agent: foo\\n\"\n        \"Allow: /some/path\\n\"\n        \"User-Agent: bar\\n\"\n        \"\\n\"\n        \"\\n\";\n    absl::StrAppend(&robotstxt, \"Sitemap: \", sitemap_loc, \"\\n\");\n\n    googlebot::ParseRobotsTxt(robotstxt, &report);\n    EXPECT_EQ(sitemap_loc, report.sitemap());\n  }\n  // A sitemap line may appear anywhere in the file.\n  {\n    std::string robotstxt;\n    std::string sitemap_loc = \"http://foo.bar/sitemap.xml\";\n    std::string robotstxt_temp =\n        \"User-Agent: foo\\n\"\n        \"Allow: /some/path\\n\"\n        \"User-Agent: bar\\n\"\n        \"\\n\"\n        \"\\n\";\n    absl::StrAppend(&robotstxt, \"Sitemap: \", sitemap_loc, \"\\n\", robotstxt_temp);\n\n    googlebot::ParseRobotsTxt(robotstxt, &report);\n    EXPECT_EQ(sitemap_loc, report.sitemap());\n  }\n}\n\n}  // namespace\n\n// Integrity tests. These functions are available to the linker, but not in the\n// header, because they should only be used for testing.\nnamespace googlebot {\nstd::string GetPathParamsQuery(const std::string& url);\nbool MaybeEscapePattern(const char* src, char** dst);\n}  // namespace googlebot\n\nvoid TestPath(const std::string& url, const std::string& expected_path) {\n  EXPECT_EQ(expected_path, googlebot::GetPathParamsQuery(url));\n}\n\nvoid TestEscape(const std::string& url, const std::string& expected) {\n  char* escaped_value = nullptr;\n  const bool is_escaped =\n      googlebot::MaybeEscapePattern(url.c_str(), &escaped_value);\n  const std::string escaped = escaped_value;\n  if (is_escaped) delete[] escaped_value;\n\n  EXPECT_EQ(expected, escaped);\n}\n\nTEST(RobotsUnittest, TestGetPathParamsQuery) {\n  // Only testing URLs that are already correctly escaped here.\n  TestPath(\"\", \"/\");\n  TestPath(\"http://www.example.com\", \"/\");\n  TestPath(\"http://www.example.com/\", \"/\");\n  TestPath(\"http://www.example.com/a\", \"/a\");\n  TestPath(\"http://www.example.com/a/\", \"/a/\");\n  TestPath(\"http://www.example.com/a/b?c=http://d.e/\", \"/a/b?c=http://d.e/\");\n  TestPath(\"http://www.example.com/a/b?c=d&e=f#fragment\", \"/a/b?c=d&e=f\");\n  TestPath(\"example.com\", \"/\");\n  TestPath(\"example.com/\", \"/\");\n  TestPath(\"example.com/a\", \"/a\");\n  TestPath(\"example.com/a/\", \"/a/\");\n  TestPath(\"example.com/a/b?c=d&e=f#fragment\", \"/a/b?c=d&e=f\");\n  TestPath(\"a\", \"/\");\n  TestPath(\"a/\", \"/\");\n  TestPath(\"/a\", \"/a\");\n  TestPath(\"a/b\", \"/b\");\n  TestPath(\"example.com?a\", \"/?a\");\n  TestPath(\"example.com/a;b#c\", \"/a;b\");\n  TestPath(\"//a/b/c\", \"/b/c\");\n}\n\nTEST(RobotsUnittest, TestMaybeEscapePattern) {\n  TestEscape(\"http://www.example.com\", \"http://www.example.com\");\n  TestEscape(\"/a/b/c\", \"/a/b/c\");\n  TestEscape(\"\", \"%C3%A1\");\n  TestEscape(\"%aa\", \"%AA\");\n}\n"
        }
      ]
    }
  ]
}